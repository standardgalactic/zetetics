The Principles of Deep Learning Theory
An Eﬀective Theory Approach to Understanding Neural Networks
Daniel A. Roberts and Sho Yaida
based on research in collaboration with
Boris Hanin
drob@mit.edu, shoyaida@fb.com
arXiv:2106.10165v2  [cs.LG]  24 Aug 2021

ii

Contents
Preface
vii
0
Initialization
1
0.1
An Eﬀective Theory Approach
. . . . . . . . . . . . . . . . . . . . . . . .
2
0.2
The Theoretical Minimum . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1
Pretraining
13
1.1
Gaussian Integrals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.2
Probability, Correlation and Statistics, and All That . . . . . . . . . . . .
23
1.3
Nearly-Gaussian Distributions . . . . . . . . . . . . . . . . . . . . . . . . .
28
2
Neural Networks
37
2.1
Function Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.2
Activation Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.3
Ensembles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3
Eﬀective Theory of Deep Linear Networks at Initialization
53
3.1
Deep Linear Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.2
Criticality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
3.3
Fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.4
Chaos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4
RG Flow of Preactivations
71
4.1
First Layer: Good-Old Gaussian
. . . . . . . . . . . . . . . . . . . . . . .
73
4.2
Second Layer: Genesis of Non-Gaussianity . . . . . . . . . . . . . . . . . .
79
4.3
Deeper Layers: Accumulation of Non-Gaussianity . . . . . . . . . . . . . .
89
4.4
Marginalization Rules
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
4.5
Subleading Corrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
4.6
RG Flow and RG Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5
Eﬀective Theory of Preactivations at Initialization
109
5.1
Criticality Analysis of the Kernel . . . . . . . . . . . . . . . . . . . . . . . 110
5.2
Criticality for Scale-Invariant Activations
. . . . . . . . . . . . . . . . . . 123
5.3
Universality beyond Scale-Invariant Activations . . . . . . . . . . . . . . . 125
iii

5.3.1
General Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
5.3.2
No Criticality: sigmoid, softplus, nonlinear monomials, etc. . . . . 127
5.3.3
K⋆= 0 Universality Class: tanh, sin, etc. . . . . . . . . . . . . . . 129
5.3.4
Half-Stable Universality Classes: SWISH, etc. and GELU, etc.
. . 134
5.4
Fluctuations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
5.4.1
Fluctuations for the Scale-Invariant Universality Class . . . . . . . 139
5.4.2
Fluctuations for the K⋆= 0 Universality Class . . . . . . . . . . . 140
5.5
Finite-Angle Analysis for the Scale-Invariant Universality Class . . . . . . 145
6
Bayesian Learning
153
6.1
Bayesian Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
6.2
Bayesian Inference and Neural Networks . . . . . . . . . . . . . . . . . . . 156
6.2.1
Bayesian Model Fitting
. . . . . . . . . . . . . . . . . . . . . . . . 156
6.2.2
Bayesian Model Comparison
. . . . . . . . . . . . . . . . . . . . . 165
6.3
Bayesian Inference at Inﬁnite Width . . . . . . . . . . . . . . . . . . . . . 168
6.3.1
The Evidence for Criticality . . . . . . . . . . . . . . . . . . . . . . 169
6.3.2
Let’s Not Wire Together . . . . . . . . . . . . . . . . . . . . . . . . 173
6.3.3
Absence of Representation Learning . . . . . . . . . . . . . . . . . 177
6.4
Bayesian Inference at Finite Width . . . . . . . . . . . . . . . . . . . . . . 178
6.4.1
Hebbian Learning, Inc. . . . . . . . . . . . . . . . . . . . . . . . . . 179
6.4.2
Let’s Wire Together . . . . . . . . . . . . . . . . . . . . . . . . . . 182
6.4.3
Presence of Representation Learning . . . . . . . . . . . . . . . . . 185
7
Gradient-Based Learning
191
7.1
Supervised Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
7.2
Gradient Descent and Function Approximation . . . . . . . . . . . . . . . 194
8
RG Flow of the Neural Tangent Kernel
199
8.0
Forward Equation for the NTK . . . . . . . . . . . . . . . . . . . . . . . . 200
8.1
First Layer: Deterministic NTK . . . . . . . . . . . . . . . . . . . . . . . . 206
8.2
Second Layer: Fluctuating NTK
. . . . . . . . . . . . . . . . . . . . . . . 207
8.3
Deeper Layers: Accumulation of NTK Fluctuations . . . . . . . . . . . . . 211
8.3.0
Interlude: Interlayer Correlations
. . . . . . . . . . . . . . . . . . 211
8.3.1
NTK Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
8.3.2
NTK-Preactivation Cross Correlations . . . . . . . . . . . . . . . . 216
8.3.3
NTK Variance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
9
Eﬀective Theory of the NTK at Initialization
227
9.1
Criticality Analysis of the NTK . . . . . . . . . . . . . . . . . . . . . . . . 228
9.2
Scale-Invariant Universality Class . . . . . . . . . . . . . . . . . . . . . . . 233
9.3
K⋆= 0 Universality Class . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
9.4
Criticality, Exploding and Vanishing Problems, and None of That . . . . . 241
iv

10 Kernel Learning
247
10.1 A Small Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
10.1.1 No Wiring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
10.1.2 No Representation Learning . . . . . . . . . . . . . . . . . . . . . . 250
10.2 A Giant Leap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
10.2.1 Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
10.2.2 Algorithm Independence . . . . . . . . . . . . . . . . . . . . . . . . 257
10.2.3 Aside: Cross-Entropy Loss . . . . . . . . . . . . . . . . . . . . . . . 259
10.2.4 Kernel Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
10.3 Generalization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264
10.3.1 Bias-Variance Tradeoﬀand Criticality . . . . . . . . . . . . . . . . 267
10.3.2 Interpolation and Extrapolation
. . . . . . . . . . . . . . . . . . . 277
10.4 Linear Models and Kernel Methods . . . . . . . . . . . . . . . . . . . . . . 282
10.4.1 Linear Models
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
10.4.2 Kernel Methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
10.4.3 Inﬁnite-Width Networks as Linear Models . . . . . . . . . . . . . . 287
11 Representation Learning
291
11.1 Diﬀerential of the Neural Tangent Kernel
. . . . . . . . . . . . . . . . . . 293
11.2 RG Flow of the dNTK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
11.2.0 Forward Equation for the dNTK . . . . . . . . . . . . . . . . . . . 297
11.2.1 First Layer: Zero dNTK . . . . . . . . . . . . . . . . . . . . . . . . 299
11.2.2 Second Layer: Nonzero dNTK
. . . . . . . . . . . . . . . . . . . . 299
11.2.3 Deeper Layers: Growing dNTK . . . . . . . . . . . . . . . . . . . . 301
11.3 Eﬀective Theory of the dNTK at Initialization
. . . . . . . . . . . . . . . 310
11.3.1 Scale-Invariant Universality Class . . . . . . . . . . . . . . . . . . . 312
11.3.2 K⋆= 0 Universality Class . . . . . . . . . . . . . . . . . . . . . . . 314
11.4 Nonlinear Models and Nearly-Kernel Methods . . . . . . . . . . . . . . . . 317
11.4.1 Nonlinear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
11.4.2 Nearly-Kernel Methods
. . . . . . . . . . . . . . . . . . . . . . . . 323
11.4.3 Finite-Width Networks as Nonlinear Models . . . . . . . . . . . . . 329
∞The End of Training
335
∞.1 Two More Diﬀerentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
∞.2 Training at Finite Width
. . . . . . . . . . . . . . . . . . . . . . . . . . . 347
∞.2.1 A Small Step Following a Giant Leap
. . . . . . . . . . . . . . . . 352
∞.2.2 Many Many Steps of Gradient Descent . . . . . . . . . . . . . . . . 357
∞.2.3 Prediction at Finite Width
. . . . . . . . . . . . . . . . . . . . . . 374
∞.3 RG Flow of the ddNTKs: The Full Expressions . . . . . . . . . . . . . . . 385
ε
Epilogue: Model Complexity from the Macroscopic Perspective
391
v

A Information in Deep Learning
401
A.1 Entropy and Mutual Information . . . . . . . . . . . . . . . . . . . . . . . 402
A.2 Information at Inﬁnite Width: Criticality
. . . . . . . . . . . . . . . . . . 410
A.3 Information at Finite Width: Optimal Aspect Ratio
. . . . . . . . . . . . 412
B Residual Learning
425
B.1
Residual Multilayer Perceptrons . . . . . . . . . . . . . . . . . . . . . . . . 428
B.2
Residual Inﬁnite Width: Criticality Analysis
. . . . . . . . . . . . . . . . 429
B.3
Residual Finite Width: Optimal Aspect Ratio . . . . . . . . . . . . . . . . 431
B.4
Residual Building Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
References
439
Index
447
vi

Preface
This has necessitated a complete break from the historical line of development, but this
break is an advantage through enabling the approach to the new ideas to be made as
direct as possible.
P. A. M. Dirac in the 1930 preface of The Principles of Quantum Mechanics [1].
This is a research monograph in the style of a textbook about the theory of deep learning.
While this book might look a little diﬀerent from the other deep learning books that
you’ve seen before, we assure you that it is appropriate for everyone with knowledge
of linear algebra, multivariable calculus, and informal probability theory, and with a
healthy interest in neural networks. Practitioner and theorist alike, we want all of you
to enjoy this book. Now, let us tell you some things.
First and foremost, in this book we’ve strived for pedagogy in every choice we’ve
made, placing intuition above formality. This doesn’t mean that calculations are incom-
plete or sloppy; quite the opposite, we’ve tried to provide full details of every calculation
– of which there are certainly very many – and place a particular emphasis on the tools
needed to carry out related calculations of interest. In fact, understanding how the calcu-
lations are done is as important as knowing their results, and thus often our pedagogical
focus is on the details therein.
Second, while we present the details of all our calculations, we’ve kept the experi-
mental conﬁrmations to the privacy of our own computerized notebooks. Our reason
for this is simple: while there’s much to learn from explaining a derivation, there’s not
much more to learn from printing a veriﬁcation plot that shows two curves lying on top
of each other. Given the simplicity of modern deep-learning codes and the availability
of compute, it’s easy to verify any formula on your own; we certainly have thoroughly
checked them all this way, so if knowledge of the existence of such plots are comforting
to you, know at least that they do exist on our personal and cloud-based hard drives.
Third, our main focus is on realistic models that are used by the deep learning
community in practice: we want to study deep neural networks.
In particular, this
means that (i) a number of special results on single-hidden-layer networks will not be
discussed and (ii) the inﬁnite-width limit of a neural network – which corresponds to a
zero-hidden-layer network – will be introduced only as a starting point. All such idealized
models will eventually be perturbed until they correspond to a real model. We certainly
acknowledge that there’s a vibrant community of deep-learning theorists devoted to
vii

exploring diﬀerent kinds of idealized theoretical limits. However, our interests are ﬁxed
ﬁrmly on providing explanations for the tools and approaches used by practitioners, in
an eﬀort to shed light on what makes them work so well.
Fourth, a large part of the book is focused on deep multilayer perceptrons. We made
this choice in order to pedagogically illustrate the power of the eﬀective theory framework
– not due to any technical obstruction – and along the way we give pointers for how this
formalism can be extended to other architectures of interest. In fact, we expect that
many of our results have a broad applicability, and we’ve tried to focus on aspects that
we expect to have lasting and universal value to the deep learning community.
Fifth, while much of the material is novel and appears for the ﬁrst time in this
book, and while much of our framing, notation, language, and emphasis breaks with
the historical line of development, we’re also very much indebted to the deep learning
community. With that in mind, throughout the book we will try to reference important
prior contributions, with an emphasis on recent seminal deep-learning results rather than
on being completely comprehensive. Additional references for those interested can easily
be found within the work that we cite.
Sixth, this book initially grew out of a research project in collaboration with Boris
Hanin. To account for his eﬀort and then support, we’ve accordingly commemorated him
on the cover. More broadly, we’ve variously appreciated the artwork, discussions, en-
couragement, epigraphs, feedback, management, refereeing, reintroduction, and support
from Rafael Araujo, L´eon Bottou, Paul Dirac, Ethan Dyer, John Frank, Ross Girshick,
Vince Higgs, Yoni Kahn, Yann LeCun, Kyle Mahowald, Eric Mintun, Xiaoliang Qi,
Mike Rabbat, David Schwab, Stephen Shenker, Eva Silverstein, PJ Steiner, DJ Strouse,
and Jesse Thaler. Organizationally, we’re grateful to FAIR and Facebook, Diﬀeo and
Salesforce, MIT and IAIFI, and Cambridge University Press and the arXiv.
Seventh, given intense (and variously uncertain) spacetime and energy-momentum
commitment that writing this book entailed, Dan is grateful to Aya, Lumi, and Lisa
Yaida; from the dual sample-space perspective, Sho is grateful to Adrienne Rothschilds
and would be retroactively grateful to any hypothetical future Mark or Emily that would
have otherwise been thanked in this paragraph.
Eighth, we hope that this book spreads our optimism that it is possible to have
a general theory of deep learning, one that’s both derived from ﬁrst principles and at
the same time focused on describing how realistic models actually work: nearly-simple
phenomena in practice should correspond to nearly-simple eﬀective theories. We dream
that this type of thinking will not only lead to more [redacted] AI models but also guide
us towards a unifying framework for understanding universal aspects of intelligence.
As if that eightfold way of prefacing the book wasn’t nearly-enough already, please
note: this book has a website, deeplearningtheory.com, and you may want to visit
it in order to determine whether the error that you just discovered is already common
knowledge. If it’s not, please let us know. There may be pie.
Dan Roberts & Sho Yaida
Remotely Located
June, 2021
viii

Chapter 0
Initialization
The simulation is such that [one] generally perceives the sum of many billions of
elementary processes simultaneously, so that the leveling law of large numbers
completely obscures the real nature of the individual processes.
John von Neumann [2]
Thanks to substantial investments into computer technology, modern artiﬁcial intel-
ligence (AI) systems can now come equipped with many billions of elementary com-
ponents. When these components are properly initialized and then trained, AI can ac-
complish tasks once considered so incredibly complex that philosophers have previously
argued that only natural intelligence systems – i.e. humans – could perform them.
Behind much of this success in AI is deep learning. Deep learning uses artiﬁcial
neural networks as an underlying model for AI: while loosely based on biological neural
networks such as your brain, artiﬁcial neural networks are probably best thought of as
an especially nice way of specifying a ﬂexible set of functions, built out of many basic
computational blocks called neurons.
This model of computation is actually quite
diﬀerent from the one used to power the computer you’re likely using to read this book.
In particular, rather than programming a speciﬁc set of instructions to solve a problem
directly, deep learning models are trained on data from the real world and learn how to
solve problems.
The real power of the deep learning framework comes from deep neural networks,
with many neurons in parallel organized into sequential computational layers, learning
useful representations of the world. Such representation learning transforms data
into increasingly reﬁned forms that are helpful for solving an underlying task, and is
thought to be a hallmark of success in intelligence, both artiﬁcial and biological.
Despite these successes and the intense interest they created, deep learning theory
is still in its infancy. Indeed, there is a serious disconnect between theory and prac-
tice: while practitioners have reached amazing milestones, they have far outpaced the
theorists, whose analyses often involve assumptions so unrealistic that they lead to con-
clusions that are irrelevant to understanding deep neural networks as they are typically
1

used. More importantly, very little theoretical work directly confronts the deep of deep
learning, despite a mass of empirical evidence for its importance in the success of the
framework.
The goal of this book is to put forth a set of principles that enable us to theoretically
analyze deep neural networks of actual relevance. To initialize you to this task, in the
rest of this chapter we’ll explain at a very high-level both (i) why such a goal is even
attainable in theory and (ii) how we are able to get there in practice.
0.1
An Eﬀective Theory Approach
Steam navigation brings nearer together the most distant nations. . . . their theory is
very little understood, and the attempts to improve them are still directed almost by
chance. . . . We propose now to submit these questions to a deliberate examination.
Sadi Carnot, commenting on the need for a theory of deep learning [3].
While modern deep learning models are built up from seemingly innumerable elementary
computational components, a ﬁrst-principles microscopic description of how a trained
neural network computes a function from these low-level components is entirely manifest.
This microscopic description is just the set of instructions for transforming an input
through the many layers of components into an output. Importantly, during the training
process, these components become very ﬁnely-tuned, and knowledge of the particular
tunings is necessary for a system to produce useful output.
Unfortunately, the complexity of these tunings obscures any ﬁrst-principles macro-
scopic understanding of why a deep neural network computes a particular function and
not another. With many neurons performing diﬀerent tasks as part of such a computa-
tion, it seems hopeless to think that we can use theory to understand these models at
all, and silly to believe that a small set of mathematical principles will be suﬃcient for
that job.
Fortunately, theoretical physics has a long tradition of ﬁnding simple eﬀective
theories of complicated systems with a large number of components. The immense
success of the program of physics in modeling our physical universe suggests that per-
haps some of the same tools may be useful for theoretically understanding deep neural
networks.
To motivate this connection, let’s very brieﬂy reﬂect on the successes of
thermodynamics and statistical mechanics, physical theories that together explain from
microscopic ﬁrst principles the macroscopic behavior of systems with many elementary
constituents.
A scientiﬁc consequence of the Industrial Age, thermodynamics arose out of an
eﬀort to describe and innovate upon the steam engine – a system consisting of many many
particles and perhaps the original black box. The laws of thermodynamics, derived from
careful empirical observations, were used to codify the mechanics of steam, providing a
high-level understanding of these macroscopic artiﬁcial machines that were transforming
society. While the advent of thermodynamics led to tremendous improvements in the
2

eﬃciency of steam power, its laws were in no way fundamental.
It wasn’t until much later that Maxwell, Boltzmann, and Gibbs provided the missing
link between experimentally-derived eﬀective description on the one hand and a ﬁrst-
principles theory on the other hand. Their statistical mechanics explains how the
macroscopic laws of thermodynamics describing human-scale machines could arise sta-
tistically from the deterministic dynamics of many microscopic elementary constituents.
From this perspective, the laws of thermodynamics were emergent phenomena that only
appear from the collective statistical behavior of a very large number of microscopic
particles.
In fact, it was the detailed theoretical predictions derived from statistical
mechanics that ultimately led to the general scientiﬁc acceptance that matter is really
comprised of molecules and atoms. Relentless application of statistical mechanics led
to the discovery of quantum mechanics, which is a precursor to the invention of the
transistor that powers the Information Age, and – taking the long view – is what has
allowed us to begin to realize artiﬁcial machines that can think intelligently.
Notably, these physical theories originated from a desire to understand artiﬁcial
human-engineered objects, such as the steam engine. Despite a potential misconception,
physics doesn’t make a distinction between natural and artiﬁcial phenomena.
Most
fundamentally, it’s concerned with providing a uniﬁed set of principles that account
for past empirical observations and predict the result of future experiments; the point
of theoretical calculations is to connect measurable outcomes or observables directly
to the fundamental underlying constants or parameters that deﬁne the theory. This
perspective also implies a tradeoﬀbetween the predictive accuracy of a model and its
mathematical tractability, and the former must take precedence over the latter for any
theory to be successful: a short tether from theory to physical reality is essential. When
successful, such theories provide a comprehensive understanding of phenomena and em-
power practical advances in technology, as exempliﬁed by the statistical-physics bridge
from the Age of Steam to the Age of Information.
For our study of deep learning, the key takeaway from this discussion is that a theo-
retical matter simpliﬁes when it is made up of many elementary constituents. Moreover,
unlike the molecules of water contained in a box of steam – with their existence once
being a controversial conjecture in need of experimental veriﬁcation – the neurons com-
prising a deep neural network are put in (the box) by hand. Indeed, in this case we
already understand the microscopic laws – how a network computes – and so instead
our task is to understand the new types of regularity that appear at the macroscopic
scale – why it computes one particular function rather than another – that emerge from
the statistical properties of these gigantic deep learning models.
3

Figure 1: A graph of a simple multilayer neural network, depicting how the input x is
transformed through a sequence of intermediate signals, s(1), s(2), and s(3), into the out-
put f(x; θ). The white circles represent the neurons, the black dot at the top represents
the network output, and the parameters θ are implicit; they weight the importance of
the diﬀerent arrows carrying the signals and bias the ﬁring threshold of each neuron.
0.2
The Theoretical Minimum
The method is more important than the discovery, because the correct method of
research will lead to new, even more valuable discoveries.
Lev Landau [4].
In this section, we’ll give a high-level overview of our method, providing a minimal
explanation for why we should expect a ﬁrst-principles theoretical understanding of deep
neural networks to be possible. We’ll then ﬁll in all the details in the coming chapters.
In essence, a neural network is a recipe for computing a function built out of many
computational units called neurons. Each neuron is itself a very simple function that
considers a weighted sum of incoming signals and then ﬁres in a characteristic way by
comparing the value of that sum against some threshold. Neurons are then organized
in parallel into layers, and deep neural networks are those composed of multiple layers
in sequence. The network is parametrized by the ﬁring thresholds and the weighted
connections between the neurons, and, to give a sense of the potential scale, current
state-of-the-art neural networks can have over 100 billion parameters. A graph depicting
the structure of a much more reasonably-sized neural network is shown in Figure 1.
For a moment, let’s ignore all that structure and simply think of a neural network
4

as a parameterized function
f(x; θ) ,
(0.1)
where x is the input to the function and θ is a vector of a large number of parameters
controlling the shape of the function.
For such a function to be useful, we need to
somehow tune the high-dimensional parameter vector θ. In practice, this is done in two
steps:
• First, we initialize the network by randomly sampling the parameter vector θ from
a computationally simple probability distribution,
p(θ) .
(0.2)
We’ll later discuss the theoretical reason why it is a good strategy to have an
initialization distribution p(θ) but, more importantly, this corresponds to what
is done in practice, and our approach in this book is to have our theoretical analysis
correspond to realistic deep learning scenarios.
• Second, we adjust the parameter vector as θ →θ⋆, such that the resulting network
function f(x; θ⋆) is as close as possible to a desired target function f(x):
f(x; θ⋆) ≈f(x) .
(0.3)
This is called function approximation.
To ﬁnd these tunings θ⋆, we ﬁt the
network function f(x; θ) to training data, consisting of many pairs of the form
 x, f(x)
 observed from the desired – but only partially observable – target function
f(x). Overall, making these adjustments to the parameters is called training, and
the particular procedure used to tune them is called a learning algorithm.
Our goal is to understand this trained network function:
f(x; θ⋆) .
(0.4)
In particular, we’d like to understand the macroscopic behavior of this function from
a ﬁrst-principles microscopic description of the network in terms of these trained pa-
rameters θ⋆. We’d also like to understand how the function approximation (0.3) works
and evaluate how f(x; θ⋆) uses the training data
 x, f(x)
 in its approximation of f(x).
Given the high dimensionality of the parameters θ and the degree of ﬁne-tuning required
for the approximation (0.3), this goal might seem naive and beyond the reach of any
realistic theoretical approach.
One way to more directly see the kinds of technical problems that we’ll encounter is
to Taylor expand our trained network function f(x; θ⋆) around the initialized value of
the parameters θ. Being schematic and ignoring for a moment that θ is a vector and
that the derivatives of f(x; θ) are tensors, we see
f(x; θ⋆) =f(x; θ) + (θ⋆−θ) df
dθ + 1
2 (θ⋆−θ)2 d2f
dθ2 + . . . ,
(0.5)
where f(x; θ) and its derivatives on the right-hand side are all evaluated at initialized
value of the parameters. This Taylor representation illustrates our three main problems:
5

Problem 1
In general, the series (0.5) contains an inﬁnite number of terms
f ,
df
dθ ,
d2f
dθ2 ,
d3f
dθ3 ,
d4f
dθ4 ,
. . . ,
(0.6)
and to use this Taylor representation of the function (0.5), in principle we need
to compute them all. More speciﬁcally, as the diﬀerence between the trained and
initialized parameters, (θ⋆−θ), becomes large, so too does the number of terms
needed to get a good approximation of the trained network function f(x; θ⋆).
Problem 2
Since the parameters θ are randomly sampled from the initialization distribution,
p(θ), each time we initialize our network we get a diﬀerent function f(x; θ). This
means that each term f, df/dθ, d2f/dθ2, . . . , from (0.6) is really a random function
of the input x. Thus, the initialization induces a distribution over the network
function and its derivatives, and we need to determine the mapping,
p(θ) →p
 
f, df
dθ, d2f
dθ2 , . . .
!
,
(0.7)
that takes us from the distribution of initial parameters θ to the joint distribution
of the network function, f(x; θ), its gradient, df/dθ, its Hessian, d2f/dθ2, and so
on. This is a joint distribution comprised of an inﬁnite number of random functions,
and in general such functions will have an intricate statistical dependence. Even if
we set aside this inﬁnity of functions for a moment and consider just the marginal
distribution of the network function only, p(f), there’s still no reason to expect
that it’s analytically tractable.
Problem 3
The learned value of the parameters, θ⋆, is the result of a complicated training
process. In general, θ⋆is not unique and can depend on everything:
θ⋆≡[θ⋆]
 
θ, f, df
dθ, d2f
dθ2 , . . . ; learning algorithm; training data
!
.
(0.8)
In practice, the learning algorithm is iterative, accumulating changes over many
steps, and the dynamics are nonlinear. Thus, the trained parameters θ⋆will depend
in a very complicated way on all the quantities at initialization – such as the
speciﬁc random sample of the parameters θ, the network function f(x; θ) and all
of its derivatives, df/dθ, d2f/dθ2, . . .
– as well as on the details of the learning
algorithm and also on the particular pairs,
 x, f(x)
, that comprise the training
data. Determining an analytical expression for θ⋆must involve taking all of this
into account.
6

If we could solve all three of these problems, then we could in principle use the Taylor-
series representation (0.5) to study the trained network function. More speciﬁcally, we’d
ﬁnd a distribution over trained network functions
p(f⋆) ≡p

f(x; θ⋆)
 learning algorithm; training data

,
(0.9)
now conditioned in a simple way on the learning algorithm and the data we used for
training. Here, by simple we mean that it is easy to evaluate this distribution for diﬀerent
algorithms or choices of training data without having to solve a version of Problem 3
each time. The development of a method for the analytical computation of (0.9) is a
principle goal of this book.
Of course, solving our three problems for a general parameterized function f(x; θ) is
not tractable. However, we are not trying to solve these problems in general; we only
care about the functions that are deep neural networks. Necessarily, any solution to the
above problems will thus have to make use of the particular structure of neural-network
function. While speciﬁcs of how this works form the basis of the book, in the rest of this
section we’ll try to give intuition for how these complications can be resolved.
A Principle of Sparsity
To elaborate on the structure of neural networks, please scroll back a bit and look at
Figure 1. Note that for the network depicted in this ﬁgure, each intermediate or hidden
layer consists of ﬁve neurons, and the input x passes through three such hidden layers
before the output is produced at the top after the ﬁnal layer. In general, two essential
aspects of a neural network architecture are its width, n, and its depth, L.
As we foreshadowed in §0.1, there are often simpliﬁcations to be found in the limit
of a large number of components. However, it’s not enough to consider any massive
macroscopic system, and taking the right limit often requires some care. Regarding the
neurons as the components of the network, there are essentially two primal ways that we
can make a network grow in size: we can increase its width n holding its depth L ﬁxed,
or we can increase its depth L holding its width n ﬁxed. In this case, it will actually
turn out that the former limit will make everything really simple, while the latter limit
will be hopelessly complicated and useless in practice.
So let’s begin by formally taking the limit
lim
n→∞p(f⋆) ,
(0.10)
and studying an idealized neural network in this limit. This is known as the inﬁnite-
width limit of the network, and as a strict limit it’s rather unphysical for a network:
obviously you cannot directly program a function to have an inﬁnite number of com-
ponents on a ﬁnite computer. However, this extreme limit does massively simplify the
distribution over trained networks p(f⋆), rendering each of our three problems completely
benign:
7

• Addressing Problem 1, all the higher derivative terms dkf/dθk for k ≥2 will
eﬀectively vanish, meaning we only need to keep track of two terms,
f ,
df
dθ .
(0.11)
• Addressing Problem 2, the distributions of these random functions will be inde-
pendent,
lim
n→∞p
 
f, df
dθ, d2f
dθ2 , . . .
!
= p(f) p
df
dθ

,
(0.12)
with each marginal distribution factor taking a very simple form.
• Addressing Problem 3, the training dynamics become linear and completely in-
dependent of the details of the learning algorithm, letting us ﬁnd a complete ana-
lytical solution for θ⋆in a closed form
lim
n→∞θ⋆= [θ⋆]

θ, f, df
dθ; training data

.
(0.13)
As a result, the trained distribution (0.10) is a simple Gaussian distribution with a
nonzero mean, and we can easily analyze the functions that such networks are computing.
These simpliﬁcations are the consequence of a principle of sparsity. Even though
it seems like we’ve made the network more complicated by growing it to have an inﬁnite
number of components, from the perspective of any particular neuron the input of an
inﬁnite number of signals is such that the leveling law of large numbers completely
obscures much of the details in the signals. The result is that the eﬀective theory of many
such inﬁnite-width networks leads to extreme sparsity in their description, e.g. enabling
the truncation (0.11).
Unfortunately, the formal inﬁnite-width limit, n →∞, leads to a poor model of
deep neural networks: not only is inﬁnite width an unphysical property for a network to
possess, but the resulting trained distribution (0.10) also leads to a mismatch between
theoretical description and practical observation for networks of more than one layer. In
particular, it’s empirically known that the distribution over such trained networks does
depend on the properties of the learning algorithm used to train them. Additionally, we
will show in detail that such inﬁnite-width networks cannot learn representations of their
inputs: for any input x, its transformations in the hidden layers, s(1), s(2), . . . , will re-
main unchanged from initialization, leading to random representations and thus severely
restricting the class of functions that such networks are capable of learning. Since non-
trivial representation learning is an empirically demonstrated essential property of
multilayer networks, this really underscores the breakdown of the correspondence be-
tween theory and reality in this strict inﬁnite-width limit.
From the theoretical perspective, the problem with this limit is the washing out
of the ﬁne details at each neuron due to the consideration of an inﬁnite number of
incoming signals.
In particular, such an inﬁnite accumulation completely eliminates
8

the subtle correlations between neurons that get ampliﬁed over the course of training
for representation learning. To make progress, we’ll need to ﬁnd a way to restore and
then study the interactions between neurons that are present in realistic ﬁnite-width
networks.
With that in mind, perhaps the inﬁnite-width limit can be corrected in a way such
that the corrections become small when the width n is large. To do so, we can use
perturbation theory – just as we do in physics to analyze interacting systems – and
study deep learning using a 1/n expansion, treating the inverse layer width, ϵ ≡1/n,
as our small parameter of expansion: ϵ ≪1. In other words, we’re going to back oﬀthe
strict inﬁnite-width limit and compute the trained distribution (0.9) with the following
expansion:
p(f⋆) ≡p{0}(f⋆) + p{1}(f⋆)
n
+ p{2}(f⋆)
n2
+ . . . ,
(0.14)
where p{0}(f⋆) ≡limn→∞p(f⋆) is the inﬁnite-width limit we discussed above, (0.10),
and the p{k}(f⋆) for k ≥1 give a series of corrections to this limit.
In this book, we’ll in particular compute the ﬁrst such correction, truncating the
expansion as
p(f⋆) ≡p{0}(f⋆) + p{1}(f⋆)
n
+ O
 1
n2

.
(0.15)
This interacting theory is still simple enough to make our three problems tractable:
• Addressing Problem 1, now all the higher derivative terms dkf/dθk for k ≥4 will
eﬀectively give contributions of the order 1/n2 or smaller, meaning that to capture
the leading contributions of order 1/n, we only need to keep track of four terms:
f ,
df
dθ ,
d2f
dθ2 ,
d3f
dθ3 .
(0.16)
Thus, we see that the principle of sparsity will still limit the dual eﬀective theory
description, though not quite as extensively as in the inﬁnite-width limit.
• Addressing Problem 2, the distribution of these random functions at initializa-
tion,
p
 
f, df
dθ, d2f
dθ2 , d3f
dθ3
!
,
(0.17)
will be nearly simple at order 1/n, and we’ll be able to work it out in full detail
using perturbation theory.
• Addressing Problem 3, we’ll be able to use a dynamical perturbation theory to
tame the nonlinear training dynamics and ﬁnd an analytic solution for θ⋆in a
closed form:
θ⋆= [θ⋆]
 
θ, f, df
dθ , d2f
dθ2
d3f
dθ3 ; learning algorithm; training data
!
.
(0.18)
9

In particular, this will make the dependence of the solution on the details of the
learning algorithm transparent and manifest.
As a result, our description of the trained distribution at order 1/n, (0.15), will be a
nearly-Gaussian distribution.
In addition to being analytically tractable, this truncated description at order 1/n
will satisfy our goal of computing and understanding the distribution over trained net-
work functions p(f⋆). As a consequence of incorporating the interactions between neu-
rons, this description has a dependence on the details of the learning algorithm and, as
we’ll see, includes nontrivial representation learning. Thus, qualitatively, this eﬀective
theory at order 1/n corresponds much more closely to realistic neural networks than the
inﬁnite-width description, making it far more useful as a theoretically minimal model
for understanding deep learning.
How about the quantitative correspondence? As there is a sequence of ﬁner descrip-
tions that we can get by computing higher-order terms in the expansion (0.14), do these
terms also need to be included?
While the formalism we introduce in the book makes computing these additional
terms in the 1/n expansion completely systematic – though perhaps somewhat tedious
– an important byproduct of studying the leading correction is actually a deeper under-
standing of this truncation error. In particular, what we’ll ﬁnd is that the correct scale
to compare with width n is the depth L. That is, we’ll see that the relative magnitudes
of the terms in the expansion (0.14) are given by the depth-to-width aspect ratio:
r ≡L/n .
(0.19)
This lets us recast our understanding of inﬁnite-width vs. ﬁnite-width and shallow
vs. deep in the following way:
• In the strict limit r →0, the interactions between neurons turn oﬀ: the inﬁnite-
width limit (0.10) is actually a decent description. However, these networks are
not really deep, as their relative depth is zero: L/n = 0.
• In the regime 0 < r ≪1, there are nontrivial interactions between neurons: the
ﬁnite-width eﬀective theory truncated at order 1/n, (0.15), gives an accurate ac-
counting of the trained network output. These networks are eﬀectively deep.
• In the regime r ≫1, the neurons are strongly coupled: networks will behave
chaotically, and there is no eﬀective description due to large ﬂuctuations from
instantiation to instantiation. These networks are overly deep.
As such, most networks of practical use actually have reasonably small depth-to-width
ratios, and so our truncated description at order 1/n, (0.15), will provide a great quan-
titative correspondence as well.1
1More precisely, there is an optimal aspect ratio, r⋆, that divides the eﬀective regime r ≤r⋆and the
ineﬀective regime r > r⋆. In Appendix A, we’ll estimate this optimal aspect ratio from an information-
theoretic perspective. In Appendix B, we’ll further show how residual connections can be introduced
to shift the optimal aspect ratio r⋆to larger values, making the formerly overly-deep networks more
practically trainable as well as quantitatively describable by our eﬀective theory approach.
10

From this, we see that to really describe the properties of multilayer neural networks,
i.e. to understand deep learning, we need to study large-but-ﬁnite-width networks. In
this way, we’ll be able to ﬁnd a macroscopic eﬀective theory description of realistic deep
neural networks.
11

12

Chapter 1
Pretraining
My strongest memory of the class is the very beginning, when he started, not with
some deep principle of nature, or some experiment, but with a review of Gaussian
integrals. Clearly, there was some calculating to be done.
Joe Polchinski, reminiscing about Richard Feynman’s quantum mechanics class [5].
The goal of this book is to develop principles that enable a theoretical understanding
of deep learning. Perhaps the most important principle is that wide and deep neural
networks are governed by nearly-Gaussian distributions. Thus, to make it through the
book, you will need to achieve mastery of Gaussian integration and perturbation theory.
Our pretraining in this chapter consists of whirlwind introductions to these toolkits
as well as a brief overview of some key concepts in statistics that we’ll need.
The
only prerequisite is ﬂuency in linear algebra, multivariable calculus, and rudimentary
probability theory.
With that in mind, we begin in §1.1 with an extended discussion of Gaussian inte-
grals. Our emphasis will be on calculational tools for computing averages of monomials
against Gaussian distributions, culminating in a derivation of Wick’s theorem.
Next, in §1.2, we begin by giving a general discussion of expectation values and ob-
servables. Thinking of observables as a way of learning about a probability distribution
through repeated experiments, we’re led to the statistical concepts of moment and cumu-
lant and the corresponding physicists’ concepts of full M-point correlator and connected
M-point correlator. A particular emphasis is placed on the connected correlators as they
directly characterize a distribution’s deviation from Gaussianity.
In §1.3, we introduce the negative log probability or action representation of a prob-
ability distribution and explain how the action lets us systematically deform Gaussian
distributions in order to give a compact representation of non-Gaussian distributions.
In particular, we specialize to nearly-Gaussian distributions, for which deviations from
Gaussianity are implemented by small couplings in the action, and show how perturba-
tion theory can be used to connect the non-Gaussian couplings to observables such as
the connected correlators. By treating such couplings perturbatively, we can transform
13

any correlator of a nearly-Gaussian distribution into a sum of Gaussian integrals; each
integral can then be evaluated by the tools we developed in §1.1. This will be one of our
most important tricks, as the neural networks we’ll study are all governed by nearly-
Gaussian distributions, with non-Gaussian couplings that become perturbatively small
as the networks become wide.
Since all these manipulations need to be on our ﬁngertips, in this ﬁrst chapter we’ve
erred on the side of being verbose – in words and equations and examples – with the
goal of making these materials as transparent and comprehensible as possible.
1.1
Gaussian Integrals
The goal of this section is to introduce Gaussian integrals and Gaussian probability
distributions, and ultimately derive Wick’s theorem (1.45). This theorem provides an
operational formula for computing any moment of a multivariable Gaussian distribution,
and will be used throughout the book.
Single-variable Gaussian integrals
Let’s take it slow and start with the simplest single-variable Gaussian function,
e−z2
2 .
(1.1)
The graph of this function depicts the famous bell curve, symmetric around the peak
at z = 0 and quickly tapering oﬀfor large |z| ≫1. By itself, (1.1) cannot serve as
a probability distribution since it’s not normalized.
In order to ﬁnd out the proper
normalization, we need to perform the Gaussian integral
I1 ≡
Z ∞
−∞
dz e−z2
2 .
(1.2)
As an ancient object, there exists a neat trick to evaluate such an integral. To begin,
consider its square
I2
1 =
Z ∞
−∞
dz e−z2
2
2
=
Z ∞
−∞
dx e−x2
2
Z ∞
−∞
dy e−y2
2 =
Z ∞
−∞
Z ∞
−∞
dxdy e−1
2(x2+y2) , (1.3)
where in the middle we just changed the names of the dummy integration variables. Next,
we change variables to polar coordinates (x, y) = (r cos φ, r sin φ), which transforms the
integral measure as dxdy = rdrdφ and gives us two elementary integrals to compute:
I2
1 =
Z ∞
−∞
Z ∞
−∞
dxdy e−1
2(x2+y2) =
Z ∞
0
rdr
Z 2π
0
dφ e−r2
2
(1.4)
=2π
Z ∞
0
dr re−r2
2 = 2π
−e−r2
2

r=∞
r=0
= 2π .
14

Finally, by taking a square root we can evaluate the Gaussian integral (1.2) as
I1 =
Z ∞
−∞
dz e−z2
2 =
√
2π .
(1.5)
Dividing the Gaussian function with this normalization factor, we deﬁne the Gaussian
probability distribution with unit variance as
p(z) ≡
1
√
2πe−z2
2 ,
(1.6)
which is now properly normalized, i.e.,
R ∞
−∞dz p(z) = 1. Such a distribution with zero
mean and unit variance is sometimes called the standard normal distribution.
Extending this result to a Gaussian distribution with variance K > 0 is super-easy.
The corresponding normalization factor is given by
IK ≡
Z ∞
−∞
dz e−z2
2K =
√
K
Z ∞
−∞
du e−u2
2 =
√
2πK ,
(1.7)
where in the middle we rescaled the integration variable as u = z/
√
K. We can then
deﬁne the Gaussian distribution with variance K as
p(z) ≡
1
√
2πK
e−z2
2K .
(1.8)
The graph of this distribution again depicts a bell curve symmetric around z = 0, but
it’s now equipped with a scale K characterizing its broadness, tapering oﬀfor |z| ≫
√
K.
More generally, we can shift the center of the bell curve as
p(z) ≡
1
√
2πK
e−(z−s)2
2K
,
(1.9)
so that it is now symmetric around z = s. This center value s is called the mean of the
distribution, because it is:
E [z] ≡
Z ∞
−∞
dz p(z) z =
1
√
2πK
Z ∞
−∞
dz e−(z−s)2
2K z
(1.10)
= 1
IK
Z ∞
−∞
dw e−w2
2K (s + w)
=sIK
IK
+ 1
IK
Z ∞
−∞
dw

e−w2
2K w

=s ,
where in the middle we shifted the variable as w = z −s and in the very last step
noticed that the integrand of the second term is odd with respect to the sign ﬂip of the
integration variable w ↔−w and hence integrates to zero.
15

Focusing on Gaussian distributions with zero mean, let’s consider other expectation
values for general functions O(z), i.e.,
E [O(z)] ≡
Z ∞
−∞
dz p(z) O(z) =
1
√
2πK
Z ∞
−∞
dz e−z2
2K O(z) .
(1.11)
We’ll often refer to such functions O(z) as observables, since they can correspond to
measurement outcomes of experiments. A special class of expectation values are called
moments and correspond to the insertion of zM into the integrand for any integer M:
E
h
zMi
=
1
√
2πK
Z ∞
−∞
dz e−z2
2K zM .
(1.12)
Note that the integral vanishes for any odd exponent M, because then the integrand
is odd with respect to the sign ﬂip z ↔−z. As for the even number M = 2m of z
insertions, we will need to evaluate integrals of the form
IK,m ≡
Z ∞
−∞
dz e−z2
2K z2m .
(1.13)
As objects almost as ancient as (1.2), again there exists a neat trick to evaluate them:
IK,m =
Z ∞
−∞
dz e−z2
2K z2m =

2K2 d
dK
m Z ∞
−∞
dz e−z2
2K =

2K2 d
dK
m
IK
(1.14)
=

2K2 d
dK
m √
2πK
1
2 =
√
2πK
2m+1
2
(2m −1)(2m −3) · · · 1 ,
where in going to the second line we substituted in our expression (1.7) for IK. Therefore,
we see that the even moments are given by the simple formula1
E
h
z2mi
= IK,m
√
2πK
= Km (2m −1)!! ,
(1.15)
where we have introduced the double factorial
(2m −1)!! ≡(2m −1)(2m −3) · · · 1 = (2m)!
2mm! .
(1.16)
The result (1.15) is Wick’s theorem for single-variable Gaussian distributions.
There’s actually another nice way to derive (1.15), which can much more naturally
be extended to multivariable Gaussian distributions. This derivation starts with the
consideration of a Gaussian integral with a source term J, which we deﬁne as
ZK,J ≡
Z ∞
−∞
dz e−z2
2K +Jz .
(1.17)
Note that when setting the source to zero we recover the normalization of the Gaussian
integral, giving the relationship ZK,J=0 = IK. In the physics literature ZK,J is sometimes
1This equation with 2m = 2 makes clear why we called K the variance, since for zero-mean Gaussian
distributions with variance K we have var(z) ≡E 
(z −E [z])2
= E 
z2
−E [z]2 = E 
z2
= K.
16

called a partition function with source and, as we will soon see, this integral serves
as a generating function for the moments. We can evaluate ZK,J by completing the
square in the exponent
−z2
2K + Jz = −(z −JK)2
2K
+ KJ2
2
,
(1.18)
which lets us rewrite the integral (1.17) as
ZK,J = e
KJ2
2
Z ∞
−∞
dz e−(z−JK)2
2K
= e
KJ2
2 IK = e
KJ2
2 √
2πK ,
(1.19)
where in the middle equality we noticed that the integrand is just a shifted Gaussian
function with variance K.
We can now relate the Gaussian integral with a source ZK,J to the Gaussian integral
with insertions IK,m. By diﬀerentiating ZK,J with respect to the source J and then
setting the source to zero, we observe that
IK,m =
Z ∞
−∞
dz e−z2
2K z2m =
" d
dJ
2m Z ∞
−∞
dz e−z2
2K +Jz
# 
J=0
=
" d
dJ
2m
ZK,J
# 
J=0
.
(1.20)
In other words, the integrals IK,m are simply related to the even Taylor coeﬃcients of
the partition function ZK,J around J = 0. For instance, for 2m = 2 we have
E
h
z2i
=
IK,1
√
2πK
=
" d
dJ
2
e
KJ2
2
# 
J=0
=

e
KJ2
2

K + K2J2 
J=0
= K ,
(1.21)
and for 2m = 4 we have
E
h
z4i
=
IK,2
√
2πK
=
" d
dJ
4
e
KJ2
2
# 
J=0
=

e
KJ2
2

3K2 + 6K3J2 + K4J4 
J=0
= 3K2 .
(1.22)
Notice that any terms with dangling sources J vanish upon setting J = 0. This ob-
servation gives a simple way to evaluate correlators for general m: Taylor-expand the
exponential ZK,J/IK = exp

KJ2
2

and keep the term with the right amount of sources
such that the expression doesn’t vanish. Doing exactly that, we get
E
h
z2mi
= IK,m
√
2πK
=
" d
dJ
2m
e
KJ2
2
# 
J=0
=
( d
dJ
2m " ∞
X
k=0
1
k!
K
2
k
J2k
#) 
J=0
(1.23)
=
 d
dJ
2m  1
m!
K
2
m
J2m

= Km (2m)!
2mm! = Km(2m −1)!! ,
which completes our second derivation of Wick’s theorem (1.15) for the single-variable
Gaussian distribution. This derivation was much longer than the ﬁrst neat derivation,
but can be very naturally extended to the multivariable Gaussian distribution, which
we turn to next.
17

Multivariable Gaussian integrals
Picking up speed, we are now ready to handle multivariable Gaussian integrals for an
N-dimensional variable zµ with µ = 1, . . . , N.2 The multivariable Gaussian function is
deﬁned as
exp

−1
2
N
X
µ,ν=1
zµ(K−1)µν zν

,
(1.24)
where the variance or covariance matrix Kµν is an N-by-N symmetric positive deﬁnite
matrix, and its inverse (K−1)µν is deﬁned so that their matrix product gives the N-by-N
identity matrix
N
X
ρ=1
(K−1)µρ Kρν = δµν .
(1.25)
Here we have also introduced the Kronecker delta δµν, which satisﬁes
δµν ≡
(
1 ,
µ = ν ,
0 ,
µ ̸= ν .
(1.26)
The Kronecker delta is just a convenient representation of the identity matrix.
Now, to construct a probability distribution from the Gaussian function (1.24), we
again need to evaluate the normalization factor
IK ≡
Z
dNz exp

−1
2
N
X
µ,ν=1
zµ(K−1)µν zν


(1.27)
=
Z ∞
−∞
dz1
Z ∞
−∞
dz2 · · ·
Z ∞
−∞
dzN exp

−1
2
N
X
µ,ν=1
zµ(K−1)µν zν

.
To compute this integral, ﬁrst recall from linear algebra that, given an N-by-N sym-
metric matrix Kµν, there is always an orthogonal matrix3 Oµν that diagonalizes Kµν as
(OKOT )µν = λµδµν with eigenvalues λµ=1,...,N and diagonalizes its inverse as (OK−1OT )µν =
(1/λµ) δµν.
With this in mind, after twice inserting the identity matrix as δµν =
(OT O)µν, the sum in the exponent of the integral can be expressed in terms of the
2Throughout this book, we will explicitly write out the component indices of vectors, matrices, and
tensors as much as possible, except on some occasions when it is clear enough from context.
3An orthogonal matrix Oµν is a matrix whose transpose  OT
µν equals its inverse, i.e., (OT O)µν =
δµν.
18

eigenvalues as
N
X
µ,ν=1
zµ(K−1)µνzν =
N
X
µ,ρ,σ,ν=1
zµ (OT O)µρ(K−1)ρσ(OT O)σν zν
(1.28)
=
N
X
µ,ν=1
(Oz)µ(OK−1OT )µν(Oz)ν
=
N
X
µ=1
1
λµ
(Oz)2
µ ,
where to reach the ﬁnal line we used the diagonalization property of the inverse covari-
ance matrix. Remembering that for a positive deﬁnite matrix Kµν the eigenvalues are all
positive λµ > 0, we see that the λµ sets the scale of the falloﬀof the Gaussian function
in each of the eigendirections. Next, recall from multivariable calculus that a change
of variables uµ ≡(Oz)µ with an orthogonal matrix O leaves the integration measure
invariant, i.e., dNz = dNu. All together, this lets us factorize the multivariable Gaussian
integral (1.27) into a product of single-variable Gaussian integrals (1.7), yielding
IK =
Z ∞
−∞
du1
Z ∞
−∞
du2 · · ·
Z ∞
−∞
duN exp
 
−u2
1
2λ1
−u2
2
2λ2
−. . . −u2
N
2λN
!
(1.29)
=
N
Y
µ=1
"Z ∞
−∞
duµ exp
 
−u2
µ
2λµ
!#
=
N
Y
µ=1
q
2πλµ =
v
u
u
t
N
Y
µ=1
(2πλµ) .
Finally, recall one last fact from linear algebra that the product of the eigenvalues of a
matrix is equal to the matrix determinant. Thus, compactly, we can express the value
of the multivariable Gaussian integral as
IK =
Z
dNz exp

−1
2
N
X
µ,ν=1
zµ(K−1)µνzν

=
q
|2πK| ,
(1.30)
where |A| denotes the determinant of a square matrix A.
Having ﬁgured out the normalization factor, we can deﬁne the zero-mean multivari-
able Gaussian probability distribution with variance Kµν as
p(z) =
1
p
|2πK| exp

−1
2
N
X
µ,ν=1
zµ(K−1)µν zν

.
(1.31)
While we’re at it, let us also introduce the conventions of suppressing the superscript
“−1” for the inverse covariance (K−1)µν, instead placing the component indices upstairs
as
Kµν ≡(K−1)µν .
(1.32)
This way, we distinguish the covariance Kµν and the inverse covariance Kµν by whether
or not component indices are lowered or raised.
With this notation, inherited from
19

general relativity, the deﬁning equation for the inverse covariance (1.25) is written instead
as
N
X
ρ=1
KµρKρν = δµ
ν ,
(1.33)
and the multivariable Gaussian distribution (1.31) is written as
p(z) =
1
p
|2πK| exp

−1
2
N
X
µ,ν=1
zµKµνzν

.
(1.34)
Although it might take some getting used to, this notation saves us some space and saves
you some handwriting pain.4 Regardless of how it’s written, the zero-mean multivariable
Gaussian probability distribution (1.34) peaks at z = 0, and its falloﬀis direction-
dependent, determined by the covariance matrix Kµν. More generally, we can shift the
peak of the Gaussian distribution to sµ
p(z) =
1
p
|2πK| exp

−1
2
N
X
µ,ν=1
(z −s)µ Kµν (z −s)ν

,
(1.35)
which deﬁnes a general multivariable Gaussian distribution with mean E [zµ] = sµ and
covariance Kµν. This is the most general version of the Gaussian distribution.
Next, let’s consider the moments of the mean-zero multivariable Gaussian distribu-
tion
E [zµ1 · · · zµM ] ≡
Z
dNz p(z) zµ1 · · · zµM
(1.36)
=
1
p
|2πK|
Z
dNz exp

−1
2
N
X
µ,ν=1
zµKµνzν

zµ1 · · · zµM = IK,(µ1,...,µM)
IK
,
where we introduced multivariable Gaussian integrals with insertions
IK,(µ1,...,µM) ≡
Z
dNz exp

−1
2
N
X
µ,ν=1
zµKµνzν

zµ1 · · · zµM .
(1.37)
Following our approach in the single-variable case, let’s construct the generating function
for the integrals IK,(µ1,...,µM) by including a source term Jµ as
ZK,J ≡
Z
dNz exp

−1
2
N
X
µ,ν=1
zµKµνzν +
N
X
µ=1
Jµzµ

.
(1.38)
4If you like, in your notes you can also go full general-relativistic mode and adopt Einstein summation
convention, suppressing the summation symbol any time indices are repeated in upstair-downstair pairs.
For instance, if we adopted this convention we would write the deﬁning equation for inverse simply as
KµρKρν = δµ
ν and the Gaussian function as exp −1
2zµKµνzν

.
Speciﬁcally for neural networks, you might ﬁnd the Einstein summation convention helpful for sample
indices, but sometimes confusing for neural indices. For extra clarity, we won’t adopt this convention in
the text of the book, but we mention it now since we do often use such a convention to simplify our own
calculations in private.
20

As the name suggests, diﬀerentiating the generating function ZK,J with respect to the
source Jµ brings down a power of zµ such that after M such diﬀerentiations we have

d
dJµ1
d
dJµ2 · · ·
d
dJµM ZK,J
 
J=0
(1.39)
=
Z
dNz exp

−1
2
N
X
µ,ν=1
zµKµνzν

zµ1 · · · zµM = IK,(µ1,...,µM) .
So, as in the single-variable case, the Taylor coeﬃcients of the partition function ZK,J
expanded around Jµ = 0 are simply related to the integrals with insertions IK,(µ1,...,µM).
Therefore, if we knew a closed-form expression for ZK,J, we could easily compute the
values of the integrals IK,(µ1,...,µM).
To evaluate the generating function ZK,J in a closed form, again we follow the lead
of the single-variable case and complete the square in the exponent of the integrand in
(1.38) as
−1
2
N
X
µ,ν=1
zµKµνzν +
N
X
µ=1
Jµzµ
(1.40)
= −1
2
N
X
µ,ν=1

zµ −
N
X
ρ=1
KµρJρ

Kµν
 
zν −
N
X
λ=1
KνλJλ
!
+ 1
2
N
X
µ,ν=1
JµKµνJν
= −1
2
N
X
µ,ν=1
wµKµνwν + 1
2
N
X
µ,ν=1
JµKµνJν ,
where we have introduced the shifted variable wµ ≡zµ −PN
ρ=1 KµρJρ.
Using this
substitution, the generating function can be evaluated explicitly
ZK,J = exp

1
2
N
X
µ,ν=1
JµKµνJν


Z
dNw exp

−1
2
N
X
µ,ν=1
wµKµνwν


(1.41)
=
q
|2πK| exp

1
2
N
X
µ,ν=1
JµKµνJν

,
where at the end we used our formula for the multivariable integral IK, (1.30). With
our closed-form expression (1.41) for the generating function ZK,J, we can compute the
Gaussian integrals with insertions IK,(µ1,...,µM) by diﬀerentiating it, using (1.39). For an
even number M = 2m of insertions, we ﬁnd a really nice formula
E [zµ1 · · · zµ2m] =IK,(µ1,...,µ2m)
IK
= 1
IK

d
dJµ1 · · ·
d
dJµ2m ZK,J
 
J=0
(1.42)
=
1
2mm!
d
dJµ1
d
dJµ2 · · ·
d
dJµ2m


N
X
µ,ν=1
JµKµνJν


m
.
21

For an odd number M = 2m + 1 of insertions, there is dangling source upon setting
J = 0, and so those integrals vanish. You can also see this by looking at the integrand
for any odd moment and noticing that it is odd with respect to the sign ﬂip of the
integration variables zµ ↔−zµ.
Now, let’s take a few moments to evaluate a few moments using this formula. For
2m = 2, we have
E [zµ1zµ2] = 1
2
d
dJµ1
d
dJµ2


N
X
µ,ν=1
JµKµνJν

= Kµ1µ2.
(1.43)
Here, there are 2! = 2 ways to apply the product rule for derivatives and diﬀerentiate
the two J’s, both of which evaluate to the same expression due to the symmetry of the
covariance, Kµ1µ2 = Kµ2µ1. This expression (1.43) validates in the multivariable setting
why we have been calling Kµν the covariance, because we see explicitly that it is the
covariance.
Next, for 2m = 4 we get a more complicated expression
E [zµ1zµ2zµ3zµ4] = 1
222!
d
dJµ1
d
dJµ2
d
dJµ3
d
dJµ4


N
X
µ,ν=1
JµKµνJν




N
X
ρ,λ=1
JρKρλJλ


=Kµ1µ2Kµ3µ4 + Kµ1µ3Kµ2µ4 + Kµ1µ4Kµ2µ3 .
(1.44)
Here we note that there are now 4! = 24 ways to diﬀerentiate the four J’s, though only
three distinct ways to pair the four auxiliary indices 1, 2, 3, 4 that sit under µ. This gives
24/3 = 8 = 222! equivalent terms for each of the three pairings, which cancels against
the overall factor 1/(222!).
For general 2m, there are (2m)! ways to diﬀerentiate the sources, of which 2mm!
of those ways are equivalent.
This gives (2m)!/(2mm!) = (2m −1)!! distinct terms,
corresponding to the (2m −1)!! distinct pairings of 2m auxiliary indices 1, . . . , 2m that
sit under µ.
The factor of 1/(2mm!) in the denominator of (1.42) ensures that the
coeﬃcient of each of these terms is normalized to unity. Thus, most generally, we can
express the moments of the multivariable Gaussian with the following formula
E [zµ1 · · · zµ2m] =
X
all pairing
Kµk1µk2 · · · Kµk2m−1µk2m ,
(1.45)
where, to reiterate, the sum is over all the possible distinct pairings of the 2m auxiliary
indices under µ such that the result has the (2m −1)!! terms that we described above.
Each factor of the covariance Kµν in a term in sum is called a Wick contraction,
corresponding to a particular pairing of auxiliary indices. Each term then is composed of
m diﬀerent Wick contractions, representing a distinct way of pairing up all the auxiliary
indices. To make sure you understand how this pairing works, look back at the 2m = 2
case (1.43) – with a single Wick contraction – and the 2m = 4 case (1.44) – with three
distinct ways of making two Wick contractions – and try to work out the 2m = 6 case,
22

which yields (6 −1)!! = 15 distinct ways of making three Wick contractions:
E [zµ1zµ2zµ3zµ4zµ5zµ6] =Kµ1µ2Kµ3µ4Kµ5µ6 + Kµ1µ3Kµ2µ4Kµ5µ6 + Kµ1µ4Kµ2µ3Kµ5µ6
+Kµ1µ2Kµ3µ5Kµ4µ6 + Kµ1µ3Kµ2µ5Kµ4µ6 + Kµ1µ5Kµ2µ3Kµ4µ6
+Kµ1µ2Kµ5µ4Kµ3µ6 + Kµ1µ5Kµ2µ4Kµ3µ6 + Kµ1µ4Kµ2µ5Kµ3µ6
+Kµ1µ5Kµ3µ4Kµ2µ6 + Kµ1µ3Kµ5µ4Kµ2µ6 + Kµ1µ4Kµ5µ3Kµ2µ6
+Kµ5µ2Kµ3µ4Kµ1µ6 + Kµ5µ3Kµ2µ4Kµ1µ6 + Kµ5µ4Kµ2µ3Kµ1µ6 .
The formula (1.45) is Wick’s theorem. Put a box around it. Take a few moments
for reﬂection.
. . .
. . .
. . .
Good. You are now a Gaussian sensei. Exhale, and then say as Neo would say, “I know
Gaussian integrals.”
Now that the moments have passed, it is an appropriate time to transition to the
next section where you will learn about more general probability distributions.
1.2
Probability, Correlation and Statistics, and All That
In introducing the Gaussian distribution in the last section we brieﬂy touched upon the
concepts of expectation and moments. These are deﬁned for non-Gaussian probability
distributions too, so now let us reintroduce these concepts and expand on their deﬁni-
tions, with an eye towards understanding the nearly-Gaussian distributions that describe
wide neural networks.
Given a probability distribution p(z) of an N-dimensional random variable zµ, we
can learn about its statistics by measuring functions of zµ. We’ll refer to such measurable
functions in a generic sense as observables and denote them as O(z). The expectation
value of an observable
E [O(z)] ≡
Z
dNz p(z) O(z)
(1.46)
characterizes the mean value of the random function O(z). Note that the observable
O(z) needs not be a scalar-valued function, e.g. the second moment of a distribution is
a matrix-valued observable given by O(z) = zµzν.
Operationally, an observable is a quantity that we measure by conducting exper-
iments in order to connect to a theoretical model for the underlying probability dis-
tribution describing zµ. In particular, we repeatedly measure the observables that are
naturally accessible to us as experimenters, collect their statistics, and then compare
them with predictions for the expectation values of those observables computed from
some theoretical model of p(z).
With that in mind, it’s very natural to ask: what kind of information can we learn
about an underlying distribution p(z) by measuring an observable O(z)?
For an a
23

priori unknown distribution, is there a set of observables that can serve as a suﬃcient
probe of p(z) such that we could use that information to predict the result of all future
experiments involving zµ?
Consider a class of observables that we’ve already encountered, the moments or
M-point correlators of zµ, given by the expectation5
E [zµ1zµ2 · · · zµM ] =
Z
dNz p(z) zµ1zµ2 · · · zµM .
(1.47)
In principle, knowing the M-point correlators of a distribution lets us compute the
expectation value of any analytic observable O(z) via Taylor expansion
E [O(z)] =E


∞
X
M=0
1
M!
N
X
µ1,...,µM=1
∂MO
∂zµ1 · · · ∂zµM

z=0
zµ1zµ2 · · · zµM


(1.48)
=
∞
X
M=0
1
M!
N
X
µ1,...,µM=1
∂MO
∂zµ1 · · · ∂zµM

z=0
E [zµ1zµ2 · · · zµM ] ,
where on the last line we took the Taylor coeﬃcients out of the expectation by using
the linearity property of the expectation, inherited from the linearity property of the
integral in (1.46). As such, it’s clear that the collection of all the M-point correlators
completely characterizes a probability distribution for all intents and purposes.6
However, this description in terms of all the correlators is somewhat cumbersome and
operationally infeasible. To get a reliable estimate of the M-point correlator, we must
simultaneously measure M components of a random variable for each draw and repeat
such measurements many times. As M grows, this task quickly becomes impractical. In
fact, if we could easily perform such measurements for all M, then our theoretical model
of p(z) would no longer be a useful abstraction; from (1.48) we would already know the
outcome of all possible experiments that we could perform, leaving nothing for us to
predict.
To that point, essentially all useful distributions can be eﬀectively described in terms
of a ﬁnite number of quantities, giving them a parsimonious representation. For instance,
consider the zero-mean n-dimensional Gaussian distribution with the variance Kµν. The
5In the rest of this book, we’ll often use the physics term M-point correlator rather than the statistics
term moment, though they mean the same thing and can be used interchangeably.
6In fact, the moments oﬀer a dual description of the probability distribution through either the
Laplace transform or the Fourier transform.
For instance, the Laplace transform of the probability
distribution p(z) is given by
ZJ ≡E
"
exp
 X
µ
Jµzµ
!#
=
Z "Y
µ
dzµ
#
p(z) exp
 X
µ
Jµzµ
!
.
(1.49)
As in the Gaussian case, this integral gives a generating function for the M-point correlators of p(z),
which means that ZJ can be reconstructed from these correlators. The probability distribution can then
be obtained through the inverse Laplace transform.
24

nonzero 2m-point correlators are given by Wick’s theorem (1.45) as
E [zµ1zµ2 · · · zµ2m] =
X
all pairing
Kµk1µk2 · · · Kµk2m−1µk2m ,
(1.50)
and are determined entirely by the N(N +1)/2 independent components of the variance
Kµν. The variance itself can be estimated by measuring the two-point correlator
E [zµzν] = Kµν .
(1.51)
This is consistent with our description of the distribution itself as “the zero-mean N-
dimensional Gaussian distribution with the variance Kµν” in which we only had to
specify these same set of numbers, Kµν, to pick out the particular distribution we had
in mind. For zero-mean Gaussian distributions, there’s no reason to measure or keep
track of any of the higher-point correlators as they are completely constrained by the
variance through (1.50).
More generally, it would be nice if there were a systematic way for learning about
non-Gaussian probability distributions without performing an inﬁnite number of exper-
iments. For nearly-Gaussian distributions, a useful set of observables is given by what
statisticians call cumulants and physicists call connected correlators.7 As the for-
mal deﬁnition of these quantities is somewhat cumbersome and unintuitive, let’s start
with a few simple examples.
The ﬁrst cumulant or the connected one-point correlator is the same as the full
one-point correlator
E [zµ]

connected ≡E [zµ] .
(1.52)
This is just the mean of the distribution. The second cumulant or the connected two-
point correlator is given by
E [zµzν]

connected ≡E [zµzν] −E [zµ] E [zν]
(1.53)
=E [(zµ −E [zµ]) (zν −E [zν])] ≡Cov[zµ, zν] ,
which is also known as the covariance of the distribution. Note how the mean is sub-
tracted from the random variable zµ before taking the square in the connected version.
The quantity d
∆zµ ≡zµ −E [zµ] represents a ﬂuctuation of the random variable around
its mean. Intuitively, such ﬂuctuations are equally likely to contribute positively as they
are likely to contribute negatively, E
hd
∆zµ
i
= E [zµ] −E [zµ] = 0, so it’s necessary to
take the square in order to get an estimate of the magnitude of such ﬂuctuations.
At this point, let us restrict our focus to distributions that are invariant under a sign-
ﬂip symmetry zµ →−zµ, which holds for the zero-mean Gaussian distribution (1.34).
Importantly, this parity symmetry will also hold for the nearly-Gaussian distributions
7Outside of this chapter, just as we’ll often use the term M-point correlator rather than the term
moment, we’ll use the term M-point connected correlator rather than the term cumulant. When we
want to refer to the moment and not the cumulant, we might sometimes say full correlator to contrast
with connected correlator.
25

that we will study in order to describe neural networks. For all such even distributions
with this symmetry, all odd moments and all odd-point connected correlators vanish.
With this restriction, the next simplest observable is the fourth cumulant or the
connected four-point correlator, given by the formula
E [zµ1zµ2zµ3zµ4]

connected
(1.54)
=E [zµ1zµ2zµ3zµ4]
−E [zµ1zµ2] E [zµ3zµ4] −E [zµ1zµ3] E [zµ2zµ4] −E [zµ1zµ4] E [zµ2zµ3] .
For the Gaussian distribution, recalling the Wick theorem (1.50), the last three terms
precisely subtract oﬀthe three pairs of Wick contractions used to evaluate the ﬁrst term,
meaning
E [zµ1zµ2zµ3zµ4]

connected = 0.
(1.55)
Essentially by design, the connected four-point correlator vanishes for the Gaussian
distribution, and a nonzero value signiﬁes a deviation from Gaussian statistics.8 In fact,
the connected four-point correlator is perhaps the simplest measure of non-Gaussianity.
Now that we have a little intuition, we are as ready as we’ll ever be to discuss the
deﬁnition for the M-th cumulant or the M-point connected correlator. For complete-
ness, we’ll give the general deﬁnition, before restricting again to distributions that are
symmetric under parity zµ →−zµ. The deﬁnition is inductive and somewhat counter-
intuitive, expressing the M-th moment in terms of connected correlators from degree 1
to M:
E [zµ1zµ2 · · · zµM ]
(1.56)
≡E [zµ1zµ2 · · · zµM ]

connected
+
X
all subdivisions
E
"
zµ
k[1]
1
· · · zµ
k[1]
ν1
# 
connected
· · · E

zµ
k[s]
1
· · · zµ
k[s]
νs
 
connected
,
where the sum is over all the possible subdivisions of M variables into s > 1 clusters of
sizes (ν1, . . . , νs) as (k[1]
1 , . . . , k[1]
ν1 ), . . . , (k[s]
1 , . . . , k[s]
νs ). By decomposing the M-th moment
into a sum of products of connected correlators of degree M and lower, we see that the
connected M-point correlator corresponds to a new type of correlation that cannot be
expressed by the connected correlators of a lower degree. We saw an example of this
above when discussing the connected four-point correlator as a simple measure of non-
Gaussianity.
To see how this abstract deﬁnition actually works, let’s revisit the examples. First,
we trivially recover the relation between the mean and the one-point connected correlator
E [zµ]

connected = E [zµ] ,
(1.57)
8In statistics, the connected four-point correlator for a single random variable z is called the excess
kurtosis when normalized by the square of the variance.
It is a natural measure of the tails of the
distribution, as compared to a Gaussian distribution, and also serves as a measure of the potential for
outliers. In particular, a positive value indicates fatter tails while a negative value indicates thinner tails.
26

as there is no subdivision of a M = 1 variable into any smaller pieces. For M = 2, the
deﬁnition (1.56) gives
E [zµ1zµ2] =E [zµ1zµ2]

connected + E [zµ1]

connectedE [zµ2]

connected
(1.58)
=E [zµ1zµ2]

connected + E [zµ1] E [zµ2] .
Rearranging to solve for the connected two-point function in terms of the moments, we
see that this is equivalent to our previous deﬁnition for the covariance (1.53).
At this point, let us again restrict to parity-symmetric distributions invariant under
zµ →−zµ, remembering that this means that all the odd-point connected correlators
will vanish. For such distributions, evaluating the deﬁnition (1.56) for M = 4 gives
E [zµ1zµ2zµ3zµ4] =E [zµ1zµ2zµ3zµ4]

connected
(1.59)
+ E [zµ1zµ2]

connectedE [zµ3zµ4]

connected
+ E [zµ1zµ3]

connectedE [zµ2zµ4]

connected
+ E [zµ1zµ4]

connectedE [zµ2zµ3]

connected .
Since E [zµ1zµ2] = E [zµ1zµ2]

connected when the mean vanishes, this is also just a rear-
rangement of our previous expression (1.54) for the connected four-point correlator for
such zero-mean distributions.
In order to see something new, let us carry on for M = 6:
E [zµ1zµ2zµ3zµ4zµ5zµ6] =E [zµ1zµ2zµ3zµ4zµ5zµ6]

connected
(1.60)
+ E [zµ1zµ2]

connectedE [zµ3zµ4]

connectedE [zµ5zµ6]

connected
+ [14 other (2, 2, 2) subdivisions]
+ E [zµ1zµ2zµ3zµ4]

connectedE [zµ5zµ6]

connected
+ [14 other (4, 2) subdivisions] ,
in which we have expressed the full six-point correlator in terms of a sum of products
of connected two-point, four-point, and six-point correlators. Rearranging the above
expression and expressing the two-point and four-point connected correlators in terms
of their deﬁnitions, (1.53) and (1.54), we obtain an expression for the connected six-point
correlator:
E [zµ1zµ2zµ3zµ4zµ5zµ6]

connected
(1.61)
=E [zµ1zµ2zµ3zµ4zµ5zµ6]
−{E [zµ1zµ2zµ3zµ4] E [zµ5zµ6] + [14 other (4, 2) subdivisions]}
+ 2 {E [zµ1zµ2] E [zµ3zµ4] E [zµ5zµ6] + [14 other (2, 2, 2) subdivisions]} .
The rearrangement is useful for computational purposes, in that it’s simple to ﬁrst
compute the moments of a distribution and then organize the resulting expressions in
order to evaluate the connected correlators.
27

Focusing back on (1.60), it’s easy to see that the connected six-point correlator van-
ishes for Gaussian distributions. Remembering that the connected four-point correlator
also vanishes for Gaussian distributions, we see that the ﬁfteen (2, 2, 2) subdivision terms
are exactly equal to the ﬁfteen terms generated by the Wick contractions resulting from
evaluating the full correlator on the left-hand side of the equation. In fact, applying the
general deﬁnition of connected correlators (1.56) to the zero-mean Gaussian distribution,
we see inductively that all M-point connected correlators for M > 2 will vanish.9 Thus,
the connected correlators are a very natural measure of how a distribution deviates from
Gaussianity.
With this in mind, we can ﬁnally deﬁne a nearly-Gaussian distribution as a
distribution for which all the connected correlators for M > 2 are small.10
In fact,
the non-Gaussian distributions that describe neural networks generally have the prop-
erty that, as the network becomes wide, the connected four-point correlator becomes
small and the higher-point connected correlators become even smaller. For these nearly-
Gaussian distributions, a few leading connected correlators give a concise and accurate
description of the distribution, just as a few leading Taylor coeﬃcients can give a good
description of a function near the point of expansion.
1.3
Nearly-Gaussian Distributions
Now that we have deﬁned nearly-Gaussian distributions in terms of measurable devi-
ations from Gaussian statistics, i.e. via small but nonzero connected correlators, it’s
natural to ask how we can link these observables to the actual functional form of the
distribution, p(z). We can make this connection through the action.
The action S(z) is a function that deﬁnes a probability distribution p(z) through
the relation
p(z) ∝e−S(z) .
(1.62)
In the statistics literature, the action S(z) is sometimes called the negative log probability,
but we will again follow the physics literature and call it the action. In order for (1.62)
to make sense as a probability distribution, p(z) needs be normalizable so that we can
satisfy
Z
dNz p(z) = 1 .
(1.63)
That’s where the normalization factoror partition function
Z ≡
Z
dNz e−S(z)
(1.64)
9To see this, note that if all the higher-point connected correlators vanish, then the deﬁnition (1.56)
is equivalent to Wick’s theorem (1.50), with nonzero terms in (1.56) – the subdivisions into clusters of
sizes (2, . . . , 2) – corresponding exactly to the diﬀerent pairings in (1.50).
10As we discussed in §1.1, the variance sets the scale of the Gaussian distribution. For nearly-Gaussian
distributions, we require that all 2m-point connected correlators be parametrically small when compared
to an appropriate power of the variance, i.e., |E [zµ1 · · · zµ2m] |connected| ≪|Kµν|m, schematically.
28

comes in. After computing the partition function, we can deﬁne a probability distribution
for a particular action S(z) as
p(z) ≡e−S(z)
Z
.
(1.65)
Conversely, given a probability distribution we can associate an action, S(z) = −log [p(z)],
up to an additive ambiguity: the ambiguity arises because a constant shift in the action
can be oﬀset by the multiplicative factor in the partition function.11
The action is a very convenient way to approximate certain types of statistical pro-
cesses, particularly those with nearly-Gaussian statistics. To demonstrate this, we’ll ﬁrst
start with the simplest action, which describes the Gaussian distribution, and then we’ll
show how to systematically perturb it in order to include various non-Gaussianities.
Quadratic action and the Gaussian distribution
Since we already know the functional form of the Gaussian distribution, it’s simple to
identify the action by reading it oﬀfrom the exponent in (1.34)
S(z) = 1
2
N
X
µ,ν=1
Kµνzµzν ,
(1.66)
where, as a reminder, the matrix Kµν is the inverse of the variance matrix Kµν. The
partition function is given by the normalization integral (1.30) that we computed in §1.1
Z =
Z
dNz e−S(z) = IK =
q
|2πK| .
(1.67)
This quadratic action is the simplest normalizable action and serves as a starting point
for deﬁning other distributions.
As we will show next, integrals against the Gaussian distribution are a primitive
for evaluating expectations against nearly-Gaussian distributions. Therefore, in order
to diﬀerentiate between a general expectation and an integral against the Gaussian
distribution, let us introduce a special bra-ket, or ⟨·⟩notation for computing Gaussian
expectation values. For an observable O(z), deﬁne a Gaussian expectation as
⟨O(z)⟩K ≡
1
p
|2πK|
Z 

N
Y
µ=1
dzµ

exp

−1
2
N
X
µ,ν=1
Kµνzµzν

O(z) .
(1.68)
In particular, with this notation we can write Wick’s theorem as
⟨zµ1zµ2 · · · zµ2m⟩K =
X
all pairing
Kµk1µk2 · · · Kµk2m−1µk2m .
(1.69)
If we’re talking about a Gaussian distribution with variance Kµν, then we can use the
notation E [ · ] and ⟨·⟩K interchangeably. If instead we’re talking about a nearly-Gaussian
11One convention is to pick the constant such that the action vanishes when evaluated at its global
minimum.
29

distribution p(z), then E [ · ] indicates expectation with respect to p(z), (1.46). However,
in the evaluation of such an expectation, we’ll often encounter Gaussian integrals, for
which we’ll use this bra-ket notation ⟨·⟩K to simplify expressions.
Quartic action and perturbation theory
Now, let’s ﬁnd an action that represents a nearly-Gaussian distribution with a connected
four-point correlator that is small but non-vanishing
E [zµ1zµ2zµ3zµ4]

connected = O(ϵ) .
(1.70)
Here we have introduced a small parameter ϵ ≪1 and indicated that the correlator
should be of order ϵ. For neural networks, we will later ﬁnd that the role of the small
parameter ϵ is played by 1/width.
We should be able to generate a small connected four-point correlator by deforming
the Gaussian distribution through the addition of a small quartic term to the quadratic
action (1.66), giving us a quartic action
S(z) = 1
2
N
X
µ,ν=1
Kµνzµzν + ϵ
4!
N
X
µ,ν,ρ,λ=1
V µνρλzµzνzρzλ ,
(1.71)
where the quartic coupling ϵV µνρλ is an (N × N × N × N)-dimensional tensor that
is completely symmetric in all of its four indices. The factor of 1/4! is conventional in
order to compensate for the overcounting in the sum due to the symmetry of the indices.
While it’s not a proof of the connection, note that the coupling ϵV µνρλ has the right
number of components to faithfully reproduce the four-point connected correlator (1.70),
which is also an (N × N × N × N)-dimensional symmetric tensor. At least from this
perspective we’re oﬀto a good start.
Let us now establish this correspondence between the quartic coupling and connected
four-point correlator. Note that in general it is impossible to compute any expectation
value in closed form with a non-Gaussian action – this includes even the partition func-
tion.
Instead, in order to compute the connected four-point correlator we’ll need to
employ perturbation theory to expand everything to ﬁrst order in the small parame-
ter ϵ, each term of which can then be evaluated in a closed form. As this is easier done
than said, let’s get to the computations.
To start, let’s evaluate the partition function:
Z =
Z "Y
µ
dzµ
#
e−S(z)
(1.72)
=
Z "Y
µ
dzµ
#
exp
 
−1
2
X
µ,ν
Kµνzµzν −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4zρ1zρ2zρ3zρ4
!
=
q
|2πK|
*
exp
 
−ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4zρ1zρ2zρ3zρ4
!+
K
.
30

In the second line we inserted our expression for the quartic action (1.71), and in the last
line we used our bra-ket notation (1.68) for a Gaussian expectation with variance Kµν.
As advertised, the Gaussian expectation in the ﬁnal line cannot be evaluated in closed
form. However, since our parameter ϵ is small, we can Taylor-expand the exponential
to express the partition function as a sum of simple Gaussian expectations that can be
evaluated using Wick’s theorem (1.69):
Z =
q
|2πK|
*
1 −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4zρ1zρ2zρ3zρ4 + O

ϵ2+
K
(1.73)
=
q
|2πK|
"
1 −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 ⟨zρ1zρ2zρ3zρ4⟩K + O

ϵ2#
=
q
|2πK|
"
1 −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 (Kρ1ρ2Kρ3ρ4 + Kρ1ρ3Kρ2ρ4 + Kρ1ρ4Kρ2ρ3) + O

ϵ2#
=
q
|2πK|
"
1 −1
8ϵ
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kρ1ρ2Kρ3ρ4 + O

ϵ2#
.
In the ﬁnal line, we were able to combine the three K2 terms together by using the total
symmetry of the quartic coupling and then relabeling some of the summed-over dummy
indices.
Similarly, let’s evaluate the two-point correlator:
E [zµ1zµ2] = 1
Z
Z "Y
µ
dzµ
#
e−S(z) zµ1zµ2
(1.74)
=
p
|2πK|
Z
*
zµ1zµ2 exp
 
−ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4zρ1zρ2zρ3zρ4
!+
K
=
p
|2πK|
Z
"
⟨zµ1zµ2⟩K −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 ⟨zµ1zµ2zρ1zρ2zρ3zρ4⟩K + O

ϵ2#
=
"
1 + 1
8ϵ
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kρ1ρ2Kρ3ρ4
#
Kµ1µ2
−ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 (3Kµ1µ2Kρ1ρ2Kρ3ρ4 + 12Kµ1ρ1Kµ2ρ2Kρ3ρ4) + O

ϵ2
=Kµ1µ2 −ϵ
2
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kµ1ρ1Kµ2ρ2Kρ3ρ4 + O

ϵ2
.
Here, to go from the ﬁrst line to the second line we inserted our expression for the quartic
action (1.71) and rewrote the integral as a Gaussian expectation. Then, after expanding
in ϵ to ﬁrst order, in the next step we substituted (1.73) in for the partition function
Z in the denominator and expanded 1/Z to the ﬁrst order in ϵ using the expansion
1/(1 −x) = 1 + x + O
 x2. In that same step, we also noted that, of the ﬁfteen terms
coming from the Gaussian expectation ⟨zµ1zµ2zρ1zρ2zρ3zρ4⟩K, there are three ways in
31

which zµ1 and zµ2 contract with each other but twelve ways in which they don’t. Given
again the symmetry of V ρ1ρ2ρ3ρ4, this is the only distinction that matters.
At last, let’s compute the full four-point correlator:
E [zµ1zµ2zµ3zµ4] = 1
Z
Z "Y
µ
dzµ
#
e−S(z) zµ1zµ2zµ3zµ4
(1.75)
=
p
|2πK|
Z
"
⟨zµ1zµ2zµ3zµ4⟩K −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 ⟨zµ1zµ2zµ3zµ4zρ1zρ2zρ3zρ4⟩K + O

ϵ2#
=
"
1 + 1
8ϵ
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kρ1ρ2Kρ3ρ4
#
[Kµ1µ2Kµ3µ4 + Kµ1µ3Kµ2µ4 + Kµ1µ4Kµ2µ3]
−ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4
×

3Kµ1µ2Kµ3µ4Kρ1ρ2Kρ3ρ4 + 12Kµ1ρ1Kµ2ρ2Kµ3µ4Kρ3ρ4 + 12Kµ3ρ1Kµ4ρ2Kµ1µ2Kρ3ρ4
+ 3Kµ1µ3Kµ2µ4Kρ1ρ2Kρ3ρ4 + 12Kµ1ρ1Kµ3ρ2Kµ2µ4Kρ3ρ4 + 12Kµ2ρ1Kµ4ρ2Kµ1µ3Kρ3ρ4
+ 3Kµ1µ4Kµ2µ3Kρ1ρ2Kρ3ρ4 + 12Kµ1ρ1Kµ4ρ2Kµ2µ3Kρ3ρ4 + 12Kµ2ρ1Kµ3ρ2Kµ1µ4Kρ3ρ4
+ 24Kµ1ρ1Kµ2ρ2Kµ3ρ3Kµ4ρ4

+ O

ϵ2
.
To go from the ﬁrst line to the second line we inserted our expression for the quartic
action (1.71), expanded to ﬁrst order in ϵ, and rewrote in the bra-ket notation (1.68). On
the third line, we again substituted in the expression (1.73) for the partition function
Z, expanded 1/Z to ﬁrst order in ϵ, and then used Wick’s theorem (1.69) to evalu-
ate the fourth and eighth Gaussian moments. (Yes, we know that the evaluation of
⟨zµ1zµ2zµ3zµ4zρ1zρ2zρ3zρ4⟩K is not fun. The breakdown of the terms depends again on
whether or not the µ-type indices are contracted with the ρ-type indices or not.) We
can simplify this expression by noticing that some terms cancel due to 1
8 −3
24 = 0 and
some other terms can be nicely regrouped once we notice through the expression for the
two-point correlator (1.74) that
Kµ1µ2Kµ3µ4 −ϵ
24
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4 (12Kµ1ρ1Kµ2ρ2Kµ3µ4Kρ3ρ4 + 12Kµ3ρ1Kµ4ρ2Kµ1µ2Kρ3ρ4)
= E [zµ1zµ2] E [zµ3zµ4] + O

ϵ2
,
(1.76)
yielding in the end
E [zµ1zµ2zµ3zµ4]
(1.77)
=E [zµ1zµ2] E [zµ3zµ4] + E [zµ1zµ3] E [zµ2zµ4] + E [zµ1zµ4] E [zµ2zµ3]
−ϵ
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kµ1ρ1Kµ2ρ2Kµ3ρ3Kµ4ρ4 + O

ϵ2
.
32

Given the full four-point correlator (1.75) and the two-point correlator (1.74), we can
ﬁnally evaluate the connected four-point correlator (1.54) as
E [zµ1zµ2zµ3zµ4]

connected = −ϵ
X
ρ1,...,ρ4
V ρ1ρ2ρ3ρ4Kµ1ρ1Kµ2ρ2Kµ3ρ3Kµ4ρ4 + O

ϵ2
.
(1.78)
This makes explicit the relationship between the connected four-point correlator and the
quartic coupling in the action, when both are small. We see that for the nearly-Gaussian
distribution realized by the quartic action (1.71), the distribution is – as promised –
nearly Gaussian: the strength of the coupling ϵV ρ1ρ2ρ3ρ4 directly controls the distri-
bution’s deviation from Gaussian statistics, as measured by the connected four-point
correlator. This also shows that the four-index tensor V ρ1ρ2ρ3ρ4 creates nontrivial cor-
relations between the components zρ1zρ2zρ3zρ4 that cannot otherwise be built up by the
correlation Kµν in any pair of random variables zµzν.
Finally, note that the connected two-point correlator (1.74) – i.e. the covariance of
this nearly-Gaussian distribution – is also shifted from its Gaussian value of Kµ1µ2 by
the quartic coupling ϵV ρ1ρ2ρ3ρ4. Thus, the nearly-Gaussian deformation not only creates
complicated patterns of four-point correlation as measured by the connected four-point
correlator (1.78), it also can modify the details of the Gaussian two-point correlation.
Now that we see how to compute the statistics of a nearly-Gaussian distribution, let’s
take a step back and think about what made this possible. We can perform these per-
turbative calculations any time there exists in the problem a dimensionless parameter ϵ
that is small ϵ ≪1, but nonzero ϵ > 0. This makes perturbation theory an extremely
powerful tool for theoretical analysis any time a problem has any extreme scales, small
or large.
Importantly, this is directly relevant to theoretically understanding neural networks
in practice. As we will explain in the following chapters, real networks have a parameter
n – the number of neurons in a layer – that is typically large n ≫1, but certainly not
inﬁnite n < ∞. This means that we can expand the distributions that describe such
networks in the inverse of the large parameter as ϵ = 1/n. Indeed, when the parameter
n is large – as is typical in practice – the distributions that describe neural networks
become nearly-Gaussian and thus theoretically tractable.
This type of expansion is
known as the 1/n expansion or large-n expansion and will be one of our main tools
for learning the principles of deep learning theory.
Aside: statistical independence and interactions
The quartic action (1.71) is one of the simplest models of an interacting theory. We
showed this explicitly by connecting the quartic coupling to the non-Gaussian statistics
of the non-vanishing connected four-point correlator. Here, let us try to oﬀer an intuitive
meaning of interaction by appealing to the notion of statistical independence.
Recall from the probability theory that two random variables x and y are statistically
independent if their joint distribution factorizes as
p(x, y) = p(x)p(y) .
(1.79)
33

For the Gaussian distribution, if the variance matrix Kµν is diagonal, there is no corre-
lation at all between diﬀerent components of zµ; they are manifestly statistically inde-
pendent from each other.
Even if Kµν is not diagonal, we can still unwind the correlation of a Gaussian dis-
tribution by rotating to the right basis. As discussed in §1.1, there always exists an
orthogonal matrix O that diagonalizes the covariance as (OKOT )µν = λµδµν. In terms
of the variables uµ ≡(Oz)µ, the distribution looks like
p(z) =
1
p
|2πK| exp

−
N
X
µ=1
u2
µ
2λµ

=
N
Y
µ=1


e
−
u2µ
2λµ
p2πλµ


= p(u1) · · · p(uN) .
(1.80)
Thus, we see that in the u-coordinate basis the original multivariable Gaussian distri-
bution factorizes into N single-variable Gaussians that are statistically independent.
We also see that in terms of the action, statistical independence is characterized
by the action breaking into a sum over separate terms. This unwinding of interaction
between variables is generically impossible when there are nonzero non-Gaussian cou-
plings. For instance, there are ∼N2 components of an orthogonal matrix Oµν to change
basis, while there are ∼N4 components of the quartic coupling ϵV µνρλ that correlate
random variables, so it is generically impossible to re-express the quartic action as a sum
of functions of N diﬀerent variables. Since the action cannot be put into a sum over
N separate terms, the joint distribution cannot factorize, and the components will not
be independent from each other. Thus, it is impossible to factor the nearly-Gaussian
distribution into the product of N statistically independent distributions. In this sense,
what is meant by interaction is the breakdown of statistical independence.12
Nearly-Gaussian actions
Having given a concrete example in which we illustrated how to deform the quadratic
action to realize the simplest nearly-Gaussian distribution, we now give a more general
perspective on nearly-Gaussian distributions. In what follows, we will continue to require
that our distributions are invariant under the parity symmetry that takes zµ →−zµ. In
the action representation, this corresponds to including only terms of even degree.13
12An astute reader might wonder if there is any interaction when we consider a single-variable distri-
bution with N = 1, since there’s no other variables to interact with. For nearly-Gaussian distributions,
even if N = 1, we saw in (1.74) that the variance of the distribution is shifted from its Gaussian value, K,
and depends on the quartic coupling ϵV . In physics, we say that this shift is due to the self-interaction
induced by the quartic coupling ϵV , since it modiﬁes the value of observables from the free Gaussian
theory that we are comparing to, even though there’s no notion of statistical independence to appeal to
here.
Said another way, even though the action just involves one term, such a non-Gaussian distribution
does not have a closed-form solution for its partition function or correlators; i.e. there’s no trick that
lets us compute integrals of the form e−S(z) exactly, when S(z) =
z2
2K + 1
4!ϵV z4. This means that we
still have to make use of perturbation theory to analyze the self-interaction in such distributions.
13The imposition of such a parity symmetry, and thus the absence of odd-degree terms in the action,
means that all of the odd moments and hence all of the odd-point connected correlators will vanish.
34

With that caveat in mind, though otherwise very generally, we can express a non-
Gaussian distribution by deforming the Gaussian action as
S(z) = 1
2
N
X
µ,ν=1
Kµνzµzν +
k
X
m=2
1
(2m)!
N
X
µ1,...,µ2m=1
sµ1···µ2mzµ1 · · · zµ2m ,
(1.81)
where the factor of 1/(2m)! is conventional in order to compensate for the overcounting
in the sum due to the implied symmetry of the indices µ1, . . . , µ2m in the coeﬃcients
sµ1···µ2m, given the permutation symmetry of the product of variables zµ1 · · · zµ2m. The
number of terms in the non-Gaussian part of the action is controlled by the integer k. If
k were unbounded, then S(z) would be an arbitrary even function, and p(z) could be any
parity-symmetric distribution. The action is most useful when the expanded polynomial
S(z) truncated to reasonably small degree k – like k = 2 for the quartic action – yields
a good representation for the statistical process of interest.
The coeﬃcients sµ1···µ2m are generally known as non-Gaussian couplings, and they
control the interactions of the zµ.14
In particular, there is a direct correspondence
between the product of the speciﬁc components zµ that appear together in the action
and the presence of connected correlation between those variables, with the degree of the
term in (1.81) directly contributing to connected correlators of that degree. We saw an
example of this in (1.78), which connected the quartic term to the connected four-point
correlator. In this way, the couplings give a very direct way of controlling the degree and
pattern of non-Gaussian correlation, and the overall degree of the action oﬀers a way of
systematically including more and more complicated patterns of such correlations.
If you recall from §1.2, we deﬁned nearly-Gaussian distributions as ones for which
all these connected correlators are small. Equivalently, from the action perspective, a
nearly-Gaussian distribution is a non-Gaussian distribution with an action of the form
(1.81) for which all the couplings sµ1···µ2m are parametrically small for all 1 ≤m ≤k:
|sµ1···µ2m| ≪|Kµν|m ,
(1.82)
where this equation is somewhat schematic given the mismatch of the indices.15 Impor-
tantly the comparison is with an appropriate power of the inverse variance or quadratic
14In the similar vein, the coeﬃcient Kµν in the action is sometimes called a quadratic coupling since
the coupling of the component zµ with the component zν in the quadratic action leads to a nontrivial
correlation, i.e. Cov[zµ, zν] = Kµν.
15This schematic equation is, nonetheless, dimensionally consistent. To support that remark, let us
give a brief introduction to dimensional analysis: let the random variable zµ have dimension ζ, which we
denote as [zµ] = ζ1. By dimension, you should have in mind something like a unit of length, so e.g. we
read the expression [zµ] = ζ1 as “a component of z is measured in units of ζ.” The particular units are
arbitrary: e.g. for length, we can choose between meters or inches or parsecs as long as we use a unit
of length but not, say, meters2, which instead would be a unit of area. Importantly, we cannot add
or equate quantities that have diﬀerent units: it doesn’t make any logical sense to add a length to an
area. This is similar to the concept of type safety in computer science, e.g. we should not add a type str
variable to a type int variable.
Now, since the action S(z) is the argument of an exponential p(z) ∝e−S(z), it must be dimensionless;
otherwise, the exponential e−S = 1−S + S2
2 +. . . would violate the addition rule that we just described.
From this dimensionless requirement for the action, we surmise that the inverse of the covariance matrix
35

coupling Kµν since, as we already explained, the variance sets the scale of the Gaussian
distribution to which we are comparing these nearly-Gaussian distributions.
As we will see in §4, wide neural networks are described by nearly-Gaussian distri-
butions. In particular, we will ﬁnd that such networks are described by a special type
of nearly-Gaussian distribution where the connected correlators are hierarchically small,
scaling as
E [zµ1 · · · zµ2m]

connected = O(ϵm−1) ,
(1.83)
with the same parameter ϵ controlling the diﬀerent scalings for each of the 2m-point
connected correlators.
Importantly, the non-Gaussianities coming from higher-point
connected correlators become parametrically less important as ϵ becomes smaller.
This means that for a nearly-Gaussian distribution with hierarchical scalings (1.83),
we can consistently approximate the distribution by truncating the action at some ﬁxed
order in ϵ. To be concrete, we can use an action of the form (1.81) to faithfully represent
all the correlations up to order O(ϵk−1), neglecting connected correlations of order O(ϵk)
and higher. The resulting action oﬀers a useful and eﬀective description for the statistical
process of interest, as long as ϵ is small enough and k is high enough that O(ϵk) is
negligible.
In practice, a quartic action (1.71) truncated to k = 2 will let us model realis-
tic ﬁnite-width neural networks. This quartic action captures the important qualitative
diﬀerence between nearly-Gaussian distributions and the Gaussian distribution, incorpo-
rating nontrivial interactions between the diﬀerent components of the random variable.
In addition, the diﬀerence between the statistics (1.83) of a nearly-Gaussian distribution
truncated to O(ϵ) versus one truncated to O
 ϵ2 is mostly quantitative: in both cases
there are nontrivial non-Gaussian correlations, but the pattern of higher-order correla-
tion diﬀers only in a small way, with the diﬀerence suppressed as O
 ϵ2. In this way, the
distribution represented by the quartic action is complex enough to capture the most
salient non-Gaussian eﬀects in neural networks while still being simple enough to be
analytically tractable.
has dimension [Kµν] = ζ−2, and that the covariance itself has dimension [Kµν] = ζ2. Similarly, all the
non-Gaussian couplings in (1.81) have dimensions [sµ1···µ2m] = ζ−2m. Thus, both sides of (1.82) have
the same dimension, making this equation dimensionally consistent.
Even more concretely, consider the quartic action (1.71). If we let the tensorial part of the quartic
coupling have dimensions [V µνρλ] = ζ−4, then the parameter ϵ is dimensionless, as claimed.
This
means that we can consistently compare ϵ to unity, and its parametric smallness ϵ ≪1 means that
the full quartic coupling ϵV µνρλ is much smaller than the square of the quadratic coupling, and that
the connected four-point correlator (1.78) is much smaller than the square of the connected two-point
correlator (1.74).
36

Chapter 2
Neural Networks
On being asked, “How is Perceptron performing today?” I am often tempted to
respond, “Very well, thank you, and how are Neutron and Electron behaving?”
Frank Rosenblatt, inventor of the perceptron and also the Perceptron [6].
With our mathematical lessons concluded, we turn to an introductory overview of deep
learning.
In §2.1, we introduce the basic components of neural network architectures – neu-
rons, activations, biases, weights, and layers – in order to deﬁne the multilayer perceptron
(MLP), a simple model that is iteratively composed of these basic components. Given
that all deep networks are by deﬁnition iteratively composed of many structurally iden-
tical layers, MLPs will play the role of archetype network architecture for illustrating the
principles of deep learning throughout the book. This class of neural-network models
is rich enough to capture all the essential aspects of deep learning theory, while simple
enough to maintain the pedagogical focus of the book. Nevertheless, we’ll also brieﬂy
comment on how one could work out an eﬀective theory for other network architectures.
In §2.2 we list some common activation functions that are often used in practice.
Finally, we discuss in §2.3 how MLPs are initialized. Here, we make a key conceptual
shift from thinking about the weights and biases as the random variables to thinking
about the induced distribution over the neural activities and network outputs.
The
expressions we derive here will provide a natural starting point for our analysis in §4
when we start developing our eﬀective theory of MLPs with general activation functions.
2.1
Function Approximation
The subject of artiﬁcial neural networks has a rich history as cognitive science and
neuroscience–inspired artiﬁcial intelligence.1 Here, our starting point will be a discussion
1The artiﬁcial neuron was invented by McCulloch and Pitts in 1943 [7] as a model of the biological
neuron. Their neuron was essentially a perceptron with a bias, but did not have learnable weights. The
perceptron model, with learnable weights, was invented by Rosenblatt in 1958 [8]. Deep learning really
37

of the function, f(x).
Some functions are really simple, easily described in terms of the elementary opera-
tions: addition, subtraction, multiplication, and division. For instance, consider either
the identity function f(x) = x or the exponential function f(x) = ex. The former is the
deﬁnition of trivial, involving no operations. The latter is a special function and can be
deﬁned in many ways, e.g. through its Taylor series
ex ≡
∞
X
k=0
xk
k! .
(2.1)
This deﬁnition constructs the exponential function in terms of elementary operations of
addition, multiplication, and division: the numerator xk represents the repeated multi-
plication of the variable x for k times, and the factorial k! in the denominator represents
the repeated multiplication of integers k! = 1 × 2 × · · · × (k −1) × k. Although this
description of the exponential function involves a sum of an inﬁnite number of terms,
the actual instructions (2.1) for computing this function in terms of these simple opera-
tions are so compact that they takes up only about one seventh of a line and, for many
purposes, it only takes the ﬁrst few terms in the sum to get a useful approximation of
ex.
Some functions are really complicated and their description in terms of elementary
operations is unlikely to ﬁt in the conﬁnes of any printed book. For instance, imagine
a function f(x) that takes as input an image xi – represented as a vector of numbers
corresponding to a black-and-white pixelated image – and outputs 1 if the image xi
depicts a cat and 0 otherwise. While such a classiﬁcation function should exist since
humans can recognize images of cats, it’s not at all clear how to describe such a function
in terms of simple operations like addition and multiplication. The subject of artiﬁcial
intelligence (AI) is mostly concerned with functions of this sort: easy for humans to
compute, but diﬃcult for humans to describe in terms of elementary operations.
The conceptual leap needed to represent such hard-to-describe functions is to start
with a ﬂexible set of functions {f(x; θ)}, constructed from simple components parametrized
by a vector of adjustable model parameters θµ. One then tries to tune these model
parameters θµ judiciously in order to approximate the original complicated function such
that f(x; θ⋆) ≈f(x). The description of the set of functions {f(x; θ)} as well as the set-
tings of the model parameters θ⋆
µ then serve as a useful approximate description of a
desired function f(x). This is called function approximation and the procedure for
adjusting the model parameters θµ is called a learning algorithm.
To be more concrete, let us represent the collection of inputs to our function f(x) as
a set D of n0-dimensional vectors
D = {xi;α}α=1,...,ND ,
(2.2)
called input data. Here, the sample index α labels each sample in the dataset of ND
elements, and the vector index i = 1, . . . , n0 labels the component of the input vector.
came into its own in 2012 [9] after the realization that the graphical processing unit (GPU) is well-suited
for the parallel computations required to train and run neural networks.
38

In our motivating example above, each number xi;α refers to the i-th pixel of the α-th
image in the dataset D of ND images, each of which might or might not depict a cat.
By adjusting the model parameters θµ so that the function f(x; θ⋆) outputs the correct
answer for as much input data as possible, we can try to approximate the elusive cat-
or-not function in a way that no longer deﬁes description. The overall idea of training
such functions using a dataset D – rather than programming them – goes by the name
machine learning and stands in contrast to the conventional von Neumann model of
the digital computer.
While any set of parameterized functions can be used for function approximation,2
our focus will be on a particular set of composable functions originally derived from a
simpliﬁed model of the brain. Such functions were originally termed artiﬁcial neural
networks and are now just referred to as neural networks.
Deep learning is a
branch of machine learning that uses neural networks as function approximators, with a
particular emphasis on stacking many layers of structurally similar components. Let’s
see how this works in more detail.
The most basic component of the neural network is the neuron. Loosely inspired by
the behavior of biological neurons, the artiﬁcial neuron essentially consists of two simple
operations:
• The preactivation zi of a neuron is a linear aggregation of incoming signals sj
where each signal is weighted by Wij and biased by bi
zi(s) = bi +
nin
X
j=1
Wij sj
for
i = 1, . . . , nout .
(2.3)
• Each neuron then ﬁres or not according to the weighted and biased evidence,
i.e. according to the value of the preactivation zi, and produces an activation
σi ≡σ(zi) .
(2.4)
The scalar-valued function σ(z) is called the activation function and acts inde-
pendently on each component of the preactivation vector.
Taken together, these nout neurons form a layer, which takes in the nin-dimensional
vector of signals sj and outputs the nout-dimensional vector of activations σi. With this
collective perspective, a layer is parameterized by a vector of biases bi and a matrix of
weights Wij, where i = 1, . . . , nout and j = 1, . . . , nin, together with a ﬁxed activation
function σ(z).
With these components, we can make an increasingly ﬂexible set of functions by
organizing many neurons into a layer and then iteratively stacking many such layers,
so that the outgoing activations of the neurons in one layer become the input signals
to the neurons in some other layer. The organization of the neurons and their pattern
2E.g. consider a sum of Gaussian functions, where the mean and variance of each Gaussian play the
role of the adjustable parameters.
39

of connections is known as the neural network architecture. The archetypical neural
network architecture based on this principle of stacking layers of many neurons is called
the multilayer perceptron (MLP).3
The activation function is usually chosen to be a nonlinear function in order to
increase the expressivity of the neural-network function f(x; θ).
The simplest – and
historically ﬁrst – activation function either ﬁres or does not ﬁre: σ(z) = 1 for z ≥0
and σ(z) = 0 for z < 0. In other words, each neuron ﬁres if and only if the weighted
accumulated evidence P
j Wij xj exceeds the ﬁring threshold −bi. More generally, acti-
vation functions are not binary and can incorporate the strength of the evidence into
their output. In §2.2 we’ll describe many of the commonly-used activation functions in
deep learning.
The MLP is recursively deﬁned through the following iteration equations
z(1)
i
(xα) ≡b(1)
i
+
n0
X
j=1
W (1)
ij xj;α ,
for
i = 1, . . . , n1 ,
(2.5)
z(ℓ+1)
i
(xα) ≡b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
σ

z(ℓ)
j (xα)

,
for
i = 1, . . . , nℓ+1 ; ℓ= 1, . . . , L −1 ,
which describes a network with L layers of neurons, with each layer ℓcomposed of
nℓneurons.4 We depict an example MLP architecture in Figure 2.1. The number of
layers L deﬁnes the depth of the network and the diﬀerent number of neurons in each
layer nℓ=1,...,L−1 deﬁne the widths of the layers. The depth and hidden-layer widths
are variable architecture hyperparameters that deﬁne the shape of the network,
while the values of n0 and nL are set by input and output dimensions of the function-
approximation task, respectively. In particular, the ﬁnal-layer preactivations computed
by the network
f(x; θ) = z(L)(x),
(2.6)
serves as the function approximator, with its model parameters θµ being the union of
the biases and weights from all the layers. Sometimes it will be convenient to think of
this collection of model parameters as an explicit vector θµ whose components cover all
the model parameters. In that case, the dimension of θµ and therefore the total number
of the model parameters is given by
P =
L
X
ℓ=1
(nℓ+ nℓnℓ−1) ,
(2.7)
3Here, the name “perceptron” was inherited from Rosenblatt’s Perceptron architecture [8], which was
originally envisioned for emulating human perception. The name perceptron is also used to refer to the
original step-function activation function, cf. the ﬁrst entry of §2.2.
4A more modern name for the MLP is the fully-connected network (FCN), highlighting the fact that
each neuron in a given layer ℓhas a connection to every neuron in layer ℓ+ 1, as Figure 2.1 makes clear.
Such a dense pattern of connections is computationally expensive in terms of the number of parameters
required for the architecture and should be contrasted with the sparser architectures described at the
end of this section. To place an emphasis on the deepness of networks rather than on the density of
the connections, we’ll mainly stick with the name multilayer perceptron over the name fully-connected
network in this book.
40

Figure 2.1:
Left: depiction of the neurons and connections for an example multilayer
perceptron (MLP) architecture. This particular MLP has L = 4 layers, deﬁning a set
of functions f(x; θ) with input dimension n0 = 4 and output dimension n4 = 1. The
three hidden layers have ﬁve neurons each n1, n2, n3 = 5, implying P = 91 total model
parameters. The graph describing the connections between neurons is a directed acyclic
graph, meaning that signals only propagate in one direction and do not loop inside the
network. For this reason, MLPs are also sometimes called feedforward networks. Right:
the detailed structure of each neuron that (i) adds the bias and the weighted signals to
produce the preactivation, (ii) generates the activation from the preactivation, and (iii)
multiplies the activation by the next-layer weight.
41

which scales quadratically with the widths of the network and linearly with the depth.
The intermediate layers ℓ= 1, . . . , L −1 are referred to as hidden layers, since pre-
activations and activations of the neurons from those layers are not part of the network’s
output. On the one hand, the variables z(ℓ)(x) for ℓ< L are simply temporary vari-
ables introduced to construct an increasingly ﬂexible set of functions, expressive enough
to have a chance of approximating hard-to-describe functions. On the other hand, in
analogy to the physical brain, these variables are thought to encode useful information
about how the neural network is approximating; for example, a particular neuron might
ﬁre if it recognizes a tail, a whisker, or a pattern representing fur – all potentially useful
features for determining whether an image contains a cat or not.
Moving beyond MLPs, the choice of neural network architecture is often motivated
by the nature of the function we are trying to approximate. For instance, the properties
of the dataset D, when known and articulated, can be used to build inductive biases
into the architecture so that the resulting set of functions may better represent the
underlying function.5 Let’s look at a few examples.
• For computer vision (CV) applications, convolutional neural networks (CNN)
or conv-nets [9–13] are used to take advantage of the fact that information in images
is organized in a spatially local manner, often respecting translational invariance.6
• For natural language processing (NLP) applications, the transformer archi-
tecture (no acronym yet) is used to process sequential input – such as a paragraph
of text or an amino acid sequence coding a protein – in a way that encourages
correlations to develop between any of the elements in the sequence [14]. This
property of the model is aptly called attention.
An important property of these inductive biases is that they induce constraints or re-
lationships between the weights. For instance, we can think of the convolutional layer
as a particular type of MLP layer, where many weights are set to zero and the values
of remaining weights are further shared among several diﬀerent neurons. This property
is known as weight tying. That means that convolutional layers are actually within the
class of functions describable by using MLP layers, but they are very unlikely to be found
via training unless the constraints are explicitly enforced. As long as the inductive bias
5We’ll discuss the inductive bias of MLPs from various diﬀerent perspectives in §6, §11, and Epilogue ε.
6For a two-dimensional convolutional layer, the iteration equation (2.5) for MLPs is replaced by
z(ℓ+1)
i,(c,d)(xα) ≡b(ℓ+1)
i
+
nℓ
X
j=1
k
X
c′=−k
k
X
d′=−k
W (ℓ+1)
ij
σ

z(ℓ)
j,(c+c′,d+d′)(xα)

,
(2.8)
where in z(ℓ)
i,(c,d), the ﬁrst index i is an auxiliary channel index and the paired index (c, d) is a two-
dimensional spatial index, and the number k is a ﬁxed constant for each layer, determining the size of
the convolutional window. In particular, the same weights are used on diﬀerent spatial locations of the
input, which promotes the inductive bias that image data are often translationally invariant. In other
words, a cat is still a cat regardless of its location in an image. At the time of writing, the convolutional
layer is an essential part of many modern deep learning architectures, but this situation may change in
the future. Please pay attention.
42

of spatial locality and translational invariance is well founded, the convolution layer has
obvious computational advantages by heavily curtailing the number of weights to be
trained and stored.
Regardless of these speciﬁc inductive biases ingrained into modern neural network
architectures used in deep learning, the common thread to all is the idea of constructing
a ﬂexible set of functions by organizing neural components into many iterated layers.
MLPs are the simplest of these neural network architectures that hinges on this stacking
idea, and thus provide a minimal model for an eﬀective theory of deep learning.
Speciﬁcally, we expect that (a) the principles of deep learning theory that we uncover
to be general and valid across the large variety of architectures that are based on the
idea of stacking many layers of neural components and (b) the resulting eﬀective theory
formalism can be specialized to speciﬁc architectures of interest as needed, using this
book as a guide for how to work out such a theory. In particular, one can study other
architectures in our formalism simply by swapping out the MLP iteration equation (2.5)
– e.g. for the convolution layer iteration equation (2.8) – in the appropriate place. We’ll
provide pointers on where to make such substitutions when we begin working out our
eﬀective theory in §4.
Finally, in Appendix B we’ll study neural networks with residual connections, known
as residual networks. These architectures are specially modiﬁed to enable the training
of deeper and deeper networks. In the ﬁnal section of that appendix, we’ll also explain
how our eﬀective theory approach can be extended to general residual networks, includ-
ing the residual convolutional network or ResNet and the transformer architecture.
2.2
Activation Functions
In this section, we discuss some of the most common activation functions. This list is
non-exhaustive, so hopefully you won’t ﬁnd this section exhausting. To make it easier
for you, we’ve plotted all these activation functions together in Figure 2.2. In §5, we’ll
use our eﬀective theory to evaluate the relative usefulness of these activation functions
in allowing input signals to eﬀectively pass through a deep network.
Perceptron
The perceptron was the original activation function [7]. It is just a step function
σ(z) =
(
1 ,
z ≥0 ,
0 ,
z < 0 ,
(2.9)
corresponding to a computer scientist’s notion of simplicity: the neuron either ﬁres and
outputs 1 or doesn’t ﬁre and outputs 0.7
Despite the logical simplicity, this turns out to be a poor choice. As we will see, in
order to both eﬀectively pass signals through networks (§5 and §9) and train them (§10),
7Alternatively, the perceptron may be shifted and scaled such that σ(z) = sign(z).
43

Figure 2.2: Commonly-used activation functions σ(z). Grids are in units of one for both
the preactivation z and activation σ. (The leaky ReLU is not shown.)
44

it’s helpful to propagate more than one bit of information about the preactivation z. The
perceptron has historical signiﬁcance, but is never used in deep neural networks.
Sigmoid
The sigmoid activation function is a logistic function
σ(z) =
1
1 + e−z = 1
2 + 1
2 tanh
z
2

,
(2.10)
which is a smoothed version of the perceptron. Not only is it continuous, but also it
preserves information about the magnitude of the preactivation, albeit mostly in the
range near z = 0 where the function is nearly linear. Outside of this range, the sigmoid
heavily compresses such information as it becomes more and more perceptron-like,
saturating as σ(z) = 1 when z →∞and as σ(z) = 0 when z →−∞.
As a mapping from the domain of (−∞, ∞) to the range [0, 1], the sigmoid also
has a natural interpretation of converting log-odds to a probability, which is its main
application in machine learning. For deep learning, the diﬀerentiability of the sigmoid
was essential in the development of a learning algorithm – backpropagation – for training
neural networks with hidden layers [15]. Nevertheless, the sigmoid activation function
is still a poor choice in deep neural networks: as we’ll see in §5, a problem arises from
the fact that it doesn’t pass through the origin.
Tanh
The hyperbolic tangent or tanh activation function
σ(z) = tanh(z) = ez −e−z
ez + e−z = e2z −1
e2z + 1 ,
(2.11)
is a scaled (both before and after the activation) and shifted sigmoid, as is clear
from (2.10). Of particular importance is the fact that it’s shifted such that σ(0) = 0
[16].
The tanh is probably the most popular choice of activation function aside from the
ReLU or ReLU-like activation functions to be discussed shortly, and arguably tanh is the
most popular smooth activation function. As an exemplary smooth activation function,
the tanh will be of signiﬁcant interest for us in this book.
Sin
The sin activation function is just what it sounds like:
σ(z) = sin(z) ,
(2.12)
i.e. one of the three standard trigonometric function. Periodic nonlinearities have been
cycling in and out of popularity for a long while now, see e.g. [17], though they have
never really achieved true popularity.
45

Scale-invariant: linear, ReLU, and leaky ReLU
A scale-invariant activation function is any activation function that satisﬁes
σ(λz) = λσ(z) ,
(2.13)
for any positive rescaling λ. We call these activation functions scale-invariant because
any scaling of the preactivation z →λz can be undone by an inverse scaling of the
activation σ(z) →λ−1σ(z).
This condition is met by – and only by8 – activation
functions of the form
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 .
(2.14)
The class of scale-invariant activation functions includes linear (a+ = a−= a),
Rectiﬁed Linear Unit or ReLU (a+ = 1, a−= 0) [18, 19], and leaky ReLU (a+ = 1,
a−= a) [20] activation functions.
The ReLU is the most popular of the activation
functions used in deep neural networks and therefore will be of substantial interest for
us in this book.
In order to deepen our understanding of scale invariance, let’s consider how other
activation functions can break it. For instance, consider the tanh activation function
σ(z) = tanh(z).
Mathematically, tanh violates scale invariance because tanh(λz) ̸=
λ tanh σ(z) unless λ = 1. In particular, while the activation function is approximately
linear for small preactivations, i.e. tanh(z) ≈z for |z| ≪1, it saturates for large preac-
tivations, i.e. | tanh(z)| ≈1 for |z| ≫1. Thus, tanh comes with an intrinsic crossover
scale |z| ∼1 that separates the two regimes. We can see this visually in Figure 2.2: if we
zoom out, all the non-scale-invariant activation functions – e.g. perceptron, sigmoid,
and tanh – will look squashed, while the scale-invariant activation functions – e.g. ReLU
and linear – will look the same at any scale.
Finally, note that all the scale-invariant activation functions – except the aptly-
named linear activation – create a nonlinear relationship between the network inputs
and outputs due to the kink at the origin z = 0. Stacking up many layers of neurons
with these nonlinear activation functions accumulates the nonlinearity, allowing such
deep neural networks to express highly nonlinear functions.
ReLU-like: softplus, SWISH, and GELU
Despite the popularity of the ReLU, there’s an uneasiness about the fact that it’s not
smooth.
In an attempt to rectify the situation, a variety of smoothed-out ReLU-like
activations have been proposed and achieved semi-popularity, of which we will consider
the following three:
8In order to prove this necessity statement, ﬁrst take the derivative of the scale-invariance equa-
tion (2.13) with respect to z, which gives σ′(λz) = σ′(z) for any λ > 0. Then note that this enforces a
constant derivative, a+, for z > 0 and another constant derivative, a−, for z < 0. Finally, to satisfy (2.13)
we also must have limz→±0 σ(z) = 0. Quantum Electrodynamics.
46

• The softplus activation function [21]
σ(z) = log(1 + ez) ,
(2.15)
behaves linearly σ(z) ≈z for a large argument z ≫1 and vanishes exponentially
for a negative argument, σ(z) ≈e−|z| for z < 0. Importantly the softplus does
not pass through the origin: σ(0) = log(2).
• The SWISH activation function [22] is deﬁned as
σ(z) =
z
1 + e−z ,
(2.16)
which is a logistic function (2.10) multiplied by the preactivation z. The logistic
function behaves as a continuous on/oﬀswitch, and so the SWISH approximates
the ReLU, which we recall was deﬁned as a discrete on/oﬀswitch multiplied by the
preactivation z. In particular, for z > 0 the SWISH behaves as σ(z) ≈z, but for
z < 0 it behaves as σ(z) ≈0. Also, the multiplication by z ensures that the SWISH
passes through the origin, σ(0) = 0.
• The Gaussian Error Linear Unit (GELU) activation function [23] is a lot like the
SWISH. It’s given by the expression
σ(z) =
1
2 + 1
2erf
 z
√
2

× z ,
(2.17)
where the error function erf(z) is given by
erf(z) ≡
2
√π
Z z
0
dt e−t2 ,
(2.18)
which is a partial integration of the Gaussian function. In particular, the graph of
erf(z) looks very similar to graph of tanh(z), and so the graph of the scaled and
shifted version used in the deﬁnition of the GELU, 1
2 + 1
2erf

z
√
2

, looks very similar
to the graph of the logistic function (2.10). Like the SWISH, it crosses the origin
and behaves more like the ReLU the further we go away from 0 in either direction.
In smoothing the ReLU, all three of these activation functions introduce an intrinsic scale
and violate the scale-invariance condition (2.13).
2.3
Ensembles
As we discussed in §2.1, neural networks are trained rather than programmed. Practically
speaking, to begin training a neural network for function approximation, we need to set
initial values of the biases b(ℓ)
i
and weights W (ℓ)
ij .
Since the learned values of these
model parameters are almost always iteratively built up from their initial values, the
47

initialization strategy can have a major impact on the success or failure of the function
approximation.
Perhaps the simplest strategy would be to set all the biases and weights to zero,
b(ℓ)
i
= W (ℓ)
ij
= 0. However, this initialization fails to break the permutation symmetry
among the nℓdiﬀerent neurons in a hidden layer ℓ. If this symmetry isn’t broken, then we
cannot distinguish between the diﬀerent neurons in a layer as all these neurons perform
exactly the same computation. In eﬀect, the network would behave as if it only had
single neuron nℓ= 1 in each hidden layer. Thus, in order to leverage all the diﬀerent
components of the biases and weights in a wider network, we need to somehow break
the permutation symmetry.
Perhaps the simplest strategy that breaks this permutation symmetry is to sample
each bias and weight independently from some probability distribution. Theoretically
speaking, we should pick this initialization distribution so that the resulting ensem-
ble of networks are well behaved with respect to the function-approximation task. This
section initializes ourselves for analyzing such an ensemble.
Initialization distribution of biases and weights
Among the many potential reasonable choices for the initialization distribution, the
obvious choice is the Gaussian distribution.9 As we discussed, Gaussian distributions
are deﬁned solely in terms of their mean and variance, so they’re easy to specify and work
with in theory. They’re also extremely easy to sample from, which is also an essential
consideration when picking a sampling distribution in practice.
In particular, to initialize MLPs, we’ll independently sample each bias and each
weight from zero-mean Gaussian distributions with variances given by
E
h
b(ℓ)
i1 b(ℓ)
i2
i
= δi1i2C(ℓ)
b
,
(2.19)
E
h
W (ℓ)
i1j1W (ℓ)
i2j2
i
= δi1i2δj1j2
C(ℓ)
W
nℓ−1
,
(2.20)
respectively. Here the Kronecker deltas indicate that each bias and each weight are all
drawn independently from the others. Explicitly, the functional forms of these Gaussian
9Two other choices seen in the wild for the initialization distribution are the uniform distribution and
the truncated normal distribution. For the weights, the diﬀerence between the Gaussian distribution
and any other distribution – when the means are set zero and the variances are set equal – turns out
to be suppressed by 1/width for wide networks. That is, due to the central limit theorem, ultimately
only the ﬁrst and second moment – i.e. the mean and variance – for the weight initialization distribution
is of any real consequence. Thus, we might as well just use a Gaussian distribution. For the biases,
the diﬀerence between the Gaussian distribution and any other distribution is mostly moot in practice
because we shall ﬁnd that the bias variance C(ℓ)
b
should be set to zero for all good activation functions.
48

distributions are given by
p

b(ℓ)
i

=
1
q
2πC(ℓ)
b
exp
"
−
1
2C(ℓ)
b

b(ℓ)
i
2
#
,
(2.21)
p

W (ℓ)
ij

=
s nℓ−1
2πC(ℓ)
W
exp
"
−nℓ−1
2C(ℓ)
W

W (ℓ)
ij
2
#
.
(2.22)
Here the normalization of weight variances by 1/nℓ−1 is purely conventional but, as we
will show explicitly in §3 and §4, it is necessary for wide neural networks and natural
for comparing the behavior of networks with diﬀerent widths.10 Also note that we allow
the bias variance C(ℓ)
b
and rescaled weight variance C(ℓ)
W to potentially vary from layer to
layer. Together, the set of bias variances
n
C(1)
b , . . . , C(L)
b
o
and the set of rescaled weight
variances
n
C(1)
W , . . . , C(L)
W
o
are called initialization hyperparameters. One practical
result of our eﬀective theory approach will be prescriptions for setting these initialization
hyperparameters so that the output of the neural network is well behaved.
Induced distributions
Given a dataset D = {xi;α} consisting of ND input data, an MLP with model parameters
θµ =
n
b(ℓ)
i , W (ℓ)
ij
o
evaluated on D outputs an array of nL × ND numbers
fi(xα; θ) = z(L)
i
(xα) ≡z(L)
i;α ,
(2.23)
indexed by both neural indices i = 1, . . . , nL and sample indices α = 1, . . . , ND.
Each time we instantiate MLPs by drawing model parameters θµ from the initialization
distribution p(θ), we get a diﬀerent initial set of outputs z(L)
i;α . It follows that since the
biases b(ℓ)
i
and weights W (ℓ)
ij
are random variables at initialization, then so must be the
network outputs z(L)
i;α . In this way, the initialization distribution induces a distribution
on the network outputs.
This output distribution p

z(L)D

controls the statistics of network outputs at
the point of initialization. In practice, the properties of this distribution are directly
related to how hard it is for a network to approximate its target function through iterated
adjustments of its model parameters. As such, having control over this distribution is
of signiﬁcant interest from a practitioner’s perspective. From a theorist’s perspective,
even though the initialization distribution for model parameters is simple by design, the
10We can trace this convention to the MLP iteration equation (2.5). To compute the preactivation
z(ℓ)
i;α = b(ℓ)
i
+ Pnℓ−1
j=1 W (ℓ)
ij σ(ℓ−1)
j;α
, we essentially add together nℓ−1 random weights. For large nℓ−1, the
normalization factor of 1/nℓ−1 in the variance – which is tantamount to normalizing each weight by
1/√nℓ−1 – essentially counteracts this summation of many zero-mean random numbers. Since there is
no such summation for the biases, there is no need for such a normalization factor.
49

induced output distribution is not. In theory, we need to calculate the following gigantic
integral over all the model parameters
p

z(L)D

=
Z 

P
Y
µ=1
dθµ

p

z(L)θ, D

p(θ) .
(2.24)
Before performing this heroic integration, notice that the conditional distribution
p

z(L)θ, D

in the integrand (2.24) is actually deterministic. In other words, if we know
the set of inputs D and the settings of all the model parameters θµ, then we know how
to compute the network outputs: we just use the iteration equation (2.5) that deﬁnes
the MLP. What we don’t yet know is how to express this determinism as a distribution.
Deterministic distributions and the Dirac delta function
What kind of a probability distribution is deterministic? Let’s abstractly denote such
a distribution as p(z|s) = δ(z|s), which intend to encode the deterministic relationship
z = s. What properties should this distribution have? First, the mean of z should be s
E [z] =
Z
dz δ(z|s) z ≡s .
(2.25)
Second, the variance should vanish, since this is a deterministic relationship. In other
words,
E[z2] −(E [z])2 =
Z
dz δ(z|s) z2

−s2 ≡0 ,
(2.26)
or, equivalently,
Z
dz δ(z|s) z2 ≡s2.
(2.27)
In fact, this determinism implies an even stronger condition. In particular, the expecta-
tion of any function f(z) of z, should evaluate to f(s):
E [f(z)] =
Z
dz δ(z|s) f(z) ≡f(s) ,
(2.28)
which includes the properties (2.25) and (2.27) as special cases when f(z) = z and
f(z) = z2, respectively, as well as the probability normalization condition
Z
dz δ(z|s) = 1 ,
(2.29)
when f(z) = 1.11 In fact, (2.28) is the deﬁning property of the Dirac delta function.12
11A random variable that obeys E [f(z)] = f(E [z]) is said to self-average, meaning that we can
exchange the order of the expectation with the function evaluation. The condition (2.28) is equivalent
to saying that the distribution δ(z|s) is self-averaging.
12The Dirac delta function is really a generalization of the Kronecker delta (1.26) for continuous
variables. In this footnote we also include the obligatory disclaimer that – despite its name – the Dirac
delta function is a distribution and not a function, as should have been clear from our discussion. Despite
this, we will stick with common convention and continue to refer to it as the Dirac delta function.
50

As a representation though, (2.28) is a little too abstract, even for us. However,
our discussion above paves the way for a much more concrete representation. Since the
Dirac delta function is a normalized distribution (2.29) with mean s (2.25) and zero
variance (2.26), let’s consider a normalized Gaussian distribution with mean s (1.9) and
take the limit as the variance K goes to zero:
δ(z|s) ≡
lim
K→+0
1
√
2πK
e−(z−s)2
2K
.
(2.30)
This distribution is inﬁnitely peaked at z = s while vanishing everywhere else, so any
function f(z) integrated against (2.30) will give f(s) after taking the limit. In other
words, it satisﬁes the deﬁning property of the Dirac delta function (2.28).
The limit in (2.30) should always be taken after integrating the distribution against
some function. Having said that, perhaps this representation still makes you a little bit
uncomfortable as it is still a very singular limit. Let’s try to ﬁx this and ﬁnd a yet even
better representation. Here’s a magic trick: starting from (2.30), let’s insert “1” on the
right hand side as
δ(z|s) = lim
K→+0
1
√
2πK
e−(z−s)2
2K
(
1
p
2π/K
Z ∞
−∞
dΛ exp
"
−K
2

Λ −i(z −s)
K
2#)
(2.31)
= lim
K→+0
1
2π
Z ∞
−∞
dΛ exp

−1
2KΛ2 + iΛ(z −s)

,
where in the curly brackets we inserted an integral over a dummy variable Λ of a nor-
malized Gaussian with variance 1/K and imaginary mean i(z −s)/K, and on the second
line we simply combined the exponentials. Now we can easily take the limit K →+0 to
ﬁnd an integral representation of the Dirac delta function
δ(z|s) = 1
2π
Z ∞
−∞
dΛ eiΛ(z−s) ≡δ(z −s) .
(2.32)
In this ﬁnal expression, we noted that the function depends only on the diﬀerence z −s.
This integral representation will come in handy in §4.
Induced distributions, redux
Now that we are familiar with the Dirac delta function, we can use it to express the
output distribution (2.24) more concretely. To start, for a one-layer network of depth
L = 1, the distribution of the ﬁrst layer output (2.5) is given by
p

z(1)D

=
Z " n1
Y
i=1
db(1)
i
p

b(1)
i
# 

n1
Y
i=1
n0
Y
j=1
dW (1)
ij
p

W (1)
ij



(2.33)
×


n1
Y
i=1
Y
α∈D
δ

z(1)
i;α −b(1)
i
−
n0
X
j=1
W (1)
ij xj;α



.
51

Here, we needed n1 × ND Dirac delta functions, one for each component of z(1)
i;α. In §4.1
we will explicitly evaluate the above integrals, though you should feel free to do so now
on your own, if you’re impatient. In passing, let us also introduce a cousin of (2.33)
p

z(ℓ+1)z(ℓ)
=
Z "nℓ+1
Y
i=1
db(ℓ+1)
i
p

b(ℓ+1)
i
# 

nℓ+1
Y
i=1
nℓ
Y
j=1
dW (ℓ+1)
ij
p

W (ℓ+1)
ij



(2.34)
×


nℓ+1
Y
i=1
Y
α∈D
δ

z(ℓ+1)
i;α
−b(ℓ+1)
i
−
nℓ
X
j=1
W (ℓ+1)
ij
σ

z(ℓ)
j;α




,
which determines the distribution of the preactivations in the (ℓ+1)-th layer, conditioned
on the preactivations in the ℓ-th layer, after integrating out the model parameters.
More generally, for any parameterized model with output zout
i;α ≡fi(xα; θ) for i =
1, . . . , nout and with the model parameters θµ distributed according to p(θ), the output
distribution (2.24) can be written using the Dirac delta function as
p

zoutD

=
Z 

P
Y
µ=1
dθµ

p(θ)
"nout
Y
i=1
Y
α∈D
δ

zout
i;α −fi(xα; θ)
#
.
(2.35)
Our pretraining was designed precisely to prepare ourselves for performing this integral
for MLPs.
52

Chapter 3
Eﬀective Theory of Deep Linear
Networks at Initialization
. . . a system which has spherical symmetry . . . certainly cannot result in an organism
such as a horse, which is not spherically symmetrical.
Alan Turing, on the limitations of toy models [24].
In this ﬁnal warm-up chapter, we introduce and then solve a toy model of deep learning,
the deep linear network.1 As will be explained in §3.1, the deep linear network is
simply an MLP with linear activation functions. In particular, such a network can only
compute linear transformations of its inputs and certainly cannot result in a function
such as a human, which is empirically known to be nonlinear. Nonetheless, the study
of deep linear networks will serve as a useful blueprint for an eﬀective theory of deep
learning that we will develop more generally over the subsequent chapters. Speciﬁcally,
the exercises in this chapter illustrate how layer-to-layer recursions control the statistics
of deep neural networks in a very intuitive way, without getting bogged down by all the
technical details.
To that end, in §3.2 we obtain and then exactly solve a layer-to-layer recursion for
the two-point correlator of preactivations in deep linear networks. The result highlights
that the statistics of the network sensitively depend on the setting of the initialization
hyperparameters, with the sensitivity increasing exponentially with depth. This leads to
the important concept of criticality, which we will explore in §5 in greater depth and sen-
sitivity. In short, we learn that for networks to be well behaved, these hyperparameters
need to be ﬁnely tuned.
Next, in §3.3 we obtain and then solve a layer-to-layer recursion for the four-point
correlator, albeit for a single input to further simplify the algebra. This showcases the
way in which the behavior of the network can depend on the architecture hyperparam-
eters, particularly the width and depth of the network. In addition, we interpret the
1For physicists, we give an analogy: the deep linear network is to deep learning as the simple harmonic
oscillator is to quantum mechanics.
53

four-point connected correlator as a measurement of the ﬂuctuation of the network func-
tion from draw to draw of the model parameters. Such ﬂuctuations can interfere with the
tuning of the initialization hyperparameters and need to be controlled so that networks
behave reliably for typical draws. The scale of the ﬂuctuations are set by the depth-to-
width ratio of the network, highlighting this important emergent scale in the analysis
of MLPs, and we’ll see that the ﬂuctuations can be kept under control by keeping the
depth-to-width ratio of the network suﬃciently small.
Finally, in §3.4 we obtain a recursion for an arbitrary M-point correlator for a deep
linear network evaluated on a single input. Such recursions are all exactly solvable at any
width n and depth L, meaning we can fully determine the statistics of these networks at
initialization.2 Given these nonperturbative solutions, we take the limit of large width,
with ﬁxed depth, and the limit of large depth, with ﬁxed width, and show explicitly that
these two limits do not commute. We also construct an interpolating solution with both
large width and large depth, but ﬁxed depth-to-width ratio L/n, and see how this scale
serves as a perturbative parameter that controls all the interactions in the network and
controls the validity of the perturbative analysis.
3.1
Deep Linear Networks
A deep linear network iteratively transforms an input xi;α through a sequence of simple
linear transformations
z(ℓ+1)
i;α
= b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
z(ℓ)
j;α ,
(3.1)
with z(0)
i;α ≡xi;α and z(ℓ)
i;α ≡z(ℓ)
i (xα). Since the linear activation function is the identity
function, σ(z) = z, there’s no distinction here between preactivations and activations.
In this chapter, we’ll simplify matters a bit by turning oﬀall the biases, b(ℓ)
i
= 0, so
that the preactivations in layer ℓare simply given by a repeated matrix multiplication
of weight matrices as
z(ℓ)
i;α =
n0
X
j0=1
n1
X
j1=1
· · ·
nℓ−1
X
jℓ−1=1
W (ℓ)
ijℓ−1W (ℓ−1)
jℓ−1jℓ−2 · · · W (1)
j1j0xj0;α ≡
n0
X
j=1
W(ℓ)
ij xj;α .
(3.2)
Here we have introduced an nℓ-by-n0 matrix
W(ℓ)
ij =
n1
X
j1=1
· · ·
nℓ−1
X
jℓ−1=1
W (ℓ)
ijℓ−1W (ℓ−1)
jℓ−1jℓ−2 · · · W (1)
j1j ,
(3.3)
which highlights the fact that the preactivation at the ℓ-th layer is simply a linear
transformation of the input. Additionally, let us set C(ℓ)
W ≡CW so that the order-one
2This notion of solve should not be confused with the solving of the training dynamics for a particular
learning algorithm.
In the context of deep linear networks, the dynamics of gradient descent were
analyzed in [25]. In §10 and §∞, we will solve the training dynamics of gradient descent for MLPs with
general activation functions in the context of our eﬀective theory formalism.
54

part of the weight variance is layer independent.
All together, this means that the
initialization distribution over the weights is characterized by the following expectations
E
h
W (ℓ)
ij
i
= 0 ,
E
h
W (ℓ)
i1j1W (ℓ)
i2j2
i
= δi1i2δj1j2
CW
nℓ−1
.
(3.4)
Somewhat counterintuitively, deep linear networks generically represent a smaller set
of functions than fully general linear transformations, a.k.a. one-layer networks of the
same input-output dimensions.3
As an extreme example, let’s take a two-layer deep
linear network in which the ﬁrst hidden layer consists of a single neuron n1 = 1 and
consider the network output in the second layer ℓ= 2. In this case, all the information
in the input is compressed through a bottleneck into a single number in the ﬁrst layer
before being converted into an n2-dimensional vector in the output layer. Surely, such
a deep linear network represents a tinier subspace of linear transformations than those
given by all the possible n2-by-n0 matrices, so long as n0, n2 > 1.
More importantly, we will show that the statistics of deep linear networks at initial-
ization are also very diﬀerent from those of one-layer networks. In particular, while the
statistics of each W (ℓ)
ij
are given by a simple Gaussian distribution, the statistics of their
product W(ℓ)
ij
are non-Gaussian, depending in a complicated way on the depth ℓand
widths n1, . . . , nℓof the network.
The goal of the rest of this chapter is to exactly work out this dependence. Concretely,
we are going to compute the nontrivial distribution
p

z(ℓ)D

≡p

z(ℓ) (x1) , . . . , z(ℓ) (xND)

,
(3.5)
of the preactivations z(ℓ)
i;α ≡z(ℓ)
i (xα) implied by the iterated multiplication (3.2) when
evaluated on the entire dataset D. As mentioned in §1.2, a distribution is completely
determined by the set of all its M-point correlators, and so our method for determining
p

z(ℓ)D

will be to directly compute these correlators.
Before moving onto the next section, let’s consider the simplest observable, the mean
of the preactivation z(ℓ)
i;α. Taking an expectation of the deﬁning equation (3.2), it’s easy
to see that the mean preactivation must vanish at any layer:
E
h
z(ℓ)
i;α
i
=
n0
X
j0=1
n1
X
j1=1
· · ·
nℓ−1
X
jℓ−1=1
E
h
W (ℓ)
ijℓ−1W (ℓ−1)
jℓ−1jℓ−2 · · · W (1)
j1j0xj0;α
i
(3.6)
=
n0
X
j0=1
n1
X
j1=1
· · ·
nℓ−1
X
jℓ−1=1
E
h
W (ℓ)
ijℓ−1
i
E
h
W (ℓ−1)
jℓ−1jℓ−2
i
· · · E
h
W (1)
j1j0
i
xj0;α = 0 ,
3This is not necessarily a bad thing, since there are often both computational and representational
advantages to focusing on a specialized class of functions.
For instance, we saw that convolutional
networks represent a much smaller set of functions than MLPs, and yet they are known to perform
better on computer vision tasks due to their translational-invariance-respecting inductive bias as well
as the fact that they require signiﬁcantly less computation due to their sparse pattern of connections.
Having said that, it’s not obvious if deep linear networks have a useful inductive bias when compared to
general linear transformations.
55

since the weight matrices are mutually independent – and independent of the input –
and have zero mean (3.4). By a similar argument, it’s easy to see that any odd-point
correlator of preactivations will vanish as well. Thus, going forward, we will only have
to concern ourselves with the even-point correlators.
3.2
Criticality
Since the mean is trivial, the next simplest candidate for an interesting observable is
the two-point correlator E
h
z(ℓ)
i1;α1z(ℓ)
i2;α2
i
, which quantiﬁes the typical magnitudes of the
preactivations. We’ll ﬁrst go through the math, and then we’ll discuss the physics.
Math: recursion for the two-point correlator
Let’s start slowly by ﬁrst considering the two-point correlator in the ﬁrst layer. Using
the deﬁning equation (3.2) to express the ﬁrst-layer preactivations in terms of the inputs
as
z(1)
i;α =
n0
X
j
W (1)
ij xj;α ,
(3.7)
we can express the two-point correlator as
E
h
z(1)
i1;α1z(1)
i2;α2
i
=
n0
X
j1,j2=1
E
h
W (1)
i1j1xj1;α1W (1)
i2j2xj2;α2
i
(3.8)
=
n0
X
j1,j2=1
E
h
W (1)
i1j1W (1)
i2j2
i
xj1;α1xj2;α2
=
n0
X
j1,j2=1
CW
n0
δi1i2δj1j2xj1;α1xj2;α2 = δi1i2CW
1
n0
n0
X
j=1
xj;α1xj;α2 ,
where to go from the second line to the third line we Wick-contracted the two weights
and inserted the variance (3.4). Additionally, let us introduce the notation
G(0)
α1α2 ≡1
n0
n0
X
i=1
xi;α1xi;α2 ,
(3.9)
for the inner product of the two inputs, normalized by the input dimension n0. In terms
of this object, we can rewrite the ﬁrst-layer two-point correlator (3.8) as
E
h
z(1)
i1;α1z(1)
i2;α2
i
= δi1i2CW G(0)
α1α2 .
(3.10)
Next, we could mindlessly repeat the same exercise to get the two-point correlator
in any arbitrary layer, using the deﬁning equation (3.2) to express z(ℓ)
i;α in terms of the
input. Instead, in order to practice our recursive approach, let’s evaluate the two-point
correlator recursively. To do so, we inductively assume that the two-point correlator at
56

the ℓ-th layer is known and then derive the two-point correlator at the (ℓ+ 1)-th layer.
Using the iteration equation (3.1) with the bias set to zero, we ﬁnd
E
h
z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2
i
=
nℓ
X
j1,j2=1
E
h
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
z(ℓ)
j1;α1z(ℓ)
j2;α2
i
(3.11)
=
nℓ
X
j1,j2=1
E
h
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
i
E
h
z(ℓ)
j1;α1z(ℓ)
j2;α2
i
=δi1i2CW
1
nℓ
nℓ
X
j=1
E
h
z(ℓ)
j;α1z(ℓ)
j;α2
i
,
where to go from the ﬁrst line to the second line we used the fact that the weights W (ℓ+1)
of the (ℓ+ 1)-th layer are statistically independent from the preactivations z(ℓ) in the
ℓ-th layer, and to go from the second line to the third line we Wick-contracted the two
weights and substituted in the variance (3.4). Notice that at any layer, the two-point
correlator is proportional to the Kronecker delta δi1i2, vanishing unless the neural indices
i1 and i2 are the same. With that in mind, let us decompose the two-point correlator as
E
h
z(ℓ)
i1;α1z(ℓ)
i2;α2
i
≡δi1i2G(ℓ)
α1α2 ,
(3.12)
and introduce a generalization of the above notation (3.9) for an arbitrary layer ℓ. Mul-
tiplying this equation by δi1i2, summing over i1, i2 = 1, . . . , nℓand dividing it by nℓ, the
quantity G(ℓ)
α1α2 can also be expressed as
G(ℓ)
α1α2 = 1
nℓ
nℓ
X
j=1
E
h
z(ℓ)
j;α1z(ℓ)
j;α2
i
,
(3.13)
and can thus be thought of as the average inner-product of preactivations in the ℓ-th
layer, divided by the number of neurons in the layer nℓ. This inner product depends on
sample indices only and lets us interpret G(ℓ)
α1α2 ≡G(ℓ)(xα1, xα2) as the covariance of the
two inputs, xα1 and xα2, after passing through an ℓ-layer deep linear network.
With all this notation introduced and fully interpreted, it’s easy to see that the above
recursion (3.11) can be compactly summarized by
G(ℓ+1)
α1α2 = CW G(ℓ)
α1α2 ,
(3.14)
which describes how the covariance G(ℓ)
α1α2 evolves from layer to layer. Apparently, to
transform the covariance from layer ℓto layer ℓ+ 1, we simply multiply by the constant
CW . The initial condition G(0)
α1α2 is given by the inner product of the two inputs (3.9),
and the solution is an exponential
G(ℓ)
α1α2 = (CW )ℓG(0)
α1α2 ,
(3.15)
as is typical for a repeated application of matrix multiplication. Note that the factor of
the width nℓin the variance of the weights (3.4) nicely dropped out, indicating that this
was in fact the proper way to scale the variance.
57

Physics: criticality
Already at this point our analysis illustrates an interesting and very general phenomenon.
Considering the solution (3.15), generically one of two things happens. If CW > 1, the
covariance blows up exponentially, quickly being driven to a ﬁxed point G⋆
α1α2 = ∞for
all pairs of inputs and leading to a divergent network output. If CW < 1, the covariance
exponentially decays to a ﬁxed point G⋆
α1α2 = 0 for all pairs of inputs, quickly curtailing
any data dependence in the network output. Any time an observable approaches a value
exponentially quickly, we’ll refer to the limiting value as a trivial ﬁxed point. The
value G⋆
α1α2 = ∞associated with CW > 1 and the value G⋆
α1α2 = 0 associated with
CW < 1 are prime examples of a trivial ﬁxed point.
Exploring this further, ﬁrst note that the diagonal part of the covariance at the
output layer L estimates the typical magnitude of the output for a given input xi;α
G(L)
αα = E

1
nL
nL
X
j=1

z(L)
j;α
2

.
(3.16)
With this observable in mind, the aforementioned exponential behavior should immedi-
ately set oﬀalarm bells, signaling either some sort of numerical instability (CW > 1)
or loss of information (CW < 1). In addition, note that the target values for the diﬀer-
ent components of the network output are typically O(1) numbers, neither exponentially
large nor small. Such exponential behavior of the network should thus make it extremely
diﬃcult to learn to approximate the desired function. In this way, this exploding and
vanishing covariance problem is a baby version of the infamous exploding and vanishing
gradient problem – a well-known obstacle to gradient-based training of deep networks –
which we shall make more precise in §9.
However, we were actually a little too quick in our analysis before: what happens if
we tune the weight variance CW so that it’s precisely equal to 1? This is clearly a special
point in the hyperparameter space of initialization, separating the exponentially growing
solution from the exponentially decaying solution. Going back to the recursion (3.14),
we see that if CW = 1 then the covariance is ﬁxed G(ℓ)
α1α2 = G(0)
α1α2 ≡G⋆
α1α2, manifestly
preserving the full covariance of the input data even after passing through many layers
of the deep linear network. This is a bona ﬁde nontrivial ﬁxed point, as it doesn’t
exponentially trivialize the structure of input data. Thus, at least at this heuristic level
of analysis, choosing CW = 1 appears to be essential for preserving the structure of the
input data in a numerically stable manner. More generally, ﬂowing to a nontrivial ﬁxed
point seems to be a necessary condition for deep networks to do anything useful.
When we ﬁne-tune the initialization hyperparameters of a network so that the co-
variance avoids exponential behavior, we’ll call them critical initialization hyperpa-
rameters.4 For deep linear networks, the critical initialization hyperparameter CW = 1
separates two regimes, one with an exponentially growing covariance for CW > 1, and
4This word choice is motivated by the analogy to critical phenomena in statistical physics.
For
instance, consider the prototypical example: a magnet made of iron. At high temperature, the magnetic
moments – or spins – of the iron atoms point in random directions, leading to a paramagnetic phase
58

the other with an exponentially decaying covariance for CW < 1. When the weight
variance is tuned to criticality CW = 1, the network has a perfect self-similarity of the
covariance, preserving it exactly through the evolution from layer to layer.
In §5, we will extend our analysis of criticality to MLPs that use any particular
activation function. And, as shall be seen further on in §10 and §∞, tuning a network to
criticality is critical for any deep network to be well behaved and perform useful tasks –
at least without otherwise employing ad-hoc tricks to ensure that signals can propagate
stably.
3.3
Fluctuations
Recall from §1 that if a distribution is Gaussian and has a zero mean, then the covari-
ance completely speciﬁes the distribution. If the preactivation distribution p

z(ℓ)D

were Gaussian, this would mean that the critical tuning of the one initialization hyper-
parameter CW = 1 would be suﬃcient to ensure that any observable is well behaved.
However, if the distribution p

z(ℓ)D

is not Gaussian, then it’s not clear a priori whether
observables depending on higher-point connected correlators will be well behaved with
the same tuning. In principle, such observables could require other tunings of CW that
are incompatible with the critical setting CW = 1 for the covariance G(ℓ)
α1α2. To settle this
question, let’s look at the next simplest observable, the connected four-point correlator.
As before, we’ll go through the math ﬁrst and discuss the physics second.
In this section and the next, to simplify the algebra we’ll focus on correlators of
preactivations that are evaluated only on a single input xα = x. This is suﬃcient to
qualitatively highlight the importance of the higher-point correlators while letting us
avoid the interference of some annoying technical manipulations. Accordingly, in these
sections we will drop the sample indices on preactivations and denote the covariance as
G(ℓ)
2
≡G(ℓ)
αα = G(ℓ)(x, x) .
(3.17)
In the next chapter, we’ll consider the fully general case.
Math: recursion for the four-point correlator
As we did for the two-point correlator in the previous section, we’ll begin by working
out the four-point correlator in the ﬁrst layer and then next derive and solve a recur-
sion for the correlator in the deeper layers. First for the ﬁrst layer, using the deﬁning
without any coherent magnetic ﬁeld. By contrast, at low temperature, the spins instead try to collectively
orient in the same direction, leading to a ferromagnetic phase with coherent magnetic ﬁeld – think of
the ∩-shaped cartoon magnet that children play with. A critical temperature separates these two phases
of magnetism, and the magnet set to the critical temperature will exhibit very special behavior that is
neither paramagnetism nor ferromagnetism but known as self-similarity.
59

equation (3.7) with the sample index omitted, we have for the full four-point correlator
E
h
z(1)
i1 z(1)
i2 z(1)
i3 z(1)
i4
i
(3.18)
=
n0
X
j1,j2,j3,j4=1
E
h
W (1)
i1j1W (1)
i2j2W (1)
i3j3W (1)
i4j4
i
xj1xj2xj3xj4
=C2
W
n2
0
n0
X
j1,j2,j3,j4=1
(δi1i2δj1j2δi3i4δj3j4 + δi1i3δj1j3δi2i4δj2j4 + δi1i4δj1j4δi2i3δj2j3) xj1xj2xj3xj4
=C2
W (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3)

G(0)
2
2 .
where to go from line two to line three, we made three distinct pairings for the two Wick
contractions of the four weights, and then used the weight variance (3.4) to evaluate
each contraction. To get to the ﬁnal line, we evaluated the sums over the j indices and
then substituted using our deﬁnition of the inner product (3.9), which for a single input
simply reads
G(0)
2
= 1
n0
n0
X
j=1
xjxj .
(3.19)
Comparing this result (3.18) with the two-point correlator in the ﬁrst layer (3.10), we
note that this answer is precisely what we’d expect for the full four-point correlator if
the preactivation distribution were exactly Gaussian. Thus, deep linear networks appear
to be simply Gaussian after a single layer, at least at the four-point correlator level of
analysis.5
This Gaussianity does not hold in deeper layers. To see that, let’s derive and solve a
recursion for the four-point correlator. Beginning with the iteration equation (3.1) with
zero bias, we ﬁnd
E
h
z(ℓ+1)
i1
z(ℓ+1)
i2
z(ℓ+1)
i3
z(ℓ+1)
i4
i
(3.20)
=
nℓ
X
j1,j2,j3,j4=1
E
h
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
i
E
h
z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j3 z(ℓ)
j4
i
=C2
W
n2
ℓ
nℓ
X
j1,j2,j3,j4=1
(δi1i2δj1j2δi3i4δj3j4 + δi1i3δj1j3δi2i4δj2j4 + δi1i4δj1j4δi2i3δj2j3)
× E
h
z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j3 z(ℓ)
j4
i
=C2
W (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3) 1
n2
ℓ
nℓ
X
j,k=1
E
h
z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
i
,
where on the second line we used the independence of the (ℓ+ 1)-th-layer weights from
the ℓ-th-layer preactivations, on the third line we again made three distinct pairings for
5In the next chapter, we’ll show very generally that the preactivation distribution is always Gaussian
in the ﬁrst layer.
60

the two pairs of Wick contractions of the four weights, and on the last line we made
judicious use of the Kronecker deltas to collapse the sums.
Now, we see from this recursion that at any layer the full four-point correlator is
proportional to the factor (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3), a ﬁxed tensor structure that
speciﬁes the neural-index dependence of the correlator. Thus by decomposing the full
four-point correlator as
E
h
z(ℓ)
i1 z(ℓ)
i2 z(ℓ)
i3 z(ℓ)
i4
i
≡(δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3) G(ℓ)
4 ,
(3.21)
we can put all of the layer dependence into this simpler object G(ℓ)
4
and not worry about
neural indices in our recursion. In terms of this decomposition, the result (3.18) for the
correlator in the ﬁrst layer becomes
G(1)
4
= C2
W

G(0)
2
2 ,
(3.22)
and the ﬁnal factor in the above recursion (3.20) can be rewritten as
1
n2
ℓ
nℓ
X
j,k=1
E
h
z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
i
= 1
n2
ℓ
nℓ
X
j,k=1
(δjjδkk + δjkδjk + δjkδkj) G(ℓ)
4
=

1 + 2
nℓ

G(ℓ)
4 .
(3.23)
Using this, the entire recursion above (3.20) can be rewritten simply as a recursion for
G(ℓ)
4
as
G(ℓ+1)
4
= C2
W

1 + 2
nℓ

G(ℓ)
4 .
(3.24)
This recursion, with the initial condition set by (3.22), has a simple solution
G(ℓ)
4
=C2ℓ
W
" ℓ−1
Y
ℓ′=1

1 + 2
nℓ′
# 
G(0)
2
2
(3.25)
=
" ℓ−1
Y
ℓ′=1

1 + 2
nℓ′
# 
G(ℓ)
2
2 ,
where in the ﬁnal line we substituted in the solution (3.15) for the covariance. Now let’s
extract some physics from this compact formula.
Physics: large-n expansion, non-Gaussianities, interactions, and ﬂuctuations
To start, we note that the four-point correlator (3.25) drastically simpliﬁes in the limit
of an inﬁnite number of neurons per hidden layer (nℓ→∞).
In such a limit, the
solution (3.25) degenerates to
G(ℓ)
4
=

G(ℓ)
2
2 ,
(3.26)
and the full four-point correlator (3.21) becomes
E
h
z(ℓ)
i1 z(ℓ)
i2 z(ℓ)
i3 z(ℓ)
i4
i
= (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3)

G(ℓ)
2
2 .
(3.27)
61

This is exactly what we’d ﬁnd if the preactivation distribution were Gaussian: the
four-point correlator is determined entirely by the two-point correlator, with the tensor
structure determined by Wick’s theorem. In fact, as we will show in the next chapter, for
any MLP with any particular choice of a nonlinear activation function, the preactivation
distribution is governed by Gaussian statistics in this inﬁnite-width limit, implying no
interactions between the neurons in such a limit.
However, despite the rather large
computational resources that big tech can throw at machine-learning problems, realistic
MLPs simply do not have an inﬁnite number of neurons per layer. To understand such
realistic MLPs, we’ll have to back oﬀthis inﬁnite-width limit.
To illustrate this most clearly, let’s set all the hidden layer widths to be equal n1 =
n2 = . . . = nL−1 ≡n. Then, evaluating (3.25), the deviation from the inﬁnite-width
limit at the level of four-point correlator statistics is encoded by the diﬀerence
G(ℓ)
4
−

G(ℓ)
2
2 =
"
1 + 2
n
ℓ−1
−1
# 
G(ℓ)
2
2
(3.28)
=2(ℓ−1)
n

G(ℓ)
2
2 + O
 1
n2

,
where in the last line we expanded in 1/n and kept the leading correction to the inﬁnite-
width limit.6 In particular, at criticality where G(ℓ)
2
is constant, this leading correction
(3.28) scales inversely proportionally with the width and proportionally with the depth.
Thus, the deviation from inﬁnite width is proportional to the depth-to-width ratio of the
network, our ﬁrst encounter with this important emergent scale. There are multiple
ways to think about this ﬁnite-width correction.
First, the connected four-point correlator (1.54) is given by
E
h
z(ℓ)
i1 z(ℓ)
i2 z(ℓ)
i3 z(ℓ)
i4
i 
connected = (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3)

G(ℓ)
4
−

G(ℓ)
2
2
,
(3.29)
which directly connects the diﬀerence (3.28) to our measure of non-Gaussianity for the
distribution. We see that the non-Gaussianity grows as the network deepens, and the
preactivation statistics in layer ℓare nearly-Gaussian so long as the emergent scale, the
depth-to-width-ratio, remains perturbatively small. From the action perspective, this
means that the quartic coupling changes – or runs – as the layer at which we consider
the preactivation distribution changes, with the coupling growing in proportion with
layer ℓ.
Second, in §1.3 we gave another interpretation for a nonzero connected four-point
correlator as measuring interactions – i.e. the breakdown of statistical independence –
between the diﬀerent components of the random vector. To be very speciﬁc, let us look
at a particular entry of the connected four-point correlator tensor with i1 = i2 = j and
i3 = i4 = k for j ̸= k. This entry can be expressed as
E
h
z(ℓ)
j z(ℓ)
j
−G(ℓ)
2
 
z(ℓ)
k z(ℓ)
k
−G(ℓ)
2
i
= G(ℓ)
4
−

G(ℓ)
2
2 ,
for j ̸= k .
(3.30)
6This approximation is valid so long as the depth of the network doesn’t grow too large. Stay tuned
for the analysis in the next section where we will discuss how this limit breaks down.
62

This shows that the deviation of z(ℓ)
j z(ℓ)
j
from its mean value E
h
z(ℓ)
j z(ℓ)
j
i
= G(ℓ)
2
on a
particular neuron j is correlated with the same deviation from the mean on a diﬀerent
neuron k. We can thus interpret the ﬁnite-width diﬀerence (3.28) as controlling intralayer
interactions between distinct neurons, with the strength of the interactions growing with
depth.
Third, we can see that some observables that are deterministic in the inﬁnite-width
limit start to ﬂuctuate at ﬁnite width. To this end, let us consider the simple observable
O(ℓ) ≡O

z(ℓ)
≡1
n
n
X
j=1
z(ℓ)
j z(ℓ)
j
,
for ℓ< L ,
(3.31)
which captures the average magnitude of the preactivations over all the diﬀerent neurons
in a hidden layer ℓfor a given instantiation of the network weights. Its mean over diﬀerent
realizations of the weights is given by the expectation
E
h
O(ℓ)i
= 1
n
n
X
j=1
E
h
z(ℓ)
j z(ℓ)
j
i
= G(ℓ)
2 ,
(3.32)
and the magnitude of this observable’s ﬂuctuation from instantiation to instantiation is
measured by its variance
E

O(ℓ) −E
h
O(ℓ)i2
= 1
n2
n
X
j,k=1
E
h
z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
i
−

G(ℓ)
2
2
(3.33)
= 1
n2
n
X
j,k=1
(δjjδkk + δjkδjk + δjkδkj) G(ℓ)
4
−

G(ℓ)
2
2
= 2
nG(ℓ)
4
+

G(ℓ)
4
−

G(ℓ)
2
2
=2ℓ
n

G(ℓ)
2
2 + O
 1
n2

,
where in the last step we recalled the expansion (3.28) for the ﬁnite-width diﬀerence. As
promised, O(ℓ) is deterministic at inﬁnite width, since this variance is suppressed by 1/n
and vanishes identically in the inﬁnite-width limit. However, as we back oﬀthe inﬁnite-
width limit, the variance grows linearly with depth at criticality due to the ﬁnite-width
correction (3.28). As such depth increases, the ﬂuctuation becomes larger, meaning that
the typical magnitude of the preactivations O(ℓ) measured on any given realization of
the deep linear network may deviate more from the mean value E
h
O(ℓ)i
= G(ℓ)
2 .
All these ﬁnite-width eﬀects – be they non-Gaussianities, intralayer interactions, or
ﬁnite-width ﬂuctuations – are proportional to the depth-to-width ratio of the network.
This is perhaps the most important recurring theme of the book: the leading ﬁnite-
width contributions at criticality grow linearly with depth, despite being suppressed by
the inverse of the layer widths. Since the depths of a real networks with at least one
hidden layer are bounded from below as L ≥2 – that is, at minimum such networks
63

have one hidden layer and one output layer – in practice, networks of any ﬁnite size will
express some amount of ﬁnite-width eﬀects in their output distribution proportional to
their aspect ratio L/n. As we will see later in §5, this emergent scaling will hold very
generally at criticality for networks with any particular activation function.
Thus, the deeper a network is, the less the inﬁnite-width Gaussian description will
apply, due to accumulation of ﬁnite-width ﬂuctuations. This is actually a good thing
because, as we shall emphasize more in §11, inﬁnite-width networks do not have corre-
lations among neurons within a layer and cannot learn nontrivial representations from
input data. Real useful deep learning systems that are used in practice do both of these
things, and our later analysis will show that deeper networks have the capacity to do
more of these things.
Depth, however, is a double-edged sword. As the overall depth L of a network be-
comes comparable to its hidden-layer width, ﬂuctuations can begin to dominate.
In
particular, such extremely deep networks will have a huge variation in observables from
instantiation to instantiation. Thus, even if we choose the critical initialization hyperpa-
rameter CW = 1, in some instantiations signals blow up, in other instantiations signals
decay, and rarely do they stay tamed to be of order one. From a practical point of view,
these networks are pretty useless.
This set of circumstances is actually very fortuitous from a theorist’s vantage point:
our eﬀective theory of deep learning is most accurate when the aspect ratio L/n of the
network is small but nonzero – due to the applicability of the perturbative large-width
expansion – and this is exactly the setting of these architecture hyperparameters where
networks work best in practice. In fact, one could expect that balancing the utility of
nonzero depth for learning features against the cost of growing ﬂuctuations could result
in some optimal aspect ratio L/n for MLPs of a particular activation, just as we saw
that there is a correct tuning for the initialization hyperparameter CW for deep linear
networks.
We will return to this question of tuning L/n when we discuss inductive
bias in §6 after ﬁrst redoing our analysis of criticality and ﬂuctuations for arbitrary
activation functions in the following two chapters, §4 and §5. In particular, in these
chapters we will understand how the statistics of the preactivations run with depth, and
see the emergence of the depth-to-width ratio as a scale that controls the validity of the
perturbative 1/n expansion, as was the case here for deep linear networks.
Quite generally, in the regime where perturbation theory works, the ﬁnite-width
corrections grow linearly – not exponentially – with depth and the network remains well
behaved. By contrast, when the depth-to-width ratio becomes large, perturbation theory
breaks down, making it very diﬃcult to analyze such networks. However, in the special
case of deep linear networks a nonperturbative analysis is possible. In the next section
we’ll illustrate explicitly what happens to deep linear networks when the depth-to-width
ratio grows very large in order to paint an intuitive picture of the way networks behave
in this regime.
64

3.4
Chaos
In the last two sections we used the method of Wick contractions to derive recursions for
the two-point and four-point correlators of deep linear networks, which we then easily
solved. Now, we will use this same method to compute all the higher-point correlators
in order to complete our goal of determining the full distribution p

z(ℓ)D

. So that
we may ﬁrst simplify the algebra and then focus on the interesting properties of this
distribution, we’ll again evaluate the correlators only on a single input xα = x and drop
the sample indices in all of the following equations. Math then physics.
Math: recursions for six-point and higher-point correlators
Starting with the ﬁrst layer, let’s compute a general 2m-point full correlator. As this
involves many Wick contractions, it might be helpful to remind yourself of the formal
statement of Wick’s theorem by ﬂipping back to §1.1 and consulting (1.45). . . . Good.
Now, using the deﬁning equation (3.7) to express the ﬁrst-layer preactivations in
terms of the input, we get
E
h
z(1)
i1 z(1)
i2 · · · z(1)
i2m−1z(1)
i2m
i
(3.34)
=
n0
X
j1,...,j2m=1
E
h
W (1)
i1j1W (1)
i2j2 · · · W (1)
i2m−1j2m−1W (1)
i2mj2m
i
xj1xj2 · · · xj2m−1xj2m
=


X
all parings
δik1ik2 · · · δik2m−1ik2m

Cm
W

G(0)
2
m
=


X
all parings
δik1ik2 · · · δik2m−1ik2m



G(1)
2
m ,
where, as before we used Wick’s theorem to determine the Wick contractions and then
evaluated each contraction by substituting in (3.4) for the variance. Here, the sum is over
all the possible pairing of the 2m auxiliary indices, k1, . . . , k2m, resulting in (2m −1)!!
distinct terms, and on the ﬁnal line we substituted in the solution (3.15) for the ﬁrst-layer
covariance.
The result (3.34) conﬁrms what we suspected in the last section, that the preac-
tivation distribution for the ﬁrst layer is completely Gaussian.
If this isn’t clear by
inspection, it’s easy to check directly – via basically the same application of Wick’s the-
orem – that the correlators (3.34) are precisely the 2m-point correlators of a Gaussian
distribution with zero mean and variance δi1i2G(1)
2 . In other words, the preactivation
distribution in the ﬁrst layer is governed by the quadratic action
S

z(1)
=
1
2G(1)
2
n1
X
i=1
z(1)
i
z(1)
i
.
(3.35)
65

Before presenting a recursion for general 2m-point correlators, let us work out the re-
cursion for the six-point correlator in detail. Beginning with the iteration equation (3.1)
with the bias set to zero, we ﬁnd
E
h
z(ℓ+1)
i1
z(ℓ+1)
i2
z(ℓ+1)
i3
z(ℓ+1)
i4
z(ℓ+1)
i5
z(ℓ+1)
i6
i
(3.36)
=
nℓ
X
j1,j2,j3,j4,j5,j6=1
E
h
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
W (ℓ+1)
i5j5
W (ℓ+1)
i6j6
i
E
h
z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j3 z(ℓ)
j4 z(ℓ)
j5 z(ℓ)
j6
i
=C3
W

δi1i2δi3i4δi5i6 + δi1i3δi2i4δi5i6 + δi1i4δi2i3δi5i6
+ δi1i2δi3i5δi4i6 + δi1i3δi2i5δi4i6 + δi1i5δi2i3δi4i6
+ δi1i2δi5i4δi3i6 + δi1i5δi2i4δi3i6 + δi1i4δi2i5δi3i6
+ δi1i5δi3i4δi2i6 + δi1i3δi5i4δi2i6 + δi1i4δi5i3δi2i6
+ δi5i2δi3i4δi1i6 + δi5i3δi2i4δi1i6 + δi5i4δi2i3δi1i6
 1
n3
ℓ
nℓ
X
i,j,k=1
E
h
z(ℓ)
i z(ℓ)
i z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
i
,
noting again the independence of the (ℓ+ 1)-th layer weights from the ℓ-th layer pre-
activations. On the ﬁnal line, we see that there were ﬁfteen distinct ways to make the
three Wick contractions of six weights.
As we saw for the four-point correlator, the structure of neural indices for the full
six-point correlator is the same for any layer and proportional to a constant tensor, given
by the object in the parenthesis above with all those Kronecker deltas. This suggests a
decomposition of the six-point correlator as
E
h
z(ℓ)
i1 z(ℓ)
i2 z(ℓ)
i3 z(ℓ)
i4 z(ℓ)
i5 z(ℓ)
i6
i
≡(δi1i2δi3i4δi5i6 + . . . + δi5i4δi2i3δi1i6) G(ℓ)
6 ,
(3.37)
with the neural-dependence encapsulated by that complicated sum-over-products of Kro-
necker deltas and the layer dependence captured solely by G(ℓ)
6 .
Now, to ﬁnd a recursion for G(ℓ)
6 , we need to perform the sum
1
n3
ℓ
nℓ
X
i,j,k=1
E
h
z(ℓ)
i z(ℓ)
i z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
i
(3.38)
after substituting in the decomposition (3.37). With the given pattern of neural indices,
there are really only three types of terms in the sum. In particular, there is one term
that looks like this
1
n3
ℓ
nℓ
X
i,j,k=1
δiiδjjδkk = 1 ,
(3.39)
six terms that look like this
1
n3
ℓ
nℓ
X
i,j,k=1
δijδjiδkk = 1
nℓ
,
(3.40)
66

and eight terms that look like this
1
n3
ℓ
nℓ
X
i,j,k=1
δijδjkδki = 1
n2
ℓ
.
(3.41)
Putting all these terms together, we ﬁnd a recursion for the layer-dependence of the full
six-point correlator
G(ℓ+1)
6
= C3
W
 
1 + 6
nℓ
+ 8
n2
ℓ
!
G(ℓ)
6 ,
(3.42)
which has a simple solution
G(ℓ)
6
=C3ℓ
W
" ℓ−1
Y
ℓ′=1
 
1 + 6
nℓ′ + 8
n2
ℓ′
!# 
G(0)
2
3
(3.43)
=
" ℓ−1
Y
ℓ′=1
 
1 + 6
nℓ′ + 8
n2
ℓ′
!# 
G(ℓ)
2
3 .
Here, we used the initial condition (3.34) G(0)
6
=

G(0)
2
3, and on the ﬁnal line we
substituted in our solution for the variance of a single input (3.15).
Similarly, we can decompose an arbitrary 2m-point correlator as
E
h
z(ℓ)
i1 z(ℓ)
i2 · · · z(ℓ)
i2m−1z(ℓ)
i2m
i
=


X
all parings
δik1ik2 · · · δik2m−1ik2m

G(ℓ)
2m ,
(3.44)
and use a similar set of manipulations to show that the layer dependence G(ℓ)
2m obeys a
recursion
G(ℓ+1)
2m
= c2m(nℓ) Cm
W G(ℓ)
2m ,
(3.45)
with the combinatorial factor c2m(n) given by
c2m(n) =

1 + 2
n
 
1 + 4
n

· · ·

1 + 2m −2
n

=
  n
2 −1 + m
!
  n
2 −1
!
 2
n
m
.
(3.46)
We included the explicit form of this factor only for completeness.
If you insist on
checking this factor, note that it reproduces the right combinatorial factors for 2m =
2, 4, 6, though we strongly suggest that you do not explicitly write out all of the terms
for any other particular value of m. Overall, this recursion is still just a simple sequence
of multiplications, with a simple solution
G(ℓ)
2m =
" ℓ−1
Y
ℓ′=1
c2m(n′
ℓ)
# 
G(ℓ)
2
m .
(3.47)
Enough with the math, time for the physics.7
7If you do want more math, check out [26] for an alternative derivation of these 2m-point correlators
and a nonperturbative expression for the distribution p z(ℓ)x
.
67

Physics: breakdown of perturbation theory and the emergence of chaos
Let’s play with this formula (3.47) a bit by taking various limits. For simplicity, let’s set
all the hidden layer widths to be equal n1 = n2 = . . . = nL−1 ≡n, and also focus only
on output distribution p

z(L)x

.
• On the one hand, if we send the network width to inﬁnity, n →∞, while keeping
the depth L ﬁxed, then all the combinatorial factors (3.46) become unity:
lim
n→∞c2m(n) = 1 .
(3.48)
In this inﬁnite-width limit, all the correlators (3.47) are given by their Gaussian
values
G(L)
2m =

G(L)
2
m ,
(3.49)
and the output distribution p

z(L)x

is precisely Gaussian. More generally, even
for multiple inputs the output distribution p

z(L)D

remains Gaussian, with co-
variance G(L)
α1α2 = CL
W

1
n0
Pn0
i=1 xi;α1xi;α2

. As this distribution is equivalent to
that of one-layer networks initialized with weight variance CL
W , we see that such
networks are not really deep after all.
• On the other hand, if we send the depth to inﬁnity, L →∞, while keeping the
width n ﬁxed, then all the combinatorial factors are ﬁxed and greater than one,
c2m > 1. This means that the higher-point correlators for 2m > 2 will all blow up
exponentially as
G(L)
2m =
h
c2m(n)
iL−1 
G(L)
2
m .
(3.50)
Note that this behavior persists even if we tune the network to criticality by
setting CW = 1 so that the two-point correlator is ﬁxed G(ℓ)
2
= G(0)
2 . This shows
explicitly how our large-width analysis from the last section can break down if the
network depth becomes too large. Furthermore, the distribution implied by these
correlators is extremely non-Gaussian, to say the least, and in practice the outputs
of these networks will ﬂuctuate chaotically from instantiation to instantiation.
Such networks are entirely unusable.
• Clearly these limits do not commute, i.e.,
lim
n→∞lim
L→∞G(L)
2m ̸= lim
L→∞lim
n→∞G(L)
2m .
(3.51)
However, we can construct an interpolating solution by sending both the width
and depth to inﬁnity, n, L →∞, while keeping their ratio ﬁxed:
r ≡L
n .
(3.52)
68

Noting that we can expand the combinatorial factors as
c2m(n) = 1 + 1
n
 m−1
X
s=1
2s
!
+ O
 1
n2

= 1 + m(m −1)
n
+ O
 1
n2

,
(3.53)
and then using the well-known formula for the exponential
lim
L→∞

1 + a
L + O
 1
L2
L
= ea ,
(3.54)
we can construct a limiting value for any correlator at a given value of m and ﬁxed
aspect ratio r:
G(L)
2m →em(m−1)r 
G(L)
2
m .
(3.55)
This solution interpolates between the two extreme limits: by sending r →0 we
recover the Gaussian limit (3.49), and by sending r →∞we recover the chaotic
limit (3.50) that demonstrates the breakdown of criticality.8
Let us play a tiny bit more with the last interpolating formula (3.55) at criticality
where G(L)
2
= G(0)
2 . Here, the ﬁnite-width diﬀerence (3.28) that governs the connected
four-point correlator (3.29) becomes
G(L)
4
−

G(L)
2
2 =

e2r −1
 
G(0)
2
2
(3.56)
= 2r

G(0)
2
2 + O

r2
.
This reproduces the running of the quartic coupling with the depth-to-width ratio (3.28).
Similarly, the corresponding quantity governing the layer dependence of the connected
six-point correlator (1.61) is given by
G(L)
6
−3G(L)
2 G(L)
4
+ 2

G(L)
2
3 =

e6r −3e2r + 2
 
G(0)
2
3
(3.57)
= 12r2 
G(0)
2
3 + O

r3
,
which scales like the depth-to-width ratio squared. Therefore, the connected six-point
correlator is even more suppressed than the connected four-point correlator for large
networks with suﬃciently small depth-to-width ratio r.
This is in accord with the
comments we made in §1.3: neural networks obey nearly-Gaussian statistics, and the
8This double-scaling limit corresponds to neglecting terms that scale like
L
n2 ,
L3
n5 ,
L120
n157 , etc., which
are all subleading when the depth and the width are large, n, L →∞, but their ratio r is ﬁxed.
Furthermore, there is a very subtle point in using this interpolating solution – albeit a theoretical
subtlety – when we consider not just a particular correlator at a given 2m, but the set of all the
correlators. Namely, for any ﬁnite n, L – no matter how big – there always exist higher-point correlators
for which the exponential approximation (3.55) is invalid because the factor of m(m −1) becomes too
big. That is, since we constructed this interpolating solution assuming ﬁxed m, such a solution can break
down if m is large enough.
69

connected correlators have a hierarchical structure. In particular, we see here that the
scaling of the correlators is controlled by the same small parameter r, with the higher-
point connected correlators suppressed by a higher power of that parameter. This means
that for small r, we should be able to consistently truncate our distribution and only
compute up to a ﬁxed order in r.
70

Chapter 4
RG Flow of Preactivations
“You can hide a lot in a large-N matrix.” – Steve Shenker – John McGreevy [27].
At the end of the last chapter, we computed the statistics of preactivations for deep
linear networks at initialization and saw them run as a function of the network depth.
For that toy model, using a handful of Wick contractions and the recursive structure of
the network architecture, we were able to fully understand the eﬀects of the network’s
hyperparameters – its initialization scheme, width, and depth – on preactivation cor-
relators. This exercise in particular highlighted the importance of critical initialization
hyperparameters and suﬃciently small depth-to-width ratio in order for the network out-
puts to be well-behaved, theoretically and practically. To extend these insights beyond
deep linear networks, we need to develop an eﬀective theory of deep learning for networks
with any activation function.
While ultimately the goal of our eﬀective theory is to explain how a particular neural
network learns from a given dataset, our immediate goal in §4 and §5 will be to under-
stand how an ensemble of neural networks at initialization behaves as a function of data.
In §10, §11, and §∞, we’ll ﬁnd that these goals are closely tied together: through the
judicious study of the ensemble, we can systematically evaluate the typical behavior
of trained networks as well as how any particular network may ﬂuctuate away from
typicality. Our starting point will thus be a study of the statistics of neural-network
preactivations with Gaussian-initialized biases and weights. All in all, the formalism
developed in this chapter for analyzing the ensemble of networks at initialization will be
the key to a principled understanding of deep learning.
As stressed in the introduction, §0, our focus will always be on describing real ﬁnite-
width networks, since a lot is lost in idealized inﬁnite-width networks.
One salient
phenomenon lost in the inﬁnite-width limit is the increasing non-Gaussianity in the pre-
activation distributions of deeper layers. Such non-Gaussianity makes the behavior of
ﬁnite-width networks much richer but more complicated to analyze. In order to tame
these complications, we’ll need to borrow some tools from theoretical physics. In particu-
71

lar, physicists have a long tradition of ﬁnding simple descriptions of complicated systems
in the limit of a large number of degrees of freedom, while keeping in mind the true goal
of modeling real systems. In our context, this hints at tractability and simpliﬁcation in
the regime where networks become very wide, though not inﬁnitely so. To make this
precise, in this chapter we introduce the large-n expansion or 1/n expansion in order
to perform perturbative expansions when hidden-layer width n becomes parametrically
big. With this tool, we’ll be able to systematically study the preactivation distributions
of ﬁnite neural networks to arbitrary precision.1
As we did for deep linear networks, we will proceed recursively, investigating how the
distribution of preactivations changes from layer to layer by following the transformation
of inputs via the iterative MLP forward-pass equation. We start in §4.1 by computing the
distribution of preactivations in the ﬁrst layer, integrating out the ﬁrst set of weights
and biases. This procedure recovers a well-known result that the distribution of the
ﬁrst-layer preactivations is Gaussian.
Since this calculation is so central to the rest
of the chapter, we’ll present two diﬀerent derivations: a combinatorial derivation in
terms of Wick contractions and an algebraic derivation using the Hubbard-Stratonovich
transformation.
Next, in §4.2, we’ll consider the distribution of preactivations in the second layer and
see the emergence of non-Gaussianity in four-point and higher-point connected correla-
tors. The magnitude of these correlators is suppressed when the network is very wide,
vanishing in the strict inﬁnite-width limit. This suppression for wide networks in turn
enables us to write down an action describing the preactivation distribution, building on
the correspondence explored in §1 between such connected correlators and the couplings
in the action. In particular, the large-n expansion lets us start with the quadratic action
describing the Gaussian distribution in the inﬁnite-with limit and then perturbatively
expand around it in a series of the inverse width, 1/n, to arbitrary desired precision.
Given the importance of this result, we again provide two derivations, one based on Wick
contractions and the other based on expanding the stochastic metric.
Finally, in §4.3, we’ll analyze the distribution of preactivations at any depth. At this
point we can simply repurpose the calculations from the preceding sections to see how the
distribution of preactivations recursively transforms from the ℓ-th layer to the (ℓ+ 1)-
th layer. In particular, keeping the leading ﬁnite-width 1/n corrections, we’ll obtain
recursion equations for the two-point and four-point correlators, encoding how these
observables evolve with increasing depth. We’ll see that the preactivation distribution
1Back in 1996, Neal introduced the inﬁnite-width limit in a seminal work [28], focusing on single-
hidden-layer networks. Much later, this program was continued in [29, 30], extending the inﬁnite-width
limit to deeper networks, and then was extended further by Yaida in [31] to ﬁnite-width networks. A
large part of this chapter is focused on reproducing the recursions ﬁrst derived in [31].
However, our perspective here is diﬀerent than the one taken in this prior work. In particular, our
main motivation is in computing the distribution of preactivations at initialization, with an eye towards
ultimately understanding gradient-based training (§10, §11, §∞), rather than providing a starting point
for Bayesian inference. (We will give our own perspective on Bayesian learning for deep learning in §6.)
Additionally, in contrast to [31], our results here are derived by ﬁrst focusing on the couplings in the
action, rather than directly on the correlators of the distribution. This method is more intuitive and
can be more easily extended.
72

of the (ℓ+1)-th layer contains a nearly-Gaussian piece inherited from the ℓ-th layer as well
as an additional near-Gaussianity generated in the transition from the ℓ-th to (ℓ+ 1)-th
layer. In the next chapter, §5, we’ll see in detail how the near-Gaussianity accumulates
with depth by explicitly solving these recursions and analyzing their solutions, which
extends the notion of criticality and emergence of the depth-to-width ratio to networks
with general activation functions.
After a short clarifying section on some implications of marginalization (§4.4) and
a section on subleading corrections (§4.5), we take a step back in §4.6 in order to draw
a parallel between our formalism and the renormalization group in theoretical physics.
Renormalization group is a powerful recursive method for understanding complicated
interacting systems, capturing how the eﬀective interactions between the constituents of
a system change when the scale at which they are measured changes from microscopic to
macroscopic. Speciﬁcally, renormalization marginalizes over the microscopic degrees of
freedom in the system to yield an eﬀective coarse-grained description at long distances.
This is analogous to the way we recursively marginalize over preactivations in previous
layers to obtain an eﬀective description of a representation at the current layer, in our
case capturing how the interactions between neurons change with depth. In both cases
the ﬂow of the distributions is created by the marginalization of ﬁne-grained information.
Given the complete parallel, we will call our ﬂow representation group (RG) ﬂow.
If this sounds like a popular heuristic explanation for what deep neural networks do
– transforming ﬁne-grained information at the input level into coarser information at
the feature levels and ﬁnally into fully coarse-grained representation at the output level
– that’s because our formalism makes this heuristic picture of representation coarse-
graining concrete.2 Our formalism will further let us directly probe the eﬀect of the deep
in deep learning by tracking the change in preactivation distributions as we increase the
number of layers. Thus, it is the starting point for an eﬀective theory of deep learning,
which we will continue to develop throughout the book.
4.1
First Layer: Good-Old Gaussian
Given a dataset
D = {xi;α}i=1,...,n0; α=1,...,ND
(4.1)
containing ND inputs of n0-dimensional vectors, the preactivations in the ﬁrst layer are
given by
z(1)
i;α ≡z(1)
i
(xα) = b(1)
i
+
n0
X
j=1
W (1)
ij xj;α ,
for
i = 1, . . . , n1 .
(4.2)
2There have been many formal and informal comments on the connection between renormalization
and deep learning, but the relationship has never before been made precise.
73

At initialization the biases b(1) and weights W (1) are independently distributed according
to mean-zero Gaussian distributions with variances
E
h
b(1)
i b(1)
j
i
= δijC(1)
b
,
(4.3)
E
h
W (1)
i1j1W (1)
i2j2
i
= δi1i2δj1j2
C(1)
W
n0
.
(4.4)
The ﬁrst-layer preactivations z(1) = z(1)
i;α form an (n1ND)-dimensional vector, and we are
interested in its distribution at initialization,
p

z(1)D

= p

z(1) (x1) , . . . , z(1) (xND)

.
(4.5)
Note how this distribution depends conditionally on the input data, representing the
fact that the preactivations are functions of the input.
Now, let us compute the distribution of the ﬁrst-layer preactivations at initialization.
Since this will be so important, we give two derivations, one combinatorial and one
algebraic.
Wick this way: combinatorial derivation via correlators
The ﬁrst derivation involves direct application of Wick contractions to compute corre-
lators of the ﬁrst-layer distribution (4.5). Starting with the one-point correlator, simply
inserting the deﬁnition of the ﬁrst-layer preactivations (4.2) gives
E
h
z(1)
i;α
i
= E

b(1)
i
+
n0
X
j=1
W (1)
ij xj;α1

= 0 ,
(4.6)
since E
h
b(1)
i
i
= E
h
W (1)
ij
i
= 0. In fact, it’s easy to see that all the odd-point correlators of
p

z(1)D

vanish because there always is an odd number of either biases b(1) or weights
W (1) left unpaired under Wick contractions.
Next for the two-point correlator, again inserting the deﬁnition (4.2), we see
E
h
z(1)
i1;α1z(1)
i2;α2
i
= E



b(1)
i1 +
n0
X
j1=1
W (1)
i1j1xj1;α1



b(1)
i2 +
n0
X
j2=1
W (1)
i2j2xj2;α2




(4.7)
= δi1i2

C(1)
b
+ C(1)
W
1
n0
n0
X
j=1
xj;α1xj;α2

= δi1i2G(1)
α1α2 ,
where to get to the second line we Wick-contracted the biases and weights using (4.3)
and (4.4). We also introduced the ﬁrst-layer metric
G(1)
α1α2 ≡C(1)
b
+ C(1)
W
1
n0
n0
X
j=1
xj;α1xj;α2 ,
(4.8)
74

which is a function of the two samples, G(1)
α1α2 = G(1)(xα1, xα2), and represents the
two-point correlation of preactivations in the ﬁrst layer between diﬀerent samples.
The higher-point correlators can be obtained similarly. For instance, the full four-
point correlation can be obtained by inserting the deﬁnition (4.2) four times and Wick-
contracting the biases and weights, yielding
E
h
z(1)
i1;α1z(1)
i2;α2z(1)
i3;α3z(1)
i4;α4
i
(4.9)
=δi1i2δi3i4G(1)
α1α2G(1)
α3α4 + δi1i3δi2i4G(1)
α1α3G(1)
α2α4 + δi1i4δi2i3G(1)
α1α4G(1)
α2α3
=E
h
z(1)
i1;α1z(1)
i2;α2
i
E
h
z(1)
i3;α3z(1)
i4;α4
i
+ E
h
z(1)
i1;α1z(1)
i3;α3
i
E
h
z(1)
i2;α2z(1)
i4;α4
i
+ E
h
z(1)
i1;α1z(1)
i4;α4
i
E
h
z(1)
i2;α2z(1)
i3;α3
i
.
Note that the end result is same as Wick-contracting z(1)’s with the variance given
by (4.7). As we recall from §1, this can compactly be summarized by saying that the
connected four-point correlator vanishes,
E
h
z(1)
i1;α1z(1)
i2;α2z(1)
i3;α3z(1)
i4;α4
i 
connected = 0 .
(4.10)
Similar Wick combinatorics shows that all the full higher-point correlators can be ob-
tained simply by Wick-contracting z(1)’s with the variance given by (4.7), and hence all
the connected higher-point correlators vanish. This means that all correlators can be
generated from a Gaussian distribution with zero mean and the variance (4.7).
Then, in order to write down the ﬁrst-layer action, all we need is the inverse of this
variance, given by a matrix δi1i2Gα1α2
(1)
that satisﬁes
n1
X
j=1
X
β∈D

δi1jGα1β
(1)
 
δji2G(1)
βα2

= δi1i2δα1α2 ,
(4.11)
with the inverse of the ﬁrst-layer metric G(1)
α1α2 denoted as Gα1α2
(1)
and deﬁned by
X
β∈D
Gα1β
(1) G(1)
βα2 = δα1α2 .
(4.12)
Just as in §1, we follow the conventions of general relativity and suppress the superscript
“−1” for the inverse metric, distinguishing the metric G(1)
α1α2 and the inverse metric
Gα1α2
(1)
by whether sample indices are lowered or raised. With this notation, the Gaussian
distribution for the ﬁrst-layer preactivations is expressed as
p

z(1)D

= 1
Z e−S(z(1)) ,
(4.13)
with the quadratic action
S

z(1)
= 1
2
n1
X
i=1
X
α1,α2∈D
Gα1α2
(1)
z(1)
i;α1z(1)
i;α2 ,
(4.14)
75

and the partition function
Z =
Z 
Y
i,α
dz(1)
i;α

e−S(z(1)) =
2πG(1)
n1
2 ,
(4.15)
where
2πG(1) is the determinant of the ND-by-ND matrix 2πG(1)
α1α2 and, whenever we
write out a determinant involving the metric, it will always be that of the metric and
not of the inverse metric.3
Hubbard-Stratonovich this way: algebraic derivation via action
Rather than ﬁrst computing correlators and then backing out the distribution that gener-
ates them, we can instead work with the distribution directly. Let’s start with the formal
expression for the preactivation distribution (2.33) worked out in the last chapter4
p
 z
D
 =
Z "Y
i
dbi p(bi)
# 
Y
i,j
dWij p(Wij)

Y
i,α
δ

zi;α −bi −
X
j
Wijxj;α

,
(4.16)
where we have momentarily suppressed the layer superscripts “(1)” because it is dis-
tracting. At this point, we could try to eliminate some of the integrals over the model
parameters against the constraints imposed by the Dirac delta functions, but it’s easy to
get confused by the diﬀerent numbers of model-parameter integrals and delta-function
constraints.
To clarify matters, we import a neat trick from theoretical physics called the Hubbard-
Stratonovich transformation. Speciﬁcally, using the following integral representation
of the Dirac delta function (2.32)
δ(z −a) =
Z dΛ
2π eiΛ(z−a)
(4.17)
for each constraint and also plugging in explicit expressions for the Gaussian distributions
over the parameters, we obtain
p
 z
D
 =
Z "Y
i
dbi
√2πCb
# 
Y
i,j
dWij
p
2πCW /n0



Y
i,α
dΛ α
i
2π


(4.18)
× exp

−
X
i
b2
i
2Cb
−n0
X
i,j
W 2
ij
2CW
+ i
X
i,α
Λ α
i

zi;α −bi −
X
j
Wijxj;α



.
3N.B. compared to the generic quadratic action introduced in (1.66) where the random variable zµ
was a vector with a general index µ, here in (4.14) we’ve subdivided the general index into a pair of
indices, µ →(i, α), so that the ﬁrst-layer preactivation z(1)
i;α is a tensor with a neural index i and a sample
index α.
4For architectures other than MLPs, the expression inside the Dirac delta function would be diﬀer-
ent, but we expect much of the following to hold so long as the parameters are sampled from simple
distributions.
76

Completing the square in the exponential for both the biases b and weights W, we see
that the action is quadratic in the model parameters
−
X
i
b2
i
2Cb
−n0
X
i,j
W 2
ij
2CW
+ i
X
i,α
Λ α
i

zi;α −bi −
X
j
Wijxj;α


(4.19)
= −
1
2Cb
X
i
 
bi + iCb
X
α
Λ α
i
!2
−Cb
2
X
i
 X
α
Λ α
i
!2
−
n0
2CW
X
i,j
 
Wij + iCW
n0
X
α
Λ α
i xj;α
!2
−CW
2n0
X
i,j
 X
α
Λ α
i xj;α
!2
+ i
X
i,α
Λ α
i zi;α .
The biases and weights can then be integrated out, yielding an integral representation
for the ﬁrst-layer distribution p(z) as
Z 
Y
i,α
dΛ α
i
2π

exp

−1
2
X
i,α1,α2
Λ α1
i
Λ α2
i

Cb + CW
X
j
xj;α1xj;α2
n0

+ i
X
i,α
Λ α
i zi;α

.
(4.20)
In essence, we’ve so far traded the delta-function constraints and the model parameters
for the auxiliary Hubbard-Stratonovich variables Λ α
i , which have quadratic action and
a simple linear interaction with the preactivations zi;α.
Note that the inverse variance for the Hubbard-Stratonovich variables Λ α
i
is just the
ﬁrst-layer metric (4.8) we introduced in the Wick-contraction derivation,
C(1)
b
+ C(1)
W
X
j
xj;α1xj;α2
n0
= G(1)
α1α2 ,
(4.21)
where by now enough dust has settled that layer superscripts “(1)” have been restored.
Once again completing the square, the argument of the exponential becomes
−1
2
X
i,α1,α2

G(1)
α1α2

Λ α1
i
−i
X
β1
Gα1β1
(1) z(1)
i;β1



Λ α2
i
−i
X
β2
Gα2β2
(1) z(1)
i;β2

+ Gα1α2
(1)
z(1)
i;α1z(1)
i;α2

,
(4.22)
which ﬁnally lets us integrate out the Hubbard-Stratonovich variables Λ α
i
and recover
our previous result
p

z(1)D

=
1
2πG(1)
n1
2
exp

−1
2
n1
X
i=1
X
α1,α2∈D
Gα1α2
(1)
z(1)
i;α1z(1)
i;α2

.
(4.23)
As before,
2πG(1) represents the determinant of the matrix 2πG(1)
α1α2. The ﬁrst-layer
distribution is Gaussian with each neuron independent, and correlations between preac-
tivations for diﬀerent samples are encoded entirely in the metric G(1)
α1α2.
77

Gaussian action in action
Now that we’ve obtained an action representation for the distribution of the ﬁrst-layer
preactivations in two diﬀerent ways, let’s get a feel for how to compute with it. We’ll
start by computing the expectation of some quantities that will be needed in §4.2:
the expectation of two activations on the same neuron, E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2
i
, and the
expectation of four activations, E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2

σ

z(1)
i2;α3

σ

z(1)
i2;α4
i
, either with all
four on the same neuron i1 = i2 or with each pair on two separate neurons i1 ̸= i2.
Let’s start with the two-point correlator of activations. Using the deﬁnition of the
expectation and inserting the action representation of the distribution (4.23), we get
E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2
i
(4.24)
=
Z 

n1
Y
i=1
Q
α∈D dzi;α
q2πG(1)

exp

−1
2
n1
X
j=1
X
β1,β2∈D
Gβ1β2
(1) zj;β1zj;β2

σ(zi1;α1) σ(zi1;α2)
=



Y
i̸=i1
Z 

Q
α∈D dzi;α
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zi;β1zi;β2





×
Z 

Q
α∈D dzi1;α
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zi1;β1zi1;β2

σ(zi1;α1) σ(zi1;α2)
={1} ×


Z
Q
α∈D dzα
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zβ1zβ2

σ(zα1) σ(zα2)
≡⟨σ(zα1) σ(zα2)⟩G(1) .
The second equality states that the probability distribution factorizes for each neuron
due to the relation ex+y = exey. To go from the second equality to the third, we compute
the integrals for the neurons with i ̸= i1, which are all trivial, and we also rename the
dummy integral variable zi1;α to zα. The ﬁnal equality reintroduces the notation (1.68)
⟨F (zα1, . . . , zαm)⟩g ≡
Z "Q
α∈D dzα
p
|2πg|
#
exp

−1
2
X
β1,β2∈D
gβ1β2zβ1zβ2

F(zα1, . . . , zαm)
(4.25)
to describe a Gaussian expectation with variance g and an arbitrary function F(zα1, . . . , zαm)
over variables with sample indices only. In other parts of this book we’ll explicitly evalu-
ate this type of Gaussian expectation in various setups for concrete choices of activation
functions, but for the purpose of this chapter we will view computations as complete
when they are reduced to such Gaussian expectations without any neural indices. In-
troducing further the simplifying notation
σα ≡σ(zα) ,
(4.26)
78

the result of the computation above can be succinctly summarized as
E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2
i
= ⟨σα1σα2⟩G(1) .
(4.27)
It’s easy to generalize this to correlators of more than two activations. For instance,
for four activations on the same neuron i1 = i2, we have by the exact same manipulations
E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2

σ

z(1)
i1;α3

σ

z(1)
i1;α4
i
= ⟨σα1σα2σα3σα4⟩G(1) ,
(4.28)
and for each pair on two diﬀerent neurons i1 ̸= i2, we have
E
h
σ

z(1)
i1;α1

σ

z(1)
i1;α2

σ

z(1)
i2;α3

σ

z(1)
i2;α4
i
(4.29)
=



Y
i/∈{i1,i2}
Z 

Q
α∈D dzi;α
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zi;β1zi;β2





×
Z 

Q
α∈D dzi1;α
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zi1;β1zi1;β2

σ(zi1;α1) σ(zi1;α2)
×
Z 

Q
α∈D dzi2;α
q2πG(1)

exp

−1
2
X
β1,β2∈D
Gβ1β2
(1) zi2;β1zi2;β2

σ(zi2;α3) σ(zi2;α4)
= ⟨σα1σα2⟩G(1) ⟨σα3σα4⟩G(1) ,
where it’s clear each neuron factorizes and gives separate Gaussian integrals. This il-
lustrates the fact that neurons are independent, and thus there is no interaction among
diﬀerent neurons in the ﬁrst layer. In deeper layers, the preactivation distributions are
nearly-Gaussian and things will be a bit more complicated.
4.2
Second Layer: Genesis of Non-Gaussianity
In this section, we’ll move onto evaluating the distribution of preactivations in the second
layer of an MLP. The second-layer preactivations are deﬁned via
z(2)
i;α ≡z(2)
i
(xα) = b(2)
i
+
n1
X
j=1
W (2)
ij σ(1)
j;α ,
for
i = 1, . . . , n2 ,
(4.30)
with the ﬁrst-layer activations denoted as
σ(1)
i;α ≡σ

z(1)
i;α

,
(4.31)
and the biases b(2) and weights W (2) sampled from Gaussian distributions.
The joint distribution of preactivations in the ﬁrst and second layers can be factorized
as
p

z(2), z(1)D

= p

z(2)z(1)
p

z(1)D

.
(4.32)
79

Here the ﬁrst-layer marginal distribution p

z(1)D

was evaluated in the last section,
§4.1, to be a Gaussian distribution (4.23) with the variance given in terms of the ﬁrst-
layer metric G(1)
α1α2. As for the conditional distribution, we know that it can be expressed
as5
p

z(2)z(1)
(4.33)
=
Z "Y
i
db(2)
i
p

b(2)
i
# 
Y
i,j
dW (2)
ij
p

W (2)
ij


Y
i,α
δ

z(2)
i;α −b(2)
i
−
X
j
W (2)
ij σ(1)
j;α

,
from the formal expression (2.34) for the preactivation distribution conditioned on the
activations in the previous layer. The marginal distribution of the second-layer preacti-
vations can then be obtained by marginalizing over or integrating out the ﬁrst-layer
preactivations as
p

z(2)D

=
Z 
Y
i,α
dz(1)
i;α

p

z(2)z(1)
p

z(1)D

.
(4.34)
To evaluate this expression for the marginal distribution p

z(2)D

, ﬁrst we’ll discuss how
to treat the conditional distribution p

z(2)z(1)
, and then we’ll explain how to integrate
over the ﬁrst-layer preactivations z(1) governed by the Gaussian distribution (4.23).
Second-layer conditional distribution
The conditional distribution (4.33) can be evaluated exactly in the same way as we
evaluated the ﬁrst-layer distribution (4.16) conditioned on the inputs, with the simple
replacement of the layer indices ℓas 1 →2 and exchanging the network input for the
ﬁrst-layer preactivation as xj;α →σ(1)
j;α. Giving you a moment to ﬂip back to (4.16) to
make these substitutions and then remind yourself of the answer (4.23), it’s easy to see
that this evaluation yields
p

z(2)z(1)
=
1
r2π bG(2)

n2 exp

−1
2
n2
X
i=1
X
α1,α2∈D
bGα1α2
(2)
z(2)
i;α1z(2)
i;α2

,
(4.35)
where we have deﬁned the stochastic second-layer metric
bG(2)
α1α2 ≡C(2)
b
+ C(2)
W
1
n1
n1
X
j=1
σ(1)
j;α1σ(1)
j;α2 ,
(4.36)
with a hat to emphasize that it is a random variable that depends on the stochastic
variable z(1) through σ(1) ≡σ

z(1)
. Thus, we see that the second-layer conditional
5Again, the expression in the Dirac delta function is speciﬁc to multilayer perceptron architectures,
but this formalism can easily be adapted for other architectures.
80

distribution (4.35) is a Gaussian whose variance itself is a random variable. In particular,
the stochastic second-layer metric ﬂuctuates around the mean second-layer metric
G(2)
α1α2 ≡E
h
bG(2)
α1α2
i
= C(2)
b
+ C(2)
W
1
n1
n1
X
j=1
E
h
σ(1)
j;α1σ(1)
j;α2
i
(4.37)
= C(2)
b
+ C(2)
W ⟨σα1σα2⟩G(1) ,
where in the last step we recalled the result (4.27) for evaluating the two-point correlator
of the ﬁrst-layer activations on the same neuron.
Around this mean, we deﬁne the ﬂuctuation of the second-layer metric as
d
∆G
(2)
α1α2 ≡bG(2)
α1α2 −G(2)
α1α2 = C(2)
W
1
n1
n1
X
j=1

σ(1)
j;α1σ(1)
j;α2 −⟨σα1σα2⟩G(1)

,
(4.38)
which by construction has the mean zero when averaged over the ﬁrst-layer preactiva-
tions,
E

d
∆G
(2)
α1α2

= 0 .
(4.39)
The typical size of the ﬂuctuations is given by its two-point correlator. Recalling the
expressions we derived for Gaussian integrals (4.27) and (4.28) of two and four activations
on the same neuron and their factorization property on separate neurons (4.29), we
obtain
E

d
∆G
(2)
α1α2 d
∆G
(2)
α3α4

(4.40)
=
 
C(2)
W
n1
!2
n1
X
j,k=1
E
h
σ(1)
j;α1σ(1)
j;α2 −E
h
σ(1)
j;α1σ(1)
j;α2
i 
σ(1)
k;α3σ(1)
k;α4 −E
h
σ(1)
k;α3σ(1)
k;α4
ii
=
 
C(2)
W
n1
!2 n1
X
j=1
n
E
h
σ(1)
j;α1σ(1)
j;α2σ(1)
j;α3σ(1)
j;α4
i
−E
h
σ(1)
j;α1σ(1)
j;α2
i
E
h
σ(1)
j;α3σ(1)
j;α4
io
= 1
n1

C(2)
W
2 [⟨σα1σα2σα3σα4⟩G(1) −⟨σα1σα2⟩G(1) ⟨σα3σα4⟩G(1)]
≡1
n1
V (2)
(α1α2)(α3α4) ,
where at the end we introduced the second-layer four-point vertex V (2)
(α1α2)(α3α4) =
V (xα1, xα2; xα3, xα4), which depends on four input data points and is symmetric under
the exchanges of sample indices α1 ↔α2, α3 ↔α4, and (α1, α2) ↔(α3, α4). We will
understand the signiﬁcance of this quantity soon in a future equation, (4.43).
Here, we also see our ﬁrst hint of simpliﬁcation in the wide regime n1 ≫1: since the
four-point vertex here is manifestly of order one, we see that the metric ﬂuctuation will
be suppressed in that regime. Essentially, as the number of neurons in the ﬁrst layer
grows, the metric ﬂuctuation becomes more and more Gaussian due to the central limit
81

theorem. In the strict limit of inﬁnite n1, the metric would self-average, meaning that
the ﬂuctuation would vanish.
Now that we have a feel for the distribution of metric ﬂuctuations, we are only
too ready to actually integrate out the ﬁrst-layer preactivations z(1) and obtain the
marginal distribution of the second-layer preactivations p

z(2)D

. We again provide
two derivations, one brute-force and the other clever.
Wick Wick Wick: combinatorial derivation
The correlators of the second-layer preactivations can be written nicely in terms of the
expectations of the stochastic metric that we just computed. In order to compute the
correlators, ﬁrst we use the fact that the conditional distribution p

z(2)z(1)
is Gaussian
(4.35) to Wick contract the second-layer preactivations z(2), resulting in expressions
involving expectations of the stochastic metric bG(2)
α1α2; we then insert expressions for the
expectations of the stochastic metric obtained above.
With this in mind, the two-point correlator of the second-layer preactivations is given
by
E
h
z(2)
i1;α1z(2)
i2;α2
i
= δi1i2E
h
bG(2)
α1α2
i
= δi1i2G(2)
α1α2 = δi1i2

C(2)
b
+ C(2)
W ⟨σα1σα2⟩G(1)

,
(4.41)
where to be clear we ﬁrst used (4.35) to do the single Wick contraction and then inserted
the expression (4.37) for the mean of the stochastic metric.
Similarly, the full four-point function can be evaluated as
E
h
z(2)
i1;α1z(2)
i2;α2z(2)
i3;α3z(2)
i4;α4
i
(4.42)
=δi1i2δi3i4E
h
bG(2)
α1α2 bG(2)
α3α4
i
+ δi1i3δi2i4E
h
bG(2)
α1α3 bG(2)
α2α4
i
+ δi1i4δi2i3E
h
bG(2)
α1α4 bG(2)
α2α3
i
,
=δi1i2δi3i4G(2)
α1α2G(2)
α3α4 + δi1i3δi2i4G(2)
α1α3G(2)
α2α4 + δi1i4δi2i3G(2)
α1α4G(2)
α2α3
+ 1
n1
h
δi1i2δi3i4V (2)
(α1α2)(α3α4) + δi1i3δi2i4V (2)
(α1α3)(α2α4) + δi1i4δi2i3V (2)
(α1α4)(α2α3)
i
,
where in the ﬁrst line we made three Wick contractions of the four second-layer pre-
activations z(2)’s using the Gaussian distribution (4.35), and then in the second line
we recalled (4.39) and (4.40) for the expectations of the stochastic metric bG(2)
α1α2 =
G(2)
α1α2 + d
∆G
(2)
α1α2 over the ﬁrst-layer preactivations z(1). This means that the connected
four-point correlator – recall (1.54) – after subtracting the contributions from the two-
point correlators of the second-layer preactivations is given by
E
h
z(2)
i1;α1z(2)
i2;α2z(2)
i3;α3z(2)
i4;α4
i 
connected
(4.43)
= 1
n1
h
δi1i2δi3i4V (2)
(α1α2)(α3α4) + δi1i3δi2i4V (2)
(α1α3)(α2α4) + δi1i4δi2i3V (2)
(α1α4)(α2α3)
i
.
Here we see the true importance of the four-point vertex we introduced in (4.40); it gives
the connected second-layer four-point correlator and controls the near-Gaussianity of the
82

second-layer preactivation distribution. Thus, we see that this connected correlator is
suppressed in the wide regime of n1 ≫1, suggesting that the preactivation distribution
will become more and more Gaussian as the network gets wider and wider.
Given
this, we see that the second-layer preactivation distribution p

z(2)D

is in general non-
Gaussian but also simpliﬁes signiﬁcantly in the large-n1 regime, becoming Gaussian
in the strict n1 = ∞limit and with the four-point vertex V (2)
(α1α3)(α2α4) measuring the
leading deviation from Gaussianity.
To complete our combinatorial derivation, we need to ﬁnd an action that generates
correlations (4.41) and (4.43). As we know, a quadratic action cannot generate non-
Gaussian distributions with nontrivial connected four-point correlators, so we need a
diﬀerent action that’s appropriate for a nearly-Gaussian distribution. Intuition from
single-variable non-Gaussian integrals in §1.2 suggests that we could perhaps generate
the requisite correlations by including a quartic term in the action.
With that in mind, let’s start with a quartic action for an (nND)-dimensional random
variable z
S[z] = 1
2
X
α1,α2∈D
gα1α2
n
X
i=1
zi;α1zi;α2
−1
8
X
α1,...,α4∈D
v(α1α2)(α3α4)
n
X
i1,i2=1
zi1;α1zi1;α2 zi2;α3zi2;α4 ,
(4.44)
with undetermined couplings g and v.
We will treat the quartic coupling v pertur-
batively, an assumption that we will justify later by relating the quartic coupling v
to the 1/n1-suppressed connected four-point correlator. Note that by construction the
quartic coupling v(α1α2)(α3α4) has the same symmetric structure as the four-point vertex
V (2)
(α1α2)(α3α4) with respect to the sample indices.6 Using this action, we can compute to
the ﬁrst order in v the two-point and four-point correlators. Then, by matching with
the expressions (4.41) and (4.43) for these quantities, we’ll learn how to adjust the cou-
plings g and v to reproduce the right statistics of second-layer preactivations in the wide
regime.
Before proceeding further, it is convenient to introduce some notation. In (4.25), we
deﬁned ⟨F(zα1, . . . , zαm)⟩g for the average of an arbitrary function F over a Gaussian
distribution with variance g, where preactivation variables zα have sample indices only.
In addition, we here deﬁne
⟨⟨F(zi1;α1, . . . , zim;αm)⟩⟩g
(4.45)
≡
Z " n
Y
i=1
Q
α∈D dzi;α
p
|2πg|
#
exp

−1
2
n
X
j=1
X
β1,β2∈D
gβ1β2zj;β1zj;β2

F(zi1;α1, . . . , zim;αm) ,
which now includes neural indices. As we saw while working through (4.27) and (4.29),
this type of average factorizes into integrals of the form (4.25) for each neuron.
6The conventional factor of 1/8 in (4.44) is to account for this symmetry.
83

With this notation in hand, the expectation of an arbitrary function F(zi1;α1, . . . , zim;αm)
against a distribution with the quartic action (4.44) can be rewritten in terms of Gaus-
sian expectations, enabling the perturbative expansion in the coupling v as
E [F(zi1;α1, . . . , zim;αm)]
(4.46)
=
R hQ
i,α dzi;α
i
e−S(z)F(zi1;α1, . . . , zim;αm)
R hQ
i,α dzi;α
i
e−S(z)
=
DD
exp
n
1
8
P
β1,...,β4∈D v(β1β2)(β3β4) Pn
j1,j2=1 zj1;β1zj1;β2 zj2;β3zj2;β4
o
F(zi1;α1, . . . , zim;αm)
EE
g
DD
exp
n
1
8
P
β1,...,β4∈D v(β1β2)(β3β4) Pn
j1,j2=1 zj1;β1zj1;β2 zj2;β3zj2;β4
oEE
g
= ⟨⟨F(zi1;α1, . . . , zim;αm)⟩⟩g
+ 1
8
X
β1,...,β4∈D
v(β1β2)(β3β4)
n
X
j1,j2=1
h
⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4F(zi1;α1, . . . , zim;αm)⟩⟩g
−⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4⟩⟩g ⟨⟨F(zi1;α1, . . . , zim;αm)⟩⟩g
i
+ O

v2
,
where in the ﬁrst line we used the deﬁnition of the expectation, in the second line we
rewrote the numerator and denominator using the notation (4.45) that we just intro-
duced, and in the third line we expanded the exponential in the coupling v, both in the
denominator and numerator. In short, this tells us how to perturbatively express an
expectation against the full distribution with the quartic action (4.44) in terms of the
leading Gaussian expectation and perturbative corrections; these perturbative contribu-
tions nonetheless involve only Gaussian expectations and hence are easy to evaluate.
With this in mind, let’s consider some particular choices for F. Starting with the
two-point correlator, we get
E [zi1;α1zi2;α2]
(4.47)
=δi1i2

gα1α2 + 1
2
X
β1,...,β4∈D
v(β1β2)(β3β4) (ngα1β1gα2β2gβ3β4 + 2gα1β1gα2β3gβ2β4)

+ O

v2
.
Here the variance gα1α2 is the inverse of the quadratic coupling, with P
β gα1βgβα2 = δ α2
α1 .
84

Similarly, we ﬁnd that the connected four-point correlator evaluates to
E [zi1;α1zi2;α2zi3;α3zi4;α4]

connected
(4.48)
≡E [zi1;α1zi2;α2zi3;α3zi4;α4] −E [zi1;α1zi2;α2] E [zi3;α3zi4;α4]
−E [zi1;α1zi3;α3] E [zi2;α2zi4;α4] −E [zi1;α1zi4;α4] E [zi2;α2zi3;α3]
=δi1i2δi3i4
X
β1,...,β4∈D
v(β1β2)(β3β4)gα1β1gα2β2gα3β3gα4β4
+ δi1i3δi2i4
X
β1,...,β4∈D
v(β1β3)(β2β4)gα1β1gα3β3gα2β2gα4β4
+ δi1i4δi2i3
X
β1,...,β4∈D
v(β1β4)(β2β3)gα1β1gα4β4gα2β2gα3β3 + O

v2
.
Comparing these expressions, (4.47) and (4.48), with correlators in the second layer, (4.41)
and (4.43), it’s easy to see that setting the couplings as
gα1α2 = Gα1α2
(2)
+ O
 1
n1

,
(4.49)
v(α1α2)(α3α4) = 1
n1
V (α1α2)(α3α4)
(2)
+ O
 1
n2
1

,
(4.50)
reproduces the second-layer preactivation correlators to the leading order in 1/n1, with
the marginal distribution
p

z(2)D

= 1
Z e−S(z(2))
(4.51)
and quartic action (4.44). Here for convenience we have deﬁned a version of the four-
point vertex with indices raised by the inverse of the second-layer mean metric
V (α1α2)(α3α4)
(2)
≡
X
β1,...,β4
Gα1β1
(2) Gα2β2
(2) Gα3β3
(2) Gα4β4
(2) V (2)
(β1β2)(β3β4) .
(4.52)
Note that the quartic coupling v is O(1/n1), justifying our earlier perturbative treatment
of the coupling for wide networks. Note also that these couplings – the inverse metric
Gα1α2
(2)
and the quartic coupling V (α1α2)(α3α4)
(2)
– are input-dependent. In particular, the
eﬀective strength of interaction between neurons is set by the particular set of inputs to
the network.
This completes our ﬁrst combinatorial derivation of the second-layer preactivation
distribution.
Schwinger-Dyson this way: algebraic derivation
Here is a neat way to derive the action for the second-layer preactivation distribution.
Plugging the conditional distribution (4.35) into the marginalization equation (4.34),
the second-layer marginal distribution becomes
p

z(2)D

=
Z 
Y
i,α
dz(1)
i;α

p

z(1)D
 exp

−1
2
Pn2
j=1
P
α1,α2∈D bGα1α2
(2)
z(2)
j;α1z(2)
j;α2

r2π bG(2)

n2
.
(4.53)
85

We saw that the stochastic metric has a natural decomposition into mean and ﬂuctuating
parts as
bG(2)
α1α2 = G(2)
α1α2 + d
∆G
(2)
α1α2 .
(4.54)
Inverting this matrix to the second order in the ﬂuctuation around the mean, we get the
inverse stochastic metric7
bGα1α2
(2)
=Gα1α2
(2)
−
X
β1,β2∈D
Gα1β1
(2)
d
∆G
(2)
β1β2Gβ2α2
(2)
(4.55)
+
X
β1,...,β4∈D
Gα1β1
(2)
d
∆G
(2)
β1β2Gβ2β3
(2)
d
∆G
(2)
β3β4Gβ4α2
(2)
+ O

∆3
.
Putting this into the exponential that appears in the integrand of the marginal distri-
bution (4.53) and Taylor-expanding in the ﬂuctuation d
∆G
(2)
α1α2, we ﬁnd
exp

−1
2
n2
X
j=1
X
α1,α2∈D
bGα1α2
(2)
z(2)
j;α1z(2)
j;α2


(4.56)
= exp

−1
2
n2
X
j=1
X
α1,α2∈D
Gα1α2
(2)
z(2)
j;α1z(2)
j;α2


×
(
1 + 1
2
n2
X
i=1
X
α1,α2∈D


X
β1,β2∈D
Gα1β1
(2)
d
∆G
(2)
β1β2Gβ2α2
(2)

z(2)
i;α1z(2)
i;α2
−1
2
n2
X
i=1
X
α1,α2∈D


X
β1,...,β4∈D
Gα1β1
(2)
d
∆G
(2)
β1β2Gβ2β3
(2)
d
∆G
(2)
β3β4Gβ4α2
(2)

z(2)
i;α1z(2)
i;α2
+ 1
2!
1
2
2
n2
X
i1,i2=1
X
α1,...,β4∈D
Gα1β1
(2)
· · · Gα4β4
(2)
d
∆G
(2)
β1β2 d
∆G
(2)
β3β4z(2)
i1;α1z(2)
i1;α2z(2)
i2;α3z(2)
i2;α4 + O

∆3 )
.
Using this expression, the determinant in the denominator becomes
r2π bG(2)

n2 =
Z 
Y
i,α
dz(2)
i;α

exp

−1
2
n2
X
j=1
X
α1,α2∈D
bGα1α2
(2)
z(2)
j;α1z(2)
j;α2


(4.57)
=
q2πG(2)n2
"
1 + n2
2
X
β1,β2∈D
d
∆G
(2)
β1β2Gβ1β2
(2)
+
X
β1,...,β4∈D
d
∆G
(2)
β1β2 d
∆G
(2)
β3β4
 
n2
2
8 Gβ1β2
(2) Gβ3β4
(2)
−n2
4 Gβ1β3
(2) Gβ2β4
(2)
!
+ O

∆3 #
,
7This together with the deﬁning equation for the metric ﬂuctuation (4.38) are sometimes called the
Schwinger-Dyson equations [32, 33] from which this subsubsection takes its title.
86

where on the ﬁrst line we re-expressed the determinant as a Gaussian integral, and on the
subsequent line we plugged in (4.56) and integrated over the second-layer preactivations
z(2).
Next, plugging these two expressions (4.56) and (4.57) back into our expression for the
second-layer distribution (4.53), we can now integrate out the ﬁrst-layer preactivations,
giving
p

z(2)D

=
1
q2πG(2)n2 exp

−1
2
n2
X
j=1
X
α1,α2∈D
Gα1α2
(2)
z(2)
j;α1z(2)
j;α2


(4.58)
×
( 
1 + O
 1
n1

+
n2
X
i=1
X
α1,α2∈D

O
 1
n1

z(2)
i1;α1z(2)
i1;α2
+
1
8n1
n2
X
i1,i2=1
X
α1,...,α4∈D
V (α1α2)(α3α4)
(2)
z(2)
i1;α1z(2)
i1;α2z(2)
i2;α3z(2)
i2;α4
)
+ O
 1
n2
1

,
where we have used the fact that expectations of the metric ﬂuctuation are given by
E

d
∆G
(2)
β1β2

= 0 and E

d
∆G
(2)
β1β2 d
∆G
(2)
β3β4

=
1
n1 V (2)
(β1β2)(β3β4).8 Taking the logarithm to
isolate the action and absorbing the irrelevant constant terms into the partition function,
we arrive at the correct expression for the second-layer quartic action to leading order
in the ﬁrst layer width
S(z) =1
2
X
α1,α2∈D

Gα1α2
(2)
+ O
 1
n1
 n2
X
i=1
zi;α1zi;α2
(4.60)
−1
8
X
α1,...,α4∈D
1
n1
V (α1α2)(α3α4)
(2)
n2
X
i1,i2=1
zi1;α1zi1;α2zi2;α3zi2;α4 + O
 1
n2
1

.
Here, a prudent reader might wonder about our dropping of the 1/n1 correction to
the quadratic coupling, while keeping the quartic coupling despite being of the same
order. The main reason for this is that such a correction is a subleading contribution
8We tacitly assumed that the expectation of d
∆G
m≥3 are of order O 1/n2
1

or greater. For instance,
you can follow exactly the same steps as in (4.40) and compute
E
h
d
∆G
(2)
β1β2d
∆G
(2)
β3β4d
∆G
(2)
β5β6
i
(4.59)
= 1
n2
1

C(2)
W
3 h
⟨σα1σα2σα3σα4σα5σα6⟩G(1) −⟨σα1σα2⟩G(1) ⟨σα3σα4σα5σα6⟩G(1)
−⟨σα3σα4⟩G(1) ⟨σα5σα6σα1σα2⟩G(1) −⟨σα5σα6⟩G(1) ⟨σα1σα2σα3σα4⟩G(1)
+ 2 ⟨σα1σα2⟩G(1) ⟨σα3σα4⟩G(1) ⟨σα5σα6⟩G(1)
i
.
Just as in the middle step of (4.40), here again you’ve likely noticed that nonzero contributions arise only
when all the neural indices coincide. You can further use that same insight to show that E
h
d
∆G
(2)mi
=
O 1/nm−1
1

.
87

to the two-point correlator, while the quartic coupling gives the leading contribution
to the connected four-point correlator. Indeed, we shall encounter various observables
whose leading contributions stem solely from the nontrivial neuron-neuron interaction
induced by the quartic coupling. By contrast, the correction to the quadratic coupling
at ﬁnite-width is just a small quantitative eﬀect. Nevertheless, we will compute this
subleading correction in §4.5 for completeness.9
Nearly-Gaussian action in action
Having completed the two derivations, before moving on to the next section, let’s use
this opportunity to get a bit more of a feel for how to compute with a nearly-Gaussian
distribution. Paralleling what we did with the Gaussian action in the last section, let’s
evaluate the expectation of two activations on the same neuron and four activations,
with all four on the same neuron or pairs on separate neurons. The resulting expressions
will enable us to obtain the distributions of the preactivations in deeper layers.
In the following, we are just applying the formula (4.46) for the expectation of a
general function. These expressions will be valid for any layer ℓ> 1. First, for two
activations on the same neuron, we ﬁnd
E [σ(zi1;α1) σ(zi1;α2)]
(4.61)
= ⟨σα1σα2⟩g + 1
8
X
β1,...,β4∈D
v(β1β2)(β3β4)
×
h
⟨σα1σα2 (zβ1zβ2 −gβ1β2) (zβ3zβ4 −gβ3β4)⟩g
+ 2n ⟨σα1σα2 (zβ1zβ2 −gβ1β2)⟩g gβ3β4 −2 ⟨σα1σα2⟩g gβ1β3gβ2β4
i
+ O

v2
,
where we assume the reader is by now familiar enough with Gaussian integrals and
factorization into separate neurons so as not to include the middle steps. This result
highlights that the addition of the quartic coupling v has a nontrivial eﬀect even on
the two-point correlator of same-neuron activations.
We can similarly compute the
expectation of four activations on the same neuron, but we’ll need only the leading
Gaussian contribution, namely
E [σ(zi1;α1) σ(zi1;α2) σ(zi1;α3) σ(zi1;α4)] −E [σ(zi1;α1) σ(zi1;α2)] E [σ(zi1;α3) σ(zi1;α4)]
(4.62)
= ⟨σα1σα2σα3σα4⟩g −⟨σα1σα2⟩g ⟨σα3σα4⟩g + O(v) ,
where we subtracted oﬀthe contribution from the two-point correlators as that’s what’ll
appear in the next section. Finally, the similar expectation of four activations on two
9It will also turn out (§5.4) that by ﬁne-tuning the initialization hyperparameters such subleading
corrections are suppressed with depth in comparison to nearly-Gaussian corrections, so in a sense this
subleading correction to the quadratic coupling can be doubly ignored.
88

diﬀerent pairs of neurons i1 ̸= i2 can be evaluated by the application of the formula (4.46)
and neuron factorizations in Gaussian expectations, yielding
E [σ(zi1;α1) σ(zi1;α2) σ(zi2;α3) σ(zi2;α4)] −E [σ(zi1;α1) σ(zi1;α2)] E [σ(zi2;α3) σ(zi2;α4)]
=1
8
X
β1,...,β4∈D
v(β1β2)(β3β4)
n
X
j1,j2=1
(4.63)
×
h
⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4σi1;α1σi1;α2σi2;α3σi2;α4⟩⟩g
−⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4σi1;α1σi1;α2⟩⟩g ⟨⟨σi2;α3σi2;α4⟩⟩g
−⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4σi2;α3σi2;α4⟩⟩g ⟨⟨σi1;α1σi1;α2⟩⟩g
+ ⟨⟨zj1;β1zj1;β2 zj2;β3zj2;β4⟩⟩g ⟨⟨σi1;α1σi1;α2⟩⟩g ⟨⟨σi2;α3σi2;α4⟩⟩g
i
=1
4
X
β1,...,β4∈D
v(β1β2)(β3β4) ⟨σα1σα2 (zβ1zβ2 −gβ1β2)⟩g ⟨σα3σα4 (zβ3zβ4 −gβ3β4)⟩g + O

v2
,
where we get nonzero contributions only when j1 = i1 and j2 = i2 or when j1 = i2 and
j2 = i1. This shows that pairs of activations can only correlate with the addition of the
quartic coupling to the action, hinting at the role of ﬁnite width for features learning.
More generally, consider functions F(zi1;A1) and G(zi2;A2) of preactivations that depend
on subsamples A1 and A2 ⊂D, respectively, where with a slight abuse of notation we
put the set dependences into the subscripts. For distinct neurons i1 ̸= i2, the calculation
identical to the one just above shows that their covariance is given by
Cov
h
F(zi1;A1), G(zi2;A2)
i
(4.64)
≡E
h
F(zi1;A1) G(zi2;A2)
i
−E
h
F(zi1;A1)
i
E
h
G(zi2;A2)
i
=1
4
X
β1,...,β4∈D
v(β1β2)(β3β4)D
(zβ1zβ2 −gβ1β2) F(zA1)
E
g
D
(zβ3zβ4 −gβ3β4) G(zA2)
E
g + O

v2
.
This formula will be very useful in the future.
4.3
Deeper Layers: Accumulation of Non-Gaussianity
The preactivations in the deeper layers are recursively given by
z(ℓ+1)
i;α
= b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;α ,
for
i = 1, . . . , nℓ+1 ,
(4.65)
with the activations in the previous layer abbreviated as
σ(ℓ)
i;α ≡σ

z(ℓ)
i;α

.
(4.66)
89

We can obtain the marginal distributions of the preactivations in these deeper layers –
including the output distribution p

z(L)D

– by following the procedure that we imple-
mented for the second-layer distribution. The only complication is that the preactivation
distribution in the previous layer is no longer Gaussian, like it was for the ﬁrst layer.
The three key concepts of the derivation are: recursion, action, and 1/n-expansion.
Let’s walk through them one by one.
Recursion
The idea of recursion is to start with information contained in the marginal distribution
p

z(ℓ)D

in the ℓ-th layer and obtain the marginal distribution for the (ℓ+ 1)-th layer.
The change of the marginal preactivation distribution from layer to layer can be captured
by ﬁrst writing out the joint probability distribution of preactivations in adjacent layers
ℓand ℓ+ 1,
p

z(ℓ+1), z(ℓ)D

= p

z(ℓ+1)z(ℓ)
p

z(ℓ)D

,
(4.67)
then calculating the conditional probability distribution p

z(ℓ+1)z(ℓ)
, and ﬁnally marginal-
izing over the preactivations at the ℓ-th layer as
p

z(ℓ+1)D

=
Z 
Y
i,α
dz(ℓ)
i;α

p

z(ℓ+1)z(ℓ)
p

z(ℓ)D

.
(4.68)
In particular, the conditional probability distribution p

z(ℓ+1)z(ℓ)
serves as a transi-
tion matrix, bridging preactivation distributions in adjacent layers.
The calculation of this conditional distribution proceeds identically to the one we
performed for the ﬁrst layer (4.16) and then repurposed for computing the second-layer
conditional distribution (4.33).
If you’d like, you can again follow along with §4.1,
replacing z(1) by z(ℓ+1) and xj;α by σ(ℓ)
j;α, and obtain
p

z(ℓ+1)z(ℓ)
=
1
r2π bG(ℓ+1)

nℓ+1 exp

−1
2
nℓ+1
X
i=1
X
α1,α2∈D
bGα1α2
(ℓ+1)z(ℓ+1)
i;α1 z(ℓ+1)
i;α2

,
(4.69)
with the (ℓ+ 1)-th-layer stochastic metric
bG(ℓ+1)
α1α2 ≡C(ℓ+1)
b
+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
σ(ℓ)
j;α1σ(ℓ)
j;α2 ,
(4.70)
depending on the random variables z(ℓ) in the previous layer ℓthrough the activations
σ(ℓ). Note that all the correlators with odd numbers of the (ℓ+1)-th-layer preactivations
vanish while even-point correlators are obtained through Wick’s contractions, yielding
E
h
z(ℓ+1)
i1;α1 · · · z(ℓ+1)
i2m;α2m
i
=
X
all pairings
δik1ik2 · · · δik2m−1ik2mE
h
bG(ℓ+1)
αk1αk2 · · · bG(ℓ+1)
αk2m−1αk2m
i
,
(4.71)
90

where the sum runs over all the (2m −1)!! parings of auxiliary indices (k1, . . . , k2m).
On the left hand, the expectation value characterizes the (ℓ+ 1)-th-layer preactivation
distribution; on the right hand, the expectation value becomes a correlator of ℓ-th-layer
activations upon plugging in the stochastic metric (4.70), which can be evaluated with
the ℓ-th-layer distribution.
The mean of the stochastic metric is given by
G(ℓ+1)
α1α2 ≡E
h
bG(ℓ+1)
α1α2
i
= C(ℓ+1)
b
+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
,
(4.72)
and this mean metric governs the two-point correlator in the (ℓ+ 1)-th layer through
E
h
z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2
i
= δi1i2E
h
bG(ℓ+1)
α1α2
i
= δi1i2G(ℓ+1)
α1α2 ,
(4.73)
as we saw for the second layer (4.41) as a special case of the equation (4.71). Meanwhile,
the ﬂuctuation around the mean
d
∆G
(ℓ+1)
α1α2 ≡bG(ℓ+1)
α1α2 −G(ℓ+1)
α1α2 = C(ℓ+1)
W
1
nℓ
nℓ
X
j=1

σ(ℓ)
j;α1σ(ℓ)
j;α2 −E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
,
(4.74)
obviously has zero mean,
E

d
∆G
(ℓ+1)
α1α2

= 0 ,
(4.75)
and has a magnitude
1
nℓ
V (ℓ+1)
(α1α2)(α3α4) ≡E

d
∆G
(ℓ+1)
α1α2 d
∆G
(ℓ+1)
α3α4

= E
h
bG(ℓ+1)
α1α2 bG(ℓ+1)
α3α4
i
−G(ℓ+1)
α1α2 G(ℓ+1)
α3α4 .
(4.76)
Here we have introduced the (ℓ+1)-th-layer four-point vertex V (ℓ+1)
(α1α2)(α3α4), generalizing
the second-layer four-point vertex (4.40), which governs the connected four-point corre-
lator in the (ℓ+ 1)-th layer. Speciﬁcally, following along with the manipulations for the
second layer – cf. (4.42) and (4.43) – or simply applying the general expression (4.71),
we see
E
h
z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2 z(ℓ+1)
i3;α3 z(ℓ+1)
i4;α4
i 
connected
(4.77)
= 1
nℓ
h
δi1i2δi3i4V (ℓ+1)
(α1α2)(α3α4) + δi1i3δi2i4V (ℓ+1)
(α1α3)(α2α4) + δi1i4δi2i3V (ℓ+1)
(α1α4)(α2α3)
i
.
In summary, what we have so far are the expressions for the two-point correla-
tor (4.73) and the connected four-point correlator (4.77) of the (ℓ+ 1)-th-layer preacti-
vations in terms of the correlators of the ℓ-th-layer activations, and related expressions
for higher-point correlators (4.71) if the need arises. The strategy of our recursive ap-
proach is to ﬁrst evaluate these ℓ-th-layer activation correlators given the ℓ-th-layer dis-
tribution p

z(ℓ)D

and from them obtain the (ℓ+ 1)-th-layer preactivation correlators.
Using these correlators, we can then reconstruct the (ℓ+ 1)-th layer marginal distribu-
tion p

z(ℓ+1)D

. Both the evaluation of the ℓ-th-layer activation correlators and the
reconstruction of the distribution at the (ℓ+ 1)-th layer can be eﬃciently implemented
through the use of the action.
91

Action
The preactivation distribution p

z(ℓ)D

can be written in terms of an action as
p

z(ℓ)D

= e−S(z(ℓ))
Z(ℓ)
,
(4.78)
with the ℓ-th layer partition function given by
Z(ℓ) ≡
Z 
Y
i,α
dz(ℓ)
i;α

e−S(z(ℓ)) ,
(4.79)
and our ansatz for the action given by the following expansion:
S

z(ℓ)
≡1
2
nℓ
X
i=1
X
α1,α2∈D
gα1α2
(ℓ)
z(ℓ)
i;α1z(ℓ)
i;α2
(4.80)
−1
8
nℓ
X
i1,i2=1
X
α1,...,α4∈D
v(α1α2)(α3α4)
(ℓ)
z(ℓ)
i1;α1z(ℓ)
i1;α2 z(ℓ)
i2;α3z(ℓ)
i2;α4 + . . . .
This ansatz encompasses both the actions we had in §4.1 for the ﬁrst-layer preactivations
– with gα1α2
(1)
= Gα1α2
(1)
and v(1) = 0 – and for the second-layer preactivations in §4.2 –
with the couplings g(2) and v(2) given by (4.49) and (4.50), respectively. In fact, this
represents the most general expansion around the Gaussian action, given the symmetries
of preactivation correlators (4.71). In particular, only even powers of preactivations show
up in the action since we know that correlators with odd numbers of preactivations
vanish.
Here, the coeﬃcients gα1α2
(ℓ)
, v(α1α2)(α3α4)
(ℓ)
, and the implied additional terms in the
expansion are data-dependent couplings that together govern the interactions of the
neural preactivations and are simply related to the correlators of preactivations z(ℓ).
In particular, in §4.2 we gave two derivations for the relations between quadratic and
quartic couplings on the one hand and two-point and four-point correlators on the other
hand. The same argument applies for an arbitrary layer ℓ, and so we have
gα1α2
(ℓ)
= Gα1α2
(ℓ)
+ O(v, . . .) ,
(4.81)
v(α1α2)(α3α4)
(ℓ)
=
1
nℓ−1
V (α1α2)(α3α4)
(ℓ)
+ O

v2, . . .

,
(4.82)
with the understanding that the raised indices of the four-point vertex are shorthand for
contraction with the ℓ-th-layer inverse metric
V (α1α2)(α3α4)
(ℓ)
≡
X
β1,...,β4∈D
Gα1β1
(ℓ)
Gα2β2
(ℓ)
Gα3β3
(ℓ)
Gα4β4
(ℓ)
V (ℓ)
(β1β2)(β3β4) .
(4.83)
Note that the higher-order terms O(...) in (4.81) and (4.82) can be neglected self-
consistently if and only if the quartic coupling v and higher-order couplings are per-
turbatively small. This is indeed the case when networks are suﬃciently wide, as we will
show next.
92

Large-width expansion
Now we have our work cut out for us. First, note that these mappings, (4.81) and (4.82),
between the correlators and couplings already accomplish one task mentioned in our re-
cursive strategy. Namely, when applied to the (ℓ+ 1)-th layer, they reconstruct the
(ℓ+ 1)-th-layer distribution out of the (ℓ+ 1)-th-layer preactivation correlators. The
only remaining task then is to use the ℓ-th-layer action (4.80) to compute the expecta-
tions of the ℓ-th-layer activations σ(ℓ) that appear in the expressions for the two-point
correlator (4.73) and four-point correlator (4.77) of the (ℓ+ 1)-th-layer preactivations
z(ℓ+1).
These calculations simplify in the wide regime with a large number of neurons per
layer
n1, n2, . . . , nL−1 ∼n ≫1 .
(4.84)
As has been advertised, this large-but-ﬁnite-width regime is where networks become
both practically usable and theoretically tractable.
Speciﬁcally, the relations (4.81)
and (4.82) between correlators and couplings simplify in this regime and higher-order
non-Gaussian corrections can be self-consistently truncated in a series in 1/n.10 To be
precise, we inductively assume that the mean metric G(ℓ) = O(1) and the four-point
vertex V (ℓ) = O(1) are both of order one at the ℓ-th layer – as was the case for the ﬁrst
and second layers – and show that the same holds true at the (ℓ+ 1)-th layer. This
inductive assumption in particular implies through (4.81) and (4.82) that the quartic
coupling v(ℓ) = O(1/n) is perturbatively small at the ℓ-th layer and that the quadratic
coupling is given by g(ℓ) = G(ℓ) +O(1/n). In carrying out this inductive proof, we obtain
the recursion relations that govern the change in the preactivation distributions from
the ℓ-th layer to the (ℓ+ 1)-th layer.
To begin, we see that the two-point correlator in the (ℓ+ 1)-th layer (4.73) is given
simply in terms of the metric
G(ℓ+1)
α1α2 = C(ℓ+1)
b
+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
.
(4.85)
With foresight, we already evaluated this particular two-point correlator of activations
(4.61) in the last section. Inserting this result, along with the quadratic coupling g(ℓ) =
G(ℓ) + O(1/n) and quartic coupling v(ℓ) = O(1/n), we ﬁnd
G(ℓ+1)
α1α2 = C(ℓ+1)
b
+ C(ℓ+1)
W
⟨σα1σα2⟩G(ℓ) + O
 1
n

,
(4.86)
which is the leading recursion for the two-point correlator of preactivations.11 We see
that this is self-consistent; any metric G(ℓ) that is of order one will give an order-one
10In the language of §4.6, such a truncation is preserved under the RG ﬂow.
11Note that the diﬀerence from the second-layer calculation in §4.2 is just that the expectation in
(4.85) is not exactly Gaussian, but has a 1/n correction. This highlights the main diﬀerence with that
section, which is that the distribution in the prior layer is nearly-Gaussian.
93

metric G(ℓ+1) in the next layer as well. The correction is suppressed by O(1/n), which
aﬀects only the subleading term in the quadratic coupling g(ℓ+1) = G(ℓ+1)+O(1/n). Note
that, neglecting the subleading 1/n correction and replacing G by g, the recursion (4.86)
for the two-point correlator can also be thought of as the leading recursion for the
quadratic coupling.
Next, let’s evaluate the four-point correlator (4.77), which involves computing the
magnitude of the metric ﬂuctuation (4.76). Substituting in our general expression for
the (ℓ+ 1)-th-layer metric ﬂuctuation (4.74), we get
1
nℓ
V (ℓ+1)
(α1α2)(α3α4)
(4.87)
=
 
C(ℓ+1)
W
nℓ
!2
nℓ
X
j,k=1
n
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2σ(ℓ)
k;α3σ(ℓ)
k;α4
i
−E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
E
h
σ(ℓ)
k;α3σ(ℓ)
k;α4
io
.
Here, there are two types of the contributions: from coincident neurons and from separate
pairs of neurons. Again, with foresight, we have already evaluated both types of four-
point activation correlators in the last section. When all four are coincident j = k,
substituting in (4.62) we ﬁnd
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2σ(ℓ)
j;α3σ(ℓ)
j;α4
i
−E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
E
h
σ(ℓ)
j;α3σ(ℓ)
j;α4
i
(4.88)
= ⟨σα1σα2σα3σα4⟩G(ℓ) −⟨σα1σα2⟩G(ℓ) ⟨σα3σα4⟩G(ℓ) + O
 1
n

,
where we have truncated to leading order in 1/n as a consequence of the inductive
assumption at the ℓ-th layer. Meanwhile, when j ̸= k and the correlation is between two
neurons, we substitute in our expression (4.63), ﬁnding
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2σ(ℓ)
k;α3σ(ℓ)
k;α4
i
−E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
E
h
σ(ℓ)
k;α3σ(ℓ)
k;α4
i
(4.89)
=
1
4nℓ−1
X
β1,...,β4∈D
V (β1β2)(β3β4)
(ℓ)
⟨σα1σα2 (zβ1zβ2 −gβ1β2)⟩G(ℓ) ⟨σα3σα4 (zβ3zβ4 −gβ3β4)⟩G(ℓ)
+ O
 1
n2

,
where again we have truncated to leading order in the large-width expansion using
the inductive assumption.12 Inserting both of these expressions back into (4.87) and
12Again, the diﬀerence with the second-layer calculation is that in §4.2 these expectations are over the
exactly Gaussian ﬁrst-layer distribution. In that case, there was a contribution of the form (4.88) from
the case with all neurons coincident, but not of the form (4.89) from the two neurons – cf. (4.40).
94

performing the sums, we get a recursion for the four-point vertex
1
nℓ
V (ℓ+1)
(α1α2)(α3α4)
(4.90)
= 1
nℓ

C(ℓ+1)
W
2 [⟨σα1σα2σα3σα4⟩G(ℓ) −⟨σα1σα2⟩G(ℓ) ⟨σα3σα4⟩G(ℓ)]
+
1
nℓ−1

C(ℓ+1)
W
2
4
X
β1,...,β4∈D
V (β1β2)(β3β4)
(ℓ)
⟨σα1σα2 (zβ1zβ2 −gβ1β2)⟩G(ℓ)
× ⟨σα3σα4 (zβ3zβ4 −gβ3β4)⟩G(ℓ) + O
 1
n2

.
Importantly, we see that
1
nℓ
V (ℓ+1) = O
 1
n

,
(4.91)
and V (ℓ+1) = O(1), thus completing our inductive proof and concluding our derivations of
the recursion relations (4.86) and (4.90) for the two-point and four-point correlators. As
was the case for the quadratic coupling, if we neglect the subleading 1/n2 correction and
replace G by g and V by v, the recursion (4.90) for the connected four-point correlator
can also be thought of as the recursion for the quartic coupling.
Note that in the strict n →∞limit, the quartic coupling vanishes, and the marginal
distribution of preactivations p

z(ℓ)D

is Gaussian for all layers ℓ. The ﬁrst nontrivial
correction to this inﬁnite-width limit is captured by studying the quartic action with
couplings v(ℓ). In what follows, we will mostly focus on the eﬀective theory with this
quartic action, as we expect signiﬁcant qualitative diﬀerences in the behavior of networks
described by the quadratic action vs. the quartic action. The additional ﬁnite-width
corrections given by the higher-order terms in the action can change quantitative results
but should not really exhibit qualitative diﬀerences.
4.4
Marginalization Rules
In the past sections, at each step in the recursions we marginalized over all the preac-
tivations in a given layer. This section collects two remarks on other sorts of partial
marginalizations we can perform, rather than integrating out an entire layer. In partic-
ular, we’ll discuss marginalization over a subset of the ND samples in the dataset D and
marginalization over a subset of neurons in a layer.
Loosely speaking, these marginalizations let us focus on speciﬁc input data and
neurons of interest.
Tightly speaking, let’s consider evaluating the expectation of a
function F(zI;A) = F

{zi;α}i∈I;α∈A

that depends on a subsample A ⊂D and a subset
of neurons I ⊂{1, . . . , nℓ} ≡N in a layer ℓ, where with a slight abuse of notation we
95

put the set dependences into the subscripts. We then have
E [F(zI;A)]
(4.92)
=
Z " Y
i∈N
Y
α∈D
dzi;α
#
F(zI;A) p

zN;D
D

=
Z "Y
i∈I
Y
α∈A
dzi;α
#
F(zI;A)



Z 

Y
(j;β)∈[N×D−I×A]
dzj;β

p

zN;D
D




=
Z "Y
i∈I
Y
α∈A
dzi;α
#
F(zI;A) p

zI;A
A

where the last equality is just the marginalization over the spectator variables that do
not enter into the observable of interest and, in a sense, deﬁnes the subsampled and
subneuroned distribution as
p

zI;A
A

≡
Z 

Y
(j;β)∈[N×D−I×A]
dzj;β

p

zN;D
D

.
(4.93)
In words, in evaluating the expectation of the function F(zI;A), the full distribution
p

zN;D
D

can simply be restricted to that of the subsample A and subneurons I, i.e.,
p

zI;A
A

.
We call this property a marginalization rule.
Yes, this is somewhat
trivial – we’re just restating the consistency of probability distributions with respect to
marginalization – but it has two rather useful consequences for us.
Marginalization over samples
The ﬁrst corollary of the marginalization rule is that we can use it to reduce a gigantic
integral over all the samples in the dataset to a compact integral over only a handful of
samples. For example, in recursively obtaining the two-point correlator through
E
h
z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2
i
= δi1i2

C(ℓ+1)
b
+ C(ℓ+1)
W
⟨σα1σα2⟩G(ℓ) + O
 1
n
 
,
(4.94)
we can reduce the ND-dimensional Gaussian integrals ⟨σα1σα2⟩G(ℓ) with the ND-by-
ND variance matrix G(ℓ) to a manageable two-dimensional integral with a two-by-two
submatrix spanned by α1 and α2 (or a one-dimensional integral if α1 = α2). Similarly, a
Gaussian integral for four activations ⟨σα1σα2σα3σα4⟩G(ℓ) that appears in the recursion
for four-point vertex involves integrals over four variables at most. Generally, in using the
action (4.80) to evaluate a speciﬁc expectation, the summation over the whole dataset
D in the action can be restricted to the subset of input data that actually appears in
the expectation.
By the same token, in recursively evaluating the four-point vertex
V (ℓ+1)
(α1α2)(α3α4) via the recursion (4.90), the summation on the right-hand side over the
dataset D can be restricted to the set of samples being correlated, {α1, α2, α3, α4}.
96

However, please keep in mind that the inverse metrics used to construct V (β1β2)(β3β4)
(ℓ)
in
(4.83) must then be taken to be the inverse of the metric submatrix on this restricted
subspace.13
Marginalization over neurons
The second corollary involves integrating out a subset of neurons in a layer. Prudent
readers might have worried that the quartic term in the ℓ-th-layer action,
−1
8
nℓ
X
i1,i2=1
X
α1,...,α4∈D
v(α1α2)(α3α4)
(ℓ)
z(ℓ)
i1;α1z(ℓ)
i1;α2 z(ℓ)
i2;α3z(ℓ)
i2;α4 ,
(4.95)
seems to naively scale like ∼n2
ℓ/nℓ−1 = O(n), since there are two sums over nℓ, and we
know from (4.82) that the coupling v(ℓ) scales like ∼1/nℓ−1. Similarly, the quadratic
term,
1
2
nℓ
X
i=1
X
α1,α2∈D
gα1α2
(ℓ)
z(ℓ)
i;α1z(ℓ)
i;α2 ,
(4.96)
has a single sum over nℓand so seems naively O(n) as well. This would imply that
the quartic term isn’t perturbatively suppressed in comparison to the quadratic term,
naively calling our perturbative approach into question.
We ﬁrst observe that this problem never arises for the ﬁnal layer ℓ= L, since the
output dimension nL is never parametrically large: the quadratic term scales as ∼nL =
O(1) while the quartic term scales as ∼n2
L/nL−1 = O(1/n), which is perturbatively
suppressed.
This observation, combined with the marginalization rule, points at a resolution to
the naive scale-counting problem above for the hidden layers. Indeed, all the expectation
we evaluated so far – both preactivation and activation correlators – each individually
involves only a few neurons mℓin any given layer ℓ, with mℓ≪nℓ. This will always
be true; we can’t actually correlate an inﬁnite number of neurons at once! Thus, when
using the action representation (4.80) of the probability distribution to compute these
correlators at the ℓ-th layer, we can ﬁrst use the marginalization rule (4.92) to integrate
out the (nℓ−mℓ) spectator neurons that do not participate in the computation, letting
us focus on those mℓrelevant neurons that actually appear in the expectation. This in
turn lets us replace the summations over nℓneurons by ones over the mℓneurons.14
All the while, the numbers of neurons in the previous layers n1, . . . , nℓ−1, having been
integrated out to get the action representation at the ℓ-th layer, are parametrically large.
This means that the quadratic term in the ℓ-th-layer action, reduced to the mℓrelevant
13A similar restriction of the summation can be applied to any of our other recursions and will prove
especially useful when you try to evaluate them numerically or analytically.
14In evaluating generic expectation value such as (4.46), one can always check that the contributions
from the (nℓ−mℓ) spectator neurons consistently cancel out at each order in 1/nℓ−1 expansion. If you
go back to your personal note that ﬁlls in the small gaps between lines in our computations, you will
surely notice this cancellation due to Gaussian factorization.
97

neurons, scales as ∼mℓ= O(1), while the quartic term scales as ∼m2
ℓ/nℓ−1 = O(1/n).
Thus, this ensures a perturbative treatment of the non-Gaussianity.
Running couplings with partial marginalizations
In focusing our attention on only a subset of samples or neurons, the data-dependent
couplings of the action need to be adjusted.
Since this running of the couplings is
instructive and will be necessary for later computations, let us illustrate here how the
quadratic coupling gα1α2
(ℓ),mℓdepends on the number of neurons mℓin the action.
For simplicity in our illustration, let us specialize to a single input x and drop all the
sample indices. Then, denote the distribution over mℓneurons as
p

z(ℓ)
1 , . . . , z(ℓ)
mℓ

∝e−S
 z(ℓ)
1 , ... , z(ℓ)
mℓ

(4.97)
= exp

−g(ℓ),mℓ
2
mℓ
X
j=1
z(ℓ)
j z(ℓ)
j
+ v(ℓ)
8
mℓ
X
j1,j2=1
z(ℓ)
j1 z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j2

,
which is expressed by the same action we’ve already been using (4.80), though now the
dependence of the quadratic coupling on mℓis made explicit.15 We’ll now see in two
ways how the quadratic coupling g(ℓ),mℓruns with mℓ.
The ﬁrst way is to begin with the action for nℓneurons and formally integrate out
(nℓ−mℓ) neurons. Without loss of generality, let’s integrate out the last (nℓ−mℓ)
neurons, leaving the ﬁrst mℓneurons labeled as 1, . . . , mℓ. Using the marginalization
rule (4.93), we see that
e−S
 z(ℓ)
1 , ... , z(ℓ)
mℓ

∝p

z(ℓ)
1 , . . . , z(ℓ)
mℓ

=
Z
dz(ℓ)
mℓ+1 · · · dz(ℓ)
nℓp

z(ℓ)
1 , . . . , z(ℓ)
nℓ

(4.98)
∝
Z
dz(ℓ)
mℓ+1 · · · dz(ℓ)
nℓexp

−g(ℓ),nℓ
2
nℓ
X
i=1
z(ℓ)
i z(ℓ)
i
+ v(ℓ)
8
nℓ
X
i1,i2=1
z(ℓ)
i1 z(ℓ)
i1 z(ℓ)
i2 z(ℓ)
i2

,
throughout which we neglected normalization factors that are irrelevant if we’re just
interested in the running of the coupling. Next, we can separate out the dependence
on the mℓneurons, perturbatively expand the integrand in quartic coupling, and ﬁnally
15Note that in principle the quartic coupling should also depend on mℓ: v(ℓ) →v(ℓ),mℓ. However, since
such a dependence only shows up at higher order in v, we will suppress it.
98

integrate out the last (nℓ−mℓ) neurons by computing a few simple Gaussian integrals:
p

z(ℓ)
1 , . . . , z(ℓ)
mℓ

(4.99)
∝exp

−g(ℓ),nℓ
2
mℓ
X
j=1
z(ℓ)
j z(ℓ)
j
+ v(ℓ)
8
mℓ
X
j1,j2=1
z(ℓ)
j1 z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j2


×
Z
dz(ℓ)
mℓ+1 · · · dz(ℓ)
nℓexp

−g(ℓ),nℓ
2
nℓ
X
k=mℓ+1
z(ℓ)
k z(ℓ)
k


×

1 + 2v(ℓ)
8
mℓ
X
j=1
nℓ
X
k=mℓ+1
z(ℓ)
j z(ℓ)
j z(ℓ)
k z(ℓ)
k
+ v(ℓ)
8
nℓ
X
k1,k2=mℓ+1
z(ℓ)
k1 z(ℓ)
k1 z(ℓ)
k2 z(ℓ)
k2 + O

v2


= exp

−g(ℓ),nℓ
2
mℓ
X
j=1
z(ℓ)
j z(ℓ)
j
+ v(ℓ)
8
mℓ
X
j1,j2=1
z(ℓ)
j1 z(ℓ)
j1 z(ℓ)
j2 z(ℓ)
j2


×
(
1 + (nℓ−mℓ)
4
v(ℓ)
g(ℓ),nℓ
 mℓ
X
i=1
z(ℓ)
i z(ℓ)
i
!
+
v(ℓ)
8g2
(ℓ),nℓ
h
(nℓ−mℓ)2 + 2(nℓ−mℓ)
i
+O

v2)
.
Finally, resumming the correction arising from the quartic coupling proportional to
Pmℓ
i=1 z(ℓ)
i z(ℓ)
i
back into the exponential, ignoring the proportionality factor, and compar-
ing with the action for mℓneurons (4.97), we ﬁnd
g(ℓ),mℓ= g(ℓ),nℓ−(nℓ−mℓ)
2
v(ℓ)
g(ℓ),nℓ
+ O

v2
(4.100)
as the running equation for the quadratic coupling.
The second way to see the coupling run – and ﬁnd a solution to the running equa-
tion (4.100) – is to compute the single-input metric G(ℓ) ≡E
h
z(ℓ)
i z(ℓ)
i
i
and compute it
directly using the mℓ-neuron action (4.97). We’ve already computed this in (4.47) using
the quartic action for multiple inputs. Specializing to a single input, considering an
action of mℓneurons, and being explicit about the dependence of the quadratic coupling
on the number of neurons, we get
G(ℓ) =
"
1
g(ℓ),mℓ
+ (mℓ+ 2)
2
v(ℓ)
g3
(ℓ),mℓ
#
+ O

v2
.
(4.101)
Solving this equation for g(ℓ),mℓby perturbatively expanding in v(ℓ), we ﬁnd
1
g(ℓ),mℓ
= G(ℓ) −(mℓ+ 2)
2
V (ℓ)
nℓ−1G(ℓ) + O
 1
n2

,
(4.102)
where we have also plugged in
v(ℓ) =
V (ℓ)
nℓ−1
 G(ℓ)4 + O
 1
n2

,
(4.103)
99

using (4.82) and (4.83) to relate the quartic coupling to the four-point vertex and again
specializing to a single input. Now, it’s easy to check that this expression (4.102) solves
the running equation (4.100).16
The key step in this alternative derivation is realizing that observables without any
neural indices such as G(ℓ) should not depend on which version of the mℓaction we use
in computing them. Interpreted another way, what this running of the coupling means
is that for diﬀerent numbers of neurons in a layer ℓ– e.g. mℓand nℓ– we need diﬀerent
quadratic couplings – in this case g(ℓ),mℓand g(ℓ),nℓ– in order to give the correct value
for an ℓ-th-layer observable such as G(ℓ). If you’re ever in doubt, it’s always safest to
express an observable of interest in terms of the metric G(ℓ) and the four-point vertex
V (ℓ) rather than the couplings.
4.5
Subleading Corrections
At ﬁnite width, all of the correlators receive an inﬁnite series of subleading corrections.
Concretely, the metric governing two-point correlator and the four-point vertex governing
the connected four-point correlator have 1/n series expansions of the form
G(ℓ)
α1α2 =G{0}(ℓ)
α1α2 +
1
nℓ−1
G{1}(ℓ)
α1α2 +
1
n2
ℓ−1
G{2}(ℓ)
α1α2 + O
 1
n3

,
(4.104)
V (ℓ)
(α1α2)(α3α4) =V {0}(ℓ)
(α1α2)(α3α4) +
1
nℓ−1
V {1}(ℓ)
(α1α2)(α3α4) + O
 1
n2

.
(4.105)
While so far we have focused on the leading contributions G{0}(ℓ)
α1α2 and V {0}(ℓ)
(α1α2)(α3α4), the
subleading corrections can be systematically calculated as well. Let us illustrate the
procedure by deriving the recursion for the next-to-leading-order (NLO) correction to
the metric, G{1}(ℓ)
α1α2 .
Before proceeding, let us remark that the leading contribution of the mean metric
fully describes the inﬁnite-width limit of the preactivation distributions and so is given
a symbol
K(ℓ)
α1α2 ≡G{0}(ℓ)
α1α2 ,
(4.106)
and name, the kernel. Since the kernel captures the leading-order correlation between
any pair of samples, it will be a central object of study for us in the following chapters.
In a similar vein, we will call G{1}(ℓ)
α1α2 the NLO metric.
Our ﬁrst step will be to express the layer-ℓquadratic coupling gβ1β2
(ℓ)
to order 1/n
in terms of the 1/n correlator data in (4.104) and (4.105). Let’s begin by recalling the
expression (4.47) for the two-point correlator that we derived from the quartic action,
16Note that the coupling g(ℓ),mℓdepends on mℓ– and also on the other hidden-layer widths
n1, n2, . . . , nℓ−1 – but does not depend on the overall width of the current layer nℓ. This implies that
the quadratic coupling g(ℓ),mℓis the same coupling we would have used if instead there were actually
only mℓneurons in the ℓ-th layer.
100

reprinted here for layer ℓ
E
h
z(ℓ)
i1;α1z(ℓ)
i2;α2
i
= δi1i2G(ℓ)
α1α2
(4.107)
=δi1i2

g(ℓ)
α1α2 + 1
2
X
β1,...,β4∈D
v(β1β2)(β3β4)
(ℓ)

nℓg(ℓ)
α1β1g(ℓ)
α2β2g(ℓ)
β3β4 + 2g(ℓ)
α1β1g(ℓ)
α2β3g(ℓ)
β2β4


+ O

v2
.
As a reminder g(ℓ)
α1α2 is the matrix inverse of the quadratic coupling gα1α2
(ℓ)
.
Sub-
stituting in the expansion (4.104) into (4.107), substituting for the quartic coupling
v(ℓ) = V(ℓ)/nℓ−1 (4.82), and rearranging to solve for g(ℓ)
α1α2 to the subleading order, we
get
g(ℓ)
α1α2 = K(ℓ)
α1α2+
1
nℓ−1

G{1}(ℓ)
α1α2 −
X
β1,β2∈D
Kβ1β2
(ℓ)
nℓ
2 V (ℓ)
(α1α2)(β1β2) + V (ℓ)
(α1β1)(α2β2)

+O
 1
n2

.
(4.108)
Note that in obtaining the above, we have self-consistently replaced g(ℓ) by K(ℓ) in the
subleading term, which in turn let us lower the indices of the four-point vertices. Invert-
ing this expression (4.108) yields the subleading correction to the quadratic coupling in
terms of the correlators
gβ1β2
(ℓ)
−Kβ1β2
(ℓ)
(4.109)
=
1
nℓ−1
X
β3,β4∈D

−Kβ1β3
(ℓ)
Kβ2β4
(ℓ)
G{1}(ℓ)
β3β4 + K(ℓ)
β3β4
nℓ
2 V (β1β2)(β3β4)
(ℓ)
+ V (β1β3)(β2β4)
(ℓ)

+ O
 1
n2

.
Note that one term in this correction scales as nℓ/nℓ−1. As discussed in the previous
section, the marginalization rule for the ℓ-th-layer action guarantees that we can treat
this quantity as small, nℓ/nℓ−1 ≪1, ensuring that g(ℓ)
α1α2 −K(ℓ)
α1α2 is a subleading-in-
1/n correction to the quadratic coupling. In line with this statement, we’ll soon see
the cancellation for the factor of nℓwhen computing the recursion for this subleading
correction to the metric G{1}(ℓ).
Having ﬁnished working out the 1/n-corrected ℓ-th-layer action, we turn to comput-
ing the (ℓ+1)-th-layer two-point correlator.17 This will let us express the (ℓ+1)-th-layer
two-point correlator in terms of the ℓ-th-layer statistics, ultimately yielding a recursion
for G{1}(ℓ)
α1α2 . Starting with the expansion (4.104) in the (ℓ+ 1)-th layer and substituting
in the expression (4.85) for the two-point correlator, we obtain
K(ℓ+1)
α1α2 + 1
nℓ
G{1}(ℓ+1)
α1α2
+O
 1
n2

= G(ℓ+1)
α1α2 = C(ℓ+1)
b
+C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
. (4.110)
Thus, we need the expectation of two activations in the ℓ-th layer up to the order O(1/n),
which we evaluated before in expression (4.61) in terms of the ℓ-th-layer couplings.
17We already knew the 1/n contribution to the quartic coupling, namely the relation v(ℓ) = V(ℓ)/nℓ−1.
101

Looking at (4.61), there are two types of contributions at the subleading order, one
arising from the 1/n correction to the quadratic coupling g(ℓ) in (4.108) and the other
from the near-Gaussianity of the distribution due to the quartic coupling v(ℓ). The latter
contribution is easy to handle: since the quartic coupling is already suppressed by 1/n,
we can just make the replacement g(ℓ) →K(ℓ) in the second term in (4.61), yielding
1
8nℓ−1
X
β1,...,β4∈D
V (β1β2)(β3β4)
(ℓ)
(4.111)
×
h D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
 
zβ3zβ4 −K(ℓ)
β3β4
E
K(ℓ)
+ 2nℓ
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ) K(ℓ)
β3β4 −2 ⟨σα1σα2⟩K(ℓ) K(ℓ)
β1β3K(ℓ)
β2β4
i
+ O
 1
n2

.
However, for the former contribution, the Gaussian term ⟨σα1σα2⟩g(ℓ) needs be carefully
separated into the leading and subleading pieces. To that end, we can trade the Gaussian
expectation with g(ℓ) for one in terms of the leading kernel K(ℓ)
⟨σα1σα2⟩g(ℓ)
(4.112)
=
D
σα1σα2 exp
h
−1
2
P
β1,β2

gβ1β2
(ℓ)
−Kβ1β2
(ℓ)

zβ1zβ2
iE
K(ℓ)
D
exp
h
−1
2
P
β1,β2

gβ1β2
(ℓ)
−Kβ1β2
(ℓ)

zβ1zβ2
iE
K(ℓ)
= ⟨σα1σα2⟩K(ℓ) −1
2
X
β1,β2

gβ1β2
(ℓ)
−Kβ1β2
(ℓ)
 D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ) + O
 1
n2

.
Plugging (4.109) into (4.112), we obtain the subleading contribution due to the change
in the quadratic coupling, giving
⟨σα1σα2⟩g(ℓ)
(4.113)
= ⟨σα1σα2⟩K(ℓ) +
1
2nℓ−1
Kβ1β3
(ℓ)
Kβ2β4
(ℓ)
G{1}(ℓ)
β3β4
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ)
−
1
nℓ−1
X
β1,...,β4
nℓ
4 V (β1β2)(β3β4)
(ℓ)
+ 1
2V (β1β3)(β2β4)
(ℓ)

K(ℓ)
β3β4
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ)
+ O
 1
n2

.
Now that we’ve computed everything, we can add the two contributions to E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
,
(4.111) and (4.113), and plug them into the expression for the preactivation correla-
tor (4.110). Collecting terms, we recover the leading contribution, the recursion for the
kernel
K(ℓ+1)
α1α2 = C(ℓ+1)
b
+ C(ℓ+1)
W
⟨σα1σα2⟩K(ℓ) ,
(4.114)
102

and also ﬁnd a recursion for the NLO metric as promised
1
nℓ
G{1}(ℓ+1)
α1α2
(4.115)
=C(ℓ+1)
W
1
nℓ−1
X
β1,...,β4∈D
"
1
2Kβ1β3
(ℓ)
Kβ2β4
(ℓ)
G{1}(ℓ)
β3β4
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ)
+ 1
8V (β1β2)(β3β4)
(ℓ)
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
 
zβ3zβ4 −K(ℓ)
β3β4
E
K(ℓ)
+ 1
4V (β1β3)(β2β4)
(ℓ)
K(ℓ)
β3β4
D
σα1σα2

−2zβ1zβ2 + K(ℓ)
β1β2
E
K(ℓ)
#
.
In going through this calculation in your personal notes or on the margins of this book,
you can explicitly see the cancellation of contributions from nℓ−1 spectator neurons that
does not participate in the expectation E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
as required by the marginalization
rule for the ℓ-th-layer action. Indeed, every term in the square bracket on the right-hand
side of the equation (4.115) is manifestly of order one.
This process can be systematically pushed to higher orders. Just as the computation
of the NLO metric G{1}(ℓ)
α1α2 involved the leading quartic coupling, the computation of the
subleading correction to the four-point vertex, V {1}(ℓ)
(α1α2)(α3α4), and the computation of the
order 1/n2 correction to the two-point correlator, G{2}(ℓ)
α1α2 , would involve the leading sextic
coupling. Such a sextic coupling appears at order 1/n2 in the action and contributes to
the connected six-point function, which also vanishes as O
 1/n2.18
4.6
RG Flow and RG Flow
Since the past ﬁve sections have been a whirlwind of equations, algebra, and integration,
let’s take a moment to recap and assemble the main results.
The goal of this chapter was to ﬁnd the marginal distribution of preactivations
p

z(ℓ)D

in a given layer ℓin terms of an eﬀective action with data-dependent cou-
plings. These couplings change – or run – from layer to layer, and the running is de-
termined via recursions, which in turn determine how the distribution of preactivations
changes with depth. Equivalently, these recursions tell us how correlators of preactiva-
tions evolve with layer. In this language, starting with independent neurons in the ﬁrst
layer (§4.1), we saw how interactions among neurons are induced in the second layer
(§4.2) and then ampliﬁed in deeper layers (§4.3).
Concretely, let’s summarize the behavior of ﬁnite-width networks to leading order
in the wide-network expansion. Expressing the two-point correlator of preactivations in
18For those familiar with ﬁeld theory, the leading part of the couplings in the action are tree-level
contributions to correlators. They are to be contrasted with subleading corrections to the two-point
correlator discussed in this section, which included both loop-level contributions from quartic interaction
and tree-level contributions from the NLO correction to the bare quadratic coupling.
103

terms of the kernel K(ℓ)
α1α2 as
E
h
z(ℓ)
i1;α1z(ℓ)
i2;α2
i
= δi1i2G(ℓ)
α1α2 = δi1i2

K(ℓ)
α1α2 + O
 1
n

,
(4.116)
and expressing the four-point connected correlator in terms of the four-point vertex
V (ℓ)
(α1α2)(α3α4) as
E
h
z(ℓ)
i1;α1z(ℓ)
i2;α2z(ℓ)
i3;α3z(ℓ)
i4;α4
i 
connected
(4.117)
=
1
nℓ−1
h
δi1i2δi3i4V (ℓ)
(α1α2)(α3α4) + δi1i3δi2i4V (ℓ)
(α1α3)(α2α4) + δi1i4δi2i3V (ℓ)
(α1α4)(α2α3)
i
,
the running of these correlators is given by the recursions
K(ℓ+1)
α1α2 =C(ℓ+1)
b
+ C(ℓ+1)
W
⟨σα1σα2⟩K(ℓ) ,
(4.118)
V (ℓ+1)
(α1α2)(α3α4) =

C(ℓ+1)
W
2 h
⟨σα1σα2σα3σα4⟩K(ℓ) −⟨σα1σα2⟩K(ℓ) ⟨σα3σα4⟩K(ℓ)
i
(4.119)
+ 1
4

C(ℓ+1)
W
2
nℓ
nℓ−1
X
β1,...,β4∈D
V (β1β2)(β3β4)
(ℓ)
D
σα1σα2

zβ1zβ2 −K(ℓ)
β1β2
E
K(ℓ)
×
D
σα3σα4

zβ3zβ4 −K(ℓ)
β3β4
E
K(ℓ) + O
 1
n

,
where the indices on the four-point vertex are raised by the inverse metric G(ℓ)
V (α1α2)(α3α4)
(ℓ)
≡
X
β1,...,β4∈D
Gα1β1
(ℓ)
Gα2β2
(ℓ)
Gα3β3
(ℓ)
Gα4β4
(ℓ)
V (ℓ)
(β1β2)(β3β4)
(4.120)
=
X
β1,...,β4∈D
Kα1β1
(ℓ)
Kα2β2
(ℓ)
Kα3β3
(ℓ)
Kα4β4
(ℓ)
V (ℓ)
(β1β2)(β3β4) + O
 1
n

.
These recursions dictate how the statistics of preactivations ﬂow with depth.
This ﬂow is very reminiscent of the following heuristic picture, which is oﬀered as an
explanation for how neural networks are supposed to work: given an input, such as the
image of a cat, the ﬁrst few layers identify low-level features from the pixels – such as
the edges between areas of low and high intensity – and then the middle layers assemble
these low-level features into mid-level features – such as the texture and pattern of fur –
which are further aggregated in deeper layers into higher-level representations – such as
tails and ears – which the last layer combines into an estimate of the probability that
original pixels represents a cat. Indeed, some studies support this hierarchically-ordered
arrangement of feature representation in trained networks [34].19 The desirability of such
an arrangement emphasizes both the role and importance of depth in deep learning.
Some of the terms we used in discussing this heuristic picture can actually be given
more precise deﬁnitions. For instance, each neuron in the network – including not only
19It has been suggested that even untrained networks have features that can act as types of ﬁlters,
eﬀectively allowing for primitive edge detecting in untrained networks. For a related set of ideas, see [35].
104

those in the output layer but also those in the hidden layers – is a scalar function of
the input and called a feature. The neurons of a given layer can be organized into a
vector-valued function of the input, which we’ll refer to as a representation.20 In terms
of these concepts, our formalism tracks the transformation of representations from one
layer to the next. It is this ﬂow of representations that we term representation group
ﬂow or RG ﬂow for short.21 RG ﬂow is induced via the repeated marginalization of
ﬁne-grained features in the shallow layers to give a coarse-grained representation in the
output layer. Our notion of RG ﬂow makes the heuristic picture given above concrete.
This pattern of coarse-graining has a parallel in theoretical physics, known as renor-
malization group ﬂow or RG ﬂow for short. In this case, the RG ﬂow is generated
by the repeated marginalization of the microscopic ﬁne-grained degrees of freedom in
the system in order to obtain an eﬀective theory of the system in terms of macroscopic
coarse-grained variables. Analogously, the physical couplings controlling the interactions
of these eﬀective degrees of freedom run with the length scale at which they are probed –
e.g. the eﬀective charge of the electron will change when interrogated at diﬀerent scales.
Similar to the recursion equations describing the running couplings of the network rep-
resentations, one can derive diﬀerential equations – historically called beta functions –
that govern the running of the physical couplings with scale.22
20While the main focus of our study of supervised learning (§7) will be understanding how the represen-
tation z(L) in the output layer is learned via gradient-based training, it is also important to understand
how representations are learned in hidden layers (§11). In addition to being necessary components of de-
termining the coarse-grained representation at the output, in some applications of deep learning learned
representations in the hidden layers can be used as inputs themselves for other learning tasks. This
occurs quite often in unsupervised learning, for example with the word embeddings of natural language
processing tasks. In these scenarios, the embeddings – representations for an input word in the larger
context of a full sentence – typically are taken not just from the ﬁnal layer, but from the concatenation
of the ﬁnal few layers. See e.g. [36].
21Two apologies are in order for the name representation group ﬂow: (i) it is confusingly close to
the notion of group representation theory in mathematics; and (ii) the ﬂow is technically a semigroup,
rather than a group. (Both group theory and semigroup theory are the studies of transformations but a
group requires inverses while a semigroup does not; and the ﬂow has no inverse.) This is just to repeat
a historic mistake in physics as we’ll explain further in a footnote below.
22A brief history of renormalization in physics.
Renormalization was originally developed in the 1930s and 1940s to deal with divergences – inﬁnities
– that plagued the calculations of experimental observables in quantum ﬁeld theory.
At ﬁrst, these
inﬁnities were simply subtracted oﬀ– swept under the rug, if you will – yielding answers that, despite
these shenanigans, matched extremely well with experiments. This whole state of aﬀairs was considered
embarrassing, leading to near abandonment of the theory.
These divergences arose essentially due to a failure to properly take into account that couplings can
be scale-dependent. The idea of running couplings was ﬁrst put forth by Gell-Mann and Low [37] in
1954, however a full conceptualization of renormalization wasn’t available until Wilson developed the
modern notion of RG ﬂow [38, 39] in 1971, oﬀering a theoretical explanation for critical phenomena in
statistical physics as well as giving a sound grounding for the understanding of divergences in quantum
ﬁeld theory.
At this point, all historical accounts of RG are contractually obligated to mention the following: the
renormalization group is not a group; it’s a semigroup. (The mistake was made in an early paper by
Stueckelberg and Petermann, referring to the ﬂow as a “group of normalization” [40].) Mathematically,
this is because there are no inverse elements; the marginalization of variables out of a joint distribution
105

To make the connection between this RG ﬂow and that RG ﬂow abundantly clear,
let’s peek into how it is implemented in ﬁeld theory in physics. In this scenario, the
degrees of freedom are represented by a ﬁeld φ(x) that may take diﬀerent values as a
function of spacetime coordinate x. First, one divides φ(x) into ﬁne-grained variables
φ+ consisting of high-frequency modes and coarse-grained variables φ−consisting of
low-frequency modes, such that the ﬁeld decomposes as φ(x) = φ+(x) + φ−(x). The full
distribution is governed by the full action
Sfull(φ) = S(φ+) + S(φ−) + SI(φ+, φ−) ,
(4.121)
where in particular the last term describes the interactions between these two sets of
modes.
Now, if all we care about are observables that depend only on the coarse-grained
modes φ−at macroscopic scales – and such long-range scales are usually the relevant
ones for experiments – then this full description is too cumbersome to usefully describe
the outcome of such experiments. In order to obtain an eﬀective description in terms of
only these coarse-grained variable φ−, we can integrate out (i.e. marginalizes over) the
ﬁne-grained variables φ+ as
e−Seﬀ(φ−) =
Z
dφ+ e−Sfull(φ) ,
(4.122)
and obtain an eﬀective action Seﬀ(φ−), providing an eﬀective theory for the observ-
ables of experimental interest. In practice, this marginalization is carried out scale by
scale, dividing up the ﬁeld as φ = φ(1) + . . . + φ(L) from microscopic modes φ(1) all the
way to macroscopic modes φ(L) = φ−, and then integrating out the variables φ(1), . . . ,
φ(L−1) in sequence. Tracking the ﬂow of couplings in the eﬀective action through this
marginalization results in the aforementioned beta functions, and in solving these diﬀer-
ential equations up to the scale of interest, we get an eﬀective description of observables
at that scale.
This is precisely what we have been doing in this chapter for neural networks. The
full ﬁeld φ is analogous to a collection of all the preactivations
n
z(1), . . . , z(L)o
. Their
distribution is governed by the full joint distribution of preactivations
p

z(1), . . . , z(L)D

= p

z(L)z(L−1)
· · · p

z(2)z(1)
p

z(1)D

,
(4.123)
with the full action
Sfull

z(1), . . . , z(L)
≡
L
X
ℓ=1
SM

z(ℓ)
+
L−1
X
ℓ=1
SI

z(ℓ+1)z(ℓ)
.
(4.124)
deletes information and cannot be undone. In particular, two diﬀerent joint distributions can sometimes
ﬂow to the same distribution after marginalization.
Intuitively, this is because these ﬂows go from
ﬁne-grained descriptions to coarse-grained descriptions. (Such convergent ﬂows lead to the notion of
universality, which we will explain in §5 in the context of neural networks with diﬀerent activations that
ﬂow to the same marginal distributions under RG.)
Clearly, RG ﬂow in physics is a very rich subject. If you’re interested in learning more, we recommend
both [41, 42].
106

Here, the full action is decomposed into the mean quadratic action for variables z(ℓ)
SM

z(ℓ)
= 1
2
nℓ
X
i=1
X
α1,α2∈D
Gα1α2
(ℓ)
z(ℓ)
i;α1z(ℓ)
i;α2
(4.125)
in terms of the mean metric G(ℓ), (4.72), and the interaction between neighboring layers
SI

z(ℓ+1)z(ℓ)
= 1
2
nℓ+1
X
i=1
X
α1,α2∈D
h
bGα1α2
(ℓ+1)

z(ℓ)
−Gα1α2
(ℓ+1)
i
z(ℓ+1)
i;α1 z(ℓ+1)
i;α2
.
(4.126)
Here we emphasized that the stochastic metric bG(ℓ+1)
α1α2 is a function of z(ℓ), and the
induced coupling of z(ℓ) with z(ℓ+1) is what leads to the interlayer interactions.
Now, if all we care about are observables that depend only on the outputs of the
network – which includes a very important observable . . . the output! – then this full de-
scription is too cumbersome. In order to obtain an eﬀective (i.e. useful) description of the
distribution of outputs z(L), we can marginalizes over all the features
n
z(1), . . . , z(L−1)o
as
e−Seﬀ(z(L)) =
Z "L−1
Y
ℓ=1
dz(ℓ)
#
e−Sfull(z(1),...,z(L)) ,
(4.127)
just as we integrated out the ﬁne-grained modes φ+ in (4.122) to get the eﬀective de-
scription in terms of coarse-grained modes φ−. And, just like in the ﬁeld theory example,
rather than carrying out this marginalization all at once, we proceeded sequentially, in-
tegrating out the preactivations layer by layer. This resulted in the recursion relations
(4.118) and (4.119), and in solving these recursion relations up to the depth of interest,
we get an eﬀective description of neural network output at that depth.23
Now, this last sentence suggests a subtle but interesting shift of perspective, so let
us elaborate. So far in this chapter, we have implicitly assumed a ﬁxed network depth
L and described how the preactivation distribution changes as an input x propagates
through the intermediate layers, yielding recursion relations for correlators and couplings
for the evolution from layer ℓto layer ℓ+1, for ℓ= 0, . . . , L−1. However, it is also valid
to view the resulting recursion equations as governing the change in output distributions
as the overall network depth changes from L to L + 1.24 In other words, these recursion
relations describe the eﬀect of adding an additional layer to the neural network by
comparing distributions p

z(L)D

and p

z(L+1)D

.
Given this perspective, our RG ﬂow can address head-on the eﬀect of the deep in deep
learning. For instance, as a network get deeper, do the interactions between neurons –
encoded in the ﬁnite-width corrections such as the four-point vertex V (ℓ) – get ampliﬁed
or attenuated? In the language of RG ﬂow, couplings that grow with the ﬂow are called
23Note to physicists: the ﬂow in networks from input to output is a ﬂow from the ultraviolet to the
infrared.
24To be precise, the output dimension nout is ﬁxed. So, as the depth changes from L to L + 1, we
imagine holding ﬁxed the widths for ℓ< L, inserting a new layer L with nL ∼n ≫1, and then setting
the ﬁnal layer L + 1 to have width nout.
107

relevant and those that shrink are called irrelevant.25 These names are evocative of
whether the interaction matters or not for the eﬀective theory, and so we’ll employ the
same terminology. Thus, to explore the eﬀect of depth on the neuron-neuron interactions,
we are simply asking whether the four-point vertex V (ℓ) is relevant or irrelevant.
This question has important implications for deep learning. If all the ﬁnite-width
couplings were irrelevant, then ﬁnite-width networks would asymptote to inﬁnite-width
architectures under RG ﬂow. This would then mean that these networks behave more
like inﬁnite-width models as they get deeper, and so deep learning would really be the
study of these much simpler Gaussian models.
Fortunately we’ll soon ﬁnd that the
couplings are relevant, making our life richer, albeit more complicated.
In the next
chapter, we’ll show that ﬁnite networks deviate more and more from their inﬁnite-width
counterparts as they get deeper. This has important practical consequences in controlling
the instantiation-to-instantiation ﬂuctuations in supervised training and also in allowing
networks to learn nontrivial representations of their input (§11).
The next chapter explores these relevant questions by explicitly solving recursion
equations such as (4.118) and (4.119).
25Couplings that neither grow nor shrink are called marginal.
108

Chapter 5
Eﬀective Theory of Preactivations
at Initialization
We believe this realm of work to be immensely important and rich, but we expect its
growth to require a degree of critical analysis that its more romantic advocates have
always been reluctant to pursue . . . .
Minsky and Papert in the prologue to their 1988 expanded edition of Perceptrons [43].
The key deﬁning feature of deep learning is the stacking of components on top of each
other in order to get a deep neural network architecture. Despite the empirical preference
for deeper networks, it’s not at all obvious why deep is good for learning. For a ﬁxed
number of neurons per layer, deep implies many more parameters, and often in deep
learning more parameters lead to better performance.
But there are other ways to
include more parameters. For instance, why not just have a single hidden layer that is
very wide? In fact, in the strict inﬁnite-width limit, such a single-layer model has the
same number of parameters as any deeper MLP: inﬁnity.
The proper way to think about the eﬀects of depth is not to just count the number
of model parameters, but instead to ask what happens when we add an additional
layer to our MLP. In §4, we developed a formalism to address exactly this question
through recursions for observable quantities of interest, enabling us to compute how the
distributions of initial network outputs change upon adding a layer. What we need,
then, is a tool to eﬀectively extract the explicit depth dependence from these recursions.
Building on the eﬀective theory formalism developed in §4, in this chapter we’ll ex-
tend the criticality and ﬂuctuation analyses performed in §3 to MLPs with any nonlinear
activation function. Enlightened by the success of the previous chapter in ﬁnding sim-
pliﬁcation in the wide regime (n ≫1), we now seek additional simplicity in the limit
of large depth (L ≫1).1
We’ll ﬁrst analyze the limit of inﬁnite number of neurons
per layer, and then back oﬀthis limit to consider networks of large but ﬁnite width and
1What this means – in light of the discussion in §3.4 – is that we take the limit of large width ﬁrst
and then look at the limit of large depth.
109

depth. The result will be explicit expressions for the two-point and four-point correlators
of preactivations in these asymptotic limits.2
This will let us address the question of what happens to input signals as they propa-
gate through the many layers of deep neural networks at initialization (§5.1). We’ll come
to understand that the order-one values of the initialization hyperparameters – i.e. the
bias variances C(ℓ)
b
and the rescaled weight variances C(ℓ)
W – have pronounced qualitative
eﬀects on the behavior of the observables, just as we saw in §3 for deep linear networks.
In particular, we’ll explain how neural-network behavior becomes increasingly sensitive
to these initialization hyperparameters with increasing depth.3
Such a tuning brings a network to criticality, a term we borrow from statistical
physics used to describe self-similar systems. To this end, we give a general prescription
for tuning initialization hyperparameters to their critical values for a given activation
function and network architecture (§5.2 and §5.3). In the process, we also identify some
activation functions that don’t allow for criticality. We will also see that certain activa-
tion functions behave very similarly to each other when tuned to criticality, highlighting
an important connection to the notion of universality in statistical physics.
The study of ﬁnite-width corrections at criticality then leads us to one of the main
results of this chapter, an emergent scale given by the aspect ratio of the network depth
to the network width, L/n (§5.4). This aspect ratio ultimately serves as the cutoﬀof our
eﬀective theory, controlling the region of validity of our eﬀective theory as well as deter-
mining the strength and importance of the ﬁnite-width corrections to the inﬁnite-width
description. On the one end of the spectrum, we ﬁnd that the shorter and fatter networks
are, the more and more that they behave like their inﬁnite-width counterparts. On the
other end, skinny and tall networks become increasingly dominated by non-Gaussian
ﬂuctuations due to interactions between neurons. Overall, this serves to generalize our
ﬂuctuation analysis for deep linear networks in §3.3.
Lastly, we’ll conclude the chapter by addressing and resolving a subtlety that arises
in our criticality analysis for non-smooth activation functions such as the ReLU (§5.5).
5.1
Criticality Analysis of the Kernel
For the bulk of this chapter, our goal is to extend the notion of criticality discussed in §3
to deep MLPs with general activation functions σ(z). Our starting point is the kernel
recursion
K(ℓ+1)
αβ
= Cb + CW ⟨σασβ⟩K(ℓ) ,
(5.1)
2Despite the asymptotic nature of these solutions, we note many of these tools were developed to
study the strong interactions, where a parameter that is 3 in practice is taken to be inﬁnity. Thus,
sometimes even 3 ∼∞, since 1/3 ≪1 can be made to work as perturbative parameter [44].
3The initial part of this analysis was ﬁrst carried out in a series of papers, [45–47], using a diﬀerent set
of techniques that are ultimately equivalent to ours in the inﬁnite-width limit. Extending this analysis,
we’ll identify two very general conditions according to the principle of criticality that let us determine
the correct order-one values for the initialization hyperparameters. In §10.3, we’ll see that the need for
these conditions can also be understood by demanding that fully-trained networks generalize well.
110

derived in the previous chapter. As a reminder, the kernel K(ℓ)
αβ, deﬁned by (4.116), is
the inﬁnite-width limit of the mean metric G(ℓ)
αβ. Here, in order to restrict the number of
distinct hyperparameters, we have set the bias variance C(ℓ)
b
= Cb and the rescaled weight
variance C(ℓ)
W = CW to be layer-independent. The initial condition for this recursion is
given by the ﬁrst-layer kernel
K(1)
αβ = Cb + CW
 
1
n0
n0
X
i=1
xi;αxi;β
!
,
(5.2)
set by the inner products of inputs Pn0
i=1 xi;αxi;β. Our goal is to analyze how the kernel
changes as a function of layer and, as we’ll see, this analysis at the level of the kernel
is suﬃcient to pin down the critical initialization hyperparameters to leading order in
1/width.
For deep linear networks, the Gaussian expectation of the activations with respect
to the kernel is just given by the kernel, ⟨σασβ⟩K(ℓ) = ⟨zαzβ⟩K(ℓ) = K(ℓ)
αβ, and the
recursion equation was simple enough for us to obtain a full solution. Stepping outside
the realm of the linear activation function, however, the kernel recursion acquires two
new complications that require some care: (i) the expectation value ⟨σασβ⟩K(ℓ) will be
a nonlinear function of the kernel, and (ii) for two distinct inputs α ̸= β the recursion
mixes oﬀ-diagonal components K(ℓ)
αβ with diagonal components K(ℓ)
αα and K(ℓ)
ββ.
Let’s illustrate this with a quadratic activation function σ(z) = z2. As should be
second nature by now, evaluating (5.1) requires two pairs of Wick contractions, giving
K(ℓ+1)
αβ
= Cb + CW
D
z2
αz2
β
E
K(ℓ)
(5.3)
= Cb + CW

K(ℓ)
ααK(ℓ)
ββ + 2K(ℓ)
αβK(ℓ)
αβ

.
Thus, unlike deep linear networks, for quadratic activations K(ℓ+1)
αβ
depends not only on
K(ℓ)
αβ but also on K(ℓ)
αα and K(ℓ)
ββ, requiring us to solve three coupled nonlinear recursion
equations for α ̸= β. This mixing is generic for nonlinear activation functions.
For practitioners, this is good news: the oﬀ-diagonal elements of the kernel are related
to the generalization ability of the network. For deep linear networks the lack of mixing
via nonlinearity suggests an inductive bias that limits such networks’ ability to develop
nontrivial correlations for pairs of samples. While mixing is a beneﬁt in practice, it’s
an obstacle in theory, albeit a surmountable one. Since the kernel recursion mixes at
most two inputs, it is suﬃcient to analyze the case with a single input and the case with
two distinct inputs. We shall now perform these analyses in turn, with an eye towards
deriving the general conditions for criticality.
111

A single input
Each diagonal component of the kernel can be solved self-consistently by itself. Speciﬁ-
cally, labeling a single input by α = 0,
K(ℓ+1)
00
= Cb + CW ⟨σ(z0) σ(z0)⟩K(ℓ)
(5.4)
= Cb + CW g

K(ℓ)
00

.
Here, we introduced a helper function
g(K) ≡⟨σ(z) σ(z)⟩K ≡
1
√
2πK
Z ∞
−∞
dz e−z2
2K σ(z) σ(z) ,
(5.5)
to emphasize that the expectation ⟨σ(z0)σ(z0)⟩K(ℓ) is a function only of a single compo-
nent of the kernel, K(ℓ)
00 . By focusing our attention ﬁrst on the single-input kernel, we
can deal with the nonlinearity before confronting the mixing of kernel components.
In particular, the single-input recursion (5.4) is really telling us how the average
magnitude of the preactivations for the input
K(ℓ)
00 = E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;0
2
#
,
(5.6)
changes as a function of layer ℓ, with the initial condition for the recursion set by (5.2).
For the same reasons we considered in §3.2, we would like that the kernel K(ℓ)
00 neither
exponentially explode nor exponentially vanish. However, such exponential behavior is
generic, and so for most choices of initialization hyperparameters (Cb, CW ), the kernel
will either explode exponentially towards a trivial ﬁxed point at inﬁnity or collapse
exponentially onto a trivial ﬁxed point at a ﬁnite value K⋆
00. Thus, our ﬁrst criticality
condition is to mitigate this exploding or collapsing kernel problem for the single input.
There is an ancient technique used to analyze nonlinear recursions such as the single-
input kernel recursion (5.4): linearization around a ﬁxed point. Namely, we ﬁrst identify
a ﬁxed point of the recursion, i.e. a value K⋆
00 that satisﬁes
K⋆
00 = Cb + CW g(K⋆
00) ,
(5.7)
and then expand the kernel around it as
K(ℓ)
00 = K⋆
00 + ∆K(ℓ)
00 .
(5.8)
This expansion for the single-input recursion (5.4) results in the linearized recursion
∆K(ℓ+1)
00
= χ∥(K⋆
00) ∆K(ℓ)
00 + O

∆2
,
(5.9)
where we introduced the parallel susceptibility
χ∥(K) ≡CW g′(K)
(5.10)
= CW
d
dK

1
√
2πK
Z ∞
−∞
dz e−z2
2K σ(z) σ(z)

= CW
2K2
D
σ(z) σ(z)

z2 −K
E
K .
112

The susceptibility χ∥(K) characterizes how susceptible the kernel is to perturbations
around the ﬁxed point, hence the name: the kernel value exponentially expands away
from or contracts towards the ﬁxed-point value, according to whether χ∥(K⋆
00) > 1 or
χ∥(K⋆
00) < 1. (The label parallel will be explained at the very end of this section.)
Thus, we see that in order to mitigate this exploding and vanishing kernel problem
for a single input at linear order, we require the tuning of initialization hyperparameters
(Cb, CW ) such that
χ∥(K⋆
00) = 1 ,
(5.11)
with the ﬁxed-point value K⋆
00 deﬁned implicitly through the ﬁxed-point equation (5.7).
As we shall detail later, criticality can happen in three ways depending on the choice of
activation functions.
• First, as we saw for deep linear networks, the single-input kernel can be perfectly
preserved as K(ℓ)
00 = K(1)
00 = Cb + CW (P
i xi;0xi;0/n0), resulting in a line of ﬁxed
points parametrized by input norms P
i xi;0xi;0. We will see this happening in §5.2
for scale-invariant activation functions due to the absence of higher-order correc-
tions O
 ∆p>1 in (5.9).
• Second, the kernel can slowly decay toward a ﬁxed point K⋆
00 = 0 for all input
norms, with a power law K(ℓ)
00 ∼1/ℓq with 0 < q ≤1. We will see this happening
in §5.3.3 for a class of activation functions that include tanh and sin, due to the
presence of O
 ∆p>1 corrections in (5.9).
• Third, criticality can happen with a power-law decay towards a nonzero ﬁxed-
point value K⋆
00 ̸= 0. We will see this happening in §5.3.4 for the SWISH and GELU
activation functions.
In all these cases, when initialization hyperparameters are tuned to criticality, we call
K⋆
00 = K⋆
00

Ccritical
b
, Ccritical
W

a nontrivial ﬁxed point, distinguishing it from trivial
ﬁxed points for generic hyperparameters around which perturbations behave exponen-
tially.
Two inputs
Now, given two distinct inputs, let’s label them with sample indices α = ±. For such
a pair of inputs, we have three distinct kernel components to consider: K(ℓ)
++, K(ℓ)
−−,
and K(ℓ)
+−= K(ℓ)
−+. The single-input analysis can be directly applied to determine the
layer dependence of the diagonal components K(ℓ)
++ and K(ℓ)
−−, so to complete our anal-
ysis we need to extract the layer dependence of the oﬀ-diagonal component K(ℓ)
+−, given
solutions for the diagonal pieces. Such an analysis will yield a second criticality condi-
tion that, together with (5.11), will pin down the critical initialization hyperparameters
(Cb, CW )critical for a given activation function.
Ultimately, our approach will be to linearize around the degenerate limit where both
inputs coincide identically, i.e., xi;+, xi;−→xi;0. In such a limit, all the ways of pairing
113

up the two inputs are the same, and so all the components of the full kernel matrix must
take the same value, i.e., K(ℓ)
++, K(ℓ)
−−, K(ℓ)
+−→K(ℓ)
00 . Thus, each recursion degenerates to
the same single-input recursion (5.4), which we know has a ﬁxed-point value K⋆
00. This
means that the coincident-limit solution,
 
K(ℓ)
++
K(ℓ)
+−
K(ℓ)
−+
K(ℓ)
−−
!
=
 
K⋆
00
K⋆
00
K⋆
00
K⋆
00
!
= K⋆
00
 
1
1
1
1
!
,
(5.12)
must also be a ﬁxed point of the full kernel recursion for the two inputs.
There are three diﬀerent kinds of perturbations that we need to consider in order
to understand the approach of the full kernel matrix to this degenerate ﬁxed point.
The ﬁrst kind corresponds to the perturbation ∆K(ℓ)
00 that appeared in our single-input
analysis, which controls how the average of the kernel’s four components approaches the
ﬁxed-point value K⋆
00. Next, in backing oﬀthe coincident limit, the single input splits
into two distinct inputs, xi;0 →xi;+, xi;−. Then, we could imagine separating these two
inputs so that they become endowed with diﬀerent magnitudes, i.e. P
i x2
i;+ ̸= P
i x2
i;−,
and follow the expected evolution of this diﬀerence through the network:
R(ℓ) ≡E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;+
2
#
−E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;−
2
#
= K(ℓ)
++ −K(ℓ)
−−.
(5.13)
Such a perturbation is actually still covered by the single-input analysis, since the evolu-
tions of the diagonal components K(ℓ)
++ and K(ℓ)
−−are mutually independent of each other,
with their approach to the ﬁxed point simply controlled by the single-input recursion.
Finally, rather than considering the diﬀerence of the squares, we could consider the
square of the diﬀerence
D(ℓ) ≡E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;+ −z(ℓ)
i;−
2
#
= K(ℓ)
++ + K(ℓ)
−−−2K(ℓ)
+−,
(5.14)
where to get the expression on the right-hand side we expanded the binomial and then
used the deﬁnition of the kernel components. This quantity measures the magnitude
of the diﬀerence between the two inputs after being passed through ℓlayers of the
network and can be non-vanishing even when such inputs themselves have the same
magnitudes, i.e. P
i x2
i;+ = P
i x2
i;−. Importantly, this distance measure D(ℓ) depends
on the oﬀ-diagonal component of the kernel, K(ℓ)
+−, and so we expect that analyzing
this perturbation will give something new. As we will see, the approach of this third
perturbation D(ℓ) to the coincident ﬁxed point with D(ℓ) = 0 will yield a second criti-
cality condition. Together with the single-input criticality condition (5.11), this will be
suﬃcient to completely determine the critical initialization hyperparameters.
Let’s translate the above discussion into math. To do so, we will ﬁnd it convenient
to project the full kernel matrix into the following basis
K(ℓ)
α1α2 =
 
K(ℓ)
++
K(ℓ)
+−
K(ℓ)
−+
K(ℓ)
−−
!
= K(ℓ)
[0] γ[0]
α1α2 + K(ℓ)
[1] γ[1]
α1α2 + K(ℓ)
[2] γ[2]
α1α2 ,
(5.15)
114

where we’ve introduced symmetric matrices
γ[0]
α1α2 ≡
 
1
1
1
1
!
,
γ[1]
α1α2 ≡
 
1
0
0
−1
!
,
γ[2]
α1α2 ≡
 
1
−1
−1
1
!
.
(5.16)
In this basis, the components of the kernel are
K(ℓ)
[0] = 1
4
h
K(ℓ)
++ + K(ℓ)
−−+ 2K(ℓ)
+−
i
= E

1
nℓ
nℓ
X
i=1

z(ℓ)
i;+ + z(ℓ)
i;−
2


2
,
(5.17)
K(ℓ)
[1] = 1
2
h
K(ℓ)
++ −K(ℓ)
−−
i
= 1
2R(ℓ) ,
(5.18)
K(ℓ)
[2] = 1
4
h
K(ℓ)
++ + K(ℓ)
−−−2K(ℓ)
+−
i
= 1
4D(ℓ) .
(5.19)
This basis was strategically chosen so that both K(ℓ)
[1] and K(ℓ)
[2] correspond to our two
natural distance measures of two distinct inputs – the diﬀerence in the magnitudes R(ℓ)
and the magnitude of the diﬀerence D(ℓ) – and both vanish in the coincident limit. The
remaining component, K(ℓ)
[0] , measures the overall average magnitude or the magnitude
of the center of mass of the two ℓ-th-layer preactivations.
This basis has two additional nice properties that will later prove useful: (i) both
K(ℓ)
[0] and K(ℓ)
[2] are even (invariant) under the parity swap of two inputs + ↔−, while
K(ℓ)
[1] is odd, changing its sign K(ℓ)
[1] →−K(ℓ)
[1] as + ↔−, and (ii) the γ[a] matrices are
orthogonal.4
Now, let’s discuss perturbations around the coincident ﬁxed point (5.12) in terms
of this new basis. These perturbations have a very natural interpretation in terms of
two inﬁnitesimally-separated input points, xi;+ and xi;−, perturbed around a midpoint
input xi;0 ≡(xi;+ + xi;−)/2 as
xi;± = xi;0 ± 1
2δxi .
(5.21)
The dynamics of such preactivations z(ℓ)
i;± ≡z(ℓ)
i (x±) then encode the evolution of these
perturbed signals through the network as a function of layer depth ℓ. The coincident
limit corresponds to δxi →0 and, as we back oﬀfrom this limit, we should be able
4This symmetry decomposition mirrors tensorial decomposition used by physicists to organize par-
ticles by their spin. Operationally, we can project out the components of any 2 × 2 matrix Mαβ into
components M[a] in the γ[a] basis by tracing over the sample indices and normalizing
M[a] =
P
α,β Mαβγ[a]
βα
P
α,β γ[a]
αβγ[a]
βα
,
(5.20)
with α, β ∈{+, −}. One can easily check that the γ[a]
αβ matrices themselves are orthogonal under this
inner product (5.20).
115

to expand the K(ℓ)
[a] components around their coincident-limit values K(ℓ)
[0] = K(ℓ)
00 and
K(ℓ)
[1] = K(ℓ)
[2] = 0. This results in an expansion
K(ℓ)
[0] = K(ℓ)
00 + δδK(ℓ)
[0] + O

δ4
,
(5.22)
K(ℓ)
[1] = δK(ℓ)
[1] + δδδK(ℓ)
[1] + O

δ5
,
(5.23)
K(ℓ)
[2] = δδK(ℓ)
[2] + δδδδK(ℓ)
[2] + O

δ6
,
(5.24)
where the order of the kernel perturbation in δx is denoted by a preceding δp, and we
used the even/odd behavior of the components K(ℓ)
[a] under the parity symmetry + ↔−
to limit which terms appear in each expansion.5 Next, we will use these expansions to
determine whether the behavior of the leading perturbations δK(ℓ)
[1] and δδK(ℓ)
[2] around
their ﬁxed point values are exponential, power-law, or constant.
To do so, we’ll need to expand the original kernel recursion (5.1) order by order in δ.
Before embarking on such an algebraic journey, let’s think about what we should expect.
At the zeroth order in δ, we’ll just recover the recursion for the single-input kernel (5.4)
K(ℓ+1)
00
= Cb + CW g

K(ℓ)
00

.
(5.25)
This should demystify our notational choice in the single-input analysis, as K(ℓ)
00 simply
represents the kernel for a single input x0 corresponding to the midpoint of a pair of
inputs x+, x−, as per (5.21). Going forward, we will call K(ℓ)
00 the midpoint kernel.6
Next, at ﬁrst order in δ, we will get the recursion
δK(ℓ+1)
[1]
= χ∥

K(ℓ)
00

δK(ℓ)
[1] .
(5.26)
This is to be expected. On account of the parity symmetry, δK(ℓ+1)
[1]
can only be propor-
tional to δK(ℓ)
[1] at this order, and the proportionality factor must be none other than the
parallel susceptibility, because K(ℓ)
[1] = 1
2
h
K(ℓ)
++ −K(ℓ)
−−
i
behaves in the same way as the
single-input kernels: if the single-input kernels behave exponentially, then this diﬀerence
should as well.
Lastly, at the second order in δ, we expect a recursion of the form
δδK(ℓ+1)
[2]
= [something] δδK(ℓ)
[2] +
something′ 
δK(ℓ)
[1]
2 +
something′′ δδK(ℓ)
[0] , (5.27)
5These expansions are valid when the activation function σ(z) is suﬃciently smooth. For non-smooth
activation functions such as ReLU, these expansions are more complicated, though still analyzable. We
will consider these subtleties in more detail in §5.5.
6We note that K(ℓ)
[0] is the kernel for the midpoint of the layer-ℓpreactivations, (z(ℓ)
i;++z(ℓ)
i;−)/2, which is
not quite the same as the midpoint kernel K(ℓ)
00 for the preactivations of the midpoint input x0 propagated
to layer ℓ. The diﬀerence is expressed in (5.22) and will turn out negligible for quantities at leading order
in the δ expansion.
116

which is the most general form it can take given the even parity symmetry of δδK(ℓ)
[2] ,
and where the [somethings] can be functions of the single-input kernel K(ℓ)
00 . In the rest
of this subsection we will derive the form of [something] and
something′, while also
showing that
something′′ vanishes due to the orthogonality of the γ[a]
αβ matrices.
Already at this heuristic level of the analysis, the bootstrapping nature of the system
of equations should be clear. First, we ﬁnd a solution for the midpoint kernel K(ℓ)
00 , which
then bootstraps the layer dependence of δK(ℓ)
[1] through (5.26), the solution of which in
turn feeds into (5.27) and together with K(ℓ)
00 bootstraps the layer dependence of δδK(ℓ)
[2] .
In other words, rather than confronting three coupled nonlinear recursions, we can solve
decoupled recursions one by one.
Deriving bootstrapped recursions
Now let’s embark on our algebraic journey. Our goal is to expand the kernel recur-
sion (5.1) up to the second order in δ. This requires us to evaluate ⟨σ(z+)σ(z+)⟩K(ℓ),
⟨σ(z−)σ(z−)⟩K(ℓ), and ⟨σ(z+)σ(z−)⟩K(ℓ) to that order, all of which are two-dimensional
Gaussian integrals. Rather than treating all of these Gaussian integrals separately, we
instead will evaluate the Gaussian expectation of an arbitrary function ⟨F (z+, z−)⟩K(ℓ)
and then will plug in F (z+, z−) = σ(z+)σ(z+), σ(z−)σ(z−), or σ(z+)σ(z−). Moreover,
we will ﬁnd that this general expression ⟨F (z+, z−)⟩K(ℓ) will come in handy in later
chapters.
In order to evaluate this Gaussian expectation, it is natural to write the integral
in the eigenbasis of the kernel rather than in the (z+, z−) coordinates.
Denote such
orthonormal eigenvectors by {ˆeu, ˆew}, which satisfy the eigenvalue equations
X
β=±
K(ℓ)
αβˆeu
β = λuˆeu
α ,
X
β=±
K(ℓ)
αβˆew
β = λwˆew
α ,
(5.28)
with eigenvalues λu and λw, respectively. Transforming to coordinates (u, w) deﬁned via
zα(u, w) = uˆeu
α + wˆew
α ,
(5.29)
the Gaussian expectation becomes
⟨F(z+, z−)⟩K(ℓ) =
R dudw exp

−u2
2λu −w2
2λw

F

z+ (u, w) , z−(u, w)

R dudw exp

−u2
2λu −w2
2λw

.
(5.30)
As we discussed in §1.3, this equation expresses the idea that the (u, w)-coordinate basis
diagonalizes the kernel such that the distribution factorizes as p(z+, z−) = p(u)p(w).
The integral in the denominator represents the normalization factor of this factorized
Gaussian expectation.
Now, we need to actually determine the eigenvalues λu and λw and eigenvectors
{ˆeu, ˆew}. We’ll start our perturbative eigen-analysis by taking the by-now-familiar co-
incidental limit, δ →0.
As we discussed around (5.22), in this limit the kernel is
117

degenerate:
K(ℓ)
αβ = K(ℓ)
00 γ[0]
αβ = K(ℓ)
00
 
1
1
1
1
!
,
(5.31)
with the γ[0]
αβ component equal to the midpoint kernel K(ℓ)
[0] = K(ℓ)
00 , and the other com-
ponents vanishing. Such a matrix has the normalized eigenvectors
ˆeu
α =
1
√
2
 
1
1
!
,
and
ˆew
α =
1
√
2
 
1
−1
!
,
(5.32)
with eigenvalues λu = 2K(ℓ)
00 and λw = 0, respectively.7
Next, let’s back oﬀfrom the coincidental limit and look back at the δ expan-
sions (5.22)–(5.24) for K(ℓ)
[0,1,2] around the midpoint kernel. With similar expansions for
the eigenvectors ˆeu,w
±
and eigenvalues λu,w, we can solve the eigenvalue equations (5.28)
order by order.8 Carrying out such expansions (in the margins or – if this isn’t your
personal copy of our book – in a private notebook) and solving (5.28) to second order,
we ﬁnd normalized eigenvectors
ˆeu
α =
 
ˆeu
+
ˆeu
−
!
=
1
√
2






1 +
δK(ℓ)
[1]
2K(ℓ)
00
−1
8
 
δK(ℓ)
[1]
K(ℓ)
00
!2
1 −
δK(ℓ)
[1]
2K(ℓ)
00
−1
8
 
δK(ℓ)
[1]
K(ℓ)
00
!2






+ O

δ3
,
(5.33)
ˆew
α =
 
ˆew
+
ˆew
−
!
=
1
√
2






1 −
δK(ℓ)
[1]
2K(ℓ)
00
−1
8
 
δK(ℓ)
[1]
K(ℓ)
00
!2
−1 −
δK(ℓ)
[1]
2K(ℓ)
00
+ 1
8
 
δK(ℓ)
[1]
K(ℓ)
00
!2






+ O

δ3
,
and corresponding eigenvalues
λu = 2K(ℓ)
00 + 2δδK(ℓ)
[0] +

δK(ℓ)
[1]
2
2K(ℓ)
00
+ O

δ4
,
(5.34)
λw = 2δδK(ℓ)
[2] −

δK(ℓ)
[1]
2
2K(ℓ)
00
+ O

δ4
.
Even if you don’t have a private notebook, it’s easy to check on a scrap of paper that
(5.33) and (5.34) solve (5.28) to O
 δ2.
7Here, the zero eigenvalue for w signiﬁes that the matrix is degenerate. This implies that the dis-
tribution for the w coordinate is given by a Dirac delta function, p(w) = δ(w), indicating that there’s
really only one input in this limit.
8For physicists, note that this is second-order time-independent perturbation theory from quantum
mechanics.
118

Now, having solved the eigenproblem, we can implement the change of coordinates.
Before doing so, notice that the u coordinate is closely related to the coordinate z0, the
preactivation corresponding to the midpoint input. This makes it very natural to use z0
as a coordinate instead of u. We can implement this by rescaling u as
u2
2λu
=
z2
0
2K(ℓ)
00
,
(5.35)
changing variables in the integral (5.30) so that the Gaussian integral over u becomes a
Gaussian integral over z0 with a variance given by the midpoint kernel K(ℓ)
00 . With this
rescaling, the full coordinate transformation becomes
z±(z0, w) = z0

1 ±

δK(ℓ)
[1]
2K(ℓ)
00

+

δδK(ℓ)
[0]
2K(ℓ)
00

+ O

δ3

+ w
√
2 [±1 + O(δ)] .
(5.36)
Here, we can truncate the term in the square brackets multiplying w at O(1), since the w
coordinate has zero mean and a variance λw = O
 δ2. This means that, when performing
the w integration, terms proportional to w0 will be O(1), terms proportional to w2 will
be O
 δ2, higher-order terms will be subleading, and, of course, all the odd terms will
vanish. By contrast, the z0 coordinate has zero mean and a variance K(ℓ)
00 = O(1), so we
actually need keep terms up to O
 δ2.
Next, we need to plug this expression (5.36) into our arbitrary function
F(z+, z−) = F

z+(z0, w), z−(z0, w)

,
(5.37)
now viewed as a function of the two independent Gaussian variables z0 and w, and
perform the integration over them. To do so, ﬁrst we need to Taylor expand the function
in both δ and w around F(z0, z0). This gives
F(z+, z−)
(5.38)
=F(z0, z0) + z0

δK(ℓ)
[1]
2K(ℓ)
00

(∂+ −∂−) F + z0

δδK(ℓ)
[0]
2K(ℓ)
00

(∂+ + ∂−) F
+ z2
0

δK(ℓ)
[1]
2K(ℓ)
00


2
(∂+ −∂−)2 F
2
+ w2
2
(∂+ −∂−)2 F
2
+ (odd in w) + O

δ3, w2δ, w4
,
with the abbreviation ∂p
+∂q
−F ≡∂p
+∂q
−F(z+, z−)|z+=z−=z0. The Gaussian integral over
w is simple to perform, we just replace w2 with its variance λw (5.34).
Finally, we
will express our ﬁnal answer in terms of single-variable Gaussian expectations over the
variable z0 – which, as you should recall, has a variance given by the scalar midpoint
119

kernel K(ℓ)
00 – giving
⟨F(z+, z−)⟩K(ℓ)
(5.39)
= ⟨F(z0, z0)⟩K(ℓ)
00 +

δK(ℓ)
[1]
2K(ℓ)
00

⟨z0 (∂+ −∂−) F⟩K(ℓ)
00 +

δδK(ℓ)
[0]
2K(ℓ)
00

⟨z0 (∂+ + ∂−) F⟩K(ℓ)
00
+ 1
2
*
δδK(ℓ)
[2] +

δK(ℓ)
[1]
2K(ℓ)
00


2 
z2
0 −K(ℓ)
00


(∂+ −∂−)2 F
+
K(ℓ)
00
+ O

δ3
.
This completes our computation of this general expectation.
In order to apply this formula to evaluate the expectations ⟨σ(zα)σ(zβ)⟩K(ℓ) in the
kernel recursion (5.1), recall deﬁnitions of gamma matrices in (5.16) and note
[σ(zα)σ(zβ)] |z+=z−=z0 = σ(z0)σ(z0)γ[0]
αβ ,
(5.40)
{(∂+ −∂−) [σ(zα)σ(zβ)]} |z+=z−=z0 = 2σ′(z0)σ(z0)γ[1]
αβ ,
(5.41)
{(∂+ + ∂−) [σ(zα)σ(zβ)]} |z+=z−=z0 = 2σ′(z0)σ(z0)γ[0]
αβ ,
(5.42)
n
(∂+ −∂−)2 [σ(zα)σ(zβ)]
o
|z+=z−=z0 = 2σ′′(z0)σ(z0)γ[0]
αβ + 2σ′(z0)σ′(z0)γ[2]
αβ .
(5.43)
Plugging these individually into our general expression (5.39), we get
⟨σ(zα)σ(zβ)⟩K(ℓ)
(5.44)
=

⟨σ(z0)σ(z0)⟩K(ℓ)
00 + O

δ2
γ[0]
αβ
+



δK(ℓ)
[1]
K(ℓ)
00


z0σ′(z0)σ(z0)

K(ℓ)
00

γ[1]
αβ
+

δδK(ℓ)
[2]

σ′(z0)σ′(z0)

K(ℓ)
00 +

δK(ℓ)
[1]
2K(ℓ)
00


2 D
z2
0 −K(ℓ)
00

σ′(z0)σ′(z0)
E
K(ℓ)
00

γ[2]
αβ .
The coeﬃcients of the matrix ⟨σ(zα)σ(zβ)⟩K(ℓ) in the γ[a]
αβ basis can be simply read oﬀ
from the above expression. Therefore, plugging this into the right-hand side of the full
kernel recursion (5.1), we can expand the left-hand side of that equation in this basis as
K(ℓ+1)
αβ
= K(ℓ+1)
[0]
γ[0]
αβ + K(ℓ+1)
[1]
γ[1]
αβ + K(ℓ+1)
[2]
γ[2]
αβ ,
(5.45)
and equate both sides to ﬁnd recursions for each component in this basis. These are
given just below.
120

Summary
Just above, we explained how to derive the recursions
K(ℓ+1)
00
= Cb + CW g

K(ℓ)
00

,
(5.46)
δK(ℓ+1)
[1]
= χ∥

K(ℓ)
00

δK(ℓ)
[1] ,
(5.47)
δδK(ℓ+1)
[2]
= χ⊥

K(ℓ)
00

δδK(ℓ)
[2] + h

K(ℓ)
00
 
δK(ℓ)
[1]
2 .
(5.48)
Here the by-now familiar helper function (5.5) is deﬁned as
g(K) = ⟨σ(z) σ(z)⟩K ,
(5.49)
the parallel susceptibility that we already encountered in (5.10) is given by
χ∥(K) = CW g′(K) = CW
2K2
D
σ(z) σ(z)

z2 −K
E
K = CW
K

z σ′(z) σ(z)

K ,
(5.50)
the perpendicular susceptibility is newly introduced as
χ⊥(K) ≡CW

σ′(z) σ′(z)

K ,
(5.51)
and the helper function that generates perturbations δδK(ℓ+1)
[2]
from perturbations δK(ℓ)
[1]
is given by
h(K) ≡CW
4K2
D
σ′(z) σ′(z)

z2 −K
E
K = 1
2
d
dK χ⊥(K) .
(5.52)
In the last steps of (5.50) and (5.52) we made use of the following identity for the
single-variable Gaussian expectation
d
dK

1
√
2πK
Z ∞
−∞
dz e−z2
2K F(z)

= 1
2K2

1
√
2πK
Z ∞
−∞
dz e−z2
2K F(z)(z2 −K)

(5.53)
= 1
2K

1
√
2πK
Z ∞
−∞
dz e−z2
2K z d
dz F(z)

,
where to go from the ﬁrst line to the second line we integrated by parts. These three
recursions (5.46)–(5.48) are suﬃcient to completely ﬁx the initialization hyperparameters
and tune the network to criticality.
The ﬁrst equation (5.46) is a recursion for the midpoint kernel K(ℓ)
00 . To analyze
this equation, we look for a ﬁxed-point value K⋆
00 satisfying K⋆
00 = Cb + CW g(K⋆
00) and
then linearize around such a ﬁxed point as K(ℓ)
00 = K⋆
00 + ∆K(ℓ)
00 . Doing so, we see that
∆K(ℓ+1)
00
= χ∥(K⋆
00) ∆K(ℓ)
00 + O
 ∆2 and realize that the parallel susceptibility χ∥(K⋆
00)
governs the growth/decay of deviations ∆K(ℓ)
00 from the ﬁxed-point value K⋆
00.
The second equation (5.47) is the ﬁrst equation (5.46) in disguise, since the δK(ℓ)
[1]
component is the leading diﬀerence in magnitude R(ℓ) =

K(ℓ)
++ −K(ℓ)
−−

/2 of preactiva-
tions for two inputs. As such, the same susceptibility χ∥

K(ℓ)
00

governs its growth/decay.
121

Another perspective is that the δK(ℓ)
[1] component can be generated by considering a per-
turbation δxi ∝xi;0 that is parallel to the original input xi;0, creating a diﬀerence in
the norm of the two inputs. This deviation is naturally measured by R(ℓ), and setting
χ∥(K⋆
00) = 1 ensures that such a perturbation neither exponentially explodes nor expo-
nentially vanishes. And that, after a long-winded journey, explains why we called this
susceptibility parallel.
This third recursion (5.48) is something new, controlling the layer dependence of
the magnitude of the diﬀerence of the two inputs D(ℓ) = 4δδK(ℓ)
[2] + O
 δ4.
Such a
perturbation in layer ℓ+1 is sourced by two types of perturbations in layer ℓ, as exhibited
by the two terms on right-hand side of (5.48).
One term ∝

δK(ℓ)
[1]
2 is generated
by preactivations in the ℓ-th layer with diﬀerent norms. The other term ∝δδK(ℓ)
[2] is
generated by preactivations in the ℓ-th layer with a nonzero diﬀerence D(ℓ) and is present
even if the preactivations have the same norm. Such same-norm perturbations in the
inﬁnitesimal regime correspond to perturbations of the input that are perpendicular to
the midpoint input, i.e. Pn0
i=1 xi;0 δxi = 0. The perpendicular susceptibility χ⊥(K⋆
00)
determines the dynamics of such perpendicular perturbations.9 As a nonzero distance
D(ℓ) is essential for being able to compare and contrast the two inputs xi;± after being
propagated to layer ℓ, we need to ensure that this quantity is well behaved. To avoid
exponential behavior, we will demand χ⊥(K⋆
00) = 1.
Taken all together, our general notion of criticality requires the following two condi-
tions to hold10
χ∥(K⋆
00) = 1 ,
χ⊥(K⋆
00) = 1 ,
(5.55)
with the ﬁxed-point value of the midpoint kernel K⋆
00 implicitly deﬁned via
K⋆
00 = Cb + CW g(K⋆
00) .
(5.56)
These conditions are suﬃcient to ensure that the entire kernel matrix is preserved to
9An alternative view is that, for a given instantiation of the network, this perpendicular susceptibility
χ⊥

K(ℓ)
00

controls changes of the preactivations with respect to changes in the input. To see that, note
that the distance D(ℓ) can be rewritten to leading order in the perturbation as
D(ℓ) = 1
nℓ
nℓ
X
i=1
E

z(ℓ)
i;+ −z(ℓ)
i;−
2
= 1
nℓ
nℓ
X
i=1
E
" n0
X
j=1
dz(ℓ)
i;0
dxj;0 δxj
!2 #
+ O δ4
.
(5.54)
This makes quantity χ⊥(K⋆
00) of interest for controlling the infamous exploding and vanishing gradient
problem, a perspective that we will make more concrete in §9.
10Note that this further underscores the need for an ensemble. In §2.3, we motivated the initialization
distribution by pointing out that the zero initialization b(ℓ)
i
= W (ℓ)
ij
= 0 doesn’t break the permutation
symmetry among the nℓneurons of a layer. Here we see more generally that any zero-mean deterministic
(i.e. CW = 0) distribution for the weights – which includes the zero initialization – cannot satisfy the
criticality conditions χ∥= χ⊥= 1, since both susceptibilities (5.50) and (5.51) are proportional to CW .
Such a zero-weight initialization will always suﬀer from an exponential decay towards a trivial ﬁxed point
at K⋆
00 = Cb.
122

leading order, namely that
∆K(ℓ+1)
00
= ∆K(ℓ)
00 + O

∆2
,
K(ℓ+1)
[1]
= K(ℓ)
[1] + O

δ3
,
K(ℓ+1)
[2]
= K(ℓ)
[2] + O

δ4
.
(5.57)
This generalizes the notion of criticality that we discussed for deep linear networks in
§3. Over two sections we will give a prescription for ﬁnding these critical initialization
hyperparameters (Cb, CW )critical for any nonlinear activation function.
5.2
Criticality for Scale-Invariant Activations
Now, let’s extend our criticality analysis to scale-invariant activation functions by ap-
plying the formalism that we just developed. Recall from §2.2 that a scale-invariant
activation function satisﬁes
σ(λz) = λσ(z) ,
(5.58)
for any positive rescaling λ > 0, and always takes the form
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 .
(5.59)
As a reminder, this class of activations includes the linear activation – by setting
a+ = a−= 1 – and the ReLU – by setting a+ = 1 and a−= 0.
These activation functions are particularly simple in that the criticality conditions
(5.55), χ∥

K(ℓ)
00

= χ⊥

K(ℓ)
00

= 1, can be solved exactly. To start, we can easily compute
g(K), (5.49), which reduces to two Gaussian integrals on half the real line times an even
polynomial, yielding
g(K) = A2K ,
(5.60)
where we have introduced an activation-dependent constant
A2 ≡a2
+ + a2
−
2
.
(5.61)
From (5.50), we see that we can ﬁnd χ∥(K) by diﬀerentiating this expression with
respect to K and multiplying by CW . Inspecting (5.51), we see that to get χ⊥(K), we
can perform two more simple Gaussian integrals on half the real line. Together, we ﬁnd
that both susceptibilities are equal and independent of K(ℓ)
00
χ∥

K(ℓ)
00

= χ⊥

K(ℓ)
00

= A2CW ≡χ .
(5.62)
Lastly h(K), (5.52), identically vanishes because it is a derivative of χ⊥(K).
With all that, we can write the general kernel recursions (5.46), (5.47), and (5.48)
for scale-invariant activations as
K(ℓ+1)
00
= Cb + χK(ℓ)
00 ,
(5.63)
δK(ℓ+1)
[1]
= χδK(ℓ)
[1] ,
(5.64)
δδK(ℓ+1)
[2]
= χδδK(ℓ)
[2] .
(5.65)
123

These are quite simple to solve. Just as the initialization hyperparameter CW governed
the exploding and vanishing kernel problem in §3.2, the constant susceptibility χ =
A2CW governs the same problem here:
• If χ > 1, all quantities explode exponentially in ℓtowards a trivial ﬁxed point at
inﬁnity.
• If χ < 1, the ﬁxed-point value of the kernel is given by K⋆
00 =
Cb
1−χ and all pertur-
bations around the ﬁxed point vanish exponentially with ℓ.
• If CW = 1/A2 and Cb = 0, then the network is at criticality. Not only does every
perturbation stays constant,11 but also any value of K⋆
00 serves as a nontrivial ﬁxed
point, i.e., there is a line of nontrivial ﬁxed points.12 In particular, the value of the
ﬁxed point is given by
K⋆
00 = 1
A2
 
1
n0
n0
X
i=1
x2
i;0
!
.
(5.66)
• If CW = 1/A2 and Cb > 0, then δK(ℓ)
[1] and δδK(ℓ)
[2] stay constant at this inﬁnitesimal
level of analysis. However, K(ℓ)
00 grows linearly towards a nontrivial ﬁxed point at
inﬁnity, with the rate set by Cb. Since the kernel does not exhibit any exponential
behavior, such a network is at criticality in a broad sense. This semi-criticality
results in a line of semi-critical initialization hyperparameters parameterized by
Cb in the hyperparameter plane spanned by (Cb, CW ).
In conclusion, this study generalizes the analysis carried out for deep linear networks
in §3.2 and identiﬁes
(Cb, CW )critical =

0, 1
A2

,
(5.67)
with A2 = (a2
+ + a2
−)/2 as the critical initialization hyperparameters for scale-invariant
activation functions.13 For the ReLU activation function, this reproduces the Kaiming
initialization (Cb, CW )critical = (0, 2) [48].
11 One caveat is in order.
While the constancy of the preactivation norm K(ℓ)
[0] and the parallel
perturbation K(ℓ)
[1] is exact, the constancy of K(ℓ)
[2] is an artifact of our inﬁnitesimal perturbation analysis.
In fact, the ﬁnite-angle analysis of nonlinear scale-invariant activation functions in §5.5 describes how
K(ℓ)
[2] crosses over from near constancy for small ℓto a power-law decay ∼1/ℓ2 for large ℓ. In short, the
preservation of the whole kernel matrix seen in §3.2 is a special property of the linear activation, and
for nonlinear scale-invariant activation functions there is a slow power-law decay of some observables.
This power-law behavior is quite benign compared to exponential behavior and is typical at criticality.
12For physicists, note that a similar line of ﬁxed points often appears in scale-invariant ﬁeld theories
with exactly marginal deformations.
13We see here that our simpliﬁcation of Cb = 0 for deep linear networks in §3 was completely warranted,
ex post facto.
124

5.3
Universality beyond Scale-Invariant Activations
All of the activation functions treated in the last section shared a rather special property:
scale invariance (5.58). This property gave rise to equal and kernel-independent parallel
and perpendicular susceptibilities, χ∥(K) = χ and χ⊥(K) = χ, all together enabling us
to drastically simplify the criticality analysis for these activation functions.14 Such an
analysis showed that networks equipped with a scale-invariant activation function will
behave similarly to each other under representation group ﬂow at criticality.
In theoretical physics, systems at criticality that behave similarly under renormal-
ization group ﬂow are said to fall into the same universality class. The eﬀective action
describing such systems converge under the iterative coarse-graining procedure, such
that at long-range scales these systems share the same underlying mathematical model
or eﬀective theory, independent of the microscopic details of the particular system. This
phenomenon is known as universality [49].
This motivates the use of the same term, universality class, to characterize activation
functions that share the same limiting behavior under representation group ﬂow, thus
furthering the connection between RG ﬂow and RG ﬂow that we began developing in
§4.6. Activation functions that form a universality class will have an identical eﬀec-
tive description after ﬂowing through many layers, meaning that the eﬀective theory
describing the preactivation distribution becomes independent of the ﬁne details of the
particular activation function. The power of universality is that a single eﬀective theory
enables us to understand criticality for the many diﬀerent activation functions within
the same universality class.
Clearly, all the scale-invariant activation functions form a universality class. However,
the simpliﬁcations that enabled us to easily analyze this scale-invariant universality
class, e.g. the kernel-independence of the susceptibilities, do not hold for other activa-
tion functions. For activation functions such as the sigmoid, tanh, or SWISH, we’ll need
to develop a much more general algorithm to ﬁnd critical initialization hyperparame-
ters. In §5.3.1, we’ll illustrate how this algorithm works, and then we’ll analyze speciﬁc
activation functions in §5.3.2, §5.3.3, and §5.3.4.
5.3.1
General Strategy
Let’s start with some recollections. As discussed most recently in §5.1, for a generic
choice of initialization hyperparameters Cb and CW , the kernel recursion for a single-
input x0,
K(ℓ+1)
00
= Cb + CW g

K(ℓ)
00

,
(5.68)
admits a ﬁxed-point solution satisfying
K⋆
00 = Cb + CW g(K⋆
00) ,
(5.69)
14The kernel-independence property follows directly from the scale-invariance, as any dependence
would have introduced a scale into the problem.
125

where the helper function
g(K) ≡⟨σ(z)σ(z)⟩K ,
(5.70)
is understood as a function of the kernel value K. Our goal is to ﬁnd critical initialization
hyperparameters whose associated ﬁxed-point value K⋆
00 = K⋆
00(Cb, CW ) gives rise to
χ∥(K⋆
00) = χ⊥(K⋆
00) = 1.
How do we actually ﬁnd these critical values? Conceptually, the most obvious route
– illustrated in Figure 5.1 for the tanh activation function – is the following procedure:
1. For each value of Cb and CW , with Cb ≥0 and CW ≥0, ﬁnd a ﬁxed-point value of
the kernel K⋆
00 = K⋆
00(Cb, CW ), implicitly deﬁned via K⋆
00 = Cb + CW g0(K⋆
00) with
the constraint K⋆
00 ≥0.
2. With K⋆
00(Cb, CW ), evaluate both χ∥(K⋆
00) and χ⊥(K⋆
00), scanning over values in
the (Cb, CW ) plane until the criticality conditions χ∥= 1 and χ⊥= 1 are both
met.
-1
0
1
2
3
4
5
0
1
2
3
0
1
2
3
4
5
Figure 5.1: Two algorithms to pin down a nontrivial ﬁxed point, illustrated here for
the tanh activation function. Left: the lines deﬁned by the conditions χ⋆
⊥= 1 (solid)
and χ⋆
∥= 1 (dashed) are shown in the hyperparameter plane (CW , Cb) for the tanh
activation function. The intersection of these two lines gives the critical initialization
hyperparameters (CW , Cb) = (1, 0). Right: the left-hand side of the condition (5.73) is
plotted as a function of K⋆
00. The plotted line hits unity as K⋆
00 →0.
This algorithm, however, is practically cumbersome to carry out for general activation
functions, both numerically and analytically. In order to obtain a more implementation-
friendly algorithm, let’s reshuﬄe the logic a bit. First, note that for a candidate ﬁxed-
126

point value K⋆
00, setting
CW =
h
σ′(z)σ′(z)

K⋆
00
i−1 ,
(5.71)
Cb = K⋆
00 −
⟨σ(z)σ(z)⟩K⋆
00
⟨σ′(z)σ′(z)⟩K⋆
00
,
(5.72)
satisﬁes both the ﬁxed-point equation K⋆
00 = Cb + CW g0(K⋆
00) as well as the ﬁrst crit-
icality condition χ⊥(K⋆
00) = 1. The second criticality condition χ∥(K⋆
00) = 1 then is
tantamount to χ⊥(K⋆
00) /χ∥(K⋆
00) = 1, which is simply the following ratio of expecta-
tions
"
2K2 ⟨σ′(z)σ′(z)⟩K
⟨σ(z)σ(z) (z2 −K)⟩K
# 
K=K⋆
00
= 1 ,
(5.73)
independent of the initialization hyperparameters CW and Cb. Therefore, we can use
the following simpler algorithm:
1. Scan over values of K⋆
00 ≥0 until (5.73) is satisﬁed.
2. Plug the resulting value of K⋆
00 into (5.71) and (5.72) to evaluate the critical
initialization hyperparameters (and also make sure Cb ≥0).
In Figure 5.1, the left-hand side of (5.73) is plotted as a function of K⋆
00 for the tanh ac-
tivation function, which we see hits unity at K⋆
00 = 0. Then, evaluating equations (5.71)
and (5.72) in the limit K⋆
00 →0 eﬃciently gives the critical initialization hyperparame-
ters for tanh: (CW , Cb) = (1, 0).15
In passing, we note that scale-invariant activation functions trivially satisfy the con-
dition (5.73) for any ﬁxed-point value K⋆
00, since the susceptibilities are equal to the
same kernel-independent constant, χ∥(K) = χ⊥(K) = χ. It’s easy to check that for this
universality class, the above algorithm recovers the critical initialization hyperparame-
ters (5.67) given in §5.2.
5.3.2
No Criticality: sigmoid, softplus, nonlinear monomials, etc.
For some activation functions, a nontrivial ﬁxed point for the kernel does not exist. For
example, consider the sigmoid activation function
σ(z) =
1
1 + e−z .
(5.74)
The condition (5.73) is plotted for this activation in Figure 5.2. While this condition is
satisﬁed at K⋆
00 = 0, evaluating (5.72) in this limit yields Cb = −
 σ(0)
σ′(0)
2 < 0. Since the
15Even though the ﬁxed-point value of the midpoint kernel is zero, this is a nontrivial ﬁxed point. In
particular, we will see in §5.3.3 that kernels with a nontrivial ﬁxed point at K⋆
00 = 0 form a universality
class, characterized by a benign power-law decay in ℓ. In practice, the power-law behavior means that
for any ﬁnite depth the kernel will remain ﬁnite.
127

variance of the bias cannot be negative, this is unphysical.16 Thus, the sigmoid cannot
be tuned to criticality and should not be used.17
0
1
2
3
0
1
2
0
1
2
3
0
1
2
Figure 5.2: The left-hand side of the condition (5.73) is plotted as a function of K⋆
00 for
the sigmoid activation function (left) and the softplus activation function (right). For
the sigmoid, the plotted line hits unity as K⋆
00 →0, but the associated critical initializa-
tion hyperparameters (Cb, CW ) are unphysical because Cb < 0. For the softplus, the
plotted line does not hit unity. These activation functions cannot be tuned to criticality.
Next let’s consider the softplus activation function
σ(z) = log(1 + ez) ,
(5.75)
which, as a reminder, is a smooth approximation of the ReLU.
Plotting the condi-
tion (5.73) in Figure 5.2, we see that it cannot be satisﬁed for any K⋆
00 ≥0. Thus, in
contrast to the ReLU, the softplus cannot be tuned to criticality. This supports the
lore in the community that the ReLU is superior to the softplus, despite their similarity
and the softplus’ smoothness.
As we will see in the next subsection, the real problem with these activation functions
is that they do not cross zero at z = 0. There is an easy ﬁx, namely, setting
σ(0) = 0 ,
(5.76)
by an appropriate constant shift for each activation. With such a shift the sigmoid
turns into the tanh, albeit with the preactivation and activation each scaled by a half.
Such a scaled tanh indeed admits a critical initialization, which is easy to check after
reading the discussion in the next subsection.
16The limiting value of Cb = −

σ(0)
σ′(0)
2
hints that the conditions σ(0) = 0 and σ′(0) ̸= 0 may be
necessary constraints for an activation function to have a nontrivial ﬁxed point.
17Similarly, as a non-smooth limit of a logistic function, the perceptron activation function is even
worse and doesn’t merit discussion.
128

With that in mind, let’s see what happens for activation functions that cross zero
nonlinearly. For simplicity, take any nonlinear monomial activation function
σ(z) = zp ,
p = 2, 3, 4, . . . .
(5.77)
In this case, direct Gaussian integration translates the condition (5.73) into the con-
straint
p
2p −1 = 1 ,
(5.78)
which cannot be satisﬁed for nonlinear monomials, since p ̸= 1. Thus, such nonlinear
monomials also shouldn’t be used in deep networks. More importantly, in addition to
σ(0) = 0, criticality seems to require the condition
σ′(0) ̸= 0 ,
(5.79)
which we will investigate more generally in the next subsection.
The impossibility of criticality for all of the activation functions discussed in this
subsection means that their use should be discouraged. While the problem is somewhat
mitigated for shallow networks – since there are fewer layers for the exponential behavior
to damage the signals – as networks become deeper and deeper, criticality becomes more
and more essential.
5.3.3
K⋆= 0 Universality Class: tanh, sin, etc.
In §5.3.1, we learned through a numerical investigation that tanh has a nontrivial ﬁxed
point at K⋆
00 = 0. In addition, in the last subsection §5.3.2, our analysis suggested that
the conditions σ(0) = 0 and σ′(0) ̸= 0 are important for any smooth activation to have
a nontrivial ﬁxed point.
In this subsection, we will connect these two observations.
In particular, in the
vicinity of K⋆
00 = 0, we can analytically analyze the kernel recursions (5.46)–(5.48) by
Taylor expanding around K⋆
00 = 0 and directly integrating the Gaussian expectations.
This analysis will show that the conditions σ(0) = 0 and σ′(0) ̸= 0 are both necessary
and suﬃcient for a smooth activation function to have a nontrivial ﬁxed point at K⋆
00 = 0,
leading to the deﬁnition of our second universality class.
Let’s use the following notation for the Taylor coeﬃcients of any analytic activation
function:
σ(z) =
∞
X
p=0
σp
p! zp .
(5.80)
Plugging this expansion into the deﬁnition of the helper function (5.70) and performing
the Gaussian integral, we ﬁnd
g(K) = ⟨σ(z)σ(z)⟩K = σ2
0 +

σ2
1 + 2σ0σ2

K + O

K2
.
(5.81)
From this we see that the ﬁxed point of the recursion for the midpoint kernel
K⋆
00 = Cb + CW g(K⋆
00) ,
(5.82)
129

has a solution at K⋆
00 = 0 if and only if Cb = CW σ2
0 = 0. Recalling that CW = 0 violates
the criticality conditions, we must pick σ0 = 0. Henceforth we will assume that this
choice has been made.
Continuing on with σ0 = 0 and Cb = 0 in mind, inserting the expansion (5.80) into
our expressions for the susceptibilities, (5.50) and (5.51), and performing the Gaussian
integrals we ﬁnd
CW g(K) =

CW σ2
1
 h
K + a1K2 + a2K3 + O

K4i
,
(5.83)
χ∥(K) =

CW σ2
1
 h
1 + 2a1K + 3a2K2 + O

K3i
,
(5.84)
χ⊥(K) =

CW σ2
1
 h
1 + b1K + O

K2i
,
(5.85)
where here we have also expanded g(K) to higher order in the kernel, and the coeﬃcients
a1, a2, and b1 are given by the following combinations of Taylor coeﬃcients of the
activation function
a1 ≡
σ3
σ1

+ 3
4
σ2
σ1
2
,
(5.86)
a2 ≡1
4
σ5
σ1

+ 5
8
σ4
σ1
 σ2
σ1

+ 5
12
σ3
σ1
2
,
(5.87)
b1 ≡
σ3
σ1

+
σ2
σ1
2
.
(5.88)
It’s easy to check that, e.g., for tanh these coeﬃcients take the following values a1 = −2,
a2 = 17/3, b1 = −2. Now, examining expansions (5.84) and (5.85), we see that to satisfy
the criticality conditions χ∥(K⋆
00 = 0) = 1 and χ⊥(K⋆
00 = 0) = 1 we must set CW = 1/σ2
1.
To ensure a ﬁnite variance, we also see that the activation function must have σ1 ̸= 0.
Thus, for any smooth activation function to have a nontrivial ﬁxed point at K⋆
00 = 0,
it is necessary and suﬃcient that σ(z) satisfy
σ0 = 0 ,
σ1 ̸= 0 .
(5.89)
For such an activation, the critical initialization hyperparameters are then given by
(Cb, CW )critical =

0, 1
σ2
1

.
(5.90)
Just to emphasize this a bit, any activation with these conditions (5.89) initialized with
(5.90) will have a nontrivial ﬁxed point at K⋆
00 = 0. The set of activation functions that
vanish at the origin with a nonzero ﬁrst derivative make up the K⋆= 0 universality
class. The canonical class member is the tanh activation function, though there are
obviously a very large number of members in this class, e.g. the sin activation function
is a member too.
Having determined the critical initialization hyperparameters, let’s now try to un-
derstand the behavior of the kernel for the K⋆= 0 universality class. We will see that
when tuned to criticality the activations satisfying (5.89) all behave similarly under RG
ﬂow, with the large-depth behavior of the kernel depending only on the ﬁrst few Taylor
coeﬃcients of σ(z).
130

Deep asymptotic analysis for the midpoint kernel
Recalling the expansion K(ℓ)
00 = K⋆
00 + ∆K(ℓ)
00 around the ﬁxed point and considering the
expansion (5.83) for g(K), the midpoint kernel recursion at K⋆
00 = 0 criticality becomes
∆K(ℓ+1)
00
= ∆K(ℓ)
00 + a1

∆K(ℓ)
00
2 + a2

∆K(ℓ)
00
3 + O

∆K(ℓ)
00
4
.
(5.91)
Since the whole point of criticality is to alleviate exponential behavior, we expect a
gentler decay back to the K(ℓ)
00 = 0 ﬁxed point. With that in mind, let’s plug a power-
law ansatz ∆K(ℓ)
00 ∼

1
ℓ
p0 into (5.91). Noting that

1
ℓ+1
p0 =
1
ℓp0
h
1 −p0
ℓ+ O

1
ℓ2
i
and
matching the leading terms on both sides, we get a solution
∆K(ℓ)
00 =

1
(−a1)
 1
ℓ+ . . . .
(5.92)
Thus, the behavior at criticality is a mild power law decay, with a critical exponent
p0 = 1. Such an exponent is said to be universal for the K⋆= 0 universality class, since
it is completely independent of the details of the particular activation function.
Importantly, for this asymptotic solution to be consistent, we must have (−a1) > 0
to ensure the positivity of the kernel. If instead we had (−a1) < 0, then the asymptotic
solution (5.92) would be negative, making it invalid. In this case the ﬁxed point would
be unstable, exponentially repelling the kernel away from K⋆
00 = 0.18 We will see in
the next subsection that SWISH and GELU activation functions exhibit such an instability
near K⋆
00 = 0.
Moreover, in the last subsection we suggested that an activation function that doesn’t
satisfy σ(0) = 0 could be potentially salvaged with a constant shift.
In particular,
perhaps the softplus could be saved by subtracting a constant log(2) so that σ(0) = 0?
However, in this case we’d have (−a1) < 0, and the kernel will get repelled from the only
candidate nontrivial ﬁxed point at K⋆
00 = 0. And since χ∥(K) > 1 away from K = 0,
the midpoint kernel will diverge exponentially. Thus, despite this attempt, we see that
the softplus cannot be saved.
Returning to our solution (5.92), we can actually do quite a bit better than “. . .”
for the subleading asymptotic analysis. As a ﬁrst guess to improve our ansatz, let’s
include a subleading 1/ℓ2 term in ∆K(ℓ)
00 . However, if we try to match terms on both
sides of (5.91), we’d ﬁnd that there’s no way of canceling the 1/ℓ3 terms. What we can
do instead is to also add log(ℓ)/ℓ2 with an independent coeﬃcient to our ansatz. This
generates an additional 1/ℓ3 term, allowing for a consistent solution. Generally for any
of the observables O(ℓ) that we will consider, the correct scaling ansatz for the large-ℓ
18Generically, (−a1) < 0 implies that χ∥> 1 away from K⋆
00 = 0, which repels the midpoint kernel
ﬁrst with a power law and then exponentially. However, the semi-criticality that we discussed in §5.2 for
scale-invariant activations was exceptional. For this universality class, a1 = 0 and hence growth towards
the ﬁxed point at inﬁnity is governed by a power law.
131

asymptotic expansion is of the form
O(ℓ) =
1
ℓ
pO "
c0,0 + c1,1
log ℓ
ℓ

+ c1,0
1
ℓ

+ c2,2
 
log2 ℓ
ℓ2
!
+ . . .
#
=
1
ℓ
pO


∞
X
s=0
s
X
q=0
cs,q
logq ℓ
ℓs

,
(5.93)
where the critical exponent pO is expected to be universal for a given class, while the
constants cs,q will depend on the details of a particular activation function. Carrying
this process forward for O(ℓ) = ∆K(ℓ)
00 , we can systematically determine the subleading
behavior of the kernel perturbation as
∆K(ℓ)
00 =

1
(−a1)
 1
ℓ+
"
−(a2 −a2
1)
a3
1
# log

ℓ
ℓ0

ℓ2
(5.94)
+
"
−
 a2 −a2
1
2
a5
1
# h
log

ℓ
ℓ0
i2
ℓ3
+
" a2 −a2
1
2
a5
1
# log

ℓ
ℓ0

ℓ3
+ O
 1
ℓ3

,
and with enough eﬀort this asymptotic expansion can be reﬁned to arbitrary degree by
including the higher-order corrections according to the scaling ansatz (5.93) described
above.
Here, the constant ℓ0 is undetermined by this large-ℓasymptotic analysis and non-
trivially depends on the input norm through
K(1)
00 = 1
σ2
1
1
n0
n0
X
i=1
x2
i;0 ,
(5.95)
which sets the initial condition (5.2) for the kernel recursion (5.1) when the rescaled
weight variance is set to criticality, CW = 1/σ2
1. To get a sense of what this means, let’s
assume that χ∥(K) is monotonically decreasing for K ≥0 with χ∥(0) = 1 – as is true for
tanh – and consider what happens when an input xi;0 has a very large magnitude. Such a
large-norm input will lead to a large value for the ﬁrst-layer midpoint kernel, K(1)
00 ≫1.
In the range 0 < k♯< K(ℓ)
00 , for some constant k♯, the kernel K(ℓ)
00 will decay quicker
than χ∥(k♯)ℓ, with χ∥(k♯) < 1, until it enters the power-law regime near K⋆
00 = 0. The
undetermined constant ℓ0 is a remnant of this complicated crossover behavior, capturing
the leading data dependence of the midpoint kernel.
Additionally, the asymptotic expansion for the midpoint kernel (5.94) has a nice
interpretation under RG ﬂow. While the critical exponent of the falloﬀp0 = 1 is generic
for the universality class, we see that the coeﬃcients of the terms do depend on the
details of the activation function, albeit only the ﬁrst few Taylor coeﬃcients. In fact,
for larger and larger ℓ, the dependence is on fewer and fewer of the coeﬃcients, with
the leading term only depending on a1, (5.86). In this asymptotic limit, any activation
function in the K⋆= 0 universality class with the same ﬁrst three Taylor coeﬃcients
132

around zero will be completely indistinguishable. Thus, from the representation group
ﬂow perspective, one of the results of having a deeper network is to make the particular
details of the activation function more and more irrelevant.
Lastly, let us note for all aspiring “activation designers” out there that we can en-
gineer critical exponents other than p0 = 1 by ﬁne-tuning the Taylor coeﬃcients of the
activation function. For example, if we set a1 = 0 by balancing σ3 and σ2, then the
kernel approaches a K⋆
00 = 0 nontrivial ﬁxed point with a 1/
√
ℓpower law decay so long
as (−a2) > 0. The need for such tuning indicates that the ∼1/ℓbehavior is generic for
activation functions in the K⋆= 0 universality class.19
Deep asymptotic analysis for parallel perturbations
Next, let’s solve the δK(ℓ)
[1] recursion for parallel perturbations. Plugging the expansion
(5.84) for χ∥(K) into the recursion (5.47), we get an algebraic equation
δK(ℓ+1)
[1]
=

1 + 2a1∆K(ℓ)
00 + 3a2

∆K(ℓ)
00
2 + O

∆K(ℓ)
00
3
δK(ℓ)
[1] .
(5.96)
Then, plugging in the large-ℓsolution for ∆K(ℓ)
00 (5.94) and a large-ℓasymptotic expan-
sion for δK(ℓ)
[1] based on our scaling ansatz (5.93), we can solve the resulting equation by
matching the terms on both sides:
δK(ℓ)
[1] = δ∥
ℓ2

1 + 2a1
 a2 −a2
1

a3
1
log

ℓ
ℓ0

ℓ
+ O
1
ℓ

.
(5.97)
Inspecting our solution, we identify our second critical exponent for the K⋆= 0
universality class: p∥= 2 corresponding to the 1/ℓ2 falloﬀof δK(ℓ)
[1] .
The particular
value of this exponent is to be expected. As noted before, the parallel perturbation
is just a diﬀerence of single-input kernels for two inputs with diﬀering norms, K(ℓ)
[1] =

K(ℓ)
++ −K(ℓ)
−−

/2. The leading 1/ℓ2 scaling occurs because the diagonal components
K(ℓ)
++ and K(ℓ)
−−are governed by the same asymptotic behavior up to order log(ℓ)/ℓ2,
including the same coeﬃcients. Thus, the leading diﬀerence appears at order 1/ℓ2, due
to diﬀerent input-dependent constants ℓ+ and ℓ−in expansions analogous to (5.94) for
K(ℓ)
++ and K(ℓ)
−−, with the undetermined constant δ∥∝log(ℓ+/ℓ−).
In this way, this
constant explicitly carries the data dependence of the parallel perturbation.
Deep asymptotic analysis for perpendicular perturbations
Finally, let’s conclude our analysis by solving the δδK(ℓ)
[2] recursion for perpendicular
perturbations. Let’s begin by plugging the expansion (5.85) for χ⊥(K) into the recursion
19More precisely, we should have deﬁned the K⋆= 0 universality class with the requirement a1 ̸= 0.
This in turn would lead us to deﬁne a whole family of universality classes labeled by the degree of ﬁne
tuning of the a1, a2, etc., or equivalently labeled by the value of the critical exponent p0.
133

(5.48). Since we want to focus on perpendicular perturbations with Pn0
i=1 xi;0 δxi = 0, we
will also turn oﬀparallel perturbations by setting δK(ℓ)
[1] = 0. Putting this all together
gives an algebraic equation
δδK(ℓ+1)
[2]
=

1 + b1∆K(ℓ)
00 + O

∆K(ℓ)
00
2
δδK(ℓ)
[2] .
(5.98)
Plugging in the large-ℓasymptotic solution for ∆K(ℓ)
00 and solving the resulting equation
with another large-ℓasymptotic expansion for δδK(ℓ)
[2] based on our scaling ansatz (5.93),
we get
δδK(ℓ)
[2] = δ2
ℓ
b1
a1

1 + b1
 a2 −a2
1

a3
1
log

ℓ
ℓ0

ℓ
+ O
1
ℓ

,
(5.99)
where δ2 is another unﬁxed constant undetermined by the large-ℓsolution, in this case
related nontrivially to the magnitude of the diﬀerence of the inputs: Pn0
i=1 (xi;+ −xi;−)2.
Here we see that the presumptive critical exponent, p⊥≡b1/a1, depends mildly on the
details of the activation function.
However, note that something nice happens for odd activation functions such as tanh
and sin. In this case, we see from (5.86) and (5.88) that a1 = b1, giving us a bona ﬁde
critical exponent, p⊥= 1, when restricting the universality class to odd activations.
This means that perpendicular perturbations decay with the same power in ℓas the
midpoint kernel decays to the ﬁxed point, ∼1/ℓ. Thus, at criticality the ratio K(ℓ)
[2] /K(ℓ)
[0]
is ﬁxed at the leading order, preserving the angles between nearby perpendicular inputs.
Importantly, this ensures that the relationship between input points is conserved under
the RG ﬂow, even if the signals propagate through a very deep network.
Furthermore, the milder falloﬀof the perpendicular perturbations suggests that they
are in some sense more important than the parallel ones. This is because with enough
depth the K(ℓ)
[1] component will become subleading to the K(ℓ)
[0] and the K(ℓ)
[2] components,
due to the 1/ℓ2 scaling of the former compared to the 1/ℓscaling of the latter two.
For this reason, we are going to ignore these parallel perturbations of the kernel going
forward.
5.3.4
Half-Stable Universality Classes: SWISH, etc. and GELU, etc.
In this ﬁnal subsection, we consider two other semi-popular activation functions in order
to explore nontrivial ﬁxed points away from zero, K⋆
00 ̸= 0.
• The SWISH activation function is deﬁned as
σ(z) =
z
1 + e−z .
(5.100)
Similar to the intuition for the softplus, the SWISH is intended as a smooth
version of the ReLU.
Following our general algorithm in §5.3.1 for ﬁnding the
critical initialization hyperparameters, we actually ﬁnd two nontrivial ﬁxed points
134

0
5
10
15
20
0
1
0
5
10
15
20
0
1
Figure 5.3: The left-hand side of the condition (5.73) is plotted as a function of K⋆
00 for
the SWISH activation function (left) and the GELU activation function (right). For both
activation functions, the plotted line hits unity (black dots) at K⋆
00 = 0 as well as at a
nonzero half-stable nontrivial ﬁxed point K⋆
00 ̸= 0.
for the kernel, see Figure 5.3. In particular, the condition (5.73) is met at K⋆
00 = 0
with (Cb, CW ) = (0, 4) and at K⋆
00 ≈14.32017362 with
(Cb, CW ) ≈(0.55514317, 1.98800468) .
(5.101)
For the K⋆
00 = 0 nontrivial ﬁxed point, one can check that (−a1) < 0, and hence
it’s unstable. For the K⋆
00 ≈14.3 nontrivial ﬁxed point, we expand the midpoint
kernel recursion as K(ℓ)
00 = K⋆
00 + ∆K(ℓ)
00 , yielding
∆K(ℓ+1)
00
= ∆K(ℓ)
00 + ˜a1

∆K(ℓ)
00
2 + O

∆K(ℓ)
00
3
,
(5.102)
with (−˜a1) ≈−2.84979219 · 10−6.
Here, the large-ℓasymptotic analysis around the ﬁnite ﬁxed point is identical to
the case of K⋆
00 = 0, resulting in
∆K(ℓ)
00 ∼

1
(−˜a1)
 1
ℓ.
(5.103)
However, the interpretation is slightly diﬀerent, because the ﬁxed-point value
K⋆
00 ≈14.3 is non-vanishing. In particular, this implies that when K(ℓ)
00 < K⋆
00
the kernel is attracted to the ﬁxed point, while when K(ℓ)
00 > K⋆
00 the kernel is
repelled.20 Hence, this ﬁxed point is half-stable, and so the activation function
20With the half-critical initialization hyperparameters for the SWISH (5.101), there is a trivial ﬁxed
point at K⋆
00 ≈14.5 that exponentially attracts the midpoint kernel when K(ℓ)
00 > 14.3.
135

is perhaps half-useful. In practice, however, |˜a1| is small enough that the SWISH
behaves in an almost scale-invariant manner around K(ℓ)
00 ∼K⋆
00 ≈14.3.
• The GELU activation is deﬁned as
σ(z) = z
2

1 + erf
 z
√
2

,
(5.104)
and as a reminder is another smoothed ReLU. Following our recipe for criticality,
the condition (5.73) is again met twice, at K⋆
00 = 0 with (Cb, CW ) = (0, 4) and at
K⋆
00 = 3+
√
17
2
with
(Cb, CW ) ≈(0.17292239, 1.98305826) ,
(5.105)
see Figure 5.3. Similar to the SWISH, the ﬁxed point at K⋆
00 = 0 is unstable with
(−a1) = −6/π < 0, and the ﬁxed point at K⋆
00 = 3+
√
17
2
is half-stable, in this
case with (−˜a1) ≈(1.43626419) · 10−4. Note that the sign of ˜a1 here diﬀers from
the sign for the SWISH. Thus, this time, when K(ℓ)
00 > K⋆
00 the midpoint kernel is
attracted to the ﬁxed point, while when K(ℓ)
00 < K⋆
00 it is repelled.21 Note that
the absolute value |˜a1| is bigger for the GELU than for the SWISH, meaning that it
behaves less scale-invariantly and looks less like the ReLU.
Unlike the shifted softplus which admits only an unstable nontrivial ﬁxed point at
K⋆
00 = 0, here the non-monotonicity of the GELU and SWISH activation functions gave
rise to half-stable nontrivial ﬁxed points at K⋆
00 ̸= 0. They are both representatives
of half-stable universality classes. For both of these ReLU-like activations functions, the
critical initialization hyperparameters for the K⋆
00 ̸= 0 half-stable nontrivial ﬁxed points
are very similar to the critical ReLU initialization (Cb, CW ) = (0, 2); the activations
in each of these classes really are just small perturbations of the ReLU. At the same
time, the fact that there’s a ﬁxed point at a particular kernel value K⋆
00 ̸= 0 indicates
– however weakly – the introduction of a particular scale. This is one way to see that
these universality classes break scale invariance.
In summary, despite being ReLU-like and also smooth, both of the SWISH and GELU
are inferior to the ReLU itself. If you want to use a smooth activation function, use tanh.
5.4
Fluctuations
Now that we fully understand how to tune inﬁnite-width networks to criticality, let’s
back oﬀthis large-n limit to analyze the behavior of realistic networks. Speciﬁcally, we’re
going to extend the ﬁnite-width analysis that we performed for deep linear networks
in §3.3 to MLPs with nonlinear activation functions. Before diving in, let’s review the
motivation for carrying out such an analysis.
21With the half-critical initialization hyperparameters for the GELU (5.105), there is a trivial ﬁxed point
at K⋆
00 ≈3.2 that exponentially attracts the midpoint kernel when K(ℓ)
00 < 3+
√
17
2
≈3.6.
136

First, note that practitioners only use a single network rather than an ensemble
of networks.22 As we have discussed, sometimes a single instantiation will generically
deviate from the mean. Therefore, in order to understand what typically happens in
a single instantiation for an observable of interest, we have to compute not only the
mean but also instantiation-to-instantiation ﬂuctuations around the mean. As we ex-
plained in §3.3, such ﬂuctuations are generically ﬁnite-width eﬀects, controlled by the
1/n-suppressed four-point vertex V (ℓ)
(α1α2)(α3α4). If ﬂuctuations are large, then a single in-
stantiation can behave poorly, despite being sampled from an initialization distribution
tune to criticality.
Second, we saw in §4.3 that the inﬁnite-width ℓ-th-layer preactivation distribution
factorizes as
p

z(ℓ)
1 , . . . , z(ℓ)
nℓ
D

= p

z(ℓ)
1
D

· · · p

z(ℓ)
nℓ
D

+ O
 1
nℓ

,
(5.106)
where the distributions p

z(ℓ)
i
D

on each neuron are given by statistically independent
Gaussian distributions. (To emphasize the neural dependence here, we have included
neural indices while suppressing sample indices.) Recalling our discussion of interac-
tions and statistical independence in §1.3, this means that intralayer correlations among
neurons are entirely ﬁnite-width phenomenon. Later, we will show how this lack of inter-
actions connects to the fact that the representations of an inﬁnite-width network cannot
evolve during gradient-based learning. Thus, understanding these ﬁnite-width eﬀects is
a prerequisite to understanding how practical networks actually learn from input data.23
Third, ﬁnite-width corrections can modify the mean value of observables. As we saw
in §4.5, at ﬁnite width all observables in principle receive an inﬁnite series of subleading
corrections. For instance, a possible ﬁnite-width NLO correction to the metric, G{1}(ℓ)
α1α2 ,
can shift the inﬁnite-width metric, G{0}(ℓ)
α1α2
≡K(ℓ)
α1α2, a.k.a. the kernel. Such a ﬁnite-
width correction could potentially ruin criticality, since our derivation of the critical
initialization hyperparameters depended explicitly on the inﬁnite-width ﬁxed-point value
of the kernel.24
There will be two main takeaways from this section.
• First, we will ﬁnd that the leading ﬁnite-width ﬂuctuations scale with the depth-
to-width ratio of the network, L/n.
We saw the importance of this emergent
scale for the linear activation function in §3.3; here, we see that it persists very
generally for nonlinear activation functions. In the language of §4.6, this means
that ﬁnite-width corrections are relevant under representation group ﬂow and that
deeper networks deviate more and more from the simple inﬁnite-width limit. This
22Actually in some cases practitioners can use ensembles of networks, though the computational cost
of such models grows in proportion to the number of networks in the ensemble.
23We’ll go into more detail about the role that these correlations play in the inductive bias of MLPs
in §6 and then connect these interactions to representation learning in §11.
24In §5.4.1 we will show that the NLO metric G{1}(ℓ)
α1α2 = 0 vanishes for the scale-invariant universality
class, which is why we didn’t discuss this type of correction for deep linear networks in §3.
137

emphasizes the importance of including such corrections when analyzing such net-
works and – taking into account the fact that overly deep networks suﬀer from
overwhelming ﬂuctuations – suggests that our perturbative eﬀective theory works
best in the regime where practical networks also work best.
• Second, the NLO metric G{1}(ℓ)
α1α2 is subdominant to the kernel K(ℓ)
α1α2 as long as an
appropriate O(1/n) correction is made to CW . This means that the NLO metric
vanishes in the interpolating limit – n, L →∞, with L/n ﬁxed – and thus can
safely be neglected for most wide networks of reasonable depths.
A single input, reloaded
In order to illustrate the important qualitative eﬀects of ﬁnite width, we will again
specialize to just a single input. The reason for this choice can be best understood by
progressing through another twofold list:
(i) Once the two initialization hyperparameters, Cb and CW , are tuned to criticality at
leading order by the one- and two-input analysis of the kernel, the only additional
tuning comes from the single-input analysis of the NLO metric G{1}(ℓ)
α1α2 . Therefore,
the multi-input solutions for the vertex and NLO metric do not add anything to
the criticality analysis.
(ii) The most interesting part of the two-input vertex is a component that gives vari-
ance of the input-output Jacobian of the network. (As we described in footnote 9,
the mean value of this Jacobian is captured by the K(ℓ)
[2] component of the kernel.)
However, the would-be analysis of this input-output variance will be subsumed by
our analysis of the variance of the neural tangent kernel in §8, which more directly
gives the variance of gradients relevant for training.
In the rest of this section we’ll omit the α = 0 sample indices, since such notation
is unnecessarily cumbersome when considering only a single input. We’ll also simplify
things further by picking all the hidden-layer widths to be equal
n1 = n2 = · · · = nL−1 ≡n .
(5.107)
In addition to being a sensible choice, this means notationally that we don’t have to
carry around factors of nℓ/nℓ−1 everywhere. With these decisions in mind, the relevant
recursions from §4 become
K(ℓ+1) = Cb + CW g

K(ℓ)
,
(5.108)
V (ℓ+1) = χ2
∥

K(ℓ)
V (ℓ) + C2
W
D
σ4(z)
E
K(ℓ) −
D
σ2(z)
E2
K(ℓ)

,
(5.109)
G{1}(ℓ+1) = χ∥

K(ℓ)
G{1}(ℓ) + 1
8 j

K(ℓ)
V (ℓ)
 K(ℓ)2 ,
(5.110)
138

where the helper function g(K) and the parallel susceptibility χ∥(K) were deﬁned in (5.5)
and (5.50), and we have deﬁned another helper function
j(K) ≡CW
*
σ(z) σ(z)


 
z2
K
!2
−6
 
z2
K
!
+ 3


+
K
.
(5.111)
These three recursions can be solved for each universality class by mirroring our boot-
strap analysis of K(ℓ)
00 , δK(ℓ)
[1] , δδK(ℓ)
[2] in §5.2 and §5.3.
5.4.1
Fluctuations for the Scale-Invariant Universality Class
Recall from §5.2 that the scale-invariant universality class contains any activation func-
tion of the form
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 ,
(5.112)
with the ReLU (a+ = 1, a−= 0) as the exemplar member to keep in mind. Also recall
that for this class we evaluated the helper function as g(K) = A2K and the parallel
susceptibility as χ∥= A2CW ≡χ, with the activation-dependent constant given by
A2 ≡(a2
+ + a2
−)/2.
The other terms in the new recursions (5.109) and (5.110) can
similarly be evaluated by computing Gaussian integrals on the half-line, yielding
C2
W
D
σ4(z)
E
K −
D
σ2(z)
E2
K

= C2
W

3A4 −A2
2

K2 ,
j(K) = 0 ,
(5.113)
with a new activation-dependent constant
A4 ≡a4
+ + a4
−
2
,
(5.114)
to pair with our other constant, A2. With these expressions, the three recursions can be
simpliﬁed as
K(ℓ+1) = Cb + χK(ℓ) ,
(5.115)
V (ℓ+1) = χ2
3A4
A2
2
−1
 
K(ℓ)2 + χ2 V (ℓ) ,
(5.116)
G{1}(ℓ+1) = χ G{1}(ℓ) .
(5.117)
As a reminder, we already solved the kernel recursion in §5.2.
Things are now quite simple.
• First, remember from §4.1 that the ﬁrst layer preactivation distribution is always
exactly Gaussian, implying that the ﬁrst-layer two-point correlator is simply given
in terms of the ﬁrst-layer kernel K(1) to all orders in n
E
h
z(1)
i
z(1)
j
i
= δijK(1) .
(5.118)
139

This means that the ﬁrst-layer NLO metric must vanish G{1}(1) = 0, and recursion
(5.117) then tell us that the NLO metric will vanish in any subsequent layer.
Thus, for activations in the scale-invariant universality class, we learn that the
single-input metric does not get corrected at O(1/n).
• Second, let’s focus on criticality by setting Cb = 0 and CW = 1/A2. As discussed
in §5.2, this setting of hyperparameters ﬁxes the kernel to be an input-dependent
layer-independent constant
K(ℓ) = K⋆≡1
A2
 
1
n0
n0
X
i=1
x2
i
!
.
(5.119)
In particular, this means that the critical exponent for the single-input kernel is
given by p0 = 0. Setting χ = 1 and substituting this expression into (5.116), we
ﬁnd a linearly growing solution for the four-point vertex
V (ℓ) = (ℓ−1)
3A4
A2
2
−1

(K⋆)2 .
(5.120)
By inspection, we identify another critical exponent for the scale-invariant univer-
sality class: assuming V (ℓ) ∼(1/ℓ)pV , then pV = −1. This exponent encodes the
linear growth of the vertex under RG ﬂow. Of particular note, the coeﬃcient in
front of (5.120) evaluates to

3A4
A2
2 −1

= 2 for linear activations in contrast to
=5 for ReLU activations. Apparently the ﬂuctuations in ReLU networks are signif-
icantly stronger than in deep linear networks. More generally, we conclude that
the strength of such ﬂuctuations is not universal.
• Third, let’s revisit semi-criticality by setting CW = 1/A2, but setting the bias
variance to an arbitrary positive constant, Cb > 0. As we saw in §5.2, in this case
the kernel grows linearly towards a nontrivial ﬁxed point at inﬁnity, K(ℓ) ∼ℓ, i.e.,
p0 = −1. Plugging such a solution into the vertex recursion (5.116), we see that the
four-point vertex grows cubicly V (ℓ) ∼ℓ3, i.e., pV = −3. However, the appropriate
dimensionless quantity – normalizing the vertex by the square of the kernel – still
grows linearly in ℓ, i.e., pV −2p0 = −1.25
Thus, even for semi-criticality the
universal ℓ/n-scaling of the ﬁnite-width corrections is preserved.
5.4.2
Fluctuations for the K⋆= 0 Universality Class
Let’s now consider the K⋆= 0 universality class. As a reminder, this class contains
all smooth activation functions that satisfy σ(0) = 0 and σ′(0) ̸= 0, with tanh as the
exemplar member to keep in mind. In §5.3.3, we determined that activations in this
class have a nontrivial ﬁxed point at K⋆= 0 and found that the associated critical
25To elaborate a bit more, ﬁrst please reread footnote 15 in §1.3 on dimensional analysis. Now, if we
give the preactivations a dimension [z] = ζ, then we have for the kernel [K] = ζ2, while for the four-point
vertex [V ] = ζ4. Thus, the ratio V/K2 is dimensionless.
140

initialization hyperparameters are given by Cb = 0 and CW = 1/σ2
1. For the rest of this
subsection we will focus on such networks at criticality.
Mirroring our approach in §5.3.3 to solve the kernel recursions, we can evaluate
the Gaussian expectations in the vertex recursion (5.109) and the NLO-metric recur-
sion (5.110) by Taylor expanding the activation around z = 0 and explicitly computing
the Gaussian integrals. Keeping in mind the criticality condition CW = 1/σ2
1, this gives
the following expressions
χ∥(K) = 1 + 2a1K + 3a2K2 + O

K3
,
(5.121)
C2
W
D
σ4(z)
E
K −
D
σ2(z)
E2
K

= 2K2 + (−52a1 + 60b1) K3 + O

K4
,
(5.122)
j(K)
8K2 = a1 + 3a2K + O

K2
.
(5.123)
Here, the expression for χ∥(K) is simply reprinted from §5.3.3. Similarly, to limit the
amount of time you have to ﬂip back and forth, let us also reprint the large-ℓasymptotic
expansion of the kernel perturbation originally given by (5.94):
∆K(ℓ) =

1
(−a1)
 1
ℓ+
"
−(a2 −a2
1)
a3
1
# log

ℓ
ℓ0

ℓ2
(5.124)
+
"
−
 a2 −a2
1
2
a5
1
# h
log

ℓ
ℓ0
i2
ℓ3
+
" a2 −a2
1
2
a5
1
# log

ℓ
ℓ0

ℓ3
+ O
 1
ℓ3

.
Four-Point Vertex
Now, let’s ﬁnd a solution for the four-point vertex. Substituting in (5.121) and (5.122)
into the single-input vertex recursion (5.109) gives an algebraic equation
V (ℓ+1) = V (ℓ)

1 + 4a1∆K(ℓ) +

6a2 + 4a2
1
 
∆K(ℓ)2 + . . .

(5.125)
+ 2

∆K(ℓ)2 + (−52a1 + 60b1)

∆K(ℓ)3 + . . . .
Using our scaling ansatz (5.93) for the large-ℓasymptotic expansion
V (ℓ) =
1
ℓ
pV 
# + #′ log ℓ
ℓ
+ #′′
ℓ+ . . .

,
(5.126)
and (5.124) for ∆K(ℓ) and then matching terms, we ﬁnd
V (ℓ) =
 2
3a2
1
 1
ℓ+
"
2(a2 −a2
1)
3a4
1
# log

ℓ
ℓ0

ℓ2
(5.127)
+
5a2 + a1(82a1 −90b1)
3a4
1
 1
ℓ2 + O
 
log2(ℓ)
ℓ3
!
,
141

where the constant scale ℓ0 is same as the one in the ∆K(ℓ) expansion just above, again
carrying the data dependence of the solution. We can also read oﬀthe critical exponent
controlling the asymptotic falloﬀof the vertex for the K⋆= 0 universality class: pV = 1.
Note that the value of the exponent pV = 1 and the behavior of the four-point vertex
V (ℓ) ∼1/ℓhere is diﬀerent from the value of the exponent pV = −1 and the associated
behavior V (ℓ) ∼ℓthat we found for the scale-invariant universality class. Also note
that we saw this diﬀerence in the behavior of the kernel, p0 = 1 vs. p0 = 0, for the
K⋆= 0 and scale-invariant classes, respectively. However, when instead considering the
dimensionless quantity
V (ℓ)
n
 K(ℓ)2 ∼1
n
1
ℓ
pV −2p0
+ . . . ,
(5.128)
we see that its scaling is consistent across both classes of activations:
pV −2p0 = −1 .
(5.129)
Thus, this scaling law holds across diﬀerent universality classes. As the normalized
quantity (5.128) controls leading ﬁnite-width corrections to observables – this was dis-
cussed in detail in §3.3 – such a law means that these corrections are always relevant
under representation group ﬂow.
Concretely, the normalized vertex function is given by
V (ℓ)
n
 K(ℓ)2 =
3A4
A2
2
−1
 ℓ
n + O
 1
n

,
(5.130)
for the scale-invariant universality class and
V (ℓ)
n
 K(ℓ)2 =
2
3
 ℓ
n + O
log (ℓ)
n

,
(5.131)
for the K⋆= 0 universality class. Of practical relevance, this means that ReLU networks
and tanh networks of the same depth and width will have a mostly similar sensitivity
to such corrections. However, the O(1) coeﬃcient of this quantity does depend on the
particular activation function: =5 for ReLU and =2/3 for tanh. In Appendix A, we’ll
analyze this a bit more using tools from information theory and see how it can lead to a
preferred aspect ratio, L/n, that is diﬀerent for speciﬁc choices of activation functions.
NLO metric, bare
Next, let’s solve the NLO-metric recursion (5.110). Substituting in (5.121) for χ∥(K)
and (5.123) for j(K), we get
G{1}(ℓ+1) = G{1}(ℓ) h
1 + 2a1∆K(ℓ) + . . .
i
+ V (ℓ) h
a1 + 3a2∆K(ℓ) + . . .
i
.
(5.132)
142

As should now be familiar, let’s assume a large-ℓscaling ansatz
G{1}(ℓ) = #
1
ℓ
p1
+ . . . ,
(5.133)
with p1 as the associated critical exponent. Bootstrapping (5.132) by substituting in our
previous solutions – (5.124) for ∆K(ℓ) and (5.127) for V (ℓ) – we then insert our ansatz
for G{1}(ℓ) (5.133) and match terms to ﬁnd
G{1}(ℓ) = −

1
3(−a1)

+ O
log (ℓ)
ℓ

.
(5.134)
This solution required us to set p1 = 0 and gave a constant-in-ℓleading contribution.
Combining this with the kernel, we see that the ﬁnite-width-corrected two-point corre-
lator
E
h
z(ℓ)
i z(ℓ)
j
i
= δij

K(ℓ) + 1
nG{1}(ℓ) + O

1/n2
,
(5.135)
is given by
K(ℓ) + 1
nG{1}(ℓ) =

1
(−a1)
 1
ℓ−1
3n

+ . . . .
(5.136)
This result is to be contrasted with the scale-invariant universality class, where the NLO
metric vanished identically.
For the NLO metric, the appropriate dimensionless quantity to consider is the ratio
between the correction term and the inﬁnite-width term in the two-point correlator
(5.135)
1
n
G{1}(ℓ)
K(ℓ)
∼1
n
1
ℓ
p1−p0
+ . . . ,
(5.137)
with the exponent p1 −p0 controlling the relative importance of this NLO correction.
In this case we see that p1 −p0 = −1, meaning that the above ratio scales with the
depth-to-width ratio ℓ/n. This again illustrates the perturbative cutoﬀof our eﬀective
theory, ℓ≲n. However, in this particular case such a scaling turns out to be an artifact
of not properly tuning the initialization hyperparameters CW at ﬁnite width, as we will
see next.
NLO metric, renormalized
In §5.3.3, we learned how to ﬁnd the critical initialization hyperparameters for the K⋆=
0 universality class, ﬁxing the hyperparameters Cb and CW using the inﬁnite-width
recursions for the kernel components.
However, in §4.5 we explained that all of the
observables computed in a large-n expansion receive an inﬁnite series of subleading
corrections in 1/n. This suggests that we should have allowed further ﬁne-tuning of the
143

initialization hyperparameters at criticality by considering large-n expansions
C(ℓ)
b
= c(ℓ){0}
b
+ c(ℓ){1}
b
nℓ−1
+ c(ℓ){2}
b
n2
ℓ−1
+ . . . ,
(5.138)
C(ℓ)
W = c(ℓ){0}
W
+ c(ℓ){1}
W
nℓ−1
+ c(ℓ){2}
W
n2
ℓ−1
+ . . . ,
(5.139)
allowing us to adjust such hyperparameters order by order in 1/n. Such an expansion
could potentially give additional criticality conditions at each order in perturbation
theory.
Considering the ﬁnite-width recursions (5.109) and (5.110), we see that such sub-
leading tunings will not aﬀect the leading order result for observables that depend on
the four-point vertex, since the leading contributions to such observables are already at
O(1/n). However, these tunings do aﬀect the solution for the NLO metric, because the
NLO metric is itself subleading.
Concretely, there is an additional contribution to the NLO-metric recursion (5.110)
coming from inserting the expansions (5.138) and (5.139) into the kernel recursion
(5.108). The terms proportional to c(ℓ){1}
b
or c(ℓ){1}
W
are now subleading and thus con-
tribute to the NLO metric recursion:
G{1}(ℓ+1) =
h
c(ℓ){1}
b
+ c(ℓ){1}
W
g

K(ℓ)i
+ χ(ℓ)
∥G{1}(ℓ) + 1
8 j

K(ℓ)
V (ℓ)
 K(ℓ)2 .
(5.140)
With this new “renormalized” perspective, we see that the analysis we did in the “bare”
subsubsection before was just a particular choice of subleading corrections, c(ℓ){1}
b
=
c(ℓ){1}
W
= 0. More generally, we really do have additional knobs to turn at this subleading
order.
Substituting in (5.121) for χ∥(K), (5.123) for j(K), and (5.83) for g(K), we ﬁnd an
algebraic equation
G{1}(ℓ+1) =c(ℓ){1}
b
+ c(ℓ){1}
W
σ2
1

K(ℓ) + a1

K(ℓ)2 + . . .

(5.141)
+ G{1}(ℓ) h
1 + 2a1K(ℓ) + . . .
i
+ V (ℓ) h
a1 + 3a2K(ℓ) + . . .
i
.
Plugging in the solution for the kernel (5.124) and vertex (5.127) – making sure to include
the subleading-in-ℓterms in both – inserting our large-ℓscaling ansatz for G{1}(ℓ) (5.133)
and matching terms, we ﬁnd that the tunings
c(ℓ){1}
b
= 0 ,
c(ℓ){1}
W
= 2
3c(ℓ){0}
W
=
2
3σ2
1
,
(5.142)
result in an asymptotically suppressed solution for the NLO metric
G{1}(ℓ) = 2
3
"
3a2 −a2
1
(−a1)3
#
1
ℓ+ O
log(ℓ)
ℓ2

,
(5.143)
144

with a critical exponent p1 = 1. Speciﬁcally, the tuning of c(ℓ){1}
b
was required to suppress
a linear growing ∼ℓcontribution, while the tuning of c(ℓ){1}
W
cancels the constant O(1)
piece we found before in (5.134).
• In a sense, we got lucky before in our bare analysis: redoing this analysis with-
out a c(ℓ){1}
b
= 0 tuning, the dimensionless ratio (5.137) grows quadratically with
depth and implies that the NLO metric dominates the kernel at ℓ∼√n. The
fact that this subleading correction becomes parametrically large before reaching
the ℓ/n perturbative cutoﬀof the eﬀective theory really means that it’s growing
exponentially; c(ℓ){1}
b
̸= 0 eventually spoils criticality.
• In another sense, we got unlucky before: without the c(ℓ){1}
W
= 2
3c(ℓ){0}
W
tuning, the
NLO metric is a leading ℓ/n correction. We see now that when properly handled,
p1 −p0 = 0 and the dimensionless ratio (5.137) is O(1) in depth at leading order.
Such a correction is said to be marginal under the RG ﬂow.
This means that, while
we’ll always have to take into account the relevant four-point vertex corrections,
we should be able to neglect NLO metric corrections as long as we respect the
ﬁnite-width tunings (5.142).
Finally, the necessity of including such perturbative corrections to the critical initial-
ization hyperparameters gives an alternate perspective on what can go wrong in practice
when the network depth L approaches the network width n. Even for ensembles of such
networks, the averaged quantities will require ﬁner and ﬁner tunings – e.g. (5.138) and
(5.139) – in order for the eﬀective theory describing the ensemble to reach criticality. For
any reasonable value of n, such corrections will quickly become ﬁner than the ﬂoating-
point precision limit used to represent the hyperparameters. Thus, in practice it becomes
essentially impossible to tune such large square networks to criticality.26
5.5
Finite-Angle Analysis for the Scale-Invariant Univer-
sality Class
In this section, we’ll confront an important subtlety for activation functions in the scale-
invariant universality class.
Recall that activation functions in this class take the form
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 ,
(5.144)
and generally have a kink at the origin z = 0 (except for the degenerate member, the
linear activation function, which has a+ = a−).
In footnote 5 we ﬁrst mentioned
26Note that this is an entirely diﬀerent problem than the chaotic behavior at large depth that we
described in §3.4 for deep linear networks. For the scale-invariant universality class, the NLO metric
correction vanishes and therefore c(ℓ){1}
W
= 0.
145

the existence of a subtlety after giving our δ expansions for the kernel (5.22)–(5.24),
lightly questioning the validity of our expansions for non-smooth σ(z). In footnote 11,
we then described the main consequence of this subtlety.
In particular, we claimed
that for nonlinear scale-invariant activation functions the constant value – as a function
of layer – of the perpendicular perturbation δδK(ℓ)
[2] at criticality is an artifact of the
perturbative δ expansion. To understand this claim properly, we’ll need to work out the
full nonperturbative kernel recursion for activation functions in this class. This in turn
will let us see the aforementioned correction to the asymptotic large-ℓbehavior of the
kernel component δδK(ℓ)
[2] .
For this analysis, it will be suﬃcient to focus on two inputs xi;± of the same norm. In
our previous setup, we assumed that both inputs were nearby such that their diﬀerence
δxi ≡(xi;+−xi;−) was perturbatively small, δxi ≪1; here, we will make no assumptions
at all about their diﬀerence. Given the symmetries of the network evolution, the indi-
vidual norms of the two preactivations corresponding to these inputs will also be equal:
K(ℓ)
d
≡E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;+
2
#
= E
"
1
nℓ
nℓ
X
i=1

z(ℓ)
i;−
2
#
.
(5.145)
Geometrically this means that our preactivations live together on an nℓ-dimensional
sphere with radius
q
nℓK(ℓ)
d , and algebraically this means that the parallel component
vanishes K(ℓ)
[1] = 0, cf. (5.18). Going forward, we will call K(ℓ)
d
the diagonal kernel.27
The remaining dynamical variable is the polar angle between the preactivations.
Therefore, we can decompose the two-input kernel matrix with the following parameter-
ization:
K(ℓ)
α1α2 =
 
K(ℓ)
++
K(ℓ)
+−
K(ℓ)
−+
K(ℓ)
−−
!
= K(ℓ)
d


1
cos

ψ(ℓ)
cos

ψ(ℓ)
1

,
ψ(ℓ) ∈[0, π] .
(5.146)
The polar angle ψ(ℓ) ranges from 0 – where the preactivations are coincident as zi;+ =
zi;−, making the kernel matrix degenerate – to π – where they’re anti-correlated as
zi;+ = −zi;−. So far all we’ve done is ﬁxed the norm of our two inputs to be equal
and decomposed the kernel into a particular choice of coordinates; such a choice and
parameterization can be applied to the analysis of any activation function. We’ll now
specialize to scale-invariant activation functions for which class it’s possible to derive a
nonperturbative recursion for the polar angle.
27Perturbatively, the diagonal kernel K(ℓ)
d
is equal to the midpoint kernel K(ℓ)
00 – the kernel for the
midpoint input xi;0 ≡(xi;+ + xi;−)/2 – at leading order in the δ expansion, cf. (5.22)–(5.24). Non-
perturbatively, these two kernels are very diﬀerent. To see this most vividly, consider two antipodal
inputs xi;+ = −xi;−.
Then, the midpoint input is the zero vector xi;0 = 0, and the midpoint ker-
nel in the ﬁrst layer is given by K(1)
00
= C(1)
b
. In contrast, the diagonal kernel is given by either of
K(1)
d
= C(1)
b
+ (C(1)
W /n0) Pn0
i=1 x2
i;±.
146

RG ﬂow of the polar angle
The diagonal kernel follows the by-now familiar recursion for the single-input kernel (5.4)
K(ℓ+1)
d
= Cb + CW g

K(ℓ)
d

= Cb + A2CW K(ℓ)
d
,
(5.147)
where on the right-hand side we plugged in the explicit details for the scale-invariant
universality class (5.63) and recalled A2 ≡
 a2
+ + a2
−
 /2. This part of the analysis carries
over from §5.2. We recall here that we can readily solve the recursion for any choice of
initialization hyperparameters, and in particular criticality is attained by setting Cb = 0
and A2CW = 1, where the diagonal kernel stays exactly constant: K(ℓ)
d
= K(1)
d
≡K⋆
d.
With the evolution of the magnitude determined, we now need to ﬁnd a recursion
for the polar angle ψ(ℓ). Plugging our new decomposition (5.146) into the full kernel
recursion (5.1), the oﬀ-diagonal component of the recursion becomes
K(ℓ+1)
d
cos

ψ(ℓ+1)
= Cb + CW ⟨σ(z+) σ(z−)⟩K(ℓ) .
(5.148)
In this parameterization, the Gaussian expectation reads
⟨σ(z+) σ(z−)⟩K(ℓ) ≡
R dz+dz−σ(z+) σ(z−) e
−1
2
P
α1,α2=± Kα1α2
(ℓ)
zα1zα2
2πK(ℓ)
d
sin
 ψ(ℓ)
,
(5.149)
where the denominator comes from evaluating the determinant
q2πK(ℓ). To make
further progress, we need to evaluate this painful integral.
Before working out the general case, let’s focus on the ReLU. Setting a+ = 1 and
a−= 0, we see that the argument of the Gaussian expectation is given by σ(z+) σ(z−) =
z+z−when z+ > 0 and z−> 0 and vanishes otherwise. This means that the Gaussian
expectation (5.149) is concentrated entirely in the ﬁrst quadrant. In addition, noting that
the integrand is invariant under parity (z+, z−) →(−z+, −z−), we can niftily substitute
the integral over the ﬁrst quadrant for half the integral over the ﬁrst and third quadrants.
This lets us rewrite the above Gaussian expectation as
⟨σ(z+) σ(z−)⟩K(ℓ) =
1
2
R dz+dz−

z+z−>0 z+z−e
−1
2
P
α1,α2=± Kα1α2
(ℓ)
zα1zα2
2πK(ℓ)
d
sin
 ψ(ℓ)
.
(5.150)
The above actually turns out to be the only nifty step of the derivation; everything else
is just a Herculean sequence of coordinate changes.
There are three coordinate changes in said sequence:
z± =u ± w
√
2
(5.151)
=
s
K(ℓ)
d
1 + cos
 ψ(ℓ)
2
x ±
s
K(ℓ)
d
1 −cos
 ψ(ℓ)
2
y
=
s
K(ℓ)
d
1 + cos
 ψ(ℓ)
2
r cos(φ) ±
s
K(ℓ)
d
1 −cos
 ψ(ℓ)
2
r sin(φ) .
147

The ﬁrst one diagonalizes the kernel so that the distribution factorizes p(z+, z−) =
p(u)p(w), the second one normalizes the coordinates with the kernel’s eigenvalues, and
the last one exchanges Cartesian coordinates for polar coordinates.28 Accordingly, this
lets us rewrite the sum in the exponential in (5.150) as
X
α1,α2=±
Kα1α2
(ℓ)
zα1zα2 =
u2
K(ℓ)
d
1 + cos
 ψ(ℓ) +
w2
K(ℓ)
d
1 −cos
 ψ(ℓ) = x2 + y2 = r2 ,
(5.152)
while the product in the integrand becomes
z+z−= K(ℓ)
d r2
2
h
cos(2φ) + cos

ψ(ℓ)i
,
(5.153)
and the integral measure transforms as
dz+dz−= K(ℓ)
d
sin

ψ(ℓ)
r dr dφ .
(5.154)
Substituting (5.152)–(5.154) back into the Gaussian expectation (5.150), we get
⟨σ(z+) σ(z−)⟩K(ℓ) =K(ℓ)
d
8π
Z ∞
0
dr r3e−r2
2
Z 2π
0
dφ

cos(2φ)+cos(ψ(ℓ))>0
h
cos(2φ) + cos

ψ(ℓ)i
.
(5.155)
The rest is now relatively straightforward. The radial integral can be evaluated by
another change of the coordinate s = r2/2:
Z ∞
0
dr r3e−r2
2 =
Z ∞
0
ds 2s e−s =
h
−2e−s −2s e−si
∞
0 = 2 .
(5.156)
For the angle integral, note that any function of cos(2φ) gives the same contribution
from the four intervals eφ ≡2φ ∈[0, π], [π, 2π], [2π, 3π], [3π, 4π]. Further, within that ﬁrst
interval the constraint cos

eφ

> −cos

ψ(ℓ)
can be simply expressed as eφ < π −ψ(ℓ).
Together, this lets us write
Z 2π
0
dφ

cos(2φ)+cos(ψ(ℓ))>0
h
cos(2φ) + cos

ψ(ℓ)i
(5.157)
=4
Z π
0
deφ
2

cos(eφ)+cos(ψ(ℓ))>0
h
cos(eφ) + cos

ψ(ℓ)i
=2
Z π−ψ(ℓ)
0
deφ
h
cos

eφ

+ cos

ψ(ℓ)i
= 2 sin

ψ(ℓ)
+ 2

π −ψ(ℓ)
cos

ψ(ℓ)
.
28Unlike the perturbative calculations in (5.33) and (5.34), the diagonalization and normalization here
are nonperturbatively exact. To reﬂect more on this, while we can always change coordinates as (5.151),
we used the details of the ReLU in going from (5.149) to (5.150), establishing both the restricted domain
of integration and the simpliﬁed form of the integrand, σ(z+) σ(z−) →z+z−, within that domain. For a
general activation function, the resulting integral in the new coordinates (5.151) would still be diﬃcult
to evaluate, and we would have to resort to a perturbative expansion in ψ(ℓ), ultimately analogous to
the δ expansion, in order to make progress.
148

Inserting (5.156) and (5.157) into (5.155), we ﬁnally arrive at an expression for the
Gaussian expectation of ReLU activations:
⟨σ(z+) σ(z−)⟩K(ℓ) = K(ℓ)
d
2π
h
sin

ψ(ℓ)
+

π −ψ(ℓ)
cos

ψ(ℓ)i
.
(5.158)
Now, let’s work out the painful integral (5.149) for an arbitrary scale-invariant ac-
tivation function (5.144). In general, there are contributions from the ﬁrst quadrant
proportional to a2
+ and similar contributions from the third quadrant proportional to
a2
−, in both cases with the constraint z+z−> 0 after our nifty trick. Then, there are
also contributions from the second and fourth quadrants, both proportional to a+a−
and with the constraint z+z−< 0. Following a very similar sequence of steps as we did
before for the ReLU, we can evaluate the Gaussian expectation (5.149) as
⟨σ(z+) σ(z−)⟩K(ℓ) = K(ℓ)
d
2π (a2
+ + a2
−)
Z π−ψ(ℓ)
0
deφ
h
cos

eφ

+ cos

ψ(ℓ)i
(5.159)
+ K(ℓ)
d
2π (2a+a−)
Z π
π−ψ(ℓ) deφ
h
cos

eφ

+ cos

ψ(ℓ)i
= K(ℓ)
d
2π (a+ −a−)2 h
sin

ψ(ℓ)
−ψ(ℓ) cos

ψ(ℓ)i
+
 
a2
+ + a2
−
2
!
K(ℓ)
d
cos

ψ(ℓ)
.
The full nonperturbative recursion for the oﬀ-diagonal part of the kernel (5.148) thus
evaluates to
K(ℓ+1)
d
cos

ψ(ℓ+1)
(5.160)
=Cb + CW
(
K(ℓ)
d
2π (a+ −a−)2h
sin

ψ(ℓ)
−ψ(ℓ) cos

ψ(ℓ)i
+
 
a2
+ + a2
−
2
!
K(ℓ)
d
cos

ψ(ℓ))
.
One thing we notice here is that even though we evaluated the Gaussian expectation,
we’ll still have to deal with the fact the recursion is highly nonlinear in ψ(ℓ+1).
While you’re here and we have your attention, let’s record the result for one addi-
tional nonperturbative Gaussian expectation for the scale-invariant universality class:
⟨σ′(z+) σ′(z−)⟩K(ℓ). The integral here is much simpler to evaluate than the undiﬀerenti-
ated one above since in each quadrant the argument of the expectation, σ′(z+) σ′(z−),
is constant. Following otherwise the same set of steps as above, in this case we ﬁnd

σ′(z+) σ′(z−)

K(ℓ) =(a2
+ + a2
−)
4π
Z ∞
0
dr re−r2
2
 Z 2π
0
dφ

cos(2φ)+cos(ψ(ℓ))>0
(5.161)
+ 2a+a−
4π
Z ∞
0
dr re−r2
2
 Z 2π
0
dφ

cos(2φ)+cos(ψ(ℓ))<0
=
 
a2
+ + a2
−
2
!
−ψ(ℓ)
2π (a+ −a−)2 .
149

We guess you guys aren’t ready for that yet. But your future-selves are gonna love it.29
Criticality analysis of the polar angle
Having evaluated the recursion, let’s now tune to criticality and work out the correct
large-ℓasymptotic behavior of the polar angle ψ(ℓ). Working at scale-invariant criticality,
with Cb = 0 and A2CW = 1, and where the diagonal kernel is constant as K(ℓ)
d
= K⋆
d,
the oﬀ-diagonal recursion (5.160) simpliﬁes to a decoupled recursion for the polar angle,
cos

ψ(ℓ+1)
= cos

ψ(ℓ)
+ ρ
h
sin

ψ(ℓ)
−ψ(ℓ) cos

ψ(ℓ)i
.
(5.162)
Here, it was convenient to deﬁne a new constant,
ρ ≡1
π
(a+ −a−)2
 a2
+ + a2
−
 ,
(5.163)
that encapsulates all of the details of the speciﬁc scale-invariant activation function.
Roughly, ρ is a dimensionless measure of the kinkiness of the activation function at the
origin, equal to zero for the linear activation function and 1/π for the ReLU. We see
right away that the polar angle is exactly preserved for, and only for, ρ = 0. In particular,
the preservation of the full two-input kernel matrix that we saw for the linear activation
function in §3.2 doesn’t extend to any other member of the universality class.
In order to determine the large-ℓbehavior of the polar angle ψ(ℓ), we need a way to
analyze the recursion (5.162). As we’ve been emphasizing, our main tool for analyzing
such a nonlinear recursion is to ﬁnd a ﬁxed point and then linearize around it.30 By
inspection of the recursion, it’s clear that ψ⋆= 0 is a ﬁxed point. Thus, we should focus
in on the small-angle regime: ψ(ℓ) ≪1.
Taylor expanding the trigonometric functions in the recursion (5.162) around a van-
ishing polar angle, the linearized recursion becomes
ψ(ℓ+1) = ψ(ℓ)
r
1 −2ρ
3 ψ(ℓ) + O(ψ2) = ψ(ℓ) −ρ
3

ψ(ℓ)2 + O

ψ3
.
(5.164)
To solve this recursion, we can use our scaling ansatz (5.93), which here reads
ψ(ℓ) =
1
ℓ
pψ 
c0,0 + O
log ℓ
ℓ

,
(5.165)
with the critical exponent pψ governing the decay of the polar angle.
Plugging this
ansatz into our recursion (5.164) and matching the terms on both sides of the equation,
we ﬁnd a solution:
ψ(ℓ) =
3
ρ
 1
ℓ+ O
log ℓ
ℓ2

.
(5.166)
29This result will turn out to be really useful in §10.3 when we investigate generalization error for the
scale-invariant universality class at inﬁnite width.
30Since we already nonperturbatively evaluated the Gaussian expectation (5.159) and fully took into
account the lack of smoothness of the activation function – with the constant ρ (5.163) characterizing
its kinkiness – at this point it’s completely safe to employ a perturbative expansion.
150

From this we can read oﬀthe critical exponent, pψ = 1, which is universal excepting the
degenerate linear limit of ρ = 0, for which we instead have pψ = 0.
In order to recast this result in the language of the rest of this chapter, let’s project
the two-input kernel (5.146) into the γ[a]
αβ representation using (5.20) and then insert
(5.166):
K(ℓ)
[0] = K(ℓ)
d

1 + cos

ψ(ℓ)
2

= K⋆
d + O
 1
ℓ2

,
(5.167)
K(ℓ)
[2] = K(ℓ)
d

1 −cos

ψ(ℓ)
2

= K⋆
d
 9
4ρ2
 1
ℓ2 + O
log ℓ
ℓ3

.
(5.168)
These solutions form the basis of what we claimed earlier in footnote 11. In particular,
the perpendicular perturbation K(ℓ)
[2] crosses over from being nearly constant for small
depth ℓ≪ℓcross to power-law decaying ∼1/ℓ2 for large depth ℓ≫ℓcross.31 This implies
that the true critical exponent for scale-invariant perpendicular perturbations is p⊥= 2.
Here, the crossover scale ℓcross is approximately given by
ℓcross ∼
3
ρψ(ℓ=1) ∼3
2ρ
v
u
u
u
t
K(ℓ=1)
[0]
K(ℓ=1)
[2]
.
(5.169)
We get this by equating the small-depth constant answer, set by the ﬁrst-layer condition,
with the large-ℓasymptotic answer given by (5.166); on the right-hand side of (5.169) we
further wrote the polar angle ψ(ℓ=1) in terms of the kernel components using (5.167) and
(5.168). What we see is that the smaller this initial angle ψ(ℓ=1) is – meaning that the
closer the two inputs are to each other – the longer our original constant solution to the
naive perpendicular recursion (5.65) is valid, and the longer it takes for the power-law
regime to kick in.
The discussion above explains why our δ expansion failed to see the crossover: in
such an analysis, by construction, K(ℓ=1)
[2]
is inﬁnitesimally small. This means that the
crossover scale (5.169) is pushed to inﬁnity, invisible to perturbation theory. Here is
another way to see it. For small separation of two inputs, we can rewrite the angle as
ψ(ℓ) ≈2
v
u
u
tδδK(ℓ)
[2]
K⋆
d
+ . . . ,
(5.170)
and hence the angle recursion (5.164) can be recast – upon a squaring and a rearrange-
ment of terms – as
δδK(ℓ+1)
[2]
= δδK(ℓ)
[2] −
4ρ
3
pK⋆
d

δδK(ℓ)
[2]
 3
2 + . . . .
(5.171)
31For deep linear networks where ρ = 0, the solution (5.168) is degenerate and doesn’t apply. However,
from our discussion just before we know that for such networks the polar angle remains constant at any
depth.
151

Unfortunately, it’s impossible to generate such a non-integer power, 3/2, via a Taylor
expansion. Given our our ansatz for the perpendicular perturbation K(ℓ)
[2] (5.24), this
explains why the correction term was invisible before. (There is no such issue for smooth
activation functions; our Taylor expansion and subsequent analysis can be completely
trusted for the K⋆= 0 universality class.)
The overall lesson here is that we should be very careful whenever encountering
singular Gaussian expectations. In the future when we need to consider multiple inputs
for nonlinear scale-invariant activation functions, we’ll make sure to recall the results
here.
152

Chapter 6
Bayesian Learning
. . . the mathematical rules of probability theory are not merely rules for calculating
frequencies of ‘random variables’; they are also the unique rules for conducting
inference (i.e. plausible reasoning) of any kind, and we shall apply them in full
generality to that end.
E. T. Jaynes, explaining the theme of his book [50].
In the previous three chapters, we’ve spent a considerable amount of spacetime ana-
lyzing the ensemble of wide neural networks at initialization. In particular, through
the 1/n expansion and deep asymptotic analysis, we’ve obtained a rather thorough un-
derstanding of the interplay between the architecture, width, depth, and initialization
hyperparameters that together deﬁne the eﬀective distribution of preactivations.
In this study, we’ve paid very careful attention to the deep of deep learning to the
total neglect of the learning. But this is a deep learning book, not just a deep book. Thus,
in this chapter we will begin to learn about learning and – if the titles of our chapters
are any real guide to their contents – will continue learning about learning for the rest
of the book.
We’ll begin on our learning quest with a discussion of Bayesian inference, as it
provides a natural framework for thinking about learning in general. We’ll ﬁrst explain
in §6.1 the Bayesian approach to probability, in which probabilities are reinterpreted to
represent the strength of our beliefs about the world according to diﬀerent hypotheses.
There, we’ll learn that the rules of Bayesian inference – really the rules of logic extended
to probabilistic reasoning – pick out a logically consistent way of incorporating newly
observed information into the probabilistic models representing our hypotheses.
From §6.2 on out, we’ll see why this simple yet powerful framework enables us to
analyze and then understand how deep neural networks learn from observed data.
In §6.2.1, we’ll detail how Bayesian model ﬁtting works for neural networks. First,
we’ll reinterpret our well-studied eﬀective preactivation distribution as a prior distribu-
tion, encoding our initial beliefs about the model outputs before observing any data.
With this as a starting point, the rules of Bayesian inference then imply a learning algo-
153

rithm for sharpening our beliefs so as to best ﬁt our observations. The result of inference
– the posterior distribution – further lets us make Bayesian predictions on novel inputs
whose outputs we haven’t observed but need to infer. This naturally segues into a dis-
cussion of practical implementations: ﬁrst we’ll discuss approximation methods – giving
a Bayesian interpretation to the gradient-based learning methods that we’ll explore in
the epochs following this chapter – and then we’ll discuss an exact method on which the
rest of the current chapter will be based.
In §6.2.2, we’ll expand our horizons by contemplating the ultimate question of Life,
the Universe, and Everything: Bayesian model comparison. We’ll explain how to use
Bayesian evidence to select between diﬀerent plausible hypotheses, organized according
to diﬀerent choices of hyperparameters and network architectures, in order to pick the
best ones. Bayesian model comparison also gives us a quantitative means to address
inductive biases, the often hidden assumptions built into deep learning models. As a
bonus, we’ll further see how Occam’s razor is automatically incorporated in the rules of
Bayesian inference applied to such model comparison. With these tools, we can really
begin to address one of the fundamental questions we posed at the beginning of the
book: why do some neural network models perform so well while others fail?
These abstract discussions are then followed by an onslaught of concrete calculations
for inﬁnite- and ﬁnite-width neural networks in §6.3 and §6.4, respectively.
Some of these calculations reinforce the themes of the previous chapter. We’ll ﬁrst
show that Bayesian model comparison prefers critical initialization hyperparameters,
giving additional evidence for the principle of criticality (§6.3.1). We’ll also illustrate
another role of ﬁnite-width interactions.
Speciﬁcally, the accumulation of correlated
ﬂuctuations induces an inductive bias for neural association, leading to a propensity for
Hebbian learning – a learning principle inspired by biological neurons (§6.4.1).
Some of these calculations contrast qualitatively diﬀerent characteristics of inﬁnite-
and ﬁnite-width models that are trained with exact Bayesian learning. Analyzing the
posterior distribution of network outputs, we’ll see that correlations among diﬀerent
components of the output are nonzero at ﬁnite width only (§6.3.2⊥§6.4.2). The resulting
expressions will also make it clear why – while theoretically quite tractable – exact
Bayesian learning is impractical for any dataset of reasonable size. Next, analyzing the
posterior distribution of hidden-layer representations, we’ll see the absence/presence of
representation learning at inﬁnite/ﬁnite width (§6.3.3⊥§6.4.3). Overall, this contrasting
will provide a valuable blueprint for when we later consider inﬁnite- and ﬁnite-width
models trained with gradient-based learning (§10 ⊥§∞).
6.1
Bayesian Probability
A Bayesian always starts with a hypothesis H.
Mathematically, a hypothesis is a
mechanism for assigning numbers p(A|H) to statements A about the world.
These
statements are logical propositions – such as “it will rain tomorrow” or “this image x
contains a cat” or “the output value for this function f(x) evaluated on an input x is
z” – and these numbers p(A|H) represent the relative plausibilities of those statements
154

according to the assumptions or model of the world summarized by the hypothesis H.
In the context of machine learning, p(A|H) is often called a probabilistic model.
As this notation and discussion should make clear, these beliefs p(A|H) are expressed
in the language of probability. However, the Bayesian interpretation of the probability
p(A|H) subtlety diﬀers from the ensemble interpretation that we gave in §2.3. Namely,
rather than representing the statistics of a random variable – the relative frequency or
chance observing A, given the conditions H – this probability instead constitutes the
strength of our belief in the proposition A according to the assumptions H.1 Further,
with such a Bayesian perspective all of probability theory and statistical inference can be
uniquely derived as a consequence of logical constraints on these beliefs p(A|H).2 We’ll
next brief you through these constraints as they form the foundation of this chapter but,
as we have been using probabilities for quite a while now in this book, let us be brief.
Formally, the ﬁrst logical constraint is known as the product rule,
p(A, B|H) = p(A|B, H) p(B|H) = p(B|A, H) p(A|H) ,
(6.1)
where p(A, B|H) represents a joint belief in both A and B according to the hypothesis
H, while p(A|B, H) represents a conditional belief in A according to H given that B has
been observed. The second logical constraint is known as the sum rule,
p(A|H) =
X
B
p(A, B|H) ,
(6.2)
and relates the joint belief in A and B to a marginal belief in just A.3 Here, the symbol
P
B represents a sum over all the logically possible values of a discrete variable B, or for
a continuous variable it represents an integral.4 This sum rule in particular implies the
normalization condition if we assign p(C|H) ≡1 for the statement C that holds with
absolute certainty according to H:
X
B
p(B|H) =
X
B
p(C, B|H) = p(C|H) = 1 .
(6.3)
1The ensemble interpretation is often called frequentist probability when contrasted with
Bayesian probability. In this book, we use the interpretation that is most appropriate for the partic-
ular problem under consideration: if we’re instantiating models by randomly drawing parameters from
an initialization distribution, it makes sense to analyze an ensemble; if we’re making inferences based on
a ﬁxed hypothesis or comparing diﬀerent hypotheses, it makes sense to adopt the Bayesian perspective.
2See Jaynes’ book [50] for an extended development of this perspective for which our brief summary
does not give justice.
3We essentially discussed this sum rule as (4.93) under §4.4 Marginalization Rules.
4Though (Bayesian) probably it’s already clear if you’ve made it this deep in the book, as we cannot
be (Bayesian) certain, let us clarify the meaning of the statement A inside the belief system p(A|H).
Sometimes a statement represents a ﬁxed logical proposition, such as A = “Schr¨odinger’s cat is alive” with
p(A|H) encoding the plausibility of cat’s aliveness. Sometimes a statement represents a binary variable,
such as B = “the livelihood of Schr¨odinger’s cat” which takes values in {dead, alive} with p(B|H)
giving the distribution over the two binary outcomes.
More generally, the statement can represent
observable outcomes O of experiments – a.k.a. observables – with p(O|H) encoding our relative belief in
the plausibilities of the diﬀerent outcomes, where such observables can take on a discrete or continuous
spectrum of values. Prominent examples of such general observables for us include the model parameters
θ and preactivations z(ℓ).
155

With these rules in mind, after ﬁxing a hypothesis a Bayesian then gathers informa-
tion in order to reﬁne the plausibilities of diﬀerent beliefs. For instance, after observing
A, we may want to update our beliefs about B. Such Bayesian inference can be ac-
complished by noting that an algebraic rearrangement of the product rule (6.1) tells us
how our beliefs should change as we condition on additional information A:
p(B|A, H) = p(A|B, H) p(B|H)
p(A|H)
.
(6.4)
This rearrangement is so important that it’s given its own name, Bayes’ rule, and even
each individual factor of the equation is named as well:
• The factor p(B|H) is called the prior of B, thusly named because it quantiﬁes our
belief in B a priori; that is, it encodes our belief in B based entirely on our model
H before we observe any additional information.
• The factor p(B|A, H) is called the posterior of B given A, thusly named because
it quantiﬁes our belief in B a posteriori upon learning A; that is, it encodes how
our model H updates its belief in B after observing A.
• The factor p(A|B, H) is called the likelihood. We’ll elaborate more on its name
and interpretation later in §6.2.1 where we talk about model ﬁtting.
• The factor p(A|H) is called the evidence for H. We’ll elaborate more on its name
and interpretation later in §6.2.2 where we talk about model comparison.
Note that the posterior is automatically normalized:
X
B
p(B|A, H) =
X
B
p(A|B, H) p(B|H)
p(A|H)
=
X
B
p(A, B|H)
p(A|H)
= p(A|H)
p(A|H) = 1 .
(6.5)
More importantly, Bayes’ rule is the only logically consistent way to update a set of
beliefs after making observations.
6.2
Bayesian Inference and Neural Networks
The Bayesian framework for inference can be used for building, updating, and reasoning
with powerful probabilistic models of the world. Let’s now see how we can apply the
Bayesian framework to deep learning, ﬁrst for model ﬁtting (§6.2.1) and then for model
comparison (§6.2.2).
6.2.1
Bayesian Model Fitting
For neural networks, it’s most natural to begin by discussing the prior distribution p(θ|H)
of the model parameters θµ =
n
b(ℓ)
i , W (ℓ)
ij
o
. This prior lets us quantify our initial beliefs
about the particular values of the model parameters that determine our neural-network
156

function approximator f(x; θ). The most common choice is to simply reinterpret the
initialization distribution of the ensemble,
p(θ|H) ≡
L
Y
ℓ=1



" nℓ
Y
i=1
p

b(ℓ)
i
# 

nℓ
Y
i=1
nℓ−1
Y
j=1
p

W (ℓ)
ij





,
(6.6)
as our Bayesian prior distribution. Here we recall that p

b(ℓ)
i

and p

W (ℓ)
ij

– given by
(2.21) and (2.22) – are zero-mean Gaussian distributions with bias variance C(ℓ)
b
and
weight variance C(ℓ)
W /nℓ−1, respectively.
From the Bayesian perspective, these initialization hyperparameters are part of the
hypothesis H. This hypothesis H also contains our choice of architecture – MLP, CNN,
transformer, etc. – as well as all the architecture hyperparameters within that architec-
ture class – e.g. for MLPs we need to further select the depth L, the hidden-layer widths
nℓ, and the activation function σ(z). In short, H is for Hyperparameters.5
Here, we’ve taken familiar objects – the hyperparameters and the initialization dis-
tribution characterizing the frequency of potential network realizations – and interpreted
them in the way of Bayes – as the hypothesis H and as the prior distribution p(θ|H) char-
acterizing our initial beliefs about the value of the model parameters. Another familiar
object, of course, is the distribution of ℓ-th-layer preactivations that we’ve spent last
three chapters evaluating explicitly. To give that a Bayesian interpretation, let us ﬁrst
denote by z(ℓ)
D ≡
n
z(ℓ)
i;δ
o
the set of ℓ-th-layer preactivations evaluated on inputs xj;δ ∈D
in some dataset D. Then, the prior distribution over these ℓ-th-layer preactivations can
be related to the prior distribution over the model parameters by
p

z(ℓ)
D
H

=
Z "
P
Y
µ=1
dθµ
#
p

z(ℓ)
D , θ
H

=
Z "
P
Y
µ=1
dθµ
#
p

z(ℓ)
D
θ, H

p(θ|H) ,
(6.7)
where we’ve applied the sum rule (6.2) in the ﬁrst equality and the product rule (6.1)
in the second. This prior quantiﬁes our initial beliefs about the diﬀerent neural-network
variables. More speciﬁcally, for a hidden layer ℓ, this distribution represents our be-
liefs about a particular feature representation of the input and, for the output layer
L, this represents our initial beliefs about the behavior of the function approximation
f(x; θ). More generally, for any neural-network observable O = O(θ), our prior beliefs
5To be strict, we should have always conditioned on C(ℓ)
b , C(ℓ)
W , and nℓwhenever we discussed the
initialization distribution: p(θ) →p

θ
n0, C(1)
b
, C(1)
W , . . . , nL−1, C(L)
b
, C(L)
W

. Thankfully we’ve so far left,
and will continue to leave, this type of detailed dependence implicit for notational simplicity. However,
to underscore the importance of the hypothesis for Bayesian inference, in this chapter we (i) will leave
the conditioning on the overall hypothesis H explicit until the end of §6.3.1 and at the same time (ii)
will move the dependence of a dataset D to an overall subscript of the preactivations. As a particular
example, the prior distribution of the ℓ-layer preactivations p z(ℓ)
D
H
, deﬁned next paragraph in (6.7),
is equivalent to what we’ve been denoting as p z(ℓ)D
outside of this chapter.
157

are determined by
p
 O
H
 =
Z "
P
Y
µ=1
dθµ
#
p
 O
θ, H
 p(θ|H) .
(6.8)
To better illustrate what these formal expressions represent, let us take the net-
work output z(L)
D
as an observable. Then, the prior distribution for the output layer
p

z(L)
D
H

, (6.7), is the same distribution as the output distribution induced by the ini-
tialization ensemble, (2.35), if and only if we also pick the conditional distribution of
the outputs given the parameters to be deterministic:
p

z(L)
D
θ, H

=
nL
Y
i=1
Y
δ∈D
δ

z(L)
i;δ −fi(xδ; θ)

.
(6.9)
Here, fi(xδ; θ) is an expression for the network output given in terms of the iteration
equation that deﬁnes the MLP (2.5), while z(L)
i;δ
is interpreted as a random variable.
The resulting prior distribution for the network outputs p

z(L)
D
H

then characterizes
our overall initial belief about the joint set of output values for a given set of inputs D
according to the hypothesis H, instead of characterizing the relative frequency of such
output values at initialization across diﬀerent realizations of the model parameters. That
said, operationally, the formalism developed in the previous chapters can be directly
brought to bear on calculating with these beliefs.
Importantly, note that the deterministic conditional distribution for the output (6.9)
is a part of our hypothesis within the Bayesian framework: according to the hypothesis
H, given the model parameters θ, the outputs are deﬁnitely the ones computed by the
function f(x; θ). Another common hypothesis is the uncertain hypothesis
p

z(L)
D
θ, H

=
nL
Y
i=1
Y
δ∈D
(
1
p
2πσ2ε
exp

−1
2σ2ε

z(L)
i;δ −fi(xδ; θ)
2)
,
(6.10)
which reduces to the deterministic hypothesis (6.9) in the limit of zero variance and
absolute certainty: σ2
ε →0.6
6This hypothesis is equivalent to injecting random noise εi with mean zero and variance σ2
ε into the
network output. This in turn is tantamount to shifting the last-layer biases as b(L)
i
→b(L)
i
+εi, and hence
we can easily incorporate this in our analysis by shifting the ﬁnal bias variance as C(L)
b
→C(L)
b
+σ2
ε. You
should keep in mind, however, that εi is separate from the bias b(L)
i
and is not a part of the adjustable
model parameters θµ; instead, this noise is intended to embody an intrinsic uncertainty present in our
observation of the model’s output.
Before moving on, let us also mention one other common hypothesis for the network output, the
categorical hypothesis, deﬁned for each input x by
p(i|θ, H) ≡
exp[fi(x; θ)]
PnL
j=1 exp[fj(x; θ)] .
(6.11)
This distribution is also sometimes known as the softmax. Here, instead of considering a continuous
158

Having now thoroughly discussed the prior, let’s next consider the posterior. As
we gather more information A about the true behavior of our desired function f(x),
we should update our beliefs about our probabilistic model for f(x; θ).
In order to
incorporate this information in a logically consistent manner, we should use Bayes’ rule.
Speciﬁcally, to update our belief about the model parameters, Bayes’ rule (6.4) instructs
us to use
p(θ|A, H) = p(A|θ, H) p(θ|H)
p(A|H)
.
(6.12)
Here, to ﬁnd the posterior distribution p(θ|A, H), the prior distribution p(θ|H) gets
multiplied by the likelihood p(A|θ, H) of the model parameters θ for the observation of
A, and divided by the evidence p(A|H). Consequently, with such a posterior distribution
of the model parameters, our beliefs about any neural-network observable O shifts from
our prior p
 O
H
 (6.8) to a posterior with the insertion of A,
p
 O
A, H
 =
Z "
P
Y
µ=1
dθµ
#
p
 O
θ, H
 p(θ|A, H) .
(6.13)
These two equations (6.12) and (6.13) uniquely determine how new information A can
be incorporated to change our beliefs about the value of any neural-network observable.
For function approximation tasks, such information often comes in the form of some
dataset A containing observed input-output pairs:
A ≡{(xj;˜α, yi;˜α)} |˜α∈A .
(6.14)
Here, each input xj;˜α ∈A is paired with its corresponding true output yi;˜α ≡fi(x˜α)
recorded from our desired function f(x).7 With our observation of the true values yA ≡
{yi;˜α}, the likelihood and evidence are then given by the conditional belief p
 yA
θ, H

and the belief p
 yA
H
 for outputs, respectively. Such beliefs appeared before when
considering the prior distribution of the outputs, (6.7) with ℓ= L, but are now evaluated
on the ﬁxed values yA associated with the given inputs xA.
To develop some intuition for what this means, let’s again take the deterministic
hypothesis (6.9). In this case, the likelihood is given by
p(A|θ, H) ≡p(yA|θ, H) =
Y
˜α∈A
nL
Y
i=1
δ

yi;˜α −fi(x˜α; θ)

.
(6.15)
This likelihood quite explicitly restricts the model parameters to those exactly satisfying
the constraints fi(x˜α; θ) = yi;˜α ﬁtting our observations. Vice versa, the functions in our
distribution over the nL output values z(L)
i
, we consider a discrete distribution over output classes i,
such as dog or cat or car; then, for such classiﬁcation tasks, each number p(i|θ, H) quantiﬁes our belief
about how likely the input x represents the class i. Functionally, the softmax can be thought of as a
generalization of the logistic function (2.10) in the sense that it maps a vector of real numbers to a
discrete probability distribution.
7For maximal disambiguation, in this chapter we’ll use sample indices of the form ˜α – the Greek
letter alpha with a tilde on top – for elements of the dataset A corresponding to input-output pairs for
which the true output values from f(x) are observed.
159

set that do not satisfy these constraints are completely thrown away from the poste-
rior distribution, deemed unlikely. Note what has just happened. Naively, p(yA|θ, H)
represents our beliefs about the output values yA, given that we set the parameters of
our model to θ. However, here we ﬁrst observed the true output values yA and then
interpreted p(yA|θ, H) in terms of how likely the model parameters θ ﬁt the observation
A. This is the origin of the name “likelihood” and why the proper way to refer to it is
“the likelihood of the model parameters θ for the observation A.”
To develop even more intuition, it’s customary to introduce the negative log-
likelihood LA(θ) – or loss – representation of the likelihood:
p(yA|θ, H) ≡exp[−LA(θ)] .
(6.16)
Here, by parameterizing the loss as a function of the parameters θ, we are emphasizing
that it’s the (negative log-)likelihood of the parameters.8 For the uncertain hypothe-
sis (6.10), the negative log-likelihood takes the form of the famous mean-squared-error
or MSE loss:
LMSE(θ) =
X
˜α∈A
 1
2σ2ε
fi(x˜α; θ) −yi;˜α
2 + 1
2 log

2πσ2
ε

.
(6.17)
In particular, as the network outputs fi(x˜α; θ) get closer to their target values yi;˜α, the
MSE loss decreases and the likelihood increases.9 As such, the loss is a natural measure
of how well our model is approximating the true behavior of the function. Additionally,
since the loss (6.17) involves an explicit sum over observations, as the number of observed
input-output pairs NA increases, the likelihood can dominate the prior; that is, if we
gather enough information, eventually our prior beliefs can become entirely replaced by
what we learned from our observations.
This is Bayesian model ﬁtting: Bayesian inference (6.12) is used as a learning
algorithm to increase the accuracy of a function approximation. It gives greater pref-
erence to the functions that better ﬁt the constraints fi(x˜α; θ) = yi;˜α and penalize the
ones that don’t. The posterior (6.12) is then updated to reﬂect a balance between this
preference for ﬁtting our observations and an adherence to our prior beliefs about the
values the model parameters should take.
Ultimately, we want to use our ﬁt Bayesian model to make Bayesian predictions.
This is generically and abstractly embodied in (6.13). Speciﬁcally and concretely, for
8While the likelihood function – and therefore the loss – is considered auxiliary from the perspec-
tive of function approximation, from the perspective of Bayesian inference the form of the likelihood is
considered to be part of the hypothesis, cf. the deterministic hypothesis (6.9) vs. the uncertain hypoth-
esis (6.10).
9In the deterministic limit σ2
ε →0, the loss LA(θ) would be inﬁnite for functions that don’t exactly ﬁt
all the constraints fi(x˜α; θ) = yi;˜α and negative inﬁnite for those that do. Thus, the uncertain hypothesis
softens these hard-ﬁtting constraints of the deterministic hypothesis by relaxing the Dirac delta function
distribution to a Gaussian distribution with a ﬁnite variance σ2
ε.
When we consider the categorical hypothesis (6.11), the negative log-likelihood of the softmax distri-
bution gives the cross-entropy loss. We’ll more systematically address the consequences of these diﬀerent
choices of loss functions in §10.
160

function approximation tasks we are most often interested in posterior beliefs about the
network outputs O = z(L), for which (6.13) reads
p

z(L)A, H

=
Z "
P
Y
µ=1
dθµ
#
p

z(L)θ, H

p(θ|A, H) .
(6.18)
Once we have this distribution, then we can in particular use its mean as our prediction
and its variance as our level of conﬁdence. To compute any of these quantities, one
way or another we need to perform a gigantic integral over the model parameters θ in
order to properly weight our diﬀerent beliefs. With that in mind, we’ll now present two
kinds of methods to tackle this model marginalization: (i) approximate methods based
on saddle-point approximations and (ii) an exact method based on our eﬀective theory
approach.
Approximation methods for model marginalization: MAP and MLE
One way to tackle such a gigantic integral is to presume that the integral measure, given
by the posterior distribution p(θ|A, H) (6.12), is very concentrated around its mode:
θ⋆
MAP ≡arg max
θ
p(θ|A, H) = arg max
θ
[p(yA|θ, H) p(θ|H)] .
(6.19)
This maximum is known as the maximum a posteriori (MAP) estimate. After such a
maximization, we can use the function f(x; θ⋆
MAP) for tasks and more generally approx-
imate the full posterior distribution p
 O
A, H
 (6.12) by the point estimate O(θ⋆
MAP).
This notion of approximating a probability distribution with single value of the random
variable is known in statistics as a point estimate and in physics as a saddle-point ap-
proximation. Another commonly-used saddle is given by the maximum of the likelihood,
θ⋆
MLE ≡arg max
θ
p(yA|θ, H) ,
(6.20)
known as the maximum likelihood estimation (MLE) of the model parameters.
In terms of the negative log-likelihood LA(θ), MLE is equivalent to the minimization
of the loss
θ⋆
MLE = arg min
θ
LA(θ) ,
(6.21)
while MAP estimate (6.19) is a joint minimization of the loss and the negative log of
the prior,
θ⋆
MAP = arg min
θ
[LA(θ) −log p(θ|H)] .
(6.22)
In particular for a generic Gaussian prior of the form p(θ|H) ∝exp
 −PP
µ=1aµθ2
µ
, the
negative-log prior acts as a regularization term of the form PP
µ=1 aµθ2
µ that has an eﬀect of
penalizing large parameter magnitudes. Since the loss grows extensively with the size of
the dataset A while this regularization term stays constant, when we’ve made suﬃciently
161

many observations, we naively expect that the prior will be eventually overwhelmed by
the likelihood and that the MAP and MLE estimates will become similar.
If we are to apply these approximation methods to wide neural networks, there are
certain things we need to keep in mind.10 First of all, there is actually no single opti-
mal value for the maximum likelihood estimation θ⋆
MLE. Instead, there are continuum
of such optima, and we still have to consider a distribution over them. Importantly,
such a distribution over maxima depends critically on how the maxima are obtained.
For instance, it depends on the way you initialize model parameters θinit, the learning
algorithm used to estimate these maxima – such as gradient descent vs. stochastic gradi-
ent descent – and the training hyperparameters controlling the learning algorithm. The
study of this ensemble over optima and its dependence on the initialization and training
hyperparameters will more or less be the focus of the following chapters §7–§∞.11
Exact method for model marginalization: eﬀective theory
For the prior (6.7), we know very well that it’s possible to directly integrate out the
model parameters through the use of a 1/n expansion. Such a gigantic marginalization
was the focus of §4, and in writing (6.7) we already reinterpreted our eﬀective preac-
tivation distribution at initialization as our prior beliefs about the preactivations. For
the posterior, the only hypothetical worry would be that we’d need to carry out entirely
diﬀerent sets of integrals. We’ll show next that there is no such need. Thus, in a very
real sense the most painstaking theoretical part of Bayesian inference has already been
taken care of for us!
Let’s continue to suppose that we’ve made some observations A of the true outputs
yi;˜α ≡fi(x˜α) of our function f(x) for a given set of inputs xA in a subsample A as deﬁned
by (6.14). We now want to incorporate what we’ve learned from these observations in
order to update our beliefs about the output values z(L)
B
≡
n
z(L)
i; ˙β
o
for a potentially
10In §10, we’ll go through how all of this works in detail.
11Since those following chapters will unsentimentally drop our Bayesian lens, let’s interpret these
diﬀerent methods with fresh Bayesian eyes here.
In the impure Bayesian approach – that is MLE – we have an initialization distribution p(θinit), but
no prior distribution p(θ|H). By construction, the prior distribution does not enter into the estimate of
the impure Bayesian (6.20), but the initialization distributions (2.21) and (2.22) enters into their code
to give particular realizations of networks acting as the starting points for optimization and training.
Thus, such an initialization distribution induces a distribution over the resulting MLE estimates.
In the less impure Bayesian approach – that is MAP – we have both an initialization distribution
p(θinit) and a prior distribution p(θ|H). For the former, we again use the initialization distributions
(2.21) and (2.22) to provide starting points for optimization; for the latter, we typically use a Gaussian
prior p(θ|H) ∝exp −PP
µ=1aµθ2
µ

which, as we said, serves as a regularization term when added to the
optimization objective – the loss – as per (6.22).
In the pure Bayesian approach – which is the focus of the rest of this chapter – there is a prior distri-
bution p(θ|H) but the initialization distribution p(θinit) isn’t needed. Pure Bayesians always integrate.
What we really did with (6.6) was pick a Gaussian prior over the parameters and then adopt the same
conventions for the variances as we’ve been using for the initialization distribution (2.21) and (2.22).
We’ll see in the rest of the chapter why this is sensible.
162

diﬀerent set of inputs xj; ˙β ∈B in another subsample B.12 Beginning with the joint prior
for the network outputs over the union of both subsamples D ≡A ∪B,
p

z(L)
D
H

≡p

z(L)
A , z(L)
B
H

,
(6.23)
we can set z(L)
A
→yA and use the product rule (6.1) to condition our beliefs about z(L)
B
on the observed true values yA:
p

yA, z(L)
B
H

= p

z(L)
B
yA, H

p

yA
H

.
(6.24)
Then, rearranging terms like we are Reverend Thomas Bayes, we get
p

z(L)
B
yA, H

=
p

yA, z(L)
B
H

p

yA
H

.
(6.25)
Since this iteration of Bayes’ rule is so important, let us be verbose and crystal clear
about its interpretation: the denominator p
 yA
H
 is the prior for the network outputs
given the inputs xA in the subsample A, evaluated on the ﬁxed observed values yA, hence
it is just a number; the numerator p

yA, z(L)
B
H

is the prior for the network outputs
given the inputs xD in the joint dataset D ≡A ∪B, evaluated on the ﬁxed observed
values yA but with the network outputs z(L)
B
still variable, hence it is a function of the
z(L)
B .13 The numerator and denominator combine to make the posterior on the left-hand
side, which is thus a function of the random variable z(L)
B
encoding our posterior beliefs
about the plausible values of the network outputs z(L)
B
for the inputs xB in B, updated
with our observations about the true values yA of the outputs for the inputs xA in A. In
this way, rather than performing Bayesian inference to learn about the model parameters
as way of maintaining diﬀerent beliefs about the diﬀerent functions f(x; θ) in our ﬂexible
set, here we simply update our beliefs about the behavior of the function f(x) directly.
In this presentation of Bayes’ rule (6.25), the marginalization over all the model
parameters already occurred in our transition from (6.6), the prior over the parameters,
to (6.7), the prior over the preactivations.
The resulting posterior (6.25) is in fact
exactly equivalent to what you’d get by explicitly doing a marginalization over a posterior
12For maximal disambiguation, in this chapter we’ll use sample indices of the form ˙β – the Greek letter
beta with a dot on top – for elements of the dataset B corresponding to input-output pairs for which
outputs values from f(x) are not observed but instead to be inferred.
13The reason we say given the inputs here is that technically we should also be conditioning on xA
and xB as well. In particular, while yA is ﬁxed and z(L)
B
is completely variable in the expression for the
joint prior
p

yA, z(L)
B
H

≡p

yA, z(L)
B
xD, H

,
(6.26)
the full set of inputs xD ≡xA∪xB determines the data-dependent couplings g(L) and v(L) – or equivalently
the metric G(L) and the four-point vertex V (L) – that parameterize the output distribution. We will see
how this works in more detail in the following sections.
163

distribution of the model parameters, e.g. as in (6.13). To see why, consider the following
set of manipulations:
p

z(L)
B
yA, H

=
Z 

P
Y
µ=1
dθµ

p

z(L)
B , θ
yA, H

=
Z 

P
Y
µ=1
dθµ

p

z(L)
B
θ, H

p(θ|yA, H)
=
Z 

P
Y
µ=1
dθµ

p

z(L)
B
θ, H
 p(yA|θ, H) p(θ|H)
p(yA|H)

=
1
p(yA|H)
Z 

P
Y
µ=1
dθµ

p

yA, z(L)
B
θ, H

p(θ|H)
=
1
p(yA|H)
Z 

P
Y
µ=1
dθµ

p

yA, z(L)
B , θ
H

=
p

yA, z(L)
B
H

p

yA
H

.
(6.27)
The only nontrivial step is in the third line, where we reversed the factorization,
p

z(L)
A , z(L)
B
θ, H

= p

z(L)
A
θ, H

p

z(L)
B
θ, H

,
(6.28)
and evaluated at z(L)
A
→yA. This factorization (6.28) says that the network outputs
are conditionally independent, given the parameters. This is a consequence of the fact
that – for a ﬁxed set of network parameters – the output on an example x˜α is entirely
independent from the output evaluated on any other example x ˙β, which is manifestly
true for all the hypotheses that we mentioned. (If it were not, neural networks would
be pretty useless in practice.) The use of Bayes’ rule for the model parameters in the
square brackets in the second line also makes manifest the connection between Bayesian
model ﬁtting (6.12) on the one hand and Bayesian prediction (6.13) on the other hand.
As we already alluded to, this exact method for model marginalization is closely
connected with our eﬀective theory approach to understanding neural networks.
In
particular, while the model parameters are always part of the deﬁnition of our neural
networks, we’ve always had to integrate them out in the process of determining the
distribution over the network outputs. In this way, our eﬀective theory of deep learning
has always worked directly with the entire ensemble of network functions implied by
the initialization distribution of the parameters (6.6) rather than with any particular
network. Up until now, we’ve motivated this ensemble approach via the principle of
typicality, in which we use the ensemble to analyze how a typical realization is likely to
behave.14 Here we have a slightly diﬀerent interpretation: rather than trying to make
the ensemble describe a typical network, we actually want to consider the posterior
predictions across the full set of potential networks, each weighted according to our
posterior beliefs about how plausible those predictions are.
14In §8 and onwards, we’ll see how this principle is manifested in neural networks trained via gradient-
based learning.
164

Now, after a brief detour into Bayesian model comparison, much of the focus of §6.3
and §6.4 will be the explicit evaluation of these Bayesian predictions (6.25) for inﬁnite-
and ﬁnite-width MLPs, respectively.
6.2.2
Bayesian Model Comparison
In the context of Bayesian model ﬁtting and Bayesian prediction, the evidence p
 yA
H

has thus far played essentially no role. In the context of our approximation methods,
MAP and MLE and their respective maximizations (6.19) and (6.20), the value of the
argument maximization is strictly independent of the evidence, since it doesn’t depend
on the model parameters. In the context of our exact method for Bayesian prediction,
the evidence is simply the normalization factor of the posterior, which is trivial for us
to compute.
To actually see the role of the evidence in action, you mustn’t be afraid to dream
a little bigger, darling. That is, rather than being ﬁxated on a single hypothesis H,
we instead consider a multitude of diﬀerent hypotheses Ha as possible explanations for
our data. This is the essence of Bayesian model comparison: using the evidence
to weigh the plausibility of diﬀerent probabilistic models as explanations for all of our
observations. In the context of deep learning, this corresponds to comparing our relative
beliefs in the diﬀerent modeling choices encapsulated in each Ha – i.e. comparing diﬀerent
hyperparameter settings – and determining which modeling choice provides the best
description of our observations yA.
To begin, let us again use Bayes’ rule – this time on the evidence – to invert the
conditioning as
p
 Ha
yA
 = p
 yA
Ha
 p(Ha)
p(yA)
.
(6.29)
In this form, the posterior p
 Ha
yA
 on the left-hand side encodes our updated beliefs in
the plausibility of the diﬀerent hypotheses Ha – the diﬀerent hyperparameters settings –
given our observation yA, while the prior p(Ha) on the right-hand side encodes our initial
beliefs about these hypotheses. Amusingly, the old evidence p
 yA
Ha
 for the hypothesis
Ha from our Bayesian model ﬁtting now appears as the new likelihood p
 yA
Ha
 of the
hypothesis Ha for the observation yA in the context of Bayesian model comparison.
Lastly, the new evidence p(yA) is just a normalization factor that we can safely ignore.15
To see how the model comparison works, let’s use (6.29) to compare two diﬀerent
hypothesis, H1 and H2, in order to determine which is a better ﬁt for our observations.
15Unless, of course, we aren’t afraid to dream even bigger. If we did – narrator: they won’t – we’d need
to introduce a meta hypothesis, G, that encodes our prior beliefs about diﬀerent hyperparameter conﬁg-
urations p(Ha|G). This is sometimes called Bayesian hierarchical modeling. In this case, Bayesian model
comparison in terms of this even grander evidence p(yA) →p(yA|G) in principle involves integrating
overall all the probabilistic models as p(yA|G) = P
a p(yA|Ha) p(Ha|G), i.e. any and all hypotheses Ha
that are encoded by G. The distinction between the meta hypothesis G and hypotheses Ha is somewhat
arbitrary, however; for instance, we could put into G our overall choice of architecture – e.g. MLP, CNN,
transformer – and then let Ha index the diﬀerent settings of Hyperparameters. Then, recursing again, a
Bayesian model comparison over G would be a weighted evaluation of the best architecture for the data,
taking into account all possible settings of the hyperparameters for those architectures.
165

Since our relative beliefs are all that matter, let’s take the ratio of the two posteriors,
p
 H1
yA

p
 H2
yA
 =
"
p
 yA
H1

p
 yA
H2

#
p(H1)
p(H2) ,
(6.30)
from which we see that the irrelevant normalization factor p(yA) simply drops out. Here,
the ratio in the square brackets is sometimes given the name the Bayes’ factor, which
in turn multiplies the ratio of our prior beliefs. In particular, the Bayes’ factor contains
all of the observation dependence and characterizes how we should update our relative
prior beliefs in each hypothesis given the new data yA. A ratio greater than one indicates
that the model speciﬁed by hypothesis H1 is favored, while a ratio less than one indicates
that the model speciﬁed by hypothesis H2 is favored. In this way, the old evidence –
i.e. the new likelihood – p
 yA
Ha
 can be very useful, indeed.
Occam’s razor
In order to further elaborate on the mechanism behind Bayesian model comparison (6.30),
let us pick up Occam’s razor [51], which is the famous principle of sparsity. It says
that we should favor the simplest hypothesis that ﬁts all the observations. In the context
of machine learning and parameterized probabilistic modeling, this principle is often in-
tended as a heuristic that guides us to favor models with fewer parameters, all else being
equal. The intuitive explanation for this heuristic is that models with more parameters
have greater ﬂexibility to ﬁt the observed data, making them more likely to overﬁt and
less likely to generalize to explain new observations.16
Naively, Bayesian model comparison (6.29) seems to give us a very natural way to im-
plement this razor: we can subjectively adjust the ratio of our prior beliefs p(H1)/p(H2)
to explicitly favor the simpler hypothesis, a priori penalizing more complicated models.
However, as MacKay [52] points out:
Coherent [Bayesian] inference embodies Occam’s Razor automatically and quantitatively.
That is, Occam’s razor is objectively built into Bayesian model comparison (6.30) through
the Bayes’ factor.17
16It’s natural to wonder here how to interpret this overﬁtting in light of the fact that we’ve actually in-
tegrated out all our parameters! (In the machine learning literature, such ensembles are sometimes called
non-parametric models, though we really do not like such terminology, given the following explanation.)
The culprit for this potential confusion is the overloaded usage of the word parameter. To illustrate
this with the extreme, let’s consider the inﬁnite-width limit. Despite formally starting with an inﬁnite
number of model parameters – giving a model that is naively very overparameterized, to say the least
– the eﬀective theory of the output distribution is completely characterized by the kernel K(L), which
can be described by a ﬁnite number of data-dependent couplings ∼N 2
D. Thus, from the macroscopic
perspective of Bayesian model comparison, it’s these couplings that control the model complexity and
not what we usually call the parameters, the tunable weights and biases. We will discuss this further and
in greater detail in Epilogue ε, and in particular we’ll highlight how the 1/n expansion for ﬁnite-width
networks leads to a sequence of eﬀective theories with increasing complexity.
17See MacKay’s excellent exposition [52] for further details and examples, with a particular emphasis
on (pre-deep-learning-era) neural networks.
166

To understand why, note that the prior distribution p

z(L)
A
Ha

needs to be normal-
ized. This means that for a given hypothesis Ha to be complicated enough to explain an
overwhelmingly wide variety of potential observations z(L)
A , it must have small support on
any particular observation yA. Hence the evidence p(yA|Ha) for such a hypothesis will
be small regardless of which actual observation we make. In contrast, if the hypothesis
is very simple, the prior p

z(L)
A
Ha

will make a constrained set of predictions, but make
them strongly, by concentrating its support on only a few plausible outcomes. Thus, the
simplest models that still correctly predict the observation yA are naturally preferred by
the Bayes’ factor p(yA|H1)/p(yA|H2) alone. In addition, the more observations we make
that are correctly predicted, the more the Bayes’ factor will amplify this preference for
simpler models that still ﬁt.18
Since the Bayes’ factor automatically and objectively implements Occam’s razor,
there’s no need to subjectively express a preference for simpler models using the prior
over hypotheses p(Ha). This means that for a discrete set of hypothesis {Ha}, we can
choose the prior distribution to be uniform, giving equal a priori preference to any
particular hypothesis Ha regardless of their complexity. With this choice our Bayesian
model comparison is completely characterized by the Bayes’ factor:
p
 H1
yA

p
 H2
yA
 = p
 yA
H1

p
 yA
H2
 .
(6.31)
Thus, we should really think of Occam’s razor as the inductive bias of Bayesian inference
applied to model comparison.
Inductive Bias
Given our last statement, we should clarify about something that we’ve been informally
referring to since §2.1 but now are ﬁnally ready to formally address: inductive bias.
Way back in §2.1, inductive biases were introduced as something implicit that is built
into a neural network architecture in order that the set of functions {f(x; θ)} may better
represent the properties of a particular dataset D and the function approximation task at
hand. From the Bayesian perspective, inductive biases represent the a priori assumptions
made about the desired function f(x) before any observations are made. More broadly,
both hypotheses and learning algorithms may have their own set of inductive biases;
e.g. we’ve just pointed out that Occam’s razor is an inductive bias of Bayesian inference.
Throughout §6.3 and §6.4, we’ll encounter various inductive biases while performing
concrete calculations for inﬁnite- and ﬁnite-width MLPs. Here, let’s consider a very
simple example for illustration: suppose that a Bayesian ﬁrmly believes with absolute
certainty that a statement B is false such that their hypothesis HB assigns an a priori
probability of zero to this belief as p(B|HB) = 0; then, via Bayes’ rule (6.4), there’s
no way that the posterior on B can be updated to be anything other than zero, even
18This is analogous to the way the likelihood factor will dominate the prior as observations accumulate
when Bayesian model ﬁtting.
167

if the Bayesian gathers some new information A that would serve as positive evidence
for B. In this case, HB is clearly a bad hypothesis; its inductive bias is leading to an
absurdly stubborn set of beliefs. Alternatively, if B turns out to be actually false, HB
is a good hypothesis because it can then assign more probability to other plausibly true
statements. As this gedanken inference illustrates, the advantage and disadvantage of
an inductive bias depends on the ground truth.
Returning to our initial example in §2.1 of the inductive bias of diﬀerent neural-
network architectures, the advantage of one architecture over another is a highly data-
and task-dependent question.
In principle, we could use Bayesian model compari-
son (6.30) to directly compare these diﬀerent architectures – MLPs, CNNs, and trans-
formers – for diﬀerent sets of observations yA if only we knew how to compute the evi-
dence p
 yA
H
 for those architectures.19 The formalism of our eﬀective theory of deep
learning as laid out in the earlier chapters is precisely a blueprint for computing such
factors for diﬀerent architectures as a function of a particular dataset. We encourage
you to give it a try.
6.3
Bayesian Inference at Inﬁnite Width
In this section, we’ll give three lessons on Bayesian learning in the inﬁnite-width limit.
First, we’ll calculate the evidence p(yA|H) and see that Bayesian model comparison
prefers criticality for suﬃciently deep networks (§6.3.1). Then, we’ll calculate the pos-
terior distribution for the network outputs p

z(L)
B
yA, H

and see that diﬀerent output
components are completely independent in this limit (§6.3.2). Finally, we’ll calculate
the posterior distribution of preactivations in the penultimate layer p

z(L−1)
D
yA, H

and
show that it’s identical to the penultimate prior distribution p

z(L−1)
D
H

, thus implying
that such inﬁnitely-wide networks lack representation learning (§6.3.3).
Before we begin, let’s start with some reminiscence, recast through the lens of our
new Bayesian glasses. In the inﬁnite-width limit, the prior distribution over the network
outputs is given by a simple zero-mean Gaussian distribution
p

z(L)
D
H

=
1
q
|2πK|nL exp

−1
2
nL
X
i=1
X
δ1,δ2∈D
Kδ1δ2z(L)
i;δ1z(L)
i;δ2

,
(6.32)
with the variance Kδ1δ2 ≡K(L)
δ1δ2 = K(L)(xδ1, xδ2) given by the kernel at the output layer
– here with the layer index dropped – depending explicitly on pairs of inputs xδ1 and
19Recall from §2.1 that CNNs (2.8) are designed to capitalize on the fact that computer vision data
organizes useful information in a spatially-local translationally-invariant manner.
Incorporating this
property into the architecture design is an inductive bias of the CNN; in particular, the assumption
is that a cat is still a cat, even if it’s shifted up up down down left right left right BA. The advan-
tage of such an inductive bias as compared to MLPs should be directly encoded in a Bayes’ factor
p yA
HCNN

/p yA
HMLP

. This ratio should presumably be greater than one for any dataset with
desired outputs yA for which the assumption of spatial locality is a useful inductive bias.
168

xδ2 from the dataset D and implicitly on the Hyperparameters Cb and CW . Also recall
that, as per our general relativistic conventions, the matrix Kδ1δ2 is the inverse of the
covariance matrix Kδ1δ2
X
δ2∈D
Kδ1δ2Kδ2δ3 = δδ1
δ3 ,
(6.33)
where we are entertained by – but also apologize for – the collision of sample indices
δ1, δ2 with the overall Kronecker delta, and further recall that |2πK| is the determinant
of the ND-by-ND matrix (2πK)δ1δ2.
6.3.1
The Evidence for Criticality
As we elaborated on in the last section, the evidence is just the prior distribution for
the network outputs evaluated on the observed true output values yi;˜α given the inputs
xi;˜α in the subsample A:
p(yA|H) =
1
r2πf
K

nL exp

−1
2
nL
X
i=1
X
˜α1,˜α2∈A
f
K ˜α1 ˜α2yi;˜α1yi;˜α2

.
(6.34)
Here we’ve put tildes both on the sample indices ˜α and on the kernel as well, f
K˜α1 ˜α2,
in order to indicate that it’s an NA-by-NA submatrix built from the pairs of inputs
(x˜α1, x˜α2) in the subsample A of size NA.
Importantly, this means that the inverse
f
K ˜α1 ˜α2 is taken with respect to the samples in the set A only,
X
˜α2∈A
f
K ˜α1 ˜α2 f
K˜α2 ˜α3 = δ ˜α1
˜α3 ,
(6.35)
and in particular that f
K ˜α1 ˜α2 ̸= K ˜α1 ˜α2.
In other words, f
K ˜α1 ˜α2 is not the same as
the inverse of the kernel Kδ1δ2 on the whole dataset D (6.33) evaluated on the sample
indices δ1 = ˜α1 and δ2 = ˜α2; if you’d like, ﬂip ahead and cf. (6.53). Accordingly, the
determinant
2πf
K
 is also computed from this NA-by-NA submatrix. The usefulness
of this notation and the essentialness of this distinction will become clearer when we
consider the posterior in §6.3.2.
Before we analyze the evidence (6.34) in detail, we should establish our space of
hypotheses.
Considering MLP architectures in the inﬁnite-width limit, there’s only
three hyperparameters of relevance, the bias variance and rescaled weight variance Cb
and CW , and the depth L. In principle, each combination of these three hyperparameters
is a diﬀerent hypothesis. However, in the asymptotic limit of large depth L ≫1, we
know from our discussion in §3.2 and our analysis in §5 that generically the kernel
recursion will either exponentially lead to a trivial ﬁxed point at zero K⋆= 0 or at
inﬁnity K⋆= ∞, or slowly approach a nontrivial ﬁxed point at criticality.20 Thus, for
20Yes, we know, for some activation functions there exist hyperparameter settings that lead to trivial
ﬁxed points at nonzero values of the kernel K⋆̸= 0. We’ll eventually consider – and make a case against
– such hypotheses as well, though only in a future footnote, 23, and only after ﬁrst considering the details
of the two-input evidence.
169

deep networks Bayesian model comparison essentially reduces to the comparison of three
diﬀerent hypotheses, H0, H∞and Hcritical, corresponding to the two trivial ﬁxed points
and the one nontrivial ﬁxed point, respectively.
Having established our space of hypotheses, let’s ﬁrst see how Bayesian model com-
parison works when we have only a single input x. In this case the kernel is just a scalar,
and the evidence is simply given by
p(y|H) =
1

2πf
K
 nL
2
exp
 
−1
2f
K
nL
X
i=1
y2
i
!
.
(6.36)
Here, the output norm PnL
i=1 y2
i is ﬁxed by a given function approximation task.21 Thus
all the dependence on the hyperparameters H is encoded in a single number: f
K.
Let’s start with H∞, for which f
K →∞. In this case, the argument of the exponential
in (6.36) vanishes and thus the exponential evaluates to unity, while the normalization
factor in front vanishes. Therefore, the evidence will vanish polynomially:
p(y|H∞) = lim
e
K→∞
1

2πf
K
 nL
2
= 0 .
(6.37)
In fact, in this limit the output distribution becomes an (unnormalizable) uniform dis-
tribution over all possible output norms. Next, let’s consider H0 with f
K →0. In this
case, while the normalization factor grows polynomially, the argument in the exponent
approaches negative inﬁnity. Thus, the evidence approaches zero exponentially quickly:
p(y|H0) = lim
e
K→0
exp
"
−1
2f
K
nL
X
i=1
y2
i + O

log f
K
#
= 0 .
(6.38)
Indeed, recalling (2.30), the evidence (6.36) in this limit becomes a Dirac delta function,
p(y|H0) =
nL
Y
i=1
δ(yi) ,
(6.39)
which is a fairly useless hypothesis unless all of the true outputs are the zero vector.
Therefore, for generic nonzero and ﬁnite output values, the maximal evidence should lie
between these two extrema. Speciﬁcally, seen as a function of f
K, the evidence (6.36)
peaks at f
K = f
K(L)(x, x) ≡PnL
i=1 y2
i /nL. Our remaining hypothesis, criticality Hcritical,
comes the closest to realizing this maximum.
To reiterate, for a single input we just need the kernel f
K to be of order one. For
deep neural networks, this is precisely the condition that we imposed in order to avoid
the exploding and vanishing kernel problem for a single input, which we satisﬁed with
the parallel susceptibility condition χ∥(K⋆) = 1. Physically, the exploding kernel gives
21Many common datasets for classiﬁcation tasks employ “one-hot” true outputs in which all but one
component yi of a particular output are zero, and the remaining single component – corresponding to
the correct class – is equal to one. For such datasets, the output norm is trivial PnL
i=1 y2
i = 1.
170

a very ﬂat distribution spread over a big range of output norms, yielding insubstantial
evidence for any particular output norm; the vanishing kernel gives sharp support for
the zero norm (6.39) and no support anywhere else. Clearly the Bayes’ factor (6.31) will
prefer any hypothesis that gives more focused support over reasonable output norms.
In the language of our Occam’s razor discussion, H∞is too complex, predicting every
possible norm, while H0 is too simple, predicting only one particular norm. The only
hypothesis that gives a ﬁnite and nonzero f
K in the deep asymptotic regime is Hcritical,
whereat the initialization hyperparameters are tuned to satisfy χ∥(K⋆) = 1.22
Now that we see how this works, let’s extend our analysis of the evidence to two
inputs, with ˜α = ±.
Intuitively, we expect to ﬁnd the perpendicular susceptibility
condition χ⊥(K⋆) = 1 and thus demonstrate a conclusive preference for the criticality
hypothesis Hcritical. To rediscover χ⊥(K⋆) = 1, it will be suﬃcient to consider the case
where both inputs have the same norm
n0
X
i=1
x2
i;+ =
n0
X
i=1
x2
i;−.
(6.40)
Then, recalling our decomposition into the γ[a]
˜α1 ˜α2 basis (5.15), we can write the kernel
as
f
K˜α1 ˜α2 =
 f
K[0] + f
K[2]
f
K[0] −f
K[2]
f
K[0] −f
K[2]
f
K[0] + f
K[2]
!
,
(6.41)
where we’ve used the fact that f
K[1] = 0 when both inputs have the same norm (6.40).
In this basis, the determinant is given by
2πf
K
 = 16π2f
K[0]f
K[2], and the inverse of
the kernel is given by
f
K ˜α1 ˜α2 =
1
4f
K[0]f
K[2]
 f
K[0] + f
K[2]
−f
K[0] + f
K[2]
−f
K[0] + f
K[2]
f
K[0] + f
K[2]
!
,
(6.42)
which in turn lets us evaluate the argument of the exponential in (6.34) as
nL
X
i=1
X
˜α1,˜α2=±
f
K ˜α1 ˜α2yi;˜α1yi;˜α2 =
nL
X
i=1
1
4f
K[0]f
K[2]
h
f
K[2] (yi;+ + yi;−)2 + f
K[0] (yi;+ −yi;−)2i
= Y[0]
f
K[0]
+ Y[2]
f
K[2]
,
(6.43)
where in the last equality we introduced the components
Y[0] ≡
nL
X
i
yi;+ + yi;−
2
2
,
Y[2] ≡
nL
X
i
yi;+ −yi;−
2
2
.
(6.44)
22N.B. polynomially vanishing kernels give ﬁnite evidence for all practical depths. To be very pedantic
about this, for such kernels – for instance, for the tanh – for absurdly deep networks the truly Bayesian-
optimal CW would be ever so slightly above its critical value.
171

All together, this gives a simple expression for the two-input evidence,
p(y+, y−|H) =

16π2f
K[0]f
K[2]
−nL
2 exp
 
−Y[0]
2f
K[0]
−Y[2]
2f
K[2]
!
(6.45)
=
"
4πf
K[0]
−nL
2 exp
 
−Y[0]
2f
K[0]
!#
×
"
4πf
K[2]
−nL
2 exp
 
−Y[2]
2f
K[2]
!#
.
Now, let’s consider a generic pair of input-output pairs (x+, y+) and (x−, y−) for
which both the average and the diﬀerence of the true outputs, Y[0] and Y[2] (6.44), are
nonzero and of order one. Then, running the same argument as we did for the single-
input evidence, we prefer a hypothesis that comes as close as possible to having both
f
K[0] ≈Y[0]/nL = O(1) – from maximizing the object in the ﬁrst square brackets of (6.45)
– and f
K[2] ≈Y[2]/nL = O(1) – from maximizing the object in the second square brack-
ets of (6.45). And, as we learned in §5, to keep both f
K[0] and f
K[2] of order one, we
need to set both the critical parallel susceptibility condition χ∥(K⋆) = 1 and the critical
perpendicular susceptibility condition χ⊥(K⋆) = 1.23 Therefore, with this evidence for
criticality, Bayesian model comparison demonstrates a full preference for Hcritical.24
23Finally, let’s consider the trivial ﬁxed points with nonzero kernel values K⋆̸= 0. (This can occur,
e.g., for the K⋆= 0 universality class, for which there exists ﬁxed points K⋆that have χ⊥(K⋆) = 1 but
χ∥(K⋆) < 1.)For this analysis, we need to relax the same-norm condition (6.40) and consider the most
general form of the two-input kernel. Projecting the kernel into the γ[a]
˜α1 ˜α2 basis (5.15) as
e
K˜α1 ˜α2 =
 e
K[0] + e
K[1] + e
K[2]
e
K[0] −e
K[2]
e
K[0] −e
K[2]
e
K[0] −e
K[1] + e
K[2]

,
(6.46)
we can similarly use (5.20) to decompose the output matrix, Y˜α1 ˜α2 ≡PnL
i=1 yi;˜α1yi;˜α2, into components
Y[0] =
X
i
yi;+ + yi;−
2
2
,
Y[1] =
P
i y2
i;+ −P
i y2
i;−
2
,
Y[2] =
X
i
yi;+ −yi;−
2
2
.
(6.47)
Then, a quick calculations shows that the evidence evaluates to
p(y+, y−|H) =
h
4π2(4 e
K[0] e
K[2] −e
K2
[1])
i−nL
2 exp
"
−(4 e
K[0]Y[2] + 4 e
K[2]Y[0] −2 e
K[1]Y[1])
2(4 e
K[0] e
K[2] −e
K2
[1])
#
.
(6.48)
Now, we see from this expression that a hypothesis with e
K[1]Y[1] > 0 has improved evidence compared to
the one with non-positive e
K[1]Y[1]. In particular, if a ﬁxed point is trivial then the parallel perturbation
e
K[1] always vanishes exponentially, even if the ﬁxed-point value of the kernel is non-vanishing K⋆̸= 0.
Thus, such a hypothesis will be disfavored compared to Hcritical, completing our argument.
It should be noted that for this distinction to matter, we must have a nonzero Y[1], meaning P
i y2
i;+ ̸=
P
i y2
i;−. For networks used as generic function approximators – or for tasks where the network outputs
are general and used downstream for other tasks – this may matter. For deep-learning tasks where all
the true outputs have the same norm, this may not matter.
24Technically, what we’ve shown here is a preference for criticality in the Bayesian prior distribution.
In §9.4, we’ll also ﬁnd a natural preference for criticality in the initialization distribution, by showing
that such a tuning is necessary for controlling the exploding and vanishing gradient problem that arises
with gradient-based learning.
172

Programming note: since conditioning on H is so deeply ingrained in our minds by now,
for notational simplicity we’ll re-start the suppression of this conditioning from here on
out.
6.3.2
Let’s Not Wire Together
Now, let’s work out the full posterior distribution (6.25) at inﬁnite width.25
As we
already have an expression for the evidence p(yA) (6.34) in the denominator, let’s focus
on the joint distribution p

yA, z(L)
B

in the numerator. Recall also that to discuss the
posterior we need to partition the data into two subsamples, D ≡A ∪B, one for which
we have observed the true output values yA and the other for which we are going to infer
the output values.
With such a data partitioning in mind, we can write out the joint distribution as
p

yA, z(L)
B

(6.49)
=
1
q
|2πK|nL exp
"
−1
2
nL
X
i=1
 
X
˜α1,˜α2∈A
K ˜α1 ˜α2yi;˜α1yi;˜α2 +
X
˜α1∈A, ˙β2∈B
K ˜α1 ˙β2yi;˜α1z(L)
i; ˙β2
+
X
˙β1∈B,˜α2∈A
K
˙β1 ˜α2z(L)
i; ˙β1yi;˜α2 +
X
˙β1, ˙β2∈B
K
˙β1 ˙β2z(L)
i; ˙β1z(L)
i; ˙β2
!#
,
where K ˜α1 ˜α2, K ˜α1 ˙β2, K ˙β1 ˜α2, and K ˙β1 ˙β2 are the blocks of
Kδ1δ2 ≡
 
K ˜α1 ˜α2
K ˜α1 ˙β2
K ˙β1 ˜α2
K ˙β1 ˙β2
!
,
(6.50)
which is the inverse of the whole ND-by-ND kernel matrix,
Kδ1δ2 =
 f
K˜α1 ˜α2
K˜α1 ˙β2
K ˙β1 ˜α2
K ˙β1 ˙β2
!
.
(6.51)
To make progress, we need to relate the submatrices in the inverse (6.50) to the subma-
trices in the kernel decomposition (6.51), since, recalling
Kδ1δ2 ≡1
nL
nL
X
i
E
h
z(L)
i
(xδ1) z(L)
i
(xδ2)
i
+ O
 1
n

,
(6.52)
it’s these blocks that are naturally deﬁned in terms of the data.26
25The form of this distribution was ﬁrst worked out by Williams in [53] for one-hidden-layer networks.
26As we explained before, the over-tilde on e
K˜α1 ˜α2 indicates that it’s a submatrix of the kernel evaluated
on samples in the set A, only. The inverse of that block was deﬁned explicitly in (6.35) and is symbolized
as e
K ˜α1 ˜α2.
Also note that the symmetry of the full kernel, Kδ1δ2 = Kδ2δ1, endows a similar set of
symmetries on the submatrices: e
K˜α1 ˜α2 = e
K˜α2 ˜α1, K ˙β1 ˙β2 = K ˙β2 ˙β1, and K ˙β ˜α = K˜α ˙β.
173

Explicitly inverting Kδ1δ2 according to the inverse formula (6.33), we ﬁnd that the
submatrices of (6.50) can be deﬁned in terms of the blocks of the kernel (6.51) and the
inverse submatrix f
K ˜α1 ˜α2 on A as
K ˜α1 ˜α2 ≡f
K ˜α1 ˜α2 +
X
˜α3,˜α4∈A
X
˙β3, ˙β4∈B
f
K ˜α1 ˜α3K˜α3 ˙β3K
˙β3 ˙β4K ˙β4 ˜α4 f
K ˜α4 ˜α2 ,
(6.53)
K ˜α1 ˙β2 ≡−
X
˜α3∈A
X
˙β3∈B
f
K ˜α1 ˜α3K˜α3 ˙β3K
˙β3 ˙β2 ,
(6.54)
K
˙β1 ˜α2 ≡−
X
˜α3∈A
X
˙β3∈B
K
˙β1 ˙β3K ˙β3 ˜α3 f
K ˜α3 ˜α2 ,
(6.55)
K
˙β1 ˙β2 ≡K
˙β1 ˙β2 ,
(6.56)
where we’ve had to introduce (and name a posteori) the posterior covariance,
K ˙β1 ˙β2 ≡K ˙β1 ˙β2 −
X
˜α3,˜α4∈A
K ˙β1 ˜α3 f
K ˜α3 ˜α4K˜α4 ˙β2 .
(6.57)
The expression for (6.56) is deﬁned implicitly by taking the inverse of (6.57):
X
˙β2∈B
K
˙β1 ˙β2 K ˙β2 ˙β3 = δ
˙β1
˙β3 .
(6.58)
Since these are essential relations, let us check all the components of the inverse
formula (6.33), one by one. Firstly, considering the δδ1
δ3 →δ ˜α1
˜α3 component, we see
X
δ2∈D
K ˜α1δ2Kδ2 ˜α3 =
X
˜α2∈A
K ˜α1 ˜α2K˜α2 ˜α3 +
X
˙β2∈B
K ˜α1 ˙β2K ˙β2 ˜α3
=
X
˜α2∈A
f
K ˜α1 ˜α2 f
K˜α2 ˜α3 = δ ˜α1
˜α3 ,
(6.59)
where in the ﬁrst line we decomposed the sum over δ2 ∈D into separate sums over
˜α2 ∈A and over ˙β2 ∈B according to our partitioning D = A ∪B, then in going to
the second line we plugged in our expressions for the inverse blocks (6.53) and (6.54),
and ﬁnally in the last step we used the fact that f
K ˜α1 ˜α2 is the inverse of the submatrix
f
K˜α1 ˜α2 (6.35). Secondly, considering the δδ1
δ3 →δ
˙β1
˙β3 component, we see
X
δ2∈D
K
˙β1δ2Kδ2 ˙β3 =
X
˜α2∈A
K
˙β1 ˜α2K˜α2 ˙β3 +
X
˙β2∈B
K
˙β1 ˙β2K ˙β2 ˙β3
(6.60)
=
X
˙β2∈B
K
˙β1 ˙β2

K ˙β2 ˙β3 −
X
˜α3,˜α2∈A
K ˙β2 ˜α3 f
K ˜α3 ˜α2K˜α2 ˙β3

=
X
˙β2∈B
K
˙β1 ˙β2K ˙β2 ˙β3 = δ
˙β1
˙β3 ,
where as before in the ﬁrst line we decomposed the sum over δ2 ∈D into separate sums
over ˜α2 ∈A and over ˙β2 ∈B according to our partitioning D = A ∪B, then in going to
174

the second line we plugged in our expressions for the inverse blocks (6.55) and (6.56),
and ﬁnally, identifying the expression in the parenthesis as the deﬁnition of the posterior
covariance K ˙β1 ˙β2 (6.57), we get the ﬁnal result since K ˙β1 ˙β2 is the inverse of the posterior
covariance (6.58). Lastly, we consider the oﬀ-diagonal block:
X
δ2∈D
K ˜α1δ2Kδ2 ˙β3 =
X
˜α2∈A
K ˜α1 ˜α2K˜α2 ˙β3 +
X
˙β2∈B
K ˜α1 ˙β2K ˙β2 ˙β3
(6.61)
=
X
˜α2∈A, ˙β2∈B
f
K ˜α1 ˜α2K˜α2 ˙β2

δ
˙β2
˙β3 +
X
˜α3,˜α4∈A
X
˙β4∈B
K
˙β2 ˙β4K ˙β4 ˜α4 f
K ˜α4 ˜α3K˜α3 ˙β3 −
X
˙β4∈B
K
˙β2 ˙β4K ˙β4 ˙β3


=
X
˜α2∈A, ˙β2∈B
f
K ˜α1 ˜α2K˜α2 ˙β2

δ
˙β2
˙β3 −
X
˙β4∈B
K
˙β2 ˙β4K ˙β4 ˙β3

= 0 ,
Here, we follow the same pattern as before, (i) decomposing the sum according to the
partitioning D = A ∪B, (ii) plugging in expressions for inverse blocks (6.53) and (6.54),
and (iii) using the posterior covariance (6.57) and the inverse equation (6.58). Every-
thing checks out.
Now that we have some conﬁdence in our inversions, let’s plug our expressions for
these submatrices (6.53)–(6.56) into the joint prior (6.49). Since the posterior (6.25) is
only a function of the outputs z(L)
B , we can make things easier by limiting our focus to
the z(L)
B
dependence only, ignoring the yA terms independent of z(L)
B
and ignoring the
normalization factor:
p

yA, z(L)
B

∝exp
"
−1
2
nL
X
i=1
X
˙β1, ˙β2∈B
K
˙β1 ˙β2z(L)
i; ˙β1z(L)
i; ˙β2
(6.62)
+
nL
X
i=1
X
˙β1∈B,˜α1∈A
z(L)
i; ˙β1


X
˜α2∈A, ˙β2∈B
K
˙β1 ˙β2K ˙β2 ˜α2 f
K ˜α2 ˜α1

yi;˜α1
#
.
At this point you know what to do: completing the square – as should be your sec-
ond nature by now – and ignoring the new z(L)
B -independent additive constant in the
exponential, you get
p

yA, z(L)
B

∝exp
"
−1
2
nL
X
i=1
X
˙β1, ˙β2∈B
K
˙β1 ˙β2
 
z(L)
i; ˙β1 −
X
˜α3,˜α4∈A
K ˙β1 ˜α3 f
K ˜α3 ˜α4yi;˜α4
!
(6.63)
×
 
z(L)
i; ˙β2 −
X
˜α5,˜α6∈A
K ˙β2 ˜α5 f
K ˜α5 ˜α6yi;˜α6
!#
.
This distribution (6.63) is still Gaussian, with a variance given by the posterior covari-
ance K ˙β1 ˙β2 and a nonzero posterior mean:
m∞
i; ˙β ≡
X
˜α1,˜α2∈A
K ˙β ˜α1 f
K ˜α1 ˜α2yi;˜α2 .
(6.64)
175

Here, the superscript ∞is used to remind us that we’re in the inﬁnite-width limit.
Finally, we realize that the posterior distribution (6.25) is proportional to the joint
prior (6.63),
p

z(L)
B
yA

∝p

yA, z(L)
B

,
(6.65)
and that the posterior distribution is automatically normalized (6.5) as a function of
the variable z(L)
B . Thus, computing the normalization factor for (6.63) – or really just
writing it down, since at this point you know by heart how to normalize any Gaussian
distribution – we get the posterior at inﬁnite width:
p

z(L)
B
yA

=
1
q
|2πK|nL exp

−1
2
nL
X
i=1
X
˙β1, ˙β2∈B
K
˙β1 ˙β2 
z(L)
i; ˙β1 −m∞
i; ˙β1
 
z(L)
i; ˙β2 −m∞
i; ˙β2


.
(6.66)
The posterior mean m∞
i; ˙β represents our updated belief about the expected network
output for the input xj; ˙β ∈B after incorporating information about the true outputs yA
for all the inputs xj;˜α ∈A; as such, it is explicitly a function of the true input-output
pairs xA and yA in the subsample A, as we see in (6.64). Importantly, our expected
predictions were a priori zero – indicating an inductive bias towards vanishing outputs
on average – and now a posteriori our predictions are shifted to something nonzero.
Such a nonzero posterior mean is a signature that learning is (ﬁnally!) happening. In
addition, the posterior covariance K ˙β1 ˙β2 encodes the conﬁdence interval: the smaller the
covariance is, the more sharply peaked the posterior is around its mean, and the more
conﬁdent the model is about its predictions.
Practically speaking, note that in order to compute the mean prediction m∞
i; ˙β1 ac-
cording to its deﬁnition (6.64), we’d in principle need to invert – and then represent –
the NA-by-NA submatrix f
K˜α1 ˜α2. As the size of our observations NA grows, the compu-
tational cost of such an inversion grows very fast.27 This hidden catch is why – though
theoretically quite elegant – (at least any naive implementation of) Bayesian learning is
not practical for large datasets. Instead, for this reason we will essentially need to rely
on approximation methods for model ﬁtting, such as MLE (6.20). We’ll comment more
on this next chapter (§7).
Theoretically and practically speaking, there is another serious issue with the inﬁnite-
width posterior mean. Looking at its expression (6.64), we see that the mean prediction
on the output component i is entirely independent from the observations yj;α that we
made on the other components with j ̸= i. Thus, our updated best estimate of these
diﬀerent output components are entirely uncorrelated, though in principle observations
of diﬀerent components j may contain very useful information about a given component
27For instance, the computational cost of Gauss-Jordan elimination scales as ∼N 3
A and requires us
to represent the NA × NA-dimensional inverse in memory. Things can be improved a bit by realizing
that to compute the posterior mean we only really require the matrix-vector product of the inverse with
the observations: P
˜α2∈A e
K ˜α1 ˜α2yi;˜α2. However, such an improvement is still not really suﬃcient for
Bayesian learning to compete practically with gradient-based learning for large datasets A.
176

i.28 In fact, we see from (6.66) that the posterior distribution actually factorizes as
p

z(L)
i;B , z(L)
j;B
yi;A, yj;A

= p

z(L)
i;B
yi;A

p

z(L)
j;B
yj;A

,
(i ̸= j) ,
(6.67)
meaning that the diﬀerent output components are entirely statistically independent.29
We can trace this independence back to a similar property of the inﬁnite-width prior
distribution
p

z(L)
i;A, z(L)
j;A

= p

z(L)
i;A

p

z(L)
j;A

,
(i ̸= j),
(6.68)
a property that we’ve recognized for a while now, see e.g. (5.106). Thus, with Bayesian
learning output features do not wire together: recalling our discussion of inductive bias
before (§6.2.2), we see that the prior endows on the posterior an absurdly stubborn
set of beliefs, namely that the components of the output are completely independent
with absolute certainty. Such an inductive bias is incurable by any amount of learning,
irregardless of how large the set of observations A are; the inductive bias of this prior
can never be overwhelmed in the inﬁnite width limit.
Luckily, this state of aﬃars is completely curable – for both learning algorithms,
Bayesian learning and gradient-based learning – by backing oﬀof the inﬁnite-width
limit and working with ﬁnite-width networks . . . the actual kind of networks that are
used in practice.
6.3.3
Absence of Representation Learning
Considering the independence of the diﬀerent components of the output in the posterior,
a natural follow-up question is whether or not Bayesian learning at inﬁnite width enables
representation learning. Here, we will show decisively that it does not.
As a representative avatar of this question, let’s compute the posterior distribution
of preactivations in the penultimate layer ℓ= L −1 on the full set of samples D, given
observations yA:
p

z(L−1)
D
yA

=
p

yA
z(L−1)
D

p

z(L−1)
D

p(yA)
.
(6.69)
This is an application of Bayes’ rule (6.4), following from applying the product rule (6.1)
to the joint distribution p

yA, z(L−1)
D

between the observations yA and the penultimate
preactivations z(L−1)
D
. Here, the likelihood p

yA
z(L−1)
D

is the conditional distribution
p

z(L)
A
z(L−1)
D

evaluated on our set of observations z(L)
A
→yA.
28The concept of knowledge distillation [54] is predicated on this principle of correlations among the
output components. For example, if a network is trying to classify images of hand-written digits, a
certain example of a “2” may be more “7”-like or more “3”-like. Such feature information is quite useful,
especially if the output of the network is used downstream for some other task.
29To be FAIR, the issue is with the inﬁnite-width limit itself, as diﬀerent output components are also
decorrelated for inﬁnite-width networks trained with gradient-based learning (§10).
177

We already know the form of this conditional distribution, as it is the same ob-
ject (4.69) that we needed in order to work out the layer-to-layer RG ﬂow of the preactiva-
tions. In general, this distribution involves the stochastic metric bG(L)
˜α1 ˜α2 = bG(L)
˜α1 ˜α2

z(L−1)
D

.
However, in the inﬁnite-width limit the metric is entirely deterministic bG(L)
˜α1 ˜α2 →G(L)
˜α1 ˜α2,
with no dependence at all on the penultimate-layer preactivations z(L−1)
D
. Thus, the
likelihood at inﬁnite width – swapping the deterministic metric for the kernel – is given
by
p

yA
z(L−1)
D

=
1
r2πf
K(L)

nL exp

−1
2
nL
X
i=1
X
˜α1,˜α2∈A
f
K ˜α1 ˜α2
(L) yi;˜α1yi;˜α2

= p(yA) ,
(6.70)
and our expression for the posterior of the penultimate layer (6.69) reduces to the prior:
p

z(L−1)
D
yA

= p

z(L−1)
D

.
(6.71)
Since the posterior equals the prior, our observation of yA had no consequence on the
penultimate-layer representation; thus, we conclude that there is no representation learn-
ing at inﬁnite width.
This lack of representation learning stems from the lack of interlayer correlation
in the joint distribution p

z(ℓ)
D , z(ℓ+1)
D

at inﬁnite width, and thus it persists for all
hidden layers with ℓ< L.
This is another bad inductive bias of the inﬁnite-width
hypotheses: regardless of the set of observations yA that we make, there’s no amount of
new information that will allow the network to update its representations in the hidden
layers ℓ< L.
This state of aﬀairs is somewhat tragic as the whole point of having many layers –
in fact, the main motivation given for deep learning on the whole – is the learning of
complex representations in those hidden layers. As we will see next, we can solve this
lack of representation learning – as well as the lack of wiring together in the output – by
backing oﬀthe inﬁnite-width limit and looking at ﬁnite-width eﬀects.30
6.4
Bayesian Inference at Finite Width
In this section, we’ll give three lessons on Bayesian learning at ﬁnite width. To begin,
we’ll show that ﬁnite-width neural networks are automatically endowed with an inductive
bias for neural association due to non-Gaussian interactions between neurons, leading
to a natural predisposition towards Hebbian learning (§6.4.1). With that in mind, we’ll
in turn demonstrate how such learning works by ﬁrst calculating the mean of the pos-
terior distribution for the network outputs p

z(L)
B
yA

– showing how intralayer neural
30In §10, will also show the same lack of representation learning occurs for the ensemble of inﬁnite-
width networks that are (theoretically) trained with gradient-based learning. This issue is also resolved
(practically) in §11 by going to ﬁnite width.
178

interactions in the prior give rise to nontrivial correlations among the components of
the output (§6.4.2) – and then calculating the posterior distribution of preactivations in
the penultimate layer p

z(L−1)
B
yA

– showing how interlayer interactions give rise to a
nonzero shift between prior and posterior, thus signaling the presence of representation
learning at ﬁnite width (§6.4.3).
6.4.1
Hebbian Learning, Inc.
In this subsection, we’ll see that ﬁnite-width neural networks have an inductive bias that
facilitates neural association. To explain Hebbian learning, let’s begin ﬁrst with a few
words from our honorary guest speaker, Donald Hebb:
The general idea is an old one, that any two cells or systems of cells that are
repeatedly active at the same time will tend to become “associated,” so that activity in
one facilitates activity in the other.
Donald Hebb, in his 1949 classic The Organization of Behavior [55].
(Applause.)
Thank you very much.
Donald Hebb, apocryphal.
While Hebb was originally thinking about biological neurons, Hebbian learning has be-
come a popular guiding principle for systems of artiﬁcial neurons as well. We’ve actually
already seen this inductive bias for neural association any of the numerous times we’ve
discussed the presence of neural interactions in the ﬁnite-width prior distribution. To
make this manifest, we’re now going to explicitly determine the neural inﬂuence of one
preactivation on another in our eﬀective preactivation distribution at initialization.
Concretely, let’s suppose that a single input x is fed into a network, and we’ve checked
that at layer ℓthe value of the ﬁrst preactivation z(ℓ)
1
= ˇz(ℓ)
1
is larger than typical; given
this atypical value ˇz(ℓ)
1 , we can then ask whether the second preactivation z(ℓ)
2
is likely
to be atypically large. This kind of neural association or inﬂuence is encoded in the
conditional distribution
p

z(ℓ)
2
ˇz(ℓ)
1

=
p

ˇz(ℓ)
1 , z(ℓ)
2

p

ˇz(ℓ)
1

.
(6.72)
Note that at inﬁnite width p

z(ℓ)
2
ˇz(ℓ)
1

= p

z(ℓ)
2

due to the factorization of the prior
on neurons (5.106), and so we see right away that there is a complete absence of neural
association in such a limit.
To compute this association for ﬁnite-width networks, recall from §4.4 the action
179

representation (4.97) for a distribution over m neurons
p(z1, . . . , zm) ∝exp

−gm
2
m
X
i=1
z2
i + v
8
m
X
i,j=1
z2
i z2
j

,
(6.73)
where we have temporarily dropped layer indices from the variables and couplings. Here,
the quadratic coupling gm is given implicitly by the expression (4.102),
1
gm
= G(ℓ) −(m + 2)
2nℓ−1
V (ℓ)
G(ℓ) + O
 1
n2

,
(6.74)
and we have emphasized the dependence of the coupling on m; similarly, the quartic
coupling is given by (4.103),
v =
1
nℓ−1
V (ℓ)
 G(ℓ)4 + O
 1
n2

,
(6.75)
which is independent of m to this order in 1/n.
Evaluating the action representa-
tion (6.73) on m = 1 and m = 2 neurons and plugging the resulting distributions into
our expression for the conditional distribution (6.72), we get
p(z2|ˇz1) ∝exp

−g2
2 z2
2 + v
8

z4
2 + 2z2
2ˇz2
1

,
(6.76)
where, similar to the last section, for such a conditional distribution we only need to
keep track of the terms in the action that depend on z2.
Now that we have a conditional distribution, let’s evaluate some conditional expec-
tations. Since this distribution is manifestly even in z2, i.e. invariant under a sign ﬂip
z2 ↔−z2, all the odd-point correlators vanish, including the conditional mean. This
means that the ﬁrst nontrivial observable is the two-point correlator or conditional vari-
ance:
Z
dz2 p(z2|ˇz1) z2
2 =
R dz2 exp
−g2
2 z2
2 + v
8
 z4
2 + 2z2
2ˇz2
1
 z2
2
R dz2 exp
−g2
2 z2
2 + v
8
 z4
2 + 2z2
2ˇz2
1

(6.77)
=
R dz2 e−
g2z2
2
2
z2
2 + v
8
 z6
2 + 2z4
2ˇz2
1
 + O
 v2
R dz2 e−
g2z2
2
2
1 + v
8
 z4
2 + 2z2
2ˇz2
1
 + O(v2)

=
g−1
2
+ v
8

15g−3
2
+ 6g−2
2 ˇz2
1

1 + v
8

3g−2
2
+ 2g−1
2 ˇz2
1

+ O

v2
=g−1
2
+ v
2g−2
2

3g−1
2
+ ˇz2
1

+ O

v2
.
Above, on the ﬁrst line we used (6.76) in the numerator and at the same time com-
puted its normalization in the denominator, on the second line we expanded both the
180

numerator and denominator in v, on the third line we computed the single-variable
Gaussian integrals, and on the ﬁnal line we expanded the denominator in v. Plugging
in our expressions for the quadratic coupling (6.74) and the quartic coupling (6.75) and
reimplementing layer indices, we ﬁnd
Z
dz(ℓ)
2
p

z(ℓ)
2
ˇz(ℓ)
1
 
z(ℓ)
2
2 = G(ℓ)+ 1
2

ˇz(ℓ)
1
2 −G(ℓ)
 "
V (ℓ)
nℓ−1
 G(ℓ)2
#
+O
 1
n2

. (6.78)
In passing, note for later that this result holds for any distinct pair of neurons by
replacing neural indices as 1, 2 →i1, i2, with i1 ̸= i2.
This conditional variance (6.78) embodies some really interesting physics.
If the
observed value

ˇz(ℓ)
1
2 is larger/smaller than its expected value E

z(ℓ)
1
2
= G(ℓ), then
the variance of z(ℓ)
2
will itself be larger/smaller than is typical.
Thus, z(ℓ)
1
and z(ℓ)
2
correlate their atypical ﬁring.31 This eﬀect is proportional to the normalized four-point
vertex in the second square brackets of (6.78), which as we know from (5.128) and (5.129)
is proportional to ℓ/n across our universality classes when at criticality. In other words,
deeper layers have an inductive bias to build more neural associations. Moreover, the
presence of these associations is mediated by the interactions in the eﬀective action
induced at ﬁnite width only. As we will soon show, nontrivial representation learning is
a direct descendant of such associations.
Note that this result should be interpreted as a propensity for atypicality rather
than a guarantee. Since the conditional variance (6.78) applies to any pair of neurons,
conditioned on a particular neuron i∗having a larger/smaller norm than expected, then
all of the other neurons with i ̸= i∗are more likely to have a larger/smaller norm, though
not all will. In a given realization of a network in practice, the ones that happen to have
a larger/smaller norm are the ones that are more likely to develop a correlation with i∗
as learning progresses.
Hebbian learning is often summarized by the following slogan: neurons that ﬁre
together, wire together.
What we see here is that conditioned on an atypical ﬁring
ˇz1, another preactivation, e.g. z2, is much more likely to have an atypical ﬁring itself.
This propensity of ﬁnite-width networks to ﬁre together is an inductive bias of our
prior beliefs before Bayesian learning as well as of our initialization distribution before
gradient-based learning. To understand the wire together part, let’s now consider the
Bayesian posterior.32
31You may or may not recall from footnote 8 in §1.2 that having a nontrivial connected four-point
correlator serves as a measure of the potential for outliers. In statistics, for single-variable distributions
this is called the excess kurtosis; here, we see a multi-neuron generalization (which apparently can be
called the cokurtosis). In particular, observing an outlying value z(ℓ)
1
= ˇz(ℓ)
1
implies that we are more
likely to see outlying values for z(ℓ)
2
as well. At the end of Appendix A, we’ll provide an information-
theoretic reformulation of this phenomenon that will also shed further light on how deep a network
should be in order to best take advantage of it.
32For some models of artiﬁcial neurons – such as the Hopﬁeld network – Hebbian learning is often
added in by hand.
For instance, one learning rule for such networks that explicitly implements the
Hebbian principle is updating the weights connecting two neurons i and j as Wij ∝zi(x)zj(x) when
181

6.4.2
Let’s Wire Together
Let’s start with some more reminiscing through our now well-adjusted Bayesian lens.
Recall from (4.80) that the prior distribution over peractivations is nearly-Gaussian at
large-but-ﬁnite width:
p

z(L)
D

∝exp
"
−1
2
nL
X
j=1
X
δ1,δ2∈D
gδ1δ2z(L)
j;δ1z(L)
j;δ2
(6.79)
+ 1
8
nL
X
j,k=1
X
δ1,...,δ4∈D
v(δ1δ2)(δ3δ4)z(L)
j;δ1z(L)
j;δ2 z(L)
k;δ3z(L)
k;δ4 + . . .
#
.
As a reminder, the quadratic coupling gδ1δ2 ≡gδ1δ2
(L)
(4.81) and the quartic coupling
v(δ1δ2)(δ3δ4) ≡v(δ1δ2)(δ3δ4)
(L)
(4.82) depend explicitly on groups of inputs from the dataset D
and implicitly on the Hyperparameters Cb and CW , the widths nℓ, and the depth L. As a
consequence of the nonzero intralayer interaction between diﬀerent output preactivations
in the prior, there will be non-vanishing correlations between the components of the
network outputs in the posterior.
As we did at inﬁnite width, we’ll start with the prior distribution (6.79) and then
obtain the posterior distribution p

z(L)
B
yA

∝p

yA, z(L)
B

by plugging in our observa-
tions z(L)
i;˜α →yi;˜α and keeping track of the dependence on the remaining variables z(L)
i; ˙β .
For the quadratic term in the action, with exactly the same set of manipulations as we
did in the inﬁnite-width limit (§6.3.2), replacing the inverse kernel with the quadratic
coupling at ﬁnite width Kδ1δ2 →gδ1δ2, we ﬁnd
1
2
nL
X
j=1
X
δ1,δ2∈D
gδ1δ2z(L)
j;δ1z(L)
j;δ2

z(L)
i;˜α =yi;˜α
(6.80)
= constant + 1
2
nL
X
j=1
X
˙β1, ˙β2∈B
G
˙β1 ˙β2 
z(L)
j; ˙β1 −mj; ˙β1
 
z(L)
j; ˙β2 −mj; ˙β2

,
with the naive posterior mean
mi; ˙β ≡
X
˜α1,˜α2∈A
g ˙β ˜α1eg ˜α1 ˜α2yi;˜α2 ,
(6.81)
and the naive posterior covariance
G ˙β1 ˙β2 ≡g ˙β1 ˙β2 −
X
˜α3,˜α4∈A
g ˙β1 ˜α3eg ˜α3 ˜α4g˜α4 ˙β2 .
(6.82)
observing activities zi(x) and zj(x) for a given input x.
In contrast, any ﬁnite-width feedforward neural network should automatically incorporate Hebbian
learning by nature. To underscore this point further, in §∞we’ll perform an analogous computation for
a gradient-descent update. Since the prior has the same form as the initialization distribution, we expect
that all learned ﬁnite-width networks will inc. the Hebbian learning principle automatically, regardless
of whether that learning is Bayesian or gradient-based.
182

We say naive here because there are additional corrections we need to consider coming
from the the quartic term in the action. Let see explicitly how this works for the posterior
mean.
Given the observed true outputs yi;˜α and the quadratic term (6.80) centered at the
naive posterior mean mi; ˙β, it is natural to center ourselves at
Φi;δ ≡

yi;˜α , mi; ˙β

=

yi;˜α ,
X
˜α1,˜α2∈A
g ˙β ˜α1eg ˜α1 ˜α2yi;˜α2

,
(6.83)
and deﬁne a ﬂuctuating variable wi; ˙β ≡z(L)
i; ˙β −mi; ˙β so that we can plug the decomposition
z(L)
i;δ =

z(L)
i;˜α , z(L)
i; ˙β

→

yi;˜α , mi; ˙β + wi; ˙β

= Φi;δ +

0, wi; ˙β

,
(6.84)
into the action (6.79), thus making the partitioning into subsamples D = A∪B manifest.
In terms of this ﬂuctuation, the quadratic term (6.80) takes the form
constant + 1
2
nL
X
j=1
X
˙β1, ˙β2∈B
G
˙β1 ˙β2wj; ˙β1wj; ˙β2 ,
(6.85)
and the quartic term can be evaluated as
Q(w) ≡
"
1
8
nL
X
j,k=1
X
δ1,...,δ4∈D
v(δ1δ2)(δ3δ4)z(L)
j;δ1z(L)
j;δ2 z(L)
k;δ3z(L)
k;δ4
#
z(L)
i;˜α =Φi;˜α; z(L)
i; ˙β =Φi; ˙β+wi; ˙β
= constant + 4
8
X
j
X
˙β1∈B
wj; ˙β1

X
k
X
δ1,δ2,δ3∈D
v( ˙β1δ1)(δ2δ3)Φj;δ1Φk;δ2Φk;δ3


+ 2
8
X
j
X
˙β1, ˙β2∈B
wj; ˙β1wj; ˙β2

X
k
X
δ1,δ2∈D
v( ˙β1 ˙β2)(δ1δ2)Φk;δ1Φk;δ2


+ 4
8
X
j,k
X
˙β1, ˙β2∈B
wj; ˙β1wk; ˙β2


X
δ1,δ2∈D
v( ˙β1δ1)( ˙β2δ2)Φj;δ1Φk;δ2


+ 4
8
X
j,k
X
˙β1, ˙β2, ˙β3∈B
wj; ˙β1wj; ˙β2wk; ˙β3

X
δ1∈D
v( ˙β1 ˙β2)( ˙β3δ)Φk;δ1


+ 1
8
X
j,k
X
˙β1,..., ˙β4∈B
wj; ˙β1wj; ˙β2 wk; ˙β3wk; ˙β4

v( ˙β1 ˙β2)( ˙β3 ˙β4)
.
(6.86)
Given all these expressions, we can ﬁnally determine the true posterior mean by
183

computing the following expectation:
Z
dz(L)
B p

z(L)
B
yA

z(L)
i; ˙β =
Z
dz(L)
B
p

yA, z(L)
B

p
 yA

z(L)
i; ˙β
=mi; ˙β +
Z
dwB
p(yA, mB + wB)
p(yA)
wi; ˙β
=mi; ˙β +
DD
wi; ˙βeQ(w)EE
G


eQ(w)
G
=mi; ˙β +
DD
wi; ˙β [1 + Q(w)]
EE
G
⟨⟨1 + Q(w)⟩⟩G
+ O

v2
=mi; ˙β +
DD
wi; ˙βQ(w)
EE
G + O

v2
,
(6.87)
where on the ﬁrst line we used Bayes’ rule for the posterior (6.25), on the second line we
inserted our decomposition (6.84) in two places, on the third line we separated out the
quartic term in order to rewrite the posterior expectation as a Gaussian expectation with
respect to the naive posterior covariance G divided by the distribution’s normalization,
on the fourth line we expanded the exponential, and on the ﬁnal line we used the fact
that the ﬂuctuation has zero mean
DD
wi; ˙β
EE
G = 0 in Gaussian expectation.
We can
now evaluate the remaining Gaussian expectation by plugging in our expression for the
quartic term (6.86) and making Wick contractions:
mi; ˙β +
DD
wi; ˙βQ (w)
EE
G
(6.88)
=mi; ˙β + 1
2
X
˙β1∈B
G ˙β ˙β1

X
k
X
δ1,δ2,δ3∈D
v( ˙β1δ1)(δ2δ3)Φi;δ1Φk;δ2Φk;δ3


+ 1
2
X
˙β1, ˙β2, ˙β3∈B

nLG ˙β1 ˙β2G ˙β ˙β3 + 2G ˙β ˙β1G ˙β2 ˙β3


X
δ1∈D
v( ˙β1 ˙β2)( ˙β3δ1)Φi;δ1

.
Thus, we see that the naive posterior mean (6.81) is further corrected by a number of
v-dependent terms.
To extract some physics from this complicated expression, note from the deﬁnition
of Φ (6.83) that the i-th component of Φi;δ depends on the i-th component of our
observation yi;˜α. This in particular means that the term above ∝P
k Φi;δ1Φk;δ2Φk;δ3
does incorporate information from all of the components of the observed true outputs.
In other words, information from the k-th component of the observed outputs successfully
inﬂuences the posterior mean prediction on the i-th component for i ̸= k. This means
that at ﬁnite width we have a dependence among the components of the posterior outputs
p

z(L)
i;B , z(L)
k;B
yi;A, yk;A

̸= p

z(L)
i;B
yi;A

p

z(L)
k;B
yk;A

.
(6.89)
184

This property of the posterior distribution descends from the nontrivial ﬁre-together
inductive bias p

z(L)
i
z(L)
k

present in the ﬁnite-width prior as discussed in §6.4.1. The
dependence among the components of the posterior outputs (6.89) is a signature of our
posterior beliefs’ learning to wire together, and we will see a further manifestation of this
when we again consider representation learning in the next section.
Before we move on, we should address practical matters. Practically speaking, it is
even more computationally infeasible to evaluate the ﬁnite-width predictions of Bayesian
learning (6.87) than it was at inﬁnite width. In particular, evaluating the quartic cou-
pling involves ﬁrst representing the four-point vertex – a NA×NA×NA×NA-dimensional
tensor – and then multiply contracting it with inverse kernels. Thus, both the cost of
computation and the memory requirements of Bayesian learning grow terrifyingly quickly
with our observations, i.e. with size of our dataset A. However, please Don’t Panic: we
are getting ever closer to the point where we can show you how gradient-based learning
resolves all these practical diﬃculties at ﬁnite width.
6.4.3
Presence of Representation Learning
The fact that the individual components of the ﬁnite-width posterior mean prediction
can incorporate information from our observations of the other components is suggestive
of the idea that these observations might also be used to build up representations in the
hidden layers. Here we will show that such representation learning actually does occur
at ﬁnite width as a direct consequence of the nonzero interlayer interactions.
Analogous to our parallel subsection at inﬁnite width (§6.3.3), we can investigate
representation learning by considering the posterior distribution in the penultimate layer
ℓ= L−1 on the full set of samples D, given observations yA. In particular, to show how
the features of the penultimate-layer representation evolve, our goal will be to compute
the change in the expectation of a penultimate-layer observable O

z(L−1)
D

taken with
respect to the posterior as compared to the expectation taken with respect to the prior
d¯O ≡
Z
dz(L−1)
D
p

z(L−1)
D
yA

O

z(L−1)
D

−
Z
dz(L−1)
D
p

z(L−1)
D

O

z(L−1)
D

,
(6.90)
where p

z(L−1)
D

and p

z(L−1)
D
yA

are the prior and posterior distributions, respec-
tively. This expectation diﬀerence was strictly zero in the inﬁnite-width limit since the
penultimate-layer posterior was exactly equal to the penultimate-layer prior (6.71). A
non-vanishing diﬀerence in contrast will mean that the penultimate-layer preactivations
are being updated after making observations yA. Such an update is a direct avatar of
representation learning.
As before, by Bayes’ rule we can write the posterior distribution of the penultimate
preactivations z(L−1)
D
given our observations yA as
p

z(L−1)
D
yA

=
p

yA
z(L−1)
D

p

z(L−1)
D

p(yA)
.
(6.91)
185

Just as before, the likelihood p

yA
z(L−1)
D

is the conditional distribution p

z(L)
A
z(L−1)
D

evaluated on our set of observations z(L)
A
→yA. With this expression for the posterior
(6.91), we can express the update d¯O after Bayesian learning as
d¯O = E


p

yA
z(L−1)
D

p(yA)
O

z(L−1)
D


−E
h
O

z(L−1)
D
i
.
(6.92)
As always, the full expectation E [ · ] is to be evaluated with respect to the prior or
initialization distribution p

z(L−1)
D

; all learning will always be represented explicitly
with the insertion of other factors as we did above.
Let’s now determine how this insertion, the likelihood-to-evidence ratio
p

yA
z(L−1)
D

p(yA)
,
(6.93)
depends on the preactivations z(L−1)
D
. As we pointed out when working through the
inﬁnite-width example, we already worked out the form of this likelihood in (4.69) as
the conditional distribution between layers. In our current context and notation, the
likelihood reads
p

yA
z(L−1)
D

=
1
r2π bG(L)

nL exp

−1
2
nL
X
i=1
X
˜α1,˜α2∈A
bG˜α1 ˜α2
(L) yi;˜α1yi;˜α2

,
(6.94)
where as a reminder the stochastic metric bG(L)
˜α1 ˜α2 = bG(L)
˜α1 ˜α2

z(L−1)
A

depends explicitly on
the preactivations in the penultimate layer z(L−1)
i;A
.33 Thus, the stochastic metric acts as a
coupling here, inducing interlayer interactions between the (L−1)-th-layer preactivations
and the observations yA. As we will see, this endows the updated distribution over z(L−1)
D
with a dependence on yA.
As should be fairly familiar at this point, we can decompose the stochastic metric
into a mean and a ﬂuctuation,
bG(L)
˜α1 ˜α2 ≡G(L)
˜α1 ˜α2 + d
∆G
(L)
˜α1 ˜α2 ,
(6.95)
33Strictly speaking, we should really denote the stochastic metric here as beG
(L)
˜α1 ˜α2 to indicate that we’re
focusing on the NA-by-NA submatrix of the full stochastic metric on D, bG(L)
δ1δ2. It’s the matrix inverse of
this submatrix beG
(L)
˜α1 ˜α2 – and not the (˜α1, ˜α2) block of the inverse of the full matrix bG(L)
δ1δ2 – that appears
in (6.94). Since this tilde-with-a-hat looks ridiculous – and since we are already heavily overburdened
on the notational front – if you promise to keep this caveat in mind, we’ll do everyone a favor and
temporarily suppress this tilde.
186

in terms of which the likelihood (6.94) can be Taylor-expanded `a la Schwinger-Dyson as
we did before in (4.56) and (4.57). At ﬁrst nontrivial order, we ﬁnd for the likelihood-
to-evidence ratio (6.93)
p

yA
z(L−1)
D

p(yA)
(6.96)
=
1
p(yA)
q2πG(L)nL

1+ 1
2
X
˜α1,...,˜α4∈A
d
∆G
(L)
˜α1 ˜α2G˜α1 ˜α3
(L) G˜α2 ˜α4
(L)
nL
X
i=1

yi;˜α3yi;˜α4−G(L)
˜α3 ˜α4

+O

∆2

.
Here, the prefactor before the square brackets is constant with respect to the variables
z(L−1)
D
, and so all of the relevant dependence needed to evaluate update d¯O (6.92) is
contained implicitly in the metric ﬂuctuation d
∆G
(L)
˜α1 ˜α2. We can thus compute a posterior
expectation – i.e. the ﬁrst expectation in (6.92) – of any observable by integrating against
the quantity in the square bracket, so long as we also divide by an integral of “1” against
the same quantity in order to properly normalize. With this by-now familiar trick in
mind, we can rewrite the posterior expectation as
E

O

z(L−1)
D

1 + 1
2
P
˜α1,...,˜α4 d
∆G
(L)
˜α1 ˜α2G˜α1 ˜α3
(L) G˜α2 ˜α4
(L)
P
i

yi;˜α3yi;˜α4−G(L)
˜α3 ˜α4

+O
 ∆2
E

1 + 1
2
P
˜α1,...,˜α4 d
∆G
(L)
˜α1 ˜α2G˜α1 ˜α3
(L) G˜α2 ˜α4
(L)
P
i

yi;˜α3yi;˜α4−G(L)
˜α3 ˜α4

+O(∆2)

(6.97)
=E
h
O

z(L−1)
D
i
+ 1
2
X
˜α1,...,˜α4∈A
E

d
∆G
(L)
˜α1 ˜α2O

z(L−1)
D

G˜α1 ˜α3
(L) G˜α2 ˜α4
(L)
X
i

yi;˜α3yi;˜α4−G(L)
˜α3 ˜α4

+ O
 1
n2

,
where the details of what we actually did are hidden in this here footnote.34 We see
that the ﬁrst term is just the prior expectation, while the second term expresses the
update d¯O (6.90). Finally, taking only the leading ﬁnite-width corrections at order 1/n
and restoring the tildes to correctly represent the submatrices on A alone, we can write
down a very general expression for the update to any penultimate-layer observable at
34The reason that we treated the additional O ∆2
pieces as O 1/n2
is hidden under the rug in
the main body. To peak under that rug, ﬁrst let us schematically express the likelihood-to-evidence
ratio (6.96) as constant × 
1 + ♯1∆G + ♯2(∆G)2 + O ∆3
. Then, the posterior expectation becomes
E 
O 
1 + ♯1∆G + ♯2(∆G)2 + O ∆3	
E [1 + ♯1∆G + ♯2(∆G)2 + O(∆3)]
=
E [O] + ♯1E [O∆G] + ♯2E 
(∆G)2 O
+ O 1/n2
1 + ♯2E [(∆G)2] + O(1/n2)
(6.98)
=E [O] + ♯1E [O∆G] + ♯2

E 
(∆G)2 O
−E 
(∆G)2
E [O]	
+ O 1/n2
.
Decomposing the observable into a mean and a ﬂuctuation as O = E [O] + ∆O, we see that the term
proportional to the coeﬃcient ♯2 is E 
O ∆3
= O 1/n2
and thus can be neglected, while the leading
ﬁnite-width correction cannot be neglected: ♯1E [O∆G] = ♯1E [∆O∆G] = O(1/n).
187

leading nontrivial order in 1/n:
d¯O = 1
2
X
˜α1,...,˜α4∈A
E

d
∆G
(L)
˜α1 ˜α2O

z(L−1)
D

f
K ˜α1 ˜α3
(L)
f
K ˜α2 ˜α4
(L)
nL
X
i

yi;˜α3yi;˜α4−f
K(L)
˜α3 ˜α4

.
(6.99)
Again, please be careful and remember that the E [ · ] in (6.99) is to be evaluated with
respect to the prior distribution p

z(L−1)
D

. Note also that the lone expectation in the
update (6.99) is just the covariance of the stochastic metric with the observable:
E

d
∆G
(L)
˜α1 ˜α2O

z(L−1)
D

= E
h
bG(L)
˜α1 ˜α2O

z(L−1)
D
i
−E
h
bG(L)
˜α1 ˜α2
i
E
h
O

z(L−1)
D
i
.
(6.100)
As we addressed in that rugly footnote, for a general order-one observable this covariance
is 1/n-suppressed but nonzero. Thus, we see that at large-but-ﬁnite width (1 ≪n < ∞),
such observables get updated: representations are learned.
In order to see how this works, let’s consider a concrete example.
The simplest
observable turns out to be the average norm of the activations
O

z(L−1)
D

≡
1
nL−1
nL−1
X
j=1
σ(L−1)
j;δ1
σ(L−1)
j;δ2
,
(6.101)
which we can decompose in terms of a mean and a ﬂuctuation as
O

z(L−1)
D

= E
h
O

z(L−1)
D
i
+
1
C(L)
W
d
∆G
(L)
δ1δ2 ,
(6.102)
if we also recall the explicit form of the metric ﬂuctuation (4.74)
d
∆G
(L)
˜α1 ˜α2 = C(L)
W
1
nL−1
nL−1
X
j=1

σ(L−1)
j;˜α1
σ(L−1)
j;˜α2
−E
h
σ(L−1)
j;˜α1
σ(L−1)
j;˜α2
i
.
(6.103)
Then, plugging into our expression for the leading-order ﬁnite-width update (6.99), we
ﬁnd
d¯O =
1
2C(L)
W
X
˜α1,...,˜α4∈A
E

d
∆G
(L)
δ1δ2 d
∆G
(L)
˜α1 ˜α2

f
K ˜α1 ˜α3
(L)
f
K ˜α2 ˜α4
(L)
X
i

yi;˜α3yi;˜α4−f
K(L)
˜α3 ˜α4

=
1
2nL−1C(L)
W
X
˜α1,...,˜α4∈A
V (L)
(δ1δ2)(˜α1 ˜α2)f
K ˜α1 ˜α3
(L)
f
K ˜α2 ˜α4
(L)
X
i

yi;˜α3yi;˜α4−f
K(L)
˜α3 ˜α4

,
(6.104)
where to go to the second line we used the deﬁnition of the four-point vertex in terms
of the two-point function of the metric ﬂuctuation (4.76). As this vertex characterizes
the non-Gaussianity of the output distribution, we see explicitly here how interactions
are mediating updates to the penultimate-layer activations.
In addition, the leading
factor of 1/nL−1 makes it clear that this update is a ﬁnite-width eﬀect. Further, the
term in the last parenthesis shows that the update depends explicitly on the diﬀerence
188

between our observations of the outputs, yi;˜α3yi;˜α4, and our prior expectations of them,
E
h
z(L)
i;˜α3z(L)
i;˜α4
i
≡f
K(L)
˜α3 ˜α4+O(1/n). This means that the observations are in fact propagating
backward to induce changes in the hidden-layer representations.35
Although perhaps not practically useful, this Bayesian analysis of representation
learning at ﬁnite width will serve as a theoretically useful blueprint for studying a similar
type of representation learning that occurs with gradient-based learning at ﬁnite width
in §11. Now, with all these allusions to gradient-based learning having accrued with
interest, you must be really excited to ﬂip the page to the next chapter!
35This kind of backward-propagation or backpropagation, if you will, persists further into the shallower
hidden layers as well. However, in the (L −2)-th layer, the posterior update turns out to be of order
O 1/n2
. Intuitively this makes sense because the change in the representation in the penultimate layer
(L −1) is already down by a factor of 1/n, and it gets further suppressed due to the 1/n-suppression of
the interlayer interaction in going back to the (L −2)-th layer.
Mathematically, we can consider the update to an (L −2)-th-layer observable O

z(L−2)
D

as
d¯O ≡
Z
dz(L−2)
D
p

z(L−2)
D
yA

O

z(L−2)
D

−
Z
dz(L−2)
D
p

z(L−2)
D

O

z(L−2)
D

.
(6.105)
Through the chain of Bayes’, sum, and product rules, the posterior insertion in this formula is given in
terms of the following marginalization:
p

z(L−2)
D
yA

=
p

yA
z(L−2)
D

p

z(L−2)
D

p(yA)
=
Z
dz(L−1)
D
p

yA
z(L−1)
D

p(yA)
p

z(L−1)
D
, z(L−2)
D

.
(6.106)
From here, through the same set of manipulations that led to the update equation for the penultimate
layer (6.99), we get
d¯O = 1
2
X
˜α1,...,˜α4∈A
E
h
d
∆G
(L)
˜α1 ˜α2

z(L−1)
D

O

z(L−2)
D
i
e
K ˜α1 ˜α3
(L)
e
K ˜α2 ˜α4
(L)
nL
X
i

yi;˜α3yi;˜α4 −e
K(L)
˜α3 ˜α4

+ O
 1
n2

.
(6.107)
Thus, to show that this change is of order O 1/n2
, we need to show that the interlayer correlation,
E
h
d
∆G
(L)
˜α1 ˜α2

z(L−1)
D

O

z(L−2)
D
i
,
(6.108)
is of order O 1/n2
.
This is most swiftly carried out in the future, ﬁrst by the application of the
formula (8.54) with ℓ= L −2 and then with the associated trickery (8.70). If you are up for a challenge,
please ﬂip forward and write a note next to (8.70) reminding yourself to come back to footnote 35 in
§6.4.3. Spoiler alert: you should in fact ﬁnd that (6.108) is of order O 1/n2
.
189

190

Chapter 7
Gradient-Based Learning
Of course, that’s like saying Newton’s second law F = ma, as it appears in textbooks
on mechanics, is just a deﬁnition of what you mean by “force”. That’s true, strictly
speaking, but we live in a landscape where there is an implicit promise that when
someone writes that down . . . that they will give laws for the force, and not, say, for
some quantity involving the 17th time derivative of the position.
Sidney Coleman, in his “Quantum Mechanics in Your Face” Dirac Lecture [56].
In the last chapter, we discussed Bayesian inference as a learning algorithm, which fol-
lowed naturally from our study of networks at initialization. Starting from a description
of a neural network architecture with parameters – weights and biases – we integrated
out these parameters to ﬁnd a distribution over preactivations z(ℓ)(x) as a function of
layer and input sample, which in particular includes the output distribution p

z(L)(x)

.
This was interpreted as a prior distribution over an ensemble of such models, and then
we explained how the logic of Bayes’ rule lets us evolve the prior into a posterior dis-
tribution conditioned on observed data. Despite the theoretical elegance of Bayesian
inference, the naive implementation quickly became computationally intractable as the
number of conditioned data samples grew large.
Stepping back, there’s actually something a little bit odd about this setup. Once
we worked out the output distribution, the actual network itself was discarded, with
the parameters long since integrated out. Since Bayesian inference only cares about
the output distribution of a model, the starting point for inference can really be any
ensemble of models as it isn’t speciﬁcally tailored to neural networks at all. So why go
through all the trouble of starting with neural-network models? How did we even know
that these models are a good abstraction to begin with?
Deep neural networks are exciting because they work surprisingly well. We know
this because in practice such networks are explicitly trained and used to perform useful
tasks. Most commonly, learning occurs by repeatedly updating the model parameters
via a gradient-based optimization procedure such as gradient descent.
In particular, gradient-based learning algorithms can eﬃciently process a large amount
191

of training data by optimizing an auxiliary loss function that directly compares the net-
work output f(x; θ) ≡z(L)(x) to some desired result or label. This optimization proce-
dure involves sampling only a single set of network parameters from the initialization
distribution, yielding just a single network trained for the task of interest rather than
a full ensemble of networks. In this way, gradient-based learning methods oﬀset their
inability to express conﬁdence in their predictions – due to the absence of an ensemble
– with data eﬃciency and easy scalability.
Since gradient descent involves making explicit updates to the model parameters,
the ﬁrst step is to bring them back (from whatever place that variables go when they
are integrated out). In supervised learning, the adjustments of model parameters are di-
rectly proportional to the function-approximation error times the gradient of the model
output with respect to the parameters. This decomposition motivates the study of the
neural tangent kernel (NTK).1 In short, the NTK is a type of Hamiltonian that controls
the training dynamics of observables whenever gradient descent is used to optimize an
auxiliary loss that scores a function approximation. As we detail in §10, §11, and §∞,
understanding the NTK for a given neural-network architecture will enable us to eﬀec-
tively describe gradient-based learning for that model.
In this chapter, we give a short introduction to supervised learning in §7.1, followed
by a discussion of gradient descent in §7.2 with a very general focus on how the NTK
arises in supervised learning. In the next chapter, we’ll incorporate the NTK into our
eﬀective theory of deep learning by exploiting the same layer-to-layer RG ﬂow technique
we used in §4.
7.1
Supervised Learning
One of the most basic modeling tasks at which neural networks excel is known as super-
vised learning. Given a data distribution p(x, y) = p(y|x)p(x), the goal is to predict
a label y given an input x, for any pair that is jointly sampled from the distribution.2
To be precise, the model tries to learn the conditional distribution p(y|x), and the re-
sulting model is sometimes called a discriminative model. In one canonical example
from computer vision, we might want to classify an image xδ of a hand-written digit
“3” according to its literal value yδ = 3. Or, for a natural language processing example,
given a sentence containing the word xδ = cat we might want to identify the part of the
speech as yδ = noun. The better the probabilistic model learns the distribution p(y|x),
the more accurately it can predict a true label y for a novel input example x. Generating
these datasets generally requires human annotators to label the inputs, hence the name
supervised learning.
In this setup, the supervised-learning model outputs a prediction z(xδ; θ).
This
notation emphasizes that the model output is both a function of the input xδ as well as
1The NTK was ﬁrst identiﬁed in the seminal work of Jacot et al. [57] in the context of inﬁnite-width
networks.
2In this section, we suppress vectorial indices on the inputs xδ, labels yδ, and model outputs z (xδ; θ),
while often retaining sample indices δ ∈D.
192

some adjustable parameters θ. This should already be familiar in the context of neural-
network function approximation, where the model parameters consist of the biases and
weights.
As discussed in §2.3, the model parameters are drawn from an easy-to-sample prior
distribution over the parameters, which is also known as the initialization distribution in
the context of gradient-based learning. Importantly, this parameter distribution knows
nothing about the data distribution. Thus, in order for the model to make good pre-
dictions, its parameters will need to be adjusted somehow. Really, this is just a speciﬁc
application of the function approximation that we discussed in §2.1 where the function
to be approximated is a conditional distribution p(y|x).
Before we understand how to adjust or ﬁt the model parameters, we need to un-
derstand what we mean by making good predictions. What we want is, for a typical
input xδ and a label yδ sampled from the data distribution p(x, y), that the model out-
put z(xδ; θ) is as close to the label yδ as possible on average. In order to measure this
proximity, for a prediction-label pair we need to deﬁne an auxiliary objective function
or loss,
L

z(xδ; θ), yδ

,
(7.1)
with the property that the closer z(xδ; θ) is to yδ, the lower the value of the function is.
One very intuitive choice for the loss is MSE loss (6.17),
LMSE

z(xδ; θ), yδ

≡1
2
h
z(xδ; θ) −yδ
i2
,
(7.2)
which clearly has the required property, though this is not the most common choice in
deep learning. The speciﬁc form of the loss will not matter for the rest of the chapter.
With the loss function in hand, the goal of training is to adjust model parameters so
as to minimize the loss for as many input-label pairs as possible. Ideally, we would like
to minimize the loss averaged over the entire data distribution,
E [L(θ)] =
Z
dxdy p(x, y) L

z(x; θ), y

.
(7.3)
But since we almost never have access to the analytical form of the data distribution
p(x, y), in practice this would require the sampling of an inﬁnite number of input-label
pairs. Instead, as a proxy of the entire loss (7.3), we sample a large-but-ﬁnite number
of pairs (x˜α, y˜α)˜α∈A and try to minimize
LA(θ) ≡
X
˜α∈A
L

z(x˜α; θ), y˜α

.
(7.4)
This set of examples A is referred to as the training set, and the estimate of the loss
(7.4) is called the training loss; here we’ve also inherited from §6 our sample-index
notation of alpha-with-tilde for the inputs in the training set ˜α ∈A, while denoting
generic inputs as delta-with-no-decoration δ ∈D, and soon we’ll use beta-with-dot for
193

inputs in the test set ˙β ∈B.3 To train our model, we try to ﬁnd a conﬁguration of the
model parameters that minimizes the training loss
θ⋆= arg min
θ
LA(θ) = arg min
θ
" X
˜α∈A
L

z(x˜α; θ), y˜α
#
.
(7.6)
In the next section, we will present the gradient descent algorithm as a way to accomplish
this goal.
Having set the minimization of the training loss (7.4) as our optimization problem, it
is important to keep in mind that the true goal of supervised learning is the minimization
of the loss over the entire data distribution in the sense of (7.3). Said another way, the
question is not whether the model is able to memorize all the input-label pairs in the
training set, but rather whether it’s able to generalize its predictions to additional input-
label pairs not seen during training. One might then worry about whether a training set
is biased in its sampling of the data distribution or whether there is high variance in a
particular set of samples.
To explicitly assess this generalization property of a model, a separate set of input-
label samples (x ˙β, y ˙β) ˙β∈B – known as the test set – is typically set aside and only used
to evaluate a model after training is complete. To the extent that the training set A is
representative of the full data distribution p(x, y), decreasing the training loss will often
decrease the entire loss (7.3), as estimated by the test loss LB. We will address this
question directly in §10.
7.2
Gradient Descent and Function Approximation
Considering the training loss minimization (7.6), we see that learning is a complicated
optimization problem.
Being entirely naive about it, in order to ﬁnd extrema of a
function, calculus instructs us to diﬀerentiate the training loss and ﬁnd the value of the
3Note that our deﬁnition of the training loss (7.4) is a bit at odds with our deﬁnition of the expected
loss (7.3). In particular, the expected loss is intensive, while the training loss is extensive, scaling linearly
with the size of the training set NA ≡|A|. This latter choice is consistent with our ﬁrst deﬁnition of
this loss, (6.17), in the context of MLE as an approximate method for Bayesian model ﬁtting in §6.2.1.
There, the extensivity of the loss was natural according to the Bayesian framework: as the number of
observed input-output pairs NA increases, we want the likelihood to dominate the prior. As such, we
will ﬁnd it natural to follow that convention. (You also might more accurately call the extensive loss
(6.17) as the mean squared error.) However, from a non-Bayesian perspective, it is often customary to
deﬁne a training loss as
LA(θ) ≡
1
|A|
X
˜α∈A
L

z(x˜α; θ), y˜α

,
(7.5)
which better corresponds to the expected loss (7.3). Since in the context of gradient-based learning the
overall normalization can always be absorbed in a redeﬁnition of the global learning rate η, to be intro-
duced next section, the only advantage we see of this latter deﬁnition (7.5) is the better correspondence
of the loss with its name.
194

argument for which the resulting expression vanishes:
0 = dLA
dθµ

θ=θ⋆
.
(7.7)
Unfortunately this equation is exactly solvable only in special cases, for instance when the
loss is quadratic in the model parameters. Rather than trying to ﬁnd minima analytically,
practitioners typically employ an iterative procedure to bring the loss closer and closer
to a minimum.
Gradient descent is one such method that can be used to minimize nontrivial
functions like the training loss (7.4), and so it’s a natural candidate for model ﬁtting.
The algorithm involves the computation of the gradient of the loss and iteratively updates
the model parameters in the (negative) direction of the gradient
θµ(t + 1) = θµ(t) −ηdLA
dθµ

θµ=θµ(t)
,
(7.8)
where t keeps track of the number of steps in the iterative training process, with t = 0
conventionally being the point of initialization. Here, η is a positive training hyper-
parameter called the learning rate, which controls how large of a step is taken in
parameter space. Note that the computational cost of gradient descent scales linearly
with the size of the dataset A, as one just needs to compute the gradient for each sample
and then add them up.
For suﬃciently small learning rates, the updates (7.8) are guaranteed to decrease the
training loss LA. In order to see this, let us Taylor-expand the training loss around the
current value of the parameters θ(t) and compute the change in the loss after making
an update
∆LA ≡LA

θ(t + 1)

−LA

θ(t)

= −η
X
µ
 
dLA
dθµ
!2 
θ=θ(t)
+ O(η2) .
(7.9)
As minus a sum of squares, this is strictly negative. Pretty typically, iterating these
updates will eventually lead to (at least) a local minimum of the training loss.
In
practice, small variants of the gradient descent algorithm are responsible for almost all
training and optimization in deep learning.4
4In particular, the most popular learning algorithm is stochastic gradient descent (SGD). SGD
uses updates of the form
θµ(t + 1) = θµ(t) −η dLSt
dθµ

θµ=θµ(t)
,
(7.10)
where St is a subset of the training set, St ⊂A. Each subset St is called a mini-batch or batch. Training
is then organized by epoch, which is a complete passes through the training set. Typically, for each
epoch the training set is stochastically partitioned into subsets of equal size, which are then sequentially
used to estimate the gradient.
195

Tensorial Gradient Descent
In one such variant, we can deﬁne a more general family of learning algorithms by
modifying the update (7.8) as
θµ(t + 1) = θµ(t) −η
X
ν
λµν
dLA
dθν

θ=θ(t)
,
(7.11)
where the tensor λµν is a learning-rate tensor on parameter space; the original
gradient-descent update (7.8) is a special case with the Kronecker delta as the ten-
sor λµν = δµν. While in the original gradient descent (7.8) we have one global learning
rate η, in the tensorial gradient descent (7.11) we have the freedom to separately specify
how the ν-th component of the gradient dLA/dθν contributes to the update of the µ-th
parameter θµ via the tensor λµν. Repeating the same Taylor-expansion in η (7.9) with
the generalized update (7.11), we ﬁnd
∆LA = −η
X
µ,ν
λµν
dLA
dθµ
dLA
dθν
+ O(η2) ,
(7.12)
indicating that the training loss again is almost surely decreasing for suﬃciently small
learning rates, so long as the learning-rate tensor λµν is a positive semideﬁnite matrix.
Neural Tangent Kernel
Everything we have said so far about gradient descent could be applied equally to the
optimization of any function. However, in the context of function approximation there
is additional structure: the optimization objective is a function of the model output.
To take advantage of this structure, ﬁrst note that by the chain rule the gradient of
the loss can be expressed as
dLA
dθµ
=
nout
X
i=1
X
˜α∈A
∂LA
∂zi;˜α
dzi;˜α
dθµ
,
(7.13)
which means that the change in the loss (7.12) after an update can be nicely decomposed
as
∆LA = −η
nout
X
i1,i2=1
X
˜α1,˜α2∈A
"
∂LA
∂zi1;˜α1
∂LA
∂zi2;˜α2
# "X
µ,ν
λµν
dzi1;˜α1
dθµ
dzi2;˜α2
dθν
#
+ O(η2) .
(7.14)
The quantity in the ﬁrst square bracket is a measure of the function approximation error.
For instance, for the MSE loss (7.2) we see that the gradient of the loss with respect to
the model output is exactly the prediction error,
∂LA
∂zi;˜α
= zi(x˜α; θ) −yi;˜α .
(7.15)
The advantage of this algorithm is twofold: (i) the computational cost of training now scales with the
ﬁxed size of the sets St rather than with the size of the whole training set A; and (ii) SGD is thought
to have better generalization properties than gradient descent. Nevertheless, essentially everything we
will say about gradient descent will apply to stochastic gradient descent as well.
196

More generally for other losses, the gradient of the loss or error factor
ϵi;˜α ≡∂LA
∂zi;˜α
,
(7.16)
is small when the model output is close to the label. Sensibly, the greater the error
factor, the larger the update (7.13), and the greater the change in the loss (7.14). The
quantity in the second square bracket is called the neural tangent kernel (NTK)
Hi1i2;˜α1 ˜α2 ≡
X
µ,ν
λµν
dzi1;˜α1
dθµ
dzi2;˜α2
dθν
.
(7.17)
As is clear from (7.17), the NTK is independent of the auxiliary loss function.
Importantly, the NTK is the main driver of the function-approximation dynamics.
To the point, it governs the evolution of a much more general set of observables than
the training loss. Consider any observable that depends on the model’s outputs
O(θ) ≡O

z(xδ1; θ), . . . , z(xδM ; θ)

,
(7.18)
where xδ1, . . . , xδM ∈D for some dataset D. For example, if D is the test set B and O
is the loss function, then this observable would be the test loss LB. In addition to the
test loss, one might want to observe the change in a particular component of the output
O = zi(x) or perhaps track correlations among diﬀerent vectorial components of the
output O = zi(x) zj(x) for a given input x. For any such observable (7.18), its change
after an update is given by the expression
O

θ(t + 1)

−O

θ(t)

= −η
nout
X
i1,i2=1
X
˜α∈A
X
δ∈D
"
∂LA
∂zi1;˜α
∂O
∂zi2;δ
#
Hi1i2;˜αδ + O(η2) .
(7.19)
As we see, the square bracket contains the function-approximation error as well as the
particulars about how the observable depends on the model output. In contrast, the NTK
contains all the dynamical information pertaining to the particular model, depending
only on the model architecture and parameters.5
We can further understand the function-approximation dynamics under gradient de-
scent by considering a particular vectorial component of the output for a particular
sample as an observable, i.e. O = zi(xδ). In this case, the derivative of O in (7.19) is
a Kronecker delta on both the vectorial indices and sample indices, and the evolution
reduces to
zi

xδ; θ(t + 1)

−zi

xδ; θ(t)

= −η
nout
X
j=1
X
˜α∈A
Hij;δ˜αϵj;˜α + O(η2) .
(7.20)
5As our discussion makes clear, the NTK can generally be deﬁned for any function approximator.
This means that its name masks its true generality. In addition to objecting to the “neural” part of the
name, one could object to the “kernel” part. In particular, the NTK is more akin to a Hamiltonian than
a kernel as it generates the evolution of observables; we’ll fully justify this claim in §∞.2.2.
197

This equation shows how the model output changes after a training update. Importantly,
we see how the error factor ϵj;˜α (7.16) from example x˜α on the model output component j
aﬀects the updated behavior of the model output component i on a diﬀerent example xδ:
it’s mediated by the NTK component Hij;δ˜α. This is what makes function approximation
possible; the ability to learn something about one example, xδ, by observing another, x˜α.
We see that the oﬀ-diagonal components of the NTK in the sample indices determine
the generalization behavior of the model, while the oﬀ-diagonal components in vectorial
indices allow for one feature to aﬀect the training of another feature. We will have more
to say about the former property in §10 and the latter property in §∞.
Finally, let us note in passing that unlike the case of the training loss (7.14), for
general observables (7.19) the term in the square bracket is not necessarily positive.
While the training loss LA will always decrease for small enough learning rates, a given
observable may not. In particular, nothing guarantees that the test loss will decrease
and – for models that overﬁt their training set – the test loss may even increase.
198

Chapter 8
RG Flow of the Neural Tangent
Kernel
People get things backwards and they shouldn’t—it has been said, and wisely said, that
every successful physical theory swallows its predecessors alive.
Sidney Coleman, more forward and a little bit deeper in that same
“Quantum Mechanics in Your Face” Dirac Lecture [56].
In the last chapter, we introduced gradient-based learning as an alternative to Bayesian
learning and speciﬁcally focused on the gradient descent algorithm. In short, the gradi-
ent descent algorithm involved instantiating a network from the prior distribution and
then repeatedly updating the model parameters by running training data through the
network. This algorithm is straightforward to implement and very eﬃcient to run for
any particular network. In practice, it makes things very easy.
In theory, it makes things a little more diﬃcult. For the Bayesian prior, we were able
to integrate out the model parameters layer by layer in deriving the output distribution
because the initialization distribution of the biases and weights was extremely simple;
in addition, the large-width expansion made it possible to derive analytic expressions
for the Bayesian posterior for ﬁnite-width networks. By contrast, the model parameters
and the outputs of any particular network trained by gradient descent are a complicated
correlated mess.
To make progress, we ﬁrst need to shift the perspective back to a statistical one.
Rather than focusing on how any particular network learns from the data, we instead
ask how a typical network behaves when being trained. If we understand the typical
behavior (i.e. the mean) under gradient descent and have control of the ﬂuctuations
from network instantiation to instantiation (i.e. the variance), then we can describe
gradient-based learning as used in practice.
With that statistical perspective in mind, recall from the last chapter that the
gradient-descent updates decompose into an error factor times a function-approximation
factor. The latter factor was dubbed the neural tangent kernel (NTK) and conveniently
199

summarizes the eﬀect of the model parameters’ changes on the behavior of the network.
This means that the statistics of changes in network observables in the initial stage of
training are governed by the statistics of the NTKs at initialization. To proceed forward,
the core of the current chapter and the next will involve explicitly computing such NTK
statistics for deep MLPs; we will postpone the actual analysis of neural network training
– enabled by these computations of the NTK statistics – until §10 and §∞.
In §8.0, we will lay the groundwork for the recursive computation of the NTK statis-
tics. Namely, starting from the MLP iteration equation, or the forward equation for the
preactivations, we’ll derive a corresponding forward equation for the NTK. This equa-
tion is a layer-to-layer iteration equation that holds for each distinct instantiation of the
model parameters. (Here we’ll also remark on how the learning-rate tensor should be
scaled with network width, an important point that is often neglected in practice.)
By averaging over diﬀerent instantiations, we can then use the forward equation to
recursively compute the joint statistics of the NTK and the preactivations. The approach
taken here completely mirrors the RG-ﬂow approach taken in §4 for the preactivations.
In §8.1, §8.2, and §8.3, we will progressively determine the sequence of joint NTK-
preactivation distributions in the ﬁrst, second, and deeper layers, respectively.
8.0
Forward Equation for the NTK
As we saw in the previous chapter, the evolution of observables O(z) under gradient
descent is governed by the NTK,
Hi1i2;α1α2 ≡
X
µ,ν
λµν
dzi1;α1
dθµ
dzi2;α2
dθν
,
(8.1)
where λµν is the learning-rate tensor.
Specializing to MLPs, observables can depend not only on the network’s output
zi;α = z(L)
i;α , but also on the preactivations zi;α = z(ℓ)
i;α in any layer.
Such ℓ-th-layer
observables for ℓ< L tell us about the hidden-layer representations of the network.
For instance, the neural component O = z(ℓ)
i (x) tells us about an ℓ-th-layer feature
evaluated on an input x, while O = z(ℓ)
i (x) z(ℓ)
j (x) with neural indices i ̸= j tracks
correlations among diﬀerent features given x.
With similar manipulations as before, we ﬁnd that an observable O that depends
only on the ℓ-th-layer preactivations
O(θ) ≡O

z(ℓ)(xδ1; θ), . . . , z(ℓ)(xδM ; θ)

,
(8.2)
evolves after a gradient descent update as
O

θ(t + 1)

−O

θ(t)

= −η
nℓ
X
i1,i2=1
X
α∈A
X
δ∈D

dLA
dz(ℓ)
i1;α
∂O
∂z(ℓ)
i2;δ

H(ℓ)
i1i2;αδ + O(η2) ,
(8.3)
200

where xδ1, . . . , xδM ∈D for some dataset D.1
Here, we have deﬁned the ℓ-th-layer
NTK as
H(ℓ)
i1i2;α1α2 ≡
X
µ,ν
λµν
dz(ℓ)
i1;α1
dθµ
dz(ℓ)
i2;α2
dθν
,
(8.5)
which governs the evolution of the ℓ-th-layer observables; in terms of this notation, the
output NTK is simply H(ℓ=L)
i1i2;α1α2. Note that whenever we write the ℓ-th-layer NTK as
above, we will always assume that the learning-rate tensor λµν does not mix network
parameters from diﬀerent layers, though in general it can still mix the biases and weights
within a layer. We will place further restrictions on this in another paragraph.
At initialization, the model parameters are sampled from their initialization distribu-
tions, and the ℓ-th-layer NTK is a stochastic object. In order to emphasize this stochas-
ticity, in what follows we’ll decorate the NTK at initialization with a hat:
bH(ℓ)
i1i2;α1α2.
Our goal is to evaluate its statistics.
Before we go any further, it is convenient to make a specialized choice for the learning-
rate tensor λµν. In practice, typically λµν = δµν, and there is only the global learning
rate η for the entire model.
Even in a more general setup, a learning rate is often
shared among each group of parameters that are sampled from the same distribution.
Recalling that the same distribution was shared among the biases in a given layer with
the same variance C(ℓ)
b
(2.19) and similarly for the weights with the rescaled weight
variance C(ℓ)
W (2.20), this suggests an ansatz for our training hyperparameters: we
should decompose the learning-rate tensor λµν into a diagonal matrix
λb(ℓ)
i1 b(ℓ)
i2
= δi1i2λ(ℓ)
b
,
λW (ℓ)
i1j1W (ℓ)
i2j2
= δi1i2δj1j2
λ(ℓ)
W
nℓ−1
,
(8.6)
giving each group of biases in a layer the same learning rate and each group of weights
in a layer the same learning rate, and allowing such learning rates to vary from layer to
layer.
Importantly, we have normalized the learning rate for a given weight W (ℓ)
i1j1 by the
width of the previous layer nℓ−1, just as we did for the variance of the weight’s initializa-
tion distribution. This normalization is there for much the same reason: the freedom to
tune the weight learning rates separately from the bias learning rates will prove necessary
for having a sensible large-width expansion. Going forward, our training hyperparame-
1However, note that this is not quite as simple as the expression for the evolution of the network
output that we gave in the last chapter (7.19). In particular, the derivative of the loss with respect to
the ℓ-th-layer preactivations needs to be computed by the chain rule as
dLA
dz(ℓ)
i1;α
=
nL
X
j=1
∂LA
∂z(L)
j;α
dz(L)
j;α
dz(ℓ)
i1;α
,
(8.4)
with the error factor ∂LA/∂z(L)
j;α now multiplied by the chain-rule factor dz(L)
j;α /dz(ℓ)
i1;α. For observables
that depend on preactivations from multiple layers, the generalization of (8.3) further involves additional
chain-rule factors as well as a sum over NTKs from diﬀerent layers.
201

ters will consist of the global learning rate η and the individual ℓ-th-layer learning rates
for the biases and weights, λ(ℓ)
b
and λ(ℓ)
W .
Substituting our choice for λµν back into the deﬁnition of the ℓ-th-layer NTK (8.5),
this expression decomposes as
bH(ℓ)
i1i2;α1α2 =
ℓ
X
ℓ′=1


nℓ′
X
j=1

λ(ℓ′)
b
dz(ℓ)
i1;α1
db(ℓ′)
j
dz(ℓ)
i2;α2
db(ℓ′)
j
+ λ(ℓ′)
W
nℓ′−1
nℓ′−1
X
k=1
dz(ℓ)
i1;α1
dW (ℓ′)
jk
dz(ℓ)
i2;α2
dW (ℓ′)
jk



.
(8.7)
Here, the part in the square brackets is the per-layer contribution of the model param-
eters to the ℓ-th-layer NTK, treating the biases and weights separately. We also see
that our intuition above in (8.6) was correct: the ℓ′-th-layer weight learning rate λ(ℓ′)
W
needs to be accompanied by a factor of 1/nℓ′−1 in order to compensate for the additional
summation over the (ℓ′ −1)-th layer neural indices in the second term as compared to
the ﬁrst. Even so, the layer sum in (8.7) makes this expression somewhat unwieldy and
suggests that we should search for an alternate representation.
Following our analysis of the preactivations, let’s try to ﬁnd a recursive expression.
To that end, consider the (ℓ+ 1)-th-layer NTK, bH(ℓ+1)
i1i2;α1α2, and decompose the sum over
layers in its deﬁnition by separating the (ℓ+ 1)-th-layer term from all of the rest, giving
bH(ℓ+1)
i1i2;α1α2 =
nℓ+1
X
j=1

λ(ℓ+1)
b
dz(ℓ+1)
i1;α1
db(ℓ+1)
j
dz(ℓ+1)
i2;α2
db(ℓ+1)
j
+ λ(ℓ+1)
W
nℓ
nℓ
X
k=1
dz(ℓ+1)
i1;α1
dW (ℓ+1)
jk
dz(ℓ+1)
i2;α2
dW (ℓ+1)
jk


(8.8)
+
nℓ
X
j1,j2=1
dz(ℓ+1)
i1;α1
dz(ℓ)
j1;α1
dz(ℓ+1)
i2;α2
dz(ℓ)
j2;α2
bH(ℓ)
j1j2;α1α2 .
Here, the ﬁrst line is the (ℓ+ 1)-th-layer term that we left alone, while the second line
gives the terms from all the other layers after applying the chain rule and then recalling
the deﬁnition (8.7). In this way, the ℓ-th-layer NTK appears naturally. This means that
we can ﬁnd a simple iterative expression for the NTK, similar in spirit to the forward
equation for the preactivations that deﬁnes the MLP.
To ﬁnish our derivation, we need to evaluate the derivatives in (8.8). To do so, recall
the preactivation forward iteration equation
z(ℓ+1)
i;α
= b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;α ,
(8.9)
and remember that the activations are explicit functions of the preactivations σ(ℓ)
i;α ≡
σ

z(ℓ)
i;α

. The factors in the second line of (8.8) coming from the chain rule evaluate to
dz(ℓ+1)
i;α
dz(ℓ)
j;α
= W (ℓ+1)
ij
σ′ (ℓ)
j;α ,
(8.10)
202

while the derivatives with respect to the (ℓ+ 1)-th-layer parameters evaluate to
dz(ℓ+1)
i;α
db(ℓ+1)
j
= δij ,
dz(ℓ+1)
i;α
dW (ℓ+1)
jk
= δij σ(ℓ)
k;α .
(8.11)
All together, we can rewrite (8.8) as
bH(ℓ+1)
i1i2;α1α2 =δi1i2

λ(ℓ+1)
b
+ λ(ℓ+1)
W

1
nℓ
nℓ
X
j=1
σ(ℓ)
j;α1σ(ℓ)
j;α2




(8.12)
+
nℓ
X
j1,j2=1
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
σ′ (ℓ)
j1;α1σ′ (ℓ)
j2;α2 bH(ℓ)
j1j2;α1α2 .
This is the forward equation for the NTK, which is an iteration equation that
computes the NTK layer by layer for any realization of the biases and weights. This
is analogous to the way in which (8.9) computes the network output – as well as all
the hidden-layer preactivations – via a layer-to-layer iteration for a given realization of
model parameters.
Scaling in the eﬀective theory
The forward equation (8.12) further clariﬁes our decomposition (8.6) in which we made
a distinction between the learning rates for the biases and those for the weights, giving
each a diﬀerent scaling with respect to the layer widths nℓof the network.2
To see why, ﬁrst recall from §7 that the change in the training loss after a step of
gradient descent is proportional to the product of the global learning rate η and the
ﬁnal-layer NTK bH(L)
i1i2;α1α2:
∆LA = −η
nL
X
i1,i2=1
X
α1,α2∈A
ϵi1;α1ϵi2;α2 bH(L)
i1i2;α1α2 + O(η2) ,
(8.13)
where here we also recall the deﬁnition of the error factor
ϵi;α ≡∂LA
∂z(L)
i;α
.
(8.14)
Note that this error factor generally stays of order one in the large-width limit, cf. the
explicit expression when using the MSE loss (7.15). Thus, it’s essential that the product
of the global learning rate and the NTK, η bH(L), also stays of order one for large-width
networks: if it diverged as the width increases, then the higher-order terms in (8.13)
would dominate and the loss would no longer be guaranteed to decrease; if instead it
2You’ll have to wait until §9 to understand why it is advantageous to give a layer dependence to λ(ℓ)
b
and λ(ℓ)
W and to learn how they should be scaled with depth.
203

vanished in this limit, then no training would take place. Either way, training would
fail.
With that in mind, we chose the width scaling of our learning-rate tensor so that
the NTK naturally stays of order one in the large-width limit and hence a (suﬃciently
small but not parametrically small) order-one global learning η ensures the success of
training.
In particular, the (ℓ+ 1)-th-layer contribution in the ﬁrst line of the for-
ward equation (8.12) stays of order one if we take λ(ℓ+1)
b
, λ(ℓ+1)
W
= O(1), with the 1/nℓ
normalization of the (ℓ+ 1)-th-layer weight learning rate playing an essential role in
compensating for the summation over the nℓterms.3
If instead we had considered the original version of gradient descent with λµν = δµν
rather than tensorial gradient descent, we would have been in trouble. In the language
of our eﬀective theory, the original gradient descent corresponds to setting λ(ℓ)
b
= 1
and λ(ℓ)
W = nℓ−1, which means that the NTK itself would be O(n). We’d then have to
scale the global learning rate as η = O(1/n) to compensate for this O(n) scaling of the
NTK. However, since in this case ηλ(ℓ)
b
= O(1/n), the order-one contribution from the
weights to the NTK would completely overwhelm the 1/n-suppressed contribution from
the biases. This would lead to both a lack of appropriate contribution of the biases to
the updates of the weights as well as an extreme under-training of the biases themselves.
Finally, let’s make a general point: in any eﬀective theory, it’s really essential to
make all large or small scales explicit – and rescale hyperparameters accordingly – as
we did earlier for the variance of the weight initialization distribution, did here for
the weight learning rate, and will do later for the depth scaling of both the bias and
weight learning rates. For the eﬀective theorist this ensures that the asymptotic 1/n and
1/ℓexpansions are sound and nontrivial, and for the practical practitioner this enables
comparisons of hyperparameter values across architectures with diﬀerent widths and
depths. In particular, we expect very generally that this should help mitigate expensive
hyperparameter tuning, remove the need for heuristic ﬁxes, and increase the robustness
of optimal hyperparameter settings when scaling a model up.
3With this choice, the recursive term in the second line of the forward equation (8.12) also stays of
order one. To see this, let’s evaluate its expectation:
E
"
nℓ
X
j1,j2=1
W (ℓ+1)
i1j1 W (ℓ+1)
i2j2 σ′ (ℓ)
j1;α1σ′ (ℓ)
j2;α2 b
H(ℓ)
j1j2;α1α2
#
=
nℓ
X
j1,j2=1
E
h
W (ℓ+1)
i1j1 W (ℓ+1)
i2j2
i
E
h
σ′ (ℓ)
j1;α1σ′ (ℓ)
j2;α2 b
H(ℓ)
j1j2;α1α2
i
= δi1i2 C(ℓ+1)
W
 
1
nℓ
nℓ
X
j=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 b
H(ℓ)
jj;α1α2
i!
.
(8.15)
In particular, we see that the 1/nℓscaling of the initialization weight variance C(ℓ+1)
W
is important for
ensuring principled behavior of not only the network output, but also the NTK.
204

Getting things backwards
N.B. the chain-rule factors (8.10) also appear when evaluating the derivative of the
network outputs with respect to model parameters
dz(L)
i;α
db(ℓ)
j
=
dz(L)
i;α
dz(ℓ)
j;α
,
dz(L)
i;α
dW (ℓ)
jk
=
X
m
dz(L)
i;α
dz(ℓ)
m;α
dz(ℓ)
m;α
dW (ℓ)
jk
=
dz(L)
i;α
dz(ℓ)
j;α
σ(ℓ−1)
k;α
.
(8.16)
Evaluating these derivatives gives another neural-network iteration equation,
dz(L)
i;α
dz(ℓ)
j;α
=
nℓ+1
X
k=1
dz(L)
i;α
dz(ℓ+1)
k;α
dz(ℓ+1)
k;α
dz(ℓ)
j;α
=
nℓ+1
X
k=1
dz(L)
i;α
dz(ℓ+1)
k;α
W (ℓ+1)
kj
σ′ (ℓ)
j;α
for
ℓ< L ,
(8.17)
but in this case for the derivative of the output. In particular, (8.17) is a backward
equation: starting from the ﬁnal condition
dz(L)
i;α
dz(L)
j;α
= δij ,
(8.18)
we iterate layer-to-layer backwards, ℓ= L−1, L−2, . . . , 1, by sequential multiplications
of the chain-rule factors (8.10).
An algorithm based on this backward equation can be eﬃciently implemented to
compute derivatives with respect to the model parameters and, for that reason, is used
by most deep-learning packages to compute the gradient as part of any neural network
gradient-based learning algorithm. Such a package typically lets practitioners specify
a deep learning model by deﬁning a forward pass – for MLPs a practitioner would
implement the forward equation (8.9) – and then the package will automatically work
out the backward pass – i.e. for MLPs it would implement (8.17).
The computa-
tional algorithm based on (8.17) is termed backpropagation, which was discovered
and rediscovered numerous times in the history of deep learning. Among them, a par-
ticular rediscovery [15] was essential in convincing the machine learning community that
multilayer neural networks can be trained eﬃciently.
All that said, when evaluating the NTK in the eﬀective theory it’s essential that
we use the forward equation (8.12) rather than getting things backwards. In the next
three-plus-one sections, we’ll indeed use the forward equation to recursively compute the
joint initialization distribution for the ℓ-th-layer preactivations and the ℓ-th-layer NTK:
p

z(ℓ), bH(ℓ)D

≡p








z(ℓ)(x1)
z(ℓ)(x2)
. . .
z(ℓ)(xND)
bH(ℓ)(x1, x1)
bH(ℓ)(x1, x2)
. . .
bH(ℓ)(x1, xND)
bH(ℓ)(x2, x1)
bH(ℓ)(x2, x2)
. . .
bH(ℓ)(x2, xND)
...
...
...
...
bH(ℓ)(xND, x1)
bH(ℓ)(xND, x2)
. . .
bH(ℓ)(xND, xND)








.
(8.19)
205

(On the right-hand side, we’ve suppressed neural indices while explicitly writing out the
input dependence. This emphasizes that the preactivations are each functions of a single
input and that the NTK components are each functions of a pair of inputs.)
8.1
First Layer: Deterministic NTK
Recall from §4.1 that at initialization the ﬁrst-layer preactivations,
z(1)
i;α ≡b(1)
i
+
n0
X
j=1
W (1)
ij xj;α ,
(8.20)
are distributed according to a zero-mean Gaussian distribution,
p

z(1)D

=
1
2πG(1)
n1
2
exp

−1
2
n1
X
i=1
X
α1,α2∈D
Gα1α2
(1)
z(1)
i;α1z(1)
i;α2

,
(8.21)
with the ﬁrst-layer deterministic metric – a function of the inputs – given by
G(1)
α1α2 ≡C(1)
b
+ C(1)
W
1
n0
n0
X
j=1
xj;α1xj;α2 .
(8.22)
In particular, the quadratic action in the exponent of (8.21) indicates the absence of
interactions between neurons. This enables us to factor expectation values of ﬁrst-layer
observables into separate Gaussian integrals for each neuron.
The ﬁrst-layer NTK at initialization is even more trivial and can be read oﬀfrom the
original deﬁnition of the NTK (8.7) by plugging in the derivatives (8.11) and remember-
ing the identiﬁcation σ(0)
i;α = xi;α:
bH(1)
i1i2;α1α2 = δi1i2

λ(1)
b
+ λ(1)
W

1
n0
n0
X
j=1
xj;α1xj;α2



≡δi1i2H(1)
α1α2 .
(8.23)
Like the ﬁrst-layer metric, the ﬁrst-layer NTK is completely deterministic – hence no hat
on the right-hand side of the equation – and is diagonal in its neural indices. Remember-
ing our exposition on the oﬀ-diagonal components of the NTK in §7.2, this in particular
means that, for single-layer networks, a feature captured by a particular neuron cannot
aﬀect the gradient-descent update for another feature on any other neuron.
Finally, recalling our discussion of deterministic distributions in §2.3, the joint dis-
tribution of the ﬁrst-layer preactivations and the ﬁrst-layer NTK can be written as
p

z(1), bH(1)D

= p

z(1)D

Y
(i1i2),(α1α2)
δ

bH(1)
i1i2;α1α2 −δi1i2H(1)
α1α2

,
(8.24)
where the product of the Dirac delta functions runs over all pairs of neural indices and
sample indices. Just as the ﬁrst-layer preactivation distribution was representative of
206

deeper layers in the inﬁnite-width limit, this ﬁrst-layer joint distribution is also repre-
sentative of deeper-layer joint distributions in the inﬁnite-width limit: the preactivation
distribution is exactly Gaussian, the NTK distribution is completely deterministic, and
there is no correlation between the two, i.e., they are statistically independent from each
other once the dataset is ﬁxed.
8.2
Second Layer: Fluctuating NTK
Now, let us see how ﬁnite-width corrections can modify this picture in the second layer.
Recall from §4.2 that the second-layer preactivations are given by
z(2)
i;α = b(2)
i
+
n1
X
j=1
W (2)
ij σ(1)
j;α .
(8.25)
After marginalizing over the ﬁrst-layer preactivations z(1), the correlated ﬂuctuations of
the preactivations in the ﬁrst layer resulted in nontrivial interaction between diﬀerent
neurons in the second layer. At the leading nontrivial order in 1/n1, this led to a nearly-
Gaussian distribution with a quartic action (4.60) for the second-layer preactivations,
with the leading non-Gaussianity captured by a nonzero connected four-point correlator.
As for the NTK, looking at its forward equation (8.12) and recalling that the ﬁrst-
layer NTK is deterministic (8.23), we see that the second-layer NTK is given by
bH(2)
i1i2;α1α2 = δi1i2

λ(2)
b
+ λ(2)
W

1
n1
n1
X
j=1
σ(1)
j;α1σ(1)
j;α2



+
n1
X
j=1
W (2)
i1j W (2)
i2j σ′ (1)
j;α1σ′ (1)
j;α2 H(1)
α1α2 .
(8.26)
This second-layer NTK depends on two sets of stochastic variables, the weights W (2)
ij
and the ﬁrst-layer preactivations z(1)
i;α, and hence it ﬂuctuates.
To compute its mean we take an expectation of (8.26), ﬁnding
E
h
bH(2)
i1i2;α1α2
i
(8.27)
=δi1i2

λ(2)
b
+ λ(2)
W

1
n1
n1
X
j=1
E
h
σ(1)
j;α1σ(1)
j;α2
i



+
n1
X
j=1
E
h
W (2)
i1j W (2)
i2j
i
E
h
σ′ (1)
j;α1σ′ (1)
j;α2
i
H(1)
α1α2
=δi1i2
h
λ(2)
b
+ λ(2)
W ⟨σα1σα2⟩G(1) + C(2)
W

σ′
α1σ′
α2

G(1) H(1)
α1α2
i
≡δi1i2 H(2)
α1α2 .
Here, in the second line, the expectation of the recursive term factorized because the
second-layer weights W (2)
ij
are statistically independent from the ﬁrst-layer preactiva-
tions. Additionally, in the third line we recalled (4.27), in which we showed that the
two-point correlators can be expressed as a separate Gaussian expectations for each
207

neuron, with the variance given by the ﬁrst-layer metric G(1).4 Further, inspecting our
answer (8.27), we see that the mean of the second-layer NTK is diagonal in its neural
indices. Furthermore, we separated the part that encodes the sample dependence and
symbolized it by taking oﬀits hat because it is a mean, not a stochastic variable.
Now, let’s compute the variance.
First, deﬁne the second-layer NTK ﬂuctuation
through our usual decomposition,
bH(2)
i1i2;α1α2 ≡δi1i2H(2)
α1α2 + d
∆H
(2)
i1i2;α1α2 ,
(8.28)
so that the expectation of the magnitude of this ﬂuctuation determines the covariance:
E

d
∆H
(2)
i1i2;α1α2 d
∆H
(2)
i3i4;α3α4

= E
h
bH(2)
i1i2;α1α2 bH(2)
i3i4;α3α4
i
−E
h
bH(2)
i1i2;α1α2
i
E
h
bH(2)
i3i4;α3α4
i
.
(8.29)
Substituting in our expression (8.26) for the second-layer stochastic NTK and using the
independence of the second-layer weights from the ﬁrst-layer preactivations, we ﬁnd a
complicated-looking result
E

d
∆H
(2)
i1i2;α1α2 d
∆H
(2)
i3i4;α3α4

(8.30)
= 1
n1
δi1i2δi3i4
( 
λ(2)
W
2 h
⟨σα1σα2σα3σα4⟩G(1) −⟨σα1σα2⟩G(1) ⟨σα3σα4⟩G(1)
i
+ C(2)
W H(1)
α1α2λ(2)
W
h
σ′
α1σ′
α2σα3σα4

G(1) −

σ′
α1σ′
α2

G(1) ⟨σα3σα4⟩G(1)
i
+ λ(2)
W C(2)
W H(1)
α3α4
h
σα1σα2σ′
α3σ′
α4

G(1) −⟨σα1σα2⟩G(1)

σ′
α3σ′
α4

G(1)
i
+

C(2)
W
2 H(1)
α1α2H(1)
α3α4
h
σ′
α1σ′
α2σ′
α3σ′
α4

G(1) −

σ′
α1σ′
α2

G(1)

σ′
α3σ′
α4

G(1)
i )
+ 1
n1
(δi1i3δi2i4 + δi1i4δi2i3)

C(2)
W
2 H(1)
α1α2H(1)
α3α4

σ′
α1σ′
α2σ′
α3σ′
α4

G(1) .
To get this expression, we recalled not only (4.27) for the two-point correlators, but
also both (4.28) and (4.29) for the diﬀerent pairings of the four-point correlators, with
the pairings depending on whether all activations are on the same neuron or are on
two diﬀerent neurons, respectively. As with our computation of the mean above, the
computations of these four-point correlators proceed similarly regardless of whether an
activation has a derivative or not.
To help make sense of this rather ugly expression (8.30), let’s ﬁrst decompose the
4Note that the logic around (4.27) is the same whether or not the Gaussian expectation is of acti-
vations or derivatives of the activation. In other words, for the ﬁrst-layer preactivations we also have
E
h
σ′ (1)
j;α1σ′ (1)
j;α2
i
= ⟨σ′
α1σ′
α2⟩G(1).
208

second-layer NTK variance into a sum of two diﬀerent types of tensors
E

d
∆H
(2)
i1i2;α1α2 d
∆H
(2)
i3i4;α3α4

(8.31)
≡1
n1
h
δi1i2δi3i4A(2)
(α1α2)(α3α4) + δi1i3δi2i4B(2)
α1α3α2α4 + δi1i4δi2i3B(2)
α1α4α2α3
i
.
This decomposition was motivated by the pattern of Kronecker deltas that appear in
(8.30).
Next, by comparing this to our original expression (8.30), we see that these
tensors are given by
A(2)
(α1α2)(α3α4) =
D
bΩ(2)
α1α2 bΩ(2)
α3α4
E
G(1) −
D
bΩ(2)
α1α2
E
G(1)
D
bΩ(2)
α3α4
E
G(1) ,
(8.32)
B(2)
α1α3α2α4 =

C(2)
W
2 H(1)
α1α2H(1)
α3α4

σ′
α1σ′
α2σ′
α3σ′
α4

G(1) ,
(8.33)
where on the ﬁrst line we’ve introduced an auxiliary stochastic variable,
bΩ(2)
α1α2 ≡λ(2)
W σ(1)
α1 σ(1)
α2 + C(2)
W H(1)
α1α2 σ′ (1)
α1 σ′ (1)
α2 ,
(8.34)
in order to remedy the ugliness of what would have otherwise been a very long expres-
sion.5 From (8.32) and (8.33) we see clearly that these tensors are of order one. Given
(8.31), this in turn means that the second-layer NTK variance is suppressed by 1/n1
in the large-width limit. In other words, the second-layer NTK is deterministic in the
strict inﬁnite-width limit, but in backing oﬀthat limit it ﬂuctuates according to (8.31),
(8.32), and (8.33).
Moreover, at ﬁnite width the second-layer NTK not only ﬂuctuates, but also has
nontrivial cross correlation with the second-layer preactivations. This can ultimately
be traced to the fact that the second-layer preactivations (8.25) and the second-layer
NTK (8.26) are both functions of the same stochastic variables: the second-layer weights
W (2)
ij
and the ﬁrst-layer preactivations z(1)
i;α.
This cross correlation can be computed analogously to the way we computed the
NTK mean and variance. Substituting in the deﬁnition of the second-layer preactiva-
tions (8.25) and the second-layer NTK (8.26), and again using the statistical indepen-
5Note that A(2)
(α1α2)(α3α4) (8.32) has the same symmetries as the four-point vertex V (ℓ)
(α1α2)(α3α4). In
particular, it’s symmetric under exchanges of sample indices α1 ↔α2, α3 ↔α4, and (α1α2) ↔(α3α4),
and this symmetry persists to deeper layers, cf. (8.97).
Now, you might raise your hand and say that B(2)
α1α3α2α4 (8.33) also respects the same symmetry.
That’s correct here, but this symmetry will be broken in deeper layers. In general – cf. (8.89) – B(ℓ)
α1α3α2α4
will be symmetric under (α1α2) ↔(α3α4) and (α1α3) ↔(α2α4) but not under α1 ↔α2 or α3 ↔α4
individually.
209

dence of the second-layer weights W (2)
ij
from the ﬁrst-layer preactivations z(1)
i;α, we ﬁnd
E

z(2)
i1;α1 d
∆H
(2)
i2i3;α2α3

= 0 ,
(8.35)
E

z(2)
i1;α1z(2)
i2;α2 d
∆H
(2)
i3i4;α3α4

= E
h
z(2)
i1;α1z(2)
i2;α2 bH(2)
i3i4;α3α4
i
−E
h
z(2)
i1;α1z(2)
i2;α2
i
E
h
bH(2)
i3i4;α3α4
i
= 1
n1
δi1i2δi3i4
(
λ(2)
W C(2)
W
h
⟨σα1σα2σα3σα4⟩G(1) −⟨σα1σα2⟩G(1) ⟨σα3σα4⟩G(1)
i
+

C(2)
W
2 H(1)
α3α4
h 
σα1σα2σ′
α3σ′
α4

G(1) −⟨σα1σα2⟩G(1)

σ′
α3σ′
α4

G(1)
i)
+ 1
n1
(δi1i3δi2i4 + δi1i4δi2i3)

C(2)
W
2 H(1)
α3α4

σα1σα2σ′
α3σ′
α4

G(1) .
(8.36)
Here, as for the variance (8.30), we recalled the suitably generalized versions of (4.27),
(4.28), and (4.29) for the two- and four-point correlators. Thus, we see that the ﬁrst
measure of cross correlation between the second-layer preactivations and the second-layer
NTK (8.35) vanishes, but the second one (8.36) is nonzero at ﬁnite width.
To aid us in our deep-layer analysis, it will be convenient to decompose this cross
correlation (8.36) into two tensors with sample indices only, just as we did for the variance
in (8.31):
E

z(2)
i1;α1z(2)
i2;α2 d
∆H
(2)
i3i4;α3α4

(8.37)
= 1
n1
h
δi1i2δi3i4D(2)
α1α2α3α4 + δi1i3δi2i4F (2)
α1α3α2α4 + δi1i4δi2i3F (2)
α1α4α2α3
i
.
Comparing this decomposition with our explicit formula for the correlator (8.36), we can
identify expressions for these tensors
D(2)
α1α2α3α4 =C(2)
W
hD
σα1σα2 bΩ(2)
α3α4
E
G(1) −⟨σα1σα2⟩G(1)
D
bΩ(2)
α3α4
E
G(1)
i
,
(8.38)
F (2)
α1α3α2α4 =

C(2)
W
2 H(1)
α3α4

σα1σα2σ′
α3σ′
α4

G(1) ,
(8.39)
where we’ve also recalled the stochastic tensor bΩ(2)
α1α2 deﬁned in (8.34).6 Just as for A(2)
and B(2) above, both D(2) and F (2) are manifestly of order one. Similar to the second-
layer NTK variance, this means that the cross correlator (8.37) is suppressed by 1/n1 in
the large-width limit, vanishing in the strict inﬁnite-width limit.
In summary, the joint distribution of the second-layer preactivations and second-layer
NTK,
p

z(2), bH(2)D

,
(8.40)
6The cross-correlation tensor D(2)
α1α2α3α4 (8.38) – and more generally D(ℓ)
α1α2α3α4 in deeper layers,
cf. (8.77) – is symmetric under exchanges of sample indices α1 ↔α2 and α3 ↔α4, but of course not
under (α1α2) ↔(α3α4). The other tensor F (2)
α1α3α2α4 (8.39) respects this same symmetry in the second
layer, but has no symmetry at all in deeper layers, cf. (8.79).
210

at leading nontrivial order in the 1/n expansion is nearly-Gaussian distribution with (i)
a quartic interaction among preactivations on diﬀerent neurons, (ii) a ﬂuctuating NTK,
and (iii) cross correlation between the preactivations and NTK. All of these ﬁnite-width
eﬀects become more complicated for deeper layers.
8.3
Deeper Layers: Accumulation of NTK Fluctuations
As before with §4.1 ∥§8.1 and §4.2 ∥§8.2, this section parallels §4.3. In §4.3, we inves-
tigated the nearly-Gaussian distribution of preactivations p

z(ℓ+1)D

at ﬁnite width
by considering an interlayer joint distribution p

z(ℓ+1), z(ℓ)D

and then integrating out
the ℓ-th-layer preactivations. In particular, due to correlated dependence on the pre-
activations in previous layers, the non-Gaussianity in the preactivation distribution ac-
cumulated as depth increased, manifesting itself in the running four-point vertex V (ℓ).
This same mechanism makes the NTK ﬂuctuations accumulate, amplifying the NTK
variance as well as the cross correlation between the NTK and preactivations. In this
section, we will derive recursions for the NTK mean, the NTK-preactivation cross cor-
relation, and the NTK variance that together determine the ℓ-th-layer joint distribution
at leading nontrivial order in 1/n. What follows is a goode olde calculation, so please
sharpen your quills, unfurl your parchment, and inform your majordomo that you require
a cleared schedule for the rest of the day.
8.3.0
Interlude: Interlayer Correlations
If you have an eidetic memory, then perhaps you recall that the main complication with
our derivation of the general (ℓ+ 1)-th-layer preactivation statistics – as compared to
the second layer statistics – was that the ℓ-th-layer preactivation distribution p

z(ℓ)D

was also non-Gaussian, unlike the Gaussian preactivation distribution in the ﬁrst layer.
For such a nearly-Gaussian distribution p

z(ℓ)D

, interactions imply a nontrivial in-
tralayer correlation between observables of the preactivations across diﬀerent neurons
i1 ̸= i2. Speciﬁcally, the covariance of two arbitrary single-neuron functions F

z(ℓ)
i1;A1

and G

z(ℓ)
i2;A2

depending on data subsamples A1, A2 ⊂D, respectively, is given by (4.64)
and reprinted here:
Cov
h
F

z(ℓ)
i1;A1

, G

z(ℓ)
i2;A2
i
≡E
h
F

z(ℓ)
i1;A1

G

z(ℓ)
i2;A2
i
−E
h
F

z(ℓ)
i1;A1
i
E
h
G

z(ℓ)
i2;A2
i
=
X
β1,...,β4∈D
1
4nℓ−1
V (β1β2)(β3β4)
(ℓ)
D
zβ1zβ2 −G(ℓ)
β1β2

F(zA1)
E
G(ℓ)
D
zβ3zβ4 −G(ℓ)
β3β4

G(zA2)
E
G(ℓ)
+ O
 1
n2

.
(8.41)
In this reprinting, we implicitly substituted in our leading large-width expressions (4.81)
and (4.82) for the quadratic coupling g(ℓ) and quartic coupling v(ℓ), respectively. We
211

have also recalled our long-forgotten shorthand notation for the covariance of random
variables (1.53), which we will use judiciously throughout this section. This intralayer
formula will soon prove itself useful.
Enlarging our view to the preactivation-NTK joint distribution (8.19), we’ll en-
counter another complication due to interlayer correlation of the form
E
h
O

z(ℓ+1)
P

W (ℓ+1)
Q

z(ℓ), bH(ℓ)i
,
(8.42)
where O is some function of (ℓ+ 1)-th-layer preactivations, P is a polynomial of (ℓ+ 1)-
th-layer weights, and Q is a function of ℓ-th-layer preactivations and the ℓ-th-layer NTK.
For instance, taking the NTK-preactivation cross correlation
E
h
z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2 bH(ℓ+1)
i3i4;α3α4
i
(8.43)
and unraveling the NTK through its forward equation (8.12), we get an interlayer corre-
lation of the form (8.42) with P = 1 from the additive term in the square brackets and
an interlayer correlation of the same form with P = W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
from the recursive
term. While it was simple enough to evaluate such an expectation for the second layer,
it’s somewhat subtle for a general layer.
That said, there’s actually a pretty neat trick that lets us reduce such interlayer
correlations (8.42) to expectations of solely ℓ-th-layer variables. Such expectations can
subsequently be evaluated with the intralayer formula (8.41) above. Let us now teach
you this magic trick before diving deep into learning the deeper-layer analysis.7
First, using the deﬁnition of the expectation and the conditional structure of the
distribution, the interlayer correlation (8.42) can be expressed as (suppressing all indices)
E
h
O

z(ℓ+1)
P

W (ℓ+1)
Q

z(ℓ), bH(ℓ)i
(8.44)
=
Z
dz(ℓ)d bH(ℓ)p

z(ℓ), bH(ℓ)D

Q

z(ℓ), bH(ℓ)
×
" Z
db(ℓ+1)dW (ℓ+1)p

b(ℓ+1)
p

W (ℓ+1)
P

W (ℓ+1)
×
Z
dz(ℓ+1)p

z(ℓ+1)b(ℓ+1), W (ℓ+1), z(ℓ)
O

z(ℓ+1) #
.
Our strategy will be to integrate out or marginalize over the (ℓ+ 1)-th-layer parameters
in order to express the object inside the square bracket as a function of the ℓ-th-layer
variables only. In this way, the entire object will become an ℓ-th-layer expectation that
we already know how to handle.
Second, rather than working with an abstract polynomial P, let’s construct a gener-
ating function for these interlayer correlations through the use of a source term:
P

W (ℓ+1)
= e
P
i,j JijW (ℓ+1)
ij
.
(8.45)
7As similar interlayer correlations appear in §11, we’ll keep our exposition completely general rather
than specializing to the NTK-preactivation cross correlation (8.43).
212

Recall from your pretraining days (§1.1) that a generating function such as (8.45) could
be used to evaluate expectations such as (8.42) with any polynomial insertions of weights
W (ℓ+1)
ij
. To do this, we diﬀerentiate the evaluated generating function some number of
times with respect to the source Jij and then set the source to zero.
Now with our choice (8.45) in mind for P, we can explicitly evaluate the expression in
the square brackets in (8.44) as follows: (i) recall the initialization distributions for the
biases (2.21) and weights (2.22), (ii) recall from (2.34) that the conditional distribution
p

z(ℓ+1)b(ℓ+1), W (ℓ+1), z(ℓ)
encodes the MLP forward equation (8.9) as a Dirac delta
function, and ﬁnally (iii) recall the integral representation of the Dirac delta function
(2.32). All together, this gives the following set of integrals
Z 
Y
i
dbi
q
2πC(ℓ+1)
b



Y
i,j
dWij
q
2πC(ℓ+1)
W
/nℓ



Y
i,α
dΛ α
i dz(ℓ+1)
i;α
2π

O

z(ℓ+1)
(8.46)
× exp

−
X
i
b2
i
2C(ℓ+1)
b
−
X
i,j
nℓW 2
ij
2C(ℓ+1)
W
+ i
X
i,α
Λ α
i

z(ℓ+1)
i;α
−bi −
X
j
Wij xj;α

+
X
i,j
JijWij

,
which we recognize as the good-old Hubbard-Stratonovich transformation that we ﬁrst
encountered in §4.1.
Next, as we did in §4.1 and in high school, we can complete the squares with respect
to the biases and weights and integrate them out. The only substantial deviation here
from the presentation in §4.1 is that the source term shifts the linear coupling of the
weights as
−iWij
X
α
Λ α
i σ(ℓ)
j;α →−iWij
 X
α
Λ α
i σ(ℓ)
j;α + iJij
!
.
(8.47)
Performing these Gaussian integrals, we ﬁnd
Z 
Y
i,α
dΛ α
i dz(ℓ+1)
i;α
2π

O

z(ℓ+1)
exp
"
−
X
i,α1,α2
Λ α1
i
Λ α2
i

C(ℓ+1)
b
2
+ C(ℓ+1)
W
2nℓ
X
j
σ(ℓ)
α1;jσ(ℓ)
α2;j


+ i
X
i,α
Λ α
i

z(ℓ+1)
i;α
−C(ℓ+1)
W
nℓ
X
j
Jijσ(ℓ)
j;α

+ C(ℓ+1)
W
2nℓ
X
i,j
J 2
ij
#
. (8.48)
Just as in our previous Hubbard-Stratonoviching (4.20), the stochastic metric (4.70),
bG(ℓ+1)
α1α2 ≡C(ℓ+1)
b
+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
σ(ℓ)
j;α1σ(ℓ)
j;α2 ,
(8.49)
appears in the quadratic term of the Hubbard-Stratonovich variables Λ α
i , while the
linear term is slightly modiﬁed by a shifting of the preactivations with the subtraction
of the quantity
c
Mi;α ≡C(ℓ+1)
W

1
nℓ
nℓ
X
j=1
Jijσ(ℓ)
j;α

.
(8.50)
213

Completing the squares with the Hubbard-Stratonovich variables and integrating them
out, we get
exp

C(ℓ+1)
W
2nℓ
P
i,j J 2
ij

r2π bG(ℓ+1)

nℓ+1
Z 
Y
i,α
dz(ℓ+1)
i;α

O

z(ℓ+1)
(8.51)
× exp
"
−1
2
X
i
X
α1,α2
bGα1α2
(ℓ+1)

z(ℓ+1)
i;α1
−c
Mi;α1
 
z(ℓ+1)
i;α2
−c
Mi;α2
#
.
Ignoring the quadratic source factor J 2
ij outside the integral, this is just a Gaussian
expectation of O against the (ℓ+ 1)-th-layer preactivation distribution with a mean
c
Mi;α and a variance bG(ℓ+1)
α1α2 . (Make sure you remember our general relativity convention:
bGα1α2
(ℓ+1) is the inverse of bG(ℓ+1)
α1α2 .)
Now, let’s compensate for this mean by shifting the dummy integration variable as
z(ℓ+1)
i;α
→z(ℓ+1)
i;α
+ c
Mi;α1, which yields a compact expression in terms of our zero-mean
Gaussian expectation notation (4.45):
exp

C(ℓ+1)
W
2nℓ
X
i,j
J 2
ij


DD
O

z(ℓ+1) + c
M
EE
b
G(ℓ+1) .
(8.52)
Plugging this result (8.52) back into our interlayer correlation (8.42) and substituting
back in for the mean shift (8.50), we arrive at a simple formula for our generating
function:
E

O

z(ℓ+1)
e
P
i,j JijW (ℓ+1)
ij
Q

z(ℓ), bH(ℓ)
(8.53)
= exp

C(ℓ+1)
W
2nℓ
X
i,j
J 2
ij

E


**
O

z(ℓ+1)
i;α
+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
Jijσ(ℓ)
j;α
++
b
G(ℓ+1)
Q

z(ℓ), bH(ℓ)

.
After performing the Gaussian expectation over the (ℓ+1)-th-layer preactivations z(ℓ+1)
i;α
– which is typically trivial in all the concrete applications that we’ll encounter – the
expectation in (8.53) is only with respect to ℓ-th-layer variables.8 This was our desired
result.
To see how to use the generating function (8.53), let’s work out some explicit ex-
amples. First, consider the case with no weight insertions. Setting the source to zero,
Jij = 0, we ﬁnd
E
h
O

z(ℓ+1)
Q

z(ℓ), bH(ℓ)i
= E
DD
O

z(ℓ+1)EE
b
G(ℓ+1) Q

z(ℓ), bH(ℓ)
.
(8.54)
This formula is not trivial and is not something we knew before: here we see that the
correlation between preactivations in neighboring layers is given by ﬁrst computing a
8Recall that the stochastic metric bG(ℓ+1) (8.49) depends only on the ℓ-th-layer preactivations.
214

Gaussian expectation of the (ℓ+ 1)-th-layer function against the stochastic metric and
then taking the full expectation of the resulting ℓ-th-layer quantity.
Next, let’s consider two weight insertions. Twice-diﬀerentiating the generating func-
tion (8.53) by the source as
d
dJi3j3
d
dJi4j4 and then setting the source to zero, we get
E
h
O

z(ℓ+1)
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
Q

z(ℓ), bH(ℓ)i
(8.55)
=δi3i4δj3j4
C(ℓ+1)
W
nℓ
E
h
⟨⟨O⟩⟩b
G(ℓ+1) Q

z(ℓ), bH(ℓ)i
+
 
C(ℓ+1)
W
nℓ
!2
X
β3,β4,γ3,γ4
E
" DD
z(ℓ+1)
i3;β3 z(ℓ+1)
i4;β4 −δi3i4 bG(ℓ+1)
β3β4

O
EE
b
G(ℓ+1)
× bGβ3γ3
(ℓ+1) bGβ4γ4
(ℓ+1)σ(ℓ)
j3;γ3σ(ℓ)
j4;γ4Q

z(ℓ), bH(ℓ) #
.
Here, we used integration by parts to exchange the derivatives for a projection as
**
∂2O
∂z(ℓ+1)
i3;γ3 ∂z(ℓ+1)
i4;γ4
++
b
G(ℓ+1)
=
X
β3,β4
bGβ3γ3
(ℓ+1) bGβ4γ4
(ℓ+1)
DD
z(ℓ+1)
i3;β3 z(ℓ+1)
i4;β4 −δi3i4 bG(ℓ+1)
β3β4

O
EE
b
G(ℓ+1) .
(8.56)
Intuitively, the ﬁrst term in (8.55) comes from forming a Wick contraction with the two
weight insertions, while the second term comes from two pairs of Wick contractions,
each between an inserted weight and a weight hidden inside the z(ℓ+1)’s in O.
Thusly, with the intralayer formula (8.41) recalled and the interlayer formulae (8.54)
and (8.55) derived, we are as ready as we’ll ever be to recursively analyze the joint
statistics of the NTK and preactivations in deeper layers. This concludes our interlude.
8.3.1
NTK Mean
Taking the expectation of the stochastic NTK forward equation (8.12), we get
E
h
bH(ℓ+1)
i1i2;α1α2
i
=δi1i2

λ(ℓ+1)
b
+ λ(ℓ+1)
W

1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i




(8.57)
+ δi1i2C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 bH(ℓ)
jj;α1α2
i
,
where, as is now familiar, on the second line we used the independence of the (ℓ+ 1)-
th-layer weights from the ℓ-th-layer preactivations, and then immediately evaluated the
weight expectation. Immediately, we see that NTK mean is diagonal in neural indices
at any network depth.
Given that, let’s decompose the ℓ-th-layer NTK into a mean and ﬂuctuation as
bH(ℓ)
i1i2;α1α2 ≡δi1i2H(ℓ)
α1α2 + d
∆H
(ℓ)
i1i2;α1α2 ,
(8.58)
215

where we have denoted the ℓ-th-layer NTK mean as δi1i2H(ℓ)
α1α2. As before, we separated
the part of the mean that encodes the sample dependence and symbolized it without a
hat. Substituting this decomposition into our expression (8.57) for the NTK mean, we
see that the (ℓ+ 1)-th-layer mean obeys a recursion
H(ℓ+1)
α1α2 =λ(ℓ+1)
b
+ λ(ℓ+1)
W

1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i

+ C(ℓ+1)
W
H(ℓ)
α1α2

1
nℓ
nℓ
X
j=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2
i


+ C(ℓ+1)
W

1
nℓ
nℓ
X
j=1
E

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2

,
(8.59)
depending on both the mean and ﬂuctuation in the previous layer ℓ.
To the leading order in 1/n, the ﬁrst two expectation values on the right-hand side
of (8.59) are given by Gaussian expectations
E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
= ⟨σα1σα2⟩G(ℓ) + O
 1
n

,
(8.60)
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2
i
=

σ′
α1σ′
α2

G(ℓ) + O
 1
n

.
(8.61)
To see this, note that the ﬁrst expectation is just the leading Gaussian contribution (4.61)
with the non-Gaussian coupling v suppressed as ∼1/n as per (4.82), and that the
evaluation of the second expectation proceeds identically to the ﬁrst regardless of whether
the activation has a derivative or not. Meanwhile, the ﬁnal expectation on the second
line of (8.59) involves an NTK-preactivation cross correlation, which is also suppressed
in the large-width limit:
E

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2

= O
 1
n

.
(8.62)
We will prove this rather shortly in the next subsection in (8.71).
Assembling these leading contributions, the NTK mean recursion simpliﬁes to
H(ℓ+1)
α1α2 =λ(ℓ+1)
b
+ λ(ℓ+1)
W
⟨σα1σα2⟩G(ℓ) + C(ℓ+1)
W

σ′
α1σ′
α2

G(ℓ) H(ℓ)
α1α2 + O
 1
n

.
(8.63)
If you’re in the habit of marking up your book, feel free to draw a box around this
formula.
8.3.2
NTK-Preactivation Cross Correlations
Next, let’s evaluate a cross-correlation expectation of a very general form
E

O

z(ℓ+1) d
∆H
(ℓ+1)
i3i4;α3α4

.
(8.64)
216

For instance, setting O = z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2 gives the elementary cross correlation (8.43), while
setting O = σ′ (ℓ)
i1;α1σ′ (ℓ)
i2;α2 gives the subleading cross correlation (8.62) that just appeared
in (and then immediately disappeared from) our recursion for the NTK mean.
To begin, simply substitute the NTK forward equation (8.12) into the cross correla-
tor (8.64), which yields
E

O

z(ℓ+1) d
∆H
(ℓ+1)
i3i4;α3α4

= Cov
h
O

z(ℓ+1)
, bH(ℓ+1)
i3i4;α3α4
i
(8.65)
=δi3i4 λ(ℓ+1)
W
1
nℓ
nℓ
X
j=1
n
E
h
O

z(ℓ+1)
σ(ℓ)
j;α3σ(ℓ)
j;α4
i
−E
h
O

z(ℓ+1)i
E
h
σ(ℓ)
j;α3σ(ℓ)
j;α4
io
+
nℓ
X
j3,j4=1
n
E
h
O

z(ℓ+1)
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
σ′ (ℓ)
j3;α3σ′ (ℓ)
j4;α4 bH(ℓ)
j3j4;α3α4
i
−E
h
O

z(ℓ+1)i
E
h
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
i
E
h
σ′ (ℓ)
j3;α3σ′ (ℓ)
j4;α4 bH(ℓ)
j3j4;α3α4
i o
.
Now putting the freshly-derived interlayer formulae (8.54) and (8.55) to use, this cross
correlator becomes
E

O

z(ℓ+1) d
∆H
(ℓ+1)
i3i4;α3α4

(8.66)
=δi3i4
λ(ℓ+1)
W
C(ℓ+1)
W
E
DD
O

z(ℓ+1)EE
b
G(ℓ+1) d
∆G
(ℓ+1)
α3α4

+ δi3i4
C(ℓ+1)
W
nℓ
nℓ
X
j=1
n
E
hDD
O

z(ℓ+1)EE
b
G(ℓ+1) σ′ (ℓ)
j;α3σ′ (ℓ)
j;α4 bH(ℓ)
jj;α3α4
i
−E
hDD
O

z(ℓ+1)EE
b
G(ℓ+1)
i
E
h
σ′ (ℓ)
j;α3σ′ (ℓ)
j;α4 bH(ℓ)
jj;α3α4
i o
+
 
C(ℓ+1)
W
nℓ
!2
nℓ
X
j3,j4=1
X
β3,β4,γ3,γ4
E
" DD
z(ℓ+1)
i3;β3 z(ℓ+1)
i4;β4 −δi3i4 bG(ℓ+1)
β3β4

O

z(ℓ+1)EE
b
G(ℓ+1)
× bGβ3γ3
(ℓ+1) bGβ4γ4
(ℓ+1)σ(ℓ)
j3;γ3σ(ℓ)
j4;γ4σ′ (ℓ)
j3;α3σ′ (ℓ)
j4;α4 bH(ℓ)
j3j4;α3α4
#
.
Here, for the ﬁrst term, we also recalled the deﬁnition of the metric ﬂuctuation (4.74).
From this general expression, we can already learn two important lessons.
First, setting O = z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2 , we get an expression for the elementary (ℓ+1)-th-layer
217

cross correlation in terms of ℓ-th-layer variables
E

z(ℓ+1)
i1;α1 z(ℓ+1)
i2;α2 d
∆H
(ℓ+1)
i3i4;α3α4

=δi1i2δi3i4



λ(ℓ+1)
W
C(ℓ+1)
W
E

d
∆G
(ℓ+1)
α1α2 d
∆G
(ℓ+1)
α3α4

+ C(ℓ+1)
W
1
nℓ
nℓ
X
j=1
E

d
∆G
(ℓ+1)
α1α2 σ′ (ℓ)
j;α3σ′ (ℓ)
j;α4 bH(ℓ)
jj;α3α4



+ δi1i3δi2i4
 
C(ℓ+1)
W
nℓ
!2
nℓ
X
j,k=1
E
"
σ(ℓ)
j;α1σ(ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 bH(ℓ)
jk;α3α4
#
+ δi1i4δi2i3
 
C(ℓ+1)
W
nℓ
!2
nℓ
X
j,k=1
E
"
σ(ℓ)
j;α2σ(ℓ)
k;α1σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 bH(ℓ)
jk;α3α4
#
≡1
nℓ
h
δi1i2δi3i4D(ℓ+1)
α1α2α3α4 + δi1i3δi2i4F (ℓ+1)
α1α3α2α4 + δi1i4δi2i3F (ℓ+1)
α1α4α2α3
i
,
(8.67)
where on the ﬁnal line we decomposed the cross correlation into two tensors with sample
indices only, just as we did for the second layer in (8.37). Equating the ﬁrst expres-
sion with the second, we see that these tensors are deﬁned by the following ℓ-th-layer
expectations:
1
nℓ
D(ℓ+1)
α1α2α3α4 ≡λ(ℓ+1)
W
C(ℓ+1)
W
E

d
∆G
(ℓ+1)
α1α2 d
∆G
(ℓ+1)
α3α4

(8.68)
+ C(ℓ+1)
W

1
nℓ
nℓ
X
j=1
E

d
∆G
(ℓ+1)
α1α2 σ′ (ℓ)
j;α3σ′ (ℓ)
j;α4 bH(ℓ)
jj;α3α4

,
1
nℓ
F (ℓ+1)
α1α3α2α4 ≡

C(ℓ+1)
W
2

1
n2
ℓ
nℓ
X
j,k=1
E
"
σ(ℓ)
j;α1σ(ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 bH(ℓ)
jk;α3α4
#
.
(8.69)
We’ll come back to evaluate these last two expressions – and thereby derive recursions
for both cross correlation tensors – after we reveal the second lesson.
Second, we can start again from the general cross correlator (8.66) and push our
calculation a little bit further to leading order in 1/n. The key step is perturbatively
expanding the Gaussian expectation – just as we expanded the stochastic Gaussian
distribution (4.56) before using the Schwinger-Dyson equation (4.55) – to get
DD
O

z(ℓ+1)EE
b
G(ℓ+1)
(8.70)
=
DD
O

z(ℓ+1)EE
G(ℓ+1)
+ 1
2
X
β1,β2,γ1,γ2
**X
m

z(ℓ+1)
m;β1 z(ℓ+1)
m;β2 −G(ℓ+1)
β1β2

O

z(ℓ+1)++
G(ℓ+1)
Gβ1γ1
(ℓ+1)Gβ2γ2
(ℓ+1) d
∆G
(ℓ+1)
γ1γ2 + O

∆2
.
Plugging this back into (8.66), picking up the (leading-order) pieces, and using the
218

deﬁnitions (8.68) and (8.69), we get
E

O

z(ℓ) d
∆H
(ℓ)
i3i4;α3α4

(8.71)
=δi3i4
1
nℓ−1

1
2
X
β1,β2,γ1,γ2
** nℓ
X
m=1

z(ℓ)
m;β1z(ℓ)
m;β2 −G(ℓ)
β1β2

O

z(ℓ)++
G(ℓ)
Gβ1γ1
(ℓ) Gβ2γ2
(ℓ)

D(ℓ)
γ1γ2α3α4
+
1
nℓ−1
X
β1,β2,γ1,γ2
DD
z(ℓ)
i3;β1z(ℓ)
i4;β2 −δi3i4G(ℓ)
β1β2

O

z(ℓ)EE
G(ℓ) Gβ1γ1
(ℓ) Gβ2γ2
(ℓ) F (ℓ)
γ1α3γ2α4
+ O
 1
n2

,
where we have also relabeled layer indices as (ℓ+ 1) →ℓeverywhere for the ease of
later substitutions.9 This result illustrates that these more general cross correlations are
governed by the same tensors, D(ℓ) and F (ℓ), as the elementary cross correlation (8.67).
We can indeed compute all the cross correlators if we ﬁnd and solve recursions for D(ℓ)
and F (ℓ). It is this task that we turn to next.
D-recursion
Starting from our expression for D(ℓ+1) (8.68) and substituting in the deﬁnition of the
stochastic metric (8.49), we get
D(ℓ+1)
α1α2α3α4 =C(ℓ+1)
W
1
nℓ
nℓ
X
j,k=1
Cov
h
σ(ℓ)
j;α1σ(ℓ)
j;α2, λ(ℓ+1)
W
σ(ℓ)
k;α3σ(ℓ)
k;α4 + C(ℓ+1)
W
H(ℓ)
α3α4σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
i
+

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j,k=1
Cov

σ(ℓ)
j;α1σ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

+ O
 1
n

,
(8.72)
where we have again decomposed the ℓ-th-layer NTK into a mean and ﬂuctuation piece.
We see that there are two types of terms here: covariances on a single neuron j = k and
covariances between pairs of neurons j ̸= k.
For the single-neuron contribution with j = k, at the leading order, we ﬁnd the same
contribution that we found for the second layer (8.38)10
C(ℓ+1)
W
hD
σα1σα2 bΩ(ℓ+1)
α3α4
E
G(ℓ) −⟨σα1σα2⟩G(ℓ)
D
bΩ(ℓ+1)
α3α4
E
G(ℓ)
i
+ O
 1
n

.
(8.73)
Here, the auxiliary stochastic matrix bΩ(ℓ+1)
α1α2 is deﬁned as
bΩ(ℓ+1)
α1α2 ≡λ(ℓ+1)
W
σ(ℓ)
α1 σ(ℓ)
α2 + C(ℓ+1)
W
H(ℓ)
α1α2σ′ (ℓ)
α1 σ′ (ℓ)
α2 ,
(8.74)
9You might worry about the summation Pnℓ
m=1 inside the ﬁrst Gaussian expectation in (8.71). How-
ever, due to Gaussian factorization, this expectation stays of order one so long as the observable O
depends on only a ﬁnite number of neurons.
10The subleading O(1/n) piece includes both a contribution from the non-Gaussian part of the distri-
bution as well as a cross correlation contribution from the previous layer.
219

which simply generalizes the second-layer deﬁnition (8.34). As a reminder the unhatted
matrix H(ℓ)
α1α2 is the NTK mean, which is not a random variable and can safely be taken
outside the Gaussian expectation ⟨·⟩G(ℓ).
This means that, as a stochastic variable,
bΩ(ℓ+1)
α1α2 depends only on the ℓ-th-layer preactivations.
Next, for the pairs-of-neurons contribution to (8.72) with j ̸= k, the ﬁrst term can
be evaluated by the intralayer formula (8.41) and yields
nℓ
4nℓ−1
C(ℓ+1)
W
X
γ1,γ2,γ3,γ4
V (γ1γ2)(γ3γ4)
(ℓ)
D
zγ1zγ2−G(ℓ)
γ1γ2

σα1σα2
E
G(ℓ)
D
zγ3zγ4−G(ℓ)
γ3γ4

bΩ(ℓ+1)
α3α4
E
G(ℓ) .
(8.75)
Meanwhile, the covariance in the second term can be unrolled as
Cov

σ(ℓ)
j;α1σ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

(8.76)
=E

σ(ℓ)
j;α1σ(ℓ)
j;α2σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

−E
h
σ(ℓ)
j;α1σ(ℓ)
j;α2
i
E

σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

=
1
2nℓ−1
X
β1,β2,γ1,γ2
D
zβ1zβ2−G(ℓ)
β1β2

σα1σα2
E
G(ℓ)

σ′
α3σ′
α4

G(ℓ)Gβ1γ1
(ℓ) Gβ2γ2
(ℓ) D(ℓ)
γ1γ2α3α4+ O
 1
n2

,
where in the last line we used the cross-correlation formula (8.71) with the observables
O = σ(ℓ)
j;α1σ(ℓ)
j;α2σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 and O = σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4, respectively. Combining these contribu-
tions with the ﬁrst piece (8.73), we get our desired recursion:
D(ℓ+1)
α1α2α3α4
(8.77)
=C(ℓ+1)
W
D
σα1σα2 bΩ(ℓ+1)
α3α4
E
G(ℓ) −⟨σα1σα2⟩G(ℓ)
D
bΩ(ℓ+1)
α3α4
E
G(ℓ)

+
nℓ
4nℓ−1
C(ℓ+1)
W
X
γ1,γ2,γ3,γ4
V (γ1γ2)(γ3γ4)
(ℓ)
D
zγ1zγ2−G(ℓ)
γ1γ2

σα1σα2
E
G(ℓ)
D
zγ3zγ4−G(ℓ)
γ3γ4

bΩ(ℓ+1)
α3α4
E
G(ℓ)
+
nℓ
2nℓ−1

C(ℓ+1)
W
2 X
β1,β2,γ1,γ2
D(ℓ)
γ1γ2α3α4
D
zβ1zβ2−G(ℓ)
β1β2

σα1σα2
E
G(ℓ)Gβ1γ1
(ℓ) Gβ2γ2
(ℓ)

σ′
α3σ′
α4

G(ℓ)
+ O
 1
n

.
Interestingly, we see that at leading order the D-type cross correlation in layer (ℓ+ 1)
mixes D-type correlations from layer ℓwith the four-point vertex V (ℓ), but does not mix
with the F-type cross correlations or any part of the NTK variance.
220

F-recursion
Starting from our expression for F (ℓ+1) (8.69) and decomposing the NTK into a mean
and ﬂuctuation, we get
F (ℓ+1)
α1α3α2α4 =

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j=1
E
h
σ(ℓ)
j;α1σ′ (ℓ)
j;α3σ(ℓ)
j;α2σ′ (ℓ)
j;α4
i
H(ℓ)
α3α4
(8.78)
+

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j,k=1
E

σ(ℓ)
j;α1σ′ (ℓ)
j;α3σ(ℓ)
k;α2σ′ (ℓ)
k;α4
d
∆H
(ℓ)
jk;α3α4

.
At leading order, the ﬁrst term simply becomes a single-neuron Gaussian expectation;
the second term can be evaluated with the cross-correlation formula (8.71), where the
diagonal sum with j = k is of order O(1/n) and can be neglected while the oﬀ-diagonal
sum with j ̸= k yields the term involving F (ℓ). All together, this gives
F (ℓ+1)
α1α3α2α4 =

C(ℓ+1)
W
2 
σα1σα2σ′
α3σ′
α4

G(ℓ) H(ℓ)
α3α4
(8.79)
+
nℓ
nℓ−1

C(ℓ+1)
W
2 X
β1,β2,γ1,γ2

σα1σ′
α3zβ1

G(ℓ)

σα2σ′
α4zβ2

G(ℓ) Gβ1γ1
(ℓ) Gβ2γ2
(ℓ)
F (ℓ)
γ1α3γ2α4
+ O
 1
n

.
As with the D-recursion before, the ﬁrst term was present in the second-layer F (2)
(8.39), while the second term is a direct consequence of having a ﬂuctuating NTK in the
previous layer ℓ. Additionally, we see that at leading order the F-type cross correlation
doesn’t mix at all with any of our other ﬁnite-width tensors.
Finally, before moving on to discuss the NTK variance, let us note that the recursions
for both D(ℓ) and F (ℓ) – combined with the initial condition D(1) = F (1) = 0 from the
ﬁrst layer where the NTK is deterministic – ensure that they each stay of order one.
Given the factor of 1/nℓin the decomposition of the cross correlation into these tensors
(8.67) and our “second lesson” encapsulated by the cross-correlation formula (8.71), this
means that any and all cross correlations are suppressed in the 1/n expansion and vanish
identically in the strict inﬁnite-width limit.
8.3.3
NTK Variance
Now let’s ﬁnally slay the beast that is the NTK variance. Similar to the NTK-preactivation
cross correlation, the NTK-variance calculation in deeper layers diﬀers from the second-
layer calculation due to nontrivial intralayer correlations (8.41) in the previous layer and
due to the ﬂuctuating NTK (8.58).
The NTK variance is given by the expected magnitude of the NTK ﬂuctuation
E

d
∆H
(ℓ+1)
i1i2;α1α2 d
∆H
(ℓ+1)
i3i4;α3α4

= Cov
h
bH(ℓ+1)
i1i2;α1α2, bH(ℓ+1)
i3i4;α3α4
i
.
(8.80)
221

To begin our calculation, let us plug the NTK forward equation (8.12) into this deﬁning
expression and then integrate out the weights W (ℓ+1), which is easy since they are
independent random variables. Although there are many terms, the algebra is mostly
straightforward:
E

d
∆H
(ℓ+1)
i1i2;α1α2 d
∆H
(ℓ+1)
i3i4;α3α4

(8.81)
=δi1i2δi3i4
1
n2
ℓ
nℓ
X
j,k=1
( 
λ(ℓ+1)
W
2 Cov
h
σ(ℓ)
j;α1σ(ℓ)
j;α2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
+

λ(ℓ+1)
W

C(ℓ+1)
W
Cov
h
σ(ℓ)
j;ασ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 bH(ℓ)
kk;α3α4
i
+

λ(ℓ+1)
W

C(ℓ+1)
W
Cov
h
σ′ (ℓ)
j;α σ′ (ℓ)
j;α2 bH(ℓ)
jj;αα2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
+

C(ℓ+1)
W
2 Cov
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 bH(ℓ)
jj;α1α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 bH(ℓ)
kk;α3α4
i )
+ δi1i3δi2i4

C(ℓ+1)
W
2 1
n2
ℓ
nℓ
X
j,k=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2 bH(ℓ)
jk;α1α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 bH(ℓ)
jk;α3α4
i
+ δi1i4δi2i3

C(ℓ+1)
W
2 1
n2
ℓ
nℓ
X
j,k=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2 bH(ℓ)
jk;α1α2σ′ (ℓ)
k;α3σ′ (ℓ)
j;α4 bH(ℓ)
kj;α3α4
i
.
In obtaining these last three terms, you should have made three distinct pairings for the
two pairs of Wick contractions of the four W (ℓ+1)’s, one paring within the same NTK
and two pairings across the NTKs.
An inspection of the pattern of neural indices in the Kronecker deltas from (8.81)
suggests that we should again decompose the NTK variance into two tensors as
E

d
∆H
(ℓ)
i1i2;α1α2 d
∆H
(ℓ)
i3i4;α3α4

(8.82)
≡
1
nℓ−1
h
δi1i2δi3i4A(ℓ)
(α1α2)(α3α4) + δi1i3δi2i4B(ℓ)
α1α3α2α4 + δi1i4δi2i3B(ℓ)
α1α4α2α3
i
,
just as we did for the second layer before in (8.31). Here, a factor of 1/nℓ−1 was pulled out
in anticipation that the overall variance will be O(1/n) just as it was for the second layer.
For now you can think of this parameterization as an ansatz; we will soon recursively
show that A(ℓ)
(α1α2)(α3α4) and B(ℓ)
α1α3α2α4 stay of order one as the network width increases.
Now, let’s work out the layer recursions for A(ℓ) and B(ℓ).
B-recursion
We’ll start with B-recursion because it’s simpler. Considering (8.81) with the decompo-
sition (8.82) in mind, we see that B(ℓ+1) is given by the following ℓ-th-layer expectation:
B(ℓ+1)
α1α3α2α4 =

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j,k=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2 bH(ℓ)
jk;α1α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 bH(ℓ)
jk;α3α4
i
.
(8.83)
222

As should now be familiar, the double summation in (8.83) splits into two types of terms,
diagonal ones with j = k and oﬀ-diagonal ones with j ̸= k.
For the diagonal part, the leading contribution is from the NTK mean

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j=1
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2σ′ (ℓ)
j;α3σ′ (ℓ)
j;α4
i
H(ℓ)
α1α2H(ℓ)
α3α4 + O
 1
n

(8.84)
=

C(ℓ+1)
W
2 
σ′
α1σ′
α2σ′
α3σ′
α4

G(ℓ) H(ℓ)
α1α2H(ℓ)
α3α4 + O
 1
n

,
which is analogous to what we found in the second layer (8.33).11
For the oﬀ-diagonal part of (8.83), the NTK mean vanishes and the leading contri-
bution is from the NTK ﬂuctuation

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j,k=1
j̸=k
E

σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4

.
(8.85)
The expectation already is O
 ∆2 from the two NTK ﬂuctuations inside it and thus,
neglecting higher-order correlations of order O
 ∆3, we have
E

σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4

(8.86)
=E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
i
E

d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4

+ O
 1
n2

,
where the detailed explanation for such a factorization is given in this footnote.12 Then,
using the decomposition (8.82) for the NTK variance and similar logic as (4.63) to
11Again, the subleading O(1/n) piece includes both a contribution from the non-Gaussian distribution
as well as a cross correlation contribution from the previous layer.
12In greater detail, you can think of what we are doing here as separating σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 into a
mean E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
i
and ﬂuctuation and – since the expectation already contains two ﬂuctua-
tions d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4 – the latter ﬂuctuating piece contributes O ∆3
and thus can be neglected.
In alternate detail, we can view this expectation (8.86) as a correlator of three random variables
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4, d
∆H
(ℓ)
jk;α1α2, and d
∆H
(ℓ)
jk;α3α4, and decompose it into one-point and two-point corre-
lators as
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4
i
(8.87)
=E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
i
E
h
d
∆H
(ℓ)
jk;α1α2
i
E
h
d
∆H
(ℓ)
jk;α3α4
i
+ E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
i
E
h
d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4
i
+ E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 d
∆H
(ℓ)
jk;α1α2
i
E
h
d
∆H
(ℓ)
jk;α3α4
i
+ E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4 d
∆H
(ℓ)
jk;α3α4
i
E
h
d
∆H
(ℓ)
jk;α1α2
i
+ O
 1
n2

,
where the O 1/n2
part contains the connected piece of the decomposition. Since the NTK ﬂuctuation
has mean zero, only the second term survives at this order.
223

evaluate the four-point correlator of oﬀ-diagonal activations, we get

C(ℓ+1)
W
2 1
nℓ
nℓ
X
j,k=1
j̸=k
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
k;α2σ′ (ℓ)
j;α3σ′ (ℓ)
k;α4
i
E

d
∆H
(ℓ)
jk;α1α2 d
∆H
(ℓ)
jk;α3α4

+ O
 1
n

(8.88)
=

C(ℓ+1)
W
2 
σ′
α1σ′
α3

G(ℓ)

σ′
α2σ′
α4

G(ℓ)
nℓ
nℓ−1
B(ℓ)
α1α3α2α4 + O
 1
n

.
Substituting both the diagonal contribution (8.84) and oﬀ-diagonal contribution (8.88)
back into (8.83), we get the B-recursion:
B(ℓ+1)
α1α3α2α4 =

C(ℓ+1)
W
2
"

σ′
α1σ′
α2σ′
α3σ′
α4

G(ℓ) H(ℓ)
α1α2H(ℓ)
α3α4
(8.89)
+
 nℓ
nℓ−1
 
σ′
α1σ′
α3

G(ℓ)

σ′
α2σ′
α4

G(ℓ) B(ℓ)
α1α3α2α4
#
+ O
 1
n

.
As promised, we recursively see that B(ℓ) is an order-one quantity. Additionally, we note
that at leading order this B-type NTK variance doesn’t mix with any other ﬁnite-width
tensors.
A-recursion
Let us now determine the A-recursion.
Again equating our expression for the NTK
variance (8.81) with the A/B-decomposition (8.82), we see that A(ℓ+1) is given by the
following ℓ-th-layer covariances:
A(ℓ+1)
(α1α2)(α3α4) = 1
nℓ
nℓ
X
j,k=1
( 
λ(ℓ+1)
W
2 Cov
h
σ(ℓ)
j;α1σ(ℓ)
j;α2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
(8.90)
+ λ(ℓ+1)
W
C(ℓ+1)
W
Cov
h
σ(ℓ)
j;ασ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 bH(ℓ)
kk;α3α4
i
+ λ(ℓ+1)
W
C(ℓ+1)
W
Cov
h
σ′ (ℓ)
j;α σ′ (ℓ)
j;α2 bH(ℓ)
jj;αα2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
+

C(ℓ+1)
W
2 Cov
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 bH(ℓ)
jj;α1α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 bH(ℓ)
kk;α3α4
i )
.
As we’ve now seen many times previously, our approach will be to divide up the double
summation in (8.90) into two types of terms, diagonal terms on a single neuron with
j = k and oﬀ-diagonal terms on pairs of neurons with j ̸= k.
As was the case for the B-recursion, the leading contribution from the diagonal part
with j = k comes from the NTK mean and matches what we found for the second
layer (8.32)
D
bΩ(ℓ+1)
α1α2 bΩ(ℓ+1)
α3α4
E
G(ℓ) −
D
bΩ(ℓ+1)
α1α2
E
G(ℓ)
D
bΩ(ℓ+1)
α3α4
E
G(ℓ) + O
 1
n

,
(8.91)
224

where the deﬁnition of the auxiliary stochastic tensor bΩ(ℓ+1)
α1α2 was given in (8.74).13
This leaves us with the oﬀ-diagonal part of (8.90) with j ̸= k. Here, there will be
leading contributions both from the NTK mean and from the NTK ﬂuctuations. The
contributions from the mean are given by replacing bH(ℓ)
i1i2;α1α2 →δi1i2H(ℓ)
α1α2:
1
nℓ
nℓ
X
j,k=1
j̸=k
( 
λ(ℓ+1)
W
2 Cov
h
σ(ℓ)
j;α1σ(ℓ)
j;α2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
(8.92)
+ λ(ℓ+1)
W
C(ℓ+1)
W
H(ℓ)
α3α4Cov
h
σ(ℓ)
j;ασ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
i
+ λ(ℓ+1)
W
C(ℓ+1)
W
H(ℓ)
α1α2Cov
h
σ′ (ℓ)
j;α σ′ (ℓ)
j;α2, σ(ℓ)
k;α3σ(ℓ)
k;α4
i
+

C(ℓ+1)
W
2 H(ℓ)
α1α2H(ℓ)
α3α4Cov
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
i )
.
All four of these covariances can be evaluated using the intralayer formula (8.41). After
a little bit of algebra, this gives
nℓ
4nℓ−1
X
γ1,γ2,γ3,γ4
V (γ1γ2)(γ3γ4)
(ℓ)
D
zγ1zγ2 −G(ℓ)
γ1γ2

bΩ(ℓ+1)
α1α2
E
G(ℓ)
D
zγ3zγ4 −G(ℓ)
γ3γ4

bΩ(ℓ+1)
α3α4
E
G(ℓ)
(8.93)
at leading order, where we again made use of the deﬁnition of bΩ(ℓ+1)
α1α2 (8.74).
Finally, we’re left with the oﬀ-diagonal contributions from the NTK ﬂuctuations,
which – if we write them out in excruciating detail – are given by
1
nℓ
nℓ
X
j,k=1
j̸=k
( 
λ(ℓ+1)
W

C(ℓ+1)
W
Cov

σ(ℓ)
j;α1σ(ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

(8.94)
+

λ(ℓ+1)
W

C(ℓ+1)
W
Cov

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2, σ(ℓ)
k;α3σ(ℓ)
k;α4

+

C(ℓ+1)
W
2 H(ℓ)
α1α2Cov

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4

+

C(ℓ+1)
W
2 H(ℓ)
α3α4Cov

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4

+

C(ℓ+1)
W
2 Cov

σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2, σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4
d
∆H
(ℓ)
kk;α3α4
 )
.
The last term involving the two NTK ﬂuctuations can be evaluated similarly to how we
13Once again, the subleading O(1/n) piece includes both a contribution from the non-Gaussian dis-
tribution as well as a cross correlation contribution from the previous layer and now also a contribution
from the previous layer’s NTK variance.
225

evaluated such a term for the B-recursion in (8.86),14 here giving

C(ℓ+1)
W
2 
σ′
α1σ′
α2

G(ℓ)

σ′
α3σ′
α4

G(ℓ)
nℓ
nℓ−1
A(ℓ)
(α1α2)(α3α4) + O
 1
n

.
(8.96)
The remaining four covariances in (8.94) with only a single NTK ﬂuctuation are identical
in structure to (8.76), letting us leave the details of this to you and your roll of parchment.
At this point, let’s review all the components of our expression for A(ℓ+1): we have the
diagonal contribution (8.91); and oﬀ-diagonal contributions from the NTK mean (8.93),
from the covariance of two NTK ﬂuctuations (8.96), and from the four covariances on
your parchment. Assembling these components, we get the A-recursion:
A(ℓ+1)
(α1α2)(α3α4)
(8.97)
=
D
bΩ(ℓ+1)
α1α2 bΩ(ℓ+1)
α3α4
E
G(ℓ) −
D
bΩ(ℓ+1)
α1α2
E
G(ℓ)
D
bΩ(ℓ+1)
α3α4
E
G(ℓ)
+
nℓ
4nℓ−1
X
γ1,γ2,γ3,γ4
V (γ1γ2)(γ3γ4)
(ℓ)
D
bΩ(ℓ+1)
α1α2

zγ1zγ2 −G(ℓ)
γ1γ2
E
G(ℓ)
D
bΩ(ℓ+1)
α3α4

zγ3zγ4 −G(ℓ)
γ3γ4
E
G(ℓ)
+
nℓ
nℓ−1

C(ℓ+1)
W
2 
σ′
α1σ′
α2

G(ℓ)

σ′
α3σ′
α4

G(ℓ) A(ℓ)
(α1α2)(α3α4)
+
nℓ
nℓ−1
C(ℓ+1)
W
2
X
β1,β2,γ1,γ2
h D
bΩ(ℓ+1)
α1α2

zβ1zβ2 −G(ℓ)
β1β2
E
G(ℓ) Gβ1γ1
(ℓ) Gβ2γ2
(ℓ) D(ℓ)
γ1γ2α3α4

σ′
α3σ′
α4

G(ℓ)
+
D
bΩ(ℓ+1)
α3α4

zβ1zβ2 −G(ℓ)
β1β2
E
G(ℓ) Gβ1γ1
(ℓ) Gβ2γ2
(ℓ) D(ℓ)
γ1γ2α1α2

σ′
α1σ′
α2

G(ℓ)
i
+ O
 1
n

.
As promised, we recursively see that A(ℓ) is an order-one quantity. Additionally, we note
with interest that at leading order this A-type contribution to the NTK variance at layer
(ℓ+ 1) mixes with the four-point vertex V (ℓ) and the cross correlation D(ℓ) in layer ℓ,
though not with B(ℓ) or F (ℓ).
This completes our analysis of all the ﬁnite-width eﬀects for the NTK-preactivation
joint distribution; both the leading NTK-preactivation cross correlations and the NTK
variance scale as ∼1/n in the large-width expansion and vanish in the inﬁnite-width
limit.15 These quantities are suﬃcient to fully characterize the leading ﬁnite-width eﬀects
of gradient-based learning.
14The only diﬀerence between this and the B-version before (8.86) is that here, since we’re evaluating
a covariance, the term
E
h
σ′ (ℓ)
j;α1σ′ (ℓ)
j;α2 d
∆H
(ℓ)
jj;α1α2
i
E
h
σ′ (ℓ)
k;α3σ′ (ℓ)
k;α4 d
∆H
(ℓ)
kk;α3α4
i
(8.95)
is being subtracted. However, this term is of order O 1/n2
and can thus be neglected.
15Recalling our discussion from footnote 11 in §2.3, this means that the NTK self-averages in the strict
inﬁnite-width limit; in this limit, the particular value of the NTK in any instantiation of the network
parameters is ﬁxed and equal to the ensemble mean.
226

Chapter 9
Eﬀective Theory of the NTK at
Initialization
In short, we believe that we have answered Minsky and Papert’s challenge and have
found a learning result suﬃciently powerful to demonstrate that their pessimism about
learning in multilayer machines was misplaced.
Rumelhart, Hinton, and Williams [15], acausally rising to meet the criticism from §5.
Since the last chapter was a tempest of equations, algebra, and integration, let’s take
some moments to value our expectations.
Our goal in §8 was to determine the NTK-preactivation joint distribution for a given
layer ℓat initialization: p

z(ℓ), bH(ℓ)D

. The data-dependent couplings and the con-
nected correlators of this distribution run with depth according to the recursions that
we just laboriously derived – (8.63) for the NTK mean, (8.77) and (8.79) for the NTK-
preactivation cross correlations, and (8.89) and (8.97) for the NTK variance – in addition
to the recursions derived in §4 for the kernel (4.118) and for the four-point vertex (4.119).
This RG-ﬂow analysis taught us that the NTK is a deterministic object in the ﬁrst layer
(§8.1), stochastically ﬂuctuates and cross-correlates in the second layer (§8.2), and then
further accumulates ﬂuctuations and cross correlations in deeper layers (§8.3).
Now that we’ve thoroughly discussed the math, in this chapter we’ll ﬁnally be able
to consider the physics of this joint distribution. Building on our discussion of criticality
and universality in §5, we’ll ﬁrst lay the groundwork for a similar analysis of the NTK
while highlighting the relevant results from the last chapter (§9.1). In particular, our
focus will be on understanding how the initialization hyperparameters and the training
hyperparameters aﬀect gradient descent at ﬁnite width. We’ll once again ﬁnd that the
depth-to-width ratio L/n plays a starring role in controlling ﬁnite-width eﬀects, ﬁrst
for the scale-invariant universality class (§9.2) and then for the K⋆= 0 universality
class (§9.3). For both cases, the growing importance of NTK ﬂuctuations and cross
correlations with depth makes the ﬁnite-width interaction relevant under RG ﬂow of the
NTK.
227

Finally, we’ll introduce the infamous exploding and vanishing gradient problem of
deep learning and see how our notion of criticality completely mitigates this problem
(§9.4). In this context, we also explain how the bias and weight learning rates should
each be scaled with the network depth.
9.1
Criticality Analysis of the NTK
Let’s set the stage for our criticality analysis. As we did in our discussion of preactivation
criticality in §5, throughout this section we’ll set the bias variance C(ℓ)
b
and the rescaled
weight variance C(ℓ)
W to be uniform across layers
C(ℓ)
b
= Cb ,
C(ℓ)
W = CW .
(9.1)
Further paralleling §5.4, we will consider MLPs with uniform hidden layer widths
n1 = . . . = nL−1 ≡n ,
(9.2)
which is a sensible choice in practice as well as notationally simplifying.
For the training hyperparameters, however, we’ll preserve the layer dependence of
the bias learning rate λ(ℓ)
b
and weight learning rate λ(ℓ)
W for now, as diﬀerent universality
classes will require diﬀerent treatments. We’ll explore the general principle behind these
hyperparameter choices in §9.4.
Going forward, we’ll only focus on the leading contributions from the 1/n expansion
to the single-input statistics, neglecting the subleading corrections at next-to-leading-
order and reserving the multi-input analysis for your private amusement.
Leading-order NTK recursions for a single input
Let’s start with the NTK mean recursion. Analogously to all other observables, the 1/n
expansion induces a series expansion on the NTK mean of the form
H(ℓ)
α1α2 =H{0}(ℓ)
α1α2 +
1
nℓ−1
H{1}(ℓ)
α1α2 +
1
n2
ℓ−1
H{2}(ℓ)
α1α2 + O
 1
n3

.
(9.3)
Just as we deﬁned the kernel K(ℓ)
α1α2 as the inﬁnite-width limit of the mean metric G(ℓ)
α1α2
(4.106), let us give the leading O(1) piece of the NTK mean a special symbol,
Θ(ℓ)
α1α2 ≡H{0}(ℓ)
α1α2 ,
(9.4)
and a special name: the frozen NTK. The frozen NTK controls the training dynamics
in the inﬁnite-width limit, which we will investigate in detail next chapter.1
1Typically in the literature, the neural tangent kernel or NTK refers to this deterministic inﬁnite-
width NTK mean Θ(ℓ)
α1α2. Since we are principally concerned with understanding ﬁnite-width networks,
we instead chose to deﬁne and refer to the stochastic object b
H(ℓ)
i1i2;α1α2 as the NTK. As a concession
228

Now, taking the leading piece of the NTK mean recursion (8.63), we get a recursion
solely for the frozen NTK
Θ(ℓ+1)
α1α2 = λ(ℓ+1)
b
+ λ(ℓ+1)
W
⟨σα1σα2⟩K(ℓ) + CW

σ′
α1σ′
α2

K(ℓ) Θ(ℓ)
α1α2 .
(9.5)
Concurrently, as we are neglecting subleading contributions, we have exchanged the
Gaussian expectations over the mean metric G(ℓ) for ones over the kernel K(ℓ). Finally
specializing to a single input, we simply drop the sample indices to get this recursion’s
ﬁnal form,
Θ(ℓ+1) = λ(ℓ+1)
b
+ λ(ℓ+1)
W
g

K(ℓ)
+ χ⊥

K(ℓ)
Θ(ℓ) ,
(9.6)
with the initial condition coming directly from our ﬁrst-layer NTK analysis (8.23)
Θ(1) = λ(1)
b
+ λ(1)
W

1
n0
n0
X
j=1
x2
j

.
(9.7)
Note that here we have also made use of a helper function and susceptibility from §5.
For your convenience, let us also recall and reprint the full set of helper functions
– (5.5) and (5.52) – and susceptibilities – (5.50) and (5.51) – that we ﬁrst made popular
in §5:
g(K) = ⟨σ(z) σ(z)⟩K ,
(9.8)
h(K) ≡CW
4K2
D
σ′(z) σ′(z)

z2 −K
E
K = 1
2
d
dK χ⊥(K) ,
(9.9)
χ∥(K) = CW g′(K) = CW
2K2
D
σ(z) σ(z)

z2 −K
E
K = CW
K

z σ′(z) σ(z)

K ,
(9.10)
χ⊥(K) = CW

σ′(z) σ′(z)

K .
(9.11)
As a reminder, to go between the middle and right-hand expression in (9.10) you should
integrate by parts.
For the remaining recursions, we’re going to fast-forward the process as the procedure
for converting the multi-input recursions to leading-order single-input recursions surely
requires your attention but is somewhat mindless: (i) drop the layer dependence of the
initialization hyperparameters as (9.1) and uniformize the layer widths as (9.2); (ii) drop
sample indices everywhere; (iii) replace the mean metric G(ℓ) and the NTK mean H(ℓ)
with the kernel K(ℓ) and the frozen NTK Θ(ℓ), respectively;2 and (iv) substitute in for
to the literature, we’ve used the customary symbol for the NTK, Θ, to represent the frozen NTK. (As
a helpful mnemonic, note that there is an H frozen inside the Θ.) Unfortunately, you’ll have to wait
until §11 to understand the reason why we call the inﬁnite-width NTK frozen; there we’ll see how ﬁnite-
width eﬀects defrost the training process and make the NTK move. Here, in this chapter, you can at
least see how it gets agitated by ﬁnite-width ﬂuctuations.
2Picking nits, we should really make 1/n expansions – similar to (4.106) for the mean metric G(ℓ)
and (9.3) for the NTK mean H(ℓ) – for the ﬁnite-width tensors A(ℓ), B(ℓ), D(ℓ), F (ℓ), and also properly
make use of the one that we made for V (ℓ) (4.105), denoting the leading-order pieces as A{0}(ℓ) and
such, and dropping the subleading pieces. For the interest of notational sanity we won’t impose this on
229

helper functions and susceptibilities (9.8)–(9.11). In particular, this last step has the
beneﬁt of letting us recycle our results from §5 on the deep asymptotic behavior of these
functions.
It will also be necessary to recall the single-input leading-order expression for the
auxiliary stochastic variable (8.74),
bΩ(ℓ+1) ≡λ(ℓ+1)
W
σ(z)σ(z) + CW Θ(ℓ) σ′(z)σ′(z) ,
(9.12)
which appears in the recursions for D(ℓ) (8.77) and A(ℓ) (8.97); we’ll make this substitu-
tion the penultimate step (iii-b), if you will. In making these substitutions, please keep
in mind that the frozen NTK Θ(ℓ) multiplying the second term is not a random variable
and hence can be escorted out of any Gaussian expectations.
At this point, you should grab another roll of parchment, jot down expressions (9.8)–
(9.12), ﬂip back a few pages to locate recursions (8.77), (8.79), (8.89), and (8.97), for D(ℓ),
F (ℓ), B(ℓ), and A(ℓ), respectively (or perhaps you kiddos can simply click the equation
references in your eBook and copy over the equations to your tablet), and simplify
them according to the four-(though-sometimes-secretly-ﬁve-)step process (i)–(iv) above.
When you’re ﬁnished, make sure you agree with us:
D(ℓ+1) = χ(ℓ)
⊥χ(ℓ)
∥D(ℓ) +
 
λ(ℓ+1)
W
CW
!
C2
W ⟨σ(z)σ(z)σ(z)σ(z)⟩K(ℓ) −

CW g(ℓ)2 +

χ(ℓ)
∥
2 V (ℓ)

+ Θ(ℓ) h
C2
W

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) −CW g(ℓ)χ(ℓ)
⊥+ 2h(ℓ)χ(ℓ)
∥V (ℓ)i
,
(9.13)
F (ℓ+1) =

χ(ℓ)
∥
2 F (ℓ) + C2
W

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) Θ(ℓ) ,
(9.14)
B(ℓ+1) =

χ(ℓ)
⊥
2 B(ℓ) + C2
W

σ′(z)σ′(z)σ′(z)σ′(z)

K(ℓ)

Θ(ℓ)2 ,
(9.15)
A(ℓ+1) =

χ(ℓ)
⊥
2 A(ℓ) +
 
λ(ℓ+1)
W
CW
!2
C2
W ⟨σ(z)σ(z)σ(z)σ(z)⟩K(ℓ) −

CW g(ℓ)2 +

χ(ℓ)
∥
2 V (ℓ)

+ 2
 
λ(ℓ+1)
W
CW
!
Θ(ℓ) h
C2
W

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) −CW g(ℓ)χ(ℓ)
⊥+ 2h(ℓ)χ(ℓ)
∥V (ℓ)i
+ 2
 
λ(ℓ+1)
W
CW
!
χ(ℓ)
⊥χ(ℓ)
∥D(ℓ) + 4h(ℓ)χ(ℓ)
⊥Θ(ℓ)D(ℓ)
+

Θ(ℓ)2 
C2
W

σ′(z)σ′(z)σ′(z)σ′(z)

K(ℓ) −

χ(ℓ)
⊥
2 +

2h(ℓ)2 V (ℓ)

.
(9.16)
For these recursions, the initial conditions (recalling that the ﬁrst-layer NTK is fully
deterministic) all vanish identically as
A(1) = B(1) = D(1) = F (1) = 0 .
(9.17)
you, though our recursions for these tensors should all be understood as referring to these leading-order
pieces. (The kernel and the frozen NTK are special in that these inﬁnite-width objects have already been
well-studied by the community, and so in this case it’s important to diﬀerentiate between the ﬁnite-width
object and the inﬁnite-width piece.)
230

Here also, for helper functions and susceptibilities, we used the following simplifying
notation
g(ℓ) ≡g

K(ℓ)
,
h(ℓ) ≡h

K(ℓ)
,
χ(ℓ)
∥
≡χ∥

K(ℓ)
,
χ(ℓ)
⊥≡χ⊥

K(ℓ)
,
(9.18)
making the kernel dependence implicit.
We can further simplify (9.13) and (9.16) by recalling the single-input recursion for
the four-point vertex (5.109)
V (ℓ+1) =

χ(ℓ)
∥
2 V (ℓ) + C2
W

⟨σ(z)σ(z)σ(z)σ(z)⟩K(ℓ) −

g(ℓ)2
.
(9.19)
Keep staring at these equations, and you’ll see slightly more compact expressions emerge
D(ℓ+1) = χ(ℓ)
⊥χ(ℓ)
∥D(ℓ) +
 
λ(ℓ+1)
W
CW
!
V (ℓ+1)
(9.20)
+ Θ(ℓ) h
C2
W

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) −CW g(ℓ)χ(ℓ)
⊥+ 2h(ℓ)χ(ℓ)
∥V (ℓ)i
,
A(ℓ+1) =

χ(ℓ)
⊥
2 A(ℓ) −
 
λ(ℓ+1)
W
CW
!2
V (ℓ+1) + 2
 
λ(ℓ+1)
W
CW
!
D(ℓ+1) + 4h(ℓ)χ(ℓ)
⊥Θ(ℓ)D(ℓ)
+

Θ(ℓ)2 
C2
W

σ′(z)σ′(z)σ′(z)σ′(z)

K(ℓ) −

χ(ℓ)
⊥
2 +

2h(ℓ)2 V (ℓ)

,
(9.21)
which you may ﬁnd makes things simpler when solving these recursions. However, please
use these formulae with caution as both ℓ-th-layer and (ℓ+ 1)-th-layer objects appear
on their right-hand sides.
The relevance of scaling laws
For the rest of this chapter, we will work through solving the ﬁve leading-order single-
input NTK recursions (9.6) and (9.13)–(9.16). (Remember that we already solved single-
input recursions for the kernel and four-point vertex way back in §5.) In solving these
recursions, we will ﬁnd that each observable obeys our scaling ansatz (5.93):
O(ℓ) =
1
ℓ
pO "
c0,0 + c1,1
log ℓ
ℓ

+ c1,0
1
ℓ

+ c2,2
 
log2 ℓ
ℓ2
!
+ . . .
#
=
1
ℓ
pO


∞
X
s=0
s
X
q=0
cs,q
logq ℓ
ℓs

.
(9.22)
Recall that pO is a critical exponent, which is universal for a given universality class
of activation functions, while the constants cs,q depend on some of the details of the
particular activation function under consideration.
To properly understand the physics of these observables, recall from §5.4 that we
need to consider dimensionless quantities.
For the two tensors controlling the NTK
231

variance, we should normalize by the square of the frozen NTK
A(ℓ)
n
 Θ(ℓ)2 ∼1
n
1
ℓ
pA−2pΘ
+ . . . ,
B(ℓ)
n
 Θ(ℓ)2 ∼1
n
1
ℓ
pB−2pΘ
+ . . . ,
(9.23)
while for the NTK-preactivation cross correlation, we should instead normalize by one
factor of the frozen NTK and one factor of the kernel
D(ℓ)
nK(ℓ)Θ(ℓ) ∼1
n
1
ℓ
pD−pΘ−p0
+ . . . ,
F (ℓ)
nK(ℓ)Θ(ℓ) ∼1
n
1
ℓ
pF −pΘ−p0
+ . . . ,
(9.24)
where p0 was the critical exponent for the single-input kernel K(ℓ).
By looking at these dimensionless quantities, we’ll ﬁnd scaling laws that transcend
even beyond universality classes. As a particular example, recall that the normalized
four-point vertex (5.128),
V (ℓ)
n
 K(ℓ)2 ∼1
n
1
ℓ
pV −2p0
+ . . . ,
(9.25)
gave rise to a scaling law (5.129)
pV −2p0 = −1 ,
(9.26)
for both scale-invariant and K⋆= 0 activation functions. This scaling law let us interpret
the ratio ℓ/n as an emergent scale controlling the leading ﬁnite-width behavior of the
preactivation distribution. Spoiler alert: in much the same way, we’ll ﬁnd scaling laws
pA −2pΘ = −1 ,
pB −2pΘ = −1 ,
pD −pΘ −p0 = −1 ,
pF −pΘ −p0 = −1 , (9.27)
that also hold for both the scale-invariant and K⋆= 0 universality classes. Thus, we’ll
be able to conclude that all the leading ﬁnite-width eﬀects of the NTK-preactivation
joint distribution are relevant and controlled by the same ℓ/n perturbative cutoﬀ. This
means that we can eﬀectively describe the training of realistic deep networks of ﬁnite
width and nonzero L/n.
Formalities: perpendicular perturbations and the frozen NTK
Before explicitly analyzing universality classes, let us note that the frozen-NTK recur-
sion (9.6) admits a formal solution
Θ(ℓ) =
ℓ
X
ℓ′=1
(h
λ(ℓ′)
b
+ λ(ℓ′)
W g(ℓ′−1)i " ℓ−1
Y
ℓ′′=ℓ′
χ(ℓ′′)
⊥
#)
.
(9.28)
In words, we see that the solution involves a sum over all the previous layers 1, . . . , ℓ,
and that each term in the sum involves an additive contribution λ(ℓ′)
b
+λ(ℓ′)
W g(ℓ′−1). Such
a contribution then gets recursively multiplied by perpendicular susceptibilities up to
232

the (ℓ−1)-th layer, resulting in an overall multiplicative factor Qℓ−1
ℓ′′=ℓ′ χ(ℓ′′)
⊥
. To avoid
the exponential behavior that’s generic with such a factor, we must set χ⊥= 1.
It is enlightening to tie this insight to the discussion we had in §5.1 where we per-
formed our general criticality analysis of the kernel recursion. There, we ﬁrst looked
at the single-input kernel and set χ∥= 1 to avoid exponential behavior in the network
outputs. Then, we looked at the two-input kernel and analyzed how the oﬀ-diagonal
perpendicular perturbations δδK(ℓ)
[2] ﬂow. Turning oﬀthe odd perturbations δK(ℓ)
[1] , a
brief inspection of the perpendicular recursion (5.48)
δδK(ℓ+1)
[2]
= χ(ℓ)
⊥δδK(ℓ)
[2] ,
(9.29)
necessitated the criticality condition χ⊥= 1 so as to preserve the diﬀerence between
nearby inputs as they propagate through the network. At the time, we presumed that
such a condition would be useful for comparing nearby inputs when learning from data.
Indeed, the same multiplicative factor that appeared in the formal solution for the frozen
NTK (9.28),
ℓ−1
Y
ℓ′′=ℓ′
χ(ℓ′′)
⊥
=
δδK(ℓ)
[2]
δδK(ℓ′)
[2]
,
(9.30)
also appears in a formal solution for δδK(ℓ)
[2] . Thus, with both formal solutions (9.28) and
(9.30), we have formalized the connection between preserving δδK(ℓ)
[2] data and learning
from data.
With the formalities out of the way, let’s now analyze our two eminent universality
classes, the scale-invariant universality class and the K⋆= 0 universality class.
9.2
Scale-Invariant Universality Class
As a reminder, the canonical members of the scale-invariant universality class are the
ReLU and linear activation functions. For a general activation function in this univer-
sality class,
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 ,
(9.31)
recall from §5.2 that the helper functions and susceptibilities evaluate to
g(ℓ) = A2K(ℓ) ,
(9.32)
h(ℓ) = 0 ,
(9.33)
χ(ℓ)
∥
= χ ,
(9.34)
χ(ℓ)
⊥= χ ,
(9.35)
233

with χ ≡CW A2. By substituting in (9.31) and performing the integrals, we can just as
easily evaluate the three other Gaussian expectations that we’ll need
⟨σ(z)σ(z)σ(z)σ(z)⟩K(ℓ) = 3A4

K(ℓ)2 ,
(9.36)

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) = A4K(ℓ) ,
(9.37)

σ′(z)σ′(z)σ′(z)σ′(z)

K(ℓ) = A4 .
(9.38)
Here and right before, we’ve also made use of our previous deﬁnitions for the constants
that naturally arise from these integrations:
A2 ≡a2
+ + a2
−
2
,
A4 ≡a4
+ + a4
−
2
.
(9.39)
With these recollections, we are reminded of one of this class’s principal characteris-
tics: both susceptibilities are independent of the kernel and constant for all layers. With
that in mind, we were able to easily satisfy criticality for the scale-invariant universality
class by setting the initialization hyperparameters to
Cb = 0 ,
CW = 1
A2
.
(9.40)
With these tunings, both susceptibilities are set to unity χ = 1 and the ﬁxed-point value
of the kernel is given in terms of the input by the expression (5.66)
K⋆≡1
A2

1
n0
n0
X
j=1
x2
j

,
(9.41)
and the four-point vertex at criticality is given by (5.120)
V (ℓ) = (ℓ−1)
3A4
A2
2
−1

(K⋆)2 .
(9.42)
NTK mean (frozen NTK)
With the above expressions in mind, at criticality the recursion (9.6) for the single-input
frozen NTK simpliﬁes to
Θ(ℓ+1) = Θ(ℓ) + λ(ℓ+1)
b
+ λ(ℓ+1)
W
A2K⋆,
(9.43)
This recursion, together with the initial condition (9.7), is easy to solve for a given set
of bias and weight learning rates.
For instance, assuming layer-independent learning rates λ(ℓ)
b
= λb and λ(ℓ)
W = λW , we
ﬁnd
Θ(ℓ) = (λb + λW A2K⋆) ℓ.
(9.44)
With these uniform learning rates, we see that the frozen NTK for the scale-invariant
universality class grows linearly with depth. Since the NTK involves a sum over all
234

the previous layers (9.28), linear growth implies that these contributions are uniform
across the layers; this is in contrast to the non-critical cases, for which we would have
had exponentially diﬀerent contributions from the diﬀerent layers of a deep network, as
is clear from the formal solution (9.28). Finally, a comparison with the ansatz (9.22)
implies that the critical exponent for the frozen NTK is given by pΘ = −1. We will
interpret all these points further in §9.4.
NTK variance and NTK-preactivation cross correlation (agitated NTK)
Now, let’s evaluate our ﬁnite-width recursions (9.13)–(9.16) to ﬁnd the NTK variance and
the NTK-preactivation cross correlations.3 First, we can simplify them by substituting
in for the helper functions g(ℓ) = A2K(ℓ) (9.32) and h(ℓ) = 0 (9.33) as well as making
use of our formulae for the three other Gaussian expectations (9.36)–(9.38) involving
A4. Then, let us tune the initialization hyperparameters to criticality (9.40) by picking
CW = 1/A2, which sets both susceptibilities to unity χ(ℓ)
∥
= χ(ℓ)
⊥= 1 and makes the
kernel ﬁxed K(ℓ) = K⋆. With these manipulations, we get
D(ℓ+1) =D(ℓ) + λW A2
3A4
A2
2
−1

(K⋆)2 + V (ℓ)

+
A4
A2
2
−1

K⋆Θ(ℓ) ,
(9.45)
F (ℓ+1) =F (ℓ) + A4
A2
2
K⋆Θ(ℓ) ,
(9.46)
B(ℓ+1) =B(ℓ) + A4
A2
2

Θ(ℓ)2 ,
(9.47)
A(ℓ+1) =A(ℓ) + (λW A2)2
3A4
A2
2
−1

(K⋆)2 + V (ℓ)

(9.48)
+ 2λW A2
A4
A2
2
−1

K⋆Θ(ℓ) + 2λW A2D(ℓ) +
A4
A2
2
−1
 
Θ(ℓ)2 .
Note that we have also assumed layer-independent learning rates as we did just before
when working out the NTK mean.
Next, substituting in our solutions for V (ℓ) (9.42) and Θ(ℓ) (9.44), we can easily solve
the recursions for D(ℓ), F (ℓ), and B(ℓ). Then, with our solution for D(ℓ) in hand, we can
3You could also choose to evaluate (9.20) and then (9.21) for D(ℓ) and A(ℓ), respectively; it’s about
the same level of diﬃculty and obviously yields the same solution either way.
235

also solve the recursion for A(ℓ). All together, this gives the following solutions
D(ℓ) = ℓ(ℓ−1)
2

λb
A4
A2
2
−1

K⋆+ λW A2
4A4
A2
2
−2

(K⋆)2

,
(9.49)
F (ℓ) = ℓ(ℓ−1)
2
A4
A2
2
(λb + λW A2K⋆) K⋆

,
(9.50)
B(ℓ) = ℓ(ℓ−1)(2ℓ−1)
6
A4
A2
2

(λb + λW A2K⋆)2 ,
(9.51)
A(ℓ) = ℓ3
3
A4
A2
2
−1

λ2
b + 3
A4
A2
2
−1

λbλW A2K⋆+

5A4 −3A2
2

λ2
W (K⋆)2

+ . . . ,
(9.52)
where for A(ℓ) we kept only the leading large-ℓcontribution.
From these four solutions, we can read oﬀanother four critical exponents for the
scale-invariant universality class,
pD = −2 ,
pF = −2 ,
pB = −3 ,
pA = −3 ,
(9.53)
which corresponds to the quadratic growth of D(ℓ) and F (ℓ) and the cubic growth of
B(ℓ) and A(ℓ). Combined with p0 = 0 for the kernel and pΘ = −1 for the frozen NTK,
we obtain the advertised ℓ/n-scaling (9.27) of the appropriately normalized quantities
(9.23) and (9.24).
9.3
K⋆= 0 Universality Class
As a reminder, two notable members of this class are tanh and sin. More generally, the
K⋆= 0 universality class contains any activation function with a corresponding kernel
that has a nontrivial ﬁxed point at K⋆= 0.
Speciﬁcally, recall from §5.3.3 that we used the following notation for the Taylor
coeﬃcients of an activation function:
σ(z) =
∞
X
p=0
σp
p! zp .
(9.54)
Then, from an analysis of the single-input kernel recursion we learned that there’s non-
trivial ﬁxed point at K⋆= 0 if and only if the activation function vanishes at the origin
with nonzero slope (5.89)
σ0 = 0 ,
σ1 ̸= 0 ,
(9.55)
for which we can satisfy the criticality conditions by tuning the initialization hyperpa-
rameters as (5.90)
Cb = 0 ,
CW = 1
σ2
1
.
(9.56)
Going forward, we will assume that the bias variance and rescaled weight variance have
been tuned to criticality as (9.56).
236

Unlike the scale-invariant universality class, the criticality analysis for the K⋆= 0
universality class was perturbative around K = 0. For this analysis, we expanded the
helper function g(K) and both susceptibilities as (5.83)–(5.85), which – now with (9.55)
and (9.56) in mind – evaluate to
g(K) = σ2
1
h
K + a1K2 + O

K3i
,
(9.57)
χ∥(K) = 1 + 2a1K + O

K2
,
(9.58)
χ⊥(K) = 1 + b1K + O

K2
,
(9.59)
where we’ve also recalled the following combinations of Taylor coeﬃcients
a1 ≡
σ3
σ1

+ 3
4
σ2
σ1
2
,
(9.60)
b1 ≡
σ3
σ1

+
σ2
σ1
2
.
(9.61)
As a reminder, to get these expressions we ﬁrst Taylor expanded their deﬁnitions (9.8),
(9.10), and (9.11) in z, and then evaluated each series of Gaussian expectations to the
desired order. Following the same method, we can evaluate the helper function h(K)
(9.9) as well as the two other Gaussian expectations needed to solve our NTK recursions:
h(K) = b1
2 + O

K1
,
(9.62)

σ(z)σ(z)σ′(z)σ′(z)

K = σ4
1
h
K + O

K2i
,
(9.63)

σ′(z)σ′(z)σ′(z)σ′(z)

K = σ4
1
h
1 + O

K1i
.
(9.64)
Remembering that the parallel susceptibility characterizes the linear response of the
kernel perturbations around the ﬁxed point (5.10), we note from above that the parallel
susceptibility at criticality (9.58) is close to one near the nontrivial ﬁxed point at K⋆= 0.
Consequently, we found a power-law large-ℓasymptotic solution for the single-input
kernel (5.94)
K(ℓ) = K⋆+ ∆K(ℓ) = ∆K(ℓ) =

1
(−a1)
 1
ℓ+ O
log ℓ
ℓ2

,
(9.65)
which slowly but surely approaches the K⋆= 0 nontrivial ﬁxed point, justifying our
perturbative approach to the deep asymptotics. As for the single-input four-point vertex,
we previously found (5.127)
V (ℓ) =
 2
3a2
1
 1
ℓ+ O
log ℓ
ℓ2

,
(9.66)
and the appropriately normalized quantity (9.25) has an ℓ/n scaling:
V (ℓ)
n
 K(ℓ)2 =
2
3
 ℓ
n + O
log (ℓ)
n

.
(9.67)
237

NTK mean (frozen NTK)
Let’s start with our generic formal solution (9.28) to the frozen NTK recursion (9.6).
Note that for K⋆= 0 activation functions the multiplicative factor (9.30) takes the form
ℓ−1
Y
ℓ′′=ℓ′
χ(ℓ′′)
⊥
=
δδK(ℓ)
[2]
δδK(ℓ′)
[2]
=
ℓ′
ℓ
p⊥
+ . . . ,
(9.68)
when we plug in our large-ℓasymptotic solution for δδK(ℓ)
[2] (5.99).
As a reminder,
the critical exponent controlling the falloﬀwas given in terms of the Taylor coeﬃcient
combinations, p⊥= b1/a1, which evaluates to 1 for the tanh and sin activation functions.
Plugging this multiplicative factor back into the formal solution (9.28) along with our
expansion for g(K) (9.57) evaluated on the asymptotic kernel (9.65), we ﬁnd
Θ(ℓ) =
ℓ
X
ℓ′=1
("
λ(ℓ′)
b
+ λ(ℓ′)
W
σ2
1
(−a1)
 1
ℓ′

+ . . .
# "ℓ′
ℓ
p⊥
+ . . .
#)
.
(9.69)
Here, the factor in the ﬁrst square bracket is an additive contribution picked up from the
ℓ′-th layer, while the factor in the second square bracket is a multiplicative contribution
from recursively passing from the ℓ′-th layer to the ℓ-th layer.
Eﬀective theorists may take issue with two aspects of this solution (9.69) if we naively
continue to choose layer-independent learning rates λ(ℓ)
b
= λb and λ(ℓ)
W = λW .
• Firstly, notice in the ﬁrst square bracket that the ℓ′-dependence of the bias term
diﬀers from the ℓ′-dependence of the weight term by a factor of ∼(1/ℓ′). This
means that the contribution of the weights to the NTK decreases with depth
relative to the contribution of the biases.
• Secondly, notice in the second square bracket that the ∼(ℓ′)p⊥behavior means
that the NTK is dominated by contributions from deeper layers for p⊥> 0 in
comparison to the shallower layers.
Remembering that the NTK controls the
dynamics of observables (8.3), this in turn means that the training dynamics will
be heavily inﬂuenced by the model parameters near the output layer.
Additionally, practical practitioners may now wonder whether these unnatural depth
scalings also contribute to the empirical preference for ReLU over tanh in the deep learn-
ing community. (More on this in §9.4.)
Having said all that, we can rectify this imbalance by scaling out the layer dependence
as
λ(ℓ)
b
≡eλb
1
ℓ
p⊥
,
λ(ℓ)
W ≡eλW
1
ℓ
p⊥−1
,
(9.70)
where eλb and eλW are layer-independent constants. Substituting this ansatz into our
solution (9.69), we ﬁnd
Θ(ℓ) =
"
eλb +
eλW σ2
1
(−a1)
# 1
ℓ
p⊥−1
+ . . . ,
(9.71)
238

which manifestly balances the weight and bias contributions. Thus, we see for the K⋆= 0
universality class that the critical exponent for the frozen NTK is given by
pΘ = p⊥−1 .
(9.72)
In particular, for both the tanh and sin activation functions, pΘ = 0.
NTK variance and NTK-preactivation cross correlation (agitated NTK)
Now, let’s ﬁnally deal with the agitated NTK statistics for the K⋆= 0 universality class.
To aid our computation at criticality, let us make use of the asymptotic behavior of the
kernel (9.65) and record the leading large-ℓasymptotics of the helper functions (9.57)
and (9.62), the susceptibilities (9.58) and (9.59), and the two other needed Gaussian
expectations (9.63) and (9.64):
CW g(ℓ) =

1
(−a1)
 1
ℓ+ . . . ,
(9.73)
h(ℓ) = b1
2 + . . . ,
(9.74)
χ(ℓ)
∥
= 1 −2
ℓ+ . . . ,
(9.75)
χ(ℓ)
⊥= 1 −p⊥
ℓ+ . . . ,
(9.76)
C2
W

σ(z)σ(z)σ′(z)σ′(z)

K(ℓ) =

1
(−a1)
 1
ℓ+ . . . ,
(9.77)
C2
W

σ′(z)σ′(z)σ′(z)σ′(z)

K(ℓ) = 1 + . . . .
(9.78)
Additionally, going forward we will assume that the bias and weight learning rates have
the layer-dependence (9.70) motivated by equal per-layer NTK contribution.
With all this out of the way, it’s straightforward to evaluate the large-ℓasymptotics
of F (ℓ) and B(ℓ). Plugging in the above expressions and the frozen NTK asymptotic
solution (9.71) into their recursions (9.14) and (9.15), we get
F (ℓ+1) =

1 −4
ℓ+ . . .

F (ℓ) +
(
1
(−a1)
 "
eλb +
eλW σ2
1
(−a1)
# 1
ℓ
p⊥
+ . . .
)
,
(9.79)
B(ℓ+1) =

1 −2p⊥
ℓ
+ . . .

B(ℓ) +



"
eλb +
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−2
+ . . .


.
(9.80)
Substituting in our scaling ansatz (9.22), they have the following asymptotic solutions
at large ℓ:
F (ℓ) =
1
(5 −p⊥)

1
(−a1)
 "
eλb +
eλW σ2
1
(−a1)
# 1
ℓ
p⊥−1
+ . . . ,
(9.81)
B(ℓ) = 1
3
"
eλb +
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−3
+ . . . .
(9.82)
239

Next, for the D(ℓ) recursion, let’s start with the slightly more compact expres-
sion (9.20). Plugging in the expressions (9.73)–(9.78) along with the learning rates (9.70)
and the asymptotic solutions for the four-point vertex (9.66) and the frozen NTK (9.71),
we get a recursion
D(ℓ+1) =

1 −(p⊥+ 2)
ℓ
+ . . .

D(ℓ) +
(
2
3(−a1)
"
−p⊥eλb −(p⊥−1)
eλW σ2
1
(−a1)
#1
ℓ
p⊥
+ . . .
)
.
(9.83)
This recursion can also be easily solved by using our scaling ansatz (9.22), giving
D(ℓ) =
−2
9(−a1)
"
p⊥eλb + (p⊥−1)
eλW σ2
1
(−a1)
# 1
ℓ
p⊥−1
+ . . . .
(9.84)
Finally, for A(ℓ) recursion (9.21), the by-now-familiar routine of ﬂipping back and
forth in your book and plugging in the large-ℓasymptotic expressions (9.73)–(9.78), the
learning rates (9.70), and the asymptotic solutions for the four-point vertex (9.66), for
the frozen NTK (9.71), and for D(ℓ) (9.84), gives
A(ℓ+1) =

1 −2p⊥
ℓ
+ . . .

A(ℓ) +



4
9
"
p⊥eλb + (p⊥−1)
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−2
+ . . .


,
(9.85)
which can be solved using the same large-ℓscaling ansatz (9.22), giving
A(ℓ) = 4
27
"
p⊥eλb + (p⊥−1)
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−3
+ . . . .
(9.86)
With this, we complete our evaluation of the agitated NTK statistics for the K⋆= 0
universality class.4
Having solved all the recursions we have, let’s collect and recollect the critical expo-
nents. From (9.81) and (9.84) we collect pF = pD = p⊥−1, while from (9.82) and (9.86)
we collect pB = pA = 2p⊥−3. Recollecting p0 = 1 for the kernel and pΘ = p⊥−1 for
the frozen NTK, these critical exponents for the K⋆= 0 universality class again obey
ℓ/n-scaling (9.27) for the normalized quantities deﬁned in (9.23) and (9.24). Together
with our scale-invariant results (9.53), this means the posited relations (9.27) do indeed
persists across universality classes as scaling laws.
In summary, we have found that the leading ﬁnite-width behavior of the NTK-
preactivation joint distribution – as measured by the NTK variance and NTK-preacitvation
cross correlation – has a relevant ℓ/n scaling regardless of activation function, as is nat-
ural according to the principles of our eﬀective theory.
4Curiously, for activation functions with p⊥= 1 such as tanh and sin, the single-input tensors D(ℓ)
and A(ℓ) are independent of the weight learning rate at leading order.
240

9.4
Criticality, Exploding and Vanishing Problems, and
None of That
Having now analyzed the NTK statistics of deep networks, let us culminate our discussion
by revisiting our original motivation for criticality: exploding and vanishing problems.
In particular, let us ﬁnally introduce – and then immediately abolish – the exploding
and vanishing gradient problem.5
Traditional view on the exploding and vanishing gradient problem
Traditionally, the exploding and vanishing gradient problem is manifested by considering
the behavior of the gradient of the loss for a deep network. Using the chain rule twice,
the derivative of the loss with respect to a model parameter θ(ℓ)
µ
in the ℓ-th layer – either
a bias θ(ℓ)
µ
≡b(ℓ)
j
or a weight θ(ℓ)
µ
≡W (ℓ)
jk – takes the form
dLA
dθ(ℓ)
µ
=
X
α∈D
nL
X
iL=1
nℓ
X
iℓ=1
ϵiL;α
dz(L)
iL;α
dz(ℓ)
iℓ;α
dz(ℓ)
iℓ;α
dθ(ℓ)
µ
.
(9.87)
In this gradient, the ﬁrst factor is the error factor (8.14)
ϵi;α ≡∂LA
∂z(L)
i;α
,
(9.88)
the ﬁnal factor is a trivial factor (8.11)
dz(ℓ)
i;α
db(ℓ)
j
= δij ,
dz(ℓ)
i;α
dW (ℓ)
jk
= δij σ(ℓ−1)
k;α
,
(9.89)
and the middle factor is the chain-rule factor
dz(L)
iL;α
dz(ℓ)
iℓ;α
=
X
iℓ+1,...,iL−1
dz(L)
iL;α
dz(L−1)
iL−1;α
· · ·
dz(ℓ+1)
iℓ+1;α
dz(ℓ)
iℓ;α
=
X
iℓ+1,...,iL−1
L−1
Y
ℓ′=ℓ
h
W (ℓ′+1)
iℓ′+1iℓ′σ′ (ℓ′)
iℓ′;α
i
,
(9.90)
which can be derived by iterating the backward equation (8.17) or equivalently by re-
peatedly using the chain rule in conjunction with the MLP forward equation (8.9). If this
text causes you to experience a large error factor yourself, please ﬂip backward to §8.0
and review our discussion of the backpropagation algorithm.
The point is that without any ﬁne-tuning, the product of matrices from layer ℓto
layer L in the chain-rule factor (9.90) will generically lead to exponential behavior. Even
5This problem was ﬁrst noticed [58, 59] in the context of training (the now somewhat deprecated)
recurrent neural networks (RNNs), during the era when neural networks were still neural networks and
not yet deep learning, that is, at a time when MLPs weren’t yet deep enough for this to have been an
obvious issue.
241

for networks of moderate depth, this makes it extremely diﬃcult for the shallower-layer
parameters to receive a well-behaved gradient and consequentially be properly trained:
a vanishing gradient means that such parameters receive no training signal from the
data and loss, while an exploding gradient is indicative of an instability in which the
loss may increase or even blow up. This is the exploding and vanishing gradient
problem. In a sense, this is a backward iteration dual of the already familiar exploding
and vanishing kernel problem that arises from the forward iteration equation.6
Of course, not only does the chain-rule factor (9.90) need to be well behaved for
stable training, but the error factor (9.88) and trivial factor (9.89) must be as well. As
we’ll explain next, these latter factors are directly tied to the exploding and vanishing
kernel problem. However, we’ll also see that our well-understood notion of criticality is
already suﬃcient to mitigate both exploding and vanishing problems together.
Critical view on the exploding and vanishing gradient problem
Critically, let us recall from §3.2 and then §5.1 our discussion of the exploding and
vanishing kernel problem. In those sections, we ﬁrst motivated criticality as remedying
exponential behavior in the kernel K(ℓ)
α1α2. As the L-th-layer kernel controls the typical
values of the network output – and as the dataset’s labels are generically order-one
numbers – we suggested that such an exploding or vanishing kernel would be problematic
for training. Now that we know a little about gradient descent, we can actually see a
more direct manifestation of this instability by considering all the factors that make up
the network’s gradient (9.87).
First, let’s see how the error factor (9.88) is tied to the exploding kernel problem.
For example, the error factor for the MSE loss (7.2) is given by (7.15)
ϵi;α = z(L)
i;α −yi;α .
(9.91)
As you can clearly see, if the kernel explodes, then the typical output – and hence typical
values of the error factor – will explode as well.7 To ensure this does not happen, we
must set χ∥(K⋆) ≤1.
Second, notice that the trivial factor (9.89) for the weights is proportional to the
activation. For activation functions contained in either of the scale-invariant and K⋆=
0 universality classes, if the kernel – and consequently the typical preactivation – is
exponentially small, then the activation – and consequently the trivial factor – will be
exponentially suppressed. Subsequently, the weights in the deeper layers of the network
would struggle to train as they only receive an exponentially small update. Thus, in
6If you’d like, you can see this duality concretely by considering a deep linear network, for which the
statistics of such a product of weights can be worked out exactly exactly as in §3.
7Getting ahead of ourselves, a precocious reader might wonder whether this matters for the cross-
entropy loss, since for that loss the error factor will stay of order one even if the network output explodes.
However, in this case the model would then be (exponentially) overconﬁdent on its predictions and such
an inductive bias would be diﬃcult to correct via training.
242

order to avoid this vanishing kernel problem, we demand χ∥(K⋆) ≥1.8
Combining these two observations, we see that the exploding and vanishing kernel
problem is directly manifested as a subproblem of the exploding and vanishing gradient
problem and further see how our criticality condition imposed on the parallel suscepti-
bility, χ∥(K⋆) = 1, serves to mitigate it.
Moreover, we can shed further light on the vanishing of the trivial factor by con-
sidering its embodiment in the NTK. Considering our formal solution for the frozen
NTK (9.28) and recalling the original deﬁnition (8.7), we can track the contribution of
the weight derivatives as leading to the additive term λ(ℓ′)
W g(ℓ′−1). For an exponentially
vanishing kernel, this factor is exponentially suppressed as
λ(ℓ′)
W g(ℓ′−1) ∝K(ℓ′−1) ≪1 ,
(9.92)
since g(K) = A2K (9.32) for the scale-invariant universality class and g(K) = σ2
1K +
O
 K2 (9.57) for the K⋆= 0 universality class. This is another way of seeing that such
deeper-layer weights are not contributing to the training dynamics and, in particular, also
implies that such weights will have a minimal eﬀect on the updates to other parameters.
Similarly, we see that the chain-rule factor (9.90) is also encoded in the NTK in
the multiplicative factor Qℓ−1
ℓ′′=ℓ′ χ(ℓ′′)
⊥
(9.30). In fact, such a factor was secretly always
lurking in the NTK forward equation (8.8) or (8.12) as the coeﬃcient of the recursive
term. To disclose that secret, note that in the inﬁnite-width limit the expectation of the
chain-rule factor factorizes, and we see from (8.8) or (8.12) that
E

X
j1,j2
dz(ℓ+1)
i
dz(ℓ)
j1
dz(ℓ+1)
i
dz(ℓ)
j2

= E

X
j1,j2
W (ℓ+1)
ij1
W (ℓ+1)
ij2
σ′ (ℓ)
j1 σ′ (ℓ)
j2

= CW

σ′(z)σ′(z)

K(ℓ) = χ(ℓ)
⊥,
(9.93)
thus making this connection explicit.
As we discussed in §9.1 under the heading Formalities: perpendicular perturbations
and the frozen NTK, we need to ensure that these multiplicative chain-rule factors are
under control for training to be well behaved, on average. In particular, if χ⊥(K⋆) > 1,
then the deeper-layer NTKs will exponentially explode, and the training dynamics will
be unstable, potentially leading to growing losses.
If instead χ⊥(K⋆) < 1, then the
contribution of the biases and weights in the shallower layers to the deeper-layer NTKs
will be exponentially diminished. This means both that such parameters will struggle
to move via gradient-descent updates and also that they will struggle to inﬂuence the
evolution of the parameters in the deeper layers.
All in all, we see that contribution of the chain-rule factor to the exploding and
vanishing gradient problem is directly connected to an exponentially growing or decaying
multiplicative factor in the NTK and further see how our criticality condition imposed
on the perpendicular susceptibility, χ⊥(K⋆) = 1, serves to mitigate it.9
8Incidentally, for the scale-invariant universality class, this same logic provides an additional justiﬁ-
cation for avoiding the exploding kernel problem. That is, since scale-invariant activation functions don’t
saturate, if χ∥(K⋆) > 1, then the activation – and consequentially the trivial factor – would explode.
9Having now explained why criticality is a complete solution to the exploding and vanishing gradient
243

An equivalence principle for learning rates
Although originally derived by considering the behavior of the kernel recursion, we just
recovered both of our criticality conditions χ∥(K⋆) = 1 and χ⊥(K⋆) = 1 by a direct
analysis of gradient-descent updates and of the NTK forward equation. Importantly,
the guiding principle we found was that each layer should not make an exponentially
diﬀerent contribution to the NTK.
However, there’s really no reason for us to stop at the exponential level. In fact, we
can further demand at the polynomial level that no type of model parameter and no layer
dominate over another for training. In other words, rather than requiring more or less
equal contributions from the parameters in diﬀerent layers, we demand parametrically
equal contributions to the NTK for each parameter group from every layer. This gives
an equivalence principle for setting the training hyperparameters, i.e., the bias and
weight learning rates. In fact, en route to this section, we already found a way to satisfy
this equivalence principle for each of our universality classes.
As we retroactively saw in §9.2, the equivalence principle was easily met for activation
functions contained in the scale-invariant universality class by setting the bias and weight
learning rates to be layer-independent:
ηλ(ℓ)
b
= ηeλb
L ,
ηλ(ℓ)
W
nℓ−1
= ηeλW
Lnℓ−1
.
(9.94)
Here, we have also re-emphasized the rescaling of the weight learning rates by width
of the previous layer as we discussed multiple times in §8.0. In particular, you should
understand the parametrically equal contributions provision of equivalence principle as
requiring appropriate depth and width scalings. Also here – with our discussion in §8.0
under the heading Scaling in the eﬀective theory in mind – note that we rescaled the
learning rates by the overall depth L of the network so as to have an order-one NTK
in the output layer. This ensures that we can naturally compare these rescaled learning
rates ηeλb and ηeλW across models of diﬀerent depths as well as that we have properly
scaled order-one changes in observables, e.g. the loss, after any gradient-descent update.
Meanwhile for the K⋆= 0 universality class, we retroactively saw in §9.3 that the
problem, let us discuss remedies of traditionality.
One of the ﬁrst heuristic solutions – ﬁrst discussed in the context of recurrent neural networks –
is gradient clipping [60], in which the norm of the gradient is reduced whenever it exceeds a certain
threshold. As should be apparent given our discussion, such an ad hoc, distortionary, and hard-to-tune
heuristic is completely unnecessary – and potentially even destructive – in networks that are at criticality.
A second heuristic solution was the adoption of the ReLU. Recall that activation functions such as the
tanh and the sigmoid saturate when |z| →∞. This implies that the derivative of the activation vanishes
upon saturation as σ′(z) = 0. Such saturation naturally leads to vanishing gradients, as we can easily
see from the right-hand side of (9.90). It is partially within this context that practitioners adopted the
ReLU over saturating activation functions such as the tanh (see, e.g. [19]). However, with our deeper and
more critical understanding, we now appreciate that criticality is suﬃcient to mitigate this vanishing
gradient problem for any activation function that admits critical initialization hyperparameters, even for
saturating ones like tanh.
244

equivalence principle requires
ηλ(ℓ)
b
= ηeλb
1
ℓ
p⊥
Lp⊥−1 ,
ηλ(ℓ)
W
nℓ−1
= ηeλW
nℓ−1
L
ℓ
p⊥−1
,
(9.95)
where here we recall our discussion of having separate depth scalings for the bias and
weight learning rates (9.70) as a way to ensure both uniform contributions in parameter
groups and in layers to the asymptotic frozen NTK solution (9.71).10
In particular,
for odd smooth activation functions such as tanh and sin the critical exponent for
perpendicular perturbations is given by p⊥= 1, and we have a simpler prescription:
ηλ(ℓ)
b
= ηeλb
ℓ
,
ηλ(ℓ)
W
nℓ−1
= ηeλW
nℓ−1
.
(9.96)
With this, we further wonder whether the empirical preference for ReLU over tanh is at
least partially due to the fact that the ReLU learning rates are naturally ℓ-independent
(9.94) while the tanh learning rates require nontrivial ℓ-rescalings (9.96).
In conclusion, while the optimal values of the order-one training hyperparameters
ηeλb and ηeλW surely depend on the speciﬁcs of a task, we expect that the layer ℓ, depth
L, and width nℓ−1 scalings dictated by the equivalence principle will lead to the least
variation across network architectures with diﬀering widths nℓand depths L.
10Please do not confuse these ℓ-rescalings with L-rescalings. The former modiﬁes the learning rates of
a given layer ℓby a factor of ℓ, and the later modiﬁes the learning rates of any layer in the network by
the overall network depth L.
Also, with the recent critical discussion of the exploding and vanishing kernel problem in mind, you
should see that this relative ℓ-scaling between the bias and weight learning rates is just a polynomial
version of the vanishing kernel problem: the equivalence principle ensures that deeper-layer weights
receive polynomially non-vanishing gradients as well as contribute polynomially-equally to the training
dynamics of other parameters.
245

246

Chapter 10
Kernel Learning
I protest against the use of an inﬁnite quantity as an actual entity; this is never
allowed in mathematics. The inﬁnite is only a manner of speaking . . . .
Carl Friedrich Gauss [61].
Now that we know essentially everything we can possibly know about the initialization
distribution of the preactivations and the NTK, it’s ﬁnally time to learn with gradients!
In this chapter, we’ll analyze the training of inﬁnite-width neural networks by gradient-
descent optimization. Of course, the inﬁnite-width network is really only a manner of
speaking, and we cannot actually instantiate one in practice. However, as we saw from
our previous ﬁnite-width analyses, they can still provide a useful model of an actual
entity when the depth-to-width ratio is suﬃciently small.
Thus, the analysis of such networks is important for two reasons. First, this limit
can tell us a lot about the correct scaling and tuning of our various hyperparameters;
we’ve already seen this previously as our criticality analysis always begins at inﬁnite
width. Second, since our ﬁnite-width analysis is perturbative in 1/n, understanding the
inﬁnite-width limit is a prerequisite for us to understand learning with more realistic
ﬁnite-width networks in §11 and §∞. With those remarks in mind, let’s preview our
analysis of gradient-based learning at inﬁnite width.
Just as a new biological neural network begins its journey by taking a small step, so
does a freshly-initialized artiﬁcial neural network. In §10.1 we’ll take such a step, ob-
serving that the gradient-descent training of inﬁnite-width networks is simply described
by the frozen NTK and that the change in the network outputs can be consitently trun-
cated to linear order in the global learning rate. This simplicity leads us ﬁrst to observe
that the network’s output components move independently of each other (§10.1.1) and
then second to ﬁnd an absence of representation learning in the hidden layers (§10.1.2).
At this point you might have an uncanny sense of d´ej`a vu, as we found the exact same
limitations in §6.3 for inﬁnite-width networks that learn via exact Bayesian inference.
After a small step comes a giant leap. In §10.2 we’ll make a large parameter update
and ﬁnd a closed-form solution for a fully-trained inﬁnite-width network.
For these
247

networks, such a solution memorizes the entire training set, and we’ll show that this
solution is the same regardless of whether we reach it in one Newton step (§10.2.1) or
many steps of (stochastic) gradient descent (§10.2.2), and doesn’t depend on the form
of the loss that we use (§10.2.3).
In fact, in §10.2.4 we’ll see that the prediction of a particular fully-trained inﬁnite-
width network on unseen test inputs is ﬁxed entirely by the initial network output, the
frozen NTK, and the contents of the training set. To analyze this, we evaluate the statis-
tics of the associated ensemble, identifying the mean (neural tangent) kernel prediction
as well as the covariance of that prediction across diﬀerent realizations. Recalling our
discussion of approximate methods for Bayesian model ﬁtting in §6.2.1, we are now able
to make more precise the connection between gradient-based learning and maximum
likelihood estimation by discussing the sense in which our distribution over fully-trained
inﬁnite-width networks is a generalized posterior distribution.
In §10.3, we’ll put the predictions of these fully-trained inﬁnite-width networks to the
test. Here we’ll introduce the quantitative measure of training success, the generalization
error, and decompose it into a bias term and a variance term. The former term compares
the mean predictions of the ensemble on the test inputs to the true function values from
the test set, while the latter term measures the instantiation-to-instantiation ﬂuctuations
of that prediction across diﬀerent fully-trained networks in our ensemble.
Naturally, there is a tradeoﬀbetween these bias and variance terms, corresponding to
our preference for the ensemble to contain networks that are both ﬂexible and conﬁdent.
By explicitly working out the generalization error when a test input is near one of the
training samples in §10.3.1, we’ll see how balancing such a tradeoﬀgives a prescription
for tuning the initialization hyperparmeters, via the principle of criticality, and for tuning
the training hyperparameters, according to the learning rate equivalence principle.
In §10.3.2 we’ll extend our analysis to situations where a test input is near two
training samples. This will let us understand how our fully-trained networks interpolate
and extrapolate, letting us comment on the activation-function-induced inductive bias of
the network output in general. In particular, we’ll be able to see how nonlinear activation
functions are able to nonlinearly interpolate or extrapolate around two training examples.
Finally, in §10.4 we’ll take a small step back to give our discussion of inﬁnite-width
networks a broader context. In particular, we’ll introduce the linear model, one of the
simplest models in traditional machine learning, and explain its relationship to another
traditional set of algorithms known as kernel methods. This will let us see that inﬁnite-
width MLPs are essentially just linear models based on random features and dually let
us identify both the inﬁnite-width Bayesian kernel and the frozen neural tangent kernel
with this more traditional notion of a kernel.
After discussing the limitations of such kernel methods, you will thoroughly under-
stand the need to go beyond the inﬁnite-width limit so that our eﬀective theory can fully
incorporate some of the more exciting properties of practical deep learning models.
248

10.1
A Small Step
Now let’s take our ﬁrst step in a journey towards the minimum of the loss. We’ll begin
by considering how the preactivations change in the ﬁrst step after initialization at t = 0.
Recalling that the evolution of any neural-network observable O

z(ℓ)
is governed
by the NTK through the update equation (8.3), we have for an ℓ-th-layer preactivation:
d¯z(ℓ)
i;δ ≡z(ℓ)
i;δ (t = 1) −z(ℓ)
i;δ (t = 0)
(10.1)
= −η
nℓ
X
j=1
X
˜α∈A
dLA
dz(ℓ)
j;˜α
bH(ℓ)
ij;˜αδ + O

η2
.
Here, the ℓ-th-layer NTK bH(ℓ)
ij;˜αδ and the factor dLA/dz(ℓ)
j;˜α are both evaluated at initial-
ization; from now on we’ll drop the explicit dependence on the number of steps t when
a quantity is being evaluated at initialization t = 0, unless we want to emphasize it for
clarity. Henceforth, we’ll use the preﬁx d¯ to indicate the update to a quantity after the
ﬁrst step of gradient descent.
Further, in writing (10.1) we have resurrected the sample-index notation of alpha-
with-tilde for the inputs in the training set ˜α ∈A, and we will soon resurrect beta-with-
dot for inputs in the test set ˙β ∈B; as before, we’ll also use delta-with-no-decoration
for generic inputs in either set: δ ∈D = A ∪B. Thus, to be explicitly clear, the update
(10.1) gives the change after the ﬁrst gradient descent training step in an ℓ-th-layer
preactivation evaluated on a sample from either the test set or the training set.
Now, let’s specialize to the inﬁnite-width limit. Recall in this limit that the NTK
self-averages, such that the NTK for any particular realization of the network parameters
will be equal to the inﬁnite-width NTK mean, which we have been calling the frozen
NTK: bH(ℓ)
ij;˜αδ = δijΘ(ℓ)
˜αδ + O(1/n). With this in mind, the update equation at inﬁnite
width simpliﬁes to
d¯z(ℓ)
i;δ = −η
X
˜α∈A
Θ(ℓ)
δ˜α
dLA
dz(ℓ)
i;˜α
+ O
 1
n

.
(10.2)
Here, the update does not mix neural indices as the mean of the NTK is diagonal in those
indices, while the presence of oﬀ-diagonal terms in the frozen NTK would indicate that
information from one training sample informs the update of another. Note importantly
that we have purposefully truncated the + O
 η2 part of (10.1) that contains higher-
order corrections to the update from the series expansion in the global learning rate η;
in §11 we’ll explicitly analyze these O
 η2 terms and show that they are suppressed by
1/n. Thus, in the strict inﬁnite-width limit they identically vanish, making the linear
truncation exact.
In this section, we’ll take a look at what such an inﬁnite-width small-step update
entails for the network outputs with ℓ= L (§10.1.1) and for preactivations in the ﬁnal
hidden layer with ℓ= L −1 (§10.1.2).1 These analyses will more or less parallel §6.3.2
1After reading the next section, it should be clear that the results here are true even at the minimum
of the loss at the end of training. We say this now to head oﬀany potential objections of the form,
“What if there are some number of steps t for which the quantity ηt/n is of order one?”
249

and §6.3.3, where we considered the posterior distribution for inﬁnite-width networks
updated via exact Bayesian inference.
10.1.1
No Wiring
Specializing to the network output z(L)
i;δ , the update (10.2) simply becomes
d¯z(L)
i;δ = −η
X
˜α∈A
Θ(L)
δ˜α ϵi;˜α ,
(10.3)
where we recall the now-familiar error factor deﬁned in (7.16) as
ϵi;˜α ≡∂LA
∂z(L)
i;˜α
.
(10.4)
We’re going to extensively analyze this update to network outputs in §10.2 and onwards.
Here, let us just point out that the update to the i-th feature z(L)
i;δ (t = 1) depends only
on the i-th component of the error factor ϵi;˜α. This mirrors the phenomenon of no wiring
for the network outputs that we observed in §6.3.2 for the exact Bayesian inference at
inﬁnite width.
To be more concrete, for the MSE loss (7.2),
LA ≡1
2
nL
X
i=1
X
˜α∈A

z(L)
i;˜α −yi;˜α
2
,
(10.5)
the error factor is simply given by the diﬀerence between the true output and the initial
output, ϵi;˜α = z(L)
i;˜α −yi;˜α. We thus see that all the output components move indepen-
dently from each other, and there’s no way for correlations between these components to
be learned.2 In addition, since (10.3) is a stochastic equation describing the update to
any particular network in the ensemble, there is no wiring for any particular realization
of a one-step-into-training inﬁnite-width network.
10.1.2
No Representation Learning
We’ll have to work a little harder to analyze the update to the preactivations in the
penultimate layer z(L−1)
i;δ
. To start, we can evaluate the derivative of the loss in the
2As another example, we can take a cross-entropy loss of the form LA = −P
j,˜α pj;˜α log(qj;˜α); feel
free to ﬂip forward and look at (10.36). In this case we have a target distribution pi;˜α for which we want
to ﬁt the softmax distribution – cf. (6.11) – of the network outputs qi;˜α ≡exp(z(L)
i;˜α )/ P
k exp(z(L)
k;˜α)
.
After noting that ∂qj;˜α/∂z(L)
i;˜α = (δij −qi;˜α)qj;˜α, we ﬁnd for the error factor
ϵi;˜α =
X
j
∂LA
∂qj;˜α
∂qj;˜α
∂z(L)
i;˜α
= −
X
j
pj;˜α
qj;˜α qj;˜α(δij −qi;˜α) = −pi;˜α +   X
j
pj;˜α

qi;˜α = qi;˜α −pi;˜α .
(10.6)
Therefore, in this case too we see that the error factor ϵi;˜α depends only on the i-th component of the
softmax distribution, and no correlation between output components will be generated.
250

update equation (10.2) using the backward equation (8.17):
dLA
dz(L−1)
j;˜α
=
nL
X
i=1
∂LA
∂z(L)
i;˜α
dz(L)
i;˜α
dz(L−1)
j;˜α
=
nL
X
i=1
∂LA
∂z(L)
i;˜α
W (L)
ij σ′ (L−1)
j;˜α
.
(10.7)
Substituting this into the update (10.2) at ℓ= L −1, we get a stochastic equation
describing the change in the ﬁnal hidden-layer representation for any particular network:
d¯z(L−1)
j;δ
= −η
nL
X
i=1
X
˜α∈A
Θ(L−1)
δ˜α
∂LA
∂z(L)
i;˜α
W (L)
ij σ′ (L−1)
j;˜α
.
(10.8)
To make progress here, we’re going to have to analyze the distribution over such updates.
First, the mean update is given by
E
h
d¯z(L−1)
j;δ
i
= −η
nL
X
i=1
X
˜α∈A
Θ(L−1)
δ˜α
E

∂LA
∂z(L)
i;˜α
W (L)
ij σ′ (L−1)
j;˜α

.
(10.9)
This expectation involves an interlayer correlation between the error factor ∂LA/∂z(L)
i;˜α
from the L-th layer and the derivative of the activation σ′ (L−1)
j;˜α
from the (L −1)-th
layer, in addition to a weight insertion W (L)
ij . From our previous experience we know
that such interlayer expectations are suppressed by a factor of 1/n, vanishing in the
strict inﬁnite-width limit. To be extra careful, let’s compute the mean explicitly.
To do so, recall our generating function for interlayer correlations (8.53) and specialize
to the penultimate layer ℓ= L −1. (You’ll probably want to ﬂip back and remind
yourself.) Since we have not previously evaluated the case with a single weight insertion,
let’s calculate and record it here.
Diﬀerentiating the generating function once with
respect to the source as
d
dJij and then setting the source to zero, we get
E
h
O

z(L)
W (L)
ij Q

z(L−1)i
= C(L)
W
nL−1
X
δ∈D
E


**
∂O
∂z(L)
i;δ
++
b
G(L)
σ(L−1)
j;δ
Q

z(L−1)

.
(10.10)
Applying this formula to the above expression for our update (10.9), we get
E
h
d¯z(L−1)
j;δ
i
= −η C(L)
W
nL−1
nL
X
i=1
X
˜α1 ˜α2∈A
Θ(L−1)
δ˜α1
E


**
∂2LA
∂z(L)
i;˜α1∂z(L)
i;˜α2
++
b
G(L)
σ(L−1)
j;˜α2
σ′ (L−1)
j;˜α1

(10.11)
= −η C(L)
W
nL−1
X
˜α1,˜α2∈A
Θ(L−1)
δ˜α1
nL
X
i=1
**
∂2LA
∂z(L)
i;˜α1∂z(L)
i;˜α2
++
K(L)

σ′
˜α1σ˜α2

K(L−1)+O
 1
n2

,
and see that this expression is manifestly suppressed by 1/n. Thus, on average across
our ensemble, we have found that there’s no representation learning in the penultimate
layer in the inﬁnite-width limit.
251

Next, let’s consider the variance of the update (10.8):
E
h
d¯z(L−1)
j1;δ1 d¯z(L−1)
j2;δ2
i
(10.12)
=η2
nL
X
i1,i2=1
X
˜α1 ˜α2∈A
Θ(L−1)
δ1 ˜α1 Θ(L−1)
δ2 ˜α2 E

∂LA
∂z(L)
i1;˜α1
∂LA
∂z(L)
i2;˜α2
W (L)
i1j1W (L)
i2j2σ′ (L−1)
j1;˜α1
σ′ (L−1)
j2;˜α2


=δj1j2 η2 C(L)
W
nL−1
X
˜α1 ˜α2∈A
Θ(L−1)
δ1 ˜α1 Θ(L−1)
δ2 ˜α2
nL
X
i=1
**
∂LA
∂z(L)
i1;˜α1
∂LA
∂z(L)
i2;˜α2
++
K(L)

σ′
˜α1σ′
˜α2

K(L−1) + O
 1
n2

.
In the last step, we applied our interlayer correlation formula with two weight insertions
(8.55) and then picked up the leading contribution.3 With this, we see that the covariance
of the update,
Cov
h
d¯z(L−1)
j1;δ1 , d¯z(L−1)
j2;δ2
i
≡E
h
d¯z(L−1)
j1;δ1 d¯z(L−1)
j2;δ2
i
−E
h
d¯z(L−1)
j1;δ1
i
E
h
d¯z(L−1)
j2;δ2
i
= O
 1
n

,
(10.13)
is manifestly suppressed by 1/n, vanishing in the strict inﬁnite-width limit. Since the
distribution of updates to the penultimate-layer preactivations has a vanishing mean
and covariance, we conclude that the distributions before and after the learning update
are equal: mirroring what we found for exact Bayesian inference in §6.3.3, there’s no
representation learning for gradient-based learning in the inﬁnite-width limit.4
10.2
A Giant Leap
That’s one small step for [a] machine, one giant leap for AI.
Neil AI-Strong
In the last section, we started to understand training for inﬁnite-width networks by tak-
ing a small step of gradient descent. Of course, what we’d actually like is to understand
the behavior of fully-trained networks at the minimum of their losses. Naturally, we
could continue by taking many many small steps until our networks are fully trained,
and indeed this is how networks are typically trained in practice.
That said, in §10.2.1 we’ll ﬁrst show that we can actually fully train inﬁnite-width
networks in one theoretical gradient-descent step. That is, we can take a giant leap right
to the minimum of the loss. We’ll then explain in §10.2.2 that the theoretical minimum
we’ve found by our giant leap is the same minimum we would have found in practice by
taking many steps of gradient descent, or even by using stochastic gradient descent with
3Note that the second term in (8.55) is of order O 1/n2
and hence subleading.
4You can check the higher-order connected correlators of the update distribution, if you’d like. How-
ever, as we already said before we started these computations, these sorts of interlayer correlations are
naturally suppressed by factors of 1/n, and so will be the higher-order connected correlators.
252

decreasing learning rates. This equivalence makes our giant leap a powerful theoretical
tool. After a brief aside about the cross-entropy loss in §10.2.3, ﬁnally in §10.2.4 we’ll
see how our fully-trained inﬁnite-width networks make predictions on previously unseen
examples, though a detailed analyses of these test-set predictions will be postponed until
the following section.
10.2.1
Newton’s Method
Our ﬁrst goal is to ﬁnd a single step such that the network outputs equal the true
outputs,
z(L)
i;˜α (t = 1) = yi;˜α ,
(10.14)
for all samples x˜α in the training set ˜α ∈A. This condition will be our deﬁnition of
fully trained, and it’s easy to see that such a condition will minimize the training loss for
any of the loss functions that we’ve described. Recalling the gradient-descent update for
neural-network outputs (10.3) and rearranging terms, we see that our giant-leap update
must satisfy
z(L)
i;˜α1 −yi;˜α1 = η
X
˜α2∈A
eΘ(L)
˜α1 ˜α2
∂LA
∂z(L)
i;˜α2
(10.15)
for the network to be fully trained.
As a reminder, our convention is that quantities without an explicit step argument
are evaluated at the point of initialization t = 0; in particular, the constraint (10.15)
is written solely in terms of the quantities at initialization. Additionally, note that the
tilde on eΘ(L)
˜α1 ˜α2 emphasizes that it’s a NA × NA-dimensional submatrix of the full frozen
NTK matrix Θ(L)
δ1δ2 evaluated on pairs of inputs (x˜α1, x˜α2) in the training set A only.
This emphasis will soon prove itself useful, as it did before in §6.
How can we satisfy our giant-leap condition (10.15)?
Since the left-hand side is
exactly the error factor of the MSE loss (10.5), let’s ﬁrst specialize to the MSE loss.
Plugging in (10.5) for LA, we get a concrete equation to solve:
z(L)
i;˜α1 −yi;˜α1 =
X
˜α2∈A
η eΘ(L)
˜α1 ˜α2

z(L)
i;˜α2 −yi;˜α2

.
(10.16)
However, for generic neural networks, the frozen NTK Θ(L)
˜α1 ˜α2 will have nonzero oﬀ-
diagonal components mixing diﬀerent sample indices. This unfortunately means that
the condition (10.16) is impossible to satisfy by the tuning of the single global learning
rate η.
Said another way, the issue is that our global learning rate η is a scalar, but here we
need it to be tensor in order to undo the mixing of the sample indices by the frozen NTK.
To enable this, we need to further generalize gradient descent. In our ﬁrst extension of
the gradient descent algorithm (7.11) – discussed under the heading Tensorial Gradient
Descent – we introduced a learning-rate tensor on parameter space,
η →ηλµν ,
(10.17)
253

which let us mediate how each model parameter individually contributes to the gradient-
descent update of the others and let us take steps with unequal magnitudes in various
directions in parameter space. The consequence of having such a learning-rate tensor
was integrated into the deﬁnition of the NTK and then informed our analyses in §7–§9.5
Now, we need to further extend this generalization to sample indices as
ηλµν →ηλµνκ˜α1 ˜α2 ,
(10.18)
where we have introduced a new symmetric matrix κ˜α1 ˜α2 that we will call the Newton
tensor. This enables us to take an anisotropic step in sample space as well. Speciﬁcally,
we extend the parameter update equation (7.11) to
d¯θµ ≡θµ(t = 1) −θµ(t = 0) = −
X
ν,˜α1,˜α2,i
ηλµνκ˜α1 ˜α2 dz(L)
i;˜α1
dθν
∂LA
∂z(L)
i;˜α2
,
(10.19)
which we will call a second-order update.6 Plugging this second-order update into
our expansion for the network outputs, we get
z(L)
i;δ1(t = 1) = z(L)
i;δ1 +
X
µ
dz(L)
i;δ1
dθµ
d¯θµ + O
 1
n

(10.20)
= z(L)
i;δ1 −η
X
˜α2,˜α3∈A
Θ(L)
δ1 ˜α2κ˜α2 ˜α3 ∂LA
∂z(L)
i;˜α3
+ O
 1
n

.
Substituting this update into our fully-trained condition (10.14) and still using the MSE
loss, we get a new constraint
z(L)
i;˜α1 −yi;˜α1 =
X
˜α2,˜α3∈A

η eΘ(L)
˜α1 ˜α2κ˜α2 ˜α3 
z(L)
i;˜α3 −yi;˜α3

.
(10.21)
We’ll satisfy this constraint shortly.
For a diﬀerent perspective on this new second-order update, rather than modifying
the optimization algorithm, we can instead ﬁnd the same constraint (10.21) by adopting
a diﬀerent loss. Consider a generalized MSE loss
LA, κ(θ) = 1
2
nL
X
i=1
X
˜α1,˜α2∈A
κ˜α1 ˜α2 
z(L)
i;˜α1 −yi;˜α1
 
z(L)
i;˜α2 −yi;˜α2

,
(10.22)
5Most importantly, this let us scale the eﬀective learning rate diﬀerently for the biases and weights;
we saw in §8.0 and then in §9.4 that this was essential for ensuring that both parameter groups get
properly trained. Also, please remember that, even when written generally as λµν, our learning-rate
tensor is restricted such that it does not mix parameters from diﬀerent layers.
6The name of this update descends from the fact that similar updates are used to deﬁne optimization
algorithms that incorporate information from the second derivative of the loss. Such algorithms are
generally called second-order methods. We will show shortly that this new algorithm minimizes the loss
just as well (better, actually).
254

where here the Newton tensor κ˜α1 ˜α2 acts as a metric on sample space.7 For this loss the
derivative with respect to the network output – i.e. the error factor – is now given by
∂LA
∂z(L)
i;˜α2
=
X
˜α3∈A
κ˜α2 ˜α3 
z(L)
i;˜α3 −yi;˜α3

.
(10.23)
Substituting this error factor into our condition for being fully trained (10.15), we ﬁnd the
same constraint (10.21) using a standard gradient-descent update with our generalized
MSE loss (10.22) as we did just before using our second-order update (10.19) with the
standard MSE loss. Either perspective is a valid way to think about our theoretical
optimization and, as we will explain more generally in §10.2.2, any of these choices of
algorithms and losses will lead to the same fully-trained network.
Now, let’s ﬁnd the solution to our giant-leap constraint (10.21). By inspection, this
is satisﬁable if we can set the term in the ﬁrst parenthesis to the identity matrix
X
˜α2∈A
η eΘ(L)
˜α1 ˜α2κ˜α2 ˜α3 = δ ˜α3
˜α1 ,
(10.24)
which we can ensure by setting the product of the global learning rate and the Newton
tensor as
ηκ˜α1 ˜α2 = eΘ˜α1 ˜α2
(L)
.
(10.25)
Here, the object on the right-hand side is the inverse of the NA × NA-dimensional
submatrix eΘ(L)
˜α1 ˜α2, which as a reminder is evaluated on pairs of inputs (x˜α1, x˜α2) in the
training set A only, and is deﬁned via the equation
X
˜α2∈A
eΘ˜α1 ˜α2
(L)
eΘ(L)
˜α2 ˜α3 = δ ˜α1
˜α3 .
(10.26)
Similarly to our work in §6.3 on inﬁnite-width Bayesian inference, the decoration of
these submatrices with tildes is useful in order to clearly distinguish these submatrices
from submatrices that also involve the test set B. Also, as before for the kernel and its
inverse, we will always denote the NTK inverse by an object with sample indices raised.
The algorithm with the particular choice (10.25) is known as Newton’s method
(which acausally explains why we called κ˜α1 ˜α2 the Newton tensor).8 With it, we can
simply write down a solution that fully trains the network in one step:
θ⋆
µ = θµ(t = 0) −
X
ν,˜α1,˜α2,i
λµν
dz(L)
i;˜α1
dθν
eΘ˜α1 ˜α2
(L)

z(L)
i;˜α2 −yi;˜α2

.
(10.30)
7Similarly, we could have taken the perspective that the learning-rate tensor λµν acts as a metric on
parameter space.
Note also that with this interpretation, the standard MSE loss is just the generalized MSE loss with
the Euclidean metric κ˜α1 ˜α2 →δ ˜α1 ˜α2. In some sense, it’s more pleasing to write it this way if you’re
familiar with general relativity; writing the Newton tensor with sample indices raised allows us to adopt
a rule of only summing over sample indices when they come in a raised-lowered pair. Similarly, note
that the insertion of the Newton tensor in our second-order update (10.19) follows this pattern as well.
8Newton’s method is a numerical method for ﬁnding a zero of a function. For simplicity of presentation,
let’s take a single-variable function g(x) and suppose that we want to ﬁnd a solution to the equation
g(x⋆) = 0; note that this is equivalent to extremizing a function L(x) whose derivative is g(x), i.e. L′(x) =
255

In particular, this is exactly what we’d ﬁnd by setting the gradient of the loss to zero
and solving for the optimal parameters as in (7.7). As we explained back there, such a
direct and explicit solution to an optimization problem is only available in special cases,
and it turns out that this is precisely the case at inﬁnite width.9
Plugging the Newton’s method update in our expansion for the network outputs
(10.20), we then ﬁnd the fully-trained network output for a general input δ ∈D:
z(L)
i;δ (t = 1) = z(L)
i;δ −
X
˜α1,˜α2∈A
Θ(L)
δ˜α1 eΘ˜α1 ˜α2
(L)

z(L)
i;˜α2 −yi;˜α2

.
(10.31)
In particular, for samples in the training set A, the network output equals the true output
z(L)
i;˜α (t = 1) = yi;˜α, satisfying our condition for the network to be fully trained (10.14). In
other words, our network has perfectly memorized the entire training set.10 As such, this
solution (10.30) also minimizes any loss LA(θ) that is minimized by setting the network
outputs to the true outputs z(L)(x˜α; θ) = yi;˜α:
θ⋆
Newton = arg min
θ
LA(θ) .
(10.32)
This means that regardless of whether we used the standard MSE loss (10.5) or the
g(x). Newton’s method instructs us to start with some guess x0 and then iterate as
xt+1 = xt −g(xt)
g′(xt) = xt −L′(xt)
L′′(xt) .
(10.27)
This algorithm is based on making a linear approximation g′(xt) ≈[g(xt+1) −g(xt)]/(xt+1 −xt) and
then solving for g(xt+1) = 0. In general, one needs to iterate (10.27) for several steps in order to get a
good approximate solution x⋆. When the function is linear as g(x) = a(x −x⋆), however, we get
x1 = x0 −a(x0 −x⋆)
a
= x⋆,
(10.28)
for any starting point x0. Hence Newton’s method can land right on the solution in one step, just like
our giant leap (10.30) did.
The right-hand side of (10.27) oﬀers another perspective: Newton’s method is gradient descent with
a “loss” L(x) and a learning rate set as ηt = 1/L′′(xt). To see why this is a good choice for the learning
rate, let’s choose a generic learning rate xt+1 = xt−ηtL′(xt) and Taylor-expand the updated loss L(xt+1)
to the second order in ηt:
L(xt+1) = L(xt) −ηtL′(xt)2 + η2
t
2 L′(xt)2 L′′(xt) + O η3
t

.
(10.29)
Optimizing the learning rate, we see that the truncated expression on the right-hand side is minimized
when ηt = 1/L′′(xt). In particular, for a quadratic function L(x) = a(x −x⋆)2/2 this truncation is
exact, and Newton’s method again reaches the minimum in one step.
This also makes it clear why
optimization algorithms based on Newton’s method fall in the class of second-order methods: each
iteration uses second-order information from the function – the second derivative L′′(x) – to set the
locally optimal learning rate. Our giant leap expressed in (10.30) and (10.31) is doing exactly that –
successfully – for the parameter optimization and for the function approximation, respectively.
9We’ll later show in §∞.2 that perturbative solutions are possible at ﬁnite width.
10Since inﬁnite-width networks have inﬁnite parameters, it shouldn’t be surprising that in this limit
the network can memorize the ﬁnite training set.
256

generalized MSE loss (10.22) (or an entirely diﬀerent loss as long as it has a minimum
at z(L)(x˜α; θ) = yi;˜α), our solution (10.31) will faithfully describe the minimum.11
10.2.2
Algorithm Independence
Now let’s discuss a related – and by now well anticipated – property of the inﬁnite-width
limit: given a particular initialization z(L)
i;δ (t = 0) and the frozen NTK Θ(L)
δ˜α1, we’ll always
get to exactly the same minimum (10.31), whether we get there by one giant leap or
we get there by a sequence of many small steps. That is, at inﬁnite width we have
algorithm independence.
Let’s suppose that we have taken T −1 steps towards the minimum with a global
learning rate η(t), a Newton tensor κ˜α1 ˜α2(t), and loss LA(t), where these quantities
will in general depend on the step t. Diﬀerent choices of η(t), κ˜α1 ˜α2(t), and LA(t) will
lead to diﬀerent optimization algorithms; included in this class are Newton’s method,
gradient descent, and stochastic gradient descent (SGD).12 Iterating the update (10.20),
the network outputs accumulate the changes as
z(L)
i;δ (T −1) = z(L)
i;δ (t = 0) −
T−2
X
t=0
X
˜α1,˜α2
Θ(L)
δ˜α1 η(t) κ˜α1 ˜α2(t) ϵi;˜α2(t) ,
(10.33)
where ϵi;˜α2(t) ≡∂LA(t)/∂z(L)
i;˜α2 is the error factor for the training loss LA(t) evaluated
with respect to the network output z(L)
i;˜α (t) at step t.
Let’s then suppose that in the next step t = T that we reach the true minimum. We
can ensure this by taking a Newton step from z(L)
i;δ (T −1) such that
z(L)
i;δ (T) = z(L)
i;δ (T −1) −
X
˜α1,˜α2
Θ(L)
δ˜α1 eΘ˜α1 ˜α2
(L)
h
z(L)
i;˜α2(T −1) −yi;˜α2
i
,
(10.34)
where here we set η(T −1) κ˜α1 ˜α2(T −1) = eΘ˜α1 ˜α2
(L)
and chose the standard MSE loss at
t = T −1 with ϵi;˜α2(T −1) = z(L)
i;˜α2(T −1) −yi;˜α2.13 Plugging in our expression for the
11Note that the solution (10.30) depends on the network output at initialization z(L)
i;δ , which ultimately
depend on the initialization of the parameters θinit = θ(t = 0). For diﬀerent initializations, we will reach
diﬀerent solutions (10.30), each of which will minimize the loss given that particular initialization θinit.
We’ll have more to say about this in §10.2.4.
12To see how this includes SGD (7.10), note that we can either restrict the loss to be a summation
over a diﬀerent batch St ⊂A at each step t as LA(t) = LSt, or equivalently we can choose the Newton
tensor κ˜α1 ˜α2(t) to project onto the subset St.
13Note that if we had already reached a minimum at step T −1, then this last Newton’s step in (10.34)
would give no change to the network outputs: z(L)
i;δ (T) = z(L)
i;δ (T −1). Thus, our argument also applies to
any algorithm that already reached a minimum with T −1 other steps, and we do not have to actually
apply the Newton step in practice. We’ll address this point again in the next-to-next footnote.
257

network outputs after the ﬁrst T −1 steps (10.33), we see
z(L)
i;δ (T) =z(L)
i;δ (t = 0) −
T−2
X
t=0
X
˜α1,˜α2
Θ(L)
δ˜α1 η(t) κ˜α1 ˜α2(t) ϵi;˜α2(t)
−
X
˜α1,˜α2
Θ(L)
δ˜α1 eΘ˜α1 ˜α2
(L)




z(L)
i;˜α2(t = 0)−
T−2
X
t=0
X
˜α3,˜α4
eΘ(L)
˜α2 ˜α3 η(t) κ˜α3 ˜α4(t) ϵi;˜α4(t)

−yi;˜α2



=z(L)
i;δ (t = 0) −
X
˜α1,˜α2∈A
Θ(L)
δ˜α1 eΘ˜α1 ˜α2
(L)
h
z(L)
i;˜α2(t = 0) −yi;˜α2
i
,
(10.35)
where to go from the second to the third line we made use of the deﬁning equation
for the NTK submatrix inverse (10.26), thus enabling the cancellation.
What this
result shows is that all the details of the training algorithm in the previous steps
η(t), κ˜α1 ˜α2(t), LA(t)
	
t=0,...,T−2 were erased: the network output z(L)
i;δ (T) after our ﬁ-
nal step t = T here in (10.35) is exactly the same as the network output reached after
one giant leap (10.31).14
Thus, in the inﬁnite-width limit the fully-trained solution is determined by (i) the
frozen NTK Θ(L)
δ˜α , with its details depending on the training hyperparameters in the
learning-rate tensor λµν, (ii) the initial newtork outputs z(L)
i;δ (t = 0), with its distribution
depending on the initialization hyperparameters, and (iii) the true outputs yi;˜α for the
training set A. It doesn’t matter which loss function we used, e.g. MSE or cross-entropy,
how many steps we took to get to the minimum, or whether we used gradient descent
or SGD.15 Said another way, algorithm independence means that these hyperparameters
14In §∞.2.2, we’ll explicitly analyze the dynamics of another optimization algorithm – many many steps
of vanilla gradient descent (7.11) – and evaluate its corresponding fully-trained solution. As expected
by algorithm independence (10.35), in the inﬁnite-width limit this solution agrees completely with other
solutions obtained by diﬀerent training algorithms.
15Some additional comments that didn’t make the cut for the main body:
• Newton’s method is often impractical to implement directly, since we have to compute the inverse
of the frozen NTK submatrix evaluated on the entire training set, eΘ˜α1 ˜α2
(L) , similar to how we had
to invert the kernel for Bayesian inference in §6.3.2. Unlike the case of exact Bayesian inference
where we were considering the feasibility of the learning algorithm, here the point is that Newton’s
method is a theoretical tool that lets us describe a fully-trained extremely-wide network, even if
the network was trained very practically by a many-step version of (stochastic) gradient descent.
• Often theorists will take the limit of a very small step size and approximate the optimization
dynamics with an ordinary diﬀerential equation (ODE). Such an approximation is sometimes
misleading, and here we see that it’s entirely unnecessary.
• For SGD to actually converge to a minimum, you need to decrease the learning rate over the
course of training, otherwise the network will ﬂuctuate around, but never actually reach, the
minimum. Intuitively, this is because at each step the optimization problem does not include the
entire training set.
• A curious reader might wonder what happens if one cannot take a ﬁnal step according to Newton’s
method, for instance because it’s impractical to invert the frozen NTK submatrix. In fact, if you’re
already close to the minimum at t = T −1, i.e. if you’re essentially at the end of training, then
the ﬁnal step to land exactly on the minimum will be extremely small, and the solution (10.35)
is a very good approximation of the network before taking this last theoretical jump.
258

and training set uniquely specify the statistics of fully-trained networks in the inﬁnite-
width limit; Newton’s method is just a nice theoretical trick to leap right to the solution.
In this way, we can use the giant-leap solution (10.31) to study the outcome of all these
diﬀerent optimization algorithms, which is what we’ll do after a brief aside about the
cross-entropy loss.
10.2.3
Aside: Cross-Entropy Loss
Let’s take a brief aside to bring the cross-entropy loss out of the footnotes and into the
main body. In general, the cross-entropy loss for some dataset D is deﬁned as
LD = −
X
δ∈D
nout
X
i=1
p(i|xδ) log[q(i|xδ)] ,
(10.36)
where p(i|xδ) is a discrete distribution over the components i of the true output
p(i|xδ) ≡
exp[yi;δ]
Pnout
j=1 exp[yj;δ] ,
(10.37)
and q(i|xδ) is similarly a discrete distribution over the components i of the network’s
output
q(i|xδ) ≡
exp
h
z(L)
i;δ (t)
i
Pnout
j=1 exp
h
z(L)
i;δ (t)
i .
(10.38)
As we mentioned when discussing the categorical hypothesis in the context of Bayesian
model ﬁtting in §6.2.1, the discrete distribution used for (10.37) and (10.38) is sometimes
referred to as the softmax (6.11). The cross-entropy loss (10.36) is a natural measure of
the closeness of discrete distributions such as (10.37) and (10.38).16
In particular, cross-entropy loss is the appropriate choice for classiﬁcation, when we
want to sort the input x into one of nout diﬀerent classes or categories. Accordingly,
the softmax distribution (10.38) transforms the model’s output vector with nout real
components into a discrete probability distribution. In contrast, the MSE loss is the
appropriate choice for regression, when the function we want to learn is a vector of real
numbers.17 Importantly, when the initialization hyperparameters are tuned to criticality
16The proper measure of closeness of distributions is really the Kullback–Leibler (KL) divergence
(A.12), which we will describe in detail in Appendix A. However, the KL divergence KL [p || q] and
the cross-entropy loss (10.36) only diﬀer by a z(L)-independent constant, the entropy S[p(i|xδ)] =
−P
δ∈D
Pnout
i=1 p(i|xδ) log[p(i|xδ)] to be exact, and thus the use of one versus the other is identical
under any gradient-based learning algorithm. Note also the lack of exchange symmetry in either loss
between p and q. The choice in (10.36) is purposeful and reﬂects the fact that an untrained model is on
a diﬀerent footing than the true distribution from which observations arise, analogous to the asymmetry
between the prior and posterior in Bayesian inference.
17We can think of each loss as descending from a diﬀerent Bayesian hypothesis, cf. our discussion
of the uncertain hypothesis and the MSE loss (6.10) and the categorical hypothesis and the softmax
distribution (6.11) in the context of Bayesian model ﬁtting in §6.2.1.
259

and the training hyperparameters are selected according to the learning rate equivalence
principle, both losses will be completely well behaved during training.
When using the cross-entropy loss, typically the true outputs for the training set are
given in terms of the softmax values p(i|x˜α) rather than in terms of continuous vectors
yi;˜α.
Even more typically, the values p(i|x˜α) specify a particular label, i = i⋆
˜α, with
absolute certainty, p(i⋆
˜α|x˜α) = 1, while the rest of the components vanish, p(i|x˜α) = 0
for i ̸= i⋆
˜α; this is known as hard labeling or one-hot encoding, and puts the true
value of the network output yi⋆
˜α;˜α at inﬁnity. In such case, no ﬁnite amount of training
will actually reach the minimum, and in practice as you approach such a minimum the
generalization of the network becomes worse and worse. To remedy this, early stopping
of the training algorithm is used as a regularization technique to eﬀectively get ﬁnite
targets yi⋆
˜α;˜α.18
Now let’s specialize to the current context of training neural networks in the inﬁnite-
width limit (and assume some kind of regularization is used as described above). In the
last section, we noted that any loss that’s minimized by setting the network outputs to
the true outputs for the training set, z(L)(x˜α; θ) = yi;˜α, is described at the minimum
by the Newton’s method giant-leap solution (10.32). It’s easy to check that the cross-
entropy loss (10.36) is minimized when q(i|xδ) = p(i|xδ), and a quick inspection of
(10.37) and (10.38) shows that this is obtained by the condition z(L)(x˜α; θ) = yi;˜α.
We do need to make one additional important remark for the cross-entropy loss.
Since in this setting we specify the true output in terms of a softmax p(i|x˜α) rather than
an nL-component vector of real numbers yi;˜α, there is an ambiguity in how to set the
network output z(L)
i;˜α : any component-independent shift yi;˜α →yi;˜α + c˜α keeps the target
distribution p(i|xδ) invariant. However, since in this case we care not about the network
outputs z(L)
i;δ , but rather their softmax q(i|xδ) (10.38), this ambiguity doesn’t matter in
the end. In particular, a shift yi;˜α →yi;˜α + c˜α in the giant-leap solution (10.35) shifts all
the output components by the same amount for each input xδ, P
˜α1,˜α2∈A Θ(L)
δ˜α1 eΘ˜α1 ˜α2
(L) c˜α2,
leading to the same softmax q(i|xδ). Thus, we see explicitly that our solution (10.35)
unambiguously describes networks fully-trained according to the cross-entropy loss.
10.2.4
Kernel Prediction
After an intelligence – artiﬁcial or otherwise – undergoes an intense memorization session,
often that intelligence is then subjected to test with unseen problems in order to probe
its actual understanding. In the context of machine learning, we typically evaluate our
model’s understanding by asking it to make predictions on novel inputs x ˙β from the test
set ˙β ∈B.
In the inﬁnite-width limit, the predictions of a fully-trained MLP are governed by
18Alternatively we can also explicitly pick a target distribution over the output classes with multiple
nonzero components p(i|x˜α), which is known as soft labeling. This can be implemented as a regularization
technique called label smoothing, where p(i⋆
˜α|x˜α) = 1−ϵ and p(i ̸= i⋆
˜α|x˜α) = ϵ/(nout −1), or as knowledge
distillation, mentioned in footnote 28 of §6, when you actually want to learn such a distribution over
output classes.
260

the stochastic equation
z(L)
i; ˙β (T) = z(L)
i; ˙β −
X
˜α1,˜α2∈A
Θ(L)
˙β ˜α1
eΘ˜α1 ˜α2
(L)

z(L)
i;˜α2 −yi;˜α2

,
(10.39)
whether we train the model in one step (10.31) or in many steps (10.35), and regardless
of the choice of loss function or any other details of the learning algorithm.19 For clarity,
note that the inverse frozen NTK eΘ˜α1 ˜α2
(L)
is taken with respect to the NA-by-NA training-
set submatrix only, while the frozen NTK Θ(L)
˙β ˜α1 is an oﬀ-diagonal block of the full frozen
NTK, connecting an element of the training set to an element of the test set. Note also
that the network outputs z(L)
i;δ on the right-hand side of the equation are evaluated at
initialization: once again, observables without any step argument should be assumed
to be evaluated at initialization, while observables with a step argument T should be
assumed to be evaluated at the end of training.
The stochastic equation (10.39) describes the predictions of a particular instantiation
of a fully-trained neural network. The stochasticity arises from the fact that the pre-
diction (10.39) depends on the network outputs at initialization z(L)
i;δ , which themselves
depend on the particular realization of the initialized parameters θinit ≡θ(t = 0). Since
we already know that such a network has completely memorized the training set so that
z(L)
i;˜α (T) = yi;˜α, the stochasticity here means that any given network in the ensemble can
potentially make diﬀerent predictions on elements of the test set.
With that in mind, let us now compute the full distribution over such test-set predic-
tions for our entire ensemble of fully-trained networks. Inspecting (10.39), we see that
the prediction z(L)
i; ˙β (T) is a simple linear transformations of the Gaussian-distributed ini-
tial outputs z(L)
i;δ and thus will itself be Gaussian. The mean prediction is simply given
by
m∞
i; ˙β ≡E
h
z(L)
i; ˙β (T)
i
=
X
˜α1,˜α2∈A
Θ(L)
˙β ˜α1
eΘ˜α1 ˜α2
(L) yi;˜α2 .
(10.40)
This expression is entirely analogous to the inﬁnite-width posterior mean prediction for
exact Bayesian inference (6.64), with a simple replacement of all types of frozen neural
tangent kernels with kernels: Θ(L) →K(L).
(More on this soon.)
Meanwhile, the
covariance of the prediction (10.39) is given by
Cov
h
z(L)
i1; ˙β1(T), z(L)
i2; ˙β2(T)
i
≡E
h
z(L)
i1; ˙β1(T) z(L)
i2; ˙β2(T)
i
−m∞
i1; ˙β1m∞
i2; ˙β2
=δi1i2
"
K(L)
˙β1 ˙β2 −
X
˜α1,˜α2∈A
Θ(L)
˙β2 ˜α1
eΘ˜α1 ˜α2
(L) K(L)
˙β1 ˜α2 −
X
˜α1,˜α2∈A
Θ(L)
˙β1 ˜α1
eΘ˜α1 ˜α2
(L) K(L)
˙β2 ˜α2
+
X
˜α1,˜α2,˜α3,˜α4∈A
Θ(L)
˙β1 ˜α1
eΘ˜α1 ˜α2
(L) Θ(L)
˙β2 ˜α3
eΘ˜α3 ˜α4
(L) K(L)
˜α4 ˜α2
#
.
(10.41)
19Note that our analyses of §10.1 apply just as much to a small step as they do to a giant leap:
the fully-trained inﬁnite-width network has neither wiring in the vectorial components of the network
output (§10.1.1) nor representation learning (§10.1.2).
261

While this expression is somewhat complicated looking, involving both kernels and frozen
NTKs, it similarly reduces to the Bayesian inﬁnite-width posterior covariance (6.57) with
the substitution Θ(L) →K(L).20
This ensemble of network predictions (10.39), completely speciﬁed by its mean (10.40)
and covariance (10.41), deﬁnes a kind of generalized posterior distribution. This
distribution comprises a complete closed-form solution for our ensemble of inﬁnite-width
networks at the end of training, regardless of the path that we take to get there. The
mean of the distribution is the prediction of the network averaged over instantiations,
while the covariance quantiﬁes the instantiation-to-instantiation ﬂuctuations of these
predictions.
Indeed, it is sensible to identify the ensemble of trained networks as a kind of posterior
distribution, if you recall our discussion of approximation methods for Bayesian inference
in §6.2.1: minimizing a training loss LA(θ) gives the maximum likelihood estimation
(MLE) of the model parameters (6.21), which we now identify with our fully-trained
solution (10.32) θ⋆
MLE = θ⋆
Newton. Further recalling the content of footnote 11 in §6.2.1,
for wide networks the minimum of the loss is not unique, and the MLE approach will
give a family of minima parameterized by the initialization: θ⋆
MLE(θinit).21 This lack
of uniqueness ultimately stems from the lingering dependence of the trained network
prediction z(L)
i; ˙β (T) on the initial function output z(L)
i;δ , which stochastically varies from
instantiation to instantiation, cf. our prediction (10.39). Considering the ensemble over
instantiations of θinit, we now see exactly how this generalized distribution with the
mean (10.40) and covariance (10.41) depends on the training hyperparameters λ(ℓ)
b
and
λ(ℓ)
W , initialization hyperparameters C(ℓ)
b
and C(ℓ)
W , and the training data (x˜α, y˜α)˜α∈A.
To be a little pedantic for a paragraph, the covariance in the generalized posterior dis-
tribution (10.41) really has a diﬀerent interpretation than the posterior covariance (6.57)
20The fact that exact Bayesian inference and gradient descent in general make diﬀerent predictions is
indicative of the fact that – for general hyperparameter settings – they are actually diﬀerent learning
algorithms.
21As per that same footnote, we could also try to analyze the MAP estimate (6.22) in the context of
inﬁnite-width gradient-based learning with the addition of a regularization term of the form PP
µ=1 aµθ2
µ
to the loss.
If you start this analysis, you’ll immediately ﬁnd that the gradient-descent update d¯θµ
includes an additional term −2η P
ν λµν aνθν, and after some reﬂection you’ll likely also realize the need
to deﬁne a new stochastic tensor,
b
R(ℓ)
i;δ ≡
X
µ,ν
λµν aµθµ
dz(ℓ)
i;δ
dθν ,
(10.42)
which has a stochastic iteration given by
b
R(ℓ+1)
i;δ
= a(ℓ+1)
b
λ(ℓ+1)
b
b(ℓ+1)
i
+ a(ℓ+1)
W
λ(ℓ+1)
W
nℓ
nℓ
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;δ +
nℓ
X
j=1
W (ℓ+1)
ij
σ′ (ℓ)
j;δ b
R(ℓ)
j;δ ,
(10.43)
and whose cross-correlation with the preactivations you’ll want to compute. Here, you will have deﬁned
separate layer-dependent bias and weight regularizations for the coeﬃcients aµ analogous to what we
did for the learning-rate tensor in (8.6), and you may want to work out the interplay between these
regularization hyperpameters and initialization hyperparameters for extra credit.
262

we computed for exact Bayesian inference at inﬁnite width. In the setting of gradient-
based learning, the covariance of the output encodes the variation in the predictions
among networks in the ensemble, each corresponding to diﬀerent parameter settings
that still minimize the training loss LA(θ). In the setting of exact Bayesian inference,
the covariance encodes our intrinsic uncertainty about unseen data and a small uncer-
tainty can serve as a measure of conﬁdence in our prediction. Thus, these covariances
arise for diﬀerent reasons and are epistemologically quite diﬀerent in nature.
However, if we can be somewhat pragmatic for a sentence, when you have multiple
trained models it’s not entirely unreasonable to try to think of this generalized posterior
covariance (10.41) as a measure of conﬁdence as well.
That One Place Where Gradient Descent = Exact Bayesian Inference
Just before, we casually noticed that if we replaced all frozen neural tangent kernels
with kernels, Θ(L) →K(L), then the generalized posterior distribution based on (10.39)
reduces to the exact Bayesian posterior distribution (6.66). Let us now show how we can
actually implement such a substitution in the context of gradient-based learning with a
particular choice of training hyperparameters.
To see how to do this, let’s put side-by-side the recursion that deﬁnes the output-layer
kernel (4.118) and the recursion that deﬁnes the output-layer frozen NTK (9.5):
K(L)
δ1δ2 = C(L)
b
+ C(L)
W ⟨σδ1σδ2⟩K(L−1) ,
(10.44)
Θ(L)
δ1δ2 = λ(L)
b
+ λ(L)
W ⟨σδ1σδ2⟩K(L−1) + C(L)
W

σ′
δ1σ′
δ2

K(L−1) Θ(L−1)
δ1δ2
.
(10.45)
By inspection, it’s immediately clear that setting the ﬁnal-layer learning rates as
λ(L)
b
= C(L)
b
,
λ(L)
W = C(L)
W ,
(10.46)
gives us what we want, almost:
Θ(L)
δ1δ2 = K(L)
δ1δ2 + C(L)
W

σ′
δ1σ′
δ2

K(L−1) Θ(L−1)
δ1δ2
.
(10.47)
To get rid of that pesky last term, we need a way to make the penultimate-layer frozen
NTK Θ(L−1)
δ1δ2 vanish. To ensure this, we can simply set all the other training hyperpa-
rameters to zero:
λ(ℓ)
b
= 0 ,
λ(ℓ)
W = 0 ,
for
ℓ< L .
(10.48)
Combined with the initial condition for the NTK recursion (8.23), this ensures that
Θ(ℓ)
δ1δ2 = 0 for ℓ< L, including ℓ= L−1. Hence, this particular conﬁguration of training
hyperparameters sets
Θ(L)
δ1δ2 = K(L)
δ1δ2 ,
(10.49)
giving us what we wanted, exactly.
In words, this choice of the training hyperparameters (10.46) and (10.48) means that
we are training only the biases and weights in the last layer. This establishes that, in
263

the inﬁnite-width limit, exact Bayesian inference is actually a very special case of an
ensemble of networks trained with gradient-based learning.
In practice, this means that one could train an ensemble of networks with gradient
descent using the training hyperparameters choices (10.46) and (10.48) in order to im-
plement a very good approximation of exact Bayesian inference. (The approximation
would become exact if you had an inﬁnite number of networks in your ensemble.) On the
one hand, unlike the exact version of Bayesian inference presented in §6.3, in this case
we no longer need to explicitly store or invert the kernel. On the other hand, we may
need to fully train a large number of very wide networks for our ensemble to give a good
approximation, which may again be expensive in terms of computation and memory.
Interestingly, by explicitly turning oﬀthe learning in the hidden layers, we are sig-
niﬁcantly changing the features used to compute our predictions. In particular, in this
case the NTK only has contributions from the biases and weights in the ﬁnal layer. In-
tuitively, what’s happening here is that we’re taking random features in the penultimate
layer σ

z(L−1)
i

and then explicitly training the biases b(L) and weights W (L) to ﬁt the
best possible linear model of these random features.22
10.3
Generalization
As remarked before, ultimately we care about how well a model performs on a previ-
ously unseen test set B as compared to the training set A. Some fully-trained networks
will generalize to these new examples better than others, depending strongly on their
initialization and training hyperparameters.
To assess this, we can compute the generalization error:
E ≡LB −LA .
(10.50)
The generalization error is a quantitative measure of how well a network is really approx-
imating the desired function.23 Speciﬁcally, if the training loss is small but the test loss
is large such that there’s signiﬁcant generalization error, then the network isn’t really
a good model of f(x); instead it’s just a lookup table of the values of f(x) when x is
taken from the training set A. This is known as overﬁtting. In contrast, if the training
and test losses are both small such that there’s little generalization error, then we expect
that our model is going beyond simple memorization.24 As such, the generalization error
is often considered to be the main quantitative measure of success of a machine learning
model.
22We’ll explain in §10.4 that the general version of gradient-based learning in the inﬁnite-width limit is
also a linear model of random features, but is constructed from a larger set of such features encompassing
all the hidden layers.
23Without loss of generality, in the following discussion we’ll implicitly assume that the minimal value
of the loss is zero.
24If the generalization error E is small but the training loss LA is large, then the model is said to
be underﬁtting.
This situation is not really relevant for very wide networks since, as we’ve already
explained, we can fully train them to achieve zero training loss.
264

In the inﬁnite-width limit, we saw in the last section that we can easily set the
training loss to zero LA = 0 for any particular network. Thus, in the current context
the generalization error is completely assessed by the test loss,
E = LB ,
(10.51)
and our current goal is to understand the statistics of the test error in order to charac-
terize how inﬁnite-width networks generalize.
For wide networks we know that there are many conﬁgurations of the model pa-
rameters that will minimize the training loss and memorize the training data. Some of
these conﬁgurations might generalize well leading to a small test loss, while some might
overﬁt leading to a large test loss. Thus, the statistics of the generalization error are
determined by the statistics of these conﬁgurations, which are in turn determined by
our initialization and training hyperparameters.
The mean generalization error captures the generalization properties of the ensemble
of networks, and its variance tells us how the instantiation-to-instantiation ﬂuctuations
lead some particular networks to generalize better than others. Understanding these
statistics will inform how we should pick our hyperparameters to achieve the best gen-
eralization performance on average as well as to ensure that the typical behavior of any
fully-trained network is likely to be close to the average.25
With that in mind, let’s ﬁrst evaluate the MSE test loss, averaged over an ensemble
of fully-trained networks:
E [LB(T)] =E

1
2
nL
X
i=1
X
˙β∈B

z(L)
i; ˙β (T) −yi; ˙β
2


=E

1
2
nL
X
i=1
X
˙β∈B

z(L)
i; ˙β (T) −m∞
i; ˙β + m∞
i; ˙β −yi; ˙β
2


=1
2
X
˙β∈B
( nL
X
i=1

m∞
i; ˙β −yi; ˙β
2 +
nL
X
i=1
Cov
h
z(L)
i; ˙β (T), z(L)
i; ˙β (T)
i)
.
(10.52)
In the second line, we added and subtracted the inﬁnite-width mean prediction (10.40),
and to get to the third line we noted that the cross terms cancel under the expectation.
This decomposition (10.52) illustrates a type of generalized bias-variance tradeoﬀ:
the ﬁrst term, the bias, measures the deviation of the mean prediction of the ensem-
ble m∞
i; ˙β from the true output yi; ˙β; the second term, the variance – or speciﬁcally the
25In some applications of machine learning, unseen examples are further divided into two types of
datasets, (i) a validation set, used generally for model selection and often speciﬁcally for tuning hyper-
parameters, and (ii) a test set, used to assess the generalization properties of a particular trained model.
Despite our liberal usage of the term test set, here, as we analytically compute the statistics of the loss
on unseen examples and then use them to tune the hyperparameters, what we have is really closer in
meaning to a validation set, as we are tuning an ensemble of models rather than assessing any particular
one.
265

covariance of the generalized posterior distribution (10.41) – measures the instantiation-
to-instantiation ﬂuctuations of that prediction across diﬀerent models in our ensemble.
The reason why this is called a tradeoﬀis that typically diﬀerent settings of the hy-
perparameters will decrease one term at the cost of increasing the other, making the
modeler have to choose between improving one at the cost of the other.26
For more general losses, we can Taylor expand the test loss around the mean predic-
tion as
LB =LB(m∞) +
X
i, ˙β
∂LB
∂z(L)
i; ˙β

z(L)=m∞

z(L)
i; ˙β (T) −m∞
i; ˙β

(10.53)
+ 1
2
X
i1,i2, ˙β1, ˙β2
∂2LB
∂z(L)
i1; ˙β1∂z(L)
i2; ˙β2

z(L)=m∞

z(L)
i1; ˙β1(T) −m∞
i1; ˙β1
 
z(L)
i2; ˙β2(T) −m∞
i2; ˙β2

+ . . . ,
where we’ve denoted the test loss evaluated at the mean prediction as
LB(m∞) ≡LB

z(L) = m∞
.
(10.54)
Performing the expectation over the ensemble and noting that E
h
z(L)
i; ˙β (T) −m∞
i; ˙β
i
= 0
by deﬁnition (10.40), we get
E [LB] = LB(m∞) + 1
2
X
i1,i2, ˙β1, ˙β2
∂2LB
∂z(L)
i1; ˙β1∂z(L)
i2; ˙β2

z(L)=m∞
Cov
h
z(L)
i1; ˙β1(T), z(L)
i2; ˙β2(T)
i
+ . . . .
(10.55)
Here again we ﬁnd a generalized bias-variance decomposition: the ﬁrst term LB(m∞) is
the bias, measuring the deviation of the mean prediction from the true output on the test
set, and the second term is the variance, measuring the instantiation-to-instantiation
uncertainty as the trace of the covariance multiplied by the Hessian of the loss with
respect to the network outputs. Thus, for any choice of loss function, these bias and
variance terms will give a good proxy for the generalization error E so long as our models
are making predictions that are close to the mean prediction.
Now, let’s see how to compute these bias and variance terms in a few diﬀerent
setups. In most common cases in practice the loss is extensive or additive in samples,
i.e. LB = P
˙β∈B L ˙β, and we can consider the test loss evaluated on one test sample at a
time. Thus, for the purpose of our analysis here, the question is: for a given test input,
how many training examples are relevant for making a prediction?
In §10.3.1, we’ll compute the bias and variance terms of the generalization error
(10.55) around one training sample using our δ expansion introduced in §5, giving an-
other lens into hyperparameter tuning and criticality for our two universality classes.
26The reason why we call it generalized bias-variance tradeoﬀis that, in the standard bias-variance
tradeoﬀ, the expectation is over diﬀerent realizations of the training set A rather than over diﬀerent
initializations of the model parameters θµ. In that typical setting we have only a single model, and the
bias characterizes how well that model can be trained on each diﬀerent training set – with a large bias
indicative of underﬁtting – while the variance characterizes the ﬂuctuations of that model’s performance
over the diﬀerent training sets – with a large variance indicative of overﬁtting.
266

However, this view will be somewhat limited by the restriction of our training set to one
sample.
In §10.3.2, we’ll enlarge our training set to include two samples. Rather than com-
puting the generalization error itself, here we’ll be able to explore directly a diﬀerent
aspect of generalization: how a fully-trained network either interpolates or extrapolates
to make predictions.
10.3.1
Bias-Variance Tradeoﬀand Criticality
Let us index one training sample by ˜α = + and a nearby test sample by ˙β = −; let
us also focus on the output layer ℓ= L and temporarily drop the layer index from the
frozen NTK, Θδ1δ2 ≡Θ(L)
δ1δ2, until later when we need to discuss the depth dependence
of various NTK components.
The bias term in the generalization error is determined by the deviation of the mean
prediction (10.40) from the true output:
m∞
i;−−yi;−= Θ−+
Θ++
yi;+ −yi;−
= (yi;+ −yi;−) +
Θ−+
Θ++
−1

yi;+ .
(10.56)
In this expression, the ﬁrst term is the true diﬀerence in the function outputs on the two
diﬀerent inputs, f(x+) −f(x−), while the second term is a similar (expected) diﬀerence
between our predicted output on x−and the learned true output on x+, z−(T)−z+(T).27
Note the opposite ordering of + and −in these two terms: if our prediction is exactly
correct, these two terms are equal in magnitude and opposite in sign, and the bias term
in the generalization error will vanish.
With that in mind, the quantity in parenthesis in the second factor of the bias
term (10.56),
Θ−+
Θ++
−1 ,
(10.57)
serves as a natural measure of robustness since it characterizes how sensitively our pre-
diction changes – i.e. how |z−(T) −z+(T)| grows – with corresponding small changes in
the input. A model that isn’t robust will often be incorrect, making predictions that vary
greatly from the network output on nearby training points, while too robust a model
will not have much ﬂexibility in its output. Since a priori we don’t know what type of
function we are going to approximate, we naturally would want to pick hyperparameters
that include a class of networks that are robust, but not overly inﬂexible.28
27In general, the bias term LB(m∞) in the generalization error (10.55) depends on the details of the
loss. Of course, we can expand LB(m∞) around the true output yi;−, and the expansion will depend on
the diﬀerence m∞
i;−−yi;−(10.56). For the MSE loss, the bias is precisely the square of this diﬀerence.
For the cross-entropy loss, it is more natural to expand in terms of the diﬀerence q(i|x−) −p(i|x−), with
q(i|xδ) ≡exp
m∞
i;δ

/ Pnout
j=1 exp
m∞
j;δ

.
28A dedicated reader might notice the parallel with §6.3.1 where we argued for criticality by considering
the evidence for two inputs with diﬀering true outputs, f(x+) −f(x−) ̸= 0, which called for similar
ﬂexibility in choice of function approximators.
267

Now, since we’re considering a test input that’s nearby our training input, that should
remind you of our δ expansion from our criticality analysis in §5.1.29 Speciﬁcally, we
can expand the frozen NTK in our γ[a] basis as we did for the kernel in (5.15),
Θ±± = Θ[0] ± Θ[1] + Θ[2] ,
Θ±∓= Θ[0] −Θ[2] ,
(10.58)
and make δ expansions similar to the ones we did for the kernel in (5.22)–(5.24),
Θ[0] =Θ00 + δδΘ[0] + O

δ4
,
(10.59)
Θ[1] =δΘ[1] + O

δ3
,
(10.60)
Θ[2] =δδΘ[2] + O

δ4
,
(10.61)
where the expansion is taken around the midpoint frozen NTK Θ00 evaluated on the
midpoint input xi;0 ≡(xi;+ + xi;−)/2.
For simplicity of our presentation, let’s now assume that the two inputs have the
same norm Pn0
i=1 x2
i;+ = Pn0
i=1 x2
i;−, so that K[1] = 0 and Θ[1] = (Θ++ −Θ−−)/2 = 0.
With this simpliﬁcation, plugging the decomposition (10.58) and then the expansions
(10.59) and (10.61) into the expression for our robustness measure (10.57), we get
Θ−+
Θ++
−1 =
 
Θ[0] −Θ[2]
Θ[0] + Θ[2]
−1
!
= −2δδΘ[2]
Θ00
+ O

δ4
.
(10.62)
Thus, we see that the ratio δδΘ[2]/Θ00 captures the robustness of predictions for nearby
test inputs. We’ll analyze its depth dependence for two universality classes shortly.
Having covered the bias term in the generalization error, let’s next consider the
variance term. The loss-independent piece of the variance is given by the covariance of
the generalized posterior distribution (10.41). Evaluating (10.41) for a single training
sample ˜α = +, using our decompositions for the kernel (5.15) and frozen NTK (10.58),
and then using expansions (5.22), (5.24), (10.59), and (10.61), we ﬁnd
Cov
h
z(L)
i;−(T), z(L)
i;−(T)
i
(10.63)
=K−−−2Θ−+
Θ++
K−+ +
Θ−+
Θ++
2
K++
=K[0] + K[2] −2
 
Θ[0] −Θ[2]
Θ[0] + Θ[2]
! 
K[0] −K[2]

+
 
Θ[0] −Θ[2]
Θ[0] + Θ[2]
!2 
K[0] + K[2]

=4δδK[2] + O

δ4
.
Thus, to leading order the variance term depends only on the perpendicular perturbation
of the kernel δδK[2].
29The following applies to smooth activation functions and is intended to give the general picture. We
will give an analysis particular to nonlinear scale-invariant activation functions later when discussing
them in particular.
268

At this point, we know everything there is to know about how δδK[2] behaves as
a function of depth for our universality classes (cf. §5.3 and §5.5). On the one hand,
we could pick initialization hyperparameters such that δδK[2] grows exponentially with
depth. However, with this choice the variance term will grow very quickly, leading to
large ﬂuctuations in model predictions between diﬀerent realizations. On the other hand,
we could pick initialization hyperparameters that decay exponentially with depth, lead-
ing to a quickly vanishing variance term and very overconﬁdent predictions. However,
we will soon see that this overconﬁdence comes at a cost: an exponentially vanish-
ing perpendicular perturbation δδK[2] implies an exponentially vanishing frozen NTK
component δδΘ[2] and thus a vanishing robustness measure (10.62) signaling extreme
inﬂexibility. In particular, we will have learned a constant function that’s always equal
to y+, regardless of the input.
This is precisely the generalized bias-variance tradeoﬀthat we described above: if we
try to set the variance to zero by having δδK[2] vanish exponentially, then the vanishing
of δδΘ[2] will cause our bias to be larger for generic inputs and consequently the network
will not be able to generalize in a nontrivial manner. Vice versa, making the function
too ﬂexible with large δδΘ[2] will make the model predictions not only too sensitive to
small changes in the input through δδΘ[2], but also will cause large ﬂuctuations in that
prediction from realization to realization through δδK[2].
Of course, we know that there’s a third option: we could pick our criticality condition
χ⊥(K⋆) = 1. This setting of the initialization hyperparameters has the potential to
balance the bias-variance tradeoﬀ, leading to the best outcome without a priori knowing
anything more about the underling dataset we’re trying to model.
What about our other criticality condition χ∥(K⋆) = 1? Recall from our discussion
of the exploding and vanishing gradient problem in §9.4 that the parallel susceptibility
χ∥aﬀects the way in which the midpoint frozen NTK Θ00 receives contributions from
diﬀerent layers. As the midpoint frozen NTK Θ00 enters in the robustness measure as
in (10.62), this suggests that it also plays an important role in generalization. In fact,
we will see soon in §10.4 that ensuring equal contributions from all layers is another way
of saying that we use the greatest set of features available to us in making a prediction.
Thus, it stands to reason that also picking the criticality condition χ∥(K⋆) = 1 in
conjunction with the condition χ⊥(K⋆) = 1 is a natural choice for generalization, in
269

addition to all our other evidence for such a choice.30
Now, returning to the bias part of the generalization error, to complete our analysis
we’ll need to solve a recursion for the δδΘ[2] component of the frozen NTK recursion (9.5),
reprinted here in full:
Θ(ℓ+1)
δ1δ2
= λ(ℓ+1)
b
+ λ(ℓ+1)
W
⟨σδ1σδ2⟩K(ℓ) + CW

σ′
δ1σ′
δ2

K(ℓ) Θ(ℓ)
δ1δ2 .
(10.67)
Let’s ﬁrst work this out for the K⋆= 0 universality class, and then we’ll consider the
scale-invariant universality class for which we’ll need to make use of our ﬁnite-angle
results from §5.5. Either way, this should be child’s play for us at this point.31
K⋆= 0 Universality Class
Recall (5.44) from much much earlier describing the decomposition of the Gaussian
expectation of two activations in the γ[a] basis. With the parallel perturbation turned
30Just like in footnote 23 of §6.3.1, additional justiﬁcation comes from the consideration of two inputs
with unequal norms: Pn0
i=1 x2
i;+ ̸= Pn0
i=1 x2
i;−. In such a case, the robustness measure (10.62) is given by
Θ−+
Θ++ −1 = −δΘ[1]
Θ00 −2δδΘ[2]
Θ00
+

δΘ[1]
Θ00
2
+ O δ3
,
(10.64)
and the covariance is given by
Cov
h
z(L)
i;−(T), z(L)
i;−(T)
i
= 4δδK[2] −2δK[1]
δΘ[1]
Θ00 + K00

δΘ[1]
Θ00
2
+ O δ3
.
(10.65)
First, we see that the kernel components δK[1] and K00 both contribute, necessitating that we set
χ∥= 1 as per our previous discussions.
In addition, we will also need to tame the exploding and
vanishing problem of δΘ[1].
For the scale-invariant universality class, Θ[1] = (Θ++ −Θ−−)/2 has exactly the same depth de-
pendence as the single-input frozen NTK (9.44). In this case, χ∥= χ⊥≡χ, and all the exponential
explosions and vanishments are mitigated by setting χ = 1.
For the K⋆= 0 universality class, we can write a recursion for δΘ[1] by projecting out the γ[1]
component of the full frozen NTK recursion (10.67) using (5.20):
δΘ(ℓ+1)
[1]
= χ(ℓ)
⊥δΘ(ℓ)
[1] +

λ(ℓ+1)
W
CW
χ(ℓ)
∥
+ CW
K(ℓ)
00

zσ′σ′′
K(ℓ)
00 Θ(ℓ)
00

δK(ℓ)
[1] ,
(10.66)
i.e. with a derivation almost isomorphic to the one below for δδΘ[2] (10.70). We in particular see that
δK(ℓ)
[1] contributes to δΘ(ℓ)
[1] – which can be thought of as the Bayesian contribution per our last discussion
in §10.2.4 – and its exploding and vanishing problem is mitigated by setting χ∥(K⋆) = 1: cf. (5.47). (At
this point you may ﬁnd it useful to re-read and re-ﬂect on the last paragraph of footnote 23 in §6.3.1.)
You can further study the depth dependence of δΘ(ℓ)
[1] at criticality and ﬁnd that δΘ(ℓ)
[1] decays faster than
Θ(ℓ)
00 and δδΘ(ℓ)
[2] , thus reducing the problem back to the one studied in the main text.
31An even more childish play would be studying the Bayesian version of generalization error by setting
the training hyperparameters according to (10.46) and (10.48), such that Θ(L)
δ1δ2 = K(L)
δ1δ2. In this case, we
know exactly how the bias and variance terms of the generalization error behave. This is a very particular
setting of the training hyperparameters and unlikely to be optimal in general (cf. our discussion of the
diﬀerences between the frozen NTK and Bayesian kernel in terms of feature functions in §10.4). Indeed
for the scale-invariant universality class, we’ll explicitly see around (10.94) that exact Bayesian inference
has inferior asymptotic behavior than the more general gradient-based learning.
270

oﬀ, K(ℓ)
[1] = 0, this expansion reads
⟨σδ1σδ2⟩K(ℓ) =

⟨σσ⟩K(ℓ)
00 + O

δ2
γ[0]
δ1δ2+

δδK(ℓ)
[2]

σ′σ′
K(ℓ)
00 + O

δ4
γ[2]
δ1δ2 .
(10.68)
With a replacement σ →σ′, we have a similar decomposition for the Gaussian expecta-
tion of the derivatives of activations:

σ′
δ1σ′
δ2

K(ℓ) =

σ′σ′
K(ℓ)
00 + O

δ2
γ[0]
δ1δ2+

δδK(ℓ)
[2]

σ′′σ′′
K(ℓ)
00 + O

δ4
γ[2]
δ1δ2 . (10.69)
Plugging these expansions into the full frozen NTK recursion (10.67) and using the
component-wise identities γ[0]
δ1δ2γ[0]
δ1δ2 = γ[2]
δ1δ2γ[2]
δ1δ2 = γ[0]
δ1δ2 and γ[0]
δ1δ2γ[2]
δ1δ2 = γ[2]
δ1δ2, we get
δδΘ(ℓ+1)
[2]
= χ(ℓ)
⊥δδΘ(ℓ)
[2] +
 
λ(ℓ+1)
W
CW
χ(ℓ)
⊥+ CW

σ′′σ′′
K(ℓ)
00 Θ(ℓ)
00
!
δδK(ℓ)
[2] ,
(10.70)
where we’ve recalled the deﬁnition of the perpendicular susceptibility (5.51), χ(ℓ)
⊥
=
CW ⟨σ′σ′⟩K(ℓ)
00 .32
We learned long ago that the perpendicular susceptibility governs the behavior of
the perpendicular perturbation δδK(ℓ)
[2] , and we see from (10.70) that it also controls the
behavior of the frozen NTK component δδΘ(ℓ)
[2]. As we alluded to before, the exponential
decay/growth of δδK(ℓ)
[2] and δδΘ(ℓ)
[2] are thusly linked.
In particular, trying to elimi-
nate the variance term of the generalization error by letting δδK(ℓ)
[2] exponentially decay
will also cause δδΘ(ℓ)
[2] to exponentially decay, making the model prediction constant,
inﬂexible, and highly biased.
Given this and our previous discussion on the role of the parallel susceptibility χ∥,
let’s now tune to criticality χ∥(K⋆) = χ⊥(K⋆) = 1 and evaluate the depth dependence
of δδΘ(ℓ)
[2]. For the K⋆= 0 universality class, criticality (5.90) is found by tuning Cb = 0
and CW =
1
σ2
1 . With these settings, we recall the large-ℓasymptotic solutions from (5.92)
and (5.99)
K(ℓ)
00 =

1
(−a1)
 1
ℓ+ . . . ,
δδK(ℓ)
[2] = δ2
ℓp⊥+ . . . ,
(10.71)
where p⊥≡b1/a1, and the activation-function dependent constants a1 and b1 were
deﬁned in (5.86) and (5.88), and δ2 is a constant related to the initial separation of
the inputs but isn’t ﬁxed by the asymptotic analysis. Also recall from the more recent
past (9.76) that we can asymptotically expand the perpendicular susceptibility as
χ(ℓ)
⊥= 1 −p⊥
ℓ+ . . . .
(10.72)
32More generally there are terms proportional to  δK[1]
2 and δK[1]δΘ[1] in this recursion for δδΘ[2];
however, when training and test inputs have equal norm, δK[1] = 0, these terms vanish.
271

Similarly, by a simple Gaussian integral we can evaluate the following Gaussian expec-
tation,

σ′′σ′′
K(ℓ)
00 = σ2
2 + O

K(ℓ)
00

= σ2
2 + O
1
ℓ

,
(10.73)
remembering our notation σ2 ≡σ′′(0).
Next, we also have to make a choice about the training hyperparameters λ(ℓ)
b
and
λ(ℓ)
W . Indeed, the depth scaling of the generalization error will depend on these hyper-
parameters, a fact that should not be surprising: we expect the selection of our relative
learning rates to aﬀect the performance of our model. Let’s ﬁrst follow the guidance
of §9.4 where we discussed an equivalence principle for learning rates, and set these
training hyperparameters according to (9.95), i.e. (9.70) multiplied by Lp⊥−1:
λ(ℓ)
b
= eλb
1
ℓ
p⊥
Lp⊥−1 ,
λ(ℓ)
W = eλW
L
ℓ
p⊥−1
.
(10.74)
With such a choice, we have an asymptotic solution for the midpoint frozen NTK, which
is the same solution as in (9.71) up to a multiplication by Lp⊥−1:
Θ(ℓ)
00 =
"
eλb +
eλW σ2
1
(−a1)
# L
ℓ
p⊥−1
+ . . . .
(10.75)
Further plugging these results (10.71)–(10.75) into (10.70), we get
δδΘ(ℓ+1)
[2]
=

1 −p⊥
ℓ+ . . .

δδΘ(ℓ)
[2]
(10.76)
+ δ2
(
eλW σ2
1 + σ2
2
σ2
1
"
eλb +
eλW σ2
1
(−a1)
#)
Lp⊥−1
1
ℓ
2p⊥−1
+ . . . .
With our usual methods, we can solve this recursion in the asymptotically large-ℓlimit
with
δδΘ(ℓ)
[2] = δ2 Lp⊥−1
(2 −p⊥)
(
eλW σ2
1 + σ2
2
σ2
1
"
eλb +
eλW σ2
1
(−a1)
#) 1
ℓ
2p⊥−2
+ . . . .
(10.77)
Finally, taking the ratio of the midpoint frozen NTK (10.75) and the perpendicular
perturbation (10.77) and evaluating at the output layer ℓ= L, we ﬁnd the overall
network depth dependence for our robustness measure:
−2δδΘ(L)
[2]
Θ(L)
00
=
2δ2
(p⊥−2)



eλW σ2
1
h
eλb + (eλW σ2
1)/(−a1)
i + σ2
2
σ2
1


L1−p⊥∝L1−p⊥.
(10.78)
This is astonishing: the desire to keep the robustness measure of order one for very deep
networks exactly picks out activation functions in this universality class with p⊥= 1!33
33Since p⊥= b1/a1, cf. (5.99), b1 ≥a1, cf. (5.86) and (5.88), and a1 < 0, cf. (5.92), these overall imply
that p⊥≤1 for any K⋆= 0 activation function. In particular, for a non-odd activation function with
σ2 ̸= 0, the exponent for perpendicular perturbations is strictly less than one, p⊥< 1, and thus the bias
term in the generalization error (10.56) will grow with network depth L.
272

Such a condition is satisﬁed by any odd K⋆= 0 activation function, such as tanh and
sin, both of which we’ve been discussing prominently throughout the book.
Now, let’s zoom out for a moment and reﬂect on these calculations more broadly.
Somewhat miraculously, the theoretically-motivated tuning of all our hyperparameters –
criticality for the initialization hyperparameters and the learning rate equivalence prin-
ciple for the training hyperparameters – has led to the most practically-optimal solution
for the generalization error in this one-training-one-test setting, keeping the bias-variance
tradeoﬀin check. Even more importantly, these choices and solutions are robust across
many diﬀerent network widths and depths, making them quite useful for experimentation
and the scaling up of models.34 Of course, there really was no miracle: our theoretical
principles were practically motivated from the start.
Now that we understand what we should do, let’s discuss a diﬀerent choice of train-
ing hyperparameters that we should not make. Had we not followed the learning rate
equivalence principle, perhaps we would have just made both weight and bias learning
rates layer independent as λ(ℓ)
b
= λb and λ(ℓ)
W = λW . Let’s see what happens then, spe-
cializing to odd activation functions with σ2 = 0 and p⊥= 1 for simplicity. In this case,
our general formal solution for the single-input frozen NTK (9.69) reduces to
Θ(ℓ)
00 =
λb
2

ℓ+ . . . ,
(10.79)
with a linear dependence on the layer ℓ, while the same calculation as above with σ2 = 0
and p⊥= 1 in mind gives a layer-independent constant asymptotic solution for δδΘ(ℓ)
[2]:
δδΘ(ℓ)
[2] = λW σ2
1δ2 + . . . .
(10.80)
Combined, our robustness measure becomes
−2δδΘ(L)
[2]
Θ(L)
00
=
"
−4λW σ2
1δ2
λb
#
1
L + . . . ,
(10.81)
which is slowly but surely decaying with the overall depth L of the network. (The consid-
eration of more general activation functions with p⊥̸= 1 doesn’t change this conclusion.)
Therefore, this choice of the training hyperparameters is polynomially suboptimal com-
pared to the choice based on our equivalence principle.35
Reﬂecting back, when we ﬁrst discussed the learning rate equivalence principle by
staring at our formal solution (9.69), we were motivated by the desire to ensure equal
contributions to the NTK from each layer. Then in §9.4 we realized that such choices
solve a polynomial version of the exploding and vanishing gradient problem. Here we see
34These tunings are more or less still valid even as we relax the inﬁnite-width requirement to allow
nonzero aspect ratio, L/n ≪1.
35We leave it to the reader to see how disastrous things would be – in terms of our one-training-one-
test generalization error – if we had decided not to rescale the weight learning rate by the widths of the
previous layer as in (8.6), leading to an even more extreme violation of the learning rate equivalence
principle.
273

the downstream consequences of those choices through the lens of generalization error,
giving a solid support for the equivalence principle according to our quantitative measure
of training success.
Scale-Invariant Universality Class
To analyze scale-invariant activation functions, we need to use results from our ﬁnite-
angle analysis in §5.5. In particular, the Gaussian expectation ⟨σ′′σ′′⟩that appeared in
the δ expansion of ⟨σ′σ′⟩in (10.69) is singular for nonlinear scale-invariant functions due
to the kink at the origin, and we promised we’d have to recall our ﬁnite-angle results
when such a singularity occurs.
Keeping our promise to you, let’s recall a bunch of things from that section. First,
we decomposed the two-input kernel matrix as (5.146)
K(ℓ)
δ1δ2 =
 
K(ℓ)
++
K(ℓ)
+−
K(ℓ)
−+
K(ℓ)
−−
!
= K(ℓ)
d


1
cos

ψ(ℓ)
cos

ψ(ℓ)
1

,
ψ(ℓ) ∈[0, π] ,
(10.82)
with two dynamical variables being the diagonal kernel K(ℓ)
d
and the polar angle ψ(ℓ).
With this parametrization in mind, let us reprint a bunch of the previous results that
we’ll need, (5.60), (5.62), (5.159), and (5.161):
⟨σ+σ+⟩K(ℓ) = ⟨σ−σ−⟩K(ℓ) = A2K(ℓ)
d
,
(10.83)
CW

σ′
+σ′
+

K(ℓ) =CW

σ′
−σ′
−

K(ℓ) = CW A2 ≡χ ,
(10.84)
⟨σ+σ−⟩K(ℓ) =A2K(ℓ)
d
n
cos

ψ(ℓ)
+ ρ
h
sin

ψ(ℓ)
−ψ(ℓ) cos

ψ(ℓ)io
,
(10.85)
CW

σ′
+σ′
−

K(ℓ) =χ(1 −ρψ(ℓ)) ,
(10.86)
where A2 ≡(a2
+ + a2
−)/2, ρ ≡1
π
(a+−a−)2
(a2
++a2
−) , and a+ and a−are the two constants that
deﬁne the particular activation function (though by now you know that by heart).
Let us now make a similar decomposition for the frozen NTK as
Θ(ℓ)
δ1δ2 =
 
Θ(ℓ)
++
Θ(ℓ)
+−
Θ(ℓ)
−+
Θ(ℓ)
−−
!
= Θ(ℓ)
d


1
cos

ζ(ℓ)
cos

ζ(ℓ)
1

,
ζ(ℓ) ∈[0, π] ,
(10.87)
with a diagonal frozen NTK Θ(ℓ)
d
and another polar angle ζ(ℓ).
Then, plugging this decomposition (10.87) and recollected results (10.83)–(10.86)
into the frozen NTK recursion (10.67), we get coupled recursions for the frozen NTK,
casted in our ﬁnite-angle parameterization:
Θ(ℓ+1)
d
=χΘ(ℓ)
d + λ(ℓ+1)
b
+ λ(ℓ+1)
W
A2K(ℓ)
d
,
(10.88)
Θ(ℓ+1)
d
cos

ζ(ℓ+1)
=χ(1 −ρψ(ℓ))Θ(ℓ)
d cos

ζ(ℓ)
(10.89)
+ λ(ℓ+1)
b
+λ(ℓ+1)
W
A2K(ℓ)
d
n
cos

ψ(ℓ)
+ ρ
h
sin

ψ(ℓ)
−ψ(ℓ) cos

ψ(ℓ)io
.
274

We see here in the oﬀ-diagonal recursion (10.89) a ﬁnite-angle analog of what we saw per-
turbatively for the K⋆= 0 universality class in the inﬁnitesimal-angle recursion (10.70):
the polar angle for the kernel ψ(ℓ) sources the ﬁnite angle for the frozen NTK ζ(ℓ). Said
another way, the exponential growth and decay of the kernel angle ψ(ℓ) – at least for
small enough angle – are linked to the exponential growth and decay of the frozen-NTK
angle ζ(ℓ), which are in turn linked to the generalized bias-variance tradeoﬀ.
With that chain of links in mind (as well as parallel discussions of similar issues in
almost every other chapter of this book), it’s natural that we should set our initialization
hyperparameters by tuning to criticality: χ = 1. With this choice, we recall the critical
solutions from our ﬁnite-angle analysis of the kernel in §5.5:
K(ℓ)
d
= K⋆
d ,
ψ(ℓ) =
3
ρ
 1
ℓ+ . . . ,
(10.90)
where K⋆
d is exactly constant, set by the ﬁrst layer. Additionally, having already made
the case for the learning rate equivalence principle when discussing the K⋆= 0 univer-
sality class, let’s just simplify our discussion here by setting training hyperparameters
according to that equivalence principle for scale-invariant activations (9.94): λ(ℓ)
b
= eλb/L
and λ(ℓ)
W = eλW /L. With this choice, we see that
Θ(ℓ)
d
=

eλb + eλW A2K⋆
d
 ℓ
L
(10.91)
solves the recursion for the diagonal frozen NTK (10.88) with the initial condition (9.7).
Importantly, here ℓrefers to a particular layer of the network, while L is the overall
network depth.36
Plugging our choice of learning rates, our kernel solution (10.90), and NTK solution
(10.91) into the ﬁnite-angle recursion (10.89), we get after a bit of rearranging
cos

ζ(ℓ+1)
=

1 −4
ℓ+ . . .

cos

ζ(ℓ)
+
1
ℓ+ . . .

,
(10.92)
which we can see easily is solved by an everything-independent constant
cos

ζ(ℓ)
= 1
4 + . . . .
(10.93)
Thus, our robustness measure (10.57) in the bias term of generalization error for non-
linear scale-invariant activation functions is given by a simple order-one number:
Θ(L)
−+
Θ(L)
++
−1 = cos

ζ(L)
−1 = −3
4 + . . . .
(10.94)
Similarly, given that the nearby-input analysis can break down for nonlinear scale-
invariant activations, let’s use our ﬁnite-angle analysis here to also work out the variance
36The frozen NTK solution (10.91) is identical to our previous single-input solution (9.44), here we
have just rescaled the bias and weight learning rates by the overall depth, λb = eλb/L and λW = eλW /L,
as required by the equivalence principle (9.94).
275

term of the generalization error (10.63); plugging in the asymptotic falloﬀfor the kernel
(5.167) and (5.168) as well as using (10.94) for the frozen NTK, we get
Cov
h
z(L)
i;−(T), z(L)
i;−(T)
i
= K−−−2Θ−+
Θ++
K−+ +
Θ−+
Θ++
2
K++
(10.95)
= K⋆
d
h
1 −cos

ζ(L)i2 = 9
16K⋆
d + . . . .
Unlike the previous case for K⋆= 0 activations, these asymptotic results for the
generalization error, (10.94) and (10.95), don’t depend on the training hyperparameters
eλb and eλW , nor do they depend on a constant like δ2 that knows about the separation
of the test and training points.
(However, just as we discussed for ψ(ℓ) in §5.5, the
depth at which these asymptotic results become valid does depend on δ2, the input
norm, the activation function, and the training hyperparameters.) Nonetheless, again
with the correct tuning of our hyperparameters based on the principles of criticality and
equivalence, we found a constant bias and variance, giving us the best possible tradeoﬀ
when training deep networks with nonlinear scale-invariant activations.37
Let us end with the special remark on deep linear networks. For these networks, we
use the linear activation function with a+ = a−and hence have ρ = 0. In particular, we
saw in §5.5 that not only was the diagonal kernel preserved at criticality, but the polar
angle was preserved as well: K(ℓ)
d
= K⋆
d and ψ(ℓ) = ψ⋆. Noting this, the oﬀ-diagonal
recursion for the frozen NTK (10.89) then becomes
Θ(ℓ+1)
d
cos

ζ(ℓ+1)
= Θ(ℓ)
d cos

ζ(ℓ)
+
eλb
L +
eλW
L A2K⋆
d cos(ψ⋆) .
(10.96)
This recursion is exactly solved by
Θ(ℓ)
d cos

ζ(ℓ)
=
h
eλb + eλW A2K⋆
d cos(ψ⋆)
i (ℓ−1)
L
+ Θ(1)
d
cos

ζ(1)
.
(10.97)
Dividing this result by our solution for the diagonal frozen NTK (10.91) then gives
cos

ζ(ℓ)
=
eλb + eλW A2K⋆
d cos(ψ⋆)
eλb + eλW A2K⋆
d
+ . . . ,
(10.98)
which polynomially asymptotes to a constant. Unlike the case for the nonlinear scale-
invariant activation functions, this constant depends on the observables in the ﬁrst layer
in a rather detailed manner, naturally connecting the generalization properties of the
network to the input. The real limitation of the deep linear networks becomes immedi-
ately apparent upon considering their (in)ability to interpolate/extrapolate, which we’ll
analyze next for linearly and nonlinearly activated MLPs.
37It is worth noting what happens with the special case of exact Bayesian inference where the only
nonzero learning rates are in the last layer in order to set Θ(L) = K(L). In that case, the robustness
measure is given by cos ψ(L)
−1 = O 1/ℓ2
. Given this decay with depth, we see that the restricted
Bayesian case is clearly inferior to an ensemble of networks that are fully-trained via gradient descent
with uniform learning rates across layers.
276

10.3.2
Interpolation and Extrapolation
Rather than focusing primarily on our evaluation criteria for successful training, the
generalization error, in this subsection we will focus more on the kinds of functions
that our trained neural networks actually compute.
This analysis will enable us to
consider the inductive bias of diﬀerent activation functions and tell us how to relate the
properties of those activation functions to the properties of the dataset and function
that we’re trying to approximate.
In the previous subsection we asked: given the true output yi;+ for an input xi;+,
what does a fully-trained MLP in the inﬁnite-width limit predict for the output of a
nearby input xi;−? Here, we up the ante and ask: given the true outputs yi;± for two
inputs xi;± = xi;0 ± δxi
2 , what is the prediction for a one-parameter family of test inputs,
sxi;+ + (1 −s)xi;−= xi;0 + (2s −1)
2
δxi ≡xi;(2s−1) ,
(10.99)
that sit on a line passing through xi;+ and xi;−? When our parameter s is inside the unit
interval s ∈[0, 1], this is a question about neural-network interpolation; for s outside
the unit interval, it’s a question about extrapolation. For general s, let’s refer to this
collectively as ∗-polation.
First we’ll perform a little exercise in ∗-polation with deep linear networks to see
what networks with linear activation functions do. Accordingly, we’ll see concretely
how deep linear networks approximate a very limited set of functions. Then we’ll follow
up by assessing smooth nonlinear networks.
Linear ∗-Polation by Deep Linear Networks
There’s a very simple way to see how ∗-polation works for deep linear networks.
If
we recall for a moment (and for one last time) the forward equation for deep linear
networks (3.1),
z(ℓ+1)
i;α
= b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
z(ℓ)
j;α ,
(10.100)
with z(0)
j;α ≡xj;α, it’s clear that the linear structure in the input (10.99) will be preserved
from layer to layer. That is, given an ℓ-th-layer preactivations of the form
z(ℓ)
i;(2s−1) = sz(ℓ)
i;+ + (1 −s)z(ℓ)
i;−
(10.101)
that has such a linear structure, we then have for the next layer
z(ℓ+1)
i;(2s−1) = b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
h
sz(ℓ)
j;+ + (1 −s)z(ℓ)
j;−
i
(10.102)
= s

b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
z(ℓ)
j;+

+ (1 −s)

b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
z(ℓ)
j;−


= sz(ℓ+1)
i;+
+ (1 −s)z(ℓ+1)
i;−
,
277

which still respects the linear structure. This is just a direct consequence of the fact
that deep linear networks compute linear functions of their input.
Therefore, for a test input that’s a linear sum of our two training points (10.99), the
network will output the linear sum of the network outputs on the two individual training
points:
z(L)
i;(2s−1) = sz(L)
i;+ + (1 −s)z(L)
i;−.
(10.103)
This equation holds at initialization as well as at the end of training, which means that
any particular fully-trained deep linear network will ∗-polate as
z(L)
i;(2s−1)(T) = syi;+ + (1 −s)yi;−,
(10.104)
since the fully-trained network output will equal the true output for any element in the
training set: z(L)
i;±(T) = yi;±. With this, we see that fully-trained deep linear network
linearly ∗-polate, no matter what. This is both intuitive and pretty obvious; as deep
linear networks perform linear transformations they can only compute linear functions.
Of course, this is exactly what we said when we studied deep linear networks way
back in §3. Here, we explicitly see why these networks are limited after training, by
showing the (limited) way in which they can use training examples to make predictions.
Accordingly, if the function you’re trying to approximate is a linear function of the input
data, then deep linear networks are a great modeling choice. If the function is nonlinear,
we’ll have to consider nonlinear activation functions. It’s not that deep.
Nonlinear ∗-Polation by Smooth Nonlinear Deep Networks
We’ll have to work a little harder to see what nonlinear networks do.38
In the last
section, we saw that the output of a fully-trained network is given by the stochastic
kernel prediction equation (10.39), which we reprint here for convenience:
z(L)
i; ˙β (T) = z(L)
i; ˙β −
X
˜α1,˜α2∈A
Θ(L)
˙β ˜α1
eΘ˜α1 ˜α2
(L)

z(L)
i;˜α2 −yi;˜α2

.
(10.105)
Thus, we see that to study ∗-polation more generally we will need to evaluate elements
of the frozen NTK between our test and training set, Θ(L)
(2s−1)±, and also need to invert
the two-by-two submatrix of the frozen NTK on the training set only, eΘ˜α1 ˜α2
(L) .
This latter inversion can be easily completed with the standard textbook formula for
the inverse of a two-by-two matrix,
eΘ˜α1 ˜α2 =
1
Θ++Θ−−−Θ2
+−
 
Θ−−
−Θ+−
−Θ+−
Θ++
!
,
(10.106)
38We would have to work even harder to see what nonlinear scale-invariant activation functions do, so
here we’ll focus on smooth nonlinear activation functions. For such nonlinear scale-invariant activation
functions with kinks, since the ∗-polated input xi;(2s−1) does not have the same norm as xi;± for s ̸= 0, 1,
we would need to extend the ﬁnite-angle analysis from §5.5 to the case of unequal input norms. This is
left as a challenge in pedagogy to future deep-learning book authors.
278

where here we’ve also used the symmetry Θ+−= Θ−+ and further dropped the layer
indices. For the rest of this section, we will always assume that these frozen NTKs are
evaluated at the output layer.
Next, to compute the oﬀ-diagonal elements between the training set and the test set
Θ(2s−1)±, we’ll need to generalize our δ expansion a bit. Let’s ﬁrst recall our expressions
for the components of the frozen NTK in γ[a] basis, (10.58), and then plug in the δ
expansion we performed on the two-by-two submatrix eΘ˜α1 ˜α2, (10.59)–(10.61), which
gives
Θ±± =Θ00 ± δΘ[1] +

δδΘ[2] + δδΘ[0]

+ O

δ3
,
(10.107)
Θ±∓=Θ00 +

−δδΘ[2] + δδΘ[0]

+ O

δ3
,
(10.108)
for the pair of inputs xi;± = xi;0 ± δxi
2 . Let’s now consider a pair of perturbed inputs of
a more general form
xi;ϵ1 ≡xi;0 + ϵ1
2 δxi ,
xi;ϵ2 ≡xi;0 + ϵ2
2 δxi .
(10.109)
Note that picking ϵ1,2 from ±1 reduces them to xi;±, while the new case of the interest,
Θ(2s−1)±, corresponds to setting ϵ1 = (2s −1) and ϵ2 = ±1. For a generic pair of inputs
(10.109), the δ expansion gets modiﬁed as
Θϵ1ϵ2 =Θ00 +
ϵ1 + ϵ2
2

δΘ[1] +
ϵ1 + ϵ2
2
2 
δδΘ[2] + δδΘ[0]

(10.110)
+
ϵ1 −ϵ2
2
2 
−δδΘ[2] + δδΘ[0]

+ O

ϵ3δ3
.
To see why this is the correct expression, note that (i) each term has the right scaling with
ϵ1,2, (ii) for ϵ1 = ϵ2 = ±1 we correctly recover the expression for Θϵ1ϵ2 = Θ±± (10.107),
(iii) for ϵ1 = −ϵ2 = ±1, we correctly recover the expression for Θϵ1ϵ2 = Θ±∓(10.108),
and (iv) the expression is symmetric under ϵ1 ↔ϵ2. The frozen NTK component Θϵ1ϵ2
must satisfy these four constraints, and the expression (10.110) is the unique formula
that satisﬁes them all.
Applying this formula to evaluate Θ(2s−1)± and simplifying a bit, we ﬁnd
Θ(2s−1)± = sΘ±+ + (1 −s)Θ±−−2s(1 −s)δδΘ[0] + O

δ3
.
(10.111)
As we’ll see, the key to nonlinear ∗-polation, at least for nearby inputs, is in the δδΘ[0]
term.39
Firstly, it’s clear that this term nonlinearly depends on the test input, as
evidenced by s(1 −s) prefactor.
Indeed, you can go back and check that this term
identically vanishes for deep linear networks, i.e., for those networks we simply have
39N.B. δδΘ[0] is very diﬀerent from δδΘ[2]: the former is the second term in the expansion of the γ[0]
component Θ[0], cf. (10.59), while the latter is the ﬁrst term in the expansion of the γ[2] component Θ[2],
cf. (10.61).
279

Θ(2s−1)± = sΘ±+ + (1 −s)Θ±−.40 With that in mind, it also helps to decompose the
initial preactivation into linear and nonlinear pieces as
z(L)
i;(2s−1) = sz(L)
i;+ + (1 −s)z(L)
i;−+
h
z(L)
i;(2s−1) −sz(L)
i;+ −(1 −s)z(L)
i;−
i
.
(10.112)
Here, the second term vanishes for deep linear networks, as per (10.103), and so in
general it captures nonlinearity of the network output at initialization.
Plugging (10.106), (10.111), and (10.112) into our kernel prediction formula (10.105),
we see that our fully-trained prediction on the test input xi;(2s−1) = sxi;+ + (1 −s)xi;−
is given by
z(L)
i;(2s−1)(T)
(10.113)
=
h
z(L)
i;(2s−1) −sz(L)
i;+ −(1 −s)z(L)
i;−
i
+ [syi;+ + (1 −s)yi;−]
−s(1 −s)
"
2δδΘ[0]
Θ00δδΘ[2] −δΘ2
[1]
# h
2δδΘ[2]

z(L)
i;+ + z(L)
i;−+ yi;+ + yi;−

−δΘ[1]

z(L)
i;+ −z(L)
i;−+ yi;+ −yi;−
 i
+ O

δ3
.
Comparing with our linear ∗-polation formula (10.104), we see that both the ﬁrst and
last terms are new: nonlinear networks can nonlinearly ∗-polate! Interestingly, the fully-
trained ∗-polation for nonlinear activation functions depends on the network output at
initialization through the nonlinearity z(L)
i;(2s−1) −sz(L)
i;+ −(1 −s)z(L)
i;−; in contrast, for
deep linear networks the ∗-polation only depended on the true output of the training
examples.
As a particular illustration of this formula, consider the case when the two training
inputs have the same norm. In such a case Θ[1] = 0, and we ﬁnd a much simpler formula:
z(L)
i;(2s−1)(T) =
h
z(L)
i;(2s−1) −sz(L)
i;+ −(1 −s)z(L)
i;−
i
+ [syi;+ + (1 −s)yi;−]
(10.114)
−4s(1 −s)
δδΘ[0]
Θ00
 
z(L)
i;+ + z(L)
i;−+ yi;+ + yi;−

+ O

δ3
.
Averaging over our ensemble, this prediction has a mean
m∞
i;(2s−1) = syi;+ + (1 −s)yi;−−4s(1 −s)
δδΘ[0]
Θ00

(yi;+ + yi;−) + O

δ3
.
(10.115)
Here the ﬁrst term in (10.114) that captured the nonlinearity of the network output at
initialization vanished under the expectation, and so the nonlinearity of the ∗-polation
mean is entirely captured by the dimensionless ratio δδΘ[0]/Θ00.
40To see this quickly, note that both the ﬁrst-layer metric (4.8) and the ﬁrst-layer NTK (8.23) are
bilinear in the two inputs, and that such bilinear structure is preserved under the recursions for deep linear
networks: K(ℓ+1)
δ1δ2
= C(ℓ+1)
b
+ C(ℓ+1)
W
K(ℓ)
δ1δ2, cf. (4.118), and Θ(ℓ+1)
δ1δ2 = λ(ℓ+1)
b
+ λ(ℓ+1)
W
K(ℓ)
δ1δ2 + C(ℓ+1)
W
Θ(ℓ)
δ1δ2,
cf. (9.5).
280

So, what kind of a function is our fully-trained inﬁnite-width nonlinear neural net-
work computing? To assess this, note that the ratio δδΘ[0]/Θ00 captures the curvature
of the ∗-polation in the neighborhood of the training points.41 This curvature encodes a
non-universal inductive bias of the activation function and architecture indicating how
this class of function approximators will generalize to novel data.
For a given task and dataset, some activation functions might produce a more desired
type of ∗-polation. This can be measured directly via the bias term in the generalization
error. Substituting in our equal norm expression for the mean (10.115),
m∞
i;(2s−1) −yi;(2s−1) =
h
yi;+ + (1 −s)yi;−−yi;(2s−1)
i
−4s(1 −s)
δδΘ[0]
Θ00

(yi;+ + yi;−) + O

δ3
,
(10.117)
we see that this generalization error bias decomposes into a comparison between the
nonlinearity in the true output – given by the ﬁrst square brackets – and the network
curvature around the midpoint of the true output (yi;+ + yi;−) /2. With this framing,
deep linear networks promote a very particular type of inductive bias: only linear func-
tions are computed. More generally, we could (but won’t here) compute and solve a
recursion for δδΘ[0] for any particular activation function in order to learn more about
the kinds of functions computed by deep networks with that activation function.
Finally, note that this analysis doesn’t make any particular distinction between in-
terpolation and extrapolation, and also that as s →0, 1, the ∗-polation (10.113) reduces
to yi;± with absolute certainty. In fact, in the neighborhood of s = 0, 1, the ∗-polation
bias (10.117) has much in common with the prediction bias (10.56) and (10.62) that we
saw in §10.3.1 when considering a training set consisting of only one training sample.
Importantly, it is the most nearby training point that contributes the most to a test
point’s prediction.
Taken as a guide to thinking about larger training sets, the local nature of these
predictions is highly suggestive of some ways to make further progress. On the one hand,
we might be able to make theoretical progress on more complicated prediction formulae
by weighting the predictions given by nearby training points to a given test point, perhaps
using an approximation from §10.3.1 when there’s only one nearby training point and
using ∗-polation (10.113) when there’s a nearby pair. On the other hand, we might be
able to make practical progress on training-set design – given the network’s inductive
biases – by using this kind of analysis to inform how best to sample training inputs over
the data manifold.
41Note that as the two training samples begin to coincide x± →x0, the curvature vanishes quadrati-
cally δδΘ[0]/Θ00 = O δ2
, and the closer the ∗-polation will be to a linear ∗-polation. Further applying
our generalized δ expansion (10.110) to the kernel, we can show that that the variance of the ∗-polation
vanishes even more quickly in this coincident limit as
E
h
z(L)
i;2s−1(T)z(L)
i;2s−1(T)
i
−

E
h
z(L)
i;2s−1(T)
i2
= O δ3
.
(10.116)
281

10.4
Linear Models and Kernel Methods
Before we back oﬀthe inﬁnite-width limit, let’s take a section to place what we’ve done
in this chapter into the broader context of machine learning. In the next chapter, such
a context will help us understand the ways in which deep learning at ﬁnite width is
qualitatively quite diﬀerent from its inﬁnite-width counterpart.
In particular, in this section we’ll explain a dual way of thinking about the class of
models that can be described by a kernel prediction formula such as (10.39). On the one
hand, kernel predictions can be thought of as being made by ∗-polating the training data
using the kernel. On the other hand, we can think of them as the output of a trained
model that’s linear in its parameters. The former perspective has been more natural
to us, given that we always consider an ensemble over the model parameters and then
integrate them out. So let’s begin by explaining the latter linear model perspective.42
10.4.1
Linear Models
The simplest linear model – and perhaps the simplest machine learning model – is just
a one-layer (i.e. zero-hidden-layer) network
zi(xδ; θ) = bi +
n0
X
j=1
Wijxj;δ .
(10.118)
While this model is linear in both the parameters θ = {bi, Wij} and the input xj;δ, the
linear in linear model takes its name from the dependence on the parameters θ and not
the input x. In particular, while the components of the input samples xj;δ sometime
can serve as a reasonable set of features for function approximation, in general they do
not. Indeed, considering how much ink we’ve already spilled on representation group
ﬂow and representation learning in the context of deep learning, it’s natural to expect
that we would need to (pre-)process the input data before it’s useful for any machine
learning task.
One traditional way to ﬁx this, inherited from statistics, is to engineer better features.
Such an approach was necessary when computers were less powerful and models had to
be much simpler to optimize. For instance, in addition to the features xj perhaps it
would also be useful for the model to take into account features xjxk that let us consider
the dependence of one component upon another. More generally, we might design a
ﬁxed set of feature functions φj(x) that’s meant to work well for the dataset D and
the underlying task at hand.43
In this traditional approach, the hope is that all the complicated modeling work goes
into the construction of these feature functions φj(x) and, if we do a good enough job,
42The connection between inﬁnite-width networks trained by gradient descent and kernel methods was
pointed out in [57] in the context of introducing the NTK. Following that, an extended discussion of
such networks as linear models was given in [62].
43These type of feature functions are also useful if the input x is something abstract – such as a
document of text – and thus needs to be transformed into a numerical vector before it can be processed
by a parameterized model.
282

then its associated linear model,
zi(xδ; θ) = bi +
nf
X
j=1
Wijφj(xδ) =
nf
X
j=0
Wijφj(xδ) ,
(10.119)
is simple to train, easy to interpret, and performs well on the desired task. Here, we’ve
followed a customary notational reductionism, subsuming the bias vector into the weight
matrix by setting φ0(x) ≡1 and Wi0 ≡bi. Thus, the output zi(x; θ) of a linear model
depends linearly on the model parameters θ, consisting of a combined weight matrix
Wij of dimension nout × (nf + 1). We can still think of this model as a one-layer neural
network, but in this case we pre-process each input with the function φj(x) before passing
it through the network.
Now let’s explain how to learn the optimal values for weight matrix W ⋆
ij given a
training set A. The most common approach is to minimize the MSE loss
LA(θ) = 1
2
X
˜α∈A
nout
X
i=1
[yi;˜α −zi(x˜α; θ)]2 = 1
2
X
˜α∈A
nout
X
i=1

yi;˜α −
nf
X
j=0
Wijφj(x˜α)


2
.
(10.120)
Supervised learning with a linear model is known as linear regression, and – as the
MSE loss of a linear model is necessarily quadratic in the model parameters – this is
another case of an analytically-solvable learning problem (7.7). Taking the derivative of
LA with respect to the parameters and setting it to zero, we get an implicit equation
that determines the optimal weight matrix W ⋆
ij:
nf
X
k=0
W ⋆
ik
" X
˜α∈A
φk(x˜α)φj(x˜α)
#
=
X
˜α∈A
yi;˜αφj(x˜α) .
(10.121)
To solve this equation, let’s deﬁne a symmetric (nf + 1)-by-(nf + 1) matrix of features,
Mij ≡
X
˜α∈A
φi(x˜α)φj(x˜α) ,
(10.122)
with elements that give a pairwise aggregation of feature functions summed over all
the training samples ˜α ∈A. Then, applying its inverse to both sides of the implicit
expression (10.121), we ﬁnd a solution:
W ⋆
ij =
nf
X
k=0
X
˜α∈A
yi;˜αφk(x˜α)

M−1
kj .
(10.123)
Notice that the solution depends on the training set, linearly for the true function values
yi;˜α and in a more complicated way on the input features φk(x˜α).44
Finally, we can
44If the number of features (nf + 1) is larger than the size of the training set NA, then the model is
overparameterized, and Mij is not uniquely invertible. One scheme to specify the solution is to add a
283

use this fully-trained linear model with its associated optimal parameters W ⋆
ij to make
predictions on novel test-set inputs x ˙β as
zi
 x ˙β; θ⋆ =
nf
X
j=0
W ⋆
ijφj(x ˙β) ,
(10.125)
giving us a closed-form solution for our linear regression problem. Importantly, after
learning is complete we can simply store the optimal parameters W ⋆
ij and forget about
the training data.
10.4.2
Kernel Methods
While this is all very easy, it’s less familiar in our book since we typically do not work
explicitly with the parameters. To cast our linear model into a more familiar form, let’s
consider a dual expression for the solution. First, let’s substitute our expression for the
optimal parameters W ⋆
ij, (10.123), into our linear regression solution, (10.125), giving
zi
 x ˙β; θ⋆ =
X
˜α∈A


nf
X
j,k=0
φj(x ˙β)

M−1
jk φk(x˜α)

yi;˜α .
(10.126)
Note that the expression in the square brackets involves the inversion of an (nf + 1) ×
(nf + 1)-dimensional matrix Mij, which was required to obtain the optimal parameters
W ⋆
ij. This works well if the number of features is small, but if the number of feature
functions we deﬁned is very large nf ≫1, then representing and inverting such a matrix
might be computationally diﬃcult.
However, it turns out that we actually don’t need to do any of that. To see why, let
us introduce a new ND × ND-dimensional symmetric matrix:
kδ1δ2 ≡k(xδ1, xδ2) ≡
nf
X
i=0
φi(xδ1) φi(xδ2) .
(10.127)
As an inner product of feature functions, kδ1δ2 is a measure of similarity between two
inputs xi;δ1 and xi;δ2 in feature space. Such a measure of similarity is called a kernel.45
regularization term of the form a P
ij W 2
ij to the loss (10.120), cf. footnote 21 in §10.2.4 for a related
discussion of regularization for inﬁnite-width networks. In this modiﬁed regression problem, we can then
invert the regularized matrix
Mij = 2a δij +
X
˜α∈A
φi(x˜α)φj(x˜α) ,
(10.124)
and send the regulator to zero, a →0+, at the end of our calculations. Note that either when the
regulator a is kept ﬁnite or when we’re in the underparameterized regime with (nf + 1) < NA, the linear
model will no longer reach zero training loss even when fully optimized.
45For instance, in the case of the simplest linear model (10.118), the kernel is just given by the inner
product between the two inputs
kδ1δ2 ≡
n0
X
i=1
xi;δ1xi;δ2 ,
(10.128)
which is often called the linear kernel.
284

In a way that should feel very familiar, we’ll also denote an NA-by-NA-dimensional
submatrix of the kernel evaluated on the training set as ek˜α1 ˜α2 with a tilde. This lets us
write its inverse as ek˜α1 ˜α2, which satisﬁes
X
˜α2∈A
ek˜α1 ˜α2ek˜α2 ˜α3 = δ ˜α1
˜α3 .
(10.129)
Note that given the deﬁnition of the kernel (10.127), for this inverse to exist and for this
equation to hold we must be in the overparameterized regime with (nf + 1) ≥NA.
Now with this, let’s see how we might rearrange the factor in the square brackets of
our solution (10.126). Multiplying it by the submatrix ek˜α˜α1, we can simplify this factor
as
X
˜α∈A


nf
X
j,k=0
φj(x ˙β)

M−1
jk φk(x˜α)

ek˜α˜α1
(10.130)
=
X
˜α∈A
nf
X
j,k=0
φj(x ˙β)

M−1
jk φk(x˜α)
nf
X
i=0
φi(x˜α) φi(x˜α1)
=
nf
X
i,j,k=0
φj(x ˙β)

M−1
jk Mki φi(x˜α1)
=
nf
X
i=0
φi(x ˙β)φi(x˜α1) = k ˙β ˜α1 .
To get this result, in the second line we plugged in the deﬁnition of the kernel (10.127),
in the third line we performed the sum over ˜α using the deﬁnition of the feature matrix
Mij (10.122), and in the last equality of the fourth line we again used the deﬁnition of
the kernel. Finally, multiplying the ﬁrst and last expressions by the inverse submatrix
ek˜α1 ˜α2, we get a new representation for the factor in the square brackets


nf
X
j,k=0
φj(x ˙β)

M−1
jk φk(x˜α2)

=
X
˜α1∈A
k ˙β ˜α1ek˜α1 ˜α2 ,
(10.131)
which lets us rewrite the prediction of our linear model (10.125) as
zi
 x ˙β; θ⋆ =
X
˜α1,˜α2∈A
k ˙β ˜α1ek˜α1 ˜α2yi;˜α2 .
(10.132)
When the prediction of a linear model is computed in this way, it’s known as a kernel
machine or kernel methods.
Note that in this dual expression of the solution, the optimal parameters W ⋆
ij and the
feature functions φi(x) don’t appear. Thus, we’ve successfully exchanged our feature-
space quantities, an (nf + 1)-dimensional feature vector and the inverse of an (nf + 1) ×
(nf +1)-dimensional matrix, for sample-space quantities, an NA-dimensional vector k ˙β ˜α1
285

and the inverse of an NA × NA-dimensional matrix ek˜α1 ˜α2.46 This works because in our
solution (10.132), we actually only care about the inner product of the feature functions
– i.e. the kernel – and not the values of the features themselves.
By writing the linear model’s prediction in terms of the kernel in (10.132), we can
interpret the prediction in terms of direct comparison with previously-seen examples.
In particular, this solution computes the similarity of a new test input x ˙β with all the
training examples with k ˙β ˜α1 and then uses that similarity to linearly weight the true
function values from the training set yi;˜α2 with the sample-space metric ek˜α1 ˜α2.
For
this reason, kernel methods are sometimes referred to as memory-based methods since
they involve memorizing the entire training set.47 This should be contrasted with the
parameterized linear model solution (10.125), where we forget the training set samples
and instead just explicitly store the optimal parameter values W ⋆
ij.
Finally, note that there’s “no wiring” in the prediction: the zi component of the
prediction is entirely determined by the yi component of the training examples. This
is only implicit in the optimal weight matrix W ⋆
ij (10.123) in the linear model solution
(10.125) but is explicit in the kernel method solution (10.132). This is one of many ways
that such linear models and kernel methods are limited machine learning models.
46In some situations, specifying and evaluating the kernel is much simpler than specifying and evalu-
ating the feature functions. For instance, the Gaussian kernel, given by
kδ1δ2 ≡exp
"
−1
2σ2
n0
X
i=1
(xi;δ1 −xi;δ2)2
#
,
(10.133)
implies an inﬁnite-dimensional feature space, but can be evaluated by simply computing the squared
distance between the nin-dimensional input vectors and then exponentiating the result. (To see why the
Gaussian kernel implies an inﬁnite number of feature functions, we can express the squared distance as a
sum of three inner products and then Taylor expand the exponential in those inner products; the terms
in the Taylor expansion give the feature functions.) In this way, we see how computing the kernel can
be far easier than representing the features explicitly.
In fact, any algorithm based on a linear kernel (10.128) can be generalized by swapping the simple
kernel for a more complicated kernel like the Gaussian kernel (10.133). This is known as the kernel
trick and is a way to describe in the language of kernel methods how we generalized our simplest linear
model (10.118) – that was linear in the input – to the more general linear model (10.119) – that was
nonlinear in the input.
47Often, for a particular kernel method to be tractable, the model’s predictions are made locally,
incorporating information mostly from the training samples nearest to the test sample of interest. Given
the ∗-polation results of last section, it’s not hard to imagine how such methods could be made to work
well.
A canonical example of such a local method is k-nearest neighbors [63, 64], which is a special type
of kernel method. By only considering nearby training points, these kinds of local algorithms can skirt
some of the impracticality that we’ve been pointing out for our exact Bayesian inference predictions
as well as for our frozen NTK predictions. It would be interesting to extend such local algorithms to
the ﬁnite-width exact Bayesian inference that we discussed in §6.4 or the ﬁnite-width gradient-based
learning prediction that we’ll discuss in §∞.
286

10.4.3
Inﬁnite-Width Networks as Linear Models
Surely, the kernel methods’ prediction formula (10.132) should seem awfully familiar to
you: it is precisely the same as the exact Bayesian mean prediction (6.64) if we identify
the kernel methods’ kernel kδ1δ2 with the Bayesian kernel K(L)
δ1δ2, and it is exactly the
same as the (neural tangent) kernel mean prediction (10.40) if we identify the kernel
methods’ kernel kδ1δ2 with the frozen neural tangent kernel Θ(L)
δ1δ2.48 This ﬁnally provides
a justiﬁcation for the names of these objects as well as for the name of the current chapter.
Indeed, there is a very direct connection between these traditional linear models and
kernel methods on the one hand and our (neural tangent) kernel learning of inﬁnite-
width models on the other hand. First, let’s discuss the simpler case of the Bayesian
kernel. As pointed out in §10.2.4, this choice corresponds to treating only output-layer
biases b(L)
i
and weights W (L)
ij
as trainable model parameters, and so the network output
at initialization is given by
z(L)
i
(xδ; θ) = b(L)
i
+
nL−1
X
j=1
W (L)
ij σ(L−1)
j;δ
=
nL−1
X
j=0
W (L)
ij σ(L−1)
j;δ
,
(10.134)
where on the right-hand side we deﬁned σ(L−1)
0;δ
≡1 and W (L)
i0
≡b(L)
i
. This is almost
the same as our linear model (10.119) except that the feature functions are random:
bφj;δ ≡σ(L−1)
j;δ
.
In particular, here we’ve hatted these feature functions to emphasize
that they depend on the parameters in the hidden layers that are sampled from the
initialization distribution at the beginning and then ﬁxed; this is sometimes called a
random feature model.
In this case, the kernel methods’ notion of the kernel is also stochastic
bkδ1δ2 =C(L)
b
bφ0;δ1 bφ0;δ2 + C(L)
W
nL−1
nL−1
X
j=1
bφj;δ1 bφj;δ2
(10.135)
=C(L)
b
+ C(L)
W


1
nL−1
nL−1
X
j=1
σ(L−1)
j;δ1
σ(L−1)
j;δ2

,
where in the ﬁrst line we have re-weighted the terms in the feature sum in the deﬁnition
of the kernel (10.127) by C(L)
b
and C(L)
W /nL.49 Note that we called this object (10.135)
the stochastic metric (4.70) when studying the RG ﬂow of preactivations. Now, taking
48You may have noticed that our stochastic (neural tangent) kernel prediction formula (10.39) also
depended on the network output at initialization and had a nonzero covariance. This is related to our
earlier discussion in footnote 44 that, when we’re in the overparameterized regime with (nf + 1) > NA,
as is especially the case when we have an inﬁnite number of features, the (nf + 1)-by-(nf + 1) matrix
matrix Mij ≡P
˜α∈A φi(x˜α)φj(x˜α) (10.122) does not have a unique inverse. Thus, in this regime, the
optimal weight matrix W ⋆is not unique: if we don’t use the regulation trick (10.124) to uniquely pick
out one of the solutions, the prediction in the dual kernel description will have a dependence on the
model’s initialization.
49A more general deﬁnition of the kernel methods’ kernel (10.127) allows us to weight the contribution
287

an expectation over the initialization ensemble, in the inﬁnite-width limit we have
E
h
bkδ1δ2
i
= C(L)
b
+ C(L)
W ⟨σδ1σδ2⟩K(L−1) = K(L)
δ1δ2 ,
(10.137)
where in the last equality we used the recursion for the kernel (10.44).50 In this way,
we see how we can interpret exact Bayesian inference at inﬁnite width as a simple linear
model (10.134) of ﬁxed random features.
Now, let’s give a linear model interpretation to gradient-based learning at inﬁnite
width. Since a linear model is linear in its parameters, we can more generally deﬁne the
random features by
bφi,µ(xδ) ≡
dz(L)
i;δ
dθµ
.
(10.138)
To be clear, this derivative is evaluated at initialization and these features are thus ﬁxed
in the inﬁnite-width limit. Explicitly, for an MLP the random features are given by
bφi,W (ℓ)
k1k2
(xδ) =


X
jL−1,...,jℓ+1
W (L)
ijL−1σ′ (L−1)
jL−1;δ · · · W (ℓ+1)
jℓ+1k1σ′ (ℓ)
k1;δ

σ(ℓ−1)
k2;δ
,
(10.139)
with the bias component given by setting σ(ℓ)
0;δ ≡1 and W (ℓ)
k0 ≡b(ℓ)
k . As is apparent from
this expression, these features are stochastic, depending on the speciﬁc values of the
biases and weights at initialization. Note also that the Bayesian linear model (10.134)
only uses a subset of these features, bφi,W (L)
ij
= σ(L−1)
j;δ
, and thus is a much more limited
and less expressive model.
Note further that these feature functions bφi,µ(xδ) are related to, but not exactly
equivalent to, the previous notion of feature we gave when we discussed representa-
tion group ﬂow in §4.6. In that case, our ℓ-th-layer features corresponds to ℓ-th-layer
preactivations z(ℓ)
i;δ or activations σ(ℓ)
i;δ . However, here we see that the random feature
functions (10.139) are proportional to (ℓ−1)-th-layer activations σ(ℓ−1)
i;δ
but are also
multiplied by objects from deeper layers.51
of each pair of feature functions as
kδ1δ2 ≡
nf
X
i,j=0
cij φi(xδ1) φj(xδ2) .
(10.136)
50Note alternatively, by the central limit theorem, that the stochastic kernel bkδ1δ2 will be equal to
the kernel K(L)
δ1δ2 in the inﬁnite-width limit without explicitly averaging over initializations. This self-
averaging of the kernel is equivalent to the fact that the connected four-point correlator vanishes at
inﬁnite width. Here we see that such self-averaging of the kernel can also be thought of as arising from
a sum over an inﬁnite number of random features.
51Going forward, when referring to the features of a network, we will mean the kernel methods’ notion
of a feature function bφi,µ(xδ), rather than an activation σ(ℓ)
i (xδ). Accordingly, we will now understand
representation learning to describe how such feature functions develop a data dependence during training.
288

As should be clear, the stochastic kernel associated with these features,
bkij;δ1δ2 ≡
X
µ,ν
λµν bφi,µ(xδ1)bφj,ν(xδ2) =
X
µ,ν
λµν
dz(L)
i;δ1
dθµ
dz(L)
j;δ2
dθν
≡bH(L)
ij;δ1δ2 ,
(10.140)
is just the L-th-layer stochastic NTK (8.5).52 Here, we have taken advantage of our more
general deﬁnition of the kernel methods’ kernel (10.136) to incorporate the learning-rate
tensor λµν into the expression. Accordingly, at inﬁnite width the NTK is frozen and
diagonal in ﬁnal layer neural indices, giving
kδ1δ2 ≡
X
µ,ν
λµν bφi,µ(xδ1) bφi,ν(xδ2) =
X
µ,ν
λµν
dz(L)
i;δ1
dθµ
dz(L)
i;δ2
dθν
≡Θ(L)
δ1δ2 .
(10.141)
In this way, we see that at inﬁnite width the fully-trained mean network output is just
a linear model based on random features (10.138). In this sense, inﬁnite-width neural
networks are rather shallow in terms of model complexity, however deep they may appear.
Looking back, when we discussed the linear model at the beginning of this section,
we had to introduce feature functions φi(x), designed using our knowledge of the task
and data at hand, as a way to (pre-)process the input. This way, the parametric model
that we learn is really simple, i.e. linear.53 We then reinterpreted this ﬁt linear model
in terms of its associated kernel, which itself has a natural interpretation as measuring
similarity between our designed features.
However, for inﬁnite-width networks we didn’t design the frozen NTK, and its as-
sociated features are random. Instead, the network is deﬁned by the architecture, hy-
perparameters, and biases and weights, and the path from those variables to the NTK
and kernel prediction is ﬁlled with calculations. So the abstraction of the actual neural
network seems like a very odd way to design a kernel.
Indeed, we’ve just learned that inﬁnite-width neural networks can only make predic-
tions that are linear in the true outputs from the training set; they are linear models
that can only compute linear combinations of random features. Of course, deep learning
52However please note importantly that at ﬁnite width the stochastic neural tangent kernel bkij;δ1δ2 =
b
H(L)
i1i2;δ1δ2 is not ﬁxed during training – learning useful features from the data – and randomly varies
across initializations; hence – despite its name – it is not actually a kernel.
53Please don’t confuse our discussion of linear models here and our discussion of linear vs. nonlinear
functions as in §10.3.2.
A linear model is a model that’s linear in the model parameters (10.119) and has a dual kernel descrip-
tion that’s linear in the true outputs yi;˜α in the training set A (10.132); as we have seen, linear models
are very simple and easy to solve analytically. As is clear from the deﬁnition (10.119), a linear model
in general will be nonlinear in the inputs x for general nonlinear feature functions φi(x). Accordingly,
linear models can compute nonlinear functions of their input. We saw this explicitly when we worked
out how nonlinear ∗-polation works for smooth nonlinear networks in (10.113).
In contrast, a deep linear network is a neural network that uses a linear activation function. Such
networks compute linear functions of their input and thus may only linearly ∗-polate between training
points, cf. (10.104). However, since they are not linear models (for L > 1), the function they compute
depends nonlinearly on the model parameters. Accordingly, their training dynamics can be somewhat
complicated, and at ﬁnite width they even exhibit representation learning.
289

is exciting because it works on problems where classic machine learning methods have
failed; it works in cases where we don’t know how to design feature functions or kernels,
or doing so would be too complicated. For neural networks to go beyond the kernel
methods, they need to be able to learn useful features from the data, not just make use
of a complicated linear combination of random features. Thus, in order for the feature
functions (10.138) to evolve during training – that is, in order to have representation
learning – we will need to go beyond kernel methods.
Luckily, ﬁnite-width networks are not kernel methods. Please now turn to the next
chapter to ﬁnd out exactly what they are instead.
290

Chapter 11
Representation Learning
It can scarcely be denied that the supreme goal of all theory is to make the irreducible
basic elements as simple and as few as possible without having to surrender the
adequate representation of a single datum of experience.
Albert Einstein in a 1933 lecture, “On the Method of Theoretical Physics,” [65].
Last chapter, we understood that linear models cannot learn features from data. Thus,
the inﬁnite-width limit is too simple to provide an adequate representation of deep
learning; in order to include its irreducible basic element – representation learning – it
is qualitatively important to study ﬁnite-width networks.
In the ﬁrst half of this chapter, we’ll analyze the leading correction to the gradient-
descent update to the network output by extending its Taylor expansion to second order
in the global learning rate η. After further seeing that a similar contribution arises in
the ﬁrst-order Taylor expansion of the update to the NTK, we’ll then show that this
correction is a ﬁnite-width eﬀect. This upgrade of the NTK from ﬁxed to dynamical
indicates that for ﬁnite-width networks, the feature functions that comprise the NTK
are themselves learning from the data over the course of training.
Unfortunately, the complete O(1/n) contribution to the dynamics further includes
terms that arise from Taylor expanding the update to the network output to third order
in the global learning rate η, and similarly Taylor expanding the update to the NTK to
second order in η. While it’s necessary to include these contributions in order to actually
compute the distribution of fully-trained ﬁnite-width networks, the O
 η2 expansion of
the network output and the O(η) expansion of the NTK is suﬃcient to qualitatively
investigate the mechanism for representation learning in these models.
With that in mind, in order to separate the pedagogy of representation learning from
the messy phenomenological details of the real MLP, we’ll spend the second half of this
chapter focusing on a simpliﬁed model that’s equivalent to this O
 η2 truncation and
gives a minimal qualitative picture of representation learning. These minimal models
that we discuss form a valid and potentially useful class of machine learning models
that perform representation learning, though annoyingly ﬁnite-width MLPs are not in
291

this class. Listening carefully to these real MLPs, we’ll spend all of next chapter (§∞)
working out their O(1/n) training dynamics in full intricate detail.
To begin, in §11.1 we’ll work out this second-order-in-η contribution to the update to
preactivations and the ﬁrst-order-in-η contribution to the update to the NTK. This lets
us source all representation learning from a single irreducible basic element, the diﬀeren-
tial of the neural tangent kernel (dNTK): just as the NTK governs the leading-order-in-η
dynamics of the preactivations, the dNTK governs the leading-order-in-η dynamics of
the NTK.
After thusly identifying the dNTK as a driver of representation learning, in §11.2
we’ll recursively determine its correlation with the preactivations as a function of network
layer ℓ. In detail, we’ll ﬁrst derive a stochastic forward equation for the dNTK and
then evaluate the remaining recursions needed to determine the statistics of the joint
preactivation-NTK-dNTK distribution at initialization. As such, this section mirrors
the structure of our RG-ﬂow analysis in §4 and §8. Importantly, we’ll see that all the
statistics involving the dNTK are O(1/n) and thus only contribute at ﬁnite width.
In §11.3, we’ll apply the principles of criticality and universality to analyze the new
dNTK recursions. Since all of our hyperparameters have already been ﬁxed by the par-
allel analysis of the preactivations in §5 – ﬁxing the initialization hyperparameters –
and the NTK in §9 – ﬁxing the training hyperparameters – our focus here will be on
evaluating the depth and width scaling of the dNTK statistics with these ﬁxed hyper-
parameters. As you might guess, we’ll ﬁnd across our two universality classes (§11.3.1
and §11.3.2) that the eﬀect of the dNTK – and therefore one source of representation
learning – is proportional to our eﬀective theory cutoﬀ, the depth-to-width ratio L/n.
Having now ﬁrmly established that the NTK evolves at ﬁnite width – and having
worked out an important contribution to its dynamics – in §11.4, we’ll take a step back
and look for a broader context, mirroring our discussion in §10.4 for inﬁnite-width net-
works.
To that end, in §11.4.1 we’ll introduce a class of nonlinear models – with a
particular focus on the quadratic model – and thus minimally extend the traditional
workhorse of machine learning, the linear model. This quadratic model provides a min-
imal model of representation learning, independent of any neural-network abstraction.
Moreover, these models are simple and completely analyzable, and yet are able to capture
the essence of representation learning.
After solving the implied nearly-linear quadratic regression problem, in §11.4.2 we’ll
further provide a dual description of the quadratic model solution, which we’ll call nearly-
kernel methods. This will let us identify an object that corresponds to the dNTK in this
minimal setting, and show us how to make test-set predictions with a trained kernel
that learns from the data. Overall, we hope this framework will be of further theoretical
and practical interest as a new class of nearly-simple machine learning models that learn
representations.
At this point, the connection between these nearly-kernel methods and ﬁnite-width
networks – at least at order η2 – will be nearly manifest, and in §11.4.3 we’ll make it
explicit. By doing so, we’ll understand precisely how deep learning is a non-minimal
model of representation learning.
Ultimately, we’ll conclude that the power of deep
292

learning is the deep – the inductive bias of the network architecture induced by the layer-
to-layer RG ﬂow – providing a particularly good choice of initial features as a starting
point for learning. These observations will be quite helpful for us in interpreting our
somewhat messy ﬁnite-width solution in the following chapter.
11.1
Diﬀerential of the Neural Tangent Kernel
Recall that in the ﬁrst step of gradient descent, the change in the ℓ-th-layer parameters
of any particular network is given by (7.11)
d¯θ(ℓ)
µ
≡θ(ℓ)
µ (t = 1) −θ(ℓ)
µ (t = 0) = −η
X
ν
λ(ℓ)
µν


nL
X
k=1
X
˜α∈A
∂LA
∂z(L)
k;˜α
dz(L)
k;˜α
dθ(ℓ)
ν

.
(11.1)
In this section, it will be helpful to specify explicitly which layer each parameter comes
from. In particular, here θ(ℓ)
µ
denotes either an ℓ-th-layer bias θ(ℓ)
µ
≡b(ℓ)
i
or an ℓ-th-
layer weight θ(ℓ)
µ
≡W (ℓ)
ij , and the ℓ-th-layer model-parameter indices µ, ν run over all
the components of the bias vector b(ℓ)
i
and the weight matrix W (ℓ)
ij
in the ℓ-th layer
only. Additionally, to emphasize that the learning-rate tensor λ(ℓ)
µν only connects the
parameters within a given layer ℓ, we’ve decorated it with a layer index for clarity. For
now we’ll let λ(ℓ)
µν act arbitrarily within a layer, though ultimately we’ll be interested in
the case where it’s diagonal, with two training hyperparameters per layer, λ(ℓ)
b
and λ(ℓ)
W ,
as usual.
As a further reminder, quantities without any explicit step argument are taken to be
evaluated at initialization – though sometimes we may also explicitly denote t = 0 for
extra emphasis – and our sample-index notation is alpha-with-tilde for the inputs in the
training set, ˜α ∈A, beta-with-dot for inputs in the test set, ˙β ∈B, and delta-with-no-
decoration for inputs that could be in either set, δ ∈D = A ∪B.
Now, to go beyond the inﬁnite-width limit, we’ll need to expand the change in ℓ-th-
layer preactivations to second order in the parameter update:
d¯z(ℓ)
i;δ ≡z(ℓ)
i;δ (t = 1) −z(ℓ)
i;δ (t = 0)
(11.2)
=
ℓ
X
ℓ1=1
X
µ
dz(ℓ)
i;δ
dθ(ℓ1)
µ
d¯θ(ℓ1)
µ
+ 1
2
ℓ
X
ℓ1,ℓ2=1
X
µ1,µ2
d2z(ℓ)
i;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
d¯θ(ℓ1)
µ1 d¯θ(ℓ2)
µ2 + . . . .
Note that the ℓ-th-layer preactivations z(ℓ)
i;δ cannot depend on model parameters θ(ℓ′)
µ
from
layers ℓ′ that are deeper than the ℓ-th layer. Thus, when ℓ′ > ℓ, we have dz(ℓ)
i;δ /dθ(ℓ′)
µ
= 0,
and so we truncated our layer sums in the above expression at ℓ.
Next, we are going to slightly rewrite the parameter update equation (11.1) for the
parameters θ(ℓa)
µ
appearing in our preactivation expansion (11.2), i.e. for those param-
eters in layers ℓa ≤ℓthat contribute. To do so, we’ll make use of the chain rule to
293

decompose the derivative of the output-layer preactivations z(L)
k;˜α with respect to the
ℓa-th-layer model parameters as
dz(L)
k;˜α
dθ(ℓa)
ν
=
X
j
dz(L)
k;˜α
dz(ℓ)
j;˜α
dz(ℓ)
j;˜α
dθ(ℓa)
ν
,
(11.3)
for an intermediate layer ℓsuch that ℓa ≤ℓ. Using this decomposition, we can rewrite
our parameter update (11.1) as
d¯θ(ℓa)
µ
= −η
X
ν
λ(ℓa)
µν

X
j,k,˜α
∂LA
∂z(L)
k;˜α
dz(L)
k;˜α
dz(ℓ)
j;˜α
dz(ℓ)
j;˜α
dθ(ℓa)
ν

= −η
X
ν,j,˜α
λ(ℓa)
µν ϵ(ℓ)
j;˜α
dz(ℓ)
j;˜α
dθ(ℓa)
ν
,
(11.4)
where in the last equality we introduced an ℓ-th-layer error factor:
ϵ(ℓ)
j;˜α ≡
nL
X
k=1
∂LA
∂z(L)
k;˜α
dz(L)
k;˜α
dz(ℓ)
j;˜α
= dLA
dz(ℓ)
j;˜α
.
(11.5)
Substituting this form of the parameter update (11.4) into the ℓ-th-layer preactivation
update (11.2), our second-order expansion becomes
d¯z(ℓ)
i;δ = −η
X
j,˜α


ℓ
X
ℓ1=1
X
µ,ν
λ(ℓ1)
µν
dz(ℓ)
i;δ
dθ(ℓ1)
µ
dz(ℓ)
j;˜α
dθ(ℓ1)
ν

ϵ(ℓ)
j;˜α
(11.6)
+ η2
2
X
j1,j2,˜α1,˜α2



ℓ
X
ℓ1,ℓ2=1
X
µ1,ν1,
µ2,ν2
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2
d2z(ℓ)
i;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
dz(ℓ)
j1;˜α1
dθ(ℓ1)
ν1
dz(ℓ)
j2;˜α2
dθ(ℓ2)
ν2


ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2
+ . . . ,
which is quadratic in such error factors.
Here, it was essential that we treated the
parameters in a per-layer manner and that each learning-rate tensor λ(ℓa)
µν
was restricted
to a single layer ℓa; had we not done that, our decomposition (11.4) and update equation
(11.6) would have been far more complicated.
Naturally, the object in the ﬁrst parenthesis of the update equation (11.6) is the
stochastic ℓ-th-layer NTK(8.5)
bH(ℓ)
i1i2;δ1δ2 ≡
ℓ
X
ℓ1=1
X
µ,ν
λ(ℓ1)
µν
dz(ℓ)
i1;δ1
dθ(ℓ1)
µ
dz(ℓ)
i2;δ2
dθ(ℓ1)
ν
,
(11.7)
as you know quite well by now, though in this version of the deﬁnition we represent the
sum over layers explicitly and the sum over parameter indices µ, ν runs per layer.
In contrast, the object in the second parenthesis is new.1
Let’s call this object
the stochastic ℓ-th-layer diﬀerential of the neural tangent kernel (dNTK) and
1This object ﬁrst appeared, unnamed, in both [66] and [67] around the same time. Here, we’ll compute
its recursion, determine its scaling with depth, and emphasize its physical importance by highlighting
its connection to representation learning.
294

symbolize it as
d
dH
(ℓ)
i0i1i2;δ0δ1δ2 ≡
ℓ
X
ℓ1,ℓ2=1
X
µ1,ν1,
µ2,ν2
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2
d2z(ℓ)
i0;δ0
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
dz(ℓ)
i1;δ1
dθ(ℓ1)
ν1
dz(ℓ)
i2;δ2
dθ(ℓ2)
ν2
.
(11.8)
Here, the hats on both the NTK and the dNTK remind us that these objects are stochas-
tic, depending on the particular realization of the model parameters at initialization.
Also, from its deﬁnition note that the dNTK is symmetric in its second and third paired
set of indices (i1, δ1) ↔(i2, δ2), while the ﬁrst neural-sample index (i0, δ0) is distinguished
from the other two.
Using both deﬁnitions (11.7) and (11.8), our second-order expansion (11.6) can be
more compactly written as
d¯z(ℓ)
i;δ = −η
X
j,˜α
bH(ℓ)
ij;δ˜αϵ(ℓ)
j;˜α + η2
2
X
j1,j2,˜α1,˜α2
d
dH
(ℓ)
ij1j2;δ˜α1 ˜α2ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2 + . . . .
(11.9)
In other words, we have a power series in error factors. To ultimately understand how
the preactivations evolve under gradient descent at leading order in 1/n, we’ll actually
need to extend this expansion to order η3, which in turn will require that we introduce
a few additional tensors. Rather than worry about that now, we’ll put it oﬀto §∞.
Regardless of those additional higher-order terms, from (11.9) we already see that we’ll
need to know the joint statistics of the preactivations – encoding the error factors ϵ(ℓ)
j;˜α –
the NTK bH(ℓ)
ij;δ˜α, and the dNTK d
dH
(ℓ)
ij1j2;δ˜α1 ˜α2.
Finally, as an explanation for our choice of name and symbol for the dNTK, consider
the leading-order update to the ℓ-th-layer NTK after a step of gradient descent:
d¯H(ℓ)
i1i2;δ1δ2 ≡H(ℓ)
i1i2;δ1δ2(t = 1) −H(ℓ)
i1i2;δ1δ2(t = 0)
(11.10)
=
ℓ
X
ℓ1=1
X
µ1
dH(ℓ)
i1i2;δ1δ2
dθ(ℓ1)
µ1
d¯θ(ℓ1)
µ1 + . . .
= −η
ℓ
X
ℓ1=1
X
µ1


d
dθ(ℓ1)
µ1


ℓ
X
ℓ2=1
X
µ2,ν2
λ(ℓ2)
µ2ν2
dz(ℓ)
i1;δ1
dθ(ℓ2)
µ2
dz(ℓ)
i2;δ2
dθ(ℓ2)
ν2





X
ν1
λ(ℓ1)
µ1ν1
X
j,˜α
dz(ℓ)
j;˜α
dθ(ℓ1)
ν1
ϵ(ℓ)
j;˜α

+ . . .
= −η
X
j,˜α


ℓ
X
ℓ1,ℓ2=1
X
µ1,ν1,
µ2,ν2
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2
d2z(ℓ)
i1;δ1
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
dz(ℓ)
i2;δ2
dθ(ℓ2)
ν2
dz(ℓ)
j;˜α
dθ(ℓ1)
ν1

ϵ(ℓ)
j;˜α
−η
X
j,˜α


ℓ
X
ℓ1,ℓ2=1
X
µ1,ν1,
µ2,ν2
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2
dz(ℓ)
i1;δ1
dθ(ℓ2)
µ2
d2z(ℓ)
i2;δ2
dθ(ℓ1)
µ1 dθ(ℓ2)
ν2
dz(ℓ)
j;˜α
dθ(ℓ1)
ν1

ϵ(ℓ)
j;˜α + . . .
= −η
X
j,˜α

d
dH
(ℓ)
i1i2j;δ1δ2 ˜α + d
dH
(ℓ)
i2i1j;δ2δ1 ˜α

ϵ(ℓ)
j;˜α + . . . .
295

Here, in the third line we inserted the deﬁnition of NTK (11.7) and the parameter update
(11.4) for ℓ1 ≤ℓ, and on the ﬁnal line we used the deﬁnition of the dNTK (11.8). Thus
we see that the dNTK – when multiplied by the global learning rate and contracted
with an ℓ-th-layer error factor – gives the update to the ℓ-th-layer NTK after a step of
gradient descent.2
Since we know that the inﬁnite-width NTK is frozen
bH(ℓ) →Θ(ℓ), the relation
between the NTK update and the dNTK implies that the dNTK must be a ﬁnite-width
eﬀect, vanishing in the strict inﬁnite-width limit d
dH
(ℓ) →0. Similarly, at inﬁnite width
we truncated the preactivation updates (11.9) to be linear in the global learning rate
η, cf. (10.2). In the next section, we will verify all of this by computing the dNTK
recursively and showing explicitly that d
dH
(ℓ) = O(1/n).
11.2
RG Flow of the dNTK
As its title suggests, the structure of this section parallels §4 – where we worked out the
layer-to-layer representation group (RG) ﬂow of the preactivation distribution p

z(ℓ)D

– and §8 – where we worked out the layer-to-layer RG ﬂow of the NTK-preactivation joint
distribution p

z(ℓ), bH(ℓ)D

. Speciﬁcally, we will now work out the eﬀective ℓ-th-layer
joint distribution of the preactivations, the NTK, and the dNTK:
p

z(ℓ), bH(ℓ), d
dH
(ℓ)D

.
(11.11)
This analysis is important for two reasons: (i) ﬁrstly, understanding the statistics of this
ℓ-th-layer joint distribution at order 1/n is a necessary prerequisite for understanding
the leading nontrivial ﬁnite-width corrections for deep neural networks trained with
gradient-based learning; (ii) secondly, in §11.4 we will see that a nonvanishing dNTK
is suﬃcient for a network to exhibit representation learning, and thus by showing that
the dNTK is of order 1/n, we will ﬁrmly establish that the leading-order ﬁnite-width
eﬀective theory is able to describe this essential property of deep learning.
Zeroth, we’ll establish the stochastic iteration equation for the dNTK (§11.2.0).
Then, beginning our statistical analysis, ﬁrst we’ll see that the dNTK vanishes iden-
tically in the ﬁrst layer (§11.2.1). Second, we’ll see that there’s a nontrivial cross cor-
relation between the dNTK and the preactivations in the second layer (§11.2.2). Third
and ﬁnally, we’ll work out a general recursion that controls the accumulation of such
dNTK-preactivation cross correlations in deeper layers (§11.2.3).
2Please don’t confuse our italicized, crossed, and unhatted notation, d¯H(ℓ)
i1i2;δ1δ2, representing the ﬁrst
update to the NTK, with our unitalicized, uncrossed, and hatted notation, c
dH
(ℓ)
i0i1i2;δ0δ1δ2, representing
the dNTK. In this chapter we will focus on the statistics of the dNTK, and we will not use this notation
when evaluating the NTK dynamics in the following chapter.
296

11.2.0
Forward Equation for the dNTK
Just as we needed to derive a stochastic forward iteration equation for the NTK (8.12)
in §8.0 before working out recursions for its statistics, here we’ll derive such an equation
for the dNTK.
Let’s start by writing out the deﬁnition of the dNTK (11.8) at layer (ℓ+ 1):
d
dH
(ℓ+1)
i0i1i2;δ0δ1δ2 ≡
ℓ+1
X
ℓ1,ℓ2=1


X
µ1,ν1,
µ2,ν2
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2
d2z(ℓ+1)
i0;δ0
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
dz(ℓ+1)
i1;δ1
dθ(ℓ1)
ν1
dz(ℓ+1)
i2;δ2
dθ(ℓ2)
ν2

.
(11.12)
To determine its forward equation, we need to explicitly evaluate the derivatives with
respect to the (ℓ+1)-th-layer parameters and also rewrite all the (ℓ+1)-th-layer quantities
in terms of the ℓ-th-layer quantities using the chain rule. Depending on the values of ℓ1
and ℓ2, there are thus three cases to consider for the double summation over layers.
First, when both layers are maximal ℓ1 = ℓ2 = ℓ+1 there is no contribution. Recalling
for one ﬁnal time the preactivation forward equation,
z(ℓ+1)
i;δ
= b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;δ ,
(11.13)
we see that the (ℓ+ 1)-th-layer preactivations are always linear in the (ℓ+ 1)-th-layer
model parameters θ(ℓ+1)
µ
. Thus, in this case the second derivative in the dNTK deﬁnition
(11.12) will vanish.
Second, when ℓ1 = ℓ+ 1 and ℓ2 < ℓ+ 1, there is a contribution from the (ℓ+ 1)-th-
layer weights but not from the (ℓ+1)-th-layer biases. Considering the bias θ(ℓ1)
µ1
= b(ℓ+1)
j
,
the (ℓ+ 1)-th-layer derivative gives a Kronecker delta
dz(ℓ+1)
i;δ
db(ℓ+1)
j
= δij ,
(11.14)
and so the second derivative again vanishes
d2z(ℓ+1)
i;δ
db(ℓ+1)
j
dθ(ℓ2)
µ2
= 0 .
(11.15)
Instead considering the weight matrix θ(ℓ1)
µ1
= W (ℓ+1)
jk
, the (ℓ+ 1)-th-layer derivative is
not a constant
dz(ℓ+1)
i;δ
dW (ℓ+1)
jk
=δijσ(ℓ)
k;δ .
(11.16)
Thus, the second derivative evaluates to something nontrivial
d2z(ℓ+1)
i;δ
dW (ℓ+1)
jk
dθ(ℓ2)
µ2
=δijσ′ (ℓ)
k;δ
dz(ℓ)
k;δ
dθ(ℓ2)
µ2
,
(11.17)
297

while the remaining ﬁrst derivative gives
dz(ℓ+1)
i;δ
dθ(ℓ2)
ν2
=
X
k
W (ℓ+1)
ik
σ′ (ℓ)
k;δ
dz(ℓ)
k;δ
dθ(ℓ2)
ν2
,
(11.18)
with the use of the chain rule. Plugging in these three derivative evaluations (11.16),
(11.17), and (11.18) to evaluate terms in the dNTK deﬁnition (11.12) with ℓ1 = ℓ+ 1
and ℓ2 < ℓ+ 1, we ﬁnd
ℓ
X
ℓ2=1
X
j,k
λW (ℓ+1)
jk
W (ℓ+1)
jk
X
µ2,ν2
λ(ℓ2)
µ2ν2
d2z(ℓ+1)
i0;δ0
dW (ℓ+1)
jk
dθ(ℓ2)
µ2
dz(ℓ+1)
i1;δ1
dW (ℓ+1)
jk
dz(ℓ+1)
i2;δ2
dθ(ℓ2)
ν2
=λ(ℓ+1)
W
nℓ
ℓ
X
ℓ2=1
X
j,k
X
µ2,ν2
λ(ℓ2)
µ2ν2

δi0j σ′ (ℓ)
k;δ0
dz(ℓ)
k;δ0
dθ(ℓ2)
µ2



δi1j σ(ℓ)
k;δ1


X
k2
W (ℓ+1)
i2k2 σ′ (ℓ)
k2;δ2
dz(ℓ)
k2;δ2
dθ(ℓ2)
ν2


=λ(ℓ+1)
W
nℓ
δi0i1
X
k0,k2
W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k2;δ0δ2 .
(11.19)
To get this result, on the second line we implemented our choice of a single intralayer
learning rate for the weights (8.6),
λW (ℓ+1)
j1k1 W (ℓ+1)
j2k2
= δj1j2δk1k2
λ(ℓ+1)
W
nℓ
,
(11.20)
importantly rescaled by nℓ, and on the third line we used the deﬁnition of the stochastic
NTK (11.7) and relabeled a dummy index.
By symmetry, there must be a similar
contribution when instead ℓ2 = ℓ+ 1 and ℓ1 < ℓ+ 1. This term is given by (11.19) after
swapping neural-sample index pairs (i1, δ1) ↔(i2, δ2).
Third and ﬁnally, when both ℓ1 < ℓ+1 and ℓ2 < ℓ+1 both the biases and the weights
contribute to the second derivative. When θ(ℓ1)
µ
and θ(ℓ2)
ν
are not from the (ℓ+ 1)-th
layer, we computed their ﬁrst derivative in (11.18), and their second derivative is given
by
d2z(ℓ+1)
i;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
=
X
k
W (ℓ+1)
ik
σ′′ (ℓ)
k;δ
dz(ℓ)
k;δ
dθ(ℓ1)
µ1
dz(ℓ)
k;δ
dθ(ℓ2)
µ2
+
X
k
W (ℓ+1)
ik
σ′ (ℓ)
k;δ
d2z(ℓ)
k;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
.
(11.21)
Multiplying these second derivative terms by the learning-rate tensors λ(ℓ1)
µ1ν1 λ(ℓ2)
µ2ν2 and by
the appropriate ﬁrst derivatives (11.18), and implementing all the sums over ℓ1, ℓ2, µ1,
ν1, µ2, ν2 in the dNTK deﬁnition (11.12), the ﬁrst term from (11.21) gives a contribution
of
X
k0,k1,k2
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k1;δ0δ1 bH(ℓ)
k0k2;δ0δ2 ,
(11.22)
where we made use of the NTK deﬁnition (11.7) twice, while the second term from
(11.21) gives a contribution of
X
k0,k1,k2
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 d
dH
(ℓ)
k0k1k2;δ0δ1δ2 ,
(11.23)
298

where we made use of the dNTK deﬁnition (11.8) once.
Combining our three types of contributions (11.19), (11.22), and (11.23), we get a
rather involved stochastic iteration equation:
d
dH
(ℓ+1)
i0i1i2;δ0δ1δ2 =
X
k0,k1,k2
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 d
dH
(ℓ)
k0k1k2;δ0δ1δ2
(11.24)
+
X
k0,k1,k2
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k1;δ0δ1 bH(ℓ)
k0k2;δ0δ2
+ λ(ℓ+1)
W
nℓ
δi0i1
X
k0,k2
W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k2;δ0δ2
+ λ(ℓ+1)
W
nℓ
δi0i2
X
k0,k1
W (ℓ+1)
i1k1 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ(ℓ)
k0;δ2 bH(ℓ)
k0k1;δ0δ1 .
This is the forward equation for the dNTK, and we’re next going to work out the
recursions that determine its statistics.
11.2.1
First Layer: Zero dNTK
Recall from §4.1 and §8.1 that at initialization the ﬁrst-layer preactivations,
z(1)
i;δ ≡b(1)
i
+
n0
X
k=1
W (1)
ik xk;δ ,
(11.25)
are distributed according to a zero-mean Gaussian distribution (4.23) and that the NTK
bH(1)
i1i2;δ1δ2 = δi1i2H(1)
δ1δ2 is deterministic (8.23).
As we discussed just before, since the preactivations are linear in the model parame-
ters, their second derivative must vanish. Thus, the dNTK trivially vanishes in the ﬁrst
layer:
d
dH
(1)
i0i1i2;δ0δ1δ2 ≡
X
µ1,ν1,
µ2,ν2
λ(1)
µ1ν1λ(1)
µ2ν2
d2z(1)
i0;δ0
dθ(1)
µ1 dθ(1)
µ2
dz(1)
i1;δ1
dθ(1)
ν1
dz(1)
i2;δ2
dθ(1)
ν2
= 0 .
(11.26)
This gives the initial condition for our recursions.
Note that this result should have been expected as the ﬁrst-layer NTK (8.23) is
independent of the model parameters, and thus cannot change with any training. As we
saw before for the ﬁrst-layer preactivations and ﬁrst-layer NTK – zero-mean Gaussian
and ﬁxed, respectively – this ﬁrst-layer result for the dNTK will be representative of its
inﬁnite-width limit for all layers.
11.2.2
Second Layer: Nonzero dNTK
Now, let’s analyze the dNTK (11.24) in the second layer. Remembering again that the
ﬁrst-layer NTK (8.23) is deterministic and diagonal in its neural indices as bH(1)
i1i2;δ1δ2 =
299

δi1i2H(1)
δ1δ2, and remembering for the ﬁrst time that the dNTK vanishes in the ﬁrst layer
d
dH
(1)
i0i1i2;δ0δ1δ2 = 0 from (11.26), the forward equation (11.24) in the second layer simpli-
ﬁes to
d
dH
(2)
i0i1i2;δ0δ1δ2 =H(1)
δ0δ1H(1)
δ0δ2
n1
X
k=1
W (2)
i0kW (2)
i1kW (2)
i2kσ′′ (1)
k;δ0 σ′ (1)
k;δ1σ′ (1)
k;δ2
+ δi0i1
λ(2)
W
n1
H(1)
δ0δ2
n1
X
k=1
W (2)
i2kσ′ (1)
k;δ0σ(1)
k;δ1σ′ (1)
k;δ2
+ δi0i2
λ(2)
W
n1
H(1)
δ0δ1
n1
X
k=1
W (2)
i1kσ′ (1)
k;δ0σ′ (1)
k;δ1σ(1)
k;δ2 .
(11.27)
Interestingly, since each term has an odd number of weights, the mean of the dNTK will
vanish, and we’ll have to look at cross correlations to ﬁnd leading dNTK statistics that
are non-vanishing.
The simplest cross correlation is with a single preactivation. Considering the product
of the second-layer dNTK (11.27) with second-layer preactivations,
z(2)
i;δ = b(2)
i
+
n1
X
k=1
W (2)
ik σ(1)
k;δ ,
(11.28)
and taking an expectation, we ﬁnd
E

d
dH
(2)
i0i1i2;δ0δ1δ2z(2)
i3;δ3

=H(1)
δ0δ1H(1)
δ0δ2
 
C(2)
W
n1
!2
(δi0i3δi1i2 + δi0i1δi2i3 + δi0i2δi1i3)
n1
X
k=1
E
h
σ′′ (1)
k;δ0 σ′ (1)
k;δ1σ′ (1)
k;δ2σ(1)
k;δ3
i
+ λ(2)
W
n1
H(1)
δ0δ2δi0i1δi2i3
C(2)
W
n1
n1
X
k=1
E
h
σ′ (1)
k;δ0σ(1)
k;δ1σ′ (1)
k;δ2σ(1)
k;δ3
i
+ λ(2)
W
n1
H(1)
δ0δ1δi0i2δi1i3
C(2)
W
n1
n1
X
k=1
E
h
σ′ (1)
k;δ0σ′ (1)
k;δ1σ(1)
k;δ2σ(1)
k;δ3
i
= 1
n1
(δi0i3δi1i2 + δi0i1δi2i3 + δi0i2δi1i3) C(2)
W H(1)
δ0δ1C(2)
W H(1)
δ0δ2

σ′′
δ0σ′
δ1σ′
δ2σδ3

G(1)
+ 1
n1
δi0i1δi2i3λ(2)
W C(2)
W H(1)
δ0δ2

σ′
δ0σδ1σ′
δ2σδ3

G(1)
+ 1
n1
δi0i2δi1i3λ(2)
W C(2)
W H(1)
δ0δ1

σ′
δ0σ′
δ1σδ2σδ3

G(1)
(11.29)
To get this ﬁnal result, in the ﬁrst equality we dropped the bias term from (11.28), since it
vanishes under the expectation, and performed various Wick contractions of the weights
using E
h
W (2)
i1j1W (2)
i2j2
i
= δi1i2δj1j2C(2)
W /n1 (2.20). For the second equality, we remembered
that the ﬁrst-layer preactivation distribution is a zero-mean Gaussian with a two-point
300

correlator that’s diagonal in neural indices, E
h
z(1)
i1;δ1z(1)
i2;δ2
i
= δi1i2G(1)
δ1δ2 (4.23), and used
this to swap full expectations for Gaussian expectations and then performed the sums.
As we did before for the NTK variance (8.31) and the NTK-preactivation cross
correlation (8.37), it is convenient to decompose this dNTK-preactivation cross correla-
tion (11.29) into two tensors with sample indices only:
E

d
dH
(2)
i0i1i2;δ0δ1δ2z(2)
i3;δ3

≡1
n1
h
δi0i3δi1i2P (2)
δ0δ1δ2δ3 + δi0i1δi2i3Q(2)
δ0δ1δ2δ3 + δi0i2δi1i3Q(2)
δ0δ2δ1δ3
i
.
(11.30)
Comparing with our explicit formula for the second-layer cross correlation (11.29), we
see that these tensors have the following deﬁnitions,
P (2)
δ0δ1δ2δ3 ≡

C(2)
W
2 H(1)
δ0δ1H(1)
δ0δ2

σ′′
δ0σ′
δ1σ′
δ2σδ3

G(1) ,
(11.31)
Q(2)
δ0δ1δ2δ3 ≡

C(2)
W
2 H(1)
δ0δ1H(1)
δ0δ2

σ′′
δ0σ′
δ1σ′
δ2σδ3

G(1) + λ(2)
W C(2)
W H(1)
δ0δ2

σ′
δ0σδ1σ′
δ2σδ3

G(1) ,
(11.32)
and that they are manifestly of order one.3
Overall, the dNTK-preactivation cross
correlation (11.30) is of order 1/n1, vanishing in the strict inﬁnite-width limit n1 →∞.
These tensors (11.31) and (11.32) – and their deeper-layer siblings – control all the
leading ﬁnite-width correlation between the preactivations and the dNTK, and in fact
encapsulate the entire eﬀect of the dNTK at order 1/n. As we’ll show next, any other
dNTK-preactivation cross correlators, e.g. E

d
dH
(ℓ)
i0i1i2;δ0δ1δ2z(ℓ)
j3;δ3z(ℓ)
j4;δ4z(ℓ)
j5;δ5

, can always
be expressed in terms of a combination of P (ℓ) and Q(ℓ) at this order.
11.2.3
Deeper Layers: Growing dNTK
As before with §4.1 ∥§8.1 ∥§11.2.1 and §4.2 ∥§8.2 ∥§11.2.2, this section parallels our
other sections analyzing the RG ﬂow in deeper layers (§4.3 ∥§8.3).
To proceed forward, we’ll ﬁrst need to evaluate an interlayer formula with three
weight insertions (extending our work in §8.3.0), and then we’ll immediately put it to
use in order to obtain recursions for the dNTK-preactivation cross correlation.
Interlude 2: Interlayer Correlations Reloaded
Since the forward equation for the dNTK (11.24) has terms with one or three (ℓ+1)-th-
layer weight matrices, we’ll need interlayer formulae with one or three weight insertions.
3The cross-correlation tensor P (2)
δ0δ1δ2δ3 (11.31) – and more generally P (ℓ)
δ0δ1δ2δ3 in deeper layers – is
only symmetric under the exchange of its middle samples indices δ1 ↔δ2. Meanwhile, it’s manifestly
clear from (11.32) that the other cross-correlation tensor Q(2)
δ0δ1δ2δ3 has no symmetry whatsoever.
301

Let’s start by recalling our generating function for interlayer correlations (8.53):
E

O

z(ℓ+1)
e
P
i,j JijW (ℓ+1)
ij
Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
(11.33)
= exp

C(ℓ+1)
W
2nℓ
X
i,j
J 2
ij

E


**
O

z(ℓ+1)
i;δ
+ C(ℓ+1)
W
nℓ
nℓ
X
j=1
Jijσ(ℓ)
j;δ
++
b
G(ℓ+1)
Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
.
In this formula, O is a generic function of (ℓ+ 1)-th-layer preactivations only, and Q is
a function of any of our ℓ-th-layer objects; in particular, since the original derivation of
this formula didn’t depend on speciﬁcs of Q, we’ve also included the ℓ-th-layer dNTK
as part of its argument.
With that in mind, we ﬁrst wrote down an interlayer formula with one insertion as
(10.10) when analyzing (the lack of) representation learning in the inﬁnite-width limit.
To save you the need to ﬂip back and refresh your memory, we’ll reprint it here after
making some minor notational adjustments:
E

O

z(ℓ+1)
W (ℓ+1)
ij
Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
(11.34)
=C(ℓ+1)
W
nℓ
X
δ∈D
E


**
∂O
∂z(ℓ+1)
i;δ
++
b
G(ℓ+1)
σ(ℓ)
j;δ Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
.
This can be readily rederived by diﬀerentiating the generating function (11.33) with
respect to the source as
d
dJij once and then setting the source to zero.
In contrast, three-weight insertion formula will be new. Thrice-diﬀerentiating the
generating function (11.33) with respect to the source as
d
dJi0j0
d
dJi1j1
d
dJi2j2 and then
setting the source to zero J = 0, we ﬁnd
E

O

z(ℓ+1)
W (ℓ+1)
i0j0
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
(11.35)
=
 
C(ℓ+1)
W
nℓ
!2
δi0i1δj0j1
X
δ∈D
E


**
∂O
∂z(ℓ+1)
i2;δ
++
b
G(ℓ+1)
σ(ℓ)
j2;δ Q

z(ℓ), bH(ℓ), d
dH
(ℓ)

+
 
C(ℓ+1)
W
nℓ
!2
δi0i2δj0j2
X
δ∈D
E


**
∂O
∂z(ℓ+1)
i1;δ
++
b
G(ℓ+1)
σ(ℓ)
j1;δ Q

z(ℓ), bH(ℓ), d
dH
(ℓ)

+
 
C(ℓ+1)
W
nℓ
!2
δi1i2δj1j2
X
δ∈D
E


**
∂O
∂z(ℓ+1)
i0;δ
++
b
G(ℓ+1)
σ(ℓ)
j0;δ Q

z(ℓ), bH(ℓ), d
dH
(ℓ)

+
 
C(ℓ+1)
W
nℓ
!3
X
δ0,δ1,δ2∈D
E


**
∂3O
∂z(ℓ+1)
i0;δ0 ∂z(ℓ+1)
i1;δ1 ∂z(ℓ+1)
i2;δ2
++
b
G(ℓ+1)
σ(ℓ)
j0;δ0σ(ℓ)
j1;δ1σ(ℓ)
j2;δ2Q

z(ℓ), bH(ℓ), d
dH
(ℓ)
.
Intuitively, we can understand this formula as follows: each of the ﬁrst three terms
comes from forming one Wick contraction with two of the weight insertions and then
302

forming another contraction between the remaining weight and a weight hidden inside
the z(ℓ+1) in O, while the ﬁnal term comes from all three weight insertions each forming
a contraction with other weights inside the observable O.
dNTK-Preactivation Cross Correlations
Let’s ﬁrst use the interlayer formulae derived above to show that all of the dNTK’s
contributions to the statistics of the joint distribution p

z(ℓ), bH(ℓ), d
dH
(ℓ)D

at order
1/n are captured by the cross correlation of the dNTK with a single preactivation,
E

d
dH
(ℓ)
i0i1i2;δ0δ1δ2 z(ℓ+1)
i3;δ3

. To that end, we’ll examine a dNTK-preactivation cross corre-
lator of a very general form:
E

O

z(ℓ+1)
d
dH
(ℓ+1)
i0i1i2;δ0δ1δ2

(11.36)
=δi0i1
λ(ℓ+1)
W
nℓ
X
k0,k2
E
h
O

z(ℓ+1)
W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k2;δ0δ2
i
+ δi0i2
λ(ℓ+1)
W
nℓ
X
k0,k1
E
h
O

z(ℓ+1)
W (ℓ+1)
i1k1 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ(ℓ)
k0;δ2 bH(ℓ)
k0k1;δ0δ1
i
+
X
k0,k1,k2
E
h
O

z(ℓ+1)
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k1;δ0δ1 bH(ℓ)
k0k2;δ0δ2
i
+
X
k0,k1,k2
E

O

z(ℓ+1)
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 d
dH
(ℓ)
k0k1k2;δ0δ1δ2

.
Here, we took the expectation of the dNTK forward equation (11.24) multiplied by a
generic observable O

z(ℓ+1)
of (ℓ+1)-th-layer preactivations. We also ordered the four
terms to reﬂect the order in which we will subsequently evaluate them.
First, let’s simplify the ﬁrst two terms. Using our interlayer formula with one weight
insertion (11.34) on the ﬁrst term in (11.36), we get
λ(ℓ+1)
W
nℓ
δi0i1
X
k0,k2
E
h
O

z(ℓ+1)
W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k2;δ0δ2
i
(11.37)
=λ(ℓ+1)
W
C(ℓ+1)
W
n2
ℓ
X
k0,k2
X
δ∈D
δi0i1E


**
∂O
∂z(ℓ+1)
i2;δ
++
b
G(ℓ+1)
σ(ℓ)
k2;δσ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k2;δ0δ2


=λ(ℓ+1)
W
C(ℓ+1)
W
n2
ℓ
X
k,m
X
δ∈D
δi0i1
**
∂O
∂z(ℓ+1)
i2;δ
++
G(ℓ+1)
E
h
σ(ℓ)
k;δ1σ(ℓ)
m;δσ′ (ℓ)
k;δ0σ′ (ℓ)
m;δ2 bH(ℓ)
km;δ0δ2
i
+ O
 1
n2

= 1
nℓ
λ(ℓ+1)
W
C(ℓ+1)
W
X
δ∈D
δi0i1
**
∂O
∂z(ℓ+1)
i2;δ
++
G(ℓ+1)
F (ℓ+1)
δ1δ0δ3δ2 + O
 1
n2

,
303

where in the third line we used the Schwinger-Dyson formula (8.70) to expand the metric
ﬂuctuation around its mean G(ℓ+1), noting that the second term with the ﬂuctuating is
subleading, and in the last line we identiﬁed the remaining expectation as the deﬁnition
of the NTK-preactivation cross correlation tensor F (ℓ+1) from (8.69) up to constants.4
Similarly, for the second term in (11.36) we get an identical contribution up to the
swapping of neural-sample index pairs as (i1, δ1) ↔(i2, δ2).
Next, let’s tackle the third term in (11.36). Applying our interlayer formula with
three weight insertions (11.35), we get
X
k0,k1,k2
E
h
O

z(ℓ+1)
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k1;δ0δ1 bH(ℓ)
k0k2;δ0δ2
i
=

C(ℓ+1)
W
2
nℓ
δi0i1
X
δ∈D
**
∂O
∂z(ℓ+1)
i2;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E
h
σ(ℓ)
m;δσ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2 bH(ℓ)
kk;δ0δ1 bH(ℓ)
km;δ0δ2
i



+

C(ℓ+1)
W
2
nℓ
δi0i2
X
δ∈D
**
∂O
∂z(ℓ+1)
i1;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E
h
σ(ℓ)
m;δσ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ2σ′ (ℓ)
m;δ1 bH(ℓ)
kk;δ0δ2 bH(ℓ)
km;δ0δ1
i



+

C(ℓ+1)
W
2
nℓ
δi1i2
X
δ∈D
**
∂O
∂z(ℓ+1)
i0;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E
h
σ(ℓ)
m;δσ′′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2 bH(ℓ)
mk;δ0δ1 bH(ℓ)
mk;δ0δ2
i



+ O
 1
n2

,
(11.38)
where for each term we again used the Schwinger-Dyson formula (8.70) to expand the
metric ﬂuctuation around its mean G(ℓ+1), picking up the leading contribution from mean
metric, and then took the Gaussian expectation outside the full ℓ-th-layer expectation.
Note that the ﬁnal would-be term proportional to ∂3O is subleading:

C(ℓ+1)
W
3
X
δ,δ4,δ5∈D
**
∂3O
∂z(ℓ+1)
i0;δ ∂z(ℓ+1)
i1;δ4 ∂z(ℓ+1)
i2;δ5
++
G(ℓ+1)
(11.39)
× 1
n3
ℓ
X
k0,k1,k2
E
h
σ(ℓ)
k0;δσ(ℓ)
k1;δ4σ(ℓ)
k2;δ5σ′′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 bH(ℓ)
k0k1;δ0δ1 bH(ℓ)
k0k2;δ0δ2
i
= O
 1
n2

.
To see why, decompose each stochastic NTK into a mean and ﬂuctuation as bH(ℓ)
i1i2;δ1δ2 =
δi1i2H(ℓ)
δ1δ2 + d
∆H
(ℓ)
i1i2;δ1δ2 (8.58) and evaluate resulting four terms. In each case, you’ll ﬁnd
the terms are O
 1/n2 suppressed due to the Kronecker deltas constraining the triple
sum and/or the additional 1/n suppression coming from the ﬂuctuations.
Lastly, let’s process the fourth and ﬁnal term in our general cross correlation (11.36).
Again applying our interlayer formula with three weight insertions (11.35) and taking
4Note that this is an (ℓ+ 1)-th-layer quantity, rather than an ℓ-th-layer quantity as we typically have
on the right-hand side of recursions. If you prefer, you can use the F-recursion (8.79) to re-express it in
terms of a more complicated collection of ℓ-th-layer quantities.
304

only the leading-order pieces, we get
X
k0,k1,k2
E

O

z(ℓ+1)
W (ℓ+1)
i0k0 W (ℓ+1)
i1k1 W (ℓ+1)
i2k2 σ′ (ℓ)
k0;δ0σ′ (ℓ)
k1;δ1σ′ (ℓ)
k2;δ2 d
dH
(ℓ)
k0k1k2;δ0δ1δ2

=

C(ℓ+1)
W
2
nℓ
X
δ∈D
δi0i1
**
∂O
∂z(ℓ+1)
i2;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E

σ′ (ℓ)
k;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ d
dH
(ℓ)
kkm;δ0δ1δ2



+

C(ℓ+1)
W
2
nℓ
X
δ∈D
δi0i2
**
∂O
∂z(ℓ+1)
i1;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E

σ′ (ℓ)
k;δ0σ′ (ℓ)
m;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ d
dH
(ℓ)
kmk;δ0δ1δ2



+

C(ℓ+1)
W
2
nℓ
X
δ∈D
δi1i2
**
∂O
∂z(ℓ+1)
i0;δ
++
G(ℓ+1)



1
nℓ
X
k,m
E

σ′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ d
dH
(ℓ)
mkk;δ0δ1δ2



+ O
 1
n2

.
(11.40)
Note that the factors in the curly brackets are actually of order one since – as we saw in
the second layer and will recursively show next for general layers ℓ– the leading dNTK-
preactivation cross correlation is of order 1/n. Meanwhile, again the ﬁnal would-be term
proportional to ∂3O is subleading:

C(ℓ+1)
W
3
X
δ3,δ4,δ5∈D
**
∂3O
∂z(ℓ+1)
i0;δ3 ∂z(ℓ+1)
i1;δ4 ∂z(ℓ+1)
i2;δ5
++
K(ℓ+1)
(11.41)
× 1
n3
ℓ
X
k0,k1,k2
E

σ′ (ℓ)
k0;δ0σ(ℓ)
k0;δ3
 
σ′ (ℓ)
k1;δ1σ(ℓ)
k1;δ4
 
σ′ (ℓ)
k2;δ2σ(ℓ)
k2;δ5

d
dH
(ℓ)
k0k1k2;δ0δ1δ2

= O
 1
n2

.
To see why, note that the expectation is another dNTK-preactivation cross correlator
and thus is at most of order 1/n. Further, we only get such an order-1/n contribution
when two out of three neural indices k0, k1, k2 coincide: this should be clear from the
pattern of Kronecker deltas that arise when we evaluate such dNTK-preactivation cross
correlations in terms of our P-Q decomposition; cf. (11.30) for the second layer, or
look ahead a paragraph to (11.42) for deeper layers. This means that the sum over
all three neural indices will be restricted to be only two independent sums and, taking
the prefactor of 1/n3 into account, the overall contribution of this term will thus go as
∼(1/n3)(n2)(1/n) ∼1/n2.
Substituting all our evaluated contributions (11.37) (twice), (11.38), and (11.40) into
305

our expression for a general dNTK-preactivation cross correlator (11.36), we get
E

O

z(ℓ+1)
d
dH
(ℓ+1)
i0i1i2;δ0δ1δ2

(11.42)
= 1
nℓ
X
δ∈D
"
δi1i2
**
∂O
∂z(ℓ+1)
i0;δ
++
G(ℓ+1)
P (ℓ+1)
δ0δ1δ2δ
+ δi0i1
**
∂O
∂z(ℓ+1)
i2;δ
++
G(ℓ+1)
Q(ℓ+1)
δ0δ1δ2δ + δi0i2
**
∂O
∂z(ℓ+1)
i1;δ
++
G(ℓ+1)
Q(ℓ+1)
δ0δ2δ1δ
#
+ O
 1
n2

,
where we’ve introduced the (ℓ+ 1)-th-layer generalizations of the second-layer tensors
P (2) (11.31) and Q(2) (11.32):
P (ℓ+1)
δ0δ1δ2δ3 ≡

C(ℓ+1)
W
2 1
nℓ
nℓ
X
k,m=1
E
h
σ′′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ3 bH(ℓ)
mk;δ0δ1 bH(ℓ)
mk;δ0δ2
i
(11.43)
+

C(ℓ+1)
W
2 1
nℓ
nℓ
X
k,m=1
E

σ′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ3 d
dH
(ℓ)
mkk;δ0δ1δ2

+ O
 1
n

,
Q(ℓ+1)
δ0δ1δ2δ3 ≡

C(ℓ+1)
W
2 1
nℓ
nℓ
X
k,m=1
E
h
σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3 bH(ℓ)
kk;δ0δ1 bH(ℓ)
km;δ0δ2
i
+ λ(ℓ+1)
W
C(ℓ+1)
W
F (ℓ+1)
δ1δ0δ3δ2
+

C(ℓ+1)
W
2 1
nℓ
nℓ
X
k,m=1
E

σ′ (ℓ)
k;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3 d
dH
(ℓ)
kkm;δ0δ1δ2

+ O
 1
n

.
(11.44)
To see how these general expressions reduce to the ones we had for P (2)
δ0δ1δ2δ3 and Q(2)
δ0δ1δ2δ3
in the second layer, (11.31) and (11.32), recall that the ﬁrst-layer NTK is deterministic
and diagonal in neural indices bH(1)
i1i2;δ1δ2 = δi1i2H(1)
δ1δ2, that the ﬁrst-layer dNTK vanishes
d
dH
(1)
i0i1i2;δ0δ1δ2 = 0, and that in the second layer we had for the NTK-preactivation cross
correlation tensor F (2)
δ1δ0δ3δ2 =

C(2)
W
2 H(1)
δ0δ2
D
σδ1σδ3σ′
δ0σ′
δ2
E
G(1) (8.39).
Finally, note that by setting our observable to O = z(ℓ+1)
i3;δ3 , we get
E

d
dH
(ℓ+1)
i0i1i2;δ0δ1δ2z(ℓ+1)
i3;δ3

= 1
nℓ
h
δi0i3δi1i2P (ℓ+1)
δ0δ1δ2δ3 + δi0i1δi2i3Q(ℓ+1)
δ0δ1δ2δ3 + δi0i2δi1i3Q(ℓ+1)
δ0δ2δ1δ3
i
.
(11.45)
Importantly, this means that at leading non-vanishing order in 1/n, the tensors in the
decomposition of our elementary dNTK-preactivation cross correlator with a single pre-
activation (11.45) completely ﬁx the general dNTK-preactivation cross correlation with
more complicated observables (11.42).5
Thus, to completely incorporate the leading
5In other words, at our order in 1/n we can always replace the ℓ-th-layer dNTK inside any expectations
306

eﬀects of the dNTK in our analysis we only need to evaluate recursions for P (ℓ) and
Q(ℓ).6
P-recursion
To ﬁnd a recursion for P (ℓ), we need to evaluate the two expectations in (11.43).
For the ﬁrst expectation, making a decomposition of the NTK into a mean and
ﬂuctuation as bH(ℓ)
i1i2;δ1δ2 = δi1i2H(ℓ)
δ1δ2 + d
∆H
(ℓ)
i1i2;δ1δ2, we get
1
n2
ℓ
X
k,m
E
h
σ′′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ3 bH(ℓ)
mk;δ0δ1 bH(ℓ)
mk;δ0δ2
i
(11.47)
= 1
nℓ

σ′′
δ0σ′
δ1σ′
δ2σδ3

G(ℓ) H(ℓ)
δ0δ1H(ℓ)
δ0δ2 +
1
nℓ−1

σ′′
δ0σδ3

G(ℓ)

σ′
δ1σ′
δ2

G(ℓ) B(ℓ)
δ0δ0δ1δ2 + O
 1
n2

.
In particular, the cross terms consisting of an NTK mean and an NTK ﬂuctuation
dropped out because the Kronecker delta from the mean constrained the double sum
and the ﬂuctuation gave an additional 1/n suppression. If this explanation was a little
too fast, you should review our slower derivation of the B-recursion from (8.83) to
(8.89), which is identical in form to (11.47) above up to where the sample indices and
the derivatives go.
For the second expectation in (11.43), we can just apply our general cross-correlation
formula (11.42) for layer ℓ, letting us simplify it as
1
nℓ
nℓ
X
k,m=1
E

σ′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ3 d
dH
(ℓ)
mkk;δ0δ1δ2

(11.48)
=
1
nℓnℓ−1
nℓ
X
k,m=1
X
δ4∈D
" **
∂
∂z(ℓ)
m;δ4

σ′ (ℓ)
m;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
m;δ3
++
G(ℓ)
P (ℓ)
δ0δ1δ2δ4 + δmk × O(1)
#
+ O
 1
n

=
 nℓ
nℓ−1
 h
σ′′
δ0σδ3

G(ℓ)

σ′
δ1σ′
δ2

G(ℓ) P (ℓ)
δ0δ1δ2δ0 +

σ′
δ0σ′
δ3

K(ℓ)

σ′
δ1σ′
δ2

K(ℓ) P (ℓ)
δ0δ1δ2δ3
i
+ O
 1
n

.
Here in the second line, we’ve simply written the Q(ℓ) terms proportional to δmk as O(1);
the details of these terms do not matter because when we perform the double sum with
by the following diﬀerential operator
c
dH
(ℓ)
i0i1i2;δ0δ1δ2 →
1
nℓ−1
X
δ∈D
"
δi1i2P (ℓ)
δ0δ1δ2δ
∂
∂z(ℓ)
i0;δ
+ δi0i1Q(ℓ)
δ0δ1δ2δ
∂
∂z(ℓ)
i2;δ
+ δi0i2Q(ℓ)
δ0δ2δ1δ
∂
∂z(ℓ)
i1;δ
#
, (11.46)
with non-ﬂuctuating coeﬃcients P (ℓ) and Q(ℓ). When we use this replacement, we must remember that
the derivatives act on all of the ℓ-th-layer preactivations multiplying the dNTK; i.e. move the dNTK all
the way to the left side of the expectation before making such a replacement.
6Any preactivation-NTK-dNTK cross correlators such as E
h
d
∆H
(ℓ)
i1i2;δ1δ2 c
dH
(ℓ)
i3i4i5;δ3δ4δ5z(ℓ)
i6;δ6
i
and any
higher-order dNTK correlators such as E
h
c
dH
(ℓ)
i0i1i2;δ0δ1δ2 c
dH
(ℓ)
i3i4i5;δ3δ4δ5
i
are subleading. We won’t show
this explicitly, but you may ﬁnd additional tranquility in working it out for yourself; both examples are
relatively simple to work out for the second layer, L = 2.
307

the restriction m = k, they will be subleading. As for the term proportional to P (ℓ), the
diagonal contribution with k = m is similarly subleading; the leading contribution comes
from the (n2
ℓ−nℓ) oﬀ-diagonal pieces with k ̸= m, for which the derivative acts only on
two activations out of the four, and then we can further use Gaussian factorization to
write each term as a product of single-neuron Gaussian expectations.
Plugging these two simpliﬁed expectations back into our expression for P (ℓ+1) (11.43),
we get a recursion:
P (ℓ+1)
δ0δ1δ2δ3 =

C(ℓ+1)
W
2
σ′′
δ0σ′
δ1σ′
δ2σδ3

G(ℓ) H(ℓ)
δ0δ1H(ℓ)
δ0δ2
(11.49)
+
 nℓ
nℓ−1
 
C(ℓ+1)
W
2 
σ′
δ1σ′
δ2

G(ℓ)
×
"

σ′′
δ0σδ3

G(ℓ)P (ℓ)
δ0δ1δ2δ0+

σ′
δ0σ′
δ3

G(ℓ)P (ℓ)
δ0δ1δ2δ3+

σ′′
δ0σδ3

G(ℓ)B(ℓ)
δ0δ0δ1δ2
#
+O
 1
n

.
Interestingly, we see that this dNTK tensor P (ℓ) mixes with the NTK mean H(ℓ) as
well as the NTK-variance tensor B(ℓ). Since H(ℓ) and B(ℓ) are of order one, and since
P (1) = 0, this recursion shows that P (ℓ) will recursively stay of order one for all layers ℓ.
Q-recursion
To ﬁnd a recursion for Q(ℓ), we need to work out the two expectations in (11.44).
For the ﬁrst expectation, again making a decomposition of the NTK into a mean and
ﬂuctuation as bH(ℓ)
i1i2;δ1δ2 = δi1i2H(ℓ)
δ1δ2 + d
∆H
(ℓ)
i1i2;δ1δ2, we get
1
n2
ℓ
X
k,m
E
h
σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3 bH(ℓ)
kk;δ0δ1 bH(ℓ)
km;δ0δ2
i
(11.50)
= 1
n2
ℓ
X
k
E
h
σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2σ(ℓ)
k;δ3
i
H(ℓ)
δ0δ1H(ℓ)
δ0δ2 + 1
n2
ℓ
X
k
E

σ(ℓ)
k;δ3σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
k;δ2
d
∆H
(ℓ)
kk;δ0δ1

H(ℓ)
δ0δ2
+ 1
n2
ℓ
X
k,m
E

σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3
d
∆H
(ℓ)
km;δ0δ2

H(ℓ)
δ0δ1
+ 1
n2
ℓ
X
k,m
E

σ′′ (ℓ)
k;δ0 σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3
d
∆H
(ℓ)
kk;δ0δ1 d
∆H
(ℓ)
km;δ0δ2

= 1
nℓ

σ′′
δ0σ′
δ1σ′
δ2σδ3

G(ℓ) H(ℓ)
δ0δ1H(ℓ)
δ0δ2
+
1
nℓ−1
X
δ4,δ5,δ6,δ7
H(ℓ)
δ0δ1

zδ4σ′′
δ0σ′
δ1

G(ℓ)

zδ5σ′
δ2σδ3

G(ℓ) Gδ4δ6
(ℓ) Gδ5δ7
(ℓ) F (ℓ)
δ6δ0δ7δ2 + O
 1
n2

.
Here, to go from the second expression to the third expression, we had to evaluate four
expectations: the ﬁrst expectation gives a single-neuron Gaussian expectation at leading
order; the second expectation is subleading, cf. (8.71); the third expectation can also be
evaluated with that same general NTK-preactivation cross-correlation formula (8.71),
308

but in this case gives a leading term proportional to F (ℓ); and the ﬁnal expectation
vanishes due to the unpaired m neural index, cf. similar manipulations in (8.87) and
then the decomposition (8.82).
To simplify the second expectation in (11.44), we can again apply our general cross-
correlation formula (11.42) for layer ℓ:
1
nℓ
nℓ
X
k,m=1
E

σ′ (ℓ)
k;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3 d
dH
(ℓ)
kkm;δ0δ1δ2

(11.51)
=
1
nℓnℓ−1
nℓ
X
k,m=1
X
δ4∈D
" **
∂
∂z(ℓ)
m;δ4

σ′ (ℓ)
k;δ0σ′ (ℓ)
k;δ1σ′ (ℓ)
m;δ2σ(ℓ)
m;δ3
++
G(ℓ)
Q(ℓ)
δ0δ1δ2δ4
#
+ O
 1
n

=
 nℓ
nℓ−1
 h
σ′′
δ2σδ3

K(ℓ)

σ′
δ0σ′
δ1

G(ℓ) Q(ℓ)
δ0δ1δ2δ2 +

σ′
δ2σ′
δ3

G(ℓ)

σ′
δ1σ′
δ2

G(ℓ) Q(ℓ)
δ0δ1δ2δ3
i
+ O
 1
n

.
Here in the second line, this time we didn’t even write the terms proportional to δmk;
just as we saw before when working out the P-recursion, the restriction k = m for the
double sum will make such terms subleading. In the ﬁnal equality – again similarly to
the P-recursion – we kept the oﬀ-diagonal terms with k ̸= m, took the derivative, used
Gaussian factorization, and performed the double sum.
Plugging these two simpliﬁed expectations back into our expression for Q(ℓ+1) (11.44),
we get our ﬁnal recursion of the book:
Q(ℓ+1)
δ0δ1δ2δ3 =

C(ℓ+1)
W
2
σ′′
δ0σ′
δ1σ′
δ2σδ3

G(ℓ) H(ℓ)
δ0δ1H(ℓ)
δ0δ2 + λ(ℓ+1)
W
C(ℓ+1)
W
F (ℓ+1)
δ1δ0δ3δ2
+
 nℓ
nℓ−1

C(ℓ+1)
W
2
σ′
δ0σ′
δ1

G(ℓ)
h
σ′′
δ2σδ3

G(ℓ) Q(ℓ)
δ0δ1δ2δ2 +

σ′
δ2σ′
δ3

G(ℓ) Q(ℓ)
δ0δ1δ2δ3
i
+
 nℓ
nℓ−1

C(ℓ+1)
W
2H(ℓ)
δ0δ1
X
δ4,...,δ7∈D

zδ4σ′′
δ0σ′
δ1

G(ℓ)

zδ5σ′
δ2σδ3

G(ℓ)Gδ4δ6
(ℓ) Gδ5δ7
(ℓ) F (ℓ)
δ6δ0δ7δ2
+ O
 1
n

.
(11.52)
Interestingly, in this case we see that this dNTK tensor Q(ℓ) mixes with the NTK mean
H(ℓ) as well as the NTK-preactivation cross-correlation tensor F (ℓ).7 Again, since H(ℓ)
and F (ℓ) are both of order one, and since Q(1) = 0, this recursion shows that Q(ℓ) will
also recursively stay of order one for all layers ℓ.
7Also of possible interest, while the recursions for F (ℓ) (8.79) and B(ℓ) (8.89) were sourced by the NTK
mean H(ℓ), but otherwise didn’t mix with any ﬁnite-width tensors, the recursion for NTK-preactivation
cross-correlation tensor D(ℓ) (8.77) mixed with the four-point vertex V (ℓ), and the recursion for NTK-
variance tensor A(ℓ) (8.97) mixed with both V (ℓ) and D(ℓ). Thus, at least at this order, it seems like
the correlations and ﬂuctuations of the type F (ℓ) and B(ℓ) are potentially useful for the representation
learning that the dNTK induces, while V (ℓ), D(ℓ) and A(ℓ) may be more associated with instantiation-
to-instantiation ﬂuctuations.
309

In conclusion, together with the general dNTK-preactivation cross-correlation for-
mula (11.42), the P-Q recursions, (11.49) and (11.52), show that the leading dNTK-
preactivation cross correlation is 1/n-suppressed.
In other words, the eﬀects of the
dNTK are visible at ﬁnite width only.
11.3
Eﬀective Theory of the dNTK at Initialization
This section parallels our previous eﬀective theory work on preactivation statistics and
NTK-preactivation joint statistics (§5 ∥§9).
In particular, since we already know so
many diﬀerent reasons why criticality is essential, cf. §5, §6, §9, and §10, we’ll spend less
time on the disastrous consequences of not picking critical initialization hyperparameters
and more time on ﬁnding asymptotic solutions to the P- and Q-recursions at criticality.
As we did in our discussion of preactivation criticality in §5 and NTK criticality
in §9, throughout this section we’ll set the bias variance C(ℓ)
b
and the rescaled weight
variance C(ℓ)
W to be uniform across layers
C(ℓ)
b
= Cb ,
C(ℓ)
W = CW .
(11.53)
Further mirroring §5.4 and §9.1–§9.3, we’ll consider MLPs with uniform hidden layer
widths
n1 = . . . = nL−1 ≡n .
(11.54)
Finally, analogous to §9, we’re only going to focus on single-input statistics, leaving the
evaluation of the multi-input recursions as an adventure for thrill seekers.8
With these choices made, let’s write down the leading single-input recursions for P (ℓ)
and Q(ℓ). Dropping the sample indices and contributions that are subleading in 1/n, in
particular replacing the mean metric by the kernel G(ℓ) →K(ℓ) and the NTK mean by
the frozen NTK H(ℓ) →Θ(ℓ), the recursions (11.49) and (11.52) reduce to
P (ℓ+1) =C2
W

σ′′σ′σ′σ

K(ℓ)

Θ(ℓ)2 + CW χ(ℓ)
⊥

σ′′σ

K(ℓ) B(ℓ)
+

CW χ(ℓ)
⊥

σ′′σ

K(ℓ) +

χ(ℓ)
⊥
2
P (ℓ) ,
(11.55)
Q(ℓ+1) =C2
W

σ′′σ′σ′σ

K(ℓ)

Θ(ℓ)2 + λ(ℓ+1)
W
CW
F (ℓ+1) + 2h(ℓ)χ(ℓ)
∥Θ(ℓ)F (ℓ)
+

CW χ(ℓ)
⊥

σ′′σ

K(ℓ) +

χ(ℓ)
⊥
2
Q(ℓ) ,
(11.56)
with the initial conditions (cf. §11.2.1)
P (1) = Q(1) = 0 .
(11.57)
8You’ll have to generalize the γ[a] into a tensor product γ[a] ⊗γ[b] and then further decompose such
a basis according to the symmetries of the ﬁnite-width tensors you’ll want to expand.
310

To simplify these expressions, we have recalled our two susceptibilities, the parallel
susceptibility (5.50) and the perpendicular susceptibility (5.51), which are given by
χ(ℓ)
∥
≡CW
K(ℓ)

σ′σz

K(ℓ) ,
(11.58)
χ(ℓ)
⊥≡CW

σ′σ′
K(ℓ) ,
(11.59)
and we have also recalled our least favorite helper function (5.52),
h(ℓ) ≡1
2
d
dK(ℓ) χ(ℓ)
⊥= CW
2K(ℓ)

σ′′σ′z

K(ℓ) ,
(11.60)
though we’ve given a new expression for it on the right-hand side, obtained through
integration by parts, cf. (5.53).
To solve these recursions at criticality, we need to remember our scaling ansatz for
observables (5.93),
O(ℓ) =
1
ℓ
pO "
c0,0 + c1,1
log ℓ
ℓ

+ c1,0
1
ℓ

+ c2,2
 
log2 ℓ
ℓ2
!
+ . . .
#
.
(11.61)
Here, solving the dNTK recursions will yield new critical exponents pP and pQ that
describe the asymptotic depth scaling of P (ℓ) and Q(ℓ), respectively.
Additionally, in order to understand the relative size of the dNTK-preactivation cross
correlation, we will need to identify appropriate dimensionless quantities just as we did
before in §5.4 and §9.1. In this case, it will turn out that we should normalize by two
factors of the (frozen) NTK
P (ℓ)
n
 Θ(ℓ)2 ∼1
n
1
ℓ
pP −2pΘ
,
Q(ℓ)
n
 Θ(ℓ)2 ∼1
n
1
ℓ
pQ−2pΘ
,
(11.62)
where on the right-hand side pΘ is the critical exponent for the frozen NTK.
To see why these are the appropriate quantities to consider, recall our discussion
of dimensional analysis in footnote 15 of §1.3 and that our notation of [z] means “z
is measured in units of [z].” Looking at our second-order update for preactivations in
(11.9), and remembering that we can only add terms that have the same dimensions, we
must have
[z] = [η] [ϵ] [ bH] = [η]2 [ϵ]2 [d
dH] ,
(11.63)
from which it’s clear that [η] [ϵ] = [z] [ bH]−1, and subsequently we see that P and Q have
dimensions of NTK squared:
[P] = [Q] ≡[z] [d
dH] = [ bH]2 .
(11.64)
If this still seems a little counterintuitive, the utility of considering these particular ratios
(11.62) will become even more apparent when we analyze the stochastic prediction of a
fully-trained ﬁnite-width network in §∞.2.3.
311

After solving the P- and Q-recursions for both universality classes and looking at
these dimensionless quantities (11.62), we’ll again ﬁnd scaling laws that transcend uni-
versality class:
pP −2pΘ = −1 ,
pQ −2pΘ = −1 .
(11.65)
Speciﬁcally, we’ll see that these laws hold for both the scale-invariant and K⋆= 0
universality classes.9 Thus, we’ll be able to conclude that all the leading ﬁnite-width
eﬀects of the preactivation-NTK-dNTK joint distribution are relevant – in the sense of
RG ﬂow – controlled by the same ℓ/n perturbative cutoﬀ.
Now that we’re fully prepared for what we’re going to see, let’s actually solve the
dNTK recursions for our two important universality classes.
11.3.1
Scale-Invariant Universality Class
First, let’s recall some previous results that we’ll need in order to evaluate the recursions
(11.55) and (11.56).
For the scale-invariant universality class, we know from (5.62),
(9.33), and (9.37) that
χ(ℓ)
⊥= CW A2 ≡χ ,
h(ℓ) = 0 ,

σ′σ′σσ

K(ℓ) = A4K(ℓ) ,
(11.66)
where as a reminder A2 ≡(a2
++a2
−)/2 and A4 ≡(a4
++a4
−)/2, and the a± are the respec-
tive slopes of the positive/negative linear pieces of the activation function, cf. (5.59).
Next, we see that all the new Gaussian expectations in the recursions (11.55) and
(11.56) involve the second derivative of the activation. For these, we need to be somewhat
careful with nonlinear scale-invariant activation functions, since they have an undiﬀer-
entiable kink at the origin. (For linear activation functions, σ′′(z) = 0, and there’s no
subtlety.) For the ﬁrst of these new expectations, note that we can integrate it by parts
as

σ′′σ

K =
1
√
2πK
Z ∞
−∞
dze−z2
2K
 d
dz σ′

σ = 1
K

zσ′σ

K −

σ′σ′
K = 0 ,
(11.67)
where in the ﬁnal equality we used
1
K

zσ′σ

K =

σ′σ′
K = A2 .
(11.68)
Similarly, integrating the other new Gaussian expectation by parts, we ﬁnd

σ′′σ′σ′σ

K = 1
K

zσ′σ′σ′σ

K −2

σ′′σ′σ′σ

K −

σ′σ′σ′σ′
K .
(11.69)
Rearranging this, we easily see that

σ′′σ′σ′σ

K =
1
3K

zσ′σ′σ′σ

K −1
3

σ′σ′σ′σ′
K = 0 ,
(11.70)
9To be more speciﬁc, for the scale-invariant class, we’ll ﬁnd that P (ℓ) identically vanishes and that
Q(ℓ) solely determines the leading ﬁnite-width dNTK-preactivation cross correlation.
312

where in the ﬁnal equality we used
1
K

zσ′σ′σ′σ

K =

σ′σ′σ′σ′
K = A4 .
(11.71)
Thus, we can safely ignore both of these new expectations.
Substituting in all of (11.66) and ignoring ignorable expectations, our single-input
recursions (11.55) and (11.56) are extremely simple:
P (ℓ+1) =χ2P (ℓ) ,
(11.72)
Q(ℓ+1) =χ2Q(ℓ) + λ(ℓ+1)
W
CW
F (ℓ+1) .
(11.73)
In particular, since our initial condition (11.57) is P (1) = 0, we see immediately that
P (ℓ) vanishes identically for all layers:
P (ℓ) = 0 .
(11.74)
In contrast, for Q(ℓ) we see that the susceptibility χ is going to generically lead to
exponential behavior.
Now, let’s tune to scale-invariant criticality by setting the initialization hyperparam-
eters as Cb = 0 and CW = 1/A2. As a consequence, this ﬁxes the susceptibility to unity,
χ = 1, and leaves the kernel ﬁxed for all layers as K(ℓ) = K⋆. Additionally, let’s pick our
training hyperparameters according to the learning rate equivalence principle, for which
we’re instructed to choose layer-independent learning rates (9.94) as
λ(ℓ)
b
=
eλb
L ,
λ(ℓ)
W =
eλW
L .
(11.75)
Finally, with these hyperparameter choices, the single-input solution for the frozen NTK
(9.44) and the single-input solution for the NTK-preactivation cross correlation F (ℓ)
(9.50) are given by
Θ(ℓ) =

eλb + eλW A2K⋆ ℓ
L ,
(11.76)
F (ℓ) = ℓ(ℓ−1)
2L
A4
A2
2

eλb + eλW A2K⋆
K⋆

.
(11.77)
Plugging in the critical initialization hyperparameters, the ﬁxed kernel, the learning
rates (11.75), and the expression for F (ℓ) (11.77), the Q-recursion (11.73) becomes
Q(ℓ+1) = Q(ℓ) + ℓ(ℓ+ 1)
2L2
A4
A2

eλb + eλW A2K⋆
eλW K⋆

.
(11.78)
This simple recursion is exactly solved by
Q(ℓ) = ℓ(ℓ2 −1)
6L2
A4
A2

eλb + eλW A2K⋆
eλW K⋆

,
(11.79)
313

which satisﬁes the initial condition Q(1) = 0.
With this solution, we can identify the critical exponent associated with the large-ℓ
behavior of Q(ℓ): pQ = −3. Further, we see that our dimensionless quantity (11.62) will
satisfy the scaling law (11.65) as promised,
pQ −2pΘ = −1 ,
(11.80)
where we have also used pΘ = −1 from the scale-invariant frozen NTK solution reprinted
above in (11.76). More speciﬁcally, substituting in the solution for Θ(ℓ) (11.76) and the
solution for Q(ℓ) (11.79) into the dimensionless ratio (11.62), we ﬁnd
Q(ℓ)
n
 Θ(ℓ)2 = A4
6A2
"
eλW K⋆
eλb + eλW A2K⋆
#
ℓ
n + . . . .
(11.81)
Thus, we have veriﬁed the ℓ/n scaling of the leading dNTK-preactivation cross correla-
tion for scale-invariant activation functions.
11.3.2
K⋆= 0 Universality Class
For the K⋆= 0 universality class, we’ll begin again by recalling some previous results.
First, we know from (5.84), (5.85), (9.64), and (11.60) the following:
χ∥(K) =

CW σ2
1
 h
1 + 2a1K + O

K2i
,
(11.82)
χ⊥(K) =

CW σ2
1
 h
1 + b1K + O

K2i
,
(11.83)
C2
W

σ′σ′σσ

K =

CW σ2
1
2 h
K + O|1

K2i
,
(11.84)
h(K) =1
2
d
dK χ⊥(K) =

CW σ2
1
 b1
2 + O

K1
.
(11.85)
To interpret these results, remember that we Taylor expanded the activation function
as
σ(z) =
∞
X
p=0
σp
p! zp ,
(11.86)
deﬁned the following combination of Taylor coeﬃcients for convenience
a1 ≡
σ3
σ1

+ 3
4
σ2
σ1
2
,
(11.87)
b1 ≡
σ3
σ1

+
σ2
σ1
2
,
(11.88)
and required that all activation functions in this class satisfy σ0 = 0 and σ1 ̸= 0.
Then, making analogous Taylor expansions and performing Gaussian integrations order
314

by order, we can evaluate the new Gaussian expectations in the recursions (11.55) and
(11.56) as
CW

σ′′σ

K =

CW σ2
1
 h
(2a1 −b1)K + O

K2i
,
(11.89)
C2
W

σ′′σ′σ′σ

K =

CW σ2
1
2 h
(−6a1 + 7b1)K + O

K2i
.
(11.90)
Now, let’s jump right into K⋆= 0 criticality (5.90) by tuning the initialization
hyperparameters as Cb = 0 and CW = 1/σ2
1. At the same time, let’s also tune our
training hyperparameters according to the learning rate equivalence principle, which for
K⋆= 0 activation functions is given by (9.95),
λ(ℓ)
b
= eλb
1
ℓ
p⊥
Lp⊥−1 ,
λ(ℓ)
W = eλW
L
ℓ
p⊥−1
,
(11.91)
where the critical exponent for perpendicular perturbations is deﬁned as p⊥≡b1/a1.
With these hyperparameter settings, let’s also record the other single-input solutions
that we need for the recursions, the kernel K(ℓ) (5.92), the frozen NTK Θ(ℓ) (9.71), the
NTK-preactivation cross correlation F (ℓ) (9.81), and the NTK variance B(ℓ) (9.82):
K(ℓ) =

1
(−a1)
 1
ℓ+ . . . ,
(11.92)
Θ(ℓ) =
"
eλb +
eλW σ2
1
(−a1)
# L
ℓ
p⊥−1
+ . . . ,
(11.93)
F (ℓ) =
1
(5 −p⊥)

1
(−a1)
 "
eλb +
eλW σ2
1
(−a1)
# L
ℓ
p⊥−1
+ . . . ,
(11.94)
B(ℓ) =L2p⊥−2
3
"
eλb +
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−3
+ . . . .
(11.95)
Finally, plugging all our collected results and tunings (11.82)–(11.85) and (11.89)–(11.95)
into the P-recursion (11.55) and the Q-recursion (11.56), we get
P (ℓ+1) =

1 −(p⊥+ 2)
ℓ
+ . . .

P (ℓ) + (p⊥−2)
3
"
eλb +
eλW σ2
1
(−a1)
#2 L
ℓ
2p⊥−2
+ . . . ,
(11.96)
Q(ℓ+1) =

1 −(p⊥+ 2)
ℓ
+ . . .

Q(ℓ)
(11.97)
+
1
(5 −p⊥)
"
(1 −p⊥)
eλW σ2
1
(−a1) −p⊥eλb
# "
eλb +
eλW σ2
1
(−a1)
# L
ℓ
2p⊥−2
+ . . . .
Let’s tackle the P-recursion ﬁrst. Plugging our scaling ansatz (11.61) into the recur-
sion (11.96) and matching the terms, we ﬁnd an asymptotic solution
P (ℓ) = −L2p⊥−2
3
2 −p⊥
5 −p⊥
 "
eλb +
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−3
+ . . . .
(11.98)
315

Thus, the critical exponent for this dNTK-preactivation cross correlation is pP = 2p⊥−3,
and we obtain our promised scaling law (11.65)
pP −2pΘ = −1 ,
(11.99)
after substituting in pΘ = p⊥−1 from the K⋆= 0 frozen NTK solution (11.93). More
speciﬁcally, substituting in the solution for Θ(ℓ) (11.93) and the solution for P (ℓ) (11.98)
into the dimensionless ratio (11.62), we get
P (ℓ)
n
 Θ(ℓ)2 = −1
3
2 −p⊥
5 −p⊥
 ℓ
n + . . . ,
(11.100)
which (i) scales as ℓ/n, (ii) is independent of the training hyperparameters, and (iii) is
manifestly negative, given p⊥≤1.10
Similarly for the Q-recursion (11.97), plugging our scaling ansatz (11.61) into the
recursion (11.97) and matching the terms one ﬁnal time, we ﬁnd
Q(ℓ) =
L2p⊥−2
(5 −p⊥)2
"
1 −p⊥−
eλb
eλb + eλW σ2
1/(−a1)
# "
eλb +
eλW σ2
1
(−a1)
#2 1
ℓ
2p⊥−3
.
(11.101)
This gives a critical exponent of pQ = 2p⊥−3 and a dimensionless ratio of
Q(ℓ)
n
 Θ(ℓ)2 =
1
(5 −p⊥)2
"
1 −p⊥−
eλb
eλb + eλW σ2
1/(−a1)
#
ℓ
n + . . . .
(11.102)
Thus, we’ve now fully veriﬁed our scaling law (11.65):
pQ −2pΘ = −1 .
(11.103)
♦♦♦♦
In conclusion, all the nonzero dimensionless dNTK-preactivation cross correlations
will grow as ℓ/n at leading order in the ﬁnite-width expansion. As the dNTK leads to
a dynamical NTK, this is suﬃcient to see that deep ﬁnite-width networks are represen-
tation learners.
In the next section, we’re going to set aside our direct investigation of deep MLPs and
instead focus on a pedagogical model of representation learning that directly incorporates
the eﬀect of a nonzero dNTK. In the following chapter, we’ll return to our ﬁnite-width
networks and complete our goal of computing the eﬀective distribution over wide and
deep ﬁnite-width MLPs after training. The simpliﬁed model presented in the next section
will make it easier to understand the results of our later analysis of such fully-trained
networks.
10To recall why p⊥≤1, without ﬂipping back to footnote 33 of §10, (i) remember that we must have
a1 < 0 in order for the kernel K(ℓ) to stay positive when asymptotically approaching the K⋆= 0 ﬁxed
point, cf. (11.92), (ii) note to yourself that b1 ≥a1, cf. (11.87) and (11.88), and (iii) realize that a1 < 0
and b1 ≥a1 together imply that p⊥≡b1/a1 ≤1.
316

11.4
Nonlinear Models and Nearly-Kernel Methods
In §10.4, we placed inﬁnite-width networks into a broader context of machine learning
models. There, we discussed how the inﬁnite-width network can be understood as either
a linear model with ﬁxed random features or, dually, as a kernel method with the kernel
given by the frozen NTK. Now we are ready to ﬁnd a broader context for ﬁnite-width
networks.
In this chapter, we’ve seen that the statistics needed to compute the dynamics of
ﬁnite-width networks (§11.2 and §11.3) are far more complicated than for inﬁnite-width
networks, nontrivially incorporating the second derivative of the network function. This
additional complexity is irreducibly encapsulated by a single object: the dNTK (§11.1).
The dNTK enables the NTK to evolve during training, leading to nontrivial representa-
tion learning from the training data.
The goal of this section is to abstract this property away from the deep learning
framework and distill it into a minimal model of representation learning that cap-
tures all of its important features. Such a model provides a framework for studying the
type of representation learning exhibited by deep neural networks, but more succinctly
and more broadly. In other words, we hope that this endeavor will extend the standard
toolbox of machine learning.
First in §11.4.1, we’ll extend linear models discussed in §10.4.1 to nonlinear models.
Then in §11.4.2, we’ll give a dual description of these nonlinear models, nearly-kernel
methods, which will extend the standard kernel methods that we discussed in §10.4.2.
Finally in §11.4.3, as we did analogously for inﬁnite-width networks before in §10.4.3,
we’ll see how ﬁnite-width networks can be understood in terms of this new framework.
11.4.1
Nonlinear Models
As a reminder from §10.4.1, a linear model is given by (10.119)
zi(xδ; θ) =
nf
X
j=0
Wijφj(xδ) ,
(11.104)
where the model parameters are given by the weight matrix θ = Wij, the model’s
features are given by the feature function φj(x), and we again adopt the old-school
convention for incorporating biases, including a constant feature φ0(x) ≡1 so that the
bias vector is given by Wi0 ≡bi. Note that since we’re not discussing neural networks in
particular, there’s no neural indices or layer indices here. Instead, in this equation, δ is
a sample index, running over the ND samples in the dataset δ ∈D; i is a vectorial index,
running over the nout vectorial component of the model output zi; and j is a feature
index, running over (nf + 1) diﬀerent features. In this setup, a feature function φj(x)
is computed on an input sample x, and the weight matrix Wij determines the eﬀect of
the j-th feature on the i-th component of the output. Traditionally, feature functions
φj(x) are often designed such that the linear model works well for the desired task after
optimization or linear regression.
317

To go beyond this linear paradigm, let’s slightly deform it to get a nonlinear model:
zi(xδ; θ) =
nf
X
j=0
Wijφj(xδ) + ϵ
2
nf
X
j1,j2=0
Wij1Wij2ψj1j2(xδ) + . . . .
(11.105)
Here, ϵ ≪1 is small parameter that controls the size of the deformation, and more
importantly, we’ve introduced another set of feature functions, ψj1j2(x), with two feature
indices, making the model nonlinear in its weights. By deﬁnition, ψj1j2(x) is symmetric
under the exchange of the feature indices j1 ↔j2, and the factor of 1/2 is there because
of the double counting of the double sum. For reasons that will be made clear shortly,
we will call each component of ψj1j2(x) a meta feature function, and so there are
(nf + 1)(nf + 2)/2 diﬀerent meta feature functions.
To familiarize ourselves with the features of this model, let’s see how the model
outputs change given a small change in the model parameters Wij →Wij + d¯Wij:
zi(xδ; θ + d¯θ)
=zi(xδ; θ) +
nf
X
j=0
d¯Wij

φj(xδ) + ϵ
nf
X
j1=0
Wij1ψj1j(xδ)

+ ϵ
2
nf
X
j1,j2=0
d¯Wij1d¯Wij2ψj1j2(xδ) + . . .
=zi(xδ; θ) +
nf
X
j=0
d¯Wij φE
ij(xδ; θ) + ϵ
2
nf
X
j1,j2=0
d¯Wij1d¯Wij2ψj1j2(xδ) + . . . ,
(11.106)
where in the last line we summarized the quantity in the square bracket in terms of an
eﬀective feature function:
φE
ij(xδ; θ) ≡dzi(xδ; θ)
dWij
= φj(xδ) + ϵ
nf
X
k=0
Wikψkj(xδ) .
(11.107)
The utility of this is as follows: the linear response of the model to changing its param-
eters is as if it eﬀectively has a feature function, φE
ij(xδ; θ), which itself depends on the
value of the model parameters. Moreover, the change in the eﬀective feature function
given a small change in the model parameters Wik →Wik + d¯Wik is given by
φE
ij(xδ; θ + d¯θ) = φE
ij(xδ; θ) + ϵ
nf
X
k=0
d¯Wik ψkj(xδ) .
(11.108)
Thus, the eﬀective features φE
ij(xδ; θ) are learnable, evolving as a linear model, and for
these eﬀective features, the meta features ψkj(xδ) play the role usually played by the
features.11 In this way, we can think of our nonlinear model as having a hierarchical
structure, where the features evolve as if they are described by a linear model according
11This role of the meta features ψkj(xδ) in the linear model for the eﬀective features explains our
choice of name. Note also that in this setup, we assume that both the feature function φj(x) and the
meta feature function ψj1j2(x) are picked by the model designer, just as the feature function was before
for the linear model.
318

to (11.108), while the model’s output evolves in a more complicated nonlinear way
according to (11.106).
Note that we could extend this hierarchical structure further, e.g. by further de-
forming our nonlinear model (11.105) with a term that’s cubic in the parameters and
includes a three-indexed meta-meta feature function that analogously allows the meta
feature functions to learn.12 However, as the quadratic term already suﬃces to provide
a minimal model of representation learning – just as a nonzero dNTK suﬃced to induce
nontrivial representation learning in the MLP – from here on, we’ll explicitly truncate
the model (11.105) at the quadratic order, giving us a quadratic model.13
To understand how representation learning works in this model, we should ﬁnd the
optimal values for the weights W ⋆
ij given a training set A. Mirroring what we did before
for linear models, let’s minimize the MSE loss, which for our quadratic model is given
by
LA(θ) = 1
2
X
˜α∈A
nout
X
i=1
h
yi;˜α −zi(x˜α; θ)
i2
(11.109)
= 1
2
X
˜α∈A
nout
X
i=1

yi;˜α −
nf
X
j=0
Wij φj(x˜α) −ϵ
2
nf
X
j1,j2=0
Wij1Wij2ψj1j2(x˜α)


2
.
Supervised learning with such a quadratic model (11.105) doesn’t have a particular name,
but if it did, we’d all probably agree that its name should be quadratic regression.
However, unlike linear regression – where the MSE loss (10.120) was quadratic in the
model parameters – quadratic regression – where the loss is now quartic in the parameters
– does not in general yield analytical solutions. Nevertheless, we can perturbatively ﬁnd
a solution to the quadratic regression problem by expanding in our small parameter ϵ,
for which the regression is nearly linear.
Nearly-Linear Quadratic Regression
Taking the derivative of the loss LA (11.109) with respect to the model parameter Wij0
and setting it to zero, we ﬁnd
0 =
X
˜α
φE
ij0(x˜α; θ⋆) [zi(x˜α; θ⋆) −yi;˜α] ,
(11.110)
making clear that the eﬀective features (11.107) give the linear response of the model.
Next, substituting in for the quadratic model (11.104) and the eﬀective feature function
12Such deformations are essentially equivalent to incorporating further terms in the Taylor expansion
of an arbitrary general model function zi(xδ; θ), where the linear model is the base model, and the
nonlinear model (11.105) gives the ﬁrst improvement from the expansion.
13To leading order in the 1/n expansion, ﬁnite-width networks cannot self-consistently be described by
a truncated quadratic model; instead, they fall into a class of cubic models, with evolving meta feature
functions. This is one of the ways in which such ﬁnite-width networks are non-minimal representation
learners and is the main reason for discussing quadratic models ﬁrst before moving on to the more
complicated analysis of ﬁnite-width networks with their dynamical dNTKs.
319

(11.107) and rearranging we ﬁnd
X
˜α∈A



nf
X
j1=0
W ⋆
ij1φj1(x˜α)φj0(x˜α) + ϵ
nf
X
j1,j2=0
W ⋆
ij1W ⋆
ij2

φj1(x˜α)ψj2j0(x˜α)+ 1
2ψj1j2(x˜α)φj0(x˜α)



=
X
˜α∈A
yi;˜α

φj0(x˜α) + ϵ
nf
X
j1=0
W ⋆
ij1ψj1j0(x˜α)

+ O

ϵ2
.
(11.111)
This expression contains the two terms we found for linear regression in (10.121) as well
as three additional terms proportional to the meta features ψj1j2(x). Additionally, we
truncated the O
 ϵ2 term since ϵ is assumed to be parametrically small. Importantly,
we see clearly that the equation is overall nonlinear, with the two terms quadratic in
the weights. In the language of physics, the linear equation of linear regression is free
and exactly solvable, while the nonlinear equation of quadratic regression is interacting.
Since ϵ multiplies all the new terms associated with quadratic regression, the nonlinear
terms are all small, and so (11.111) exhibits weakly-interacting dynamics. This means
that we can systematically solve this nonlinear equation via perturbation theory.
With that in mind, let’s decompose the optimal weight matrix into a free linear part
and an interacting nonlinear part as
W ⋆
ij ≡W F
ij + W I
ij ,
(11.112)
with the idea being that the free part W F
ij will solve the free linear regression equation
(10.121), while the interacting part W I
ij will solve the remaining linearized equation after
substituting back in the solution for W F
ij. Given that the quadratic regression problem
(11.111) becomes a linear regression problem (10.121) in the limit of ϵ →0, we naturally
expect that the interacting part of the optimal weights should be proportional to the
small parameter W I
ij = O(ϵ).
Let’s quickly review our direct optimization solution to linear regression from
§10.4.1 in the current context: deﬁning an (nf +1)-by-(nf +1) symmetric matrix (10.122)
Mj1j2 ≡
X
˜α∈A
φj1(x˜α) φj2(x˜α) ,
(11.113)
the linear part of the quadratic regression problem (11.111) can be written as
nf
X
j1=0
W F
ij1Mj1j0 =
X
˜α∈A
yi;˜αφj0(x˜α) ,
(11.114)
which can be solved by the multiplication of the inverse (M−1)j0j,
W F
ij =
nf
X
j0=0
X
˜α∈A
yi;˜αφj0(x˜α)

M−1
j0j .
(11.115)
Recall that the inverse (M−1)j0j will not uniquely exist if we’re in the overparameterized
regime with more features than training examples, (nf + 1) > NA, but we can use our
320

regularization trick (10.124) to pick out a particular inverse. Going forward, we will
assume that we’re in this overparameterized regime and that the inverse was picked in
this way.
Next, plugging in our decomposition (11.112) into our equation (11.111) and collect-
ing the terms of order ϵ, remembering also that W I
ij = O(ϵ), we ﬁnd for our linearized
interacting dynamics,
nf
X
j1=0
W I
ij1Mj1j0 =ϵ
nf
X
j1=0
W F
ij1
X
˜α∈A
yi;˜αψj1j0(x˜α) −ϵ
nf
X
j1,j2=0
W F
ij1W F
ij2
X
˜α∈A
φj1(x˜α)ψj2j0(x˜α)
−ϵ
2
nf
X
j1,j2=0
W F
ij1W F
ij2
X
˜α∈A
φj0(x˜α)ψj1j2(x˜α) + O

ϵ2
.
(11.116)
Here on the right-hand side, the ﬁrst two terms actually cancel each other, since the free
solution satisﬁes
nf
X
j=0
W F
ijφj(x˜α) = yi;˜α ,
(11.117)
i.e. for overparameterized models, linear part of the optimized model can correctly pre-
dict all the training-set examples.14 After making that cancellation, we can multiply by
the inverse (M−1)j0j to ﬁnd a solution:
W I
ij = −ϵ
2
nf
X
j1,j2,j3=0
W F
ij1W F
ij2
X
˜α∈A
[ψj1j2(x˜α)φj3(x˜α)]

M−1
j3j + O

ϵ2
.
(11.118)
In particular, the free solution, (11.115), and the interacting solution, (11.118), together
solve the nonlinear optimization problem (11.111) to order ϵ.15
Finally, having obtained the solution, we can throw away the training data and
simply store the optimal parameters W ⋆
ij = W F
ij + W I
ij, making predictions on novel test
inputs x ˙β as
zi(x ˙β; θ⋆) =
nf
X
j=0
W ⋆
ijφj(x ˙β) + ϵ
2
nf
X
j1,j2=0
W ⋆
ij1W ⋆
ij2ψj1j2(x ˙β)
(11.119)
=
nf
X
j=0
W F
ijφj(x ˙β) +
nf
X
j=0
W I
ijφj(x ˙β) + ϵ
2
nf
X
j1,j2=0
W F
ij1W F
ij2ψj1j2(x ˙β) + O

ϵ2
=1
2
nf
X
j=0
W ⋆
ij
h
φj(x ˙β) + φE
ij(x ˙β; θ⋆)
i
+ O

ϵ2
.
14This is shown in detail in §10.4.2. Note that for underparametrized models, the solution can still be
analyzed, but the details will be diﬀerent. We’re focusing on overparameterized models here since (i)
deep learning models are typically overparameterized and (ii) we suspect that the sort of representation
learning our model exhibits is most useful in that regime. We’ll elaborate on this quite a bit more in
Epilogue ε.
15When doing nearly-linear quadratic regression practically, it would probably make the most sense to
ﬁrst ﬁnd the optimal linear parameters W F
ij and then plug them back into (11.118) to ﬁnd the additional
nonlinear parameters W I
ij.
321

Here, we’ve given two diﬀerent ways to think about the optimal output. On the ﬁrst and
second lines, we simply have the prediction of the quadratic model (11.105) expressed
in terms of the ﬁxed features φj(x ˙β) and the ﬁxed meta features ψj1j2(x ˙β). This pre-
sentation makes the nonlinearity manifest. After regrouping the terms and using our
deﬁnition (11.107), in the last line we instead wrote the model prediction in the form
of a linear model, where we see that features in this interpretation are the mean of the
ﬁxed unlearned features and the learned eﬀective features. From this perspective, repre-
sentation learning is manifest: the eﬀective features φE
ij(x ˙β; θ⋆) depend on the training
data through the optimal parameters, W ⋆
ij = W ⋆
ij(x˜α, y˜α).
In summary, our nonlinear quadratic model (11.105) serves as a minimal model of
representation learning. As we will see soon, this captures the mechanism of feature
evolution for a nonzero but ﬁxed dNTK.
Aside: Model Comparison of Linear Regression and Quadratic Regression
Before we move on to the dual sample-space description of the quadratic model, let’s
brieﬂy perform a model comparison between linear regression and quadratic regression.
In particular, let’s think about the model complexity of these classes of models.16
For both linear and quadratic regression, the number of model parameters is given
by the number of elements in the combined weight matrix Wij:
P ≡nout × (nf + 1) .
(11.120)
Since both models completely memorize the same training set for a ﬁxed and equal
number of parameters, we obviously cannot naively use the Occam’s razor heuristic
(§6.2.2) for model comparison. This makes our model comparison somewhat subtle.
On the one hand, there is a sense in which the quadratic model is more complicated,
as it computes far more functions of the input per parameter. Speciﬁcally, on a per
parameter basis, we need to specify a far greater number of underlying functions for the
quadratic model than we do for the linear model: i.e. we need
(nf + 1) +
1
2(nf + 1)(nf + 2)

= O

P 2
(11.121)
numbers to specify φj(x) and ψj1j2(x), while we need just
(nf + 1) = O(P)
(11.122)
numbers to specify φj(x). In particular, the counting of the model functions is dominated
by the meta feature functions ψj1j2(x). As such, this type of complexity is not really
captured by the counting of model parameters, P; instead, it is expressed in the structure
of the model, with the addition of the meta feature functions ψj1j2(x).
16For a further discussion of model complexity, with a direct focus on overparameterized deep learning
models, see Epilogue ε.
322

On the other hand, we can interpret these additional meta feature functions as con-
straining the quadratic model according to an explicit inductive bias for representa-
tion learning.17 In particular, this additional structure alters the linear model solution
(11.115) with the addition of O(ϵ) tunings W I
ij, constrained by the O
 P 2 meta features
that are deﬁned before any learning takes place. Assuming these meta feature functions
are useful, we might expect that the quadratic model will overﬁt less and generalize
better.18 (In fact, that was the whole point of introducing them.)
This latter point is worth a little further discussion. One typical signature of overﬁt-
ting is that the parameters are extremely ﬁnely-tuned; these tunings are in some sense
unnatural as they can arise from the extreme ﬂexibility aﬀorded to overparameterized
models, enabling models to pass through all the training points exactly, to the extreme
detriment of the test predictions.19 Adding a regularization term on the parameter norm
– i.e. the one we just discussed in footnote 17 – combats such tuning: the additional
constraints on the optimization problem drive the norm of the parameters towards zero,
eﬀectively promoting parameter sparsity. Here, we see that since the nonlinear contribu-
tion to the optimal weights, W I
ij, is ﬁxed to be small, O(ϵ), it’s adding constraints that
– if they’re useful – can combat any ﬁne tunings that may appear in the linear solution,
W F
ij, and lead to better generalization.
11.4.2
Nearly-Kernel Methods
Now that we have some more parameter-space intuition for the potential advantages of
nonlinear models over linear models, let’s now develop a dual sample-space description
17Similarly, we could naively think that the addition of a regularization term such as PP
µ=1 aµθ2
µ to
the loss as making a model more complex with its extra structure, despite being a well-known remedy
for overﬁtting. Instead, it’s probably better to think of this regularization term as an inductive bias for
constraining the norm squared of the optimized model parameters.
18Interestingly, for the quadratic model, the number of eﬀective feature functions (11.107), is actually
the same as the number of model parameters: nout ×(nf +1) = P. Since it’s only through these eﬀective
features that the meta feature functions enter the model predictions, cf. (11.119), this further underscores
that, despite the additional model structure, there aren’t actually O P 2
independent degrees of freedom
that can be applied towards ﬁtting the training data.
19This is very commonly illustrated by using polynomial basis of feature functions for linear regression,
which is sometimes called polynomial regression. In this case, consider a linear model of a scalar function
f(x) with a scalar input x:
z(xδ; θ) =
nf
X
j=0
wjφj(xδ) ≡w0 + w1xδ + w2x2
δ + · · · + wnf x
nf
δ
.
(11.123)
If there’s any noise at all in the data, when the model is overparameterized, nf + 1 > NA, the plot of
this one-dimensional function will make ∼nf wild turns to go through the NA training points. (This is
particularly evocative if the target function is a simple linear function with noise, i.e. f(x) = ax + b + ε,
with ε a zero-mean Gaussian noise with small variance σ2
ε ≪1.) In order to make these turns, the
optimal coeﬃcients, w⋆
j , computed by (11.115), will be ﬁnely-tuned to many signiﬁcant ﬁgures. This
kind of ﬁne-tuning problem in model parameters is indicative of the model being unnatural or wrong; in
fact, the analog of this problem in high-energy theoretical physics is called naturalness (see e.g. [68] for
a non-technical discussion).
323

of quadratic regression where a quadratic-model analog of the dNTK appears naturally.
Starting with the expression in the second line of the prediction formula (11.119)
and plugging in the free solution (11.115) and the interacting solution (11.118), we get
zi(x ˙β; θ⋆) =
X
˜α∈A
yi;˜α


nf
X
j1,j2=0
φj1(x˜α)

M−1
j1j2 φj2(x ˙β)


(11.124)
+ ϵ
2
X
˜α1,˜α2∈A
yi;˜α1yi;˜α2
nf
X
j1,j2,j3,j4=0
φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4
×

ψj3j4(x ˙β) −
X
˜α∈A
ψj3j4(x˜α)
nf
X
j5,j6=0
φj5(x˜α)

M−1
j5j6φj6(x ˙β)

+ O

ϵ2
.
To simplify this expression, recall formula (10.131) that we derived when discussing
kernel methods,
nf
X
j1,j2=0
φj1(x˜α)

M−1
j1j2 φj2(x ˙β) = k ˙β ˜α1ek˜α1 ˜α ,
(11.125)
where the kernel was deﬁned in (10.127) as
kδ1δ2 ≡k(xδ1, xδ2) ≡
nf
X
j=0
φj(xδ1) φj(xδ2) ,
(11.126)
and provided a measure of similarity between two inputs xi;δ1 and xi;δ2 in feature space.
Plugging this formula (11.125) back into our quadratic regression prediction formula
(11.124), we get
zi(x ˙β; θ⋆) =
X
˜α1,˜α2∈A
k ˙β ˜α1ek˜α1 ˜α2yi;˜α2
(11.127)
+ ϵ
2
X
˜α1,˜α2∈A
yi;˜α1yi;˜α2
nf
X
j1,j2,j3,j4=0
φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4
×

ψj3j4(x ˙β) −
X
˜α1,˜α2∈A
k ˙β ˜α1ek˜α1 ˜α2ψj3j4(x˜α2)

+ O

ϵ2
,
which is already starting to look a little better.
To simplify this expression further, we need to understand an object of the following
form:
nf
X
j1,j2,j3,j4=0
ϵ φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4 ψj3j4(xδ) .
(11.128)
Taking inspiration from the steps (10.130) that we took to derive our kernel-method
324

formula (11.125), let’s act on this object with two training-set kernels:
X
˜α1,˜α2∈A


nf
X
j1,j2,j3,j4=0
ϵ φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4 ψj3j4(xδ)

ek˜α1 ˜α3ek˜α2 ˜α4
=
X
˜α1,˜α2∈A
nf
X
j1,...,j6=0
ϵ φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4
× ψj3j4(xδ)φj5(x˜α1)φj5(x˜α3)φj6(x˜α2)φj6(x˜α4)
=
nf
X
j1,...,j6=0
ϵ Mj1j5

M−1
j1j3 Mj2j6

M−1
j2j4 ψj3j4(xδ)φj5(x˜α3)φj6(x˜α4)
=
nf
X
j1,j2=0
ϵ ψj1j2(xδ)φj1(x˜α3)φj2(x˜α4) .
(11.129)
Here on the second line, we used the deﬁnition of the kernel (11.126) to swap both
kernels for feature functions, on the third line we used the deﬁnition of the symmetric
matrix Mj1j2, (11.113), to replace two pairs of feature functions, and on the ﬁnal line we
simply canceled these matrices against their inverses.
This last expression suggests that an important object worth deﬁning is
µδ0δ1δ2 ≡
nf
X
j1,j2=0
ϵ ψj1j2(xδ0)φj1(xδ1)φj2(xδ2) ,
(11.130)
which we will call the meta kernel.20 Analogous to the kernel methods’ kernel (11.126),
the meta kernel is a parameter-independent tensor, symmetric under an exchange of its
ﬁnal two sample indices δ1 ↔δ2, and given entirely in terms of the ﬁxed feature and
meta feature functions that deﬁne the model. One way to think about (11.130) is that
for a ﬁxed particular input, xδ0, the meta kernel computes a diﬀerent feature-space inner
product between the two other inputs, xδ1 and xδ2. Note also that due to the inclusion
of the small parameter ϵ into the deﬁnition of the meta kernel (11.130), we should think
of µδ0δ1δ2 as being parametrically small too.
With this deﬁnition, the relation (11.129) can now be succinctly summarized as
nf
X
j1,j2,j3,j4=0
ϵ φj1(x˜α1)

M−1
j1j3 φj2(x˜α2)

M−1
j2j4 ψj3j4(xδ)
(11.131)
=
X
˜α3,˜α4∈A
µδ˜α3 ˜α4ek˜α3 ˜α1ek˜α4 ˜α2 .
20An alternate name for this object is the diﬀerential of the kernel, which we would consider symbolizing
as µδ0δ1δ2 →dkδ0δ1δ2. This name-symbol pair highlights the connection we’re about to make to ﬁnite-
width networks, but is perhaps less general in the context of making a broader model of representation
learning.
325

Finally, plugging this simple relation back into (11.127), we get
zi(x ˙β; θ⋆) =
X
˜α1,˜α2∈A
k ˙β ˜α1ek˜α1 ˜α2yi;˜α2
(11.132)
+ 1
2
X
˜α1,...,˜α4∈A

µ ˙β ˜α1 ˜α2 −
X
˜α5,˜α6∈A
k ˙β ˜α5ek˜α5 ˜α6µ˜α6 ˜α1 ˜α2



ek˜α1 ˜α3yi;˜α3

ek˜α2 ˜α4yi;˜α4

.
When the prediction of a quadratic model is computed in this way, we’ll hereby make it
known as a nearly-kernel machine or nearly-kernel methods.21
Analogous to linear models, we again have two ways of thinking about the solution
of our nonlinear quadratic model’s predictions: on the one hand, we can use the optimal
parameters (11.115) and (11.118) to make predictions (11.119); on the other hand, we
can make nearly-kernel predictions using the formula (11.132) in which the features, the
meta features, and the model parameters do not appear. That is, we’ve successfully
traded our feature-space quantities φj(x), ψj1j2(x), and W ⋆
ij for sample-space quantities
kδ˜α, µδ0 ˜α1 ˜α2, and yi;˜α. As was the case before for kernel methods, this works because
all the feature indices are contracted in our prediction formula (11.119), and so only
combinations of the form kδ˜α and µδ0 ˜α1 ˜α2 ever show up in the result and not the value
21Unlike kernel methods, this solution actually depends on the details of the learning algorithm. For
instance, if we had optimized the quadratic-regression loss (11.109) by gradient descent rather than by
direct optimization (11.110), then we would have found instead (for zero initialization Wij = 0)
zi(x ˙β; θ⋆) =
X
˜α1,˜α2∈A
k ˙β ˜α1ek ˜α1 ˜α2yi;˜α2
(11.133)
+
X
˜α1,...,˜α4∈A
"
µ˜α1 ˙β ˜α2 −
X
˜α5,˜α6∈A
k ˙β ˜α5ek ˜α5 ˜α6µ˜α1 ˜α6 ˜α2
#
Z ˜α1 ˜α2 ˜α3 ˜α4
A
yi;˜α3yi;˜α4
+
X
˜α1,...,˜α4∈A
"
µ ˙β ˜α1 ˜α2 −
X
˜α5,˜α6∈A
k ˙β ˜α5ek ˜α5 ˜α6µ˜α6 ˜α1 ˜α2
#
Z ˜α1 ˜α2 ˜α3 ˜α4
B
yi;˜α3yi;˜α4
for our nearly-kernel methods prediction formula, where the algorithm projectors are given by
Z ˜α1 ˜α2 ˜α3 ˜α4
A
≡ek ˜α1 ˜α3ek ˜α2 ˜α4 −
X
˜α5
ek ˜α2 ˜α5X ˜α1 ˜α5 ˜α3 ˜α4
II
,
(11.134)
Z ˜α1 ˜α2 ˜α3 ˜α4
B
≡ek ˜α1 ˜α3ek ˜α2 ˜α4 −
X
˜α5
ek ˜α2 ˜α5X ˜α1 ˜α5 ˜α3 ˜α4
II
+ η
2 X ˜α1 ˜α2 ˜α3 ˜α4
II
,
(11.135)
with the tensor X ˜α1 ˜α2 ˜α3 ˜α4
II
implicitly satisfying
X
˜α3,˜α4∈A
X ˜α1 ˜α2 ˜α3 ˜α4
II

ek˜α3 ˜α5δ˜α4 ˜α6 + δ˜α3 ˜α5ek˜α4 ˜α6 −ηek˜α3 ˜α5ek˜α4 ˜α6

= δ ˜α1
˜α5δ ˜α2
˜α6 ,
(11.136)
and global learning rate η. The origin of this gradient-descent solution should be clear after you traverse
through §∞.2.2. Such algorithm dependence is to be expected for a nonlinear overparameterized model
and is an important characteristic of ﬁnite-width networks as well. However, for the rest of the section we
will continue to analyze the direct optimization formula, (11.132), with Z ˜α1 ˜α2 ˜α3 ˜α4
A
= 0 and Z ˜α1 ˜α2 ˜α3 ˜α4
B
=
ek ˜α1 ˜α3ek ˜α2 ˜α4/2.
326

of the features or meta features themselves.22
This duality between the microscopic
feature-space description of the model and a macroscopic sample-space description is
another realization of the eﬀective theory approach discussed in §0.1, and we will return
to comment more broadly on this duality in Epilogue ε after we discuss the dynamics of
ﬁnite-width networks in §∞.
Finally, as we saw before for kernel methods, the nearly-kernel prediction is computed
by direct comparison with previously-seen examples. In this case, it has the same piece
linear in the true outputs proportional to yi;˜α2, and also has a new piece that’s quadratic
in the true output across diﬀerent training examples proportional to yi;˜α1yi;˜α2. In this
way, nearly-kernel methods are also memory-based methods that involve memorizing the
entire training set.
Trained-Kernel Prediction
Even though these nearly-kernel methods are very-nearly kernel methods, there’s a real
qualitative diﬀerence between them due to the presence of interactions between the
parameters. In the feature-space picture described in §11.4.1, this diﬀerence manifested
itself in terms of the nontrivial feature learning for the eﬀective features φE
ij(x, θ), as
expressed in the last line of the quadratic model prediction formula (11.119). To better
understand this from the dual sample-space picture, let’s analogously deﬁne an eﬀective
kernel
kE
ii;δ1δ2(θ) ≡
nf
X
j=0
φE
ij(xδ1; θ) φE
ij(xδ2; θ) ,
(11.137)
which measures a parameter-dependent similarity between two inputs xδ1 and xδ2 using
our eﬀective features (11.107).
Interestingly, we see that the model actually gives a
diﬀerent eﬀective kernel for each output component i.23 Let’s try to understand this a
little better by evaluating the eﬀective kernel at the end of training:
kE
ii;δ1δ2(θ⋆) ≡
nf
X
j=0
φE
ij(xδ1; θ⋆) φE
ij(xδ2; θ⋆)
(11.138)
=
nf
X
j=0
φj(xδ1) φj(xδ2) +
nf
X
j1,j2=0
W F
ij1 [ψj1j2(xδ1)φj2(xδ2) + ψj1j2(xδ2)φj2(xδ1)] + O

ϵ2
=kδ1δ2 +
X
˜α1,˜α2∈A
(µδ1δ2 ˜α1 + µδ2δ1 ˜α1)ek˜α1 ˜α2yi;˜α2 + O

ϵ2
.
22Just as we discussed for kernel methods in footnote 46 of §10, in some situations we expect that
specifying and evaluating the meta kernel µδ0δ1δ2 is much simpler than specifying and evaluating meta
feature function ψj1j2(x). Although picking these out of the thin air seems diﬃcult, perhaps there are
other inspired ways of determining these functions that don’t require an underlying description in terms
of neural networks. It would be interesting to determine the necessary and suﬃcient conditions for a
general three-input function, µ(xδ0, xδ1, xδ2) ≡µδ0δ1δ2, to be a meta kernel.
23Here, the use of two i’s in the subscript of the eﬀective kernel to represent the output-component is
just our convention; we’ll later require a version with oﬀ-diagonal components in the slightly-less minimal
model (11.146).
327

To get this last result on the ﬁnal line we plugged in the free solution (11.115), and then
secretly used the following relation
φj0(x˜α)

M−1
j0j1 ψj1j2(xδ1)φj2(xδ2) =
X
˜α1∈A
µδ1δ2 ˜α1ek˜α1 ˜α ,
(11.139)
which can be derived with manipulations analogous to those that we used in (10.130) and
(11.129).24 Here, in (11.138) we see that the eﬀective kernel is shifted from the kernel
and includes a contribution proportional to the meta kernel as well as the true training
outputs yi;˜α; this is what gives the eﬀective kernel its output-component dependence.
Finally, let’s deﬁne one more kernel:
k♯
ii;δ1δ2 ≡1
2
h
kδ1δ2 + kE
ii;δ1δ2(θ⋆)
i
.
(11.140)
This trained kernel averages between the simple kernel methods’ kernel from the linear
model and the learned nearly-kernel methods’ eﬀective kernel. Deﬁning the inverse of
the trained-kernel submatrix evaluated on the training set in the usual way,
X
˜α2∈A
ek♯˜α1 ˜α2
ii
ek♯ii;˜α2 ˜α3 = δ ˜α1
˜α3 ,
(11.141)
the utility of this ﬁnal formulation is that the nearly-kernel prediction formula (11.132)
can now be compressed as
zi(x ˙β; θ⋆) =
X
˜α1,˜α2∈A
k♯
ii; ˙β ˜α1
ek♯˜α1 ˜α2
ii
yi;˜α2 + O

ϵ2
,
(11.142)
taking the form of a kernel prediction, but with the beneﬁt of nontrivial feature evolution
incorporated into the trained kernel.25 This is how representation learning manifests
itself in nearly-kernel methods.
Finally, note that in our minimal model of representation learning, there’s no wiring
or mixing among the nout diﬀerent output components: while the prediction zi(x ˙β; θ⋆)
is quadratic in the true output yi;˜α – most easily seen in (11.132) – it still only involves
the i-th component. From the perspective of the trained-kernel prediction, (11.142),
each output component i has a diﬀerent trained kernel associated with its prediction,
but the i-th prediction never depends on other true output components yi′;˜α with i′ ̸= i.
However, this lack of wiring is by our design; this representation-learning model is
really intended to be minimal. To enable mixing of the output components, we’ll have
to slightly generalize the quadratic model. This we’ll do next when we explain how
ﬁnite-width networks can be described in this nearly-kernel methods framework.
24Note that if we had instead optimized the quadratic-regression loss, (11.109), using gradient descent,
then the eﬀective kernel at the end of training, kE
ii;δ1δ2(θ⋆), would have a diﬀerent expression than the
one above, (11.138), for direct optimization, cf. our discussion in footnote 21.
25To verify the formula, use the deﬁnition of the trained kernel, (11.140), then expand in the eﬀective
kernel using the Schwinger-Dyson equations (4.55) to evaluate the matrix inverse. The result should
agree with the nearly-kernel prediction formula (11.132).
328

11.4.3
Finite-Width Networks as Nonlinear Models
While the discussion so far in this section has been somewhat disconnected from the deep
learning framework, much of it should still feel pretty familiar to you. For instance, the
formula for the eﬀective kernel at the end of training, (11.138), seems like it could be
related to the update to the NTK, (11.10), if we identify the meta kernel µδ0δ1δ2 with the
dNTK d
dHi0i1i2;δ0δ1δ2 and also make the previous identiﬁcations that we made in §10.4.3
between the kernel methods’ kernel and the NTK. Let’s now make these connections
between ﬁnite-width networks and nonlinear models more precise.
To start, for neural networks, let us deﬁne an analog of the eﬀective feature function
φE
ij(xδ; θ) (11.107) by
φE
i,µ(xδ; θ) ≡
dz(L)
i;δ
dθµ
.
(11.143)
Note that for the linear model description of inﬁnite-width networks, the derivative of the
model output is a constant, and these features are completely ﬁxed throughout training.
In contrast, for quadratic models and ﬁnite-width networks, the derivative (11.143) is
not constant, and so these eﬀective features evolve throughout training as the model
parameters move. As for the function approximator itself, after a small change in the
parameters θ →θ + d¯θ, the network output evolves as
z(L)
i;δ (θ + d¯θ) = z(L)
i;δ (θ) +
P
X
µ=1
dz(L)
i;δ
dθµ
d¯θµ + 1
2
P
X
µ,ν=1
d2z(L)
i;δ
dθµdθν
d¯θµd¯θν + . . . ,
= z(L)
i;δ (θ) +
P
X
µ=1
φE
i,µ(xδ; θ) d¯θµ + ϵ
2
P
X
µ,ν=1
bψi,µν(xδ) d¯θµd¯θν + . . . ,
(11.144)
where we’ve additionally deﬁned an analog of the meta feature function ψj1j2(xδ) for
neural networks by
ϵ bψi,µν(xδ) ≡
d2z(L)
i;δ
dθµdθν
.
(11.145)
For this discussion, we truncated the “. . .” in (11.144) so that the update to the output
is exactly quadratic in the small change in the parameters. With this truncation, the
update (11.144) for a ﬁnite-width neural network is identical to the update equation
(11.106) that we found for our quadratic model after taking a small step.26
Let us further note that for the linear model description of inﬁnite-width networks,
the meta feature functions (11.145) vanish identically – as any linear function has a zero
second derivative – and thus have no eﬀect on the dynamics. For ﬁnite-width networks
with a quadratic truncation, these meta features (11.145) are parametrically small but
26Considering the deﬁnition of our quadratic model, (11.105), we have included the small parameter
ϵ as part of our identiﬁcation. For MLPs, this parameter will be set automatically by the architecture,
and is given by the eﬀective theory cutoﬀ, the depth-to-width ratio of the network: ϵ ≡L/n. However,
for such ﬁnite-width networks there are additional terms of order ϵ ≡L/n that need to be incorporated
in order to have a consistent description, as we will explain soon.
329

no longer zero; they are stochastically sampled at initialization and then ﬁxed over the
course of training, hence decorated with a hat. Therefore, at quadratic order we will call
these meta feature functions, bψi,µν(x), as random meta features, just as we called the
feature functions as random features for inﬁnite-width networks.
Having established a connection in the feature space, let us now establish a similar
connection in the sample-space dual description.
First, associated with the eﬀective
feature functions (11.143) is the analog of the eﬀective kernel kE
ii;δ1δ2(θ) (11.137), deﬁned
by
kE
i1i2;δ1δ2(θ) =
X
µ,ν
λµν φE
i1,µ(xδ1; θ)φE
i2,ν(xδ2; θ) =
X
µ,ν
λµν
dz(L)
i1;δ1
dθµ
dz(L)
i2;δ2
dθν
≡H(L)
i1i2;δ1δ2(θ) .
(11.146)
Here, we used our more general deﬁnition of the kernel (10.136) to include the learning-
rate tensor, and since the eﬀective features (11.143) have a parameter dependence, in
the ﬁnal equality we used most general deﬁnition of the NTK, (7.17), and gave it a
θ argument, H(L)
i1i2;δ1δ2(θ), to indicate its parameter dependence.
In particular, if we
evaluated the eﬀective kernel at initialization θ = θ(t = 0) in terms of the random
features
bφi,µ(xδ) ≡φE
i,µ

xδ; θ(t = 0)

=
dz(L)
i;δ
dθµ

θ=θ(t=0)
,
(11.147)
we’d just have the usual L-th-layer stochastic NTK at initialization (8.5):
bki1i2;δ1δ2 ≡kE
i1i2;δ1δ2

θ(t = 0)

=
X
µ,ν
λµν bφi1,µ(xδ1) bφi2,ν(xδ2)
(11.148)
=
X
µ,ν
λµν

dz(L)
i1;δ1
dθµ
dz(L)
i2;δ2
dθν



θ=θ(t=0)
≡bH(L)
i1i2;δ1δ2 .
For inﬁnite-width networks, this NTK doesn’t evolve during training and is composed of
random features at initialization (10.138). In contrast, as we saw in §11.1, for ﬁnite-width
networks the eﬀective kernel (11.146) does evolve during training, just as the analogous
eﬀective kernel (11.137) did for the quadratic model.
Finally, analogously to the meta kernel for the quadratic model (11.130), we can
form a meta kernel for ﬁnite-width networks from the random features (11.147) and the
random meta features (11.145) as
bµi0i1i2;δ0δ1δ2 ≡
X
µ1,ν1,
µ2,ν2
ϵλµ1ν1λµ2ν2 bψi0,µ1µ2(xδ0) bφi1,ν1(xδ1) bφi2,ν2(xδ2)
(11.149)
=
X
µ1,ν1,
µ2,ν2
λµ1ν1λµ2ν2

d2z(L)
i0;δ0
dθµ1dθµ2
dz(L)
i1;δ1
dθν1
dz(L)
i2;δ2
dθν2



θ=θ(t=0)
≡d
dH
(L)
i0i1i2;δ0δ1δ2 ,
330

where we slightly generalized our earlier deﬁnition of the meta kernel (11.130) with the
inclusion of the learning-rate tensors.27 Thus, we’ve now identiﬁed the random meta
kernel (11.149) with the L-th-layer stochastic dNTK (11.8).
With all these connections established, there are three notable diﬀerences between
our minimal quadratic model and ﬁnite-width neural networks.
First, as should be clear from the deﬁnitions of the random features and random meta
features, (11.147) and (11.145), these functions are stochastic rather than designed: they
are determined by the details of the neural network architecture and depend on the values
of the randomly-sampled parameters at initialization.
We might more generally call
such a quadratic model of the form (11.105) with random functions bφj(x) and bψj1j2(x) a
random meta feature model, generalizing the notion of a random feature model that
we discussed in conjunction with inﬁnite-width networks and linear models in §10.4.3.
Second, as we discussed at the end of §11.4.2, the quadratic model (11.105) does
not wire together diﬀerent components of the true outputs from the training set when
making nearly-kernel predictions (11.132) on test-set inputs. In contrast, we will show
soon in §∞.2.3 that the ﬁnite-width network predictions do have this wiring property.
This deﬁciency of the quadratic model was actually by design on our part in an eﬀort
to eliminate extra complications when working through our minimal model of represen-
tation learning. To include wiring in the quadratic model, we can generalize it slightly
as
zi(xδ; θ) =
P
X
µ=1
θµ bφi,µ(xδ) + ϵ
2
P
X
µ,ν=1
θµθν bψi,µν(xδ) .
(11.150)
This slightly-less-minimal model will now allow a parameter θµ to connect to various
diﬀerent output components, as the feature functions and meta feature functions now
also carry vectorial indices specifying an output-component.28
Third, as we’ve mentioned throughout this chapter, the leading ﬁnite-width contri-
butions to the update to the network output include O
 η3 terms. To capture these
eﬀects, we need to deform our quadratic model (11.105) into a cubic model:
zi(xδ; θ) =
P
X
µ=1
θµ bφi,µ(xδ) + 1
2
P
X
µ,ν=1
θµθν bψi,µν(xδ) + 1
6
P
X
µ,ν,ρ=1
θµθνθρ bΨi,µνρ(xδ) . (11.151)
Here, the random meta-meta feature function, are given by the third derivative of
the network output,
bΨi,µνρ(xδ) ≡
d3z(L)
i;δ
dθµdθνdθρ
;
(11.152)
27Our slightly more general deﬁnition of the meta kernel here should be understood as analogous to
the slightly more general deﬁnition of the kernel (10.136).
28Note that these feature functions may have constraints, cf. the explicit form of the random feature
(10.139). These constraints end up causing the inﬁnite-width model not to wire, while allowing to wire
the predictions of any particular network at ﬁnite width. These constraints can be thought of as a type
of weight-tying.
331

the addition of this cubic term will enable the meta features to eﬀectively evolve as if
they’re described by a linear model, while in turn the features will eﬀectively evolve as
if they’re described by a quadratic model.29 In summary, for ﬁnite-width networks of
depth L > 1, this less-minimal model, (11.151), is a consistent description, with ran-
dom features (11.147), random meta features (11.145), and random meta-meta features
(11.152).
Deep Learning: A Non-Minimal Model of Representation Learning
Representation learning is a big part of what makes deep learning exciting. What our
minimal model of representation learning has shown us is that we can actually decouple
the analysis of the learning from the analysis of the deep: the simple quadratic model
(11.105) exhibits nontrivial representation learning for general choices of feature func-
tions φj(x) and meta feature functions ψjk(x), or dually, of a kernel kδ1δ2 and a meta
kernel µδ0δ1δ2. In particular, the meta kernel is what made learning features from the
training data possible, and we hope that this broader class of representation-learning
models will be of both theoretical and practical interest in their own right.
Of course, deep learning is a non-minimal model of representation learning, and the
structure of these kernels and meta kernels do matter. Speciﬁcally, for deep neural net-
works the statistics of these functions encoded in the joint preactivation-NTK-dNTK
distribution p

z(L), bH(L), d
dH
(L)D

are controlled by the representation group ﬂow re-
cursions – cf. §4, §8, and §11.2 – the details of which are implicitly determined by the
underlying architecture and hyperparameters. In particular, we can understand the im-
portance of this RG ﬂow by remembering there can be a vast improvement from selecting
other architectures beyond MLPs when applying function approximation to speciﬁc do-
mains or datasets: RG ﬂow is the inductive bias of the deep learning architecture.30
Thus, even in the set of models that exhibit nontrivial representation learning, these
choices – the initial features, meta features, and so on – are still really important.31
29To make this connection precise, we must give the small parameter ϵ not in the cubic model deﬁnition
(11.151), but instead in the statistics of the joint distribution, p(bφi,µ, bψi,µν, bΨi,µνρ), that controls the
random meta-meta feature model. Schematically, the nontrivial combinations are the following:
E
h
bφ2i
= O(1) ,
E
h
bψ bφ2z
i
= O(ϵ) ,
E
h
bΨ bφ3i
= O(ϵ) ,
E
h
bψ2 bφ2i
= O(ϵ) .
(11.153)
In the next chapter, we’ll identify these combinations with the NTK, the dNTK, and (soon-to-be-
revealed) two ddNTKs, respectively. Importantly, since all of these combinations are the same order in
ϵ = L/n, to describe ﬁnite-width networks self-consistently, we need to think of them as cubic models.
30Note that the formalism of nonlinear models and nearly-kernel methods that we outlined in this
section should also describe these other deep learning architectures so long as they admit an expansion
around an inﬁnite-width (or inﬁnite-channel or inﬁnite-head) limit. In particular, everything we learned
here about representation learning and the training dynamics can be carried over; the only diﬀerence
is that we will have have diﬀerent functions φi,µ(x) and ψi,µν(x), leading to diﬀerent kernels and meta
kernels, ki1i2;δ1δ2 and µi0i1i2;δ0δ1δ2, that can be built up from a diﬀerent set of recursions than the ones
that we studied in this book.
31In Appendix B, we’ll explore an aspect of this question directly by studying residual networks: these
networks let us introduce a parameter that in a single network has an interpretation of trading oﬀmore
332

The full power of deep learning is likely due to the deep – i.e. the representation group
ﬂow induced by interactions between neurons in deep models of many iterated layers
– working in conjunction with the learning – i.e. the representation learning induced
by the nonlinear dynamical interactions present at ﬁnite width. The principles of deep
learning theory presented in this book are precisely those that will let you analyze both
of these irreducible basic elements in full generality.
layers of representation group ﬂow against more eﬀective realizations from the ensemble.
333

334

Chapter ∞
The End of Training
The job of a scientist is to listen carefully to nature, not to tell nature how to behave.
Freeman Dyson, explaining Richard Feynman’s approach [69].
In this chapter, we’ll ﬁnally ﬁnish our leading-order eﬀective-theory analysis of ﬁnite-
width networks and solve their training dynamics under gradient descent. In contrast
to the inﬁnite-width limit, for which the solution is independent of the training algo-
rithm, the dynamics of such deep networks have a rich phenomenology that captures
the diﬀerent ways in which useful features may develop over the course of training. The
solution to these training dynamics gives ﬁrst-principles description of the ensemble of
fully-trained ﬁnite-width networks, realizing a main goal of the book.
Unfortunately, our job will be disrupted by two facts of nature: (i) in order to have
a consistent description of training dynamics at order 1/n, we’ll need to incorporate
two additional objects that arise in the Taylor expansion of the update to the network
output to third order, the update to the NTK to second order, and the update to the
dNTK to ﬁrst order; and (ii) due to a lack of smoothness, we won’t be able to describe
the dynamics of ReLU networks nor networks consisting of any of the other nonlinear
activation functions from the scale-invariant universality class.
As for the ﬁrst point, while the analysis of representation learning in the context of the
quadratic model was illuminating, we’ve already telegraphed that it was insuﬃcient to
capture the particular details of ﬁnite-width networks. In particular, to leading order in
1/n, there are two more NTK diﬀerentials, which we’ll refer to as ddNTKs. Although it’s
straightforward, working out the stochastic forward equations, recursions, and eﬀective
theory for these ddNTKs is somewhat tedious, and no longer has any pedagogical value.
As such, we won’t provide the details of our derivations – you’ve already seen these sets of
manipulations three times before in §4–§5, §8–§9, and §11.2–§11.3, for the preactivations,
NTK, and dNTK, respectively – instead we’ll simply state the results, leaving the details
for you as a kind of post-training test evaluation; after all, this is the end of your training
as well.
As for the second point, throughout the book we’ve had to use special methods in
335

order to work out exceptional explanations for any non-smooth activation function such
as the ReLU. In our minds, this extra work was justiﬁed by the ReLU’s current privileged
status as one of the most popular activation functions in practice. However, we have
ﬁnally run out of tricks and will have to give up: for a reason that is simple to explain,
our Taylor expansion in the global learning rate η will break down when applied to the
dynamics of networks built with non-smooth activation functions. Instead, we’ll have to
follow the direction of the community and begin thinking again about smoothed versions
of the ReLU – though only the ones that permit a type of criticality – such as the GELU
and the SWISH.
With both those disruptions to our work heard, in §∞.1 we’ll present all the relevant
results for the ddNTKs – we’ll deﬁne them, we’ll give their tensor decomposition, and
we’ll explain their scaling with width and depth – while hiding all the irrelevant details
at the back of the chapter in §∞.3.
If you’ve been paying attention, you’ll not be
shocked to hear that – when properly normalized – the ddNTKs scale as the eﬀective
theory cutoﬀ: ℓ/n. This scaling indicates that we need to consider the joint statistics of
the preactivation-NTK-dNTK-ddNTKs in order to understand the leading-order ﬁnite-
width dynamics of deep MLPs. Importantly, these ddNTKs endow the dNTK with its
own dynamics; from the parameter-space perspective of §11.4.1, this means that the
meta feature functions of the model will now evolve.
With those results stated, in §∞.2 we’ll return to our regularly scheduled pedagogy
and, at long last, solve the training dynamics at ﬁnite width. After an initial false start
following our inﬁnite-width giant leap, ﬁrst in §∞.2.1 we’ll learn how to take a small
step following an adjusted giant leap, giving us our ﬁrst ﬁnite-width solution. Then
in §∞.2.2, we’ll analyze many many steps of vanilla gradient descent, giving us our
second ﬁnite-width solution. The nonlinear dynamics at ﬁnite width ultimately lead to
a dependence of the fully-trained solution on the training algorithm, and so the solutions
derived in these two subsections actually exhibit meaningful diﬀerences.
In particular, the function approximation of a fully-trained ﬁnite-width network can
be decomposed into a universal part, independent of the optimization details, and a
set of algorithm projectors, whose functional form encodes the entire dependence of
the solution on the training algorithm. These projectors provide a dual sample-space
perspective on the learning algorithm, analogous to the relationship between the model
parameters and the diﬀerent kernels.
Accordingly, in §∞.2.3 we’ll discuss how these projectors impact the solution, letting
us understand the inductive bias of the training dynamics separately from the inductive
bias of the network architecture. We’ll also further analyze the predictions made by such
fully-trained networks, considering the growing tradeoﬀbetween increased representation
learning and increased instantiation-to-instantiation ﬂuctuations with network depth.
While this is the ﬁnal chapter of the main text, in a small epilogue following this
chapter, Epilogue ε, we’ll explore how to deﬁne model complexity for overparameterized
networks from our eﬀective theory’s macroscopic perspective. Then in two appendices,
we’ll further touch on some topics that are outside the scope of our main line of inquiry.
In Appendix A, we’ll introduce the framework of information theory, which will give us
336

the tools we need in order to estimate the optimal aspect ratio that separates eﬀectively-
deep networks from overly-deep networks. In Appendix B, we’ll apply our eﬀective theory
approach to learn about residual networks and see how they can be used to extend the
range of eﬀectively-deep networks to greater and greater depths.
∞.1
Two More Diﬀerentials
Who ordered that?
I. I. Rabi, quipping about the O(1/n) ddNTKs.
One last time, let’s expand the ℓ-th-layer preactivations after a parameter update, this
time recording terms up to third order:
d¯z(ℓ)
i;δ ≡z(ℓ)
i;δ (t = 1) −z(ℓ)
i;δ (t = 0)
(∞.1)
=
ℓ
X
ℓ1=1
X
µ
dz(ℓ)
i;δ
dθ(ℓ1)
µ
d¯θ(ℓ1)
µ
+ 1
2
ℓ
X
ℓ1,ℓ2=1
X
µ1,µ2
d2z(ℓ)
i;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
d¯θ(ℓ1)
µ1 d¯θ(ℓ2)
µ2
+ 1
6
ℓ
X
ℓ1,ℓ2,ℓ3=1
X
µ1,µ2,µ3
d3z(ℓ)
i;δ
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2 dθ(ℓ3)
µ3
d¯θ(ℓ1)
µ1 d¯θ(ℓ2)
µ2 d¯θ(ℓ3)
µ3 + . . . .
For gradient descent, also recall that after use of the chain rule (11.3), the change in the
ℓa-th-layer parameters of any particular network is given by (11.4),
d¯θ(ℓa)
µ
= −η
X
ν
λ(ℓa)
µν

X
j,k,˜α
∂LA
∂z(L)
k;˜α
dz(L)
k;˜α
dz(ℓ)
j;˜α
dz(ℓ)
j;˜α
dθ(ℓa)
ν

= −η
X
ν,j,˜α
λ(ℓa)
µν ϵ(ℓ)
j;˜α
dz(ℓ)
j;˜α
dθ(ℓa)
ν
,
(∞.2)
where we’ve used our convention from §11.1 of explicitly specifying which layer each
parameter comes from. Please also recall from there that the learning-rate tensor λ(ℓ)
µν
only connects the parameters within a given layer ℓ. In the above expression, ℓis an
intermediate layer such that ℓa ≤ℓ, and we also used our ℓ-th-layer error factor (11.5):
ϵ(ℓ)
j;˜α ≡
nL
X
k=1
∂LA
∂z(L)
k;˜α
dz(L)
k;˜α
dz(ℓ)
j;˜α
= dLA
dz(ℓ)
j;˜α
.
(∞.3)
After substituting the parameter update (∞.2) back into the preactivation update (∞.1),
you should be able to write it in the form
d¯z(ℓ)
i;δ = −η
X
j,˜α
bH(ℓ)
ij;δ˜αϵ(ℓ)
j;˜α + η2
2
X
j1,j2,˜α1,˜α2
d
dH
(ℓ)
ij1j2;δ˜α1 ˜α2ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2
(∞.4)
−η3
6
X
j1,j2,j3,˜α1,˜α2,˜α3
\
ddIH
(ℓ)
ij1j2j3;δ˜α1 ˜α2 ˜α3ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2ϵ(ℓ)
j3;˜α3 ,
+ O

η4
337

where the ﬁrst two terms we found in the last chapter (11.9), and the cubic term is new,
with the ﬁrst of the ddNTKs deﬁned as
\
ddIH
(ℓ)
i0i1i2i3;δ0δ1δ2δ3 ≡
ℓ
X
ℓ1,ℓ2,ℓ3=1
X
µ1,ν1,
µ2,ν2,
µ3,ν3
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2λ(ℓ3)
µ3ν3
d3z(ℓ)
i0;δ0
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2 dθ(ℓ3)
µ3
dz(ℓ)
i1;δ1
dθ(ℓ1)
ν1
dz(ℓ)
i2;δ2
dθ(ℓ2)
ν2
dz(ℓ)
i3;δ3
dθ(ℓ3)
ν3
.
(∞.5)
As always, the hat on the ddNTK indicates that it’s stochastic, depending on the par-
ticular realization of the model parameters at initialization. Also, similar to the dNTK,
this ddNTK is totally symmetric in its second, third, and fourth paired set of indices
(i1, δ1) ↔(i2, δ2) ↔(i3, δ3), while the ﬁrst neural-sample index (i0, δ0) is distinguished
from the other three.
By expanding to order η3, the update, (∞.4), is now cubic in error factors. Expanding
to this order is necessary because the ﬁrst ddNTK has statistics at initialization that are
O(1/n), and so needs to be included in our analysis. However, any higher-order terms
in the update are subleading, so we may replace O
 η4 = O
 1/n2 in this expression.
Just as we had to expand the update to the NTK to order η when we expanded the
update to the preactivations to order η2, we will now have to expand the update to the
NTK to order η2 for the dynamics with our cubic update (∞.4) to be consistent:
d¯H(ℓ)
i1i2;δ1δ2 ≡H(ℓ)
i1i2;δ1δ2(t = 1) −H(ℓ)
i1i2;δ1δ2(t = 0)
(∞.6)
=
ℓ
X
ℓ1=1
X
µ1
dH(ℓ)
i1i2;δ1δ2
dθ(ℓ1)
µ1
d¯θ(ℓ1)
µ1 +
ℓ
X
ℓ1,ℓ2=1
X
µ1,µ2
d2H(ℓ)
i1i2;δ1δ2
dθ(ℓ1)
µ1 dθ(ℓ2)
µ2
d¯θ(ℓ1)
µ1 d¯θ(ℓ2)
µ2 + . . .
= −η
X
j,˜α

d
dH
(ℓ)
i1i2j;δ1δ2 ˜α + d
dH
(ℓ)
i2i1j;δ2δ1 ˜α

ϵ(ℓ)
j;˜α
+ η2
2
X
j1,j2,˜α1,˜α2

\
ddIH
(ℓ)
i1i2j1j2;δ1δ2 ˜α1 ˜α2 + \
ddIH
(ℓ)
i2i1j1j2;δ2δ1 ˜α1 ˜α2

ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2
+ η2
X
j1,j2,˜α1,˜α2
\
ddIIH
(ℓ)
i1i2j1j2;δ1δ2 ˜α1 ˜α2ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2 + O

η3
.
To go to the ﬁnal equality, we substituted in our NTK deﬁnition (11.7) and our parameter
update (∞.2), computed the derivatives, and then collected the terms. To do so, we
identiﬁed the second of the ddNTKs, deﬁned as
\
ddIIH
(ℓ)
i1i2i3i4;δ1δ2δ3δ4 ≡
ℓ
X
ℓ1,ℓ2,ℓ3=1
X
µ1,ν1,
µ2,ν2,
µ3,ν3
λ(ℓ1)
µ1ν1λ(ℓ2)
µ2ν2λ(ℓ3)
µ3ν3
d2z(ℓ)
i1;δ1
dθ(ℓ1)
µ1 dθ(ℓ3)
µ3
d2z(ℓ)
i2;δ2
dθ(ℓ2)
µ2 dθ(ℓ3)
ν3
dz(ℓ)
i3;δ3
dθ(ℓ1)
ν1
dz(ℓ)
i4;δ4
dθ(ℓ2)
ν2
.
(∞.7)
The hat on this ddNTK indicates that it’s also stochastic at initialization, and we will
soon detail that it also has O(1/n) statistics at leading order. Finally, \
ddIIHi1i2i3i4;δ1δ2δ3δ4
338

has a more constrained symmetry, only symmetric under a joint swap of the paired set
of indices as (i1, δ1) ↔(i2, δ2) and (i3, δ4) ↔(i4, δ4). However, this means that we can
also swap indices as
X
j1,j2,˜α1,˜α2
\
ddIIH
(ℓ)
i1i2j1j2;δ1δ2 ˜α1 ˜α2ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2 =
X
j1,j2,˜α1,˜α2
\
ddIIH
(ℓ)
i2i1j1j2;δ2δ1 ˜α1 ˜α2ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2 ,
(∞.8)
which we used to simplify the ﬁnal line of the NTK update (∞.6).
Overall, this NTK update is now quadratic in error factors, making its dynamics
coupled and nonlinear. Again, expanding to this order is necessary because both ddNTKs
have statistics at initialization that are O(1/n), and so both need to be included in our
analysis. However, any higher-order terms in the NTK update are subleading, so in
(∞.6) we may replace O
 η3 = O
 1/n2.
Finally, consider the leading-order update to the dNTK:
d¯dH(ℓ)
i0i1i2;δ0δ1δ2 ≡dH(ℓ)
i0i1i2;δ0δ1δ2(t = 1) −dH(ℓ)
i0i1i2;δ0δ1δ2(t = 0)
(∞.9)
=
ℓ
X
ℓ1=1
X
µ1
ddH(ℓ)
i0i1i2;δ0δ1δ2
dθ(ℓ1)
µ1
d¯θ(ℓ1)
µ1 + . . .
= −η
X
j,˜α

\
ddIH
(ℓ)
i0i1i2j;δ0δ1δ2 ˜α + \
ddIIH
(ℓ)
i0i1i2j;δ0δ1δ2 ˜α + \
ddIIH
(ℓ)
i0i2i1j;δ0δ2δ1 ˜α

ϵ(ℓ)
j;˜α
+ O

η2
.
To go to the ﬁnal equality, we substituted our dNTK deﬁnition (11.8) and parameter
update (∞.2), took the derivative, and then collected all the terms using our ddNTK def-
initions, (∞.5) and (∞.7). The dNTK update is linear in error factors and its dynamics
will be the simplest. Finally, the higher-order terms in the dNTK update are subleading,
so in (∞.9) we may replace O
 η2 = O
 1/n2. We also would like to apologize for the
d¯dH notation, and we promise that we won’t have to use it again.
The updates (∞.4), (∞.6), and (∞.9) comprise the complete set of ﬁnite-width
updates at O(1/n). Thus, to proceed further in our analysis, we’ll need to work out the
leading-order statistics of the ddNTKs.
ddNTK Statistics
To ﬁnd the distribution of fully-trained ﬁnite-width networks, we need the joint distri-
bution of the network output, the NTK, the dNTK, and the ddNTKs:
p

z(L), bH(L), d
dH
(L), \
ddIH
(L), \
ddIIH
(L)D

.
(∞.10)
Rather than working through the details here, we’ll do it in our own private notebooks.
You already have all the tools you need: you can follow §4, §8, and §11.2 for examples
of how to use RG ﬂow to work out the recursions; and you can follow §5, §9, and §11.3
339

for examples of how to work out the details of the eﬀective theory at initialization after
tuning to criticality. The full details of this distribution (∞.10) can be found at the
end of the chapter in §∞.3. Here, we’ll highlight the results that you need in order to
understand our dynamical computations in the following section.
After working out the stochastic forward equations for both ddNTKs – feel free to
ﬂip forward to (∞.174) and (∞.175) if you’re curious about them – we’ll need to ﬁnd
recursions for its statistics. For these tensors, the leading-order statistics come from their
means; their cross correlations and their variances are all subleading. When evaluating
the mean of each of the ddNTKs, it will become convenient to decompose them into the
following set of tensors with sample indices only:
E

\
ddIH
(ℓ)
i0i1i2i3;δ0δ1δ2δ3

≡
1
nℓ−1
h
δi0i1δi2i3R(ℓ)
δ0δ1δ2δ3 + δi0i2δi3i1R(ℓ)
δ0δ2δ3δ1 + δi0i3δi1i2R(ℓ)
δ0δ3δ1δ2
i
,
(∞.11)
E

\
ddIIH
(ℓ)
i1i2i3i4;δ1δ2δ3δ4

≡
1
nℓ−1
h
δi1i2δi3i4S(ℓ)
δ1δ2δ3δ4 + δi1i3δi4i2T (ℓ)
δ1δ3δ4δ2 + δi1i4δi2i3U(ℓ)
δ1δ4δ2δ3
i
.
(∞.12)
Thus, there are four new objects whose recursions we need to determine and whose
scaling with depth we’ll need to compute.
The ddNTKs both vanish in the ﬁrst layer, just like the dNTK. Proceeding then
from the second layer to deeper layers, after a bunch of tedious algebra and plenty of
ﬂipping around in the book to make various substitutions, you can ﬁnd recursions for
R(ℓ), S(ℓ), T (ℓ), and U(ℓ): (∞.177), (∞.179), (∞.180), and (∞.181), respectively. You
can ﬂip forward to take a look at them, but you probably won’t want to. . . . Importantly,
these recursions in conjunction with the decompositions (∞.11) and (∞.12) altogether
demonstrate the both ddNTKs have nontrivial order-1/n statistics:
E

\
ddIH
(ℓ)
i0i1i2i3;δ0δ1δ2δ3

= O
 1
n

,
E

\
ddIIH
(ℓ)
i1i2i3i4;δ1δ2δ3δ4

= O
 1
n

.
(∞.13)
Thus, as we’ve been forecasting, we must include them in our ﬁnite-width analysis of
training dynamics.
Focusing on the single-input statistics, we again make the layer-independent choices
C(ℓ)
b
= Cb, C(ℓ)
W = CW , and n1 = · · · = nL−1 ≡n. Ignoring contributions that are
subleading in 1/n, in particular replacing the mean metric by the kernel G(ℓ) →K(ℓ)
and the NTK mean by the frozen NTK H(ℓ) →Θ(ℓ), the four recursions (∞.177),
340

(∞.179), (∞.180), and (∞.181) together reduce to a form that at least ﬁts on a page:
R(ℓ+1) =

χ(ℓ)
⊥
2 R(ℓ)
(∞.14)
+ λ(ℓ+1)
W
CW

σ′′σ′σ′σ

K(ℓ)

Θ(ℓ)2 + C2
W

σ′′′σ′σ′σ′
K(ℓ)

Θ(ℓ)3
+ χ(ℓ)
⊥

λ(ℓ+1)
W

σ′′σ

K(ℓ) + CW Θ(ℓ) 
σ′′′σ′
K(ℓ)
 
B(ℓ) + P (ℓ)
+ χ(ℓ)
⊥

λ(ℓ+1)
W

σ′σ′
K(ℓ) + CW Θ(ℓ) 
σ′′σ′′
K(ℓ)

P (ℓ) ,
S(ℓ+1) =

χ(ℓ)
⊥
2 S(ℓ)
(∞.15)
+ CW λ(ℓ+1)
W

σ′σ′σ′σ′
K(ℓ)

Θ(ℓ)2 + C2
W

σ′′σ′′σ′σ′
K(ℓ)

Θ(ℓ)3
+ χ(ℓ)
⊥
h
λ(ℓ+1)
W

σ′σ′
K(ℓ) + CW Θ(ℓ) 
σ′′σ′′
K(ℓ)
i
B(ℓ) ,
T (ℓ+1) =

χ(ℓ)
⊥
2 T (ℓ)
(∞.16)
+ 2CW λ(ℓ+1)
W

σ′′σ′σ′σ

K(ℓ)

Θ(ℓ)2 + C2
W

σ′′σ′′σ′σ′
K(ℓ)

Θ(ℓ)3
+

λ(ℓ+1)
W
2 
σ′σ′σσ

K(ℓ) Θ(ℓ)
+

λ(ℓ+1)
W

zσ′σ

K(ℓ) + CW Θ(ℓ) 
zσ′′σ′
K(ℓ)
2
F (ℓ)
 K(ℓ)2
+ 2χ(ℓ)
⊥
h
λ(ℓ+1)
W
 
σ′′σ

K(ℓ) +

σ′σ′
K(ℓ)
 + CW Θ(ℓ)  
σ′′′σ′
K(ℓ) +

σ′′σ′′
K(ℓ)
i
Q(ℓ) ,
U(ℓ+1) =

χ(ℓ)
⊥
2 U(ℓ) + C2
W

σ′′σ′′σ′σ′
K(ℓ)

Θ(ℓ)3 .
(∞.17)
Here, we also simpliﬁed these expressions by recalling the two susceptibilities, (5.50)
and (5.51):
χ(ℓ)
∥
≡CW
K(ℓ)

σ′σz

K(ℓ) ,
χ(ℓ)
⊥≡CW

σ′σ′
K(ℓ) .
(∞.18)
ddNTK Scalings
Now, let’s tune to criticality, and use our scaling ansatz (5.93) to ﬁnd the critical expo-
nents pR, pS, pT , and pU that describe the asymptotic depth scaling of R(ℓ), S(ℓ), T (ℓ),
and U(ℓ), respectively.
To understand the relative size of these tensors controlling the means of the ddNTKs,
we will again need to identify appropriate dimensionless ratios. Following the dimen-
sional analysis logic from our dNTK discussion, cf. (11.63), let’s look at our third-order
update for preactivations (∞.4). Remembering again that we can only add terms that
have the same dimensions, we see that
[z] = [η] [ϵ] [ bH] = [η]2 [ϵ]2 [d
dH] = [η]3 [ϵ]3 [\
ddIH] = [η]3 [ϵ]3 [ \
ddIIH] .
(∞.19)
341

From the ﬁrst equality, we see as before that [η] [ϵ] = [z] [ bH]−1, and so R, S, T, and U
each have dimensions of NTK cubed:
[R] ≡[\
ddIH] = [ bH]3 [z]−2 ,
[S] = [T] = [U] ≡[ \
ddIIH] = [ bH]3 [z]−2 .
(∞.20)
This means that the proper dimensionless combinations for the ddNTKs are
R(ℓ)K(ℓ)
n
 Θ(ℓ)3 ∼1
n
1
ℓ
pR+p0−3pΘ
,
S(ℓ)K(ℓ)
n
 Θ(ℓ)3 ∼1
n
1
ℓ
pS+p0−3pΘ
,
(∞.21)
T (ℓ)K(ℓ)
n
 Θ(ℓ)3 ∼1
n
1
ℓ
pT +p0−3pΘ
,
U(ℓ)K(ℓ)
n
 Θ(ℓ)3 ∼1
n
1
ℓ
pU+p0−3pΘ
,
(∞.22)
where to get a normalized ratio, we multiplied by the kernel K(ℓ) to account for the [z]−2
and then divided by the three factors of the frozen NTK Θ(ℓ). Here, p0 is the critical
exponent for the kernel, and pΘ is the critical exponent for the NTK.
Finally, as a brief aside, let us comment on one aspect of dimensionality that we’ve
been ignoring but will soon become important. In particular, since the network output
z is set to the true output y, they should really have the same dimensions:
[z] = [y] .
(∞.23)
However, while the leading depth scaling of the preactivations is given by the kernel,
E
h
z(ℓ)z(ℓ)i
= K(ℓ) ∼
1
ℓ
p0
,
(∞.24)
the true output y is ﬁxed and doesn’t scale with depth. When comparing the performance
of networks of diﬀerent depths, this suggests that it might be helpful to rescale the ﬁnal
network outputs as
zi;δ →zi;δ
 1
L
−p0/2
,
(∞.25)
eﬀectively ﬁxing the scaling of the overall network output as p0 = 0, or equivalently
rescale the training outputs as
yi;˜α →yi;˜α
 1
L
p0/2
.
(∞.26)
We will further see how this can aﬀect the predictions of a fully-trained network on its
test set in §∞.2.3.1
1For regression tasks, where we want to learn a vector of real numbers, this rescaling is appropriate.
For classiﬁcation tasks, where we want to learn a discrete probability distribution, we should rescale
either the network outputs (before any softmax layer) or the raw output targets yi;δ (again before any
softmax layer) as in (10.37).
342

K⋆= 0 Universality Class
For the K⋆= 0 universality class, remember that we Taylor expanded the activation
function as
σ(z) =
∞
X
p=0
σp
p! zp ,
(∞.27)
and deﬁned the following Taylor coeﬃcient for convenience
a1 ≡
σ3
σ1

+ 3
4
σ2
σ1
2
,
(∞.28)
and required that all activation functions in this class satisfy σ0 = 0 and σ1 ̸= 0.
To solve our single input ddNTK recursions, (∞.14)–(∞.17), you’ll have to evaluate
a few new Gaussian expectations, taking particular note of that some of them now
depend on the third derivative of the activation function. Finally, to tune to K⋆= 0
criticality (5.90), we need to set the initialization hyperparameters as Cb = 0 and CW =
1/σ2
1; to implement the learning rate equivalence principle, we need to set our training
hyperparameters as (9.95),
λ(ℓ)
b
= eλb
1
ℓ
p⊥
Lp⊥−1 ,
λ(ℓ)
W = eλW
L
ℓ
p⊥−1
.
(∞.29)
For simplicity, let us also focus on odd activation functions, such as tanh, for which
importantly σ2 = 0 and p⊥= 1.
Inspecting the single-input ddNTK recursions (∞.14)–(∞.17), we see that they de-
pend on Gaussian expectations of preactivations as well as our previous solutions for
all the other objects that we’ve considered: the NTK variance, the NTK-preactivation
cross correlation, and the dNTK-preactivation cross correlation. Substituting in the so-
lutions for all these quantities as needed – you’ll have to ﬂip around to ﬁnd them, though
most were reprinted in §11.3.2 for the dNTK analysis – we can ﬁnd solutions to all the
single-input ddNTK recursions:
R(ℓ) = −ℓ2
48
"
3eλb + 4
eλW σ2
1
(−a1)
# "
eλb +
eλW σ2
1
(−a1)
#2
(−a1) + . . . ,
(∞.30)
S(ℓ) = ℓ2
12
"
eλb +
eλW σ2
1
(−a1)
#2
eλW σ2
1 + . . . ,
(∞.31)
T (ℓ) = ℓ2
32
"
eλb +
eλW σ2
1
(−a1)
#
(−a1)eλ2
b + . . . ,
(∞.32)
U(ℓ) =1
2
"
eλb +
eλW σ2
1
(−a1)
#3
(−a1) + . . . .
(∞.33)
From these, we can read oﬀthe critical exponents as pR = pS = pT = −2, and pU = 0,
343

and we see that the dimensionless ratios are given by
R(ℓ)K(ℓ)
n
 Θ(ℓ)3 = −1
48
"
3eλb + 4eλW σ2
1/(−a1)
eλb + eλW σ2
1/(−a1)
#
ℓ
n + . . . ,
(∞.34)
S(ℓ)K(ℓ)
n
 Θ(ℓ)3 = 1
12
"
eλW σ2
1/(−a1)
eλb + eλW σ2
1/(−a1)
#
ℓ
n + . . . ,
(∞.35)
T (ℓ)K(ℓ)
n
 Θ(ℓ)3 = 1
32
"
eλb
eλb + eλW σ2
1/(−a1)
#2 ℓ
n + . . . ,
(∞.36)
U(ℓ)K(ℓ)
n
 Θ(ℓ)3 = 1
2
1
ℓn + . . . .
(∞.37)
This means that R(ℓ), S(ℓ), and T (ℓ) all scale according to our leading eﬀective theory
cutoﬀas
pR + p0 −3pΘ = −1
pS + p0 −3pΘ = −1 ,
pT + p0 −3pΘ = −1 .
(∞.38)
Thus, we see that the leading-order ﬁnite-width dynamics of K⋆= 0 activation functions
have contributions from the ﬁrst ddNTK, \
ddIH, via R(ℓ), and from the second ddNTK,
\
ddIIH, via S(ℓ) and T (ℓ).2
Scale-Invariant Universality Class
Perhaps the most important fact to remember about nonlinear scale-invariant activation
functions (2.13),
σ(z) =
(
a+z ,
z ≥0 ,
a−z ,
z < 0 ,
(∞.39)
with a+ ̸= −a−, is that they are not smooth: their ﬁrst derivative is a step function
centered at the origin, and their second derivative is a Dirac delta function, (2.32),
σ′(z) =
(
a+ ,
z ≥0 ,
a−,
z < 0 , ,
σ′′(z) = (a+ −a−)δ(z) ,
(∞.40)
and higher derivatives will involve derivatives of the Dirac delta function. Inspecting
again the single-input ddNTK recursions (∞.14)–(∞.17), the kink in these activation
functions and the presence of Gaussian expectations with up to three derivatives of σ(z)
should scare you, especially if you heeded our warning at the end of §5.5. In fact, if you
formally try to evaluate some of these expectations, particularly ⟨σ′′σ′′⟩K and others
related to it via integration by parts, you’ll ﬁnd that they want to blow up, even if you
2If we relaxed the restriction for the activation function to be odd, we’d ﬁnd the same scalings for
R(ℓ), S(ℓ), and T (ℓ) – though with diﬀerent coeﬃcients – and we’d ﬁnd that U (ℓ) was up by a factor of
ℓ, but still subleading overall.
344

use all the magic tricks from physics that you might have at your disposal for trying to
make sense of divergent integrals.3
The divergence of these Gaussian correlators is actually telling us that there is some-
thing very wrong with our expansions for the updates to the preactivations (∞.4), the
NTK (∞.6), and the dNTK (∞.9). In particular, the Taylor expansion in the global
learning rate η breaks down for these non-smooth activation functions and doesn’t ac-
curately describe how a network is updated. As a result, our approach for solving the
ﬁnite-width dynamics will not work for nonlinear scale-invariant activation functions.
To understand why, let’s consider an extremely simple model of a network, a single
neuron with a bias:
z(x) = σ(x + b) .
(∞.41)
Here, the input x and the output z are both scalars, and for the activation function σ(z)
we’ll pick the ReLU, with a+ = 1 and a−= 0. Accordingly, for a particular input x such
that x + b > 0, the activation ﬁres, and the output is z(x) = x + b; for a particular input
x′ such that x′ + b < 0, the activation doesn’t ﬁre, and the output is z(x′) = 0.
Now, let’s consider a gradient-descent update to the parameters with a training
example x > −b such that the activation ﬁres. Then, the bias updates as
d¯b = −ηdz
dbϵ = −ηϵ = O(η) ,
(∞.42)
where ϵ is the error factor of the loss, depending on the true output y. Now, if x+b+d¯b >
0, then the change in the output is
d¯z = −ηϵ = O(η) ,
(∞.43)
and would be perfectly described by our Taylor expansion (∞.4). However, if the training
error is large enough such that x + b + d¯b = x + b −ηϵ < 0, then the activation turns oﬀ
and
d¯z = −x −b = O(1) .
(∞.44)
Importantly, this update is independent of our expansion parameter η, and a Taylor
expansion in η cannot detect this discontinuity at η = (x + b)/ϵ.
Thus, for the ReLU, any time an activation crosses its ﬁring threshold it can contribute
to the gradient-descent update in an η-independent way.
Empirically, if you try to
measure the NTK update for a deep MLP consisting of ReLU activation functions, you
don’t ﬁnd anything consistent with the expansion (∞.6). Instead, for the appropriately
normalized quantity, you’ll ﬁnd an ∼1/√n scaling with width and a linear scaling
with depth ℓ, in contrast to the ℓ/n scaling expected from our perturbative formalism.4
3We were somewhat lucky in §11.3.1 when analyzing the dNTK: all the higher-derivative Gaussian
integrals that we needed simpliﬁed via integration by parts and gave ﬁnite – in fact, vanishing – answers,
cf. (11.67) and (11.69).
4It’s temping to think that this ∼1/√n scaling arises from accumulating the probabilities that one
of the Ln total hidden-layer n ≫1 activations experiences an η-independent O(1) change after the
gradient-descent step.
345

From this we reach the unfortunate conclusion that we’ll have to give up on describing
the ﬁnite-width dynamics of nonlinear scale-invariant activation functions using these
methods.
Note that everything we have discussed for nonlinear scale-invariant activation func-
tions with respect to criticality and the inﬁnite-width training dynamics is perfectly ﬁne
and experimentally validated, and the presence of a nonzero dNTK at ﬁnite width is
still indicative of a dynamical NTK and representation learning at ﬁnite width. The
problem we just described only aﬀects the analysis we’re going to perform in the fol-
lowing section for the training dynamics of ﬁnite-width networks, and the breakdown of
the Taylor expansion just means that we will be unable to give a quantitative picture of
representation learning for these non-smooth activation functions.5 So, if you do want
to understand this, you’ll probably need an entirely new approach.
This leaves us one activation function in the entire scale-invariant universality class:
the linear activation function used for deep linear networks.
Tuning to criticality,
Cb = 0 and CW = 1, which ﬁxes χ = 1, and choosing layer-independent learning rates
(9.94) as
λ(ℓ)
b
=
eλb
L ,
λ(ℓ)
W =
eλW
L ,
(∞.45)
we can solve the single-input ddNTK recursions (∞.14)–(∞.17). Note that for every
term in the U-recursion, (∞.17), and for every term but one in the R-recursion, (∞.14),
the Gaussian expectations of activations involve second derivatives or third derivatives,
which vanish for a linear function. For the one term in the R-recursion that does not
vanish, it is also multiplied by P (ℓ), which vanishes for all scale-invariant activations,
cf. (11.74). This means that
R(ℓ) = 0 ,
U(ℓ) = 0 ,
(∞.46)
and in particular that the ﬁrst ddNTK, \
ddIH, does not contribute to the deep linear
network’s dynamics at O(1/n) since it’s entirely determined by R(ℓ). However, for the
S-recursion, (∞.15), and the T-recursion, (∞.16), we ﬁnd something nontrivial: these
recursions can be exactly solved as
S(ℓ) =ℓ2(ℓ2 −1)
12L3
(eλb + eλW K⋆)2eλW ,
(∞.47)
T (ℓ) =ℓ2(ℓ2 −1)
12L3
(eλb + eλW K⋆)eλ2
W K⋆,
(∞.48)
from which we see that the critical exponents are pS = pT = −4. Finally, the dimen-
5However, our analysis applies to any of the smoothed versions of the ReLU that permit a type of
criticality, cf. our criticality discussion of the SWISH and GELU in §5.3.4.
In particular, the training
dynamics we’ll work out in the next section describe these networks. These dynamical solutions, in
conjunction with the output-layer solutions to all their associated recursions, will accurately characterize
such fully-trained ReLU-like networks in practice.
346

sionless ratios are
S(ℓ)K(ℓ)
n
 Θ(ℓ)3 = 1
12
"
eλW K⋆
eλb + eλW K⋆
#
ℓ
n + . . . ,
(∞.49)
T (ℓ)K(ℓ)
n
 Θ(ℓ)3 = 1
12
"
eλW K⋆
eλb + eλW K⋆
#2 ℓ
n + . . . ,
(∞.50)
and we see that these scale according to our leading eﬀective theory cutoﬀas
pS + p0 −3pΘ = −1 ,
pT + p0 −3pΘ = −1 .
(∞.51)
In conclusion, we see that the second ddNTK, \
ddIIH, contributes via S(ℓ) and T (ℓ) to
the dynamics of deep linear networks at leading order.
∞.2
Training at Finite Width
Now that we understand the joint statistics of the preactivations, the NTK, the dNTK,
and the ddNTKs, we have nearly all the tools we need in order to evaluate the distribution
of fully-trained networks at ﬁnite width and nonzero depth. To see why, recall our ﬁnite-
width expansion of the network output evolution (∞.4)
z(L)
i;δ (t = 1) = z(L)
i;δ −η
X
j,˜α
bH(L)
ij;δ˜αϵ(L)
j;˜α + η2
2
X
j1,j2,˜α1,˜α2
d
dH
(L)
ij1j2;δ˜α1 ˜α2ϵ(L)
j1;˜α1ϵ(L)
j2;˜α2
(∞.52)
−η3
6
X
j1,j2,j3,˜α1,˜α2,˜α3
\
ddIH
(L)
ij1j2j3;δ˜α1 ˜α2 ˜α3ϵ(ℓ)
j1;˜α1ϵ(ℓ)
j2;˜α2ϵ(ℓ)
j3;˜α3 + O
 1
n2

.
Importantly, all the quantities on the right-hand side of the network update (∞.52) –
z(L)
i;δ , bH(L)
ij;δ˜α, ϵ(L)
j;˜α, d
dH
(L)
ij1j2;δ˜α1 ˜α2, and \
ddIH
(ℓ)
ij1j2j3;δ˜α1 ˜α2 ˜α3 – are evaluated at initialization
and thus are determined completely by the statistics of the joint preactivation-NTK-
dNTK-ddNTKs distribution,
p

z(L), bH(L), d
dH
(L), \
ddIH
(L), \
ddIIH
(L)D

,
(∞.53)
that we spent the majority of this book evaluating in detail. Accordingly – just as we did
before at inﬁnite width (§10) – we could use the joint distribution (∞.53) to compute
the statistics of the fully-trained network outputs after the update (∞.52), if we tuned
a single step of gradient descent for each realization of a network so that we landed on
the minimum of the training loss z(L)
i;˜α (t = 1) = yi;˜α.
More generally, if we trained the network with T steps of gradient descent such that
z(L)
i;˜α (t = T) = yi;˜α ,
for all ˜α ∈A ,
(∞.54)
347

and if we determined how to express the output of such a fully-trained network for
general inputs δ ∈D as a functional of our statistical variables at initialization,
z(L)
i;δ (T) ≡
h
z(L)
i;δ (t = T)
i
z(L), bH(L), d
dH
(L), \
ddIH
(L), \
ddIIH
(L)
,
(∞.55)
then we could give an analytical expression for the distribution of fully-trained network
outputs:
p

z(L)(T)

.
(∞.56)
The equation (∞.54) is our fully-trained condition, and the distribution (∞.56) com-
pletely describes the ensemble of ﬁnite-width networks at the end of training.
The
theoretical understanding of this distribution is exactly the goal we set for ourselves at
the beginning of this book in §0. The only thing left for us to work out is the functional
(∞.55); to do so, we ﬁrst need to ﬁgure out what kind of steps to take in order to fully
train these ﬁnite-width networks.
Now, recall from §10.2.2 that in the inﬁnite-width limit the fully-trained network
solution (∞.55) had an algorithm independence: the distribution at the end of training
didn’t depend on the details of the optimization algorithm, and thus we could perform
our theoretical analysis with any algorithm we wanted.
In contrast, at ﬁnite width
the fully-trained solution (∞.55) will have an algorithm dependence: diﬀerent fully-
trained solutions will make diﬀerent test-set predictions depending on the details of the
particular optimization algorithm used to train the network, even when holding ﬁxed
the statistical variables at initialization and their associated initialization and training
hyperparameters. Encouragingly, the form of the solutions will nonetheless take a uni-
versal form, with the non-universal details of the particular training algorithm captured
by six projective tensors: cf. (∞.154). Thus, we will be able to very generally study the
distribution of fully-trained networks at ﬁnite width by working out such solutions.
With that in mind, in this section we’ll present fully-trained solutions for two diﬀerent
optimization algorithms. First, in §∞.2.1 we’ll take two Newton-like steps in order to
satisfy our fully-trained condition (∞.54). While practically infeasible for large training
sets, this training algorithm is rich in pedagogical value, emphasizing the way in which
a ﬁnite-width network needs to adapt its representation to the training data in order to
minimize its training error. Then, in §∞.2.2 we’ll analytically solve the dynamics of the
vanilla gradient descent at order 1/n and obtain a slightly diﬀerent ensemble of fully-
trained ﬁnite-width networks. This algorithm is not only practically implementable, but
also quite often used to optimize real neural networks, and our corresponding solution is
an actual theoretical description of such fully-trained networks. Together, these solutions
will help us understand the ways in which the details of the optimization algorithm
can aﬀect the corresponding fully-trained solution.
Finally, in §∞.2.3 we’ll be able
to generally analyze the predictions of these diﬀerent fully-trained networks on novel
examples from the test set.
Throughout this section, we will declutter the notation a bit by dropping the layer
indices, since to understand training we only need to focus on the network output at
layer ℓ= L.
348

An Inﬁnite-Width Giant Leap at Finite Width
Before we begin, let’s ﬁrst review the giant leap that we took in §10.2 at inﬁnite width.
From the ﬁner-grained perspective of ﬁnite width, we’ll see that our leap actually missed
the minimum, exhibiting training errors of order 1/n. However, our new eyes on this
leap will be instructive, as they will help us see how we can correct for these errors and
reduce the ﬁnite-width training error even further.
Recall from §10.2 that, in order to fully train an inﬁnite-width network in a single
step, we needed to make a second-order update of the form
d¯θµ = −
X
ν,˜α1,˜α2,i
ηλµνκ˜α1 ˜α2 dzi;˜α1
dθν
(zi;˜α2 −yi;˜α2) ,
(∞.57)
which we interpreted either (i) as a generalized training algorithm (10.19) optimizing the
standard MSE loss (10.5), or (ii) as a standard (tensorial) gradient-descent step (7.11)
optimizing a generalized MSE loss (10.22). Here, κ˜α1 ˜α2 was called the Newton tensor,
and – in the ﬁrst understanding of (∞.57) – could be interpreted as allowing us to take
anisotropic steps in training sample space.
With this type of parameter update, a ﬁnite-width network output will evolves as
zi;δ(t = 1)
(∞.58)
=zi;δ −η
X
˜α1,˜α2
Hδ˜α1κ˜α1 ˜α2 (zi;˜α2 −yi;˜α2) −η
X
j,˜α1,˜α2
d
∆Hij;δ˜α1κ˜α1 ˜α2 (zj;˜α2 −yj;˜α2)
+ η2
2
X
j1,j2,˜α1,˜α2,˜α3,˜α4
d
dHij1j2;δ˜α1 ˜α2κ˜α1 ˜α3κ˜α2 ˜α4(zj1;˜α3 −yj1;˜α3)(zj2;˜α4 −yj2;˜α4)
−η3
6
X
j1,j2,j3,˜α1,...,˜α6
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3κ˜α1 ˜α4κ˜α2 ˜α5κ˜α3 ˜α6
× (zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6) + O
 1
n2

,
where here we’ve made our usual decomposition of the NTK into a mean and ﬂuctuation
bHij;δ˜α ≡δijHδ˜α + d
∆Hij;δ˜α .
(∞.59)
In terms of such an update, our fully-trained condition (∞.54) after a single step T = 1,
zi;˜α(t = 1) = yi;˜α ,
for all ˜α ∈A ,
(∞.60)
349

can be written as
zi;˜α −yi;˜α
(∞.61)
=η
X
˜α1,˜α2
H˜α˜α1κ˜α1 ˜α2 (zi;˜α2 −yi;˜α2) + η
X
j,˜α1,˜α2
d
∆Hij;˜α˜α1κ˜α1 ˜α2 (zj;˜α2 −yj;˜α2)
−η2
2
X
j1,j2,˜α1,˜α2,˜α3,˜α4
d
dHij1j2;˜α˜α1 ˜α2κ˜α1 ˜α3κ˜α2 ˜α4(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
+ η3
6
X
j1,j2,j3,˜α1,...,˜α6
\
ddIHij1j2j3;˜α˜α1 ˜α2 ˜α3κ˜α1 ˜α4κ˜α2 ˜α5κ˜α3 ˜α6
× (zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6) + O
 1
n2

.
This new giant-leap condition (∞.61) perhaps seems a little daunting, and further it’s
not obvious that there’s any particular choice of Newton tensor κ˜α1 ˜α2 that can land us
on the minimum. Nonetheless, we do expect that our inﬁnite-width solution should be
near the true ﬁnite-width solution, up to errors of order O(1/n).6
With that in mind, as a ﬁrst step let’s try our inﬁnite-width giant leap (10.30) and
see where we land. This inﬁnite-width giant leap had an interpretation as Newton’s
method and was given by the second-order update (∞.57), with a particular choice of
the product of the global learning rate and the Newton tensor,
ηκ˜α1 ˜α2 = eH ˜α1 ˜α2 ,
(∞.62)
where the inverse NTK mean submatrix eH ˜α1 ˜α2 was deﬁned implicitly via
X
˜α2∈A
eH ˜α1 ˜α2 eH˜α2 ˜α3 = δ ˜α1
˜α3 .
(∞.63)
As always, the tilde on eH˜α1 ˜α2 emphasizes that it’s an NA × NA-dimensional submatrix
of the NTK mean evaluated on pairs of training inputs only. By now this distinction
should be familiar enough that we will stop belaboring it.
More importantly, here we’ve used the inverse of the full NTK mean eH˜α1 ˜α2 rather
than the inﬁnite-width frozen NTK eΘ˜α1 ˜α2.
To explain why, let us recall from (9.3)
that the NTK mean receives a series of corrections at each order in the 1/n expansion,
of which the leading-order piece is the frozen NTK (9.4). Since we’re now working at
order 1/n, we should in particular take into account the next-to-leading-order (NLO)
1/n correction to the NTK mean H{1}
˜α1 ˜α2 by working with eH˜α1 ˜α2 instead of eΘ˜α1 ˜α2.7
6For deep networks, we know that technically the corrections will be of order O(L/n), corresponding
to the cutoﬀscale of our eﬀective theory description.
7While we won’t show it explicitly, we expect that this NLO NTK mean, H{1}(ℓ)
˜α1 ˜α2 , will have a solution
that scales like O(1/n) as compared to the frozen NTK; this would be analogous to what we found for
the NLO metric G{1}(ℓ)
˜α1 ˜α2 , cf. the discussion in §5.4 after (5.143). In particular, if we make a 1/n expansion
for our training hyperparameters λ(ℓ)
b
and λ(ℓ)
W as we did for our initialization hyperparameters in (5.138)
350

Substituting our inﬁnite-width giant-leap Newton tensor (∞.62) into our giant-leap
condition (∞.61) at ﬁnite width and rearranging, we get
0 =
X
j,˜α1,˜α2
d
∆Hij;˜α˜α1 eH ˜α1 ˜α2(zj;˜α1 −yj;˜α1)
(∞.64)
−1
2
X
j1,j2,
˜α1,...,˜α4
d
dHij1j2;˜α˜α1 ˜α2 eH ˜α1 ˜α3 eH ˜α2 ˜α4(zj1;˜α3 −yj1;˜α3)(zj2;˜α4 −yj2;˜α4)
+ 1
6
X
j1,j2,j3,
˜α1,...,˜α6
\
ddIHij1j2j3;˜α˜α1 ˜α2 ˜α3 eH ˜α1 ˜α4 eH ˜α2 ˜α5 eH ˜α3 ˜α6
× (zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6) + O
 1
n2

.
Thus, we actually missed the minimum: for the network to be fully trained, the right-
hand side of (∞.64) should have vanished, while here it is clearly nonzero in general.
Taking a step back, it’s clear now that what we actually found at inﬁnite width in §10.2
was
zi;˜α(t = 1) −yi;˜α = O
 1
n

.
(∞.65)
In other words, our networks were fully trained only at leading order, and (∞.64) gives
an explicit expression for the 1/n correction.
Disentangling a little further, there are two such corrections at order 1/n: the ﬁrst
correction – the ﬁrst term on the right-hand side of (∞.64) – arises from the instantiation-
to-instantiation ﬂuctuations of the NTK across diﬀerent realizations of the biases and
weights at initialization; the second correction – comprised of the second and third
terms on the right-hand side of (∞.64) – arises from nonlinear changes in the output
as we take our step. In particular, this second correction is a bona ﬁde manifestation
of representation learning at ﬁnite width, accounting for the fact that the network’s
eﬀective features are evolving. If we properly account for these two types of the 1/n
corrections, we should be able to attain a training error of order 1/n2:
zi;˜α(T) −yi;˜α = O
 1
n2

.
(∞.66)
That is, we should be able to improve our eﬀective theory of fully-trained networks,
quantitatively by another multiplicative factor of 1/n, and qualitatively by properly
including representation learning into such an eﬀective description.
and (5.139), then we will have extra freedom in the subleading hyperparameters λ(ℓ){1}
b
and λ(ℓ){1}
W
to
eliminate the growing-in-ℓcontribution to H{1}(ℓ)
˜α1 ˜α2 . Overall, this will make the NLO correction to the
NTK mean scale as O(1/n), subleading to the leading ﬁnite-width eﬀects which scale as O(L/n): in the
language of RG ﬂow, the NLO NTK mean is marginal. In practice, such a contribution is negligible and
can thus be neglected for networks of any real depth.
351

Our ﬁrst approach (§∞.2.1) to attain such eﬀectively-zero training error, (∞.66), is
to continue to engineer theoretical giant leaps so as to account for both the instantiation-
to-instantiation ﬂuctuations of the NTK and the eﬀect of the dNTK and the ﬁrst ddNTK.
Another approach (§∞.2.2) is to simply use the vanilla tensorial gradient descent
algorithm as we do in practice; in that case, we will have to not only account for the
dynamics of the network output, but also account for the dynamics of the NTK and
dNTK. After doing so, we will see that we can iteratively decrease the training loss to
zero after many many such steps.
∞.2.1
A Small Step Following a Giant Leap
Here we’ll train our networks with two second-order updates. For the ﬁrst update, a giant
leap, we’ll need to further generalize our theoretical optimization algorithm in order to
properly account for the instantiation-to-instantiation ﬂuctuations of the NTK. In the
second update, a small step, we’ll be able to account for the 1/n change in representation
due to the nonzero dNTK and ddNTK, ultimately landing on the minimum of the
loss as (∞.66). In particular, we can think of these updates as loosely corresponding
to distinct phases of training that arise when implementing gradient-based training of
neural networks in practice.
First Update: One Final Generalization of Gradient Decent
Please ﬂip back to take a closer look at our unsatisﬁed condition for fully training our
networks (∞.64). Right away, you should notice a serious problem in satisfying this
constraint: the NTK ﬂuctuation, d
∆Hij;˜α˜α1, the dNTK, d
dHij1j2;˜α˜α1 ˜α2, and the ddNTK,
\
ddIHij1j2j3;˜α˜α1 ˜α2 ˜α3, all mix diﬀerent output components together in an update; i.e. the j-
th component of the prediction error zj;˜α−yj;˜α at initialization aﬀects the i-th component
of the output zi;δ after the update, even for i ̸= j. This stands in contrast to what we
found for an inﬁnite-width update in §10.1.1, where there was no such mixing or wiring of
output components. Meanwhile, we did see a similar wiring eﬀect for Bayesian inference
at ﬁnite width as we discussed in depth in §6.4.2.
While such wiring at ﬁnite width is fantastic from a practitioner’s standpoint, it
makes our theoretical work slightly more complicated. In particular, in order to satisfy
the training constraint (∞.64), we will need to further generalize our second-order update
(∞.57). Following in the footsteps of our previous two generalizations, (7.11) and (10.18),
let’s make one ﬁnal generalization
η →ηλµν →ηλµνκ˜α1 ˜α2 →ηλµνκ˜α1 ˜α2
ij
,
(∞.67)
with the ﬁnal form of our theoretical update given by
d¯θµ = −
X
ν,˜α1,˜α2,i,j
ηλµνκ˜α1 ˜α2
ij
dzi;˜α1
dθν
ϵj;˜α2 .
(∞.68)
352

Here, we also introduced a further generalized Newton tensor κ˜α1 ˜α2
ij
with output-component
indices. This ﬂexibility will allow us to resolve the mixing of the output components in
the residual training error from our inﬁnite-width leap (∞.64).8
Note importantly that the µ, ν indices of λµν are very diﬀerent from the i, j indices
of κ˜α1 ˜α2
ij
: the former each runs over all P parameters, while the latter each only runs
over the nL components of the network output. In particular, while learning-rate tensor
λµν lets us control how the gradient of the ν-th parameter aﬀects the update to the µ-th
parameter, the i, j indices of the generalized Newton tensor κ˜α1 ˜α2
ij
instead control how
the network’s features (10.138) dzi;˜α1/dθν are combined with the error factor ϵj;˜α2 in
order to make the update.9 Allowing for a κ˜α1 ˜α2
ij
with nonzero oﬀ-diagonal components
in the i, j indices, we can precisely tune the wiring or mixing of output components that
occurs in a particular update.
Finally, plugging our ﬁnal-form second-order update (∞.68) back into the 1/n ex-
pansion of the network update (∞.4) and using the NTK, dNTK, and ﬁrst ddNTK
deﬁnitions, we can see how the network output changes after making an update with
this new optimization algorithm:
zi;δ(t = 1)
(∞.70)
=zi;δ −η
X
j,˜α1,˜α2
Hδ˜α1κ˜α1 ˜α2
ij
(zj;˜α2 −yj;˜α2) −η
X
j,k,˜α1,˜α2
d
∆Hij;δ˜α1κ˜α1 ˜α2
jk
(zk;˜α2 −yk;˜α2)
+ η2
2
X
j1,j2,k1,k2,
˜α1,˜α2,˜α3,˜α4
d
dHij1j2;δ˜α1 ˜α2κ˜α1 ˜α3
j1k1 κ˜α2 ˜α4
j2k2 (zk1;˜α3 −yk1;˜α3) (zk2;˜α4 −yk2;˜α4)
−η3
6
X
j1,j2,j3,k1,k2,k3
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3κ˜α1 ˜α4
j1k1 κ˜α2 ˜α5
j2k2 κ˜α3 ˜α6
j3k3
× (zk1;˜α4 −yk1;˜α4) (zk2;˜α5 −yk2;˜α5) (zk3;˜α6 −yk3;˜α6) + O
 1
n2

.
Here, we’ve again used the standard MSE loss, for which the error factor is given by
8Similarly to the generalized MSE loss (10.22) discussed in §10.2.1, we can alternatively think of
this further-generalized second-order update (∞.68) optimizing the standard MSE loss as instead arising
from a standard ﬁrst-order (tensorial) gradient descent update (7.11) on a further-generalized MSE loss,
LA, κ(θ) ≡1
2
nL
X
i1,i2=1
X
˜α1,˜α2∈A
κ˜α1 ˜α2
i1i2 (zi1;˜α1 −yi1;˜α1) (zi2;˜α2 −yi2;˜α2) ,
(∞.69)
as long as the Newton tensor κ˜α1 ˜α2
i1i2
is also assumed to be symmetric under the exchange of paired indices
(i1, ˜α1) ↔(i2, ˜α2). This symmetry will in fact be present for both our ﬁrst update, the giant leap, and
our second update, the small step. With this interpretation (∞.69), our generalized Newton tensor κ˜α1 ˜α2
ij
acts as a metric on sample space, through its sample indices ˜α1, ˜α2, and on output-component space,
through its L-th layer neural indices i, j.
9Note that when we developed our interpretation of an inﬁnite-width network as a linear model in
§10.4, we made a similar distinction between parameter indices µ and output-component indices i in
(10.138) when deﬁning the random feature functions bφi,µ(x).
We also discussed how wiring can be
incorporated into a non-minimal model of representation learning in §11.4.3.
353

the residual training error ϵj;˜α = zj;˜α −yj;˜α. Now, we’ll need to pick our generalized
Newton tensor κ˜α1 ˜α2
ij
judiciously in order to make a ﬁrst update that fully accounts for
instantiation-to-instantiation ﬂuctuations of particular networks in our ensemble.
Taking inspiration from our 1/n-failed inﬁnite-width Newton’s step (∞.62), let’s take
a similar-looking ﬁrst step according to
ηκ˜α1 ˜α2
ij
=

bH−1˜α1 ˜α2
ij
(∞.71)
=δij eH ˜α1 ˜α2 −
X
˜α3,˜α4∈A
eH ˜α1 ˜α3 d
∆Hij;˜α3 ˜α4 eH ˜α4 ˜α2
+
nL
X
k=1
X
˜α3,...,˜α6∈A
eH ˜α1 ˜α3 d
∆Hik;˜α3 ˜α4 eH ˜α4 ˜α5 d
∆Hkj;˜α5 ˜α6 eH ˜α6 ˜α2 + O

∆3
.
Here, we’ve introduced the complete inverse of the stochastic NTK sub-tensor evaluated
on the training set, satisfying
X
j,˜α2

bH−1˜α1 ˜α2
ij
bHjk;˜α2 ˜α3 = δikδ ˜α1
˜α3 ,
(∞.72)
and in the last equality of (∞.71) we’ve used the Schwinger-Dyson equations (4.55),
which is a physicist’s way of saying that we expanded the inverse of the stochastic NTK
around the NTK mean.10 The main diﬀerence between this new giant leap (∞.71) and
our previous inﬁnite-width giant leap (∞.62) is that we’re now taking into account the
instantiation-to-instantiation ﬂuctuations of the NTK across diﬀerent realizations of the
model parameters; in other words, we’re implementing a diﬀerent Newton step for each
particular network with its associated NTK bHi1i2;˜α1 ˜α2. Accordingly, this step can be
thought of as loosely corresponding to the ﬁrst phase of training for such a particular
network.
Taking this step, i.e. plugging this generalized Newton tensor (∞.71) into our update
to the network output (∞.70), we ﬁnd the training error decreases to
zi;˜α(t = 1) −yi;˜α
(∞.73)
=1
2
X
j1,j2,
˜α1,...,˜α4
d
dHij1j2;˜α˜α1 ˜α2 eH ˜α1 ˜α3 eH ˜α2 ˜α4(zj1;˜α3 −yj1;˜α3)(zj2;˜α4 −yj2;˜α4)
−1
6
X
j1,j2,j3,
˜α1,...˜α6
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 eH ˜α1 ˜α4 eH ˜α2 ˜α5 eH ˜α3 ˜α6
× (zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6) + O
 1
n2

.
10Despite saying that we wouldn’t belabor this any further, this is one of those unfortunate situations
where we had to decide between decorating the inverse

b
H−1˜α1 ˜α2
ij
with either a hat or a tilde, and
we went with the hat. Hopefully the tildes on the sample indices, e.g. ˜α1, ˜α2, will remind you that the
inverse of this stochastic object is taken only with respect to the training set. Note that at no point will
we ever need the inverse of the stochastic NTK evaluated on a general dataset D.
354

Thus, we’ve correctly taken care of the ﬁrst type of the 1/n corrections, and with this
step we’ve reduced the residual prediction error on the training set from the order-one
error of our initial prediction
zi;˜α(t = 0) −yi;˜α = O(1) ,
(∞.74)
to a much smaller error of
zi;˜α(t = 1) −yi;˜α = O
 1
n

,
(∞.75)
loosely corresponding to the empirically-large initial decrease of error when training
networks in practice.
Moreover, the rest of the error in (∞.73) is now entirely due
to the additional ﬁnite-width corrections encoded by the dNTK and ﬁrst ddNTK, a
consequence of representation learning.11
Second Update: Representation Learning Strikes Back
To reduce the training error further, we’ll need to update our network again to further
account for the fact that its representation evolved with the ﬁrst update. In other words,
we’ll need to make a second gradient-descent update. If you’d like, you can imagine
that this update corresponds to a second phase of training – following a ﬁrst phase
where the network signiﬁcantly decreased its training error with the NTK evaluated at
initialization – and so now the model must reﬁne its features in order to further improve
its performance.
Accordingly, since our training error is already down to ∼1/n, our second update is
going to be a lot smaller than our ﬁrst. In particular, the update itself will necessarily
only be of order 1/n so as to precisely cancel the remaining 1/n training error; that is,
it’s actually more of a small step than another giant leap.12
To determine which step we need to take, let’s write the network output after a
second update as
zi;δ(t = 2)
(∞.76)
=zi;δ(t = 1) −
X
j,k,˜α1,˜α2
Hij;δ˜α1(t = 1) ηκ˜α1 ˜α2
jk
(t = 1) [zk;˜α2(t = 1) −yk;˜α2] + O
 1
n2

=zi;δ(t = 1) −
X
j,˜α1,˜α2
Hδ˜α1 ηκ˜α1 ˜α2
ij
(t = 1) [zj;˜α2(t = 1) −yj;˜α2] + O
 1
n2

,
11In particular, this ﬁrst update (∞.71) would satisfy the training condition (∞.66) only if the NTK
were constant under gradient descent. There’s actually a name given to this type of phenomenon, lazy
training, referring to situations when the network function behaves as if it is equal to a linearzation
around the initial value of its parameters [70]. As we know from our discussion in §10.4, if the NTK is
constant, then the network is a linear model.
12As we will see, the overall learning rate is essentially the same for both updates; the second update
is only smaller because the gradient of the loss after the ﬁrst update is itself much smaller when near a
minimum.
355

where we left the second update’s product of global learning rate and generalized Newton
tensor ηκ˜α1 ˜α2
ij
(t = 1) unspeciﬁed for now. Note here that in the ﬁrst equality we dropped
the d(d)NTK terms from the update: since after our ﬁrst update (∞.73) the training
error has already decreased to O(1/n), the would-be dNTK term is very subleading
∼d
dH ×[ϵ(t = 1)]2 = O
 1/n3, and the would-be ddNTK term is ridiculously subleading
∼\
ddIH × [ϵ(t = 1)]3 = O
 1/n4. Similarly, on the third line we replaced the NTK after
the ﬁrst step Hij;δ˜α1(t = 1) by the NTK mean at initialization δijHδ˜α1. Given the 1/n-
suppressed training error after the ﬁrst step, this substitution can be justiﬁed for these
two reasons in conjunction: (i) the update to the NTK bHij;δ˜α(t = 1) −bHij;δ˜α(t = 0) is
itself suppressed by 1/n, cf. (∞.6), so we may use the version from before the update,
and (ii) the NTK ﬂuctuation is also suppressed compared to its mean, so we may then
swap the stochastic NTK at initialization for its mean. This means that if we make the
following choice for our small step second update
ηκ˜α1 ˜α2
ij
(t = 1) =δij eH ˜α1 ˜α2 + O
 1
n

,
(∞.77)
then it’s easy to see from (∞.76) that our network will now be fully trained as (∞.66):
zi;˜α(t = 2) −yi;˜α = O
 1
n2

.
(∞.78)
Thus, with our second update we were able to reduce the residual training error by an
additional factor of 1/n as compared to the error after the ﬁrst update (∞.73).13
For general inputs δ ∈D, plugging our choice of learning rate and Newton tensor
(∞.77) back into our second update (∞.76) and further re-expressing the network output
13This suggests that additional updates could continue to reduce the training error by additional factors
of 1/n. However, these further reﬁnements – determined in terms of the higher-order corrections to our
eﬀective theory description – will be qualitatively the same as our leading ﬁnite-width description at
O(L/n). In other words, improving an inﬁnite-width description to ﬁnite-width description incorporates
representation learning, while more precise ﬁnite-width descriptions just allow the model to make further
reﬁnements to its features. Practically speaking, we expect our leading ﬁnite-width description to be
very accurate for networks with reasonable values of the aspect ratio L/n.
356

after the ﬁrst step zi;δ(t = 1) by (∞.70) with (∞.71), we get
zi;δ(t = 2)
(∞.79)
=zi;δ −
X
˜α1,˜α2∈A
Hδ˜α1 eH ˜α1 ˜α2(zi;˜α2 −yi;˜α2)
+
nL
X
j=1
X
˜α1,˜α2∈A

d
∆Hij;δ˜α1 −
X
˜α3,˜α4∈A
Hδ˜α3 eH ˜α3 ˜α4 d
∆Hij;˜α4 ˜α1

eH ˜α1 ˜α2(zj;˜α2 −yj;˜α2)
−
nL
X
j,k=1
X
˜α1,...,˜α4∈A

d
∆Hij;δ˜α1 −
X
˜α5,˜α6∈A
Hδ˜α5 eH ˜α5 ˜α6 d
∆Hij;˜α6 ˜α1


× eH ˜α1 ˜α2 d
∆Hjk;˜α2 ˜α3 eH ˜α3 ˜α4(zk;˜α4 −yk;˜α4)
+ 1
2
nL
X
j1,j2=1
X
˜α1,...,˜α4∈A

d
dHij1j2;δ˜α1 ˜α2 −
X
˜α5,˜α6∈A
Hδ˜α5 eH ˜α5 ˜α6 d
dHij1j2;˜α6 ˜α1 ˜α2


× eH ˜α1 ˜α3 eH ˜α2 ˜α4(zj1;˜α3 −yj1;˜α3)(zj2;˜α4 −yj2;˜α4)
−1
6
X
j1,j2,j3=1
X
˜α1,...,˜α6∈A

\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 −
X
˜α7,˜α8∈A
Hδ˜α7 eH ˜α7 ˜α8\
ddIHij1j2j3;˜α8 ˜α1 ˜α2 ˜α3


× eH ˜α1 ˜α4 eH ˜α2 ˜α5 eH ˜α3 ˜α6 (zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+ O
 1
n2

.
Here, we see that the expressions in the square brackets vanish identically for all the
training inputs δ = ˜α ∈A: our network is thus fully trained. In particular, since all of
the variables on the right-hand side of (∞.79) are at initialization, this solution realizes
our goal (∞.55) of expressing the output of a fully-trained network as a functional of
such variables at initialization. Accordingly, the statistics of the fully-trained distribu-
tion (∞.56) can now be worked out from the joint preactivation-NTK-dNTK-ddNTKs
distribution (∞.53) at initialization.14
While this is all very exciting, let us also caution you that these generalized second-
order updates are probably best thought of as giving a simple theoretical model of a
training algorithm – designed to let us understand the training process analytically –
and by no means are we suggesting that they provide a good or useful option for practical
optimization. Instead, the most practical algorithm for optimization is vanilla ﬁrst-order
gradient descent. As we’ve already pointed out that the output of a fully-trained network
does depend on the details of the algorithm used for optimization, now we really have
no choice left other than to explicitly analyze many many steps of gradient descent.
∞.2.2
Many Many Steps of Gradient Descent
In this extended subsection, we’re going to study tensorial gradient descent (7.11) and
optimize ﬁnite-width neural networks according to the MSE loss with a constant global
14Alternatively, we could have reached this same solution in a single update if we had instead made a
357

learning rate η.15
With this progenitor-of-all-other-gradient-based-learning-algorithm
algorithm, the network output will evolve as (∞.4)
zi;δ(t + 1) =zi;δ(t) −η
X
j,˜α
Hij;δ˜α(t) [zj;˜α(t) −yj;˜α]
(∞.81)
+ η2
2
X
j1,j2,˜α1,˜α2
dHij1j2;δ˜α1 ˜α2(t) [zj1;˜α1(t) −yj1;˜α1] [zj2;˜α2(t) −yj2;˜α2]
−η3
6
X
j1,j2,j3,˜α1,˜α2,˜α3
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3
× [zj1;˜α1(t) −yj1;˜α1] [zj2;˜α2(t) −yj2;˜α2] [zj3;˜α3(t) −yj3;˜α3] ,
ﬁnely tuned giant leap, picking the generalized Newton tensor as
ηκ˜α1 ˜α2
ij
=δij e
H ˜α1 ˜α2 −
X
˜α3,˜α4∈A
e
H ˜α1 ˜α3 d
∆Hij;˜α3 ˜α4 e
H ˜α4 ˜α2
(∞.80)
+
nL
X
k=1
X
˜α3,...,˜α6∈A
e
H ˜α1 ˜α3 d
∆Hik;˜α3 ˜α4 e
H ˜α4 ˜α5 d
∆Hkj;˜α5 ˜α6 e
H ˜α6 ˜α2
+ 1
2
nL
X
k=1
X
˜α3,...,˜α6∈A
e
H ˜α1 ˜α3 e
H ˜α2 ˜α4 e
H ˜α5 ˜α6 c
dHijk;˜α3 ˜α4 ˜α5 (zk;˜α6 −yk;˜α6)
−1
6
nL
X
k1,k2=1
X
˜α3,...,˜α8∈A
e
H ˜α1 ˜α3 e
H ˜α2 ˜α4 e
H ˜α5 ˜α7 e
H ˜α6 ˜α8 \
ddIHijk1k2;˜α3 ˜α4 ˜α5 ˜α6
× (zk1;˜α7 −yk1;˜α7) (zk2;˜α8 −yk2;˜α8) .
In essence, the ﬁrst three terms of (∞.80) come from inverting the stochastic NTK, corresponding to our
ﬁrst-update giant leap (∞.71) and accounting for the instantiation-to-instantiation ﬂuctuations in the
NTK. In contrast, the ﬁnal two terms correspond to our second-update small step (∞.78) and accounts
for the dNTK-ddNTK-induced representation learning. Note that this generalized Newton tensor (∞.80)
is asymmetric under the exchange of paired indices (i, ˜α1) ↔(j, ˜α2) due to the last term, and thus this
ﬁnely-tuned update doesn’t admit an alternative interpretation of optimization with a further generalized
loss (∞.69).
This single update is analogous to the direct optimization solution (11.132) for quadratic regression in
§11.4. The very ﬁnely-tuned nature of this single-update algorithm (∞.80) suggests that it’s easier to
make the ﬁne adjustments required to reach a solution by taking many simpler steps rather than fewer
complicated steps. We will see the other side of this next in §∞.2.2 when we study training by many
many steps of vanilla gradient descent.
15Don’t let the word tensorial scare you here; this just means that we will allow for our training
hyperparameters λ(ℓ)
b
and λ(ℓ)
W as part of the deﬁnition of the NTK. We hope it’s already clear why
including these hyperparameters is a good idea – if not, please ﬂip back to §9.4 and reread the paragraphs
on the learning rate equivalence principle – and they are actually simple to include practically as part
of any optimization algorithm.
Moreover, as they are just part of the deﬁnition of the NTK, they have absolutely no consequence
on the dynamics presented here; i.e. our solution also covers non-tensorial gradient descent, though in
such a case you’d have diﬀerent asymptotic solutions for the statistics of the NTK, dNTK, and ddNTKs.
This is also why we think of the training hyperparameters λ(ℓ)
b
and λ(ℓ)
W as being independent from the
details of the optimization algorithm itself.
358

in conjunction the NTK will evolve as (∞.6)
Hi1i2;δ1δ2(t + 1)
(∞.82)
=Hi1i2;δ1δ2(t) −η
X
j,˜α

dHi1i2j;δ1δ2 ˜α(t) + dHi2i1j;δ2δ1 ˜α(t)

[zj;˜α(t) −yj;˜α]
+ η2
2
X
j1,j2,˜α1,˜α2

\
ddIHi1i2j1j2;δ1δ2 ˜α1 ˜α2 + \
ddIHi2i1j1j2;δ2δ1 ˜α1 ˜α2 + 2 \
ddIIHi1i2j1j2;δ1δ2 ˜α1 ˜α2

× [zj1;˜α1(t) −yj1;˜α1] [zj2;˜α2(t) −yj2;˜α2] ,
and in further conjunction the dNTK will evolve as (∞.9)
dHi0i1i2;δ0δ1δ2(t + 1)
(∞.83)
=dHi0i1i2;δ0δ1δ2(t)
−η
X
j,˜α

\
ddIHi0i1i2j;δ0δ1δ2 ˜α + \
ddIIHi0i1i2j;δ0δ1δ2 ˜α + \
ddIIHi0i2i1j;δ0δ2δ1 ˜α

[zj;˜α(t) −yj;˜α] .
In writing these dynamical equations, we’ve stopped explicitly denoting that our equa-
tions have O
 1/n2 errors, and we’ve also expressed the fact that the ddNTKs are
t-independent at order 1/n, using their values at initialization, \
ddIHi1i2i3i4;δ1δ2δ3δ4 and
\
ddIIHi1i2i3i4;δ1δ2δ3δ4, with hats to remind us of their stochasticity. These joint updates
(∞.81), (∞.82), and (∞.83) are coupled diﬀerence equations, and the equations for the
network output and the dynamical NTK are both nonlinear in the output zi;δ.
Although this seems daunting, we are now going to solve these equations in a closed
form. First, we’ll analyze the dynamics while neglecting the ﬁnite-width eﬀects of the
dNTK and ddNTKs, in which the problem will reduce to a single linear diﬀerence
equation. Then, we’ll use perturbation theory to incorporate the eﬀect of all the NTK
diﬀerentials and allow for a dynamically evolving NTK and dNTK.
Free Theory: Step-Independent NTK
Let’s begin by setting the dNTK and ddNTKs to zero. Since their leading statistics
are 1/n-suppressed, we’ll be able to use perturbation theory to reincorporate all their
eﬀects later. In this limit the NTK update equation (∞.82) is trivial, solved by the free
or step-independent NTK:
Hi1i1;δ1δ2(t) = Hi1i2;δ1δ2(t = 0) ≡bHi1i2;δ1δ2 .
(∞.84)
Unsurprisingly, this just means that when the dNTK and ddNTKs vanish, the NTK
doesn’t update from its initialization.
Plugging this solution into the preactivation update equation (∞.81) and turning oﬀ
the dNTK and ddNTKs, the remaining dynamical equation simpliﬁes to
zi;δ(t + 1) = zi;δ(t) −η
X
j,˜α
bHij;δ˜α[zj;˜α(t) −yj;˜α] .
(∞.85)
359

Thus, the residual training error, z˜α(t) −y˜α, sources the updates to the network output
zδ(t) for general inputs δ ∈D. Moreover, when restricted to the inputs from the training
set ˜α ∈A, we can rewrite this diﬀerence equation (∞.85) as
zi;˜α(t + 1) −yi;˜α =
X
j,˜α1

Iij;˜α˜α1 −η bHij;˜α˜α1

[zj;˜α1(t) −yj;˜α1] ,
(∞.86)
where we’ve deﬁned the identity operator
Ii1i2;˜α1 ˜α2 ≡δi1i2δ˜α1 ˜α2 .
(∞.87)
In this form, (∞.86) is a ﬁrst-order homogeneous linear diﬀerence equation for the resid-
ual training error, z˜α(t) −y˜α, which is just a fancy way of saying that this is going to be
a piece of cake.
In particular, the update to the prediction error is just a simple multiplication by a
constant matrix, and the solution is given by an exponential:
zF
i;˜α(t) −yi;˜α =
X
j,˜α1
Uij;˜α˜α1(t) (zj;˜α1 −yj;˜α1) .
(∞.88)
Here, on the left-hand side, we’ve labeled the solution with an “F” to indicate it’s the free
solution, with the nonlinear eﬀects from the dNTK and ddNTKs turned oﬀ; on the right-
hand side, we have the residual training error at initialization, and the step-evolution
operator is deﬁned as an iterative product of t steps:
Uiti0;˜αt ˜α0(t) ≡

I −η bH
t
iti0;˜αt ˜α0
(∞.89)
=
X
i1,...it−1
˜α1,...,˜αt−1

Iitit−1;˜αt ˜αt−1 −η bHitit−1;˜αt ˜αt−1

· · ·

Ii1i0;˜α1,˜α0 −η bHi1i0;˜α1 ˜α0

.
For any positive-deﬁnite NTK and suﬃciently small global learning rate η, this operator
will exponentially decay to zero, U(t) →0, as the number of steps becomes large,
t →∞.16 Thus, the residual training error (∞.88) will vanish exponentially quickly,
lim
t→∞zF
i;˜α(t) = yi;˜α ,
(∞.90)
with the step scale for the decay of the individual components set by the step-independent
NTK.17
16In particular, for this limit to converge we just need ||I −η b
H||∞< 1, i.e. the largest eigenvalue
of the operator I −η b
H must be less than one. With our attention to the principles of criticality and
equivalence, our choices of initialization and training hyperparameters were made so that the NTK is
always of order one, and thus it’s very easy for our networks to satisfy this constraint.
17If we wanted to study the ODE or continuum limit of the dynamics, we could take the global
learning rate to zero, η →0, while holding the product, τ ≡ηt, ﬁxed.
In such a limit, the step-
evolution operator becomes simply U(t) →exp(−b
Hτ). While such a limit is mostly unnecessary for
any theoretical purpose – it’s just as easy to study the discrete dynamics that actually describe the
360

Having now solved the free dynamics on the training set, we can plug this solu-
tion (∞.88) back into the diﬀerence equation (∞.85) for general inputs δ ∈D. With
the source known explicitly, we can easily write down a solution that satisﬁes the initial
condition zF
i;δ(t = 0) = zi;δ:
zF
i;δ(t) =zi;δ −
X
j,˜α
bHij;δ˜α
(
η
t−1
X
s=0
h
zF
j;˜α(s) −yj;˜α
i)
(∞.91)
=zi;δ −
X
j,˜α1,˜α2
bHij;δ˜α1aj;˜α(t) .
Here we’ve deﬁned a dynamical helper function, with an explicit representation given by
aj;˜α(t) ≡η
t−1
X
s=0
h
zF
j;˜α(s) −yj;˜α
i
= η
t−1
X
s=0

X
k,˜α1
Ujk;˜α˜α1(t) (zk;˜α1 −yk;˜α1)


(∞.92)
=η
X
k,˜α1
(t−1
X
s=0
h
I −η bH
si
jk;˜α˜α1
)
(zk;˜α1 −yk;˜α1)
=η
X
k,m,˜α1,˜α2
h
I −

I −η bH
i−1˜α˜α2
jm

I −

I −η bH
t
mk;˜α2 ˜α1
(zk;˜α1 −yk;˜α1)
=
X
m,˜α2

bH−1˜α˜α2
jm
n
zm;˜α2 −ym;˜α2 −
h
zF
m;˜α2(t) −ym;˜α2
io
.
In this expression, to get to the third line we used the standard formula for evaluating
geometric sums, 1 + x + x2 + · · · + xt−1 = (1 −xt)/(1 −x), and to get to the ﬁnal
line we evaluated the inverse and substituted in for the step-evolution operator (∞.89).
Recall also that bH−1, deﬁned by (∞.72), is the inverse of the stochastic NTK submatrix
evaluated on the training set for a particular realization of the parameters.18
In particular, for suﬃciently small η, as the number of steps becomes large, t →∞,
the residual error zF
m;˜α2(t) −ym;˜α2 exponentially vanishes (∞.90). Thus, the prediction
for a general input δ ∈D (∞.91) will exponentially converge to
zF
i;δ(t = ∞) = zi;δ −
X
j,k,˜α1,˜α2
bHij;δ˜α1

bH−1˜α1 ˜α2
jk
(zk;˜α2 −yk;˜α2) .
(∞.93)
practical optimization algorithm – it does provide more substance to objection in footnote 5 of §7.2
of the name neural tangent kernel for the stochastic operator b
H. In particular, this continuum limit
makes clear that the NTK is really best thought of as a Hamiltonian, as it generates the evolution
of observables, and the step-evolution operator U(t) is like a unitary time-evolution operator, albeit
in imaginary time.
More precisely, in the limit where the dNTK and ddNTKs are set to zero, the
NTK is akin to a free Hamiltonian, with exactly solvable dynamics; with a nonzero dNTK or ddNTKs,
the Hamiltonian includes nontrivial interactions and can be analyzed via time-dependent perturbation
theory.
18Unfortunately, in the absence of a Newton tensor, the raised/lowered sample indices in (∞.92) do not
align well. (In fact, the problem really began in the update equation (∞.85) with the doubled-lowered
˜α index.) If you’d like, you can ﬁx this by judicious use of identity matrices such as δ˜α1 ˜α2 and δ ˜α3 ˜α4.
However, such usage over-clutters the presentation and so is probably not worth it, despite the additional
clarity and type safety that such index alignment provides.
361

Although we took the limit of a large number of steps here, the exponential convergence
means that for any number of steps T such that T ≳(η bH)−1, the prediction error will
be exponentially close to its ﬁnal T →∞value.
In fact, this solution (∞.93) precisely matches the solution we would have gotten
in §∞.2.1 after the ﬁrst update (∞.68) with the Newton tensor (∞.71), so long as the
dNTK and ddNTKs are turned oﬀ. This means that the algorithm dependence that
emerges at ﬁnite width is solely due to the presence of the NTK diﬀerentials and the
resulting nonlinear dynamics.
Interacting Theory: Dynamical NTK and dNTK
Now, let’s incorporate the nonzero dNTK and ddNTKs into our analysis. To do so, we
need to decompose both the dynamical network output and the dynamical NTK as
zi;δ(t) ≡zF
i;δ(t) + zI
i;δ(t) ,
(∞.94)
Hij;δ˜α(t) ≡bHij;δ˜α + HI
ij;δ˜α(t) .
(∞.95)
Here, for the network output, zF
i;δ(t) is the free part, satisfying the linear diﬀerence
equation (∞.85) with a solution given by (∞.91), and zI
i;δ(t) is the interacting part,
encapsulating the corrections due to all the nonzero NTK diﬀerentials. Similarly, for the
NTK, bHij;δ˜α is the step-independent or free part, ﬁxed at initialization, and HI
ij;δ˜α(t) is
the dynamical step-dependent or interaction NTK. Since both zI
i;δ(t) and HI
ij;δ˜α(t) are
absent in the free limit, d
dH, \
ddIH, \
ddIIH →0, at leading order we expect them to be a
linear combination of these objects, schematically
zI(t) = [thing 0](t) d
dH + [thing 1](t) \
ddIH + [thing 2](t) \
ddIIH ,
(∞.96)
HI(t) = [ ]
thing 0](t) d
dH + [ ]
thing 1](t) \
ddIH + [ ]
thing 2](t) \
ddIIH ,
(∞.97)
where various tensorial things will have various time dependencies. This means in turn
that any product of zI
i;δ(t) or HI
ij;δ˜α(t) with one of d
dH, \
ddIH, \
ddIIH can be neglected
to leading order, which is the main reason why we’ll be able to systematically solve
these nonlinear dynamics by perturbation theory. Finally, the initial condition for the
interacting parts of the network output and the NTK must satisfy
zI
i;δ(t = 0) = 0 ,
(∞.98)
HI
ij;δ˜α(t = 0) = 0 ,
(∞.99)
since the free part of the network output already satisﬁes zF
i;δ(t = 0) = zi;δ at initial-
ization, and the free part of the NTK is step-independent and thus is the NTK at
initialization. With all those in mind, our goal now is to ﬁnd a solution for zI
i;δ(t) such
that the full network output, zi;δ(t), the interaction NTK, HI
ij;δ˜α(t), and the dynami-
cal dNTK, dHi0i1i2;δ0δ1δ2(t), all together satisfy the coupled nonlinear dynamics (∞.81),
(∞.82), and (∞.83) to leading order in 1/n.
362

First, let’s work out the dynamics of the dNTK. From (∞.83) we see that the up-
dates to the dNTK are sourced by the residual training error zi;˜α(t) −yi;˜α. Using our
decomposition for the network output, (∞.94) and the initial condition,
dHi0i1i2;δ0δ1δ2(t = 0) = d
dHi0i1i2;δ0δ1δ2 ,
(∞.100)
after iterating the dynamics, we have
dHi0i1i2;δ0δ1δ2(t)
(∞.101)
=d
dHi0i1i2;δ0δ1δ2 −
X
j,˜α

\
ddIHi0i1i2j;δ0δ1δ2 ˜α + \
ddIIHi0i1i2j;δ0δ1δ2 ˜α + \
ddIIHi0i2i1j;δ0δ2δ1 ˜α

×
(
η
t−1
X
s=0
h
zF
j;˜α(s) + zI
j;˜α(s) −yj;˜α
i)
=d
dHi0i1i2;δ0δ1δ2 −
X
j,˜α

\
ddIHi0i1i2j;δ0δ1δ2 ˜α+ \
ddIIHi0i1i2j;δ0δ1δ2 ˜α+ \
ddIIHi0i2i1j;δ0δ2δ1 ˜α

aj;˜α(t) .
Here in the last step, we ﬁrst dropped the product of zI
i;δ(t) with \
ddIH as explained
earlier, and then substituted in our dynamical helper function (∞.92). Now, plugging in
our ﬁnal expression from (∞.92) and neglecting the ﬂuctuation part of the NTK inverse
as subleading, we ﬁnd an expression for the dynamical dNTK:
d
dHi0i1i2;δ0δ1δ2(t)
=d
dHi0i1i2;δ0δ1δ2 −
X
k,˜α1,˜α2

\
ddIHi0i1i2j;δ0δ1δ2 ˜α1 + \
ddIIHi0i1i2j;δ0δ1δ2 ˜α1 + \
ddIIHi0i2i1j;δ0δ2δ1 ˜α1

× eH ˜α1 ˜α2 n
zj;˜α2 −yj;˜α2 −
h
zF
j;˜α2(t) −yj;˜α2
io
.
(∞.102)
In particular, the quantity in the curly brackets represents the diﬀerence in training
errors between initialization and step t: the larger this diﬀerence – i.e. the more the
residual training error decreases – the greater the evolution of the dNTK, and the more
the meta feature functions evolve, undergoing their own form of meta representation
learning over the course of training.
Next, using our decompositions for the network output and NTK, (∞.94) and (∞.95),
we can rewrite the NTK dynamics (∞.82) as a diﬀerence equation for the interaction
NTK:
HI
i1i2;δ1δ2(t + 1)
(∞.103)
=HI
i1i2;δ1δ2(t) −η
X
j,˜α

dHi1i2j;δ1δ2 ˜α(t) + dHi2i1j;δ2δ1 ˜α(t)
 h
zF
j;˜α(t) −yj;˜α
i
+ η2
2
X
j1,j2,˜α1,˜α2

\
ddIHi1i2j1j2;δ1δ2 ˜α1 ˜α2 + \
ddIHi2i1j1j2;δ2δ1 ˜α1 ˜α2 + 2 \
ddIIHi1i2j1j2;δ1δ2 ˜α1 ˜α2

×
h
zF
j1;˜α1(t) −yj1;˜α1
i h
zF
j2;˜α2(t) −yj2;˜α2
i
.
363

Here, once again, we’ve dropped the interacting part of the network output on the right-
hand side, as it is always multiplied by either the dNTK or the ddNTKs and thus will
be subleading in 1/n. Denoting the free part of the residual training error (∞.88) as
ϵF
j;˜α(t) ≡zF
j;˜α(t) −yj;˜α ,
(∞.104)
for notational convenience, and substituting in our solution for the dynamical dNTK
(∞.102), we get
HI
i1i2;δ1δ2(t + 1) −HI
i1i2;δ1δ2(t)
(∞.105)
= −η
X
j,˜α

d
dHi1i2j;δ1δ2 ˜α + d
dHi2i1j;δ2δ1 ˜α

ϵF
j;˜α(t)
+ η
X
j,k,˜α,˜α1,˜α2

\
ddIHi1i2jk;δ1δ2 ˜α˜α1 + \
ddIIHi1i2jk;δ1δ2 ˜α˜α1 + \
ddIIHi1ji2k;δ1 ˜αδ2 ˜α1
+ \
ddIHi2i1jk;δ2δ1 ˜α˜α1 + \
ddIIHi2i1jk;δ2δ1 ˜α˜α1 + \
ddIIHi2ji1k;δ2 ˜αδ1 ˜α1

× eH ˜α1 ˜α2ϵF
j;˜α(t)
h
zk;˜α2 −yk;˜α2 −ϵF
k;˜α2(t)
i
+ η2
2
X
j1,j2,˜α1,˜α2

\
ddIHi1i2j1j2;δ1δ2 ˜α1 ˜α2 + \
ddIHi2i1j1j2;δ2δ1 ˜α1 ˜α2 + 2 \
ddIIHi1i2j1j2;δ1δ2 ˜α1 ˜α2

× ϵF
j1;˜α1(t) ϵF
j2;˜α2(t) .
In particular, the step dependence on the right-hand side is expressed entirely in terms
of the free residual training error ϵF
j;˜α(t), and each term is either linear or quadratic in
ϵF
j;˜α(t). Thus, in order to solve this diﬀerence equation and get HI
i1i2;δ1δ2(t), we’ll just
have to compute sums over these terms.
One of those sums – the one that’s linear in the free residual training error – is the
dynamical helper function aj;˜α(t) ≡η Pt−1
s=0 ϵF
j;˜α(t) that we evaluated in (∞.92). The
other type of sum is quadratic in the free residual training error, which will deﬁne a
second dynamical helper function:
bj1j2;˜α1 ˜α2(t)
(∞.106)
≡η
t−1
X
s=0
ϵF
j1;˜α1(t)ϵF
j2;˜α2(t)
=η
X
k1,k2,˜α3,˜α4
t−1
X
s=0
h
I −η bH
si
j1k1;˜α1 ˜α3
h
I −η bH
si
j2k2;˜α2 ˜α4(zk1;˜α3 −yk1;˜α3) (zk2;˜α4 −yk2;˜α4)
=
X
˜α3,˜α4
X ˜α1 ˜α2 ˜α3 ˜α4
II
h
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4) −ϵF
j1;˜α3(t)ϵF
j2;˜α4(t)
i
+ O
 1
n

.
To evaluate this sum, we again used our expression for the free residual training error,
(∞.88), in conjunction with the deﬁnition of the step-evolution operator, (∞.89). Then,
we replaced the stochastic NTK bHij;˜α1 ˜α2 of the training set by its mean δijH˜α1 ˜α2 at the
364

cost of subleading corrections, and formally performed the geometric sum as in (∞.92).
This last operation yielded an inverting tensor X ˜α1 ˜α2 ˜α3 ˜α4
II
implicitly deﬁned by
δ ˜α1
˜α5δ ˜α2
˜α6 =
X
˜α3,˜α4∈A
X ˜α1 ˜α2 ˜α3 ˜α4
II
1
η
h
δ˜α3 ˜α5δ˜α4 ˜α6 −(δ˜α3 ˜α5 −η eH˜α3 ˜α5)(δ˜α4 ˜α6 −η eH˜α4 ˜α6)
i
=
X
˜α3,˜α4∈A
X ˜α1 ˜α2 ˜α3 ˜α4
II

eH˜α3 ˜α5δ˜α4 ˜α6 + δ˜α3 ˜α5 eH˜α4 ˜α6 −η eH˜α3 ˜α5 eH˜α4 ˜α6

.
(∞.107)
This tensor is a generalization of the familiar inverting matrix X ˜α1 ˜α2
I
≡
eH ˜α1 ˜α2 for
geometric sums over matrices that satisﬁes
δ ˜α1
˜α3 =
X
˜α2∈A
X ˜α1 ˜α2
I
1
η
h
δ˜α2 ˜α3 −(δ˜α2 ˜α3 −η eH˜α2 ˜α3)
i
=
X
˜α2∈A
X ˜α1 ˜α2
I
eH˜α2 ˜α3 ,
(∞.108)
and appeared in the last expression of the dynamical helper function aj;˜α(t) (∞.92).
While we are on ﬁre like an activated neuron, let’s also deﬁne a ﬁnal dynamical helper
function for sums that are cubic in the free residual training error:
cj1j2j3;˜α1 ˜α2 ˜α3(t)
(∞.109)
≡η
t
X
s=0
ϵF
j1;˜α1(t)ϵF
j2;˜α2(t)ϵF
j3;˜α3(t)
=
X
˜α4,˜α5,˜α6
X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III
h
(zk1;˜α4 −yk1;˜α4) (zk2;˜α5 −yk2;˜α5) (zk3;˜α6 −yk3;˜α6)
−ϵF
j1;˜α4(t) ϵF
j2;˜α5(t) ϵF
j3;˜α6(t)
i
+O
 1
n

.
In this case, the inverting tensor X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III
is implicitly deﬁned by
δ ˜α1
˜α7δ ˜α2
˜α8δ ˜α3
˜α9
(∞.110)
=
X
˜α4,˜α5,˜α6
X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III

eH˜α4 ˜α7δ˜α5 ˜α8δ˜α6 ˜α9 + δ˜α4 ˜α7 eH˜α5 ˜α8δ˜α6 ˜α9 + δ˜α4 ˜α7δ˜α5 ˜α8 eH˜α6 ˜α9

−η

eH˜α4 ˜α7 eH˜α5 ˜α8δ˜α6 ˜α9 + eH˜α4 ˜α7δ˜α5 ˜α8 eH˜α6 ˜α9 + δ˜α4 ˜α7 eH˜α5 ˜α8 eH˜α6 ˜α9

+ η2
eH˜α4 ˜α7 eH˜α5 ˜α8 eH˜α6 ˜α9

.
Essentially, these three dynamical helper functions encode the step dependence of various
geometric sums of the free residual training error ϵF
j;˜α(t). Note that we have
aj1;˜α1(t = 0) = 0 ,
(∞.111)
bj1j2;˜α1 ˜α2(t = 0) = 0 ,
(∞.112)
cj1j2j3;˜α1 ˜α2 ˜α3(t = 0) = 0 ,
(∞.113)
365

and so they all vanish at initialization, while at the end of training we have
aj1;˜α1(∞) =
X
˜α2
X ˜α1 ˜α2
I
(zj1;˜α2 −yj1;˜α2) + O
 1
n

,
(∞.114)
bj1j2;˜α1 ˜α2(∞) =
X
˜α3,˜α4
X ˜α1 ˜α2 ˜α3 ˜α4
II
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4) + O
 1
n

, (∞.115)
cj1j2j3;˜α1 ˜α2 ˜α3(∞) =
X
˜α4,˜α5,˜α6
X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+ O
 1
n

,
(∞.116)
since the free residual training error ϵF
j;˜α(t) vanishes exponentially quickly, cf. (∞.90).
With the help of the ﬁrst two of these dynamical helper functions, we can semi-
compactly write the solution to the diﬀerence equation for the interaction NTK (∞.105)
as
HI
i1i2;δ1δ2(t)
(∞.117)
= −
X
j,˜α

d
dHi1i2j;δ1δ2 ˜α + d
dHi2i1j;δ2δ1 ˜α

aj;˜α(t)
+
X
j,k,˜α1,˜α2,˜α3

\
ddIHi1i2jk;δ1δ2 ˜α1 ˜α2 + \
ddIIHi1i2jk;δ1δ2 ˜α1 ˜α2 + \
ddIIHi1ji2k;δ1 ˜α1δ2 ˜α2
+ \
ddIHi2i1jk;δ2δ1 ˜α1 ˜α2 + \
ddIIHi2i1jk;δ2δ1 ˜α1 ˜α2 + \
ddIIHi2ji1k;δ2 ˜α1δ1 ˜α2

× eH ˜α2 ˜α3h
(zk;˜α3 −yk;˜α3) aj;˜α1(t) −bjk;˜α1 ˜α3(t)
i
+ η
2
X
j1,j2,˜α1,˜α2

\
ddIHi1i2j1j2;δ1δ2 ˜α1 ˜α2 + \
ddIHi2i1j1j2;δ2δ1 ˜α1 ˜α2 + 2 \
ddIIHi1i2j1j2;δ1δ2 ˜α1 ˜α2

bj1j2;˜α1 ˜α2(t) .
As a quick sanity check, the vanishing initial condition for the interaction NTK, (∞.99),
is satisﬁed due to the vanishing of the helper functions at initialization, (∞.111) and
(∞.112). We can also plug in (∞.114) and (∞.115) to evaluate the change in NTK at
the end of training. In particular, we see that the larger the change in the NTK, the
more the feature functions evolve. Thus, more initial error entails more representation
learning over the course of training.
Lastly, we need to determine the step dependence of the interacting part of the
network output, zI
i;δ(t). Inserting our free-interacting decomposition, (∞.94), into our
dynamics for the network output, (∞.81), and using the fact that the free part satis-
ﬁes the step-independent evolution equation (∞.86), we can reorganize terms to ﬁnd a
dynamical equation for the interacting part only:
zI
i;δ(t + 1) = zI
i;δ(t) −
X
j,˜α
η bHij;δ˜α zI
j;˜α(t) + η Fi;δ(t) .
(∞.118)
366

Here, we’ve deﬁned a damping force:
Fi;δ(t) ≡−
X
j,˜α
HI
ij;δ˜α(t) ϵF
j;˜α(t) + η
2
X
j1,j2,˜α1,˜α2
dHij1j2;δ˜α1 ˜α2(t) ϵF
j1;˜α1(t) ϵF
j2;˜α2(t)
(∞.119)
−η2
6
X
j1,j2,j3,˜α1,˜α2,˜α3
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 ϵF
j1;˜α1(t) ϵF
j2;˜α2(t) ϵF
j3;˜α3(t) .
Since we have solutions for the interaction NTK and the dNTK dynamics in terms of
the free residual training error solution ϵF
j;˜α(t), (∞.117) and (∞.102), this damping force
is an explicitly known function of the step t.
Let us now try to implicitly express the solution to this diﬀerence equation (∞.118)
as a sum over steps. First for inputs in the training set ˜α ∈A, the dynamics of the
interacting part of the output, (∞.118), reduce to a ﬁrst-order inhomogeneous linear
diﬀerence equation, with the damping force ruining the homogeneity:
zI
i;˜α(t + 1) =
X
j,˜α1

Iij;˜α˜α1 −η bHij;˜α˜α1

zI
j;˜α1(t) + η Fi;˜α(t) .
(∞.120)
We can formally solve this equation as a convolution of the damping force with the free
step-evolution operator (∞.89),
zI
i;˜α(t) = η
t−1
X
s=0
X
j,˜α1
Uij;˜α˜α1(t −1 −s) Fj;˜α1(s) ,
(∞.121)
which satisﬁes the initial condition zI
i;˜α(t = 0) = 0. Plugging this result back into the
dynamical equation for general inputs δ ∈D, (∞.118), we can then ﬁnd a solution for
the interacting part of the network output for such general inputs:
zI
i;δ(t) = η
t−1
X
s=0

Fi;δ(s) −
X
j,˜α
bHij;δ˜α zI
j;˜α(s)

.
(∞.122)
In order to further simplify these expressions, we need to work out one convoluted
sum:
η
t−1
X
s=0
zI
j;˜α(s) =η2
t−1
X
s=0
s−1
X
˜s=0
X
k,˜α1
Ujk;˜α˜α1(s −1 −˜s) Fk;˜α1(˜s)
(∞.123)
=η
t−2
X
u=0
X
k,˜α1
"
η
t−u−2
X
˜u=0
Ujk;˜α˜α1(˜u)
#
Fk;˜α1(u)
=η
t−2
X
u=0
X
k,m,˜α1,˜α2

bH−1˜α˜α2
jm [I −U(t −u −1)]mk;˜α2 ˜α1 Fk;˜α1(u)
=
X
m,˜α2

bH−1˜α˜α2
jm
("
η
t−2
X
u=0
Fm;˜α2(u)
#
+
h
ηFm;˜α2(t −1) −zI
m;˜α2(t)
i)
=
X
m,˜α2

bH−1˜α˜α2
jm
("
η
t−1
X
u=0
Fm;˜α2(u)
#
−zI
m;˜α2(t)
)
,
367

Step by step, on the ﬁrst line we used the expression for the formal solution (∞.121); on
the second line we ﬁrst reversed the order of the sums as Pt−1
s=0
Ps−1
˜s=0 = Pt−2
˜s=0
Pt−1
s=˜s+1,
and then we rewrote these sums in terms of new variables, u ≡˜s and ˜u ≡s −1 −˜s; on
the third line, we performed the geometric sum exactly as in (∞.92); on the fourth line,
we used our formal solution (∞.121) at step t; and on the ﬁnal line, we combined terms
to extend the limits of the sum over the damping force. Plugging this evaluation back
into our formal solution for zI
i;δ(t), (∞.122), we get a slightly less formal solution:
zI
i;δ(t) =η
t−1
X
s=0

Fi;δ(s) −
X
˜α1,˜α2
Hδ˜α1 eH ˜α1 ˜α2Fi;˜α2(s)

+
X
˜α1,˜α2
Hδ˜α1 eH ˜α1 ˜α2zI
i;˜α2(t) + O
 1
n2

.
(∞.124)
In this result, we’ve replaced the stochastic NTK by its mean, bHij;δ1δ2 →δijHδ1,δ2, as
every term is otherwise already proportional to the dNTK or ddNTKs. As a quick sanity
check, note that for inputs in the training set, δ →˜α ∈A, this general expression reduces
to an identity on zI
i;˜α(t).
Ultimately, what we care about is the interacting solution at the end of training,
t →∞.
As before, let’s assume that the product of η with the stochastic NTK is
suﬃciently small such that the free step-evolution operator,
lim
t→∞U(t) ∝exp

−η bHt

,
(∞.125)
exponentially decays to zero.19 Then, for training inputs, the interacting part of the
network outputs, zI
i;˜α(t), will exponentially converge to zero
lim
t→∞zI
i;˜α(t) = 0 .
(∞.126)
To see why this holds, note that the dampening force (∞.119) decays exponentially as
lim
s→∞F(s) ∝exp

−η bHs

,
(∞.127)
since its leading behavior is linearly proportional to the free residual training error ϵF
j;˜α(t).
Combined with (∞.125), this means that the interacting solution (∞.121) converges as
lim
t→∞zI
i;˜α(t) = η lim
t→∞
t−1
X
s=0
X
j,˜α1
Uij;˜α˜α1(t −1 −s) Fj;˜α1(s)
∝lim
t→∞
n
ηt exp
h
−(t −1)η bH
io
= 0 ,
(∞.128)
which is slightly slower than the convergence of the free solution zF
i;˜α(t) ∝exp(−bHt).
Thus, overall the training algorithm converges:
lim
t→∞zi;˜α(t) −yi;˜α = lim
t→∞
h
zF
i;˜α(t) + zI
i;˜α(t)
i
−yi;˜α = 0 ,
(∞.129)
19Note that since the step-evolution operator U(t) is constructed in (∞.89) from the step-independent
NTK, b
Hij;δ ˜α, the condition for this convergence is the same as the free analysis discussed in footnote 16.
368

where here we also used the free solution (∞.90).20 Incidentally, and of possible broader
interest, this altogether shows that gradient descent converges exponentially quickly to a
zero-error minimum for realistic deep neural networks of ﬁnite width and nonzero depth,
up to errors that are at most quadratic in our eﬀective theory cutoﬀL/n.
For general inputs, the interacting part of the output, zI
i;δ(t), is given by the expres-
sion (∞.124). With the convergence on the training set in mind (∞.126), the expression
in the end-of-training limit reduces to
lim
t→∞zI
i;δ(t) =
"
η
∞
X
s=0
Fi;δ(s)
#
−
X
˜α1,˜α2
Hδ˜α1 eH ˜α1 ˜α2
"
η
∞
X
s=0
Fi;˜α2(s)
#
.
(∞.130)
Thus, all that remains is for us to perform an inﬁnite sum over the damping force:
η
∞
X
s=0
Fi;δ(s) ≡−
X
j,˜α1
"
η
∞
X
s=0
HI
ij;δ˜α1(s) ϵF
j;˜α1(s)
#
(∞.131)
+ η
2
X
j1,j2,˜α1,˜α2
"
η
∞
X
s=0
dHij1j2;δ˜α1 ˜α2(s) ϵF
j1;˜α1(s) ϵF
j2;˜α2(s)
#
−η2
6
X
j1,j2,j3,˜α1,˜α2,˜α3
"
η
∞
X
s=0
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 ϵF
j1;˜α1(s) ϵF
j2;˜α2(s) ϵF
j3;˜α3(s)
#
.
The third sum is exactly the end-of-training limit of the third dynamical helper function
cj1j2j3;˜α1 ˜α2 ˜α3(t) that we already evaluated in (∞.116), giving
η
∞
X
s=0
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 ϵF
j1;˜α1(s) ϵF
j2;˜α2(s) ϵF
j3;˜α3(s)
(∞.132)
=
X
˜α4,˜α5,˜α6
\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6) .
Thus, we are left with two more sums to evaluate. Let’s proceed slowly: everything is
simple, but there are a lot of terms to get right.
To start, we can evaluate the second sum in (∞.131) as
η
∞
X
s=0
d
dHij1j2;δ˜α1 ˜α2(s) ϵF
j1;˜α1(s) ϵF
j2;˜α2(s)
(∞.133)
=d
dHij1j2;δ˜α1 ˜α2
∞
X
s=0

η ϵF
j1;˜α1(s) ϵF
j2;˜α2(s)

−
X
k,˜α3,˜α4

\
ddIHij1j2k;δ˜α1 ˜α2 ˜α3 + 2 \
ddIIHij1j2k;δ˜α1 ˜α2 ˜α3

× eH ˜α3 ˜α4
∞
X
s=0

η ϵF
j1;˜α1(s) ϵF
j2;˜α2(s)
h
zk;˜α4 −yk;˜α4 −ϵF
k;˜α4(s)
i 
,
20Remember that in this section we have stopped explicitly denoting that there are O 1/n2
correc-
tions. Recalling our fully-trained condition (∞.66), this result (∞.129) should be understood to be true
up to such corrections, cf. (∞.78) for our two-step solution where the situation was analogous.
369

where we substituted in our dynamical dNTK solution, (∞.102) and used the fact that
the overall expression is symmetric under (˜α1, j1) ↔(˜α2, j2) to combine the two ddIIH
terms. Then, using our already evaluated sums, (∞.115) and (∞.116), we get
η
∞
X
s=0
d
dHij1j2;δ˜α1 ˜α2(s) ϵF
j1;˜α1(s) ϵF
j2;˜α2(s)
(∞.134)
=
X
˜α3,˜α4
d
dHij1j2;δ˜α1 ˜α2X ˜α1 ˜α2 ˜α3 ˜α4
II
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
−
X
k,˜α3,...,˜α6

\
ddIHij1j2k;δ˜α1 ˜α2 ˜α3 + 2 \
ddIIHij1j2k;δ˜α1 ˜α2 ˜α3

× Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
1
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zk;˜α6 −yk;˜α6) .
where we introduced a shorthand
Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
1
≡X ˜α1 ˜α2 ˜α4 ˜α5
II
eH ˜α3 ˜α6 −
X
˜α7
eH ˜α3 ˜α7X ˜α1 ˜α2 ˜α7 ˜α4 ˜α5 ˜α6
III
,
(∞.135)
to ease the collection of terms later.
To ﬁnish, let us write the ﬁrst sum in (∞.131) as
η
∞
X
s=0
HI
ij;δ˜α1(s) ϵF
j;˜α1(s)
(∞.136)
= −
X
k,˜α2

d
dHijk;δ˜α1 ˜α2 + d
dHjik;˜α1δ˜α2
 ∞
X
s=0

η ϵF
j;˜α1(s) ak;˜α2(s)

+
X
k1,k2,˜α2,˜α3,˜α4

\
ddIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIIHik1jk2;δ˜α2 ˜α1 ˜α3
+ \
ddIHjik1k2;˜α1δ˜α2 ˜α3 + \
ddIIHjik1k2;˜α1δ˜α2 ˜α3 + \
ddIIHjk1ik2;˜α1 ˜α2δ˜α3

× eH ˜α3 ˜α4
∞
X
s=0

η ϵF
j;˜α1(s)
h
(zk2;˜α4 −yk2;˜α4) ak1;˜α2(s) −bk1k2;˜α2 ˜α4(s)
i
+ η
2
X
k1,k2,˜α2,˜α3

\
ddIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIHjik1k2;˜α1δ˜α2 ˜α3 + 2 \
ddIIHijk1k2;δ˜α1 ˜α2 ˜α3

×
∞
X
s=0

η ϵF
j;˜α1(s) bk1k2;˜α2 ˜α3(s)

,
where we substituted in our solution for the interaction NTK, (∞.117). Then, substitut-
ing for the helper functions with (∞.92) and (∞.106), performing the additional sums
370

over these terms, and then using the end-of-training limits (∞.114)–(∞.116), we get
η
∞
X
s=0
HI
ij;δ˜α1(s)ϵF
j;˜α1(s)
(∞.137)
= −
X
k,˜α2,˜α3,˜α4

d
dHijk;δ˜α1 ˜α2 + d
dHjik;˜α1δ˜α2

Y ˜α1 ˜α2 ˜α3 ˜α4
2
(zj;˜α3 −yj;˜α3) (zk;˜α4 −yk;˜α4)
+
X
k1,k2,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIIHik1jk2;δ˜α2 ˜α1 ˜α3
+ \
ddIHjik1k2;˜α1δ˜α2 ˜α3 + \
ddIIHjik1k2;˜α1δ˜α2 ˜α3 + \
ddIIHjk1ik2;˜α1 ˜α2δ˜α3

× Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
(zj;˜α4 −yj;˜α4) (zk1;˜α5 −yk1;˜α5) (zk2;˜α6 −yk2;˜α6)
+ η
2
X
k1,k2,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHijk1k2;δ˜α1 ˜α2 ˜α3 + \
ddIHjik1k2;˜α1δ˜α2 ˜α3 + 2 \
ddIIHijk1k2;δ˜α1 ˜α2 ˜α3

× Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
4
(zj;˜α4 −yj;˜α4) (zk1;˜α5 −yk1;˜α5) (zk2;˜α6 −yk2;˜α6) ,
where we introduced additional shorthands
Y ˜α1 ˜α2 ˜α3 ˜α4
2
≡eH ˜α1 ˜α3 eH ˜α2 ˜α4 −
X
˜α5
eH ˜α2 ˜α5X ˜α1 ˜α5 ˜α3 ˜α4
II
,
(∞.138)
Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
≡eH ˜α1 ˜α4 eH ˜α2 ˜α5 eH ˜α3 ˜α6 −
X
˜α7
eH ˜α2 ˜α7X ˜α1 ˜α7 ˜α4 ˜α5
II
eH ˜α3 ˜α6
(∞.139)
−
X
˜α7
eH ˜α1 ˜α4 eH ˜α3 ˜α7X ˜α2 ˜α7 ˜α5 ˜α6
II
+
X
˜α7,˜α8,˜α9
eH ˜α3 ˜α9X ˜α2 ˜α9 ˜α7 ˜α8
II
X ˜α1 ˜α7 ˜α8 ˜α4 ˜α5 ˜α6
III
,
Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
4
≡eH ˜α1 ˜α4X ˜α2 ˜α3 ˜α5 ˜α6
II
−
X
˜α7,˜α8
X ˜α2 ˜α3 ˜α7 ˜α8
II
X ˜α1 ˜α7 ˜α8 ˜α4 ˜α5 ˜α6
III
.
(∞.140)
Now, it’s time to frantically ﬂip through pages and collect everything we computed.
Plugging the sums (∞.137), (∞.134), and (∞.132) back into our expression for the
sum over the damping force, (∞.131), plugging that back into our expression for the
interaction part of the network output (∞.130), and then combining with the free part
of the network output, (∞.93), we obtain our fully-trained solution for ﬁnite-width
371

networks trained by gradient descent:
zi;δ(t = ∞)
(∞.141)
≡zF
i;δ(t = ∞) + zI
i;δ(t = ∞)
=zi;δ −
X
j,k,˜α1,˜α2
bHij;δ˜α1

bH−1˜α1 ˜α2
jk
(zk;˜α2 −yk;˜α2)
+
X
j1,j2,˜α1,˜α2,˜α3,˜α4

d
dHj1ij2;˜α1δ˜α2 −
X
˜α5,˜α6
Hδ˜α5 eH ˜α5 ˜α6 d
dHj1ij2;˜α1 ˜α6 ˜α2


× Z ˜α1 ˜α2 ˜α3 ˜α4
A
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
+
X
j1,j2,˜α1,˜α2,˜α3,˜α4

d
dHij1j2;δ˜α1 ˜α2 −
X
˜α5,˜α6
Hδ˜α5 eH ˜α5 ˜α6 d
dHij1j2;˜α6 ˜α1 ˜α2


× Z ˜α1 ˜α2 ˜α3 ˜α4
B
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHj1ij2j3;˜α1δ˜α2 ˜α3 −
X
˜α7,˜α8
Hδ˜α7 eH ˜α7 ˜α8\
ddIHj1ij2j3;˜α1 ˜α8 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHij1j2j3;δ˜α1 ˜α2 ˜α3 −
X
˜α7,˜α8
Hδ˜α7 eH ˜α7 ˜α8\
ddIHij1j2j3;˜α8 ˜α1 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIIHj1j2ij3;˜α1 ˜α2δ˜α3 −
X
˜α7,˜α8
Hδ˜α7 eH ˜α7 ˜α8 \
ddIIHj1j2ij3;˜α1 ˜α2 ˜α8 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIIHij1j2j3;δ˜α1 ˜α2 ˜α3 −
X
˜α7,˜α8
Hδ˜α7 eH ˜α7 ˜α8 \
ddIIHij1j2j3;˜α8 ˜α1 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+ O
 1
n2

.
372

Here we deﬁned our ﬁnal tensors with our ﬁnal alphabet letter (and various subscripts),
Z ˜α1 ˜α2 ˜α3 ˜α4
A
≡Y ˜α1 ˜α2 ˜α3 ˜α4
2
,
(∞.142)
Z ˜α1 ˜α2 ˜α3 ˜α4
B
≡Y ˜α1 ˜α2 ˜α3 ˜α4
2
+ η
2X ˜α1 ˜α2 ˜α3 ˜α4
II
,
(∞.143)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
−η
2Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
4
,
(∞.144)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
−η
2Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
4
−η
2Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
1
−η2
6 X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III
,
(∞.145)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
,
(∞.146)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
−Y ˜α2 ˜α1 ˜α3 ˜α5 ˜α4 ˜α6
3
−Y ˜α1 ˜α3 ˜α2 ˜α4 ˜α6 ˜α5
3
−ηY ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
4
−ηY ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
1
,
(∞.147)
making use of our previous shorthand tensors (∞.135) and (∞.138)–(∞.140).21 These
algorithm projectors, (∞.142)–(∞.147), serve to project the initial training error onto
two diﬀerent combinations of the dNTK, two diﬀerent combinations of the ﬁrst ddNTK,
and two other diﬀerent combinations of the second ddNTK, all according to the details
of the gradient descent algorithm. As a ﬁnal sanity check, note that as for inputs in
the training set, δ →˜α ∈A, the quantities in the square brackets in the ﬁnite-width
solution (∞.141) each vanish, and we recover our fully-trained condition (∞.66).
Before we retire this subsection, let us elaborate on the algorithm dependence. First,
note that our two-update solution (∞.79) has the same form as the gradient-descent
solution (∞.141), but with diﬀerent algorithm projectors:
Z ˜α1 ˜α2 ˜α3 ˜α4
B
≡1
2
eH ˜α1 ˜α3 eH ˜α2 ˜α4 ,
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
≡−1
6
eH ˜α1 ˜α4 eH ˜α2 ˜α5 eH ˜α3 ˜α6 ,
(∞.148)
and all the others vanishing.
Clearly these algorithms have very diﬀerent inductive
biases! Second, we can study the ODE limit of the dynamics by taking η →0, cf. foot-
note 17: in this case, we see that the ODE dynamics have a solution given by
Z ˜α1 ˜α2 ˜α3 ˜α4
A
= Z ˜α1 ˜α2 ˜α3 ˜α4
B
≡Y ˜α1 ˜α2 ˜α3 ˜α4
2
,
(∞.149)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
= Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
= Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
(∞.150)
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
≡−Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
−Y ˜α2 ˜α1 ˜α3 ˜α5 ˜α4 ˜α6
3
−Y ˜α1 ˜α3 ˜α2 ˜α4 ˜α6 ˜α5
3
,
(∞.151)
where Y ˜α1 ˜α2 ˜α3 ˜α4
2
and Y ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
3
are given by (∞.138) and (∞.139), respectively, and
the inverting tensor X ˜α1 ˜α2 ˜α3 ˜α4
II
, (∞.107), now satisﬁes
X
˜α3,˜α4∈A
X ˜α1 ˜α2 ˜α3 ˜α4
II

eH˜α3 ˜α5δ˜α4 ˜α6 + δ˜α3 ˜α5 eH˜α4 ˜α6

= δ ˜α1
˜α5δ ˜α2
˜α6 ,
(∞.152)
21Note that for the last of these tensors, (∞.147), in order to coax the various contributions in our
solution (∞.141) into the proper form, we used the symmetry of \
ddIIHij1j2j3;δ ˜α1 ˜α2 ˜α3 and relabeled
various dummy sample indices.
373

and the other inverting tensor X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
II
, (∞.110), now satisﬁes
δ ˜α1
˜α7δ ˜α2
˜α8δ ˜α3
˜α9
(∞.153)
=
X
˜α4,˜α5,˜α6
X ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
III

eH˜α4 ˜α7δ˜α5 ˜α8δ˜α6 ˜α9 + δ˜α4 ˜α7 eH˜α5 ˜α8δ˜α6 ˜α9 + δ˜α4 ˜α7δ˜α5 ˜α8 eH˜α6 ˜α9

.
This entirely captures the diﬀerence between gradient ﬂow and gradient descent for
these fully-trained networks. In general, we conjecture that for ﬁnite-width networks, at
leading order the fully-trained solution takes the universal form of (∞.141), with all of
the algorithm dependence encoded by the six algorithm projectors: Z ˜α1 ˜α2 ˜α3 ˜α4
A
, Z ˜α1 ˜α2 ˜α3 ˜α4
B
,
Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
, Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
, Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
, and Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
.22
Finally, let’s understand what is meant by the remaining error in our solution
(∞.141). It is not really the error of an actual network that is instantiated and then
fully trained through many many gradient-descent steps, but instead it is the error in our
eﬀective description of such a particular fully-trained network. Of course, our eﬀective
theory formalism can compute higher-order corrections, if they’re of interest. However,
the leading-order ﬁnite-width corrections should really be suﬃcient for most cases: for
instance, for a network of depth L = 10 layers and of hidden-layer width n = 100 neurons
each, our eﬀective description will only be oﬀby ∼(L/n)2 = 1%.
Furthermore, for any theoretical analysis, the main qualitative diﬀerence in the solu-
tion appears when going from inﬁnite width to ﬁnite width, as we go from a free theory
to an interacting theory and from linear dynamics to nonlinear dynamics. Thus, the
eﬀective theory that gave us the solution (∞.141) really is “as simple . . . as possible”
while still providing an extremely accurate description of real deep learning models.
∞.2.3
Prediction at Finite Width
Having solved the training dynamics in two diﬀerent ways, we can now rather generally
study the predictions of our networks on novel inputs x ˙β from the test set ˙β ∈B.
At ﬁnite width, the predictions of a fully-trained network are universally governed
22More precisely, we conjecture that our conjecture, as stated, holds for the MSE loss.
For the
cross-entropy loss the solution will take a slightly diﬀerent form, but with a similar partition into an
algorithm-independent part and an algorithm-dependent part described by similar algorithm projectors.
374

by the stochastic equation
zi; ˙β(t = T)
(∞.154)
=zi; ˙β −
X
˜α1,˜α2
H ˙β ˜α1 eH ˜α1 ˜α2(zi;˜α2 −yi;˜α2)
+
X
j,˜α1,˜α2

d
∆Hij; ˙β ˜α1 −
X
˜α3,˜α4∈A
H ˙β ˜α3 eH ˜α3 ˜α4 d
∆Hij;˜α4 ˜α1

eH ˜α1 ˜α2(zj;˜α2 −yj;˜α2)
−
X
j,k
˜α1,...,˜α4

d
∆Hij; ˙β ˜α1 −
X
˜α5,˜α6
H ˙β ˜α5 eH ˜α5 ˜α6 d
∆Hij;˜α6 ˜α1

eH ˜α1 ˜α2 d
∆Hjk;˜α2 ˜α3 eH ˜α3 ˜α4(zk;˜α4 −yk;˜α4)
+
X
j1,j2,˜α1,˜α2,˜α3,˜α4

d
dHj1ij2;˜α1 ˙β ˜α2 −
X
˜α5,˜α6
H ˙β ˜α5 eH ˜α5 ˜α6 d
dHj1ij2;˜α1 ˜α6 ˜α2


× Z ˜α1 ˜α2 ˜α3 ˜α4
A
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
+
X
j1,j2,˜α1,˜α2,˜α3,˜α4

d
dHij1j2; ˙β ˜α1 ˜α2 −
X
˜α5,˜α6
H ˙β ˜α5 eH ˜α5 ˜α6 d
dHij1j2;˜α6 ˜α1 ˜α2


× Z ˜α1 ˜α2 ˜α3 ˜α4
B
(zj1;˜α3 −yj1;˜α3) (zj2;˜α4 −yj2;˜α4)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHj1ij2j3;˜α1 ˙β ˜α2 ˜α3 −
X
˜α7,˜α8
H ˙β ˜α7 eH ˜α7 ˜α8\
ddIHj1ij2j3;˜α1 ˜α8 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIHij1j2j3; ˙β ˜α1 ˜α2 ˜α3 −
X
˜α7,˜α8
H ˙β ˜α7 eH ˜α7 ˜α8\
ddIHij1j2j3;˜α8 ˜α1 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIIHj1j2ij3;˜α1 ˜α2 ˙β ˜α3 −
X
˜α7,˜α8
H ˙β ˜α7 eH ˜α7 ˜α8 \
ddIIHj1j2ij3;˜α1 ˜α2 ˜α8 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+
X
j1,j2,j3,
˜α1,˜α2,˜α3,˜α4,˜α5,˜α6

\
ddIIHij1j2j3; ˙β ˜α1 ˜α2 ˜α3 −
X
˜α7,˜α8
H ˙β ˜α7 eH ˜α7 ˜α8 \
ddIIHij1j2j3;˜α8 ˜α1 ˜α2 ˜α3


× Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
(zj1;˜α4 −yj1;˜α4) (zj2;˜α5 −yj2;˜α5) (zj3;˜α6 −yj3;˜α6)
+ O
 1
n2

.
This formula could boarder-line be ﬁt on a T-shirt.23 For this expression, we’ve expanded
23To better facilitate such brand awareness, ﬁrst recall that for nearly-kernel methods we were able
375

the complete inverse of the stochastic NTK sub-tensor as per (∞.71); in particular,
the second line is more or less the inﬁnite-width kernel prediction (10.39), and the
terms on the third and fourth lines are ﬁnite-width corrections due to NTK ﬂuctuations.
Further, the algorithm projectors (∞.142)–(∞.147) contain all the ﬁnite-width algorithm
dependence of the solution, and thus this general solution (∞.154) can describe both our
explicit solutions, whether we train in two steps (∞.79), in many many steps (∞.141),
or with any other choice of optimization algorithm that uses the MSE loss.
Furthermore, this solution (∞.154) describes the predictions of a particular network
from our ensemble: the instantiation-to-instantiation diﬀerence is encoded in the par-
ticular initial network output, z, the NTK ﬂuctuation, d
∆H, the dNTK, d
dH, the ﬁrst
ddNTK, \
ddIH, and the second ddNTK, \
ddIIH. Since we know explicitly the statistics
to compress the model predictions in terms of a trained kernel (11.142); similarly, we can compress the
predictions of ﬁnite-width networks (∞.154) in terms of a trained NTK, H♯
ij;δ ˜α, as
zi; ˙β(t = T) = zi; ˙β −
X
j,k,˜α1,˜α2
H♯
ij; ˙β ˜α1 f
H♯
˜α1 ˜α2
jk
(zk;˜α2 −yk;˜α2) + O
 1
n2

,
(∞.155)
taking the form of a (neural tangent) kernel prediction (10.39). To see how this works, let us decompose
the trained NTK into free and training-dependent terms:
H♯
ij;δ ˜α ≡b
Hij;δ ˜α + Hij;δ ˜α .
(∞.156)
Please don’t confuse this decomposition with our earlier decomposition (∞.95): that former one was
convenient for solving the training dynamics, while this new one is useful for determining the trained
NTK in (∞.155). Considering the inverse of the trained NTK restricted to the training set only,
f
H♯
˜α1 ˜α2
jk
≡

b
H−1˜α1 ˜α2
jk
−
X
˜α3,˜α4
Hjk;˜α3 ˜α4 e
H ˜α1 ˜α3 e
H ˜α2 ˜α4 + O
 1
n2

,
(∞.157)
and plugging it along with the decomposition (∞.156) into the formula (∞.155), we get
zi; ˙β(t = T) =zi; ˙β −
X
j,k,˜α1,˜α2
b
Hij; ˙β ˜α1

b
H−1˜α1 ˜α2
jk
(zk;˜α2 −yk;˜α2)
(∞.158)
−
X
j,˜α1,˜α2
"
Hij; ˙β ˜α1 −
X
˜α3,˜α4
H ˙β ˜α3 e
H ˜α3 ˜α4Hij;˜α4 ˜α1
#
e
H ˜α1 ˜α2(zj;˜α2 −yj;˜α2) + O
 1
n2

.
The terms on the ﬁrst line of the right-hand side of this expression give the free contribution to the
solution, (∞.93), while the terms on the second line give the interacting contribution, (∞.130), encap-
sulating the eﬀect of nontrivial representation learning at ﬁnite width. The speciﬁc form of H♯
ij;δ ˜α can
be found by matching the terms on the right-hand sides of (∞.158) and (∞.154). You can also express
the training-dependent part, Hij;δ ˜α, implicitly in terms of the damping force (∞.119) as
η
∞
X
s=0
Fi;δ(s) = −
X
j,˜α1,˜α2
Hij;δ ˜α1 e
H ˜α1 ˜α2 (zj;˜α2 −yj;˜α2) ,
(∞.159)
as is clear by comparing (∞.158) with (∞.130).
This implicit expression also makes it clear that
the form of Hij;δ ˜α isn’t unique and can always be adjusted by the addition of a term orthogonal to
P
˜α2 e
H ˜α1 ˜α2 (zj;˜α2 −yj;˜α2). In any event, with the trained NTK you can now ﬁt the ﬁnite-width predic-
tion formula (∞.155) on any newborn AI’s onesie.
376

of these variables at initialization, we can also analyze the statistics of the fully-trained
distribution in full. With an eye towards a discussion of the depth dependence of these
statistics, we’ll now revive the layer indices.
First and foremost, the mean prediction is given by
mi; ˙β ≡E
h
z(L)
i; ˙β (T)
i
(∞.160)
=mNTK
i; ˙β
+
1
nL−1

m∆NTK
i; ˙β
+ mdNTK
i; ˙β
+ mddNTK-I
i; ˙β
+ mddNTK-II
i; ˙β

−
1
nL−1
X
˜α1,˜α2
H(L)
˙β ˜α1
eH ˜α1 ˜α2
(L)

m∆NTK
i;˜α2
+ mdNTK
i;˜α2
+ mddNTK-I
i;˜α2
+ mddNTK-II
i;˜α2

+ O
 1
n2

,
where the ﬁrst term is the (neural tangent) kernel prediction
mNTK
i; ˙β
≡
X
˜α1,˜α2
H(L)
˙β ˜α1
eH ˜α1 ˜α2
(L) yi;˜α2 ,
(∞.161)
and the four other kinds of terms come from the leading-order ﬁnite-width correction.
Speciﬁcally, (i) the ﬂuctuation of the NTK gives
m∆NTK
i;δ
≡
X
˜α1,...,˜α4

A(L)
(δ˜α1)(˜α2 ˜α3) + B(L)
δ˜α2 ˜α1 ˜α3 + nLB(L)
δ˜α3 ˜α1 ˜α2

eH ˜α1 ˜α2
(L)
eH ˜α3 ˜α4
(L) yi;˜α4
(∞.162)
where we used (8.82) to evaluate the NTK variance in terms of our decomposition into
tensors A(L) and B(L); (ii) the dNTK gives
mdNTK
i;δ
≡−
X
˜α1,...,˜α4
"
2

P (L)
δ˜α1 ˜α2 ˜α3 + Q(L)
δ˜α1 ˜α2 ˜α3 + nLQ(L)
δ˜α2 ˜α1 ˜α3

Z ˜α1 ˜α2 ˜α3 ˜α4
B
(∞.163)
+

nLP (L)
˜α1δ˜α2 ˜α3 + Q(L)
˜α1δ˜α2 ˜α3 + Q(L)
˜α1 ˜α2δ˜α3

Z ˜α1 ˜α2 ˜α3 ˜α4
A
+

P (L)
˜α1δ˜α2 ˜α3 + nLQ(L)
˜α1δ˜α2 ˜α3 + Q(L)
˜α1 ˜α2δ˜α3

Z ˜α1 ˜α2 ˜α4 ˜α3
A
#
yi;˜α4 ,
where we used (11.42) to evaluate the dNTK-preactivation cross correlators in terms of
our decomposition into tensors P (L) and Q(L); (iii) the ﬁrst ddNTK gives
mddNTK-I
i;δ
(∞.164)
≡−
X
˜α1,...,˜α6
h
R(L)
δ˜α1 ˜α2 ˜α3

Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IB
+ Z ˜α2 ˜α3 ˜α1 ˜α5 ˜α6 ˜α4
IB
+ Z ˜α3 ˜α1 ˜α2 ˜α6 ˜α4 ˜α5
IB

+ R(L)
˜α1δ˜α2 ˜α3Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IA
+ R(L)
˜α1 ˜α2 ˜α3δZ ˜α1 ˜α2 ˜α3 ˜α5 ˜α6 ˜α4
IA
+ R(L)
˜α1 ˜α3δ˜α2Z ˜α1 ˜α2 ˜α3 ˜α6 ˜α4 ˜α5
IA
i
×
h
yi;˜α4
 X
j
yj;˜α5yj;˜α6 + nLK(L)
˜α5 ˜α6

+ yi;˜α5K(L)
˜α6 ˜α4 + yi;˜α6K(L)
˜α4 ˜α5
i
377

where we used (∞.11) to evaluate the ﬁrst ddNTK mean in terms of decomposition into
the tensor R(L); and (iv) the second ddNTK gives
mddNTK-II
i;δ
(∞.165)
≡−
X
˜α1,...,˜α6
h
S(L)
δ˜α1 ˜α2 ˜α3Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIB
+ T (L)
δ˜α1 ˜α2 ˜α3Z ˜α2 ˜α3 ˜α1 ˜α5 ˜α6 ˜α4
IIB
+ U(L)
δ˜α1 ˜α2 ˜α3Z ˜α3 ˜α1 ˜α2 ˜α6 ˜α4 ˜α5
IIB
+ S(L)
˜α1 ˜α2δ˜α3Z ˜α1 ˜α2 ˜α3 ˜α5 ˜α6 ˜α4
IIA
+ T (L)
˜α1δ˜α3 ˜α2Z ˜α1 ˜α2 ˜α3 ˜α4 ˜α5 ˜α6
IIA
+ U(L)
˜α1 ˜α3 ˜α2δZ ˜α1 ˜α2 ˜α3 ˜α6 ˜α4 ˜α5
IIA
i
×
h
yi;˜α4
 X
j
yj;˜α5yj;˜α6 + nLK(L)
˜α5 ˜α6

+ yi;˜α5K(L)
˜α6 ˜α4 + yi;˜α6K(L)
˜α4 ˜α5
i
where we used (∞.12) to evaluate the second ddNTK mean in terms of decomposition
into the tensors S(L), T (L), and U(L).
Interestingly, we see that not only does the mean prediction depend on the NTK
diﬀerentials – as we expect, given the nontrivial representation learning at ﬁnite width
– but also it depends on the NTK variance as well, cf. (∞.162). This is natural as each
network ﬁts the training data with its own particular NTK, and so the resulting fully-
trained particular output (∞.154) depends on the NTK ﬂuctuation, as we’ve already
emphasized. In general, it is really important to understand the tradeoﬀs between both
contributions, and in the next subsubsection we will return to comment on this inter-
play between random ﬂuctuations and directed representation learning in the overall
ensemble.
In addition, looking at the terms that are cubic in yi;˜α in the contributions from the
ddNTKs, (∞.164) and (∞.165), we see that the j-th component of the observed outputs
inﬂuences the i-th component of the mean prediction for i ̸= j. Just as we saw for
the mean prediction of Bayesian inference, (6.88), this is one consequence of the wiring
property of ﬁnite-width neural networks. To see another manifestation of this wiring
property, let’s consider the covariance:
Cov
h
z(L)
i1; ˙β1(T), z(L)
i2; ˙β2(T)
i
≡E
h
z(L)
i1; ˙β1(T) z(L)
i2; ˙β2(T)
i
−E
h
z(L)
i1; ˙β1(T)
i
E
h
z(L)
i2; ˙β2(T)
i
. (∞.166)
While we won’t print this quantity in full – the full expression doesn’t really play nicely
with the constraints of the page – you can easily extract insight by considering some
speciﬁc contributions. For instance, we can see the imprint of output-component wiring
378

by looking at the following contribution to the covariance,
X
j1,j2
˜α1,...,˜α4
E

z(L)
i2; ˙β2
d
dH
(L)
i1j1j2; ˙β1 ˜α1 ˜α2

Z ˜α1 ˜α2 ˜α3 ˜α4
B
E
h
z(L)
j1;˜α3 −yj1;˜α3
 
z(L)
j2;˜α4 −yj2;˜α4
i
=
1
nL−1
δi1i2
X
˜α1,...,˜α4
" 
nLP (L)
˙β1 ˜α1 ˜α2 ˙β2 + Q(L)
˙β1 ˜α1 ˜α2 ˙β2 + Q(L)
˙β1 ˜α2 ˜α1 ˙β2

G(L)
˜α3 ˜α4
+ P (L)
˙β1 ˜α1 ˜α2 ˙β2
 X
j
yj;˜α3yj;˜α4
#
Z ˜α1 ˜α2 ˜α3 ˜α4
B
+
1
nL−1
X
˜α1,...,˜α4
Q(L)
˙β1 ˜α1 ˜α2 ˙β2Z ˜α1 ˜α2 ˜α3 ˜α4
B
(yi1;˜α3yi2;˜α4 + yi1;˜α4yi2;˜α3) ,
(∞.167)
which comes from the cross correlation between a factor of z(L)
i2; ˙β2 from z(L)
i2; ˙β2(T) and one
of the dNTK terms from z(L)
i1; ˙β1(T) that involves the algorithm projector Z ˜α1 ˜α2 ˜α3 ˜α4
B
. In
particular, we see that wiring is exhibited in the last term on the ﬁnal line when the
true outputs have nonzero components for both i1 and i2. In this case, this correlation
(∞.167) implies that the test-set predictions for z(L)
i1; ˙β1(T) can be shifted given z(L)
i2; ˙β2(T),
for i1 ̸= i2: cf. our related discussion of Hebbian learning in terms of the ﬁre-together
inductive bias in the ﬁnite-width prior in §6.4.1 and then our following discussion of how
that leads to the posterior distribution’s wiring together in §6.4.2. Here, the presence
of this wiring in the covariance of the solution means that this contribution to wiring
occurs diﬀerently for each network in the ensemble: the ﬂuctuations from instantiation to
instantiation of the parameters at initialization break the permutation symmetry among
the output neurons. This type of wiring suggests that each network is able to use the
ﬂuctuations between the diﬀerent initial output components to its advantage in order to
correlate such components together over the course of learning.
More broadly, unlike the kernel prediction at inﬁnite width (10.39), the prediction at
ﬁnite width (∞.154) now has non-Gaussian statistics. In particular, since ﬁnite-width
prediction is a nontrivial functional of the network output, the NTK, the dNTK, and the
ddNTKs – all at initialization – and since we know that the joint distribution of those
quantities p

z(L), bH(L), d
dH
(L), \
ddIH
(L), \
ddIIH
(L)D

is a nearly-Gaussian distribution,
then so is the fully-trained distribution p

z(L)(T)
D

. In particular, there are nontrivial
higher-point connected correlators; the explicit expressions of such correlators are chal-
lenging to display in any media format, though all the information needed to do so is
contained in (∞.154), and it’s not that hard to zero in on any particular term of interest.
The information carried in such correlators probably contains useful insight into some
of the behavior of fully-trained networks and is likely worth further consideration.
379

Generalization at Finite Width
Having discussed many of the qualitative diﬀerences between the ﬁnite-width prediction
(∞.154) and the inﬁnite-width kernel prediction (10.39), now let’s make some quanti-
tative statements. In order to get a high-level understanding of how generalization is
modiﬁed at ﬁnite width as compared to our extensive inﬁnite-width analysis in §10.3, we
need to determine the size of the relative importance of the ﬁnite-width corrections to
our predictions, namely how these corrections depend on the widths of the hidden layers
nℓand the depth of the network L. In particular, we want to understand corrections to
our generalized bias-variance tradeoﬀ(10.52) at ﬁnite width.
Let’s start by considering the bias term, mi; ˙β −yi; ˙β, for which we just need to look at
the mean prediction mi; ˙β in (∞.160). In particular, we need to compare the ﬁrst term,
(∞.161), which more or less corresponds to the inﬁnite-width kernel prediction m∞
i; ˙β –
up to the subleading and unimportant correction to the NTK mean – against the other
set of the ﬁnite-width contributions to the mean, (∞.162)–(∞.165).
Now, among the ﬁnite-width contributions in (∞.160), the terms inside the ﬁrst set
of large parentheses have a sample index corresponding to a test input, ˙β, in each of
the tensors A(L), B(L), P (L), Q(L), R(L), S(L), T (L), and U(L). Thus, even for a training
set with one training sample as we studied in §10.3.1, the particular details of these
terms require an understanding of asymptotic multiple-input solutions of the recursions
for the NTK variance, the preactivation-dNTK cross correlation, and the ddNTK mean;
such an analysis is kind of annoying and was previously left as an adventure for thrill
seekers, with a brief instruction manual for those interested buried in footnote 8 of §11.3.
Unfortunately, we have no plans here to update that manual any further, and we don’t
expect to miss much by not doing so.24
In contrast, the terms inside the second set of large parentheses in (∞.160) can be
analyzed with the results we already have in hand. Thus, to nonlinearly gain a large
amount of intuition with a small amount of eﬀort, let’s compare the inﬁnite-width term
(∞.161) against only this last set of terms. In particular, since both of these terms
are preceded by a common prefactor P
˜α1,˜α2 H(L)
˙β ˜α1
eH ˜α1 ˜α2
(L)
– whose eﬀect was analyzed in
§10.3 – we simply need to understand the depth and width dependence of the tensors
inside the second set of brackets in (∞.160). Here we’ll evaluate these tensors for a
single training set input x˜α = x, dropping the training sample indices for the rest of this
analysis for notational simplicity.
To see the physics, it’s simplest to look at the ODE limit of many many steps of
gradient descent, η ↘0, for which the algorithm projectors take particularly simple
24In particular, we expect that this ﬁrst set of terms will behave qualitatively similar to the second set
of terms that we will discuss in the next paragraph, though the particular order-one-level details may
diﬀer.
380

form,
ZA = ZB =
1
2

eH(L)
2 ,
ZIA = ZIB = ZIIA =
−1
6

eH(L)
3 ,
ZIIB =
−1
2

eH(L)
3 ,
(∞.168)
cf. (∞.138), (∞.139), and (∞.149)–(∞.153) to derive these expressions.25 Substituting
these projectors (∞.168) into the dNTK and ddNTK contributions to the mean predic-
tion, (∞.163)–(∞.165), and then considering the 1/n part of the mean prediction mi; ˙β
(∞.160), we see that all the individual terms are proportional to one of the following
dimensionless ratios:
A(L)
nL−1

eH(L)
2 ,
B(L)
nL−1

eH(L)
2 ,
P (L)
nL−1

eH(L)
2 ,
Q(L)
nL−1

eH(L)
2 ,
(∞.170)
R(L)K(L)
nL−1

eH(L)
3 ,
S(L)K(L)
nL−1

eH(L)
3 ,
T (L)K(L)
nL−1

eH(L)
3 ,
U(L)K(L)
nL−1

eH(L)
3 .
Thus, these eight ratios determine the size of the ﬁnite-width eﬀects as compared to the
leading inﬁnite-width term.26
Importantly, recalling our scaling laws for the NTK variance, (9.27), for the dNTK-
preactivation cross correlation, (11.65), and for the dNTK-preactivation cross correla-
tion, (∞.21) and (∞.22), we see that each of these dimensionless ratios will scale like
the depth-to-width ratio L/n (except for the subdominant U(L) contribution). Thus,
overall we should ﬁnd for the ﬁnite-width corrections
mi; ˙β −m∞
i; ˙β = O
L
n

,
(∞.171)
25Backing oﬀthe ODE limit, for many many steps of gradient descent with a ﬁnite learning rate η,
the single-input dNTK algorithm projectors, (∞.142) and (∞.143), are instead given by
ZA =
1
  e
H(L)2

1 −η e
H(L)
2 −η e
H(L)

,
ZB =
1
2  e
H(L)2 .
(∞.169)
In conjunction with similar single-input limits of the ddNTK algorithm projectors, this will mean that the
same set of ratios, (∞.170), determine the generalization error bias term, though now with an additional
η dependence. In particular, the projector ZA diverges as the global learning rate approaches from below
η ↗2/ e
H(L). This divergence is expected: as we noted in footnote 16, we need ||I −η b
H||∞< 1 for the
training dynamics to converge after many many steps t →∞. If you check, you’ll also see that some of
the ddNTK algorithm projectors have the same divergence.
26Incidentally, the form of the ﬁnal two ratios on the ﬁrst line of (∞.170) is the ultimate justiﬁcation
for why in (11.62) we divided the dNTK-preactivation cross-correlation tensors by two factors of the
NTK mean, cf. our discussion of dimensional analysis around (11.64). Similarly the form of the four
ratios on the second line of (∞.170) is the ultimate justiﬁcation for the ratios (∞.21) and (∞.22). For
this latter set of ratios, we should have really written K(L) + n−1
L
P
j y2
j instead of just K(L), especially
when K(L) behaves as a nontrivial power law in L; in such cases, we should really rescale the target
output yi;˜α by the power-law factor L−p0/2 as discussed around (∞.26).
381

where the exact order-one constant is not important. What is important is that we
conﬁrmed our expectation for the overall scaling of this correction, going as the cutoﬀ
of our eﬀective theory: r ≡L/n. Similarly, we could do an analysis for the variance
term of the generalization error by looking at the covariance (∞.166), which will be a
whole lot of work with very little payoﬀ; such an analysis would merely conﬁrm that the
leading ﬁnite-width corrections will again be of order O(L/n).
In general, given that the aspect ratio L/n controls both the ﬂuctuations in the en-
semble and representation learning, the optimal value of the ratio is likely nonzero but
also small. In particular, representation learning is enhanced by the depth, but net-
works with too large a value of L/n will both have an eﬀect on the mean prediction of
the ensemble, but perhaps even more importantly lead to exponentially-large problems
when working with only a particular network: for large enough L/n our principle of typ-
icality can break down, and so the generalization error can begin to exhibit exponential
behavior.27
This further explains the success of our eﬀective theory description at O(L/n): a
description with vanishing L/n, i.e. the inﬁnite-width limit, is too simple to model the
properties of deep neural networks in practice; a description for larger values of L/n, i.e. a
small-width or overly-deep regime that includes many higher-order corrections, describes
networks that are unlikely to be trainable; a description with small but nonzero L/n,
i.e. the leading ﬁnite-width eﬀective theory accurate to O(L/n), is as simple as it can
be and still accurately describe the networks that work well in practice.28
In summary, we’ve seen that the leading ﬁnite-width corrections all scale exactly
according to our long-standing expectations, and we’ve discussed at a high level the
potential tradeoﬀof depth versus width. In principle, we could go further in our anal-
ysis, evaluating the multi-input recursions for nearby inputs, evaluating all the terms
in the covariance, and ﬁnding all the O(L/n) contributions to the generalization error
with speciﬁc coeﬃcients.29 If we did this, what could it tell us? In particular, can we
theoretically optimize the aspect ratio L/n for a particular activation function without
any experimentation?
Unfortunately at the order we’re working, we won’t be able to optimize the aspect
ratio L/n using the prediction formula (∞.154) from the end of training: the linear
dependence of the generalization on the ratio means that its derivative is independent of
the ratio; instead, we’d need to compute the higher-order corrections of order O
 L2/n2
27In our discussion of ﬂuctuations in §5.4, we explained that too large a value of L/n may lead to a
diﬃcult ﬁne-tuning problem in terms of getting a particular networks to behave critically.
On the one hand, the contribution of such ﬂuctuations to the bias part of the generalization error is
one way to see how the downstream eﬀect of such ﬂuctuations may lead to problems after training. On
the other hand, the fact that ﬂuctuations can ruin criticality for a particular network is another problem.
The former problem aﬀects the ensemble as a whole, while the latter problem aﬀects individual networks.
28To better understand the scale that separates the trainable regime from the overly-deep regime, see
Appendix A. For a discussion of how to extend this trainable regime to greater depths for ﬁxed width,
see our discussion of residual networks in Appendix B.
29You can also more easily evaluate the multi-input version of the relevant recursions numerically with
this scaling in mind in order to determine the overall coeﬃcient for a particular activation function of
interest. This would be one way to analyze these solutions for the ReLU-like GELU and SWISH networks.
382

to optimize the ratio – which is hard, though at least straightforward to do in the eﬀective
theory framework we’ve developed.30 At order O(L/n), the best we could hope to see is
whether nonzero L/n improves generalization or not by looking at the sign of its overall
coeﬃcient. Rather than going through that hassle, we’ll instead get a little more mileage
out of the mean prediction mi; ˙β by trying to understand how the algorithm dependence
at ﬁnite width leads to additional tradeoﬀs within the bias term of the generalization
error.
Inductive Bias of the Training Algorithm
As multiply mentioned, one of the key diﬀerences between the inﬁnite-width and ﬁnite-
width networks optimized via gradient-based learning is the algorithm dependence of
the fully-trained solution for ﬁnite-width networks.
In particular, the solution – at
least for MSE losses – takes a universal form, (∞.154), with all of the dependence of the
solution on the training algorithm encoded in the algorithm projectors, ZA, ZB, ZIA, ZIB,
ZIIA, and ZIIB, whose functional forms we’ve established explicitly for the two second-
order updates in (∞.148), for many many steps of gradient descent in (∞.142)–(∞.147),
and for the ODE limit of gradient descent in (∞.149)–(∞.151). Through this universal
projection, we now have a theoretical means of isolating the inductive bias of the training
algorithm from all the other inductive biases that are present in a fully-trained neural
network. This provides a natural way of theoretically evaluating the relative merits of
diﬀerent training algorithms.
One aspect that is apparent just from staring at the stochastic prediction (∞.154)
is that the algorithm projectors only induce projections on the NTK-diﬀerential part of
the ﬁnite-width prediction and cannot aﬀect the NTK-ﬂuctuation contribution. More
concretely, let’s continue our discussion of the bias term in the generalization error. In
particular, the bias is given by the following diﬀerence:
mi; ˙β −yi; ˙β .
(∞.172)
On the one hand, it’s clear that the NTK-variance contribution to the mean prediction,
(∞.162), encoded in A(L) and B(L) is irreducible, entirely independent of the training
algorithm; this irreducible NTK-variance contribution depends on the training data in a
ﬁxed way and arises due to instantiation-to-instantiation ﬂuctuations in the NTK across
diﬀerent realizations of the parameters. On the other hand, the projectors always act on
the NTK-diﬀerential tensors P (L), Q(L), R(L), S(L), T (L), and U(L); this means that the
dNTK’s and ddNTKs’ eﬀect on the network’s predictions is adjustable by the training
algorithm and can be tuned diﬀerently for diﬀerent datasets and tasks.
30In §A.3, we’ll compute such higher-order eﬀects from an information-theoretic perspective. This
analysis will give a heuristic prescription for optimizing the network’s depth-to-width ratio r: we consider
an auxiliary unsupervised learning objective thought to be beneﬁcial for building representations and
optimize the aspect ratio in terms of that criterion rather than the generalization error. In this setting,
we’re able to determine optimal aspect ratios for diﬀerent activation functions by trading oﬀleading-
order eﬀects against higher-order eﬀects. This is somewhat similar in spirit to the way in which Newton’s
method trades oﬀthe leading decrease of the loss against the subleading loss increase in order to optimize
the overall learning rate, cf. our discussion of Newton’s method in footnote 8 of §10.2.
383

To reiterate our previous discussion of the tradeoﬀ, on the one hand the NTK-
ﬂuctuation contribution is likely harmful as it leads to a breakdown of criticality for any
particular network. On the other hand, the latter NTK-diﬀerential contribution is the
ultimate source of the nontrivial representation learning at ﬁnite width and so it would
be nice to make this contribution large. With these algorithm projectors, we now have
direct means to enhance the latter beneﬁt while keeping ﬁxed the former cost.
One way of understanding these algorithm projectors is as the sample-space dual
description of a learning algorithm, analogous to the relationship between the feature
functions φj(x) and the kernel k(xδ1, xδ2) that we explored in §10.4 and §11.4. From
the parameter-space microscopic perspective, a learning algorithm, such as gradient
descent, explicitly operates on the parameters. At the end of training, all the details
of the algorithm are left implicit in the trained parameters θ⋆, making it diﬃcult to
understand its eﬀect on the model’s predictions. From this perspective, the algorithm is
simple to deﬁne (§7.2), but decidedly diﬃcult to analyze for general interacting machine-
learning models (§∞.2.2).
However, if you can analyze or solve a model to ﬁnd its
sample-space dual description, then the algorithm projectors make the inﬂuence of the
learning algorithm explicit.31
Analogously to the (nearly-)kernel perspective on (nearly-)linear models, the appear-
ance of the algorithm projectors in the generalization error means that we can begin to
think about engineering them directly, either by picking them to be directly used with a
prediction formula such as (∞.154), or alternatively by attempting to ﬁnd the parameter-
space dual of an algorithm projector. While we expect this latter task to be diﬃcult –
just as it’s often hard to ﬁnd the feature functions that correspond to a particular kernel
– it could be very worthwhile to explore as a means of engineering the inductive bias of
the training algorithm directly.32
In particular, we expect that this could signiﬁcantly improve generalization by de-
signing algorithms that are better tailored to the details of an architecture or on the
properties of the underlying dataset or task.33 This design process could in principle
proceed as follows: (i) engineer a desired functional form for the algorithm projectors
and then (ii) determine the parameter-space training algorithm that leads to the pro-
jectors having those desired properties or speciﬁc functional form. While further details
of such inverse algorithm design is outside the scope of this book, we think that this
line of dual analysis has the potential to unlock a much deeper understanding of the
relationship among the inductive bias of the training algorithm, the inductive bias of
the network architecture, and the ultimate success of the model.
31This is the sense in which we meant that the conditioning on the learning algorithm would be simple
for the trained ensemble in (0.9).
32This can also be seen as another advantage of nonlinear or interacting machine learning models. For
linear models, the solution is always independent of the details of the learning algorithm (§10.2.2).
33Since the algorithm projectors have sample indices, the optimal choice of the algorithm will in general
depend on the details and structure of the training set in addition to the deﬁnition of the model.
384

There’s No Place Where Gradient Descent = Exact Bayesian Inference
Finally, a curious reader might wonder whether the connection that we detailed in §10.2.4
between gradient descent optimization and exact Bayesian inference for the inﬁnite-width
limit persists for more realistic networks at ﬁnite width. In short, the answer is no.
In long, recall the training hyperparameter settings (10.46) and (10.48) that matched
gradient-based learning with Bayesian inference at inﬁnite width. In particular, the latter
condition (10.48) required turning oﬀany learning in the hidden layers:
λ(ℓ)
b
= 0 ,
λ(ℓ)
W = 0 ,
for
ℓ< L .
(∞.173)
Importantly, this makes the hidden-layer NTK vanish exactly: bH(ℓ)
i1i2;δ1δ2 = 0, for ℓ< L
at any ﬁnite width, cf. (8.12). Next, ﬂip back and look at the P- and Q-recursions for
the dNTK-preactivation cross-correlation tensors, (11.49) and (11.52), and then you’ll
probably also want to ﬂip further back and visit the F- and B-recursions, (8.79) and
(8.89), respectively. (We’ll be waiting here for you when you return.) Immediately, you
should notice a problem: if the hidden-layer NTK mean vanishes, then at this order
B(ℓ) = 0, F (ℓ) = 0, and all this together implies that P (L) = Q(L) = 0. Thus, all the
eﬀects of the dNTK are turned oﬀ. Similarly, if you ﬂip forth and look at the R-, S-, T-,
and U-recursions, (∞.177) and (∞.179)–(∞.181), then you’ll notice that all the eﬀects
of the ddNTKs are turned oﬀas well. Thus, the mean prediction of our ﬁnite-width
gradient-based-learning ensemble (∞.160) cannot match the exact Bayesian posterior
mean at ﬁnite width (6.88).
Note that this mismatch is obvious in hindsight; by only training the last layer
(∞.173), we get a linear model (10.134).
In other words, since the hyperparameter
choices of (∞.173) lead to a model with random features that are ﬁxed over the course
of training, there’s no representation learning possible for these settings. In contrast,
we found nontrivial representation learning when studying exact Bayesian inference at
ﬁnite width in §6.4.3.
Ultimately, for Bayesian inference we only care about the preactivation distribution
p

z(ℓ)D

, while for gradient-based learning we need to consider the joint preactivation-
NTK-dNTK-ddNTKs distribution p

z(L), bH(L), d
dH
(L), \
ddIH
(L), \
ddIIH
(L)D

, which in-
corporates the statistics of the derivatives of the preactivations at initialization: such
derivatives of the model output are invisible to the exact Bayesian inferencer.
∞.3
RG Flow of the ddNTKs: The Full Expressions
These expressions were kind of horrible, so we decided to hide them here at the end of the
chapter. As such, they are only really needed for three reasons: (i) to explicitly check the
absence of any NTK diﬀerentials for the hyperparameter setup (∞.173) and thus conﬁrm
that the connection between gradient descent and Bayesian inference doesn’t persist at
ﬁnite width, (ii) to check the details of the depth-to-width scaling of the ddNTKs that
we discussed in §∞.1, and (iii) to more generally evaluate the ddNTKs’ contributions to
385

the ensemble’s mean prediction, (∞.164) and (∞.165), for multiple inputs – analytically
or numerically – or to compute other higher-order statistics of our stochastic prediction
(∞.154).
\
ddIH Stochastic Forward Equation
\
ddIH
(ℓ+1)
i0i1i2i3;δ0δ1δ2δ3
(∞.174)
=
δi0i1
λ(ℓ+1)
W
nℓ
nℓ
X
j0,j1,j2,j3=1
δj0j1W (ℓ+1)
i2j2
W (ℓ+1)
i3j3
σ(ℓ)
j1;δ1σ′ (ℓ)
j2;δ2σ′ (ℓ)
j3;δ3
×
h
σ′′ (ℓ)
j0;δ0 bH(ℓ)
j0j2;δ0δ2 bH(ℓ)
j0j3;δ0δ3 + σ′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j2j3;δ0δ2δ3
i
+ δi0i2
λ(ℓ+1)
W
nℓ
nℓ
X
j0,j1,j2,j3=1
δj0j2W (ℓ+1)
i3j3
W (ℓ+1)
i1j1
σ(ℓ)
j2;δ2σ′ (ℓ)
j3;δ3σ′ (ℓ)
j1;δ1
×
h
σ′′ (ℓ)
j0;δ0 bH(ℓ)
j0j3;δ0δ3 bH(ℓ)
j0j1;δ0δ1 + σ′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j3j1;δ0δ3δ1
i
+ δi0i3
λ(ℓ+1)
W
nℓ
nℓ
X
j0,j1,j2,j3=1
δj0j3W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
σ(ℓ)
j3;δ3σ′ (ℓ)
j1;δ1σ′ (ℓ)
j2;δ2
×
h
σ′′ (ℓ)
j0;δ0 bH(ℓ)
j0j1;δ0δ1 bH(ℓ)
j0j2;δ0δ2 + σ′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j1j2;δ0δ1δ2
i
+
nℓ
X
j0,j1,j2,j3=1
W (ℓ+1)
i0j0
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
W (ℓ+1)
i3j3
σ′ (ℓ)
j1;δ1σ′ (ℓ)
j2;δ2σ′ (ℓ)
j3;δ3
×
h
σ′ (ℓ)
j0;δ0\
ddIH
(ℓ)
j0j1j2j3;δ0δ1δ2δ3 + σ′′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j1j2;δ0δ1δ2 bH(ℓ)
j0j3;δ0δ3
+ σ′′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j2j3;δ0δ2δ3 bH(ℓ)
j0j1;δ0δ1 + σ′′ (ℓ)
j0;δ0 d
dH
(ℓ)
j0j3j1;δ0δ3δ1 bH(ℓ)
j0j2;δ0δ2
+ σ′′′(ℓ)
j0δ0 bH(ℓ)
j0j1;δ0δ1 bH(ℓ)
j0j2;δ0δ2 bH(ℓ)
j0j3;δ0δ3
i
,
386

\
ddIIH Stochastic Forward Equation
\
ddIIH
(ℓ+1)
i1i2i3i4;δ1δ2δ3δ4
(∞.175)
=δi1i3δi2i4
 
λ(ℓ+1)
W
nℓ
!2 nℓ
X
j,k=1
σ′ (ℓ)
j;δ1σ′ (ℓ)
k;δ2σ(ℓ)
j;δ3σ(ℓ)
k;δ4 bH(ℓ)
jk;δ1δ2
+ δi1i2
λ(ℓ+1)
W
nℓ
nℓ
X
j1,...,j4=1
δj1j2W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
σ′ (ℓ)
j1;δ1σ′ (ℓ)
j2;δ2σ′ (ℓ)
j3;δ3σ′ (ℓ)
j4;δ4 bH(ℓ)
j1j3;δ1δ3 bH(ℓ)
j2j4;δ2δ4
+ δi1i3
λ(ℓ+1)
W
nℓ
nℓ
X
j1,...,j4=1
δj1j3W (ℓ+1)
i2j2
W (ℓ+1)
i4j4
σ(ℓ)
j3;δ3σ′ (ℓ)
j4;δ4σ′ (ℓ)
j1;δ1
×
h
σ′′ (ℓ)
j2;δ2 bH(ℓ)
j2j1;δ2δ1 bH(ℓ)
j2j4;δ2δ4 + σ′ (ℓ)
j2;δ2 d
dH
(ℓ)
j2j1j4;δ2δ1δ4
i
+ δi2i4
λ(ℓ+1)
W
nℓ
nℓ
X
j1,...,j4=1
δj2j4W (ℓ+1)
i1j1
W (ℓ+1)
i3j3
σ(ℓ)
j4;δ4σ′ (ℓ)
j3;δ3σ′ (ℓ)
j2;δ2
×
h
σ′′ (ℓ)
j1;δ1 bH(ℓ)
j1j2;δ1δ2 bH(ℓ)
j1j3;δ1δ3 + σ′ (ℓ)
j1;δ1 d
dH
(ℓ)
j1j2j3;δ1δ2δ3
i
+
nℓ
X
j1,j2,j3,j4=1
W (ℓ+1)
i1j1
W (ℓ+1)
i2j2
W (ℓ+1)
i3j3
W (ℓ+1)
i4j4
σ′ (ℓ)
j3;δ3σ′ (ℓ)
j4;δ4
×
h
σ′ (ℓ)
j1;δ1σ′ (ℓ)
j2;δ2 \
ddIIH
(ℓ)
j1j2j3j4;δ1δ2δ3δ4 + σ′′ (ℓ)
j1;δ1σ′′ (ℓ)
j2;δ2 bH(ℓ)
j1j2;δ1δ2 bH(ℓ)
j1j3;δ1δ3 bH(ℓ)
j2j4;δ2δ4
+ σ′ (ℓ)
j1;δ1σ′′ (ℓ)
j2;δ2 bH(ℓ)
j2j4;δ2δ4 d
dH
(ℓ)
j1j2j3;δ1δ2δ3 + σ′ (ℓ)
j2;δ2σ′′ (ℓ)
j1;δ1 bH(ℓ)
j1j3;δ1δ3 d
dH
(ℓ)
j2j1j4;δ2δ1δ4
i
,
\
ddIH Recursion
The mean of the ﬁrst ddNTK decomposes as
E

\
ddIH
(ℓ)
i0i1i2i3;δ0δ1δ2δ3

(∞.176)
=
1
nℓ−1
h
δi0i1δi2i3R(ℓ)
δ0δ1δ2δ3 + δi0i2δi3i1R(ℓ)
δ0δ2δ3δ1 + δi0i3δi1i2R(ℓ)
δ0δ3δ1δ2
i
.
387

The tensor R(ℓ) satisﬁes the following layer-to-layer recursion:
R(ℓ+1)
δ0δ1δ2δ3
(∞.177)
=λ(ℓ+1)
W
C(ℓ+1)
W

σ′′
δ0σδ1σ′
δ2σ′
δ3

G(ℓ) H(ℓ)
δ0δ2H(ℓ)
δ0δ3
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)
 
λ(ℓ+1)
W

σ′′
δ0σδ1

G(ℓ)

B(ℓ)
δ0δ0δ2δ3
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)
 h
λ(ℓ+1)
W

σ′′
δ0σδ1

G(ℓ) P (ℓ)
δ0δ2δ3δ0 +

σ′
δ0σ′
δ1

G(ℓ) P (ℓ)
δ0δ2δ3δ1
i
+

C(ℓ+1)
W
2 
σ′′′
δ0σ′
δ1σ′
δ2σ′
δ3

G(ℓ) H(ℓ)
δ0δ1H(ℓ)
δ0δ2H(ℓ)
δ0δ3
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)
 
C(ℓ+1)
W

σ′′′
δ0σ′
δ1

G(ℓ)

B(ℓ)
δ0δ0δ2δ3H(ℓ)
δ0δ1
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)
 h
C(ℓ+1)
W

σ′′′
δ0σ′
δ1

G(ℓ) P (ℓ)
δ0δ2δ3δ0 +

σ′′
δ0σ′′
δ1

G(ℓ) P (ℓ)
δ0δ2δ3δ1
i
H(ℓ)
δ0δ1
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)
 
C(ℓ+1)
W

σ′
δ0σ′
δ1

G(ℓ)

R(ℓ)
δ0δ1δ2δ3 + O
 1
n

.
\
ddIIH Recursions
The mean of the second ddNTK decomposes as
E

\
ddIIH
(ℓ)
i1i2i3i4;δ1δ2δ3δ4

(∞.178)
=
1
nℓ−1
h
δi1i2δi3i4S(ℓ)
δ1δ2δ3δ4 + δi1i3δi4i2T (ℓ)
δ1δ3δ4δ2 + δi1i4δi2i3U(ℓ)
δ1δ4δ2δ3
i
.
The tensor S(ℓ) satisﬁes the following layer-to-layer recursion:
S(ℓ+1)
δ1δ2δ3δ4
(∞.179)
=C(ℓ+1)
W
λ(ℓ+1)
W

σ′
δ1σ′
δ2σ′
δ3σ′
δ4

G(ℓ) H(ℓ)
δ1δ3H(ℓ)
δ2δ4
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ3σ′
δ4

G(ℓ)
 h
λ(ℓ+1)
W

σ′
δ1σ′
δ2

G(ℓ) + C(ℓ+1)
W
H(ℓ)
δ1δ2

σ′′
δ1σ′′
δ2

G(ℓ)
i
B(ℓ)
δ1δ2δ3δ4
+

C(ℓ+1)
W
2 
σ′′
δ1σ′′
δ2σ′
δ3σ′
δ4

G(ℓ) H(ℓ)
δ1δ2H(ℓ)
δ1δ3H(ℓ)
δ2δ4
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ1σ′
δ2

G(ℓ)
 
C(ℓ+1)
W

σ′
δ3σ′
δ4

G(ℓ)

S(ℓ)
δ1δ2δ3δ4 + O
 1
n

.
388

The tensor T (ℓ) satisﬁes the following layer-to-layer recursion:
T (ℓ+1)
δ1δ3δ4δ2
(∞.180)
=

λ(ℓ+1)
W
2 
σ′
δ1σ′
δ2σδ3σδ4

G(ℓ) H(ℓ)
δ1δ2
+
 nℓ
nℓ−1
 
λ(ℓ+1)
W
2
X
δ5,...,δ8∈D

zδ5σ′
δ1σδ3

G(ℓ)

zδ6σ′
δ2σδ4

G(ℓ) Gδ5δ7
(ℓ) Gδ6δ8
(ℓ) F (ℓ)
δ7δ1δ8δ2
+ C(ℓ+1)
W
λ(ℓ+1)
W

σ′
δ1σ′′
δ2σδ3σ′
δ4

G(ℓ) H(ℓ)
δ2δ1H(ℓ)
δ2δ4
+
 nℓ
nℓ−1

C(ℓ+1)
W
λ(ℓ+1)
W
H(ℓ)
δ2δ4
X
δ5,...,δ8∈D

zδ5σ′
δ1σδ3

G(ℓ)

zδ6σ′′
δ2σ′
δ4

G(ℓ) Gδ5δ7
(ℓ) Gδ6δ8
(ℓ) F (ℓ)
δ7δ1δ8δ2
+
 nℓ
nℓ−1

C(ℓ+1)
W
λ(ℓ+1)
W

σ′
δ2σ′
δ4

G(ℓ)

σ′′
δ1σδ3

G(ℓ) Q(ℓ)
δ2δ4δ1δ1 +

σ′
δ1σ′
δ3

G(ℓ) Q(ℓ)
δ2δ4δ1δ3

+ C(ℓ+1)
W
λ(ℓ+1)
W

σ′
δ2σ′′
δ1σδ4σ′
δ3

G(ℓ) H(ℓ)
δ1δ2H(ℓ)
δ1δ3
+
 nℓ
nℓ−1

C(ℓ+1)
W
λ(ℓ+1)
W
H(ℓ)
δ1δ3
X
δ5,...,δ8∈D

zδ5σ′
δ2σδ4

G(ℓ)

zδ6σ′′
δ1σ′
δ3

G(ℓ) Gδ5δ7
(ℓ) Gδ6δ8
(ℓ) F (ℓ)
δ7δ2δ8δ1
+
 nℓ
nℓ−1

C(ℓ+1)
W
λ(ℓ+1)
W

σ′
δ1σ′
δ3

G(ℓ)

σ′′
δ2σδ4

G(ℓ) Q(ℓ)
δ1δ3δ2δ2 +

σ′
δ2σ′
δ4

G(ℓ) Q(ℓ)
δ1δ3δ2δ4

+
 nℓ
nℓ−1
 
C(ℓ+1)
W
2 
σ′
δ1σ′
δ3

G(ℓ)

σ′
δ2σ′
δ4

G(ℓ) T (ℓ)
δ1δ3δ4δ2
+

C(ℓ+1)
W
2 
σ′′
δ1σ′′
δ2σ′
δ3σ′
δ4

G(ℓ) H(ℓ)
δ1δ2H(ℓ)
δ1δ3H(ℓ)
δ2δ4
+
 nℓ
nℓ−1
 
C(ℓ+1)
W
2 H(ℓ)
δ1δ3H(ℓ)
δ2δ4
X
δ5,...,δ8∈D

zδ5σ′′
δ1σ′
δ3

G(ℓ)

zδ6σ′′
δ2σ′
δ4

G(ℓ) Gδ5δ7
(ℓ) Gδ6δ8
(ℓ) F (ℓ)
δ7δ1δ8δ2
+
 nℓ
nℓ−1
 
C(ℓ+1)
W
2 H(ℓ)
δ2δ4

σ′
δ1σ′
δ3

G(ℓ)

σ′′′
δ2σ′
δ4

G(ℓ) Q(ℓ)
δ1δ3δ2δ2 +

σ′′
δ2σ′′
δ4

G(ℓ) Q(ℓ)
δ1δ3δ2δ4

+
 nℓ
nℓ−1
 
C(ℓ+1)
W
2 H(ℓ)
δ1δ3

σ′
δ2σ′
δ4

G(ℓ)

σ′′′
δ1σ′
δ3

G(ℓ) Q(ℓ)
δ2δ4δ1δ1 +

σ′′
δ1σ′′
δ3

G(ℓ) Q(ℓ)
δ2δ4δ1δ3

+ O
 1
n

.
The tensor U(ℓ) satisﬁes the following layer-to-layer recursion:
U(ℓ+1)
δ1δ4δ2δ3 =

C(ℓ+1)
W
2 
σ′′
δ1σ′′
δ2σ′
δ3σ′
δ4

G(ℓ) H(ℓ)
δ1δ2H(ℓ)
δ1δ3H(ℓ)
δ2δ4
(∞.181)
+
 nℓ
nℓ−1
 
C(ℓ+1)
W

σ′
δ1σ′
δ4

G(ℓ)
 
C(ℓ+1)
W

σ′
δ2σ′
δ3

G(ℓ)

U(ℓ)
δ1δ4δ2δ3 + O
 1
n

.
389

390

Epilogue ε
Model Complexity from the
Macroscopic Perspective
According to the hype of 1987, neural networks were meant to be intelligent models
that discovered features and patterns in data. Gaussian processes in contrast are
simply smoothing devices. How can Gaussian processes possibly replace neural
networks? Were neural networks over-hyped, or have we underestimated the power of
smoothing methods?
David MacKay [71].
Throughout this book, we’ve focused on deep-learning models that are very wide but
also deep. Our reason for this focus is that such large neural networks with many many
model parameters work extremely well in practice and thus form the foundation of the
modern approach to artiﬁcial intelligence.
The success of these overparameterized models with far more parameters than
training data has led many to simply conjecture that “more is better” when it comes
to deep learning. In a more reﬁned sense, there’s mounting empirical evidence that a
scaling hypothesis can accurately capture the behavior of deep neural networks, and
its associated scaling laws overall point towards the optimality of the overparameterized
regime.1 The simplicity of these empirical laws recalls an earlier period in statistical
physics, when a similar scaling hypothesis was conjectured to govern the behavior of
certain complicated systems in statistical mechanics.2
1See e.g. [72] for an empirical study of scaling laws in deep learning language models based on
the transformer architecture. Empirically, it’s observed that overparameterization is good; the optimal
growth of the number of training samples NA scales sublinearly with the growth in parameters P, though
importantly they should still scale together with a power law: NA ∝P α for 0 < α < 1.
2In physics, such scaling laws are an example of the phenomenon of universality, the fact that when
a system has many elementary components, it can often be described by a very simple eﬀective theory
that’s independent of the microscopic details of the underlying system [49]. The framework of renor-
malization group then oﬀers an explanation for how this universality arises by characterizing the ﬂow
from the microscopic to the macroscopic [38, 39]. This perhaps suggests that the analogous notion of
391

However, the practical success of overparameterized models in deep learning appears
to be in tension with orthodox machine learning and classic statistical theory. Heuris-
tically, the Occam’s razor principle of sparsity posits that we should favor the simplest
hypothesis that explains our observations: in the context of machine learning, this is
usually interpreted to mean that we should prefer models with fewer parameters when
comparing models performing the same tasks. More quantitatively, we expect that mod-
els with fewer parameters will have smaller generalization errors, E ≡LB −LA, and will
be less prone to overﬁt their training set A. Vice versa, we should naively expect that
overparameterized models will overﬁt their training data and generalize poorly. Thus,
this orthodoxy is in direct conﬂict with the empirical success of overparameterized neural
networks and is a big theoretical puzzle in understanding modern deep learning.3
In this brief epilogue, we’re going to oﬀer a resolution of this puzzle. The crux of the
matter hinges on the notion of model complexity. On the one hand, our orthodox
discussion of generalization above took a microscopic perspective – focusing on how
a network works in terms of its explicit low-level components – and wanted to identify
model complexity with model parameters. On the other hand, in this book we integrated
out the model parameters and developed a macroscopic perspective – providing an
eﬀective theory description of the predictions of realistic fully-trained networks – for
which this notion of model complexity is completely reversed.
Indeed, we motivated our eﬀective theory approach in §0 on the basis that we would
be able to ﬁnd simplicity in the limit of a large number of model parameters, and from
§1 to §∞we’ve now seen how this hypothesis has been borne out again and again for
realistic large-but-ﬁnite-width networks. Having ﬁnished all the technical calculations
(outside of the appendices), we can see now that it’s the depth-to-width aspect ratio,
r ≡L/n ,
(ε.1)
that controls the model complexity of overparameterized neural networks. To understand
why, recall that this ratio emerged from our calculations as the expansion parameter or
cutoﬀof our eﬀective theory and determined how we could truncate the series expansion
of the fully-trained distribution while still approximating the true behavior of networks
with minimal error. This means that it’s the number of data-dependent couplings of
the truncated nearly-Gaussian distribution – and not the number of model parameters
– that ultimately deﬁne the model complexity in deep learning. From this macroscopic
perspective, there’s absolutely no conﬂict between the sparse intuition of Occam’s razor
in theory and the simplicity of the scaling hypothesis in practice.
To see how this works in even greater detail, let us recall the three main problems
we discussed at the beginning of the book in §0.2, and then review how the principle of
sparsity enabled us to solve them.4 Taylor expanding the trained network output around
representation group ﬂow (cf. §4.6) may be able to explain the neural scaling laws of [72].
3See, e.g., the extensive discussion in [73] on the diﬃculty of trying to understand why large neural
networks generalize so well according to traditional measures of model complexity.
4In this epilogue, we’ll drop layer indices on our variables to ease the notation, as everything is
evaluated at the output layer; we’ll also sometimes drop the neural indices when they are unimportant.
392

the network’s initialization (0.5),
z(xδ; θ⋆) =z(xδ; θ) +
P
X
µ=1
(θ⋆−θ)µ
dzδ
dθµ
+ 1
2
P
X
µ,ν=1
(θ⋆−θ)µ (θ⋆−θ)ν
d2zδ
dθµdθν
+ . . . ,
(ε.2)
we illustrated Problem 1, (0.6), that we might have to compute an inﬁnite number of
terms,
z ,
dz
dθ ,
d2z
dθ2 ,
d3z
dθ3 ,
d4z
dθ4 ,
. . . ,
(ε.3)
Problem 2, (0.7), that we have to determine the map from the initialization distribution
over the model parameters to the induced initialization distribution over the network
output and its derivatives,
p(θ) →p
 
z, dz
dθ, d2z
dθ2 , . . .
!
,
(ε.4)
and Problem 3,(0.8), that we have to solve the training dynamics, which can depend
on everything,
θ⋆≡[θ⋆]
 
θ, z, dz
dθ, d2z
dθ2 , . . . ; learning algorithm; training data
!
.
(ε.5)
Let us now give our detailed solutions to this problem set – expanding on the schematic
explanations that we gave in §0.2 – and then carefully examine them through the lens of
model complexity. We’ll begin ﬁrst with the simple inﬁnite-width limit and then discuss
the nearly-simple-but-realistic 1/n truncation.
Sparsity at Inﬁnite Width
In the inﬁnite-width limit, we can now understand our solutions as follows:
• Addressing Problem 1, (0.11), all the higher derivative terms vanish, and we only
need to keep track of two statistical variables:
z ,
dz
dθ
=⇒
zδ ,
bHδ1δ2 .
(ε.6)
Note that this ﬁrst derivative gives the random features of the linear model de-
scription, cf. (10.138), and the kernel associated with these features is just the
NTK.
• Addressing Problem 2, we found that the network output and its ﬁrst derivative
are statistically independent, each governed by the simple distribution (0.12):
lim
n→∞p
 
z, dz
dθ, d2z
dθ2 , . . .
!
= p(zδ) δ
 X
µ,ν
λµν
dzδ1
dθµ
dzδ2
dθν
−Θδ1δ2
!
,
(ε.7)
393

where on the right-hand side, p(zδ) is a zero-mean Gaussian distribution with its
variance given by the kernel Kδ1δ2 (4.106), and the second factor is a Dirac delta
function distribution that ﬁxes the contraction of ﬁrst derivatives that make up
the NTK bHδ1δ2 to be deterministically given by the frozen NTK Θδ1δ2 (9.4).
• Addressing Problem 3, we obtained a solution for the trained model parameters
θ⋆in a closed form (0.13):
lim
n→∞θ⋆
µ = θµ(t = 0) −
X
ν,˜α1,˜α2,i
λµν
dzi;˜α1
dθν
eΘ˜α1 ˜α2 (zi;˜α2 −yi;˜α2) ,
(ε.8)
with the associated fully-trained network outputs zδ(T) = z(xδ; θ⋆) given in (10.31).
We further showed in §10.2.2 that this fully-trained solution is independent of the
algorithm used to train the network.
Combining these insights, we found that the fully-trained distribution,
lim
n→∞p

z(T)

≡p

z(T)
y˜α, Kδ1δ2, Θδ1δ2

,
(ε.9)
is a Gaussian distribution; the reason for writing it as a conditional distribution in this
way is that the mean, (10.40), is only a function of vector of training set labels, y˜α, and
the frozen NTK matrix, Θ(L)
δ1δ2, while the variance, (10.41), is only a function of the kernel
matrix, K(L)
δ1δ2, and the frozen NTK matrix, Θ(L)
δ1δ2. In other words, the distribution of
predictions on a test sample, x ˙β, will depend on the test sample together with all of the
training data, D = { ˙β} ∪A, and the shape of that data dependence is governed by the
speciﬁc functional forms of the data-dependent couplings, i.e. the output-layer kernel
K(xδ1, xδ2) and output-layer frozen NTK Θ(xδ1, xδ2). Thus, the inﬁnite-width solution
(ε.9) allows for a very sparse description, depending only on a few objects in a simple
way.
Near-Sparsity at Finite Width
Similarly, at large-but-ﬁnite width, we can now understand our solutions as follows:
• Addressing Problem 1, (0.16), all derivatives dkf/dθk for k ≥4 are O
 1/n2, and
so we only need to keep track of the statistical variables up to the third derivative:
z ,
dz
dθ ,
d2z
dθ2 ,
d3z
dθ3
=⇒
zδ , bHδ1δ2 , d
dHδ0δ1δ2 , \
ddIHδ0δ1δ2δ3 , \
ddIIHδ1δ2δ3δ4 .
(ε.10)
Here, we note that the NTK, dNTK, and ddNTKs capture all the terms up to the
third derivative in our Taylor expansion (ε.2) for a gradient-based learning update,
cf. (∞.4), (∞.6), and (∞.9).
394

• Addressing Problem 2, (0.17), we evaluated the distribution of all these statistical
variables, and found that its joint distribution is nearly-Gaussian:
p
 
z, dz
dθ, d2z
dθ2 , . . .
!
= p

z, bH, d
dH, \
ddIH, \
ddIIH

+ O
 1
n2

.
(ε.11)
• Addressing Problem 3, (0.18), we were able to use perturbation theory to solve the
nonlinear training dynamics and evaluate the predictions of fully-trained networks
at ﬁnite width:
z(xδ; θ⋆) = [z(xδ; θ⋆)]

z, bH, d
dH, \
ddIH, \
ddIIH; algorithm projectors

.
(ε.12)
Here, the details of the algorithm dependence of the prediction is manifest, captured
entirely by a handful of algorithm projectors, cf. (∞.142)–(∞.147).
Combining these insights, we found that the fully-trained distribution,
p

z(T)

≡p

z(T)
y, G, H, V, A, B, D, F, P, Q, R, S, T, U

+ O
 1
n2

,
(ε.13)
is a nearly-Gaussian distribution; its statistics are entirely described by the conditional
variables listed here, though we’ve suppressed the sample indices in order to ﬁt them all
on one line.5 In addition to the metric G and the NTK mean H, in this conditioning
we’re accounting for the ﬁnite-width data-dependent couplings arising from our decom-
positions of the four-point connected correlator of preactivations E [zzzz]connected, (4.77),
the NTK-preactivation cross correlator E
h d
∆Hzz
i
, (8.67), the NTK variance E

d
∆H
2
,
(8.82), the dNTK-preactivation cross correlator E
h
d
dHz
i
, (11.45), and the means of the
ddNTKs E
h
\
ddIH
i
and E
h
\
ddIIH
i
, (∞.11) and (∞.12). Importantly, all of these ﬁnite-
width tensors at O(1/n) are functions of exactly four input samples each, e.g. for the
four-point vertex we have V(δ1δ2)(δ3δ4) ≡V (xδ1, xδ2, xδ3, xδ4), and the speciﬁc functional
forms of these data-dependent couplings determine the overall data dependence of the
distribution. Thus, though slightly more complicated than the inﬁnite-width description
(ε.9), the solution truncated to O(1/n), (ε.13), is a nearly-sparse description, depending
only on two-hands-full of objects in a nearly-simple way.
Model Complexity of Fully-Trained Neural Networks
These eﬀective theory results, (ε.9) and (ε.13), should make it clear that for overparam-
eterized neural networks, it is no longer appropriate to identify the number of model
5Note that we’ve also suppressed the algorithm projectors in this conditioning, presuming that we
are considering a ﬁxed learning algorithm: once an algorithm is ﬁxed, the projectors can only be ﬁxed
functions of any training set tensors that contain O(1) terms – i.e. the metric submatrix and the NTK
mean submatrix, both evaluated on the training set only, eG˜α1 ˜α2 and e
H˜α1 ˜α2 – and of the global learning
rate, η, see e.g. (∞.142)–(∞.147) for gradient descent. Thus, the algorithm dependence is taken care of
entirely by the tensors we’re conditioning on already.
395

parameters with the model complexity. Consider a ﬁxed combined training and test
dataset of size ND:
• For the Gaussian distribution, (ε.9), that describes an ensemble of fully-trained
inﬁnite-width networks, we only need
noutNA +
ND(ND + 1)
2

+
ND(ND + 1)
2

= O

N2
D

(ε.14)
numbers in order to completely specify the distribution, with each term corre-
sponding to the numbers needed to enumerate yi;˜α, Kδ1δ2, and Θδ1δ2, respectively.
• For the nearly-Gaussian distribution, (ε.13), that describes an ensemble of fully-
trained ﬁnite-width networks with small-but-nonzero aspect ratios, 0 < r ≪1, we
now need
O

N4
D

(ε.15)
numbers in order to completely specify the distribution, with the counting domi-
nated by the ﬁnite-width tensors, each of which having exactly four sample indices.
Thus, while each inﬁnite-width network has an inﬁnite number of microscopic model
parameters, its macroscopic data-dependent couplings are only quadratic in samples.
Meanwhile, our ﬁnite-width networks have less model parameters than our inﬁnite-
width network – i.e. ﬁnite < inﬁnite – but their macroscopic eﬀective description is
more complicated! What we have found here is the manifestation of the microscopic-
macroscopic duality: under this duality, complexity in parameter space is transformed
into simplicity in sample space, and density in model parameters is exchanged for spar-
sity in data-dependent couplings. In the overparameterized regime, this duality indicates
that we really should identify the model complexity with the data-dependent couplings
rather than the model parameters.
To further elaborate on this general point, we could imagine carrying out our ﬁnite-
width 1/n expansion, (0.15), to higher orders as
p

z(T)

= p{0}
z(T)

+
p{1}
z(T)

n
+
p{2}
z(T)

n2
+ O
 1
n3

.
(ε.16)
To begin, now the preactivation distribution at initialization,
p

z
D

= 1
Z e−S(z) + O
 1
n3

,
(ε.17)
396

will be eﬀectively described in terms of a sextic action,
S(z) ≡1
2
nL
X
i=1
X
δ1,δ2∈D
gδ1δ2zi;δ1zi;δ2
(ε.18)
−1
8
nL
X
i1,i2=1
X
δ1,...,δ4∈D
v(δ1δ2)(δ3δ4)zi1;δ1zi1;δ2 zi2;δ3zi2;δ4
+ 1
24
nL
X
i1,i2,i3=1
X
δ1,...,δ6∈D
u(δ1δ2)(δ3δ4)(δ5δ6)zi1;δ1zi1;δ2 zi2;δ3zi2;δ4 zi3;δ5zi3;δ6 .
In this action, the sextic coupling scales as
u(δ1δ2)(δ3δ4)(δ5δ6) = O
 1
n2

,
(ε.19)
and leads to a nontrivial connected six-point correlator characterized by a six-point
vertex: U(δ1δ2)(δ3δ4)(δ5δ6).6 At criticality, this connected correlator scales as
1
n2 U(δ1δ2)(δ3δ4)(δ5δ6) ∝O
 
L2
n2
!
,
(ε.22)
consistent with the expectations of our eﬀective theory cutoﬀ. Thus, we expect that the
reﬁned description, (ε.16), is accurate to order L2/n2, but at the cost of a signiﬁcant
increase in the model complexity: the counting will be dominated by the 1/n2 ﬁnite-
width tensors – each of which has six sample indices – and so we now require
O

N6
D

(ε.23)
numbers in order to specify all the data-dependent couplings of the distribution. In
general, to achieve an accuracy of order Lk/nk, we expect that a macroscopic description
p

z(T)

=
k
X
m=0
p{m}
z(T)

nm
+ O
 
Lk+1
nk+1
!
,
(ε.24)
6We computed this vertex in the second layer in footnote 8 of §4.2, and we’ve further taught you
everything that you need to know in order to extend the computation of such higher-point correlators
to deeper layers, and then analyze their scaling at criticality. As a checkpoint for your algebra, the
single-input recursion for the six-point vertex specialized to the ReLU activation function is
U (ℓ+1) = 1
8
h
C(ℓ+1)
W
i3 h
U (ℓ) + 30V (ℓ)K(ℓ) + 44  K(ℓ)3i
,
(ε.20)
which at criticality has a solution
U (ℓ)
n2 (K(ℓ))3 = 75 ℓ2
n2 + . . . .
(ε.21)
397

will have its model complexity dominated by data-dependent couplings with 2k-sample
indices, requiring
O

N2k
D

(ε.25)
numbers. In this way, the 1/n expansion gives a sequence of eﬀective theories with
increasing accuracy at the cost of increasing complexity.7
Importantly, for any particular network architecture that we want to describe, as
the depth-to-width ratio r ≡L/n increases, we’ll in principle need to include more and
more of these higher-order terms, making our macroscopic eﬀective theory more and
more complex:
• In the strict limit r →0, the interactions between neurons turn oﬀ, and the sparse
O
 N2
D
 Gaussian description of the inﬁnite-width limit, (ε.9), will be accurate.
Such networks are not really deep, as L/n = 0, and they do not learn representa-
tions (§10.4.3).
• In the regime 0 < r ≪1, there are small nontrivial interactions between neurons,
and the nearly-sparse O
 N4
D
 nearly-Gaussian description of the ﬁnite-width ef-
fective theory truncated at order 1/n, (ε.13), will be accurate. Such networks are
wide while at the same time having nontrivial depth, L/n ̸= 0, and they do learn
representations (§11.4.3).
• For larger r, the neurons are strongly-coupled, and a more generic O
 N2k
D
 non-
Gaussian description, (ε.24), would in principle be necessary. However, in this
case the the macroscopic perspective leads to an ineﬀective description that is not
7Note here that this counting is for the union of the training set and the test set, D = A ∪B, rather
than just for the training set A; in other words, the stochastic predictions made on a test sample, x ˙β,
will necessarily depend on that test sample. In particular, every time you want to make a prediction on
a new sample that you haven’t predicted before, ND increases in size, and so does your description of
the joint distribution – (ε.9) and (ε.13) – over the entire dataset D. Statistical models that have this
property are sometimes called non-parametric models, since the “parameters” of the full distribution
– i.e. the data-dependent couplings – depend on data points that you may not have even seen yet.
From the macroscopic perspective, the description of any single prediction scales with the size of the
training set as O (NA + 1)p
≈O(N p
A); in principle you can just plug x ˙β into a prediction formula such
as (∞.154). From the microscopic perspective, if we train a model and ﬁnd a solution θ⋆≡θ⋆(A) for
the model parameters given a training set A, then in practice we can then forget about that training
set and simply make predictions as z(x ˙β; θ⋆) – paying only the computation complexity cost of using
the model to make a prediction. Thus, in deep learning we can think of this non-parametric growth of
macroscopic model complexity with the size of the test set as similar in nature to the microscopic O(P)
complexity of the forward pass needed to make a new prediction.
Potentially it may have been useful to tell you – at the very least in order to understand this epilogue’s
epigraph – that a non-parametric model based on a Gaussian distribution is called a Gaussian process,
and accordingly people sometimes say that neural networks in the inﬁnite-width limit are Gaussian
processes.
Similarly, if you wanted to talk about the distribution over ﬁnite-width networks in the
context of non-parametric statistics, you might call it a nearly-Gaussian process. These processes
are distributions over functions z(x), where e.g. z is the trained model output function. However, the
reason why we haven’t brought this up before is that we ﬁnd this terminology unnecessary: for any ﬁxed
dataset D, we don’t have to worry about distributions over functions and instead can just think about
a joint distribution over the ﬁnite sets of outputs evaluated on the dataset.
398

tractable, and relatedly, we do not expect such networks to be practically useful
for machine learning tasks (§3.4).8
In this way, our eﬀective theory cutoﬀscale r governs model complexity of the statistics
needed to faithfully describe the behavior of diﬀerent neural networks. The simplicity
of the macroscopic perspective only emerges for small values of the cutoﬀr.
With that in mind, the practical success of deep learning in the overparameterized
regime and the empirical accuracy of a simple scaling hypothesis is really telling us that
useful neural networks should be sparse – hence the preference for larger and larger
models – but not too sparse – so that they are also deep. Thus, from the macroscopic
perspective, a nearly-sparse model complexity is perhaps the most important inductive
bias of deep learning.
♦♦♦♦
For an information-theoretic estimate of the depth-to-width ratio r⋆for which the
wide attraction of simplicity and the deep need of complexity are balanced to the end of
near-sparsity, please feel free to ﬂip the page and make your way through Appendix A.
8In §3.4, we were able to access this regime in the special case of deep linear networks. There, we
saw that higher-point connected correlators can grow uncontrollably even when the network is tuned to
criticality, and there’s no reason to expect that this would be any more favorable for nonlinear activation
functions. Moreover, as we discussed in §5.4, the growth of these higher-order correlators in networks
for all choices of activation functions will lead to large ﬂuctuations from instantiation-to-instantiation,
meaning that the ensemble description (ε.24) can no longer be trusted for any particular network.
Altogether, this suggests that networks of an aspect ratio r that require large sample-space complexity,
O N 2k
D

with large k, will generically exhibit strongly-coupled chaos; we do not expect such networks to
be eﬀectively describable or practically trainable.
Even if we could ﬁnd a workable network with such a large complexity, would we ever need it? As
a gedanken model, let’s consider an ineﬀective description with almost no truncation, say k ∼ND,
which comes with an exponential number of data-dependent couplings, O N ND
D

. Such an exponentially
complex description would only really be appropriate when we have unstructured data: e.g. if we have
a binary labeling f(x) = {0, 1} for which each label is chosen randomly, then the number of possible
functions for ND uncorrelated data points is 2ND; each time we want to incorporate a new input-
output pair into our dataset, we’d have to double the complexity of our description. Such unstructured
data is at odds with one of the main purposes of machine learning: recognizing patterns in the data –
i.e. correlations – which allow for the learning of representations and the ﬁnding of sparse descriptions.
In fact, as there’s no eﬃcient way to learn such unstructured datasets – see e.g. the no-free-lunch theorem
of [74, 75] – in practice we cannot possibly require these ineﬀective descriptions for any realistic machine
learning scenarios.
399

400

Appendix A
Information in Deep Learning
What can we demand from any physical theory? . . . nature [is] a diﬃcult problem, but
not a mystery for the human mind.
Ludwig Boltzmann [76]
In our Initialization, §0, we introduced our eﬀective theory approach to understanding
neural networks via the lens of theoretical physics. In particular, we discussed how ther-
modynamics was used to clarify the behavior of artiﬁcial machines such as the steam
engine, and then we described how statistical mechanics was developed to explain how
these macroscopic laws arise from the statistical behavior of many microscopic elements.
With this perspective, we suggested that a similar framework might be applied to the dif-
ﬁcult problem of deep learning theory, which we have now demystiﬁed from Pretraining
all the way to the End of Training, §1 to §∞.
In this ﬁrst appendix, we’ll make the connection between deep learning and these
ﬁelds of physics even more detailed. To do so, we’ll reformulate a few of our previous
results in terms of information theory. Initially formalized by Shannon as a way of
quantitatively understanding digital communication, information theory was developed
about half a century after statistical mechanics and is the statistical microscopic theory
most ﬁtting for the digital Information Age.
Although they a priori consider very diﬀerent subject matter, both statistical me-
chanics and information theory share a joint language and ultimate focus on the same
main fundamental concept: entropy. A particularly nice organization of entropies deﬁnes
the mutual information, a positive quantity that can be used to characterize how much
the measurement of one observable can inform us about another. This gives a nice way
to quantify the overall statistical dependence due to the interactions of random variables.
As such, the ﬁrst section of this appendix (§A.1) will give a self-contained introduction
to entropy and mutual information, making sure to point out the connections between
the very physical and analog setting of statistical mechanics and the very abstract and
digital world of information theory.
With these new tools, we will be able to further understand information in deep
401

learning. In particular, for inﬁnite-width neural networks (§A.2) we’ll ﬁnd new perspec-
tives on the non-interaction of neurons within a layer and on the necessity of criticality
for preserving the information essential for distinguishing between input samples in deep
networks. Then, at ﬁnite width (§A.3) we’ll use a variational principle to demonstrate
how the principle of maximum entropy for nearly-Gaussian distributions enables us to
compute entropies and informations up to order 1/n3 while only needing to know the
eﬀective ℓ-th-layer preactivation distribution truncated at order 1/n.
At order 1/n2, this will let us see a nonzero mutual information between groups of
neurons in a layer that grows quadratically with depth, further quantifying the inter-
action of neurons at ﬁnite width and providing an information-theoretic interpretation
and generalization of our Hebbian learning result from §6.4.1.
At order 1/n3, we will see how to pick a depth-to-width ratio, r ≡L/n, that max-
imizes the mutual information as a functional of the activation function. This optimal
aspect ratio, r⋆, arises from an unsupervised learning objective, and deﬁnes an activation-
function-dependent scale that separates eﬀectively-deep networks – that perform well –
from overly-deep networks – that are no longer trainable.
Also at order 1/n3, a generalization of the mutual information for three groups of
neurons will show that the information in a ﬁnite-width layer is stored redundantly
between neurons. This analysis provides a new perspective on coarse-graining mecha-
nism of RG ﬂow by enabling us to understand how the information from inputs gets
represented by, and shared among, the deeper-layer neurons.
Finally, note that while most of the computations presented here focus on the prior
distribution of preactivations as a means of investigating the inductive bias of the net-
work architecture and activation function, these information-theoretic tools naturally
extend to the various joint distributions we’ve considered throughout the book as well
as the Bayesian posterior distribution and the complicated distributions of fully-trained
networks. This leaves many more things to be computed, and so we hope that this
chapter provides a useful introduction to a new toolkit that can be used for furthering
your understanding of deep learning. In our second and ﬁnal appendix, §B, we’ll give a
residual example to demonstrate this in the setting of residual networks.
A.1
Entropy and Mutual Information
In this section we give a very brief overview of the concepts of entropy and mutual
information that play essential roles both in statistical mechanics [77] and information
theory [78, 79].
Let’s start with a discrete random variable x ∈X governed by the probability dis-
tribution p(x). For this discussion, it is nice to think of x as a particular observable
outcome, and X as the set of possible outcomes. The entropy of a probability distri-
bution is given by
S[p(x)] ≡−
X
x∈X
p(x) log p(x) ,
(A.1)
which is a functional of the distribution, taking a distribution as an argument and
402

outputting a number. Thus, we should think of the entropy as a property of an entire
probability distribution.
To gain some intuition for why this quantity could be useful, let’s consider its two
extremes. First, when the distribution is perfectly ordered – that is, p(x) = δxx′ such
that p(x′) = 1 with absolute certainty for a particular outcome, x′ ∈X, and zero for all
others – then the entropy is minimal, given by
S[p(x)] = −
X
x∈X
δxx′ log(δxx′) = 0 ,
(A.2)
since x′ contributes 1 log(1) = 0 and the other values of x contribute 0 log(0) = 0.
Second, when the distribution is completely disordered – that is, p(x) = 1/|X|, such
that the possible outcomes x are distributed uniformly and no outcome is more likely
than any other – then entropy is maximal, given by
S[p(x)] = −
X
x∈X
1
|X| log
 1
|X|

= log(|X|) .
(A.3)
For this reason, in physics the entropy is often regarded as a measure of the disorder
of a system characterized by a random variable x.1 In this maximal case when each
outcome is equally likely, the entropy can also be interpreted as a way of counting the
number of states of a system – i.e. the number of distinct outcomes in X – since it’s
equal to the logarithm of such a count.2
1Since we’re currently wearing our physicist hats, one remark is in (dis)order for the units of the
entropy. As per our discussion of dimensional analysis in footnote 15 of §1.3, the logarithm of a prob-
ability has the same units as the action, and for the same reason must be dimensionless. Nevertheless,
by changing the base of the logarithm, we can change the multiplicative constant in front of (A.1), and
thus change the meaning of the entropy:
• With our physicist hats still on, we would use the natural logarithm with base e, which measures
entropy in units with a very silly name called nats. (Some physicists also multiply the expres-
sion (A.1) by the Boltzmann constant kB, as is natural in macroscopic applications of the entropy
in thermodynamics, which gives the entropy the unit of joules per kelvin.)
• With our computer scientist hats on, which we put on in anticipation of the next paragraph in
the main text, we would use the logarithm with base 2, which measures units in binary digits
or the hopefully familiar bits. This is most natural in an information theory context, for which
the entropy (A.1) is sometimes called the Shannon entropy. The reason bits are also used as the
units for computer memory is due to the counting-of-states intuition for the entropy: a hard drive
that can store 109 bits has 2109 unique states, with a priori equal plausibility for any particular
arrangement of those bits, i.e. for any particular state of the system.
In this chapter, we’ll use the natural base e, which is also very natural when studying the entropy of
Gaussian distributions and nearly-Gaussian distributions.
2This connection between the uniform distribution over a ﬁnite number of states and the maximum
of the entropy is an example of the principle of maximum entropy and descends from what’s called
Laplace’s principle of indiﬀerence: without any other information, all the a priori probabilities for a
system to be in any particular state should be equal. (As we will discuss in §A.3, when we do have some
information about the state, then the entropy is no longer maximized by a uniform distribution.)
Note that this indiﬀerence principle is applied to macroscopic thermodynamic states of (nearly-)equal
403

To gain even more intuition, let’s consider a perspective from information theory. In
the entropy formula (A.1), the quantity inside the expectation is called the surprisal:
s(x) ≡−log p(x) = log
 1
p(x)

;
(A.4)
since for a particular outcome, x, the distribution p(x) quantiﬁes the frequency or plausi-
bility of observing x, and since the logarithm is monotonic in its argument, the surprisal
grows with the a priori rareness of the outcome x, and hence quantiﬁes how surprising or
informative actually observing a particular x is. As such, the surprisal is also a quantita-
tive measure of how much new information is gained after making such an observation.
Averaging the surprisal over all possible outcomes, X, gives the entropy (A.1), which can
thus be understood as the expected surprise or amount of information to be gained from
making an observation:
S[p(x)] ≡E [s(x)] .
(A.5)
In addition to admitting various nice interpretations, the entropy also obeys a few
nice mathematical properties. To start, it is manifestly nonnegative:
S[p(x)] ≥0 .
(A.6)
You can see this by noting that the allowed range of a probability, 0 ≤p(x) ≤1, implies
that the nonnegativity of the quantity, −p(x) log p(x) ≥0, which in turn implies that the
entropy (A.1) is a sum of nonnegative terms. Moreover, the entropy takes its minimum
value and vanishes, (A.2), if and only if the distribution is perfectly ordered, given by a
Kronecker delta for one particular outcome.
Another important property of the entropy is its additivity for two random variables
x ∈X and y ∈Y that are described by a factorized joint distribution, p(x, y) = p(x) p(y),
and thus are statistically independent (1.79):
S[p(x, y)] = −
X
x∈X,y∈Y
p(x, y) log p(x, y)
= −
X
x∈X,y∈Y
p(x)p(y) [log p(x) + log p(y)]
= −
X
x∈X
p(x) log p(x) −
X
y∈Y
p(y) log p(y)
= S[p(x)] + S[p(y)] .
(A.7)
energy as a principal assumption of microscopic statistical mechanics, and the associated entropy, (A.3),
is sometimes called the Boltzmann entropy. In contrast, the fully-general formula (A.1) is sometimes
called the Gibbs entropy in the context of statistical mechanics.
Finally, for a Bayesian, this principle of indiﬀerence oﬀers a natural way to pick a set of prior beliefs,
and a prior distribution that respects this indiﬀerence principle is sometimes called a non-informative
prior. While motivated in part by the Occam’s razor heuristic, a Bayesian’s subjective adoption of such
a prior is very diﬀerent from the automatic and objective embodiment of Occam’s razor by the Bayes’
factor, cf. our discussion of Bayesian model comparison in §6.2.2.
404

Intuitively, if two observables are independent, then the expected amount of information
learned from making an observation of each is just the sum of the information learned
from making each individual observation. For macroscopic systems with small proba-
bilities for individual outcomes, this makes the entropy a practically useful quantity to
work with: while the probability of independent outcomes multiply and create smaller
and smaller numbers, the surprisals, and thus the entropies, simply add.
The additivity property (A.7) has a very physical interpretation: the entropy is
typically an extensive quantity, meaning that it scales with the number of degrees of
freedom or microscopic size of the system. This should make sense given the counting-
of-states interpretation of the entropy: if a variable x describes the potential contents
of a 1 TB hard drive, and the variable y independently describes the potential contents
of another 1 TB hard drive, then the total storage capacity of the hard drives together
is additive and equal to 2 TB. As the number of states available to the combined “hard
drive” system is the product of the states of the individual “hard drive” systems, their
entropies are additive.
However, if the hard drives are not independent and constrained so that one hard
drive mirrors the other, then their total storage capacity is subadditive and instead would
be equal to 1 TB.3 In this case, the system had only half as many degrees of freedom
as we naively thought it had due to the strict constraint creating strong correlations
between the contents of the hard drives.
Thus, more generally for two statistically dependent observables constrained by a
nonzero interaction between them, the entropy obeys a property called subadditivity:
S[p(x, y)] < S[p(x)] + S[p(y)] .
(A.8)
In words, this means that the entropy of a joint distribution p(x, y) will be always less
than or equal to the sum of the entropies of the marginal distributions p(x) and p(y).
This is also physically intuitive: if two random variables are statistically dependent,
then an observation of x conveys information about the likely outcome of making an
observation of y, and so an observation of y doesn’t convey as much information as it
would have if we didn’t already know x.4
3This conﬁguration has a practical realization called RAID 1, where “RAID” stands for Redundant
Array of Inexpensive Disks, and allows for fault tolerance by creating data redundancy between the two
hard drives.
4For the mathematically curious, we can turn this intuition into a quick proof. The usual route is to
ﬁrst prove the Jensen inequality and then apply it to an auxiliary object called the Kullback–Leibler
(KL) divergence.
First let’s state and then prove the Jensen inequality. Consider a discrete probability distribution
over N elements aµ=1,...,N with corresponding probabilities pµ such that PN
µ=1 pµ = 1. Next, consider
a convex functions f(a), i.e. a function that satisﬁes
f λa1 + (1 −λ)a2

≥λf(a1) + (1 −λ)f(a2) ,
(A.9)
for any λ ∈[0, 1] and for any numbers a1 and a2 in the domain of the function. The Jensen inequality
states that the expectation of such a function is greater than or equal to the function applied to the
mean:
E [f(a)] ≥f(E [a]) .
(A.10)
405

Turning this argument upside down and shuﬄing (A.8) leftside right, let us deﬁne
the mutual information between two random variables as:
I [p(x, y)] ≡S[p(x)] + S[p(y)] −S[p(x, y)]
(A.14)
=
X
x∈X,y∈Y
p(x, y) log
 p(x, y)
p(x) p(y)

.
This is a functional of a joint probability distribution and gives an average measure of
how much information an observation of x ∈X conveys about an observation of y ∈Y,
and vice versa. Rearranged in this way, we see that the subadditivity of the entropy (A.8)
implies the nonnegativity of the mutual information,
I [p(x, y)] ≥0 ,
(A.15)
with equality holding when and only when the sets of observable outcomes, X and Y, are
This can be proved by induction on N as follows. First, note that (A.10) holds trivially when N = 1.
Then, see that
E [f(a)] ≡
N+1
X
µ=1
pµf(aµ) =pN+1f(aN+1) + (1 −pN+1)
N
X
µ=1
pµ
1 −pN+1 f(aµ)
(A.11)
≥pN+1f(aN+1) + (1 −pN+1) f
 N
X
µ=1
pµ
1 −pN+1 aµ
!
≥f
 
pN+1aN+1 + (1 −pN+1)
N
X
µ=1
pµ
1 −pN+1 aµ
!
= f(E [a]) ,
where in going from the ﬁrst line to the second line we used the Jensen inequality (A.10) for N elements,
and in going from the second line to the third line we used the convexity of the function (A.9).
As the next step, let us introduce the KL divergence (sometimes known as the relative entropy) between
two diﬀerent probability distributions p(x) and q(x) for a discrete variable, x ∈X,
KL [p(x) || q(x)] ≡
X
x∈X
p(x) log

p(x)
q(x)

= −S[p(x)] + S[p(x), q(x)] ,
(A.12)
which is a multi-function functional of both distributions, p(x) and q(x), and importantly is not sym-
metric in its function arguments. Here S[p(x), q(x)] ≡−P
x∈X p(x) log q(x) is an asymmetric quantity
known as the cross entropy. As ﬁrst mentioned in footnote 16 of §10.2.3, the KL divergence is a asym-
metric measure of the closeness of two diﬀerent distributions and is closely related to the cross-entropy
loss (10.36).
Finally, let’s show that the KL divergence is nonnegative by applying the Jensen inequality to the
convex function f(a) = −log(a) with discrete elements aµ = q(x)/p(x):
KL [p(x) || q(x)] ≡
X
x∈X
p(x) log

p(x)
q(x)

≥−log
"X
x∈X
p(x)q(x)
p(x)
#
= −log(1) = 0 .
(A.13)
To complete our proof, note that the positivity of the KL divergence (A.13) implies the subadditivty of
the entropy (A.8) for a choice of distributions as KL [p(x, y) || p(x) p(y)].
406

statistically independent.5 Thus, the mutual information of a joint distribution is telling
us something about the interactions that create the nontrivial correlations that are a
signature of statistical dependence. We’ll compute explicitly the mutual information
of preactivations for inﬁnite-width neural networks in §A.2 and for ﬁnite-width neural
networks in §A.3: as you might imagine, the interaction of neurons at ﬁnite width will
lead to a nonzero mutual information.
As the last preparation before such computations, we will need to extend the deﬁ-
nition of the entropy (A.1) from discrete outcomes to continuous random variables as
S[p(x)] ≡−
Z
dx p(x) log p(x) = E [s(x)] .
(A.16)
While the entropy is still the expectation of the surprisal (A.4), to take that expectation
we now have to evaluate an integral rather than compute a sum. As passing to the
continuum actually involves a physically interesting subtlety, let’s take a few paragraphs
to unravel this deﬁnition (A.16).
First, let’s understand this subtlety mathematically. Since our random variable is
continuous, we can make a smooth and monotonic change of coordinates in our con-
tinuous outcome space X from x to ex ≡ex(x). Since such coordinates are arbitrary,
consistency requires that the probability of observing x in the interval between [x1, x2]
be the same as the probability of observing ex in [ex(x1), ex(x2)]. In equations, this means
that
p(x1 < x < x2) ≡
Z x2
x1
dx p(x)
(A.17)
must equal
p(ex1 < ex < ex2) ≡
Z ex(x2)
ex(x1)
dex p(ex) =
Z x2
x1
dx dex
dx p

ex(x)

,
(A.18)
where in the last step we used the standard transformation property of the integral
measure under a change of coordinates. Thus, the two distributions p(ex) and p(x) must
be related as
p

ex(x)

≡dx
dex p(x) ,
(A.19)
which is the well-known coordinate transformation formula for a probability density
of a single variable. What about the expected surprisal? Under this same coordinate
5Note that the mutual information (A.14) can also be thought of as the KL divergence (A.12) between
the joint distribution p(x, y) and the product of the marginal distributions p(x) p(y).
That is, the
mutual information I [p(x, y)] tells us how close a joint distribution is to being a product of independent
distributions, with a nonzero mutual information signaling statistical dependence.
407

transformation, the entropy (A.16) is given by
S[p(ex)] = −
Z
dex p(ex) log p(ex)
(A.20)
= −
Z
dx dex
dx
dx
dex p(x)

log p(x) + log
dx
dex

=S[p(x)] +
Z
dx p(x) log
dex
dx

,
where in the second line we again used the standard transformation property of the
integral measure under a change of coordinates and we also used (A.19) to express the
probability distribution in the old coordinates. So with a general (monotonic) change of
coordinates, we can make the entropy take any value we’d like by a judicious choice of
the Jacobian of the transformation, dex/dx, even a negative one!
Now, let’s understand the physical meaning of this subtlety. Let’s consider a coordi-
nate change that’s just a multiplicative factor, ex = cx, which is like changing the base
unit that we use to measure the observable x from inches to meters. In this case, the
surprisal of each particular outcome shifts by the same constant, ∆s(x) = log c, and thus
so does the entropy. Thus, for continuous quantities the entropy is additively sensitive
to the choice of units with which we measure our observable quantities, and we really
should specify these units along with the deﬁnition of the entropy (A.16).6
Perhaps the most sensible choice is to set the measuring units according to the
smallest possible measurements of x that we can physically distinguish – in other words,
according to the precision limit set by the constraints of the physical world. This preci-
sion limit ϵ is sometimes called a cutoﬀ, and in practice means that we only care about
the discrete probability of ﬁnding x between [x0, x0 + ϵ].7 Such a discretization of the
outcome space will then ensure that the entropy is always positive, (A.6), since we’re
6For any dimensionful observable, the probability density p(x) must also be dimensionful so that
the probability of observing x in the interval [x1, x2], p(x1 < x < x2) ≡R x2
x1 dx p(x), is properly
dimensionless.
Yet, putting such a dimensionful object p(x) into an argument of the logarithm as
log p(x) is illegitimate, as we discussed in footnote 15 of §1.
This is another way of seeing that for
continuous probability distributions, i.e. probability densities, we need to specify the measuring units to
properly deﬁne the entropy.
For the curious and potentially worried reader, it should be noted here that the Bayes’ factor (6.30)
that contained the observation dependence of our Bayesian model comparison is invariant under a coordi-
nate transformation of our observations yA. What ultimately mattered there was the relative magnitude
of the evidence of diﬀerent hypotheses, and not the absolute value of the individual evidences.
Also, please do not confuse this issue of the units for a probability density, which change the entropy
by an additive constant, with the units of entropy that we discussed in footnote 1, which change the
entropy by a multiplicative constant. Note that for discrete probability distributions, the probability
distribution gives a simple probability and is already dimensionless, cf. our brief discussion in footnote 1.
7Please don’t confuse our measurement precision cutoﬀhere with the perturbative cutoﬀof the
eﬀective theory, the depth-to-width ratio r ≡L/n.
While in both cases we can think of them as
important scales, they have very diﬀerent physical meanings: in the former case, the precision cutoﬀgives
the minimum distinguishable diﬀerence between measurements of two observables, ϵ ≡min(|z2 −z1|);
in the latter case, the cutoﬀof the eﬀective theory, L/n, sets the scale at which ﬁnite-width corrections
need to be taken into account in the preactivation distribution p(z).
408

now dealing with discrete probabilities again.8
If this physical sensitivity of the entropy to the somewhat arbitrary cutoﬀbothers
you, then perhaps you should consider the continuous analog of mutual information,
I [p(x, y)] ≡S[p(x)] + S[p(y)] −S[p(x, y)]
(A.21)
=
Z
dxdy p(x, y) log
 p(x, y)
p(x) p(y)

,
where the deﬁnition in terms of the entropy is the same as in the discrete case, (A.14). In
particular, mutual information is insensitive to the choice of the measuring coordinates.
To see why, let’s consider two continuous random variables x and y and independent
monotonic coordinate transformations,
p

ex(x)

= dx
dex p(x) ,
p

ey(y)

= dy
dey p(y) ,
p

ex(x), ey(y)

= dx
dex
dy
dey p(x, y) ,
(A.22)
where to get this we applied a similar consistency-of-probabilities argument to the one
that we gave above. With this in mind, we can now show that the mutual information
(A.21) stays invariant under these coordinate transformations:
I [p(ex, ey)] ≡S[p(ex)] + S[p(ey)] −S[p(ex, ey)]
(A.23)
=
Z
dexdey p(ex, ey) log
 p(ex, ey)
p(ex) p(ey)

=
Z
dxdy p(x, y) log
 p(x, y)
p(x) p(y)

≡I [p(x, y)] .
In going from the second line to the third line, the coordinate transformation factors
dx/dex and dy/dey completely cancelled inside the logarithm, and the transformation
of the measure cancelled the coordinate transformation factors outside the logarithm.
Thus, the mutual information is completely well deﬁned for continuous random variables,
independent of the cutoﬀϵ. For this reason, with a consistent and ﬁxed choice of units,
it’s completely valid to compute the entropy as an intermediate step in the computation
of the mutual information, and we will make use of this fact in the following sections.
Finally, note that the notion of the mutual information can be extended to more than
two random variables. For instance, in §A.3 we will consider the tripartite information
among three random variables x ∈X, y ∈Y, and z ∈Z:
I3 [p(x, y, z)]
(A.24)
≡I [p(x, y)] −I [p(x, y|z)]
=S[p(x)] + S[p(y)] + S[p(z)] −S[p(x, y)] −S[p(y, z)] −S[p(z, x)] + S[p(x, y, z)]
=
X
x∈X,y∈Y,z∈Z
p(x, y, z) log
 p(x, y) p(y, z) p(z, x)
p(x) p(y) p(z) p(x, y, z)

.
8In the context of deep learning, a natural choice is the precision limit of the ﬂoating-point represen-
tation of the network’s variables. Since type float eponymously has a precision that’s relative to the
value being stored, one could perhaps choose the minimum precision in the relevant range.
409

Here, I [p(x, y|z)] is the mutual information of the joint distribution between x and
y conditioned on z, p(x, y|z), and the ﬁnal expression for I3 [p(x, y, z)] makes it clear
that (a) it is fully symmetric in its three arguments, and that (b) its continuous analog
that’s in your imagination is cutoﬀindependent and invariant under similar coordinate
transformations as those in (A.22).
What is not immediately obvious is that tripartite information can be either positive
or negative. From the ﬁrst expression in (A.24), we can gain some intuition for the
meaning of the tripartite information: it is a measure of whether knowledge of a third
random variable, z in this way of writing the expression, increases or decreases the mutual
information between the other two variables. When positive, it indicates that z contains
information about x and y, and so knowing z decreases the amount of information you’d
learn about x by measuring y; the information is stored redundantly between these three
variables. When negative, it indicates that the information is distributed across x, y,
and z in such a way that you’d learn less about x by measuring y than you would with
ﬁrst knowing z; in this case, the information is stored synergistically between these three
variables.9
A.2
Information at Inﬁnite Width: Criticality
With that informative introduction out of the way, let’s us now make these abstract
deﬁnitions concrete by using them to better understand the neural-network prior distri-
bution.
To begin, let’s focus on m preactivations z(ℓ)
i;α from the ℓ-th layer of an inﬁnite-width
neural network. Depending on when you’re coming to this appendix from the main text,
it’s probably ingrained in your mind already that such preactivations are distributed
according to a zero-mean Gaussian distribution
p

z1, . . . , zm
D

=
1
q
|2πK|m exp

−1
2
m
X
i=1
X
α1,α2∈D
Kα1α2zi;α1zi;α2

,
(A.25)
where in this expression, and in this section, we will drop layer indices everywhere to
9An extreme example of positive tripartite information occurs when x, y, and z are completely
dependent and exact copies of each other. In this redundant case, I [p(x, y)] > 0, since knowledge of y
tells you everything about x, but I [p(x, y|z)] = 0, since conditioning on z means that there’s nothing
left for you to learn about x by also observing y. An example of such a situation would be three copies
of the same book.
An extreme example of negative tripartite information occurs when the joint distribution between
x and y factorizes, p(x, y) = p(x) p(y), but joint distribution conditioned on z does not, p(x, y|z) ̸=
p(x|z) p(y|z).
In this synergistic case, I [p(x, y)] = 0, since without z the variables are statistically
independent, but I [p(x, y|z)] > 0, since there are correlations that are mediated by z. An example of
such a situation could be a code that distributes a key among three diﬀerent parties: knowledge of any
two parts would give absolutely no information about the key, but with all three parts together the key
can be reconstructed.
410

declutter expressions. The entropy (A.16) of this distribution is then given by
S
h
p

z1, . . . , zm
D
i
=
**
1
2
m
X
i=1
X
α1,α2∈D
Kα1α2zi;α1zi;α2
++
K
+ log
q
|2πK|m

(A.26)
=m
2 (ND + log |2πK|) = m
2 log |2eπK| ,
where as a reminder |2πeK| is the determinant of the ND-by-ND matrix 2πeKα1α2.10
From this we conclude that entropy is extensive, proportional to the number of neurons
m in the joint distribution (A.25). This shows how the entropy can count the degrees
of freedom in a deep learning context – in this case, by counting the neurons – and the
exact additivity in the number of neurons signals to us that the individual neurons in
an inﬁnite-width layer are non-interacting and statistically independent.
To conﬁrm this directly, let’s work out the mutual information between two sets of
neurons, M1 = {1, . . . , m1} and M2 = {m1 + 1, . . . , m1 + m2}, in the same layer ℓ. In
excruciating detail for its triviality, we have
I
h
p

M1, M2
D
i
(A.27)
=S
h
p

z1, . . . , zm1
D
i
+ S
h
p

zm1+1, . . . , zm1+m2
D
i
−S
h
p

z1, . . . , zm1+m2
D
i
= [m1 + m2 −(m1 + m2)] 1
2 log |2eπK| = 0 .
where to go to the last line we used (A.26) three diﬀerent ways.
This zero mutual
information conﬁrms that at inﬁnite width learning the activities of some neurons in a
layer conveys no information about the activities of any of the other neurons. To ﬁnd a
ﬁnite result, we’ll have to back oﬀthe inﬁnite-width limit (§A.3).
That said, for a ﬁxed neuron we do expect nontrivial correlations between diﬀerent
inputs and therefore also a ﬁnite mutual information. To investigate this, let’s take two
inputs xi;+ and xi;−and compute the mutual information between the preactivations
for a particular neuron, z1;+ and z1;−. Plugging the entropy (A.26) into the deﬁnition
of the mutual information (A.14), we ﬁnd
I [p(z1;+, z1;−|x±)] =1
2
h
log(K++) + log(K−−) −log

K++K−−−K2
+−
i
(A.28)
=1
2 log
 
K++K−−
K++K−−−K2
+−
!
.
Focusing on inputs xi;± with equal norms such that K++ = K−−= K[0] + K[2] and
10In this and the next section, for convenience we will ignore the ambiguity in the deﬁnition of the
continuous entropy and use the formula (A.16) naively.
This is permissible since we ultimately are
interested in cutoﬀ-independent quantities, such as the mutual information, and when interpreting an
entropy such as (A.26) we will never care about its absolute value.
411

K+−= K[0] −K[2], cf. (5.17) and (5.19), we can rewrite this mutual information as
I [p(z1;+, z1;−)] = 1
2 log



1 +
K[2]
K[0]
2
4
K[2]
K[0]

,
(A.29)
which is parameterized entirely by the dimensionless ratio K[2]/K[0] that for nearby
inputs xi;± = xi;0 ± δxi/2 characterizes their relative angle after passing through the
network to the ℓ-th layer.
There are two interesting limits here. First, when K[2]/K[0] →0, the mutual informa-
tion becomes large. This follows because as the relative angle between the preactivations
vanishes, they become close to each other: knowing the preactivation for one input tells
us about the value of the preactivation for the other input. In a classiﬁcation problem,
this is a great prior if the inputs are from the same class, but would make learning
really diﬃcult if they’re from diﬀerent classes. Second, when K[0] = K[2], the mutual
information vanishes. This follows because in this limit the oﬀ-diagonal components of
the kernel, K+−= K[0] −K[2], vanishes: the preactivations become statistically inde-
pendent. In a classiﬁcation problem, this may be a good prior if the inputs are from
diﬀerent classes – so long as the details of the classes don’t correlate in some way – but
would make learning really diﬃcult if the inputs are from the same class. Altogether,
this suggests that as a prior we don’t want K[2]/K[0] to be too big or too small, which
can be best ensured by setting both χ∥= 1 and χ⊥= 1, cf. (5.55).
This gives an
information-theoretic perspective on criticality.
Finally, while we have focused here on the prior distribution for networks at ini-
tialization, we could also study these same quantities after learning using the Bayesian
posterior (6.66) or the fully-trained distribution of gradient-based learning (10.39). Since
the entropy of a Gaussian distribution is independent of its mean – it can be eliminated
by a change of dummy integration variables – the mutual information for either trained
inﬁnite-width network is given by this same expression, (A.28), but with the kernel re-
placed by the covariance of the Bayesian posterior distribution (6.57) or the covariance
of the generalized posterior distribution (10.41). In the latter case, the mutual infor-
mation will involve both the kernel and the frozen NTK, and its analysis would yield
similar results to those that we found in §10.3.1 when we investigated the bias-variance
tradeoﬀ. This would give an information-theoretic perspective on generalization.11
A.3
Information at Finite Width: Optimal Aspect Ratio
In this section, we’ll see how ﬁnite-width networks have a prior for nonzero mutual
information between diﬀerent neurons. Assuming that nonzero mutual information is
desirable by intuition – and by analogy to an unsupervised learning objective – we can
use this computation to optimize the depth-to-width ratio for ﬁnite-width MLPs at
11Analogously, studying the tripartite information generalization of (A.28) for either type of posterior
distribution would give an alternative perspective on the ∗-polation results of §10.3.2.
412

criticality. This optimal aspect ratio, r⋆, deﬁnes the scale that separates eﬀectively-deep
networks – describable by our eﬀective theory approach – from overly-deep networks –
not describable due to strong interactions and not trainable due to large ﬂuctuations.12
In general, the entropy and mutual information of any interacting theory are really
diﬃcult to compute. However, when the interactions are nearly-Gaussian, we can use
perturbation theory. To do so, there is actually a neat variational principle that we can
use to organize our calculations, so let us explain that ﬁrst.
As in the last section, we’ll focus on the distribution of m preactivations in layer ℓ,
drop (almost) all the layer indices, and focus exclusively on a single input x. As we have
been doing since the beginning of time (§1.3), let’s express the probability distribution
in terms of an action as
p(z1, . . . , zm|x) = e−S(z1,...,zm)
Z
,
(A.30)
which must be normalized by the partition function
Z =
Z " m
Y
i=1
dzi
#
e−S(z1,...,zm) .
(A.31)
In terms of these quantities, the entropy (A.16) can be expressed as
S[p(z1, . . . , zm|x)] = log(Z) + 1
Z
Z " m
Y
i=1
dzi
#
e−S(z1,...,zm)S(z1, . . . , zm) .
(A.32)
Just to make sure, please don’t get confused between the action S(z), which is a function
of the preactivations, and the entropy S[p(z)], which is a functional of the probability
distribution.13
To proceed, let’s adopt a variational ansatz: we’ll divide the action into two parts
as
S(z1, . . . , zm) = SF(z1, . . . , zm) + SI(z1, . . . , zm) ,
(A.33)
with the idea being that the second term, SI, encodes the part of the distribution that
is perturbatively small. Speciﬁcally, the variational principle instructs us to choose
the ﬁrst term in (A.33), SF(z), that gives no variations of the entropy with respect to
SI(z):
0 = δS[p(z)]
δSI(z)

SI(z)=0
.
(A.34)
12Note that in this section, we’ll need the expressions for the running couplings that we derived in §4.4
when ﬁnding the eﬀective distribution of m neurons in a wide-but-ﬁnite layer. As such, it may be helpful
to reread that section before proceeding.
13For choices of units of the preactivations z for which the partition function is unity, Z = 1, the action
is simply the surprisal (A.4), and thus the entropy is the expectation of the action: S[p(z)] = E [S(z)] .
Note that in thermodynamics, the constant −log Z is sometimes called the free energy; this discussion
should make clear that only its relative value for two diﬀerent distributions is physical.
413

We’ll satisfy this shortly. Additionally, SI(z) should not be completely arbitrary, but
instead be constructed to properly reﬂect the statistics of the preactivation distribution
p(z1, . . . , zm|x).
The ﬁrst such constraint comes from demanding that the two-point
correlator, when computed with the variational action, is determined by the exact single-
input metric G:
E [zi1zi2] ≡δi1i2G .
(A.35)
The second constraint comes from demanding that the connected four-point correlator,
when computed with the variational action, is determined by the exact single-input
four-point vertex:
E [zi1zi2zi3zi4] |connected = 1
n (δi1i2δi3i4 + δi1i3δi2i4 + δi1i4δi2i3) V .
(A.36)
Together, the constraints (A.35) and (A.36) will ﬁx the couplings of the variation action
SI so that the full action (A.33) correctly speciﬁes the m-neuron preactivation distribu-
tion (A.30).14
To understand why we’re doing this, note that our variational principle is ultimately
just a realization of the principle of maximum entropy for nearly-Gaussian distri-
butions.15 In particular, ﬁrst note that the Gaussian distribution itself can be derived
as a distribution that maximizes the entropy (A.16), subject to the constraints of ﬁxed
ﬁrst and second moment.16 As we will see in a moment, in (A.34) we are maximizing
the entropy of the distribution with respect to the deformation of the action away from
Gaussianity, SI, subject to the constraint of ﬁxing the higher-order cumulants order by
order in perturbation theory. In general, the maximum entropy principle is an appro-
priate procedure when we have ﬁxed observable information for which we want to ﬁnd
an underlying distribution. Here, we actually know the distribution that produces G
14In principle there are additional constraints coming from the statistics of higher-point correlators,
but their contribution is subleading to both the leading and next-to-leading orders that we will work.
15For those readers that enjoy our historical asides, the maximum entropy principle was discovered by
Jaynes and provides a link between information theory on the one hand and statistical mechanics on the
other hand [80, 81]. As an example of this, consider a central problem in statistical mechanics: ﬁnd the
probability distribution pi for the fundamental microstates i of a system that has a macroscopic average
energy E ≡E [Ei] = P
i piEi. An application of the principle of maximum entropy then correctly picks
out the Boltzmann distribution (or sometimes, the Gibbs distribution) of statistical mechanics
pi ∝e−βEi ,
(A.37)
if we optimize the entropy (A.1) subject to the observable constraint for the energy, P
i piEi = E, and the
normalization condition for the distribution, P
i pi = 1. Here, β is a Lagrange multiplier that depends
on the energy E and has a physical interpretation as the inverse temperature, β = 1/(kBT), with the
aforementioned Boltzmann constant kB and the familiar-to-everyone temperature T. This example also
shows how statistical mechanics links the details of the fundamental microstates i to the macroscopic
thermodynamic variables such as E and T.
16For those of you keeping track: the maximum entropy distribution with no information ﬁxed is
the uniform distribution, cf. (A.3); the maximum entropy distribution with a ﬁxed ﬁrst moment is the
Boltzmann distribution, cf. (A.37); and the maximum entropy distribution with a ﬁxed ﬁrst and second
moment is the Gaussian distribution, cf. (nearly-)everywhere.
414

and V , (A.30), but we can still use this principle as convenient tool for computing the
entropy.17
Now, to satisfy the variational principle (A.34), let’s choose
SF(z1, . . . , zm) = 1
2G
m
X
i=1
z2
i .
(A.38)
Importantly, G is not just the inverse of the quadratic coeﬃcient in the action S(z),
but instead is the exact two-point correlator that we would actually measure, (A.35),
incorporating the full series of corrections due to the interactions, cf. (4.104).18 To see
why such a choice satisﬁes the variational principle, let us start by rewriting expectations
with respect to the full distribution (A.30) in terms of simpler Gaussian expectations
taken with respect to a zero-mean Gaussian distribution with the same variance (A.35):
⟨⟨zi1zi2⟩⟩G = δi1i2G .
(A.39)
Here, please recall our notation ⟨⟨·⟩⟩G for a Gaussian expectation of a multi-neuron
function with variance δi1i2G,
⟨⟨f(z1, . . . , zm)⟩⟩G ≡
1
ZG
Z " m
Y
i=1
dzi
#
e−1
2G
Pm
i=1 z2
i f(z1, . . . , zm) ,
(A.40)
and note also that such a Gaussian distribution will require a diﬀerent partition function
ZG ≡
Z " m
Y
i=1
dzi
#
e−1
2G
Pm
i=1 z2
i = (2πG)
m
2 ;
(A.41)
importantly, Z ̸= ZG. Next, let us rewrite the entropy (A.32) in terms of these simpler
expectations as
S[p(z1, . . . , zm|x)] = log Z + E
"
1
2G
m
X
i=1
z2
i
#
+ 1
Z
Z " m
Y
i=1
dzi
#
e−1
2G
Pm
i=1 z2
i e−SISI
=m
2 log(2πG) + log
 Z
ZG

+ m
2 +
 Z
ZG
−1 DD
e−SISI
EE
G ,
(A.42)
where in the ﬁrst equality we just plugged in (A.33), and in the second equality we
rewrote all the full expectations in terms of the simpler Gaussian expectations us-
ing (A.40). Then, by Taylor-expanding in SI, we can evaluate the ratio of partition
functions as
Z
ZG
=
1
ZG
Z " m
Y
i=1
dzi
#
e−1
2G
Pm
i=1 z2
i

e−SI

=
DD
e−SI
EE
G
(A.43)
=1 −⟨⟨SI⟩⟩G + 1
2
DD
S2
I
EE
G −1
6
DD
S3
I
EE
G + O

S4
I

,
17In particular, this procedure will organize our perturbative computation of the entropy and ensure
that we only need to compute Gaussian expectations of the form (A.40).
18This will let us express the entropy in terms of these measurable quantities and in no way will we
need to actually compute any of the corrections in this series.
415

and similarly we can evaluate the needed Gaussian expectation as
DD
e−SISI
EE
G = ⟨⟨SI⟩⟩G −
DD
S2
I
EE
G + 1
2
DD
S3
I
EE
G + O

S4
I

.
(A.44)
Plugging these back into the variational expression for the entropy (A.42) and organizing
terms, for which you might ﬁnd log(1 + x) = x −x2
2 + x3
3 + . . . and 1/(1 + x) = 1 −x +
x2 −x3 + . . . helpful, we get
S[p(z1, . . . , zm|x)] =m
2 log(2πeG) −1
2
hDD
S2
I
EE
G −⟨⟨SI⟩⟩2
G
i
(A.45)
+ 1
3
hDD
S3
I
EE
G −3
DD
S2
I
EE
G ⟨⟨SI⟩⟩G + 2 ⟨⟨SI⟩⟩3
G
i
+ O

S4
I

.
First, note that the ﬁrst term is exactly the entropy for a multivariate Gaussian distri-
bution with a covariance δi1i2G, cf. (A.26). Second, and most importantly, note that the
would-be linear term proportional to ⟨⟨SI⟩⟩G exactly cancelled out. In other words, our
ansatz for the decomposition of the action (A.33) with the choice (A.38) automatically
satisﬁes the variational principle (A.34).
Finally, let us note in passing that the leading correction coming from the quadratic
term is deﬁnitively negative.19 This negativity is actually necessary for mathematical
consistency: as the Gaussian distribution maximizes the entropy of any set of random
variables with known and ﬁxed ﬁrst and second moments, any deformation of a dis-
tribution away from Gaussianity, while also respecting such moment constraints, must
necessarily have less entropy. In the current case, our variational ansatz (A.33) gives a
nearly-Gaussian deformation of a zero-mean Gaussian distribution.20
Now that we’re done passing notes, let’s satisfy our constraints, (A.35) and (A.36),
and then use our variational expression (A.45) to compute the entropy and mutual
information of the preactivations in a ﬁnite-width network.
Leading-Order Correction: Nonzero Mutual Information
At leading order, we’ve already worked out how to relate the couplings of the variational
action SI to the single-input metric G and the single-input four-point correlator V . Recall
from our discussion of the running couplings when integrating out neurons in §4.4 that
the leading correction to the action was given by
SI(z1, . . . , zm) = 1
2

gm −1
G
 m
X
i=1
z2
i −vm
8
m
X
i,j=1
z2
i z2
j + O
 1
n2

,
(A.46)
where the running quadratic coupling was given by (4.102),
gm = 1
G +
m + 2
2
 V
nG3 + O
 1
n2

,
(A.47)
19To see this, notice that the expression in the square brackets of the quadratic term is the variance
of the variational part of the action, and thus is positive: 

S2
I

G −⟨⟨SI⟩⟩2
G = ⟨⟨ SI −⟨⟨SI⟩⟩G
2⟩⟩G ≥0.
20N.B. the terms in the second set of the square brackets in (A.45) are necessary for computing the
next-to-leading-order correction.
416

and the running quartic coupling was given by (4.103),
vm =
V
nG4 + O
 1
n2

.
(A.48)
To get the expression (A.46), look at our expression for the distribution of m preactiva-
tions, (4.97), and then rearrange (A.33) with (A.38) to solve for SI. If you don’t recall
how to get (A.47) and (A.48), feel free to ﬂip back and reread the last subsubsection of
§4.4, or feel free to ﬂip forward to the next subsection where we’ll have to derive these
expressions again to higher order in the 1/n expansion, cf. (A.59) and (A.63).
Given that this leading-order variational correction to the action SI (A.46) now
satisﬁes the constraints (A.35) and (A.36), we can now evaluate the leading correction
to the entropy (A.45):
DD
S2
I
EE
G −⟨⟨SI⟩⟩2
G
(A.49)
=1
4

gm −1
G
2
m
X
i1,i2=1
hDD
z2
i1z2
i2
EE
G −
DD
z2
i1
EE
G
DD
z2
i2
EE
G
i
−1
8

gm −1
G

vm
m
X
i1,i2,i3=1
hDD
z2
i1z2
i2z2
i3
EE
G −
DD
z2
i1z2
i2
EE
G
DD
z2
i3
EE
G
i
+ 1
64v2
m
m
X
i1,i2,i3,i4=1
hDD
z2
i1z2
i2z2
i3z2
i4
EE
G −
DD
z2
i1z2
i2
EE
G
DD
z2
i3z2
i4
EE
G
i
+ O
 1
n3

=m
2

gm −1
G
2
G2−m(m + 2)
2

gm −1
G

vmG3+ m(m + 2)(m + 3)
8
v2
mG4+ O
 1
n3

.
Here, in going from the second line to the last line, you may ﬁnd this formula for these
Gaussian expectations useful:
m
X
i1,...,ik=1
DD
z2
i1 · · · z2
ik
EE
G = m(m + 2) · · · [m + 2(k −1)] Gk ,
(A.50)
which is akin to (3.46) and will be used again in the next subsubsection repeatedly, up
to k = 6. To complete the computation, plug in the quadratic coupling, (A.47), and the
quartic coupling, (A.48), into (A.49), giving
DD
S2
I
EE
G −⟨⟨SI⟩⟩2
G = m(m + 2)
8
 V
nG2
2
+ O
 1
n3

,
(A.51)
for the variance of the variational action. Therefore, the entropy (A.45) is given by
S[p(z1, . . . , zm|x)] = m
2 log(2πeG) −
 m2 + 2m

16
 V
nG2
2
+ O
 1
n3

,
(A.52)
exhibiting a nontrivial correction at ﬁnite width.
417

Let us reﬂect on this formula by making some comments. First, note that the correc-
tion is deﬁnitely negative, as we pointed out before: recall that the Gaussian distribution
maximizes the entropy of a set of random variables with a ﬁxed covariance, and since our
ﬁrst variational constraint (A.35) ﬁxes the two-point correlator of the preactivations, the
entropy for the nearly-Gaussian distribution (A.30) must be less than the entropy of a
Gaussian distribution with the same variance. Second, note that unlike all our previous
results, the leading correction here is second order in the inverse layer width as ∼V 2/n2
and not ∼V/n. Indeed, since the quartic coupling vm in a generic nearly-Gaussian
action can take either sign – corresponding to a distribution with either fat tails or thin
tails – the leading correction to the entropy must be proportional to the minus the square
of the quartic coupling, v2
m ∝V 2/n2, in order to guarantee that the entropy decreases.21
Finally, we see that this correction breaks the perfect additivity for the neurons that
we found in the inﬁnite-width limit (A.26). In particular, although the decrease in the
entropy is perturbatively small with an order 1/n2 scaling, it also depends quadratically
on m. This nonlinearity in the number of neurons signals the presence of nontrivial
interactions at ﬁnite width.
Accordingly, we can characterize this statistical dependence by computing mutual
information between two non-overlapping sets of neurons, M1 = {1, . . . , m1} and M2 =
{m1 + 1, . . . , m1 + m2},
I [p(M1, M2|x)] ≡S[p(M1|x)] + S[p(M2|x)] −S[p(M1, M2|x)]
(A.53)
=m1m2
8
"
V (ℓ)
nℓ−1
 G(ℓ)2
#2
+ O
 1
n3

,
where we used our entropy formula (A.52) in three diﬀerent ways and also restored layer
indices to better interpret this formula. This nonzero mutual information signiﬁes that
– at ﬁnite width, only – for a given layer ℓ, observing the activities of a group of neurons
M1 will convey information about the activities of another group of neurons M2. This
can be thought of as an information-theoretic reformulation and generalization of the
Hebbian learning principle that we saw for the conditional variance (6.78) in §6.4.1: there
we more simply saw that the variance of one neuron z(ℓ)
2 , conditioned on an atypical
observation of a second neuron, z(ℓ)
1
= ˇz(ℓ)
1 , will itself be atypical; here, we can directly
characterize how much one group of neurons can know about another non-overlapping
group.22
Finally, remembering our scaling law for the normalized vertex (5.128), we see that
the mutual information (A.53) scales with the depth ℓof the hidden layer squared:
I [p(M1, M2|x)] ∝ℓ2/n2 .
(A.54)
21This same argument excludes a contribution of the form O(um) = O U/n2
coming from the sextic
coupling, cf. (A.55), and similarly excludes any linear contributions from other higher-order couplings.
22More generally, it would be interesting to work out the mutual information for multiple inputs in
order to understand its data dependence. In that case, we expect it to be mediated by the multi-input
four-point vertex and depend on the details of diﬀerent groupings of four samples from the dataset D.
418

In terms of RG ﬂow, this means that the mutual information is relevant, suggesting
that a growing mutual information is helpful when coarse-graining representations: as
the ﬁne-grained features are marginalized over, deeper hidden layers will have a growing
set of correlations between groups of neurons. In other words, by increasing the size of
the nonlinear subadditive term in the entropy, this reduces the number of independently
available degrees of freedom in these deeper layers.23
NLO Correction: Optimal Aspect Ratio and Tripartite Information
Our leading-order result for the ﬁnite-width mutual information, (A.54), naively grows
in depth without bounds, suggesting that deeper is always better.
Of course, if the
depth becomes too large, this naive answer breaks down as the higher-order terms in the
perturbation series start to become important. To understand the mutual information at
even greater depths, we’ll need to compute the next-to-leading-order (NLO) correction.
To push our calculations to the next level, we need to ensure that our two varia-
tional constraints, (A.35) and (A.36), are satisﬁed to next-to-leading-order, O
 1/n2. In
principle, this means that we will need to also include an O
 1/n2 sextic term in our
variational action SI as
SI(z1, . . . , zm) = 1
2

gm −1
G
 m
X
i=1
z2
i −1
8vm
m
X
i,j=1
z2
i z2
j + 1
24um
m
X
i,j,k=1
z2
i z2
j z2
k + O
 1
n3

.
(A.55)
Such a sextic term was originally introduced in (ε.18); here, we’ve specialized it to focus
on m preactivations in a layer ℓ, with um the running sextic coupling.
Now, let’s explain how to explicitly satisfy the variational constraints. First, just as
we did before for the entropy in (A.45), note that for a general observable we can express
its full expectation in terms of simpler Gaussian expectations as
E [O] ≡1
Z
Z " m
Y
i=1
dzi
#
e−SO =
 Z
ZG
−1 DD
e−SIO
EE
G
(A.56)
= ⟨⟨O⟩⟩G −
h
⟨⟨OSI⟩⟩G −⟨⟨O⟩⟩G ⟨⟨SI⟩⟩G
i
+ 1
2
hDD
OS2
I
EE
G −2 ⟨⟨OSI⟩⟩G ⟨⟨SI⟩⟩G −⟨⟨O⟩⟩G
DD
S2
I
EE
G + 2 ⟨⟨O⟩⟩G ⟨⟨SI⟩⟩2
G
i
+ O
 1
n3

,
where in the last equality, we expanded
DD
e−SIO
EE
G in SI and also used the formula that
we already evaluated in (A.43) for the ratio of partition functions. Physically, the ﬁrst
square brackets says that in an interacting theory, the leading variational correction to
an observable is given by the correlation of the observable with the variational part of
the action.
23It would be interesting to try to interpret this in terms of the optimal brain damage of [82] or the
lottery ticket hypothesis of [83].
419

To satisfy our ﬁrst constraint for the metric, (A.35), we can use this general expression
(A.56) with the observable
O ≡1
m
m
X
i=1
z2
i ,
(A.57)
for which we can use our constraint (A.35) to easily see that E [O] = G. Evaluating this
same expectation using the variational sextic action, (A.55), we ﬁnd
G = 1
m
m
X
i=1
E
h
z2
i
i
(A.58)
=G −

gm −1
G

G2 + (m + 2)
2
vmG3 +

gm −1
G
2
G3 −(m + 2)(m + 4)
4
umG4
−3(m + 2)
2

gm −1
G

vmG4 + (m + 2)(m + 3)
2
v2
mG5 + O
 1
n3

.
In evaluating this, you might again ﬁnd the formula (A.50) helpful. This expression,
(A.58), shows how the interacting theory modiﬁes the two-point correlator. Rearranging
terms to solve for gm order by order, we ﬁnd an NLO version of (A.47), which determines
the variational quadratic coupling in terms of the metric and the other higher-order
couplings:
gm = 1
G + (m + 2)
2
vmG −(m + 2)(m + 4)
4
umG2 + (m + 2)
2
v2
mG3 + O
 1
n3

.
(A.59)
Next, to satisfy the second constraint for the four-point vertex, (A.36), we can use
our general expression (A.56) with the observable
O ≡
1
m(m + 2)
m
X
i,j=1
z2
i z2
j ,
(A.60)
for which we can use both our constraints (A.35) and (A.36) to show that E [O] =
G2 + V/n. Now evaluating O according to the variational sextic action, (A.55), we get
G2 + V
n =
1
m(m + 2)
m
X
i,j=1
E
h
z2
i z2
j
i
(A.61)
=G2 −2

gm −1
G

G3 + 3

gm −1
G
2
G4 + (m + 3)vmG4 −(m + 4)2
2
umG5
−4(m + 3)

gm −1
G

vmG5 + (5m2 + 34m + 60)
4
v2
mG6 + O
 1
n3

=G2 + vmG4 −(m + 4)umG5 + (m + 8)
2
v2
mG6 + O
 1
n3

,
(A.62)
where to get to the second line we again made heavy use of our formula (A.50), and to
get to the ﬁnal line we plugged in (A.59) for the quadratic coupling. Rearranging terms
420

to solve for vm order by order, we ﬁnd an NLO version of (A.48), which determines the
variational quartic coupling in terms of the two constraints and the sextic coupling:
vm =
V
nG4 −(m + 8)
2
 V
nG3
2
+ (m + 4)umG + O
 1
n3

.
(A.63)
We now ﬁnally have all the pieces we need to evaluate the NLO correction to the
entropy (A.45).24 First, let’s reevaluate the variance of the variational action to NLO
by repeated use of the formula (A.50),
DD
S2
I
EE
G −⟨⟨SI⟩⟩2
G
(A.64)
=m
2

gm −1
G
2
G2−m(m + 2)
2

gm −1
G

vmG3+ m(m + 2)(m + 3)
8
v2
mG4
+ m(m + 2)(m + 4)
4

gm −1
G

umG4 −m(m + 2)(m + 4)2
8
vmumG5 + O
 1
n4

=m(m + 2)
8

vmG22 −2(m + 4)

vmG2 
umG3
+ O
 1
n4

=m(m + 2)
8
" V
nG2
2
−(m + 8)
 V
nG2
3#
+ O
 1
n4

.
Here, to get to the penultimate line we used our NLO expression for the quadratic
coupling (A.59), and then to get to the ﬁnal line we used our NLO expression for the
quartic coupling (A.63).
Second, let’s similarly evaluate the subleading term in our
expression (A.45) for the entropy:
DD
S3
I
EE
G −3
DD
S2
I
EE
G ⟨⟨SI⟩⟩G + 2 ⟨⟨SI⟩⟩3
G
(A.65)
=m

gm −1
G
3
G3 −9m(m + 2)
4

gm −1
G
2
vmG4
+ 3m(m + 2)(m + 3)
2

gm −1
G

v2
mG5 −m(m + 2)(5m2 + 34m + 60)
16
v3
mG6 + O
 1
n4

= −m(m + 2)(m + 8)
8

vmG23 + O
 1
n4

= −m(m + 2)(m + 8)
8
 V
nG2
3
+ O
 1
n4

.
Putting (A.64) and (A.65) back into our variational expression for the entropy (A.45),
we ﬁnally arrive at
S[p(z1, . . . , zm|x)]
(A.66)
=m
2 log(2πeG) −
 m2 + 2m

16
 V
nG2
2
+
 m3 + 10m2 + 16m

48
 V
nG2
3
+ O
 1
n4

.
24In principle, at this order we would need to continue and ﬁnd an expression for um in terms of
an additional six-point vertex constraint, but as we will soon see, um doesn’t factor into any of our
expressions for the mutual information. This is the reason why we are able to analyze these next-to-
leading-order corrections without otherwise evaluating additional MLP recursions.
421

This result in turn lets us compute the NLO correction to the mutual information be-
tween two sets of neurons, M1 = {1, . . . , m1} and M2 = {m1 + 1, . . . , m1 + m2}:
I [p(M1, M2|x)]
(A.67)
≡S[p(M1|x)] + S[p(M2|x)] −S[p(M1, M2|x)]
=m1m2
8
"
V (ℓ)
nℓ−1
 G(ℓ)2
#2
−m1m2(20 + 3m1 + 3m2)
48
"
V (ℓ)
nℓ−1
 G(ℓ)2
#3
+ O
 1
n4

,
where once again we have restored layer indices on the metric, the four-point vertex,
and the layer width. Note that as promised, the sextic coupling um dropped out in the
end.25
Excitingly, these two terms have opposite signs. To explain our excitement, let us
plug in our scaling solution for the normalized four-point vertex (5.128) evaluated at the
ﬁnal layer ℓ= L:
V (L)
nL−1
 G(L)2 ≡νr .
(A.68)
Here, r ≡L/n is the overall depth-to-width aspect ratio of the network, and ν is an
activation-function dependent constant: for the K⋆= 0 universality, we have (5.131)
ν = 2
3 ,
(A.69)
independent of the details of the activation function itself; for scale-invariant activation
functions, we have (5.130)
ν =
3A4
A2
2
−1

,
(A.70)
with A2 ≡(a2
+ + a2
−)/2 and A4 ≡(a4
+ + a4
−)/2.26 Plugging this scaling solution (A.68)
into our NLO expression for the mutual informtion (A.67), we get
I [p(M1, M2|x)] =m1m2
8
ν2r2 −m1m2(20 + 3m1 + 3m2)
48
ν3r3 + O

r4
.
(A.71)
Now, let us explain our excitement. At one extreme, in the inﬁnite-width limit r →0,
this mutual information vanishes, as we already knew from (A.27). Thus, for small aspect
ratios r ≪1, the ﬁrst leading-order term dominates and the mutual information increases
as depth increases. As the depth continues to increase, then the second term begins to
dominate, decreasing the mutual information. But we know by the subadditivity of the
25This is another realization of the principle of maximum entropy for nearly-Gaussian distributions.
In particular, the entropy for a zero-mean distribution that satisﬁes constraints ﬁxing its two-point
correlator and its connected four-point correlator – and otherwise leaves unﬁxed its connected six-point
correlator – is given to order 1/n3 by our expression (A.66). Thus, if we did add a third constraint that
ﬁxed the connected six-point correlator, with U = O 1/n2
, then any terms of the form O(vmum) =
O UV/n3
cannot appear in (A.66), as they could increase the entropy depending on the sign of UV .
26This gives ν = 2 for linear activations and ν = 5 for the ReLU.
422

entropy, (A.15), that the mutual information is always bounded from below by zero,
and so for large enough aspect ratios r, this decreasing will be balanced by additional
higher-order corrections. Taken altogether, we expect that at some nonzero but not too
large r, the mutual information will reach a local maximum. Indeed, maximizing (A.71),
we ﬁnd an optimal aspect ratio for the network:
r⋆=

4
20 + 3m1 + 3m2
 1
ν ,
(A.72)
with ν containing all of the details of the activation function.27
Although it’s not immediately obvious, maximizing a mutual information such as
(A.68) is closely related to well-known unsupervised learning objectives.28 In con-
trast to a supervised learning setting where the goal is to predict the true output y ≡f(x)
for any input sample x, in an unsupervised learning setting the goal is to learn repre-
sentations for a collection of input samples by observing patterns in the data.
For
human-generated datasets, this has the advantage of eliminating the tedious task of la-
beling all the samples. It should also be no surprise, given the beneﬁt of representation
learning (§11), that models (pre)trained with unsupervised learning algorithms can often
be eﬃciently ﬁne-tuned on subsequent supervised learning tasks.
In the current context, rather than doing any actual learning, we are understanding,
a priori, which choice of the architecture and activation function can lead to a larger
mutual information between neurons in deeper layers.29 In particular, comparing the
values of ν in (A.69) and (A.70) for diﬀerent choices of activation function, we see
in principle that networks built from tanh activation function should be deeper than
networks built from ReLU activation functions to have the same mutual information in
a layer.
With this objective in mind, we should continue maximizing (A.71) across all the
possible partitions (m1, m2). This picks out a very natural choice of a partition spanning
the whole layer and of equal sizes: m1 = m2 = nL/2. With this further maximization,
we get
r⋆=

4
20 + 3nL
 1
ν ,
(A.73)
27Don’t worry about the higher-order contributions O r4
that we neglected in obtaining the solu-
tion (A.72): for a particular choice of activation function, i.e. for a choice of ν, the estimate of the
optimal value (A.72) is a posteriori justiﬁed so long as the product νr⋆is perturbatively small for a
particular ν and a particular grouping of neurons (m1, m2). FYI, this argument is analogous the one
given to justify the two-loop Banks-Zaks ﬁxed point of the renormalization group in non-Abelian gauge
theory [84].
28In particular, the InfoMax principle [85] recommends maximizing the mutual information between
the input x and a representation z(x). A related notion involves maximizing the mutual information
between diﬀerent representations, z1(x) and z2(x), for the same input x [86]. This latter notion can be
shown to lower bound the InfoMax objective and thus motivates our analysis here.
29Similarly, we may think of our criticality analysis as a type of unsupervised learning or pretraining
criteria in which we are understanding, a priori, which choice of initialization hyperparameters leads to
an ensemble that generalizes most robustly after training, cf. §10.3.
423

for the optimal aspect ratio and
I [p(M1, M2|x)] = 1
6

nL
20 + 3nL
2
,
(A.74)
for the associated value of the maximized mutual information. In particular, this corre-
sponds to a lower bound on the InfoMax objective that we discussed in footnote 28.30
As a ﬁnal comment on (A.73), we can think of this optimal aspect ratio as deﬁning
a scale that separates the eﬀectively-deep regime for which our eﬀective theory is valid
from the overly-deep regime where the theory is strongly-coupled and networks are no
longer trainable. We will push this interpretation further in the next appendix when we
discuss residual networks.
For a ﬁnal computation, let’s look at the tripartite information (A.24) for mutually-
exclusive subsystems M1, M2, and M3 of sizes (m1, m2, m3) neurons, respectively.
Plugging in (A.66) for various diﬀerent combinations of the sizes, we ﬁnd
I3 [p(M1, M2, M3|x)] = m1m2m3
8
"
V (ℓ)
nℓ−1
 G(ℓ)2
#3
+ O
 1
n4

,
(A.75)
which, as per (5.128), scales cubically with depth, I3 [p(M1, M2, M3|x)] ∝ℓ3/n3
ℓ−1, and
thus indicates that a nonzero result only ﬁrst appears at O
 1/n3.31 Importantly, we
see that the tripartite information is exclusively positive, meaning that any three groups
of neurons in a layer will form redundant representations under RG ﬂow: knowing the
activities of any one group of neurons M1 means you would learn less information about
a second group of neurons M2 from observing a third group M3 than you otherwise
would have learned had you not already known M1. It would be interesting to try to
understand further how this property relates to the coarse-graining mechanism of the
representation group ﬂow to the deeper layers.32
30While this value (A.74) may seem small, remember that our analysis is based entirely on the prior
distribution before any learning has taken place.
31In hindsight, it’s obvious that we needed the entropy to have at least a cubic dependence on the
sizes mi to ﬁnd a nonzero answer for the tripartite information, just as we needed the entropy to have
at least a quadratic dependence on the sizes to ﬁnd a nonzero answer for the mutual information (A.53).
32It would also be interesting to try and interpret this in terms of the optimal brain damage of [82] or
the lottery ticket hypothesis of [83].
424

Appendix B
Residual Learning
No Doc, not me, the other me! The one that’s up on stage playing Johnny B. Goode!
Marty McFly [87].
In this ﬁnal appendix, we’ll analyze neural networks with residual connections, generally
called residual networks. These networks were originally introduced in order to enable
the training of deeper and deeper networks: traditionally deep networks suﬀer from the
exploding and vanishing gradient problem, but even in networks where various tricks of
the trade are used to ensure the propagation of forward and backward signals, overly-
deep networks are empirically found to have higher training and test errors than their
shallower-network counterparts.
From the microscopic perspective, the increase of the generalization error is intuitive
for a deeper model with more model parameters, but the increase in the training error
is not: the additional parameters should naively enable better ﬁts to the training data.
At the very least, one might hope that the additional layers could – in principle –
approximate the identity map and do no worse. Yet the empirical evidence mentioned
above suggests that it’s diﬃcult for optimization algorithms to tune the hidden layers
of a deep network to such a nearly-identity map. This is called degradation, and in
principle is a major limiting factor for developing larger scale deep-learning models.
From the macroscopic perspective of our eﬀective theory, we can oﬀer a dual expla-
nation for this degradation problem. As a precursor to our explanation, ﬁrst recall that,
rather than using any heuristic approach to solve the exploding and vanishing gradi-
ent problem, in §9.4 we analytically solved its exponential manifestation by means of
criticality and then solved its polynomial manifestation by means of the learning-rate
equivalence principle. In §10.3 we further conﬁrmed that the associated tunings of the
initialization hyperparameters, C(ℓ)
b
and C(ℓ)
W , and of the training hyperparameters, λ(ℓ)
b
and λ(ℓ)
W , lead to the most robust generalization performance for MLPs.
Now, even if these hyperparameters are properly tuned, we would expect that overly-
deep networks will suﬀer horribly from instantiation-to-instantiation ﬂuctuations, lead-
425

ing to the breakdown of criticality for any particular network. This problem was ﬁrst
discussed in §3.4 for extremely deep (linear) networks, then more generally in §5.4, and
ﬁnally in footnote 8 of Epilogue ε. Thus, combined with our discussion of the MLP’s
depth-to-width ratio r ≡L/n in §A.3, perhaps we can understand the training loss
degradation problem in terms of the network leaving the regime of optimality and pro-
ceeding towards the regime of chaos.
If you’ll please move the microscopic explanation back to the front of your mind, we
can explain an ingenious solution to degradation by He et al. [88]. Rather than trying to
ﬁnd a better learning algorithm, we can instead modify the deep network architecture
so that the hidden layers only have to learn a residual function: in place of a generic
nonlinear (ℓ+ 1)-th layer
z(ℓ+1) = L

z(ℓ); θ(ℓ+1)
,
(B.1)
we design the layer as
z(ℓ+1) = R

z(ℓ); θ(ℓ+1)
+ z(ℓ) ,
(B.2)
such that the residual block, R

z(ℓ); θ(ℓ+1)
, is the residual of the function that we
want our layer, (B.1), to implement. The basic structure of this generic residual layer is
depicted in the left panel of Figure B.1 and will be further explained later on.
From a microscopic perspective, these residual connections make learning a nearly-
identity map much easier. Indeed, it is far easier to set the residual block R

z(ℓ); θ(ℓ+1)
to near-zero than it is to coax a generic nonlinear function L

z(ℓ); θ(ℓ+1)
to approximate
the identity. In particular, since standard building blocks of the neural network often
have the property that they vanish when their parameters are set to zero – cf. (2.5)
for the MLP-layer block – and since typical initialization distributions have zero mean
– cf. (2.21) and (2.22) – residual networks make it fairly easy to ﬁnd a solution with
R

z(ℓ); θ⋆
≈0 such that the addition of the hidden layer doesn’t necessarily degrade
the performance of the network.
More generally, we hope that the preactivation will actually play two roles, one of
coarse-graining the input signal according to representation group ﬂow (§4.6) and the
other of propagating an undegraded copy of the input signal. This is plausibly quite
helpful as it allows us to train deeper models with more parameters, and it has indeed
been empirically demonstrated that such deeper residual networks lead to signiﬁcant
performance gains on the test set.
One of the goals of this appendix is to provide a dual macroscopic explanation for why
the residual connections let us train overly-deep networks. Our macroscopic explanation
above for the origin of the degradation problem – combined with the empirical success of
very deep residual networks – suggests that the inclusion of residual connections shifts
the optimal aspect ratio r⋆from its MLP value, (A.72), to higher values. This would
extend the range of eﬀectively-deep networks and thus explain why residual connections
let us train deeper networks. To test this hypothesis, we will need to carry out our
eﬀective theory analysis for residual networks.
426

Figure B.1:
Left: two residual blocks from adjacent layers for a very general residual
network. This detailed structure depicts how each layer (i) adds a weighted block and
the weighted preactivation to produce the next layer’s preactivation, (ii) copies the
preactivation to skip to the next layer, and (iii) generates the next layer’s block. Right:
two neurons from adjacent layers for a residual MLP. This detailed structure depicts how
each layer (i) adds the bias, the weighted activation, and the weighted preactivation
to produce the next layer’s preactivation, (ii) copies the preactivation to skip to the
next layer, (iii) generates the activation from the preactivation, and (iv) multiplies the
activation by the next-layer weight.
427

Despite the long prelude, this appendix will be relatively brief and only involve
some simple calculations.
These exercises will serve two purposes.
First, given the
overwhelming ubiquity of residual connections – sometimes called skip connections or
shortcuts – in modern deep learning architectures, it is practically useful to explain how
the critical initialization hyperparameters shift when residual connections are included.
Second, this will showcase how our eﬀective theory formalism can easily be applied to
neural network architectures other than vanilla MLPs.
To those ends – and to the end of the book – in §B.1 we’ll begin by brieﬂy introducing
the perhaps simplest residual network, a multilayer perceptron with residual connections.
In §B.2, we’ll study the inﬁnite-width limit of this model, performing a criticality analysis
in order to understand how the residual hyperparameters interplay with the critical
initialization hyperparameters.
Then, in §B.3 we’ll study the residual MLP at ﬁnite width. Using our auxiliary
unsupervised learning objective from the last appendix, we’ll see how the inclusion of
residual connections can shift the optimal aspect ratio of a network r⋆to large values. We
will also learn that for networks with aspect ratios below a certain threshold, residual
connections are always harmful according to this criterion. Altogether, this provides
a new eﬀective-theory macroscopic perspective on how residual connections solve the
degradation problem described above, and further lets us understand the tradeoﬀof
propagating signals through the residual block – in this case, a nonlinear MLP-layer
block – against propagation through identity block – skipping signals to deeper layers.
Finally, in §B.4 we’ll give a hybrid theoretical-empirical recipe applying the analyses
in the previous two sections to general residual networks with arbitrarily complicated
residual blocks R

z(ℓ); θ(ℓ+1)
. We hope this may have broad application to the many
deep learning architectures that implement residual connections.1 After this, you will
have ﬁnished all your residual learning from this book.
B.1
Residual Multilayer Perceptrons
A multilayer perceptron with residual connections can be deﬁned by the forward
equation,
z(ℓ+1)
i;δ
= ξ(ℓ+1)

b(ℓ+1)
i
+
nℓ
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;δ

+ γ(ℓ+1)z(ℓ)
i;δ .
(B.3)
Compared with our schematic description, (B.2), here we’ve picked just a standard
MLP layer as our residual block, R

z(ℓ); θ(ℓ+1)
, and we’ve allowed for scalar residual
1Residual networks were ﬁrst described by He et al. [88]. Typically, when referring to a residual
network as a ResNet, the base block is composed of convolutional layers, cf. (2.8), that are further
augmented with very popular heuristic for mitigating the exploding and vanishing gradient problem
[89].
While original domain of ResNets was computer vision tasks, they have now been applied to
other domains; more broadly, residual connections are components in a wide variety of modern deep
learning architectures, including importantly the transformer-based language models that have been
revolutionizing natural language processing [14].
428

hyperparameters, ξ(ℓ) and γ(ℓ), that control the relative magnitudes of the residual-
block term vs. the identity term. Here we need to take
nℓ= nℓ+1 ≡n
(B.4)
so that we can add the weighted and biased activation back to the preactivation before
the activation, and such residual connections are thus only included for the network’s
hidden layers.2 A graph of two neurons in adjacent layers from a residual MLP is shown
in the right panel of Figure B.1, and if you need to recall the overall global structure of
an MLP, please refer back to Figure 2.1.
Although nice for the symmetry, one of the residual hyperparameters is redundant:
you can show that adjusting ξ(ℓ) has the same eﬀect on the ensemble of residual MLPs
as rescaling the initialization hyperparameters, C(ℓ)
b
and C(ℓ)
W , and the training hyperpa-
rameters, λ(ℓ)
b
and λ(ℓ)
W . As such, we’ll henceforth set
ξ(ℓ) = 1 ,
(B.6)
without loss of generality, leaving us only one (potentially layer-dependent) residual
hyperparameter: γ(ℓ). Note importantly that in the limit γ(ℓ) →0 we recover a vanilla
MLP without any residual connections. A customary choice for this hyperparameter
is γ(ℓ) = 1, cf. (B.2), but let us now show that this is suboptimal because it breaks
criticality.3
B.2
Residual Inﬁnite Width: Criticality Analysis
To understand how to set the residual hyperparameter γ(ℓ) and preserve criticality for
the residual MLP, let’s work out a recursion for the two-point correlator:
E
h
z(ℓ)
i1;δ1z(ℓ)
i2;δ2
i
= δi1i2G(ℓ)
δ1δ2 .
(B.7)
Using the forward equation (B.3), we ﬁnd
G(ℓ+1)
δ1δ2
= Cb + CW
1
n
n
X
j=1
E
h
σ(ℓ)
j;δ1σ(ℓ)
j;δ2
i
+ γ2G(ℓ)
δ1δ2 .
(B.8)
2More generally, if we wanted nℓ+1 ̸= nℓwe could instead use an iteration equation such as
z(ℓ+1)
i;δ
=
nℓ+1
X
j=1
ξ(ℓ+1)
ij
"
b(ℓ+1)
j
+
nℓ
X
k=1
W (ℓ+1)
jk
σ(ℓ)
k;δ
#
+
nℓ
X
j=1
γ(ℓ+1)
ij
z(ℓ)
j;δ ,
(B.5)
where the residual hyperparameters are now matrices: ξ(ℓ+1)
ij
is an nℓ+1-by-nℓ+1 matrix and γ(ℓ+1)
ij
is an
nℓ+1-by-nℓmatrix, and in general both matrix could vary from layer to layer.
3In particular, this choice is problematic without any additional heuristics, e.g. [89], for otherwise mit-
igating the exploding and vanishing gradient problem. The following analysis thus oﬀers an explanation
for why deep γ(ℓ) = 1 residual networks without such heuristics will always perform poorly.
429

Here, mirroring all our previous discussions of criticality that are now too many to
enumerate with references, throughout this section we’ll set the bias variance, C(ℓ)
b , the
rescaled weight variance, C(ℓ)
W , and the residual hyperparameter, γ(ℓ), to be uniform
across layers,
C(ℓ)
b
= Cb ,
C(ℓ)
W = CW ,
γ(ℓ) = γ .
(B.9)
Compared to the metric recursion for a vanilla MLP (4.72),
G(ℓ+1)
δ1δ2
= Cb + CW
1
n
n
X
j=1
E
h
σ(ℓ)
j;δ1σ(ℓ)
j;δ2
i
,
(B.10)
the (ℓ+1)-th-layer metric for the residual MLP is shifted by the (rescaled) identity term
from the forward equation, leading to an additional contribution of the ℓ-th-layer metric
as the (rescaled) ﬁnal term of (B.8).
In particular, the inﬁnite-width limit of the residual MLP leads to the following
recursion for its kernel,
K(ℓ+1)
δ1δ2
= Cb + CW ⟨σδ1σδ2⟩K(ℓ) + γ2K(ℓ)
δ1δ2 .
(B.11)
Then, writing the kernel in our γ[a] basis (5.15), using our δ expansion, (5.22)–(5.24), we
can follow the two-input perturbative analysis of §5.1 to derive component recursions
K(ℓ+1)
00
= Cb + CW g

K(ℓ)
00

+ γ2K(ℓ)
00 ,
(B.12)
δK(ℓ+1)
[1]
= χ∥

K(ℓ)
00

δK(ℓ)
[1] ,
(B.13)
δδK(ℓ+1)
[2]
= χ⊥

K(ℓ)
00

δδK(ℓ)
[2] + h

K(ℓ)
00
 
δK(ℓ)
[1]
2 ,
(B.14)
which are modiﬁed from their vanilla expressions (5.46)–(5.48).4 Here, the parallel sus-
ceptibility,
χ∥(K) ≡γ2 + CW
2K2
D
σ(z) σ(z)

z2 −K
E
K ,
(B.15)
and the perpendicular susceptibility,
χ⊥(K) ≡γ2 + CW

σ′(z) σ′(z)

K ,
(B.16)
are each shifted from their previous values, (5.50) and (5.51), by a constant γ2 term,
while the helper functions, g(K) and h(K), (5.49) and (5.52), are the same as before:
g(K) ≡⟨σ(z) σ(z)⟩K ,
(B.17)
h(K) ≡1
2
d
dK χ⊥(K) .
(B.18)
4Note that, as per this ﬁrst recursion (B.12), a perturbation to a single-input, K(ℓ)
00 = K⋆
00 + ∆K(ℓ)
00 ,
is governed by the shifted parallel susceptibility (B.15), cf. (5.9).
430

As should be already clear from this simple analysis, inclusion of residual connec-
tions (γ ̸= 0) will shift the critical initialization hyperparameters. Fixing the criticality
conditions
χ∥(K⋆) = 1 ,
χ⊥(K⋆) = 1 ,
(B.19)
we can ﬁnd the new critical initializations for our universality classes: speciﬁcally, for
the scale-invariant universality class, the critical initialization hyperparameters, (5.67),
are modiﬁed as
Cb = 0 ,
CW = CW (γ) ≡1
A2

1 −γ2
,
(B.20)
where A2 ≡(a2
+ + a2
−)/2, cf. (5.59); analogously, for the the K⋆= 0 universality class,
the critical initialization hyperparameters (5.90) are modiﬁed as
Cb = 0 ,
CW = CW (γ) ≡1
σ2
1

1 −γ2
,
(B.21)
where σ1 ≡σ′(0).
In §2.3, we discussed that the zero initialization, b(ℓ)
i
= W (ℓ)
ij
= 0, fails to break the
permutation symmetry among the n neurons in a hidden layer. In conjunction with
this reasoning, we now see that the criticality conditions for either class, (B.20) and
(B.21), are unsatisﬁable for the customary choice γ = 1 of the residual hyperparame-
ter. More broadly, for each universality class there is a one-parameter family of critical
rescaled weight variances: for each 0 < γ2 < 1, there is an associated critical value of
CW = CW (γ). Thus, in order to directly mitigate the exploding and vanishing gradient
problem, for a particular choice of 0 < γ2 < 1 and activation function, we must pick CW
according to the appropriate CW (γ).
B.3
Residual Finite Width: Optimal Aspect Ratio
From our inﬁnite-width analysis, we just saw that residual networks have a one-parameter
family of critical solutions: CW (γ). As per the kernel recursion (B.11), these solutions
trade oﬀthe degree to which the identity branch vs. the MLP-layer branch contributes
to the next layer’s preactivations. In the strict inﬁnite-width limit, criticality ensures
that the kernel is preserved for any choice CW (γ) in the range 0 < γ2 < 1, and all
ﬂuctuations are suppressed; thus, we’re in principle completely indiﬀerent between any
of these critical tunings.
However, as should now be a familiar point of discussion, networks in the inﬁnite-
width limit eﬀectively have a depth-to-width ratio r ≡L/n →0.
This means that
inﬁnite-width analysis cannot really get at the question of degradation: why do ex-
tremely deep networks have higher training errors than otherwise equivalent shallower
networks. To compare wide networks of diﬀerent depths, we need to consider ﬁnite-width
networks with diﬀerent aspect ratios r > 0.5
5In particular, at ﬁnite width the representation group ﬂow through the MLP-layer blocks leads to
two competing ﬁnite-width eﬀects: (i) the relevant and thus growing dNTK and ddNTKs lead to non-
431

Our main tool of analysis in this section will be a computation of the mutual in-
formation at initialization between non-overlapping representations in deep layers of a
residual MLP. As per our auxiliary unsupervised learning criterion from the last ap-
pendix, cf. footnote 28 of §A.3, this mutual information gives a natural way to estimate
the optimal aspect ratio r⋆of a ﬁnite-width network.
Given that residual networks
without an exploding and vanishing gradient problem solve the degradation problem,
a natural theoretical prediction for our residual MLPs is that the inclusion of residual
connections, γ > 0, and then tuning to criticality as CW (γ) will together shift their
optimal aspect ratios to larger values.
To evaluate the optimal aspect ratio via our formula (A.73), we just need to com-
pute the coeﬃcient ν of normalized, output-layer, single-input four-point vertex for the
residual MLP:
V (L)
n
 G(L)2 ≡νr .
(B.22)
Note that we’ve already derived the recursion for the leading-order single-input metric,
i.e. the kernel K(L) ≡G{0}(L), in (B.12). To compute the four-point vertex, ﬁrst recall
that the multi-input four-point vertex deﬁnes the four-point connected correlator (4.77)
as
E
h
z(ℓ)
i1;δ1z(ℓ)
i2;δ2z(ℓ)
i3;δ3z(ℓ)
i4;δ4
i 
connected
(B.23)
= 1
n
h
δi1i2δi3i4V (ℓ)
(δ1δ2)(δ3δ4) + δi1i3δi2i4V (ℓ)
(δ1δ3)(δ2δ4) + δi1i4δi2i3V (ℓ)
(δ1δ4)(δ2δ3)
i
.
Using the forward equation (B.3) and following our analysis from §4.3, we see that the
recursion for the residual-MLP’s four-point vertex is shifted by two additional terms
proportional to γ2 and γ4 as compared to the leading-order recursion for the vanilla
trivial representation learning during training, and (ii) the relevant and growing four-point vertex, NTK
variance, and NTK-preactivation cross correlation lead to ﬂuctuations in the ensemble from instantia-
tion to instantiation. As the residual connections are supposed to mitigate this second harmful eﬀect by
xeroxing the undegraded input via the identity branch, we naturally expect that they will have a mean-
ingful physical eﬀect on this competition. In other words, we expect that residual networks of diﬀerent
residual hyperparameters γ and diﬀerent aspect ratios r will lead to very diﬀerent test-set generalization.
432

MLP (4.90),
V (ℓ+1)
(δ1δ2)(δ3δ4)
(B.24)
=C2
W
⟨σδ1σδ2σδ3σδ4⟩K(ℓ) −⟨σδ1σδ2⟩K(ℓ) ⟨σδ3σδ4⟩K(ℓ)

+ C2
W
4
X
δ5,...,δ8∈D
V (δ5δ6)(δ7δ8)
(ℓ)
⟨σδ1σδ2 (zδ5zδ6 −Kδ5δ6)⟩K(ℓ) ⟨σδ3σδ4 (zδ7zδ8 −Kδ7δ8)⟩K(ℓ)
+ CW γ2h
⟨σδ1σδ2 (zδ3zδ4 −Kδ3δ4)⟩K(ℓ) + ⟨σδ3σδ4 (zδ1zδ2 −Kδ1δ2)⟩K(ℓ)
+ 1
2
X
δ5,...,δ8∈D
V (δ5δ6)(δ7δ8)
(ℓ)
⟨σδ1σδ2 (zδ5zδ6 −Kδ5δ6)⟩K(ℓ) K(ℓ)
δ7δ3K(ℓ)
δ8δ4
+ 1
2
X
δ5,...,δ8∈D
V (δ5δ6)(δ7δ8)
(ℓ)
⟨σδ3σδ4 (zδ5zδ6 −Kδ5δ6)⟩K(ℓ) K(ℓ)
δ7δ1K(ℓ)
δ8δ2
i
+ γ4 V (ℓ)
(δ1δ2)(δ3δ4) ,
(B.25)
where in parsing this expression, please remember that the raised indices of the four-point
vertex are our shorthand for contraction with the ℓ-th-layer inverse kernel, cf. (4.83).
Here, in working out the middle terms proportional to γ2, you might ﬁnd the intralayer
formula (4.64) to be useful.
Specializing now to a single input, we get
V (ℓ+1) =C2
W
D
σ4(z)
E
K(ℓ) −
D
σ2(z)
E2
K(ℓ)

+

χ(ℓ)
∥
2 V (ℓ) + 4γ2 
χ(ℓ)
∥
−γ2 
K(ℓ)2 ,
(B.26)
where for convenience we have abbreviated χ(ℓ)
∥
≡χ∥

K(ℓ)
as we often do.
Here,
this parallel susceptibility is the shifted one appropriate for residual MLPs as deﬁned
in (B.15) with the γ2 term; otherwise, we notice the addition of the γ-dependent ﬁnal
term compared with the single-input recursion for vanilla MLPs (5.109). At criticality
with χ(ℓ)
∥
= 1, this ﬁnal term will make a nontrivial diﬀerence on the deep asymptotic
behavior of the four-point vertex. Now, let’s solve this recursion at criticality for our
two universality classes, with the usual initial condition V (1) = 0 (cf. the title of §4.1).
Scale-Invariant Universality Class
For the scale-invariant universality class, we tune the rescaled weight variance to crit-
icality with (B.20) so that χ(ℓ)
∥
= 1.
Then, the single-input kernel is exactly ﬁxed,
K(ℓ) = K⋆, and we also need to remember (5.113),
D
σ4E
K⋆−
D
σ2E2
K⋆

= (3A4 −A2
2) (K⋆)2 ,
(B.27)
with A2 ≡(a2
+ + a2
−)/2 and A4 ≡(a4
+ + a4
−)/2. Substituting all of this into (B.26), the
recursion becomes
V (ℓ+1) = V (ℓ) + (1 −γ2)

(1 −γ2)
3A4
A2
2
−1

+ 4γ2

(K⋆)2 ,
(B.28)
433

which gives a simple additive solution of the form (B.22), with
ν = ν(γ) ≡(1 −γ2)

(1 −γ2)
3A4
A2
2
−1

+ 4γ2

.
(B.29)
As a quick check, we see that ν reduces to our previous result for vanilla MLPs, (A.70),
as γ →0. More importantly, ν(γ) is strictly positive in the allowed range 0 < γ2 < 1
and monotonically decreases to ν(1) = 0 with increasing γ.6
K⋆= 0 Universality Class
For the K⋆= 0 universality class, we tune the rescaled weight variance to criticality
with (B.21). In this case, the single-input asymptotic behavior of the kernel is modi-
ﬁed. Evaluating the residual-MLP kernel recursion (B.12) at criticality and recalling our
expansion (5.83),
g(K) = σ2
1
h
K + a1K2 + O

K3i
,
(B.30)
the single-input kernel recursion becomes
K(ℓ+1) = K(ℓ) + a1(1 −γ2)

K(ℓ)2 + . . . ,
(B.31)
which has deep asymptotic behavior of
K(ℓ) =

1
(−a1)(1 −γ2)
 1
ℓ+ . . . .
(B.32)
To evaluate the four-point vertex recursion, we need
χ∥(K) = 1 + (1 −γ2)
h
2a1K + O

K2i
,
(B.33)
D
σ4E
K⋆−
D
σ2E2
K⋆

= σ4
1
h
2K2 + O

K3i
,
(B.34)
where the ﬁrst expression is shifted from (5.121), as per (B.15) and (B.21), but the second
expression is the same as before (5.122). Plugging in these expressions and matching
terms, we ﬁnd
ν = ν(γ) ≡2
3(1 −γ4) .
(B.35)
This also reduces to our previous result for vanilla K⋆= 0 MLPs, (A.69), as γ →0,
and also is strictly positive in the allowed range 0 < γ2 < 1, monotonically decreasing
to ν(1) = 0 with increasing γ. Thus, ν(γ) for K⋆= 0 universality class is qualitatively
comparable to the scale-invariant universality class.
6To see this, you’ll need to use the fact that A4 ≥A2
2 for all scale-invariant activation functions.
434

Physics of the Optimal Aspect Ratio
As we saw at the end of §A.3, the maximization of the 1/n3 mutual information (A.67)
according to our unsupervised learning criterion led to a natural estimate of the optimal
aspect ratio of a network (A.73). For residual MLPs, this gives
r⋆(γ) ≡

4
20 + 3nL

1
ν(γ) ,
(B.36)
with ν(γ) given by (B.29) for the scale-invariant universality class and (B.35) for the
K⋆= 0 universality class. Thus, we see that the monotonic decrease of ν(γ) for either
class leads to a monotonic increase of r⋆(γ) as γ increases: beginning from vanilla MLPs
at γ = 0, residual MLPs will prefer larger and larger aspect ratios.
Thus, we have
realized our theoretical prediction at the beginning of the section, ﬁnding support in
our eﬀective theory formalism for the hypothesis that residual connections can solve the
degradation problem.
Let us oﬀer a further interpretation of this mechanism. In [90], it was suggested
that the beneﬁt of residual connections in extremely deep networks is that they let the
global network architecture behave as an ensemble of many shallower networks: in this
interpretation, each particular “shallow network” is given by following the path of a
signal from the input to the output of the single residual network. What was discovered
is that gradients are dominated by paths skipping over most of the network and only
passing through a small fraction of the hidden-layer residual blocks. This actually gives
an interesting way to look at our result (B.36): if our maximization of mutual information
(A.67) is weighing the helpful eﬀect of depth as an inductive bias for neural association
(§6.4.1) against the harmful eﬀect of depth due to growing ﬂuctuations (§5.4), then such
ensembling would be a natural way to suppress the ﬂuctuations while preserving the
neural association. Then, using residual connections to extend the depth of trainable
networks should, at least to a point, also lead to better test set performance.
Note ﬁnally that we can actually interpret (B.36) in another way: rather than es-
timating the optimal aspect ratio r⋆(γ) for a ﬁxed residual hyperparameter γ, instead
we can think of it as estimating the optimal residual hyperparameter γ⋆(r) for a ﬁxed
aspect aspect ratio r. Taking this latter perspective, we learn something interesting. On
the one hand, for very shallow network with r ≪1, our criterion (B.36) is unsatisﬁable
since ν(γ) monotonically decreases from its maximal value at γ = 0. Such networks
are shallower than optimal according to our original criterion (A.73) for vanilla MLPs,
r < r⋆(γ = 0), and their mutual information will be greatest without the residual con-
nections γ⋆(r) = 0. On the other hand, for very deep networks with r > r⋆(γ = 0),
the optimal residual hyperparameter γ⋆(r) monotonically asymptotes to 1 with growing
r.7 Altogether, this suggests that we should only turn on the residual connections for
7When setting γ⋆it’s important to always remember to also adjust the rescaled weight variance
CW (γ⋆) according to (B.20) or (B.21) in order to maintain criticality. In the limit of r ≫r⋆(γ = 0), the
optimal residual hyperparameter asymptotes to one as 1 −[γ⋆(r)]2 ∼1/r and the critical initialization
gets smaller as CW
 γ = γ⋆(r)
∼1/r.
435

networks with aspect ratios r that are greater than the threshold r⋆(γ = 0) set by the
optimal aspect ratio of vanilla MLPs.
Incidentally, if you are worried about the validity of perturbation theory for large r,
note that the real physical expansion parameter is given by the combination
V (L)
n
 G(L)2 = ν

γ = γ⋆(r)

× r ,
(B.37)
which stays ﬁnite even as the ratio r asymptotes to inﬁnity. In this sense, we can use
γ⋆(r) to arbitrarily extend the regime of eﬀectively-deep networks that can be described
by our eﬀective theory.
B.4
Residual Building Blocks
In this ﬁnal section, we will explain a hybrid theoretical-empirical method for tuning a
very general residual network to criticality. This method may be implemented practi-
cally in order to tune the hyperparameters of residual networks beyond the multilayer
perceptron architecture. We hope this discussion provides a blueprint for how a few
simple measurements on small networks can then be scaled up to eﬃciently design much
larger models according to our eﬀective theory approach.
A general residual network can be deﬁned by replacing a simple MLP-layer block (B.3)
by a generic nonlinear residual block:
b(ℓ+1)
i
+
n
X
j=1
W (ℓ+1)
ij
σ(ℓ)
j;δ →Ri

z(ℓ)
δ ; θ(ℓ+1)
≡R(ℓ+1)
i;δ
.
(B.38)
Here, the ℓ-th-layer residual block R(ℓ)
i;δ is shaped by model parameters θ(ℓ), and we should
pick R(ℓ)
i;δ to be a square matrix in its neural indices, nℓ= nℓ+1 ≡n, so that we can add
the output of the residual block back to the preactivation. This leads to the following
forward equation,
z(ℓ+1)
i;δ
= ξ(ℓ+1)Ri

z(ℓ)
δ ; θ(ℓ+1)
+ γ(ℓ+1)z(ℓ)
i;δ ,
(B.39)
where we’ve restored the second residual hyperparameter, ξ(ℓ), in order to let us scale
the overall magnitude of the residual-block term.
This iteration equation (B.39) is
suﬃciently generic to schematically capture many popular deep learning architectures,
including the computer vision workhorse architecture, the residual convolutional net-
work or ResNet, and the multi-headed self-attention-seeking language-modeling trans-
former. A graph of two residual blocks in adjacent layers from a general residual network
described by (B.39) is shown in the left panel of Figure B.1.
In practice, certain heuristics and ad hoc methods are used in these general archi-
tectures to try and mitigate the exploding and vanishing gradient problem. However, as
we know from §9.4, criticality is a much more motivated solution. In §B.2, we saw that
436

for residual MLPs, the residual hyperparameters can be used to control criticality for
the network. Now, let’s see how we can implement a form of criticality for our general
residual network (B.39).
Broadly speaking, we now need to solve two problems of diﬀerent nature and diﬃ-
culties:
• First, we need to ensure that signals can easily propagate through the residual
block, especially for blocks that are deep in some sense; for instance, if a block
individually consists of LR MLP layers, i.e. an MLP-LR-layer block, it will then
have an internal version of the exploding and vanishing gradient problem. Theorists
should make every eﬀort to critically analyze such blocks of practical interest, but
if we have to treat the residual block R(ℓ)
i;δ as a black box for one reason or another,
then this will require some engineering: you need to measure the two-point block-
block correlator at the output of a block
E
h
R(ℓ+1)
i1;δ1 R(ℓ+1)
i2;δ2
i
,
(B.40)
and then compare it with the two-point correlator of the input to the block
E
h
z(ℓ)
i1;δ1z(ℓ)
i2;δ2
i
.
(B.41)
To be brief and concrete here, we’ll focus on their diagonal components with i1 =
i2 ≡i and δ1 = δ2 ≡δ. In particular, let us take an average over neural indices
i = 1, . . . , n as well as over the sample indices δ ∈D, so that we can compare two
scalar quantities
G(ℓ+1)
RR
≡
1
|D|n
n
X
i=1
X
δ∈D
E
h
R(ℓ+1)
i;δ
R(ℓ+1)
i;δ
i
,
G(ℓ)
zz ≡
1
|D|n
n
X
i=1
X
δ∈D
E
h
z(ℓ)
i;δ z(ℓ)
i;δ
i
(B.42)
rather than two matrices, (B.40) and (B.41).8
After setting up these measure-
ments, we need to adjust the initialization hyperparameters for the block such that
these quantities have similar magnitudes. For instance, if internally the block is
parameterized by many iterative layers – like our MLP-LR-layer block – then we
need to make sure that the diﬀerence in magnitudes is no worse than a polynomial
in this depth hyperparameter LR; importantly, we want to ensure that there’s no
exponential growth or decay in G(ℓ+1)
RR
/G(ℓ)
zz .9 Note that while you’re setting up
measurements, it will also be helpful to measure the cross correlator
G(ℓ+0.5)
Rz
≡
1
|D|n
n
X
i=1
X
δ∈D
E
h
R(ℓ+1)
i;δ
z(ℓ)
i;δ
i
,
(B.43)
8More generally, we should also account for oﬀ-diagonal components by averaging over pairs of inputs
to estimate the appropriate analogs of K(ℓ)
[2] , cf. (5.19). We’d then also want to ensure that the analog of
the recursion for this component, e.g. (B.14), is preserved. As our discussion in these bullets is somewhat
schematic, we will leave these details to the PyTorch documentation.
9One reasonable heuristic solution to this problem, used by the transformer architecture, could be
layer normalization [91]. However, more ideally, the block is not treated as a black box, and instead we
use something about the structure of the block itself to ﬁnd the criticality conditions.
437

which will be needed in the next bullet point.
• Now, using the forward equation (B.39), we can write a recursion for the diagonal
component of the two-point correlator as
G(ℓ+1)
zz
= γ2 G(ℓ)
zz + 2γξ G(ℓ+0.5)
Rz
+ ξ2 G(ℓ+1)
RR
,
(B.44)
where we’ve suppressed the layer indices in the residual hyperparameters to de-
clutter the expression.10 To preserve this component of the two-point correlator
of preactivations, we set the right-hand side of this equation equal to ℓ-th-layer
correlator. Rearranging, we thus want

1 −γ2
G(ℓ)
zz = ξ2 G(ℓ+1)
RR
+ 2γξ G(ℓ+0.5)
Rz
.
(B.45)
Since we’ve supposedly measured all these quantities, this equation should give
simple analytical solutions for the residual hyperparameters.11
Overall, this hybrid approach realizes one of our goals of using experimentally mea-
surable observables as input to an eﬀective theory analysis. We hope that similar ways
of thinking will lead to powerful ways of designing and tuning deep-learning models in
the future.
10Note that unlike the MLP-single-layer block we discussed in §B.2, we cannot in general simply scale
away ξ by a rescaling of CW : for instance, if we have a deep MLP-LR-layer block with LR ≫1 built with
ReLU activations, then it is probably easier to set CW = 2 and adjust ξ at the end. Thus, in general we
should think of the initialization hyperparameters of the block as being ﬁxed ﬁrst – to ensure criticality
of the block internally – and then for each γ, we tune ξ(γ) according to (B.45) to ensure criticality of
the whole network.
11Accordingly, each critical solution (γ, ξ) to the equation (B.45) will then yield architectures with
diﬀerent optimal aspect ratios r⋆(γ, ξ). The optimal aspect ratio for a particular (γ, ξ) can be analogously
estimated for general residual networks with a hybrid approach by combining a theoretical analysis as
we did in §B.3 with measurements of an appropriate combination of four-point connected correlators.
438

References
[1] P. A. M. Dirac, The Principles of Quantum Mechanics. No. 27 in The
International Series of Monographs on Physics. Oxford University Press, 1930.
[2] J. von Neumann, Mathematical Foundations of Quantum Mechanics. Princeton
University Press, 1955.
[3] S. Carnot, Reﬂections on the Motive Power of Heat and on Machines Fitted to
Develop that Power. J. Wiley, 1890. Trans. by R. H. Thurston from R´eﬂexions sur
la puissance motrice du feu et sur les machines propres `a d´evelopper cette
puissance (1824).
[4] M. Bessarab, Landau. M., Moscow worker, 1971. Trans. by B. Hanin from the
original Russian source. http://www.ega-math.narod.ru/Landau/Dau1971.htm.
[5] J. Polchinski, “Memories of a Theoretical Physicist,” arXiv:1708.09093
[physics.hist-ph].
[6] F. Rosenblatt, “Principles of Neurodynamics: Perceptrons and the Theory of
Brain Mechanism,” tech. rep., Cornell Aeronautical Lab, Inc., 1961.
[7] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in
nervous activity,” The bulletin of mathematical biophysics 5 no. 4, (1943) 115–133.
[8] F. Rosenblatt, “The perceptron: a probabilistic model for information storage and
organization in the brain.,” Psychological review 65 no. 6, (1958) 386.
[9] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “ImageNet Classiﬁcation with
Deep Convolutional Neural Networks,” in Advances in Neural Information
Processing Systems, vol. 25, pp. 1097–1105. 2012.
[10] K. Fukushima, “Neocognitron: a self organizing neural network model for a
mechanism of pattern recognition unaﬀected by shift in position,” Biological
Cybernetics 36 no. 4, (1980) 193–202.
[11] Y. LeCun, “Generalization and Network Design Strategies,” No. CRG-TR-89-4.
1989.
439

[12] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and
L. D. Jackel, “Backpropagation Applied to Handwritten Zip Code Recognition,”
Neural Computation 1 no. 4, (1989) 541–551.
[13] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner, “Gradient-based learning applied
to document recognition,” Proceedings of the IEEE 86 no. 11, (1998) 2278–2324.
[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u.
Kaiser, and I. Polosukhin, “Attention is All you Need,” in Advances in Neural
Information Processing Systems, vol. 30, pp. 5998–6008. 2017. arXiv:1706.03762
[cs.CL].
[15] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning Internal
Representations by Error Propagation,” in Parallel Distributed Processing:
Explorations in the Microstructure of Cognition. Volume 1: Foundations, D. E.
Rumelhart, J. L. McClelland, and the PDP Research Group, eds., ch. 8,
pp. 318–362. MIT Press, Cambridge, MA, 1986.
[16] Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller, “Eﬃcient backprop,” in
Neural Networks: tricks of the trade, pp. 9–48. Springer, 1998.
[17] Gallant and White, “There exists a neural network that does not make avoidable
mistakes,” in IEEE 1988 International Conference on Neural Networks,
pp. 657–664 vol.1. 1988.
[18] V. Nair and G. E. Hinton, “Rectiﬁed Linear Units Improve Restricted Boltzmann
Machines,” in International Conference on Machine Learning. 2010.
[19] X. Glorot, A. Bordes, and Y. Bengio, “Deep Sparse Rectiﬁer Neural Networks,” in
Proceedings of the Fourteenth International Conference on Artiﬁcial Intelligence
and Statistics, pp. 315–323, JMLR Workshop and Conference Proceedings. 2011.
[20] A. L. Maas, A. Y. Hannun, and A. Y. Ng, “Rectiﬁer nonlinearities improve neural
network acoustic models,” in ICML Workshop on Deep Learning for Audio,
Speech, and Language Processing. 2013.
[21] C. Dugas, Y. Bengio, F. B´elisle, C. Nadeau, and R. Garcia, “Incorporating
Second-Order Functional Knowledge for Better Option Pricing,” in Advances in
Neural Information Processing Systems, vol. 13, pp. 472–478. 2000.
[22] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for Activation Functions,”
arXiv:1710.05941 [cs.NE].
[23] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units (GELUs),”
arXiv:1606.08415 [cs.LG].
[24] A. M. Turing, “The chemical basis of morphogenesis,” Philosophical Transactions
of the Royal Society of London. Series B, Biological Sciences 237 no. 641, (1952)
37–72.
440

[25] A. M. Saxe, J. L. McClelland, and S. Ganguli, “Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks,” arXiv:1312.6120 [cs.NE].
[26] J. A. Zavatone-Veth and C. Pehlevan, “Exact priors of ﬁnite neural networks,”
arXiv:2104.11734 [cs.LG].
[27] J. McGreevy, “Holographic duality with a view toward many-body physics,” Adv.
High Energy Phys. 2010 (2010) 723105, arXiv:0909.0518 [hep-th].
[28] R. M. Neal, “Priors for inﬁnite networks,” in Bayesian Learning for Neural
Networks, pp. 29–53. Springer, 1996.
[29] J. Lee, Y. Bahri, R. Novak, S. S. Schoenholz, J. Pennington, and
J. Sohl-Dickstein, “Deep Neural Networks as Gaussian Processes,” in International
Conference on Learning Representations. 2018. arXiv:1711.00165 [stat.ML].
[30] A. G. de G. Matthews, M. Rowland, J. Hron, R. E. Turner, and Z. Ghahramani,
“Gaussian Process Behaviour in Wide Deep Neural Networks,” in International
Conference on Learning Representations. 2018. arXiv:1804.11271 [stat.ML].
[31] S. Yaida, “Non-Gaussian Processes and Neural Networks at Finite Widths,” in
Mathematical and Scientiﬁc Machine Learning Conference. 2020.
arXiv:1910.00019 [stat.ML].
[32] F. J. Dyson, “The S Matrix in Quantum Electrodynamics,” Phys. Rev. 75 (Jun,
1949) 1736–1755.
[33] J. Schwinger, “On the Green’s functions of quantized ﬁelds. I,” Proceedings of the
National Academy of Sciences 37 no. 7, (1951) 452–455.
[34] M. D. Zeiler and R. Fergus, “Visualizing and Understanding Convolutional
Networks,” in Computer Vision – ECCV 2014, pp. 818–833. 2014.
[35] A. Rahimi and B. Recht, “Random Features for Large-Scale Kernel Machines,” in
Advances in Neural Information Processing Systems, vol. 20, pp. 1177–1184. 2008.
[36] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding,” arXiv:1810.04805
[cs.CL].
[37] M. Gell-Mann and F. E. Low, “Quantum Electrodynamics at Small Distances,”
Phys. Rev. 95 (Sep, 1954) 1300–1312.
[38] K. G. Wilson, “Renormalization Group and Critical Phenomena. I.
Renormalization Group and the KadanoﬀScaling Picture,” Phys. Rev. B 4 (Nov,
1971) 3174–3183.
[39] K. G. Wilson, “Renormalization Group and Critical Phenomena. II. Phase-Space
Cell Analysis of Critical Behavior,” Phys. Rev. B 4 (Nov, 1971) 3184–3205.
441

[40] E. C. G. Stueckelberg de Breidenbach and A. Petermann, “Normalization of
constants in the quanta theory,” Helv. Phys. Acta 26 (1953) 499–520.
[41] N. Goldenfeld, Lectures on phase transitions and the renormalization group. CRC
Press, 2018.
[42] J. Cardy, Scaling and Renormalization in Statistical Physics. Cambridge Lecture
Notes in Physics. Cambridge University Press, 1996.
[43] M. Minsky and S. A. Papert, Perceptrons: An Introduction to Computational
Geometry. MIT Press, 1988.
[44] S. Coleman, Aspects of Symmetry: Selected Erice Lectures. Cambridge University
Press, Cambridge, U.K., 1985.
[45] B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli, “Exponential
Expressivity in Deep Neural Networks Through Transient Chaos,” in Advances in
Neural Information Processing Systems, vol. 29, pp. 3360–3368. 2016.
arXiv:1606.05340 [stat.ML].
[46] M. Raghu, B. Poole, J. Kleinberg, S. Ganguli, and J. Sohl-Dickstein, “On the
Expressive Power of Deep Neural Networks,” in International Conference on
Machine Learning, pp. 2847–2854. 2017. arXiv:1606.05336 [stat.ML].
[47] S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein, “Deep Information
Propagation,” in 5th International Conference on Learning Representations. 2017.
arXiv:1611.01232 [stat.ML].
[48] K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectiﬁers: Surpassing
Human-Level Performance on ImageNet Classiﬁcation,” in Proceedings of the
IEEE international conference on computer vision, pp. 1026–1034. 2015.
arXiv:1502.01852 [cs.CV].
[49] L. Kadanoﬀ, “Critical Behavior. Universality and Scaling,” in Proceedings of the
International School of Physics Enrico Fermi, Course LI (27 July – 8 August
1970). 1971.
[50] E. T. Jaynes, Probability Theory: The Logic of Science. Cambridge University
Press, 2003.
[51] L. Froidmont, On Christian Philosophy of the Soul. 1649.
[52] D. J. MacKay, “Probable networks and plausible predictions—a review of
practical Bayesian methods for supervised neural networks,” Network:
computation in neural systems 6 no. 3, (1995) 469–505.
[53] C. K. I. Williams, “Computing with Inﬁnite Networks,” in Advances in Neural
Information Processing Systems, vol. 9, p. 295–301. 1996.
442

[54] G. Hinton, O. Vinyals, and J. Dean, “Distilling the Knowledge in a Neural
Network,” in NIPS Deep Learning and Representation Learning Workshop. 2015.
arXiv:1503.02531 [stat.ML].
[55] D. Hebb, The Organization of Behavior: A Neuropsychological Theory. Taylor &
Francis, 2005.
[56] S. Coleman, “Sidney Coleman’s Dirac Lecture ‘Quantum Mechanics in Your
Face’,” arXiv:2011.12671 [physics.hist-ph].
[57] A. Jacot, F. Gabriel, and C. Hongler, “Neural tangent kernel: Convergence and
generalization in neural networks,” in Advances in Neural Information Processing
Systems, vol. 31, pp. 8571–8580. 2018. arXiv:1806.07572 [cs.LG].
[58] S. Hochreiter, Untersuchungen zu dynamischen neuronalen Netzen. Diploma,
Technische Universit¨at M¨unchen, 1991.
[59] Y. Bengio, P. Frasconi, and P. Simard, “The problem of learning long-term
dependencies in recurrent networks,” in IEEE International Conference on Neural
Networks, vol. 3, pp. 1183–1188, IEEE. 1993.
[60] R. Pascanu, T. Mikolov, and Y. Bengio, “On the diﬃculty of training Recurrent
Neural Networks,” in International Conference on Machine Learning,
pp. 1310–1318, PMLR. 2013. arXiv:1211.5063 [cs.LG].
[61] M. Kline, Mathematical Thought From Ancient to Modern Times: Volume 3.
Oxford University Press, 1990.
[62] J. Lee, L. Xiao, S. Schoenholz, Y. Bahri, R. Novak, J. Sohl-Dickstein, and
J. Pennington, “Wide Neural Networks of Any Depth Evolve as Linear Models
Under Gradient Descent,” in Advances in Neural Information Processing Systems,
vol. 32, pp. 8572–8583. 2019. arXiv:1902.06720 [stat.ML].
[63] E. Fix and J. Hodges, “Discriminatory Analysis. Nonparametric Discrimination:
Consistency Properties,” USAF School of Aviation Medicine, Project Number:
21-49-004, Report Number: 4 (1951) .
[64] T. Cover and P. Hart, “Nearest neighbor pattern classiﬁcation,” IEEE Trans. Inf.
Theory 13 (1967) 21–27.
[65] A. Einstein, “On the Method of Theoretical Physics,” Philosophy of Science 1
no. 2, (1934) 163–169.
[66] B. Hanin and M. Nica, “Finite Depth and Width Corrections to the Neural
Tangent Kernel,” in International Conference on Learning Representations. 2020.
arXiv:1909.05989 [cs.LG].
443

[67] E. Dyer and G. Gur-Ari, “Asymptotics of Wide Networks from Feynman
Diagrams,” in International Conference on Learning Representations. 2020.
arXiv:1909.11304 [cs.LG].
[68] G. F. Giudice, “Naturally Speaking: The Naturalness Criterion and Physics at the
LHC,” arXiv:0801.2562 [hep-ph].
[69] F. J. Dyson, “Forward,” in Classic Feynman: All the Adventures of a Curious
Character, R. Leighton, ed., pp. 5–9. W. W. Norton & Company Ltd., 2006.
[70] L. Chizat, E. Oyallon, and F. Bach, “On Lazy Training in Diﬀerentiable
Programming,” in Advances in Neural Information Processing Systems, vol. 32.
2019. arXiv:1812.07956 [math.OC].
[71] D. J. MacKay, Information Theory, Inference and Learning Algorithms.
Cambridge University Press, 2003.
[72] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child,
S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language
models,” arXiv:2001.08361 [cs.LG].
[73] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals, “Understanding deep
learning requires rethinking generalization,” arXiv:1611.03530 [cs.LG].
[74] D. H. Wolpert, “The lack of a priori distinctions between learning algorithms,”
Neural computation 8 no. 7, (1996) 1341–1390.
[75] D. H. Wolpert and W. G. Macready, “No free lunch theorems for optimization,”
IEEE transactions on evolutionary computation 1 no. 1, (1997) 67–82.
[76] L. Boltzmann, “On Certain Questions of the Theory of Gases,” Nature 51
no. 1322, (1895) 413–415.
[77] L. Boltzmann, Lectures on Gas Theory. Berkeley, University of California Press,
1964. Trans. by S. G. Brush from Vorlesungen ueber Gastheorie (2 vols., 1896 &
1898).
[78] C. E. Shannon, “A mathematical theory of communication,” The Bell System
Technical Journal 27 no. 3, (1948) 379–423.
[79] C. E. Shannon, “A mathematical theory of communication,” The Bell System
Technical Journal 27 no. 4, (1948) 623–656.
[80] E. T. Jaynes, “Information Theory and Statistical Mechanics,” Phys. Rev. 106
(May, 1957) 620–630.
[81] E. T. Jaynes, “Information Theory and Statistical Mechanics. II,” Phys. Rev. 108
(Oct, 1957) 171–190.
444

[82] Y. LeCun, J. Denker, and S. Solla, “Optimal Brain Damage,” in Advances in
Neural Information Processing Systems, vol. 2. Morgan-Kaufmann, 1990.
[83] J. Frankle and M. Carbin, “The Lottery Ticket Hypothesis: Finding Sparse,
Trainable Neural Networks,” in International Conference on Learning
Representations. 2019. arXiv:1803.03635 [cs.LG].
[84] T. Banks and A. Zaks, “On the phase structure of vector-like gauge theories with
massless fermions,” Nuclear Physics B 196 no. 2, (1982) 189–204.
[85] R. Linsker, “Self-organization in a perceptual network,” Computer 21 no. 3,
(1988) 105–117.
[86] S. Becker and G. E. Hinton, “Self-organizing neural network that discovers
surfaces in random-dot stereograms,” Nature 355 no. 6356, (1992) 161–163.
[87] B. Gale, R. Zemeckis, M. J. Fox, and C. Lloyd, Back to the Future Part II.
Universal Pictures, 1989.
[88] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 770–778. 2016.
[89] S. Ioﬀe and C. Szegedy, “Batch Normalization: Accelerating Deep Network
Training by Reducing Internal Covariate Shift,” in International Conference on
Machine Learning, pp. 448–456. 2015. arXiv:1502.03167 [cs.LG].
[90] A. Veit, M. Wilber, and S. Belongie, “Residual Networks Behave Like Ensembles
of Relatively Shallow Networks,” in Advances in Neural Information Processing
Systems, vol. 30, pp. 550–558. 2016. arXiv:1605.06431 [cs.CV].
[91] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer Normalization,” in Deep Learning
Symposium, Neural Information Processing Systems. 2016. arXiv:1607.06450
[stat.ML].
445

446

Index
1/n expansion, 8, 10, 31, 59, 62, 70, 91–
93, 98, 99, 101, 134, 141, 151, 160,
164, 202, 209, 219, 224, 226, 228,
317, 348, 394, 396, 415
∗-polation, 275, 276, 277, 410
curvature, 278, 279
deep linear networks, 275
nonlinear networks, 276, 278
same for inter- and extra-, 279
δ expansion, 114, 266, 272
generalized to ϵ1,2 expansion, 277
γ[a] basis, 112
σσ, 118, 268
σ′σ′, 269
frozen NTK, 269
kernel, 169, 266, 410
for hybrid approach, 435
output matrix, 170
absolute certainty, 153, 156, 165, 175, 258,
401
∗-polation, 279
action, 11, 26, 27, 32, 33, 70, 401, 411
eﬀective, see eﬀective action
quadratic, see also Gaussian distribu-
tion, 27, 63, 73, 204
quartic, see also nearly-Gaussian dis-
tribution, 28, 29–31, 34
sextic, 395
truncation, see also hierarchical scal-
ing, 34
activation, 37
activation function, 37, 41, 242
GELU, 45, 111, 129, 133, 134, 334, 344,
380
leaky ReLU, 42, 44
linear, 44, 51, 52, 109, 121, 122, 135,
138, 143, 148, 149, 231, 274, 275,
287, 344, 420
monomial, 127
perceptron, 38, 41, 43, 44, 126
quadratic, 109
ReLU, 43–45, 108, 114, 121, 122, 126,
132, 134, 137, 138, 140, 145–148,
231, 236, 242, 243, 333, 334, 343,
344, 380, 395, 420, 421, 435
sigmoid, 43, 44, 123, 125, 126, 242
sin, 43, 111, 128, 132, 234, 236–238,
243, 271
softplus, 45, 126, 129, 132, 134
SWISH, 45, 111, 123, 129, 132–134, 334,
344, 380
tanh, 43, 44, 111, 123–128, 130, 132,
134, 138, 140, 169, 234, 236–238,
242, 243, 271, 341, 421
adventure for thrill seekers, 308, 378
algorithm dependence, 324, 346, 360, 372,
374, 381, 393
algorithm independence, 255, 256, 346, 382
algorithm projector, 324, 334, 371, 372,
374, 378, 381, 382, 393
Anderson, Thomas A. “Neo”, 21
applause, 177
architecture, 7, 38, 40, 107, 195
architecture hyperparameters, 38, 51, 62,
155
Armstrong, Neil, 250
artiﬁcial intelligence, 1, 35, 36, 37, 51
447

artiﬁcial neural network, see neural network
artiﬁcial neuron, 35, 37, 177, 179
atom, 3, 56
attention, see also transformer, 40
backpropagation, 43, 187, 203, 239
backward equation
MLP, 203, 249
backward pass, 203
Banks-Zaks ﬁxed point, see ﬁxed point
batch, see also stochastic gradient descent,
193, 255
Bayes, Reverend Thomas, 161
Bayesian inference, 151–153, 154, 155, 158,
160, 161, 165, 189, 248, 253, 256,
261, 262
connection to linear models, 286
evidence, 152, 154, 157, 163, 166, 167,
169, 170, 406
relation to generalization error, 265
hierarchical modeling, 163
hypothesis, see hypothesis (Bayesian in-
ference)
likelihood, 154, 157–159, 163, 165, 175,
176, 184
model comparison, 152, 154, 163, 163–
166, 168, 170, 402, 406
Bayes’ factor, 164, 165, 166, 169, 402,
406
model ﬁtting, 151, 154, 157, 158, 162,
163, 165, 192, 257, 260
approximation methods, 159
exact marginalization, 160
posterior, see posterior
practicalities, 174, 183
prediction, 152, 158, 162, 163
prior, see prior
via gradient descent
at inﬁnite width, 261
but not at ﬁnite width, 383
wiring
ﬁnite width, 180
inﬁnite width, 176
Bayesian probability, 152, 153
Bayes’ rule, 154, 157, 161–163, 165,
175, 183, 189
hypothesis, see hypothesis (Bayesian in-
ference)
product rule, 153, 155
statements, 152, 153
sum rule, 153, 155
bell curve, see also Gaussian function, 12,
13
bias-variance decomposition, see also gen-
eralization error, 264
bias-variance tradeoﬀ, 192, 263, 267, 378,
410
for a universality class
K⋆= 0 activations, 269
scale-invariant activations, 273
generalized, 264, 273
vs. standard, 264
relation to criticality, 267
biases, see also model parameters, 37, 191
big tech, 60
biological neural network, see also brain, 1,
245
biological neuron, 35, 37, 152, 177
bit (unit of entropy), 401
black box, 2
blueprint, 433
Boltzmann constant, 401, 412
Boltzmann distribution, 412
Boltzmann entropy, see entropy
Boltzmann, Ludwig, 3, 399
bona ﬁde, 349
bottleneck, 53
bra-ket notation, see also Gaussian expec-
tation, 27, 29, 30
brain, 1, 37, 40, 51
brand awareness, 373
Brown, Emmett Lathrop “Doc”, 423
Carnot, Sadi, 2
central limit theorem, 46
chain rule, 194, 199, 200, 203, 239
chain-rule factor, 199, 203, 239–241
448

channel, see also convolutional neural net-
work, 40
chaos, see also overly deep, 63, 397, 424
checkpoint, 395
classiﬁcation, 168, 175, 190, 257, 340, 410
CNN, see convolutional neural network
coarse-graining, see also renormalization group
ﬂow, see also representation group
ﬂow, 71, 104
cognitive science, 35
cokurtosis, see kurtosis, excess
Coleman, Sidney, 189, 197
complete the square, 19, 173, 211, 212
computer vision, 40, 53, 166, 190, 426, 434
connected correlator, 23, 33, 225, 397
four-point, see also four-point vertex,
see also kurtosis, excess, 24, 28,
30, 51, 57, 60, 67, 179, 205
general deﬁnition, 24
higher-point, 34
odd-point vanish with parity, 23
one-point, see also mean, 23
relation to nearly-Gaussian distributions,
26
six-point, 67
two-point, see also covariance, see also
metric, 23
continuum limit, see gradient descent, 358
convex function, 403
ConvNet, see convolutional neural network
convolutional neural network, 40, 53, 155,
163, 166, 426
with residual connections, see ResNet
correlator
2m-point, 65
M-point, 22, 53
connected, see connected correlator
four-point, 57, 60, 206, 207
full, see also moment, 23
higher-point, 57, 63, 66
six-point, 63
two-point, 54, 206, 207
coupling, 11
data-dependent, see data-dependent cou-
pling
non-Gaussian, 32, 33
quadratic, 33, 34, 96, 98, 101, 178–180,
210
quartic, 28–31, 34, 60, 67, 96, 101, 178–
181, 183, 210, 416
running, see running coupling
sextic, 395, 416
covariance, see also cumulant, 16, 23
critical exponent, 129, 130–132, 138, 140,
141, 143, 148, 149, 229, 230, 233,
234, 236–238, 243, 309, 312, 314,
340
critical initialization hyperparameters, see
initialization hyperparameters
critical phenomena, 56
critical temperature, 57
criticality, 51, 56, 57, 61, 62, 66, 67, 108,
110–112, 120–123, 125–130, 132, 134–
136, 138, 139, 142–145, 148, 166–
168, 170, 179, 225, 226, 231–235,
237, 239–242, 264–267, 269, 271,
273, 308, 311, 334, 338, 341, 397,
400, 410, 423, 434, 435
as unsupervised learning, 421
principle of, 108, 152, 271, 274, 358
semi-criticality, 122, 129, 138
cross correlation
dNTK-preactivation, 301
P-recursion, 305
Q-recursion, 306
NTK-preactivation, 207–210, 214, 215,
216, 218, 219, 224, 225, 230, 238
D-recursion, 217
F-recursion, 219
cross entropy, 404
cross-entropy loss, see loss
cubic model, see nonlinear model
cumulant, see also connected correlator, 23,
412
ﬁrst (mean), see also mean, 23
general deﬁnition, 24
449

second (covariance), see also covariance,
23
cutoﬀ, eﬀective theory, 108, 141, 143, 230,
290, 310, 380, 395, 407
nearly-kernel methods, 327
vs. measurement precision, 406
damping force, 365–367, 369, 374
data dependence, see also connected corre-
lator, see also data-dependent cou-
pling, 130, 131, 140
data distribution, see also input data, 190,
191, 192
data-dependent coupling, 90, 96, 101, 161,
164, 225, 390, 392–396
dataset, see input data
ddNTKs, 333–335, 336
contribution to ﬁnite-width prediction,
376
full expressions, 383
scaling laws, 339
statistics, 337
R-recursion, 339, 386
S-recursion, 339, 386
T-recursion, 339, 387
U-recursion, 339, 387
step-independence, 357
deep learning, 1, 10, 37, 43, 107, 151, 154,
163, 176, 193, 203, 226, 236, 239,
389, 397
abstracted, 315
deep but not yet learning, 151
history, 36
deep linear network, see also linear, 51,
51–53, 55, 56, 58, 62, 63, 108, 109,
111, 122, 134, 135, 138, 143, 149,
240, 274, 275, 279, 287, 397
limitations, 275
deformation
Gaussian distribution, 28, 33, 412, 414
linear model, 316
quadratic model, 329
defrosted NTK, see neural tangent kernel
degradation problem, see also overly deep,
see also residual network, 423, 424,
426, 429, 430, 432
degrees of freedom, 104, 403, 409
depth, 7, 38
determinant, 17
diagonalization, 17, 32, 115, 146
diﬀerence equation, see also training dy-
namics, 358, 359, 361, 362, 364
linear, 357, 360
homogeneous, 358
inhomogeneous, 365
nonlinear, 357
diﬀerential of the neural tangent kernel, see
also meta kernel, 290–292, 337
connection to representation learning,
327
dNTK-preactivation cross correlation,
see cross correlation
dynamical, see dynamical dNTK
iteration equation, see also forward equa-
tion, 295
name, 293
scaling laws, 309
dimensional analysis, 33, 34, 138, 309, 339,
401, 406
Dirac delta function, 48, 49, 50, 74, 78, 116,
158, 168, 204, 211, 342, 392
integral representation, 49, 74
Dirac, Paul Adrien Maurice, vii, viii, 189,
197
direct optimization, 318, 324, 356
directed acyclic graph, 39
discriminative model, see also probabilistic
model, 190
disorder, see entropy
distillation, see knowledge distillation
dNTK, see diﬀerential of the neural tangent
kernel
Don’t Panic, HHGTTG, 183
double factorial, 14
duality, 9, 240, 282, 283, 285, 287, 321, 325,
394
450

learning algorithm – algorithm projec-
tors, 382
linear model – kernel methods, 280
microscopic-macroscopic, 389, 394, 423
nonlinear model – nearly-kernel meth-
ods, 315
dynamical dNTK, 317, 360, 368
dynamical NTK, see also eﬀective kernel,
see also interaction NTK, 314, 344,
357, 360
dynamics, see training dynamics
Eames (Inception meme), 163
early stopping, see regularization
eBook, 228
eﬀective action, 101, 123
as an eﬀective theory, 104
connection to RG ﬂow, 104
in physics, 104
eﬀective feature function, see feature func-
tion
eﬀective kernel, see nearly-kernel methods
eﬀective theory, 2, 41, 93, 103, 104, 123,
136, 141, 143, 159, 162, 164, 201–
203, 236, 238, 325, 372, 389, 395,
396, 406, 433
representation learning, 349
eﬀective theory of deep learning, 41, 51, 62,
69, 71, 108, 162, 166, 190
eﬀectively deep, see also optimal aspect ra-
tio, 10, 335, 400, 422, 433
range extended by residual connections,
424
eigenvalue, 17
eightfold way, see also Gell-Mann, Murray,
viii
Einstein summation convention, 18
Einstein, Albert, 289
elementary operation, 36
emergent scale, 52, 60, 108, 135, 230
end of training, 260, 325, 333, 399
engineering, 382, 434
ensemble, see also probability distribution,
46, 120, 135, 143, 153, 162, 189,
190, 421, 432, 433
entropy, 399, 400, 401–407, 409, 411, 414,
415, 417, 419
additivity, 402, 403, 416
subadditivity, 403, 404, 421
as a measure of disorder, 401
Boltzmann entropy, 402
Gibbs entropy, 402
next-to-leading-order correction, 417
Shannon entropy, 401
epigraph, 396
epilogue, 390, 396
epoch, see also stochastic gradient descent,
193
equivalence principle, 242, 243, 258, 270–
273, 311, 313, 341, 356, 358, 423
connection to generalization, 271, 274
error factor, 195, 196, 199, 201, 239, 240,
248
ℓ-th-layer, 292, 335
cross-entropy loss, 248
MSE loss, 194
error function, 45
evidence, see Bayesian inference
expectation value, 11, 14, 21, 21, 225
exploding and vanishing gradient problem,
56, 120, 170, 226, 239, 240, 241,
242, 267, 423, 427, 429, 430, 434
connection to generalization, 271
for residual networks, 434
relation to criticality, 240
exploding and vanishing kernel problem, 56,
110, 111, 122, 168, 239–241, 243
expressivity, 38
extensivity
of entropy, 403, 409
of loss, 159, 192, 264
extrapolation, see also ∗-polation, 275
Facebook AI Research, viii, 175
FAIR, see Facebook AI Research
FCN, see fully-connected network
feature, see also representation, 40, 62, 103,
155, 183, 196, 198, 262, 351, 353,
451

389
vs. feature function, 286
feature function, 280, 286, 315, 330, 382
eﬀective, 316, 321, 349
feature engineering for abstract inputs,
280
meta, see meta feature function
meta-meta, see meta-meta feature func-
tion
nonlinear model, 316
random, see also random feature model,
8, 285–287, 391
feature indices, 315
feature space, 282, 322
feedforward network, 39, 180
ferromagnetism, 56
Feynman, Richard, 11
ﬁne tuning, 56, 321
ﬁnite-width prediction
Bayesian inference, 181
gradient descent, see also T-shirt equa-
tion, 372
ﬁxed point
Banks-Zaks, see also optimal aspect ra-
tio, 421
nontrivial, see also criticality, 56, 111,
122, 124, 125, 127, 128, 131, 138,
167
half-stable, see also GELU, see also
SWISH, 132–134
trivial, 56, 110, 111, 120, 122, 134, 167,
170
float, see type (data)
ﬂuctuations, 23, 52, 62, 108, 135, 152, 206,
380, 397, 423, 429
in deep linear networks, 61
vs. representation learning, 380
for your information, 421
force, see Newton’s second law
forward equation
ddNTKs, 384, 385
dNTK, 297
general residual network, 434
MLP preactivations, 198, 200, 203–205,
297, 298
NTK, 198, 200–203, 205, 210, 213, 215,
241, 242
residual MLP preactivations, 426
forward pass, 203
four-point vertex, see also data-dependent
coupling, 79, 98, 102, 135, 138,
139, 142, 143, 161, 179, 183, 207,
209, 218, 224, 225, 229, 230, 232,
235, 412, 420, 430
residual MLPs, 430
Fourier transform, 22
free energy, 411
frequentist probability, 153
frozen NTK, 226, 227, 229, 230, 232, 233,
236, 237, 241, 242, 247, 251, 256,
261, 265, 266, 268, 273, 274, 285,
309, 311, 348
δ expansion, see δ expansion
ϵ1,2 expansion, see δ expansion
features, 286
inﬁnite-width limit of the NTK, 226
midpoint, 266, 270
polar angle parameterization, 272
full correlator, see correlator
fully-connected network, see also multilayer
perceptron, 38
fully-trained condition
ﬁnite width, 346
inﬁnite width, 251
function approximation, see also machine
learning, 5, 36, 37, 38, 45, 46, 51,
155, 158, 165, 190, 191, 194–196
for linear models, 280
functional, 400, 404
Gauss, Carl Friedrich, 245
Gauss-Jordan elimination, 174
Gaussian distribution, 8, 12, 21, 32, 34, 46,
47, 52, 53, 63, 66, 135, 204, 209,
297, 392, 394, 396, 401, 412, 414
action, 27, 73
as a Gaussian process, 396
452

entropy, 408, 410
multivariable, 17
normal distribution, standard, 13
relationship to Dirac delta function, 48
single-variable, 13
zero-mean, deﬁned by variance, 22
Gaussian expectation, see also bra-ket no-
tation, 27, 76, 145–148, 150, 182,
205, 206, 212–214, 218, 219, 227,
228, 232, 233, 235, 272, 413
Gaussian function, 12, 16, 37
Gaussian integral, 12
Gaussian process, see Gaussian distribution
gedanken inference, 166
gedanken model, 397
GELU, see activation function
general relativity, 18, 167, 212, 253
generalization, 164, 192, 194, 196, 262, 263,
410
generalization error, 262, 265, 270, 275,
390, 423
bias, 263–266, 268, 378, 381
related to ∗-polation, 279
exact Bayesian inference, 268
ﬁnite-width, 378, 382
optimal hyperparameter tuning, 271
robustness measure, 265, 266
universality class analysis, 266
variance, 263, 264, 266, 269, 380
generalized posterior distribution, see pos-
terior distribution
generating function, 15, 18, 19, 22, 210,
212, 213, 249, 300
giant leap, see also small step, 250, 347
Gibbs distribution, see Boltzmann distri-
bution
Gibbs entropy, see entropy
Gibbs, J. Willard, 3
glasses (Bayesian), 166
goode olde calculation, 209
GPU, see graphical processing unit
gradient, 6
gradient clipping, see also exploding and
vanishing gradient problem, 242
gradient descent, 52, 160, 180, 189, 190,
192, 193, 194, 195, 197, 198, 204,
225, 240–242, 250, 255, 324, 382,
393
as Bayesian inference
at inﬁnite width, 261
but not at ﬁnite width, 383
continuum or ODE limit, 256, 358, 371,
378, 379, 381
model ﬁtting, 191, 193
stochastic, see stochastic gradient de-
scent
tensorial, 194, 202, 251
wiring
ﬁnite width, 350, 351, 376
inﬁnite width, 248
gradient-based learning, see gradient descent
graphical processing unit, 36
group representation theory, 103
Hamiltonian, see also neural tangent ker-
nel, 190, 195, 359
hard drive, 401
hat (occupational), 401
Hebb, Donald, 177
Hebbian learning, see also neural associa-
tion, 152, 176, 177, 179, 180, 377,
400, 416
Herculean sequence, 145
Hessian, 6, 264
hidden layer, 40
hierarchical scaling, 34
Hinton, Geoﬀrey Everest, 225
Hopﬁeld network, 179
Hubbard-Stratonovich transformation, 74,
211
human perception, 38
hybrid approach, 436
hype, 389
hyperparameters
architecture, see architecture hyperpa-
rameters
453

initialization, see initialization hyper-
parameters
regularization, see regularization hyper-
pameters
residual, see residual hyperparameters
scaling in an eﬀective theory, 202
training, see training hyperparameters
Hyperparameters, see also hypothesis (Bay-
esian inference), 155
hypothesis (Bayesian inference), 152, 153–
155, 165
categorical, see also cross-entropy loss,
see also softmax distribution, 156,
158, 257
deterministic, see also Dirac delta func-
tion, 156–158
meta hypothesis, see meta hypothesis
uncertain, see also Gaussian distribu-
tion, see also mean squared error,
156, 158, 257
identity matrix, 16
identity operator, 358
imaginary time, 359
indices
feature, see feature indices
layer, see layer indices
neural, see neural indices
sample, see sample indices
vectorial, see vectorial indices
induced distribution, 47, 49
inductive bias, 62, 109, 135, 152, 165, 166,
174–177, 179, 183, 240, 397, 400,
433
for representation learning in nonlinear
models, 321
of activation functions, 275, 279
of learning algorithms, see also algo-
rithm projector, 334, 371, 381, 382
of model architectures, 40, 330
of sparsity in deep learning, 397
Industrial Age, 2
inﬁnite-width limit, see also not really deep,
7, 61, 66, 70, 93, 135, 164, 166,
175, 176, 205, 207, 208, 219, 224,
226, 241, 247, 380, 396, 409, 420,
429
connection to linear models, 287
of deep linear networks, 59, 60
of residual MLPs, 428
inﬁnity, 245
InfoMax principle, see also unsupervised learn-
ing, 421, 422
information, see also surprisal, 402, 403
Information Age, 3, 399
information theory, see also statistical me-
chanics, 10, 140, 381, 397, 399,
400, 402, 412
perspective on ∗-polation, 410
perspective on criticality, 410
perspective on generalization, 410
infrared (RG ﬂow), 105
initialization (of you), 1, 399
initialization distribution, 5, 46, 53, 120,
135, 153, 155, 160, 162, 170, 179,
190, 191, 197, 199, 202, 203, 211,
391, 424
initialization hyperparameters, 47, 51, 56,
62, 108, 111, 121, 122, 125, 145,
151, 155, 160, 169, 225, 227, 232–
234, 242, 256, 260, 262, 267, 271,
273, 290, 421, 423, 427, 435
critical, 56, 109, 111, 112, 121, 123–
125, 128, 133–135, 139, 426
at ﬁnite width, 141, 143
for K⋆= 0 universality, 128
for residual networks, 428
for scale-invariant universality, 121,
125
input data, see also test set, see also train-
ing set, see also validation set, 37,
40, 47, 53, 93, 94, 135, 155, 165,
167, 180, 195, 199, 257, 280, 352,
394, 396, 416
instruction manual, 378
int, see type (data)
integral representation, 49, 74
454

integrating out, see also marginalizing over,
78, 96, 160, 162, 163, 210–212, 220,
390
integration by parts, 119, 213, 227
intensivity (of loss), 192
interacting theory, 9, see also non-Gaussian
distribution, 31
entropy and mutual information, 411
variational method, 417
interaction NTK, 360, 361, 364, 365, 368
interactions, 8, 32, 33, 34, 60, 101, 108,
135, 176, 179, 183, 204, 205, 209,
396, 399, 403, 405
connection to statistical (in)dependence,
31
dynamics, 318, 359, 360
weakly-interacting, 318
nearly-kernel methods, 325
self-interactions, 32
strong coupling, 396
interlayer correlation, 187, 209, 210, 212,
249, 250, 300
for dNTK evaluation, 299
interpolation, see also ∗-polation, 275
intralayer correlation, 209
inverse algorithm design, see also algorithm
projector, 382
inverting tensor, 363
iron, 56
irrelevant (RG ﬂow), 106
Jacobian, 406
input-output, 136
Jaynes, Edwin T., 151, 153, 412
Jensen inequality, 403, 404
Johnny B. Goode, 423
joules per kelvin (unit of entropy), 401
k-nearest neighbors, see kernel methods
Kaiming initialization, see also initializa-
tion hyperparameters, 122
kernel, see also metric, 98, 102, 109, 135,
225–227, 229, 230, 240, 242
δ expansion, see δ expansion
γ[a] basis, see γ[a] basis
eﬀective kernel, see nearly-kernel meth-
ods
inﬁnite-width limit of the metric, 98
kernel matrix
diagonal, 144, 148, 272
polar angle parameterization, 144, 272
linearized recursion, 110
meta kernel, see nearly-kernel methods
midpoint, 114, 115–120, 125, 127, 129,
130, 132, 133, 144
NTK, see neural tangent kernel
trained kernel, see nearly-kernel meth-
ods
kernel machine, see kernel methods
kernel methods, 283, 315, 322, 325
k-nearest neighbors, 284
as a memory-based method, 284
feature, see feature function
kernel, 282, 326, 382
Gaussian, 284
linear, 282
stochastic, 287
kernel trick, 284
prediction, 258, 285, 326, 374, 377
as a linear model, 280
stochastic kernel, see also random fea-
ture model, 285
kernel trick, see kernel methods
kink, see also leaky ReLU, see also ReLU,
44
KL divergence, see Kullback–Leibler diver-
gence
knowledge distillation, 175, 258
Konami Code, 166
Kronecker delta, 16, 46, 48, 55, 59, 64, 167,
194, 195, 207, 220, 402
Kullback–Leibler divergence, 257, 403–405
kurtosis, excess, see also connected corre-
lator, 24, 179
cokurtosis, 179
label, 190, 240, 258
hard, see also one-hot encoding, 258
455

soft, 258
label smoothing, see regularization
Landau, Lev, 3
language model, 40, 426, 434
Laplace transform, 22
Laplace’s principle of indiﬀerence, 401
large-n expansion, see 1/n expansion
layer, 4, 37
layer indices, 178, 179, 217, 277, 315, 408,
411
layer normalization, 435
lazy training, 353
leaky ReLU, see activation function
learning algorithm, see also Bayesian in-
ference, see also gradient descent,
5, 36, 52, 152, 158, 160, 165, 175,
189, 193, 194, 259, 260, 324, 382,
393
dual to algorithm projector, 382
learning rate, 193, 194, 196, 202, 232, 236,
238
global, 192, 194, 199, 201, 202, 247,
251, 253, 393
step-dependent, 255
learning-rate tensor, 194, 198, 199, 202,
251, 253, 256, 291, 335, 351
layer-diagonal, 199
layer-independence, 292
learning rate equivalence principle, see equiv-
alence principle
Life, the Universe, & Everything, HHGTTG,
152
likelihood, see Bayesian inference
linear, see activation function
linear model, 262, 280, 281, 287, 315, 353,
383, 391
for eﬀective features, 316
is not a deep linear network, 287
linear regression, see also linear model, 281,
282, 315, 318
vs. quadratic regression, 317, 318
linear transformations, 52, 53, 275
logistic function, see also softmax distribu-
tion, 43, 45, 126, 157
loss, 158, 159, 190, 191, 192–195, 240
algorithm dependence at ﬁnite width,
372
auxiliary, 158
comparison of MSE and cross-entropy,
257
cross-entropy, 158, 240, 248, 256–258,
265, 404
MSE, 158, 191, 194, 195, 202, 240, 248,
251, 252, 256, 263, 265, 351
for linear models, 281
generalized, 252, 351
name, 192
nonlinear models, 317
of generality, 262
SE, 192
test loss, 192, 195, 196, 264
relation to generalization, 262
training loss, 191, 192–196
relation to overﬁtting, 262
relation to underﬁtting, 262
lottery ticket hypothesis, 417, 422
machine learning, see also statistics (branch
of mathematics), 37, 43, 153, 164,
189, 203, 258, 263, 280, 315, 390,
397
MacKay, David, 164, 389
macroscopic perspective, see also sample
space, 2, 164, 390, 394, 396, 397,
399, 401, 423, 426
magic trick, 49, 210, 343
magnetic ﬁeld, 56
magnetism, 56
MAP, see maximum a posteriori
marginal (RG ﬂow), 106, 143, 349
marginalization rule, 94, 95, 96, 153
marginalizing over, see also integrating out,
78, 98, 161–163, 205, 210
matrix-vector product, 174
matter, 3
maximum a posteriori, 159, 160, 163
gradient descent approximation, 260
456

maximum entropy, principle, 400, 401, 412,
420
maximum likelihood estimation, 159, 160,
163, 174, 192
gradient descent approximation, 260
Maxwell, James Clerk, 3
McFly, Martin Seamus “Marty”, 423
McGreevy, John, 69
mean, see also cumulant, see also moment,
13, 23
mean squared error, see loss
measurement precision cutoﬀ, see cutoﬀ
mechanics (physics), 189
memory-based method, see kernel methods,
see nearly-kernel methods
meta feature function, 316, 330
dynamical, 334
random, 328
meta hypothesis, 163
meta kernel, see nearly-kernel methods
meta representation learning, see represen-
tation learning
meta-meta feature function, 317, 329
metric, see also data-dependent coupling,
see also kernel, 72
ﬁrst-layer, 72
inﬁnite-width limit, 98
inverse, 73
ℓ-th-layer, 88
mean, 79
next-to-leading-order correction, 98, 101,
135–138, 140–143, 348
second-layer, 78
stochastic, 78, 285
microscopic perspective, see also parameter
space, 2, 382, 390, 394, 396, 399,
401, 423, 424
microstate (statistical mechanics), 412
midpoint input, 113, 114, 117, 120, 144,
266
midpoint kernel, see kernel
mini-batch, see batch
minimal model, 10
of deep learning, 41
of representation learning, see repre-
sentation learning
Minsky, Marvin, 107, 225
MLE, see maximum likelihood estimation
MLP, see multilayer perceptron
mode, see also maximum a posteriori, 159
model comparison
Bayesian, see Bayesian inference
linear model vs. quadratic model, 320
model complexity, 164, 320, 390, 391, 394–
397
model ﬁtting, see also training
Bayesian, see Bayesian inference
gradient-based optimization, see gradi-
ent descent
model parameters, see also biases, see also
weights, 4, 36, 38, 47, 50, 163, 189–
191, 193, 195, 389, 394, 423
connection to observables, 3
residual network, 434
molecule, 3
moment, see also full correlator, 14, 15, 18,
20, 21, 22, 24, 225
MSE, see mean squared error
MSE loss, see loss
multilayer perceptron, 38, 39, 74, 78, 155,
163, 166, 225, 239
a.k.a. a fully-connected network, 38
beyond, 433
vanilla, 426–428
with residual connections, 425–427, 430,
434
mutual information, 399, 400, 404, 405,
407, 409, 411, 414, 416, 421, 430
next-to-leading-order correction, 417
Narrator (Arrested Development), 163
nat (unit of entropy), 401
natural language processing, 40, 190, 389,
426, 434
natural logarithm, 401
naturalness, see also ﬁne tuning, 321
near-sparsity, see sparsity, principle of
457

nearly-Gaussian distribution, 9, 11, 23, 26,
28, 30–34, 60, 67, 77, 81, 86, 180,
205, 209, 377, 393, 394, 396, 400,
401, 411, 412
action, 32, 86
as a nearly-Gaussian process, 396
connected correlators as observables, 26
entropy, 411
nearly-Gaussian process, see nearly-Gaussian
distribution
nearly-kernel machine, see nearly-kernel meth-
ods
nearly-kernel methods, 290, 315, 324, 325,
326, 373
as a memory-based method, 325
eﬀective kernel, 325–328
in terms of eﬀective feature functions,
328
relation to dynamical NTK, 328
kernel, 322
meta kernel, 323
other potential names, 323
prediction, 324
trained kernel, see also trained NTK,
326, 374
prediction, 326
wiring, 326
nearly-linear model, see nonlinear model
nearly-linear regression, see quadratic re-
gression
negative log probability, see action
negative log-likelihood, see also loss, 158,
159
neural association, see also Hebbian learn-
ing, 152, 176, 177, 179, 433
neural indices, 47, 55, 59, 64, 74, 98, 135,
179, 195, 198, 200, 204, 206, 213,
220, 247, 315
neural network, 1, 4, 35, 37, 40, 107, 189–
191, 239, 389, 397
history, 36
neural tangent kernel, 136, 190, 195, 197,
202, 225, 226, 359
agitated, 227, 233, 237, 238
deﬁned in conjunction with dNTK, 292
defrosted, 227
dynamical, see dynamical NTK
dynamics, 361
ﬁrst-layer, 204, 205
frozen, see frozen NTK
interaction, see interaction NTK
ℓ-th-layer, 199, 200, 209, 210, 213, 217
mean, 209, 213, 214, 215, 218, 221–
227, 232, 236, 237, 241
next-to-leading-order correction, 348
name, 195, 227, 359
NTK-preactivation cross correlation, see
cross correlation
second-layer, 205, 206
step-independent, 357, 366
trained, see also trained kernel, 374
variance, 206–209, 218, 219, 220, 221,
224, 225, 229, 238
A-recursion, 222
B-recursion, 220
neuron, 1, 4, 37, 39
neuroscience, 35
Newton tensor, see also second-order up-
date, 252, 253, 255, 347
as a metric on sample space, 253, 351
generalized, 351
Newton’s method, 253, 254–257, 348, 381
as a second-order method, 254
Newton’s second law, 189
NLO metric, see metric
no-free-lunch theorem, 397
non-Abelian gauge theory, see also Banks-
Zaks ﬁxed point, 421
non-Gaussian distribution, see also nearly-
Gaussian distribution, 31, 33, 66,
396
non-informative prior, see prior
non-parametric model, see also Gaussian
process, 164, 396
nonlinear model, 290, 315, 316
cubic model, 317, 329
458

quadratic model, 290, 317, 320, 324,
327, 329
with wiring, 329
nontrivial ﬁxed point, see ﬁxed point
normal distribution, see Gaussian distribu-
tion
normalization factor, see also partition func-
tion, 13, 16, 26, 96, 115, 163, 164
not really deep, see also inﬁnite-width limit,
10
NTK, see neural tangent kernel
objective function, see also loss, 191
observable, 3, 11, 14, 21, 153, 190, 195,
196, 242, 400, 403, 436
Occam’s razor, see also sparsity, principle
of, 152, 164, 165, 169, 320, 390,
402
ODE limit, see gradient descent
one-hot encoding, 168, 258
one-parameter families, 275, 429
optimal aspect ratio, see also eﬀectively deep,
10, 335, 380, 400, 411, 421, 426,
430, 432, 433, 436
optimal brain damage, 417, 422
optimization, see gradient descent, see train-
ing, see also direct optimization,
see also Newton’s method
orthogonal matrix, 16, 32
outcome space, 405, 406
output distribution, 47, 49, 50, 62, 66, 156,
189
output matrix, 170
γ[a] basis, see γ[a] basis
overﬁtting, see also generalization, 164, 196,
262, 264, 390
by ﬁne tuning the parameters, 321
overly deep, see also chaos, see also degra-
dation problem, 10, 335, 400, 422–
424
overparameterization, 164, 281, 283, 285,
389, 393, 394, 397
in quadratic models, 318
Papert, Seymour, 107, 225
parallel susceptibility, 110, 119, 123, 168,
170, 227, 229, 231, 234, 235, 241,
267, 309, 428
paramagnetism, 56
parameter space, see also microscopic per-
spective, 193, 194, 251, 253, 334,
394
parameters, see model parameters
parity symmetry, 23, 25, 32
partition function, see also normalization
factor, 15, 19, 26, 28, 74, 90, 411
quadratic action, 27
with source, see also generating func-
tion, 15
perceptron, see Perceptron architecture
perceptron, see activation function
Perceptron architecture, 35, 38
permutation symmetry, 46, 120, 429
perpendicular susceptibility, 119, 120, 123,
169, 170, 227, 229, 231, 234, 241,
269, 309, 428
perturbation theory, 8, 11, 28, 31, 32, 116,
318, 359, 393, 411
perturbative cutoﬀ, see cutoﬀ
phase transition, 56
physics, 2, 3, 8, 69, 71, 74, 103, 123, 159,
179, 182, 318, 321, 343, 401
piece of cake, see also free dynamics, 358
point estimate, see also mode, 159
Polchinski, Joseph, 11
polynomial regression, 321
positive semideﬁnite matrix, 194
positive deﬁnite matrix, 16
posterior, 152, 154, 157–161, 163–166, 171,
174, 177, 179, 183, 189, 260, 261
generalized posterior distribution, see
also gradient-based learning, 246,
260, 261, 264, 266, 410
inﬁnite-width distribution, 174
posterior covariance, 172–174, 180, 182,
260
ﬁnite width, 180
459

posterior mean, 173, 174, 181–183, 259,
383
ﬁnite width, 180
practical practitioners, 202, 236
preactivation, 37
pretraining, 11, 399, 421
principle, 2
criticality, see criticality
InfoMax, see InfoMax principle
learning-rate equivalence, see equiva-
lence principle
maximum entropy, see maximum en-
tropy, principle
of indiﬀerence, see Laplace’s principle
of indiﬀerence
sparsity and near-sparsity, see sparsity,
principle of
typicality, see typicality
variational, see variational principle
principles of deep learning theory, 41, 331
prior, 154, 155–157, 159–161, 163, 165–
167, 179, 180, 189, 191, 400, 408
non-information prior, see also Laplace’s
principle of indiﬀerence, 402
probabilistic model, 153, 154, 157, 163, 164,
190
probability (branch of mathematics), see
also Bayesian probability, see also
frequentist probability, 11, 31, 153
probability distribution, 11, 12, 16, 18, 21,
22, 23, 26, 27, 46, 400
as a density, 406
programming, 37, 45
programming note, 171
PyTorch, 435
QED, 44
quadratic model, see nonlinear model
quadratic regression, see also quadratic model,
317, 318, 356
nearly-linear, 317
quantum electrodynamics, see QED
quantum mechanics, 3, 51, 116, 189, 197
Rabi, Isidor Isaac, 335
RAID, see also Redundant Array of Inex-
pensive Disks, 403
random feature function, see feature func-
tion
random feature model, 285, 329
random meta feature model, 329
recurrent neural network, 239, 242
redundancy (information theory), 400, 408,
422
Redundant Array of Inexpensive Disks, see
also RAID, 403
regression, 257, 340
linear, see linear regression
nearly-linear, see quadratic regression
polynomial, see polynomial regression
regularization, 159, 160, 258, 260, 321
early stopping, 258
for linear models, 282
interpretation of representation learn-
ing, 321
label smoothing, 258
regularization hyperpameters, 260
relative entropy, see Kullback–Leibler di-
vergence
relevant (RG ﬂow), 106, 135, 140, 143, 225,
229, 230, 238, 310, 417, 429
ReLU, see activation function
renormalization group ﬂow, 103, 123, 141,
349, 389, 421
representation, see also feature, 71, 103,
135, 155, 176, 183, 198, 421
representation group ﬂow, 71, 103, 105,
123, 128, 130, 132, 134, 135, 140,
143, 176, 190, 198, 225, 280, 286,
290, 330, 331, 337, 349, 389, 400,
417, 422, 424, 429
name, 103
of preactivations, 69
of the ddNTKs, 337
of the dNTK, 294
of the NTK, 197
representation learning, 1, 8, 62, 166, 175–
460

177, 179, 183, 186, 187, 259, 280,
286, 315, 331, 364, 380, 396, 421
as the evolution of feature functions,
286
for deep linear networks, 287
for quadratic models, 317, 320
manifested at ﬁnite width, 349
meta representation learning, 361
minimal model, 290, 315, 317, 326, 329,
330
non-minimal model, 317
vs. ﬂuctuations, 380
vs. kernel learning, 288
residual block, 424, 425, 426, 433, 434
residual connection, 10, 41, 423, 424, 426,
427–430, 432, 433
other names, 426
residual function, 424
residual hyperparameters, 426, 433, 434
optimal, 433
residual network, 41, 330, 380, 400, 422,
423, 424, 425, 427, 429, 434
general, 434
ResNet, 41, 426, 434
RG ﬂow, see renormalization group ﬂow,
see representation group ﬂow
RG ﬂow and RG ﬂow, 101, 123
RNN, see recurrent neural network
Rosenblatt, Frank, 35
Rumelhart, David Everett, 225
running coupling, 60, 62, 67, 96–98, 101,
103, 225, 414
quadratic, 96, 97, 414
quartic, 415
sextic, 417
saddle-point approximation, see also point
estimate, 159
sample indices, 37, 47, 57, 63, 74, 96, 113,
135, 136, 157, 161, 167, 190, 195,
196, 204, 207, 208, 216, 227, 253
sample space, viii, 252, 253, 347, 351, 394
saturation (of an activation), 43, 44, 241,
242
scale invariance, 44, 45, 111, 121, 123, 134
scaling ansatz, 129, 131, 132, 139, 141, 142,
148, 229, 237, 238, 309, 339
scaling hypothesis, 389, 390, 397
scaling law, 140, 229, 230, 238, 310, 314,
379, 389, 416
Schr¨odinger’s cat, 153
Schwinger-Dyson equations, 84, 184, 326,
352
second-order method (optimization), see also
Newton’s method, 252, 254
second-order update, see also Newton ten-
sor, 252, 253, 350
generalized, 350, 351
self-averaging, see also Dirac delta func-
tion, 48, 80, 224, 247, 286
self-interaction, see interactions
semi-criticality, see criticality
semigroup, see also RG ﬂow, 103
SGD, see stochastic gradient descent
Shannon entropy, see entropy
Shannon, Claude, 399
Shenker, Stephen, 69
shortcuts, see residual connection
sigmoid, see activation function
simple harmonic oscillator, see also Sho,
viii, 51
sin, see activation function
six-point vertex, see also data-dependent
coupling, 395
skip connection, see residual connection
slay the beast (NTK variance), 219
small step, see also giant leap, 247, 250
softmax distribution, see also logistic func-
tion, 156–158, 248, 257, 258
softplus, see activation function
source term, see also generating function,
14, 19, 210
spacetime, 151
sparsity, principle of, 8, 9, 164, 390, 397
near-sparsity at ﬁnite width, 392, 393,
396, 397
spin, see also bit (unit of entropy), 56, 113
461

spoiler alert, 187, 230
statement, see Bayesian probability
statistical dependence, see also interactions,
see also nearly-Gaussian distribu-
tion, 34, 402, 403, 416
statistical independence, 31, 32, 34, 60, 135,
175, 205, 206, 208, 402, 403, 409
absence of interactions and connection
to Gaussian distribution, 31
statistical mechanics, see statistical physics
statistical physics, 3, 56, 108, 389, 400, 402,
412
statistics (branch of mathematics), see also
machine learning, 159, 179, 390
statistics (of a random variable), see also
probability distribution, 21
Bayesian interpretation, 153
steam engine, 2
step-evolution operator, 358, 359, 362, 366
stochastic gradient descent, 160, 193, 250,
255, 256
str, see type (data)
subleading corrections, see also 1/n expan-
sion, 98, 99–101, 135, 136, 140–
143, 226
supervised learning, 190, 192, 195, 421
with linear models, see linear regres-
sion
with quadratic models, see quadratic
regression
surprisal (information theory), 402, 405,
411
susceptibility
parallel, see parallel susceptibility
perpendicular, see perpendicular sus-
ceptibility
SWISH, see activation function
synergy (information theory), 408
T-shirt equation, 373
tablet, 228
tanh, see activation function
Taylor series, 5, 36
temperature, 56, 412
tensor, 5, 16, 28, 74, 194, 251
learning-rate tensor, see learning rate
Newton tensor, see Newton tensor
tensor decomposition
γ[a] basis, see γ[a] basis
ddNTKs R/S/T/U, 338, 376
dNTK-preactivation P/Q, 299, 304, 375
four-point correlator, 59
giving data-dependent couplings, 393
metric mean and ﬂuctuation, 84, 184
NTK mean and ﬂuctuation, 206, 213,
305, 306, 347
NTK variance A/B, 207, 220, 375
NTK-preactivation D/F, 208, 216
six-point correlator, 64
tensorial gradient descent, see gradient de-
scent
test loss, see loss
test set, 192, 247, 253, 258, 262–264, 396
thermodynamics, 2, 401, 411
traditionality, see also exploding and van-
ishing gradient problem, 242
trained kernel, see nearly-kernel methods
trained NTK, see neural tangent kernel
training, see also gradient descent, see also
model ﬁtting, 5, 37, 45, 160, 189,
191–193, 226, 227, 239–242, 250
training data, see training set
training dynamics
controlled by the NTK, 190
ﬁnite width, 345–372
inductive bias, 334
inﬁnite width, 247–254
training hyperparameters, 160, 193, 199,
200, 225, 226, 242, 256, 260–262,
268, 270, 271, 273, 274, 291, 423,
427
independent from the optimization al-
gorithm, 356
training loss, see loss
training set, 5, 191, 192, 193, 196, 247,
251, 253, 259, 262, 390, 396
transformer, 40, 41, 155, 163, 166, 389,
462

426, 434, 435
transistor, 3
transition matrix, 88
translational invariance, 40, 53, 166
tripartite information, 407, 410, 422
trivial factor, see also exploding and van-
ishing gradient problem, 239–241
trivial ﬁxed point, see ﬁxed point
truncated normal distribution, 46
Turing, Alan, 51
type (data)
ﬂoating-point precision, 407
integer, 33
string, 33
type safety, see also dimensional analysis,
33, 359
typicality, 61, 69, 134, 197
principle of, 162, 380
ultraviolet (RG ﬂow), 105
underﬁtting, 262, 264
underparameterization, 282, 319
uniform distribution, 46, 412
universality, 104, 108, 123, 225, 389
of the fully-trained network solution,
346, 372, 381
universality class, 123
half-stable, 134
K⋆= 0, 125, 127, 128, 131, 132, 139–
141, 150, 225, 230, 231, 234–238,
240–242, 268–270, 273, 310, 312,
341, 420, 429, 431
scale-invariant, 123, 125, 129, 135, 137,
138, 140, 141, 143, 145, 147, 148,
225, 230–232, 234–236, 240–242, 268,
272, 273, 310, 333, 342, 344, 420,
429, 431
transcended by scaling laws, 140
unstructured data, 397
unsupervised learning, 103, 381, 400, 410,
421, 426, 430, 432
as pretraining, 421
validation set, 263
variance, see also cumulant, 13, 16
variational ansatz, 411, 414
variational principle, see also maximum en-
tropy, principle, 400, 411, 412, 414
vectorial indices, 190, 195, 196, 315
von Neumann, John, 1, 37
website, see deeplearningtheory.com
weight tying, see also convolutional neural
network, 40
weights, see also model parameters, 37, 191
Wick contraction, 20, 54, 58, 59, 63, 72,
109, 182, 213, 220
Wick’s theorem, 11, 14, 15, 21, 27, 29, 30,
60, 63
width, 7, 38
Williams, Ronald J., 225
wiring, see also Hebbian learning
in Bayesian inference, see Bayesian in-
ference
in gradient-based learning, see gradient
descent
in nearly-kernel methods, see nearly-
kernel methods
zero initialization, see also initialization dis-
tribution, 46, 120, 429
463

