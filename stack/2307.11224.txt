JINA EMBEDDINGS: A Novel Set of High-Performance Sentence
Embedding Models
Michael Günther and Louis Milliken and Jonathan Geuter
Georgios Mastrapas and Bo Wang and Han Xiao
Jina AI
Ohlauer Str. 43, 10999 Berlin, Germany
{michael.guenther,louis.milliken,jonathan.geuter,
georgios.mastrapas,bo.wang,han.xiao}@jina.ai
Abstract
JINA EMBEDDINGS constitutes a set of high-
performance sentence embedding models adept
at translating various textual inputs into numer-
ical representations, thereby capturing the se-
mantic essence of the text. While these models
are not exclusively designed for text genera-
tion, they excel in applications such as dense
retrieval and semantic textual similarity. This
paper details the development of JINA EMBED-
DINGS, starting with the creation of a high-
quality pairwise and triplet dataset. It under-
lines the crucial role of data cleaning in dataset
preparation, gives in-depth insights into the
model training process, and concludes with a
comprehensive performance evaluation using
the Massive Textual Embedding Benchmark
(MTEB).
1
Introduction
Sentence embedding models serve as an effec-
tive instrument for encoding semantic nuances
of words, phrases, and larger textual units into a
continuous vector space. They encapsulate inher-
ent contextual complexities and interrelationships
within the text, thereby facilitating numerous down-
stream tasks such as information retrieval, semantic
similarity evaluations, and text classification.
Despite the potential of these models, lingering
questions remain about the effectiveness of differ-
ent data preprocessing strategies, the optimal loss
function for training sentence embedding models,
and the impact of increasing parameters on per-
formance. This paper addresses these challenges
and makes substantial contributions in the field of
sentence embeddings.
We introduce a novel dataset developed specifi-
cally for training our sentence embedding models.
Additionally, we present JINA EMBEDDINGS, a set
of high-performance sentence embedding models
trained on our dataset. The JINA EMBEDDINGS
set comprises five distinct models, ranging in size
from 35 million to 6 billion parameters 1.
The models in the JINA EMBEDDINGS set em-
ploy contrastive fine-tuning on the T5 architec-
ture [Raffel et al., 2020]. It’s important to note
that we opted to use the T5 model as our base due
to its pre-training on a mixed set of downstream
tasks. We argue that incorporating this approach
can potentially enhance our capacity for accurately
gauging the efficacy of our training strategy.
Our large-scale contrastive fine-tuning approach
surpasses zero-shot T5 and delivers a performance
level on par with other leading T5-based sentence
embedding models such as Sentence-T5 [Ni et al.,
2022a] and GTR [Ni et al., 2022b]. Consequently,
this paper marks significant strides in the realm of
sentence embedding models, demonstrating that
high-quality sentence embeddings can be achieved
with a judicious use of resources and innovative
training methodologies.
2
Dataset Preparation
In order to develop models that excel across a wide
range of tasks, we collated a comprehensive set of
both public and bespoke datasets. These datasets
encompass various retrieval objectives, such as
e-commerce search, duplicate detection, web re-
trieval, article retrieval for question-answering, and
text classification. Consolidating these datasets
into a unified format facilitates concurrent model
training across all tasks.
Given the lack of non-relevance information in
many of the datasets, we reformatted each training
item into pairs, designated as (q, p) ∈Dpairs. Each
pair includes a query string q and an associated
target string p. To leverage explicit non-relevance
judgements, we created an auxiliary set of triplets
(q, p, n) ∈Dtriplets, which pair a query string q
with a match p (positive) and a non-matching string
n (negative).
1jina-small-v1 , jina-base-v1 , jina-large-v1 are
available at https://huggingface.co/jinaai.
arXiv:2307.11224v1  [cs.CL]  20 Jul 2023

Our training process is a two-step approach. Ini-
tially, we train on pairs and subsequently fine-tune
the model using the triplets, as detailed in Section
3.3.
2.1
Pairwise Data Preparation
The substantial size and inconsistent quality of nu-
merous datasets necessitates a rigorous filtering
pipeline. We apply the following sequence of steps
for dataset refinement:
De-Duplication: Duplicated entries within train-
ing data can negatively impact language model per-
formance [Hernandez et al., 2022], and potentially
lead to overfitting. Consequently, we expunge du-
plicate entries from our dataset. Considering the
dataset’s volume, we employ hash values to iden-
tify and eliminate duplicates. De-duplication is
performed considering whitespace and capitaliza-
tion. Pairs with identical elements, or those that are
empty, are also removed.
Language Filtering:
As our embedding
models
are
designed
for
English,
we
uti-
lized the fasttext-language-identification
model2 based on the fasttext text classification
method [Joulin et al., 2017] to isolate and discard
non-English training items from the dataset.
Consistency Filtering: Consistency filtering is
predicated on the exclusion of pairs exhibiting low
semantic similarity. Previous studies suggest that
eliminating low-similarity pairs using an auxiliary,
albeit less precise, model boosts performance [Dai
et al., 2023, Wang et al., 2022]. We employ the
all-MiniLM-L6-v2 model3 for consistency filter-
ing in this manner: We generate embeddings for
1M pairs (qi, pi)i randomly selected from the com-
plete dataset. For every pair (q, p) in the dataset,
we verify whether p is among the top two passages
most similar to q based on the cosine similarity
of their embeddings compared to all passages pi,
i = 1, ..., 1M.
The application of these preprocessing steps re-
sulted in a reduction of the dataset from over 1.5
billion mixed-quality pairs to 385 million high-
quality pairs. This reduction permits us to train
our model with significantly less data than typical
embedding models, without sacrificing embedding
quality.4
2fasttext-language-identification
(https:
//huggingface.co/facebook/fasttext-language-identification)
3all-MiniLM-L6-v2
model
(https://huggingface.co/
sentence-transformers/all-MiniLM-L6-v2)
4For instance, models like all-MiniLM-L6-v2 and all-
mpnet-base-v2 are trained on nearly 1.2 billion pairs, whereas
2.2
Triplet Data Preparation
For the triplet dataset, we forego de-duplication
and language filtering, as we assume the qual-
ity of these datasets already meets our require-
ments in these aspects.
However, akin to con-
sistency filtering, we validate the relevance of
the “positive” with respect to the query for each
triplet. Instead of contrasting the similarity s(q, p)
against a substantial sample set, we compare it
solely with the similarity (q, n) of the same pair.
This is accomplished using a cross-encoder model,
which evaluates the pair directly without gener-
ating embedding representations. More specifi-
cally, we leverage the ms-marco-MiniLM-L-6-v2
model5 to verify whether the difference in retrieval
scores determined by the model exceeds a threshold
r(q, p) −r(q, n) > κ, with threshold κ = 0.2, and
eliminate all other pairs. This methodology draws
inspiration from the de-noising strategy proposed
in [Qu et al., 2021].
2.3
Negation Data Preparation
We observed many embedding models struggle to
accurately embed negations. For instance, when
embedding the three sentences: “A couple walks
hand in hand down a street.”, “A couple is walk-
ing together.”, “A couple is not walking together.”,
the first two should be embedded closely, while
the second and third, contradictory in meaning,
should be positioned further apart.6 However, for
instance, the all-MiniLM-L6-v2 model assigns a
cosine similarity of 0.7 to the first two sentences,
while attributing a similarity of 0.86 to the second
and third.7
Thus, we decided to create our own negation
dataset 8. This dataset, based on positive pairs from
the SNLI dataset, comprises triplets (anchor, entail-
ment, negative) akin to the example given above,
where (anchor, entailment) form a positive pair, and
the “negative” contradicts both the “anchor” and
other T5-based models such as sentence-t5-base or sentence-
t5-large are trained on 2.2 billion pairs.
5ms-marco-MiniLM-L-6-v2
(https://huggingface.co/
cross-encoder/ms-marco-MiniLM-L-6-v2)
6Although it could be argued that for certain tasks, like
document retrieval, it might still be desirable for contradicting
texts to be embedded closely. Regardless, in this example, the
first two sentences should be assigned a higher similarity.
7Interestingly, our large model does correctly assign a co-
sine similarity of 0.77 to the positive pair, and only a similarity
of 0.62 to the negative pair, post fine-tuning on the negation
dataset.
8The
negation
dataset
is
available
at
https://
huggingface.co/datasets/jinaai/negation-dataset

(a) Original Distribution after Filtering
(b) Adjusted by Sampling Rates
Figure 1: The composition of 385 million pairwise data
Figure 2: The composition of 927,000 triplets data
“entailment”, while remaining syntactically very
similar to “entailment”. This dataset forms a subset
of our aforementioned triplet dataset, with training
details provided in Section 3.3.
Our model evaluation on the negation dataset,
which includes a comparative analysis with other
popular open-source models, is presented in Sec-
tion 4.3.
2.4
Data Composition
Our dataset for text pairs, represented as Dpairs =
D1 ⊔· · · ⊔Dn, is aggregated from 32 individual
datasets. This amasses to a total of 1.6 billion pairs
before filtering, which is subsequently reduced to a
robust 385 million pairs after rigorous filtering.
In comparison, our dataset for triplets initially
comprises a total of 1.13 million entries before
filtering. After the filtering process, we are left
with a streamlined count of 927,000 triplets.
Post-filtering, the composition of our datasets is
visually illustrated in Figure 1a for the text pairs,
and in Figure 2 for the triplets. Together, these
form the final dataset used in the training of the
JINA EMBEDDINGS model.
3
Training
The training regimen is organized into two distinct
phases. The first phase centers on training the
model using a voluminous quantity of text pairs, fa-
cilitating the consolidation of the semantic meaning
of an entire text phrase into a single representative
embedding. The second phase engages the model
with a relatively smaller set of triplets, comprising
an anchor, match, and hard-negative, teaching it
to differentiate between similar and dissimilar text
phrases.
3.1
Training on Pairwise Data
Each model within the JINA EMBEDDINGS set is
based on, and trained using, the zero-shot T5 mod-
els of corresponding size, as detailed in [Raffel
et al., 2020]. The zero-shot T5 models are com-
posed of encoder-decoder pairs. However, it has
been demonstrated that calculating text embed-
dings is more efficient using solely the encoder
component of the T5 models, as opposed to de-
ploying both components [Ni et al., 2022a]. Con-
sequently, the JINA EMBEDDINGS models utilize
only the encoders of their respective T5 models.
During tokenization, both the T5 models and
JINA EMBEDDINGS models employ Sentence-
Piece [Kudo and Richardson, 2018] to segment
input text and encode them into WordPiece to-
kens [Kudo, 2018]. Following the encoder model,
a mean pooling layer is implemented to generate
fixed-length representations from the token embed-
dings.
For the training process involving pairs, we em-
ploy InfoNCE [van den Oord et al., 2018], a con-
trastive loss function. This function calculates the
loss for a pair (q, p) ∼B within a batch B ∈Dk
of text pairs, where the batch size is k, as follows:

LNCE = E(q,p)∼B
"
−log
exp(s(q, p)/τ)
Pk
i=1 exp(s(q, pi)/τ)
#
The loss is calculated by comparing the cosine sim-
ilarity between a given question q and its target p,
with the similarity to all other targets in the batch.
We found that calculating the loss in both direc-
tions results in greater improvements during train-
ing. Accordingly, the loss is defined as follows:
L := LNCE + LNCE
LNCE := E(q,p)∼B
"
−log
exp(s(p, q)/τ)
Pk
i=1 exp(s(p, qi)/τ)
#
Intuitively, LNCE matches the target string to all
query strings instead. The constant τ denotes a
temperature parameter which we set to τ = 0.05.
This method of calculating the loss is based on a
similar method in [Neelakantan et al., 2022].
3.2
Data Sampling in Pairwise Training
Rather than sequentially training on individual
datasets, we opted for a parallel approach, train-
ing on all datasets concurrently. We postulate that
this parallel training promotes enhanced model gen-
eralization across diverse tasks. Despite this, each
training batch is exclusively composed of data from
a single dataset. This ensures that loss calculations,
performed across the entire batch, do not conflate
data from different tasks.
Our dataloader operates by initially selecting a
dataset, followed by sampling the requisite number
of data points from it to constitute a batch for the
worker (refer to Section 4). Prior to training, the
pairs within the datasets are thoroughly shuffled.
Sampling a dataset Di follows a probability
distribution ρ across all datasets Di. The prob-
ability of sampling Di, represented as ρ (Di) =
|Di|si
Pn
j=1 |Dj|sj , is contingent upon the dataset’s size
|Di| and a scaling factor si.
Given the disparity in dataset sizes, it is critical
to frequently sample from larger datasets to circum-
vent overfitting on the smaller ones. Furthermore,
we manipulate the sampling rates of datasets using
scaling factors to prioritize training on high-quality
datasets and achieve balance among text domains.
In scenarios where datasets with higher sampling
rates deplete their items before the completion of
a training epoch, the dataset is reset, enabling the
model to cycle through its items anew. This en-
sures that high-rate datasets contribute multiple
times within a single training epoch.
Figure 1b visualizes the proportion of each
dataset based on their sampling rates. Following
the creation of this adjusted distribution, the fre-
quency of sampling from larger datasets signifi-
cantly diminishes, resulting in only 180 million
pairs being sampled during training.
3.3
Training on Triplet Data
Following the completion of pairwise training, the
model progresses to the next phase which involves
training on the triplet datasets. This phase differs
significantly in its loss function utilization, leverag-
ing negatives for improved model performance.
We experimented with various triplet loss func-
tions and found that the optimal results are achieved
through a combination of multiple commonly used
triplet loss functions. Specifically, we utilize the
extended version of the InfoNCE loss LNCE+,
given by (2), which employs additional nega-
tives [Reimers, 2023], the reverse InfoNCE loss
LNCE from the initial training phase as given by
(3), and the triplet margin loss function L3 as pre-
sented in (4) [Chechik et al., 2010].
The triplet function L3 determines the cosine
similarity difference between the query and target
s(q, n), and the query and negative match s(q, n).
Furthermore, it establishes a minimal margin ε =
0.05 between these two values. If the negative is
more similar to the query or the margin is violated,
L3 returns a positive value. Otherwise, it yields
0, which is achieved through the application of
the ReLU activation function. For the temperature
parameter, we opted for a value of τ = 0.05.
4
Evaluation
We incorporated callbacks in our training loop to
track and document the training progression. The
model undergoes evaluation against the MTEB
benchmark every 10,000 batches, where the MTEB
consists of the STS12, STS13, STS14, STS15,
STS16, STS17, SciFact, TRECCOVID, and Quo-
raRetrieval tasks in English. Additionally, we gen-
erate a checkpoint every 20,000 batches that stores
the states of the model and optimizer, along with
the progress status of each data source.
For training, we deployed A100 GPUs and lever-
aged the DeepSpeed stage 2 distributed training
strategy [Rajbhandari et al., 2020] for effective

L := LNCE+ + LNCE + L3
(1)
LNCE+ := E(q,p,n)∼B
"
−log
exp(s(q, p)/τ)
Pk
i=1 exp(s(q, pi)/τ) + exp(s(q, ni)/τ)
#
(2)
LNCE := E(q,p,n)∼B
"
−log
exp(s(p, q)/τ)
Pk
i=1 exp(s(p, qi)/τ)
#
(3)
L3 := E(q,p,n)∼B
"
ReLU

s(q, n) −s(q, p) + ε
#
(4)
Hyperparameters
Value
# of devices
8
Sequence length
512
Model precision
32 bit
Learning rate
0.00005
# of steps for learning rate warm-up
500
Batch size for jina-small-v1
4096
Batch size for jina-base-v1
2048
Batch size for jina-large-v1
1024
Table 1: Hyperparameters
multi-device management. Our models use the
AdamW optimizer, coupled with a learning rate
scheduler that adjusts the learning rate during the
initial stages of training. The hyperparameters used
across all three models throughout the training pro-
cess are detailed in Table 1.
4.1
Performance Against State-of-the-Art
Models
To gauge the performance of the JINA EMBED-
DINGS set in relation to other similarly sized mod-
els, we selected representative models from four
distinct size categories, as depicted in Table 2. Ad-
ditionally, we included sentence-t5 and gtr-t5 xl
and xxl models, which are based on T5 models
with 3 billion and 11 billion parameters, respec-
tively. This inclusion was to investigate the per-
formance variation with models of such massive
scales.
Table 6 presents the scores for sentence simi-
larity tasks, wherein the models within the JINA
EMBEDDINGS set outshine their similarly sized
counterparts across numerous tasks.
Notably,
the jina-large-v1 model consistently delivers
comparable, if not superior, results to models in
the billion-parameter scale. jina-base-v1 and
jina-small-v1 also exhibit competitive perfor-
mances with models of analogous sizes, exceeding
their peers on the BIOSSES task. This highlights
the benefit of training across a more diverse set of
domains.
jina-base-v1 consistently demonstrates perfor-
mances akin to or better than gtr-t5-base, which
was trained specifically for retrieval tasks [Ni et al.,
2022b]. However, it seldom matches the scores of
sentence-t5-base, which was trained on sentence
similarity tasks [Ni et al., 2022a].
The evaluation of model performances on re-
trieval tasks, presented in Table 8, reflects a similar
relationship among gtr-t5, sentence-t5, and JINA
EMBEDDINGS. Here, gtr-t5 models, which have
been specially trained on retrieval tasks, consis-
tently score the highest for their respective sizes.
JINA EMBEDDINGS models follow closely behind,
whereas sentence-t5 models trail significantly. The
JINA EMBEDDINGS set’s capability to maintain
competitive scores across these tasks underscores
the advantage of multi-task training.
As illustrated in Table 7, jina-large-v1 also
achieves exceedingly high scores on reranking
tasks, often outperforming larger models. Similarly,
jina-base-v1 surpasses gtr-t5-large and sentence-
t5-large on several reranking tasks, which could
once again be attributed to the specific training
tasks of sentence-t5 and gtr-t5.
4.2
Impact of Filtering Steps
We evaluate the effectiveness of our dataset prepro-
cessing pipeline by performing an ablation study.
In this study, our smallest model is fine-tuned on
the Reddit dataset, where various preprocessing
steps are individually applied. The corresponding
results are presented in Table 4.
The ablation study’s results underscore the value
of both language and consistency filtering as cru-

Model
Parameters
Embedding
Dimensions
sentence-t5-xxl
4.9b
768
gtr-t5-xxl
4.9b
768
gtr-t5-xl
1.2b
768
sentence-t5-xl
1.2b
768
jina-large-v1
330m
1024
gtr-t5-large
330m
768
sentence-t5-large
330m
768
all-mpnet-base-v2
110m
768
jina-base-v1
110m
768
gtr-t5-base
110m
768
sentence-t5-base
110m
768
jina-small-v1
35m
512
all-MiniLM-L6-v2
23m
384
Table 2: Model sizes and output dimensions
cial preprocessing steps. Their combined applica-
tion results in the highest performance across the
majority of benchmarks.
Specifically for the Reddit dataset, we observe a
significant performance boost with the application
of consistency filtering, while language filtering
only marginally enhances the performance. This
disparity can be accounted for by the fact that only
17.4% of the Reddit data is filtered through the lan-
guage filter, whereas consistency filtering screens
out a considerable 84.3%.9 Thus, while most Red-
dit samples are primarily in English, many consist
of positive pairs with very low similarity, making
consistency filtering more effective than language
filtering.
The effectiveness of these preprocessing steps,
however, does exhibit variability across different
datasets.
4.3
Effectiveness of Negation Data
To determine the effectiveness of our models on
negation data, we evaluate them against the test
split of our negation dataset, comparing the re-
sults with other open source models. The evalua-
tion metric employed is the percentage of samples
where the model positions the anchor and entail-
ment closer than the anchor and negative. The
outcomes of these evaluations are displayed in Ta-
9It is pertinent to note that the subsets filtered out overlap,
thus the combined application of language and consistency
filtering filters out only 86.8% of the data.
Model
Reranking
Retrieval
STS
sentence-t5-xxl
56.42
42.24
82.63
gtr-t5-xxl
56.66
48.48
78.38
gtr-t5-xl
55.96
47.96
77.80
sentence-t5-xl
54.71
38.47
81.66
jina-large-v1
56.94
44.04
80.85
gtr-t5-large
55.36
47.42
78.19
sentence-t5-large
54.00
36.71
81.83
all-mpnet-base-v2
59.36
43.81
80.28
jina-base-v1
56.04
41.79
78.65
gtr-t5-base
54.23
44.67
77.07
sentence-t5-base
53.09
33.63
81.14
jina-small-v1
53.07
38.91
78.06
all-MiniLM-L6-v2
58.04
41.95
78.90
Table 3: Average Scores for Reranking, Retrieval and
STS tasks
ble 5. We assessed our models both before and after
fine-tuning on the triplet data, denoted as <model
size>pairwise and <model size>all, respectively.
From the results, we observe that across all
model sizes, fine-tuning on triplet data (which in-
cludes our negation training dataset) enhances per-
formance. Although our models do not quite reach
the performance level of other open source mod-
els of similar sizes, it’s important to note that the
performance gap is not significantly wide. Interest-
ingly, our models achieve this with only a fraction
of the training data required by their counterparts.
5
Related Work
The field of embedding models has seen signif-
icant advancements over the years with the de-
velopment of various models featuring diverse ar-
chitectures and training pipelines. For instance,
Sentence-BERT [Reimers and Gurevych, 2019]
utilizes BERT to generate sentence embeddings.
Similarly, sentence-T5 [Ni et al., 2022a], based
on the encoder architecture of T5, demonstrates
superior performance over Sentence-BERT on nu-
merous benchmarks. The study underscores the
effectiveness of encoders for sentence embeddings,
contrasting with another approach that explores the
use of decoders [Muennighoff, 2022].
Knowledge distillation [Hinton et al., 2015] of-
fers a distinct approach to model training. In this
setup, a larger, pre-trained model acts as a mentor,
instructing a smaller model during training. This

Retrieval
Data Preparation
Quora
SciFact
Trec-Cov
No Extra Filter
0.734
0.218
0.242
Language
0.741
0.218
0.250
Consistency
0.805
0.381
0.297
Language + Consistency
0.806
0.379
0.306
Sentence Similarity
Data Preparation
STS12
STS13
STS14
STS15
STS16
STS17
STS22
No Extra Filter
0.558
0.668
0.573
0.694
0.706
0.764
0.606
Language
0.561
0.668
0.579
0.697
0.704
0.765
0.609
Consistency
0.652
0.728
0.652
0.760
0.755
0.808
0.610
Language + Consistency
0.653
0.727
0.656
0.764
0.757
0.810
0.609
Table 4: Evaluation of Data-Preparation Effectiveness on the Reddit Dataset. Retrieval evaluated on nDCG@10,
Sentence Similarity on Spearman.
Negation @500
Parameters
Training samples
jina-small-v1pairwise
88.4%
35m
385m
jina-base-v1pairwise
93.0%
110m
385m
jina-large-v1pairwise
94.6%
330m
385m
jina-small-v1all
93.2%
35m
386m
jina-base-v1all
95.6%
110m
386m
jina-large-v1all
n/a
330m
386m
all-MiniLM-L6-v2
94.8%
23m
1170m
all-mpnet-base-v2
97.4%
110m
1170m
sentence-t5-base
96.0%
110m
2275m
sentence-t5-large
98.2%
330m
2275m
Table 5: Evaluating a Range of Models on the Negation Dataset: A Benchmark Analysis of JINA EMBEDDINGS
Trained on Both Pairwise-Only and Combined Pairwise and Triplet Data. The negation dataset is available at
https://huggingface.co/datasets/jinaai/negation-dataset
methodology can be seamlessly integrated with a
contrastive loss function, presenting an avenue for
future investigation.
Embedding models can also be characterized
based on their functionality. For instance, while
some models are designed to solely embed queries,
others are trained to embed queries along with spe-
cific instructions, generating task-dependent em-
beddings [Su et al., 2023]. An example of a T5-
based model is the large dual encoder [Ni et al.,
2022b], which is fine-tuned for retrieval tasks and
computes a retrieval score directly.
The benefits of contrastive pre-training cou-
pled with fine-tuning on hard negatives have been
emphasized in recent studies [Neelakantan et al.,
2022, Wang et al., 2022]. Both approaches have
achieved state-of-the-art results on multiple bench-
marks, with [Wang et al., 2022] also employing
consistency filtering as part of their preprocessing
pipeline.
As part of our evaluation process, we utilized
two widely-accepted benchmarks for embedding
models: MTEB [Muennighoff et al., 2023] and
BEIR [Thakur et al., 2021].
6
Conclusion
This paper has introduced the JINA EMBEDDINGS
set of embedding models, demonstrating that com-
petitive results on various tasks can be achieved
while substantially reducing the volume of training
data compared to other models with comparable
backbones. Through an extensive evaluation on the
MTEB benchmark, we have shown that employ-
ing judicious data filtering techniques can lead to

enhanced performance in comparison to training
with a larger, yet lower-quality dataset. These find-
ings significantly shift the paradigm, indicating that
training large language models for embedding tasks
can be conducted with less data than previously as-
sumed, leading to potential savings in training time
and resources.
However, we acknowledge the limitations of the
current methodologies and the performance of the
JINA EMBEDDINGS set. During the training on
pairs, the sampling rate selection was based on
a heuristic approach. Given the vast size of the
search space for these sampling rates, we leaned
on our intuition and dataset familiarity to prioritize
higher-value datasets over their lower-value coun-
terparts. This subjective approach, however, points
to the need for more objective methods for future
advancements.
Additionally, the JINA EMBEDDINGS set fell
short on some tasks.
For instance, calculating
sentence similarity on our negation dataset (as de-
scribed in Section 4.3) didn’t meet our expectations,
nor did it achieve competitive scores for classifica-
tion and clustering tasks on the MTEB benchmark.
These performance shortcomings suggest a possi-
ble deficit in the representation of these types of
tasks in our training data, necessitating further in-
vestigation.
Looking ahead, we aim to refine our training
processes to deliver models with improved perfor-
mance and greater sequence length. Our future
endeavors also include generating bilingual train-
ing data and training an embedding model capable
of understanding and translating between two lan-
guages, thereby expanding the utility and versatility
of the JINA EMBEDDINGS set.
References
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of trans-
fer learning with a unified text-to-text transformer.
The Journal of Machine Learning Research, 21(1):
5485–5551, 2020.
Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-
stant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang.
Sentence-t5: Scalable sentence encoders from pre-
trained text-to-text models. In Findings of the As-
sociation for Computational Linguistics: ACL 2022,
pages 1864–1874, 2022a.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-
nandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith
Hall, Ming-Wei Chang, et al. Large dual encoders
are generalizable retrievers. In Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing, pages 9844–9855, 2022b.
Danny Hernandez, Tom Brown, Tom Conerly, Nova
DasSarma, Dawn Drain, Sheer El-Showk, Nelson
Elhage, Zac Hatfield-Dodds, Tom Henighan, Tris-
tan Hume, et al.
Scaling laws and interpretabil-
ity of learning from repeated data. arXiv preprint
arXiv:2205.10487, 2022.
Armand Joulin, Édouard Grave, Piotr Bojanowski, and
Tomáš Mikolov. Bag of tricks for efficient text clas-
sification. In Proceedings of the 15th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Volume 2, Short Papers, pages
427–431, 2017.
Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith
Hall, and Ming-Wei Chang. Promptagator: Few-shot
dense retrieval from 8 examples. In The Eleventh In-
ternational Conference on Learning Representations,
2023. URL https://openreview.net/forum?id=
gmL46YMpu2J.
Liang Wang, Nan Yang, Xiaolong Huang, Binx-
ing Jiao, Linjun Yang, Daxin Jiang, Rangan Ma-
jumder, and Furu Wei. Text embeddings by weakly-
supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533, 2022.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and
Haifeng Wang. Rocketqa: An optimized training
approach to dense passage retrieval for open-domain
question answering. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 5835–5847, 2021.
Taku Kudo and John Richardson. Sentencepiece: A sim-
ple and language independent subword tokenizer and
detokenizer for neural text processing. In Proceed-
ings of the 2018 Conference on Empirical Methods
in Natural Language Processing: System Demonstra-
tions, pages 66–71, 2018.
Taku Kudo. Subword regularization: Improving neural
network translation models with multiple subword
candidates. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 66–75, 2018.
Aäron van den Oord, Yazhe Li, and Oriol Vinyals.
Representation learning with contrastive predictive
coding. CoRR, abs/1807.03748, 2018. URL http:
//arxiv.org/abs/1807.03748.
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-
ford, Jesse Michael Han, Jerry Tworek, Qiming
Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy,
Johannes Heidecke, Pranav Shyam, Boris Power,
Tyna Eloundou Nekoul, Girish Sastry, Gretchen

Krueger, David Schnurr, Felipe Petroski Such,
Kenny Hsu, Madeleine Thompson, Tabarak Khan,
Toki Sherbakov andcalculating Joanne Jang, Pe-
ter Welinder, and Lilian Weng.
Text and code
embeddings by contrastive pre-training.
CoRR,
abs/2201.10005, 2022. URL https://arxiv.org/
abs/2201.10005.
Nils Reimers. http://plastimatch.org/doxygen/
classHausdorff__distance.html##details
Last Access: 14th July 2023, 2023.
Gal Chechik, Varun Sharma, Uri Shalit, and Samy Ben-
gio. Large scale online learning of image similarity
through ranking. Journal of Machine Learning Re-
search, 11(3), 2010.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and
Yuxiong He. Zero: Memory optimizations toward
training trillion parameter models. In SC20: Interna-
tional Conference for High Performance Computing,
Networking, Storage and Analysis, pages 1–16. IEEE,
2020.
Nils Reimers and Iryna Gurevych. Sentence-bert: Sen-
tence embeddings using siamese bert-networks. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP), pages 3982–
3992, 2019.
Niklas Muennighoff. Sgpt: Gpt sentence embeddings
for semantic search, 2022.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-
ing the knowledge in a neural network, 2015.
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. One embed-
der, any task: Instruction-finetuned text embeddings,
2023.
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and
Nils Reimers. Mteb: Massive text embedding bench-
mark, 2023.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych.
Beir: A
heterogenous benchmark for zero-shot evaluation of
information retrieval models, 2021.

Appendix
Model
BIOSSES
SICK-R
STS12
STS13
STS14
STS15
STS16
STS17
STS22
STS
Benchmark
sentence-t5-xxl
80.43
80.47
78.85
88.94
84.86
89.32
84.67
89.46
65.33
84.01
sentence-t5-xl
73.12
79.98
79.02
88.8
84.33
88.89
85.31
88.91
64.32
83.93
gtr-t5-xxl
81.91
74.29
70.12
82.72
78.24
86.26
81.61
85.18
65.76
77.73
gtr-t5-xl
78.94
73.63
69.11
81.82
77.07
86.01
82.23
84.9
66.61
77.65
sentence-t5-large
78.93
80.34
79.11
87.33
83.17
88.28
84.36
88.99
62.39
85.36
gtr-t5-large
84.86
73.39
70.33
82.19
77.16
86.31
81.85
83.93
64.30
77.60
jina-large-v1
84.31
80.31
73.85
84.37
77.84
86.29
82.14
89.57
66.18
83.64
sentence-t5-base
75.89
80.18
78.05
85.85
82.19
87.46
84.03
89.57
62.66
85.52
gtr-t5-base
79.00
71.45
68.59
79.09
74.64
84.85
81.57
85.8
66.17
79.58
all-mpnet-base-v2
80.43
80.59
72.63
83.48
78.00
85.66
80.03
90.60
67.95
83.42
jina-base-v1
82.64
75.04
73.51
79.15
75.24
85.12
80.07
87.96
66.08
81.69
all-MiniLM-L6-v2
81.64
77.58
72.37
80.60
75.59
85.39
78.99
87.59
67.21
82.03
jina-small-v1
82.96
76.33
74.28
78.55
73.84
83.71
80.03
87.49
64.25
79.20
Table 6: Spearman Correlation for Sentence Similarity Tasks
Model
AskUbuntuDupQuestions
MindSmallReranking
SciDocsRR
StackOverflowDupQuestions
sentence-t5-xxl
66.16
30.60
76.09
52.85
sentence-t5-xl
62.86
29.77
75.16
51.05
gtr-t5-xxl
63.23
31.93
77.96
53.50
gtr-t5-xl
63.08
31.50
76.49
52.79
sentence-t5-large
61.51
30.27
74.88
49.34
gtr-t5-large
61.64
31.84
76.39
51.58
jina-large-v1
63.47
31.64
81.63
51.02
sentence-t5-base
59.73
30.20
73.96
48.46
gtr-t5-base
60.86
31.33
73.71
51.01
all-mpnet-base-v2
65.85
30.97
88.65
51.98
jina-base-v1
62.72
31.01
79.54
50.87
all-MiniLM-L6-v2
63.48
30.80
87.12
50.76
jina-small-v1
60.25
30.68
74.16
47.18
Table 7: Mean Average Precision (mAP@10) for Reranking Tasks

Model
FEVER
HotpotQA
MSMARCO
NQ
Quora
Retrieval
SciFact
TREC
COVID
Argu
Ana
Climate
FEVER
DBPedia
FiQA
2018
NFCorpus
SCIDOCS
Touche
2020
sentence-t5-xxl
51.20
42.14
27.67
52.87
85.96
55.38
59.48
39.85
14.63
39.19
46.68
35.08
17.17
21.65
sentence-t5-xl
36.12
37.17
25.17
46.29
85.85
50.91
54.77
39.40
10.61
33.65
44.71
33.18
15.97
22.51
gtr-t5-xxl
74.08
59.67
44.05
57.24
89.09
66.77
51.90
53.77
27.21
41.28
46.78
34.18
15.88
26.76
gtr-t5-xl
72.18
58.91
43.52
56.16
88.91
64.2
60.09
52.81
27.01
39.74
44.19
33.34
15.71
25.26
sentence-t5-large
36.21
33.95
23.96
42.02
85.73
49.91
46.11
39.27
11.36
31.55
43.55
31.10
15.38
21.63
gtr-t5-large
72.66
57.85
42.73
55.09
88.47
63.42
56.68
52.09
26.90
39.55
42.79
32.63
15.51
28.29
jina-large-v1
75.34
53.63
35.80
47.90
88.21
60.84
56.58
49.48
21.36
36.03
39.90
33.29
19.39
17.34
sentence-t5-base
26.17
33.20
20.70
36.32
85.49
45.76
40.70
44.85
10.37
27.77
34.83
28.65
14.15
20.30
gtr-t5-base
68.93
54.93
41.16
50.47
87.98
59.74
56.05
50.83
24.88
35.24
35.15
30.22
14.00
25.89
all-mpnet-base-v2
50.86
39.29
39.75
50.45
87.46
65.57
51.33
46.52
21.97
32.09
49.96
33.29
23.76
19.93
jina-base-v1
67.59
49.64
33.02
43.61
87.07
58.60
54.63
49.35
20.78
31.35
35.48
29.30
17.71
18.06
all-MiniLM-L6-v2
51.93
46.51
36.54
43.87
87.56
64.51
47.25
50.17
20.27
32.33
36.87
31.59
21.64
16.90
jina-small-v1
69.12
47.48
31.80
38.89
85.69
52.40
52.30
43.57
17.25
28.28
25.19
25.96
15.29
16.67
Table 8: Normalized Discounted Cumulative Gain (nDCG@10) for retrieval tasks

