

A First Course in Probability
and Markov Chains


A First Course in Probability
and Markov Chains
Giuseppe Modica
and
Laura Poggiolini
University of Firenze, Italy
A John Wiley & Sons, Ltd., Publication

This edition ﬁrst published 2013
© 2013 John Wiley & Sons, Ltd
Registered ofﬁce
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, United Kingdom
For details of our global editorial ofﬁces, for customer services and for information about how to apply for
permission to reuse the copyright material in this book please see our website at www.wiley.com.
The right of the author to be identiﬁed as the author of this work has been asserted in accordance with the
Copyright, Designs and Patents Act 1988.
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted,
in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as
permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not
be available in electronic books.
Designations used by companies to distinguish their products are often claimed as trademarks. All brand
names and product names used in this book are trade names, service marks, trademarks or registered
trademarks of their respective owners. The publisher is not associated with any product or vendor mentioned
in this book. This publication is designed to provide accurate and authoritative information in regard to the
subject matter covered. It is sold on the understanding that the publisher is not engaged in rendering
professional services. If professional advice or other expert assistance is required, the services of a competent
professional should be sought.
Library of Congress Cataloging-in-Publication Data
Modica, Giuseppe.
A ﬁrst course in probability and Markov chains / Giuseppe Modica and Laura Poggiolini.
pages cm
Summary: “A ﬁrst course in Probability and Markov Chains presents an introduction to the basic
elements in statistics and focuses in two main areas” – Provided by publisher.
Includes bibliographical references and index.
ISBN 978-1-119-94487-4 (hardback)
1. Markov processes. I. Poggiolini, Laura. II. Title.
QA274.7.M63 2013
519.2′33 – dc23
2012033463
A catalogue record for this book is available from the British Library.
ISBN: 978-1-119-94487-4
Set in 9.5/12pt Times by Laserwords Private Limited, Chennai, India.

Contents
Preface
xi
1
Combinatorics
1
1.1
Binomial coefﬁcients
1
1.1.1
Pascal triangle
1
1.1.2
Some properties of binomial coefﬁcients
2
1.1.3
Generalized binomial coefﬁcients and binomial series
3
1.1.4
Inversion formulas
4
1.1.5
Exercises
6
1.2
Sets, permutations and functions
8
1.2.1
Sets
8
1.2.2
Permutations
8
1.2.3
Multisets
10
1.2.4
Lists and functions
11
1.2.5
Injective functions
12
1.2.6
Monotone increasing functions
12
1.2.7
Monotone nondecreasing functions
13
1.2.8
Surjective functions
14
1.2.9
Exercises
16
1.3
Drawings
16
1.3.1
Ordered drawings
16
1.3.2
Simple drawings
17
1.3.3
Multiplicative property of drawings
17
1.3.4
Exercises
18
1.4
Grouping
19
1.4.1
Collocations of pairwise different objects
19
1.4.2
Collocations of identical objects
22
1.4.3
Multiplicative property
23
1.4.4
Collocations in statistical physics
24
1.4.5
Exercises
24
2
Probability measures
27
2.1
Elementary probability
28
2.1.1
Exercises
29

vi
CONTENTS
2.2
Basic facts
33
2.2.1
Events
34
2.2.2
Probability measures
36
2.2.3
Continuity of measures
37
2.2.4
Integral with respect to a measure
39
2.2.5
Probabilities on ﬁnite and denumerable sets
40
2.2.6
Probabilities on denumerable sets
42
2.2.7
Probabilities on uncountable sets
44
2.2.8
Exercises
46
2.3
Conditional probability
51
2.3.1
Deﬁnition
51
2.3.2
Bayes formula
52
2.3.3
Exercises
54
2.4
Inclusion–exclusion principle
60
2.4.1
Exercises
63
3
Random variables
68
3.1
Random variables
68
3.1.1
Deﬁnitions
69
3.1.2
Expected value
75
3.1.3
Functions of random variables
77
3.1.4
Cavalieri formula
80
3.1.5
Variance
82
3.1.6
Markov and Chebyshev inequalities
82
3.1.7
Variational characterization of the median and of the
expected value
83
3.1.8
Exercises
84
3.2
A few discrete distributions
91
3.2.1
Bernoulli distribution
91
3.2.2
Binomial distribution
91
3.2.3
Hypergeometric distribution
93
3.2.4
Negative binomial distribution
94
3.2.5
Poisson distribution
95
3.2.6
Geometric distribution
98
3.2.7
Exercises
101
3.3
Some absolutely continuous distributions
102
3.3.1
Uniform distribution
102
3.3.2
Normal distribution
104
3.3.3
Exponential distribution
106
3.3.4
Gamma distributions
108
3.3.5
Failure rate
110
3.3.6
Exercises
111
4
Vector valued random variables
113
4.1
Joint distribution
113

CONTENTS
vii
4.1.1
Joint and marginal distributions
114
4.1.2
Exercises
117
4.2
Covariance
120
4.2.1
Random variables with ﬁnite expected value and
variance
120
4.2.2
Correlation coefﬁcient
123
4.2.3
Exercises
123
4.3
Independent random variables
124
4.3.1
Independent events
124
4.3.2
Independent random variables
127
4.3.3
Independence of many random variables
128
4.3.4
Sum of independent random variables
130
4.3.5
Exercises
131
4.4
Sequences of independent random variables
140
4.4.1
Weak law of large numbers
140
4.4.2
Borel–Cantelli lemma
142
4.4.3
Convergences of random variables
143
4.4.4
Strong law of large numbers
146
4.4.5
A few applications of the law of large numbers
152
4.4.6
Central limit theorem
159
4.4.7
Exercises
163
5
Discrete time Markov chains
168
5.1
Stochastic matrices
168
5.1.1
Deﬁnitions
169
5.1.2
Oriented graphs
170
5.1.3
Exercises
172
5.2
Markov chains
173
5.2.1
Stochastic processes
173
5.2.2
Transition matrices
174
5.2.3
Homogeneous processes
174
5.2.4
Markov chains
174
5.2.5
Canonical Markov chains
178
5.2.6
Exercises
181
5.3
Some characteristic parameters
187
5.3.1
Steps for a ﬁrst visit
187
5.3.2
Probability of (at least) r visits
189
5.3.3
Recurrent and transient states
191
5.3.4
Mean ﬁrst passage time
193
5.3.5
Hitting time and hitting probabilities
195
5.3.6
Exercises
198
5.4
Finite stochastic matrices
201
5.4.1
Canonical representation
201
5.4.2
States classiﬁcation
203
5.4.3
Exercises
205

viii
CONTENTS
5.5
Regular stochastic matrices
206
5.5.1
Iterated maps
206
5.5.2
Existence of ﬁxed points
209
5.5.3
Regular stochastic matrices
210
5.5.4
Characteristic parameters
218
5.5.5
Exercises
220
5.6
Ergodic property
222
5.6.1
Number of steps between consecutive visits
222
5.6.2
Ergodic theorem
224
5.6.3
Powers of irreducible stochastic matrices
226
5.6.4
Markov chain Monte Carlo
228
5.7
Renewal theorem
233
5.7.1
Periodicity
233
5.7.2
Renewal theorem
234
5.7.3
Exercises
239
6
An introduction to continuous time Markov chains
241
6.1
Poisson process
241
6.2
Continuous time Markov chains
246
6.2.1
Deﬁnitions
246
6.2.2
Continuous semigroups of stochastic matrices
248
6.2.3
Examples of right-continuous Markov chains
256
6.2.4
Holding times
259
Appendix A Power series
261
A.1
Basic properties
261
A.2
Product of series
263
A.3
Banach space valued power series
264
A.3.2
Exercises
267
Appendix B Measure and integration
270
B.1
Measures
270
B.1.1
Basic properties
270
B.1.2
Construction of measures
272
B.1.3
Exercises
279
B.2
Measurable functions and integration
279
B.2.1
Measurable functions
280
B.2.2
The integral
283
B.2.3
Properties of the integral
284
B.2.4
Cavalieri formula
286
B.2.5
Markov inequality
287
B.2.6
Null sets and the integral
287
B.2.7
Push forward of a measure
289
B.2.8
Exercises
290

CONTENTS
ix
B.3
Product measures and iterated integrals
294
B.3.1
Product measures
294
B.3.2
Reduction formulas
296
B.3.3
Exercises
297
B.4
Convergence theorems
298
B.4.1
Almost everywhere convergence
298
B.4.2
Strong convergence
300
B.4.3
Fatou lemma
301
B.4.4
Dominated convergence theorem
302
B.4.5
Absolute continuity of integrals
305
B.4.6
Differentiation of the integral
305
B.4.7
Weak convergence of measures
308
B.4.8
Exercises
312
Appendix C Systems of linear ordinary differential equations
313
C.1
Cauchy problem
313
C.1.1
Uniqueness
313
C.1.2
Existence
315
C.2
Efﬁcient computation of eQt
317
C.2.1
Similarity methods
317
C.2.2
Putzer method
319
C.3
Continuous semigroups
321
References
324
Index
327


Preface
This book collects topics covered in introductory courses in probability delivered
by the authors at the University of Florence. It aims to introduce the reader to
typical structures of probability with a language appropriate for further advanced
reading. The attention is mainly focused on basic structures.
There is a well established tradition of studies in probability due to the wide
range of possible applications of related concepts and structures in science and
technology. Therefore, an enormous amount of literature on the subject is avail-
able, including treatises, lecture notes, reports, journal papers and web pages. The
list of references at the end of this book is obviously incomplete and includes
only references used directly in writing the following pages. Throughout this
book we adopt the language of measure theory (relevant notions are recalled in
the appendices).
The ﬁrst part of the book deals with basic notions of combinatorics and proba-
bility calculus: counting problems and uniform probability, probability measures,
probability distributions, conditional probability, inclusion–exclusion principle,
random variables, dispersion indexes, independence, and the law of large num-
bers are also discussed. Central limit theorem is presented without proof. Only
a basic knowledge of linear algebra and mathematical analysis is required.
In the second part we discuss, as a ﬁrst example of stochastic processes,
Markov chains with discrete time and discrete states, including the Markov chain
Monte Carlo method, and we introduce Poisson process and continuous time
Markov chains with ﬁnite states. For this part, further notions in mathematical
analysis (summarized in the appendices) are required: the Banach ﬁxed point
theorem, systems of linear ordinary differential equations, powers and power
series of matrices.
We wish to thank all the students who have attended our courses. We also
wish to thank our colleagues Matteo Focardi, Mariano Giaquinta, Paolo Maria
Mariano, Andrea Mennucci, Marco Romito and Enrico Vicario who helped us
with suggestions and comments. Special gratitude goes to Enrico Vicario for
many helpful discussions on the applications.
Our special thanks also go to the editorial staff of Wiley for the excellent
quality of their work.

xii
PREFACE
We have tried to avoid misprints and errors. However, we would be very
grateful to be notiﬁed of any errors or misprints and would be glad to receive
any criticism or comments. Our e-mail addresses are:
giuseppe.modica@uniﬁ.it
laura.poggiolini@uniﬁ.it
We will try to keep up an errata corrige at the following web pages:
http://www.dma.unifi.it/∼modica
http://www.dma.unifi.it/∼poggiolini
http://www.dmi.unifi.it/∼modica
http://www.dmi.unifi.it/∼poggiolini

1
Combinatorics
Combinatorics deals with the cardinality of classes of objects. The ﬁrst example
that jumps to our minds is the computation of how many triplets can be drawn
from 90 different balls. In this chapter and the next we are going to compute the
cardinality of several classes of objects.
1.1
Binomial coefﬁcients
1.1.1
Pascal triangle
Binomial coeffcients are deﬁned as
n
k

:=
⎧
⎪⎪⎨
⎪⎪⎩
1
if k = 0,
0
k > n,
n(n −1) . . . (n −k + 1)
k!
if n ≥1 and 1 ≤k ≤n.
Binomial coefﬁcients are usually grouped in an inﬁnite matrix
C := (Cn
k), n, k ≥0,
Cn
k :=
n
k

called a Pascal triangle given the triangular arrangement of the nonzero entries,
see Figure 1.1. Here and throughout the book we denote the entries of a matrix
(ﬁnite or inﬁnite) A = (ai
j) where the superscript i and the subscript j mean the
ith row and the jth column, respectively. Notice that the entries of each row of
C are zero if the column index is large enough, Ci
j = 0 ∀i, j with j > i ≥0. We
also recall the Newton binomial formula,
(1 + z)n =
n
0

+
n
1

z + · · · +
n
n

zn =
n

k=0
n
k

zk =
∞

k=0
n
k

zk.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

2
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
0
0
0
0
0
0
0
. . .
1
1
0
0
0
0
0
0
0
0
. . .
1
2
1
0
0
0
0
0
0
0
. . .
1
3
3
1
0
0
0
0
0
0
. . .
1
4
6
4
1
0
0
0
0
0
. . .
1
5
10
10
5
1
0
0
0
0
. . .
1
6
15
20
15
6
1
0
0
0
. . .
1
7
21
35
35
21
7
1
0
0
. . .
1
8
28
56
70
56
28
8
1
0
. . .
1
9
36
84
126
126
84
36
9
1
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Figure 1.1
Pascal matrix of binomial coefﬁcients (Cn
k), k, n ≥0.
Thus formula can be proven with an induction argument on n or by means of
Taylor formula.
1.1.2
Some properties of binomial coefﬁcients
Many formulas are known on binomial coefﬁcients. In the following proposition
we collect some of the simplest and most useful ones.
Proposition 1.1 The following hold.
(i)
n
k

=
n!
k!(n −k)! ∀k, n, 0 ≤k ≤n.
(ii)
n
k

= n
k
n −1
k −1

∀k, n, 1 ≤k ≤n.
(iii) Stifel formula
n
k

=
n −1
k

+
n −1
k −1

∀k, n, 1 ≤k ≤n.
(iv)
n
j
j
k

=
n
k
n −k
j −k

∀k, j, n, 0 ≤k ≤j ≤n.
(v)
n
k

=

n
n −k

∀k, 0 ≤k ≤n.
(vi) the map k →
n
k

achieves its maximum at k =
 n
2

.
(vii)
n

k=0
n
k

= 2n ∀n ≥0.
(viii)
n

k=0
(−1)k
n
k

= δ0,n =

1
if n = 0,
0
if n ̸= 0.
(ix)
n
k

≤2n ∀k, n, 0 ≤k ≤n.

COMBINATORICS
3
Proof. Formulas (i), (ii), (iii), (iv) and (v) directly follow from the deﬁnition.
(vi) is a direct consequence of (v). (vii) and (viii) follow from the Newton
binomial formula n
k=0
n
k

zk = (1 + z)n choosing z = 1 and z = −1. Finally,
(ix) is a direct consequence of (vii).
Estimate (ix) in Proposition 1.1 can be made more precise. For instance, from
the Stirling asymptotical estimate of the factorial,
n!
nne−n√
2πn
→1
as n →∞
one gets
(2n)! = 4nn2ne−2n√
4πn (1 + o(1)),
(n!)2 = n2ne−2n2πn (1 + o(1)),
so that
2n
n

=
4n
√πn
1 + o(1)
1 + o(1)
or, equivalently,
2n
n

4n
√πn
→1
as n →∞.
(1.1)
Estimate (1.1) is ‘accurate’ also for small values of n. For instance, for n = 4,
one has
8
4

= 70 and 44
1
√
π4 ≃72.2.
1.1.3
Generalized binomial coefﬁcients and binomial series
For α ∈R we deﬁne the sequence
α
n

of generalized binomial coefﬁcients as
α
n

:=
⎧
⎪⎨
⎪⎩
1
if n = 0,
α(α −1)(α −2) · · · (α −n + 1)
n!
if n ≥1.
Notice that
α
k

̸= 0 ∀k if α /∈N and
α
k

= 0 ∀k ≥α + 1 if α ∈N. The power
series
∞

n=0
α
n

zn
(1.2)
is called the binomial series.
Proposition 1.2 (Binomial series) The binomial series converges if |z| < 1 and
∞

n=0
α
n

zn = (1 + z)α
if |z| < 1.

4
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Proof. Since

 α
n+1


α
n

= |α −n|
|n + 1| →1
as n →∞,
it is well known that
n
|an| →1 as well; thus, the radius of the power series in
(1.2) is 1.
Differentiating n times the map z →(1 + z)α, one gets Dn(1 + z)α = α(α −
1) · · · (α −n + 1)(1 + z)α−n, so that the series on the left-hand side of (1.2) is
the McLaurin expansion of (1 + z)α.
Another proof is the following. Let S(z) := ∞
n=0
α
n

zn, |z| < 1, be the sum
of the binomial series. Differentiating one gets
(1 + z)S′(z) = αS(z),
|z| < 1,
hence

S(z)
(1 + z)α
′
= (1 + z)S′(z) −αS(z)
(1 + z)α+1
= 0.
Thus there exists c ∈R such that S(z) = c (1 + z)α if |z| < 1. Finally, c = 1
since S(0) = 1.
Proposition 1.3 Let α ∈R. The following hold.
(i)
α
k

= α
k
α −1
k −1

∀k ≥1.
(ii)
α
k

=
α −1
k

+
α −1
k −1

∀k ≥1.
(iii)
−α
k

= (−1)k
α + k −1
k

∀k ≥0.
Proof. The proofs of (i) and (ii) are left to the reader. Proving (iii) is a matter
of computation:
−α
k

= −α(−α −1) · · · (−α −k + 1)
k!
= (−1)k α(α + 1) · · · (α + k −1)
k!
= (−1)k
α + k −1
k

.
A few negative binomial coefﬁcients are quoted in Figure 1.2.
1.1.4
Inversion formulas
For any N, the matrix CN := (Cn
k), n, k = 0, . . . , N, is lower triangular and
all its diagonal entries are equal to 1. Hence 1 is the only eigenvalue of CN

COMBINATORICS
5
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
0
0
0
0
0
0
0
. . .
1
−1
1
−1
1
−1
1
−1
1
−1
. . .
1
−2
3
−4
5
−6
7
−8
9
−10
. . .
1
−3
6
−10
15
−21
28
−36
45
−55
. . .
1
−4
10
−20
35
−56
84
−120
165
−220
. . .
1
−5
15
−35
70
−126
210
−330
495
−715
. . .
1
−6
21
−56
126
−252
462
−792
1 287
−2 002
. . .
1
−7
28
−84
210
−462
924
−1 716
3 003
−5 005
. . .
1
−8
36
−120
330
−792
1 716
−3 432
6 435
−11 440
. . .
1
−9
45
−165
495
−1 287
3 003
−6 435
12 870
−24 310
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Figure 1.2
The coefﬁcients
−n
k

.
with algebraic multiplicity N. In particular CN is invertible, its inverse is lower
triangular, all its entries are integers and its diagonal entries are equal to 1.
Theorem 1.4 For any n, k = 0, . . . , N, (C−1
N )n
k = (−1)n+kn
k

.
Proof. Let B := (Bk
n), Bk
n := (−1)n+kn
k

so that both B and CNB are lower
triangular, i.e. (CNB)n
k = 0 if 0 ≤n < k. Moreover, (iv) and (viii) of Proposition
1.1 yield for any n ≥k
(CNB)n
k =
N

j=1
n
j

(−1)j+k
j
k

=
n

j=k
(−1)j+k
n
j
j
k

=
n
k

n

j=k
(−1)j+k
n −k
j −k

=
n
k
 n−k

i=0
(−1)i
n −k
i

=
n
k

δ0,n−k = δn,k.
A few entries of the inverse of the matrix of binomial coefﬁcients are shown
in Figure 1.3. As a consequence of Theorem 1.4 the following inversion formulas
hold.
Corollary 1.5 Two sequences

xn

,

yn

satisfy
yn =
n

k=0
n
k

xk,
∀n ≥0
if and only if
xn =
n

k=0
(−1)n+k
n
k

yk,
∀n ≥0.

6
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
0
0
0
0
0
0
0
. . .
−1
1
0
0
0
0
0
0
0
0
. . .
1
−2
1
0
0
0
0
0
0
0
. . .
−1
3
−3
1
0
0
0
0
0
0
. . .
1
−4
6
−4
1
0
0
0
0
0
. . .
−1
5
−10
10
−5
1
0
0
0
0
. . .
1
−6
15
−20
15
−6
1
0
0
0
. . .
−1
7
−21
35
−35
21
−7
1
0
0
. . .
1
−8
28
−56
70
−56
28
−8
1
0
. . .
−1
9
−36
84
−126
126
−84
36
−9
1
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Figure 1.3
The inverse of the matrix of binomial coefﬁcients C−1
N .
Similarly,
Corollary 1.6 Two N-tuples or real numbers

xn

and

yn

satisfy
yn =
N

k=n
k
n

xk,
∀n, 0 ≤n ≤N,
if and only if
xn =
N

k=n
(−1)n+k
k
n

yk,
∀n, 0 ≤n ≤N.
1.1.5
Exercises
Exercise 1.7 Prove Newton binomial formula:
(i) directly, with an induction argument on n;
(ii) applying Taylor expansion formula;
(iii) starting from the formula D((1 + z)n) = n(1 + z)n−1.
Exercise 1.8 Differentiating the power series, see Appendix A, prove the formulas
in Figure 1.4.
Solution. Differentiating the identity ∞
k=0 zk =
1
1−z, |z| < 1, we get for
|z|<1
∞

k=0
kzk = z
∞

k=0
D(zk) = zD
 ∞

k=0
zk
= zD

1
1 −z

=
z
(1 −z)2 ;

COMBINATORICS
7
Let z ∈C, |z| < 1, and n ∈Z. We have the followings.
(i)
∞

k=0
kzk =
z
(1 −z)2 ,
(ii)
∞

k=0
k2zk−1 = D

z
(1 −z)2

=
1 + z
(1 −z)3 ,
(iii)
∞

k=0
n
k

zk = (1 + z)n,
(iv)
∞

k=0
k
n
k

zk = nz(1 + z)n−1,
(v)
∞

k=0
k2
n
k

zk = nz(1 + nz)(1 + z)n−2.
Figure 1.4
The sum of a few series related to the geometric series.
∞

k=0
k2zk−1 =
∞

k=0
D(kzk)
= D
 ∞

k=0
kzk
= D

z
(1 −z)2

=
1 + z
(1 −z)3 .
Moreover, for any non-negative integer n, differentiating the identities
∞

k=0
n
k

zk = (1 + z)n
and
∞

k=0
−n
k

zk = (1 + z)−n
for any |z| < 1, we get
∞

k=0
k
n
k

zk = z
n

k=0
D
n
k

zk
= zD

n

k=0
n
k

zk
= zD((1 + z)n) = nz(1 + z)n−1;
∞

k=0
k2
n
k

zk = z
n

k=0
D

k
n
k

zk
= zD

n

k=0
n
k

zk
= zD(nz(1 + z)n−1) = nz(1 + nz)(1 + z)n−2.

8
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
1.2
Sets, permutations and functions
1.2.1
Sets
We recall that a ﬁnite set A is an unordered collection of pairwise different
objects. For example, the collection A hose objects are 1,2,3 is a ﬁnite set which
we denote as A = {1, 2, 3}; the collection 1,2,2,3 is not a ﬁnite set, and {1, 2, 3}
and {2, 1, 3} are the same set.
If A is a ﬁnite set with n objects (or elements), we may enumerate the
elements of A so that A =

x1, . . . , xn

. Therefore, for counting purposes, we
can assume without loss of generality that A = {1, . . . , n}. The number n is the
cardinality of A, and we write |A| = n.
Proposition 1.9 Let A be a ﬁnite set with n elements, n ≥1. There are Cn
k =
n
k

subsets of A with k elements.
Proof. Different proofs can be done. We propose one of them. Let dn,k be
the number of subsets of A with k elements. Obviously, dn,1 = n and dn,n = 1.
For 2 ≤k ≤n −1, assume we have n football players and we want to select a
team of k of them, including the captain of the team. We may proceed in the
following way: ﬁrst we choose the team of k-players: dn,k different teams can
be selected. Then, among the team, we select the captain: k different choices are
possible: so there are kdn,k ways to select the team and the captain. However,
we can proceed in another way: ﬁrst we choose the captain among the n players:
there are n different possible choices. Then we choose k −1 players among the
remaining n −1 players: there are dn−1,k−1 possible choices. Thus
dn,k = n
k dn−1,k−1
which by induction, gives
dn,k = n
k
n −1
k −1 . . . n −k + 2
2
dn−k+1,1
= n
k
n −1
k −1 . . . n −k + 2
2
n −k + 1
1
=
n
k

.
1.2.2
Permutations
Let N be a ﬁnite set and let n be its cardinality. Without loss of generality,
we can assume N = {1, 2, . . . , n}. A permutation of N is an injective (and thus
one-to-one) mapping π : N →N. Since composing bijective maps yields another
bijective map, the family of permutations of a set N is a group with respect to
the composition of maps; the unit element is the identity map; this group is called
the group of permutations of N. It is denoted as Sn or Pn. Notice that Pn is a
not a commutative group if n ≥3.
Each permutation is characterized by its image-word or image-list, i.e. by the
n-tuple (π(1), . . . , π(n)). For instance, the permutation π ∈P6 deﬁned by

COMBINATORICS
9
π(1) = 2, π(2) = 3, π(3) = 1, π(4) = 4, π(5) = 6 and π(6) = 5 is denoted as

1
2
3
4
5
6
2
3
1
4
6
5

.
or, in brief, with its image-word 231465.
The set of permutations of N = {1 . . . , n} has n! elements,
|Pn| = n!
In fact, the image π(1) of 1 can be chosen among n possible values, then the
image π(2) of 2 can be chosen among n −1 possible values and so on. Hence
|Pn| = n(n −1)(n −2) · · · 3 · 2 · · · 1 = n!
1.2.2.1
Derangements
Let π ∈Pn be a permutation of N = {1, . . . , n}. A point x ∈N is a ﬁxed point
of π if π(x) = x.
We now compute the cardinality dn of the set Dn of permutations without
ﬁxed points, also called derangements.
Dn :=

π ∈Pn
 π(i) ̸= i ∀i ∈N

.
Proposition 1.10 The cardinality of Dn is
dn = n!
n

j=0
(−1)j 1
j!
∀n ≥1.
Proof. If a permutation of N has j ﬁxed points, 0 ≤j ≤n, then it is
a derangement of the other n −j points of N. Thus, a permutation with j
ﬁxed points splits as a couple: the set of its ﬁxed points and a derangement
of n −j points. There are
n
j

different choices for the j ﬁxed points and
dn−j
derangements of the remaining n −j
points, so that, the possible
permutations of N with exactly j ﬁxed points are
n
j

dn−j (where d0 = 1). Thus
|Pn| = n
j=0
n
j

dn−j ∀n ≥1, i.e.
n! =
n

j=0
n
j

dn−j
∀n ≥0.
(1.3)
The inversion formula of binomial coefﬁcients, see Corollary 1.5, reads
dn =
n

j=0
(−1)(n+j)
n
j

j! = n!
n

j=0
(−1)j
j!
∀n ≥0.

10
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
0, 0, 1, 2, 9, 44, 265, 1 854, 14 833, 133 496, 1 334 961, 14 684 570, . . .
Figure 1.5
From the left, the numbers d0, d1, d2, d3, . . . of derangements of
0, 1, 2, 3, . . . points.
Corollary 1.11 The number dn of derangements of n points is the nearest integer
to n!/e.
Proof. The elementary estimate between the exponential and its McLaurin
expansion gives
ex −
n

j=0
xj
j!
 ≤
|x|n+1
(n + 1)!,
∀x ≤0;
hence for x = −1 we get
1
e −
n

j=0
(−1)j
j!
 ≤
1
(n + 1)!,
so that, from Proposition 1.10 one gets
dn −n!
e
 = n!

n

j=0
(−1)j
j!
−1
e
 ≤
n!
(n + 1)! =
1
n + 1 ≤1
3
for each n ≥2.
Figure 1.5 contains the ﬁrst elements of the sequence {dn}.
1.2.3
Multisets
Another interesting structure is an unordered list of elements taken from a given
set A. This structure is called a multiset on A. More formally, a multiset on a
set A is a couple (A, a) where A is a given set and a : A →N ∪{+∞} is the
multiplicity function which counts ‘how many times’ an element x ∈A appears
in the multiset. Clearly, each set is a multiset where each object has multiplicity
1. We denote as

a2, b2, c5
or a2b2c5 the multiset on A := {a, b, c} where a
and b have multiplicity 2 and c has multiplicity 5. The cardinality of a multiset
(A, a) is 
x∈A a(x) and is denoted by |(A, a)| or #(A, a). For instance, the
cardinality of a2b2c5 is 9.
If B is a subset of A, then B is also the multiset (A, a) on A where
a(x) =
1
if x ∈B,
0
if x /∈B.

COMBINATORICS
11
Given two multisets (B, b) and (A, a), we say that (B, b) is included in (A, a)
if B ⊂A and b(x) ≤a(x) ∀x ∈B. In this case, (B, b) = (A,b) where
b(x) =

b(x)
if x ∈B,
0
if x /∈B.
Proposition 1.12 Let A be a ﬁnite set, |A| = n. Let (A, a) be a multiset on A and
let k be a non-negative integer such that k ≤a(x) ∀x ∈A. The multisets included
in (A, a) with k elements are
n + k −1
k

.
Proof. Let A = {1, . . . , n}. A multiset S of cardinality k included in (A, a)
contains the element 1 x1 times, the element 2 x2 times, and so on, with x1 +
x2 + · · · + xn = k. Moreover, the n-tuple (x1, . . . , xn) characterizes S. We can
associate to a n-tuple (x1, . . . , xn) the binary sequence
00 . . . 0
  
x1
1 00 . . . 0
  
x2
1 . . . 1 00 . . . 0
  
xn−1
1 00 . . . 0
  
xn
(1.4)
where the symbol 1 denotes the fact that we are changing the element of A. This
is a binary word of length n + k −1 with k zeroes.
The correspondence described above is a one-to-one correspondence between
the set of multisets of cardinality k included in (A, a) and the set of binary words
of length n + k −1 with k zeroes. There are exactly
n + k −1
k

different words of this kind, so that the claim is proven.
1.2.4
Lists and functions
Given a set A, a list of k objects from the set A or a k-word with symbols in A
is an ordered k-tuple of objects. For instance, if A = {1, 2, 3}, then the 6-tuples
(1,2,3,3,2,1) and (3,2,1,3,2,1) are two different 6-words of objects in A. In these
lists, or words, repetitions are allowed and the order of the objects is taken into
account. Since each element of the list can be chosen independently of the others,
there are n possible choices for each object in the list. Hence, the following holds.
Proposition 1.13 The number of k-lists of objects from a set A of cardinality n is
nk.
A function f : X →A is deﬁned by the value it assumes on each element of
X: if f : {1, . . . , k} →A, then f is deﬁned by the k-list (f (1), f (2), . . . , f (k)),

12
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
which we refer to as the image-list or image-word of f . Conversely, each k-list
(a1, a2, . . . , ak) with symbols from A deﬁnes the function f : {1, . . . , k} →A
given by f (i) := ai ∀i. If |A| is ﬁnite, |A| = n, we have a one-to-one correspon-
dence between the set Fk
n of maps f : {1, . . . , k} →A, and the set of the k-lists
with symbols in A. Therefore, we have the following.
Proposition 1.14 The number of functions in Fk
n is F k
n := |Fk
n| = nk.
1.2.5
Injective functions
We use the symbol Ik
n to denote the set of injective functions f : {1, . . . , k} →A,
|A| = n, k ≤n. Let I k
n = |Ik
n|. Obviously, I k
n = 0 if k > n. The image-list of an
injective function f ∈Ik
n is a k-word of pairwise different symbols taken from A
To form any such image-list, one can choose the ﬁrst entry among n elements, the
second entry can be chosen among n −1 elements, . . ., the kth entry can be cho-
sen among the remaining n −k + 1 elements of A, so that we have the following.
Proposition 1.15 The cardinality I k
n of Ik
n is
I k
n = |Ik
n| = n(n −1) · · · · · (n −k + 1) = k!
n
k

=
n!
(n −k)! .
Some of the I k
n’s are in Figure 1.6.
1.2.6
Monotone increasing functions
Let
Ck
n,
k ≤n,
be
the
set
of
strictly
monotone
increasing
functions
f : {1, . . . , k} →{1, . . . , n}. The image-list of any such function is an ordered
k-tuple of strictly increasing–hence pairwise disjoint–elements of {1, . . . , n}.
The k-tuple is thus identiﬁed by the subset of the elements of {1, . . . , n} appearing
in it, so that we have the following.
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
1
1
1
1
1
1
1
1
1
. . .
0
1
2
3
4
5
6
7
8
9
. . .
0
0
2
6
12
20
30
42
56
72
. . .
0
0
0
6
24
60
120
210
336
504
. . .
0
0
0
0
24
120
360
840
1 680
3 024
. . .
0
0
0
0
0
120
720
2 520
6 720
15 120
. . .
0
0
0
0
0
0
720
5 040
20 160
60 480
. . .
0
0
0
0
0
0
0
5 040
40 320
181 440
. . .
0
0
0
0
0
0
0
0
40 320
362 880
. . .
0
0
0
0
0
0
0
0
0
362 880
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Figure 1.6
The cardinality I k
n of the set of injective maps Ik
n for n, k ≥0.

COMBINATORICS
13
Proposition 1.16 The cardinality Ck
n of Ck
n is
Ck
n := |Ck
n| =
n
k

= Cn
k = (CT )k
n.
1.2.7
Monotone nondecreasing functions
Let Dk
n be the class of monotone nondecreasing functions f : {1, . . . , k} →
{1, . . . , n}. The image-list of any such function is a nondecreasing ordered
k-tuple of elements of {1, . . . , n}, so that elements can be repeated. The
functions in Dk
n are as many as the multisets with cardinality k included in
a multiset (A, a), where A = {1, . . . , n} and a(x) ≥k ∀x ∈A. Thus, see
Proposition 1.12, we have the following.
Proposition 1.17 The cardinality Dk
n of Dk
n is
Dk
n := |Dk
n| = |Ck
n+k−1| =
n + k −1
k

.
Another proof of Proposition 1.17. Consider the map φ : Dk
n →Fk
n+k−1 deﬁned
by φ(f )(i) := f (i) + i −1 ∀i ∈{1, . . . , k}, ∀f ∈Dk
n. Obviously, if f ∈Dk
n,
then φ(f ) is strictly monotone increasing, φ(f ) ∈Ck
n+k−1. Moreover, the corre-
spondence φ : Dk
n →Ck
n+k−1 is one-to-one, thus
Dk
n = |Dk
n| = |Ck
n+k−1| =
n + k −1
k

.
Yet another proof of Proposition 1.17. We are now going to deﬁne a one-to-one
correspondence between a family of multisets and Dk
n. Let (A, a) be a multiset
on A = {1, . . . , n} with a(x) ≥k ∀k. For any multiset (S, nS) of cardinality k
included in (A, a), let fS : A →{0, . . . , k} be the function deﬁned by
fS(x) :=

y≤x
nS(y),
i.e. for each x ∈A, fS(x) is the sum of the multiplicities nS(y) of all elements
y ∈A, y ≤x. fS is obviously a nondecreasing function and fS(n) = k. More-
over, it is easy to show that the map
S →fS
is a one-to-one correspondence between the family of the multisets included
in (A, a) of cardinality k and the family of monotone nondecreasing func-
tions from {1, . . . , n} to {0, . . . , k} such that f (k) = 1. In turn, there is an
obvious one-to-one correspondence between this class of functions and the class

14
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
of monotone nondecreasing functions from {1, . . . , n −1} to {0, . . . , k}. Thus,
applying Proposition 1.17 we get
|Dn−1
k+1| =
k + 1 + (n −1) −1
n −1

=
n + k −1
k

.
1.2.8
Surjective functions
The computation of the number of surjective functions is more delicate. Let Sk
n
denote the family of surjective functions from {1, . . . , k} onto {1, . . . , n} and let
Sk
n =
⎧
⎪⎨
⎪⎩
1
if n = k = 0,
0
if n = 0, k > 0
|Sk
n|
if n ≥1.
Obviously, Sk
n = |Sk
n| = 0 if k < n. Moreover, if k = n ≥1, then a function
f : {1, . . . , n} →{1, . . . , n} is surjective if and only if f is injective, so that
Sn
n = |Sk
n| = I n
n = n!
If k > n ≥1, then Sk
n ̸= ∅. Observe that any function is onto on its range.
Thus, for each j = 1, . . . , n, consider the set Aj of functions f : {1, . . . , k} →
{1, . . . , n} whose range has cardinality j. We must have
nk = |Fk
n| =
n

j=1
|Aj|.
There are exactly
n
j

subsets of {1, . . . , n} with cardinality j and there are Sk
j
different surjective functions onto each of these sets. Thus, |Aj| =
n
j

Sk
j and
nk =
n

j=1
n
j

Sk
j
∀n ≥1.
Since we deﬁned Sk
0 = 0, we get
nk =
n

j=0
n
j

Sk
j
∀n ≥0.
(1.5)
Therefore, applying the inversion formula in Corollary 1.5 we conclude the fol-
lowing.
Proposition 1.18 The cardinality Sk
n of the set Sk
n of surjective functions from
{1, . . . , k} onto {1, . . . , n} is
Sk
n =
n

j=0
(−1)n+j
n
j

j k =
n

j=0
(−1)j
n
j

(n −j)k
∀n, k ≥1.

COMBINATORICS
15
We point out that the equality holds also if k ≤n so that
1
n!
n

j=0
(−1)j
n
j

(n −j)k = 1
n!Sk
n =

1
if k = n,
0
if k < n.
Another useful formula for Sk
n is an inductive one, obtained starting from
Sn
n = n! ∀n ≥0 and Sk
n = 0 for any k and n with k < n.
Proposition 1.19 We have
⎧
⎪⎨
⎪⎩
Sk
n = n(Sk−1
n
+ Sk−1
n−1)
if k ≥1, n ≥0,
Sn
n = n!,
Sk
0 = 0
if k ≥1.
(1.6)
Proof. Assume n ≥1 and k ≥1 and let f : {1, . . . , k} →{1, . . . , n} be a sur-
jective function. Let A ⊂Sk
n be the class of functions such that the restriction
f : {1, . . . , k −1} →{1, . . . , n} of f is surjective and let B := Sk
n \ A. The car-
dinality of A is nSk−1
n
because there are Sk−1
n
surjective maps from {1, . . . , k −1}
onto {1, . . . , n} and there are n possible choices for f (k). Since the maps on B
have a range of (n −1) elements, we infer that there are nSk−1
n−1 maps of this
kind. In fact, there are
 n
n−1

= n subsets E of {1, . . . , n} of cardinality n −1
and there are Sk−1
n−1 surjective functions from {1, . . . , k −1} onto E. Therefore,
Sk
n = |A| + |B| = nSk−1
n
+ nSk−1
n−1.
i.e. (1.6).
Some of the Sk
n’s are in Figure 1.7.
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
0
0
0
0
0
0
0
0
. . .
0
1
0
0
0
0
0
0
0
. . .
0
1
2
0
0
0
0
0
0
. . .
0
1
6
6
0
0
0
0
0
. . .
0
1
14
36
24
0
0
0
0
. . .
0
1
30
150
240
120
0
0
0
. . .
0
1
62
540
1 560
1 800
720
0
0
. . .
0
1
126
1 806
8 400
16 800
15 120
5 040
0
. . .
0
1
254
5 796
40 824
126 000
191 520
141 120
40 320
. . .
0
1
510
18 150
186 480
834 120
1 905 120
2 328 480
1 451 520
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
Figure 1.7
The cardinality Sk
n of the set of surjective maps Sk
n for n, k ≥0.

16
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
1.2.9
Exercises
Exercise 1.20 How many diagonals are there in a polygon having n edges?
1.3
Drawings
A drawing or selection of k objects from a population of n is the choice of k
elements among the n available ones. We want to compute how many of such
selections are possible. In order to make this computation, it is necessary to be
more precise, both on the composition of the population and on the rules of the
selection as, for instance, if the order of selection is relevant or not. We consider
a few cases:
• The population is made by pairwise different elements, as in a lottery: in
other words, the population is a set.
• The population is a multiset (A, a). In this case, we say that we are dealing
with a drawing from A with repetitions.
• The selected objects may be given an order. In this case we say that we
consider an ordered selection. Unordered selections are also called simple
selections.
Some drawing policies simply boil down to the previous cases:
• In the lottery game, numbers are drawn one after another, but the order of
drawings is not taken into account: it is a simple selection of objects from
a set.
• In ordered selections the k-elements are selected one after another and the
order is taken into account.
• A drawing with replacement, i.e. a drawing from a set where each selected
object is put back into the population before the next drawing is equivalent
to a drawing with repetitions, i.e. to drawing from a multiset where each
element has multiplicity larger than the total number of selected objects.
1.3.1
Ordered drawings
Ordered drawings of k objects from a multiset (A, a) are k-words with symbols
taken from A.
1.3.1.1
Ordered drawings from a set
Each ordered drawing of k objects from a set A is a k-list with symbols in A
that are pairwise different. Thus the number of possible ordered drawings of k
elements from A is the number of k-lists with pairwise different symbols in A.

COMBINATORICS
17
If |A| = n, there are n possible choices for the ﬁrst symbol, n −1 for the second
and so on, so that there are
n(n −1) . . . (n −k + 1)
different k-words with pairwise different symbols.
1.3.1.2
Ordered drawings from a multiset
Let (A, a) be a multiset where |A| = n and let k ∈N be less than or equal to
min {a(x) | x ∈A}. Each ordered drawing of k elements from (A, a) is a k-list
with symbols in A, where the same symbol may appear more than once. We
have already proven that there are nk possible k-lists of this kind, so that the
following holds.
Proposition 1.21 The number of ordered drawings of k elements from a multiset
(A, a) where k ≤min {a(x) | x ∈A} is nk.
In particular, the number of ordered drawings with replacement of k elements
from A is nk.
1.3.2
Simple drawings
1.3.2.1
Drawings from a set
The population from which we make the selection is a set A. To draw k objects
from A is equivalent to selecting a subset of k elements of A: we do not distin-
guish selections that contain the same objects with a different ordering.
Proposition 1.22 The number of possible drawings of k elements from a set of
cardinality n is
n
k

.
1.3.2.2
Drawings from a multiset
Let (A, a) be a multiset, |A| = n, and let k ≤min {a(x) | x ∈A}. Each sequence
S drawn from (A, a) is a sequence of symbols in A where repetitions may occur
and the order of the symbols is not taken into accout, e.g.
FABADABDF = FBFDDAABA
i.e. S is a multiset of k elements included in (A, a) (cf. Proposition 1.17).
Proposition 1.23 The number of simple drawings of k elements from a multiset
(A, a), is
n+k−1
k

provided k ≤min {a(x) | x ∈A}.
1.3.3
Multiplicative property of drawings
The previous results on drawings can also be obtained from the following com-
binatorics properties of drawings.

18
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Theorem 1.24 For each non-negative integer k let ak and bk be the numbers of
drawings of k objects from the multisets (A, a) and (B, b) made according to
policies P1 and P2, respectively. If A and B are disjoint, then the number of draw-
ings of k elements from the population obtained by the union of (A, a) and (B, b)
made according to policies P1
and P2
for the drawings from (A, a)
and (B, b), respectively, is
ck =
k

j=0
ajbk−j.
Proof. A drawing of k objects from the union of the two populations contains,
say, j elements from (A, a) and k −j elements from (B, b), where j is an
integer, 0 ≤j ≤k. The j elements drawn from (A, a) can be chosen in aj
different ways, while the n −j elements drawn from (B, b) can be chosen in
bk−j different ways and the two choices are independent. Thus,
ck =
k

j=0
ajbk−j.
A similar result holds for ordered drawings
Theorem 1.25 For each non-negative integer k let ak and bk be the number of
ordered drawings from the multisets (A, a) and (B, b) made according to policies
P1 and P2, respectively. If A and B are disjoint, then the number of ordered
drawings from the population union of (A, a) and (B, b) made according to policy
P1 for the elements of (A, a) and according to policy P2 for elements of (B, b) are
ck =
k

j=0
k
j

ajbk−j.
Proof. A drawing of k elements from the union of the two populations con-
tains j elements from (A, a) and n −j elements from (B, b) for some integer
j ∈{0, . . . , k}. The j elements from (A, a) can be chosen in aj different ways,
the k −j elements drawn from (B, b) can be chosen in bk−j different ways and
the two chosen groups are independent. Finally, there are
k
j

ways to order such
selections. Thus,
ck =
k

j=0
k
j

ajbk−j.
1.3.4
Exercises
Exercise 1.26 A committee of 7 people has to be chosen among 11 women and 8
men. In each of the following cases compute how many different committees can
be chosen:

COMBINATORICS
19
• No constraint is imposed.
• At least two women and at least one man must be present.
• There must be more women than men.
• At least two women and no more than three men must be present.
1.4
Grouping
Many classical counting problems amount to a collocation or grouping problem:
how many different arrangements of k objects in n boxes are there? Putting it
another way, how many different ways of grouping k objects into n groups are
there? Also in this case a deﬁnite answer cannot be given: we must be more
precise both on the population to be arranged, on the rules (or policy) of the
procedure, and on the way the groups are evaluated. For example, one must say
whether the objects to be arranged are pairwise different or not, whether the order
of the objects in each box must be taken into account or not, whether the boxes
are pairwise distinct or not, and if further constraints are imposed. Here we deal
with a few cases, all referring to collocation or grouping in pairwise different
boxes. We consider the formed groups as a list instead of as a set: for instance,
if we start with the objects {1, 2, 3} then the two arrangements in two boxes
({1}, {2, 3}) and ({2, 3}, {1}) are considered to be different.
1.4.1
Collocations of pairwise different objects
Arranging k distinct objects in n pairwise different boxes is the act of
deciding the box in which each object is going to be located. Since both the
objects and the boxes are pairwise distinct, we may identify the objects and the
boxes with the sets {1, . . . , k} and {1, . . . , n}, respectively. Each arrangement
corresponds to a grouping map f : {1, . . . , k} →{1, . . . , n} that puts the object
j into the box f (j).
1.4.1.1
No further constraint
In this case the set of possible locations is in a one-to-one correspondence with the
set Fk
n of all maps f : {1, . . . , k} →{1, . . . , n}. Therefore, there are nk different
ways to locate k-different objects in n boxes.
A different way to do the computation is the following. Assume i1, . . . , in
objects are placed in the boxes 1, . . . , n, respectively, so that i1 + · · · + in = k.
There are
k
i1

different choices for the elements located in the ﬁrst box,
k−i1
i2

different choices for the elements in the second box, and so on, so that there are
k −i1 −· · · −in−1
in


20
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
different choices for the elements in the nth box. Thus the different possible
arrangements are
k
i1
k −i1
i2

· · ·
k −i1 −· · · −in−1
in

=
=
k!
i1!(k −i1)!
(k −i1)!
i2!(k −i1 −i2)! · · · =
k!
i1! i2! · · · in!;
(1.7)
the ratio in (1.7) is called the multinomial coefﬁcient and is denoted as

k
i1 i2 · · · in

.
From (1.7) we infer that the possible collocations of k pairwise different objects
in n pairwise different boxes are

i1+···+in=k

k
i1 i2 · · · in

where the sum is performed over all the n-tuples i1, . . . , in of non-negative inte-
gers such that i1 + · · · + in = k. Thus, from the two different ways of computing
collocations, we get the equality
nk =

i1+···+in=k

k
i1 i2 · · · in

.
1.4.1.2
At least one object in each box
We now want to compute the number of different arrangements with at least one
object in each box. Assuming we have k objects and n boxes, collocations of
this type are in a one-to-one correspondence with the class of surjective maps
Sk
n from {1, . . . , k} onto {1, . . . , n}, thus there are
Sk
n =
n

j=0
(−1)j
n
j

(n −j)k
collocations of k pairwise different into n pairwise different boxes that place at
least one object in each box.
Another way to compute the previous number is the following. Assume
i1, . . . , in objects are located in the boxes 1, . . . , n, respectively, with at least
one object in each box, i.e. i1 + · · · + in = k and i1, . . . , in ≥1. As in (1.7),
there are

k
i1 i2 · · · in

(1.8)

COMBINATORICS
21
ways to arrange k different objects in n boxes with ij objects in the box j. Thus
the number of arrangements with no empty box is

i1+···+in=k
i1,...,in≥1

k
i1 i2 · · · in

;
here, the sum is performed over all the n-tuples i1, . . . , in with positive compo-
nents with i1 + · · · + in = k. The above two ways of computing the number of
such collocations yield the identity
Sk
n =

i1+···+in=k
i1,...,in≥1

k
i1 i2 · · · in

.
(1.9)
1.4.1.3
At most one object in each box
We now impose a different constraint: each box may contain at most one object.
Assuming we have k objects and n boxes, collocations of this type are in a
one-to-one correspondence with the class of injective grouping maps Ik
n from
{1, . . . , k} onto {1, . . . , n}, thus there are
I k
n = k!
n
k

collocations of this type.
1.4.1.4
Grouping into lists
Here, we want to compute the number of ways of grouping k pairwise different
objects in n pairwise different boxes and pretend that the order of the objects in
each box matters. In other words we want to compute how many different ways
exist to group k objects in a list of n lists of objects. We proceed as follows.
The ﬁrst object can be collocated in one of the n boxes, that is in n different
ways. The second object can be collocated in n + 1 different ways: in fact, it can
be either collocated in each of the n −1 empty boxes, or it can be collocated
in the same box as the ﬁrst object. In the latter case it can be collocated either
as the ﬁrst or as the second object in that box. So the possible arrangements of
the second object are (n −1) + 2 = n + 1. The third object can be collocated in
n + 2 ways. In fact, if the ﬁrst two objects are collocated in two different boxes,
then the third object can either be collocated in one of the n −2 empty boxes
or in two different ways in each of the two nonempty boxes. Thus, there are
(n −2) + 2 + 2 = n + 2 possible arragements. If the ﬁrst two objects are in the
same box, then the third object can either be collocated in one of the n −1 empty
boxes or in the nonempty one. In the latter case, it can be collocated in three
different ways: either as the ﬁrst, or between the two objects already present, or

22
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
as the last one. Again, the third object can be collocated in (n −1) + 3 = n + 2
ways. By an induction argument, we infer that there are n + k −1 different
arrangements for the kth object. Thus, the number of different ordered locations
of k objects in n boxes is
n(n + 1)(n + 2) . . . (n + k −1) = k!
n + k −1
k

.
1.4.2
Collocations of identical objects
We want to compute the number of ways to arrange k identical objects in n
pairwise different boxes. In this case each arrangement is characterized by the
number of elements in each box, that is by the map x : {1, . . . , n} →{0, . . . , k}
which counts how many objects are in each box. Obviously, n
s=1 x(s) = k. If
the k objects are copies of the number ‘0’, then each arrangement is identiﬁed
by the binary sequence
00 . . . 0
  
x(1)
1 00 . . . 0
  
x(2)
1 . . . 1 00 . . . 0
  
x(n−1)
1 00 . . . 0
  
x(n)
(1.10)
where the number ‘0’ denotes the fact that we are changing box.
1.4.2.1
No further constraint
Let us compute the number of such arrangements with no further constraint.
There is a one-to-one correspondence between such arrangements and the set
of all binary sequences of the type (1.10). Therefore, see Proposition 1.12, the
different collocations of k identical objects in n pairwise different boxes is
n + k −1
k

.
(1.11)
1.4.2.2
At least one in each box
We add now the constraint that each box must contain at least one object. If
k < n no such arrangement is possible. If k ≥n, we then place one object in
each box so that the constraint is satisﬁed. The remaining k −n objects can be
now collocated without constraints. Therefore, cf. (1.11), there are
n + (k −n) −1
k −n

=
k −1
k −n

=
k −1
n −1

ways to arrange k identical objects in n boxes, so that no box remains empty.
1.4.2.3
At most one in each box
We consider arrangements of k identical objects in n pairwise different boxes
that place at most one object into each box. In this case, each arrangement is

COMBINATORICS
23
completely characterized by the subset of ﬁlled boxes. Since we can choose them
in
n
k

different ways, we conclude that the collocations of k identical objects in
n pairwise different boxes with at most one object per box is
n
k

.
1.4.3
Multiplicative property
Combinatorial properties hold for collocations as well as for drawings.
Theorem 1.27 For each non-negative integer k, let ak and bk be the number
of collocations of k pairwise different objects in two sets S1 and S2 of pairwise
different boxes with policies P1 and P2, respectively. If S1 ∩S2 = ∅, then the dif-
ferent collocations of the k objects in S1 ∪S2 following policy P1 for collocations
in boxes of S1 and policy P2 for collocations in boxes of S2 is
ck =
k

j=0
k
j

ajbk−j.
Proof. Let j objects, 0 ≤j ≤k be collocated in the boxes of the set S1 and
let the other k −j objects be collocated in the boxes of S2. There are aj different
ways of placing j objects in the boxes of S1 and bk−j different ways of placing
(k −j) objects in the boxes of S2. Moreover, there are
k
j

different ways to
choose which objects are collocated in the boxes of S1. Hence,
ck =
k

j=0
k
j

ajbk−j
∀k ≥0.
A similar result holds for the collocations of identical objects.
Theorem 1.28 For each non-negative integer k, let ak and bk be the number of
collocations of k identical objects in two sets S1 and S2 of pairwise different boxes
with policies P1 and P2, respectively. If S1 ∩S2 = ∅, then the collocations of the k
objects in the boxes of S1 ∪S2 made according to policy P1 for the collocations in
the boxes of S1 and according to policy P2 for the collocations in the boxes of S2 is
ck =
k

j=0
ajbk−j.
Proof. Let j objects, 0 ≤j ≤k be collocated in the boxes of the set S1 and
let the other k −j objects be collocated in the boxes of S2. There are aj ways of
placing j objects in the boxes of S1 and bk−j different ways of placing (k −j)

24
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
objects in the boxes of S2. Since the objects are identical, there is no way to
select which the j objects to be placed in the boxes of S1 are. Then the possible
different collocations are
ck =
k

j=0
ajbk−j
∀k ≥0.
1.4.4
Collocations in statistical physics
In statistical physics, each ‘particle’ is allowed to be in a certain ‘state’; an
‘energy level’ is associated with each state. The total energy of a system of
particles depends on how many particles are in each of the possible states; the
mean value of the energy depends on the probabilities that particles stay in a
certain state. Thus, the number of possible collocations of the particles in the
available states must be evaluated.
1.4.4.1
Maxwell-Boltzmann statistics
This is the case of classical statistical physics: the particles are distinct and no
constraint is imposed on their distribution in different states. The number of
possible collocations of k particles in n states is thus the number of collocations
of k pairwise different objects in n pairwise different boxes, i.e. nk.
1.4.4.2
Bose–Einstein statistics
The particles are indistinguishable and no constraint is imposed on their distri-
bution in different states. Particles with this behaviour are called bosons. The
number of collocations of k particles in n states is then the number of collocations
of k identical objects in n pairwise different boxes, i.e.
n + k −1
k

=
n + k −1
n −1

.
1.4.4.3
Fermi–Dirac statistics
The particles are indistinguishable and each state can be occupied by at most
one particle (Pauli exclusion principle). Particles following this behaviour are
called fermions. Then the collocations of k particles in n states is the number
of possible choices for the states to be occupied, i.e.
n
k

. Obviously, the Pauli
exclusion principle implies n ≥k.
1.4.5
Exercises
Exercise 1.29 A group of eight people sits around a table with eight seats. How
many different ways of sitting are there?

COMBINATORICS
25
Exercise 1.30 Compute the number gn,k of subsets of {1 . . . , n} having cardinality
k and that do not contain two consecutive integers.
Solution. There is a one-to-one correspondence between the family of the
subsets of cardinality k and the set of binary n-words given by mapping a
subset A ⊂{1, . . . , n} to its characteristic function 1A. Namely, to the subset
A ⊂{1, . . . , n} we associate the binary n-word (a1, a2, . . . , an) where ai = 1 if
i ∈A and ai = 0 otherwise. Consequently, the family we are considering is in
a one-to-one correspondence with the binary n-words in which there cannot be
two consecutive 1’s, in
0001000101000101
Considering the 0’s as the sides of a box that contains at most one 1, we have
k 1’s and n −k + 1 boxes with at most one 1 per box. Thus, each collocation
is uniquely detected by the choice of the k nonempty boxes. Thus, see Section
1.4.2, gn,k =
n−k+1
k

.
Exercise 1.31 A physical system is made by identical particles. The total energy of
the system is 4E0 where E0 is a given positive constant. The possible energy levels
of each particle are k E0, k = 0, 1, 2, 3, 4. How many different conﬁgurations are
possible if;
(i) the kth energy level is made of k2 + 1 states;
(ii) the kth energy level is made of 2(k2 + 1) states;
(iii) the kth energy level is made of k2 + 1 states and two particles cannot occupy
the same state.
Exercise 1.32 For any non-negative integer n ≥0, deﬁne
xn := x(x + 1)(x + 2) · · · (x + n −1),
xn := x(x −1)(x −2) · · · (x −n + 1).
Prove that
xn = (−1)n(−x)n.
(1.12)
[Hint. Take advantage of (iii) of Proposition 1.3.]
Exercise 1.33 n players participate in a single-elimination tennis tournament.
How many matches shall be played?
Exercise 1.34 You are going to place 4 mathematics books, 3 chemistry books
and 2 physics books on a shelf. Compute how many different arrangements are
possible. What if you want to keep books of the same subject together?

26
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 1.35 A comittee of 4 is to be chosen from a group of 20 people. How
many different choices are there for the roles of president, chairman, secretary,
and treasurer?
Exercise 1.36 How many bytes of N digits exist with more zeroes than ones?
Exercise 1.37 There are 40 cabinet ministers sitting around a circular table. One
seat is reserved for the Prime Minister. In how many ways can the other ministers
seat themselves?
Exercise 1.38 Thirty people meet after a long time and they all shake hands. How
many handshakes are there?
Exercise 1.39 Find the number of solutions to equation x + y + z + w = 15:
(i) in the set of non-negative integers;
(ii) in the set of positive integers;
(iii) in the set of integers such that x > 2, y > −2, z > 0, w > 3.
Exercise 1.40 Find the number of non-negative integer solutions (x, y, z, t) of
x + y + z + t ≤6.

2
Probability measures
The correspondence between Blaise Pascal (1623–1662) and Pierre de Fermat
(1601–1665) on some matters related to card games suggested by the nobleman
and gambler Chevalier de M´er´e is considered to be the birth of probability. The
ﬁrst treatise, De Ratiociniis in ludo aleae, by Christiaan Huygens (1629–1695)
was published in 1647. Other publications followed in the eighteenth century: at
the beginning of the century, the Ars conjectandi by Jacob Bernoulli (1654–1705)
and the Doctrine of Chances by Abraham de Moivre (1667–1754) were published
in 1713 and 1718, respectively. Then the Th´eorie analytique des probabilit´es by
Pierre-Simon Laplace (1749–1827), which appeared in 1812, summarized the
knowledge on probability of the whole century.
The deﬁnition of probability given by Pascal for games of chance is the so-
called classical interpretation: the probability of an event E is the ratio between
the number of successes (the cases in which the event E happens) and the number
of all possible cases:
P(E) := favourable cases
possible cases .
(2.1)
Clearly, the deﬁnition can be criticized: it cannot deﬁne the probability of those
events for which we do not know a priori the number of successes and it does
not make sense when there are an inﬁnite number of possible cases. Generally
speaking, the deﬁnition does not apply to a multitude of cases which we would
like to speak of in probability terms.
Another deﬁnition, given by Richard von Mises (1883–1953), is the frequen-
tist interpretation: the probability of an event is the limit of the frequencies of
successes as the number of trials goes to inﬁnity:
P(E) = lim
n→∞
favourable cases among n trials
n
(2.2)
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

28
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
This deﬁnition can also be easily criticized: how can the limit of a sequence be
computed if the sequence itself is not known? Moreover, the limit may not exist
and in many situations, such as in economics, experiments cannot be repeated.
A further deﬁnition, ﬁrst given by Jacob Bernoulli (1654–1705), is the sub-
jectivist interpretation: the probability of an event E is a number p = P(E) that
measures the degree of belief that E may happen, expressed by a coherent indi-
vidual. The ‘coherence’ is expressed by the fact that if the individual measures
as p ∈[0, 1] the degree of belief that E happens, then 1 −p is the degree of
belief that E does not happen.
Another way to describe the subjectivist interpretation is the bet that an indi-
vidual, according to the information in his possession, regards as a fair cost to pay
(to get paid) in order to get (pay) one if the event E happens and in order to get
(pay) nothing if E does not happen. The coherence is in a fair evaluation of the
probability that E happens: the individual may be either the gambler or the book-
maker but the evaluation remains the same. The following example is from [1].
Example 2.1 Betting on horse racing helps to illustrate the subjectivist point of
view. In ﬁxed-odds betting the bookmaker decides a different bet for each horse.
The gambler, if he or she wins, receives the amount a(1 + q) where a is the
money bet and q is the bet. In other words, the gambler pays a to win a(1 + q)
if the horse bet on wins the race and pays a to get no money if the horse bet on
does not win the race.
Let p be the probability that the horse wins the race. Then the ratio between
a and a(1 + q) must be equal to the ratio between p and 1, i.e. p = 1/(1 + q)
or q = 1
p −1. The game is fair if the sum of the probabilities of all horses in
the race is 1. UNIRE, the public corporation that regulates Italian horse racing,
has ﬁxed an upper bound on the bookmakers bets in order to protect gamblers:
the betting is fair if the sum of the probabilities of the horses is not greater than
1.6, 
i pi ≤1.6.
2.1
Elementary probability
In the classic deﬁnition by Pascal, the probability of an event is the ratio between
favourables cases (successes) and possible ones. Let  be the set of all possible
cases and let A ⊂ be the subset of favourable cases, then
P(A) = |A|
||
∀A ⊂.
Calculus of probabilities thus boils down to a counting problem. In mathematics
nowadays, this probability is known as the uniform distribution on a ﬁnite set.
Example 2.2 In ﬂipping a coin, the possible results are ‘heads’ or ‘tails’. The
set of all possible cases is  = {H, T }. For a fair coin, one has P({T }) =
P({H}) = 0.5, i.e. the uniform probability on a set of two elements.

PROBABILITY MEASURES
29
Example 2.3 When throwing a dice, the possible results are the six faces of the
dice. The set of possible cases is  = {1, 2, 3, 4, 5, 6}. If the dice is fair, then all
the possible results have the same probability 1/6, i.e. the uniform probability
on a set of 6 elements.
Example 2.4 When throwing two dice, the possible results are the couples (i, j),
i, j = 1, . . . , 6, where the ﬁrst and the second components are the result of the
ﬁrst and second dice, respectively. In other words, the set of possible cases is
 := {1, 2, 3, 4, 5, 6}2. If the dice are fair and do not interfere with each other,
then each couple has the same probability, i.e. we have the uniform probability
on a set of 36 elements. For instance, the probability to obtain two sixes or to
obtain (4, 6), that is, 4 on the ﬁrst dice and 6 on the second dice or, to obtain
(6, 4), i.e. 6 on the ﬁrst dice and 4 on the second dice are equal to 1/(36). Notice
that in dice games the outcomes are not ordered, thus the probability to obtain
six on both dices is 1/(36) while the probability of obtaining one 4 and one 6,
i.e. either (4, 6) or (6, 4), is 1/(18).
Example 2.5 Draw a number among ninety available ones. The set of possible
cases is  = {1, 2, . . . , 90}. In a lottery game ﬁve different numbers are drawn;
the order of the drawing is not taken into account. If the drawing is fair, each
subset of 5 elements of  has the same probability. The set of possible cases is
the family of all the subsets of  with 5 elements, and the probability of each
element of the family is 1/
90
5

.
Example 2.6 We draw 5 numbers among a set of 90 different ones. The order of
the drawing is taken into account, so that the possible cases are all the ordered
lists of ﬁve pairwise different integers in 1, 2, . . . , 90. Thus the possible cases
are 5!
90
5

= 90!
85!. If the drawing is fair, then any drawn list has the same proba-
bility p = 1/(5!
90
5

) ≃1.9 · 10−10.
2.1.1
Exercises
Exercise 2.7 In a group of n people, each person writes his or her name on a
card and drops the card in a common urn. Later each person gets one card from
the urn. Compute the probability that one (and only one) person in the group gets
the card with his or her own name. Compute the probability that no one in the
group gets the card with his or her own name.
Exercise 2.8 Five balls are drawn from an urn containing 90 balls labelled from
1 to 90. A player can play each of the following gambles:
• He or she tries to guess three of the ﬁve drawn balls.
• He or she tries to guess four of the ﬁve drawn balls.
• He or she tries to guess all ﬁve drawn balls.
Compute the probability of winning in each of the cases above.

30
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 2.9 k items of a stock of N objects are subject to testing. If only one item
of the stock is defective, compute the probability that the test detects that item. If
two items are defective, compute the probability that the test identiﬁes at least one
of them and both of them.
Exercise 2.10 (Birthday paradox) A group of n people meets at a party,
n ≤365. Assuming that all the years are 365 days long (i.e. 29th February does
not exist) and that childbirth is equiprobable on each day of the year, compute
the probability that the n people at the party are born on n different days of the
year.
Solution. We label the people of the party from 1 to n and compute the number
of favourable cases. Person ‘1’ can be born on any day of the year, person ‘2’ can
be born on any of the other 364 days of the year, person ‘3’ can be born on any of
the other 363 days of the year and so on. Thus the favourable cases are as
many as the collocations of n pairwise distinct objects in 365 pairwise different
boxes with at most one object per box, i.e.
365!
(365 −n)!.
Let us now compute the number of possible cases. Each person can be born
on any of the 365 days of the year, i.e. the possible cases are as many as the
collocations of n pairwise objects in 365 pairwise different boxes with no further
constraint, i.e. there are 365n possible cases. Thus the probability that the n
people at the party are born on n different days of the year is
365!
365n (365 −n)!.
We leave it to the reader to prove that the map
p : n ∈{1, . . . , 365} →
365!
365n (365 −n)! ∈R
is strictly monotone decreasing, that p(n) ≥0.5 if and only if n ≤22 and
p(n) ≥0.1 if and only if n ≤40. In particular, 1 −p(n) ≥0.5 if n ≥23 and
1 −p(n) ≥0.9 if n ≥41: in words, if at least 23 people are at the party, then
the probability that at least two of them are born on the same day of the year is
greater than 0.5, while if there are at least 41 people, then the probability that
at least two of them are born on the same day of the year is greater than 0.9.
Exercise 2.11 An urn contains n balls labelled 1, 2, . . . , n; another urn contains
k balls labelled 1, 2, . . . , k. Assume k ≥n and draw randomly a ball from each
urn. Compute the following:
• The probability that the two balls are labelled with the same number.
• The probability that the two balls are labelled with an even number.
Solution. The set of possible cases is
 := {(a, b) | a ∈{1, . . . , n}, b ∈{1, . . . , k}} .

PROBABILITY MEASURES
31
Clearly the cardinality of  is || = kn. In the ﬁrst case, the set of successes is
A = {(a, a) | a ∈{1, . . . , n}}
with cardinality |A| = min(k, n) = n. Thus P(A) = |A|
|| = 1
k .
In the second case the set of successes is
B =

(2i, 2j)
 i ∈

1, . . . ,
 n
2
!
, j ∈

1, . . . ,
"k
2
#$$
with cardinality |B| =
 n
2
!"k
2
#
so that P(B) = |B|
|| =
 n
2
!"k
2
#
kn
.
Exercise 2.12 An urn contains n balls labelled 1, 2, . . . , n. We randomly draw
two balls. Compute the probability that both balls are labelled with even numbers.
Solution. We are now drawing two balls from the same urn, thus the set 
of all possible cases is the family of the subset of {1, 2, . . . , 20} having exactly
two elements. Thus,
|| =
n
2

= n(n −1)
2
.
Since the balls are randomly drawn, we are dealing with the uniform probability
over a ﬁnite set. The set of favourable cases A is the family of all subsets of

2j | 1 ≤j ≤
 n
2

having two elements. Thus,
|A| =
 n
2

2

and
P(A) = |A|
|| =
⌊n
2⌋
2

n
2
 .
Exercise 2.13 An urn contains 20 white balls, 30 red balls, 10 green balls and
40 black balls. Ten balls are randomly drawn. In each of the following cases
compute the probability of drawing 2 white balls, 3 red balls, 1 green ball and 4
black balls:
• The balls are pairwise distinguishable, i.e. they are labelled 1, 2, . . . , 100.
• The balls can be distinguished only by their colour.
Solution. In the ﬁrst case we draw 10 balls from an urn containing 100 balls
labelled 1, 2, . . . , 100. Thus the set  of all possible events is the family of all
the subsets of {1, . . . , 100} having cardinality 10, i.e.
|| =
100
10

.

32
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
We can assume that the white balls are labelled 1, 2, . . . , 20, the red balls
21, . . . , 50, the green balls 51, . . . , 60 and the black ones 61, . . . , 100.
The set A of successes is the Cartesian product of the following events:
• the set of all the subsets of {1, . . . , 20} made by 2 elements;
• the set of all the subsets of {21, . . . , 50} made by 3 elements;
• the singletons in {51, . . . , 60};
• the set of all the subsets of {61, . . . , 100} made by 4 elements.
Thus
|A| = C20
2 C30
3 C10
1 C40
4 =
20
2
30
3
10
1
40
4

and
P(A) =
20
2
30
3
10
1
40
4

100
10

≃0.407
When the balls can be distinguished only by their colour, we draw 10 balls
from a population of 4 different objects, where each object has multiplicity greater
than or equal to 10; thus we are playing a simultaneous drawing from a multiset,
so the number of possible drawings is
4 + 10 −1
10

=
13
10

= 246.
Moreover, there is only one way to draw 2 white balls, 3 red balls, 1 green ball
and 4 black ones, i.e. there is only one favourable case. Thus the probability of
success is
P(A) =
1
13
10
 =
1
246 ≃0.004.
Exercise 2.14 An urn contains 5 white balls, 8 red balls and 7 black balls, labelled
1, 2, . . . , 20. Two balls are drawn without replacement. Compute the probability
that two balls are painted the same colour.
[≃0.31]
Exercise 2.15 An urn contains 6 white balls, 5 red balls and 9 black balls,
labelled 1, 2, . . . , 20. Three balls are drawn without replacement. Compute the
probability that the three drawn balls are three different colours.
%
9/38 ≃0.24
&
Exercise 2.16 We are given two urns. The ﬁrst urn contains 6 white balls and
4 red ones. The second urn contains 4 white balls and 6 red ones. The balls are
pairwise distinguishable. A ball is randomly drawn from each urn. Compute the
probability that the two balls are the same colour.
[0.48]

PROBABILITY MEASURES
33
Exercise 2.17 Let A be a ﬁnite set, |A| = k. Compute the average cardinality of
its subsets.
Solution. By deﬁnition the average cardinality of the subsets of A is
1
|P(A)|

S⊂P(A)
|S|.
For any j ∈{0, 1, . . . , k}, there are
k
j

subsets with j elements. Thus
1
|P(A)|

S⊂P(A)
|S| = 1
2k
k

j=0
j
k
j

= 1
2k
k

j=1
k
k −1
j −1

= k2k−1
2k
= k
2.
Exercise 2.18 Compute the average number of ﬁxed points in the set of permuta-
tions of n elements. The average number of ﬁxed points is the ratio between the
total number of ﬁxed points in all the permutations and the number of permuta-
tions.
Solution. For each interger j = 0, . . . , n, there are
n
j

dn−j permutations with
j ﬁxed points, see the proof of Proposition 1.10; thus the average number of
ﬁxed points is
1
n!
n

j=0
j
n
j

dn−j.
This ratio can be computed explicitly. We have
1
n!
n

j=0
j
n
j

dn−j = 1
n!n
n

j=1
n −1
j −1

dn−j = 1
n!n
n

j=1
n −1
n −j

dn−j
=
1
(n −1)!
n−1

j=0
n −1
j

dj = (n −1)!
(n −1)! = 1,
see (1.3). Thus the average number of ﬁxed points is 1.
2.2
Basic facts
Independently of the interpretation–and the subjective interpretation is the most
ﬂexible one–one gets to an axiomatic deﬁnition of probability, which is due
to Andrey Kolmogorov (1903–1987) and is based on the deﬁnition of abstract
measure. In this setting, the choice of the set of possible cases, the set family
of possible events and the probability itself are not speciﬁed. The choice will
be made from time to time on the basis of statistical considerations and model
theory. For our purposes, the choice is basically irrelevant: in this volume we
deal with the computation of the probability of events deﬁned by means of other
events whose probability is already known.

34
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
2.2.1
Events
Intuitively, an event is a mathematical proposition to which we assign a number
in [0,1] which is called the probability of the event. In order to make negations
meaningful and to avoid paradoxes, one should consider only predicates, i.e.
mathematical propositions indexed by the elements of a given set . This way,
each predicate deﬁnes a subset
E =

x ∈
 p(x) is true

,
and two predicates are equivalent if they deﬁne the same subset of . For the
sake of clarity, it is better to call an event a subset of  instead of the different
equivalent predicates that deﬁne it.
Logical connectives are thus ‘translated’ into operators between sets: unions,
intersections, subsets and complementary sets. In fact, if p and q are two predi-
cates on  and
A =

x ∈
 p(x) is true

B =

x ∈
 q(x) is true

then
A ∪B =

x ∈
 p(x) or q(x) is true

A ∩B =

x ∈
 both p(x) and q(x) are true

Ac =

x ∈
 p(x) is false

A ⊂B
if and only if
p(x) ⇒q(x)
∀x ∈.
Propositional logic suggests that the set of events E has the following prop-
erties:
(i) ∅∈E.
(ii) If A ∈E, then Ac :=  \ A ∈E.
(iii) If A and B ∈E, then A ∪B ∈E and A ∩B ∈E.
So that, by induction:
(iv) If A1, . . . , An ∈E, then ∪n
i=1Ai ∈E and ∩n
i=1Ai ∈E.
Notice that by (i) and (ii),  ∈E and that, by (iv), if A, B ∈E then A \ B and
B \ A ∈E.
The event ∅(characterized by x ∈∅is always false) is called the impossible
event while the event  (x ∈ is always true) is called the certain event. The
event Ac :=  \ A is called the complementary event of A. Finally, two events
A, B ∈E are said to be incompatible if A ∩B = ∅, i.e. if and only if A ⊂Bc.

PROBABILITY MEASURES
35
Beware, one should resist the temptation to think that every subset of  is an
event, i.e. that the set of all events E is the set of all subsets of . Although this
is possible, and preferable, if  is ﬁnite or denumerable, it leads to contradictions
if  is not denumerable, see Section 2.2.7; for the moment, let us think of E as
a possible proper class of subsets of  with the properties (i)–(iv) above.
A family E of events of  satisfying properties (i), (ii) and (iii) above is
called an algebra of subsets of .
When the family of events E is inﬁnite, we require a stronger property for the
family E: the family of events E is closed with respect to denumerable unions
and intersections, i.e.
(v) For any sequence

Ai

⊂E we have ∪∞
i=1Ai ∈E and ∩∞
i=1Ai ∈E.
This property will bring many further properties that will be shown later.
Clearly this property boils down to (iii) when E is a ﬁnite family. Moreover, by
De Moivre formulas it can be also simpliﬁed to:
(vi) If (ii) holds, then for any sequence

Ai

⊂E either ∪∞
i=1Ai ∈E or
∩∞
i=1Ai ∈E.
We summarize the previous requests in a formal deﬁnition.
Deﬁnition 2.19 Let  be a nonempty set and let P() be the family of all subsets
of .
• An algebra of subsets of  is a family E ⊂P() such that:
(i) ∅∈E.
(ii) If A ∈E, then Ac :=  \ A ∈E.
(iii) If A, B ∈E, then A ∪B ∈E.
• A σ-algebra of subsets of  is a family E ⊂P() such that:
(i) ∅∈E.
(ii) If A ∈E, then Ac :=  \ A ∈E.
(iii) For any sequence

Ai

⊂E we have ∪∞
i=1Ai ∈E.
Notice that if D ⊂P() is a family of subsets of , then the family
S :=
' 
E
 E is a σ-algebra, E ⊃D

is a well deﬁned family of subsets of . It is easy to show that S itself is
a σ-algebra, and is in fact the smallest σ-algebra that includes D. S is called
the σ-algebra generated by D.
Example 2.20 Let  := {1, 2, 3, 4, 5}. Let us describe the algebra E generated
by the subsets E1 = {1, 2} and E2 = {2, 3} of .

36
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Clearly, E1, E2, ∅and  ∈E. Moreover E contains the sets Ec
1 := {3, 4, 5}
and Ec
2 = {1, 4, 5} and their intersections E1 ∩Ec
2, E2 ∩Ec
1. Thus
{1},
{2},
{3},
{4, 5} ∈E.
The singletons {4} and {5} are not in E. The ﬁnite unions of the above sets
must be in E, so that |E| = 24 = 16. More precisely, the subsets of  that are
in E are:
∅,
{1, 2, 3, 4, 5}
{1},
{2},
{3},
{4, 5}
{1, 2},
{1, 3},
{1, 4, 5},
{2, 3},
{2, 4, 5},
{3, 4, 5},
{1, 2, 3},
{1, 2, 4, 5},
{1, 3, 4, 5},
{2, 3, 4, 5}.
Deﬁnition 2.21 We denote by B(Rn) the smallest σ-algebra of subsets of Rn that
contains all the open sets of Rn. The subsets E ∈B(Rn) are called Borel sets of
Rn and B(Rn) is called the Borel σ-algebra of Rn.
In what follows the σ-algebra B(Rn) will play an inportant role.
2.2.2
Probability measures
Let  be the certain event and let E ⊂P() be a σ-algebra. We ‘measure’ the
probability of an event E ∈E by a real number P(E) ∈[0, 1]; in particular, we
assign probability 0 to the impossible event and probability 1 to the certain event
. The subjective interpretation of probability suggests that the map P : E →R
has the following properties:
• If A ⊂B, then P(A) ≤P(B).
• P(Ac) = 1 −P(A).
• If A, B are incompatible, A ∩B = ∅, then P(A ∪B) = P(A) + P(B).
Deﬁnition 2.22 Let  be a nonempty set. A non-negative measure on  is a
couple (E, P) where E ⊂P() is a σ-algebra of subsets of , called the family
of measurable sets and P : E →R+ is a map with the following properties:
(i) P(∅) = 0.
(ii) (Monotonicity) If A, B ∈E and A ⊂B, then P(A) ≤P(B).
(iii) (σ-additivity) For any disjoint sequence

Ai

⊂E we have
P
 ∞
(
i=1
Ai

=
∞

i=1
P(Ai).

PROBABILITY MEASURES
37
If, moreover, P() = 1, we say that the non-negative measure (E, P) is a
probability measure on . In this case E is called a family of events and the
triplet (, E, P) is called a probability space.
Remark 2.23 Notice the following:
• If E is ﬁnite, then (iii) boils down to ﬁnite additivity for incompatible
events.
• If E is inﬁnite, then the sum on the right-hand side is to be understood as
a series with non-negative addenda. σ-additivity is crucial and is natural
in a large number of situations.
• For a non-negative measure (E, μ) we may have μ() = +∞.
Deﬁnition 2.24 Let (, E, P) be a probability space. An event A ∈E such that
P(A) = 0 is said to be almost impossible, an event A ∈E such that P(A) = 1 is
almost sure. If A is a singleton, then A is called an atomic event.
Let (E, P) be a measure on a set . We say that N ⊂ is P-negligible, or
simply a null set, if there exists E ∈E such that N ⊂E and P(E) = 0. Let )E be
the collection of all the subsets of  of the form F = E ∪N where E ∈E and
N is P-negligible. It is easy to check that )E is a σ-algebra which is called the
P-completion of E. Moreover, setting P(F) := P(E) if F = E ∪N ∈)E, then
()E, P) is again a measure on  called the P-completion of (E, P). It is customary
to consider measures as P-complete measures.
2.2.3
Continuity of measures
The following proposition collects some properties that easily follow from Deﬁ-
nition 2.22.
Proposition 2.25 Let (, E, P) be a probability space.
(i) If A ∈E, then 0 ≤P(A) ≤P() = 1.
(ii) If A, B ∈E and A ⊂B, then P(B \ A) = P(B) −P(A).
(iii) If A, B ∈E, then P(A ∪B) + P(A ∩B) = P(A) + P(B).
(iv) If A, B ∈E, then P(A ∪B) ≤P(A) + P(B).
(v) (σ-subadditivity) For any sequence

Ai

⊂E we have
P
 ∞
(
i=1
Ai

≤
∞

i=1
P(Ai).
(2.3)
Another easy consequence of Deﬁnition 2.22 is the law of total probability.

38
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Proposition 2.26 (Law of total probability) Let (, E, P) be a probability space
and let

Di

i ⊂E be a ﬁnite or denumerable partition of , i.e.  = ∪∞
i=1Di and
Di ∩Dj = ∅∀i, j, i ̸= j. Then
P(A) =
∞

i=1
P(A ∩Di)
∀A ∈E.
(2.4)
We ﬁnally point out the following important continuity property of probability
measures.
Proposition 2.27 (Continuity) Let (, E, P) be a probability space and let

Ei

⊂E be a denumerable family of events.
(i) If Ei ⊂Ei+1 ∀i, then ∪∞
i=1Ei ∈E and
P
 ∞
(
i=1
Ei

= lim
i→+∞P(Ei).
(2.5)
(ii) If Ei ⊃Ei+1 ∀i, then ∩∞
i=1Ei ∈E and
P
 ∞
'
i=1
Ei) = lim
i→+∞P(Ei).
(2.6)
Proof. We prove (i). Write E := ∪∞
k=1Ek as
E = E1
(  ∞
(
k=2
(Ek \ Ek−1)

.
The sets E1 and Ek \ Ek−1, k ≥2, are pairwise disjoint events of E. By the
σ-additivity property of measures, we get
P(E) = P(E1) +
∞

k=2
P(Ek \ Ek−1)
= P(E1) +
∞

k=2
(P(Ek) −P(Ek−1)) = lim
k→∞P(Ek).
In order to prove (ii) we point out that from P(E1) < +∞and Ek ⊂E1 ∀k ≥2,
we get P(E1) −P(Ek) = P(E1 \ Ek) for any k ≥2. Moreover, the sets E1 \ Ek
are an increasing sequence of events of . Thus, applying (i) to the family of
events

E1 \ Ek

one gets
P(E1) −lim
k→∞P(Ek) = lim
k→∞P(E1 \ Ek)
= P
 (
k
(E1 \ Ek)

= P(E1) −P
 ∞
'
k=2
Ek

.

PROBABILITY MEASURES
39
2.2.4
Integral with respect to a measure
Given a measure (E, P) on a nonempty set , one deﬁnes an integral with respect
to such measure,
*

f (x) P(dx)
whenever f is in an appropriate class of functions, called random variables, see
Section 3.1. Here we deﬁne only the integral of functions f :  →R that have
a ﬁnite range. For the characteristic function 1E
1E(x) =

1
if x ∈E,
0
if x /∈E
of E ∈E, set
*

1E(x) P(dx) := P(E).
A simple function is a linear combination of characteristic functions of pairwise
disjoint events

Ei

. The integral of a simple function ϕ(x) = n
i=1 ci1Ei(x),
x ∈, is then deﬁned by
*

ϕ(x) P(dx) :=
n

i=1
ciP(Ei).
Integration is a linear operator on the family of simple functions. In fact, the
following holds.
Proposition 2.28 Let ϕ and ψ be simple functions, and let a, b ∈R. Then aϕ(x) +
bψ(x) is a simple function and
*

(aϕ(x) + bψ(x)) P(dx) = a
*

ϕ(x) P(dx) + b
*

ψ(x) P(dx).
Proof. Let ϕ(x) = n
i=1 ci1Ei(x) and ψ(x) = m
j=1 dj1Fj (x) where the sets

Ei

are pairwise disjoint and so are the sets

Fj

. Without any loss of generality,
we can assume that both the families

Ei

and

Fj

are ﬁnite partitions of .
Then

Ei ∩Fj

is a ﬁnite partition of  and
aϕ(x) + bψ(x) = aci + bdj
if x ∈Ei ∩Fj.
Thus aϕ + bψ is a simple function and by the deﬁnition of integral
*

(aϕ(x) + bψ(x)) P(dx) =

i=1,n
j=1,m
(aci + bdj)P(Ei ∩Fj)

40
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
=
n

i=1
aci
m

j=1
P(Ei ∩Fj) +
m

j=1
bdj
n

i=1
P(Ei ∩Fj)
= a
n

i=1
ciP(Ei) + b
m

j=1
djP(Fj)
= a
*

ϕ(x) P(dx) + b
*

ψ(x) P(dx).
A function ϕ having a ﬁnite range, ϕ(x) = n
i=1 ai1Ai(x), where the events

Ai

are not necessarily disjoint, is a linear combination of simple functions.
Proposition 2.28 then yields
*

ϕ(x) P(dx) =
n

i=1
ai
*

1Ai(x) P(dx) =
n

i=1
aiP(Ai),
(2.7)
as for simple functions.
2.2.5
Probabilities on ﬁnite and denumerable sets
2.2.5.1
Probabilities on ﬁnite sets
In Example 2.20 the certain event is  = {1, 2, 3, 4, 5} and the σ-algebra E
generated by the sets E1 = {1, 2}, E2 = {2, 3} is given by all the ﬁnite unions
of
{1},
{2},
{3},
{4, 5}.
In particular, the singletons {4} and {5} are not events. Thus, if (E, P) is a
probability measure on , then P({4}) and P({5}) are not deﬁned. If one is
willing to deﬁne the probability on any subset of , then one of the two following
procedures may be followed:
(i) Deﬁne P({4}) and P({5}) so that P({4}) + P({5}) = P({4, 5}). Obviously
there are inﬁnitely many ways to do that.
(ii) Make  ‘smaller’ so that the events by which the σ-algebra can be gen-
erated by unions only are atomic. In the example we are considering, one
identiﬁes 4 and 5 by setting {A} = {4, 5} and chooses  = {1, 2, 3, A}.
Thus, if  is a ﬁnite set, by applying one of the previous procedures, one
may always assume that all the subsets of  are events, E = P(), so that P(A)
is deﬁned for all A ⊂. In what follows, we always assume that E = P() if
 is ﬁnite, without explicitly stating it.
In particular, assuming  =

x1, . . . , xn

, each singleton

xk

is an atomic
event, with its own probability pk := P(

xk

). We call pk the density of P at xk

PROBABILITY MEASURES
41
and the vector p := (p1, . . . , pn) the mass density of P. Clearly,
pk ≥0 ∀k,
n

k=1
pk = 1
and for any subset A ⊂,
P(A) =

xk∈A
P(

xk

) =

xk∈A
pk.
2.2.5.2
Uniform probability
Let  be a ﬁnite set. If all the atomic events have the same probability, P({x}) =
p ∀x ∈, then, by the additivity property, one gets,
1 = P() =

x∈
p = p ||
i.e.
p =
1
||.
Thus, for any set A ⊂, we may compute
P(A) = p |A| = |A|
||.
This case is called uniform probability on a ﬁnite set and the probability of
the event A coincides with the classical interpretation of probability: the ratio
between the number of favourable events and the number of possible events. Thus
the computation of the probability of an event implies only two counting opera-
tions, see Chapter 1.
Given a ﬁnite set , the everyday-language phrase ‘choose x ∈ randomly’
will here be interpreted as: consider  as equipped with the uniform probability,
i.e. P({x}) :=
1
|| ∀x ∈.
2.2.5.3
Finite Bernoulli process
Assume you perform an experiment which has only two possible results: success,
with probability p, 0 ≤p ≤1, or failure with probability q, q = 1 −p. This is
called a Bernoulli trial of parameter p. Thus the certain event has only two
elements:  = {0, 1}, where 0 is for failure and 1 is for success. The mass
density vector is (q, p).
We then perform the same experiment N times, and assume that each trial
does not interfere with the results of the other trials. This procedure is called a
ﬁnite Bernoulli process. The result of the repeated experiment is a N-tuple of
zeroes and ones, where the kth component is 1 if and only if we get a success at
the kth trial. Thus the certain event is  = {0, 1}N, the set of all binary N-tuples,
which is a ﬁnite set with 2N elements. The probability pv of a given sequence of
results v = (v1, . . . , vN) ∈{0, 1}N depends only on the number of successes (or

42
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
failures) that are in it: if k components of v are equal to 1 and N −k components
of v are equal to zero, then
pv = P({v}) := pkqN−k
The mass density vector p = (pv)v∈{0,1}N , which has 2N components, completely
describes a probability measure on  := {0, 1}N, called a Bernoulli distribution of
parameter p on {0, 1}N and denoted by (P({0, 1}N), Ber(N, p)). We then have
Ber(N, p)(A) :=

v∈A
pv
∀A ⊂{0, 1}N.
Observe that, in the case p = q = 1/2, then pv =
1
2N
∀v ∈{0, 1}N, i.e.
Ber(N, 1/2) is the uniform probability on the set {0, 1}N.
Various examples ﬁt in the ﬁnite Bernoulli process: the repeated ﬂipping of
a coin, games where further matches are played, such as championships, random
walks, the behaviour (up and down) of the stock exchange, medical tests (positive
or negative) or even answers (yes or no) of a population of N individuals.
The meaning of the sentence ‘the trials of the experiment do not interfere one
another’ must be made precise, see Section 4.3.1, and is a crucial request which
must be accurately checked any time one wishes to apply the ﬁnite Bernoulli
process model and its distribution.
Example 2.29 Compute the probability of getting k successes in n trials in a
ﬁnite Bernoulli process.
Let p, 0 ≤p ≤1 be the probability of success in each trial. We want to
compute the probability of the event Ek ⊂{0, 1}n deﬁned by
Ek :=

x ∈{0, 1}n  exactly k components of x are equal to 1

The probability of each atomic event v ∈Ek is P({v}) = Ber(n, p)({v}) =
pk(1 −p)n−k and the cardinality of Ek is
n
k

, hence
P(Ek) =

v∈Ek
P({v}) = pk(1 −p)n−k 
v∈Ek
1 =
n
k

pk(1 −p)n−k.
2.2.6
Probabilities on denumerable sets
Let  be a denumerable set. As in the ﬁnite case, one may always assume that
all the subsets of  are events, E = P(), so that P(A) is deﬁned for all A ⊂.
This will be tacitly assumed if the certain event  is denumerable.
Assume  =

xk

. As in the ﬁnite case, each singleton

xk

is an atomic
event, with its own probability pk := P(

xk

). We call pk the density of P at xk

PROBABILITY MEASURES
43
and the sequence p :=

pk

the mass density of P. Clearly,
pk ≥0 ∀k,
∞

k=1
pk = 1
and for any subset A ⊂,
P(A) =

xk∈A
P(

xk

) =

xk∈A
pk.
A noticeable departure from the ﬁnite case is the following.
Proposition 2.30 If  is denumerable, then there exists no uniform probability
on .
Proof. Assume, by contradiction, that (P(), P) is a uniform probability
measure on , P({x}) = p ∀x ∈. Then σ-additivity yields
1 = P() =

x∈
p = p|| =

+∞
if p > 0,
0
if p = 0,
a contradiction.
If one wants to deﬁne some uniform ‘probability’ P, say, on N, then P cannot
be countably additive. It may be useful, anyhow, to have a uniform and ﬁnitely
additive ‘probability measure’ P : P(N) →R such that a non-negative integer
is even with probability 1/2, or is divisible by 3 with probability 1/3 and so on.
For any E ⊂N we may deﬁne P(E) using the frequentist interpretation
P(E) := lim
n→∞
P(E ∩{1, 2, . . . , n})
n
,
but the limit on the right-hand side may not exist. For instance, let
E := {1, 10, 11, 12, 100, . . . , 129, 1000, . . . , 1299, . . . }
It can be shown that the limit limn P(E ∩{1, . . . , n})/n does not exist.
Another possible deﬁnition of a ﬁnitely additive ‘probability’ on N is due to
Lejeune Dirichlet (1805–1859). For any α > 1, consider the probability measure
(P(N), Pα) (it is a true probability measure) whose mass density is pα(n) :=
1
nα
so that, for any E ⊂N one gets
Pα(E) =

n∈E
1
nα
+ ∞

n=1
1
nα .
Deﬁne P(E) := lim supα→1+ Pα(E). One can easily verify that
P({even numbers}) = 1/2.
More generally, we have the following.

44
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Proposition 2.31 If P(E ∩{1, . . . , n})/n →L as n →∞, then P(E) = L.
We omit the proof. The interested reader may refer to, e.g. [2].
2.2.7
Probabilities on uncountable sets
2.2.7.1
Uniform probability on an interval
Let  = [0, 1] ⊂R. One may think of doing an experiment whose result is a
random number x ∈[0, 1], where each number has the same chance of being
the result to the experiment. In particular, we require any closed interval [a, b]
0 ≤a ≤b ≤1 to be an event whose probability equals its length, P([a, b]) =
b −a. Since the class of the events E must be a σ-algebra, choosing a = b, we get
{a} = ∩n[a, a + 1/n] ∈E and P({a}) ≤1/n ∀n, i.e. P({a}) = 0. Thus the family
of the events E contains the closed intervals, the singletons, hence the open and
the half-closed intervals; moreover, P(]a, b[) = P(]a, b]) = P([a, b[) = b −a.
Since any open set in R can be written as the union of a denumerable family
of open intervals, one gets that open and closed sets in [0,1] are events and so
is any countable union of countable instersections of closed and open sets and
so on. One wishes to choose E as the σ-algebra of all subsets of [0,1], but a
famous example by Giuseppe Vitali (1875–1932) shows that this is not possible:
one cannot deﬁne a σ-additive function on P([0, 1]) which is consistent with the
elementary length for intervals.
It can be shown that one can choose as the family of events the σ-algebra
generated by the closed intervals, called the σ-algebra of Borel sets of [0,1].
In fact, it can be proven that the map P deﬁned on the closed intervals of [0,1]
as P([a, b]) := b −a can be extented to a probability measure (B([0, 1]), P) and
that the extension is unique. The procedure above actually deﬁnes the Lebesgue
measure on [0,1] starting from the elementary measure of the intervals, see
Appendix B.
Notice that each singleton {a} is an event of zero probability. Thus, atomic
events do not provide any information on the probability of the uncountably
inﬁnite events. In other words, the probability measure cannot be described by
its mass density, which is identically zero.
2.2.7.2
Inﬁnite Bernoulli process
Even in problems related to counting, the certain event may not be countable. For
instance, we want to compute the time of the ﬁrst success in the iterated ﬂipping
of a coin, i.e. the ﬁrst time we get a head. We assume that the trials do not interfere
with each other. In principle, we may get the ﬁrst head after an arbitrary number
of coin ﬂips or we may even never get it. Thus we must consider an inﬁnite
number of coin ﬂips or, more generally, a denumerable number of Bernoulli
trials that do not interfere with each other. The process of doing inﬁnitely many
Bernoulli trials of parameter p that do not interfere with each other is called an
inﬁnite Bernoulli process.

PROBABILITY MEASURES
45
Let 1 and 0 denote the success (head) or the failure (tail) in each trial. Then
the certain event  is the set of all sequences with values in {0, 1}
0, 0, . . . , 0, 1, . . . , 1, 0, 1, 0 . . . .
which we denote as  = {0, 1}∞. It is worth recalling that {0, 1}∞is also the
set of all maps a : N →{0, 1}, or the set of all sequences of binary digits
00 . . . 01 . . . 1000 . . . ,
which form an uncountable set (one may prove this fact by means of the Cantor
diagonal process). To be more precise, the following holds.
Proposition 2.32 {0, 1}∞has the cardinality of the continuum, i.e. the same car-
dinality of R.
Proof. Consider the map T : {0, 1}∞→[0, 1] deﬁned as
T (

an

) :=
∞

i=1
ai
2i .
(2.8)
Clearly, T :  →[0, 1] is surjective, since for any x ∈[0, 1[, the binary sequence
of
x,
x = 0, a0a1a2 · · ·
is
such
that
T (

an

) = x
and,
for

an

=
{1, 1, 1, 1, 1, . . . } we have T (

an

) = 1. Thus, the cardinality of {0, 1}∞
is greater than or equal to the cardinality of [0, 1], i.e. of the continuum.
T is not injective since there are binary sequences that give the same real
number (e.g. 0, 00001111111 · · · = 0, 00010000 . . . ). These sequences are con-
stant for large enough n’s hence they form a denumerable set. Thus, {0, 1}∞has
the same cardinality of [0, 1], i.e. of the continuum.
We want to deﬁne a probability measure on {0, 1}∞related to the Bernoulli
distributions Ber(n, p) constructed by means of the ﬁnite Bernoulli process.
Intuitively, n-tuples of trials must be events. This cannot be imposed as it is
since n-tuples are not sequences, so we proceed as follows. To any binary n-tuple
a = (a1, . . . , an) and any set J = (j1, . . . , jn) of index there corresponds a set
of sequences of trials, the ‘cylinder’,
EJ,a :=

x ∈{0, 1}∞ xji = ai ∀i = 1, . . . , n

that is the set of all sequences of trials that at step jk have value ak. We require
that the sets EJ,a ⊂{0, 1}∞are events, and that the probability of the cylinders
Ej,a in {0, 1}∞is the same as the probability of the n-tuple a in the ﬁnite
Bernoulli process, i.e.
P(EJ,a) := Ber(n, p)({a}) = pkqn−k
if a contains k ones and n −k zeroes.

46
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Consider the σ-algebra E generated by the cylinders. It can be shown that the
map EJ,a →P(EJ,a) extends from the family of cylinders to E as a probability
measure (E, Ber(∞, p)) and that this extension is unique, see Appendix B.
(E, Ber(∞, p)) is the Bernoulli distribution of parameter p on the set of binary
sequences.
Notice that we are close to the construction of the Lebesgue measure on R.
In fact, if we ‘identify’ real numbers in [0, 1] with their binary representation
through the map T in (2.8), then E agrees with the family of Borel sets B([0, 1])
and Ber(∞, 1/2) agrees with the Lebesgue measure L1 on the interval [0, 1].
For a precise statement, see Theorem 3.43.
Example 2.33 The probability of getting successes only in an inﬁnite Bernoulli
process of parameter p < 1 is zero.
Let v = (1, 1, . . . , ). We want to compute P({v}) := Ber(∞, p)({v}). Let
An =

x = (xi) ∈{0, 1}∞| xi = 1 ∀i = 1, . . . , n

. Then
{v} =
∞
'
n=1
An
and
An ⊃An+1.
Thus {v}, being a countable intersection of events, is an event and P({v}) =
limn→∞P(An). Let p be the probability of success in each trial. If p = 1, then
P(An) = 1 ∀n so that P({v}) = 1. If p < 1, then P(An) = pn so that P({v}) = 0.
Proceeding as in Example 2.33, one can prove that any singleton {v} ∈
{0, 1}∞is an almost impossible event for the Bernoulli distribution (E,
Ber(∞, p)) of parameter p if 0 < p < 1, i.e. {v} ∈E and Ber(∞, p)({v}) = 0.
2.2.8
Exercises
Exercise 2.34 Show that B(R) is the smallest σ-algebra generated by one of the
following families of sets:
• the closed sets;
• the open intervals;
• the closed intervals;
• the intervals [a, b[, a, b ∈R, a < b;
• the intervals ]a, b], a, b ∈R, a < b;
• the closed half-lines ] −∞, t], t ∈R.
Solution. [Hint. Show that any open set can be written as the union of a
countable family of intervals.]

PROBABILITY MEASURES
47
Exercise 2.35 A multiple choice test comprises 10 questions. Each question has
four possible answers: one is right, three are wrong. To pass the test the candidate
must correctly answer at least eight questions. Compute the probability of passing
the test if the candidate answers each question randomly.
Solution. The test is a ﬁnite Bernoulli process of 10 Bernoulli trials, and the
probablity of success is p = 0.25 at each trial. Let Ek be the event ‘the candidate
answers k questions correctly’. Then we want to compute the probability of
E =

x ∈{0, 1}10  there are at least 8 successes

= E8 ∪E9 ∪E10.
Thus,
P(E) = P(E8) + P(E9) + P(E10) =
10

k=8
10
k

(.25)k(.75)10−k ≃0.0004.
Exercise 2.36 It is more probable to obtain a 6 by throwing a single dice 4 times
than obtaining a double 6 by throwing two dice 24 times. This is the remark
made by the nobleman and hard gambler Chevalier de M´er´e to Pascal Blaise
(1623–1662) around 1650. The following (wrong) consideration in a letter from
Pascal to Pierre de Fermat (1601–1665), dated 1654, seems to reject the empiric
remark of de M´er´e: the probability of obtaining a 6 when throwing a dice is 1/6;
the probability of getting a double 6 when throwing two dice is 1/36; since 4/6 =
24/36, the two events have the same probability. Find the mistake.
Solution. At each throw of a dice, the probability of failure is 5/6, so that the
probability of 4 failures in 4 throws is (5/6)4; hence the probability of at least 1
success in 4 throws is
1 −
5
6
4
≃0.5177.
Similarly the probability of never getting a double 6 in 24 throws of two dice is
(35/36)24, so that the probability of getting at least a double 6 is
1 −
35
36
24
≃0.4914,
which is less than the previous probability.
In general, the probability of at least one success in n trials is
1 −(1 −p)n,
not np, as Pascal thought. Notice that if p is small enough, then 1 −(1 −p)n
and np are quite close since 1 −(1 −p)n = np + O(p2) as p →0.
Exercise 2.37 Compute the probability of getting the ﬁrst success at the kth trial
of a Bernoulli processs of n trials, n ≥k ≥1.

48
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Solution. Let p be the probability of success in a single trial, 0 ≤p ≤1.
Clearly, it sufﬁces to play k trials and compute the probability of the k-tuple
x = (0, 0, 0, . . . , 0, 1), i.e.
P({x}) = (1 −p)k−1p.
Exercise 2.38 Compute the probability of getting k failures before obtaining the
ﬁrst success in a Bernoulli process of trials.
Solution. Let p be the probability of success in a single trial, 0 ≤p ≤1.
It sufﬁces to play k + 1 trials and compute the probability of the (k + 1)-tuple
x = (0, 0, 0, . . . , 0, 1). Therefore,
P({x}) = (1 −p)kp.
Exercise 2.39 Compute the probability of getting k failures before obtaining the
nth success in a Bernoulli process of trials.
Solution. Let p be the probability of success in a single trial, 0 ≤p ≤1. It
sufﬁces to play n + k trials to get the answer. We want to compute the probability
of the event E containing all the sequences of {0, 1}n+k with k zeroes and n ones,
whose (n + k) component is one. Thus |E| =
n+k−1
k

, cf. Section 1.4.2, so that
P(E) =
n + k −1
k

(1 −p)kpn =
−n
k

pn(p −1)k.
Exercise 2.40 Two players A and B play a 7 rubber game. In each match A wins
with probability p and loses with probability 1 −p. Compute the following:
• The probability that the game ends after the fourth, the ﬁfth, the sixth or the
seventh match.
• The probability that A wins the game.
Exercise 2.41 (Rubber games) Two players A and B play a series of fair matches
until one of them wins s matches. After some time A has won a matches and B
has won b matches and a, b < s. Compute the probability that A wins the game.
Solution. A must obtain a further s −a successes before obtaining a further
s −b failures. Let E be this event. For any k = 0, 1, . . . , s −b −1, let Ek be the
event ‘A obtains k failures before obtaining s −a successes’. The events

Ek

are pairwise disjoint and E = E0 ∪E1 ∪E2 . . . Es−b−1. Thus,
P(E) =
s−b−1

k=0
P(Ek) =
s−b−1

k=0
s −a + k −1
k

(0.5)k(0.5)s−a.

PROBABILITY MEASURES
49
Exercise 2.42 (Production lines) In a production line, each produced item has
probability 0.03 of being faulty, independently of all the other produced items.
Compute the following:
• The probability that 3 items out of 100 are faulty.
• The probability that the quality control inspector ﬁnds the ﬁrst faulty item
when inspecting the 15th item.
• The probability that the quality control inspector ﬁnds 100 nonfaulty items
before ﬁnding the ﬁrst faulty one.
• The probability that the quality control inspector ﬁnds at least 5 faulty items
when checking a stock of 200 produced items.
• The probability that the quality control inspector ﬁnds exactly 5 faulty items
when checking a stock of 200 produced items.
Exercise 2.43 We repeatedly throw a fair dice and count how many times we
obtain a 6. What is the probability of having to throw the dice more than 8 times
before getting a 6?
Solution. We need to consider the inﬁnite Bernoulli process and the Bernoulli
distribution P of parameter p = 1/6. For any k let Ek be the event ‘The ﬁrst
success is at the kth throw’. The events Ek, k ≥0, are pairwise disjoint and
we need to compute the probability of E := ∪∞
k=9Ek. Thus, by the σ-additivity
property, we get
P(E) =
∞

k=9
P(Ek) =
∞

k=9
p(1 −p)k−1 = p
∞

k=8
(1 −p)k
= p(1 −p)8
∞

k=0
(1 −p)k = p(1 −p)8
1
1 −(1 −p) = (1 −p)8.
Another procedure is the computation of the complementary event ‘Either there
is a success during the ﬁrst 8 trials or there never is a success’. The probability
of never obtaining a success is zero, see Example 2.33. Thus,
P(E) = 1 −P(Ec) = 1 −
8

k=1
P(Ek) = 1 −
7

k=0
p(1 −p)k
= 1 −p1 −(1 −p)8
1 −1 + p
= (1 −p)8.
Exercise 2.44 Compute the probability of having to throw a fair dice more than
50 times in order to obtain 6 eight times.

50
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Solution. We consider the inﬁnite Bernoulli process and the Bernoulli
distribution P of parameter p = 1/6. We must compute the probability of
having n = 8 successes, at least k = 51 −n = 43 failures and that the last trial
is a success. For any k let Ek be the event ‘We obtain k failures before getting
8 successes’. We want to compute the probability of the event E = ∪∞
k=43Ek,
hence, see Exercise 2.39,
P(E) =
∞

k=43
8 + k −1
k

p8(1 −p)k = p8
∞

k=43
8 + k −1
k

(1 −p)k
Since
∞

k=0
8 + k −1
k

(1 −p)k =
∞

k=0
−8
k

(−(1 −p))k
= (1 −1 + p)−8 = p−8,
i.e.
p8
∞

k=0
8 + k −1
k

(1 −p)k = 1,
we ﬁnally get
P(E) = 1 −p8
42

k=0
8 + k −1
k

(1 −p)k.
Exercise 2.45 Two players A and B ﬂip in turn a fair coin. Player A is the ﬁrst
to play. The winner is the one who obtains the ﬁrst head. What is the probability
of the event ‘A wins the game’?
Solution. We consider the Bernoulli distribution of parameter p = 0.5. It is
evident that A has more chances than B of winning the game, in fact, the probabil-
ity of the event ‘A wins the game at the ﬁrst ﬂipping’ is 1/2. Let Ek be the event
‘A wins the game at the kth ﬂipping’. Since A does only the odd ﬂippings, then
k must be odd, k = 2h + 1. The probability of obtaining the ﬁrst success at the
(2h + 1)th trial is P(E2h+1) = (1 −p)2hp. Since the events

E2h+1

are pairwise
disjoint and the event ‘Player A wins the game’ is E = ∪hE2h+1, we obtain
P(E) =
∞

h=0
P(E2h+1) = p
∞

h=0
(1 −p)2h
= p
∞

h=0

(1 −p)2h
=
p
1 −(1 −p)2 =
1
2 −p.
Since p = 0.5, player A wins the game with probability 2/3. Notice that
1
2−p > 1
2 for any p, 0 < p < 1, so that, however unbalanced the coin is,
player A has always more chances than player B.

PROBABILITY MEASURES
51
2.3
Conditional probability
In this section we introduce the notion of conditional probability, a very useful
tool when we want to compute the probability that two or more events are veriﬁed.
2.3.1
Deﬁnition
Deﬁnition 2.46 Let (, E, P) be a probability space and let A, B ∈E such
that P(B) > 0. The conditional probability of A given B, denoted by P(A | B)
is deﬁned as
P(A|B) := P(A ∩B)
P(B)
.
(2.9)
P(A|B) is the probability of A when B is taken as the certain event. In fact, let
PB : A ∈E →P(A|B) ∈[0, 1]. Then (E, PB) is a new measure on  such that
PB(B) = 1 and, more generally, PB(C) = 1 ∀C ∈E such that C ⊃B.
Example 2.47 Throw a fair dice. The probability of the event A =‘we get 6’ is
1/6, the probability of the event B =‘we get an even number’ is 3/6 = 1/2 and
the probability of getting 6 knowing that we get an even number is
P(A|B) = P(A ∩B)
P(B)
= P(A)
P(B) = 1/6
1/2 = 1/3.
By deﬁnition P(A|B) can be used to compute P(A ∩B). In fact, formula
(2.9) rewrites as the multiplication formula
P(A ∩B) = P(A|B)P(B)
(2.10)
if P(B) > 0. If P(B) = 0, P(A|B) is not deﬁned; to make (2.10) meaningful
also in the case P(B) = 0, one sets P(A|B)P(B) = 0 if P(B) = 0.
Let

Di

be a partition of . Then the total probability formula P(A) =
∞
i=1 P(A ∩Di) reads
P(A) =
∞

i=1
P(A | Di)P(Di).
(2.11)
Example 2.48 Two cards are picked from a deck of 40. Compute the probability
of picking two aces. We already know that the required probability is the ratio
between successful and possible cases, i.e.
4
2

/
40
2

=
1
130.
Another procedure is the following. Let A and B be the events ‘The ﬁrst
picked card is an ace’ and ‘The second picked card is an ace’, respectively. Then
we must compute P(A ∩B). Clearly, P(A) = 4/40 = 1/10 and the probability
of the second card being an ace, given that the ﬁrst card was an ace also, is 3/39.
Thus, by the multiplication formula,
P(A ∩B) = P(A|B)P(B) =
4 · 3
40 · 39 =
1
130.

52
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
We conclude observing that formula (2.10) can be iterated. For three events
A, B and C with P(B ∩C) > 0 we have
P(A ∩B ∩C) = P(A|B ∩C)P(B ∩C)
= P(A|B ∩C)P(B | C)P(C);
(2.12)
for a ﬁnite family

A1, A2, . . . , An

of events, such that P(A2 ∩· · · ∩An) > 0
one gets
P(A1 ∩A2 ∩· · · ∩An) =P(A1 | A2 ∩· · · ∩An)
· P(A2 | A3 ∩· · · ∩An)
· P(A3 | A4 ∩· · · ∩An)
· · ·
· P(An−1 | An)P(An).
(2.13)
2.3.2
Bayes formula
Let (, E, P) be a probability space and let A, B ∈E. If both P(A) and P(B)
are positive, then
P(A|B)P(B) = P(A ∩B) = P(B ∩A) = P(B|A)P(A);
since
P(A) = P(A ∩B) + P(A ∩Bc) = P(A|B)P(B) + P(A|Bc)(1 −P(B))
we get the formula due to Thomas Bayes (1702–1761).
Proposition 2.49 (Bayes formula) Let (, E, P) be a probability space and let
A, B ∈E such that P(A)P(B)P(Bc) > 0. Then
P(B|A) =
P(A|B)P(B)
P(A|B)P(B) + P(A|Bc)(1 −P(B)).
In other words, one can compute the probability of B given A in terms of the
probability of A given B and the absolute probability of B.
Example 2.50 Clinical tests are a classical example. Clinical tests detect indi-
viduals ill with a disease. The test gives a result that can be either positive or
negative. In order to qualify the ability of detecting ill individuals, an ‘unbiased
sample’ of a population is examined and divided into two groups of healthy and
ill individuals, respectively. Individuals of the sample are then subjected to the
test. Let α and β be the percentages of positive results for ill and healthy individ-
uals, respectively. Since the sample is assumed to be representative of the whole
population, we assume α and β to be the percentages of positive results for all

PROBABILITY MEASURES
53
the ill individuals and all the healthy individuals of the population, respectively.
Denoting by H and I the sets of healthy and ill individuals, respectively, and
denoting by P the set of individuals that would get a positive result in the test,
we assume
α := P(P |I),
β = P(P |H);
From α, β and the percentage x := P(I) of ill individuals, Bayes formula allows
us to compute the probability that an individual is ill given a positive result was
obtained:
P(I|P) =
P(P |I)P(I)
P(P |I)P(I) + P(P |H)(1 −P(I)) = α
1
(α −β) + β
x
.
(2.14)
If the test is a good one, then α is close to 1 and β is close to zero. Formula
(2.14) shows how, in practice, the test loses efﬁcacy in the ﬁeld with respect to
the quality shown in the laboratory. In fact, if x = β one gets
1
(α −β) + β
x
≃
1
α −β + 1 ≃1
2.
If the test is to be helpful in detecting ill individuals, then the ratio β/x =
P(P |H)/P(I) must be small.
Of course, in the previous example, one can replace the population with the
items produced in a production process, where healthy individuals are replaced
by non-faulty items and ill individuals correspond to faulty items. The clinical
test is, of course, substituted by a quality control. The mathematics involved is
exactly the same. Knowing the probability that the test has of detecting known
faulty and non faulty items and the percentage of faulty items, we can compute
the probability that the test is able to detect the faulty items.
Example 2.51 In modelling an email spam ﬁlter, one needs to compute the proba-
bility that the messages containing a given word are spam messages. Let M be the
set of spam messages and let W be the set of the messages that contain the given
word. We want to compute P(M|W). By Bayes formula, it sufﬁces to know:
(i) P(W|M) and P(W|Mc), i.e. the probabilities that the word appears in spam
messages and in non-spam messages, respectively. These probabilities can
be obtained by a statistical analysis on the messages already arrived.
(ii) P(M), the probability that a message is a spam one. This is an absolute
number that can be found in the net or via a statistical analysis of the
trafﬁc.
We then get
P(M|W) = P(W|M)
P(M)
P(W|M)P(M) + P(W|Mc)(1 −P(M)).

54
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
2.3.3
Exercises
Exercise 2.52 An urn contains 3 white balls and 5 red balls. The balls are pairwise
distinguished, for example by numbering them. Two balls are extracted randomly.
• Compute the probability of drawing 2 red balls.
• Compute the probability of drawing 2 red balls, given that at least one is
red.
Solution. For the ﬁrst question, the certain event  is given by all the subsets
having cardinality 2 of a set of 8 elements, thus
|| =
8
2

= 28.
The favourable event A is in a one-to-one correspondence with the family of the
subsets of cardinality 2 of a set of 5 elements, i.e.
|A| =
5
2

= 10,
P(A) = |A|
|| = 10
28 = 5
14.
For the second question, we take advantage of Bayes formula: Let B be the
event ‘At least one of the extracted ball is red’. One gets
|B| = C5
2 + C5
1C3
1 =
5
2

+
5
1
3
1

= 10 + 15 = 25,
P(B) = |B|
|| = 25
28.
Thus
P(A|B) = P(A ∩B)
P(B)
= P(A)
P(B) = |A|
|B| = 10
25 = 2
5.
Find the mistake. In the second question the certain event is smaller. In fact,
since one of the drawn balls is red, the problem reduces to a simpler one: we
may think that the urn contains 3 white balls and 4 red ones and that we are to
extract only one ball. The successful cases are the ones where the drawn ball
is red. Denote by ) the new certain event and let )
A be the new event of the
favourable cases
|)| =
7
1

= 7,
|)
A| =
4
1

= 4.
Thus,
P()
A) = 4
7.
This wrong answer is in fact the right answer for another question. Which
one?

PROBABILITY MEASURES
55
Exercise 2.53 We throw two fair dice:
• Compute the probability that the sum of the two dice is at least 7.
• On one of the two dice we get a 4. Compute the probability that the sum of
the two dice is at least 7.
Solution. The certain event  is the set of all the possible results of the
throwing:  = {1, . . . , 6}2, || = 36. The event of successes is
A = {(i, j) ∈: i + j ≥7}
whose cardinality is |A| = 1 + 2 + . . . + 6 = 6 · 7
2
= 21. Since the dice are fair,
the probability measure P on  is the uniform one. Thus,
P(A) = |A|
|| = 21
36 = 7
12.
In the second case, it is known that on one of the two dice we get a 4. Consider
the event
B =

(i, j) ∈
 i = 4 or j = 4

.
Bayes formula then gives
P(A|B) = P(A ∩B)
P(B)
= |A ∩B|
|B|
= 7
11.
Find the mistake. In the second question the certain event can be reduced
from  to a smaller one. In fact, since on a dice we get a 4, we may assume
we are throwing only a dice. The successful event is ‘We get a 3 or more on the
thrown dice’. Thus, denoting by ) the new certain event and by )
A the successful
event, we get
|)| = 6,
|)
A| = 4,
so that
P()
A) = 4
6 = 2
3.
Exercise 2.54 An urn contains w white balls and r red balls. Compute the prob-
ability of drawing one white ball in one drawing only. Assume you do not know
the colour of the ﬁrst drawn ball. Without replacing it, you draw another ball.
Compute the probability of drawing a white ball in this second drawing.
Solution. In this case, the certain event is the fact that two balls are drawn
and each of them can be either red or white. So we take  = {0, 1}2 where the
ith component of z = (z1, z2) ∈ is zero if the ith drawn is white and 1 if the
ith drawn is red. Thus the event ‘The ﬁrst ball is white’ is associated with
W1 = {(0, 0), (0, 1)}

56
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
so that P(W1) := w/(w + r). In fact, when we draw ‘randomly’ the ﬁrst ball we
are performing a Bernoulli trial where the probability of success is w/(w + r).
Similarly, the event ‘The ﬁrst ball is red’ is associated with
R1 = {(1, 0), (1, 1)} =  \ W1
so that P(R1) = 1 −P(W1) = r/(r + w). We are asked to compute the proba-
bility of the event ‘The second ball is white’, which is is associated with
W2 = {(0, 0), (1, 0)}.
Once we have drawn the ﬁrst ball, two cases may occur. If the ﬁrst ball is white,
then we are left with w −1 white balls and r red balls, i.e.
P(W2 | W1) =
w −1
w + r −1,
and, similarly,
P(W2 | R1) =
w
w + r −1.
The law of total probability then yields
P(W2) = P(W2 ∩W1) + P(W2 ∩R1)
= P(W2|W1)P(W1) + P(W2|R1)P(R1)
=
w −1
w + r −1
w
w + r +
w
w + r −1
r
w + r =
w
w + r .
Notice that, without any knowledge of the colour of the ﬁrst drawn ball, the
probability of drawing a ball of the same colour in the second drawing remains
the same. Thus, iterating the result, we get the following: without any knowledge
of the colour of the previously drawn balls, the probability remains the same also
in the following drawings [as far as the drawn balls are less than min(r, w)].
Exercise 2.55 Two submarines comunicate with each other by Morse code, a
binary code whose two letters are dots and dashes. The operator of the ﬁrst sub-
marine sends a message where 45% of the letters are dashes.
The operator of the second submarine receives dashes and dots correctly with
probability 96% and 98% , respectively. Compute the following:
• The probability that the operator of the second submarine receives a dash.
• The probability that the ﬁrst operator has sent a dash, given that the second
operator has received a dash.
• The probability that the ﬁrst operator has sent a dot given that the second
operator has received a dot.

PROBABILITY MEASURES
57
Solution. For a single bit of the comunication the following cases may occur:
• When considering its transmission, the letter can be either a dot (0) or a
dash (1).
• When considering its reception, the letter can be either a dot (0) or a dash
(1).
Thus we may identify the certain event with  = {0, 1}2 where the ﬁrst compo-
nent of z = (z1, z2) ∈ is 0 if a dot is transmitted and is 1 if a dash is transmitted,
and the second component z2 is 0 if a dot is received and 1 otherwise. Thus, the
event ‘A dot is transmitted’ is associated with
S0 = {(0, 0), (0, 1)},
the event ‘A dash is transmitted’ is associated with
S1 = {(1, 0), (1, 1)} =  \ S0
while the events ‘A dot is received’ and ‘A dash is received’ are associated with
R0 = {(0, 0), (1, 0)}
and
R1 = {(0, 1), (1, 1)} =  \ R0,
respectively. Assumption tells us that P(S1) := 0.45 so that P(S0) = 1 −
P(S1) = 0.55 and that P(R0 | S0) = 0.98 and P(R1 | S1) = 0.96.
We are asked to compute P(R1), P(S1|R1) and P(S0|R0). We have
P(R1) = P(R1|S0)P(S0) + P(R1|S1)P(S1)
= (1 −P(R0|S0))P(S0) + P(R1|S1)P(S1)
= (1 −0.98)(0.55) + (0.96)(0.45) = 0.443,
P(S1|R1) = P(S1 ∩R1)
P(R1)
= P(R1|S1)P(S1)
P(R1)
≃0.975,
P(S0|R0) = P(S0 ∩R0)
P(R0)
= P(R0|S0)P(S0)
1 −P(R1)
≃0.968.
Exercise 2.56 We have n urns, labelled 1 to n. The ith urn contains i white
balls and n + 1 −i red balls. Two balls are randomly drawn from a randomly
chosen urn:
• Compute the probability of drawing a white ball and a red one.
• Compute the probability of having chosen the ith urn given that a white
ball and a red ball are drawn. Which urn has the maximum probability of
having been chosen?

58
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Solution. Here the certain event is that fact that one urn is chosen and two
balls are drawn from that urn, so we identify the certain event with
 := {1, . . . , n} ×

z = (z1, z2) | zi ∈{0, 1}

where zj = 0 if the jth drawn ball is red and zj = 0 if the jth drawn ball is
white.
Denote N := {1, . . . , n} and Z = {0, 1}2 so that  = N × Z. We know that
the urn is ‘randomly chosen’, i.e.
P({i} × Z) = 1
n
∀i = 1, . . . , n.
The event ‘A white ball and a red ball are drawn’ is associated with
A := N × {(0, 1), (1, 0)}
Since the balls are ‘randomly’ drawn, we know that
P(A | Ui) =
i
1
n+1−i
1

n+1
2

= 2i(n + 1 −i)
n(n + 1)
.
Thus, since A = ,n
i=1

A ∩Ui

, we get
P(A) =
n

i=1
P

A ∩Ui

=
n

i=1
P

A|Ui

P(Ui)
=
n

i=1
2i(n + 1 −i)
n(n + 1)
1
n = n + 2
3n .
To answer the second question, we apply Bayes formula, hence
P(Ui|A) = P(A|Ui)P(Ui)
P(A)
= 2i(n + 1 −i)
n(n + 1)
1
n
3n
n + 2 =
6i(n + 1 −i)
n2(n + 1)(n + 2).
The urn with the maximum probability of having been chosen is Ui where
i =

k, k + 1
if n = 2k,
k + 1
if n = 2k + 1.
Exercise 2.57 A ﬁrm produces a certain instrument. The failure rate in the pro-
duction process is 8%. A sample of the produced instruments is subjected to a
quality check. Eighty per cent of the faulty instruments fail the quality check,
while 1% of the nonfaulty instruments fails the check. Compute the probability
that an instrument that passed the test is faulty.

PROBABILITY MEASURES
59
Exercise 2.58 We have three cards. Both sides of one card are black, both sides
of another card are red and the other card has one black side and one red side.
We choose a card: one of its side is red. Compute the probability of having chosen
the card with two red sides.
Exercise 2.59 An urn a contains 2 red balls and 1 black ball. Another urn, b,
contains 3 red balls and 2 black ones. An urn is randomly chosen and a ball is
randomly drawn from that urn. Compute the probability of having drawn a black
ball. Given that you have drawn a black ball, what is the probability of having
chosen urn a?
Exercise 2.60 In a TV quiz show, the host shows three doors to a contestant
and states that behind one of the doors there is a prize. The contestant randomly
chooses a door and marks it. Then the host, who knows where the prize is, opens
a door that was not marked by the contestant and that does not hide the prize. The
host now asks the contestant whether he would like to change his choice. What
should the contestant do?
Exercise 2.61 A ﬁrm produces two kinds of bolt. Bolts of type A are 70% of the
production. Only 95% of them pass the quality check and can be sold, while only
80% of bolts of type B pass the check and can be sold. What percentage of the
production is ﬁt to be sold?
[90.5%]
Exercise 2.62 A web server may be either working or broken. When broken, it
cannot be accessed. Even when it is working, an access attempt may fail, due to
web congestion (which the web server cannot control). Assume the probability
that the server is working is 0.8 and that each access attempt is successful with
probability 0.9. Compute the following:
(i) The probability that the ﬁrst access attempt fails.
(ii) The probability that the web server is working, given that the ﬁrst access
attempt has failed.
(iii) The probability that the second attempt fails, given that the ﬁrst attempt has
failed.
(iv) The probability that the server is working, given that two consecutive access
attempts have failed.
Exercise 2.63 Assume that each vertex of a cube may be coloured in blue with
probability p, independently of the other vertices. Let B be the event ‘All the
vertices of at least one face of the cube are coloured in blue’. Compute;
(i) The probability of B given that 5 vertices are blue.
(ii) The probability of B.

60
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 2.64 We are given 3 urns a, b and c. Urn a contains 5 red balls, 5 ﬁve
white balls and 5 black ones; urn b contains 3 red balls, 5 white balls and 7
black balls; urn c contains 6 red balls, 7 white balls and 2 black balls. Two balls
are randomly drawn by one randomly chosen urn (no replacement occurs). Given
that the drawn balls are both black, compute the probability that they were drawn
from urn b.
%
21/32
&
Exercise 2.65 We are given six urns, which we label as Uj j = 1, . . . , 6. Each urn
Uj contains j red balls and 6 −j black balls. An urn is selected. The probability
of selecting the urn Uj is proportional to j. A ball is randomly drawn from the
selected urn. Compute the probability that the drawn ball is red.
%
13/18
&
2.4
Inclusion–exclusion principle
A typical problem is the computation of the probability of at least one event
among n given ones: In this section we show how to compute;
• the probability of at least one event among ngiven ones;
• the probability of exactly k events among n;
• the probability of at least k events among n.
With the inclusion–exclusion principle we can compute these probabilities
knowing the probabilities that two or more events happen, something which is
usually more easily evaluated. For example, if A, B ∈E, then A \ B, A ∩B and
B \ A are pairwise disjoint, so that
P(A ∪B) = P(A \ B) + P(A ∩B) + P(B \ A) + P(A ∩B) −P(A ∩B)
= P(A) + P(B) −P(A ∩B)
(2.15)
and
P((A ∪B)c) = 1 −P(A ∪B) = 1 −P(A) −P(B) + P(A ∩B).
(2.16)
In order to discuss the case of many events, it is useful to introduce a con-
venient notation. A multi-index is a subset I ⊂{1, . . . , n}. The cardinality |I| of
the multi-index I is called the length of I. Let  be a set and let A1, . . . , An be
nonempty subsets of . For any multi-index I ⊂{1, . . . , n}, deﬁne
AI :=


if I = ∅,
Ai1 ∩Ai2 ∩· · · ∩Aik
if k ≥1 and I =

i1, . . . , ik

.
(2.17)

PROBABILITY MEASURES
61
Example 2.66 Let A1, A2, A3 ⊂ be subsets of . The multi-indexes of {1, 2, 3}
are
∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}
and the associated subsets AI, I ⊂{1, 2, 3}, are, respectively,
A∅= ,
A{1} = A1,
A{2} = A2,
A{3} = A3,
A{1,2} = A1 ∩A2,
A{2,3} = A2 ∩A3,
A{1,3} = A1 ∩A3,
A{1,2,3} = A1 ∩A2 ∩A3.
Deﬁnition 2.67 Let  be a set and let A1, . . . , An ⊂. We call the multiplicity
of the family A1, . . . , An at x ∈ the number val(x) of subsets of the family
A1, . . . , An that contain x:
val(x) :=


j ∈{1, . . . , n}
 x ∈Aj
.
Moreover, for every k = 0, . . . , n, let
Ek :=

x ∈
 val(x) = k

be the set of points of  that belong to exactly k subsets of the family A1, . . . , An.
Clearly,
(A1 ∪A2 ∪· · · ∪An)c =

x ∈
 val(x) = 0

= E0.
Proposition 2.68 Let A1, . . . , An be nonempty subsets of a set , let val(x) be
the multiplicity of the family A1, . . . , An at x ∈ and let Ek, k = 0, 1, . . . , n, be
the kth level set of the multiplicity function val(x). Then

|J|=j
1AJ (x) =
val(x)
j

=
n

k=0
k
j

1Ek(x)
∀j = 0, . . . , n.
(2.18)
and
1Ek(x) =
n

j=k
(−1)k+j
j
k
 
|J|=j
1AJ (x)

∀j = 0, . . . , n.
(2.19)
Proof. The number

|J|=j
1AJ (x) =

|J|=j
x∈AJ
1

62
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
is the number of subfamilies of

A1, . . . , An

of cardinality j whose elements
contain x. Since x belongs to val(x) subsets, there are
val(x)
j

ways to choose
any such subfamily, hence the equality

|J|=j
1AJ (x) =
val(x)
j

in (2.18). Moreover, the second equality in (2.18) follows since
val(x)
j

if and
only if x ∈Ek,
Formula (2.19) is an equivalent formulation of (2.18) as an application of the
inversion formula for the Pascal matrix of binomial coefﬁcients, see Corollary 1.6.
Theorem 2.69 Let (, E, P) be a probability space and let A1, . . . , An ∈E.
Let val(x), x ∈, be the multiplicity function of A1, . . . , An and for every
k = 0, . . . , n let Ek be the k-level set of val(x). Then

|J|=j
P(AJ )(x) =
n

k=0
k
j

P(Ek)(x)
∀j = 0, . . . , n
(2.20)
and
P(Ek) =
n

j=k
(−1)k+j
j
k
 
|J|=j
P(AJ )

∀k = 0, . . . , n.
(2.21)
Proof. Notice that all the terms in the sums in (2.18) and (2.19) are linear
combinations of simple functions. Thus, (2.20) and (2.21) follow integrating with
respect to P both terms of (2.18) and (2.19), respectively, taking into account
the linearity of the integral of simple functions, see Proposition 2.28.
Equation (2.21) reduces the computation of the probability that exactly k
events among n happen to the computation of the probabilities of the intersections
of the events of the family. As a consequence, we get the inclusion–exclusion
principle, a particularly useful tool for computing the probability that at least
one event among n happens.
Corollary 2.70 With the notation above, we have the Sylvester formula
P
 n
(
i=1
Ai
c
=
n

j=0
(−1)j 
|J|=j
P(AJ)

(2.22)
and the inclusion–exclusion principle
P

n
(
i=1
Ai

=
n

j=1
(−1)j+1 
|J|=j
P(AJ)

.
(2.23)

PROBABILITY MEASURES
63
Proof. Since (A1 ∪· · · ∪An)c = E0, (2.22) is (2.21) with k = 0. Moreover,
since P(∪n
i=1Ai) = 1 −P((∪n
i=1)Ai)c), and 
|J|=0 P(AJ ) = P() = 1, (2.23)
follows from, and, actually, is equivalent to, (2.22).
Summing (2.21) for k = k, k + 1, . . . , n, we get an useful formula for com-
puting the probability that at least k events among n happen. In fact, the following
holds.
Corollary 2.71 Let (, E, P) be a probability space, let A1, . . . , An ∈E and, for
k = 0, 1, . . . , n, let Fk be the set of points of  that belong to at least k sets of
the family A1, . . . , An, k ≥1,
Fk :=

x ∈
 val(x) ≥k

.
Then
P(Fk) =
n

j=k
(−1)j+k
j −1
k −1


J⊂{1,...,n}
|J|=j
P(AJ )

.
(2.24)
Proof. For p = 0, 1, . . . , n, let Ep := {x ∈ | val(x) = p}. Obviously, the
sets Ep are pairwise disjoint and Fk = ∪n
p=kEp. Thus, by (2.19) for any x ∈
1Fk(x) =
n

p=k
1Ep(x) =
n

p=k
n

j=p
(−1)p+j
j
p


J⊂{1,...,n}
|J|=j
1AJ (x)

=
n

j=k
(−1)j

j

p=k
(−1)p
j
p


J⊂{1,...,n}
|J|=j
1AJ (x)

.
Since
j

p=k
(−1)p
j
p

= −
k−1

p=0
(−1)p
j
p

= −(−1)k−1
j −1
k −1

,
see Exercise A.13, one gets
1Fk(x) =
n

j=k
(−1)j+k
j −1
k −1


J⊂{1,...,n}
|J|=j
1AJ (x)

.
The claim then follows by integrating the previous equality with respect to P.
2.4.1
Exercises
Exercise 2.72 (Derangements) Taking advantage of the inclusion–exclusion
principle (2.23), compute the number of derangements of n objects, see
Section 1.2.2,
|Dn| := n!
n

j=0
(−1)j 1
j!.

64
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Solution. Let Pn and Dn be the set of permutations and of derangements of n
objects, respectively. For any i = 1, . . . , n, let Ai ⊂Pn be the set of permutations
that do not move i,
Ai =

π ∈Pn
 π(i) = i

.
Clearly,
Dn = (A1 ∪A2 ∪· · · ∪An)c
so that, by the inclusion–exclusion principle,
|Dn| =

J⊂{1,...,n}
(−1)|J||AJ | =
n

j=0
(−1)j

J⊂{1,...,n}
|J|=j
|AJ |.
If J =

k1, . . . , kj

then AJ is the set of permutations that do not move the
points

k1, . . . , kj

. These permutations are as many as the permutations of
the n −j remaining elements, i.e. |AJ | = (n −j)! Thus, denoting by cj the
number of multi-indexes J of length j in {1, . . . , n}, we get
|Dn| =
n

j=0
(−1)j(n −j)!


J⊂{1,...,n}
|J|=j
1

=
n

j=0
(−1)j(n −j)! cj.
Since cj =
n
j

we ﬁnally get
|Dn| =
n

j=0
(−1)j(n −j)!
n
j

= n!
n

j=0
(−1)j 1
j!.
Exercise 2.73 (M´enage problem) We ask for the number of different ways in
which it is possible to seat a set of n heterosexual couples at a dining round
table so that men and women alternate and nobody sits next to his or her partner.
The m´enage problem is a classical problem in combinatorics, ﬁrst formulated by
´Edouard Lucas in 1891.
Solution. Let Kn be the possible arrangements. If n = 1 or n = 2 then obvi-
ously Kn = 0. If n = 3, there are 3! = 6 possible arrangements with the given
constraints. For larger n, n ≥4, we can proceed as follows: there are n! seat-
ing arrangements for women, thus it remains to count the number Un ⊂Pn of
permutations of seating men away from their wives.
Theorem (Touchard) We have
Kn = n! Un,
Un :=
n

k=0
(−1)k
2n
2n −k
2n −k
k

(n −k)!
(2.25)

PROBABILITY MEASURES
65
Proof. Without loss of generality, we may assume that the seats are anticlock-
wise numbered from 1 to 2n. For any h = 1, . . . , n, let
A2h−1 =

permutations in Pn that sit the husband h
on the left of his wife

,
A2h =

permutations in Pn that sit the husband h
on the right of his wife

.
The acceptable arrangements are one (A1 ∪· · · ∪A2n)c ⊂Pn, thus
Un = |(A1 ∪· · · ∪A2n)c|.
By the inclusion–exclusion principle (2.22),
Un = |(A1 ∪· · · ∪A2n)c| =

J⊂{1,...,2n}
(−1)|J||AJ |
=
2n

j=0
(−1)j

J⊂{1,...,2n}
|J|=j
|AJ |

,
thus, we have to compute |AJ| for J ⊂{1, 2, . . . , 2n}.
If |J| = j and J =

k1, . . . , kj

, then AJ = Ak1 ∩· · · ∩Akj . Notice that
A2h ∩A2h+1 for h = 1, . . . , n −1 are empty since any permutation of one of
these intersections puts husbands h and h + 1 on the same chair. Similarly,
A2n ∩A1 is empty since the permutations in it force the husbands 1 and n on the
same chair. Also the intersection A2h−1 ∩A2h is empty since any permutation
in this intersection is such that the husband h sits in two different chairs. We
can summarize these facts by saying that AJ = ∅if J contains two consecutive
integers in the sequence (1, 2, . . . , 2n, 1). In particular, AJ = ∅if j = |J| > n.
If J does not contain two consecutive integers in the sequence (1, 2, . . . , 2n, 1),
then every permutation in AJ, J = (k1, . . . , kj), ﬁxes the chairs of the husbands
k1, . . . , kj. Hence AJ contains as many permutations as the free permutations
of the remaining n −|J| husbands, i.e. |AJ | = (n −|J|)! Summarizing, if
j := |J|, then
|AJ| =
⎧
⎪⎨
⎪⎩
(n −j)!
if J does not contain two consecutive integers
in the sequence (1, . . . , 2n, 1),
0
otherwise.
Denote
by
f2n,j
the
number
of
subsets
J ⊂{1, . . . , 2n}
such
that
|J| = j and which do not contain two consecutive integers in the sequence
(1, 2, . . . , 2n, 1). Then
Un =
2n

j=0
(−1)j

J⊂{1,...,2n}
|J|=j
|AJ|

=
n

j=0
(−1)j

J⊂{1,...,2n}
|J|=j
|AJ|


66
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
=
n

j=0
(−1)j(n −j)! f2n,j.
(2.26)
In the next lemma, see (2.27), we will compute f2n,j:
f2n,j =
2n
2n −j
2n −j
j

.
Substituting in (2.26) we obtain the claim.
Lemma The number fn,k of subsets of {1, . . . , n} having cardinality k and that
do not contain two consecutive integers in the sequence (1, 2, . . . , n, 1) is
fn,k =
n
n −k
n −k
k

.
(2.27)
Proof. It can be shown, see Exercise 1.30, that the number gn,k of subsets of
cardinality k that do not contain two consecutive integers in (1, . . . , n) is
gn,k =
n −k + 1
k

.
Let S be the family of subsets of {1, . . . , n} having cardinality k and that do not
contain two consecutive integers in the sequence (1, 2, . . . , n, 1). Let S1 be the
family of the subsets in S that do not contain 1 and let S2 be its complement
in S, so that |S| = |S1| + |S2|. Each subset in S1 has cardinality k and its
elements are integers in {2, . . . , n} which are not pairwise consecutive. Thus
|S1| = gn−1,k. Each subset in S2 has cardinality k and since it contains 1, it
contains neither 2 nor n, and is detected by its remaining k −1 elements, Thus
|S2| = gn−3,k−1. Consequently,
fn,k = |S| = |S1| + |S2| = gn−1,k + gn−3,k−1
=
n −1 −k + 1
k

+
n −3 −k + 1 + 1
k −1

=
n
n −k
n −k
k

.
Exercise 2.74 (Euler function) For any positive integer n compute the number
of primes in {1, . . . , n −1} which are relative prime to n, i.e.
φ(n) :=


d
 1 ≤d ≤n, gcd(d, n) = 1
.
Solution. Write n as the product of prime numbers, n = pα1
1 . . . pαk
k
with
p1, p2, . . . , pk pairwise different prime numbers. If d and n are not coprime,

PROBABILITY MEASURES
67
then there exists i ∈{1, . . . , k} such that pi divides d. For any i = 1, . . . , k let
Ai : =

d
 1 ≤d ≤n, pi divides d

=

pi, 2pi, 3pi, . . .

∩{1, . . . , n}.
We want to compute
φ(n) =
(A1 ∪A2 ∪. . . Ak)c
by means of the inclusion-exclusion principle.
We ﬁrst compute |AJ| for every J ⊂{1, . . . , n}. By (2.17) we have A∅=
{1, . . . , n}, so that |A∅| = n. For any i, Ai =

pi, 2pi, 3pi, . . .

so that |Ai| =
n
pi . Moreover,
A{i,j} = Ai ∩Aj =

pipj, 2pipj, . . . , n

,
so that |A{i,j}| =
n
pipj and, in general,
|AJ | =
n
-
i∈J pi
∀J ⊂{1, . . . , k}.
Applying the inclusion-exclusion principle we get
φ(n) =

J⊂{1,...,n}
(−1)|J||AJ|
= n

1 −

i
1
pi
+

i<j
1
pipj
−

i<j<k
1
pipjpk
+ · · · + (−1)n
1
p1p2 · · · pk

= n

1 −1
p1

1 −1
p2

. . .

1 −1
pk

.
(2.28)

3
Random variables
Events and their probabilities are important objects in probability theory, but
not the only ones. Consider for instance the waiting time at a trafﬁc light. It is
impossible to evaluate the probability that the waiting time is, say, between 10
s and 20 s, simply because the answer depends on the arrival time probabilities.
For instance, if everybody arrives during the green phase, then the waiting time
is always zero! Waiting time probabilities depend in a deﬁnite manner on the
arrival time. This is a typical example of random variable, i.e. of maps that link
probability spaces. In Section 3.1 we ﬁrst introduce real valued random variables
and the basic related objects of value distribution, law and generated events, and
the standard two descriptors of expected value and variance. We also state a few
useful formulas to compute the distribution and law of the composition of random
variables. A few classical discrete and absolutely continuous distributions on R
are then presented in Section 3.2 and Section 3.3, respectively. Each section is
complemented with examples and exercises. Other examples and exercises can
be found e.g. in [3].
3.1
Random variables
Suppose (, E, P) is a probability space. Heuristically, a random variable is
the result of an experiment (that is a well-deﬁned deterministic experience) on the
probability space (, E, P). The result of such an experiment is nevertheless
random since the probability measure on  induces a probability measure on the
set of the possible results of the experiment. As an example, consider a dice, for
which the base space is the set  = {1, . . . , 6}. If the experiment is ‘What number
is on the top face?’, then the set of possible results is the set {1, 2, 3, 4, 5, 6} and
the result is random since the throwing of a dice is random. If the experiment
is ‘Is the number on the top face an even number?’ then the set of possible
results is {‘True’, ‘False’}, and again the answer is random since the throwing of
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

RANDOM VARIABLES
69
a dice is random. Notice that in both cases the question is deterministic. Another
example we have already presented is the waiting time at a trafﬁc light: also in
this case, the waiting time depends in a deterministic way on the time of arrival
and randomness of the waiting time reﬂects the randomness of arrival times. As
a further example, one may observe a certain feature of a physical system, say
the temperature, a parameter that is known to vary with the state of the system in
a well-deﬁned way. If  is the base set of all possible states of the system, then
the experiment of ‘measuring the temperature’ is a given map T :  →R from
 to R. What we observe is the value of such a mapping, not the state of the
system. A certain degree of randomness in the system may induce randomness
in the observed value of the temperature.
Summing up:
• A random variable is a map X :  →R deﬁned on the base space  of
a probability space (, E, P). We use the name ‘variable’ to stress the
importance of the image of X.
• The probability measure P on  induces a probability measure on the
image X(), hence the name ‘random variable’. The emphasis will be on
the properties of the measure induced on X().
• In general, a random variable ‘sees’ only some events in , as well as an
observable on a physical system gives only partial information on the state
of the system.
To give a formal presentation, we must furthermore keep in mind that, in
general, not every subset of  is an event.
3.1.1
Deﬁnitions
3.1.1.1
Real valued random variables
Deﬁnition 3.1 A real valued random variable on a probability space (, E, P)
is an E-measurable function X :  →R, i.e. a real-valued function deﬁned on 
such that for any t ∈R the t-sublevel set of X belongs to E,

x ∈
 X(x) ≤t

∈E.
Here R := R ∪{+∞} ∪{−∞}.
As previously noticed, we tacitly assume that E = P() if X() is ﬁnite or
denumerable. In this case, any function X :  →R is a random variable.
Example 3.2 Let (, E, P) be a probability space and let E ∈E. Then he char-
acteristic function of E,
1E(x) =

1
if x ∈E,
0
otherwise

70
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
is a random variable since for any t ∈R

x ∈
 1E(x) ≤t

=
⎧
⎪⎨
⎪⎩

if t ≥1,
Ec
if 0 ≤t < 1,
∅
if t < 0
and, of course, ∅, Ec and  are events.
3.1.1.2
Distribution of a random variable
Let X :  →R be a random variable on the probability space (, E, P). Starting
from
X−1([−∞, t]) =

x ∈
 X(x) ≤t

∈E
and taking into account that E is a σ-algebra, one can show that
X−1(A) =

x ∈
 X(x) ∈A

is an event, X−1(A) ∈E, for every Borel set A ∈B(R), see also Appendix B.
Consequently, we may deﬁne
PX(A) := P(X ∈A) := P(X−1(A))
∀A ∈B(R).
(3.1)
We will write P(X ∈A) for P({x ∈ | X(x) ∈A}).
Using De Morgan formulas, it is then easy to show that (B(R), PX) is a
measure on R which is called the distribution of X induced by (E, P). Since
PX(R) = 1 −P(X = ±∞), PX is a probability measure on R if and only if
P(X = ±∞) = 0. If μ is a measure and PX = μ we also say that X is distributed
as μ or that X follows μ.
We emphasize that the measure PX depends on X as a function and on the
measure P, see Example 3.3. However, as usual, we still refer in the following to
PX as to the distribution of the random variable X, thinking of the domain prob-
ability space as already understood in the deﬁnition of the random variable X.
3.1.1.3
Law or probability distribution function
An efﬁcient way to describe the distribution of a random variable X :  →R
is through the so-called probability distribution function or law of X. It is the
function FX : R →[0, 1] deﬁned by
FX(t) := P(X ≤t),
t ∈R.
Example 3.3 In Figure 3.1 the laws of two random variables relative to the
function X(x) = x are drawn. Figure 3.1(a) shows the law of X driven by

RANDOM VARIABLES
71
t
0
1
1
t
6
(a)
(b)
1
1
Figure 3.1
The laws in Example 3.3.
the probability measure associated with the throwing of a dice; Figure 3.1(b)
shows the law of X driven by the uniform probability on the interval [0, 1].
Notice that the law, as well as the probability distribution, depends both on the
function X and on the domain probability space.
The law FX need not be continuous even if X is smooth, see e.g. Figure 3.1
where X(x) = x. However, we have the following.
Proposition 3.4 Let FX : R →R be the law of a random variable X. Then we
have:
(i) FX is monotone increasing.
(ii) FX(t) →P(X = −∞) as t →−∞.
(iii) FX(t) →1 −P(X = +∞) as t →+∞.
(iv) FX is continuous from the right, i.e. FX(t) = lims→t+ FX(s) ∀t ∈R.
(v) FX(t) = PX([−∞, t]) ∀t ∈R.
(vi) P({x ∈ | X(x) = t}) = FX(t) −lims→t−FX(s) ∀t ∈R.
Proof. Properties (i), (ii), (iii) and (v) are trivial. Let us prove (iv) and (vi).
(iv) Let t ∈[−∞, ∞[ and let sn ↓t. For every n set En :=

x ∈ | X(x) ≤sn

∈E so that

En

is a monotone decreasing sequence in E. Since

x ∈
 X(x) ≤t

=
'
n
En,
we infer from the continuity of measures that
FX(t) = P(X ≤t) = lim
n→∞P(En) =
lim
n→+∞FX(sn).
Claim (iv) then follows, since

sn

is arbitrary.

72
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
(vi) Let t ∈] −∞, ∞] and let sn ↑t. Then for every n the set Fn :=

x ∈ | X(x) ≤sn

∈E; moreover, the sequence

Fn

is monotone increasing.
Since

x ∈
 X(x) < t

=
(
n
Fn,
we infer from the continuity of measures that
P(X < t) = lim
n→∞P(Fn) =
lim
n→+∞FX(sn),
hence
P(X < t) = lim
s→t−FX(s) =: FX(t−).
Claim (vi) follows.
Although FX need not be continuous, it is worth recalling that any monotone
function f has at most a countable set of discontinuity points. In fact, for any
integer k there are at most a countable number of points (a ﬁnite number if f is
bounded) where the jump is larger than 1/k.
The law FX and the measure distribution PX are two equivalent ways to
describe the distribution of the random variable X. One can obtain FX from PX
just by deﬁnition. On the other hand, from (v) of Proposition 3.4 and taking into
account that PX is a measure, one gets
PX(]a, b]) = FX(b) −FX(a)
∀a, b ∈R,
i.e. given FX, one recovers the value of PX on intervals that are left-open and
right-closed. Since any open set A ∈R is an at most denumerable and disjoint
union of such intervals, we can recover PX(A) from FX for any open set A ⊂R
and then for every Borel set because of the σ-additivity and continuity properties
of measures. In this way, the law t →FX(t) completely describes the measure
(B(R), PX), see also Appendix B.
3.1.1.4
Events generated by a random variable
Let X :  →R be a random variable on a probability space (, E, P). As already
stated, for every A ∈B(R) the preimage of A with respect to the random vari-
able X,
X−1(A) :=

x ∈
 X(x) ∈A

,
(3.2)
belongs to E. Using De Morgan formulas, it is easy to check that the family of
such events
EX :=

E ∈E
 E = X−1(A) for some A ∈B(R)

is a σ-algebra, hence EX is called the family of the events detectable or generated
by X. Trivially, EX is the smallest σ-algebra that contains all the events of the
type X−1(A) for A ∈B(R).

RANDOM VARIABLES
73
Example 3.5 Let X :  →R be a constant c. Then
X−1(A) =


if c ∈A,
∅
if c /∈A.
Therefore, the only events detectable by X are ∅and , i.e. EX = {∅, }.
Example 3.6 Suppose that the image of X :  →R is ﬁnite or denumerable,
X() =

yi

i∈N. Then E(X) is the σ-algebra generated by the denumerable
family of events Ei, i ∈N, given for every i ∈N by Ei :=

x ∈ | X(x) = yi

,
i ∈N.
3.1.1.5
Special classes of distributions
The distributions of many random variables that appear in applications either ﬁt
in one of the two classes of measures described below or are linear combinations
of them.
• Discrete distribution. We recall that a measure μ on R is atomic or discrete
if it is concentrated on a ﬁnite or denumerable set of points

tj

⊂R, that
is, μ(A) = 0 if and only if A ∩

tj

= ∅. The vector (or sequence if the
number of points is denumerable) p = (pj), pj := μ(

tj

) is called the
density (with respect to the counting measure), or mass density function of
the measure μ. Moreover, the number pj is called the density of μ at tj.
If X is a random variable with a discrete distribution PX concentrated
on

tj

, we can express the law of X in terms of the density (pj) of PX;
in fact,
FX(t) = PX([−∞, t]) =

tj ≤t
PX(

tj

) =

tj ≤t
pj.
Notice that FX(t) is monotone increasing and piecewise constant with
jumps at

tj

, and the jump at tj has size PX(

tj

).
• Absolutely continuous distribution. Recall that a measure (B(R), μ) on
R is said to be absolutely continuous if there exists a non-negative and
Lebesgue summable function f : R →R such that
μ(A) =
*
A
f (s) ds
∀A ∈B(R).
(3.3)
The function f (s) is called the density of μ (with respect to the Lebesgue
measure L1). In particular, for F(t) := μ(] −∞, t]) we have
F(t) :=
* t
−∞
f (s) ds
∀t ∈R.
(3.4)
Observe that F(t) →0 as t →−∞and F(t) →μ(R) as t →+∞.

74
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
The reader recognizes the relation (3.4) as a classical point in mathematical
analysis. For instance, if f is piecewise continuous, then F is continuous
and piecewise C1 by the fundamental theorem of calculus. Moreover,
F ′(t) = f (t) if f is continuous at t. If f is merely summable, it can be
shown, see the absolute continuity theorem for the integral, Theorem B.58,
that F(t) is an absolutely continuous function, a condition strictly stronger
than continuity, see Deﬁnition B.66. A celebrated theorem by Vitali,
Theorem B.64, yields the converse: if F is an absolutely continuous
function, then there exists a summable function f such that (3.4) holds.
In conclusion, the following two claims agree:
— μ is an absolutely continuous measure, (3.3);
— F(t) := μ(] −∞, t]) is an absolutely continuous function.
If either of them is true, then the derivative of F(t) exists for almost all t
and
F ′(t) = f (t)
for a.e. t ∈R :
The density function agrees almost everywhere with the derivative of the
law.
In particular, if X is a real valued random variable with an absolutely con-
tinuous distribution of density fX, i.e.
PX(A) =
*
A
fX(t) dt
∀A ⊂B(R),
then trivially,
FX(t) = P(X ≤t) = P(−∞< X ≤t) =
* t
−∞
fX(s) ds
∀t ∈R.
Observe that
.
R fX(s) ds = 1.
A typical example in this class is a random variable X uniformly distributed
on an interval ]a, b[, that is, such that PX(A) :=
.
A fX(t) dt ∀A ∈B(R) where
fX(t) =
⎧
⎨
⎩
1
b −a
if t ∈]a, b[,
0
otherwise.
Distributions of random variables may also be a combination of the two types
above, as in the following example.
Example 3.7 (Waiting time at a trafﬁc light) Consider for simplicity a trafﬁc
light with two states, red and green. The red and green lights are on for r and g
seconds, respectively. The cycle is iterated every r + g seconds. Here the certain
event is the time interval [0, r + g]. A car randomly reaches the trafﬁc light
according to the uniform distribution on [0, r + g], i.e. the probability that a car

RANDOM VARIABLES
75
driver arrives in a time interval I = [a, b], 0 ≤a ≤b ≤r + g is P(I) := (b −
a)/(r + g). Thus the probability associated with arrival time is (B([0, r + g]), P)
where P(A) :=
1
r+gL1(A).
For every arrival time x ∈[0, r + g] the waiting time at the trafﬁc light is
T (x) :=

r −x
if 0 ≤x ≤r,
0
if r ≤x ≤r + g.
Since T is continuous, T is a random variable on the probability space ([0, r +
g], B([0, r + g]), P) with law
FT (t) := P(T ≤t) =
⎧
⎪⎨
⎪⎩
0
if t < 0,
t+g
r+g
if 0 < t < r,
1
if t ≥r,
see Figure 3.2. We point out that the law FT is neither piecewise constant nor
continuous. It has a jump at time t = 0 of amplitude g/(r + g). Of course,
FT is the sum of a continuous piecewise linear function and of a piecewise
constant function.
3.1.2
Expected value
Let X :  →R be a random variable on (, E, P). The integral
E [ X ] =
*

X(x) P(dx),
(3.5)
if it exists, is also called the expected value or the expectation of the random
variable X. Notice that E [ X ] is simply the mean value of X on 
E [ X ] =
1
P()
*

X(x) P(dx)
since P() = 1.
r
r + g
r + g
g
r
T(x)
FT(t)
1
r
0
(a)
(b)
Figure 3.2
The trafﬁc light. (a) The waiting time T (x); (b) the law according to
the uniform probability on [0, r + g].

76
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
The integral in (3.5) is the Lebesgue integral, which we now brieﬂy describe
in this contest, even though the assumption P() = 1 is not necessary. The
reader may refer to Appendix B for details.
We have already mentioned the deﬁnition of the Lebesgue integral of a simple
function, i.e. of a random variable with a ﬁnite range, i.e. of a function
ϕ(x) =
n

i=1
ci1Ei(x)
where c1, . . . , cn are pairwise distinct and the sets Ei :=

x ∈ | ϕ(x) = ci

are
events of the σ-algebra E. The integral of ϕ (with respect to the measure P) has
been deﬁned as
E [ ϕ ] =
*

ϕ(x) P(dx) :=
n

i=1
ciP(Ei),
(3.6)
as intuition suggests.
Let X :  →R+ be a non-negative random variable (X may take the value
+∞). One can show that there exists a sequence

ϕn

of simple functions such
that ϕ1(x) ≤ϕ2(x) ≤. . . and ϕn(x) ↑X(x) ∀x ∈. For each n the integral
In :=
.
 ϕn(x) P(dx) is deﬁned in (3.6) and

In

is an increasing sequence of real
numbers, so that limn→∞In exists and is in [0, +∞]. It is an easy consequence
of the Beppo Levi theorem that the limit does not depend on the choice of the
approximating sequence

ϕn

. We then deﬁne the Lebesgue integral, equivalently
the expectation of X, by
E [ X ] =
*

X(x) P(dx) := lim
n→∞
*

ϕn(x) P(dx).
Notice that E [ X ] = +∞whenever P(X = +∞) > 0, and that E [ X ] = 0 if and
only if P(X > 0) = 0; in the latter case, one says that X = 0 P-almost everywhere
or, when no confusion may arise we say that X = 0 almost everywhere, which
is usually shortened to X = 0 a.e.
Let now X :  →R be a random variable, not necessarily non-negative.
Split
X
as
X(x) = X+(x) −X−(x)
where
X+(x) := max {X(x), 0}
and
X−(x) = max {−X(x), 0}.
Since
X+
and
X−
are
non-negative
random
variables, their expectations are deﬁned. If either E
%
X+
&
or E
%
X−
&
is ﬁnite, we say that X is Lebesgue integrable
and deﬁne E [ X ] as
E [ X ] := E
%
X+
&
−E
%
X−
&
. We point out that the expectation of X is
not deﬁned if both E
%
X+
&
and E
%
X−
&
are inﬁnite. Finally, observe that
|X(x)| = X+(x) + X−(x) so that E [ |X| ] can be computed, and that E [ X ]
exists and is ﬁnite if and only if E [ |X| ] is ﬁnite. In the latter case, we say that
X is summable.
The usual properties of the integral or expectation are collected in
Theorem B.21. There the reader may ﬁnd the standard properties of the integral:

RANDOM VARIABLES
77
linearity, monotonicity and additivity, and the main result about limits, known
as the Beppo Levi theorem.
Since the above holds whatever P() is, this procedure deﬁnes the integral
of random variables X on a probability space (, E, P) (with respect to P) and
the integral of B(R)-measurable functions with respect to PX. Therefore, the
notations
*

X(x) P(dx)
and
*
R
ϕ(t)PX(dt)
make sense if X is a non-negative random variable and if ϕ : R →R is non-
negative and B(R)-measurable, respectively.
As already shown, the distribution function FX(t) : R →R completely
describes the distribution of the values of the random variable X. Thus, one
also writes
*

ϕ(t) dFX(t)
instead of
*
R
ϕ(t) PX(dt),
(3.7)
see also Appendix B and the exercises therein for explicit formulas of the integral
of random variables with respect to several measures. Here we only quote the
following two facts.
• Let X be a random variable with a discrete distribution PX concentrated
on

xi

and of density pi at xi. Then
*
R
ϕ(t) PX(dt) =

i
ϕ(xi)pi.
• Let X :  →R be a random variable with absolutely continuous distribu-
tion of density fX. Then
*
R
ϕ(t) dFX(t) =
*
R
ϕ(t) PX(dt) =
*
R
ϕ(t)fX(t) dt.
Because of this last formula, the absolute continuity property (3.3) is usu-
ally written as
PX(dt) = dFX(t) = fX(t) dt.
3.1.3
Functions of random variables
Let X be a real valued random variable on the probability space (, E, P), let
(B(R), PX) be its distribution and let ϕ : R →R be a Borel-measurable func-
tion on R. The composition Y := ϕ ◦X :  →R is again a random variable
on (, E, P). In fact, for any open set A ⊂R the set ϕ−1(A) ⊂R is a Borel
set, hence (ϕ ◦X)−1(A) = X−1(ϕ−1(A)) is an event. The following formulas for
Y = ϕ ◦X trivially hold:
PY (A) = P(Y ∈A) = P(X ∈ϕ−1(A)) = PX(ϕ−1(A)),
(3.8)

78
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
FY(s) = PX({t ∈R | ϕ(t) ≤s}).
(3.9)
From (3.9) we trivially infer that FY (s) = P(ϕ ◦X ≤s) = 0 if s is below
the image of ϕ and FY(s) = 1 if s is above the image of ϕ. However, there is
no easy formula that follows from (3.9) as the following example shows.
Example 3.8 Let X be a real valued random variable and, for 0 < a ≤1, let
ϕ(t) :=
⎧
⎪⎨
⎪⎩
0
if t < 0,
a
if t = 0,
1
if t > 0.
Then

x ∈
 ϕ(X(x)) ≤s

=
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
∅
if s < 0,
{x | X(x) < 0}
if 0 ≤s < a,
{x | X(x) ≤0}
if a ≤s < 1,

if s ≥1.
Hence FY (0) = P(ϕ(X(x)) ≤0) = P(X < 0) = FX(0−).
A few special cases of (3.9) are described below.
• If ϕ : R →R is continuous and strictly monotone increasing then for any
s ∈ϕ(R) we have ϕ(t) ≤s if and only if t ≤ϕ−1(s), hence
FY (s) = PX(y ≤ϕ−1(s)) = FX(ϕ−1(s)).
• If ϕ is continuous and strictly monotone decreasing then for any s ∈ϕ(R)
we have ϕ(t) ≤s if and only if t ≥ϕ−1(s), so that
FY(s) = PX(y ≥ϕ−1(s)) = 1 −FX(t−),
t := ϕ−1(s).
• Assume ϕ : R →R is right-continuous and monotone increasing, possibly
with jumps. For any s ∈R the set
Es :=

x
 s ≤ϕ(x)

is either empty or a closed half-line or R. Accordingly, let ψ(s) := min Es.
Trivially, ψ(s) ≤t if and only if t ∈Es i.e. if and only if s ≤ϕ(t). Con-
sequently,

t
 t ≤ψ(s)

=

t
 s ≤ϕ(t)

and we conclude that
FY(s) = Fϕ◦X(s) = FX(ψ(s)).
(3.10)

RANDOM VARIABLES
79
The graph of s →ψ(s) is obtained from the graph of ϕ by adding vertical
segments at the jump points and then looking at the resulting curve from
the vertical axis, see Figure 3.3. Clearly, if ϕ is continuous and invertible,
then ψ(s) is ﬁnite on the range of ϕ and coincides with the inverse of ϕ.
The links between the distributions of X and Y := ϕ ◦X given by (3.8) and
(3.9) may be cumbersome, as we have seen, if either ϕ has jumps or is not strictly
monotone. On the contrary, the relation between the relative integrals is much
more convenient. Let us state the main property.
Theorem 3.9 Let X :  →R be a random variable on a probability space
(, E, P). Then for every non-negative Borel-measurable function ϕ : R →R
we have
E [ ϕ ◦X ] =
*

ϕ(X(x)) P(dx) =
*
R
ϕ(t) PX(dt).
(3.11)
Proof. Assume ﬁrst that ϕ is a simple function, ϕ(t) = n
i=1 ci1Ei(t),
where c1, . . . , cn are pairwise distinct real numbers and the sets Ei := {t ∈R |
ϕ(t) = ci} are events. Since

x ∈ | X(x) ∈Ei

=

x ∈ | ϕ ◦X(x) = ci

and by the deﬁnition of integral, we have
*
R
ϕ(t) PX(dt) =
n

i=1
ciPX(Ei) =
n

i=1
ciP(X ∈Ei)
=
n

i=1
ci P(ϕ ◦X = ci) =
*

ϕ ◦X(x) P(dx),
hence the claim if ϕ is simple.
In the general case, let

ϕk

be a monotone increasing sequence of simple
functions such that ϕk ↑ϕ pointwise. Then, from the above, for any integer k
we have
*
R
ϕk(t) PX(dt) =
*

ϕk(X(x)) P(dx).
Beppo Levi theorem allows us to pass to the limit in the previous equality as
k →∞, thus proving (3.11) for ϕ.
j
z
y
y(z)
Figure 3.3
The generalized inverse of ϕ.

80
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Corollary 3.10 Let X be a random variable on (, E, P). Then E [ X ] is well-
deﬁned if and only if
.
R t PX(dt) exists and, in this case,
E [ X ] =
*
X(x) P(dx) =
* +∞
−∞
t PX(dt).
(3.12)
Proof. The claim trivially follows by applying (3.11) to X separately with
ϕ(t) = max {t, 0} and ϕ(t) = max {−t, 0}.
Corollary 3.10 shows in particular that the expectation of X depends only on
the distribution of values of X. There is no need to go back to the probability
space (, E, P) in order to compute the expectation of X. In particular, if
two random variables X and Y have the same distribution, PX = PY , then
E [ X ] = E [ Y ].
Theorem 3.9 yields a convenient formula for the distribution of a composition.
Corollary 3.11 (Composition) Let X be a random variable on (, E, P) and let
ϕ : R →R be a B(R)-measurable function. Then for every non-negative B(R)-
measurable function ψ : R →R we have
*
R
ψ(s) Pϕ◦X(ds) =
*
R
ψ(ϕ(t))PX(dt).
(3.13)
Proof. By applying (3.11) to X with test function ψ ◦ϕ and to ϕ ◦X with
test function ψ we get
*
R
ψ(s) Pϕ◦X(ds) =
*

ψ(ϕ(X(x)) P(dx)
and
*
R
ψ(ϕ(t))PX(dt) =
*

ψ(ϕ(X(x)) P(dx),
respectively. The claim follows at once.
3.1.4
Cavalieri formula
Another interesting formula is the following consequence of the Cavalieri formula
(B.22):
E [ X ] =
* +∞
0
P(X > t) dt =
* +∞
0
(1 −FX(t)) dt
(3.14)
which holds for any non-negative random variable X. Applying (3.14) to X+
and X−and summing, one gets
E [ X ] =
* +∞
0
(1 −FX(t)) dt −
* 0
−∞
FX(t) dt
(3.15)

RANDOM VARIABLES
81
provided X is summable, see Corollary B.24. Formula (3.15) shows again that
the expected value of a random variable X depends only on the distribution PX;
actually it shows how the expected value depends on the law of X. In particular
(3.15) yields E [ X ] = E [ Y ] if FX = FY.
Theorem 3.12 (Composition) Let X :  →R be a random variable on
(, E, P), let I be an interval such that I ⊃X() and let ϕ : I →R be a
non-negative and monotone increasing function of class C1. Then
E [ ϕ ◦X ] = mP() +
*
I
ϕ′(t)(1 −FX(t)) dt
(3.16)
where m := inft∈I ϕ(t).
Proof. (i) Assume ϕ is strictly increasing. Let M := supt∈I ϕ(t) so that
]m, M[⊂ϕ(I) ⊂[m, M]. Notice that

x ∈
 ϕ ◦X(x) > s

=
⎧
⎪⎪⎨
⎪⎪⎩

if s ≤m, s /∈ϕ(I),

x ∈
 X(x) > ϕ−1(s)

if s ∈ϕ(I),
∅
if s ≥M, s /∈ϕ(I).
Therefore, applying the Cavalieri formula (3.15) to the random variable ϕ ◦X
and then, changing variable with s = ϕ(t), t ∈I, we get
E [ ϕ ◦X ] =
* ∞
0
P(ϕ ◦X > s) ds = mP() +
* M
m
P(X > ϕ−1(s)) ds
= mP() +
*
I
ϕ′(t)P(X > t) dt
= m P() +
*
I
ϕ′(t)(1 −FX(t)) dt.
(ii) When ϕ is not strictly monotone, consider a non-negative bounded, strictly
monotone function ψ : I →R of class C1(I) with inft∈I ψ(t) = 0. Then ϕ + ψ
is strictly monotone and (i) yields
E [ ϕ ◦X ] + E [ ψ ◦X ] = E [ (ϕ + ψ) ◦X ]
= mP() +
*
I
(ϕ′(t) + ψ′(t))(1 −FX(t)) dt,
E [ ψ ◦X ] =
*
I
ψ′(t)(1 −FX(t)) dt.
The claim then follows by subtracting the second equality from the ﬁrst one since
E [ ψ ◦X ] is ﬁnite.

82
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
3.1.5
Variance
The expectation of a random variable X is a ‘central’ value around which the
values of the range of X are distributed. Several indicators can be deﬁned to
roughly describe the distribution of X around the expected value. For instance,
to evaluate how much of the range of X is dispersed (or concentrated) around the
expected value, a relevant indicator is the so-called variance of X deﬁned as
Var [ X ] := E
%
(X −E [X])2 &
.
Of course, the variance is deﬁned if and only if E [ |X| ] < +∞; moreover, in
this case, it may happen that Var [ X ] = +∞. The following formulas are easily
proven by using the linearity property of the expected value:
Var [ X ] = 0 if and only if X(x) = E [ X ] for P-a.e. x,
(3.17)
Var [ X ] = E
%
X2 −2E [X] X + E
%
X2& &
= E
%
X2 &
−(E [ X ])2,
(3.18)
Var [αX + β] = α2Var [ X ]
∀α, β ∈R.
(3.19)
Finally, the number
σ(X) :=

Var [ X ].
(3.20)
is called the standard deviation of the random variable X. Notice that σ(X) is
positively homogeneous of degree 1, σ(λX) = |λ| σ(X) ∀λ ∈R.
3.1.6
Markov and Chebyshev inequalities
Proposition 3.13 Let X :  →R be a random variable on the probability space
(, E, P). Let I be an interval such that X() ⊂I and let ϕ : I →R be a non-
negative monotone increasing function of class C1. Then ∀t ∈I
ϕ(t) P(X ≥t) ≤E [ ϕ ◦X ].
(3.21)
The inequality (3.21) is called Markov inequality. Moreover, if E[ |X| ] is
ﬁnite, then ∀t > 0 the Chebyshev inequality
P
X −E [ X ]
 ≥t

≤Var [ X ]
t2
(3.22)
holds.
Proof. Let t ∈R. Since
{x ∈ | X(x) ≥t} ⊂{x ∈ | ϕ(X(x)) ≥ϕ(t)}
we have
ϕ(t)P(X ≥t) ≤ϕ(t)P

ϕ ◦X ≥ϕ(t)

≤
*
{ϕ◦X≥ϕ(t)}
ϕ(X(x)) P(dx) ≤
*
ϕ(X(x)) P(dx),
i.e. Markov inequality.

RANDOM VARIABLES
83
Assume E [ |X| ] is ﬁnite. By applying Markov inequality to the random
variable x →|X(x) −E [ X ]| and the map ϕ(t) := t2 we get
P

|X −E [ X ]| ≥t

≤1
t2 E
%
(X −E [X])2 &
= Var [ X ]
t2
,
that is, Chebyshev inequality.
3.1.7
Variational characterization of the median and of the
expected value
Deﬁnition 3.14 Let X :  →R be a random variable on a probability space
(, E, P). The real number
tM := inf

t
 FX(t) ≥1/2

.
is called the median of the distribution PX.
From the deﬁnition and the right-continuity of FX, we get
FX(tM) ≥1/2
and
P(X < tM) = lim
t→t−
M
FX(t) ≤1/2.
Trivially, FX(tM) = 1/2 if FX is continuous at tM.
Proposition 3.15 Let X :  →R be a random variable on a probability space
(, E, P) with E [ |X| ] < +∞. Then tM is a minimum point of the function
φ(s) :=
*

|X(x) −s| P(dx),
s ∈R.
Proof. Let s < tM. Let us prove that φ(s) ≥φ(tM).
φ(s) =
*
{X(x)≤s}
(s −X(x)) P(dx) +
*
{X(x) > s}
(X(x) −s) P(dx)
=
*
{X(x)≤s}
(tM −X(x)) P(dx) +
*
{X(x)≤s}
(s −tM) P(dx)
+
*
{s<X(x)≤tM}
(X(x) −tM) P(dx) +
*
{s<X(x)≤tM}
(tM −s) P(dx)
+
*
{X(x) > tM}
(X(x) −tM) P(dx) +
*
{X(x) > tM}
(tM −s) P(dx)

84
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
=
*
|X(x) −tM| P(dx) −2
*
{s<X(x)≤tM}
(tM −X(x)) P(dx)
+ (tM −s)

−P(X ≤s) + P(s < X ≤tM) + P(X > s)

≥φ(tM) −2(tM −s)P(s < X ≤tM)
+ (tM −s)

−P(X ≤s) + P(s < X ≤tM) + P(X > tM)

= φ(tM) + (tM −s)

−P(X ≤s) −P(s < X ≤tM) + 1 −P(X ≤tM)

= φ(tM) + (tM −s)

1 −2P(X ≤tM)

≥φ(tM).
Similarly, one shows that φ(s) ≥φ(tM) if s > tM.
Proposition 3.16 Let X :  →R be a random variable on the probability space
(, E, P) with E [ |X| ]) < +∞and Var [ X ] < +∞. Then E [ X ] is the unique
minimizer of the function
φ(s) :=
*

|X(x) −s|2 P(dx),
s ∈R.
Proof. The function s →φ(s) = E
%
X2 &
−2 s E [ X ] + s2 is a polynomial
of degree two. Therefore, φ(s) has a unique minimizer at E [ X ].
3.1.8
Exercises
For the reader’s convenience the main deﬁnitions and formulas given in this
chapter are summarized in Figures 3.4–3.6.
Exercise 3.17 The life of a bulb (measured in days) is a random variable X with
an absolute continuous distribution of density
f (x) =
⎧
⎨
⎩
0
if x < 100,
3 · 106
x4
if x ≥100.
Compute the expected value of X.
[150]
Exercise 3.18 Let X be a random variable. Compute the law FX2 of the random
variable X2. Moreover, assuming that PX is absolutely continuous with density
ρX(t), show that PX2 is absolutely continuous by computing its density.

RANDOM VARIABLES
85
Random variables
A random variable X on the probability space (, E, P) is an E-measurable
function X :  →R. Then its distribution, law, expected value and variance
are deﬁned by
PX(A) := P(X ∈A)
∀A ∈B(R),
FX(t) := P(X ≤t) = PX([−∞, t]),
E [ X ] :=
*

X(x) P(dx) =
* +∞
−∞
t PX(dt),
=
* +∞
0
(1 −FX(t)) dt −
* 0
−∞
FX(t) dt,
Var[X] := E
%
(X −E [X])2 &
= E
%
X2 &
−E [ X ]2,
respectively.
Composition formulas
Let X be a random variable on the probability space (, E, P), let ϕ : R →R
be a continuous function and let Y = ϕ ◦X. Then
• PY (A) = PX(ϕ−1(A))
∀A ∈B(R),
• FY(s) =

FX(t)
if ϕ is strictly increasing,
1 −FX(t−)
if ϕ is strictly decreasing, ,
t = ϕ−1(s)
• E [ Y ] =
. +∞
−∞s PY (ds) =
. +∞
−∞ϕ(t) PX(dt),
•
.
ψ(s) PY (ds) =
.
ψ(ϕ(t)) PX(dt).
If ϕ ∈C1(R) is strictly monotone and PX is absolutely continuous, PX(ds) =
ρX(s) ds, then also PY is absolutely continuous and
PY (ds) = ρY(s) dx,
where
ρY(s) = ρX(t)
|ϕ′(t)|,
t = ϕ−1(s).
Figure 3.4
Random variables.
Solution. Since
FX2(t) = P(X2 ≤t) =

0
if t < 0,
P(−√t ≤X ≤√t)
if t ≥0.

86
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Random variables with discrete distribution
A random variable X is in this class if PX is a discrete measure concentrated
at {tj}. The sequence p = (pj) where
pj := PX(

tj

) = P(X = tj),
j = 1, . . . ,
is the density, or mass density function, of PX, from which the basic quantities
of distribution PX, law FX(t), expected value E [ X ] and variance Var[X] can
be computed as follows:
PX(A) =

tj ∈A
P(X = tj),
FX(t) =

tj ≤t
P(X = tj),
*
R
ϕ(t) PX(dt) =

j
ϕ(tj)P(X = tj),
E [ X ] =

j
tjP(X = tj),
Var[X] =

j
(tj −E [ X ])2P(X = tj).
Moreover, if the range of X is integer valued, then
E [ X ] =
∞

k=0
P(X > k) −
0

k−∞
P(X < k).
Figure 3.5
A few formulas relative to discrete random variables.
For every t > 0 we have
FX2(t) = P(−
√
t ≤X ≤
√
t) = FX(
√
t) −FX(−
√
t) + P(X =
√
t).
Assume now that PX(dt) = ρX(t) dt. Then for every t ≥0 we have
FX2(t) =
* √t
−∞
ρ(s) ds −
* −√t
−∞
ρ(s) ds =
* 0
−√t
ρ(s) ds +
* √t
0
ρ(s) ds
=
* t
0
ρ(√σ)
2√σ dσ +
* t
0
ρ(−√σ)
2√σ
dσ,

RANDOM VARIABLES
87
Random variables with absolutely continuous distribution
The distribution PX of a random variable X :  →R deﬁned on a probability
space (, E, P) is said to be absolutely continuous if there exists a non-
negative L1-summable function fX : R →R such that
PX(A) =
*
A
fX(t) dt
∀A ∈B(R)
or, equivalently, if the law FX is such that
FX(t) =
* t
−∞
fX(s) ds
∀t ∈R.
The function fX is called the density of PX (with respect to the Lebesgue
measure). If PX(ds) = fX(s) ds, then
• FX is an absolutely continuous function,
• FX(t) is differentiable at a.e. t ∈R and F ′
X(t) = fX(t) at a.e. t,
• if fX ∈C0(R) then FX ∈C1(R) and F ′
X(t) = fX(t) ∀t,
• E [ X ] =
.
R t fX(t) dt,
• Var[X] =
.
R(t −E [ X ])2fX(t) dt =
.
R t2fX(t) dt −(E [ X ])2,
• for every Borel-measurable function ϕ : R →R
*
R
ϕ(t) PX(dt) =
*
R
ϕ(t)fX(t) dt.
Figure 3.6
A few formulas relative to absolutely continuous random variables.
hence PX2 is absolutely continuous, PX2(dt) = τ(t) dt, where
τ(t) :=

0
if t < 0,
ρ(√t)+ρ(−√t)
2√t
if t > 0.
Exercise 3.19 Let X be a random variable. Compute the law of the random
variable Y := αX + β where α, β ∈R, α ̸= 0. Assuming PX to be absolutely
continuous of density ρ, PX(dt) = ρ(t) dt, show that PY is also absolutely con-
tinuous by computing its density.
Solution. We proceed by computing the law of Y. We have
FY(t) = P(αX + β ≤t) =
⎧
⎨
⎩
P

X ≤t−β
α

if α > 0,
P

X ≥t−β
α

if α < 0,

88
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
hence
FY(t) =
⎧
⎨
⎩
FX

t−β
α

if α > 0,
1 −FX

t−β
α

+ P

X = t−β
α

if α < 0.
Assume now PX(dt) = ρ(t) dt. If α > 0, then changing variable with
u = αs + β, we get
FY (t) = FX
t −β
α

=
*
t−β
α
−∞
ρ(s) ds =
* t
−∞
ρ
u −β
α
 1
α du
i.e. PY (dt) = 1
αρ

t−β
α

dt. If α < 0, then αX + β ≤t if and only if X ≥t−β
α ,
consequently, changing variable with u = αs + β
FY (t) = 1 −FX
t −β
α

=
* +∞
t−β
α
ρ(s) ds
=
* −∞
t
u −β
α
 1
α du = −
* t
−∞
u −β
α
 1
α du
i.e. PY (dt) = −1
α ρ

t−β
α

dt. The two cases above can be summarized to
PY(dt) = 1
|α|ρ
t −β
α

dt
if
PX(dt) = ρ(t) dt.
Exercise 3.20 Let X be a random variable with PX(dt) = r(t) dt and let
f : R →R be a strictly monotone function of class C1(R). Show that
f ◦X
is
also
absolutely
continuous,
Pf ◦X(dt) = s(t) dt,
with
density
s(t) = r(w)/|f ′(w)|, w = f −1(t).
Solution. For any non-negative Borel-measurable function ϕ : R →R
we have
*
R
ϕ(s) Pf ◦X(ds) =
*
R
ϕ(f (t)) PX(dt)
=
*
R
ϕ(f (t)) r(t) dt =
*
R
ϕ(s)r(f −1(s))
1
|f ′(f −1(s))| ds.
In order to get the last equality, we substituted s = f (t), so that t = f −1(s) and
dt = (f −1)′(s) ds = 1/|f ′(f −1(s))| ds. Therefore,
Pf ◦X(ds) =
r(w)
|f ′(w)| ds,
w = f −1(s).
since ϕ is arbitrary.

RANDOM VARIABLES
89
FX
1
0.5
1
2
Figure 3.7
The law in Exercise 3.22.
Exercise 3.21 Let X(x) be the life (measured in years) of a battery, rounded to
an integer. Assume that the density of PX is given by
pX(k) = PX({k}) =

0.2
if 3 ≤k ≤7,
0
otherwise.
Compute:
(i) The probability that a 3-years-old battery is still working.
(ii) The probability that a battery which is at least 3 years old is still working.
(iii) The probability that the battery is working a further 3 years even if it has
already been working for 5 years.
Exercise 3.22 Assume Figure 3.7 represents the graph of the law of a random
variable X. Compute E [ X ], Var [ X ] and P(X ≤0.8).
Exercise 3.23 Let the real-valued random variable X be uniformly distributed in
[0, 1]. Compute the law of Y = max {X −0.5, 0}.
Exercise 3.24 Show that for any real valued random variable X
E
%
X+
&
=
* +∞
0
(1 −FX(s)) ds,
E
%
|X|p &
= p
* +∞
0
tp−1(1 −FX(s)) ds.
As usual, X+ := max {X, 0}.
Exercise 3.25 Show that for any random variable X :  →R the following equal-
ities hold:
E [ max {X, 0} ] =
* +∞
0
|t| PX(dt),

90
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
E [ min {X, 0} ] =
* 0
−∞
|t| PX(dt),
E [ |X| ] =
* +∞
−∞
|t| PX(dt),
(3.23)
E
%
|X|p &
=
* +∞
−∞
|t|p PX(dt)
∀p ≥0.
Exercise 3.26 Let X :  →R be a discrete random variable. Prove the following
inequalities.
E [ max {X, 0} ] =

y∈X(),y≥0
yP(X = y),
E [ min {X, 0} ] =

y∈X(),y≤0
yP(X = y),
E [ |X| ] =

y∈X()
|y|P(X = y),
E
%
|X|p &
=

y∈X()
|y|pP(X = y).
(3.24)
Exercise 3.27 Let X :  →R be a random variable and let ϕ : [a, b] →R be
a strictly monotone increasing function of class C1([a, b]). Prove the integration
by parts formula:
* b
a
ϕ′(t)FX(t) dt +
*
]a,b]
ϕ(t) PX(dt) = ϕ(b)FX(b) −ϕ(a)FX(a).
(3.25)
Solution. Let Eτ :=

t ∈R
 ϕ(t)1]a,b](t) > τ

. Cavalieri formula (B.22)
yields
*
]a,b]
ϕ(t) PX(dt) =
*
R
ϕ(t)1]a,b](t) PX(dt) =
* +∞
0
PX(Eτ) dτ.
(3.26)
Since
Eτ =

t ∈]a, b]
 ϕ(t) > τ

=
⎧
⎪⎨
⎪⎩
]a, b]
if τ ≤ϕ(a),
]ϕ−1(τ), b]
if ϕ(a) < τ ≤b,
∅
if τ > ϕ(b),
we get
PX(Eτ) =
⎧
⎪⎨
⎪⎩
FX(b) −FX(a)
if 0 < τ ≤ϕ(a),
FX(b) −FX(ϕ−1(τ))
if ϕ(a) < τ ≤ϕ(b),
0
if τ > ϕ(b).

RANDOM VARIABLES
91
Thus
* ∞
0
PX(Eτ) dτ =
* ϕ(a)
0

FX(b) −FX(a)

dτ
+
* ϕ(b)
ϕ(a)

FX(b) −FX(ϕ−1(τ))

dτ
= ϕ(a)(FX(b) −FX(a))
+ FX(b)(ϕ(b) −ϕ(a)) −
* b
a
ϕ′(s)FX(s) ds
= FX(b)ϕ(b) −ϕ(a)FX(a) −
* b
a
ϕ′(s)FX(s) ds.
The last equality and (3.26) yield the claim.
3.2
A few discrete distributions
3.2.1
Bernoulli distribution
The Bernoulli distribution of parameter p, 0 ≤p ≤1, is the probability measure
B(p) supported on the two points set {0, 1} whose density is
B(p)({0}) = 1 −p,
B(p)({1}) = p.
The expectation and the variance of random variables X such that PX = B(p) are
E [ X ] = p
and
Var [ X ] = p(1 −p),
(3.27)
respectively.
Any random variable X whose range is the two points set {0, 1} has a
Bernoulli type distribution B(p) with p := P(X = 1). These random variables
are called Bernoulli trials of parameter p. For instance let (, E, P) be a prob-
ability space and E ∈E. The characteristic function 1E(x) is a Bernoulli trial of
parameter P(E) since

x ∈ | 1E(x) = 1

= E.
3.2.2
Binomial distribution
The binomial distribution B(n, p) of parameters n and p, n ≥1, 0 ≤p ≤1, is
the measure supported on the n + 1 points set {0, . . . , n} of density
B(n, p)({k}) =
n
k

pk(1 −p)n−k,
0 ≤k ≤n.
B(n, p) is a probability measure since the Newton binomial formula yields

k
B(n, p)({k}) =
n

k=0
n
k

pk(1 −p)n−k = (p + 1 −p)n = 1.

92
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
The expected value and the variance of a random variable X such that PX =
B(n, p) are
E [ X ] = np
and
Var [ X ] = np(1 −p),
(3.28)
respectively.
Let us prove (3.28). For p = 0 or 1, (3.28) is trivial. Assume 0 < p < 1 and
let z := p/(1 −p). From (iv) of Figure 1.4, we infer
E [ X ] =
n

k=0
k
n
k

pk(1 −p)n−k
= (1 −p)n
n

k=0
k
n
k

zk = (1 −p)nnz(1 + z)n−1 = np;
moreover, (v) of Figure 1.4 yields
E
%
X2 &
=
n

k=0
k2
n
k

pk(1 −p)n−k = (1 −p)n
n

k=0
k2
n
k

zk
= (1 −p)nnz(1 + z)n−2(1 + nz) = np(1 −p + np),
hence
Var [ X ] = E
%
X2 &
−E [ X ]2 = np(1 −p + np) −n2p2 = np(1 −p).
Example 3.28 Consider n ‘independent’ random trials of parameter p,
0 ≤p ≤1. Each sequence x of outcomes can be arranged as n-tuple of
zeros and ones, a n-byte, x = (x1, . . . , xn). We already know that any such
sequence has probability pk(1 −p)n−k, see Section 2.2.5, therefore we model n
‘independent’ trials as the probability space (, E, P) where the sample space is
 = {0, 1}n, E = P() and P is deﬁned by its density, P({x}) := pk(1 −p)n−k
if x has k ones and n −k zeros.
Let X : {0, 1}n →N be the random variable counting the number of suc-
cesses, i.e. the function that for each n-byte x returns the number of ones in it.
Then for every k = 0, 1, . . . ,
P(X = k) =
n
k

pk(1 −p)n−k
(3.29)
since there are
n
k

n-bytes with exactly k ones and all of them have probability
pk(1 −p)n−k. Therefore, the distribution of X is the binomial one of parameters
n and p, PX = B(n, p) for short.
3.2.2.1
Graph of the binomial distribution
As k increases from 0 to n −1, the quotient
B(n, p)({k + 1})
B(n, p)({k})
=
p
1 −p
n −k
k + 1
(3.30)

RANDOM VARIABLES
93
B(30, 0.8)
B(30, 0.2)
30
25
20
15
10
5
0
30
25
20
15
10
5
0
0.3
0.25
0.15
0.05
−0.05
0.2
0.1
0
−0.2
0
1.2
1
0.8
0.6
0.4
0.2
B(30, 0.8)
B(30, 0.2)
(a)
(b)
Figure 3.8
(a) The densities and (b) the laws of two binomial distributions.
decreases from
p
1−pn to
p
1−p
1
n. Thus,
(i) if n p
1−p < 1, then k →B(n, p)({k}) is monotone decreasing,
(ii) if 1
n
p
1−p > 1, then k →B(n, p)({k}) is monotone increasing,
(iii) if 1
n
p
1−p < 1 <
p
1−pn, then there exists k such that k →B(n, p)({k}) is
strictly increasing on 0, . . . , k −1 and strictly decreasing on k, . . . , n. By
(3.30), k is the smallest integer greater than the solution of
p
1−p
n−k
k+1 = 1,
that is the smallest integer greater than np −1 + p.
The densities and the laws of two binomial distributions are plotted in
Figure 3.8.
3.2.3
Hypergeometric distribution
Let w, r be non-negative and let 0 ≤n ≤w + r. The hypergeometric distribution
H(w, r, n) of parameters w, r and n is the discrete measure whose density is
p(k) = H(w, r, n)({k}) :=
w
k
 r
n−k

w+r
n
 ,
k = 0, 1, . . . , n.
H(w, r, n) is a probability measure since

k
H(w, r, n)({k}) =
n

k=0
w
k
 r
n−k

w+r
n

= 1
by the Vandermonde formula (A.5).
Of course, p(k) ̸= 0 only if 0 ≤k ≤w and n −r ≤k ≤n, so that H(w, r, n)
is supported on the integers between max {0, n −r} and min {w, n}.

94
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
One can prove, see Exercise 4.10, that the expected value and the variance
of any random variable X with PX = H(w, r, n) are
E [ X ] =
nw
w + r
and
Var [ X ] =
nwr
(w + r)2

1 −
n −1
w + r −1

,
(3.31)
respectively.
Here we only want to point out that X has the same expected value as
a random variable with binomial distribution B(n, p) where p := w/(w + r).
Moreover, when (w + r)/n →∞, that is, when the ratio between the total num-
ber of balls inside the urn and the number of drawings becomes larger and larger,
then the variance Var [ X ] of X converges to np(1 −p), i.e. to the variance of a
variable with binomial distribution B(n, p). Note that B(n, p) is also the distribu-
tion of the random variable that counts the number of drawn white balls when we
draw in sequence n balls reinserting each extracted ball before drawing the next.
Example 3.29 Assume we have an urn containing w distinguishable white balls
and r distinguishable red balls. We extract (without replacement) n balls. Con-
sider the probability space (, E, P) where the sample space  is the set of
all admissible drawings, i.e. of the n-words made of symbols chosen among w
white balls and r red balls, E = P() and P is the uniform probability on  of
density 1/
w+r
n

.
Trivially, the random variable X(x) from  into N that counts the number
of white balls in the drawn sequence x of n balls follows the distribution PX =
H(w, r, n). In fact, for any k, the number of drawings with k white balls and
n −k red balls is
w
k
 r
n−k

.
3.2.4
Negative binomial distribution
Let n ≥1 and p ∈[0, 1]. The negative binomial or Pascal distribution B(−n, p)
is the measure supported on non-negative integers of density
B(−n, p)({k}) :=
n + k −1
k

pn(1 −p)k,
∀k ≥0.
Since
∞

k=0
B(−n, p)({k}) =
∞

k=0
n + k −1
k

pn(1 −p)k = pn
∞

k=0
−n
k

(p −1)k
=
pn
(1 + p −1)n = 1,
see Proposition 1.2, B(−n, p) is a probability measure.
The expected value and the variance of a random variable X with PX =
B(−n, p) are given by
E [ X ] = n1 −p
p
and
Var [ X ] = n1 −p
p2
,
(3.32)

RANDOM VARIABLES
95
respectively. We have
E [ X ] = pn
∞

k=0
k
−n
k

(p −1)k
= −npnz(1 + z)n−1 = −npn(p −1)p−n−1 = n1 −p
p
from (iv) of Figure 1.4 with z := p −1; moreover, from (v) of Figure 1.4 again
with z := p −1, we get
E
%
X2 &
= pn
∞

k=0
k2
−n
k

(p −1)k
= −npnz(1 −nz)(1 + z)−n−2 = 1 −p
p2
(1 + n −np),
hence
Var [ X ] = E
%
X2 &
−E [ X ]2 = 1 −p
p2
(1 + n −np) −n2 (1 −p)2
p2
= n1 −p
p2
.
Example 3.30 Consider a denumerable sequence of Bernoulli trials. Each out-
come is a binary sequence. Consider the function T that counts the number of
failures before the nth success. T is a discrete random variable with values in
the set of non-negative integers. We claim that PT = B(−n, p).
Let En,k be the set of sequences of n + k trials with k zeroes and n ones,
and with a one at the (n + k)th trial. Then, trivially, P(T = k) = P(En,k). The
cardinality of En,k is
n+k−1
n−1

=
n+k−1
k

, i.e. the number of n + k −1 draw-
ings with n −1 ones and k zeroes, and each sequence in En,k has probability
(1 −p)kpn, see Section 2.2.5. Therefore,
PT ({k}) :=
n + k −1
k

(1 −p)kpn = B(−n, p)({k}),
thus concluding that PT = B(−n, p).
The densities and the laws of two negative binomial distributions are plotted
in Figure 3.9.
3.2.5
Poisson distribution
The Poisson distribution P(λ) of parameter λ > 0 is the measure supported on
the non-negative integers of mass density
P(λ)({k}) := λk
k! e−λ.
(3.33)

96
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
20
15
10
5
0
−0.2
1.2
1
0.8
0.6
0.4
0.2
0
20
15
10
5
0
−0.05
0
0.05
0.1
0.2
0.15
(a)
(b)
B(−20, 0.8)
B(−10, 0.5)
B(−20, 0.8)
B(−10, 0.5)
Figure 3.9
(a) The densities and (b) the laws of two negative binomial distribu-
tions.
P(λ) is a probability measure since
∞

k=0
P(λ)({k}) = e−λ
∞

k=0
λk
k! = e−λeλ = 1.
The mean and the variance of a random variable X such that PX = P(λ) are
given by
E [ X ] = λ
and
Var [ X ] = λ,
(3.34)
respectively. The proof is elementary.
Poisson distribution approximates B(n, λ/n) for large n. More precisely, the
following holds.
Proposition 3.31 (Rare events property) Let λ ∈R+ and let

pn

be a non-
negative sequence such that npn →λ. Then, for any k ∈N
B(n, pn)({k}) −→

P(λ)({k})
if 0 < λ < +∞,
0
if λ = 0, +∞
as n →∞.
(3.35)
Proof. Since
B(n, pn)({k}) =
n
k

pk
n(1 −pn)n−k
= n(n −1) · · · (n −k + 1)
k!nk
(npn)k(1 −pn)n−k
=: n(n −1) · · · (n −k + 1)
k!nk
hn
(3.36)

RANDOM VARIABLES
97
and n(n−1)···(n−k+1)
nk
→1 as n →∞, it sufﬁces to compute the limit of hn, or,
equivalently, of
log hn = k log(npn) + (n −k) log(1 −pn)
= npn

k log(npn)
npn
+

1 −k
n
log(1 −pn)
pn

.
If λ < +∞, then pn →0. Since log(1−x)
x
→−1 as x →0, we get
log hn →k log λ −λ,
i.e.
hn →λke−λ
as n →∞. This proves (3.36) for ﬁnite λ. If λ = +∞, recalling that log x/x →0
as x →+∞and log(1 −x)/x ≤−1 for any 0 < x < 1, we get that log hn →
−∞, so that hn →0.
Remark 3.32 Actually, we have just proved that if njpnj →λ for some subse-
quence {pnj } of {pn}, then
B(nj, pnj )({k}) −→

P(λ)(k)
if 0 < λ < +∞,
0
if λ = 0, +∞
as j →∞.
Remark 3.33 Using McLaurin expansions
n(n −1) · · · (n −k + 1)
nk
= 1 −k(k −1)
2n
+ O
 1
n2

,

1 −λ
n
n
= e−λ + e−λ λ2
n + O
 1
n2

one easily estimates the relative error: we have
B(n, λ/n)({k}) −P(λ)({k})
 ≤P(λ)({k})(k + λ)2
n
.
P(λ) is known as the distribution of rare events. In fact, many counting
problems can be seen as the counting of the number of successes in n
‘independent’ trials with n large and the probability of success in each trial,
p, small.
Example 3.34 Assume a public service (such as the emergency services) receives
– on average– λ calls per day. Assume there are n possible users and that they
call the service independently of the others. The number of daily calls thus follows
a binomial distribution B(n, p), where p is unknown. Since the expected value
of B(n, p) is np, we can assume p = λ/n. Thus, for large n, the probability that
the service gets k daily calls is approximately P(λ)({k}) = λk
k! e−λ.
Events randomly distributed in time or space that can be modelled by means of
a Poisson distribution with good approximation appear in many different contexts.

98
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
For instance, the number of calls per hour a callcentre receives, the number of
cars passing through a toll gate at a certain time of the day, the number of
car accidents that occur on a certain stretch of road on a public holiday, the
number of ﬂaws in a certain length of yarn produced by a textile machine, and
the number of wrong notes played per hour by a musician during a concert are
possible examples.
Example 3.35 Assume a machine is subject to an average number λ of
breakdowns per unit time; assume also the average number of breakdowns
is proportional to the observation time. Thus, in an arbitrary time interval
[s, s + t], the average number of breakdowns is λt.
Let us divide the interval [0, t] in to N intervals I1, . . . , IN of equal length.
Assume:
• the probability that a breakdown occurs in the interval Ii is p = λt/N;
• the probability that two or more breakdowns occur in any of the intervals
Ii is negligible with respect to p;
• the events Xi, ‘a breakdown occurs during the interval Ii’ are independent,
so that the distribution of the random variable N
i=1 Xi that counts the num-
ber of breakdowns in the interval [0, t] is the binomial one, B(N, λt/N).
If the assumptions stated above hold for any large enough N, then the prob-
ability that k breakdowns occur in the time interval [0, t] is approximately
P(λt)({k}) = λktk
k! e−λt.
For a precise statement and further details see Section 6.1.
The densities and the laws of a binomial distribution and of a Poisson one
are plotted in Figure 3.10.
3.2.6
Geometric distribution
The geometric distribution of parameter p, 0 < p < 1, is the probability measure
G(p) supported on positive integers of density
G(p)({k}) := (1 −p)k−1p,
∀k ≥1.
Thus
FG(p)(t) =

1≤k≤t
(1 −p)k−1p = 1 −(1 −p)[t].
The expected value and the variance of a random variable X with geometric
distribution, PX = G(p) are
E [ X ] = 1
p
and
Var [ X ] = (1 −p)
p2
,
(3.37)

RANDOM VARIABLES
99
P(15)
P(15)
−0.05
0
0.05
0.1
0.2
0.15
20
30
15
25
10
5
0
20
30
15
25
10
5
0
−0.2
1.2
1
0.8
0.6
0.4
0.2
0
(a)
(b)
B(30, 0.5)
B(30, 0.5)
Figure 3.10
(a) The densities and (b) the laws of the binomial distribution
B(30, 0.5) and of the Poisson distribution P(15), respectively.
respectively. To show the former, apply formula (i) of Figure 1.4 with
z = 1 −p; we get
E [ X ] =
∞

k=1
kG(p)({k}) =
p
1 −p
∞

k=1
k(1 −p)k =
p
1 −p
1 −p
p2
= 1
p.
For the latter, apply formula (ii) of Figure 1.4 with z = 1 −p to get
E
%
X2 &
=
∞

k=0
k2p(1 −p)k−1 = p2 −p
p3
,
hence
Var [ X ] = E
%
X2 &
−E [ X ]2 = 2 −p
p2
−1
p2 = 1 −p
p2
.
Example 3.36 Let the random variable T describe the waiting time for the ﬁrst
success in a Bernoulli process of parameter p, 0 < p < 1,
T (x) := min

n
 x = (xk), x1 = · · · = xn−1 = 0, xn = 1

,
where T (x) = +∞if success never occurs, i.e. if x = (xn) is such that xn = 0
∀n. The distribution of the random variable X is the geometric one G(p). In
fact, in Section 2.2.5 we observed that
P(T = +∞) = P({0, 0, . . . }) = 0
and
P(T = k) = P({(0, 0, . . . , 0



k−1
, 1)}) = (1 −p)k−1p.

100
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
One can consider also the shifted geometric distribution deﬁned on the non-
negative integers
G′(p)({k}) := p(1 −p)k
∀k ≥0.
(3.38)
For instance, the random variable that counts the number of failures before the
ﬁrst success in a Bernoulli process of parameter p has this type of distribution.
The densities of both G(p) and G′(p) are geometric sequences.
3.2.6.1
Memorylessness
An important feature of random variables with discrete geometric distribution is
the memoryless property.
Deﬁnition 3.37 Let X be a non-negative, integer valued random variable on a
probability space (, E, P). We say that X has the memoryless property if
P

X ≤i + j
 X ≥j

= P(X ≤i)
∀i, j ∈N.
(3.39)
We have the following.
Theorem 3.38 Let X be a non-negative random variable with discrete distribution
supported on N and assume that p := PX(X = 0) is such that 0 < p < 1. Then
X is a discrete memoryless random variable if and only if PX = G′(p).
Proof. Suppose that PX = G′(p) with 0 < p < 1. Then for every k ≥0,
P(X = k) = p(1 −p)k, hence we have P(X = 0) = p and, moreover, for every
i, j ≥0
P(X ≤i) = p
i

k=0
(1 −p)k,
P(j ≤X ≤i + j) =
j+i

k=j
p(1 −p)k
= p(1 −p)j
i
k=0
(1 −p)k = (1 −p)jP(X ≤i)
P(X ≥j) = p
∞

k=j
(1 −p)k = p(1 −p)j
1
1 −(1 −p) = (1 −q)j,
from which (3.39) follows.
Let us prove the converse. For k = 0, 1, . . . , let xk := P(X = k) be the den-
sity of PX at k. We ﬁrst infer from (3.39) that P(X = 0) < 1 since P(X ≥j) > 0

RANDOM VARIABLES
101
for all non-negative integers j, and that P(X = 0) > 0 since otherwise
P(X = j) = 0 ∀j. Memorylessness property (3.39) with i = 0 yields
P(X = j)
P(X ≥j) = P(X = 0)
∀j
i.e.
xj = x0
∞

k=j
xk
∀j ≥0.
Subtracting from the previous formula a similar formula with j replaced by
j + 1, we get xj −xj+1 = x0xj, i.e.
xj+1 = xj(1 −x0)
∀j ≥0,
hence by induction xj = x0(1 −x0)j ∀j ≥0, as required.
The density and the law of a geometric distribution are plotted in Figure 3.11.
3.2.7
Exercises
Exercise 3.39 Let X be the number of defective parts produced each day in
a production chain. Assume X is a random variable that follows the binomial
distribution, with mean 6 and variance 5.88:
• Compute the density of X.
• Compute the probability that no defective part is produced today. How can
this value be approximated?
%
50−300 ≃e−6&
G(0.7)
G(0.7)
30
25
20
15
10
5
0
30
25
20
15
10
5
0
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
−0.04
−0.02
1.2
1
0.8
0.6
0.4
0.2
0
−0.2
(a)
(b)
Figure 3.11
(a) The density and (b) the law of a geometric distribution.

102
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 3.40 A random variable X follows the binomial distribution. The mean
and the variance are 6 and 2.4, respectively. Compute P(X = 5), P(X = 6) and
P(X = 7).
[P(X = 5) ≃0.2, P(X = 6) ≃0.25, P(X = 7) ≃0.215]
Exercise 3.41 Show that the memoryless property (3.39) is equivalent to
P(X = 0) = P(X = j)
P(X ≥j)
∀j ≥0.
Exercise 3.42 Let X be a geometrically distributed random variable, that is
P(X = k) = (1 −p)k−1p ∀k ≥1. Show that
P

X < i + j
 X ≥j) = P(X ≤i)
∀i, j ≥1.
Infer that the above equality is equivalent to
P(X = 1) = P(X = j)
P(X ≥j)
∀j ≥1.
3.3
Some absolutely continuous distributions
3.3.1
Uniform distribution
Let ]a, b[⊂R, a < b, be an interval of R and let
ρ(x) =

0
if x /∈]a, b[,
1
b−a
if x ∈]a, b[.
The measure (B(R), U) deﬁned as
U(A) := L1(A ∩]a, b[)
b −a
=
*
A
ρ(x) dx,
A ∈B(R),
is absolutely continuous with density ρ. Moreover, (B(R), U) is a probability
measure on  :=]a, b[ called the uniform distribution on ]a, b[. If X is a random
variable with PX = U, then
E [ X ] =
* b
a
t
1
b −a dt = a + b
2
,
Var [ X ] = E
%
X2 &
−E [ X ]2 = (a −b)2
12
.
(3.40)
Flipping a fair coin inﬁnitely many times is strictly related to the uniform
distribution. Let T : {0, 1}∞→[0, 1] map each binary sequence

αk

into the
real number ∞
k=1
αk
2k :
T ({αk}) :=
∞

k=1
αk
2k .

RANDOM VARIABLES
103
Theorem 3.43
Let ({0, 1}∞, E, Ber(∞, 1
2)) denote the probability space
generated by an inﬁnite Bernoulli process of parameter p = 1/2. Then T is a
random variable on ({0, 1}∞, E, Ber(∞, 1
2)) which is uniformly distributed on
the interval [0, 1]. In other words, PT is the Lebesgue measure on the interval
[0, 1].
Proof. Let E be the family of events associated with the inﬁnite Bernoulli
process. We have to prove the following:
• T is a random variable, i.e.

{βk} | T ({βk}) ≤x

∈E ∀x ∈[0, 1].
• Ber

∞, 1
2

(T ≤x) = L1([0, x]) = x ∀x ∈[0, 1].
As previously stated, see the proof of Proposition 2.32, the map T is surjective
but it is not injective. In fact if a sequence

αk

is deﬁnitely 0 but not identically
0 (deﬁnitely 1 but not identically 1), then there exists a sequence

βk

which
is deﬁnitely 1 (0, respectively) such that T (

αk

) = T (

βk

). Thus, if N is the
subset of the sequences which are deﬁnitely 1, T is a one-to-one map between
{0, 1}∞\ N and [0, 1[. Notice that N is denumerable, hence a zero-probability
event in E, see Example 2.33.
Let x, y ∈[0, 1[ and α =

αk

, β =

βk

∈{0, 1}∞\ N such that T (α) = x,
T (β) = y. One can easily check that y ≤x if and only if β ≤α with respect
to the lexicographic order, i.e. β1 < α1, or β1 = α1 and β2 < α2, or β1 = α1,
β2 = α2 and β3 < α3 and so on.
For any α =

αk

∈{0, 1}∞\ N and for any non-negative integer k ≥1
deﬁne
Ak :=

β ∈{0, 1}∞ βi = αi ∀i = 1, . . . , k −1, βk < αk

so that T (β) = y ≤x = T (α) if and only if β ∈Ak for some k, that is
∪kAk ⊂

β
 T (β) ≤x

⊂∪kAk ∪N.
Thus, there exists a zero-probability event M ⊂N such that

β
 T (β) ≤x

= ∪kAk ∪M.
Notice that Ak is empty whenever αk = 0. If αk = 1, then the elements of
Ak are the sequences β =

βj

such that βj = αj for every j = 1, . . . , k −1
and βk = 0. In particular each set Ak is a cylinder, hence Ak ∈E and therefore,
{β | T (β) ≤x} ∈E being a denumerable union of E-measurable sets. Since x ∈
[0, 1[ is arbitrary, we have proven that T is a random variable.
Moreover, the previous description proves that for every k ≥1
Ber

∞, 1
2

(Ak) =
⎧
⎨
⎩
0
if αk = 0,
Ber

k, 1
2

({α1, . . . , αk−1, 0}) = 1
2k
if αk = 1,

104
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
i.e. Ber(∞, 1
2)(Ak) = αk
2k . Since Ber(∞, 1
2)(M) = 0 and the sets Ak are pairwise
disjoint, we get
Ber

∞, 1
2

(T ≤x) =
∞

k=1
Ber

∞, 1
2

(Ak) =
∞

k=1
αk
2k = x.
Both claims are proven.
3.3.2
Normal distribution
The
normal or Gaussian distribution of parameters m ∈R and σ > 0 is the
measure (B(R), N(m, σ 2)) with
N(m, σ 2)(A) :=
*
A
f (t) dt,
A ∈B(R)
where
f (t) :=
1
√
2πσ 2 exp
−(t −m)2
2σ 2

.
Since
* +∞
−∞
e−s2 ds = √π,
N(m, σ 2) is a probability measure on R. Any random variable X such that
PX = N(m, σ 2) has mean m and variance σ 2, that is
E [ X ] = m
and
Var [ X ] = σ 2,
as an easy computation shows. The law of the normal distribution N(0, 1) of
parameters m = 0 and σ 2 = 1 is usually denoted by (t) instead of FN(0,1)(t),
(t) :=
1
√
2π
* t
−∞
e−x2
2 dx.
Clearly (−∞) = 0, (0) = 0.5 and (+∞) = 1, see Figures 3.12 and 3.13.
In practice, ‘independent’ measurements are often ‘distributed’ according to
a normal distribution. As we shall see, the Gaussian distribution plays a central
role among distributions associated with ‘independent’ random variables.
Exercise 3.44 The densities of two normal distributions are rescaled versions of
each other since
f (t) =
1
√
2πσ 2 exp
−(t −m)2
2σ 2

= 1
σ ′t −m
σ

.
Infer that if X is a random variable such that PX = N(m, σ 2), then the random
variable Y = (X −m)/σ follows the Gaussian distribution N(0, 1).

RANDOM VARIABLES
105
6
4
2
0
−6
−4
−2
4
2
0
−4
−2
0.5
0.2
0.3
0.4
0.1
0
−0.1
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
(a)
(b)
N(0.00, 1.00)
N(0.0, 1.0)
Figure 3.12
(a) The density and (b) the law of the normal distribution N(0, 1).
t
FN(0,1)(t)
t
FN(0,1)(t) FN(0,1)(t)
t
FN(0,1)(t)
t
0.0
0.5
2.0
0.9772
0.5
0.0
0.8
0.8416
0.25
0.5987
2.25
0.9878
0.53
0.0753
0.85
1.0364
0.50
0.6915
2.5
0.9938
0.57
0.1764
0.9
1.2816
0.75
0.7734
2.75
0.9970
0.6
0.2533
0.95
1.6449
1.0
0.8413
3.0
0.9987
0.62
0.3055
0.99
2.3263
1.25
0.8944
3.25
0.9994
0.67
0.4399
0.999
3.0902
1.5
0.9332
3.5
0.9998
0.7
0.5244
0.9999
3.7190
1.75
0.9599
3.75
0.9999
0.75
0.6745
0.99999
4.2649
Figure 3.13
A few values of the normal law and of its inverse.
Exercise 3.45 Assume X is a Gaussian random variable whose mean and variance
are m and σ 2, respectively. Compute the distribution and the expected value of
the random variable |X −m|.
Solution. We ﬁrst compute the law of |X −m|. Clearly, P(|X −m| ≤t) = 0
if t ≤0. Since the random variable (X −m)/σ follows the distribution N(0, 1),
see Exercise 3.44, for t > 0 we have
P

|X −m| ≤t

= P

−t
σ ≤X −m
σ
≤t
σ

=
2
√
2π
*
t
σ
0
exp

−x2
2

dx,
from which we infer that |X −m| is absolutely continuous and
P|X−m|(A) =
*
A
g(t) dt

106
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
where
g(t) =
⎧
⎪⎨
⎪⎩
0
if t ≤0,
/
2
π
1
σ exp

−t2
2σ 2

if t > 0.
We now compute the expected value:
E [ |X −m| ] =
* +∞
−∞
|t| g(t) dt =
* +∞
0
t g(t) dt =
/
2
π σ.
Exercise 3.46 The life (measured in hours) of a bulb is a Gaussian random vari-
able of expected value 200 and variance σ 2. A customer asks that at least 90%
of the bulbs live longer than 150h. What is the maximum possible value for σ 2?
[≃39]
3.3.3
Exponential distribution
For λ > 0, set
ρλ(t) =

0
if t < 0,
λe−λt
if t ≥0,
(3.41)
and for any Borel set A ⊂R let
exp(λ)(A) :=
*
A
ρλ(t) dt.
Since exp(λ)(R) =
. +∞
0
λe−λt dt = 1, exp(λ) is a probability measure on R sup-
ported on [0, ∞[, called the exponential distribution of parameter λ. The expected
value and the variance of an exponentially distributed random variable X are
E [ X ] = 1
λ
and
Var [ X ] = 1
λ2 ,
(3.42)
respectively.
Deﬁnition 3.47 A non-negative random variable X is memoryless if
P

X ≤t + s
 X ≥t

= P(X ≤s)
∀t, s > 0.
(3.43)
The meaning of the above property is clear: X is memoryless if the distribu-
tion of X does not change when we shift the observation interval from [0, s] to
[t, t + s], whatever t. The (conditional) distribution remains the same indepen-
dently of the beginning of observation. Observe that P(X ≤t + s | X ≥t) and
P(X ≤t + s | X > t) differ if and only if P(X = t) ̸= 0, i.e. if and only if FX
has a jump at t.

RANDOM VARIABLES
107
Example 3.48 Random variables that describe ‘time’ often follow a memoryless
distribution. Assume, for instance, that a random variable X describes the time
needed for completing a certain task. Then P(X ≤t + s | X ≥s) is the proba-
bility that the task is completed by time t + s knowing that it was still going on
immediately before time s.
Exponentially distributed random variables are the only random variables
where the distribution takes values in [0, +∞] and which are ‘memoryless’.
Theorem 3.49 Assume that the range of a random variable X is in [0, +∞] and
FX(0) < 1. Thus X is memoryless if and only if X follows the exponential law
exp(μ) where μ = −log(1 −FX(1)) = F ′
X(0+).
In order to prove the theorem, we need the following.
Lemma 3.50 Let α : R+ →R be either continuous or monotone and such that
α(t + s) = α(t) + α(s) for any t, s ≥0. Then α(t) = t α(1) ∀t ≥0.
Proof. Choosing t = s = 0 we immediately get α(0) = 0. By induction, we
get α(k) = α(1 + · · · + 1) = k α(1) and nα(k/n) = α(k/n) + · · · + α(k/n) =
α(k) = k α(1) for any two positive integers k and n. Thus α(q) = q α(1) for
any positive rational number q. If α is continuous the claim follows. If α
is monotone, then for any positive t let

pn

and

qn

be two sequences of
rational numbers approximating t from below and from the above, respectively.
Assuming, for instance, α monotone increasing, we have
pnα(1) = α(pn) ≤α(t) ≤α(qn) = qnα(1)
for any n, hence the claim, letting n →∞.
Proof of Theorem 3.49. If PX = exp(μ), then FX(t) = 1 −e−μt for any t ≥0,
so that, for any t, s > 0 we get
P(X ≤t + s, X ≥t) = FX(t + s) −lim
s→t−FX(s) = e−μt −e−μ(t+s)
= e−μt(1 −e−μs) = (1 −FX(t))FX(s)
= P(X ≥t)P(X ≤s),
i.e. (3.43) is proven.
Conversely, deﬁne FX(t−) := lims→t−FX(s) and δX(t) := FX(t) −FX(t−).
Property (3.43) amounts to the equality
(1 −FX(t−))FX(s) = FX(t + s) −FX(t−)
∀t, s > 0.
i.e.
(1 −FX(t) + δX(t))FX(s) = FX(t + s) −FX(t) + δX(t)
∀t, s > 0. (3.44)

108
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
From the right continuity of FX and letting s →0+ in (3.44) we get
(1 −FX(t) + δX(t))FX(0) = δX(t)
∀t > 0.
(3.45)
We now prove that FX(0) = 0. Assume, by contradiction, that FX(0) > 0. Since
FX is right-continuous and, by assumption FX(0) < 1, then FX(t) ∈]0, 1[ for any
t in a right-hand neighbourhood of 0, so that by (3.45) the jump δX(t) would be
positive for any t in such a neighbourhood. This is a contradiction, since FX can
be discontinuous in at most a denumerable set of points.
Since FX(0) = 0, again from (3.45) we infer δX(t) = 0 for any t ≥0, i.e. FX
is continuous on R, hence (3.44) reduces to
(1 −FX(t))FX(s) = FX(t + s) −FX(t)
∀t, s > 0,
or, equivalently, to
(1 −FX(t))(1 −FX(s)) = (1 −FX(t + s))
∀t, s > 0.
(3.46)
Set α(t) := log(1 −FX(t)), t > 0. Then α(t + s) = α(t) + α(s) ∀t, s > 0. Since
α(t) is monotone decreasing, Lemma 3.50 yields α(t) = tα(1) ∀t > 0, i.e.
1 −FX(t) = e−μt
∀t > 0
where μ := −α(1) > 0. Finally, by continuity we infer 1 −FX(t) = e−μt ∀t ≥0
and, consequently, the right-hand side derivative of FX at 0 exists and is equal
to μ
F ′
X+(0) = lim
t→0+
FX(t)
t
= μ.
Remark 3.51 Looking at the proof of Theorem 3.49, one sees that the exponen-
tially distributed random variables are the only random variables X with range
of the distribution in [0, +∞[ such that FX(0) < 1 and
P

X ≤t + s
 X > t

= P(X ≤s)
∀t, s > 0.
(3.47)
Therefore (3.43) and (3.47) are equivalent formulations of the memoryless prop-
erty. We need a proof, since we have not assumed that the law of X is continuous.
The densities and the laws of three exponential distributions are plotted in
Figure 3.14.
3.3.4
Gamma distributions
Let α and λ be positive real parameters, and for t ∈R, let
f (t) = fα,λ(t) :=

0
if t ≤0,
λαtα−1e−λx
(α)
if t > 0,
(3.48)

RANDOM VARIABLES
109
4
3
2
1
0
–1
4
3
2
1
0
–1
0
0.5
1.5
1
2
0
0.5
1.5
1
2
fexp(0.5)
fexp(1.5)
fexp(2.0)
Fexp(0.5)
Fexp(1.0)
Fexp(2.0)
(a)
(b)
Figure 3.14
Three exponential distributions. On the left, their densities; on the
right, their laws.
0
1
0.5
1.5
2.5
3.5
4.5
2
3
4
10
9
8
7
6
5
4
3
2
1
0
Figure 3.15
The (α) function.
where (α) :=
. +∞
0
sα−1 e−s ds, see Figure 3.15. For any Borel set A ⊂R let
(α, λ)(A) :=
*
A
f (t) dt.
It can be easily shown that (α, λ) is a probability measure on R supported
on R+, called the Gamma distribution of shape α and scale 1/λ. Observe that
(1, λ) is the exponential distribution exp(λ).
If X is a Gamma distributed random variable with shape α and scale 1/λ,
then the expected value and the variance of X are
E [ X ] = α
λ
and
Var [ X ] = α
λ2 ,
(3.49)
respectively.

110
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
2
0
6
5
4
3
2
1
0
0.5
1.5
2.5
3
1
2
0
0.2
0.4
0.6
0.8
1
0
fΓ(2.0,0.8)
fΓ(2.0,1.0)
fΓ(2.0,2.0)
FΓ(1.0,0.5)
FΓ(1.0,1.0)
FΓ(1.0,2.0)
Figure 3.16
Three Gamma distributions. On the left, their densities; on the right,
their laws.
Gamma distributions (α, λ) with integer shape parameter α are also called
Erlang distributions. The distribution ( n
2, 1
2) is called a chi-squared distribution
with n degrees of freedom; it is also denoted by χ2(n). Erlang and chi-squared
distributions are connected to exponential and normal distributions, respectively,
see Exercises 4.49 and 4.62.
The densities and the laws of three Gamma distributions are plotted in
Figure 3.16.
3.3.5
Failure rate
Assume the random variable X follows an absolutely continuous distribution
whose density is f (t) = F ′
X(t). Deﬁne
w(τ; t) := f (t + τ)
1 −FX(t).
Then Vitali theorem yields
P

X ≤t + s
 X ≥t

= FX(t + s) −FX(t)
1 −FX(t)
=
* s
0
w(τ; t) dτ.
The function w(0; t),
w(0; t) =
f (t)
1 −FX(t) =
F ′
X(t)
1 −FX(t) = −d
dt log(1 −FX(t)),
is called the hazard function or failure rate of the distribution of X. In fact,
assuming that X represents a failure time, then
W(A) := P

X ∈A
 X ≥t

=
1
1 −FX(t)
*
A∩[t,∞[
f (s) ds

RANDOM VARIABLES
111
is the distribution probability of the failure time, assuming that no failure has
occurred before time t. Notice that W is supported on [t, +∞]. In particular, for
the probability of a failure during a small time interval [t, t + δt], we have
P(X ≤t + δt | X ≥t) =
1
1 −FX(t)
* t+δt
t
f (s) ds ≃w(0; t) δt
if f is continuous at t. In this sense w(0; t) models the urgency of completing
a task at time t before the equipment devoted to it breaks down.
It can be easily checked that the failure rate function of a random variable
that follows the exponential distribution exp(λ) is constant and equal to λ.
3.3.6
Exercises
Exercise 3.52 The temperature T (measured in degrees Celsius) of a certain
volume of gas follows the Gaussian distribution of mean μ = 40 and variance
σ 2 = 100. Compute:
• P(T ≤50).
• P(|T −μ| ≤5).
• P(|T −μ| ≤10|T ≥35).
0
(1), (0.5), (1)+(0.5)−1
(0.5)
1
Exercise 3.53 Assume X is a Gaussian random variable with mean and variance
equal to 1 and 4, respectively. Compute P(|X −1| ≤2) in terms of the law  of
the standard Gaussian distribution.
2

1
2

−
−3
2

= 
1
2

+ 
3
2

−1
3
Exercise 3.54 Assume X is a random variable with absolutely continuous distri-
bution of density
f (x) =

|1 −x|
if 0 ≤x ≤2,
0
otherwise.
Compute Var [ X ].
21
2
3
Exercise 3.55 Let
f (x) =
⎧
⎪⎨
⎪⎩
c
if −2 ≤x ≤−1,
2c
if 1 ≤x ≤2,
0
otherwise,
be the density of an absolutely continuously distributed random variable X. Com-
pute the density of Y := X2.

112
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 3.56 Let θ > 0 and assume that
f (x, θ) =

θx−θ−1
if x > 1,
0
otherwise,
is the density of an absolutely continuously distributed random variable X. Com-
pute E [ X ].
Exercise 3.57 John thinks that the life (measured in kilometers) of a car is an
exponentially distributed random variable with mean μ = 300 000. He buys a
used car that has already done 100 000 km. What is the probability that the car
does another 25 000 km before it breaks down?
[≃0.435]
Exercise 3.58 The daily production (measured in kilograms) of cheese from a
certain dairy farm is a Gaussian random variable of mean μ = 1000 and standard
deviation σ = 50. Compute the probability that the daily production is at least
975 kg.
[(0.50) ≃0.69]
Exercise 3.59 The fuel consumption (measured in litres per/100 km) of an AAA
car is a Gaussian random variable with mean μ1 = 6.5 and standard deviation
σ1 = 0.8. The fuel consumption of a BBB car is another Gaussian random variable
with mean μ2 = 7 and standard deviation σ2 = 0.5. Compute the probability that
the AAA car consumes more fuel than the BBB car.
[≃1 −(0.53) ≃0.30]
Exercise 3.60 The weight of a box of spaghetti (measured in grams) is a Gaussian
random variable whose expected value and standard deviation are 510 and 12,
respectively. Compute the probability that a box weighs more than 500 g.
[≃0.80]
Exercise 3.61 Assume X is an exponentially distributed random variable whose
expected value is 1/λ. Compute the laws of the random variables eX and
min {X, 3}.
Exercise 3.62 Let X be a normally distributed random variable, PX = N(10, 9).
Compute for every t > 0 P(X ≥t) and P(X2 > t) in terms of the density of the
normalized Gaussian distribution N(0, 1).

4
Vector valued random
variables
4.1
Joint distribution
Let X and Y be two random variables. The distribution vector (PX, PY) does
not bear enough information about the relations occurring between X and Y,
for instance to compute P(A ∩B) where A and B are generated by X and Y,
respectively; one needs to introduce a new object called the joint (probability)
distribution of X and Y.
Example 4.1 (Discrete random variables) Let X and Y be two discrete random
variables on the same probability space (, E, P). Let X() =

x1, . . . , xn

,
Y() =

y1, . . . , ym

and let (p1, . . . , pn) and (q1, . . . , qm) be the mass density
vectors of PX and PY , respectively.
The image of the map (X, Y) :  →R2 is a subset of {(xi, yj), i = 1, . . . , n,
j = 1, . . . , m} (whose cardinality is nm) and its distribution is concentrated at
this set. Let

pij

be the nm-tuple deﬁned by
pij := P(X = xi, Y = yj).
Clearly, the mass densities (pi) and (qj) can be computed in terms of the pij’s:
in fact,
pi = P(X = xi) =
m

j=1
P(X = xi, Y = yj) =

j
pij
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

114
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
qj = P(Y = yj) =
n

i=1
P(X = xi, Y = yj) =

i
pij.
But, since (pij) has nm components while the components of (pi) and (qj) are
only n + m, one cannot expect in general to recover (pij) from (pi) and (qj):
if n ∧m > 2, the vector (pij) is likely to bear more information than the (pi)’s
and the (qj)’s. Example 4.5 shows that, in general, even in the simplest case
n = m = 2, the knowledge of (p1, p2) and (q1, q2) does not yield the possibility
to compute (pij).
4.1.1
Joint and marginal distributions
4.1.1.1
Vector valued random variables
Deﬁnition 4.2 Let X1, . . . , XN :  →R be real-valued maps deﬁned on a prob-
ability space (, E, P) and let X :  →RN, X = (X1, . . . , XN). If

x ∈
 X(x) ∈A

∈E
for any Borel set A ⊂RN, then X is said to be an E-measurable map, or a
RN-valued random variable or a multivariate random variable.
The following proposition can be easily proved.
Proposition 4.3 X = (X1, . . . , XN) is a RN-valued random variable if and only
if all the Xi’s are real-valued random variables.
4.1.1.2
Joint distribution
Let X = (X1, . . . , XN) :  →RN be a random variable on the probability space
(, E, P). We can deﬁne a new function on the Borel sets of RN, PX : B(RN) →
[0, 1], as
PX(A) := P(X ∈A) = P(X−1(A))
∀A ⊂B(RN).
Following the same line as in the scalar case, see Section 3.1.1, one can use De
Morgan formulas to prove that (B(RN), PX) is a probability measure on RN.
(B(RN), PX) is called the joint distribution of X = (X1, . . . , XN).
Also,
we
deﬁne
the
joint distribution law
FX : RN →R
of
X =
(X1, . . . , XN) as
FX(t1, . . . , tN) = P(X1 ≤t1, X2 ≤t2, . . . , XN ≤tN).
Namely, if for each t = (t1, . . . , tN) ∈RN, we introduce the set
Et :=] −∞, t1]×] −∞, t2] × · · · ×] −∞, tN] ⊂RN;
(4.1)

VECTOR VALUED RANDOM VARIABLES
115
then
FX(t) := P(X1 ≤t1, X2 ≤t2, . . . , XN ≤tN) = PX(Et).
Thus, knowing PX one can compute FX and, as in the scalar case, PX can be
recovered from FX, see Section 3.1.1 and Appendix B.
4.1.1.3
Marginal random variables and marginal distributions
Let X = (X1, X2) :  →R2 be a random variable on a probability space
(, E, P), Both X1 and X2 are real valued random variables and their
distributions PX1 and PX2 can be computed from the joint distribution PX of
X1 and X2. In fact, for any Borel set A ⊂B(R), the sets A × R and R × A are
Borel sets in R2,
PX1(A) = P(X1 ∈A) = P(X1 ∈A, X2 ∈R)
= P((X1, X2) ∈A × R) = P(X1,X2)(A × R)
and, analogously,
PX2(A) = P(X1,X2)(R × A).
When the stress is on the distribution PX of X := (X1, X2), PX1 and PX2 are
called the marginal distributions of PX.
Clearly, the above can be extended to random variables of arbitrary dimen-
sion. Assume, for example, that X :  →Rh, Y :  →Rk and Z := (X, Y) :
 →Rh+k. Then X and Y are random variables if and only if Z is a random
variable. Moreover, for any A ∈B(Rh) and any B ∈B(Rk) we have
PX(A) = PZ(A × Rk)
and
PY(B) = PZ(Rh × B).
Remark 4.4 Random variables are deﬁned only by means of the set theoretic
notion of preimage. This fact suggests a possible generalization: let (, E, P) and
(, F, Q) be two probability spaces. A map X :  → is said to be a random
variable between such spaces if X−1(F) ∈E for every F ∈F. If X is a random
variable (according to this deﬁnition), then PX(F) := P(X−1(F)), F ∈F, is a
set function deﬁned on F and the couple (F, PX) is a probability measure on
, called the distribution of X. We are not going to treat this point any further.
Example 4.1 suggests that the joint distribution conveys more information
than the marginal ones. Here we provide one explicit example.
Example 4.5 Let E, F ⊂[0, 1] be two intervals. Let X = 1E, Y = 1F and let P
be the uniform distribution on [0, 1]. Thus
P(X = 0) = P(Ec),
P(X = 1) = P(E),
P(Y = 0) = P(F c),
P(Y = 1) = P(F),

116
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
and the matrix of joint densities is
P1E,1F ({i, j}) =

P(Ec ∩F c)
P(E ∩F c)
P(Ec ∩F)
P(E ∩F)

This clearly shows that moving E with respect to F changes PX,Y but does not
change PX and PY .
4.1.1.4
Composition
Let X :  →RN be a random variable on the probability space (, E, P). The
deﬁnition of joint distribution in terms of the probability P,
PX(A) := P(X ∈A)
∀A ⊂RN
can be conveniently written by means of integrals on  and RN.
Theorem 4.6 Let X1, . . . , XN :  →R be random variables on (, E, P). Then,
for any B(RN)-measurable non-negative function ψ : RN →R, we have
*

ψ(X1(x), X2(x), . . . , XN(x)) P(dx)
=
*
RN ψ(t1, . . . , tN) P(X1,...,XN )(dt1dt2 · · · dtN).
(4.2)
Proof. The proof follows the same line of the proof of Theorem 3.9.
The composition formula easily follows, see Corollary 3.11.
Corollary 4.7 Let X :  →RN be a random variable on (, E, P) and let ϕ :
RN →Rk be a B(RN)-measurable function. Then ϕ ◦X :  →Rk is a random
variable on (, E, P) and for every non-negative B(Rk)-measurable function ψ :
Rk →R we have
*
Rk ψ(s1, . . . , sk) Pϕ◦X(ds1 · · · dsk)
=
*
RN ψ(ϕ(t1, . . . , tN)) PX(dt1 · · · dtN).
(4.3)
4.1.1.5
Sum of random variables
Formula (4.3) allows to compute the distribution of the sum of two random
variables in terms of their joint distribution.
Proposition 4.8 Let X and Y :  →RN be two random variables on (, E, P).
Then
*
RN ψ(s) PX+Y (ds) =
**
R2N ψ(t + s) P(X,Y)(dtds)
(4.4)

VECTOR VALUED RANDOM VARIABLES
117
for any non-negative B(RN)-measurable function ϕ : RN →R. In particular,
PX+Y (A) = PX,Y

(x, y) ∈R2N  x + y ∈A

∀A ∈B(R)
and
FX+Y(t) = PX,Y

(x, y) ∈R2N  x1 + y1 ≤t1, . . . , xN + yN ≤tN

for every t = (t1, . . . , tN) ∈RN.
Proof. Equality (4.4) is the claim of Corollary 4.7 for ϕ : RN × RN →RN,
ϕ(x, y) := x + y. The other claims follow choosing ψ = 1A in (4.4) and then
choosing A = -N
i=1] −∞, ti[.
4.1.2
Exercises
Exercise 4.9 Let X, Y be two random variables on (, E, P). Prove the following
identities:
E [ X ] =
**
R2 x PX,Y (dxdy),
E [ Y ] =
**
R2 y PX,Y (dxdy),
E [ X + Y ] =
**
R2(x + y) PX,Y (dxdy),
E [ XY ] =
**
R2 xy PX,Y (dxdy),
E [ ϕ ◦X ] =
**
R2 ϕ(x) PX,Y (dxdy),
E [ ϕ ◦Y ] =
**
R2 ϕ(y) PX,Y (dxdy),
E [ max(X, Y) ] =
**
R2 max(x, y) PX,Y (dxdy),
E [ min(X, Y) ] =
**
R2 min(x, y) PX,Y (dxdy).
(4.5)
Exercise 4.10 (Hypergeometric distribution) Assume we are given an urn
containing w white balls and r red balls. We draw n balls. Let X be the random
variable that counts the number of drawn white balls. Compute E [ X ] and
Var [ X ].
Solution. X follows the hypergeometric distribution. Let Xj, j = 1, . . . , n, be
the random variables such that Xj = 1 if the jth drawn ball is white and Xj = 0

118
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
otherwise. Then X = n
j=1 Xj and each random variable Xj is a Bernoulli
trial with distribution B(1, p) where p = w/(w + r) (see Section 3.2.3). Thus
E
%
Xj
&
= w/(w + r) for every j = 1, . . . , n so that
E [ X ] =
n

j=1
E
%
Xj
&
=
wr
w + r .
Iterating formula (4.15) one gets
Var [ X ] =
n

j=1
Var [Xj] + 2
n

j=2
j−1

i=1
Cov(Xi, Xj).
Since Cov(Xi, Xj) = E
%
XiXj
&
−E
%
Xi
&
E
%
Xj
&
and
E
%
XiXj
&
= P(Xi = 1, Xj = 1) = P(Xj = 1|Xi = 1)P(Xi = 1)
=
w −1
w + r −1
w
w + r ,
we get
Cov(Xi, Xj) =
(w −1)w
(w + r −1)(w + r) −
w2
(w + r)2
=
−wr
(w + r)2(w + r −1).
Finally,
Var [ X ] =
nwr
(w + r)2 −n(n −1)
2
wr
(w + r)2(w + r −1) =
wr
(w + r)2
w + r −n
w + r −1.
Exercise 4.11 Assume the joint distribution PX,Y of X and Y is uniformly
distributed on the square [0, 1]2. Compute the distribution of the random
variable XY.
Exercise 4.12 Assume the joint distribution PX,Y of X and Y is uniformly dis-
tributed on the parallelogram of vertices (0, 0), (1, 0), (2, 1) and (3, 1). Compute:
(i) The distributions PX and PY.
(ii) Mean and variance of X and Y.
Exercise 4.13 Assume that the joint distribution PX,Y of two random variables
X and Y is absolutely continuous with respect to L2 with density
f (x, y) =

cx2y2
if 0 ≤x ≤1 and 0 ≤y ≤1,
0
otherwise.
Find c and compute the joint distribution of 3X and XY.

VECTOR VALUED RANDOM VARIABLES
119
Exercise 4.14 Assume that the joint distribution PX,Y of two random variables
X and Y is absolutely continuous with respect to L2 with density
f (x, y) =
c(x −y)2
if 0 ≤x, y ≤1,
0
otherwise.
Find c and compute the joint distribution of X2 and X2Y 2.
Exercise 4.15 Assume that the joint distribution PX,Y of two random variables
X and Y is absolutely continuous with respect to L2 with density
f (x, y) =
⎧
⎪⎨
⎪⎩
3

x2 + y2
14 π
if 1 ≤x2 + y2 ≤4,
0
otherwise.
Find r > 0 such that P(X2 + Y 2 ≥r2) = 1
2.
4
3√
36
2
5
Exercise 4.16 Assume that the joint distribution PX,Y of two random variables
X and Y is absolutely continuous with respect to L2 with density
f (x, y) =
⎧
⎨
⎩
√
2(x + y)
36
if x2 + y2 ≤9, x + y > 0,
0
otherwise.
Compute P(XY ≥0).
4√
2
2
5
Exercise 4.17 Let a and b > 0 and let X and Y be two random variables with an
absolutely continuous joint distribution PX,Y of density
f (x, y) =
Ce−axe−by
if x > 0, y > 0,
0
otherwise.
Compute:
(i) The constant C.
(ii) The probability P(Y ≤X).
(iii) The joint law of X and Y.2
(i) ab, (ii)
a
a + b, (iii) (1 −e−as)(1 −e−bt)
3

120
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
4.2
Covariance
4.2.1
Random variables with ﬁnite expected
value and variance
Proposition 4.18 Let X, Y :  →R be two random variables on the probability
space (, E, P). Then
E [ |XY| ] ≤E
%
X2 &1/2E
%
Y 2 &1/2.
(4.6)
Proof. If either E
%
X2 &
= +∞or E
%
Y 2 &
= +∞, the claim is trivial. If
E
%
X2 &
= 0, then X = 0 P-a.e., so also E [ |XY| ] = 0, hence (4.6) holds true.
The same is true if E
%
Y 2 &
= 0.
Let us assume that E
%
X2 &
and E
%
Y 2 &
are both ﬁnite and positive. Consider
the random variables
a(x) =
1
E
%
X2 &1/2 |X(x)|,
b(x) =
1
(E
%
Y 2 &
)1/2 |Y(x)|.
Since 2a(x)b(x) ≤a2(x) + b2(x) ∀x ∈, integrating with respect to P we get
2 E [ ab ] = 2
*

a(x)b(x) P(dx)
≤E
%
a2 &
+ E
%
b2 &
=
1
E
%
X2 &E
%
X2 &
+
1
E
%
Y 2 &E
%
Y 2 &
= 2,
i.e. E [ ab ] ≤1, which proves (4.6).
Let
L2(; P) :=

X :  →R is a random variable
 E
%
X2 &
< ∞

.
(4.7)
When the context is clear, we shall write L2() or L2 in place of L2(; P). If
X ∈L2 we say that X is a square integrable random variable.
Proposition 4.19 If X, Y ∈L2, then E [ XY ] is ﬁnite. Moreover, L2 is a linear
space, the map (X, Y) →E [ XY ] is a positive semideﬁnite symmetric bilinear
form on L2, and X →E
%
X2 &1/2 is a seminorm on L2.
Proof. By inequality (4.6)
E [ XY ]
 is ﬁnite whenever X and Y ∈L2. Hence,
also E [ XY ] ∈R.
Let X, Y ∈L2. Since E [ XY ] is bilinear and again by (4.6) we get
E
%
(X + Y)2 &
= E
%
X2 &
+ 2E [ XY ] + E
%
Y 2 &
≤

E
%
X2 &1/2 + E
%
Y 2 &1/22
< ∞;

VECTOR VALUED RANDOM VARIABLES
121
thus X + Y ∈L2. Moreover, for any α ∈R and any X ∈L2, trivially
E
%
(αX)2 &
= α2E
%
X2 &
< ∞, i.e. αX ∈L2. This proves that L2 is a linear
space. It is now obvious that (X, Y) →E [ XY ] is real valued, and therefore a
positive semideﬁnite symmetric bilinear form.
Remark 4.20 Notice the following:
• In general, the form (X|Y) := E [ XY ] is not positive deﬁnite and, con-
sequently, X →E
%
X2 &1/2 is not a norm. In fact, E
%
X2 &
= 0 does not
imply that X(x) = 0 ∀x but only that X = 0 P-a.e.
• Inequality (4.6) is the Cauchy–Schwarz inequality for the positive semidef-
inite symmetric bilinear form (X, Y) →E [ XY ] on L2. In particular, for
X, Y ∈L2 we have
E [ XY ] ≤E [ |XY| ] ≤E
%
X2 &1/2 E
%
Y 2 &1/2
(4.8)
and the equality holds if and only if either Y = 0 P-a.e. or X(x) = λY(x)
P-a.e. for some λ ∈R, λ ≥0.
Applying inequality (4.6) with Y = 1 we get the classical inequality between
mean and mean square of X
E [ |X| ] ≤
6
E
%
X2 &
.
(4.9)
Finally, (4.8) and (4.9) yield the following.
Proposition 4.21 X is square integrable, i.e. E
%
X2 &
< ∞if and only if both
E [ X ] and Var [ X ] are ﬁnite. In particular
L2 ⊂L1.
(4.10)
Example 4.22 In general, L2 and L1 do not coincide. For example, let (B(R), P)
the Lebesgue measure on the interval [0, 1] and let X(t) = 1/√t, 0 < t < 1.
Then
E [ X ] =
* 1
0
1
√t dt = 2
while
E
%
X2 &
=
* 1
0
1
t dt = +∞.
Deﬁnition 4.23 Let X, Y be two random variables in L2, equivalently two random
variables whose expected value and variance are both ﬁnite. The number
Cov(X, Y) := E
0 
X −E[X]

Y −E[Y]
 1
(4.11)
is called the covariance of X and Y.
Trivially:
• (X, Y) →Cov(X, Y) is a positive semideﬁnite symmetric bilinear form on
the linear space L2.

122
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
• Cov(X, X) = 0 if and only if X is P-a.e. constant. In this case X(x) =
E [ X ] for P-a.e. x.
• The following formulas hold:
Cov(X, Y) = E [ XY ] −E [ X ]E [ Y ],
Cov(αX + β, γ Y + δ) = αγ Cov(X, Y),
Cov(X, X) = Var [ X ] .
(4.12)
• Theorem 4.6 yields
Cov(X, Y) =
**
R2 xy PX,Y (dxdy) −E [ X ]E [ Y ]
=
**
R2(x −E [ X ]) (y −E [ Y ]) PX,Y (dxdy).
(4.13)
• The Cauchy–Schwarz inequality for the form (X, Y) →Cov(X, Y) on L2
reads as follows:
Cov(X, Y) ≤

Var [ X ]

Var [Y],
(4.14)
where the equality holds if and only if either Y is constant P-a.e. or there
exist α ≥0 and β ∈R such that X(x) = α Y(x) + β P-a.e.
• The Carnot theorem reads
Var [X + Y] = Var [ X ] + Var [Y] + 2 Cov(X, Y)
(4.15)
from which the Pythagoras formula follows:
Var [X + Y] = Var [ X ] + Var [Y]
(4.16)
if and only if Cov(X, Y) = 0.
Deﬁnition 4.24 If X, Y ∈L2 and Cov(X, Y) = 0, i.e.
E [ XY ] = E [ X ]E [ Y ] ,
we say that X and Y are uncorrelated.
Finally, iterating (4.16) we get that
Var [X1 + · · · + Xn] = Var [X1] + · · · + Var [Xn]
(4.17)
if and only if the random variables X1, . . . , Xn are pairwise uncorrelated.

VECTOR VALUED RANDOM VARIABLES
123
4.2.2
Correlation coefﬁcient
For X, Y ∈L2, let σ(X) := Var [ X ]1/2 and σ(Y) := Var [Y]1/2. If X and Y are
not constant P-a.e., one deﬁnes the correlation coefﬁcient of X and Y as
Corr(X, Y) := Cov(X, Y)
σ(X)σ(Y).
(4.18)
Corr(X, Y) gives some measure of the ‘similarities between the shapes’ of the
distributions of X and Y, regardless of their expected value and variance. In fact,
Corr(X, Y) is invariant by change of scale either in X or in Y. Moreover, the
Cauchy–Schwarz inequality reads
−1 ≤Corr(X, Y) ≤1,
(4.19)
and Corr(X, Y) = 1 (= −1) if and only if there exists α > 0 (α < 0, respectively)
and β ∈R such that X = αY + β P-a.e.
4.2.3
Exercises
Exercise 4.25 Let X be a random variable with ﬁnite expected value and variance.
Let Y = aX + b, a ̸= 0, b ∈R. Show that Corr(X, Y) = sgn(a).
Exercise 4.26 Let X be a random variable uniformly distributed in [0, b]. Com-
pute Cov(X, X2) and the correlation coefﬁcient Corr(X, X2).
Solution. We have
E [ X ] =
* b
0
x 1
b dx = b
2,
E
%
X2 &
= 1
b
* b
0
x2 dx = b2
3 ,
E
%
X3 &
= 1
b
* b
0
x3 dx = b3
4 ,
E
%
X4 &
= 1
b
* b
0
x4 dx = b4
4 ,
Var [ X ] = E
%
X2 &
−E [ X ]2 = 1
12b2,
Var [X2] = E
%
X4 &
−E
%
X2 &2 = 4
45b4.
Hence
Cov(X, X2) = E
%
X3 &
−E [ X ]E
%
X2 &
=
1
4 −1
2
1
3

b3 = b3
12.
and
Corr(X, X2) =
Cov(X, X2)
√Var [ X ]

Var [X2]
=
1
12
1
√
12
2
√
45
=
√
15
4 .

124
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 4.27 Throw a fair dice. If the result is less than or equal to 3, ﬂip a fair
coin. Otherwise ﬂip two fair coins. Let Y be the result of the dice and let X be
the number of heads obtained in the ﬂipping of the coins. Compute:
1. The distribution of X.
2. The expected value and the variance of X.
3. The correlation coefﬁcient Corr(X, Y).
Exercise 4.28 An experiment has three possible results, 1, 2 and 3, whose proba-
bilities are p1, p2 and p3, respectively. Repeat the experiment n times and assume
that the result of each trial is independent of the other trials.
Let N1 and N2 be the number of times that you obtain the results 1 and 2,
respectively. Compute Corr(N1, N2).
Taking
into
account
the
constraints
p1 ≥0,
p2 ≥0,
p3 ≥0
and
p1 + p2 + p3 = 1 compute the maximum and the minimum possible values for ρ.
Can Corr(N1, N2) be zero?
4.3
Independent random variables
4.3.1
Independent events
Deﬁnition 4.29 Let (E, P) be a probability space and let A, B ∈E. If
P(A ∩B) = P(A)P(B), then we say that A and B are independent events.
It can easily checked that, if P(B) > 0, then A and B are independent if and
only if P(A) = P(A | B): no knowledge on A is gained from the knowledge of B.
Example 4.30 Throw two dice. Here  = {1, 2, . . . , 6}2 and, if the dice are
fair, then each pair (i, j) in  has the same probability, i.e. P({(i, j)}) = 1
36
∀(i, j) ∈. Let A be the event ‘The number on the top face of the ﬁrst dice
is 6’, i.e.
A =

(6, j)
 j = 1, . . . , 6

and let B be the event ‘The number on the second dice is 4’, i.e.
B =

(j, 4)
 j = 1, . . . , 6

.
Then
P(A) = P(B) = 1
6.
Since
A ∩B = {(6, 4)},
then
P(A ∩B) = 1
36 =
P(A)P(B), thus A and B are independent events.
Deﬁnition 4.31 Let (, E, P) be a probability space and let A1, . . . , An ∈E.
A1, . . . , An are said to be independent if for any k = 2, . . . , n and for any choice
of k events Aj1, . . . , Ajk among them, then
P(Aj1 ∩Aj2 ∩· · · ∩Ajk) = P(Aj1)P(Aj2) · · · P(Ajk).
(4.20)

VECTOR VALUED RANDOM VARIABLES
125
We say that a denumerable family of events

An

is independent if for every
n the events A1, . . . , An are independent.
Notice that the pairwise independence of the events A1, . . . , An, n ≥3, does
not guarantee the independence of A1, . . . , An. Here we provide an example.
Example 4.32 Consider the uniform probability on  = {0, 1}2. Each single-
ton has probability 1/4. Let A1 = {(1, 0), (1, 1)}, A2 = {(0, 1), (1, 1)} and A3 =
{(i, j) | i + j = 1} = {(1, 0), (0, 1)}. Then
P(A1) = 1/2,
P(A2) = 1/2,
P(A3) = 1/2,
P(A1 ∩A2) = P({1, 1}) = 1/4,
P(A2 ∩A3) = P({0, 1}) = 1/4,
P(A1 ∩A3) = P({(1, 0)}) = 1/4.
Hence A1, A2 and A3 are pairwise independent but they are not independent
since
P(A1 ∩A2 ∩A3) = P(∅) = 0 ̸= 1
8.
If A and B are independent events, then also Ac and B are independent.
In fact,
P(Ac ∩B) = P(B) −P(A ∩B) = P(B) −P(A)P(B) = P(Ac)P(B).
Similarly, one proves the independence of the couples (A, Bc) and (Ac, Bc). In
general, if A1, . . . , An are independent, then all the n-tuples of events whose ﬁrst
component is either A1 or Ac
1, the second component is either A2 or Ac
2 and so
on, are independent.
The above shows that the notion of independence is actually the independence
of the corresponding generated σ-algebras. This suggests the following extension:
let (, E, P) be a probability space. Two σ-algebras A, B ⊂E are independent
if P(A ∩B) = P(A)P(B) for every A ∈A and B ∈B.
Example 4.33 (Reliability: series and parallel systems) Assume you are given
a system made of n subsystems working in parallel, see Figure 4.1. For each
i = 1, . . . , n let Ei be the event ‘The ith subsystem is not working’ and let E
be the event ‘The whole system does not work’. Then E = E1 ∩E2 ∩· · · ∩En,
hence
P(E) ≤min(P(E1), P(E2), . . . , P(En)).
The probability of a breakdown of the system is less than or equal to the
probability of a breakdown of the strongest component. If we assume that the
failures of the subsystems are independent, then we actually have
P(E) = P(E1)P(E2) · · · P(En).
For instance, if the probability that each subsystem fails is 0.5, then the
probability that the whole system is not working is (0.5)n, a huge increase in

126
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Figure 4.1
A parallel system.
Figure 4.2
A series system.
the reliability of the whole system. However, it is worth recalling that this gain
in the reliability of the system relies on the independence of the failures of the
subsystems, which is not so easily granted!
Assume now that the subsystems are working in series, see Figure 4.2. The
event E can now be written as:
E = E1 ∪E2 ∪· · · ∪En;
therefore,
P(E) ≥max(P(E1), . . . , P(En)),
that is, the occurrence of the breakdown of the whole system is at least as probable
as the occurrence of a breakdown of the weakest component of the system. If
we assume that the failures of the subsystems

Ei

are independent, since
Ec = Ec
1 ∩Ec
2 ∩· · · ∩Ec
n.
then
P(Ec) = P(Ec
1)P(Ec
2) . . . P(Ec
n)
or, equivalently,
P(E) = 1 −(1 −P(E1))(1 −P(E2)) . . . (1 −P(En)).
For instance, if P(Ei) = 0.5 then P(E) = 1 −1/2n: the reliability is much worse
in a series system if the failures on the subsystems are independent.

VECTOR VALUED RANDOM VARIABLES
127
4.3.2
Independent random variables
Deﬁnition 4.34 Let X and Y be two random variables on the probability space
(, E, P). Assume X and Y take values on Rn and Rm, respectively. X and Y are
said to be independent random variables if
P(X ∈A, Y ∈B) = P(X ∈A) P(Y ∈B),
(4.21)
for any A ∈B(Rn) and B ∈B(Rm).
The independence of two random variables X and Y can be restated in several
equivalent ways:
• The σ-algebras EX and EY of the events detected by X and Y are indepen-
dent.
• The distribution of X does not provide any piece of information on the
distribution of Y and conversely: if A, B are such that P(X ∈A)P(Y ∈
B) > 0, then
P

X ∈A
 Y ∈B

= P(X ∈A),
P

Y ∈B
 X ∈A

= P(Y ∈B).
• Equality (4.21) can be rewritten as
PX,Y (A × B) = PX(A)PY (B)
∀A ∈B(Rn), B ∈B(Rm).
(4.22)
Since the value of the joint distribution on ‘rectangles’. A × B determines
the joint distribution on all of B(Rn+m), we conclude that the joint distri-
bution of X and Y, hence the probabilistic interaction between X and Y,
is completely described by PX and PY .
• Since a law completely determines a measure, we also have that
X and Y
are independent if and only if F(X,Y)(t, s) = FX(t)FY (s)
∀(t, s) ∈Rn × Rm.
In the jargon of measure theory, (4.22) says that PX,Y is the product measure
of PX and PY, PX,Y = PX × PY. From Fubini theorem, see Appendix B, we
get the following.
Proposition 4.35 Let X and Y be two random variables on the probability space
(, E, P) with values in Rn and Rm, respectively. Then X and Y are independent
if and only if
**
Rn+m ϕ(x, y) PX,Y (dxdy) =
*
Rm
 *
Rn ϕ(x, y) PX(dx)

PY(dy)
=
*
Rn
 *
Rm ϕ(x, y) PY (dy)

PX(dx)
(4.23)
for any B(Rn+m)-measurable and non-negative function ϕ : Rn+m →R.

128
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Finally, from Deﬁnition 4.34 we get the following.
Proposition 4.36 Let X and Y be two independent variables on (, E, P) with
values in Rn and Rm, respectively, and let α : Rn →Rh and β : Rm →Rk be
Borel-measurable. Then the random variables α ◦X and β ◦Y are also indepen-
dent. In particular, equality (4.23) holds true also for the random variables α ◦X
and β ◦Y (replacing n and m with h and k, of course).
Theorem 4.37 (Expected value of a product) Let X and Y be two independent
random variables with ﬁnite expected value. Then X and Y are uncorrelated, i.e.
E [ XY ] = E [ X ] E [ Y ]
equivalently,
Cov(X, Y) = 0.
(4.24)
Proof. Applying (4.23)
E [ |XY| ] =
**
R2 |xy| PX,Y (dxdy) =
*
R
 *
R
|xy| PY(dy)

PX(dx)
=
 *
R
|x| PX(dx)
 *
R
|y| PY (dy)

= E [ |X| ] E [ |Y| ] < +∞.
Thus the function ϕ : (x, y) →xy is summable with respect to PX,Y and (4.23)
can be applied to ϕ to get
E [ XY ] =
**
R2 xy PX,Y (dxdy) =
*
R
 *
R
xy PY(dy)

PX(dx)
=
 *
R
x PX(dx)
 *
R
y PY (dy)

= E [ X ]E [ Y ].
Remark 4.38 Notice that proving that two variables are uncorrelated is a
much easier task than proving that they are independent. In fact, X and Y are
uncorrelated if and only if E [ XY ] = E [ X ]E [ Y ], which can be checked with
only one equality. In contrast, X and Y are independent means that (4.22) holds
for every couple of events A and B, which amounts to a huge, possibly inﬁnite,
set of equalities.
4.3.3
Independence of many random variables
The notion of independence extends to many variables, even sequences of random
variables.
Deﬁnition 4.39 Let Xn, n ∈N, be random variables on (, E, P).
(i) We say that the random variables Xn are pairwise independent if for every
i, j ∈N, i ̸= j, the random variables Xi and Xj are independent, see Def-
inition 4.34.

VECTOR VALUED RANDOM VARIABLES
129
(ii) n random variables X1, . . . , Xn are said to be independent if for
any k ≥2 and for any choice Xr1, . . . , Xrk of k of them and for any
A1, . . . , Ak ∈B(R)
P(Xr1 ∈A1, Xr2 ∈A2, . . . , Xrk ∈Ak)
= P(Xr1 ∈A1)P(Xr2 ∈A2) · · · P(Xrk ∈Ak).
(4.25)
(iii) The random variables of the sequence

Xn

are said to be independent if
for every k ≥2 the variables X1, . . . , Xk are independent.
Equation (4.25) says that the joint distribution of Xr1, . . . , Xrk in B(Rk) is
the product measure of the distributions of Xr1, . . . , Xrk, i.e.
PXr1,...,Xrk = PXr1 × · · · × PXrk on B(Rk).
Thus independence of X1, . . . , Xn is equivalent to the following: choose k
random variables between X1, . . . , Xn and group them to form two vector
valued random variables say Y := (Xr1, . . . , Xrh) and Z = (Xrh+1, . . . , Xrk),
then X1, . . . , Xn are independent if and only if Y and Z are independent,
PY,Z = PY × PZ
on B(Rk)
whatever the variables used to build Y and Z are.
Although three or more independent variables are trivially pairwise indepen-
dent, the converse is false as the following example shows.
Example 4.40 Let A1, A2, A3 be the events in Example 4.32. Then the corre-
sponding random variables Xi = 1Ai i = 1, 2, 3 are pairwise independent but not
independent as a whole.
Remark 4.41 Beware that the notion of independence is somewhat delicate.
For instance, suppose that X, Y, Z, T are independent random variables. Then
(X, Y) and (Z, T ) are independent by deﬁnition and Proposition 4.36 yields that
X + Y and Z + T are independent variables, too. However, in general, X + Y
and Z + Y are not independent, as shown by Example 4.42.
Example 4.42 Throw three dice and let X, Y, Z be the outcome of each dice.
Then X + Y and Z + Y are not independent. In fact, let A := {X + Y = 6} and
B = {Y + Z = 6}. Then
P(X + Y = 6) =
5

j=1
P(X = j, Y = 6 −j)
=
5

j=1
P(X = j) P(Y = 6 −j) = 5
62

130
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
and, similarly, P(Z + Y = 6) = 5
62 . But
P(X + Y = 6, Z + Y = 6) =
5

j=1
P(Y = j, X = 6 −j, Z = 6 −j)
=
5

j=1
P(Y = j) P(X = 6 −j) P(Z = 6 −j) = 5
63 ̸= 25
64 .
4.3.4
Sum of independent random variables
Theorem 4.43 Let X and Y be two independent random variables with values
in Z. Let

pk

and

qk

be the mass densities of PX and PY , respectively. Then
X + Y is a random variable with values in Z and the mass density

rk

of PX+Y
is given by
rk =
+∞

j=−∞
pjqk−j
∀k ∈Z.
Proof. For any k ∈Z one has
PX+Y({k}) = P(X + Y = k) = P
 (
j

x ∈
 X = j, Y = k −j

=
+∞

j=−∞
P (X = j, Y = k −j) =
+∞

j=−∞
P(X = j)P(Y = k −j)
=
+∞

j=−∞
pjqk−j.
A similar result holds for the sum of two independent random variables
following an absolutely continuous distribution. We have the following.
Theorem 4.44 Let X and Y be two independent random variables with PX(dt) =
f (t) dt and PY(dt) = g(t) dt, where f and g are two summable functions. Then
X + Y follows the absolutely continuous distribution, PX+Y(dt) = τ(t) dt with
τ(t) =
* +∞
−∞
f (s)g(t −s) ds.
Proof. Let ϕ : R →R be a B(R)-measurable non-negative function. Then
*
R
ϕ(t) PX+Y(dt) =
**
R2 ϕ(x + y) PX,Y (dxdy)
(i)

VECTOR VALUED RANDOM VARIABLES
131
=
*
R
 *
R
ϕ(x + y) PY (dy)

PX(dx)
(ii)
=
* +∞
−∞
f (x)
 * +∞
−∞
ϕ(x + y)g(y) dy

dx
(iii)
=
* +∞
−∞
f (x)
 * +∞
−∞
ϕ(t)g(t −x) dt

dx
(iv)
=
* +∞
−∞
ϕ(t)
 * +∞
−∞
f (x)g(t −x) dx

dt
(v)
=
* +∞
−∞
ϕ(t)τ(t) dt.
Here we have taken advantage of (4.2) in (i), of the independence of X and Y
in (ii), and of the Fubini theorem in (iii). Moreover, in (iv) we have changed
variable y →t := x + y and in (v) we have used the Fubini theorem in order to
change the order of integration.
4.3.5
Exercises
Exercise 4.45 (Sum of two uniformly distributed random variables) Let X
and Y be two random variables, uniformly distributed on the same interval [a, b].
Compute the distribution of X + Y, and compare with the distribution of 2X.
Solution. With the notation of Theorem 4.44 we have
f (t) = g(t) =
⎧
⎨
⎩
1
b −a
if t ∈(a, b),
0
otherwise.
The product f (s)g(t −s) is not zero (and equal to
1
(b−a)2 ) if and only if s and
t −s ∈(a, b), i.e. if and only if
s ∈(a, t −a), t ∈(2a, a + b], s ∈(t −b, b), t ∈(a + b, 2b).
Thus
τ(t) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
t −2a
(b −a)2
if t ∈(2a, a + b],
2b −t
(b −a)2
if t ∈(a + b, 2b),
0
otherwise.
The density distribution τ(t) is called the triangular distribution on the interval
(2a, 2b): the graph of τ is shown in Figure 4.3. The distribution of 2X is uniform
on the interval (2a, 2b) with density
1
2(b−a).

132
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
2a
2b
t
t(t)
a + b
1
b − a
Figure 4.3
The sum of two independent random variables uniformly distributed
on (a, b) follows the triangular distribution on the interval (2a, 2b).
Exercise 4.46 (Sum of independent Poisson random variables) Assume X and
Y are independent random variables with PX = P(λ) and PY = P(μ). Prove the
following:
(i) PX+Y = P(λ + μ).
(ii) For any k, n ∈N
P

X = k
 X + Y = n

= B(n, p)({k})
where p :=
λ
λ+μ.
Solution. (i) Let k ∈Z; then
PX+Y ({k}) =
+∞

j=−∞
P(λ)({j})P(μ)({k −j}) =
k

j=0
P(λ)({j})P(μ)({k −j})
=
k

j=0
λj
j! e−λ
μk−j
(k −j)!e−μ = e−(λ+μ) 1
k!
k

j=0
k
j

λjμk−j
= e−(λ+μ) 1
k!(λ + μ)k = P(λ + μ)({k}).
(ii) For any n ∈N and k ∈{0, . . . , n} one computes
P

X = k
 X + Y = n

= P(X=k, Y=n −k)
P(X + Y = n)
= P(X = k)P(Y=n −k)
P(X + Y=n)
= e−λλk
k!
e−μμn−k
(n −k)!
n!
e−λ−μ(λ + μ)n
=
n
k
 
λ
λ + μ
k 
μ
λ + μ
n−k
=
n
k

pk (1 −p)n−k = B(n, p)({k}).

VECTOR VALUED RANDOM VARIABLES
133
Exercise 4.47 (Sum of independent Gaussian random variables) Assume that
X and Y are two independent random variables following two Gaussian distribu-
tions, PX = N(ℓ, σ) and PY = N(m, δ). Show that X + Y is a Gaussian random
variable of parameters ℓ+ m and
√
σ 2 + δ2, PX+Y = N(ℓ+ m,
√
σ 2 + δ2).
Solution. By considering X −ℓand Y −m instead of X and Y, it sufﬁces to
prove the claim when ℓ= m = 0. By Theorem 4.44, PX+Y (dt) = τ(t) dt where
τ(t) =
1
2πσδ
* +∞
−∞
e−s2
2σ2 e−(t−s)2
2δ2 ds.
One computes
s2
σ 2 + (t −s)2
δ2
= α2s2 −2αβst + β2t2 + t2
δ2 −β2t2 = (αs −βt)2 + γ 2t2
where
α :=
/
1
σ 2 + 1
δ2 ,
β :=
1
δ2α ,
γ 2 :=
1
σ 2 + δ2 .
Thus
τ(t) =
1
2πσδ e−γ 2t2
2
* +∞
−∞
e−(αs−βt)2
2
ds =
1
2πσδ e−γ 2t2
2
√
2π
α
=
1
√
2π
√
σ 2 + δ2 e
−
t2
2(σ2+δ2) .
Exercise 4.48 (Minimum of exponential random variables) Assume that X and
Y are two independent random variables with PX = exp(λ) and PY = exp(μ).
Show that the random variable Z := min(X, Y) follows the exponential distribu-
tion of parameter λ + μ, PZ = exp(λ + μ).
Solution. Let ϕ : R →R be a B(R)-measurable non-negative function. Then
*
R
ϕ(t) Pmin(X,Y)(dt) =
**
R2 ϕ(min(x, y)) PX,Y (dxdy)
=
*
R
 *
R
ϕ(min(x, y)) PX(dx)

PY (dy)
=
* +∞
0
λe−λt
 * +∞
0
ϕ(min(t, s))μe−μs ds

dt
= λμ
* +∞
0
e−λtϕ(t)
 * +∞
t
e−μsds

dt
+ λμ
* +∞
0
e−μsϕ(s)
 * +∞
s
e−λtdt

ds

134
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
= λ
* +∞
0
ϕ(t)e−(λ+μ)t dt + μ
* +∞
0
ϕ(s)e−(λ+μ)s ds
=
* +∞
0
ϕ(t)(λ + μ)e−(λ+μ)t dt
=
*
R
ϕ(t) exp(λ + μ)(dt).
Exercise 4.49 (Sum of Gamma distributed random variables) Let X and Y be
two independent random variables with PX = (α, λ) and PY = (β, λ), where
α, β and λ are positive constants. Prove that PX+Y = (α + β, λ).
Solution. We recall that the Euler Beta function
B(p, q) :=
* ∞
0
xp−1(1 −x)q−1 dx,
p, q > 0
can be computed in terms of the Euler Gamma function , in fact,
B(p, q) = (p) (q)
(p + q) .
Let fα,λ(t) and fβ,λ(t) be the densities of PX and PY , respectively, see (3.48).
Since X and Y are independent, Theorem 4.44 yields PX+Y (dt) = r(t) dt where
r(t) is given by
r(t) =
*
R
fα,λ(x)fβ,λ(t −x) dx.
Since fα,λ(s) and fβ,λ(s) are zero if s ≤0, we infer that r(t) = 0 = fα+β,λ(t)
for t ≤0. For t > 0 we have
r(t) =
* ∞
0
λαxα−1e−λx
(α)
λβ(t −x)β−1e−λ(t−x)
(β)
dx
= λα+βe−λt
(α) (β)
* ∞
0
xα−1(t −x)β−1 dx.
After substituting y := x/t, the last integral is equal to
* ∞
0
tα−1yα−1tβ−1(1 −y)β−1t dy = tα+β−1B(α, β) = tα+β−1 (α) (β)
(a + β) ,
so that
r(t) = λα+βtα+β−1e−λt
(α + β)
= fα+β,λ(t).
Exercise 4.50 Consider the communication network in Figure 4.4. The capaci-
ties of the links, expressed in Mb/s, are C1,2 = C1,3 = 5, C2,4 = C3,4 = 10 and

VECTOR VALUED RANDOM VARIABLES
135
2
1
4
3
Figure 4.4
The network involved in Exercise 4.50.
C2,3 = 8, all working in the same direction. Let Fij be the event ‘The link Cij is
broken’. Assume that the events Fij are independent and that P(Fij) = 0.2 for
each link. For each couple of connected nodes (i, j) compute the capacity of the
network in Mb/s when the link Cij is broken.
Exercise 4.51 Flip two fair coins. Let A =‘head on the ﬁrst coin’, B =‘the same
outcome on both coins’ and C =‘head on the second coin’. Show that:
• A, B and C are pairwise independent, but are not independent.
• C and A ∩B are not independent.
Exercise 4.52 Let X and Y be two independent random variables, and assume that
PX = B(2, 1/2) and PY = B(2, 1/3), respectively. Compute P(XY = 2).
21
6
3
Exercise 4.53 Let X and Y be two independent random variables and assume
that X is uniformly distributed on the interval [0, 1] while PY = exp(λ);
(i) Compute E
%
Y 2/(1 + X)
&
.
(ii) Compute P(X ≤Y).
(iii) Compute the joint distribution of X2 and XY.
Exercise 4.54 Assume that X and Y are two independent random variables
on (, E, P). Assume X() = Y() = {−1, 1}, and that P(X = 1) = 1
2 and
P(Y = 1) = 1
3. Compute P(XY = 1).
21
2
3
Exercise 4.55 Assume X and Y are independent random variables. Suppose
that X and Y follow the geometric distribution of parameters p = 1/2 and
q = 1/4, respectively. Compute P(X = Y).
21
5
3

136
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 4.56 The joint density of two random variables X and Y is
f (x, y) =

C
if −1 < x < y < 1,
0
otherwise.
(i) Compute C.
21
2
3
(ii) Compute the densities of the marginal distributions PX and PY.
(iii) Are X and Y independent?
[No]
(iv) For each a ∈R compute P(X + Y > a).
⎡
⎢⎢⎢⎢⎢⎢⎣
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
0
if a ≥1,
(2 −a)2
8
if a ∈[0, 1[,
1 −(a + 2)2
8
if a ∈] −2, 0[,
1
if a ≤−2.
⎤
⎥⎥⎥⎥⎥⎥⎦
Answer the same questions for the joint density
f (x, y) =

C
if x2 + y2 ≤R2,
0
otherwise,
where R > 0 is given.
Exercise 4.57 Let X and Y be two independent random variables with absolutely
continuous distributions, PX(dt) = fX(t) dt, PY (dt) = fY (t) dt where
fX(x) =

6x(1 −x)
x ∈[0, 1],
0
otherwise,
and
fY (y) =

2y
y ∈[0, 1],
0
otherwise,
respectively.
(i) Check that
.
R fX(s) ds =
.
R fY (s) ds = 1.
(ii) Compute the density of the random variable Z = X2Y.
%
x2 −8x3/2 + 6x if x ∈[0, 1], 0 otherwise
&
.
Exercise 4.58 Let X and Y be independent random variables following the stan-
dard Gaussian distribution N(0, 1);
(i) Compute Cov(3X + 2Y, X + 5Y + 10).
(ii) Compute P(X + 4Y ≥2).
(iii) Compute P((X −Y)2 > 9).

VECTOR VALUED RANDOM VARIABLES
137
Exercise 4.59 Assume X and Y are independent random variables following two
exponential distributions. Compute the distribution of X/Y.
Exercise 4.60 Assume X and Y are independent random variables following the
exponential distribution exp(λ). Compute the distribution of |X −Y|.
Exercise 4.61 Assume that both the independent random variables X and Y follow
the distribution (α, λ). Compute the joint law of U := X + Y and V :=
X
X + Y .
Solution. Let ϕ be a non-negative B(R2) function and let fα,λ and fβ,λ be
the densities of PX and PY, respectively. We have
**
R2 ϕ(u, v) PU,V (dudv) =
**
R2 ϕ(x + y,
x
x + y ) PX,Y (dxdy)
=
**
R2 ϕ(x + y,
x
x + y )fα,λ(x)fβ,λ, (y) dxdy.
Consider the map g(x, y) := (x + y,
x
x+y ). g is a one-to-one map between
R2 \ {x + y = 0}
onto

(r, s) ∈R2 | r = 0

with
inverse
h(r, s) :=
(rx, r(1 −s)). By changing variable the last integral amounts to
=
**
R2 ϕ(r, s)fα,λ(rs)fβ,λ(r(1 −s)) |J(Dh(r, s))| drds
so that
P(U ≤u, V ≤v) =
**
r≤u, s≤v
fα,λ(rs)fβ,λ(r(1 −s)) |J(Dh(r, s))| drds.
Computing the previous double integral yields
P(U ≤u, V ≤v)
=
* u
0
dr
* min(1,v)
0
r λα(rs)α−1e−λrs
(α)
λβ(r(1 −s))β−1e−λr(1−s)
(β)
ds =
= (α + β)
(α)(β)
* u
0
fα+β,λ(r)dr
* min(1,v)
0
sα−1(1 −s)β−1 ds.
Thus, the random variable U follows a  distribution of parameters α + β and
λ (a fact which was already proved in Exercise 4.49) and the distribution of V
is supported in the interval [0, 1], with density (α + β)
(α)(β)vα−1(1 −v)β−1.
Exercise 4.62 Assume X1, . . . , Xn are independent random variables following
the standard Gaussian distribution N(0, 1), and let Y := n
k=1 X2
k. Show that
PY = χ2(n). We recall that χ2(n) = ( n
2, 1
2).

138
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 4.63 Mark and John have decided to meet at the Pub ‘Born to be
Stochastic’ between noon and 1pm without waiting for more than 15min for each
other. Assume Mark and John’s arrival times at the Pub are independent random
variables that are uniformly distributed in the time interval from noon to 1pm.
Compute the probability of Mark and John meeting at the Pub.
Solution. Denote with M and J the random variables describing the arrival
time for Mark and John, respectively. Using the minute as the unit of measure-
ment, both M and J are uniformly distributed in the interval (0, 60). Mark and
John meet if and only if |G −M| ≤15. Since J and M are independent, the joint
distribution of (J, M) is uniformly distributed on the square [0, 60]2, that is, it
is absolutely continuous with density
1
36001[0,60]2(x, y). Denoting by R the set
R :=

(s, t) ∈[0, 60]2  |s −t| ≤15

,
we conclude that
P(|G −M| ≤15) =
**
R
1
3600 dsdt = 7
16.
Exercise 4.64 Flip a fair coin and throw a fair dice together many times. Compute
the probability of the event ‘We obtain a head on the coin before obtaining a six
on the dice’. Compute the probability of the event ‘The success of the ﬁrst process
occurs before the ﬁrst success of the second process’.
Solution. We are considering two independent Bernoulli processes whose
probability of success is p = 1/2 and q = 1/6, respectively. Let H be the random
variable ‘Flipping of the coin at which the ﬁrst head occurs’ and let S be the
random variable ‘Throw of the dice at which the ﬁrst six occurs’. Thus, H and S
are geometric random variables of parameter p = 1/2 and q = 1/6, respectively:
P(H = k) = p(1 −p)k−1,
P(S = k) = q(1 −q)k−1,
k = 1, 2, . . . .
We want to compute P(H < S). Since
{H < S} =
∞
(
k=2
{S = k, H < k}
and H and S are independent random variables, we get
P(H < S) =
∞

k=2
P(S = k, H < k) =
+∞

k=2
P(S = k)P(H < k).
On the other hand,
P(H < k) =
k−1

j=1
P(H = j) =
k−1

j=1
p(1 −p)j−1

VECTOR VALUED RANDOM VARIABLES
139
=
k−2

i=0
p(1 −p)i = 1 −(1 −p)k−1,
hence we infer
P(H < S) =
+∞

k=2
q(1 −q)k−1 
1 −(1 −p)k−1
=
p(1 −q)
p + q −pq .
In particular, in the case of a fair coin and a fair dice we have P(H < S) = 5/7.
Exercise 4.65
Let W1, . . . , Wn be identically distributed independent random
variables following the exponential distribution exp(λ). Compute the law of Sn :=
n
k=0 Wk.
Solution. By assumption, for each k = 1, . . . , n we have PWk = ρ(s) ds with
ρ(s) =
0
if s < 0,
λe−λs
if s ≥0.
By Theorem 4.44, PW1+W2(ds) = ρ2(s) ds where
ρ2(s) =
0
if s < 0,
. s
0 λ2e−λte−λ(s−t) dt = λe−λs(λs)
if s ≥0.
By induction, one proves that PSn(ds) = ρn(s) ds where
ρn(s) =
⎧
⎪⎨
⎪⎩
0
if s < 0,
λe−λs (λs)n−1
(n −1)! = λnsn−1e−λs
(n)
if s ≥0.
i.e. Sn follows the Gamma distribution (n, λ), also known as the Erlang distri-
bution of parameters n and λ.
Clearly, P(Sn ≤t) = 0 if t ≤0. Assume now t > 0. Deﬁne I0 = 0 and, for
each k ≥1 let Ik := P(Sk > t). Integrating by parts, we get the recursion formula
Ik+1 =
* ∞
t
ρk+1(s) ds =
* +∞
t
λe−λs (λs)k
k!
ds
=
* +∞
λt
e−τ τ k
k! dτ = e−λt (λt)k
k!
+ Ik
for every k ≥1. This yields In = n−1
k=0 e−λt (λt)n
n!
and therefore
P(W1 + W2 + · · · + Wn ≤t) = 1 −In = 1 −
n−1

k=0
e−λt (λt)k
k! .

140
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
4.4
Sequences of independent random variables
Let

Xn

be a sequence of random variables deﬁned on the same probability
space (, E, P) and with the same ﬁnite expected value: E
%
Xn
&
= E ∈R ∀n.
We are concerned with the convergence
X1 + X2 + · · · + Xn
n
→E
(4.26)
under appropriate independence relations of the sequence

Xn

.
Consider, for instance, a sequence

Xn

of independent Bernoulli trials of
parameter 1/2, so that E
%
Xn
&
= 1/2 and Var [Xn] = 1/4. Thus Sn := X1 +
X2 + · · · + Xn yields the number of successes in n trials and Sn/n is the rate
of success in n trials. The limit as n →∞of Sn(x)/n may even not exist and,
in general, it should depend on x, that is, on the particular sequence of trials: if
success always occurs, then the rate of success is constantly 1, while, if failure
always occurs, then such a rate is zero for every n. Nevertheless, experience
suggests that almost certainly, that is for almost all sequences of trials, such a
rate approaches 1/2 as n →∞. The mathematical formulation of this intuition
is the so-called strong law of large numbers.
4.4.1
Weak law of large numbers
Before dealing with the law of large numbers, we start with an estimate of the
average of a ﬁxed number of random variables.
Theorem 4.66 (L2 and weak estimates) Let X1, . . . , Xn be real-valued random
variables on a probability space (, E, P), and let Sn(x) := n
k=1 Xk(x). If the

Xn

are pairwise uncorrelated, with the same expected value E := E
%
Xk
&
∀k
and with ﬁnite variance, then
*

Sn(x)
n
−E

2
P(dx) ≤C2
n
(4.27)
where C2 := maxk Var
%
Xk
&
; in particular,
P

|Sn
n −E| ≥δ

≤C2
δ2 n
∀δ > 0.
(4.28)
Proof. Observe that
E
%
Sn
&
=
n

k=1
E
%
Xk
&
= nE,
hence, by (4.17),
*

Sn(x)
n
−E

2
P(dx) = Var
2 Sn
n
3
= 1
n2
n

k=1
Var
%
Xk
&
≤C2
n .

VECTOR VALUED RANDOM VARIABLES
141
Therefore, the Chebyshev inequality yields
P

|Sn
n −E| ≥δ

≤1
δ2
*

|Sn(x)
n
−E|2 P(dx) ≤C2
δ2 n.
Inequality (4.28) says that the event ‘The distance of Sn(x)/n from its mean
E is greater that δ’ has probability smaller than C2/(δ2n). If P is the uniform
probability on , then (4.28) reads as ‘The distance between Sn(x)/n and its
mean E is larger than δ in no-more than 100 C2/(δ2n) per cent of cases’. Notice
that we have obtained an estimate on the percentage of cases, a fact which agrees
with having only a probabilistic knowledge of the phenomenon. In particular,
increasing n yields a better relative estimate, but, likely, the absolute number of
cases may increase.
Clearly, (4.28) is not interesting if C2/(δ2n) is larger than or equal to 1.
Theorem 4.66 is not the only result of this kind; experience teaches than
if n is large enough, then the left-hand term in (4.28) is much smaller than the
right-hand one. The following theorem, also known as the law of large deviations
provides a hint.
Theorem 4.67 (Cernoff) Let X1, . . . , Xn be equidistributed independent random
variables such that E
%
eθ0|X1| &
< +∞for some θ0 > 0. Let E := E
%
X1
&
and
Sn := n
k=1 Xk. Then for any δ > 0 there exists a positive constant C = C(δ) > 0
such that
P

|Sn
n −E| > δ

≤2 e−C(δ)n.
C(δ) can be explicitly computed:
C(δ) :=
sup
0<θ<θ0
(θδ −log M(θ)),
where
M(θ) := E
%
eθ|X1| &
.
Theorem 4.66 yields immediately a version of the weak law of large numbers,
or WLN, for short.
Theorem 4.68 (Weak law of large numbers) Let

Xn

be a sequence of random
variables on a probability space (, E, P). Assume that the Xn’s are pairwise
uncorrelated with the same expected value E
%
Xn
&
= E and with equibounded
variances, Var
%
Xn
&
≤C2 ∀n. For every n = 1, 2, . . . , set Sn := n
k=1 Xk. Then
*

Sn(x)
n
−E

2
P(dx) →0
as n →∞
and
P

|Sn
n −E| > δ

→0
as n →∞.
(4.29)
Convergence in probability (4.29) also holds in other situations. Since the
almost sure convergence discussed below implies convergence in probability, the

142
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Etemadi theorem, Theorem 4.81, improves a classical result by Khinchin,
as follows.
Theorem 4.69 Let

Xn

be a sequence of summable, pairwise independent and
equidistributed random variables on a probability space (, E, P). Set Sn :=
n
k=1 Xk. Then for every δ > 0
P

|Sn
n −E| > δ

→0
as n →∞.
4.4.2
Borel–Cantelli lemma
We are now going to discuss an interesting property of the intersection of
inﬁnitely many events.
Assume (, E, P) is a probability space and let

An

⊂E be a sequence of
events. Deﬁne
lim sup
n
An :=
∞
'
m=1
∞
(
n=m
An
i.e. the set of those x ∈ belonging to inﬁnitely many An’s.
Proposition 4.70 (Borel–Cantelli lemma) Let

An

be a sequence of events on
a probability space (, E, P). If ∞
n=1 P(An) < +∞, then
P(lim sup
n
An) = 0.
Proof. Since P is denumerably subadditive and since the series ∞
n=1 P(An)
converges, we infer
P(lim sup
n
An) ≤P
 ∞
(
n=m
An

≤
∞

n=m
P(An) →0
as m →∞.
If the events

An

are independent, then the statement of the Borel–Cantelli
lemma is indeed an equivalence. In fact, the following holds.
Proposition 4.71 Assume that

An

is a sequence of independent events
on a probability space (, E, P). If the series ∞
n=1 P(An) diverges, then
P(lim supn An) = 1.
Proof. Let m, q ∈N, m ≤q. By the independence assumption, using also the
inequality 1 −x ≤e−x ∀x ∈[0, 1], we get
P

q'
n=m
Ac
n

=
q=
n=m
P(Ac
n) =
q=
n=m
(1 −P(An))
≤
q=
n=m
e−P(An) = exp

−
q

n=m
P(An)

.

VECTOR VALUED RANDOM VARIABLES
143
As q goes to inﬁnity we get
P
 ∞
'
n=m
Ac
n

→0
for every m,
thus,
P

(lim sup
n
An)c
= lim
m→∞P
 ∞
'
n=m
Ac
n

= 0.
Combining Propositions 4.70 and 4.71 we get the following.
Corollary 4.72 (Kolmogorov 0-1 law) Let

An

be a sequence of independent
events in a probability space (, E, P). Then
P(lim sup
n
An) = 0
if and only if
∞

n=1
P(An) < ∞
and
P(lim sup
n
An) = 1
if and only if
∞

n=1
P(An) = +∞.
Actually, the claims in Proposition 4.71 and Corollary 4.72 still hold for
pairwise independent events

An

.
Theorem 4.73 Let

An

be a sequence of pairwise independent events in a prob-
ability space (, E, P). If ∞
n=1 P(An) = +∞, then
lim
n→∞
n
k=1 1Ak(x)
n
k=1 P(Ak) = 1
P-a.e.;
in particular, for P-almost every x ∈ the series ∞
k=1 1Ak(x) diverges, i.e. x
belongs to inﬁnitely many An’s for P-a.e. x, that is, P(lim supn An) = 1.
We omit the proof of Theorem 4.73, which can be found e.g. in [4].
Remark 4.74 Notice that the assumption of independence cannot be dropped. As
an example, throw a fair dice and let A be the event ‘6 on the top’. Deﬁne An :=
A for every n. Thus ∞
n=1 P(An) = 1/6 + 1/6 + · · · = +∞while lim supn An =
A so that P(A) = 1/6.
4.4.3
Convergences of random variables
Three different kinds of convergence of random variables appear in discussing
the law of large numbers.
Deﬁnition 4.75 Let X and Xn, n ∈N, be random variables in a probability space
(, E, P).

144
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
(i) The sequence

Xn

is said to converge in L2 to X if
*

|Xn −X|2 P(dx) →0
as n →∞.
(ii) The sequence

Xn

is said to converge in probability to X if for every δ > 0
P

|Xn −X| > δ

→0
as n →∞.
(iii) The sequence

Xn

is said to converge P-almost surely or P-almost
everywhere if the probability of the event
E : =

x ∈
 Xn(x) →±∞or Xn(x) ̸→X(x)

=

x ∈
 lim sup
n→∞
|Xn(x) −X(x)| > 0
$
is null, P(E) = 0. With a shortened notation we write Xn →X P-a.s., or
Xn →X P-a.e., or
P(Xn →X) = 1
when

Xn

converges to X P-almost surely.
Of course, the above three types of limit are unique P-a.e., provided they
exist. The three types of convergence are related in several ways although they
are not equivalent. In particular, it can be shown, see Appendix B, that
(i) If Xn →X in L2, then Xn →X in probability.
(ii) If Xn →X P-a.e., then Xn →X in probability.
The following example shows that convergence in probability and conver-
gence P-a.e. are not equivalent.
Example 4.76 Divide [0, 1] in one part, two parts, three parts, etc. Enu-
merate sequentially the intervals thus obtained, i.e. I0 = [0, 1], I1 = [0, 1/2],
I2 = [1/2, 1], I3 = [0, 1/3]. . . .. We therefore obtain a sequence

In

of
intervals. Consider the uniform probability on [0, 1]. It is then easy to check
that the sequence

1In(x)

converges in probability to the null random variable
but does not converge pointwisely at any x, in particular,

1In(x)

does not
converge almost surely.
The following proposition clariﬁes the meaning of the almost sure conver-
gence.
Proposition 4.77 Let X, Xn, n ∈N be random variables on the probability space
(, E, P). The following are equivalent:

VECTOR VALUED RANDOM VARIABLES
145
(i) Xn →X P-almost surely, P(Xn →X) = 1.
(ii) For every δ > 0 then with zero probability |Xn(x) −X(x)| > δ for inﬁnitely
many indexes n.
(iii) For every δ > 0 then P-almost surely |Xn(x) −X| ≤δ for n large enough
(depending on x).
More formally, if for δ > 0 and n ∈N,
An,δ :=

x ∈
 |Xn −X| > δ

and
Eδ := lim inf
n
An,δ,
then (ii) means that P(Eδ) = 0 for every δ > 0.
Proof of Proposition 4.77.
(i) ⇔(ii). Let
E =

x ∈
 lim sup
n→∞
|Xn(x) −X(x)| > 0
$
and let

δk

be a sequence monotonically decreasing to 0. Trivially,

Eδk

is an
increasing sequence of events and E = ∪kEδk. This shows that E is an event
and that P(Eδ) ≤P(E) ∀δ > 0. If (i) holds, then P(Eδ) ≤P(E) = 1 −P(Xn →
X) = 0, i.e. (ii).
Conversely, if (ii) holds, i.e. P(Eδ) = 0 for every δ > 0, from the continuity
property of the measure, we get
P(E) = lim
k P(Eδk) = 0,
hence P(Xn →X) = 1 −P(E) = 1.
(ii) ⇔(iii). In fact, (ii) is equivalent to P(Ec
δ) = 1 for every δ > 0, hence (ii) is
equivalent to (iii) since
Ec
δ =

x ∈
 |Xn(x) −X(x)| ≤δ deﬁnitely

.
Let X, Xn, n ≥1, be random variables. For every δ > 0 and n ∈N set
An,δ =

x ∈
 |Xn(x) −X(x)| > δ

and Eδ := lim supn→∞An,δ. If ∞
n=1 P(An,δ) < +∞, then by the Borel–Cantelli
lemma, P(Eδ) = 0. Taking into account Proposition 4.77, we then state the fol-
lowing.
Proposition 4.78 If for every δ > 0 the series ∞
n=1 P(An,δ) converges, then
Xn →X P-almost surely.

146
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
4.4.4
Strong law of large numbers
Let

Xn

be a sequence of random variables and let Sn := n
j=1 Xj. We now
discuss P-almost sure convergence of the averages Sn/n of the Xn’s.
Lemma 4.79 Let

Xn

be a sequence of random variables in the probability space
(, E, P) such that E
%
Xn
&
= E ∀n. If ∞
n=1 Var
%
Xn
&
< +∞, then Xn →E
P-almost surely.
Proof. Since, by assumption, the series ∞
n=1
.
 |Xn(x) −E|2 P(dx) con-
verges, Lebesgue dominated convergence theorem for series yields that the series
∞
k=1 |Xk(x) −E| converges in L2 and P-almost surely. In particular, Xn →E
P-almost surely.
Another proof of Lemma 4.79. For any positive δ and any n ∈N, let An,δ =

x ∈ | |Xn(x) −E| > δ

. By the Chebyshev inequality
P(An,δ) ≤1
δ2
*

|Xn(x) −E|2 P(dx).
Thus, from the assumptions, the series ∞
n=1 P(An,δ) converges. The claim then
follows from Proposition 4.78 since δ is an arbitrary positive number.
We now state a version of the strong law of large numbers.
Theorem 4.80 (Rajchman) Let

Xn

be a sequence of pairwise uncorrelated
random variables on the probability space (, E, P). Assume E
%
Xn
&
= E and
Var
%
Xn
&
≤C2 ∀n, and for n = 1, 2, . . . , let Sn := n
k=1 Xk. Then Sn/n →E
in L2, in probability and P-almost surely.
Proof. The L2 convergence and the convergence in probability are already
proved in Theorem 4.68. It remains to prove P-almost sure convergence.
Replacing Xn by Xn −E, we may assume E
%
Xn
&
= 0. Set, for convenience,
S0 = 0. Since the random variables Xn are pairwise uncorrelated, for any i, j,
0 ≤i < j, we have
Var [Sj −Si] = Var
4
j

k=i+1
Xk
5
=
j

k=i+1
Var [Xk] ≤C2(j −i).
(4.30)
Applying (4.30) with j = n2 and i = 0 we get
Var [Sn2] ≤C2 n2,
hence
∞

n=1
Var
2 Sn2
n2
3
≤C2
∞

n=1
1
n2 < +∞,

VECTOR VALUED RANDOM VARIABLES
147
and consequently, since E
%
Sn2
&
= 0, we get
Sn2
n2 →0
P-almost surely
by Lemma 4.79.
For any p ∈N, let n = n(p) be such that n2 ≤p < (n + 1)2. Clearly,
Sn2
p = Sn2
n2
n2
p →0
P-almost surely.
(4.31)
On the other hand, since (n + 1)2 −n2 = 2n + 1 ≤2√p + 1, by (4.30) we get
∞

p=1
Var
2Sp
p −Sn2
p
3
≤C2
∞

p=1
p −n2
p2
≤C2
∞

p=1
2√p + 1
p2
< +∞,
and, since E
0 Sp
p −
Sn2
p
1
= 0, Lemma 4.79 implies that
Sp
p −Sn2
p →0
P-almost surely.
(4.32)
Since
Sp
p =
Sp
p −Sn2
p

+ Sn2
p ,
from (4.31) and (4.32) we ﬁnally conclude that Sp/p converges to zero P-almost
surely.
Theorem 4.80 is one of the possible results known as the strong law of large
numbers. It can be generalized in various directions, weakening certain assump-
tions and strengthening others. We refer the reader to the specialized literature,
see e.g. [4–6]. Here we quote the following theorem, where no assumption on
the variances is required.
Theorem 4.81 (Etemadi) Let

Xn

be a sequence of pairwise independent,
equally distributed and summable random variables on the probability space
(, E, P). Let E = E
%
X1
&
∈R and for n ≥1 let Sn = n
k=1 Xk. Then
Sn(x)
n
→E
P-almost surely.
We remark that the summability assumption can be weakened: integrability
sufﬁces, allowing E = +∞or E = −∞, see Exercise 4.11.
Let us ﬁrst state a few lemmas.
Lemma 4.82

n > x
1
n2 ≤2
x for any x > 0.

148
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Proof. In fact, let m = ⌊x⌋+ 1. We then have
∞

n=m
1
n2 ≤1
m2 +
* ∞
m
1
t2 dt = 1
m2 + 1
m ≤2
x .
Lemma 4.83 Let α > 1 and kn := ⌊αn⌋. Then

n,kn≥j
1
k2n
≤
4α2
α2 −1
1
j2 .
Proof. We ﬁrst show that kn ≥αn/2. In fact, if αn ≤2, then kn ≥1 ≥αn/2;
if αn > 2 then kn ≥αn −1 > αn/2.
Let n0 :=
 
log j
log α
!
. If kn ≥j, then αn ≥j, so that n ≥log j
log α ≥n0. Thus

n, kn≥j
1
k2n
≤4
∞

n=n0
1
α2n =
4
α2n0
1
1 −α−2 ≤
4α2
α2 −1
1
j 2 .
Lemma 4.84 Let

Xn

be a sequence of identically distributed real valued random
variables. For any n ∈N, set
Yn(x) := Xn(x)1{|Xn|<n}(x) =

Xn(x)
if |Xn(x)| ≤n,
0
otherwise
and let Sn := n
j=1 Xj and Tn := n
j=1 Yj. If T n(x)/n →μ P-almost surely,
then Sn(x)/n →μ almost surely.
Proof. Let An :=

x ∈ | Xn(x) ̸= Yn(x)

. Applying the Cavalieri formula
(3.14) we get
∞

n=1
P(An) =
∞

n=1
P(|Xn| > n) =
∞

n=1
P(|X1| > n) = E
%
|X1|
&
< +∞.
Thus, we infer from the Borel–Cantelli lemma that for almost every x ∈ there
exists n = n(x) such that Xn(x) = Yn(x) for every n ≥n, hence
Sn(x) −Tn(x)
n
=
Sn0(x) −Tn0(x)
n
→0
as n →∞
P-almost surely.
Since by assumption Tn(x)/n →μ almost surely, also Sn(x)/n →μ P-almost
surely.
Lemma 4.85 Let

Xn

be a sequence of summable, identically distributed random
variables. For any n ≥1, let Yn := Xn1{|Xn|≤n}. Then
∞

j=1
E
0
Y 2
j
1
j2
≤4E
%
|X1|
&
.

VECTOR VALUED RANDOM VARIABLES
149
Proof. The Cavalieri formula yields
E
%
Y 2
j
&
=
* ∞
0
P(Y 2
j > t) dt =
* ∞
0
2 s P(|Yj| > s) ds =
* j
0
2 s P(|Yj| > s) ds
≤
* j
0
2 s P(|Xj| > s) ds =
* j
0
2 s P(|X1| > s) ds
=
* ∞
0
2 s 1[0,j](s)P(|X1| > s) ds.
Then, using the Beppo Levi theorem, Lemma 4.82 and Cavalieri formula, we
conclude
∞

j=1
E
0
Y 2
j
1
j2
=
* ∞
0
2 s
 ∞

j=1
1
j2
1[0,j](s)

P(|X1| > s) ds
=
* ∞
0
2 s

j > s
1
j 2 P(|X1| > s) ds
≤4
* ∞
0
P(|X1| > s) ds = 4E
%
|X1|
&
.
Proof of the Etemadi theorem. Both

X+
n

and

X−
n

are sequences of
summable, identically distributed and pairwise independent random variables,
thus we may restrict ourselves to the case Xn ≥0 ∀n. The random variables
Yn(x) = Xn(x)1{|Xn|≤n},
x ∈,
are
bounded,
non-negative
and
pairwise
independent. Let Tk(x) := k
j=1 Yj(x). Thanks to Lemma 4.84 it is enough to
show that for P-almost all x ∈
Tk(x)
k
→E
%
X1
&
as k →∞.
Let α > 1 and kn := ⌊αn⌋. Since the random variables

Yn

are pairwise
independent, they are also uncorrelated, thus, using Chebyshev inequality we get
P

|Tkn −E
%
Tkn
&
| > knδ

≤Var [Tkn]
δ2k2n
=
1
δ2k2n
kn

j=1
Var [Yj]
∀δ > 0.
Thus, by Lemmas 4.83 and 4.84 we infer
∞

n=1
P

|Tkn −E
%
Tkn
&
| > knδ

≤1
δ2
∞

n=1
1
k2n
kn

j=1
Var [Yj] = 1
δ2
∞

j=1
Var [Yj]
 
n, kn≥j
1
k2n


150
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
≤1
δ2
4α2
α2 −1
∞

j=1
Var [Yj]
j2
≤16α2
a2 −1
1
δ2 E
%
|X1|
&
< +∞
for every δ > 0. Therefore, see Proposition 4.78,
Tkn(x) −E
%
Tkn
&
kn
→0
P-almost surely.
(4.33)
Since the random variables are non-negative, Beppo Levi’s theorem yields
E
%
Yn
&
= E
%
Xn1{Xn≤n}
&
= E
%
X11{X1≤n}
&
→E
%
X1
&
.
Hence by the Cesaro theorem,
E
%
Tkn
&
kn
→E
%
X1
&
.
(4.34)
Thus, (4.33) and (4.34) yield
Tkn(x)
kn
→E
%
X1
&
P-almost surely.
(4.35)
For any k ≥1, let n = n(k) such that kn < k ≤kn+1. Clearly n goes to inﬁnity
as k goes to inﬁnity. Since Yk ≥0 for any k, we have
kn
n
Tkn(x)
kn
≤Tk(x)
k
≤
Tkn+1(x)
kn+1
kn+1
n .
From kn ≤αn ≤kn + 1 ≤k ≤kn+1 ≤αn+1, we have
αn −1
αn+1
≤kn
n ,
kn+1
n
≤αn+1
αn
= α,
hence the inequalities
αn −1
αn
Tkn(x)
kn
≤Tk(x)
k
≤
Tkn+1(x)
kn+1
α.
(4.36)
Letting k →∞in (4.36), we then get from (4.35)
1
α E
%
X1
&
≤lim inf
k→∞
Tk(x)
k
≤lim sup
k→∞
Tk(x)
k
≤α E
%
X1
&
(4.37)
P-almost surely. Since α > 1 is arbitrary, we conclude that
lim
k→∞
Tk(x)
x
= E
%
X1
&
P-almost surely,
see Exercise 4.110. The claim is proven.

VECTOR VALUED RANDOM VARIABLES
151
Finally, we quote a further estimate on the averages of a sequence of random
variables, known as the law of the iterated logarithm which yields a more precise
asymptotic behaviour as n →∞.
Theorem 4.86 (Hartman–Wintner) Let

Xn

be a sequence of independent
identically distributed random variables with zero expected value and variance,
E = E
%
Xn
&
= 0 and Var
%
Xn
&
= C2 (thus Sn/n →0 P-almost surely), and, for
n ≥1, let Sn = n
k=1 Xk. Then
lim sup
n→∞
Sn(x)
C

2 n log log n
= 1
P-almost surely.
In particular, for any δ > 0 P-almost surely we have
|Sn(x)| ≤C
√
2(1 + δ)

n log log n
deﬁnitely.
4.4.4.1
Existence of a sequence of independent
random variables
A problem one should consider is the one of the existence of a sequence of inde-
pendent identically distributed random variables on the same probability space
(, E, P). Let us construct a sequence of this kind going back to the Bernoulli
inﬁnite process: ﬁx a probability space (, E, P). Denote by ∞the set of all
sequences taking values in . As for the inﬁnite Bernoulli scheme, one proves,
using the Method I of construction of measures, see Section 2.2.5 and Appendix
B, the existence of a probability measure (E∞, P∞) on ∞such that for any
n ∈N and for any E1, . . . , En ∈E the set E ⊂∞deﬁned by
E =

x =

xn

∈∞ xi ∈Ei ∀i = 1, . . . , n

is an event, E ∈E∞and
P∞(E) =
n
=
i=1
P(Ei).
One can now easily check the following.
Proposition 4.87 Let X :  →R be a random variable on (, E, P). Then the
random variables Xk deﬁned on (∞, E∞, P∞) for k = 1, 2, . . . by
Xk(x) := X(xk)
if
x =

xk

are independent random variables equidistributed as X, i.e. (P∞)Xk = PX
∀k.

152
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
4.4.5
A few applications of the law of large numbers
4.4.5.1
Bernstein polynomials
The following approximation theorem is a classical application of the weak law
of large numbers. Let f : [0, 1] →R be a function. The polynomials
pn(x) :=
n

j=0
n
j

f
j
n

xj(1 −x)n−j
are called Bernstein polynomials of f . Clearly the degree of pn(x) cannot
exceed n.
Theorem 4.88 (Bernstein) Let f ∈C0([0, 1]). Then pn(x) →f (x) uniformly in
the interval [0, 1].
In particular, the Bernstein theorem proves that the space of polynomials is
dense in C0([0, 1]) with respect to the uniform convergence; a property which
the space of partial sums of power series does not satisfy.
Proof. Let

Xn

be a sequence of independent Bernoulli trials of parameter
x. Thus E
%
Xn
&
= x and Var [Xn] = x(1 −x) ≤1/4. Recall the weak estimate
P

|Sn
n −x| > δ

≤
1
4 δ2 n
∀δ > 0
(4.38)
where Sn := n
k=1 Xk.
The random variable Sn counts the number of successes in n trials, hence Sn
follows the binomial distribution of parameters n and x, so that for every j ∈N
P
Sn
n = j
n

= B(n, x)({j}) =
n
j

xj(1 −x)n−j.
Thus, if f ∈C0([0, 1]), we have
E
%
f (Sn/n)
&
=
n

j=0
f
j
n

P(Sn = j) = pn(x).
Let ε > 0. f is continuous, hence it is bounded, i.e. there exists a constant
M > 0 such that ||f ||∞≤M. Moreover, f is uniformly continuous, so that there
exists δ > 0 such that |f (x) −f (y)| < ε whenever |x −y| ≤δ. Thus, by (4.38),
|pn(x) −f (x)| =
E
%
f (Sn/n)
&
−E [ 1 ]f (x)
 =
E
%
f (Sn/n) −f (x)
&
≤E
%
|f (Sn/n) −f (x)|
&
=
*
| Sn(t)
n
−x| > δ
f
Sn(t)
n

−f (x)
 P(dt)

VECTOR VALUED RANDOM VARIABLES
153
+
*
| Sn(t)
n
−x|≤δ
f
Sn(t)
n

−f (x)
 P(dt)
≤2 M
4 δ2 n + ε.
Consequently, choosing n := M/(2δ2ε) we conclude that |pn(x) −f (x)| ≤2ε
whenever n ≥n and x ∈[0, 1]. Since ε is arbitrary, pn(x) →f (x) uniformly in
[0, 1].
4.4.5.2
The Monte Carlo method
Let Q = [0, 1]N and f ∈C0(Q). We want to estimate
*
Q
f (x) dx.
We may consider a method which is analogous to the one-dimensional Simpson
method. Divide each edge of Q in k parts, then one has to evaluate f in kN
points, i.e. in a very large number of points (consider for instance k = 100,
N = 4). During the Second World War, Enrico Fermi (1901–1954), John von
Neumann (1903–1957) and Stanislaw Ulam (1909–1984) used a probabilistic
method, called the Monte Carlo method.
Example 4.89 Take a list of 2n phone numbers, for instance from a couple of
pages of the telephone directory and, with the last two digits of each number,
form n couples of integers (i, j) ∈{0, . . . , 99}2. Let T (n) be the number of
couples (i, j) such that i2 + j2 ≤(99)2. One can see that, as n increases, the
ratio T (n)/n gets nearer and nearer to 0.769 ≃π
4 (0.99)2.
Let f : RN →R be a bounded measurable function and set M := ||f ||∞.
Assume (B(RN), μ) is a probability measure on RN. We want to compute
f :=
*
RN f (t) μ(dt).
The idea is to ‘randomly choose’ points Xj in RN and to approximate
.
f (t) μ(dt) with 1
n
n
j=1 f (Xj).
We can model the random choice of points as a sequence of random variables

Xn

that follow the distribution μ, PX = μ, with a suitable degree of indepen-
dence. For instance, one can consider a random variable on a probability space
(, E, P) with values in Q and following the distribution PX = μ, and choose
the corresponding sequence

Xn

of independent and identically distributed ran-
dom variables on the Bernoulli scheme (∞, E∞, P∞) following the distribution
PX. Then, for every positive integer n we get
E
%
f (Xn)
&
= E
%
f (X)
&
=
*
RN f (t) μ(dt) = f ,

154
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
E
%
|f (Xn)|
&
= E
%
|f (X)|
&
=
*
RN |f (t)| μ(dt) ≤M,
Var [f (Xn)] = Var [f (X)] =
*
RN |f (t) −f |2 μ(dt) ≤4M2.
The random variables Yn := f (Xn) are independent, see Proposition 4.36, iden-
tically distributed and their variance is not larger than 4M2. Applying the strong
law of large numbers
1
n
n

j=1
f (Xj(x)) →f
P-almost surely on ∞, in L2 and in probability. In particular, for any positive
δ > 0 and any positive integer n
P∞1
n
n

j=1
f (Xj) −f
 > δ

≤4M2
δ2 n .
(4.39)
Consider, e.g. the case M = 1 and n = 106. Then the probability that the distance
between 1
n
n
j=1 f (Xj) and f is larger than 0.02 is less than 0.01. In particular,
if μ is uniform, in 99% of cases (i.e. of random choices of n points) the error is
smaller than 0.02.
Notice that, in order to discuss the Monte Carlo method, it is enough to sup-
pose that the random variables

Xn

are such that PXn = μ for each n and that the
random variables Yn = f (Xn) are pairwise uncorrelated, an easier condition since
true independence is very hard to prove in practice. If we insist on having an inde-
pendence condition on the Xn’s, the pairwise independence of the Xn’s sufﬁces.
Example 4.90 Consider E ⊂RN, so that
μ(E) =
*
1E(t) μ(dt).
Let X be a random variable on (, E, P) following the distribution μ and let

Xn

be the sequence of independent and equidistributed random variables with
PXn = PX on the Bernoulli scheme (∞, E∞, P∞). For any random point Xk,
we say that we have a success if Xk ∈E, failure otherwise. The number of
successes in n trials Tn,E is given by
Tn,E :=
n

j=1
f (Xj),
where
f (t) := 1E(t).
Since the variables f (Xj) are pairwise independent,
1
nTn,E →P(E)
P∞-almost surely, in L2 and in probability with respect to the measure P∞.

VECTOR VALUED RANDOM VARIABLES
155
4.4.5.3
Empirical distribution function
Let X :  →R be a random variable on the probability space (, E, P). We
want to approximate the law FX(t) := P(X ≤t).
Let us apply Monte Carlo method: ﬁx t ∈R and set
Et :=

x ∈
 X(x) ≤t

.
Consider a sequence of independent identically distributed random variables

Xn

following the distribution PX of X. For each n ∈N, let Tn,t(x) be the
number of random variables Xj such that j ≤n and Xj(x) ≤t (i.e. count how
many times, in n trials, a number, randomly chosen according the distribution
PX, is not larger than t). Of course,
Tn,t(x) =
n

j=1
1Et(x)
and Monte Carlo method yields
1
nTn,t(x) →E
%
1Et
&
= P(Et) = FX(t)
where the convergence is in probability, in L2 and P∞-almost surely.
The family of random variables

Fn(x, t)

n deﬁned for x ∈∞by
Fn(x, t) := 1
nTn,t(x)
is called the empirical distribution function of X. One may think of these random
variables as distributed on the vertical line through (t, 0) on the plane where the
graph (t, FX(t)) of FX(t) lives.
As we have seen, Monte Carlo method yields
Fn(x, t) →FX(t)
as n →∞
(4.40)
P∞-almost surely, in L2 and in probability at every t. With the same procedure
one can prove that we also have
Fn(x, t−) →FX(t−)
as n →∞
(4.41)
P∞-almost surely, in L2 and in probability at every t. One may also prove that
the convergence in (4.40) is uniform with respect to t ∈R and thus get the
following result.
Theorem 4.91 (Glivenko–Cantelli) With the notations above,
sup
t∈R
|Fn(t, x) −FX(t)| →0
P∞-almost surely.
Theorem 4.91 easily follows from (4.40), (4.41) and the following lemma.

156
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Lemma 4.92 Let f and fn : R →R, ∀n ≥1 be monotone nondecreasing
and
right-continuous
functions
such
that
f (−∞) = fn(−∞) = 0
and
f (+∞) = fn(+∞) = 1. Assume that
fn(t) →f (t)
and
fn(t−) →f (t−)
(4.42)
for any t ∈R. Then
sup
t→R
|fn(t) −f (t)| →0
as n →∞.
Proof. Let k ≥1. For any j = 0, 1, . . . , k, set Ej := {t | f (t) ≥j/k} and let
tj := inf Ej. Clearly, t0 = −∞, tk ≤+∞and tj−1 ≤tj for every j. Since f
is right-continuous, |f (t) −f (s)| ≤1
k if tj−1 ≤t, s < tj; in particular, |f (t) −
f (t−
j )| and |f (t) −f (t+
j−1)| ≤1
k. Let tj−1 ≤t < tj, then
fn(t) ≤fn(t−
j ) = f (t−
j ) −(fn(t−
j ) −f (t−
j ))
≤f (t) + |fn(t−
j ) −f (t−
j )| + 1
k
fn(t) ≥fn(t+
j−1) = f (t+
j−1) −(fn(t+
j−1) −f (t+
j−1))
≥f (t) −1
k −|fn(tj−1) −f (tj−1)|
hence
|fn(t) −f (t)| ≤Rn + 2
k
∀t ∈R
(4.43)
where
Rn := max
j=1,...,k(|fn(tj−1) −f (tj−1)| + |fn(t−
j ) −f (t−
j )|).
By assumption (4.42) and, since by assumption fn(−∞) = f (−∞), fn(+∞) =
f (+∞), we infer that Rn converges to 0 as n →∞. Thus, by (4.43) |fn(t) −
f (t)| ≤3/k uniformly on R for large enough n’s. Since k is arbitrary, the claim
is proven.
4.4.5.4
Entropy
Consider a source of information producing a random sequence of symbols from
a ﬁnite alphabet E = {1, . . . , q}. Assume that each symbol i ∈E has a given
probability p(i). We can model the situation as follows: consider a random
variable X :  →E deﬁned on a probability space (, E, P) with mass den-
sity p = (p(i)) and let

Xn

be the sequence of independent random variables
following the same distribution deﬁned on the Bernoulli scheme (∞, E∞, P∞).
For each n ≥1, let Mn = (X1, . . . , Xn) be a sequence of n symbols in E. Then,

VECTOR VALUED RANDOM VARIABLES
157
the probability that the source of information has produced the sequence Mn is
the product of the probabilities of the symbols in Mn,
P(Mn) =
n
=
i=1
p(Xi).
(4.44)
Consider, for instance, the case of a fair dice, E = {1, . . . , 6}, where each symbol
is equiprobable, i.e. p(i) = 1/6 for every i. Then the probability of each message
is 1/6n.
In general, when different symbols have different probabilities, then P(Mn)
depends on the symbols appearing in Mn: if, e.g. Mn = (1, . . . , 1) then P(Mn) =
p(1)n while if Mn = (2, . . . , 2), then P(Mn) = p(2)n and so on. However,
we claim that, when n is large enough, P-almost surely P(Mn) has the same
behaviour. In fact, (4.44) can be written as
1
n log P(Mn) = 1
n
n

k=1
log(p(Xk)).
On the other hand, the random variables Yn := −log(p ◦Xn) are independent,
see Proposition 4.36, and follow the same distribution, that is the distribution
of Y := log(p(X)). Thus they have also ﬁnite expected value and variance.
In particular,
E
%
Yn
&
= E [ Y ] =
*
log(p(x)) PX(dx) =
q

i=1
log(p(i)) p(i).
The strong law of large numbers applies, and we infer that
1
n log P(Mn) →E [ Y ]
as n →∞
P-almost surely, in L2 and in probability. The number
H(p) := −

y∈E
p(y) log p(y),
is called the entropy of the probability distribution of the alphabet E.
The entropy function
H(p) := −
q

i=1
pi log pi
deﬁned on the set
 :=

p = (p1, . . . , pn) ∈Rn 
n

i=1
pi = 1, pi ≥0 ∀i
>
,
plays a crucial role in information theory. Here we just point out that H(p)
achieves its maximum on  in just one point, i.e. (1/n, . . . , 1/n). Thus the

158
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
entropy of a set E of n-symbols is maximized if and only if the symbols in E
are equiprobable.
4.4.5.5
Waiting time
Assume that the intervals of time occurring between two breakdowns in a certain
piece of machinery have all the same length E. Of course, during an interval of
time of length nE, exactly n breakdowns occur, so that the number of breakdowns
per unit of time is
N = n
nE = 1
E .
A similar relation occurs when the times between two subsequent breakdowns
are independent random variables with the same expected value; namely, we have
the following.
Proposition 4.93 Let

Tn

be a sequence of non-negative random variables on
(, E, P). Moreover, assume the Tn’s are:
• summable, with the same expected value, pairwise uncorrelated and with
equibounded variances; or
• integrable, identically distributed and pairwise independent.
Set E := E
%
Xn
&
and let Sn := n
k=1 Tk, and, for each t ∈R, let
Nt(x) := sup

n
 Sn(x) ≤t

.
Then
lim
t→∞
Nt(x)
t
= 1
E
P-almost surely.
Example 4.94 Let

Tn

be the length of time between the (n −1)th and the nth
event, so that Sn is the length of time one has to wait to have n events. Nt is
the number of events occurring until time t. We say that

Nt

is the counting
process associated with the time random variables

Tn

. Finally, Nt/t is the
average number of events occurring in [0, t].
Proof of Proposition 4.93. For each x ∈, Nt := Nt(x) is an integer and, by
deﬁnition, SNt ≤t ≤SNt+1. The random variables Tk(x) are summable, so that
Tk < +∞P-almost surely ∀k. As t goes to +∞,
Nt(x)+1

k=1
Tk(x) = SNt+1(x) →+∞
so that Nt(x) →∞P-almost surely. Since SNt (x) ≤t ≤SNt+1(x) we have
SNt (x)
Nt(x) ≤
t
Nt(x) ≤SNt+1(x)
Nt(x) + 1
Nt(x) + 1
Nt(x)
.
(4.45)

VECTOR VALUED RANDOM VARIABLES
159
By the appropriate version of the strong law of large numbers, either the
Rajchman theorem, Theorem 4.80, or the Etemadi theorem, Theorem 4.81, or
its extension to integrable random variables, Exercise 4.111, we have
Sn(x)
n
→E
P-almost surely.
Since Nt(x)+1
Nt(x)
→1 P-almost surely, (4.45) yields
t
Nt(x) →E
P-almost surely.
4.4.6
Central limit theorem
4.4.6.1
Convergence in law and weak convergence
Let

Xn

be a sequence of random variables on a probability space (, E, P).
Assume

Xn

converges P-almost surely to a random variable X. What can be
said about the laws FXn and the distributions PXn?
Example 4.95 Let X be a random variable and let FXbe its law. For each n ∈N,
set Xn := X + 1
n, so that Xn converges uniformly to X. When we consider the
laws we have
FXn(t) = P(Xn ≤t) = P

X ≤t −1
n

hence FXn(t) converges to FX(t−) which agrees with FX(t) if and only if FX is
continuous at t.
Example 4.96 Consider the example provided in Exercise 4.105. There

Xn

is a sequence of Bernoulli trials of probability p = 1/2,

Xn

converges to 1/2
P-almost surely, but, for each t, FXn(t) converges pointwisely to the function
φ(t) :=
⎧
⎪⎨
⎪⎩
0
if t < 1/2,
1/2
if t = 1/2,
1
if t > 1/2.
The function φ(t) is not right-continuous, so it cannot be the law of a random
variable.
These examples justify the following deﬁnition.
Deﬁnition 4.97 Let

Xn

be a sequence of random variables. We say that

Xn

converges in law to a random variable X if FXn(t) converges to FX(t) at each
point t where FX is continuous.
It can be shown, see Proposition B.73, that if

Xn

converges to X in probabil-
ity (in particular if

Xn

converges to X P-almost surely), then

Xn

converges
to X in law.

160
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
The notion of convergence in law is strongly related to the notion of weak
convergence of measures.
Deﬁnition 4.98 Let (B(R), μ) and (B(R), μn), n ≥1, be probability measures
on R. We say that

μn

weakly converges to μ and one writes μn ⇀μ, if
*
R
ϕ(t) μn(dt) →
*
R
ϕ(t) μ(dt)
for every continuous bounded function ϕ : R →R.
It can be shown, see Theorem B.71, that μn ⇀μ if and only if Fn(t) :=
μn([−∞, t]) →F(t) := μ([−∞, t]) at every t ∈R such that F(t) is continuous.
In particular, if X and Xn, n ≥1, are random variables then

Xn

converges to
X in law if and only if

PXn

weakly converges to PX.
4.4.6.2
Central limit theorem
Recall that the normal distribution N(0, 1) is the absolutely continuous proba-
bility measure on R, whose density and law are respectively given by:
f (x) :=
1
√
2π
e−x2
2 ,
(t) :=
1
√
2π
* t
−∞
e−x2
2 dx.
The following convergence result can be proven.
Theorem 4.99 Central limit theorem Let

Xn

be a sequence of independent,
identically distributed random variables such that E
%
Xn
&
= E and Var
%
Xn
&
=
σ 2. For each n ≥1, set Sn := n
k=1 Xk. Then for any t ∈R
P
Sn −nE
σ√n
≤t

→(t)
as n →∞.
(4.46)
The central limit theorem has a long history which dates to the works of Abraham
de Moivre (1667–1754) and Pierre-Simon Laplace (1749–1827) on the limits
of Bernoulli trials. The proof of Theorem 4.99 requires the introduction of ideas
and methods that are beyond the scope of this book. In this volume we limit
ourselves to a few simple remarks and refer the interested reader to, e.g. [4–10]
where several versions of the theorem, that also applies to sequences of random
variables following different distributions, are considered.
With the same assumptions of the central limit theorem, the weak law (4.29)
yields
P
Sn
n −E ≤t

→

0
if t < 0,
1
if t > 0.
On the other hand, (4.46) yields
P
Sn
n −E ≤σt
√n

= P
Sn −nE
σ√n
≤t

→(t) ̸= 0,
(4.47)

VECTOR VALUED RANDOM VARIABLES
161
In words, the central limit theorem provides the right rescaling (1/√n) in
order to achieve a nontrivial limit (different from 0 and 1). The convergence
stated in (4.46) is equivalent to the weak convergence of the distribution of
(Sn −nE)/(σ√n) to N(0, 1), so that
P
 σa
√n < Sn
n −E ≤σb
√n

→(b) −(a)
as n →∞.
Trivially, we cannot rewrite (4.47) as
P
Sn
n −E ≤σb

−→(b√n),
and, moreover, (4.47) does not straightforwardly imply that
P
Sn
n −E ≤σb

−(b√n)
 −→0.
(4.48)
In order to get (4.48) one needs to know that the convergence in (4.46) is uniform
with respect to t ∈R. Lemma 4.92 provides this piece of information: hence,
(4.46) can be improved to
sup
t∈R
P
Sn −nE
σ√n
≤t

−(t)
 −→0
as n →∞,
or, equivalently, to
sup
t∈R
P(Sn ≤t) −
t −nE
σ√n
 −→0
as n →∞.
Another piece of information that is missing is the speed of such convergence.
The following theorem provides a useful estimate under an additional hypothesis.
Theorem 4.100 (Berry–Esseen) Let

Xn

be a sequence of independent, inden-
tically distributed random variables such that E
%
Xn
&
= 0, Var
%
Xn
&
= σ 2 > 0
and γ := E
%
|Xn|3 &
< ∞. Let Sn = n
k=1 Xk. Then
P
 Sn
√n ≤σt

−(t)
 ≤C
√n,
(4.49)
where C := 0.8 γ
σ 3 .
Estimate (4.49) is uniform with respect to t ∈R. Therefore, with the same
notations, we also have
P(Sn ≤t) −
t −nE
σ√n
 ≤C
√n.
(4.50)

162
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Estimate
(4.50)
allows
us
to
approximate
FSn(t)
with
a
suitable
renormalization of the standard normal law,
FSn(t) ≃
t −nE
σ√n

(4.51)
with a known bound on the error.
In many applications, where PXn is not too badly distributed around its
expected value, both experience and numerics suggest that the approximation
in (4.51) is good enough for n ≥30, so that approximation (4.51) is often used
as if it were an equality.
Remark 4.101 One may ask if the sequence {Sn/√n} converges P-almost surely
to a random variable Z following the normal law N(0, 1). The answer is negative.
In fact, applying the iterated logarithm law, Theorem 4.86,
lim sup
n→∞
Sn
C

2n log log n
= 1
P-almost surely,
where C2 = Var [Xn], so that
lim sup
n→+∞
Sn
√n = +∞
P-almost surely.
As a consequence, if Sn/√n →Z almost surely, then Z = +∞P-almost surely,
a contradiction.
The proof of the Berry–Esseen theorem, Theorem 4.100, as well as the proof
of central limit theorem requires the introduction of ideas and methods that are
beyond our scope. The interested reader may consult e.g. [4–7, 10].
Example 4.102 (Bernoulli trials and central limit theorem) Let Xn,
n ≥1, be independent Bernoulli trials of parameter p. Then E
%
Xn
&
= p,
σ 2 := Var [Xn] = p(1 −p) and
E
%
|X1 −p|3 &
= p3(1 −p) + (1 −p)3p = p(1 −p)(1 −2p).
For n ≥1, the random variable Sn := n
k=1 Xk follows the Bernoulli distribution
B(n, p),
P(Sn = k) = B(n, p)({k}) =
n
k

pk(1 −p)n−k
∀k = 0, . . . , n,
hence
FSn(t) =

k≤t
B(n, p)({k}).
Thus, the Berry–Esseen formula reads
FSn(t) −

t −np
√np(1 −p)
 ≤C
√n
where C = 0.8 γ σ −3 = 0.8p−1/2(1 −p)−3/2.

VECTOR VALUED RANDOM VARIABLES
163
Experience teaches that the left-hand term in the previous estimate is in many
cases much smaller than the right-hand one. An empirical rule for a sequence of
Bernoulli trials of parameter p is that, if both np and n(1 −p) are larger than
5, then the approximation of B(n, p) with the normal distribution works well in
applications. A better approximation is achieved by a translation of the normal
law of 0.5 to the left: in fact, the law of the Bernoulli distribution is constant
on intervals of length 1 and is right-continuous. This procedure is called the
continuity correction and it must be done any time we approximate a discrete
law with the normal law. Thus a better approximation of FB(n,p) is
FB(n,p)(t) ≃
t −np + 0.5
√np(1 −p)

.
(4.52)
Another possibility is to normalize the Bernoulli distribution before ‘compar-
ing’ it with the normal law. In fact,
P
Sn −np
√n
≤σ t

=

k−np≤σ√nt
B(n, p)({k}) =

xk≤t
B(n, p)({k})
where xk := k−np
σ√n . Let ρ(t) be the piecewise constant function
ρ(t) =
n

k=0
yk1[xk,xk+1[(t),
yk := σ√nB(n, p)({k}).
Since xk+1 −xk =
1
σ√n, we get
P
Sn −np
√n
≤σ t

=
* t
−∞
ρ(t) dt.
The Berry–Esseen theorem yields the estimate

* t
−∞
ρ(s) ds −(t)
 ≤C
√n,
C ≤0.8 γ
σ 3 ,
see Figure 4.5.
4.4.7
Exercises
Exercise 4.103 Let

Xn

be a sequence of independent random variables with
the same expected value E, E
%
Xn
&
= E ∀n, and with equibounded variances,
Var
%
Xn
&
≤C2 ∀n. For n = 1, 2, . . . , let Yn := 1
n
n
k=1 Xk and let FYn(t) be its
law. Prove that
FYn(t) →

0
if t < E,
1
if t > E,
that is,

Yn

converges in law to the constant random variable Y(x) := E ∀x ∈.

164
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
6
4
2
0
−6
−4
−2
4
2
0
(a)
(b)
−4
−2
0
−0.1
0.1
0.2
0
−0.2
0.4
0.6
0.8
1
1.2
0.2
0.3
0.4
0.5
0.6
N(0, 1)
B(30, 0.5)
N(0, 1)
B(30, 0.5)
Figure 4.5
A comparison between (a) the densities and (b) the laws of a rescaling
of B(30, 1/2) and N(0, 1).
Exercise 4.104 Let

Xn

be a sequence of independent Bernoulli trials of param-
eter 1/2. For n ≥1, set Yn := 1
n
n
k=1 Xk and let FYn(t) be the law of Yn. Show
that
P

|Yn −1
2| = 0

→0.
Notice that the weak law of large numbers yields instead
P

|Yn −1
2| ≤δ

→1
∀δ > 0.
Solution. Let Sn = n
j=1 Xj. We have
P

Yn = 1
2

= P

Sn = n
2

= B(n, 1/2)({n/2})
=

0
if n is odd,
1
2n
 n
n/2

if n is even = O
 1
√n

by the Stirling formula.
Exercise 4.105 Let

Xn

be a sequence of independent Bernoulli trials of param-
eter 1/2. For n ≥1, set Yn := 1
n
n
k=1 Xk and let FYn(t) be the law of Yn. Show
that
FYn(t) →
⎧
⎪⎨
⎪⎩
0
if t < 1/2,
1/2
if t = 1/2,
1
if t > 1/2.
Notice that the limit function is not right-continuous.

VECTOR VALUED RANDOM VARIABLES
165
Solution. The weak law of large numbers yields FYn(t) →1 if t > 1/2 and
FYn(t) →0 if t < 1/2, see Exercise 4.103. Assume now t = 1/2 and let
Sn := n
j=1 Xj. For any q ∈N we have
P

Yn = q

= P

Sn = qn

= B(n, 1/2)({nq})
=

1
2n
 n
nq

if nq ∈{0, . . . , n},
0
otherwise.
Therefore
P

Yn ≤1
2

= 1
2n
⌊n/2⌋

k=0
n
k

=

1
2
if n is odd,
1
2 −1
2
 n
n/2

if n is even.
Hence P(Yn ≤1/2) = 1
2 + O( 1
√n) by the Stirling formula.
Exercise 4.106 Let X, Xn, n ∈N, be random variables on a probability space
(, E, P). Show that lim supn Xn(x) ≤X(x) P-almost surely if and only if for
every δ > 0, P-almost surely Xn(x) ≤X(x) + δ deﬁnitely. We mean that for every
δ > 0 we have P(Fδ) = 1 where
Fδ :=

x ∈
 Xn(x) ≤X(x) + δ deﬁnitely

.
Exercise 4.107 Let X1, . . . , Xn be n independent, identically distributed random
variables on a probability space (, E, P) and set E := E
%
Xn
&
and μ := PXn.
Show that
*

1
n
n

j=1
Xj −E

2
P(dx)
=
*
Rn
1
n
n

j=1
tj −E

2
μ(dt1)μ(dt2) · · · μ(dtn)
and
P
1
n
n

j=1
Xj −E
 > δ

= μ × · · · × μ

t = (tj) ∈Rn 
1
n
n

j=1
tj −E
 > δ

.
Exercise 4.108 Let

Xn

be a sequence of random variables such that Xn is
independent of X1, . . . , Xn−1 for every n ≥2. Show that the random variables
Xn are independent.

166
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 4.109 Let

Xn

and

Yn

converge to X and Y P-almost surely, respec-
tively. Show that

Xn + Yn

converges to X + Y P-almost surely.
Exercise 4.110 Let X, Xn, n ≥1 be random variables deﬁned on a probability
space (, E, P) and assume that for any α > 1,
1
α X(x) ≤lim inf
n→∞Xn(x) ≤lim sup
n→∞
Xn(x) ≤αX(x)
P-almost surely. (4.53)
Show that Xn →X P-almost surely.
Solution. By assumption for any α > 1 there exists a set Nα ∈E with P(Nα) =
0 such that inequalities (4.53) hold for each x ∈ \ Nα. Let

αn

be a monotone
decreasing sequence converging to 1 and let N = ∪nNαn so that P(N) = 0. For
any x ∈ \ N and any n ∈N we have
1
αn
X(x) ≤lim inf
k→∞Xk(x) ≤lim sup
k→∞
Xk ≤αnX(x).
As n goes to inﬁnity we get
X(x) ≤lim inf
k→∞Xk(x) ≤lim sup
k→∞
Xk ≤X(x),
i.e.
lim
k→∞Xk(x) = X(x)
∀x ∈ \ N.
Exercise 4.111 Prove the following extension of the Etemadi version of the strong
law of large numbers, Theorem 4.81.
Theorem
Let

Xn

be integrable, pairwise independent and identically
distributed random variables. For every n ≥1, set Sn(x) := n
k=1 Xk(x). Then
E
2 Sn
n
3
→E
%
X1
&
.
Solution. Thanks to the Etemadi theorem, it remains to prove the claim
for sequences

Xn

of random variables such that Xn ≥0 ∀n and E
%
Xn
&
=
+∞
∀n.
For any M > 0, let Yn,M(x) := Xn(x)1{0≤Xn≤M}(x). The random variables

Yn,M

are pairwise independent, equidistributed and summable. Therefore, if
Sn,M(x) := n
k=1 Xn,M(x), the Etemadi theorem yields
E
2 Sn,M(x)
n
3
→E
%
Y1,M
&
so that
lim inf
n→∞E
2 Sn
n
3
≥E
%
Y1,M
&
∀M

VECTOR VALUED RANDOM VARIABLES
167
since Sn(x) ≥Sn,M(x). Since M is arbitrary, the Beppo Levi theorem implies
E
%
Y1,M
&
→E
%
X1
&
= ∞, hence the claim is proven.
Exercise 4.112 Throw 100 fair dice and compute, approximately the probability
that the sum of the values of the dice is in [325, 375].
[≃0.86]
Solution. Let Xn, n = 1, 2, . . . , 100, be the result of the nth dice. The random
variables Xn are independent, identically distributed with expected value μ = 7/2
and variance σ 2 = 35/12.
We want to compute P
100
i=1 Xi ∈[325, 375]

. By the central limit theorem
with the continuity correction, see (4.52), we get
P
? 100

i=1
Xi ∈]324, 375]
@
= P
?
324.5 ≤
100

i=1
Xi ≤375.5
@
= P
⎛
⎜⎝
324.5 −100 7
2
10
6
35
12
≤
100
i=1 Xi −100 7
2
10
6
35
12
≤375.5 −100 7
2
10
6
35
12
⎞
⎟⎠
≃2(1.49 −1) ≃0.864.

5
Discrete time Markov chains
In this chapter we give a short survey of the properties of a class of simple
stochastic processes, namely, the discrete time homogeneous Markov chains with
discrete states, that widely appear both in pure and applied mathematics, and have
many applications in science and technology, see e.g. [11–15].
After introducing stochastic matrices in 5.1, and discrete time stochastic pro-
cesses and their transition matrices, we deal with discrete time Markov chains
and the Markov properties in Section 5.2. Section 5.3 is devoted to computing a
few interesting parameters related to a Markov chain. Convergence of the pow-
ers of the transition matrices and the characterization of the limit in terms of
return times are investigated in Sections 5.5 and 5.7. Markov chains with a ﬁnite
number of states are peculiar. The canonical form of a ﬁnite stochastic matrix is
presented in Section 5.4; in Section 5.5 we discuss the convergence of powers
of a ﬁnite regular transition matrix, the relation between the limit and the return
times and, ﬁnally, summarize the relations between the parameters introduced in
Section 5.3 and the transition matrix. In Section 5.6 we deal with the ergodic
property of Markov chains with an irreducible transition matrix. This eventually
leads to a probabilistic method for the computation of the expected value of a
random variable, known as Markov chain Monte Carlo, which plays a prominent
role in numerical and statistical applications. Section 5.7 contains the conver-
gence results for arbitrary transition matrices; therein the notion of periodicity is
introduced and the renewal theorem is proven.
5.1
Stochastic matrices
Before discussing stochastic processes and Markov chains, it is worth introducing
both stochastic vectors and matrices.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

DISCRETE TIME MARKOV CHAINS
169
5.1.1
Deﬁnitions
Let us start by ﬁxing some notations. Let RN∗be the linear space of row vec-
tors having N real components. To each matrix P ∈MN,N(R) we can associate
the linear map P : RN∗→RN∗deﬁned by x →xP. The row vectors e1 :=
(1, 0, . . . , 0), . . . , en := (0, . . . , 0, 1) are a basis for RN∗, called the standard
basis of RN∗. For each i = 1, . . . , N the row vector P(ei) := eiP is the ith row
of the matrix P.
A vector x = (xi) ∈RN∗is a stochastic vector if xi ≥0 ∀i and N
i=1 xi = 1.
The set of stochastic vectors in RN∗is denoted by T . Finally, a matrix P = (Pi
j) ∈
MN,N is called stochastic if each of its rows is a stochastic vector
Pi
j ≥0,
N

j=1
Pi
j = 1
∀i, j.
Proposition 5.1 Let P ∈MN,N(R) and let P(x) := xP. Then P is stochastic if
and only if P(T ) ⊂T .
Proof. Assume P is stochastic and let x = (x1, . . . , xN) ∈T . Thus (xP)j =
N
i=1 xiPi
j is non-negative and
N

j=1
(xP)j =
N

j=1
N

i=1
xiPi
j =
N

i=1
xi
N

j=1
Pi
j =
N

i=1
xi = 1.
This proves that P(x) ∈T .
Conversely, assume P(T ) ⊂T . Since for each i = 1, . . . , N, the ith row of
P is eiP = P(ei) ∈T , we conclude that P is stochastic.
With some care, the same holds when the set of indexes is denumerable.
Let S be a denumerable set. We call a vector indexed by S a sequence of real
numbers and denote it as x =

xj

j∈S or x = (xj)j∈S. We denote by ℓ1 the set of
all vectors x = (xj) indexed by S such that 
j∈S |xj| < ∞. A vector x = (xj)
indexed by S is called a stochastic vector if
xj ≥0 ∀j ∈S
and

j∈S
xj = 1.
The set of stochastic vectors will be denoted by T . Trivially T ⊂ℓ1.
We shall also deal with double sequences P = (Pi
j)i,j∈S that we call matrices
as in the case when S is ﬁnite. The set of relevant matrices are those for which
||P|| := sup
i∈S

j∈S
|Pi
j| < +∞.

170
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
A stochastic matrix is a matrix P = (Pi
j) such that
Pi
j ≥0,

j∈S
Pi
j = 1
∀i, j ∈S.
Notice that ||P|| = 1 and Pi
j ≤1 ∀i, j ∈S if P is stochastic.
If x = (xi) ∈ℓ1 and P is a matrix such that ||P|| < ∞, we notice that
(xP)j := 
i∈S xiPi
j is well deﬁned, being the sum of an absolute convergent
series. Moreover,

j∈S
|(xP)j| ≤

j∈S

i∈S
|xi||Pi
j| =

i∈S
|xi|

j∈S
|Pi
j| ≤||P||

i∈S
|xi|,
i.e. xP ∈ℓ1. Therefore every matrix P such that ||P|| < ∞deﬁnes a map P(x) :=
xP from ℓ1 into itself.
Observe also that if P and Q are two matrices indexed by S such that
||P||, ||Q|| < ∞, then their product PQ is well deﬁned by (PQ)i
j := 
k∈S Pi
kQk
j,
being the sum of an absolutely convergent series, and, moreover, ||PQ|| ≤
||Q|| ||P|| < ∞. Denoting by P, Q : ℓ1 →ℓ1 the maps deﬁned by P(x) := xP,
Q(x) := xQ, respectively, then Q(P(x)) = (xP)Q = x(PQ).
Proposition 5.2 Let P be a matrix indexed by S such that ||P|| < ∞and let
P : ℓ1 →ℓ1, P(x) := xP. Then P is a stochastic matrix if and only if P(T ) ⊂T .
The proof goes as the proof of proposition 5.1.
An easy consequence of proposition 5.1 and 5.2 is that products of stochastic
matrices are stochastic matrices hence, in particular, any power of a stochastic
matrix is a stochastic matrix. Notice also that a convex combination of stochastic
matrices, P = N
k=1 tiPi, ti ≥0, n
i=1 ti = 1, Pi stochastic, is a stochastic matrix
since

j∈S
Pi
j =

j∈S
n

k=1
tk(Pk)i
j =
n

k=1
tk

j∈S
(Pk)i
j =
n

k=1
tk = 1.
In other words, the set of stochastic matrices is a convex set of non-negative
matrices.
5.1.2
Oriented graphs
An oriented graph is a pair (S, E) where S is a ﬁnite or denumerable set of points,
called nodes, and E is a set of ordered pairs of nodes E = {(i, j) | i, j ∈S}, called
arcs. We say that the arc (i, j) goes from i to j, or, equivalently, that it joins i to
j. An oriented path from i to j is a ﬁnite union of arcs (i, i1)(i1, i2) · · · (ik, j).
An oriented graph (S, E) can be represented by a matrix A indexed by S,
called the incidence matrix of the graph and deﬁned as
Ai
j =

1
if there is an arc from i to j,
0
otherwise.

DISCRETE TIME MARKOV CHAINS
171
For each i, j ∈S we have
(A2)i
j =

k∈S
Ai
kAk
j > 0
if and only if there exists k ∈S such that Ai
k = Ak
j = 1, i.e. if and only if there
exists at least a path from i to j made of exactly two arcs. Similarly, for each
positive integer n
(An)i
j =

i1,i2,...,in−1∈S
Ai
i1Ai1
i2Ai2
i3 · · · Ain−1
j
is positive if and only if there exists at least one path from i to j made of exactly
n arcs.
The same reasoning applies to weighted oriented graphs, i.e. to those oriented
graphs such that a weight pij > 0 is attached to each arc (i, j) of the graph. In
fact, we can repeat the previous argument replacing the matrix A with the matrix
(A′)i
j =

pij
if there is an arc from i to j,
0
otherwise.
This way, a weighted oriended graph is represented by a matrix.
Conversely, to each non-negative matrix P indexed by S one associates a
weighted oriented graph as follows: let P = (pij)i,j∈S. Whenever pij > 0 draw
a directed arc from i to j and associate the weight pij to such arc.
For example, assume S = {1, 2, 3}. Then the stochastic matrix
P =
⎛
⎝
2/9
0
7/9
1/2
0
1/2
1/7
6/7
0
⎞
⎠
(5.1)
can be represented as shown in Figure 5.1.
Deﬁnition 5.3 Let P be a stochastic matrix indexed by S and let i, j ∈S. If there
exists n ≥0 such that p(n)
ij := (Pn)i
j > 0, we say that i leads to j, or that j is
accessible from i, and denote it as i →j. If i →j and j →i, we say that i and
1
2
3
7 / 9
1 / 7
2 / 9
1 / 2
1 / 2
6 / 7
Figure 5.1
A graph representing the matrix P deﬁned in (5.1).

172
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
j are comunicating and write i ↔j. If i ↔j for every i, j ∈S then the matrix
P is called irreducible.
Since P0 = Id, clearly i →i for any i ∈S. Moreover, if i →j and j →
k, then i →k. Furthermore, the relation i ↔j is an equivalence relation on
the set of indexes S, and therefore S can be split into equivalence classes of
comunicating indexes.
It is clear that i leads to j if and only if either i = j or i ̸= j and there exists
a path from i to j.
If the set of nodes S is ﬁnite, S = {1, . . . , N}, a path from i to j exists if
and only if there exists at least one path made by no more than N arcs [actually
(N −1) if i ̸= j]. Thus a path from i to j, i ̸= j, exists if and only if
Ci
j := (P + P2 + · · · + PN−1)i
j > 0.
In particular, for each node i ∈S, the set

j ∈S
 Ci
j > 0

is the set of the nodes that are reachable by at least an oriented path starting
from i.
Whenever S = {1, . . . , N}, i →j if and only if either i = j or i ̸= j and
there exists a path from i to j made by at most N −1 arcs. Thus i leads to j if
and only if
Bi
j := (Id + P + P2 + PN−1)i
j > 0.
(5.2)
In particular, the set of indexes (nodes) accessible from i is the set of indexes
(nodes) j such that Bi
j > 0,

j ∈{1, . . . , N}
 i →j

=

j
 Bi
j > 0

.
(5.3)
5.1.3
Exercises
Exercise 5.4 Show that the set T ⊂RN∗of stochastic vectors is closed and con-
vex. Show that the standard basis of RN∗is the set of extreme points of T , i.e.
that T is the convex envelope of

e1, . . . eN
.
Exercise 5.5 Find the extreme points of the set of stochastic matrices in MN,N(R).
Exercise 5.6 Show that a matrix P ∈MN,N(R) is irreducible if and only if all the
elements of
P + P2 + · · · + PN
are positive.

DISCRETE TIME MARKOV CHAINS
173
5.2
Markov chains
In this section, after a short introduction to stochastic processes and their
transition matrices, we introduce Markov chains, a restricted class of stochastic
processes

Xn

with ﬁnite or denumerable state-space. Their main property is
that the joint distributions of the involved variables are ﬁxed by the transition
matrices of the process and by the distribution of the initial variable X0.
5.2.1
Stochastic processes
Let S be a ﬁnite or denumerable set. Without any loss of generality we can
assume S = Z or S = N or, when S is ﬁnite, S = {1, . . . , N}. A sequence of
random variables on a probability space (, E, P)

Xn

, Xn :  →S is called
a (discrete time) stochastic process with values in S. The elements of S are
called states and S is the state-space of the process. One can look at

Xn

in
several ways:
(i) The deﬁnition: a sequence

Xn

of random variables with values in S.
(ii) A sequence of states

Xn(x)

⊂S indexed by the parameter x ∈, that
is, the sequence of states taken by the point x ∈.
(iii) A map X : N ×  →S on the product  × N.
We are interested in describing the probabilistic laws related to the evolution of
the process and in computing the probabilities of various events detected by the
random variables

Xn

. Notice that, formally, we also need to have a probability
space structure on  × N in such a way that X : N ×  →S, X(n, x) := Xn(x)
is a random variable. Here, we just point out that, in general, describing the evo-
lution of a stochastic process, i.e. computing for each n ∈N the joint distribution
of (X0, X1, . . . , Xn) is a difﬁcult task. In fact, in general, the complexity of this
computation grows exponentially with respect to n since (X0, . . . , Xn) is a ran-
dom variable with values in Sn := S × · · · × S. A huge complexity reduction in
computing the joint distributions appears for a subclass of stochastic processes,
the Markov chains, see Section 5.3.
Description (ii) provides a helpful point of view on the process: at the ‘ﬁrst
step’ n = 0, we start from a random point X0(x) ∈S then, for each positive
integer n, we move from Xn(x) to Xn+1(x). Each x ∈ ﬁxes the sequence of
states

Xn(x)

⊂S, i.e. the path on S deﬁned by x. If j := Xn(x) we say that
the path (deﬁned by) x visits j at step n. Then the set

x ∈ | Xn(x) = j

is the
set of parameters such that the corresponding path visits j at step n. By abuse
of language (in general, two points x, y ∈, x ̸= y, can ﬁx the same path), one
refers to

x ∈ | Xn(x) = j

as the set of paths that visit j at the nth step.
Consequently P(Xn = j) is the probability that a path visits j at the nth step
and P(Xn = j | X0 = i) is the probability that a path starting from i visits j at
step n.

174
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
5.2.2
Transition matrices
Let

Xn

be a stochastic process with a ﬁnite or denumerable state-space S. For
each n ≥0, let P(n) := (P(n)i
j)i,j∈S be the matrix
P(n + 1)i
j :=

P

Xn+1 = j
 Xn = i

if P(Xn = i) > 0,
δi
j
if P(Xn = i) = 0.
We call the stochastic matrix P(n) the matrix of transition probabilities or the
transition matrix of the process

Xn

from the nth step to the (n + 1)th step.
For every n ≥0, the matrix P(n + 1) links the distribution of the random
variables Xn and Xn+1. In fact, for every n ≥0, let π(n) := (π(n)i)i∈S be the
mass density of PXn, that is, π(n)i := P(Xn = i) ∀i ∈S. Then
π(n + 1)j =

i∈S
P

Xn+1 = j
 Xn = i

P(Xn = i) =

i∈S
π(n)iP(n + 1)i
j
i.e.
π(n + 1) = π(n)P(n + 1).
(5.4)
Also notice that iterating (5.4), we get for each n, k ≥0
π(n + k) = π(k)P(k + 1)P(k + 2) · · · P(k + n).
(5.5)
5.2.3
Homogeneous processes
Deﬁnition 5.7 A stochastic process

Xn

with a ﬁnite or denumerable state-space
S is said to be homogeneous if the transition matrices of the random variables

Xn

are independent of n, i.e. if there exists a stochastic matrix P = (Pi
j),
i, j ∈S, such that for any n ∈N and for any (i, j) ∈S such that P(Xn = i) > 0,
we have
P

Xn+1 = j
 Xn = i

= Pi
j.
Thus, if

Xn

is a homogeneous stochastic process with transition matrix P with
a ﬁnite or denumerable state-space S, then for every n, k ≥0, the mass densities
π(n) and π(n + k) of Xn and Xn+k, respectively, are related by
π(n + k) = π(k)Pn.
5.2.4
Markov chains
5.2.4.1
Markov property
Let us start with a deﬁnition.
Deﬁnition 5.8 (Markov chain) A stochastic process

Xn

with a ﬁnite or denu-
merable state-space S is said to have the Markov property if for any positive

DISCRETE TIME MARKOV CHAINS
175
integers k, n, k ≤n, and for any choice of states i0, . . . , in+1 in S we have
P

Xn+1 = in+1
 Xn = in, . . . , Xk+1 = ik+1, Xk = ik

= P

Xn+1 = in+1
 Xn = in

(5.6)
whenever the involved conditional probabilities are deﬁned, i.e. whenever the
conditioning events have positive probabilities. If

Xn

has the Markov property,
we say that

Xn

is a Markov chain with state-space S.
We now state in a more appealing way the Markov property (5.6). Recall
that an event is said to be detected by X0, . . . Xn−1 if it is detected by their joint
distribution, i.e. if it is the disjoint union of events of the following kind

x ∈
 Xn−1(x) = in−1, . . . , X1(x) = i1, X0(x) = i0

.
We then state the following.
Theorem 5.9 Let

Xn

be a stochastic process with a ﬁnite or denumerable state-
space S. Then

Xn

is a Markov chain if and only if for any triplet of integers
r, n, k with 0 ≤r < n < n + k and for any triplet of events G, F, E detected by
X0, . . . , Xr, Xr+1, . . . , Xn and Xn+1, . . . , Xn+k, respectively, we have
P

E
 F ∩G

= P

E
 F

.
(5.7)
Moreover, if P(F ∩G) > 0, then
P

E ∩F
 G

= P

E
 F

P

F
 G

.
(5.8)
Equality (5.7) says that from a statistical point of view, the future state of the
chain depends only on the present state, memoryless of the past history. Equality
(5.8) is similar to the formula for the probability of the intersection of independent
events and will play a crucial role in what follows.
Proof of theorem 5.9. Choose a sequence

in

of points in S, and for n ≥0 set
An :=

x ∈
 Xn(x) = in

.
The Markov property, (5.6) and the law of total probability yield
P

An+1
 An ∩B

= P

An+1
 An

(5.9)
for any n ≥0 and for any event B detected by the random variables
X0, . . . , Xn−1. Clearly, (5.9) is meaningful if and only if P(An ∩B) > 0. We
now claim that we also have
P

An+1+k ∩· · · ∩An+1
 An ∩B

=
k+n
=
j=n
P

Aj+1
 Aj

(5.10)

176
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
if P(An+k ∩· · · ∩An ∩B) > 0. In fact, applying (5.9) and recalling that P(E ∩
F | G) = P(E | F ∩G)P(F | G) we get
P

An+1+k ∩· · · ∩An+1
 An ∩B

= P

An+1+k
 An+k ∩· · · ∩An+1 ∩An ∩B

P

An+k ∩· · · ∩An+1
 An ∩B

= P

An+1+k
 An+k

P

An+k ∩· · · ∩An+1
 An ∩B

.
An induction argument thus gives (5.10).
Equality (5.7) is an immediate consequence of (5.10) and of the law of total
probability. Moreover, (5.8) follows from (5.7) since
P(E ∩F | G) = P(E | F ∩G)P(F ∩G | G)
= P(E | F ∩G)P(F | G) = P(E | F)P(F | G).
We point out that equalities in (5.10) and the Markov property are in fact
special cases of (5.7). Finally, we mention another consequence of (5.7). Let

Xn

, r, k, n, and E, G be as in Theorem 5.9 and let

Fi

i∈I be a family of
disjoint events detected by Xr+1, . . . , Xn and F = ∪i∈IFi. Then using the law
of total probability, we have
P

E ∩(∪iFi)
 G

=

i∈I
P(E | Fi)P(Fi | G).
(5.11)
Let

Xn

be a stochastic process with a ﬁnite or denumerable state-space S.
As we have seen, the transition matrices {P(n)} of the process allow to compute
for any n ≥0 the mass density π(n + 1) of Xn+1 in terms of the mass density
π(n) of Xn by π(n + 1) = π(n)P(n + 1). Thus by an induction argument the
mass density of Xn+1 can be computed in terms of the mass density of X0,
π(n + 1) = π(0)P(1)P(2) · · · P(n + 1).
However, in general, the sequence {P(n)} does not contain enough information
to compute the joint distribution of two of the Xn’s: for instance, in general, we
cannot compute from P the probability P(Xn = j | X0 = i) for n ≥2 and ﬁxed i
and j. Examples can be given of different stochastic processes having the same
initial state, the same transition matrices but different joint distributions.
A dramatic reduction of complexity occurs for Markov chains: if

Xn

is
a Markov chain, then for every n ≥1 the joint distribution of X0, . . . , Xn
can be computed in terms of the transition matrices P(1), . . . , P(n) and
of the mass density of
X0. In fact, for any choice of i0, . . . , in ∈S, set

DISCRETE TIME MARKOV CHAINS
177
Aj :=

x ∈ | Xj(x) = ij

. From (5.10) or (5.7) we then infer
P

Xn+k = in+k, . . . , Xn+1 = in+1
 Xn = in

= P

An+k ∩· · · ∩An+1
 An

=
n+k−1
=
j=n
P

Aj+1
 Aj

= P(n)in
in+1P(n + 1)in+1
in+2 · · · P(n + k)in+k−1
in+k
(5.12)
whenever P(An+k ∩· · · ∩An) > 0. In particular,
P

Xk = ik, . . . , X1 = i1, X0 = i0

= π(0)i0P(1)i0
i1P(2)i1
i2 · · · P(k)ik−1
ik
.
(5.13)
5.2.4.2
Homogeneity
A Markov chain

Xn

with a ﬁnite of denumerable state-space is homogeneous
if P(n) = P ∀n. Therefore, (5.13) reads
P

Xk+n = ik, · · ·, Xn+1 = i1
 Xn = i0

=
k−1
=
j=0
P
ij
ij+1 = Pi0
i1Pi1
i2· · ·Pik−1
ik
.
(5.14)
As a consequence, since the right-hand side of (5.14) is independent of n, we
then deduce for any n, k ≥0 and for any i0, . . . , in ∈S the equality
P

Xn+k = ik, · · ·, Xn+1 = i1
 Xn = i0

= P

Xk = ik, · · ·, X1 = i1
 X0 = i0

.
(5.15)
This is the renewal property of homogeneous Markov chains: if P(Xn = i0) =
P(X0 = i0), then the joint distribution of the k variables following Xn is the
same as the joint distribution of the ﬁrst k variables. In words, the probabilistic
knowledge of Xn, that is of P(Xn = i0), restarts the chain: the stochastic process

Xn+k

k is a Markov chain with the same transition matrix P of {Xk}k.
5.2.4.3
Strong Markov property
Deﬁnition 5.10 Let

Xn

be a Markov chain on (, E, P) with a ﬁnite or
denumerable state-space S. A random variable T :  →N ∪{+∞} is called a
stopping time for

Xn

if for any non-negative integer n, the set

x ∈
 T (x) = n

(5.16)
is detected by the random variables X0, X1, . . . , Xn.

178
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Trivially, each of the following are stopping times:
• all constants;
• the ﬁrst passage time, i.e. the step at which the ﬁrst visit in A ⊂S occurs;
• the kth passage time, i.e. the step at which the k visit in A ⊂S occurs;
• the ﬁrst step at which we leave A ⊂S,
Observe that the step at which we leave a state j deﬁnitively, called the last exit
time, is not a stopping time.
Theorem 5.11 (Strong Markov property) Let

Xn

be a homogeneous Markov
chain with values in a ﬁnite or denumerable set S deﬁned on a probability space
(, E, P). Let T be a stopping time for

Xn

. Then, for any i, j ∈S and any n ≥1
P

XT +n = j
 T < +∞, XT = i

= P

Xn = j
 X0 = i

.
That is, the sequence

Yn

, Yn(x) := XT (x)+n(x) restricted to the event
{x ∈ | T < +∞} ⊂ is a homogeneous Markov chain whose transition
matrix is the transition matrix of

Xn

. By abuse of language, one says that the
chain ‘restarts’ at each stopping time.
Proof. By the Markov property and homogeneity, for each integer h we get
P

XT +n = j,T = h, XT = i

= P

Xh+n = j, T = h, Xh = i

= P

Xh+n = j
 Xh = i, T = h

P

T = h, Xh = i

= P

Xh+n = j
 Xh = i

P

T = h, Xh = i

= P

Xn = j
 X0 = i

P

T = h, Xh = i

.
Thus, summing with respect to h, we conclude
P

XT +n = j,T < +∞, XT = i

= P

Xn = j
 X0 = i
 ∞

h=0
P

T = h, XT = i

= P

Xn = j
 X0 = i

P

T < +∞, XT = i

.
5.2.5
Canonical Markov chains
Example 5.12 A typical example which may help intuition is that of random
walks. A person is at a random position k, k ∈Z, and at each step moves either

DISCRETE TIME MARKOV CHAINS
179
. . .
−1
0
1
2
p
p
p
p
p
q
q
q
q
q
. . .
Figure 5.2
A graph representing the transition matrix of an unbounded random
walk.
to the position k −1 or to the position k + 1 according to a Bernoulli trial of
parameter p, for example by ﬂipping a coin. Let Xn be the position occupied
at the nth step and let

ζn

be a sequence of independent random variables that
follow B(1, p) and such that ζn decides if the person moves backward or foward
at step n. Then
Xn+1 = Xn + 2ζn −1
so that

Xn

is a Markov chain, see Theorem 5.13, whose transition matrix is
P =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
p
0
0
0
0
. . .
. . .
0
p
0
0
0
. . .
. . .
q
0
p
0
0
. . .
. . .
0
q
0
p
0
. . .
. . .
0
0
q
0
p
. . .
. . .
0
0
0
q
0
. . .
. . .
0
0
0
0
q
. . .
. . .
. . .
. . .
. . .
. . .
. . .
. . .
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
Matrix P is schematically represented in Figure 5.2.
Example 5.12 is indeed a standard way to construct Markov chains. The
following holds.
Theorem 5.13 Let S be a ﬁnite or denumerable set, and let X0 :  →S be a ran-
dom variable on (, E, P). Let

ξn

, ξn :  →RN be a sequence of independent,
identically distributed random variables on (, E, P) that are also independent
of X0 and let f : S × RN →S be a Borel function. Then the sequence

Xn

,
Xn :  →S deﬁned by
Xn+1(x) := f (Xn(x), ξn(x)),
∀n ≥0,
(5.17)
is a homogeneous Markov chain with state-space S and transition matrix
Pi
j := P(f (i, ξk) = j)
∀i, j ∈S.
Proof. From the deﬁnition, it is clear that for any integer k, the random
variable Xk is a function of X0 and ξ1, . . . , ξk−1. Consequently, for any integer n,
the random variable ξn, which is by deﬁnition independent of (X0, ξ1, . . . , ξn−1),

180
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
is also independent of (X0, . . . , Xn). Therefore,
P

Xn+1 = j
 Xn = i, . . . , X0 = i0

= P

f (i, ξn) = j
 Xn = i, . . . , X0 = i0

= P(f (i, ξn) = j);
(5.18)
the last equality holds since the random variable (X0, . . . , Xn) is independent of
ξn, hence of f (i, ξn). Since the right-hand side of (5.18) is independent of the
values of Xn−1, . . . , X0, we conclude that
P

Xn+1 = j
 Xn = i, . . . , X0 = i0

= P

Xn+1 = j
 Xn = i

,
i.e.

Xn

is a Markov chain. The transition matrix is then P = (Pi
j), Pi
j :=
P(f (i, ξn) = j) and is independent of n since the random variables ξn are iden-
tically distributed.
Theorem 5.14 Let S be a ﬁnite or denumerable set, and let P be a stochastic
matrix. Let X0 :  →S be a random variable on (, E, P) and let

ξn

, ξn :
 →[0, 1] be a sequence of independent, uniformly distributed random variables
that are also independent of X0. Deﬁne
f (i, s) := min

j

j

h=1
Pi
h ≥s
>
∀i > 0, s ∈R.
Then the sequence

Xn

, Xn :  →S deﬁned by
Xn+1(x) = f (Xn(x), ξn(x)),
x ∈, n ≥0,
is a homogeneous Markov chain with state-space S and transition matrix P.
Proof. By Theorem 5.13, the sequence

Xn

is a Markov chain. Moreover,
f (i, ξn(x)) = j if and only if
j−1

h=1
Pi
h < ξn(x) ≤
j

h=1
Pi
h
hence
P(f (i, ξn(x)) = j) =
j

h=1
Pi
h −
j−1

h=1
Pi
h = Pi
j.
The previous theorem constructs in fact a homogeneous Markov chain

Xn

with
a given transition matrix and a given initial data X0. In particular, Theorem 5.14
reduces the problem of the existence of a Markov chain with a given transition
matrix P on a probability space (, E, P) and a given initial data X0 to the
existence of a sequence of independent random variables that are uniformly
distributed on [0, 1], see Sections 2.2.5 and 4.4.4.

DISCRETE TIME MARKOV CHAINS
181
5.2.6
Exercises
Exercise 5.15 Let S be a ﬁnite or denumerable set and assume X and Y are
random variables taking values in S. Show that X and Y are independent if and
only if ∀(i, j) ∈S × S if P(X = i) > 0 then the probability P(Y = j | X = i)
does not depend on i.
Exercise 5.16 Let {P(n)} be the sequence of transition matrices of a Markov
chain

Xn

with values in a ﬁnite or denumerable set S. For i, j ∈S and n, k ≥0
compute P

Xn+k = j
 Xk = i

.
Solution. Applying (5.11) we get
P

Xk+2 = j
 Xk = i

=

h∈S
P

Xk+2 = j
 Xk+1 = h

P

Xk+1 = h
 Xk = i

= (P(k + 1)P(k + 2))i
j.
Thus, with an induction argument, we have
P

Xn+k = j
 Xk = i

=

P(k + 1) · · · P(k + n)
i
j.
For a homogeneous Markov chain, P(n) := P ∀n, so that
P

Xn+k = j
 Xk = i

= (Pn)i
j.
(5.19)
Exercise 5.17 Let

Xn

be a sequence of random variables with values in a ﬁnite
or denumerable set S. Prove the following.
(i) If the random variables Xn are independent, then

Xn

is a Markov chain.
(ii) If

Xn

is a Markov chain and for each integer n, the random variables Xn
and Xn+1 are independent, then all the Xn’s are independent.
Exercise 5.18 Assume you need a certain piece of machinery for your work. Every
time you switch on this machinery it may either be broken or start working. At
the beginning of each working day, you switch it on. Assume the following:
(i) At the ﬁrst switch on, the probability that the machinery works is a, and the
probability that it is broken is b = 1 −a.
(ii) At each of the subsequent times it is switched on:
• If the machinery is broken, then it will be ﬁxed for the next day with
probability q.

182
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
• If the machinery works, then it is going to break down without being
repaired on time for the next day with probability p.
Describe the process.
Solution. Let Xn denote the state of the machinery at the nth switch on. The
state-space has only two points (working or broken), say S = {0, 1}: Xn = 0 if the
machinery is broken, Xn = 1 otherwise. The process is evidently a homogeneous
Markov chain: what happens at each switch on depends only on the previous
switch on. The process is then described by the initial condition,
P(X0 = 0) = 1 −a,
P(X0 = 1) = a
and by the transition matrix
P =

1 −q
q
p
1 −p

.
(5.20)
Matrix P is schematically represented in Figure 5.3.
The probabilities that the machinery is either broken or not on the nth day
are the components of the vector π(n) = (P(Xn = 0), P(Xn = 1)) and

π(n + 1) = π(n) P
∀n ≥0,
π(0) = (b, a).
i.e. π(n) = π(0)Pn.
Let us compute Pn. If p = q = 0, then obviously
Pn =

1
0
0
1

∀n.
If either p > 0 or q > 0, then P has two different eigenvalues. One can compute,
or prove by induction, that
Pn =
1
p + q

p
q
p
q

+ (1 −p −q)n

q
−q
−p
p

.
(5.21)
0
1
p
q
1 – q
1 – p
Figure 5.3
A graph representing the transition matrix in (5.20).

DISCRETE TIME MARKOV CHAINS
183
In particular, the probability that the machinery is working at the nth switch on,
knowing that it was working at the ﬁrst switch on is
P(Xn = 1 | X0 = 1

= π(n)1 =
1
p + q

(ap + bq) + (1 −p −q)n(ap −bq)

=
1
p + q

q + (1 −p −q)n(ap −bq)

.
Let us compute the steady state or the asymptotic behaviour of the chain, i.e.
the limit of π(0)Pn as n →∞.
• If (p, q) = (0, 0), then Pn = Id ∀n, so that the probabilities that the
machinery is either broken or working are the same at every switch on.
• If (p, q) ̸= (0, 0) and (p, q) ̸= (1, 1), then p + q < 2, hence |1 −p −
q| < 1, thus, by (5.21) Pn converges to the matrix
W :=
1
p + q

p
q
p
q

.
The asymptotic probabilities are π(0)W =

p
p+q ,
q
p+q

which are indepen-
dent of the initial probabilities a, 1 −a.
• If p = q = 1, then P =

0
1
1
0

and
Pn =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
?
1
0
0
1
@
if n is even,
?
0
1
1
0
@
if n is odd.
In this case the limit of Pn as n →∞does not exist.
Exercise 5.19 (Random walks) Let us go back to Example 5.12 and let us assume
that the set of all possible positions is ﬁnite, S = {0, . . . , N}. We must specify how
to proceed when Xn = 0 or Xn = N. Typical examples are an absorbing walk as
in Figure 5.4, a reﬂecting walk as in Figure 5.5 or a cyclic walk as in Figure 5.6.
For each of the ﬁgures, write the corresponding transition matrix.
1
2
3
4
1
p
p
q
q
1
Figure 5.4
A graph representing the transition matrix of an absorbing Markov
chain.

184
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
1
2
3
4
1
p
p
q
q
1
Figure 5.5
A graph representing the transition matrix of a reﬂecting chain.
1
2
3
4
1
p
p
q
q
1
Figure 5.6
A graph representing the transition matrix of a cyclic chain.
Solution. For the random walk in Figure 5.4, which is absorbing at the right
end, the transition matrix is
P =
⎛
⎜⎜⎜⎜⎜⎝
0
1
0
0
. . .
0
0
0
q
0
p
0
. . .
0
0
0
0
q
0
p
. . .
0
0
0
. . .
. . .
. . .
. . .
. . .
. . .
. . .
0
0
0
0
. . .
q
0
p
0
0
0
0
. . .
0
0
1
⎞
⎟⎟⎟⎟⎟⎠
.
For the reﬂecting random walk in Figure 5.5 the transition matrix is
P =
⎛
⎜⎜⎜⎜⎜⎝
0
1
0
0
. . .
0
0
0
q
0
p
0
. . .
0
0
0
0
q
0
p
. . .
0
0
0
. . .
. . .
. . .
. . .
. . .
. . .
. . .
0
0
0
0
. . .
q
0
p
0
0
0
0
. . .
0
1
0
⎞
⎟⎟⎟⎟⎟⎠
.
Finally, for the cyclic random walk in Figure 5.6 the transition matrix is
P =
⎛
⎜⎜⎜⎜⎜⎝
0
1
0
0
. . .
0
0
0
q
0
p
0
. . .
0
0
0
0
q
0
p
. . .
0
0
0
. . .
. . .
. . .
. . .
. . .
. . .
. . .
0
0
0
0
. . .
q
0
p
1
0
0
0
. . .
0
0
0
⎞
⎟⎟⎟⎟⎟⎠
.

DISCRETE TIME MARKOV CHAINS
185
Exercise 5.20 (Ehrenfest diffusion model) Assume that c molecules, c ≥1, are
distributed in two boxes A and B. At each step a molecule is randomly chosen
(with probability 1/c for each molecule) and moved from its box to the other one.
Describe the process.
Solution. The state of the system is described by the number of molecules in
the box A. If, at a certain step, A contains i molecules, then at the next step we
pick a molecule from A with probability i/c or from B with probability 1 −i/c
and we move it to the other box. Therefore, A will contain either i −1 molecules
with probability i/c or i + 1 molecules with probability 1 −i/c. Since the state
at each step depends only on the state at the previous step, the process is a
Markov chain, see Figure 5.7. The transition matrix is the same at each step and
is given by the (c + 1) × (c + 1) matrix
P =
⎛
⎜⎜⎜⎜⎜⎜⎝
0
1
0
0
. . .
0
0
1
c
0
1 −1
c
0
. . .
0
0
0
2
c
0
1 −2
c
. . .
0
0
. . .
. . .
. . .
. . .
. . .
. . .
. . .
0
0
0
0
. . .
0
1
c
0
0
0
0
. . .
1
0
⎞
⎟⎟⎟⎟⎟⎟⎠
.
Exercise 5.21 A repair workshop receives Zn objects to be repaired and repairs
one object per unit of time. At each time n, let Xn be the number of broken objects
which are in the workshop. Describe the process

Xn

.
Solution. Clearly, Xn+1 = max(Xn −1, 0) + Zn+1 ∀n ≥0. If the random
variables

Zn

are independent, identically distributed and also independent of
X0, then

Xn

is a homogeneous Markov chain, see Theorem 5.13. Let us com-
pute the transition matrix. If i = 0 (no object to be repaired is in the workshop)
P0
j = P

X1 = j
 X0 = 0

= P

Z1 = j
 X0 = 0

= P(Z1 = j).
If i = 1, then
P1
j = P

X1 = j
 X0 = 1

= P

Z1 = j
 X0 = 1

= P(Z1 = j)
1
2
3
4
5
1
1 –
1
c
1
2
c
3
c
1
c
1
c
1 – 2
c
Figure 5.7
A graph representing the transition matrix of the Ehrenfest diffusion
model with c = 4.

186
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
and, if i > 1, then
P1
j = P

X1 = j
 X0 = i

= P

Z1 = j −i + 1
 X0 = i

= P(Z1 = j −i + 1).
Therefore, setting aj := P(Z1 = j) the transition matrix of

Xn

is
P =
⎛
⎜⎜⎜⎜⎜⎝
a0
a1
a2
a3
. . .
a0
a1
a2
a3
. . .
0
a0
a1
a2
. . .
0
0
a0
a1
. . .
. . .
. . .
. . .
. . .
. . .
⎞
⎟⎟⎟⎟⎟⎠
.
Exercise 5.22 (Queues) In a shop customers arrive at the pay desk at times
0, 1, . . . . Let ξn be the number of customers arriving at the desk at time n. At
each time one customer (if there is at least one) pays and leaves the shop. Let

Xn

be the number of customers queuing at the pay desk at time n. Describe the
process.
Solution. Clearly,
Xn+1 =

Xn + ξn+1 −1
if Xn ≥1,
ξn
if Xn = 0.
or, equivalently, Xn := max(Xn −1, 0) + ξn+1. It is the same process as
described in Exercise 5.21.
Exercise 5.23 (Inventory) A logistic company manages the stock of a certain
good with the following strategy: the company ﬁxes two quantities m and M,
m < M; every day the company delivers the orders and at the end of the day the
people in charge check the stock. If, after delivering all the pending orders, the
stock is more than m they do not do anything. Otherwise, they buy immediately
enough supplies to fulﬁl all the pending orders and get the stock back to level M.
Describe the process, assuming that the initial stock amount is less than M.
Solution. For every n ≥0, let Zn be the aggregated demand of goods at day
n and let Xn be the current stock at the end of day n. Then Zn ≥0, X0 ≤M
and, clearly,
Xn+1 =

Xn −Zn+1
if Xn > m,
M −Zn+1
if Xn ≤m,
for every n ≥0. In particular, Xn ≤M ∀n. Assuming that the aggregated
demands Zn are independent, identically distributed random variables and also
independent of the initial stock X0, then

Xn

is a homogeneous Markov

DISCRETE TIME MARKOV CHAINS
187
chain, see Theorem 5.13. Let us compute the transition matrix of the

Xn

.
If −∞< i ≤m, then for every −∞< j ≤M
Pi
j = P

X1 = j
 X0 = i

= P

Z1 = M −j
 X0 = i) = P(Z1 = M −j)
while, if m < i ≤M, then for every −∞< j ≤M
Pi
j = P

X1 = j
 X0 = i

= P

Z1 = i −j
 X0 = i) = P(Z1 = i −j).
5.3
Some characteristic parameters
In this section, we compute a few characteristic parameters of homogeneous
Markov chains.
5.3.1
Steps for a ﬁrst visit
Let

Xn

be a homogeneous Markov chain on (, E, P) with a ﬁnite or denu-
merable state-space S and let C ⊂S. We recall that one says that x ∈ visits C
at step n if Xn(x) ∈C. We also say that x ∈ visits C if Xn(x) ∈C for some
n ≥1.
Consider the minimum number of steps to visit C,
tC(x) := min

n ≥1
 Xn(x) ∈C

,
called the ﬁrst passage time or the waiting time to get to C. We set tC(x) = +∞
if Xn(x) /∈C ∀n. Trivially, the waiting time tC(x) is a random variable with
values in {1, 2, . . . } ∪{+∞}; its distribution is obtained by computing its mass
distribution, that is the probabilities P(tC(x) = k) that the waiting time to get to
C occurs at step k for every k.
For any i ∈S we introduce the probabilities of visiting C at step k (starting)
from i, or ﬁrst passage time probabilities
f (k)
iC : = P

tC(x) = k
 X0 = i

= P

Xk ∈C, Xk−1 /∈C, . . . , X1 /∈C
 X0 = i

.
(5.22)
Thus, the total probability law gives
P(tC = k) =

i∈S
f (k)
iC P(X0 = i).
Since the sets Ek :=

x ∈ | tC(x) = k

are pairwise disjoint, the probability of
visiting C (at least once) starting from i is
fiC := P

tC < +∞
 X0 = i

=
∞

k=1
P

tC = k
 X0 = i

=
∞

k=1
f (k)
iC .
(5.23)

188
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
If C is a singleton, C = {j}, we write tj(x), f (k)
ij , fij instead of t{j}(x), f (k)
i{j}
and fi{j}, respectively. If i = j, fjj and f (k)
jj
are more appropriately called the
return probability to state j and the return probability to state j at step k,
respectively. The return probabilities f (k)
jj
play a crucial role in what follows.
Proposition 5.24 The ﬁrst passage time probabilities f (k)
iC , k ≥1, satisfy the recur-
rence property
f (k)
iC =
⎧
⎪⎪⎨
⎪⎪⎩

ℓ∈C
piℓ
if k = 1,

ℓ/∈C
piℓf (k−1)
ℓC
if k ≥2.
(5.24)
Proof. Let us prove the claim by induction on k. For k = 1, observe that f (1)
iC
is the probabilty to visit C at the ﬁrst step starting from i,
f (1)
iC =

ℓ∈C
P

X1 = ℓ
 X0 = i

=

ℓ∈C
piℓ.
Let k ≥2, and set
Ek
C :=

x ∈
 X1(x) /∈C, . . . , Xk−1(x) /∈C, Xk(x) ∈C

so that Ek
C =

x | tC(x) = k

. If a path x from i visits C at step k, then the ﬁrst
step is outside C, X1(x) /∈C, the next k −2 steps are again outside C and ﬁnally
the path hits C at the kth step. Set
E :=

x ∈
 X2(x) /∈C, . . . , Xk−1(x) /∈C, Xk(x) ∈C

,
Fℓ:=

x ∈
 X1(x) = ℓ

, ℓ/∈C.
Of course, Ek
C = E ∩(∪ℓ/∈CFℓ) and, by (5.11), we get
f (k)
iC = P

Ek
C
 X0 = i

= P

E ∩(∪ℓ/∈CFℓ)
 X0 = i

=

ℓ/∈C
P(E | Fℓ)P(Fℓ| X0 = i).
(5.25)
Since P

Fℓ
 X0 = i

= P

X1 = ℓ
 X0 = i

= piℓand
P

E
 Fℓ

= P

X2 /∈C, . . . , Xk−1 /∈C, Xk ∈C
 X1 = ℓ

= P

X1 /∈C, . . . , Xk−2 /∈C, Xk−1 ∈C
 X0 = ℓ

= f (k−1)
ℓC
by the inductive assumption, (5.24) follows from (5.25).

DISCRETE TIME MARKOV CHAINS
189
5.3.2
Probability of (at least) r visits
Let

Xn

be a homogeneous Markov chain on (, E, P) with a ﬁnite or denu-
merable state-space S. For n ≥1 and C ⊂S, consider the event En := {x ∈
 | Xn(x) ∈C} and let 1En be its characteristic function. Since 1En(x) = 1 if x
visits C at step n and is zero otherwise, the random variables
V (n)
C (x) :=
n

k=1
1Ek(x)
and
VC(x) :=
∞

k=1
1Ek(x)
(5.26)
give the number of visits of x in C in the ﬁrst n steps and the total number of
visits of x in C, respectively. Clearly, we may have VC(x) = +∞. Moreover,

x ∈
 V (n)
C (x) ≥1

=

x ∈
 tC ≤n

,

x ∈
 VC(x) ≥1

=

x ∈
 tC < +∞

.
(5.27)
Similarly, for each non-negative integer r the rth passage time in C is deﬁned
recursively by setting T (0)
C (x) = 0, T (1)
C (x) = tC(x) and, for r ≥2,
T (r)
C (x) = inf

n ≥T (r−1)
C
(x) + 1
 Xn(x) ∈C

,
x ∈.
If no n ≥T (r−1)
C
+ 1 such that Xn ∈C exists, we set T (r)
C (x) = +∞. The ran-
dom variables T (r)(x) are stopping times since, for each positive integer k, the
event {x | T (r)
C
= k} is detected by the random variables X0, . . . , Xk. The relations
between the random variables V (n)
C
and T (r)
C
and between the random variables
VC and TC are quite clear:

x ∈
 V (n)
C (x) ≥r

=

x ∈
 T (r)
C
≤n

,

x ∈
 VC(x) ≥r

=

x ∈
 T (r)
C
< +∞

.
If C is a singleton, C = {j}, we use the shorter notation T (r)
j
, Tj, V (n)
j
, Vj
instead of T (r)
{j} , T{j}, V (n)
{j} , V{j}, respectively. Finally, we point out that
V (n)
C (x) =

j∈C
V (n)
j
(x)
and
VC(x) =

j∈C
Vj(x).
Let

Xn

be a Markov chain on (, E, P) with a ﬁnite or denumerable
state-space S. For any i, j ∈S and r ≥1, we look for the probabilities of visiting
j at least r times starting from i:
P

Vj ≥r
 X0 = i

.

190
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Theorem 5.25 For any positive integer r and for any pair of states i, j ∈S we
have
P

Vj ≥r
 X0 = i

= fij f r−1
jj
.
In particular, the probability of having at least r visits in j starting from j is
P

Vj ≥r
 X0 = j

= f r
jj.
(5.28)
Proof. If r = 1, then (5.23) and (5.27) yield
P

Vj ≥1
 X0 = i

= P

tj < +∞
 X0 = i

= fij.
Assume now r ≥2. Let En :=

x ∈ | Xn(x) = j

and, for h ≥1, let
Rh(x) := ∞
k=h+1 1Ek(x) so that

x ∈
 Vj(x) ≥r

=
∞
(
h=1

x ∈
 Rh(x) ≥r −1, tj(x) = h

∀r ≥2.
Set
E :=

x ∈ | Rh(x) ≥2

,
F =

x ∈ | tj(x) = h

and
G = {x ∈ |
X0 = i}. From the disintegration formula, the renewal property (5.15) and the
homogeneity of the chain, we obtain
P

Rh ≥r −1, tj = h
 X0 = i

= P

E ∩F
 G

= P

E
 F ∩G

P

F ∩G

= P

Rh ≥r −1
 tj = h, X0 = i

P

tj = h
 X0 = i

= P

Rh ≥r −1
 Xh = j

P

tj = h
 X0 = i

= P

Vj ≥r −1
 X0 = j

f (h)
ij .
Thus,
P

Vj ≥r
 X0 = i

=
∞

h=1
P

Rh ≥r −1, tj = h
 X0 = i

=
 ∞

h=1
f h
ij

P

Vj ≥r −1
 X0 = j

= fij P

Vj ≥r −1
 X0 = j

.
Then the claim follows by an induction argument.

DISCRETE TIME MARKOV CHAINS
191
5.3.3
Recurrent and transient states
Let

Xn

be a homogeneous Markov chain on (, E, P) with a ﬁnite or denu-
merable state-space S. For any i, j ∈S, we now compute the expected number
of visits in the state j starting from the state i, i.e. the averaged number of visits
with respect to the probability measure Pi(A) := P(A | X0 = i), i.e.
Ei[Vj] :=
1
P(X0 = i)
*
{X0=i}
Vj(x) P(dx).
Theorem 5.26 The following hold:
Ei[Vj] =
⎧
⎨
⎩
+∞
if P

Vj = +∞
 X0 = i

> 0,
∞
k=0 kP

Vj = k
 X0 = i

otherwise.
(5.29)
Ei[Vj] =
∞

n=0
p(n)
ij .
(5.30)
Ei[Vj] =
⎧
⎨
⎩
+∞
if fjj = 1,
fij
1−fjj
if fjj < 1.
(5.31)
In particular,
∞

n=0
p(n)
ij =
⎧
⎪⎨
⎪⎩
+∞
if fjj = 1,
fij
1 −fjj
if fjj < 1.
(5.32)
Proof. The Beppo Levi theorem, see Exercise B.33, gives
*
{X0=i}
Vj(x)P(dx) =
⎧
⎨
⎩
+∞
if P

Vj = +∞, X0 = i

> 0,
∞
k=0 kP

Vj = k, X0 = i

otherwise,
hence (5.29) is proven. Taking advantage of (5.26) and (5.19), the Beppo Levi
theorem also gives
Ei[Vj] := Ei
4 ∞

n=0
1En
5
=
∞

n=0
Ei[1En]
=
∞

n=0
P

Xn = j
 X0 = i

=
∞

n=0
p(n)
ij .

192
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Finally, the Cavalieri formula yields
Ei[Vj] =
1
P(X0 = i)
∞

k=0
P

Vj > k, X0 = i

=
∞

k=0
P

Vj > k
 X0 = i

=
∞

k=1
P

Vj ≥k
 X0 = i

=
∞

k=1
fijf k−1
jj
=

+∞
if fjj = 1,
fij
1−fjj
if fjj < 1.
Deﬁnition 5.27 Let

Xn

be a homogeneous Markov chain with a ﬁnite or denu-
merable state-space S. Let P = (pij) be its transition matrix:
(i) j ∈S is called a recurrent state if ∞
n=1 p(n)
jj = +∞,
(ii) j ∈S is called a transient state if ∞
n=1 p(n)
jj < +∞.
Thus, a state can be either transient or recurrent. These terms are justiﬁed by the
following propositions.
Proposition 5.28 The following are equivalent:
(i) j is a recurrent state.
(ii) Paths from j visit j almost surely, fjj = 1.
(iii) Paths from j visit j an inﬁnite number of times almost surely.
If j is recurrent, paths from i visit j an inﬁnite number of times with probability
fij, that is with the same probability of visiting j at least once.
Proof. (i) and (ii) are equivalent by (5.30) and (5.31). Moreover, (iii) implies
(ii) and, if (ii) holds, i.e. fjj = 1, then
P

Vj = +∞
 X0 = i

= lim
r→∞P

Vj ≥r
 X0 = j

= lim
r→∞fijf r−1
jj
= fij.
In particular, choosing i = j, we get P

Vj = +∞
 X0 = j

= fjj = 1, i.e. (iii).
Proposition 5.29 The following are equivalent:
(i) j is a transient state.
(ii) The probability that paths from j visit j is less than 1, fjj < 1.
(iii) The probability that paths from j visit j an inﬁnite number of times is zero.
(iv) Paths from j visit j at most a ﬁnite number of times almost surely.

DISCRETE TIME MARKOV CHAINS
193
If j is transient, paths from i visit j an inﬁnite number of times with zero
probability.
Proof. (i) and (ii) are equivalent by (5.30) and (5.31). Moreover,
P

Vj = +∞
 X0 = i

= lim
r→∞P

Vj ≥r
 X0 = i

= lim
r→∞fijf r−1
jj
.
Thus, P(Vj = +∞| X0 = j) = 0 if and only if fjj < 1, so that (ii) and (iii) are
equivalent, and P(Vj = +∞| X0 = i) = 0 if j is transient. Finally, the equiva-
lence between (iii) and (iv) is obvious.
Proposition 5.30 Let

Xn

be a homogeneous Markov chain with a ﬁnite or
denumerable state-space S. Let P = (pij) be its transition matrix. Then the fol-
lowing hold:
(i) If i →j and j is recurrent, then ∞
k=1 p(k)
ij = +∞. Therefore, if fjj = 1
and i →j, then fij = 1.
(ii) If i and j communicate, i ↔j, then either both i and j are transient or
both i and j are recurrent. That is, if i ↔j, then fii = 1 if and only if
fjj = 1.
Proof. (i) If j is recurrrent and i →j, then there exists k such that p(k)
ij > 0.
Thus, for each n ≥0
p(k+n)
ij
=

α∈S
p(k)
iα p(n)
αj ≥p(k)
ij p(n)
jj .
In particular, ∞
n=1 p(n)
ij = +∞if ∞
n=1 p(n)
jj = +∞.
(ii) If i ↔j, then there exist h, k such that p(h)
ij > 0 and p(k)
ji > 0. Thus, for each
n ≥0
p(h+k+n)
ii
=

α,β∈S
p(h)
iα p(n)
αβ p(k)
βi ≥p(h)
ij p(n)
jj p(k)
ji .
In particular, ∞
n=1 p(n)
ii
= +∞if ∞
n=1 p(n)
jj = +∞. Since the roles of i and j
can be interchanged, either both series converge or both series diverge.
5.3.4
Mean ﬁrst passage time
Let

Xn

be a homogeneous Markov chain with a ﬁnite or denumerable state-
space S and let C ⊂S. The mean ﬁrst passage time, or expected waiting time,
to get to C starting from i ∈S is the expected number of steps to get to C
starting from the state i, i.e. the averaged waiting time tC(x) with respect to the
probability measure Pi(A) := P(A | X0 = i),
Ei[tC] :=
1
P(X0 = i)
*
{X0=i}
tC(x) P(dx).

194
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
If C is a singleton, C = {j}, then Ej[tC] is brieﬂy denoted by T jj,
T jj := Ej[tj]
and is called the expected return time to j.
The Beppo Levi theorem, see B.33, yields
Ei[tC] =
⎧
⎨
⎩
+∞
if P

tC = ∞
 X0 = i

> 0,
∞
k=1 kP

tC = k
 X0 = i

otherwise.
(5.33)
Since the probability of visiting C starting from i is
P

tC < +∞
 X0 = i

=
∞

k=1
P

tC = k
 X0 = i

= fiC,
see (5.22), we have
Ei[tC] :=

+∞
if fiC < 1,
∞
k=1 kf (k)
iC
if fiC = 1.
(5.34)
In other words, the expected time to get to C is inﬁnite if and only if either
fiC < 1 or fiC = 1 and ∞
k=1 kf (k)
iC = +∞. In particular,
T jj =

+∞
if fjj < 1,
∞
k=1 kf (k)
jj
if fjj = 1.
(5.35)
Summarizing, the average return time T jj in j:
• is inﬁnite either if j is transient, fjj < 1, or if j is recurrent, fjj = 1,
and ∞
k=1 kf (k)
jj = ∞;
• is ﬁnite if and only if j is recurrent, fjj = 1, and ∞
k=1 kf (k)
jj < ∞.
If the expected return time of state j is ﬁnite, then the state j is called
positive recurrent. We shall see later, see Theorem 5.71 and 5.82, that the states
of a Markov chain with ﬁnite state-space whose transition matrix is irreducible
are positive recurrent.
Proposition 5.31 Let C ⊂S and i ∈S. Then
Ei[tC] = 1 +

ℓ/∈C
piℓEℓ[tC].
(5.36)
Proof. It sufﬁces to assume Ei[tC] < +∞and, consequently, fiC = 1. Recur-
rence equation (5.24) gives
Ei[tC] = f (1)
iC +
∞

k=2
kf (k)
iC = f (1)
iC +
∞

k=1
(k + 1)
 
ℓ/∈C
piℓf (k)
ℓC


DISCRETE TIME MARKOV CHAINS
195
= f (1)
iC +
∞

k=1

ℓ/∈C
piℓf (k)
ℓC +

ℓ/∈C
piℓ
 ∞

k=1
kf (k)
ℓC

= f (1)
iC +
∞

k=2
f (k)
iC +

ℓ/∈C
piℓEℓ[tC]
= fiC +

ℓ/∈C
piℓEℓ[tC].
Since fiC = 1, the claim is proven.
5.3.5
Hitting time and hitting probabilities
Let

Xn

be a homogeneous Markov chain on a probability space (, E, P) with
a ﬁnite or denumerate state-space S, and let P = (pij) be its transition matrix.
For each C ⊂S deﬁne
τC(x) := min

n ≥0 | Xn(x) ∈C

where, of course, we set τC(x) = +∞if Xn(x) /∈C for any n ≥0. The random
variable τC(x) is called the hitting time or ﬁrst time to absorption in C. Obviously,
τC(x) = 0 if and only if x ∈C, and τC(x) = tC(x) if and only if X0(x) /∈C. The
mass density of τC with respect to Pi is the sequence

h(k)
iC

where
h(k)
iC := P

τC(x) = k
 X0 = i

is the probability that state i hits the set C at time k. Obviously,
h(0)
iC =

1
if i ∈C,
0
if i /∈C,
(5.37)
and, for each k ≥1,
h(k)
iC =

0
if i ∈C,
f (k)
iC
if i /∈C.
(5.38)
Starting from the mass density

h(k)
iC

of τC we can compute the probability that
state i hits C,
hiC := P

τC < ∞
 X0 = i

=
∞

k=0
h(k)
iC
and the expected hitting time in C
Ei[τC] :=
1
P(X0 = i)
*
{X0=i}
τC(x) P(dx).

196
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Thus, (5.37) and (5.38) give
hiC =
1
if i ∈C,
fiC
if i /∈C,
(5.39)
see (5.70), and
Ei[τC] =
∞
if hiC < 1,
∞
k=1 kh(k)
iC
if hiC = 1.
(5.40)
In particular, see (5.34),
Ei[τC] =
0
if i ∈C,
Ei[tC]
if i /∈C.
In the next two theorems we characterize the hitting probabilities and the
expected hitting times.
Theorem 5.32 The vector (hiC)i∈S of hitting probabilities is the minimal non-
negative solution x = (xi)i∈S of the system
xi =
⎧
⎪⎨
⎪⎩
1
if i ∈C,

ℓ∈S
piℓxℓ
if i /∈C.
(5.41)
That is, (hiC) is a solution of (5.41) and, if (xi)i∈S is any further non-negative
solution of (5.41), then xi ≥hiC ∀i ∈S.
Proof. (i) We ﬁrst show that (hiC)i∈S is a solution of system (5.41). If i ∈C,
then τC(x) = 0 ∀x ∈, so that h(0)
iC = 1 and hiC = 1. If i /∈C, then h(k)
iC = f (k)
iC ,
hence by (5.24), we get
hiC =
∞

k=1
h(k)
iC = h(1)
iC +
∞

k=2

ℓ/∈C
piℓh(k−1)
ℓC
=

ℓ∈C
piℓ+

ℓ/∈C
piℓhℓC
=

ℓ∈S
piℓhℓC.
This proves that (hiC)i∈S is a solution to system (5.41).
(ii) Assume (xi)i∈S is a solution to (5.41). Trivially, xi = 1 = hiC if i ∈C. We
now prove by an induction argument that
xi ≥
n

k=0
h(k)
iC
∀n ∈N, ∀i /∈C.
(5.42)

DISCRETE TIME MARKOV CHAINS
197
Since xℓ= 1 ∀ℓ∈C and (xi)i∈S is non-negative, we get
xi =

ℓ∈C
piℓ+

ℓ/∈C
piℓxℓ≥h(1)
iC ,
i.e. the claim holds true for n = 1.
Assume n > 1. By (5.41), the induction argument and since xi ≥0 ∀i, we
get
xi =

ℓ∈C
piℓ+

ℓ/∈C
piℓxℓ≥h(1)
iC +

ℓ/∈C
piℓxℓ≥h(1)
iC +

ℓ/∈C
piℓ

n

k=1
h(k)
iC

= h(1)
iC +
n

k=1
 
ℓ/∈C
piℓh(k)
ℓC

= h(1)
iC +
n+1

k=2
h(k)
iC =
n+1

k=1
h(k)
iC ,
i.e. (5.42) is proven. If we now let n go to ∞in (5.42), we get xi ≥∞
k=0 h(k)
iC =
hiC.
Theorem 5.33 The vector (Ei[τC])i∈S of the expected hitting times in C is a
solution of the system
yi =

0
if i ∈C,
1 + 
ℓ/∈C piℓyℓ
if i /∈C,
(5.43)
where we accept yi = +∞. If hiC = 1 and (yi)i∈S is any further solution of (5.43),
then yi ≥Ei[τC].
Proof. We ﬁrst show that Ei[τC] solves (5.43). If i ∈C, then Ei[τC] = 0. If
i /∈C, then Ei[τC] = Ei[tC], hence Ei[τC] solves (5.43) by Proposition 5.31.
Assume (yi)i∈S is a solution of (5.43). If i ∈C, then yi = 0 = Ei[τC]. If
i /∈C and hiC = 1, we show by induction that
yi ≥
n

k=1
kh(k)
iC
(5.44)
so that, as n goes to inﬁnity, we get yi ≥∞
k=1 kh(k)
iC = Ei[τC], see (5.34).
If n = 1, then (5.44) holds trivially: yi ≥1 = hiC ≥h(1)
iC . Let n ̸= 2. From
the induction argument and (5.24) we infer
yi = 1 +

ℓ/∈C
piℓyℓ≥1 +

ℓ/∈C
piℓ

n

k=1
kh(k)
ℓC

= 1 +
n

k=1
k
 
ℓ/∈C
piℓh(k)
ℓC

= 1 +
n

k=1
kh(k+1)
iC
= 1 +
n

k=2
(k −1)h(k)
iC = 1 −
n

k=2
h(k)
iC +
n+1

k=2
kh(k)
iC
≥h(1)
iC +
n+1

k=2
kh(k)
iC =
n+1

k=1
kh(k)
iC .

198
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
5.3.6
Exercises
For the reader’s convenience the main deﬁnitions on discrete time Markov chains
are summarized in Figure 5.8.
Exercise 5.34 (Holding time) Let

Xn

be a homogeneous Markov chain with a
ﬁnite or denumerable state-space S and transition matrix P = (pij). Compute the
probability distribution of the holding time in a state. We regard the paths starting
from j as staying at j at least once. Compute also the expected holding time in
a state.
0
Ej[SJj] =
1
1−pjj
1
Solution. The holding time at j ∈S is the random variable deﬁned as
SJj(x) = min

n
 Xn(x) ̸= j

(SJj(x) = +∞if Xn(x) = j ∀n). For k ≥1, set
Ek :=

x ∈
 X1(x) = j, . . . , Xk−1(x) = j, Xk(x) ̸= j

.
We want to compute the holding time probabilities P

SJj = k
 X0 = j

=
P

Ek
 X0 = j

. If k = 1, then
P

SJj = 1
 X0 = j

= P

X1 ̸= j
 X0 = j

= 1 −pjj.
If k ≥2, (5.8) implies
P

SJj = k
 X0 = j

= P

Ek
 X0 = j

= P

Xk ̸= j
 Xk−1 = j
 k−1
=
h=1
P

Xh = j
 Xh−1 = j

= (1 −pjj)pk−1
jj .
Thus, the holding time in state j follows the geometric distribution of parameter
1 −pjj, i.e. it is a memoryless random variable.
Integrating SJj(x) one gets the expected holding time in state j starting
from j:
Ej[SJj] =
∞

k=1
kP

SJj = k
 X0 = j

= (1 −pjj)
∞

k=1
kpk−1
jj
= (1 −pjj)
1
(1 −pjj)2 =
1
1 −pjj
.
Exercise 5.35 (Gambler’s ruin paradox) A player possesses a capital i ∈N,
i ≥1 and plays a sequence of games. In each game, he either wins or loses

DISCRETE TIME MARKOV CHAINS
199
Characteristic parameters of discrete Markov chains
Let

Xn

be a homogeneous Markov chain with a ﬁnite or denumerable state-
space S and let i, j ∈S and C ⊂S.
• Conditional probability given the event

X0 = i

Pi(A) := P(A | X0 = i).
• Conditional expectation given the event

X0 = i

Ei[f ] :=
1
P(X0 = i)
*
{X0=i}
f (x) P(dx).
• Waiting time to get to C
tC(x) := min

n ≥1 | Xn ∈C

,
tj(x) if C = {j}.
• Probability to get to C for the ﬁrst time at step k starting from i
f (k)
iC := P

tC = k
 X0 = i

,
f (k)
ij
if C = {j}.
• Probability to get to C starting from i
fiC := P

tC < +∞
 X0 = i

,
fij if C = {j}.
• Number of visits at j in the ﬁrst n steps
V (n)
C (x),
V (n)
j
(x) if C = {j}.
• Expected number of visits at j starting from i Ei[Vj].
• Expected waiting time to get to C from i
Ei[tC],
Ei[tj] if C = {j}.
• Expected return time to j T jj := Ej[tj].
Figure 5.8
Some characteristic parameters for a homogeneous Markov chain
with ﬁnite or denumerable state-space.
1 with probabilities p and q := 1 −p, respectively. Compute the probability of
losing the capital and the expected number of matches to be played in order to
lose all the capital.
Solution. Let Xn be the capital available before the nth play. The game is
a one-dimensional random walk where we move forwards (the capital increases

200
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
0
1
2
3
. . .
p
p
p
q
q
q
q
1
Figure 5.9
A graph representing the transition matrix P in Exercise 5.35.
of 1 unit) with probability p and backwards with probability q := 1 −p. The
sequence

Xn

is a homogeneous Markov chain with transition matrix
P =
⎛
⎜⎜⎜⎝
1
0
0
0
. . .
0
. . .
q
0
p
0
. . .
0
. . .
0
q
0
p
. . .
0
. . .
...
...
...
...
...
...
...
We want to compute the probability fi0 that, starting with a capital i the player
hits 0, hi0 = P(τ0 < ∞| X0 = 1). If i = 0 then trivially hi0 = 1. If i ̸= 0, then
hi0 = fi0. Since 0 is recurrent and i →j, see Figure 5.9, then fi0 = 1. So,
whatever the initial capital and the seeming fortune of the game are, the player
will almost surely end up with no money left.
Let us now compute the expected number of matches the player has to play
before getting ruined. We denote by ti such expected time, when the player starts
with capital i, i.e. ti := hi0. Thus, see Theorem 5.33, the vector (ti) is the minimal
non-negative solution of the following system

t0 = 0,
ti = 1 + qti−1 + pti+1
i > 0.
(5.45)
• If p ̸= q, then the solutions of the system (5.45) are the one parameter
family of sequences (ti), ti = 1 −a + a

q
p
i
+
i
q−p.
• If q < p, then ti diverges to −∞as i →+∞, i.e. system (5.45) admits
no non-negative solution. Hence ti = +∞∀i.
• If p < q, then the minimal non-negative solution is ti =
i
q−p ∀i.
• If p = q, then the solutions of system (5.45) are the sequences (ti), ti =
a + bi −i2, a, b ∈R. There is no non-negative ﬁnite solution, so that ti =
+∞∀i.
Summarizing, the expected number of matches the gambler has to play, start-
ing with capital i is
ti =
⎧
⎨
⎩
i
q −p
if q > p,
+∞
if q ≤p.

DISCRETE TIME MARKOV CHAINS
201
5.4
Finite stochastic matrices
In this section we go back to ﬁnite stochastic matrices deﬁned in Section 5.1
presenting their canonical representation, a very useful tool for describing the
powers of the matrix itself.
5.4.1
Canonical representation
5.4.1.1
Minimal absorbing classes
Let P be a N × N stochastic matrix. For notational convenience, for every n ∈N,
we write p(n)
ij
instead of (Pn)i
j and pij := p(1)
ij = Pi
j. Of course p(0)
ij = δij.
Deﬁnition 5.36 A nonempty subset C of {1, . . . , N} is called a closed class or an
absorbing class for P if for any i ∈C and for any j /∈C, j is not accessible from
i, i↛j; equivalently, C is a closed class if and only if i ∈C and i →j imply
j ∈C.
In other words, C is a closed class if there is no path starting from any point
of C and arriving to points outside C. Notice that the points of C are not required
to be pairwise connected.
The simplest way to enumerate the closed classes of {1, . . . , N} is to consider
for any i ∈{1, . . . , N} the set of nodes that are accessible from i

j ∈{1, . . . , N}
 i →j

,
and which can be computed, e.g. via the matrix B in (5.2), see (5.3).
Trivially, the whole set of indexes is a closed class for P. Moreover, it is
easy to show that if C and D are closed classes, then so is C ∪D, and so is also
C ∩D provided it is nonempty. Thus, closed classes are partially ordered with
respect to inclusion, and minimal closed classes C1, . . . , Cr, r ≥1 exist since P
is ﬁnite; each Ci is a closed class such that if D is a closed class and D ⊂Ci,
then D = Ci. In particular, minimal closed classes are pairwise disjoint. Minimal
closed classes can be found in the set of all closed classes. Using the partial order
with respect to inclusion, write the tree diagram of the closed classes starting
from the maximal closed class {1, . . . , N}; then the leaves of the tree are the
closed minimal classes.
Minimal closed classes can be also detected by the following proposition.
Proposition 5.37 A closed class is minimal if and only if all its elements are
pairwise communicating.
Proof. Let C be a closed class whose elements are pairwise comunicating.
Assume, by contradiction, that C is not minimal: then there exists a minimal
closed class D ⊂C, D ̸= C, and each element of D comunicates with at least
one element in C \ D ⊂Dc, a contradiction.

202
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Conversely, assume C is a minimal closed class. Let j ∈C and deﬁne
Dj :=

i
 i↛j

.
(5.46)
We want to prove that Dj = ∅. Clearly, j /∈Dj, hence Dj is a proper subset
of C. We show that if Dj ̸= ∅, then Dj is a closed class, a contradiction since
Dj ⊂C, Dj ̸= C and C is minimal.
Assume Dj ̸= ∅. In order to prove that Dj is a closed class, we only need
to show that i↛k for i ∈Dj and k /∈Dj. We distinguish between two cases:
• if k /∈C, then i↛k since C is a closed class;
• if k ∈C \ Dj, then k →j by the deﬁnition of Dj. If i →k, then by
transitivity i →j, a contradiction since i ∈Dj.
Minimal closed classes for P can be also detected via the matrix B in (5.2). In
fact, i ↔j if and only if Bi
j > 0 and Bj
i > 0, cf. (5.3), so that the following holds.
Proposition 5.38 The index i ∈{1, . . . , N} belongs to a minimal closed class if
and only if for any j such that Bi
j > 0 then also Bj
i > 0.
Proof. In fact, by Proposition 5.37, a closed class C =

j | Bi
j > 0

containing
i is minimal if and only if

j
 Bi
j > 0

=

j
 Bi
j > 0, Bj
i > 0

.
We then summarize the above in the following theorem.
Theorem
5.39
Canonical
representation
Let
P ∈MN,N
be
stochastic,
S = {1, . . . , N}. Let C1, . . . , Cr be the minimal closed classes for P and for
i = 1, . . . .r denote by ki the cardinality of Ci. Finally, let T := {1, . . . , N} \
(C1 ∪· · · ∪Cr). Up to a reordering of rows and columns, a stochastic matrix P
has the canonical form
P =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
B1
0
0
· · ·
0
0
0
B2
0
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
Br
0
R
V
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
(5.47)
where B1, B2, . . . ; , Br are irreducible stochastic matrices of dimension k1 × k1,
. . . , kr × kr, respectively, see Proposition 5.37.

DISCRETE TIME MARKOV CHAINS
203
5.4.2
States classiﬁcation
Following the Markov chains terminology, we call states the indexes in {1, . . . ,
N}. Given a N × N stochastic matrix P = (pij), we recall that a state j ∈
{1, . . . , N} is recurrent if ∞
k=1 p(k)
jj = +∞and transient if ∞
k=1 p(k)
jj < +∞,
cf. Deﬁnition 5.27.
Proposition 5.40 Let P be a N × N stochastic matrix, S = {1, . . . , N}, and let
C1, . . . , Cr be the minimal closed classes in S. Set
C := C1 ∪· · · ∪Cr
and
T := {1, . . . , N} \ C
and let i, j ∈{1, . . . , N}. We have the following:
(i) If j ∈C, then j is recurrent. Therefore, if i →j, then ∞
k=1 p(k)
ij = +∞.
(ii) If j ∈T then j is transient. More precisely, for all i, j ∈T , p(k)
ij converges
to 0 exponentially fast as k →+∞, hence ∞
k=1 p(k)
ij < +∞.
Proof. (i) Let j ∈Cs for some s. Let us prove that j is recurrent.
Since p(n)
jh = 0 for any h /∈Cs
and any n ≥0, 
h∈Cs p(n)
jh = 1, hence

h∈Cs
∞
n=0 p(n)
jh = ∞
n=0

h∈Cs p(n)
jh = +∞. In particular, there exists j0 ∈Cs
such that ∞
n=1 p(n)
jj0 = +∞. Since j0 ↔j, there exists k such that p(k)
j0j > 0, so
that from
p(n+k)
jj
=

α
p(n)
jα p(k)
αj ≥p(n)
jj0p(k)
j0j
∀n
we infer ∞
n=1 p(n)
jj = +∞, i.e. j is recurrent. Now, if i →j ∈C, then
∞
k=1 p(k)
ij = +∞by (i) of Proposition 5.30.
(ii) Let i ∈T . The set
D :=

j ∈{1, . . . , N}
 i →j

is a closed class, so that D ∩C ̸= ∅. Thus, setting
p(n)
iC :=

j∈C
p(n)
ij ,
there exists k = k(i) such that p(k)
iC > 0. We claim that the sequence

p(k)
iC

is
monotone nondecreasing with respect to k. In fact, for any α ∈C since pαj = 0
∀j /∈C, we have 
j∈C pαj = 1. Therefore,
p(k+1)
iC
=

j∈C
N

α=1
p(k)
iα pαj =
N

α=1
p(k)
iα
 
j∈C
pαj

≥

α∈C
p(k)
iα
 
j∈C
pαj

= p(k)
iC .

204
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Thus, for any i ∈T p(k)
iC ≥p(k)
iC > 0 if k ≥k. Since T is ﬁnite, there exist p0 > 0
and k0 such that p(k)
iC ≥p0 for any k ≥k0 and every i ∈T . In particular,
p(k0)
iT
=

j∈T
p(k0)
ij
= 1 −p(k0)
iC
≤1 −p0.
The equality p(k)
αj = 0 if α ∈C, j /∈C also implies
p(2k0)
iT
=

j∈T
p(2k0)
ij
=

j∈T
N

α=1
pk0
iαpk0
αj =

α∈T
pk0
iα

j∈T
pk0
αj ≤(1 −p0)2
and, moreover, by an induction argument,
p(qk0)
iT
≤(1 −p0)q
for any positive integer q.
Divide k by k0, k = qk0 + r, 0 ≤r < k0. Since the map k →p(k)
iT = 1 −p(k)
iC is
monotone decreasing, we ﬁnally get
p(k)
iT ≤p(qk0)
iT
≤(1 −p0)q ≤(1 −p0)k/k0−1,
and
∞

k=1
p(k)
iT ≤k0 + k0
∞

q=1
p(qk0)
iT
≤k0 + k0
∞

q=1
(1 −p0)q = k0
p0
.
We summarize Proposition 5.40 as follows. Let C1, . . . , Cr be the minimal
closed classes in {1, . . . , N} for P and, for i = 1, . . . .r, let ki be the cardinality
of Ci. As we have seen in Proposition 5.37, up to a relabelling of the states, we
may assume that P has the canonical form (5.47). Consequently, for each integer
n ≥1,
Pn =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
Bn
1
0
0
· · ·
0
0
0
Bn
2
0
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
Bn
r
0
Rn
Vn
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠

DISCRETE TIME MARKOV CHAINS
205
so that, by (i) and (ii) of Proposition 5.40, setting for each i ∈T and j /∈T
(R∞)i
j =

+∞
if i →j,
0
otherwise
we have
∞

n=0
Pn =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
+∞
0
0
· · ·
0
0
0
+∞
0
· · ·
0
0
...
...
...
...
...
...
0
0
0
· · ·
+∞
0
R∞
∞
n=0 Vn
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(5.48)
Finally, we point out that the matrix ∞
n=0 Vn can be easily computed starting
from (5.47). Consider the identity
(Id −V)(Id + V + · · · + Vn) = Id −Vn+1.
(5.49)
Since Vn converges to the zero matrix exponentially fast as n goes to inﬁn-
ity, see (ii) of Proposition 5.40, the series ∞
n=0 Vn converges. Passing to the
limit as n →∞, (5.40) yields (Id −V) ∞
n=0 Vn = Id. In other words, Id −V
is invertible and
∞

n=0
Vn = (Id −V)−1.
(5.50)
5.4.3
Exercises
Exercise 5.41 Prove the following:
(i) The matrix P =

1
0
0
1

has two minimal closed classes and no transient
state.
(ii) The matrix P :=

0
1
1/2
1/2

is irreducible.
(iii) The matrix P :=

1
0
1/2
1/2

has one minimal closed class, {1}, and 2 is
a transient state.
(iv) The matrix P :=

0
1
1
0

is irreducible.

206
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Exercise 5.42 Find the canonical form of the stochastic matrix
P =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
0
0
0
0
0
0.5
0
0.5
0
0
0.3
0
0.7
0
0
0
0
0
0
0
0
0
0
0.5
0
0
0.5
0
0
0
0
0
1
0
0
0
0
0
0
0
0.1
0.1
0.1
0.1
0
0
0
0
0.6
0.1
0
0
0
0.9
0
0
0
0
0
0.2
0
0
0
0
0
0
0
0.8
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0.6
0
0.4
0
0
0
0
0
0
0
0
0
0
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
Exercise 5.43 Let P = (pij) be a N × N irreducible stochastic matrix and let
C ⊂{1, . . . , N}, C ̸= ∅. Show that there exist h ∈C and k /∈C such that phk > 0.
Solution. Without loss of generality, assume C = {1, . . . , s}. If phk = 0 ∀h ∈
C and ∀k /∈C, then P has the form
P =
?
A
0
B
@
,
a contradiction, since P is irreducible.
5.5
Regular stochastic matrices
In this section we deal with the convergence of the sequence of the powers of
the transition matrix for a particular class of homogenenous Markov chains with
ﬁnite state-space S = {1, . . . , N}.
A fundamental difference exists between ﬁnite dimensional stochastic vectors
and inﬁnite dimensional ones. Let

x(n)
⊂[0, 1]∞be the sequence of stochastic
vectors deﬁned by x(n) =

x(n)
j

j, x(n)
j
:= δj,n. For each ﬁxed j, x(n)
j
→0 as
n →∞, i.e.

x(n)
converges to (0, 0, 0, 0, . . . ) componentwise. This shows
that, in general, a converging sequence of stochastic vectors does not converge
to a stochastic vector. On the other hand, the set of stochastic vectors in RN∗
T :=

x = (x1, . . . , xn) ∈RN  xi ≥0,
N

i=1
xi = 1
>
is a closed set: if

xn

⊂T and xn →x, then x ∈T .
5.5.1
Iterated maps
We now recall a few facts from the theory of iterated maps.

DISCRETE TIME MARKOV CHAINS
207
Let X be a metric space and let f : X →X be a continuous map from
X into itself. For every k ∈N, we denote by f k : X →X the kth iterate of
f , i.e. f k := f ◦f ◦· · · ◦f k times. For the sake of convenience we also set
f 0(x) = x. For every x ∈X, the sequence

f k(x)

k is called the path of x. We
say that p ∈X is a ﬁxed point for f if f (p) = p. Finally, if for every x ∈X
f k(x) →p we say that p is the sink of f .
Simple examples of the above can be drawn considering afﬁne maps
f : [0, 1] →[0, 1]. The following claims are easily proved, see Exercise 5.63.
Proposition 5.44 The following hold:
(i) If p is a ﬁxed point for f , then, for every k ≥0, f k(p) = p, i.e. p is a ﬁxed
point for f k.
(ii) Let n > 1 and let p be a ﬁxed point for f n. Then p, f (p), . . . , f n−1(p)
are ﬁxed points for f n, too. In particular, if f n has a unique ﬁxed point,
then p is also the unique ﬁxed point of f .
(iii) If for some x ∈X, f k(x) →p as k →∞, then p is a ﬁxed point for f .
(iv) If p is the sink of f , then p is the unique ﬁxed point of f .
(v) There exist maps f that have ﬁxed points but no sink.
(vi) Let n ≥1. A point p ∈X is the sink of f if and only if p is the sink of f n.
The existence of ﬁxed points can be granted under quite general assumptions.
We have the following.
Theorem 5.45 (Brouwer) Let X be a compact and convex set in RN and let
f : X →X be a continuous map. Then f has a ﬁxed point.
Proof. Let x ∈X ⊂RN. Consider the path {f n(x)} of x. For every n
wn :=
1
n + 1
n

k=0
f n(x)
is a convex combination of f 0(x), f 1(x), . . . , f n(x), hence wn ∈X since X is
convex. Since X is also compact, there exist w ∈X and a subsequence of

wn

,
that for convenience we still denote by

wn

, such that wn →w. Let us prove
that f (w) = w. For every n we have
f (wn) −wn
 =
1
n + 1

n

j=0

f j+1(x) −f j(x)

=
1
n + 1
f n+1(x) −x
 ≤diam(X)
n + 1 .
Passing to the limit as n →∞, continuity of f yields |f (w) −w| = 0, i.e.
f (w) = w.

208
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Existence of a sink for f requires different assumptions on f . Let us start
with a deﬁnition.
A contraction map, or simply a contraction, on X is a map f : X →X which
shrinks distances uniformly, i.e. a map for which there exists a constant L, 0 <
L < 1, such that |f (x) −f (y)| ≤L|x −y| ∀x, y ∈X. Of course, a contraction
map is continuous on X.
Theorem 5.46 (Banach ﬁxed point theorem) Let X be a complete metric space
with distance d and let f : X →X be a contraction with contraction factor
L < 1. Then f has an unique sink p ∈X. More precisely, for every x ∈X the
path {f n(x)} converges to p at least exponentially fast,
d(f n(x), p) ≤
Ln
1 −Ld(f (x), x)
∀n.
(5.51)
Proof. Uniqueness. If p, q ∈X are two ﬁxed points, then
d(p, q) = d(f (p), f (q)) ≤L d(p, q),
hence d(p, q) = 0, since L < 1.
Existence. For any x ∈X, let xk := f k(x), where, we recall, f k := f ◦· · · ◦f
k times. We then have
d(xk+1, xk) ≤Ld(xk, xk−1) ≤· · · ≤Lkd(x1, x0) = Lkd(f (x), x)
hence, for q ≥p ≥1,
d(xq, xp) ≤
q−1

k=p
d(xk+1, xk) ≤
q−1

k=p
Lk d(f (x), x) ≤
Lp
1 −L d(f (x), x)
(5.52)
since L < 1. The right-hand side of (5.52) converges to zero as p →∞, so that

xn

is a Cauchy sequence and thus converges to some p = p(x) ∈X which
is a ﬁxed point for f , see (iii) of Proposition 5.44. Since f has a unique ﬁxed
point, p(x) = p ∀x so that for every x ∈X, f k(x) →p, i.e. p is the sink of f .
Combining the Banach ﬁxed point theorem with (vi) of Proposition 5.44, we
have the following.
Corollary 5.47 Let X be a complete metric space and let f : X →X be contin-
uous. If, for some n ≥1, f n is a contraction with contraction factor L < 1, then
f has a unique sink p ∈X and for every x ∈X
d(f k(x)), p) ≤L
 
k
n
!
1 −L max
0≤j<n d(f n(xj), xj)
(5.53)

DISCRETE TIME MARKOV CHAINS
209
where for j = 0, . . . , n −1, we set xj := f j(x). Moreover, if X is bounded,
diam(X) < ∞, then
d(f k(x)), p) ≤diam(X)L
 
k
n
!
.
(5.54)
Proof. By the Banach ﬁxed point theorem, f n has a sink p and for every
x ∈X
d(f n(x), p) ≤
Ln
1 −Ld(f n(x), x).
Consequently, p is the sink of f by (vi) of Proposition 5.44. In order to prove
estimate (5.53), write every k as k = qn + j, 0 ≤j < n, q =
 k
n

. Then
d(f k(x), p) = d(f qn(f j(x)), p) ≤
Lq
1 −Ld(f n(xj), xj).
Taking the maximum of the right-hand sides for j = 0, . . . , n −1, we get (5.53).
Finally, with f n being a contraction and p being a ﬁxed point for f n, we have
d(f k(x)), p) = d(f qn(f j(x)), p) ≤L d(f (q−1)n(f j(x)), p)
≤· · · ≤Lq d(f j(x), p) ≤Lq diam(X).
5.5.2
Existence of ﬁxed points
Let T ⊂RN be the set of stochastic vectors and let P be a stochastic matrix,
so that the linear map P(x) := xP maps T into itself. Since T is convex and
compact, the Brouwer ﬁxed point theorem applies to the map P and we have the
following.
Theorem 5.48 (Perron–Frobenius) There exists at least a stochastic vector w
such that w := wP.
Example 5.49 Let P =

1
0
0
1

. Each vector of R2∗is a ﬁxed point for the
associated linear map P(x) = xP and P has no sink.
Let P =

0
1
1
0

, then the only ﬁxed point for the map P(x) := xP in T
is w := (1/2, 1/2). Moreover,
Pn =

Id
if n is even,
P
if n is odd,
so that, if x ∈T , then xPn converges to w if and only if x = w and the map
P(x) := xP has no sink.

210
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Example 5.49 shows that, in general, a map can have more than one ﬁxed point
and, in general, the sequence {xPn} does not converge.
Example 5.50 Let P =

1 −α
α
β
1 −β

with 0 < α, β < 1. The only ﬁxed point
in T for P(x) = xP is w :=
1
α+β (β, α). Moreover, computing Pn one can show
that w is a sink for P ,
xPn →
1
α + β (β, α)
∀x ∈T .
5.5.3
Regular stochastic matrices
Deﬁnition 5.51 A ﬁnite stochastic matrix P is said to be regular if there exists k0
such that all the entries of the matrix Pk0 are nonzero.
Trivially, a regular stochastic matrix is irreducible.
5.5.3.1
Convergence
We now prove that if P is regular then P(x) := xP has a unique sink w ∈T , that
is, xPn →w ∀x ∈T . In order to do that, we need to introduce some deﬁnitions.
For each x ∈RN∗, set
||x||1 :=
N

i=1
|xi|.
(5.55)
Thus, x →||x||1 is a norm in RN∗and for each stochastic vector x, ||x||1 = 1.
With an induction argument on the dimension N, it can be also easily proven
that
||x||2 ≤||x||1 ≤
√
N||x||2
∀x ∈RN∗,
(5.56)
where ||x||2 :=
6N
i=1 |xi|2 is the Euclidean norm of x.
Lemma 5.52 The space RN∗, equipped with the norm || ||1, is complete. Thus T ,
a closed subset of RN∗, is a complete metric space.
Proof. By inequality (5.56) a sequence in RN∗converges with respect to the
|| ||1 norm if and only if it converges with respect to the || ||2 norm. Similarly,
a sequence in RN∗is a Cauchy sequence with respect to the || ||1 norm if and
only if it is a Cauchy sequence with respect to the || ||2 norm. Thus, since RN∗
is complete with respect to the Euclidean norm, it is also complete with respect
to the || ||1 norm.
Proposition 5.53 Let P be a stochastic N × N matrix. Denote by R1, . . . , RN the
rows of P and let K ⊂RN∗be their convex envelope. Set
C := 1
2 max
i,j ||Ri −Rj||1.

DISCRETE TIME MARKOV CHAINS
211
Then:
(i) C ≤1.
(ii) If all the entries of P are nonzero, then C < 1.
(iii) diam1(K) := supx,y∈K ||x −y||1 = 2C.
(iv) Let P(x) := xP. Then ||P(x) −P(y)||1 ≤C||x −y||1 ∀x, y ∈T .
Proof. (i) Each row of P belongs to T , hence ||Ri −Rj||1 ≤||Ri||1 +
||Rj||1 = 2 ∀i, j ∈{1, . . . , N}.
(ii) Let x = (x1, . . . , xN) and y = (y1, . . . , yN) ∈T be such that ||x −y||1 =
2 = ||x||1 + ||y||1. Let A =

i ∈{1, . . . , N} | xi > yi

and B := {1, . . . , N} \ A.
We claim that both A and B are nonempty sets. Assume by contradiction that
A is empty, so that xi ≤yi ∀i. Since x and y are stochastic, then x = y,
hence ||x −y||1 = 0 ̸= 2, a contradiction. If B is empty, then xi > yi ∀i, so that
1 = ||x||1 = N
i=1 xi > N
i= yi = ||y||1 = 1, a contradiction.
We show that some component of both x and y is null. In fact,
N

i=1
xi +
N

i=1
yi = 2 = ||x −y||1 =

i∈A
(xi −yi) +

i∈B
(yi −xi)
i.e.

i∈A
yi +

i∈B
xi = 0.
In particular, yi = 0 ∀i ∈A and xi = 0 ∀i ∈B.
If each entry of P is nonzero, then, from the above, ||Ri −Rj||1 < 2 for any
i, j = 1, . . . , N, i.e. C < 1.
(iii) Let δ := diam1(K). By deﬁnition, 2C ≤δ. Let us show that δ ≤2C. Let
B(x, r) :=

y | ||y −x||1 ≤r

be the closed ball in RN∗with respect to the || ||1
norm centred at x and with radius r. By deﬁnition of C, Ri ∈B(Rj, 2C) for any
i, j ∈{1, . . . , N}. Since B(Rj, 2C) is convex, then K ⊂B(Rj, 2C) for any j, or,
equivalently, Rj ∈B(x, 2C) ∀x ∈K. Since B(x, 2C) is convex, K ⊂B(x, 2C)
∀x ∈K, i.e. ||x −y||1 ≤2C ∀x, y ∈K.
(iv) We show that, if a = (a1, . . . , aN) is such that N
i=1 ai = 0, then
||P(a)||1 ≤C ||a||1.
(5.57)
Without loss of generality, we may assume ||a||1 = 2. For any x ∈RN∗let
′
i xi = 
xi > 0 xi and ′′
i xi = 
xi<0 xi, so that ′
i ai = −′′
i ai = 1. Thus,
both the convex combinations x′ = ′
iaiRi and x′′ = −′′
i aiRi belong to K, so
that by (iii)
||P(a)||1 = ||

i
aiRi||1 = ||x′ −x′′||1 ≤2C = C ||a||1.

212
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Claim (iv) follows by choosing a = x −y in (5.57).
Theorem 5.54 Let P be a ﬁnite stochastic matrix. The following are equivalent:
(i) P is regular,
(ii) P is irreducible and the map P : T →T , P(x) = xP has a sink w ∈T ,
i.e. xPn →w ∀x ∈T .
(iii) ∃n such that ∀n ≥n all the entries of Pn are nonzero.
Moreover, in this case:
(a) w is the only ﬁxed point in T of the map x →xP.
(b) All the components of w are nonzero.
(c) Let R1, . . . , RN be the rows of Pk0. Then
C := 1
2
max
i,j=1,...,N ||Ri −Rj||1 < 1
and, for any x ∈T ,
||xPn −w||1 ≤2 C⌊n/k0⌋.
(5.58)
Proof. (i) ⇒(ii). If all the entries of Pk0 are positive, then all the states
1, . . . , N are pairwise communicating, i.e. P is irreducible. By Proposition 5.53
C < 1 so that P k0(x) := xPk0 is a contraction on T with contraction factor less
than or equal to C. Thus, applying the Banach ﬁxed point theorem, P k0 admits
a unique sink w. Since diam(T ) ≤2, Corollary 5.47 yields
||P n(x) −w||1 ≤2C
 
n
k0
!
.
Finally, the components of w = (w1, . . . , wn) are positive, see Exercise 5.65.
(ii) ⇒(iii). P n(x) = xPn converges to w for any x ∈T if and only if eiPn →w
for any i = 1, . . . , N, i.e. if and only if each row of Pn converges w. Since P is
irreducible, all the components of w are positive, see Exercise 5.65, so that all
the entries of Pn are positive for n large enough.
(iii) ⇒(i). Obvious.
Remark 5.55 If P denotes the transition matrix of a Markov chain, the con-
vergence of xPn to w ∀x ∈T reads as follows: for any j ∈{1, . . . , N} we
have
P(Xn = j) =
N

i=1
P(X0 = i) (Pn)i
j −→wj;
in other words, whatever the distribution of the initial random variable X0, the
mass distributions of the Xn’s converge to the stochastic vector w = (wj).

DISCRETE TIME MARKOV CHAINS
213
We may rewrite the claim of Theorem 5.54 in terms of matrices. With the
notation above, let w ∈T be the ﬁxed point of the map P(x) := xP and let W
be the N × N stochastic matrix whose rows coincide with w,
W := w(1, 1, . . . , 1)T .
Then:
(i) xPn →w ∀x ∈T if and only if Pn →W.
(ii) The exponential rate of convergence in (5.58) reads
||Pn −W|| ≤2 C⌊n/k0⌋.
where
||A|| =
max
i=1,...,N ||Ai||1.
(iii) wP = w if and only if WP = W.
Finally, since all the rows of W are the same stochastic vector w and W = WP,
the following equalities can be easily proven
W = W2 = WP = PW.
(5.59)
5.5.3.2
Eigenvalues
Proposition 5.56 Let P be a N × N stochastic matrix. Then 1 is an eigenvalue for
P with corresponding eigenvector v := (1, 1, . . . , 1)T . Moreover, for any eigen-
value λ ∈C of P we have |λ| ≤1.
Proof. Since P is stochastic, trivially, Pv = v.
Let λ ∈C be an eigenvalue of P and z = (z1, . . . , zN) ̸= 0 be an eigenvector
of λ, λz = Pz. Let i0 be such that |zi0| = maxi |zi| so that |zi0| ̸= 0 and
|λ| |zi0| = |(Pz)i0| =

N

j=1
Pi0
j zj ≤
N

i=1
Pi0
j |zj| ≤|zi0|
N

j=1
Pi0
j = |zi0|.
Therefore |λ| ≤1.
The next theorem characterizes the eigenvalues of regular stochastic matrices.
We omit the proof, which can be found, e.g. in [16] Proposition 9.2.
Theorem 5.57 Let P be a regular stochastic matrix. Then 1 is the only eigenvalue
of P of modulus 1. Its algebraic and geometric multiplicities are 1. Consequently,
all the eigenvalues of P but 1 have modulus strictly less than 1.
Since P is regular, one can easily compute the powers of P and their limit
as n →∞by Theorem 5.57. In fact, Theorem 5.57 implies that P has a Jordan

214
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
decomposition P = SJS−1 where the ﬁrst column of S is an eigenvector of P
relative to the eigenvalue 1, i.e. (1, 1, . . . , 1)T ,
J =
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
. . .
0
0
J1
0
. . .
0
0
0
J2
. . .
0
...
...
...
...
0
0
0
0
. . .
Jp
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
and J1, J2, . . . , Jp are Jordan blocks relative to the eigenvalues of P but 1.
Therefore, from P = SJS−1 we infer Pn = SJnS−1 where
Jn =
⎛
⎜⎜⎜⎜⎜⎜⎜⎝
1
0
0
. . .
0
0
Jn
1
0
. . .
0
0
0
Jn
2
. . .
0
...
...
...
...
0
0
0
0
. . .
Jn
r
⎞
⎟⎟⎟⎟⎟⎟⎟⎠
.
Since all the eigenvalues but 1 have modulus strictly smaller than 1, Jn con-
verges to
K :=
⎛
⎜⎜⎜⎝
1
0
. . .
0
0
0
. . .
0
...
...
...
...
0
0
. . .
0
⎞
⎟⎟⎟⎠
and, for each ϵ > 0 there exists a constant Cϵ such that
||Jn −K|| ≤Cϵ(λ∗+ ϵ)n
as n →∞
where
λ∗:= max

|λ|
 λeigenvalue of P, |λ| < 1

.
We then infer
(Pn)i
j →(SKS−1)i
j = Si
1(S−1)1
j = (1, 1, . . . , 1)T w,
(5.60)
where w = (w1, . . . , wN) is the row vector deﬁned by wj := (S−1)1
j ∀j. Denot-
ing by W the matrix with each row equal to w, we conclude from (5.60) that
Pn →W
with an exponential rate of convergence driven by λ∗: for any ϵ > 0 there exists
Cϵ such that
||Pn −W|| ≤Cϵ(λ∗+ ϵ)n.

DISCRETE TIME MARKOV CHAINS
215
Finally, observe that W is a stochastic matrix, being the limit of stochastic
matrices. Moreover, cf. Exercise 5.65, all its entries are positive.
5.5.3.3
Fixed point and expected return times
Let

Xn

be a homogeneous Markov chain with a ﬁnite or denumerable state-
space S and transition matrix P = (pij). Recall that for i, j ∈S, f (k)
ij
denotes the
probability that a path from i visits for the ﬁrst time j at step k.
Proposition 5.58 For any n ≥1 we have
p(n)
ij =
n

k=1
f (k)
ij p(n−k)
jj
.
(5.61)
Proof. Let k ∈{1, . . . , n} and let
Fk,n :=

x ∈
 X1(x) ̸= j, . . . , Xk−1(x) ̸= j, Xk(x) = j, Xn(x) = j

be the set of paths that visit state j for the ﬁrst time at step k and then return to
j again at time n. Let
E :=

x ∈
 Xn = j

,
F :=

x ∈
 Xk(x) = j, Xk−1(x) ̸= j, . . . , X1(x) ̸= j

,
G :=

x ∈
 X0 = i

.
By (5.8) and the homogeneity of the chain, we infer
P

Fk,n
 X0 = i

= P(E | F)P(F | G)
= P

Xn = j
 Xk = j

P

Xk = j, Xk−1 ̸= j, . . . , X1 ̸= j
 X0 = i

= p(n−k)
jj
f (k)
ij .
Since the sets

Fk,n

, k = 1, . . . , n, are a partition of the set

x ∈ | Xn = j

,
we get
p(n)
ij = P

Xn = j
 X0 = i

=
n

k=1
P

Fk,n
 X0 = i

=
n

k=1
f (k)
ij p(n−k)
jj
.
Lemma 5.59 Let

pn

and

fn

be two sequences of non-negative numbers such
that p0 = 1, ∞
i=1 fi = 1 and
pn =
n

j=1
fjpn−j
∀n ≥1.
(5.62)

216
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
If pn →a ∈R+ (ﬁnite or inﬁnite), then
a = 1
T
T :=
∞

k=1
kfk.
Proof. Summing equalities (5.62) from 1 to N, we get
N

n=1
pn =
N

n=1
n

j=1
fjpn−j =
N

n=1
 n−1

j=0
fn−jpj

,
(5.63)
or, equivalently,
pN +
N−1

j=1

1 −
N−j

i=1
fi

pj =
N

j=1
fj.
(5.64)
or, taking into account that ∞
j=1 fj = 1,
N

j=1
pj

∞

i=N−j+1
fi

=
N

j=1
fj.
(5.65)
For each j ≥1, let Tj := ∞
i=j fi. Then T1 = ∞
i=1 fi = 1, and
∞

j=1
Tj =
∞

j=1
∞

i=j
fi =
∞

i=1
i

j=1
fi =
∞

i=1
ifi = T.
Thus, (5.64) or (5.65) reads N
j=1 pjTN−j−1 = N
j=1 fj, i.e.
N

j=1
TjpN+1−j =
N

j=1
pjTN−j+1 =
N

j=1
fj.
(5.66)
Let

ϕN(j)

be the double indexed sequence deﬁned as
ϕN(j) :=

TjpN+1−j
if 1 ≤j ≤N,
0
if j > N.
Then equality (5.66) reads
∞

j=1
ϕN(j) =
N

j=1
fj.
(5.67)
Since, by assumption, for any j ∈S ϕN(j) →aTj as N →∞, the following
hold:

DISCRETE TIME MARKOV CHAINS
217
• If T = ∞
j=1 Tj = +∞, then the Fatou lemma, Lemma B.51, together with
(5.67), gives
a T = a
∞

j=1
Tj =
∞

j=1
lim
N→∞ϕN(j)
≤lim inf
N→∞
∞

j=1
ϕN(j) =
∞

i=1
fi = 1
so that a = 0.
• If T < +∞, then supN ϕN(j) ≤Tj ∀j, and ∞
j=1 Tj = T < +∞. Thus,
the dominated convergence theorem for series, see Example B.56, yields
a T = a
∞

j=1
Tj =
∞

j=1
lim
N→∞ϕN(j) = lim
N→∞
∞

j=1
ϕN(j) =
∞

i=1
fi = 1.
The proof is thus complete.
Lemma 5.60 Let

Xn

be a homogeneous Markov chain with ﬁnite or
denumerable state-space S and transition matrix P = (pij) and let j ∈S. If
p(n)
jj →λ, then
p(n)
ij →fijλ;
Similarly, if 1
n
n
k=1 p(k)
jj →λ, then 1
n
n
k=1 p(k)
ij →fijλ.
Proof. Equality (5.61) can be written as
p(n)
ij =
n

k=1
f (k)
ij p(n−k)
jj
=
∞

k=1
ϕn(k)
where
ϕn(k) := 1{1,...,n}(k)f (k)
ij p(n−k)
jj
.
Since for any ﬁxed k, ϕn(k) →f (k)
ij λ as n →∞, applying the dominated con-
vergence theorem for series, Example B.56, we get
lim
n→∞p(n)
ij = lim
n→∞
∞

k=1
ϕn(k) =
∞

k=1
lim
n→∞ϕn(k)
=
 ∞

k=1
f (k)
ij

λ = fijλ.
Similarly, one proves the last claim, writing this time
1
n
n

k=1
p(k)
ij =
∞

r=1
ψn(r)
where ψn(r) := 1{1,...,n}(r)f (r)
ij

1
n
n−r
k=1 p(k)
jj

.

218
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Theorem 5.61 Let

Xn

be a homogeneous Markov chain with ﬁnite or
denumerable state-space S and transition matrix P = (pij). For any i, j ∈S let
T jj ∈R and fij be the expected return time in state j and the probability of
visiting j starting from i, respectively. If p(n)
jj →wj, then
wj =
1
T jj
and
p(n)
ij →fijwj.
Notice that, if j is transient, then trivially wj = 0 = fijwj. Moreover, if j is
recurrent and i →j, then fij = 1 so that fijwj = wj.
Proof. In order to prove that wj :=
1
T jj , we distinguish two cases:
• Assume j is transient, i.e. fjj < 1. Thus, T jj = +∞by (5.34) and
∞
n=i p(n)
jj < +∞by (5.32). Hence, p(n)
jj →0 = 1/T jj = wj.
• Assume j is recurrent, fjj = 1. Then, by (5.34) T jj = ∞
j=1 kf (k)
jj
and,
by Proposition 5.58,
p(n)
jj =
n

k=1
f (k)
jj p(n−k)
jj
∀n ≥1.
The claim now follows applying Lemma 5.59 to the sequences

pn

and

fk

where pn := p(n)
jj and fk := f (k)
jj .
The last claim follows from Lemma 5.60.
5.5.4
Characteristic parameters
For the reader’s convenience we summarize the formulas that can be used to
compute the characteristic parameters introduced in Section 5.3 in terms of the
transition matrix P. Assuming P is reordered in a canonical form (5.47), then
(5.48) gives:
• the expected number of visits, see (5.30);
• the probability that a path from i visits j at least once, see (5.32);
• the probability of many passages in j starting from i, see (5.28).
5.5.4.1
Waiting time matrix
In Section 5.3.4 we have proven that if j is transient then the expected return time
T jj is inﬁnite, while, if j is recurrent, then T jj = ∞
k=1 kf (k)
jj , see formula (5.35).
We now deal with the waiting time matrix T = (T
i
j), where T
i
j := Ei[tj] is
the conditional expected ﬁrst passage time to j starting from i. We only consider

DISCRETE TIME MARKOV CHAINS
219
homogeneous Markov chains

Xn

with a ﬁnite state-space S so that we can take
advantage of the canonical decomposition of its transition matrix P and therefore
conﬁne ourselves to the case when P is irreducible.
We have already shown that, if P is regular, then P has a unique stochas-
tic ﬁxed point, w = (wj) ∈T with non-zero components, cf. Theorem 5.54.
Moreover, the expected return time to the state j is T jj = 1/wj, in particu-
lar, j is positive recurrent. This fact has already been proven if P is regular, see
Theorems 5.54 and 5.61, but the same holds if P is irreducible, see Theorem 5.71
and Remark 5.82. In other words, the computation of the diagonal entries of T,
i.e. of the expected return times, boils down to the computation of the unique
solution w ∈T of the equation w = wP.
In order to write an explicit formula for all the entries of T, let us introduce
a few notations. For any square matrix A = (aij), let Ad := (aijδij). Also let E
be the square matrix whose entries are all equal to 1. Clearly, (ABd)d = AdBd
for any A and B, and EAd is such that all its rows are equal to the row vector
of the diagonal elements of A. Finally, let W be the matrix whose rows agree
with the stochastic vector w ∈T such that w = wP. Then
Td = ((1/Wi
j)δij) = (δij/wj).
Proposition 5.62 (Expected ﬁrst passage time matrix) The matrix
Id −(P −W)
is invertible. Denoting Z := (Id −(P −W))−1, we have
T = (Id −Z + EZd)Td,
(5.68)
i.e. Ti
j = (δi
j −Zi
j + Zj
j) 1
wj ∀i, j ∈S.
Proof. Since W = W2 = PW = WP, cf. (5.59), we have (P −W)2 = P2 −
2PW + W2 = P2 −W. Moreover, it can be proved (either with an induction
argument or using Newton binomial theorem) that
(P −W)k = Pk −W
∀k.
Since Pn →W, the previous formula yields (P −W)k →0. Therefore, one can
pass to the limit as n →∞in the identity
(Id −(P −W))
n

k=0
(P −W)k = Id −(P −W)n+1
to get both the invertibility of Id −(P −W) and a formula for its inverse Z,
Z =
∞

k=0
(P −W)k = Id +
∞

k=1
(Pk −W).

220
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
We now prove (5.68) in three steps.
Step 1. From Proposition 5.31 we infer
Ei[tj] = 1 +

ℓ̸=j
piℓEℓ[tj],
i.e.
T = E + P(T −Td).
Therefore, T is a solution of the system

Xd = Td,
X = P(X −Td) + E.
(5.69)
Step 2. Let M be the right-hand side of (5.68),
M := (Id −Z + EZd)Td.
We claim that the matrix M satisﬁes system (5.69). In fact, Md = Td since all
the diagonal entries of (Id −Z + EZd) are equal to 1. In order to prove that M
satisﬁes also the second equation in (5.69), let us point out the followings:
(i) Id −Z = W −PZ: to prove it, multiply on the right by Z both sides
of the equality Z−1 = Id −P + W and recall that WZ−1 = W, i.e
W = WZ.
(ii) WTd = E, which follows from the deﬁnition of W.
(iii) PEZd = EZd which easily follows from the identities N
j=1 pij = 1
∀i ∈S.
Thus we have
P(M −Td) + E = P(−Z + EZd)Td + E = (−PZ + EZd)Td + E
= M −WTd + E = M.
Step 3. We now show that system (5.69) has a unique solution R, so that T = M
as claimed. Let M1 and M2 be two different solutions to system (5.69) and let
R := M1 −M2. Then, the diagonal entries of R are zero and R = PR. Let r be
a column of R so that r = Pr. Consequently, r = Pkr for any integer k and,
passing to the limit as k →∞, we get Wr = r. Since the rows of W are equal,
then so are the components of r, and since the diagonal of R is null, then all the
components of r are null. Hence r is the null vector, and since r is an arbitrary
column of R, we conclude that R = 0, i.e. M1 = M2.
5.5.5
Exercises
Exercise 5.63 Prove Proposition 5.44.

DISCRETE TIME MARKOV CHAINS
221
Solution. (i) Trivial.
(ii) If f n(p) = p, then for every j ≥1 f n(f j(p)) = f j(f n(p)) = f j(p), i.e.
f j(p) is a ﬁxed point for f n. If f n has a unique ﬁxed point p, then p = f (p),
i.e. p is a ﬁxed point for f , actually the unique ﬁxed point by (i).
(iii) If f k(x) →p, then f k+1(x) →p and f k+1(x) = f (f k(x)) →f (p) by
continuity, hence f (p) = p.
(iv) Let p1 be another ﬁxed point for f . Then f k(p1) = p1 for every k, i.e.
f k(p1) is constant and converges to p, hence p = p1.
(v) Consider f (x) = −x, x ∈[−1, 1]. Then 0 is a ﬁxed point but for every x ̸= 0
f k(x) = (−1)kx.
(vi) If f k(x) →p, then trivially (f n)k(x) = f kn(x) →p. Conversely, if p is
the sink of f n, then p is the unique ﬁxed point of f n by (iv) and consequently
the unique ﬁxed point of f by (ii). Moreover, for every j ≥0, f hn+j(x) =
f j(f hn(x)) →f j(p) = p as h →∞. Writing every k as k = hn + j, 0 ≤j <
n, we conclude that f k(x) →p as k →+∞.
Exercise 5.64 Let P = (pij) be a N × N irreducible stochastic matrix. If there
exists h ∈{1, . . . , N} such that phh > 0, then P is regular.
Solution. P is irreducible, hence for every α, β ∈{1, . . . , N} there exists n =
n(α, β) such that p(n(α,β))
αβ
> 0. Let k0 := 2 maxα,β n(α, β) + 1. Then, for any
i, j ∈{1, . . . , N}, we get
p(k0)
ij
≥p(n(i,h))
ih
p(q)
hh p(n(h,j))
hj
≥p(n(i,h))
ih
(phh)qp(n(h,j))
hj
> 0
where q := k0 −n(i, h) −n(h, j) ≥1.
Exercise 5.65
Let P = (pij) be an irreducible N × N stochastic matrix and
let j ∈{1, . . . , N}. Show that, if 1
n
n
k=1 p(k)
jj converges to λ (in particular, if
p(n)
jj →λ), then λ > 0.
Solution. Assume, by contradiction, that 1
n
n
k=1 p(k)
jj →0 converges to zero.
Since P is irreducible, for any i ∈{1, . . . , N} there exists k0 = k0(i) such that
p(k0)
ij
> 0. Thus p(k)
ji p(k0)
ij
≤p(k+k0)
jj
and
lim
n→∞
1
n
n

k=1
p(k)
ji

p(k0)
ij
≤lim
n→∞
1
n
n

k=1
p(k+k0)
jj

= 0.
Therefore, 1
n
n
k=1 p(k)
ji →0 ∀i, a contradiction since N
i=1 p(k)
ji = 1.
Exercise 5.66 Let

Xn

be a homogeneous Markov chain with a ﬁnite state-
space S and a regular transition matrix P = (pij). For j ∈S, let T jj ∈R be the

222
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
expected return time to j. Show that p(n)
ij →
1
T jj ∀i, j ∈S. Conclude that each
state is positive recurrent, T jj < +∞∀j ∈S.
Solution. Since P is regular, P has a unique sink w = (wj), i.e. p(n)
ij →wj,
∀i, j ∈S, and, moreover, wj > 0 ∀j, cf. Theorem 5.54. Finally, Theorem 5.61
yields wj =
1
T jj .
Exercise 5.67 Prove the following: let A be a N × N complex matrix. For any
i = 1, . . . N set xi := Ai
i, ri := 
j̸=i |Ai
j| and let Bi be the closed ball Bi :=

z ∈C | |z −xi| ≤ri

. Then the eigenvalues of A are contained in ,
i=1,...,N Bi.
Solution. Let λ be an eigenvalue of A and let z = (z1, . . . , zN) be an eigen-
vector of λ, Az = λz. Let h = h(λ) be such that |zh| = maxi |zi|. Then
|λ −xh| |zh| = |(Az)h −Ah
hzh| = |

j̸=h
Ah
jzj|
≤
 
j̸=h
|Ah
j|

max
i
|zi| ≤rh|zh|.
Therefore |λ −xh| ≤rh, i.e. λ ∈Bh.
5.6
Ergodic property
Let

Xn

be a homogeneous Markov chain whose state-space S is either ﬁnite
or denumerable. Assume its transition matrix P is irreducible. Let T jj be the
expected return time to state j and let V (n)
j
(x) be the number of visits in j
made by the path x in the ﬁrst n steps. Roughly speaking, the ergodic theorem
says that in the long run the frequency of the visits in state j converges almost
surely to
1
T jj ,
V (n)
j
(x)
n
−→
1
T jj
P-almost surely.
5.6.1
Number of steps between consecutive visits
Let

Xn

be a homogeneous Markov chain on a probability space (, E, P)
with a ﬁnite or denumerable state-space S and let j ∈S. Closely related to
the sequence of passage times

T (r)
j

r in j are the random variables

S(r)
j (x)

r = 1, . . . , where S(1)
j
= T (1)
j
= tj and, for r ≥2,
S(r)
j (x) :=

T (r)
j
(x) −T (r−1)
j
(x)
if T (r−1)
j
(x) < +∞,
0
otherwise.
S(r)
j (x) counts the number of steps between the (r −1)th visit and the rth visit
in j.

DISCRETE TIME MARKOV CHAINS
223
Theorem
5.68
Let
j ∈S
be
a
recurrent
state
and
let
i →j.
Set
i :=

x | X0(x) = i

. Assume P(i) > 0 and let Pi(A) := P(A | X0 = i). Then:
(i) S(r)
j (x) ≥1 for almost any x ∈i and for all r ≥1.
(ii)

S(r)
j

r is a sequence of independent random variables with respect to Pi.
(iii) The variables S(r)
j , r ≥2, are identically distributed with respect to the
probability measure Pi and, for every n ≥1 and r ≥2,
Pi(S(r)
j
= n) = f (n)
jj
and
Ei[S(r)
j ] = T jj.
Proof. (i) Since j is recurrent and i →j, paths from i visit j inﬁnitely many
times almost surely, see Proposition 5.30. Therefore, for any r ≥2, T (r)
j (x) <
+∞for a.e. x ∈i. In particular, 1 ≤S(r)
j (x) < +∞for almost every x ∈i.
(ii) Let r ≥2, T := T (r−1)
j
, h1, . . . , hr−1 ≥1, h = (h1, . . . , hr−1) ∈Nr−1,
r ≥1 and set
Hh :=

S(r−1)
j
= hr−1, S(r−2)
j
= hr−2, . . . , S(1)
j
= h1

.
Observe that:
• The events

x ∈i | S(r)
j (x) ≥1

are detected by the random variables
XT +1, XT +2, . . . .
• The event Hh is generated by the random variables X0, . . . , Xk where
k := h1 + · · · + hr−1 ≤T , and Hh ⊂

Xk = j

.
• T is a stopping time and T (x) < ∞for a.e. x ∈i, that is, Pi-almost
surely.
Then the strong Markov property, Theorem 5.11, gives
Pi

S(r)
j
= n
 Hh

= Pi

S(r)
j
= n


Xk = j

∩Hh ∩{T < ∞}

= Pi

S(r)
j
= n
 T < ∞, Xk = j

= P

S(r)
j
= n
 T < +∞, Xk = j

= P

S(1)
j
= n
 X0 = j

= f (n)
jj .
(5.70)
Therefore we conclude that the events detected by S(r)
j
are independent of h,
that is, independent of the events detected by S(r−1)
j
, . . . , S(1)
j , if Pi(Hh) > 0.
Thus, S(r)
j
is independent of S(r−1)
j
, . . . , S(1)
j , and, since r is arbitrary,

S(r)
j

is
a sequence of independent random variables with respect to Pi.

224
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
(iii) From (5.70), we get for r ≥2 and n ≥1
Pi(S(r)
j
= n) =

h
Pi

S(r)
j
= n
 Hh

Pi(Hh) = f (n)
jj

h∈Rr−1
Pi(Hh) = f (n)
jj ,
hence
Ei[S(r)
j ] =
∞

k=0
kPi(S(r)
j
= k) =
∞

k=1
kf (k)
jj = T jj.
5.6.2
Ergodic theorem
Let

Xn

be a homogeneous Markov chain on (, E, P) with a ﬁnite or denu-
merable state-space S and let j ∈S. Set
wj :=
1
T jj
where
T jj
is
the
expected
return
time
to
j.
For
any
i ∈S,
set
i :=

x ∈ | X0(x) = i

. If P(i) > 0, let Pi be the probability measure
deﬁned by Pi(A) := P(A | i).
Proposition 5.69 The following hold:
(i) Assume i →j. Then the frequency of visits in the ﬁrst n steps,
V (n)
j
(x)
n
,
converges to wj for a.e. x ∈i, that is
Pi
V (n)
j
(x)
n
→wj

= 1.
(5.71)
(ii) For every i, j ∈S we have
1
n
n

k=1
p(k)
ij →fijwj.
(5.72)
Proof. (i) If j is transient, then wj = 0 by (5.35) and paths from i visit j a
ﬁnite number of times almost surely, see Proposition 5.37. Therefore
V (n)
j
(x)
n
≤
Vj(x)
n
→0 = wj
P-almost surely.
If j ∈S is recurrent and i →j, then the random variables S(r)
j , r ≥2, are
independent and identically distributed with respect to Pi and Ei[S(r)
j ] = T jj ∈
R+, see Theorem 5.68. Moreover,
V (n)
j
(x) = sup

k

k

r=2
S(r)
j (x) ≤n
>

DISCRETE TIME MARKOV CHAINS
225
almost surely. Thus, a consequence of the strong law of large numbers, Proposi-
tion 4.93, yields
Pi
?
V (n)
j
(x)
n
→wj
@
= 1.
(ii) Since
V (n)
j
(x)
n
≤1 from (5.71) with i = j we get
*
j

V (n)
j
n
−wj
 P(dx) →0,
by the dominated convergence theorem, see Theorem B.54, hence by (5.30),
1
n
n

k=1
p(k)
jj = Ej
4
V (n)
j
n
5
→wj.
The conclusion (5.72) then follows from Lemma 5.60.
Theorem 5.70 (Ergodic theorem) Let

Xn

be a homogeneous Markov chain
with a ﬁnite or denumerable state-space and with an irreducible transition matrix
P. Then the following hold:
(i) For every state j ∈S we have
V (n)
j
(x)
n
→wj
P-almost surely.
(5.73)
(ii) Let φ : S →R be a function. Then
1
n
n

k=1
φ(Xk) →

j∈S
φ(j)wj
P-almost surely.
(5.74)
Proof. (i) By (i) of Proposition 5.69
V (n)
j
(x)
n
→wj almost surely in i :=

x ∈ | X0(x) = i

for every i ∈S, hence
V (n)
j
(x)
n
→wj for P-a.e. x ∈.
(ii) Set Ej,k :=

x | Xk(x) = j

. Trivially, V (n)
j
(x) = n
k=1 1Ek,j (x) and
φ(Xk(x)) =

j∈S
φ(j)1Ej,k(x)
Therefore,
1
n
n

k=1
φ(Xk(x)) = 1
n
n

k=1
 
j∈S
φ(j)1Ej,k(x)

=

j∈S
φ(j)1
n
n

k=1
1Ej,k(x) =

j∈S
φ(j)
V (n)
j
(x)
n
,

226
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
hence, by the dominated convergence theorem, Example B.56, and (5.73) one
concludes
lim
n→∞
1
n
n

k=1
φ(Xk(x)) = lim
n→∞

j∈S
φ(j)
V (n)
j
(x)
n
=

j∈S
φ(j)

lim
n→∞
V (n)
j
(x)
n

=

j∈S
φ(j)wj
P-almost surely.
Notice that in the previous computation the sum and the limit order can be
interchanged also if S is denumerable; in fact, since φ is bounded and V (n)
j
/n ≤1,
one can apply Lebesgue dominated convergence theorem, see Example B.56.
5.6.3
Powers of irreducible stochastic matrices
Let

Xn

be a homogeneous Markov chain with a ﬁnite or denumerable state-
space and an irreducible transition matrix P. Then from (5.72)
1
n
n

k=1
p(k)
ij →wj
∀i, j ∈S
(5.75)
since fij = 1 ∀i, j ∈S.
Equation (5.75) allows the conclusions of Theorem 5.61 and Exercise 5.66
to be extended to irreducible matrices. We have the following.
Theorem 5.71 Let

Xn

be a homogeneous Markov chain with a ﬁnite or denu-
merable state-space S and an irreducible transition matrix P. For every j ∈S,
let wj :=
1
T jj . Then the following hold:
(i) The vector w = (wj)j∈S is such that w = wP. Moreover, if z := (zi)i∈S is
such that 
i∈S |zi| < +∞and z = zP, then z = λw for some λ > 0.
(ii) Assume the state-space S is ﬁnite. Then w = (wj) is a stochastic vector,
w ∈T , hence the unique stochastic vector such that z = zP. Moreover,
all the components of w are nonzero, equivalently, every state is positive
recurrent.
Proof. (i) From the Beppo Levi theorem and (5.75), we have, for any j ∈S,
(wP)j =

ℓ∈S
wℓpℓj =

ℓ∈S

lim
n→∞
1
n
n

k=0
p(k)
iℓ

pℓj
= lim
n→∞
1
n
n

k=0

ℓ∈S
p(k)
iℓpℓj = lim
n→∞
1
n
n+1

k=1
p(k)
ij = wj.

DISCRETE TIME MARKOV CHAINS
227
Let z = (zi) with 
i∈S |zi| < ∞. and z = zP. Then z = zP = (zP)P = zP2,
and, by an induction argument, z = zPk for any k, i.e. zj := 
i∈S zip(k)
ij for any
j and for any k. Therefore,
zj = 1
n
n

k=1

i∈S
zip(k)
ij =

i∈S
zi
1
n
n

k=1
p(k)
ij

.
Since 1
n
n
k=1 p(k)
ij ≤1 and 
i∈S |zi| < +∞, one can apply the dominated con-
vergence theorem, see Example B.56. Thus, taking into account also (5.75), as
n →∞one gets
zj = lim
n→∞

i∈S
zi
1
n
n

k=1
p(k)
ij

=

i∈S
zi

lim
n→∞
1
n
n

k=1
p(k)
ij ) =
 
i∈S
zi

wj
i.e. z = λw, λ := 
i∈S zi ∈R.
(ii) If S is ﬁnite, S = {1, . . . , N}, then
N

j=1
wj =

j∈S

lim
n→∞
1
n
n

k=1
p(k)
ij

= lim
n→∞
1
n

j∈S

n

k=1
p(k)
ij

= lim
n→∞
1
n
n

k=1
 
j∈S
p(k)
ij

= lim
n→∞
1
n
n

k=1
1 = 1.
i.e. w is a stochastic vector. Finally, the components of w are nonzero by
Exercise 5.65.
Remark 5.72 We point out that in the second part of the ergodic theorem,
Theorem 5.71, the assumption that S if ﬁnite cannot be dropped: consider the
unlimited one-dimensional random walk whose transition matrix is P = (pij)
where pij = 1/2 if |j −i| = 1 and zero otherwise. In this case the matrix is
clearly irreducible, and for every i, j ∈Z, (Pn)i
j →0 as n →∞since as n
goes to inﬁnity, more and more states are visited. What fails in extending the
proof we have given when S is ﬁnite, is that if S is not ﬁnite, then we cannot
exchange the limits:
lim
n→∞
1
n
n

k=1

j∈S
p(k)
ij ̸=

j∈S
lim
n→∞
1
n
n

k=1
p(k)
ij .
Finally, we observe that (5.75) and, consequently, Theorem 5.71 can also
be proven without taking advantage of the ergodic property of the chain, see
Theorem 5.81.

228
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
5.6.4
Markov chain Monte Carlo
In many applications one needs to evaluate the weighted sum of a certain quantity
f : S →R deﬁned on a ﬁnite set S
E := 1
z

j∈S
f (j)πj,
z :=

j∈S
πj
(5.76)
with respect to the weight π = (πj). With the language of probability, denoting
by μ the probability measure on S with point mass density given by 1
z(πj)j∈S,
we want to evaluate
E :=
*
S
f (t)μ(dt).
(5.77)
If the set S is small, then it is easy to compute E straightforwardly. If S is a
large set (say |E| = (50)!), then the computation of E or even just of the total
mass z cannot be performed directly, so that some strategy has to be devised in
order to compute or at least approximate E.
5.6.4.1
Monte Carlo
One such strategy is the Monte Carlo method, see Section 4.4.5. In practice,
one has to specify a probability space (, E, P) and a random variable X :
 →S such that PX = μ; then one considers the Bernoulli scheme of -valued
sequences (∞, E∞, P∞) and the random variables Xn : ∞→R deﬁned for
n = 1, 2, . . . , by Xn(x) := X(xn) if x =

xn

∈∞. Such random variables are
independent and equidistributed as X, see Proposition 4.87. The strong law of
large numbers says that, for almost any sequence of points x =

xk

, xk ∈,
1
n
n

j=1
f (X(xj)) = 1
n
n

j=1
f (Xj(x)) →E
(5.78)
and the weak estimate (4.9) holds.
In order to obtain an algorithm from the strong law of large numbers, we
must face two issues:
(i) How to choose (as efﬁciently as possible) the probability space (, E, P)
and the random variable X :  →S such that PX = μ.
(ii) Since the limit in (5.78) holds for almost any sequence

xk

∈∞(and
not for every sequence), we must be sure to pick up a ‘good’ sequence.
The ‘risk’ to pick up a ‘bad’ sequence is zero, but, nevertheless, it cannot
be completely neglected.
If some sort of regularity (or pattern) appears in the sequence, then it
may inﬂuence the value of the limit or even its existence. For example,
assume we are ﬂipping a fair coin an inﬁnite number of times. If we get
a success (head) at every ﬂipping or every three ﬂippings, then, in either

DISCRETE TIME MARKOV CHAINS
229
case, 1
n
n
k=1 Xk(x) = 1
x
n
k=1 X(xk) converges to 1 or 1/3 and not to the
expected value 1/2. We should be able to pick up a sequence that is as
‘irregular’ or as ‘random’ as possible.
There are several ways to get sequences or, better, large ﬁnite tuples of ran-
dom numbers uniformly distributed in the interval [0, 1]. We have to be cautious
with the terminology since these numbers are in fact integer multiples of a ﬁxed
small number. However, if f is not fastly oscillating and the approximation
needs not be too accurate, this would not be a problem. For instance, physi-
cal random number generators can be based on an essentially random atomic
or subatomic physical phenomenon whose unpredictability can be traced to the
laws of quantum mechanics. Potential sources include radioactive decay, ther-
mal noise and shot noise. With some conditioning they are suitable sources of
random samples that are uniformly distributed in [0, 1]. Also pseudo-random
number generators can be useful. They are deterministic algorithms that can
automatically create long runs of numbers with good random properties. They
actually output periodic sequences, although with a hopefully very long period,
so a complete ‘randomless’ is out of the question. Also, correlations between
the outcomes, especially between far outcomes, or between groups of outcomes,
usually involve surprises and must be carefully checked. Nevertheless, software
libraries that contain pseudo-random generators with good and speciﬁed statistic
properties are available. We do not enter into these aspects, which are or course
relevant in simulation, see e.g. [17], but which are out of the scope of proba-
bility. For the purpose of this discussion, we simply assume the availability of
generators of ‘random sequences’ uniformly distributed in the interval [0, 1] and
such that grouping them in N-tuples yields a ‘random sequence of N-tuples’
that are also uniformly distributed in Q = [0, 1]N. Thus, typical choices for
(, E, P) and X in computing E in (5.77) when μ = LN are (Q, B(RN), LN)
and X(x) := x, respectively.
Integrating with respect to a nonuniform measure μ is harder. We give a hint
in the scalar case. Assume we want to compute
E :=
* +∞
−∞
f (t) dμ(t)
where f is a bounded and Borel-regular function and (B(R), μ) is a mea-
sure on R. In this case one takes ([0, 1], B(R), L1), considers the law F(t) :=
μ([−∞, t]) of μ and deﬁnes X : [0, 1] →R by (3.10), i.e.
X(s) := min

t
 s ≤F(t)

.
(5.79)
Since F(t) is right-continuous, X(s) is left-continuous, hence Borel-measurable.
Moreover, {s ∈[0, 1] | X(s) ≤t} = {s ∈[0, 1] | 0 ≤s ≤F(t)} for every t ∈R
so that
L1(X ≤t) = L1([0, F(t)]) = F(t)
∀t ∈R,

230
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
i.e. X follows μ. Therefore, for almost any sequence {xk} ⊂[0, 1] we get
1
n
n

k=1
f (X(xk)) →
* ∞
−∞
f (t) μ(dt)
as n →∞.
The procedure described above cannot be implemented to evaluate (5.76) for
large sets S, since the computation of the law of μ and of the random variable
X in (5.79) are of the same order of complexity as the direct computation of E.
When S can be seen as a product, S = S1 × · · · × SN and the measure μ with
mass density 1
z(πj) is a product,
μ = μ1 × μ2 × · · · × μN
where each μi is a measure on Si, we can factorize the computation of X.
For instance, if N = 2, we deﬁne two random variables X : [0, 1] →S1 and
Y : [0, 1] →S2 following the distributions μ1 and μ2, respectively. Then
*
f (t, s)μ1(dt) × μ2(dt) =
*  *
f (t, s)μ2(ds)

μ1(dt)
=
*  *
f (t, Y(y))P(dy)

μ1(dt) =
**
g(X(x), Y(y)) P × P(dxdy),
so that the strong law of large numbers yields
1
n
n

k=1
f (X(xk), Y(yk)) →E
for almost any sequence

(xk, yk)

∈[0, 1]2.
5.6.4.2
Markov chain Monte Carlo
When μ is not a product measure, one takes advantage of the ergodic property
of Markov chains and of the fact that arbitrary Markov chains can be run from
uniformly distributed random variables.
The paradigm we pursue in computing E in (5.76) is the following: given
the probability mass distribution w = (wj) ∈S, wj := 1
zπj, we deﬁne a homo-
geneous Markov chain on a probability space (, E, P) with state-space S and
such that w is a ﬁxed point for its transition matrix P, w = wP, i.e. π = πP.
We have already proven, Theorem 5.14, that given a stochastic matrix P one
can always deﬁne a homogeneous Markov chain whose transition matrix is P.
Thus, it sufﬁces to deﬁne a stochastic matrix P such that π is a ﬁxed point for P.
This is realized by the Hastings–Metropolis algorithm.
Let Q = (qij) be any stochastic matrix. For i, j ∈S, i ̸= j, let
pij := 1
πi
min

πiqij, πjqji

,
(5.80)

DISCRETE TIME MARKOV CHAINS
231
so that 0 ≤pij ≤1 and 
j̸=i pij ≤1. Deﬁne P = (pij) as follows:
• if i ̸= j, then pij is deﬁned by (5.80);
• if i = j, set pii = 1 −
j̸=i pij;
so that P is a stochastic matrix. The matrix R = (rij), rij := min{πiqij,
πjqji} is symmetric, hence
πipij = πjpji
∀i, j ∈S
and
N

i=1
πipij =
N

i=1
πjpji = πj
∀j ∈S
i.e. π = πP.
With the Metropolis algorithm one chooses Q = (qij) as an irreducible and
symmetric matrix, so that if i ̸= j, then
pij = min

1,
πj
πi
$
qij,
hence P = (pij) where
pij =
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
qij
if πj ≥πi,
πj
πi
qij
if πj < πi,
1 −
j̸=i pij
if i = j.
(5.81)
Possible choices for Q are
qij =

1/(N −1)
if i ̸= j,
0
if i = j,
where N = |S|, or, for instance, Q can be chosen as the matrix associated with
a symmetric circular random walk. Notice that, in general, in order to deﬁne P
one does not need to know the total mass z or the cardinality N of S.
Now, let

Xn

be a homogeneous Markov chain whose transition matrix is P,
see Theorem 5.14. If P is irreducible, then 1
zπ is the only stochastic vector which
is also a ﬁxed point of P(x) = xP. Moreover, for any j ∈S the number ( 1
zπj)−1
is the return time in j for

Xn

, see Theorem 5.71. The ergodic theorem, (ii) of
Theorem 5.70, gives the convergence
1
n
n

k=1
f (Xk(x)) →1
z

j∈S
f (j)πj
for almost any x ∈.

232
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
In order to simulate the chain, one only needs to have a large number
of independent variables that follow the uniform distribution in [0, 1], see
Theorem 5.14, so that, this procedure, known as a Markov chain Monte Carlo
procedure, may eventually lead to numerical algorithms for the approximate
computation of E, see e.g. [18].
The simulation of a Markov chain with transition matrix P is just the transition
from a position i ∈S at step n to a new position j at step (n + 1) following the
probability density (pij)j. In this case
pij = min

1,
πj
πi
$
qij.
Assume we are in state i at step n. We pick j ∈S according to the mass distri-
bution (qij)j. Then:
• if πi ≤πj we move to state j, Xn+1 = j;
• if πi > πj we play a Bernoulli trial of parameter p = πi/πj. Then we set
Xn+1 = j if we obtain a success and Xn+1 = i otherwise.
Two facts are crucial for numerical applications. First of all, there is the
irreducibility assumption on P, which can be achieved in many applications. The
second crucial point is the number of steps one needs to perform in order to
obtain an estimate of E which is as accurate as wished. We shall not deal with
these problems which involve both statistical and analytic estimates and are still
an open ﬁeld of research. We refer the reader to [18] and to the stimulating paper
[19] where further applications, such as optimization problems, of the Markov
chain Monte Carlo method, are investigated. Here, we only give the following
result, see [20].
Theorem 5.73 (Metropolis) If π is not constant and the matrix P deﬁned in (5.81)
in the Metropolis algorithm is irreducible, then P is regular.
Proof.
We
prove
that
there
exists
h ∈S
such
that
phh > 0.
Let
C =

i | πi = maxj πj

.
Obviously,
C ̸= ∅
and,
by
assumption,
C ̸= S.
Thus, see Exercise 5.64, there exists h ∈C and k /∈C such that qhk > 0 and
πh > πk. If i ̸= j, then pij ≤qij, and by deﬁnition of P
phh = 1 −

j̸=h
phj = 1 −

j̸=h,k
phj −phk
≥1 −

j̸=h,k
qhj −πk
πh
qhk = 1 −

j̸=h
qhj + qhk

1 −πk
πh

= qhh + qhk

1 −πk
πh

> 0.
The claim thus follows from Exercise 5.64.

DISCRETE TIME MARKOV CHAINS
233
5.7
Renewal theorem
Let

Xn

be a homogeneous Markov chain with a ﬁnite or denumerable state-
space S and transition matrix P. In this section, we discuss the convergence of the
sequence {Pn} as n →∞. The results extend partially the results of Section 5.5:
there S was supposed ﬁnite and P regular.
5.7.1
Periodicity
Given two positive integers d and n, we say that d divides n and write d ⊥n
if n = kd for some positive integer k. If N is a ﬁnite or denumerable set of
positive integers, then the largest integer d ≥1 that divides any element of N
is called the greatest common divisor of N and is denoted by gcd(N). It can
be easily proven that if N is inﬁnite and d = gcd(N), then there exists a ﬁnite
subset M ⊂N such that d = gcd(M).
Deﬁnition 5.74 Let P = (pij) be a stochastic matrix with a ﬁnite or denumerable
set S of indices. The number
dj := gcd

n
 p(n)
jj > 0

is called the period of the state j. If dj = 1, then the state j is said to be aperiodic,
while if dj = d ≥2, then j is said to be d-periodic.
By Deﬁnition 5.74, if dj is d-periodic, then p(k)
jj = 0 whenever k is not a mul-
tiple of d. Notice that it is not excluded that p(k)
jj = 0 for some k such that
d ⊥k.
Exercise 5.75 Prove the following:
(i) If P = Id, then Pn = Id ∀n, so that all the states are aperiodic.
(ii) If P is irreducible and all its rows coincide, then Pn = P for every n and
all the states are aperiodic.
(iii) If P =

0
1
1
0

, then Pn =

Id
if n is even,
P
if n is odd. Both the states 1 and 2 have
period 2. Moreover, the limit of Pn as n →∞does not exist.
Proposition 5.76 If two states i are j are communicating, i ↔j, then they have
the same period. Thus, the period of an irreducible stochastic matrix is well deﬁned
as the period of any of its states.
Proof.
Let
Ni =

k | p(k)
ii > 0

,
Nj =

k | p(k)
jj > 0

,
di = gcd(Ni)
and
dj = gcd(Nj).
Let
h, k
be
such
that
p(h)
ij > 0
and
p(k)
ji > 0.
Then
p(h+k)
ii
= N
ℓ=1 p(h)
iℓp(k)
ℓi ≥p(h)
ij p(k)
ji > 0, i.e. h + k ∈Ni, so that di ⊥(h + k).

234
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
For
each
n ∈Nj
we
also
have
p(h+k+n)
ii
≥p(h)
ij p(n)
jj p(k)
ji > 0.
Therefore,
h + k + n ∈Ni, hence di ⊥(h + k + n). Thus di ⊥n ∀n ∈Nj, so that di ≤dj.
Interchanging i and j, the previous argument gives also dj ≤di, hence di = dj.
Proposition 5.77 Let

Xn

be a homogeneous Markov chain with a ﬁnite or
denumerable state-space S and transition matrix P = (pij). Let j ∈S be a state,
let f (k)
jj
be the probability that a path from j visits j for the ﬁrst time at step k
and denote by dj ≥1 the period of j. Then
gcd

k ≥1
 f (k)
jj > 0

= dj.
Proof. Let us denote pk := p(k)
jj , fk := f (k)
jj
and let P :=

k | pk > 0

and
F :=

k | fk > 0

. It sufﬁces to show that d ⊥k ∀k ∈P if and only if d ⊥k
∀k ∈F.
Assume d divides any h ∈P. Let k ∈F. Since, see Proposition 5.58,
pk = fk + fk−1p1 + · · · + f1pk−1
(5.82)
we get pk ≥fk > 0 so that k ∈P and d ⊥k.
Assume d divides any h ∈F. We show, with an induction argument on k,
that d divides any h ∈P such that h ≤k.
If k = 1, the claim is obviously true since p1 = f1 > 0. Assume the claim
holds true for k −1. Let k be such that pk > 0. Two cases may occur. Either
fk > 0, so that d ⊥k, or fk = 0. In the latter case (5.82) implies
fk−1p1 + · · · + f1pk−1 = pk > 0
so that at least one addendum must be positive: there exists k′ < k such that
pk′ > 0 and fk−k′ > 0. Since k′ < k, d ⊥k′ and d ⊥(k −k′), so that d ⊥k.
5.7.2
Renewal theorem
Theorem 5.78 (Erd¨os–Feller–Pollard) Let

fn

, n ≥1, and

pn

, n ≥0, be
two non-negative sequences such that p0 = 1 and ∞
n=1 fn = 1. Assume
pn =
n

j=1
fjpn−j
∀n ≥1.
(5.83)
Let
T := ∞
j=1 jfj
(T = +∞
is
allowed)
and
S :=

n | fn > 0

.
If
gcd(S) = 1, then
lim
n→+∞pn = 1
T
(0 if T = +∞).
Lemma 5.79 Let a1, . . . , ar be relatively prime integers. Then there exists n0 such
that any integer n ≥n0 is a linear combination of a1, . . . , ar with non-negative
coefﬁcients.

DISCRETE TIME MARKOV CHAINS
235
Proof. Since gcd(a1, . . . , ar) = 1, the Bezout identity holds: namely, there
exist β1, . . . , βr ∈Z such that r
i=1 βiai = 1. Let a := r
i=1 ai. Divide n by a,
n = qa + s with 0 ≤s < a, so that
n = q
r

i=1
ai + s
r

i=1
βiai =
r

i=1
(q + βis)ai.
The right-hand side of the previous equality is a linear combination of a1, . . . , ar
with positive coefﬁcients if n is such that q ≥a maxi(|βi|).
Lemma 5.80 Let

fj

, j ≥1, be a non-negative sequence such that ∞
j=1 fj = 1,
and set S :=

j
 fj > 0

. Let a ∈R and let

qn

, n ∈Z, be a sequence such that
⎧
⎪⎨
⎪⎩
qn = ∞
j=1 fjqn−j
∀n ∈Z,
q0 = a,
qn ≤a
∀n ≤0.
(5.84)
If gcd(S) = 1, then qn = a ∀n. The same holds if the assumption qn ≤a ∀n ≤0
in (5.84) is replaced by qn ≥a ∀n ≤0.
Proof. Up to a change of sign of

qn

we may assume qn ≤a ∀n ≤0. System
(5.84) gives
a = q0 =
∞

j=1
fjq−j ≤a
∞

j=1
fj = a.
Thus ∞
j=1 fj(a −q−j) = 0 so that q−n = a whenever fn > 0, i.e. for any n ∈S.
For any n ∈S, (5.84) gives
a = q−n =
∞

j=1
fjq−n−j ≤a
∞

j=1
fj = a
so that ∞
j=1 fj(a −q−n−j) = 0, i.e. q−n−m = a if both n, m ∈S. With an
induction argument, one can now show that q−αn−βm = 0 for any positive inte-
gers α, β, and then that q−n = 0 for any integer n which is a linear combination
with positive coefﬁcients of integers in S.
Since gcd(S) = 1, there exist a1, . . . , ar ∈S such that gcd(a1, . . . , ar) = 1
so that by Lemma 5.79 each integer n ≥n0 is a linear combination of integers
in S with positive coefﬁcients. Therefore, we conclude that
q−n = a
∀n ≥n0.
It is now easy to check by an induction argument that q−n0+1, q−n0+2, · · · = a.
In fact, if qn = a ∀n ≤h, then (5.84) reads
qh+1 =
∞

j=1
fjqh+1−j = a
∞

j=1
fj = a.

236
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Proof of Theorem 5.78. Step 1. Let a ∈R and let

hn

be a sequence such
that phn →a. We show that there exist a sequence

qj

j∈Z and a subsequence

kn

of

hn

such that
pkn−j →qj
for any j
(5.85)
and

qn = ∞
j=1 fjqn−j
∀n ∈Z,
q0 = a.
(5.86)
Let an,j := pn−j. For any integer j, the subsequence

an,j

n is bounded, hence,
by compactness, there exist a number qj and a subsequence k(j)
n
such that
ak(j)
n ,j →qj as n →∞. Iterating the procedure for j = 0, ±1, ±2, . . . , we may
assume that

k(−j)
n

=

k(j)
n

and that

k(j+1)
n

is a subsequence of

k(j)
n

. Let
kn := k(n)
n . Then, for every ﬁxed j and nj large enough, the sequence

kn

n≥nj
is a subsequence of

k(j)
n

so that akn,j →qj. Thus the limit (5.85) holds.
Let us now prove (5.86). Equation (5.83) reads
pkn−h =
kn−h

j=1
fjpkn−h−j.
Since
• fjpkn−h−j →fjqh−j as n →∞for any j,
• supn(fjpkn−h−j) ≤fj for any j and ∞
j=1 fj = 1,
applying the dominated convergence theorem, see Example B.56, we get
qh =
∞

j=1
fjqh−j
∀h ∈Z,
i.e. (5.86) holds since q0 = a.
Step 2. Let a = lim supn pn and let

hn

be a sequence such that phn →a.
Let

kn

and

qj

as in Step 1. Then pkn−j →qj so that qj ≤a ∀j. Since

qj

satisﬁes system (5.86), applying Lemma 5.80 we get qj = a ∀j ∈Z, thus
obtaining
lim
n→∞pkn−j = a
∀j ∈Z.
Step 3. Summing up (5.83) for 1, 2, . . . , N, we get
N

n=1
pn =
N

n=1
n

j=1
fjpn−j =
N

n=1
 n−1

j=0
fn−jpj

,
(5.87)

DISCRETE TIME MARKOV CHAINS
237
or, equivalently,
pN +
N−1

j=1

1 −
N−j

i=1
fi

pj =
N

j=1
fj.
(5.88)
For any j ≥1, let Tj := ∞
i=j fi. Then T1 = ∞
i=1 fi = 1 and ∞
j=1 Tj =
∞
k=1 kfk = T . Equation (5.88) can equivalently be written as
pNT1 +
N−1

j=1

1 −
N−j

i=1
fi

pj =
N

j=1
fj,
or as
N

j=1
TjpN+1−j =
N

j=1
fj,
(5.89)
see (5.64), (5.65) and (5.66). Consider now the double index sequence
ϕN(j) :=

TjpN+1−j
if 1 ≤j ≤N,
0
if j > N.
Equation (5.89) or (5.87) thus reads
∞

j=1
ϕN(j) =
N

j=1
fj.
(5.90)
Since ϕkn−1(j) →Tjqj = aTj by Step 2, we get the following:
• If T = +∞, then Step 2, (5.90) and the Fatou lemma yield
T a =
 ∞

j=1
Tj

a =
∞

j=1
lim
n→∞ϕkn−1(j) ≤lim inf
n→∞
∞

j=1
ϕkn−1(j)
= lim inf
n→∞
kn−1

j=1
fj =
∞

j=1
fj = 1,
so that a = 0, i.e. lim supn pn = 0. Thus the claim is proven whenever
T = +∞.
• If T < ∞, then Step 2, (5.90) and the dominated convergence theorem,
see Example B.56, yield
T a =
 ∞

j=1
Tj

a =
∞

j=0
lim
n→∞ϕkn−1(j) = lim
n→∞
∞

j=1
ϕkn−1(j)
= lim inf
n→∞
kn−1

j=1
fj =
∞

j=1
fj = 1,

238
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
so that lim supn pn = a = 1/T .
Since we may repeat Step 2 for any sequence hn such that phn →b :=
lim infn pn, from the above we also get lim infn pn = 1/T , thus concluding
that pn →1/T .
The renewal theorem has important consequences.
Theorem 5.81 Let

Xn

be a homogeneous Markov chain with a ﬁnite or denu-
merable space-state S and denote by P = (pij) its transition matrix. For j ∈S, let
T jj be the expected return time to j, wj :=
1
T jj and let d be the period of j. Then
p(nd)
jj
→dwj
and
1
n
n

k=1
p(k)
ij →fijwj
∀i ∈S.
(5.91)
Proof. Step 1. Let us prove the ﬁrst limit of (5.91) when j is aperiodic, d = 1.
If j is a transient state, then wj = 1/T jj = 0, see (5.34) and ∞
n=1 p(n)
jj <
∞. Therefore, p(n)
jj →0 = wj. If j is recurrent, let fk := f (k)
jj
and pk := p(k)
jj
so that fjj = ∞
k=1 fk = 1. Thus, Proposition 5.77 gives gcd

k
 fk > 0

= 1.
Moreover, p0 = 1, pn = n
k=1 fkpn−k ∀n ≥1 by (5.61) and T jj = ∞
k=1 kfk,
cf. (5.35). The renewal theorem, Theorem 5.78, gives p(n)
jj →wj.
Step 2. Let us prove the ﬁrst limit of (5.91) assuming j is d-periodic, d ≥2.
The sequence of random variables

Xkd

k is a homogeneous Markov chain
with state-space S and transition matrix )P := Pd. For this chain, j is aperiodic
since
)d = gcd

k
 p(kd)
jj
> 0

= 1
d gcd

kd
 p(kd)
jj
> 0

= d
d = 1.
Let )Tjj be the return time to j for the new chain and let )wj := 1/)Tjj. From
Step 1
p(nd)
jj
→)wj.
(5.92)
Denote by )
f (k)
ij
the probability that a path of the new chain from i visits j for
the ﬁrst time at step k. Trivially, )
f (k)
jj
is also the probability that the initial chain
visits j starting from j for the ﬁrst time at step kd, )
f (k)
jj = f (kd)
jj
. Moreover,
f (n)
jj
= 0 if n is not a multiple of d. Therefore,
)Tjj =
∞

k=1
k )
f (k)
jj = 1
d
∞

k=1
(kd)f (kd)
jj
= 1
d
∞

k=1
kf (k)
jj = 1
d T jj.
Therefore, (5.92) yields p(nd)
jj
→dwj.

DISCRETE TIME MARKOV CHAINS
239
Step 3. Let us prove that 1
n
n
k=1 p(k)
jj →wj. Since p(k)
jj = 0 if k is not a multiple
of d, by the Cesaro theorem,
1
n
n

k=1
p(k)
jj = 1
n
⌊n/d⌋

k=1
p(kd)
jj
→1
d dwj = wj.
Step 4. The second limit in (5.91) then follows from Step 3 and (ii) of
Lemma 5.60.
Remark 5.82 The second limit in (5.91) is sufﬁcient to prove Theorem 5.71.
Thus we have another proof of Theorem 5.71 that does not mention the ergodic
property of Markov chains.
5.7.3
Exercises
Exercise 5.83 Let

Xn

be a homogeneous Markov chain with a ﬁnite state-space
S = {1, . . . , N} and irreducible transition matrix P. Let Mjj(n) be the expected
return time to j measured on a window of n steps, and let Njj be the expected
number of visits in j that paths starting from j do in the ﬁrst n steps. Compute
lim
n→∞
Mjj(n)Njj(n)
n
.
Solution. By deﬁnition, Njj(n) = 1
nEj[V (n)
j
] = 1
n
n
k=1 p(k)
jj and
Mjj(n) = Ej[tj1{tj <n}] =
n

k=1
kf (k)
jj .
Therefore, since 1
n
n
k=1 p(k)
jj →
1
T j , we have
Mjj(n)Njj(n)
n
=

n

k=1
kf (k)
jj
1
n
n

k=1
p(k)
jj

→T jj
1
T jj
= 1.
Exercise 5.84 Let P = (pij) be the transition matrix of a Markov chain with ﬁnite
state-space S. Prove that P is irreducible and aperiodic if and only if P is regular.
Solution. Assume P is irreducibile and aperiodic. Then all the states are
pairwise communicating, aperiodic, and, since S is also a minimal closed class,
all states are recurrent, fij = 1 ∀i, j ∈S, see Proposition 5.40. Theorem 5.81
yields p(n)
jj →wj := 1/T jj and wj > 0 by Exercise 5.65. Moreover, Lemma
5.60 yields for any i and j ∈S
p(n)
ij →fijwj = wj > 0.
We then conclude that for n large enough, p(n)
ij > 0 ∀i, j ∈S.

240
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Conversely, if P is regular, then trivially P is irreducible. Moreover, for any
large enough n, all the entries of Pn are nonzero, see Theorem 5.54. P is therefore
aperiodic.
Exercise 5.85 Let P be a stochastic, irreducible matrix. Show that, if P is
d-periodic, d ≥2, then Pd is not irreducible.
Solution. Suppose, contrary to our claim, that Pd is irreducible. Since Pd is
aperiodic, then Pd is regular, see Exercise 5.84. Therefore P is regular, hence
aperiodic, again by Exercise 5.84, a contradiction, since d ≥2.
Exercise 5.86 Let P be a stochastic matrix and let i, j be two communicating
states. Show that either both i and j are positive recurrent or both are transient.
Solution. Assume j is positive recurrent. Then wj = 1/T jj > 0 and j is
recurrent fjj = 1. Let d be the period of j. We have p(nd)
jj
→dwj > 0, see
Theorem 5.81. Since i ↔j, i is d-periodic, see Proposition 5.76 and recur-
rent fii = 1, see Proposition 5.40. By Theorem 5.81 p(n)
ii
→wi, wi =
1
T ii . On
the other hand, there exist r, s > 0 such that p(r)
ij > 0 and p(s)
ji > 0. Therefore,
p(r+n+s)
ii
≥p(r)
ij p(n)
jj p(s)
ji . Letting n →∞, we conclude that
wi ≥p(r)
ij wjjp(s)
ji > 0,
i.e. T ii < ∞.

6
An introduction to continuous
time Markov chains
In this chapter, we give a very short introduction to continuous time Markov
chains. In Section 6.1 we discuss the Poisson process while in Section 6.2 we
consider homogeneous continuous time Markov chains with ﬁnite state-space and
right-continuous trajectories. The convergence to equilibrium of the transition
probabilities matrices and the description of the holding times are discussed.
6.1
Poisson process
We go back to Example 3.34 by giving an explicit example: the emission of clicks
of a Geiger counter. In this section we follow closely the presentation in [5].
Evidence shows the following features of the distribution of the number of
clicks produced by the counter:
(i) The number of clicks produced in pairwise disjoint time intervals are inde-
pendent.
(ii) The clicks are uniformly distributed with respect to time, i.e. the number of
clicks produced in the intervals [a + c, b + c] a, b, c ≥0 does not depend
on c.
(iii) In each time interval, the average number of clicks is ﬁnite,
(iv) At each moment, the Geiger counter produces at most one click.
When deﬁning a model, the previous features can be formalized as follows.
Let I be the family of time intervals
I :=

I = [a, b]
 0 ≤a < b < +∞

.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

242
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Consider the Geiger counter as a probability space (, E, P) and, for any I ∈I,
let NI :  →N be the random variable on (, E, P) that counts the number of
clicks produced in the interval I. In terms of the family

NI

I∈I, features (i),
(ii) and (iii) read:
(P1) NI∪J = NI + NJ if I ∩J = ∅and I ∪J ∈I.
(P2) If J ⊂I is a family of pairwise disjoint intervals, then

NI

I∈J is a
family of independent random variables.
(P3) For any c > 0 and any I ∈I, NI and Nc+I follow the same distribution.
We recall that c + I := [c + a, c + b] if I = [a, b].
(P4) E
%
NI
&
< +∞∀I ∈I.
Properties (P1), (P2), (P3) and (P4) formalize properties (i), (ii) and (iii). We
must formulate (iv). Consider the event F ⊂ deﬁned by the property ‘Two or
more clicks are produced at a certain time t ∈[0, 1]’. Then x ∈F if and only
if for any positive integer n there exists an interval Ik,n := [k/2n, (k + 1)/2n],
k = k(x, n) such that NIk,n(x) ≥2, i.e.
F :=
∞
'
n=1
2n−1
(
k=0

NIk,n ≥2

.
Consequently, the event E ⊂ deﬁned by the property that two or more clicks
are produced at a certain time t ∈R+ is
E =
∞
(
q=0
(q + F).
and we can formalize feature (iv) as
(P5-1)
P(E) = 0
or
P(F) = 0.
Lemma 6.1 Let {NI, I ∈I} be a family of random variables satisfying (P1)–(P4).
Then P(E) = 0 if and only if
(P5)
lim supϵ→0
P(N[0,ϵ] ≥2)
ϵ
= 0.
Proof. Using (P1), (P2) and (P3), we get
P(E) = lim
n→∞P
 2n−1
(
k=0

NIk,n ≥2

= 1 −lim
n→∞P
 2n−1
'
k=0

NIk,n ≤1

= 1 −lim
n→∞
2n−1
=
k=0
P

NIk,n ≤1

= 1 −lim
n→∞
2n−1
=
k=0

1 −P(NIk,n ≥2)


AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
243
= 1 −lim
n→∞

1 −P(NI0,n ≥2)
2n
.
Let f (t) := P(N{[0,t]} ≥2), t ≥0. By the above computation P(E) = 0 if and
only if
(1 −f (2−n))2n →1.
(6.1)
Let tn := 2−n then (1 −f (tn))1/tn →1 if and only if
log(1−f (tn))
tn
→0. Since
tn →0, (6.1) is equivalent to f (tn)/tn →0. Finally, for any ϵ ∈[0, 1], let n =
n(ϵ) such that tn−1 ≤ϵ < tn. Then
n(ϵ) →+∞
and
f (ϵ)
ϵ
≤f (tn)
tn−1
= 2f (tn)
tn
→0
as ϵ →0.
By the above, (P5-1) is equivalent to (P5).
Deﬁnition 6.2 (Poisson process) A family

Nt

t≥0 of non-negative integer
valued random variables on a probability space (, E, P) is called a
Poisson process with intensity α if:
(i) N0(x) = 0 almost surely.
(ii) The map t →Nt(x) is increasing and right-continuous for a.e. x ∈.
(iii) For any n and any list 0 = t0 < t1 < · · · < tn the random variables Ntk −
Ntk−1, k = 1, . . . , n, are pairwise independent.
(iv) For any t ≥s ≥0 the random variable Nt −Ns follows the Poisson dis-
tribution of parameter α(t −s), i.e.
P(Nt −Ns = k) = e−α(t−s) αk(t −s)k
k!
∀k ≥0.
(6.2)
In particular, we point out that
P(Nt = k) = e−αt αktk
k!
∀k ≥0
so that
E
%
Nt
&
=
∞

k=0
e−αtk αktk
k!
= α t.
Theorem 6.3 Let

NI

I∈I be a family of random variables on the probability
space (, E, P) satisfying (P1)–(P5). Then the continuous family of random vari-
ables

Nt

t≥0, Nt := N[0,t], is a Poisson process with intensity α := E
%
N[0,1]
&
.
Conversely, let

Nt

t≥0 be a Poisson process of intensity α > 0 on (, E, P).
Then the family of random variables

NI

I∈I deﬁned by NI := Nt −Ns if I =
[s, t] satisﬁes properties (P1)–(P5) and E
%
N[0,1]
&
= α.
Proof. (i) Assume

NI

I∈I satisﬁes (P1)–(P5). Obviously, (P1)–(P4) imply
that the process

Nt

, Nt = N[0,t] satisﬁes (i), (ii) and (iii) of Deﬁnition 6.2.

244
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Let α(t) := E
%
N[0,t]
&
, t > 0. By (P2) and (P3), α(t) is monotone increasing
and α(t + s) = α(t) + α(s). Thus, by Lemma 3.50, α(t) = α t, α := α(1).
In particular, E
%
Nt
&
= αt.
In order to prove that Nt follows the Poisson distribution of parameter αt,
we deﬁne a sequence of Bernoulli processes that approximates Nt. Partition the
interval [0, t] in 2n pairwise intervals of equal length
Ik,n =
0(k −1)t
2n
, kt
2n
1
,
k = 1, . . . , 2n.
Let Xk,n(x) := NIk,n(x) and Xk,n(x) := 1{Xk,n≥1}(x) = 1[1,∞](Xk,n(x)). By (P2)
and (P3) the random variables

Xk,n

k are independent and identically distributed
and so are the random variables

Xk,n

k, see Proposition 4.36. More precisely, the
random variables
Xk,n

k are Bernoulli trials of parameter pn := P(Xk,n ≥1) =
P(N[0,2−n] ≥1). Therefore, the random variable Nn
t := 2n
k=1 Xk,n follows the
binomial distribution B(2n, pn). Let us now show that the random variables

Nn
t

approximate the random variable Nt. Clearly, Nn+1
t
≥Nn
t , i.e.

Nn
t

n is a mono-
tone increasing sequence of random variables. Moreover, by (P5) and Lemma 6.1
P(Nn
t ̸= Nt) =
2n

k=1
P(Xk,n −Xk,n) =
2n

n=1
P(Xk,n ≥2)
= 2nP(N2−nt ≥2) →0,
i.e. Nn
t (x) ↑Nt(x) almost surely. By the Beppo Levi theorem we then get
αt = E
%
Nt
&
= lim
n→∞E
%
Nn
t
&
= lim
n→∞2npn,
and
P(Nn
t ≥k) →P(Nt ≥k)
∀k ∈N.
Finally, we recall that limits of binomial distributions are Poisson distributions,
see Proposition 3.31. Thus, for any integer k,
P(Nt = k) = lim
n→∞P(Nn
t = k) = lim
n→∞B(2n, pn)({k}) = P(αt)({k}).
Conversely, assume

Nt

is a Poisson process with intensity α. Let I =
[s, t]. By properties (i) and (ii) of Poisson processes, the random variables NI :=
Nt −Ns are non-negative and (iii) of Deﬁnition 6.2 implies (P1). Moreover,
P(NI = k) = P(Nt −Ns = k) = P(α(t −s))({k}), so that also (P2) and (P3)
are satisﬁed and E
%
NI
&
= α(t −s) < +∞, i.e. also (P4) holds. Finally, since
P(N[0,ϵ] ≥2) = 1 −P(N[0,ϵ] = 0) −P(N[0,ϵ] = 1) = 1 −e−αϵ −αϵe−αϵ,
dividing by ϵ and letting ϵ →0, we get (P5). The claims are then proven.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
245
If there is a click at time s > 0, then the probability that there is another click
before a further time, i.e. the probability that the time T (x) between two clicks or,
more formally, between two singularities of Nt(x) is smaller than t, is given by
P

T < t + s
 T ≥s

:= P(N[s,s+t[ = 0) = P(Nt+s −Ns = 0) = e−αt.
Thus:
(i) The waiting times between two consecutive clicks are independent.
(ii) The waiting times between two consecutive clicks are exponentially dis-
tributed, P(T < t + s | T ≥s) = e−αt.
The two properties (i) and (ii) above characterize Poisson processes. Let

Wk

be a sequence of positive random variables satisfying (i) and (ii). Let
Tn := n
k=1 Wk and let
Nt(x) := #

n
 Tn(x) ≤t

.

Nt

is an increasing family of random variables and, moreover, the map
t →Nt(x) is right-continuous. In fact, let tk ↓t. Then the sets Ek := {n |
Tn+1(x) > tk} is an increasing family of sets, Ek ⊂Ek+1 ∀k and E := ∪kEk =

n | Tn+1(x) > t

. Thus, Nt(x) = inf E = limk(inf Ek) = limk Ntk(x) and

x ∈
 Nt(x) = k

=

x ∈
 Tk(x) ≤t < Tk+1(x)

.
Thus, each Nt, t ≥0, is a random variable and
P(Nt = k) = P(Tk ≤t < Tk+1) = P(t < Tk+1) −P(t < Tk)
= e−αt (αt)k
k! ,
see Exercise 4.65. Then one proves that

Nt(x)

is a Poisson process of
parameter α. We omit the proof as the proof of independence of the increments,
(iii) of Deﬁnition 6.2, is technical. We refer the interested reader to, e.g. [5].
Summarizing, we have the following.
Theorem 6.4 With the previous notation, the following are equivalent:
(i)

Nt

t≥0 is a Poisson process with intensity α.
(ii) The waiting time random variables

Wk

, that measure the time between
two consecutives jumps of Nt(x), are pairwise independent and identically
distributed with exponential distribution exp(α).

246
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
6.2
Continuous time Markov chains
6.2.1
Deﬁnitions
6.2.1.1
Transition probabilities matrix
A continuous time stochastic process is a one real parameter family

Xt

t≥0 of
random variables on a probability space (, E, P). We only consider stochastic
processes with a ﬁnite or denumerable state-space S.
For each s, t ≥0 and each i, j ∈S, let P(s, t) := (P(s, t)i
j) be the matrix
deﬁned by
Pi
j(s, t) :=
⎧
⎨
⎩
P

Xt = j
 Xs = i

if P(Xs = i) > 0,
δi
j
if P(Xs = i) = 0.
We call the matrix P(s, t) the transition probabilities matrix from Xs to Xt.
Observe that P(s, t) is a stochastic matrix.
The law of total probability yields
P(Xt = j) =

i∈S
P(Xs = i)P(s, t)i
j,
(6.3)
thus, denoting by π(t) = (πj(t)), πj(t) := P(Xt = j), the mass density row vec-
tor of Xt, (6.3) reads equivalently as
π(t) = π(s)P(s, t).
(6.4)
Of course, Pi
j(t, t) = δi
j if P(Xt = i) > 0 so P(t, t) = Id ∀t ≥0.
Finally, by an induction argument, from (6.4) one gets for any n ∈N and any
0 ≤t0 ≤t1 ≤· · · ≤tn,
π(tn) = π(t0)P(t0, t1)P(t1, t2) · · · P(tn−1, tn).
6.2.1.2
Homogeneous processes
Deﬁnition 6.5 A continuous time stochastic process

Xt

t≥0 with values in a ﬁnite
or denumerable state-space S is called homogeneous if for any s, t, h ≥0 and
for any i, j ∈S
P

Xt+h = j
 Xs+h = i

= P

Xt = j
 Xs = i

.
For the transition probabilities matrices P(s, t) of the process we then have
P(s, t) = P(0, t −s)
∀0 ≤s ≤t.
The map t →P(t) := P(0, t) is called the transition probabilities map of the
process.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
247
6.2.1.3
Markovian processes
Deﬁnition 6.6 (Markov property) Let

Xt

t≥0 be a continuous time stochastic
process with a ﬁnite or denumerable state-space S. The process

Xt

t≥0 is called
Markovian if, for any positive integer n, for any 0 ≤t0 < t1 < · · · < tn, and any
i1, . . . , in ∈S we have
P

Xtn = in
 Xtn−1 = in−1, . . . , Xt0 = i0

= P

Xtn = in
 Xtn−1 = in−1

(6.5)
whenever P(Xtn−1 = in−1, . . . , Xt0 = i0) > 0.
With the same reasoning used for discrete time Markov chains, the process

Xt

t≥0 is Markovian if and only if for any 0 ≤r ≤s ≤n, for any 0 ≤t0 ≤t1 ≤
· · · ≤tn and any triplet of events G, F, E detected by Xt0, . . . , Xtr, Xtr+1, . . . , Xts
and Xts+1, . . . , Xtn, respectively, we have
P

E
 F ∩G

= P

E
 F

or
P

E ∩F
 G

= P

E
 F

P

F
 G

.
(6.6)
It is also still possible to compute the joint distribution of a ﬁnite number
of the random variables Xt’s in terms of the transition matrices. Let k, n ≥0,
0 ≤t0 ≤t1 ≤· · · ≤tn+k. Denote
Ak :=

x ∈
 Xtk = ik

.
By (6.6) one gets
P

An ∩· · · ∩A1
 A0

= P

An
 An−1

P

An−1 ∩· · · ∩A1
 A0

.
Thus, by induction,
P

An ∩· · · ∩A1
 A0

=
n−1
=
k=0
P

Ak+1
 Ak

i.e.
P

Xtn = in, . . . , Xt0 = i0

= P(Xt0 = i0)P(t0, t1)i0
i1 · · · P(tn−1, tn)in−1
in
.
(6.7)
Remark 6.7 Since the probabilities of the events detected by the Xt’s are actually
deﬁned by the value of the probabilities of ﬁnite intersections of the generat-
ing events, see Theorem B.6, property (6.5) or, equivalently, (6.6) impact on

248
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
the whole set of the events detected by the Xt’s. As in the discrete case, the
Markov property says that the probabilistic prevision of the future state of the
chain depends on the past history only through its present state. Also, formula
(6.7) implies that the transiton matrices ﬁx, in principle, the probability of any
event detected by the random variables

Xt

. However, we must be careful,
since not every subset of  determined by the Xt’s is an event, in general. For
instance, the set
E =

x ∈
 Xτ(x) = i ∀τ, r ≤τ ≤s

needs not be an event, since E is the intersection of a non denumerable family
of events.
6.2.2
Continuous semigroups of stochastic matrices
6.2.2.1
Chapman–Kolmogorov equations
Proposition 6.8 Let

Xt

t≥0 be a Markovian stochastic process with ﬁnite or
denumerable state-space and transition probabilities matrix P(s, t). Then, the
Chapman–Kolmogorov equations hold

P(s, t) = P(s, τ)P(τ, t)
∀s, τ, t such that 0 ≤s < τ < t,
P(s, s) = Id
∀s ≥0.
If the process is homogeneous and P(t) denotes the transition probabilities map
of the process, then the Chapman–Kolmogorov equations reduce to

P(t + s) = P(t)P(s)
∀s, t ≥0,
P(0) = Id.
Proof. By the Markov property,
P(s, t)i
j = P

Xt = j
 Xs = i

=

h∈S
P

Xt = j, Xτ = h
 Xs = i

=

h∈S
P

Xt = j
 Xτ = h

P

Xτ = h
 Xs = i

=

h∈S
P(τ, t)h
jP(s, τ)i
h = (P(s, τ)P(τ, t))i
j.
6.2.2.2
Right-continuous Markov processes
Examples show that no regularity can be expected for the transition probabil-
ities map (s, t) →P(s, t) of stochastic processes. However, for a subclass of
stochastic processes, the situation is better.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
249
Deﬁnition 6.9 A stochastic process

Xt

t≥0 on (, E, P) is said to be right-
continuous if the trajectories t →Xt(x) are right-continuous for P-almost every
x ∈.
Proposition 6.10 Let

Xt

t≥0 be a right-continuous stochastic process with a
ﬁnite state-space S. Then for any s ≥0
P(s, t) →Id
as t →s+.
In particular, the transition probabilities map of a homogeneous process t →P(t)
is continuous at 0,
P(t) →Id
as t →0+.
Proof. Let

tn

be such that tn ↓s. Observe that since the states are ﬁnite,
for any x ∈ the map t →Xt(x) is right-continuous at s if and only if there
exists n = n(x) such that Xtn(x) = i for any n ≥n. In other words, denoting by
E ⊂ the set of points x ∈ such that the map t →Xt(x) is right-continuous,
we have

x ∈E
 Xs(x) = i

=
∞
(
n=1
'
k≥n

x ∈
 Xtk(x) = i

,
hence the events

x ∈
 Xs(x) = i

and
∞
(
k=1
'
k≥n

x ∈
 Xtk(x) = i

differ by a null set. Therefore, assuming P(Xs = i) > 0,
1 = P(s, s)i
i = P

Xs = i
 Xs = i

= lim
n→∞P
 '
k≥n

x ∈ | Xtk(x) = i
  Xs = i

≤lim
n→∞P

Xtn = i
 Xs = i

= lim
n→∞P(s, tn)i
i.
Since obviously P(s, tn)i
i ≤1, we get P(s, tn)i
i →1. Thus, since the sequence

tn

is arbitrary, P(s, t)i
i →1 as t →s+. Furthermore, since for any i, j ∈S
with i ̸= j, we have
P(s, t)i
j = P

Xt = j
 Xs = i

≤P

Xt ̸= i
 Xs = i

= 1 −P(s, t)i
i,
we also deduce that P(s, t)i
j →0 as t →s+.

250
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Remark 6.11 Let

Xt

t≥0 be a right-continuous stochastic process with ﬁnite
state-space S and let D be a denumerable dense set in R. Repeating the argument
in the proof of Proposition 6.10 one gets for every s ∈R+, for every i ∈S and
every sequence

tn

⊂D such that tn ↓s, that

x ∈
 Xs(x) = i

and
∞
(
k=1
'
k≥n

x ∈
 Xtk(x) = i

differ by a null set. In particular, the generating events

x ∈ | Xs(x) = i

are
generated by the events detected by the random variables

Xs, s ∈D

. A similar
argument shows that any ﬁnite intersection of generating events is detected by
the random variables

Xs, s ∈D

. We then infer that the P-completions of the
σ-algebra of the events detected by all the Xt’s and of the σ-algebra of the events
detected by the Xt’s with t ∈D coincide, see Theorem B.6.
Deﬁnition 6.12 A homogeneous, Markovian and right-continuous stochastic pro-
cess

Xt

t≥0 is called a right-continuous Markov chain.
By combining Propositions 6.10 and 6.8 we get the following.
Theorem 6.13 Let

Xt

t≥0 be a right-continuous Markov chain with a ﬁnite state-
space S. Then the associated transition map t →P(t), t ≥0, is a continuous
semigroup, i.e.
⎧
⎪⎨
⎪⎩
P(t) →P(0)
as t →0+,
P(t + s) = P(t)P(s)
for every t, s ≥0,
P(0) = Id.
(6.8)
The theory of ordinary differential equations then applies. In particular,
Theorem C.10 yields the following.
Theorem 6.14 Let

Xt

t≥0 be a right-continuous Markov chain with transition
probabilites matrix P(t), t ≥0. Then P(t) is differentiable, and, if
Q := P′
+(0) := lim
t→0+
P(t) −Id
t
,
(6.9)
then
P(t) = eQt =
∞

k=0
Qktk
k!
∀t ≥0.
(6.10)
Actually, (6.8) are equivalent to (6.10). Also, it is well known that P(t) = eQt is
the unique solution of both the systems

P′(t) = QP(t),
P(0) = Id
and

P′(t) = P(t)Q,
P(0) = Id.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
251
The matrix Q, which is the inﬁnitesimal generator of the semigroup P(t),
expresses the rate of change of P(t). We call Q the matrix of the intensity
factors or the intensities matrix of the (transition probabilities map of the) chain.
Example 6.15 At the initial stage 0 we wait for a random length of time that
follows the exponential distribution with intensity α, then we move to state 1.
There again, we wait for a random length of time that follows the same distri-
bution exp(α) then we move to the state 2 and so on. This is the description of
the Poisson process of parameter α, see Theorem 6.4. Its transition matrix and
its intensities matrix are
P(t) =
⎛
⎜⎝
e−αt
1 −e−αt
0
. . .
0
e−αt
1 −e−αt
. . .
...
...
...
...
and
Q =
⎛
⎜⎜⎜⎝
−α
α
0
0
. . .
0
−α
α
0
. . .
0
0
−α
α
. . .
...
...
...
...
...
.
Example 6.16 At the initial stage we are, for certain time in a state 1 and
move to states 2 and 3 after a random time following the distribution exp(2)
and exp(4), respectively. We move from state 2 to state 3 with a random time
following the distribution exp(5). Then the intensities matrix Q is
Q =
⎛
⎝
−6
2
4
0
−5
5
0
0
0
⎞
⎠.
6.2.2.3
Q-matrices
Deﬁnition 6.17 A matrix Q ∈MN,N(R) is called a Q-matrix if
Qi
j ≥0
∀i ̸= j,
N

j=1
Qi
j = 0
∀i.
In particular, all the diagonal entries of Q are nonpositive, Qi
i ≤0, and
||Q||∞:=
max
i,j=1,...,N |Qi
j| =
max
i=1,...,N(−Qi
i).
Q-matrices provide an useful tool in order to deﬁne stochastic matrices. Let
Q be a N × N Q-matrix. For α ≥||Q||∞deﬁne
R := Rα = Id + 1
α Q.

252
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
A trivial computation shows that R is stochastic. Moreover, since Q = α(R −Id),
we get
P(t) = eQt = e−αteαRt = e−αt
∞

k=0
αkRk tk
k!.
(6.11)
Proposition 6.18 Let Q ∈MN,N(R). Then the matrix P(t) = eQt is stochastic for
any t > 0 if and only if Q is a Q-matrix. Moreover, for any i, j ∈{1, . . . , N} the
following are equivalent:
(i) there exists t0 > 0 such that P(t0)i
j > 0;
(ii) P(t)i
j > 0 for any t > 0.
In particular, P(t)i
i > 0 for any state i and any positive t. Finally, if there exists
t0 > 0 such that P(t0) is irreducible, then P(t)i
j > 0 for any i, j ∈{1, . . . , N} and
any positive t > 0.
Before proving the proposition, let us point out that the proposition asserts
that the evolution of a continuous time Markov chain is much more regular than
the evolution of a discrete time one. Periodicity issues that could not be neglected
in the discrete case do not appear in the continuous time case.
Proof of Proposition 6.18. Assume P(t) is stochastic for any t ∈R. Since
P(t) = eQt ∀t, then Q = limt→0+ P(t)−Id
t
from which we easily infer that Q is a
Q-matrix. Conversely, let Q be a Q-matrix. Let α ≥||Q||∞and R := Id + 1
αQ.
By (6.11) P(t) is the sum of a series with non-negative coefﬁcients
P(t) = e−αt
∞

k=0
αktkRk
k!
.
(6.12)
Since the matrix Rk is stochastic for any k, we get P(t)i
j ≥0 for any t > 0 and
for any i, j ∈{1, . . . , N}. Moreover,
N

j=1
P(t)i
j = e−αt
∞

k=0
αktk
k!
 N

j=1
(Rk)i
j

= e−αt
∞

k=0
αktk
k!
= 1.
Let us show the equivalence between claims (i) and (ii). Obviously, (ii) ⇒(i).
It remains to prove that (i) ⇒(ii). If, by contradiction, P(t)i
j = 0 for some
positive t, then by (6.12) (Rk)i
j = 0 for any k and P(t)i
j = 0 ∀t ≥0.
Since P(t) = eQt, then for any t > 0 and for any positive integer k we have
P(t)k = P(kt). If for some t0 > 0 the matrix P(t0) is irreducible, then for any
i, j ∈{1, . . . N} there exists k = k(i, j) such that P(kt0)i
j = (P(t0)k)i
j > 0. There-
fore equivalence between claims (i) and (ii) yields P(t)i
j > 0 for any t > 0 and
for any i, j.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
253
Proposition 6.19 Let Q be a Q-matrix. Then 0 is an eigenvalue of Q. Any other
eigenvalue of Q has strictly negative real part.
Proof. For any i = 1, . . . , N, let ri := −Qi
i ≥0. By Exercise 5.67 the eigen-
values of Q are in ∪N
i=1B(−ri, ri), where B(x, r) denote the closed ball of
radius r centered at x. Thus, all the eigenvalues but 0 of Q have strictly negative
real part. Moreover, if 0 were not an eigenvalue of Q, then all the eigenval-
ues of Q would have strictly negative real part, so that P(t) →0 as t →∞,
a contradiction since P(t) is stochastic for every t.
Example 6.20 Assume the matrix Q is deﬁned as
Q =
⎛
⎜⎜⎝
−0.5
0.5
0
0
0.5
−1.5
1
0
0
0
−2
2
0
0
3
−3
⎞
⎟⎟⎠
and let α := 4. Then
4R = 4Id + Q =
⎛
⎜⎜⎝
3.5
0.5
0
0
0.5
2.5
1
0
0
0
2
2
0
0
3
1
⎞
⎟⎟⎠.
Figure 6.1 shows a representation of both matrices Q and R.
6.2.2.4
Asymptotic behaviour
Using the Banach ﬁxed point theorem, it is easy to discuss the asymptotic
behaviour of P(t) := eQt as t →+∞when Q is a Q-matrix.
1
2
3
4
.5
.5
1
2
3
1
2
3
4
1/8
3/4
1/8
1/4
1/2
7/8
5/8
1/2
1/4
(a)
(b)
Figure 6.1
Graphs representing the matrices (a) Q and (b) R in Example 6.20.

254
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
Lemma 6.21 Let Q be a N × N Q-matrix, let P(t) := eQt, t ≥0, and let w be a
stochastic vector, w ∈T . The following hold:
(i) If for some t0 > 0 the vector w is the only solution of wP(t0) = w, then
wQ = 0.
(ii) wQ = 0 if and only if w = wP(t) for any positive t.
Proof. (i) If w is the only ﬁxed point of the map x →xP(t0), x ∈T , then for
any n, w is a ﬁxed point for x →xP(t0/n), since P(t0/n)n = P

n t0
n

= P(t0).
Therefore, for any positive integer n
w = weQt0/n = w + t0
n wQ + o
1
n

.
Since n is arbitrary, then wQ = 0.
(ii) If wQ = 0, then wQk = 0 for any positive integer k ≥1. Thus, for any t > 0
wP(t) = w
∞

k=0
Qktk
k!
= w +
∞

k=1
wQktk
k!
= w.
Conversely, if w = wP(t) for any positive t, then
w = weQt = w + t wQ + o(t)
as t →0+,
so that wQ = 0.
Theorem 6.22 Let P(t) = eQt be stochastic for any positive t. The following are
equivalent:
(i) There exists t0 > 0 such that P(t0) is irreducible.
(ii) There exists w ∈T whose components are all different from zero such that
xP(t) →w for any x ∈T .
In this case, w is the unique solution in T of wQ = 0.
Proof.
(i)
⇒
(ii).
If
P(t0)
is
irreducible,
then
P(t)i
j > 0
for
any
i, j ∈{1, . . . , N} and for any positive t, see Proposition 6.18. Thus x →xP(t)
is a contraction map on T , see Proposition 5.53. Let C(t) < 1 be its contraction
coefﬁcient. By the Banach ﬁxed point theorem x →xP(t) has a unique ﬁxed
point w(t) ∈T and, recalling that P(kt) = P(t)k, there exists A > 0 such that
for any x ∈T and any k ≥1
|xP(kt) −w(t)| ≤A C(t)k.
(6.13)
(i) of Lemma 6.21 yields w(t)Q = 0 and (ii) of Lemma 6.21 yields that for any
s > 0 w(t) is also a ﬁxed point for the map x →xP(s). Since a contraction has
a unique ﬁxed point, we conclude that w(t) = w(s) =: w for any positive s, t.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
255
Let us now show that xP(t) →w for any x ∈T . From (iv) of Proposition
5.53 we can assume that the contraction coefﬁcient C(t) is continuous with
respect to t. Thus we may assume that C(t) ≤C < 1 for any t ∈[1, 2]. Let
s ≥1. Applying (6.13) with k := ⌊s⌋and t := s/k yields
|xP(s) −w| ≤A C(t)k ≤A C⌊s⌋.
Thus xP(s) →w as s →∞.
(ii) ⇒(i) If xP(t) →w for any x ∈T , then all the components of w are nonzero,
see Exercise 5.65. Thus there exists a large enough t0 such that P(t0) is regular.
Consequently, P(t) is regular for any postive t, by Proposition 6.18.
6.2.2.5
Computing eQt
Quite often in applications one needs to compute the exponential matrix
P(t) = eQt, given Q. Various methods can be found in the literature, see e.g.
[21], having different degrees of efﬁciency, accuracy and usefulness either
from the analytical, numerical or symbolic viewpoint. Formula (6.11) proves to
be quite efﬁcient in computing P(t) = eQt when Q is a Q-matrix: it permits
to compute, for ﬁxed t, good approximations of P(t) with explicitly bounded
errors simply by truncation of the power series on the right-hand side.
Proposition 6.23 (Uniformization method) Let Q be a N × N Q-matrix and let
P(t) = eQt. Let α ≥||Q||∞, R = Id + 1
αQ, and
PM(t) = e−αt
M

k=0
αkRk tk
k!.
Then
||P(t) −PM(t)|| ≤
(αt)M+1
(M + 1)! .
where ||A|| := maxi
N
j=1 |Ai
j|.
Proof. Since R is stochastic, then so are the matrices Rk’s for every k, hence
||Rk|| = 1. Thus the elementary estimate
ex −
n

k=0
xk
k!
 ≤ex
xn+1
(n + 1)!
∀x ≥0,
gives
||P(t) −PM(t)|| ≤e−αt
∞

k=M+1
αk||Rk||tk
k! = e−αt
∞

k=M+1
(αt)k
k!
≤e−αteαt
(αt)M+1
(M + 1)! =
(αt)M+1
(M + 1)! .

256
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
6.2.3
Examples of right-continuous Markov chains
At the initial stage 0 we wait for a random length of time that follows the
exponential distribution with intensity α1, then we move immediately to the
state 1. We then wait again independently for a random length of time that follows
the exponential distribution exp(α2) and we move immediately to the state 2 and
so on. This way one describes the counting process

Nt

t≥0 associated with a
point process in time.
If moreover

Yn

is a discrete Markov chain, and the families of random
variables

Yn

and

Nt

t≥0 are independent of each other, then it can be shown
that

Xt

t≥0, Xt = YNt is a right-continuous Markov chain.
We do not discuss the argument in its full generality and refer the interested
reader to the specialized literature, see e.g. [22–24]. Here we consider the sim-
plest case in which all the exponential distributions are the same, so that the
counting process

Nt

t≥0 is the Poisson process of intensity α, see Theorem 6.4.
Thanks to the uniformization method, we have the following.
Theorem 6.24 (Uniformization)
Let Q be a N × N Q-matrix and let
P(t) := eQt. Let α ≥||Q||∞and R := Id + Q/α. Let

Yn

be a discrete time
homogeneous Markov chain with ﬁnite state-space S, |S| = N, on a probability
space (, E, P) and transition matrix R, and let

Nt

t≥0 be a Poisson
process with intensity α on (, E, P). If the sets of random variables

Yn

and

Nt

t≥0 are independent, then the stochastic process

Xt

t≥0 deﬁned by
Xt(x) := YNt(x)(x) is a right-continuous Markov chain with transition matrix
P(t) := eQt.
Lemma 6.25 Let

Nt

t≥0 be an integer valued right-continuous stochastic process
on (, E, P). Assume that Nt :  →N has the following properties:
(i) N0(x) = 0 and Nt is monotone nondecreasing, Ns(x) ≤Nt(x) for a.e. x
for any s, t with 0 ≤s ≤t.
(ii)

Nt

t≥0 is time homogeneous, i.e. Nt −Ns = Nt−s for any 0 ≤s ≤t.
(iii)

Nt

t≥0 has independent increments, i.e. for any 0 ≤rn ≤· · · ≤r1 ≤s ≤t
and for any n ≤q ≤p
P

Nt = p
 Ns = q, Nr1 = q1, . . . , Nrn = qn

= P(Nt−s = p −q).
Let

Yn

be a homogeneous discrete time Markov chain with a ﬁnite or denumer-
able state-space S on (, E, P). If

Nt

t≥0 and

Yn

are each other independent,
then the stochastic process

Xt

t≥0,
Xt(x) := YNt(x)(x)
is a right-continuous Markov chain.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
257
Proof. Let us prove that

Xt

t≥0 is a homogeneous process. Let s < t and
i, j ∈S. From the properties (ii) and (iii) of

Nt

t≥0
P(Nt = p, Ns = q) = P(Nt −Ns = p −q, Ns = q)
= P(Nt−s = p −q)P(Ns = q).
Moreover, since

Yn

is a homogeneous Markov chain, for non-negative integers
p, q with p ≥q we have
P(Yp = j, Yq = i) = P

Yp−q = j
 Y0 = i

P(Yq = i).
Therefore
P(Xt = j, Xs = i) =

p≥q
P(Yp = j, Yq = i, Nt = p, Ns = q)
=

p≥q
P(Yp = j, Yq = i)P(Nt = p, Ns = q)
=

p≥q
P

Yp−q = j
 Y0 = i

P(Yq = i)P(Nt−s = p −q)P(Ns = q)
=
 
ℓ≥0
P

Yℓ= j
 Y0 = i

P(Nt−s = ℓ)
 
q≥0
P(Yq = i)P(Ns = q)

= P

Xt−s = j
 X0 = i

P(Xs = i).
i.e.
P

Xt = j
 Xs = i

= P

Xt−s = j
 X0 = i

.
(6.14)
Let us now prove that

Xt

t≥0 has the Markov property. Let r < s < t and
let h, i, j, ∈S. Then Nt(x) ≥Ns(x) ≥Nr(x), and for any non-negative integers
p, q, n such that p ≥q ≥n we get
P(Nt = p, Ns = q, Nr = n) = P(Nt−s = p −q)P(Ns = q, Nr = n)
P(Yp = j, Yq = i, Yn = h) = P

Yp−q = j
 Y0 = i

P(Yq = i, Yn = h).
We have
P(Xt = j, Xs = i, Xr = h)
=

p≥q≥n≥0
P(Yp = j, Yq = i, Yn = h, Nt = p, Ns = q, Nr = n)
=

p≥q≥n≥0
P(Yp = j, Yq = i, Yn = h)P(Nt = p, Ns = q, Nr = n)

258
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
=

p≥q≥n
P

Yp−q = j
 Y0 = i

P(Yq = i, Yn = h)
P(Nt−s = p −q)P(Ns = q, Nr = n)
=
 
λ≥0
P

Yl = j
 Y0 = i

P(Nt−s = l)

 
q≥n≥0
P(Yq = i, Yn = h)P(Ns = q, Nr = n)

= P

Xt−s = j
 X0 = i

P(Xt = i, Xr = h),
therefore, taking also into account (6.14), we ﬁnally get
P

Xt = j
 Xs = i, Xr = h

= P

Xt−s = j
 X0 = i

= P

Xt = j
 Xs = i

.
Similarly one proceeds to get
P

Xt = j
 Xs1 = i1, . . . , Xin = in

= P

Xt = j
 Xs1 = i1

.
Proof of Theorem 6.24. By Lemma 6.25 Xt(x) = YNt(x)(x) is a right-continuous
Markov chain.
Let S(t) be the transition matrix of

YNt

. Since N0(x) = 0 for a.e. x ∈,
we have
S(t)i
j : = P

YNt(x)(x) = j
 YN0(x)(x) = i

=
1
P(Y0(x) = i)
∞

k=0
P

Yk(x) = j, Y0(x) = i, Nt(x) = k

=
∞

k=0
P

Yk(x) = j
 Y0(x) = i

P

Nt(y) = k

=
∞

k=0
(Rk)i
je−αt αktk
k!
= e−αteαRt = eα(−Id+R)t = eQt = P(t).
Remark 6.26 Notice that Theorem 6.24 reduces the existence of a right-
continuous Markov chain with a given Q-matrix Q to the existence of a Poisson
process of suitable intensity α > ||Q||∞and of a discrete time Markov chain
with transition matrix R := Id + Q/α.

AN INTRODUCTION TO CONTINUOUS TIME MARKOV CHAINS
259
Example 6.27 Poisson processes are right-continuous Markov chains. This is a
special case of Lemma 6.25.
6.2.4
Holding times
It can be shown that the holding times between two transitions of a right-
continuous Markov chain with a ﬁnite state-space are independent and follow
exponential distributions of possibly different intensities. We do not enter into
the whole argument, which is well-described in the literature, see e.g. [22–24]
and restrict ourselves to discuss the ﬁrst holding time.
Let

Xt

t≥0 be a homogeneous right-continuous stochastic process with a
ﬁnite or denumerable state-space S. Let P(t) be the transition matrix of the
process. For any state i ∈S let SJi be the holding time in state i, deﬁned for
any x ∈ as
SJi(x) := inf

t
 Xt(x) ̸= i

.
Proposition 6.28 The holding time in a state i ∈S is a random variable. For any
t > 0 and any n ≥0, let tj := jt/2n, j = 0, . . . , 2n −1. Then
P(SJi(x) > t) = lim
n→∞P

Xtj (x) = i ∀j, 0 ≤j ≤2n −1

.
Proof. Let D be a denumerable dense set of R+ and let SD(x) :=
inf

t ∈D
 Xt(x) ̸= i

. Observe that
SJi(x) = SD(x)
if the trajectory t →Xt(x) is right-continuous. In fact, trivially SJi(x) ≤SD(x).
Moreover, let

tn

⊂D be such that tn ↓SJi(x). Since t →Xt(x) is right-
continuous, then Xt ̸= i, t := SJi(x) and Xtn ̸= i for large enough n. Therefore
SD(x) ≤tn for large enough n and passing to the limit as n →∞, one gets
SD(x) ≤SJi(x).
Since the process is right-continuous, by the above the sets

x ∈
 SJi(x) > t

and

x ∈
 SD(x) > t

differ by a null set. Observe now that for t > 0
t < SD(x)
if and only if
Xs(x) = i ∀s ∈D, 0 ≤s ≤t
for a.e. x. Consequently,

x ∈
 SJi(x) > t

and

x ∈
 Xs(x) = i ∀s ∈D, 0 ≤s ≤t


260
A FIRST COURSE IN PROBABILITY AND MARKOV CHAINS
differ by a null set and
P(SJi(x) > t) = P

Xs(x) = i ∀s ∈D, 0 ≤s ≤t

.
In particular, SJi(x) is a random variable on (, E, P).
Fix now t and choose D as D := {jt/2n}j,n. For any n, set Dn := {jt/2n}j.
Trivially, Dn ⊂Dn+1 ∀n and D = ∪∞
n=1Dn. Moreover,
P(SJi(x) > t) = P

Xs(x) = i ∀s ∈D, 0 ≤s ≤t

= P
 ∞
'
n=1

x ∈
 Xtj (x) = i ∀j, 0 ≤j ≤2n −1

= lim
n→∞P

Xtj (x) = i ∀j, 0 ≤j ≤2n −1

where tj = tj,n := tj/2n.
Proposition 6.29 Let

Xt

t≥0 be a homogeneous right-continuous Markov chain
with ﬁnite state-space S. Let Q = (qij) be its intensities matrix. Then the holding
time in a state i ∈S follows the exponential distribution of parameter −qii.
Proof. By Proposition 6.28 SJi is a random variable and
P(SJi(x) > t) = lim
n→∞P

Xtj (x) = i ∀j, 0 ≤j ≤2n −1

where tj = tj,n := tj/2n. Let P(t) = (pij(t)) be the transition probabilities matrix
of the process. Thus, by the Markov property and the homogeneity of the chain
we get
P(Xtj = i ∀j = 0, . . . , 2n −1) =
2n−1
=
j=0
P

Xtj+1 = i
 Xtj = i

=

pii
 t
2n
2n
.
Thus, since P(t) = (pij(t)) is right-differentiable at 0 and P′
+(0) = Q, we con-
clude
log P(SJi > t) = lim
n→∞2n log pii
 t
2n

= lim
k→∞k

pii
 t
k

−1

= qiit.

Appendix A
Power series
We recall some results from the theory of power series which should already
be known to the reader. Proofs and further results can be found in textbooks on
Analysis, see e.g. [25, 26].
A.1
Basic properties
Let

an

be a sequence of complex numbers. The power series centred at zero
with coefﬁcients

an

is
∞

n=0
anzn := a0 +
∞

n=1
anzn,
z ∈C,
i.e. the sequence of polynomials

sn(z)

n
sn(z) =
n

k=0
akzk := a0 +
n

k=1
akzk,
z ∈C.
Given a power series ∞
n=0 anzn, the non-negative real number ρ deﬁned by
1
ρ := lim sup
n→∞
n
|an|,
where 1/0+ = +∞and 1/(+∞) = 0, is called the radius of convergence of the
series ∞
n=0 anzn. In fact, one proves that the sequence

sn(z)

is absolutely
convergent if |z| < ρ and it does not converge if |z| > ρ. It can be shown that
ρ > 0 if and only if the sequence

|an|

is not more than exponentially increasing.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

262
APPENDIX A
In this case, the sum of the series
A(z) :=
∞

n=0
anzn
is deﬁned in the disc {|z| < ρ}.
Power series can be integrated and differentiated term by term. More pre-
cisely, the following holds.
Theorem A.1 Let A(z) = ∞
n=0 anzn be the sum of a power series with radius
of convergence ρ. Then the series ∞
n=0 nanzn−1 and ∞
n=0 an
zn+1
n+1 have the
same radii of convergence ρ. If ρ > 0, then A(z) is holomorphic in B(0, ρ),
A′(z) =
∞

n=0
nanzn−1,
and, given any piecewise C1 curve γ : [0, 1] →B(0, ρ),
*
γ
A(z) dz =
∞

n=0
an
γ (1)n+1 −γ (0)n+1
n + 1
Then Corollary A.2 easily follows.
Corollary A.2 Let A(z) = ∞
n=0 anzn be the sum of a power series of radius of
convergence ρ > 0. Then
an = DnA(0)
n!
.
Thus, if ρ > 0 the sum A(z), |z| < ρ, completely determines the sequence

an

. The function A(z) is called the generating function of the sequence

an

.
In other words,

an

and A(z) := ∞
n=0 anzn contain the same ‘information’
(we are assuming that

an

grows at most exponentially or, equivalently that
ρ > 0): we can say that the function A(z), |z| < ρ, is a new ‘viewpoint’ on the
sequence

an

.
By Corollary A.2 if A(z) = ∞
n=0 anzn and B(z) = ∞
n=0 bnzn are the sums
of two power series with positive radii of convergence, that coincide near zero,
then an = bn ∀n, both series have the same radius ρ, and A(z) = B(z) for any
z such that |z| < ρ.
Example A.3 (Geometric series) The geometric series is the most classical
example of power series. It generates the sequence {1, 1, 1, . . . }
∞

n=0
zn

APPENDIX A
263
and its sum is
S(z) :=
∞

n=0
zn =
1
1 −z,
|z| < 1.
Example A.4 (Exponential) Taking advantage of Taylor expansions, one can
prove that
∞

n=0
zn
n! = ez,
z ∈C.
Example A.5 (Logarithm) Replacing z with −z in the equality
1
1−z = ∞
n=0 zn
|z| < 1, we get
1
1 + z =
∞

n=0
(−1)nzn,
|z| < 1.
Integrating along the interval [0, x] ⊂R, we get
log(1 + x) =
* x
0
1
1 + t dt
=
* x
0
∞

n=0
(−1)ntn dt
=
∞

n=0
(−1)n xn+1
n + 1,
|x| < 1.
(A.1)
A.2
Product of series
Deﬁnition A.6 The convolution product of two sequences a =

an

and b =

bn

is the sequence {a ∗b}n deﬁned for n = 0, 1, . . . by
(a ∗b)n :=

i+j=n
aibj =
n

j=0
ajbn−j.
In the ﬁrst sum we sum over all couples (i, j) of non-negative integers such that
i + j = n.
The ﬁrst terms are
(a ∗b)0 = a0b0,
(a ∗b)1 = a0b1 + a1b0,
(a ∗b)2 = a0b2 + a1b1 + a2b0,
. . . .
The convolution product is commutative, associative and bilinear. Moreover
the following holds.

264
APPENDIX A
Theorem A.7 (Cauchy) Let a =

an

and b =

bn

be two sequences and
let A(z) = ∞
n=0 anzn and B(z) = ∞
n=0 bnzn be the sums of the associated power
series deﬁned for |z| < ρa and |z| < ρb, respectively. Then the power series of
their convolution products converges for any z such that |z| < min(ρa, ρb) and
∞

n=0
(a ∗b)nzn = A(z)B(z)
∀z, |z| < min(ρa, ρb).
A.3
Banach space valued power series
Let V be a Banach space with norm || || and let

fn

⊂V . Then one may consider
the series, with values in V ,
∞

k=0
fkzk =

n

k=0
fkzk
n.
(A.2)
Let ρ ≥0 be deﬁned by
1
ρ := lim sup
n→∞
n
||fn||.
As in the case of power series with complex coefﬁcients, the power series in
(A.2) absolutely converges in V for any z ∈C, |z| < ρ. Thus the sum of the
series is a well deﬁned function on the disc |z| < ρ of the complex plane with
values in V
F(z) :=
∞

k=0
fkzk ∈V,
|z| < ρ.
As for complex valued power series, one differentiates term by term Banach
valued power series: the sum F(z) is a holomorphic function on the open disc
|z| < ρ and
F ′(z) :=
∞

k=0
kfkzk ∈V,
|z| < ρ.
As a special case, one considers the space MN,N(C) of N × N complex
matrices with norm
||F|| := sup
x∈CN
x̸=0
|Fx|
|x| ,
called the maximum expansion coefﬁcient of F. It is easy to prove that
|Fz| ≤||F|| |z|,
||Fn|| ≤||F||n
∀n,

APPENDIX A
265
and that MN,N(C) equipped with this norm is a Banach space. Given a sequence

Fn

⊂MN,N(C), the associated power series
∞

n=0
Fnzn
(A.3)
converges if |z| < ρ where
1
ρ := lim sup
n→∞
n
||Fn||.
From the above, we have the following:
(i) ρ > 0 if and only if the sequence

||Fn||

grows at most exponentially fast.
(ii) If |z| < ρ, then the series (A.3) converges, both pointwise and absolutely,
to a matrix F(z) ∈MN,N(C),
F(z) :=
∞

n=0
Fnzn,
|z| < ρ,
i.e. we have ∀i, j ∈{1, . . . , N} the complex valued limits
Fi
j(z) =
∞

n=0
(Fn)i
jzn,
|z| < ρ.
A.3.1.1
Power expansion of the inverse of a matrix
Let
P ∈MN,N(C)
be
a
N × N
square
matrix
with
complex
entries.
If ||P|| |z| < 1, then ∞
n=0 ||P||n|z|n =
1
1−||P|| |z| < ∞. Thus the series ∞
n=0 Pnzn
converges, both pointwisely and absolutely, to a matrix S(z) := ∞
n=0 Pnzn. For
any positive n we can write
(Id −zP)
n

k=0
Pkzk =
n

k=0
(Pkzk −Pk+1zk+1) = Id −Pn+1zn+1
so that


Id −zP

n

k=0
Pkzk
−Id
 = ||Pn+1|| |z|n+1 ≤||P||n+1|z|n+1.
Since ||P|| |z| < 1, as n →∞we get

Id −zP

S(z) = Id.

266
APPENDIX A
Therefore, we conclude that, if |z| <
1
||P||, then Id −zP is invertible and
(Id −zP)−1 = S(z) =
∞

k=0
Pkzk.
A.3.1.2
Exponential of a matrix
Let Q ∈MN,N(C). The radius of convergence of the power series
∞

n=0
Qn
n! zn
is +∞, so that, for any z ∈C the power series ∞
n=0
Qn
n! zn converges, both
absolutely and pointwisely, to a matrix denoted by eQz,
eQz :=
∞

n=0
Qn
n! zn
∀z ∈C.
(A.4)
Proposition A.8 The following hold:
(i) eQ0 = Id.
(ii) eQz and Q commute.
(iii)

eQz′
= QeQz for any z ∈C.
(iv) eQ(z+w) = eQzeQw for any z, w ∈C.
(v) eQz is invertible and its inverse is (eQz)−1 = e−Qz.
(vi) (eQz)n = eQnz for any z ∈C and any n ∈Z.
Proof. Properties (i) and (ii) are a direct consequence of the deﬁnition of eQz.
Property (iii) follows differentiating term by term the series ∞
n=0
Qn
n! zn. Property
(iv) is a consequence of the formula for the product of power series: in fact,
eQzeQw =
∞

k=0
Qk
k! zk
∞

k=0
Qk
k! wk =
∞

n=0
n

k=0
QkQn−kzkwn−k
k!(n −k)!
=
∞

n=0
Qn
n!
n

k=0
n
k

zkwn−k =
∞

n=0
Qn
n! (z + w)n = eQ(z+w).
Finally, properties (v) and (vi) are particular cases of (iv).

APPENDIX A
267
A.3.2
Exercises
Exercise A.9 Prove the Newton binomial theorem:
(i) directly, with an induction argument on n;
(ii) taking advantage of the Taylor expansions;
(iii) starting from the formula D((1 + z)n) = n(1 + z)n−1;
(iv) starting from the identity et(x+y) = etxety.
Exercise A.10 Prove the following equalities:
n

j=0
2j
n
j

= 3n,
n

j=0
 n
2j

=
n

j=0

n
2j + 1

= 2n−1,
n

j=0
(−1)j
n
j
2
=

(−1)n/2 n
n/2

if n is even,
0
if n is odd,
n

j=a
j
a

=
n + 1
a + 1

,
∞

j=0
α + j
j

xj =
1
(1 −x)α+1 ,
2n
n

= (−4)n
−1/2
n

.
Exercise A.11 Show that
n

j=0
(−1)j
n
j
n + m −j
k −j

=
n
k

if m ≥k,
0
if m < k.
Exercise A.12 Let

an

be a complex valued sequence such that

|an|

grows at
most exponentially fast. Let A(z) = ∞
n=0 anzn, |z| < ρ be its generating function.
Compute the generating function of the following sequences:
•

αa0, αa1, αa2, αa3, . . .

, α ∈C,
•

a0, 0, a1, 0, a2, 0, . . .

,

268
APPENDIX A
•

a0, 0, a2, 0, a4, 0, . . .

,
•

a1, 0, a3, 0, a5, 0, . . .

,
•

0, 0, 0, a0, a1, a2, a3, . . .

,
•

a3, a4, a5, a6, . . .

,
•

a0, 2a1, 3a2, 4a3, 5a4, . . .

,
•

a0, a1/2, a2/3, a3/4, a4/5, . . .

,
•

a0, a0 + a1, a1 + a2, a2 + a3, . . .

,
•

a0 + a1, a1 + a2, a2 + a3, a3 + a4, . . .

,
•

a0, a0 + a1, a0 + a1 + a2, a0 + a1 + a2 + a3, . . .

.
Exercise A.13 Compute p
j=0(−1)jn
j

.
0
(−1)pn−1
p
1
.
Solution.
We prove this equality directly. Since for any p = 1, . . . , n −1 we have
n
p

=
n−1
p

+
n−1
p−1

, one gets
p

j=0
(−1)j
n
j

= 1 −n +
p

j=2
(−1)j
n −1
j

+
p−1

j=1
(−1)j+1
n −1
j

= 1 −n +
p−1

j=2
(−1)jn −1
j

−
n −1
j

+
n −1
1

+ (−1)p
n −1
p

= 1 −n + 0 + (n −1) + (−1)p
n −1
p

= (−1)p
n −1
p

.
The same result can be obtained via generating functions. From the formula
for the product of two power series, we get
∞

p=0

p

j=0
(−1)p−j
n
j

xp =
 ∞

j=0
(−x)j ∞

j=0
n
j

xj
=
1
1 + x (1 + x)n = (1 + x)n−1 =
n−1

p=0
n −1
p

xp.
The claim follows by the identity principle for polynomials.

APPENDIX A
269
Exercise A.14 For any p, q, k ≥0, prove Vandermonde formula
k

j=0
p
j

q
k −j

=
p + q
k

.
(A.5)
Solution. We prove the equality (A.5) by using generating functions. From
the formula for the product of power series, we get
∞

k=0

k

j=0
p
j

q
k −j

zk =
 ∞

k=0
p
k

zk ∞

k=0
q
k

zk
= (1 + z)p+q =
∞

k=0
p + q
k

zk,
hence the claim.
Exercise A.15 Show that ∞
j=0
p
j
 q
j−1

=
p+q
p−1

∀p, q ≥0.
Solution. Applying the Vandermonde formula (A.5), we get
∞

j=0
p
j

q
j −1

=
∞

j=0
p
j

q
q −j + 1

=
p + q
q + 1

=
p + q
p −1

.
Exercise A.16 Show that n
k=0
n
k
2 =
2n
n

.
Solution. Applying Vandermonde formula we get
n

k=0
n
k
2
=
n

k=0
n
k

n
n −k

=
2n
n

.

Appendix B
Measure and integration
The axiomatic approach to probability by Andrey Kolmogorov (1903–1987)
makes essential use of the measure theory. In this appendix we review the aspects
of the theory that are relevant to us. We do not prove everything and refer the
interested reader for proofs and further study to one of the many volumes on this
now classic subject, see e.g. [7, 27].
B.1
Measures
B.1.1
Basic properties
Here  shall denote a generic set. For a generic subset E of , Ec :=  \ E
denotes the complement of E in  and P() denotes the family of all subsets of
. A family E of subsets of  is then a subset of P(), E ⊂P(). We say that
a family E ⊂P() of subsets of a set  is an algebra if ∅,  ∈E and E ∪F,
E ∩F and Ec ∈E whenever E, F ∈E.
Deﬁnition B.1 We say that E is a σ-algebra if E is an algebra and for every
sequence of subsets

Ek

⊂E we also have ∪kEk and ∩kFk ∈E.
In other words, if we operate on sets of a σ-algebra with differences, countable
unions or intersections, we get sets of the same σ-algebra: we also say that a
σ-algebra
is
closed
with
respect
to
differences,
countable
unions
and
intersections.
Let D ⊂P() be a family of subsets of . It is readily seen that the class
S :=
' 
E
 E is a σ-algebra, E ⊃D

is again a σ-algebra, hence the smallest σ-algebra containing D. We say that S
is the σ-algebra generated by D.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

APPENDIX B
271
Deﬁnition B.2 The smallest σ-algebra B ⊂P(Rn) containing the open sets of
Rn is called the σ-algebra of Borel sets.
Deﬁnition B.3 A measure on  is a couple (E, P) of a σ-algebra E ⊂P() and
of a map P : E →R+ with the following properties:
(i) P(∅) = 0.
(ii) (Monotonicity) If A, B ∈E with A ⊂B, then P(A) ≤P(B).
(iii) (σ-additivity) For any disjoint sequence

Ai

⊂E we have
P
 ∞
(
i=1
Ai

=
∞

i=1
P(Ai).
Obviously (iii) reduces to ﬁnite additivity for pairwise disjoint subsets if E is
ﬁnite. When E is inﬁnite, the inﬁnite sum on the right-hand side is understood
as the sum of a series of non-negative terms. From Deﬁnition B.3 we easily get
the following.
Proposition B.4 Let (E, P) be a measure on . We have:
(i) If A ∈E, then 0 ≤P(A).
(ii) If A, B ∈E with A ⊂B, then P(B \ A) + P(A) = P(B).
(iii) If A, B ∈E then P(A ∪B) + P(A ∩B) = P(A) + P(B).
(iv) If A, B ∈E then P(A ∪B) ≤P(A) + P(B).
(v) (σ-subadditivity) For any sequence

Ai

⊂E we have
P
 ∞
(
i=1
Ai

≤
∞

i=1
P(Ai).
(B.1)
(vi) (Disintegration formula) If

Di

⊂E is a partition of , then for every
A ⊂,
P(A) =
∞

i=1
P(A ∩Di).
(B.2)
(vii) (Continuity)
(a) If

Ei

⊂E with Ei ⊂Ei+1 ∀i, then ∪∞
i=1Ei ∈E and
P(∪∞
i=1Ei) = lim
i→∞P(Ei).
(B.3)
(b) Let

Ei

⊂E be such that Ei ⊃Ei+1 ∀i. Then ∩∞
i=1Ei ∈E and more-
over, if P(E1) < +∞, then
P(∩∞
i=1Ei) = lim
i→∞P(Ei).
(B.4)

272
APPENDIX B
Proof. (i)–(vi) follow trivially from the deﬁnition of measure. Let us prove
claim (a) of (vii). Since P(Ek) ≤P(∪kEk) for every k, the claim is trivial if for
some k P(Ek) = +∞. We may therefore assume P(Ek) < ∞for all k. We set
E := ∪kEk and decompose E as
E = E1
(  ∞
(
k=2
(Ek \ Ek−1)

.
The sets E1 and Ek \ Ek−1, k ≥1, are of course in E and pairwise disjoint.
Because of the σ-additivity of P we then have
P(E) = P(E1) +
∞

k=2
P(Ek \ Ek−1)
= P(E1) +
∞

k=2
(P(Ek) −P(Ek−1)) = lim
k→∞P(Ek).
Claim (b) of (vii) easily follows. In fact, since P(E1) < +∞and Ek ⊂E1, we
have P(Ek) = P(E1) −P(E1 \ Ek) for all k. Since

E1 \ Ek

is an increasing
sequence of sets, we deduce from (a) that
P(E1) −lim
k→∞P(Ek) = lim
k→∞P(E1 \ Ek)
= P
 (
k
(E1 \ Ek)

= P(E1) −P
 '
k
Ek

.
Let (E, P) be a measure on . We say that N ⊂ is P-negligible, or simply
a null set, if there exists E ∈E such that N ⊂E and P(E) = 0. Let )E be the
collection of all the subsets of  of the form F = E ∪N where E ∈E and N
is P-negligible. It is easy to check that )E is a σ-algebra which is called the
P-completion of E. Moreover, setting P(F) := P(E) if F = E ∪N ∈)E, then
()E, P) is also a measure on  called the P-completion of (E, P). It is often
customary to consider measures as P-complete measures.
B.1.2
Construction of measures
B.1.2.1
Uniqueness
Let (E, P) be a measure on . The σ-additivity property of P suggests that the
values of P on E are in fact determined by the values of P on a restricted class
of subsets of E.
Deﬁnition B.5 A family D ⊂P() of subsets of  is said to be closed under
ﬁnite intersections if A, B ∈D implies A ∩B ∈D.
A set function α : I ⊂P() →R+ is σ-ﬁnite if there exists a sequence

Ik

⊂I such that  = ∪kIk and α(Ik) < ∞∀k.

APPENDIX B
273
We have the following coincidence criterion. A proof can be found in, e.g. [7].
Theorem B.6 (Coincidence criterion) Let (E1, P1) and (E2, P2) be two measures
on  and let D ⊂E1 ∩E2 be a family that is closed under ﬁnite intersections.
Assume that P1(A) = P2(A) ∀A ∈D and that there exists a sequence

Dh

∈D
such that  = ∪hDh and P1(Dh) = P2(Dh) < +∞for any h. Then P1 and P2
coincide on the σ-algebra generated by D.
Corollary B.7 (Uniqueness of extension) Let  be an open set, let I be a family
of subsets of  closed under ﬁnite intersections and let α : I →R+ be σ-ﬁnite.
Then α has at most one extension μ : E →R+ to the σ-algebra E generated by
I such that (E, μ) is a measure.
B.1.2.2
Carath´eodory Method I
We now present the so-called Method I for constructing measures.
Let I be a family of subsets of  containing the empty set, and let α :
I →R+ be a set function such that α(∅) = 0. For any E ⊂ set
μ∗(E) := inf
 ∞

i=1
α(Ii)
 ∪i Ii ⊃E, Ii ∈I
>
.
(B.5)
Of course, we set μ∗(E) = +∞if no covering of E by subsets in I exists. It
is easy to check that μ∗(∅) = 0, that μ∗is monotone increasing and that μ∗is
σ-subadditive, i.e.
μ∗ ∞
(
i=1
Ei

≤
∞

i=1
μ∗(Ei)
for every denumerable family

Ei

of subsets of .
We now deﬁne a σ-algebra M on which μ∗is σ-additive. A ﬁrst attempt
is to choose the class of sets on which μ∗is σ-additive, i.e. the class of sets E
such that
μ∗(B ∪E) = μ∗(E) + μ∗(B)
for every subset B disjoint from E, or, equivalently such that
μ∗(A ∩E) + μ∗(A ∩Ec) ≤μ∗(A)
∀A, A ⊃E.
(B.6)
However, in general, this class is not a σ-algebra. Following Carath´eodory, a
localization of (B.6) sufﬁces. A set E ⊂ is said to be μ∗-measurable if
μ∗(A ∩E) + μ∗(A ∩Ec) = μ∗(A)
∀A ∈P()
and the class of μ∗-measurable sets will be denoted by M.
Theorem B.8 (Carath´eodory) M is a σ-algebra and μ∗is σ-additive on M. In
other words, (M, μ∗) is a measure on .

274
APPENDIX B
Without additional hypotheses both on I and α, we might end up with I not
included in M or with a μ∗that is not an extension of α.
Deﬁnition B.9 A family I ⊂P() of subsets of  is a semiring if:
(i) ∅∈I.
(ii) For any E, F ∈I we have E ∩F ∈I.
(iii) If E, F ∈I, then E \ F = ∪N
j=1Ij where the Ij’s are pairwise disjoint ele-
ments in I.
Notice that, if E, F ∈I, then E ∪F decomposes as E ∪F = ∪n
j=1Ij where
I1, . . . , In belong to I and are pairwise disjoint.
Theorem B.10 (Carath´eodory) Let I ⊂P() be a semiring of subsets of , let
α : I →R+ be a σ-additive set function such that α(∅) = 0 and let (M, μ∗) be
the measure constructed by the above starting from I and α. Then:
(i) I ⊂M.
(ii) μ∗extends α to M.
(iii) Let E ⊂ with μ∗(E) < ∞. Then E ∈M if and only if E = ∩kFk \ N
where μ∗(N) = 0,

Fk

is a decreasing sequence of sets Fk and, for k ≥1,
Fk is a disjoint union Fk = ∪jIk,j of sets Ik,j ∈I.
Assume α : I →R+ be such that I is a semiring, α is σ-additive and σ-ﬁnite
and let E be the σ-algebra generated by I. From Theorems B.6 and B.10 the
following easily follows:
• (iii) of Theorem B.10 implies that M is the μ∗-completion of E: every set
A ∈M has the form A = E ∪N where E ∈E and μ∗(N) = 0.
• Corollary B.7 imples that (E, μ∗) is the unique measure that extends α
to E.
B.1.2.3
Lebesgue measure in Rn
A right-closed interval I of Rn, n ≥1, is the product of n intervals closed to
the right and open to the left, I = -n
i=1]ai, bi]. The elementary volume of this
interval is |I| := -n
i=1(bi −ai).
An induction argument on the dimension n shows that I is a semiring. For
instance, let n = 2 and let A, B, C, D be right-closed intervals on R. Then
(A × B) \ (C × D) =

(A \ C) × (B \ D)

∪

(A \ C) × (B ∩D)

∪

(A ∩C) × (B \ D)

.
The family of right-closed intervals of Rn will be denoted by I. We know
that B(Rn) is the σ-algebra generated by I, see Exercise B.16. Since I is trivially

APPENDIX B
275
closed under ﬁnite intersections, we infer from Theorem B.6 that two measures
that coincide on I and that are ﬁnite on bounded open sets coincide on every
Borel set E ∈B(Rn).
Proposition B.11 The volume map | | : I →R+ is a σ-additive set function.
Proof. It is easily seen that the elementary measure | | is ﬁnitely additive
on intervals. Let us prove that it is σ-subadditive. For that, let I, Ik be intervals
with I = ∪kIk and, for ϵ > 0 and any k denote by Jk an interval centred as
Ik that contains strictly Ik with |Jk| ≤|Ik| + ϵ 2−k. The family of open sets

int(Jk)

k covers the compact set I, hence we can select k1, k2, . . . kN such that
I ⊂∪N
i=1int(Jki) concluding
|I| ≤
N

i=1
|Jki| ≤
∞

k=1
|Jk| ≤
∞

k=1
(|Ik| + ϵ2−k) ≤
∞

k=1
|Ik| + 2ϵ,
i.e. that || is σ-subadditive on I.
Suppose now that I = ∪kIk where the

Ik

’s are pairwise disjoint. Of
course, by the σ-subadditivity property of ||, |I| ≤∞
k=1 |Ik|. On the other
hand, ∪N
k=0Ik ⊂I for any integer N. Finite additivity then yields
N

k=0
|Ik| =

N
(
k=0
Ik
 ≤|I|
∀N
and, as N →∞, also the opposite inequality ∞
k=0 |Ik| ≤|I|.
Taking advantage of Proposition B.11, Theorem B.10 applies. We get the
existence of a unique measure (B(Rn), Ln) that is ﬁnite on bounded open sets,
called the Lebesgue measure on Rn, that extends to Borel sets the elementary
measure of intervals. From (B.5) we also get a formula for the measure of a
Borel set E ∈B(Rn),
Ln(E) = inf
 ∞

k=1
|Ik|
 Ik intervals, E ⊂
∞
(
k=1
Ik
>
.
(B.7)
B.1.2.4
Stieltjes–Lebesgue measure
Proposition B.12 Let F : R →R be a right-continuous and monotone nonde-
creasing function. Then the set function ζ : I →R deﬁned by ζ(]a, b]) := F(b) −
F(a) on the class I of right-closed intervals is σ-additive.
Proof. Obviously, ζ is additive and monotone increasing, hence ﬁnitely sub-
additive, on I. We now prove that ζ is σ-additive. Let

Ii

⊂I, Ii :=]xi, yi],
be a disjoint partition of I :=]a, b]. Since ζ is additive, we get
∞

i=1
ζ(Ii) ≤ζ(I).

276
APPENDIX B
Let us prove the opposite inequality. For ϵ > 0, let

δi

be such that F(yi +
δi) ≤F(yi) + ϵ 2−i. The open intervals ]xi, yi + δi[ form an open covering of
[a + ϵ, b], hence ﬁnitely many among them cover again [a + ϵ, b]. Therefore,
by the ﬁnite subadditivity of ζ,
F(b) −F(a + ϵ) ≤
N

k=1
(F(yik + δik) −F(xik)
=
N

k=1
(ζ(Iik) + ϵ 2−ik) ≤
∞

i=1
ζ(Ii) + ϵ.
Letting ϵ go to zero, we conclude
ζ(I) = F(b) −F(a) ≤
∞

i=1
ζ(Ii).
Example B.13 If F is not right-continuous, the set function ζ : I →R,
ζ(]a, b]) := F(b) −F(a) is not in general subadditive. For instance, for
0 ≤a ≤1, set
F(t) = Fa(t) =
⎧
⎪⎨
⎪⎩
0
if −1 ≤t < 0,
a
if t = 0,
1
if 0 < t ≤1.
Let I = {] −1, 0]} ∪

]
1
j+1, 1
j ]

j, clearly ∪I∈II =] −1, 1], but
1 = F(1) −F(−1) = ζ
 (
I∈I
I

̸=

I∈I
ζ(I) = F(0) −F(−1) = a
as soon as a < 1.
Theorem B.14 (Lebesgue) The following hold:
(i) Let (B(R), P) be a ﬁnite measure on R. Then the law F(t) := P(] −∞,
t]),
t ∈R,
is
real
valued,
monotone
nondecreasing
and
right-
continuous.
(ii) Let F : R →R be a real valued, monotone nonderecreasing and right-
continuous function. Then there exists a unique measure (B(R), P) ﬁnite
on bounded sets of R such that
P(]a, b]) = F(b) −F(a)
∀a, b, a < b.
Proof. (i) F is real valued since (B(R), P) is ﬁnite on bounded Borel sets.
Moreover, monotonicity property of measures implies that F is monotone
nondecreasing. Let us prove that F is right-continuous. Let t ∈R and let

tn


APPENDIX B
277
be a monotone decreasing sequence such that tn ↓t. Since ] −∞, t] =
D∞
n=1] −∞, tn]
and
P
is
ﬁnite,
one
gets
F(tn) = P(] −∞, tn]) →
P(] −∞, t]) = F(t) by the continuity property of measures.
(ii) Assume F(t) is right-continuous and monotone nondecreasing. Let I be
the semiring of right-closed intervals. The set function ζ : I →R+, ζ(]a, b]) :=
F(b) −F(a) is σ-additive, see Proposition B.12. Therefore Theorem B.6 and
B.10 apply and ζ extends in a unique way to a measure on the σ-algebra generated
by I, i.e. on B(R), that is ﬁnite on bounded open sets.
The measure (B(R), P) in Theorem B.14 is called the Stieltjes–Lebesgue
measure associated with the right-continuous monotone nondecreasing func-
tion F.
B.1.2.5
Approximation of Borel sets
Borel
sets are quite complicated if compared with open sets that are simply
denumerable unions of closed cubes with disjoint interiors. However, the follow-
ing holds.
Theorem B.15 Let (B(Rn), P) be a measure on Rn that is ﬁnite on bounded open
sets. Then for any E ∈B(Rn)
P(E) = inf

P(A)
 A ⊃E, A open

,
(B.8)
P(E) = sup

P(F)
 F ⊂E, F closed

.
(B.9)
In particular, if E ∈B(Rn) has ﬁnite measure, P(E) < +∞, then for every ϵ > 0
there exists an open set  and a compact set K such that K ⊂E ⊂ and P( \
K) < ϵ.
Although the result can be derived from (iii) of Theorem B.10, it is actually
independent of it. We give here a proof that does not use Theorem B.10.
Proof. Step 1. Let us prove the claim assuming P is ﬁnite. Consider the family
A :=

E Borel
 (B.8) holds true for E

.
Of course, A contains the family of open sets. We prove that A is closed under
denumerable unions and intersections. Let

Ej

⊂A and, for ϵ > 0 and j =
1, 2, . . . , let Aj be open sets with Aj ⊃Ej and P(Aj) ≤P(Ej) + ϵ 2−j, that
we rewrite as P(Aj \ Ej) < ϵ 2−j since Ej and Aj are measurable with ﬁnite
measure. Since
 (
j
Aj

\
 (
j
Ej

⊂
(
j
(Aj \ Ej),
 '
j
Aj

\
 '
j
Ej

⊂
(
j
(Aj \ Ej),
we infer
P(A \ ∪jEj) ≤ϵ,
P(B \ ∩jEj) ≤ϵ,
(B.10)

278
APPENDIX B
where
A := ∪jAj
and
B := ∩jAj.
Since
A
is
open
and
A ⊃∪jEj,
the
ﬁrst
inequality
of
(B.10)
yields
∪jEj ∈A.
On
the
other
hand,
CN := ∩N
j=1Aj is open, contains ∩jEj and, by the second inequality of (B.10),
P(CN \ ∩jEj) ≤2 ϵ for sufﬁciently large N. Therefore ∩jEj ∈A.
Moreover, since every closed set is the intersection of a denumerable family
of open sets, A also contains all closed sets. In particular, the family
˜A :=

A ∈A, Ac ∈A

is a σ-algebra that contains the family of open sets. Consequently, A ⊃˜A ⊃
B(Rn) and (B.8) holds for all Borel sets of .
Since (B.9) for E is (B.8) for Ec, we have also proved (B.9).
Step 2. Let us prove (B.8) and (B.9) for measures that are ﬁnite on bounded open
sets. Let us introduce the following notation: given a Borel set A ⊂Rn, deﬁne
the restriction of P to A as the set function
P
A(E) := P(A ∩E)
∀E ∈B(Rn).
(B.11)
It is easily seen that (B(Rn), P
A) is a measure on Rn that is ﬁnite
if P(A) < ∞.
Let us prove (B.8). We may assume P(E) < +∞since otherwise (B.8) is
trivial. Let Vj := B(0, j) be the open ball centred at 0 or radius j. The measures
P
Vj are Borel and P
Vj(Rn) = P(Vj) < +∞. Step 1 then yields that for
any ϵ > 0 there are open sets Aj with Aj ⊃E and P
Vj(Aj \ E) < ϵ 2−j. The
set A := ∪j(Aj ∩Vj) is open, A ⊃E and, by the subadditivity of P
P(A \ E) = P(∪j((Aj ∩Vj) \ E))
≤
∞

j=1
P((Aj ∩Vj) \ E) ≤
∞

j=1
P
Vj(Aj \ E) ≤ϵ.
Let us prove (B.9). The claim easily follows applying Step 1 to the measure
P
E if P(E) < +∞. If P(E) = +∞, then E = ∪jEj with Ej measurable
and P(Ej) < +∞, then for every ϵ > 0 and every j there exists a closed set Fj
with Fj ⊂Ej and P(Ej \ Fj) < ϵ 2−j. The set F := ∪jFj is contained in E and
P(E \ F) ≤P(∪j(Ej \ Fj)) ≤
∞

j=i
P(Ej \ Fj) ≤ϵ,
hence, for sufﬁciently large N, GN := ∪N
i=1Fi is closed and P(E \ GN) < 2ϵ.
Step 3. By assumption, E ∈B(Rn) and P(E) < +∞. By Step 2 for each ϵ > 0
there exists an open set  and a closed set F such that F ⊂E ⊂ and

APPENDIX B
279
P() ≤P(E) + ϵ, P(E) ≤P(F) + ϵ, so that P() −P(F) ≤2ϵ. Setting K :=
F ∩B(0, n) with large enough n, we still get
P(E) ≤P(K) + 2ϵ,
thus concluding that A ⊃E ⊃K and P(A \ K) < 3ϵ.
B.1.3
Exercises
Exercise B.16 Show that B(R) is the smallest σ-algebra generated by one of the
following families of sets:
• the closed sets;
• the open intervals;
• the closed intervals;
• the intervals ]a, b], a, b ∈R, a < b;
• the intervals [a, b[, a, b ∈R, a < b;
• the closed half-lines ] −∞, t], t ∈R.
Solution. [Hint. Show that any open set can be written as the union of an at
most denumerable family of intervals.]
Exercise B.17 The law of a ﬁnite measure (B(Rn), P) on Rn is deﬁned by
F(t) = F(t1, . . . , tn) := P(] −∞, t1] × · · · ×] −∞, tn]).
Show that two ﬁnite measures (B(Rn), P) and (B(Rn), Q) on Rn coincide if and
only if the corresponding laws agree.
B.2
Measurable functions and integration
Characterizing the class of Riemann integrable functions and understanding the
range of applicability of the fundamental theorem of calculus were the problems
that led to measures and to a more general notion of integral due to Henri
Lebesgue (1875–1941). The approach we follow here, which is very well adapted
to calculus of probability, is to start with a measure and deﬁne the associated
notion of integral.
The basic idea is the following. Suppose one wants to compute the area of the
subgraph of a non-negative function f : R →R. One can do it by approximating
the subgraph in two different ways, see Figure B.1. One can take partitions of the

280
APPENDIX B
(a)
(b)
Figure B.1
The computation of an integral according to (a) Riemann and
(b) Lebesgue.
x axis, and approximate the integral by the area of a piecewise constant function
as we do when deﬁning the Riemann integral, or one can take a partition of the y
axis, and approximate the area of the subgraph by the areas of the strips. The
latter deﬁnes the area of the subgraph as
* b
a
f (x) dx = lim
N→∞
1
2N
∞

k=1
|Ef,k 2−N|,
(B.12)
where
Ef,t :=

x ∈R
 f (x) > t

,
t ∈R,
is the t upper level of f and |Ef,t| denotes its ‘measure’. Since t →|Ef,t| is
monotone nonincreasing, hence Riemann integrable, (B.12) suggests deﬁning the
integral by means of the Cavalieri formula
Lebesgue
* b
a
f (x) dx := Riemann
* ∞
0
|Ef,t| dt
(B.13)
From this point of view, it is clear that the notion of integral makes essential
use of a measure on the domain, that must be able to measure even irregular
sets, since the upper levels can be very irregular, for instance if the function
is oscillating.
In the following, instead of deﬁning the integral by means of (B.13), we adopt
a slightly more direct approach to the integral and then prove (B.13).
B.2.1
Measurable functions
Deﬁnition B.18 Let E be a σ-algebra of subsets of a set . We say that f :  →R
is E-measurable if for any t ∈R we have

x ∈
 f (x) > t

∈E.
There are several equivalent ways to say that a function is E-measurable.
Taking advantage of the fact that E is a σ-algebra, one proves that the following
are equivalent:
(i) {x ∈ | f (x) > t} ∈E for any t.
(ii) {x ∈ | f (x) ≥t} ∈E for any t.

APPENDIX B
281
(iii) {x ∈ | f (x) ≤t} ∈E for any t.
(iv) {x ∈ | f (x) < t} ∈E for any t.
Moreover, in the previous statements one can substitute ‘for any t’ with ‘for any
t in a dense subset of R’, in particular, with ‘for any t ∈Q’.
Since any open set of R is an at most denumerable union of intervals, the
following are also equivalent:
(i) {x ∈ | f (x) > t} ∈E for any t.
(ii) For any open set A ⊂R we have f −1(A) ∈E.
(iii) For any closed F ⊂R we have f −1(F) ∈E.
(iv) For any Borel set B ⊂R we have f −1(B) ∈E.
The three last statements are independent of the ordering relation of R. They
suggest the following extension.
Deﬁnition B.19 Let E be a σ-algebra of subsets of a set . A vector valued
function f :  →RN, N ≥1, is E-measurable if one of the following holds:
(i) For any open set A ⊂RN we have f −1(A) ∈E.
(ii) For any closed set F ⊂RN we have f −1(F) ∈E.
(iii) For any Borel set B ⊂RN we have f −1(B) ∈E.
In general, not every function is E-measurable. However, since E is a
σ-algebra, one can prove that the algebraic manipulations as well as the
pointwise limits of E-measurable functions always result in E-measurable
functions. For instance, if f and g are E-measurable and α ∈R, then the
functions
f + g,
fg,
1
f ,
αf,
|f |,
max(f, g),
min(f, g)
are E-measurable. Moreover, let

fn

be a sequence of E-measurable functions.
Then:
• The functions
sup
n
fn(x),
inf
n fn(x),
lim inf
n→∞fn(x),
lim sup
n→∞
fn(x)
are E-measurable.

282
APPENDIX B
• Let E ⊂ be the set
E :=

x ∈
 ∃lim
n→∞fn(x)

and let f (x) := limn→∞fn(x), x ∈E. Then E ∈E and for any t ∈R we
have {x ∈E | f (x) > t} ∈E.
Recalling that a function φ : X →Y between metric spaces is continuous
if and only if for any open set A ⊂Y the set φ−1(A) ⊂X is open, we get
immediately the following:
• Continuous functions g : RN →Rm are B(RN)-measurable,
• Let f :  →RN be E-measurable and let φ : RN →Rm be B(RN)-
measurable, then φ ◦f is E-measurable.
In particular, (ii) implies that |f |p, log |f |, . . . are E-measurable functions if f is
E-measurable and that f :  →RN, f = (f 1, . . . , f N) is E-measurable if and
only if its components f 1, . . . , f N are E-measurable.
Let (E, P) be a measure on . A simple function ϕ :  →R is a function
with a ﬁnite range and E-measurable level sets, that is,
ϕ(x) =
k

j=1
ajχEj (x),
Ej :=

x | ϕ(x) = aj

∈E.
The class of simple functions will be denoted by S. Simple functions being linear
combinations of E-measurable functions are E-measurable.
Proposition B.20 (Sampling) Let E be a σ-algebra of subsets of a set . A
non-negative function f :  →R+ is E-measurable if and only if there exists a
nondecreasing sequence

ϕk

of non-negative simple functions such that ϕk(x) ↑
f (x) ∀x ∈.
Proof. Let f be the pointwise limit of a sequence

ϕk

of simple functions.
Since every ϕk is E-measurable, then f is E-measurable.
Conversely, let f :  →R be a function. By sampling f , we then construct
a sequence

ϕk

of functions with ﬁnite range approaching f , see Figure B.1.
More precisely, let Ek :=

x | f (x) > 2k
and for h = 0, 1, , . . . 4k −1, let
Ek,h :=

x | h/2k < f (x) ≤(h + 1)/2k
.
Deﬁne ϕk :  →R as
ϕk(x) =

2k
if x ∈Ek,
h
2k
if x ∈Ek,h.
(B.14)
By deﬁnition, ϕk(x) ≤f (x), moreover, ϕk(x) ≤ϕk+1(x) ∀x ∈, since
passing from k to k + 1 we half the sampling intervals. Let us prove that
ϕk(x) →f (x) ∀x. If f (x) = +∞, then ϕk(x) = 2k ∀k, hence ϕk(x) →+∞=

APPENDIX B
283
f (x). If f (x) < +∞, then for sufﬁciently large k, f (x) ≤2k, hence there exists
h ∈

0, . . . , 4k −1

such that x ∈Ek,h. Therefore,
f (x) −ϕk(x) ≤h + 1
2k
−h
2k = 1
2k .
Passing to the limit as k →∞we get again ϕk(x) →f (x).
The previous construction applies to any non-negative function f :  →R+.
To conclude, notice that if f is E-measurable, then the sets Ek and Ek,h are
E-measurable for every k, h. Since
ϕk(x) =
4k−1

h=0
h
2k χEk,h(x) + 2kχEk(x)
∀x ∈,
ϕk is a simple function.
B.2.2
The integral
Let (E, P) be a measure on . For any simple function ϕ :  →R, one deﬁnes
the integral of ϕ with respect to the measure (E, P) as
I(ϕ) :=
k

j=1
ajP(ϕ = aj),
(B.15)
as intuition suggests. Since a priori P(ϕ = aj) may be inﬁnite, we adopt the
convention that ajP(ϕ = aj) := 0 if aj = 0. Notice that the integral may be
inﬁnite.
We then deﬁne the integral of a non-negative E-measurable function with
respect to (E, P) as
*

f (x) P(dx) := sup

I(ϕ)
 ϕ simple, ϕ(x) ≤f (x) ∀x ∈

.
(B.16)
For a generic E-measurable function f :  →R, decompose f as f (x) =
f+(x) −f−(x) where
f+(x) := max(f (x), 0),
f−(x) := max(−f (x), 0),
and deﬁne
*

f (x) P(dx) :=
*

f+(x) P(dx) −
*

f−(x) P(dx)
(B.17)
provided that at least one of the integrals on the right-hand side of (B.17) is
ﬁnite. In this case one says that f is integrable with respect to (E, P). If both
the integrals on the right-hand side of (B.17) are ﬁnite, then one says that f
is summable. Notice that for functions that do not change sign, integrability is
equivalent to measurability.

284
APPENDIX B
Since |f (x)| = f+(x) + f−(x) and f+(x), f−(x) ≤|f (x)|, it is easy to check
that if f is E-measurable then so is |f |, and f is summable if and only if f is
E-measurable and
.
|f (x)| P(dx) < +∞. Moreover,

*

f (x) P(dx)
 ≤
*

|f (x)| P(dx).
The class of summable functions will be denoted by L1(, E, P) or simply by
L1() when the measure is understood.
When  = Rn, one refers to the integral with respect to the Lebesgue measure
(B(Rn), Ln) in (B.17) as the Lebesgue integral.
Finally, let f :  →R be a function and let E ∈E. One says that f is
measurable on E, integrable on E, and summable on E if f (x)χ E(x) is E-
measurable, integrable, and summable, respectively. If f is integrable on E,
one sets
*
E
f (x) P(dx) :=
*

f (x)χE(x) P(dx).
(B.18)
In particular,
*
E
1 P(dx) =
*

χE(x)dx = P(E).
B.2.3
Properties of the integral
From the deﬁnition of integral with respect to the measure (E, P) and taking also
advantage of the σ-additivity of the measure one gets the following.
Theorem B.21 Let (E, P) be a measure on .
(i) For any c ∈R and f integrable on E, we have
.
E c f (x) P(dx) =
c
.
E f (x) P(dx).
(ii) (Monotonicity) Let f, g be two integrable functions such that f (x) ≤g(x)
∀x ∈. Then
*

f (x) P(dx) ≤
*

g(x) P(dx).
(iii) (Continuity, the Beppo Levi theorem) Let

fk

be a nondecreasing
sequence of non-negative E-measurable functions fk :  →R+ and
let f (x) := limk→∞fk(x) be the pointwise limit of the fk’s. Then f is
integrable and
*

f (x) P(dx) = lim
k→∞
*

fk(x) P(dx).
(B.19)
(iv) (Linearity) L1 is a vector space and the integral is a linear operator on it:
for f, g ∈L1 and α, β ∈R we have
*

(αf (x) + βg(x)) P(dx) = α
*

f (x) P(dx) + β
*

g(x) P(dx).

APPENDIX B
285
A few comments on the Beppo Levi theorem are appropriate. Notice that the
measurability assumption is on the sequence

fk

. The measurability of the limit
f is for free, thanks to the fact that E is a σ-algebra. Moreover, the integrals in
(B.19) may be inﬁnite, and the equality is in both directions: we can compute
one side of the equality in (B.19) and conclude that the other side has the same
value. The Beppo Levi theorem is of course strictly related to the continuity
property of measures, and at the end, to the σ-additivity of measures.
Proof of theorem B.21. (i) and (ii) are trivial.
(iii) Let us prove the Beppo Levi theorem. Since f is the pointwise limit of
E-measurable functions, f is E-measurable. Moreover, since fk(x) ≤f (x) for
any x ∈ and every k, from (i) we infer limk→∞
.
 fk P(dx) ≤
.
 f P(dx).
We now prove the opposite inequality
*

f (x) P(dx) ≤α := lim
k→∞
*

fk(x) P(dx).
Assume without loss of generality that α < +∞. Let φ be a simple function,
φ = N
i=1 aiχBi, Bi =

x | φ(x) = ai

∈E, such that φ ≤f and let β be a real
number, 0 < β < 1. For k = 1, 2, . . . set
Ak :=

x ∈
 fk(x) ≥β φ(x)

.

Ak

is a nondecreasing sequence of measurable sets such that ∪kAk = . Hence,
from (B.15) and the continuity property of measures
*
Ak
φ(x) P(dx) =
N

i=1
aiP(Bi ∩Ak) →
N

i=1
aiP(Bi) =
*
φ(x) P(dx)
as k →∞. On the other hand, for any k we have
β
*
Ak
φ(x) P(dx) =
*
Ak
β φ(x) P(dx) ≤
*
Ak
fk(x) P(dx)
≤
*

fk(x) P(dx) ≤α.
(B.20)
Therefore, passing to the limit ﬁrst as k →∞in (B.20) and then letting β →1−
we get
*

φ(x) P(dx) ≤α.
Since the previous inequality holds for any simple function φ below f , the
deﬁnition of integral yields
.
 f (x) P(dx) ≤α, as required.
(iv) We have already proved the linearity of the integral on the class of simple
functions, see Proposition 2.28. To prove (iv), it sufﬁces to approximate f and g
by simple functions, see Proposition B.20, and then pass to the limit using (iii).

286
APPENDIX B
We conclude with a few simple consequences.
Proposition B.22 Let (E, P) be a measure on .
(i) Let E ∈E have ﬁnite measure and let f : E →R be an integrable function
on E ∈E such that |f (x)| ≤M for any x ∈E. Then f is summable on E
and
.
E |f | P(dx) ≤M P(E).
(ii) Let E, F ∈E and let f : E ∪F →R be an integrable function on E ∪F.
Then f is integrable both on E and F and
*
E
f (x) P(dx) +
*
F
f (x) P(dx)
=
*
E∪F
f (x) P(dx) +
*
E∩F
f (x) P(dx).
(B.21)
B.2.4
Cavalieri formula
Theorem B.23 (Cavalieri formula) Let (E, P) be a measure on . For any
non-negative E-measurable function f :  →R+ we have
*

f (x) P(dx) = (Riemann)
* ∞
0
P(f > t) dt.
(B.22)
As usual, we shorten the notation P({x ∈ | f (x) > t}) to P(f > t).
Proof. Let us prove the claim for non-negative simple functions. Assume
ϕ(x) = N
i=1 aiχEi(x), where the sets

Ei

are measurable and pairwise disjoint.
For i = 1, . . . , N let αi(t) := 1[0,ai](t). For the piecewise (hence simple) function
t →P(ϕ > t) we have
P(ϕ > t) =
N

i=1
P(Ei ∩{ϕ > t}) =
N

i=1
αi(t)P(Ei)
hence, integrating with respect to t
* ∞
0
P(ϕ > t) dt =
N

i=1
 * ∞
0
αi(t) dt

P(Ei)
=
N

i=1
aiP(Ei) =
*
ϕ(x) P(dx).
Assume now f :  →R is non-negative and E-measurable. Proposition B.20
yields a nondecreasing sequence

ϕk

of non-negative simple functions such that

APPENDIX B
287
ϕk(x) ↑f (x) pointwisely. As shown before, for each k = 1, 2, . . .
*

ϕk(x) P(dx) =
* ∞
0
P(ϕk > t)dt.
Since ϕk ↑f (x) and P(ϕk > t) ↑P(f > t) as k goes to ∞, we can pass to the
limit in the previous equality using the Beppo Levi theorem to get
Lebesgue
*

f (x) P(dx) = Lebesgue
* ∞
0
P(f > t) dL1(t).
The claim then follows, since t →P(f > t), being nondecreasing, is Riemann
integrable and Riemann and Lebesgue integrals of Riemann integrable functions
coincide.
Corollary B.24 Let f :  →R be integrable and for any t ∈R, let F(t) :=
P(f ≤t). Then
*

f (x) P(dx) =
* +∞
0
(1 −F(t)) dt −
* 0
−∞
F(t) dt.
Proof. Apply (B.22) to the positive and negative parts of f and sum the
resulting equalities.
B.2.5
Markov inequality
Let (E, P) be a measure on . From the monotonicity of the integral one deduces
that for any non-negative measurable function f :  →R we have the inequality
t P(f > t) =
*
{f > t}
t P(dx) ≤
*
{f > t}
f (x) P(dx)
∀t ≥0
i.e.
P(f > t) ≤1
t
*
{f > t}
f (x) P(dx)
∀t > 0.
(B.23)
This last inequality has different names: Markov inequality, weak estimate or
Chebyshev inequality.
B.2.6
Null sets and the integral
Let (E, P) be a measure on .
Deﬁnition B.25 We
say that a set N ⊂ is a null set if there exists F ∈E
such that N ⊂F and P(F) = 0. We say that a predicate p(x), x ∈, is true for

288
APPENDIX B
P-almost every x or P-a.e., and we write ‘p(x) is true a.e.’ if the set
N :=

x ∈
 p(x) is false

is a null set.
In particular, given an E-measurable function f :  →R, we say that ‘f = 0
P-a.e.’ or that ‘f (x) = 0 for P-almost every x ∈’ if the set {x ∈ | f (x) ̸= 0}
has zero measure,
P

x ∈
 f (x) ̸= 0

= 0.
Similarly, one says that ‘|f | ≤M P-a.e.’ or that ‘|f (x)| ≤M for P-almost
every x’, if P({x ∈ | |f (x)| > M}) = 0. From the σ-additivity of the measure,
we immediately get the following.
Proposition B.26 Let (E, P) be a measure on  and let f :  →R be a
E-measurable function.
(i) If
.
 |f (x)| P(dx) < ∞, then |f (x)| < +∞P-a.e.
(ii)
.
 |f (x)| P(dx) = 0 if and only if f (x) = 0 for P-almost every x ∈.
Proof. (i) Let C :=
.
 |f (x)| P(dx). Markov inequality yields for any posi-
tive integer k
P

x ∈
 f (x) = +∞

≤P

x ∈
 f (x) > k

≤C
k
Hence, passing to the limit as k →∞we infer that P({x ∈ | f (x) =
+∞}) = 0.
(ii) If f (x) = 0 for almost every x ∈, then every simple function ϕ such that
ϕ ≤|f |, is nonzero on at most a null set. Thus
.
 ϕ(x) P(dx) = 0 and, by the
deﬁnition of the integral of |f |,
.
 |f (x)| P(dx) = 0.
Conversely, from the Markov inequality we get for any positive integer k
k P

x ∈
 |f (x)| > 1/k

≤
*

|f (x)| P(dx) = 0
so that P({x ∈ | |f (x)| > 1/k}) = 0. Since

x ∈
 |f (x)| > 0

=
(
k

x ∈
 |f (x)| > 1/k

,
passing to the limit as k →∞thanks to the continuity property of the measure,
we conclude that P({x ∈ | |f (x)| > 0}) = 0, i.e. |f (x)| = 0 P-a.e.

APPENDIX B
289
B.2.7
Push forward of a measure
Let (E, P) be a measure on  and let f :  →RN be an E-measurable function.
Since inverse images of Borel sets are E-measurable, we deﬁne a set function
Pf : B(RN) →[0, 1] on RN, also denoted by f#P, by
Pf (A) = f#P(A) := P(f −1(A))
∀A ∈B(RN),
(B.24)
called the pushforward or image of the measure P. It is easy to check the
following.
Proposition B.27 (B(RN), f#P) is a measure on RN and for every non-negative
Borel function ϕ on RN we have
*
RN ϕ(t) f#P(dt) =
*

ϕ(f (x)) P(dx).
(B.25)
Proof. We essentially repeat the proof of Theorem 3.9. For the reader’s con-
venience, we outline it again.
The σ-additivity of f#P follows from the σ-additivity of P using the De Mor-
gan formulas and the relations
f −1(∪iAi) = ∪if −1(Ai),
f −1(∩iAi) = ∩if −1(Ai),
which are true for any family of subsets

Ai

of RN.
In order to prove (B.25), we ﬁrst consider the case in which ϕ is a simple
function, ϕ(t) = n
i=1 ci1Ei(t) where c1, . . . , cn are distinct constants and the
level sets Ei :=

t | ϕ(t) = ci

, i = 1, . . . , n, are measurable. Then
ϕ ◦f (x) =
n

i=1
ci1f −1(Ei)(x),
f −1(Ei) =

x ∈ | ϕ ◦f (x) = ci

so that
*
RN g(t) f#P(dt) =
n

i=1
cif#P(Ei)
=
n

i=1
ci P(ϕ ◦f = ci) =
*

ϕ(f (x)) P(dx),
i.e. (B.25) holds when ϕ is simple.
Let now ϕ be a non-negative measurable function. Proposition B.20 yields
an increasing sequence

ϕk

of simple functions pointwisely converging to ϕ.

290
APPENDIX B
Since for every k we have already proved that
*
RN ϕk(t) f#P(dt) =
*

ϕk(f (x)) P(dx)
we can pass to the limit as k →∞and take advantage of the Beppo Levi theorem
to get (B.25).
Pushforward of measures can be composed. Let (E, P) be a measure on ,
let f :  →RN be E-measurable and let g : RN →RM be B(RN)-measurable.
Then from (B.24) we infer
(g ◦f )#P(A) = g#(f#P(A)) =: (g# ◦f#)P(A)
∀A ∈B(RM).
(B.26)
From (B.25) we infer the following relations for the associated integrals
*
RM ϕ(t) (g ◦f )#P(dt) =
*

ϕ(g(f (x))) P(dx) =
*
RN ϕ(g(s)) f#P(ds)
(B.27)
for
every
non-negative,
B(RM)-measurable
function
ϕ : RM →R,
see
Theorem 4.6.
B.2.8
Exercises
Exercise B.28 Let E be a σ-algebra of subsets of a set  and let f, g :  →R
be E-measurable. Then {x ∈ | f (x) > g(x)} ∈E.
Solution. For any rational number r ∈Q, the set Ar := {x ∈ | f (x) >
r, g(x) < r} belongs to E. Moreover,

x ∈E
 f (x) > g(x)

=
(
r∈Q
Ar.
Thus {x ∈E | f (x) > g(x)} is a denumerable union of sets in E.
Exercise B.29 Let E be a σ-algebra of subsets of a set , let E ∈E and let
f, g :  →R be two E-measurable functions. Then the function
h(x) :=

f (x)
if x ∈E,
g(x)
if x ∈Ec
is E-measurable.
Exercise B.30 Let E be a σ-algebra of subsets of a set , let f :  →R be
E-measurable and let E ∈E be such that P(E) < ∞. Then P(E ∩{f = t}) ̸= 0
for at most a denumerable set of t’s.
Exercise B.31 Show that if ϕ is a simple function, then
.
 ϕ(x) P(dx) = I(ϕ).

APPENDIX B
291
Exercise B.32 (Discrete value functions) Let (E, P) be a measure on a set .
Let X :  →R be an E-measurable non-negative function with discrete values,
i.e. X() is a countable set

ak

. Give an explicit formula for
.
 X(x) P(dx).
Solution. Let Ek :=

x ∈ | X(x) = ak

. Then
X(x) =
∞

j=1
aj1Ej (x)
∀x ∈.
Given x, the series has only one addendum since only one set Ej contains x.
If X has a ﬁnite range, then X is a simple function so that by deﬁnition
*

X(x) P(dx) =
k

j=1
ajP(X = aj).
(B.28)
If X() is denumerable, then for any non-negative integer k we have
*

k

j=1
aj1Ej (x) P(dx) =
k

j=1
ajP(X = aj).
Since X is non-negative, we can apply the Beppo Levi theorem and, as k →∞,
we get
*

X(x) P(dx) = lim
k→∞
*


k

j=1
aj1Ej (x)

P(dx)
= lim
k→∞
k

j=1
ajP(X = aj) =
∞

j=1
ajP(X = aj).
(B.29)
Formula (B.29) can also be written as
*

X(x) P(dx) =

t∈R
tP(X = t)
(B.30)
since P(X = t) = 0 if t /∈

ak

.
Exercise B.33 Let (E, P) be a measure on a set  and let X :  →R be an
E-measurable non-negative function with discrete values and such that +∞∈
X(). Give an explicit formula for
.
 X(x) P(dx).
Solution. Let

ak

∪{+∞} be the range of X. For k ≥1, let Ek := {x |
X(x) = ak}, so that
∞
(
k=1

x ∈ | X = ak

= {x ∈ | X < +∞}

292
APPENDIX B
and X(x) = ∞
j=1 aj1Ej (x) if X(x) < +∞. From Exercise B.32,
∞

i=1
aiP(X = ai) =
*
{X<+∞}
X(x) P(dx).
Moreover,
*
{X=+∞}
X(x) P(dx) =
lim
k→+∞k P(X = +∞)
=

0
if P(X = +∞) = 0,
+∞
if P(X = +∞) > 0.
Thus
*

X(x) P(dx) =
∞
i=1 aiP(X = ai)
if P(X = +∞) = 0,
+∞
if P(X = +∞) > 0.
Exercise B.34 (Integral on countable sets) Let (E, P) be a measure on a ﬁnite
or denumerable set . Denote by p :  →R, p(x) := P({x}), its mass den-
sity. Let X :  →R be a non-negative function. Give an explicit formula for
.
 X(x) P(dx).
Solution. Let

aj

be the range of X. By (B.29)
*

X(x) P(dx) =
∞

j=0
ajP(X = aj) =
∞

j=0
aj


X(x)=aj
p(x)

=
∞

j=0

X(x)=aj
X(x)p(x) =

x∈
X(x)p(x).
Exercise B.35 (Dirac delta) Let  be a set and let x0 ∈. The set function
δx0 : P() →R such that
δx0(A) =

1
if x0 ∈A,
0
otherwise,
is called the Dirac delta [named after Paul Dirac (1902–1984)] at x0, and is a
probability measure on . Prove that for any X :  →R,
*

X(x) δx0(dx) = X(x0).
(B.31)
Exercise B.36 (Sum of measures) Let (E, α) and (E, β) be two measures on 
and let λ, μ ∈R+.

APPENDIX B
293
(i) Show that λα + μβ : E →R+ deﬁned by (λα + μβ)(E) := λα(E) +
μβ(E) ∀E ∈E is such that (E, λα + μβ) is a measure on .
(ii) Show that for amy E-measurable non-negative function f :  →R+
*

f (x) (λα + μβ)(dx) = λ
*

f (x) α(dx) + μ
*

f (x) β(dx). (B.32)
Solution. We ﬁrst consider the case when f is a non-negative simple function:
f = N
i=1 ciχEi where Ei =

x ∈ | f = ci

, ci ≥0. Thus
*

f (x) (λα + μβ)(dx) =
n

i=1
ci(λα + μβ)(f = ci)
= λ
n

i=1
ciα(f = ci) + μ
n

i=1
ciβ(f = ci)
= λ
*

f (x) α(dx) + μ
*

f (x) β(dx).
When f is an E-measurable non-negative function, we approximate it from below
with an increasing sequence

ϕk

of simple functions that pointwise converges
to f (x). Since any ϕk is simple, we have
*

ϕk(x) (λα + μβ)(dx) = λ
*

ϕk(x) α(dx) + μ
*

ϕk(x) β(dx).
Letting k →+∞and taking advantage of the Beppo Levi theorem we get (B.32).
Example B.37 (Counting measure) Let  be a set. Given a subset A ⊂ let
H0(A) := |A| be the cardinality of A. It is easy to see that (P(), H0) is a
measure on , called counting measure. Clearly,
H0(A) =

x∈A
1,
where the sum on the right-hand side is +∞if A has inﬁnite many points. The
corresponding integral is
*

f (x) H0(dx) =

x∈
f (x).
The formula above is obvious if f is nonzero on a ﬁnite set only and can be
proven by passing to the limit and taking advantage of the Beppo Levi theorem
in the general case.

294
APPENDIX B
Exercise B.38 (Absolutely continuous measures) Let (B(R), P) be an absolutely
continuous measure with respect to the Lebesgue measure, i.e. assume there exists
a summable function ρ : R →R+ such that
P(A) =
*
A
ρ(t) dt
∀A ∈B(R).
Show that, for any non-negative B(R)-measurable function f ,
*
R
f (x) P(dx) =
*
R
f (x)ρ(x) dx.
(B.33)
Solution. Assume f is simple, i.e. f (x) = n
i=1 ci1Ei(x), Ei = {x | f (x) =
ci} ∈E. Then
*
R
f (x) P(dx) =
n

i=1
ciP(f = ci) =
n

i=1
ci
*
R
1Ei(x)ρ(x) dx
=
*
R

n

i=1
ci1Ei(x)

ρ(x) dx =
*
R
f (x)ρ(x) dx.
The general case can be proven by an approximation argument, using Propo-
sition B.20 and the Beppo Levi theorem.
B.3
Product measures and iterated integrals
B.3.1
Product measures
Let (E, P) and (F, Q) be measures on two sets X and Y, respectively. Denote
by I the family of all ‘rectangles’ in the Cartesian product X × Y
I :=

A = E × F
 E ∈E, F ∈F

and let ζ : I →R+ be the set function that maps any rectangle A × B ∈I into
ζ(A × B) := P(A)Q(B). The following can be easily shown.
Proposition B.39 I is a semiring and ζ : I →R is a σ-additive set function.
Proof. It is quite trivial to show that I is a semiring. In fact, if E := A × B
and F := C × D ∈I, then E ∩F = (A ∩C) × (B ∩D) and
E \ F =

(A \ C) × (B \ D)

∪

(A ∩C) × (B \ D)

∪

(A \ C) × (B ∩D)

.

APPENDIX B
295
Let us prove that ζ is σ-additive. Let E × F = ∪k(Ek × Fk), E, Ek ∈E,
F, Fk ∈F be such that the sets

Ek × Fk

are pairwise disjoint so that
χE(x) χF(y) =
∞

k=1
χEk(x)χFk(y)
∀x ∈X, y ∈Y.
Integrating with respect to Q on Y and applying the Beppo Levi theorem, we
obtain
χE(x) Q(F) =
∞

k=1
Q(Fk)χEk(x)
∀x ∈X.
Moreover, integrating with respect to P on X and again by the Beppo Levi
theorem we get
ζ(E × F) := P(E)Q(F) =
∞

k=1
P(Ek)Q(Fk).
Thus, see Theorem B.10, ζ extends to a measure denoted (G, P × Q) on the
smallest σ-algebra G containing I. This measure is called the product measure of
(E, P) and (F, Q). Moreover, such an extension is unique, provided ζ : I →R+
is σ-ﬁnite, see Theorem B.6. This happens in particular, if both (E, P) and (F, Q)
are σ-ﬁnite.
Of course one can consider the product of ﬁnitely many measures. Taking for
instance the product of n Bernoulli trials, one obtains the Bernoulli distribution
on {0, 1}n
Ber(n, p) = B(1, p) × · · · × B(1, p).
B.3.1.1
Inﬁnite Bernoulli process
Let (, E, P) be a probability measure on . Consider the set ∞= -∞
i=1 
of -valued sequences, and consider the family C ⊂P(∞) of sets E ⊂∞of
the form
E =
∞
=
i=1
Ei
where Ei ∈E ∀i and Ei =  except for a ﬁnite number of indexes, i.e. the family
of ‘cylinders’ with the terminology of Section 2.2.7. Deﬁne also α : C →[0, 1]
by setting for E = -∞
i=1 Ei ∈C,
α(E) =
∞
=
i=1
P(Ei).
Notice that the product is actually a ﬁnite product, since P(Ei) = 1 except for a
ﬁnite number of indexes.

296
APPENDIX B
The following theorem holds. The interested reader may ﬁnd a proof in,
e.g. [7].
Theorem B.40 (Kolmogorov) C is a semiring and α is σ-additive on C.
Therefore, Theorem B.6 and B.10 apply so that there exists a unique proba-
bility measure (E, P∞) on ∞that extends α to the σ-algebra generated by C.
The existence and uniqueness of the Bernoulli distribution of parameter p
introduced in Section 2.2.7 is a particular case of the previous statement. One
obtains it by choosing the Bernoulli trial distribution B(1, p) on {0, 1} as starting
probability space (, E, P).
B.3.2
Reduction formulas
Let A ⊂X × Y. For any point x ∈A let Ax be the subset of Y deﬁned as
Ax :=

y ∈Y
 (x, y) ∈A

.
Ax is called the section of A at x.
Theorem B.41 (Fubini) Let X, Y be two sets and let (G, P × Q) be the product
measure on X × Y of the two σ-ﬁnite measures (E, P) and (F, Q) on X and Y,
respectively. Then, for any A ∈G the following hold:
(i) Ax ∈F P-a.e. x ∈X.
(ii) x →Q(Ax) is an E-measurable function.
(iii) (P × Q)(A) =
.
X Q(Ax) P(dx).
Changing the roles of the two variables, one also has:
(iv) Ay ∈E Q-a.e. y ∈Y.
(v) y →P(Ay) is an F-measurable function.
(vi) (P × Q)(A) =
.
Y P(Ay) Q(dy).
From the Fubini theorem, Theorem B.41, one obtains the following reduction
formulas.
Theorem B.42 (Fubini–Tonelli) Let (E, P) and (F, Q) be two σ-ﬁnite mea-
sures on the sets X and Y, respectively, and let (G, P × Q) be the product measure
on X × Y. Let f : X × Y →R be G-measurable and non-negative (respectively,
P × Q summable). Then the following hold:
(i) y →f (x, y) is F-measurable (respectively, Q-summable) P-a.e. x ∈X.
(ii) x →
.
Y f (x, y) Q(dy) is E-measurable (respectively, P-summable).

APPENDIX B
297
(iii) We have
*
X×Y
f (x, y) P × Q(dx dy) =
*
X
P(dx)
*
Y
f (x, y) Q(dy).
Of course, the two variables can be interchanged, so under the same assumption
of Theorem B.42 we also have:
(i) x →f (x, y) is E-measurable (respectively, P-summable) Q-a.e. y ∈Y.
(ii) y →
.
X f (x, y) P(dx) is F-measurable (respectively, Q-summable).
(iii) We have
*
X×Y
f (x, y) P × Q(dx dy) =
*
Y
Q(dy)
*
X
f (x, y) P(dx).
Proof. The proof is done in three steps.
(i) If f is the characteristic function of a P × Q measurable set, then we apply
the Fubini theorem, Theorem B.41. Because of additivity, the result still
holds true for any G-measurable simple function f .
(ii) If f is non-negative, then f can be approximated from below by an increas-
ing sequence of simple functions. Applying the Beppo Levi theorem and
the continuity of measures, the result holds true for f .
(iii) If f is P × Q summable, then one applies the result of Step (ii) to the
positive and negative parts f+ and f−of f .
Notice that the ﬁniteness assumption on the two measures (E, P) and (F, Q)
in Theorems B.41 and B.42 cannot be dropped as the following example shows.
Example B.43 Let X = Y = R, P = L1, and let Q be the measure that counts the
points: Q(A) = |A|. Let S := {(x, x) | x ∈[0, 1]} and let f (x, y) = χS(x, y) be
its characteristic function. S is closed, hence S belongs to the smallest σ-algebra
generated by ‘intervals’, i.e. B(R2). Clearly (P × Q)(S) = ∞, but
*
R
P(dx)
*
R
f (x, y) Q(dy) =
* 1
0
1 dx = 1,
*
R
Q(dy)
*
R
f (x, y) P(dx) =
* 1
0
0 dQ = 0.
B.3.3
Exercises
Exercise B.44 Show that Ln × Lk = Ln+k on the Borel sets of Rn × Rk.

298
APPENDIX B
Exercise B.45 Let (E, P) be a measure on  and let f :  →R+ be a P-
measurable function. Show that the subgraph of f
SGf :=

(x, t) ∈ × R
 0 < t < f (x)

is P × L1-measurable and
P × L1(SGf ) =
*

f (x) P(dx).
[Hint. Prove the claim for simple functions and use an approximation argument
for the general case.]
B.4
Convergence theorems
B.4.1
Almost everywhere convergence
Deﬁnition B.46 Let
(E, P) be a measure on  and let

Xn

and X be
E-measurable functions.
(i) We say that

Xn

converges in measure to X, if for any δ > 0
P

x
 |Xn −X| > δ

→0
as n →∞.
(ii) We say that

Xn

converges to X almost everywhere, and we write Xn →X
P-a.e., if the measure of the set
E : =

x
 X(x) = ±∞o Xn(x) ̸→X(x)

=

x
 lim sup
n→∞
|Xn(x) −X(x)| > 0
$
is null, P(E) = 0.
The difference between the above deﬁned convergences becomes clear if one
ﬁrst considers the following sets, which can be constructed starting from a given
sequence of sets

An

; namely, the sets
lim sup
n
An :=
∞
'
m=1
∞
(
n=m
An
and
lim inf
n
An :=
∞
(
m=1
∞
'
n=m
An
In the following proposition we collect the elementary properties of such sets.
Proposition B.47 We have the following:
(i) x ∈lim infn An if and only if there exists n such that x ∈An ∀n ≥n.

APPENDIX B
299
(ii) x ∈lim supn An if and only if there exists inﬁnite values of n such that
x ∈An.
(iii) x ∈(lim supn An)c if and only if

n | x ∈An

is ﬁnite.
(iv) (lim supn An)c = lim infn Ac
n.
(v) Let E be a σ-algebra of subsets of . If

An

⊂E, then both lim infn An
and lim supn An are E-measurable. Moreover,
P(lim inf
n
An) ≤lim inf
n→∞P(An) ≤lim sup
n→∞
P(An) ≤P(lim sup
n
An).
Proof. (i) and (ii) agree with the deﬁnitions of lim infn An and lim supn An,
respectively. (iii) is a rewrite of (ii) and (iv) is a consequence of De Moivre for-
mulas. To prove (v) it sufﬁces to observe that the E-measurability of lim infn An
and lim supn An comes from the properties of σ-algebras and that the inequality
in (v) is a consequence of the continuity of measures.
Let (E, P) be a measure on  and let

Xn

and X be E-measurable functions.
Given any δ ≥0, deﬁne
An,δ :=

x ∈
 |Xn(x) −X(x)| > δ

,
Eδ :=

x ∈
 lim sup
n→∞
|Xn(x) −X(x)| > δ
$
.
Since x ∈Eδ if and only if there exists a sequence

kn

such that |Xkn(x) −
X(x)| > δ, then
Eδ = lim sup
n
An,δ
∀δ ≥0.
(B.34)
Proposition B.48 Let
(E, P) be a measure on  and let

Xn

and X be E-
measurable functions. With the notation above,

Xn

converges to X in measure
if and only if P(An,δ) →0 as n →∞for any positive δ. Moreover, the following
are equivalent:
(i) Xn →X P-a.e.
(ii) P(lim supn An,0) = 0.
(iii) P(lim supn An,δ) = 0 for any δ > 0.
Proof. By deﬁnition, Xn →X P-a.e. if and only if P(E0) = 0. For any δ ≥0,
Eδ ⊂E0 = ∪δ > 0Eδ, hence P(E0) = 0 if and only if P(Eδ) = 0 for any δ > 0.
The claim follows from (B.34).
Convergence in measure and almost everywhere convergence are not equiv-
alent, see Example 4.76. Nevertheless, the two convergences are related, as the
following proposition shows.

300
APPENDIX B
Proposition B.49 Let (E, P) be a measure on  and let

Xn

and X be E-
measurable functions on . Then:
(i) If Xn →X P-a.e., then Xn →X in measure.
(ii) If Xn →X in measure, then there exists a subsequence

Xkn

of

Xn

such
that Xkn →X P-a.e.
Proof. (i) Let δ > 0. For any n let An,δ :=

x ∈ | |Xn −X| > δ

. By Propo-
sition B.48, P(lim supn An,δ) = 0 for any δ > 0. Let m ≥1 and deﬁne Bm :=
∪∞
n=mAn,δ. Then An,δ ⊂Bm ∀n ≥m hence
P(An,δ) ≤P(Bm) →P
 ∞
'
m=1
Bm

= P(lim sup
n
An,δ) = 0.
(ii) Let An,0 :=

x ∈ | |Xn −X| > 0

. We must show that there exists a
sequence nj such that
P(lim sup
j
Anj ,0) = 0,
see
Deﬁnition
4.75
and
(B.34).
Let
An,δ =

x ∈ | |Xn −X| > δ

.
By
assumption P(An,δ) →0 for any δ > 0. Let n1 be the smallest integer such that
P(An1,1) < 1/2, and for any k ≥2, let nk+1 be the smallest integer greater
than nk such that P(Ank+1,1/(k+1)) ≤1/2k+1. Let Bm := ∪j≥mAnj ,1/j. Since
Bm ↓∩mBm = lim supj(Anj ,1/j) we obtain
P(lim sup
j
Anj ,1/j) = lim
m→∞P(Bm) ≤
lim
m→+∞
∞

j=m
1/2j+1 = 0.
Since
P(lim sup
j
Anj ,0) ≤P(lim sup
j
Anj ,1/j),
the claim follows.
B.4.2
Strong convergence
We see here some different results related to the Beppo Levi theorem and the
convergence of integrals.
The ﬁrst result is about the convergence of integrals of series of non-negative
functions.
Proposition B.50 (Series of non-negative functions) Let (E, P) be a measure
on . Let

fk

be a sequence of E-measurable non-negative functions. Then
*

∞

k=1
fk(x) P(dx) =
∞

k=1
*

fk(x) P(dx).

APPENDIX B
301
Proof. The partial sums
n
k=1 fk(x)

are a nondecreasing sequence of
E-measurable non-negative functions. Applying the Beppo Levi theorem to this
sequence yields the result.
B.4.3
Fatou lemma
In the following lemma, the monotonicity assumption in the Beppo Levi theorem
is removed.
Lemma B.51 (Fatou) Let (E, P) be a measure on  and let

fk

be a sequence of
E-measurable non-negative functions. Then
*

lim inf
k→∞fk(x) P(dx) ≤lim inf
k→∞
*

fk(x) P(dx).
Proof. Let gn(x) := infk≥n fk(x).

gn(x)

is an increasing sequence of
E-measurable non-negative functions. Moreover,
0 ≤gn(x) ≤fk(x), ∀k ≥n,
lim inf
κ→∞fk(x) = lim
n→∞gn(x).
Thus
.
 gn(x) P(dx) ≤infk≥n
.
 fk(x) P(dx) and, applying the Beppo Levi
theorem, we get
*

lim inf
k→∞fk(x) P(dx) = lim
n→∞
*

gn(x) P(dx) ≤lim
n→∞inf
k≥n
*

fk(x) P(dx)
= lim inf
k→∞
*

fk(x) P(dx).
Remark B.52 The Fatou lemma implies the Beppo Levi theorem. In fact, let

fn

be an increasing sequence of functions that converges to f (x). Then f (x) =
limk→∞fk(x) = lim infk→∞fk(x). Since the sequence

fk

is monotone, we get
lim inf
k→∞
*

fk(x) P(dx) = lim
k→∞
*

fk(x) P(dx) ≤
*

f (x) P(dx),
and, by the Fatou lemma, we get the opposite inequality:
*

f (x) P(dx) =
*

lim inf
k→∞fk(x) P(dx) ≤lim inf
k→∞
*

fk(x) P(dx).
Corollary B.53 (Fatou lemma) Let (E, P) be a measure on . Let

fk

be a
sequence of E-measurable functions and let φ :  →R be a P-summable func-
tion.
(i) If fk(x) ≥φ(x) ∀k and P-a.e. x ∈, then
*

lim inf
k→∞fk(x) P(dx) ≤lim inf
k→∞
*

fk(x) P(dx).

302
APPENDIX B
(ii) If fk(x) ≤φ(x) ∀k and P-a.e., then
lim sup
k→∞
*

fk(x) P(dx) ≤
*

lim sup
k→∞
fk(x) P(dx).
Proof.
Let
Ek :=

x ∈ | fk(x) ≥φ(x)

and
let
E := ∩kEk.
Since
P(Ec) = 0, we can assume without loss of generality that fk(x) ≥φ(x) ∀k and
∀x ∈. To prove (i) it sufﬁces to apply the Fatou lemma, Lemma B.51, to the
sequence

fk −φ

. (ii) is proven similarly.
B.4.4
Dominated convergence theorem
Theorem B.54 (Lebesgue dominated convergence) Let (E, P) be a measure on
 and let

fk

be a sequence of E-measurable functions. Assume:
(i) fk(x) →f (x) P-a.e. x ∈.
(ii) There exists a P-summable function φ such that |fk(x)| ≤φ(x) ∀k and for
P-a.e. x.
Then
*

|fk(x) −f (x)| P(dx) →0
and, in particular,
*

fk(x) P(dx) →
*

f (x) P(dx).
Proof. By assumption |fk(x) −f (x)| ≤2φ(x) for P-a.e. x and for any k.
Moreover, |fk(x) −f (x)| →0 ∀k and for P-a.e. x. Thus, by the Fatou lemma,
Corollary B.53, we get
lim sup
k→∞
*

|fk(x) −f (x)| P(dx) ≤
*

lim sup
k→∞
|fk(x) −f (x)| dx
=
*

0 P(dx) = 0.
The last claim is proven by the following inequality:

*

fk(x) P(dx) −
*

f (x) P(dx)
 =

*

(fk(x) −f (x)) P(dx)

≤
*

|fk(x) −f (x)| P(dx).

APPENDIX B
303
Remark B.55 Notice that in Theorem B.54:
• Assumption (ii) is equivalent to the P-summability of the envelope φ(x) :=
supk |fk(x)| of the functions |fk|.
• Assumption (ii) cannot be dropped as the following sequence

fk

shows:
fk(x) =

k
if 0 < x < 1/k,
0
otherwise.
Example B.56 The dominated convergence theorem extends to arbitrary mea-
sures a classical dominated convergence theorem for series.
Theorem (Dominated convergence for series) Let

aj,n

be a double sequence
such that:
(i) For any j, aj,n →aj as n →∞.
(ii) There exists a non-negative sequence

cj

such that |aj,n| ≤cj for any n
and any j and ∞
j=1 |cj| < ∞.
Then the series ∞
j=1 aj is absolutely convergent and
∞

j=1
aj,n →
∞

j=1
aj
as n →∞.
Proof. Consider the counting measure (P(N), H0) on  = N and apply the
Lebesgue dominated convergence theorem to the sequence

fn

deﬁned by
fn(j) = aj,n.
For the reader’s convenience, we include a direct proof. Since aj,n →aj and
|an,j| ≤cj ∀n, j, we get |aj| ≤cj ∀j so that ∞
j=0 aj is absolutely convergent.
Let ϵ > 0. Choose p = p(ϵ) such that 2 ∞
j=p+1 cj < ϵ. Then

∞

j=0
aj,n −
∞

j=0
aj
 ≤
∞

j=0
|aj,n −aj| =
p

j=0
|aj,n −aj| +
∞

j=p+1
|aj,n −aj|
≤
p

j=0
|aj,n −aj| + 2
∞

j=p+1
cj ≤
p

j=0
|aj,n −aj| + ϵ.
Thus, as n →∞, we obtain
lim sup
n→∞

∞

j=0
aj,n −
∞

j=0
aj
 ≤ϵ.
Since ϵ is arbitrary, the claim follows.

304
APPENDIX B
The next theorem is an important consequence on the convergence of integrals
of series of functions.
Theorem B.57 (Lebesgue) Let (E, P) be a measure on  and let

fk

be a
sequence of E-measurable functions such that
∞

k=0
*

|fk(x)| P(dx) < +∞.
Then for P-a.e. x the series ∞
k=0 fk(x) is absolutely convergent to a P-summable
function f (x). Moreover,
*

f (x) −
p

k=0
fk(x)
 P(dx) →0
as p →∞,
(B.35)
and
*

f (x) P(dx) =
∞

k=0
*

fk(x) P(dx).
Notice that the assumptions are on the integrals only, while the claim is about
the P-a.e. convergence of the series ∞
k=0 |fk(x)|.
Proof. For any x ∈ let g(x) ∈R+ be the sum of the non-negative addenda
series ∞
k=0 |fk(x)|. Applying the Beppo Levi theorem, the assumption gives
*

g(x) P(dx) =
∞

k=0
*

|fk(x)| P(dx) < +∞,
i.e. g is P-summable. Thus, by Proposition B.26, g(x) < +∞for P-a.e. x, i.e.
the series ∞
k=0 fk(x) absolutely converges to f (x) := ∞
k=0 fk(x) and, for any
integer p ≥1 we have

∞

k=p
fk(x)
 ≤
∞

k=p
|fk(x)|.
(B.36)
In particular,
|f (x)| ≤
∞

k=0
|fk(x)| = g(x)
P-a.e. x ∈,
so that f is summable. Integrating (B.36) we get
*

f (x) −
p−1

k=0
fk(x)
 P(dx) =
*


∞

k=p
fk(x)
 P(dx) ≤
*

∞

k=p
|fk(x)| P(dx)
=
∞

k=p
*

|fk(x)| P(dx).

APPENDIX B
305
As p →∞we get the ﬁrst part of the claim. The second part of the claim easily
follows since

*

f (x) P(dx)−
p−1

k=0
*

fk(x) P(dx)

≤
*

f (x) −
p−1

k=0
fk(x)
 P(dx) →0
as p →∞.
B.4.5
Absolute continuity of integrals
Theorem B.58 (Absolute continuity of integrals) Let (E, P) be a measure on
 and let f be a P-summable function. For any ϵ > 0 there exists δ > 0 such that
.
E |f | P(dx) < ϵ for any E ∈E such that P(E) < δ. Equivalently,
*
E
f (x) P(dx) →0
as P(E) →0.
Proof. Let
fk(x) =
⎧
⎪⎨
⎪⎩
k
if f (x) > k,
f (x)
if −k ≤f (x) ≤k,
−k
if f (x) < −k.
Then |fk(x) −f (x)| →0 for P-a.e. x and |fk(x) −f (x)| ≤2|f (x)|, Since |f |
is P-summable, the dominated convergence theorem, Theorem B.54, applies for
any ϵ > 0 there exists N = Nϵ such that
*

|f (x) −fN(x)| P(dx) < ϵ/2.
Let δ := ϵ/(2N). Then for any E ∈E such that P(E) ≤δ we get
*
E
|fN(x)| P(dx) ≤N P(E) ≤N
ϵ
2N = ϵ/2
so that
*
E
|f (x)| P(dx) ≤
*
E
|fN(x)| P(dx) +
*

|f −fN| P(dx) ≤ϵ
2 + ϵ
2 = ϵ.
B.4.6
Differentiation of the integral
Let f, g : Rn →R be Lebesgue-summable non-negative functions. Clearly,
.
A f (x) dx =
.
A g(x) dx
∀A ⊂Rn
if
and
only
if
f (x) = g(x)
almost

306
APPENDIX B
everywhere. Thus one would like to characterize f (x) in terms of its integral,
i.e. of the map A →
.
A f (x) dx. Differentiation theory provides such a charac-
terization. Obviously, if f is continuous, then the mean value theorem gives
lim
r→0
1
|B(x, r)|
*
B(x,r)
f (y) dy = f (x)
∀x.
More generally, the following theorem holds.
Theorem B.59 (Lebesgue) Let E ⊂Rn be a B(Rn)-measurable set and let f :
E →R be B(Rn)-measurable such that
.
E |f |p dx < +∞for some 1 ≤p <
+∞. Then for almost every x ∈E,
1
|B(x, r)|
*
E∩B(x,r)
|f (y) −f (x)|p dy →0
as r →0+.
In particular, for almost every x ∈E, the limit
lim
r→0+
1
|B(x, r)|
*
E∩B(x,r)
f (y) dy
exists, is ﬁnite and equal to f (x).
Example B.60 Let f be L1-summable on ] −1, 1[. Show that
lim
r→0+
1
2r
* x+r
x−r
f (y) dy = f (x)
a.e. x ∈] −1, 1[.
Deﬁnition B.61 Let
f : E ⊂Rn →R be Ln-summable on E. The points of
the set
Lf :=

x ∈E
 ∃λ ∈R such that
1
|B(x, r)|
*
E∩B(x,r)
|f (y) −λ| dy →0
$
are called Lebesgue points of f . For any x ∈Lf the limit
λf (x) := lim
r→0+
1
|B(x, r)|
*
B(x,r)
f (y) dy
exists and is ﬁnite thus it deﬁnes a function λf : Lf →R called the Lebesgue
representative of f .
From the Lebesgue differentiation theorem, Theorem B.59, we get
Theorem B.62 Let f be a Ln-summable function on Rn. Then Ln(Rn \ Lf ) = 0
and f = λf Ln-a.e.
The differentiation theorem can be extended to more general sets than balls
centred at x. One may use cubes centred at x, cubes containing x or even

APPENDIX B
307
different objects. For example, let A be a bounded Borel set such that Ln(A) > 0.
Assume, e.g.
A ⊂B(0, 100) ⊂Rn,
|A| = c |B1|.
For any x ∈Rn and any r > 0, let Ax,r := x + r A. Obviously, Ax,r ⊂
B(x, 100 r) and |Ax,r| = rn |A| = c rn |B1| = c |B(x, r)|. Theorem B.59 implies
the following.
Theorem B.63 Let E ⊂Rn be a Borel-measurable set and let f : E →R be
B(Rn)-measurable with
.
E |f |p dx < +∞for some 1 ≤p < ∞. Then for Ln-
a.e. x ∈E
1
|Ax,r|
*
E∩Ax,r
|f (y) −f (x)|p dy →0
as r →0+.
We now collect some results due to Giuseppe Vitali (1875–1932) on the
differentiation of integrals and of monotone functions.
Theorem B.64 (Vitali) Let h : R →R be monotone nondecreasing. Then h is
differentiable at L1-a.e. x ∈R and the derivative h′(x) is non-negative at L1-a.e.
x ∈R. Moreover, h′ is L1-summable on any bounded interval ]a, b[∈R and
* y
x
h′(t) dt ≤h(y) −h(x)
∀x < y.
(B.37)
Remark B.65 Equality may not hold in (B.37). Take, e.g. h(x) := sgn(x) so
that h′(x) = 0 ∀x ̸= 0, and, of course, 0 =
. 1
−1 h′(t) dt < h(1) −h(−1) = 2.
Although surprising, one may construct examples of continuous and strictly
increasing functions whose derivative is zero almost everywhere: one some-
what simpler example of a continuous, nonconstant and nondecreasing function
with zero derivative almost everywhere is the famous Cantor–Vitali function.
Obviously, for such functions the inequality in (B.37) may be strict.
Deﬁnition B.66 A function f : R →R is said to be absolutely continuous if for
any ϵ > 0 there exists δ > 0 such that for any pair of sequences

xk

,

yk

such
that ∞
k=1 |xk −yk| < δ we have ∞
k=1 |f (xk) −f (yk)| < ϵ.
Let f ∈L1([a, b]), Theorem B.58 implies that the integral function
F(x) :=
* x
a
f (t) dt,
x ∈[a, b],
is absolutely continuous. The next theorem shows that integral functions are the
only absolutely continuous functions.
Theorem B.67 (Vitali) A function h : [a, b] →R is absolutely continuous if and
only if there exists a L1-summable function f on [a, b] such that
h(y) −h(x) =
* y
x
f (s) ds
∀x, y ∈[a, b], x < y.

308
APPENDIX B
Moreover, h is differentiable at almost every x ∈[a, b] and h′(x) = f (x) for a.e.
x ∈[a, b].
Lipschitz continuous functions f : R →R are absolutely continuous; thus
by Theorem B.67 they are differentiable L1-a.e., the derivative f ′(x) is L1-
summable and
f (y) −f (x) =
* y
x
f ′(s) ds
∀x, y ∈R, x < y.
Moreover, the following holds in Rn.
Theorem B.68 (Rademacher) Let f : Rn →R be Lipschitz continuous. Then
f is differentiable at Ln-almost every x ∈Rn, the map x →Df (x) is B(Rn)-
measurable and |Df (x)| ≤Lip(f ) Ln-a.e. x ∈Rn.
B.4.7
Weak convergence of measures
In this section we consider Borel measures on R, that is measures (B(R), μ) on
R. Since the σ algebra is understood, we simply write μ to denote the measure
(B(R), μ).
Recall that the law of a ﬁnite Borel measure μ on R is the function F :
R →R, F(t) := μ(] −∞, t]). We recall that F is monotone nondecreasing, in
particular, is right-continuous on R and the set of its discontinuity points is at most
denumerable. Moreover, F(t) →F(−∞) as t →−∞and F(t) →F(+∞) as
t →+∞and the measure μ is completely determined by F.
Deﬁnition B.69 Let

μn

, μ be ﬁnite Borel measures on R. We say that

μn

weakly converges to μ, and we write μn ⇀μ, if for any continuous bounded
function ϕ : R →R one has
*
R
ϕ(t) μn(dt) →
*
R
ϕ(t) μ(dt).
Proposition B.70 If the weak limit of a sequence of measures exists, then it is
unique.
Proof. Assume μn ⇀μ1 and μn ⇀μ2, and let P := μ1 −μ2. Then, for
every continuous bounded function ϕ : R →R
*
R
ϕ(t) P(dt) = 0.
The characteristic function of ] −∞, a] can be approximated from below by
an increasing sequence of continuous non-negative functions, thus obtaining
P([−∞, a]) = 0 ∀∈R, hence P(A) = 0 ∀A ∈B(R).
Theorem B.71 Let

μn

, μ be ﬁnite Borel measures on R. Assume μn(R) = μ(R)
∀n and let Fn and F be their laws, respectively. The following are equivalent:

APPENDIX B
309
(i) If F is continous at t, then Fn(t) →F(t).
(ii) μn weakly converges to μ.
Proof. (i) ⇒(ii) Without any loss of generality, we can assume that for any
n ∈N, Fn(−∞) = F(−∞) = 0 and Fn(+∞) = μn(R) = μ(R) = F(+∞) = 1.
Fix δ > 0 and let a, b ∈R, a < b, such that F is continuous at a and b, and such
that F(a) ≤δ and 1 −F(b) ≤δ. Fn(a) and Fn(b) converge to F(a) and F(b),
respectively, hence for large enough n’s, Fn(a) ≤2δ, 1 −Fn(b) ≤2δ.
Let ϕ : R →R be a bounded continuous function, |ϕ| ≤M. Since ϕ is uni-
formly continuous in [a, b], there exists N = Nδ and intervals Ij = [aj, aj+1],
j = 1, . . . , Nδ where a = a1 < a2 < · · · < aN+1 = b such that the oscillation of
ϕ on every Ij is less than δ, maxIj ϕ −minIj ϕ ≤δ∀j. Morever, perturbating the
extrema aj if necessary, we can assume that all the points aj are continuity points
for F. Let h(x) := N
j=1 ϕ(aj)1Ij (x). h is a simple function h|Ij = ϕ(aj) and
h = 0 in R \ [a, b]. Moreover, |ϕ(x) −h(x)| ≤δ on ]a, b]. Since ϕ is bounded
and μn(R) = 1,

*
R
(ϕ(x) −h(x)) μn(dx)

≤
*
]−∞,a]
|ϕ(x)| μn(dx) +
*
]b,+∞[
|ϕ(x)| dx
+
*
]a,b]
|ϕ(x) −h(x)| μn(dx)
≤4Mδ + δ = (4M + 1)δ
and, similarly,

*
R
(ϕ(x) −h(x)) μn(dx)
 ≤(2M + 1)δ,
hence

*
R
ϕ(x) μn(dx) −
*
R
ϕ(x) μ(dx)

≤2(3M + 1)δ +

*
]a,b]
h(x)(μn −μ)(dx)

≤2(3M + 1)δ +
N−1

j=1
|ϕ(aj)|

|Fn(aj+1) −F(aj+1)| + |Fn(aj) −F(aj)|

.
Since Fn(aj) →F(aj) for any j = 1, . . . , N, we get as n →∞
lim sup
n→∞

*
R
ϕ(x) μn(dx) −
*
R
ϕ(x) μ(dx)
 ≤2(3M + 1)δ.

310
APPENDIX B
Since δ > 0 is arbitrary, the claim is proven.
(ii) ⇒(i) Let a ∈R. It sufﬁces to show that
F(a−) ≤lim inf
n→∞Fn(a−)
and
lim sup
n→∞
Fn(a) ≤F(a).
(B.38)
In fact, from (B.38) one gets
F(a−) ≤lim inf
n→∞Fn(a−) ≤lim inf
n→∞Fn(a) ≤lim sup
n→∞
Fn(a) ≤F(a),
i.e. F(a) = limn→∞Fn(a) if F is continuous at a.
For any δ > 0, let ϕ(t) be the piecewise linear function that is null for any
t ≥a and is identically 1 for t ≤a −δ. Then
F(a −δ) =
*
R
1]−∞,a−δ](t) μ(dt)
≤
*
R
ϕ(t) μ(dt) = lim
n→∞
*
R
ϕ(t) μn(dt) = lim inf
n→∞
*
R
ϕ(t) μn(dt)
≤lim inf
n→∞
*
R
1]−∞,a[(t) μn(dt) ≤lim inf
n→∞Fn(a−).
As δ →0+, F(a −δ) →F(a−), so the ﬁrst inequality in (B.38) is proven. Sim-
ilarly, let ϕ(t) be the piecewise linear function that is null for t ≥a + δ and is
identically 1 for t ≤a. Then
F(a + δ) =
*
R
1]−∞,a+δ](t) μ(dt)
≥
*
R
ϕ(t) μ(dt) = lim
n→∞
*
R
ϕ(t) μn(dt) = lim sup
n→∞
*
R
ϕ(t) μn(dt)
≥lim sup
n→∞
*
R
1]−∞,a](t) μn(dt) ≥lim sup
n→∞
Fn(a).
As δ →0+, F(a + δ) →F(a) so that the second inequality in (B.38) holds. The
proof of (B.38) is then complete.
Let (E, P) be a probability measure on a set . We recall that the law FX
associated with an E-measurable function X :  →R is the law of the image
measure of P through X, i.e.
FX(t) := P

x ∈
 X(x) ≤t

.
Deﬁnition B.72 Let (E, P) be a ﬁnite measure on a set  and let

Xn

, X be
E-measurable functions. We say that

Xn

converges in law to X if FXn(t) con-
verges to FX(t) pointwisely in all points of continuity of FX.

APPENDIX B
311
Proposition B.73 Let (E, P) be a ﬁnite measure on a set  and let

Xn

, X be
E-measurable functions. If Xn →X in measure then Xn converges in law to X.
Proof. If sufﬁces to prove that
FX(a−) ≤lim inf
n→∞FXn(a)
and
lim sup
n→∞
FXn(a) ≤FX(a).
(B.39)
In fact, from (B.39) one gets
FX(a−) ≤lim inf
n→∞FXn(a) ≤lim sup
n→∞
FXn(a) ≤FX(a),
hence FX(a) = limn→∞FXn(a) if FX is continuous at a.
Let us prove (B.39). Let δ > 0. Since

x ∈
 X(x) ≤a −δ

=

x ∈
 X(x) ≤a −δ, Xn(x) ≤a

( 
x ∈
 X(x) ≤a −δ, Xn(x) > a

⊂

x ∈
 Xn(x) ≤a
 ( 
x ∈
 |Xn −X| > δ

,
we have
FX(a −δ) ≤FXn(a) + P(|X −Xn| > δ).
Thus, passing to the limit with respect to n, we obtain
FX(a −δ) ≤lim inf
n→∞FXn(a).
If we now let δ →0+, we get the ﬁrst inequality (B.39). Similarly,

x ∈
 Xn(x) ≤a

=

x ∈
 Xn(x) ≤a, X(x) ≤a + δ

( 
x ∈
 Xn(x) ≤a, X(x) > a + δ

⊂

x ∈
 X(x) ≤a + δ
 ( 
x ∈
 |Xn −X| > δ

so that
FXn(a) ≤FX(a + δ) + P(|X −Xn| > δ).
As n →∞we get
lim sup
n→∞
FXn(a) ≤FX(a + δ),
so that, by letting δ →0+ we obtain the second inequality of (B.39).

312
APPENDIX B
B.4.8
Exercises
Exercise B.74 Let f : Rn →R be Ln-summable. Prove that the function F :
Rn × [0, ∞[→R deﬁned by
F(x, r) :=
*
B(x,r)
f (t) dLn(t)
is continuous.
Exercise B.75 Let f ∈L1(R). Prove that the following equalities hold for a.e.
x ∈R:
lim
r→0+
1
r
* r
0
f (x + t) dt = f (x);
lim
r→0+
1
r
* 0
−r
f (x + t) dt = f (x);
lim
r→0+
1
r
* 2r
r
f (x + t) dt = f (x).
Exercise B.76 Let ϕ : [a, b] →[c, d] be continuous and piecewise differentiable.
Let h : [c, d] →R be absolutely continuous. Prove that f ◦ϕ : [a, b] →R is
absolutely continuous.
Exercise B.77 Let ϕ : [a, b] →[c, d] be continuous. Then ϕ is absolutely contin-
uous if and only if the graph of ϕ has ﬁnite length.
Exercise B.78 Let (E, P) be a measure on a set . Let

Xn

, X be E-measurable
functions and let p ∈[1, ∞[. Assume that:
(i) Xn →X P-a.e. x ∈.
(ii)
.
 |Xn|p dx →
.
 |X|p dx.
Prove that
.
 |Xn −X|p dx →0.
Solution. For any positive n the function Yn := 2p−1(|X|p + |Xn|p) −|Xn −
X|p is non-negative. Moreover, as n →∞, Yn converges to 2p|X|p P-a.e. Thus,
by the Fatou lemma
2p
*

|X|p dx ≤lim inf
n→∞
*

%
2p−1(|X|p + |Xn|p) −|Xn −X|p&
= 2p
*

|X|p dx −lim sup
n→∞
*

|Xn −X|p dx.

Appendix C
Systems of linear ordinary
differential equations
We assume the reader is already familiar with ﬁrst- and second-order linear
ordinary differential equations (ODEs) with constant coefﬁcients, either homoge-
neous or not. Here, we review results on the solutions to systems of N ﬁrst-order
linear ODEs.
C.1
Cauchy problem
We ﬁrst consider the existence and uniqueness of a C1 solution X : [a, b] →RN
to the problem
⎧
⎪⎨
⎪⎩
t0 ∈[a, b],
X(t0) = X0,
X′(t) = QX(t) + F(t)
∀t ∈[a, b]
(C.1)
where (t0, X0) ∈[a, b] × RN is the given initial datum, Q is a given real N × N
matrix and F : [a, b] →RN is a given continuous function.
C.1.1
Uniqueness
Lemma C.1 (Gr¨onwall) Let W ∈C1(]a, b[, RN) satisfy the inequality
|W ′(t)| ≤α + β |W(t)|
∀t ∈]a, b[
for some α ≥0 and β > 0. Then
|W(t)| ≤

|W(t0)| + α
β

eβ|t−t0|
∀t, t0 ∈]a, b[.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

314
APPENDIX C
Proof. Let ϵ > 0. The function z(t) :=

ϵ2 + |W(t)|2 is strictly positive and
of class C1(]a, b[), and
z′(t) = (W(t)|W ′(t))

ϵ2 + |W(t)|2 ≤|W ′(t)|
|W(t)|

ϵ2 + |W(t)|2
≤|W ′(t)| ≤α + β|W(t)| ≤α + β z(t).
Thus,
z′(t)
α + βz(t) ≤1
∀t ∈]a, b[
and integrating from t0 to t we get
1
β log
 α + βz(t)
α + βz(t0)
 =

* t
t0
z′(s)
α + βz(s) ds
 ≤|t −t0|,
i.e.
 α + βz(t)
α + βz(t0)
 ≤eβ|t−t0|
∀t ∈]a, b[.
Thus,
β|W(t)| ≤α + βz(t) ≤(α + βz(t0))eβ|t−t0|
=

α + β
6
ϵ2 + |W(t0)|2

eβ|t−t0|.
As ϵ →0, we get the claim.
Theorem C.2 Let Q ∈MN,N and F(t) ∈C0([a, b], RN). Then the Cauchy prob-
lem (C.1) admits at most one solution.
Proof. Let us introduce the maximum expansion norm of the matrix
Q ∈MN,N
||Q|| := sup
|x|=1
|Qx|.
(C.2)
so that
|Qx| ≤||Q|| |x|
∀x ∈RN.
We point out that the sup in (C.2) is in fact a maximum since the map x →|Qx|
is continuous.
Assume that X1(t) and X2(t) are two different C1 solutions to (C.1). Then
their difference W(t) := X1(t) −X2(t) is a solution to the Cauchy problem:

W ′(t) = QW(t)
∀t ∈]a, b[,
W(t0) = 0.

APPENDIX C
315
Thus, |W ′(t)| = |QW(t)| ≤||Q|| |W(t)| and the Gr¨onwall lemma applied to
W(t) with α = 0 and β = ||Q|| yields
|W(t)| ≤|W(t0)| exp(||Q|| |t −t0|)
∀t ∈]a, b[.
Since W(t0) = 0, the claim is proven.
C.1.2
Existence
Let Q ∈MN,N(R). For each z ∈C, the power series ∞
k=0
Qkzk
k!
absolutely con-
verges to a matrix denoted by eQz, cf. (A.4),
eQz :=
∞

k=0
Qkzk
k!
Moreover, the series can be differentiated term by term, thus giving
(eQz)′ =
∞

k=1
Qkzk−1
(k −1)! = Q
∞

k=1
Qk−1zk−1
(k −1)! = Q eQz
and eQ(t+s) = eQteQs ∀t, s ∈R, see Proposition A.8. Since eQ0 = Id, we have
the following.
Theorem C.3 The only C1 solution to the Cauchy problem (C.1) is the function
X : [a, b] →RN deﬁned by
X(t) := eQ(t−t0)X0 + eQt
* t
t0
e−QsF(s) ds.
Proof. It sufﬁces to show that X(t) satisﬁes all the equations in (C.1): we
have
X(t0) = eQ0X0 = IdX0 = X0,
X′(t) = QeQ(t−t0)X0 + QeQt
* t
t0
e−QsF(s) ds + eQte−QtF(t)
= QX(t) + F(t).
Corollary C.4 Let Q ∈MN,N(R). Then P(t) := eQt is the unique solution to

P′(t) = QP(t)
∀t ∈R,
P(0) = Id.
(C.3)

316
APPENDIX C
Proof. By Theorem C.3 the ith column of P(t) is the solution to the Cauchy
problem

X(0) = ei,
X′(t) = QX(t)
(C.4)
where ei = (0, . . . , 1, . . . , 0)T is the ith vector of the standard basis of RN. Thus
P(t) is the only matrix solution of (C.3).
The exponential matrix P(t) = eQt has several properties, see Proposition A.8.
In particular, for any t, s the matrices P(t) and P(s) commute, since
P(s)P(t) = eQseQt = eQ(t+s) = eQteQs = P(t)P(s).
Moreover, P(t) commutes with Q and P(t) is invertible with inverse matrix
P(t)−1 = (eQt)−1 = eQ(−t) = P(−t)
∀t ∈R.
In particular, for any t ∈R and n ∈Z
P(t)n = eQnt = P(nt).
The following proposition computes the determinant of P(t).
Proposition C.5 Let Q ∈MN,N(R), P(t) := eQt and let w(t) := det P(t). Then
w′(t) = tr (Q)w(t) so that
w(t) = exp(tr (Q)t), ∀t ∈R,
where tr (Q) = N
i=1 Qi
i denotes the trace of the matrix Q = (Qi
j).
Proof. The determinant of a matrix is a multilinear function of the columns.
Let P(t) = [P1(t) | P2(t) | . . . | PN(t)] ∈MN,N. Then
d
dt det P(t) = det [P ′
1 | P2 | . . . | PN]
+ det [P1 | P ′
2 | . . . | PN] + · · · + det [P1 | P2 | . . . | P ′
N].
Since P(0) = Id, we get
det [P ′
1(0) | P2(0) | . . . | PN(0)] = (P1
1)′(0),
. . . ,
det [P1(0) | P2(0) | . . . | P ′
N(0)] = (PN
N)′(0),
so that
ddet P(t)
dt
(0) = tr P′(0) = tr Q.
(C.5)

APPENDIX C
317
Thus, recalling that P(t + s) = P(s)P(t),
w′(t) = ddet P(t)
dt
(t) = ddet P(s + t)
ds
(0)
= ddet (P(s)P(t))
ds
(0) = ddet P(s)
ds
(0)det P(t) = tr (Q)w(t).
Since w(0) = 1, we ﬁnally get w(t) = exp(tr (Q)t).
C.2
Efﬁcient computation of eQt
The exponential matrix P(t) := eQt can be computed via different techniques
which have different degrees of numerical efﬁciency and accuracy also depending
on the structure of Q. This leads to a variety of methods and techniques to
compute, or to approximately compute, P(t), see e.g. [21]. For instance, if Q is
a Q-matrix, the uniformization method allows to easily approximate P(t) with
bounded errors, see Proposition 6.23. Here we discuss two simple methods that
apply to any matrix Q.
C.2.1
Similarity methods
A ﬁrst approach is by a similarity transformation. In fact, if Q = SJS−1, i.e. if
Q is C-similar to a matrix J via an invertible matrix S ∈MN,N(C), then for any
positive integer k, Qk = SJkS−1, so that
eQt =
∞

k=0
SJkS−1tk
k!
= S
∞

k=0
Jktk
k! S−1 = SeJtS−1.
The computation thus boils down to the computation of S, S−1 and the exponen-
tial of another matrix J.
Example C.6 Assume there exists a basis u1, . . . , uN of CN of eigenvectors of
Q and let λ1, . . . , λN be the corresponding eigenvalues. Then Q = SJS−1 where
S = [u1|u2| · · · |un] and J = diag(λ1, . . . , λn). Since J is diagonal, one can easily
show that
eJt = diag(eλ1t, eλ2t, . . . , eλnt)
so that
eQt = S diag(eλ1t, eλ2t, . . . , eλnt)S−1 =
0
u1eλ1t|u2eλ2t| . . . |uneλnt1
S−1.
The same result can be proven by the following reasoning: let u be an eigen-
vector of Q and let λ be the associated eigenvalue. It is trivial to check that
x(t) := eλtu is the solution of the Cauchy problem x′ = Qx x(0) = u. Thus, if

318
APPENDIX C
there exists a basis (u1, . . . , uN) of CN where u1, . . . , uN are eigenvectors of Q,
then the matrix
R(t) :=
0
eλ1tu1|eλ2tu2| . . . |eλNtuN
1
,
where λ1, . . . , λN are the eigenvalues associated with u1, . . . , uN, is a solution to
⎧
⎨
⎩
R′(t) = QR(t),
R(0) =
0
u1|u2| . . . |uN
1
.
Therefore, S(t) := R(t)R(0)−1 is a solution to
⎧
⎨
⎩
S′(t) = QS(t),
S(0) = Id.
thus concluding that
eQt = S(t) = R(t)
0
u1|u2| . . . |uN
1−1
.
A basis of RN made by eigenvectors of Q may not exist, but in general, the
Jordan decomposition formula holds, see e.g. [26]: one can ﬁnd an invertible
matrix S describing a change of basis in CN such that J = S−1QS is in the
canonical Jordan form
J =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
J1,1
0
0
. . .
0
0
J1,2
0
. . .
0
...
...
...
...
...
0
0
0
. . .
Jk,pk
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
where for any i = 1, . . . , k and any j = 1, . . . , pi
Ji,j =
⎧
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
λi
if Ji,j is a 1 × 1 matrix,
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
λi
1
0
0
. . .
0
0
λi
1
0
. . .
0
0
0
λi
1
. . .
0
...
...
...
...
...
...
0
0
0
. . .
λi
1
0
0
0
. . .
0
λi
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
otherwise.

APPENDIX C
319
Thus, one easily gets
eJt :=
⎛
⎜⎜⎜⎜⎜⎝
eJ1,1t
0
. . .
0
0
eJ1,2t
. . .
0
...
...
...
...
0
0
. . .
eJk,pk t
⎞
⎟⎟⎟⎟⎟⎠
;
where the matrices eJi,j t have to be computed.
If J′ = Ji,j = (λ) is 1 × 1, then obviously, etJ′ = eλt. If J′ = Ji,j is a Jordan
block of dimension ℓ≥2, then
J′ = λiId + N,
N = (Nα
β), Nα
β := δα+1,β.
Since N and Id commute and since (Nk)α
β = (δα+k,β), so that Nk = 0 for any
k ≥ℓ, we get
eJ′t =
∞

n=0
(λiId + N)ntn
n!
=
∞

n=0
n

k=0
tn
n!
n
k

λn−k
i
Nk
=
∞

n=0
min(n,ℓ−1)

k=0
tn
n!
n
k

λn−k
i
Nk =
ℓ−1

k=0
∞

n=k
tn
n!
n
k

λn−k
i
Nk
=
ℓ−1

k=0
 ∞

n=k
tn−kλn−k
i
(n −k)!
tk
k!Nk = eλit
ℓ−1

k=0
tk
k!Nk.
Notice that eJt depends only on the eigenvalues of Q and that each entry of eJt
is the product of an exponential function and a polynomial. Thus, eQt = SeJtS−1
depends only on the eigenvalues of Q and on its Jordan basis, which characterizes
both S and S−1.
C.2.2
Putzer method
The Jordan decomposition of Q yields a satisfying analytical description of the
exponential matrix eQt. On the other hand, the computation of S and S−1, which
depends on a basis of generalized eigenvectors of Q, is often numerically unsta-
ble. Numerical algorithms that do not require an a priori knowledge of a basis
of generalized eigenvectors of Q have been developed, see [21].
The following algorithm, the Putzer method, is often sufﬁciently efﬁcient and
precise. Let λ1, . . . , λN be the N eigenvalues of Q. Consider the matrix
A =
⎛
⎜⎜⎜⎜⎜⎝
λ1
0
0
. . .
0
1
λ2
0
. . .
0
...
...
...
...
...
0
. . .
1
λN−1
0
0
0
. . .
1
λN
⎞
⎟⎟⎟⎟⎟⎠

320
APPENDIX C
and let P(t) be the solution to the following Cauchy problem

P ′(t) = AP(t),
P(0) = (1, 0, . . . , 0)T ,
i.e. P(t) is the vector P(t) = (p1(t), p2(t), . . . , pN(t))T whose ﬁrst component
is the solution to the problem

p′
1(t) = λ1p1(t),
p1(0) = 1
and, for k = 1, 2, 3, . . . , N −1

p′
k+1(t) = λk+1pk+1(t) + pk(t),
pk+1(0) = 0.
Notice that each component of P(t) is obtained by solving a ﬁrst-order linear
nonhomogeneous ODE.
Let Mk, k = 0, 1, . . . N, be the N × N matrices such that

M0 := Id
if k = 0,
Mk := (Q −λkId)Mk−1
if k ≥1.
Notice that -N
j=1(z −λj) = det (zId −Q) =: pQ(z) is the characteristic poly-
nomial of Q, so that
MN =
N
=
j=1
(Q −λjId) = pQ(Q) = 0
by the Cayley–Hamilton theorem.
Proposition C.7 With the notation above, the exponential matrix P(t) := eQt is
given by
eQt =
N−1

k=0
pk+1(t)Mk
∀t ∈R.
Proof. Let Z(t) := N−1
k=0 pk+1(t)Mk. It sufﬁces to show, see Corollary C.4,
that

Z′(t) = QZ(t)∀t,
Z(0) = Id.
Clearly, Z(0) = N−1
k=0 pk+1(0)Mk = M0 = Id. Moreover,
Z′(t) −QZ(t) =
N−1

k=0

p′
k+1(t)Mk −pk+1(t)QMk


APPENDIX C
321
= p′
1(t)M0 −p1(t)QM0
+
N−1

k=1

pk(t)Mk + λk+1pk+1(t)Mk −pk+1(t)QMk

= p1(t)

λ1Id −Q

M0 +
N−1

k=1
pk(t)Mk
+
N−1

k=1
pk+1(t)

λk+1Id −Q

Mk
= −p1(t)M1 +
N−1

k=1

pk(t)Mk −pk+1(t)Mk+1

= −p1(t)M1 + p1(t)M1 −pN(t)MN = 0.
C.3
Continuous semigroups
Deﬁnition C.8 A map P : R+ →MN,N is called a continuous semigroup on R+
if P is continuous at 0+ and

P(t + s) = P(t)P(s)
∀t, s ≥0,
P(0) = Id.
(C.6)
As a consequence of Corollary C.4 and of the properties of the matrix expo-
nential, Proposition A.8, one immediately sees the following.
Proposition C.9 The exponential matrix P(t) := eQt, t ∈R is a continuous semi-
group on R+.
Proof. Obviously, the map t →P(t) = eQt is continuous. For any s ∈R, the
function
W(t) := P(t + s) −P(t)P(s)
t ∈R.
is a solution to the Cauchy problem

W′(t) = QW(t)
∀t ∈R,
W(0) = 0,
Therefore, W(t) = 0 ∀t ∈R, i.e. P(t + s) = P(t)P(s) ∀t ∈R. Since s is arbi-
trary, the claim follows.
The converse is also true and the following holds.

322
APPENDIX C
Theorem C.10 A continuous semigroup P : R+ →MN,N on R+ is differentiable
at every t ∈R+ and, setting
Q := P′
+(0) := lim
t→0+
P(t) −Id
t
,
we have
P(t) = eQt
∀t ≥0.
Proof. We ﬁrst show that t →P(t) is continuous at any t > 0. Since
P(h) →Id as h →0+, there exists a right-hand side neighbourhood of h = 0
such that P(h) is invertible for any h in such neighbourhood. Moreover,
P(h)−1 →Id as h →0+ and, by the semigroup property,
P(t + h) = P(t)P(h) →P(t)
as h →0+,
P(t −h) = P(t)P(h)−1 →P(t)
as h →0+.
Thus t →P(t) is continuous at any t ≥0.
The differentiability follows by Theorem C.12, hence the matrix Q :=
(P)′
+(0) exists. Since
P(t + s) −P(t)
s
= P(s)P(t) −P(t)
s
= P(s) −Id
s
P(t),
as s →0+ we obtain that P(t) is a solution to the Cauchy problem

P′(t) = QP(t)
∀t ≥0,
P(0) = Id.
Thus, by Proposition C.9,
eQz :=
∞

k=0
Qkzk
k! .
Lemma C.11 Let f (t), t ≥0, be integrable, continuous at 0 and such that
|f (t)| ≤C ekt for some non-negative constants C, k ≥0. Then
* ∞
0
ne−nsf (s) ds →f (0)
as n →∞.
Proof. Let ϵ > 0. There exists δ > 0 such that |f (t) −f (0)| < ϵ for any
t ∈[0, δ[. Since n
. ∞
0 e−ns ds = 1, we get

* ∞
0
ne−nsf (s) ds −f (0)
 ≤
* ∞
0
ne−ns|f (s) −f (0)| ds
=
* δ
0
ne−ns|f (s) −f (0)| ds +
* ∞
δ
ne−ns|f (s) −f (0)| ds
≤ϵ + Cne−nδ(1 + ekδ).

APPENDIX C
323
As n →∞we obtain

* ∞
0
ne−nsf (s) ds −f (0)
 ≤ϵ.
Since ϵ is arbitrary, the claim is proven.
Theorem C.12 Let P(t) : [0, +∞[→MN,N(R) be a semigroup continuous at
every t ∈R+. Then P(t) is differentiable.
Proof. We ﬁrst show that ||P(t)|| grows at most exponentially fast. We recall
that we are considering the norm
||A|| := sup
|x|=1
|Ax|
so that ||AB|| ≤||A|| ||B|| and |Ai
j| ≤||A|| ∀i, j.
By the semigroup property, ||P(t + s)|| ≤||P(t)|| ||P(s)||, hence the function
w(t) := sups≤t log ||P(s)|| is nondecreasing and satisﬁes

w(t + s) ≤w(t) + w(s),
w(0) = 0.
In particular, w(t) ≤qw(1) for any positive rational number q. By approxima-
tion the same property holds for any non-negative t: w(t) ≤w(1)t ∀t, so that
||P(t)||1 ≤ek0t, k0 := w(1).
We now prove the claim of the theorem. For any n > k0 consider the matrix
Zn :=
* ∞
0
ne−nsP(s) ds.
From the above all the entries of P(t) grow at most exponentially fast. Thus,
one applies Lemma C.11 to the sequence

Zn

n > k0 to get Zn →P(0) = Id as
n →∞. In particular, if n is large enough, the matrix Zn is invertible. Finally,
consider the product
P(t)Zn = P(t)
* ∞
0
ne−nsP(s) ds =
* ∞
0
ne−nsP(t)P(s) ds
=
* ∞
0
ne−nsP(t + s) ds = ent
* ∞
t
ne−nuP(u) du
=: Fn(t).
Since Fn(t) is of class C1 by the fundamental theorem of calculus, P(t) =
Fn(t)Z−1
n
is also of class C1.

References
[1] Dall’Aglio, G. Calcolo delle Probabilit`a. Zanichelli, Bologna, 1987.
[2] Baclawski, K., Cerasoli, M., and Rota, G. Introduzione alla Probabilit`a. UMI,
Bologna, 1984.
[3] Hajek, B. ECE 313, Probability with Engineering Applications. 2010. http://www.ifp
.illinois.edu/∼hajek/Papers/probability.html.
[4] Ba˜nuelos, R. Lecture Notes on Measure Theory and Probability. 2003. http://www
.math.purdue.edu/∼banuelos/probability.pdf.
[5] Klenke, A. Probability Theory. Universitext. Springer-Verlag London Ltd, London,
2008. A comprehensive course, Translated from the 2006 German original. http://dx
.doi.org/10.1007/978-1-84800-048-3.
[6] Varadhan, S. R. S. Probability Theory, vol. 7 of Courant Lecture Notes in Math-
ematics. New York University, Courant Institute of Mathematical Sciences, New
York, 2001.
[7] Ambrosio, L., Da Prato, G., and Mennucci, A. An Introduction to Measure Theory
and Probability. Lectures at Scuola Normale Superiore, Pisa, 2010.
[8] Feller, W. An Introduction to Probability Theory and its Applications. Vol. I . Third
edition. John Wiley & Sons, Ltd, New York, 1968.
[9] Feller, W. An Introduction to Probability Theory and its Applications. Vol. II . Second
edition. John Wiley & Sons, Ltd, New York, 1971.
[10] Mennucci, A., and Mitter, S. K. Probabilit`a e Informazione. Edizioni della Normale.
Scuola Normale Superiore, Pisa, 2008.
[11] Hajek, B. Communication Network Analysis. 2006. http://www.ifp.illinois.edu/
∼hajek/Papers/networkanalysisDec06.pdf.
[12] Hajek, B. An Exploration of Random Processes for Engineers. 2009. http://www.ifp
.illinois.edu/∼hajek/Papers/randomprocJan09.pdf.
[13] Marsan, M., Balbo, G., Conte, G., Donatelli, S., and Franceschinis, G. Modelling
with Generalized Stochastic Petri Nets. Wiley Series in Parallel Computing. John
Wiley & Sons, Ltd, New York, 1994.
[14] Varadhan, S. R. S. Stochastic Processes, vol. 16 of Courant Lecture Notes in
Mathematics. New York University, Courant Institute of Mathematical Sciences,
New York, 2007.
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.

REFERENCES
325
[15] Vicario, E. Metodi di Veriﬁca e Testing. Appunti del corso. Dipartimento di Sistemi
e Informatica, University of Florence, 2008.
[16] Behrends, E. Introduction to Markov Chains with Special Emphasis on Rapid
Mixing. Advanced Lectures in Mathematics. F. Vieweg and Son, Wiesbaden, 2000.
[17] Marinari, E., and Parisi, G. Trattatello di Probabilit`a. 2000. http://chimera.roma1
.infn.it/ENZO/SISDIS/PROBABILITA/versione26.pdf.
[18] Liu, J. S. Monte Carlo Strategies on Scientiﬁc Computing. Springer Series in Statis-
tics. Springer–Verlag, New York, 2001.
[19] Diaconis, P. The Markov chain Monte Carlo revolution. Bull. AMS 2009, 46,
179–205.
[20] Baldi, P. Calcolo delle Probabilit`a e Statistica. McGraw-Hill Libri Italia, Milan,
1992.
[21] Molery, C., and Van Loan, C. Nineteen Dubious Ways to Compute the Exponential
of a Matrix, Twenty Five Years Later. SIAM Rev. 2003, 45. http://www.cs.cornell
.edu/cv/researchpdf/19ways+.pdf.
[22] Norris, J. R. Markov Chains. Cambridge Series on Statistical and Probabilistic
Mathematics Cambridge University Press, Cambridge, 1997.
[23] Kulkarni, V. G. Modeling and Analysis of Stochastic Systems. Chapman & Hall,
London, 1995.
[24] Br´emaud, P. Markov Chains - Gibbs Fields, Monte Carlo Simulation and Queues.
Texts in Applied Mathematics, 31. Springer–Verlag, New York, 1999.
[25] Giaquinta, M., and Modica, G. Mathematical Analysis, vol. 2. Approximation and
Discrete Processes. Birkh¨auser, Boston, 2004.
[26] Giaquinta, M., and Modica, G. Note di metodi matematici per Ingegneria Informat-
ica, Edizione 2007. Pitagora Ed., Bologna, 2007.
[27] Giaquinta, M., and Modica, G. Mathematical Analysis, vol. 5. Foundations and
Advanced Techniques for Functions of Several Variables. Birkh¨auser, Boston, 2012.


Index
algebra, 35
events, 35
algorithm
Hastings–Metropolis, 230
Bernoulli
distribution, 42, 46
process
ﬁnite, 41, 295
inﬁnite, 44, 45, 95, 103, 151,
296
Bernstein polynomials, 152
binary sequences, 45
inﬁnite Bernoulli process, 46,
103
binomial
coefﬁcient, 1
generalized binomial, 3
series, 3
cardinality
denumerable, 45
multiplicity function, 62
of the continuum, 45
of the set of
derangements, 9, 63
functions, 12
lists, 11
multisets, 11
permutations, 8, 9
subsets, 8
central limit, 160
continuity correction, 163
uniform bound, 161
A First Course in Probability and Markov Chains, First Edition. Giuseppe Modica and Laura Poggiolini.
© 2013 John Wiley & Sons, Ltd. Published 2013 by John Wiley & Sons, Ltd.
uniform convergence, 161
coefﬁcient
binomial, 1
correlation, 123
generalized binomial, 3
multinomial, 20
collocations of objects
identical, 22
multiplicative property, 23
multiplicity function, 22
pairwise distinct, 19–21
grouping map, 19
multiplicative property, 23
conditional probability, 51
Bayes, 52
multiplication formula, 51
transition probabilities, 174
continuous semigroup, 250, 321
differentiability, 322, 323
exponential of a matrix, 321
inﬁnitesimal generator, 250, 322
intensities matrix, 250
matrix exponential, 250
convergence
almost everywhere, 298–300
almost sure, 143–5, 159, 166
in L2, 143
in law, 159, 308, 310
in measure, 298–300
in probability, 143
weak of measures, 160, 308
counting process, 158
covariance, 121

328
INDEX
density
mass, 41, 43
of the sum, 130
with respect to the Lebesgue
measure, 73
derangement, 9
derivative of the determinant, 316
Dirac delta, 292
approximation of, 322
distribution
Bernoulli, 42, 46
Bernoulli trial, 91
binomial, 91
χ-square, 110, 137
discrete, 84
Erlang, 110, 139
exponential, 106, 107
Gamma, 108
Gaussian, 104
geometric, 98, 100
hypergeometric, 93, 117
intensity, 110
joint, 113–14, 116
Lebesgue, 95
marginals, 115
mass density, 84
negative binomial, 94
normal, 104
of a composition of random
variables, 77
of a random variable, 70
of rare events, 96
of the minimum of independent
exponentials, 133
of the sum of
independent exponentials, 139
independent Gammas, 134
independent Gaussians, 133
independent Poisson, 132
independent uniformly
distributed, 131
independent variables, 130
squared independent
Gaussians, 137
variables, 116
Pascal, 94
Poisson, 95
triangular, 131
uniform, 41, 74, 102
on an interval, 44
waiting time at the trafﬁc light,
74
drawing
ordered, 16
from a multiset, 17
from a set, 16
multiplicative property, 18
simple, 16
from a multiset, 17
from a set, 17
multiplicative property, 18
with repetition, 16
equations
Chapman–Kolmogorov, 248
events, 34
almost impossible, 37
almost sure, 37
atomic, 37
certain, 34
complementary, 34
detected by a random variable,
70, 72
impossible, 34
incompatible, 34
independent, 124
rare, 96
expectation, 75
expected number of
ﬁxed points, 33
points in subsets, 33
expected value, 75
from the distribution, 80
from the law, 80
of a composition, 116
of binary operations, 117
of the product, 127
of the sum, 116
variational characterization,
84

INDEX
329
failure rate, 110
family of sets
algebra, 270
closed under ﬁnite intersections,
272
semiring, 274
σ-algebra, 270
σ-algebra generated, 270
σ-algebra of Borel sets, 270
ﬁxed point of a
contraction, 208
map, 207
permutation, 9
stochastic matrix, 209
formula
Bayes, 52
clinic tests, 52
spam ﬁlters, 53
binomial coefﬁcients, 2, 4
Cavalieri, 80, 280, 286
integration by parts, 90
inverse of Pascal matrix, 5, 6
Jordan decomposition, 318
Newton binomial, 1
reduction for multiple integrals,
296
Sylvester, 62
Vandermonde, 93, 269
function
absolutely continuous, 307
approximation by sampling, 282
Bernstein polynomials, 152
characteristic, 39
contraction, 208
distribution function, 70
empirical distribution, 155
entropy, 157
Euler, 66
ﬁxed point, 207
hazard, 110
image, 12
image-list, 8, 12
image-word, 12
integrable, 76, 283
iterated composition
path, 207
sink, 207
Lebesgue representative, 306
mass density, 73
measurable, 76, 280, 281
multiplicity of a covering, 61
of a random variable, 77
simple, 39, 282
square integrable, 120
summable, 76, 283
Geiger counter, 241
graph, 170
arc, 170
incidence matrix, 170
matrix of arcs, 172
node, 170
oriented, 170
hazard function, 110
hitting time
expected, 195, 197
probabilities, 196
holding time
expected, 198
probabilities, 198
independence
Bernoulli process
ﬁnite, 41
inﬁnite, 44
of a sequence of random
variables, 128, 151
of compositions, 128
of events, 124
of holding times, 223, 245
of random variables, 127
of sequences, 165
product measure, 127
reliability of systems, 125, 126
inequality
between mean and mean square,
121
Chebyshev, 82, 287
L2 estimate, 140

330
INDEX
inequality (continued)
Markov, 82, 287
weak estimate, 140
inﬁnitesimal generator, 250
integral, 283
absolute continuity, 305
Lebesgue, 76
linearity of, 39
Monte Carlo method, 153
of a composition, 81
of a random variable, 75
of discrete value functions, 291
of simple functions, 283
on countable sets, 292
with respect to
a measure, 39
an absolutely continuous
measure, 294
Dirac delta, 292
the counting measure, 293
the image measure, 79, 289
the joint distribution, 116
the sum of measures, 292
law, 70
absolutely continuous, 74
differentiable a.e., 74
discrete, 73
empirical, 155
joint, 114
of a composition of random
variables, 77
of a measure, 277
of large deviations, 141
of the iterated logarithm, 151
of total probability, 38, 51–2
strong of large numbers, 146,
147, 166
uniform approximation, 156
weak of large numbers, 141–2
Lebesgue points, 306
lemma
Borel–Cantelli, 142
Fatou, 301
Gr¨onwall, 313
Markov chain, 174
asymptotic behaviour, 183
canonical, 179–180
Chapman–Kolmogorov
equations, 248
composition, 256
Ehrenfest diffusion model, 185
ergodic theorem, 225
existence of a, 179, 180, 256,
258
expected return time, 193
hitting time, 195
holding time, 198, 223, 245, 259
Q-matrix, 259
homogeneous, 177
Monte Carlo method, 230
Poisson process, 251
queue, 186
random walk, 183
renewal property, 177
return time, 238
right-continuous, 250
stochastic matrix, 250
stopping time, 177
transition probabilities, 246
visits, 178
with two states, 181
mass density, 41, 43
matrix
exponential, 250
exponential of a, 266, 314–5,
317
continuous semigroup, 321,
322
Jordan decomposition, 318
Putzer method, 319
similarity transformations
method, 317
incidence, 170
Q-matrix, 251
stochastic, 169, 246
asymptotic behaviour, 254
Chapman–Kolmogorov
equations, 248
continuous semigroup, 250

INDEX
331
ﬁxed point, 254
intensities, 250
uniformization, 255
transition probabilities, 246
mean value, 75
measure, 36, 271
P-complete, 37, 272
absolutely continuous, 73
density, 73
approximation of Borel sets, 277
atomic, 73
mass density, 73
continuity, 38, 271
discrete, 73
mass density, 73
image, 289
integral, 39
law, 277
Lebesgue, 44, 103, 274
probability, 36
product, 127, 294–5
push-forward, 70, 77, 114, 289
restriction, 278
σ-ﬁnite, 272
Stieltjes–Lebesgue, 275–6
median, 83
variational characterization, 83
memoryless property, 100, 106–7
method
Carath´eodory, 274
Jordan decomposition, 318
Markov chain Monte Carlo,
228
Monte Carlo, 153
Putzer, 319
similarity transformations, 317
uniformization, 255, 317
multiindex, 60
multiplicity of a covering, 61
norm
of a matrix
L1, 169, 210, 255
maximum expansion
coefﬁcient, 264
ODE
Cauchy problem, 313
linear systems of, 313
existence for the Cauchy
problem, 315
uniqueness for the Cauchy
problem, 314
P-almost everywhere, 287–8
paradox
birthday’s, 30
gambler’s ruin, 198
Pascal triangle, 1
permutation
derangement, 9
ﬁxed point, 9
image-list, 9
image-word, 9
with no ﬁxed points, 9
power series, 261
binomial series, 3
expansion
of the exponential, 263
of the inverse matrix, 265
of the logarithm, 263
geometric series, 262
matrix exponential, 266
matrix valued, 265
radius of convergence, 261,
264–5
sum, 6
term by term differentiation, 262
term by term integration, 262
vector valued, 264
principle
inclusion–exclusion, 60, 62
probability
A given B, 51
σ-algebra of events, 35
algebra of events, 35
certain event, 34
conditional, 51
distribution, 28
event, 34
horse racing, 28

332
INDEX
probability (continued)
inclusion–exclusion principle,
60, 62
interpretation
classic, 27
frequentist, 27
subjectivist, 27
measure, 36
of at least k events among n
given, 63
of one event among n given, 60,
62
space, 36
uniform, 41
problem
Cauchy for ODE, 313
m´enage problem, 64
three cards, 59
product
Cauchy, 263, 264
convolution, 130, 263–4
property
Markov, 174
memoryless, 100, 106–7
multiplicative
of collocations, 23
of drawings, 18
renewal, 177
strong Markov, 178
Q-matrix, 251
eigenvalues, 253
exponential
asymptotic behaviour, 254
ﬁxed point, 254
uniformization, 255–6
exponential of a, 252
kernel, 254
radius of convergence, 261, 265
random variable, 69
detected events, 72
distribution, 70
distribution function, 70
expectation, 75
expected value, 75
generated events, 72
integrable, 76
integral, 75
law, 70
mean value, 75
median, 83
RN-valued, 114
square integrable, 120–121
standard deviation, 82
summable, 76
variance, 82
with ﬁnite expected value and
variance, 121
X follows μ, 70
random variables
almost sure convergence, 146,
147
correlation coefﬁcient, 123
covariance, 121
identically distributed, 128
independent, 127–8
integral
with respect to joint
distribution, 116
joint distribution, 113–14, 116
joint law, 114
uncorrelated, 122
random walks, 178
reliability of systems
parallel connected, 125
series connected, 126
σ-algebra, 35, 270
completion, 37, 272
events, 35
of Borel sets, 36, 271
of events, 72
selection
ordered, 16
from a multiset, 17
from a set, 16
simple, 16
from a multiset, 17
from a set, 17

INDEX
333
with repetition, 16
semiring, 274
sequences
convolution product, 263
set
Borel, 36, 271
measurable, 36
negligible, 37, 272
null, 37, 272, 287
singleton, 37
set function
measure, 271
σ-additive, 274
σ-ﬁnite, 272
standard deviation, 82
state
j leads to i, 171
aperiodic, 233
communicating, 171, 193, 203
period, 233
i →j, 171
period, 233
periodic, 233
positive recurrent, 194, 221
recurrent, 192–3, 203
transient, 192, 203
statistics
Bose–Einstein, 24
Fermi–Dirac, 24
Maxwell–Boltzmann, 24
stochastic matrix, 169
absorbing class, 201
canonical form, 202
powers, 204
canonical representation, 202
closed class, 201
convergence of powers, 212
convex envelop, 210
eigenvalues, 213
ﬁxed points, 212
irreducible, 171, 221
minimal closed class, 201
regular, 210
asymptotics, 212
convergence of powers, 214
eigenvalues, 213
ﬁxed point, 218
Jordan normal form, 214
sink, 221
transition probabilities, 174
stochastic process, 246
Bernoulli, 44
counting, 256
discrete time, 173
homogeneous, 174, 177, 246
Markov chain, 174, 247
Markov property, 174–5, 177
Markovian, 247
path, 173
point process, 256
Poisson, 241, 243
holding time, 245
intensity, 243
right-continuous, 249
state-space, 173
states, 173
transition matrix, 174
transition probabilities, 246
stochastic vector, 169
stopping time, 177
strong Markov property, 178
theorem
Banach ﬁxed point, 208
Bayes, 52
Beppo Levi, 77, 284
Beppo Levi for series, 300
Bernstein, 152
Berry–Esseen, 161
Brouwer ﬁxed point, 207
Caley–Hamilton, 320
Carath´eodory, 274
Carnot, 122
central limit, 160
Cernoff, 141
coincidence criterion for
measures, 273
differentiability of continuous
semigroups, 323
dominated convergence, 302

334
INDEX
theorem (continued)
for series, 303
Erd¨os–Feller–Pollard, 234
ergodic, 225
Etemadi, 147
existence for the Cauchy
problem, 315
Fubini, 296
Fubini–Tonelli, 296
fundamental of calculus, 74, 307
Glivenko–Cantelli, 155
Hartman–Wintner, 151
inclusion–exclusion, 62
Khitchin, 142
Kolmogorov, 296
Kolmogorov 0-1, 143
Lebesgue differentiation, 306
Lebesgue for series, 304
Markov chain realization, 179,
180
Metropolis, 232
Perron–Frobenius, 209
Rademacher, 308
Rajchman, 146
renewal, 234
strong law of large numbers,
146, 147
Touchard, 64
uniformization, 256
uniqueness for the Cauchy
problem, 314
Vitali, 74
Vitali characterization of
absolute continuity, 307
Vitali differentiation of
monotone functions, 307
variance, 82
of the sum, 122
variational characterization
expected value, 84
median, 83
visits
expected number of, 191–2
expected return time, 194, 218,
239
expected return times, 218
ﬁrst passage time probabilities,
187, 188, 218
frequency of, 239
multiple visits, 218
number of, 189, 218
probability of more, 190
return probabilities, 188
return time, 188
expected, 193
steps between, 223
time for a ﬁrst visit, 178
time to r visits, 189
waiting time, 187
waiting time, 99, 158
weak estimate, 287

