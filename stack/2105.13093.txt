Towards Understanding Knowledge Distillation
Mary Phuong 1 Christoph H. Lampert 1
Abstract
Knowledge distillation, i.e. one classiﬁer being
trained on the outputs of another classiﬁer, is an
empirically very successful technique for knowl-
edge transfer between classiﬁers. It has even been
observed that classiﬁers learn much faster and
more reliably if trained with the outputs of another
classiﬁer as soft labels, instead of from ground
truth data. So far, however, there is no satisfactory
theoretical explanation of this phenomenon. In
this work, we provide the ﬁrst insights into the
working mechanisms of distillation by studying
the special case of linear and deep linear clas-
siﬁers. Speciﬁcally, we prove a generalization
bound that establishes fast convergence of the ex-
pected risk of a distillation-trained linear classiﬁer.
From the bound and its proof we extract three key
factors that determine the success of distillation:
data geometry – geometric properties of the data
distribution, in particular class separation, has an
immediate inﬂuence on the convergence speed
of the risk; optimization bias – gradient descent
optimization ﬁnds a very favorable minimum of
the distillation objective; and strong monotonicity
– the expected risk of the student classiﬁer always
decreases when the size of the training set grows.
1. Introduction
In 2014, Hinton et al. (2014) made a surprising observa-
tion: they found it easier to train classiﬁer using the real-
valued outputs of another classiﬁer as target values than
using actual ground-truth labels. Calling the procedure
knowledge distillation, or distillation for short, they noticed
the positive effect to occur even when the existing classiﬁer
(called teacher) was trained on the same data as it used
afterwards for the distillation-training of the new classiﬁer
(called students). Since that time, the positive properties of
1IST Austria (Institute of Science and Technology Austria).
Correspondence to: Mary Phuong <bphuong@ist.ac.at>.
Proceedings of the 36 th International Conference on Machine
Learning, Long Beach, California, PMLR 97, 2019. Copyright
2019 by the author(s).
distillation-based training has been conﬁrmed several times:
the optimization step is generally more well-behaved than
the optimization step in label-based training, and it needs
less if any regularization or speciﬁc optimization tricks.
Consequently, in several ﬁelds, distillation has become a
standard technique for transfering the information between
classiﬁers with different architectures, such as from deep to
shallow neural networks or from ensembles of classiﬁers to
individual ones.
While the practical beneﬁts of distillation are beyond doubt,
its theoretical justiﬁcation remains almost completely un-
clear. Existing explanations rarely go beyond qualitative
statements, e.g. claiming that learning from soft labels
should be easier than learning from hard labels, or that in a
multi-class setting the teacher’s output provides information
about how similar different classes are to each other.
In this work, we follow a different approach. Instead of
studying distillation in full generality, we restrict our at-
tention to a simpliﬁed, analytically tractable, setting: bi-
nary classiﬁcation with linear teacher and linear student
(either shallow or deep linear networks). For this situa-
tion, we achieve the ﬁrst quantitative results about the ef-
fectiveness of distillation-based training. Speciﬁcally, our
main results are: 1) We prove a generalization bound
that establishes extremely fast convergence of the risk
of distillation-trained classiﬁers. In fact, it can reach zero
risk from ﬁnite training sets. 2) We identify three key
factors that explain the success of distillation: data ge-
ometry – geometric properties of the data distribution, in
particular class separation, directly inﬂuence the conver-
gence speed of the student’s risk; optimization bias – even
though the distillation objective can have many optima, gra-
dient descent optimization is guaranteed to ﬁnd a particu-
larly favorable one; and strong monotonicity – increasing
the training set always decreases the risk of the student
classiﬁer.
2. Related Work
Ideas underpinning distillation have a long history dating
back to the work of Ba & Caruana (2014); Bucilua et al.
(2006); Craven & Shavlik (1996); Li et al. (2014); Liang
et al. (2008). In its current and most widely known form,
it was introduced by Hinton et al. (2014) in the context of
arXiv:2105.13093v1  [cs.LG]  27 May 2021

Towards Understanding Knowledge Distillation
neural network compression.
Since then, distillation has quickly gained popularity among
practitioners and established its place in deep learning folk-
lore. It has been found to work well across a wide range of
applications, including e.g. transferring from one architec-
ture to another (Geras et al., 2016), compression (Howard
et al., 2017; Polino et al., 2018), integration with ﬁrst-order
logic (Hu et al., 2016) or other prior knowledge (Yu et al.,
2017), learning from noisy labels (Li et al., 2017), defending
against adversarial attacks (Papernot et al., 2016), training
stabilization (Romero et al., 2015; Tang et al., 2016), dis-
tributed learning (Polino et al., 2018), reinforcement learn-
ing (Rusu et al., 2016) and data privacy (Celik et al., 2017).
In contrast to the empirical success, the mathematical prin-
ciples underlying distillation’s effectiveness have largely
remained a mystery. Only very works examine distillation
from a theoretical perspective. Lopez-Paz et al. (2016) cast
distillation as a form of learning using privileged informa-
tion (LUPI, Vapnik & Izmailov 2015), a learning setting
in which additional per-instance information is available at
training time but not at test time. However, the LUPI view
concentrates on the aspect that the teacher’s supervision to
the student is noise-free. This argument fails to explain, e.g.,
the success of distillation even when the original problem
is noise-free to start with. The only other theoretical anal-
ysis we are aware of is by Urner et al. (2011), who study
distillation as a form of semi-supervised learning. Specif-
ically, they show that a two-step procedure, consisting of
ﬁrst training a teacher on a small labelled dataset and then
training the student on a separate large dataset labelled by
the teacher, can be more effective than training the student
directly on the small labelled dataset. The paper’s focus is
on the semi-supervised aspect, i.e. the gains from having a
large unlabelled dataset.
A more distantly related topic is machine teaching (Zhu,
2015). In machine teaching, a machine learning system
is trained by a human teacher, whose goal is to hand-pick
as small a training set as possible, while ensuring that the
machine learns a desired hypothesis. Transferring knowl-
edge via machine teaching techniques is extremely effective:
perfect transfer is often possible from a small ﬁnite teaching
set (Zhu, 2013; Liu & Zhu, 2016). However, the price for
this radical reduction in sample complexity is the expensive
training set construction. Our work shows that, at least in
the linear setting, distillation achieves a similar effectiveness
with a more practical form of supervision.
3. Background: Linear Distillation
We formally introduce distillation in the context of binary
classiﬁcation. Let X ⊆Rd be the input space, Y = {0, 1}
the label space, and Px the probability distribution of inputs.
We assume Px has a density.
The teacher h∗: X →Y is a ﬁxed linear classiﬁer, i.e.
h∗(x) = 1{w⊺
∗x ≥0} for some w∗∈Rd \ {0}, where
1{.} returns 1 if the argument is true and 0 otherwise. The
student also is a linear classiﬁer, h(x) = 1{w⊺x ≥0}.
We allow the weight vector to be parameterised as a product
of matrices, w⊺= WNWN−1 · · · W1 for some N ≥1.
When N ≥2, this parameterisation is known as a deep
linear network. Although deep linear networks have no
additional capacity compared to directly parameterised lin-
ear classiﬁers (N = 1; w⊺= W1), they induce different
gradient-descent dynamics, and are often studied as a ﬁrst
step towards understanding deep nonlinear networks (Saxe
et al., 2014; Kawaguchi, 2016; Hardt & Ma, 2017).
Distillation proceeds as follows. First, we collect a transfer
set {(xi, yi)}n
i=1 consisting of inputs xi sampled i.i.d. from
Px, and soft labels yi = σ(w⊺
∗xi) provided by the teacher,
where σ is the sigmoid function, σ(x) = 1/(1 + exp(−x)).
The soft (real-valued) labels can be thought of as a more in-
formative version of the hard (0/1-valued) labels of the stan-
dard classiﬁcation setting. We write X = [x1, . . . , xn] ∈
Rd×n for the data matrix. Second, the student is trained by
minimizing the (normalized) cross-entropy loss,
L1(w) = −1
n
n
X
i=1
h
yi log σ(w⊺xi)
+ (1 −yi) log(1 −σ(w⊺xi))
i
−L∗,
(1)
where L∗is a normalization constant, such that the mini-
mum of L1 is 0. It only serves the purpose of simplifying
notation and has no effect on the optimization.
The student observes the loss as a function of its parameters,
i.e. the individual weight matrices,
L(W1, . . . , WN) := L1((WNWN−1 · · · W1)⊺),
(2)
and optimizes it via gradient descent. For the theoretical
analysis, we avoid the complications of stepsize selection
and adopt the notion of inﬁnitesimal step size1, which turns
the gradient descent procedure into a continuous gradient
ﬂow. We write Wi(τ) for the value of the matrix Wi at
time τ ∈[0, ∞), with Wi(0) denoting the initial value, and
w(τ)⊺= WN(τ) · · · W1(τ). Then, each Wi(τ), for i ∈
{1, . . . , N}, evolves according to the following differential
equation.
∂Wi(τ)
∂τ
= −∂L
∂Wi
(W1(τ), . . . , WN(τ)).
(3)
The student is trained until convergence, i.e. τ →∞. We
measure the transfer risk of the trained student, deﬁned as
1For readers who are unfamiliar with gradient ﬂows, it sufﬁces
to think of the stepsize as ﬁnite and ”sufﬁciently small”.

Towards Understanding Knowledge Distillation
the probability that its prediction differs from that of the
teacher,
R(h) =
P
x∼Px [h(x) ̸= h∗(x)].
(4)
In Section 4.2, we will derive a bound for the transfer risk
and establish how rapidly it decreases as a function of n.
4. Generalization Properties of Linear
Distillation
This section contains our main technical results. First, in
Section 4.1, we provide an explicit characterization of the
outcome of distillation-based training in the linear setting.
In other words, we identify what the student actually learns.
In particular, we prove that the student is able to perfectly
identify the teacher’s weight vector, if the number of training
examples (n) is equal to the dimensionality of the data (d)
or higher. If less data is available, under minor assumptions,
the student ﬁnds the best approximation of the teacher’s
weight vector that is possible within the subspace spanned
by the training data.
In Section 4.2 we use these results to study the generaliza-
tion properties of the student classiﬁer, i.e. we characerize
how fast the student learns. Speciﬁcally, we prove a gener-
alization bound with much more appealing properties than
what is possible in the classic situation of learning from
hard labels. As soon as enough training data is available
(n ≥d), the student’s risk is simply 0. Otherwise, the risk
can be bounded explicitly in a distribution-dependent way
that, in particular, allows us to identify three key factors that
explain the success of distillation, and to understand when
distillation-based transfer is most effective.
4.1. What Does the Student Learn?
In this section, we derive in closed form the asymptotic solu-
tion to the gradient ﬂow (3) undergone by the student when
trained by distillation. We state the results separately for
directly parameterized linear classiﬁers (N = 1) and deep
linear networks (N ≥2), as the settings require slightly
different ways of initializing parameters. Namely, in the
former case, initializing w(0) = 0 is valid, while in the
latter case, this would lead to vanishing gradients, and we
have to initialize with small (typically random) values.
Theorem 1. Assume the student is a directly parameterised
linear classiﬁer (N = 1) with weight vector initialised at
zero, w(0) = 0. Then, the student’s weight vector fulﬁlls
almost surely
w(t) →ˆw,
(5)
for t →∞, with
ˆw =

w∗,
n ≥d,
X(X⊺X)−1X⊺w∗,
n < d.
(6)
Theorem 1 shows a remarkable property of distillation-based
training for linear systems: if sufﬁciently many (at least d)
data points are available, the student exactly recovers the
teacher’s weight vector, w∗. This is a strong justiﬁcation
for distillation as a method of knowledge transfer between
linear classiﬁers and the theorem establishes that the effect
occurs not just in the inﬁnite data limit (n →∞), as one
might have expected, but already in the ﬁnite sample regime
(n ≥d).
When few data points are available (n < d), the weight
vector learned by the student is simply the projection of the
teacher’s weight vector onto the data span (the subspace
spanned by the columns of X). In a sense, this is the best
the student can do: the gradient descent update direction
∂w(τ)
∂τ
always lies in the data span, so there is no way for the
student to learn anything outside of it. The projection is the
best subspace-constrained approximation of w∗with respect
to the Euclidean norm. The extent to which Euclidean close-
ness implies closeness in predictions is a separate matter,
and the subject of Section 4.2.
Proof sketch of Theorem 1. First, notice that ˆw is a global
minimiser of L1. Moreover, when n ≥d, it is (almost surely
wrt. X ∼P n
x ) unique, and when n < d, it is (almost surely)
the only one lying in the span of X and thus potentially
reachable by gradient descent.
The proof consists of two parts. We prove that a) the gradient
ﬂow (3) drives the objective value towards the optimum,
L1(w(t)) →0 as t →∞, and b) the distance between
w(t) and the claimed asymptote ˆw is upper-bounded by the
objective gap,
∥w(t) −ˆw∥2 ≤cL1(w(t))
(7)
for some constant c > 0 and all t ∈[0, ∞).
For part a), observe that L1 is convex. For any τ ∈[0, ∞),
the time-derivative of L1(w(τ)) is negative unless we are
at a global minimum,
d
dτ L1(w(τ)) = ∇L1(w(τ))⊺
∂w(τ)
∂τ

= −
∇L1(w(τ))
2,
(8)
implying that the objective value L1(w(τ)) decreases
monotonically in τ.
Hence, if we denote by W
=

w : L1(w) ≤L1(0)
	
the L1(0)-sublevel set of the objec-
tive, we know that w(τ) ∈W for all τ ∈[0, ∞). One can
show that on this set, L1 satisﬁes strong convexity, but only
along certain directions: for some µ > 0 and all w, v ∈W
such that v −w ∈span(X),
L1(v) ≥L1(w)+∇L1(w)⊺(v−w)+ µ
2 ∥v −w∥2. (9)

Towards Understanding Knowledge Distillation
This allows us (via a technical derivation that we omit here)
to relate the objective gap to the gradient norm: it can be
shown that there exists c′ > 0, such that
c′L1(w) ≤1
2
∇L1(w)
2.
(10)
Applying the above to w(τ) in (8), we are able to bound
the amount of reduction in the objective in terms of the
objective itself, ultimately proving linear convergence.
For part b), invoke (9) with v = w(τ) and w = ˆw; this
gives L1(w(τ)) ≥µ
2 ∥w(τ) −ˆw∥2.
The full proof is given in the Supplementary Material.
The next results is the analog of Theorem 1 for deep lin-
ear networks. Here, some technical conditions are needed
because the parameters cannot all be initialized at 0.
Theorem 2. Let ˆw be deﬁned as in Theorem 1. Assume the
student is a deep linear network, initialized such that for
some ϵ > 0,
∥w(0)∥< min
n
∥ˆw∥, ϵN
ϵ2∥ˆw∥−2
N + ∥ˆw∥2−2
N
−N
2 o
,
(11)
L1(w(0)) < L1(0),
(12)
Wj+1(0)⊺Wj+1(0) = Wj(0)Wj(0)⊺
(13)
for j = 1, . . . , N −1. Then, for n ≥d, student’s weight
vector fulﬁlls almost surely
w(t) →ˆw,
(14)
and for n < d,
∥w(t) −ˆw∥≤ϵ,
(15)
for all t large enough.
The interpretation of the theorem is analogous to Theorem 1.
Given enough data (n ≥d), the student learns to perfectly
mimic the teacher. Otherwise, it learns an approximation at
least ϵ-close to the projection of the teacher’s weight vector
onto the data span.
The conditions (11)–(13) appear for technical reasons and
a closer look at them shows that they do not pose prob-
lems in practice. Condition (11) states that the network’s
weights should be initialised with sufﬁciently small values.
Consequently, this assumption is easy to satisfy in practice.
Condition (12) requires that the initial loss is smaller than
the loss at w = 0. This condition guarantees that the gra-
dient ﬂow does not hit the point w = 0, where all gradient
vanish and the optimization would stop prematurely. In
practice, when the step size is ﬁnite, the condition is not
needed. Nevertheless, it is also not hard to satisfy: for any
near-zero initialisation, w(0) = w0, either w0 or −w0 will
satisfy (12), so at most one has to ﬂip the sign on one of the
Wi(0) matrices. Finally, condition (13) is called balanced-
ness (Arora et al., 2018) and discussed in-depth in (Arora
et al., 2019)). It simpliﬁes the analysis of matrix products
and makes it possible to explicitly analyze the evolution of
w induced by gradient ﬂow in the Wi’s. Assuming near-
zero initialization, the condition is automatically satisﬁed
approximately and there is some evidence (Arora et al.,
2019) suggesting that approximate balancedness may suf-
ﬁce for convergence results of the kind we are interested
in. Otherwise, the condition can also simply be enforced
numerically.
Proof sketch of Theorem 2. First, we establish convergence
in the objective, L1(w(t)) →0 as t →∞, similarly to the
case N = 1. Unlike that case, however, the evolution of
the end-to-end weight vector w(τ) is governed by complex
mechanics induced by gradient ﬂow in Wi’s. A key tool
for analyzing this induced ﬂow was recently established
in (Arora et al., 2018): the authors show that the induced
ﬂow behaves similarly to gradient ﬂow with momentum
applied directly to w. Making use of this result, one can
proceed analogously as in the case of N = 1 to show con-
vergence in the objective.
Second, to show convergence in parameter space, we de-
compose w(t) into its projection onto the span of X, and an
orthogonal component. The X-component converges to ˆw,
by strong convexity arguments as in the case N = 1. It re-
mains to show that the orthogonal component is small. Now,
recall that in the case N = 1, we initialise at w(0) = 0
and move within the span, so the orthogonal component is
always zero. When N ≥2, the situation is different: a)
we initialise with a potentially non-zero orthogonal compo-
nent (because we need to avoid the spurious stationary point
w = 0), and b) the momentum term causes the orthogonal
component to grow during optimisation. Luckily, the rate
of growth can be precisely characterised and controlled by
the initialisation norm ∥w(0)∥, so depending on how close
to zero we initialise, we can upper-bound the size of the
orthogonal component. This yields a bound on the distance
∥w(t) −ˆw∥.
For the formal proof, we refer the reader to the Supplemental
Material.
4.2. How Fast Does the Student Learn?
In this section, we present our main quantitative result, a
bound for the expected transfer risk in linear distillation.
We ﬁrst introduce some geometric concepts. For any u, v ∈
Rd \ {0}, denote by ¯α(u, v) ∈[0, π/2] the unsigned angle

Towards Understanding Knowledge Distillation
w∗
w∗
w∗
Px
Px
Px
Task A
Task B
Task C
0
¼=2
1
p(µ)
0
¼=2
1
p(µ)
0
¼=2
1
p(µ)
Figure 1. Schematic illustration of p(θ) for three different transfer
tasks. In Task A, the angular alignment between the data and the
teacher’s weight vector is high, so p(θ) is fast descreasing. In Task
B, it is also high, and in additional the classes are separated by a
margin, so p(θ) reaches 0 before β = π/2. In Task C, the angular
alignment is low, so p(θ) decreases rather slowly.
between the vectors u and v
¯α(u, v) = cos−1

|u⊺v|
∥u∥· ∥v∥

.
(16)
A key quantity for us is the angle between w∗and a ran-
domly chosen x, for x ∼Px. For a given transfer task
(Px, w∗), we denote by p the reverse cdf of ¯α(w∗, x),
p(θ) =
P
x∼Px[¯α(w∗, x) ≥θ]
for
θ ∈[0, π/2]. (17)
By construction, p(θ) is monotonically decreasing, starting
with p(0) = 1 and approaches 0 for θ →π/2. Figure 1
illustrates this behavior for three exemplary data distribu-
tions as Tasks A,B and C. In Task A, the probability mass
is well aligned with the direction of the teacher’s weight
vector. The probability that a randomly chosen data point
x ∼Px has a large angle with w∗is small. Therefore, the
value of p(θ) quickly drops with growing angle θ. In Task B,
the data also aligns well with w∗, but in addition, the data
region remains bounded away from the decision boundary.
Therefore, certain large angles can never occur, i.e. there
exists a value θ0 < π/2, such that p(θ) = 0 for θ ≥θ0. In
Task C, the situation is different: the data distribution is con-
centrated along the decision boundary and the probability
of a angle between w∗and a randomly chosen data point
x ∼Px is large. As a consequence, p(θ) drops more slowly
with growing angle than in the previous two settings.
We are now ready to state the main result. For improved
readability, we phrase it for a student with inﬁnitesimally
small initialization, i.e. ϵ →0. The general formulation can
be found in the supplemental material.
w ¤
Margin, ¯ = 5¼=12
w ¤
Polynomial, ∙= 1:0
w ¤
Polynomial, ∙= 2:0
Figure 2. Examples of 2D distributions that fulﬁll the large-margin
condition (left) and the polynomial condition with different param-
eters (center, right).
Theorem 3 (Transfer risk bound for linear distillation). For
any training set X ∈Rd×n, let ˆhX(x) = 1{ ˆw⊺x ≥0} be
the linear classiﬁer learned by distillation from a teacher
with weight vector w∗. Then, when n ≥d, it holds that
E
X∼P ⊗n
x
h
R
 ˆhX
i
= 0.
(18)
For n < d, it holds for any β ∈[0, π/2] that
E
X∼P ⊗n
x
h
R
 ˆhX
i
≤p(β) + p(π/2 −β)n
(19)
Equation (18) is unsurprising, of course, because in Sec-
tion 4.1 we already established that for n ≥d the student is
able to perfectly mimic the teacher.
Inequality (19), however, is –to our knowledge– the ﬁrst
quantitative characterization how well a student can learn
via distillation.
Before we provide the proof sketch, we present two in-
stantiations of the bound for speciﬁc classes of tasks that
provide insight how fast the right hand side of (19) actually
decreases.
The margin case. The ﬁrst class of tasks we consider are
tasks in which the classes are separated by an angular mar-
gin, illustrated in Figure 2 (left). These tasks are char-
acterized by a ‘wedge’ of zero probability mass near the
boundary2. For these tasks, we obtain from Theorem 3 that
the expected risk decays exponentially in n, up to n = d−1.
Corollary 1 (Transfer risk of large-margin distributions).
If there exists β ∈[0, π/2] such that p(β) = 0 and γ :=
p(π/2 −β) < 1, then
E
X∼P n
x
h
R
 ˆhX
i
≤γn.
(20)
The polynomial case. The second class are tasks for which
we can upper-bound p by a κ-order polynomial. This can
2In bounded domains this condition is, in particular, fulﬁlled in
the classical margin situation (Sch¨olkopf & Smola, 2002), when
the classes are separated by a positive distance from each other.

Towards Understanding Knowledge Distillation
be done trivially for any task by setting κ = 0.0, but that
choice would yield a vacuous bound. Higher values of κ
correspond to stronger assumptions on the distribution but
enable better rates. Figure 2 (center, right) shows examples
of polynomial distributions for κ ∈{1.0, 2.0}. The special
case κ = 1.0 corresponds to a uniform angle distribution,
while distribution with κ = 2.0 have low probability mass
near the decision boundary, while not necessarily exhibiting
a margin.
The following corollary establishes that for tasks with poly-
nomial behavior of p(θ), the expected risk decays essentially
at a rate of (log n/n)κ or faster.
Corollary 2 (Transfer risk of polynomial distributions). If
there exists a κ ≥0 be such that p(θ) ≤c · (1 −(2/π)θ)κ
for all θ ∈[0, π/2], then
E
X∼P n
x
h
R
 ˆhX)
i
≤c · 1 + (log n)κ
nκ
(21)
Proof. We apply Theorem 3 and insert the polynomial upper
bound for p. For the case n < d, we get
E
X∼P n
x
h
R
 ˆhX
i
≤(1 −(2/π)β)κ + (1 −(2/π)(π/2 −β))nκ.
(22)
Setting β = (π/2) · n−1/n and simplifying the resulting
expressions yields
≤
 1 −e−log n
n κ + n−κ.
(23)
Finally, we use the inequality ex ≥1 + x and the claim
follows.
Note that, in contrast to many results in statistical learning
theory, the bounds are far from vacuous, even when only
little data is available. This can best be seen in Corollary 1,
where γ < 1 and hence γn is an informative upper bound
for the classiﬁcation error. These observations suggest that
distillation operates in a very different regime from classical
hard-target learning. Standard bounds usually have little to
say when n < d and only start to be useful when n ≫d. In
contrast, (linear) distillation ensures perfect transfer when
n ≥d and non-vacuous bounds are possible even when
n < d.
4.3. Proof of Theorem 3
The case n ≥d follows trivially from the result of Theorem
1 and 2. For the case n < d, the following property turns
out to be crucial for obtaining a transfer rate of the form that
we do.
Lemma 1 (Strong monotonicity). Let ˆw(X) denote the
distillation solution ˆw as a function of the training data
X. Then, for any full-rank datasets X−∈Rd×n−and
X+ ∈Rd×n+ such that X−is contained in X+,
¯α(w∗, ˆw(X+)) ≤¯α(w∗, ˆw(X−)).
(24)
Proof. If n+ ≥d, then the left-hand side of (24) is zero
and the claim follows. Otherwise, assume wlog that the ﬁrst
n−columns of X−and X+ coincide. Let Q+R+ = X+
be the QR factorisation of X+ with Q+ ∈Rd×n+ and
R+ ∈Rn+×n+, and similarly for X−. Then ˆw(X+) =
Q+Q⊺
+w∗and
cos(¯α(w∗, ˆw(X+))) =
w⊺
∗Q+Q⊺
+w∗
∥w∗∥·
Q+Q⊺
+w∗

(25)
=
Q⊺
+w∗

∥w∗∥
,
(26)
and an analogous statement holds for X−. Now, because
the ﬁrst n−columns of Q+ coincide with Q−, we have
Q⊺
+w∗
 ≥
Q⊺
−w∗
 and
cos(¯α(w∗, ˆw(X+))) ≥cos(¯α(w∗, ˆw(X−))).
(27)
Taking cos−1 on both sides (and remembering that cos−1 is
decreasing) yields the claim.
For the moment, think of ¯α(w∗, ˆw) as a proxy for the trans-
fer risk, i.e. the closer the trained student ˆw is to the teacher
w∗in terms of angles, the lower the transfer risk. A direct
consequence of Lemma 1, and the reason we call it ‘strong
mononoticity’, is that including additional data in the trans-
fer set can never harm the transfer risk, only improve it.
This property is speciﬁc to distillation; it does not hold in
hard-target learning.
Proof of Theorem 3 (n < d). For nonzero vectors u, v ∈
Rd, we deﬁne α(u, v) ∈[0, π] as a variant of ¯α (Equa-
tion 16) that takes the sign of u⊺v into account,
α(u, v) = cos−1

u⊺v
∥u∥· ∥v∥

.
(28)
We decompose the expected risk as follows:
E
X∼P n
x
h
R
 ˆhX
i
=
P
X∼P n
x
x∼Px
[w⊺
∗x · ˆw⊺x < 0]
=
Z
x:¯α(w∗,x)≥β
P
X∼P n
x
[w⊺
∗x · ˆw⊺x < 0|x] dPx
+
Z
x:¯α(w∗,x)<β, w⊺
∗x>0
P
X∼P n
x
[ ˆw⊺x < 0|x] dPx
+
Z
x:¯α(w∗,x)<β, w⊺
∗x<0
P
X∼P n
x
[ ˆw⊺x > 0|x] dPx.
(29)

Towards Understanding Knowledge Distillation
Let us ﬁx some x for which ¯α(w∗, x) < β and w⊺
∗x > 0
(i.e. an ‘easy’ positive test example); for this x we have
α(w∗, x) =
¯α(w∗, x).
Consider the situation where
¯α(w∗, xi) < π/2 −β for some i (i.e. there is at least one
good teaching point). Then, Lemma 1 with X+ = X and
X−= xi yields ¯α(w∗, ˆw) ≤¯α(w∗, xi) < π/2 −β. Com-
bined with the triangle inequality, we obtain
α( ˆw, x) ≤α(w∗, ˆw) + α(w∗, x)
(30)
≤¯α(w∗, xi) + ¯α(w∗, x) < π/2,
(31)
which implies ˆw⊺x > 0, i.e. a correct prediction (same
as the teacher’s). Conversely, an error can occur only if
¯α(w∗, xi) ≥π/2 −β for all i. Because xi are independent,
we have
P
X∼P n
x
[ ˆw⊺x < 0| x : ¯α(w∗, x) < β, w⊺
∗x > 0]
≤
P
X∼P n
x
[∀i : ¯α(w∗, xi) ≥π/2 −β]
= p(π/2 −β)n.
(32)
By a symmetric argument, one can show that
P
X∼P n
x
[ ˆw⊺x > 0| x : ¯α(w∗, x) < β, w⊺
∗x < 0]
≤p(π/2 −β)n.
(33)
Combining (29), (32) and (33) yields the result:
P
X∼P n
x
x∼Px
[w⊺
∗x · ˆw⊺x < 0] ≤
≤P
x[¯α(w∗, x) ≥β] + P
x[¯α(w∗, x) < β] · p(π/2 −β)n
= p(β) + (1 −p(β)) · p(π/2 −β)n.
5. Why Does Distillation Work?
From the formal analysis in the previous section, three con-
cepts emerge as key factors for the success of distillation:
data geometry, optimization bias, and strong monotonicity.
In this section, we discuss these factors and provide some
empirical conﬁrmation how they affect or explain variations
in the transfer risk.
5.1. Data Geometry
From Theorem 3 we know that the data geometry, in particu-
lar the angular alignment between the data distribution and
the teacher, crucially impact how fast the student can learn.
Formally, this is reﬂected in p(θ): the faster it decreases, the
easier it should be for the student to learn the task.
To experimentally test the effect of data geometry on the
effectiveness of distillation, we adopt the setting of Corol-
lary 2. We consider a series of tasks of varying angular
2-5
2-3
2-1
∙ (angular alignment)
0.0
0.2
0.4
Transfer risk
Figure 3. Transfer risk of linear distillation on tasks of varying
angular alignment.
alignment, as measured by the degree, κ, of the polynomial
by which p(θ) is upper bounded.
Speciﬁcally, for any κ, the task (P κ
x , wκ
∗) is deﬁned by the
following sampling procedure. First, an angle a is sampled
from the κ-polynomial distribution, i.e. P [a ≥θ] = (1 −
(2/π)θ)κ for θ ∈[0, π/2]. Then, a direction z is uniformly
sampled from all unit-length vectors that are at angle a with
the teacher’s weight vector, ¯α(w∗, z) = a. Finally, x = νz
is returned for a random ν, distributed as a one-dimensional
standard Gaussian.
We use an input space dimension of d = 1000 and a transfer
set size n = 20. Then, we train a linear student by distil-
lation on each of the tasks and evaluate its transfer risk on
held-out data. Figure 3 shows the results. The plot shows
a clearly decreasing trend: on tasks with more favorable
data geometry (higher κ), transfer via distillation is more
effective and the student achieves lower risk.
5.2. Optimization Bias
A second key factor for the success of distillation is a spe-
ciﬁc optimization bias. For n < d, the distillation training
objective (1) has many minima of identical function value
but potentially different generalization properties. There-
fore, the optimization method used could have a large impact
on the transfer risk. As Theorems 1 and 2 show, gradient
descent has a particularly favorable bias for distillation.
To verify this observation experimentally, we consider learn-
ers that are guided by an optimisation bias to different de-
grees: at one end of the spectrum is the gradient-descent
learner we have studied in previous sections, while at the
other end is a learner that treats all minimizers of the dis-
tillation training loss equally, i.e. that has no bias toward
any of the solutions. Speciﬁcally, consider learners with
weights of the form wδ = ˆw + δ ∥ˆw∥
∥q∥q, where ˆw is the
gradient-descent distillation solution and q is a Gaussian
random vector in the subspace orthogonal to the data span,
i.e. if X is the data matrix, then X⊺q = 0. All learners
of this form globally minimize the distillation training loss,
and depending on δ, they are more or less guided by the

Towards Understanding Knowledge Distillation
0
20
40
60
80
² (larger = less optimisation bias)
0.0
0.2
Transfer risk
Figure 4. Transfer risk of linear distillation variants with different
degrees of optimization bias, on the digits 0 and 1 of MNIST.
gradient-descent bias: δ = 0 and |δ| →∞represent the
two extremes mentioned above.
We train the learners wδ for δ ∈{0, 10, . . . , 90} on the
digits 0 and 1 of the MNIST dataset, where inputs are treated
as vectors in R784 and the teacher w∗is a logistic regression
trained to classify 0s and 1s on an independent training set.
We set the transfer set size to n = 100 and evaluate the risk
on the test set.
Figure 4 shows the result. There is a clear trend in favor
of learners that are more strongly guided by the gradient-
descent bias (small δ); these learners generally achieve
lower transfer risk. This result supports the idea of opti-
mization bias as a key component of distillation’s success.
5.3. Strong Monotonicity
The third key factor we identify is strong monotonicity,
as established in Lemma 1: training the student on more
data always leads to a better approximation of the teacher’s
weight vector.
Compared to data geometry and optimisation bias, strong
monotonicity is less amenable to experimental study be-
cause it is a downstream property that cannot directly be
manipulated. We therefore take an indirect approach. We
consider a set of learners including the gradient-descent dis-
tillation learner, the hard-target learner, and several learners
with reduced optimisation bias (as in Section 5.2), and train
them on the same task. For each learner, we note its ex-
pected risk calculated on a held-out set, and its monotonicity
index, deﬁned as the probability that an additional training
example reduces the angle between the student’s and the
teacher’s weight vectors rather than increasing it, i.e.
m(w) =
P
X∼P n
x
x∼Px
[¯α(w∗, w([X, x])) < ¯α(w∗, w(X))],
(34)
where the student’s weight vector w is now treated as a
function of the training set. Thus, we can relate a learner’s
risk and its monotonicity.
We train the learners on the polynomial-angle task (P κ
x , wκ
∗)
0.5
0.6
0.7
0.8
0.9
1.0
Monotonicity index
0.00
0.01
0.02
0.03
0.04
0.05
0.06
Transfer risk
Distillation ^w
Hard-target
Dist. reduced bias w±
Figure 5. Expected transfer risk vs. monotonicity of different learn-
ers: gradient-descent based distillation (blue), hard-target learner
(orange), and a series of distillation learners with reduced optimi-
sation bias (green): wδ for δ ∈{1/16, 1/8, 1/4, 1/2, 1}, listed
in order from left to right.
from Section 5.1, with κ = 1, d = 100 and n = 5. The ex-
pected risk as well as the monotonicity index are estimated
as averages over 1000 transfer sets.
The results are shown in Figure 5. There is a negative
correlation between monotonicity and transfer risk, which
supports the intuition of monotonicity as a desirable property
and a possible explanation of distillation’s success.
However, a few reservations are in order. First, as mentioned
above, monotonicity cannot easily be manipulated, so its
effect on transfer risk remains unknown. We can only mea-
sure correlation. Second, monotonicity is of binary nature;
it only captures whether an extra data point helps or not. Yet
for a quantitative characterization of risk, one would have
to capture by how much an extra data point helps. We leave
more reﬁned deﬁnitions of monotonicity for future work.
6. Conclusion
In this work, we have formulated and studied a linear model
of knowledge distillation. Within this model, we have de-
rived a) a characterization of the solution learned by the
student, b) a bound on the transfer risk, meaningful even in
the low-data regime, and c) three key factors that explain the
success of distillation. In doing so, we hope to have enriched
both the current intuitive and theoretical understanding of
distillation, both of which have only been weakly developed.
Our work paints a picture of distillation as an extremely ef-
fective method for knowledge transfer that derives its power
from an optimization bias of gradient-based methods initial-
ized near the origin, which in particular has the effect that
any additionally included training point can only improve
the student’s approximation of the teacher. Distillation fur-
ther beneﬁts strongly from a favorable data geometry, in
particular a margin between classes.
While we have supported this picture by theory and em-

Towards Understanding Knowledge Distillation
pirical work only in the linear case, we hypothesize that
similar properties also govern the behavior of distillation in
the nonlinear setting. If this hypothesis turns out to be true,
it would have implications for the design of transfer sets (a
large teacher model being stored along with only the mini-
mal dataset necessary for future transfer) or active learning
(which samples are most informative to have labeled by the
teacher). Potentially, strong monotonicity could serve as a
leading design principle for new sample-efﬁcient algorithms.
We thus consider the extension to nonlinear models the main
direction for future work.

Towards Understanding Knowledge Distillation
References
Arora, S., Cohen, N., and Hazan, E. On the optimization of
deep networks: Implicit acceleration by overparameteri-
zation. In International Conference on Machine Learing
(ICML), 2018.
Arora, S., Cohen, N., Golowich, N., and Hu, W. A conver-
gence analysis of gradient descent for deep linear neural
networks. In International Conference on Learning Rep-
resentations (ICLR), 2019.
Ba, J. and Caruana, R. Do deep nets really need to be deep?
In Conference on Neural Information Processing Systems
(NIPS), 2014.
Bucilua, C., Caruana, R., and Niculescu-Mizil, A. Model
compression. In Conference on Knowledge Discovery
and Data Mining (KDD), 2006.
Celik, Z. B., Lopez-Paz, D., and McDaniel, P. Patient-driven
privacy control through generalized distillation. In IEEE
Symposium on Privacy-Aware Computing (PAC), 2017.
Craven, M. and Shavlik, J. W. Extracting tree-structured
representations of trained networks. In Conference on
Neural Information Processing Systems (NIPS), 1996.
Geras, K. J., Mohamed, A.-R., Caruana, R., Urban, G.,
Wang, S., Aslan, O., Philipose, M., Richardson, M., and
Sutton, C. Blending LSTMs into CNNs. In Interna-
tional Conference on Learning Representations (ICLR)
Workshop, 2016.
Hardt, M. and Ma, T. Identity matters in deep learning. In
International Conference on Learning Representations
(ICLR), 2017.
Hinton, G., Vinyals, O., and Dean, J. Distilling the knowl-
edge in a neural network. In Deep Learning Workshop at
NIPS, 2014.
Howard, A. G., Zhu, M., Chen, B., Kalenichenko, D.,
Wang, W., Weyand, T., Andreetto, M., and Adam, H.
MobileNets: Efﬁcient convolutional neural networks for
mobile vision applications. In arXiv:1704.04861, 2017.
Hu, Z., Ma, X., Liu, Z., Hovy, E., and Xing, E. Harnessing
deep neural networks with logic rules. In Annual Meeting
of the Association for Computational Linguistics (ACL),
2016.
Kawaguchi, K. Deep learning without poor local minima.
In Conference on Neural Information Processing Systems
(NIPS), 2016.
Li, J., Zhao, R., Huang, J.-T., and Gong, Y. Learning small-
size DNN with output-distribution-based criteria. In Con-
ference of the International Speech Communication Asso-
ciation (Interspeech), 2014.
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J.
Learning from noisy labels with distillation. In Interna-
tional Conference on Computer Vision (ICCV), 2017.
Liang, P., Daum´e III, H., and Klein, D. Structure compi-
lation: trading structure for features. In International
Conference on Machine Learing (ICML), 2008.
Liu, J. and Zhu, X. The teaching dimension of linear learn-
ers. Journal of Machine Learning Research (JMLR), 17
(1):5631–5655, 2016.
Lopez-Paz, D., Bottou, L., Sch¨olkopf, B., and Vapnik, V.
Unifying distillation and privileged information. In Inter-
national Conference on Learning Representations (ICLR),
2016.
Papernot, N., McDaniel, P., Wu, X., Jha, S., and Swami,
A. Distillation as a defense to adversarial perturbations
against deep neural networks. In IEEE Symposium on
Security and Privacy (S&P), 2016.
Polino, A., Pascanu, R., and Alistarh, D. Model compres-
sion via distillation and quantization. In International
Conference on Learning Representations (ICLR), 2018.
Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta,
C., and Bengio, Y. Fitnets: Hints for thin deep nets. In
International Conference on Learning Representations
(ICLR), 2015.
Rusu, A. A., Colmenarejo, S. G., Gulcehre, C., Desjardins,
G., Kirkpatrick, J., Pascanu, R., Mnih, V., Kavukcuoglu,
K., and Hadsell, R. Policy distillation. In International
Conference on Learning Representations (ICLR), 2016.
Saxe, A. M., McClelland, J. L., and Ganguli, S. Exact
solutions to the nonlinear dynamics of learning in deep
linear neural networks. In International Conference on
Learning Representations (ICLR), 2014.
Sch¨olkopf, B. and Smola, A. J. Learning With Kernels. MIT
Press, 2002.
Tang, Z., Wang, D., and Zhang, Z. Recurrent neural net-
work training with dark knowledge transfer. In IEEE
International Conference on Acoustics, Speech and Sig-
nal Processing (ICASSP), 2016.
Urner, R., Shalev-Shwartz, S., and Ben-David, S. Access to
unlabeled data can speed up prediction time. In Interna-
tional Conference on Machine Learing (ICML), 2011.
Vapnik, V. and Izmailov, R.
Learning using privileged
information: similarity control and knowledge transfer.
Journal of Machine Learning Research (JMLR), 16(2):
2023–2049, 2015.

Towards Understanding Knowledge Distillation
Yu, R., Li, A., Morariu, V. I., and Davis, L. S. Visual
relationship detection with internal and external linguistic
knowledge distillation. In International Conference on
Computer Vision (ICCV), 2017.
Zhu, J. Machine teaching for Bayesian learners in the ex-
ponential family. In Conference on Neural Information
Processing Systems (NIPS), 2013.
Zhu, X. Machine teaching: An inverse problem to machine
learning and an approach toward optimal education. In
AAAI Conference on Artiﬁcial Intelligence, 2015.

Supplementary Material
We deﬁne here some notation in addition to that of Section 3
in the main text. We denote by ℓi the per-instance loss,
L1(w) = 1
n
n
X
i=1
ℓi(w⊺xi),
(35)
ℓi(u) = −yi log σ(u) −(1 −yi) log(1 −σ(u)) −ℓ∗
i ,
(36)
where ℓ∗
i are constants chosen such that the minimum of ℓi
is 0, namely ℓ∗
i = −yi log yi −(1 −yi) log(1 −yi).
Slightly abusing notation, we write L(τ) = L1(w(τ)) =
L(W1(τ), . . . , WN(τ)) for the objective value at time τ.
Finally, for a full-rank matrix A ∈Rd×m (m ≥1), we
denote by PA ∈Rd×d the matrix of projection onto the
span of A,
PA =

I,
m ≥d,
A(A⊺A)−1A⊺,
m < d.
(37)
A. Properties of the Cross-Entropy Loss
Theorem A.1 (Gradient). The gradient of the cross-entropy
loss (35) takes the form
∇L1(w) = 1
n
n
X
i=1
(σ(w⊺xi) −yi) · xi.
(38)
It always lies in the data span, ∇L1(w) ∈span(X).
Proof. Straightforward calculation.
Theorem A.2 (Global minima). The global minimum of the
cross-entropy loss (35) is 0 and the set of global minimisers
is

w ∈Rd : X⊺w = X⊺w∗
	
.
(39)
Proof. We know that L1 ≥0 and L1(w∗) = 0, so 0 is
the optimal objective value, and the set of global optima
consists of all w such that L1(w) = 0. The last condition
is equivalent to ∀i : ℓi(w) = 0, which in turn is equivalent
to ∀i : σ(w⊺xi) = σ(w⊺
∗xi). By monotonicity of σ, this
is further equivalent to ∀i : w⊺xi = w⊺
∗xi, which is a
restatement of (39).
Theorem A.3 (Restricted strong convexity). Assume X is
full-rank. For any sublevel set W =

w : L1(w) ≤l
	
,
there exists µ > 0 such that
L1(v) ≥L1(w) + ∇L1(w)⊺(v −w) + µ
2 ∥v −w∥2
(40)
for all w, v ∈W such that v −w ∈span(X).
Proof. Consider the 2nd-order Taylor expansion of L1
around w,
L1(v) = L1(w) + ∇L1(w)⊺(v −w)
+ 1
2(v −w)⊺[∇2L1( ¯w)](v −w),
(41)
where ∇2L1( ¯w) is the Hessian of L1 evaluated at ¯w, a
point lying between v and w. A straightforward calculation
shows that the Hessian takes the form
∇2L1( ¯w) = XD ¯wX⊺,
(42)
where
D ¯w = diag[σ( ¯w⊺x1)(1 −σ( ¯w⊺x1)),
. . . , σ( ¯w⊺xn)(1 −σ( ¯w⊺xn))].
(43)
We will now show that there is a constant ω > 0 such that
σ( ¯w⊺xi)(1 −σ( ¯w⊺xi)) ≥ω
(44)
for all ¯w ∈W and i ∈{1, . . . , n}, so that we can claim
D ¯w ⪰ωI, or consequently ∇2L1( ¯w) ⪰ωXX⊺.
Let w ∈W. The bound on L1(w) implies a bound on
ℓi(w⊺xi) for all i,
ℓi(w⊺xi) ≤nL1(w) ≤nl.
(45)
Because ℓi is convex and ℓi(u) →∞as u →±∞, we
know that ℓ−1
i ((−∞, nl]) is a bounded interval, and the
ﬁnite union ∪n
i=1ℓ−1
i ((−∞, nl]) is also a bounded interval,
whose size depends only on nl and the data. Hence, there
exists K > 0 such that w⊺xi ∈[−K, K] for all w ∈W
and i ∈{1, . . . , n}. The existence of ω > 0 satisfying (44)
follows.
Now, let us apply ∇2L1(w) ⪰ωXX⊺to lower-bound (41):
L1(v) ≥L1(w) + ∇L1(w)⊺(v −w)
+ ω
2 (v −w)⊺XX⊺(v −w).
(46)

Towards Understanding Knowledge Distillation
Consider two cases.
If n ≥d, XX⊺is full-rank and
XX⊺⪰λminI holds, where λmin > 0 is the smallest
eigenvalue of XX⊺. Combined with (46), this proves the
claim for n ≥d and µ = ωλmin.
If n < d, X⊺X is full rank. We can use the assumption
v −w ∈span(X) to deduce
∥v −w∥2 = ∥PX(v −w)∥2
= (v −w)⊺X(X⊺X)−1X⊺(v −w)
≤λmax(v −w)⊺XX⊺(v −w),
(47)
where λmax > 0 is the largest eigenvalue of (X⊺X)−1.
Combined with (46), this proves the claim for n < d and
µ = ω/λmax.
Corollary A.1 (Restricted Polyak-Lojasiewicz). Assume X
is full-rank. For any sublevel set W =

w : L1(w) ≤l
	
,
there exists c > 0 such that
cL1(w) ≤1
2
∇L1(w)
2
(48)
for all w ∈W.
Proof. Let w ∈W. (If W is empty, the claim is trivially
true.) Theorem A.3 applied to W implies that for some
µ > 0,
L1(v) ≥L1(w)+∇L1(w)⊺(v−w)+ µ
2 ∥v −w∥2 (49)
for all v ∈W ∩V where V = {v : v −w ∈span(X)}.
Taking minv∈W∩V on both sides, then relaxing part of the
constraint on the right-hand side yields
min
v∈W∩V L1(v)
≥
min
v∈W∩V L1(w) + ∇L1(w)⊺(v −w) + µ
2 ∥v −w∥2
≥min
v∈V L1(w) + ∇L1(w)⊺(v −w) + µ
2 ∥v −w∥2.
(50)
Now, the minimum on the left-hand side is equal to 0 and
is attained at v = w + PX(w∗−w), as can be seen from
Theorem A.2. For the right-hand side, we can substitute v =
w + Xa for a ∈Rn and ﬁnd the unconstrained minimum
with respect to a. We get
0 ≥L1(w) −1
2µ∇L1(w)⊺X(X⊺X)−1X⊺∇L1(w)
≥L1(w) −λmax
2µ
∇L1(w)
2,
(51)
where λmax > 0 is the largest eigenvalue of X(X⊺X)−1X⊺.
This yields the result with c = µ/λmax.
B. Proof of Theorem 1
We will prove a supporting lemma, and then the theorem.
Lemma B.1. Assume the student is a directly parameterised
linear classiﬁer (N = 1) initialised at zero, w(0) = 0.
Then, w(τ) ∈span(X) for τ ∈[0, ∞).
Proof. Let q ∈Rd be any vector orthogonal to the span of
X. It sufﬁces to show that q⊺w(τ) = 0. For that, notice
that q⊺w(0) = 0 and
d
dτ (q⊺w(τ)) = −q⊺∇L1(w(τ)) = 0,
(52)
where the last equality follows from the fact that
∇L1(w(τ)) ∈span(X) (Theorem A.1). The claim fol-
lows.
Theorem 1. Assume the student is a directly parameterised
linear classiﬁer (N = 1) with weight vector initialised at
zero, w(0) = 0. Then, the student’s weight vector fulﬁlls
almost surely
w(t) →ˆw,
(5)
for t →∞, with
ˆw =

w∗,
n ≥d,
X(X⊺X)−1X⊺w∗,
n < d.
(6)
Proof. Recall the time-derivative of L,
L′(τ) = −
∇L1(w(τ))
2.
(53)
The data matrix X is almost surely (wrt.
X ∼P n
x )
full-rank, we can therefore apply Corollary A.1 to W =

w : L1(w) ≤L1(0)
	
and w(τ) to lower-bound the gra-
dient norm on the right-hand side of (53).
We obtain
L′(τ) ≤−cL(τ) for some c > 0 and all τ ∈[0, ∞), or
equivalently,
(log L(τ))′ ≤−c.
(54)
Integrating over [0, t] yields L(t) ≤L(0) · e−ct, which
proves global convergence in the objective: L(t) →0 as
t →∞.
Now invoke Theorem A.3 with W as above, v = w(t) and
w = ˆw (we know that both w(τ), ˆw ∈W ∩span(X),
partly by Lemma B.1):
L(t) ≥µ
2 ∥w(t) −ˆw∥2.
(55)
Since L(t) →0 as t →∞, the theorem follows.

Towards Understanding Knowledge Distillation
C. Proof of Theorem 2
Theorem 2. Let ˆw be deﬁned as in Theorem 1. Assume the
student is a deep linear network, initialized such that for
some ϵ > 0,
∥w(0)∥< min
n
∥ˆw∥, ϵN
ϵ2∥ˆw∥−2
N + ∥ˆw∥2−2
N
−N
2 o
,
(11)
L1(w(0)) < L1(0),
(12)
Wj+1(0)⊺Wj+1(0) = Wj(0)Wj(0)⊺
(13)
for j = 1, . . . , N −1. Then, for n ≥d, student’s weight
vector fulﬁlls almost surely
w(t) →ˆw,
(14)
and for n < d,
∥w(t) −ˆw∥≤ϵ,
(15)
for all t large enough.
For the proof, we will need a result by (Arora et al., 2018),
which characterises the induced ﬂow on w(τ) when running
gradient descent on the component matrices Wi.
Lemma C.1 ((Arora et al., 2018, Claim 2)). If the balanced-
ness condition (13) holds, then
∂w(τ)
∂τ
= −∥w(τ)∥
2(N−1)
N
 ∇L1(w(τ))+
(N −1) · Pw(τ)∇L1(w(τ))

.
(56)
Proof of Theorem 2. Similarly to the case N = 1, we start
by looking at the time-derivative of L,
L′(τ) =∇L1(w(τ))⊺
∂w(τ)
∂τ

= −∥w(τ)∥
2(N−1)
N
∇L1(w(τ))
2
+(N −1) ·
Pw(τ)∇L1(w(τ))
2
≤−∥w(τ)∥
2(N−1)
N
·
∇L1(w(τ))
2.
(57)
It is non-positive, so w(τ) stays within the L(0)-sublevel
set throughout optimisation,
w(τ) ∈W =

w : L1(w) ≤L(0)
	
.
(58)
Also, W is convex and by Assumption (12) it does not
contain 0. We can therefore take δ > 0 to be the distance
between W and 0, and it follows that ∥w(τ)∥≥δ for
τ ∈[0, ∞).
Now, noting that X is almost surely full-rank, apply Corol-
lary A.1 to W and w(τ) to upper-bound the right-hand side
of (57),
L′(τ) ≤−cδ
2(N−1)
N
L(τ).
(59)
Letting ˜c = cδ
2(N−1)
N
, we get (log L(τ))′ ≤−˜c and conse-
quently L(t) ≤L(0) · e−˜ct. This proves convergence in the
objective, L(t) →0 as t →∞.
To prove convergence in parameters, we decompose the
‘error’ w(τ) −ˆw into orthogonal components and bound
each of them separately,
∥w(τ) −ˆw∥2 = ∥PX(w(τ) −ˆw)∥2
+ ∥PQ(w(τ) −ˆw)∥2,
(60)
where the columns of Q ∈Rd×(d−n) orthogonally comple-
ment those of X. If n ≥d, we simply bound the ﬁrst term
and disregard the second one.
To bound the ﬁrst term, invoke Theorem A.3 with W, v =
PXw(τ) and w = PX ˆw. One can check that L1(PXu) =
L1(u) for all u ∈Rd, so PXw(τ) ∈W and our use of the
theorem is legal. We obtain
L(τ) ≥µ
2 ∥PX(w(τ) −ˆw)∥2.
(61)
Since L(τ) →0, it follows that
∥PX(w(τ) −ˆw)∥2 →0
(62)
as τ →∞.
For the second term, notice that ˆw ∈span(X), so PQ ˆw
vanishes and we are left with ∥PQw(τ)∥2. Denote this
quantity q(τ). Its time derivative is
q′(τ) = 2(PQw(τ))⊺
∂w(τ)
∂τ

= −2∥w(τ)∥
2(N−1)
N

w(τ)⊺PQ∇L1(w(τ))+
(N −1)
∥w(τ)∥2 · w(τ)⊺PQw(τ) · w(τ)⊺∇L1(w(τ))

= −2q(τ)(N −1)∥w(τ)∥−2/Nw(τ)⊺∇L1(w(τ)),
(63)
where we have used the fact that ∇L1(w(τ)) ∈span(X)
(Theorem A.1) and Q is orthogonal to X. Rearranging, we
obtain
d
dτ
 log q(τ)
2(N −1)

= −∥w(τ)∥−2/N · w(τ)⊺∇L1(w(τ)).
(64)
It turns out that the right-hand side expression is integrable
in yet another way, namely
d
dτ
 1
2N log ∥w(τ)∥2

=
−∥w(τ)∥−2/N · w(τ)⊺∇L1(w(τ)).
(65)

Towards Understanding Knowledge Distillation
Equating the two and integrating over [0, t] yields
log q(t)
q(0) = N −1
N
· log ∥w(t)∥2
∥w(0)∥2 ,
(66)
which implies
q(t)
∥w(t)∥2 ≤
∥w(0)∥
∥w(t)∥
2/N
,
(67)
because q(0) ≤∥w(0)∥2.
We now bound the norm of w(t). Starting from an orthog-
onal decomposition similar to (60) and applying (62) with
(67), we get
∥w(t)∥2 =∥PXw(t)∥2 + ∥PQw(t)∥2
lim sup
t→∞∥w(t)∥2 ≤∥ˆw∥2 + ∥w(0)∥
2
N lim sup
t→∞∥w(t)∥2−2
N .
(68)
Denote ν
:=
lim supt→∞∥w(t)∥.
By the same or-
thogonal decomposition, we also know that ν2
≥
lim supt→∞∥PXw(t)∥2 = ∥ˆw∥2 > 0, so we can divide
both sides above by ν2,
1 ≤∥ˆw∥2
ν2
+ ∥w(0)∥2/N
ν2/N
=: f(ν).
(69)
On the right-hand side, we now have a decreasing function
of ν that goes to zero as ν →∞. However, evaluated at our
speciﬁc ν, it is lower-bounded by 1, implying an implicit
upper bound for ν.
How do we ﬁnd this bound? Suppose we ﬁnd some constant
K such that f(K) ≤1. Then, because f is decreasing, it
must be the case that ν ≤K. One such candidate for K is
K = ∥ˆw∥·
 
1 −∥w(0)∥2/N
∥ˆw∥2/N
!
−N
2(N−1)
.
(70)
(Here we have used condition (11): ∥w(0)∥< ∥ˆw∥.) To
check that indeed f(K) ≤1, start from the inequality
(∥ˆw∥/K)
2(N−1)
N
+ ∥w(0)∥2/N
∥ˆw∥2/N
= 1
≤
 
1 −∥w(0)∥2/N
∥ˆw∥2/N
!
−1
N−1
= (∥ˆw∥/K)−2
N .
(71)
Taking the leftmost and rightmost expression and multiply-
ing by (∥ˆw∥/K)2/N yields
f(K) = ∥ˆw∥2
K2
+ ∥w(0)∥2/N
K2/N
≤1.
(72)
Hence,
lim sup
t→∞∥w(t)∥≤∥ˆw∥·
 
1 −∥w(0)∥2/N
∥ˆw∥2/N
!
−N
2(N−1)
.
(73)
Finally, let us turn back to our original goal of bounding
∥w(τ) −ˆw∥2. With (60), (62), (67) and (73), we now know
that
lim sup
t→∞∥w(τ) −ˆw∥2
(74)
≤∥w(0)∥
2
N ∥ˆw∥
2(N−1)
N
 
1 −∥w(0)∥
2
N
∥ˆw∥
2
N
!−1
(75)
=
∥ˆw∥2+2/N
∥ˆw∥2/N −∥w(0)∥2/N −∥ˆw∥2.
(76)
Hence, if we initialise close enough to zero, as speciﬁed by
condition (11), we can ensure that
lim sup
t→∞∥w(τ) −ˆw∥2 < ϵ2.
(77)
This concludes the proof.
D. Theorem 3 for Approximate Distillation
We extend Theorem 3 to the setting where the student learns
the solution ˆw = X(X⊺X)−1X⊺w∗only ϵ-approximately,
as is the case for deep linear networks initialised as in Theo-
rem 2. When n ≥d, the teacher’s weight vector is recovered
exactly and the transfer risk is zero, even when the student
is deep. The following theorem therefore only covers the
case n < d.
Theorem D.1 (Risk bound for approximate distillation).
Let n < d. For any training set X ∈Rd×n, let ˆhX(x) =
1{ ˆw⊺
ϵ x ≥0} be a linear classiﬁer whose weight vector is
ϵ-close to the distillation solution ˆw, i.e. ∥ˆwϵ −ˆw∥≤ϵ,
where ϵ is a positive constant such that ϵ ≤1
2∥ˆw∥. Deﬁne
δ :=
q
2πϵ
∥ˆw∥. Then, it holds for any β ∈[0, π/2 −δ] that
E
X∼P ⊗n
x
h
R
 ˆhX
Px, w∗
i
≤p(β) + p(π/2 −δ −β)n.
(78)
The result is very similar to Theorem 3 in the main text, the
only difference is the constant δ which compensates for the
imprecision in learning ˆw by pushing the bound up (recall
that p is decreasing). However, as ϵ goes to zero, so does δ
and we recover the original bound.
For the proof, we start with a tool for controlling the angle
between ˆw and ˆwϵ. Recall that the angle is deﬁned as
α(w, v) = cos−1

w⊺v
∥w∥· ∥v∥

(79)

Towards Understanding Knowledge Distillation
for w, v ∈Rd \ {0}.
Lemma D.1. Let w, v ∈Rd be such that ∥w −v∥≤ϵ,
where ϵ ≤1
2∥w∥. Then α(w, v) ≤
q
2πϵ
∥w∥.
Proof of Lemma D.1. The ﬁrst step is to lower-bound the
inner product w⊺v. To that end, we expand and rearrange
∥w −v∥2 ≤ϵ2 to obtain
2w⊺v ≥∥w∥2 + ∥v∥2 −ϵ2.
(80)
Now use the triangle relation ∥v∥≥∥w∥−ϵ squared to
lower-bound the right-hand side of (80) and get
2w⊺v ≥2∥w∥2 −2ϵ∥w∥,
(81)
which implies
w⊺v
∥w∥· ∥v∥≥∥w∥−ϵ
∥v∥
≥∥w∥−ϵ
∥w∥+ ϵ ≥1 −
2ϵ
∥w∥.
(82)
Thus,
1 −
2ϵ
∥w∥≤
w⊺v
∥w∥· ∥v∥= cos(α(w, v)).
(83)
The left-hand side is by assumption non-negative, so we
have α(w, v) ∈[−π/2, π/2]. On this domain,
cos x ≤1 −x2
π ,
(84)
which lets us deduce
1 −
2ϵ
∥w∥≤1 −α(w, v)2
π
.
(85)
Rearranging yields the result.
Proof of Theorem D.1. We decompose the expected risk as
follows:
E
X∼P n
x
h
R
 ˆhX
Px, w∗
i
=
P
X∼P n
x
x∼Px
[w⊺
∗x · ˆw⊺
ϵ x < 0] =
=
Z
x:¯α(w∗,x)≥β
P
X∼P n
x
[w⊺
∗x · ˆw⊺
ϵ x < 0|x] dPx
+
Z
x:¯α(w∗,x)<β, w⊺
∗x>0
P
X∼P n
x
[ ˆw⊺
ϵ x < 0|x] dPx
+
Z
x:¯α(w∗,x)<β, w⊺
∗x<0
P
X∼P n
x
[ ˆw⊺
ϵ x > 0|x] dPx.
(86)
Let us ﬁx some x for which ¯α(w∗, x) < β and w⊺
∗x > 0;
for this x we have α(w∗, x) = ¯α(w∗, x). Consider the
situation where ¯α(w∗, xi) < π/2 −β −δ for some i. Then
by the triangle inequality, Lemma D.1 and Lemma 1,
α( ˆwϵ, x) ≤α( ˆwϵ, ˆw) + α(w∗, ˆw) + α(w∗, x)
(87)
≤δ + ¯α(w∗, xi) + ¯α(w∗, x)
(88)
< π/2,
(89)
which implies ˆw⊺
ϵ x > 0, i.e. a correct prediction (same
as the teacher’s). Conversely, an error can occur only if
¯α(w∗, xi) ≥π/2 −δ −β for all i. Because xi are indepen-
dent, we have
P
X∼P n
x
[ ˆw⊺
ϵ x < 0| x : ¯α(w∗, x) < β, w⊺
∗x > 0]
≤
P
X∼P n
x
[∀i : ¯α(w∗, xi) ≥π/2 −δ −β]
= p(π/2 −δ −β)n.
(90)
By a symmetric argument, one can show that
P
X∼P n
x
[ ˆw⊺
ϵ x > 0| x : ¯α(w∗, x) < β, w⊺
∗x < 0]
≤p(π/2 −δ −β)n.
(91)
Combining (86), (90) and (91) yields the result.

