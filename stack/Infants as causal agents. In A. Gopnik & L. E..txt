
Causal Learning

This page intentionally left blank 

Causal Learning
Psychology, Philosophy, and Computation
Edited by
Alison Gopnik
Laura Schulz
1
2007

1
Oxford University Press, Inc., publishes works that further
Oxford University’s objective of excellence
in research, scholarship, and education.
Oxford
New York
Auckland
Cape Town
Dar es Salaam
Hong Kong
Karachi
Kuala Lumpur
Madrid
Melbourne
Mexico City
Nairobi
New Delhi
Shanghai
Taipei
Toronto
With offices in
Argentina
Austria
Brazil
Chile
Czech Republic
France
Greece
Guatemala
Hungary
Italy
Japan
Poland
Portugal
Singapore
South Korea
Switzerland
Thailand
Turkey
Ukraine
Vietnam
Copyright © 2007 by Alison Gopnik and Laura Schulz
Published by Oxford University Press, Inc.
198 Madison Avenue, New York, New York 10016
www.oup.com
Oxford is a registered trademark of Oxford University Press
All rights reserved.  No part of this publication may be reproduced,
stored in a retrieval system, or transmitted, in any form or by any means,
electronic, mechanical, photocopying, recording, or otherwise,
without the prior permission of Oxford University Press.
Library of Congress Cataloging-in-Publication Data
Causal learning : psychology, philosophy, and computation / edited by
Alison Gopnik and Laura Schulz.
p. cm.
Includes bibliographical references and index.
ISBN 978-0-19-517680-3
1. Learning, Psychology of.
2. Causation.
I. Gopnik, Alison.
II. Schulz, Laura.
BF318.C38 2007
153.1′5—dc22
2006018902
9
8
7
6
5
4
3
2
1
Printed in the United States of America
on acid-free paper

This volume originated in a causal learning “group”
(Gopnik, Richardson, and Campbell) and a series of
workshops between September 2003 and June 2004 at
the Center for Advanced Studies in the Behavioral
Sciences at Stanford University, Stanford, California.
It is well known that the center is, almost unique
among human experiences, even better than you
think it is going to be, and we are extremely grateful to
everyone at that magnificent institution, particularly
Douglas Adams and Mark Turner, the then-directors,
and the staff who made organizing the workshops such
a pleasure. We are also grateful to the Hewlett
Foundation, which supported A. G.’s fellowship at the
center. A. G. was also supported in the preparation of
this volume by a grant from the National Science
Foundation (DLS0132480), and L. S. was supported
by a National Science Foundation graduate fellowship
and an American Association of University Women
Fellowship.
The principal founder of this feast, however, is the
McDonnell Foundation. In addition to funding 
the workshops themselves, the workshops led to the
McDonnell Causal Learning Collaborative, linking
developmental and philosophical and computational
research and involving many of the authors in this vol-
ume. We are grateful to the foundation, particularly
its president, John Bruer, who saw the potential of this
unusual interdisciplinary enterprise.
Finally, we thank Oxford University Press, espe-
cially our editor there, Catharine Carlin, for all her
support on this project.
Preface

This page intentionally left blank 

Contributors
ix
Introduction
1
Alison Gopnik and Laura Schulz
PART I: CAUSATION AND INTERVENTION
1 Interventionist Theories of Causation in Psychological Perspective
19
Jim Woodward
2 Infants’ Causal Learning: Intervention, Observation, Imitation
37
Andrew N. Meltzoff
3 Detecting Causal Structure: The Role of Interventions in 
Infants’ Understanding of Psychological and Physical Causal Relations
48
Jessica A. Sommerville
4 An Interventionist Approach to Causation in Psychology
58
John Campbell
5 Learning From Doing: Intervention and Causal Inference
67
Laura Schulz, Tamar Kushnir, and Alison Gopnik
6 Causal Reasoning Through Intervention
86
York Hagmayer, Steven Sloman, David Lagnado, and Michael R. Waldmann
7 On the Importance of Causal Taxonomy
101
Christopher Hitchcock
PART II: CAUSATION AND PROBABILITY
Introduction to Part II: Causation and Probability
117
Alison Gopnik and Laura Schulz
Contents

viii
CONTENTS
8 Teaching the Normative Theory of Causal Reasoning
119
Richard Scheines, Matt Easterday, and David Danks
9 Interactions Between Causal and Statistical Learning
139
David M. Sobel and Natasha Z. Kirkham
10
Beyond Covariation: Cues to Causal Structure
154
David A. Lagnado, Michael R. Waldmann, York Hagmayer, and 
Steven A. Sloman
11
Theory Unification and Graphical Models in Human Categorization
173
David Danks
12
Essentialism as a Generative Theory of Classification
190
Bob Rehder
13
Data-Mining Probabilists or Experimental Determinists? A Dialogue on the Principles
Underlying Causal Learning in Children
208
Thomas Richardson, Laura Schulz, and Alison Gopnik
14
Learning the Structure of Deterministic Systems
231
Clark Glymour
PART III: CAUSATION, THEORIES, AND MECHANISMS
Introduction to Part III: Causation, Theories, and Mechanisms
243
Alison Gopnik and Laura Schulz
15
Why Represent Causal Relations?
245
Michael Strevens
16
Causal Reasoning as Informed by the Early Development of Explanations
261
Henry M. Wellman and David Liu
17
Dynamic Interpretations of Covariation Data
280
Woo-kyoung Ahn, Jessecae K. Marsh, and Christian C. Luhmann
18
Statistical Jokes and Social Effects: Intervention and Invariance in 
Causal Relations
294
Clark Glymour
19
Intuitive Theories as Grammars for Causal Inference
301
Joshua B. Tenenbaum, Thomas L. Griffiths, and Sourabh Niyogi
20
Two Proposals for Causal Grammars
323
Thomas L. Griffiths and Joshua B. Tenenbaum
Notes
347
Index
353

ix
Contributors
Woo-kyoung Ahn
Department of Psychology
Yale University
New Haven, CT 06520
John Campbell 
Department of Philosophy
University of California at 
Berkeley
Berkeley, CA 94720-2390
David Danks
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213
Clark Glymour 
Department of Philosophy
Carnegie Mellon University
Pittsburgh, PA 15213
Alison Gopnik
Department of Psychology
University of California at Berkeley
Berkeley, California 94720
Tom Griffiths 
Department of Psychology
University of California at Berkeley
Berkeley, California 94720
York Hagmeyer
Department of Psychology
University of Göttingen
37077 Göttingen
Germany
Christopher Hitchcock
Division of the Humanities and Social Sciences
California Institute of Technology
Pasadena, CA 91125
David A. Lagnado
Department of Psychology
University College London
Gower Street
London WC1E 6BT, UK
Andrew N. Meltzoff
Institute for Learning and Brain Sciences
University of Washington 
Seattle, WA 98195
Bob Rehder 
Department of Psychology
New York University
New York, NY 10003
Thomas Richardson 
Department of Statistics
University of Washington
Seattle, WA 98195

Richard Scheines
Department of Philosophy, CALD, 
and HCII
Carnegie Mellon University
Pittsburgh, PA 15213
Laura Schulz
Department of Brain and Cognitive Sciences
Massachussetts Institute of Technology
Cambridge, MA 02139
David Sobel
Causality and Mind Lab
Brown University
Providence, RI 02912
Jessica Sommerville
Department of Psychology and Institute for
Learning & Brain Sciences
University of Washington
Seattle, WA 98195
Michael Strevens 
Department of Philosophy 
New York University
New York, NY 10003
Joshua Tenenbaum
Department of Brain and Cognitive Sciences
Massachussetts Institute of Technology
Cambridge, MA 02139
Henry Wellman 
Department of Psychology
Center for Human Growth and Development
University of Michigan
Ann Arbor, MI 48103
Jim Woodward
Division of the Humanities and Social 
Sciences
California Institute of Technology
Pasadena, CA 91125
x
CONTRIBUTORS

Causal Learning

This page intentionally left blank 

From: mherskovits@psych.ucarcadia.arcadia.edu
To: brook_russell@turing.carnegietech.edu
Hi Brook,
We haven’t met, but I’m writing about this
series of workshops on causal learning that my
advisor and yours have cooked up for this year
at the center in Stanford. My advisor has gone
completely crazy over this causal Bayes nets
stuff and is insisting that I go to this conference
(on the pittance that supports graduate
researchers) and that I learn everything there is
to know about the philosophy and computation
of causal learning. But, every time I look at one
of the papers, all I see are unintelligible 
sentences like this: For any variable R in the
directed graph, the graph represents the 
proposition that for any set S of variables in the
graph (not containing any descendants of R) R
is jointly independent of the variables in S
conditional on any set of values of the variables
that are parents of R!
Let me give you a brief sense of where I’m
coming from, as we say in mellow Arcadia
(though I’m a New Yorker myself). I went to
Public School 164 and did my undergraduate
degree in cognitive science at the City
University of Brooklyn, and I’ve always thought
that the problem of how we learn about the
world was the most central and interesting
question cognitive science could ask. That’s why
I became a developmental psychologist. But,
I’m suspicious about whether philosophy and
computation have much to offer. The history of
cognitive development, and the study of
learning more generally, has been a history of
theoretical answers that didn’t really fit the
phenomena and empirical phenomena that
didn’t really fit the theories. What we empirical
psychologists see is that learners infer abstract,
structured hierarchical representations of the
world. And those representations are true—they
really do get us to a better picture of the world.
But, the data that actually reach us from the
world are incomplete, fragmented, probabilistic,
and concrete. So, the baffling thing for
psychologists has been how we could get from
that kind of data to those kinds of
representations.
The philosophers and computationalists
keep telling us that the kind of learning we
1
Introduction
Alison Gopnik & Laura Schulz

developmentalists see every day is nothing but
an illusion! The Platonic (read Cartesian, read
Chomskyan, read Spelkean) view has been that,
although we seem to infer structure from data,
actually the structure was there all along. Insofar
as our representations are accurate, it is because
of a long phylogenetic evolutionary history, not
a brief ontogenetic inferential one. And, there is
no real learning involved in development but
only triggering or enrichment.
The Aristotelian (Lockean, behaviorist,
connectionist) view has been that, although it
looks as if we are building abstract veridical
representations, really all we are doing is
summarizing and associating bits of data.
Accuracy is beside the point; associationistic
processes just let us muddle through with the
right responses to the right stimuli. There aren’t
really any abstract representations, just distributed
collections of particular input-output links.
So, all that the philosophers and
computationalists seem to be doing, on either
side, is to tell us empirical developmental
psychologists not to believe our eyes. Actually,
I think Gopnik puts it quite well in her book
about theory formation (Gopnik & Meltzoff,
1997) (she does tend to let her conclusions
outstrip her data, but she sure has an ear for a
slogan):
Far too often in the past psychologists have been
willing to abandon their own autonomous theoriz-
ing because of some infatuation with the current
account of computation and neurology. We wake up
one morning and discover that the account that
looked so promising and scientific, S-R connections,
Gestaltian field theory, Hebbian cell-assemblies, has
vanished and we have spent another couple of
decades trying to accommodate our psychological
theories to it. We think we should summon up our
self-esteem and be more stand-offish in the future.
Any implementations of psychological theories,
either computational or neurological, will first
depend on properly psychological accounts of 
psychological phenomena (Gopnik & Meltzoff
1997, p. 220).
But anyway, although I’ve argued and
argued, my advisor is still insisting that I go to
this thing. And, it sounds like you’re in the
same boat. So, I’m writing to you with a deal:
How about a tutorial swap? I mean, I’ll tell you
all about causal learning in psychology if you’ll
explain those directed acyclic graphs in plain
English words? So, how about it?
All best, Morgan Herskovits
From: brook_russell@turing.carnegietech.edu
To: mherskovits@psych.ucarcadia.arcadia.edu
My dear Morgan,
Thank you for your letter of the 21st. I can’t say
that we seem to have much else in common, but
apparently your advisor matches mine in dotty
obstinacy. Mine is insisting that I read all this
barbaric and incomprehensible stuff about
subjects and methods. Worse, it appears that quite
a few of the subjects appear to be between 30.1
and 40.8 months old—sprogs in short! But, what
on earth methods for sprogs are supposed to have
to do with discovering normatively reliable
methods for causal inference I can’t imagine. He
is also insisting that I attend these workshops.
I can’t say I caught all your references. Plato
certainly, but Spelke? Gopnik? (And what
ghastly names.) However, I completely agree
with you about the lack of connection between
our two enterprises. The philosopher of science
Clark Glymour (Glymour 1992) put it very well,
I think, in his critique of cognitive theories of
science, appropriately called “Invasion of the
Mind Snatchers”: The idea that theories are
something you would find in somebody’s head,
rather than being abstract mathematical
objects, is an idea fit only for Ichabod Crane.
My own work began in my undergraduate
days at Oxford, as an attempt at a conceptual
analysis of causation. (I also am a public school
product by the way, though I find the idea of
numbered public schools rather puzzling. Would
Eton or Harrow get a lower number on your
American scheme?) The conceptual in
philosophy, of course, is not like the conceptual
in psychology. In philosophy, we want to know
what causation is in all conceivable 
circumstances, not what a few mere mortals (let
alone sprogs!) think that it is. There is a long
history in philosophy of trying to develop an
analytic definition of causation through the
2
CAUSAL LEARNING

method of examples and counterexamples;
philosophers give examples of cases in which
everyone agrees that X causes Y and then try to
find some generalization that will capture those
examples. Then, other philosophers find
examples that fit the definitions but don’t seem to
be causal or vice versa.
I was working on counterexamples of
quadruple countervailing causal prevention
(you know the sort of thing where one assassin
tries to stop another assassin, but first poison is
slipped in the antidote, and then a brick hits a
wooden board before the king can brake for the
stop sign). I was beginning to find it all rather
discouraging when finally my math tutor put
me on to the theory of causal graphical models,
and it came to me as a revelation.
You see, causal graphical models are to
causation as geometry is to space. Rather than
providing a reductive definition of causation
they instead provide a formal mathematical
framework that captures important regularities
in causal facts, just as the mathematical
structure of geometry captures important spatial
regularities. Causal graphical models capture
just the right kind of asymmetries in causal
relations, allow one to generate the appropriate
predictions about conditional probabilities and
interventions, and perhaps most significantly
discriminate between conditional probabilities
and interventions and counterfactuals. So, I
decided to move to Carnegie Tech for graduate
school and work on some of the many unsolved
problems the formalism poses.
Imagine my shock, then, when my advisor,
a philosopher of science notorious for the
austerity and rigor of his views on just about
everything, began insisting that I read
psychology and, worse, child psychology!
Because, of course, it is obvious that even
sophisticated adults are unable to handle even
the simplest problems involving causality or
probability. The undergraduate students at
Carnegie Tech, for example, who, although
admittedly handicapped by an American
secondary school education, are among the
brightest and best but are quite hopeless at
these computations. Anyone who has, for their
sins, had to teach introductory statistics is aware
of that. So, how could mere sprogs of 3 or 4
years be expected to use anything like Bayes net
learning algorithms? They are, I understand,
inept at even quite elementary differential
integration problems and have, at best, only the
most primitive understanding of basic linear
algebra.
However, one of the benefits of an Oxford
education is the training it provides in
possessing a deep and thorough knowledge of
the most recondite subjects based on a brief
weekly perusal of the Times Literary
Supplement. So, I will, in fact, be grateful for a
(preferably equally brief) summary of this work.
In return, I will do my best to give you an
extremely simple introduction to causal Bayes
nets (see attached).
Yours very truly,
Brook Russell
Attachment 1: Causal Bayes Nets 
for Dummies
Causal Bayes Nets
Causal-directed graphical models, or causal Bayes
nets, were developed in the philosophy of science
and statistical literature (Glymour, 2001; Pearl,
1988, 2000; Spirtes, Glymour, & Scheines, 1993).
Scientists seem to infer theories about the causal
structure of the world from patterns of evidence.
But, philosophers of science found it difficult to
explain how these inferences are possible. Although
classical logic could provide a formal account of
deductive inferences, it was much more difficult to
provide an inductive logic—an account of how evi-
dence could confirm theories. One reason is that
deductive logic deals in certainties, but inductive
inference is always a matter of probabilities—acquir-
ing more evidence for a hypothesis makes the
hypothesis more likely, but there is always the possi-
bility that it will be overturned.
An even more difficult question was what philoso-
phers of science called “the logic of discovery.” Again,
the conventional wisdom, going back to Karl Popper,
was that particular hypotheses could be proposed and
could be falsified (definitely) or confirmed (tentatively).
The origins of those hypotheses were mysterious; there
INTRODUCTION
3

was no way of explaining how the evidence itself
could generate a hypothesis.
Causal Bayes nets provide a kind of logic of induc-
tive inference and discovery. They do so, at least, for
one type of inference that is particularly important in
scientific theory formation. Many scientific hypothe-
ses involve the causal structure of the world. Scientists
infer causal structure by observing the patterns of con-
ditional probability among events (as in statistical
analysis), by examining the consequences of interven-
tions (as in experiments), or usually, by combining
the two types of evidence. Causal Bayes nets formal-
ize these kinds of inferences.
In causal Bayes nets, causal hypotheses are rep-
resented by directed acyclic graphs like that of
Figure I-1. The graphs consist of variables, represent-
ing types of events or states of the world and directed
edges (arrows) representing the direct causal relations
between those variables. The variables can be discrete
(like school grade) or continuous (like weight); they
can be binary (like “having eyes” or “not having eyes”)
or take a range of values (like color). Similarly, the
direct causal relations can have many forms; they can
be deterministic or probabilistic, generative or
inhibitory, linear or nonlinear. The exact specifica-
tion of the nature of these relations is called the para-
meterization of the graph. In most applications of the
formalism, we assume that the graphs are acyclic—an
arrow cannot feed back on itself. However, there are
some generalizations of the formalism to cyclic cases.
Causal Structure and Conditional 
Probabilities
The Bayes net formalism makes systematic connections
between the causal hypotheses that are represented by
the graphs and particular patterns of evidence. The
structure of a causal graph constrains the conditional
probabilities among the variables in that graph, no mat-
ter what the variables are or what the parameterization
of the graph is. In particular, it constrains the conditional
independencies among those variables. Given a par-
ticular causal structure, only some patterns of condi-
tional independence will occur among the variables.
Conditional and unconditional dependence and
independence can be defined mathematically. Two
discrete variables X and Y are unconditionally inde-
pendent in probability if and only if for every value x of
X and y of Y the probability of x and y occurring
together equals the unconditional probability of x mul-
tiplied by the unconditional probability of y. That is 
p(x & y)p(x) * p(y). Two variables are independent in
probability conditional on some third variable Z if and
only if p(x, y | z)p(x | z) * p(y | z). That is, for every
value x, y, and z of X, Y, and Z the probability of x and
y given z equals the probability of x given z multiplied
by the probability of y given z. This definition can be
extended to continuous variables. When we say three
variables x, y, and z are correlated, we mean that they
are dependent in probability. When we say that x and y
are correlated but that that correlation disappears when
z is partialed out, we mean that x and y are independent
in probability conditional on z.
The structure of the causal graph puts constraints
on these patterns of probability among the variables.
These constraints can be captured by a single formal
assumption, the causal Markov assumption.
The Causal Markov Assumption
For any variable X
in an acyclic causal graph, X is independent of all other
variables in the graph (except for its own direct and
indirect effects) conditional on its own direct causes.
If we make further assumptions about the parame-
terization of the graph, that is, about the particular
nature of the causal relations among the variables,
then we can constrain the kinds of inferences we
make still further. For example, if we assume that
each cause independently has a certain power to
bring about an effect and that this power leads to a
certain likelihood of the effect given the cause, then
we can further constrain the patterns of conditional
probability among causes and effects. This is a
common assumption in studies of human causal
learning. The causal Markov assumption, however,
applies to all parameterizations.
To illustrate, consider a simple causal problem that
is far too common for academics who attend many
4
CAUSAL LEARNING
FIGURE I-1 A causal Bayes net.

learned conferences. Suppose that I notice that I often
cannot sleep when I have been to a party and drunk
lots of wine. Partying P and insomnia I covary and so do
wine W and insomnia I. There are at least two possibil-
ities about the relations among these variables, which I
can represent by two simple causal graphs: Graph 1 is
a chain P →W →I; Graph 2 is a common cause
structure I ←P →W. Maybe parties lead me to drink,
and wine keeps me up; maybe parties both keep me up
and lead me to drink. The covariation among the vari-
ables by itself is consistent with both these structures.
You can discriminate between these two graphs by
looking at the patterns of conditional probability
among the three variables. Suppose you keep track of
all the times you drink and party and examine the
effects on your insomnia. If Graph 1 is correct, then
you should observe that you are more likely to have
insomnia when you drink wine, whether or not you
party. If instead Graph 2 is correct, then you will
observe that, regardless of how much or how little
wine you drink, you are only more likely to have
insomnia when you go to a party.
More formally, if Graph 1 is right, and there is a
causal chain that goes from parties to wine to insomnia,
then I ⊥P | W; the probability of insomnia occurring is
independent (in probability) of the probability of party
going occurring conditional on the occurrence of wine
drinking. If Graph 2 is right and parties are a common
cause of wine and insomnia, then I ⊥W | P; the prob-
ability of wine-drinking occurring is independent (in
probability) of the probability of insomnia occurring
conditional on the occurrence of party going.
The philosopher of science Hans Reichenbach
(1971) long ago pointed out these consistent relations
between conditional independence and causal struc-
ture and talked about them in terms of “screening
off.” When there is a chain going from partying to
wine to insomnia, the wine screens off insomnia from
the influence of partying; when partying directly
causes both wine and insomnia, wine does not screen
off insomnia from partying—partying leads to insom-
nia directly. But, partying does screen off insomnia
from the effects of wine. The causal Markov assump-
tion generalizes this screening-off principle to all
acyclic causal graphs.
Thus, if we know the structure of the graph
and know the values of some of the variables in the
graph, we can make consistent predictions about
the conditional probability of other variables. In fact,
the first applications of Bayes nets involved predicting
conditional probabilities (Pearl, 1988). Many real-life
inferences involve complex combinations of condi-
tional probabilities among variables—consider a med-
ical expert, for example, trying to predict one set of
symptoms from another set. Trying to predict all the
combinations of conditional probabilities rapidly
becomes an exponentially complicated problem.
Computer scientists were trying to find a tractable way
to calculate these conditional probabilities and discov-
ered that representing the variables in a directed graph
allowed them to do this. The graph allowed computer
scientists to “read off” quite complicated patterns of
conditional dependence among variables. The first
applications of Bayes nets treated the graphs as calcula-
tion devices—summaries of the conditional probabili-
ties among events.
Bayes Nets and Interventions
Why think of these graphs as representations of causal
relations among variables, rather than simply thinking
of them as a convenient way to represent the probabil-
ities of variables? The earlier Bayes net iterations were
confined to techniques for predicting some probabili-
ties from others. However, the development of causal
Bayes net algorithms also allows us to determine what
will happen when we intervene from outside to
change the value of a particular variable. When two
variables are genuinely related in a causal way, holding
other variables constant, then intervening to change
one variable should change the other. Indeed, philoso-
phers have argued that this is just what it means for two
variables to be causally related (J. Woodward, 2003).
Predictions about probabilities may be quite differ-
ent from predictions about interventions. For example,
in a common cause structure like Graph 2, we will
indeed be able to predict something about the value of
insomnia from the value of wine. If that structure is the
correct one, then knowing that someone drank wine
will indeed make you more likely to predict that they
will have insomnia (because drinking wine is corre-
lated with partying, which leads to insomnia). But,
intervening on their wine drinking, forbidding them
from drinking, for example, will have no effect on their
insomnia. Only intervening on partying will do that.
The Bayes net formalism captures these relations
between causation, intervention, and conditional
probability through a second assumption, an assump-
tion about how interventions should be represented
in the graph.
INTRODUCTION
5

The Intervention Assumption
A variable I is an inter-
vention on a variable X in a causal graph if and only
if (a) I is exogenous (that is, it is not caused by any
other variables in the graph), (b) directly fixes the
value of X to x, and (c) does not affect the values of
any other variables in the graph except through its
influence on X.
Given this assumption, we can accurately predict
the effects of interventions on particular variables in a
graph on other variables. (We can also sometimes
make accurate predictions about the effects of inter-
ventions that do not meet all these conditions). In
causal Bayes nets, interventions systematically alter
the nature of the graph they intervene on, and these
systematic alterations follow directly from the formal-
ism itself. In particular, when an external intervention
fixes the value of a variable, it also eliminates the
causal influence of other variables on that variable. If
I simply decide to stop drinking wine, then my inter-
vention alone will determine the value of wine drink-
ing; partying will no longer have any effect. This can
be represented by replacing the original graph with an
altered graph in which arrows directed into the inter-
vened on variable are eliminated (Judea Pearl in 2000
vividly referred to this process as graph surgery). The
conditional dependencies among the variables after
the intervention can be read from this altered graph.
Suppose, for example, I want to know what I can
do to prevent my insomnia. Should I sit in my room
alone but continue to drink when I want to or go to
parties just the same but stick to Perrier? I can calcu-
late the effects of such interventions on each of the
causal structures using graph surgery and predict the
results. I will obtain different results from these inter-
ventions depending on the true causal structure (soli-
tary drinking will lead to insomnia, and sober partying
will not for Graph 1; sober partying will lead to insom-
nia, and solitary drinking will not for Graph 2).
Exactly the same inferential apparatus can be used
to generate counterfactual predictions. Suppose I want
to ask what would have happened had things been oth-
erwise. If I had refrained from wine at all those confer-
ences, then would my life, or at least my insomnia,
have been better? Graph surgery will also answer this
question. Just as in an intervention, a counterfactual
“fixes” the value of certain variables and allows you to
infer the consequences.
A central aspect of causal Bayes nets, indeed the
thing that makes them causal, is that they allow us to
go back and forth freely from evidence about observed
probabilities to inferences about interventions and
vice versa.
These two assumptions, then, allow us to take a par-
ticular causal structure and accurately predict the con-
ditional probabilities of events, and the consequences
of interventions on those events, from that structure.
Bayes Nets and Learning
We just saw that knowing the causal structure lets us
make the right predictions about interventions and
probabilities. We can also use this fact to learn causal
structure from the evidence of interventions and
probabilities.
Let us go back to the wine-insomnia example. You
could distinguish between these graphs by either
intervention or observation. You could, for instance,
hold partying constant (always partying or never par-
tying) and vary whether you drink wine, or you could
hold drinking constant (always drinking or never
drinking) and vary whether you party. In either case,
you could observe the effect on your sleep. If drinking
affects your sleep when partying is held constant, but
partying has no effect on your sleep when drinking is
held constant, then you could conclude that Graph 1
is correct. Such reasoning underlies the logic of
experimental design in science.
You could also, however, simply observe the rela-
tive frequencies of the three events. If you notice that
you are more likely to have insomnia when you drink
wine, whether or not you party, then you can infer
that Graph 1 is correct. If you observe that, regardless
of how much or how little wine you drink, you are
only more likely to have insomnia when you go to a
party, then you will opt instead for Graph 2. These
inferences reflect the logic of correlational statistics in
science. In effect, what you did was to “partial out”
the effects of partying on the wine-insomnia correla-
tion and draw a causal conclusion as a result.
This type of learning, however, requires an addi-
tional assumption. The assumption is that the pat-
terns of dependence and independence we see
among the variables really are the result of the causal
relations among them. Suppose that wine actually
makes you sleepy instead of keeping you awake. But,
it just happens to be the case that this influence of
wine on insomnia is perfectly canceled out by the
countervailing exciting influence of parties. We will
incorrectly conclude that there are no causal relations
between the three variables. We need to assume that
6
CAUSAL LEARNING

these sinister coincidences will not occur. Formally,
this is called the faithfulness assumption.
The Faithfulness Assumption
In the joint distribu-
tion on the variables in the graph, all conditional
independencies are consequences of the Markov
assumption applied to the graph.
Given the faithfulness assumption, it is possible to
infer complex causal structure from patterns of condi-
tional probability and intervention (Glymour &
Cooper, 1999; Spirtes et al., 1993). Computationally
tractable learning algorithms have been designed to
accomplish this task and have been extensively applied
in a range of disciplines (e.g., Ramsey, Roush, Gazis, &
Glymour, 2002; Shipley, 2000). In some cases, it is also
possible to accurately infer the existence of new unob-
served variables that are common causes of the
observed variables (Richardson & Spirtes, 2003; Silva,
Scheines, Glymour, & Spirtes, 2003).
Causal Bayes net representations and learning
algorithms allow learners to predict patterns of evi-
dence accurately from causal structure and to learn
causal structure accurately from patterns of evidence.
They constitute a kind of inductive causal logic, and
a logic of causal discovery. It is possible to prove that
only certain patterns of evidence will follow from par-
ticular causal structures, given the Markov, interven-
tion, and faithfulness assumptions, just as only certain
conclusions follow from particular logical premises
given the axioms of logic.
From: mherskovits@psych.ucarcadia.arcadia.edu
To: brook_russell@turing.carnegietech.edu
3:15 a.m., August 5, 2003
Righto Brook.
Well, quadruple countervailing causal
prevention sounds just fascinating. I’m so glad
I’m going to this conference now.
But, thanks for the attachment. Actually,
I think I might be getting the hang of these
Bayes net things, even with all the formal stuff.
(Though there’s one thing about the math
I still don’t get: Why do you Brits insist on
making it plural?) They sound like something
we know a lot about in Arcadia: vision. Not of
course the political kind or the hallucination
kind (although we know a lot about those, too),
but the kind we study in psychophysics and
perceptual psychology.
The world out there is full of real three-
dimensional objects, but our perceptual system
just gets some distorted two-dimensional retinal
input. Still, the merest “sprog,” as you would say,
has the computational power to turn that input
back into a three-dimensional representation of a
table or a lamp without even thinking about it.
And (ignoring the occasional illusion), those
representations are accurate: They capture the
truth about the spatial world.
In vision science, we have “ideal observer”
theories about how that happens—how any
system, animal, human or robotic, sprog,
or Ph.D. could infer the structure of a three-
dimensional world from two-dimensional data.
Vision science tells us that the visual system
implicitly assumes that there is a world of three-
dimensional moving objects and then makes
assumptions about how those objects lead to
particular patterns on the retina. By making
the further assumption that the retinal patterns
were, in fact, produced by the objects in
this way, the system can work backward and
infer the structure of objects from those
patterns (see, e.g., Palmer, 1999).
Your causal Bayes net inferences sound sort
of like that. The visual system assumes that the
patterns at the retina were produced by 
three-dimensional objects in a particular way
and then uses those assumptions to infer the
objects from the retinal patterns. Your causal
Bayes nets assume that causal structure
produced patterns of evidence and uses those
assumptions to learn the structure from the
evidence (your causal Markov, intervention,
and faithfulness assumptions). You guys seem
to think that you’re going to do the same thing
for causality that the psychophysicists have
done for vision: You’re going to tell us how we
could transform information about probabilities
and interventions into accurate representations
of the causal structure of the world.
So, I guess if you’re right (and I’m not
committing myself yet there), causal Bayes nets
could give us a way of formally specifying
accurate inductive causal inferences—just like
ideal observer theories in vision provide a way
of formally specifying accurate visual inferences
INTRODUCTION
7

and like logic provides a way of formally
specifying accurate deductive inferences.
But if that’s right, then I have to say, Brook,
the rest of your letter doesn’t make a whole lot
of sense  :)  You seem to be under the bizarre
impression that any knowledge you can’t find
in the Times Literary Supplement isn’t really
“knowledge.” So, I guess you think my sprogs
can’t see because they can’t write an article on
Fourier transforms.
But, of course, my sprogs see just as well as
you and I do. And, of course, sprogs can use
vision to learn all sorts of new things about
objects. In fact, they engage in perfectly
sophisticated “maths” all the time—and if they
can perform complex, implicit computations to
support vision, then they could, in principle,
perform complex, implicit computational
procedures to support causal inference.
Your computers may or may not be able to
solve this causal learning problem, but it’s certain
that my sprogs can do it. In fact, they might be
the most powerful causal learning devices in the
universe. Thirty years of work on the “theory theory”
shows that children have abstract, coherent
representations of the causal structure of the
world. Those representations allow children to
make predictions, perform interventions, and
even generate counterfactuals. As soon as they
can talk, they even offer explanations of the
world around them. And, they seem to learn
those causal structures from patterns of evidence.
Plus, even the very smallest sprogs can
combine information from observation and
intervention. Little babies who learn a new
skill—like reaching for objects—understand
other people’s actions on objects better than
babies who don’t have the skill. Jessica
Sommerville will show you next week how
giving babies “sticky mittens” and changing
their own ability to act on the world changes
the babies’ ability to understand the actions of
others. Andrew Meltzoff will show you
something like the reverse: how babies take
information they only observe and turn it into
actions of their own. Sprogs do all sorts of other
things: make good interventions, discriminate
confounded and unconfounded interventions,
reason about unobserved causes, learn complex
causal structure. . . . Laura Schulz, Tamar
Kushnir, and that Gopnik woman whose name
you like so much will also show you all that on
Saturday. When it comes to grown-ups, York
Hagmayer, Steve Sloman, Dave Lagnado, and
Michael Waldmann will show you that even
those stats class undergraduates can make
remarkably sophisticated inferences about both
predictions and interventions.
Best of all, sprogs never do absolutely
useless things like reason about quadruple
causal prevention.
Anyway, I’m doing my part and attaching
some fairly primitive stuff about the psychology
of causal learning. As you’ll see, even the best
theoretical accounts we have don’t really even
start to capture the richness of what people,
even very small people, can actually do.
All the best,
Morgan
Attachment 2: The Psychology of Causal
Learning for Nerds
The Piagetian Account of Causal Reasoning
Research on children’s causal reasoning, like research
on cognitive development in general, was initiated by
the work of Jean Piaget (1929, 1930). Piaget believed
that causal reasoning developed very gradually.
Indeed, Piaget proposed no less than 17 distinct stages
of causal learning.
In particular, however, Piaget believed that chil-
dren’s reasoning from early to middle childhood was
“precausal.” It was characterized by a confusion
between psychological activity and physical mecha-
nism (Piaget 1930). This conclusion was based chiefly
on his investigation of children’s explanations of nat-
ural phenomena. Piaget found that children’s early
explanations of physical events were artificialistic
(meaning events were attributed to human interven-
tion: clouds move because we walk, the river flows
because of boats) and animistic (meaning that physi-
cal events were attributed to psychological intention:
the string turns because it wants to unwind itself)
(1929). According to Piaget’s account, not until quite
late in development are children able to provide a
complete, functional account of a chain of causal
events and reason accurately about intervening causal
mechanisms.
8
CAUSAL LEARNING

Nativist and Modular Views of Causal
Reasoning
Over the past several decades, however—and with 
the development of new methods for assessing the
cognitive abilities of infants and young children—
considerable research has suggested that Piaget
underestimated the causal reasoning abilities of
young children. Both infants and adults seem to per-
ceive causality when objects (like billiard balls) col-
lide and launch one another (Leslie & Keeble, 1987;
Michotte, 1962; Oakes & Cohen, 1990). Infants also
seem to expect causal constraints on object motion,
assuming that objects respect principles of support,
containment, cohesion, continuity, and contact
(Baillargeon, Kotovsky, & Needham, 1995; Spelke,
Breinlinger, Macomber, & Jacobson, 1992; Spelke,
Katz, Purcell, Ehrlich, & Breinlinger, 1994).
Moreover, contra Piaget, considerable evidence
suggests that even babies appropriately distinguish
psychological and physical causality. Specifically,
infants seem to interpret human, but not mechanical,
action as goal directed and self-initiated (Meltzoff,
1995; A. L. Woodward, 1998; A. L. Woodward,
Phillips, & Spelke, 1993). Thus, for instance, babies
expect physical objects to move through contact
(Leslie & Keeble, 1987; Oakes & Cohen, 1990) 
but do not expect the same of human agents 
(A. L. Woodward et al., 1993); expect that an object
will be entrained when grasped by a human hand but
not by an inanimate object (Leslie, 1982, 1984); and
treat the reach of a human hand, but not the trajec-
tory of a metal claw, as goal directed (A. L. Woodward,
1998). Furthermore, almost as soon as children can
speak they offer causal explanations (at least of famil-
iar, everyday events) that respect domain boundaries
(Hickling & Wellman, 2001). Finally, preschoolers’
predictions, causal judgments, and counterfactual
inferences are remarkably accurate across a wide
range of tasks and content areas (e.g., Flavell, Green, &
Flavell, 1995; Gelman & Wellman, 1991; Gopnik &
Wellman, 1994; Kalish, 1996; Sobel, 2004).
To account for the early emergence of structured,
coherent, causal knowledge, some psychologists have
suggested that children’s early causal representations
might be largely innate rather than learned. Following
Kant’s conception of a priori causal knowledge
(1787/1899), some researchers have proposed that
children’s early causal understanding might originate
in domain-specific modules (Leslie & Keeble, 1987)
or from innate concepts in core domains (Carey &
Spelke, 1994; Keil, 1995; Spelke et al., 1994). These
researchers have suggested that children’s causal
knowledge might be accurate not because of general
learning mechanisms designed to infer structure from
evidence but because of specialized mechanisms
dedicated to relatively constrained information-
processing tasks (Leslie, 1994).
It may be that infants’ object concepts, their abil-
ity to distinguish objects from agents, and their per-
ception of Michottean causality do indeed have an
innate basis. However, there seems less reason to
believe that children’s abilities to reason broadly
about the causes of human behavior, physical events,
and biological transformations are an outgrowth of
domain-specific modules. In particular, modular,
domain-specific accounts of causal reasoning do not
seem to explain how we identify particular causal rela-
tions within a domain, how we make causal infer-
ences that transcend domain boundaries (i.e., that
physical causes can be responsible for psychological
effects and vice versa), and why causal reasoning is
sensitive to patterns of evidence. Nonetheless, the
majority of post-Piagetian research on preschool chil-
dren’s causal reasoning has emphasized the centrality
of substantive, domain-appropriate principles.
Domain-Specific Causal Knowledge, Causal
Mechanisms, and the “Generative
Transmission” Account
In particular, researchers have focused on the role that
substantive concepts, like force and spatial contact,
might play in constraining young children’s inferences
about physical causal events (e.g., Bullock, Gelman,
& Baillargeon, 1982; Leslie, 1984; Shultz, Pardo, &
Altmann, 1982; Shultz, 1982). In an influential mono-
graph on children’s causal reasoning, the psychologist
Thomas Shultz distinguished between a statistical
view of causal relations, in which the causal connec-
tion between events is determined by the covariation
of cause and effect, and a causal mechanism view of
causality, in which causation is understood “primarily
in terms of generative transmission” of force and
energy (1982, p. 46). In a series of experiments, Shultz
demonstrated that, in their causal judgments,
preschoolers privilege evidence for spatially continu-
ous processes compatible with the transmission of
energy over evidence for covariation. Preschoolers
inferred, for instance, that a tuning fork with vibrations
that were not obstructed was more likely to produce a
sound than a tuning fork with vibrations that were
blocked, even when the effect immediately followed
INTRODUCTION
9

an intervention on the latter and followed the former
only after a delay.
Similarly, Bullock, Gelman, and Baillargeon con-
cluded that the idea that “causes bring about their
effects by transfer of causal impetus” is “central to the
psychological definition of cause-effect relations”
(1982). Consistent with this view, psychologists have
shown that even adults prefer information about plau-
sible, domain-specific mechanisms of causal transmis-
sion to statistical and covariation information in
making causal judgments (Ahn, Kalish, Medin, &
Gelman, 1995).
Covariation Accounts
However, the generative transmission view of causa-
tion in particular and domain-specific knowledge in
general have played a rather limited role in accounts
of adult causal learning. Indeed, in the adult cogni-
tive science literature, researchers have largely
focused on the role of contingency and covariation in
causal learning, as opposed to principles about mech-
anisms. Two accounts of causal learning have been
particularly influential: associative learning or con-
nectionist accounts and Patricia Cheng’s causal
power theory (1997).
Associative Learning and Connectionist
Accounts of Causal Learning
Although not all contingencies are causal, all causal
relationships involve contingencies. There is a vast
body of literature on contingency learning in 
both human and nonhuman animals, and some
researchers have proposed that mechanisms similar to
those underlying contingency learning in operant and
classical conditioning can account for human causal
reasoning (Dickinson, Shanks, & Evendon, 1984;
Shanks & Dickinson, 1987; Shanks, Holyoak, &
Medin, 1996; Wasserman, Elek, Chatlosh, & Baker,
1993).
Instrumental and Imitative Learning
Thorndike found that cats could learn to escape from
cages by trial and error, and that with practice, the cats
became faster at escaping. He described this as the law
of effect: Actions with positive consequences are likely
to be repeated and actions with negative consequences
avoided (1911/2000). A large body of research on
learning subsequently elaborated the ways in which
behavior could be shaped by reinforcing or punishing
outcomes. Operant learning has been demonstrated in
nonhuman animals ranging from pigeons to primates;
unsurprisingly, it has been demonstrated in human
babies as well. Thus, infants who learn, for instance,
that kicking makes a mobile spin, will both repeat the
behavior and remember it after significant delays
(Rovee-Collier, 1980; Watson & Ramey, 1972).
Instrumental learning—the ability to learn from the
immediate consequence of one’s own actions—seems
to be an early development, both phylogenetically and
ontogentically.
Importantly, human beings (if not uniquely
among animals, then at least characteristically; see
Tomasello & Call, 1997) are able to learn not only
from the consequence of their own actions but also
from the consequences of others’ actions. Thus, for
instance, 9-month-old babies who see an experi-
menter light up a toy by touching it with his head will
spontaneously touch their own heads to the toy
(Meltzoff, 1988). By 18 months, infants will even rec-
ognize the goal of another’s intervention and produce
the completed action when they have seen only a
failed attempt (Meltzoff, 1995). Such research sug-
gests that young children can learn the causal relation
between human actions and the events that follow
them. However, it does not explain how children
learn causal relations when human action is not 
the causal variable (e.g., the causal relationship
between two parts of a toy, the causal relationship
between growth and food, and the causal relation-
ship between mental states and behavior). Instru-
mental learning and learning from the direct
outcome of others’ interventions do not seem to
explain our ability to engage in nonegocentric causal
reasoning about distal events.
Classical Learning and the Rescorla-
Wagner Theory
Shortly after Thorndike formulated the law of effect,
Pavlov famously discovered that an animal regularly
exposed to a temporal contiguity between a condi-
tioned stimulus (like a tone) and an unconditioned
stimulus (like food) would learn to associate the
two stimuli. When presented only with the conditioned
stimulus, the animal would produce a response (e.g.,
salivating) normally elicited by the unconditioned
stimulus (1953). This finding has also been replicated
across species and ages; like instrumental learning,
10
CAUSAL LEARNING

classical conditioning is an ontogenetically, phylogenti-
cally, early, robust development.
Rescorla modified Pavlov’s theory to suggest that
contingency, not just contiguity, was critical for
learning Rescorla & Wagner (1972). That is, for
learning to occur, cues have to be predictive: The
probability of the effect given the cue must be greater
than the probability of the effect in the absence of the
cue. The Rescorla-Wagner theory (R-W theory; 1972)
specified that learning occurred on a trial-by-trial
basis and predicted that early trials would be more
important to learning than later trials.
In its simplest form, the R-W equation for associa-
tive learning is VK(V), where V is the
change in the perceived strength of the association
(e.g., the amount of learning that occurs on any given
trial), K is a parameter between 0 and 1 that reflects
the salience of the cue multiplied by the salience of
the effect,  is the association between cue and stim-
ulus at asymptote, and V is the sum of the associa-
tive strength on previous trials.
Thus, the R-W theory predicts that the change in
associative strength on any trial is proportional to the
difference between the maximum possible associative
strength between a cue and an outcome and the pre-
vious estimate of the strength of association. Thus, the
stronger the prior association is, the less learning there
will be on any given trial.
The model can be applied to human causal learn-
ing by substituting causes for the conditioned stimu-
lus and effects for the unconditioned stimulus. The
associative strength between the two variables is then
taken as indicating the causal connection between
them. This equation successfully predicts findings in
the animal learning literature such as blocking, over-
shadowing, and conditioned inhibition and many
findings in the human contingency learning literature
(Baker, Mercier, Valee-Tourangeau, Frank, & Maria,
1989; Dickinson et al., 1984; Shanks et al., 1996;
Wasserman et al., 1993). The R-W rule, or generaliza-
tions of the rule, have often been implemented in
connectionist networks aimed at explaining human
causal learning (see, e.g., Gluck & Bower, 1988;
Rogers & McLelland, 2004; Shanks, 1990).
However, there is substantial agreement that the
R-W equation by itself does not adequately account
for the psychology of human causal learning (see,
e.g., Cheng, 1997; Glymour, 2001; Gopnik 
et al., 2004; Waldmann, 1996, 2000; Waldmann &
Holyoak, 1992). In fact, it may not even explain 
animal learning. The R-W account predicts neither
learned irrelevancy (the fact that an animal first
exposed to a cue without any reward or punishment
has difficulty on later conditioning trials learning to
associate the cue with an outcome) nor failures of
extinction (the fact that an animal that has learned
through operant conditioning to avoid a cue once
associated with a punishment retains the behavior in
the presence of the cue long after the association has
disappeared).
In the human case, Patricia Cheng demonstrated,
for instance, that the R-W approach fails to account
for boundary conditions on causal inference (1997).
When an effect always occurs (i.e., whether the can-
didate cause is present or not), the R-W equation pre-
dicts that we should conclude that the candidate
generative cause is ineffective. In contrast, human
reasoners believe that if the effect occurs at ceiling,
then there is no way to determine the efficacy of a
candidate cause. Similarly, if an effect never occurs,
then the R-W equation predicts that we should
believe a candidate inhibitory cause is ineffective,
whereas people believe that if the effect never occurs,
then it is impossible to determine the strength of an
inhibitory cause. Similarly, Waldmann (1996, 2000;
Waldmann & Holyoak, 1992) showed asymmetries in
the predictive and diagnostic uses of causal informa-
tion that were difficult to explain in associationist
terms.
The R-W account also fails to explain a phenome-
non known as backward blocking (Sobel, Tenenbaum,
& Gopnik, 2004). If two candidate causes A and B
together produce an effect and it is also the case that
A by itself is sufficient to produce the effect, then
human reasoners (including young children) are less
likely to believe that B is a cause of the effect.
However, since observing A by itself provides no new
evidence about the association between B and the
effect, the R-W rule predicts that our estimate of the
causal strength of B should not change (although
some researchers, e.g., Wasserman & Berglan, 1998,
have suggested modifications to the R-W rule that do
allow for this prediction).
In addition to those aspects of human causal reason-
ing that seem to contradict the predictions of the R-W
model, there are many aspects of human causal learn-
ing that would require ad hoc modification of the R-W
rule. The R-W model, for instance, calculates the
strength of every candidate cause separately; thus, to
judge the interaction of two causes, it must treat the
INTRODUCTION
11

interaction as a “third” candidate cause (see Gopnik
et al., 2004). Similarly, the R-W equation assumes that
all the variables have already been categorized as
causes or effects and then calculates the associative
strength between each cause and each effect. However,
the model cannot determine whether variables are
causes or effects (i.e., it cannot decide whether 
A causes B, B causes A, or neither). One might run the
equation multiple times, sometimes with one variable
as a cause and sometimes with the other, and then
compare the relative strength of each pairing, but this
is an ad hoc modification of the theory.
The Power Theory of Probabilistic Contrast
Patricia Cheng (1997) proposes an account of human
causal learning that resolves some of the difficulties
with the R-W account. Cheng proposes that people
innately treat covariation as an index of causal power
(an unobservable entity) and suggests that people rea-
son about causes with respect to particular focal sets,
a contextually determined set of events over which
people compute contrasts in covariation.
Cheng uses probabilistic contrast (P) as an index
of covariation. P is simply the difference between
the probability of an effect given a candidate cause
and in the absence of the candidate cause; formally,
PP(e | c)P(e | ~c). However, in distinction from
purely covariational accounts of causal reasoning,
Cheng introduces the idea of causal power. Although
we cannot know the real causal power of any variable
(because causal power is a theoretical entity), we can
estimate causal power by distinguishing between the
probability of the effect in the presence of a candidate
cause and the probability of the effect in the presence
of all causes (known and unknown) alternative to the
candidate cause. Cheng assumes (a) that candidate
causes and alternative causes influence the effect
independently; (b) that there are no unobserved com-
mon causes of the candidate cause and the effect
(although the account can be generalized to relax this
assumption; Glymour, 2001); and (c) that candidate
causes are noninteractive (although Novick and
Cheng, 2004, have since modified the account to
explain inferences about interactive causes).
The causal power of a candidate cause is not
equivalent to either P(e | c) or P because even when
the candidate cause is present and the effect occurs,
the effect could be caused by alternative causes.
However, if you assume that alternative causes occur
independently of the candidate cause, then the prob-
ability of the effect when the candidate cause is pres-
ent and all alternative causes are absent can be
estimated as 1P(e | ~c). Thus, generative causal
power pc can be estimated as pc P/(1  P(e | ~c)).
As this equation illustrates, when alternative
causes are absent, P will reflect the causal power of
c. However, as P(e | ~c) increases, P becomes an
increasingly conservative estimate of causal power.
The limiting case, of course, is when the effect
always occurs (whether c is present or not). In that
case, the reasoner can no longer use covariation as an
index of causation, and the causal power of c is unde-
fined. This explains both why ceiling effects are a
boundary condition on causal inference and why
covariation is not, in general, equivalent to causation.
A parallel account explains inferences about candi-
date inhibitory causes.
Although compelling as a psychological account
of human causal learning, one weakness of Cheng’s
account is that, like the R-W account, it assumes that
variables in the world are already identified as causes
or effects. The account does not explain how, in the
absence of prior knowledge or temporal cues, people
could use data to distinguish causes and effects (i.e.,
to infer whether A causes B or B causes A).
Put another way, both the R-W account and the
Cheng account are explanations of how people judge
the strength of different causal variables. These theo-
ries do not explain how people make judgments about
causal structure. In addition, neither the R-W nor the
power PC theory provides a unified account of how
people might go from judgments about causes to
inferences about the effects of interventions. Finally,
both of these accounts assume that the candidate
causes and effects are observed. Neither account
explains how people might use observational data to
infer the existence of unobserved causes.
From: brook_russell@turing.carnegietech.edu
To: mherskovits@psych.ucarcadia.arcadia.edu
My dear Morgan,
Thank you for your letter and the attachment.
Well, perhaps you are right that there is more
similarity between our problems than one
might at first think. Your description of the
different positions in the psychology of causal
learning is indeed reminiscent of the classical
12
CAUSAL LEARNING

positions in the philosophical literature –partly,
I suppose, because historically speaking this is
where the psychological positions ultimately
come from. In philosophy, accounts of
causation have been similarly divided. Some
accounts, like those of Dowe (2000) or Salmon
(1998), stress “mechanism” and “transmission.”
Much like your Shultz they argue that
causation involves the spatiotemporal
transmission of some sort of “mark” or
“impetus” from cause to effect. Since Hume,
the alternative account, usually phrased in
skeptical terms, has been that causation just
amounts to covariation–sounding rather like
your associationists two centuries later. As,
Bertrand Russell put it: “The law of causality, 
I believe, like much that passes muster among
philosophers, is a relic of a bygone age,
surviving, like the monarchy, only because it is
erroneously supposed to do no harm.”
But, you see recently, and in tandem
with all the new maths I told you about in
that attachment, there’s been a new way of
thinking about causation in philosophy.
Philosophers increasingly think about
causation in relation to intervention: In terms
that would suit your sprogs—if X causes Y,
then if you wiggled X, Y would also wiggle.
Jim Woodward will tell you all about it on
Saturday, and Chris Hitchcock will show you
how it helps explain even those cases of
quadruple countervailing prevention you find
so amusing. And, John Campbell will tell you
how it applies to even the kind of causation
your particular brand of scientist deals in—the
psychological kind.
Here is the really important and, I must
confess, somewhat against my will, even
intriguing thing about your letter. The unsolved
problems you describe in the psychology of
causal learning—the things you say your sprogs
are so good at doing and the theories are so bad
at explaining—well, they’re just the sort of
things that the interventionist/causal Bayes net
account seems, well, destined for.
My learning algorithms, like your sprogs,
can infer causal structure rather than just
strength; they can appropriately combine
information from interventions and
observations and distinguish appropriately
between them, and they can even infer
unobserved variables from evidence. So, if the
two actually were conjoined, . . .
As ever,
Brook
P.S. Oh and, by the way, there seems to be a
defect in your word-processing program. In
several places where a full stop is clearly
intended, it seems to transmit a colon or
semicolon followed by a right parenthesis
instead; quite mysterious.
References
Ahn, W. K., Kalish, C. W., Medin, D. L., Gelman, S. A.
The role of covariation versus mechanism infor-
mation in causal attribution. Cognition, 54,
299–352.
Baillargeon, R., Kotovsky, L., & Needham, A. (1995).
The acquisition of physical knowledge in infancy.
In D. Sperber & D. Premack (Eds.), Causal cogni-
tion: A multidisciplinary debate. Symposia of the
Fyssen Foundation; Fyssen Symposium, 6th January
1993, Pavillon Henri IV, St-Germain-en-Laye,
France (pp. 79–115). New York: Clarendon
Press/Oxford University Press.
Baker, A., Mercier, P., Valee-Tourangeau, F., Frank, R., &
Maria, P. (1993). Selective associations and causality
judgments: Presence of a strong causal factor may
reduce judgments of a weaker one. Journal of
Experimental Psychology: Learning, Memory, and
Cognition, 19, 414–432.
Bullock, M., Gelman, R., & Baillargeon, R. (1982). 
The development of causal reasoning. In 
W. J. Friedman (Ed.), The developmental psychology
of time (pp. 209–254). New York: Academic Press.
Carey, S., & Spelke, E. S. (1994). Domain-specific
knowledge and conceptual change. In L. A.
Hirschfeld & S. A. Gelman (Eds.), Mapping the
mind: Domain specificity in cognition and culture;
based on a conference entitled “Cultural Knowledge
and Domain Specificity,” held in Ann Arbor,
Michigan, October 13–16 (pp. 169–200). New York:
Cambridge University Press.
Cheng, P. W. (1997). From covariation to causation: 
A causal power theory. Psychological Review, 104,
367–405.
Dickinson, A., Shanks, D. R., & Evendon, J. (1984).
Judgment of act-outcome contingency: The role of
INTRODUCTION
13

selective attribution. Quarterly Journal of Experi-
mental Psychology, 36, 29–50.
Dowe, P. (2000). Physical causation. Cambridge:
Cambridge University Press.
Flavell, J. H., Green, F. L., & Flavell, E. R. (1995). Young
children’s knowledge about thinking. Monographs of
the Society for Research in Child Development, 60,
pp. v–96.
Gelman, S. A., & Wellman, H. M. (1991). Insides and
essence: Early understandings of the non-obvious.
Cognition, 38, 213–244.
Gluck, M., & Bower, G. H. (1988). Evaluating an adap-
tive network model of human learning. Journal of
Memory and Language, 27, 166–195.
Glymour, C. Invasion of the mind snatchers. In Giere, R.
(1992) (ed.) Cognitive models of science. Minneapolis,
University of Minnesota Press, pp. 419–501.
Glymour, C. (2001). The mind’s arrows: Bayes nets and
causal graphical models in psychology. Cambridge,
MA: MIT Press.
Glymour, C., & Cooper, G. F. (1999). Computation, causa-
tion, and discovery. Cambridge, MA: MIT/AAAI Press.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 1–31.
Gopnik, A., & Wellman, H. M. (1994). The theory theory.
In S. A. Gelman & L. A. Hirschfeld (Eds.), Mapping
the mind: Domain specificity in cognition and culture;
based on a conference entitled “Cultural Knowledge
and Domain Specificity,” held in Ann Arbor,
Michigan, October 13–16, 1990 (pp. 257–293). 
New York: Cambridge University Press.
Hickling, A. K., & Wellman, H. M. (2001). The emer-
gence of children’s causal explanations and theories:
Evidence from everyday conversation. Developmental
Psychology, 37, 668–684.
Kalish, C. (1996). Causes and symptoms in preschoolers’
conceptions of illness. Child Development, 67,
1647–1670.
Kant, I. (1899). Critique of Pure Reason (J. Meiklejohn,
Trans.). New York: Colonial Press. (Original work
published 1787)
Keil, F. C. (1995). The growth of causal understandings
of natural kinds. In D. Sperber & D. Premack
(Eds.), Causal cognition: A multidisciplinary debate.
Symposia of the Fyssen Foundation; Fyssen
Symposium, 6th January 1993, Pavillon Henri IV, 
St-Germain-en-Laye, France (pp. 234–267). New
York: Clarendon Press/Oxford University Press.
Leslie, A. M. (1982). The perception of causality in
infants. Perception, 11, 173–186.
Leslie, A. M. (1984). Infant perception of a manual pick-up
event. British Journal of Developmental Psychology, 2,
19–32.
Leslie, A. M. (1994). ToMM, ToBy, and agency: Core
architecture and domain specificity. In L. A.
Hirschfeld & S. A. Gelman (Eds.), Mapping the
mind: Domain specificity in cognition and culture;
based on a conference entitled “Cultural Knowledge
and Domain Specificity,” held in Ann Arbor,
Michigan, October 13–16, 1990 (pp. 119–148). New
York: Cambridge University Press.
Leslie, A. M., & Keeble, S. (1987). Do six-month-old
infants perceive causality? Cognition, 25, 265–288.
Meltzoff, A. N. (1988). Infant imitation after a 1-week
delay: Long term memory for novel acts and multi-
ple stimuli. Developmental Psychology, 24,
470–476.
Meltzoff, A. N. (1995). Understanding the intentions of
others: Re-enactment of intended acts by 18-month-
old children. Developmental Psychology, 31,
838–850.
Michotte, A. E. (1962). Causalite, permanence et realite
phenomenales; etudes de psychologie experimentale.
Louvain, France: Publications universitaires.
Novick, L. R., & Cheng, P. W. (2004). Assessing interactive
causal influence. Psychological Review, 111, 455–485.
Oakes, L. M., & Cohen, L. B. (1990). Infant perception
of a causal event. Cognitive Development, 5,
193–207.
Palmer, S. (1999). Vision science: From photons to
phenomenology. Cambridge, MA: MIT Press.
Pavlov, I. P. (1953). Collected works. Oxford, England:
Akademie Verlag.
Pearl, J. (1988). Probabilistic reasoning in intelligent
systems. San Mateo, CA: Morgan Kaufmann.
Pearl, J. (2000). Causality. New York: Oxford University
Press.
Piaget, J. (1929). The child’s conception of the world. New
York: Harcourt, Brace.
Piaget, J. (1930). The child’s conception of physical
causality. London: Kegan Paul.
Ramsey, J., Roush, T., Gazis, P., & Glymour, C. (2002).
Automated remote sensing with near-infra-red
reflectance spectra: Carbonate recognition. Data
Mining and Knowledge Discovery, 6, 277–293.
Reichenbach, H. (1971). The direction of time. Berkeley:
University of California Press.
Rescorla, R. A., & Wagner, A. R. (1972). A theory of
Pavlovian conditioning: Variations in the effective-
ness of reinforcement and nonreinforcement. In
A. H. Black & W. F. Prokasy (Eds.), Classical condi-
tioning II: Current theory and research (pp. 64–99).
New York: Appleton-Century-Crofts.
Richardson, T., & Spirtes, P. (2003). Causal inference
via ancestral graph models. In P. Green, N. Hjort,
& S. Richardson (Eds.), Highly structured stochastic
systems. Oxford, England: Oxford University Press
pp. 1–12.
14
CAUSAL LEARNING

Rogers, T., & McLelland, J. (2004). Semantic cognition:
A parallel distributed approach. Cambridge, MA:
MIT Press.
Rovee-Collier, C. (1980). Reactivation of infant memory.
Science 208, 1159–1161.
Salmon, W. C. (1998). Causality and explanation.
New York, Oxford University Press.
Shultz, T. (1982). Rules of causal attribution.
Monographs of the Society for Research in Child
Development, 194, 47, 1.
Shultz, T. R., Pardo, S., & Altmann, E. (1982). Young
children’s use of transitive inference in causal
chains. British Journal of Psychology, 72, 235–241.
Shanks, D. R. (1990). Connectionism and the learning
of probabilistic concepts. Quarterly Journal of
Experimental Psychology: Human Experimental
Psychology, 42, 209–237.
Shanks, D. R., & Dickinson, A. (1987). Associative
accounts of causality judgment. In G. H. Bower
(Ed.), The psychology of learning and motivation:
Advances in research and theory
(Vol. 21, 
pp. 229–261). San Diego, CA: Academic Press.
Shanks, D. R., Holyoak, K., & Medin, D. L. (1996).
Causal learning. San Diego, CA: Academic Press.
Shipley, B. (2000). Cause and correlation in biology.
Oxford, England: Oxford University Press.
Silva, R., Scheines, R., Glymour, C., & Spirtes, P. (2003).
Learning measurement models for unobserved
variables. Proceedings of the 18th Conference on
Uncertainty in Artificial Intelligence. AAAI Press. 
pp. 191–246.
Sobel, D. M. (2004). Exploring the coherence of young
children’s explanatory abilities: Evidence from gen-
erating 
counterfactuals. 
British 
Journal 
of
Developmental Psychology, 22, 37–58.
Sobel, D. M., Tenenbaum, J., & Gopnik, A. (2004).
Children’s causal inferences from indirect evi-
dence: Backwards blocking and Bayesian reasoning
in preschoolers. Cognitive Science, 28(3), pp.
305–333.
Spelke, E. S., Breinlinger, K., Macomber, J., & Jacobson, K.
(1992). Origins of knowledge. Psychological Review,
99, 605–632.
Spelke, E. S., Katz, G., Purcell, S. E., Ehrlich, S. M., &
Breinlinger, K. (1994). Early knowledge of object
motion: Continuity and inertia. Cognition, 51,
131–176.
Spirtes, P., Glymour, C., & Scheines, R. (1993).
Causation, prediction, and search (Springer Lecture
Notes in Statistics). New York: Springer-Verlag.
Thorndike, E. L. (2000). Animal intelligence: Experimental
studies. New Brunswick, NJ: Transaction. (Original
work published 1911)
Tomasello, M., & Call, J. (1997). Primate cognition.
London: Oxford University Press.
Waldmann, M. R. (1996). Knowledge-based causal
induction. In D. R. Shanks, K. Holyoak, & D. L.
Medin (Eds.), Causal learning (pp. 47–88). San
Diego, CA: Academic Press.
Waldmann, M. R. (2000). Competition among causes
but not effects in predictive and diagnostic learning.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 26, 53–76.
Waldmann, M. R., & Holyoak, K. J. (1992). Predictive and
diagnostic 
learning 
within 
causal 
models:
Asymmetries in cue competition. Journal of Experi-
mental Psychology: General, 121, 222–236.
Wasserman, E. A., & Berglan, L. R. (1998). Backward
blocking and recovery from overshadowing in
human causal judgment: The role of within com-
pound associations. Quarterly Journal of Experi-
mental Psychology: Comparative and Physiological
Psychology, 51, 121–138.
Wasserman, E. A., Elek, S. M., Chatlosh, D. L., & Baker,
A. G. (1993). Rating causal relations: Role of proba-
bility in judgments of response-outcome contin-
gency. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 19, 174–188.
Watson, J. S., & Ramey, C. T. (1972). Reactions to
response-contingent stimulation in early infancy.
Merrill-Palmer Quarterly, 18, 219–227.
Woodward, A. L. (1998). Infants selectively encode the
goal object of an actor’s reach. Cognition, 69, 1–34.
Woodward, A. L., Phillips, A. T., & Spelke, E. S. (1993).
Infants’ expectations about the motion of animate
versus inanimate objects. Paper presented at the
15th annual meeting of the Cognitive Science
Society, Chicago, August.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. New York: Oxford University Press.
INTRODUCTION
15

This page intentionally left blank 

Part I
CAUSATION AND INTERVENTION

This page intentionally left blank 

19
1
Interventionist Theories of Causation in 
Psychological Perspective
Jim Woodward
occur, and (b) if C were not to occur, then E would
not occur. Following David Lewis, counterfactuals
are often understood in the philosophical literature in
terms of relationships among possible worlds:
Roughly, a counterfactual like (a) is true if and only
if there is a possible world in which C and E hold
that is “closer” or “more similar” to the actual world
than any possible world in which C holds and E
does not hold. A set of criteria is then specified for
assessing similarity among possible worlds (cf. Lewis,
1979, p.47).
The interventionist theory described in the next sec-
tion is a version of a counterfactual theory; the counter-
factuals in question describe what would happen to E
under interventions (idealized manipulations of) on C.
The interventionist theory does not require (although it
permits) thinking of counterfactuals in terms of possi-
ble worlds and, as noted below, the specification of
what sorts of changes count as interventions plays the
same role as the similarity metric in Lewis’s theory.
When causal information is represented by directed
graphs as in Bayes net representations, these may be given
Introduction
Broadly speaking, recent philosophical accounts of
causation may be grouped into two main approaches:
difference-making and causal process theories. The
former rely on the guiding idea that causes must make
a difference to their effects, in comparison with some
appropriately chosen alternative. Difference making
can be explicated in a variety of ways. Probabilistic the-
ories attempt to do this in terms of inequalities among
conditional probabilities: A cause must raise or at least
change the probability of its effect, conditional on
some suitable set of background conditions. When
probabilistic theories attempt to define causation in
terms of conditional probabilities, they have obvious
affinities with associative theories of causal learning
and with the use of contingency information (condi-
tional p) as a measure of causal strength (Dickinson &
Shanks, 1995). Counterfactual theories explicate dif-
ference making in terms of counterfactuals: A simple
version might hold that C causes E if and only if it is
true both that (a) if C were to occur, then E would

an interventionist interpretation (Gopnik & Shulz, 2004;
Woodward, 2003).
It is usual in the philosophical literature to con-
trast so-called type causal claims that relate one type
of event or factor to another (“Aspirin causes
headache relief”) with token or singular causal claims
that relate particular events (“Jones’s taking aspirin on
a particular occasion caused his headache to sub-
side”). There are versions of difference-making
accounts for both types of claim, although it is
arguable that such accounts apply most straightfor-
wardly to type causal claims. In contrast, causal
process accounts apply primarily to singular causal
claims. The key idea is that some particular event c
causes some other event e if and only if there is a con-
necting causal process from c to e (Salmon, 1994).
Processes in which one billiard ball collides with
another and causes it to move are paradigmatic.
There are a number of different accounts of what
constitutes a causal process, but it is perhaps fair to say
that the generic idea is that of a spatiotemporally con-
tinuous process that transmits a conserved quantity
such as energy and momentum or, as it sometimes is
described in the psychological literature, “force.”
Theorists in this tradition often deny that there is any
intimate connection between causation and difference
making; they claim that whether c causes e depends
only on whether there is a causal process connecting c
and e, something that (it is claimed) does not depend
in any way on a comparison with what happens or
would happen in some other, contrasting situation
(Bogen, 2004; Salmon, 1994). In contrast, such
comparisons are at the heart of difference-making
accounts.
Although most philosophical versions of causal
process accounts are not committed to claims about
the possibility of perceiving causal connections, an
obvious analogue in the psychological literature are
approaches that focus on launching or Michotte-type
phenomena. Psychological theories that attempt to
understand causation in terms of mechanisms or gen-
erative transmission (where these notions are not
understood along difference-making lines) are also in
broadly the same tradition.
Interventionism
Interventionist accounts take as their point of departure
the idea that causes are potentially a means for manip-
ulating their effects: If it is possible to manipulate 
a cause in the right way, then there would be an
associated change in its effect. Conversely, if under
some appropriately characterized manipulation of one
factor, there is an associated change in another, then
the first causes the second.
This idea has a number of attractive features. First,
it provides a natural account of the difference
between causal and merely correlational claims. The
claim that X is correlated with Y does not imply that
manipulating X is a way of changing Y, while the
claim that X causes Y does have this implication. And,
given the strong interest that humans and other ani-
mals have in finding ways to manipulate the world
around them, there is no mystery about why they
should care about the difference between causal and
correlational relationships. Second, a manipulationist
account of causation fits naturally with the way such
claims are understood and tested in many areas of
biology and the social and behavioral sciences and
with a substantial methodological tradition in statis-
tics, econometrics, and experimental design, which
connects causal claims to claims about the outcomes
of hypothetical experiments.
Although it is possible to provide a treatment of
token causation within a manipulability framework,1 I
focus on the general notion of one type of factor being
causally relevant (either positively or negatively) to
another. There are two more specific causal concepts
that may be seen as precifications of this more general
notion: total causation and direct causation. X is a
total cause of Y if and only if it has a nonnull total
effect on Y—that is, if and only if there is some inter-
vention on X alone (and no other variables) such that
for some values of other variables besides X, there will
be a change in the value of Y under this intervention.
Woodward (2003) argues that this notion is captured
by the conjunction of two principles (TC):
(SC) If (a) there are possible interventions (ideal
manipulations) that change the value of X such
that (b) if such an intervention (and no others)
were to occur X and Y would be correlated, then
X causes Y.
(NC) If X causes Y, then (a) there are possible
interventions that change the value of X such that
(b) if such interventions (and no other interven-
tions) were to occur, X and Y would be correlated.
Before turning to the notion of direct causation,
several clarificatory comments are in order. First, note
20
CAUSATION AND INTERVENTION

that if TC is to be even prima facie plausible, then we
need to impose restrictions on the sorts of changes in X
that count as interventions or ideal manipulations.
Consider a system in which A  atmospheric pressure
is a common cause of the reading B of a barometer and 
a variable S corresponding to the occurrence/nonoccur-
rence of a storm but in which B does not cause S or vice
versa. If we manipulate the value of B by manipulating
the value of A, then the value of S will change even
though, in contradiction to (SC), B does not cause S.
Intuitively, an experiment in which B is manipulated in
this way is a badly designed experiment for the purposes
of determining whether B causes S. We need to
formulate conditions that restrict the allowable ways of
changing B so as to rule out possibilities of this sort.
There are a number of slightly different characteri-
zations of the notion of an intervention in the
literature; including those by Spirtes, Glymour, and
Scheines (2000); Pearl (2000); and Woodward (2003).
Because the difference between these formulations will
not be important for what follows, I focus on the core
idea. This is that an intervention I on X with respect to
Y causes a change in X that is of such a character that
any change in Y (should it occur) can only come about
through the change in X and not in some other way. In
other words, we want to rule out the possibility that the
intervention on X (or anything that causes the interven-
tion) affects Y via a causal route that does not go
through X, as happens, for example, when B in the
example above is manipulated by changing the com-
mon cause A of B and S. I also assume in what follows
that the effect of an intervention on X is that X comes
entirely under the control of the intervention variable
and that other variables that previously were causally
relevant to X no longer influence it, that, as it is com-
monly put, an intervention on X, “breaks” the causal
arrows previously directed into X. In the case of the ABS
system, an intervention having these features might be
operationally realized by, for example, employing a ran-
domizing device that is causally independent of A and
B and then, depending on the output of this device,
experimentally imposing (or “setting”) B to some partic-
ular value. Under any such intervention, the value of S
will no longer be correlated with the value of B, and
(NC) will judge, correctly, that B does not cause S.
Note that, in this case, merely observing the values of B
and S that are generated by the ABS structure without
any intervention is a different matter from intervening
on B in this structure. In the former case, but not in the
latter, the values of B and S will be correlated. It is what
happens in the latter case that is diagnostic for whether
B causes S.
The difference between observation and interven-
tion thus roughly corresponds to the difference
between so-called backtracking and non-backtracking
counterfactuals in the philosophical literature. The
mark of a backtracking counterfactual is that it
involves reasoning or tracking back from an outcome
to causally prior events and then perhaps forward
again, as when one reasons that if the barometer read-
ing were low (high), then this would mean that the
atmospheric pressure would be low (high), which in
turn would mean that the storm would (would not)
occur. Evaluated in this backtracking way, the coun-
terfactual “If the barometer reading were low (high),
then the storm would (would not) occur” is true. By
contrast, when the antecedent of a counterfactual is
understood as made true by intervention, backtracking
is excluded because, as emphasized above, an inter-
vention breaks any previous existing relationship
between the variable intervened on and its causes.
Thus, when the barometer reading is set to some value
by means of an intervention, one cannot infer back
from this value to the value that the atmospheric pres-
sure must have had. For this reason, the counterfactual
“If the barometer reading were low (high), then the
storm would (would not) occur” is false when its
antecedent is regarded as made true by an intervention.
Lewis holds that non-backtracking rather than
backtracking counterfactuals are appropriate for
understanding causation, and the interventionist the-
ory yields a similar conclusion. This illustrates how, as
claimed, interventions play roughly the same role as
the similarity metric in Lewis’s theory and how they
lead, as in Lewis’s theory, to non-backtracking coun-
terfactuals, with arrow breaking having some of the
features of Lewisian miracles.2
What is the connection between this characteriza-
tion of interventions and manipulations that are
performed by human beings? I explore this issue
below, but several comments are helpful at this point.
Note first that the characterization makes no explicit
reference to human beings or their activities; instead,
the characterization is given entirely in nonanthro-
pocentric causal language. A naturally occurring
process (a “natural experiment”) that does not involve
human action at any point may thus qualify as an
intervention if it has the right causal characteristics.
Conversely, a manipulation carried out by a human
being will fail to qualify as an intervention if it lacks the
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
21

right causal characteristics, as in the example in which
the common cause A of B and S is manipulated.
Nonetheless, I think that it is plausible (see the
Interventions and Voluntary Actions section) that, as a
matter of contingent, empirical fact, many voluntary
human actions as well as many behaviors carried out
by animals do satisfy the conditions for an interven-
tion. Moreover, I also think that it is a plausible empir-
ical conjecture that humans and some other animals
have a default tendency to treat their voluntary actions
as though they satisfy the conditions for an interven-
tion and to behave, learn, and (in the case of humans)
make causal judgments as if their learning, behavior,
and judgments are guided by principles like TC. The
connection between interventions and human (and
animal) manipulation is thus important to the empiri-
cal psychology of causal judgment and learning, even
though the notion of an intervention is not defined by
reference to human action.
Second, note that both SC and NC involve coun-
terfactual claims about what would happen if certain
“possible” interventions “were” to be performed. I take
it to be uncontroversial that the human concept of
causation is one according to which causal relation-
ships may hold in circumstances in which it may
never be within the power of human beings actually to
carry out the interventions referred to in SC and NC.
(In this respect, the human concept may be different
from whatever underlies nonhuman causal cognition;
see section on primate causal cognition.) Both
conditions should be understood in a way that accom-
modates these points: What matters to whether the
relationship between X and Y is causal is not whether
an intervention is actually performed on X but rather
what would happen to Y if (perhaps contrary to actual
fact) such interventions were to be performed.
SC and NC connect the content of causal claims
to certain counterfactuals and, as such, are not claims
about how causal relationships are learned. However,
if SC and NC are correct, it would be natural to
expect that human beings often successfully learn
causal relationships by performing interventions; in
fact, this is what we find. But this is not to say (and SC
and NC do not claim) that this is the only way in
which we can learn about causal relationships.
Obviously, there are many other ways in which
humans may learn about causal relationships; these
include passive observation of statistical relationships,
instruction, and the combination of these with
background knowledge. What SC and NC imply is
that if, for example, one concludes on the basis of
purely observational evidence that smoking causes
lung cancer, this commits one to certain claims about
what would happen if certain experimental manipu-
lations of smoking were to be performed.
Finally, a brief remark about an issue that will prob-
ably be of much more concern to philosophers than to
psychologists: the worry that TC is “circular.” Because
the notion of intervention is characterized in causal
terms, it follows immediately that TC does not provide
a reductive definition of causation in terms of concepts
that are noncausal. I argue elsewhere (Woodward,
2003) that it does not follow from this observation that
TC is uninformative or viciously circular. Rather than
repeating those arguments here, let me just observe
that TC is inconsistent with many other claims made
about causation, for example, claims that causal rela-
tionships require a spatiotemporally connecting causal
process. So, regardless of what one makes of the circu-
larity of TC, it is certainly not vacuous or empty.
Let me now turn to the notion of direct causation.
Consider a causal structure in which taking birth
control pills B causally affects the incidence of throm-
bosis T via two different routes (Figure 1-1). B directly
boosts the probability of thrombosis and indirectly
lowers it by lowering the probability of an intermedi-
ate variable pregnancy P, which is a positive cause of
T (cf. Hesslow, 1976). Suppose further that the direct
causal influence of B on T is exactly canceled by the
indirect influence of B on T that is mediated through
P, so that there is no overall effect of B on T. In this
case, B is not a total cause of T because there are no
interventions on B alone that will change T.
Nonetheless, it seems clear that there is a sense in
which B is a cause, indeed a direct cause, of T.
The notion of direct causation can be captured in
an interventionist framework as follows:
(DC) A necessary and sufficient condition for X to
be a direct cause of Y with respect to some vari-
able set V is that there be a possible intervention
on X that will change Y (or the probability distri-
bution of Y) when all other variables in V besides
X and Y are held fixed at some value by other
independent interventions.
22
CAUSATION AND INTERVENTION
B
T
P
FIGURE 1-1

In the example under discussion, B counts as a direct
cause of T because, if we intervene to fix the value of P
and then, independent of this, intervene to change the
value of B, then the value of T will change. The notion
of X as a direct cause of Y is thus characterized in terms
of the response of Y to a combination of interventions,
including both interventions on X and interventions on
other variables Z. This contrasts with the notion of a
total cause, which is characterized just in terms of the
response of the effect variable to a single intervention
on the cause variable. The notion of direct causation
turns out to be normatively important because it is
required to capture ideas about distinctness of causal
mechanisms and to formulate a plausible relationship
between causation and probabilities (for details, see
Woodward, 2003, chapter 2). Of course, it is a separate
question whether the notion corresponds to anything
that is psychologically real in people’s causal judg-
ments and inferences. I suggest that it does: It is
involved in or connected to our ability to separate out
means and ends in causal reasoning. It is also centrally
involved in the whole idea of an intervention, which
turns on existence of a contrast between doing some-
thing that affects Y directly and doing something that
affects Y only indirectly, through X. We will see that
even young children are able to reason causally about
the consequences of combinations of interventions.
Finally, let me note that both TC and DC address
a specific question: Is the relationship between X and
Y causal rather than merely correlational? However, if
we are interested in manipulation and control, then
we typically want to know much more than this: We
want to know which interventions on X will change Y,
how they will change Y, and which background cir-
cumstances will cause the change—that is, we want
to know a whole family of more specific and fine-
grained interventionist counterfactuals connecting X
to Y. We may view this more detailed information,
which may be captured by such devices as specific
functional relationships linking X and Y, as the natu-
ral way of spelling out the detailed content of causal
claims within an interventionist framework. Such
information about detailed manipulability or depend-
ency relationships is often required for tasks involving
fine-grained control such as tool use.
Additional Features of Interventionism
I said that interventionist accounts are just one type of
approach in the more general family of theories that
conceive of causes as difference makers. To bring out
further what is distinctive about interventionism, con-
sider the following causal structures:
X ←Y →Z
(1-1)
X →Y →Z
(1-2)
Let us make the standard Bayes net assumption con-
necting causation and probabilities: the causal Markov
condition CM, according to which, conditional on its
direct causes, every variable is independent of every
other variable, singly or in combination, except for its
effects. Given this assumption, both structures 1-1
and 1-2 imply exactly the same conditional and
unconditional independence relationships: In both, X,
Y and Z are dependent and X and Z are independent
conditional on Y. The difference between the struc-
tures 1-1 and 1-2 shows up when we interpret the
directed edges in them as carrying implications about
what would happen if various hypothetical interven-
tions were to be performed in accordance with DC.
In particular, if structure 1-1 is the correct structure,
then under some possible intervention on Y, X and Z
will change; if structure 1-2 is the correct structure,
then Z but not X will change under an intervention
on Y. Similarly, structure 1-2 implies that, under some
intervention on X, both Y and Z will change; while
structure 1-1 implies that neither Y nor Z will change.
In general, if two causal structures differ at all, then they
will make different predictions about what will happen
under some hypothetical interventions, although, as
structures 1-1 and 1-2 illustrate, they may agree fully
about the actual patterns of correlations that will be
observed in the absence of these interventions.
Although an interventionist account does not
attempt to reduce causal claims to information about
conditional probabilities, it readily agrees that such
information can be highly relevant as evidence for dis-
criminating between competing causal structures.
Indeed, as I explain (Woodward, 2003, p. 339ff.), we
may think of CM as a condition that connects claims
about what happens under interventions to claims
about conditional probabilities involving observed out-
comes, thus allowing us to move back and forth
between the two kinds of claims. Arguably (see the sec-
tion on primate causal cognition), the ability to move
smoothly from claims about causal structure that follow
from information about the results of interventions to
claims about causal structure that are supported by
observations and vice versa is one of the distinctive
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
23

features of human causal cognition. In this connection,
there is considerable evidence that, at least in simple
cases, humans can learn causal Bayes nets from passive
observations, interventions, and combinations of the
two. Indeed, for at least some tasks the assumption that
subjects are Bayes net learners does a better job of
accounting for performance than alternative learning
theories.
I suggested above that an interventionist account
will lead to different causal judgments about particu-
lar cases than causal process accounts. Consider cases
of double prevention, in which A prevents the occur-
rence of B, which had it occurred, would have
prevented the occurrence of a third event C, with the
result that C occurs (cf. Schaffer, 2000). Cases of this
sort occur in ordinary life and are common in biolog-
ical contexts. For example, the presence A of lactose
in the environment of Escherichia coli results in the
production C of a protein that initiates transcription
of the enzyme that digests lactose by interfering with
the operation B of an agent that (in the absence of lac-
tose) prevents transcription. There is dependence of
the sort associated with interventionist counterfactuals
between whether lactose is present and the synthesis
(or lack of synthesis) of the enzyme that digests it—
manipulating whether lactose is present changes
whether the enzyme is synthesized—but no spa-
tiotemporally continuous process or transfer of
energy, momentum, or force between lactose and the
enzyme.3 Interventi-onist accounts along the lines of
TC will judge such relationships as causal; causal
process theories will not. Biological practice seems to
follow the interventionist assessment, but it would be
useful to have a more systematic experimental inves-
tigation of whether ordinary subjects regard double
prevention relationships as causal, how they assess
causal efficacy or strength in such cases, and the ease
with which such relationships can be learned.
Double prevention cases suggest that energy trans-
mission is not necessary for causal relatedness. Is it suf-
ficient? Arguably, energy transmission between two
events is sufficient for there to be some causal process
connecting the two. However, the information that
such a process is present is not tantamount to the
detailed information about dependency relationships
provided by interventionist counterfactuals. This is sug-
gested by the following example (Hitchcock, 1995).
A cue stick strikes a cue ball, which in turn strikes
the eight ball, causing it to drop into a pocket. The
stick has been coated with blue chalk dust, some of
which is transmitted to the cue ball and then to the
eight ball as a result of the collision. In this case,
energy, momentum, and force are all transmitted from
the stick to the cue ball. These quantities are also
transmitted through the patches of blue chalk that
eventually end up on the eight ball. The sequence
leading from the impact of the cue stick to the
dropping of the eight ball is a causal process, as is the
transmission of the blue chalk, and a connecting
mechanism is present throughout this sequence. The
problem is that there is nothing in all this information
that singles out the details of the way in which the cue
stick strikes the cue ball (and the linear and angular
momentum that are so communicated) rather than,
say, the sheer fact that the cue stick has struck the cue
ball in some way or other or the fact that there has
been transmission of blue chalk dust as causally rele-
vant to whether the eight ball drops. Someone might
both fully understand the abstract notion of a causal
process and be able to recognize that the process con-
necting cue stick, cue ball, and eight ball is a causal
process that transmits energy and yet not understand
how variations in the way the cue strikes the cue ball
make a difference to the subsequent motion of the
eight ball and that the transmission of the chalk dust is
irrelevant. Yet, this information, which is captured by
interventionist counterfactuals of the sort described in
TC, is crucial for manipulating whether the eight ball
drops in the pocket.4 As discussed below this observa-
tion has implications for primate causal understanding.
In general, then, an interventionist account pre-
dicts that, when information about spatiotemporal
connectedness is pitted against information about
dependency relations of the sort captured by interven-
tionist counterfactuals, the latter rather than the
former will guide causal judgment. For example, if
the relationship between C and E satisfies the condi-
tions in TC, people will judge that C causes E even if
there appears to be a spatiotemporal gap between C
and E. Moreover, even if there is a connecting spa-
tiotemporally continuous process from C to E, they
will judge that C does not cause E if the dependence
conditions in TC are not satisfied. Similarly, for the
information that something has been transmitted
from C to E; although chalk dust is transmitted to the
eight ball, subjects will not judge that its presence
causes the ball to go into the pocket because the
conditions in TC are not satisfied.
Despite these observations, adherents of an
interventionist account can readily acknowledge that
24
CAUSATION AND INTERVENTION

information about causal mechanisms, properly
understood, plays an important role in human causal
learning and understanding. However, rather than
trying to explicate the notion of a causal mechanism
in terms of notions like force, energy, or generative
transmission, interventionists will instead appeal to
interventionist counterfactuals. Simplifying greatly,
information about a mechanism connecting C to E
will typically be information about a set of depend-
ency relationships, specified by interventionist
counterfactuals, connecting C and E to intermediate
variables and the intermediate variables to one
another, perhaps structured in a characteristic spa-
tiotemporal pattern (cf. Woodward, 2002). Among
other things, such counterfactuals will specify how
interventions on intermediate variables will modify or
interfere with the overall pattern of dependence
between C and E.
As an illustration, consider Shultz’s classic 1982
monograph in which he argues that children rely
heavily on mechanism information in causal attribu-
tion. This mechanism information can be readily
reinterpreted as information about interventionist
counterfactuals. For example, in Experiment 2, sub-
jects must decide which of two different lamps is
responsible for the light projected on a wall. Here,
the relevant interventionist counterfactuals will
describe the relationship between turning on the
lamp and the appearance of a spot on the wall, the
orientation of the lamp and the position of the spot,
the effect of inserting a mirror in the path of transmis-
sion, and so on. Similarly, in the cue ball example, the
relevant mechanism will be specified in terms of the
dependence of the trajectories of the cue and eight
ball on variations in the momentum communicated
by the stick, the effect of intervening independently on
the eight ball (e.g., gluing it to the table), and so on.
On this construal, detailed information about the
operation of mechanisms is not, as is often supposed,
something different in kind from information about
dependency or manipulability relationships, under-
stood in terms of interventionist counterfactuals, but
rather simply more of the same: more detailed fine-
grained information about dependency relationships
involving intermediate variables.5 An additional advan-
tage of this way of looking at things is that it provides a
natural account of how it is possible, as it clearly is, for
people to learn that there is a causal relationship
between C and E without knowing anything about a
connecting mechanism. This is much harder to
understand if, as some mechanism-based approaches
claim, the existence of a causal relationship between C
and E just consists of the obtaining of a connecting
mechanism between C and E and the information that
C causes E consists of or implies information to the effect
that there is such a mechanism. By contrast, according
to TC, people will judge that C causes E if they are pre-
sented with evidence (e.g., from repeated experimental
manipulations) that the relevant interventionist coun-
terfactuals hold between C and E even if they have no
information about an intervening mechanism.
Philosophy and Psychology
The interconnections between philosophical and
psychological treatments of causation are complex
and intricate. Many, although by no means all, philo-
sophical accounts are (at least officially) intended as
accounts about the world rather than accounts of
anyone’s psychology, that is, as accounts of what cau-
sation is or (less ambitiously) of constraints that hold
between causal relationships, as they exist in the
world, and other worldly relationships (having to do,
e.g., with the obtaining of regularities). Nonetheless, it is
common for philosophers to move back and forth
between such worldly claims and claims that do
sound more psychological: claims about what people
mean (or ought to mean) when they make causal
claims, the evidence on which such claims are or
should be based, and so on. Even when no such
accompanying psychological story is explicitly
described, it is often implicit in or at least naturally
suggested by the ostensibly worldly account. For
example, it is natural to suppose that philosophers
who claim that causation can be reduced to facts
about conditional probabilities will also think that
human causal beliefs and representations encode
facts about conditional probabilities, and that causal
learning consists of learning facts about conditional
probabilities. Similarly, if a theorist claims, as some
adherents of causal process/mechanistic approaches
do, that whether C causes E has nothing to do with
what does or would happen to E in the absence of C,
one would not expect (at least on the face of things)
human causal judgment to represent or to be sensitive
to such information.6
Matters are further complicated, though, by the
fact that insofar as philosophical accounts of causation
have psychological implications, they are often
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
25

presented primarily as normative rather than straight-
forwardly descriptive accounts; that is, they are
presented as accounts of the causal judgments people
ought to make in various situations, how they ought to
use evidence in reaching such judgments, and so on.
I assume, however, that it is always in order to ask how
these accounts fare when taken as descriptive theories:
We may construe them as descriptive claims regardless
of the intentions of their authors. Moreover, quite
apart from its great intrinsic interest, there is an obvi-
ous motivation for proceeding in this way. Humans
and other animals engage in a remarkable amount of
successful causal learning and form many true or cor-
rect causal representations of the world. There must
be some unified story about this that is both an accu-
rate description of what they do and that enables us to
understand how what they do leads, often enough, to
normatively correct outcomes.7 Asking about the
descriptive adequacy of various normative theories is
an obvious route to this sort of understanding.
In addition, there are many other interconnections
between normative and descriptive theories. It is com-
mon for philosophers to appeal both to claims about
the causal judgments that ordinary people or experts
will make in particular cases and to claims about the
types of considerations on which those judgments are
based to motivate the particular theories they favor. It
is also common for philosophers to make claims
about how people’s causal judgments connect with or
fail to connect with various other concepts and pat-
terns of reasoning, such as the use of counterfactuals,
to motivate particular approaches. Claims of this sort
are of course descriptive claims about the empirical
psychology of causal inference and judgment and
should be evaluated accordingly. In addition,
although adherents of a normative theory always have
the option, in any particular case, of responding to
evidence that subjects do not in fact reason and judge
in the way that theory says they should, by saying that
such subjects are subject to processing limitations, or
are confused, extensive and fundamental divergence
between normative prescriptions and actual behavior
is often plausibly regarded as at least a prima facie
problem for a normative theory—a problem that the
normative theory needs to address rather than ignore.
In the spirit of these remarks, I explore, in the remain-
der of this chapter, some issues concerning the empir-
ical plausibility of interventionist accounts and their
philosophical rivals as descriptions of human and
nonhuman causal inference and judgment.
Instrumental Learning
A useful point of departure is the difference between
classical or Pavlovian conditioning and instrumental
or operant conditioning. In classical conditioning, a
subject learns an association between two events that
are outside its control (e.g., an association between
the ringing of a bell and the provision of food). The
subject is thus in the position of learning through pas-
sive observation rather than active intervention, and
what is learned is that one stimulus predicts another,
where this predictive relationship may or may not
reflect the fact that the first stimulus causes the sec-
ond. By way of contrast, in instrumental conditioning
what is learned is an association between some behav-
ior produced by the subject and an outcome, as when
rats learn an association between pressing a lever and
the provision of a food pellet.
From an interventionist perspective, instrumental
learning has a “causelike” flavor. An organism that
was incapable of acting on the world and could only
passively observe associations outside its control
would have no need for a notion of causation or
causelike representations, conceived along interven-
tionist lines. Such an organism might still find it
useful to predict what will happen, but sensitivity to
correlations and to temporal relationships, rather than
to anything distinctively causal, would suffice for this
purpose. Given a correlation between two variables X
and Y, it would not matter how the correlation
arises—whether because (a) X causes Y or because
(b) X and Y have a common cause—as long as the
correlation is stable and projectable. The difference
between (a) and (b) begins to matter when the animal
is interested in whether changing X is a way of
changing Y.
It is thus of considerable interest that there are
striking, if incomplete, parallels between instrumen-
tal conditioning in nonhuman animals and causal
learning and judgment in humans, a theme that has
been systematically explored by Dickinson, Shanks,
and others in a series of papers (Dickinson &
Balleine, 2000; Dickinson & Shanks, 1995). Both
instrumental learning by rats and human judgments
of causal strength (as expressed in verbal reports) in
instrumental learning tasks exhibit a similar sensitivity
to temporal delay between action and outcome. Both
rat behavior and human causal judgment are (inde-
pendently of temporal relations) highly sensitive to
the contingency p between action A and outcome
26
CAUSATION AND INTERVENTION

O, that is, to P(O/A) P(O/A). Although there are
important qualifications, both human judgments of
causal strength and the rate of lever pressing for rats
tend to decline as p approaches zero. In addition,
in both humans and rats, learning of instrumental
contingencies has a number of other features that give
it a causal flavor; for example, both exhibit backward
blocking, and both rat behavior and human causal
judgment are subject to a discounting or signaling
effect in which the usual reaction of nonresponse to a
noncontingent reward schedule does not occur when
rewards that are not paired with the instrumental
action are preceded by a brief visual signal. As
Dickinson and Balleine remark, “the intuitive expla-
nation [of this effect] is that the signal marks the
presence of a potential cause of the unpaired out-
comes, thereby discounting these outcomes in the
evaluation of control exerted by the instrumental
action” (2000, p. 192).
These results suggest that both instrumental learn-
ing in rats and human judgments of causal strength
(as well as actions based on this) behave as though
they track the perceived degree of control or manipu-
lative efficacy of the instrumental action over the
outcome, which is what one would expect on an
interventionist account on causation. In addition,
phenomena such as sensitivity to contingency, back-
ward blocking, and causal discounting show that at
least some causal representation and judgment are
sensitive not only to information about the rates of
occurrence of cause and effect and the processes that
connect them but also to information about what
would or does happen in the absence of the cause and
under the occurrence of potential alternative causes of
the effect.8 This is contrary to what some (psycholo-
gized) versions of causal process/mechanism theories
seem to imply.
Causal Judgment and Interventionist
Counterfactuals
I noted that interventionist theories are just one species
of the more general category of difference-making the-
ories. The sensitivity of causal judgment to contin-
gency information is consistent both with various
versions of probabilistic theories of causation and with
theories that appeal to interventionist counterfactuals.
Is there evidence that specifically favors intervention-
ism as a descriptive account of causal judgment, at least
in humans?
Let me begin with the issue of the relationship
between causal and counterfactual judgments.
Although, as noted, there are influential philosophical
theories such as those of Lewis (1973) that connect
causal claims to counterfactuals, many philosophers
continue to regard counterfactuals in general (and a
fortiori, their use in a theory of causation) with great
skepticism. It is contended that counterfactuals are
unclear, untestable, unscientific, and in various ways
unnatural and artificial in the sense that they are
philosophical inventions that correspond to nothing
in the way ordinary people actually think and reason.
In fact, there is considerable evidence that people
employ counterfactuals extensively in various forms of
ordinary reasoning, and that they connect causal
claims and counterfactuals in something like the way
that interventionist and counterfactual theories sug-
gest.9 Since the relevant literature is vast, I focus, for
illustrative purposes, on a charming set of experi-
ments involving young children described by Harris
(2000). Harris presented children aged 3–4 years with
a number of scenarios that probed the way in which
they connected causal and counterfactual judgments.
He found, for example, that when children were pre-
sented with a causal sequence (Carol walks across the
floor in her muddy shoes and makes the floor dirty)
and then asked counterfactual questions about what
would have happened under different possible
antecedents (what would have happened if Carol had
taken her shoes off?), a large majority gave correct
answers (that is, answers that respect the intuitive con-
nection between causal and counterfactual claims).
They were also able to discriminate correctly between
counterfactual alterations in the scenario that would
have led to the same and to different outcomes, that
is, which alterations in behavior would have avoided
mud on the floor and which would not.
Children not only connect causal and counterfac-
tual claims when explicitly prompted to do so by a
question about what would happen under a counter-
factual possibility, but also when asked why an
outcome occurred or how it might have been pre-
vented. For example, in a scenario in which Sally has a
choice between drawing with a pen and drawing with a
pencil, chooses the pen, and gets ink on her fingers,
children who are asked why Sally’s fingers got inky
motivate the causal role of the pen by appealing to
what would have happened if she had instead used the
pencil. Indeed, children spontaneously invoke what
would have happened under alternative possibilities 
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
27

in arriving at causal judgments even when those
alternatives are not explicitly mentioned in or
prompted by the scenarios. Harris’s (2000) conclusion
is that “counterfactual thinking comes readily to very
young children and is deployed in their causal analysis
of an outcome” (p. 136).
This conclusion may seem surprising if one is
accustomed, as many philosophers are, to thinking of
counterfactuals as primarily having to do with Lewis-
style similarity relationships on possible worlds and
similar metaphysical arcana. Clearly, small children
(and for that matter most adults) do not have anything
remotely like Lewis’s framework explicitly in mind
when they use counterfactual reasoning. But, what-
ever one’s assessment of Lewis’s theory, it is important
to bear in mind that one of the main everyday uses of
counterfactual and causal thinking, by both children
and adults, is in planning and in anticipating what the
consequences of various possible courses of action
would be (without necessarily performing the
actions in question). This is a perfectly ordinary, nat-
ural, practically useful activity and (relevantly, to our
story) one that even small children appear to be
much better at than nonhuman primates. Children
engage in such planning involving counterfactuals
and causal claims on an everyday basis when they
reason, for example, that if they want to avoid getting
their fingers inky they should use a pencil rather
than a pen, that using a pen with blue ink rather
than black ink will not avoid the outcome, and so
on. If we think of counterfactuals of this sort, used
for this purpose (notice, by the way, that the above
counterfactuals are all interventionist counterfactu-
als), then we should be able to see that there is
nothing particularly problematic or obscure about
them.
Turning now specifically to the notion of an
intervention, a natural worry is that this notion is too
complex and cognitively sophisticated to be psycholog-
ically realistic. In assessing this worry, we need to
distinguish two issues:
1. Do most people consciously or explicitly
represent to themselves the full technical defi-
nition of a normatively appropriate notion of
intervention when they engage in causal
reasoning?
2. Do people learn and reason in accord with the
normative requirements of the interventionist
account?
I assume that the answer to Question 1 is almost cer-
tainly no for most people without special training. On
the other hand, there is considerable evidence that
the answer to Question 2 is yes, for many people at
least some of the time.
To begin, there is evidence that, in a substantial
range of situations, adults learn causal relationships
more reliably and quickly when they are able to per-
form interventions than when they must rely entirely
on passive observations (Lagnado & Sloman, 2004;
Sobel & Kushnir, 2006).10 This true for infants as well;
Jessica Sommerville (chapter 3, this volume) reports a
series of experiments that show that infants who
actively intervene, for example, to obtain a toy by
pulling a cloth on which it rests learn to distinguish
relevant causal relationships between the cloth and toy
(presence of spatiotemporal contact, etc.) more readily
than those who rely on passive looking. Moreover, in
at least some situations a significant number of sub-
jects (although by no means all) intervene optimally
when given a choice among which interventions to
perform, choosing those interventions that are maxi-
mally informative. For example, when presented
with a scenario in which there are several possible can-
didates for the correct causal structure, one of which is
a chain structure in which X causes Y which causes Z,
people choose to intervene on the more diagnostic
intermediate variable Y rather than on X or Z
(Steyvers, Tenenbaum, Wagenmakers, & Blum,
2003). This suggests some appreciation of the connec-
tion between intervention and causal structure.
A similar conclusion is suggested by a series of
experiments by Lagnado and Sloman (2005). They
report the following:
1. Subjects are told that billiard ball 1 causes ball 2
to move, which causes ball 3 to move. Almost all
judge that if ball 2 were unable to move, then
ball 1 would still have moved, and that billiard
ball 3 would not have. On other hand, when
presented with a parallel scenario involving
conditionals that lack an obvious causal
interpretation and are of the form if p then q,
if q then r, subjects’ responses are far more
variable, with a considerable number willing
to infer not p from the information than not q.
In another words, most subjects endorse the
non-backtracking counterfactuals associated
with interventionist accounts in the causal
scenario but respond differently to noncausal
conditionals, for which a considerable number
28
CAUSATION AND INTERVENTION

do endorse a backtracking, noninterventionist
interpretation.
2. Subjects are presented with a chain structure in
which they are told that A causes B, which
causes C. They are then told either (a) someone
intervened directly on B, preventing it from
happening or (b) we observe that B did not hap-
pen. Again, consistent with the interventionist
account, subjects treat the intervention condi-
tion (a) differently from the observation condi-
tion (b). For example, they judge that the
probability of A is higher in the intervention
condition than in the observation condition;
that is, they do not backtrack in the former and
are more likely to in the latter.
These and other experiments involving more com-
plex causal structures suggest that subjects do indeed
distinguish between observing and intervening in the
way that the interventionist account says they should,
that in at least some situations they interpret an
intervention in an arrow-breaking way, and that they
associate interventionist non-backtracking counterfac-
tuals with causal claims and employ them in contexts
in which a causal interpretation is natural or a reason-
able default, while being at least somewhat more
inclined to use non-backtracking counterfactuals in
contexts that are obviously noncausal. These results
seem inconsistent with claims (e.g., Bennett, 1984)
in the philosophical literature that people either do
not distinguish at all between backtracking and non-
backtracking counterfactuals or do not preferentially
employ the latter in contexts involving causal rea-
soning. In addition, the experiments provide addi-
tional evidence (if any is needed) that subjects are
indeed able to engage in sophisticated normatively
appropriate counterfactual reasoning regarding causal
situations.
Interventions and Voluntary Actions
I noted that in many situations people make more reli-
able causal inferences when they are able to intervene.
From a design viewpoint, one thus might expect that
subjects will have more confidence in causal infer-
ences and judgments that are directly associated with
their interventions and perhaps that some of these
inferences will be fairly automatic. This suggests the
following hypothesis: Human beings (and perhaps
some animals) have (a) a default tendency to behave
or reason as though they take their own voluntary
actions to have the characteristics of interventions and
(b) associated with this a strong tendency to take
changes that temporally follow those interventions
(presumably with a relatively short delay) as caused by
them.11 Voluntary here means nothing metaphysically
fancy, just the commonsense distinction between
deliberately pouring the milk in one’s coffee and
spilling it accidentally.
I noted that it is not psychologically realistic to
suppose that most people operate with an explicit rep-
resentation of the full technical definition of the
notion of an intervention. Taken together, (a) and (b)
suggest one way in which it is nonetheless possible for
such subjects to use their interventions (note: not
their explicit concepts of intervention) to reach fairly
reliable causal conclusions in a way that respects prin-
ciples like (TC). For an account along these lines to
work, several things must be true. First, subjects must
have some way of determining (some signal that tells
them) when they have performed a voluntary action,
and this signal must be somewhat reliable, at least in
ordinary circumstances. Second, voluntary actions
(again in ordinary, ecologically realistic circum-
stances) must—not always, but often enough—have
the characteristics of an intervention.
I suggest that both claims are true. First, human
subjects do have a characteristic phenomenology
associated with voluntary action; they typically have a
sense of agency or ownership of their behavior that is
not present when they act involuntarily.12 This is not
surprising: Presumably, it is important for humans
and other animals to have some way of distinguishing
those cases in which a change occurs in their environ-
ments or in their bodies that results from their
voluntary actions from those cases in which the
change comes about in some other way—not as a
result of a movement of their bodies at all or as a result
of a movement that is nonvoluntary. It is plausible
that one role for the feeling of ownership of one’s
action is to provide information that helps organisms
to monitor this distinction. Once this feeling is avail-
able, it may be used for many purposes, including
causal inference.
Turning now to the status of (b), it is clear that the
correlation between voluntariness and satisfaction of
the conditions for an intervention is imperfect. In a
badly designed clinical trial, an experimenter might
be subconsciously influenced, in decisions to give a
drug to some patients and withhold it from others, by
the health of the patients; his decisions are voluntary
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
29

and yet correlated with an independent cause of
recovery in a way that means that the conditions for
an intervention are not satisfied. Nonetheless, it
seems plausible that many voluntary actions do, as a
matter of empirical fact, satisfy the conditions for an
intervention. If I come on a wall switch in an unfamil-
iar house and find that there is a regular association
between my flipping the position of the switch and
whether a certain overhead light is on or off, then
often enough flippings will satisfy the conditions for
an intervention on the position of the switch with
respect to the state of the light. Similarly for a baby
whose leg is attached by a string to a mobile and who
observes a correlation between leg movements and
the motion of the mobile. In both cases, subjects who
are guided by (a) and (b) will make fairly reliable
causal inferences. The existence of causal illusions in
which we experience or “perceive” salient changes
that follow our voluntary actions as caused by them
similarly suggests that such a heuristic is at work.13
Going further, it might be conjectured that involun-
tary behavior is less likely to meet the conditions for
an intervention.14 If this is so, then one might expect
that the impression of causal efficacy for outcomes
following such behavior should be attenuated.
Premack and Premack (2003) report that this is the
case, although more systematic experimental investi-
gation would be desirable.
Primate Causal Cognition
Despite the abilities of nonhuman animals in instru-
mental learning tasks and the similarities between
animal instrumental and human causal learning
described, it is a striking fact that nonhuman animals,
including primates, are greatly inferior to humans,
including small children, at many tasks involving
causal learning, especially those involving tool use,
object manipulation, and an understanding of “folk
physics.” This is so despite the fact that nonhuman pri-
mates and many other mammals have capacities on
object permanence and trajectory completion tasks
(capacities that are often taken to demonstrate the pos-
session of “causal” concepts in the psychological
literature) that that are apparently not so very different
from those possessed by human children and adults.
This suggests that although these various abilities may
well be necessary for the acquisition of the causal
learning abilities and understanding possessed by
human beings, they are not sufficient. Can an inter-
ventionist perspective cast light on what more is
involved?
In approaching this question, let me begin by
briefly describing some representative experimental
results involving nonhuman primates. In experiments
conducted by Kohler and subsequently repeated by
others, apes (including chimps, orangutans, and goril-
las) were presented with problems that required
stacking several boxes on top of each other to reach a
food reward. In comparison with humans, including
children, the apes had great difficulty. They behaved as
though they had no understanding of the physical prin-
ciples underlying the balancing of the boxes and the
achievement of structures capable of providing stable
support; as Kohler put it, they had “practically no stat-
ics” (Kohler, 1927, p. 149, quoted in Povinelli, 2000,
p. 79). The structures they succeeded in building, after
considerable trial and error, were highly unstable, and
completely neglected center of gravity considerations,
with boxes at an upper level extending in a haphazard
way far over the edges of lower-level boxes. Subjects
even on occasion removed lower-level boxes from
beneath boxes they supported. Errors of this sort were
made repeatedly, suggesting what from a human per-
spective would be described as complete lack of insight
into the principles governing the construction of stable
structures. When stable structures were achieved, this
appeared to be the result of trial-and-error learning.
There was little evidence that the apes were able to rea-
son hypothetically about what would happen if they
were to create this or that structure, without actually
creating the structures in question, and then use this
reasoning to guide their actions in the way that, for
example, the children in Harris’s experiments were
able to reason.
In another series of experiments, conducted by
Visalberghi and Trinca (1989), a desirable food item
was placed in a transparent hollow tube, and the ani-
mals were given various tools that might be used to
push it out. Both apes and monkeys were able to solve
some variants of this problem. For example, when
given a bundle of sticks that was too thick to fit into
the tube, they unbundled the sticks and used appro-
priate size sticks to dislodge the food item. On the
other hand, they also frequently behaved as though
they lacked a real understanding of the causal struc-
ture of the task. For example, they inserted sticks that
were too short to reach the reward when a stick of
appropriate length was available. They attempted to
30
CAUSATION AND INTERVENTION

use sticks with cross pieces that blocked insertion into
the tube. They also inserted nonrigid objects like tape
that were incapable of displacing the food. In still
other experiments, the animals failed to choose
implements with a hook at the end, which would
have been effective in retrieving desired objects,
instead of straight sticks, which were not.
Povinelli’s summary is that the animals “appear to
understand very little about why their successful
actions are effective” (2000, p. 104). In particular,
they appeared not to understand the significance of
the mechanical properties of the systems they were
dealing with—properties such as weight, rigidity,
shape, center of mass, and so on. Instead, as both
Povinelli (2000) and Call and Tomasello (1997)
remark, they often acted as though (any) spatiotempo-
ral contact between the target object they wished to
manipulate and the means employed was sufficient to
achieve the desired manipulation.
Both Povinelli (2000) and Call and Tomasello
(1997) go on to suggest a more general characteriza-
tion of the deficits exhibited in the experiments: They
claim that these stem from the animals’ lack of various
abstract concepts having to do with “unobservables”
(Povinelli, 2000, p. 300, mentions gravity, force,
shape, and mass, among others) that humans think of
as mediating causal relationships. In contrast to
humans, apes operate entirely within a framework of
properties that can be readily perceived, and this
underlies their lack of causal understanding.
Philosophers of science are likely to find this invo-
cation of unobservables puzzling. If we think of a
property as observable for a subject as long as the sub-
ject can reliably discriminate whether it is present (or
among different values if the property is quantitative)
by perceptual means, then it seems implausible that
properties like weight and shape are literally unob-
servable by apes—presumably, apes can be trained to
discriminate reliably between objects of different
shapes or weights. There is, however, an alternative
way of understanding this claim that makes it seem far
more plausible.15
Suppose that when an ape learns to discriminate
among objects according to (what we would call)
weight, the discrimination is made on the basis of
sensory feedback and bodily sensations associated
with differential effort in lifting. If apes’ “concept” of
weight is closely linked to these bodily sensations,
then it becomes more understandable why they are
apparently unable to make use of information about
weight in other sorts of contexts requiring causal
reasoning—why, for example, they are unable to rec-
ognize the relevance of weight to support relation-
ships. To recognize the relevance of weight to these
contexts requires possession of a more abstract way of
thinking about weight that is not so closely tied to
sensory and motor experience. Similarly for properties
like rigidity.
In this way of thinking about the matter, the apes
(in comparison with humans) operate with the wrong
variables to enable them to engage in the kind of
sophisticated causal learning required for the tasks
described above; their variables are too closely linked
to egocentric sensory experience. From the perspec-
tive of the interventionist account, we might describe
this as a situation in which certain interventionist
counterfactuals cannot be learned by the apes
because the variables in terms of which those counter-
factuals are framed are unavailable to the apes. For
example, apes are unable to learn the appropriate
interventionist counterfactuals involving the human
concept of weight because they lack that concept.
Whether this analysis is accepted, it seems clear, as a
more general point, that whatever the apes’ grasp of
notions like weight and rigidity, they do not under-
stand their causal relevance to the tasks with which
they are dealing and cannot integrate these notions
into causal representations that successfully guide
action in connection with those tasks.
As I see it, this sort of limitation in the apes’ under-
standing is not just a matter of their failure to grasp
the abstract notion of a causal process (as a process
that transmits force, energy, etc.) or an inability to rec-
ognize particular instances of such a process in the
system of interest. As noted in the section entitled
Additional Features of Interventionism, grasp notion of
a causal process is not sufficient for the sort of detailed
knowledge of dependency relationships that is
required for successful manipulation in tasks like bal-
ancing boxes or extracting food from a tube. What
needs to be explained is the apes’ lack of this latter sort
of knowledge.
Whenever a primate moves a food source with a
stick—whether the food is pushed in an appropriate
or inappropriate direction or with an appropriate
instrument—there will be transmission of force and
energy, the presence of a mechanism, and so on. A
creature that possessed the concept of force and “gen-
erative transmission” (and could recognize when force
was transmitted) and whose heuristic was: “to cause a
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
31

desired outcome, transmit force to the outcome (or
the object associated with the outcome) or set in oper-
ation a generative mechanism connected to the out-
come” would not get useful guidance from this
heuristic about exactly what it should do to balance
boxes in the stacking task or to expel food from the
tube.16 To accomplish this, far more specific informa-
tion about how the outcome that the agent wishes to
affect depends on variation in other factors (perhaps
including factors that are not linked too closely to
egocentric sensory experience) that the agent is able
to control is required, where these include factors not
linked too closely to egocentric sensory experience.
Thus, in the tube experiment the subject must 
recognize the relevance of the dimensions and rigid-
ity of the implement chosen and so on. This looks
far more like information of the sort represented 
by TC and DC than information about force trans-
mission.
The idea that the apes lack the right variables (and
hence cannot grasp counterfactual dependency rela-
tionships based on those variables) gives us one way of
explaining at least some of their deficits in causal
understanding. An alternative line of argument,
which I see as complimentary to and not in competi-
tion with the “wrong variables” analysis and which
also fits naturally into an interventionist framework,
focuses on Tomasello and Call’s notion of a tertiary
relationship (1997, especially pp. 367–400). A rela-
tionship qualifies as tertiary for a subject if the
relationship is understood or recognized as holding
between objects and individuals that are independent
of the subject. This contrasts with relationships that
are (or are conceived as) more directly egocentric in
the sense of holding between the subject and some
other object or individual. Clearly, the ability to rec-
ognize and reason in terms of tertiary relations is
closely related to the ability to think in an abstract or
context-independent way. Tomasello and Call suggest
that all primates (or at least all simians) have the abil-
ity to form and understand concepts of tertiary
relationships in both social and physical domains.
For example, primates seem to possess concepts of
tertiary social relationships between conspecifics,
such as the concept of one animal outranking
another in a dominance hierarchy (as opposed to the
notion of the nontertiary relationship of this animal
outranking me).
This suggests the following question: Do primates
understand (or at least behave in accordance with a
conception of) causation as a tertiary relationship? As
argued in the section on interventionism, the human
concept of causation is clearly a concept of a tertiary
relationship. Although people think of causal rela-
tionships as relationships that they may be able to
exploit for purposes of manipulation and control, they
also conceive of causal relationships as relationships
that can exist in nature independently of their (or,
indeed, any agent’s) manipulative activities. Thinking
along these lines suggests the usefulness of distin-
guishing among the following possibilities or “levels”
of causal/instrumental understanding:
1. An agent whose instrumental behavior and
learning is purely egocentric. That is, the agent
grasps (or behaves as if it grasps) that there are
regular, stable relationships between its manipu-
lations and various downstream effects but stops
at this point, not recognizing (or behaving as
though it recognizes) that the same relationship
can be present even when it does not act, but
other agents act similarly or when a similar rela-
tionship occurs in nature without the involve-
ment of any agents at all.
2. An agent with an agent causal viewpoint: The
agent grasps that the same relationship that it
exploits in intervening also can be present when
other agents act.
3. An agent with a fully causal viewpoint: The
agent grasps that the same relationship that the
agent exploits in intervening also can be present
both when other agents intervene and in nature
even when no other agents are involved. This
involves thinking of causation as a tertiary rela-
tionship.
Tomasello and Call (1997) suggest that nonhu-
man primates do not operate with this tertiary, Stage
3 conception of causation but rather with something
closer to what I take to be the egocentric conception
described in Stage 1 (cf. Figure 1-2):
We are not convinced that apes need to be using a
concept of causality in the experimental tasks
purporting to illustrate its use, at least not in the
humanlike sense of one independent event forc-
ing another to occur. More convincing would be
a situation in which an individual observes a
contiguity of two events, infers a cause as interme-
diary, and then finds a novel way to manipulate
that cause. For example, suppose that an individ-
ual ape, who has never before observed such an
32
CAUSATION AND INTERVENTION

event, for the first time observes the wind blowing
a tree such that the fruit falls to the ground. If it
understands the causal relations involved, that the
movement of the limb is what caused the fruit to
fall, it should be able to devise other ways to make
the limb move and so make the fruit fall. . . . We
believe that most primatologists would be
astounded to see the ape, just on the bases of hav-
ing observed the wind make fruit fall, proceed to
shake a limb, or pull an attached vine, to create
the same movement of the limb. Again, the prob-
lem is that the wind is completely independent 
of the observing individual and so causal analysis
would have to proceed without references to the
organism’s own behavior and the feedback it might
receive from that (thus, it might be able to learn to
shake the limb if its own movements had previ-
ously led to a limb shaking and the fruit falling as
a result). Moreover, performing some novel behav-
ior to make the fruit fall would involve an even
deeper causal analysis of the web of possible ways
that the cause could be repeated so as to reinstate
the desired effect. (p. 389)
Although some commentators (e.g., Povinelli,
2000) are skeptical, I think that these remarks help to
capture some important features of the limitations
exhibited in the primate experiments described
above.17 In what follows I want to develop some of the
implications of this line of thought in more detail.
First, note that the transitions from Level 1 to Level
3 are important in part because they correspond to pro-
gressively stronger forms of instrumental/causal learning.
For example, if I am a creature who thinks only in terms
of instrumental relationships that connect my own
actions to outcomes (Level 1) and not in terms of Levels
2 and 3, then the relevance of observations concerning
what happens under the interventions of others will be
unclear to me. Suppose that I do X and observe that Y
ensues, and that I have the ability to learn, from
repeated experiences of this sort, that (usually or often)
when I do X, Y regularly ensues. Clearly, it is logically
possible that I might have this ability and yet not be able
to learn or recognize that when another actor does X, Y
ensues, then this is evidence that if I were to do X, then
Y would ensue. Similarly, I may have the Level 1 learn-
ing ability just described and not be able to recognize
that there are relationships that occur in nature in the
absence of human or animal intervention that are such
that I could make use of those very relationships for pur-
poses of manipulation. Associated with this, I may not
be able to learn from observing naturally occurring
events that these instantiate relationships that I might
make use of for purposes of manipulation. In short, in
Level 1, the only way I learn about a manipulative rela-
tionship is if I perform the relevant manipulation. I take
Call Tomasello (1997) to be suggesting that this is not
only a logical possibility but also that something like this
is true for nonhuman primates.
This line of thought suggests that susceptibility to
instrumental conditioning shows only that an animal
is capable of learning instrumental relationships in
the sense of Level 1; it does not in itself show that the
animal is capable of understanding or appreciating
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
33
FIGURE 1-2

causal relationships in Sense 3 or the forms of learning
associated with it. What would go some way toward
establishing the latter would be evidence of transfer-
ence between operant and classical conditioning.
Suppose that C is some outcome that an animal
knows how to produce, and that the animal learns
that C is associated with E just via passive observation
or classical conditioning, where E is an outcome the
animal wants. Will the animal spontaneously pro-
duce C (without extensive trial-and-error learning) to
get E once it is given the opportunity to intervene? If
the animal learns in an instrumental conditioning
task that producing C is followed by E, then will the
animal expect (or quickly learn to expect) E when it
merely observes but does not produce C? Although
there is some controversy surrounding this issue, the
consensus seems to be that there is relatively little
transfer back and forth between instrumental and
classical conditioning.18 This is consistent with the
claims of Call and Tomasello (1997) about the
inability of nonhuman primates to learn instrumental
relationships from passive observation of causal rela-
tionships occurring in nature. If correct, then such
claims do indeed suggest that the representations and
abilities that underlie nonhuman instrumental
learning are not fully causal in the human, Level 3
sense, even though, as indicated, they have many fea-
tures in common with human causal learning and
representation.
There is another aspect of the contrast between
Stage 1 and Stage 3 that is worth underscoring. An ani-
mal that possesses only Stage 1 information is in effect
in the position of possessing fused action-outcome rep-
resentations and behavior patterns: representations
that its behaving in a certain way produces such and
such a desired outcome or goal. This need not involve
any appreciation of causal relationships among vari-
ables that are intermediate between the behavior and
achievement of the goal. It thus falls well short of
what might be thought of as full-fledged means-ends
understanding of how the goal might be achieved.
This last does involve the postulation of intermediate
causal links or what I take to be the same thing, some
appreciation of the contrast between direct and more
indirect causal relationships.
In particular, means-ends understanding seems to
involve a decomposition of a task into an intermediate
outcome O that can be produced fairly directly by
the subject’s action A and a further outcome O′ that is
more directly caused by O and less directly by A and
where the link between O and O′ is a tertiary link
between events rather than an action-event link. In Call
and Tomasello’s diagram (Figure 1-2), this intermediate
outcome is described by the variable limb shakes, and
this in turn causes the outcome described by fruit falls.
Note that this causal relationship holds between events
that are not manipulations by the animal.
As Call and Tomasello (1997) suggest, and is appar-
ent from their diagram, it is the introduction of the
intermediate variable that makes possible (or corre-
sponds to) the recognition that there are different ways
(involving both actions and events occurring in
nature) in which the same goal (fruit falls) might be
brought about, all of which have in common the fact
that they operate through the intermediate variable
limb shaking.19 In general, the postulation of the inter-
mediate link (and with it an appreciation that causal
relationships can be more or less direct) goes hand in
hand with a decoupling of sought-after final outcomes
and the means used to achieve them and a focus on
the latter as a separate entity.
As Call and Tomasello (1997) and Tomasello
(1999) argue, this decoupling is closely linked to
learning through imitation, that is, through observing
the interventions of others. The issue of whether
nonhuman primates ever learn through genuine
imitation, as opposed to such other possibilities as
emulation learning, is a complex and controversial
one involving, among other things, disputes over how
to best characterize imitation and issues about the
theory of mind skills required for this activity.
However, it seems uncontroversial that, in comparison
with humans, including young children, nonhuman
animals, including primates, are much inferior at
learning means-ends relationships and appropriate
tool use by observing the manipulations of other con-
specifics. It also seems uncontroversial that, whatever
else is required for successful imitation, the ability to
perform the kind of means-ends (goal) decomposition
described by Call and Tomasello is essential.
One reason for thinking this is that if imitation is to
be successful, then it will often not involve the exact
copying of another animal’s behavior, if only because
the copier (particularly if a juvenile) may differ from
the target in size, strength, and other relevant charac-
teristics. The successful imitator must be able, as Call
and Tomasello (1997) say, to separate the overall goal
of the imitation from the particular means employed,
viewing the latter as an independent step, and be able
to copy the means at something more like a functional
34
CAUSATION AND INTERVENTION

level—that is, in a way that reproduces those of its
causal characteristics that are essential to produce the
goal—while at the same time varying other features to
accommodate differences between the targets and imi-
tators situation and abilities. This suggests (here, I take
myself to be following Call and Tomasello) that we
should expect to find the following abilities occurring
together: ability to imitate activities that have a means-
ends structure, ability to learn about complex causal
structures through combinations of interventions that
reveal direct versus indirect causal relationships, abil-
ity to learn about causal relationships by observing the
interventions of others, a conception of causation
according to which it is a tertiary relationship, and
associated with this an ability to use information
learned about causal relationships through passive
observation to guide interventions and vice versa. To a
substantial extent, these abilities seem to be unique to
humans, with nonhuman animals having abilities (a
capacity for instrumental conditioning, ability to learn
action outcome sequences, etc.) that have more of a
Stage 1 feel to them.
Because a number of the experiments that most
clearly show that even small children have these abil-
ities have been performed by Gopnik, Schulz, and
others (Gopnik & Schulz, 2004; Gopnik et al., 2004)
and are described elsewhere in this volume, I confine
the discussion to a brief overview, emphasizing gen-
eral connections with the interventionist approach.
First, young children learn not only the causal conse-
quences of their single interventions but also, more
interestingly, other causal relationships from combi-
nations of interventions performed by others. They
learn in conformity with a conditional intervention
principle that is essentially just the definition of
direct cause (DC). Moreover, they do this in contexts
in which information about generative mechanisms,
the transmission of force, and spatiotemporal clues
cannot be used to identify the correct causal struc-
ture. For example, when confronted with a device
with two interlocked gears A and B that move
together, which may also be influenced by the posi-
tion of a switch and which is such that the gears are
removable only when the switch is off, the children
are able to infer correctly that the motion of A causes
B to move (when the switch is on), and that the
motion of B does not cause A to move, not on the basis
of intervening on A and observing the motion of B, but
rather on the basis of information about what happens
to A (B) when the switch is first turned off, B (A) is
removed, and then the switch is turned on. In effect,
this operation shows that the switch does not directly
influence B without going through A, while the switch
influences A even with B fixed at the value “removed.”
Moreover, children can also acquire knowledge of
causal structure from information about conditional
probabilities and then use this information to predict
the outcomes of new interventions or to produce new
interventions that are appropriate for desired goals.
That is, they can transfer or move back and forth
between observational and intervention-based causal
learning in a way that nonhuman animals apparently
cannot.
The important role that learning from the interven-
tions of others appears to play in the development of
human causal understanding suggests that two abilities
often regarded as rather different—the social cognition
abilities involved in imitation and causal understand-
ing of the nonsocial world—may be closely inter-
twined.20 There is also independent evidence that
young children are motivated to pay particular attention
to the actions of other humans, and that they have prim-
itive imitative or simulative abilities for parsing and copy-
ing the actions of other humans. One might speculate
that these attentional biases and abilities (which seem to
be specific to humans in some respects) are combined
with instrumental learning abilities that are shared with
nonhuman animals to enable the much stronger forms
of causal learning exhibited by humans.21
References
Bennett, J. (1984). Counterfactuals and temporal direc-
tion. Philosophical Review, 93, 57–91.
Bogen, J. (2004). Analysing causality: The opposite of
counterfactual is factual. International Studies in
the Philosophy of Science, 18, 3–26.
Call, J., & Tomasello, M. (1997). Primate cognition. New
York: Oxford University Press.
Dickinson, A., & Balleine, B. (2000). Causal cognition
and goal directed action. In C. Heyes & L. Huber
(Eds.), The evolution of cognition. Cambridge, MA:
MIT Press, 185–204.
Dickinson, A., & Shanks, D. (1995). Instrumental action
and causal representation. In D. Sperber, 
D. Premack, & A. Premack (Eds.), Causal cogni-
tion. Oxford, England: Oxford University Press.
Glymour, C. (1998). Learning causes: Psychological
explanations of causal explanation. Minds and
Machines, 8, 39–60.
INTERVENTIONIST THEORIES OF CAUSATION IN PSYCHOLOGICAL PERSPECTIVE
35

Glymour, C. (2004). We believe in freedom of the will so
we can learn. Behavioral and Brain Sciences, 27,
661–662.
Gopnik, A., Glymour, C., Sobel, D., Schulz, L., Kushir,
T., & Danks, D. (2004). A theory of causal learning
in children: Causal maps and Bayes’ nets.
Psychological Review, 111, 3–32.
Gopnik, A., & Schulz, L. (2004). Mechanisms of theory
formation in young children. Trends in Cognitive
Science, 8, 371–377.
Hall, N. (2004). Two concepts of causation. In J. Collins,
N. Hall, and L. Paul (Eds.), Causation and counter-
factuals. Cambridge, MA: MIT Press, 225–276.
Harris, P. (2000). The work of the imagination. Oxford,
England: Blackwell.
Hesslow, G. (1976). Two notes on the probablistic approach
to causality. Philosophy of Science, 43, 290–292.
Hitchcock, C. (1995). Discussion: Salmon on explana-
tory relevance. Philosophy of Science, 62, 304–320.
Hitchcock, C. (2001). The intransitivity of causation
revealed in equations and graphs. Journal of
Philosophy, 98, 273–299.
Kohler, W. (1927). The mentality of apes (2nd ed.). New
York: Vintage Books.
Lagnado, D., & Sloman, S. (2004). The advantage of timely
intervention. Journal of Experimental Psychology:
Learning, Memory and Cognition, 30, 856–876.
Lewis, D. (1973). Causation. Journal of Philosophy, 70,
556–567.
Lewis, D. (1979). Counterfactual dependence and time’s
arrow. Nous, 13, 455–476.
Pearl, J. (2000). Causality: Models, reasoning and infer-
ence. Cambridge, England: Cambridge University
Press.
Povinelli, D. (2000). Folk physics for apes. Oxford,
England: Oxford University Press.
Premack, D., & Premack, A. (2002). Original intelligence.
New York: McGraw-Hill.
Salmon, W. (1994). Causality without counterfactuals.
Philosophy of Science, 61, 297–312.
Schaffer, J. (2000). Causation by disconnection.
Philosophy of Science, 67, 285–300.
Schottmann, A., & Shanks, D. (1992). Evidence for a dis-
tinction between judged and perceived causality.
Quarterly Journal of Experimental Psychology, 44A,
321–342.
Schultz, T. (1982). Rules of causal attribution.
Monographs of the Society for Research in Child
Development, 47(1), 1–51.
Sloman, S., & Lagnado, D. (2005). Do we “do”?
Cognitive Science, 29, 5–39.
Sobel, D. and Kushnir, T. (2006). The Importance of
Decision-Making in Causal Learning from
Interventions. Memory and Cognition, 34, 411–419.
Spirtes, P., Glymour, C., & Scheines, R. (2000).
Causation, prediction and search
(2nd ed.).
Cambridge, MA: MIT Press.
Steyvers, M., Tenenbaum, J., Wagenmakers, E., & Blum,
B. (2003). Inferring causal networks from observa-
tions and interventions. Cognitive Science, 27,
453–489.
Tomasello, M. (1999). The cultural origins of human cog-
nition. Cambridge, MA: Harvard University Press.
Visalberghi, E., & Trinca, L. (1989). Tool use in
capuchin monkeys: Distinguishing between per-
formance and understanding. Primates,
30,
511–521.
Wegner, D. (2002). The illusion of conscious will.
Cambridge, MA: MIT Press.
Woodward, J. (2002). What is a mechanism? A counter-
factual account. In Jeffrey A. Barrett & J. McKenzie
Alexander (Eds.), PSA 00, part II. Philosophy of
Science, 69 (3, Supplement), S366–S377.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. Oxford, England: Oxford
University Press.
36
CAUSATION AND INTERVENTION

37
2
Infants’ Causal Learning
Intervention, Observation, Imitation
Andrew N. Meltzoff
covariation to be understood as fully causal. The
concept of an intervention may help us move
beyond a debate about the primacy of perception
(Michotte) versus action (Piaget) to theories that map
observations and actions to the same abstract causal
representations.
For developmental scientists, one striking feature
of the philosophical notion of an intervention is that
it is abstract—an intervention can be performed by
the self or by another person (or even by a “natural
experiment” not involving an agent). We can learn
not only through our own interventions on the world,
but also by watching the interventions of others. This
intriguing idea is incompatible with many classical
views of infancy, which explicitly deny the equiva-
lence between observing others and acting oneself.
In classical developmental views, we observe others
from the outside as a series of movements in space,
but we feel ourselves from the inside as yearnings,
intentions, and freely willed plans. The way we repre-
sent self versus other is fundamentally different. This
results in a disconnect between learning by doing
Infants’ Understanding of Interventions
by Self and Other
Causal learning by children combines both observa-
tion and action. These two sources of information
have not been well integrated in developmental
theory. Following Michotte (1963), some develop-
mental scientists argue that young infants are exquis-
itely tuned observers, and that their perceptual
understanding of causality far outstrips their ability to
use this information to manipulate the world.
Following Piaget (1954), others argue that young
infants learn little by pure observation—self-produced
motor action is critical; cognitive development gener-
ally, and causal reasoning in particular, is charted as a
progressive combination of action schemes.
Bayes net approaches provide a way of using both
observation and action (in the form of “interven-
tions”), combining them to generate veridical repre-
sentations of the causal structure in the world. In
fact, on some interpretations (Woodward, 2003), the
link to intervention is crucial for observed patterns of

(self-action) and learning by watching (other’s action).
A prime developmental achievement is to bring these
two modes of learning into line.
There are many ways of testing the psychological
linkage between observed and executed interventions.
I have used infant imitation, which has several
virtues. First, imitation is natural to humans, even
babies. Second, in imitating novel acts, infants fashion
their interventions based on observing interventions
performed by others. Third, it is widely acknowledged
that humans are far more proficient imitators than
other primates (Meltzoff, 1996; Povinelli, 2000;
Tomasello & Call, 1997), and therefore we may be
getting at distinctively human cognition by examining
human imitation and its development. Fourth, com-
putational models including Bayesian approaches have
been applied successfully to both human and robotic
imitation (e.g., Demiris & Meltzoff, in press; Meltzoff &
Moore, 1983; Rao, Shon, & Meltzoff, 2007).
Historically, there are two principal theories of
how infants come to imitate the acts of others:
Skinnerian and Piagetian theory. I argue that neither
of these can encompass the modern empirical work
on infant imitation. The new data are more compati-
ble with the view that there is a fundamental equiva-
lence between the perception and performance of
goal-directed acts—an abstract mapping connecting
acts seen and acts done—that was not envisioned in
the classical frameworks.
Skinner (1953) proposed that young infants cannot
imitate the acts of others without specific training.
When a young infant sees a mother perform an act
such as shaking a rattle to make a sound, the infant
does not know what movements to recruit to copy this
act. Rather, the mother needs to shape the child’s
response through operant conditioning. Mom shakes
the rattle, and then the infant responds with random
motor acts. Mom selectively reinforces those acts that
are similar to shaking the rattle. Over time, the
mother’s shaking comes to serve as a discriminitive cue
(a bell or a light would do as well) that elicits the rein-
forced act (the baby’s rattle shaking). To the outside
observer, the infant is imitating, but this is not because
the baby is able to translate the acts seen into acts
done. The parent essentially teaches the infant what to
do and when to do it through operant conditioning.
This is not an entirely hypothetical example. In
fact, Skinner (1953) has shown that pigeons can be
conditioned to peck a key when they see other pigeons
peck: If Pigeon 1 (P-1) pecks at a key to obtain food
and an observer Pigeon 2 (P-2) is reinforced for peck-
ing on seeing this event, then P-2 will eventually be
shaped to peck when seeing P-1 pecking. But, P-2 did
not learn this intervention on the basis of observing
the other animal. All that has happened is that the
behavior of P-1 has become a cue for eliciting a con-
ditioned response in P-2. It follows that the observer
pigeon could be conditioned to perform a nonimita-
tive act just as easily. Skinner (1953) endorses this
implication: “The similarity of stimulus and response
in imitation has no special function. We could easily
establish behavior in which the ‘imitator’ does exactly
the opposite of the ‘imitatee’” (p. 121).
It is known that human infants as young as 3 to 6
months old can be operantly conditioned quite readily
(e.g., Rovee-Collier, 1990). This means that they can
learn the contingency between their own actions and
results in the world. But, the capacity for operant condi-
tioning does not mean that the infant can learn these
action-outcome relations from observing the acts of oth-
ers. In other words, the fact that infants can learn an
intervention through their own trial and error (learning
by doing) does not mean that they can learn to perform
the intervention on the basis of observing the interven-
tions of others (learning by watching). The latter would
be imitation. The former is just a special case of operant
conditioning in which a friendly demon (a clever
mother or experimenter) has arranged it so the discrim-
inative cue matches the reinforced response. The moral
is that if we want to know whether infants can learn an
intervention through observation, then we need to
know the infant’s reinforcement history or, failing that,
use a novel act for which prior shaping is unlikely.
Piagetian theory (1962) came to similar conclu-
sions as Skinner, albeit for entirely different reasons.
Piaget also thought that young infants could not imi-
tate spontaneously. In Piaget’s case, it was not that
infants needed to be conditioned to learn to imitate,
but rather that they needed to reach a certain stage of
cognitive sophistication. Piaget realized that translat-
ing a seen intervention into one executed by the self
was nontrivial, and he claimed it was beyond the
capacity of infants in the first half year of life. He
hypothesized that infants were “egocentric,” even
“solipsistic.” The youngest infants could not learn
novel acts from observing others (whether these acts
were complex means-ends relationships or simple
body acts) because learning at first occurred through
self-action independently of other people (what
Piaget called practical intelligence).
38
CAUSATION AND INTERVENTION

The Piagetian concept of infantile egocentrism was
most famously illustrated in his predictions about facial
imitation. Infants can see you make a facial movement,
but they cannot see their own faces. If the infant is
young enough, he or she will never have seen his or her
face in a mirror. How could the infant link the observed
facial acts of others with personal unseen bodily acts?
According to Piaget, this “invisible imitation” was
impossible because self and other were known in such
different terms; there was no abstract framework for
connecting observation and performance. Piaget
(1962) put it this way: “The intellectual mechanism of
the child will not allow him to imitate movements he
sees made by others when the corresponding move-
ments of his own body are known to him only tactually
or kinesthetically (as, for instance, putting out his
tongue) . . . since the child cannot see his own face,
there will be no imitation of movements of the face
[before approximately 1 year old]” (p. 19).
Thus, Piaget shared Skinner’s view that actions
could be observed and performed, but that the obser-
vation of an act did not engender the production of a
matching act without a long path of prior learning.
Neither Skinner nor Piaget thought that imitation was
a mechanism for early learning; rather, imitation itself
needed to be learned, and a good deal of theoretical
effort was put into explaining how babies could even-
tually associate the observation of others’ actions with
manipulations performed by the self.
Newborn Imitation: Innate Mapping
Between Observation and Execution
In part because of Skinner’s and Piaget’s theories about
a gulf between the observation and the execution of
human acts, I designed a series of tests of facial imita-
tion in young infants. Contrary to classical theories,
the results show that newborns imitate facial gestures.
The work suggests an abstract notion of goal-directed
action that cuts across the observed acts of others and
one’s own freely willed actions.
In an early study, Meltzoff and Moore (1977)
tested facial imitation in 2- to 3-week-old infants. The
results showed that they could imitate four different
adult gestures: lip protrusion, mouth opening, tongue
protrusion, and finger movement. The mapping
between observation and execution was quite specific:
Infants confused neither actions nor body parts. They
differentially responded to tongue protrusion with
tongue and not lip protrusion, revealing an innate
body scheme that maps from observed body parts to
their own body, despite never having seen their own
face. Similarly, they responded accurately to lip
protrusion versus lip opening, showing that different
patterns of action can be extracted and imitated when
the specific body part is controlled.
As my psychology colleagues quickly pointed out,
these infants may not have been young enough to
answer the objections of Skinner and Piaget. In their
2 weeks of life, they might have learned the relevant
associations. Perhaps mothers conditioned their chil-
dren to stick out their tongues whenever they saw this
gesture. The definitive test involved newborns who
averaged 32 hours old at the time of the test. The old-
est infant was 72 hours old, and the youngest was just
42 minutes old. The newborns accurately imitated
(Meltzoff & Moore, 1983, 1989). Apparently, facial
imitation is innate. This suggests a fundamental
equivalence between the perception and production
of acts that is built into the mind of the human baby.
Goal-Directedness in Early Imitation
Does facial imitation involve a “goal-directed” act? In
this chapter, I discuss goal-directed acts that cause
something to happen in the world. These simple bod-
ily acts do not do that. Nonetheless, I think that early
imitation is goal directed.
A characteristic of goal-directed action is that it con-
verges toward the endpoint along flexible routes. This
has been demonstrated in early imitation. Accurate
imitation does not pop out fully formed. Infants have to
work on it. They make errors and gradually correct
their motor attempts to achieve a more accurate match
to the observed target (Meltzoff & Moore, 1994). This
error correction occurs even though the adult gives no
feedback to the child (no smiles or encouragement)
and, most important, even though the child observes
the others’ act but not their own.
The goal directedness of the response is also illus-
trated in the “creative errors” infants make. One study
showed infants the novel gesture of poking out the
tongue at 45º off midline (from the side of the mouth)
(Meltzoff & Moore, 1994). The predominant pattern
was to poke the tongue into the inside of their check
and then gradually adjust. However, some infants
adopted a novel approach. They poked out their
tongues and simultaneously turned their heads to the
side, thus creating a new version of “tongue to the side”
(Meltzoff & Moore, 1997). This head movement was
INFANTS’ CAUSAL LEARNING
39

not something the adult demonstrated but was the
infants’ construction of how to combine a tongue
protrusion and an off-midline direction. Although the
literal muscle movements were very different, the end-
state orientation of the tongue was similar, and in this
sense it can be seen as an act organized by a goal.
The Innate Representation of Human Action
One way of accounting for these results is to hypothe-
size that infants innately represent the perception and
performance of elementary human acts using the same
mental code. There is thus something like an act space
or primitive body scheme that allows the infant to unify
the visual and motor information into one common
“supramodal” framework (Meltzoff & Moore, 1997).
The nature of the supramodal framework can be
further dissected. Three pieces of data suggest that
the supramodal system is not simply a Gibsonian reso-
nance device that directly turns observations into like
movements—a perception-production transducer.
First, the voluntary nature of the response indicates that
the infant need not produce what is given to perception.
The observations of others’ acts can be stored and
accessed after a delay. At minimum, there is an interme-
diary representation and not simply an automatic trans-
duction. Second, as we have seen, infants correct their
imitative efforts (and make creative errors). Information
about one’s acts has to be available for comparison to
the representation of the adult’s act, but the representa-
tion of the observed act is not confused with or modified
by one’s own multiple motor attempts. Third, infants
show special interest in being imitated themselves; they
recognize when their behavior is being copied
(Meltzoff, 2007). Such recognition implies that there is
a representation of their bodily acts.
This takes us beyond the simple transducer story.
The data suggest a differentiation in the supramodal
system. The representation of the observations are
tagged to keep them differentiable from the represen-
tation of one’s own motor acts. The cognitive act is to
compare these two representations—in one case to
match one’s own acts to the other (imitative correc-
tion) and in the other case to detect being matched
oneself (recognizing being imitated). The mental
code may be abstract enough to unite perception and
production, but the representations deriving from
observation and self-action are not confused. They retain
some source information (e.g., tongue-beyond-lips
[observed] and tongue-beyond-lips [produced]).
I would argue that this fundamental equivalence
(with differentiation) between self and other is a
starting point for social cognition, not an endpoint
reached after months of postnatal learning à la Piaget.
The chief goal for the remainder of the chapter is to
flesh out the thesis that these innately registered
equivalences between observed and self-generated
actions provide a substrate for infants’ learning causal
relations from others’ interventions.
Learning Interventions From Observation:
Making Things Happen
Adults manipulate objects to cause other things to
happen in the world. Infants carefully observe adult’s
causally directed acts and begin reproducing what they
see as soon as they become capable of handling objects.
One study tested whether 14-month-olds could
learn an intervention purely from observation. To
ensure that a new causal relation was being learned, a
novel act was used (Meltzoff, 1988). The adult put a flat
box on the table, looked down at it, and then bent from
the waist, touching it with his head, which caused the
top panel to light up. (This was an early blicket detector
that was activated by human heads.) Later, when infants
were given the box themselves, 67% of them leaned for-
ward from the waist and touched the panel with their
own foreheads. Many kept their eyes open, staring at the
top of the box, and smiled when the light came on.
Control infants showed that the baseline probability of
infants touching the panel with their foreheads was lit-
erally 0%. Not a single infant did so in the absence of
seeing the intervention. In a recent study, I changed the
head-touch apparatus to incorporate a remote effect.
When the adult touched the box with his forehead, this
caused a remote box to light up. The remote box was 2
feet away. When the infants were given their turn, they
touched the adult’s box with their foreheads and imme-
diately turned to stare at the remote box, waiting for it 
to activate (Meltzoff & Blumenthal, 2006). Carpenter,
Nagell, & Tomasello (1998) reported related effects.
Taken together, the experiments show that infants can
learn novel interventions based purely on observation.
A Privileged Role for Manipulations
Performed by Self
These head-touch studies show that infants can learn
an intervention by watching others. Is anything added
if infants perform the intervention themselves 
(Kushnir & Gopnik, 2005; Meltzoff, 2006)?
40
CAUSATION AND INTERVENTION

I conducted a relevant study with 14-month-olds
(Meltzoff, 2006). Infants were randomly assigned to
two groups. Infants in Group 1 watched the adult per-
form manipulations of two novel objects. The experi-
menter shook one object to cause it to make a sound;
he held another one from a string and bounced it up
and down on the tabletop. Infants observed these acts
and then were sent home without manipulating the
objects themselves. Infants in Group 2 were treated
similarly but were immediately given the objects
before being sent home. Virtually all of them imitated
the actions they saw and thus had manipulatory expe-
rience as well as observational information.
The critical test came the next day when both
groups returned to the laboratory, and the objects were
put before them. The adult gave no hint what to do.
Infants who had been given the opportunity for imme-
diate imitation performed significantly more of the tar-
get acts on Day 2. Something appears to be gained if
infants perform the action themselves directly after
observing it. Infant performance is boosted if they
quickly convert an observed manipulation into a self-
produced manipulation. In line with the work on
facial imitation, it appears that the actions of self and
other are coded in commensurable terms, but that the
self-produced acts are tagged distinctively from acts
that were merely observed; converting observation into
a self-action makes it memorable.
I hasten to add that infants can remember causal
events without taking concurrent action. We know
this because the first group of children, who only
observed on the first day (by experimental design),
imitated from memory on the next day. Evidently, the
observed intervention can be stored and used to gen-
erate one’s own manipulations after a delay. But, it is
equally interesting that memory for the causal act is
stronger if the act is first performed by the self before
the delay.
Inferring an Intervention Based on
Unsuccessful Action Patterns
Learning Actions Versus Learning Outcomes
We have seen that infants who see an adult use unusual
means to accomplish an intervention do not simply
reproduce the result (making the light come on) using
any motor acts at their disposal (e.g., their hands), but
instead faithfully copy the whole behavioral envelope.
Based on this research, one might wonder whether
means and ends are differentiable aspects of an
intervention, or whether infants achieve causal results
by reenacting the precise actions used by the adults.
This makes a difference to theories because it could
be that (a) infants faithfully copy the adult’s actions,
and sharing body types and the laws of physics, the
causal results naturally follow; or (b) infants represent
the causal results and strive to achieve them by their
own invented means. This is a tricky distinction to test
empirically because if infants copy our actions, then
they are likely to achieve our causal results “for free.”
The way I investigated this question was to have
infants observe an unsuccessful intervention. I wanted
to test whether infants can read through our failed
attempts and infer the intervention we intended to
achieve. Because the adult’s actions were unsuccess-
ful, infants could not copy the adult’s actions and
achieve the desired result.
Inferred Interventions
I showed 18-month-olds unsuccessful interventions
(Meltzoff, 1995). For example, the adult used a stick
tool in an attempt to push a button to make a sound
but “accidentally” under- or overshot the target. 
Or, the adult grasped the ends of a dumbbell-shaped
object and attempted to yank it in two, but his hands
slid off as he yanked, and thus the goal was not
achieved. To an adult, it was easy to decode the actor’s
intended intervention. The measure of how infants
interpreted the event was what they chose to reenact.
In this case the “correct answer” was not to imitate the
manipulation that was seen (the unsuccessful
attempt), but to perform the intervention the adult
“meant to do.”
The study compared infants’ tendency to perform
the target act in several situations: (a) after they saw
the successful intervention demonstrated, (b) after
they saw the unsuccessful attempt to perform the
intervention, and (c) after the intervention was nei-
ther shown nor attempted (control). The results
showed that 18-month-olds can infer interventions
from adult attempts to perform them. Infants who saw
the unsuccessful attempts and infants who saw the
successful interventions both performed the goal acts
at a significantly higher rate than the controls.
Evidently, infants can understand our goals even if we
use means that are insufficient to fulfill them.
In further work, 18-month-olds were shown similar
displays but were handed a trick toy that prevented
INFANTS’ CAUSAL LEARNING
41

them from performing the intervention (Meltzoff,
2006). For example, the dumbbell-shaped object was
surreptitiously glued shut. If infants attempted to pull
it apart, then their hands slipped off the ends, dupli-
cating the adult’s behavior. The question was whether
this satisfied infants. It did not. They did not terminate
their behavior. They varied the way they yanked on the
dumbbell, systematically changing their interventions
to find one that worked. They also appealed to their
mothers and the adult for help. About 90% of the
infants looked up at an adult within 2 seconds after
failing to pull apart the trick toy, and many vocalized
while staring at the adult’s face. Why were they
appealing for help? They had matched the adult’s sur-
face behavior, but evidently they were striving toward
something else—the adult’s intended intervention.
Inventing New Means to Achieve an
Inferred Intervention
If infants are inferring the adult’s goal, then they
should also be able to achieve it using a variety of
means. I tested this. As before, an adult grasped the
ends of a gigantic dumbbell and attempted to yank it
apart, but his hands slid off. The dumbbell was then
presented to the infants. Infants did not even try to
copy the adult’s exact movements. Rather, they put
their tiny hands on the inside faces of the cubes and
pushed outward, or stood upright and used both
hands to pull upward, and so on. They used different
means than the experimenter, but these acts were
directed toward the same causal result. This fits with
the hypothesis that the infants had inferred the goal of
the intervention, differentiating it from the surface
behavior that was observed.
Work by Want and Harris (2001) goes further and
shows that older children, 3-year-olds, benefit from
observing others using multiple means to achieve a
goal. They benefit more from watching an adult mod-
ify a failed attempt into a successful act than from
watching the demonstration of successes alone. Other
work also underscores the importance of goals in imi-
tation (e.g., Gattis, Bekkering, & Wohlschläger, 2002;
Gleissner, Meltzoff, & Bekkering, 2000; Williamson
& Markman, 2005).
Agents and Goals: Infants Infer Interventions
for Agents
In the adult commonsense framework, the acts of
people can be goal directed and intentional, but
the motions of inanimate devices are not; they are
governed by physics, not psychology. Do infants
interpret the world in this way? Meltzoff (1995)
designed an inanimate device made of plastic and
wood. The device had short poles for arms and
mechanical pincers for hands. It did not look human,
but it traced the same spatiotemporal path the
human actor traced and manipulated objects much
as the human actor did. When the pincers slipped off
the ends of a dumbbell, infants did not infer the
intervention as they did with the human agent. The
infants were no more (or less) likely to pull the toy
apart after seeing the unsuccessful attempt of the
inanimate device than infants in the baseline condi-
tion. However, if the inanimate device successfully
completed this act, then infants did perform the suc-
cessful intervention.
Evidently, infants can understand and duplicate a
successful intervention displayed by the inanimate
device but do not read meaning into the device’s
unsuccessful “attempts.” This makes sense because
successes lead to a visible change in the object.
Failures leave the object intact and therefore must be
interpreted at a deeper level, in terms of the intended
interventions of the agent. Perhaps infants do not
interpret inanimate devices as psychological agents
with goals and intentions; thus, no intervention is
inferred.
In summary, the research shows that infants distin-
guish between what the adult meant to do and what
he actually did. They ascribe goals to human acts;
indeed, they can infer an intended intervention from
a pattern of behavior (multiple unsuccessful attempts)
even when the intervention was not performed. The
acts of persons—but not the motions of mechanical
devices—are understood within an agentive frame-
work involving goals and intentions.
A Natural Experiment: The Primacy of
People in Infants’ Notion of Interventions
The Involvement of People Causes Infants
to Interpret the Same Scene Differently
As we have seen, infants interpret the acts of people in
special ways. This suggests a way of testing Woodward’s
idea of a natural experiment in which a causal event
occurs without an agent as the source of the change. I
showed 18-month-old infants an intervention and varied
whether a person was involved in producing the result.
42
CAUSATION AND INTERVENTION

Infants saw the dumbbell-shaped object in three
successive states. The three views were separated from
each other by raising a black screen, so that the infants
saw three snapshot views of an event that unfolded
over time. What varied is the causal story of how it got
to be that way. After infants saw the three displays, they
were given the dumbbell. The question was whether
they produced the target behavior, which was to pull
the object apart.
Group 1 was a baseline control condition to assess
infants’ spontaneous tendency to manipulate the
object. For this control group, infants simply saw three
identical states—the assembled object sitting in place
with no person present. As expected, infants did not
pull the object apart spontaneously: They mouthed it,
banged it, and slid it across the table, but they did not
spontaneously discover pulling it apart in the absence
of seeing this intervention. For Group 2, the three snap-
shots revealed the affordances of the object but did not
specify the involvement of a person. The views were
the following: (a) object assembled, no person present;
(b) object disassembled, no person present; (c) object
assembled again, no person present. For Group 3, the
snapshots revealed an agent as a potential cause. The
views were the following: (a) object assembled, in per-
son’s hands; (b) object disassembled, in person’s hands;
(c) object assembled, in person’s hands.
Infants in Group 2 did not pull apart the toy; in
fact, they did not differ from the baseline controls. In
contrast, infants in Group 3 pulled the object apart
significantly more often than those in Group 1 or
Group 2. Thus, the involvement of a person as a
potential cause led infants to interpret the same scene
differently. In the case of the natural experiment in
which the object was seen in its pre- and posttrans-
formed state (Group 2), infants observed but did not try
to re-create the event. However, if a person held the
object, although sitting stony-faced and displaying no
effort at acting, infants did so.
These results are especially interesting when com-
bined with the Meltzoff (1995) intention-reading
study. In that study, the dumbbell remains untrans-
formed, and the person is trying to perform an inter-
vention. In the current study, the person is present
and shows no intent, but the results of the object
transformation are shown (three static states: assem-
bled, apart, reassembled). In the former case, there is
human effort and no object transform, and this suf-
fices for infants to infer the intervention. In the latter
case, there is an outcome state and no effort, but if the
person is present, this can be interpreted as a potential
cause for what happened. In both cases, it provides
enough for infants to interpret the observations as rel-
evant to their own actions and for them to fill in the
blanks and produce a manipulation that was never
directly observed but only inferred.
Agentless Transformations: Magic
In a further study, the dumbbell object was magically
pulled apart and reassembled in front of the child’s
eyes but appeared to do so autonomously. This pro-
vides object transformation data in full view.
The object was placed on a black box, and inside
the box there were magnets. The magnets were
moved, and thus the dumbbell came apart and was
reassembled without this being caused by a human
agent. The results were that 15-month-olds did not
pull the toy apart at any higher than baseline levels.
Interestingly, half of the infants picked up the object
and placed it back on the box several times, as if situ-
ating the object on the magic spot would cause the
result. Infants saw the intervention and wanted it to
repeat but, in the absence of a human cause, drew no
implication for their own causal actions. Evidently,
they thought the object transformation would happen
to the object if it was spatially positioned, rather than
thinking they could cause the transform through their
own manipulation.
Learning to Use a Tool
In the developmental and animal psychology litera-
tures, one of the most celebrated examples of causal
reasoning is the case of tool use. We know a lot about
the ability of chimpanzees to use tools—starting from
Köhler’s (1927) observations of Sultan moving crates
below an overhead banana to reach it and extending
to Jane Goodall’s (1968) reports of termite fishing on
the Gombe Stream Reserve. Although it was once
argued that tool use was uniquely human, it is now
widely acknowledged that other animals are success-
ful tool users, including the gold standard of using a
stick to obtain an out-of-reach target. The debate con-
cerns whether animals use tools based on trial and
error or based on insight about the causal relations
involved (Povinelli, 2000; Tomasello & Call, 1997).
For the purposes of this chapter, I am interested in
exploring tool use from a different perspective.
Instead of asking whether animals and infants use
INFANTS’ CAUSAL LEARNING
43

tools when left on their own to “figure it out,” I wish
to examine learning through observation—in
particular, seeing an expert use a stick to obtain an
out-of-reach goal. The extant data are mixed.
Tomasello and Call (1997) suggest that wild chim-
panzees do not readily learn how to use a tool from
observation, but that some enculturated chimps may;
Povinelli (2000) remains skeptical of the latter.
The literature concerning human infants is simi-
lar. There is good evidence that infants can eventually
learn to use sticks as tools when left to their own
devices (Bates, Carlson-Luden, & Bretherton, 1980;
Brown, 1990; Piaget, 1954) but much sparser evi-
dence concerning learning from the interventions of
others. Of course, it is well known that adults and
older children learn how to use a wide variety of tools
and complex machinery by watching experts; the
debate concerns younger ages.
To test for observational learning of tool use, one
needs a few conceptual distinctions. To begin,
one needs to distinguish imitation from stimulus
enhancement. The latter refers to the fact that the
infants’ attention may simply be drawn to a tool by
virtue of the adult handling it. With their attention
drawn to the stick, infants may increase their random
play with the object, thereby increasing the probabil-
ity that they will learn through trial and error that it
can be used as a tool. The child is not learning a new
causal relation based on what they see the other
do. Rather, the child is learning that the stick is
interesting—stimulus enhancement—and thereby is
more likely to pick it up, with the rest following by
chance or trial and error.
In the developmental literature, there have been
surprisingly few well-controlled tests of learning to use
complex tools through observation. Nagell, Olguin,
and Tomasello (1993) performed a relevant experi-
ment comparing chimps and human infants. They
reported that the 18-month-old children failed to
learn how to use a rake (to obtain a distant object)
from observation, but that 24-month-olds could do so.
I tested younger infants. The sample consisted of
120 infants evenly distributed at 16, 18, 20, and 22
months of age (Meltzoff, 2006). Within each age
group, infants were randomly assigned to one of three
test conditions: (a) learning by observation, in which
the adult modeled the correct use of the rake to obtain
the out-of-reach goal; (b) Control 1 (baseline), in which
infants saw no modeling and were simply given the
rake; and (c) Control 2 (stimulus enhancement), in
which infants saw the adult use the rake to touch the
goal, thereby drawing attention to the rake and to the
fact that it could make spatial contact with the goal
(correct use of the rake was not shown).
The tool was a 17-inch long rake. It was placed
horizontally in front of the infant, with approximately
a 2-foot spatial gap between it and the goal object.
The goal was a highly desirable rubber giraffe. Infants
had 1 minute to solve the problem. Preliminary studies
in our lab suggested that infants performed better
when they observed the model from a first-person
perspective—when the adult and infant were side by
side, rather than facing each other across the table.
This may be important because previous studies have
not modeled tool use from this perspective (e.g., in
the Nagell et al. 1993 study the adult faced the infant,
so the modeling entailed using the tool to pull the
object away from the infant and toward the adult).
Viewing the goal-directed act of the model from the
same perspective as one’s own may facilitate learning
from observation.
Infants showed great enthusiasm for obtaining the
goal (stretching out their arms, vocalizing, looking at
the adult, etc.). In the two control groups, there was
no significant difference in the successful use of the
rake as a function of age. Across all 120 subjects, only
7.5% (6 of 80) of the infants solved the problem spon-
taneously; in contrast, fully 50% (20 of 40) of the
infants succeeded after they saw the adult show them
how to use the tool, p  .001. The older infants 
(20- and 22-month-olds) profited far more from obser-
vation (70% succeeded) than did the younger infants
(30% succeeded), p  .05.
Infants learn from observation but not automati-
cally. There appears to be an interaction between the
infants’ initial cognitive level and what they gain from
observing others. The young infants learn, but they do
not exceed spontaneous rates by the same degree that
the older infants do. I would predict that still younger
infants would not learn how to use the rake from
observation. I say this because of the nature of the
failures. After watching the expert adult, the younger
infants pounce on the rake and wield it with great
confidence. However, once they move the rake to the
quarry, they are not be able to “think through” the
causal relations—that the business end of the rake has
to be behind the goal-object and the tines pointed
downward before the rake could be pulled in. (Their
reaction reminds me of undergraduates who get halfway
through a difficult conceptual distinction and then,
face fallen, find themselves lost, unable to bring
things to conclusion. The “uh-oh, what-do-I-do-next”
44
CAUSATION AND INTERVENTION

expression seems to be invariant across age.) One pos-
sibility that arises from this work is that infants have to
be “on the cusp” of solving the problem themselves to
get the boost from seeing how someone else solves it
(see Gopnik & Meltzoff, 1986, 1997, for related find-
ings). The older infants would be more intelligent
consumers of the observed interventions. I am explor-
ing this possibility through further research.
Conclusions
The work described in this chapter has implications
for both psychology and philosophy.
Psychology
The power of imitation has always been underesti-
mated in psychology. Skinner underestimated imita-
tion because he thought it was simply a variant of
operant conditioning in which the infants’ response
had been shaped up. Just as infants could be trained
to perform Behavior X when they saw a red light, so
they could be trained to perform Behavior X in
response to Behavior X. There was nothing special
about the match between self and other. Skinner
thought that the opposite behavior would do just as
well as a cue. I doubt it. I think you would be in for a
long series of training sessions if you tried to teach a
baby to open his or her hand every time the baby saw
you close yours. The intrinsic connection would
interfere with learning the arbitrary association.
Chomsky underestimated imitation because it was
a learning mechanism. To say children learn through
imitation means that they are sculpted by experience.
Chomsky relegated experience to “parameter setting”
or the “triggering” of innately structured systems. It is
difficult to see how these concepts can explain the imi-
tation of novel acts like head-touch. Infants duplicate
this act, but it is unlikely to be biologically specified
and simply triggered. Chomsky may (or may not) be
correct about the domain of grammar, but in the
domain of action, observing others’ novel acts has a
powerful effect of sculpting infants’ own actions.
Parents do not need to slavishly condition their child
for the child to begin to act like those around the
dinner table. The babies are observing and learning.
Moreover, research suggests that “auditory observation”
may be more powerful in language acquisition than
traditionally assumed, particularly for the acquisition
of culturally specific phonology. Kuhl (2004) reports that
infant phonology, as indexed by both brain measures
and perceptual measures, is influenced by the sounds
infants hear in their culture; furthermore, studies show
that young infants reproduce speech sounds they hear
through imitation (Kuhl & Meltzoff, 1996).
Piaget underestimated imitation because he
thought that infants were born with “heterogeneous
spaces”—a “visual space” that was initially independ-
ent of their “motor space.” A major task of the first
2 years of life was to unify these spaces so infants
could learn from watching, not just from doing.
Piaget predicted that facial imitation was impossible
until about 1 year of age and deferred imitation (imi-
tation from the memory of observed, now absent,
events) impossible before about 18 months of age.
My research shows facial imitation at birth and
deferred imitation soon thereafter.
These theorists missed the idea that there is a fun-
damental equivalence between observing and per-
forming goal-directed motor acts. It is not that seeing
and doing need to be linked by associative learning or
conditioning. Imitation is innate. Infants can even
imitate facial gestures they have never seen them-
selves perform. Infants have an abstract mental code,
we call it a supramodal code, that unites acts seen and
acts done within the same framework.
The innate equivalence between elementary acts
of self and other has implications for learning about
cause-effect relations. Instead of relying exclusively on
the contingencies between your acts and the conse-
quences in the world, you can learn through observing
the actions of others—actions that you immediately
recognize as “like my own.” If acts performed by
another make something happen, perhaps they will
make the same thing happen when I do them. Such
learning could not get off the ground if the observed
acts were not recognized to be the same as my own
acts. That much is nature’s share.
Philosophy
Woodward (chapter 1, this volume) describes three
levels of causal understanding:
1. A purely egocentric causal view: The subject
understands the relationships between personal
actions and the resulting effects but is unable to
grasp that the same relationships can occur
when the self is not the cause.
2. An agent causal view: The subject understands
that the causal relationships that exist between
personal actions and effects also apply to the
actions of other people.
INFANTS’ CAUSAL LEARNING
45

3. A fully causal view: The subject understands
that the same causal relationships that the sub-
ject exploits in intervening can also be used by
other agents and can exist in nature even when
no other agents are involved.
The egocentric infant described by Piaget’s theory
(1952, 1954) closely resembles what Woodward called
the egocentric causal view. This egocentric organism is
capable of being conditioned because he or she can
grasp the relation between bodily movements and
effects in the world but cannot learn from watching
the causal actions of others. The modern empirical
results suggest that the egocentric infant is a fiction.
Laboratory rats and other animals may conform to this
description, but the human infant does not.
There is evidence, however, that up to about 18
months of age, the human infant is not fully causal in
Woodward’s sense. Several experiments suggest that
the human infants learn interventions differently from
a person than from an inanimate device (inferred
intervention studies) and draw only limited inferences
when no agent is present (natural experiment stud-
ies). Based on the current research, it may be that
Woodward’s (chapter 1, this volume) characterization
of an “agentive view” is a reasonable description of
the prelinguistic toddler. How and when an infant
develops into a fully causal agent is a central question
for developmental cognitive science (Gopnik et al.,
2004; Meltzoff, 2006).
Summary
The perception of others’ actions and production of
self-action are mapped onto commensurate represen-
tations starting from birth. This allows infants not
only to learn interventions through their own manip-
ulations but also to multiply greatly their learning
opportunities by observing the manipulations of oth-
ers and profiting from them. For example, in the
novel head-touch case, infants immediately knew
how to activate the object 24 hours after seeing the
adult do so, without ever having handled the object
themselves. Importantly, infants do not seem to confuse
acts of self and other. On the one hand, they correct
their behavior (showing a retention of the observed
target that is differentiable from the self’s motor
efforts). On the other hand, they treat their own acts
in a privileged manner that suggests some sort of
mental tagging that helps track whether an act was of
external or internal origins.
Infants imitate but do not blindly copy everything
they see. First, they make creative errors. Second, they
skip over the literal behavior they see and choose to
duplicate inferred interventions—what the adult
meant to do, not what the adult did do. Third, when
causal relations are difficult, as in the rake case for
younger infants, observation alone does not seem to
guarantee success; older infants glean more from the
modeling than do younger ones.
Starting at birth, there seems to be a delicate inter-
play between learning by observation and learning by
doing. The two are not quarantined from each other
as Michotte (with an emphasis on observation over
motor experience) or Piaget (with an emphasis on
motor experience over pure perception) might have
supposed. Instead, there seems to be a reciprocal
exchange between these two modes of learning. What
infants observe influences what they do (novel head-
touch imitation), and what they can do changes their
attention to the model and how they interpret it (tool
use from observation).
ACKNOWLEDGMENTS
This work was supported by
the National Institutes of Health (HD-22514), the
James S. McDonnell Foundation, and the Tamaki
Foundation. I thank Alison Gopnik for her causal
powers as editor, colleague, and friend. I also acknow-
ledge helpful conversations with Jim Woodward,
Laura Schulz, and other members of the McDonnell
Foundation Causal Inference Workshop at the
Stanford Center for Advanced Study in 2003. Thanks
also to Craig Harris and Calle Fisher for help on the
final stages of chapter preparation.
References
Bates, E., Carlson-Luden, V., & Bretherton, I. (1980).
Perceptual aspects of tool using in infancy. Infant
Behavior & Development, 3, 127–140.
Brown, A. L. (1990). Domain-specific principles affect
learning and transfer in children. Cognitive Science,
14, 107–133.
Carpenter, M., Nagell, K., & Tomasello, M. (1998).
Social cognition, joint attention, and communicative
competence from 9 to 15 months of age.
Monographs of the Society for Research in Child
Development, 63(4, Serial No. 255).
Demiris, Y., & Meltzoff, A. N. (In press). The robot in the
crib: A developmental analysis of imitation skills in
infants and robots. Infant and Child Development.
Gattis, M., Bekkering, H., & Wolschläger, A. (2002).
Goal-directed imitation. In A. N. Meltzoff & 
W. Prinz (Eds.), The imitative mind: Development,
46
CAUSATION AND INTERVENTION

evolution, and brain bases
(pp. 183–205).
Cambridge, England: Cambridge University Press.
Gleissner, B., Meltzoff, A. N., & Bekkering, H. (2000).
Children’s coding of human action: Cognitive
factors influencing imitation in 3-year-olds.
Developmental Science, 3, 405–414.
Goodall, J. (1968). The behavior of free-living chim-
panzees in the Gombe Stream Reserve. Animal
Behavior Monographs, 1, 3.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, 
L. E., Kushnir, T., & Danks, D. (2004). A theory of
causal learning in children: Causal maps and Bayes
nets. Psychological Review, 111, 3–32.
Gopnik, A., & Meltzoff, A. N. (1986). Relations between
semantic and cognitive development in the one-
word stage: The specificity hypothesis. Child
Development, 57, 1040–1053.
Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts,
and theories. Cambridge, MA: MIT Press.
Köhler, W. (1927). The mentality of apes (E. Winter,
Trans.) (2nd ed.). London: Routledge & Kegan Paul.
Kuhl, P. K. (2004). Early language acquisition: Cracking
the speech code. Nature Reviews Neuroscience, 5,
831–843.
Kuhl, P. K., & Meltzoff, A. N. (1996). Infant vocaliza-
tions in response to speech: Vocal imitation and
developmental change. Journal of the Acoustical
Society of America, 100, 2425–2438.
Kushnir, T., & Gopnik, A. (2005). Young children infer
causal strength from probabilities and interven-
tions. Psychological Science, 16, 678–683.
Meltzoff, A. N. (1988). Infant imitation after a 1-week
delay: Long-term memory for novel acts and multi-
ple stimuli. Developmental Psychology, 24, 470–476.
Meltzoff, A. N. (1995). Understanding the intentions of oth-
ers: Re-enactment of intended acts by 18-month-old
children. Developmental Psychology, 31, 838–850.
Meltzoff, A. N. (1996). The human infant as imitative
generalist: A 20-year progress report on infant imita-
tion with implications for comparative psychology.
In C. M. Heyes & B. G. Galef (Eds.), Social learn-
ing in animals: The roots of culture (pp. 347–370).
New York: Academic Press.
Meltzoff, A. N. (2006). The “Like Me” framework for
recognizing and becoming an intentional agent.
Acta Psychologica, doi:10.1016/j.actpsy.2006.09.005.
Meltzoff, A. N. (2007). “Like Me”: A foundation for social
cognition. Developmental Science, 10, 126–134.
Meltzoff, A. N., & Blumenthal, E. J. (2006, April).
Causal monitoring. Paper given at the meeting of
the McDonnell Foundation Causal Learning
Workshop. Pasadena, CA.
Meltzoff, A. N., & Moore, M. K. (1977). Imitation of
facial and manual gestures by human neonates.
Science, 198, 75–78.
Meltzoff, A. N., & Moore, M. K. (1983). Newborn infants
imitate adult facial gestures. Child Development, 54,
702–709.
Meltzoff, A. N., & Moore, M. K. (1989). Imitation in
newborn infants: Exploring the range of gestures
imitated and the underlying mechanisms. Deve-
lopmental Psychology, 25, 954–962.
Meltzoff, A. N., & Moore, M. K. (1994). Imitation, mem-
ory, and the representation of persons. Infant
Behavior & Development, 17, 83–99.
Meltzoff, A. N., & Moore, M. K. (1997). Explaining
facial imitation: A theoretical model. Early
Development and Parenting, 6, 179–192.
Michotte, A. (1963). The perception of causality (T. R. Miles
& E. Miles, Trans.). London: Methuen & Co. Ltd.
Nagell, K., Olguin, R. S., & Tomasello, M. (1993). Processes
of social learning in the tool use of chimpanzees (Pan
troglodytes) and human children (Homo sapiens).
Journal of Comparative Psychology, 107, 174–186.
Piaget, J. (1952). The origins of intelligence in children
(M. Cook, Trans.). New York: International
Universities Press.
Piaget, J. (1954). The construction of reality in the child
(M. Cook, Trans.). New York: Basic Books.
Piaget, J. (1962). Play, dreams and imitation in childhood
(C. Attegno & F. M. Hodgson, Trans.). New York:
Norton.
Povinelli, D. J. (2000). Folk physics for apes: The chim-
panzee’s theory of how the world works. New York:
Oxford University Press.
Rao, R. P., Shon, A. P., & Meltzoff, A. N. (2007). A
Bayesian model of imitation in infants and robots. In
K. Dautenhahn & C. L. Nehaniv (Eds.), Imitation
and Social Learning in Robots, Humans and Animals:
Behavioural, Social and Communicative Dimensions.
Cambridge, UK: Cambridge University Press.
Rovee-Collier, C. K. (1990). The “memory system” of
prelinguistic infants. In A. Diamond (Ed.), The
development and neural bases of higher cognitive
functions (Vol. 608, pp. 517–542). New York: New
York Academy of Sciences.
Skinner, B. F. (1953). Science and human behavior. New
York: Macmillan.
Tomasello, M., & Call, J. (1997). Primate cognition. New
York: Oxford University Press.
Want, S. C., & Harris, P. L. (2001). Learning from other
people’s mistakes: Causal understanding in learn-
ing to use a tool. Child Development, 72, 431–443.
Williamson, R. A., & Markman, E. M. (2006). Precision
of imitation as a function of preschoolers’ under-
standing of the goal of the demonstration.
Developmental Psychology, 42, 723–731.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. New York: Oxford University
Press.
INFANTS’ CAUSAL LEARNING
47

Introduction
Humans are causal animals. We see events not merely
as occurring, but as caused. Integral to mature causal
reasoning is the ability to understand particular causal
relations in our environment. Indeed, adults readily
detect both psychological and physical causal
relations across a range of human action and object
motion events. Imagine watching a dinner companion
hungrily devouring his dessert. You might assume that
the actor’s grasp of the dessert fork is caused by his
goal of obtaining the brownie, and that the movement
of the dessert item on his plate is caused by the con-
tact between fork and brownie. Thus, at a basic level,
identifying causal relations involves at least two com-
ponents. First, one must segment ongoing human
action and object motion into psychological and
physical causal episodes. In the example, one must
recognize that the fork-grasping, brownie-moving,
and brownie-eating actions cohere to form causal
units. Second, one must identify variables relevant to
causal outcomes. A desire or predisposition to eat
sweets and a particular type of contact between the
fork and brownie are relevant to identifying the
respective psychological and physical causal relations
involved in our dessert scenario.
An integral question then is how an understanding
of these rudimentary aspects of causal understanding
is achieved. In this chapter, I argue that infants’ expe-
rience of their own actions and the consequences that
these actions have on the world play an important role
in their developing understanding of causal relations.
Recent philosophical theories of causation take an
interventionist perspective on causality: If manipulations
on one factor (interventions) are associated with a
change in a second factor, then the first causes the
second (e.g., Woodward, chapter 1, this volume). In
addition, empirical evidence suggests that both adults
and young children readily learn causal structure from
enacting and observing interventions (see Gopnik &
Schulz, 2004, for a review). In this chapter, I present
evidence that infants’ developing ability to act on the
world is intimately linked to their causal understand-
ing. Infants’ interventions may enable them to evaluate
48
3
Detecting Causal Structure
The Role of Interventions in Infants’ Understanding of Psychological 
and Physical Causal Relations
Jessica A. Sommerville

causal hypotheses and detect the causal structure of
various events in the world.
Infants’ Understanding of Physical and
Psychological Causation
A variety of research reveals that children have a rich
understanding of the causal structure of the physical
and psychological world (e.g., Gopnik &  Meltzoff,
1997). By the end of the preschool period, children
appreciate human action as psychologically caused:
They describe, predict, and explain their own and
others’ behavior with reference to mental states
(Bartsch & Wellman, 1995; Gopnik & Astington,
1988; Wellman, Cross, & Watson, 2001). These
developments are paralleled in children’s understand-
ing of physical causation.
The sophistication of young children’s causal
reasoning has led researchers to focus on the origins
of causal understanding. Work suggests that toddlers
possess at least one key aspect of understanding
behavior as psychologically caused: They understand
human action as guided by goals. Eighteen-month-
old infants readily imitate the inferred goals of others
(Meltzoff, 1995), selectively reproduce goal-directed
acts (vs. accidental acts; Carpenter, Akhtar, &
Tomasello, 1998), and can distinguish between their
own goals and those of another person (Repacholi &
Gopnik, 1997).
Studies have assessed the roots of preverbal
infants’ ability to view human action as goal directed.
This work reveals that even young infants construe
simple actions of others as goal or object directed.
After watching an actor reach for and grasp an object,
6-month-old infants attend more to changes in an
actor’s goal than they do to other, more superficial
aspects of the reach and grasp, such as the reach
trajectory or location (Woodward, 1998). Over the
next 6 months of life, infants’ ability to construe action
as goal directed becomes increasingly elaborate: 12-
month-olds also perceive attentional (e.g., eye gaze;
Woodward, 2003), instrumental (e.g., point gesture;
Woodward & Guajardo, 2002), and novel actions
(e.g., pushing with the back of hand; Jovanovic et al.,
2006; Kiraly, Jovanovic, Prinz, Aschersleben, &
Gergely, 2003) as object directed. Infants can also
move beyond construing action as goal directed to
parse the ongoing stream of behavior into goal-relevant
units (Baldwin, Baird, Saylor, & Clark, 2001) and are
able to use previous action and attentional cues to
predict future action (Phillips, Wellman, & Spelke,
2002; Sodian & Thoermer, 2004). These findings sug-
gest that a key element of understanding psychologi-
cal causal relations begins in infancy and is
elaborated over the first year of life.
A nascent sensitivity to physical causal relations is
also present in infancy. Traditionally, infants’ under-
standing of physical causal relations has been exam-
ined from the vantage point of problem solving or tool
use. Piaget (1953) suggests that causal understanding
emerges toward the end of the first year of life; this
was based on observing that his own infants developed
the ability to use an intermediary object to achieve a
target object (e.g., pulling a support to obtain an out-
of-reach toy, pulling a string to get a toy) by this age.
Additional empirical works bear out and extend
Piaget’s observations. Numerous studies demonstrate
that, by the end of the first year of life, infants can
solve a variety of simple tool use tasks (Bates, Carlson-
Luden, & Bretherton, 1980; Uzgiris & Hunt, 1975;
Willatts, 1984, 1999). Shortly thereafter, infants can
generalize tool use solutions across problems based
on their underlying causal structure, as opposed to
strictly on the basis of shared perceptual features
(Chen, Sanchez, & Campbell, 1997). Moreover,
older infants not only can transfer causal solutions
across problems, but also can pick tools to solve a
problem based on their causal efficacy. After learning
to solve a tool use problem, toddlers select novel tools
based on their utility in goal attainment as opposed to
their perceptual similarity to previous tools (Brown,
1990; Chen & Siegler, 2000).
Subsequent work has revealed that preverbal
infants detect causal relations in certain types of
object motion events. By 6 months of age, infants
recognize the causal status of Michottian-type
launching sequences and distinguish this causal event
from other events that share spatiotemporal properties
but are not causal (e.g., delayed launching and no
collision events; Leslie, 1982; Leslie & Keeble, 1987).
Over the next several months, infants’ causal percep-
tion becomes more sophisticated. By 10 months of
age, infants respond to the causal status of the launch-
ing events that feature real objects (Oakes & Cohen,
1990) and perceive the causality of launching events
in which objects move along dissimilar paths (Oakes,
1994). It is also by this age that infants become
increasingly sensitive to causal roles within a causal
event (Cohen & Oakes, 1993). Several months later,
infants differentiate the primary cause of a causal
DETECTING CAUSAL STRUCTURE
49

chain versus a temporal chain (Cohen, Rundell,
Spellman, & Cashon, 1999). Thus, over the first year
of life infants recognize physical causal relations in a
variety of different object motion events.
Taken together, these findings suggest that the
roots of causal understanding are present in infancy.
Infants recognize psychological and physical causal
episodes and can identify some of the variables affecting
these relations. Controversy exists, however, over the
means by which this understanding is achieved.
Mechanisms Underlying the
Development of Causal Understanding
Some investigators have argued that infants are
innately endowed with an ability to understand certain
psychological and physical causal relations. For exam-
ple, it has been suggested that infants possess an
abstract system for construing action as goal directed,
and that such a system is automatically activated by a
set of perceptual cues (such as self-propelled motion,
contingent action, etc.; e.g., Baron-Cohen, 1995;
Gergeley, Nasady, Csibra, & Biro, 1995; Premack,
1990). Similarly, Leslie argues that infants’ sensitivity
to causality in launching events is guided by an innate
perceptual module (e.g., Leslie, 1994).
However, innate knowledge cannot be the whole
story. Prior to 6 months of age, infants respond to
launching events on the basis of simpler perceptual
features (e.g., spatial-temporal features) but not
causality (Cohen & Amsel, 1998), and they do not
spontaneously encode the goal of a human actor’s
reach and grasp (Sommerville, Woodward, &
Needham, 2005). In addition, there is general agree-
ment that sensitivity to both psychological and physi-
cal causal relations becomes increasingly elaborate
over the course of infancy (e.g., Cohen, Chaput, &
Cashon, 2002; Csibra, Biro, Koos, & Gergely, 2003;
Gopnik & Meltzoff, 1997; Woodward, Sommerville, &
Guajardo, 2001).
Other authors suggest that various domain-general
developments may underlie infants’ causal under-
standing. Cohen and colleagues (e.g., Cohen et al.,
2002) argue that information-processing develop-
ments throughout infancy enable infants to integrate
increasingly higher-order elements, including causal
relations, of object motion displays. Studies have doc-
umented that infants are adept at detecting statistical
regularities across a range of stimuli, and that infants’
ability to do so forms the basis of learning (e.g., Aslin,
Saffran, & Newport, 1998; Kirkham, Slemmer, &
Johnson, 2002; Saffran, Aslin, & Newport, 1996;
Saffran, Johnson, Aslin, & Newport, 1999). For exam-
ple, infants segment words from fluent speech based
on statistical relationships between adjacent speech
sounds (Saffran et al., 1996) and appear to do so based
on transitional probabilities of successive speech
sounds (e.g., Aslin, Saffran, & Newport, 1998).
Baldwin and colleagues (Baird & Baldwin, 2001;
Baldwin et al., 2001) have argued that infants may
capitalize on such statistical learning skills to identify
behavioral cues associated with goal attainment when
observing ongoing human action. By extension,
infants could also use statistical covariation detection
to identify aspects of physical causal structure.
The Role of Interventions in Infants’
Developing Sensitivity to Causal
Relations
All of the aforementioned abilities likely contribute
to infants’ capacity to appreciate physical and psycho-
logical causal relations. In addition, infants’ develop-
ing experience as actors may play a powerful role in
their causal understanding. Indeed, Piaget suggests
that infants’ causal understanding emerges through
their sensorimotor actions on their environment
(e.g., Piaget, 1953). Other scholars argue that under-
standing human behavior as psychologically caused
relies on our ability to map from our own experience
to those of others (e.g., Goldman, 1989; Gordon,
1986; Harris, 1989; Heal, 1989), and that this may be
particularly true in early development (Meltzoff,
2002; Meltzoff & Brooks, 2001; Tomasello, 1999;
Woodward et al., 2001).
Previous work suggests that adults and children
readily detect causal structure by intervening on their
environment (Gopnik & Schulz, 2004; Gopnik et al.,
2004; Kushnir & Gopnik, in press; Lagnado & Sloman,
2004; Sobel & Kushnir, 2003; Steyvers, Tenenbaum,
Wagenmakers, & Blum, 2003). Inter-ventions are
particularly crucial when one must disambiguate
multiple causes or identify variables relevant to causal
outcomes. Critically, interventions enable learners to
test causal hypotheses and compare the outcomes of
their interventions to expected outcomes (e.g., Sobel &
Kushnir, 2003). In keeping with this suggestion, evi-
dence suggests that, at least in some circumstances,
self-generated interventions may result in more
50
CAUSATION AND INTERVENTION

accurate and thorough causal learning than watching
the interventions of others (e.g., Kushnir & Gopnik, in
press; Sobel, 2003).
Dramatic changes occur over the first 2 years of
life in infants’ ability to act effectively on their world.
Infants’ changing action capacities may provide them
with the opportunity to intervene on their environ-
ment, that is, to manipulate one factor intentionally
and observe the results of their manipulations on
another factor. Infants’ own interventions may be a
particularly rich source of information for causal
learning as they allow them to investigate directly
the causal hypothesis that infants hold in the
moment and to observe the effects of their interven-
tions. Such a perspective predicts that infants should
(a) be able to learn from their own causal interven-
tions and (b) be able to relate their interventions to
those of others.
Existing studies provide preliminary evidence for
both of these proposals. Infants’ ability to solve a box-
opening problem improves as a result of their own
dynamic engagement with this problem-solving task
(Bojcyzk & Corbetta, 2004). Put another way, infants
readily and spontaneously learn from their own
interventions. In addition, even very young infants
relate their own interventions to those of others.
Three-month-old infants who participated in a task
that facilitated their ability to intervene on objects sub-
sequently appreciated similar interventions performed
by another person as goal directed (Sommerville et al.,
2005).
Thus, infants possess prerequisites that may enable
them to use information from their own interventions as
a means to understanding psychological and physical
causal relations.
In the studies discussed in this chapter, Woodward
and I (Sommerville & Woodward, 2005a, 2005b) inves-
tigated the extent to which infants’ ability to intervene
on a particular problem was related to their sensitivity
to causal relations in a similar problem when watching
another person act. To do so, we presented infants with
a simple tool use scenario: Infants saw an actor pull a
support supporting an out-of-reach toy and grasp the
toy. An observer watching this sequence must under-
stand that the actor acts on the support with the inten-
tion of getting the toy (psychological causal relation),
and that the movement of the support causes the toy to
move (physical causal relation). Moreover, observers
must understand variables influencing the causal rela-
tions in this sequence, namely, the presence of the
actor’s desire to get the toy and the need for contact
between the support and toy so that the first can cause
the second to move.
Across both studies (Sommerville & Woodward,
2005a, 2005b), infants received an intervention task in
which they were given the opportunity to act on the
support to obtain the out-of-reach toy. To assess
whether infants understand the psychological causal
relations of this sequence, one group of infants subse-
quently took part in a paradigm that assessed whether
infants recognized that an actor acting on a similar
support did so with the intention of getting the toy. To
assess whether infants were sensitive to the physical
causal structure of the sequence, another group of
infants subsequently took part in a paradigm that
assessed their ability to recognize the need for contact
between the toy and support for movement of the sup-
port to cause the toy to be displaced. Our questions
were whether (a) infants were sensitive to the respec-
tive psychological and physical causal relations and (b)
whether this sensitivity was linked to their own inter-
vention experience.
The Role of Interventions in Infants’
Understanding of Psychological Causal
Relations
Adults shown the support-pulling sequence under-
stand that the actor’s goal in this situation (getting the
toy) guides the actor’s actions on the support. Previous
work suggests that the support-pulling problem is
readily solved by 1 year of age (e.g., Piaget, 1953;
Willatts, 1999). Moreover, a study provided evidence
that by this same age infants recognize the psycholog-
ical causal relations of the support-pulling sequence.
After watching an actor pull a support that supported
a toy, infants construed the actor’s subsequent actions
on the support as directed toward the toy rather than
the support itself (Sommerville & Woodward, 2005a).
The present study assessed whether younger infants
could also appreciate the psychological causal
relations involved in the support-pulling sequence
and whether their ability to do so is related to their
developing experience intervening on a similar 
problem.
Ten-month-old infants took part in two para-
digms. During the support-pulling intervention task,
infants were given multiple opportunities to pull a
support that supported an out-of-reach toy, bringing
the toy within reach so that it could be grasped. 
DETECTING CAUSAL STRUCTURE
51

We assessed infants’ ability to solve this problem in a
spontaneous manner without prior instruction.
Specifically, we assessed how frequently infants
solved the task in a way that appeared clearly
directed at obtaining a toy.
Infants also took part in a habituation paradigm
(see Figure 3-1 psychological structure paradigm).
This paradigm capitalizes on infants’ visual attention
as a way to gauge their event representations. While
sitting in a high chair or on a caregiver’s lap, infants
watch live events presented by a human actor on a
puppet stage. An experimenter who is unaware of the
events that infants are watching observes infants’ eye
gaze using a computer program that times infants’
looking to the outcomes of the events. Infants are
separated from the stage by a screen that can be lowered
to reveal the event and raised when a trial is termi-
nated. During the initial phase of the study, infants
repeatedly watch a single event until their attention
wanes (habituation). Infants then see two test events
in alteration that each differ along a single dimension
from the habituation event. The features that infants
weight most heavily in their event representations are
gauged by examining infants’ novelty preference (e.g.,
which event they look longer at) to the test events.
Some test events are perceptually similar to the habit-
uation event but feature a change in the actor’s overar-
ching goal. Other test events preserve the actor’s
overarching goal but feature changes in the way in
which this goal is achieved in comparison to the habit-
uation event. Infants’ sensitivity to the psychological
causal relation of the sequence is inferred from the
52
CAUSATION AND INTERVENTION
FIGURE 3-1

extent to which they prefer (e.g., look longer at) events
that vary the actor’s goal over those that vary the way in
which the actor’s original goal is achieved.
During the habituation trials, infants saw an actor
sitting behind a stage that contained two different
colored supports, each of which sat under a different
toy. The screen was lowered, and the actor said
“Hi. Look.” while subsequently reaching toward and
grasping one of the supports. She next pulled the sup-
port toward her and grasped the toy that it supported.
Infants’ looking was timed to the static outcome of
this event (the actor grasping the toy).
Once infants’ looking time had decreased to half of
its initial level (habituation), the locations of the toys
were switched. This enabled us to show infants two
new test events that tapped their sensitivity to the goal
of the sequence. On new support events, infants saw
the actor turn in a new direction and grasp a different
support than she had during habituation trials (which
now supported the same toy that she had acted toward
during habituation). On new toy events, infants saw the
actor turn in the same direction and grasp the same
support that she had during habituation trials (which
now supported a different toy than she had acted on
during habituation trials). In both cases, infants’ look-
ing was timed to the static outcome of the event (the
actor grasping the support). Longer looking to the new
support events would suggest that infants construed the
actor’s initial actions on the support as directed toward
the support itself rather than as directed toward the toy.
Longer looking on the new toy events would suggest
that infants inferred that the actor’s actions on the sup-
port were directed toward the toy and thus showed a
novelty preference for events featuring a change in this
dimension of the action sequence.
To address our question of interest, we sought to
assess the extent to which infants’ ability to intervene
on the support-pulling problem was related to their
habituation performance. To do so, we categorized
infants into two groups based on their intervention task
performance. Infants in the top 25% in terms of action
task performance were dubbed planful infants. Planful
infants produced clearly goal-directed strategies to
solve the action task on 83%–100% of trials. Infants in
the bottom 25% in terms of action task performance
were dubbed nonplanful infants. Nonplanful infants
produced clearly goal-directed strategies to solve the
action task on 0%–20% of trials.
To assess whether planful and nonplanful infants
construed the habituation events differently, we
compared looking times to the new toy and new
support events for both groups. Planful infants looked
significantly longer to the new toy than new support
events. Nonplanful infants showed the opposite pat-
tern of looking: They preferred the new support over
the new toy event. These findings suggest that infants
who were good at organizing their actions toward the
goal of the sequence (the toy) likewise perceived the
actions of another person in a similar context as
directed toward the overarching goal of the sequence.
Infants who were poor at organizing their actions
toward the goal of the sequence, in contrast, may have
misperceived the actor’s actions on the support as
directed toward the support itself.
These findings suggest that infants begin to
abstract the goal of action sequences toward the end
of the first year of life, and that this ability is tightly
linked to infants’ own tendency to intervene on the
support in a goal-directed manner. In another study
Woodward and I (Sommerville & Woodward, 2005b)
assessed whether infants’ experience intervening on
the support-pulling problem was related to their abil-
ity to detect physical causal relations.
The Role of Interventions in Infants’
Understanding of Physical Causal 
Relations
The ability to understand the support-pulling sequence
entails not only an appreciation of the causal relations
between actor and toy in this situation, but also an
appreciation of the causal relation between the toy and
support. This entails the ability to recognize the need
for contact between the toy and the support: If the toy
were sitting adjacent to, rather than on top of, the sup-
port, then the toy would not move when the support was
pulled. Previous work suggests that infants begin to take
into account the need for contact between the toy and
the support by about 12 months of age, both in their
own actions (e.g., Piaget, 1953; Schlesinger & Langer,
1999) and the actions of others (Schlesinger & Langer,
1999; Sommerville & Woodward, 2005b). In this study
(Sommerville & Woodward, 2005b), Woodward and I
sought to assess whether younger infants are also sensi-
tive to this physical causal relation and whether their
ability to appreciate the need for contact between the
toy and the support was related to their own interven-
tion experience in a similar situation.
A different group of 10-month-old infants from
those in the aforementioned study took part in 
DETECTING CAUSAL STRUCTURE
53

a support-pulling intervention task and a habituation
paradigm. The intervention task was similar to that
described; however, in this task infants were given
multiple opportunities to pull the support both when
the toy sat on the support and when it sat adjacent to
the support. We assessed infants’ ability to solve the
problem in a goal-directed manner as a function of
the location of the toy.
The habituation paradigm differed slightly from
that of Woodward and I in 2005. Infants again watched
live events presented by a human actor on a puppet
stage, and their looking was timed to the static outcome
of these events. In this study, however, after seeing a
simple support-pulling event, infants saw events that
were perceptually similar to the initial event but
causally implausible along with those that were percep-
tually dissimilar to the initial event but causally plausi-
ble. The prediction was that sensitivity to the physical
causal structure of this event would be evident in
longer looking at the causally implausible event.
During habituation trials, infants saw an actor sit-
ting behind a stage that contained a single support
that supported a toy. The screen was lowered, and the
actor said, “Hi. Look.” She subsequently pulled the
support toward her, entraining motion on the part of
the toy. Unlike in the previous study, the actor did not
grasp the toy. Infants’ looking was timed to the static
outcome of the event (the actor grasping the support).
Infants watched this event on repeated trials until
looking had declined to half of its initial level (habit-
uation criteria).
While the stage was hidden from view, we next
removed the toy from the support and placed it on an
invisible black platform that sat roughly 3.5 inches
adjacent to the support. On test trials, infants saw two
types of test events. On consistent test events, once the
screen was lowered the actor said “Hi. Look.” and
subsequently pulled the support toward her while the
toy remained in place. On inconsistent test events,
once the screen was lowered, the actor said “Hi.
Look.” and subsequently pulled the support toward
her while the toy moved alongside the support. The
actor accomplished this by surreptitiously pulling
the toy on the platform along a track from underneath
the display. To an adult observer, the inconsistent
event represents a causal violation: The toy appears to
be moving “magically” along with the support.
Our prediction was that the extent to which infants
varied their own interventions as a function of the
location of the toy would be related to their ability to
recognize causal violations to the support sequence in
the actions of another person. To test this prediction,
we subdivided infants into two groups based on their
performance on the intervention task. Infants dubbed
discriminators pulled the support as a means to get the
toy more frequently when the toy sat on, rather than
adjacent to, the support. Infants dubbed nondiscrimi-
nators did not vary their support-pulling behavior as a
function of the toy location. The results of this study
met with our predictions. Discriminators looked longer
at the inconsistent than consistent test events, suggest-
ing that they recognized that a causal violation had
occurred when support movement appeared to cause
the toy to move in the absence of contact between the
toy and the support. Nondiscriminating infants looked
equally to both test events, suggesting that they were
not sensitive to this causal violation. Thus, the way in
which infants intervened on the support problem
predicted whether they would be sensitive to the
physical causal violation of the support sequence.
Subsequent analyses revealed interesting and
important differences among nondiscriminating
infants with respect to their intervention task and
habituation performance. Some nondiscriminating
infants pulled the support as a means to get the toy at
high frequencies regardless of the location of the toy.
Other nondiscriminating infants pulled the support to
get the toy infrequently regardless of the location of
the toy. Thus, high-frequency pullers had multiple
opportunities to observe the effects of their interven-
tions under two different setting conditions: when the
toy sat on and when the toy sat off the support. Low-
frequency pullers had far fewer opportunities. In sub-
sequent analyses, we took into account that frequently
infants pulled the support across both contact and
noncontact trials during the intervention task. High-
frequency pullers subsequently recognized the causal
violation featured in the habituation paradigm.
Specifically, they looked longer at the inconsistent
than the consistent test event. These findings suggest
that infants may use their interventions to evaluate
causal hypotheses, and that they can rapidly detect
causal structure in the actions of others based on this
experience.
Taken together, the results indicate that infants
begin to appreciate causal relations in the support
sequence toward the end of the first year of life, and
that their ability to do so is intimately linked to their
own intervention experience and expertise. Infants’
ability to planfully solve the support sequence was
54
CAUSATION AND INTERVENTION

related to their ability to recognize that another
person’s actions on the support were directed at the
toy (psychological causal relation). Infants’ ability to
guide their interventions according to the degree of
contact between the support and the toy was related
to how readily they attended to the need for contact
between the support and the toy for the first to cause
the second to move (physical causal relation).
Importantly, the way in which infants enacted inter-
ventions in this latter case predicted their ability to
learn which variables were important for producing a
successful causal outcome. Thus, like young children
(e.g., Gopnik et al., 2004; Kushnir & Gopnik, in
press), infants may also utilize their own interventions
as a means to detecting causal structure.
Conclusion
Research using a range of methodologies suggests that
causal understanding has its roots in infancy and is
instantiated in infants’ sensitivity to a number of psy-
chological and physical causal relations. In addition,
evidence suggests that infants may possess or
encounter a range of learning mechanisms and abili-
ties that support early causal learning. Chief among
these factors is infants’ own active experience. Formal
and descriptive accounts of causation stress the role of
interventions, actions that bring about or prevent a
certain event from occurring, in detecting causal
structure (e.g., Gopnik & Schulz, 2004; Woodward,
chapter 1, this volume). Indeed, causal learning in
both adults and children is informed by an opportu-
nity to enact interventions (Gopnik et al., 2004;
Kushnir & Gopnik, in press; Lagnado & Sloman,
2004; Sobel, 2003; Steyvers et al., 2003). The evi-
dence discussed in this chapter suggests that infants’
developing action abilities, their action experience,
and the extent to which they capitalize on opportuni-
ties to act on their environment and observe the con-
sequences of their actions may play on important role
in causal learning. Through acting on the world,
infants can bring about interventions that may enable
them to test causal hypotheses and observe the effect
that these interventions have on the causal structure
of the world.
ACKNOWLEDGMENTS
I would like to thank Alison
Gopnik, Laura Schulz, and David Sobel for comments
on this chapter. I would also like to thank the parents
and infants who participated in the research presented
in this chapter. A significant portion of this chapter was
completed at the University of Washington’s Whiteley
Center. I am grateful to the Whiteley Center for their
resources and support.
References
Aslin, R. N., Saffran, J. R., & Newport, E. L. (1998).
Computation of conditional probability statistics by
8 month-old infants. Psychological Science, 9,
321–324.
Baird, J., & Baldwin, D. A. (2001). Making sense of
human behavior: Action parsing and intentional
inference. In B. F. Malle, L. J. Moses, & D. A.
Baldwin (Eds.), Intentions and intentionality:
Foundations of social cognition. Cambridge, MA:
MIT Press.
Baldwin, D. A., Baird, J. A., Saylor, M. M., & Clark, M. A.
(2001). Infants parse dynamic action. Child
Development, 72, 708–717.
Baron-Cohen, S. (1995). Mindblindness: An essay on
autism and theory of mind. Cambridge, MA: MIT
Press.
Bartsch, K., & Wellman, H. M. (1995). Children talk
about the mind. London: Oxford University Press.
Bates, E., Carlson-Luden, V., & Bretherton, I. (1980).
Perceptual aspects of tool using in infancy. Infant
Behavior & Development, 3, 127–140.
Bojczyk, K. E., & Corbetta, D. (2004). Object retrieval
in the first year of life: Learning effects of task
exposure and box transparency. Developmental
Psychology, 40, 54–66.
Brown, A. L. (1990). Domain-specific principles affect
learning and transfer in children. Cognitive Science,
14, 107–133.
Carpenter, M., Akhtar, N., & Tomasello, M. (1998).
Fourteen- through 18-month-old infants differen-
tially imitate intentional and accidental actions.
Infant Behavior & Development, 21, 315–330.
Chen, Z., Sanchez, R. P., & Campbell, T. (1997). From
beyond to within their grasp: The rudiments of
problem-solving in 10- and 13-month-olds.
Developmental Psychology, 33, 790–801.
Chen, Z., & Siegler, R. S. (2000). Across the great divide:
Bridging the gap between understanding of toddlers’
and older children’s thinking. Monographs of the
Society for Research in Child Development, 65,
1–108.
Cohen, L. B., & Amsel, G. (1998). Precursors to infants’
perception of the causality of a simple event. Infant
Behavior & Development, 21, 713–731.
DETECTING CAUSAL STRUCTURE
55

Cohen, L. B., Chaput, H. H., & Cashon, C. H. (2002).
A constructivist model of infant cognition.
Cognitive Development, 17, 1323–1343.
Cohen, L. B., & Oakes, L. M. (1993). How infants
perceive a simple causal event. Developmental
Psychology, 29, 421–433.
Cohen, L. B., Rundell, L. J., Spellman, B. A., & Cashon,
C. H. (1999). Infants’ perception of causal chains.
Psychological Science, 10, 412–418.
Csibra, G., Biro, S., Koos, O., & Gergely, G. (2003).
One-year-old infants use teleological representations
of actions productively. Cognitive Science, 27,
111–133.
Gergely, G., Nasady, Z., Csibra, G., & Biro, S. (1995).
Taking the intentional stance at 12 months of age.
Cognition, 56, 165–193.
Goldman, A. (1989). Interpretation psychologized. Mind
and Language, 4, 161–185.
Gopnik, A., & Astington, J. W. (1988). Children’s under-
standing of representational change and its relation
to the understanding of false belief and the appear-
ance-reality distinction. Child Development, 59,
26–37.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes’ Nets.
Psychological review, 11, 3–22.
Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts
and theories. Cambridge, MA: MIT Press.
Gopnik, A., & Schulz, L. (2004). Mechanism of theory
formation in young children. Trends in Cognitive
Science, 8, 371–377.
Gordon, R. M. (1986). Folk psychology as simulation.
Mind and Language, 1, 158–171.
Harris, P. (1989). Children and emotion. Oxford,
England: Blackwell.
Heal, J. (1998). Co-cognition and off-line simulation:
Two ways of understanding the simulation
approach. Mind and Language, 13, 477–498.
Jovanovic, B., Kiraly, I., Elsner, B., Gergely, G., Prinz,
W., & Aschersleben, G. (2006). The role of effects
for infants’ perception of action goals. Manuscript
submitted for publication.
Kiraly, I., Jovanovic, B., Prinz, W., Aschersleben, G., &
Gergely, G. (in press). The early origins of goal
attribution 
in 
infancy. 
Consciousness 
and
Cognition, 12, 752–769.
Kirkham, N. Z., Slemmer, J. A., & Johnson, S. P. (2002).
Visual statistical learning in infancy: Evidence for a
domain general learning mechanism. Cognition,
83, B35–B42.
Kushnir, T., & Gopnik, A. (2005). Young children infer
causal strength from probabilities and interven-
tions. Psychological Science, 16, 678–683.
Lagnado, D. A., & Sloman, S. (2004). The advantage of
timely intervention. Journal of Experimental
Psychology: Learning, Memory and Cognition, 30,
856–876.
Leslie, A. M. (1982). The perception of causality in
infants. Perception, 11, 173–186.
Leslie, A. M. (1994). ToMM, ToBy, and Agency: Core
architecture and domain specificity. In L. A.
Hirschfeld, & S. A. Gelman (Eds). Mapping the
mind: Domain specificity in cognition and cultrue
(pp. 119–148). New York, NY: Cambridge University
Press.
Leslie, A. M., & Keeble, S. (1987). Do 6-month-old
infants perceive causality? Cognition, 25, 265–288.
Meltzoff, A. N. (1995). Understanding the intentions of
others: Re-enactment of intended acts by 18-month-
old children. Developmental Psychology, 31,
838–850.
Meltzoff, A. N. (2002). Imitation as a mechanism of social
cognition: Origins of empathy, theory of mind, and
the representation of action. In U. Goswami (Ed.),
Blackwell handbook of childhood cognitive develop-
ment. Blackwell handbooks of developmental
psychology (pp. 6–25). Malden, MA: Blackwell.
Meltzoff, A. N., & Brooks, R. (2001). “Like me” as a
building block for understanding other minds:
Bodily acts, attention and intention. In B. F. Malle,
L. J. Moses, & D. A. Baldwin (Eds.), Intentions and
intentionality: Foundations of social cognition
(pp. 125–148). Cambridge, MA: MIT Press.
Oakes, L. M. (1994). Development of infants’ use of con-
tinuity cues in their perception of causality.
Developmental Psychology, 30, 869–879.
Oakes, L. M., & Cohen, L. B. (1990). Infant perception
of a causal event. Cognitive Development, 5,
193–207.
Piaget, J. (1953). The origins of intelligence in the child.
London: Routledge & Kegan Paul.
Phillips, A. T., Wellman, H. M., & Spelke, E. S. (2002).
Infants’ ability to connect gaze and emotional
expression to intentional action. Cognition, 85,
53–78.
Premack, D. (1990). The infants’ theory of self-pro-
pelled objects. In D. Frye & C. Moore (Eds.),
Children’s theories of mind: Mental states and
social understanding (pp. 303–325). Hillsdale, 
NJ: Erlbaum.
Repacholi, B. M., & Gopnik, A. (1997). Early reasoning
about desires: Evidence from 14- and 18-month-
olds. Developmental Psychology, 33, 12–21.
Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
Statistical learning by 8-month-old infants. Science,
274, 1926–1928.
56
CAUSATION AND INTERVENTION

Saffran, J. R., Johnson, E. K., Aslin, R. N., & Newport, E.
L. (1999). Statistical learning of tone sequences by
human infants and adults. Cognition, 70, 27–52.
Schlesigner, M., & Langer, J. (1999). Infants’ developing
expectations of possible and impossible tool-use
events between ages 8 and 12 months. Develop-
mental Science, 2, 195–205.
Sobel, D. M., & Kushnir, T. (2003). Interventions do not
solely benefit causal learning: Being told what to do
results in worse learning than doing it yourself.
Poster presented at the 25th annual meeting of the
Cognitive Science Society, Boston, MA.
Sodian, B., & Thoermer, C. (2004). Infants’ understand-
ing of looking, pointing, and reaching as cues to
goal-directed action. Journal of Cognition &
Development, 5, 289–316.
Sommerville, J. A., & Woodward, A. L. (2005a). Pulling
out the intentional structure of action: The relation
between action processing and action production in
infancy. Cognition, 95, 1–30.
Sommerville, J. A., & Woodward, A. L. (2005b). Infants’
sensitivity to the causal features of means-end
support sequences in action and perception.
Infancy, 8, 119–145.
Sommerville, J. A., Woodward, A. L., & Needham, A.
(2005). Action experience alters 3-month-old infants’
perception of others’ actions. Cognition, 96, B1–B11.
Steyvers, M., Tenenbaum, J., Wagenmakers, E., & Blum,
B. (2003). Inferring causal networks from observation
and interventions. Cognitive Science, 27, 453–489.
Tomasello, M. (1999). Having intentions, understand-
ing intentions and understanding communicative
intentions. In P. D. Zelazo, J. W. Astington, & 
D. R. Olson (Eds.), Developing theories of inten-
tion: Social understanding and self-control (pp.
63–75). Mahwah, NJ: Erlbaum.
Uzgiris, I. C., & Hunt, J. M. (1975). Assessment in
infancy: Ordinal scales of psychological develop-
ment. Chicago: University of Illinois Press.
Wellman, H. M., Cross, D., & Watson, J. (2001). Meta-
analysis of theory-of-mind development: The truth
about false belief. Child Development, 72,
655–684.
Willatts, P. (1984). The stage IV infants’ solution of prob-
lems requiring the use of supports. Infant Behavior
& Development, 7, 125–134.
Willatts, P. (1999). Development of means-end behavior
in young infants: Pulling a support to retrieve a
distant object. Developmental Psychology, 35,
651–667.
Woodward, A. L. (1998). Infants selectively encode the
goal object of an actor’s reach. Cognition, 69, 1–34.
Woodward, A. L. (2003). Infants’ developing understand-
ing of the link between looker and object.
Developmental Science, 6, 297–311.
Woodward, A. L., & Guajardo, J. J. (2002). Infants’ under-
standing of the point gesture as an object-directed
action. Cognitive Development, 17, 1061–1084.
Woodward, A. L., Sommerville, J. A., & Guajardo, J. J.
(2001). How infants make sense of intentional
action. In B. F. Malle, L. J. Moses, & D. A. Baldwin
(Eds.), Intentions and intentionality: Foundations of
social cognition (pp. 149–169). Cambridge, MA:
MIT Press.
DETECTING CAUSAL STRUCTURE
57

This chapter extends the interventionist analysis of
causation to give an account of causation in psychol-
ogy. Many aspects of empirical investigation into psy-
chological causation fit straightforwardly into the
interventionist framework. I address three problems.
First is the problem of explaining what it is for a
causal relation to be properly psychological rather
than merely biological. Second is the problem of
rational causation: how it is that reasons can be
causes. Finally, I look at the implications of an inter-
ventionist analysis for the idea that an inquiry into psy-
chological causes must be an inquiry into causal
mechanisms. I begin by setting out the main ideas of
the interventionist approach.
Interventionism
Interventionism is the view that for X to be a cause of
Y is for intervening on X to be a way of intervening on
Y (cf. Pearl 2000; Spirtes, Glymour, & Scheines,
1993; Woodward, 2003; Woodward & Hitchcock,
2003; see also Woodward, chapter 1, this volume).
The interventionist approach can be vividly
expressed by means of causal graphs, which use
arrows to depict causal relations between variables.
These arrows may represent positive or inhibiting
causal relations. Suppose we consider a causal rela-
tion between variables X and Y. Suppose, for exam-
ple, that X represents the level of a drug in someone’s
blood, and that Y represents whether and how well
the subject recovers from an illness. Suppose further
that the body endogenously produces the drug in
varying quantities in different people. There will be
some biological factor responsible for the level of
endogenous production of the drug in someone’s
body; suppose we express this by variable R. And, sup-
pose that the drug is also spontaneously ingested by
people as part of their ordinary diet, in varying
amounts by different people; suppose we summarize
the factors responsible for spontaneous ingestion of
the drug in ordinary diet by variable S. Then, we can
represent the hypothesis that the level of the drug is
a cause of degree of recovery from the illness as in
Figure 4-1.
The arrows in Figure 4-1 show variables R and S
causally affecting X and X causally affecting Y.
58
4
An Interventionist Approach to Causation in Psychology
John Campbell

The objective of an interventionist analysis is to
explain what it is for X to be causally affecting Y. The
intuitive idea is that for X to cause Y is for intervening
on X to be a way of intervening on Y (intervening on
the level of drug will be a way of intervening on
degree of recovery from the illness).
Following Woodward and Hitchcock (2003), we
can exhibit an intervention on X in terms of a variable
I that acts on X. (For instance, we might think of an
external agent giving people various amounts of the
drug and observers keeping track of the subsequent
degrees of recovery of people from the illness.) The
idea then is that there are at any rate some circum-
stances in which, if there were an intervention on X,
then there would be a difference in the value of Y
(Figure 4-2).
There is a possibility that R and S might be
common causes of both X and Y. In that case, variations
in X will be correlated with variations in Y, but that
may not be because X causes Y. (So, for example, we
have to keep in mind the possibility that the factors
that cause endogenous production of the drug, or lead
a person to ingest a lot of it, might each be a common
cause of both the level of drug in a person’s body and
the degree of recovery from the illness. In that case, we
will find that there is indeed a correlation between
degree of recovery and level of drug in the body but
that will not constitute a causal relation between the
level of drug and the degree of recovery. So, we should
want an intervention on X to suspend the influence of
these other factors on the level of drug in the blood.)
In general, then, the intervention variable I should
take over control of the value of X, removing it from
the influence of R and S. To use Pearl’s term, the inter-
vention should be surgical, breaking the arrows from R
and S to X. Given that condition on the intervention
variable I, then we can say that for X to cause Y is for
it to be the case that there is a correlation between X
and Y under potential interventions on X.
There are further conditions to be met. We have to
exclude the possibility that the intervention I on X
also affects Y directly. (For example, administering the
drug should not have a placebo effect.) So, we should
stipulate that an intervention variable for X with
respect to Y must not affect Y otherwise than by affect-
ing X. We should require that there is no bias in
which interventions are carried out; that is, that there
should be no correlation between intervention and
recovery (i.e., we should not be administering the
drug only to those who are going to recover anyway).
Finally, we should have a requirement of causal suffi-
ciency on the variables we have explicitly represented;
in particular, there should be no unrepresented
variables that are common causes of pairs of variables
we do have explicitly represented, so that spurious
correlations can be generated.
With these stipulations in place, though, we can
define what it is for X to cause Y by saying that if there
were an intervention on X, then there would in some
cases be a difference in the value of Y. Or, equiva-
lently, we can say that for X to cause Y is for X and Y
to be correlated under potential interventions on X.
This is not a reductive definition of causation. On the
contrary, it makes free use of causal notions in defin-
ing the idea of an intervention and in explaining what
it is for a set of variables to be causally sufficient.
Nonetheless, the definition I have just given does not
appeal to the idea of a causal relation specifically
between X and Y. It has therefore some claim to provide
a nonreductive illumination of the notion by locating
it in a broader framework of causal notions.
In my remarks in this section, I follow closely the
approach to causation developed by Woodward and
Hitchcock, building on the earlier work of Pearl
(2000) and Spirtes, Glymour and Scheines (1993);
any originality so far is accidental. Notice that the
approach presupposes a certain modularity in the
system of variables in question. It presupposes that inter-
ventions on the system can in principle leave undis-
turbed the causal relations among particular variables.
INTERVENTIONIST APPROACH TO CAUSATION
59
FIGURE 4-2
FIGURE 4-1

That is, an intervention can selectively disturb certain
causal relations—those involving the usual causes of
the target variable X—while leaving others intact,
particularly the causal relation between the target
variable X and the outcome variable Y (cf. Hausman,
and Woodward 1999).
Control Variables
I want now to ask whether this approach can be used
to illuminate causation in psychology. On the face of
it, there should be no special problem here. Consider
any psychological variable M1 and the hypothesis
that M1 is a cause of some other psychological vari-
able M2. So, for example, consider the hypothesis
that worry is a cause of insomnia (Harvey (2005)). For
worry to be a cause of insomnia is, on this approach,
for it to be the case that if there were an intervention
on worry, then there would be a difference in the
level of insomnia. The trouble with this, though, is
that any intervention on worry is also going to be an
intervention on some underlying set of biological vari-
ables. You cannot affect worry without affecting the
underlying biology. So, how do we describe the situa-
tion? Is it that the worry is causing the insomnia—that
intervening on the worry is correlated with a difference
in the insomnia? Or, is it that there is a biological vari-
able underlying the worry, and it is causally related to
a biological variable underlying the insomnia? In that
case, the situation is better described by saying that
intervention on the first biological variable is correlated
with the second biological variables. The psychological
variables, in that case, are epiphenomenal on the
underlying biological causation.
Think how you would characterize the relation
between the positions of the controls on a radio and
the output of the radio, such as the volume of the
sound or the radio station heard. All that goes on here
does indeed supervene on a microphysical reality. But,
we would ordinarily have no hesitation in saying that
someone turning the controls is making a difference to
the output. Why does it seem so evident here that the
position of the dials is causing the output, and that we
are not here dealing merely with epiphenomena?
I think we can get at this by recalling a famous set
of criteria proposed in 1965 by the epidemiologist
Austin Bradford Hill to determine whether particular
environmental hazards were causes of particular dis-
eases or merely correlated with them. Central among
Hill’s criteria are three things. The first is the existence
of a dose-response effect. Most simply, this demands
that there be an identifiable relationship between the
value of the input variable and the correlated output
variable. To demonstrate that smoking is a cause of
cancer, for example, one critical piece is the datum
that the amount one smokes is correlated with the
probability of contracting cancer. Second, it enhances
the case for saying that smoking causes cancer if there
is a large effect of smoking on cancer. Finally, it
enhances the case for saying that smoking is a cause of
cancer if smoking is correlated specifically with cancer
rather than any other outcome. I can sum this up by
saying that the case for saying that smoking causes
cancer is a case for saying that smoking is a control
variable for cancer. Here, I am using control in the
sense in which the buttons on a radio are controls.
There is a large, specific, and systematic correlation
between the volume coming out of the radio and the
degree to which you turn the volume dial. Just so,
under interventions on the level of smoking, there are
large, specific, and systematic effects on cancer.
I am proposing that we should use this notion of a
control variable to identify the level at which we find
the causally significant variables in a complex system.
I think there is no question but that, in the case of the
radio, the positions of the various buttons and knobs
are control variables in this sense, and that this is 
why it seems so evident that making a difference to
the controls of the radio is making a difference to the
upshot; we are not dealing here with epiphenomena.
For the case of smoking, consider how you would
react to a spokesperson for the tobacco industry who
argued that smoking is not a cause of cancer, that
smoking and cancer are both merely epiphenomenal
on an underlying microphysical reality at which the
true causal relations are to be found. The natural
point to make in reply is that smoking is a control vari-
able for cancer; interventions on smoking have large,
specific, and systematic correlations with cancer. That
is the case for saying that the causal relations between
smoking and cancer are to be found at the macro-
physical level.
The example of the controls on a radio is in some
ways special. The relationship between control variable
and output need not always be analog. This will be par-
ticularly important when we consider how we can affect
one another through the use of language. You can tell me
how things are or make requests of me, and the control
system here, assuming I am compliant, is not analog.
60
CAUSATION AND INTERVENTION

But, you may nonetheless have large, specific, and
systematic effects on my states.
Of course, it will be a matter of degree whether one
variable functions as a control variable for another,
and there will be a certain relativity to context. But,
that is how it is with causal ascription generally. Hill
(1965) did not explicitly formulate his criteria as
criteria for choice of variables to use in characterizing
the data, and what I have said here by no means
exhausts his points. But, the force of the idea, that we
find the right level at which to characterize causal
relations by looking for the level of control variables,
seems undeniable.
One way to see the force of that idea is to look
again at the background picture of an interventionist
approach to causation. An interventionist approach
sees the interest or point of our notion of cause as hav-
ing to do with our manipulations of our environment.
It is not that the notion of cause is explained in terms
of agency; it is, rather, that to characterize causal rela-
tions is to characterize the aspects of the world that we
exploit when we manipulate it. If you think of causa-
tion in this way, then it seems evident that control
variables will be of great importance in describing
causation. For, in manipulating the world, we want, as
much as possible, to be intervening on variables that
are correlated with large, specific, and systematic
upshots. We want to be intervening on control vari-
ables in our actions. In these terms, then, the case for
saying that worry is a cause of insomnia is that worry
is a control variable for insomnia. What is it to say that
worry causes insomnia, and that the two are not
merely epiphenomena? It is to say that interventions
on worry are correlated with large, specific, and sys-
tematic variations in insomnia.
Causation by Reasons
Some difficult issues concern the application of the
interventionist picture to what we might call rational
causation, cases in which the causal explanation
appeals to the subject’s possession of reasons. Suppose
we consider, for instance, the hypothesis that the
intention to do X causes doing X. Can we think of this
in terms of whether there would be differences in
whether X was performed if there were interventions
on the intention to do X?
The really difficult thing here is to find the right
characterization of a psychological intervention.
What is it to intervene on whether someone has the
intention to do X? We would naturally think of this in
terms of providing someone with reasons to do X or
reasons not to do X. “You think doing X will make you
happy, but it won’t,” you might say as an opening
move. And, you might present further considerations
in favor of your remark. You would be appealing to
the rationality of the subject. The trouble with this is
that it leaves intact the factors that are the usual
causes of the someone’s forming, or not forming, the
intention to do something.
For example, suppose that one of the usual causes
of a person’s intending to do X is that they think doing
X will make them happy. If your intervention takes the
form of arguing about whether doing X will in fact
make that person happy, then you have left in place
one variable that is a usual cause of whether the
person forms the intention to do X. This means that
the intervention is not, in Pearl’s term, surgical. To use
again the example of a drug trial, suppose you are ask-
ing whether the level of drug in someone’s body causes
recovery from illness. If you manipulate the level of
drug in that person’s body by acting on the mechanism
involved in the body’s endogenous production of the
drug, then this does not constitute an intervention in
the sense I explained in the last section. Similarly, if an
endogenous cause of whether someone forms the
intention to do X is whether the person believes that
doing X will make them happy, then a manipulation of
whether the person forms the intention that proceeds
by manipulating whether the person believes that
doing X will make them happy does not constitute an
intervention in the sense I explained.
The reason for insisting on a surgical intervention
in the case of the drug trial was the problem of
common causes: that the endogenous cause of the
level of drug in the blood might also be directly caus-
ing recovery from illness, so that the level of drug in
the blood actually played no role in causing recovery
from illness despite being correlated with recovery. It
is to rule out this scenario that we have to consider
interventions that seize control from outside the level
of drug in the blood. Similarly, suppose we leave intact
the endogenous causes of formation of the intention to
do X, such as the belief that doing X will make one
happy. Then, it is possible that the belief that doing X
will make one happy causes both formation of the
intention to do X and directly causes performance of
the action itself. In that case, the intention to do X will
be correlated with doing X even though the intention
INTERVENTIONIST APPROACH TO CAUSATION
61

plays no role in causing the action. It is to rule out this
scenario that we have to consider only surgical inter-
ventions on the intention to do X, according to the
interventionist picture as I have so far set it out.
What would it be to have a surgical intervention
on someone’s possession of an intention to do X? The
intervention would have to come from outside and
seize control of whether the subject had the intention,
suspending the influence of the subject’s usual reasons
for forming an intention, such as whether the subject
had reasons for forming the intention to do X. We can
diagram the situation by means of a causal graph
(Figure 4-3).
This is evidently quite an unusual situation. It does
not happen very often, if it happens at all, that a person’s
rational autonomy is suspended and some alien force
seizes control over whether that person has a particular
intention. Still, even though it does not happen very
often, it could still be that an interest in psychological
causation is an interest in what would happen in such
an unusual case. Similarly, you might say that an inter-
est in causation in physics often deals with what would
happen in various idealized conditions—in a complete
vacuum or on a frictionless plane, for example—even
though such situations do not arise often.
The real problem for the interventionist picture
here is that it is not credible that our interest in
psychological causation is an interest in what would
happen under such idealized conditions of alien
control. There are two aspects of our ordinary concep-
tion of the psychological life that have been removed
in this scenario, and without them our psychological
life would not be recognizable.
Notice first that ordinarily we have our intentions
under continuous review. If you hit an obstacle in try-
ing to execute your plan, then you may review whether
to sustain the intention in the light of all your back-
ground beliefs and objectives—just how important is
this anyhow?—and how far you stick with an inten-
tion often depends on continuous review in the light
of your other psychological states, your priorities, and
your beliefs regarding the likelihood of success. If you
could not do this kind of continuous monitoring, then
you would be said to be “not responsible for your
actions.” It is exactly this situation that we are envisag-
ing, though, when we think in terms of surgical inter-
vention on possession of an intention.
Second, this scenario is one that would undermine
our ordinary conception of the ownership of an inten-
tion. One element in our ordinary notion of the 
ownership of an intention is the idea that the long-
standing objectives, interests, preferences, and so on
of that person were causally responsible for the
formation of that particular intention. It is a reason-
able description of the situation envisaged as surgical
intervention here to say that someone else’s intention
has been thrust into the mind of the subject.
Someone who seemed to find him- or herself in that
situation—someone who encountered in introspec-
tion an intention that seemed to have been the direct
result of someone else’s long-standing objectives,
interests, preferences, and so on—would experience
this as thought insertion, the feeling that someone
else’s token thought has been pushed into your mind,
one of the symptoms of schizophrenia.
There are many systems for which an approach in
terms of surgical interventions seems appropriate.
Suppose, for example, that our descendants come
upon an archive of electrical machines, present-day
radios, perhaps. And, they want to find out just how
the circuitry works. They are not concerned with the
function of these devices. They just want to under-
stand the electrical engineering involved. In this case,
an approach in terms of surgical interventions seems
entirely apt. Even if it turns out not to be in practice
possible to tear the systems apart into their modular
constituents, still the objective is to find out what
would happen in each constituent module were we to
have a surgical intervention that ripped out this piece
of wiring from its context and tampered with the
input end to see what would happen at the output.
We have understood the causal structure of the
circuitry when we have answered all such questions.
In the case of rational causation, in contrast, we have
no such interest in ripping out individual pieces of
circuitry from their context to see how they would
behave in isolation. The attempt to do this would
result in a system so different from the original that
62
CAUSATION AND INTERVENTION
FIGURE 4-3

what happened in that context could not be said to
have any significant implications for the functioning
of the original intact system. This is a fundamental
point about rational causation in psychology, which
underpins some of the hesitation philosophers have
felt in talking about mental causation at all.
Two Types of Intervention
I think that we can resolve this problem within a
broadly interventionist framework, but to do so we
have to rethink our conception of an intervention; we
have to move away from the focus on surgical inter-
ventions. We want to consider interventions that keep
intact the rational autonomy of the subject, which
means leaving in place the usual causes of the sub-
ject’s psychological states and actions. Then, what
kind of thing are we looking for, to be a psychological
intervention? Let me first give a couple of examples,
then provide a more abstract statement of the general
notion of intervention being presupposed.
Suppose that I am the passenger and you are
driving as we come to a pool of water in the middle of
the road. You stop to weigh the situation. Should you
drive on, or should you back off? As you pause, I say,
“Go for it!” and you put your foot on the accelerator.
One possibility is that you have such admiration for
my judgment and such concern to act as I would like
that the mere fact of my making my remark gives you
a reason to form the intention to press on. However,
that is not the most obvious or the natural analysis of
the situation I have described. Perhaps you know that
my judgment is in general questionable; perhaps you
and I have just quarreled so that far from giving you a
reason to form the intention to proceed, had you
paused to reflect on the matter for a moment you
would have found that my remark gives you good rea-
son to swing around and go the other way. As it is,
though, it is undeniable that my remark had the effect
of making you form the intention to drive on, and that
consequently you did drive on. In this case, my inter-
vention affects the formation of your intention, but it
does not do so by providing you with reasons for or
against forming the intention. Rather, it directly
affects the formation of your intention. I did manage
to reach into your mind and affect the formation of
your thought, otherwise than by giving reasons.
It is not, though, as if you had given over the reins of
your mind to me. You remained an autonomous rational
agent throughout. You could have resisted my remark;
you may later regret that you did not do so. Had you
mustered reasons that struck you as compelling, one way
or another, it could have been that my remark would
have had little effect. The structure of the example can
be given by the causal graph of Figure 4-4.
The problem we encountered with this kind of sit-
uation was as follows. We are attempting to explain the
existence of a causal relation between the intention to
act and the action as a matter of the intention and the
action being correlated under interventions on the
intention. But, we have not yet excluded the possibil-
ity that the usual causes of the intention may also be
direct causes of performance of the action. So, even if
the intention and the action are correlated under this
kind of intervention on the intention, it may be that
this correlation is only a residue of the role of the
usual causes of the intention in operating as common
causes of both the intention and the action.
There is, though, another way in which we could
think of interventions. Suppose we go back to the
example of drug level and recovery from illness.
Suppose we consider a range of actual or possible exter-
nal administrations of the drug to individuals across a
population. And, suppose that when the drug is admin-
istered to an individual it is administered without the
level of endogenous or spontaneous ingestion of the
drug being taken into account; these factors are allowed
to operate as usual. So, this is not a surgical interven-
tion. Nonetheless, we can look at the level of drug that
is endogenously produced by the individual and at the
level of drug that is spontaneously ingested by the indi-
vidual. For each combination of a particular level of
endogenous production and a particular level of spon-
taneous ingestion, we can consider what would be the
outcome of administering a particular level of the drug.
We can say the following: Suppose that there is
some combination of a particular level of endogenous
production of the drug and some level of spontaneous
INTERVENTIONIST APPROACH TO CAUSATION
63
FIGURE 4-4

ingestion of the drug such that, were the external
administration of the drug to be varied while those
levels remained the same, there would be a differ-
ence in whether the subject recovered from illness.
In that case, the level of drug in the blood is a cause
of recovery from illness. In fact, in the way I propose
of developing the interventionist account, this is
what it is for the level of drug in the blood to be a
cause of recovery from illness. (For formal develop-
ment of a related notion of soft intervention, see
Markowetz, Grossman, & Spang, 2005.)
The difference between this formulation of inter-
ventionism and the analysis I reported in the section
on control variables emerges vividly when we
consider cases, such as that of rational causation, for
which modularity assumptions are not correct. We are
not any longer considering whether the value of Y is
independent of the value of X, when the value of X is
set by surgical intervention. We are, rather, consider-
ing whether Y is independent of the intervention
variable I given the usual causes of X. And, the condi-
tions that have to be met by the intervention variable
I are just as before, except that we are no longer
requiring that the influence of the usual causes of X
should be suspended, and that I should be the sole
determinant of the value of X.
We can apply this picture to rational causation in
psychology. We do not need to consider a scenario in
which the rational autonomy of the agent is suspended
and some external factor seizes control of the agent’s
intentions. We can, rather, consider cases in which the
usual causes of the agent’s formation of intentions oper-
ate as usual and look at whether external interventions
that make a difference to whether the agent forms an
intention, for some set of values for the agent’s other psy-
chological states, would be correlated with differences in
whether the agent performs the action. Is intention a
cause of action? My proposal is that this is the question is
whether interventions on intention are correlated with
action given the agent’s other psychological states.
Psychological Causation Without
Psychological Mechanisms
One of the most striking features of an interventionist
approach to causation in psychology is that it makes no
appeal to the idea of mechanism. All that we are asking,
when we ask whether X causes Y, is whether X is corre-
lated with Y under interventions on X. Whether there
is a mechanism linking X and Y is a further question.
Indeed, you could maintain an interventionist
approach to causation while being skeptical about the
very idea of a mechanism. What does it mean to ask
whether there is a mechanism linking X and Y? All
that it comes to, you might say, is that we are asking
whether we can find any causally significant variables
mediating X and Y. Or, perhaps in some cases, we are
asking merely that the link between X and Y should be
explained in terms of one or another familiar pattern
of explanation, for example, biological explanation.
But, the very idea of a causal link does not demand
that there should be intervening variables, or that
assimilation to a favored paradigm should be available.
To see why this perspective matters, consider some
findings in psychiatry. It has long been known that
stressful life events such as bereavement or unemploy-
ment are good predictors of chronic depression. In a
study of several thousand subjects, Kendler, Hettema,
Butera, Gardner, and Prescott (2003) tried to determine
which aspects of stressful life events might be playing a
causal role here. They found that the strongest correla-
tions with later chronic depression were with humilia-
tion rather than with loss; that other-initiated separation
was a stronger predictor of chronic depression than
bereavement, for example. To interpret the study as
showing something about the causes of depression is to
read it as having implications for what the upshot would
be of clinical interventions: The implication is that,
under interventions to ameliorate the sense of humilia-
tion, there would be differences in the degree of
chronic depression. In the sense I explained, humilia-
tion is a control variable, in the kind of nonsurgical
intervention I just described, for later depression.
Stressful life events, however, are not the only
predictors of later depression; there are also biological
factors that seem to be relevant. Kendler, Kuhn,
Vittum, Prescott, and Riley (in press) found that
genetically acquired deficiencies in the serotonin
transport system are correlated with later depression.
Now, given the complexity of the phenomena, all
such findings have to be regarded as provisional. In
this chapter, I want finally to suggest a simple reading
of them, on which they provide a simple, illlustrative
example of a quite general pattern emerging from
current empirical work in psychology and psychiatry.
Although stressful life events predict depression,
not everyone who is humiliated ends up with depres-
sion. People vary in how resilient they are. One read-
ing of the serotonin data is that they reveal serotonin
deficiencies to be the basis of a lack of resilience.
64
CAUSATION AND INTERVENTION

On this reading, then, we have found two causal
variables underlying later chronic depression: humili-
ation and serotonin deficiency. These are control vari-
ables for depression, let us suppose. And, the relevant
notion of intervention, let us suppose, is of the kind I
indicated, for which we consider psychological factors
that affect the level of humiliation directly rather than
by acting on the usual causes of humiliation. So, we
have two variables, one psychological and one biolog-
ical, that are joint causes of later depression.
In this situation, it is natural to ask, What is the
mechanism by which these variables jointly cause
later depression? The radical suggestion I want to con-
sider is that there may be no mechanism. Explanation
by means of mechanisms must bottom out some-
where, and then we are left with the bare facts about
what would happen under interventions. At the
moment, the empirical data show only that both
psychological and biological variables are in general
relevant to psychological outcomes. There is no
empirical support for the idea that all causation that
involves both psychological and biological variables
bringing about a psychological outcome must be sus-
tained by biological mechanisms. In particular, there
is no reason to suppose that a comprehensive set of
control variables for depression will ever be found at
the biological level. It may be that the control vari-
ables for depression will always include psychological
as well as biological variables.
For anyone familiar with vision science, the ubiq-
uity of something like Marr’s three levels of compu-
tation, algorithm, and implementation may seem to
provide a pattern that has been so successful that its
application ought to be pursued across the board.
Scientists working on vision move back and forth
between the cognitive level and the level of biological
mechanism so seamlessly that, in vision science, doing
without the level of biological mechanism is almost
unimaginable. Although that is certainly so for vision
science, it depends on quite special features of the area
that do not hold for psychological causation in 
general.
To explain what these special features are, I want
to introduce the notion of the “robustness” of a vari-
able. The idea here is that if a variable does play a self-
standing role in some causal process, then it ought also
to play a role in endlessly many other causal processes.
For example, consider the so-called hot chocolate
effect: As you stir a cup of hot chocolate and the spoon
sounds against the base of the cup, each successive
“ting” rises in pitch. Why is that? The usual explana-
tion is in terms of the aeration of the liquid. As you stir,
trapped air bubbles are released from the liquid, and it
becomes stiffer. The more rigid a substance, the faster
sound travels through it. Hence, the pitch of the sound
goes up (Crawford, 1982). This explanation appeals to
a variable, aeration. Now, this variable does not figure
only as the explanation of the hot chocolate effect.
There are endlessly many ways in which you can get
at the air bubbles trapped in a liquid. They are affected
by the temperature of the liquid being poured into the
contained and the speed at which it is poured, and
they show up in as simple a way as the visible clouding
of the liquid. This is what I mean by the robustness of
the variable: It shows up in endlessly many different
causal processes and so can be investigated in end-
lessly many different ways.
Now, consider the kinds of variables appealed to in
information-processing accounts of vision. Vision is
generally thought to be modular, in something 
like the sense of Fodor (1983; cf. Coltheart, 1999).
So, the variables appealed to in explaining, for
instance, the finer points of motion perception or
color perception are being used to explain processing
going on within a module. Now, the cognitive 
variables—wavelength pattern X at place p, for
instance—that are used in this kind of explanation
really are internal to the characterization of the pro-
cessing in a single module. What gives the brain states
the contents they have is their role in the processing
within a particular modular system. It makes no sense
to ask, What is the representational content of that
cell-firing? outside the context of inquiry into the pro-
cessing going on in some particular module.
For that reason, the cognitive variables appealed
to in an account of some aspect of visual information
processing cannot be allowed to take on a life of their
own. As purely cognitive variables, it would make no
sense to suppose that the very cognitive variable that
is playing a causal role in the processing going on in
one module could also be playing a role in the pro-
cessing going on in some other module; the determi-
nation of the content of a cognitive state here is always
internal to the working of one particular module or
other. The whole situation is in sharp contrast to the
appeal to aeration in explaining the hot chocolate
effect, for which one and the same variable can evi-
dently figure in a whole sequence of quite different
processes. In that sense, then, the cognitive variables
appealed to in the psychology of vision are not robust.
INTERVENTIONIST APPROACH TO CAUSATION
65

That is why we have the seamless moving back and
forth between these variables and biological mecha-
nisms. The physiological variables are, of course,
robust and can be investigated through their roles in
endlessly many different processes. In contrast, we
give a cognitive characterization of the physiology
only when we are considering the working of some
one modular system.
I think that this point about robustness explains
why we cannot, in vision science generally, make
sense of the idea of cognitive explanation without
biological mechanisms. The point evidently does 
not generalize to every psychological variable.
Humiliation, for example, is evidently robust. The
degree to which you have been humiliated shows up
in many different causal processes. So, too, do the
variables of rational psychology. A particular desire
may figure in causal process after causal process, lead-
ing from endlessly many different inputs to endlessly
many different outputs. Your attentive awareness of an
object before you may be caused by anything from it
suddenly lighting up to it having been the target of
years of search, and it may play a role in processes as
diverse as the starting of a train of thought and the fad-
ing of a smile. So, these personal-level variables are,
in general, robust. We can therefore appeal to them in
causal explanation without having to look for the
robust biological variables that might underlie them.
There may be such variables. It may be that, in the
end, it will turn out that the most effective control vari-
ables for psychological outcomes in human beings are
one and all biological. At the moment, we have no evi-
dence to support such a conclusion. What we find are
more and more biological variables working together
with ever-better understood psychological variables to
yield psychological outcomes. One great merit of an
interventionist approach to causation in psychology, it
seems to me, is that it acknowledges the possibility that
this may be the right picture. We are not obliged to
force the empirical findings to yield biological mecha-
nisms where there may be none.
ACKNOWLEDGMENTS
This chapter was begun
while I was on leave at Stanford’s Center for
Advanced Study in the Behavioral Sciences, and I am
deeply indebted to Alison Gopnik and Thomas
Richardson for many discussions of these problems. 
I also had many helpful discussions with Ken
Kendler. This chapter was presented to a Center for
Advanced Study in the Behavioral Sciences workshop
on causal reasoning and benefited from many helpful
comments there. I also thank the Institute of
Cognitive and Brain Sciences in Berkeley.
References
Coltheart, M. (1999). Modularity and Cognition. Trends
in Cognitive Sciences, 3, 115–120.
Crawford, F. S. (1982). The  hot chocolate effect.
American Journal of Physics, 50, 398–404.
Fodor, J. (1983). The Modularity of Mind. Cambridge,
Mass.: MIT Press.
Harvey, A. G. (2005). Unwanted intrusive thought in
insomnia. In D. A. Clark (Ed.), Intrusive thoughts
in clinical disorders: research, theory and treatment.
New York: Guilford Press.
Hausman, D., & Woodward, J. (1999). Independence,
invariance, and the causal Markov condition. British
Journal for the Philosophy of Science, 50, 521–583.
Hill, A. B. (1965). The environment and disease: associ-
ation or causation? Proceedings of the Royal Society
of Medicine, 58, 295–300.
Kendler, K. S., Hettema, J. M., Butera, F., Gardner,
C. O., & Prescott, C. A. (2003). Life event dimen-
sions of loss, humiliation, entrapment, and danger
in the prediction of onsets of major depression and
generalized 
anxiety. 
Archives 
of 
General
Psychiatry, 60, 789–796.
Kendler, K. S., Kuhn, J. W., Vittum, J., Prescott, C. A., &
Riley, B. (2005). The interaction of stressful life
events and a serotonin transporter polymorphism in
the prediction of episodes of major depression: A
replication. Archives of General Psychiatry, 529–535.
Markowetz, F., Grossman, S., & Spang, R. (2005).
Probabilistic soft interventions in conditional
Gaussian networks. In R. Cowell & Z. Ghahramani
(Eds.), Proceedings of the tenth international work-
shop on artificial intelligence and statistics, Barbados:
Society for Artificial Intelligence and Statistics.
Marr, D. (1982). Vision. San Francisco: W.H. Freeman.
Pearl, J. (2000). Causation. Cambridge, England:
Cambridge University Press.
Spirtes, P., Glymour, C., & Scheines, R. (1993).
Causation, prediction and search. New York:
Springer-Verlag.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. Oxford, England: Oxford
University Press.
Woodward, J., & Hitchcock, C. (2003). Explanatory
generalizations, part 1: A counterfactual account.
Nous, 37, 1–24.
66
CAUSATION AND INTERVENTION

Twain meant his comment as a witticism, of course,
but there is something fascinating about science. From
a few bones, scientists infer the existence of dinosaurs;
from a few spectral lines, the composition of nebulae;
and from a few fruit flies, the mechanisms of heredity.
From a similarly trifling investment, some of us pre-
sume to conjecture even about the mechanisms of
conjecture itself.
Why does science, at least some of the time, suc-
ceed? Why does it generate accurate predictions and
effective interventions? With due respect for our
accomplished colleagues, we believe it may be
because getting wholesale returns out of minimal data
is a commonplace feature of human cognition.
Indeed, we believe the most fascinating thing about
science may be its connection to human learning in
general and in particular to the rapid, dramatic learn-
ing that takes place in early childhood. This view, 
the theory theory, suggests that starting in infancy,
continuing through the life span, and canalized in sci-
entific inquiry, many aspects of human learning can
be best explained in terms of theory formation and
theory change.
Theories have been described with respect to their
structural, functional, and dynamic properties
(Gopnik & Meltzoff, 1997). Thanks to several decades
of work in developmental psychology, we now know a
great deal about the structural and functional aspects
of children’s theories. That is, in many domains, we
know that children have abstract, coherent, causal rep-
resentations of events, we know something about the
content of those representations, and we know what
types of inferences they support.
We know, for instance, that 6-month-olds’ naïve
physics includes principles of cohesion, continuity,
and contact but not the details of support relations
(Baillargeon, Kotovsky, & Needham, 1995; Spelke,
Breinlinger, Macomber, & Jacobson, 1992; Spelke,
Katz, Purcell, Ehrlich, & Breinlinger, 1994). We
know that 4-year-olds’ naïve biology supports infer-
ences about growth, inheritance, and illness but not
the adult concept of living thing or alive (Carey, 1985;
67
5
Learning From Doing
Intervention and Causal Inference
Laura Schulz, Tamar Kushnir, & Alison Gopnik
There is something fascinating about science. One gets such wholesale returns of conjecture out of
such a trifling investment of fact.
Mark Twain, 1883

Gelman & Wellman, 1991; Inagaki & Hatano, 1993;
Kalish, 1996). We know that 2-year-olds’ naïve psy-
chology includes the concepts of intention and desire
but not the concept of belief (Flavell, Green, &
Flavell, 1995; Gopnik & Wellman, 1994; Perner,
1991). Moreover, we know that, across domains, chil-
dren’s naïve theories support coherent predictions,
explanations, and even counterfactual claims (Harris,
German, & Mills, 1996; Sobel, 2004; Wellman,
Hickling, & Schult, 1997).
However, the theory theory is not just a theory
about what children know or what children can do. It
is, centrally, a claim about how children learn. In this
respect, it is the dynamic rather than the structural
and functional aspect of theories that is critical. If
children’s reasoning is like scientific theory forma-
tion, then children’s naïve theories should be subject
to confirmation, revision, and refutation, and chil-
dren should be able to make inferences based on evi-
dence from observation, experimentation, and
combinations of the two.
Until recently, this dynamic feature of theories has
been difficult to explain. If children’s knowledge
about the world takes the form of naïve theories—and
if conceptual development in childhood is analogous
to theory change in science—then we would expect
the causal reasoning of even very young children to be
very sophisticated. A causal “theory” (as distinct from,
for instance, a causal module or a causal script) must
support novel predictions and interventions, account
for a wide range of data, enable inferences about the
existence of unobserved and even unobservable
causes, and change flexibly with evidence (Gopnik &
Meltzoff, 1997). Moreover, theories have a complex
relationship with evidence; they must be defeasible in
the face of counterevidence, but they cannot be too
defeasible. Because evidence is sometimes misleading
and sometimes fails to be representative, the process
of theory formation must be at once conservative and
flexible.
In recent work, we have focused on causal learn-
ing as a fundamental dynamic mechanism underlying
theory formation. In thinking about what causal knowl-
edge is, we have been influenced by philosophical and
computational work proposing an “interventionist”
view of causation (see Woodward, Hitchcock, &
Campbell, chapters 1, 7, 4, this volume). This view
stands in contrast to many traditional ideas about cau-
sation in both adult and developmental psychology.
However, we believe that an interventionist account of
causation not only helps to elucidate tricky metaphys-
ical questions in philosophy but also provides a partic-
ularly promising way to think about children’s causal
knowledge.
As noted, much developmental research on causal
reasoning has looked at children’s understanding of
domain-specific causal mechanisms (Bullock,
Gelman, & Baillargeon, 1982; Leslie & Keeble, 1987;
Meltzoff, 1995; Shultz, 1982; Spelke et al., 1992;
Wellman et al., 1997; A. L. Woodward, 1998; A. L.
Woodward, Phillips, & Spelke, 1993). Although this
research tradition has successfully overturned Piaget’s
idea that young children are “precausal” (1930), it has
followed Piaget’s lead in treating knowledge of dis-
tinct physical and psychological mechanisms of
causal transmission as the hallmark of causal under-
standing.
Specifically, developmental researchers have
largely accepted the idea that causal knowledge
involves knowing that causes produce effects by trans-
fer of information or energy through appropriate
intervening mechanisms. In an influential mono-
graph on children’s causal reasoning, the psychologist
Thomas Shultz wrote that children understand 
causation “primarily in terms of generative transmis-
sion” (1982, p. 48). Similarly, Schlottman writes that
“mechanism is part of the very definition of a cause”
(2001, p. 112), and Bullock et al. (1982, p. 211) con-
clude that the idea that “causes bring about their
effects by transfer of causal impetus” is “central to the
psychological definition of cause-effect relations.”
Consistent with this causal mechanism or “genera-
tive transmission” approach, psychologists have sug-
gested that even adults prefer information about
plausible, domain-specific mechanisms of causal
transmission to statistical and covariation information
in making causal judgments (Ahn, Kalish, Medin, &
Gelman, 1995). Some philosophers have also adopted
a transmission perspective, arguing that causal interac-
tions are characterized by spatiotemporally continuous
processes involving the exchange of energy and
momentum or the ability to transmit “a mark” (Dowe,
2000; Salmon, 1984, 1998).
However, although the generative transmission
model of causation is arguably the dominant view of
causal knowledge in the developmental literature,
there are several respects in which this model criti-
cally fails to account for our causal intuitions. Many
events that we believe are causally connected (e.g.,
losing track of time and being late for class; taxing
68
CAUSATION AND INTERVENTION

cigarettes and reducing smoking) are not, at least in
any obvious way, characterized by mechanisms of
transmission. Second, as the philosopher Jim
Woodward observes, there is no obvious reason why it
should be of value to us to distinguish those events
that transmit energy or information from those that do
not (2003); those aspects of causality that make it of
central importance to human cognition (prediction
and control) do not seem to be captured by the con-
cern with spatial and energy relations that character-
ize the transmission view. Furthermore, nothing in
the generative transmission model distinguishes
causally relevant from causally irrelevant features of
transmission. Generative transmission models fail to
explain why, for instance, the momentum transferred
from a cue stick to a cue ball is causally relevant to the
ball’s movement, while the blue chalk mark, transmit-
ted at the same time and in the same manner, is not
(Hitchcock, 1995, and chapter 7, this volume).
Critically, the tendency to equate causal under-
standing with an understanding of mechanisms of
causal transmission may pose a particular problem for
the theory theory. Research suggests that adults can-
not generate a plausible account of causal mecha-
nisms, even in domains in which they consider
themselves highly knowledgeable (Rozenblit & Keil,
2002). Keil has suggested that we suffer from an “illu-
sion of explanatory depth,” and that our causal knowl-
edge may amount to little more than “one or two
connected causal beliefs” (2003). He has argued that
“calling this causal knowledge folk ‘science’ seems
almost a misnomer,” and that “the rise of appeals to
intuitive theories in many areas of cognitive science
must cope with a powerful fact. People understand
the workings of the world around them in far less
detail than they think.”
If having a theory is coextensive with having an
account of causal mechanisms, then Keil’s suggestion
is troubling, particularly because an impoverished
understanding of causal mechanisms is presumably
even more characteristic of young children than
adults. Perhaps children’s causal reasoning is not par-
ticularly sophisticated after all.
However, the interventionist account explicit in
recent philosophical work and implicit in computa-
tional models such as causal Bayes nets provides a
quite different account of what it might mean to have
causal knowledge. In the context of a causal model,
the proposition that X causes Y (X →Y) means, all
else being equal, that an intervention to change the
value or probability distribution of X will change the
value or probability distribution of Y. That is, the
causal arrows in the graphical models are defined, not
with respect to their relevance to a domain, their spa-
tiotemporal features, or their ability to transmit energy
or force, but (mirroring the way causality is under-
stood in science) in terms of possible interventions.
These interventions need not actually be realized or
even feasible, but they must be conceivable (see 
J. Woodward, 2003, for details). A causal relation then
is defined not in terms of its physical instantiation but
in terms of the real and counterfactual interventions
it supports. A theory, in this view, represents a coher-
ent and organized set of such relations rather than
necessarily involving a set of beliefs about physical
processes or mechanisms.
Both statisticians and philosophers have argued
that this interventionist account captures precisely
what it means for a variable to be a cause (see, 
e.g., Pearl, 2000, and J. Woodward, 2003). Learning 
algorithms based on these models support novel pre-
dictions, interventions, inferences about a range of
causal structures, and inferences about unobserved
causes. Arguably, then, knowledge of causal mecha-
nisms and processes of transmission may not be of
central importance for at least some of what we need
theories to do.
Note, moreover, that an interventionist account of
causal learning is consistent with and indeed predicts
many of the findings that have been associated with
the generative transmission model. In looking, for
instance, at children’s inferences about force relation-
ships, Shultz (1982) first taught the children what
types of interventions were relevant to outcomes (e.g.,
that striking a tuning fork in front of an open box cre-
ated a sound). Shultz then struck two tuning forks; the
first failed to covary temporally with the sound
(because it was positioned to the side of the box); the
second did covary with the effect (because the exper-
imenter struck the second fork and simultaneously
turned the box to face the first). Children chose the
first tuning fork (with the appropriate transmission
relationship) as a cause and rejected the tuning fork
that merely covaried with the effect.
However, the relevant covariation information for
children might not be merely the temporal covariation
of the tuning fork with the effect but the covariation
of interventions and outcomes; that is, children could
have learned that turning the box was as critical to the
effect as striking the fork. Indeed, in novel cases like
LEARNING FROM DOING
69

this, arguably the only information that children have
about processes of causal transmission is the evidence
of effective patterns of intervention. Given that any
causal relationship (e.g., flipping a switch and a light
turning on) can be instantiated by a vast number of
causal mechanisms (many types of wires, bulbs, cir-
cuits, etc.), it may make sense that children’s naïve
theories should focus on the connection between
interventions and outcomes rather than on the myriad
mechanisms that might realize it. Indeed, one of the
virtues of theories may be that they enable us to make
powerful predictions despite our often-substantial
ignorance about underlying processes and mecha-
nisms (our “trifling investment in fact”).
Note that scientific theories, as well as naïve ones,
often remain agnostic about processes of transmission
while committing to hypothetical interventions.
Newton developed his theory of gravitation without
knowing any mechanism that might enable masses to
attract one another; Darwin developed his theory of
evolution without knowing any mechanism that
might make variation in the species heritable. Thus,
although we might say informally that Darwin posited
natural and sexual selection as “mechanisms” for evo-
lution, we do not mean that Darwin discovered spa-
tiotemporally continuous processes by which energy
or information is transferred. Rather, Darwin inferred
that traits that enhance an organism’s reproductive
success will be more prevalent in the population; that
is, changes to one set of variables will affect the out-
come of other variables. Thus, scientific theories, like
naïve ones, are not necessarily derived from, or com-
mitted to, particular causal mechanisms. Rather, in
identifying the causal structure—the real and hypo-
thetical interventions the variables support—theories
help narrow the search space for the relevant physical
processes.
Critically, we do not mean to suggest that substan-
tive assumptions about spatiotemporal relations and
domain-specific knowledge do not play a fundamental
role in children’s causal understanding. Indeed, one of
the important challenges for cognitive science is to
understand how knowledge about particular physical
relations in the world is integrated with evidence
about interventions and patterns of covariation. In
what follows, we discuss some important interactions
between children’s substantive causal knowledge and
formal learning mechanisms. Even more critically,
we do not mean that children only learn causal rela-
tions from interventions. Children may infer causal
relations in myriad ways, including from spatial
relations, temporal relations, patterns of covariation,
and simply by being told. The claim rather is that cer-
tain patterns of interventions and outcomes indicate
causal relationships, and when children infer that a
relationship is causal, they commit to the idea that
certain patterns of interventions and outcomes will
hold.
One of the exciting features of the interventionist
account of causation is that, together with theory the-
ory, it generates an array of interesting and testable
predictions about children’s early learning. At a mini-
mum, if children’s causal knowledge takes the form of
naïve theories and if causal knowledge is knowledge
that supports interventions, then children should be
able to (a) use patterns of evidence to create novel
interventions; (b) do this for any of a variety of possi-
ble causal structures; (c) use evidence from interven-
tions to infer the existence of unobserved causes; 
(d) distinguish evidence from observation and inter-
vention in their inferences about causal structure; 
(e) effectively weigh new evidence from interventions
against prior beliefs; and (f) distinguish good interven-
tions from confounded ones.
In what follows, we walk through this alphabet of
inferences. We discuss respects in which the causal
Bayes net formalism provides a normative account of
these components of theory formation, and we review
evidence from our lab suggesting that young children
are capable of this type of learning.1
Making Novel Interventions
In the absence of theories, you could safely navigate a
lot of causal territory. Classical conditioning, trial-
and-error learning, and hardwired causally significant
representations (of the sort that make nestlings cower
when hawks fly overhead, and arguably of the sort that
is triggered by seeing one object strike and displace
another; e.g., Michotte, 1962) are effective ways of
tapping into real causal relations in the world. Each
of these abilities lets us track regularities in the envi-
ronment and predict some events from the occur-
rence of others. Some of these abilities even support
effective interventions.
Like other animals, human beings seem to have
innate, domain-specific causal knowledge (Spelke et al.,
1992), the ability to detect statistical contingencies
(Saffran, Aslin, & Newport, 1996), and the ability to
70
CAUSATION AND INTERVENTION

learn from the immediate consequences of our own
actions (Rovee-Collier, 1980; Watson & Ramey, 1987).
Unlike other animals, however, we routinely use the
contingencies and interventions we observe to design
novel interventions. We routinely meet regularities with
innovation.
Some of this inferential power may come from the
way that human beings represent causal knowledge.
Elsewhere (see Gopnik et al., 2004), we have sug-
gested that causal Bayes net representations provide a
causal map of events in the world. The analogy to a
spatial map is helpful because it explains both some
of the advantages of the causal Bayes net representa-
tion and some of the disadvantages of alternative ways
of storing causal knowledge.
Some animals, like ants, seem to represent spatial
relations egocentrically. Ants know where their nest is
in relation to their own body movements, but if they
are scooped up and displaced even slightly, they lose
their way, even in familiar terrain (Sommer &
Wehner, 2004). Other animals, like mice, construct
spatial maps. Once mice have explored a territory,
they can always take the shortest route to a goal, no
matter where they are placed initially (Tolman, 1932).
Such cognitive spatial maps reveal the underlying sta-
bility of geometric relations.
Causal relations can also be represented egocentri-
cally in terms of the immediate outcome of one’s own
actions (e.g., as in operant learning). However, like an
egocentric spatial representation, operant learning
fails to represent the relationship of variables to one
another. Operant learning restricts you to learning the
immediate outcome of your own actions, and even
these can only be learned by trial and error. However,
if you represent causal events as they relate to one
another, then—even if you are not part of the causal
structure, or even if you own relationship to the event
changes—the stability of the underlying causal struc-
ture is preserved. From such stability may come the
ability to negotiate novelty.
Causal Bayes nets provide just such a coherent,
nonegocentric representation of the causal relation-
ship among events. In a literature rife with stories
about cigarette smoking, stained fingers, and lung
cancer; birth control pills, thrombosis, and strokes;
and prisoners, sergeants, and firing squads, almost
any concept can be illustrated with a macabre exam-
ple. We work with preschoolers, however, so we make
use of a more benign, indeed suburban, illustration
(adapted from Pearl, 2000): Suppose you walk
outside and see that the grass in your front yard is wet.
You might guess that it has rained. Because you
believe the weather is a common cause of the state of
your front yard and your backyard, you will be able to
infer that the grass in your backyard is most likely wet
as well. You could represent this causal structure as
the causal Bayes net in Figure 5-1, in which each
node is a binary variable taking either the value wet
or dry.
In this causal structure, the state of the front yard
and the state of the backyard are dependent in proba-
bility. Knowing something about the front yard will
tell you (in probability) something about the state of
the backyard. That is, you can use knowledge of the
causal graph and the known value of some variables
in the system to predict the (otherwise unknown)
value of other variables.
However, the critical thing about causal Bayes
nets, indeed the thing that makes them causal, is that
they can also support inferences about the effects of
interventions. We discuss interventions in more detail
in the following section, but roughly speaking, the
arrow in the graph between the weather and the front
yard encodes the proposition that, all else being
equal, changing the state of the weather will change
the state of the front yard. Importantly, the arrow
retains this meaning even though (in the real world)
we cannot actually intervene on the weather (short of
global climate change, anyway). Knowing the causal
graph lets you predict the outcome of interventions—
whether or not you have ever seen them performed
and indeed whether or not you could ever perform
them. Thus, unlike hardwired representations or trial-
by-error learning, causal graphs support genuinely
novel inferences.
However, the absence of the arrow between the
front yard and the backyard is also informative.
Although the states of the yards are dependent in
probability, there is no direct causal link between
them; all else being equal, changing the one will not
change the other. Causal graphs thus represent the
LEARNING FROM DOING
71
FIGURE 5-1 A causal Bayes net.

distinction between predictions from observation (if
the front yard is dry, then the backyard is probably dry
as well) and predictions from intervention (wetting
the front yard will not wet the backyard).
In a series of experiments, we looked at whether,
consistent with the formalism, young children could
use patterns of dependence and independence to
make novel predictions and interventions (Gopnik,
Sobel, Schulz, & Glymour, 2001; Schulz & Gopnik,
2004). We showed preschoolers, for instance, that
three flowers were associated with a monkey puppet
sneezing (see Figure 5-2). One flower (A) always
made the monkey sneeze; the other flowers (B and C)
only made the monkey sneeze when Flower A was
also present.
Formally, A and the effect were unconditionally
dependent; B, C, and the effect were independent
conditional on A. Applied to this case (and assuming
no unobserved common causes), a Bayes nets learn-
ing algorithm will construct the graph in Figure 5-3\
The graph in Figure 5-3 says that A causes the effect,
and B and C do not. (It also says that there is an unde-
termined causal link between A, B, and C, repre-
sented by the circles and the ends of the edge
connecting those variables. In fact, there is such a
link, namely, the experimenter, who put all three
flowers in the vase together.) This structure in turn
generates predictions about interventions. In particu-
lar, it implies that an intervention on A will change
the value of C, but an intervention on B or C will not
have this effect.
Children were asked, “Can you make it so that
Monkey won’t sneeze?” Consistent with the predic-
tion of the formalism, children screened-off flowers B
and C and removed only flower A from the vase.
Control experiments established that the inference
was caused by the pattern of conditional dependence
and independence, not frequency information.
One might argue, however, that children have
only a limited ability to make novel and appropriate
inferences. Children might, for instance, be able to
use patterns of dependence to differentiate equally
plausible causal candidates within a domain (i.e., the
causal power of one flower vs. another). However,
innate or domain-specific knowledge might restrict
the range of evidence children are willing to consider
in the first place. Formal inference procedures might
not be able to override or change children’s prior
beliefs.
However, if, consistent with the theory theory,
children develop their causal understanding from pat-
terns of evidence, then domain-specific judgments
ought to be defeasible. Given appropriate evidence,
children ought to be able to override prior knowledge
and reason about truly novel events, including events
that cross the boundaries of domains, and design truly
novel interventions accordingly. To look at the extent
to which children could flexibly use evidence and 
formal inferential procedures to make genuinely
novel causal inferences, we pitted children’s domain-
specific knowledge against patterns of evidence.
We showed children, for instance, that three
causes were associated with a machine turning on.
Two of the causes were domain appropriate (but-
tons), and one was domain inappropriate (talking to
the machine). Talking to the machine and the
machine turning on were unconditionally dependent
but conditional on talking; the buttons were inde-
pendent of the effect. Thus, the structure was for-
mally identical to the structure in Figure 5-3. We
asked the children if they could turn off the machine.
In a baseline condition, we provided children with
no evidence and simply asked the children whether
72
CAUSATION AND INTERVENTION
Ahchoo!
Ahchoo!
FIGURE 5-2 Evidence about three flowers.
FIGURE 5-3 Graph representing inference that
Flower A screens off B and C as a cause of E.

talking or pushing buttons was more likely to turn off
the machine.
Consistent with past research showing that chil-
dren’s causal inferences respect domain boundaries,
children in the baseline condition chose the domain-
appropriate causes (the buttons) at ceiling. However,
consistent with the predictions of the formalism,
when asked to turn off the machine, 75% of the chil-
dren ignored the buttons and said, “Machine, please
stop.” Children were able to use the pattern of condi-
tional dependence and independence to create a new
causal map and to generate an appropriate, but novel,
causal intervention.
In this experiment, the relations between causes
and effect were deterministic. Such definitive evi-
dence might have made it particularly easy for chil-
dren to override their prior knowledge. However, in
another experiment (Kushnir, Gopnik, & Schaefer,
2005), we tested whether children’s domain-specific
preference for contact in physical causal relations
could be overridden in light of probabilistic evidence
that physical causes could act at a distance. We
showed children a toy with a colored surface and told
them, “Sometimes the toy lights up.” Without further
instruction, we gave children a block and asked them
to make the toy light up. Of 16 children, 13 (81%)
demonstrated a strong initial assumption of contact
causality, touching the block to the surface of the toy
(the other 3 did nothing). After their intervention, we
showed children four pairs of blocks. In each pair,
one block activated the toy one third of the time and
always by contact. The other block activated the toy
two thirds of the time and always at a distance (i.e.,
by being held 5–6 inches above the toy). At the end
of the experiment, we asked children to make the toy
light up again. A significant number of children
revised their original intervention and activated the
toy at a distance (McNemar’s test, p  .05). Thus,
children seem to be able to revise their domain-spe-
cific knowledge and create novel interventions, even
when given only stochastic evidence for new causal
relations.
If children’s causal reasoning were constrained by
innate representations or informationally encapsulated
modules, then such flexibility and sensitivity to evi-
dence would be surprising. However, it is less surprising
from a theory theory perspective. The ability to over-
turn prior knowledge and learn something genuinely
new is one of the chief virtues of scientific inquiry. It
may also be one of the hallmarks of childhood.
Learning a Wide Range of Causal
Structures
If you were a Martian reading much of the classic liter-
ature on human causal reasoning, then you might
assume that Earth was a relatively simple place. The
stakes are sometimes high (Does camouflage protect
tanks from being blown up? Does gender affect college
admissions? Does medication cause headaches? Baker
et al., 1989; Bickel, Hammel, & O’Connell, 1975;
Novick & Cheng, 2004), but the questions, at least, are
straightforward: Given a particular set of evidence, is C
a cause of E?
Many theories have tried to explain how people
answer this question. Accounts ranging from the asso-
ciative learning accounts we discussed to Patricia
Cheng’s elegant power theory of probabilistic contrast
(Cheng, 1997; Novick & Cheng, 2004) have looked
at how people might estimate the relative strength (or,
uniquely in Novick & Cheng, 2004, the conjunctive
strength) of variables to produce an outcome.
However, both the question and the ways we
might answer it assume that variables in the world are
already identified as (potential) causes or as effects. A
Martian might reasonably wonder whether events on
Earth come with labels. The question does not ask,
and the theories do not answer, how we might distin-
guish causes from effects in the first place. Put
another way, both associative learning accounts and
the power theory account aim to explain how people
distinguish the strength of different causal variables.
They do not explain how people make judgments
about causal structure.
Sometimes, of course, events in the world are
essentially “labeled” by the information around them.
Spatial cues, combined with prior knowledge about
plausible causal mechanisms, may identify some vari-
ables as potential causes and others as effects. In other
cases (not coincidentally including camouflage and
explosions, gender and college admissions, medicine
and headaches), temporal priority makes the distinc-
tion transparent (Lagnado, Waldmann, Hagmayer, &
Sloman, chapter 10, this volume).
However, spatiotemporal cues are not always avail-
able in the input. If cause and effect occur at nearly
the same time (the dog barks, and the cat runs) or if
you walk in on the middle of a scene (brother is sulk-
ing, and sister is mad), there may be no way to know
“who started it.” Moreover, even when temporal cues
are present, they may be misleading. A naïve learner
LEARNING FROM DOING
73

who sees Mom search under the bed and then
exclaim with joy on finding her car keys might be jus-
tified in concluding that searching caused Mom to
want her keys rather than that desire motivated the
search.
More critically, any theory (naïve or scientific)
requires knowing something more than the set of binary
relations (does X cause Y?) that obtain between events.
A prerequisite to theory formation must be the ability
not only to distinguish the strength of causal variables,
but also to organize variables within a causal structure.
Indeed, part of what differentiates a theory from an
empirical generalization is that, within a theory, causal
relations are coherent and mutually reinforcing.
The causal Bayes net formalism provides a way to
represent and learn complex, coherent causal struc-
tures without prior knowledge about whether variables
are causes or effects. Although the formalism can
incorporate background information from prior knowl-
edge, substantive cues, and temporal order (see the sec-
tion on weighing new evidence against old beliefs), the
direction of causal arrows can also be derived directly
from the patterns of conditional dependence and inde-
pendence in the data. Some structures can be distin-
guished by observation only; others require a
combination of observation and interventions.
Suppose, for instance, that you see three correlated
events and are trying to decide whether A and B cause
C or whether C causes A and B. If the causal structure
is a common effect (A →C ←B), then you are more
likely to see A and C co-occur and B and C co-occur
than to see A and B co-occur. However, if the structure
is a common cause (A ←C →B), then you are likely
to see all three variables co-occur. B will be independ-
ent of A conditional on C in the common cause case
but not the common effects case. These structures can
be distinguished just by observation.
The situation is more complex if you are trying to
distinguish other structures. For example, suppose you
are trying to distinguish the common cause structure
(A ←C →B) from the causal chain (A →C →B). In
the common cause structure, if C occurs exogenously,
then it will activate both A and B, and you will tend to
see all three variables together. Similarly, in the chain,
if A occurs exogenously, then it will activate C, which
will activate B, and again you are likely to see all three
variables co-occur. In both cases, B is independent of A
conditional on C. Such Markov-equivalent structures
are indistinguishable under observation. However,
these structures can be distinguished by intervention. 
If you intervene to make C happen, then you will
increase the probability of seeing A and B if the struc-
ture is a common cause (A ←C →B) but will have no
impact on the probability of observing A if the structure
is a chain (A →C →B) (see Steyvers, Tenenbaum,
Wagenmakers, & Blum, 2003, for discussion and evi-
dence that adults are sensitive to these distinctions).
Given a combination of evidence from observation and
intervention, the causal Bayes net formalism allows for
learning the structure even of complex, multivariable
systems.
Within the formalism, interventions are treated as
variables with special features. Specifically, they must
be exogenous (that is, they must not be influenced by
any other causal factors in the graph), and they must
fix the value or probability distribution of the variables
of interest. After an intervention, the value of the
intervened-upon variable is entirely determined by
the intervention and not by any preexisting causes
(see Figure 5-4). Thus, interventions on a causal
Bayes net break arrows into the variables of interest,
performing what Judea Pearl vividly described as
graph surgery (2000). We can then look at the “post-
surgical” graph (after the intervention has taken
place) and figure out what has happened to the other
variables in the graph.
There are several different ways of formally captur-
ing these relations between interventions, dependen-
cies, and causal arrows (see Pearl, 2000; Spirtes,
Glymour, & Scheines, 1993; J. Woodward, 2003).
One way to do this is in terms of what we have called
the conditional intervention principle. The condi-
tional intervention principle can be formally stated as
follows: For a set of variables in a causal graph, A
directly causes B (that is, A →B) if and only if
(a) there is some intervention that fixes the values of
all other variables in the graph, results in B having a 
particular probability distribution pr(Y) such that 
(b) there is another intervention that changes the value
of A, (c) changes the probability distribution of B from
pr(B) to pr′(B) but (c) does not influence B other than
through A, and (d) does not undo the fixed value of the
other variables in the graph (Gopnik et al., 2004).
74
CAUSATION AND INTERVENTION
FIGURE 5-4 (a) A causal chain. (b) A causal chain
after graph surgery; the intervention on Y breaks the
arrow between X and Y.
(a)
(b)

Although this principle may sound complex, it is
simply a formal statement of the sort of intuitions
about intervention and causation that underlie exper-
imental design. In an experiment, if you want to find
out the causal relationship between two variables,
then you intervene to hold all other variables con-
stant, and then you intervene to manipulate the value
of the variable of interest. If, for instance, you want 
to know the causal relationship between A and B
(represented by an arrow with a question mark in
Figure 5-5a), then you can perform one intervention
(I1 in Figure 5-5a) to hold all other potential causes of
B constant and another intervention to change the
value of A (I2 in Figure 5-5a). If the value (or proba-
bility distribution) of B changes, then you can con-
clude that A causes B.
Note also that the conditional intervention princi-
ple rules out confounded interventions. Line 4 of the
conditional intervention principle eliminates the
graph in Figure 5-5b (because the intervention on A
cannot influence B except through A), and Line 5
rules out the confounded graph in Figure 5-5c
(because interventions cannot change the fixed value
of any other variable in the graph).
Motivated by causal Bayes net theory (in this 
volume, see also Hagmeyer, Sloman, Lagnado, &
Waldmann, chapter 6; Lagnado et al., chapter 10;
Rehder, chapter 12; Griffiths & Tenenbaum, chapter
20), researchers have shown that adults can make
appropriate inferences about a wide range of causal
structures beyond simple cause-effect pairings.
Importantly, the evidence suggests that causal strength
learning (and subsequent inferences) can and does
take place in the context of complex causal models.
For example, Waldman (2000, 2001) has shown that
adults are sensitive to the direction of causal arrows
when learning and reasoning about causal strength
relations; that is, they make the distinction between
predictive and diagnostic inferences, a fact that
cannot be predicted based on associative learning
mechanisms alone. Other studies (Lagnado et al.,
chapter 10, this volume; Sloman & Lagnado, 2005;
Waldman & Hagmayer, 2005) have shown that,
given causal models, adults can make inferences
about the effects of hypothetical interventions as
well. Thus, psychologically, causal strength judg-
ments do not take place outside the context of causal
structures.
All this should satisfy a Martian that adult humans
can make appropriate predictions about observations
and interventions in a broader causal context. But, of
course, adult humans, particularly the university
undergraduates tested in these studies, have extensive
experience and often quite explicit tuition in causal
inference. Moreover, for the most part these studies
have focused on making inferences about evidence
given knowledge of a particular structure rather than
learning structure from evidence. These studies do
not tell us whether this sort of causal learning is part
of a more fundamental human learning mechanism
and in particular whether it might be responsible for
the impressive learning we see in young children.
Conversely, the studies of children we have just
described all presented them with the classical prob-
lem of inferring which cause was responsible for a
particular effect—which blicket set off the detector,
which flower made the monkey sneeze. In principal,
these results might be explained by variations of earlier
theories such as associationism or causal power theory.
Studies so far have not tested explicitly whether adults
or children can use the conditional intervention 
principle to make inferences about complex causal
structures, as the Bayes net formalism would suggest.
In the absence of distinguishing spatiotemporal infor-
mation, can children use evidence from observations
and interventions to learn the structure of causal
chains, common effects, common causes, and causal
conjunctions?
To find out, we introduced preschool children
(mean age 4 years 6 months) to a gear toy. Children
saw that, when a switch was flipped, two gears, A and B,
spun simultaneously. There were four possibilities: 
(a) The switch activated gear A and A made B go; 
(b) the switch activated gear B and B made A go; 
LEARNING FROM DOING
75
FIGURE 5-5 Graphs illustrating the conditional inter-
vention principle (a) I1 fixes the value of other causes
of B (Clause 1 of the conditional intervention princi-
ple). I2 changes the value of A (Clause 2 of the condi-
tional intervention principle). (b) I* is ruled out by
Clause 4 of the intervention principle because the
intervention affects the value of B directly. (c) I* is
ruled out by Clause 5 of the intervention principle
because the intervention affects other causes of B.
(a)
(b)
(c)

(c) the switch activated each gear independently; or 
(d) the switch activated the gears but neither gear
would spin without the other. Note that these struc-
tures are indistinguishable under observation; no
matter which structure obtains, when you flip the
switch, both gears will spin together.
The structures, however, are distinguishable under
intervention. If, for instance, you remove gear B, flip
the switch on, and gear A spins, then you can elimi-
nate structures (b) and (d). If you replace gear B,
remove gear A, flip the switch on, and gear B fails to
spin, then you can eliminate structure (c) and infer
that structure (a) is correct. This type of inference is a
direct application of the conditional intervention
principle. Controlling for other causes of A (the state
of the switch), an intervention on A changes the value
of B (when the switch is on and A is present, B spins;
when A is absent, B does not), whereas controlling for
other causes of B, an intervention on B does not
change the value of A. You should conclude that
structure (a) is correct, and A →B. Because the pat-
terns of evidence under intervention are unique to
each structure, the correct structure can be deter-
mined from the data that result from interventions.
Over a series of experiments, we found that, con-
sistent with the formalism, 4.5-year-olds were able to
learn the correct causal structure, represented by a
simple picture, from the type of evidence described.
Children were equally good at learning all four struc-
tures (the two chains, the common effect, and the
conjunction). In each case, when children were pre-
sented with the appropriate evidence, they chose the
correct structure significantly more often than any of
the other structures. Control experiments suggested
that children’s judgments were not based on substan-
tive cues or prior knowledge about gears. In addition,
consistent with the data reported in the section on
making novel interventions, children were able to use
their knowledge of the causal structure to make novel
predictions. Children who had never seen gears A and
B on the toy but were told the structure (e.g., that A
spun B) were able to predict the evidence that would
result from interventions (e.g., that when the switch
was on and A was on the toy by itself, A would spin,
but that when B was on by itself, B would not). Again,
children were equally good at predicting the out-
comes of interventions for all four structures (Schulz,
2003; Schulz, Gopnik, & Glymour, in press).
These experiments are particularly noteworthy
because they were explicitly inspired by the Bayes net
formalism and are not explicable by any other existing
theory of causal learning. The physical and mechani-
cal features of the gears were identical in all cases,
and the associations and covariations between the
gears were also held constant. The complex pattern of
relations between interventions and observations
allowed children to learn complex causal structure—
in just the way the formalism would suggest.
In their everyday life, children intervene widely on
the world and see a wide range of interventions per-
formed by others. At least in simple, generative, deter-
ministic cases, preschool children seem to be able to
infer a range of different causal structures from pat-
terns of evidence and to predict patterns of evidence
from knowledge of causal structure. Even young chil-
dren seem to rely on some of the same formal princi-
ples of causal inference that underlie scientific
discovery. Such mechanisms may help children to
develop intuitive theories of the world around them.
Inferring the Existence of Unobserved
Causes
One of the critical respects in which science some-
times brings us genuinely new insight is by invoking
unobserved causes to explain events. However, unob-
served causes are not the exclusive provenance of sci-
entific theories. Children’s naïve physics relies on
unobservable forces, children’s naïve psychology on
unobservable mental states, and children’s concept of
natural kinds on unobservable essences (e.g., Bullock
et al., 1982). It is thus perhaps surprising that most
psychological accounts of causal reasoning (Cheng,
1997; Shanks & Dickinson, 1987) relegate unob-
served causes to a background condition.
We already discussed respects in which the causal
Bayes net formalism supports inferences about the
unknown value of some variables from the known
value of others. However, in some cases the formalism
supports inferences about the existence of variables
themselves. In particular, if the known values in the
graph generate patterns of conditional dependence
and independence that appear to violate the causal
Markov assumption, then the formalism infers the
existence of an unobserved cause.
In a series of experiments, participants (both adults
and children) were introduced to a “stickball
machine” (see Figure 5-6). The two stickballs could
move up and down (either simultaneously or inde-
pendently) without any visible intervention (because
76
CAUSATION AND INTERVENTION

they could be manipulated from behind the
machine). The experimenter could also visibly inter-
vene on a stickball by pulling up on the stick. This
might cause—or fail to cause—the other stickball to
move.
We looked at whether, consistent with the causal
Markov assumption, adults and kindergarteners could
use interventions and the pattern of outcomes to infer
the existence of an unobserved common cause. In
these studies, participants saw that the movement of the
two stickballs was correlated in probability. They then
saw that an intervention on Stickball A (pulling on A)
failed to move B, and that an intervention on B failed
to move A. On comparison trials, participants were
given evidence consistent with A →B (e.g., they saw
that pulling on B failed to move A, but they did not see
an intervention on A).
If the movements of A and B are probabilistically
dependent but intervening to do A fails to increase the
probability of B moving and intervening to do B fails
to increase the probability of A moving, then the
causal Markov assumption can be preserved only by
inferring the existence of an unobserved common
cause of A and B (i.e., that the true causal structure is
A ←U →B). This structure predicts the observed evi-
dence: A and B are unconditionally dependent in
probability, but an intervention on either A or B
breaks the dependence.
Consistent with the formalism, both adults and
children inferred the existence of an unobserved
common cause when interventions on either stick-
ball failed to correlate with the movement of the
other. Adults drew the appropriate graph (A ←U →B);
children inferred that “something else” (besides
either of the stickballs) was making the stickballs
move (Kushnir, Gopnik, Schulz, & Danks, 2003;
Schaefer & Gopnik, 2003). Importantly, participants
only postulated an unobserved common cause when
no other graph was consistent with the observed pat-
tern of dependencies. The causal Bayes net formal-
ism thus provides a mechanism by which evidence
about observed variables can lead to inferences about
the existence of unobserved variables. Processes like
these might help explain how both children and 
scientists bring new theoretical entities into the
world.
Distinguishing Evidence From
Observations and Interventions
At the core of the theory theory is the idea that chil-
dren learn causal structure from evidence. There are
two ways we can get (firsthand) evidence about an
event: We can see the event happen, or we can make
the event happen. Importantly, as we have implied in
the previous sections, these two ways of getting
data—seeing and doing—can lead to radically differ-
ent conclusions, even when the evidence itself is oth-
erwise identical. What you can learn depends not
only on what you already know, but also on how you
know it.
In the section on making novel interventions, we
discussed a simple causal graph in which the weather
was a common cause of the state of the front yard and
the backyard (F ←W →B). We noted that, using this
graph, you could predict the state of the backyard
from the state of the front yard.
Suppose, however, that you buy a sprinkler for
your front yard and set it to go off every morning at 
6 a.m. Setting the sprinkler cuts the arrow between
the weather and the front yard and breaks the depend-
ence between the front yard and the backyard. The
altered graph is shown in Figure 5-7.
If the graph is as depicted in Figure 5-7, then
when you look outside and see that the grass in your
front yard is wet, you will not be able to infer that the
grass in your backyard is also wet. Evidence that was
informative under observation is uninformative under
this intervention.2
One of the strengths of the causal Bayes net for-
malism is that it supports accurate inferences whether
the evidence comes from observations, interventions,
or combinations of the two. Because the causal graph
under intervention is different from the graph under
LEARNING FROM DOING
77
Child’s View
Back View
FIGURE 5-6 The stickball machine.

observation, the same evidence should lead to different
inferences.
The theory theory implies that young children
should be sophisticated causal reasoners. Are children
also sensitive to the distinction between evidence
from observations and evidence from interventions,
and do they modify their inferences accordingly?
Note that such sensitivity is not predicted by all mod-
els of causal reasoning. Accounts of causal reasoning
that use the strength of the association between two
variables as indicative of the probabilistic strength of
the causal connection between them (see, e.g.,
Dickinson, Shanks, & Evenden, 1984; Shanks, 1985;
Shanks & Dickinson, 1987; Wasserman, Elek,
Chatlosh, & Baker, 1993) are indifferent to whether
the association is caused by intervention or observa-
tion. Because of this, the predictions made by causal
variants of the Rescorla-Wagner equation and the
causal Bayes net formalism sometimes differ.
In a series of experiments (designed primarily to
look at children’s ability to distinguish common cause
structures from causal chains), we looked at whether
children’s conclusions changed depending on
whether they observed the relevant evidence with or
without an intervention. Children were introduced to
the stickball machine described in the section on
inferring the existence
of unobserved causes.
Children were told the following: “Some stickballs
are special. Special stickballs almost always make
other stickballs move.” Children were taught that one
stickball might be special, both stickballs might be
special, or neither stickball might be special.
In the test condition, children saw the stickballs
move up and down simultaneously (without an inter-
vention) three times. The experimenter then visibly
intervened by pulling on the top of one stickball; the
other stickball failed to move. In the control condi-
tion, the experimenter intervened by pulling on one
stickball, and both stickballs moved simultaneously
three times. The experimenter then pulled on the
stickball a fourth time, and the other stickball failed to
move. At the end of the trials, the experimenter
pointed to each stickball and asked, “Is this stickball
special?”
In the test condition, there is a correlation
between seeing stickball Y move and seeing stickball
X move. However, intervening to move Y breaks the
dependence. From a causal Bayes net perspective,
this pattern of evidence is consistent with the graph 
X →Y but not with the graph Y →X. Children
should say that X is special but deny that Y is special.
In the control condition, intervening on Y and seeing
X move are probabilistically dependent throughout.
This is consistent with Y →X but not X →Y; children
should say that Y is special, and X is not.
Note, however, that from an associative learning
perspective, the strength of association between the
stickballs is the same in both conditions. The move-
ment of stickball Y is associated with the movement of
stickball X every time but one. If children are reason-
ing associatively, then in both conditions they should
say that Y is special.
The 
children 
(4.5-year-olds) 
distinguished
between evidence from observations and interven-
tions and reasoned not as predicted by associative
learning models, but as predicted by the causal Bayes
net formalism. That is, children were significantly
more likely to affirm that X was special and deny that
Y was special in the test condition than in the control
condition and significantly more likely to affirm that
Y was special and deny that X was special in the con-
trol condition than in the test (Gopnik et al., 2004;
Schulz, 2001).
Similarly, in the unobserved cause studies discussed
in the preceeding section, we reported that participants
saw that intervening to move stickball X failed to move
stickball Y, and intervening to move Y failed to move X.
In control conditions, however, participants saw X
move by itself and Y move by itself, but this time the
stickballs moved without visible intervention—the
78
CAUSATION AND INTERVENTION
FIGURE 5-7 A causal Bayes net with a sprinkler.

experimenter simply pointed at X when it moved by
itself and then pointed at Y while it moved by itself.
Consistent with the predictions of the formalism, par-
ticipants distinguished between the two conditions and
only inferred the existence of an unobserved common
cause of X and Y (X ←U →Y) in the intervention con-
dition. (In the observation condition, they inferred the
existence of two independent unobserved causes: 
U1 →X and U2 →Y.)
Pearl writes that, “Scientific activity, as we know it,
consists of two basic components: Observations and
interventions. The combination of the two is what we
call a laboratory” (2000). Although making inferences
about stickball machines may seem a far cry from sci-
entific inquiry, the ability to distinguish evidence
from observations and interventions is fundamental to
both. Sensitivity to the different role played by these
“basic components” may help support children’s abil-
ity to learn the causal structure of events in the world.
Weighing New Evidence Against 
Old Beliefs
We reported that preschoolers ignored a machine’s
buttons and asked a machine to stop after seeing—
once—that talking and the toy activating were uncon-
ditionally dependent. We reported this as partial proof
of the cleverness of 4-year-olds. This might worry you.
This might also worry our institutional review board.
Are preschoolers unreasonably impressionable?
Surely, it is not that clever to override the whole of
naïve physics on the evidence of a single trial.
Surely—even in Berkeley—we do not want children
going around talking to machines. Learning flexibly
from evidence is all very well, but can causal Bayes
nets run amok?
Well, no—at least not in this respect. Causal Bayes
net representations can be inferred by a variety of dif-
ferent learning algorithms discussed, such as con-
straint-based and Bayesian learning algorithms (see
Gopnik et al., 2004, for discussion). Both of these algo-
rithms can take prior knowledge into account.
Constraint-based algorithms test pairs and triads of vari-
ables for independence and conditional independ-
ence. By adjusting the significance level of the
statistical test used to deter-mine independence, con-
straint-based methods ensure that variables likely to be
independent based on prior knowledge (e.g., talking
and a machine activating) are subject to less-rigorous
tests of independence than variables that, given prior
knowledge, are less likely to be independent.
A somewhat more elegant approach is adopted by
Bayesian causal learning methods. Bayesian algo-
rithms assign all the possible causal hypotheses (the
causal graphs) a prior probability. This probability is
then updated given the actual data (by application of
Bayes theorem). The posterior probability of each
causal graph is evaluated to see which model best fits
the data. Thus, it will take more evidence to support
an initially unlikely causal hypothesis than an initially
probable one.
Several studies show that, under conditions of
uncertainty, people do take current evidence and
prior knowledge into account as predicted by
Bayesian learning algorithms (Griffiths & Tenenbaum,
2001; Tenenbaum & Griffiths, 2003, Griffiths &
Tenenbaum, chapter 20,  this volume). In one study,
for instance, adults were taught that “superpencils”
would activate a “superlead” detector. During a train-
ing period, adults were taught that superpencils were
either rare (2 of 12 pencils activated the detector) or
common (10 of 12 activated the detector). Two (pre-
viously untested) pencils were then placed on the
detector, and adults saw that both pencils (A and B)
together activated the detector, and that A by itself
activated the detector. The adults were asked to esti-
mate the likelihood that B by itself would activate the
machine.
As predicted by the Bayesian learning algorithms
(but not as predicted by associative learning
accounts), prior knowledge about the prevalence of
superpencils affected people’s causal judgments.
Despite seeing identical evidence about B in both
conditions, participants believed B was much more
likely to activate the machine in the common condi-
tion than in the rare condition (Tenenbaum &
Griffiths, 2003). Other studies showed that 4-year-old
children could make similar judgments. Taught
either that blickets were rare or common and shown
the “backwards blocking” condition described in the
previous paragraph, children inferred that B was a
blicket when blickets were common and that B was
not a blicket when blickets were rare (Sobel &
Kirkham, chapter 9, this volume; Sobel, Tenenbaum,
& Gopnik, 2004; Griffiths & Tenenbaum, chapter 20,
this volume).
So, if preschool children take prior knowledge into
account when making causal judgments, why did chil-
dren in the talking machine experiment violate their
LEARNING FROM DOING
79

knowledge about domain-appropriate causes on the
evidence of a single trial? Note that in the cross-
domain experiment, children were given deterministic
data: Buttons and the machine turning on were always
independent conditional on talking; talking and the
machine turning on were always unconditionally
dependent. When evidence is deterministic, you do
not need statistical tests to determine independence,
and whatever the prior probability of the hypothesis,
the posterior probability is 100%. Given the determin-
istic evidence, children’s inferences were identical to
those that would be made by the formalism.
Importantly, however, in a more ambiguous sce-
nario, children did take prior knowledge into
account. We replicated the machine/talking experi-
ment with a new group of children and then tested
the children on a “transfer condition” with a novel
toy, a novel speech act, and two novel switches. In the
transfer condition, children received no evidence
about the novel stimuli; we simply asked the children
how they would activate the novel toy: by talking to it
or by flipping the switches. In the test condition, the
children talked to the machine, just as in the previous
study. However, in the transfer condition, despite the
similarity of the stimuli, the children largely reverted
to their prior knowledge: 75% of the children chose
the switches (the domain-appropriate cause).
Equally important, however, the prior exposure to
the domain-inappropriate evidence did affect chil-
dren’s causal judgments. Children were significantly
more likely to choose the domain-inappropriate cause
in the transfer condition than in the previous baseline
condition (i.e., in which they had no evidence whatso-
ever about domain-inappropriate causes). The recent
exposure to counterintuitive evidence affected how
children extended their causal inferences. Similarly, as
discussed in the section on making novel interven-
tions, we found that many children would override
their preference for contact in physical causal relations
in light of probabilistic evidence for action at a dis-
tance. Thus, the combination of prior knowledge and
formal inference procedures seems to allow for learn-
ing that is both conservative and innovative.
This tension between conservativism and innova-
tion is consistent with a theory theory approach to
conceptual development and is also a salient feature
of adult scientific inquiry. Surprising evidence is often
questioned or dismissed before it is taken seriously
enough to establish the theories that will, in turn,
make the evidence predictable. As William James
(perhaps apocryphally) is said to have quipped:
“When a thing is new, people say: ‘It is not true.’
Later, when its truth becomes obvious, they say: ‘It is
not important.’ Finally, when its importance cannot
be denied they say: ‘Anyway, it is not new.’”
As scientists, we may complain about the tendency
of prior beliefs to squelch innovation; however, as an
extension of the inferential procedures used in child-
hood, the advantages of carefully weighing new evi-
dence against old is clear. If children’s learning were
too flexible—if it were, for instance, wholly dictated
by the most recent evidence observed—then children
would be subject to endless error. Children live in a
noisy world and might easily be exposed to misleading
data. If, on the other hand, innate or prior knowledge
acted as a strong constraint on children’s causal learn-
ing, then errors made early in development would be
irreparable. Children would be intransigent in the
fact of corrective evidence and helpless in genuinely
novel environments.
Although science has a reputation for objectivity,
one of the advantages of having a theory (naïve or sci-
entific) is precisely that all evidence is not treated
equally. By limiting the evidence to which we attend,
or that we take seriously, theories explain in part why
science can get so much inferential power out of a
“trifling investment in fact.” Formal inference proce-
dures, able to take into account both prior knowledge
and new evidence, may provide just the sort of learn-
ing mechanism that allows children’s causal theories
to be both stable and defeasible.
Distinguishing Good Interventions From
Confounded Ones
People who become exercised by the concept of child
as scientist frequently point out what is indisputably
the case: Children, unlike scientists, do not go around
designing controlled experiments to test their theo-
ries. Moreover, when children do try to design exper-
iments (i.e., because a teacher or a researcher asks
them to), they perform poorly. Children tend to inter-
vene on many variables at once, change interventions
between conditions, and then draw all the wrong con-
clusions. Adults (and often scientists) do little better
(Kuhn, 1989; Kuhn, Amsel, & O’Laughlin, 1988;
Masnick & Klahr, 2003).
However, designing an experiment requires
metacognition. To design an appropriate intervention,
80
CAUSATION AND INTERVENTION

you have to know what makes an intervention appro-
priate. Learning from interventions does not require
metacognition. You may have no idea what makes one
intervention better than another and still be able to
draw correct conclusions from the patterns of evidence
that result.
In the previous sections, we provided evidence sug-
gesting that when children are given good evidence,
they draw normative causal conclusions. What hap-
pens, however, when children are given bad evidence?
Are there conditions under which children realize that
interventions are confounded? Does confounding
change the types of inferences children make?
The conditional intervention principle defined an
intervention to rule out instances of confounding: An
intervention on X should be exogenous, should break
all the arrows into X, and should not influence any
other variable in the graph except through X. In the
test condition of the gear toy experiment, we showed
children evidence consistent with the conditional
intervention principle, and children were able to
learn the relationship of the gears to one another.
In the control condition, however, we concealed
the state of the switch. Thus, just as in the test condi-
tion, children saw, for instance, that gear A spun when
B was removed, but gear B failed to spin when gear A
was removed. However, with the switch hidden, the
children could not know whether B failed to spin
because gear A was removed or because the experi-
menter failed to flip on the switch. That is, there was no
way to know whether the intervention to remove gear A
broke all the arrows into B or not. Although the move-
ment of the gears was the same in both conditions, chil-
dren in the control conditions responded at chance
and—anecdotally—tried to look behind the machine
to determine whether the switch was on or off.
In a different set of studies, we looked at children’s
sensitivity to probabilistic causes and the role played
by their own interventions. In an observation condi-
tion, children saw an experimenter place a block on a
toy three times in a row. The children saw that one
block made the toy light up two of three times, and
another block made the toy light up only one of three
times. Children were told that each block had “spe-
cial stuff” inside and were asked which block has
more special stuff. The children distinguished the 2/3
probability from the 1/3 probability and said that the
2/3 block had more special stuff.
The intervention condition was identical except
that children were allowed to intervene on the block on
the third trial. For the 2/3 block, children saw the block
light up the toy twice, but when they tried the block, it
failed to light up. For the 1/3 block, children saw the
block fail to light up the toy twice, but when they tried
the block, the toy did light up. In this condition, chil-
dren said that the 1/3 block had more special stuff.
Children seemed to prefer making inferences based on
their own interventions.
Critically, however, the children were also tested
in a confounding control condition. In the control
condition, children saw exactly the same evidence as
in the test condition; however, this time when the
child intervened, the experimenter simultane-
ously pushed a button “to make the toy light up.” 
The child’s “intervention” was thus no longer a real
intervention—it did not break other arrows (like the
experimenter pushing the button) into the effect.
When the children’s own interventions were con-
founded in this way, they did not express a preference
for their own interventions; the children returned to
judging the blocks on the basis of the probabilities
(Kushnir, 2003; Kushnir & Gopnik, in press).
These findings suggest that, although children
may not be able to design controlled experiments,
they do, at least in certain cases, recognize instances
of confounding. Children seem to be sensitive to
some of the fundamental features of experimental
design and make different inferences when causal
manipulations are consistent with the conditional
intervention principle than when they are not.
Still, we might ask how, in the absence of con-
trolled experiments, children are able to learn so
much from interventions. We rely on experimental
design heavily in science; how can children learn so
much in its absence? Why aren’t children constantly
running into confounded interventions and drawing
inaccurate causal conclusions?
One possibility is that the very fact of being a
child might serve children well. Children are notori-
ous for being impulsive (they get into a lot of things)
and perseverative (they get into the same things over
and over again). Cast in a more positive light, chil-
dren tend to intervene a lot, and they tend to replicate
their interventions. Children’s very immaturity and,
in particular, the protracted development of their pre-
frontal cortex, which (in adults) seem to inhibit
impulsivity (e.g., Casey, Giedd, & Thomas, 2000;
Chao & Knight, 1998) and prevent perseveration
(e.g., Goel & Grafman, 1995), may support causal
learning.
LEARNING FROM DOING
81

How might immaturity and noise substitute for
controlled experimental design? Note that to infer that
X causes Y, you do not necessarily have to hold other
causes of Y constant. You can also randomize other
causes of Y. Children’s tendency to intervene in many
different contexts and their tendency to replicate their
actions might be advantageous. Other causes of Y
(whatever Y is) might exist, but children’s own actions
are unlikely always to coincide with those causes.
Certainly, children may occasionally leap to the
wrong causal conclusion from bad evidence. Wu and
Cheng (1999), for instance, cite a childhood anecdote
in which one of the authors dropped a vase at the same
time that a power outage occurred and thus blamed
herself for the blackout. However, such anecdotes are
funny in part because they are rare. In general, chil-
dren’s own actions may be a trustworthy foundation for
their causal inferences and naïve theories.
Conclusion
In many respects, the causal Bayes net formalism
seems to provide a learning mechanism that captures
the dynamic nature of theories—and in many respects,
children’s learning seems to be commensurate with the
predictions made by the formalism. However, the
causal Bayes net formalism may not tell the whole
story. In particular, the formalism may not entirely sat-
isfy Mark Twain. How we get such “wholesale returns
of conjecture out of a trifling investment in fact”
remains something of a mystery.
Causal Bayes net algorithms were developed for
use in procedures like data mining, for which evi-
dence is plentiful, but the causal relationships are
obscure. Constraint-based search methods thus rely
on the evidence of many trials or assume the available
data are representative of a larger sample. Bayesian
learning algorithms rely on either an abundance of
data or an abundance of prior knowledge.
In our experiments, by contrast, evidence was
scarce. Children made causal inferences from a min-
imal amount of data, often using only the evidence of
a single trial. As Tenenbaum and Griffiths (2003)
note, in “many cases . . . causal inference follow(s)
from just one or a few observations, where there isn’t
even enough data to reliably infer correlation!”
Note, however, that the causal Bayes net formalism
was also developed to infer causal structure from noisy,
probabilistic data in contexts in which interventions
were impossible (e.g., in epidemiological studies).
By contrast, in all of our studies, children observed or
performed interventions, and in most cases the evi-
dence they saw was deterministic. Such contexts (when
interventions are possible and determinism is assumed)
may be plentiful in everyday life, and within such con-
texts, children may not need the full apparatus of the
causal Bayes net learning algorithms. Children may be
able to represent structure as a causal Bayes net and
may use some of the same principles about the rela-
tionship between evidence and structure without
requiring the full power of the learning algorithms (see
Richardson, Schultz, & Gopnik, chapter 13, this 
volume). Thus, the causal Bayes net formalism may be
“too big” for what children need to accomplish.
Alternatively, causal Bayes nets formalism may be
“too small.” The algorithms may miss a level of
abstraction (what Tenenbaum & Niyogi, 2003, and
Griffiths & Tenenbaum, chapter 20, this volume, call
a causal grammar) that encompasses higher-order
causal laws that are assumed but never explicitly pre-
sented to the children (i.e., that blocks activate detec-
tors, and detectors do not activate blocks). Children
may be successful at learning causal relationships from
a few observations (in our lab and in the world)
because they are already bringing a rich theoretical
structure to bear on the inferential tasks. Thus, the
causal Bayes net algorithms may allow children to
learn structure from minimal data only when they are
embedded within higher-order causal theories (see
Tenenbaum & Griffiths, 2003; Tenenbaum & Niyogi,
2003; Griffiths & Tenenbaum, chapter 20, this 
volume).
Critically, however, this account may only move
the problem of causal inference back a step.
Knowledge of higher-order causal laws might support
children’s ability to learn particular causal relations.
However, somehow children must also learn the
higher-order causal laws—and it seems tempting to
assume that children infer higher-order causal laws
from particular causal relations. One of the challenges
for future research is to determine whether such cir-
cles can be benign rather than vicious. In principle,
children might be able to bootstrap an abstract causal
grammar from clear evidence for particular causal
relationships and then use the higher-order theory to
handle more complex or ambiguous evidence for 
particular causal relations.
However, even if (as we expect) the causal Bayes
net formalism does not end up being “just right,” it
more than any other current computational account
82
CAUSATION AND INTERVENTION

suggests a learning mechanism that does justice to
much of the breadth and depth of children’s naïve
theories. In supporting novel predictions, novel inter-
ventions, structure learning, inferences about unob-
served causes, distinctions between observations and
interventions, and the criteria for a good intervention,
the causal Bayes net formalism captures much that is
critical about a theory. Our hope is that children’s
ability to engage in theory formation and theory
change might similarly set the standard for future
computational accounts of learning.
If you are persuaded by little else by this chapter,
we hope we have at least convinced you of the value
of interdisciplinary work. Research in computer sci-
ence, artificial intelligence, and philosophy has sug-
gested some of the fundamental assumptions that
might underlie the development of children’s naïve
theories. Work in developmental psychology has
demonstrated that young children are able to learn
the causal structure of events with remarkable speed
and accuracy. We hope that investigators in all these
areas will continue to find causal learning, in both
children and science, fascinating for years to come.
References
Ahn, W., Kalish, C. W., Medin, D. L., & Gelman, S. A.
(1995). The role of covariation versus mechanism
information in causal attribution. Cognition, 57,
299–352.
Baillargeon, R., Kotovsky, L., & Needham, A. (1995).
The acquisition of physical knowledge in infancy. In
D. Sperber & D. Premack (Eds.), Causal cognition:
A multidisciplinary debate. Symposia of the Fyssen
Foundation; Fyssen Symposium, 6th January 1993,
Pavillon Henri IV, St-Germain-en-Laye, France
(pp. 79–115). New York: Clarendon Press/Oxford
University Press.
Baker, A. G., Berbrier, M., & Vallée-Tourangeau, F.
(1989). Judgements of a 2  2 contingency table:
Sequential processing and the learning curve.
Quarterly Journal of Experimental Psychology, 41B,
65–97.
Bickel, P. J., Hammel, E. A., & O’Connell, J. W. (1975).
Sex bias in graduate admissions: Data from
Berkeley. Science, 187, 389–404.
Bullock, M., Gelman, R., & Baillargeon, R. (1982). The
development of causal reasoning. In W. J.
Friedman (Ed.), The developmental psychology of
time (pp. 209–254). New York: Academic Press.
Carey, S. (1985). Conceptual change in childhood.
Cambridge, MA: MIT Press/Bradford Books.
Casey, B. J., Giedd, J. N., & Thomas, K. M. (2000).
Structural and functional brain development and
its relation to cognitive development. Biological
Psychology, 54, 241–257.
Chao, L. L., & Knight, R. T. (1998). Contribution of
human prefrontal cortex to delay performance.
Journal of Cognitive Neuroscience, 10, 167–177.
Cheng, P. W. (1997). From covariation to causation: A
causal power theory. Psychological Review, 104,
367–405.
Dickinson, A., Shanks, D. R., & Evendon, J. (1984).
Judgment of act-outcome contingency: The role of
selective attribution. Quarterly Journal of Ex-
perimental Psychology, 36, 29–50.
Dowe, P. (2000). Physical causation. New York:
Cambridge University Press.
Flavell, J. H., Green, F. L., & Flavell, E. R. (1995).
Young children’s knowledge about thinking.
Monographs of the Society for Research in Child
Development, 60, v-96.
Gelman, S. A., & Wellman, H. M. (1991). Insides and
essence: Early understandings of the non-obvious.
Cognition, 38, 213–244.
Goel, V., & Grafman, J. (1995). Are the frontal lobes
implicated in “planning” functions? Interpreting
data from the Tower of Hanoi. Neuropsychologia,
33, 623–642.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 1–31.
Gopnik, A., & Meltzoff, A. N. (1997). Words, thoughts
and theories. Cambridge, MA: MIT Press.
Gopnik, A., Sobel, D. M., Schulz, L. E., & Glymour, C.
(2001). Causal learning mechanisms in very young
children: 2-, 3-, and 4-year-olds infer causal rela-
tions from patterns of variation and covariation.
Developmental Psychology, 37, 620–629.
Gopnik, A., & Wellman, H. M. (1994). The theory the-
ory. In S. A. Gelman & L. A. Hirschfeld (Eds.),
Mapping the mind: Domain specificity in cognition
and culture; based on a conference entitled
“Cultural Knowledge and Domain Specificity,”
held in Ann Arbor, Michigan, October 13–16, 1990
(pp. 257–293). New York: Cambridge University
Press.
Griffiths, T. L., & Tenenbaum, J. B. (2001). Randomness
and coincidences: Reconciling intuition and 
probability theory. Proceedings of the 23rd Annual
Conference of the Cognitive Science Society.
Edinburgh: LEA. pp. 370–375.
LEARNING FROM DOING
83

Harris, P. L., German, T., & Mills, P. (1996). Children’s
use of counterfactual thinking in causal reasoning.
Cognition, 61, 233–259.
Inagaki, K., & Hatano, G. (1993). Young children’s
understanding of the mind body distinction. Child
Development, 64, 1534–1549.
Kalish, C. (1996). Causes and symptoms in preschoolers’
conceptions of illness. Child Development, 67,
1647–1670.
Keil, F. C. (2003). Folkscience: Coarse interpretations of
a complex reality. Trends in Cognitive Sciences, 18,
663–692.
Kuhn, D. (1989). Children and adults as intuitive scien-
tists. Psychological Review, 96, 674–689.
Kuhn, D., Amsel, E. & O’Laughlin, M. (1988). The
development of scientific thinking skills. Orlando,
FL: Academic Press.
Kushnir, T. (2003, April). Seeing versus doing: The effect
of direct intervention on preschooler’s understanding
of probabilistic causes. Poster presented at the bien-
nial meeting of the Society for Research in Child
Development, Tampa, FL.
Kushnir, T., & Gopnik, A. (2005). Children infer causal
strength from probabilities and interventions.
Psychological Science, 16(9), 678–683.
Kushnir, T., Gopnik, A., & Schaefer, C. (2005, April).
Children infer hidden causes from probabilistic evi-
dence. Paper presented at the biennial meeting of
the Society for Research in Child Development,
Atlanta, GA.
Kushnir, T., Gopnik, A., Schulz, L., & Danks, D. (2003).
Inferring hidden causes. In R. Alterman & D. Kirsh
(Eds.), Proceedings of the 25th Annual Meeting of
the Cognitive Science Society. Boston, pp. 699–703. 
Leslie, A. M., & Keeble, S. (1987). Do 6-month-old
infants perceive causality? Cognition, 25, 265–288.
Masnick, A. M., & Klahr, D. (2003). Error matters: An
initial exploration of elementary school children’s
understanding of experimental error. Journal of
Cognition & Development, 4, 67–98.
Meltzoff, A. N. (1995). Understanding the intentions of
others: Re-enactment of intended acts by 18-month-
old children. Developmental Psychology, 31,
838–850.
Michotte, A. (1962). The perception of causality. New
York: Basic Books. (Original work published
1946).
Novick, L. R., & Cheng, P. W. (2004). Assessing interac-
tive causal influence. Psychological Review, 111,
455–485.
Pearl, J. (2000). Causality. New York: Oxford University
Press.
Perner, J. (1991). Understanding the representational
mind. Cambridge, MA: MIT Press.
Piaget, J. (1930). The child’s conception of physical
causality. London: Kegan Paul.
Rovee-Collier, C. (1980). Reactivation of infant memory.
Science, 208, 1159–1161.
Rosenblit, L. R., & Keil, F. C. (2002). The misunderstood
limits of folk science: an illusion of explanatory depth.
Cognitive Science, 26, 521–562.
Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
Statistical learning by 8-month-old infants. Science,
274, 1926–1928.
Salmon, W. (1984). Scientific explanation and the causal
structure of the world. Princeton, NJ: Princeton
University Press.
Salmon, W. (1998). Causality and explanation. Oxford,
England: Oxford University Press.
Schaefer, C., & Gopnik, A. (2003, April). Causal reasoning
in young children: The role of unobserved variables.
Paper presented at the biennial meeting of the Society
for Research in Child Development, Tampa, FL.
Schlottman, A. (2001). Perception versus knowledge of
cause and effect in children: When seeing is believ-
ing. Current Directions in Psychological Science, 10,
111–115.
Schulz, L. E., (2001, December). Spinning wheels and
bossy ones: Children, causal structure and the calcu-
lus of intervention. Paper presented at the Causal
Inference in Humans and Machines Workshop of
the Neural Information Processing Systems annual
meeting, Vancouver, BC.
Schulz, L. (2003, April). The play’s the thing: Inter-
ventions and causal inference. Paper presented at
the biennial meeting of the Society for Research in
Child Development, Tampa, FL.
Schulz, L., & Gopnik, A. (2004). Causal learning across
domains. Developmental Psychology, 40, 162–176.
Schulz, L., Gopnik, A., & Glymour, C. (in press).
Preschool children learn causal structure from con-
ditional interventions. Developmental Science.
Shanks, D. R. (1985). Forward and backward blocking in
human contingency judgment. Quarterly Journal of
Experimental 
Psychology: 
Comparative 
and
Physiological Psychology, 37, 1–21.
Shanks, D. R., & Dickinson, A. (1987). Associative
accounts of causality judgment. In G. H. Bower
(Ed.), The psychology of learning and motivation:
Advances in research and theory (Vol. 21, pp.
229–261). San Diego, CA: Academic Press.
Shultz, T. R. (1982). Rules of causal attribution.
Monographs of the Society for Research in Child
Development, 47, 1–51.
Sloman, S. A., & Lagnado, D. A. (2005). Do we “do”?
Cognitive Science, 29, 5–39.
Sobel, D. M. (2004). Emploring the coherence of young
children’s explanatory abilities: Evidence from 
84
CAUSATION AND INTERVENTION

generating counter factuals. British Journal of
Developmental Psychology, 22, 37–58.
Sobel, D. M., Tenenbaum, J. B., & Gopnik, A. (2004).
Children’s causal inferences from indirect evidence:
Backwards blocking and Bayesian reasoning in
preschoolers. Cognitive Science, 28, 303–333.
Sommer, S., & Wehner, R. (2004). The ant’s estimation of
distance travelled: Experiments with desert ants,
Cataglyphis fortis. Journal of Comparative Physiology
A, Neuroethology Sensory Neural Behavioral
Physiology, 190, 1–6.
Spelke, E. S., Breinlinger, K., Macomber, J., & Jacobson,
K. (1992). Origins of knowledge. Psychological
Review, 99, 605–632.
Spelke, E. S., Katz, G., Purcell, S. E., Ehrlich, S. M., &
Breinlinger, K. (1994). Early knowledge of object mot-
ion: Continuity and inertia. Cognition, 51, 131–176.
Spirtes, P., Glymour, C., & Scheines, R. (1993).
Causation, prediction, and search (Springer Lecture
Notes in Statistics). New York: Springer-Verlag.
Strevens, M. (2000). The essentialist aspect of naive the-
ories. Cognition, 74(2), 149–175.
Steyvers, M., Tenenbaum, J., Wagenmakers, E. J., &
Blum, B. (2003). Inferring causal networks from
observations and interventions. Cognitive Science,
27, 453–489.
Tenenbaum, J. B., & Griffiths, T. L., (2003). Theory-
based causal inference. In S. Becker, S. Thrun, &
K. Obemayer (Eds.), Advances in neural informa-
tion processing systems 15. Cambridge, MA: MIT
Press (pp. 35–42).
Tolman, E. C. (1932). Purposive behavior in animals and
men. New York: Century.
Wasserman, E. A., Elek, S. M, Chatlosh, D. L., &
Baker, A. G. (1993). Rating causal relations: Role
of probability in judgments of response-outcome
contingency. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 19, 174–188.
Watson, J. S., & Ramey, C. T. (1987). Reactions to
response-contingent stimulation in early infancy. In
J. Oates, & S. Sheldon (Eds.), Cognitive develop-
ment in infancy; cognitive development in infancy;
portions of this paper were initially reported at the
biennial meeting of the Society for Research in Child
Development, Santa Monica, CA, 1969 (pp. 77–85).
Hillsdale, NJ: Erlbaum.
Wellman, H. M., Hickling, A. K., & Schult, C. A. (1997).
Young children’s psychological, physical, and 
biological explanations. In H. M. Wellman & K.
Inagaki (Eds.), The emergence of core domains of
thought: Children’s reasoning about physical, psy-
chological, and biological phenomena (New
Directions for Child Development, No. 75, 
pp. 7–25). San Francisco: Jossey-Bass/Pfeiffer.
Tenenbaum, J., & Griffiths, T. L. (2003). Theory-based
causal inference. In S. Becker, S. Thrun, & K.
Obemayer (Eds.), Advances in neural information
processing systems 15 (pp. 67–74). Cambridge, MA:
MIT Press.
Tenenbaum, J., & Niyogi, S. (2003). Learning causal
laws. In R. Alterman & D. Kirsh (Eds.), Proceedings
of the 25th Annual Conference of the Cognitive
Science Society. Boston, pp. 1152–1157.
Waldmann, M. R. (2000). Competition among causes
but not effects in predictive and diagnostic learning.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 26, 53–76.
Waldmann, M. R. (2001). Predictive versus diagnostic
causal learning: Evidence from an overshadowing
paradigm. Psychonomic Bulletin and Review, 8,
600–608.
Waldmann, M. R., & Hagmayer, Y. (2005). Seeing ver-
sus doing: Two modes of accessing causal knowl-
edge. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 31, 216–227.
Wellman, H. M., Hickling, A. K., & Schult, C. A. (1997).
Young children’s psychological, physical, and bio-
logical explanations. New Directions for Child
Development, 75, 7–25.
Woodward, A. L. (1998). Infants selectively encode the
goal object of an actor’s reach. Cognition, 69, 1–34.
Woodward, A. L., Phillips, A. T., & Spelke, E. S. (1993).
Infants’ expectations about the motion of animate
versus inanimate objects. In Proceedings of the 15th
annual meeting of the Cognitive Science Society
(pp. 1087–1091). Hillsdale, NJ: Erlbaum.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. New York: Oxford University
Press.
Wu, M., & Cheng, P. W. (1999). Why causation need
not follow from statistical association: Boundary
conditions for the evaluation of generative and pre-
ventive causal powers. Psychological Science, 10,
92–97.
LEARNING FROM DOING
85

Introduction
Causal knowledge enables us to predict future events,
to choose the right actions to achieve our goals, and to
envision what would have happened if things had
been different. Thus, it allows us to reason about
observations, interventions, and counterfactual possi-
bilities. Philosophers and computer scientists have
begun to unravel the relations among these three
kinds of reasoning and their common basis in causal-
ity (e.g., Pearl, 2000; Spirtes, Glymour, & Scheines,
1993; Woodward, 2003).
Observations can provide some information about
the statistical relations among events. According to the
principle of common cause (Reichenbach, 1956),
there are three possible causal explanations for a reli-
able statistical relation between two events A and B:
A causes B, B causes A, or both events are generated by
a third event or set of events, their common cause. For
example, dieting and obesity are statistically related
because obesity causes people to go on a diet, because
dieting disturbs regulatory physiological processes that
eventually lead to obesity (many obese people went on
a diet before they became extremely overweight), or
because obesity and dieting may be causal conse-
quences of our modern eating habits. In this last case,
we can say that the correlation between obesity and
dieting is spurious. Regardless of the underlying causal
structure, an observation of one of these events allows
us to infer that other events within the underlying
causal model will be present or absent as well. Thus,
when we have passively observed an event, we can rea-
son backward diagnostically to infer the causes of this
event, or we can reason forward and predict future
effects. Moreover, we can infer the presence of spuri-
ously related events.
Interventions often enable us to differentiate
among the different causal structures that are compat-
ible with an observation. If we manipulate an event A
and nothing happens, then A cannot be the cause of
event B, but if a manipulation of event B leads to a
change in A, then we know that B is a cause of A,
although there might be other causes of A as well.
Forcing some people to go on a diet can tell us
86
6
Causal Reasoning Through Intervention
York Hagmayer, Steven Sloman, David Lagnado, & Michael R. Waldmann

whether the diet increases or decreases the risk of
obesity. Alternatively, changing people’s weight by
making them exercise would show whether body mass
is causally responsible for dieting.
In contrast to observations, however, interventions
do not provide positive or negative diagnostic evi-
dence about the causes of the event on which we
intervened. Whereas observations of events allow us
to reason diagnostically about their causes, interven-
tions make the occurrence of events independent of
their typical causes. Thus, because of the statistical
independence created by interventions, these events
will occur with their usual base rate independent of
the outcome of an intervention. For example, forcing
somebody to eat 50 (and only 50) grams of fat per day
fixes fat intake independent of the presence or
absence of other factors normally affecting diet.
Counterfactual reasoning tells us what would have
happened if events other than the ones we are cur-
rently observing had happened. If we are currently
observing that both A and B are present, then we can
ask ourselves if B would still be present if we had
intervened on A and caused its absence. If we know
that B is the cause of A, then we should infer that the
absence of A makes no difference to the presence of B
because effects do not necessarily affect their causes.
But, if our intervention had prevented B from occur-
ring, then we should infer that A also would not
occur. For example, Morgan Spurlock (director and
guinea pig of the movie Supersize Me, released in
2004) ate fast food for 4 weeks and gained more than
20 pounds. What would have happened if he had not
eaten burgers and fries all the time? Assuming that his
heavy consumption of fast food was causally responsi-
ble for his increase in weight rather than the
increased weight being the cause of eating, we can
conclude that he would have stayed in better shape
without all the carbohydrates and fats.
The example indicates that counterfactual reason-
ing combines observational and interventional reason-
ing. First, we observe Morgan eating fast food and
gaining weight. Second, we assume that one of the
events had been different. We imagine him not eating
such a diet, while all other observed or inferred factors
(e.g., his genetic makeup, amount of physical exercise,
etc.) are assumed to stay at the observed level. Thus,
instantiating a counterfactual event is causally equiva-
lent to an imaginary intervention on a causal model in
which all variables that are not affected by the interven-
tion are assumed to stay at currently observed levels.
Finally, causal consequences of the intervention are
inferred on the basis of the given causal model. We
infer that Morgan would not have gained as much
weight as he did (see next section; Pearl, 2000; and
Sloman & Lagnado, 2005, for a more detailed discus-
sion of counterfactuals).
There are important differences among observa-
tion, intervention, and counterfactuals. Nevertheless,
they can be given a unified treatment within the
causal model framework. Whereas probabilistic and
associative accounts of causal knowledge fail to cap-
ture these three interrelated functions of causal
knowledge, causal Bayes nets do (Glymour, 2001;
Pearl, 2000; Spirtes et al., 1993). The next section
summarizes these accounts. Although causal Bayes
nets provide successful formal tools for expert systems,
few experiments have tested whether causal Bayes
nets also capture everyday reasoning with causal mod-
els by people who are not formally trained. The
remainder of the chapter presents experimental evi-
dence from the areas of logical reasoning, learning,
and decision making demonstrating the plausibility of
causal Bayes nets as psychological theories.
Modeling
We do not give a detailed description of causal Bayes
nets here (see Pearl, 2000, or Spirtes et al., 1993, for
detailed introductions). Research on causal Bayes
nets focuses not only on causal representation and
inference but also on other questions, such as those
regarding learning (see Lagnado, Waldmann,
Hagmayer, & Sloman, chapter 10, this volume).
Here, we show how to derive predictions from causal
Bayes nets based on observations, interventions, and
counterfactual assumptions. Although causal Bayes
nets provide tools for reasoning with complex models,
experimental studies typically present problems that
are within the grasp of naïve participants. We there-
fore concentrate our brief introduction on inferences
using the three basic causal models involving most
research: common-cause, common-effect, and causal
chain models. More complex models can be gener-
ated by combining these three models (see Sloman &
Lagnado, 2005, and Waldmann & Hagmayer, 2005,
for research on more complex models).
Figure 6-1 shows the graphs for the three models,
with the nodes representing event variables and the
arrows signifying direction of causal influence: (a) a
CAUSAL REASONING THROUGH INTERVENTION
87

common-cause model in which a single cause X
influences two effects Y and Z, (b) a causal chain
model in which an initial cause X affects an interme-
diate event Y influencing a final effect Z, and (c) a
common-effect model in which two causes X and Y
independently influence a joint effect Z.
The graphs encode assumptions about dependence
and independence, simplifying the representation of
the causal domain. One important assumption under-
lying Bayes nets is the Markov assumption, which states
(informally) that each event in a causal graph is inde-
pendent of all events other than its descendants (i.e., its
direct and indirect effects) once the values of its parent
nodes (i.e., its direct causes) are known.
The graph of the common-cause model expresses
the spurious correlation between effects Y and Z
(because of their common cause) and their independ-
ence once the state of cause X is known. This is a con-
sequence of the Markov condition. Once we know
that X is present, the probability of Y is the same
regardless of whether Z is present. Similarly, the
causal chain implies that the initial cause X and the
final effect Z are dependent but become independent
when the intermediate event Y is held constant. Once
we know that Y, the direct cause of Z, is present, the
probability of Z stays constant regardless of whether X
has occurred. Finally, the common-effect model
implies independence of the alternative causes X and
Y and their dependence once the common effect is
held fixed. This is an example of explaining away. X
and Y should occur independently, but once we know
that X and its effect Z are present, it is less likely that
Y is also present.
Independence is advantageous in a probabilistic
model not only because it simplifies the graph by
allowing omission of a link between variables but
also because it simplifies computation. Conceived
as a computational entity, a Bayes nets is merely a
representation of a joint probability distribution—
P(X,Y,Z) in Figure 6-1—that provides a more com-
plete model of how the world might be by specifying
the probability of each possible state. Each event is
represented as a variable. Causal relations are assumed
to generate the conditional probabilities relating causes
to their effects, and arrows in a causal graph represent
those causal relations. The factorizations of the three
models at issue are
Common cause: P(X, Y, Z)  P(Y | X) P(Z | X) P(X)
Causal chain: P(X, Y, Z)  P(Z | Y) P(Y | X) P(X)
Common effect: P(X, Y, Z)  P(Z | Y, X) P(Y) P(X)
The equations specify the probability distribution of
the events within the model in terms of the strength of
the causal links and the base rates of the exogenous
causes that have no parents (e.g., X in the common-
cause model). Implicit in the specification of the
parameters of a Bayes net are rules specifying how mul-
tiple causes of a common effect combine to produce
the effect or (in the case of continuous variables) func-
tional relations between variables. A parameterized
causal model allows it to make specific predictions of
the probabilities of individual events or patterns of
events within the causal model.
Modeling Observations
Observations not only tell us whether a particular event
is present or absent but also inform us about other
events that are directly or indirectly causally related to
the observed event. Therefore, the structure of the causal
model is crucial for inference. Observing an event
increases the probability of its causes and its effects. For
example, if someone has a high level of cholesterol, then
you can make the diagnostic inference that the person
88
CAUSATION AND INTERVENTION
FIGURE 6-1 Three basic causal models.

has probably had an unhealthy diet (cause) and you
can predict that the person’s risk of contracting heart
problems is relatively high (effect). These inferences
can be justified on the basis of the structure of the causal
model. No specific information about the strength of
the causal relations or the base rates of the events is
necessary to make these qualitative predictions. More
specific predictions of the probabilities of events can be
made when the model is parameterized.
Formally, observations are modeled by setting the
event variables to the values that have been observed.
Based on our equations and probability calculus, the
probabilities of other events conditional on the
observed variable can be calculated. The structure of
the causal model is crucial for these calculations.
Imagine that an effect Y of a common cause X that
also generates Z is observed. The resulting increase in
probability of the cause X can be computed using
Bayes’ rule:
P(X  1 | Y  1)
 P(Y  1| X 1) P(X  1)/ [P(Y  1| X 1)
P(X  1)  P(Y  1| X 0) P(X  0)]
For example, if the base rate of following an
unhealthy diet is P( X1).5, the probability that
an unhealthy diet will cause being overweight is 
P( Y 1| X1).9, and the probability of being
overweight despite eating healthy food is P( Y1 |
X 0) .1, then being overweight indicates a proba-
bility of P(X  1 | Y  1)  .9 that the diet was
unhealthy. The probability of the other effect Z has
to be computed by using the updated probability of
the common cause and the conditional probability
P(Z | X) referring to the causal relation connecting
the common cause and the second effect. For exam-
ple, if the probability of having high levels of choles-
terol given an unhealthy diet is P  .4 and P.1
otherwise, then the observation of a person’s being
overweight implies that the probability of having a high
level of cholesterol is .37. Note that this calculation
rests on the assumptions that the events are connected
by a common-cause model. The same conditional
probabilities have different implications given other
causal structures.
Modeling Interventions
There are different types of intervention (see
Woodward, 2003). Interventions can interact with the
other causes of events. For example, when we
increase fat in our diet, then the resulting cholesterol
level in our blood depends on our metabolism, prior
level of cholesterol, and many other factors. The
causal Bayes net literature has focused on a specific
type of intervention that completely determines the
value of the variable the intervention targets (see
Pearl, 2000; Spirtes et al., 1993; Woodward, 2003).
For example, if we set the temperature of a room to
20°C, our intervention fixes room temperature while
disconnecting it from all its causes. In this chapter, we
focus on this strong type of intervention.
How can interventions be formally modeled? The
most important assumption can be traced to Fisher’s
(1951) analysis of experimental methods. Randomly
assigning participants to experimental and control
groups creates independence between the independ-
ent variable and possible confounds. Thus, if we, as
external agents, set cholesterol levels to a specific
value, then the level of cholesterol is independent of
other factors normally determining its level. To qual-
ify as an intervention of this strong kind, the manipu-
lation has to force a value on the intervened variable
(e.g., cholesterol), thus removing all other causal
influences (e.g., diet). Moreover, the intervention
must be statistically independent of any variable
that directly or indirectly causes the predicted event
(e.g., all causes of cholesterol), and it should not
have any causal relation to the predicted event except
through the intervened-on variable (see Pearl, 2000;
Spirtes et al., 1993; Woodward, 2003).
As with observation, predictions of the outcomes
of hypothetical interventions are based on specific
values of event variables, but whereas observations
leave the surrounding causal network intact, interven-
tions alter the structure of the causal model by render-
ing the manipulated variable independent of its
causes. Thus, predictions on the basis of interventions
need to be based on the altered causal model, not the
original model. For example, the passive observation
of low cholesterol level indicates a healthy diet
because of the causal link between diet and choles-
terol, but medically inducing a specific cholesterol
level does not provide evidence about a person’s
eating habits. Manipulating cholesterol independent
of the prior value and other factors creates independ-
ence between cholesterol level and diet. Thus, predic-
tions about eating habits can only be based on
assumptions about base rates, not on evidence about
cholesterol level.
CAUSAL REASONING THROUGH INTERVENTION
89

The changes in a causal model caused by
interventions (of the strong type) can be modeled by
procedures that Pearl (2000) vividly calls graph surgery.
These procedures result in a “manipulated graph”
(Spirtes et al., 1993). The key idea is that interventions
introduce an external independent cause that fixes the
value of the manipulated event. As a consequence, all
other causal arrows pointing toward this event need to
be removed because these causal influences are not
operative during the intervention. Thus, both types of
predictions are grounded in a representation of the
underlying causal model. However, whereas observa-
tional predictions are based on the original causal
graph, interventional predictions are based on the
manipulated graph. Figure 6-2 illustrates for the three
causal models from Figure 6-1 how observing differs
from intervening. In general, the manipulated graphs
are generated by removing the incoming causal links
that point to the manipulated variable.
Traditional Bayes nets (e.g., Pearl, 1988) and other
probabilistic theories are incapable of distinguishing
between observations and interventions because they
lack the expressive power to distinguish observational
and interventional conditional probabilities. Both types
are subsumed under the general concept of conditional
probability. To distinguish observations from interven-
tions, Pearl (2000), following previous work by Spirtes
et al. (1993), introduces a do-operator. The do-operator
represents an intervention on an event that renders the
manipulated event independent of all its causes (i.e., it
is the formal equivalent of graph surgery).
For example, do(Y  1) represents the event that Y
was fixed to the value of 1 by means of an interven-
tion. Thus, it implies the removal of all previous
causal influences in Y. Applying the do-operator
allows it to make specific interventional predic-
tions about events within the causal model. For exam-
ple, the equations for the factorization of the joint 
distribution of the causal chain model (Figure 6-2) in
which the intermediate event is observed to be present
(Y  1) or manipulated [do(Y  1)], respectively, are
Observation of Y:
P(X, Y 1,Z)  P(Z | Y 1) P(Y1 | X) P(X)
Intervention on Y:
P(X, do(Y 1),Z)  P(Z | Y 1) P(X)
If the parameters of the causal model are known, we
can calculate the probabilistic consequences of inter-
ventions. The hypothetical intervention on Y (i.e., Y is
fixed to the value of 1 and therefore known to be pres-
ent) in the causal chain implies that Z occurs with the
observational probability conditional on the presence
of Y (P(Z | Y1), and that X occurs with a probability
90
CAUSATION AND INTERVENTION
FIGURE 6-2 Examples of observations of (symbolized as eyes) and interventions on (symbolized as hammers)
the three basic causal models.

corresponding to its base rate (P(X)). Notice that the
interventional probability requires fewer parameters
because graph surgery involves simplification by
inducing independence between a variable and its
causes.
As a second example, consider the common-
cause model in Figure 6-1. Whereas observing Y
allows us to reason diagnostically back to its cause X
and then reason forward predictively to its spurious
correlate Z, predictions for hypothetical interven-
tions in effect Y need to be based on the manipu-
lated graph in Figure 6-2 in which the link between
X and Y is removed. Formally, this can be expressed
by the equation1
P(X, do(Y 1), Z)  P(Z|X) P(X)
Thus, fixing Y at the value 1 removes the link to
this variable from the causal model. However, pre-
dictions are still possible on the basis of the manipu-
lated graph. The common cause X should occur
with a probability corresponding to its base rate, and
Z is determined by the base rate of its cause X and
the strength of the probabilistic relation between 
X and Z.
Modeling Counterfactuals
Counterfactuals combine observations and interven-
tions. The current state of the world is modeled as an
observation, and then the counterfactual is set by an
imaginary intervention altering the state of the
variables assumed to be different. For example, we
may currently tend to eat unhealthy fast food. For a
counterfactual analysis, we would first model this fact
as if it were an observation by inferring the conse-
quences for other unobserved events within the causal
model. We may infer that we have an increased prob-
ability of contracting diabetes. Next, we want to know
what would happen if we had eaten healthy food
instead. We model this counterfactual by means of a
hypothetical intervention that fixes the value of the
diet variable. Note that counterfactuals differ from
interventions because counterfactual interventions
alter causal models, which have been updated before
on the basis of the given facts.
As in the case of observations and interventions,
graphical causal models are sufficient to draw qualita-
tive inferences from counterfactuals. For example, con-
sider a causal chain model connecting diet, weight,
and diabetes. To model the statement, “If she were not
obese, she would not have developed diabetes,” we first
assume that we observe diabetes and obesity in a
woman. Based on these observations, we can infer that
the woman probably tends to eat an unhealthy diet.
Next, we hypothetically eliminate obesity by means of
an intervention that influences this variable by means
of a factor external to the chain model. This hypothet-
ical intervention would cut the causal link between
diet and weight, but the link between weight and dia-
betes would stay intact. Therefore, the counterfactual
implies that the person in this alternative world would
be spared diabetes, while her eating habits would stay
the same.
Formal modeling of counterfactuals requires
updating of the model twice. First, the probabilities of
all events are calculated conditional on the facts
stated in the counterfactual treating facts as observa-
tions. Second, the counterfactual event is set by the
do-operator, which entails a reanalysis of the probabil-
ities of the events in the manipulated graph. Thus,
assuming the validity of the causal model and the
attached parameters, causal Bayes nets allow us to
generate precise predictions for counterfactuals.
Summary
Causal Bayes nets capture the structure of causal
models. They allow us to generate qualitative predic-
tions for observations, interventions, and counterfac-
tuals. Moreover, parameterized causal models enable
us to make precise predictions about the probabilities
of events within the causal model. Whereas observa-
tional predictions are within the grasp of traditional
associative or probabilistic (including Bayesian) theo-
ries, modeling interventions and counterfactuals tran-
scends the conceptual power of these models. To
model hypothetical interventions and counterfactuals
correctly, a preliminary stage has to be assumed in
which the structure of the causal model generating
the predictions is modified. Based on this modified
causal model, precise predictions can be made for sit-
uations that may never before have been observed.
The distinction between observation and inter-
vention is crucial for the theory of causal Bayes nets.
Although observations allow drawing inferences about
causes and effects of the observed event, interventions
cut the event off from its causes by deleting the causal
links pointing toward the event. Sloman and Lagnado
(2005) coined the term undoing for this process. 
CAUSAL REASONING THROUGH INTERVENTION
91

If causal Bayes nets are veridical models of intuitive
human causal reasoning, then participants have to be
sensitive to undoing. Thus, a key issue will be whether
human participants are capable of predicting out-
comes of hypothetical interventions and of reasoning
about 
causal 
counterfactuals. 
This 
compe-
tency would imply that people have access to reason-
ing processes that modify causal representations prior
to deriving predictions. The next three sections report
evidence concerning this question.
Causal Reasoning Versus Logical
Reasoning
Causal Bayes nets can be used to represent and model
qualitative logical inferences in causal domains. One
implication of this account is that causal inference
differs from inference in a context in which the stan-
dard rules of propositional logic also apply. Although
standard logic does not distinguish between the obser-
vation of an event and the generation of the same
event by an intervention, the distinction is central to
causal Bayes nets. Causal models have the ability to
represent both action (intervention in the world) and
imagination (intervention in the mind). If participants
are sensitive to the difference between observation
and intervention, then they should infer that the
observation of an event is diagnostic of the presence of
its causes, but when the same event is physically or
mentally manipulated, it no longer is.
Observation Versus Intervention in
Counterfactual Scenarios
To verify that people are sensitive to the difference
between observation and intervention, Sloman and
Lagnado (2005) gave a group of students the follow-
ing scenario:
All rocketships have two components, A and B.
Movement of Component A causes Component B
to move. In other words, if A, then B. Both are
moving.
Notice that this scenario describes the simplest
possible causal model involving only a single link (see
Figure 6-3). Furthermore, the current values of the
variables A and B are stated.
After reading the scenario, half the group was
then asked the observational counterfactual ques-
tion concerning what they would expect if they had
observed components not moving: (a) Suppose
Component B were observed to not be moving,
would Component A still be moving? The other
half was asked the interventional counterfactual
question concerning what they would expect if
components had been intervened on and thereby
prevented from moving: (b) Suppose Component B
were prevented from moving, would Component A
still be moving?
The difference between observation and interven-
tion should show up in the comparison of (a) and (b).
Observing that the effect B is not moving should be
diagnostic of A, suggesting that A also is not moving. In
contrast, the logic of intervention says that we should
represent an intervention on B as P(A moves | do(B
does not move)), which reduces to P(A moves)
because B is disconnected from its normal cause A
under the do operation. As participants were told
before that A is moving, they should stick to that belief
and answer yes. This is just what happened: 85% of
participants answered yes to (b) but only 22%
answered yes to (a). B’s movement was only treated as
diagnostic of A’s movement when B was observed not
to move, not when its movement was prevented. This
shows that people are sensitive to the logic of a coun-
terfactual intervention in a situation with a transparent
causal structure.
Causal Reasoning Versus Propositional Logic
The causal model framework predicts that people are
sensitive to the logic of intervention when reasoning
causally, not necessarily when reasoning in other
ways. Sloman and Lagnado (2005) compared reason-
ing in a situation with causal relations to one with par-
allel relations that were not causal.
92
CAUSATION AND INTERVENTION
FIGURE 6-3 Simplest possible causal model.

FIGURE 6-4 Causal chain model used by Sloman and Lagnado (2005).
Consider the following causal problem described
in terms of conditional (if … then) statements:
Causal conditional: There are three billiard balls
on a table that act in the following way: If Ball 1
moves, then Ball 2 moves. If Ball 2 moves, then
Ball 3 moves.
Imagine that Ball 2 could not move. Would
Ball 1 still move?
The fact that we are talking about billiard balls—
prototypical causal elements—strongly suggests that
the conditional statements should be interpreted as
describing causal relations. The causal model under-
lying this scenario is depicted in Figure 6-4.
The causal modeling framework represents the two
questions using the do-operator because an outside
agent is preventing the ball from moving, represented
as do(Ball 2 does not move):
P(Ball 1 moves | do(Ball 2 does not move)).
To evaluate this, we must assume that Ball 2 does
not move. We must also simplify the causal model
by removing any links into Ball 2 as depicted in
Figure 6-5.
It is immediately apparent, parallel to the last
example, that the value of Ball 1 is no longer affected
by Ball 2, and therefore the causal Bayes model
framework predicts that Ball 2’s lack of movement is
not diagnostic of its normal cause, Ball 1. Of partici-
pants, 90% agreed, affirming that Ball 1 could move if
Ball 2 could not.
Standard propositional logical systems have no
way to represent this argument. They not only do not
have a representation of cause, but also have no way
of representing an intervention. A conventional logical
analysis of this problem might go as follows: The prob-
lem tells us that if Ball 1 moves, then Ball 2 moves. We
know that Ball 2 does not move. Therefore, Ball 1 does
not move by modus tollens, a logical schema that dic-
tates that the antecedent of a conditional must be false
if its consequent is. This particular argument does not
explain people’s judgments, which are that Ball 1 can
move even if Ball 2 cannot.
In the noncausal realm, modus tollens can be a
perfectly valid form of argument for deriving definite
conclusions. For example, modus tollens would be an
appropriate inference scheme to use on a problem
similar to the causal one just shown but based on log-
ical if-then relations rather than causal ones. Maybe
people would make inferences conforming to modus
tollens with such an argument. To find out, Sloman
and Lagnado (2005) gave a group of people the
following scenario:
Logical conditional. Someone is showing off her
logical abilities. She is moving balls without break-
ing the following rules: If Ball 1 moves, then Ball
2 moves. If Ball 2 moves, then Ball 3 moves.
Sloman and Lagnado then asked the group the same
question as for the causal case:
Imagine that Ball 2 could not move, would Ball 1
still move?
In this case, only 45% of participants said yes. The
majority gave the inference consistent with modus
tollens, no. Clearly, there is less consistency than in
the causal case, probably because participants are
more confused in a logical than in a causal context.
CAUSAL REASONING THROUGH INTERVENTION
93
FIGURE 6-5 Causal chain model altered by an intervention on the interme-
diate event (adapted from Sloman & Lagnado, 2005).

Their answers are more wide ranging, and they tend
to express less confidence. People’s discomfort with
logical problems relative to causal ones arises either
because there are different forms of logic and they are
not sure which one to pick or because no form of
deductive logic comes naturally.
The experiments by Sloman and Lagnado (2005)
show that causal reasoning is not adequately modeled
by either standard propositional logic formalisms nor
traditional probabilistic theories that do not distin-
guish intervention from observation. Causal Bayes
nets are the best currently available account that mod-
els this competency.
Reasoning With Parameterized 
Causal Models
The preceding section showed that people can reason
qualitatively with causal models, and that they distin-
guish between observation and intervention.
Waldmann and Hagmayer (2005) have addressed
similar questions in the realm of learning. Following
the framework of causal model–theory (Waldmann,
1996; Waldmann & Martignon, 1998; see also
Lagnado et al., chapter 10, this volume), participants
were told about the structure of a causal model gener-
ating the learning data before being shown the data.
The learning data consisted of individual cases that
allowed participants to estimate the parameters of the
assumed causal model (e.g., causal strength, base
rates). The main questions were whether learners
were capable of deriving precise predictions on the
basis of the parameterized models and whether their
predictions differ depending on whether the predic-
tions are based on hypothetical observations or hypo-
thetical interventions. Again, causal Bayes nets
provided the formal tools to analyze this competency.
Associative theories are the dominant approach in
the realm of learning. They can differentiate between
observing and intervening by postulating separate learn-
ing modes: Whereas classical conditioning might be
viewed as underlying prediction, intervention might be
driven by instrumental conditioning (Dickinson, 2001;
see Domjan, 2003, for an overview). Thus, we might
learn in an observational learning paradigm (classical
conditioning) that the barometer reading predicts the
weather; in an interventional learning paradigm
(instrumental learning), we might also learn that fid-
dling with the barometer does not change the weather.
However, although this approach approximates causal
knowledge in many contexts, it fails to capture the
relations between observation and intervention. The
separation between classical and instrumental condi-
tioning predicts that, without a prior instrumental
learning phase, we should be incapable of correctly
predicting what would happen in case of an interven-
tion in situations in which our knowledge is based on
observational learning. Our experiments show that this
is wrong. People not only were capable of deriving pre-
dictions for hypothetical interventions after a purely
observational learning phase, but also their predictions
were sensitive to the structure of the underlying causal
model and the size of the parameters.
Predicting the Outcomes of Hypothetical
Interventions From Observations
Experiment 2 of Waldmann and Hagmayer (2005)
provides an example of the learning task. In this exper-
iment, participants were taught either a common-cause
or a causal chain model. In a fictitious medical sce-
nario that involved hormone levels of chimpanzees,
they were told either that an increased level of the hor-
mone pixin P causes an increase in the level of sonin S
and of xanthan X (common-cause model), or that an
increase in the level of sonin causes the level of pixin to
rise, which in turn increases the amount of xanthan
(causal chain model) (see Figure 6-6). Waldmann and
Hagmayer compared these two models because the
common-cause model implies a dissociation between
observational and interventional predictions, whereas
the chain model implies identical predictions for both
types, allowing us to test whether people correctly dif-
ferentiate between causal models.
After the initial instructions, participants received
descriptions of the hormone levels of a set of 20 indi-
vidual chimpanzees as observational data. The causal
relations were probabilistic (see Figure 6-6). Using the
data, learners could estimate the parameters of the
causal models. Causal chain and common-cause
models have the same structural implications (they are
Markov equivalent); therefore, only one set of data was
presented that was coherent with both. The models
and the implied parameters are shown in Figure 6-6.
A Bayesian analysis of these parameterized models
implies for both models that the probability 
of increased levels of xanthan conditional on sonin
being observed to be at an elevated level is
P(X ↑S↑).82, whereas the corresponding con-
ditional probability is P(X↑S ↔) .18 when
94
CAUSATION AND INTERVENTION

the sonin level is normal. The base rate of the exoge-
nous causes in both models (i.e., sonin in the 
common-cause model, pixin in the chain model) was
set to 0.5.
For the causal chain model, the interventional
probabilities are identical to the observational proba-
bilities. For example, regardless of whether sonin is
observed to be increased or whether an increased level
was caused by means of an inoculation, the other two
hormones should be affected equally. However, an
intervention on sonin in the common-cause model
entails the removal of the causal arrow connecting
pixin and sonin. Therefore, the probability of xan-
than depends only on the base rate of its cause pixin
and the causal impact of this hormone on xanthan.
Thus, the interventional probability of xanthan is
P(X ↑do[S↑]) P(X↑do[S  ↔]) .5,
regardless of whether sonin is increased or normal.
To test whether participants’ judgments follow these
predictions, they were asked to make predictions about
hypothetical observations and hypothetical interventions
after having studied the learning data. All participants
were requested to estimate for a set of 20 new, previously
unseen chimpanzees the number of animals showing
elevated levels of xanthan based on the hypothetical
observation that sonin was at either an increased or nor-
mal level in these animals. The corresponding questions
about hypothetical interventions asked participants to
imagine inoculations that increased or lowered the level
of sonin in the 20 animals. The order of the test
CAUSAL REASONING THROUGH INTERVENTION
95
FIGURE 6-6 Conditions and data of Experiment 2 by Waldmann and Hagmayer (2005). Upward arrows sym-
bolize increased hormone levels; sideways arrows indicate normal levels. The parameters represent causal
strength (conditional probabilities) and base rates (unconditional probabilities).
Normal
Observation
Intervention
Estimates
Predicted by causal Bayes nets
Observation
Causal-Chain Model
Common-Cause Model
Intervention
Increased
Lowering
Increasing
Normal
Increased
Lowering
Increasing
0
2
4
6
8
10
12
14
16
18
20
FIGURE 6-7 Results of Experiment 2 of Waldmann and Hagmayer (2005). Mean responses and predicted
frequencies to observation and intervention questions.

questions was counterbalanced. The mean response to
the test questions and the answers predicted by the
causal model framework are shown in Figure 6-7.
The pattern of results shows that participants
correctly differentiated between observational and inter-
ventional predictions, and that they were sensitive to the
different implications of the contrasting causal models.
Whereas for the causal chain model learners correctly
predicted similar levels of xanthan independent of
whether sonin levels were observed or generated, a clear
dissociation was observed for the common-cause model.
The majority of participants concluded that the proba-
bility of xanthan is independent of the type of interven-
tion on sonin. A second finding was that, on average,
estimates were as predicted, although in some cases
there was a slight tendency to underestimate. The
largest deviation between the estimates and the norma-
tive values was found for the intervention lowering the
level of sonin (second pair of columns in Figure 6-7),
which is probably because participants had no data
about what would happen if the level of one hormone
fell below a normal level.
These results are beyond the grasp of associationist
theories. This is most obvious in the common-cause
model in which the predictions of the outcomes of
the hypothetical interventions turned out close to the
predicted value of 50%, even though participants had
never observed this value in the learning phase. These
predictions clearly support causal models as descrip-
tions of human reasoning. Apparently, reasoners rely
not only on the observed associations but also on the
underlying causal model to generate predictions.
Sensitivity to Parameters
To examine whether learners used the learned param-
eters for their predictions, Waldmann and Hagmayer
(2005) ran additional studies manipulating parameter
values across conditions. Their Experiment 4 provides
an example of this manipulation. In this experiment,
participants were instructed that a fictitious bacterial
infection in dogs has two causal effects, gastric prob-
lems and increased antibodies (i.e., common-cause
model). In two conditions, two different data sets were
shown to participants in a list format. The two data sets
varied the strength of the two causal relations. In one
condition (“strong-weak”), the bacterial infection had
a strong influence on gastric problems (P  .91) and
only a medium influence on the presence of antibod-
ies (P  .45). (P is a measure of contingency that
reflects the numeric difference between the probabil-
ity of the effect, gastric problems, conditional on the
presence and absence of the cause [e.g., bacterial
infection].) In the other condition, the assigned causal
strength was reversed (“weak-strong”) (see Figure 6-8).
The base rate was the same (0.55) in both conditions.
Participants were requested to estimate the fre-
quency of antibodies in a new set of 20 dogs assuming
either that gastritis was observed to be present or absent
or that the presence or absence of gastritis was caused
by means of an external intervention (inoculation).
Although the structure of the causal model is identi-
cal in both conditions, the parameters implied by the
two data sets have distinctive implications for the 
different types of predictions (see Figure 6-8). Because of
the underlying common-cause model, an external inter-
vention in gastric problems has no causal influence on
the infection rate and the presence of antibodies. This is
because of graph surgery, which requires removal of the
causal arrow between the common-cause infection and
gastritis. The probability of antibodies is solely deter-
mined by the base rate of the bacterial infection and its
causal impact on the antibodies. Therefore, antibodies
are more likely in the condition in which bacterial infec-
tion has a strong influence (i.e., weak-strong) than when
it has only a weak impact (i.e., strong-weak).
The different parameters in the two conditions
imply different predictions not only for the intervention
96
CAUSATION AND INTERVENTION
FIGURE 6-8 Conditions and data of Experiment 4 of Waldmann and Hagmayer (2005).

questions but also for the observation questions. In
general, the implied probabilities are higher if gastri-
tis is observed to be present than if it is absent. In
addition, the probability of antibodies is higher in
the weak-strong condition than in the strong-weak
condition.
In Figure 6-9, the mean responses are compared
with the values predicted by the causal model. The
results show that participants again differentiated
between predictions for hypothetical observations and
hypothetical interventions. Moreover, the estimates
also demonstrate that participants were sensitive to the
parameters of the causal model. On average, partici-
pants’ estimates were quite accurate, although there
were again small deviations that could be due to
regression effects. This competency is rather surprising
considering the complexity of the task.
Sensitivity to the size of parameters was shown not
only for the causal strength parameters but also for the
base rate parameters. In another experiment (Waldmann
& Hagmayer, 2005, Experiment 3), the base rate of the
common cause was manipulated while holding causal
strength constant. This should particularly affect the
interventional predictions (based on interventions on the
first effect) as the probability of the predicted second
effect in this case varied in proportion to the base rate of
its cause (see Figure 6-2). The results showed that partic-
ipants incorporated the base rate information in their
predictions in a way that was surprisingly close to the
predictions of causal Bayes nets.
Causal Decision Making
The distinction between observation and interven-
tion also has practical implications for decision mak-
ing (Sloman, 2005). For example, if we observe low
values on a barometer, then we will probably take
our umbrella because the probability of rain is high.
But, we also know that setting the barometer by
means of an intervention will not affect the weather.
The evidential relation between the barometer read-
ing and the weather is spurious and mediated by
atmospheric pressure, which acts as a common cause
that independently affects the barometer and the
weather. Thus, observing a low reading of the barom-
eter because of tampering should not influence our
decision to take an umbrella. This example shows
that causal models and the distinction between obser-
vation and intervention are highly relevant to deci-
sion making. Specifically, choice is a form of
intervention and should be modeled as such by
breaking the edge between the variable with the
value that is chosen and its normal causes. However,
most theories of decision making, certainly most nor-
mative theories, analyze decision making on the basis
of evidential relations between variables (e.g., subjec-
tive expected utility theory).
In contrast, in line with the analyses of causal Bayes
nets and previous work on causal expected utilities
(Nozick, 1969, 1995), Sloman and Hagmayer (2006)
propose that choice is equivalent to an intervention in
CAUSAL REASONING THROUGH INTERVENTION
97
Absent
Observation
Intervention
Estimates
Predicted by causal Bayes nets
Observation
Weak-Strong Condition
Strong-Weak Condition
Intervention
Present
Remove
Generate
Absent
Present
Remove
Generate
0
2
4
6
8
10
12
14
16
18
20
FIGURE 6-9 Results of Experiment 4 of Waldmann and Hagmayer (2005). Mean responses and
predicted frequencies for the observation and intervention questions.

a causal network. They claim that in decision making
people first consider a causal model of the decision
context and then explore the causal consequences of
their possible interventions.
Simple Choices
Hagmayer and Sloman (2005) presented participants
with simple decision problems, such as the following:
Recent research has shown that of 100 men who
help with the chores, 82 are in good health,
whereas only 32 of 100 men who do not help with
the chores are. Imagine a friend of yours is married
and is concerned about his health. He read about
the research and asks for your advice on whether
he should start to do chores or not to improve his
health. What is your recommendation? Should he
start to do the chores or not?
Hagmayer and Sloman also provided participants in
different conditions with one of two causal models that
might underlie the correlation between chores and
health. In one condition, the relation was because of a
common cause, the degree of concern, that independ-
ently influences the likelihood of doing the chores and of
entertaining health-related activities, or in the alternative
direct-link model, it was pointed out that chores are an
additional exercise directly improving health.
Participants received several different decision
problems involving a range of issues, from the rela-
tion between high-risk sports and drug abuse to the
relation between chess and academic achievement.
If participants equate choices with interventions,
then they should often recommend not acting in the
common-cause condition because intervening on an
effect of a common cause does not alter the spuri-
ously correlated collateral effect. Such an interven-
tion would simply render the action independent of
the rest of the model, including the desired outcome.
In contrast, in the second condition, participants
should recommend doing the chores because this
variable is directly causally related to health.
Participants’ judgments turned out to be in accor-
dance with the causal model–theory of choice.
Despite learning about an identical evidential rela-
tion, only 23% of the participants in the common-
cause condition advised their hypothetical friend to
act, in contrast to 69% of the participants in the
direct-link condition.
Complex Choices and Newcomb’s Paradox
The difference between observational and interven-
tional probabilistic relations is crucial in more
complex cases as well. Newcomb’s paradox is an
interesting test case because it involves a conflict
between two principles of good decision making: 
(a) maximizing expected utility and (b) dominance
(i.e., choosing the option that always leads to the
better outcome) (see Nozick, 1969, 1995). Classical
decision theory cannot handle this paradox as it has
no principled way to choose between these alterna-
tive criteria; however, a causal analysis in some 
cases can.
Table 6-1 illustrates a variant of Newcomb’s para-
dox that Hagmayer and Sloman (submitted) used in
an experiment. In this experiment, students were
asked to imagine being the marketing executive of a
car manufacturer and having to choose between two
advertising campaigns. The manufacturer could pro-
mote either their sedan or their minivan. However,
according to the instructions, the expected sales
depend not only on the executive’s decision but also
on the marketing decision of the manufacturer’s main
competitor (see Table 6-1).
As the payoff matrix of Table 6-1 shows, higher
sales are expected for the minivan regardless of the
competitor’s campaign. Therefore, the principle of
dominance prescribes promoting the minivan.
However, participants were also informed that in the
past the two car companies ended up promoting the
same type of car in 95% of the cases, with either car
promoted equally often. If this additional information
is taken into account, then the expected value of
promoting the sedan turns out to be higher than that
of the minivan (29.250 vs. 21.000). Thus, the princi-
ple of maximizing expected value implies the oppo-
site of the principle of dominance.
To investigate the influence of the assumed causal
model, participants were also informed about the
causal relations underlying the observed evidential
relations. In one condition, participants were told that
the competitor tends to match the participant’s strategy
(direct-cause model); in the other condition, they were
told that both car companies make their decisions
independently based on the market (common-cause
model). After considering the information, partici-
pants were requested to choose one of the available
options.
98
CAUSATION AND INTERVENTION

Under the direct-cause model, the evidential prob-
abilities between the choices of the two competitors
indicate a stable causal relation. Therefore, the causal
expected utility equals the evidential expected utility,
and the sedan should be promoted. In contrast, under
a common-cause model, the choice should be viewed
as an intervention that is independent of the competi-
tor’s choice, with the competitor supposed to choose
on the basis of the state of the market. Because a free
choice destroys the evidential relation between the
choices of the participant and the hypothetical com-
petitor, the assumption that both choices are almost
guaranteed to coincide is no longer tenable. Thus, the
dominant option is the best choice under a common-
cause model.
The results show that decision makers were sensi-
tive to the structure of the underlying causal model,
and that they tended to treat choices as interventions.
Whereas traditional theories of decision making fail,
causal Bayes nets provide a coherent account to
model decision making in causal domains.
Final Remarks
Causal Bayes net theories differentiate between pre-
dictions based on observations, interventions, and
counterfactuals. In this chapter, we reviewed evi-
dence concerning this distinction. Traditional proba-
bilistic and associationist theories are incapable of
distinguishing between the different predictions
entailed by hypothetical observations and interven-
tions. The results of the experiments show that people
are remarkably good at distinguishing between predic-
tions based on observed events on one hand and pre-
dictions based on hypothetical interventions on the
other. Although observational predictions are based on
the structure of a relevant causal model, interventions
require mentally modifying the model prior to deriv-
ing predictions by “undoing” the link between the
intervened-on variable and its causes. People not only
are capable of deriving qualitative predictions implied
by the structure of a causal model, but also proved
capable of incorporating learned quantitative param-
eters in their predictions (Waldmann & Hagmayer,
2005).
It turns out that children also excel at differentiat-
ing interventions from observations (see Schulz,
Kushnir, & Gopnik, chapter 5, this volume). They
proved capable of deriving predictions for novel inter-
ventions from previous observations. For example, in
one experiment children were shown different causal
structures, such as a switch turning a gear A, which
spins a second gear B. Children 4.5 years old were able
to predict what would happen if either of the gears was
placed on a toy and the switch turned on. Although
they expected Gear A to spin, they did not expect Gear
B to rotate. This shows that children are able to derive
correct predictions, at least for simple, deterministic
causal structures (see Schulz, Kushnir, & Gopnik,
chapter 5, this volume, for more details and further
evidence). Recently Blaisdell and Waldmann
(Blaisdell et al., 2006) showed that even rats grasp the
difference between interventions and observations.
The distinction between observation, interven-
tions, and counterfactuals is relevant not only for
inference within a causal model, but also for the
induction of causal models (in this volume, see
Schulz, Kushnir, & Gopnik, chapter 5; Lagnado et al.,
chapter 10; Sobel, chapter 9; Griffith & Tenenbaum,
chapters 19 and 20, for theory and evidence). The
empirical results indicate that adults as well as chil-
dren can infer causal models using evidence from
observations and interventions together (Gopnik et al.,
2004, Steyvers, Tenenbaum, Wagenmakers, & Blum,
2003; Tenenbaum & Griffiths, 2003). However, peo-
ple seem to have a limited capacity to derive causal
models based on observations alone (Hagmayer,
2001; Lagnado & Sloman, 2004).
The evidence reviewed in this chapter strongly
suggests that people find it natural and easy to draw
different inferences from observations and from
interventions when reasoning and when making
decisions. The difference between observation and
intervention has to do with why an event occurs or
how a variable obtains its value (i.e., with the
mechanism that produces the event or value).
CAUSAL REASONING THROUGH INTERVENTION
99
TABLE 6-1 Payoff Matrix ($)
Additional Sales
Competitor Promotes Competitor Promotes
Sedan
Minivan
You promote sedan
30,000
15,000
You promote minivan
40,000
20,000
Source: Hagmayer and Sloman (in preparation).

Hence, the distinction between observation and
intervention is grounded in causal knowledge, in an
understanding of the mechanisms that produce
change. Thus, people’s ease of reasoning about
observation versus intervention would seem to indi-
cate quite directly competence with causal reason-
ing, and this would be a direct consequence of a
system that is designed for action, for achieving
effects through intervention.
References
Blaisdell, A. P., Sawa, K., Leising, K. J., & Waldmann, M. R.
(2006). Causal reasoning in rats. Science, 311,
1020–1022.
Dickinson, A. (2001). Causal learning: An associative
analysis.
Quarterly Journal of Experimental
Psychology, 54B, 3–25.
Domjan, M. (2003). The principles of learning and behav-
ior (5th ed.). Belmont, CA: Thomson/Wadsworth.
Fisher, R. (1951). The design of experiments. Edinburgh:
Oliver and Boyd.
Glymour, C. (2001). The mind’s arrows: Bayes nets and
graphical causal models in psychology. Cambridge,
MA: MIT Press.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 3–32.
Hagmayer, Y. (2001). Reasoning through causal models
versus reasoning about causal models. Unpublished
doctoral thesis, University of Göttingen, Göttingen,
Germany.
Hagmayer, Y., & Sloman, S. A. (2005). A causal model
theory of choice. In B. G. Bara, L. Barsalou, and
M. Bucciarelli (eds.), Proceedings of the Twenty-
Seventh Annual Conference of the Cognitive
Science Society. Mahwah, NJ: Erlbaum.
Hagmayer, Y., & Sloman, S. A. (subm.). Causal consid-
erations in Newcomb’s Paradox. Paper submitted
for publication.
Lagnado, D. A., & Sloman, S. A. (2004). The advantage
of timely intervention. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 30,
856–876.
Nozick, R. (1969). Newcomb’s problem and two prin-
ciples of choice. In N. Rescher (Ed.), Essays in
honor of Carl G. Hempel (pp. 107–133).
Dordrecht, Netherlands: Reidel.
Nozick, R. (1995). The nature of rationality. Princeton,
NJ: Princeton University Press.
Pearl, J. (1988). Probabilistic reasoning in intelligent 
systems: Networks of plausible inference. San
Francisco, CA: Morgan Kaufmann.
Pearl, J. (2000). Causality. Cambridge, England:
Cambridge University Press.
Reichenbach, H. (1956). The direction of time. Berkeley,
CA: University of California Press.
Sloman, S. A. (2005). Causal models: How we think
about the world and its alternatives. Cambridge,
MA: Oxford University Press.
Sloman, S. A., & Hagmayer, Y. (2006). The causal logic
of choice. Trends in Cognitive Science, 10, 407–412.
Sloman, S. A., & Lagnado, D. (2005). Do we “do”?
Cognitive Science, 29, 5–39.
Spirtes, P., Glymour, C., & Scheines, R. (1993).
Causation, prediction, and search. New York:
Springer.
Steyvers, M., Tenenbaum, J. B., Wagenmakers, E.-J., &
Blum, B. (2003). Inferring causal networks from
observations and interventions. Cognitive Science,
27, 453–489.
Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-
based causal induction. In S. Becker, S. Thun,
and F. Obermayer (eds.), Advances in neural
information processing systems, 15, 35–42.
Cambridge: MIT Press.
Waldmann, M. R. (1996). Knowledge-based causal
induction. In D. R. Shanks, K. J. Holyoak, & D. L.
Medin (Eds.), The psychology of learning and moti-
vation: Vol. 34. Causal learning (pp. 47–88). San
Diego, CA: Academic Press.
Waldmann, M. R., & Martignon, L. (1998). A Bayesian
network model of causal learning. In M. A.
Gernsbacher & S. J. Derry, Proceedings of the 20th
Annual Conference of the Cognitive Science Society
(pp. 1102–1107). Mahwah, NJ: Erlbaum.
Waldmann, M. R., & Hagmayer, Y. (2005). Seeing 
versus doing: Two modes of accessing causal
knowledge. Journal of Experimental Psychology:
Learning, Motivation, and Cognition, 31, 216–227.
Woodward, J. (2003). Making things happen. A theory of
causal explanation. Oxford, England: Oxford
University Press.
100
CAUSATION AND INTERVENTION

Introduction
Karl Popper (1962) characterized science as a process
of “conjectures and refutations.” Scientists formulate
hypotheses or conjectures that yield falsifiable predic-
tions. They then set about testing those predictions. If
the predictions are not borne out, then the hypothesis
is refuted, and a new conjecture must be found. For
better or for worse, analytic philosophy has largely
proceeded according to a parallel method, a method
of analyses and counterexamples. Philosophers for-
mulate an analysis of some important concept, which
is then compared with a variety of test cases (often
hypothetical). I offer a standard illustration involving
the concept of causation, which may be useful for
readers without a philosophical background.
My main concern, however, is to use this example to
illustrate the variety of causal concepts that we deploy.
When we ask whether one thing causes another, we may
be asking many different things. For instance, we may 
be asking whether the relationship between them is
genuinely causal, as opposed to accidental or spurious.
Or, we may be asking about the direction of the relation-
ship between them: Does the one cause or prevent the
other? Or, we may be asking about the strength of the
relationship between them: Does the one have enough
of an impact on the other to count as causing it?
Treatments of causation in philosophy and psychology
typically run these questions together, but there is no rea-
son to expect all of these questions to have a uniform
answer. I argue that when we carefully distinguish these
sorts of questions, the different combinations of answers
that may be provided form the basis of a taxonomy of
causal relationships. Such a taxonomy is better able to
illuminate the nature of causal reasoning than is any the-
ory that aims only to define causation simpler.
An Analysis and a Counterexample
Let us begin by formulating a probabilistic theory of
causation, which I call the R-S theory because it is
101
7
On the Importance of Causal Taxonomy
Christopher Hitchcock

based on (although not quite identical to) definitions
offered by Reichenbach (1956) and Suppes (1970).
According to this theory, causes and effects are repre-
sented by events in a probability space and are indexed
according to the time when they occur. Ct is a cause of
Et′ just in case the following conditions hold:
(i) t  t′
(ii) P(Et′ | Ct) 	 P(Et′)
(iii) There is no time t
t and no event Bt″ such
that
P(Et′ | CtBt″)  P(Et′| Bt″)
The time at which C occurs must be earlier than
the time at which E occurs; the occurrence of C at
time t must render the occurrence of E at t″ more
probable, and there must be no earlier event B that
“screens off” C from E, that renders C probabilisti-
cally irrelevant to E. Although I treat R–S as illustra-
tive of the sorts of attempts philosophers have made to
define causation, I am not concerned here to evaluate
the specific proposals put forward by either
Reichenbach or Suppes.1
Hesslow (1976) has offered a counterexample to this
analysis of causation involving the effect of birth control
pills on thrombosis (the formation of blood clots in the
arteries). Let us make some background assumptions:
We have a population of healthy women, all of whom
are married, under the age of 35, nonsmokers, and
capable of conceiving children. Such women are
among the most likely consumers of birth control pills.
Thrombosis is considered to be one of the most danger-
ous side effects of birth control pills. This means, pre-
sumably, that the consumption of oral contraceptives is
a cause of thrombosis. Pregnancy is also an important
risk factor for thrombosis. This is hardly a coincidence:
Birth control pills prevent pregnancy by mimicking the
hormonal effects of pregnancy, so birth control pills
have many of the same side effects as pregnancy itself. It
turns out that pregnancy is a much stronger risk factor
for thrombosis than birth control pills. Thus, by using
birth control pills, a woman in our hypothetical popula-
tion would actually lower her overall probability of
thrombosis because birth control pill use is such an
effective preventive measure against pregnancy.
We 
represent 
this 
case 
schematically 
in 
Figure 7-1. Intuitively, the thickness of the arrow corre-
sponds to the strength of the causal influence in 
question; the sign attached to each arrow indicates
whether one factor causes or prevents the other. 
The probabilities of the various outcomes (using obvi-
ous abbreviations and suppressing temporal subscripts)
are as presented in Table 7-1. In the table,
1. The probability of thrombosis is lower, condi-
tional on birth control pill use, than it is overall.
2. Conditional on becoming pregnant, the proba-
bility of thrombosis is higher conditional on
birth control pill use.
3. Conditional on avoiding pregnancy, the proba-
bility of thrombosis is higher conditional on
birth control pill use.
We have, then, an apparent counterexample to the
R-S analysis of causation: The consumption of birth
control pills is a cause of thrombosis, even though
Clause (ii) of the analysis is not satisfied (it is contra-
dicted by inequality 1 in Table 7-1). Some authors, such
as Salmon (1984) and Dowe (2000), use related exam-
ples to reject probabilistic analyses of causation generally
and to recommend in their place accounts of causation
that focus on the physical process that connect causes
with their effects. (But, see Hitchcock, 1995a, and
Schaffer, 2000, for criticisms of these proposals.)
Many Questions
When we ask about a particular causal relationship,
such as the relationship between thrombosis and the
102
CAUSATION AND INTERVENTION
Thrombosis
Pregnancy
Birth Control Pills



FIGURE 7-1 Hesslow’s counterexample.
TABLE 7-1 The probabilities in Hesslow’s counter-
example
1. P(Throm|Pills)  P(Throm)
2. P(Throm|Pills & Preg) 	 P(Throm|Preg)
3. P(Throm|Pills & No Preg) 	 P(Throm|No Preg)

consumption of birth control pills, there are a number
of different questions that might be posed. Let us start
by drawing a distinction between two main questions:
1. Is the relationship between birth control pills
and thrombosis causal at all?
2. Given that the relationship is causal in some
broad sense, what specifically is the nature of
the relationship?
We can elaborate further on the second question
by introducing a number of subquestions:
2a. What is the direction of causal influence? Do
birth control pills promote or encourage
thrombosis? Or, do they rather prevent or
inhibit thrombosis?
2b. What is the strength of the relationship? How
efficacious are birth control pills in bringing
about thrombosis? Are they stronger causes of
thrombosis than pregnancy is?
2c. How does birth control pill use compare with
various alternatives? What are the conse-
quences of birth control pill use for thrombo-
sis in contrast with other birth control
methods or in contrast with the failure to
employ contraception at all?
2d. To what extent is the relationship between
birth control pills and thrombosis stable across
different background conditions? How do
birth control pills interact with other causes of
thrombosis? Is the effect of birth control pill
use on thrombosis different for different kinds
of women, and if so, how?
2e. Via what causal pathways do birth control pills
affect thrombosis? How does the influence of
birth control pills differ along these various
paths? How do these pathways combine to
yield an overall effect?
Let us now examine each of these questions in
greater detail.
Question 1: Causal Relationships
I do not attempt to offer any precise account of what
a relationship is. The sorts of things I have in mind
are: (a) regularities or predictable patterns of co-
instantiation among types of events; (b) probabilistic
correlations of the sort described in Table 7-1; and 
(c) mathematical functions connecting the values of
quantitative variables. The first key question, then, is
whether a specific relationship between events, event
types, or variables reflects a causal influence of one on
the other.
Let us suppose that we have observed the popula-
tion of women described in Hesslow’s (1976) exam-
ple, and that we have kept careful statistics regarding
which have taken birth control pills, which have
become pregnant, and which have suffered from
thrombosis. Let us suppose, moreover, that these sta-
tistics are in accord with the probabilities presented in
Table 7-1. These probabilities describe certain statis-
tical relationships that hold between birth control pill
use and thrombosis. What does it mean to ask
whether these relationships are causal?
We may start by saying what we mean to exclude.
There are at least two different ways in which these
relationships might fail to be causal. First, our sam-
ple might not be representative. In this case,
although thrombosis rates happen to be lower
among pill users in our particular population sam-
ple, this would not reflect any kind of underlying
connection between them; if we were to continue
sampling from an idealized infinite population, then
the negative correlation between pill use and throm-
bosis would (with probability 1) disappear. In lan-
guage familiar to philosophers, the correlations are
accidental. This terminology is drawn from philo-
sophical discussions about laws of nature. According
to one traditional view, laws of nature are universal
generalizations. For example, it is a law of nature
that all massive objects travel at a velocity less than
that of light. But, not all universal generalizations
are laws of nature. It may be that all samples of pure
gold that ever have existed or ever will exist have a
mass of less than 1,000 kilograms, but this is not a
law. It is, rather, an accidental generalization, a con-
tingent feature of the way our world just happens to
unfold. (See, for example, Goodman, 1955, for a
classic exposition of this issue.) Causal relationships
need not be laws—in saying that birth control pills
cause (or prevent) thrombosis, we are not claiming
that all (or none) of the women who consume birth
control pills develop thrombosis. Nonetheless,
causal relationships do seem to be distinguishable
from accidental relationships in much the same way
that laws of nature are often thought to be.
Second, it may be that the relationship between
birth control pills and thrombosis, although not
merely accidental, is sustained by a common cause
or by some more complex causal structure that does
ON THE IMPORTANCE OF CAUSAL TAXONOMY
103

not involve any causal influence of birth control pills
on thrombosis. It may be, for example, that women
who have more limited access to health care are less
likely to employ birth control pills and are more sus-
ceptible to thrombosis. If this were the case, then the
first inequality of Table 7-1 would not reflect any
causal influence of birth control pills on thrombosis:
Birth control pill use would merely be a symptom of
access to medical care, the factor that would really be
influencing whether thrombosis occurs. In this case,
we would say that the (negative) correlation between
birth control pills and thrombosis is spurious. The
third clause of the R-S analysis is intended to rule out
spurious correlations: In the hypothetical case just
described, conditioning on access to health care
would render oral contraceptive use probabilistically
irrelevant to thrombosis.
We should also briefly mention the possibility that
thrombosis affects birth control pill use rather than vice
versa. This possibility is consistent with the probabilistic
information reported in Table 7-1. However, if we have
the further information that birth control pill use com-
mences before the onset of thrombosis, then this possi-
bility can be ruled out. Condition (i) of R-S reflects this
temporal constraint on causal relationships.
Thus, a causal relationship between birth control
pill use and thrombosis is a relationship that is neither
accidental nor spurious. But, what are the positive
characteristics of these relationships that distinguish
them from other, noncausal relationships? It is in
response to this particular question that the interven-
tionist approach to causation, developed especially by
Woodward (2003, chapter 1, this volume), is particu-
larly promising.
This approach is best motivated by asking why we
are so interested in learning about causal relation-
ships.2 Suppose, first, that we are interested in predict-
ing who will develop thrombosis and who will not. It
is easy to see why the probabilistic information pre-
sented in Table 7-1 will be especially useful for us: It
tells us that women who fall into certain categories
are more likely than others to develop thrombosis. If
the probabilistic relationships described in Table 7-1
are merely accidental, then, although these relation-
ships are still useful for predicting thrombosis within
this particular population of women, we would have
no reason to expect that they would provide reliable
guides to prediction within other groups of women.
So, it is not surprising that we should find nonacci-
dental relationships to be particularly important.
Why should we be interested in causal rather than
spurious relationships? If we were only interested in
predicting the outcomes of observed initial condi-
tions, then we would have no reason to prefer causal
relationships to spurious ones. The consumption of
birth control pills can be a reliable indicator of
whether a woman will develop thrombosis regardless
of whether it affects thrombosis or is merely sympto-
matic of some other underlying condition that itself
affects thrombosis. It is only when we seek to inter-
vene in the normal course of events that causal rela-
tions take primacy of place.
Suppose, for example, that we are contemplating a
policy of providing free oral contraceptives to some of
the women in our hypothetical population, and we
wish to predict the incidence of thrombosis under this
protocol. If the correlation between birth control pill
use and thrombosis is spurious, then we cannot neces-
sarily rely on the probabilities given in Table 7-1 to pre-
dict the outcome. For example, if the correlation arises
because access to health care influences both pill use
and thrombosis, then we would not expect this correla-
tion to persist under our new protocol. If we provide
free access to birth control pills, then we can no longer
expect birth control pill use to be symptomatic of
access to health care more generally. Causal relation-
ships, by contrast, remain stable under the sorts of
interventions that would disrupt spurious relationships.
It is for this reason that knowledge of causal relation-
ships is especially valuable to us. According to the 
interventionist approach to causation developed by
Woodward and others, this invariance of a relationship
under interventions is a defining feature of a genuinely
causal relationship, in the sense of Question 1.
This is only a preliminary sketch of an interven-
tionist approach to causation; see Woodward’s chapter
1 in this volume for a more detailed presentation. My
intent here is not to defend the interventionist
approach in detail, but only to appeal to its broadly
practical orientation to help give a sense of what the
first question—whether the relationship between
birth control pills and thrombosis is causal at all—is
asking. The same practical orientation can also be
used to motivate the second question, along with its
various subquestions. If we want to know what the
effect of our contemplated policy intervention will be,
then it is not nearly enough to know that there is some
causal relationship between pill use and thrombosis;
we will also need to know something more about the
details of this relationship.3
104
CAUSATION AND INTERVENTION

Question 2a: Causal Direction
In our interventions in the world, we typically seek to
promote those outcomes that we deem desirable and
to prevent or inhibit those outcomes that we wish to
avoid. Thus, we will typically aim to prevent or inhibit
thrombosis, while we may seek to either prevent or
promote pregnancy, depending on our desires at a
particular stage in our lives. This distinction is fairly
intuitive and was marked by the inclusion of  and 
signs in Figure 7-1. Unfortunately, our language can
be treacherous here, for we often use the word cause
specifically to mean promote. In this usage, cause and
prevent (or inhibit) are antonyms. Nonetheless, pre-
vention is a kind of causal relationship. Birth control
pills prevent pregnancy; they do not cause pregnancy.
But, it would be wrong to conclude from this that the
relationship between birth control pills and pregnancy
is not causal (i.e., that it is merely accidental or spuri-
ous). Thus, when we ask whether one thing is a cause
of another, we must take care to specify if we are
inquiring whether a causal relationship exists or more
specifically whether the causal relationship points in a
certain direction—whether the cause promotes the
effect rather than inhibiting or preventing it.
How might we characterize the distinction
between promoting causes on the one hand and pre-
venting or inhibiting causes on the other? Within the
probabilistic framework of R-S, a natural proposal
would be that a promoting cause increases the prob-
ability of the outcome in question; a preventing
cause decreases the probability. This proposal would
have the advantage of tying our practical maxim—
promote desirable outcomes and prevent undesirable
ones—to standard decision theory: An action
increases expected utility to the extent that it renders
high-utility outcomes more probable and low-utility
outcomes less probable.
In examining the probabilities in Table 7-1, we see
that conditioning on birth control pill usage can either
increase or decrease the probability of thrombosis,
depending on what else is being conditioned on. For
this reason, it is not possible to characterize pill use
unambiguously as either a promoting or a preventing
cause of thrombosis. We return to this point in the sec-
tion on Question 2e regarding causal pathways.
So far, we have been restricting our attention to
binary variables; we have supposed that one can take
birth control pills or not, develop thrombosis or not,
become pregnant or not. (Pregnancy is a proverbial
binary variable: There is no such thing as being a “little
bit pregnant.”) The natural and social sciences are often
concerned with quantitative variables, however. For
example, a physicist might be interested in the relation-
ship between electrical potential and current, a sociolo-
gist in the relationship between education level and
income, and a macroeconomist in the relationship
between unemployment and inflation. If the relation-
ship between two variables is monotonically increasing
or monotonically decreasing, then it might be perfectly
natural to import the language of promotion and pre-
vention to describe the direction of a causal relation-
ship. If increasing levels of education correspond with
increasing levels of income, then it might be natural to
say that education is a promoting cause of income
(assuming that the relationship is causal at all, rather
than accidental or spurious). If the relationship between
unemployment and inflation is monotonically decreas-
ing, then high unemployment levels inhibit inflation.
There is no guarantee, however, that a relationship
between quantitative variables will be monotonic; it
might be U-shaped or sinusoidal. In such a case, it will
not be possible to provide a simple characterization of
the direction of the causal influence. (See Hitchcock,
1993, for further discussion of this point.)
Question 2b: Causal Strength
Some causal influences are stronger than others. In
Hesslow’s (1976) example, pregnancy is a stronger
cause of thrombosis than is pill use; we marked this
in Figure 7-1 by drawing a thick arrow from preg-
nancy to thrombosis and a thin arrow from pills to
thrombosis. Within a probabilistic framework, it is
natural to measure the strength of a causal influ-
ence in terms of the size of the difference in proba-
bility that a causal factor makes. To say that
pregnancy is a stronger cause of thrombosis than
pill use, for example, would suggest that, if we look
at the inequalities 2 and 3 in Table 7-1, then we
might expect the difference between the left- and
right-hand sides of each inequality to be small com-
pared to the difference between the left-hand sides
of 2 and 3 or to the difference between the right-
hand sides of 2 and 3.
One of the great virtues of Cheng’s (1997)
PowerPC model of causation is that it offers a pro-
babilistic measure of causal strength that is in many
ways superior to simpler measures that look only at the
difference or ratio of probabilities. When quantitative
ON THE IMPORTANCE OF CAUSAL TAXONOMY
105

variables stand in linear relationships, it is natural to
measure the strength of the relationship between them
in terms of correlation coefficients. As with the case of
causal direction, however, if the relationship between
two variables is sufficiently complex, then there may
be no natural way to characterize the strength of the
causal influence of one on the other.
In practice, we often ignore weak causal influ-
ences. Indeed, by selecting outcomes of interest we
effect a kind of course-graining that renders many
causal influences irrelevant. For example, the gravita-
tional influence of Alpha Centauri will make a slight
difference to the exact location of every molecule in a
woman’s body. We are, however, particularly inter-
ested in which women will develop thrombosis, and it
is extraordinarily unlikely that the effects wrought by
our stellar neighbor will ever make the difference
between developing thrombosis or not.
In connection with this last point, a distinction is
sometimes made between causing some outcome and
affecting or influencing it.4 Consider a woman who is
already at high risk of developing thrombosis—
indeed, she will eventually succumb regardless of
whether she takes birth control pills or becomes preg-
nant. Nonetheless, her use of birth control pills may
hasten or delay the onset of thrombosis; it may affect
the severity of her thrombosis or otherwise be relevant
to the manner in which thrombosis occurs. In such a
case, we might say that her taking birth control pills
did not cause her thrombosis (since she was going to
suffer from it anyway), but that it did affect or influ-
ence her thrombosis. We cannot take this distinction
too far, however: Socrates was mortal, but his drinking
hemlock nonetheless caused his death and did not
merely affect or influence it.
Question 2c: Contrasting Alternatives
In most philosophical treatments, and arguably in
common sense as well, causes and effects are events
or event-types. We ask about the effect of consuming
birth control pills on thrombosis or about the effects
of a particular woman’s consuming birth control pills
during a particular time frame on her case of throm-
bosis. As we have remarked, however, it is common in
the sciences to represent causal relationships as rela-
tionships among variables. The social scientist, for
example, does not ask whether having a high school
education causes one to have an income of greater
than $40,000 per year, but asks rather about the causal
relationship between education and income more
generally. The variable education might have values
representing the following education levels: never
completed high school; high school diploma; some
university education; bachelor’s degree; master’s
degree; doctorate or highest professional degree.
I have argued at length elsewhere (Hitchcock, 1993,
1995b, 1996a, 1996b) that there is something deeply
misleading about the ordinary philosophical concep-
tion of causation as a relation among events or event-
types and something deeply right about the scientific
practice of thinking in terms of causal relations among
variables. Causal claims are always explicitly or implic-
itly contrastive: When we represent causal relationships
as relations among variables, we are making explicit the
range of contrasting alternatives under consideration.
In Hesslow’s (1976) example, we are interested in
the effect of birth control pill use on thrombosis. But,
we must ask: Birth control pills as opposed to what? If
we contrast the use of oral contraceptives with the fail-
ure to employ contraception, then the probabilities
might well be as reported in Hesslow’s example. But,
suppose instead that we compare birth control pills
with other reliable forms of contraception. Then,
birth control pill use might make relatively little dif-
ference for whether a woman becomes pregnant—
women who use oral contraceptives become pregnant
at more or less the same rate as women employing
other effective means of contraception—while still
posing an additional risk of thrombosis. When we
make this comparison, then, it may turn out that birth
control pills increase the probability of thrombosis rel-
ative to other forms of contraception.
Putting the same point in a slightly different way,
we may embed the event-type birth control pill use as
a value of many different variables. One variable
might take as values {birth control pill use, no contra-
ception}; another might take as values {birth control
pill use, abstinence, male sterilization, female sterili-
zation, condom use, … }. These two variables need
not stand in the same relation to the variable that
takes as values {thrombosis, no thrombosis}.
Question 2d: Stability Across
Background Conditions
In presenting Hesslow’s (1976) counterexample, we
made a number of assumptions about the condition
106
CAUSATION AND INTERVENTION

of the women in our hypothetical population. Let us
now remove those assumptions: Suppose that some of
the women are celibate or infertile; some are older,
smokers, and otherwise at high risk of thrombosis.
Why not throw some men in there as well? The prob-
abilistic profile described in Table 7-1 will no longer
characterize all of the members of the population.
More specifically, there will now be some background
conditions B such that
P(Throm | Pills & B)  P(Throm | B);
and other background conditions B′ such that
P(Throm | Pills & B′) 	 P(Throm | B′);
and perhaps even B″, such that
P(Throm | Pills & B″)  P(Throm | B″).
The effect of birth control pills on thrombosis will
depend on what other relevant factors are present in
the background. In such a case, we say that birth con-
trol pill use interacts with these other factors.
How stable must the effect of birth control pills be
across the different background conditions in a hetero-
geneous population for us to continue talking mean-
ingfully of the effect of birth control pills? One
proposal, originally of John Stuart Mill (1843) and
given a probabilistic reformulation by Humphreys
(1989), is that a cause must raise the probability of its
effect in every possible background condition. One
upshot of this proposal is that event-types such as birth
control pill consumption will rarely count as causes of
anything. Rather, a cause will typically be a compli-
cated conjunction of factors. Thus, for example, we
would not be able to say that birth control pill con-
sumption causes (or prevents) thrombosis, but only that
pill consumption by women who are in such-and-such
specific physiological condition does so. A slightly dif-
ferent proposal, that of Eells (1991) is that causal
claims are population relative. Thus, we might say that
the consumption of birth control pills prevents throm-
bosis in the subpopulation of women who are fertile
and sexually active and satisfy various other physiologi-
cal conditions; birth control pills cause thrombosis in a
different subpopulation of women. In the population as
a whole, the best we can do is to say that oral contracep-
tive use is a “mixed cause” of thrombosis. It is clear,
however, that our ordinary usage of words like cause
and prevent is considerably more permissive than either
of these proposals would allow.
Question 2e: Causal Pathways
Figure 7-1 shows two causal pathways connecting
birth control pill use with thrombosis. Intuitively,
birth control pills affect one’s chances of developing
thrombosis in (at least) two different ways. First, they
have a direct effect by introducing hormones into the
subject’s body; second, they have an indirect effect by
preventing pregnancy, which is itself a risk factor for
thrombosis. This example is particularly interesting
because it turns out to be difficult to distinguish these
pathways by any kind of appeal to physical processes
or mechanisms. The reason is that the mechanisms
underlying the arrows in Figure 7-1 are more or less
identical. The chemical agents in birth control pills
that cause thrombosis are the same as the ones that
prevent pregnancy; pregnancy itself causes thrombo-
sis because it leads to production of essentially the
same agents.5
How, then, are we to distinguish the two different
causal pathways? From within a probabilistic framework,
it is the second and third inequalities in Table 7-1 
that supply the clue. Pregnancy is a causal intermedi-
ary between pill use and thrombosis: Oral contracep-
tive use influences whether pregnancy occurs, and
this in turn affects the occurrence of thrombosis. Yet,
when we control for pregnancy by conditioning on
whether pregnancy has occurred, there is a residual
correlation between birth control pill use and throm-
bosis. If the only effect of birth control pill use on
thrombosis was the one mediated by its effect on preg-
nancy, then we would expect pregnancy to screen off
pill use from thrombosis.
This idea is captured in the Markov condition, a
standard condition relating the probability distribu-
tion over a set of variables to the structure of a directed
graph representing the causal relations among them
(see Spirtes, Glymour, & Scheines, 2000). Although
the arrow diagram in Figure 7-1 was originally pre-
sented as an intuitive representation of the structure
of Hesslow’s example, it has the structure of a directed
graph. (We need to make a few changes to turn it into
a proper causal graph: Lose the  and – signs, ignore
the differences in the thickness of arrows, and inter-
pret the nodes as binary variables rather than event-
types.) When a graph contains an arrow from variable
X to variable Y, then X is said to be a parent of Y. The
Markov condition says that a variable X is independ-
ent of all other variables (either singly or in combina-
tion) except for descendents of X when we condition
ON THE IMPORTANCE OF CAUSAL TAXONOMY
107

on all of the parents of X; the parents of X screen it off
from all of its nondescendents. Now, we can see that
if we were to remove the arrow from Pill use to
Thrombosis in Figure 7-1, then the resulting graph
would no longer satisfy the Markov condition. With
the arrow removed, Pregnancy becomes the only par-
ent of Thrombosis. Yet, Pregnancy does not screen
Pill use off from Thrombosis, even though Pill use is
not a descendent of Thrombosis.
As Woodward notes in this book (chapter 1, sec-
tion on interventionism), an interventionist approach
to causation of the sort outlined in the section on
Question 1, causal relationships, may also be used to
identify the distinct causal pathways. First, we note
that by separately intervening on whether birth con-
trol pills are used and on whether pregnancy occurs,
we can determine that birth control pills prevent preg-
nancy, and that pregnancy is a cause of thrombosis.
(Never mind the ethical problems involved in inter-
vening to cause or prevent pregnancy; thought exper-
iments do not need to pass human subject review
boards.) Now, we can further inquire into what hap-
pens when we intervene to determine whether a
woman uses birth control pills, while simultaneously
and independently intervening to determine
whether pregnancy occurs. If the probabilistic corre-
lations reported in the second and third lines of
Table 7-1 persist under this protocol, then birth con-
trol pills have an effect on thrombosis in addition to
the influence they have in virtue of their effect on
pregnancy.
It is now possible to distinguish a variety of differ-
ent kinds of causal relation. The arrows in a causal
graph (such as the one derived from Figure 7-1) rep-
resent what are called direct effects. One variable has
a direct effect on another if it has an effect that is
unmediated by any other variable in some specified
variable set. Obviously, the notion of direct effect
must be relativized to a choice of variable set. In
another work (Hitchcock, 2001b), I provide a general-
ization of the notion of direct effect that is invariant
under the number of variables that are interpolated
along a causal pathway.6 I call this notion the compo-
nent effect of one variable on another along a causal
pathway. Pearl (2001) provides an even more power-
ful generalization, defining the notion of a path-spe-
cific effect for any causal pathway or complex network
of causal pathways. Finally, there is the notion that is
variously called net, total, or causal effect. This is the
overall effect that one variable has on another along
all of the available causal pathways. This effect is
reflected in the overall correlation between the two
variables when no causal intermediates are controlled
for. Thus, the first inequality in Table 7-1 reflects the
net effect of birth control pills on thrombosis.
Although I favor the term net effect to emphasize the
analogy between net and component forces in
Newtonian mechanics, net effects cannot simply be
computed by adding the various component effects;
the various component effects may interact with each
other in complicated ways (see Hitchcock, 2001b for
discussion).
In Hesslow’s (1976) example, birth control pill use
has two distinct component effects on thrombosis.
Along the direct pathway, the effect of birth control
pills is to weakly promote the occurrence of thrombo-
sis. Along the indirect pathway, the effect of birth con-
trol pills is to strongly prevent thrombosis. The net
effect is to moderately prevent thrombosis. Thus,
there is a sense in which it is correct to say both that
birth control pills cause thrombosis and that birth
control pills prevent thrombosis. The R-S theory cap-
tures the second claim; Hesslow’s counterexample
appeals to the first claim.
Causal Taxonomy
The different ways in which Questions 2a through 2e
can be answered correspond to different ways in
which one event or event-type can be causally related
to another. This is by no means an exhaustive list of
the ways in which we can articulate the nature of a
causal relationship, but it provides at least a healthy
start on a causal taxonomy. Note that these various
issues are not independent, but that there are interac-
tion effects between them. For example, we cannot
say anything unambiguous about the direction
(Question 2a) or the strength (2b) of the relation-
ship between birth control pill use and thrombosis 
unless we specify whether we are asking about one or
another component effect or about the net effect (2e).
How many causal pathways there are (2e) will depend
on which background conditions we are considering
(2d). Among women who are incapable of becoming
pregnant (for reasons independent of birth control
pill use), there will be no indirect effect of birth con-
trol pill use on thrombosis mediated by the possibility
of pregnancy. And, if it should turn out that the influ-
ence of birth control pills on thrombosis has zero
strength (2b) or no direction (2a), then we might
108
CAUSATION AND INTERVENTION

reject the initial assumption that they are causally
related in the first place (1).
One might try to classify causal relationships along
different dimensions. It may turn out, for instance,
that causation functions differently in the physical,
biological, and social domains. Or, we might try to
classify causal concepts according to theoretical
approach. I have discussed the R-S theory in some
detail and have made passing reference to a number
of other philosophical approaches: other probabilistic
theories, interventionist theories, counterfactual theo-
ries (for those of you paying close attention to the foot-
notes), and causal process theories. All but the last of
these theories share a common idea: the occurrence
of a cause makes a difference for its effect. The differ-
ent types of causal relationship that emerge from a
consideration of Questions 2a through 2e all corre-
spond to different ways in which a cause might make
a difference to its effect; thus, I expect that the sort of
taxonomy that I advocate may be adopted from within
the framework of any difference-making approach to
causation. Causal processes and interactions, the key
concepts in process theories of causation such as
those of Salmon (1984) and Dowe (2000), are cer-
tainly causal concepts that are distinct from those
enumerated, but they do not really belong in the tax-
onomy that I have been developing. Indeed, I am
rather skeptical that these concepts really help us to
understand better the nature of causal relationships,
in part for reasons noted (see also Hitchcock, 1995a,
2004a, 2004b; Schaffer, 2000).
Morals for Philosophy
The holy grail of philosophy, or at least for the project
of analyzing causation, has been to provide an ade-
quate account of the causal relation, to spell out just
what it means to say that C causes E. This analytic proj-
ect tacitly assumes that there is just one special type of
causal relationship called causing that is the target of
analysis. We can see how this is true of our R-S theory
by considering our various questions in turn.
First, the intent behind the R-S theory is that when
C causes E, the relationship between them is genuinely
causal in the sense of Question 1. Both Reichenbach
(1956) and Suppes (1970) advance versions of
Condition (iii) for the purpose of ruling out spurious
correlations. With hindsight, we now recognize that it is
not possible to distinguish causal relationships from 
spurious ones using purely probabilistic criteria.
Cartwright (1979/1983) offers a powerful argument for
this conclusion, and Spirtes et al. (2000) provide a vari-
ety of “statistical indistinguishability” results that bear
on this issue. Despite this shortfall, the screening-off
relations originally introduced by Reichenbach (1956)
have proven to be helpful for detecting certain kinds of
spurious relationship.
When we look closely at R-S, however, it is appar-
ent that it is not simply an attempt to define the
notion of a causal relationship in the sense of
Question 1. Let us look at how R-S relates to each of
Questions 2a through 2e in turn.
Question 2a. According to R-S, a cause must
increase the probability of its effect. This suggests
that a cause in the sense of R-S must be a promot-
ing cause.
Question 2b. R-S imposes no lower bound on how
much difference a cause must make to the proba-
bility of its effect. This suggests that something
may count as a cause no matter how weak or
insignificant its influence.
Question 2c. R-S requires us to compare the prob-
ability of E conditional on C with the uncondi-
tional probability of E, rather than with the
probability of E conditional on any specific alter-
native to C. In effect, we must compare P(E | C)
with the weighted average of probabilities of the
form P(E | C′), where C′ ranges over C and all pos-
sible alternatives to C. So, for R-S, a cause must be
a promoting cause on average, where the average
is taken with respect to alternatives to the cause.
This has the rather odd consequence that, whether
or not birth control pills cause thrombosis (e.g.)
will depend on the probabilities of various 
other forms of contraception being used. (See
Hitchcock, 1993, for elaboration.)
Question 2d. R-S does not require that a cause raise
the probability of its effect in every background con-
dition: Conditions (i)–(iii) are all compatible with
the existence of a background condition Bt″ such
that P(Et′ | CtBt″)  P(Et′ | Bt″ ) [although Condition
(iii) is obviously incompatible with a background
condition Bt″ such that P(Et′ | CtBt″)  P(Et′ | Bt″)].
R-S also does not require that a cause raise the prob-
ability of its effect in any circumscribed range of
background conditions. There is, however, an obvi-
ous sense in which Condition (ii) requires that a
cause raise the probability of its effect on average,
where the average is here taken over background
conditions.
ON THE IMPORTANCE OF CAUSAL TAXONOMY
109

Question 2e. Finally, R-S is suitable only for net
causes, for it never asks us to consider probabilities
conditional on events intermediate between the
cause and the effect. It is because of this feature, in
particular, that R-S fails to rule that birth control
pills cause thrombosis.
In brief, then, a cause in the sense of R-S is a net-
promoting-cause-on-average.
The principal moral that I wish to draw from our
taxonomy is that we should reject the assumption that
there is one specific kind of causal relationship that is
the referent of the word cause. We use the word cause
to mark a variety of different distinctions: the distinc-
tion between a causal relationship and one that is
merely spurious; the distinction between causing an
outcome and preventing it; the distinction between
causing an outcome and merely affecting it; and so
on. When we reflect on the multifaceted nature of
Hesslow’s (1976) example, it seems clear that there is
no univocal answer to the seemingly simple question:
Does the consumption of birth control pills cause
thrombosis? There is a genuinely causal relationship
between birth control pills and thrombosis. The use
of birth control pills, when compared with substan-
tially less-effective forms of contraception (or no con-
traception at all), has a net inhibiting effect on
thrombosis among women in certain conditions. In
those same background conditions, birth control pills
also have a relatively small, direct, promoting effect
on thrombosis. Birth control pills also have a small
promoting net effect on thrombosis among women
who are unable to become pregnant or when con-
trasted with equally effective forms of contraception.
Does this specific causal profile amount to causation
or not? We should reject the question. The reason
that all attempts to analyze causation have met with
failure is that there simply is no one specific relation-
ship of causation to be analyzed. At any rate, there is
no one specific relationship that is picked out by our
pretheoretic use of the word cause. (See Hitchcock,
2003, for further polemics on this issue.)
Nonetheless, we should not simply reject all
attempts to analyze causation as bankrupt. What is
needed is a little redirection of effort. What we should
demand of a theory of causation is that it be able to pro-
vide an account of at least some of the causal concepts
brought out by our various questions. In my discussion,
I offered some suggestions for how a probabilistic
approach to causation in the spirit of R-S might be
used to illuminate some of the diverse causal 
concepts we have encountered. I do not attempt to 
pass judgment on the ultimate success of any of these
strategies.
The important point to recognize is that a theory’s
success or failure in analyzing one of these concepts
may be largely independent of its performance in ana-
lyzing another. It is one thing to ask whether it is pos-
sible to capture the distinction between causal
relationships and spurious correlations in purely prob-
abilistic terms; it is quite another to ask whether the
intuitive distinction between causing and preventing
some outcome corresponds to the difference between
raising and lowering the probability of that outcome.
Yet, when authors such as Salmon (1984) and Dowe
(2000) argue that we should abandon probabilistic
theories of causation altogether, based primarily on
counterexamples like Hesslow’s (1976) in which
causes lower the probabilities of their effects, they are
conflating just these kinds of questions.
One of the reasons that philosophers have been so
interested in the topic of causation is that many other
concepts of interest to philosophy appear to have a
causal dimension: explanation, rational deliberation,
moral responsibility, perception, knowledge, refer-
ence, temporal direction, and so on. Indeed, the
importance of causation as an analytical ingredient in
other philosophical concepts is often cited as a moti-
vation for attempts at understanding causation. How
does the dismantling project that I have been recom-
mending affect these further analytical projects? I
would argue that the availability of diverse causal con-
cepts can only help to further our understanding of
other concepts with a causal dimension.
Let us consider two such concepts: prudential
rationality and morality. Prudential rationality would
seem to recommend that we aim to cause those out-
comes that are (prudentially) desirable and to prevent
those outcomes that are (prudentially) undesirable.
Morality would seem to recommend that we aim to
cause those outcomes that are (morally) desirable and
to prevent those outcomes that are (morally) undesir-
able. Are the two, then, at root the same, differing only
in the criteria that we use to evaluate the intrinsic desir-
ability of the outcome? I would argue that they are not,
and the difference lies, at least in part, in the different
notions of cause that are at work in the two cases.
Let us begin with prudential rationality. The
importance of causation to prudential rationality is
brought out by so-called medical Newcomb prob-
lems.7 Marie is an up-and-coming young economist
110
CAUSATION AND INTERVENTION

who is very fond of pickles—as she likes to put it, she
derives a great deal of utility from eating pickles.
Unfortunately, Marie also believes that pregnant
women crave—and hence eat—a lot of pickles.
Because pregnancy would disrupt her career goals,
causing her a great deal of disutility, she decides that
it would be wiser to forego the pleasures of pickle-
eating. It is easy to see where Marie has gone wrong.
Although pickle-eating is correlated with pregnancy
(assuming Marie’s beliefs in this matter are correct),
it is merely a symptom of the underlying condition
rather than a cause of pregnancy. Whether pregnant
or not, Marie has nothing to lose by enjoying her
pickles.
A version of expected utility theory formulated in
terms of simple conditional probabilities (such as
Jeffrey, 1965) will yield the wrong advice in this sort
of case. This has led a number of theorists (such as
Gibbard & Harper, 1978; Skyrms, 1980; Lewis, 1981)
to formulate versions of causal decision theory. Some
of these versions, especially Skyrms (1980), resemble
the R-S theory of causation in that they require us to
condition on certain background conditions to elimi-
nate spurious correlations. There is a further feature
that these theories share with R-S: They do not
require us to condition on any events intermediate
between the contemplated actions and their possible
outcomes. This suggests that decision theory is con-
cerned with the net effects of our actions rather than
with the component effects. When it comes to pru-
dential deliberation, we are concerned with how our
actions affect the overall probability of good and bad
outcomes.
The case of moral deliberation is quite different.
Consider the case of an Outback doctor who receives
two emergency calls. She can fly to one remote town
to save a single life, or she can fly to a remote town in
the opposite direction where two people are dying.
She cannot do both. It is certainly morally permissible,
and perhaps even morally obligatory, for her to save
two lives instead of one. The next day, she faces a dif-
ferent dilemma. Three people are dying. This time,
fortunately, all three are in the same town. The first
patient has a condition that is quite treatable, albeit
lethal if left untreated. Unfortunately, the other two
are dying from organ failure. The only way to save
them would be to find an organ donor, and as it hap-
pens, the only suitable match is the first patient. It is
certainly morally permissible, and perhaps even
morally obligatory, for her to save the first patient, even
if this results in the death of the other two patients:
The moral imperative to save two lives rather than one
is hardly an absolute. What this example suggests is
that moral evaluation is much more sensitive to the
nature of the causal pathways involved. It is permissi-
ble to save two lives, even if this results in the loss of a
third life as an unavoidable side effect, but it is some-
thing quite different to save two lives by allowing a
third person to die. The so-called doctrine of double
effect is an attempt to codify just when it is permissible
to perform actions that have both good and bad out-
comes. The concepts of direct effect and component
effect seem better suited to the articulation of such
principles than is the concept of net effect that seems
to be central to prudential rationality.
Morals for Psychology
I have argued that there is no single relation that
holds between events or event-types that serves as the
referent of the verb cause. Nonetheless, we frequently
do make judgments about what causes what in actual
and hypothetical cases, even when the question is not
more fully specified along the lines of Questions
2a–2e. There is thus an empirical question of just
what people attend to when they make such judg-
ments, and the sort of taxonomic project that I have
been encouraging can supply a ready-made set of
hypotheses.
There seems to be some evidence, for example,
that people tend to focus on direct causes. Consider
the experiment reported in Baker, Mercier, Vallée-
Tourangeau, Frank, and Pan (1993), which is dis-
cussed in detail by Glymour (2001, chapters 4, 5). In
this experiment, subjects observe a video screen on
which a tank attempts to navigate a minefield safely.
The subjects observe a number of trials, on some of
which the tank makes it safely across the minefield
and on some of which it does not. There is also an air-
plane that appears on some occasions and not on oth-
ers. Finally, subjects have the option of moving a
joystick, which has the effect of changing the color
(“camouflaging”) the tank.
Subjects were asked to assign a numerical value
to the efficacy of the camouflage in allowing the tank
to pass through the minefield safely. In one set
of observed trials, camouflaging the tank was posi-
tively correlated with the appearance of the plane,
and the appearance of the plane was perfectly cor-
related with safe navigation through the minefield.
ON THE IMPORTANCE OF CAUSAL TAXONOMY
111

On average, subjects ruled that the camouflage had
negligible efficacy in guiding the tank to safety. Baker
et al. (1993) claimed that this was a mistake because
camouflage is positively correlated with safe passage.
Glymour (2001, pp. 63–66) notes that one plausi-
ble interpretation of the data is that camouflaging the
tank causes the plane to appear, which in turn causes
the tank to pass safely through the minefield, with no
direct effect of the tank’s color on its ability to get
through. On this interpretation, the camouflage has
a net effect and a component effect on safe passage,
but no direct effect.8 This suggests that the subjects,
when asked to assess the efficacy of camouflage,
implicitly judged the direct effect of camouflage on
safe passage.
Is there a tension here? I have argued that there is
no one relationship that holds (or that we take to
hold) whenever we judge that one event or event-type
is a cause of another. Yet, I have also urged psycholo-
gists to look for such a relationship and even sug-
gested that it might be direct causation. What I fully
expect that such research will discover, however, is
that what kind of relationship we attend to when
asked to make causal judgments is a highly context-
sensitive affair. It may depend on the nature of the
example, it may depend on our interests and our rea-
sons for seeking causal information, and it may even
depend on framing effects. For example, in the Baker
et al. (1993) experiment, the subjects were told that
the mines were “visual mines” that could only destroy
a tank “if they could see it” (but the subjects did not
know which color the mines could see). In this con-
text, it would hardly be surprising if the subjects inter-
preted questions about the efficacy of the camouflage
as questions about how well the mines could see the
color in question, a causal mechanism that bypasses
any effect the plane might have on safe passage. In
other words, the wording used to describe the hypo-
thetical scenario naturally suggested to subjects that
the direct effect of the tank’s color on the mines was
of particular interest. It is my hope that careful inves-
tigation into the ways in which our causal judgments
are sensitive to contextual cues such as these will help
us to make sense of the cacophony of intuitions that
currently underlies philosophical investigation into
the nature of causation.
Finally, more careful attention to the dif-
ferent causal concepts evoked by Questions 1 and 2a
through 2e can help us to understand better the
nature of causal learning and causal induction. We
should expect causal learning to be multifaceted:
There is learning whether a relationship is causal or
not (Question 1); learning the strength and direction
of a causal relationship (2a and 2b); learning how a
cause compares with a variety of alternatives (2c);
learning how stable a causal relationship is (2d); and
learning about causal pathways (2e). Causal learning
of any one of these sorts will typically take place while
making use of background assumptions about other
facets of the causal relationships involved.
Many of the successes and failures of psychological
investigation into causal learning can be better under-
stood from within this framework. For example, a cen-
tral shortcoming of the traditional associationist
program of predicting and measuring associative
weights (see, e.g., Rescorla & Wagner, 1972) is its
attempt to answer questions about associative strengths,
questions of type 2b, while ignoring the effect of our
underlying assumptions about whether the relationship
is causal or not (Question 1). (See Waldmann &
Holyoak, 1992, and Waldmann, 2000, for detailed cri-
tiques.) Contrast this with Patricia Cheng’s (1997)
PowerPC model of causal inference. This model is
intended to capture the way in which we estimate the
strength of specific causal relationships—the way in
which we attempt to answer questions of type 2b. The
model is explicit about the types of assumptions that
need to be made about the nature of the underlying
relationships. For example, the formalism that is used
is different depending on whether we are measuring
promoting causes (which Cheng, 1997, calls genera-
tive causes) or inhibiting causes and depending on
whether other inhibiting causes are present. So, the
model requires assumptions about answers to ques-
tions of type 2a. The model, at least in its simplest
form, also requires assumptions about the absence of
interactions between the various causes that are pres-
ent—assumptions about answers to questions of type
2d. It is, of course, an empirical question whether
ordinary reasoners do in fact make just these assump-
tions when estimating causal strengths; my point is
simply that it is highly implausible that we judge
causal strengths in a cognitive vacuum, and that the
sort of taxonomy I have been encouraging would sug-
gest a variety of hypotheses about the sorts of assump-
tions we bring to the table.
As a second illustration, consider the blicket
detector experiments of Gopnik and her collaborators
(see, e.g., Gopnik et al., 2004). In these experiments,
children are shown a variety of objects, which are
112
CAUSATION AND INTERVENTION

placed on a device—a “blicket-detector”—in various
combinations. On some occasions, the machine
lights up and makes noises. The children are then
asked which objects activate the machine (i.e., which
ones are blickets). Gopnik and her collaborators argue
that the children are using screening-off relations to
make inferences about which objects are causally effi-
cacious in setting off the machine, and more gener-
ally, that they represent causal structure using Bayes
nets (directed acyclic graphs with a probability distri-
bution satisfying the Markov condition).
Some commentators (see, e.g., Griffiths and
Tenenbaum, forthcoming) have remarked on the speed
with which the children infer that some objects affect
the machine; sometimes, the children make such infer-
ences on the basis of a single observation. Subjects
make such inferences on the basis of sample sizes
much too small to detect, at a statistically significant
level, the conditional independence relations
entailed by the Markov condition. A plausible expla-
nation is that the subjects are exploiting background
assumptions about the nature of the causal relation-
ships that will be present. First, the number of possi-
ble graphs may be restricted in various ways: It is the
objects being placed on the machine that cause it to
go off, rather than the other way around; the objects
are not causing each other to be blickets, and so on.
Indeed, the set of possible graphs is restricted to the
extent that the only causal relationships under consid-
eration are direct effects of the objects on the
machine. Second, the subjects may have a particular
model in mind: The machine goes off if and only if
one or more blickets are placed on it.
Griffiths and Tenenbaum (forthcoming) present
some experimental results suggesting that subjects do
indeed work with this sort of background assumption.
In this case, the central question is of type 1. Subjects
have observed certain correlations between objects
being placed on the machine and its going off, and
they are trying to determine which objects are causing
it to go off. Their implicit model makes a number of
assumptions about the nature of the causal relation-
ships involved: There are no objects that act to inhibit
the operation of the detector (Question 2a); all of the
causal relations are deterministic or of maximal
strength (2b); the ability of an object to set off the
machine is not affected by the presence or absence of
other objects on the machine (2d); and all effects are
direct (2e). These assumptions are defeasible—they
may be abandoned if they prove to be incompatible
with observation—but while they are in place, causal
inferences may be made efficiently and rapidly.
Causal learning is a bootstrapping affair: Inferences
about one facet of a causal relationship are facilitated
by assumptions about other facets of that relationship.
Understanding causal learning therefore requires that
we understand the multidimensional nature of causal
relationships.
ACKNOWLEDGMENTS
For helpful comments and
suggestions, I would like to thank Clark Glymour,
Alison Gopnik, Steve Sloman, Jim Woodward, and the
participants in the workshop of causal learning and the-
ory formation held at the Center for Advanced Studies
in the Behavioral Sciences at Stanford University.
References
Baker, A. G., Mercier, P., Vallée-Tourangeau, F.,
Frank, R., & Pan, M. (1993). Selective associations
and causality judgments: Presence of a strong
causal factor may reduce judgments of a weaker
one. Journal of Experimental Psychology: Learning,
Memory, and Cognition, 19, 414–432.
Cartwright, N. (1979). Causal laws and effective strate-
gies. Noûs, 13, 419–437. (Reprinted in Cartwright,
How the laws of physics lie, Oxford: Clarendon
Press, 1983, pp. 21–43).
Cheng, P. (1997). From covariation to causation: A
causal power theory. Psychological Review, 104,
367–405.
Collins, J. (2000). Preemptive prevention. Journal of
Philosophy, 98, 223–234.
Dowe, P. (2000). Physical causation. Cambridge,
England: Cambridge University Press.
Dupré, J. (1984). Probabilistic causality emancipated. In
P. A. French, T. E. Uehling, Jr., & H. K. Wettstein
(Eds.), Midwest studies in philosophy IX (pp.
169–75). Minneapolis: University of Minnesota
Press.
Eells, E. (1991). Probabilistic Causality. Cambridge:
Cambridge University Press.
Gibbard, A., & Harper, W. (1978). Counterfactuals and
two kinds of expected utility. In C. A. Hooker, 
J. J. Leach, and E. F. McClennen (Eds.),
Foundations and applications of decision theory
(Vol. 1, pp. 125–162). Dordrecht, The Netherlands:
Reidel.
Glymour, C. (2001). The mind’s arrows: Bayes nets and
graphical causal models in psychology. Cambridge,
MA: MIT Press.
ON THE IMPORTANCE OF CAUSAL TAXONOMY
113

Goodman, N. (1955). Fact, fiction, and forecast.
Cambridge, MA: Harvard University Press.
Gopnik, A., Glymour, C., Sobel, D., Schulz, L.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 3–32.
Griffiths, T., & Tenenbaum, J. (forthcoming). Elemental
causal induction. Cognitive Psychology.
Hesslow, G. (1976). Discussion: Two notes on the proba-
bilistic approach to causality. Philosophy of Science,
43, 290–292.
Hitchcock, C. (1993). A generalized probabilistic theory
of causal relevance. Synthese, 97, 335–364.
Hitchcock, C. (1995a). Discussion: Salmon on explana-
tory relevance. Philosophy of Science, 62, 304–320.
Hitchcock, C. (1995b). The mishap at Reichenbach Fall:
Singular versus general causation. Philosophical
Studies, 78, 257–291.
Hitchcock, C. (1996a). Farewell to binary causation.
Canadian Journal of Philosophy, 26, 267–282.
Hitchcock, C. (1996b). The role of contrast in causal and
explanatory claims. Synthese, 107, 395–419.
Hitchcock, C. (2001a) The intransitivity of causation
revealed in equations and graphs. Journal of
Philosophy, 98, 273–299.
Hitchcock, C. (2001b). A tale of two effects. The
Philosophical Review, 110, 361–396.
Hitchcock, C. (2003). Of Humean bondage. British
Journal for the Philosophy of Science, 54, 1–25.
Hitchcock, C. (2004a). Causal processes: What are they
and what are they good for? Philosophy of Science,
S71 (Proceedings).
Hitchcock, C. (2004b). Routes, processes, and chance low-
ering causes. In P. Dowe & P. Noordhof (Eds.), Cause
and chance (pp. 138–151). London: Routledge.
Humphreys, P. (1989). The chances of explanation.
Princeton, NJ: Princeton University Press.
Jeffrey, R. (1965). The logic of decision. New York:
McGraw-Hill.
Lewis, D. (1981). Causal decision theory. Australasian
Journal of Philosophy, 59, 5–30.
Lewis, D. (2000). Causation as influence. Journal of
Philosophy, 97, 182–197.
Mellor, D. H. (1995). The facts of causation. London:
Routledge.
Mill, J. S. (1843). A system of logic. London: Parker & Son.
Nozick, R. (1969). Newcomb’s problem and two princi-
ples of choice. In N. Rescher (Ed.), Essays in honor
of Carl G. Hempel (pp. 114–146). Dordrecht, The
Netherlands: Reidel.
Pearl, J. (2001). Direct and indirect effects. In Proceed-
ings of UAI-2001 (pp. 411–420). San Mateo, CA:
Morgan Kauffman.
Popper, K. (1962). Conjectures and refutations. London:
Routledge & Kegan Paul.
Reichenbach, H. (1956). The direction of time. Berkeley,
CA: University of California Press.
Rescorla, R. A., & A. R. Wagner (1972). A theory of
Pavlovian conditioning: Variations in the effective-
ness of reinforcement and nonreinforcement. In 
A. H. Black and W. F. Prokasy (Eds.), Classical con-
ditioning II: Current theory and research 
(pp. 64–99). New York: Appleton Century Crofts.
Salmon, W. (1984). Scientific explanation and the causal
structure of the world. Princeton, NJ: Princeton
University Press.
Schaffer, J. (2000). Causation by disconnection.
Philosophy of Science, 67, 285–300.
Skyrms, B. (1980). Causal necessity. New Haven, CT:
Yale University Press.
Spirtes, P., Glymour, C., & Scheines, R. (2000).
Causation, prediction, and search (2nd ed.).
Cambridge, MA: MIT Press.
Suppes, P. (1970). A probabilistic theory of causality.
Amsterdam: North-Holland.
Waldmann, M. R. (2000). Competition among causes
but not effects in predictive and diagnostic learning.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 26, 53–76.
Waldmann, M. R., & Holyoak, K. J. (1992). Predictive and
diagnostic learning within causal models: Asymmetries
in cue competition. Journal of Experimental
Psychology: General, 121, 222–236.
Woodward, J. (2003). Making things happen. Oxford,
England: Oxford University Press.
114
CAUSATION AND INTERVENTION

Part II
CAUSATION AND PROBABILITY

This page intentionally left blank 

From: brook_russell@turing.carnegietech.edu
To: mherskovits@psych.ucarcadia.arcadia.edu
Dear Morgan,
What an amazing weekend! It has completely
changed my mind about the value of your kind
of psychology, and I simply cannot wait for the
next workshop. My head is buzzing with
questions and thoughts about human causal
learning, and I’m so longing for the answers the
next workshop seems all set to provide.
There is the whole question of probability,
for example. I am struck that so many of those
brilliant examples of causal learning you and
the rest of them described, especially with the
sprogs, seem restricted to deterministic 
contexts, in which causes always follow effects.
Of course, causal Bayes nets can be applied to
such contexts. But, the canonical application of
the systems is to cases involving what may be
quite complicated systems of conditional
probabilities. And, while Bayes nets can be
applied to deterministic systems, such systems
raise special problems for learning. Often, they
result in violations of faithfulness. I’m sure that
Thomas Richardson and Clark Glymour can
take care of that if anyone can.
Along the same lines, I do still have that
query about whether human beings of any age
are really capable of calculating probabilities.
Didn’t one of your psychologists recently get a
Nobel Prize for showing how bad even sophisti-
cated adults were at probabilistic reasoning?
And, I am curious also about the question of
classification and categorization. The Bayes net
formalism depends on the idea that variables
are specified beforehand. That is, we already
have some sense of how a particular event fits
into a category—how a particular token is a
member of a type as we say in philosophy—
before we do any causal inference at all. But, it
appears that people often categorize objects pre-
cisely according to their causal powers. Could
the formalism be applied to answer these
questions, too?
I’m sure this next workshop will answer all
this and more.
All the best,
Brook
From: mherskovits@psych.ucarcadia.arcadia.edu
To: brook_russell@turing.carnegietech.edu
Hi Brook,
Well, I have to say I feel the same way.
Remember that quote from Gopnik I sent in
that first letter? Well, I guess it’s too soon to say
Introduction to Part II: 
Causal Learning and Probability

for sure, of course, but this does seem awfully
like the real thing. For once, a computational
set of ideas really does seem to make contact
with the things we care about in psychology.
And, even better, it gives us psychologists all
sorts of new work to do. I already can think of a
zillion experiments to do to test the ideas.
And, you know, I think a lot of your
questions are going to be answered at the next
workshop. It’s true that people are just awful at
explicitly representing probabilities, but David
Sobel and Natasha Kirkham will show you that
even tiny babies, as young as 8 months old,
already can do some kinds of statistical
reasoning; in fact, they already seem to use a
kind of “screening off.” Dave Lagnado and his
colleagues will show you that adults can use
probabilities to infer causation when they’re
combined with the right kinds of other cues;
Richard Scheines will show that even those
undergraduate statistics students are, well, a lot
smarter than they look.
As for categorization, that’s an interesting
one because for a long time in psychology
people noticed that “causal powers” seemed to
play an important role in the way people
categorized objects. In fact, one of the first
areas of psychology in which people talked
about the theory theory was precisely in the
domain of categorization. Adults’ categories
seemed to be based more on their theories of
the deeper causal powers of objects—their
“essences”—than on more superficial percep-
tual features. David Danks and Bob Rheder
will show how we can use the Bayes net
formalism to make quite precise predictions
about how ordinary folk will categorize objects.
Best,
Morgan
118
CAUSATION AND PROBABILITY

Introduction
By the early to mid-1990s, a normative theory of
causation with qualitative as well as quantitative sub-
stance, called causal Bayes nets (CBNs),1 achieved
fairly widespread acceptance among key proponents
in computer science (artificial intelligence), philoso-
phy, epidemiology, and statistics. Although the repre-
sentational component of the normative theory is at
some level fairly stable and commonly accepted, how
an ideal computational agent should learn about
causal structure from data is much less settled and, in
2005, was still a hot area of research.2 To be clear, the
CBN framework arose in a community that had no
interest in modeling human learning or representa-
tion. They were interested in how a robot, or an ideal
computational agent, with obviously far different pro-
cessing and memory capacities than a human, could
best store and reason about the causal structure of the
world. Much of the early research in this community
focused on efficient algorithms for updating beliefs
about a CBN from evidence (Pearl, 1988;
Spiegelhalter & Lauritzen, 1990) or on efficiently
learning the qualitative structure of a CBN from data
(Pearl, 1988; Spirtes, Glymour, & Scheines, 2000).
In contrast, the psychological community, interested
in how humans learn not in how they should learn if
they had practically unbounded computational
resources, studied associative and causal learning for
decades. The Rescorla-Wagner theory (1972) was
offered, for example, as models of how humans (and
animals, in some cases) learned associations and causal
hypotheses from data. Only later, in the early 1990s, did
CBNs make their way into the psychological commu-
nity and only then as a model that might describe every-
day human reasoning. At the least, a broad range of
psychological theories of human causal learning can be
substantially unified when cast as different versions of
parameter learning within the CBN framework (Danks,
2005), but it is still a matter of vibrant debate whether
and to what degree humans represent and learn about
causal claims as per the normative theory of CBNs (e.g.,
Danks, Griffiths, & Tenenbaum, 2003; Glymour, 1998,
2000; Gopnik et al., 2004; Gopnik, Sobel, Schulz, &
119
8
Teaching the Normative Theory of Causal Reasoning
Richard Scheines, Matt Easterday, & David Danks

Glymour, 2001; Griffiths, Baraff, & Tenenbaum, 2004;
Lagnado & Sloman, 2002, 2004; Sloman & Lagnado,
2002; Steyvers, Tenenbaum, Wagenmakers, & Blum,
2003; Tenenbaum & Griffiths, 2001, 2003; Tenenbaum
& Niyogi, 2003; Waldmann & Hagmayer, in press;
Waldmann & Martignon, 1998).
Nearly all of the psychological research on human
causal learning involves naïve participants, that is,
individuals who have not been taught the normative
theory in any way, shape, or form. Almost all of this
research involves single-trial learning: observing how
subjects form and update their causal beliefs from the
outcome of a series of trials, each either an experi-
ment on a single individual or a single episode of a
system’s behavior. No work, as far as we are aware,
attempts to train people normatively on this and
related tasks, and no work we know of compares the
performance of naïve participants and those taught
the normative theory. The work we describe in this
chapter begins just such a project. We are specifically
interested in seeing if formal education about norma-
tive causal reasoning helps students draw accurate
causal inferences.
Although there has been, to our knowledge, no
previous research on subjects trained in the normative
theory, there has been research on whether naïve sub-
jects approximate normative learning agents. Single-
trial learning, for example, can easily be described by
the normative theory as a sequential Bayesian up-
dating problem. Some psychologists have considered
whether and how people update their beliefs in accord
with the Bayesian norm (e.g., Danks et al., 2003;
Griffiths et al., 2004; Steyvers et al., 2003; Tenenbaum
& Griffiths, 2001, 2003; Tenenbaum & Niyogi, 2003)
and have suggested that some people at least approxi-
mate a normative Bayesian learner on simple cases.
This research does not extend to subjects who have
already been taught the appropriate rules of Bayesian
updating, either abstractly or concretely.
In the late 1990s, curricular material became
available that taught the normative theory of CBNs.3
Standard introductions to the normative theory in
computer science, philosophy, and statistics do not
directly address the sorts of tasks that psychologists
have investigated, however. First, as opposed to single-
trial learning, the focus is on learning from samples
drawn from some population. Second, little or no
attention is paid to the severe computational (process-
ing time) and representational (storage space) limita-
tions of humans. Instead, abstractions and algorithms
are taught that could not possibly be used by humans
on any but the simplest of problems.
In the normative theory, learning about which
among many possible causal structures might obtain
is typically cast as iterative:
1. Enumerate a space of plausible hypotheses.
2. Design an experiment that will help distinguish
among these hypotheses.
3. Collect a sample of data from such an 
experiment.
4. Analyze these data with the help of sophisti-
cated computing tools like R4 or TETRAD5 to
update the space of hypotheses to those sup-
ported or consistent with these data.
5. Go back to Step 2.
Designing an experiment, insofar as it involves
choosing which variable or variables to manipulate, is a
natural part of the normative theory and has just
recently become a subject of study.6 The same activity,
that is, picking the best among many possible experi-
ments to run, has been studied by Lagnado and Sloman
(2004), Sobel and Kushnir (2004), Steyvers et al.
(2003), and Waldmann and Hagmayer (in press).
Another point of contact is what a student thinks the
data collected in an experiment tell them about the
model that might be generating the data. Starting with
a set of plausible models, some will be consistent with
the data collected, or favored by it, and some will not.
We would like to know whether students trained in the
normative theory are better, and if so, in what way, at
determining what models are consistent with the data.
In a series of four pilot experiments, we examined
the performance of subjects partially trained in the
normative theory on causal learning tasks that
involved choosing experiments and deciding which
models were consistent with the data. Although we
did not use single-trial learning, we did use tasks sim-
ilar to those studied recently by psychologists, espe-
cially Steyvers et al. (2003). Our students were trained
for about a month in a college course on causation
and social policy. The students were not trained in the
precise skills tested by our experiments. Although our
results are not directly comparable to those discussed
in the psychological literature, they certainly suggest
that students trained on the normative theory act
quite differently from naïve participants.
Our chapter is organized as follows: We first briefly
describe what we take to be the normative theory of
120
CAUSATION AND PROBABILITY

causal reasoning. We then describe the online corpus
we have developed for teaching it. Finally, we
describe four pilot studies we performed in the fall of
2004 with the Causality Lab, a major part of the
online corpus.
The Normative Theory of Causal
Reasoning
Although Galileo pioneered the use of fully con-
trolled experiments almost 400 years ago, it was not
until Sir Ronald Fisher’s (1935) famous work on
experimental design that real headway was made on
the statistical problem of causal discovery. Fisher’s
work, like Galileo’s, was confined to experimental set-
tings in which treatment could be assigned. In
Galileo’s case, however, all the variables in a system
could be perfectly controlled, and the treatment
could thus be isolated and made to be the only quan-
tity varying in a given experiment. In agricultural or
biological experiments, however, it is not possible to
control all the quantities (e.g., the genetic and envi-
ronmental history of each person). Fisher’s technique
of randomization not only solved this problem, but
also produced a reference distribution against which
experimental results could be compared statistically.
His work is still the statistical foundation of most
modern medical research.
Representing Causal Systems: Causal
Bayes Nets
Sewall Wright pioneered representing causal systems
as “path diagrams” in the 1920s and 1930s (Wright,
1934), but until about the middle of the 20th century
the entire topic of how causal claims can or cannot be
discovered from data collected in nonexperimental
studies was largely written off as hopeless. Herbert
Simon (1953) and Hubert Blalock (1961) made
major inroads but gave no general theory. In the mid-
1980s, however, artificial intelligence researchers,
philosophers, statisticians, and epidemiologists began
to make real headway on a rigorous theory of causal
discovery from nonexperimental as well as experi-
mental data.7
Like Fisher’s statistical work on experiments,
CBNs seek to model the relations among a set of
random variables, such as an individual’s level of edu-
cation or annual income. Alternative approaches aim
to model the causes of individual events, for example,
the causes of the space shuttle Challenger disaster.
We confine our attention to relations among vari-
ables. If we are instead concerned with a system in
which certain types of events cause other types of
events, then we represent the occurrence or nonoc-
currence of the events by binary variables. For exam-
ple, if a blue lightbulb going on is followed by a red
lightbulb going on, we use the variables red lightbulb
[lit, not lit] and blue lightbulb [lit, not lit].
Any approach that models the statistical relations
among a set of variables must first confront what we
call the ontological problem: How do we get from a
messy and complicated world to a coherent and
meaningful set of variables that might plausibly be
related either statistically or causally. For example, it
is reasonable to examine the association between the
number of years of education and the number of dol-
lars in yearly income for a sample of middle-aged men
in western Pennsylvania, but it makes no sense to
examine the average level of education for the aggre-
gate of people in a state like Pennsylvania and com-
pare it to the level of income for individual residents
of New York. It also does not make sense to posit a
“variable” with a range of values that is not exclusive
because it includes has blond hair, has curly hair, and
so on. After teaching causal reasoning to hundreds of
students over almost a decade, the ontological prob-
lem seems the most difficult to teach and the most dif-
ficult for students to learn. We need to study it much
more thoroughly, but for the present investigation, we
simply assume it has been solved for a particular
learning problem.
Assuming that we are given a set of coherent and
meaningful variables, the normative theory involves
representing the qualitative causal relations among a
set of variables with a directed graph in which there is
an edge from X to Y just in case X is a direct cause of
Y relative to the system of variables under study. X is a
direct cause of Y in such a system if and only if there
is a pair of ideal interventions that hold the other vari-
ables in the system Z fixed and change only X, such
that the probability distribution for Y also changes. We
model the quantitative relations among the variables
with a set of conditional probability distributions: one
for each variable given each possible configuration of
values of its direct causes (see Figure 8-1).
The asymmetry of causation is modeled by how
the system responds to ideal intervention, both quali-
tatively and quantitatively. Consider, for example, a
two-variable system: room temperature (of a room an
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
121

FIGURE 8-1 Causal Bayes net.
FIGURE 8-2 Manipulated graph.
FIGURE 8-3 Original and manipulated systems.
122

individual is in) 55º, 55º85º, 	85º], and wearing
a sweater [yes, no], in which the graph and set of
conditional probability tables in Figure 8-1 describe
the system.
Ideal interventions are represented by adding an
intervention variable that is a direct cause of only the
variables it targets. Ideal interventions are assumed to
have a simple property: If I is an intervention on variable
X, then when I is active, it removes all the other edges
into X. That is, the “other” causes of X no longer influ-
ence X in the postintervention, or manipulated, system.
Figure 8-2 captures the change and nonchange in the
Figure 8-1 graph in response to interventions on room
temperature (A) and on wearing a sweater (B).
Modeling the system’s quantitative response to inter-
ventions is almost as simple. Generally, we conceive 
of an ideal intervention as imposing not a value but
rather a probability distribution on its target. We thus
model the move from the original system to the
manipulated system as leaving all conditional distri-
butions intact save those over the manipulated vari-
ables, in which case we impose our own distribution.
For example, if we assume that the interventions
depicted in Figure 8-2 impose a uniform distribution
on their targets when active, then Figure 8-3 shows
the two manipulated systems that would result from
the original system shown in Figure 8-1.8
To simplify later discussions, we include the “null”
manipulation (i.e., we intervene on no variables) as
one possible manipulation. A CBN and a manipula-
tion define a joint probability distribution over the set
of variables in the system. If we use experimental setup
to refer to an exact quantitative specification of the
manipulation, then when we collect data we are draw-
ing a sample from the probability distribution defined
by the original CBN and the experimental setup.
Learning Causal Bayes Nets
There are two distinct types of CBN learning given data:
parameter estimation and structure learning. In param-
eter estimation, one fixes the qualitative (graphical)
structure of the model and estimates the conditional
probability tables by minimizing some loss function or
maximizing the likelihood of the sample data given the
model and its parameterization. In contrast, structure
learning aims to recover the qualitative structure of
graphical edges. The distinction between parameter
estimation and structure learning is not perfectly clean
because “close-to-zero parameter” and “absence of the
edge” are roughly equivalent. Danks (2005) shows
how to understand most non-Bayes net psychological
theories of causal learning (e.g., Cheng, 1997; Cheng
& Novick, 1992; Perales & Shanks, 2003; Rescorla &
Wagner, 1972) as parameter estimation theories for
particular graphical structures.
A fundamental challenge for CBN structure learn-
ing algorithms is the existence of Markov equivalence
classes (MECs): sets of CBNs that make identical pre-
dictions about the way the world looks in the absence
of experiments. For example, A →B and A ←B both
predict that variables A and B will be associated. Any
data set that can be modeled by A →B can be equally
well modeled by A ←B, so there is no reason—given
only observed data—to prefer one structure over the
other. This observation leads to the standard warning
in science that “correlation does not equal causation.”
However, patterns of correlation can enable us to
infer something about causal relationships (or, more
generally, graphical structure), although perhaps not
a unique graph. Thus, structure learning algorithms
will frequently not be able to learn the “true” graph
from data, but will be able to learn a small set of
graphs that are indistinguishable from the “truth.”
For learning the structure of the causal graph, the
normative theory splits into two approaches: constraint
based and scoring. The constraint-based approach
(Spirtes et al., 2000) aims to determine the class of
CBNs consistent with an inferred (statistical) pattern
of independencies and associations, as well as back-
ground knowledge. Any particular CBN entails a set of
statistical constraints in the population, such as inde-
pendence and tetrad constraints. Constraint-based
algorithms take as input the constraints inferred from
a given sample, as well as background assumptions
about the class of models to be considered, and output
the set of indistinguishable causal structures. That is,
the algorithms output the models that (a) entail all and
only the inferred constraints and (b) are consistent
with background knowledge. The inference task is
thus split into two parts: statistical, inference from the
sample to the constraints that hold in the population,
and causal, inference from the constraints to the CBN
or nets that entail such constraints.
Suppose, for example, that we observe a sample of
100 individuals on variables X1, X2, and X3 and after
statistical inference conclude that X1 and X2 are statis-
tically independent, conditional on X3 (i.e., X1 ⊥X2 |
X3). If we also assume there are no unobserved com-
mon causes for any pair of X1, X2, and X3, then the PC
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
123

algorithm (Spirtes et al., 2000) would output the pat-
tern shown on the left side of Figure 8-4. That pattern
is a graphical object that represents the MEC shown
on the right side of Figure 8-4; all three graphs predict
exactly the same set of unconditional and conditional
independencies. In general, two causal graphs entail
the same set of independencies if and only if they
have the same adjacencies and unshielded colliders,
where X and Y are adjacent just in case X →Y or X ←
Y, and Z is an unshielded collider between X and Y
just in case X →Z ←Y and X and Y are not adjacent.
Thus, in a pattern, we need only represent the adja-
cencies and unshielded colliders. Constraint-based
searches first compute the set of adjacencies for a set
of variables and then try to “orient” these adjacencies,
that is, test for colliders among triples in which X and
Y are adjacent, Y and Z are adjacent, but X and Z are
not: X-Y-Z.
Testing high-order conditional independence
relations—relations that involve a large number of
variables in the conditioning set—is computationally
expensive and statistically unreliable, so the con-
straint-based approach sequences the tests to mini-
mize the number of higher-order conditional
independence facts actually tested. Compared to
other methods, constraint-based algorithms are
extremely fast and under multivariate normal distri-
butions (linear systems) can handle hundreds of vari-
ables. Constraint-based algorithms can also handle
models with unobserved common causes. Their
drawback is that they are subject to errors if statistical
decisions made early in the algorithm are incorrect.
If handed the independence relations true of a
population, then people could easily perform by hand
the computations required by a constraint-based
search, even for many causal structures with dozens of
variables. Of course, people could not possibly compute
all of the precise statistical tests of independence rela-
tions required, but they could potentially approximate
a subset of such (unconditional and conditional)
independence tests (see Danks, 2004, for one tenta-
tive proposal).
In the score-based approach (Heckerman et al.,
1999), we assign a “score” to a CBN that reflects both
(a) the closeness of the CBN’s “fit” of the data and (b)
the plausibility of the CBN prior to seeing any data.
We then search (in a variety of ways) among all the
models consistent with background knowledge for the
set that has the highest score. The most common scor-
ing-based approach is based on Bayesian principles:
Calculate a score based on the CBN’s prior—the
probability we assign to the model being true before
seeing any data, and the model’s likelihood—the
probability of the observed data given this particular
CBN.9 Scoring-based searches are accurate but are
slow, as calculating each model’s score is expensive.
Given a flat prior over the models (i.e., equal proba-
bilities on all models), the set of models that have the
highest Bayesian score is identical to the MEC of
models output by a constraint-based algorithm.
Bayesian approaches are straightforwardly
applied to standard psychological tasks. By comput-
ing the posterior over the models after each new
sample point, we get a learning dynamics for that
problem (as in, e.g., Danks et al., 2003; Griffiths 
et al., 2004; Steyvers et al., 2003; Tenenbaum &
Griffiths, 2003). However, even if naïve subjects act
like approximately rational Bayesian structure learn-
ers in cases involving 2 or 3 variables, they cannot
possibly implement the approach precisely or possi-
bly implement the approach for larger numbers of
variables, e.g., 5–10. Hence, the Bayesian approach
is not necessarily appropriate for teaching the nor-
mative theory.
124
CAUSATION AND PROBABILITY
FIGURE 8-4 Equivalence class for X1 ⊥X2 | X3.

The Causality Lab
Convinced that the qualitative story behind causal
discovery should be taught to introductory-level
students either prior to or simultaneously with a basic
course on statistical methods, a team10 from Carnegie
Mellon and the University of California, San Diego,
created enough online material for an entire semes-
ter’s course in the basics of CBNs. By the spring of
2004, over 2,600 students in more than 70 courses at
almost 30 different colleges or universities had taken
all or part of our online course, which is available
through Carnegie Mellon’s Open Learning Initiative
at www.cmu.edu/oli/.
Causal and statistical reasoning involves three
components: 16 lessons, or concept modules; a virtual
laboratory for simulating social science experiments,
the Causality Lab;11 and a bank of over 120 case stud-
ies, which are reports of “studies” by social, behav-
ioral, or medical researchers. Each of the concept
modules contains approximately the same amount of
material as a textbook chapter. The Causality Lab
embodies the normative theory by making explicit all
the ideas we discussed.
Figure 8-5 shows the navigation panel for the lab.
Each of the icons may be clicked to reveal, and in
some cases manipulate, the contents of an object for a
given exercise. The instructor creates the “true” CBN
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
125
FIGURE 8-5 The Causality Lab navigation panel. hyp., 
hypothetical.

with an exercise building tool, and this constitutes the
“true graph” to be discovered by the student. Of
course, just as real scientists are confined to one side
of the Humean curtain, so are students of the
Causality Lab. In most exercises, they cannot access
any of the icons in the left column, all of which
represent one aspect of the truth to be discovered.
Students cannot simply click and see the truth.
Using the example of room temperature and
sweaters, suppose the true graph and conditional prob-
ability distributions are as given in Figure 8-1. To fully
determine the population from which the student may
draw a sample, however, the student must also provide
the (possibly null) experimental setup. Once the
student specifies one or more experimental setups,
that student can “collect data” from any of them.
For example, suppose we clicked on the
Experimental Setup icon and then created three
distinct experimental setups (Figure 8-6). On the left,
both room temperature and sweater will be passively
observed. In the middle, the value of room tempera-
ture will be randomly assigned (indicated by the icon
of a die attached to Room_Temp), and the value of
sweater will be passively observed. On the right, the
value of sweater will be randomly assigned, and the
value of room temperature will be passively observed.
As the navigation panel in Figure 8-5 shows, it is
the combination of the experimental setup and the
true CBN that defines the manipulated system, which
determines the population probability distribution.
So, if we click on Collect Data from Exp-Setup 1 (far
left side of Figure 8-6), then we will be drawing a
sample from the distribution shown at the top of
Figure 8-3. If we collect data from Exp-Setup 2, then
our sample will be drawn from the distribution shown
in the middle of Figure 8-3, and so on. The fact that
the sample population depends on both the experi-
mental setup and the true CBN is a pillar of the nor-
mative theory, but this fact is rarely, if ever, taught.
Once a sample is pseudorandomly drawn from the
appropriate distribution, we may inspect it in any way
we wish. To keep matters as qualitative as possible,
however, the focus of the Causality Lab is on
independence constraints—the normative theory’s pri-
mary connection between probability distributions
and causal structure. In particular, the Predictions and
Results window allows the student to inspect the fol-
lowing for each experimental setup: the independence
relations that hold in the population12 and the
independence relations that cannot be rejected at 
.05 by a statistical test applied to any sample drawn
from that population
For example, Figure 8-7 shows the results of an
experiment in which wearing a sweater is randomly
assigned and a sample of 40 individuals was drawn
from the resulting population. The Predictions and
Results window indicates that, in the population,
room temperature and sweater wearing are independ-
ent (notated as 
). The lab also allows students to
inspect histograms or scatterplots of their samples and
then enter their own guesses regarding which inde-
pendence relations hold in a given sample. In this
example, a student used the histograms to guess that
room temperature and sweater wearing were associ-
ated (not independent), although the statistical test
applied to the sample of 40 could not reject the
hypothesis of independence. Thus, one easy lesson
for students is that statistical tests are sometimes bet-
ter at determining independence relations than stu-
dents who eyeball sample summaries.
⊨
126
CAUSATION AND PROBABILITY
FIGURE 8-6 Three experimental setups.

Students can also create hypotheses and then
compare the predictions of their hypotheses to the
results of their experiments. For example, we may
rebel against common sense and hypothesize that
wearing a sweater causes the room temperature. The
Causality Lab helps the students learn that their hypo-
thetical graph only makes testable predictions about
independence in combination with an experimental
setup, which leads to a manipulated hypothetical
graph (see Figure 8-5).
Causal Discovery in the Lab
Equipped with the tools of the Causality Lab, we can
decompose the causal discovery task into the follow-
ing steps:
1. Enumerate all the hypotheses that are consistent
with background knowledge.
2. Create an experimental setup and collect a
sample of data.
3. Make statistical inferences about the indepen-
dences that hold in the population from the
sample.
4. Eliminate or reallocate confidence in hypo-
theses on the basis of the results from Step 3.
5. If no unique model emerges, then go back to
Step 2.
Steps 1 (enumeration) and 3 (statistics) are interest-
ing, although only necessary if one is following a
constraint-based approach. The interesting action is in
Steps 2 and 4. As operationalized in the Causality Lab
and defined in the normative theory, the first part of Step
2 (experimental design) amounts to determining, for
each variable under study, whether that variable will be
observed passively or have its values assigned randomly.
Depending on the hypotheses still under considera-
tion, experimental setups differ in the informativeness
of the experiment’s results. For example, suppose the
currently active hypotheses include X →Y →Z and
X ←Y →Z. An experimental setup (call it ES1) in
which X is randomized and Y and Z are passively
observed will uniquely determine the correct graph no
matter the outcome.13 A different experiment (call it
ES2) in which Z is randomized and X and Y passively
observed will tell us nothing, again regardless of the
outcome of the experiment. The difference in the
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
127
FIGURE 8-7 Independence results.

experiments’ informativeness arises because the manip-
ulated graphs are distinguishable in ES1 but not in
ES2 (Figure 8-8). In ES1, the two possibilities have dif-
ferent adjacencies (X →Y in one, and no edges in the
other) and thus entail different sets of independencies.
In ES2, however, the two manipulated graphs are indis-
tinguishable; they have the same adjacencies.
From this perspective, the causal discovery task
involves determining, for each possible experimental
setup one might use, the set of manipulated hypotheti-
cal graphs and whether they are (partially) distinguish-
able. This is a challenging task. What are the general
principles for experimental design, if any? When the
goal is to parameterize the dependence of one effect on
several causes, then there is a rich and powerful theory
of experimental design from the statistical literature
(Berger, 2005; Cochran & Cox, 1957). When the goal
is to discover which among many possible causal struc-
tures are true, however, the theory of optimal experi-
mental design is much less developed. From a
Bayesian perspective, we must first specify a prior distri-
bution over the hypothetical graphs. Given such a
distribution, each experimental setup has an expected
gain in information (reduction in uncertainty), and
one should thus pick the experiment that would most
reduce uncertainty (Murphy, 2001; Tong & Koller,
2001). Computing this gain is intractable for all but the
simplest of cases, although Steyvers et al. (2003) argue
that naïve subjects approximate just this sort of behav-
ior. Regardless of the descriptive question, a theory of
so-called active learning provides normative guidance
regarding the optimal sequencing of experiments.
Taking a constraint-based approach, Eberhardt,
Glymour, and Scheines (2005) show that for N vari-
ables, N  1 experiments that randomize at most a 
single variable are always sufficient to identify the cor-
rect graph and in the worst case that many are necessary.
Although there is not yet a graphical characteriza-
tion of the best experiment given a set of active hypothe-
ses, we do have a few powerful heuristics. For example,
passive observation is sufficient, under a constraint-
based approach, to identify all the adjacencies among 
a set of variables. Given the adjacencies, an intervention
on X will orient all the edges adjacent to X. Suppose X
and Z are adjacent. If X and Z are independent after an
intervention on X, then the edge is X ←Z; if X and Z
are associated, then the edge must be X →Z.
Pilot Studies
An obvious question about teaching the normative
theory is as follows: Does learning it improve a student’s
performance on causal learning tasks? In the fall of
128
CAUSATION AND PROBABILITY
FIGURE 8-8 Informative and uninformative experimental setups.

2004, one of us (R. S.) taught an upper-level seminar at
Carnegie Mellon on causation and social policy. For
about a month in the middle of the seminar, the stu-
dents went through the causal and statistical reasoning
material and learned the rudiments of the representa-
tional theory of CBNs. The class covered the idea of
causation, causal graphs, manipulations, manipulated
models, independence, conditional independence,
and d-separation,14 but included no instruction on
model equivalence and no instruction on a procedure
for causal discovery. All 15 of the students in the class
agreed to participate in a pilot study in which they 
were given four discovery tasks. The students all
worked for a little over an hour in a computer cluster.
We were unable to enforce strict silence between stu-
dents, and thus the results of our pilot study cannot be
considered rigorous. They are nevertheless interesting
and suggestive.
In all of our experiments, participants were allowed
to see the full independence relations that hold in the
population defined by an experimental setup of their
choice, and so no statistical judgments were required.
We recognize that this is different from the standard
presentation in psychological experiments, but our
intent was to focus on the skills involved in causal
discovery from known facts about the population, as
opposed to making statistical inferences from samples.
To provide familiarity with the Causality Lab inter-
face, all participants were provided a simple training
problem. In the training task, the students were
instructed to (a) do a passive observation, then (b)
eliminate all the models they could, and finally (c)
determine the true graph using the fewest number of
experiments.
Experiment 1
In Experiment 1, we asked students to determine
which model in Figure 8-9 was the true graph in the
minimum number of experiments. Students were
randomly assigned to a model, and there was no effect
of condition. The experiment explored whether stu-
dents understood the difference between direct and
indirect causation. All 15 students learned the correct
model in a single experiment.
We were also interested in the students’ choices of
experimental targets. Table 8-1 shows the independ-
ence relations entailed by both models in every possi-
ble experimental setup, as well as whether M1 and
M2 can be distinguished in that experiment. From a
normative point of view, no one should choose to ran-
domize Z because that experiment will not distin-
guish between these two models. Randomizing Y is
optimal because under that intervention the two mod-
els make different predictions about both X Z and
X Z | Y.
Steyvers et al. (2003) report a source bias in choos-
ing interventions: People prefer to intervene on vari-
ables believed to have no edges into them (i.e., no
causes in the system). If this bias holds, then people
should prefer to randomize on X when they random-
ize on any variable at all. Note that the source bias
refers only to choices among experiments; no predic-
tion was made about whether people will prefer to
experiment or passively observe.
Figure 8-10 shows the frequency with which each
experiment was chosen first. All students were norma-
tively correct; no one chose to randomize on Z. Our
students preferred the passive observation, which can
be explained by its use in the training experiment. 
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
129
FIGURE 8-9 Choices in Experiment 1.
TABLE 8-1 Independencies Implied by M1 and M2
Experimental Setup
X  Y
X Z
X  Z | Y
M1 and M2 
Distinguishable?
Passive observation
Neither
Neither
M1, not M2
Yes
Randomize X
Neither
Neither
M1, not M2
Yes
Randomize Y
Both
M1, not M2
M1, not M2
Yes
Randomize Z
Neither
Both
No

130
CAUSATION AND PROBABILITY
FIGURE 8-10 Choice of experiments in Experiment 1. Obs., observation.
PassiveObs.
Experiment 1: First Choice
Count
0
1
2
3
4
5
6
7
8
9
Intervene-X
Intervene-Y
FIGURE 8-11 Possibilities in Experiment 2.
And, in contrast to the results reported in Steyvers et al.
(2003), students exhibited no source bias whatsoever:
Six of the seven who chose to intervene did so on the
mediating variable Y.
Experiment 2
In the second experiment, the students had to choose
among four possibilities (Figure 8-11). They were
again told to find the true graph in the minimum
number of experiments, although they understood
that they were not required to do the passive observa-
tion experiment first. Since M3 and M4 are essen-
tially the same a priori, we randomized the students to
a true graph of M1, M2, or M3.
This experiment aimed to determine whether stu-
dents could choose an informative intervention; in
this problem, the choice of experimental setup
matters greatly, as shown in Table 8-2. For example, if
we passively observe all variables, then we can tell
only whether M1 is the true model or not the true
model (i.e., that the true model is one of {M2, M3,
M4}). The normatively optimal experiment to per-
form is the one in which the middle variable C is ran-
domized. That experiment is guaranteed to uniquely
identify the correct model regardless of outcome.
Again, students were quite successful in the over-
all task: 14 of 15 correctly identified the model. The
number of experiments it took to arrive at an answer
varied considerably: Two experiments were the mode,

TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
131
FIGURE 8-12 Results of Experiment 2.
Int.-on-End
Experiment 2: First Choice
Count
0
1
2
3
4
5
6
7
8
9
Int.-on-Middle
PassiveObs.
Experiment 2: First Intervention Choice
Count
0
1
2
3
4
5
6
7
8
9
End
Middle
TABLE 8-2 Distinguishable Models by Intervention
Choice
Experimental Setup
Distinguishable?
Passive observation
M1 from {M2, M3, M4}
Randomize A
M1 from {M2, M3} from M4
Randomize B
M1 from {M2, M4} from M3
Randomize C
M1 from M2 from M3 from M4
but several students used three or four. Figure 8-12
shows the students’ first experimental choice (top
graph) and the target of the first intervention they per-
formed regardless of when that first intervention
experiment occurred (bottom graph). Clearly, stu-
dents preferred passive observation as a first choice,
but the first choice for an intervention was
overwhelmingly the mediator C as opposed to either
endpoint variables A or B.

Experiment 3
In the third experiment, students were told that the true
model was one of the models in Figure 8-13, and we
randomly assigned students to have either the single-
edge model or the chain model (both highlighted in
Figure 8-13) as the true underlying causal structure.
(Students were not told that those were the only two
possibilities.) All participants were required to (a) begin
with the passive observation experiment, (b) eliminate
as many models as possible after each experiment, and
(c) find the true model in the minimum number of
experiments. Students recorded the experimental
design used to eliminate each model except the final
one. Students did not use the hypothetical graph win-
dow of the Causality Lab and so had no computational
aids to calculate the independencies implied by each
hypothesis under a given experimental setup.
In our experiment, over two thirds of participants
(11 of 15) answered correctly, and success was inde-
pendent of condition. Including the passive observa-
tion, students averaged just under three experiments
before reaching a final answer, and the number of
experiments was also independent of condition. As
one would expect, the 11 students who got the answer
right averaged significantly fewer experiments than
the 4 who got it wrong. For the remaining analyses,
we restrict our attention to the participant responses
after only the initial passive observation.
One question behind our experiment was whether
students acted as if they understood the concept of
MECs: sets of models that are indistinguishable by
passive observation because they imply the same set of
independence relations. In Figure 8-14 we show
again the 18 possible models, but group them in
boxes corresponding to the 9 MECs.
Individuals who (act as if they) understand the idea
of MECs should, for every equivalence class, either
keep or remove all its members together after the
passive observation stage. For equivalence classes D, E,
and F, which have only a single member, this necessar-
ily happens, so we exclude those classes. We then define
a (weighted) MEC “integrity” score as follows:
The weighting captures the fact that it is more
challenging to have MEC integrity for equivalence
classes G, H, and I, which have three members, than it
is for equivalence classes A, B, or C, which have two. If
a participant always keeps or removes members of an
MEC together, then MEC-Integrity equals 1; if mem-
bers of an MEC are never kept or removed together,
then MEC-Integrity equals 0. Figure 8-15 shows that
students exhibited an extremely high degree of MEC
integrity: 12 of 15 participants were perfect, and only 1
student was massively confused.
Even if someone exhibited perfect MEC integrity,
they might still be retaining or excluding the wrong
graphs (or the wrong MECs) given the data they
received. To measure whether they are including too
MEC integrity
if all models in mec were
included or all excluded

mec :
:
otherwise
mec
A B C G H I
0
:



{
}
∑
∈
, ,
,
,
,
15
132
CAUSATION AND PROBABILITY
FIGURE 8-13 Possibilities for Experiment 3.

many graphs, we computed the percentage of com-
mission errors:
Commission Error
Number of graphs retained
by student but not in co

,
rrect
MEC
Number of graphs not in
correct MEC
Similarly, to measure whether they are excluding
graphs equivalent to the truth, we computed the per-
centage of omission errors:
Omission Error
Number of graphs in the correct
MEC omitted by the stu

dent
Number of graphs not in correct
MEC
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
133
FIGURE 8-14 Equivalence classes for the passive observation experiment in Experiment 3.
FIGURE 8-15 MEC integrity.
MEC Integrity
Frequency
0
2
4
6
8
10
12
1.0
0.8
0.6
0.4
0.2

Not surprisingly, students were not as good on the
accuracy of their inferences. Figure 8-16 shows that,
although their omission error was quite low (few cor-
rect graphs were left out), students often retained
more graphs than were consistent with the passive
observation.
Interestingly, we think we can explain why.
Although we did not include equivalence classes D,
E, and F in our computation of MEC-Integrity
(because they each have only one graph as a mem-
ber), we did include those graphs in our calculations
of omission and commission error. These graphs each
have the same adjacencies as some equivalence class,
although they differ from the class in edge orienta-
tion. In Figure 8-14, classes D and G share the same
adjacencies, as do E and H, and F and I. If, for exam-
ple, the true graph was C →B →A (part of equiva-
lence Class H) and I included every graph in Classes
E and H, then I would have a perfect score on MEC-
Integrity but a nonzero commission error. In general,
if I attend only to adjacencies and ignore orientations,
I will (provably) always receive a perfect score on
MEC-Integrity, even though I might make a number
of commission errors.
After looking at the data, we hypothesized that stu-
dents were quite good at determining the correct
adjacencies but not very good at determining the cor-
rect orientations. To explore this, we first computed
participants’ Adjacency-Integrity to determine
whether the students included or excluded graphs
that share adjacencies as a unit.
The histogram in Figure 8-17 shows that students
had relatively high Adjacency-Integrity, suggesting
that the high MEC-Integrity scores were caused (at
least in part) by people keeping/removing graphs with
the same adjacencies and not necessarily those that
made the identical observational predictions.
This explanation does not completely account for
students’ performance. Many included graphs that
were neither Markov nor adjacency equivalent to the
truth. But, not all mistakes are quite the same.
Suppose the truth is A →B →C. Including the graph
A →B ←C is arguably a less-severe mistake than
including the graph B →C →A. In the former case,
the adjacencies were correctly learned, although not
the orientations. In the latter case, however, a true
adjacency (A-B) was excluded, and a false adjacency
(C-A) was included. We will say that a graph G is
adjacency consistent with a graph H if G’s adjacencies
are a subset of H’s or vice versa. The former error in
this example is adjacency consistent with the truth;
the latter error is not.
To understand better the severity of the students’
errors, we computed the proportion of the commission
Adjacency-Integrity
if all
els in adj were
included or all e

adj :
mod
xcluded
otherwise
adj
A B C D G E H F I
0
:



{
}
∑
∈
, ,
,
,
,



18
134
CAUSATION AND PROBABILITY
FIGURE 8-16 Commission and omission error.
Commission Error
Frequency
0
1
2
3
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Omission Error
Frequency
0
4
2
6
14
12
10
8
0
1

errors that were adjacency consistent with the true
MEC.
Figure 8-18 shows that students’ errors tend to be adja-
cency consistent; the majority of their mistakes
involved keeping a graph that was either a subgraph or
supergraph of the truth.
Of course, this high percentage could arise if most
graphs are adjacency consistent with the truth (although
this is not actually the case in this experiment). 
Adjacency Consistent Error
Number of graphs committed
that are adj

acency consistent
Number of graphs committed
To normalize for the number of errors that could be
adjacency consistent or inconsistent, we also computed:
Adjacency Inconsistent Inclusion
Number of committed graphs
that

are adjacency inconsistent
Number of committable graphs
that are adjacency inconsistent
Adjacency Consistent Inclusion
Number of committed graphs
that ar

e adjacency consistent
Number of committable graphs
that are adjacency consistent
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
135
FIGURE 8-17 Adjacency-integrity.
0.2
Adjacency Integrity
Frequency
0
1
2
3
4
5
6
7
0.4
0.6
0.8
1.0
FIGURE 8-18 Adjacency consistent error.
0.0
Adjacency Consistent Error
Frequency
0
1
2
3
4
0.4
0.2
0.6
0.8
1.0

If students were indifferent between adjacency consis-
tent and adjacency inconsistent errors, then the
within-student difference between these two measures
should center around 0. As Figure 8-19 shows, it
clearly does not.
These results seem to indicate that
1. Students have very high Adjacency-Integrity
(Figure 8-17).
2. A large fraction of the graphs committed are
adjacency consistent (Figure 8-18).
3. The fraction of the committable adjacency con-
sistent graphs that are actually committed is
much higher than the fraction of committable
adjacency inconsistent graphs that are actually
committed (Figure 8-19).
We interpret these results to mean that, like con-
straint-based algorithms and consistent with Danks
(2004), students are using one cognitive strategy for
detecting when two variables are adjacent and
another for detecting how the adjacencies are ori-
ented, especially in the case of data collected from
passive observation. Detecting whether X and Y are
adjacent is as simple as detecting whether X and Y are
independent conditional on any set. Detecting
whether X-Y-Z is oriented as X →Y ←Z or as one of
{X →Y →Z, X ←Y →Z, X ←Y ←Z} is much more
difficult.
Conclusions
The pilot studies discussed here are suggestive but 
still preliminary. Subjects had direct access to the
independence data true of the population, and in sev-
eral of our experiments the choices they confronted
were limited. Nevertheless, these studies suggest that
there is a lot to be learned from comparing naïve sub-
jects to those trained even for a short time on the nor-
mative theory of CBNs. For whatever reason, trained
subjects can reliably differentiate between direct and
indirect causation, and many can do so with an opti-
mal strategy for picking interventions. Indeed, our
first experiment suggests that trained students are not
subject to source bias in picking interventions, even
though they were never trained in this particular skill.
We speculate that simple training in the normative
theory sensitizes subjects to the connection between
conditional independence and indirect causation,
and attending to the mediating variable, which is the
conditioning variable, leads subjects to intervene on
the mediator instead of the source. Our pilot studies
also suggest that only minimal training in the norma-
tive theory is needed to exhibit sensitivity to model
equivalence, a core idea in the normative theory.
Finally, they suggest that students pursue a strategy by
which they find which pairs of variables are adjacent
and then attempt to find in which direction the causal
relations obtain.
136
CAUSATION AND PROBABILITY
FIGURE 8-19 Adjacency consistent-adjacency inconsistent inclusion.
0.50
Adjacency Consistent - Adjacency Inconsistent Inclusion
Frequency
0
1
2
3
4
5
0.00
0.25
0.25
0.50
0.75

Strategies for automatically learning causal struc-
tures in the normative theory divide into constraint-
based and score-based methods.15 In constraint-based
methods, one decides on individual constraints (e.g.,
independence or conditional independence facts) to
decide on local parts of the model (e.g., whether a
given pair of variables is adjacent or not). In score-
based searches, one computes a score reflecting the
goodness of fit of the entire model. Human subjects,
both naïve and trained, arguably execute a simple ver-
sion of a constraint-based search. Our subjects used
particular independence relations to decide on ques-
tions of adjacency, and were reliable at this, and then
used interventions to decide on orientation for local
fragments of the model, and were moderately reliable
at this. None judged models as a whole and attempted
to maximize some global score. As it turns out,
constraint-based approaches are much more efficient
but less accurate in the face of noisy data. Our conjec-
ture is that human subjects employ a constraint-based
approach because it allows a sequence of decisions,
each involving a potentially simple computation, like
whether two variables are independent or not.
In systems of more than toy complexity, that is,
systems involving more than two or three variables, a
score-based strategy would become computationally
prohibitive for a human cognitive agent, while a con-
straint-based approach would still be viable. Because
a constraint-based approach also lends itself to an any-
time approach, that is, using only the simplest con-
straints first and then stopping “any time” the
constraints under test become too complicated to
compute or to trust statistically, it is also well suited to
systems with severe computational or memory con-
straints (e.g., human learners).
Nevertheless, we do not claim that evolution has
trained humans to execute anything like the theoreti-
cally correct version of a constraint-based search for
causal structure. Even minimally trained subjects
using a constraint-based approach well suited for toy
systems but not theoretically correct might quickly be
overcome by the complexity of a five-variable system.
In informal observation, this is exactly what happens.
Even on systems involving four variables, if subjects
are given no background knowledge whatsoever
about which variables are prior to which others (e.g.,
which variable is the “outcome” variable), then they
become quickly lost in the more than 50 models in
their search space. In future experiments, we will
investigate the discontinuities in performance for
trained subjects as a function of system complexity.
We will train subjects to execute a modified version of
a constraint-based approach that would handle much
larger systems and see if this will help students to
become truly more reliable causal learners.
ACKNOWLEDGMENTS
This research was supported
by the James S. McDonnell Foundation, the Institute
for Education Science, the William and Flora Hewlett
Foundation, the National Aeronautics and Space
Administration, and the Office of Naval Research
(grant to the Institute for Human and Machine
Cognition: Human Systems Technology to Address
Critical Navy Need of the Present and Future 2004).
We thank Adrian Tang and Greg Price for
invaluable programming help with the Causality
Lab, Clark Glymour for forcing us to get to the
point, and Dave Sobel and Steve Sloman for several
helpful discussions.
References
Berger, M. (2005). Applied optimal designs. New York:
Wiley.
Blalock, H. (1961). Causal inferences in nonexperimental
research. Chapel Hill: University of North Carolina
Press.
Bowden, R., & Turkington, D. (1984). Instrumental vari-
ables. New York: Cambridge University Press.
Cheng, P. W. (1997). From covariation to causation: A
causal power theory. Psychological Review, 104,
367–405.
Cheng, P. W., & Novick, L. R. (1992). Covariation in
natural causal induction. Psychological Review, 99,
365–382.
Cochran, W., & Cox, G. M. (1957). Experimental
designs (2nd ed.). New York: Wiley.
Danks, D. (2004). Constraint-based human causal learn-
ing. In M. Lovett, C. Schunn, C. Lebiere, &
P.
Munro (Eds.), Proceedings of the Sixth
International Conference on Cognitive Modeling
(ICCM-2004; pp. 342–343). Mahwah, NJ: Erlbaum.
Danks, D. (2005). Causal learning from observations and
manipulations. In M. Lovett & P. Shah (Eds.),
Thinking with data. Mahwah, NJ: Erlbaum.
Danks, D., Griffiths, T. L., & Tenenbaum, J. B. (2003).
Dynamical causal learning. In S. Becker, S. Thrun,
& K. Obermayer (Eds.), Advances in neural infor-
mation processing systems 15
(pp. 67–74).
Cambridge, MA: MIT Press.
Eberhardt, F., Glymour, C., & Scheines, R. (2005). 
N  1 experiments suffice to determine the causal
TEACHING THE NORMATIVE THEORY OF CAUSAL REASONING
137

relations among N variables (Tech. Rep. No.
CMU_PHIL-161), Carnegie Mellon University,
Department of Philosophy, Pittsburgh, PA.
Glymour, C. (1998). Learning causes: Psychological
explanations of causal explanation. Minds and
Machines, 8, 39–60.
Glymour, C. (2000). Bayes nets as psychological models.
In F. C. Keil & R. A. Wilson (Eds.), Explanation
and cognition. Cambridge, MA: MIT Press.
Glymour, C., & Cooper, G. (1999). Computation, cau-
sation, and discovery. Cambridge, MA: AAAI
Press/MIT Press.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 3–32.
Gopnik, A., Sobel, D. M., Schulz, L. E., & Glymour, C.
(2001). Causal learning mechanisms in very young
children: 2-, 3-, and 4-year-olds infer causal rela-
tions from patterns of variation and covariation.
Developmental Psychology, 37, 620–629.
Griffiths, T. L., Baraff, E. R., & Tenenbaum, J. B. (2004).
Using physical theories to infer hidden causal struc-
ture. Proceedings of the 26th Annual Conference of
the Cognitive Science Society.
Lagnado, D., & Sloman, S. A. (2002). Learning causal
structure.
Proceedings of the 24th Annual
Conference of the Cognitive Science Society,
Maryland.
Lagnado, D., & Sloman, S. A. (2004). The advantage of
timely intervention. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 30,
856–876.
Murphy, K. (2001). Active learning of causal Bayes net
structure (Tech. Rep.), University of California-
Berkeley, Computer Science Division, Berkeley, CA.
Pearl, J. (1988). Probabilistic reasoning in intelligent sys-
tems. San Mateo, CA: Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, reasoning, and infer-
ence. Cambridge, England: Cambridge University
Press.
Perales, J. C., & Shanks, D. R. (2003). Normative and
descriptive accounts of the influence of power and
contingency on causal judgement. The Quarterly
Journal of Experimental Psychology, 56A, 977–1007.
Rescorla, R. A., & Wagner, A. R. (1972). A theory of
Pavlovian conditioning: Variations in the effective-
ness of reinforcement and nonreinforcement. 
In A. H. Black & W. F. Prokasy (Eds.), Classical
conditioning II: Current research and theory
(pp. 64–99). New York: Appleton-Century-Crofts.
138
CAUSATION AND PROBABILITY
Simon, H. (1953). Causal ordering and identifiability. In
Hood & Koopmans (Eds.), Studies in econometric
methods (pp. 49–74). New York: Wiley.
Sloman, S. A., & Lagnado, D. (2002). Counterfactual
undoing in deterministic causal reasoning.
Proceedings of the 24th Annual Conference of the
Cognitive Science Society, Maryland.
Sobel, D. M., & Kushnir, T. (2004). Do it, or watch it
done: The importance of decision demands in causal
learning from interventions. Manuscript submitted
for publication, Brown University.
Spiegelhalter, D., & Lauritzen, S. (1990). Sequential
updating of conditional probabilities on directed
graphical structures. Networks, 20, 579–605.
Spirtes, P., Glymour, C., & Scheines R. (2000).
Causation, prediction and search (2nd ed.),
Cambridge, MA: MIT Press.
Steyvers, M., Tenenbaum, J. B., Wagenmakers, E. J., &
Blum, B. (2003). Inferring causal networks from
observations and interventions. Cognitive Science, 27,
453–489.
Tenenbaum, J. B., & Griffiths, T. L. (2001). Structure
learning in human causal induction. In T. Leen, T.
Deitterich, & V. Tresp (Eds.), Advances in neural
information processing 13 (pp. 59–65). Cambridge,
MA: MIT Press.
Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-
based causal inference. In S. Becker, S. Thrun, &
K. Obermayer (Eds.), Advances in neural informa-
tion processing systems 15 (pp. 35–42). Cambridge,
MA: MIT Press.
Tenenbaum, J. B., & Niyogi, S. (2003). Learning causal
laws. In Proceedings of the 25th Annual Conference
of the Cognitive Science Society.
Tong, S., & Koller, D. (2001). Active learning for struc-
ture in Bayesian networks. Proceedings of the
International Joint Conference on Artificial
Intelligence.
Waldmann, M. R., & Hagmayer, Y. (in press). Seeing
versus doing: Two modes of accessing causal knowl-
edge. Journal of Experimental Psychology: Learning,
Memory, and Cognition.
Waldmann, M. R., & Martignon, L. (1998). A Bayesian
network model of causal learning. In M. A.
Gernsbacher & S. J. Derry (Eds.), Proceedings of the
20th Annual Conference of the Cognitive Science
Society. Mahwah, NJ: Erlbaum.
Wright, S. (1934). The method of path coefficients. Annals
of Mathematics Statistics, 5, 161–215.

Causal knowledge enables children to interpret the
current state of the world rationally and to engage in
predictive inference and explanation. Traditionally,
young children’s causal knowledge has been consid-
ered “perceptually driven” or “precausal” (e.g., Piaget,
1929). Contemporary research, however, has shown
that young children’s causal reasoning abilities are
actually quite sophisticated. Infants recognize causal
properties of objects, including containment, support,
and contact (e.g., Hespos & Baillargeon, 2001; 
Leslie & Keeble, 1987; Needham & Baillargeon, 1993;
Spelke, Breinlinger, Macomber, & Jacobson, 1992).
Before their second birthday, toddlers recognize vari-
ous nonobvious causal relations, especially about oth-
ers’ desires and intentions (e.g., Meltzoff, Gopnik, &
Repacholi, 1999). By age 5, children understand that
biological and psychological events rely on nonobvi-
ous, hidden causal relations (e.g., Gelman & Wellman,
1991; Gopnik & Wellman, 1994). More generally,
preschoolers recognize the importance of Hume’s 
principles—temporal priority, spatial priority, and 
contingency—in making judgments about causal rela-
tions (Bullock, Gelman, & Baillargeon, 1982; Shultz,
1982). Preschoolers also appear to have sophisticated
explanative and counterfactual reasoning abilities
(Harris, German, & Mills, 1996; Schult & Wellman,
1997; Sobel, 2004; Wellman & Liu, chapter 16, this
volume).
As developmentalists, we wish to describe how chil-
dren learn causal knowledge and develop their reason-
ing abilities. How children represent and acquire causal
knowledge, however, is an interdisciplinary question,
and this volume illustrates how philosophy, computer
science, and cognitive psychology can offer different
insights into the process. We would like to suggest that
other branches of developmental research—specifically
research on infants’ statistical learning—offer insight
into causal learning. Conversely, understanding how
children learn and reason about causal relations might
provide insight into other areas of development.
139
9
Interactions Between Causal and Statistical Learning
David M. Sobel & Natasha Z. Kirkham

In this chapter, we examine the relation between
young children’s causal learning and inference abili-
ties and their capacity to perceive the statistical associ-
ations between salient events. Of course, there are
significant differences between recognizing statistical
relations and causal knowledge. Knowing what causal
relations exist allows learners to generate explanations
and reason about counterfactuals, neither of which is
supported by pure statistical association. Statistical
associations do allow for simple predictions about
future events and the chunking of correlated stimuli
(for more efficient processing). But, causal knowledge
goes much further than that, allowing for a “calculus
of intervention” (Pearl, 2000): inferences about the
outcome of intentional manipulative actions that
change the state of events in the world. Recognizing
that two events are associated provides no information
about the result of interventions on either event. But,
although correlations do not equate to causal rela-
tions, they are often a good place to start. Under-
standing whether and how children acquire statistical
information about events in the world should provide
a starting point for researchers interested in causal
learning. Likewise, children’s causal reasoning abili-
ties might provide insight into phenomena discussed
in the statistical learning literature.
How Statistical Regularity Can
Translate to Causal Knowledge
A system for causal learning has a particular prob-
lem: Although some causal relations seem directly 
perceivable—such as watching a ball launch another
ball (Michotte, 1963)—in general, causal knowledge is
not directly perceptible. One goal of research in chil-
dren’s causal learning has been to describe how causal
knowledge can be recovered from the environment.
How children recognize correlations between objects
and events seems a good place to start. For example,
knowing that a particular causal relation exists suggests
that certain data will occur; if event X causes event Y,
then the occurrence of X will make the occurrence of
Y more likely (all other things being equal). Observing
such correlations might offer insight into causal struc-
ture. Seeing that Y is more likely in the presence of X
than in its absence often leads us to conclude that X is
a cause of Y. Indeed, some adult experiments on causal
learning suggest that such probabilistic reasoning
might be considered a normative model of causal infer-
ence (Allan, 1980; Cheng, 1997; Shanks, 1995).
Of course, such correlations do not always equate
to genuine causal conclusions. Consider three events
related by a simple causal chain X →Y →Z. In this
situation, X and Y are correlated, X and Z are corre-
lated, and Y and Z are correlated. Temporal priority
(or other forms of prior knowledge) might inform you
of the directions of the potential causal relations spec-
ified by these correlations, but the correlations them-
selves potentially overgeneralize the causal structure.
Whether X causes Z directly or only indirectly
through Y is ambiguous given only this information.
What is necessary is a system that recognizes not only
the dependencies among these events, but also their
conditional independencies as well. Observing that X
and Z only co-occur in the presence of Y (and thus are
independent in the absence of Y) suggests the causal
chain model. If this conditional independence rela-
tionship was absent, then a more general model in
which X directly causes both Y and Z (and in which Y
causes Z) is more likely.
What this suggests is that children must recognize
the dependencies among events as well as conditional
probability information to learn the causal structure
of the world. Researchers in causal learning have
examined whether children recognize conditional
dependence and independence information when
making causal inferences (Gopnik, Sobel, Schulz, &
Glymour, 2001; Sobel, Tenenbaum, & Gopnik,
2004). Much of this research introduced children to a
blicket detector, a machine that lights up and plays
music when certain objects are placed on it. The
blicket detector presents a novel, nonobvious property
of each object: its potential to activate the detector.
(The machine is actually controlled through an
“enabling” switch. When the switch is on, any object
will activate the detector. When it is off, no object will
activate the detector.)
Gopnik et al. (2001) trained 3- and 4-year-olds that
objects that activated the detector were called blickets.
Children quickly learned this association. Then, chil-
dren observed a set of trials in which objects either
independently activated the machine or did so only
dependent on the presence of another object.
Specifically, in the one-cause trials, children were
shown two objects. Children observed one object A
activate the detector by itself. Then, they saw that the
other object B did not activate the detector by itself.
Finally, they saw objects A and B activate the detector
twice together (see Figure 9-1 for a schematic of this
procedure). Children were asked whether each object
140
CAUSATION AND PROBABILITY

was a blicket. In this condition, 3- and 4-year-olds
labeled only object A as a blicket. Object B only acti-
vated the detector in the presence of the object A.
Performance on these trials was compared with per-
formance on the two-cause trials, in which children
were shown two objects that activated the detector with
the same frequency as in the one-cause trials. Speci-
fically, children saw two new objects (C and D). Object
C was placed on the machine three times and activated
it all three times. Object D was placed on the machine
three times and activated it two of three times (see
Figure 9-1). On these trials, 3- and 4-year-olds catego-
rized both objects as blickets. Both objects independ-
ently activated the detector; they just did so with
different frequencies.
These data suggest that children can recognize the
difference between dependencies and conditional
independencies between two events when faced with
information about their statistical regularity (what
Reichenbach in 1956 called screening-off reasoning).
This type of reasoning represents a move from recogni-
zing just the co-occurrence among events to recognizing
the information necessary to make causal inferences.
This procedure generalizes beyond reasoning about
physical events: Schulz and Gopnik (2004) demon-
strated that 3- and 4-year-olds make similar screening-
off inferences across a variety of domains (see also
Schulz et al., chapter 5, this volume). Younger chil-
dren also appear to make similar inferences. Using
slight manipulations to the procedure, Gopnik et al.
(2001) demonstrated that 30-month-olds made these
inferences. Sobel and Kirkham (in press) demonstrated
that children as young as 19 months also reasoned in
this manner about objects placed on a blicket detector.
The Associative Challenge
The trouble with the procedure described is that a
mechanism for causal reasoning does not exclusively
explain children’s ability to make screening-off infer-
ences. Screening off is a form of blocking, a phenom-
enon from the animal conditioning literature. In a
classic blocking experiment (Kamin, 1969), an animal
is shown an association between a conditioned and
an unconditioned stimulus (e.g., that a tone predicts
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
141
FIGURE 9-1 Schematic representation of the procedures used by Gopnik et al. (2001, Experiment 1).

the occurrence of food). This association is trained
until asymptote, and then the animal is shown a
novel conditioned stimulus presented in compound
with the established stimulus (e.g., that the same
tone paired with a light will predict food). Animals do
not learn that the light is predictive. One interpreta-
tion of these data is that the animals recognize that
light only predicts food in the presence of an estab-
lished predictor (i.e., the tone). Various models of
associative learning (e.g., Rescorla & Wagner, 1972)
were designed to explain this phenomenon.
However, inferential models that rely on calculating
the associative strength among events can have difficulty
when the data involve learners making retro-
spective inferences. One example, taken from the 
contingency judgment literature, is the phenomenon of
backward blocking (Shanks, 1985; Shanks & Dickinson,
1987). In these experiments, adult learners were pre-
sented with two stimuli in compound (A and B) that
elicited some effect. Learners were then shown that one
of those two events alone (A) elicits the effect. Given this
information, adults rated that the B stimulus did not
have the causal efficacy necessary to produce the effect.
Can children engage in backward blocking about
causal events? Sobel et al. (2004) introduced pre-
schoolers to the blicket detector and trained them that
blickets activated the machine. They showed children
two objects (A and B) that activated the machine
together. Then, they showed children that Object A
activated the machine by itself. This procedure is shown
in Figure 9-2. The critical question was how children
would rate Object B. Its causal status is uncertain. If
children engage in backward blocking, then they
should determine that it is not a blicket. This was the
case: Children rarely labeled Object B as a blicket.
Sobel et al. (2004) also demonstrated that children
did not follow a simple algorithm that only recog-
nized associations among events (e.g., Rescorla &
Wagner, 1972). In a different type of trial—indirect
screening-off trials—children were shown two differ-
ent objects (C and D) that activated the machine
together and then that Object C failed to activate the
detector. This procedure is also shown in Figure 9-2.
In this circumstance, only Object D should be con-
sidered a blicket: Object C fails to activate the detec-
tor independently, so the only logical conclusion
children could draw is that Object D has the causal
efficacy necessary to activate the detector. Indeed, 3-
and 4-year-olds generated this response. However,
responding on the basis of only associations, one
would consider Objects B and D’s associative strength
with the detector’s activation to be the same. In both
142
CAUSATION AND PROBABILITY
FIGURE 9-2 Schematic representation of the procedures used by Sobel et al. (2004, Experiment 1).

cases, the object activates the detector with another
object. The efficacy of that other object alone should
have no bearing on its associations with the detector.
Although these data are inconsistent with various
associative accounts, there are several different cate-
gories of learning algorithms that describe how adults
make inferences about contingencies among events,
which can explain these findings. Children might rec-
ognize causal relations based on a calculation of asso-
ciative strength among events that relies on more
complicated associative mechanisms (Dickinson,
2001; Kruschke & Blair, 2000; Wasserman & Berglan,
1998). These models were designed with the backward
blocking phenomenon in mind. Alternatively, children
might make causal inferences through estimates of
causal strength based on the frequency with which
events co-occur. Such models, like the P model
(Allan, 1980; Jenkins & Ward, 1965; Shanks, 1995) and
the PowerPC model (Cheng, 1997, 2000), calculate an
estimate of the strength of a presumed causal relation-
ship given a set of data. The backward blocking data
are also consistent with these possibilities.
Sobel et al. (2004) and others (Tenenbaum,
Griffiths, & Niyogi, this volume) pointed out that many
of these learning mechanisms rely on multiple expo-
sures to data (i.e., large sample sizes). Models that
calculate causal structure through associative
strength or through parameter estimation, like the P
and PowerPC models, must have large sample sizes to
function properly. In this view, the backward blocking
data are inconsistent with all of these accounts because
children make inferences based on relatively small
sample sizes. However, Sobel et al. (2004) also wanted
to demonstrate that children’s causal inferences were
well described by a different type of learning algorithm:
one that relies on Bayesian inference. In Bayesian infer-
ence, learners assign a probability value to a set of
potential causal hypotheses and then update the val-
ues of those probabilities given the observed data
based on the application of Bayes’ rule. The resulting
posterior probabilities are a rational estimate of the
likelihood of each hypothesis being the correct causal
model (see Tenenbaum, Griffiths, & Niyogi, this volume,
for a more detailed description of this model).
This account relies on the assumption that chil-
dren assign the initial probabilities of each hypothesis
nonrandomly: Those priors are set by the base rate of
blickets. If children recognize that there are many
blickets out there in the world, then hypotheses that
specify that many objects are blickets should have a
higher initial probability than hypotheses that specify
few objects are blickets. In this case, the hypothesis in
which both Object A and Object B are blickets
should have a higher initial probability than the
hypothesis that only Object A is a blicket. Both are
consistent with the observed data, and thus both will
be updated equally by the application of Bayes’ rule.
Thus, the hypothesis that both objects are blickets will
have a higher posterior probability. Thus, Bayesian
reasoning predicts that if children know there are
many blickets in the world, then they should not
demonstrate backward blocking.
To test this hypothesis, Sobel et al. (2004, Experi-
ment 3) showed children a set of identical objects that
were placed on the blicket machine. They trained chil-
dren that blickets were either rare or common: 12
objects were scanned 1 at a time, and either 2 or 10
activated the machine in the rare and common condi-
tions, respectively. Then, they presented children the
same backward blocking procedure with two new
objects (from the same set). When 4-year-olds were
trained that blickets were rare, they demonstrated back-
ward blocking: The uncertain object was not catego-
rized as a blicket. When 4-year-olds were trained that
blickets were common, they did not demonstrate back-
ward blocking: The uncertain object was categorized
as a blicket. There was not enough counterevidence to
exclude the hypothesis that both objects were blickets.
These data were qualitatively consistent with the
Bayesian account. The same ambiguous backward
blocking data were presented across the conditions,
and children relied on the base rates of blickets to
make an inference about an object with causal powers
that were uncertain. Further research demonstrated
that adults also reason about such data in a similar man-
ner (Tenenbaum, Sobel, Griffiths, & Gopnik, submitted).
Tenenbaum et al. also introduced a new learning prob-
lem in which both children and adults observed only
ambiguous data. Adult learners were introduced to a
machine like a blicket detector (a detector that
responded to a special kind of lead in pencils, dubbed
superlead, and hence, superpencils) and were trained
that the occurrence of pencils containing this lead was
rare (using the same manipulation as that of Sobel et
al. in 2004–by showing them that 2 of 12 pencils cho-
sen at random from a set activated the detector). Then,
they were shown 3 pencils taken from the set at ran-
dom (A, B, and C). Objects A and B activated the
machine together, and then Objects A and C activated
the machine together. Participants were asked to rate
the likelihood that each object was a super pencil after
each event.
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
143

The Bayesian account predicts four levels of per-
formance given these data. First, ratings of Object A
at the end of the trial should be highest, but not at
ceiling. This reflects the fact that learners did not
unambiguously observe Object A activate the
machine, but the majority of hypotheses consistent
with the data suggest that Object A is a super pencil.
The ratings of A and B after they are placed on the
machine together should be slightly lower. This
reflects the fact that the data at this point in the trial
suggest that at least one of those objects must be a
superpencil. The ratings of B and C at the end of the
trial should be lower, but still higher than the initial
ratings of each object. There are some hypotheses
consistent with B and C being superpencils (namely,
the hypothesis that B and C are superpencils, and A is
not). Tenenbaum et al. (submitted) observed exactly
this four-level pattern of responses. This pattern of
performance also extends to children. In a subsequent
experiment, they found that 4-year-olds made similar
responses, consistent with these qualitative predic-
tions of the Bayesian model.
In general, these experiments integrate children’s
(and adults’) use of statistical information from the
environment with their causal inferences. In the rare-
common backward blocking manipulation, children’s
retrospective inferences were guided by the base rate
of blickets. Because the blicket detector introduced a
novel causal relation, children had to rely on that ini-
tial exposure to establish the training. Indeed, the
original backward blocking experiments can be rean-
alyzed in terms of the base rate of blickets. In Sobel 
et al.’s first experiment, the base rate of blickets was
exactly 50%. In this case, 4-year-olds rated the
B object (the blocked object) as a blicket 13% of the
time in the backward blocking condition. In their sec-
ond experiment, the base rate of blickets was slightly
higher (it varied between 60% and 80%, with a mean
of 68%). In this case, 4-year-olds categorized this
object as a blicket 35% of the time, slightly higher
than the previous experiment. Even without explicit
training, 4-year-olds seemed to pick up on the base
rate of objects that activated the detector and used
that information to guide their inferences.
Can Younger Children Make
Retrospective Inferences?
Sobel and Kirkham (in press) examined whether 
toddlers were capable of retrospectively making
screening-off inferences. They introduced 19- and
24-month-olds to the blicket detector and established
that both age groups would place causally efficacious
objects on the machine. Then, children were shown
two objects (A and B) that activated the machine
together, and then that object A did not activate the
machine by itself. When these objects and the
machine were presented to the child with the instruc-
tion to “make it go,” 24-month-olds placed Object B
on the detector by itself significantly more often than
all other responses put together. The 19-month-olds,
in contrast, responded no differently from chance.
Sobel and Kirkham (in press) also presented these
children with a backward blocking inference.
Because the children were too young for verbal meas-
ures, they could not replicate the Sobel et al. (2004)
procedure. Instead, they showed children three
objects (A, B, and C). Objects A and B activated the
machine together, and then Object A activated the
machine by itself. Object A was removed from the dis-
play, and Objects B and C and the machine were pre-
sented to the child. If children made a backward
blocking inference, then they might be more inclined
to choose Object C (the novel object) in this condi-
tion because they would infer that Object B is ineffec-
tive. Both 19- and 24-month-olds chose between
Objects B and C at chance.
The importance of this procedure, however, is not
in these results, but in the comparison with the indirect
screening-off procedure because the associative strength
of Object B is the same across the two tasks (at least on
many associative models like the Rescorla-Wagner
model). The 24-month-olds’ use of Object B to activate
the detector differed between these two conditions; 19-
month-olds chose Object B with the same frequency
across the two trials. Importantly, Sobel and Kirkham
(in press) did find that these 19-month-olds recognized
screening-off inferences that involved no retrospection.
The critical question is whether these causal reasoning
abilities are developing during the toddler years.
Statistical Learning
A difficulty with testing toddlers’ causal inferences is
that there are some cases in which 18-month-olds fail
to engage in simple, imitative “means-ends” behav-
iors (e.g., Uzgiris & Hunt, 1975; see also Gopnik &
Meltzoff, 1992). Although the children who partici-
pated in Sobel and Kirkham’s (in press) experiment
144
CAUSATION AND PROBABILITY

were slightly older, to count as making a retrospective
inference in the indirect screening-off trials, they had
to inhibit an event they observed activate the machine
(placing both objects on it) in favor of a novel interven-
tion (placing only Object B on it). The demand char-
acteristics of this experiment might have overwhelmed
the toddlers from producing these inferences.
There is reason to believe that 18-month-olds,
and even younger children, have the ability to detect
conditional probabilities among events. Saffran,
Aslin, and colleagues found that 8-month-old infants
could parse a stream of auditory stimuli based solely
on the transitional probabilities within and between
syllables (i.e., the likelihood that one syllable would
predict the next syllable; Aslin, Saffran, & Newport,
1998; Saffran, Aslin, & Newport, 1996).
For example, Saffran et al. (1996) presented
infants with a 2-minute constant speech stream of 12
unique syllables, which could be parsed into four 
3-syllable “words.” These words were presented
through speakers located on either side of the seated
infant and were defined only by the transitional prob-
abilities between syllables (i.e., there were no pauses
or other cues to word beginnings or endings).
Syllables that occurred within words always predicted
each other; their transitional probabilities were always
equal to 1. In other words, the first syllable in a word
always predicted the second syllable, and the second
syllable always predicted the third. Syllables that
occurred across word boundaries were less pre-
dictable. In this particular case, because there were
only four words in the speech stream, the transitional
probability was equal to .33. The last syllable in a
word predicted the first syllable of the three other
words with equal likelihood. Infants were conditioned
to turn their heads toward the speaker producing the
novel strings (using a preferential head-turn para-
digm). When they turned away from the speaker, the
speech stream would stop. In this way, infants con-
trolled their individual exposure to the auditory stimuli.
After familiarization, infants listened to three sylla-
bles that made up words [i.e., three syllables A, B, and
C, in which p(B |A)1 and the p(C | B)1], alternat-
ing with three syllables that did not make up a word
(i.e., three syllables that did not obey these transi-
tional probabilities). The infants showed significantly
greater interest in the nonwords than in the words, as
measured by the amount of time spent looking at the
speakers. Because infants will consistently look longer
at novel stimuli, postfamiliarization (Bornstein, 1985),
these results suggest that the infants discriminated
between the words and the other stimuli based on
learning the transitional probabilities defining word
boundaries (see also Aslin et al., 1998, for evidence
that the results stem from true computation of input
statistics rather than simple frequency counting).
Infants’ statistical learning abilities extend beyond
learning word boundaries. Infants are capable of recog-
nizing and discriminating between complex grammars
relating words together. Using the preferential head-
turn paradigm, Gomez and Gerken (1999) exposed 12-
month-olds to a subset of novel strings produced by one
of two artificial grammars. These grammars differed
only in terms of the ordering of word pairs: Individual
words in the two sets and the starting and ending words
were always the same. The only cues to recognition
were contained in the transitional probabilities inherent
in the word order. After familiarization to the grammar,
infants were exposed to novel words embedded in either
the original grammar or a novel grammar. Infants
showed significantly increased looking time to the
speaker producing novel words in the original grammar,
suggesting that they could discriminate between the two
grammars even when the words were unfamiliar.
The ability to extract regularities in sequential
input does not seem to be a language-specific mecha-
nism, but exists broadly across audition. Infants parse
auditory streams based on statistical probabilities even
when the stimuli are tones (Saffran, Johnson, Aslin, &
Newport, 1999). Further, at least one species of non-
human primates, cottontop tamarins (which never
develop humanlike language skills), can learn statisti-
cally structured sounds (Hauser, Newport, & Aslin,
2001). This suggests that the ability to perceive statis-
tical structure is perhaps not language specific.
There is evidence from other paradigms that infants
show some sensitivity to visual spatial relations among
repetitive events. Young infants learn simple two-
location, predictable spatial sequences in a visual expec-
tation paradigm (Haith, 1993). Infants also show
sensitivity to spatial contingency in temporal sequences.
Wentworth, Haith, and Hood (2002) presented
3-month-old infants with a spatiotemporal sequence in
which a stimulus appeared on the left, in the center, or
on the right of a computer monitor. Infants viewed
either a fixed or a random pattern of locations, and in
some cases there was a contingent relation between the
identity of the central stimulus and the location of the
next peripheral picture. The fixed sequence of three
locations resulted in more eye movement anticipations,
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
145

and there were more anticipatory saccades to the
correct location when there was a contingent relation
between central and peripheral events.
Infants can also recognize statistical structure in dis-
plays of greater complexity than simple two- and three-
location events. Kirkham, Slemmer, and Johnson
(2002) demonstrated that infants as young as 2 months
old could learn temporal sequences of shapes that
were defined by transitional probabilities. Kirkham,
Slemmer, 
and 
Johnson 
(2004) 
found 
that 
8-month-olds were capable of extracting these probabil-
ities even when the visual sequence was both temporal
and spatial. In addition, Fiser and Aslin (2003) demon-
strated that 9-month-olds are capable of picking up on
the correlations between individual visual elements in
a series of static multielement scenes. After being
exposed to a number of these scenes, the infants were
shown isolated element pairs that had co-occurred
either frequently within the scenes or rarely; infants
were capable of discriminating between the two.
Statistical Learning Across Modalities
These data suggest that infants—perhaps as young 
as 2 months—recognize conditional probabilities
among events and respond to sequences based on
those transitions. These learning and inferential abili-
ties go beyond observing sequences of events; knowl-
edge about the environment requires correctly
correlating events across sensory modalities.
Indeed, infants develop a variety of intersensory
capacities that allow them to integrate information
across modalities. Newborns bind auditory stimuli to
visual stimuli and then expect that the sounds and their
associated objects will move together (Morrongiello,
Fenwick, & Chance, 1998; Richardson & Kirkham,
2004). By 4 months of age, infants perceive the
bimodal nature of objects (Spelke, 1979, 1981), and
they can perceive speech bimodally (Kuhl & Meltzoff,
1982). Four-month-olds also match faces with voices
based on age, gender, and (at 5 months) affective
expression of the speaker (Bahrick, Netto, &
Hernandez-Reif, 1998; Walker, 1982). By 5 months,
infants also recognize the importance of this sensory
integration. Bahrick and Lickliter (2000) demon-
strated that infants habituated to a bimodal presenta-
tion of an event sequence (e.g., a hammer tapping out
a particular rhythm) would dishabituate to the uni-
modal presentation of that information (e.g., just the
visual of the hammer tapping, without the sound).
These capacities indicate that infants not only prefer
multimodal cues that present them with statistical
redundancies but also recognize their importance in
perceiving the world. Infants’ sensitivity to cross-modal
information stands in contrast to the sparse, unimodal
presentations of many laboratory experiments described
here (e.g., Fiser & Aslin, 2003; Kirkham et al., 2002;
Saffran et al., 1996). If experimental studies do not fully
exploit the cross-modal sensitivity of infants, then per-
haps they risk underestimating the full capacity of their
learning abilities. Bahrick, Lickliter, and colleagues
have presented evidence that intersensory redundancy,
the overlap of information provided by amodal stimuli,
drives selective attention (e.g., Bahrick & Lickliter,
2000; Bahrick, Lickliter, & Flom, 2004). Can infants
usefully integrate statistical information across different
modalities?
One way in which this question can be answered is
in considering infants’ understanding of objects as
enduring across space and time behind an occluder.
In experimental settings, typically the demonstration is
unimodal (e.g., a silent visual display of a ball traveling
across the visual field and passing behind and then
remerging from an occluder). Kirkham and Johnson
(2006) demonstrated that 4-month-old infants, who
are right at the beginning of a transition toward success
at perceptual completion in an object constancy para-
digm (e.g., correctly perceiving the constant trajectory
the ball), benefit greatly from the presence of cross-
modal information. They incorporated a continuous
moving sound into the ball-and-occluder paradigm
such that the sound traveled with the object from one
side of the occluder to the other side. When given
these multiple, redundant, cross-modal cues, 4-month-
old infants could anticipate trajectories as well as 6-
month-olds in a unimodal condition.
Multiple redundant cues are useful when one has
to learn from probabilistic information. For example,
if you test positive for a disease on a blood test that is
90% effective on two separate occasions, then you can
be more than 90% sure that you are indeed suffering
from the disease. Research modeling language learn-
ing has shown that multiple probabilistic cues (e.g.,
lexical stress, phonemes, and pauses) can be inte-
grated to produce faster learning of word boundaries
and syntax, even though each cue individually might
be unreliable (Christiansen, Allen, & Seidenberg,
1998; Christiansen & Dale, 2001). Further, models
have shown a particularly robust effect of cross-modal
information in the service of learning (de Sa &
Ballard, 1998).
146
CAUSATION AND PROBABILITY

Kirkham, Slemmer, and Johnson (2006) demon-
strated one method in which redundant cue integration
benefited infants’ statistical learning. When 8-month-
olds were presented with a visuospatial pattern (e.g., a
red circle that appeared in one of six locations and in a
statistically probable pattern), they were unable to learn
the statistical relationships within the sequence success-
fully. However, when redundant color and shape cues
were added into the sequence (e.g., each shape in the
pattern had a unique color and shape), performance
improved significantly. Redundant information sup-
ported infants’ statistical learning abilities.
How Statistical Learning Informs Our
Understanding of Causal Learning
These studies provide compelling evidence that
infants are sensitive to statistical regularities across
various modalities but leave open the intriguing ques-
tion of how such abilities could support the complex
inferences that exist in causal reasoning. When
preschoolers use conditional probability to make
judgments about whether objects are blickets, are
they relying on the same mechanisms as infants learn-
ing word boundaries or structural information about
the visual world?
Several different research groups have suggested that
children’s and adults’ causal knowledge and reasoning
abilities can be described by a particular computational
framework: causal graphical models (Glymour, 2001;
Gopnik et al., 2004; Lagnado & Sloman, 2004;
Waldmann & Hagmayer, 2001). The data on chil-
dren’s causal inferences are all consistent with this
representation of causal knowledge. To make these
models causal, they must meet a set of assumptions
(see Gopnik et al., 2004), but at heart, causal graphi-
cal models represent joint probability distributions—
the frequency with which all possible combinations of
events occur. This would imply that recognizing sta-
tistical regularities among events is critical for causal
learning and reasoning, and infants’ statistical learn-
ing abilities build up to an understanding of causal
relations among events.
One implication of this hypothesis is that infants
should be able to engage in the kind of retrospective
inferences about statistical regularity among events.
Our previous investigations suggested that 19-month-
olds could not make these kinds of inferences when
presented with the blicket detector procedure.
However, these difficulties could have resulted from
the motor demands of the experiment. Using statistical
learning procedures that involve measuring infants’ eye
gaze eliminated these demands. We have begun inves-
tigating this hypothesis by presenting 8-month-old
infants with a statistical learning procedure that exam-
ines these abilities (Sobel & Kirkham, in press,
Experiment 2). Our procedure is shown in Figure 9-3a
to 9-3c. In both conditions, 8-month-olds observed a
sequence of four events. During the familiarization
stage (Figure 9-3a), two of these events (A and B)
always occurred together and predicted the occurrence
of another event (C) with 100% frequency. The C
event equally predicted a fourth event (D) or the AB
compound. Likewise, the D event was equally predic-
tive of C or AB. A sound effect (the same one) accom-
panied the C and D events.
After this familiarization, which lasted until infants
observed the AB→C sequence four times, infants
observed that one member of that compound (B) pre-
dicted either the C or the D event (Figure 9-3b). After
observing these data, infants were presented with the
other member of the compound (A), followed by a
blank screen (Figure 9-3c), and the sound effect that
accompanied the C and D events was played. Infants’
eye gaze was measured for an 8-second period. When
the B event did predict the C event on its own, infants
were faced with a similar backward blocking inference
concerning the A event; when B did not predict C, the
data were similar to the indirect screening-off proce-
dure used with the blicket detector.
We observed a significant interaction between
looking time to the C and D locations and experi-
mental condition. When infants were presented with
the backward blocking data, they looked more often
to the D location than the C location. The data sug-
gest that the infants did not believe that the A event
predicted the C event, even though they observed no
evidence to the contrary. When infants were pre-
sented with the indirect screening-off inference, the
pattern of looking times was reversed: Infants looked
longer to the C location than the D location.
Critically, following the A event, infants’ looking
times to the C location were different between the
two conditions, suggesting that they were not respond-
ing on the basis of a simple associative mechanism.
The present data are inconsistent with certain
models that might underlie recognizing statistical reg-
ularities. In particular, the hypothesis that children’s
reasoning is based solely on recognizing associations
does not seem to provide the proper framework 
to explain these data. Similarly, models that rely 
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
147

primarily on calculations of associative strength that
do not distinguish between forms of retrospective
inference, such as the Rescorla-Wagner (1972) equa-
tion and others based on it (e.g., Cramer et al., 2002),
seem inconsistent with the present data.
These inferential abilities are consistent with the
hypothesis that children recognize conditional proba-
bility and engage in screening-off inferences at early
ages. However, unlike the blicket detector experiments,
which showed that preschoolers’ causal inferences
148
CAUSATION AND PROBABILITY
Backward
Blocking
B
Indirect
Screening
Off
Repeats until four presentations of A & B and C
0.5
0.5
0.5
0.5
1.0
Familiarization
Disambiguation
Test
Sequence Structure
Example
C
B
D
B
A&B
C
D
A&B
C
A&B
C
D
A&B
C
A
Test
D
B
D
A
A & B
D
C
FIGURE 9-3 Schematic representation of the procedure used by Sobel and Kirkham
(in press, Experiment 2). During the familiarization phase, infants were shown that
Events A and B co-occurred and were always followed by Event C, but that no other
event predicted any other. After four occurrences of the AB →C sequence, infants
observed that the B event alone was followed by Event C (backward blocking condi-
tion) or Event D (indirect screening-off condition). After two of these sequences, infants
were shown Event A, and their looking time was measured.

could not be explained by a variety of alternative mod-
els of causal reasoning, the present data are consistent
with models of causal learning that rely on causal
strength designed with retrospective inferences in mind
(e.g., Kruschke & Blair, 2000; Wasserman & Berglan,
1998) as well as various parameter estimation models
(e.g., Allan, 1980; Cheng, 1997; Shanks, 1995).
Like the experiments on preschoolers, there is one
aspect of these data that is inconsistent with these mod-
els: Infants appear capable of making these kinds of
inferences based on a small sample of data. Estimates
of causal strength and measures of parameter estima-
tion require a relatively large amount of data to make a
meaningful estimation. In the present experiment,
infants could do so with only four trials with the com-
pound AB event and two trials with one of those events
in isolation. However, a stronger argument would be to
present infants with inferences that would be inconsis-
tent with the models listed, parallel to the method used
in previous research on preschoolers’ causal inference
(Sobel et al., 2004; Tenenbaum et al., submitted). We
are currently attempting to determine whether one of
these models best describes infants’ abilities to recog-
nize statistical regularities among events.
New Directions for Integrating Causal
Learning With Statistical Learning
In addition to attempting to map out how infants rec-
ognize co-occurrences among events, we believe
there are several other interactions between the
causal and statistical learning literature that are wor-
thy of future investigation. This list is not meant to be
exclusive or exhaustive. Rather, we wish to articulate
particular relations between the statistical learning
and causal learning literature and suggest that each
can benefit from discussions with the other.
The Problem of Multimodal Integration
The literature on infants’ multimodal integration sug-
gests that redundant information supports infants’ sta-
tistical learning abilities. Are similar effects found in
children’s causal inferences? Does redundant infor-
mation benefit children’s understanding of causal
relations?
This question has been examined indirectly by
researchers interested in relation of the role of causal
properties to conceptual development. Gopnik and
Sobel (2000) examined whether children would extend
a novel label to objects that shared the same causal
properties. They introduced children to the blicket
detector without using that description. They showed
children four objects and demonstrated each on the
blicket detector. Critically, in “conflict” trials, two iden-
tical pairs of objects were used, and one of each acti-
vated the detector. The experimenter then labeled one
of the objects that activated the detector a blicket and
asked the child to show him the other blicket. The 3-
and 4-year-olds chose between the perceptually identi-
cal and causally identical object with equal frequency.
Nazzi and Gopnik (2000) replicated this experi-
ment, but added a critical piece of redundant infor-
mation: They pointed out either the causal or the
perceptual features of each object. When an object
was placed on the detector in the causal condition,
the experimenter said, “Look, it activates the detec-
tor,” and in the perceptual condition, the experi-
menter said, “Look, this one is red.” They found that
children in the causal condition made more causal
responses on these conflict tasks than children in the
perceptual condition or those in a baseline condition.
These data suggest that children’s inferences
about category membership are influenced by redun-
dant information (the machine’s activation and the
experimenter’s language). However, there is little
research investigating what information would be
considered redundant and at what ages children are
sensitive to this information.
The Problem of Constraining Statistical
Learning
A good deal of evidence suggests that infants can rec-
ognize correlations among environmental factors and
use that information to make inferences. For example,
Younger and colleagues (Younger, 1990; Younger &
Cohen, 1983, 1986; Younger & Gottlieb, 1988) sug-
gested that, by the age of 10 months, infants recognize
correlations among object features. A question that
emerges from this discussion is whether infants are
capable of detecting any correlation or whether con-
straints must be in place to guide the child.
This question has been investigated across a number
of laboratories, and often developmental differences
emerge. Younger children appear more capable of
detecting any kind of correlation; older children only
detect correlations that have some theoretical rationale
(see, e.g., Madole & Cohen, 1995; Rakison, 2004). For
instance, Madole and Cohen found that both 14- and
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
149

18-month-olds detected the co-occurrence between the
form and function of an object part. However,
14-month-olds could also detect a correlation between
the part of an object and the function of another part of
that object. Although there are many objects in the
world in which a part’s form and function co-occur, the
latter co-occurrence has little bearing on reality. Indeed, 
18-month-olds did not detect this correlation.
There are a variety of theoretical interpretations of
these data, from a top-down, theory-driven approach
that suggests features correlate based on a set of
explanatory principles (e.g., Murphy & Medin, 1985)
to a bottom-up approach to conceptual development
motivated by detecting which correlations are critical
to category membership (e.g., Smith & Heise, 1992).
In a discussion of these data, Madole and Oakes
(1999) state that “the child’s own experience acting
on and observing objects is probably the primary insti-
gator of developmental change” (p. 289). We agree,
but how this occurs remains an open question.
The Problem of Setting Priors
Tenenbaum and Griffiths (2003; Tenenbaum et al.,
this volume; Tenenbaum et al., 2005) presented a
Bayesian algorithm that accounts for much of the data
on both preschoolers’ and adults’ causal inferences
presented here (Sobel et al., 2004; Tenenbaum et al.,
sunmitted; see also Griffiths & Tenenbaum, 2005).
An important aspect of this account is that children (at
least by age 4) might use statistical information to set
the probability of particular causal hypotheses. In these
experiments, children use the frequency with which
particular events occur to set their prior for each
hypothesis.
However, other information about the way objects
and events causally interact may be detectable from
statistical information in the environment. Griffiths
(2005) reexamined the original Gopnik et al. (2001)
screening-off experiment, in which 3- and 4-year-olds
were shown the examples of one-cause and two-cause
trials shown in Figure 9-1. Children received two of
each trial in a random order. Griffiths suggested that
the order in which these trials were presented might
have presented different information about the nature
of the blicket detector. If children observe a two-cause
trial first, in which one object’s causal power is proba-
bilistic, then children might interpret the detector as
a probabilistic device. When they then observe the
one-cause trial, in which Object B does not activate
the detector by itself once, children might interpret
Object B as a blicket that simply failed on that trial
(because, after all, it was shown to activate the detec-
tor with Object A two times subsequently).
Griffiths (2005) found that performance of 4-year-
olds demonstrated this particular order effect. Their
performance on the first one-cause trial depended on
whether it was the first test trial or if they had observed
a two-cause trial previously. If children observed a
two-cause trial before, then they were more likely not
to make a screening-off inference (i.e., to say that
Object B was a blicket). Here, 4-year-olds are not rec-
ognizing the statistical regularity among events but
rather that patterns of data suggest how new data
could be interpreted. Younger children did not show
this pattern of response. Is this developmental differ-
ence robust, or does it reflect something specific
about the blicket detector paradigm? Griffiths and
Sobel (in preparation) are currently investigating this
question systematically. But, an open question
remains: How else might the data children observe
influence which causal inferences they make?
Concluding Thoughts
What infants know about the statistic of an environ-
ment has been a seminal question in language learn-
ing for some time (Aslin et al., 1998; Jusczyk & Aslin,
1995; Saffran et al., 1996). Questions about children’s
knowledge and use of statistical regularities in the
environment have also motivated research in concep-
tual development (e.g., work by Younger and col-
leagues) as well as what infants know about object
concepts (Johnson, Amso, & Slemmer, 2003; Johnson,
Bremmer, Slater, et al., 2004; Spelke & Van de Walle,
1993). These questions have also begun to permeate
the field of theory of mind, examining what infants
know about statistical regularities in detecting inten-
tions (Baldwin, Baird, Saylor, & Clark, 2001; Brand,
Baldwin, & Ashburn, 2002) or pretending (Lillard &
Witherington, 2004).
What these literature bodies all have in common is
that they describe some type of causal knowledge that
children develop. There seem to be several places in
which children’s causal learning and their statistical
learning abilities interact and can inform each other.
Mapping out those interactions, both generally and in
specific domains of knowledge, is critical to a set of
exciting new research questions that can be asked.
150
CAUSATION AND PROBABILITY

ACKNOWLEDGMENTS
We would like to thank the
parents and children who have made all of the research
presented here possible. We would also like to thank
Alison Gopnik, Daniel Richardson, Laura Schulz, and
Jessica Sommerville for comments on this chapter.
References
Allan, L. G. (1980). A note on measurement of contin-
gency between two binary variables in judgment tasks.
Bulletin of the Psychonomic Society, 15, 147–149.
Aslin, R. N., Saffran, J. R., & Newport, E. L. (1998).
Computation of conditional probability statistics by
8-month-old infants. Psychological Science, 9,
321–324.
Bahrick, L. E., & Lickliter, R. (2000). Intersensory
redundancy guides attentional selectivity and per-
ceptual learning in infancy. Developmental
Psychology, 36, 190–201.
Bahrick, L. E., Lickliter, R., & Flom, R. (2004).
Intersensory redundancy guides the development of
selective attention, perception, and cognition in
infancy. Current Directions in Psychological
Science, 13, 99–102.
Bahrick, L. E., Netto, D., & Hernandez-Reif, M. (1998).
Intermodal perception of adult and child faces
and voices by infants. Child Development, 69,
1263–1275.
Baldwin, D. A., Baird, J. A., Saylor, M. M., & Clark, A. M.
(2001). Infants parse dynamic action. Child
Development, 72, 708–717.
Bornstein, M. H. (1985). Habituation of attention as a
measure of visual information processing in human
infants: Summary, systematization, and synthesis. In
G. Gottlieb & N. A. Krasnegor (Eds.), Measurement
of audition and vision in the first year of postnatal
life: A methodological overview (pp. 253–300).
Norwood, NJ: Ablex.
Brand, R. J., Baldwin, D. A., & Ashburn, L. J. (2002).
Evidence for “motionese”: Modifications in moth-
ers’ infant-directed action. Developmental Science,
5, 72–83.
Bullock, M., Gelman, R., & Baillargeon, R. (1982). The
development of causal reasoning. In W. J.
Friedman (Ed.), The developmental psychology of
time (pp. 209–254). New York: Academic Press.
Cheng, P. W. (1997). From covariation to causation: A
causal power theory. Psychological Review, 104,
367–405.
Cheng, P. W. (2000). Causality in the mind: Estimating
contextual and conjunctive power. In F. Keil & R. A.
Wilson 
(Eds.), 
Explanation 
and 
cognition
(pp. 227–253). Cambridge, MA: MIT Press.
Christiansen, M. H., Allen, J., & Seidenberg, M. S.
(1998). Learning to segment speech using multiple
cues: A connectionist model. Language and
Cognitive Processes, 13, 221–268.
Christiansen, M. H., & Dale, R. A. C. (2001). Integrating
distributional, prosodic and phonological informa-
tion in a connectionist model of language acquisi-
tion. In Proceedings of the 23rd Annual Conference
of the Cognitive Science Society. Mahwah, NJ:
Erlbaum.
Cramer, R. E., Weiss, R. F., Williams, R., Reid, S., Nieri, L.,
& Manning-Ryan, B. (2002). Human agency and asso-
ciative learning: Pavlovian principles govern social
process in causal relationship detection. Quarterly
Journal of Experimental Psychology: Comparative and
Physiological Psychology, 55B, 241–266.
de Sa, V. R., & Ballard, D. H. (1998). Category learning
through multimodality sensing. Neural Computa-
tion, 10, 1097–1117.
Dickinson, A. (2001). Causal learning: Association versus
computation. Current Directions in Psychological
Science, 10, 127–132.
Fiser, J., & Aslin, R. N. (2003). Statistical learning of new
visual feature combinations by infants. Proceedings
of the National Academy of Sciences of the United
States of America, 99, 15822–15826.
Gelman, S. A., & Wellman, H. M. (1991). Insides and
essence: Early understandings of the non-obvious.
Cognition, 38, 213–244.
Glymour, C. (2001). The mind’s arrow. Cambridge, MA:
MIT Press.
Gomez, R. L., & Gerken, L. (1999). Artificial grammar
learning by 1-year-olds leads to specific and abstract
knowledge. Cognition, 70, 109–135.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 1–30.
Gopnik, A., & Meltzoff, A. N. (1992). Categorization
and naming: Basic-level sorting in 18-month-olds
and its relation to language. Child Development,
63, 1091–1103.
Gopnik, A., & Sobel, D. M. (2000). Detecting blickets:
How young children use information about causal
properties in categorization and induction. Child
Development, 71, 1205–1222.
Gopnik, A., Sobel, D. M., Schulz, L., & Glymour, C.
(2001). Causal learning mechanisms in very young
children: 2-, 3-, and 4-year-olds infer causal rela-
tions from patterns of variation and co-variation.
Developmental Psychology, 37, 620–629.
Gopnik, A., & Wellman, H. M. (1994). The theory theory.
In L. Hirschfield & S. Gelman (Eds.), Mapping the
mind: Domain specificity in cognition and culture
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
151

(pp. 257–293). New York: Cambridge University
Press.
Griffiths, T. L., & Sobel, D. M. (in preparation). Learning
about probabilistic and deterministic causal systems.
Brown University.
Griffiths, T. L. (2005). Causes, coincidences, and theories.
Unpublished doctoral dissertation, Stanford
University, Stanford, CA.
Griffiths, T. L., & Tenenbaum, J. B. (2005) Structure
and strength in causal induction, Cognitive
Psychology, 51, 354–384.
Haith, M. M. (1993). Future-oriented processes in
infancy: The case of visual expectations. In C.
Granrud (Ed.), Visual perception and cognition in
infancy (pp. 235–264). Hillsdale, NJ: Erlbaum.
Harris, P. L., German, T., & Mills, P. (1996). Children’s
use of counterfactual thinking in causal reasoning.
Cognition, 61, 233–259.
Hauser, M. D., Newport, E. L., & Aslin, R. N. (2001).
Segmentation of the speech stream in a non-
human primate: Statistical learning in cotton-top
tamarins. Cognition, 78, B53–B64.
Hespos, S. J., & Baillargeon, R. (2001). Infants’ knowledge
about occlusion and containment events: A surpris-
ing discrepancy. Psychological Science, 12, 141–147.
Jenkins, H. M., & Ward, W. C. (1965). Judgment of
contingency between responses and outcomes.
Psychological Monographs: General and Applied,
79, 17.
Johnson, S. P., Amso, D., & Slemmer, J. A. (2003).
Development of object concepts in infancy:
Evidence for early learning in an eye tracking para-
digm. Proceedings of the National Academy of
Sciences, 100, 10568–10573.
Johnson, S. P., Bremner, J. G., Slater, A., Mason, U.,
Foster, & Cheshire, A. (2003). Infants’ perception of
object trajectories. Child Development, 74, 94–108.
Jusczyk, P. W., & Aslin, R. N. (1995). Infants’ detection
of sound patterns of words in fluent speech.
Cognitive Psychology, 29, 1–23.
Kamin, L. J. (1969). Predictability, surprise, attention,
and conditioning. In B. A. Campbell, & R. M.
Church (Eds.), Punishment and aversive behavior
(pp. 279–296). New York: Appleton-Century-Crofts.
Kirkham, N. Z., & Johnson, S. P. (2006). Moving sounds:
The role of inter-modal perception in solving the
problem of occlusion. Manuscript submitted for
publication.
Kirkham, N. Z., Slemmer, J. A., & Johnson, S. P. (2002).
Visual statistical learning in infancy. Cognition, 83,
B35–B42.
Kirkham, N. Z., Slemmer, J. A., & Johnson, S. P. (2006).
Location, location, location: Development of spa-
tiotemporal sequence learning in infancy. Manuscript
submitted for publication.
Kruschke, J. K., & Blair, N. J. (2000). Blocking and back-
ward blocking involve learned inattention.
Psychonomic Bulletin and Review, 7, 636–645.
Kuhl, P. K., & Meltzoff, A. N. (1982). The bimodal percep-
tion of speech in infancy. Science, 218, 1138–1140.
Lagnado, D. A., & Sloman, S. (2004). The advantage of
timely intervention. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 30,
856–876.
Leslie, A. M., & Keeble, S. (1987). Do 6-month-old
infants perceive causality? Cognition, 25, 265–288.
Lillard, A. S., & Witherington, D. C. (2004). Mothers’
behavior modifications during pretense and their
possible signal value for toddlers. Developmental
Psychology, 40, 95–113.
Madole, K. L., & Cohen, L. B. (1995). The role of object
parts in infants’ attention to form-function correla-
tions. Developmental Psychology, 31, 637–648.
Madole, K. L., & Oakes, L. M. (1999). Making sense of
infant categorization: Stable processes and changing
representations. Developmental Review, 19, 263–296.
Meltzoff, A. N., Gopnik, A., & Repacholi, B. M. (1999).
Toddlers’ understanding of intentions, desires and
emotions: Explorations of the dark ages. In P. D.
Zelazo, J. W. Astington, & D. R. Olson (Eds.),
Developing theories of intentions (pp. 17–46).
Mahwah, NJ: Erlbaum.
Michotte, A. (1963). The perception of causality. Oxford,
England: Basic Books.
Morrongiello, B. A., Fenwick, K. D., & Chance, G.
(1998). Cross modal learning in newborn infants:
Inferences about properties of auditory-visual
events. Infant Behavior and Development, 21,
543–554.
Murphy, G. L., & Medin, D. L. (1985). The role of the-
ories in conceptual coherence. Psychological
Review, 92, 289–316.
Nazzi, T., & Gopnik, A. (2000). A shift in children’s use
of perceptual and causal cues to categorization.
Developmental Science, 3, 389–396.
Needham, A., & Baillargeon, R. (1993). Intuitions about
support in 4.5-month-old infants. Cognition, 47,
121–148.
Pearl, J. (2000). Causality. New York: Oxford University
Press.
Piaget, J. (1929). The child’s conception of the world.
London: Routledge & Kegan Paul.
Rakison, D. H. (2004). Infants’ sensitivity to correlations
between static and dynamic features in a category
context. Journal of Experimental Child Psychology,
89, 1–30.
Reichenbach, H. (1956). The direction of time. Berkeley,
CA: University of California Press.
Rescorla, R. A., & Wagner, A. R. (1972). A theory of
Pavlovian conditioning: Variations in the effectiveness
152
CAUSATION AND PROBABILITY

of reinforcement and nonreinforcement. In A. H.
Black & W. F. Prokasy (Eds.), Classical conditioning
II: Current theory and research (pp. 64–99). New York:
Appleton-Century-Crofts.
Richardson, D. C., & Kirkham, N. Z. (2004). Multi-
modal events and moving locations: Eye movements
of adults and 6-month-olds reveal dynamic spatial
indexing. Journal of Experimental Psychology:
General, 133, 46–62.
Saffran, J. R., Aslin, R. N., & Newport, E. L. (1996).
Statistical learning by 8-month-old infants. Science,
274, 1926–1928.
Saffran, J. R., Johnson, E. K., Aslin, R. N., & Newport, E. L.
(1999). Statistical learning of tone sequences by
human infants and adults. Cognition, 70, 27–52.
Schult, C. A., & Wellman, H. M. (1997). Explaining
human movements and actions: Children’s under-
standing of the limits of psychological explanation.
Cognition, 62, 291–324.
Schulz, L. E., & Gopnik, A. (2004). Causal learning across
domains. Developmental Psychology, 40, 162–176.
Shanks, D. R. (1985). Forward and backward blocking in
human contingency judgment. Quarterly Journal of
Experimental Psychology, 37b, 1–21.
Shanks, D. R. (1995). Is human learning rational?
Quarterly Journal of Experimental Psychology:
Human Experimental Psychology, 48, 257–279.
Shanks, D. R., & Dickinson, A. (1987). Associative
accounts of causality judgment. In G. H. Bower
(Ed.), The psychology of learning and motivation:
Advances in research and theory (Vol. 21, pp.
229–261). San Diego, CA: Academic Press.
Shultz, T. R. (1982). Rules of causal attribution.
Monographs of the Society for Research in Child
Development, 47, 1–51.
Smith, L. B., & Heise, D. (1992). Perceptual similarity
and conceptual structure. In B. Burns (Ed.),
Percepts, Concepts, and Categories: Representation
and Processing of Information (pp. 233–272).
Oxford, England: North-Holland.
Sobel, D. M. (2004). Exploring the coherence of young chil-
dren’s explanatory abilities: Evidence from generating
counterfactuals. British Journal of Developmental
Psychology, 22, 37–58.
Sobel, D. M., & Kirkham, N. Z. (in press). Blickets and
babies: The development of causal reasoning in
toddlers and infants. Developmental Psychology.
Sobel, D. M., Tenenbaum, J. B., & Gopnik, A. (2004).
Children’s causal inferences from indirect evi-
dence: Backwards blocking and Bayesian reasoning
in preschoolers. Cognitive Science, 28, 303–333.
Spelke, E. S. (1979). Perceiving bimodally specified
events in infancy. Developmental Psychology, 15,
626–636.
Spelke, E. S. (1981). The infant’s acquisition of knowl-
edge of bimodally specified events. Journal of
Experimental Child Psychology, 31, 279–299.
Spelke, E. S., Breinlinger, K., Macomber, J., & Jacobson, K.
(1992). Origins of knowledge. Psychological Review,
99, 605–632.
Spelke, E. S., & Van de Walle, G. (1993). Perceiving and
reasoning about objects: Insights from infants. In 
N. Eilan, R. A. McCarthy, & B. Brewer (Eds.), Spatial
representation: Problems in philosophy and psychology
(pp. 132–161). Oxford, England: Blackwell.
Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-
based causal inference. Proceedings of the 14th
Annual Conference on the Advances in Neural
Information Processing Systems. Vancouver, CA.
Tenenbaum, J. B., Sobel, D. M., Griffiths, T. L., & 
Gopnik, A. (2006). Bayesian inference in causal learn-
ing from ambiguous data: Evidence from adults and
children. Manuscript submitted for publication,
Brown University.
Uzgiris, I. C., & Hunt, J. M. V. (1975). Assessment in
infancy: Ordinal scales of psychological develop-
ment. Oxford, England: University of Illinois
Press.
Waldmann, M. R., & Hagmayer, Y. (2001). Estimating
causal strength: The role of structural knowledge
and processing effort. Cognition, 82, 27–58.
Walker, A. S. (1982). Intermodal perception of expressive
behaviors 
by 
human 
infants. 
Journal 
of
Experimental Child Psychology, 33, 514–535.
Wasserman, E. A., & Berglan, L. R. (1998). Backward
blocking and recovery from overshadowing in human
causal judgment: The role of within-compound asso-
ciations. 
Quarterly 
Journal 
of 
Experimental
Psychology: 
Comparative 
and 
Physiological
Psychology, 51, 121–138.
Wentworth, N., Haith, M. M., & Hood, R. (2002).
Spatiotemporal 
regularity 
and 
interevent
contingencies as information for infants’ visual
expectations. Infancy, 3, 303–321.
Younger, B. A. (1990). Infants’ detection of correlations
among feature categories. Child Development, 61,
614–620.
Younger, B. A., & Cohen, L. B. (1983). Infant perception
of 
correlations 
among 
attributes. 
Child
Development, 54, 858–869.
Younger, B. A., & Cohen, L. B. (1986). Developmental
change in infants’ perception of correlations among
attributes. Child Development, 57, 803–815.
Younger, B. A., & Gottlieb, S. (1988). Development of
categorization skills: Changes in the nature or struc-
ture of infant form categories? Developmental
Psychology, 24, 611–619.
INTERACTIONS BETWEEN CAUSAL AND STATISTICAL LEARNING
153

154
10
Beyond Covariation
Cues to Causal Structure
David A. Lagnado, Michael R. Waldmann, York Hagmayer, & Steven A. Sloman
effects, and build larger webs of causal links to capture
the complexities of physical and social systems?
Structure Versus Strength
When investigating causality, a basic distinction can
be made between structure and strength. The former
concerns the qualitative causal relations that hold
between variables, for example, whether smoking
causes lung cancer or aspirin cures headaches. The
latter concerns the quantitative aspects of these rela-
tions: To what degree does smoking cause lung can-
cer or aspirin alleviate headaches? This distinction is
captured more formally in the causal Bayes net frame-
work. The structure of a set of variables is represented
by the graph of Figure 10-1, with the strength of these
links captured in the parameterization of the graph
(see introductory chapter).
Conceptually, the question of structure is more basic
than that of strength—one needs to know or assume the
Introduction
Imagine a person with no causal knowledge or
concept of cause and effect. That person would be
like one of Plato’s cave dwellers—destined to watch
the shifting shadows of sense experience but knowing
nothing about the reality that generates these patterns.
Such ignorance would undermine that person’s most
fundamental cognitive abilities: to predict, control,
and explain the world around them. Fortunately, we
are not trapped in such a cave; we are able to interact
with the world and learn about its generative struc-
ture. How is this possible?
The general problem, tackled by philosophers and
psychologists alike, is how people infer causality from
their rich and multifarious experiences of the world,
not only the immediate causality of collisions between
objects, but also the less transparent causation of illness
by disease, of birth through conception, of kingdoms
won through battle. What are the general principles
that the mind invokes to identify such causes and

BEYOND COVARIATION
155
existence of a link before one can estimate its strength.
This is reflected in many of the discovery algorithms
used in artificial intelligence, for which there is an ini-
tial structure learning step prior to estimating the
parameters of a graph (see Neapolitan, 2004). A natural
conjecture is that this priority of structure over strength
is likewise marked in human cognition (Pearl, 1988;
Tenenbaum, Griffiths, & Niyogi, 2001, chapter 19,
this volume;
Waldmann, 1996; Waldmann &
Martignon, 1998).
This idea receives intuitive support. We often have
knowledge about what causes what but little idea about
the strength of these relations. For example, most of us
believe that smoking causes cancer, that exercise pro-
motes health, that alcohol inhibits speed of reaction,
but we know little about the strengths of these relations.
Likewise, in the case of learning, we seek to establish
whether causal relations exist before trying to assess
how strong they are. For example, in a medical scare in
the United Kingdom, research focused on whether the
measles-mumps-rubella vaccine causes autism, not on
the degree of this relation. Indeed, the lack of evidence
in support of the link has preempted studies into how
strong this relation might be.
The idea that causal cognition is grounded in
qualitative relations has also influenced the develop-
ment of computational models of causal inference.
To motivate his structural account, Pearl (2000)
argued that people encode stable aspects of their
experiences in terms of qualitative causal relations.
This inverts the traditional view that judgments about
probabilistic relations are primary, and that causal
relations are derived from them. Rather, “if condi-
tional independence judgments are by-products of
stored causal relationships then tapping and repre-
senting those relationships directly would be a more
natural and more reliable way of expressing what we
know or believe about the world” (p. 22).
Despite the apparent primacy of structure over
strength, most research in causal learning has focused on
how people estimate the strength of separate links. In a
typical experiment, variables are presorted as potential
causes and effects, and participants are asked to estimate
the strength of these relations (e.g., Cheng, 1997;
Shanks, 2004). This approach has generated a lot of data
about how people use contingency information to esti-
mate causal strength and how these judgments are mod-
ulated by response format, but the approach does not
consider the question of how people learn about causal
structure. Thus, it fails to address an important, arguably
the most fundamental, part of the learning process.
This neglect has had various repercussions. It has
led to an overestimation of the importance of statisti-
cal data at the expense of other key cues in causal
learning. For example, associative theories focus on
learning mechanisms that encode the strength of
covariation between cues and outcomes (e.g., Shanks
& Dickinson, 1987), but they are insensitive to the
important structural distinction between causes and
effects. As a consequence, they are incapable of
distinguishing between associations that link spurious
relations (e.g., barometer and storm) from true causal
relations (atmospheric pressure and storm). More gen-
erally, these models are incapable of distinguishing
Exposure
to H pylori
Bacterial
Infection
Peptic Ulcer
Aspirin
FIGURE 10-1 A simple Bayesian network representing the
potential causes of peptic ulcers. H. pylori, Helicobacter pylori.

156
CAUSATION AND PROBABILITY
between direct and indirect causal relations or covari-
ations that are generated by hidden causal events
(Waldmann, 1996; Waldmann & Hagmayer, 2005).
Another shortcoming of this focus on strength is
that it restricts attention to a small subset of causal
structures (mainly common-effect models). For
example, PowerPC theory (Cheng, 1997) focuses on
the assessment of causal strength based on covariation
information. Although the main focus of the empiri-
cal studies lies in how people estimate causal power
(see Buehner, Cheng, & Clifford, 2003), the theory
clearly states that these power estimates are only valid
under the assumption that the causal effect is gener-
ated by a common-effect structure with specific
characteristics. The question of how people induce
these models, which are a prerequisite for the strength
calculations, is neglected in this research. Moreover,
people routinely deal with other complex structures
(e.g., common-cause and chain models). The ques-
tions of how people learn such structures and how
they combine simple structures into more complex
ones are clearly crucial to a proper understanding of
causal cognition.
Furthermore, the focus on strength fails to give
due weight to the importance of intervention (rather
than passive observation) and to the temporal order
of experienced events (over and above their
temporal contiguity). Both of these factors are pri-
marily cues to structure rather than strength, and
there is growing evidence that people readily use
them (Gopnik et al., 2004, chapter 9, this volume;
Lagnado & Sloman, 2004, 2006; Kirkham, chapter 9,
this volume; Steyvers, Tenenbaum, Wagenmakers,
& Blum, 2003; Tenenbaum, Griffiths, & Niyogi,
chapter 19, this volume; Waldmann, 1996).
Even the traditional studies on strength estimation
are open to reevaluation in the light of the structure/
strength distinction. Tenenbaum and Griffiths (2001)
contend that participants in these studies are actually
assessing the degree to which the evidence supports
the existence of a causal link rather than the strength
of that link. More generally, they propose that people
adopt a two-step procedure to learn about elemental
causal relations, first inferring structure and then esti-
mating strength. Although decisive experiments have
yet to be run, Griffiths and Tenenbaum (2005) sup-
port this claim through the reinterpretation of previ-
ous data sets and some novel experimental results.
The main moral to be drawn from these considera-
tions is not that strength estimation has no place in
causal learning, but that the role of structural inference
has been neglected. By recognizing the central role it
plays in both representation and learning, we can attain
a clearer perspective on the nature of causal cognition.
Causal Model Theory
Causal model theory was a relatively early, qualitative
attempt to capture the distinction between structure
and strength (see Hagmayer & Waldmann, 2002;
Waldmann, 1996, 2000, 2001; Waldmann &
Hagmayer, 2001; Waldmann & Holyoak, 1992;
Waldmann, Holyoak, & Fratianne, 1995; Waldmann &
Martignon, 1998; see also Rehder, 2003a, 2003b;
Tenenbaum, Griffiths, & Niyogi, chapter 19, this
volume). According to this proposal, causal induction
is guided by top-down assumptions about the structure
of causal models. These hypothetical causal models
guide the processing of the learning input.
The basic idea behind this approach is that we
rarely encounter a causal learning situation in which
we do not have some intuitions about basic causal fea-
tures, such as whether an event is a potential cause or
effect. If, for example, the task is to press a button and
observe a light (e.g., Wasserman, Chatlosh, &
Neunaber, 1983), we may not know whether these
events are causally related, but we assume that the
button is a potential cause, and the light is a potential
effect. Once a hypothetical causal model is in place,
we can start estimating causal strength by observing
covariation information. The way covariation esti-
mates are computed and interpreted is dependent on
the assumed causal model (Hagmayer & Waldmann,
2002; Waldmann & Hagmayer, 2001).
The distinction between causal structure and
causal strength raises the question of how assumptions
about causal models are generated. Our working
hypothesis is that people use a number of nonstatisti-
cal cues to generate hypothetical causal models. We
do not rule out the possibility that people occasionally
induce causal structure on the basis of covariation
information alone, but this seems rare in the world in
which we live. Whenever people do not have clear
assumptions about causal structure, causal reasoning
easily falls prey to cognitive biases, such as confusing
spurious with causal relations. In contrast, whenever
people have hypothetical knowledge about causal
structure, they show a remarkable competence to
tune this knowledge to the statistical relations in the
learning input and use this knowledge for predictions,
diagnoses, and for planning actions.

BEYOND COVARIATION
157
Cues to Causal Structure
People are active agents immersed in a dynamic
physical world. Not only do they experience events in
a diversity of ways, but also they experience a variety
of relations between these events. Perhaps most signif-
icant, they can also interact with the world, thereby
creating new relations and disrupting old ones. The
richness of these experiences of the world affords peo-
ple a variety of cues to its causal structure. Here is a
partial list:
 Statistical relations
 Temporal order
 Intervention
 Prior knowledge
Following Einhorn and Hogarth (1986), we note that
these cues are fallible, sometimes redundant (sepa-
rate cues support the same conclusion), and at other
times inconsistent (separate cues suggest opposing
conclusions). These cues can be combined to con-
struct and update causal models. For example, typical
cases of intervention combine multiple cues—
proximity in space and time, temporal order, and
covariation. This synergy explains the power of inter-
vention as a route to causal knowledge. Cues are gen-
erally strongly correlated in natural environments;
causes tend to be nearby, prior to, and correlated with
their effects.
Statistical Covariation
Hume’s (1748/1975) analysis of causation has set the
agenda for most contemporary theories of learning.
These theories assume that causation cannot be per-
ceived directly and suppose that people infer it from
the statistical patterns in what they can observe. The
key idea is that people are exposed to patterns of data,
such as the occurrence or nonoccurrence of patterns
of events, the presence or absence of features, or
more generally, the values of variables. From this
body of data, they extract certain statistical relations
on which they base their causal judgments. There are
various statistical relations that have been implicated
in this process (Cheng, 1997; Glymour, 2001;
Shanks, 2004). One of the simplest is the covariation
between two events. For example, smoking increases
the probability of heart disease. The existence of a
stable covariation between two events A and B is a
good indication that some underlying causal relation
exists, but by itself does not reveal whether A causes
B, B causes A, or both are effects of a common cause.
This highlights the incompleteness of any model 
of structure learning based solely on covariation
detection.
The advent of Bayesian networks provides a more
general framework to represent the statistical rela-
tions present in a body of observed data (Pearl, 1988).
As well as representing straightforward (uncondi-
tional) relations between variables, they also repre-
sent conditional relations. In particular, they
represent relations of conditional independence,
which holds whenever an intermediate variable (or
set of variables) renders two other variables (or sets of
variables) probabilistically independent. For exam-
ple, the unconditional dependence between intra-
venous drug usage and acquired immunodeficiency
syndrome (AIDS) is rendered independent condi-
tional on human immunodeficiency virus (HIV) sta-
tus. In other words, the probability that someone
develops AIDS, given that they are HIV positive, is
not affected by whether they contracted the virus
through drug use (assuming that drug usage does not
affect the passage from HIV infection to AIDS).
Establishing the conditional independencies that
hold in a body of data is a critical step in construct-
ing an appropriate Bayesian network.
Work in statistics and artificial intelligence forges
a crucial link between statistical data and causal
structure (Pearl, 2000; Spirtes, Glymour, & Schienes,
1993). Given certain assumptions (e.g., the causal
Markov condition and faithfulness; see intro-
ductory chapter, and Woodward, chapter 1, this
volume), they detail the patterns of dependencies that
are associated with a given causal structure and, con-
versely, the causal structures that can be inferred from
a given pattern of dependencies. Based on this analy-
sis, a range of algorithms have been developed that
can infer causal structure from large databases of sta-
tistical information. The success of this computa-
tional work has prompted some to model human
causal learning along similar lines (Glymour, 2001;
see the section Computational Models of Learning).
Despite the sophistication of Bayesian networks, it
is generally recognized that statistical data alone is
insufficient for inferring a unique causal model.
Even with the notion of conditional independence, a
particular body of correlational data will typically be
associated with several possible causal structures
(termed Markov equivalent) rather than a unique
model. For example, if it is known that A, B, and C
are all correlated (unconditionally dependent), and

158
CAUSATION AND PROBABILITY
that A is conditionally independent of C given B, then
there are three possible causal structures compatible
with these relations (A →B →C, A ←B →C,
A ←B ←C). To narrow these possibilities to just one
requires some additional information. For instance, if
one also knows that A occurs before B, then A →B →C
is the only possible model.
This sets a theoretical limit on what can be
inferred through correlation alone. At best, statistical
cues can narrow the set of possible models to those
that are Markov equivalent. There are also practical
limitations. Even with just three variables, there are a
large number of correlations and conditional correla-
tions to compute to determine viable causal models.
Each of these relations requires a sizable amount of
data before their individual reliability is established.
Thus, inferring possible causal models in a purely
data-driven fashion involves a significant computa-
tional load. Although this may be manageable by a
powerful computer, it is less likely to be achievable by
humans with limited processing and memory
resources.
Indeed, current evidence suggests that people are
limited in their ability to learn structure from cor-
relations alone, even to Markov equivalence. For
example, Lagnado and Sloman (2004) presented sub-
jects with probabilistic data generated by a three-
variable chain A →B →C. In the absence of other
cues (intervention, time order, etc.), most subjects
failed to learn the correct structure or its Markov
equivalents. This result holds across several different
learning paradigms (Lagnado & Sloman, 2006; Sobel
& Kushnir, 2006; Steyvers et al., 2003; Danks &
McKenzie, 2006).
What people seem to find most difficult is estab-
lishing the appropriate conditional independence
relations between sets of variables and using this as
a basis for inferences about causal structure. This is
tricky because learners must track the concurrent
changes in three different variables. They must deter-
mine whether the correlation between any pair of
these variables is itself dependent on a third variable.
For example, in Lagnado and Sloman (2004), partici-
pants had to figure out that (a) two different chemicals
covaried with a given effect, and (b) one of these
chemicals was probabilistically independent of the
effect conditional on the presence or absence of the
other chemical. It is not surprising that most partici-
pants failed to work this out and settled for a simpler
(but incorrect) causal model.
The experiments of Steyvers et al. (2003) also
demonstrated the difficulty of inducing structure from
covariation data. In their experiments, learners
observed data about three mind-reading aliens. The
task was to find which of the three mind readers can
send messages (i.e., is a cause) and which can receive
messages (i.e., is an effect). Generally, performance
was better than chance but was still poor. For exam-
ple, in Experiment 3, in which learners could select
multiple models compatible with the data, only 20%
of the choices were correct. This number may even
overstate what people can do with covariation alone.
In the experiments, learners were helped by the fact
that the possible models were shown to them prior to
learning. Thus, their learning was not purely data
driven but was possibly aided by top-down constraints
on possible models.
Moreover, the parameters of the models were
selected to make the statistical differences between the
models quite salient. For example, the pattern that all
three mind readers had the same thought was very
likely when the common-cause model applied but was
extremely unlikely under a common-effect model.
Similarly, the pattern that only two aliens had the same
thought was very likely under the common-effect
model hypothesis but unlikely with chains or common-
cause models. Under the assumption that people asso-
ciate these different prototypic patterns (e.g., three
mind readers with identical thoughts) with different
causal structures (e.g., common-cause model), some
participants might have solved the task by noticing the
prevalence of one of the prototypic patterns. Additional
cues further aided induction. As in Lagnado and
Sloman (2004), performance improved when partici-
pants were given the opportunity to add an additional
cue, interventions (see also Sobel & Kushnir, 2006;
and the section Intervention).
In sum, there is little evidence that people can
compute the conditional dependencies necessary for
inferring causal structure from statistical data alone
without any further structural constraints. In contrast,
when people have some prior intuitions about the
structure of the causal model with which they are
dealing, learning data can be used to estimate para-
meters within the hypothetical model or to select
among alternative models (see also Waldmann, 1996;
Waldmann & Hagmayer, 2001). Thus, the empirical
evidence collected so far suggests that cues other than
statistical covariation take precedence in the induction
of structure before statistical patterns can meaningfully

BEYOND COVARIATION
159
be processed. In the next section, we show that the
temporal order cue can override statistical covariation
as a cue to causal structure.
Temporal Order
The temporal order in which events occur provides a
fundamental cue to causal structure. Causes occur
before (or possibly simultaneously with) their effects,
so if one knows that Event A occurs after Event B, one
can be sure that A is not a cause of B. However,
although the temporal order of events can be used to
rule out potential causes, it does not provide a suffi-
cient cue to rule them in. Just because events of Type
B reliably follow events of Type A, it does not follow
that A causes B. Their regular succession may be
explained by a common cause C (e.g., heavy drinking
first causes euphoria and only later causes sickness).
Thus, the temporal order of events is an imperfect cue
to causal structure. This is compounded by the fact
that we often do not have direct knowledge of the
actual temporal order of events but are restricted to
inferring that order from the order in which we
experience (receive information about) these events.
In many situations, the experienced order will reflect
the true temporal order, but this is not guaranteed.
Sometimes, one learns about effects prior to learning
about their causes. For example, the presence of a
disease is typically learned about after experiencing
the symptoms that it gives rise to (see the section on
prior knowledge for further examples).
Despite its fallibility, temporal order will often
yield a good cue to causal structure, especially if it is
combined with other cues. Thus, if you know that A
and B covary and that they do not have a common
cause, then discovering that A occurs before B tells
you that A causes B and not vice versa. It is not surpris-
ing, therefore, that animals and humans readily use
temporal order as a guide to causality. Most previous
research, however, has focused on how the temporal
delay between events influences judgments of causal
strength and paid less attention to how temporal order
affects judgments of causal structure. The main find-
ings have been that judged causal strength decreases
with increased temporal delays (Shanks, Pearson, &
Dickinson, 1989) unless people have a good reason to
expect a delay (e.g., through prior instructions or
knowledge; see Buehner & May, 2002). This fits with
the default assumption that the closer two events are
in time, the more likely they are to be causally related.
In the absence of other information, this will be a
useful guiding heuristic.
Temporal order versus statistical data
Both temporal order and covariation information are
typically available when people learn about a causal
system. These sources can combine to give strong
evidence in favor of a specific causal relation, and
most psychological models of causal learning take
these sources as basic inputs to the inference process.
However, the two sources can also conflict. For
example, consider a causal model in which C is a
common cause of both A and B, and where B always
occurs after A. The temporal order cue in this case is
misleading as it suggests that A is a cause of B. This
misattribution will be particularly compelling if the
learner is unaware of C. However, consider a learner
who also knows about C. With sufficient exposure
to the patterns of correlation of A, B, and C, they
would have enough information to learn that A is
probabilistically independent of B given C.
Together with the knowledge that C occurs before
both A and B, the learner can infer that there is no
causal link from A to B (without such temporal
knowledge about C, they can only infer that A is not
a direct cause of B because the true model might be
a chain A →C →B).
In this situation, the learner has two conflicting
sources of evidence about the causal relation between
A and B: a temporal order cue that suggests that A
causes B and (conditional) correlational information
that there is no causal link from A to B. Here, a
learner must disregard the temporal order informa-
tion and base structural inference on the statistical
data. However, it is not clear how easy it is for people
to suppress the temporal order-based inference, espe-
cially when the statistical information is sparse.
Indeed, in two psychological studies, Lagnado and
Sloman (2004, 2006) show that people let the tempo-
ral order cue override contrary statistical data.
To explore the impact of temporal order cues on
people’s judgments about causal structure, Lagnado and
Sloman (2006) constructed an experimental learning
environment in which subjects used both temporal and
statistical cues to infer causal structure. The underlying
design was inspired by the fact that viruses (electronic
or otherwise) present a clear example of how the tem-
poral order in which information is received need
not reflect the causal order in which events happen.

160
CAUSATION AND PROBABILITY
This is because there can be considerable variability
in the time of transmission of a virus from computer
to computer, as well as variability in the time it takes
for an infection to reveal itself. Indeed, it is possible
that a virus is received and transmitted by a computer
before it reveals itself on that computer. For example,
imagine that your office mate’s computer becomes
infected with an e-mail virus that crashes his com-
puter. Twenty minutes later, your computer also
crashes. A natural reaction is to suppose that his com-
puter transmitted the virus to you, but it is possible
that your computer received the virus first and then
transmitted it to your office mate. It just so happened
that the virus subverted his computer more quickly
than yours. In this case, the temporal order in which
the virus manifests itself (by crashing the computer) is
not a reliable cue to the order in which the comput-
ers were infected.
In such situations, then, the order in which infor-
mation is received about underlying events (e.g., the
order in which viruses manifest themselves on com-
puters in a network) does not necessarily mirror the
underlying causal order (e.g., the order in which
computers are infected). Temporal order is a fallible
cue to causal structure. Moreover, there might be
statistical information (e.g., the patterns of correlation
between the manifestations of the viruses) that does
provide a veridical cue to the underlying structure.
How do people combine these two sources of informa-
tion, and what do they do when these sources conflict?
In Lagnado and Sloman’s (2006) experiment,
participants had to learn about the connections in a
simple computer network. To do so, they sent test
messages from a master computer to one of four com-
puters in a network and then observed which of the
other computers also received the messages. They
were able to send 100 test messages before being
asked about the structure of the network. Participants
completed four tasks, each with a different network of
computers. They were instructed that there would
sometimes be delays in the time taken for the mes-
sages to be transmitted from computer to computer.
They were also told that the connections, where they
existed, only worked 80% of the time. (In fact, the
probabilistic nature of the connections is essential if
the structure of the network is to be learnable from
correlational information. With a deterministic net-
work, all the connected computers would covary
perfectly, so it would be impossible to figure out the
relevant conditional independencies.)
Unknown to participants, the networks in each
problem had the same underlying structure and only
differed in the temporal order in which the com-
puters displayed their messages. The four different
temporal orderings are shown in Figure 10-2 along
with the links endorsed by the participants in the test
phase. When the temporal ordering reflected the
underlying network structure, the correct model was
generally inferred (see lower right panel in Figure 10-2).
When the information was presented simultaneously,
learners did less well (adding incorrect links) but still
tended to capture the main links. When the temporal
ordering conflicted with the underlying structure,
participants erroneously added links that fitted with
the temporal order but did not correspond to the
underlying structure.
In sum, people allowed the temporal ordering to
guide their structural inferences, even when this con-
flicted with the structure implied by the correlational
information. However, this did not amount to a total
disregard of the correlational information. For exam-
ple, in the problem with temporal ordering ABDC
(top right panel in Figure 10-2), participants erro-
neously endorsed the link from D to C (as suggested
by the temporal order) but also correctly added the
link from B to C. We hypothesize that they first used
C
D
B
A
Simultaneous
C
D
B
A
Time order ABDC
C
D
B
A
Time order ADCB
C
D
B
A
Time order AB[CD]
FIGURE 10-2 Model choices for the four temporal
conditions in Lagnado and Sloman (2006). The 
correct model is that chosen in the lower right panel.
Note that thick arrows represent links endorsed by
75%–100% of participants, thin arrows by 50–75% of
participants.

BEYOND COVARIATION
161
the temporal ordering to set up an initial model
(A →B →D →C). This model would be confirmed
by most of the test trials. However, occasionally they
saw a test pattern that contradicted this model (A, B,
not-D, C). To accommodate this new evidence, they
added a link from B to C but did not remove the
redundant link from D to C because this still fit with
the temporal ordering.
Interpreted within the causal model framework, this
study shows that people use both temporal order and
correlational cues to infer causal structure. It also sug-
gests that they construct an initial model on the basis of
the temporal ordering (when available) and then revise
this model in the light of the covariational information.
However, because of the persisting influence of the
temporal order cue, these revisions may not be optimal.
Although the reported study highlights how people
can be misled by an inappropriate temporal ordering,
in many contexts the temporal cue will reliably
indicate the correct causal structure. As with other
mental heuristics, its fallibility does not undermine its
effectiveness in most naturalistic learning situations.
It also works best when combined with other cues. In
the next section, we examine how it combines with
interventions.
Intervention
Various philosophers have argued that the core
notion of causation involves human intervention
(Collingwood, 1940; Hart & Honoré, 1983; von
Wright, 1971). It is through our actions and manipu-
lations of the environment around us that we acquire
our basic sense of causality. Several important claims
stem from this: that causes are potential handles on
the world; that they “make a difference”; that they
involve some kind of force or power. Indeed, the lan-
guage and metaphors of causal talk are rooted in this
idea of human intervention on a physical world.
More contemporary theories of causality dispense
with its anthropomorphic connotations but maintain
the notion of intervention as a central concept
(Glymour, 2001; Pearl, 2000; Spirtes et al., 1993;
Woodward, 2003, chapter 1, this volume).
Intervention not only is central to our notion of cau-
sation, but also is a fundamental means by which we
learn about causal structure. This has been a common-
place insight in scientific method since Bacon (1620)
spoke of “twisting the lion’s tail” and was refined into
axioms of experimental method by Mill (1843/1950).
More recently, it has been formalized by researchers in
artificial intelligence and philosophy (Pearl, 2000;
Spirtes et al., 1993; see Hagmayer, Sloman, Lagnado, &
Waldmann, chapter 6, this volume).
The importance of intervention in causal learning
is slowly beginning to permeate through to empirical
psychology. Although it has previously been marked
in terms of instrumental or operant conditioning
(Mackintosh & Dickinson, 1979), the full implica-
tions of its role in causal structure learning had not
been noted. This is largely because of the focus on
strength estimation rather than structural inference.
Once the emphasis is shifted to the question of how
people infer causal structure, the notion of an inter-
vention becomes critical.
Informally, an intervention involves imposing a
change on a variable in a causal system from outside
the system. A strong intervention is one that sets the
variable in question to a particular value and thus over-
rides the effects of any other causes of that variable. It
does this without directly changing anything else in
the system, although of course other variables in the
system can change indirectly as a result of changes to
the intervened-on variable (a more formal definition is
given by Woodward, chapter 1, this volume).
An intervention does not have to be a human
action (cf. Mendelian randomization; Davey Smith &
Ebrahim, 2003), but freely chosen human actions
will often qualify as such. These can range from care-
fully controlled medical trials to the haphazard
actions of a drunk trying to open a door. Somewhere
in between lays the vast majority of everyday interven-
tions. What is important for the purposes of causal
learning is that an intervention can act as a quasi-
experiment, one that eliminates (or reduces) confounds
and helps establish the existence of a causal relation
between the intervened-on variable and its effects.
A central benefit of an intervention is that it allows
one to distinguish between causal structures that are
difficult or impossible to discriminate among on the
basis of correlational data alone. For example, a high
correlation between bacteria and ulcers in the stom-
ach does not tell us whether the ulcers cause the
bacteria or vice versa (or, alternatively, if both share a
common cause). However, suppose an intervention is
made to eradicate the bacteria (and that this interven-
tion does not promote or inhibit the presence of
ulcers by some other means). If the ulcers also dis-
appear, then one can infer that the bacteria cause the
stomach ulcer.

162
CAUSATION AND PROBABILITY
Intervention aids learning
Can people make use of interventions to learn about
causal structure? Several studies have compared learning
through intervention with learning through observation,
with both adults (Lagnado & Sloman, 2002, 2004; Sobel
& Kushnir, 2006; Steyvers et al., 2003) and children
(Gopnik et al., 2004, this volume; Sobel & Kirkham,
chapter 9, this volume). All these studies have shown a
distinct advantage for intervention. When participants
are able to intervene freely on a causal system, they
learn its structure better than when they are restricted
to passive observation of its autonomous behavior.
What are the factors that drive this advantage? In
addition to the special type of information afforded by
intervention, because of the modification of the sys-
tem under study, interventions can facilitate learning
in several other ways. For instance, an intervener has
more control over the type of data they see and thus
can engage in more directed hypothesis testing than
an observer. Intervention can also focus attention on
the intervened-on variables and its effects. Further,
the act of intervention introduces an implicit tem-
poral cue into the learning situation because inter-
ventions typically precede their effects. Interveners
may use any of these factors to enhance their learning.
By using yoked designs, Lagnado and Sloman
(2004, 2006) ruled out the ability to hypothesis test as
a major contributor in their experiments (although
Sobel & Kushnir, 2006, report conflicting results).
However, they also showed that the presence of a tem-
poral cue had a substantial effect. When the informa-
tion about the variables in the causal system was
presented in a temporal order that matched the actual
causal order (rather than being inconsistent with it),
learning was greatly facilitated, irrespective of
whether participants were intervening or observing.
The authors suggested that in general people might
use a temporal order heuristic by which they assume
that any changes that occur subsequent to an action
are effects of that action. This can be an effective
heuristic, especially if these actions are uncon-
founded with other potential causes of the observed
effects. Such a heuristic can also be used in observa-
tion but is more likely to lead to spurious inferences
(because of unavoidable confounding).
An online learning paradigm
Although all of the studies reported so far demonstrate
an advantage of intervention, they also reveal low
levels of overall performance. Even when learners
were able to intervene, many failed to learn the correct
model (in most of the experiments, fewer than 40%
chose the correct models). We conjecture that this is
because of the impoverished nature of the learning
environment presented to participants. All of the stud-
ies used a trial-based paradigm, in which participants
viewed the results of their interventions in a case-by-
case fashion. And, the causal events under study were
represented by symbolic descriptions rather than
direct experience (cf. Waldmann & Hagmayer, 2001).
This is far removed from a naturalistic learning context.
Although it facilitates the presentation of the relevant
statistical contingencies, it denies the learner many of
the cues that accompany real-world interventions,
like spatiotemporal information, immediate feedback,
and continuous control.
To address this question, Lagnado and Sloman
(2003) introduced a learning paradigm that provided
some of these cues; participants manipulated on-
screen sliders in a real-time environment. Participants
had to figure out the causal connections between the
sliders by freely changing the settings of each slider
and observing the resultant changes in the other slid-
ers. In these studies, the majority of learners (>80%)
rapidly learned a range of causal models, including
models with four interconnected variables. This con-
trasted with the performance of observers, who
watched the system of sliders move autonomously and
seldom uncovered the correct model. Thus, the bene-
fits of intervention seem to be magnified greatly by
the dynamic nature of the task. This reinforces our
claim that causal cognition operates best when pre-
sented with a confluence of cues, and in particular,
that intervention works best when combined with
spatiotemporal information.
In addition, in a separate condition many learners
were able to make use of double interventions to
disambiguate between models indistinguishable
through single interventions. For example, when
restricted to moving one slider at a time, it is impossi-
ble to discriminate between a three-variable chain
A →B →C and a similar model with an extra link
from A to C. However, with an appropriate double
intervention (e.g., fixing the value of B and then see-
ing whether manipulation of A still leads to a change
in C), these models can be discriminated. The fact
that many participants were able to do this shows that
they can reason using causal representations (cf.
Hagmayer et al., chapter 6, this volume). They were

BEYOND COVARIATION
163
able to represent the two possible causal models and
work out what combination of interventions would
discriminate between them.
Intervention versus temporal order
The trial-based experiments by Lagnado and Sloman
(2004) show that temporal order plays a substantial
role in causal learning. However, the low levels of per-
formance made it difficult to assess the separate influ-
ences of intervention and temporal order cues. A
subsequent study by Lagnado and Sloman (2006)
used the slider paradigm to investigate this question.
Participants completed six problems, ranging from
two-variable to four-variable models. Participants were
divided into three groups: those who could freely
intervene on the causal system, those who observed
the system’s behavior, and those who observed the
results of another person’s interventions (yoked to the
active interveners). Within each group, participants
were presented with information about the slider val-
ues in two temporal orders, either consistent with or
opposite to the underlying causal structure. The main
results are shown in Figure 10-3 (in which the inter-
vention group is denoted as intervention 1). There is a
clear advantage of intervention (active or yoked) over
observation. There is also a clear influence of temporal
consistency for the observational and yoked groups
but not for the active interveners. The authors conjec-
tured that the active interveners overcame the incon-
sistent temporal order cue by (correctly) learning that
the variable information was presented in reverse
order. To test this, they ran a second intervention
condition in which the temporally inconsistent time
order was randomized rather than reversed (with the
constraint that it could never produce a consistent
order). The results for this follow-up are also shown in
Figure 10-3 (the new intervention group is interven-
tion 2). The interveners now showed a similar decline
in performance when information was presented in an
inconsistent order. Overall, these results confirm that
intervention and temporal order provide separate cues
to causal structure. They work best, however, in com-
bination, and this may explain the efficacy of interven-
tions made in naturalistic learning environments.
Prior Knowledge
Temporal order is a powerful cue to causality when
we experience causal events online. Whenever we
directly experience causal events, the sequence of the
learning input (i.e., learning order) mirrors the asym-
metry of causal order (causes generate effects but not
vice versa). The correlation between learning order
FIGURE 10-3 Percentage correct model choices in Lagnado and Sloman
(2006) showing influence of intervention and temporal order. Note that in
intervention 2 time-inconsistent orders were randomized rather than reversed.

164
CAUSATION AND PROBABILITY
and causal order is so strong in these situations that
some theories (e.g., associative learning models)
collapse causal order and learning order by assuming
that learning generally involves associations between
cues and outcomes, with cues presented temporally
prior to their outcomes (see Shanks & Lopez, 1996;
Waldmann, 1996, 2000).
However, whereas nonhuman animals indeed
typically experience causes prior to their effects, the
correlation between learning order and causal order
is often broken when learning is based on symbol-
ized representations of causal events. In fact, most
experimental studies of human learning are now car-
ried out on a computer in which cues and outcomes
are presented verbally. The flexibility of symbolic
representations allows it to present effect informa-
tion prior to cause information so that learning order
no longer necessarily corresponds to causal order.
For example, many experiments have studied disease
classification in which symptoms (i.e., effects of
diseases) are presented as cues prior to information
about their causes (i.e., diseases; e.g., Gluck &
Bower, 1988; Shanks & Lopez, 1996; Waldmann,
2000, 2001).
Learning order and causal order may also mis-
match when the causal events are not readily observ-
able but have to be measured or searched with more
complicated procedures. For example, a physician
may immediately observe symptoms of a new patient
prior to searching for possible causes. Or, parents
might become aware of school problems of their
child prior to finding out about the causes. Thus,
although the temporal order of learning events is
often a valid cue to causal structure, it is sometimes
necessary to override this cue when other cues
appear more valid.
Coherence with prior knowledge is a potent cue to
causal structure. Regardless of when we observe fever
in a patient, our world knowledge tells us that fever
is not a cause but rather an effect of an underlying
disease. Prior knowledge may be specific when
we have already learned about a causal relation, but
prior knowledge can also be abstract and hypotheti-
cal. We know that switches can turn on devices even
when we do not know about the specific function of a
switch in a novel device. Similarly, we know that dis-
eases can cause a wide range of symptoms prior to
finding out which symptom is caused by which dis-
ease. In contrast, rarely do we consider symptoms as
possible causes of a disease.
Prior knowledge versus temporal order
The possible mismatch between causal order and
learning order raises the question whether people are
capable of ignoring the temporal order of learning
events when their prior knowledge suggests a different
causal order. Following the framework of causal-
model theory, Waldmann and Holyoak (1990, 1992)
developed an experimental paradigm addressing this
question. In general, learners in different conditions
receive identical cues and outcomes in identical
learning order. However, based on initial instructions,
different causal orders are suggested so that in one
condition the cues represent causes and the outcomes
effects (predictive learning), whereas in the contrast-
ing condition the cues represent effects and the out-
comes causes (diagnostic learning).
A study by Waldmann (2001) exemplifies this
paradigm. In Waldmann’s Experiment 2, participants
were told that they were going to learn about new dis-
eases of the blood. In all conditions, learners observed
learning trials in which they first received information
about the presence of a Substance 1 in a patient fol-
lowed by feedback about the presence of a disease
(e.g., Midosis). Other trials showed patients whose
blood contained two substances, Substances 2 and 3,
which were a sign of a different disease (e.g., Xeritis).
Associative learning theories are only sensitive to
learning order and would therefore generally predict
that the associative strength between Substance 1 and
Midosis should be greater than between the two other
substances and Xeritis (see Cobos, López, Cano,
Almaraz, & Shanks, 2002). This so-called overshad-
owing effect derives from associative learning theories
(e.g., Rescorla & Wagner, 1972), which predict that
the two redundant always co-occurring substances
compete for predicting Xeritis. Once asymptotic per-
formance is achieved, this should lead to either sub-
stance contributing only about half of the associative
strength needed to predict the disease correctly.
To pit learning order against causal order,
Waldmann (2001) created two contrasting conditions:
In the predictive learning condition, the substances
were described as coming from food items, which
gives them the status of potential causes of the diseases
(see Figure 10-4). In contrast, in the diagnostic learning
condition, the substances were characterized as poten-
tially generated by the diseases, which assigns them
the causal status of effects. Although cues, outcomes,
and learning order were identical in both conditions,

overshadowing 
interacted 
with 
causal 
status.
Overshadowing was stronger in the predictive than in
the diagnostic learning condition. Similar interactions
have also been shown for the related blocking pheno-
menon (Waldmann, 2000; Waldmann & Holyoak,
1992; Waldmann & Walker, 2005).
This interaction can be modeled by an account that
assumes that people use prior knowledge conveyed to
them through the cover stories to form tentative causal
models with the structures displayed in Figure 10-4 (see
Waldmann & Martignon, 1998, for a Bayesian causal
model account; see also Tenenbaum et al., chapter 19,
this volume). These models free learners from learning
order as a cue to causality and allow them to assign the
learning input flexibly to the causal variables in the ten-
tative causal model. Thus, in the predictive learning
condition, the cues are mapped to the cause layer and
the effects to the outcome layer (Figure 10-4A), whereas
in the diagnostic learning condition the cues are
mapped to the effect layer and the outcomes to the
cause layer (Figure 10-4B). Although prior knowledge
generates the structure of the causal models underlying
the learning domain, the cover stories made it clear to
participants that the causal relations were only hypo-
thetical and needed to be verified by checking the
learning data. Thus, in the learning stage the learning
input is used to parameterize the model or test whether
the hypothesized links are present.
The overshadowing study illustrates this account
(Waldmann, 2001). In Experiment 2, learners observed
Substance 1 by itself as a deterministic cause (predic-
tive learning) or a deterministic effect (diagnostic
learning). However, the situation differed across the
two learning conditions for the two redundant sub-
stances. In the diagnostic learning condition, the
data suggest that each of the two substances is deter-
ministically caused by their common cause, the dis-
ease Xeritis (see Figure 10-4B). Although there may
be alternative unknown diseases also affecting these
symptoms, these alternative causes were not men-
tioned in the instructions so that their potential
impact on learners’ assessment should be relatively
small. By contrast, in the predictive learning condi-
tion, learners were confronted with an ambiguous
situation. Here, the two substances represented
perfectly confounded alternative potential causes, so
it was impossible to determine whether only one of
these potential causes was effective or whether both
shared responsibility in generating the common
effect, Midosis (see Figure 10-4A). Thus, learners
should have been uncertain about their causal status,
which would lead to a lowering of ratings (i.e., over-
shadowing). This pattern was indeed found in the
study.
Temporal order of learning events was also pitted
against causal order in other task domains. In a study
on category learning, Waldmann et al. (1995) showed
that sensitivity to correlations among cues is influ-
enced by the causal status of the cues (see also
Rehder, 2003a, 2003b; Rehder & Hastie, 2001). As
predicted by Bayesian models, when the cues repre-
sent effects within a common-cause model, learners
expect cue correlations, whereas statistical indepen-
dence among cues is expected when they represent
multiple causes of a common effect. These expecta-
tions influenced how difficult it was for participants to
learn about different category structures. Again, these
findings support the view that learners formed a struc-
tural representation of a causal model on the basis of
the initial instructions and then tried to map these
models to the learning data (see Waldmann &
Martignon, 1998, for the formal details).
Prior knowledge and parameter estimation
Even when causal order and temporal order coincide,
temporal order alone is not sufficiently constrained to
BEYOND COVARIATION
165
FIGURE 10-4 Predictive learning (A) and diagnostic learning
(B) in Waldmann (2001).

166
CAUSATION AND PROBABILITY
determine how learning events should be processed.
In a stream of learning events, the relevant events
need to be parsed first, and then it is necessary to
decide how the events are interrelated. Often, this
problem is solved by assuming that events that are
spatiotemporally contiguous (see the section on tem-
poral order) are interrelated. But, this is not always
true. For example, when eating a fish dish, we would
not view the fish as a cause of subsequent nausea that
occurred within 0.5 seconds of eating the meal. Based
on prior knowledge, we expect a longer latency of the
causal mechanism. In contrast, we would not relate a
light to a button press if there was a latency of 10 seconds
between pressing the button and the light going on.
Hagmayer and Waldmann (2002) have shown that
prior expectations about temporal delays between
causes and effects indeed mediate how causes and
effects are interrelated within a stream of events. This
selection consequently affects how causal strength is
estimated within the data set. Despite observing iden-
tical event streams, different assessments of causal
strength resulted based on how the stream was parsed
and how the events were interrelated prior to assessing
the strength of covariation.
Prior assumptions also affect which statistical indi-
cators are chosen to estimate causal strength para-
meters. When the task is to estimate causal strength
between a cause and effect, it is necessary to compute
the covariation between these events while holding
constant alternative causes that may confound the
relation. For example, the strength of the causal influ-
ence of smoking on heart disease should ideally be
assessed when alternative causes of heart disease (e.g.,
junk food) are absent or held constant. In contrast,
causally irrelevant events, alternative effects of the
target cause (within a common-cause model), or
events that lie downstream on a causal chain between
the target cause and the target effect must not be held
constant (Eells, 1991; Pearl, 2000). Otherwise,
erroneous parameter estimates might result.
Waldmann and Hagmayer (2001) have shown that
participants are indeed sensitive to these normative
constraints. In a set of experiments, learners were
given identical learning input with three interrelated
events. The participants’ task was to assess the strength
of the causal relation between a given cause and an
effect. The causal status of the third event was mani-
pulated by means of initial instructions. The results
showed that learners only tended to hold the third
event constant when this event was assumed to be an
alternative cause of the target effect. When it was
causally irrelevant, an alternative effect of the cause,
or an intermediate event on a causal chain between
cause and effect, participants ignored the status of the
third variable. Again, this is a case in which temporal
order is an insufficient cue because the learning
events were presented identically to all participants.
The correct parameter estimates depended on prior
knowledge about the causal status of the learning
events.
Use of prior knowledge and processing 
constraints
Processing learning data on the basis of a prior causal
model can be demanding. For example, in a diagnos-
tic learning task the learning order of cues and out-
comes conflicts with causal order. Also, holding
constant alternative causes can sometimes be difficult
when the presence and absence of the alternative
cause alternates so that it is hard to store separately in
memory the events in which the confound was
present and absent. A number of studies have shown
that, in situations that tax processing capacity, people
may incorrectly process the learning data, although in
less complex tasks they do better (De Houwer &
Beckers, 2003; Tangen & Allan, 2004; Waldmann &
Hagmayer, 2001; Waldmann & Walker, 2005).
Waldmann and Walker have also shown that it is
crucial that people have a strong belief in the validity
of the causal model; otherwise, their learning is dic-
tated by other cues that require less effort to use.
These studies show that people have the competence
to interrelate causal models and learning data cor-
rectly when they strongly believe in their prior
assumptions and when the learning task is within the
grasp of their information-processing capacity.
Otherwise, other cues may dominate.
Integrating Fragments of Causal Models
We rarely acquire knowledge about a complex causal
network all at once. Rather, we learn about these
models in a piecemeal fashion. Consider, for exam-
ple, the search for the causes of ulcer by medical
science (see Thagard, 1999, for a detailed description
of the history of medical theories of ulcer). It was first
thought that ulcers were caused by excessive stomach
acid, which was caused by stress. Later, scientists
found out that excessive acidity is not the cause of

BEYOND COVARIATION
167
many ulcers, but that the majority of ulcers are caused
by bacteria (Helicobacter pyloris). In addition, it was
discovered by other researchers that some acid-based
pain relievers, such as aspirin, might also cause ulcers.
As a consequence, an initially simple causal chain
model (Stress →Excessive acid →Ulcer) was replaced
by a more complex causal model (see Figure 10-1).
Theory change occurred as a result of many inde-
pendent empirical studies that focused on individual
links. These individual pieces of knowledge were
eventually integrated into a coherent, global causal
model that incorporated what we now know about
ulcers.
Similarly, in everyday life, we may independently
learn that peanuts cause an allergy and later discover
that strawberries cause the same allergy. Although we
may never have eaten peanuts and strawberries
together, we could still integrate these two pieces of
causal knowledge into a common-effect model.
Similarly, we might independently learn about two
causal relations in which the same common cause is
involved. For example, we may first experience that
aspirin relieves headache. Later, a physician might
tell us that our ulcer is also caused by aspirin. Again,
although we may never have consciously experienced
the two effects of the common cause together, we can
integrate the two fragments into a coherent common-
cause model.
What is the advantage of integrating fragments of
causal knowledge into a coherent global causal
model? In addition to representing only the direct
causal relations within the model (i.e., causes, effects,
and causal arrows), causal models allow us to infer the
relation between any pair of events within the model,
even when they are not directly causally connected.
For example, the causal model for aspirin would
imply that relief of headache and ulcer should tend
to co-occur despite no causal relation to each other.
These structural implications are a consequence of
the patterns of causal directionality inherent in causal
models.
Bayes nets provide formal tools to analyze struc-
tural implications of causal models (see Pearl, 1988,
2000; Spirtes et al., 1993). The graph of a common-
cause model expresses that the two effects are spuri-
ously related (because of their common cause) but
become independent once the state of the common
cause is known (see Figure 10-4B). This is a conse-
quence of the Markov condition. For example, once
we know that aspirin is present, the probability of
ulcers is fixed regardless of whether headache is
present or absent. Similarly, causal chains imply that
the initial cause and the final effect are dependent but
become independent when the intermediate event is
held constant. Finally, a common-effect model
(Figure 10-4A) implies independence of the altern-
ative causes, but they become dependent once the
common effect is held constant. This is an example of
explaining away. Eating peanuts and eating straw-
berries should normally occur independently. But,
once we know that someone has an allergy, finding
out that they have eaten peanuts makes it less likely
that they have also eaten strawberries.
Hagmayer and Waldmann (2000, 2006) have
investigated the question of whether people are capa-
ble of integrating fragments into global causal models
in a normative fashion (see also Ahn & Dennis, 2000;
Perales, Catena, & Maldonado, 2004). In a typical
experiment, participants had to learn about the causal
relations between the mutation of a fictitious gene
and two substances. The two relations were learned
on separated trials, so no information about the
covariation between the two substances was available.
Although the learning input was identical, the under-
lying causal model differed in different conditions. To
manipulate causal models, participants were told
either that the mutation of the fictitious gene was the
common cause of two substances or they were told
that the two substances were different causes of the
mutation of the gene. The strength of the causal rela-
tions was also manipulated to test whether people are
sensitive to the size of the parameters when making
predictions.
The main goal of the study was to test the condi-
tions under which people are aware of the different
structural implications of the common-cause and the
common-effect models. A correlation should be
expected between the two substances when they were
caused by a common cause, with the size of the
correlation dependent on the size of causal strength.
By contrast, two causes of a common effect should
be independent regardless of the size of the causal
relations.
To test sensitivity to structural implications, parti-
cipants were given two tasks: In the first task, partici-
pants were given new cases along with information
about whether a mutation had occurred or not. Their
task was to predict for each trial whether either of
the two substances was present or absent. Thus, in the
common-cause conditions people predicted the

168
CAUSATION AND PROBABILITY
presence or absence of the two effects based on infor-
mation about the presence or absence of the common
cause; in the common-effect condition, people diag-
nosed the presence or absence of either cause based
on information about the presence or absence of the
common effect. This way, participants made predic-
tions for the two substances they had never observed
together. Across multiple predictions, participants
generated a correlation between the two substances
that could be used as an indicator of sensitivity to the
implied correlations. The second task asked partici-
pants directly to estimate the conditional frequency of
the second substance given that the first substance
was present or absent.
The two tasks assess sensitivity to structural impli-
cations in different ways. Whereas the second task
assessed more explicit knowledge of the structural
implications of causal knowledge, the first task
required participants to use the causal models to pre-
dict patterns of events. Thus, this task probes sensiti-
vity to structural implications in a more implicit
fashion. For example, in the common-cause condi-
tion a possible strategy would be to run a mental sim-
ulation of the underlying common-cause model.
Whenever the presence of the common cause is
stated in the test trial, the two effects could be indivi-
dually predicted with probabilities that conform to the
learned strength of the causal relation. This strategy
would yield the normatively implied spurious correl-
ation between the substances, although the predic-
tions focused on the individual links between the
common cause and either effect.
Similarly, in the common-effect condition people
could simulate diagnoses of the two causes based on
information about the presence or absence of the
common effect by running the causal model back-
ward from effect to causes (see Figure 10-4A).
Simultaneous diagnoses of either cause should make
participants aware of the possible competition
between the causes. Because either cause suffices to
explain the effect, people should be reluctant to pre-
dict both causes too often. This would yield correct
diagnoses of the patterns of causes without requiring
participants to reflect directly on the correlation
between alternative causes.
The results of this and other experiments show
little sensitivity to the differences between common-
cause and common-effect models in the explicit meas-
ure. Although some basic explicit knowledge cannot
be ruled out (see also Perales et al., 2004), Hagmayer
and Waldmann’s (2000, 2006) experiments show that
people do not use the strength parameters to predict
the implied correlations very well. By contrast, the
implicit tasks revealed patterns that corresponded
remarkably well to the expected patterns. Whereas a
spurious correlation was predicted in the common-
cause condition, the predicted correlation stayed close
to zero in the common-effect condition. Hagmayer
and Waldmann attribute this competency to mental
simulations of causal models.
Further experiments by Hagmayer and Waldmann
(2006) explored the boundary conditions for these
effects. The dissociation between explicit and implicit
knowledge disappeared with causal chains in which
the individual links were taught separately and in
which the task in the test phase was to predict the
final effect based on information about the initial
cause (see also Ahn & Dennis, 2000). In this task,
both explicit and implicit measures were sensitive to
the implied correlation between these two events.
This result shows that spurious relations (e.g.,
between two effects of a common cause) need to be
psychologically distinguished from indirect causal
relations. Whereas people obviously have little
explicit knowledge about spurious relations, they may
view indirect relations as a subdivided global causal
relation. In fact, all direct causal relations can be sub-
divided into chains that represent the underlying
mechanisms. Thus, combining links of causal chains
into a global prediction is easier than deriving predic-
tion for spurious relations.
The implicit task also turned out to be sensitive to
boundary conditions. Whereas performance for the
common-cause model and the chain model showed
fairly robust sensitivity to spurious and indirect rela-
tions, it turned out that people’s implicit estimates in
the common-effect condition are only adequate when
the task required them to predict patterns of causes, as
in the experiment described. In this task, the links of
the causal models were simulated in parallel, which
apparently proved important for making learners
aware of the implied competition among the causes.
When the task was first to predict the effect based on
one cause and then make inferences about the other
cause, people erroneously predicted a spurious correl-
ation between the causes. Probably, participants
accessed each link consecutively and tended to forget
about the possible competition between the causes.
In sum, people are capable of integrating frag-
ments of causal knowledge in a way that corresponds

BEYOND COVARIATION
169
to the normative analyses of Bayes nets. However, this
competency is not as robust as the computer models
used to implement Bayes nets. It rather depends on a
number of task factors that include the type of relation
within a causal model and the specifics of the task.
Computational Models of Learning
Although our main concern has been with how peo-
ple learn causal structure, the story we have told is
linked in important ways to current computational
models of inference and learning. For one, the causal
Bayesian network formalism (Pearl, 2000; Spirtes
et al., 1993) offers a normative framework for causal
representation and inference. And, at a qualitative
level human inference seems to fit with the broad
prescriptions of this theory (see Hagmayer et al., chap-
ter 6, this volume; Sloman & Lagnado, 2004, 2005).
The causal Bayesian network framework also suggests
various computational procedures for learning causal
structure. These are often grouped into two types:
Bayesian methods (Heckerman, Meek, & Cooper,
1999) and constraint-based methods (Spirtes et al.,
1993). It is instructive to compare and contrast these
approaches as models of human learning in the light
of the proposals and empirical evidence surveyed in
this chapter.
In short, Bayesian methods assume that learners
have some prior belief distribution across all possible
causal structures and update these beliefs as statistical
data are gathered. Bayes rule is used to compute pos-
terior probabilities for each of the possible models
given the data, and a best-fitting model is derived
from this computation (see Tenenbaum et al.,
chapter 19, this volume). Constraint-based methods
work by computing the independencies and depend-
encies (both conditional and unconditional) in the
data set and then returning the structures that are con-
sistent with these dependencies (for more details, see
Danks, 2004, in press).
At present, these computational models have
been used as rational rather than psychological
models of human learning (Anderson, 1990; Marr,
1982). They aim to specify what the learner is com-
puting rather than how they are actually computing
it. Both Bayesian methods (Steyvers et al., 2003;
Tenenbaum et al., chapter 19, this volume) and 
constraint-based methods have been used for this 
purpose. A question closer to the concerns of the
empirical psychologist, however, is whether these
models tell us anything about the psychological or
process level of causal learning. What are the mecha-
nisms that people actually use to learn about causal
structure?
In their current state, these computational
approaches seem to both overestimate and under-
estimate the capabilities of human learners. For
instance, they overestimate the computational
resources and processing power available to humans
to make the appropriate Bayesian or constraint-based
computations. Bayesian models require priors across
all possible models and Bayesian updating with each
new piece of evidence. Constraint-based models
require the computation of all the dependencies and
independencies in the data and inference of the set of
Markov equivalent structures. Both methods appear
to place insurmountable demands on a human mind
known to be limited in its processing capacities.
There are potential solutions to these short-
comings. Bayesian methods can be heuristic rather
than exhaustive, and constraint-based methods can
use more psychologically realistic methods for com-
puting dependencies (Danks, 2004). However, both
approaches still need to deal with the basic problem,
detailed in this chapter, that there is little evidence
that people who only observe patterns of covariation
between events (without further constraints) can
induce causal models. In particular, there is no clear
evidence that people can use statistical information
from triples of events to infer causal models via condi-
tional dependence relations. And, this ability seems to
lie at the heart of both approaches.
In addition, these computational approaches seem
to underestimate human capabilities (or, more pre-
cisely, the richness of the environment around them
and their ability to exploit this information). As we
have seen throughout this chapter, people make use of
various cues aside from statistical data. These cues are
typically used to establish an initial causal model,
which is then tested against the incoming statistical
data. Bayesian approaches have sought to model this
by incorporating prior knowledge and assumptions in
the learner’s prior belief distribution (Tenenbaum 
et al., chapter 19, this volume; for a more general
approach, see Griffiths & Tenenbaum, chapter 20, this
volume) and thus account for inferences made on
sparse data. But, it is not clear how they handle cases
for which people test just a single model and then
abandon it in favor of an alternative. This kind of 

170
CAUSATION AND PROBABILITY
discontinuity in someone’s beliefs does not emerge
naturally from the Bayesian approach.
On the face of it, constraint-based methods are
largely data driven, so the use of prior knowledge and
other assumptions appears problematic, but they also
have the resources to address this issue. Along with
the constraints that stem from the statistical depen-
dencies in the data, they can include constraints
imposed by prior knowledge, temporal order informa-
tion, and other cues. This approach also seems to fit
well with the discontinuous and incremental nature
of human learning (Danks, 2004, in press).
However, in both cases further work is needed to
develop a comprehensive framework that can inte-
grate the diverse constraints and cues to structure1
(e.g., from temporal ordering, interventions, etc.) and
capture the heuristic methods that humans seem to
adopt. In particular, this framework needs to be able
to combine and trade off these constraints as new
information arrives. For example, although an initial
causal model might be based on the assumption that
temporal order reflects causal order, a revised model
could reject this constraint in the light of statistical
data that contradicts it (see the section on temporal
order).
Summary
In this chapter, we have argued for several intercon-
nected theses. First, the fundamental way that people
represent causal knowledge is qualitative in terms of
causal structure. Second, people use a variety of cues
to infer structure aside from statistical data (e.g.,
temporal order, intervention, coherence with prior
knowledge). Third, once a structural model is hypoth-
esized, subsequent statistical data are used to confirm
or refute the model and (possibly) to parameterize it.
The structure of a posited model influences how the
statistical data are processed. Fourth, people are lim-
ited in the number and complexity of causal models
that they can hold in mind to test, but they can sepa-
rately learn and then integrate simple models and
revise models by adding and removing single links.
Finally, current computational models of learning
need further development before they can be applied
to human learning. What is needed is a heuristic-
based model that shares the strengths and weaknesses
of a human learner and can take advantage of the rich
causal information that the natural environment
provides.
References
Ahn, W., & Dennis, M. (2000). Induction of causal
chains. In L. R. Gleitman & A. K. Joshi (Eds.) , Pro-
ceedings of the 22nd Annual Conference of the
Cognitive Science Society (pp. 19–24). Mahwah, NJ:
Erlbaum.
Anderson, J. R. (1990). The adaptive character of thought.
Hillsdale, NJ: Erlbaum.
Bacon, F. (1620). Novum organum. Chicago: Open Court.
Buehner, M. J., Cheng, P. W., & Clifford, D. (2003).
From covariation to causation: A test of the assump-
tion of causal power. Journal of Experimental
Psychology: Learning, Memory, and Cognition, 29,
1119–1140.
Buehner, M. J., & May, J. (2002). Knowledge mediates
the timeframe of covariation assessment in human
causal induction. Thinking and Reasoning, 8, 269–295.
Cheng, P. W. (1997). From covariation to causation:
A causal power theory. Psychological Review, 104,
367–405.
Cobos, P. L., López, F. J., Cano, A., Almaraz, J., &
Shanks, D. R. (2002). Mechanisms of predictive
and diagnostic causal induction. Journal of
Experimental 
Psychology: 
Animal 
Behavior
Processes, 28, 331–346.
Collingwood, R. (1940). An essay on metaphysics.
Oxford, England: Clarendon Press.
Danks, D. (in press). Causal learning from observations
and manipulations. In M. Lovett & P. Shah (Eds.),
Thinking with data. Hillsdale, NJ: Erlbaum.
Danks, D. (2004). Constraint-Based Human Causal
Learning. In M. Lovett, C. Schunn, C. Lebiere, &
P. Munro (Eds.), Proceedings of Sixth International
Conference on Cognitive Modelling (pp. 342–343).
Mahwah, NJ: Erlbaum.
Danks, D., & McKenzie, C. R. M. (2006). Learning com-
plex causal structures. Manuscript in preparation.
Davey Smith, G., & Ebrahim, S. (2003). “Mendelian
randomization”: Can genetic epidemiology con-
tribute to understanding environmental deter-
minants of disease? International Journal of
Epidemiology, 32, 1–22.
De Houwer, J., & Beckers, T. (2003). Secondary task
difficulty modulates forward blocking in human
contingency learning. Quarterly Journal of
Experimental Psychology, 56B, 345–357.
Eells, E. (1991). Probabilistic causality. Cambridge,
England: Cambridge University Press.
Einhorn, H. J., & Hogarth, R. M. (1986). Judging pro-
bable cause. Psychological Bulletin, 99, 3–19.
Gluck, M. A., & Bower, G. H. (1988). From condition-
ing to category learning: An adaptive network
model.
Journal of Experimental Psychology:
General, 117, 227–247.

BEYOND COVARIATION
171
Glymour, C. (2001). The mind’s arrows. Cambridge,
MA: MIT Press.
Gopnik, A., Glymour, C., Sobel, D. M., Schulz, L. E.,
Kushnir, T., & Danks, D. (2004). A theory of causal
learning in children: Causal maps and Bayes nets.
Psychological Review, 111, 1–31.
Griffiths, T. L., & Tenenbaum, J. B. (2005). Structure
and strength in causal induction. Cognitive
Psychology, 51, 354–384.
Hagmayer, Y., & Waldmann, M. R. (2000). Simulating
causal models: The way to structural sensitivity. In
L. Gleitman & A. Joshi (Eds.), Proceedings of the
22nd Annual Conference of the Cognitive Science
Society (pp. 214–219). Mahwah, NJ: Erlbaum.
Hagmayer, Y., & Waldmann, M. R. (2002). How tempo-
ral assumptions influence causal judgments.
Memory & Cognition, 30, 1128–1137.
Hagmayer, Y., & Waldmann, M. R. (2006). Integrating
fragments of causal models–Implicit versus explicit
sensitivity to structural implications. Manuscript in
preparation.
Hart, H. L. A., & Honoré, T. (1983). Causation in the
law (2nd ed.). Oxford, England: Clarendon.
Heckerman, D., Meek, C., & Cooper, G. (1999).
A Bayesian approach to causal discovery. In
C. Glymour & G. Cooper (Eds.), Computation,
causation, and discovery (pp. 143–167). Cambridge,
MA: MIT Press.
Hume, D. (1748/1975). An enquiry concerning human
understanding. Oxford, England: Clarendon.
Lagnado, D. A., & Sloman, S. A. (2002). Learning causal
structure. In W. Gray & C. D. Schunn (Eds.),
Proceedings of the 24th Annual Conference of the
Cognitive Science Society (pp. 560–565). Mahwah,
NJ: Erlbaum.
Lagnado, D. A., & Sloman, S. A. (2003). Using multiple
interventions. Unpublished raw data.
Lagnado, D. A., & Sloman, S. A. (2004). The advantage of
timely intervention. Journal of Experimental Psycho-
logy: Learning, Memory, and Cognition, 30, 856–876.
Lagnado, D. A., & Sloman, S. A. (2006). Time as a guide
to cause. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 32, 451–460.
Mackintosh, N. J., & Dickinson, A. (1979). Instrumental
(Type II) conditioning. In A. Dickinson & R. A.
Boakes (Eds.), Mechanisms of learning and motiva-
tion (pp. 143–169). Hillsdale, NJ: Erlbaum.
Marr, D. (1982). Vision. San Francisco: Freeman.
Mill, J. S. (1950). Philosophy of scientific method. New
York: Hafner. (Original work published 1843)
Neapolitan, R. E. (2004). Learning Bayesian networks.
Upper Saddle River, NJ: Pearson US.
Pearl, J. (1988). Probabilistic reasoning in intelligent
systems: Networks of plausible inference. San Mateo,
CA: Morgan Kaufmann.
Pearl, J. (2000). Causality. Cambridge, England:
Cambridge University Press.
Perales, J. C., Catena, A., & Maldonado, A. (2004).
Inferring non-observed correlations from causal
scenarios: The role of causal knowledge. Learning
and Motivation, 35, 115–135.
Rehder, B. (2003a). Categorization as causal reasoning.
Cognitive Science, 27, 709–748.
Rehder, B. (2003b). A causal-model theory of conceptual
representation and categorization. Journal of
Experimental Psychology: Learning, Memory, and
Cognition, 29, 1141–1159.
Rehder, B., & Hastie, R. (2001). Causal knowledge and
categories: The effects of causal beliefs on catego-
rization, induction, and similarity. Journal of
Experimental Psychology: General, 130, 323–360.
Rescorla, R. A., & Wagner, A. R. (1972). A theory of
Pavlovian conditioning: Variations in the effective-
ness of reinforcement and nonreinforcement. In
A. H. Black & W. F. Prokasy (Eds.), Classical condi-
tioning II: Current theory and research (pp. 64–99).
New York: Appleton-Century-Crofts.
Shanks, D. R. (2004). Judging covariation and causation.
In D. Koehler & N. Harvey (Eds.), Blackwell hand-
book of judgment and decision making (pp. 220–
239). Oxford, England: Blackwell.
Shanks, D. R., & Dickinson, A. (1987). Associative
accounts of causality judgment. In G. H. Bower
(Ed.), The psychology of learning and motivation:
Advances in research and theory (Vol. 21, 
pp. 229–261). San Diego, CA: Academic Press.
Shanks, D. R., & López, F. J. (1996). Causal order does
not affect cue selection in human associative learn-
ing. Memory and Cognition, 24, 511–522.
Shanks, D. R., Pearson, S. M., & Dickinson, A. (1989).
Temporal contiguity and the judgment of causality
by 
human 
subjects. 
Quarterly 
Journal 
of
Experimental Psychology, 41B, 139–159.
Sloman, S. A., & Lagnado, D. A. (2004). Causal invari-
ance in reasoning and learning. In B. Ross (Ed.),
The psychology of learning and motivation (Vol.
44, pp. 287–325). San Diego, CA: Elsevier
Science.
Sloman, S. A., & Lagnado, D. A. (2005). Do we “do”?
Cognitive Science, 29, 5–39.
Sobel, D. M., & Kushnir, T. (2006). The importance of
decision demands in causal learning from interven-
tions. Memory & Cognition, 34, 411–419.
Spirtes, P., Glymour, C., & Schienes, R. (1993).
Causation, prediction, and search. New York:
Springer-Verlag.
Steyvers, M., Tenenbaum, J. B., Wagenmakers, E. J., &
Blum, B. (2003). Inferring causal networks from
observations and interventions. Cognitive Science,
27, 453–489.

172
CAUSATION AND PROBABILITY
Tangen, J. M., & Allan, L. G. (2004). Cue-interaction
and judgments of causality: Contributions of causal
and associative processes. Memory & Cognition, 32,
107–124.
Tenenbaum, J. B., & Griffiths, T. L. (2001). Structure
learning in human causal induction. In T. K. Leen,
T. G. Dietterich, & V. Tresp (Eds.), Advances in
neural 
information 
processing 
systems 
13
(pp. 59–65). Cambridge, MA: MIT Press.
Tenenbaum, J. B., & Griffiths, T. L. (2003). Theory-
based causal inference. In S. Becker, S. Thrun, &
Obermayer (Eds.), Advances in Neural Information
Processing Systems 15 (pp. 35–42). Cambridge,
MA: MIT Press.
Thagard, P. (1999). How scientists explain disease.
Princeton, NJ: Princeton University Press.
von Wright, G. H. (1971). Explanation and understand-
ing. Ithaca, NY: Cornell University Press.
Waldmann, M. R. (1996). Knowledge-based causal
induction. In D. R. Shanks, K. J. Holyoak, & D. L.
Medin (Eds.), The psychology of learning and
motivation (Vol. 34, pp. 47–88). San Diego, CA:
Academic Press.
Waldmann, M. R. (2000). Competition among causes
but not effects in predictive and diagnostic learning.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 26, 53–76.
Waldmann, M. R. (2001). Predictive versus diagnostic causal
learning: Evidence from an overshadowing paradigm.
Psychonomic Bulletin and Review, 8, 600–608.
Waldmann, M. R., & Hagmayer, Y. (2001). Estimating
causal strength: The role of structural knowledge
and processing effort. Cognition, 82, 27–58.
Waldmann, M. R., & Hagmayer, Y. (2005). Seeing -
versus doing: Two modes of accessing causal know-
ledge.
Journal of Experimental Psychology:
Learning, Memory, and Cognition, 31, 216–227.
Waldmann, M. R., & Holyoak, K. J. (1990). Can causal
induction be reduced to associative learning? In 
M. Piattelli-Palmarini (Ed.), Proceedings of the 12th
Annual Conference of the Cognitive Science Society
(pp. 190–197). Hillsdale, NJ: Erlbaum.
Waldmann, M. R., & Holyoak, K. J. (1992). Predictive
and diagnostic learning within causal models:
Asymmetries in cue competition. Journal of
Experimental Psychology: General, 121, 222–236.
Waldmann, M. R., Holyoak, K. J., & Fratianne, A.
(1995). Causal models and the acquisition of cate-
gory structure. Journal of Experimental Psychology:
General, 124, 181–206.
Waldmann, M. R., & Martignon, L. (1998). A Bayesian
network model of causal learning. In M. A.
Gernsbacher & S. J. Derry (Eds.), Proceedings of the
20th Annual Conference of the Cognitive Science
Society (pp. 1102–1107). Mahwah, NJ: Erlbaum.
Waldmann, M. R., & Walker, J. M. (2005). Competence
and performance in causal learning. Learning &
Behavior, 33, 211–229.
Wasserman, E. A., Chatlosh, D. L., & Neunaber, D. J.
(1983). Perception of causal relations in humans:
Factors affecting judgments of response-outcome
contingencies under free-operant procedures.
Learning and Motivation, 14, 406–432.
Woodward, J. (2003). Making things happen: A theory of
causal explanation. Oxford, England: Oxford
University Press.

