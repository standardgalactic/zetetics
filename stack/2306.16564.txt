LLM Calibration and Automatic Hallucination
Detection via Pareto Optimal Self-supervision
Theodore Zhao
Microsoft
Mu Wei
Microsoft
J. Samuel Preston
Microsoft
Hoifung Poon
Microsoft
Abstract
Large language models (LLMs) have demonstrated remarkable capabilities out of
box for a wide range of applications, yet accuracy still remains a major growth
area, especially in mission-critical domains such as biomedicine. An effective
method to calibrate the confidence level on LLM responses is essential to auto-
matically detect errors and facilitate human-in-the-loop verification. An important
source of calibration signals stems from expert-stipulated programmatic super-
vision, which is often available at low cost but has its own limitations such as
noise and coverage. In this paper, we introduce a Pareto optimal self-supervision
framework that can leverage available programmatic supervision to systematically
calibrate LLM responses by producing a risk score for every response, without
any additional manual efforts. This is accomplished by learning a harmonizer
model to align LLM output with other available supervision sources, which would
assign higher risk scores to more uncertain LLM responses and facilitate error
correction. Experiments on standard relation extraction tasks in biomedical and
general domains demonstrate the promise of this approach, with our proposed risk
scores highly correlated with the real error rate of LLMs. For the most uncertain
test instances, dynamic prompting based on our proposed risk scores results in sig-
nificant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past
state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised
results on challenging evaluation datasets.
1
Introduction
Large language models (LLMs) have evolved to be impressively powerful in recent development
[39], with Generative pretrained transformer (GPT) models becoming increasingly effective in their
emerging ability. The evolution from GPT-3 [3] to GPT-4 [20], as well as the emergence of other
LLMs such as PaLM [4] and LLaMA [28], showed a significant leap in terms of natural language
understanding and problem-solving abilities. The generative models are also widely applied to
numerous fields, generating information in many applications. However, the problem of hallucination
is still a major challenge when LLMs are applied to applications that require a high standard for
accuracy and reliability, such as biomedical and healthcare domains.
Unfortunately, there lack systematic tools to efficiently identify hallucination, or estimate the con-
fidence level of the output. The intrinsic confidence score from the generative LLMs is often
unavailable, or not well calibrated with respect to the desired target, especially after applying rein-
forcement learning with human feedback [21, 20]. Heuristic methods, such as sampling an ensemble
of LLM responses, are computationally expensive and prone to be biased by the LLM itself.
Preprint. Under review.
arXiv:2306.16564v2  [cs.CL]  6 Jul 2023

There are two main types of approaches to assess the confidence level of LLM responses. The first
is internal evaluation, where the LLM is prompted in various ways to produce multiple responses,
which are then used to infer the reliability of the answer. Examples include self-consistency [31]
and chain-of-thought prompting [32]. Such methods are less quantitative and are vulnerable that
the estimated confidence is biased by the model itself. The quality of the results could be highly
dependent on the prompting strategy, and there is no systematical way to quantify this. The second
type of approaches resort to external sources of information, such as having human reviewers validate
the response, or use large scale labeled data to build evaluation models. These approaches require
significant manual annotation efforts, which are expensive, being one main bottleneck of modern
supervised model training. To that extend, self-supervision which can flexibly utilizes patterns within
the data as well as knowledge from external sources [15], suggests a promising alternative.
Inspired by previous works in programmatic supervision [37, 30] and the abundant research in
Pareto optimization [22], we propose a flexible framework that combines information from both the
LLM response and supervision sources using Pareto optimal learning. Our approach is driven by
the following intuitions. Firstly, external supervision sources that are independent to the LLM is
necessary to avoid bias from an LLM evaluating itself. Secondly, consider the LLM errors as noisy
perturbation on the gold labels, fitting a model with LLM noise and independent external noise is
actually performing implicit label smoothing, which helps improve calibration power [18]. In that
sense, Pareto optimal self-supervision serves as a nice framework to incorporate both features. It is
noteworthy that the proposed approach requires only unlabeled data, which makes it applicable to
areas where annotation is expensive.
Our main contribution in the paper is a novel framework for LLM calibration through Pareto
optimal self-supervision. We propose the Pareto optimal learning assessed risk (POLAR) score
for estimating the LLM error probability. We report experimental results on four different natural
language processing (NLP) tasks and show that the proposed POLAR score is well calibrated and
highly correlated with the LLM error rate evaluated on gold labels. We demonstrate improved LLM
performance using dynamic prompting techniques for high-risk instances as identified by the POLAR
score. We show this technique to correct LLM errors and boost a GPT-4 baseline performance to
outperform the state-of-the-art supervised model, without using any human-labeled training data.
2
Related Work
Early works in model calibration date back to the seminal work of Platt scaling [23], where a
Logistic calibration model is fitted on top of the original model output. Various techniques have been
developed afterwards for model calibration, including isotonic regression [36], temperature scaling
[9], and Bayesian binning [19]. A contextual calibration method for LLMs was proposed in [40],
which adjust the class balance by taking an ensemble of LLM queries with content-free input. As
accurate calibration for LLM is challenging, heuristic method such as self-consistency [31] has been
developed to estimate LLM confidence in its response. Most of these calibration work rely on labeled
calibration data, and there isn’t a systematic self-supervised approach to our knowledge.
Our work involves aggregating multiple supervision sources in the absence of training labels and this
has been extensively studied in the field of programmatic weak supervision [37]. Following early
works in distant supervision [11] and crowd-sourcing [27], data programming [26, 24] was proposed
to systematically aggregate weak labels. Numerous methods have been the field [7, 29, 30, 15], with
MeTaL (currently known as Snorkel) [25] as a popular method. Most of the existing works assign
fixed weights to all the sources across all the examples, which will be dominated by the LLM and
fail to capture the small amount of examples where the LLM is likely to be wrong. We address this
problem with Pareto optimization to be adaptive to all sources simultaneously and show that this
approach offers better LLM calibration compared to Snorkel.
3
Methodology
In this section we propose the flexible framework of Pareto optimal learning that can be adapted to
any LLM-based prediction task. We then present the formula to calculate the risk indicator score to
evaluate the confidence of an LLM response, which we refer to as the POLAR score.
2

3.1
Problem setup
Consider the joint distribution on the input-output pair space (x, y) ∈X × Y. The general workflow
of prompt-based LLM works as follows. For any input example x with specific prompt setting, the
LLM first generates a free text response. The free text response is then projected by operator P onto
the desired output space Y, plus an abstain option. The whole process can be summarized into the
function below:
Λ(x) := P(LLM(x; prompt)) ∈Y ∪{0}.
(1)
The abstain option 0 refers to any case where the free text response is not clear enough to fit into any
element in the desired output space, e.g. when the LLM is unsure about the answer. In general, we
want the LLM to give a clear response if possible, and state “unsure” when it is not confident about
the response. However, even when well-designed prompting strategies are used to achieve this goal,
the problem of hallucination still remains, where the model confidently states and answer (Λ(x) ̸= 0)
that is actually wrong (Λ(x) ̸= y). The goal of LLM calibration and hallucination detection is to
develop a risk indicator function
ζ(x, Λ(x)) ∈R+
(2)
that estimates the true probability of hallucination:
P(Λ(x) ̸= y|Λ(x) ̸= 0).
(3)
In this paper we derive the risk indicator function ζ(x, Λ(x)) using self-supervision, in which
setting, we have access to n unlabeled training examples x1, · · · , xn ∈X with ground truth outputs
completely unavailable. In addition, human expert can specify m supervision functions1 that can be
triggered to give heuristic or noisy outputs, denoted as
λj(x) ∈Y ∪{0}, j = 1, · · · , m.
(4)
Again, the output 0 refers to an abstain output where the supervision function is not triggered to
assign a label. Unlike LLMs, each supervision function is usually triggered on only a very small
portion of data (in our experiment, the average coverage rate can be as low as 1%). Even on examples
for which they assign labels, supervision functions are only required to be weak supervision sources,
with the typical theoretical requirement that the assigned label is better than a random guess [26, 33].
In practice, the supervision functions can be as simple as keywords, regular expression, or knowledge
base querying. The details are discussed in Section 4.
3.2
Pareto optimal learning
The central component in the proposed method is a harmonizer model h : X →Y, that is fit in the
semantic space, H, to be simultaneously consistent with both the LLM responses and the manually
specified supervision functions. Mathematically, the following optimization problem is to be solved:
min
h∈H
E[ℓ0(h(x), Λ(x))],
{E[ℓj(h(x), λj(x))]}m
j=1,
(5)
where all the expectations are taken for x ∼X. However, the challenge is that the multiple objectives
in this optimization problem can potentially conflict, making the problem ill-posed. Following
multi-objective learning theory [12], we resort to finding a harmonizer h∗∈H that is Pareto optimal
with the following definition [22].
Definition 1 (Pareto optimal harmonizer). h∗∈H is a Pareto optimal harmonizer to Λ and
λ1, · · · , λm, if there does not exist any h ∈H that Pareto dominates h∗in Problem 8. Mathe-
matically, if we denote λ0 := Λ, h∗needs to satisfies the following:
∄h ∈H,
s.t.

∀j = 0, 1, · · · , m,
E[ℓj(h(x), λj(x))] ≤E[ℓj(h∗(x), λj(x))],
∃j = 0, 1, · · · , m,
E[ℓj(h(x), λj(x))] < E[ℓj(h∗(x), λj(x))].
1These are also referred to as labeling functions in some literature [37]. However, we use the name supervision
function for more generalized settings.
3

Finding Pareto optimal solutions is an essential goal in the multi-objective optimization literature.
For our specific setting of learning a harmonizer model to minimize losses on all sources, we propose
to approximate the problem by minimize the following Pareto loss function G : Rm+1
+
→R+:
min
h∈H
Ex∼X [G (ℓ0(h(x), Λ(x)), ℓ1(h(x), λ1(x)), · · · , ℓm(h(x), λm(x)))].
(6)
We require G : Rm+1
+
→R+ to satisfy the following conditions.
Definition 2 (Pareto scalarizer). Loss aggregating function G(ℓ0, ℓ1, · · · , ℓm) is a Pareto scalarizer,
if it satisfies:
• G(ℓ0, · · · , ℓ′
j, · · · , ℓm) < G(ℓ0, · · · , ℓj, · · · , ℓm) if ℓ′
j < ℓj, for ∀j = 0, 1, · · · , m;
• G : Rm+1
+
→R+ is convex.
In our experiments, we explored four different types of scalarization functions, namely:
• Linear scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := Pm
j=0 wjℓj.
• Quadratic scalarizer: G(ℓ0, ℓ1, · · · , ℓm) :=
Pm
j=0 wjℓj
2
.
• Euclidean norm scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := ∥(w0ℓ0, w1ℓ1, · · · , wmℓm) ∥.
• Chebyshev scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := maxm
j=0 wjℓj.
The weights wj ∈R are parameters of G. One can check that the first three scalarizers are Pareto
scalarizer, while the last one does not satisfies the Pareto scalarizer definition, which is used as a
comparison. The proposed approach in Equation 9 is guaranteed by the following theorem.
Theorem 1. Suppose G : Rm+1
+
→R+ is a Pareto scalarizer as in Definition A.2. Solving the
problem in Equation 9 approximate a Pareto optimal harmonizer by upperbounding an objective
whose optimal solution is Pareto optimal as in Definition A.1.
We refer to the appendix for a detailed proof of the theorem. The outline of the proof is two-fold. In
the first step, Jensen’s inequality is applied to upperbound an intermediate objective. We then use the
definitions of Pareto optimal harmonizer and Pareto scalarizer and prove by contradiction.
We show in the experimental results that a proper Pareto scalarizer G automatically balances the output
from the LLM and from all the supervision functions. The algorithm to automatically determine the
weights wj will be discussed in supplementary materials.
3.3
POLAR score
In order for the harmonizer model to deliver high-quality calibration, we propose the use of cross-
entropy loss for the loss functions ℓ0, ℓ1, · · · , ℓm. We then solve problem 9 using stochastic gradient-
based algorithms such as SGD or Adam [13]. Once an optimal solution h∗∈H is found, the Pareto
optimal learning assessed risk (POLAR) score is defined as
ζ(x, Λ(x); h∗) = Ph∗(Λ(x) ̸= Y |Λ(x) ̸= 0).
(7)
Algorithm 1 below describes the procedure of training the harmonizer h∗and applying it to evaluate
a new LLM response risk.
3.4
LLM error correction with POLAR-assisted dynamic prompting
Being able to identify LLM responses that are high risk of error opens up the opportunity to correct
these responses. Towards this goal we propose two dynamic prompting strategies, both assisted by
the POLAR score, to illustrate the potential use of the POLAR score to correct LLM errors.
Dynamic self-examination
In this strategy, whenever the POLAR score ζ(x, Λ(x); h∗) > δ for
threshold δ, we ask the LLM to reflect on its answer. See appendix for detailed prompt.
4

Algorithm 1 POLAR for LLM responses
1: Input: LLM response Λ as in Equation 1, supervision functions λ1, · · · , λm as in Equation 4,
unlabeled training examples x1:n. Initialize harmonizer h ∈H.
2: for i = 1 to n do
3:
lossLLM = (Λ(xi) ̸= 0) ∗KL(h(xi), Λ(xi))
4:
for j = 1 to m do
5:
lossj = (λj(xi) ̸= 0) ∗KL(h(xi), λj(xi))
6:
end for
7:
Update h with SGD iteration of minh G(lossLLM, loss1, · · · , lossm).
8: end for
9: Output: Harmonizer h∗. For any example x and LLM response Λ(x), the POLAR score of the
response is estimated as according to Equation 7.
Algorithm 2 POLAR-assisted dynamic self-supervision for LLM
1: Input: Example x, LLM response Λ(x), supervision functions λ1, · · · , λm, harmonizer h∗.
2: if ζ(x, Λ(x); h∗) > δ then
3:
Initialize Reflection Prompt.
4:
for j = 1 to m do
5:
if λj(x) ̸= 0 then
6:
Add evidence from supervision function j to the Reflection Prompt.
7:
end if
8:
end for
9:
Respond to the LLM with the Reflection Prompt and get new response Λ′(x).
10:
return Λ′(x)
11: else
12:
return Λ(x)
13: end if
Dynamic self-supervision
In this strategy, we utilize the supervision functions as sources to help
the LLM reflect on its initial response as detailed in Algorithm 2.
4
Experiments
Dataset
We experiment on four different NLP datasets, CDR [16], ChemProt [14], SemEval [10],
and SMS [1]. While some of the datasets such as ChemProt have different versions, we refer to [38]
for consistency. The labels on the training sets are removed. Gold labels are available on the test sets
only for evaluation. We select the datasets to broadly cover the following aspects:
• Domain: General domain (SemEval, SMS), biomedical domain (CDR, ChemProt).
• Task: Relation extraction (CDR, ChemProt, SemEval), classification (SMS).
• Difficulty: The problem set includes tasks that are easily solvable by advanced LLMs like
GPT-4 such as SMS (99% F-1), as well as tasks that still challenging for GPT-4, e.g. CDR
(74% F-1), ChemProt (42% F-1), and SemEval (67% F-1).
Prompt design
In order to leverage the maximal capability of the LLMs, we carefully design the
prompt for each problem. The prompt structure is as follows (see appendix for detailed prompt
information):
• Setting: describe the role of the LLM in the task, and the overall problem setting.
• Background: necessary background knowledge for domain specific tasks, including infor-
mation from annotation guidelines for human annotators.
• Data structure: for relation extraction tasks, explain the definition of each entity.
• Desired output: describe the list of the desired output. For each of the categories, provide
explanation and optionally some examples.
5

• Chain of thought (CoT): instruction to encourage the LLM to think step-by-step, articulate
point-by-point, and give the response in the desired structure.
• Confidence: ask the model to state “unsure” if it is not confident about the answer.
• Example: state the example and ask the model to perform the task.
Supervision functions
The supervision functions for the datasets are based on simple rules provided
by human experts [24, 35, 41, 2]. The rules in the supervision functions include:
• Keywords and regular expression pattern checking.
• Knowledge base querying, e.g. the Comparative Toxicogenomics Database [5].
• Hierarchical combination of the above.
Training details
We implement Pareto optimal learning as in Equation 9 and Algorithm 1. We
explored different configurations of Pareto optimal learning below:
• Harmonizer model: we experiment 1. BERT [6] (PubMedBERT [8] for biomedical datasets
CDR and ChemProt), 2. multi-layer perceptron (MLP), 3. Logistic regression (LR). The
last two are built on top of the last layer embedding of the corresponding BERT model.
• Pareto loss scalarizer: we experiment all four loss scalarization functions as defined in
Section 3.2, namely linear, quadratic, Euclidean norm, and Chebyshevy scalarization.
• Optimizer: We use AdamW [17] optimizer with learning rate [10−4, 10−5, 10−6], weight
decay [10−4, 10−5], batch size 16. All hyperparameters are optimized on held out dev set.
• Computation: We trained on Azure Standard NC12s v3 with Nvidia V100 GPU.
Results presented in this section are restricted to BERT (PubMedBERT) as the harmonizer model
and quadratic scalarization for loss scalarizer with equal weights. We examine the contribution of
different harmonizer models and Pareto loss scalarizers in Section 5.
4.1
POLAR calibration of LLM error
In this section we present our implementation results of Algorithm 1. The fitted harmonizer is applied
to unseen test set to give a POLAR score for each LLM response. Gold labels on the test set are used
to estimate the actual error rate in Equation 3, and to evaluate how well the proposed POLAR score
is calibrated and correlated with the actual error rate.
Fig. 1 shows the POLAR score calibration results for GPT-4 on the CDR chemical-disease relation
extraction task. The calibration curve in Figure 1a shows that the POLAR score is a good estimator
of the true probability that the LLM is making a mistake, as defined in Equation 3. Figure 1b shows
that the POLAR score is highly correlated with the error rate. Lastly, Figure 1c shows that for all
three different LLMs, the responses with the highest POLAR score are the most likely to generate
hallucination or errors, and the highest POLAR score indicate almost 100% error rate.
Tabel 1 shows the performance of POLAR score in terms of LLM error calibration. We provide two
metrics, expected calibration error (ECE) and R2 between the POLAR score and the actual error
rate, both calculated the same as in Figure 1. We report the results for all four datasets, and for three
LLMs: GPT-4, GPT-3.5-turbo, and text-davinci-003. We compare our results with four different
baseline methods:
• Snorkel: Combining multiple sources of supervision via matrix completion [25]. We fit the
model on the training set combining LLM response and supervision function outputs, and
then use the class probability given by the model to estimate LLM error rate on the test set.
The implementation of the algorithm is formally known as MeTaL, which was evaluated as
one of the most recommended weak supervision method according to [38].
• Majority vote: Estimate class probability according to the voted ratios among the LLM and
the supervision functions. This method was evaluated as the other most recommended weak
supervision method according to [38].
6

0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
POLAR risk score
0.0
0.2
0.4
0.6
0.8
1.0
Error rate on gold label
ECE:  0.043
POLAR Calibration Plot        GPT-4    CDR
(a) Error calibration curve
0.2
0.4
0.6
0.8
Average POLAR risk score
0.0
0.2
0.4
0.6
0.8
1.0
Error rate on gold labels
R-squared:  0.890
POLAR Score v.s. Error Rate    CDR  GPT-4
Observed error rate
Least squares fit
(b) Correlation with error rate
0
20
40
60
80
100
Percentile of Top Risky Responses (%)
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Response error rate
Error rate among top risky LLM responses    CDR
GPT-4
GPT-3.5-turbo
Text-davinci-003
(c) LLM error detection
Figure 1: LLM error calibration and hallucination detection using POLAR score. Figure 1a shows
the error rate on the test set for LLM responses ranked into ten POLAR score bins. The expected
calibration error (ECE) is the weighted average of the absolute deviation of POLAR score from the
actual error rate in each bin. Figure 1b ranks the LLM responses into POLAR score bins each with
100 examples, and plot the average PLOAR score and error rate for each bin. The R2 is calculated on
the scatter plot. Figure 1c shows the LLM response error rate among the top percentile examples in
terms of POLAR score.
• LLM distilled: Instead of Pareto optimal learning from the LLM and the supervision
functions, fit a BERT (PubMedBERT) model to the LLM output only. Take the distribution
from the finetuned BERT model to estimate LLM error rate.
• LLM ensemble: Making multiple queries to the LLM, and estimate class probability from
the response ensemble. This approach is extremely expensive, so we only implement for
GPT-3.5-turbo on CDR. The resulting ECE = 0.4466, which is far from comparable to the
proposed approach, so we do not include in the tabel to save space.
4.2
LLM error correction with POLAR-assisted dynamic prompting
In this experiment we explore different dynamic prompting strategies (Section 3.4) to reduce LLM
errors. As this is an extension task to our main result, we only focus on the CDR relation extraction
task, and experiment on the GPT-4 and GPT-3.5-turbo models that support conversational interaction.
To illustrate the importance of the POLAR score in our dynamic prompting strategies, we sort the
initial LLM responses by their POLAR score, and compute the error rate of LLM responses before
and after dynamic prompting. Figure 2a shows that the GPT-4 error rate decreases significantly for
both strategies, if and only if the POLAR score was high on the initial response. If the first response
from the LLM was evaluated to be of low risk, asking it to self-examine or providing self-supervision
7

Table 1: LLM error calibration using POLAR score, compared with other methods. The best entries
from each row in terms of low ECE and high R2 are highlighted in bold. We can see that for all three
LLMs across all four tasks, the proposed POLAR score consistently outperform all other methods.
Among the baseline methods, Snorkel and LLM distilled model can achieve top or close-to-top
performance in some cases under specific metric, but lack the consistency to deliver stable calibration
for different LLMs on different tasks. In comparison, the proposed POLAR score is consistently
well-calibrated to the true error rate.
POLAR
Snorkel
Majority vote
LLM Distilled
LLM (CDR)
ECE
R2
ECE
R2
ECE
R2
ECE
R2
GPT-4
0.0431
0.8898
0.1669
0.2990
0.1450
0.3482
0.1643
0.5918
GPT-3.5-turbo
0.0463
0.9340
0.1642
0.3200
0.1817
0.5395
0.2216
0.0371
Text-davinci-003
0.0550
0.9071
0.1536
0.3705
0.1492
0.4499
0.0892
0.8964
LLM (ChemProt)
ECE
R2
ECE
R2
ECE
R2
ECE
R2
GPT-4
0.0351
0.9340
0.1824
0.5102
0.2334
0.2439
0.2161
0.7663
GPT-3.5-turbo
0.0481
0.9436
0.2283
0.6253
0.2822
0.0307
0.1590
0.8447
Text-davinci-003
0.0509
0.9170
0.2176
0.6995
0.2794
0.3068
0.1961
0.8248
LLM (SemEval)
ECE
R2
ECE
R2
ECE
R2
ECE
R2
GPT-4
0.0792
0.9157
0.0683
0.7140
0.1145
0.3785
0.0627
0.9470
GPT-3.5-turbo
0.0473
0.9631
0.1498
0.8212
0.2773
0.2081
0.1078
0.7571
Text-davinci-003
0.0665
0.9495
0.1186
0.7962
0.2417
0.3961
0.0647
0.9358
LLM (SMS)
ECE
R2
ECE
R2
ECE
R2
ECE
R2
GPT-4
0.0144
0.9802
0.2435
0.0887
0.5882
0.0914
0.0127
0.9768
GPT-3.5-turbo
0.0409
0.9627
0.0753
0.2021
0.1481
0.0062
0.1312
0.7754
Text-davinci-003
0.0229
0.9427
0.2006
0.0528
0.3250
0.0909
0.0328
0.9558
rules does not improve the result, and may even increase the error rate. Therefore, the best strategy is
to only perform dynamic prompting when seeing a high POLAR score.
Figure 2b shows the implementation result of dynamic self-examination and dynamic self-supervision
as in Algorithm 2. We choose the POLAR score threshold as δ = 0.5. We can see that the two
strategies increase the performance for both GPT-3.5-turbo and GPT-4 models. With the dynamic
self-supervision, GPT-3.5-turbo surpasses the best result so far that was achieved without gold labeled
training data [38], and GPT-4 outperforms the state of the art supervised method utilizing fully-labeled
data [34]. It is noteworthy that both strategies make use of zero gold-labeled examples.
5
Ablation Studies
We dedicate this section to discuss the contribution of different components in our approach to the
error calibration ability.
Pareto optimal self-supervision
We see this as the most essential part in our approach. Controlling
all other components the same, including unlabeled training data, LLM response, and supervision
function output, we refer to the experimental results using Snorkel and majority vote in Table 1. We
can see that both methods can not deliver consistently good calibration power, showing that Pareto
optimal self-supervision plays an important role.
External sources of information
The supervision functions serve as external sources of information
in helping the harmonizer to be well-calibrated, and not biased by over-fitting to the LLM responses.
To compare with the scenario where external sources of information is absent, we refer to the LLM
distilled model columns in Table 1. We can see that without the supervision functions, the calibration
ability is not consistently good due to overfitting to the LLM.
8

0.2
0.4
0.6
0.8
POLAR risk score
0.0
0.2
0.4
0.6
0.8
1.0
Response error rate
Error rate before and after dynamic prompting
Initial GPT-4 response
GPT-4 response after self-examination
GPT-4 response after self-supervision
(a) POLAR score and LLM error rate change
Zero-shot
Self-examination
Self-supervision
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.90
F-1 score
POLAR-assisted dynamic prompting
Best w/o gold label
Supervised SOTA
GPT-3.5-turbo
GPT-4
(b) Dynamic prompting performance
Figure 2: POLAR-assisted dynamic prompting. Figure 2a shows GPT-4 error rate before and after
dynamic prompting, conditional on the initial POLAR score. Figure 2b shows the performance
improvement using the two dynamic prompting strategies.
Pareto loss scalarizer
Table 2 shows the ECE and R2 measure for different loss scalarizer function
G, and different harmonizer model type, averaged over all three LLMs on the four tasks. We can see
that the nonlinear quadratic loss scalarizer paired with BERT finetuning gives the best calibration
ability. For simpler models like MLP and LR, simple linear scalarizer works best. Also note that the
Chebyshev/max scalarizer has the worst performance in almost all cases. This experimental results
supports Theorem A.1 that a Pareto loss scalarizer as in Definition A.2 is necessary to approximate a
Pareto optimal harmonizer as defined in Definition A.1.
Table 2: Average calibration ability for different loss scalarizer G and harmonizer type.
G function
Linear
Quadratic
Euclidean norm
Chebyshev/Max
Harmonizer type
ECE
R2
ECE
R2
ECE
R2
ECE
R2
BERT
0.0625
0.9273
0.0458
0.9366
0.0549
0.9003
0.0711
0.8260
MLP
0.0555
0.9392
0.0974
0.9188
0.0691
0.9302
0.0775
0.8934
LR
0.0641
0.9360
0.1072
0.9020
0.0766
0.9288
0.0948
0.8813
6
Limitation
Our approach relies on supervision functions as external sources of information. Although they don’t
necessarily need to be accurate, ensuring a decent quality is important to the result. Therefore, the
effort to create reasonable supervision functions can be a limitation in application. The proposed
framework is also limited to the broader category of classification, which presume a finite target space.
However, in many applications of generative LLM, the output space is not well-defined. Future work
is needed to generalize to other problems.
7
Conclusion
In this paper, we propose a novel framework for LLM calibration using Pareto optimal self-
supervision, without using labeled training data. In our experiments, the proposed POLAR score is
consistently well-calibrated to the probability of LLM error. We introduce POLAR-based dynamic
prompting to correct LLM errors, which boosts a GPT-4 baseline performance to outperform the
SOTA supervised model, without using any human-labeled training data.
9

References
[1] T. A. Almeida, J. M. G. Hidalgo, and A. Yamakami. Contributions to the study of sms spam
filtering: new collection and results. In Proceedings of the 11th ACM symposium on Document
engineering, pages 259–262, 2011.
[2] A. Awasthi, S. Ghosh, R. Goyal, and S. Sarawagi. Learning from rules generalizing labeled
exemplars. arXiv preprint arXiv:2004.06025, 2020.
[3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems, 33:1877–1901, 2020.
[4] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,
P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,
J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev,
H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,
H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,
T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,
X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean,
S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.
[5] A. P. Davis, C. J. Grondin, R. J. Johnson, D. Sciaky, J. Wiegers, T. C. Wiegers, and C. J.
Mattingly. Comparative toxicogenomics database (ctd): update 2021. Nucleic acids research,
49(D1):D1138–D1143, 2021.
[6] J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. CoRR, abs/1810.04805, 2018.
[7] D. Fu, M. Chen, F. Sala, S. Hooper, K. Fatahalian, and C. Ré. Fast and three-rious: Speeding
up weak supervision with triplet methods. In International Conference on Machine Learning,
pages 3280–3291. PMLR, 2020.
[8] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon.
Domain-specific language model pretraining for biomedical natural language processing, 2020.
[9] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger. On calibration of modern neural networks. In
International conference on machine learning, pages 1321–1330. PMLR, 2017.
[10] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. O. Séaghdha, S. Padó, M. Pennacchiotti,
L. Romano, and S. Szpakowicz. Semeval-2010 task 8: Multi-way classification of semantic
relations between pairs of nominals. arXiv preprint arXiv:1911.10422, 2019.
[11] R. Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, and D. S. Weld. Knowledge-based weak
supervision for information extraction of overlapping relations. In Proceedings of the 49th
annual meeting of the association for computational linguistics: human language technologies,
pages 541–550, 2011.
[12] C.-L. Hwang and A. S. M. Masud. Multiple objective decision making—methods and applica-
tions: a state-of-the-art survey, volume 164. Springer Science & Business Media, 2012.
[13] D. P. Kingma and J. Ba.
Adam: A method for stochastic optimization.
arXiv preprint
arXiv:1412.6980, 2014.
[14] M. Krallinger, O. Rabal, S. A. Akhondi, M. P. Pérez, J. Santamaría, G. P. Rodríguez, G. Tsat-
saronis, A. Intxaurrondo, J. A. López, U. Nandal, et al.
Overview of the biocreative vi
chemical-protein interaction track. In Proceedings of the sixth BioCreative challenge evaluation
workshop, volume 1, pages 141–146, 2017.
[15] H. Lang and H. Poon. Self-supervised self-supervision by combining deep learning and
probabilistic logic. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pages 4978–4986, 2021.
10

[16] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C.
Wiegers, and Z. Lu. Biocreative v cdr task corpus: a resource for chemical disease relation
extraction. Database, 2016, 2016.
[17] I. Loshchilov and F. Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017.
[18] R. Müller, S. Kornblith, and G. E. Hinton. When does label smoothing help? Advances in
neural information processing systems, 32, 2019.
[19] M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using
bayesian binning. In Proceedings of the AAAI conference on artificial intelligence, volume 29,
2015.
[20] OpenAI. Gpt-4 technical report, 2023.
[21] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal,
K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder,
P. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with
human feedback, 2022.
[22] V. Pareto. Cours d’économie politique, volume 1. Librairie Droz, 1964.
[23] J. Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in large margin classifiers, 10(3):61–74, 1999.
[24] A. Ratner, S. H. Bach, H. Ehrenberg, J. Fries, S. Wu, and C. Ré. Snorkel: Rapid training
data creation with weak supervision. In Proceedings of the VLDB Endowment. International
Conference on Very Large Data Bases, volume 11, page 269. NIH Public Access, 2017.
[25] A. Ratner, B. Hancock, J. Dunnmon, F. Sala, S. Pandey, and C. Ré. Training complex mod-
els with multi-task weak supervision. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pages 4763–4771, 2019.
[26] A. J. Ratner, C. M. De Sa, S. Wu, D. Selsam, and C. Ré. Data programming: Creating large
training sets, quickly. Advances in neural information processing systems, 29, 2016.
[27] V. C. Raykar, S. Yu, L. H. Zhao, G. H. Valadez, C. Florin, L. Bogoni, and L. Moy. Learning
from crowds. Journal of machine learning research, 11(4), 2010.
[28] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv
preprint arXiv:2302.13971, 2023.
[29] P. Varma, F. Sala, S. Sagawa, J. Fries, D. Fu, S. Khattar, A. Ramamoorthy, K. Xiao, K. Fatahalian,
J. Priest, et al. Multi-resolution weak supervision for sequential data. Advances in Neural
Information Processing Systems, 32, 2019.
[30] H. Wang and H. Poon. Deep probabilistic logic: A unifying framework for indirect supervision.
arXiv preprint arXiv:1808.08485, 2018.
[31] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain
of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[32] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought
prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.
[33] R. Wu, S.-E. Chen, J. Zhang, and X. Chu. Learning hyper label model for programmatic weak
supervision. In The Eleventh International Conference on Learning Representations.
[34] Y. Xiao, Z. Zhang, Y. Mao, C. Yang, and J. Han. Sais: supervising and augmenting intermediate
steps for document-level relation extraction. arXiv preprint arXiv:2109.12093, 2021.
11

[35] Y. Yu, S. Zuo, H. Jiang, W. Ren, T. Zhao, and C. Zhang. Fine-tuning pre-trained language
model with weak supervision: A contrastive-regularized self-training approach. In Proceedings
of the 2021 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages 1063–1077, 2021.
[36] B. Zadrozny and C. Elkan. Transforming classifier scores into accurate multiclass probability
estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 694–699, 2002.
[37] J. Zhang, C.-Y. Hsieh, Y. Yu, C. Zhang, and A. Ratner. A survey on programmatic weak
supervision. arXiv preprint arXiv:2202.05433, 2022.
[38] J. Zhang, Y. Yu, Y. Li, Y. Wang, Y. Yang, M. Yang, and A. Ratner. Wrench: A comprehensive
benchmark for weak supervision. arXiv preprint arXiv:2109.11377, 2021.
[39] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,
et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023.
[40] Z. Zhao, E. Wallace, S. Feng, D. Klein, and S. Singh. Calibrate before use: Improving few-shot
performance of language models. In International Conference on Machine Learning, pages
12697–12706. PMLR, 2021.
[41] W. Zhou, H. Lin, B. Y. Lin, Z. Wang, J. Du, L. Neves, and X. Ren. Nero: A neural rule grounding
framework for label-efficient relation extraction. In Proceedings of The Web Conference 2020,
pages 2166–2176, 2020.
12

A
Proof of Theorems
The central component in the proposed method is a harmonizer model h : X →Y, that is fit in the
semantic space, H, to be simultaneously consistent with both the LLM responses and the manually
specified supervision functions. Mathematically, the following optimization problem is to be solved:
min
h∈H
E[ℓ0(h(x), Λ(x))],
{E[ℓj(h(x), λj(x))]}m
j=1,
(8)
where all the expectations are taken for x ∼X. However, the challenge is that the multiple objectives
in this optimization problem can potentially conflict, making the problem ill-posed. Following
multi-objective learning theory, we resort to finding a harmonizer h∗∈H that is Pareto optimal with
the following definition.
Definition A.1 (Pareto optimal harmonizer). h∗∈H is a Pareto optimal harmonizer to Λ and
λ1, · · · , λm, if there does not exist any h ∈H that Pareto dominates h∗in Problem 8. Mathematically,
if we denote λ0 := Λ, h∗needs to satisfies the following:
∄h ∈H,
s.t.

∀j = 0, 1, · · · , m,
E[ℓj(h(x), λj(x))] ≤E[ℓj(h∗(x), λj(x))],
∃j = 0, 1, · · · , m,
E[ℓj(h(x), λj(x))] < E[ℓj(h∗(x), λj(x))].
Finding Pareto optimal solutions is an essential goal in the multi-objective optimization literature.
For our specific setting of learning a harmonizer model to minimize losses on all sources, we propose
to approximate the problem by minimize the following Pareto loss function G : Rm+1
+
→R+:
min
h∈H
Ex∼X [G (ℓ0(h(x), Λ(x)), ℓ1(h(x), λ1(x)), · · · , ℓm(h(x), λm(x)))].
(9)
We require G : Rm+1
+
→R+ to satisfy the following conditions.
Definition A.2 (Pareto scalarizer). Loss aggregating function G(ℓ0, ℓ1, · · · , ℓm) is a Pareto scalarizer,
if it satisfies:
• G(ℓ0, · · · , ℓ′
j, · · · , ℓm) < G(ℓ0, · · · , ℓj, · · · , ℓm) if ℓ′
j < ℓj, for ∀j = 0, 1, · · · , m;
• G : Rm+1
+
→R+ is convex.
Theorem A.1. Suppose G : Rm+1
+
→R+ is a Pareto scalarizer as in Definition A.2. Solving the
problem in Equation 9 approximate a Pareto optimal harmonizer by upperbounding an objective
whose optimal solution is Pareto optimal as in Definition A.1.
Proof. For convenience, let’s denote
uj(h) := E[ℓj(h(x), λj(x))],
j = 0, 1, · · · , m.
We first show that any h∗minimizing G(u0, u1, · · · , um) is Pareto optimal.
Proof by contradiction. Suppose h∗is not Pareto optimal. Then there must exist some h′ ∈H Pareto
dominating h∗. Without loss of generality, let’s assume uj(h′) < uj(h∗), and uk(h′) ≤uk(h∗),
∀k ̸= j. Then according to Definition A.2 of Pareto scalarizer,
G(u0(h′), · · · , uj(h′), · · · , um(h′))
(10)
≤G(u0(h∗), · · · , uj(h′), · · · , um(h∗))
(11)
<G(u0(h∗), · · · , uj(h∗), · · · , um(h∗)),
(12)
which contradicts the assumption that h∗is the minimizer for
G(u0(h), · · · , uj(h), · · · , um(h)).
Therefore, the original statement is true, and minimizing the objective
min
h∈H
G (E[ℓ0(h(x), Λ(x))], E[ℓ1(h(x), λ1(x))], · · · , E[ℓm(h(x), λm(x))])
(13)
13

gives a Pareto optimal harmonizer.
Next, we use Jensen’s inequality to upperbound this objective with the objective in problem 9. Using
the fact that G is convex, we apply Jensen’s inequality and get
G (E[ℓ0(h(x), Λ(x))], E[ℓ1(h(x), λ1(x))], · · · , E[ℓm(h(x), λm(x))])
(14)
≤
Ex∼X [G (ℓ0(h(x), Λ(x)), ℓ1(h(x), λ1(x)), · · · , ℓm(h(x), λm(x)))].
(15)
Therefore, solving the problem in Equation 9 approximates Pareto optimal harmonizer by upper-
bounding Equation 13.
B
Weights for Rebalancing the Sources
In our experiments, we explored four different types of scalarization functions, namely:
• Linear scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := Pm
j=0 wjℓj.
• Quadratic scalarizer: G(ℓ0, ℓ1, · · · , ℓm) :=
Pm
j=0 wjℓj
2
.
• Euclidean norm scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := ∥(w0ℓ0, w1ℓ1, · · · , wmℓm) ∥.
• Chebyshev scalarizer: G(ℓ0, ℓ1, · · · , ℓm) := maxm
j=0 wjℓj.
The weights wj ∈R are parameters of G. We introduce three approaches to determine the weighting.
Equal Weight
The simplest weighting scheme of
w0 = w1 = · · · = wm =
1
m + 1
gives nice performance in practice, and is the method we used for the results in the main body of
the paper. The nonlinear Pareto scalarizers have the ability to balance the sources even under equal
weights. It is always recommended to start with equal weight.
In the case that the supervision sources are highly correlated, or when the quality of the sources
varies a lot, we propose the following two approaches utilizing the correlation in prediction residual.
Maximal Eigenvalue of Residual Correlation
Suppose we have a pilot harmonizer h0 ∈H, which
can usually be obtained from minimizing a Pareto scalarizer with equal weight, it gives predicted
distribution ⃗p(x) ∈R|Y| for any input x ∈X, where
pc(x) := P(h0(x) = c).
For any source 0 ≤j ≤m, denote the one-hot vector ⃗λj(x) ∈R|Y| as:
λj,c(x) =

1
if λj(x) = c,
0
otherwise.
The prediction redisual is defined as
⃗rj(x) := ⃗λj(x) −⃗p(x),
which accounts for the supervision function label for x that is unexplained by the harmonizer h0(x).
In order to rebalance the sources, we consider the correlation matrix C between the prediction residual
⃗rj’s. Specifically, let the covariance be
Σij := Ex∼X [⃗ri(x) · ⃗rj(x)] −Ex∼X [⃗ri(x)] · Ex∼X [⃗rj(x)].
14

The correlation variance is denoted as
Cij =
Σij
p
ΣiiΣjj
.
We rebalance the sources according to the eigenvector ⃗vmax ∈Rm+1 corresponding to the largest
eigenvalue of C. In order to get reasonable weights, we first normalize ⃗vmax such that the sum of the
entries equals to one. Then we project ⃗vmax to the weights simplex with minimal values
ϵ
m+1:
wexcess,j =

⃗vmax,j −
ϵ
m + 1
+
⃗wmax =
ϵ
m + 1 +
wexcess
∥wexcess∥1
· (1 −ϵ).
This projection ensures that the weight on each source is at least ϵ portion of the value from equal
weight, with the minimal ratio threshold ϵ ∈[0, 1].
Maximal eigenvalue method is recommended when the sources are relatively independent, and when
the quality of the sources differ a lot. Intuitively, suppose two sources tends to agree with each other
when they are not fitted well by the harmonizer, because there isn’t intrinsic dependency between
the sources, it is likely that the true label is given by the sources. Therefore, a maximal eigenvalue
rebalancing scheme puts higher weights on the sources to encourage the harmonizer to fit to the
unexplained examples.
Minimal Variance of Residual Correlation
The same as in the maximal eigenvalue method, we
consider the correlation matrix C between the prediction residual ⃗rj’s. Instead of finding the maximal
eigenvalue of C, we consider solving the following minimal variance problem:
min
v
vT Cv,
s.t. 1T v = 1.
This problem admits the closed form solution of
vmin =
C−11
1T C−11.
Again, we project ⃗vmin to the weights simplex with minimal values
ϵ
m+1:
wexcess,j =

⃗vmin,j −
ϵ
m + 1
+
⃗wmin =
ϵ
m + 1 +
wexcess
∥wexcess∥1
· (1 −ϵ),
which ensures that the weight on each source is at least ϵ portion of the value from equal weight, with
the minimal ratio threshold ϵ ∈[0, 1].
The minimal variance method is a classical portfolio rebalancing strategy in financial mathematics.
The intuition behind the algorithm is minimizing the risk by diversification. This rebalancing
scheme is useful when there are intrinsic dependency between the sources. Suppose two sources are
duplicates and always tend to give the same label, their residuals should also be highly correlated.
Minimal variance optimization automatically avoid putting too much weights on the duplicating
sources.
While the equal weight method typically delivers good results in the simplest way, the other two
rebalancing schemes are designed to address the specific concern such as source dependency and
quality. It is always recommended to check against the labels on a validation set if available.
15

C
LLM Prompting Details
In this section we will describe the details of the prompts used to query the LLMs.
C.1
Out-of-the-box prompt
Each prompt for out-of-the-box (zero-shot) prediction contains:
• A problem setting part that depends on the specific dataset.
• A response regularization part that encourages chain-of-thought (CoT) and confidence check,
and specifies proper response format.
• A task instance part that contains the input instance and restates the task to perform.
Problem setting prompt
• CDR: “You are an intelligent assistant to extract chemical-disease relations from academic
literature. Your job is to determine if in the given piece of text, the drug (entity 1) induces
the disease (entity 2) or not. Negative means the drug does NOT induce the disease.
Positive means the drug induces the disease. Please use your judgement to the best of your
knowledge. Your answer should be classified into the following categories: [Negative,
Positive]. ”
• ChemProt: “You are an intelligent assistant to extract chemical-protein interaction from
academic literature. Your task is to identify the chemical-protein interactions (CHEMPROT)
between entity 2: Chemical Entities Mentions (CEMs) and entity 1: Gene and Protein
Related Objects (named as GPRO in the instruction below) in the given piece of text. In
brief, the chemical-protein interactions include direct interactions (when a physical contact
exits between a CEM and a GPRO, in most cases this GPRO being a protein or protein
family and alters its function/activity) as well as indirect regulatory interactions between
CEMs and GPROs (including genes, gene products (proteins, RNA), DNA/protein sequence
elements and protein families, domains and complexes) that alter either the function or the
quantity of the GPRO. The guidelines below provide curation rules to evaluate if the given
sentence contains a description of a chemical-protein interaction; in particular, if sufficient
detail/evidence is provided for comentioned CEMs and GPROs. Additionally, it describes
curation rules and definitions to assign each identified chemical-protein interaction to any of
the 10 classes, with detailed description listed below:
0. Part of: CEM that are structurally related to a GPRO: e.g. specific amino acid
residues of a protein.
1. Regulator: CEM that clearly regulates a GPRO, but for which there is no further
information on whether the regulation is direct or indirect.
2. Upregulator: CEM that increments a GPRO signal, without any insight on the
mechanism.
3. Downregulator: CEM that decreases a GPRO signal, without any insight on the
mechanism.
4. Agonist: CEM that binds to a receptor and alters the receptor state resulting in a
biological response.
5. Antagonist: CEM that reduces the action of another CEM, generally an agonist.
Many antagonists act at the same receptor macromolecule as the agonist.
6. Modulator: CEM that acts as allosteric modulator, compound that increases or
decreases the action of an (primary or orthosteric) agonist or antagonist by combining
with a distinct (allosteric or allotropic) site on the receptor macromolecule.
7. Cofactor: CEM that is required for a protein’s biological activity to happen.
8. Substrate/Product: CEM that is both, substrate and product of enzymatic reaction.
9. NOT: This class should be used to define the NEGATIVE occurrence of a chemical-
protein interaction, without providing any further information on the specific negative
CHEMPROT class or class.
16

Please identity the CHEMPROT interaction to the best of your knowledge. Your answer
should be classified into the following categories: [Part of, Regulator, Upregulator,
Downregulator, Agonist, Antagonist, Modulator, Cofactor, Substrate/Product, NOT]. ”
• SemEval: “You are an intelligent assistant to help recognize semantic relations between
pairs of nomimals. For example, tea and ginseng are in an ENTITY-ORIGIN relation in
"The cup contained tea from dried ginseng.". You will be given a piece of text, and Entity 1
and Entity 2 in the text for you to classify their semantic relation. The semantic relations are
in the format of "entity1-entity2". The complete semantic relation inventory is given below:
0. Cause-Effect: An event or object (entity 1) leads to an effect (entity 2). Example:
those cancers (entity 2) were caused by radiation exposures (entity 1)
1. Component-Whole: An object (entity 1) is a component of a larger whole (entity 2).
Example: my apartment (entity 2) has a large kitchen (entity 1)
2. Content-Container: An object (entity 1) is physically stored in a delineated area of
space (entity 2). Example: a bottle (entity 2) full of honey (entity 1) was weighed
3. Entity-Destination: An entity (entity 1) is moving towards a destination (entity 2).
Example: the boy (entity 1) went to bed (entity 2)
4. Entity-Origin: An entity (entity 1) is coming or is derived from an origin (entity 2)
(e.g., position or material). Example: letters (entity 1) from foreign countries (entity 2)
5. Instrument-Agency: An agent (entity 2) uses an instrument (entity 1). Example:
phone (entity 1) operator (entity 2)
6. Member-Collection: A member (entity 1) forms a nonfunctional part of a collection
(entity 2). Example: there are many trees (entity 1) in the forest (entity 2)
7. Message-Topic: A message (entity 1), written or spoken, is about a topic (entity 2).
Example: the lecture (entity 1) was about semantics (entity 2)
8. Product-Producer: A producer (entity 2) causes a product (entity 1) to exist. Example:
a factory (entity 2) manufactures suits (entity 1)
Please determine the semantic relation between entity 1 and entity 2 in the given text to
the best of your knowledge. Your answer should be classified into the following categories:
[Cause-Effect, Component-Whole, Content-Container, Entity-Destination, Entity-Origin,
Instrument-Agency, Member-Collection, Message-Topic, Product-Producer]. ”
• SMS: “You are an intelligent assistant to determine if a text message is spam or not spam
(ham). Your answer should be classified into the following categories: [ham, spam]. ”
Response regularization prompt
“You may think step by step, articulate point by point, or make
conclusion from multiple evidences, but please always state the most likely label as your answer at
the very begining of your response. You are encouraged to reflect on your response, but please keep
in mind that a clear answer is always desired. Try to give a clear answer at your best guess even when
you are not very sure, in which case any of your conserns or explanations should go after the most
likely answer to the best of your knowledge. If you are very unsure about the answer and are not
willing to explicitly state any label, please say ’unsure’ at the very begining of your response. ”
Task instance prompt
• Classification (for SMS):
“Please classify the following example into the most likely category: [TEXT] ”
• Relation extraction (for CDR, ChemProt, SemEval):
“Please classify the following example into the most likely category: [TEXT] Entity 1
[ENTITY 1] Entity 2: [ENTITY 2] ”
The complete prompt for querying the LLM is
Problem setting prompt + Response regularization prompt + Task instance prompt
17

C.2
Dynamic prompting
In dynamic prompting, we query another follow-up prompt after the LLM gives the initial out-of-the-
box response. As this is an extension to our main experiments, we only implemented for the CDR
relation extraction task. The follow-up prompts for the two dynamic prompting strategies are:
Dynamic self-examination
“Are you sure about your previous answer? If not, please give a new
answer. Otherwise, please restate your previous answer. ”
Dynamic self-supervision
“It is possible that the answer could be something else. Here are some
evidences to help you figure out the right answer.
EvidencesFromSupervisionFunctions(x, ⃗λ(x))
Are you sure about your previous answer? If not, please give a new answer. Otherwise, please restate
your previous answer. ”
EvidencesFromSupervisionFunctions(x, ⃗λ(x)) contains evidences from all the supervision
functions λj(x) ̸= 0 that are triggered by the input instance x. Examples of evidence from the
supervision functions are shown below. Note that each evidence will be provided only when the
corresponding supervision function is triggered.
• “According to the Comparative Toxicogenomics Database, the relation between the given
chemical-condition pair is listed, confirming the answer. ”
• “According to the Comparative Toxicogenomics Database, the given chemical-condition
pair "[ENTITY 1]-[ENTITY 2]" is listed that the chemical actually treats the condition, so
the answer that [ENTITY 1] does not induce [ENTITY 2] is confirmed. ”
• “According to the Comparative Toxicogenomics Database, the given chemical-condition pair
"[ENTITY 1]-[ENTITY 2]" is listed that the chemical is typically present with the condition,
which may confirm the answer if [ENTITY 1] induces [ENTITY 2]. ”
• “Based on the expression [INDUCE PATTERN], it is likely that [ENTITY 1] induces
[ENTITY 2]. ”
• “Based on the expression [NOT INDUCE PATTERN], it is not likely that [ENTITY 1]
induces [ENTITY 2]. ”
• “Based on the expression [C TREATS D PATTERN], [ENTITY 1] actually treats [ENTITY
2]. , so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
• “Based on the expression [CLOSE MENTION PATTERN], [ENTITY 1] is closely mentioned
with [ENTITY 2], so they should be closely related. ”
• “Based on the expression [DISEASE IMPROVE PATTERN], the disease [ENTITY 2] is
actually improved, so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
• “Based on the expression [INITIAL CONDITION PATTERN], [ENTITY 2] is the initial
condition of the patient(s), so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
• “Based on the expression [UNCERTAIN PATTERN], it is uncertain that [ENTITY 1] induces
[ENTITY 2]. ”
• “Based on the expression [INDUCED BY OTHER PATTERN], [ENTITY 2] is induced by
other factors, so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
• “[ENTITY 1] and [ENTITY 2] are not closely mentioned in the text, so it is not likely that
[ENTITY 1] induces [ENTITY 2]. ”
• “According to phrases like [WEAK EXPRESSION], there is no strong signal that [ENTITY
1] induces [ENTITY 2]. ”
• “According to the text, another chemical is mentioned closer to [ENTITY 2] than [ENTITY
1], so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
• “According to the text, another disease is mentioned closer to [ENTITY 1] than [ENTITY
2], so it is not likely that [ENTITY 1] induces [ENTITY 2]. ”
18

