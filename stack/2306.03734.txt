A Cross-Linguistic Pressure for
Uniform Information Density in Word Order
Thomas Hikaru Clark1
Clara Meister2
Tiago Pimentel3
Michael Hahn4
Ryan Cotterell2
Richard Futrell5
Roger Levy1
1MIT
2ETH Zürich
3University of Cambridge
4Saarland University
5UC Irvine
thclark@mit.edu meistecl@inf.ethz.ch tp472@cam.ac.uk
mhahn@lst.uni-saarland.de ryan.cotterell@inf.ethz.ch
rfutrell@uci.edu
rplevy@mit.edu
Abstract
While natural languages differ widely in
both canonical word order and word or-
der flexibility, their word orders still follow
shared cross-linguistic statistical patterns,
often attributed to functional pressures. In
the effort to identify these pressures, prior
work has compared real and counterfac-
tual word orders. Yet one functional pres-
sure has been overlooked in such investi-
gations:
the uniform information density
(UID) hypothesis, which holds that infor-
mation should be spread evenly through-
out an utterance. Here, we ask whether a
pressure for UID may have influenced word
order patterns cross-linguistically. To this
end, we use computational models to test
whether real orders lead to greater informa-
tion uniformity than counterfactual orders.
In our empirical study of 10 typologically
diverse languages, we find that: (i) among
SVO languages, real word orders consis-
tently have greater uniformity than reverse
word orders, and (ii) only linguistically im-
plausible counterfactual orders consistently
exceed the uniformity of real orders. These
findings are compatible with a pressure for
information uniformity in the development
and usage of natural languages.1
1
Introduction
Human languages differ widely in many respects,
yet there are patterns that appear to hold consis-
tently across languages. Identifying explanations
for these patterns is a fundamental goal of linguis-
tic typology. Furthermore, such explanations may
shed light on the cognitive pressures underlying
and shaping human communication.
This work studies the uniform information den-
sity (UID) hypothesis as an explanatory principle
for word order patterns (Fenk and Fenk, 1980;
Genzel and Charniak, 2002; Aylett and Turk,
1Code for reproducing our experiments is available at
https://github.com/thomashikaru/word-order-uid.
2004; Jaeger, 2010; Meister et al., 2021).
The
UID hypothesis posits a communicative pressure
to avoid spikes in information within an utterance,
thereby keeping the information profile of an
utterance relatively close to uniform over time.
While the UID hypothesis has been proposed as an
explanatory principle for many linguistic phenom-
ena, e.g., speakers’ choices when faced with lex-
ical and syntactic alternations (Levy and Jaeger,
2006), its relationship to word order patterns has
received limited attention, with the notable excep-
tion of Maurits et al. (2010) and Jain et al. (2018).
Our work investigates the relationship between
UID and word order patterns, differing from prior
work in several ways.
We (i) use Transformer
language models (LMs) (Vaswani et al., 2017) to
estimate information-theoretic operationalizations
of information uniformity; (ii) analyze large-scale
naturalistic datasets of 10 typologically diverse
languages; and (iii) compare a range of theoreti-
cally motivated counterfactual grammar variants.
Experimentally,
we find that among SVO
languages, the real word order has a more uniform
information density than nearly all counterfactual
word orders; the only orders that consistently
exceed real orders in uniformity are generated
using an implausibly strong bias for uniformity,
at the cost of expressivity. Further, we find that
counterfactual word orders that place verbs before
objects are more uniform than ones that place
objects before verbs in nearly every language.
Our findings suggest that a tendency for uni-
form information density may exist in human
language, with two potential sources: (i) word
order rules, with SVO order generally being
more uniform than SOV; and (ii) choices made
by speakers, who use the flexibility present in
real languages to structure information more
uniformly at a global level (and not only in a small
number of isolated constructions).
arXiv:2306.03734v1  [cs.CL]  6 Jun 2023

2
Functional Pressures in Language
2.1
Linguistic Optimizations
A number of linguistic theories link cross-
linguistic patterns to functional pressures.
For
example, both the grammatical rules of a language
and speakers’ choices (within the space of gram-
matically acceptable utterances) are posited to
reflect a trade-off between effort and robustness:
shorter and simpler structures are easier to produce
and comprehend, but longer and more complex ut-
terances can encode more information (Gabelentz,
1901; Zipf, 1935; Hawkins, 1994, 2004, 2014;
Haspelmath, 2008).
Another such functional
pressure follows from the principle of dependency
length minimization (DLM), which holds that, in
order to minimize working memory load during
comprehension, word orders should place words
in direct dependency relations close to each other
(Rijkhoff, 1986, 1990; Hawkins, 1990, 1994,
2004, 2014; Grodner and Gibson, 2005; Gibson,
1998, 2000; Bartek et al., 2011; Temperley and
Gildea, 2018; Futrell et al., 2020).
A growing
body of work has turned to information theory, the
mathematical theory of communication (Shannon,
1948), to formalize principles that explain linguis-
tic phenomena (Jaeger and Tily, 2011; Gibson
et al., 2019; Pimentel et al., 2021c).
One such
principle is that of uniform information density.
2.2
Uniform Information Density
According to the uniform information density
(UID) hypothesis, speakers tend to spread in-
formation evenly throughout an utterance; large
fluctuations in the per-unit information content
of an utterance can impede communication by
increasing the processing load on the listener.
Speakers may modulate the information profile
of an utterance by selectively producing linguistic
units such as optional complementizers in English
(Levy and Jaeger, 2006; Jaeger, 2010). A pressure
for UID in speaker choices has also been studied
in specific constructions in other languages,
though with mixed conclusions (Zhan and Levy,
2018; Clark et al., 2022).
Formally, the information conveyed by a lin-
guistic signal y, e.g., an utterance or piece of
text, is quantified in terms of its surprisal s(·),
which is defined as y’s negative log-probability:
s(y)
def= −log pℓ(y). Here, pℓis the underlying
probability distribution over sentences y for a lan-
guage ℓ. Note that we do not have access to the
true distribution pℓ, and typically rely on a lan-
guage model with learned parameters θ to esti-
mate surprisal values with a second distribution pθ.
Surprisal can be additively decomposed over
the units that comprise a signal. Explicitly, for a
signal y that can be expressed as a series of lin-
guistic units ⟨y1, . . . , yN⟩, where yn ∈V and V is
a set vocabulary of words or morphemes, the sur-
prisal of a unit yn is its negative log-probability
given prior context: s(yn) = −log pℓ(yn | y<n).
Note that the distribution pℓ(· | y<n) has support
V
def= V∪{EOS}, where EOS is a designated symbol
indicating the end of a sequence;2 a valid, com-
plete signal y = ⟨y1, . . . , yN⟩has yN = EOS.
The quantity s(y) can thus likewise be expressed
as s(y) = PN
n=1 s(yn). Assuming that we have
a fixed amount of information to convey and that
high-surprisal items are disproportionately diffi-
cult to process,3 it can be shown mathematically
that spreading information evenly throughout a
signal optimizes ease of processing for the com-
prehender (Levy and Jaeger, 2006; Smith and
Levy, 2013; Levy, 2018; Meister et al., 2021).
While the UID hypothesis is often discussed in
the context of speaker choices, it has also been pre-
sented as a general cognitive constraint that might
influence reading times (Meister et al., 2021),
speech duration (Pimentel et al., 2021b), and word
lengths (Piantadosi et al., 2011).
Selection for
UID has also been discussed as a potential evo-
lutionary pressure on language that can explain
typological differences (Jaeger and Tily, 2011).
Within this literature, there is not a consensus on
how to formally operationalize UID. For example,
Frank and Jaeger (2008) measure regression of
surprisal towards a language-wide mean; Collins
(2014) and Bloem (2016) consider more local
changes in surprisal in their quantification of UID.
In this work, we consider three metrics for op-
erationalizing UID (Meister et al., 2021):
UIDv(y)
def= 1
N
N
X
n=1
(s(yn) −µ)2
(1)
2This symbol allows for the global normalization of pℓ,
i.e., a valid probability distribution over finite-length se-
quences V∗(see Du et al., 2023, for a discussion).
3Most empirical results (Hale, 2001; Levy, 2008; Shain
et al., 2022) suggest that a word’s processing effort is directly
proportional to its surprisal. Yet there is also evidence of
a superlinear relationship, which would imply a preference
by the comprehender for UID (Meister et al., 2021; Hoover
et al., 2022).

In Equation (1),
UIDv is the mean within-
sentence variance of word surprisals, where µ =
1
N
PN
n=1 s(yn) is a sentence-level mean.
UIDlv(y)
def=
1
N −1
N
X
n=2
(s(yn) −s(yn−1))2
(2)
In Equation (2), UIDlv quantifies the average word-
to-word change in surprisal, a more localized mea-
sure (Collins, 2014).
Intuitively, this is maxi-
mized when high-surprisal words alternate with
low-surprisal words, and minimized when words
appear in sorted order by information content.
UIDp(y)
def= 1
N
N
X
n=1
s(yn)k
(3)
In Equation (3),
UIDp is a power mean with
k > 1, which disproportionately increases in the
presence of larger surprisal values.4
Note that
for all of these operationalizations, lower values
correspond to greater uniformity.5
3
Counterfactual Language Paradigm
Following prior work that has used counterfactual
languages to study the functional pressures at play
in word order patterns, we investigate to what
degree a language’s word order shows signs of
optimization for UID. In this approach, a corpus
of natural language is compared against a coun-
terfactual corpus containing minimally changed
versions of the same sentences, where the changes
target an attribute of interest, e.g., the language’s
word order. For example, several studies of DLM
have compared syntactic dependency lengths in
real and counterfactual corpora, generated by per-
muting the sentences’ word order either randomly
(Ferrer-i-Cancho, 2004; Liu, 2008) or determin-
istically by applying a counterfactual grammar
(Gildea and Temperley, 2010; Gildea and Jaeger,
2015; Futrell et al., 2015b, 2020).
Similarly,
we will compare measures of UID in real and
counterfactual corpora to investigate whether real
languages’ word orders exhibit more uniform
information density than alternative realizations.
4This metric suggests a super-linear processing cost for
surprisal.
5We note that, while a fully uniform language would have
value 0 for UIDv and UIDlv, it would not for UIDp(y), so the
metrics are not directly comparable.
3.1
Formal Definition
We build on the counterfactual generation proce-
dure introduced by Hahn et al. (2020) to create
parallel corpora. This procedure operates on sen-
tences’ dependency parses.
Formally, a depen-
dency parse
of a sentence y is a directed tree
with one node for every word, where each word in
y, with the exception of a designated root word, is
the child of its (unique) syntactic head; see Zmi-
grod et al. (2020) for a discussion of the role of
the root constraint in dependency tree annotation.
Each edge in the tree is annotated with the syntac-
tic relationship between the words connected by
that edge; see Fig. 1 for an example. Here we use
the set of dependency relations defined by the Uni-
versal Dependencies (UD) paradigm (de Marneffe
et al., 2021), though we follow Hahn et al. (2020)
in transforming dependency trees such that func-
tion words are treated as heads, leading to repre-
sentations closer to those of standard syntactic the-
ories; see also Gerdes et al. (2018).
Tree linearization.
While syntactic relation-
ships are naturally described hierarchically, sen-
tences are produced and processed as linear strings
of words. Importantly, there are many ways to lin-
earize a dependency parse
’s nodes into a string
y. Concretely, a grammar under our formalism is
defined by an ordering function (see Kuhlmann,
2010) g(·, ·) which takes as arguments a depen-
dency parse and a specific node in it, and returns
an ordering of the node and its dependents. For
each node, its dependents are arranged from left
to right according to this ordering; any node with-
out dependents is trivially an ordered set on its
own. This process proceeds recursively to arrive
at a final ordering of all nodes in a dependency
tree, yielding the final string y. Pseudo-code for
the linearization of a tree
based on an ordering
function g is given in Fig. 2.
Simplifying assumptions.
One consequence of
this formalism is that all counterfactual orders
correspond to projective trees, i.e., trees with
no crossing dependencies. While projectivity is
a well-attested cross-linguistic tendency, human
languages do not obey it absolutely (Ferrer-i-
Cancho et al., 2018; Yadav et al., 2021). Within
the space of projective word order interventions al-
lowed by this formalism, the grammars which we
borrow from Hahn et al. (2020) enforce two addi-
tional simplifying constraints. First, the relative

Figure 1: An example dependency tree showing syntactic relationships according UD, transformed so
that function words are heads (§3.2). Arrows point from heads to dependents.
positioning (left or right) between the head and
dependent of a particular relation is fixed. Sec-
ond, the relative ordering of different relations on
the same side of a head is also fixed. We denote
grammars which satisfy both constraints as con-
sistent. Notably, natural languages violate both of
these assumptions to varying degrees. For exam-
ple, even in English – a language with relatively
strict word order – adverbs can generally appear
before or after their head. While these simplifi-
cations mean that the formalism cannot perfectly
describe natural languages, it provides a compu-
tationally well-defined method for intervening on
many features of word order.
In particular, the
consistent grammars of Hahn et al. (2020) are pa-
rameterized by a set of scalar weights correspond-
ing to each possible syntactic relation; the order-
ing function thus reduces to sorting each head’s
dependents based on their weight values. Notably,
Hahn et al. (2020) also introduced a method for
optimizing these grammars for various objective
functions by performing stochastic gradient de-
scent on a probabilistic relaxation of the grammar
formalism; we use several of these grammars (de-
scribed in §3.2) in our subsequent analysis.
Creating counterfactual word orderings.
The
above paradigm equips us with the tools necessary
for systematically altering sentences’ word order-
ings, which in turn, enables us to create counter-
factual corpora. Notably, the large corpora we use
in this study contain sentences as strings, not as
their dependency parses. We therefore define our
counterfactual grammar intervention as the out-
put of a (deterministic) word re-ordering function
f : Y →Y, where Y
def= V∗is the set of all possi-
ble sentences that can be constructed using a lan-
guage’s vocabulary V.6 This function takes as in-
put a sentence from our original language and out-
puts a sentence with the counterfactual word order
6For notational brevity, we leave the dependency of V on
ℓimplicit as it should be clear from context.
Figure 2: Pseudo-code to linearize a dependency
tree
according to a grammar’s ordering function
g. In this code, each node contains a word and its
syntactic dependents.
defined by a given ordering function g. We de-
compose this function into two steps:
f(y) = linearize(parse(y), g)
(4)
We use a state-of-the-art parser (Straka and
Straková, 2017) to implement parse : Y →T
where
T
is
the
set
of
all
dependency
parses.
Specifically, we define parse(y)
=
argmax
∈T p(
| y) for a learned conditional
probability
distribution
over
possible
parses
p(· | y).
We then obtain the linearized form
of the resulting tree by supplying it and the
ordering function g to linearize , as defined
above.
Collectively, the outputs of this process
(parallel datasets differing only in word order) are
referred to as variants. Importantly, f here is a
deterministic function; one could instead consider
f to be probabilistic in nature, with each sentence
y having a distribution over tree structures
. We
discuss the implications of this choice in §4.
3.2
Counterfactual Grammar Specifications
In addition to the original REAL word order,
we explore the following theoretically motivated

counterfactual grammars for each language.
Consistent approximation to real order.
AP-
PROX is a consistent approximation to the real
word order within our formalism; it uses an
ordering function parameterized by weights that
were fitted to maximize the likelihood of observed
word orders for each language, as reported by
Hahn et al. (2020). This variant captures most of
the word order features of a real language while
allowing for a fair comparison to deterministic
counterfactual grammars that do not model the
flexibility of real language. From the perspective
of the UID hypothesis, we expect this variant to
be less uniform that REAL because it has less
flexibility to accommodate speakers’ choices that
optimize for UID.
Consistent random grammars.
We include
variants RANDOM1 through RANDOM5, which
use ordering functions parameterized by randomly
assigned weights.
This means that for a given
random grammar, each dependency relation has a
fixed direction (left or right), but that the directions
of these relations lack the correlations observed
in natural language (Greenberg, 1963). Random
grammars with the same numerical index share
weights across languages.
Consistent grammars optimized for efficiency.
We include two consistent grammars that are opti-
mized for the joint objective of parseability (how
much information an utterance provides about
its underlying syntactic structure) and sentence-
internal predictability, as reported by Hahn et al.
(2020), one with OV order (EFFICIENT-OV) and
one with VO order (EFFICIENT-VO). For ex-
ample, the EFFICIENT-OV grammar for English
would give a plausible version of a consistent
and efficient grammar in the counterfactual world
where English has verbs after objects.
Grammars optimized for dependency length
minimization.
From the same work we also take
consistent grammars that are optimized for DLM,
denoted as MIN-DL-OPT.
While linearizations
produced by these grammars are not guaranteed
to minimize dependency length for any particular
sentence, they minimize the expected average de-
pendency length of a large sample of sentences in a
language. In addition, we include MIN-DL-LOC,
an inconsistent grammar that applies the projec-
tive dependency-length minimization algorithm of
Gildea and Temperley (2007) at the sentence level,
leading to sentences with minimal DL but without
the constraint of consistency.
Frequency-sorted grammars.
SORT-FREQ is
an inconsistent grammar which orders words in
a sentence from highest to lowest frequency, ig-
noring dependency structure altogether. We use
this ordering as a heuristic baseline for which
we expect UID to hold relatively strongly: low-
frequency elements, which tend to have higher
surprisal even if solely from their less frequent
usage (Ellis, 2002), are given more context, and
thus should have smaller surprisals than if they oc-
curred early; more conditioning context tends to
reduce the surprisal of the next word (Luke and
Christianson, 2016).
We also test SORT-FREQ-
REV, ordering words from least to most frequent,
which for analogous reasons we expect to perform
poorly in terms of UID. However, both of these
orderings lead to massive syntactic ambiguity by
introducing many string collisions – any two sen-
tences containing the same words in different or-
ders would be linearized identically. This elimi-
nates word order as a mechanism for expressing
distinctions in meaning, so these orders are im-
plausible as alternatives to natural languages (Ma-
howald et al., 2022).
Reverse grammar.
Finally, we also include
the REVERSE variant, where the words in each
sentence appear in the reverse order of the origi-
nal. This variant preserves all pairwise distances
between words within sentences and has identical
dependency lengths as the original order, thus
isolating the effect of linear order on information
density from other potential influences. Notably,
if the original language happens to be perfectly
consistent, then REVERSE will also satisfy con-
sistency; in practice, this is unlikely to hold with
natural languages.
3.3
UID and Counterfactual Grammars
Let pℓ(y) be the probability distribution over sen-
tences y for a language of interest ℓ. We can de-
fine a language’s UID score as the expected value
of its sentences’ UID scores, where we overload
the UID function to take either a sentence y or an
entire language ℓ:
UID(ℓ)
def=
X
y∈Y
pℓ(y) UID(y)
(5)

Figure 3: The same source sentence according to 4 real and counterfactual orderings.
where
sentence-level
UID
can
be
UIDv(y),
UIDlv(y), or UIDp(y). In practice, we estimate this
language-level UID score using a Monte-Carlo es-
timator, taking the mean sentence-level UID score
across a held-out test set Sℓof sentences y in lan-
guage ℓ, where we assume y ∼pℓ:
d
UID(ℓ)
def=
1
|Sℓ|
X
y∈Sℓ
UID(y)
(6)
Similarly, the expected surprisal (or Shannon
entropy, H) of this language is computed as:
H(ℓ)
def= −
X
y∈Y
pℓ(y) log pℓ(y)
(7)
We evaluate how well a language model pθ ap-
proximates pℓby its cross-entropy:
H(pℓ, pθ) = −
X
y∈Y
pℓ(y) log pθ(y)
(8)
where a smaller value of H implies a better model.
Again using a Monte Carlo estimator, we measure
cross-entropy using the held-out test set Sℓ:
bH(pℓ, pθ) = −1
|Sℓ|
X
y∈Sℓ
log pθ(y)
(9)
This is simply the mean surprisal that the model
assigns to a corpus of naturalistic data.
These computations can also be applied to
counterfactual variants of a language. Let ℓf stand
for a language identical to ℓ, but where its strings
have been transformed by f;
this language’s
distribution over sentences would be pℓf(y) =
P
y′∈Y pℓ(y′)1{y = f(y′)}.
Since entropy is
non-increasing over function transformations (by
Jensen’s inequality), it follows that:
H(ℓ) ≥H(ℓf)
(10)
Further, if our counterfactual generation function
f is a bijection – meaning that each input string
gets mapped to a distinct output string and each
output string has an input that maps to it – then
we can create a second function f−1 : Y →Y,
which would generate ℓfrom ℓf. Then, the fol-
lowing holds:
H(ℓ) ≥H(ℓf) ≥H(ℓf−1◦f) = H(ℓ)
(11)
i.e., it must be that H(ℓ) = H(ℓf). Reversing a
sentence is an example of a bijective function, and
thus Equation (11) holds necessarily for the pair of
REAL and REVERSE variants; the counterfactual
generation procedure thus should not produce dif-
ferences in mean surprisal between these variants.
At the same time, bijectivity does not necessarily
hold for our other counterfactual transformations
and is violated to a large degree when mapping to
SORT-FREQ and SORT-FREQ-REV. Thus in gen-
eral, we can only guarantee Inequality (10).
Crucially, however, the transformation f might
change the UID score of such a language, allow-
ing us to evaluate the impact of word order on in-
formation uniformity. As a simple example, con-
sider the language ℓ1 that places a uniform distri-
bution over only four strings: aw, ax, by, and bz.
In this language, the first and second symbols al-
ways have 1 bit of surprisal, and the end of the
string has 0 bits of surprisal. If the counterfactual
language ℓ2 is the reverse of ℓ1, we have a uni-
form distribution over the strings wa, xa, yb, and
zb. Here, the first symbol always has 2 bits of sur-
prisal, and the second symbol and end of sentence
always have zero bits, as their values are determin-
istic for a given initial symbol. While the mean
surprisal per symbol is the same for ℓ1 and ℓ2, ℓ1
has more uniform information density than ℓ2.

4
Limitations
4.1
Use of Counterfactual Grammars
Real word orders are not consistent.
The con-
sistent grammars borrowed from Hahn et al.
(2020) assume that the direction of each syntactic
relation, as well as the relative ordering of depen-
dents on the same side of a head, are fixed. This
is not generally true of natural languages. We ad-
dress this difference by including the variant AP-
PROX as a comparison to the counterfactual vari-
ants, which are constrained by consistency, and
by including REVERSE as a comparison to REAL,
both of which are not constrained by consistency.
Automatic parsing errors.
Another issue is that
the dependency parses extracted for each original
sentence as part of the counterfactual generation
pipeline may contain parsing errors.
These er-
rors may introduce noise into the counterfactual
datasets that is not present in the original sen-
tences, and may cause deviations from the charac-
teristics that we assume our counterfactual gram-
mars should induce. For example, MIN-DL-LOC
only produces sentences with minimized depen-
dency length if the automatic parse is correct.
Deterministic parsing.
Finally, our counterfac-
tual generation procedure assumes a deterministic
mapping from sentences to dependency trees
as one of its steps.
However, multiple valid
parses of sentences are possible in the presence
of syntactic ambiguity. In such cases, we always
select the most likely structure according to the
parser, which learns these probabilities based on
its training data.
Therefore, this design choice
could lead to underrepresentation of certain
syntactic structures when applying a transforma-
tion. However, we note that the variants REAL,
REVERSE, SORT-FREQ, and SORT-FREQ-REV
do not depend on dependency parses and so are
unaffected by this design choice.
4.2
Choice of Dataset
Properties of language can vary across genres and
domains. When drawing conclusions about human
language in general, no single dataset will be com-
pletely representative. Due to the amount of data
required to train LMs, we use written corpora in
this work, and use the term speaker loosely to re-
fer to any language producer regardless of modal-
ity. To address potential concerns about the choice
of dataset in this study, we conducted a supple-
mentary analysis on a subset of languages using a
different web corpus, which we report in §7.5.
4.3
Errors and Inductive Biases
Model Errors.
Language model quality could
impact the estimated values of our UID metrics
UIDv, UIDp, and UIDlv. To see why, consider a
model pθ that – rather than providing unbiased
estimates of pℓ– is a smoothed interpolation
between pℓand the uniform distribution:
pθ(yn | y<n) = λ pℓ(yn | y<n) + 1 −λ
|V|
(12)
for λ
∈
[0, 1].
Here, an increase in 1 −λ
would lead to an increase in H(pℓ, pθ), since
the
cross-entropy
is
only
minimized
when
pθ(· | y<n) = pℓ(· | y<n). This change, however,
would be reflected as an increase in uniformity,
e.g., a decrease in UIDv: surprisals would be closer
to uniform for smaller values of λ. Alternatively,
consider the situation where a language ℓhas
perfect information uniformity, i.e., where UIDv,
UIDp, and UIDlv are their minimum possible val-
ues. The interpolation of pℓwith any non-uniform
distribution should instead decrease the measured
uniformity, at least with respect to UIDv and UIDlv.
In summary, our UID metrics could be biased
either positively or negatively by the quality of
our models. However, since our analysis focuses
on the comparison of UID metrics between word
order variants rather than their absolute value, this
bias should not be a major concern. We use the
same model architecture for all language–variant
combinations, and so a bias in the UID metric
corresponding to one combination should likewise
be reflected in all of the metrics that it is compared
to. Further, our results hold even when controlling
for mean surprisal, as described in §6.
Inductive Biases.
Because modern LMs have
been developed to model natural language, they
may contain subtle biases towards the properties
of real word orders or of highly resourced lan-
guages. Based on Inequality (10), if two prob-
abilistic models mℓand mℓf were to perfectly
learn the true and counterfactual distributions pℓ
and pℓf, respectively, then mℓshould assign ap-
proximately the same or higher mean surprisal to
a corpus {y(m)}M
m=1 from ℓthan mℓf assigns to
the counterfactual corpus from ℓf. This implies
that previous results of Gildea and Jaeger (2015),
Ravfogel et al. (2019), Hahn et al. (2020) and

White and Cotterell (2021), which found that real
corpora tend to have lower average per-word sur-
prisal than deterministically generated counterfac-
tual versions of the same corpora, were in fact due
to the inductive bias of the learning algorithms
used to estimate surprisals. There is a clear rea-
son why the trigram model of Gildea and Jaeger
(2015) would yield higher mean surprisals for
counterfactual corpora: the transformation func-
tions f tended to increase dependency lengths,
and words in a dependent–head relation tend to
have higher mutual information than other pairs
of words (Futrell and Levy, 2017; Futrell et al.,
2019, 2020). Hence the transformations tended to
push words that are predictive of each other out-
side of the conditioning window of the model (see
also Hahn and Xu, 2022, for similar effects). The
Transformer architecture we use in this work could
thus also contain biases favoring features of real
language, which we attempt to control for (see §6).
5
Experimental Setup
5.1
Data
This work uses the publicly available Wiki40b
dataset (Guo et al., 2020), a large text corpus de-
rived from Wikipedia articles.
We use subsets
of the Wiki40b dataset in 10 languages: English,
Russian, French, German, Hindi, Farsi, Viet-
namese, Indonesian, Hungarian, and Turkish. The
first six represent the Germanic, Slavic, Romance,
Indo-Aryan, and Iranian sub-families of the Indo-
European language family.
The latter four be-
long to the Austroasiatic, Austronesian, Uralic,
and Turkic language families, respectively. Turk-
ish, Hindi, and Farsi have basic SOV word order,
while the other languages have SVO order with
Hungarian being mixed (Dryer, 2013). Languages
were chosen based on the amount of available data
in the Wiki40b dataset, their typological proper-
ties (covering a range of families, canonical word
orders, and morphological complexity), and avail-
ability of automatic dependency parsing models.
The datasets are subsampled to yield approx-
imately 20M words in the training set of each
language and approximately 1M words in the test
and validation sets.
We automatically generate
dependency parses for all sentences using the UD-
Pipe parser (Straka and Straková, 2017), yielding
syntactic representations in the UD paradigm. We
then apply each of the counterfactual orderings
introduced in §3.2 to the original data to create
parallel corpora for each language.
Sentences
are stripped of punctuation (as determined by
the dependency parser’s PUNCT label) and are
lowercased. Periods are added back in to mark the
end of sentences, regardless of what the original
final punctuation was.
Sub-word tokenization
is then applied to the corpora using a byte-pair
encoding (BPE) model, trained with a fixed
vocabulary size of 30K tokens and using the
algorithm of Sennrich et al. (2016). 7
5.2
Language Modeling
For each variant of each language, we train
a Transformer language model (Vaswani et al.,
2017) using fairseq (Ott et al., 2019). Models
are trained on document-level inputs, with a maxi-
mum length of 512 tokens; this means that each to-
ken is predicted with the preceding material of the
entire document as context. Each model is trained
with early stopping, halting training after no im-
provement in validation loss for three epochs. The
Adam optimizer was used (Kingma and Ba, 2017),
with a learning rate of 0.0005, weight decay of
0.01, and dropout of 0.1. Training scripts are avail-
able in the project’s GitHub repository.1 In all of
our analyses, we use the word-by-word surprisals
estimated using our trained models on their corre-
sponding held-out test sets. Note that we do not
consider the designated EOS symbol in the com-
putation of any of our UID-related metrics. In the
case that a word is comprised of multiple sub-word
tokens, we aggregate their surprisals by summa-
tion, since surprisal decomposes additively.
6
Results
Estimates of mean per-word surprisal on the test
set are in Fig. 4A. Consistent with the results of
Hahn et al. (2020), our trained models for nearly
all counterfactual variants assign higher per-word
surprisal to their respective test sets than the
REAL models assign to theirs. Across all 10 lan-
guages, REVERSE has mean surprisal close to, but
consistently slightly higher than, that of the real
ordering. SORT-FREQ and SORT-FREQ-REV have
mean surprisals close to or below those of REAL.
Estimates of mean surprisal variance (UIDv)
over sentences are shown in Fig. 4B. Notably,
7All variants of the same language are tokenized using the
same BPE model, trained on a sample of 100K documents
from all variants; BPE tokens could not cross word bound-
aries for compatibility with different word orders.

Figure 4: Mean test-set surprisal and surprisal variance of language models across real and counterfactual
grammars in 10 languages. Error bars denote the 95% CI of the mean.
there is a dissociation between the rank order of
variants according to mean surprisal and accord-
ing to UIDv: variants with similar mean surprisals
did not necessarily have similar UIDv scores, and
vice versa, suggesting that information uniformity
and mean surprisal can vary independently of each
other. Our main observations are as follows: (i)
In all languages except Turkish and Hindi, our es-
timates of UIDv for REAL are lower than those
for REVERSE, despite the variants’ similarities in
mean surprisal. (ii) As predicted, the SORT-FREQ
baseline has UIDv equal to or lower than that of
REAL. (iii) The other counterfactual variants typ-
ically exhibit higher UIDv than REAL, with the
exception of mixed results for SORT-FREQ-REV.
(iv) The EFFICIENT-VO variants typically have
lower UIDv than EFFICIENT-OV (with Hungar-
ian being a noteworthy exception), which supports
findings based on toy grammars showing that SVO
orders are more uniform than SOV orders (Maurits
et al., 2010). Crucially, these results are qualita-
tively similar using the UIDlv metric (Fig. 6B).
To fairly compare variants using the UIDp met-
ric, we first need to account for the fact that, un-
like surprisal variance, the metric is sensitive to
shifts in mean surprisal. To control for this, we
fit a regression model predicting the UIDp score
based on three variables: the mean surprisal, the
grammar variant, and the dataset size (20M, 6.6M,
and 3.3M words).
We train multiple language
models for each language-variant combination (3
dataset sizes and 2 random seeds), resulting in
84 data points per language. We apply treatment
coding to the variants, with REAL as the refer-
ence level. Fig. 5 shows the resulting estimates
of the coefficients for each variant, where a co-
efficient should be positive if that variant is less
uniform than REAL.
Qualitatively, the regres-
sion results match the results given by UIDv and
UIDlv: REAL is more uniform than REVERSE in
SOV languages, SORT-FREQ is the only counter-
factual variant that is consistently more uniform
than REAL, and EFFICIENT-VO is more uniform
than EFFICIENT-OV in most languages; the op-
posite is true in Hungarian and the difference is
negligible in Russian.
7
Discussion
We offer a discussion of the results observed in §6,
including their implications for the role of func-
tional pressures in language.
7.1
Differences in mean surprisal
Across 10 typologically diverse languages, we
find that Transformer LMs learn to predict data
from real word orders better than data from
counterfactual orders, with the exception of the
SORT-FREQ
and
SORT-FREQ-REV
variants.
This suggests that these LMs’ inductive biases
somehow favor properties of real languages,
in line with previous work on other modeling
architectures (Gildea and Jaeger, 2015; Ravfogel
et al., 2019).
This is not surprising, given that
commonly used architectures and hyperparame-
ters have been selected specifically based on their
good performance on real language tasks. Unlike

Figure 5: Linear regression coefficient esti-
mates when predicting UIDp as a function of
mean surprisal, variant, and dataset size. The
reference level for variant is REAL, so positive
coefficients (blue) indicate variants with greater
UIDp, i.e., less uniformity, than real language.
in n-gram models, the precise inductive bias of
Transformer models that favors real word orders
is not transparent and merits further study.8
7.2
Differences between REAL and APPROX
We observe that despite the similarities between
the REAL and APPROX variants of a given
language, the latter are consistently assigned
higher mean surprisal by their respective LMs.
Meanwhile, the various UID metrics show similar
results for REAL and APPROX, suggesting that the
greater flexibility of REAL is not responsible for
UID differences in our results. This is somewhat
surprising, since it may appear that such flexibility
is what enables speakers’ choices, which have
been previously discussed as contributing to UID.
However, many speaker choices that potentially
impact UID, such as word choice, active versus
passive voice, and optional words, are not cap-
tured by this difference in flexibility between
REAL and APPROX.
8Notably, White and Cotterell (2021) show that there is a
large variation in how Transformer language models perform
in toy languages with diverse word orders; they, however,
do not find evidence that Transformers perform better on the
most frequently occurring orders (as opposed to, e.g., OVS
and VOS word orders, which are found in few languages).
7.3
Greater uniformity of REAL over
REVERSE in SVO languages
While mean surprisal is always very close for
REAL and REVERSE grammars, REVERSE is less
uniform in 8 out of 10 languages, including all
SVO languages. This held across multiple opera-
tionalizations of UID, with the exception of mixed
results for Hungarian, a language with consider-
able flexibility in word order. Thus, while both
REAL and REVERSE orders are learned approxi-
mately equally well by language models, they dif-
fer in how uniformly they distribute information.
One key difference between REAL and RE-
VERSE is that insofar as REAL sentences exhibit
a tendency to mention entities from the end of a
given sentence close to the beginning of the next
one, REVERSE does not preserve this property.
For example, the pair of sentences “I like dogs.
They
are
friendly.”
would become “Dogs
like I. Friendly are they.”; note that the
distance between antecedent and pronoun is sig-
nificantly increased. This feature of the REVERSE
raises the possibility that the uniformity patterns
we observe are due to speaker choices taking
cross-sentence dependencies into consideration.
To minimize the influence of cross-sentence de-
pendencies, we can consider only sentences occur-

Figure 6: A. Surprisal variance (UIDv(y)) for document-initial sentences only.
B. Mean squared
word-to-word change (UIDlv(y)) in surprisal. Error bars denote 95% CI of the mean.
ring at the start of a document, which cannot refer
to previous sentences. Fig. 6A shows that the ten-
dency for REAL to have lower surprisal variance
than REVERSE still holds in this setting across
most languages. This suggests that cross-sentence
dependencies alone cannot fully explain the ob-
served differences in information uniformity.
Notably, our results show that the UID prefer-
ence for REAL over REVERSE is not consistently
present in languages with basic SOV order (Turk-
ish, Hindi, and Farsi).
We propose the follow-
ing explanation for this result: As argued in Mau-
rits et al. (2010), SVO languages tend to have
more uniform information density profiles than
SOV languages – a finding supported by our em-
pirical results in which EFFICIENT-VO had lower
surprisal variance than EFFICIENT-OV in 9 out of
10 languages. Unlike the short, simple sentences
of Maurits et al., however, the present study con-
siders long and complex sentences where speaker
choices have considerable opportunity to influence
information uniformity, in addition to the role of
basic word order. These choices include whether
to use a pronoun, whether to use an active or pas-
sive construction, and what order to present a con-
junction or list of items, among others. Impor-
tantly, speakers make choices conditional on the
forward ordering of real language, so we expect
that the choices made in an attempt to increase
UID – which constitutes a non-trivial percentage
of utterances (Levy and Jaeger, 2007) – would
have a greater effect on UID in REAL than in RE-
VERSE. In SVO languages, the effects upon UID
of basic word order and speaker choices both go
in the same direction – towards more uniformity.
In SOV languages, these effects conflict – the ba-
sic word order is non-optimal in terms of UID,
and so uniformity can theoretically be increased
by a transformation to REVERSE, while speaker
choices are presumably already mostly optimal in
REAL. This may explain the heterogeneous pat-
terning among the three SOV languages.
Furthermore, these results can potentially shed
light on an important question in linguistic typol-
ogy: Why are some basic word orders more com-
mon than others?
According to some theories,
SOV order (the most typologically common) is the
most natural for expressing events with subjects
and objects (Goldin-Meadow et al., 2008; Gibson
et al., 2013; Futrell et al., 2015a). If these theo-
ries are correct, an evolutionary pressure on lan-
guages to shift from SOV to SVO could help ac-
count for the prevalence of SVO languages, which
are nearly as common as SOV ones. A pressure for
information uniformity offers one such account.
Finally, Pimentel et al. (2021a) has recently
shown that the distribution of per-phone informa-
tion within words is more uniform when analysed
in reverse order than in forward order – the op-
posite of what we observe on our sentence-level
analysis.
This difference may suggest qualita-
tively distinct information-theoretic pressures be-
ing present at the lexical and sentential levels and
is a potential topic for further study.

7.4
Other Variants
The variants designed to minimize dependency
length, MIN-DL-LOC and MIN-DL-OPT, showed
mixed results in terms of information uniformity
compared to REAL. The random grammars fell
into two groups:
RANDOM1, RANDOM2, and
RANDOM4 tended to be less uniform than REAL,
while RANDOM3 and RANDOM5 tended to be sim-
ilar in uniformity to REAL. Since random gram-
mars have fixed but uncorrelated directions of syn-
tactic relations, these cross-linguistically consis-
tent patterns suggest that some settings of the pa-
rameterized grammar are inherently more favor-
able from the perspective of UID than others.
The only counterfactual word order to con-
sistently have a higher degree of information
uniformity than the real orders was the highly
constrained SORT-FREQ, which turns sentences
into sorted word lists.
Thus, while it appears
possible to improve on real word orders’ informa-
tion uniformity, this comes at the cost of massive
syntactic ambiguity and reduced expressivity.
7.5
Robustness to Dataset Choice
In this study, the chosen dataset (Wiki40b) con-
tains formal writing that may not exhibit the same
communicative pressures as spoken language. It
is largely devoid of first and second person pro-
nouns, interrogatives, and other features common
in everyday speech; further, it may have dis-
proportionate amounts of translationese (Koppel
and Ordan, 2011).
As a supplementary analy-
sis, we repeated the experiments on the CC100
dataset (Conneau et al., 2020), using only a sub-
set of languages due to computational constraints.
This dataset is sourced from a web crawl and
therefore contains a wider range of genres and
styles than Wiki40b.
UIDv scores for these ex-
periments are shown in Fig. 7. The results qual-
itatively match the patterns from the Wiki40b ex-
periments in the following ways: (i) better UIDv
scores for REAL than for REVERSE among SVO
languages, (ii) better UIDv scores for EFFICIENT-
VO than EFFICIENT-OV in most languages (with
Hungarian again being an exception), and (iii) the
only variant that has higher uniformity that REAL
across a majority of languages is SORT-FREQ.
8
Conclusion
In conclusion, we have empirically demonstrated
that in many languages, real word orders distribute
Figure 7: Surprisal mean and variance for a subset
of languages on the CC100 dataset.
Error bars
denote 95% CI.
information more uniformly than a range of coun-
terfactual orders. The fact that this pattern holds
in every SVO languages but is mixed among SOV
languages lends support to the view that SVO ba-
sic word order is preferable to SOV order from
the perspective of maximizing UID. We posit that
there are two potential sources of optimization
within a language for greater UID: language evo-
lution favoring word orders that produce less vari-
ance in information content, and speaker choices
in favor of constructions that smooth the informa-
tion profile of utterances. Our results are consis-
tent with the UID hypothesis, and support the idea
that communicative pressures (operationalized in
terms of information theory) influence the struc-
ture of human language.
Acknowledgements
We thank our action editor, Mark-Jan Nederhof,
and the anonymous reviewers for their detailed
feedback on this paper. CM was supported by the
Google PhD Fellowship. TP was supported by a
Facebook PhD Fellowship. This work was sup-
ported by NSF grant BCS-2121074 to RPL.
References
Matthew Aylett and Alice Turk. 2004. The smooth
signal redundancy hypothesis: A functional ex-
planation for relationships between redundancy,
prosodic prominence, and duration in sponta-

neous speech. Language and Speech, 47(1):31–
56. PMID: 15298329.
Brian Bartek, Richard L. Lewis, Shravan Vasishth,
and Mason R. Smith. 2011. In search of on-
line locality effects in sentence comprehension.
Journal of Experimental Psychology: Learning,
Memory, and Cognition, 37(5):1178–1198.
Jelke Bloem. 2016.
Testing the processing hy-
pothesis of word order variation using a prob-
abilistic language model.
In Proceedings of
the Workshop on Computational Linguistics for
Linguistic Complexity, pages 174–185, Osaka,
Japan. The COLING 2016 Organizing Commit-
tee.
Thomas Hikaru Clark, Ethan Gotlieb Wilcox, Ed-
ward Gibson, and Roger P. Levy. 2022. Evi-
dence for availability effects on speaker choice
in the Russian comparative alternation. In Pro-
ceedings of the 44th Annual Meeting of the Cog-
nitive Science Society.
Michael Xavier Collins. 2014. Information den-
sity and dependency length as complementary
cognitive models. Journal of Psycholinguistic
Research, 43(5):651–681.
Alexis Conneau, Kartikay Khandelwal, Naman
Goyal, Vishrav Chaudhary, Guillaume Wen-
zek, Francisco Guzmán, Edouard Grave, Myle
Ott, Luke Zettlemoyer, and Veselin Stoyanov.
2020.
Unsupervised cross-lingual representa-
tion learning at scale.
In Proceedings of the
58th Annual Meeting of the Association for
Computational Linguistics, page 8440–8451,
Online. Association for Computational Linguis-
tics.
Matthew S. Dryer. 2013. Order of subject, object
and verb (v2020.3). In Matthew S. Dryer and
Martin Haspelmath, editors, The World Atlas of
Language Structures Online.
Li Du, Lucas Torroba Hennigen, Tiago Pimentel,
Clara Meister, Jason Eisner, and Ryan Cot-
terell. 2023. A measure-theoretic characteriza-
tion of tight language models. In Proceedings
of the 61st Annual Meeting of the Association
for Computational Linguistics. Association for
Computational Linguistics.
Nick Ellis. 2002. Frequency effects in language
processing. Studies in second language acqui-
sition, 24(2):143–188.
August Fenk and Gertrud Fenk. 1980.
Kon-
stanz im Kurzzeitgedächtnis—Konstanz im
sprachlichen Informationsfluß.
Zeitschrift für
experimentelle und angewandte Psychologie,
27(3):400–414.
Ramon Ferrer-i-Cancho. 2004.
Euclidean dis-
tance
between
syntactically
linked
words.
Physical Review E, 70:056135.
Ramon
Ferrer-i-Cancho,
Carlos
Gómez-
Rodríguez,
and Juan Luis Esteban. 2018.
Are
crossing
dependencies
really
scarce?
Physica
A:
Statistical
Mechanics
and
its
Applications, 493:311–329.
A. Frank and T.F. Jaeger. 2008. Speaking ratio-
nally: Uniform information density as an opti-
mal strategy for language production. In Pro-
ceedings of the Cognitive Science Society.
Richard Futrell, Tina Hickey, Aldrin Lee, Eu-
nice Lim, Elena Luchkina, and Edward Gibson.
2015a.
Cross-linguistic gestures reflect typo-
logical universals: A subject-initial, verb-final
bias in speakers of diverse languages. Cogni-
tion, 136:215–221.
Richard Futrell and Roger Levy. 2017.
Noisy-
context surprisal as a human sentence process-
ing cost model. In Proceedings of the 15th Con-
ference of the European Chapter of the Associ-
ation for Computational Linguistics: Volume 1,
Long Papers, pages 688–698, Valencia, Spain.
Association for Computational Linguistics.
Richard Futrell, Roger P. Levy, and Edward Gib-
son. 2020.
Dependency locality as an ex-
planatory principle for word order. Language,
96(2):371–413.
Richard Futrell, Kyle Mahowald, and Edward
Gibson. 2015b.
Large-scale evidence of de-
pendency length minimization in 37 languages.
Proceedings of the National Academy of Sci-
ences, 112(33):10336–10341.
Richard Futrell, Peng Qian, Edward Gibson,
Evelina Fedorenko, and Idan Blank. 2019. Syn-
tactic dependencies correspond to word pairs
with high mutual information.
In Proceed-
ings of the Fifth International Conference on
Dependency Linguistics (Depling, SyntaxFest
2019), pages 3–13, Paris, France. Association
for Computational Linguistics.
Georg von der Gabelentz. 1901. Die Sprachwis-
senschaft, ihre Aufgaben, Methoden, und bish-
erigen Ergebnisse, 2nd edition. C. H. Tauch-
nitz, Leipzig.
Dmitriy Genzel and Eugene Charniak. 2002. En-

tropy rate constancy in text.
In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 199–206,
Philadelphia, Pennsylvania, USA. Association
for Computational Linguistics.
Kim Gerdes, Bruno Guillaume, Sylvain Kahane,
and Guy Perrier. 2018.
SUD or surface-
syntactic universal dependencies: An annota-
tion scheme near-isomorphic to UD.
In Pro-
ceedings of the Second Workshop on Universal
Dependencies, UDW@EMNLP 2018, Brussels,
Belgium, November 1, 2018, pages 66–74. As-
sociation for Computational Linguistics.
Edward Gibson. 1998. Linguistic complexity: Lo-
cality of syntactic dependencies.
Cognition,
68(1):1–76.
Edward Gibson. 2000. The dependency locality
theory: A distance-based theory of linguistic
complexity. In Image, Language, Brain: Pa-
pers from the First Mind Articulation Project
Symposium, pages 95–126, Cambridge, MA.
MIT Press.
Edward Gibson, Richard Futrell, Steven P. Pianta-
dosi, Isabelle Dautriche, Kyle Mahowald, Leon
Bergen, and Roger Levy. 2019. How efficiency
shapes human language.
Trends in Cognitive
Sciences, 23(5):389–407.
Edward Gibson, Steven T. Piantadosi, Kimberly
Brink, Leon Bergen, Eunice Lim, and Re-
becca Saxe. 2013. A noisy-channel account of
crosslinguistic word-order variation.
Psycho-
logical Science, 24(7):1079–1088.
Daniel Gildea and T. Florian Jaeger. 2015. Human
languages order information efficiently. arXiv
preprint arXiv:1510.02823.
Daniel Gildea and David Temperley. 2007. Op-
timizing grammars for minimum dependency
length.
In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, page 184–191, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Daniel Gildea and David Temperley. 2010.
Do
grammars minimize dependency length? Cog-
nitive Science, 34(2):286–310.
Susan Goldin-Meadow, Wing Chee So, Aslı
Özyürek, and Carolyn Mylander. 2008.
The
natural order of events: How speakers of dif-
ferent languages represent events nonverbally.
Proceedings of the National Academy of Sci-
ences, 105(27):9163–9168.
Joseph H. Greenberg. 1963. Some universals of
grammar with particular reference to the or-
der of meaningful elements. MIT Press, Cam-
bridge, MA.
Daniel Grodner and Edward Gibson. 2005. Con-
sequences of the serial nature of linguistic input
for sentential complexity.
Cognitive Science,
29(2):261–290.
Mandy Guo, Zihang Dai, Denny Vrandeˇci´c, and
Rami Al-Rfou. 2020. Wiki-40b: Multilingual
language model dataset. In Proceedings of the
12th Language Resources and Evaluation Con-
ference, page 2440–2452, Marseille, France.
European Language Resources Association.
Michael Hahn, Dan Jurafsky, and Richard Futrell.
2020.
Universals of word order reflect opti-
mization of grammars for efficient communica-
tion. Proceedings of the National Academy of
Sciences, 117(5):2347–2353.
Michael Hahn and Yang Xu. 2022. Crosslinguistic
word order variation reflects evolutionary pres-
sures of dependency and information locality.
Proceedings of the National Academy of Sci-
ences, 119(24):e2122604119.
John Hale. 2001. A probabilistic Earley parser as
a psycholinguistic model. In Second Meeting of
the North American Chapter of the Association
for Computational Linguistics.
Martin Haspelmath. 2008.
Parametric versus
functional explanations of syntactic universals.
In T. Biberauer, editor, The limits of syntactic
variation, pages 75–107. John Benjamins, Am-
sterdam.
John A. Hawkins. 1990. A parsing theory of word
order universals. Linguistic Inquiry, 21(2):223–
261.
John A. Hawkins. 1994. A Performance Theory of
Order and Constituency. Cambridge University
Press, Cambridge.
John A. Hawkins. 2004. Efficiency and Complex-
ity in Grammars. Oxford University Press, Ox-
ford.
John A. Hawkins. 2014. Cross-linguistic Varia-
tion and Efficiency. Oxford University Press,
Oxford.
Jacob L. Hoover, Morgan Sonderegger, Steven T.
Piantadosi, and Timothy J. O’Donnell. 2022.
The plausibility of sampling as an algorith-
mic theory of sentence processing.
PsyArXiv

preprint PsyArXiv:qjnpv.
T. Florian Jaeger. 2010. Redundancy and reduc-
tion: Speakers manage syntactic information
density. Cognitive Psychology, 61(1):23–62.
T. Florian Jaeger and Harry Tily. 2011.
On
language ‘utility’: Processing complexity and
communicative efficiency.
Wiley Interdisci-
plinary Reviews: Cognitive Science, 2(3):323–
335.
Ayush Jain, Vishal Singh, Sidharth Ranjan, Ra-
jakrishnan Rajkumar, and Sumeet Agarwal.
2018.
Uniform Information Density effects
on syntactic choice in Hindi.
In Proceedings
of the Workshop on Linguistic Complexity and
Natural Language Processing, pages 38–48,
Santa Fe, New-Mexico. Association for Com-
putational Linguistics.
Diederik P. Kingma and Jimmy Ba. 2017. Adam:
A method for stochastic optimization.
Moshe Koppel and Noam Ordan. 2011.
Trans-
lationese and its dialects.
In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, page 1318–1326, Portland, Ore-
gon, USA. Association for Computational Lin-
guistics.
Marco Kuhlmann. 2010. Projective Dependency
Structures, Lecture Notes in Computer Science.
Springer, Berlin, Heidelberg.
Roger Levy. 2008.
Expectation-based syntactic
comprehension. Cognition, 106(3):1126–1177.
Roger Levy. 2018.
Communicative efficiency,
uniform information density, and the rational
speech act theory. In Proceedings for the 40th
Annual Meeting of the Cognitive Science Soci-
ety, pages 684–689.
Roger Levy and T. Florian Jaeger. 2006. Speakers
optimize information density through syntactic
reduction. In Advances in Neural Information
Processing Systems, volume 19. MIT Press.
Roger Levy and T. Florian Jaeger. 2007. Speakers
optimize information density through syntactic
reduction. Advances in neural information pro-
cessing systems, 19:849–856.
Haitao Liu. 2008. Dependency distance as a met-
ric of language comprehension difficulty. Jour-
nal of Cognitive Science, 9(2):159–191.
Steven G. Luke and Kiel Christianson. 2016. Lim-
its on lexical prediction during reading. Cogni-
tive Psychology, 88:22–60.
Kyle Mahowald, Evgeniia Diachek, Edward Gib-
son, Evelina Fedorenko, and Richard Futrell.
2022.
Experimentally measuring the redun-
dancy of grammatical cues in transitive clauses.
arXiv preprint arXiv:2201.12911.
Marie-Catherine de Marneffe, Christopher D.
Manning, Joakim Nivre, and Daniel Zeman.
2021. Universal Dependencies. Computational
Linguistics, 47(2):255–308.
Luke Maurits, Dan Navarro, and Amy Perfors.
2010. Why are some word orders more com-
mon than others? A Uniform Information Den-
sity account. In Advances in Neural Informa-
tion Processing Systems, pages 1585–1593.
Clara Meister, Tiago Pimentel, Patrick Haller,
Lena Jäger, Ryan Cotterell, and Roger Levy.
2021. Revisiting the Uniform Information Den-
sity hypothesis.
In Proceedings of the 2021
Conference on Empirical Methods in Natural
Language Processing, pages 963–980, Online
and Punta Cana, Dominican Republic. Associa-
tion for Computational Linguistics.
Myle Ott, Sergey Edunov, Alexei Baevski, Angela
Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. 2019. fairseq: A fast, ex-
tensible toolkit for sequence modeling. In Pro-
ceedings of the 2019 Conference of the North
American Chapter of the Association for Com-
putational Linguistics (Demonstrations), pages
48–53, Minneapolis, Minnesota. Association
for Computational Linguistics.
Steven T. Piantadosi, Harry Tily, and Edward Gib-
son. 2011. Word lengths are optimized for ef-
ficient communication. Proceedings of the Na-
tional Academy of Sciences, 108(9):3526–3529.
Tiago Pimentel, Ryan Cotterell, and Brian Roark.
2021a. Disambiguatory signals are stronger in
word-initial positions.
In Proceedings of the
16th Conference of the European Chapter of
the Association for Computational Linguistics:
Main Volume, pages 31–41, Online. Association
for Computational Linguistics.
Tiago Pimentel, Clara Meister, Elizabeth Salesky,
Simone Teufel, Damián Blasi, and Ryan Cot-
terell. 2021b.
A surprisal–duration trade-off
across and within the world’s languages.
In
Proceedings of the 2021 Conference on Empir-
ical Methods in Natural Language Processing,
page 949–962, Online and Punta Cana, Domini-
can Republic. Association for Computational

Linguistics.
Tiago Pimentel, Irene Nikkarinen, Kyle Ma-
howald, Ryan Cotterell, and Damián Blasi.
2021c.
How (non-)optimal is the lexicon?
In Proceedings of the 2021 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, pages 4426–4438, Online. Asso-
ciation for Computational Linguistics.
Shauli Ravfogel, Yoav Goldberg, and Tal Linzen.
2019. Studying the inductive biases of RNNs
with synthetic variations of natural languages.
In Proceedings of the 2019 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Pa-
pers), pages 3532–3542, Minneapolis, Min-
nesota. Association for Computational Linguis-
tics.
Jan Rijkhoff. 1986. Word order universals revis-
ited: The principle of head proximity. Belgian
Journal of Linguistics, 1:95–125.
Jan Rijkhoff. 1990. Explaining word order in the
noun phrase. Linguistics, 28(1):5–42.
Rico Sennrich, Barry Haddow, and Alexandra
Birch. 2016. Neural machine translation of rare
words with subword units. In Proceedings of
the 54th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Pa-
pers), pages 1715–1725, Berlin, Germany. As-
sociation for Computational Linguistics.
Cory Shain, Clara Meister, Tiago Pimentel, Ryan
Cotterell, and Roger P. Levy. 2022. Large-scale
evidence for logarithmic effects of word pre-
dictability on reading time. PsyArXiv preprint
PsyArXiv:4hyna.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Jour-
nal, 27(3):379–423.
Nathaniel J. Smith and Roger Levy. 2013.
The
effect of word predictability on reading time is
logarithmic. Cognition, 128(3):302–319.
Milan Straka and Jana Straková. 2017. Tokeniz-
ing, POS tagging, lemmatizing and parsing UD
2.0 with UDPipe. In Proceedings of the CoNLL
2017 Shared Task: Multilingual Parsing from
Raw Text to Universal Dependencies, pages 88–
99, Vancouver, Canada. Association for Com-
putational Linguistics.
David Temperley and Daniel Gildea. 2018. Mini-
mizing syntactic dependency lengths: Typolog-
ical/cognitive universal? Annual Review of Lin-
guistics, 4:1–15.
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017. At-
tention is all you need.
In Advances in Neu-
ral Information Processing Systems, volume 30.
Curran Associates, Inc.
Jennifer C. White and Ryan Cotterell. 2021. Ex-
amining the inductive bias of neural language
models with artificial languages. In Proceed-
ings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the
11th International Joint Conference on Natural
Language Processing (Volume 1: Long Papers),
pages 454–463, Online. Association for Com-
putational Linguistics.
Himanshu Yadav, Samar Husain, and Richard
Futrell. 2021. Do dependency lengths explain
constraints on crossing dependencies? Linguis-
tics Vanguard, 7(s3).
Meilin Zhan and Roger Levy. 2018. Comparing
theories of speaker choice using a model of clas-
sifier production in Mandarin Chinese. In Pro-
ceedings of the 2018 Conference of the North
American Chapter of the Association for Com-
putational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers), pages 1997–
2005, New Orleans, Louisiana. Association for
Computational Linguistics.
George Kingsley Zipf. 1935. The psycho-biology
of language: An introduction to dynamic philol-
ogy. Houghton-Mifflin, Boston.
Ran Zmigrod, Tim Vieira, and Ryan Cotterell.
2020. Please mind the root: Decoding arbores-
cences for dependency parsing.
In Proceed-
ings of the 2020 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
page 4809–4819, Online. Association for Com-
putational Linguistics.

