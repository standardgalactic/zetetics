Saving Gradient and Negative Curvature Computations:
Finding Local Minima More Eﬃciently
Yaodong Yu∗‡
and
Difan Zou†‡
and
Quanquan Gu§
Abstract
We propose a family of nonconvex optimization algorithms that are able to save gradient and
negative curvature computations to a large extent, and are guaranteed to ﬁnd an approximate
local minimum with improved runtime complexity. At the core of our algorithms is the division of
the entire domain of the objective function into small and large gradient regions: our algorithms
only perform gradient descent based procedure in the large gradient region, and only perform
negative curvature descent in the small gradient region. Our novel analysis shows that the
proposed algorithms can escape the small gradient region in only one negative curvature descent
step whenever they enter it, and thus they only need to perform at most Nϵ negative curvature
direction computations, where Nϵ is the number of times the algorithms enter small gradient
regions. For both deterministic and stochastic settings, we show that the proposed algorithms
can potentially beat the state-of-the-art local minima ﬁnding algorithms. For the ﬁnite-sum
setting, our algorithm can also outperform the best algorithm in a certain regime.
1
Introduction
We consider the following unconstrained optimization problem
min
x∈Rd f(x),
(1.1)
where f : Rd →R is twice diﬀerentiable and can be nonconvex. Note that the problem in (1.1)
becomes a stochastic optimization if f is the expectation of some underlying function indexed by
some random variable, or a ﬁnite-sum optimization if f is the average of many component functions.
For general nonconvex optimization problems, ﬁnding a global minimum can be NP-hard (Hillar
and Lim, 2013). Therefore, instead of ﬁnding the global minimum, a more modest goal is to ﬁnd a
local minimum of (1.1). In fact, for many nonconvex optimization problems in machine learning, it
is suﬃcient to ﬁnd a local minimum. Previous work in deep neural networks (Choromanska et al.,
∗Department
of
Computer
Science,
University
of
Virginia,
Charlottesville,
VA
22904,
USA;
e-
mail:yy8ms@virginia.edu
†Department of Systems and Information Engineering, University of Virginia, Charlottesville, VA 22904, USA;
e-mail: dz5an@virginia.edu
‡Equal Contribution
§Department
of
Computer
Science,
University
of
Virginia,
Charlottesville,
VA
22904,
USA;
e-mail:
qg5w@virginia.edu
1
arXiv:1712.03950v1  [cs.LG]  11 Dec 2017

2015; Dauphin et al., 2014) showed that a local minimum can be as good as a global minimum.
Moreover, recent work showed that for some machine learning problems such as matrix completion
(Ge et al., 2016), matrix sensing (Bhojanapalli et al., 2016; Park et al., 2016) and phase retrieval
(Sun et al., 2016), there is no spurious local minimum, i.e., all local minima are global minima. This
also justiﬁes the design of algorithms that guarantee convergence to a local minimum.
More speciﬁcally, we aim to ﬁnd an approximate local minimum of (1.1), which satisﬁes the
following (ϵ, ϵH)-second-order necessary optimality condition,
∥∇f(x)∥2 ≤ϵ, and λmin(∇2f(x)) ≥−ϵH,
(1.2)
where ϵ, ϵH ∈(0, 1) are predeﬁned tolerance parameters. For ρ-Hessian Lipschitz functions, if we
choose ϵH = √ρ ϵ, (1.2) reduces to the deﬁnition of ϵ-second-order stationary point in Nesterov and
Polyak (2006). In the sequel, we will use approximate local minimum and approximate second-order
stationary point interchangeably.
Cubic regularized Newton’s method (Nesterov and Polyak, 2006), which is based on the second-
order Hessian information of the function, is arguably the ﬁrst provable method that can ﬁnd the
approximate local minima (i.e., approximate second-order stationary points). However, the runtime
complexity of cubic regularization is very high, because it needs to solve a very expensive cubic
problem in each iteration. In order to overcome this computational burden, Agarwal et al. (2016)
proposed to solve the cubic problem using approximate matrix inversion. In a concurrent work,
Carmon and Duchi (2016) proposed to use gradient descent to solve the cubic problem. Both of
these methods still need to solve the cubic problem approximately in each iteration. In another
line of research, instead of using the cubic regularization technique, negative curvature direction
is directly computed for ﬁnding local minima. For example, by exploiting the structure of almost
convexity, Carmon et al. (2016) proposed a nonconvex optimization method based on accelerated
gradient descent and negative curvature descent, which only needs gradient and Hessian-vector
evaluations. Similar idea has been used in Allen-Zhu (2017) for stochastic nonconvex optimization.
Nonetheless, both algorithms need to calculate the negative curvature direction in every iteration,
which makes them less practical. On the other hand, ﬁrst-order methods with random noise injection
(Ge et al., 2015; Levy, 2016; Jin et al., 2017a) have been shown to be able to ﬁnd the approximate
second-order stationary points as well. Yet the runtime complexity of these ﬁrst-order algorithms
often has a worse dependence on ϵ than the aforementioned second-order algorithms. Very recently,
Xu and Yang (2017); Allen-Zhu and Li (2017b) showed that noise injection idea in previous work
(Jin et al., 2017a) is essentially a way to ﬁnd the negative curvature direction. Based on this insight,
they proposed ﬁrst-order methods which only need to access ﬁrst-order oracles and are able to
ﬁnd local minima as fast as the state-of-the-art Hessian-vector product-based algorithms (Agarwal
et al., 2016; Carmon et al., 2016; Allen-Zhu, 2017). Nevertheless, these algorithms (Xu and Yang,
2017; Allen-Zhu and Li, 2017b) need to perform gradient descent or its variations in each iteration,
even the algorithm is close to a ﬁrst-order stationary point. Therefore, one interesting question is:
in order to ﬁnd local minima more eﬃciently, can we design a practical algorithm that can save
gradient and negative curvature computation as much as possible?
In this paper, we show that the answer to the above question is aﬃrmative. We develop a family
of algorithms that are able to save the gradient and negative curvature computation to a large
extent. The high-level idea is: we divide the entire domain of the objective function into two regions
2

based on the magnitude of the gradient norm, i.e., ∥∇f(x)∥2. We deﬁne the large gradient region
as {x : ∥∇f(x)∥2 > ϵ} and deﬁne the small gradient region as {x : ∥∇f(x)∥2 ≤ϵ}. The proposed
algorithms will only perform gradient descent-based methods in the large gradient region, and only
perform negative curvature descent in the small gradient region. Furthermore, we will show that
a single negative curvature descent step is suﬃcient for our algorithms to escape from the small
gradient region, and therefore our algorithms must perform gradient descent-based methods in the
next iteration. This dramatically saves the gradient (or stochastic gradient) and negative curvature
computations, and improves the overall runtime complexity.
Our Contributions We summarize our major contributions as follows:
• We propose a family of practical algorithms, which are guaranteed to ﬁnd an approximate
local minimum with improved runtime complexity. The proposed algorithms are able to save
gradient and negative curvature computation strikingly. Our novel analysis shows that the
proposed algorithms can escape from each saddle point in one negative curvature descent step,
and therefore only need to compute negative curvature directions at most Nϵ times, where
Nϵ is the number of times the algorithms enter small gradient regions. Thus the runtime can
potentially be much less than that of existing state-of-the-art algorithms.
• Our proposed algorithms for ﬁnding local minima cover deterministic, stochastic and ﬁnite-sum
settings. For both deterministic and stochastic settings, we show that the proposed algorithms
can potentially outperform the state-of-the-art approximate local minima ﬁnding algorithms
in terms of runtime complexity. For the ﬁnite-sum setting, our algorithm can be better than
the state-of-the-art algorithms in a certain regime.
• Our novel proof technique is based on a reﬁned analysis of the runtime complexity, which
counts the number of gradient descent steps and the number of negative curvature descent
steps in a uniﬁed way, and rigorously integrates the high-probability and expectation-based
arguments. We believe that our proof technique is of independent interest and can be applied
to improve the analyses of many existing nonconvex optimization algorithms.
The remainder of this paper is organized as follows: We review and discuss the related work
in Section 2, and introduce some deﬁnitions in Section 3. We present our key idea, algorithm and
theoretical analysis for deterministic nonconvex optimization in Section 4. In Section 5, we present
an algorithm and theoretical analysis in the stochastic optimization setting. In Section 6, we present
an algorithm and theoretical analysis for ﬁnite-sum nonconvex optimization. Finally, we conclude
our paper and point out some future directions in Section 7.
Notation: Let A = [Aij] ∈Rd×d be a matrix and x = (x1, ..., xd)⊤∈Rd be a vector. We use
∥x∥q = (Pd
i=1 |xi|q)1/q to denote ℓq vector norm for 0 < q < +∞. Denote the spectral and Frobenius
norm of A by ∥A∥2 and ∥A∥F. For a symmetric matrix A, we denote by λmax(A), λmin(A) and
λi(A) the maximum, minimum and i-th largest eigenvalues of A. We denote by A ⪰0 that A
is positive semideﬁnite (PSD). Given two sequences {an} and {bn}, we write an = O(bn) if there
exists a constant 0 < C < +∞such that an ≤C bn. We use notation eO(·) to hide the logarithmic
factors. We also make use of the notation fn ≲gn (fn ≳gn) if fn is less than (larger than) gn up to
a constant.
3

2
Related Work
For nonconvex optimization problems, there is a vast literature on algorithms for ﬁnding approximate
local minima (i.e., approximate second-order stationary points). In general, existing work can
be divided into three setting: deterministic optimization, stochastic optimization and ﬁnite-sum
optimization. We will discuss the related work in each setting respectively.
2.1
Deterministic Setting
In the deterministic setting, algorithms can access to the full gradient and Hessian information.
Originally proposed by Nesterov and Polyak (2006), the cubic regularization algorithm converges
to the (ϵ, ϵH)-second-order stationary point in O
 max{ϵ−3/2, ϵ−3
H }

iterations. Cartis et al. (2012)
showed that the classical trust region Newton method can ﬁnd the (ϵ, ϵH)-second-order stationary
point after at most O
 max{ϵ−2ϵ−1
H , ϵ−3
H }

iterations. Later the iteration complexity of trust region
Newton methods is improved to be the same as cubic regularization (Curtis et al., 2017; Mart´ınez
and Raydan, 2017). Yet the cubic regularization method and trust region Newton method suﬀer
from solving a very expensive subproblem in each iteration. In order to alleviate the per-iteration
cost of cubic regularization, several recent work (Agarwal et al., 2016; Carmon and Duchi, 2016) was
proposed to solve the cubic subproblem approximately and achieve better runtime complexity. While
these variants of cubic regularization/trust region Newton based methods only need to compute
Hessian-vector product, they still need to access the Hessian information in each iteration.
In addition to cubic regularization and trust region Newton methods, utilizing negative curvature
directions is also able to ﬁnd second-order stationary points for nonconvex problems, which dates
back to McCormick (1977); Mor´e and Sorensen (1979); Goldfarb (1980), and is adopted in some
recent work (Carmon et al., 2016; Royer and Wright, 2017) as well. In detail, Carmon et al. (2016)
proposed to use the accelerated gradient method together with the negative curvature direction to
ﬁnd an approximate second-order stationary point, which requires eO
 ϵ−7/4
gradient and Hessian-
vector product evaluation if choosing ϵH = O(√ϵ). However, it needs to perform the negative
curvature descent step before taking accelerated gradient descent in each iteration. Royer and
Wright (2017) proposed an algorithm mainly based on line search, which involves computing the
negative curvature direction and Newton direction in each iteration. Its iteration complexity also
matches the best-known iteration complexity of second-order algorithms.
Diﬀerent from the above approaches, there is another line of research (Levy, 2016; Jin et al.,
2017a; Xu and Yang, 2017; Allen-Zhu and Li, 2017b; Jin et al., 2017b), which shows that without
second-order information, it is possible to ﬁnd a second-order stationary point using ﬁrst order
information with random noise injection. The best-known runtime complexity of this kind of
methods in the deterministic setting is O
 log6(d) ϵ−7/4
if setting ϵH = √ϵ, which is presented in a
very recent work (Jin et al., 2017b).
2.2
Stochastic Setting
In the stochastic setting, the algorithms cannot access the full gradient and Hessian directly. Instead,
they can only access the stochastic gradient and stochastic Hessian. Ge et al. (2015) analyzed a
variant of stochastic gradient descent (SGD) method by adding noise for nonconvex optimization,
4

and showed that its computational complexity for ﬁnding (ϵ, √ϵ)-second-order stationary points
is O(ϵ−4poly(d)). Based on the mechanism of variance reduction and negative curvature descent,
Allen-Zhu (2017) proposed an algorithm which is able to ﬁnd second-order stationary points faster
than SGD, and its runtime complexity is eO(ϵ−7/2) if ϵH = √ϵ. Similar to Carmon et al. (2016),
it also needs to perform negative curvature descent before applying stochastic gradient descent
based methods in each outer iteration. Kohler and Lucchi (2017); Xu et al. (2017) proposed to use
subsampled Hessian to replace the full Hessian in cubic regularization and/or trust region method,
and provided certain conditions that enable the use of subsampled Hessian while still attain the
iteration complexity of the original exact methods. Tripuraneni et al. (2017) designed a stochastic
algorithm based on cubic regularization and used eO(ϵ−7/2) stochastic gradient and Hessian-vector
product evaluations to ﬁnd (ϵ, √ϵ)-second-order stationary points. Very recently, Xu and Yang
(2017); Allen-Zhu and Li (2017b) turned the stochastically controlled stochastic gradient (SCSG)
method (Lei et al., 2017) into a local-minimum ﬁnding algorithm, by incorporating a ﬁrst-order
negative curvature ﬁnding procedure. The computational complexity of algorithms in Xu and Yang
(2017); Allen-Zhu and Li (2017b) is eO(ϵ−10/3 + ϵ−2ϵ−3
H ) for ﬁnding (ϵ, ϵH)-second-order stationary
points. However, the algorithms in Xu and Yang (2017) need to compute the negative curvature in
each iteration, and the algorithms in Allen-Zhu and Li (2017b) need to perform gradient descent
based methods in each iteration, and sometimes followed by a negative curvature descent in the
same iteration.
2.3
Finite-Sum Setting
The ﬁnite-sum optimization motivated the development of variance reduction based methods
(Johnson and Zhang, 2013; Reddi et al., 2016; Allen-Zhu and Hazan, 2016). In this setting, Agarwal
et al. (2016) proposed an algorithm to ﬁnd (ϵ, √ϵ)-second-order stationary points with eO
 nϵ−3/2 +
n3/4ϵ−7/4
gradient and Hessian-vector product evaluations. Reddi et al. (2017) introduced a generic
framework to ﬁnd (ϵ, ϵH)-second-order stationary points, and its overall runtime complexity is
eO
 n2/3ϵ−2 + nϵ−3
H + n3/4ϵ−7/2
H

by using gradient and Hessian-vector product evaluations. Very
recently, Allen-Zhu and Li (2017b) turned the nonconvex stochastic variance reduced gradient
(SVRG) (Lei et al., 2017) into a local-minimum ﬁnding algorithm, using the ﬁrst-order negative
curvature ﬁnding procedure proposed in Xu and Yang (2017); Allen-Zhu and Li (2017b). The
runtime complexity of the resulting algorithm is eO
 n2/3ϵ−2 + nϵ−3
H + n3/4ϵ−7/2
H
+ n5/12ϵ−2ϵ−1/2
H

.
Similar to the stochastic setting, the drawback of this algorithm is that it may need to perform
SVRG and negative curvature descent in the same iteration.
To compare our algorithms with the state-of-the-art methods in a more comprehensive way, we
summarize the runtime complexity of the state-of-the-art methods and our methods in Table 1. Here
we keep the second-order accuracy parameter ϵH for some algorithms for the ease of comparison,
and we can set ϵH = O(√ϵ) for diﬀerent algorithms in Table 1 to obtain the simpliﬁed results. Note
that we only show the runtime complexity of our algorithms when using the ﬁrst-order negative
curvature ﬁnding procedure (Allen-Zhu and Li, 2017b), and omit the results of our algorithms
when using Hessian-vector product based negative ﬁnding curvature procedures (Kuczy´nski and
Wo´zniakowski, 1992; Garber et al., 2016; Allen-Zhu and Li, 2017a). Note also that the algorithms
in Carmon et al. (2016); Allen-Zhu (2017); Reddi et al. (2017) can be converted to Hessian-vector
5

Table 1: A comparison of diﬀerent methods that are guaranteed to converge to second-order
stationary point in terms of run-time complexity. Here Nϵ is the number of times our algorithms
enter the small gradient region. We let Tg be the time complexity of gradient (or stochastic gradient)
evaluation, and Th be the time complexity of Hessian-vector product (or stochastic Hessian-vector
product) evaluation. Following the convention of literature, Tg and Th are considered in the same
order and hence we omit them in the big- eO notation.
Algorithm
Rumtime
Hessian-Vector
Complexity
Product
Deterministic
ANCM
(Carmon et al., 2016)
eO

1
ϵ7/4 +
1
ϵ7/2
H

needed
FastCubic
(Agarwal et al., 2016)
eO

1
ϵ3/2 +
1
ϵ7/4

needed
PAGD
(Jin et al., 2017b)
eO

1
ϵ7/4

not needed
GOSEdet
eO

1
ϵ7/4 +

1
ϵ1/4 +
1
ϵ1/2
H

min
n
1
ϵ3
H , Nϵ
o
not needed
Stochastic
Natasha2
(Allen-Zhu, 2017)
eO

1
ϵ3ϵH +
1
ϵ13/4 +
1
ϵ5
H

needed
StochasticCubic
(Tripuraneni et al., 2017)
eO

1
ϵ7/2

needed
Neon2+SCSG
(Allen-Zhu and Li, 2017b)
eO

1
ϵ10/3 +
1
ϵ2ϵ3
H +
1
ϵ5
H

not needed
GOSEstochastic
eO

1
ϵ10/3 +
1
ϵ2ϵ3
H +

1
ϵ2 +
1
ϵ2
H

min
n
1
ϵ3
H , Nϵ
o
not needed
Finite-Sum
FastCubic
(Agarwal et al., 2016)
eO

n
ϵ3/2 + n3/4
ϵ7/4

needed
Mix
(Reddi et al., 2017)
eO

n2/3
ϵ2
+
n
ϵ3
H + n3/4
ϵ7/2
H

needed
Neon2+SVRG
(Allen-Zhu and Li, 2017b)
eO

n2/3
ϵ2
+
n
ϵ3
H + n3/4
ϵ7/2
H
+ n5/12
ϵ2ϵ1/2
H

not needed
GOSEﬁnite-sum
eO

n2/3
ϵ2
+

n + n3/4
ϵ1/2
H

min
n
1
ϵ3
H , Nϵ
o
not needed
product free methods by replacing the fast PCA based negative curvature ﬁnding procedure with
the ﬁrst-order negative curvature ﬁnding procedure (Xu and Yang, 2017; Allen-Zhu and Li, 2017b).
As we can see from Table 1, in the deterministic and stochastic settings, the runtime complexity of
our algorithms can be better than other algorithms, if the number of times that our algorithms enter
small gradient regions, i.e., Nϵ, is substantially smaller than O(ϵ−3
H ). And in the worst case, the
runtime complexity of our algorithms match the best results in these two settings. In the ﬁnite-sum
setting, when n ≳ϵ−3/2, our algorithm outperforms the other algorithms if Nϵ is substantially
smaller and matches the best runtime result in the worst case.
3
Preliminary Deﬁnitions
In this section, we will introduce some deﬁnitions which will be used later.
6

Deﬁnition 3.1 (Gradient Lipschitz). A diﬀerentiable function f(·) is L-gradient Lipschitz, if for
any x, y ∈Rd, we have
∥∇f(x) −∇f(y)∥2 ≤L∥x −y∥2.
Typically, L-gradient Lipschitz implies that the eigenvalues of Hessian ∇2f(·) are conﬁned in
the region [−L, L], and this property is also known as L-smoothness.
Deﬁnition 3.2 (Hessian Lipschitz). A twice-diﬀerentiable function f(·) is ρ-Hessian Lipschitz, if
for any x, y ∈Rd, we have
∥∇2f(x) −∇2f(y)∥2 ≤ρ∥x −y∥2.
Deﬁnition 3.3 (Strongly Convex). A twice-diﬀerentiable function f(·) is µ-strongly convex, if for
any x ∈Rd, we have
λmin(∇2f(x)) ≥µ.
Deﬁnition 3.4 (Optimal Gap). For a function f(·), we introduce ∆f at point x0 which is deﬁned
as
f(x0) −inf
y∈Rd f(y) ≤∆f,
without loss of generality, we assume ∆f < +∞.
Deﬁnition 3.5 (Geometric Distribution). For a random integer N, we say it satisﬁes the geometric
distribution with parameter p, denoted as Geom(p), if it follows that
Pr(N = k) = pk(1 −p),
∀k = 0, 1, . . . .
We let Tg be the time complexity of gradient (or stochastic gradient) evaluation, and Th be time
complexity of Hessian-vector product (stochastic Hessian-vector product) evaluation. For the ease
of presentation, and also following the convention of the literature, we consider Tg and Th to be in
the same order. Thus, we use Tg to denote both Tg and Th in the sequel.
4
Deterministic Nonconvex Optimization
In this section, we present our proposed algorithm for deterministic nonconvex optimization problem
in (1.1). We ﬁrst introduce the key idea of how to save the gradient and negative curvature
evaluations and how to escape each saddle point in one step, and then present our algorithm and a
novel runtime analysis.
4.1
Key Idea
To avoid frequent negative curvature direction computation, we ﬁrst divide the domain of the
objective function into two diﬀerent regions based on the magnitude of the gradient norm: large
gradient region (∥∇f(x)∥2 > ϵ) and small gradient region (∥∇f(x)∥2 ≤ϵ). This enables us
to apply gradient-based and negative curvature-based methods respectively in these two regions.
Large gradient region: When the norm of the gradient is large, which means that the iterate
xk is in a large gradient region, we apply gradient descent based methods to make the objective
function decrease. For instance, based on the well-known convergence result of gradient descent in
7

Nesterov (1998), when the function f(·) is L-gradient Lipschitz, it can always decrease the objective
function value when the step size η ≤1/L.
Small gradient region: When the iterate xk enters the small gradient region, second-order
information is taken into account to escape from this region if the iterate is not an approximate
local minimum. In detail, suppose xk satisﬁes
∥∇f(xk)∥2 ≤ϵ, and λmin(∇2f(xk)) < −ϵH,
(4.1)
which suggests that there exists a negative curvature direction around xk. Otherwise, xk is already
the (ϵ, ϵH)-second-order stationary point. Note that the small gradient region with negative curvature
can be regarded as saddle point region. And it will become smaller as ϵ goes to 0, which indicates
that it is easy to escape from the saddle point region given an appropriate direction. Following many
existing algorithms (Carmon et al., 2016; Allen-Zhu, 2017; Reddi et al., 2017), we adopt negative
curvature descent (McCormick, 1977; Mor´e and Sorensen, 1979; Goldfarb, 1980) to escape from the
saddle point region.
More speciﬁcally, suppose λmin(∇2f(xk)) < −ϵH, a direction bv ∈Rd is called a negative
curvature direction if it satisﬁes
bv⊤∇2f(xk)bv < −ϵH
2 .
The negative curvature descent is as follows
xk+1 = xk + η ev,
(4.2)
where ev = sign(−∇f(xk)⊤bv)bv is the adjusted negative curvature direction, and η > 0 is the step
size parameter which will be speciﬁed later. In order to reduce negative curvature computation,
we try to perform negative curvature step as few as possible. We will show that by appropriately
choosing the step size η, the negative curvature descent step enjoys the following two properties,
which are formally stated later in this section,
• After taking a negative curvature descent step, the objective function value will decrease, i.e.,
f(xk+1) −f(xk) < −O(ϵ3
H).
• Gradient norm will increase after a negative curvature descent step, i.e., ∥∇f(xk+1)∥2 >
∥∇f(xk)∥2, and ∥∇f(xk+1)∥2 > ϵ.
These two properties ensure that the negative curvature descent step is able to decrease the function
value and escape from the small gradient region ∥∇f(x)∥≤ϵ at the same time. And the algorithm
will resume a gradient descent based method until it gets into another small gradient region.
4.2
Algorithm
Now we present our algorithm for ﬁnding local minima in the deterministic setting. The primary way
to ﬁnd a negative curvature direction is by computing the eigenvector corresponding to the smallest
eigenvalue of the Hessian matrix ∇2f(xk), which incurs at least O(d2) computational complexity.
Recently, some eﬃcient approaches (Kuczy´nski and Wo´zniakowski, 1992; Garber et al., 2016; Xu
and Yang, 2017; Allen-Zhu and Li, 2017b) have been proposed and investigated in order to ﬁnd the
negative curvature direction more eﬃciently.
8

One type of methods is to compute the leading eigenvector of the shift-and-invert Hessian matrix
approximately using existing fast eigenvalue decomposition/singular value decomposition algorithms.
For example, we can use the Lanczos method (Kuczy´nski and Wo´zniakowski, 1992) and the fast
PCA method (Garber et al., 2016). In the sequel, we refer to this type of methods as FastPCA.
By using any of these FastPCA algorithms, we can obtain an ϵH-approximate eigenvector with
probability at least 1 −δ in a small number of matrix-vector product operations, as spelled out in
the following lemma.
Lemma 4.1 (Kuczy´nski and Wo´zniakowski (1992)). Let f(·) be a function that is L-smooth and
ρ-Hessian Lipschitz continuous. For any x ∈Rd, let H = ∇2f(x). If λmin(H) < −ϵH, then with
probability at least 1 −δ, Lanczos method will return a unit vector bv satisfying
bv⊤H bv < −ϵH
2 ,
in at most O
 log(d/δ)
p
L/ϵH

Hessian-vector product evaluations.
Recently, another type of approaches (Xu and Yang, 2017; Allen-Zhu and Li, 2017b) has been
proposed to compute the negative curvature direction without Hessian-vector product computation.
By adding some random perturbation at the beginning, and updating the iterates based on ﬁrst-order
information, they are able to extract the approximate negative curvature. One of these algorithms
is called Neon2 (Allen-Zhu and Li, 2017b) and we summarize its result in the following lemma.
Lemma 4.2 (Allen-Zhu and Li (2017b)). Let f(·) be a function that is L-smooth and ρ-Hessian
Lipschitz continuous. For any x ∈Rd, let H = ∇2f(x). With probability at least 1 −δ, Neon2det
returns bv satisfying one of the following conditions,
• bv = ⊥, then λmin(H) ≥−ϵH.
• bv ̸= ⊥, then bv⊤H bv ≤−ϵH/2 with ∥bv∥2 = 1.
The total number of gradient evaluations is O
 log2(d/δ)
p
L/ϵH

.
Note that in both Lemmas 4.1 and 4.2, the iteration complexity depends on log d factor. This is
because the logarithmic dependence on the dimension d in eigenvector approximation is unavoidable
(Simchowitz et al., 2017).
To summarize, there exists an algorithm, denoted by ApproxNC(·), that returns a unit vector bv
such that bv⊤H bv ≤−ϵH/2 if λmin(∇2f(x)) < −ϵH, otherwise it returns bv = ⊥. Next we present
our Algorithm 1, which is able to escape from a saddle point in one step.
Algorithm 1 One-Step-Deterministic (f(·), x, ϵH, ρ, δ)
1: Set η ←ϵH/(2c1ρ)
2: bv ←ApproxNC (f(·), x, ϵH, δ)
3: if bv ̸= ⊥then
4:
ev ←sign(−∇f(x)⊤bv)bv
5:
y = x + η ev
6:
return y
7: else
8:
return ⊥
9

As described in Algorithm 1, we ﬁrst estimate the negative curvature of function f(·) at x, by
using FastPCA (Garber et al., 2016) or Neon2 (Allen-Zhu and Li, 2017b). If bv ̸= ⊥, we can adjust
the direction of bv as ev according to the gradient ∇f(x), take a negative curvature descent step along
ev with appropriate step size η and return y. Otherwise Algorithm 1 will return ⊥. As we will prove
later in this section, one can use Algorithm 1 to escape a saddle point x with λmin(∇2f(x)) < −ϵH.
Next we are going to present a practical algorithm for ﬁnding local minima in the deterministic
setting, which is summarized in Algorithm 2.
Algorithm 2 Gradient descent with One-Step Escaping, GOSE-Deterministic (x0, ϵ, ϵH, L, ρ, δ)
1: for k = 1, 2, ... do
2:
if ∥∇f(xk−1)∥2 > ϵ
3:
xk ←GN-AGD(f(·), xk−1, L, ϵ, ϵ1/2, ϵ1/2/ρ)
4:
else
5:
xk ←One-Step-Deterministic(f(·), xk−1, ϵH, ρ, δ)
6:
if xk = ⊥
7:
return xk−1
8: endfor
As we can see from the pseudocode in Algorithm 2, for large gradient regions, we apply the
Guarded Nonconvex Accelerated Gradient Descent algorithm (GN-AGD(·)) (Carmon et al., 2017),
a variant of accelerated gradient descent method for minimizing nonconvex functions, to ﬁnd
ϵ-ﬁrst-order stationary points.
Please refer to Algorithm 3 in Carmon et al. (2017) for more
details.
According to the analysis in Carmon et al. (2017), GN-AGD(·) can ﬁnd ϵ-ﬁrst-order
stationary points faster than gradient descent. More speciﬁcally, to ﬁnd ϵ-ﬁrst-order stationary
points, classical gradient descent method needs O(ϵ−2) gradient evaluations, while GN-AGD(·) only
needs O(ϵ−7/4 log(1/ϵ)) gradient and function evaluations.
In each iteration, Algorithm 2 either performs GN-AGD(·) to ﬁnd an ϵ-ﬁrst-order stationary
point xk, i.e., ∥∇f(xk)∥2 ≤ϵ, or takes a negative curvature step to escape from the small gradient
region if xk−1 is a strict saddle point. In the extreme case that the objective function is convex or
the algorithm does not encounter any strict saddle point with λmin(∇2f(x)) < −ϵH, Algorithm 2
will perform negative curvature direction computation only once. This makes our algorithm very
practical and appealing, compared with existing algorithms that heavily involve negative curvature
computation in each iteration.
4.3
Runtime Analysis
Now we are going to present the runtime analysis of Algorithm 2. We ﬁrst show that Algorithm 1
will escape from each saddle point in one step and satisfy the two conditions as we described in
Section 4.1, i.e., function value decreases and gradient norm increases. It plays a pivotal role in the
proof of the main theory.
Lemma 4.3. Suppose function f(·) is L-gradient Lipschitz and ρ-Hessian Lipschitz, c1 ≥1 and
ϵ < ϵ2
H/(16c1ρ), set η = ϵH/(2c1ρ). If the input x of Algorithm 1 that satisﬁes ∥∇f(x)∥2 ≤ϵ and
λmin(∇2f(x)) < −ϵH, then with probability at least 1 −δ, the output y generated by Algorithm 1
10

satisﬁes
f(y) −f(x) < −C′ϵ3
H
ρ2 , and ∥∇f(y)∥2 > ϵ,
where C′ > 0 is a constant, and the total runtime is eO
 p
L/ϵH

Tg

.
The following lemma, which is adapted from Carmon et al. (2017), characterizes the iteration
complexity of the GN-AGD algorithm.
Lemma 4.4 (Carmon et al. (2017)). Let f(·) be a function that is L-gradient Lipschitz and
ρ-Hessian Lipschitz continuous. Consider ϵ ∈
 0, min{∆2/3
f
ρ1/3, L2/(64ρ)}

, at the k-th outer loop
of Algorithm 1, GN-AGD(·) returns a ﬁrst-order stationary point xk such that ∥∇f(xk)∥2 < ϵ with
the total number of gradient evaluations
eO
 f(xk−1) −f(xk)

L1/2ρ1/4
ϵ7/4
+ L1/2ρ−1/4
ϵ1/4

.
(4.3)
Based on Lemmas 4.3 and 4.4, we are able to perform the runtime complexity of Algorithm 2
for ﬁnding approximate local minima.
Theorem 4.5. Suppose function f(·) is L-gradient Lipschitz and ρ-Hessian Lipschitz. Setting
ϵ < ϵ2
H/(16c1ρ) with c1 ≥1, with probability 1 −δ, Algorithm 2 returns an (ϵ, ϵH)-second-order
stationary point with runtime
eO
∆fL1/2ρ1/4
ϵ7/4
+
L1/2ρ−1/4
ϵ1/4
+ L1/2
ϵ1/2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
(4.4)
where Nϵ is the number of times Algorithm 2 enters small gradient regions.
Remark 4.6. Note that if we assume the runtime of gradient evaluation is in the same order as the
runtime of Hessian-vector product, the computational complexity of FastPCA can be slightly better
than Neon2 in the deterministic setting, since its iteration complexity has a better dependence in
dimension d by a factor of log d.
Remark 4.7. It is worth noting that Nϵ can be substantially smaller than O(1/ϵ3
H), which implies
the second term in (4.4) can be very small. Furthermore, if there only exists a ﬁnite number of
strict saddle points, such as the function f : Rd →R constructed in Du et al. (2017), Algorithm 2 is
guaranteed to estimate the negative curvature direction no more than (d + 1) times.
5
Stochastic Nonconvex Optimization
In this section, we consider the nonconvex optimization problem in the stochastic setting
min
x∈Rd f(x) = Eξ∼D[F(x; ξ)],
(5.1)
where ξ is a random variable satisfying distribution D, F(x; ξ) is a stochastic smooth function and
can be nonconvex. In the stochastic setting, one cannot directly access the full gradient and Hessian
11

information of f(x). Instead, one can only get unbiased estimators of the gradient and Hessian of
f(x). Note that the stochastic setting is referred to as online setting in some recent work (Allen-Zhu,
2017; Allen-Zhu and Li, 2017b).
Our goal is to ﬁnd a local minimum of the expected function f(x) in (5.1). Similar to the idea
in the deterministic setting, we ﬁrst need to ﬁnd an ϵ-ﬁrst-order stationary point xk, and then take
a negative curvature descent step to escape from the small gradient region if λmin(∇2f(xk)) < −ϵH.
Since we cannot access the entire information of f(x), we will instead use a mini-batch of stochastic
gradients and stochastic Hessian-vector products in the above procedures. In detail, we will ﬁrst
show that we could apply stochastic gradient and stochastic Hessian-based methods to estimate the
negative curvature direction, and the resulting algorithm will still be able to escape from a saddle
point by taking a single negative curvature descent step, which largely reduces the negative curvature
computation. Then we integrate our stochastic one-step escaping algorithm and a stochastic gradient
descent based algorithm to ﬁnd local minima more eﬃciently.
5.1
Algorithm
In order to perform a negative curvature descent step as in Algorithm 1, by which the algorithm can
escape from the small gradient region in one step, we need to ﬁnd a negative curvature direction bv
in the stochastic setting. Similar to the deterministic setting, there are mainly two types of methods
to ﬁnd the negative curvature direction by only using stochastic gradient or stochastic Hessian
vector product.
We can use online Oja’s algorithm (Allen-Zhu and Li, 2017a) to compute the negative curvature
direction in the stochastic setting, as shown in the following lemma.
Lemma 5.1 (Allen-Zhu and Li (2017a)). Let f(x) = Eξ[F(x; ξ)] and F(x; ξ) be a function that is
L-gradient Lipschitz and ρ-Hessian Lipschitz continuous. For any x ∈Rd, denote H as H = ∇2f(x).
If λmin(H) < −ϵH, then with probability at least 1 −δ, online Oja’s algorithm will return a unit
vector bv satisfying
bv⊤H bv < −ϵH
2 ,
in at most O
 log2(d/δ) log(1/δ)L2/ϵ2
H

stochastic Hessian-vector product evaluations.
We can also use online Neon2 (Allen-Zhu and Li, 2017b) to compute the negative curvature
direction.
Lemma 5.2 (Allen-Zhu and Li (2017b)). Let f(x) = Eξ[F(x; ξ)] and F(x; ξ) be a function that is
L-gradient Lipschitz and ρ-Hessian Lipschitz continuous. For any x ∈Rd, denote H as H = ∇2f(x).
With probability at least 1 −δ, Neon2online returns bv satisfying one of the following conditions,
• bv = ⊥, then λmin(H) ≥−ϵH.
• bv ̸= ⊥, then bv⊤H bv ≤−ϵH/2 with ∥bv∥2 = 1.
The total number of stochastic gradient evaluations is O
 log2(d/δ)L2/ϵ2
H

.
Based on the above two lemmas, in the stochastic setting, there exists an algorithm that uses
stochastic gradient or stochastic Hessian vector product, denote by ApproxNC-Stochastic(·), and
12

returns a unit vector bv such that bv⊤∇2f(x)bv ≤−ϵH/2 if λmin(∇2f(x)) < −ϵH, otherwise it returns
bv = ⊥.
Next we present our algorithm in Algorithm 3, which is able to escape from a saddle point in
one step in the stochastic setting. Here ∇fS(x) = 1/|S| P
ξi∈S ∇F(x; ξi) is a minibach of stochastic
gradients.
Algorithm 3 One-Step-Stochastic (f(·), x, ϵH, ρ, δ)
1: Set η ←ϵH/(2c1ρ), |S| = eO(1/ϵ2
H)
2: bv ←ApproxNC-Stochastic (f(·), x, ϵH, δ)
3: if bv ̸= ⊥then
4:
bg = ∇fS(x)
5:
ev ←sign(−bg⊤bv)bv
6:
y = x + η ev
7:
return y
8: else
9:
return ⊥
As shown in Algorithm 3, there are two diﬀerences compared with Algorithm 1. First, we adopt
ApproxNC-Stochastic(·) here to ﬁnd the negative curvature direction. Second, due to the fact that
the full gradient of the expected function f(x) is not accessible, we utilize a subsampled gradient bg
to approximate its full gradient, and then compute a descent direction accordingly, i.e., ev. Since bg
is an unbiased estimator of ∇f(x), the diﬀerence between bg and ∇f(x) can be suﬃciently small
as long as the sample size |S| is large enough. As a result, the negative curvature descent step
along ev will also make the function value decrease and escape from the small gradient region if
λmin(∇2f(x)) ≤−ϵH.
To ﬁnd local minima in the stochastic setting, similar to the deterministic setting, we need
to ﬁrst ﬁnd an ϵ-ﬁrst-order stationary point xk and then apply Algorithm 3 to escape the small
gradient region or return the current iterate xk. Here we adopt the Stochastically Controlled
Stochastic Gradient (SCSG) method in Lei et al. (2017), which is a variant of variance reduction
based method for nonconvex optimization problems. The SCSG algorithm ﬁrst estimates the full
gradient using a minibach of stochastic gradients with batch size B, and then performs variance
reduced semi-stochastic gradient with mini-batch size b for Tk times, where Tk is randomly drawn
from a geometric distribution with parameter B/(B + b). The reason why we apply SCSG is that
its iteration complexity to ﬁnd an ϵ-ﬁrst-order stationary point is O(1/ϵ10/3), which is faster than
other ﬁrst-order stochastic optimization methods such as stochastic gradient descent (SGD) for
general nonconvex optimization problems in the stochastic setting. Note that SCSG has been
used in existing local minima ﬁnding algorithms (Xu and Yang, 2017; Allen-Zhu and Li, 2017b).
Nevertheless, our algorithm is diﬀerent from these algorithms in that our algorithm can further save
the gradient and negative curvature evaluations due to the small gradient region and large gradient
region division.
Then we present Algorithm 4 by combining One-Step-Stochastic(·) and the SCSG algorithm. In
each outer loop, GOSE-Stochastic(·) either executes One-Step-Stochastic(·) or performs one-epoch
SCSG algorithm according to a subsampled gradient measurement ∇fSk(xk−1). Note that we set
13

Algorithm 4 GOSE-Stochastic (f(·), x0, ϵ, ϵH, L, ρ, δ, K)
1: Set B ←eO(H∗/ϵ2), b ←eO(ρ6H∗ϵ4/L3ϵ9
H), η ←b2/3/6LB2/3
2: for k = 1, 2, ..., K
3:
uniformly sample a batch Sk ∼D with |Sk| = B
4:
gk ←∇fSk(xk−1)
5:
if ∥gk∥2 > ϵ/2
6:
generate Tk ∼Geom(B/(B + b))
7:
y(k)
0
←xk−1
8:
for t = 1, 2, ..., Tk
9:
randomly pick It−1 ⊂[n] with |It−1| = b
10:
y(k)
t
←y(k)
t−1 −η
 ∇fIt−1(y(k)
t−1) −∇fIt−1(y(k)
0 ) + gk

11:
end for
12:
xk ←y(k)
Tk
13:
else
14:
xk ←One-Step-Stochastic(f(·), xk−1, ϵH, ρ, δ)
15:
if xk = ⊥
16:
return xk−1
17: end for
the threshold as ϵ/2 to ensure that the full gradient ∇f(x) is small when performing negative
curvature descent step. If the norm of estimated gradient is large, i.e., ∥∇fSk(xk−1)∥2 > ϵ/2, which
means that the full gradient of f(x) at xk−1 is also large according to the concentration result of
∇fSk(xk−1) on ∇f(xk−1). Then we can show that xk is not an ϵ-ﬁrst-order stationary point, and
run the one-epoch SCSG algorithm. Otherwise, One-Step-Stochastic(·) should be performed in order
to ﬁnd the negative curvature direction and escape from the saddle point if λmin(∇2f(xk)) < −ϵH.
Note that in each iteration, Algorithm 4 either performs one-epoch SCSG or a negative curvature
descent step, which can save the gradient and negative curvature computation to a large extent.
5.2
Runtime Analysis
Now we are going to provide a runtime analysis of Algorithm 4. Before we present our main result,
we ﬁrst make some additional assumptions on the stochastic function F(x; ξ).
Assumption 5.3. For all x ∈Rd, ξ ∼D, the stochastic gradient ∇F(x; ξ) is sub-Gaussian with
parameter σ, i.e., ∥∇F(x; ξ) −∇f(x)∥ψ2 ≤σ, where ∥x∥ψ2 = supp≥1 p−1/2E∥x∥p.
By Assumption 5.3, we immediately have
E∥∇F(x; ξ) −∇f(x)∥2
2 ≤2σ2 ≜H∗,
which is a uniform upper bound of the variance of F(x; ξ) for all x ∈Rd.
Next, we give a large deviation bound on the distance between the subsampled gradient bg and
the full gradient ∇f(x) in the following lemma.
Lemma 5.4. (Ghadimi et al., 2016) Suppose the stochastic gradient ∇F(x; ξ) is sub-Gaussian, for
14

any given c > 0, if the sample size |S| = O((σ2/c2ϵ2) log(1/δ)), then with probability 1 −δ,
∥bg −∇f(x)∥2 ≤c ϵ
holds for any x ∈Rd, where bg = 1/|S| P
ξi∈S ∇F(x; ξi).
The above lemma is a standard concentration bound for sub-Gaussian random vectors (Vershynin,
2010; Ghadimi et al., 2016), which implies that we can use the subsampled gradient bg to approximate
the full gradient ∇f(x) provided that the subsample size is large enough.
Next we show that Algorithm 3 can escape from each saddle point in one step and satisfy the two
conditions as we described in Section 4.1, i.e., function value decreases and gradient norm increases.
Lemma 5.5. Let f(x) = Eξ[F(x; ξ)] and suppose F(x; ξ) be a function that is L-smooth and
ρ-Hessian Lipschitz continuous, c1 ≥1 and ϵ < ϵ2
H/(16c1ρ), |S| = eO(1/ϵ2
H), and set step size
η = ϵH/(2c1ρ). If the input x of Algorithm 3 that statisﬁes ∥∇f(x)∥2 ≤ϵ and λmin(∇2f(x)) < −ϵH,
then with probability at least 1 −δ, the output y generated by Algorithm 3 satisﬁes
f(y) −f(x) < −C′ϵ3
H
ρ2 , and ∥∇f(y)∥2 > ϵ,
where C′ > 0 is a constant, and the runtime is eO
 
L2/ϵ2
H

Tg

.
The above lemma is similar to Lemma 4.3 in the deterministic setting. Note that here we need
to deal with the subsampled gradient rather than the exact one, and the runtime is diﬀerent from
that in Lemma 4.3.
In what follows, a crucial lemma that characterizes the function value increments for one-epoch
SCSG algorithm is presented.
Lemma 5.6 (Lei et al. (2017)). Consider f(x) = Eξ[F(x; ξ)], assume that F(x; ξ) is L-smooth
Lipschitz continuous, and the stochastic gradient ∇F(x; ξ) is sub-Gaussian. Suppose that Algorithm
4 performs one-epoch SCSG algorithm at k-th outer loop, there exists constant C > 1 such that the
following holds for y(k)
0
and y(k)
Tk ,
E
h∇f(y(k)
Tk )
2
2
i
≤CLb1/3
B1/3 E

f(y(k)
0 ) −f(y(k)
Tk )

+ 6 · 1{B < n}
B
· H∗,
where B and b denote the batch size and mini-batch size respectively, and H∗is an upper bound on
the variance of stochastic gradients.
Based on the above two lemmas, we are able to establish the runtime complexity guarantee of
Algorithm 4 with constant probability.
Theorem 5.7. Consider f(x) = Eξ[F(x; ξ)], assume that F(x; ξ) is L-smooth and ρ-Hessian Lips-
chitz continuous, the stochastic gradient ∇F(x; ξ) is sub-Gaussian, and ϵ ≤min

ϵ3/2
H , ϵ2
H/(16c1ρ)
	
.
Considering batch size B = eO(H∗/ϵ2), mini-batch size b = eO(ρ6H∗ϵ4/(L3ϵ9
H)), and η = b2/3/(6LB2/3),
with probability 1/3, Algorithm 4 returns an (ϵ,ϵH)-second-order stationary point with runtime
eO
L∆fH∗2/3
ϵ10/3
+ ρ2∆fH∗
ϵ3
Hϵ2
+
H∗
ϵ2 + L2
ϵ2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
15

where Nϵ is the number of times Algorithm 4 enters small gradient regions.
Remark 5.8. Note that the runtime complexity guarantee in Theorem 5.7 holds with probability
1/3. In practice, one can improve the constant probability to 1 −δ probability for any 0 < δ < 1 by
repeat GOSE-Stochastic for O(log(1/δ)) times.
Remark 5.9. We consider a special regime that ϵH ≲ϵ2/3, which implies that b ≥B. Then SCSG
will degenerate into the SGD algorithm. Since the derivation for its runtime complexity is similar
to that for SCSG, we omit the analysis for SGD in this work. The interested readers can refer to
Xu and Yang (2017); Allen-Zhu and Li (2017b) for more details. The rest of the discussion is in the
regime that ϵH ≳ϵ2/3, i.e., b < B. In this regime, if the number of encountered saddle points in
Algorithm 4 is smaller than O(ϵ−3
H ), GOSE-Stochastic(·) outperforms the state-of-the-art algorithms
(Tripuraneni et al., 2017; Xu and Yang, 2017; Allen-Zhu and Li, 2017b). In addition, when H∗= 0,
SCSG degenerates to full gradient descent method, and the number of stochastic gradient evaluations
for ﬁnding an (ϵ,ϵH)-second-order stationary point becomes eO(L∆fϵ−2 + L2ϵ−2
H min{ρ2∆fϵ−3
H , Nϵ}).
6
Finite-Sum Nonconvex Optimization
In this section, we consider a special case of nonconvex stochastic optimization, where the objective
function can be written as an average of ﬁnite sum component functions
min
x∈Rd f(x) = 1
n
n
X
i=1
fi(x),
(6.1)
where each fi(x) is smooth and can be nonconvex. Nonconvex optimization problems with this
generic structure appear widely in machine learning such as training deep neural networks (LeCun
et al., 2015). In contrast to the stochastic setting we discussed before, here we have access to the full
information of function f(·), which is referred to the oﬄine setting in some exiting work (Allen-Zhu,
2017; Tripuraneni et al., 2017).
Compared with the deterministic setting, it has been shown that stochastic algorithms can
utilize the ﬁnite-sum structure in (6.1) and further reduce the gradient complexity for both convex
and nonconvex optimization problems. As for nonconvex ﬁnite-sum optimization problems, previous
work (Reddi et al., 2016; Allen-Zhu and Hazan, 2016) showed that nonconvex stochastic variance
reduced gradient (SVRG) methods are faster than classical gradient descent methods by a factor of
O(n1/3). As a result, we could make use of the ﬁnite-sum structure and further reduce the runtime
complexity of Algorithm 2 if the objective function is of ﬁnite sum structure.
6.1
Algorithm
In order to escape from each saddle point in one step to reduce the negative curvature computation,
ﬁrst we need to ﬁnd a negative curvature direction bv in the ﬁnite-sum setting. Similar to the
previous settings, there exist two stochastic methods to ﬁnd the negative curvature direction by
using gradient or Hessian-vector product evaluations. We present the runtime complexity results of
these two methods in the following two lemmas respectively.
16

Lemma 6.1 (Garber et al. (2016)). Let f(x) = 1
n
Pn
i=1 fi(x) and each fi(·) be a function that is
L-smooth and ρ-Hessian Lipschitz continuous. For any x ∈Rd, denote H as H = ∇2f(x). Then
with probability at least 1 −δ, Shifted-and-Inverted power method will return a unit vector bv
satisfying
bv⊤H bv < λmin(H) + ϵH
2 ,
in at most O
 (n + n3/4p
L/ϵH) log3(d) log(1/δ)

stochastic Hessian-vector product evaluations.
Lemma 6.2 (Allen-Zhu and Li (2017b)). Let f(x) = 1
n
Pn
i=1 fi(x) and each fi(·) be a function
that is L-smooth and ρ-Hessian Lipschitz continuous. For any x ∈Rd, denote H as H = ∇2f(x).
With probability at least 1 −δ, Neon2svrg returns bv satisfying one of the following conditions,
• bv = ⊥, then λmin(H) ≥−ϵH.
• bv ̸= ⊥, then bv⊤H bv ≤−ϵH/2 with ∥v∥2 = 1.
The total number of stochastic gradient evaluations is O
 (n + n3/4p
L/ϵH) log2(d/δ)

.
Diﬀerent from the deterministic setting, the runtime complexities of these two methods for
ﬁnding the negative curvature direction are faster than those in the deterministic setting by a
factor of O(n1/4), because they can exploit the ﬁnite-sum structure. According to above two
lemmas, in the ﬁnite-sum setting, there exists an algorithm, denoted by ApproxNC-FiniteSum(·),
that only uses either gradient evaluation, or Hessian vector product, and returns a unit vector bv
such that bv⊤∇2f(x)bv ≤−ϵH/2 if λmin(∇2f(x)) < −ϵH, otherwise it returns bv = ⊥. Based on
ApproxNC-FiniteSum(·), we present Algorithm 5, which is able to escape from each saddle point in
one step in the ﬁnite-sum setting.
Algorithm 5 One-Step-FiniteSum (f(·), x, ϵH, ρ, δ)
1: Set η ←ϵH/2c1ρ
2: bv ←ApproxNC-FiniteSum (f(·), x, ϵH, δ)
3: if bv ̸= ⊥then
4:
ev ←sign(−∇f(x)⊤bv)bv
5:
y = x + η ev
6:
return y
7: else
8:
return ⊥
The remaining thing is similar to previous settings. In detail, to ﬁnd ϵ-ﬁrst-order stationary
points, we adopt the nonconvex stochastic variance reduced gradient (SVRG) method (Reddi et al.,
2016; Allen-Zhu and Hazan, 2016; Lei et al., 2017) in our algorithm, since it is O(n1/3) faster
than classical gradient descent methods in the nonconvex ﬁnite-sum setting. For the simplicity of
presentation, we adopt the Stochastically Controlled Stochastic Gradient (SCSG) method (Lei et al.,
2017), a variant of SVRG, as an instance of our algorithm, while other algorithms in this family are
also applicable and will give rise to the same theoretical guarantee. In order to escape from the
small gradient region in one step, we call Algorithm 5.
17

Algorithm 6 GOSE-FiniteSum (f(·), x0, ϵ, ϵH, L, ρ, δ, K)
1: Set b ←1, η ←1/Ln2/3
2: for k = 1, 2, ..., K
3:
gk ←∇f(xk−1)
4:
if ∥gk∥2 > ϵ
5:
generate Tk ∼Geom(n/(n + b))
6:
y(k)
0
←xk−1
7:
for t = 1, 2, ..., Tk
8:
randomly pick It−1 ⊂[n] with |It−1| = b
9:
y(k)
t
←y(k)
t−1 −η
 ∇fIt−1(y(k)
t−1) −∇fIt−1(y(k)
0 ) + gk

10:
end for
11:
xk ←y(k)
Tk
12:
else
13:
xk ←One-Step-FiniteSum(f(·), xk−1, ϵH, ρ, δ)
14:
if xk = ⊥
15:
return xk−1
16: end for
We present GOSE-FiniteSum(·) in Algorithm 6, which either performs One-epoch SCSG al-
gorithm or One-Step-FiniteSum algorithm, depending on the norm of gradient ∇f(xk−1) in the
beginning of each outer loop. Compared with Algorithm 4 in the stochastic setting, here ∇f(xk−1) is
the full gradient at xk−1. Algorithm 6 keeps performing One-epoch SCSG until it ﬁnds an ϵ-ﬁrst-order
stationary point. Then it will take a negative curvature descent step if λmin(∇2f(xk−1)) < −ϵH,
otherwise it will return xk−1. Similar to the algorithms in deterministic and stochastic settings,
Algorithm 6 reduces gradient and negative curvature computation as much as possible, and can be
very eﬃcient in practice.
6.2
Runtime Analysis
Now we provide the theoretical analysis of Algorithms 5 and 6. To begin with, we present the
runtime analysis of Algorithm 5.
Lemma 6.3. Suppose function f(·) is L-gradient Lipschitz and ρ-Hessian Lipschitz, c1 ≥1 and
ϵ < ϵ2
H/(16c1ρ), set step size η = ϵH/(2c1ρ). If the x of Algorithm 5 that satisﬁes ∥∇f(x)∥2 ≤ϵ and
λmin(∇2f(x)) < −ϵH, then with probability at least 1 −δ, the output y generated by Algorithm 1
satisﬁes
f(y) −f(x) < −C′ϵ3
H
ρ2 , and ∥∇f(y)∥2 > ϵ,
where C′ > 0 is a constant, and the runtime is eO
 
n + n3/4p
L/ϵH

Tg

.
The proof of the above lemma is almost identical to Lemma 4.3, except that the runtime
complexity of Algorithm 5 is improved over Algorithm 1. To analyze Algorithm 6, we ﬁrst present
the following lemma, which characterizes the expected function value increments when performing
one-epoch SCSG algorithm.
18

Lemma 6.4 (Lei et al. (2017)). Suppose function f(·) is L-gradient Lipschitz, suppose Algorithm
6 perform one-epoch SCSG algorithm at k-th outer loop, there exists constant C > 1 such that the
following holds for y(k)
0
and y(k)
Tk
E
h
∥∇f(y(k)
Tk )∥2
2
i
≤CL(b/B)1/3E

f(y(k)
0 ) −f(y(k)
Tk )

,
where B and b denote the batch size and mini-batch size, respectively.
It is worth noting that similar results as Lemma 6.4 have also been derived in Reddi et al. (2016);
Allen-Zhu and Hazan (2016). Now, based on Lemma 6.3 and Lemma 6.4, we are ready to deliver
the main result for Algorithm 6 as follows.
Theorem 6.5. Suppose function f(·) is L-gradient Lipschitz and ρ-Hessian Lipschitz. Considering
the batch size B = n, mini-batch size b = 1, and ϵ ≤ϵ2
H/(16c1ρ), with probability 1/3, Algorithm 6
returns an (ϵ, ϵH)-second-order stationary point with runtime
eO
L∆fn2/3
ϵ2
+

n + n3/4L1/2
ϵ1/2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
where Nϵ is the number of times Algorithm 6 enters small gradient regions.
Remark 6.6. Note that the runtime complexity in Theorem 6.5 holds with probability 1/3. One
can improve it to probability 1 −δ by randomly repeat GOSE-Stochastic for O(log(1/δ)) times.
In addition, when the total sample size n ≳ϵ−3/2 and Nϵ is substantially less than O(ϵ−3
H ), the
total runtime of Algorithm 6 is eO([n2/3ϵ−2]Tg), which outperforms the best ﬁnite-sum algorithms
in Agarwal et al. (2016); Allen-Zhu and Li (2017b).
7
Conclusions and Future Work
We proposed a family of algorithms for ﬁnding approximate local minima for general nonconvex
optimization in diﬀerent settings. Our algorithms are guaranteed to escape from each saddle point
in one step using negative curvature descent, and save gradient and negative curvature computation
to a large extent. An interesting question is whether our algorithm can be extended to constrained
nonconvex optimization problems. We will study it in the future work.
A
Proofs for Deterministic Nonconvex Optimization
A.1
Proof of Lemma 4.3
Proof. First, we show that the negative curvature step (4.2) will decrease the function value, i.e.,
f(y) −f(x) < 0. Based on the assumption that f(·) is ρ-Hessian Lipschitz, we can get
f(y) ≤f(x) + ⟨∇f(x), y −x⟩+ 1
2⟨y −x, ∇2f(x)(y −x)⟩+ ρ
6∥y −x∥3
2, ∀x, y ∈Rd.
19

According to the update form y = x + ηev, we could get
f(y) ≤f(x) + η⟨∇f(x), ev⟩+ η2
2 ⟨ev, ∇2f(x)ev⟩+ ρη3
6 ∥ev∥3
2.
Based on the deﬁnition of vt, we obtain
⟨∇f(x), ev⟩= sign(−∇f(x)⊤bv)⟨∇f(x), bv⟩≤0.
Then we have
f(y) ≤f(x) + η2
2 ⟨ev, ∇2f(x)ev⟩+ ρη3
6 ∥ev∥3
2
≤f(x) −
ϵHη2
4
−ρη3
6

.
Taking η = CH (ϵH/ρ) such that
0 < η < 3ϵH
2ρ ,
(A.1)
which implies that CH ∈(0, 3/2), we are able to obtain
f(y) −f(x) ≤−
C2
H
4
−C3
H
6
ϵ3
H
ρ2 < 0,
which guarantees that there will be a suﬃcient decrease after performing negative curvature step (4.2).
Secondly, we prove that the norm of the gradient increases after the negative curvature step,
i.e., ∥∇f(y)∥2 > ϵ. Note that the gradient ∇f(xk+1) can be rewritten as:
∇f(y) = ∇f(x) + η
Z 1
0
∇2f(x + θηev)evdθ
= ∇f(x) + η
Z 1
0
∇2f(x)evdθ + η′
Z 1
0

∇2f(x + θηev) −∇2f(x)

evdθ
= ∇f(xt) + η′∇2f(xt)vt + ∆t,
where ∆t = η
R 1
0
 ∇2f(x + θηev) −∇2f(x)
evdθ. Then we analyze the upper bound of ∥∆t∥2,
∥∆t∥2 = η

Z 1
0

∇2f(x + θηev) −∇2f(x)

evdθ

2
≤η
Z 1
0
∇2f(x + θηev) −∇2f(x)

2∥ev∥2dθ
≤ρη2
Z 1
0
θ∥ev∥2
2dθ
= 1
2ρη2.
20

Next we get the lower bound of ∥∇f(y) −∇f(x)∥2,
∥∇f(y) −∇f(x)∥2 ≥η∥∇2f(x)ev∥2 −1
2ρη2
= η∥ev∥2 · ∥∇2f(x)ev∥2 −1
2ρη2
≥η|⟨ev, ∇2f(x)ev⟩| −1
2ρη2
≥1
2ϵHη −1
2ρη2,
where the ﬁrst equation is because ∥ev∥2 = 1, and the third inequality is because |ev⊤∇2f(x)ev| ≥ϵH/2.
Then we can derive that
∥∇f(y)∥2 ≥1
2ϵHη −1
2ρη2 −∥∇f(x)∥2 ≥1
2ϵHη −1
2ρη2 −ϵ.
In order to guarantee that ∥f(y)∥2 > ϵ, we need to set η satisﬁes
1
2ϵHη −1
2ρη2 −ϵ > ϵ ⇔1
2ϵHη −1
2ρη2 −2ϵ > 0.
Moreover, it is necessary to set ϵ < ϵ2
H/16ρ in order to guarantee there exists a solution to the above
inequality, then we can get
ϵH
2ρ −
s
ϵ2
H
4ρ2 −4ϵ
ρ < η < ϵH
2ρ +
s
ϵ2
H
4ρ2 −4ϵ
ρ .
As we deﬁned η as η = CH (ϵH/ρ), the above inequality is equivalent to
1
2 −1
2
s
1 −16ρ ϵ
ϵ2
H
< CH < 1
2 + 1
2
s
1 −16ρ ϵ
ϵ2
H
.
(A.2)
It is worth noting that if CH satisﬁes the above condition (A.1), it also satisﬁes condition
(A.2). Finally we can obtain that if η = CH (ϵH/ρ), and CH ∈(1/2 −1/2
q
1 −16ρ ϵ/ϵ2
H, 1/2 +
1/2
q
1 −16ρ ϵ/ϵ2
H), the negative curvature step (4.2) will satisﬁes
f(y) −f(x) < −C′ϵ3
H
ρ2 , and ∥∇f(y)∥2 > ϵ,
where C′ = C2
H/4 −C3
H/6 > 0 is a constant. For choosing the step size parameter η, we can
overshoot the Hessian Lipschitz parameter ρ as c1ρ where c1 ≥1. Then the conclusion still holds
and we can simply choose the step size parameter as η = ϵH/2c1ρ, where we take CH = 1/2 and
replace ρ with c1ρ, and this concludes our proof.
21

A.2
Proof of Theorem 4.5
Proof. By Lemma 4.4, we know that in the line 4 of Algorithm 2, GN-AGD(·) outputs xk+1 with
the number of gradient evaluations
T G
k = eO
 f(xk) −f(xk+1)

L1/2ρ1/4
ϵ7/4
+ L1/2ρ−1/4
ϵ1/4

In addition, by Lemmas 4.1 and 4.2, it can be seen that the complexity of computing approximate
negative curvature is in the order of eO(
p
L/ϵH), and the number of calls to One-Step-Deterministic
function is upper bounded by eO
 min{ρ2∆f/ϵ3
H, Nϵ}

. Thus, let G denote the iteration set that
the outer loop that GN-AGD(·) is executed, and Gc denote the iteration set that performing
One-Step-Deterministic(·), the total runtime complexity of GOSE-Deterministic can be obtained by
performing the summation over the complexity in each outer loop, i.e.,
T = eO
P
k∈G

f(xk) −f(xk+1)

L1/2ρ1/4
ϵ7/4
+
X
k∈Gc
L1/2ρ−1/4
ϵ1/4
+ L1/2
ϵ1/2
H

Tg

= eO
P
k∈G

f(xk) −f(xk+1)

L1/2ρ1/4
ϵ7/4
+
L1/2ρ−1/4
ϵ1/4
+ L1/2
ϵ1/2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

= eO
∆fL1/2ρ1/4
ϵ7/4
+
L1/2ρ−1/4
ϵ1/4
+ L1/2
ϵ1/2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
where the third equality follows from the fact P
k∈G f(xk) −f(xk+1) ≤∆f, since One-Step-
Deterministic(·) is able to guarantee the non-increasing property in terms of the function value.
B
Proofs for Stochastic Nonconvex Optimization
B.1
Proof of Lemma 5.5
Proof. To begin with, we show that the negative curvature descent step will decrease the function
value. Based on the assumption that f(·) is ρ-Hessian Lipschitz, we can get
f(y) ≤f(x) + ⟨∇f(x), y −x⟩+ 1
2⟨y −x, ∇2f(x)(y −x)⟩+ ρ
6∥y −x∥3
2, ∀x, y ∈Rd.
Since λmin(∇2f(xt)) < −ϵH, we could get the negative curvature direction bv such that
bv⊤∇2f(x)bv ≤−ϵH
2 ,
∥bv∥2 = 1,
which leads to the update
y = x + η · sign(−bg⊤bv)bv = xt + η ev,
where bg = 1/|S| P
ξi∈S ∇F(x; ξi). Based on the above inequalities and update form, we have
f(y) ≤f(x) + η⟨∇f(x), ev⟩+ η2
2 ⟨ev, ∇2f(x)ev⟩+ ρη3
6 ∥ev∥3
2.
22

According to the deﬁnition of ev, we obtain
⟨bg, ev⟩= sign(−bg⊤bv)⟨bg, bv⟩≤0.
Then we are going to upper bound ⟨∇f(x), ev⟩, i.e., with probability at least 1 −δ′
⟨∇f(x), ev⟩= ⟨bg, ev⟩+ ⟨∇f(x) −bg, ev⟩
≤∥∇f(x) −bg∥2∥ev∥2
≤c ϵ,
where the last inequality follows from the concentration inequality in Lemma 5.4. Then it follows
that
f(y) ≤f(x) + η⟨∇f(x), ev⟩+ η
2⟨ev, ∇2f(x)ev⟩+ ρη3
6 ∥ev∥3
2
≤f(xt) + cηϵ −
ϵHη2
4
−ρη3
6

.
We ﬁrst set cηϵ ≤ρη3/6, which means that η ≥
p
6cϵ/ρ, then we could guarantee that
f(y) ≤f(x) −
ϵHη2
4
−ρη3
3

.
If we take η such that
r6cϵ
ρ ≤η < 3ϵH
4ρ ,
(B.1)
then the above inequality holds since c ≪1. If we take η = CH (ϵH/ρ), which means that
s
6cρϵ
ϵ2
H
≤CH ≤3
4,
then we can get
f(y) −f(x) ≤−
C2
H
4
−C3
H
3
ϵ3
H
ρ2 < 0,
which guarantees that there will be a suﬃcient decrease after performing negative curvature step
(4.2).
Secondly, we prove that the norm of the gradient increases after the negative curvature step, i.e.,
∥∇f(xt+1)∥2 > ϵ. According to the previous analysis in Lemma 4.3, we can get the lower bound of
∥∇f(xt+1) −∇f(xt)∥2 as
∥∇f(y) −∇f(x)∥2 ≥η|⟨ev, ∇2f(x)ev⟩| −1
2ρη2 ≥1
2ϵHη −1
2ρη2.
(B.2)
Then it can be derived that
∥∇f(y)∥2 ≥1
2ϵHη −1
2ρη2 −∥∇f(xt)∥2 ≥1
2ϵHη −1
2ρη2 −ϵ,
23

where the last inequality lies in the fact that ∥∇f(xt)∥2 ≤ϵ according to the subsampled gradient
∥bg∥2 ≤ϵ/2. In order to guarantee that ∥f(y)∥2 > ϵ, we need to set η satisfying
1
2ϵHη −1
2ρη2 −ϵ > ϵ.
In order to guarantee that there exists a solution to the above inequality, we further need to set
ϵ < ϵ2
H/16ρ. Then similar to the previous analysis in Lemma 4.3, we can get
1
2 −1
2
s
1 −16ρ ϵ
ϵ2
H
< CH < 1
2 + 1
2
s
1 −16ρ ϵ
ϵ2
H
.
(B.3)
Suppose c is suﬃciently small, then we could guarantee that if CH satisﬁes condition (B.1), it also
satisﬁes condition (B.3). Similar to Lemma 1, we could set the step size as η = ϵH/2c1ρ with c1 ≥1,
and then arrive at the the statements in Lemma 5.5.
B.2
Proof of Theorem 5.7
Proof. Let N = {k|∥gk∥2 ≤ϵ/2} and S = {k|∥gk∥2 > ϵ/2} be two sets, it can be seen that
N contains iterations performing One-Step-Online function and S consists of iterations running
one-epoch SCSG algorithm. According to Lemmas 5.6 and 4.3, we know that at the k-th outer loop
of Algorithm 4, it either follows that
1
CL(b/B)1/3 E
h
∥∇f(xk)∥2
2
i
≤E

f(xk−1) −f(xk)

+ 6 · 1{B < n}
CL(b/B)1/3B · H∗,
(B.4)
or
C′ϵ3
H
ρ2
≤E[f(xk−1) −f(xk)].
(B.5)
In fact, Lemma 4.3 only suggests that there exists eC′ such that eC′ϵ3
H/ρ2 ≤f(xk−1) −f(xk) holds
with probability 1 −δ. However, we are able to ensure that this inequality holds in expectation as
in (B.5). According to Assumptions 3.1 and 3.2 and suppose the step size η < 1 in Algorithm 3, we
know that f(xk−1) −f(xk) ≥−ρ −L. Then we have
E[f(xk−1) −f(xk)] ≥
eC′ϵ3
H
ρ2 (1 −δ) −(ρ + L)δ.
Considering suﬃciently small δ such that δ/(1 −δ) ≤eC′ϵ3
H/
 2ρ2(ρ + L)

, the inequality (B.5) holds
with C′ = eC′/2.
Combining (B.4) and (B.5), performing the summation on both sides over k, we have
1
CL(b/B)1/3
X
k∈S
E
h
∥∇f(xk)∥2
2
i
+
X
k∈N
C′ϵ3
H
ρ2
≤∆f +
X
k∈S
6H∗
CL(b/B)1/3B .
24

Let S = |S| and N = |N|, the above formula becomes
1
CL(b/B)1/3
X
k∈S
E
h
∥∇f(xk)∥2
2
i
+ NC′ϵ3
H
ρ2
≤∆f +
6SH∗
CL(b/B)1/3B .
(B.6)
We then consider S1 = {k ∈S|∥gk+1∥2 > ϵ/2}, and S2 = {k ∈S|∥gk+1∥2 ≤ϵ/2}, whose cardinalities
are S1 and S2, respectively. It should be noted that for each k ∈S2, we must have k + 1 ∈N, which
yields S2 ≤N + 1. Then (B.6) can be rewritten as
1
CL(b/B)1/3
X
k∈S1
E
h
∥∇f(xk)∥2
2
i
+
1
CL(b/B)1/3
X
k∈S2
E
h
∥∇f(xk)∥2
2
i
+ N
C′ϵ3
H
ρ2
−
6H∗
CL(b/B)1/3B

≤∆f + 6(S1 + 1)H∗
CL(b/B)1/3B .
Then we set the batch size B and mini-batch size b satisfy the following condition
B
 b
B
1/3
> 12H∗ρ2
CC′Lϵ3
H
,
(B.7)
then all terms on the left-hand side of the above inequality are positive, thus we have
NC′ϵ3
H
2ρ2
≤∆f + 6(S1 + 1)H∗
CL(b/B)1/3B ,
(B.8)
and
X
k∈S1
E
h
∥∇f(xk)∥2
2
i
≤CL(b/B)1/3∆f + 6(S1 + 1)H∗
B
.
According to Lemma 5.4 and setting c = 1/4, B = eO(1/ϵ2), we know that at the k-th outer loop of
Algorithm 4, for each k ∈S1, the inequality ∥∇f(xk)∥2 ≥∥gk∥2 −cϵ ≥ϵ/4 holds with probability
at least 1 −δ1. Similarly, for k ∈S2, the inequality ∥∇f(xk)∥2 ≤∥gk∥2 + cϵ ≤ϵ also holds with
probability at least 1 −δ1. Then by Markov inequality, we can derive that
X
k∈S1
∥∇f(xk)∥2
2 ≤2
X
k∈S1
E
h
∥∇f(xk)∥2
2
i
≤2CL(b/B)1/3∆f + 12(S1 + 1)H∗
B
(B.9)
holds with probability at least 1/2. Thus with probability at least 1/2, the following holds
S1ϵ2
4
≤2CL(b/B)1/3∆f + 12(S1 + 1)H∗
B
.
(B.10)
Assuming B ≥96H∗/ϵ2, we have
S1 ≤16CL(b/B)1/3∆f + 96H∗/B
ϵ2
.
25

In the following we are going to upper bound the number of iterations in N. Note that we have
already obtained the upper bound of S1, thus the following can be derived from (B.8),
NC′ϵ3
H
2ρ2
≤∆f + 96∆fH∗
Bϵ2
+
576H∗2
CL(b/B)1/3B2ϵ2 +
6H∗
CL(b/B)1/3B
≤2∆f +
12H∗
CL(b/B)1/3B ,
where the second inequality follows from the assumption B ≥96H∗/ϵ2. Then it can be seen that
N ≤4∆fρ2
C′ϵ3
H
+
24ρ2H∗
C′CL(b/B)1/3Bϵ3
H
.
Note that we require B = eO(1/ϵ2) and B ≥96H∗/ϵ2, which implies that B = eO(H∗/ϵ2). Together
with (B.7), we have
S1 = O
L∆f(b/B)1/3
ϵ2
+
1
Bϵ2

= O

L∆f
ϵ4/3H∗1/3 + ρ2∆f
ϵ3
H

,
N = O
ρ2∆f
ϵ3
H
+
ρ2
(b/B)1/3Bϵ3
H

= O
ρ2∆f
ϵ3
H

,
where the ﬁrst equality follows from the fact b1/3 = max{1, H∗ρ2ϵ3/4/(Lϵ3
H)} ≤1 + H∗ρ2ϵ3/4/(Lϵ3
H).
By Lemmas 5.2, we know that Algorithm 3 requires eO(L2/ϵ2
H) inner iterations. Moreover, since
our algorithm can guarantee escaping from saddle points in one step, thus the number of calls to this
function is also upper bounded by Nϵ, which yields N = eO
 min{ρ2∆f/ϵ3
H, Nϵ}

. In addition, since
Tk follows a geometric distribution with mean B/b, it can be seen that Tk ≤O(log δ2(B/b)) with
probability 1−δ2. Thus, the complexity of one-epoch SCSG is in the order of O(B log q) = eO(H∗/ϵ2).
Now, we can obtain the total runtime complexity of Algorithm 4,
T = (S1 + S2) · eO
H∗
ϵ2

Tg + N · eO
L2
ϵ2
H

Tg
= O

L∆f
ϵ4/3H∗1/3 + ρ2∆f
ϵ3
H

· eO
H∗
ϵ2

Tg + O

min
nρ2∆f
ϵ3
H
, Nϵ
o
· eO
H∗
ϵ2 + L2
ϵ2
H

Tg
= eO
L∆fH∗2/3
ϵ10/3
+ ρ2∆fH∗
ϵ3
Hϵ2
+
H∗
ϵ2 + L2
ϵ2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
where the second equality follows from the fact that S2 ≤N + 1. The last thing is to investigate
the success probability of Algorithm 4. Note that the one-epoch SCSG function succeeds with
probability (1 −δ1)(1 −δ2), and One-Step-Online succeeds with probability 1 −δ1. Together
with the probability introduced by Markov inequality in B.9, the total success probability is
(1 −δ1)S(1 −δ2)S(1 −δ1)N/2. Note that the probability δ1 and δ2 only exist in the logarithmic
terms, which can be hided in the notation eO(·). Thus, we consider suﬃciently small δ1 and δ2 such
that (1 −δ1)S(1 −δ2)S(1 −δ1)N/2 ≤1/3, which completes the proof.
26

C
Proofs for Finite-Sum Nonconvex Optimization
C.1
Proof of Theorem 6.5
Proof. Let N = {k|∥gk∥2 ≤ϵ} and S = {k|∥gk∥2 > ϵ} be two sets for iterations performing
One-Step-FiniteSum functions and one-epoch SCSG algorithms, respectively. According to Lemmas
6.4 and 4.3, we know that at the k-th outer loop of Algorithm 6, it either follows that
1
CL(b/B)1/3 E
h
∥∇f(xk)∥2
2
i
≤E

f(xk−1) −f(xk)

,
or
C′ϵ3
H
ρ2
≤E[f(xk−1) −f(xk)].
Combining these two cases and perform the summation on both sides over k, we have
1
CL(b/B)1/3
X
k∈S
E
h
∥∇f(xk)∥2
2
i
+ NC′ϵ3
H
ρ2
≤∆f.
(C.1)
We then consider S1 = {k ∈S|∥gk+1∥2 > ϵ}, and S2 = {k ∈S|∥gk+1∥2 ≤ϵ}, whose cardinalities
are S1 and S2, respectively. Similar to the proof for Theorem 5.7, we have S2 ≤N + 1. Then the
following two inequalities hold,
NC′ϵ3
H
2ρ2
≤∆f,
and
X
k∈S1
E
h
∥∇f(xk)∥2
2
i
≤CL(b/B)1/3∆f.
(C.2)
By Markov inequality, we have
X
k∈S1
∥∇f(xk)∥2
2 ≤2
X
k∈S1
E
h
∥∇f(xk)∥2
2
i
≤2CL(b/B)1/3∆f
holds with probability at least 1/2. Thus by setting B = n and b = 1, we can derive that
S1 ≤2CL(b/B)1/3∆f
ϵ2
= O
 L∆f
ϵ2n1/3

.
From (B.8), the upper bound of N can be directly shown as
N ≤2ρ2∆f
C′ϵ3
H
= O
ρ2∆f
ϵ3
H

.
By Lemmas 6.1 and 6.2, we know that One-Step-FiniteSum requires eO(n + n3/4p
L/ϵH) inner
iterations and N = eO
 min{ρ2∆f/ϵ3
H, Nϵ}

.
In addition, the one-epoch SCSG epoch has the
computational complexity eO(B) = eO(n). Hence, the runtime complexity of Algorithm 6 can be
27

obtained as follows,
T = (S1 + S2) · eO(n)Tg + N · eO

n + n3/4L1/2
ϵ1/2
H

Tg
= O
 L∆f
ϵ2n1/3 + N

· eO(n)Tg + N · eO

n + n3/4L1/2
ϵ1/2
H

Tg
= eO
L∆fn2/3
ϵ2
+

n + n3/4L1/2
ϵ1/2
H

min
nρ2∆f
ϵ3
H
, Nϵ
o
Tg

,
where the second equality follows from the fact that S2 ≤N + 1. Then we are going to analyze
the success probability of Algorithm 6. Note that the one-epoch SVRG function succeeds with
probability 1 −δ2, and One-Step-FiniteSum succeeds with probability 1 −δ1. Together with the
probability introduced by Markov inequality, the total success probability is (1 −δ2)S(1 −δ1)N/2.
Similarly, considering suﬃciently small δ1 and δ2 such that (1 −δ2)S(1 −δ1)N/2 ≤1/3, then we are
able to complete the proof.
References
Agarwal, N., Allen-Zhu, Z., Bullins, B., Hazan, E. and Ma, T. (2016). Finding approximate
local minima for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146 .
Allen-Zhu, Z. (2017). Natasha 2: Faster non-convex optimization than sgd. arXiv preprint
arXiv:1708.08694 .
Allen-Zhu, Z. and Hazan, E. (2016). Variance reduction for faster non-convex optimization. In
International Conference on Machine Learning.
Allen-Zhu, Z. and Li, Y. (2017a). Follow the compressed leader: Faster algorithms for matrix
multiplicative weight updates. arXiv preprint arXiv:1701.01722 .
Allen-Zhu, Z. and Li, Y. (2017b). Neon2: Finding local minima via ﬁrst-order oracles. arXiv
preprint arXiv:1711.06673 .
Bhojanapalli, S., Neyshabur, B. and Srebro, N. (2016). Global optimality of local search for
low rank matrix recovery. In Advances in Neural Information Processing Systems.
Carmon, Y. and Duchi, J. C. (2016). Gradient descent eﬃciently ﬁnds the cubic-regularized
non-convex newton step. arXiv preprint arXiv:1612.00547 .
Carmon, Y., Duchi, J. C., Hinder, O. and Sidford, A. (2016). Accelerated methods for
non-convex optimization. arXiv preprint arXiv:1611.00756 .
Carmon, Y., Hinder, O., Duchi, J. C. and Sidford, A. (2017).
” convex until proven
guilty”: Dimension-free acceleration of gradient descent on non-convex functions. arXiv preprint
arXiv:1705.02766 .
28

Cartis, C., Gould, N. I. and Toint, P. L. (2012). Complexity bounds for second-order optimality
in unconstrained optimization. Journal of Complexity 28 93–108.
Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B. and LeCun, Y. (2015). The loss
surfaces of multilayer networks. In Artiﬁcial Intelligence and Statistics.
Curtis, F. E., Robinson, D. P. and Samadi, M. (2017). A trust region algorithm with a
worst-case iteration complexity of\mathcal {O}(\epsilonˆ{-3/2}) for nonconvex optimization.
Mathematical Programming 162 1–32.
Dauphin, Y. N., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S. and Bengio, Y. (2014).
Identifying and attacking the saddle point problem in high-dimensional non-convex optimization.
In Advances in neural information processing systems.
Du, S. S., Jin, C., Lee, J. D., Jordan, M. I., Poczos, B. and Singh, A. (2017). Gradient
descent can take exponential time to escape saddle points. arXiv preprint arXiv:1705.10412 .
Garber, D., Hazan, E., Jin, C., Musco, C., Netrapalli, P., Sidford, A. et al. (2016).
Faster eigenvector computation via shift-and-invert preconditioning. In International Conference
on Machine Learning.
Ge, R., Huang, F., Jin, C. and Yuan, Y. (2015). Escaping from saddle pointsonline stochastic
gradient for tensor decomposition. In Conference on Learning Theory.
Ge, R., Lee, J. D. and Ma, T. (2016). Matrix completion has no spurious local minimum. In
Advances in Neural Information Processing Systems.
Ghadimi, S., Lan, G. and Zhang, H. (2016). Mini-batch stochastic approximation methods for
nonconvex stochastic composite optimization. Mathematical Programming 155 267–305.
Goldfarb, D. (1980). Curvilinear path steplength algorithms for minimization which use directions
of negative curvature. Mathematical programming 18 31–40.
Hillar, C. J. and Lim, L.-H. (2013). Most tensor problems are np-hard. Journal of the ACM
(JACM) 60 45.
Jin, C., Ge, R., Netrapalli, P., Kakade, S. M. and Jordan, M. I. (2017a). How to escape
saddle points eﬃciently. arXiv preprint arXiv:1703.00887 .
Jin, C., Netrapalli, P. and Jordan, M. I. (2017b). Accelerated gradient descent escapes saddle
points faster than gradient descent. arXiv preprint arXiv:1711.10456 .
Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive
variance reduction. In Advances in neural information processing systems.
Kohler, J. M. and Lucchi, A. (2017). Sub-sampled cubic regularization for non-convex optimiza-
tion. arXiv preprint arXiv:1705.05933 .
29

Kuczy´nski, J. and Wo´zniakowski, H. (1992). Estimating the largest eigenvalue by the power
and lanczos algorithms with a random start. SIAM journal on matrix analysis and applications
13 1094–1122.
LeCun, Y., Bengio, Y. and Hinton, G. (2015). Deep learning. Nature 521 436–444.
Lei, L., Ju, C., Chen, J. and Jordan, M. I. (2017). Non-convex ﬁnite-sum optimization via scsg
methods. In Advances in Neural Information Processing Systems.
Levy, K. Y. (2016). The power of normalization: Faster evasion of saddle points. arXiv preprint
arXiv:1611.04831 .
Mart´ınez, J. M. and Raydan, M. (2017). Cubic-regularization counterpart of a variable-norm
trust-region method for unconstrained minimization. Journal of Global Optimization 68 367–385.
McCormick, G. P. (1977). A modiﬁcation of armijo’s step-size rule for negative curvature.
Mathematical Programming 13 111–115.
Mor´e, J. J. and Sorensen, D. C. (1979). On the use of directions of negative curvature in a
modiﬁed newton method. Mathematical Programming 16 1–20.
Nesterov, Y. (1998). Introductory lectures on convex programming volume i: Basic course .
Nesterov, Y. and Polyak, B. T. (2006). Cubic regularization of newton method and its global
performance. Mathematical Programming 108 177–205.
Park, D., Kyrillidis, A., Caramanis, C. and Sanghavi, S. (2016). Non-square matrix sensing
without spurious local minima via the burer-monteiro approach. arXiv preprint arXiv:1609.03240
.
Reddi, S. J., Hefny, A., Sra, S., Poczos, B. and Smola, A. (2016). Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning.
Reddi, S. J., Zaheer, M., Sra, S., Poczos, B., Bach, F., Salakhutdinov, R. and Smola,
A. J. (2017). A generic approach for escaping saddle points. arXiv preprint arXiv:1709.01434 .
Royer, C. W. and Wright, S. J. (2017).
Complexity analysis of second-order line-search
algorithms for smooth nonconvex optimization. arXiv preprint arXiv:1706.03131 .
Simchowitz, M., Alaoui, A. E. and Recht, B. (2017). On the gap between strict-saddles and
true convexity: An omega (log d) lower bound for eigenvector approximation. arXiv preprint
arXiv:1704.04548 .
Sun, J., Qu, Q. and Wright, J. (2016). A geometric analysis of phase retrieval. In Information
Theory (ISIT), 2016 IEEE International Symposium on. IEEE.
Tripuraneni, N., Stern, M., Jin, C., Regier, J. and Jordan, M. I. (2017). Stochastic cubic
regularization for fast nonconvex optimization. arXiv preprint arXiv:1711.02838 .
30

Vershynin, R. (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027 .
Xu, P., Roosta-Khorasani, F. and Mahoney, M. W. (2017). Newton-type methods for
non-convex optimization under inexact hessian information. arXiv preprint arXiv:1708.07164 .
Xu, Y. and Yang, T. (2017). First-order stochastic algorithms for escaping from saddle points in
almost linear time. arXiv preprint arXiv:1711.01944 .
31

