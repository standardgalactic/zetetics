Program of Thoughts Prompting: Disentangling Computation from
Reasoning for Numerical Reasoning Tasks
♠,♣Wenhu Chen∗, ♠Xueguang Ma∗, ♦Xinyi Wang, ♥William W. Cohen
♠University of Waterloo
♣Vector Institute, Toronto
♦University of California, Santa Barabra
♥Google Research
{wenhuchen,x93ma}@uwaterloo.ca, xinyi_wang@ucsb.edu, wcohen@google.com
Abstract
Recently, there has been signiﬁcant progress
in teaching language models to perform step-
by-step reasoning to solve complex numerical
reasoning tasks. Chain-of-thoughts prompting
(CoT) is the state-of-art method for many of
these tasks.
CoT uses language models to
produce text describing reasoning, and com-
putation, and ﬁnally the answer to a ques-
tion. Here we propose ‘Program of Thoughts’
(PoT), which uses language models (mainly
Codex) to generate text and programming lan-
guage statements, and ﬁnally an answer. In
PoT, the computation can be delegated to a pro-
gram interpreter, which is used to execute the
generated program, thus decoupling complex
computation from reasoning and language un-
derstanding. We evaluate PoT on ﬁve math
word problem datasets and three ﬁnancial-QA
datasets in both few-shot and zero-shot set-
tings. We ﬁnd that PoT has an average perfor-
mance gain over CoT of around 12% across
all datasets.
By combining PoT with self-
consistency decoding, we can achieve SoTA
performance on all the math datasets and near-
SoTA performance on the ﬁnancial datasets.
All of our data and code are released in
Github1.
1
Introduction
Numerical reasoning is a long-standing task in arti-
ﬁcial intelligence. A surge of datasets has been pro-
posed recently to benchmark deep-learning mod-
els’ capabilities to perform numerical/arithmetic
reasoning.
Some widely used benchmarks are
based on Math word problems (MWP) (Cobbe
et al., 2021; Patel et al., 2021; Lu et al., 2022; Ling
et al., 2017), where systems are supposed to an-
swer math questions expressed with natural text.
∗Work done at University of Waterloo. Wenhu Chen is
responsible for paper writing and running Math QA experi-
ments, and Xueguang Ma is responsible for running ﬁnancial
QA experiments and paper polishing.
1https://github.com/wenhuchen/
Program-of-Thoughts
Besides MWP, some datasets also consider ﬁnan-
cial problems (Chen et al., 2021b, 2022; Zhu et al.,
2021), where systems need to answer math-driven
ﬁnancial questions.
Prior work (Ling et al., 2017; Cobbe et al., 2021)
has studied how to train models from scratch or
ﬁne-tune models to generate intermediate steps to
derive the ﬁnal answer. Such methods are data-
intensive, requiring a signiﬁcant number of train-
ing examples with expert-annotated steps.
Re-
cently, Wei et al. (2022) have discovered that the
large language models (LLMs) (Brown et al., 2020;
Chen et al., 2021a; Chowdhery et al., 2022) can
be prompted with a few input-output exemplars to
solve these tasks without any training or ﬁne-tuning.
In particular, when prompted with a few examples
containing inputs, natural language ‘rationales’,
and outputs, LLMs can imitate the demonstrations
to both generate rationales and answer these ques-
tions. Such a prompting method is dubbed ‘Chain
of Thoughts (CoT)’, and it is able to achieve state-
of-the-art performance on a wide spectrum of textu
and numerical reasoning datasets.
CoT uses LLMs for both reasoning and com-
putation, i.e. the language model not only needs
to generate the mathematical expressions but also
needs to perform the math in each step. We ar-
gue that language models are not ideal for actually
solving these mathematical expressions, because:
1) LLMs are very prone to arithmetic calculation
errors, especially when dealing with large num-
bers; 2) LLMs cannot solve complex mathematical
expressions like polynomial equations or even dif-
ferential equations; 3) LLMs are highly inefﬁcient
at expressing iteration, especially when the number
of iteration steps is large.
In order to solve these issues, we propose
program-of-thoughts (PoT) prompting, which will
delegate computation steps to an external language
interpreter. In PoT, LMs can express reasoning
steps as Python programs, and the computation can
arXiv:2211.12588v3  [cs.CL]  29 Nov 2022

Figure 1: Comparison between Chain of Thoughts and Program of Thoughts.
be accomplished by a Python interpreter. We depict
the difference between CoT and PoT in Figure 1.
In the upper example, for CoT the iteration runs
for 50 times, which leads to extremely low accu-
racy;2 in the lower example, CoT cannot solve the
cubic equation with language models and outputs
a wrong answer. In contrast, in the upper example,
PoT can express the iteration process with a few
lines of code, which can be executed on a Python
interpreter to derive an accurate answer; and in the
lower example, PoT can convert the problem into a
program that relies on ‘SymPy’ library in Python
to solve the complex equation.
We evaluate PoT prompting across ﬁve MWP
datasets, GSM8K, AQuA, SVAMP, TabMWP, Mul-
tiArith; and three ﬁnancial datasets, FinQA, Con-
vFinQA, and TATQA. These datasets cover various
input formats including text, tables, and conversa-
tion. We give an overview of the results in Figure 2.
Under both few-shot and zero-shot settings, PoT
outperforms CoT signiﬁcantly across all the eval-
uated datasets. Under the few-shot setting, the
average gain over CoT is around 8% for the MWP
datasets and 15% for the ﬁnancial datasets. Under
the zero-shot setting, the average gain over CoT
is around 12% for the MWP datasets. PoT com-
bined with self-consistency (SC) also outperforms
2Assuming each addition is correct with 90% chance, after
50 additions, the likelihood of a correct output is less than 1%.
CoT+SC (Wang et al., 2022b) by an average of 10%
across all datasets. Our PoT+SC achieves the best-
known results on all the evaluated MWP datasets
and near SoTA results on the ﬁnancial datasets. Fi-
nally, we conduct comprehensive ablation studies
to understand the different components of PoT.
2
Program of Thoughts
2.1
Preliminaries
In-context learning has been described in (Brown
et al., 2020; Chen et al., 2021a; Chowdhery et al.,
2022; Rae et al., 2021). Compared with ﬁne-tuning,
in-context learning (1) only takes a few annotation-
s/demonstrations as a prompt, and (2) performs
inference without training the model parameters.
With in-context learning, LLMs receive the input-
output exemplars as the preﬁx, followed by an input
problem, and generate outputs imitating the exem-
plars. More recently, ‘chain of thoughts prompt-
ing’ (Wei et al., 2022) has been proposed as a spe-
ciﬁc type of in-context learning where the exem-
plar’s output contains the ‘thought process’ or ra-
tionale instead of just an output. This approach has
been shown to elicit LLMs’ reasoning capabilities.
2.2
Program of Thoughts
Besides natural language, programs can also be
used to express our thought processes. By using se-
mantically meaningful variable names, a program

GSM8K
AQuA
SVAMP
TabMWP
FinQA
ConvFin
TATQA
63.1
45.3
76.4
65.2
40.4
45.5
61.4
71.6
54.1
85.2
73.2
64.5
64.6
69
CoT
PoT
GSM8K
AQuA
SVAMP
TabMWP
FinQA
ConvFin
TATQA
78
52
86.8
75.4
44.4
47.9
63.9
80
58.6
89.1
81.8
68.1
67.3
73.4
CoT-SC
PoT-SC
GSM8K
AQuA
SVAMP
TabMWP MultiArith
40.5
31.9
63.7
53.5
79.3
57
43.9
70.8
65.2
92.2
ZS-CoT
ZS-PoT
Figure 2: Few-shot (upper), Few-shot + SC (middle)
and Zero-Shot (lower) Performance overview of Codex
PoT and Codex CoT across different datasets.
Figure 3: Prompting LLMs to generate outputs with
few-shot exemplars.
can also be a natural representation to convey hu-
man thoughts. For example, in the lower example
in Figure 1, we ﬁrst create an unknown variable
named interest_rate. Then we bind ‘summation
in two years with ... interest rate’ to the variable
sum_in_two_years_with_XXX_interest
and
write down the equation expressing their math-
ematical relations with interest_rate. These
equations are packaged into the ‘solve’ function
provided by ‘SymPy’. The program is executed
with Python to solve the equations to derive the
variable interest_rate, which is the answer to
the problem.
Unlike CoT, PoT relegates some computation
to an external process (a Python interpreter). The
LLMs are only responsible for expressing the ‘rea-
soning process’ in the programming language. In
contrast, CoT aims to use LLMs to perform both
reasoning and computation. We argue that such an
approach is more expressive and accurate.
The ‘program of thoughts’ is different from gen-
erating equations directly, where the generation
target would be solve(20000 ∗(1 + x)3 −2000 −
x∗20000∗3−1000, x). As observed by (Wei et al.,
2022) for CoT, directly generating such equations
is challenging for LLMs. PoT differs from equation
generation in two aspects: (1) PoT breaks down
the equation into a multi-step ‘thought’ process,
and (2) PoT binds semantic meanings to variables
to help ground the model in language. We found
that this sort of ‘thoughtful’ process can elicit lan-
guage models’ reasoning capabilities and generate
more accurate programs. We provide a detailed
comparison in the experimental section.
2.3
PoT Prompting
We show the proposed PoT prompting method
in Figure 4 under the few-shot and zero-shot set-
tings. Under the few-shot setting, a few exemplars
of (question, ‘program of thoughts’) pairs will be
preﬁxed as demonstrations to teach the LLM how
to generate ‘thoughtful’ programs. Under the zero-
shot setting, the prompt only contains an instruction
without any exemplar demonstration. Unlike zero-
shot CoT (Kojima et al., 2022), which requires an
extra step to extract the answer from the ‘chain
of thoughts’, zero-shot PoT can return the answer
straightforwardly without extra steps.
2.4
PoT as an Intermediate Step
For certain problems requiring additional common-
sense reasoning, we need to ﬁrst utilize PoT to gen-
erate a program to compute an intermediate result,
which is combined with the question to continue
prompting LLM to derive the ﬁnal answer. For
instance, in the left example in Figure 4, the pro-
gram will generate a ﬂoat number 2.05. However,
adding 2.05 to 11 AM cannot be easily handled
by a Python program. Therefore, we continue to
prompt the LLM to perform an additional step of
textual reasoning to derive the ﬁnal answer. We
adopt this approach for the AQuA dataset.

Figure 4: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.
3
Experiments
3.1
Experimental Setup
Datasets
We summarize our evaluated datasets
in Table 1. We use the test set for all the evaluated
datasets except TATQA. These datasets are highly
heterogeneous in terms of their input formats. We
conduct comprehensive experiments on this broad
spectrum of datasets to show the generalizability
and applicability of PoT prompting.
To incorporate the diverse inputs, we propose
to linearize these inputs in the prompt. For table
inputs, we adopt the same strategy as Chen (2022)
to linearize a table into a text string. The columns
of the table are separated by ‘|’ and the rows are
separated by ‘\n’. If a table cell is empty, it is ﬁlled
by ’-’. For text+table hybrid inputs, we separate
tables and text with ‘\n’. For conversational his-
tory, we also separate conversation turns by ‘\n’.
The prompt is constructed by the concatenation of
task instruction, text, linearized table, and question.
For conversational question answering, we simply
concatenate all the dialog history in the prompt.
Implementation Details
We use the OpenAI
Codex (code-davinci-002) API3 and GPT-3 (text-
davinci-002) API4 model in our experiments. We
use Python 3.8 with the SymPy library5 to execute
the generated program. For the few-shot setting,
we use 4-8 shots for all the datasets, based on their
difﬁculty. For simple datasets like FinQA (Chen
et al., 2021b), we tend to use less shots, while for
3https://openai.com/blog/openai-codex/
4https://beta.openai.com/
5https://www.sympy.org/en/index.html
more challenging datasets like AQuA (Ling et al.,
2017) and TATQA (Zhu et al., 2021), we use 8
shots to cover more diverse problems. The exam-
ples are taken from the training set. We generally
write prompts for 10-20 examples, and then tune
the exemplar selection on a small validation set to
choose the best 4-8 shots for the full set evaluation.
To elicit the LLM’s capability to perform multi-
step reasoning, we use the text “Let’s write a
Python program step by step" as our prompt. How-
ever, a caveat is that LLM can fall back to gener-
ating a reasoning chain in comments rather than
in program. Therefore, we suppress the ‘#’ token
logits by -2 to decrease its probability to avoid
such cases. We found that this simple strategy can
greatly improve performance.
Metrics
We adopt exact match scores as our eval-
uation metrics for GSM8K, SVAMP, and Multi-
Arith datasets. We will round the predicted number
to a speciﬁc precision and then compare it with
the reference number. For the AQuA dataset, we
use PoT to compute the intermediate answer and
then prompt the LLM again to output the closest
option to measure the accuracy. For TabMWP, Con-
vFinQA, and TATQA datasets, we use the ofﬁcial
evaluation scripts provided on Github. For FinQA,
we relax the evaluation for CoT because LLMs can-
not perform the computation precisely (especially
with high-precision ﬂoats and large numbers), so
we adopt ‘math.isclose’ with relative tolerance of
0.001 to compare answers.
Baselines
We report results for three different
models including Codex (Chen et al., 2021a), GPT-

Dataset
Split
Example
Domain
Input
Output
GSM8K (Cobbe et al., 2021)
Test
1318
MWP
Question
Number
AQuA (Ling et al., 2017)
Test
253
MWP
Question
Option
SVAMP (Patel et al., 2021)
Test
1000
MWP
Question
Number
MultiArith (Roy and Roth, 2015)
Test
600
MWP
Question
Number
TabMWP (Lu et al., 2022)
Test
7861
MWP
Table + Question
Number + Text
FinQA (Chen et al., 2021b)
Test
1147
Finance
Table + Text + Question
Number + Binary
ConvFinQA (Chen et al., 2022)
Test
421
Finance
Table + Text + Multi-Turn Question
Number + Binary
TATQA (Zhu et al., 2021)
Dev
1668
Finance
Table + Text + Question
Number + Text
Table 1: Summarization of all the datasets being evaluated.
3 (Brown et al., 2020), PaLM (Chowdhery et al.,
2022) and LaMDA (Thoppilan et al., 2022). We
consider two types of prediction strategies includ-
ing direct answer output and chain of thought to
derive the answer. Since PaLM API is not public,
we only list PaLM results reported from previous
work (Wei et al., 2022; Wang et al., 2022b). We
also leverage an external calculator as suggested
in Wei et al. (2022) for all the equations generated
by CoT, which is denoted as CoT + calc. Besides
greedy decoding, we use self-consistency (Wang
et al., 2022b) with CoT, taking majority vote over
40 different completions as the prediction.
3.2
Main Results
Few-shot Results
We give our few-shot results
in Table 2. On MWP datasets, PoT with greedy
decoding improves on GSM8K/AQuA/TabMWP
by more than 8%. On SVAMP, the improvement is
4% mainly due to its simplicity. For ﬁnancial QA
datasets, PoT improves over CoT by roughly 20%
on FinQA/ConvFinQA and 8% on TATQA. The
larger improvements in FinQA and ConvFinQA are
mainly due to miscalculations on LLMs for large
numbers (e.g. in the millions). CoT adopts LLMs
to perform the computation, which is highly prone
to miscalculation errors, while PoT adopts a highly
precise external computer to solve the problem.
As an ablation, we also compare with CoT+calc,
which leverages an external calculator to correct
the calculation results in the generated ‘chain of
thoughts’. The experiments show that adding an
external calculator only shows mild improvement
over CoT on MWP datasets, much behind PoT.
Few-shot + Self-Consistency Results
We lever-
age self-consistency (SC) decoding to understand
the upper bound of our method. This sampling-
based decoding algorithm can greatly reduce ran-
domness in the generation procedure and boosts
performance. Speciﬁcally, we set a temperature of
0.4 and K=40 throughout our experiments. Accord-
ing to to Table 2, we found that PoT + SC still out-
performs CoT + SC on MWP datasets with notable
margins. On ﬁnancial datasets, we observe that
self-consistency decoding is less impactful for both
PoT and CoT. Similarly, PoT + SC outperforms
CoT + SC by roughly 20% on FinQA/ConvFinQA,
and 7% on TATQA.
Zero-shot Results
We also evaluate the zero-
shot performance of PoT and compare with Ko-
jima et al. (2022) in Table 3. As can be seen, zero-
shot PoT signiﬁcantly outperforms zero-shot CoT
across all the MWP datasets evaluated. Compared
to few-shot prompting, zero-shot PoT outperforms
zero-shot CoT (Kojima et al., 2022) by an even
larger margin. On the evaluated datasets, PoT’s out-
performs CoT by an average of 12%. On TabMWP,
zero-shot PoT is even higher than few-shot CoT.
These results show the great potential to directly
generalize to many unseen numerical tasks even
without any dataset-speciﬁc exemplars.
3.3
Ablation Studies
We performed multiple ablation studies under the
few-shot setting to understand the importance of
different factors in PoT.
Backend GPT-3 vs. Codex
We evaluated GPT-
3 (text-davinci-002) with PoT prompting.
Un-
like Codex, GPT-3 is not optimized for generating
programs, and one would expect degraded perfor-
mance with GPT-3 as the LLM. We choose three
datasets, GSM8K, SVAMP, and FinQA, to analyze
the performance difference of PoT and compare
that relative to CoT. We show our experimental
results in Table 4. We can see that the gap be-
tween Codex and GPT-3 with PoT is consistently
smaller than their gap with CoT. We conclude that
our prompting approach is still effective for mod-
els that are not speciﬁcally optimized for program
generation. However, we do observe that the gap

Model
#Params
GSM8K
AQuA
SVAMP
TabWMP
FinQA
ConvFinQA
TATQA
Avg
Fine-tuned or few-shot prompt
Published SoTA
-
78.0
52.0
86.8
68.2
68.0
68.9
73.6
70.7
Few-shot prompt (Greedy Decoding)
Codex Direct
175B
19.7
29.5
69.9
59.4
25.6
40.0
55.0
42.7
Codex CoT
175B
63.1
45.3
76.4
65.2
40.4
45.6
61.4
56.7
GPT-3 Direct
175B
15.6
24.8
65.7
57.1
14.4
29.1
37.9
34.9
GPT-3 CoT
175B
46.9
35.8
68.9
62.9
26.1
37.4
42.5
45.7
PaLM Direct
540B
17.9
25.2
69.4
-
-
-
-
-
PaLM CoT
540B
56.9
35.8
79.0
-
-
-
-
-
Codex CoTcalc
175B
65.4
45.3
77.0
65.8
-
-
-
-
GPT-3 CoTcalc
175B
49.6
35.8
70.3
63.4
-
-
-
-
PaLM CoTcalc
540B
58.6
35.8
79.8
-
-
-
-
-
PoT (Ours)
175B
71.6
54.1
85.2
73.2
64.5
64.6
69.0
68.9
Few-shot prompt (Self-Consistency Decoding)
LaMDA CoT-SC
137B
27.7
26.8
53.5
-
-
-
-
-
Codex CoT-SC
175B
78.0
52.0
86.8
75.4
44.4
47.9
63.2
63.9
PaLM CoT-SC
540B
74.4
48.3
86.6
-
-
-
-
-
PoT-SC (Ours)
175B
80.0
58.6
89.1
81.8
68.1
67.3
70.2
73.6
Table 2: The few-shot results for different datasets. Published SoTA includes the best-known results. On GSM8K,
AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding (Wang et al., 2022b). On FinQA,
the prior best result is from Wang et al. (2022a).
On ConvFinQA, the prior best result is achieved by Fin-
QANet (Chen et al., 2022). On TabWMP (Lu et al., 2022), the prior best result is achieved by Dynamic Prompt
Learning (Lu et al., 2022). On TATQA, the SoTA result is achieved by RegHNT (Lei et al., 2022).
Model
#Params
GSM8K
AQuA
SVAMP
TabMWP
MultiArith
Avg
Zero-shot Direct (GPT-3)
150B
12.6
22.4
58.7
38.9
22.7
31.0
Zero-shot CoT (GPT-3)
150B
40.5
31.9
63.7
53.5
79.3
53.7
Zero-shot CoT (PaLM)
540B
43.0
-
-
-
66.1
-
Zero-shot PoT (Ours)
150B
57.0
43.9
70.8
66.5
92.2
66.1
Table 3: The zero-shot results for different datasets. The baseline results are taken from Kojima et al. (2022).
Model
GSM8K
SVAMP
FinQA
Codex CoT
63.1
76.4
40.4
GPT3 CoT
46.9
58.9
26.1
Codex - GPT3 (CoT)
+16.2
+7.5
+14.3
Codex PoT
71.6
85.2
64.5
GPT3 PoT
60.4
80.1
56.7
Codex - GPT3 (PoT)
+11.2
+5.1
+7.8
Table 4: GPT-3 and Codex performance difference un-
der CoT and PoT prompting.
increases as the dataset becomes more challenging.
Sensitivity to Exemplars
To better understand
how sensitive PoT is w.r.t different exemplars, we
conduct a sensitivity analysis. Speciﬁcally, we
wrote 20 total exemplars. For k-shot learning, we
randomly sample k = (2, 4, 6, 8) out of the 20 ex-
emplars three times as v1, v2, and v3. We will use
these randomly sampled exemplars as demonstra-
2-shots
4-shots
6-shots
8-shots
0.62
0.65
0.67
0.73
0.58
0.68
0.69
0.7
0.55
0.6
0.65
0.74
v1
v2
v2
2-shots
4-shots
6-shots
8-shots
0.63
0.66
0.66
0.62
0.58
0.63
0.62
0.64
0.64
0.62
0.64
0.62
Figure 5: Exemplar sensitivity analysis for GSM8K
and FinQA, where v1, v2 and v3 are three versions of
k-shot demonstration sampled from the pool.

Method
GSM8K
SVAMP
FinQA
PoT
71.6
85.2
64.5
PoT - Binding
60.2
83.8
61.6
PoT - MultiStep
45.8
81.9
58.9
Table 5: Comparison between PoT and equation gener-
ation on three different datasets.
tions for PoT. We summarize our sensitivity analy-
sis in Figure 5. First of all, we found that increasing
the number of shots helps more for GSM8K than
FinQA. This is mainly due to the diversity of ques-
tions in GSM8K. By adding more exemplars, the
language models can better generalize to diverse
questions. Another observation is that when given
fewer exemplars, PoT’s performance variance is
larger. When K=2, the performance variance can
be as large as 7% for both datasets. With more
exemplars, the performance becomes more stable.
Semantic Binding and Multi-Step Reasoning
The two core properties of ‘program of thoughts’
are: (1) multiple steps: breaking down the thought
process into the step-by-step program, (2) semantic
binding: associating semantic meaning to the vari-
able names. To better understand how these two
properties contribute, we compared with two vari-
ants. One variant is to remove the semantic binding
and simply use a, b, c as the variable names. The
other variant is to directly predict the ﬁnal mathe-
matical equation to compute the results. We show
our ﬁndings in Table 5. As can be seen, removing
the binding will in general hurt the model’s per-
formance. On more complex questions involving
more variables like GSM8K, the performance drop
is larger. Similarly, prompting LLMs to directly
generate the target equations is also very challeng-
ing. Breaking down the target equation into mul-
tiple reasoning steps helps boost the performance.
Breakdown Analysis
We perform further anal-
ysis to determine which kinds of problems CoT
and PoT differ most in performance.
We use
AQuA (Ling et al., 2017) as our testbed for this.
Speciﬁcally, we manually classify the questions in
AQuA into several categories and show the accu-
racy on each subcategory in Figure 6. The major
categories are (1) linear equations, (2) numerical
calculation, (3) combinatorics, (4) probability, and
(5) iterative.
The largest improvements of PoT are in the cat-
egories ‘linear/polynomial equation’, ‘iterative’,
geometric
polynomial
symbolic
numerical
combinatorics
linear equation
iterative
probability
10
50
66
86
50
72
65
38
20
20
56
88
40
62
40
40
PoT
CoT
Figure 6: PoT and CoT’s breakdown accuracy across
different types of questions.
‘symbolic’, and ‘combinatorics’. These questions
require more complex arithmetic or symbolic skills
to solve. In contrast, on ‘numerical’, ‘probability’,
and ‘geometric’ questions, PoT and CoT perform
similarly.
Error Analysis
We considered two types of er-
rors: (1) value grounding error, and (2) logic gener-
ation error. The ﬁrst type indicates that the model
fails to assign correct values to the variables rel-
evant to the question. The second type indicates
that the model fails to generate the correct com-
putation process to answer the question based on
the deﬁned variables. Figure 7 shows an exam-
ple of each type of error. In the upper example,
the model fetches the value of the variables incor-
rectly while the computation logic is correct. In
the lower example, the model grounded relevant
variables correctly but fails to generate proper com-
putation logic to answer the question. We manually
examined the errors made in the TAT-QA results.
Among the 198 failure cases of numerical reason-
ing question with the PoT (greedy) method, 47%
have value grounding errors and 33% have logic
errors. In 15% both types of errors occurred and
in 5% we believe the answer is actually correct.
We found that the majority of the errors are value
grounding errors, which is also common for other
methods such as CoT.
4
Related Work
4.1
Mathematical Reasoning in NLP
Mathematical reasoning skills are essential for
general-purpose intelligent systems, which have
attracted a signiﬁcant amount of attention from the
community. Earlier, there have been studies in un-
derstanding NLP models’ capabilities to solve arith-
metic/algebraic questions (Hosseini et al., 2014;

Figure 7: Error cases on TAT-QA dev set using PoT-greedy method.
Koncel-Kedziorski et al., 2015; Roy and Roth,
2015; Ling et al., 2017; Roy and Roth, 2018). Re-
cently, more challenging datasets (Dua et al., 2019;
Saxton et al., 2019; Miao et al., 2020; Amini et al.,
2019; Hendrycks et al., 2021; Patel et al., 2021)
have been proposed to increase the difﬁculty, di-
versity or even adversarial robustness. One con-
current work (published within 20 days) similar to
ours is LiLA (Mishra et al., 2022), which proposes
to assemble a large set of mathematical datasets
into a uniﬁed dataset. LiLA also annotates Python
programs as the generation target for solving math-
ematical problems. However, LiLA is mostly fo-
cused on dataset uniﬁcation. Our work aims to
understand how to generate ‘thoughtful programs’
to best elicit LLM’s reasoning capability. Besides,
we also investigate how to solve math problems
without any exemplars.
4.2
In-context Learning with LLMs
GPT-3 (Brown et al., 2020) demonstrated a strong
capability to perform few-shot predictions, where
the model is given a description of the task in natu-
ral language with few examples. Scaling model
size, data, and computing are crucial to enable
this learning ability. Recently, (Rae et al., 2021;
Smith et al., 2022; Chowdhery et al., 2022; Du
et al., 2022) have proposed to train different types
of LLMs with different training recipes. The ca-
pability to follow few-shot exemplars to solve un-
seen tasks is not existent on smaller LMs, but only
emerge as the model scales up (Kaplan et al., 2020).
Recently, there have been several works (Xie et al.,
2021; Min et al., 2022) aiming to understand how
and why in-context learning works. Another con-
current work similar to ours is BINDER (Cheng
et al., 2022), which applies Codex to synthesize
‘soft’ SQL queries to answer questions from tables.
4.3
Chain of Reasoning with LLMs
Although LLMs have demonstrated remarkable
success across a range of NLP tasks, their abil-
ity to reason is often seen as a limitation. Recently,
CoT (Wei et al., 2022; Kojima et al., 2022; Wang
et al., 2022b) was proposed to enable LLM’s ca-
pability to perform reasoning tasks by demonstrat-
ing ‘natural language rationales’.
Suzgun et al.
(2022) have shown that CoT can already surpass hu-
man performance on challenging BIG-Bench tasks.
Later on, several other works (Drozdov et al., 2022;
Zhou et al., 2022; Nye et al., 2021) also propose
different approaches to utilize LLMs to solve com-
positional reasoning tasks by allowing intermediate
steps.
5
Conclusions
In this work, we investigate how to disentangle
computation from reasoning in solving numerical

problems. By ‘program of thoughts’ prompting,
we are able to elicit LLMs’ abilities to generate
accurate programs to express complex reasoning,
while also allowing computation to be separately
handled by an external program interpreter. This
approach is able to boost the state-of-the-art perfor-
mance on several math datasets signiﬁcantly. We
believe our work can inspire more work to combine
symbolic execution with LLMs.
References
Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik
Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-
jishirzi. 2019. Mathqa: Towards interpretable math
word problem solving with operation-based for-
malisms. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
2357–2367.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al. 2021a.
Evaluating large lan-
guage models trained on code.
arXiv preprint
arXiv:2107.03374.
Wenhu Chen. 2022.
Large language models are
few (1)-shot table reasoners.
arXiv preprint
arXiv:2210.06710.
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena
Shah, Iana Borova, Dylan Langdon, Reema Moussa,
Matt Beane, Ting-Hao Huang, Bryan R Routledge,
et al. 2021b. Finqa: A dataset of numerical reason-
ing over ﬁnancial data. In Proceedings of the 2021
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 3697–3711.
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma,
Sameena Shah, and William Yang Wang. 2022. Con-
vﬁnqa: Exploring the chain of numerical reasoning
in conversational ﬁnance question answering. arXiv
preprint arXiv:2210.03849.
Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu
Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong,
Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
et al. 2022. Binding language models in symbolic
languages. arXiv preprint arXiv:2210.02875.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, et al. 2022.
Palm: Scaling
language modeling with pathways. arXiv preprint
arXiv:2204.02311.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavar-
ian, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, and John Schulman. 2021.
Training veri-
ﬁers to solve math word problems. arXiv preprint
arXiv:2110.14168.
Andrew Drozdov, Nathanael Schärli, Ekin Akyürek,
Nathan Scales, Xinying Song, Xinyun Chen, Olivier
Bousquet, and Denny Zhou. 2022. Compositional
semantic parsing with large language models. arXiv
preprint arXiv:2209.15003.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong,
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022.
Glam: Efﬁcient scaling of language models with
mixture-of-experts. In International Conference on
Machine Learning, pages 5547–5569. PMLR.
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel
Stanovsky, Sameer Singh, and Matt Gardner. 2019.
Drop: A reading comprehension benchmark requir-
ing discrete reasoning over paragraphs. In Proceed-
ings of the 2019 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 2368–2378.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and
Jacob Steinhardt. 2021.
Measuring mathematical
problem solving with the math dataset.
arXiv
preprint arXiv:2103.03874.
Mohammad Javad Hosseini,
Hannaneh Hajishirzi,
Oren Etzioni, and Nate Kushman. 2014. Learning
to solve arithmetic word problems with verb catego-
rization. In EMNLP, pages 523–533.
Jared Kaplan,
Sam McCandlish,
Tom Henighan,
Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
2020.
Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022.
Large
language models are zero-shot reasoners.
arXiv
preprint arXiv:2205.11916.
Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish
Sabharwal, Oren Etzioni, and Siena Dumas Ang.
2015. Parsing algebraic word problems into equa-
tions. Transactions of the Association for Computa-
tional Linguistics, 3:585–597.
Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang
Liu. 2022.
Answering numerical reasoning ques-
tions in table-text hybrid contents with graph-based
encoder and tree-based decoder. In Proceedings of
the 29th International Conference on Computational
Linguistics, pages 1379–1390.

Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-
som. 2017. Program induction by rationale genera-
tion: Learning to solve and explain algebraic word
problems. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 158–167.
Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian
Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter
Clark,
and Ashwin Kalyan. 2022.
Dynamic
prompt learning via policy gradient for semi-
structured mathematical reasoning. arXiv preprint
arXiv:2209.14610.
Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su.
2020. A diverse corpus for evaluating and develop-
ing english math word problem solvers. In Proceed-
ings of the 58th Annual Meeting of the Association
for Computational Linguistics, pages 975–984.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022.
Rethinking the role of demonstra-
tions: What makes in-context learning work? arXiv
preprint arXiv:2202.12837.
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard
Tang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-
hit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark,
and Ashwin Kalyan. 2022. Lila: A uniﬁed bench-
mark for mathematical reasoning. In Proceedings of
the 2022 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, et al. 2021. Show your work: Scratch-
pads for intermediate computation with language
models. arXiv preprint arXiv:2112.00114.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal.
2021. Are NLP models really able to solve simple
math word problems?
In Proceedings of the 2021
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 2080–2094, Online.
Association for Computational Linguistics.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446.
Subhro Roy and Dan Roth. 2015. Solving general arith-
metic word problems. In Proceedings of the 2015
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1743–1752.
Subhro Roy and Dan Roth. 2018. Mapping to declara-
tive knowledge for word problem solving. Transac-
tions of the Association for Computational Linguis-
tics, 6:159–172.
David Saxton, Edward Grefenstette, Felix Hill, and
Pushmeet Kohli. 2019. Analysing mathematical rea-
soning abilities of neural models.
arXiv preprint
arXiv:1904.01557.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Casper, Zhun Liu, Shrimai Prabhumoye, George
Zerveas, Vijay Korthikanti, et al. 2022.
Using
deepspeed and megatron to train megatron-turing
nlg 530b, a large-scale generative language model.
arXiv preprint arXiv:2201.11990.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V Le, Ed H Chi,
Denny Zhou, et al. 2022.
Challenging big-bench
tasks and whether chain-of-thought can solve them.
arXiv preprint arXiv:2210.09261.
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,
et al. 2022. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239.
Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin-Yu Dai, Shu-
jian Huang, and Jiajun Chen. 2022a.
A numeri-
cal reasoning question answering system with ﬁne-
grained retriever and the ensemble of multiple gen-
erators for ﬁnqa. arXiv preprint arXiv:2206.08506.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,
Ed Chi, and Denny Zhou. 2022b. Self-consistency
improves chain of thought reasoning in language
models. arXiv preprint arXiv:2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. arXiv preprint arXiv:2201.11903.
Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2021. An explanation of in-context
learning as implicit bayesian inference. In Interna-
tional Conference on Learning Representations.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei,
Nathan Scales, Xuezhi Wang, Dale Schuurmans,
Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
Least-to-most prompting enables complex reason-
ing in large language models.
arXiv preprint
arXiv:2205.10625.
Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao
Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and
Tat-Seng Chua. 2021. Tat-qa: A question answering
benchmark on a hybrid of tabular and textual content
in ﬁnance. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers),
pages 3277–3287.

A
Appendix
A.1
PoT as intermediate step
We demonstrate the workﬂow in Figure 8. We write
Figure 8: We adopt PoT to prompt language models to
ﬁrst generate an intermediate answer and then continue
to prompt large models to generate the ﬁnal answer.
the pseudo code as follows:
1 # Function PoT ( Input ) −> Output
2 # Input :
q u e s t i o n
3 # Ouptut :
program
4 # Function
Prompt ( Input ) −> Output
5 # Input :
q u e s t i o n + i n t e r m e d i a t e
6 # Ouptut :
answer
7 program = PoT ( q u e s t i o n )
8 exec ( program )
9 i f
i s i n t a n c e ( ans ,
d i c t ) :
10
ans =
l i s t ( x . items ( ) ) . pop ( 0 )
11
e x t r a = ’ according
to
the
program :
’
12
e x t r a += ans [ 0 ] + ’ = ’ + ans [ 1 ]
13
pred = Prompt ( q u e s t i o n + e x t r a )
14 e l s e :
15
pred = ans
16 r e t u r n
pred

