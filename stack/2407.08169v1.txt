Faster Machine Unlearning via Natural Gradient
Descent
Omri Lev and Ashia Wilson
Massachusetts Institute of Technology
{omrilev, ashia07}@mit.edu
Abstract
We address the challenge of efficiently and reliably deleting data from machine
learning models trained using Empirical Risk Minimization (ERM), a process
known as machine unlearning. To avoid retraining models from scratch, we pro-
pose a novel algorithm leveraging Natural Gradient Descent (NGD). Our theoretical
framework ensures strong privacy guarantees for convex models, while a practical
Min/Max optimization algorithm is developed for non-convex models. Com-
prehensive evaluations show significant improvements in privacy, computational
efficiency, and generalization compared to state-of-the-art methods, advancing both
the theoretical and practical aspects of machine unlearning. 1
1
Introduction
The exponential growth in data collection and machine learning applications has intensified concerns
about user privacy and data security. Legislative frameworks such as the European Union’s General
Data Protection Regulation (GDPR), California’s Consumer Privacy Act (CCPA), and Canada’s
proposed Consumer Privacy Protection Act (CPPA) emphasize the right of individuals to have their
data deleted upon request. These new requirements have catalyzed the development of "machine
unlearning" — the process of modifying a trained model to eliminate the influence of specific data
inputs, effectively making the model "forget" this data. The main challenge is making this process
efficient without resorting to retraining models from scratch, which is computationally expensive and
unsustainable as the scale of models and data grows.
Historically, theoretical approaches to machine unlearning have predominantly focused on convex
models, where the problem space allows for the development of algorithms with provable guarantees
of privacy and correctness [1, 2]. These methods often rely on operations like the Newton step, which,
while effective, are computationally demanding for large-scale applications. In contrast, practical
approaches typically address more complex, non-convex models using heuristic methods that lack
formal privacy guarantees [3]. These methods often re-optimize the model on a pruned dataset, which
is believed not to contain the information intended for unlearning—–a computationally expensive
process that can inadvertently retain data influence and thus compromise privacy.
This paper proposes a novel algorithm based on Natural Gradient Descent (NGD)—a gradient
algorithm that preconditions a gradient descent update with the inverse of the Fisher information
matrix of the underlying statistical model. By treating the ERM problem through the lens of maximum-
likelihood estimation, we first develop an algorithm for convex models, proving it maintains strong
unlearning guarantees and is more computationally efficient than existing methods, which are based
on the Newton step. Leveraging NGD’s adaptability to large models, we design a practical unlearning
algorithm based on a Min/Max optimization procedure for realistic settings. Our new algorithm
outperforms state-of-the-art unlearning algorithms in multiple aspects.
1Code will be available upon publication.
Preprint. Under review.
arXiv:2407.08169v1  [cs.LG]  11 Jul 2024

Contributions
We make the following key contributions:
• We formalize the unlearning problem from a probabilistic perspective, offering a computa-
tionally efficient algorithm for data deletion that scales to large models (c.f. Alg. 1).
• We introduce a theoretical framework that supports our algorithm, ensuring it guarantees
privacy for convex models according to unlearning privacy measures (c.f. Lem. 1, Th. 1).
• Next, we extend the theoretical algorithm to real non-convex models, offering a novel
unlearning algorithm based on Min/Max training using NGD (c.f. Alg. 2).
• We validate our algorithm’s practical advantages through comprehensive simulations, show-
ing significant improvements in privacy and generalization across various non-convex
settings (c.f. Sec. 5.1).
2
Related Work
The goal of exact unlearning, first proposed by Cao and Yang [4], is to find a model whose predictions
match those of a retrained model. This can be achieved through retraining or using other techniques
that tend to be computationally or memory-intensive [5, 6, 7]. In contrast, approximate unlearning is
a less stringent requirement that allows the unlearned model to deviate from the retrained model by a
bounded amount. Several works have introduced more efficient methods that satisfy these inexact
criteria [1, 2, 8, 9, 10, 11].
We develop a computationally efficient algorithm that satisfies the (ϵ, δ)-approximate unlearning
definition from Sekhari et al. [1]. Sekhari et al. [1] also prove that approximate unlearning can be
achieved using a Newton step towards the gradient of the points we aim to unlearn in the convex
case. This algorithm was later generalized to efficiently remove points online and to delete points in
scenarios involving non-differentiable regularizers [2].
In contrast to previous work that utilizes Newton-style updates and requires a full Hessian inversion,
we aim to reduce this computational burden by leveraging the NGD [12]. NGD offers a low-cost
second-order update by replacing the Hessian with the Fisher Information Matrix (FIM). As we
will show, the NGD update is closely related to the Fisher-unlearning methods studied by Golatkar
et al. [11, 13] and Wang et al. [14]. Our contributions improve upon these previous algorithms in
two key ways. First, we are the first to prove that FIM-based methods satisfy the (ϵ, δ)-unlearning
definition, establishing the correctness of our algorithm in the convex case. Second, we demonstrate
that FIM-based methods are specific cases of second-order unlearning methods that replace the
Hessian with its Positive Semi-Definite (PSD) approximation, given by the Gauss-Newton Matrix
[12]. This exploration introduces a more general framework that paves the way for developing
advanced unlearning methods.
We end by turning our sights to unlearning in the non-convex regime. While recent works have also
introduced unlearning algorithms based on the Min/Max training procedure [9], we extend this theory
and show how these methods can be enhanced by incorporating Min/Max training based on NGD.
3
Background
We now discuss the problem of learning and unlearning and review NGD algorithm.
3.1
Machine Learning
In our learning setting, we aim to minimize an objective function comprised of a loss function ℓ, a
regularizer π, and a regularization parameter λ ∈[0, ∞). The goal of learning is to find a parameter
θ∗(λ) ∈Θ that minimizes the population risk
θ∗(λ) ≜argmin
θ∈Θ
L(Pz, θ, λ),
L(Pz, θ, λ) ≜Ez∼Pz [ℓ(z, θ) + λπ(θ)]
(1)
where usually z = (x, y) comprised of a covariate x and a label y that distributed according to
Px(x)Py|x(y|x). Our analysis assumes a one-dimensional label, as is common in many typical
machine-learning problems (regression, classification, etc). The distribution Pz is often inaccessible,
2

so given a dataset S = (z1, z2, . . . zn) with zi
iid
∼Pz we instead find a model ˆθS(λ) that minimizes
the empirical risk
ˆθS(λ) ≜argmin
θ∈Θ
L(S, θ, λ),
L(S, θ, λ) ≜1
|S|
X
z∈S
ℓ(z, θ) + λπ(θ).
(2)
As we discuss in Sec. 3.3, in many scenarios, ℓ(z, θ) = −log (P (y|f(x; θ))); that is, the loss can
be interpreted as a likelihood under a probabilistic model induced by a parameterized function
f(x; θ), often taken to be a neural network. Moreover, we study the case where P (y|f(x; θ))
belongs to an exponential-family [15] whose natural parameters are the features f(x; θ). This is
satisfied by many common loss functions in machine learning (see App. C). Our formulation assumes
that the regularization is convex but not necessarily differentiable, allowing many common convex
non-differentiable regularizers (L1, elastic-net, etc).
3.2
Machine Unlearning
After using a dataset S to train and publish a model ˆθS(λ), a set of m users in the training set
U ⊂S might request that their data points be deleted and that any models produced using their data
to be removed. An organization might initially consider re-optimizing the leave-U-out objective
L(S\U, θ, λ) to produce ˆθS\U(λ) to comply with this request. While re-optimizing from scratch
constitutes a baseline for unlearning, the computational cost makes complying with every data
deletion request undesirable. Thus, our goal is to derive an efficient method to approximate ˆθS\U(λ)
without retraining over the entire dataset S\U from scratch. Ultimately, our method will only require
access to the samples to be deleted U and possibly additional statistics about the original dataset
S, B(S), obtained during the training process. Moreover, we aim to prevent an external observer
from distinguishing between the approximated solution (denoted by ˜θS\U) and the ERM-minimizer
ˆθS\U. This goal can be defined similarly to the classical definition of Differential Privacy (DP) [1, 2],
which can be informally explained as the requirement that, with high-probability, an observer cannot
differentiate between the two cases:
1. The model is trained on the set S and then a set U is deleted by the unlearning algorithm.
2. The model is trained on the set S\U, and no points are deleted thereafter.
Mathematically, this notion is captured by the next definition by Sekhari et al. [1], which generalizes
the privacy notions from the DP literature [16, 17]
Definition 1 ((ϵ, δ)-unlearning). For all S of size n, delete requests U ⊂S such that |U| ≤m, and
learning algorithm A : S →θ ∈Θ, an unlearning algorithm ¯A is (ϵ, δ)-unlearning if ∀Υ ⊆Θ
P
  ¯A (U, A(S), B(S)) ∈Υ

≤eϵ · P
  ¯A (∅, A(S\U), B(S\U)) ∈Υ

+ δ,
P
  ¯A (∅, A(S\U), B(S\U)) ∈Υ

≤eϵ · P
  ¯A (U, A(S), B(S)) ∈Υ

+ δ
where ∅is the empty set, ϵ and δ are some positive constants, and B(S) are some statistics.
Intuitively, this definition asserts that an external observer cannot distinguish between a model trained
on S\U and a model trained on S and then unlearning U.
3.3
Natural Gradient Descent
Natural Gradient Descent (NGD) is considered an efficient method to optimize the sum of likelihood
functions from a parametric family [18, 19, 20]. In supervised learning, given a training set S and
a loss ℓ(·), we aim to minimize the population risk (1) by minimizing the empirical risk (2). This
problem is equivalent to the following maximum-likelihood estimation over the model parameters
ˆθS = argmax
θ
X
(x,y)∈S
log
 Py|x (y|x; θ)

≜argmin
θ
L(S, θ, λ = 0),
given our assumption that the data is distributed according to Py|x(y|x; θ)Px(x) and where ℓ(z, θ) ≜
−log (P (y|x; θ)). This interpretation comes up naturally in many machine learning problems when
3

the network predicts a probabilistic distribution over the target alphabet (see [21, Ch. 3.1, Ch. 4.2]).
Following this interpretation, the maximization is over probabilistic models Py|x(y|x; θ), which lies
on the probability simplex of the output alphabet Y and are controlled by the parameters θ. Unlike
classical gradient algorithms, which take steps in the steepest direction over the parameter space θ,
NGD follows the steepest direction over the probability simplex, where distances are measured via
the Kullback Leibler (KL) divergence [20]. The iterative process that defines the NGD algorithm is
given by [12, 22]
θ(t+1) = θ(t) −α ·

F

θ(t)−1
∇θL

S, θ(t)
,
where α > 0 is the learning rate and where the matrix F
 θ(t)
is the Fisher Information Matrix
(FIM), which is given by
F

θ(t)
≜E(x,y)∼Px,y;θ
h
∇θ log

Py|x;θ

y|x; θ(t)
∇T
θ log

Py|x;θ

y|x; θ(t)i
.
The FIM represents the local curvature of the KL-divergence, i.e. D
 Py|x;(θ+d)
Py|x;θ
 ∼= 1
2dT Fd.2
The gradient is typically evaluated over a batch of samples from S. Since the distribution of the
covariates Px is not accessible, the FIM is approximated using empirical estimates involving averaging
over the covariate samples and leveraging the underlying network structure to evaluate the expectation
over Py|x;θ [12, 24]. The approximated matrix is called the approximated FIM and is defined as
F
 S, θ(t)
≜
1
|S|
P
x∈S Ey∼Py|x=x;θ

∇θ log
 Py|x;θ
 y|x; θ(t)
∇T
θ log
 Py|x;θ
 y|x; θ(t)
. (3)
Notably, the approximated FIM averages over the observations of the covariates x while taking the
exact expectation with respect to Py|x. For many common network architectures, (3) is usually sim-
plified by utilizing information about the network architecture. Accordingly, many computationally
efficient algorithms are developed in practice to calculate the NGD step [12, 25, 26].
Finally, following classical developments (see, for example, [27] and [12, Sec. 9.2]) whenever the
loss function is given by ℓ(z, θ) = −log (P (y|f(x; θ))) and the distribution of P(y|f) is out of an
exponential family, the approximated FIM can be considered a PSD approximation to the Hessian.
Specifically, the Hessian can be written as H = F (S, θ) + R where F (S, θ) is guaranteed to be
PSD and, in many popular scenarios, R shrinks to zero (in L2 sense) as the model’s training accuracy
improves (see [12, 24]). Thus, the approximated FIM is interpreted as the PSD part of the Hessian.
4
Computationally Efficient Unlearning for Convex Models
Our goal is to develop a computationally efficient second-order algorithm for machine unlearning that
provably satisfies the (ϵ, δ)-unlearning of Def. 1. To that end, we use the NGD to develop such an
algorithm. We start by proposing an algorithm for the convex case, for which we formally prove the
unlearning requirement. In Sec. 5, we will extend our algorithm for non-convex cases. Our notations
correspond to the ERM framework presented in Sec. 3.1. Our development targets regression and
classification tasks.
4.1
Method: Unlearning via the NGD
The aim of our first algorithm is to guarantee fast unlearning in convex models. Building on the
existing unlearning techniques, which are based on the Newton step [1, 2], our method shows that the
Newton step can be replaced by an NGD step while still maintaining the same unlearning guarantees.
However, since the NGD step is computationally cheaper than the Newton step in many models, this
algorithm leads to significant savings in terms of computational time. Throughout the analysis, we
will make use of the proximal operator, defined via
proxH
λπ(v) = argmin
θ∈Θ
n
(v −θ)T H (v −θ) + 2λπ(θ)
o
for a PSD H. Beyond the computational efficiency of the NGD step, the FIM is guaranteed to be PSD,
further improving computational stability [10, 26, 27, 28]. The FIM structure also supports many
practical approximation algorithms useful in neural network optimization and training [25, 26, 29].
Our NGD-based unlearning algorithm is presented in Alg. 1.
2The KL-divergence between two distributions Q(x) and P(x) defined over alphabet X is defined by
D (P∥Q) = P
x∈X P(x) log

P (x)
Q(x)

. The second-order approximation can be derived as in [23, Ch. 2].
4

Algorithm 1 Unlearning using the NGD
Input: Delete request: U = {zik}m
k=1, Cost minimizer:
ˆθS(λ) ≜ˆθS, Loss gradient:
∇L

U, ˆθS, λ = 0

, FIM: F

S, ˆθS

, Cardinality: |S| ≜n, either λ∇2π

ˆθS

or λπ.
1: if π(·) is twice differentiable then
2:
Update parameter estimates
˜θS\U = ˆθS +

F

S, ˆθS

+ λ∇2π

ˆθS
−1
∇˜L

U, ˆθS, λ = 0

where ∇˜L

U, ˆθS, λ

≜
n
n−m∇L

U, ˆθS, λ

, m ≜|U|.
3: else
4:
Update parameter estimates
˜θS\U = prox
F(S,ˆθS)
λπ

ˆθS +

F

S, ˆθS
−1
∇˜L

U, ˆθS, λ = 0

(4)
where ∇˜L

U, ˆθS, λ

≜
n
n−m∇L

U, ˆθS, λ

.
5: end if
6: Set γ = Mm2C2
µ3n2
+ 2m2CQK
µ2n2
≜
˜γ
n2 and sample ν ∼N

0,

γ
ϵ
q
2 log
  1.25
δ
2
Id

7: Publish ¯θS\U = ˜θS\U + ν
We now prove that Alg. 1 satisfies the (ϵ, δ)-unlearning requirement of Def. 1. To that end, we make
the following assumptions:
Assumption 1.a. The loss functions are of the form ℓ(z, θ) = −log (P (y|f(x; θ))) where P (y|f)
belongs to an exponential family whose natural parameters are f(x; θ). We further assume that
−∇2
f log (P (y|f(x; θ))) ⪯Q · Id for some Q > 0.
Assumption 1.b. The functions ℓ(z, θ) + λπ(θ) are µ-strongly convex and C-Lipschitz. For twice-
differentiable regularizers, we assume that ℓ(z, θ) + λπ(θ) is twice differentiable with M-Lipschitz
Hessian. Otherwise, we assume these properties only for ℓ(z, θ) and assume that π(θ) is convex. The
gradient of the features f(x; θ) is assumed to be bounded, i.e. ∥∇θf(x; θ)∥2 ≤K.
In particular, we focus on the case where our loss function is either a cross-entropy loss or a mean-
squared error loss, which meets our assumptions (see App. C and App. E).
Lemma 1. Suppose our training algorithm returns ˆθS(λ) that minimizes the training loss exactly:
ℓ

z, ˆθS(λ)

= 0, ∀z ∈S. Under Assumptions 1.a and 1.b, Alg. 1 satisfies the (ϵ, δ)-unlearning
criterion given in Def. 1.
Lem. 1, whose proof can be found in App. E, follows a similar approach to the proofs of unlearning
algorithms based on the Newton step [1, 2]. Specifically, it follows by showing that our algorithm
produces an approximate solution with a distance to the ERM-minimizer that scales as O((m/n)2).
Consequently, (ϵ, δ)-unlearning is ensured by employing the Gaussian mechanism for differential
privacy [16, App. A]. We now provide guarantees on the population risk.
Theorem 1. Under the same assumptions as Lem. 1 and by using Alg. 1, the population risk satisfies
L
 Pz, ¯θS\U, λ

−L (Pz, θ∗(λ), λ) = O
 √
d˜γC
n2ϵ
q
log
  1
δ

+ mC2
µn

(5)
The proof of this theorem is in App. G and broadly follows by a technique similar to that employed in
existing unlearning methods [1, 2], by replacing the Hessian with its PSD approximation. Furthermore,
(5) ensures that whenever m = o(n), the generalization capabilities of the approximated solution are
close to those of the ERM-minimizer.
Finally, We note that the approximation of the Hessian using its PSD part exists beyond our assumed
exponential family loss model. Specifically, Schraudolph [27] proved a similar decomposition for
5

loss functions of the form a(b(θ)) with a convex a(·), where the Gauss-Newton Matrix now provides
the PSD part, depending again only on first order gradients of the model (see also [24]). With a
proper choice of a function b(θ), the additional term can be shown to disappear with the training
error, allowing the same proof from App. E and App. F. Thus, our technique can be applied to more
general loss functions.
4.2
Experiments
We demonstrate the computational efficiency of our method compared to the Newton step through
numerical simulations. We applied Alg. 1 to various architectures, measuring the total execution time
of the unlearning step. To approximate the Newton and NGD steps, we used the LiSSA algorithm
[30] (see also App. I) with T = 1000, σ = 5000, R = 5 and batches of size 32. Simulations included
four architectures: one-layer perceptron (OLP), multi-layer perceptron (MLP), convolutional neural
network (CNN), and ResNet18 [31] (A detailed description of the architectures is in App. H.2). Since
λ affects the convergence of the LiSSA algorithm, we simulate the performance for two different
values of λ. In all experiments, we first train the base network with the entire dataset to obtain the
parameters ˆθS. All models have been trained until they achieve perfect training accuracy and close to
zero training loss. For unlearning, we randomly selected a subset of 1000 examples as the forget set
U. We then applied our algorithm to obtain the modified parameters ˜θS\U and recorded the execution
time. The graphs report averages over ten runs, with error bars showing standard deviations. We note
that when the OLP is trained with L2 regularization, it is a strongly convex model, which is further
Lipschitz and twice differentiable almost everywhere (beyond the single non-linearity point induced
by the ReLU) and whose gradient norm is bounded almost everywhere and further is guaranteed to
achieve zero training loss whenever it over-parametrized relative to the data dimension [32]. Thus,
OLP satisfies the assumptions of Alg. 1 and approximate unlearning is guaranteed. Our experiments
suggest that NGD confers significant computational advantage over the Newton step for not only
OLP, but all other objectives tried.
0.005
0.010
0.015
38
40
42
execution time [sec]
OLP
0.005
0.010
0.015
55
60
65
70
execution time [sec]
CNN
0.005
0.010
0.015
45
50
55
execution time [sec]
MLP
0.005
0.010
0.015
250
300
350
execution time [sec]
ResNet18
Newton
NGD
(a) λ = 10−8
0.005
0.010
0.015
36
38
40
42
execution time [sec]
OLP
0.005
0.010
0.015
60
70
80
execution time [sec]
CNN
0.005
0.010
0.015
45
50
55
execution time [sec]
MLP
0.005
0.010
0.015
250
300
350
execution time [sec]
ResNet18
Newton
NGD
(b) λ = 10−6
Figure 1: Computational time for an NGD and Newton steps, applied to multiple models. Across all
unlearning tolerances, the calculation time of the NGD is much lower than that of the Newton step.
This trend holds for two different values of the regularization parameter λ.
4.3
Comparison to Previous Results
We compare Alg. 1 to other unlearning algorithms in terms of computational complexity, assumption
requirements, theoretical guarantees, and other algorithmic components.
1. Comparison of Assumptions: Our analysis assumes a µ-strongly convex, C-Lipschitz,
and M-Hessian Lipschitz loss function, similar to the assumptions in [1, 2]. We further
assume that the gradient of the features is bounded and an exponential family loss model,
which most common settings satisfy. Our proof assumes zero training loss, which is crucial
for good generalization [33]. Since modern machine learning models—both convex [34]
6

and non-convex [32, 35, 36]—are generally trained to zero training loss, this assumption is
practical and generally satisfied.
2. Comparison of Computational Complexity: Our algorithm involves inverting the FIM
and multiplying the inverse with a gradient vector, whereas the algorithms of Sekhari et al.
[1] and Suriyakumar and Wilson [2] require the same operation but with the Hessian. These
operations can be efficiently approximated using techniques calculating inverse-Jacobian
Vector Products (iJVP) and inverse-Hessian Vector Products (iHVP) [27, 30], respectively.
Since JVP computations are less costly than HVP ones [27], our algorithm is more efficient.
Our experiments confirm that batch removal using the FIM is faster than using the Hessian.
3. Comparison of Unlearning and Generalization Guarantees: The guarantees provided in
Lem. 1 and Th. 1 are essentially the same as those of [1] and [2] (up to different constants).
4. Algorithmic Comparison: Golatkar et al. [10] suggested using the FIM for unlearning by
adding noise whose covariance is the inverse of the FIM, but it relies on approximations
and lacks formal unlearning guarantees. Our approach uses NGD, which can be calculated
directly without needing direct access to the FIM. Similarly, Golatkar et al. [11] proposed
adding a gradient preconditioned with the inverse of the FIM, akin to the NGD step, but it
also lacks formal privacy guarantees so we omit this preconditioner.
5
Unlearning Non-Convex Models via NGD-Based Min/Max Training
Using classical theoretical developments established for NGD, we now apply Alg. 1 to real-world
models, which usually violate assumptions 1.a and 1.b. Alg. 1 employs a natural gradient ascent step
towards the data in U, which amounts to natural gradient descent step over the data in S\U, assuming
that ∇θL(S, ˆθS(λ), λ) = 0. Therefore, we propose using a Min/Max optimization algorithm that
entails executing both steps iteratively. This formulation amounts to first maximizing the loss on
U and then minimizing it on S\U and can be mathematically captured by employing a Min/Max
training [37, 38] over the combined loss
LMin/Max = L(S\U, θ, λ) −L(U, θ, λ) =
1
|S\U|
X
z∈S\U
ℓ(z, θ) −1
|U|
X
z∈U
ℓ(z, θ).
As is usually done in Min/Max training, we minimize LMin/Max with an additional smoothing term
[38]. Our algorithm is presented in Alg. 2 where we calculate the NGD steps of Alg. 2 using one of
the K-FAC schemes proposed for scaling NGD to large models [25, 26].
Comparison of our Alg. 2 to Scrub.
Alg. 2 resembles the state-of-the-art Scrub algorithm [9],
which has been proposed for unlearning data. While our algorithm directly minimizes a linear
combination of loss functions, Scrub optimizes a more complicated linear combination of the loss
function and KL-divergences. The combination of losses and KL terms used by Scrub requires
careful tuning of the weighting of both factors and scheduling of learning-rate decay. Our algorithm,
on the other hand, does not require this weighting and thus requires fewer hyperparameters, making
its training process more stable. As demonstrated in Sec. 5.1, our algorithm outperforms Scrub in
multiple settings.
5.1
Experiments
We compare our Min/Max NGD algorithm to other unlerning algorithms such as Scrub by examining
(1) the amount of information that is left about the removed examples in the new model and (2)
test accuracy. The experimental setting is similar to that described in Sec. 4.2. To measure the
amount of information that is left about the removed examples, we use a Membership Inference
Attack (MIA) that assesses if a sample was part of the training set [9, 39, 40]. In particular, we have
implemented the MIA that uses logistic regression to classify losses that correspond to both examples
from the forget set and from the test set [9]. The success rate of this attack (i.e., the number of correct
guesses between the test set and the forget set) is measured on ResNet18 and CNN models trained to
classify the CIFAR-10 dataset [41] (a precise description of the models is given in App. H). Baseline
unlearning algorithms include a fine-tuning algorithm on the retain set and the Scrub algorithm from
[9]. Each algorithm ran for six epochs. We measured the test accuracy and the MIA score of the
7

Algorithm 2 Min/Max NGD
Input: Delete request: U = {zik}m
k=1, Cost minimizer: ˆθS(λ) ≜ˆθS
1: Initialize:
ˆθ ←ˆθS, θsmooth ←ˆθS
2: for epoch = 1, . . . , max-epochs do
3:
for k1-steps do
4:
Sample batch BU of size BU from U and update ˆθ using natural gradient ascent
ˆθ ←ˆθ + α

F

ˆθ
−1
 X
z∈BU
∇θ

ℓ(z, θ) + γ ∥θ −θsmooth∥2
θ=ˆθ
!
5:
end for
6:
for k2-steps do
7:
Sample batch BS\U of size BS\U from S\U and update ˆθ using natural gradient descent
ˆθ ←ˆθ −α

F

ˆθ
−1

X
z∈BS\U
∇θ

ℓ(z, θ) + γ ∥θ −θsmooth∥2
θ=ˆθ


8:
end for
9:
Update smoothed parameters
θsmooth ←θsmooth + β

ˆθ −θsmooth

10: end for
11: Release ˜θS\U = ˆθ
new models, where the MIA score is defined as the distance between the probability of a correct
guess and 0.5. The results are reported in Fig. 2. The Min/Max NGD improves the MIA score
while achieving comparable or better test accuracy than the Scrub algorithm. Additionally, NGD
significantly enhances the test accuracy of the fine-tuning baseline while maintaining almost the same
(in the CNN example) or slightly worse (in the ResNet18 example) MIA score.
6
Discussion
We study the unlearning problem from a probabilistic approach, utilizing NGD as an unlearning
algorithm due to its known computational efficiency compared to existing second-order methods.
Theoretically, our study is the first to formally prove that, in a convex setting, NGD-based unlearning
satisfies the (ϵ, δ) notion of unlearning. Since NGD is often approximated using Kronecker factors
[26], investigating the unlearning capabilities of these approximated NGD algorithms presents another
intriguing research direction. Furthermore, in many instances, an explicit NGD step can be directly
computed by taking gradients relative to transformed versions of the model parameters [42, 43].
Exploring efficient NGD-based unlearning algorithms using these methods is a promising avenue for
future research. Unlike the Scrub algorithm, which lacks theoretical support, our algorithm is proven
to meet the (ϵ, δ)-unlearning requirement in convex cases. Extending this theory to more complex
scenarios could demonstrate that Min/Max NGD formally satisfies the (ϵ, δ)-unlearning requirement.
On the practical side, we developed a Min/Max training algorithm leveraging NGD to delete samples.
Our results show that our algorithm enhances the MIA score while maintaining nearly the same test
accuracy as state-of-the-art baselines. Additionally, our algorithm requires fewer hyperparameters
than existing algorithms. Given the known instability issues of Min/Max training procedures [9, 44],
our approach results in a more stable training process. We encourage future work to explore the
applicability of Min/Max NGD in larger models, various architectures, different training objectives,
and across diverse domains and modalities. It is important to note that while most unlearning
baselines report MIA as a measure of the amount of information the model retains about the unlearned
samples, MIA still lacks a meaningful interpretation in terms of the privacy guarantees of the system,
specifically regarding the (ϵ, δ) from Def. 1 (see a detailed discussion in [40]). The practical
8

1
2
3
4
5
6
Epoch
72
74
76
78
80
82
84
Test accuracy %
Min/Max NGD (ours)
Finetuning with NGD
Scrub
Finetuning with SGD
1
2
3
4
5
6
Epoch
0.00
0.02
0.04
0.06
0.08
0.10
0.12
MIA Score
(a) ResNet18
1
2
3
4
5
6
Epoch
55.0
57.5
60.0
62.5
65.0
67.5
70.0
Test accuracy %
Min/Max NGD (ours)
Finetuning with NGD
Scrub
Finetuning with SGD
1
2
3
4
5
6
Epoch
0.00
0.05
0.10
0.15
0.20
MIA Score
(b) CNN
Figure 2: Test accuracy and MIA score of the Min/Max NGD, Scrub, and the fine-tunning algorithms.
The upper and lower dotted lines correspond to the original and retrained model’s MIA scores. The
details of the experiments are in App. H. Compared to Scrub, Min/Max NGD achieves a better MIA
score while maintaining almost the same or slightly better test accuracy.
significance of the MIA score remains unclear. Since this connection is well established in the DP
realm (see, for example, [45]), developing similar theorems for the unlearning framework is crucial
for effectively deploying unlearning algorithms.
Broader Impact The ability to unlearning at scale is crucial for the practical deployment of artificial
intelligence (AI) systems, providing an effective tool for users to manage the amount of sensitive
information that machine learning models reveal about their training data. Our proposed technique
offers a provable solution for unlearning data in convex cases, significantly enhancing privacy control
in such scenarios. However, it is important to recognize that while the Min/Max NGD method shows
promise, it lacks a formal privacy proof for non-convex cases. Therefore, any application of this
algorithm for data deletion should be approached with caution. Rigorous auditing and validation
processes must be in place to ensure that the intended privacy guarantees are met and that the system
does not inadvertently retain sensitive information. By highlighting these considerations, we aim to
encourage responsible use and further research to extend formal privacy guarantees to non-convex
models.
References
[1] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember
what you want to forget: Algorithms for machine unlearning. In Advances in Neural Information
Processing Systems (NeurIPS), volume 34, pages 18075–18086, 2021.
[2] Vinith Suriyakumar and Ashia C. Wilson. Algorithms that approximate data removal: New
results and limitations. In Advances in Neural Information Processing Systems (NeurIPS),
volume 35, pages 18892–18903, 2022.
9

[3] Jie Xu, Zihan Wu, Cong Wang, and Xiaohua Jia. Machine unlearning: Solutions and challenges.
IEEE Trans. Emerg. Top. Comput. Intell., 2024.
[4] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In
IEEE Symp. Secur. Privacy., pages 463–480, 2015. doi: 10.1109/SP.2015.35.
[5] Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin
Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In IEEE Symp.
Secur. Privacy., pages 141–159, 2021. doi: 10.1109/SP40001.2021.00019.
[6] Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Chris
Waites. Adaptive machine unlearning. In Advances in Neural Information Processing Systems
(NeurIPS), volume 34, pages 16319–16330. Curran Associates, Inc., 2021.
[7] Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Ayush Sekhari, and Chiyuan
Zhang. Ticketed learning–unlearning schemes. In Conference on Learning Theory (COLT),
volume 195, pages 5110–5139. PMLR, 12–15 Jul 2023.
[8] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal
from machine learning models. In Proceedings of the 37th International Conference on Machine
Learning (ICML). JMLR.org, 2020.
[9] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards un-
bounded machine unlearning. In Advances in Neural Information Processing Systems (NeurIPS),
volume 36, pages 1957–1987, 2023.
[10] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:
Selective forgetting in deep networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,
pages 9304–9312, 2020.
[11] Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto.
Mixed-privacy forgetting in deep networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern
Recognit. (CVPR), pages 792–801, June 2021.
[12] James Martens. New insights and perspectives on the natural gradient method. The Journal of
Machine Learning Research (JMLR), 21(1):5776–5851, 2020.
[13] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing
deep networks of information accessible from input-output observations. In Computer Vision–
ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part
XXIX 16, pages 383–398. Springer, 2020.
[14] Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative
pruning. In Proc. ACM Web Conf. 2022, pages 622–632, 2022.
[15] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and
variational inference. Foundations and Trends® in Machine Learning, 1(1–2):1–305, 2008.
[16] Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Founda-
tions and Trends® in Theoretical Computer Science, 9(3–4):211–407, 2014.
[17] Cynthia Dwork. Differential privacy. In Int. Colloq. Automata Lang. Program., pages 1–12.
Springer, 2006.
[18] Shun-ichi Amari. Neural learning in structured parameter spaces-natural riemannian gradient.
In Advances in Neural Information Processing Systems (NeurIPS), volume 9, 1996.
[19] Shun-Ichi Amari and Scott C Douglas. Why natural gradient? In Proc. IEEE Int. Conf. Acoust.
Speech and Sig. Proc. (ICASSP), volume 2, pages 1213–1216. IEEE, 1998.
[20] Shun-ichi Amari. Information geometry and its applications, volume 194. Springer, 2016.
[21] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning,
volume 4. Springer, 2006.
10

[22] Razvan Pascanu and Yoshua Bengio.
Revisiting natural gradient for deep networks.
In
International Conference on Learning Representations (ICLR), 2014.
[23] Yury Polyanskiy and Yihong Wu. Information Theory: From Coding to Learning. Cam-
bridge University Press, 2023.
URL https://people.lids.mit.edu/yp/homepage/
data/itbook-export.pdf.
[24] Frederik Kunstner, Philipp Hennig, and Lukas Balles. Limitations of the empirical Fisher
approximation for natural gradient descent. In Advances in Neural Information Processing
Systems (NeurIPS), volume 32, 2019.
[25] Roger Grosse and James Martens. A Kronecker-factored approximate fisher matrix for convolu-
tion layers. In International Conference on Machine Learning (ICML), pages 573–582. PMLR,
2016.
[26] James Martens and Roger Grosse.
Optimizing neural networks with Kronecker-factored
approximate curvature. In International Conference on Machine Learning (ICML), pages
2408–2417. PMLR, 2015.
[27] Nicol N. Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
Neural Computation, 14(7):1723–1738, 2002.
[28] Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger B Grosse. If influence
functions are the answer, then what is the question?
In Advances in Neural Information
Processing Systems (NeurIPS), volume 35, pages 17953–17967, 2022.
[29] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini,
Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, et al. Studying large language model
generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023.
[30] Naman Agarwal, Brian Bullins, and Elad Hazan. Second-order stochastic optimization for
machine learning in linear time. Journal of Machine Learning Research (JMLR), 18(116):1–40,
2017.
[31] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit., pages 770–778, 2016.
[32] Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error
guarantees for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
[33] Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proc.
52nd Annu. ACM SIGACT Symp. Theory Comput., pages 954–959, 2020.
[34] Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. Annals of statistics, 50(2):949, 2022.
[35] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning (ICML),
pages 1675–1685. PMLR, 2019.
[36] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning
via over-parameterization. In International Conference on Machine Learning (ICML), pages
242–252. PMLR, 2019.
[37] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems (NeurIPS), volume 27, 2014.
[38] Jiawei Zhang, Peijun Xiao, Ruoyu Sun, and Zhiquan Luo. A single-loop smoothed gradient
descent-ascent algorithm for nonconvex-concave min-max problems. In Advances in Neural
Information Processing Systems (NeurIPS), volume 33, pages 7377–7389, 2020.
[39] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.
Membership inference attacks from first principles. In IEEE Symp. Secur. Privacy., pages
1897–1914. IEEE, 2022.
11

[40] Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inexact
unlearning needs more careful evaluations to avoid a false sense of privacy. arXiv preprint
arXiv:2403.01218, 2024.
[41] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, University of Torontro, ON, Canada, 2009. URL https://www.cs.toronto.
edu/~kriz/learning-features-2009-TR.pdf.
[42] Wu Lin. Computationally efficient geometric methods for optimization and inference in machine
learning. PhD thesis, University of British Columbia, 2023.
[43] Wu Lin, Frank Nielsen, Mohammad Emtiyaz Khan, and Mark Schmidt. Structured second-order
methods via natural gradient descent. arXiv preprint arXiv:2107.10884, 2021.
[44] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations (ICLR),
2019.
[45] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential
privacy. In International Conference on Machine Learning (ICML), pages 1376–1385. PMLR,
2015.
[46] Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2013.
[47] Ashia Wilson, Maximilian Kasy, and Lester Mackey. Approximate cross-validation: guarantees
for model assessment and selection. In International Conference on Artificial Intelligence and
Statistics (AISTATS), pages 4530–4540. PMLR, 2020.
[48] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex
optimization. In Conference on Learning Theory (COLT), volume 2, page 5, 2009.
[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an imperative
style, high-performance deep learning library. In Advances in Neural Information Processing
Systems (NeurIPS), volume 32, 2019.
[50] Y. LECUN. The MNIST database of handwritten digits. 1998. URL http://yann.lecun.
com/exdb/mnist/.
[51] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of DNNs with natural
gradient and parameter averaging. arXiv preprint arXiv:1410.7455, 2015.
12

A
Notation
The notation used in this manuscript is outlined as follows. We denote random variables using
sans-serif fonts (x, y, z) and their realizations using regular italic fonts (x, y, z). The Probability
Density Function (PDF) of a random variable z is denoted by Pz(·). Groups of values are represented
by capital calligraphic letters, such as S ≜{z1, z2, . . . , zn}. Real numbers are denoted by R.
Matrices are denoted by bold capital letters, with the symbol Id specifically representing the d × d
identity matrix. When it is clear from the context that we are referring to matrices (for example,
∇2f(·)), regular notation is used. L2 norms are denoted by ∥·∥. We use the usual convention of
f(x) = o(g(x)) to denote that lim
x→∞
f(x)
g(x) = 0, and f(x) = O(g(x)) to denote that lim
x→∞
f(x)
g(x) = c
for some finite constant c (c ̸= 0).
B
Definitions
The manuscript uses the next classical definitions from the convex optimization theory [46].
Definition 2 (Strong convexity). Let β > 0. A function f(·) is called β-strongly convex if and only
if
f(y) ≥f(x) + ∇T f(x)(y −x) + β ∥x −y∥2 , ∀(x, y) ∈dom(f)
Definition 3 (Lipschitzness). A function f(·) is called C-Lipschitz if
∥f(x) −f(y)∥≤C ∥x −y∥, ∀(x, y) ∈dom(f).
If f(·) is differentiable, then f(·) is called K-smooth if
∥∇f(x) −∇f(y)∥≤K ∥x −y∥, ∀(x, y) ∈dom(f).
Furthermore, if f(·) is further twice differentiable, then f(·) is called M-Hessian Lipschitz if
∇2f(x) −∇2f(y)
 ≤M ∥x −y∥, ∀(x, y) ∈dom(f)
C
Loss Functions With an Exponential Family Structure
In this section, we present a few examples of common loss functions in machine learning of the form
ℓ(P (y|f(x; θ))) where P (y|f(x; θ)) belongs to an exponential family. Throughout the paper, we
use the following convention to denote an exponential family
log (P (y|f(x; θ))) = f T (x; θ)t(y) −log

X
˜y∈Y
exp

f T (x; θ)t(˜y)
	

+ β(y)
(6)
and where t(y) are called the natural statistics and f(x; θ) are the natural parameters. Using this
convention, we provide two popular examples of loss functions whose models fit this exponential
family framework (see also [12, Sec. 9.2]).
1. Let
log (P (y|f(x; θ))) = (f(x; θ))y −log


|Y|
X
˜y=1
exp
n
(f(x; θ))˜y
o


where f(x; θ) is a vector of size |Y| and (f(x; θ))y denotes it y’th entry. By defining ey as
a vector of zeros with the y’th entry equal to 1, we get that
log (P (y|f(x; θ))) = f T (x; θ)ey −log


|Y|
X
˜y=1
exp

f T (x; θ)e˜y
	


which corresponds to an exponential family defined over a discrete alphabet of |Y| letters.
Here, the natural statistics are t(y) = ey and the natural parameters are f(x; θ).
13

2. Let
log (P (y|f(x; θ))) = −(y −f(x; θ))2
2
= f(x; θ) · y −y2
2 −(f(x; θ))2
2
,
which corresponds to the squared loss function used in regression problems 3. This cor-
responds to a Gaussian distribution, which belongs to an exponential family with natural
statistics y and natural parameters f(x; θ).
D
Fisher Information Matrix for Exponential Families
Using the fact that the distribution P (y|f(x; θ)) belongs to an exponential family, namely
log (P (y|f(x; θ))) = f T (x; θ)t(y) −log

X
˜y∈Y
exp

f T (x; θ)t(˜y)
	

+ β(y),
we can directly evaluate the terms Ey∼Py|x=x;θ
h
∇f log (P (y|f(x; θ))) ∇T
f log (P (y|f(x; θ)))
i
and
Ey∼Py|x=x;θ
h
−∇2
f log (P (y|f(x; θ)))
i
to establish the desired equality.
First, we find that:
∇f log (P (y|f(x; θ))) = ∇f

f T (x; θ)t(y) −log

X
y∈Y
exp

f T (x; θ)t(y)
	




= t(y) −Ey∼Py|x=x;θ [t(y)] ,
thus,
Ey∼Py|x=x;θ

∇f log (P (y|f(x; θ))) ∇T
f log (P (y|f(x; θ)))

= Ey∼Py|x=x;θ
h t(y) −Ey∼Py|x=x;θ [t(y)]
  t(y) −Ey∼Py|x=x;θ [t(y)]
T i
.
Next, we observe that:
−∇2
f log (P (y|f(x; θ))) = ∇f
 P
y∈Y t(y) exp

f T (x; θ)t(y)
	
P
y∈Y exp {f T (x; θ)t(y)}
!
= Ey∼Py|x=x;θ

t(y)tT (y)

−
 Ey∼Py|x=x;θ [t(y)]
  Ey∼Py|x=x;θ [t(y)]
T
= Ey∼Py|x=x;θ
h t(y) −Ey∼Py|x=x;θ [t(y)]
  t(y) −Ey∼Py|x=x;θ [t(y)]
T i
.
Moreover, we note that this final result holds for any y. This concludes the proof.
E
Proof of Th. 1 in the Differentiable Case
We give the proof for the case where π(·) is a differentiable function. The proof for the non-
differentiable case is given in App. F.
Proof. Recall that we have defined the loss function as
L(S, θ, λ) ≜1
|S|
X
(x,y)∈S
−log (P (y|f(x; θ))) + λπ(θ)
3In that case, the formulation is similar to (6) where Y = R and the summation becomes an integral
14

and we assume that P (y|f(x; θ)) belongs to anexponential family, whose natural parameters are the
features f(x; θ), namely, log (P (y|f(x; θ))) = f T (x; θ)t(y) −log
P
˜y∈Y exp

f T (x; θ)t(˜y)
	
+
β(y) for some natural statistics t(y). For this model, we have
∇θ log (P (y|f(x; θ))) = ∇θf(x; θ)∇f log (P (y|f(x; θ)))
and thus, the approximated FIM is given by
F(S, θ) = 1
|S|
X
x∈S
Ey∼Py|x=x;θ

∇θf(x; θ)∇f log (P (y|f(x; θ))) ∇T
f log (P (y|f(x; θ))) ∇T
θ f(x; θ)

= 1
|S|
X
x∈S
∇θf(x; θ)Ey∼Py|x=x;θ

−∇2
f log (P (y|f(x; θ)))

∇T
θ f(x; θ)
(7a)
= −1
|S|
X
(x,y)∈S
∇θf(x; θ)∇2
f log (P (y|f(x; θ))) ∇T
θ f(x; θ)
where (7a) is by using classical properties of the exponential family, and where the last equality is
since the Hessian of an exponential family with respect to the natural parameters f is independent of
y (see App. D). Moreover, we note that the Hessian of the loss is given by
H(S, θ, λ) = ∇2
θL(S, θ, λ) = 1
|S|
X
(x,y)∈S
∇2
θf(x; θ)∇f log (P (y|f(x; θ))) + F(S; θ) + λ∇2π(θ)
We start by defining the next functions
ψ1 = ˜L(S\U, θ, λ),
(8)
ψ2 = ∇T ˜L

S\U, ˆθS, λ
 
ˆθS −θ

+

ˆθS −θ
T
∇2 ˜L

S\U, ˆθS, λ
 
ˆθS −θ

,
ψ3 = ∇T ˜L

S\U, ˆθS, λ
 
ˆθS −θ

+

ˆθS −θ
T 
F

S, ˆθS

+ λ∇2π

ˆθS
 
ˆθS −θ

,
where ˜L (S\U, θ, λ) ≜|S\U|
|S| ·L (S\U, θ, λ) and we note that the minimizer of ψ1 is ˆθS\U. Moreover,
we define the approximated solution
˘θS\U ≜ˆθS −

F

S\U, ˆθS

+ λ∇2π

ˆθS
−1
∇˜L

S\U, ˆθS, λ

which we note is further the minimizer of ψ3 and, upon the regularization term, amounts to an
NGD step. We first note that by the convexity and differentiability assumptions on the loss function,
∇L

S, ˆθS, λ

= 0. Thus, using the structure of the loss function, we get that ∇L

S\U, ˆθS, λ

=
−∇L

U, ˆθS, λ = 0

and ˘θS\U is ˜θS\U of Alg. 1. Thus, we will continue with ˜θS\U. We note that
Assumption 1.b guarantees that the overall loss, L, is µ-strongly convex. Thus, [2, Lem. 1, (6a)]
continue to hold under our assumptions, namely
ˆθS −ˆθS\U
 ≤|U| C
|S| µ = mC
nµ
(9)
Using the optimizer comparison lemma [47, Lem. 1] and Cauchy-Schwartz inequality, we get that
µ
2
ˆθS\U −˜θS\U

2
2 ≤

ˆθS\U −˜θS\U
T 
∇(ψ3 −ψ1)

ˆθS\U

≤
ˆθS\U −˜θS\U



∇(ψ3 −ψ1)

ˆθS\U

15

We divide both sides by
ˆθS\U −˜θS\U
, and by using Cauchy-Schwartz again we get
µ
2
ˆθS\U −˜θS\U
 ≤


∇(ψ3 −ψ1)

ˆθS\U

(10)
≤


∇(ψ3 −ψ2)

ˆθS\U
 +


∇(ψ2 −ψ1)

ˆθS\U

≤
∇2 ˜L

S\U, ˆθS, λ

−F

S, ˆθS

−λ∇2π

ˆθS

ˆθS −ˆθS\U

+


∇(ψ2 −ψ1)

ˆθS\U

≤mC
nµ ·

1
|S|

X
(x,y)∈U
∇θf

x; ˆθS

∇2
f log

P

y
f

x; ˆθS

∇T
θ f

x; ˆθS

+
X
(x,y)∈S\U
∇2
θf

x; ˆθS

∇f log

P

y
f

x; ˆθS
 
+ Mm2C2
2µ2n2
where the third term was calculated similarly to the proof in [2, Lem. 1] (implied by the M-smoothness
of the gradient of L, see also [46, Lem. 1.2.3]). We further use the triangle inequality to get the next
upper bound
µ
2
ˆθS\U −˜θS\U
 ≤
mC
nµ |S|
X
(x,y)∈U
∇θf(x; θ)∇2
f log (P (y|f(x; θ))) ∇T
θ f(x; θ)

+
mC
nµ |S|
X
(x,y)∈S\U
∇2
θf

x; ˆθS

∇f log

P

y
f

x; ˆθS

+ Mm2C2
2µ2n2
We note that when the loss is minimized exactly, we have P (y|f(x; θ)) = 1 4. Thus, following the
notation of App. D we have that t(y) = Ey∼Py|x=x;θ [t(y)] and since ∇f log

P

y
f

x; ˆθS

=
t(y) −Ey∼Py|x=x;θ [t(y)] we get that the middle term is zero. Thus, by the boundedness assumption
on the Hessian, we get that
ˆθS\U −˜θS\U
 ≤2mCQ
µ2n2
X
(x,y)∈U
∇θf

x; ˆθS

2
+ Mm2C2
µ3n2
(11)
Using the assumption on the boundedness of the gradient of the features f(x; θ) and by using the
Gaussian mechanism for DP [16, App. A] (see also [2]) the proof is done.
We now demonstrate how the assumptions on the loss minimization and the Hessian boundedness
follow in many popular scenarios in machine learning. Specifically, we look at regression tasks
(where we aim at minimizing the Mean-Squared Error (MSE) between the label y and the predictor
f(x; θ)) and classification tasks (where we try to match f(x; θ) to the label y), whose probabilistic
models were specified in App. C.
Regression: We note that ∇f log (P (y|f(x; θ))) = (y −f(x; θ)), and thus we get that
X
(x,y)∈S\U
∇2
θf(x; θ)
2
2 ∥∇f log (P (y|f(x; θ)))∥2
2 =
X
(x,y)∈S\U
∇2
θf(x; θ)
2
2 (y −f(x; θ))2
4In the continuous case, this amounts to P (y|f(x; θ)) converging to a delta-function, concentrated around
the value y
16

For θ = ˆθS, we recognize this term as a weighted version of the training loss over the examples in
S\U. Thus, whenever the training error is zero, this term vanishes. Moreover, we note that in this
case −∇2
f log (P (y|f(x; θ))) = Id and thus is bounded as desired.
Classification: In this case,
∇f log (P (y|f(x; θ))) = ∇f
 
(f(x; θ))y −log
 X
t∈Y
exp {(f(x; θ))t}
!!
= ey −
exp {f(x; θ)}
P
t∈Y exp {(f(x; θ))t}
(12)
where exp {f(x; θ)} refers to applying exp {·} to the vector f(x; θ) element-wise and ey denotes
a unit vector of size |Y| whose all entries are 0 except from the entry that corresponds to the letter
y, which is 1. However, since for θ = ˆθS the second term of (12) is the vector of probabilities
the model assigns to each label, it would be zero whenever the training loss is zero, namely, when-
ever our model predicts probability 1 to the correct label and zero to the others. Thus, (12) will
be zero whenever our model achieves a zero training loss. Moreover, we note that in this case
∇2
f log (P (y|f(x; θ))) = −∇f (softmax(f(x; θ))). Then, we note that the diagonal elements of
this matrix are given by softmax(f(x; θ))i −(softmax(f(x; θ))i)2 and the off-diagonal elements are
given by softmax(f(x; θ))i · softmax(f(x; θ))j. Thus, since the softmax elements are less than 1,
each entry of this matrix is bounded in the range [0, 1], and the overall matrix is bounded as desired.
F
Proof of Th. 1 in the Non-Differentiable Case
The proof follows similarly to the proof from App. E (and similarly to [47, Thm. 11]), by changing
the definition of the functions ψ1, ψ2 and ψ3 from (8) to
ψ1 ≜2˜L (S\U, θ, λ) = 2˜L (S\U, θ, λ = 0) + 2λπ(θ),
ψ2 ≜2∇T ˜L

S\U, ˆθS, λ = 0
 
ˆθS −θ

+

ˆθS −θ
T
∇2 ˜L

S\U, ˆθS, λ = 0
 
ˆθS −θ

+ 2λπ(θ),
ψ3 ≜2∇T ˜L

S\U, ˆθS, λ = 0
 
ˆθS −θ

+

ˆθS −θ
T
F

ˆθS −θ

+ 2λπ(θ)
=

θ −

ˆθS + F−1∇˜L

S\U, ˆθS, λ = 0
T
F

θ −

ˆθS + F−1∇˜L

S\U, ˆθS, λ = 0

+ 2λπ(θ) + G
where G is a constant (which is independent of θ), we use the abbreviated notation F for F

S\U, ˆθS

and where we note that the minimizer of ψ3 equals to ˜θS\U from (4). Then, the strong convexity of
the overall loss function (implied by the strong convexity of the elements ℓ(z, θ) and the convexity of
π(θ)) implies that (in a similar way to (10))
µ
2
ˆθS\U −˜θS\U

2
≤


∇(ψ3 −ψ2)

ˆθS\U
 +


∇(ψ2 −ψ1)

ˆθS\U
 ,
where the gradient of the difference is defined even if π(θ) is not differentiable, as it canceled out
after the subtraction. Then, the proof follows similarly to App. E.
G
Proof of Generalization Guarantees
The proof follows similarly to [1, Lem. 9] and [2, Thm. 1]. Let ˆθS ≜ˆθS(λ). We first re-write the
difference between the population risks as
L
 Pz, ¯θS\U, λ

−L (Pz, θ∗(λ), λ)
= L
 Pz, ¯θS\U, λ

−L

Pz, ˆθS, λ

+ L

Pz, ˆθS, λ

−L (Pz, θ∗(λ), λ)
17

By using the Lipshitzness and strong convexity of the loss function, the second difference can be
upper bounded by (see [48, Claim 6.2], [1, Thm. 3])
L

Pz, ˆθS, λ

−L (Pz, θ∗(λ), λ) ≤4C2
µn
Moreover, by the Lipschitzness of the loss function, we get that
L
 Pz, ¯θS\U, λ

−L

Pz, ˆθS, λ

≤CE
h¯θS\U −ˆθS

i
≤CE
h¯θS\U −ˆθS\U

i
+ CE
hˆθS\U −ˆθS

i
(13a)
≤C

E
h˜θS\U −ˆθS\U

i
+ E [∥v∥]

+ mC2
µn
(13b)
≤
C˜γ
n2

·
 
1 + 1
ϵ ·
s
2 log
1.25
δ

· Γ
  d+1
2

Γ
  d
2

!
+ mC2
µn
(13c)
where (13a) is by the triangle inequality, (13b) is by using (9) and using again the triangle inequality,
and (13c) is by (11) and since ∥v∥is a χ2 variable with d-degrees of freedom, ˜γ was defined in
Alg. 1 and Γ(x) ≜
R ∞
0
tx−1e−tdt, x ≥1 is the Γ-function. The proof follows by using the fact that
m = o(n) and
Γ( d+1
2 )
Γ( d
2)
= O(
√
d).
H
Experiments Details
All experiments were implemented using the PyTorch [49] framework, and we ran all experiments on
NVIDIA Geforce RTX 3090 GPU. The datasets and models used in our experiments are detailed
below.
H.1
Datasets
We utilized the MNIST [50] and CIFAR10 [41] datasets, as provided by the torchvision pack-
age in PyTorch. During training and for our unlearning simulations, the data was shuffled. We
trained the models without data augmentation to stay aligned with previous unlearning works. The
CIFAR10 dataset was pre-processed using the next two main steps: first, we converted the images to
tensors using the transforms.ToTensor() method. Next, the images were normalized using the
transforms.Normalize() method. The normalization process adjusts the image data so that the
pixel values have a mean of 0.4914, 0.4822, and 0.4465 and a standard deviation of 0.2023, 0.1994,
and 0.2010 for the red, green, and blue channels, respectively. The MNIST dataset was pre-processed
using a similar pipeline, where we normalized the mean and the standard deviation to 0.5.
H.2
Neural Networks Architectures
All models were trained using a cross-entropy loss.
One-Layer Perceptron (OLP): We trained a one-layer perceptron to classify the MNIST dataset.
The classifier has one hidden layer with 200 neurons and a ReLU activation function. It was trained
for 100 epochs using the Adam optimizer, with a learning rate of 0.02, β1 = 0.9, β2 = 0.99,
ϵ = 10−8, a weight decay of 10−4, and a batch size of 256.
Multi-Layer Perceptron (MLP): Similar to the OLP architecture, the MLP was trained to classify
the MNIST dataset. This model comprises three fully connected layers with widths of 784 × 784,
784 × 256, and 256 × 10, with ReLU activation between the layers. The classifier was trained using
SGD for 100 epochs with a learning rate of 0.01, a weight decay of 10−4, and a batch size of 128.
The learning rate was reduced by a factor of 10 at epoch 50.
Convolutional Neural Network (CNN): The CNN architecture was designed to classify the CI-
FAR10 dataset. It comprises convolutional layers followed by fully connected layers. The convo-
lutional part includes two convolutional layers with 16 and 32 output channels, respectively, and
18

3 × 3 kernels with padding of 1. A 2 × 2 Max-pooling layer is placed after each convolutional layer.
The fully connected part includes two layers with widths of 2048 × 128 and 128 × 10, with a tanh
activation between them. The network was trained for 100 epochs using SGD with a learning rate of
0.01, a weight decay of 10−4, and a batch size of 128. We reduced the learning rate by 10 whenever
the training loss changes by less than 0.00001 for five consecutive epochs.
ResNet18: We used PyTorch’s pre-trained version of ResNet18, initially trained on the ImageNet
dataset. A fully connected layer of size 1000 × 10 was added, and the network was fine-tuned on the
CIFAR10 dataset for 50 epochs using SGD with a learning rate of 0.01 and a weight decay of 10−4.
The model was trained with a batch size of 128. We reduced the learning rate by 10 whenever the
training loss changes by less than 0.00001 for five consecutive epochs.
H.3
Unlearning Algorithms
This section details the implementation of our unlearning algorithms, Fine-Tuning, Scrub, and
Min/Max NGD.
Fine-tunning: The fine-tunning algorithm fine-tunes the model on the retain set. In our imple-
mentation, we use batches of size 128, weight decay of 5 · 10−4, momentum of 0.9, and an initial
learning rate of 0.05, reduced by a factor of 5 after each epoch. We have used two different op-
timizers: the first is the vanilla SGD optimizer of Pytorch, and the second aims to approximate
an NGD according to the algorithm from [51]. Our implementation is based on the one presented
in https://github.com/YiwenShaoStephen/NGD-SGD/tree/master, with the update-period
parameter set to 4.
Scrub: Our implementation of the Scrub algorithm follows the procedure detailed in [9], based on the
code provided at https://github.com/meghdadk/SCRUB. We set our hyperparameters similarly
to those in the original paper [9]: we have used an Adam optimizer with an initial learning rate of
0.0005, reduced by a factor of 10 after each epoch, a weight-decay of 0.1, a smoothing factor of
0.5 and the parameter α of the Scrub algorithm (which weights the KL-term and the loss term) was
set to 0.5. We used batches of 64 for the retain-set and of size 16 for the forget set. The rest of the
hyperparameters were chosen similarly to those of [9].
Min/Max NGD: The Min/Max NGD algorithm (Alg. 2) is implemented using a Min/Max training
procedure similar to that in the Scrub implementation. Smoothing is performed using PyTorch’s
built-in function torch.optim.swa utils.AveragedModel. The NGD was implemented based
on the method from [51], with code from https://github.com/YiwenShaoStephen/NGD-SGD/
tree/master. The learning rate is 0.0007 for the CNN example and 0.0005 for the ResNet example.
We reduced the learning rate by a factor of 10 after 4 epochs. We set the momentum to 0.9, the
weight-decay to 0.1, and the update period of the NGD to 4. The smoothing parameter (β of Alg. 2)
was set to 0.4. We used batches of size 128 for the retain-set and 32 for the forget set.
I
Computing the Newton and the NGD Steps Using LiSSA
In this section, we review the Linear time Stochastic Second-Order Algorithm (LiSSA) algorithm,
used for efficient implementation of second-order optimization methods [30] and which we used to
demonstrate the computational efficiency of the NGD relative to a Newton step.
I.1
The LiSSA Algorithm
The LiSSA algorithm approximates the result of a matrix inverse times a vector by using a truncated
Neumann series. Following our ERM framework, let A(S) be a positive-definite matrix of the form
A(S) ≜P
z∈S A(z) for some matrix A(z) that depends on the training point z. Using Neumann
expansion, we can approximate its inverse via the next summation
(A (S))−1 = σ−1 ·
T
X
t=0
 Id −σ−1 · (A (S))
t
(14)
where the approximation becomes exact as T →∞and where σ > 0 is a scaling hyperparameter
chosen sufficiently large to ensure convergence of the series. We note that (14) can be used to
19

calculate arguments of the form (A(S))−1 x for some vector x via the next recursive relations
v0 = x
vt = vt−1 −σ−1 · (A (S)) vt−1, t = 1, . . . , T
where ˆvT = σ−1 · vT contains the approximated result, i.e. ˆvT ∼= (A(S))−1 x.
In practice, since we can not calculate A(S) (because we can not evaluate the whole training set), we
sample a batch of data points and calculate A on this batch. Then, the overall calculation is repeated
R different times and averaged to yield the next estimate:
(A (S))−1 x ∼= 1
R
R
X
r=1
σ−1 · v(r)
T
where v(r)
T
is generated via the recursion
v(r)
0
= x,
v(r)
t
= v(r)
t−1 −σ−1 · (A (Bt,r)) v(r)
t−1, t = 1, . . . , T
and Bt,r corresponds to a batch of B ≥1 data points that were sampled to calculate A in the
(t, r) iteration. To calculate the NGD and the Newton steps, we replace the matrix A(z) with
Ey∼Py|x=x;θ

∇θ log
 Py|x;θ
 y|x; θ(t)
∇T
θ log
 Py|x;θ
 y|x; θ(t)
and with ∇2
θ log
 Py|x;θ(y|x; θ)

,
respectively. Moreover, since the LiSSA requires the matrix A(S) to be positive-definite, we must
add a regularization term to ensure this positive definiteness. Thus, usually we replace any A(z) with
A(z) + λId.
20

