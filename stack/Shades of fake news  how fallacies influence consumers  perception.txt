Full Terms & Conditions of access and use can be found at
https://www.tandfonline.com/action/journalInformation?journalCode=tjis20
European Journal of Information Systems
ISSN: (Print) (Online) Journal homepage: www.tandfonline.com/journals/tjis20
Shades of fake news: how fallacies influence
consumers’ perception
Sven Beisecker, Christian Schlereth & Sebastian Hein
To cite this article: Sven Beisecker, Christian Schlereth & Sebastian Hein (2024) Shades of
fake news: how fallacies influence consumers’ perception, European Journal of Information
Systems, 33:1, 41-60, DOI: 10.1080/0960085X.2022.2110000
To link to this article:  https://doi.org/10.1080/0960085X.2022.2110000
Published online: 11 Aug 2022.
Submit your article to this journal 
Article views: 1113
View related articles 
View Crossmark data
Citing articles: 2 View citing articles 

RESEARCH ARTICLE
Shades of fake news: how fallacies influence consumers’ perception
Sven Beisecker, Christian Schlereth
and Sebastian Hein
Chair of Digital Marketing, WHU-Otto Beisheim School of Management, Vallendar, Germany
ABSTRACT
So far, fake news has been mostly associated with fabricated content that intends to manip­
ulate or shape opinions. In this manuscript, we aim to establish that the perception of 
information as fake news is influenced by not only fabricated content but also by the rhetorical 
device used (i.e., how news authors phrase the message). Based on argumentation theory, we 
advance that fallacies – a subset of well-known deceptive rhetorical devices – share 
a conceptual overlap with fake news and are therefore suitable for shedding light on the 
issue’s grey areas. In a first two-by-two, between-subject, best-worst scaling experiment 
(case 1), we empirically test whether fallacies are related to the perception of information as 
fake news and to what extent a reader can identify them. In a second two-by-two experiment, 
we presume that a reader believes that some of a sender’s messages contain fake news and 
investigate recipients’ subsequent reactions. We find that users distinguish nuances based on 
the applied fallacies; however, they will not immediately recognise some fallacies as fake news 
while overemphasising others. Regarding users’ reactions, we observe a more severe reaction 
when the message identified as fake news comes from a company instead of an acquaintance.
ARTICLE HISTORY 
Received 27 December 2020  
Accepted 27 July 2022 
KEYWORDS 
Fake news; best-worst 
scaling; fallacies; social 
media; argumentation 
theory; rhetorical devices
1. Introduction
“Pope Francis Shocks World, Endorses Donald 
Trump for President”. This headline turned out to be 
what is commonly classified as fake news; still, it 
generated 960,000 engagements on Facebook 
(Silverman, 2016). Rather than being an outlier, this 
anecdote is emblematic of our current social media 
landscape, in which a broad audience creates, shares, 
and consumes false information without further 
reflection (Vosoughi et al., 2018).
The term fake news has been gaining relevance due 
to its far-reaching implications for society, most 
recently during the COVID-19 pandemic. Fake news 
on topics like the safety of vaccinations and political 
measures to contain the spread of the virus can influ­
ence individuals’ adoption behaviour, with downstream 
consequences for national and global health (Laato 
et al., 2020). For these reasons, policymakers around 
the world have taken steps to limit the dissemination. 
For instance, Germany’s parliamentary body passed the 
Network Enforcement Act, which requires social media 
platforms to remove fake news, hate speech, and other 
unlawful content within one day after notification, or 
else face fines of up to €50 million (Bundestag, 2017). 
However, apart from the dissemination of unlawful 
content such as propaganda material and insults, the 
law does not specify what exactly fake news is. In policy­
makers’ defence, scholars also lack a clear definition of 
the concept and have mostly focused on the facticity of 
news content (Tandoc et al., 2018).
Kim et al. (2019) defined fake news as “news [. . .] 
that are intentionally and verifiably false and could 
mislead readers” (p. 934). Here, we emphasise the 
quality of being misled, which can encompass the 
use of not only fabricated content, but also certain 
rhetorical devices. We focus on one prominent subset 
of deceptive rhetorical devices: namely, fallacies, 
which have a well-documented ability to misinform 
and shape opinions. We argue that, because of their 
deceptive nature, fallacies share a conceptual overlap 
with fake news.
The goal of our research is to examine this overlap in 
the perception between fake news and fallacies. In two 
hypotheses, we draw on argumentation theory and 
advance that the rhetorical device shapes the perception 
of information as fake news to a certain, device-specific 
degree, independent of the content. We further argue 
that not all fallacies are equally detectable: Some require 
more deliberate consideration than others.
As a second goal, we follow up on the idea that 
readers of news statements are sceptical that some 
information from a specific sender is fake news. We 
study how the users react, i.e., to what extent their 
reaction is directed towards the platform that distri­
butes the information in question or towards the sen­
der, and how this reaction is linked to observable 
characteristics of the reader and the sender. We thereby 
shed light on the repercussions that may accrue for 
individuals, companies, and platforms that share or 
distribute fake news. The second goal deviates from 
CONTACT Christian Schlereth 
christian.schlereth@whu.edu
EUROPEAN JOURNAL OF INFORMATION SYSTEMS 
2024, VOL. 33, NO. 1, 41–60 
https://doi.org/10.1080/0960085X.2022.2110000
© The Operational Research Society 2022. 

previous research on user reactions, which has mainly 
examined why some readers share information even 
though they likely perceived it as fake (Kim & Dennis,  
2019; Kim et al., 2019; Pennycook & Rand, 2021). In 
contrast to previous research, we focus on the sub- 
population that is sceptical that a post contains fake 
news and study its reaction.
Empirically, we examine the two hypotheses by 
employing best-worst scaling in a two-by-two, between- 
subject experiment. Thereby, we compare respondents’ 
perceptions of statements as fake news when a) evaluated 
with or without explanations of the fallacies and b) when 
applied to different contexts (in this case, a political and 
a business context). To study their reactions, we used 
another two-by-two, between-subject experiment, in 
which we varied the type of sender (company vs. 
acquaintance) and the share of information perceived 
as fake news in posts. Here, respondents stated how likely 
they are to express different reactions against the sender 
and the platform that distributed the information.
The insights from this research can be used in mani­
fold ways. First, if the perception of information as fake 
news relates to not only the facticity of content, but also 
the rhetorical device used, then fake news detection 
algorithms could be extended to automatically identify 
those fallacies that are highly related to fake news per­
ceptions. Indeed, there are already initial NLP-based 
machine learning algorithms being developed to detect 
some fallacies, such as ad hominem (Delobelle et al.,  
2019; Li et al., 2021). Concurrently, the field of argu­
mentation mining is researching methods for analysing 
people’s reasoning (Habernal & Gurevych, 2017). 
Second, because some other fallacies, such as formal 
logical errors, are difficult to detect automatically 
(Delobelle et al., 2019), social media platforms may 
strive to educate readers to help them manually detect 
fake news. Third, by studying user reactions, we outline 
who has more to lose when readers perceive certain 
messages as fake news (companies or individuals). 
Finally, we challenge the common wisdom that fake 
news is primarily related to politics (Bronstein et al.,  
2019; Faragó et al., 2020). Instead, and in line with, e.g., 
Visentin et al. (2019), we show that the perception of 
information as fake news can be independent of the 
context, as well as have severe business implications.
We first review the literature related to fake news and 
people’s reactions upon detecting fake news. Afterwards, 
we derive the hypotheses for our study and introduce the 
setup before reporting the results and conclusions.
2. Related work on fake news
2.1. Operationalisation
Even though the search term “fake news” peaked in 
early 2020 on Google Trends (Google Trends, 2021), 
the underlying concept has a long history that goes far 
beyond the recently surging interest. One of the ear­
liest known examples of fake news dates back to the 
Middle Ages, when the duke of Austria, Rudolf IV, 
falsely claimed that his lineage, the Habsburgs, had 
received an imperial certificate called “privilegium 
maius” that would grant them a vote in the election 
of the emperor (ZDF, 2021). Since then, researchers 
have studied this multi-faceted issue from various 
historical angles (Weiss et al. (2020). For instance, 
Weiss et al. (2020) noted that rumours are one expres­
sion of fake news, i.e., unintentional information dis­
tortions that occur out of ignorance and are repeated 
by different people over time – and, thus, are difficult 
to control (Shibutani, 1966). According to Allport and 
Postman (1947), rumours are formed through level­
ling (conveyed information is shortened over time), 
sharpening (selective reporting of limited details), and 
assimilation (information becomes consistent with the 
person’s prior views). The accuracy of a rumour 
decreases the more their spreaders are subject to nar­
rowed attention, limited memory, and perceptual 
biases (Buckner, 1965; DiFonzo & Bordia, 2007; 
Knapp, 1944).
While the issues that encourage rumours are largely 
baked into human psychology, they may be exacer­
bated by recent developments. In the present paper, 
we follow the belief of authors like Egelhofer and 
Lecheler (2019), Laato et al. (2020), and Moravec 
et al. (2019) in arguing that fake news is strongly 
connected to the rise of social media. With many 
users able to share and interact with news on social 
media, information generally spreads faster and often 
without users reflecting on the credibility of the source 
compared to traditional media.
Because modern communication technology 
enhances people’s tendency to deceive and lie 
(Hancock, 2007), the advancement of social media 
makes it increasingly difficult to distinguish profes­
sional from unprofessional content (Wineburg et al.,  
2016). Likewise, properly fact-checking digital infor­
mation gets harder with an increasing amount of 
information (Lecheler & Kruikemeier, 2016). Based 
on 4.5 million tweets, Vosoughi et al. (2018) con­
cluded that fake news is more likely to go viral and 
spread much faster than accurate news. Against this 
background, researchers have begun to study fake 
news from different angles – particularly its creators 
and their motivations, its recipients, and the mechan­
isms that foster or prevent dissemination. Without 
claiming completeness, Table 1 summarises the extant 
literature.
With the present research, we aim to investigate 
whether additional factors play a role in defining 
fake news. We contribute to this ongoing literature 
stream by studying the perception of information as 
fake news beyond the facticity of content. Thereby, we 
focus on news headlines and how the framing of an 
42
S. BEISECKER ET AL.

Table 1. Extant literature on fake news.
Study
Fake News Definition
Binary?
Methodology
Main Contribution
Pennycook and Rand 
(2021)
False or highly misleading 
political ‘news’ stories, primarily originating on social 
media
Yes
Literature 
review
Lacking discernment between true and false 
information is related to lack of careful 
reasoning
Schuetz, Sykes, and 
Venkatesh (2021)
Intentionally and verifiably false news
Yes
Survey
Awareness of fake news and commenting on 
articles instead of reading them is 
positively related to fact checking
Jones-Jang et al. 
(2021)
Fabricated information that mimics news media 
content in form but not in organisational process 
or intent
Yes
Survey
Information literacy aids identification of fake 
news
Alonso et al. (2021)
Provably false pieces of information created with the 
intention of deception
Yes
Literature 
review
Fake news detection through sentiment 
analysis
Brasoveanu and 
Andonie (2021)
News items with verifiably false content
Yes
Machine 
learning 
models
Semantic fake news detection method based 
on features like sentiment and factual 
accuracy
Mazzeo et al. (2021)
-
Yes
Machine 
learning 
models
Fake news identification on web search 
engines based on structural features of 
URLs
Bryanov et al. (2021)
Non-factual messages resembling legitimate news 
content and created with an intention to deceive
Yes
Literature 
review
Belief in fake news depends on message 
characteristics and individual factors
Pennycook et al. 
(2020)
Information that mimics the output of the news 
media in form, but not in organisational process or 
intent; a subgenre of the broader category of 
misinformation – of incorrect information about 
the state of the world.
Yes
Survey
Tendency to ascribe profundity to randomly 
generated sentences is positively related 
to perceived fake news accuracy and 
negatively to the ability to differentiate 
between fake and real news
Moravec et al. (2020)
News articles that are intentionally and verifiably 
false and could mislead readers
Yes
Experiment
Fake news flags relying on both System 1 and 
System 2 processing are most effective
Bago, Rand, and 
Pennycook (2020)
Fabricated information that mimics news media 
content in form but not in organisational process 
or intent
Yes
Experiment
Deliberation reduces individuals’ beliefs in 
false headlines
Martel and 
Pennycook (2020)
Fabricated information that mimics news media 
content in form but not in organisational process 
or intent
Yes
Exploratory 
study, 
experiment
Reliance on emotion increases belief in fake 
news
Pennycook et al. 
(2020)
Fabricated stories presented as if from 
legitimate sources
Yes
Experiment
False headlines that fail to get tagged are 
seen as more accurate (implied truth 
effect)
Weiss et al. (2020)
No uniform definition, but several components: 
principle of least effort, poisoned public discourse, 
logical fallacies, overconfidence, post-truth, 
propaganda, rumour, misinformation, conspiracy 
theory, parody, satire, political kayfabe
No
Survey
Conceptions of fake news differ among 
university faculty members
Laato et al. (2020)
False or inaccurate information, especially that which 
is deliberately intended to deceive
No
Survey
Information overload and trust in online 
information are strong predictors of users’ 
tendency to share COVID-19 related fake 
news
Zhou and Zafarani 
(2020)
Intentionally false news published by a news outlet
Yes
Conceptual
News can be detected based on false 
information, writing style, propagation 
patterns, and source credibility
Altay et al. (2020)
Fabricated information that mimics news media 
content in form but not in organisational process 
or intent
Yes
Experiment
Sharing fake news hurts sharer’s reputation 
and readers’ trust in the source
Kim et al. (2019)
News articles that are intentionally and verifiably 
false and could mislead readers
Yes
Experiment
Source ratings influence users’ belief in an 
article, particularly for low expert and user 
article ratings
Kim and Dennis 
(2019)
News articles that are intentionally and verifiably 
false and could mislead readers
Yes
Experiment
Highlighting the source of an article makes 
users more sceptical, independent of 
source credibility; low source ratings affect 
believability when source is unknown
Moravec et al. (2019)
Misinformation on social media
Yes
Behavioural 
and EEG data
Presence of a fake news flag increases users’ 
time spent on news headlines but does 
not affect truth assessments
Bronstein et al. (2019) Fabricated news stories that are presented as being 
from legitimate sources and promoted on social 
media to deceive the public for ideological or 
financial gain
Yes
Survey
Delusion-prone individuals are more likely to 
believe fake news headlines
Egelhofer and 
Lecheler (2019)
All news which is “inaccurate”
Yes
Conceptual
Fake news consists of two dimensions, fake 
news genre (deliberate creation of 
disinformation) and fake news label (use of 
the term to discredit media sources)
Pennycook and Rand, 
D (2018)
Fabricated information that mimics news 
media content in form but not in organisational 
process or intent
Yes
Correlational 
study
Propensity to engage in analytical reasoning 
is negatively associated with perceived 
accuracy of fake news and positively with 
ability to discern fake news from real news
Pennycook et al. 
(2018)
News stories that were fabricated (but presented as if 
from legitimate sources) and promoted on social 
media to deceive the public for ideological and/or 
financial gain
Yes
Experiment
Exposure to fake news headlines increases 
subsequent perceptions of accuracy 
(illusory truth effect)
(Continued)
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
43

argument impacts readers’ perceptions. We follow the 
work of Shibutani (1966), who, in the context of 
rumours, stated that how something is said matters 
just as much as what is said. To this end, we dwell on 
a subset of well-studied rhetorical devices called falla­
cies (Dowden, 1993; Van Eemeren et al., 2009). 
Although these devices have been extensively studied, 
we are not aware of any empirical work that links the 
different types of fallacies to the perception of infor­
mation as fake news. While a few extant studies have 
discussed certain individual fallacies, none have per­
formed a comparison. For instance, Van Eemeren 
et al. (2009) found that respondents consider fallacies 
to be less reasonable than sound statements; however, 
focus solely on the ad hominem fallacy, i.e., attacking 
the arguer instead of the argument. Another study 
relates to the false dilemma fallacy, i.e., framing 
a situation as having only two options when there 
are in fact more. Brisson et al. (2018) found that an 
individual’s tendency to fall for this fallacy depends on 
their background knowledge and their ability to 
retrieve options from memory other than the ones 
presented.
A second deviation from the literature is the treat­
ment of fake news as a multi-level construct. 
Although most empirical studies do not explicitly 
make this conceptual distinction (e.g., Allcott & 
Gentzkow, 2017; Vosoughi et al., 2018), they do 
operationalise the fake news construct solely in bin­
ary terms (i.e., as either true or false). A binary clas­
sification simplifies the process by matching existing 
statements with fact-checking websites. However, it 
reduces fake news detection to only explicitly verifi­
able cases, allowing current detection mechanisms to 
Table 1. (Continued).
Study
Fake News Definition
Binary?
Methodology
Main Contribution
Lazer et al. (2018)
Fabricated information that mimics news media 
content in form but not in organisational process 
or intent
Yes
Conceptual
Interventions against fake news should either 
focus on empowering individuals to 
evaluate fake news or preventing their 
exposure to fake news
Berthon and Pitt 
(2018)
False information
Yes
Conceptual
Brands can be impacted by (e.g., as target of 
fake news) and actively impact (e.g., by 
deliberate or unintentional association 
with dubious content) fake news in 
different ways
Vosoughi et al. (2018) News that has been verified as false
Yes
Descriptive, 
investigation 
of rumour 
cascades
Fake news on Twitter is retweeted by more 
people, and far more rapidly than real 
news, especially for posts in a political 
context
Tandoc et al. (2018)
Several components including news satire, news 
parody, fabrication, manipulation, advertising, and 
propaganda
No
Literature 
review
Fake news can be categorised across the two 
dimensions of levels of facticity and 
deception
Allcott and Gentzkow 
(2017)
News articles that are intentionally and verifiably 
false, and could mislead readers
Yes
Web-browsing 
data, survey
Social media are important channels to 
distribute fake news and may influence 
public elections
Shu et al. (2017)
Low-quality news with intentionally false information Yes
Literature 
Review
Summary of different approaches to 
detecting fake news
Berkowitz and 
Schwartz (2016)
Content that blurs lines between non-fiction and 
fiction
No
Textual analysis Differentiation between fake news and satire
Wineburg et al. 
(2016)
-
-
Survey
Students largely fail to accurately evaluate 
the trustworthiness of news articles
Lewandowsky et al. 
(2012)
Misinformation, false beliefs
Yes
Conceptual
Continued influence effect (retractions fail to 
eliminate the influence of misinformation)
Hancock (2007)
Use of various concepts of deception and lying
Yes
Conceptual
Modern communication technologies 
facilitate deception and complicate 
deception detection
DiFonzo and Bordia 
(2007)
Rumours
-
Conceptual
Definition of rumours, psychological aspects 
of rumour spreading
Gilbert et al. (1990)
-
-
Experiment
Interruptions in information processing make 
subjects more likely to consider false 
propositions true
Begg et al. (1985)
-
-
Experiment
New details about familiar topics are rated 
truer than new details about unfamiliar 
topics
Gardner (1975)
Advertisements that leave the consumer with 
factually untrue or potentially misleading 
impressions and/or beliefs
Yes
Conceptual
Identification of a three-level typology of 
advertising deception, focused on 
consumer reaction: unconscionable lie, 
claim-fact discrepancy, and claim-belief 
interaction
Shibutani (1966)
Rumours
-
Conceptual
Definition of rumours, processes by which 
rumours form
Buckner (1965)
Rumours
-
Survey
Effect of rumour network interactions on the 
accuracy of rumour transmission
Allport and Postman 
(1947)
Rumours
-
Conceptual
Definition of rumours, processes by which 
rumours form
Knapp (1944)
Rumours
-
Conceptual
Systematisation and classification of rumours
44
S. BEISECKER ET AL.

underestimate the severity of fake news. With our 
focus on fallacies, we deviate from most previous 
research by treating fake news as non-binary (two 
exceptions with explicit non-binary classifications 
are: Tandoc et al. (2018), who characterised fake 
news by the “level of facticity” and the “author’s 
immediate intention to deceive”, as well as 
Berkowitz and Schwartz (2016), who defined fake 
news as content that “blurs lines between nonfiction 
and fiction”).
2.2. Efforts to prevent dissemination
Previous literature has proposed a portfolio of actions 
to prevent the dissemination of fake news. We struc­
ture these efforts along the following dimensions: 1.) 
detecting fake news, 2.) understanding what impacts 
believability and dissemination, and 3.) educating peo­
ple to help them avoid falling for fake news.
There are several characteristics of fake news that 
make it detectable. According to Zhou and Zafarani 
(2020), detection approaches should address false 
information, writing style, propagation patterns, and 
source credibility. The first approach studies whether 
the news content is factually flawed by either manual 
or automized fact-checking against external sources 
(Alonso et al., 2021; Shu et al., 2017). The second 
approach aims to assess whether readers are intention­
ally misled due to how the news is written or presented 
(Shu et al., 2017). Here, certain text features (such as 
swear words or emotional words) and the character­
istics of associated news images are analysed using 
sentiment analysis (Alonso et al., 2021; Brasoveanu & 
Andonie, 2021). In line with our research, this 
approach transcends the notion of factual accuracy. 
The third approach studies information diffusion pat­
terns. However, platforms and researchers can observe 
such patterns only in hindsight (Zhou & Zafarani,  
2020). The fourth approach aims to detect fake news 
by evaluating the credibility of the news source. This 
approach is preoccupied with identifying unreliable 
websites: for instance, based on structural features of 
the URL (Mazzeo et al., 2021) or malicious social 
media users such as bots (Shu et al., 2017; Zhou & 
Zafarani, 2020).
Moreover, the extant literature has identified sev­
eral factors that influence the extent to which people 
believe in fake news. First, individuals’ traits and char­
acteristics can determine how prone they are. 
Bronstein et al. (2019) found that proclivities for delu­
sion, dogmatism, and religious fundamentalism make 
individuals particularly suspectible to believing fake 
news. Second, the tendencies to engage in elaborate 
cognitive reflection (Fiske & Taylor, 2013; Pennycook 
& Rand, 2021) and analytical reflection (Pennycook & 
Rand, D, 2018) are associated with a lower belief in 
fake news. Similarly, individuals who score high on 
information literacy are more likely to correctly detect 
fake news (Jones-Jang et al., 2021). Third, people are 
impacted by the circumstances and timing of fake 
news exposure. In the study of rumours, Shibutani 
(1966) noted that people’s default stance is to impli­
citly believe in the truthfulness of anything they hear. 
They will only distance themselves from this initial 
belief if presented with considerable reason for doubt. 
However, a person’s proclivity to doubt what they hear 
or read depends on several factors: Martel and 
Pennycook (2020) found that people in an emotional 
state are particularly prone to believing fake news. 
Aspects like a person’s time constraints and current 
energy level also play a role (Wilson & Brekke, 1994), 
as do interruptions in information processing (Gilbert 
et al., 1990). Pennycook et al. (2018) determined that 
repeated exposure leads to increased accuracy judge­
ments, which they termed the illusory truth effect. 
Finally, the characteristics of the message itself can 
influence believability. In line with confirmation bias 
(Nickerson, 1998), users tend to believe news that 
aligns with their prior views (Kim & Dennis, 2019; 
Pennycook & Rand, 2021). At the same time, users are 
less likely to believe news with an incoherent story 
(Lewandowsky et al., 2012) or a dubious source 
(Begg et al., 1985), while being more likely to believe 
news with trusted endorsements (Bryanov et al., 2021).
Another strategy to prevent fake news dissemina­
tion is by introducing mechanisms to social media 
platforms that educate users about news consumption. 
Here, researchers have mainly proposed remedies that 
address fabricated content. For instance, Moravec 
et al. (2020) studied the design of fake news flags 
(tags attached to news articles containing disputed 
content) as a potential remedy. Relying on dual- 
process theory, Kahneman and Egan (2011) investi­
gated how people’s beliefs in disputed news articles are 
affected by interventions focusing on either System 1 
(automatic evaluation triggered by a stop sign icon) or 
System 2 (deliberate evaluation triggered by a text 
warning). Pennycook et al. (2020) explored the psy­
chological effects of attaching warnings or ratings to 
the article source. In a similar vein, Kim et al. (2019) 
examined three types of ratings, namely: expert (an 
expert judges the source), user source (users judge the 
source), and user article ratings (users judge individual 
articles). The authors found that expert ratings were 
the most effective in reducing the perceived believabil­
ity for low-rated articles.
In this research, we focus on the detectability of 
fake news. In line with Clarke et al. (2020), we define 
detectability as the outcome effectiveness of an 
approach that aims to identify fake news. We contri­
bute to style-based detection approaches, as we do not 
focus on detecting certain words or sentiments, but on 
the use of fallacies and how they influence the percep­
tion of information as fake news. Fallacies use true 
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
45

information in a reliable context, but exaggerate or 
modify certain elements that distract from the factual 
level (Fearnside & Holther, 1959). The detection of 
certain fallacies is becoming operationally feasible 
thanks to early NLP-based machine learning algo­
rithms (Delobelle et al., 2019; Li et al., 2021). 
Thereby, we treat fake news as a continuum between 
clearly fabricated information (i.e., alternative facts) 
and news statements containing deliberately mislead­
ing elements. This broadened view can substantially 
extend the possibilities of combatting fake news by 
allowing one to: (i) train algorithms to better detect 
fake news automatically and (ii) educate readers about 
the use of fallacies in order to sensitise them to manu­
ally detecting fake news.
2.3. Reaction after detection
Given our presumption that social media can detect 
and flag fake news messages, as well as prompt users to 
reflect on the veracity of a message, this paper’s final 
question is how readers react when they are sceptical 
that a sender is sharing such messages. Altay et al. 
(2020) demonstrated that sharing fake news has dama­
ging effects on the sharer’s reputation and on readers’ 
trust in the source, independent of whether it is 
a media outlet or an individual sharing the news. 
Ultimately, sources suffer a greater loss of trust when 
sharing fake news than they gain from sharing a real 
news story (Altay et al., 2020).
Even in situations where companies are the targets, 
rather than originators, of fake news, their reputations 
can still be sullied. In such cases, readers of fake news 
may react by changing their consumer behaviour, 
which can have severe consequences for businesses. 
Berthon and Pitt (2018) noted that consumers tend to 
dissociate from brands that have been targets of fake 
news, with negative downstream consequences for 
companies’ brand equity. Prominent examples of 
companies impacted by fake news include PepsiCo 
and New Balance (Di Domenico & Visentin, 2020). 
However, brands do not seem to suffer if their adver­
tisements appear next to fake news articles if the host 
website is generally regarded as credible (Visentin 
et al., 2019).
In this study, we explore the reaction of users 
who are suspicious that one of their contacts has 
shared fake news. Thus, compared to previous lit­
erature, we do not focus on why content is shared 
despite being perceived as fake news, nor on the 
consequences of fake news for companies’ brand 
equity. Instead, we distinguish reactions towards 
the sender and the platform that shared the con­
tent, thereby focusing on the potential damage for 
the parties involved.
3. Theoretical framework
In this section, we develop a theoretical framework to 
underpin our empirical study. We derive our under­
standing of theory from Gregor’s (2006) theoretical 
taxonomy, with a particular focus on his theory type 
III (”Theory for Predicting”). This type of theory is 
used to discover previously unknown regularities in 
order to predict outcomes from a set of explanatory 
factors, but without determining the underlying causal 
connections between the dependent and independent 
variables (Gregor, 2006). In short, parts of the system 
remain a “black box”.
A core claim of our article is that fake news is not 
defined by content alone, but also depends on the 
employed rhetorical devices. Based on this argument, 
we derive the central hypotheses below. Thereby, we 
employ Gregor’s (2006) type III understanding of the­
ory in order to demonstrate the relationship between 
rhetorical devices and the perception of information 
as fake news. In doing so, we leave out the question of 
why specific rhetorical devices are more strongly 
related to the perception of information as fake news 
than others.
3.1. Relation between fake news, rhetorical 
devices, and fallacies
We build our hypotheses on the groundwork of argu­
mentation theory, which studies the requirements for 
an argument to be correct (D. D. Walton, 2009). An 
argument refers to finding support for a conclusion 
based on one or several reasons (Dowden, 1993). In 
argumentation theory, there are certain rules for how 
an argument should be presented. Van Eemeren et al. 
(2002) produced the most prominent summary of ten 
rules for sound argumentation. For instance, one rule 
(burden-of-proof rule) states that the burden of proof 
always lies with the person who puts forth 
a standpoint. Another rule (relevance rule) establishes 
that a person can only defend their standpoint based 
on arguments relevant to said standpoint (Van 
Eemeren et al., 2002). In this sense, a related goal of 
argumentation theory is to improve argumentative 
discourse (Van Eemeren et al., 2013). This entails 
efforts to evaluate and resolve rule violations (i.e., 
errors arising in argumentation; Van Eemeren et al.,  
2013).
A fallacy occurs when an argument contains such 
errors (D. D. Walton, 2009). Fallacies represent a well- 
documented subset of rhetorical devices (Dowden,  
2019), which news authors use to shift readers’ per­
ception of information, even in cases where the infor­
mation itself is factually correct (Madon et al., 2021). 
They stand apart from other rhetorical devices: they 
46
S. BEISECKER ET AL.

contain purposely made errors in reasoning or 
unsound or illogical arguments that claim to conform 
to the rules of sound argumentation while actually 
failing to do so (Fearnside & Holther, 1959). Fallacies 
conceptually differ from falsifiable information in that 
they use true information in a reliable context, but 
exaggerate or modify certain elements to distract 
from the factual level (Fearnside & Holther, 1959). In 
other words, they are arguments that seem valid, but 
are actually invalid (Van Eemeren et al., 2010). 
Scholars have complemented this theoretical view 
with empirical evidence: A wide array of experiments – 
involving a total of more than 1,900 participants – 
indicate that the average person’s perception of rea­
sonableness largely accords with the theoretical per­
spectives on argumentation (Van Eemeren et al.,  
2010).
In the present research, we advance that fallacies (as 
a subset of rhetorical devices) overlap with the concept 
of fake news. Our position follows from Lazer et al.’s 
(2018) argument that “fake news overlaps with other 
information disorders, such as misinformation (false 
or misleading information) and disinformation (false 
information that is purposely spread to deceive peo­
ple)” (p. 2). Notably Tandoc et al. (2018) already stated 
that deception is a necessary precondition for news to 
be regarded as fake: Fake news has no impact if audi­
ences do not erroneously perceive it as real (Tandoc 
et al., 2018). Thereby, fallacies share common ground 
with fake news since both intend to deceive, misin­
form, and shape the opinions of recipients. Based on 
the extant literature (Table 1), we recognise that prior 
research operationalised fake news only as fabricated 
information. We extend this understanding by includ­
ing the rhetorical presentation of an argument, which 
can also affect whether information is perceived as 
fake news.
Figure 1 visualises the proposed relation among 
fake news, rhetorical devices, and fallacies. The rela­
tionship implies that some rhetorical devices and 
fallacies do not overlap with fake news. One potential 
example is the conjunction fallacy, i.e., a formal fallacy 
that occurs when a decision-maker assumes that spe­
cific conditions are more likely than a single general 
one (Tversky & Kahneman, 1981).
Fallacies can be nuanced, as there are many ways to 
produce an error in reasoning. As Van Eemeren et al. 
(2002) showed, fallacies can occur by violating any one 
of several rules for sound argumentation. Given the 
variety of ways in which fallacies may obscure an 
argument’s construction, we anticipate that some fal­
lacies are more closely linked to today’s common 
perception of fake news than others. As such, they 
might be able to (partially) explain the grey area of 
fake news. Based on this reasoning, we introduce our 
first hypothesis (H1): 
H1: As a subset of rhetorical devices, fallacies can help 
to distinguish nuances (i.e., grey areas) in the perception 
of information as fake news.
3.2. Detectability of fallacies
In order to uncover (and rectify) errors in reasoning, 
argumentation theory pursues the following tasks: 
identification (identifying an argument’s premises 
and conclusion), analysis (finding implicit premises, 
i.e., the unstated assumptions of an argument, as well 
as conclusions that need to become explicit), evalua­
tion (evaluating the strength of an argument), and 
invention (developing new arguments to support the 
conclusion; D. D. Walton, 2009). Certain fallacies are 
harder to identify and analyse than others. This is 
because there are different ways in which people can 
be lured into errors in reasoning (Dowden, 1993). 
Reviewing different fallacies, Dowden (1993) sum­
marised several approaches by which these errors can 
be detected: focusing on the reasons instead of the 
reasoner; pointing out choices other than the ones 
mentioned; assessing the credibility of the argument 
source, and noticing when an argument attempts to 
divert a reader’s attention from the issue at hand. 
Given the different ways in which fallacies can deceive 
an audience, “it makes no more sense to suppose that 
they must all be given a common analysis than it does 
to suppose that all diseases should be given the same 
diagnosis and treatment” (Van Eemeren et al., 2013, 
p. 237).
For illustration, we consider two prominently used 
fallacies: a) ad hominem (e.g., Barnes et al., 2018. 
D. N. Walton, 1987), i.e., an author deliberately 
attacks the person who has brought forth an argu­
ment instead of the argument itself, often accompa­
nied by insults about the counterpart’s personality, 
and b) formal logical error (e.g., Binoy, 2014; 
Fearnside & Holther, 1959; Floridi, 2009), i.e., an 
Figure 1. Relation among fake news, rhetorical devices, and 
fallacies.
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
47

author deliberately makes a deductively invalid argu­
ment that typically commits a logical error. 
Recognising the former, i.e., seeing the insult as 
a distraction from the argument itself, can be done 
with relative ease, but recognising the latter demands 
more cognitive deliberation from the reader.
Dual-process theory (Kahneman & Egan, 2011) 
complements argumentation theory by explaining 
how people process information and make judge­
ments. Both Kahneman and Egan (2011) and Strack 
and Deutsch (2004) differentiated between two sys­
tems that work together to guide human behaviour: an 
impulsive (also referred to as System 1) and a reflective 
(also referred to as System 2) system. While the former 
primarily relies on associative links, the latter is gov­
erned by structured decision processes and a clear 
intent (Strack & Deutsch, 2004). System 1 operates 
on a low cognitive capacity and is always active per 
default, whereas System 2 requires more cognitive 
investment and can be easily disturbed (e.g., through 
distraction or arousal; Strack & Deutsch, 2004).
On social media, where more than half of U.S. adults 
consume their news (Shearer, 2021), readers tend to 
consume news incidentally as it arises in their news­
feed – i.e., they do not deliberately search for the news 
(Bergström & Jervelycke Belfrage, 2018). The resulting 
lack of critical thinking and deliberation strongly sug­
gests a predominance of System 1 processing when it 
comes to news consumption on social media. Relying 
on System 1 processing may prevent readers on social 
media from detecting the fake news character of certain 
news headlines, especially if the latter employ fallacies 
that are subtle in their intent to misinform and cannot 
be detected without further deliberation. Hence, we 
introduce our second hypothesis: 
H2: Fallacies are not all equally detectable as fake news.
Based on this hypothesis, we argue that educating 
readers on social media about the presence and type of 
fallacy in a news headline can help them manually 
detect errors in cases that require more cognitive 
deliberation.
4. Empirical study
To test the two hypotheses, we conducted an experi­
mental study that employed the best-worst scaling 
method, case 1 (e.g., Hinz et al., 2015; Kaufmann 
et al., 2018; Louviere et al., 2015, 2013). Directly after­
wards, we conducted a second experimental study to 
assess the reactions of readers who suspect that 
a contact has shared information perceived as fake 
news (described later in Section 4.4). We implemented 
and executed the questionnaire using the online sur­
vey platform DISE (Schlereth & Skiera, 2012).
The best-worst scaling method is a variant of dis­
crete choice experiments (e.g., Hauser et al., 2019; 
Schlereth & Skiera, 2017) that has recently gained 
popularity because it allows one to measure an indivi­
dual’s strength of preference for, or level of agreement 
with, a number of items. An item can be a statement or 
some other element of interest. This method is notable 
for ensuring that respondents hold consistent inter­
pretations of the same decisions (Mueller Loose & 
Lockshin, 2013). Compared with verbal measurement 
scales (e.g., Likert scales), respondents do not use 
artificially numerical, subjectively interpretable repre­
sentatives for their preferences. Instead, they choose 
among alternatives that are easy to understand and 
that can be quickly evaluated. Consequently, the 
results from this method provide a higher degree of 
discrimination, while allowing a consistent interpreta­
tion of responses across participants.
With the best-worst scaling method, we aim to 
measure the individual strength of respondents’ per­
ception that a specific fallacy in a news statement 
represents fake news. Put differently, we investigate 
whether fallacies can influence the perception of infor­
mation as fake news and whether readers distinguish 
nuances (i.e., grey areas) based on the applied fallacies. 
Figure 2 illustrates the use of the best-worst scaling 
method. Respondents repeatedly see choice sets, each 
consisting of a different subset of items, and choose 
the best and worst item from each set (Mueller et al.,  
2010). The terms “best” and “worst” constitute 
a metaphor for the extremes of a latent, subjective 
continuum (Louviere et al., 2015). In our study, we 
operationalised the “best” and “worst” labelling by 
letting respondents decide which item in each choice 
set “most closely resembles fake news” and “least clo­
sely resembles fake news”, respectively.
4.1. Setup of the fallacy experiment
In total, we used nine items: six items represented one 
particular fallacy each, two items represented addi­
tional rhetorical non-fallacy devices, and a final item 
represented fabricated content. Respondents saw the 
implementation of each item in the form of an exemp­
lary headline that represented a news statement. In 
some versions of the experiment, respondents also 
saw brief explanations of the rhetorical device 
employed in the shown headline.
We implemented news statements as headlines 
because they are a common method of garnering 
consumers’ attention. The goal of headlines is to 
make readers click on the article or provide the reader 
with immediate opinions. The headline is one of the 
most influential elements because it is prominently 
visible anywhere a page is shared or linked. Often, 
the headline is the most visible (and clickable) part, 
as it shows up in the link preview when anyone shares 
48
S. BEISECKER ET AL.

an article, and it shows up in any browser tab. So, 
when many tabs are open, the headline may drive the 
reader back to a tab.
We implemented the best-worst scaling method in 
a two-by-two between-subject experimental setting to 
empirically test the two hypotheses. We varied a) the 
topic of the news statement (business or political con­
text) and b) whether respondents saw brief explana­
tions of the rhetorical devices or just the news 
statements alone. Respondents who saw no explana­
tions made their decisions solely based on the exemp­
lary news statements. In cases where respondents saw 
the explanations, we instructed them to focus on the 
explanations and consider the corresponding state­
ments as illustrative examples.
To empirically test our hypotheses, we studied 
the correlations between the best-worst scores of the 
different versions of our experiment. The use of 
correlational analysis corresponds to Gregor’s 
(2006) understanding of type III theory in informa­
tion systems. If respondents in the political and 
business scenarios produce comparable rankings of 
the rhetorical devices when given explanations, then 
we can conclude that they complied with our 
instructions and focused on the explanations. If, in 
addition, the rankings correlate between the politi­
cal and business context within the experimental 
condition that only contained the exemplary news 
statements and no explanations, we would consider 
this scenario as support for the first hypothesis (H1, 
i.e., that fallacies as a subset of rhetorical devices 
can help illuminate nuances, i.e., grey areas, in the 
perception of information as fake news).
By taking the differences in the perceptions as fake 
news between an item that included explanations and 
the corresponding item that did not include explana­
tions, we can identify which rhetorical device is over- 
or under-detected as fake news. If this difference is 
also similar between the two contexts, we can consider 
that as support for the second hypothesis (H2, i.e., that 
fallacies are not all equally detectable as fake news).
4.2. Fallacy and item selection
The Internet Encyclopaedia of Philosophy lists over 
200 fallacies (Dowden, 2019). Even though the under­
lying rhetorical devices of fallacies are well under­
stood, we are not aware of any empirical work that 
examines how audiences react to different types of 
fallacies. One exception is the study by Van Eemeren 
et al. (2009), who found that respondents generally 
consider fallacies as less reasonable than sound state­
ments. However, the authors solely focused on one 
fallacy (i.e., ad hominem) and thus did not provide 
comparisons (e.g., according to the degree of decep­
tion) across different fallacies.
Given our lack of knowledge on how fallacies differ 
in their degree of deception or usage frequency, we 
selected six exemplary fallacies based on our subjective 
determination that they would be suitable for detecting 
nuances in people’s perceptions of information as fake 
news. Besides the aforementioned ad hominem and 
formal logical error, the remaining four fallacies are 
false dilemma, argument from ignorance, bandwagon 
effect, and false attribution. Some of the chosen fallacies 
provide an invalid argument because their pattern of 
reasoning is wrong; others use a poor reasoning struc­
ture. We acknowledge that these fallacies do not repre­
sent the full range of possible fallacies and want to 
emphasise that other fallacies could have sufficed.
We added another item to this list of fallacies that 
researchers consider quite common in Internet arti­
cles: what is frequently referred to as “clickbait” or 
“stylistic flaws” (Rubin, 2017). These terms acknowl­
edge that many creators design their statements to 
generate as much audience attention as possible 
Most closely 
resembles fake 
news
Least closely 
resembles 
fake news
X
The author deliberately conveys a completely made-up statement, which is by 
no means backed by objective evidence.
e.g., " Emissions scandal: For years, competitor BMW bribed VW employees 
to manipulate their emissions values and thus harm the VW brand."
The author deliberately attacks the person who has brought forth an argument 
instead of the argument itself, often not shying away from insults about 
counterpart’s personality.
e.g., "Herbert Diess is an absolutely unqualified and incompetent CEO: The 
entire VW board of directors has known about the emissions scandal for 
years."
The author deliberately uses stylistic flaws, including the use of emotion 
evoking words, excessive punctuation marks, and case insensitivity.
X
e.g., "ATTENTION!!! - The entire VW board of directors has known about the 
emissions scandal for years!"
Figure 2. Illustrative example choice set.
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
49

through sensational and emotionally appealing head­
lines (Bakir & McStay, 2018). They often encourage 
consumers to click on a link through the excessive use 
of punctuation marks or upper-case writing (e.g., 
“DANGER!!!”).
Finally, we added two additional items as an upper 
and lower extreme on the range of perceptions of fake 
news. The item serving as an upper range limit is 
alternative facts: a phrase that U.S. presidential coun­
sellor Kellyanne Conway used in a 2017 press confer­
ence (Bradner, 2017), defined as a “completely made 
up statement”, i.e., fabricated content. The item ser­
ving as a lower range limit is rhetorical question, which 
is a statement that is suggestive but not necessarily 
false. We summarise all items alongside the corre­
sponding descriptions in Table 2.
4.3. Topic choice and example construction
We chose one political- and one business-related con­
text for the topic manipulation. The business version, 
illustrated in Figure 2, dealt with VW’s software-based 
manipulation of emission values in their diesel cars. 
The political version dealt with the influence of social 
media on the 2016 U.S. election results. News media 
prominently discussed both topics at the time of the 
study.
To construct suitable headlines for each rhetorical 
device, we kept the facticity of statements about equal 
for each topic. The factually true parts were always 
VW’s manipulation of emission values and the Trump 
campaign’s social media activities. We then exagger­
ated or added made-up elements to this existing con­
text. For example, we stated that social media was the 
“main reason for the U.S. election results” and that the 
“entire VW board has known about the emissions 
scandal for years”. All headlines might have been 
true or false, at least to some degree, except for alter­
native facts, which were factually flawed. Table 2 lists 
all exemplary headlines.
In total, each respondent saw nine items. For these 
items, we used a balanced incomplete block design 
with 12 choice sets consisting of three items each, 
which ensures level balance (each item appears four 
times) and orthogonality (each pair of items appears 
once) across respondents. Respondents chose the 
items that most and least resembled fake news in 
each of the twelve choice sets. To avoid order effects, 
we randomised the choice set and item order within 
a choice set across respondents.
4.4. Setup of the reaction experiment and 
sampling
The second experiment directly followed the first. Its 
goal was to examine how readers react in cases where 
they are sceptical that a certain social media user is 
posting information that they perceive as fake news. 
On a scale between 1 (= totally disagree) and 7 (= 
totally agree), we asked how likely they a) would stop 
paying attention to the account, b) report the account 
to the platform, c) stop receiving messages from the 
account, and d) stop using the whole platform. These 
questions also contained an attention check that read 
“please click ‘totally disagree’ to demonstrate that you 
read the questions carefully”.
We randomly assigned respondents, independent 
of the first experimental assignment, to one of the 
following conditions: a) we varied the type of account 
(labelled as either an acquaintance, i.e., not-so-close 
friend, or a company) and b) the frequency of posts 
containing fake news (every 4th message or every 10th 
message). Regarding the first experimental condition, 
we manipulated the sharing party’s closeness to the 
reader.
Following the work by Clark and Mills (1993), we 
anticipated that relationships with a company (a non- 
personal contact) would be governed more by 
exchange norms (i.e., norms that focus on self- 
interest and material gain; Aggarwal & Larrick,  
2012), while relationships with an acquaintance (a 
person known to the reader) would be governed 
more by communal norms (i.e., norms that focus on 
mutual caring and trust; Aggarwal & Larrick, 2012; 
Clark & Mills, 1993). In the former relationship, the 
user expects a certain benefit in return for following 
the company (in this case, the provision of truthful 
content); they become willing to withdraw from the 
relationship if this benefit disappears. In the latter 
relationship, the user may not be guided solely by 
norms of reciprocity, but instead value aspects like 
mutual support (Aggarwal & Larrick, 2012; Clark & 
Mills, 1993), which extend beyond the truthful sharing 
of information. As a result, we expected users to show 
a stronger reaction to a company relative to an 
acquaintance.
A total sample of 488 participants, gathered 
through a professional panel provider in Germany, 
completed the online survey in the fourth quarter of 
2019. We excluded 72 of these respondents because 
they failed the attention check. The final sample con­
sisted of 416 respondents (201 male; Mage 
= 52.68 years).
4.5. Results of the fallacy experiment
One strength of best-worst scaling is that its observa­
tions are easy to analyse without any proprietary soft­
ware. Simply counting the number of best choices for 
an item and subtracting the number of times 
a respondent chose this item as worst provides indivi­
dual or aggregate sample preference estimates, which 
we subsequently refer to as BW scores (Finn & 
Louviere, 1992; Mueller Loose & Lockshin, 2013). 
50
S. BEISECKER ET AL.

Each item appeared four times (= 12 x 3/9). 
Consequently, an item may generate BW scores ran­
ging between −4 and +4, depending on how frequently 
a respondent chose it as “most likely resembles fake 
news” (+1), as “least likely resembles fake news” (−1), 
or not at all (+0). Adding +5 to all BW scores will 
transform them into a range between 1 and 9, i.e., the 
response we would observe on a nine-point Likert 
scale. After the estimation, we followed Louviere 
et al. (2015) and normalised the BW scores between 
100 (most closely resembles fake news) and 0 (least 
closely resembles fake news).
Figure 3 visualises the normalised BW scores for all 
items and combines three types of results. First, it 
visualises the BW scores of each experimental condi­
tion and summarises their values in a table below. 
Furthermore, we report changes in the ranking posi­
tions between the experimental condition in which 
respondents saw explanations of the rhetorical device 
and the experimental condition in which they did not 
see them, as well as display the differences of the two 
normalised BW scores for an item. Finally, we statis­
tically tested how the BW scores relate to each experi­
mental manipulation using the Pearson correlation 
coefficient on the right-hand side of Figure 3.
When looking at the first two rows (i.e., the ones 
related to the explanations of the rhetorical device), we 
found that alternative facts most closely resembled 
fake news (BW scores = 100.00 for both groups) 
while rhetorical question least closely resembled fake 
news (0.00 for both groups). A “grey area” of percep­
tion of information as fake news existed between these 
Table 2. Rhetorical device descriptions and exemplary statements of rhetorical devices (translated from German language).
Item
Description; exemplary statements (business and politics)
Alternative facts 
(Boundary item through made-up content)
The author deliberately conveys a completely made-up statement, which is by no means backed by 
objective evidence. 
“After U.S. election results: Trump founds consulting firm for successful social media manipulation in 
political elections”. 
“Emissions scandal: For years, competitor BMW bribed VW employees to manipulate their emissions 
values and thus harm the VW brand”.
False dilemma 
(Fallacy)
The author deliberately frames a situation as “either/or” when there are, in fact, more than two options. 
“Dilemma: Either use social media and support potential election manipulations or stop using them at 
all”. 
“VW dilemma: Either accept that the entire VW board of directors has known about the emissions scandal 
for years or stop using VW cars”.
Formal logical error 
(Fallacy)
The author deliberately makes a deductively invalid argument that typically commits an easily 
recognisable logical error. 
“Proved: Social media allows for election manipulation. Trump used social media extensively. Therefore, 
social media manipulation is the main reason for the presidential election results”. 
“Proved: Car manufacturers manipulated the emission levels. The board of directors has insight into all 
activities of the company. The entire VW board of directors has known about the emissions scandal for 
years!”
Argument from ignorance 
(Fallacy)
The author deliberately states he/she doesn’t have to prove his/her claim; instead someone else has to 
disprove it. 
“No counterevidence: Social media manipulations are the main reason for the U.S. presidential election 
results!” 
“No counterevidence: The entire VW board of directors has known about the emissions scandal for years!”
Bandwagon effect 
(Fallacy)
The author deliberately assumes that the probability of individual adoption increases along with the 
overall proportion of people who adopt a practice/opinion. 
“General public is certain: Social media manipulation is the main reason for the U.S. presidential election 
results – Here is why you should think so!” 
“Majority of frustrated VW users switch to other cars because the entire VW board of directors has known 
about the exhaust gas scandal for many years – Here is why you should think so too!”
False attribution 
(Fallacy)
The author deliberately appeals to an irrelevant, unqualified, unidentified, biased or fabricated source in 
support of an argument. 
“Close friend of a government official who wishes to remain anonymous, confirmed: Social media 
manipulation is the main reason for the U.S. presidential election results”. 
“Several employees who want to remain anonymous confirm: The entire VW board has known about the 
emissions scandal for years”.
Ad hominem 
(Fallacy)
The author deliberately attacks the person who has brought forth an argument instead of the argument 
itself, often not shying away from insults about counterpart’s personality. 
“Marc Zuckerberg is a completely unqualified and incompetent CEO: Social media manipulation is the 
main reason for U.S. presidential election results” 
“Herbert Diess is an absolutely unqualified and incompetent CEO: The entire VW board of directors has 
known about the emissions scandal for years”
Stylistic flaws 
(–)
The author deliberately uses stylistic flaws, including the use of emotion-evoking words, excessive 
punctuation marks, and case insensitivity. 
“WATCH OUT!!! – Social media manipulations are the main reason for the presidential election results of 
the USA!” 
“ATTENTION!!! – The entire VW board of directors has known about the emissions scandal for years!”
Rhetorical question 
(Boundary item with no or low expected 
relationship to fake news)
The author deliberately frames a question such that it has an obvious or implied answer. 
“Will you really continue to use social media, even if manipulation is the main reason for the 
U.S. presidential election results?” 
“Will your next car really be a VW, even though the entire VW board of directors has known about the 
emissions scandal for years?”
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
51

two. False attribution (mean BW score of both 
groups = 68.19), ad hominem (48.31), and formal 
logical error (42.34) ranked relatively high in terms of 
their perception as fake news, followed by stylistic 
flaws (40.92), argument from ignorance (30.82), band­
wagon effect (17.16), and false dilemma (11.14).
When respondents saw the explanations of the 
applied fallacy, we observed a high Pearson correlation 
coefficient of .94 (p < .01) between the normalised BW 
scores of the two contexts. A likely explanation is that 
respondents followed our instructions and concen­
trated on the explanations. However, when respon­
dents saw the exemplary news statements without 
the explanations, the normalised BW scores were 
also highly correlated between the political and busi­
ness contexts, with a Pearson correlation coefficient of 
.73 (p < .05). This high correlation supports our first 
hypothesis (H1) and suggests that fallacies and rheto­
rical devices can indeed capture the grey areas of fake 
news, i.e., the nuances in perception.
We noticed substantial differences between 
respondents who saw the news statements alone 
and those who also saw the explanations. The corre­
lation between the two groups was not significant for 
either the political or business context (p > .10). 
While respondents in the first group identified the 
news statement that employed alternative facts as 
made up and likewise awarded rhetorical question 
with a relatively low BW score (13.64), the BW scores 
of false attribution and formal logical error were sub­
stantially lower compared to the case where the 
explanations were also provided. For example, the 
item ranking position of false attribution decreased 
without the explanations by up to six ranks. 
Conversely, respondents strongly overemphasised 
the bandwagon effect and false dilemma technique 
as fake news compared to those who received the 
explanations. Meanwhile, the BW scores of stylistic 
flaw and ad hominem were congruent for both 
groups.
To test the second hypothesis (H2), we calculated 
the differences in BW scores and ranking positions 
between the versions in which explanations were pro­
vided vs. the versions in which the statements 
appeared alone. Figure 3 lists the results in the final 
rows. The Pearson correlation coefficients for the two 
contexts were .85 (p < .01) when comparing the differ­
ences in the ranking positions and .83 (p < .01) when 
comparing the differences in the BW scores. As such, 
we found empirical support for H2.
In a final step, we investigated the heterogeneity of 
the (normalised) BW scores. We regressed the experi­
mental condition together with the demographics age 
and gender on each score and report the results in 
Table 3. We additionally regressed them on 
a consistency measure, i.e., a quality measure of 
respondents’ choices, which we adapted from 
Louviere et al. (2015). The authors propose that con­
sistency for each respondent should be measured as 
the individual sum of all squared BW scores. Given the 
properties of the balanced incomplete block design 
(i.e., level balance and orthogonality), a perfectly con­
sistent respondent achieves the highest consistency 
measure (in our case, with each item appearing four 
times: 2 · 42 + 2 · 32 + 2 · 22 + 2 · 12 = 60). Inconsistent 
responses result in a consistency measure close to zero.
Alternative 
facts
False 
attribution
Ad 
hominem
Formal 
logical 
error
Stylistic 
flaws
Argument 
from
ignorance
Bandwagon 
effect
False 
dilemma
Rhetorical 
question
Techniques in business
context
Techniques 
in political 
context
Statements 
alone in 
business context
Explanations in business context, normalized mean 
BW scores
100.00
60.67
44.51
32.93
30.79
36.28
17.07
15.24
0.00
Correlations
Explanations in political context, normalized mean 
BW scores
100.00
75.70
52.11
51.76
51.06
25.35
17.25
7.04
0.00
.94***
Statements alone in business context, normalized 
mean BW scores
100.00
9.40
69.59
0.00
40.44
20.69
73.67
51.10
27.27
.39
.23
Statements alone in political context, normalized 
mean BW scores
86.58
13.42
100.00
12.99
80.52
27.71
43.72
24.68
0.00
.51
.50
.73**
Differences between explanations vs. statements alone
Difference in ranking positions for business context
0
-6
0
-5
0
-1
5
4
3
Correlation in ranking
differences between 
contexts
.85***
Difference in ranking positions for political context
-1
-5
2
-4
2
1
3
2
0
Difference in normalized mean BW scores for 
business context
0.00
-51.27
25.08
-32.93
9.65
-15.59
56.59
35.85
27.27
Correlation in 
normalized mean BW 
score differences 
between contexts
.83***
Difference in normalized mean BW scores for 
political context
-13.42
-62.28
47.89
-38.77
29.46
2.35
26.47
17.63
0.00
0.00
10.00
20.00
30.00
40.00
50.00
60.00
70.00
80.00
90.00
100.00
Alternative
facts
False
attribution
Ad
hominem
Formal logical
error
Stylistic
flaws
Argument from
ignorance
Bandwagon
effect
False
dilemma
Rhetorical
question
s
e
r
o
c
S
tsr
o
W
ts
e
B
d
e
zila
m
r
o
N
Explanations in business context
Explanations in political context
Statements alone in business context
Statements alone in political context
Figure 3. Results of best-worst scaling analysis. Note: N = 416; *: p < .1; **: p < .05; ***p < .01
52
S. BEISECKER ET AL.

The experimental conditions mostly explain the 
differences in individual BW scores: in particular, 
whether respondents saw the explanations for the 
respective rhetorical devices or not. For instance, 
without explanations, respondents did not perceive 
the news statements that contained false attribution 
as fake news to the same degree (−25.79 and 
−19.23, p < .01). In contrast, they overemphasised 
ad hominem as fake news (+10.11 and +13.21, 
p < .05) compared to the condition in which we 
provided explanations. With few exceptions, the 
coefficients of gender, age, and the consistency 
measure were significant for those items that we 
added as natural boundaries of the experiment (i.e., 
alternative facts and rhetorical questions). For 
example, regarding the dependent variable alterna­
tive facts, the positive and significant value for age 
(.33) indicates that older respondents had a strong 
perception that this item constitutes fake news 
(also, when we did not provide the explanations). 
Yet, most of the fallacies were unaffected by these 
variables. This suggests that the degree to which 
people perceive fallacies as fake news is mostly 
independent of age, gender, or the consistency of 
best-worst choices.
We tested the robustness of the results and repli­
cated the political subset of the first experiment (i.e., 
the two experimental conditions with explanations of 
the rhetorical device and without) with 100 under­
graduate students. We found that the consistency in 
their best-worst choices (again calculated as the indi­
vidual sum of the squared BW scores) was substan­
tially higher compared to the respondents in the panel. 
Nevertheless, the normalised BW scores were similar 
with correlation coefficients of .94 (p < .01) and .73 
(p < .05). We conclude that the results are robust 
across different samples of respondents.
4.6. Post-hoc support for item selection
To solidify our assumption that the included fallacies 
are suitable for detecting nuances in people’s percep­
tions of information as fake news, we conducted 
a post-study with either tenured professors or assistant 
professors with a background on communications 
(either from Information Systems or Marketing). 
Thereby, we tested two underlying assumptions of 
our main study, namely: (i) that the used techniques 
are deceptive (in the sense of opinion-shaping) and (ii) 
that news authors use them in news statements on 
social media.
We sent participation requests to about 60 profes­
sors and obtained a sample of 30: 25 full professors 
and 5 assistant professors or post-doctoral researchers. 
Of the 30, 93% indicated that they used one or more 
social media platforms at least once per month 
(Facebook: 73%, LinkedIn: 73%, Twitter: 47%, 
Instagram: 37%), which we took as a sign that they 
were likely familiar with news on social media.
We asked them to assess whether the techniques are 
deceptive. The respondents saw eleven techniques: 
namely, the nine techniques from the main study 
and two new fallacies that we considered when design­
ing the main study, i.e., “post hoc ergo propter hoc” 
and “relative privation” (Bennett, 2012; Fearnside & 
Holther, 1959). For each technique, the respondents 
evaluated the following statement on a Likert scale 
from 1 to 7 (1: do not agree at all, 7: fully agree): “I 
Table 3. Examination of heterogeneity in BW scores.
Coefficients for each 
independent variable (linear 
regression; DV: normalised BW 
scores)
Alternative 
facts
False 
attribution
Ad 
hominem
Formal 
logical 
error
Stylistic 
flaws
Argument 
from 
ignorance
Bandwagon 
effect
False 
dilemma
Rhetorical 
question
Intercept
46.12 
(7.78)***
66.10 
(8.41)***
51.44 
(8.40)***
.56 
(.49)
48.86 
(8.41)***
63.70 
(7.87) 
***
45.15 
(7.66)***
51.76 
(7.74)***
42.72 
(7.76)***
Techniques in business context 
(Reference: explanations in 
political context)
−9.23 
(4.07)**
.33 
(4.40)
−.33 
(4.40)
.42 
(.26)
6.49 
(4.40)
−7.16 
(4.12)*
−.32 
(4.01)
−2.88 
(4.05)
1.63 
(4.06)
Statements alone in business 
context (Reference: 
explanations in political 
context)
−2.93 
(4.24)
−25.79 
(4.58)***
10.11 
(4.57)**
−1.32  
(.27)***
2.13 
(4.58)
−9.11 
(4.29)**
23.24 
(4.17)***
14.66 
(4.21)***
9.32 
(4.23)**
Statements alone in political 
context (Reference: 
explanations in political 
context)
−16.26 
(4.04)***
−19.23 
(4.37)***
13.21 
(4.36)***
−.46 
(.26)*
13.31 
(4.37)***
−4.35 
(4.09)
8.19 
(3.98)**
4.53 
(4.02)
3.13 
(4.03)
Female (Reference: Male)
−6.51 
(2.90)**
−2.43 
(3.13)
1.23 
(3.13)
.05 
(.18)
2.10 
(3.13)
−1.63 
(2.93)
−4.35 
(2.85)
5.78 
(2.88)**
5.73 
(2.89)**
Age
.33 
(.10)***
−.07 
(.10)
−.05 
(.10)
.00 
(.01)
−.18 
(.10)*
−.25 
(.10)**
.06 
(.10)
−.02 
(.10)
−.04 
(.10)
Consistency
.47 
(.11)***
−.02 
(.12)
.09 
(.12)
−.01 
(.01)*
.14 
(.12)
−.02 
(.11)
−.11 
(.11)
−.33 
(.11)***
−.26 
(.11)**
R2
.108
.119
.037
.111
.037
.030
.102
.076
.035
Note: N = 416; *: p < .1; **: p < .05; ***p < .01
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
53

perceive the technique as deceptive (in the sense of 
opinion-shaping)”. In Figure 4, we summarise the 
results.
Alternative facts were perceived as the most 
deceptive, followed by the remaining techniques 
that we had chosen for the main study. The two 
newly added techniques, post hoc ergo propter hoc 
and relative privation, ranked lowest among the 
studied items. These results strengthen our antici­
pation that the chosen exemplary fallacies are sui­
table for detecting nuances in people’s perceptions 
of information as fake news. Interestingly, among 
the experts, rhetorical devices were perceived to be 
more deceptive than we anticipated and observed 
in the main study.
Subsequently, the respondents evaluated how often 
they thought news authors use each of the eleven 
techniques in social media news statements (not at 
all, seldom, frequently, very frequently). Out of all 
330 (=30x11) evaluations, only five evaluations were 
“not at all”. Hence, the majority of respondents felt 
that the techniques are actually used in social media. 
We summarise the results in Figure 5.
Finally, respondents evaluated the following 
statement on a Likert scale from 1 to 7 (1: do not 
agree at all, 7: fully agree): “I perceive a social 
media news statement as fake news when the afore- 
shown techniques are used”. We deliberately placed 
this question towards the end of the survey and 
used the term “fake news” here for the first time so 
as not to reveal the topic of the main study in the 
previous questions. We obtained an average score 
of 5.50 on this measure, indicating that the average 
participant agreed that using these techniques influ­
ences the perception of information as fake news 
(see, Figure 6).
Figure 4. Mean evaluation of deceptiveness, by technique.
Figure 5. Frequency distributions of technique use on social media.
54
S. BEISECKER ET AL.

4.7. Results of the reaction experiment
Table 4 summarises the results on how respondents 
would react to posts of a social media account contain­
ing fake news in their news feed, as derived from 
our second study. Consumers who noticed fake news 
in their news feed primarily reacted towards the 
authors of the post, i.e., acquaintance or company 
(4.60–5.59), and less towards the platform (2.82– 
3.88). To explore this finding further, we estimated 
a linear regression with each reaction as the dependent 
variable, while the author of the post, frequency, gen­
der, age, and social media usage intensity were treated 
as independent variables. We furthermore controlled 
for potential yea- or nay-saying behaviour in Likert 
scales, which is also referred to as acquiescence bias 
(e.g., Dinev et al., 2013; Johnston et al., 2016; 
Podsakoff et al., 2003). To this end, we incorporated 
an additional independent variable. For each reaction, 
the additional variable consisted of the product of the 
other three reactions, and thus, it captures respon­
dents’ tendency to answer on the right- or left-hand 
side of the scale.
The regression results show that companies generally 
suffer more from spreading fake news than acquain­
tances: In such cases, it is significantly more likely that 
consumers will stop following the company’s account 
or leave the platform entirely. The actual posting fre­
quency (every 4th vs. every 10th message) had no sig­
nificant impact. Regarding social media usage, we found 
that people who use social media more frequently have 
a more pronounced reaction when they notice fake 
news in their news feed: They are significantly (albeit 
weakly) more likely to report the friend to the platform 
(p < .1), significantly more likely to stop following the 
account (p < .05), but significantly less likely to stop 
using the platform (p < .01). Heavy users have devel­
oped a habit for social media use and may therefore be 
less likely to abandon the platform. At the same time, 
fake news detracts from the user experience and per­
ceived value of the platform, leading people to take 
corrective action by reporting a friend who shares fake 
news. Heavy users should also be more acquainted with 
the features offered by social media platforms, such as 
the ability to unfollow or report an account, and may, 
therefore, be more prone to using them.
To conclude, the frequency of messages that users 
perceive as fake news is not important for users’ reac­
tion towards the platform or the sender. However, we 
observe a more severe reaction among users when the 
message comes from a company instead of an 
acquaintance. Together with the findings on fallacies, 
we conclude that companies must be cautious about 
how readers perceive their messages and should care­
fully evaluate their choice of rhetorical devices.
5. Discussion and conclusions
While other disciplines have generally conceptualised 
fake news as binary (either true or false) and lacked 
a common definition, our manuscript represents a first 
Figure 6. Perceived link between use of techniques and fake 
news.
Table 4. Results of reaction towards fake news in social media feed.
Mean Likert scores (scale with 1 = lowest, 7 = highest)
Pay less attention to 
account
Report friend 
to platform
Stop 
following 
account
Stop using 
platform
Acquaintance, 4th message
5.56 (1.81)
3.78 (2.12)
4.64 (2.09)
3.08 (2.10)
Acquaintance, 10th message
5.34 (2.21)
3.88 (2.12)
4.60 (2.16)
2.82 (2.09)
Company, 4th message
5.59 (1.79)
3.79 (2.10)
5.25 (1.97)
3.69 (2.21)
Company, 10th message
5.46 (2.05)
3.80 (2.11)
5.30 (2.05)
3.74 (2.42)
Coefficients for each independent variable (linear regression; DV: Likert 
scores)
Pay less attention to 
account
Report friend 
to platform
Stop 
following 
account
Stop using 
platform
Intercept
4.73 (.56)***
2.79 (.60)***
2.97 (.58)***
4.61 (.61)***
Company (baseline: acquaintance)
.07 (.19)
.27 (.20)
−.49 (.19)**
−.70 (.20)***
4th message (baseline: 10th message)
.19 (.19)
−.04 (.20)
.09 (.19)
.07 (.20)
Gender
.05 (.19)
−.09 (.20)
−.03 (.19)
−.12 (.20)
Age
.00 (.01)
.00 (.01)
.02 (.01)***
.01 (.01)
Social media usage intensity
.04 (.08)
.14 (.08)*
.19 (.08)**
−.57 (.08)***
Acquiescence bias
.01 (.00)***
.01 (.00)***
.01 (.00)***
.00 (.00)**
Note: N = 416; *: p < .1; **: p < .05; ***p < .01
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
55

step in analysing and defining the term “fake news” 
apart from its content. Without claiming to cover the 
whole spectrum of fake news, we propose that rheto­
rical devices play a fundamental role in people’s per­
ception of information as fake news. Thus, we sought 
to study the use of exemplary fallacies in the context of 
news headlines.
In our paper, we observed that consumers differ­
entiate nuances of fake news (i.e., grey areas) with high 
correlations between political and business contexts. 
However, without explanations, some fallacies success­
fully manage to distract their audience from the state­
ments’ contents. Readers perceive these statements to 
a lesser degree as fake news when presented in isolation 
rather than when presented alongside the underlying 
rhetorical device. For example, respondents who did 
not receive explanations about false attribution did not 
immediately recognise this fallacy as fake news. At the 
same time, they overemphasised the perception of 
other fallacies like the bandwagon effect.
Our study challenges the simplistic view that fake 
news has a binary operationalisation based on the 
availability of facts. Instead, fake news is more multi- 
faceted. Focusing attention on fallacies in news state­
ments can enhance consumers’ information literacy 
(i.e., their ability to discriminate between credible 
and fake news) and help them overcome common 
individual prejudices. Besides, such a focus provides 
further opportunities for scholars and practitioners to 
identify and prevent fake news. This would help not 
only in social media, but also in a traditional media 
context (e.g., newspapers or their associated websites).
Our study also points out that fake news is not 
only of concern in politics, but that the studied 
rhetorical devices are similarly perceived as fake 
news in our business context. This observation sug­
gests that fake news can cause extensive harm to 
companies. Business cases support this argument: 
For instance, in November 2016, PepsiCo’s CEO 
was misquoted as saying that Trump voters should 
“take their business elsewhere” (Picchi, 2016); this 
proved damaging to the brand’s reputation and 
resulted in a plunge of its stock price, which took 
more than a month to recover. Of course, fake news 
has implications that go far beyond politics or busi­
ness and can impact society at large. The COVID-19 
pandemic and the issue of climate change are two 
prominent breeding grounds for fake news, as many 
decisions are irreversible and important data for their 
evaluation is simply missing.
The results of the reaction experiment suggest that 
users show a stronger reaction when a message per­
ceived as fake news originates from a company instead 
of an acquaintance. In particular, we found that users 
are less likely to withdraw from a relationship with an 
acquaintance (as opposed to a company) by unfollow­
ing the account or leaving the platform altogether. 
This is in line with our expectations based on Clark 
and Mills (1993), who suggested that the relationship 
with an acquaintance is governed by communal 
norms, while the relationship with the company fol­
lows exchange norms. Regarding respondents’ char­
acteristics, our results indicate that heavy users are 
more likely to report a friend to the platform and 
stop following an account, but are less likely to quit 
using the platform altogether.
Like any research, ours contains several opportu­
nities for future studies. For example, we selected only 
a small number of fallacies. Future scholars could 
more deeply examine other fallacies to determine 
which ones drive the perception of information as 
fake news. Moreover, we only presented items in 
a textual format. Future studies could investigate the 
effect of other mediums of communication, such as 
pictures or videos.
When comparing the influence of respondents’ 
characteristics on how fallacies were ranked (see, 
Table 3), we only collected and compared age and 
gender to keep the questionnaire as short as possible. 
These demographic details mainly affected the ranking 
of the items “alternative facts” and “rhetorical ques­
tions”, but not the ranking of the fallacies. Future 
research may collect additional characteristics to 
shed light on individual differences in the perception 
of information as fake news. For example, extant 
research finds that fake news believability is largely 
driven by confirmation bias and thus determined by 
a reader’s prior views on an issue (Kim & Dennis,  
2019). Ricco (2007) found that performance in fallacy 
identification and explanation tasks is unrelated to 
income and education level. However, social media 
usage intensity has been linked to a higher belief in 
certain conspiracy theories and misinformation 
(Enders et al., 2021).
Another limitation is that we only conducted our 
study in the German market. Future research may 
acknowledge the existence of cultural differences in 
the creation and perception of information as fake 
news. The method chosen in this study should be well- 
suited for this task, since best-worst scaling offers the 
advantage of using scales that are similarly interpreted 
by subjects of different cultural backgrounds (Adamsen 
et al., 2013). As a result, if replications of the study 
measure differences in the perceptions across respon­
dents of different cultures, these differences can be 
directly linked to variable perceptions of information 
as fake news across cultures. In other words, they will 
not be affected by different interpretations of the scale 
used to study the perceptions. Furthermore, our best- 
worst scaling approach provides new opportunities to 
classify and compare fallacies.
A final limitation is that, while we can place all 
fallacies on a scale that ranges between “most” and 
“least likely resembles fake news”, we cannot determine 
56
S. BEISECKER ET AL.

a threshold whereby respondents become indifferent in 
their perceptions. We welcome methodological 
approaches that enable researchers to measure this 
threshold when applying best-worst scaling. Usefully, 
Dyachenko et al. (2014) and Louviere et al. (2015) have 
provided some initial ideas in this direction.
Ackowledgements
The authors gratefully thank Prof. Dr. Dr. h.c. mult. 
Klaus Brockhoff for initially igniting the research 
direction of this paper. The authors further thank the 
SALTY professors for participating in our post-study. 
Finally, the authors would like to thank the Konrad- 
Adenauer-Stiftung e. V. for financially and intellec­
tually supporting the first author through a PhD 
scholarship.
Disclosure statement
No potential conflict of interest was reported by the 
author(s).
ORCID
Christian Schlereth 
http://orcid.org/0000-0003-0557- 
716X
References
Adamsen, J. M., Rundle-Thiele, S., & Whitty, J. A. (2013). 
Best-worst scaling reflections on presentation, analysis, 
and lessons learnt from case 3 BWS experiments. Market 
and Social Research, 21(1), 9–27. http://hdl.handle.net/ 
10072/58108 
Aggarwal, P., & Larrick, R. P. (2012). When consumers care 
about being treated fairly: The interaction of relationship 
norms and fairness norms. Journal of Consumer 
Psychology, 22(1), 114–127. https://doi.org/10.1016/j. 
jcps.2011.11.009 
Allcott, H., & Gentzkow, M. (2017). Social media and fake 
news in the 2016 election. Journal of Economic 
Perspectives, 31(2), 211–236. https://doi.org/10.1257/jep. 
31.2.211 
Allport, G. W., & Postman, L. 1947.Henry Holt. The psy­
chology of rumor.
Alonso, M. A., Vilares, D., Gómez-Rodríguez, C., & 
Vilares, J. (2021). Sentiment analysis for fake news 
detection. Electronics, 10(11), 1348. https://doi.org/10. 
3390/electronics10111348 
Altay, S., Hacquin, A.-S., & Mercier, H. (2020). Why do so 
few people share fake news? It hurts their reputation. New 
Media & Society, 24(6) , 1–22. https://doi.org/10.1177/ 
1461444820969893 
Bago B, Rand D G and Pennycook G. (2020). Fake news, fast 
and slow: Deliberation reduces belief in false (but not 
true) 
news 
headlines. 
Journal 
of 
Experimental 
Psychology: General, 149(8), 1608–1613. 10.1037/ 
xge0000729
Bakir, V., & McStay, A. (2018). Fake news and the economy 
of emotions: Problems, causes, solutions. Digital 
Journalism, 6(2), 154–175. https://doi.org/10.1080/ 
21670811.2017.1345645 
Barnes, R. M., Johnston, H. M., MacKenzie, N., Tobin, S. J., 
Taglang, C. M., & Smalheiser, N. R. (2018). The effect of 
ad hominem attacks on the evaluation of claims pro­
moted by scientists. PLoS ONE, 13(1), e0192025. https:// 
doi.org/10.1371/journal.pone.0192025 
Begg, I., Armour, V., & Kerr, T. (1985). On believing what 
we remember. Canadian Journal of Behavioural Science, 
17(3), 199–214. https://doi.org/10.1037/h0080140 
Bennett, B. 2012. (eBookIt.com). Logically fallacious: The 
ultimate collection of over 300 logical fallacies (academic 
edition): https://www.hostingauthors.com/books/logical 
lyfallacious .
Bergström, A., & Jervelycke Belfrage, M. (2018). News in 
social media. Digital Journalism, 6(5), 583–598. https:// 
doi.org/10.1080/21670811.2018.1423625 
Berkowitz, D., & Schwartz, D. A. (2016). Miley, CNN and 
The Onion: When fake news becomes realer than real. 
Journalism Practice, 10(1), 1–17. https://doi.org/10.1080/ 
17512786.2015.1006933 
Berthon, P. R., & Pitt, L. F. (2018). Brands, truthiness and 
post-fact: Managing brands in a post-rational world. 
Journal of Macromarketing, 38(2), 218–227. https://doi. 
org/10.1177/0276146718755869 
Binoy, S. (2014). Logical fallacies in public discourse and 
law. Economic and Political Weekly, 49(40), 24–27. 
https://www.jstor.org/stable/24480818 
Bradner, E. (2017). Conway: Trump White House offered 
‘alternative facts’ on crowd size. CNN. https://edition. 
cnn.com/2017/01/22/politics/kellyanne-conway- 
alternative-facts/index.html 
Brasoveanu, A. M. P., & Andonie, R. (2021). Integrating 
machine learning techniques in semantic fake news 
detection. Neural Processing Letters, 53(2), 3055–3072. 
https://doi.org/10.1007/s11063-020-10365-x 
Brisson, J., Markovits, H., Robert, S., & Schaeken, W. (2018). 
Reasoning from an incompatibility: False dilemma falla­
cies and content effects. Memory & Cognition, 46(5), 
657–670. https://doi.org/10.3758/s13421-018-0804-x 
Bronstein, M. V., Pennycook, G., Bear, A., Rand, D. G., & 
Cannon, T. D. (2019). Belief in fake news is associated 
with delusionality, dogmatism, religious fundamentalism, 
and reduced analytic thinking. Journal of Applied 
Research in Memory and Cognition, 8(1), 108–117. 
https://doi.org/10.1037/h0101832 
Bryanov, K., Vziatysheva, V., & Triberti, S. (2021). 
Determinants of individuals’ belief in fake news: A scoping 
review determinants of belief in fake news. PLoS ONE, 16(6), 
1–25. https://doi.org/10.1371/journal.pone.0253717 
Buckner, H. T. (1965). A theory of rumor transmission. 
Public Opinion Quarterly, 29(1), 54–70. https://doi.org/ 
10.1086/267297 
Bundestag. 2017. Act to Improve Enforcement of the Law in 
Social Networks (Network Enforcement Act).
Clark, M. S., & Mills, J. (1993). The difference between 
communal and exchange relationships: What it is and 
is not. Personality and Social Psychology Bulletin, 19 
( 6 ) ,  6 8 4 – 6 9 1 .  h t t p s : / / d o i . o r g / 1 0 . 1 1 7 7 /  
0146167293196003 
Clarke, J., Chen, H., Du, D., & Hu, Y. J. (2020). Fake news, 
investor attention, and market reaction. Information 
Systems Research, 32(1), 35–52. https://doi.org/10.1287/ 
isre.2019.0910 
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
57

Delobelle, P., Cunha, M., Cano, E. M., Peperkamp, J., & 
Berendt, B. (2019). Computational ad hominem 
detection. Paper presented at the proceedings of the 57th 
annual meeting of the association for computational lin­
guistics, Florence, Italy: Student Research Workshop.
Di Domenico, G., & Visentin, M. (2020). Fake news or true 
lies? Reflections about problematic contents in 
marketing. International Journal of Market Research, 62 
(4), 409–417. https://doi.org/10.1177/1470785320934719 
DiFonzo, N., & Bordia, P. (2007). Rumor psychology: Social 
and organizational approaches. American Psychological 
Association.
Dinev, T., Xu, H., Smith, J. H., & Hart, P. (2013). 
Information privacy and correlates: An empirical attempt 
to bridge and distinguish privacy-related concepts. 
European Journal of Information Systems, 22(3), 
295–316. https://doi.org/10.1057/ejis.2012.23 
Dowden, B. (1993). Logical reasoning: Wadsworth 
Publishing Co Inc.
Dowden, B. (2019). Fallacies. The Internet Encyclopedia of 
Philosophy. https://iep.utm.edu/fallacy/ 
Dyachenko, T., Reczek, R. W., & Allenby, G. M. (2014). 
Models of sequential evaluation in best-worst choice 
tasks. Marketing Science, 33(6), 828–848. https://doi.org/ 
10.1287/mksc.2014.0870 
Egelhofer, J. L., & Lecheler, S. (2019). Fake news as a 
two-dimensional phenomenon: A framework and 
research 
agenda. 
Annals 
of 
the 
International 
Communication Association, 43(2), 97–116. https://doi. 
org/10.1080/23808985.2019.1602782 
Enders, A. M., Uscinski, J. E., Seelig, M. I., Klofstad, C. A., 
Wuchty, S., Funchion, J. R., Stoler, J. (2021). The relation­
ship between social media use and beliefs in conspiracy 
theories and misinformation. Political Behavior, 1–24. 
doi:10.1007/s11109-021-09734-6.
Faragó, L., Kende, A., & Krekó, P. (2020). We only believe in 
news that we doctored ourselves. Social Psychology, 51(2), 
77–90. https://doi.org/10.1027/1864-9335/a000391 
Fearnside, W. W., & Holther, W. B. (1959). Fallacy. 
Prentice-Hall.
Finn, A., & Louviere, J. J. (1992). Determining the appro­
priate response to evidence of public concern: The case of 
food safety. Journal of Public Policy and Marketing, 11(2), 
12–25. https://doi.org/10.1177/074391569201100202 
Fiske, S. T., & Taylor, S. E. (2013). Social cognition: From 
brains to culture. Sage.
Floridi, L. (2009). Logical fallacies as informational 
shortcuts. Synthese, 167(2), 317–325. https://doi.org/10. 
1007/s11229-008-9410-y 
Gardner, D. M. (1975). Deception in advertising: 
A conceptual approach. Journal of Marketing, 39(1), 40– 
46. https://doi.org/10.2307/1250801 
Gilbert, D. T., Krull, D. S., & Malone, P. S. (1990). 
Unbelieving the unbelievable: Some problems in the 
rejection of false information. Journal of Personality and 
Social Psychology, 59(4), 601–613. https://doi.org/10. 
1037/0022-3514.59.4.601 
Google Trends. (2021). https://trends.google.de/trends/ 
explore?date=all&geo=DE&q=fake%20news 
Gregor, S. (2006). The nature of theory in information 
systems. Management Information Systems Quarterly, 30 
(3), 611–642. https://doi.org/10.2307/25148742 
Habernal, I., & Gurevych, I. (2017). Argumentation mining 
in user-generated web discourse. Computational 
Linguistics, 43(1), 125–179. https://doi.org/10.1162/ 
COLI_a_00276 
Hancock, J. T. (2007). Digital deception: Why, when and 
how people lie online. Adam Johnson. Oxford Handbook 
of Internet Psychology. Oxford: Oxford Handbook of 
Internet Psychology, 289–301. https://doi.org/10.1093/ 
oxfordhb/9780199561803.013.0019 
Hauser, J., Eggers, F., & Selove, M. (2019). The strategic 
implications of scale in choice-based conjoint analysis. 
Marketing Science, 38(6), 1059–1081. https://doi.org/10. 
1287/mksc.2019.1178 
Hinz, O., Schlereth, C., & Zhou, W. (2015). Fostering the 
adoption of electric vehicles by providing complemen­
tary mobility services: A two-step approach using best– 
worst scaling and dual response. Journal of Business 
Economics, 85(8), 921–951. https://doi.org/10.1007/ 
s11573-015-0765-5 
Johnston, A. C., Werkentin, M., McBride, M., & Carter, L. 
(2016). Dispositional and situational factors: Influences 
on information security policy violations. European 
Journal of Information Systems, 25(3), 231–251. https:// 
doi.org/10.1057/ejis.2015.15 
Jones-Jang, S. M., Mortensen, T., & Liu, J. (2021). Does 
media literacy help identification of fake news? 
Information literacy helps, but other literacies don’t. 
TheAmerican Behavioral Scientist, 65(2), 371–388. 
https://doi.org/10.1177/0002764219869406 
Kahneman, D., & Egan, P. (2011). Thinking, fast and slow 
(Vol. 1). Farrar, Straus and Giroux New York.
Kaufmann, L., Rottenburger, J., Carter, C. R., & Schlereth, C. 
(2018). 
Bluffs, 
lies, 
and 
consequences: 
A reconceptualization of bluffing in buyer-supplier 
negotiations. Journal of Supply Chain Management, 54 
(2), 49–70. https://doi.org/10.1111/jscm.12155 
Kim, A., & Dennis, A. R. (2019). Says who? The effects of 
presentation format and source rating on fake news in 
social 
media. 
Management 
Information 
Systems 
Quarterly, 43(3), 1025–1039. https://doi.org/10.25300/ 
MISQ/2019/15188 
Kim, A., Moravec, P. L., & Dennis, A. R. (2019). Combating 
fake news on social media with source ratings: The effects 
of user and expert reputation ratings. Journal of 
Management Information Systems, 36(3), 931–968. 
https://doi.org/10.1080/07421222.2019.1628921 
Knapp, R. H. (1944). A psychology of rumor. Public Opinion 
Quarterly, 8(1), 22–37. https://doi.org/10.1086/265665 
Laato, S., Islam, A. N., Islam, M. N., & Whelan, E. (2020). 
What drives unverified information sharing and cyberch­
ondria during the COVID-19 pandemic? European 
Journal of Information Systems, 29(3), 288–305. https:// 
doi.org/10.1080/0960085X.2020.1770632 
Lazer, D. M. J., Baum, M. A., Benkler, Y., Berinsky, A. J., 
Greenhill, K. M., Menczer, F., Metzger, M. J., Nyhan, B., 
Pennycook, 
G., 
Rothschild, 
D., 
Schudson, 
M., 
Sloman, S. A., Sunstein, C. R., Thorson, E. A., 
Watts, D. J., & Zittrain, J. L. (2018). The science of fake 
news. Science, 359(6380), 1094–1096. https://doi.org/10. 
1126/science.aao2998 
Lecheler, S., & Kruikemeier, S. (2016). Re-evaluating 
journalistic routines in a digital age: A review of 
research on the use of online sources. New Media & 
Society, 18(1), 156–171. https://doi.org/10.1177/ 
1461444815600412 
Lewandowsky, S., Ecker, U. K., Seifert, C. M., Schwarz, N., & 
Cook, J. (2012). Misinformation and its correction: 
Continued 
influence 
and 
successful 
debiasing. 
Psychological Science in the Public Interest, 13(3), 
106–131. https://doi.org/10.1177/1529100612451018 
58
S. BEISECKER ET AL.

Li, Y., Thomas, M. A., & Liu, D. (2021). From semantics to 
pragmatics: Where IS can lead in Natural Language 
Processing (NLP) research. European Journal of 
Information Systems, 30(5), 569–590. https://doi.org/10. 
1080/0960085X.2020.1816145 
Louviere, J., Flynn, T., & Marley, A. (2015). Best-Worst 
scaling: Theory, methods and applications. Cambridge 
University Press. https://doi.org/10.1017/ 
CBO9781107337855 
Louviere, J., Lings, I., Islam, T., Gudergan, S., & Flynn, T. 
(2013). An introduction to the application of (case 1) 
best–worst scaling in marketing research. International 
Journal of Research in Marketing, 30(3), 292–303. https:// 
doi.org/10.1016/j.ijresmar.2012.10.002 
Madon, H., Fadzil, I. L. M., & Rahmat, N. H. (2021). 
A rhetorical analysis of news article on work from 
home. European Journal of Applied Linguistics Studies, 3 
(2), 22–36. https://doi.org/10.46827/ejals.v3i2.239 
Martel, C., & Pennycook, G. (2020). Reliance on emotion 
promotes belief in fake news. Cognitive Research: 
Principles and Implications, 5(47), 1–20. https://doi.org/ 
10.1186/s41235-019-0201-4 
Mazzeo, V., Rapisarda, A., & Giuffrida, G. (2021). Detection 
of fake news on Covid-19 on web search engines. 
Frontiers in Physics, 9(685730). https://doi.org/10.3389/ 
fphy.2021.685730 
Moravec, P. L., Kim, A., & Dennis, A. R. (2020). Appealing 
to sense and sensibility: System 1 and system 2 interven­
tions for fake news on social media. Information Systems 
Research, 31(3), 987–1006. https://doi.org/10.1287/isre. 
2020.0927 
Moravec, P. L., Minas, R. K., & Dennis, A. R. (2019). Fake 
news on social media: People believe what they want to 
believe when it makes no sense at all. Management 
Information Systems Quarterly, 43(4), 1343–1360. 
doi:10.2139/ssrn.3269541.
Mueller Loose, S., & Lockshin, L. (2013). Testing the robust­
ness of best worst scaling for cross-national segmentation 
with different numbers of choice sets. Food Quality and 
Preference, 27(2), 230–242. https://doi.org/10.1016/j. 
foodqual.2012.02.002 
Mueller, S., Lockshin, L., & Louviere, J. J. (2010). What you 
see may not be what you get: Asking consumers what 
matters may not reflect what they choose. Marketing 
Letters, 21(4), 335–350. https://doi.org/10.1007/s11002- 
009-9098-x 
Nickerson, R. S. (1998). Confirmation bias: A ubiquitous 
phenomenon in many guises. Review of General 
Psychology, 2(2), 175–220. https://doi.org/10.1037/1089- 
2680.2.2.175 
Pennycook, G., Bear, A., Collins, E. T., & Rand, D. G. 
(2020). The implied truth effect: Attaching warnings 
to a subset of fake news headlines increases perceived 
accuracy of headlines without warnings. Management 
Science, 66(11), 4944–4957. https://doi.org/10.1287/ 
mnsc.2019.3478 
Pennycook, G., Cannon, T. D., & Rand, D. G. (2018). Prior 
exposure increases perceived accuracy of fake news. 
Journal of Experimental Psychology: General, 147(12), 
1865–1880. https://doi.org/10.1037/xge0000465 
Pennycook, G., & Rand, D. G. (2021). The psychology of 
fake news. Trends in Cognitive Sciences, 25(5), 388–402. 
https://doi.org/10.1016/j.tics.2021.02.007 
Pennycook, G., & Rand, D, G. (2018). Who falls for fake 
news? The roles of analytic thinking, motivated reason­
ing, political ideology, and bullshit receptivity. Working 
Paper.
Picchi, A. (2016). Fake news spurs Trump backers to boycott 
PepsiCo CBS News . https://www.cbsnews.com/news/ 
trump-supporters-boycott-pepsico-over-fake-ceo-reports/ 
Podsakoff, P. M., MacKenzie, S. B., Lee, J.-Y., & 
Podsakoff, N. P. (2003). Common method biases in beha­
vioral research: A critical review of the literature and 
recommended remedies. Journal of Applied Psychology, 88 
(5), 879–903. https://doi.org/10.1037/0021-9010.88.5.879 
Ricco, R. B. (2007). Individual differences in the analysis of 
informal reasoning fallacies. Contemporary Educational 
Psychology, 32(3), 459–484. https://doi.org/10.1016/j. 
cedpsych.2007.01.001 
Rubin, V. L. (2017). Deception detection and rumor 
debunking for social media. In Sloan, L., Quan-Haase, 
A.(Eds.), The SAGE Handbook of Social Media Research 
Methods. London: SAGE. https://uk.sagepub.com/en-gb/ 
eur/the-sage-handbook-of-social-media-research-meth 
ods/book245370 
Schlereth, C., & Skiera, B. (2012). , DISE: Dynamic intelli­
gent survey engine. In A.Diamantopoulos, W.Fritz, & L. 
Hildebrandt (Eds.), Quantitative marketing and market­
ing management - festschrift in honor of Udo Wagner (pp. 
225–243). Gabler Verlag.
Schlereth, C., & Skiera, B. (2017). Two new features in 
discrete choice experiments to improve willingness to 
pay estimation that result in new methods: Separated 
(adaptive) dual response. Management Science, 63(3), 
829–842. https://doi.org/10.1287/mnsc.2015.2367 
Schuetz S W, Sykes T Ann and Venkatesh V. (2021). 
Combating COVID-19 fake news on social media 
through fact checking: antecedents and consequences. 
European Journal of Information Systems, 30(4), 376– 
388. 10.1080/0960085X.2021.1895682
Shearer, E. (2021). More than eight-in-ten Americans get 
news from digital devices. Pew Research Center. https:// 
www.pewresearch.org/fact-tank/2021/01/12/more-than- 
eight-in-ten-americans-get-news-from-digital-devices/ 
Shibutani, T. (1966). Improvised news: A sociological study of 
rumor. Ardent Media.
Shu, K., Sliva, A., Wang, S., Tang, J., & Liu, H. (2017). Fake 
news detection on social media: A data mining 
perspective. ACM SIGKDD Explorations Newsletter, 19 
(1), 22–36. https://doi.org/10.1145/3137597.3137600 
Silverman, C. (2016). This analysis shows how viral fake 
election news stories outperformed real news on 
Facebook. BuzzFeed. https://www.buzzfeednews.com/arti 
cle/craigsilverman/viral-fake-election-news- 
outperformed-real-news-on-facebook 
Strack, F., & Deutsch, R. (2004). Reflective and impulsive 
determinants of social behavior. Personality and Social 
Psychology Review, 8(3), 220–247. https://doi.org/10. 
1207/s15327957pspr0803_1 
Tandoc, E. C., Lim, Z. W., & Ling, R. (2018). Defining “fake 
news”. Digital Journalism, 6(2), 137–153. https://doi.org/ 
10.1080/21670811.2017.1360143 
Tversky, A., & Kahneman, D. (1981). Judgments of and by 
representativeness. Cambridge University Press.
Van Eemeren, F. H., Garssen, B., & Meuffels, B. (2009). 
Fallacies and judgments of reasonableness: Empirical 
research concerning the pragma-dialectical discussion 
rules (Vol. 16). Springer Science & Business Media.
Van Eemeren, F. H., Garssen, B., & Meuffels, B. (2010). 
Fallacies and judgments of reasonableness: Empirical 
research concerning the pragma-dialectical discussion 
rules. Information Design Journal, 18(2), 175–177. 
https://link.springer.com/book/10.1007/978-90-481- 
2614-9 
EUROPEAN JOURNAL OF INFORMATION SYSTEMS
59

Van Eemeren, F. H., Grootendorst, R., Johnson, R. H., 
Plantin, C., & Willard, C. A. (2013). Fundamentals of 
argumentation theory: A handbook of historical back­
grounds and contemporary developments. Routledge.
Van Eemeren, F. H., Grootendorst, R., & Snoeck 
Henkemans, A. F. (2002). Argumentation: Analysis, eva­
luation, presentation. Erlbaum.
Visentin, M., Pizzi, G., & Pichierri, M. (2019). Fake news, 
real problem for brands: The impact of content truthful­
ness and source credibility on consumers’ behavioral 
intentions toward the advertised brands. Journal of 
Interactive Marketing, 45(C), 99–112. https://doi.org/10. 
1016/j.intmar.2018.09.001 
Vosoughi, S., Roy, D., & Aral, S. (2018). The spread of true 
and false news online. Science, 359(6380), 1146–1151. 
https://doi.org/10.1126/science.aap9559 
Walton, D. N. (1987). The ad hominem argument as an 
informal fallacy. Argumentation, 1(3), 317–331. https:// 
doi.org/10.1007/BF00136781 
Walton, D. (2009). Argumentation theory: A very short 
introduction. In G. Simari & I. Rahwan (Eds.), 
Argumentation in Artificial Intelligence (pp. 1–22). 
Springer US.
Weiss, A. P., Alwan, A., Garcia, E. P., & Garcia, J. (2020). 
Surveying fake news: Assessing university faculty’s frag­
mented definition of fake news and its impact on teaching 
critical thinking. International Journal for Educational 
Integrity, 16(1), 1–30. https://doi.org/10.1007/s40979- 
019-0049-x 
Wilson, T. D., & Brekke, N. (1994). Mental contamination 
and mental correction: Unwanted influences on judg­
ments and evaluations. Psychological Bulletin, 116(1), 
117–142. https://doi.org/10.1037/0033-2909.116.1.117 
Wineburg, S., McGrew, S., Breakstone, J., & Ortega, T. 
(2016). Evaluating information: The cornerstone of civic 
online reasoning. Stanford Digital Repository, 8, 2018. 
https://www.semanticscholar.org/paper/Evaluating-infor 
mation%3A-The-cornerstone-of-civic-Wineburg- 
McGrew/dbe82774015d3da280a80186d7bc1acd429cbac7 
ZDF. (2021). Die dreistesten Fake News der Geschichte. 
https://www.zdf.de/dokumentation/die-glorreichen-10/ 
die-dreistesten-fake-news-der-geschichte-102.html 
Zhou, X., & Zafarani, R. (2020). A survey of fake news: 
Fundamental 
theories, 
detection 
methods, 
and 
opportunities. ACM Computing Surveys, 53(5), 1–40. 
https://doi.org/10.1145/3395046
60
S. BEISECKER ET AL.

