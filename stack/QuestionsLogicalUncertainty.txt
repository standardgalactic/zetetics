Questions of Reasoning Under Logical Uncertainty
Nate Soares and Benja Fallenstein
Machine Intelligence Research Institute
{nate,benja}@intelligence.org
Abstract
A logically uncertain reasoner would be able to
reason as if they know both a programming lan-
guage and a program, without knowing what
the program outputs. Most practical reasoning
involves some logical uncertainty, but no satis-
factory theory of reasoning under logical uncer-
tainty yet exists. A better theory of reasoning
under logical uncertainty is needed in order to
develop the tools necessary to construct highly
reliable artiﬁcial reasoners. This paper intro-
duces the topic, discusses a number of historical
results, and describes a number of open prob-
lems.
1
Introduction
Consider a black box with one input chute and two
output chutes. The box is known to take a ball in the
input chute and then (via some complex Rube Goldberg
machine) deposit the ball in one of the output chutes.
An environmentally uncertain reasoner does not
know which Rube Goldberg machine the black box im-
plements.
A logically uncertain reasoner may know
which machine the box implements, and may under-
stand how the machine works, but does not (for lack
of computational resources) know how the machine be-
haves.
Standard probability theory is a powerful tool for
reasoning under environmental uncertainty, but it as-
sumes logical omniscience: once a probabilistic reasoner
has determined precisely which Rube Goldberg machine
is in the black box, they are assumed to know which out-
put chute will take the ball. By contrast, realistic rea-
soners must operate under logical uncertainty: we often
know how a machine works, but not precisely what it
will do.
General intelligence, at the human level, mostly con-
sists of reasoning that involves logical uncertainty. Rea-
soning about the output of a computer program, the
Research supported by the Machine Intelligence Research
Institute (intelligence.org). Published as Technical report
2015–1.
behavior of other actors in the environment, or the im-
plications of a surprising observation are all done under
logical (in addition to environmental) uncertainty. This
would also be true of smarter-than-human systems:
constructing a completely coherent Bayesian probabil-
ity distribution in a complex world is intractable. Any
artiﬁcially intelligent system writing software or eval-
uating complex plans must necessarily perform some
reasoning under logical uncertainty.
When constructing smarter-than-human systems,
the stakes are incredibly high:
superintelligent ma-
chines could have an extraordinary impact upon hu-
manity (Bostrom 2014), and if that impact is not ben-
eﬁcial, the results could be catastrophic (Yudkowsky
2008). If that system is to attain superintelligence by
way of self-modiﬁcation, logically uncertain reasoning
will be critical to its reliability.
The initial system’s
ability must reason about the unknown behavior of a
known program (the contemplated self-modiﬁcation) in
order to understand the result of modifying itself.
In order to pose the question of whether a prac-
tical system reasons well under logical uncertainty, it
is ﬁrst necessary to gain a theoretical understanding
of logically uncertain reasoning.
Yet, despite signiﬁ-
cant research going back to  Lo´s (1955), Gaifman (1964)
and before, continued by Halpern (2003), Hutter et al.
(2013), Demski (2012), Christiano (2014) and many,
many others,1 this theoretical understanding does not
yet exist.
It is natural to consider extending standard prob-
ability theory to include the consideration of worlds
which are “logically impossible” (such as where a deter-
ministic Rube Goldberg machine behaves in a way that
it doesn’t). This gives rise to two questions: What, pre-
cisely, are logically impossible possibilities? And, given
some means of reasoning about impossible possibilities,
what is a reasonable prior probability distribution over
them?
This paper discusses the ﬁeld of reasoning under log-
ical uncertainty. At present, study into logically uncer-
tain reasoning is largely concerned with the problem
of reasoning probabilistically about sentences of logic.
1. See, e.g., the work referenced by Hutter et al. (2013).
1

Sections 2 and 3 discuss the two problems posed above
in that context. Ultimately, our understanding of log-
ical uncertainty will need to move beyond the domain
of logical sentences; this point is further explored in
Section 4. Section 5 concludes by relating these prob-
lems back to the design of smarter-than-human systems
which are reliably aligned with human interests.
2
Impossible Possibilities
Consider again the black box, with the Rube Goldberg
machine inside.
An agent reasoning using standard
probability theory is environmentally uncertain; they
do not know which Rube Goldberg machine is in the
box.
Probability distributions assign probabilities to
some set of “possibilities,” and when reasoning proba-
bilistically under environmental uncertainty, the set of
possibilities is all Rube Goldberg machines consistent
with observation that could ﬁt in the box.
What is the set of possibilities considered by a logi-
cally uncertain reasoner? They may know which Rube
Goldberg machine is in the box, without knowing how
that Rube Goldberg machine behaves (for lack of de-
ductive capabilities). The machine, following the laws
of logic and physics, deposits the ball in only one of
the two chutes, but a logically uncertain reasoner must
consider both output chutes as a “possibility.” Logically
uncertain reasoning, then, requires the consideration of
logically impossible possibilities.
What sort of objects are logically impossible possi-
bilities? What is the set of all impossible possibilities,
to which probabilities are assigned? In full generality,
this question is vague and intractable. But there is one
setting in which it is natural to consider logical impos-
sibilities, and that is the domain of formal logic itself:
consider agents that are uncertain about the truth val-
ues of sentences of logic. Indeed, the study of logical
uncertainty in the literature centers on reasoning ac-
cording to assignments of probabilities to sentences of
ﬁrst-order logic (Gaifman 2004).
How does reasoning about logical sentences corre-
spond to reasoning under logical uncertainty in the real
world? Sentences of ﬁrst order logic are extremely ex-
pressive: given a description of the Rube Goldberg ma-
chine in the black box, it is possible to construct a
logical sentence which is true if and only if the ma-
chine deposits the ball in the top chute.
A reasoner
uncertain about whether that sentences is true is also
uncertain about the behavior of that Rube Goldberg
machine. Logical sentences can also encode statements
such as “this Turing machine will halt,” or “this func-
tion sorts its input and has time complexity O(log n).”
Thus, while it is ultimately necessary to understand
logically uncertain reasoning as it pertains to observa-
tion and interaction in the real world, it is reasonable to
begin studying reasoning under logical uncertainty by
studying probability assignments to logical sentences.
Picking any probability distribution over logical sen-
tences does not automatically constitute “reasoning un-
der logical uncertainty.” Intuitively, logically uncertain
reasoning must preserve some of the structure between
sentences: if a reasoner assigns probability 1 to φ and
deduces φ →ψ (via some complex implication), then
the reasoner must assign probability 1 to ψ thereafter.
But clearly, not all of the structure between sentences
can be preserved, for that would require logical om-
niscience. Which structure is preserved under logical
uncertainty, and how?
It is illuminating to ﬁrst consider the probabilities
that a reasoner would assign to sentences of logic if
they could preserve all the logical structure. How could
a deductively omniscient reasoner assign probabilities
consistently to logical sentences? It is not so simple as
claiming that omniscient reasoners assign probability 1
to true sentences and 0 to false ones, because logical
sentences are not simply “true” or “false” in a vacuum.
It depends entirely on which logical theory obtains: if
the domain is numbers and “×” is multiplication, then
the sentence ∀a, b: a×b = b×a is true; but if the domain
is vectors and “×” is the vector cross product, then the
same sentence is false. There are, in fact, two types of
logical uncertainty: uncertainty about the logical the-
ory, and uncertainty stemming from limited deductive
capabilities.
The ﬁrst type of logical uncertainty has been stud-
ied for many decades, with early work done by Gaifman
(1964), Hacking (1967), and others. It is not merely a
problem of deﬁning symbols: in logic, there are many
theories which are “incomplete,” meaning that there
exist sentences which are not necessarily either true or
false according to that logical theory. Consider Peano
Arithmetic (PA), which formalizes the natural num-
bers. PA nails down the deﬁnitions of “+” and “×,”
but there are still sentences which are true of the num-
bers but are not implied by the Peano axioms. The clas-
sic example is G¨odel’s sentence, which roughly claims
“PA cannot prove this sentence” (G¨odel, Kleene, and
Rosser 1934). This statement is true, but it does not
follow from the Peano axioms.
A consistent assignment of truth values to every sen-
tence requires a complete logical theory. A complete
theory of logic is a consistent set T of sentences such
that for every sentence φ, either φ ∈T or (¬φ) ∈T. In-
complete theories can be “completed” by starting with
the set of all consequences of the incomplete theory
and then choosing arbitrary (consistent) assignments
of truth for each independent sentence. For example,
there are many diﬀerent ways to complete PA, only
one of which is “true arithmetic.” Identifying true arith-
metic is uncomputable, as statements of true arithmetic
include statements about which Turing machines halt.2
Even a deductively unlimited reasoner—that always be-
lieves ψ whenever it believes both φ and φ →ψ, no
matter how complex and obfuscated the implication—
2. And which oracle machines halt, and which meta-
oracle machines halt, and so on.
2

may have uncertainty about which sentences are true
or false, via uncertainty about which complete theory
is the “real” one.
A logically uncertain but deductively unlimited
reasoner—which knows all consequences of everything
it knows, but does not know the “true” complete theory
of logic—only entertains consistent logically impossible
possibilities. It may not know which logical theory cor-
responds to how sentences act in “the real world,” but
each “possible world” is self-consistent. This provides a
partial answer to the question of the nature of logically
impossible possibilities: if a reasoner is deductively un-
limited, an “impossible possibility” is any complete the-
ory of logic. A “consistent” assignment of probabilities
to sentences, then, corresponds to a probability distri-
bution over complete theories of logic, where the prob-
ability of a sentence is equal to the measure of theories
in which that sentence is true.3 This is the standard
model of logically uncertain reasoning throughout the
literature, used e.g. by Gaifman (1964), Christiano et
al. (2013), and many others.
This is a ﬁne result for deductively unlimited rea-
soners, but the goal is to understand reasoning under
deductive limitations. Deductively unlimited reasoners
reason according to consistent impossible worlds, but
detecting inconsistencies can be a computationally ex-
pensive task. Deductively limited reasoners must enter-
tain inconsistent impossible possibilities. Recent study
of reasoning under logical uncertainty has been pushing
in this direction (Gaifman 2004).
Intuitively, deductively limited reasoners must rea-
son according to “theories” (assignments of truth val-
ues to sentences) that seem consistent so far, discarding
hypotheses as soon as a contradiction is deduced. An
“impossible possibility,” then, could be any assignment
of truth values to logical sentences which does not allow
a short proof of inconsistency. This intuition might be
formalized as follows:
Fix enumerations of sentences and proofs
in ﬁrst order logic. Consider all bit strings
of some huge length, say Ackerman(10100),
and interpret these as assignments of truth
to each sentence (a 1 in the nth position
claims that the nth sentence is true, and a
0 claims it is false). Now search all strings
with length of up to some far larger num-
ber, say Ackerman2(10100), for proofs in PA
that one of these bit strings makes inconsis-
tent claims, and reject any bit string which
is found to be inconsistent by this proce-
dure. Assign probabilities to each remain-
ing bit string (according to some prior); this
may be used to generate an assignment of
probabilities to sentences of length less than
Ackerman(10100) according to the measure
3. There are uncountably many complete theories of ﬁrst
order logic, but a probability distribution over them can be
deﬁned using the machinery of measure theory.
of bit strings which claim that the sentence
is true.
This procedure is wildly impractical, but it does
seem to allow for satisfactory logically uncertain rea-
soning using ﬁnite (if not limited) deduction. Clearly,
the larger the number of sentences (and the larger the
number of proofs searched), the more the remaining
bit strings will resemble a collection of complete theo-
ries which are actually consistent. This process leads
to intuitively “reasonable” logically uncertain beliefs:
no “theories” that admit a short proof of inconsistency
are considered, but “theories” with inconsistencies that
are very diﬃcult to deduce may remain. This process
corresponds to a ﬁnite version of considering all com-
plete theories: it considers bit strings assigning truth
values to many sentences, for which there are no rea-
sonably sized proofs of contradiction; deductively un-
limited reasoning considers bit strings assigning truth
values to all sentences, for which there is no proof of
contradiction at all. However, for all that this technique
seems intuitively nice, no precise statements about its
performance have yet been proven.
These techniques shed light on the nature of impos-
sible possibilities in the context of deductively limited
reasoning: an impossible possibility is any assignment
of truth values to logical sentences which has not yet
been proven inconsistent, in some fashion.
That is,
practical agents may entertain contradictory possibil-
ities, so long as the possibility is discarded once the
contradiction is deduced; an “impossible possibility”
is any assignment of truth to logical sentences which
hasn’t been found to be consistent so far. It is an open
problem to develop more practical techniques than the
one above which allow agents to reason as if according
to truth-assignments which have “not yet been found to
be inconsistent.” For further discussion, see Christiano
(2014).
While this answer is somewhat satisfactory, it only
answers the question of impossible possibilities as it re-
lates to uncertainty over logical sentences.
Realistic
reasoning under logical uncertainty will require more
than just an ability to assign probabilities to logical
sentences; discussion of this point is relegated to Sec-
tion 4.
3
Logical Priors
In the context of uncertainty about logical sentences,
deductively limited reasoners must approximate rea-
soning according to some probability distribution over
complete theories. This gives rise to a second question:
which probability distribution over complete theories
should they approximate?
Of course, the answer is in part up to us: if we design
a system which reasons about the probabilities of logi-
cal sentences, which takes questions in the form of sen-
tences and outputs predictions in the form of probabil-
ities, then the question of what logical theory it should
3

use depends entirely upon how we want the questions
to be interpreted. For example, if a deductively limited
system is built to help its designers reason about some
Boolean algebra, then the “distribution over complete
theories” might be some sort of simplicity prior which
assigns probability 1 to the complete theory of Boolean
algebras.
But what if the system is intended to reason accord-
ing to some extremely powerful theory of logic (e.g. PA)
which is capable of expressing many questions about the
real world (e.g. whether the Rube Goldberg machine
deposits the ball into the top chute), but for which we
do not know the preferred single complete theory (e.g.
because the complete theory answers all halting prob-
lems)? Then what distribution over complete theories
should be used? If the system is supposed to reason
according to true arithmetic, then what initial state
of knowledge captures our beliefs about that uncom-
putable theory?
This is the problem of logical priors.
Intuitively,
the problem may seem easy: just choose a maximum
entropy (or otherwise weak) prior. Unfortunately, it is
not obvious how to construct a weak prior over complete
theories. Starting with a maximum entropy prior on
logical sentences and reﬁning towards consistency will
not suﬃce: a prior which assigns 50% probability to
every sentence places zero probability mass on the set of
all complete theories, because there are inﬁnitely many
contradictory sentences, and so any inﬁnite sequence of
sentences generated by this prior is guaranteed to select
at least one contradiction eventually.
Hutter et al. (2013) make an early attempt to an-
swer the ﬁrst question, by deﬁning a logical prior in
terms of a probability distribution over sentences which
assigns positive probability to all consistent sentences
and zero probability to contradictions. A probability
distribution of this form allows for the deﬁnition of a
satisfactory logical prior:
The Hutter prior:
For each sentence,
select a model in which that sentence is
true, and in which certain desirable proper-
ties hold (the “Gaifman condition” and the
“Cournot condition” (Hutter et al. 2013)).
Add the complete theory of that model to
the distribution with measure in proportion
to the probability of the sentence.
This prior has many desirable properties, but it can-
not be computably approximated: the conditions that
Hutter demands of each model (which yield the prior’s
nice properties) rely on the high-powered machinery of
set theory, and it is not possible to computably ap-
proximate this prior. That is, there does not exist a
computable process reﬁning assignments of probabili-
ties to sentences which converges on the assignments of
Hutter’s prior in the limit (Sawin and Demski 2013).4
4. Remember that a probability distribution over com-
plete theories can be treated as a probability distribution
The Hutter prior yields insight into what constitutes
a desirable prior, but a study of logical uncertainty in
deductively limited systems requires that the prior be
approximable.
Just as deductively limited reasoners
must approximate reasoning about consistent theories
(by entertaining inconsistent “theories” until a contra-
diction is deduced), so must deductively limited rea-
soners start with a prior that does not quite match the
(inevitably intractable) intended prior, and then reﬁne
those probabilities as they reason.
But this process of starting with an incoherent prior
(which places probability mass on inconsistencies) and
reﬁning it towards some desirable prior (eliminating in-
consistencies as contradictions are deduced, and shift-
ing probability mass to better correspond with the
“true” prior) is precisely the problem of reasoning un-
der logical uncertainty, entire!
That is, the approximation of a satisfactory logical
prior exhibits, in miniature, all the problems of rea-
soning according to a probability distribution over sen-
tences. Thus, the deﬁnition of a satisfactory approx-
imable logical prior, and the study of its approxima-
tions, may yield solutions to the problem of reasoning
under logical uncertainty more generally.
Unfortunately, it is not entirely clear what it would
mean for an approximable logical prior to be “satisfac-
tory,” and na¨ıve attempts at constructing computably
approximable logical priors have all had undesirable
properties.
Demski (2012) proposes a computably approximable
prior that can be generated from any distribution Φ over
all sentences. A “generator” is used to generate com-
plete theories (by drawing sentences at random from
Φ), and Demksi’s prior assigns probability to a theory
T according to the probability that the generator would
generate T.
More formally, Demski’s generator is given by Al-
gorithm 1. It takes an initial set B of known sentences
and a distribution Φ over sentences.
It constructs a
complete theory T by starting with B and selecting
sentences φ at random from Φ. It either adds φ to T
(if φ is consistent with T) or adds ¬φ to T (otherwise).
For example, let the base theory be Peano Arith-
metic (PA), and let Φ be a simplicity prior over sen-
tences which assigns each sentence φ probability 2−|φ|
where |φ| is the length5 of φ.
Clearly, the simplic-
ity prior does not describe a satisfactory logical prior
over sentences, as it puts signiﬁcant probability mass
on short contradictory sentences such as “0 = 1”. Dem-
ski’s generator, however, only generates consistent the-
ories, and therefore, it places probability 0 on all con-
tradictions. Similarly, because all sentences of PA are
over sentences which assigns probability to sentences in ac-
cordance with the proportion of theories in which that sen-
tence is true.
5. Where sentences are encoded in binary, preferably us-
ing some encoding of length where the length of φ is the
same as the length of ¬φ, so that the prior is not biased in
disfavor of negations.
4

Algorithm 1: The Demski generator
Data: A probability distribution Φ over
sentences
Data: A base theory B of known sentences
Result: A complete theory T
begin
T ←−B
loop
φ ←−genrandom(Φ)
if T ∪{ φ } is consistent then
T ←−T ∪{ φ }
else
T ←−T ∪{ ¬φ }
included in B, the prior only generates theories T con-
sistent with PA, and so it assigns probability 1 on all
sentences implied by PA. Now consider a sentence φ
which is independent of PA: the probability of this
sentence depends upon how often Demski’s generator
generates a theory T in which φ is true. Clearly, this
probability is positive, as with probability 2−|φ|, φ will
be the ﬁrst random sentence added to T. Similarly, be-
cause there is a chance that the ﬁrst random sentence is
¬φ, the probability of φ will not be 1. Thus, Demski’s
prior deﬁnes a probability distribution over complete
theories extending PA.
While Demski’s prior is uncomputable, Demski
(2012) has shown that the resulting prior probability
distribution is computably approximable: There is a
computable procedure which will output successive ap-
proximations of the probability of a sentence φ, con-
verging in the limit to the probability assigned to φ
by the uncomputable procedure. Even this computable
approximation, however, is not a tractable algorithm;
recently, Christiano (2014) has proposed an alternative
approach to constructing priors which borrows from
standard machine learning techniques, making it more
likely that the priors developed in this way can be used
in realistic algorithms.
These priors, however, have some undesirable prop-
erties. For example, starting with B as the empty set,
Demski’s prior places zero probability on the set of com-
plete theories where PA holds.6
Agents approximat-
ing Demski’s would not be able to learn Peano Arith-
metic: Demski’s prior, while approximable, is not weak
enough.
In order for a reasoner using Demski’s prior to be-
lieve PA, it must be included in the base theory B. This
reveals a related issue: there are two diﬀerent ways to
“update” Demski’s prior on a sentence φ. The prior can
either be completely regenerated from the base theory
6. Speciﬁcally, Demski’s prior places zero probability
mass on any theory which is not ﬁnitely axiomatizable. The
induction schema of PA consists of inﬁnitely many axioms,
all independent of each other. With probability 1, Demski’s
generator will eventually select the negation of one of these
axioms from Φ.
B ∪{ φ }, or it can be conditioned on φ (by remov-
ing all theories in which φ is false). These two diﬀerent
updates result in two diﬀerent posterior probability dis-
tributions.
Consider the posterior probability of a sentence ψ
such that both ψ and ¬ψ are consistent with φ. If the
prior is regenerated from B ∪{ φ }, then the resulting
posterior still places at least 2−|ψ| probability on ψ,
because this is the probability that ψ is the ﬁrst sen-
tence selected at random from Φ. But if the prior is
conditioned on φ, then it may be the case that the pos-
terior probability of ψ is arbitrarily low. For example,
if ¬ψ →φ, then all theories with ¬ψ will have φ, and
if it is also the case that almost all theories with ψ also
contain ¬φ, then the posterior probability may place
arbitrarily small positive probability on ψ.
In other
words, conditioning the prior on φ favors explanations
for φ, while regenerating the prior does not alter the
Φ-based lower bound probability of any sentence that
does not directly contradict φ.
This double update is strange. An agent reasoning
using Demski’s prior would treat facts that it “learns”
(through observation and conditioning) diﬀerently from
facts that it “always knew” (sentences from the base
theory). This phenomenon is not well understood. Why
does the double update occur? Is it undesirable? Can
it be avoided? These questions remain open, and it is
possible that answers to these questions will lend insight
into the generation of satisfactory approximable logical
priors.
It is not at all clear what it would mean for a logical
prior to be “satisfactory,” in the ﬁrst place: part of
the problem is that it is not yet clear what desiderata
to demand from a logical prior. Candidate desiderata
include:
1. Coherence: A prior P(·) is coherent if it is a
probability distribution over complete theories.
(This requires P(φ) = 1 −P(¬φ), and so on.)
2. Computable approximability: A prior P(·) is
computably approximable if there is an algorithm
which computes an approximation of the proba-
bility which P assigns to φ that converges to P(φ)
in the limit.
3. The Occam property:
A prior has the Oc-
cam property if there exists a length-based lower
bound on the probability of any consistent sen-
tence.
4. Inductive: A prior is inductive if its probabil-
ity for sentences of the form ∀n.ψ(n) goes to 1
as it conditions on more and more (going to all)
conﬁrmations of ψ(·).
5. PA-weakness: A prior is PA-weak if it places
non-zero probability mass on the set of complete
extensions of PA.
5

6. Bounded regret: It may be desirable to show
that a prior has regret (in terms of log loss or some
similar measure) at most a constant worse than
any other probability distribution over complete
theories.
7. Practicality: The more tractable the algorithm
which approximates a prior, the more practical
that prior is.
8. Reﬂectivity: A prior P(·) is reﬂective if there is
some symbol P in the logical language which can
be interpreted as a representation of P(·), such
that P(·) assigns accurate probabilities to state-
ments about P.
Coherence is an extremely desirable property; while ap-
proximations of a logical prior must be incoherent, it is
prudent to demand coherence in the distribution being
approximated. Reﬂectivity has been shown to be pos-
sible (up to inﬁnitesimal error) (Christiano et al. 2013)
but diﬃcult to do in a satisfactory manner (Fallenstein
2014).
Hutter’s prior is coherent, inductive, and PA-weak.
It has the Occam property so long as the probability
distribution which it is generated from has the Occam
property; but it is not computably approximable and
it is far from practical. By contrast, Demski’s prior is
coherent and computably approximable, and has the
Occam property if Φ does, but lacks most other desir-
able properties.
Inductivity has been suggested in the literature,
where it is better known as the Gaifman condition
(Gaifman 1964). Roughly, the Gaifman condition re-
quires that if P(·) is a logical prior and φ is a true Π17
sentence of the form ∀n: ψ(n), then if P(·) is condi-
tioned on the truth values of the ﬁrst N instances
of ψ(n), then as N goes to inﬁnity, P(φ) tends to 1.
In other words, the Gaifman condition requires that if
a reasoner learns that ψ(n) is true for more and more n,
then it eventually become arbitrarily conﬁdent that it
is true for all n. However, computably approximable
probability distributions which satisfy the Gaifman con-
dition must assign probability 0 to some true Π2 sen-
tences (Sawin and Demski 2013),8 which seems strongly
undesirable: computably approximable priors that sat-
isfy the Gaifman condition are not suﬃciently “weak”;
they prevent reasoners from deducing true sentences.
Hahn (2013) reports on an investigation of a more
involved desirable property: If φ(·) is a generic predi-
cate symbol (that is, the initial set of axioms makes no
claims about φ(·)), and if the prior is conditioned on the
statement that φ(n) is true for exactly 90% of the ﬁrst
10100 natural numbers, then the posterior probability
7. φ is a Π1 sentence if it can be written in the form
∀n: ψ(n) and there is a primitive recursive function which
takes n as input and computes whether ψ(n) is true or false.
8. φ is a Π2 sentence if it can be written in the form
∀m. ∃n. ψ(m, n), where ψ(m, n) is primitive recursive.
of φ(0) should be (approximately) 0.9. (This captures
some of the intuition that a tractable algorithm will
assign probability ≈0.1 that the (10100)th digit of π is
a 7, a desideratum that is diﬃcult to formalize.) Unfor-
tunately, even this property appears to be very diﬃcult
to obtain, and we are not aware of any proposed logical
priors that have been shown to possess this property.
In large part, generating logical priors is diﬃcult
because it’s not yet clear what properties such a prior
should possess, nor which properties are possible. Con-
tinued investigation into well-behaved logical priors is
warranted, as the development of satisfactory com-
putably approximable logical priors promises insight
into problems of reasoning under logical uncertainty
more generally.
4
Beyond Logical Sentences
A study of logical uncertainty with respect to sentences
of ﬁrst-order logic has proven insightful, but even if a
practically approximable prior distribution over com-
plete theories were deﬁned, it would not provide a full
theoretical understanding of reasoning under logical un-
certainty in practice.
Ultimately, logical sentences are not the right tool
for reasoning about the behavior of objects in the real
world.
It is possible to construct a logical sentence
which is true if and only if the Rube Goldberg ma-
chine deposits the ball in the top chute, but this sen-
tence would be long and awkward. The manipulations
that are easy to do to the sentence don’t obviously
correspond to realistic reasoning shortcuts about Rube
Goldberg machines. Realistic reasoning under logical
uncertainty will likely require hierarchical and context-
ﬁlled models of the problem. It is possible that these
things could be built atop practical methods for rea-
soning according to a probability distribution over logi-
cal sentences, but the ability to reason with uncertainty
about the truth-values of logical sentences will not solve
these problems directly.
Furthermore, while logical sentences are quite ex-
pressive, it is not clear that sentences of ﬁrst order logic
are the “correct” underlying structure for logically un-
certain reasoning about the world. Practical logically
uncertain reasoning inevitably requires reasoning about
states of reality, and while most simple real-world ques-
tions can be translated into a sentence of ﬁrst-order
logic, it is by no means clear that uncertainty over log-
ical sentences is the best foundations upon which to
build practical reasoning.
By analogy, consider a billiards player who lacks
knowledge of physics. This reasoner would do well to
learn classical mechanics, not because it behooves the
player to start modeling the billiards table in terms
of individual atoms, but because various insights from
classical mechanics apply at the high level of billiards.
But though the billiards player may use knowledge
from classical mechanics in their high-level model of
6

the world, it is not the case that the high-level model
is “merely” a computational expedient standing in for
the “real” atomic model of reality. The atomic model,
too, is simply a model, and one which does not quite
explain all the phenomena in the quantum world of the
billiards player.
We are like the billiards player: our state of knowl-
edge is one where a study into uncertainty over logical
sentences may provide signiﬁcant insight that we can
use to understand logical uncertainty as it pertains to
“high level” objects, but this does not mean that practi-
cal logically uncertain reasoning will be done in terms of
logical sentences, and nor does it mean that practical
logically uncertain reasoning could always be reduced
to uncertainty about logical sentences. It is merely the
case that, given our present state of knowledge, a better
understanding of logical uncertainty in the context of
logical sentences is likely to provide insight into reason-
ing under logical uncertainty more generally.
5
Discussion
Smarter-than-human artiﬁcial systems must do most
reasoning under both logical and environmental uncer-
tainty. If high conﬁdence in this reasoning is to be jus-
tiﬁed, even in a wide array of esoteric situations, then
a theoretical understanding of logically uncertain rea-
soning is necessary: without it, it is diﬃcult to ask the
right questions (Soares and Fallenstein 2014a).
The development of reliable methods for reasoning
under logical uncertainty is work that must be done
in advance of the development of smarter-than-human
systems, if those systems are to be safe. While it may
be possible to delegate signiﬁcant AI research to early
smarter-than-human systems, the creation of reliable
methods for reasoning under logical uncertainty cannot
be delegated, because logically uncertain reasoning is
precisely what the delegatee must use in order to per-
form its research!
How could a smarter-than-human
system be trusted to accurately discover methods for
reasoning under logical uncertainty, while using unreli-
able methods of reasoning under logical uncertainty?
Furthermore, a better theoretical understanding of
logical uncertainty is necessary in order to formal-
ize many open problems related to the alignment of
smarter-than-human systems.
For example, consider
the problem of constructing realistic world models: an
agent faced with learning about the environment in
which it is embedded must reason according to a dis-
tribution over environments that contain the agent.
This problem must be fully described in order to check
whether a practical program implements a solution, but
describing the problem requires a better understanding
of reasoning under logical uncertainty (Soares 2015).
Or consider the problem of counterfactual reason-
ing: formalizing the decision problem faced by an agent
embedded within its environment requires some way
to formalize the problem of agents which may have
an accurate description of their program, but uncer-
tainty about which action it will take. Specifying this
problem, too, requires a better theory of logical uncer-
tainty. In fact, satisfactory decision theory additionally
requires an understanding of logical counterfactuals, the
ability to reason about what “would” happen if a de-
terministic program did something that it doesn’t. It is
likely that a better understanding of logical uncertainty
will yield insight in this domain (Soares and Fallenstein
2014b).
Many existing tools for studying reasoning, such as
game theory, standard probability theory, and Bayesian
networks, all assume that reasoners are logically omni-
scient. If these tools are to be extended and improved, a
better understanding of logical uncertainty is required.
In short, a developed theory of logical uncertainty
would go a long way towards putting theoretical founda-
tions under the study of smarter-than-human systems.
We are of the opinion that those theoretical foundations
are essential to the process of aligning smarter-than-
human systems with the interests of humanity.
References
Bostrom, Nick. 2014. Superintelligence: Paths, Dangers,
Strategies. New York: Oxford University Press.
Christiano, Paul. 2014. Non-Omniscience, Probabilistic In-
ference, and Metamathematics. Technical report 2014–
3. Berkeley, CA: Machine Intelligence Research In-
stitute. http : / / intelligence . org / files / Non -
Omniscience.pdf.
Christiano, Paul F., Eliezer Yudkowsky, Marcello Her-
reshoﬀ, and Mihaly Barasz. 2013. Deﬁnability of
Truth in Probabilistic Logic. Working Paper. Ma-
chine Intelligence Research Institute, Berkeley, CA,
April 2. https : / / intelligence . org / files /
DefinabilityTruthDraft.pdf.
Demski, Abram. 2012. “Logical Prior Probability.” In Ar-
tiﬁcial General Intelligence: 5th International Confer-
ence, AGI 2012, Oxford, UK, December 8–11, 2012.
Proceedings, edited by Joscha Bach, Ben Goertzel, and
Matthew Ikl´e, 50–59. Lecture Notes in Artiﬁcial Intel-
ligence 7716. New York: Springer. doi:10.1007/978-
3-642-35506-6_6.
Fallenstein, Benja. 2014. Procrastination in Probabilistic
Logic. Working Paper. Machine Intelligence Research
Institute, Berkeley, CA. http://intelligence.org/
files/ProbabilisticLogicProcrastinates.pdf.
Gaifman, Haim. 1964. “Concerning Measures in First Order
Calculi.” Israel Journal of Mathematics 2 (1): 1–18.
doi:10.1007/BF02759729.
. 2004. “Reasoning with Limited Resources and
Assigning Probabilities to Arithmetical Statements.”
Synthese 140 (1–2): 97–119. doi:10 . 1023 / B : SYNT .
0000029944.99888.a7.
7

G¨odel, Kurt, Stephen Cole Kleene, and John Barkley
Rosser. 1934. On Undecidable Propositions of Formal
Mathematical Systems. Princeton, NJ: Institute for Ad-
vanced Study.
Hacking, Ian. 1967. “Slightly More Realistic Personal Prob-
ability.” Philosophy of Science 34 (4): 311–325. http:
//www.jstor.org/stable/186120.
Hahn, Jeremy. 2013. Scientiﬁc Induction in Probabilistic
Mathematics. Brief Technical Note. Machine Intelli-
gence Research Institute, Berkeley, CA. http : / /
intelligence . org / files / ScientificInduction .
pdf.
Halpern, Joseph Y. 2003. Reasoning about Uncertainty.
Cambridge, MA: MIT Press.
Hutter, Marcus, John W. Lloyd, Kee Siong Ng, and William
T. B. Uther. 2013. “Probabilities on Sentences in an
Expressive Logic.” Journal of Applied Logic 11 (4):
386–420. doi:10.1016/j.jal.2013.03.003.
 Lo´s, Jerzy. 1955. “On the Axiomatic Treatment of Probabil-
ity.” Colloquium Mathematicae 3 (2): 125–137. http:
//eudml.org/doc/209996.
Sawin, Will, and Abram Demski. 2013. Computable proba-
bility distributions which converge on Π1 will disbelieve
true Π2 sentences. Machine Intelligence Research Insti-
tute, Berkeley, CA, July. http://intelligence.org/
files/Pi1Pi2Problem.pdf.
Soares, Nate. 2015. Formalizing Two Problems of Realis-
tic World-Models. Technical report 2015–3. Berkeley,
CA: Machine Intelligence Research Institute. https:
//intelligence.org/files/RealisticWorldModels.
pdf.
Soares, Nate, and Benja Fallenstein. 2014a. Aligning Su-
perintelligence with Human Interests: A Technical Re-
search Agenda. Technical report 2014–8. Berkeley, CA:
Machine Intelligence Research Institute. https : / /
intelligence.org/files/TechnicalAgenda.pdf.
. 2014b. Toward Idealized Decision Theory. Technical
report 2014–7. Berkeley, CA: Machine Intelligence Re-
search Institute. https://intelligence.org/files/
TowardIdealizedDecisionTheory.pdf.
Yudkowsky, Eliezer. 2008. “Artiﬁcial Intelligence as a Pos-
itive and Negative Factor in Global Risk.” In Global
Catastrophic Risks, edited by Nick Bostrom and Mi-
lan M. ´Cirkovi´c, 308–345. New York: Oxford University
Press.
8

