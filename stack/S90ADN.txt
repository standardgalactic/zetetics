Open Peer Review on Qeios
Thermodynamics, Infodynamics and Emergence
Klaus Jaffe1
1 Universidad Simón Bolivar
Funding: No specific funding was received for this work.
Potential competing interests: No potential competing interests to declare.
Abstract
Emergence, information and energy are fundamental properties of nature. We know that it takes free energy to acquire
information, and it takes information to increment free energy. Energy obeys all laws of thermodynamics, while
information does not. Emergence occurs in dynamic complex systems: when more than one dimension of reality
interacts and novel properties of energy and information emerge. Information can be either useful or not in producing
free energy. Information can reveal itself in different forms (as entropy, physically encoded, mechanical, biological,
structural, in neural or social networks, etc.). Information may increase free energy by reducing entropy in the system,
or by capturing free energy from the surroundings. The interaction between information and energy has been studied
mostly in physical-chemistry and engineering. Now we find it everywhere, including in computer sciences, genetics,
biotechnology, experimental social sciences, and experimental law. In emergent systems new possibilities of increasing
free energy and useful information appear. Emergent complexity is visible in the transitions from subatomic particles to
atoms, from atoms to molecules, to cells, to organisms, to societies and ecosystems. General and simple concepts are
presented to help untangle the forces behind evolutionary processes leading to ever more complexity with more free
energy and useful information, giving birth to life. We need to quantify changes in energy and information to better
understand the dynamics of emergence in complex far-from-equilibrium systems.
Klaus Jaffe
Universidad Simón Bolívar, Caracas
 
Keywords: Information, energy, complexity, synergy, evolution, dissipative structures.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
1/21

Table of Contents
Introduction
   Energy
   Laws of Thermodynamics
   Dissipative systems
   Free Energy
       Helmholtz Free Energy
       Endergonic processes
       Potential Energy
       Biological Energy
       Social Energy
       Cognitive energy
Information
   Complexity and Information:
   Structural Information
   Knowledge and Information
   Negentropy and Information
   Conclusions
Infodynamics and Thermodynamics
   Examples
       - Engine (heat)
       - Cannon (powder and ball)
       - Division of Labor
       - Photosynthesis
       - Life and Sex
       - Socioeconomic and politics
   Conclusion
Multidimensional Systems and Emergence
       - The atom
       - From atoms to molecules
       - From molecules to cells
       - The working of catalysts
       - From cell to organisms
       - From organisms to society and
ecosystems
       - The emergence of emotions
       - The emergence of conscience
   Conclusion
Energy, Information and Emergence
References
 
Introduction
Information and energy are fundamental properties of nature, and emergence is a concept of complex system sciences.
Understanding the thermodynamics of energy, information, and emergence is a necessary step in deepening our
understanding of the dynamics of complex far-from-equilibrium systems. This research has the potential to revolutionize
our understanding of the physical world, and it could lead to the development of new technologies that have the potential
to improve our lives and that of our planet. Only by recognizing the multidimensional nature of information and energy will
we be able to understand the emergence of complexity. Before unraveling these concepts we need to recall relevant
fundamental knowledge of basic natural science to have a shared understanding of it.
Among the Fundamental Physical Forces of Nature we recognize the existence of the following: Inertia, Gravity,
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
2/21

Electromagnetism, the Weak nuclear force and the Strong nuclear force. These forces produce Energy that can be
used to do Work. We can define Energy as the ability to exert a force causing displacement of an object or other kind of
work.
Energy
We recognize the following forms of Energy:
Mechanical Energy - Kinetic Energy energy derived from inertia and gravitational forces
Galileo identified gravity as a force and Newton postulated in physics that, if a body is at rest or moving at a constant
speed in a straight line, it will remain at rest or keep moving in a straight line at constant speed unless it is acted upon
by a force. He called it Inertia. These forces produce kinetic energy whose thermodynamic properties are fairly well
understood. These include the concept of Free Energy, or energy that produces useful work; and Entropy or thermal
energy that dissipates to the surroundings as Heat without producing useful work.
Electromagnetic Energy (Radiation, Light, Heat). Electromagnetic fields produce forces that create energy that are
involved in electromechanical and chemical processes including radiations such as light and x-rays. Electromagnetic
forces bind atoms forming molecules. They work in vision, photosynthesis and in many other vital processes.
Chemical Energy (Oxidation, Fire, ATP, Heat). As an emergent property of the combination of kinetic energy and
electromagnetic forces, chemical energy emerges through the interactions between molecules. Examples include
exothermic chemical oxidation which is expressed as fire; and physiological reactions in living organisms powered by
the controlled oxidation of ATP (Adenosine triphosphate molecules) that produce the energy required to power all living
organisms.
Nuclear Energy, Strong and Weak (radioactivity, Heat). These forces produce energy stored at the nuclear level.
They are responsible for nuclear power and atomic bombs.
Dark Energy (?). Little is known of this energy besides that it probably exist and is used to explain the expansion of our
Universe
These energies interact with matter and these interactions are studied by thermodynamics. Thermodynamics studies the
relationships between heat, work, and energy, and it provides a framework for understanding the behavior of physical
systems. Thermodynamics has been dealing traditionally with systems at equilibrium or near equilibrium. It focused on
closed systems that are isolated from the environment, and on reversible processes that do not produce entropy.
Thermodynamics of open systems, that are far from equilibrium, and experiencing irreversible processes, are much less
known. But these systems are the ones which are more important for humans to understand. Our knowledge of
thermodynamics can be summarized in their laws.
Laws of Thermodynamics
The zeroth law of thermodynamics says temperature is an empirical parameter in thermodynamic systems. It states
the transitive relationship between the temperatures of multiple bodies in thermal equilibrium. The law says: If two
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
3/21

systems are both in thermal equilibrium with a third system, then they are in thermal equilibrium with each other.
Together with the second law it implies that energy can flow spontaneously from high to low temperature systems but
not the other way round.
The first law of thermodynamics is a version of the law of conservation of energy, adapted for thermodynamic
systems. The law of conservation of energy states that the total energy of an isolated system is constant; energy can
be transformed from one form to another, but can be neither created nor destroyed. It can also be stated in the
following form: The energy gained (or lost) by a system is equal to the energy lost (or gained) by its surroundings.
The second law of thermodynamics says that some things can't be undone after they are done. This indicates that
entropy is real. It states that, in an isolated system, entropy can increase but cannot decrease. It can be stated as
follows: Natural processes tend to go only one way, toward less usable energy and more disorder.
The third law of thermodynamics can be stated as: A system's entropy approaches a constant value as its temperature
approaches absolute zero.
Dissipative systems
A proposed law or rule of irreversible thermodynamics seems to apply to open far-from-equilibrium systems with a
structure of minimum dissipation (Prigogine 1977). These systems maintain a stable state thanks to synergic processes
that increase free energy concomitantly with gains in appropriate information. Lets call them synergistic systems.
Empirical evidence from a variety of different systems (Jaffe 2023) suggest that synergistic systems occur at all levels of
organizational complexity. In all known cases, free energy increases are coupled with increases in useful information. No
counter examples have been produced so far. That is, we do not know of any stable systems that increase its free energy
while reducing its useful information. This proposed law might turn out to be a rule, if deduced from other laws and
principles.
Free Energy
Energy can or cannot be used to produce work. Free energy produces useful work. Entropy is the energy that can not be
used to produce work. This work can be mechanical or chemical, but other kinds of work may also exist.
Helmholtz Free Energy
Our thermodynamic understanding of systems driven by kinetic energy is far more advanced than that of other forms of
energy. As the relationship between information and emergence is rather diffuse, we might as well start with kinetic energy
to get an understanding of it. In order to understand the fundamental meaning of information we have to explore the
physical concepts upon which it is built. Basically, we want to understand the deep meaning of the equation:
F = E −TS
Where the abbreviations mean:
F: Free Energy: the energy available in a system to do useful work
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
4/21

E: Total Internal Energy of a system
T: Temperature or the average kinetic energy of the system
S: Entropy. The amount of Total Energy that cannot be used to produce work. In information theory, the entropy of a
random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible
outcomes.
Free energy is a thermodynamic potential that can be used to calculate the maximum amount of work, that may be
performed by a thermodynamically closed system at constant temperature and pressure. But energy exists also in
systems not governed by pressure-volume work. Thus we need to expand the concept of free energy. For example, in a
steam locomotive, E relates to the total energy produced by the fire heating the water to produce vapor, and F refers to
the actual pulling power the locomotive might expert using the vapor pressure. S relates to the energy lost in heat and
other unusable forms during the process. Free energy also provides a necessary condition for processes such as chemical
reactions that may occur under these conditions. In the example of the steam locomotive, the fire may be produced by
burning coal. That requires the oxidation of coal or C + O2 = CO2 + Heat. The direction and extent of chemical change of
this reaction can also be quantified using the free energy. In addition, the transfer of heat from the fire to the water
molecules in order to produce vapor is also a process described by F.
The entropy concept applies to mechanical energy as a loss of energy due to production of heat produced by friction. It is
because a macroscopic collective movement energy is partially distributed to chaotic, thermal movement of molecules. In
thermodynamics transformation of order into disorder is what defines increase of entropy. (Amiri et al 2010). Further
expansions of the concept for free energy and entropy to other kinds of complex phenomena will be attempted here. The
following concepts are relevant for this expansion.
Endergonic processes
Processes that dissipate energy are called exothermic. Those that absorb energy are called endothermic. If what is
dissipated is not heat but some other kind of energy, such as sound or light, we call the processes exergonic or
endergonic. This last term is more general as it considers any kind of energy.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
5/21

Endergonic reactions require a catalyst in order to proceed. This catalyst is a device that contains structural information
(see below). These processes may produce negative entropy as they absorb heat or other kinds of energy from the
environment.
Potential Energy
Potential energy emerges when forces over a system are such that any trigger or change in border conditions can unleash
a torrent of energy. Examples include water dams, weights suspended in the air, chemical compounds that unleash
energy, multidimensional systems that can store different kinds of energy to be used in special occasions such as armed
forces prepared for war. All of these energies can be used to produce work. Thus they classify as Free Energy
Biological Energy
Biological organisms use different types of energy in their workings. The synergistic interactions between physiology and
anatomy produces behaviors that can harness and/or produce energies of different kinds. These energies drive
biochemical reactions and physiological processes and are expressions of free energy
Social Energy
Biological aggregates of cells, organisms and/or ecosystems use different types of energy in its workings. The synergistic
interactions between different components of a social system can harness and/or produce energies of different kinds.
These energies are used by the system to fuel the working of its components. They are free energy.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
6/21

Cognitive energy
Conscious information or knowledge increases due to learning and research. These activities require work to produce the
energy that increases information. Thus free energy can be identified in music and language.
Information
Information is a measure of a characteristic of energy and matter. In physics, information is used to describe the state of a
system. For example, the position and momentum of a particle can be used to describe its state. In biology, information is
used to describe the structure and function of biological systems. For example, the DNA sequence of a gene can be used
to describe its structure, and the protein that is encoded by the gene can be used to describe its function. Thus, several
types of information exist:
Encrypted information such as that encoded in DNA, in music and in Language. Transmissible information is normally
encrypted onto a messenger. It is regarded as noise if the receptor has no clues as to how to decode it.
Negentropy or information related to kinetic energy or negative entropy (Negative entropy is forbidden by the third law
of thermodynamics, although negative changes in entropy are possible. That is S > 0 but ΔS < 0 is possible)
Chemical information inherent in interactions between different types of matter.
Electromagnetic information or waves that interact at large distances
Structural information or border conditions of machines and organisms and that of catalysts or molecules or
structures that modulate chemical reactions or other processes
Networks storing information. Neural networks in a brain, cell networks in an organ, computers, and social networks,
for example.
Spatial-Temporal information that allows synchronizing processes so as to produce work or synergy.
Synergy is the process that uses information to increase free energy
Others
No single tools exist to quantify all of these types of information. Strings of code can be analyzed with simple tools
developed by physicists and information sciences, but they are of no help in quantifying structural information of complex
catalytic molecules. A deeper understanding of the nature of information might help in this endeavor. Even as
quantification of information is an unresolved challenge some attempts to do so include:
Claude Shannon’s 1948 paper "A Mathematical Theory of Communication". In information theory, the entropy of a
random variable is the average level of "information", "surprise", or "uncertainty" inherent to the variable's possible
outcomes. In information theory and statistics, negentropy is used as a measure of distance to normality. Out of all
distributions with a given mean and variance, the normal or Gaussian distribution is the one with the highest entropy.
Kolmogorov A, (1965). Three Approaches to the Quantitative Definition of Information, Problems Inform. Transmission;
is the root of what we know call Kolmogorov complexity. This type of complexity of an object, such as a piece of text, is
the length of a shortest computer program (in a predetermined programming language) that produces the object as
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
7/21

output. It is a measure of the computational resources needed to specify the object, and is also known as algorithmic
complexity, Complexity and information related concepts. Complexity refers to the difficulty of understanding or
describing a system, while information refers to the amount of knowledge that is needed to specify a system
Schwartz, K. (2014) in "On the Edge of Chaos: Where Creativity Flourishes ¨ describes the concept of the "edge of
chaos" as a metaphor for a state of dynamic balance between order and disorder. In this state, systems are able to
adapt and change in response to new information and challenges, while still maintaining a basic level of structure and
stability. This state is often associated with creativity, as it allows people to think outside the box and come up with new
ideas.
Deutsch, D. & Marletto, C (2015). “Constructor theory of information”. The Constructor theory of Information is
expressed solely in terms of which transformations of physical systems are possible and which are impossible - i.e. in
constructor-theoretic terms. It includes conjectured, exact laws of physics expressing the regularities that allow
information to be physically instantiated. Although these laws are directly about information, independently of the
details of particular physical instantiations, information is not regarded as an a priori mathematical or logical concept,
but as something whose nature and properties are determined by the laws of physics alone.
Kolchinsky A., Wolpert D.H. (2021) “Work, Entropy Production, and Thermodynamics of Information under Protocol
Constraint” assumes that the thermodynamics of information in the presence of constraints can be decomposed into
the information acquired in a measurement into “accessible” and “inaccessible” components. This decomposition allows
considering the thermodynamic efficiency of different measurements of the same system, given a set of constraints.
Smith, J. (2000) describes Biological information as a product of Natural selection. Smith E (2008) explores the
“Thermodynamics of natural selection” and proposes how to measure the representation of information in the
biosphere, and the energetic constraints limiting the imposition or maintenance of that information. Biological
information is inherently a chemical property, but is equally an aspect of control flow and a result of processes
equivalent to computation. The aim is a theory of biological information capable of incorporating three characterizations
and their quantitative consequences linking energy and information by considering the problem of existence and
resilience of the biosphere.
Parrondo, J., Horowitz, J. & Sagawa, T. (2015) write about “Thermodynamics of information” Theoretical framework for
the thermodynamics of information based on stochastic thermodynamics and fluctuation theorems,
Varley, T and Hoel E. (2021) present “Emergence as the conversion of information: A unifying theory”. Dimension
reduction (macroscales) can increase the dependency between elements of a system (a phenomenon called "causal
emergence") and complexifies any notion of universal reduction in the sciences, since such reduction would likely lead
to a loss of synergistic information in scientific models.
Kelso, J. A. S. (2021). Unifying Large- and Small-Scale Theories of Coordination. Here coordination is viewed as
information coupling among component parts and processes. This is an alternative view of the process of synergy or of
the production of free energy by coordinated actions.
Rainer, F., Ebeling, W. (2016) tackle information and selforganization and relate it with information and value. It seems
clear that the concept of information permeates all disciplines and that some more rigorous conceptualization of self
organization and information is needed.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
8/21

Haken, H., Portugali. J. (2016) presented Shannon’s information that deals with the quantity of a message irrespective
of its meaning, semantic and pragmatic forms of information that deal with the meaning conveyed by messages, and
information adaptation that refers to the interplay between Shannon’s information and semantic or pragmatic
information.
Gershenson, C., & Fernández, N. (2012) review in “Complexity and information: Measuring emergence, self‐
organization, and homeostasis at multiple scales” the relationship between these concepts. This comprehensive review
on the subject shows that fundamental issues in the relationship between information and thermodynamics remain to
be solved.
None of these approaches leads us to find a universal physical definition of Information. This justifies the present effort to
build one such quantitative description. The description grows from the nature of information as a shadow of energy.
Information becomes quantifiable in the interaction between energy and matter. The definition of Free Energy will guide us
in this endeavor.
Complexity and Information
Complexity is more of a characteristic of information than an independent concept. Complexity refers to the degree of
order or disorder in a system, while information refers to the amount of knowledge that is required to describe a system. In
general, more complex systems require more information to describe. For example, a simple system like a rock can be
described with a few simple properties, such as its size, shape, and color. A more complex system like a human being, on
the other hand, requires much more information to describe, including its physical features, its personality, its memories,
motivations and its thoughts.
The relationship between complexity and information has been nicely explored in biological evolution (Adami et al 2000)
showing that because natural selection forces genomes to behave as a natural ‘‘Maxwell Demon,’’ within a fixed
environment, genomic complexity is forced to increase.
Yet not all increase in complexity leads to an increase in useful information. Longer constitutions with more articles do not
achieve better socioeconomic results of their countries than shorter ones. (Canova 2023). Nor do organisms that have
longer DNA chains in their genome have always a higher complexity than others with less DNA. For example, the
Australian lungfish has a genome with 43 billion base pairs, which is around 14 times larger than the human genome. Few
would consider a lungfish more complex than a human. In general though, genome size is smaller for viruses than for
bacteria, which in turn is smaller than that of vertebrates, etc.
Structural Information
The structure of an enzyme, the arrangements of components of a jet engine, or the architecture of a building carry
information. The information required to build it and the information that it transmits to the processes occurring inside the
structure. This information is directly related to the complexity of the structure. The more complex the structure the more
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
9/21

information it carries and the more information is required to build it. Structural information is also related to the border
conditions of a process. A chemical reaction is modulated by an enzyme that constraints the reaction, and thus imposes
border conditions on it.
Knowledge and Information
Information is the facts or details of a subject, whereas Knowledge is awareness, understanding, or skills that involve this
information. Thus, information and knowledge refer to the same phenomenons. We might thus consider knowledge as
another form of information that can be coded in words or other means, or can be stored in neural or social networks.
Negentropy and Information
The concept of information entropy was introduced by Claude Shannon in his 1948 paper "A Mathematical Theory of
Communication" and is also referred to as Shannon entropy. Shannon's theory defines a data communication system
composed of three elements: a source of data, a communication channel, and a receiver.
Complex systems have many components, each of which with different thermodynamic processes. Thus, properties such
as Entropy (S) may not be uniform. S by definition can not be negative as the third law of thermodynamics states that at T
= 0o K, S = 0. Thus negentropy, although having properties that are opposite to that of S are of a different nature and can
be better called information I that refers to information that increases Free Energy F and produces Synergy.
In physics, the simile of Maxwell’s demon seems to be more appropriate in dealing with the relationship between energy
and information (Maruyama et al 2009).
Conclusions
We need to discern between useful and useless information analogous to our perception of energy which can (Free
energy) or cannot (Entropy) be used to produce useful work. But different kinds of information have to be measured
differently. Shannon information is useful for strings of data and Kolmogorov complexity can be estimated using length of
verbal descriptions or computer algorithms. Structural information can be estimated by the number and diversity of the
parts. Each system might have a peculiar way to measure information content. As we are interested in change of amount
of information, the units are of less importance than the relative change in the estimate or proxy for information used.
Complexity and its different forms is a first choice to estimate information. Information is revealed in many different forms,
such as complexity, knowledge, entropy, structures, order, dynamics, codes and networks, among others. Quantification of
changes of each of these forms is possible, giving rise to infodynamics. This allows us to study the relationship between
infodynamics and thermodynamics in systems of different levels of complexity.
Infodynamics and Thermodynamics
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
10/21

Information may increase free energy as has been stated by Maxwell’s demon (Nature 1867). Real-life versions occur,
and in all cases have their entropy-lowering effects duly balanced by increase of entropy elsewhere. Increases of free
energy due to information are possible in open systems as entropy can exit the system and free energy can enter it. In
these systems, empirical evidence shows that increases in information correlate with increased free energy in a multitude
of different complex systems, from ant colonies to human society, and from music to legal norms (Jaffe 20123).
We might formalize these relationships generalizing Helmholtz equation as follows:
ΔF= ΣΔEi - ΔSe
Where Ei are the different types of energy and Se the entropy due to energetic processes
and ΔΦ = ΣΔIi - ΔSi
Where Φ is useful information or the information that accounts for ΔF,
Ii are the different types of information and Si is noise or information entropy.
This last expression is consilient with Kolchinsky and Wolpert (2021) definition of “accessible” and “inaccessible”
components of information.
Using these abstractions we can write ΔF ~ ΔΦ as proposed by the fourth law (Jaffe 2023), based upon empirical
evidence so far. The exact relation between F and Φ remains to be untangled but one link is the relation between Se and
Si. In energetic terms Se is related to the order or predictability of a system, and so is Si. The problem here is that order
and complexity are related, and these measures depend on the level of complexity addressed. This introduces distortions
when comparing multiple levels or multiple dimensions of energy and information. This relationship means that in order to
increase F there need to be an Increase in I, but not any type of I will do. Information may be misleading, false and/or
destructive provoking a reduction of F. For now, the real effect of I on F can only be assessed empirically. When increases
of F are concomitant to increases in I we may refer to a synergistic process. These distortions do not occur with energy,
as relations between the different forms of energy are much better understood in physics than those of information. Thus I
propose to use Φ as a proxy of useful information, order, complexity, and negative changes in entropy, until better
concepts are developed. The following examples may illustrate the issues.
Examples
Examples for far from equilibrium systems that suffer increase in free energy coupled with increase in information content.
Engine (heat)
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
11/21

All engines produce heat when working. Its combustion process is exothermic. That is, only part of the energy contained in
the fuel is converted to work. Another part is dissipated as heat. In physics, the first part is called Free Energy, the second
part Entropy. During the process F diminishes. The concept of information is not needed to explain the thermodynamic
behavior of this system
Cannon (powder and ball)
A cannon ball placed upon a heap of fire powder will hardly move when the powder is burned. But if the fire powder is
placed into a cannon with a cannon ball on top, the work produced by the flying cannonball after the explosive burning of
the constrained powder is very large indeed. The cannon has more structural information modulating the power liberated
by the burning powder (see also Constructor Theory by Deutsch & Marletto 2015) than the heap of powder. Also an
engine has structural information that converts fuel to power. But not any border conditions or structure will do. The
information relevant to obtain this free energy F we call useful information Φ.
Division of Labor
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
12/21

Already Adam Smith recognized that division of labor confers greater economic capabilities of systems employing it. That
is true for ant and for human societies alike. Evidence at the country level worldwide seems to suggest that more complex
division of labor (more sophisticated technological networks) lead to more economic output (Haussman et al 2014). This
increase in information production by country seems to be based mainly on increases in information in natural sciences
(Jaffe et al 2013). Thus higher Φ produces more F.
Photosynthesis
A clever catalytic arrangement of molecules in an organelle in plant cells called chloroplast transforms light-energy, water
and carbon dioxide into oxygen and energetic organic molecules (glucose). The process. This is an endergonic reaction
that absorbs energy from the environment in the form of light photons and transforms it to chemical energy. It produces
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
13/21

Free Energy F due to its highly complex structural information content Φ.
Life and Sex
Erwing Schrödinger made the famous remark, “What an organism feeds upon is negative entropy”. Referring to the fact
that the organism succeeds in freeing itself from all the entropy it cannot help produce while alive. But it is also often
stated that life feeds on negentropy, which in addition implies it consumes order that it harnesses for its benefit. I
recommend avoiding the term negentropy and use that of Information instead. Information management is a fundamental
requirement for evolution among living organisms (Jaffe 2000) and it allows incremental achievement of synergies that
favor evolution. Specifically, sex achieves increases in genetic information that increases Φ of future generations (Jaffe
2018). Life is a complex system that invented sex and cognition as a means to accelerate evolution to manage increments
of Φ. That is, useful information (ΔΦ) produces increments in free energy (ΔF >0). As ΔF helps access more information
triggering an evolutionary process aiming at ever more complexity and more F and Φ is possible. This process is
analogous to that described as autopoiesis by Maturana and Varela (1991).
Socioeconomic and politics
The more complex and multilevel de system, the more tangled up the information dynamics is. Free energy may be
reduced by lack of adequate information or by the excess of misleading information. Canova et al. (2023) showed that
wordy, long constitutions using many populist words are typical of countries with high infant mortality and a low rule of
law. The opposite relation also holds: countries with short constitutions rank high in socioeconomic indices. This excess of
wrong information may lead to the loss of useful information. An everyday concrete example of this, for me, is the
deterioration of the wealth and the smooth working of a society, due to misleading or wrong information, and dismissal of
scientific information. At the moment of writing this article, an electricity blackout is hindering my connection to the internet
in Caracas (and my ability to cook, to have air-conditioning, hot water, music, etc.). By exploring the deep causes of the
blackout, I discovered that the electricity network of where I live was installed some 80 years ago. Thanks to a political
revolution 20 years ago, most experts and people with knowledge about the electricity network have retired or left the
country (I live in Caracas and write this in May 2023). More impact was the dismissal for political reasons by Hugo Chavez
in 2003 of about 18000 engineers and highly trained personnel for the state oil company. This personnel was substituted
by politically chosen personnel with scant professional qualifications. This brain drain provoked a reduction in oil extraction
in Venezuela from 3.2 million barrels a day in 2002, dropping to 2.5 million in 2008 and to about 0.5 million in 2022. The
same misleading politicized information led to the exile of over 7 million citizens, 2 million of them with university degrees,
causing the collapse of the productive infrastructure of the country and a increase of economic activity in counties
receiving the migrants
Conclusion
Our conclusion based on the sample of examples given above and on 15 more detailed empirical studies presented in
(Jaffe 2023). It takes free energy to acquire information, and it takes information to increment free energy. This is the
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
14/21

meaning of ΔF ~ ΔΦ. This definition is consilient with that given by the Constructor Theory (Deutsch & Marletto 2015).
One way for information to increase free energy is by reducing entropy; but other more direct means are also possible in
open systems. Information and energy are two different physical realities: energy obeys all laws of thermodynamics,
information does not. Information can be structural or otherwise, and its action or connection to energy is studied mostly
by applied sciences such as engineering, computer sciences, genetics, biotechnology, experimental social sciences, and
experimental law.
These conclusions imply that ΔΦ is not necessarily related to ΔSe. That is, more or better information(Φ) may reduce the
production of entropy of a process (increase efficiency) and so increase the free energy of the open system (F); but it
might increase both, kinetic entropy (Se) and free energy (F), but at different rates, by allowing the system to capture more
energy from the environment. But F may only increase if Φ does, when no external flux of energy exists. However
increases of the wrong kind of information might reduce F as the effect of fake news on social harmony and many other
examples attest.
Multidimensional Systems and Emergence
A multidimensional system is a system in which more than one independent variable exists. Possible independent
variables are for example time, color, odor, selection pressure, utility function, consilience, energy, information, etc.. In
multidimensional systems the output often depends on more than one input. Multidimensional systems are used to model
complex phenomena, such as the weather, human behavior, societies and their dynamics, the stock market, and life.
However, most mathematical developments deal with up to 2 to 3 spatial dimensions. Some even include a fourth
dimension: time. But very few include more. String theory includes up to eleven or more dimensions, but all these
dimensions are mathematical constructs and have no known relation to reality.
Structural information emerges as a kind of multidimensional type of information as relevant information can be stored in
different elements in a multidimensional system. A social structure, for example, must account for the different types of
factors affecting its dynamic, some of them based upon very different dimensions. Thus emotions run on different natural
laws than rational thinking, which in turn is dissociated with ecological constraints or with psychological experience. Each
of these factors require a different dimension if we want to have an integral model of the system.
A single organism is dependent on features in multiple dimensions. The anatomical and physical constitution of the
organism limits its possibilities to interact with space, whereas its metabolic structures limit its possibilities to extract order
(to feed) from the environment, and its neurophysiological systems constrain its cognitive capabilities. All these features
run on different dimensions that are required to describe an organism. Other dimensions such as chemical composition,
physical features, energetic requirements, etc., are additional dimensions to be taken into account.
This is also true for the information dimension of any object, including organisms and societies. Interactions between
different dimensions produce synergy and create novel properties of the system Jaffe (2001).
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
15/21

Emergence is a somewhat confusing term. It might refer to order or to complexity. In both cases it relates to information
but in different thermodynamic conditions. A salt solution spontaneously settles to a crystal and thus, a spontaneous order
emerges. But a seed spontaneously develops into a tree and a very much more complex order emerges. Both cases are
referred to as self-organized. During crystallization, the chemical potential of the solution is lost, diminishing S and F of
the system. For systems experiencing Synergy F increase while S decreases. Both phenomena are called ser-organized
emergent order but one process describes equilibrium thermodynamics whereas the other describes far from equilibrium
dissipative structure. Thus, the term selforganization is too general to be useful.
In physics, emergence is the phenomenon of a complex system exhibiting properties that are not present in any of its
individual parts. For example, a colony of ants can exhibit emergent behavior such as collective foraging, even though
each individual ant is simply following its own instincts. This is also true for energy and entropy. In many complex systems
with multiple types of energy, F and S can be hard to measure.. Some kind of energy is an emergent phenomena of the
interactions of other types of energy. A chain of emergent systems can be envisioned as follows:
The atom
Subatomic particles assemble to form atoms. Quantum mechanics and nuclear physics are in charge of studying these
processes. Nuclear forces and electromagnetic interactions are involved in these processes.
From atoms to molecules
Atoms form chemical bonds between them producing ensembles of atoms called chemical compounds. Chemistry is the
science studying these emergent phenomena. Here, nuclear forces play a negligible role and electromagnetic forces are
prevalent.
From molecules to cells
Molecules aggregate in complex ways with a high degree of structural information to form biological cells. Cell biology and
biochemistry is in charge of studying these emergent phenomena. Mechanical and chemical forces are prevalent in the
functioning of these systems.
The working of catalysts
As a fundamental part of the interactions of molecules and cells in achieving emergent phenomena are catalysts, a special
science is dedicated to study them. These catalysts are able to direct mechanical and electromagnetic forces to a small
part of the system allowing for the appearance of modulation of free energy by structural information. They achieve this by
providing spatial-temporal information so as to synchronize processes and reactions that allow synergies to emerge.
From cell to organisms
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
16/21

Cells aggregate in complex ways with a high degree of structural information to form multicellular organisms. Medicine
and organismic biology studies them. Organisms develop cellular systems (brains for example) that form networks of
neurons that can store and process information in ways a single cell can not.
From organisms to society and ecosystems
Sociology and sociobiology study the emergent properties of groups of organisms. Ecologists study the emergent
properties of groups of diverse types of organisms. Here, layers of different structural information guide mechanisms to
harness free energy of diverse forms. Some of these emergent forms are forces and energies that emerge from
interactions of these at lower levels (The most recent paper is Watson & Levin 2023). So we can speak of social and/or
ecological forces that are products of myriads of interactions of subsystems. But such forces, even if they are emergent,
can be measured and their effect on other systems can be monitored. Social networks store and process information in
much larger amounts than individual organisms can, allowing the emergence of culture. Culture can produce machines to
store and use information such as computers.
The emergence of emotions
The interaction between perceived signal from outside of our organism, with neurophysiological signals, activating
networks of neurons and glia, such as action potential of neurons, filtered transmission of neuronal communication, and
hormones that communicate with all parts of our body, are perceived by or proprioceptor producing feelings, some of
which we call emotions. The emergent psychological forces are the drivers of behaviors and drive the production of new
levels of information. Love is a complex emotion that emerges from the interaction of many factors, including memory,
physiology, motivation, culture, personal experience, and more. It is essential for the formation of strong, lasting bonds
between people, which in turn are necessary for the survival and successful development of offspring. Love uses
information to increase free energy and free energy is needed to acquire more information. In colloquial words, Love
requires us to invest energy in our relationships, but this investment pays off in the long run by increasing our
psychological and material well-being. Love has its evolutionary origins in biological reproduction, but it has expanded its
role in human society. In addition to promoting reproduction, love can also foster creativity, innovation, and other adaptive
behaviors. This is because love taps into the same instinctive and cognitive tools that we use for mate selection.
The emergence of conscience
With neural networks and social networks new possibilities emerge, the mind for example (The most recent paper is Levin
2023). The interactions of hormones with physio-electric signals, emotions, neuronal memory, anatomical memory,
complex molecular structures embedding information in synapses, high level information stored in networks of neurons,
superimposed upon networks of glia, proprioceptor, sensory receptors that maintain constant contact with itself and the
environment, produce an emergent properties that we call conscience. Conscience can not exist without any of its
components. A simplified version of consciousness states that it is the capacity to view the position of oneself in the
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
17/21

environment, to forecast, even with errors, the effect of one’s action on the environment and vice versa, and to plan
actions according to a desired goal. If so, most animals have a certain degree of consciousness. Intelligence and Science
is the next step of emergence that accelerates its development with the inclusion of computer networks and artificial
intelligence.
Conclusion
When more than one dimension of reality interacts, novel properties emerge. By knowing the individual properties of
Hydrogen (H) and of Oxygen (O) we can not predict those of water (H2O), despite the fact that water contains only
hydrogen and water.
Each level of complexity draws upon other levels of less complexity. Any science studying the interaction between levels
of complexity must be aware that jumping between levels that are far apart, huge errors in the interpretation of the
knowledge between the different sciences studying each level will emerge. Two examples show that such inter-level
extrapolation in science risks misleading us. Trying to explain consciousness using only knowledge from quantum
mechanics, for example, without ensuring consilience between the sciences studying the intermediate levels of complexity
(Wilson 1997), is sure to produce more noise than knowledge and is best left to charlatans. The other example is
economics. It is clearly a multidimensional dynamic system. Trying to explain economic behavior using simple models
such as Rational Utility Functions, or even apparently more sophisticated tools of experimental economics such as “The
Prisoner's Dilemma”, without attending for emotional dimensions, expectations, social positioning, past experience, moral
dimension, learned attitudes towards strangers, and elements studied by sociobiological ethology, is a dysfunctional
approach most likely to produce only noise.
A difference regarding information between the various levels of emergent complexity is the type of structural information
relevant to produce free energy in each case. The curious fact expressed in a proposition for a fourth law of
thermodynamics, is that at all these levels, increases in free energy are always associated with increases in information.
This is true for processes occurring in open systems that are far from thermodynamic equilibrium. This relationship
between information and energy drives synergies that produce unexpected results, and often new dimensions of
organization emerge.
Energy, Information and Emergence
Summarizing our proposition we redefine:
Emergence is the phenomenon whereby complex systems exhibit properties that are not present in their individual
components, and it provides a framework for understanding the effect of information on the production of free energy in
the system, including those of living systems.
Complex multi-component systems increase their free energy by discovering novel ways for their component parts to
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
18/21

interact between them and with their environment. Novel ways that unleash synergies that augment the free energy of the
system will be selected by evolutionary processes. These novel ways represent new information that must be stored and
transmitted for future interaction of the system. However, there is no receipt to discover useful information. Only by
empirically finding that the information produces useful energy is that we know it is useful. In evolutionary terms, complex
systems need heuristic mechanisms to produce information which then is selected according to its usefulness, or
discarded if shown to be noise.
Thermodynamics, information theory, and emergence are all interconnected but not in a straightforward way. We can
summarize our exercise the conceptual decantation of this relationship to the formula:
ΔF ~ ΔΦ where ΔF = ΣΔEi - ΔSe and ΔΦ = ΣΔIi - ΔSi
This means that we have two coupled realities: that of thermodynamics and that of infodynamics, and both are
transformed by emergence. The different types of energy (Ei) and of information (Ii) in multidimensional systems have to
be identified in order to understand this dynamic relationship. No universal receipt for it exists today. But to advance we
need more efforts in bridging the communication gap between the different disciplines involved in studying these
phenomena so as to accelerate the growth of knowledge we have about these concepts and make them more useful for
eventual practical and theoretical implementations.
Useful information (ΔΦ) produces increments in free energy (ΔF >0). As ΔF helps access more information, an
evolutionary process aiming at ever more complexity and more F and Φ is possible. This helps us to better understand the
evolution of life, societies and ecosystems and makes the creation of independent artificial life feasible.
Research can identify changes in useful information (ΔΦ), producing changes in free energy (ΔF), quantify their
relationship in different complex, open, far from equilibrium systems, and identify modulators and constraints of this
relationship. This may help in focusing on relevant features of these complex systems. What we know so far is that the
proposed law for far-from-equilibrium thermodynamics has not been shown to be false so far. The present conceptual
clarification might help in eventually falsifying this proposal. A better understanding of information might allow us to
deduce this proposed rule from other more fundamental laws. Despite very broad impressive empirical knowledge in many
disciplines, we have only a very superficial grasp of the relationship between information and useful work. Or in abstract
terms between ΔF and ΔΦ. Research in the relation between infodynamics and thermodynamics can change our future!
Acknowledgments
I thank the 20 commentators of my 2023 article in Qeios, Juan Carlos Correra’s suggestions, Cora Wallis for interesting
discussions, and Carlos Gershenson for stimulating me to write this paper. I extensively used the following information
processing software: Google-Scholar, Wikipedia, Bard, Google-Browser and Edge.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
19/21

References
Adami, C., Ofria, C., & Collier, T. C. (2000). Evolution of biological complexity. Proceedings of the National Academy of
Sciences, 97(9), 4463-4468.
Amiri, M., and Michael M. Khonsari (2010). "On the Thermodynamics of Friction and Wear - A Review" Entropy 12, no.
5: 1021-1049. https://doi.org/10.3390/e12051021
Canova A. et al. 2023. Constitutions, Rule of law, Socioeconomicas… and Populism. SSRN-4077340
Deutsch, D. & Marletto, C. (2015) Constructor theory of information. Proceedings of the Royal Society A, 471:
20140540.
Gershenson, C., & Fernández, N. (2012). Complexity and information: Measuring emergence, self‐organization, and
homeostasis at multiple scales. Complexity, 18(2), 29-44.
Haken, H., Portugali. J. (2016) Information and Selforganization: A Unifying Approach and Applications. Entropy 18,
no. 6: 197. https://doi.org/10.3390/e18060197
Hausmann, R., Hidalgo, C. A., Bustos, S., Coscia, M., & Simoes, A. (2014). The atlas of economic complexity: Mapping
paths to prosperity. Mit Press.
Jaffe, K., Caicedo, M., Manzanares, M., Gil, M., Rios, A., Florez, A.,... & Davila, V. (2013). Productivity in physical and
chemical science predicts the future economic growth of developing countries better than other popular indices. PloS
One, 8(6), e66239.
Jaffe, K. (2001) The Scientific Roots of Synergy. ASIN : ​ B083HNQBKJ, arxiv 1707.06662
Jaffe, K. (2000). Emergence and maintenance of sex among diploid organisms aided by assortative mating. Acta
Biotheoretica, 48, 137-147.
Jaffe, K. (2018). Synergy from reproductive division of labor and genetic complexity drive the evolution of sex. Journal
of Biological Physics, 44(3), 317-329.
Jaffe, K (2023) A Law for Irreversible Thermodynamics? Synergy Increases Free Energy by Decreasing Entropy Qeios
ID: 2VWCJG.5
Kelso, J. A. S. (2021). Unifying Large- and Small-Scale Theories of Coordination. Entropy 23, no. 5: 537.
https://doi.org/10.3390/e23050537
Kolchinsky A., Wolpert D.H. (2021) Work, Entropy Production, and Thermodynamics of Information under Protocol
Constraints. 10.1103/PhysRevX.11.041024
Kolmogorov A, (1965). Three Approaches to the Quantitative Definition of Information, Problems Inform. Transmission,
vol. 1, pp. 1-7.
Levin, M. (2023). Bioelectric networks: the cognitive glue enabling evolutionary scaling from physiology to mind. Animal
Cognition, 1-27.
Maruyama, K., Nori, F., & Vedral, V. (2009). Colloquium: The physics of Maxwell’s demon and information. Reviews of
Modern Physics, 81(1), 1.
Maturana, H. R., & Varela, F. J. (1991). Autopoiesis and cognition: The realization of the living (Vol. 42). Springer
Science & Business Media.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
20/21

Maxwell J. C. The Sorting Demon of Maxwell 1. Nature 20, 126 (1879). https://doi.org/10.1038/020126a0
Parrondo, J., Horowitz, J. & Sagawa, T. (2015). Thermodynamics of information. Nature Phys 11, 131–139.
https://doi.org/10.1038/nphys3230
Prigogine I. 1977. Time, Structure and Fluctuations. Nobel Lecture.
Rainer, F., Ebeling, W. (2016) Entropy and the Self-Organization of Information and Value. Entropy 18, no. 5: 193.
https://doi.org/10.3390/e18050193
Schwartz, K. (2014). "On the Edge of Chaos: Where Creativity Flourishes. 4712014054020140540
Shannon C (1948). A Mathematical Theory of Communication
Smith, E. (2008). Thermodynamics of natural selection J Theor Biol 252 2, 185-197
Smith, J. (2000). The Concept of Information in Biology. Philosophy of Science, 67(2), 177-194. doi:10.1086/392768
Varley T, Hoel E. (2021) Emergence as the conversion of information: A unifying theory. arXiv:2104.13368v1
Watson, R., & Levin, M. (2023). The collective intelligence of evolution and development. Collective Intelligence, 2(2),
26339137231168355.
Wilson, E. O. (1997). Consilience.
Qeios, CC-BY 4.0   ·   Article, June 12, 2023
Qeios ID: S90ADN   ·   https://doi.org/10.32388/S90ADN
21/21

