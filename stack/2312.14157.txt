3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera
Christen Millerdurai1,2
Diogo Luvizon1
Viktor Rudnev1,2
Andr´e Jonas3
Jiayi Wang1
Christian Theobalt1
Vladislav Golyanik1
1MPI for Informatics, SIC
2Saarland University, SIC
3RPTU Kaiserslautern-Landau
Abstract
3D hand tracking from a monocular video is a very chal-
lenging problem due to hand interactions, occlusions, left-
right hand ambiguity, and fast motion. Most existing meth-
ods rely on RGB inputs, which have severe limitations un-
der low-light conditions and suffer from motion blur. In
contrast, event cameras capture local brightness changes
instead of full image frames and do not suffer from the de-
scribed effects. Unfortunately, existing image-based tech-
niques cannot be directly applied to events due to signifi-
cant differences in the data modalities. In response to these
challenges, this paper introduces the first framework for
3D tracking of two fast-moving and interacting hands from
a single monocular event camera. Our approach tackles
the left-right hand ambiguity with a novel semi-supervised
feature-wise attention mechanism and integrates an inter-
section loss to fix hand collisions. To facilitate advances in
this research domain, we release a new synthetic large-scale
dataset of two interacting hands, Ev2Hands-S, and a new
real benchmark with real event streams and ground-truth
3D annotations, Ev2Hands-R. Our approach outperforms
existing methods in terms of the 3D reconstruction accuracy
and generalises to real data under severe light conditions1.
1. Introduction
Live 3D hand tracking from visual streams is a challenging
problem arising in many applications [1, 7, 9, 21, 42, 50, 52]
such as human-computer interaction and automatic sign
language translation, among others. Existing works address
it predominantly with RGB sensors [2, 4, 12, 15, 29, 59].
However, human hands can move fast and can be observed
under low-light conditions, which often makes these 3D
hand reconstruction scenarios impractical due to apparent
motion blur and the limited temporal resolution (or under-
exposure) of RGB sensors.
In contrast to synchronously operating RGB sensors (i.e.,
1https://4dqv.mpi-inf.mpg.de/Ev2Hands/
Event Camera
Event Stream
Our Prediction
Ev2Hands
Fast Motion
Hand
Interaction
Low Light
Pred.
Pred.
Pred.
Figure 1. We propose Ev2Hands, the first method for the 3D re-
construction of two interacting hands from a single event camera.
Our method operates on event clouds and outputs the shape, pose,
and position of both hands in 3D space. The RGB images (bottom)
are not used by our method and are shown only for reference.
recording absolute brightness values of all pixels in a frame
at pre-defined time intervals), event cameras record per-
pixel brightness changes asynchronously. Event cameras
have already been successfully used in several complex
tasks, including image reconstruction [44, 47, 49], optical
flow estimation [35], human action recognition [38, 48],
and human motion capture [62], even under low lighting
conditions. Despite these successes, event-based vision for
3D hand tracking is still in its infancy [56]: Only a few
works [34, 46, 63] addressed it for single hands, and none
explicitly accounts for the complex interactions and oc-
clusions frequently occurring during two-hand interactions.
Note that tracking two hands from asynchronous events is
more challenging than the single-hand scenario, as events
can be caused by both moving hands simultaneously and by
background motion or noise, leading to left-right hand con-
fusion and spurious predictions. Moreover, there are cur-
rently no datasets in the literature with real event stream
observations, 3D annotations for both hands, two-hand in-
teractions and fast hand movements.
In response to the reviewed challenges, this paper pro-
poses Ev2Hands, a new, and the first of its kind, event-based
tracker for reconstructing high-speed two-hand interactions
in 3D from a single event stream (see Fig. 1). Our new
neural architecture operates on spatio-temporally unrolled
events aggregated as event point clouds, which are seg-
1
arXiv:2312.14157v1  [cs.CV]  21 Dec 2023

mented in a semi-supervised manner into events caused by
left hand, right hand, or background. Our method leverages
a feature-wise attention mechanism that helps to resolve the
left-right event ambiguity, while not requiring ground-truth
segmentation labels from real data. To encourage plausi-
ble 3D estimates and prevent collision and self-penetration
in the predicted hand meshes, we also introduce inter-hand
and intersection losses. Finally, we acquire Ev2Hands-S,
i.e., a new large-scale dataset with events synthesised from
sparse real hand pose annotations, and Ev2Hands-R, i.e.,
a new two-hand pose benchmark with events captured by
a real event camera with calibrated and synchronised 3D
ground-truth hand poses for both interacting hands.
In summary, our technical contributions are as follows:
• The first approach for 3D tracking of two interacting
hands from a single event camera;
• A new neural architecture with feature-wise attention
coupled with individual event point segmentation that
helps to resolve handedness based on a semi-supervised
learning strategy;
• Two
new
datasets
for
training
and
evaluation:
Ev2Hands-S, derived from sparse 3D hand pose an-
notations, and Ev2Hands-R, a new real data benchmark
with a synchronised and calibrated event stream, RGB
views and 3D hand annotations for two interacting hands.
Our
comparisons
with
the
event-based
baseline
EventHands [46] and recent RGB-based methods [4, 22, 27]
show that the proposed architecture and the training strat-
egy result in steadily more precise 3D predictions compared
to them, especially in challenging cases. The readers are
urged to watch the accompanying video.
2. Related Work
This section focuses on recent approaches for the 3D recon-
struction of two hands from a monocular input and reviews
event-based methods for 3D vision.
3D Reconstruction of Two Hands.
Existing work fo-
cuses mainly on single-hand pose estimation from RGB im-
ages [2, 4, 12, 17, 32, 67–69] or depth maps [24, 51, 65].
These methods are often limited by the low frame rate
of conventional colour cameras and depth sensors, suf-
fer from blurry images, and fail in the more challeng-
ing problem of estimating two interacting hands. Further-
more, most available datasets are for single-hand estima-
tion from RGB [20, 32, 70], depth [11, 31, 64], or synthetic
events [46], and only a very few RGB [29] and depth [33]
datasets provide 3D annotations for both hands.
Some methods for two-hand pose estimation decompose
the problem into more tractable tasks [10, 27, 59] and lever-
age large-scale annotated data to learn a hand pose predic-
tor [27, 29]. Parametric hand models [8, 28, 41, 45] offer
the possibility to predict more plausible hands interactions,
which can be coupled with intermediate supervisions [66],
attention mechanisms [14, 22], or modelled in a probabilis-
tic manner [60]. Hands segmentation is also particularly
useful as an intermediate task for hands estimation from
RGB [10] or depth maps [23, 33, 54]. However, obtained
segmentation from events is a hard problem still in its in-
fancy [53] with no available dataset for hands. Differently
from previous methods, our approach does not require RGB
or depth data and predicts two interacting hands directly
from the event stream with extremely high temporal reso-
lution, while enforcing plausible predictions by penalising
collisions between interacting hands in 3D space.
Event-based Methods for 3D Reconstruction.
Tradi-
tional image-based techniques cannot be directly applied
to events. To cope with this, Xu et al. [62] formulates a
model-fitting term using a collection of “close events” to
refine an initial pose. Other methods use event frames, i.e.,
an image-space representation by aggregating events from
a fixed time interval. This enables the usage of learning-
based techniques that take advantage of the inductive bias of
CNNs. Thus, Rudnev et al. [46] presented the first method
for the estimation of a single 3D hand pose from events with
the proposed LNES representation and a temporal Kalman
filtering stage. LNES aggregates events in a 2D image con-
sidering a temporal sliding window2. Nehvi et al. [34] track
non-rigid 3D objects by propagating through a new differ-
entiable event stream simulator and Xue et al. [63] presents
an Expectation Maximisation (EM) framework where the
parameters of a hand model are optimised by associating
events to the mesh faces, assuming that events are typically
caused by moving edges. However, such methods do not
take advantage of the sparsity of the event streams and must
process each event image entirely. Additionally, they are
limited to a single hand. When multiple hands are interact-
ing in the scene, events triggered by different hands are en-
tangled in the event frame representation, making it harder
to estimate both hands separately.
One way to preserve data sparsity is to represent event
streams as a space-time Event Point Cloud (EPC). This rep-
resentation has been applied for gesture recognition [61],
where event points are processed by PointNet [39]. Chen et
al. [6] extended EPC with a Rasterised Event Point Cloud
(REPC) representation for 2D human pose estimation. This
includes a re-sampling strategy to ensure that PointNet can
operate on fixed time windows. Our approach leverages an
EPC representation and processes it with PointNet++ [40]
to solve the much more complex task of 3D reconstruction
of two self-similar hands. Differently from previous meth-
ods, we propose a feature-wise attention mechanism cou-
pled with semi-supervised training that benefits from syn-
thetic event segmentation and real data without segmenta-
2While designing Ev2Hands, we observed that LNES does not work
well for two hands due to its inherent ambiguities when the hands occlude
each other, and we had to utilise another representation (i.e., event cloud).
2

Ev2Hands (Sec. 3.3)
Event Cloud
PointNet++
Backbone
Attention
Block
Attention
Block
R. Hand
Regressor
L. Hand
Regressor
Predicted Hands
Supervision (Sec. 3.4)
Right Hand
Left Hand
Segmentation 
Input Events
(Sec. 3.1)
Events Cloud
Representation
(Sec. 3.2)
Figure 2. Workflow of our Ev2Hands approach. Our framework converts a time-sliced event stream into an event cloud, which is fed
as input to Ev2Hands that regresses the MANO parameters of the left and right hands with their global rotations and translations. The
grey blocks are non-trainable and dashed lines indicate components only used during training. Note that the segmentation branch is only
supervised with synthetic data and trained in a semi-supervised manner on real data.
tion labels, which helps the network to focus on only rele-
vant events while generalising to real event streams.
3. Method
Given a monocular event stream (Sec. 3.1) observing two
interacting hands, we reconstruct the shape, pose, and po-
sition of each hand in the global 3D space in two stages.
First, we process the raw monocular event stream into an
event point cloud representation (Sec. 3.2), which preserves
relevant information for a given time window while com-
pressing redundant events from the same pixel in a single
point. Second, we propose Ev2Hands (Sec. 3.3), an end-
to-end attention-based neural network that takes as input an
event cloud and regresses the hand model parameters [45]
along with the global translation and rotation of the left and
right hands. The key component in the proposed architec-
ture is a feature-wise attention module that learns to use
semi-supervised segmentation features fed to hand-specific
regressors. See Fig. 2 for the method overview.
3.1. Event Camera Model
Event cameras produce a stream of asynchronous events
used to record changes in brightness. These raw events are
represented by tuples
ei = (xi, yi, ti, pi),
(1)
where the i-th event corresponding to the pixel location
(xi, yi) is triggered at time ti with polarity pi ∈{−1, 1}.
An event is emitted at time ti when the change in
logarithmic brightness L crosses the threshold C, i.e.,
|L(xi, yi, ti) −L(xi, yi, ti −tp)| ≥C, where tp is the pre-
vious triggering time at the same pixel. Due to the sparse,
asynchronous nature of the event stream, it is challenging
to directly process the raw event stream using a neural net-
work. Next, we present an event cloud representation that
is better suited for training our network architecture.
3.2. Event Cloud Representation
Our goal in this stage is to process the input stream of asyn-
chronous events in a more stable and efficient representa-
tion. Previous works employ 2D representations of events
by projecting the temporal information to the image plane
[25, 43, 46] so that CNNs can be directly applied. However,
this aggregation collapses the temporal information and cre-
ates inefficient and sparse image representations. Alterna-
tively, we treat time as a third data dimension and conceptu-
alize events as point clouds in a similar manner to REPC [6].
Specifically, let us consider a time window of size T
where the first raw event e0 occurs at time t0 = 0 (rela-
tive to the given time window) and all events at the same
pixel location (x, y) are combined into an event point Ek:
Ek = (xk, yk, tk, Pk, Nk),
(2)
where tk is the average time of the combined events and
Pk and Nk are the number of positive and negative events
in the time interval considered and normalised by the total
number of events in the pixel. When all the raw events in
the time window T are combined, we obtain an event cloud
E ∈RM×5, where M is the resulting number of events.
3.3. The Proposed Ev2Hands Approach
Given the event cloud E, we estimate the shape, pose, and
position of each hand corresponding to the end of the time
window T. In what follows, we provide the details of the
hand model we use and the proposed attention-based neural
network for processing E for both hands.
3.3.1
3D Hand Model
We use MANO [45] for human hand mesh parameterisa-
tion, which includes a hand pose vector θ ∈R6 and shape
vector β ∈R10; both vectors are coefficients obtained
through PCA decomposition.
We also encode the rigid
3

transformation parameters, i.e., translation t ∈R3 and the
rotation R ∈R3 for each hand. We can obtain the sparse
hand joints from MANO with J = J (θ, β, t, R), where J
is a function that regresses the joint locations and applies
rotation and translation, and J ∈RNJ×3 are the joint loca-
tions of the regressed 3D hand. For simplicity, we refer to
the hand parameters in the same way for both left and right
hands, unless explicitly indicated otherwise.
3.3.2
Ev2Hands Model
Considering that both hands and the background can trig-
ger independent events and some events are spurious (noisy
signals), we need a model that can determine which events
are more relevant to each hand prediction and which events
should be ignored. In addition, the model architecture has to
be specifically designed to handle the event cloud input and
to predict the parameters for both hands, as previously dis-
cussed. To achieve this goal, we leverage PointNet++ [40]
as our backbone, which takes as input the event cloud E and
outputs the event features F ∈RM×256. The event features
are then processed by a multilayer perceptron (MLP) that
produces individual features relevant for the segmentation
task and the hand regression tasks. A feature-wise atten-
tion block is individually applied to the features from both
hands, where the predicted segmentation labels are used as
keys. This allows the two-hand regressor models to take
segmentation results into account to predict the left- and
right-hand parameters. A diagram of Ev2Hands is shown
in Fig. 2. In what follows, we explain its components.
Hand Branches. Given the event features F, we want to
obtain a per-hand feature vector that will be further used
to compute hand-specific features. To achieve this, we use
the left- and right-hand branches, respectively depicted in
green and red in Fig. 2, which extract two feature vectors
QL, QR ∈RM×256 through two shallow MLP networks
that are individually applied to each point in F.
Segmentation Branch. In addition to the hand-specific fea-
tures, we want our model to reason about whether points
belong to the left hand, right hand, or background events.
Hence, we introduce a segmentation branch, which also
uses a shallow MLP applied to each point in F. Differ-
ently from the hand branches, the segmentation branch pre-
dicts the logits associated with each point in the event cloud,
which are represented by S ∈RM×4 and encode the classes
left hand, right hand, background, and no class. The left
and right-hand labels correspond to events directly pro-
duced by one of the hands and the background labels cor-
respond to events produced by non-hand objects, like torso
or arm movements, or by changes in the background. The
extra no class label is used to indicate when an event point
combines multiple events with different labels in the time
window, e.g. when a left-hand event and a background event
are triggered at the same pixel in the same time window.
Feature-wise Attention Block.
Inspired by attention
mechanisms [3, 58], we want our model to extract features
that are relevant to each hand individually. To this end, we
have a feature-wise attention module defined by:
Attention(Q(·), S, F) = F
 
Softmax
 
QT
(·)S
√ds
!!
,
(3)
where the hand features, QL or QR, are masked by the key
values S, and the Softmax operates as a linear combination
of the event features F. Note that Eq. (3) is applied indi-
vidually to each hand, which produces a two hand attention
features HL, HR ∈RM×ds; ds is the dimension of S.
The feature-wise attention mechanism is designed to al-
low the attention features H to be a function of the hand
features Q, but also to respond to the segmentation predic-
tions from S. This helps the model to identify features that
are more relevant to specific hands or conditions of inter-
actions, as demonstrated in our experiments. In addition,
we also show that the feature-wise attention mechanism
can refine the segmentation prediction S, which is semi-
supervised with synthetic data and trained end-to-end on
real event data by supervising the hand parameters only.
Hand Parameters Regressor.
Finally, given the hand-
specific attention features, we predict the parameters {θ, β,
R, t} for each hand. For this task, we use a mini-PointNet
[39] followed by an MLP. Since each hand regressor takes
as input its own hand-specific features, there is no weight
sharing between both models.
3.4. Loss Functions
3D Joint Loss. This loss penalises deviations between the
regressed 3D joints from the hand model and the ground
truth 3D joint annotations of each hand. For simplicity, we
omit the left- and right-hand indexes:
Ljoints =
1
NJ
NJ
X
i=1
∥ˆJi −Ji∥,
(4)
where ˆJi and Ji are the predicted and ground-truth i-th 3D
joints, and NJ is the number of hand joints.
MANO Loss. This term penalises deviations between the
predicted and reference hand parameters. The PCA pose
and shape coefficients θ and β, and the global rotation R
are penalised using ℓ2 distance, while the rigid translation t
is supervised with using ℓ1 distance:
LMANO = ∥ˆθ−θ∥2+∥ˆβ−β∥2+∥ˆR−R∥2+∥ˆt−t∥, (5)
where ˆθ, ˆβ, ˆR,ˆt are predicted by the hand regressors and
θ, β, R, t are the reference hand parameters.
Segmentation Loss. The segmentation loss is intermedi-
ate supervision used to penalise wrong event classes in S,
4

which is the case in our experiments only when the model is
trained on synthetic data. When training on real event data,
the segmentation branch is indirectly supervised only by the
gradients propagated from the supervision of the hand pa-
rameters. The segmentation loss reads:
Lseg = CrossEntropy(Softmax(S), c),
(6)
where c are the event class labels considering left hand,
right hand, and background. Note that the key correspond-
ing to no class is not supervised.
Inter-hand Loss. This loss term considers both hands si-
multaneously and penalises deviations between left- and
right-hand shape parameters and the relative position be-
tween both hands. The inter-hand loss is expressed as:
Linterhand = ∥βleft −βright∥2 + IJ + IT , where
(7)
IJ =
1
NJ
NJ
X
i=1
∥(ˆJleft,i −ˆJright,i) −(Jleft,i −Jright,i)∥2 (8)
IT = ∥(ˆtleft −ˆtright) −(tleft −tright)∥2.
(9)
The inter-3D joint IJ and the inter-translation IT terms ac-
count respectively for the relative articulation errors and the
relative distance errors considering the left and right hands.
Intersection Loss.
We avoid physically invalid predic-
tions by penalising intersections, both due to articulation
and hand-hand interactions. We adopt the conic distance
fields approximation of meshes [57] for our collision loss.
This is done by first finding the set of colliding triangles us-
ing bounding volume hierarchies [18]. For each triangle, a
3D cone is constructed, defined by a circumscribing circle
and the face orientation. The distance to the surface of the
cone can be calculated for each query point, and the sum
of these distances over all triangles under consideration ap-
proximates the distance field of the hand. The value of the
distance field represents the amount of the repulsion Lisec
that is needed to penalise the intrusion. For the exact defi-
nition of Lisec, we refer the reader to Tzionas et al. [57].
MANO Regularisation.
In addition to the losses intro-
duced above, we also regularise the hand predictions in
the PCA parameter space of MANO through the use of
Tikhonov regulariser [33]:
Lreg = λθ∥ˆθ∥2 + λβ∥ˆβ∥2,
(10)
where λθ = 0.025 and λβ = 25. This term penalises statis-
tically unlikely MANO parameters.
The Total Loss. Overall, our total loss reads:
L = λjointsLjoints + λMANOLMANO + λsegLseg
+ λinterhandLinterhand + λisecLisec + Lreg,
(11)
where the weights λjoints=0.01,
λMANO=10,
λseg=1,
λinterhand=100, λisec=100 are chosen empirically to account
for the different magnitudes of each loss term.
Training on Real Data. Although our large-scale synthetic
dataset (Sec. 4.1) contains a high variability of poses, our
model can further benefit from fine-tuning on real event
streams to reduce the domain gap.
However, real data
does not provide event segmentation labels and the intrin-
sics from DAVIS346C can differ from the synthetic data.
To cope with the first problem, we remove Lseg from our
losses and indirectly learn S based only on the hand param-
eters supervision. To mitigate the variations in the intrinsics
that could cause discrepancies in 3D joint positions, we re-
place the 3D joint loss with a 2D projection joint loss:
Lreal =λjoints2DLjoints2D + λinterhandLinterhand+
λisecLisec + Lreg, where
(12)
Ljoints2D =
1
NJ
NJ
X
i=1
∥ΠS(ˆJi) −ΠR(Ji)∥,
(13)
and ΠS and ΠR are the camera projection operators for the
simulated and real event cameras. Lreal enables training on
real data while the semi-supervised segmentation branch is
still optimised through our attention mechanism.
4. The Ev2Hands Datasets
To train our model, we need to provide event data and la-
bels to supervise the loss functions described in Sec. 3.4.
However, no dataset with annotated two-hand interactions
and an event stream is available. To solve this issue, we
synthesise a large-scale event stream dataset and record
a real-world dataset using one DAVIS346C event camera.
Both datasets have synchronised RGB videos along with the
event streams. We refer to the synthetic and real datasets as
Ev2Hands-S and Ev2Hands-R. See Figs. 3 and 4 for exam-
ples of our data. To the best of our knowledge, these are the
first event stream datasets to model two-hand interactions.
Please also see our video for dynamic visualisations. We
next provide details on our datasets.
4.1. Ev2Hands-S Dataset
We generate our synthetic dataset by rendering synthetic
videos of two interacting hands, which are then fed to the
event stream simulator VID2E [13].
Interacting Hand Animation. We obtain realistic hand
motion by leveraging the InterHand2.6M dataset [29, 30],
providing ground-truth MANO parameters for two hands.
For each sequence from InterHand2.6M, we linearly inter-
polate the annotations to obtain smooth sequences with a
higher framerate and to fill in possible gaps in the annota-
tions (which are mainly due to sporadically missing ground
truth). This also allows us to vary the animation frame rate
5

Figure 3. Sample from Ev2Hands-S with input image (left), event
stream (middle), and segmentation labels (right).
Figure 4. Sample from Ev2Hands-R with motion tracking setup
(left) used for obtaining the reference hand poses, RGB stream
(middle), and event stream (right).
by re-sampling the sequences, which provides more vari-
ability to the training data. In total, we obtain 3.12 · 108
events from the simulated videos.
Scene Modelling. We use MANO [45] to model the hands
and attach cylinders onto the base to fit the forearms. We
texture the hands with HTML [41] and render the models
with Pyrender [26]. Please see the supplementary material
for more details.
Rendering and Event Stream Generation. We render the
scenes using a perspective camera with 30◦as a vertical
field of view and a resolution of 346 × 240 to emulate the
specifications of DAVIS346C [16]. We apply the RGGB
Bayer filter [44] to the rendered image, as we use a colour
event camera in our experiments. We convert the “Bayered”
image Bi at frame i to log intensity L(ti) at time ti by:
L(ti) = log(Bi + ϵ),
(14)
where ϵ is set as 10−4 for numerical stability.
We pass
the log intensity image to an event stream simulator [13]
with the event threshold C. We estimate C to match our
DAVIS346C event camera following the procedure adopted
by Rudnev et al. [46] and obtain C = 0.4.
4.2. Ev2Hands-R Dataset
To evaluate our method with an actual event camera and to
further bridge the gap between synthetic and real data, we
collected the Ev2Hands-R dataset.
Dataset composition. The dataset focuses on the everyday
usage of hands with a high range of motions and variations
among the participants, who were instructed to perform a
set of actions in an unconstrained manner. We recorded in
total eight sequences with five different subjects, including
persons of different sizes and different skin colours, result-
ing in variations in the shape and aspect of the hands. The
sequences depict various hand motions performed with pro-
gressively increasing complexity: palm wave, dorsal wave
(back of the hand), wrist rotation, hand articulation, clap,
intersection, occlusion, and free style.
Data Capture Setup. We capture all sequences with event
camera DAVIS346C and a high-speed RGB camera Sony
RX0.
The reference 3D hand poses are obtained by a
commercial multi-view human motion tracking system [55]
with 29 external cameras (see Fig. 4). All the 31 cameras
are synchronised and jointly calibrated with a reprojection
error below 3mm (which is substantially more accurate than
the accuracy of monocular 3D hand pose estimation tech-
niques). The acquired events and RGB frames (along with
the corresponding 3D hand poses) span 20.1 minutes.
5. Experiments
We evaluate Ev2Hands on two-hand 3D reconstruction con-
sidering Ev2Hands-S and Ev2Hands-R datasets.
Ev2Hands-S Dataset. We follow the original train and test
split from InterHand2.6M [29] and evaluate our method in
the test split using the synthesised event stream as input.
We compare our predictions with the interpolated reference
poses, as described in Sec. 4.1 and in the supplement.
Ev2Hands-R Dataset. We use two subjects to fine-tune
our method on real data and three subjects for evaluation.
To establish a reference for the performance metrics, we
evaluate three RGB methods that claim to work on in-the-
wild data [4, 22, 27] on the synchronised RGB video stream.
Note that although we only evaluate event predictions that
have corresponding RGB frames, our method works even
when RGB methods have no input due to the much higher
temporal resolution of our method. This advantage is not
reflected in the performance metrics.
Evaluation Metrics. For our quantitative comparisons, we
use the Percentage of Correct Keypoints (PCK) and the area
under the PCK curve (AUC) with thresholds ranging from
0 to 100 millimetres (mm). Following Li et al. [22], we re-
port the relative PCK (R-PCK) and AUC (R-AUC) scores
to evaluate the performance of 3D hand pose estimation
of each hand individually.
To evaluate the performance
of localisation of each hand with respect to the other, we
extend the mean relative-root position error [29] and re-
port the relative-root PCK (RR-PCK) and AUC (RR-PCK)
scores. Unlike R-PCK, which is computed after performing
root-joint alignment of each hand individually, the RR-PCK
aligns the entire two-hand configuration to the right-hand
root of the reference pose. Thus, the RR-PCK metric better
evaluates the relative 3D position of the hands. To evaluate
the mesh penetration of interacting hands, we take the colli-
sion percentage ”Coll%” of each mesh. This metric takes
the percentage of mesh triangles that intersect with each
other. We report the mean over frames where the hands are
positioned less than 50mm to each other, and a lower value
6

Palm wave
Dorsal wave
Wrist rot.
Articulation
Clap
Intersection
Occlusion
Free Style
Avg.
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
R-AUC
RR-AUC
†Boukhayma et al. [4]∗
0.50
–
0.49
–
0.33
–
0.45
–
0.29
–
0.27
–
0.28
–
0.45
–
0.38
–
†Li et al. [22]
0.67
0.43
0.69
0.42
0.57
0.34
0.62
0.43
0.59
0.41
0.51
0.38
0.55
0.37
0.63
0.46
0.60
0.40
†Moon et al. [27]
0.65
0.33
0.69
0.35
0.67
0.34
0.65
0.35
0.6
0.36
0.62
0.42
0.64
0.44
0.61
0.33
0.64
0.36
EventHands [46]∗
0.41
0.31
0.48
0.21
0.31
0.40
0.43
0.42
0.45
0.29
0.28
0.35
0.28
0.33
0.43
0.28
0.38
0.32
Ev2Hands (Ours)
0.66
0.53
0.69
0.54
0.63
0.52
0.63
0.52
0.65
0.54
0.60
0.50
0.63
0.54
0.62
0.50
0.64
0.52
Table 1. Comparison on Ev2Hands-R. “†” denotes RGB-based and “*” denotes single-hand methods. Ev2Hands outperforms existing
approaches in most of the activities by a fair margin while estimating hands with a much higher temporal resolution. Red, orange and
yellow denote the highest, the second-highest and the third-highest AUC scores, respectively.
RGB View
Our Event Classes
Ev2Hands
EventHands
Boukhayma et al.
Li et al.
Event-based Methods
RGB-based Methods
Moon et al.
Figure 5. A qualitative comparison of our Ev2Hands method with previous RGB-based [4, 22, 27] and event-based [46] methods. Note
how EventHands and Boukhayma et al. fail to predict interacting hands and how Li et al. fail with low-light RGB.
indicates less mesh penetration of interacting hands.
Implementation. We implement our network in PyTorch
[36] and use Adam optimiser [19] with a learning rate of
5 · 10−5 and a mini-batch of 128. We train for 8 · 105 itera-
tions with Ev2Hands-S and fine-tune for 1.5 · 104 iterations
on Ev2Hands-R. The network is supervised by the reference
pose considering the last frame in the time window T, emu-
lating the position of hands closest to the current time. The
temporal resolution of the event stream is set to 1000 FPS,
achieved by using a 2 ms window time with 1 ms overlap.
5.1. Comparisons to State of the Art
In Table 1, we compare our method with state-of-the-
art pose estimation methods on Ev2Hands-R. We evalu-
ate single-hand [4, 46] and two-hand [22, 27] methods.
Ev2Hands significantly outperforms both the single-hand
event and single-hand RGB approaches, presumably be-
cause they do not handle heavy occlusions. The proposed
approach also outperforms the RGB-based two-hand meth-
ods [22, 27] in most cases, especially when considering
the more challenging RR-AUC metric, while operating at
significantly higher temporal resolution. For RGB-based
methods [4, 22], we provide ground-truth cropped hands
as the input, while [27] and our method do not require hand
crops. For the event-based method [46], we use our ap-
proach to generate event labels for each hand. The latter
are then given as input to [46] to reconstruct the position
and pose of the hands.
As Boukhayma et al. [4] use a
scaled orthographic projection for each hand, the RR-PCK
and RR-AUC metrics cannot be evaluated. Note in Fig. 5
how EventHands [46] fails to reconstruct hand interactions
and how the RGB-based methods fail with low-light im-
ages. We also compare our approach to RGB-based meth-
ods on images captured under different camera frame rates
in Table 2 and Fig. 6 emulating low lighting arising from the
high shutter speed and motion blur due to the fast motion
of the hands. Under these conditions, RGB-based methods
fail drastically, while our method outputs reasonable predic-
tions with much higher temporal resolution.
High Shutter Speed (500 FPS)
Fast Motion (25 FPS)
R-AUC
RR-AUC
Coll%
R-AUC
RR-AUC
Coll%
Li et al. [22]
0.24
0.12
19.99
0.38
0.25
12.67
Moon et al. [27]
0.41
0.23
6.71
0.4
0.23
6.82
Ev2Hands (Ours)
0.47
0.30
0.57
0.53
0.36
4.09
Table 2. Ev2Hands outperforms Li et al. [22] and Moon et al.
[27] on under-exposed high-speed (500 FPS) videos and blurry
fast motions (25 FPS) by a fair margin while estimating 3D hand
poses with a much higher temporal resolution (1000 FPS).
5.2. Ablation Study
Influence of Representation. We systematically examine
the impact of different event representations in Tab. 3-(top).
7

Li et al.
RGB View
500 FPS
Ev2Hands
RGB View
25 FPS
Event Cloud
Moon et al.
Figure 6. Our event-based method performs well, whereas RGB-
based methods [22, 27] fail, notably due to motion blur and scene
underexposure under low lighting conditions.
R-AUC ↑
RR-AUC ↑
Coll% ↓
LNES [46]
0.56
0.46
8.13
Raw events
0.62
0.46
7.98
Event Cloud (EC) adapted from [6]
0.66
0.53
8.39
EC+Attention
0.69
0.57
8.38
EC+Attention+Lseg
0.75
0.66
8.39
EC+Attention+Lseg+IAL
0.72
0.63
6.69
Table 3. Ablation study on Ev2Hands-S with different event repre-
sentations (top) and different components of our method (bottom).
“IAL” refers to the Intersection Aware Loss Lisec.
R-AUC ↑
RR-AUC ↑
Coll% ↓
Event Cloud (EC)
0.35
0.37
7.53
EC+Attention
0.38
0.40
7.62
EC+Attention+Lseg
0.41
0.43
7.52
EC+Attention+Lseg+IAL
0.39
0.41
7.49
EC+Attention+Lseg+IAL+FT
0.64
0.52
5.12
Table 4. Ablation on Ev2Hands-R with different losses and train-
ing strategies. “FT” refers to fine-tuning on real data.
RGB View
Input Event Cloud
without IAL
with IAL
Figure 7. Influence of the Intersection Aware Loss (IAL).
Although LNES [46] outperforms event point cloud repre-
sentations in Coll%, it fails to provide precise pose estima-
tion of interacting hands (R-PCK metric). The EC repre-
sentation provides the best pose estimation scores.
Influence of Attention. We conduct ablations on the pro-
posed architecture and losses on both synthetic and real
datasets (Tables 3 and 4).
We see that the feature-wise
attention mechanism, even without supervision, improved
the method’s performance (EC+Attention). With additional
segmentation supervision from the synthetic data, a sub-
Figure 8. Predicted segmentation without (middle) and with fine-
tuning on real data without segmentation labels (right).
stantial performance improvement can be observed. Inter-
estingly, we observe that the attention mechanism learns
plausible segmentation values even on real data when fine-
tuned using only pose annotations; see Table 4 and Fig. 8.
Influence of the Intersection Loss. By modelling hand
intersections explicitly (experiments+IAL), the amount of
interpenetration decreases as indicated by Coll%. Although
the pose estimation performance slightly drops, we theo-
rise this is because small deviations in pose prediction can
cause a lot of interpenetration in heavy interaction cases.
However, since the physical plausibility of the interaction is
essential for many applications, this trade-off could still be
advantageous. A qualitative comparison is shown in Fig. 7.
6. Conclusion
We presented Ev2Hands, the first method for two-hand 3D
hand pose estimation from event streams. Our event cloud
representation, when combined with the novel attention-
based segmentation mechanism and collision mitigation
loss, regresses reasonable 3D poses of two interacting
hands and outperforms the related methods on our proposed
benchmark dataset, Ev2Hands-R, on real event streams.
This is enabled by our new synthetic dataset, Ev2Hands-
S, which provides 3D pose, segmentation labels, and corre-
sponding RGB images, all of which are difficult to obtain
for real event streams. Furthermore, Ev2Hands works well
in low illumination conditions and can estimate high-speed
3D hand motions.
Our Ev2Hands assumes a fixed camera. While this does
not pose an issue in traditional RGB methods, a moving
(portable) event camera would generate a large amount of
background clutter. Future work could investigate how to
extract events caused by the object of interest. Another ex-
citing avenue for future research would be to combine the
RGB and event streams, thereby increasing the visual fi-
delity of the data while preserving the low latency of the
event stream. This will make high-quality textured 3D re-
constructions of fast-moving hands possible. As this is also
the first work on the 3D reconstruction of more than one
non-rigid object from a single event stream, we believe it
could inspire future research.
Acknowledgement. This work was supported by the ERC
consolidator grant 4DReply (770784).
8

References
[1] Ahmad Sami Al-Shamayleh, Rodina Ahmad, Moham-
mad AM Abushariah, Khubaib Amjad Alam, and Nazean
Jomhari. A systematic literature review on vision based ges-
ture recognition techniques. Multimedia Tools and Applica-
tions, 77:28121–28184, 2018. 1
[2] Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Push-
ing the envelope for rgb-based dense 3d hand pose estima-
tion via neural rendering. In Computer Vision and Pattern
Recognition (CVPR), 2019. 1, 2
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
Neural machine translation by jointly learning to align and
translate. arXiv:1409.0473, 2014. 4
[4] Adnane Boukhayma, Rodrigo de Bem, and Philip HS Torr.
3d hand shape and pose from images in the wild. In Com-
puter Vision and Pattern Recognition (CVPR), 2019. 1, 2, 6,
7, 13, 15
[5] G´ery Casiez, Nicolas Roussel, and Daniel Vogel. 1C filter:
a simple speed-based low-pass filter for noisy input in inter-
active systems. In SIGCHI Conference on Human Factors in
Computing Systems, pages 2527–2530, 2012. 13
[6] Jiaan Chen, Hao Shi, Yaozu Ye, Kailun Yang, Lei Sun,
and Kaiwei Wang. Efficient human pose estimation via 3d
event point cloud. In International Conference on 3D Vision
(3DV), 2022. 2, 3, 8
[7] Ming Jin Cheok, Zaid Omar, and Mohamed Hisham Jaward.
A review of hand gesture and sign language recognition tech-
niques. International Journal of Machine Learning and Cy-
bernetics, 10:131–153, 2019. 1
[8] Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-
Noguer, Chris Sweeney, Richard Newcombe, and Lingni
Ma. Lisa: Learning implicit shape and appearance of hands.
In Computer Vision and Pattern Recognition (CVPR), 2022.
2
[9] Laura Dipietro, Angelo M. Sabatini, and Paolo Dario.
A
survey of glove-based systems and their applications. IEEE
Transactions on Systems, Man, and Cybernetics, Part C (Ap-
plications and Reviews), 38(4):461–482, 2008. 1
[10] Zicong Fan, Adrian Spurr, Muhammed Kocabas, Siyu Tang,
Michael J Black, and Otmar Hilliges. Learning to disam-
biguate strongly interacting hands via probabilistic per-pixel
part segmentation. In International Conference on 3D Vision
(3DV), 2021. 2
[11] Guillermo Garcia-Hernando,
Shanxin Yuan,
Seungryul
Baek, and Tae-Kyun Kim. First-person hand action bench-
mark with rgb-d videos and 3d hand pose annotations. In
Computer Vision and Pattern Recognition (CVPR), 2018. 2
[12] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying
Wang, Jianfei Cai, and Junsong Yuan. 3d hand shape and
pose estimation from a single rgb image. In Computer Vision
and Pattern Recognition (CVPR), 2019. 1, 2
[13] Daniel Gehrig, Mathias Gehrig, Javier Hidalgo-Carri´o, and
Davide Scaramuzza.
Video to events:
Recycling video
datasets for event cameras. In Computer Vision and Pattern
Recognition (CVPR), 2020. 5, 6, 12
[14] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vin-
cent Lepetit. Keypoint transformer: Solving joint identifica-
tion in challenging hands and object interactions for accurate
3d pose estimation. In Computer Vision and Pattern Recog-
nition (CVPR), pages 11090–11100, 2022. 2
[15] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,
and Michael J Black. Resolving 3d human pose ambigui-
ties with 3d scene constraints. In International Conference
on Computer Vision (ICCV), pages 2282–2292, 2019. 1
[16] iniVation.
Davis 346.
https://inivation.com/
wp-content/uploads/2019/08/DAVIS346.pdf,
2019. 6
[17] Umar Iqbal, Pavlo Molchanov, Thomas Breuel Juergen Gall,
and Jan Kautz. Hand pose estimation via latent 2.5d heatmap
regression.
In European Conference on Computer Vision
(ECCV), 2018. 2
[18] Tero Karras.
Maximizing parallelism in the construc-
tion of bvhs, octrees, and k-d trees.
In Proceedings of
the Fourth ACM SIGGRAPH / Eurographics Conference
on High-Performance Graphics, pages 33–37. Eurographics
Association, 2012. 5
[19] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. arXiv:1412.6980, 2014. 7
[20] Dominik Kulon,
Riza Alp Guler,
Iasonas Kokkinos,
Michael M. Bronstein, and Stefanos Zafeiriou.
Weakly-
supervised mesh-convolutional hand reconstruction in the
wild. In Computer Vision and Pattern Recognition (CVPR),
2020. 2
[21] Qujiang Lei, Hongda Zhang, Zanwu Xia, Yang Yang, Yue
He, and Shoubin Liu. Applications of hand gestures recog-
nition in industrial robots: a review. In Eleventh Interna-
tional Conference on Machine Vision (ICMV 2018), pages
455–465. SPIE, 2019. 1
[22] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu,
Feng Chen, Tao Yu, and Yebin Liu.
Interacting attention
graph for single image two-hand reconstruction. In Com-
puter Vision and Pattern Recognition (CVPR), pages 2761–
2770, 2022. 2, 6, 7, 8, 13, 15, 16
[23] Shile Li and Dongheui Lee. Point-to-pose voting based hand
pose estimation using residual permutation equivariant layer.
In Computer Vision and Pattern Recognition (CVPR), pages
11927–11936, 2019. 2
[24] Jameel Malik, Ibrahim Abdelaziz, Ahmed Elhayek, Soshi
Shimada, Sk Aziz Ali, Vladislav Golyanik, Christian
Theobalt, and Didier Stricker.
Handvoxnet: Deep voxel-
based network for 3d hand shape and pose estimation from a
single depth map. In Computer Vision and Pattern Recogni-
tion (CVPR), pages 7113–7122, 2020. 2
[25] Ana I Maqueda, Antonio Loquercio, Guillermo Gallego,
Narciso Garc´ıa, and Davide Scaramuzza. Event-based vision
meets deep learning on steering prediction for self-driving
cars. In Computer Vision and Pattern Recognition (CVPR),
pages 5419–5427, 2018. 3
[26] Matthew Matl.
Pyrender.
https://github.com/
mmatl/pyrender, 2019. 6, 12
[27] Gyeongsik Moon. Bringing inputs to shared domains for 3d
interacting hands recovery in the wild. In The IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), 2023. 2, 6, 7, 8, 13, 15, 16
9

[28] Gyeongsik Moon, Takaaki Shiratori, and Kyoung Mu Lee.
Deephandmesh: A weakly-supervised deep encoder-decoder
framework for high-fidelity hand mesh modeling. In Euro-
pean Conference on Computer Vision (ECCV), 2020. 2
[29] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori,
and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline
for 3d interacting hand pose estimation from a single rgb im-
age. In European Conference on Computer Vision (ECCV),
2020. 1, 2, 5, 6, 12
[30] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.
Neuralannot: Neural annotator for 3d human mesh training
sets. In Computer Vision and Pattern Recognition (CVPR),
2022. 5, 12
[31] Franziska Mueller, Dushyant Mehta, Oleksandr Sotny-
chenko, Srinath Sridhar, Dan Casas, and Christian Theobalt.
Real-time hand tracking under occlusion from an egocentric
rgb-d sensor. In International Conference on Computer Vi-
sion (ICCV), 2017. 2
[32] Franziska Mueller,
Florian Bernard,
Oleksandr Sotny-
chenko, Dushyant Mehta, Srinath Sridhar, Dan Casas, and
Christian Theobalt. Ganerated hands for real-time 3d hand
tracking from monocular rgb. In Computer Vision and Pat-
tern Recognition (CVPR), 2018. 2
[33] Franziska Mueller, Micah Davis, Florian Bernard, Oleksandr
Sotnychenko, Mickeal Verschoor, Miguel A Otaduy, Dan
Casas, and Christian Theobalt. Real-time pose and shape
reconstruction of two interacting hands with a single depth
camera. ACM Transactions on Graphics (ToG), 38(4):1–13,
2019. 2, 5
[34] Jalees Nehvi, Vladislav Golyanik, Franziska Mueller, Hans-
Peter Seidel, Mohamed Elgharib, and Christian Theobalt.
Differentiable event stream simulator for non-rigid 3d track-
ing. In CVPR Workshop on Event-based Vision, 2021. 1,
2
[35] Federico Paredes-Vall´es and Guido CHE de Croon. Back to
event basics: Self-supervised learning of image reconstruc-
tion for event cameras via photometric constancy. In Com-
puter Vision and Pattern Recognition (CVPR), pages 3446–
3455, 2021. 1
[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-
perative style, high-performance deep learning library. Ad-
vances in Neural Information Processing Systems, 32, 2019.
7
[37] Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically
based rendering: From theory to implementation. Morgan
Kaufmann, 2016. 12
[38] Chiara Plizzari, Mirco Planamente, Gabriele Goletto, Marco
Cannici, Emanuele Gusso, Matteo Matteucci, and Barbara
Caputo. E2 (go) motion: Motion augmented event stream
for egocentric action recognition. In Computer Vision and
Pattern Recognition (CVPR), 2022. 1
[39] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.
Pointnet: Deep learning on point sets for 3d classification
and segmentation. In Computer Vision and Pattern Recogni-
tion (CVPR), 2017. 2, 4
[40] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-
net++: Deep hierarchical feature learning on point sets in a
metric space. arXiv:1706.02413, 2017. 2, 4
[41] Neng Qian, Jiayi Wang, Franziska Mueller, Florian Bernard,
Vladislav Golyanik, and Christian Theobalt. HTML: A Para-
metric Hand Texture Model for 3D Hand Reconstruction and
Personalization. In European Conference on Computer Vi-
sion (ECCV). Springer, 2020. 2, 6
[42] Siddharth S Rautaray and Anupam Agrawal. Vision based
hand gesture recognition for human computer interaction: a
survey. Artificial intelligence review, 43:1–54, 2015. 1
[43] Henri Rebecq, Timo Horstschaefer, and Davide Scaramuzza.
Real-time visual-inertial odometry for event cameras using
keyframe-based nonlinear optimization. In The British Ma-
chine Vision Conference (BMVC). IEEE, 2017. 3
[44] Henri Rebecq, Ren´e Ranftl, Vladlen Koltun, and Davide
Scaramuzza. High speed and high dynamic range video with
an event camera. IEEE Transactions on Pattern Analysis and
Machine Intelligence (TPAMI), 43(6):1964–1980, 2019. 1, 6
[45] Javier Romero, Dimitrios Tzionas, and Michael J. Black.
Embodied hands: Modeling and capturing hands and bod-
ies together. ACM Transactions on Graphics, (Proc. SIG-
GRAPH Asia), 36(6), 2017. 2, 3, 6
[46] Viktor Rudnev, Vladislav Golyanik, Jiayi Wang, Hans-Peter
Seidel, Franziska Mueller, Mohamed Elgharib, and Christian
Theobalt. Eventhands: Real-time neural 3d hand pose esti-
mation from an event stream. In International Conference
on Computer Vision (ICCV), 2021. 1, 2, 3, 6, 7, 8, 13, 15
[47] Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, and
Vladislav Golyanik. Eventnerf: Neural radiance fields from a
single colour event camera. In Computer Vision and Pattern
Recognition (CVPR), 2023. 1
[48] Alberto Sabater, Luis Montesano, and Ana C Murillo. Event
transformer. a sparse-aware solution for efficient event data
processing.
In Computer Vision and Pattern Recognition
(CVPR), 2022. 1
[49] Cedric Scheerlinck, Nick Barnes, and Robert Mahony.
Continuous-time intensity estimation using event cameras.
In Asian Conference on Computer Vision (ACCV), pages
308–324. Springer, 2018. 1
[50] Soshi Shimada, Vladislav Golyanik, Patrick P´erez, and
Christian Theobalt. Decaf: Monocular deformation capture
for face and hand interactions. ACM Transactions on Graph-
ics (TOG), 42(6), 2023. 1
[51] Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, and
Christian Theobalt.
Fast and robust hand tracking using
detection-guided optimization. In Computer Vision and Pat-
tern Recognition (CVPR), pages 3213–3221, 2015. 2
[52] Thad Starner, Jake Auxier, Daniel Ashbrook, and Maribeth
Gandy. The gesture pendant: a self-illuminating, wearable,
infrared computer vision system for home automation con-
trol and medical monitoring. In Digest of Papers. Fourth In-
ternational Symposium on Wearable Computers, pages 87–
94, 2000. 1
[53] Zhaoning Sun, Nico Messikommer, Daniel Gehrig, and Da-
vide Scaramuzza. Ess: Learning event-based semantic seg-
mentation from still images.
In European Conference on
10

Computer Vision (ECCV), pages 341–357. Springer, 2022.
2
[54] Jonathan Taylor, Vladimir Tankovich, Danhang Tang, Cem
Keskin, David Kim, Philip Davidson, Adarsh Kowdle, and
Shahram Izadi.
Articulated distance fields for ultra-fast
tracking of hands interacting. ACM Transactions on Graph-
ics (TOG), 36(6):1–12, 2017. 2
[55] The Captury. http://www.thecaptury.com/, 2023.
6, 12
[56] Edith Tretschk, Navami Kairanda, Mallikarjun B R, Rishabh
Dabral, Adam Kortylewski, Bernhard Egger, Marc Haber-
mann,
Pascal Fua,
Christian Theobalt,
and Vladislav
Golyanik. State of the art in dense monocular non-rigid 3d
reconstruction.
Computer Graphics Forum (Eurographics
State of the Art Reports), 2023. 1
[57] Dimitrios Tzionas, Luca Ballan, Abhilash Srikantha, Pablo
Aponte, Marc Pollefeys, and Juergen Gall. Capturing hands
in action using discriminative salient points and physics sim-
ulation. International Journal of Computer Vision (IJCV),
118(2):172–193, 2016. 5
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Il-
lia Polosukhin.
Attention is all you need.
In Interna-
tional Conference on Neural Information Processing Sys-
tems (NeurIPS), 2017. 4
[59] Jiayi Wang, Franziska Mueller, Florian Bernard, Suzanne
Sorli, Oleksandr Sotnychenko, Neng Qian, Miguel A
Otaduy, Dan Casas, and Christian Theobalt.
Rgb2hands:
Real-time tracking of 3d hand interactions from monocu-
lar rgb video. ACM Transactions on Graphics (ToG), 39(6),
2020. 1, 2, 13
[60] Jiayi Wang, Diogo Luvizon, Franziska Mueller, Florian
Bernard, Adam Kortylewski, Dan Casas, and Christian
Theobalt. Handflow: Quantifying view-dependent 3d am-
biguity in two-hand reconstruction with normalizing flow. In
Vision, Modeling, and Visualization (VMV), 2022. 2
[61] Qinyi Wang, Yexin Zhang, Junsong Yuan, and Yilong Lu.
Space-time event clouds for gesture recognition: From rgb
cameras to event cameras. In Winter Conference on Applica-
tions of Computer Vision (WACV), pages 1826–1835, 2019.
2
[62] Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Haber-
mann, Lu Fang, and Christian Theobalt. Eventcap: Monoc-
ular 3d capture of high-speed human motions using an
event camera. In Computer Vision and Pattern Recognition
(CVPR), 2020. 1, 2
[63] Yuxuan Xue, Haolong Li, Stefan Leutenegger, and Joerg
Stueckler. Event-based non-rigid reconstruction from con-
tours. In The British Machine Vision Conference (BMVC),
2022. 1, 2
[64] Shanxin Yuan, Qi Ye, Bjorn Stenger, Siddhant Jain, and Tae-
Kyun Kim.
Bighand2.2m benchmark: Hand pose dataset
and state of the art analysis. In Computer Vision and Pattern
Recognition (CVPR), pages 4866–4874, 2017. 2
[65] Shanxin Yuan, Guillermo Garcia-Hernando, Bj¨orn Stenger,
Gyeongsik Moon, Ju Yong Chang, Kyoung Mu Lee, Pavlo
Molchanov, Jan Kautz, Sina Honari, Liuhao Ge, Junsong
Yuan, Xinghao Chen, Guijin Wang, Fan Yang, Kai Akiyama,
Yang Wu, Qingfu Wan, Meysam Madadi, Sergio Escalera,
Shile Li, Dongheui Lee, Iason Oikonomidis, Antonis Argy-
ros, and Tae-Kyun Kim. Depth-based 3d hand pose estima-
tion: From current achievements to future goals. In Com-
puter Vision and Pattern Recognition (CVPR), 2018. 2
[66] Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda
Zhang, Ping Tan, Cuixia Ma, and Hongan Wang. Interact-
ing two-hand 3d pose and shape reconstruction from single
color image. In International Conference on Computer Vi-
sion (ICCV), pages 11354–11363, 2021. 2, 13
[67] Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen
Zheng. End-to-end hand mesh recovery from a monocular
rgb image. In International Conference on Computer Vision
(ICCV), 2019. 2
[68] Yuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul
Habibie, Christian Theobalt, and Feng Xu. Monocular real-
time hand shape and motion capture using multi-modal data.
In Computer Vision and Pattern Recognition (CVPR), 2020.
[69] Christian Zimmermann and Thomas Brox. Learning to esti-
mate 3d hand pose from single rgb images. In International
Conference on Computer Vision (ICCV), pages 4903–4911,
2017. 2
[70] Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan
Russell, Max Argus, and Thomas Brox. Freihand: Dataset
for markerless capture of hand pose and shape from single
rgb images. In International Conference on Computer Vision
(ICCV), 2019. 2
11

3D Pose Estimation of Two Interacting Hands from a Monocular Event Camera
Supplementary Material
This supplement provides additional details about our
datasets in Section A and our experimental settings in Sec-
tion B. Additional results are presented in Section C. We
use Roman numerals to reference figures and tables.
A. Dataset Details
A.1. Ev2Hands-S Dataset
Adaptation from InterHand2.6M. We obtain realistic
hand motion by leveraging the InterHand2.6M dataset
[29, 30] providing ground-truth MANO parameters for two
hands. We use the provided MANO annotations, which are
originally available at 5 frames per second (FPS), and inter-
polate each sequence to achieve 30 FPS. The interpolated
hand models are rendered and composed with background
scenes to generate the synthetic RGB frames. The RGB se-
quences are fed into the event stream simulator VID2E [13],
which outputs the corresponding event streams. We follow
train and test splits as Moon et al. [29].
Material and Lighting. To synthesise realistic images that
will be fed to the event stream simulator, we need to model
the material properties and the lighting conditions in our
simulations. To provide clear transitions in the boundaries
of the hands as well as realistic colour changes in the inte-
rior parts of the hands, we apply a metallic-roughness ma-
terial model [26, 37] to the hand models.
The scene is illuminated with ambient light along with
five-point light sources with positions and intensities ran-
domly perturbed for each sequence. We also rendered each
sequence with nine different backgrounds.
Hand Surface Modeling. In addition, we add a Gaussian
noise with the std. dev. of 3mm to the MANO vertices (hand
surfaces) before rendering, resulting in more realistic event
streams and helping the model to generalise to real data.
Data Augmentation. We augment the event stream with
noise by perturbing the existing events with random posi-
tion and time offsets, and polarity swaps. The augmented
events emulate noise (i.e., spurious events) and event pat-
terns due to possible changes in the illumination.
A.2. Ev2Hands-R Dataset
Dataset composition.
The dataset comprises eight
recorded sequences with five subjects. The subjects encom-
pass a spectrum of physical attributes, including variations
in both body size and skin colour. The cumulative duration
of the recorded sequences amounts to 19.4 minutes. We use
two subjects for fine-tuning our method and three subjects
for evaluation purposes.
Figure I. Recording setup for our Ev2Hands-R dataset with an
event camera (DAVIS 346C) and an RGB camera (Sony RX0),
which are synchronised and calibrated with respect to the multi-
view markerless motion capture system Captury [55].
Event+RGB Camera Setup. Our setup for real data cap-
ture includes an event camera (DAVIS 346C) and a high-
speed RGB camera (Sony RX0); see Fig. I. All the cameras
are synchronised and calibrated, and the RGB video stream
is used only to compare our method with the existing RGB-
based approaches and for reference.
Synchronisation is achieved through a sequence of claps
both at the start and the end of the recording. We transform
the stream of events into event frames with a time window
of 20 ms matching the frame time of a RGB frame. Sub-
sequently, we manually align the timestamp of the event
frame with the corresponding timestamp of the RGB frame
containing the clap sequence.
Reference 3D Hand Pose. The 3D reference hand poses
for Ev2Hands-R are obtained by a three-step process. First,
we calibrate the camera setup to obtain the intrinsic and
extrinsic parameters of the event and RGB cameras. The
extrinsic parameters are obtained w.r.t. the markerless mo-
tion capture system [55] using a chequerboard. Second, we
ask the target actor to perform hand sequences in front of
our Event+RGB camera setup. The motion capture system
produces full-body 3D human poses of the actor in global
space, including the body and both hands. Finally, the 3D
hand markers are projected onto the camera views of the
event and RGB cameras. Table I shows the list of actions
performed by one of the subjects. The way each individual
carries out actions is affected by their personal style, re-
sulting in small differences in each sequence. These slight
12

Figure II. The predictions of RGB2Hands [59].
Due to the
differences between the training regime and assumptions of
RGB2Hands and our setup, the obtained predictions are poor.
variations contribute to the dataset diversity.
B. Experimental Settings
Comparisons to EventHands.
We retrain and evaluate
EventHands [46] on Ev2Hands-R by individually inferring
on the left- and right-hand events. The left- and right-hand
event labels are obtained by the predicted segmentation la-
bels using Ev2Hands, since the ground-truth event labels
are not available for real event streams.
Comparisons to Additional RGB-based Methods. In ad-
dition to the methods evaluated in the main paper, i.e.,
Moon [27], Li et al. [22] and Boukhayma et al. [4], we also
test other RGB-based methods for 3D hand pose estimation
of two hands from monocular videos [59, 66]. However,
due to the differences in the training regime of these meth-
ods and our setup, the obtained predictions are poor. There-
fore, we do not report quantitative metrics; see Fig. II for
qualitative results for one of these RGB-based methods.
Ablative Study. To investigate the contributions of the key
components of our method, we conduct an ablation study
on the Ev2Hands-R and Ev2Hands-S datasets. The PCK
curves and AUC are shown in Fig. III. The PCK curves and
AUC of our method compared to Rudnev et al. [46], Li et
al. [22], and Boukhayma et al. [4] are shown in Fig. IV.
When assessing performance using the R-PCK metric with
Ev2Hands-S, the use of raw events as input surpasses the
performance of the LNES [46] approach as demonstrated
by an AUC value of 0.62. However, compared to the event
clouds, raw events exhibit inferior performance. Incorporat-
ing the feature-wise attention mechanism leads to a higher
AUC value of 0.69. This is further improved by the seg-
mentation supervision, resulting in the highest AUC score
of 0.75. The introduction of Intersection Aware Loss (IAL)
leads to a marginal reduction in the AUC score to 0.72. Im-
portantly, this is accompanied by a decrease in the rate of
collisions down to a value of 6.69%, enhancing the plausi-
bility of valid hand poses during the scenarios with highly
interacting hands. When assessing the performance of our
method on Ev2Hands-R, the same trend is observed. Ad-
ditionally, Fine Tuning (FT) our method with Ev2Hands-R
increases the R-PCK AUC score from 0.41 to 0.64. The
same trend is also reflected in RR-PCK values.
Temporal Stability. We reduce the jitter of high-speed mo-
tions generated by our method with the 1C filter [5] for vi-
sualisations in the accompanying supplementary video. For
a fair comparison, the same procedure is also performed for
all the methods we evaluate. Note that the temporal filter-
ing of the motions as shown in the video (5:23) produces a
slight lag when observing the predictions at 1000 FPS.
C. Additional Results
C.1. Comparison with RGB Camera Methods
To demonstrate the robustness of our method to low-light
and high-speed motion sequences, we compare it with
RGB-based methods for 3D reconstruction of interacting
hands [22, 27] on fast motion sequences captured by Sony
RX0. We consider two scenarios, i.e., 25 FPS and at a high
shutter speed, which is 500 FPS in our case. The two se-
quences are captured in different setups: Background activ-
ity from the subject, the distance between the subjects and
the cameras are different compared to Ev2Hands-R; calibra-
tion of the event camera, i.e., p-n bias3 settings and back-
ground noise filter settings are also different compared to
Ev2Hands-R. This shows our method can generalise to dif-
ferent recording setups. Unlike RGB-based hand pose esti-
mation methods, which mostly fail on poorly lit frames (due
to fast shutter speed or motion blur induced by fast hand
motion), our method infers more accurate articulations and
inter-hand distances; see Table II.
C.2. Additional Visualisations
We provide additional visualisations for our method in well-
lit scenarios along with results for low-light and high-speed
motion cases; see Table III. The predicted segmentations, as
shown in our experiments, help the network to disambiguate
left and right hands even under very challenging conditions.
Hence, our Ev2Hands approach is robust to hand occlu-
sions in a wide range of scenarios, making it also suitable
for high-speed two-hand interactions. Please see our video
(1:23) for further qualitative results.
Segmentation Supervision. We next compare the perfor-
mance of our method with pretrained and non-pretrained
segmentation branches.
Both experiments are fine-tuned
with real data. In the supplementary video (6:03), we ob-
serve that segmentation supervision makes our method ro-
bust to ambiguities arising due to intense hand interactions.
3The ”p-n bias” refers to the sensitivity of firing positive and negative
events.
13

Action
RGB
Events
Palm Wave
Dorsal Wave
Wrist Rotation
Articulation
Clap
Intersection
Occlusion
Free Style
Table I. The Ev2Hands-R dataset consists of eight different actions performed by each of the five different actors captured with synchronised
RGB and event cameras.
14

Figure III. PCK curves for the ablation experiments on the synthetic Ev2Hands-S and real Ev2Hands-R datasets. The curve is plotted for
relative PCK (R-PCK) and relative root PCK (RR-PCK).
Figure IV. PCK curves for Moon [27], Li et al. [22], Boukhayma et al. [4], Rudnev et al. [46] and Ev2Hands (Ours) on real Ev2Hands-R
dataset. The curve is plotted for relative PCK (R-PCK) and relative root PCK (RR-PCK).
15

RGB
Events
Li et al.
Moon
Ev2Hands (Ours)
Table II. Comparisons between our method and Li et al. and Moon [22, 27]. The first two rows show the results obtained from the high
shutter speed (500 FPS) sequence while the last three rows show the results obtained from the fast motion sequence recorded at 25 FPS. We
observe that the predictions from Ev2Hands are better with more precise articulations and inter-hand distances. Note: The 3D visualizations
of our method are overlaid onto the event stream.
16

RGB
Events
Event Cloud
Segmentation
Predictions
Table III. Additional visualisations of Ev2Hands along with the visualisations of event clouds, segmentations and the 3D predictions.
17

