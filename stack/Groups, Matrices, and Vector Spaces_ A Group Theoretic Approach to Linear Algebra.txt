Groups, Matrices, 
and Vector Spaces
James B. Carrell
A Group Theoretic Approach 
to Linear Algebra

Groups, Matrices, and Vector Spaces

James B. Carrell
Groups, Matrices, and Vector
Spaces
A Group Theoretic Approach to Linear
Algebra
123

James B. Carrell
Department of Mathematics
University of British Columbia
Vancouver, BC
Canada
ISBN 978-0-387-79427-3
ISBN 978-0-387-79428-0
(eBook)
DOI 10.1007/978-0-387-79428-0
Library of Congress Control Number: 2017943222
Mathematics Subject Classiﬁcation (2010): 15-01, 20-01
© Springer Science+Business Media LLC 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer Science+Business Media, LLC
The registered company address is: 233 Spring Street, New York, NY 10013, U.S.A.

Foreword
This book is an introduction to group theory and linear algebra from a geometric
viewpoint. It is intended for motivated students who want a solid foundation in both
subjects and are curious about the geometric aspects of group theory that cannot be
appreciated without linear algebra. Linear algebra and group theory are connected in
very pretty ways, and so it seems that presenting them together is an appropriate goal.
Group theory, founded by Galois to study the symmetries of roots of polynomial
equations, was extended by many nineteenth-century mathematicians who were also
leading ﬁgures in the development of linear algebra such as Cauchy, Cayley, Schur,
and Lagrange. It is amazing that such a simple concept has touched so many rich areas
of current research: algebraic geometry, number theory, invariant theory, represen-
tation theory, combinatorics, and cryptography, to name some. Matrix groups, which
are part matrix theory, part linear algebra, and part group theory, have turned out to be
richest source of ﬁnite simple groups and the basis for the theory of linear algebraic
groups and for representation theory, two very active areas of current research that
have linear algebra as their basis. The orthogonal and unitary groups are matrix
groups that are fundamental tools for particle physicists and for quantum mechanics.
And to bring linear algebra in, we should note that every student of physics also needs
to know about eigentheory and Jordan canonical form.
For the curious reader, let me give a brief description of what is covered. After a
brief preliminary chapter on combinatorics, mappings, binary operations, and rela-
tions, the ﬁrst chapter covers the basics of group theory (cyclic groups, permutation
groups, Lagrange’s theorem, cosets, normal subgroups, homomorphisms, and quo-
tient groups) and gives an introduction to the notion of a ﬁeld. We deﬁne the basic
ﬁelds Q, R, and C, and discuss the geometry of the complex plane. We also state the
fundamental theorem of algebra and deﬁne algebraically closed ﬁelds. We then
construct the prime ﬁelds Fp for all primes p and deﬁne Galois ﬁelds. It is especially
nice to have ﬁnite ﬁelds, since computations involving matrices over F2 are delight-
fullyeasy.Thelovelysubjectoflinearcodingtheory,whichrequiresF2,willbetreated
in due course. Finally, we deﬁne polynomial rings and prove the multiple root test.
v

We next turn to matrix theory, studying matrices over an arbitrary ﬁeld. The
standard results on Gaussian elimination are proven, and LPDU factorization is
studied. We show that the reduced row echelon form of a matrix is unique, thereby
enabling us to give a rigorous treatment of the rank of a matrix. (The uniqueness
of the reduced row echelon form is a result that most linear algebra books curiously
ignore.) After treating matrix inverses, we deﬁne matrix groups, and give examples
such as the general linear group, the orthogonal group, and the n  n permutation
matrices, which we show are isomorphic to the symmetric group SðnÞ. We conclude
the chapter with the Birkhoff decomposition of the general linear group.
The next chapter treats the determinant. After deﬁning the signature of a per-
mutation and showing that it is a homomorphism, we deﬁne detðAÞ via the alter-
nating sum over the symmetric group known as Leibniz’s formula. The proofs
of the product formula (that is, that det is a homomorphism) and the other basic
results about the determinant are surprisingly simple consequences of the deﬁnition.
The determinant is an important and rich topic, so we treat the standard applications
such as the Laplace expansion and Cramer’s rule, and we introduce the important
special linear group. Finally, we consider a recent application of the determinant
known as Dodgson condensation.
In the next chapter, ﬁnite-dimensional vector spaces, bases, and dimension are
covered in succession, followed by more advanced topics such as direct sums,
quotient spaces, and the Grassmann intersection formula. Inner product spaces over
R and C are covered, and in the appendix, we give an introduction to linear coding
theory and error-correcting codes, ending with perfect codes and the hat game, in
which a player must guess the color of her hat based on the colors of the hats her
teammates are wearing.
The next chapter moves us on to linear mappings. The basic properties such as
the rank–nullity theorem are covered. We treat orthogonal linear mappings and the
orthogonal and unitary groups, and we classify the ﬁnite subgroups of SOð2; RÞ.
Using the Oð2; RÞ dichotomy, we also obtain Leonardo da Vinci’s classiﬁcation
that all ﬁnite subgroups of Oð2; RÞ are cyclic or dihedral. This chapter also covers
dual spaces, coordinates, and the change of basis formulas for matrices of linear
mappings.
We then take up eigentheory: eigenvalues and eigenvectors, the characteristic
polynomial of a linear mapping, and its matrix and diagonalization. We show how
the Fibonacci sequence is obtained from the eigenvalue analysis of a certain
dynamical system. Next, we consider eigenspace decompositions and prove that a
linear mapping is semisimple—equivalently, that its matrix is diagonalizable—if
and only if its minimal polynomial has simple roots. We give a geometric proof
of the principal axis theorem for both Hermitian and real symmetric matrices and
for self-adjoint linear mappings. Our proof of the Cayley–Hamilton theorem uses a
simple inductive argument noticed by the author and Jochen Kuttler. Finally,
returning to the geometry of R3, we show that SOð3; RÞ is the group of rotations of
R3 and conclude with the computation of the rotation groups of several of the
Platonic solids.
vi
Foreword

Following eigentheory, we cover the normal matrix theorem and quadratic
forms, including diagonalization and Sylvester’s law of inertia. Then we classify
linear mappings, proving the Jordan–Chevalley decomposition theorem and the
existence of the Jordan canonical form for matrices over an algebraically closed
ﬁeld. The ﬁnal two chapters concentrate on group theory. The penultimate chapter
establishes the basic theorems of abstract group theory up to the Jordan-Schreier
theorem and gives a treatment of ﬁnite group theory (e.g., Cauchy’s theorem and
the Sylow theorems) using the very efﬁcient approach via group actions and the
orbit-stabilizer theorem. We also classify the ﬁnite subgroups of SOð3; RÞ. The
appendix to this chapter contains a description of how Polish mathematicians
reconstructed the German Enigma machine before the Second World War via group
theory. This was a milestone in abstract algebra and to this day is surely the most
signiﬁcant application of group theory ever made.
The ﬁnal chapter is an informal introduction to the theory of linear algebraic
groups. We give the basic deﬁnitions and discuss the basic concepts: maximal tori,
the Weyl group, Borel subgroups, and the Bruhat decomposition. While these
concepts were already introduced for the general linear group, the general notions
came into use relatively recently. We also consider reductive groups and invariant
theory, which are two topics of contemporary research involving both linear algebra
and group theory.
Acknowledgements: The author is greatly indebted to the editors at Springer, Ann
Kostant (now retired) and Elizabeth Loew, who, patiently, gave me the opportunity
to publish this text. I would also like to thank Ann for suggesting the subtitle.
I would like to thank several colleagues who made contributions and gave me
valuable suggestions. They include Kai Behrend, Patrick Brosnan, Kiumars Kaveh,
Hanspeter Kraft, Jochen Kuttler, David Lieberman, Vladimir Popov, Edward
Richmond, and Zinovy Reichstein. I would also like to thank Cameron Howie for
his very careful reading of the manuscript and many comments.
May 2017
Jim Carrell
Foreword
vii

Contents
1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Sets and Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Binary operations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.1.2
Equivalence relations and equivalence classes . . . . . . .
4
1.2
Some Elementary Combinatorics . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.1
Mathematical induction . . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.2
The Binomial Theorem . . . . . . . . . . . . . . . . . . . . . . . .
8
2
Groups and Fields: The Two Fundamental Notions of Algebra . . .
. . . .
11
2.1
Groups and homomorphisms . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
The Deﬁnition of a Group. . . . . . . . . . . . . . . . . . . . . .
12
2.1.2
Some basic properties of groups . . . . . . . . . . . . . . . . .
13
2.1.3
The symmetric groups SðnÞ . . . . . . . . . . . . . . . . . . . . .
14
2.1.4
Cyclic groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.1.5
Dihedral groups: generators and relations . . . . . . . . . .
16
2.1.6
Subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2.1.7
Homomorphisms and Cayley’s Theorem . . . . . . . . . . .
19
2.2
The Cosets of a Subgroup and Lagrange’s Theorem . . . . . . . . .
23
2.2.1
The deﬁnition of a coset . . . . . . . . . . . . . . . . . . . . . . .
23
2.2.2
Lagrange’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.3
Normal Subgroups and Quotient Groups . . . . . . . . . . . . . . . . . .
29
2.3.1
Normal subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.3.2
Constructing the quotient group G=H . . . . . . . . . . . . .
30
2.3.3
Euler’s Theorem via quotient groups. . . . . . . . . . . . . .
32
2.3.4
The First Isomorphism Theorem . . . . . . . . . . . . . . . . .
34
2.4
Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4.1
The deﬁnition of a ﬁeld. . . . . . . . . . . . . . . . . . . . . . . .
36
2.4.2
Arbitrary sums and products . . . . . . . . . . . . . . . . . . . .
38
2.5
The Basic Number Fields Q, R, and C . . . . . . . . . . . . . . . . . . .
40
2.5.1
The rational numbers Q. . . . . . . . . . . . . . . . . . . . . . . .
40
ix

2.5.2
The real numbers R. . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.5.3
The complex numbers C . . . . . . . . . . . . . . . . . . . . . . .
41
2.5.4
The geometry of C . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.5.5
The Fundamental Theorem of Algebra . . . . . . . . . . . .
45
2.6
Galois ﬁelds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.6.1
The prime ﬁelds Fp . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.6.2
A four-element ﬁeld . . . . . . . . . . . . . . . . . . . . . . . . . .
48
2.6.3
The characteristic of a ﬁeld . . . . . . . . . . . . . . . . . . . . .
49
2.6.4
Appendix: polynomials over a ﬁeld. . . . . . . . . . . . . . .
51
3
Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.1
Introduction to matrices and matrix algebra . . . . . . . . . . . . . . . .
57
3.1.1
What is a matrix? . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
3.1.2
Matrix addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
3.1.3
Examples: matrices over F2. . . . . . . . . . . . . . . . . . . . .
60
3.1.4
Matrix multiplication . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.1.5
The Algebra of Matrix Multiplication . . . . . . . . . . . . .
63
3.1.6
The transpose of a matrix . . . . . . . . . . . . . . . . . . . . . .
64
3.1.7
Matrices and linear mappings . . . . . . . . . . . . . . . . . . .
65
3.2
Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.2.1
Reduced row echelon form and row operations. . . . . .
68
3.2.2
Elementary matrices and row operations . . . . . . . . . . .
70
3.2.3
The row space and uniqueness of reduced row
echelon form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
3.3
Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.3.1
The coefﬁcient matrix of a linear system. . . . . . . . . . .
77
3.3.2
Writing the solutions: the homogeneous case . . . . . . .
78
3.3.3
The inhomogeneous case. . . . . . . . . . . . . . . . . . . . . . .
79
3.3.4
A useful identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition . . .
. . . .
85
4.1
The Inverse of a Square Matrix . . . . . . . . . . . . . . . . . . . . . . . . .
85
4.1.1
The deﬁnition of the inverse . . . . . . . . . . . . . . . . . . . .
85
4.1.2
Results on Inverses . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4.1.3
Computing inverses . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
4.2
Matrix Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.2.1
The deﬁnition of a matrix group . . . . . . . . . . . . . . . . .
93
4.2.2
Examples of matrix groups . . . . . . . . . . . . . . . . . . . . .
94
4.2.3
The group of permutation matrices . . . . . . . . . . . . . . .
95
4.3
The LPDU Factorization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.3.1
The basic ingredients: L, P, D, and U . . . . . . . . . . . . .
100
4.3.2
The main result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
4.3.3
Matrices with an LDU decomposition . . . . . . . . . . . . .
105
4.3.4
The Symmetric LDU Decomposition. . . . . . . . . . . . . .
107
x
Contents

4.3.5
The Ranks of A and AT . . . . . . . . . . . . . . . . . . . . . . . .
108
5
An Introduction to the Theory of Determinants . . . . . . . . . . . . . . . .
113
5.1
An Introduction to the Determinant Function . . . . . . . . . . . . . . .
114
5.1.1
The main theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
5.1.2
The computation of a determinant . . . . . . . . . . . . . . . .
115
5.2
The Deﬁnition of the Determinant . . . . . . . . . . . . . . . . . . . . . . .
119
5.2.1
The signature of a permutation . . . . . . . . . . . . . . . . . .
119
5.2.2
The determinant via Leibniz’s Formula . . . . . . . . . . . .
121
5.2.3
Consequences of the deﬁnition . . . . . . . . . . . . . . . . . .
122
5.2.4
The effect of row operations on the determinant . . . . .
123
5.2.5
The proof of the main theorem . . . . . . . . . . . . . . . . . .
125
5.2.6
Determinants and LPDU . . . . . . . . . . . . . . . . . . . . . . .
125
5.2.7
A beautiful formula: Lewis Carroll’s identity . . . . . . .
126
5.3
Appendix: Further Results on Determinants . . . . . . . . . . . . . . . .
130
5.3.1
The Laplace expansion . . . . . . . . . . . . . . . . . . . . . . . .
130
5.3.2
Cramer’s Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.3.3
The inverse of a matrix over Z . . . . . . . . . . . . . . . . . .
134
6
Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
6.1
The Deﬁnition of a Vector Space and Examples . . . . . . . . . . . .
136
6.1.1
The vector space axioms . . . . . . . . . . . . . . . . . . . . . . .
136
6.1.2
Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
6.2
Subspaces and Spanning Sets . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
6.2.1
Spanning sets. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
6.3
Linear Independence and Bases . . . . . . . . . . . . . . . . . . . . . . . . .
145
6.3.1
The deﬁnition of linear independence . . . . . . . . . . . . .
145
6.3.2
The deﬁnition of a basis . . . . . . . . . . . . . . . . . . . . . . .
147
6.4
Bases and Dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
6.4.1
The deﬁnition of dimension. . . . . . . . . . . . . . . . . . . . .
151
6.4.2
Some examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
6.4.3
The Dimension Theorem . . . . . . . . . . . . . . . . . . . . . . .
153
6.4.4
Finding a basis of the column space . . . . . . . . . . . . . .
156
6.4.5
A Galois ﬁeld application . . . . . . . . . . . . . . . . . . . . . .
157
6.5
The Grassmann Intersection Formula . . . . . . . . . . . . . . . . . . . . .
162
6.5.1
Intersections and sums of subspaces . . . . . . . . . . . . . .
162
6.5.2
Proof of the Grassmann intersection formula. . . . . . . .
163
6.5.3
Direct sums of subspaces. . . . . . . . . . . . . . . . . . . . . . .
165
6.5.4
External direct sums . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.6
Inner Product Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6.6.1
The deﬁnition of an inner product . . . . . . . . . . . . . . . .
169
6.6.2
Orthogonality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
6.6.3
Hermitian inner products . . . . . . . . . . . . . . . . . . . . . . .
173
6.6.4
Orthonormal bases. . . . . . . . . . . . . . . . . . . . . . . . . . . .
174
Contents
xi

6.6.5
The existence of orthonormal bases. . . . . . . . . . . . . . .
175
6.6.6
Fourier coefﬁcients . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
6.6.7
The orthogonal complement of a subspace . . . . . . . . .
177
6.6.8
Hermitian inner product spaces . . . . . . . . . . . . . . . . . .
178
6.7
Vector Space Quotients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
6.7.1
Cosets of a subspace . . . . . . . . . . . . . . . . . . . . . . . . . .
183
6.7.2
The quotient V=W and the dimension formula . . . . . .
184
6.8
Appendix: Linear Coding Theory . . . . . . . . . . . . . . . . . . . . . . . .
187
6.8.1
The notion of a code . . . . . . . . . . . . . . . . . . . . . . . . . .
187
6.8.2
Generating matrices . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
6.8.3
Hamming distance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
6.8.4
Error-correcting codes . . . . . . . . . . . . . . . . . . . . . . . . .
190
6.8.5
Cosets and perfect codes . . . . . . . . . . . . . . . . . . . . . . .
192
6.8.6
The hat problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
7
Linear Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
7.1
Deﬁnitions and Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
7.1.1
Mappings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
7.1.2
The deﬁnition of a linear mapping . . . . . . . . . . . . . . .
198
7.1.3
Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198
7.1.4
Matrix linear mappings . . . . . . . . . . . . . . . . . . . . . . . .
200
7.1.5
An Application: rotations of the plane. . . . . . . . . . . . .
201
7.2
Theorems on Linear Mappings . . . . . . . . . . . . . . . . . . . . . . . . . .
205
7.2.1
The kernel and image of a linear mapping . . . . . . . . .
205
7.2.2
The Rank–Nullity Theorem . . . . . . . . . . . . . . . . . . . . .
206
7.2.3
An existence theorem . . . . . . . . . . . . . . . . . . . . . . . . .
206
7.2.4
Vector space isomorphisms . . . . . . . . . . . . . . . . . . . . .
207
7.3
Isometries and Orthogonal Mappings . . . . . . . . . . . . . . . . . . . . .
211
7.3.1
Isometries and orthogonal linear mappings . . . . . . . . .
211
7.3.2
Orthogonal linear mappings on Rn . . . . . . . . . . . . . . .
212
7.3.3
Projections. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
7.3.4
Reﬂections. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
7.3.5
Projections on a general subspace . . . . . . . . . . . . . . . .
215
7.3.6
Dimension two and the Oð2; RÞ-dichotomy. . . . . . . . .
216
7.3.7
The dihedral group as a subgroup of Oð2; RÞ . . . . . . .
218
7.3.8
The ﬁnite subgroups of Oð2; RÞ . . . . . . . . . . . . . . . . .
219
7.4
Coordinates with Respect to a Basis and Matrices of Linear
Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
7.4.1
Coordinates with respect to a basis . . . . . . . . . . . . . . .
222
7.4.2
The change of basis matrix . . . . . . . . . . . . . . . . . . . . .
223
7.4.3
The matrix of a linear mapping . . . . . . . . . . . . . . . . . .
225
7.4.4
The Case V ¼ W. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226
7.4.5
Similar matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
7.4.6
The matrix of a composition T  S . . . . . . . . . . . . . . .
228
xii
Contents

7.4.7
The determinant of a linear mapping. . . . . . . . . . . . . .
228
7.5
Further Results on Mappings . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.5.1
The space LðV; WÞ . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.5.2
The dual space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
7.5.3
Multilinear maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
7.5.4
A characterization of the determinant . . . . . . . . . . . . .
235
8
Eigentheory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
8.1
The Eigenvalue Problem and the Characteristic Polynomial . . . .
239
8.1.1
First considerations: the eigenvalue problem for
matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
8.1.2
The characteristic polynomial . . . . . . . . . . . . . . . . . . .
241
8.1.3
The characteristic polynomial of a 2  2 matrix . . . . .
243
8.1.4
A general formula for the characteristic polynomial . .
244
8.2
Basic Results on Eigentheory . . . . . . . . . . . . . . . . . . . . . . . . . . .
251
8.2.1
Eigenpairs for linear mappings . . . . . . . . . . . . . . . . . .
251
8.2.2
Diagonalizable matrices. . . . . . . . . . . . . . . . . . . . . . . .
252
8.2.3
A criterion for diagonalizability. . . . . . . . . . . . . . . . . .
254
8.2.4
The powers of a diagonalizable matrix . . . . . . . . . . . .
255
8.2.5
The Fibonacci sequence as a dynamical system . . . . .
256
8.3
Two Characterizations of Diagonalizability. . . . . . . . . . . . . . . . .
259
8.3.1
Diagonalization via eigenspace decomposition . . . . . .
259
8.3.2
A test for diagonalizability . . . . . . . . . . . . . . . . . . . . .
261
8.4
The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . .
268
8.4.1
Statement of the theorem. . . . . . . . . . . . . . . . . . . . . . .
268
8.4.2
The real and complex cases. . . . . . . . . . . . . . . . . . . . .
268
8.4.3
Nilpotent matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
8.4.4
A proof of the Cayley–Hamilton theorem . . . . . . . . . .
269
8.4.5
The minimal polynomial of a linear mapping . . . . . . .
271
8.5
Self Adjoint Mappings and the Principal Axis Theorem. . . . . . .
274
8.5.1
The notion of self-adjointness . . . . . . . . . . . . . . . . . . .
274
8.5.2
Principal Axis Theorem for self-adjoint linear
mappings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
8.5.3
Examples of self-adjoint linear mappings . . . . . . . . . .
277
8.5.4
A projection formula for symmetric matrices . . . . . . .
278
8.6
The Group of Rotations of R3 and the Platonic Solids. . . . . . . .
283
8.6.1
Rotations of R3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283
8.6.2
The Platonic solids . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
8.6.3
The rotation group of a Platonic solid . . . . . . . . . . . . .
287
8.6.4
The cube and the octahedron. . . . . . . . . . . . . . . . . . . .
288
8.6.5
Symmetry groups . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
8.7
An Appendix on Field Extensions . . . . . . . . . . . . . . . . . . . . . . .
294
Contents
xiii

9
Unitary Diagonalization and Quadratic Forms . . . . . . . . . . . . . . . . .
297
9.1
Schur Triangularization and the Normal Matrix Theorem. . . . . .
297
9.1.1
Upper triangularization via the unitary group . . . . . . .
298
9.1.2
The normal matrix theorem . . . . . . . . . . . . . . . . . . . . .
299
9.1.3
The Principal axis theorem: the short proof. . . . . . . . .
300
9.1.4
Other examples of normal matrices . . . . . . . . . . . . . . .
301
9.2
Quadratic Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
9.2.1
Quadratic forms and congruence . . . . . . . . . . . . . . . . .
305
9.2.2
Diagonalization of quadratic forms . . . . . . . . . . . . . . .
306
9.2.3
Diagonalization in the real case. . . . . . . . . . . . . . . . . .
307
9.2.4
Hermitian forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
9.2.5
Positive deﬁnite matrices . . . . . . . . . . . . . . . . . . . . . . .
308
9.2.6
The positive semideﬁnite case . . . . . . . . . . . . . . . . . . .
310
9.3
Sylvester’s Law of Inertia and Polar Decomposition . . . . . . . . .
313
9.3.1
The law of inertia . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
9.3.2
The polar decomposition of a complex linear
mapping. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
10
The Structure Theory of Linear Mappings . . . . . . . . . . . . . . . . . . . .
319
10.1
The Jordan–Chevalley Theorem . . . . . . . . . . . . . . . . . . . . . . . . .
320
10.1.1
The statement of the theorem . . . . . . . . . . . . . . . . . . .
320
10.1.2
The multiplicative Jordan–Chevalley decomposition . .
322
10.1.3
The proof of the Jordan–Chevalley theorem . . . . . . . .
323
10.1.4
An example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
10.1.5
The Lie bracket . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
10.2
The Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
10.2.1
Jordan blocks and string bases . . . . . . . . . . . . . . . . . .
328
10.2.2
Jordan canonical form . . . . . . . . . . . . . . . . . . . . . . . . .
329
10.2.3
String bases and nilpotent endomorphisms . . . . . . . . .
330
10.2.4
Jordan canonical form and the minimal
polynomial. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
10.2.5
The conjugacy class of a nilpotent matrix . . . . . . . . . .
334
11
Theorems on Group Theory. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
11.1
Group Actions and the Orbit Stabilizer Theorem . . . . . . . . . . . .
338
11.1.1
Group actions and G-sets . . . . . . . . . . . . . . . . . . . . . .
338
11.1.2
The orbit stabilizer theorem. . . . . . . . . . . . . . . . . . . . .
341
11.1.3
Cauchy’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
11.1.4
Conjugacy classes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
11.1.5
Remarks on the center. . . . . . . . . . . . . . . . . . . . . . . . .
344
11.1.6
A ﬁxed-point theorem for p-groups . . . . . . . . . . . . . . .
344
11.1.7
Conjugacy classes in the symmetric group . . . . . . . . .
345
11.2
The Finite Subgroups of SOð3; RÞ . . . . . . . . . . . . . . . . . . . . . . .
349
11.2.1
The order of a ﬁnite subgroup of SOð3; RÞ . . . . . . . . .
349
xiv
Contents

11.2.2
The order of a stabilizer Gp . . . . . . . . . . . . . . . . . . . .
351
11.3
The Sylow Theorems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
11.3.1
The ﬁrst Sylow theorem . . . . . . . . . . . . . . . . . . . . . . .
354
11.3.2
The second Sylow theorem . . . . . . . . . . . . . . . . . . . . .
355
11.3.3
The third Sylow theorem. . . . . . . . . . . . . . . . . . . . . . .
355
11.3.4
Groups of order 12, 15, and 24 . . . . . . . . . . . . . . . . . .
356
11.4
The Structure of Finite Abelian Groups . . . . . . . . . . . . . . . . . . .
359
11.4.1
Direct products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
11.4.2
The structure theorem for ﬁnite abelian groups . . . . . .
361
11.4.3
The Chinese Remainder Theorem . . . . . . . . . . . . . . . .
362
11.5
Solvable Groups and Simple Groups . . . . . . . . . . . . . . . . . . . . .
364
11.5.1
The deﬁnition of a solvable group. . . . . . . . . . . . . . . .
364
11.5.2
The commutator subgroup. . . . . . . . . . . . . . . . . . . . . .
366
11.5.3
An example: Að5Þ is simple. . . . . . . . . . . . . . . . . . . . .
367
11.5.4
Simple groups and the Jordan–Hölder theorem . . . . . .
369
11.5.5
A few brief remarks on Galois theory . . . . . . . . . . . . .
370
11.6
Appendix: SðnÞ, Cryptography, and the Enigma. . . . . . . . . . . . .
374
11.6.1
Substitution ciphers via Sð26Þ . . . . . . . . . . . . . . . . . . .
374
11.6.2
The Enigma. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
11.6.3
Rejewski’s theorem on idempotents in SðnÞ . . . . . . . .
377
11.7
Breaking the Enigma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
12
Linear Algebraic Groups: an Introduction . . . . . . . . . . . . . . . . . . . .
383
12.1
Linear Algebraic Groups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
383
12.1.1
Reductive and semisimple groups . . . . . . . . . . . . . . . .
385
12.1.2
The classical groups . . . . . . . . . . . . . . . . . . . . . . . . . .
386
12.1.3
Algebraic tori . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
386
12.1.4
The Weyl group . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
388
12.1.5
Borel subgroups. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
12.1.6
The conjugacy of Borel subgroups . . . . . . . . . . . . . . .
391
12.1.7
The ﬂag variety of a linear algebraic group. . . . . . . . .
392
12.1.8
The Bruhat decomposition of GLðn; FÞ . . . . . . . . . . . .
393
12.1.9
The Bruhat decomposition of a reductive group . . . . .
395
12.1.10
Parabolic subgroups. . . . . . . . . . . . . . . . . . . . . . . . . . .
396
12.2
Linearly reductive groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
12.2.1
Invariant subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
12.2.2
Maschke’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . .
398
12.2.3
Reductive groups. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
12.2.4
Invariant theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Contents
xv

Chapter 1
Preliminaries
In this brief chapter, we will introduce (or in many cases recall) some elemen-
tary concepts that will be used throughout the text. It will be convenient to
state them at the beginning so that they will all be in the same place.
1.1
Sets and Mappings
A set is a collection of objects called the elements of X. If X is a set, the
notation x ∈X will mean that x is an element of X. Sets are frequently
deﬁned in terms of a property. For example, if R denotes the set of all real
numbers, then the set of all positive real numbers is denoted by the expression
{r ∈R | r > 0}. One can also deﬁne a set by listing its elements, an example
being the set consisting of the integers 1, 2, and 3, which could be denoted
by either
{1, 2, 3},
or, more clumsily,
{r ∈R | r is an integer and 1 ≤r ≤3}.
A set with exactly one element is called a singleton.
The union of two sets X and Y is the set
X ∪Y = {a | a ∈X or a ∈Y }
whose elements are the elements of X together with the elements of Y . The
intersection of X and Y is the set
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 1
1

2
1
Preliminaries
X ∩Y = {a | a ∈X and a ∈Y }.
The diﬀerence of X and Y is the set
X\Y = {x ∈X | x /∈Y }.
Note that Y does not need to be a subset of X for one to speak about the
diﬀerence X\Y . Notice that set union and diﬀerence are analogous to addition
and subtraction. Intersection is somewhat analogous to multiplication. For
example, for any three sets X, Y, Z, one has
X ∩(Y ∪Z) = (X ∩Y ) ∪(X ∩Z),
which is analogous to the distributive law a(b + c) = ab + ac for real numbers
a, b, c. The product of X and Y is the set
X × Y = {(x, y) | x ∈X and y ∈Y }.
We call (x, y) an ordered pair. That is, when X = Y , then (x, y) ̸= (y, x)
unless x = y. The product X × X can be denoted by X2. For example, R × R
is the Cartesian plane, usually denoted by R2.
A mapping from X to Y is a rule F that assigns to every element x ∈X
a unique element F(x) ∈Y . The notation F : X →Y will be used to denote
a mapping from X to Y ; X is called the domain of F, and Y is called its
target. The image of F is
F(X) = {y ∈Y | y = F(x) for some x ∈X}.
For example, if F : R →R is the mapping F(r) = r2, then F(R) is the set
of all nonnegative reals. The composition of two mappings F : X →Y and
G : W →X is the mapping F ◦G : W →Y deﬁned by G ◦F(w) = F(G(w)).
The composition F ◦G is deﬁned whenever the domain of F is contained in
the image of G.
1.1.1
Binary operations
The following notion will be used in the deﬁnition of a ﬁeld.
Deﬁnition 1.1. A binary operation on a set X is a function
F : X × X →X.

1.1
Sets and Mappings
3
Example 1.1. Let Z denote the set of integers. There are two binary opera-
tions on Z called addition and multiplication. They are deﬁned, respectively,
by F+(m, n) = m + n and F·(m, n) = mn. Note that division is not a binary
operation on Z.
We also need the notion of a subset being closed with respect to a binary
operation.
Deﬁnition 1.2. Let F be a binary operation on a set A. A subset B of A
such that F(x, y) ∈B whenever x, y ∈B is said to be closed under the binary
operation F.
For example, let A = Z and let B be the set of all nonnegative integers.
Then B is closed under both addition and multiplication. The odd integers
are closed under multiplication, but not closed under addition, since, for
instance, 1 + 1 = 2.
If F is a mapping from X to Y and y ∈Y , then the inverse image of y is
F −1(y) = {x ∈X | F(x) = y}.
Of course, F −1(y) may not have any elements; that is, it may be the empty
set. For example, if F : R →R is the mapping F(r) = r2, then F −1(−1) is
empty. Notice that if y ̸= y′, then F −1(y) ∩F −1(y′) is the empty set.
The notion of the inverse image of an element is useful in deﬁning some
further properties of mappings. For example, a mapping F : X →Y is one
to one, or injective, if and only if F −1(y) is either empty or a single element
of X for all y ∈Y . In other words, F is injective if and only if F(x) = F(x′)
implies x = x′. Similarly, F is onto, or equivalently, surjective, if F −1(y) is
nonempty for all y ∈Y . Alternatively, F is surjective if and only if F(X) = Y .
A mapping F : X →Y that is both injective and surjective is said to be
bijective. A mapping that is injective, surjective, or bijective will be called
an injection, surjection, or bijection respectively. A bijective map F : X →Y
has an inverse mapping F −1, which is deﬁned by putting F −1(y) = x if and
only if F(x) = y. It follows directly from the deﬁnition that F −1 ◦F(x) = x
and F ◦F −1(y) = y for all x ∈X and y ∈Y .
The following proposition gives criteria for injectivity and surjectivity.
Proposition 1.1. Suppose F : X →Y is a mapping and suppose there exists
a mapping G : Y →X such that G ◦F(x) = x for all x ∈X. Then F is injec-
tive and G is surjective. Moreover, F is bijective if and only if G ◦F and
F ◦G are the identity mappings on X and Y respectively.

4
1
Preliminaries
1.1.2
Equivalence relations and equivalence
classes
We now want to deﬁne an equivalence relation on a set. This will give us a
way of partitioning a set into disjoint subsets called equivalence classes. The
notion of equivalence is a generalization of the notion of equality. First, we
need to recall what a relation on a set is.
Deﬁnition 1.3. Let S be a nonempty set. A subset E of S × S is called a
relation on S. If E is a relation on S, and a and b are elements of S, we will
say that a and b are related by E and write aEb if and only if (a, b) ∈E.
A relation E on S is called an equivalence relation when the following three
conditions hold for all a, b, c ∈S:
(i) (E is reﬂexive) aEa,
(ii) (E is symmetric) if aEb, then bEa, and
(iii) (E is transitive) if aEb and bEc, then aEc.
If E is an equivalence relation on S and a ∈S, then the equivalence class of
a is deﬁned to be the set of all elements b ∈S such that bEa. An element of
an equivalence class is called a representative of the class.
Before proving the main property of an equivalence relation, we will con-
sider two basic examples.
Example 1.2. The model on which the notion of an equivalence relation is
built is equality. On an arbitrary nonempty set S, let us say that sEt if and
only if s = t. The equivalence classes consist of the singletons {s}, as s varies
over S.
⊠
The second example is an equivalence relation on the integers frequently
used in number theory.
Example 1.3 (The Integers Modulo m). Let m denote an integer, and con-
sider the pairs of integers (r, s) such that r −s is divisible by m. That is,
r −s = km for some integer k. This deﬁnes a relation Cm on Z × Z called
congruence modulo m. When (r, s) ∈Cm, one usually writes r ≡s mod(m).
We claim that congruence modulo m is an equivalence relation. For exam-
ple, rCmr, and if rCms, then certainly sCmr. For transitivity, assume rCms
and sCmt. Then r −s = 2i and s −t = 2j for some integers i and j. Hence
r −t = (r −s) + (s −t) = 2i + 2j = 2(i + j), so rCmt. Hence Cm is an equiv-
alence relation on Z.
⊠
Here is the basic result on equivalence relations.

1.1
Sets and Mappings
5
Proposition 1.2. Let E be an equivalence relation on a set S. Then every
element a ∈S is in its own equivalence class, and two equivalence classes are
either disjoint or equal. Therefore, S is the disjoint union of the equivalence
classes of E.
Proof. Every element is equivalent to itself, so S is the union of its equivalence
classes. We have to show that two equivalence classes are either equal or
disjoint. Suppose C1 and C2 are equivalence classes, and let c ∈C1 ∩C2. If
a ∈C1, then aEc. If C2 is the equivalence class of b, then cEb, so aEb. Hence,
a ∈C2, so C1 ⊂C2. Similarly, C2 ⊂C1, so C1 = C2.
□
Deﬁnition 1.4. The set of equivalence classes of an equivalence relation E
on S is called the quotient of S by E.

6
1
Preliminaries
1.2
Some Elementary Combinatorics
Combinatorics deals with the properties of various kinds of ﬁnite sets. A
nonempty set X is said to be ﬁnite if there exist an integer n > 0 and a
bijection σ : {1, 2, . . . , n} →X. If X is ﬁnite, the number of elements of X
is denoted by |X|. If X is the empty set, we deﬁne |X| = 0. The following
result is an example of an elementary combinatorial result.
Proposition 1.3. Let X be ﬁnite set that is the union of mutually disjoint
(nonempty) subsets X1, . . . , Xk. Then
|X| =
k

i=1
|Xi|.
In particular, if Y is a proper subset of X, then |Y | < |X|.
Proof. We will consider the case k = 2 ﬁrst and then ﬁnish the proof by apply-
ing the principle of mathematical induction, which is introduced in the next
section. Suppose X = X1 ∪X2, where X1 ∩X2 is empty and both X1 and X2
are nonempty. Let |X1| = j and |X2| = k. By deﬁnition, there exist bijections
σ1 : {1, . . . , i} →X1 and σ2 : {1, . . . , j} →X2. Deﬁne σ : {1, . . . , i + j} →X
by σ(r) = σ1(r) if 1 ≤r ≤i and σ(r) = σ2(r −i) if i + 1 ≤r ≤i + j. By con-
struction, σ is a bijection from {1, . . . , i + j} to X = X1 ∪X2. Therefore,
|X| = i + j = |X1| + |X2|.
□
This identity applies, for example, to the equivalence classes of an equiv-
alence relation on a ﬁnite set. Another application (left to the reader) is
contained in the following proposition.
Proposition 1.4. If X and Y are ﬁnite sets, then
|X ∪Y | = |X| + |Y | −|X ∩Y |.
Here is another consequence.
Proposition 1.5. Let X and Y be ﬁnite and F : X →Y . Then
|X| =

y∈Y
|F −1(y)|.
(1.1)
In particular, if F is surjective, then |X| ≥|Y |.
Proof. Let y ∈Y . By deﬁnition, F −1(y) is a nonempty subset of X if and only
if y ∈F(X). Moreover, F −1(y) ∩F −1(y′) is empty if y ̸= y′ for any y′ ∈Y .
Thus, X can be written as the disjoint union of nonempty subsets

1.2
Some Elementary Combinatorics
7
X =

y∈F (X)
F −1(y).
Since
|F −1(y)| = 0
if
y /∈F(X),
the
identity
(1.1)
follows
from
Proposition 1.3.
□
Proposition 1.6 (The Pigeonhole Principle). Let X and Y be ﬁnite sets
with |X| = |Y |, and suppose F : X −→Y is a mapping. If F is either injective
or surjective, then F is a bijection.
Proof. If F is injective, then |F −1(y)| is either 0 or 1 for all y ∈Y . But if
|F −1(y)| = 0 for some y ∈Y , then
|X| =

y∈Y
|F −1(y)| < |Y |,
which is impossible, since |X| = |Y |. Thus, F is surjective. On the other hand,
if F is surjective, then |F −1(y)| ≥1 for all y. Thus,
|Y | ≤

y∈Y
|F −1(y)| = |X|.
Since |X| = |Y |, |F −1(y)| = 1 for all y, so F is a bijection.
□
1.2.1
Mathematical induction
Mathematical induction is a method of proof that allows one to prove propo-
sitions that state that some property holds for the set of all positive integers.
Here is an elementary example.
Proposition 1.7 (The Principle of Mathematical Induction). A proposition
P(n) deﬁned for each positive integer n holds for all positive integers provided:
(i) P(n) holds for n = 1, and
(ii) P(n + 1) holds whenever P(n) holds.
The proof is an application of the fact that every nonempty set of positive
integers has a least element.
□
Let us now ﬁnish the proof of Proposition 1.3. Let P(k) be the conclusion of
the proposition when X is any ﬁnite set that is the union of mutually disjoint
(nonempty) subsets X1, . . . , Xk. The statement P(1) is true, since X = X1.
Now assume that P(i) holds for i < k, where k > 1. Let Y1 = X1 ∪· · · Xk−1
and Y2 = Xk. Now X = Y1 ∪Y2, and since Y1 and Y2 are disjoint, |X| =

8
1
Preliminaries
|Y1| + |Y2|, as we already showed. Now apply the principle of mathematical
induction: since P(k −1) holds, |Y1| = |X1| + · · · |Xk−1|. Thus,
|X| = |Y1| + |Y2| = |X1| + · · · |Xk−1| + |Xk|,
which is exactly the assertion that P(k) holds. Hence Proposition 1.3 is proved
for all k.
□
Here is a less pedestrian application.
Proposition 1.8. For every positive integer n,
1 + 2 + · · · + n = n(n + 1)
2
.
(1.2)
Proof. Equality certainly holds if n = 1. Suppose (1.2) holds for an integer
k > 0. We have to show that it holds for k + 1. Applying (1.2) for k, we see
that
1 + 2 + · · · + k + (k + 1) = k(k + 1)
2
+ (k + 1).
But
k(k + 1)
2
+ (k + 1) = k(k + 1) + 2(k + 1)
2
= (k + 2)(k + 1)
2
,
so indeed (1.2) holds for (k + 1). Hence, by the principle of mathematical
induction, (1.2) holds for all positive integers n.
Induction proofs can often be avoided. For example, one can also see the
identity (1.2) by observing that the sum of the integers in the array
1
2
· · ·
(n −1)
n
n
(n −1)
· · ·
2
1
is n(n + 1), since there are n columns, and each column sum is n + 1.
1.2.2
The Binomial Theorem
The binomial theorem is a formula for expanding (a + b)n for any positive
integer n, where a and b are variables that commute. The formula uses the
binomial coeﬃcients. First we note that if n is a positive integer, then by
deﬁnition, n! = 1 · 2 · · · n: we also deﬁne 0! = 1. Then the binomial coeﬃcients
are the integers

1.2
Some Elementary Combinatorics
9
n
i

=
n!
i! (n −i)!,
(1.3)
where 0 ≤i ≤n.
One can show that the binomial coeﬃcient (1.3) is exactly the number of
subsets of {1, 2, . . . , n} with exactly i elements. The binomial theorem states
that
(a + b)n =
n

i=0
n
i

an−ibi.
(1.4)
A typical application of the binomial theorem is the formula
2n =
n

i=0
n
i

.
This shows that a set with n elements has exactly 2n subsets.
The binomial theorem is typical of the kind of result that is most easily
proven by induction. The multinomial theorem is a generalization of the
binomial theorem that gives a formula for expanding quantities such as (a +
b + c)3. Let n be a positive integer and suppose n1, n2, . . . , nk are nonnegative
integers such that n1 + · · · + nk = n. The associated multinomial coeﬃcient
is deﬁned as

n
n1, n2, · · · , nk

=
n!
n1!n2! · · · nk!.
This multinomial coeﬃcient is the number of ways of partitioning a set with n
objects into k subsets, the ﬁrst with n1 elements, the second with n2 elements,
and so forth. The multinomial theorem goes as follows.
Theorem 1.9 (Multinomial theorem). Let a1, . . . , ak be commuting vari-
ables. Then
(a1 + a2 + · · · + ak)n =

n1+···+nk=n

n
n1, n2, · · · , nk

an1
1 an2
2 · · · ank
k .
(1.5)

Chapter 2
Groups and Fields: The Two
Fundamental Notions of Algebra
Algebra is the mathematical discipline that arose from the problem of solving
equations. If one starts with the integers Z, one knows that every equa-
tion a + x = b, where a and b are integers, has a unique solution. However,
the equation ax = b does not necessarily have a solution in Z, or it might
have inﬁnitely many solutions (take a = b = 0). So let us enlarge Z to the
rational numbers Q, consisting of all fractions c/d, where d ̸= 0. Then both
equations have a unique solution in Q, provided that a ̸= 0 for the equation
ax = b. So Q is a ﬁeld. If, for example, one takes the solutions of an equation
such as x2 −5 = 0 and forms the set of all numbers of the form a + b
√
5,
where a and b are rational, we get a larger ﬁeld, denoted by Q(
√
5), called
an algebraic number ﬁeld. In the study of ﬁelds obtained by adjoining the
roots of polynomial equations, a new notion arose, namely, the symmetries of
the ﬁeld that permute the roots of the equation. ´Evariste Galois (1811–1832)
coined the term group for these symmetries, and now this group is called
the Galois group of the ﬁeld. While still a teenager, Galois showed that the
roots of an equation are expressible by radicals if and only if the group of the
equation has a property now called solvability. This stunning result solved
the 350-year-old question whether the roots of every polynomial equation are
expressible by radicals.
2.1
Groups and homomorphisms
We now justly celebrate the Galois group of a polynomial, and indeed, the
Galois group is still an active participant in the fascinating theory of elliptic
curves. It even played an important role in the solution of Fermat’s last
theorem. However, groups themselves have turned out to be central in all
sorts of mathematical disciplines, particularly in geometry, where they allow
us to classify the symmetries of a particular geometry. And they have also
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 2
11

12
2
Groups and Fields: The Two Fundamental Notions of Algebra
become a staple in other disciplines such as chemistry (crystallography) and
physics (quantum mechanics).
In this section we will deﬁne the basic concepts of group theory starting
with the deﬁnition of a group itself and the most basic related concepts such
as cyclic groups, the symmetric group. and group homomorphisms. We will
also prove some of the beginning results in group theory such as Lagrange’s
theorem and Cayley’s theorem.
2.1.1
The Deﬁnition of a Group
The notion of a group involves a set with a binary operation that satisﬁes
three natural properties. Before stating the deﬁnition, let us mention some
basic but very diﬀerent examples to keep in mind. The ﬁrst is the integers
under the operation of addition. The second is the set of all bijections of a
set, and the third is the set of all complex numbers ζ such that ζn = 1. We
now state the deﬁnition.
Deﬁnition 2.1. A group is a set G with a binary operation written (x, y) →
xy such that
(i) (xy)z = x(yz) for all x, y, z ∈G;
(ii) G contains an identity element 1 such that 1x = x1 = x for all x ∈G,
and
(iii) if x ∈G, then there exists y ∈G such that xy = 1. In this case, we say
that every element of G has a right inverse.
Property (i) is called the associative law. In other words, the group oper-
ation is associative. In group theory, it is customary to use the letter e to
denote an identity. But it is more convenient for us to use 1. Note that prop-
erty (iii) involves being able to solve an equation that is a special case of
the equation ax = b considered above. There are several additional proper-
ties that one can impose to deﬁne special classes of groups. For example, we
might require that the group operation be independent of the order in which
we take the group elements. More precisely, we make the following deﬁnition.
Deﬁnition 2.2. A group G is said to be commutative or abelian if and only
if for all x, y ∈G, we have xy = yx. A group that is not abelian is said to be
nonabelian.
Example 2.1 (The Integers). The integers Z form a group under addition.
The fact that addition is associative is well known. Zero is an additive identity.
In fact, it is the only additive identity. An additive inverse of m ∈Z is its
negative −m: m + (−m) = 0. Moreover, Z is abelian: m + n = n + m for all
m, n ∈Z.
⊠

2.1
Groups and homomorphisms
13
The group G = {1, −1} under multiplication is an even simpler example
of an abelian group. Before we consider some examples of groups that are
nonabelian, we will introduce a much more interesting class of groups, namely
the ﬁnite groups.
Deﬁnition 2.3. A group G is said to be ﬁnite if the number |G| of elements
in the set G is ﬁnite. We will call |G| the order of G.
The order of G = {1, −1} is two, while Z is an inﬁnite group. Of course,
it is not clear yet why we say that the abelian groups are not as interesting
as the ﬁnite groups. But this will become evident later.
2.1.2
Some basic properties of groups
Before going on to more examples of groups, we would like to prove a propo-
sition that gives some basic consequences of the deﬁnition of a group. In
particular, we will show that there is only one identity element 1, and we will
also show that every element x in a group has a unique two-sided inverse x−1.
The reader may ﬁnd the proofs amusing. Before we state this proposition,
the reader may want to recall that we already noticed these facts in Z: there
is only one additive identity, 0, and likewise only one right inverse, −m, for
each m. Moreover, −m is also a left inverse of m. These properties are usually
stated as part of the deﬁnition of a group, but for reasons we cannot explain
now, we have chosen to use a minimal set of group axioms.
Proposition 2.1. In every group, there is exactly one identity element, 1.
Furthermore, if y is a right inverse of x, then xy = yx = 1. Hence, a right
inverse is also a left inverse. Therefore, each x ∈G has a two-sided inverse
y, which is characterized by the property that either xy = 1 or yx = 1. More-
over, each two sided inverse is unique.
Proof. To prove the uniqueness of 1, suppose the 1 and 1′ are both identity
elements. Then 1 = 1 · 1′ = 1′. Thus the identity is unique. Now let x have a
right inverse y and let w be a right inverse of y. Then
w = 1w = (xy)w = x(yw) = x1 = x.
Since w = x, it follows that if xy = 1, then yx = 1. Thus, every right inverse is
a two-sided inverse. We will leave the assertion that inverses are unique as an
exercise.
□
From now on, we will refer to the unique left or right inverse of x as the
inverse of x. The notation for the inverse of x is x−1. The next result is the
formula for the inverse of a product.
Proposition 2.2. For all x, y ∈G, we have (xy)−1 = y−1x−1.

14
2
Groups and Fields: The Two Fundamental Notions of Algebra
Proof. Let w = y−1x−1. Then it suﬃces to show that w(xy) = 1. But
w(xy) = (wx)y = ((y−1x−1)x)y = (y−1(x−1x))y = (y−11)y = y−1y = 1.
□
If x1, x2, . . . , xn are arbitrary elements of a group G, then the expression
x1x2 · · · xn will stand for x1(x2 · · · xn), where x2 · · · xn = x2(x3 · · · xn) and so
on. This gives an inductive deﬁnition of the product of an arbitrary ﬁnite
number of elements of G. Moreover, by associativity, pairs (· · · ) of parentheses
can be inserted or removed in the expression x1x2 · · · xn without making any
change in the group element being represented, provided the new expression
makes sense. (For example, you can’t have an empty pair of parentheses,
and the number of left parentheses has to be the same as the number of
right parentheses.) Thus the calculation in the proof of Proposition 2.2 can
be simpliﬁed to
(y−1x−1)(xy) = y−1(x−1x)y−1 = y1y−1 = 1.
2.1.3
The symmetric groups S(n)
We now come to the symmetric groups, also known as the permutation
groups. They form the single most important class of ﬁnite groups. All per-
mutation groups of order greater than two are nonabelian, but more impor-
tantly, permutation groups are one of the foundational tools in the discipline
of combinatorics. The symmetric group is undoubtedly the single most fre-
quently encountered ﬁnite group in mathematics. In fact, as we shall soon
see, all ﬁnite groups of order n can be realized inside the symmetric group
S(n). This fact, known as Cayley’s theorem, will be proved at the end of this
section.
Let X denote a set. A bijective mapping σ : X →X will be called
a permutation of X. The set of all permutations of X is denoted by Sym(X)
and (due to the next result) is called the symmetric group of X. When
X = {1, 2, . . . , n}, Sym(X) is denoted by S(n) and called (somewhat inaccu-
rately) the symmetric group on n letters.
Proposition 2.3. The set Sym(X) of permutations of X is a group under
composition whose identity element is the identity map idX : X →X. If
|X| = n, then |Sym(X)| = n!.
Proof. That Sym(X) is a group follows from the fact that the composi-
tion of two bijections is a bijection, the inverse of a bijection is a bijec-
tion, and the identity map is a bijection. The associativity follows from
the fact that composition of mappings is associative. Now suppose that
X = {x1, . . . , xn}. To deﬁne an element of Sym(X), it suﬃces, by the pigeon-
hole principle (see Chap. 1), to deﬁne an injection σ : X →X. Note that

2.1
Groups and homomorphisms
15
there are n choices for the image σ(x1). In order to ensure that σ is one to
one, σ(x2) cannot be σ(x1). Hence there are n −1 possible choices for σ(x2).
Similarly, there are n −2 possible choices for σ(x3), and so on. Thus the num-
ber of injective maps σ : X →X is n(n −1)(n −2) · · · 2 · 1 = n!. Therefore,
|Sym(X)| = n!.
□
In the following example, we will consider a scheme for writing down the
elements of S(3) that easily generalizes to S(n) for all n > 0. We will also see
that S(3) is nonabelian.
Example 2.2. To write down the six elements σ of S(3), we need a way to
encode σ(1), σ(2), and σ(3). To do so, we represent σ by the array
 1
2
3
σ(1)
σ(2)
σ(3)

.
For example, if σ(1) = 2, σ(2) = 3, and σ(3) = 1, then
σ =

1
2
3
2
3
1

.
We will leave it to the reader to complete the list of elements of S(3). If
n > 2, then S(n) is nonabelian: the order in which two permutations are
applied matters. For example, if n = 3 and
τ =

1
2
3
3
2
1

,
then
στ =
1
2
3
1
3
2

,
while
τσ =
1
2
3
2
1
3

.
Hence στ ̸= τσ.
⊠
2.1.4
Cyclic groups
The next class of groups we will consider consists of the cyclic groups. Before
deﬁning these groups, we need to explain how exponents work. If G is a group
and x ∈G, then if m is a positive integer, xm = x · · · x (m factors). We deﬁne
x−m to be (x−1)m. Also, x0 = 1. Then the usual laws of exponentiation hold
for all integers m, n:

16
2
Groups and Fields: The Two Fundamental Notions of Algebra
(i) xmxn = xm+n,
(ii) (xm)n = xmn.
Deﬁnition 2.4. A group G is said to be cyclic if there exists an element
x ∈G such that for every y ∈G, there is an integer m such that y = xm.
Such an element x is said to generate G.
In particular, Z is an example of an inﬁnite cyclic group in which zm is
interpreted as mz. The multiplicative group G = {1, −1} is also cyclic. The
additive groups Zm consisting of integers modulo a positive integer m form an
important class of cyclic groups, which we will deﬁne after discussing quotient
groups. They are the building blocks of the ﬁnite (in fact, ﬁnitely generated)
abelian groups. However, this will not be proven until Chap. 11. Notice that
all cyclic groups are abelian. However, cyclic groups are not so common. For
example, S(3) is not cyclic, nor is Q, the additive group of rational numbers.
We will soon prove that all ﬁnite groups of prime order are cyclic.
When G is a ﬁnite cyclic group and x ∈G is a generator of G, one often
writes G =< x >. It turns out, however, that a ﬁnite cyclic group can have
several generators, so the expression G =< x > is not necessarily unique.
To see an example of this, consider the twenty-four hour clock as a ﬁnite
cyclic group. This is a preview of the group Zm of integers modulo m, where
m = 24.
Example 2.3. Take a clock with 24 hours numbered 0 through 23. The
group operation on this clock is time shift by some whole number n of hours.
A forward time shift occurs when n is positive, and a backward time shift
occurs when n is negative. When n = 0, no shift occurs, so hour 0 will be the
identity. A one-hour time shift at 23 hours sends the time to 0 hours, while
a two-hour time shift sends 23 hours to 1 hour, and so on. In other words,
the group operation is addition modulo 24. The inverse of, say, the ninth
hour is the ﬁfteenth hour. Two hours are inverse to each other if shifting one
by the other puts the time at 0 hour. This makes the 24-hour clock into a
group of order 24, which is in fact cyclic, since repeatedly time shifting by
one hour starting at 0 hours can put you at any hour. However, there are
other generators, and we will leave it as an exercise to ﬁnd all of them.
⊠
Another interesting ﬁnite cyclic group is the group Cn of nth roots of unity,
that is, the solutions of the equation ζn = 1. We will postpone the discussion
of Cn until we consider the complex numbers C.
2.1.5
Dihedral groups: generators and relations
In the next example, we give an illustration of a group G that is described
by giving a set of its generators and the relations the generators satisfy. The

2.1
Groups and homomorphisms
17
group we will study is called the dihedral group. We will see in due course that
the dihedral groups are the symmetry groups of the regular polygons in the
plane. As we will see in this example, deﬁning a group by giving generators
and relations does not necessarily reveal much information about the group.
Example 2.4. (Dihedral Groups) The dihedral groups are groups that are
deﬁned by specifying two generators a and b and also specifying the rela-
tions that the generators satisfy. When we deﬁne a group by generators and
relations, we consider all words in the generators, in this case a and b: these
are all the strings or products x1x2 · · · xn, where each xi is either a or b,
and n is an arbitrary positive integer. For example, abbaabbaabba is a word
with n = 16. Two words are multiplied together by placing them side by side.
Thus,
(x1x2 · · · xn)(y1y2 · · · yp) = x1x2 · · · xny1y2 · · · yp.
This produces an associative binary operation on the set of words. The next
step is to impose some relations that a and b satisfy. Suppose m > 1. The
dihedral group D(m) is deﬁned to be the set of all words in a and b with the
above multiplication that we assume is subject to the following relations:
am = b2 = 1,
ab = bam−1.
(2.1)
It is understood that the cyclic groups < a > and < b > have orders m and
two respectively. By (2.1), a−1 = am−1 and b = b−1. For example, if m = 3,
then a3 = b2 = 1, so
aaabababbb = (aaa)(bab)(ab)(bb) = (a2)(ab) = a3b = b.
The reader can show that D(3) = {1, a, a2, b, ab, ba}. For example, a2b =
a(ab) = a(ba2) = (ab)a2 = ba4 = ba. Hence, D(3) has order 6. We will give
a more convincing argument in due course.
⊠
Example 2.5. Let us now verify that D(2) is a group. Since the multiplica-
tion of words is associative, it follows from the requirement that a2 = b2 = 1
and ab = ba that every word can be collapsed to one of 1, a, b, ab, ba. But
ab = ba, so D(2) = {1, a, b, ab}. To see that D(2) is closed under multipli-
cation, we observe that a(ab) = a2b = b, b(ab) = (ba)b = ab2 = a, (ab)a =
(ba)a = ba2 = b, and (ab)(ab) = (ba)(ab) = ba2b = b2 = 1. Therefore, D(2) is
closed under multiplication, so it follows from our other remarks that D(2)
a group. Note that the order of D(2) is 4.
⊠
It turns out that D(m) is a ﬁnite group of order 2m for all m > 0. This
will be proved in Example 2.12. But ﬁrst we must deﬁne subgroups and show
that every subgroup H of a group G partitions G into disjoint subsets gH,
called cosets, all of which have the same number of elements when G is ﬁnite.

18
2
Groups and Fields: The Two Fundamental Notions of Algebra
Another computation of the order |D(m)| uses the fact that D(m) is the
symmetry group of a regular m-gon and a principle called O(2, R)-dichotomy.
The details of this are in Section 7.3.7.
2.1.6
Subgroups
We now single out the most important subsets of a group: namely those that
are also groups.
Deﬁnition 2.5. A nonempty subset H of a group G is called a subgroup of
G if whenever x, y ∈H, we have xy−1 ∈H.
In particular, since every subgroup is nonempty, every subgroup H of
G contains the identity of G, hence also the inverses of all of its elements.
Moreover, by deﬁnition, H is closed under the group operation of G. Finally,
associativity of the group operation on H follows from its associativity in G.
Consequently, every subgroup of G is also a group. Thus we have proved the
following result.
Proposition 2.4. A subset H of a group G is a subgroup if and only if H
is a group under the group operations of G. That is, H is closed under the
group operation and contains the identity of G, and the inverse of an element
of H is its inverse in G.
Example 2.6. Suppose G denotes the integers. The even integers make up
a subgroup of G, since the diﬀerence of two even integers is even. On the
other hand, the odd integers do not, since the diﬀerence of two odd integers
is even.
⊠
Example 2.7. Here are some other examples of subgroups.
(i) Let m ∈Z. Then all integral multiples mn of m form the subgroup mZ
of Z.
(ii)
In
every
group,
the
identity
element
1
determines
the
trivial
subgroup {1}.
(iii) If H and K are subgroups of a group G, then H ∩K is also a subgroup
of G.
(iv) If G =< a > is a ﬁnite cyclic group and k ∈Z, then H =< ak > is a
subgroup of G. Note that < ak > need not be a proper subgroup of < a >.
If |G| = n, then H = G if and only if the greatest common divisor of k and
n is 1.
⊠
(v) The dihedral group D(m) generated by a and b deﬁned in Example 2.4
has a cyclic subgroup of order m, namely < a >. The subgroup < b > is cyclic
of order two.
⊠

2.1
Groups and homomorphisms
19
Here is a nice criterion for a subgroup.
Proposition 2.5. Let G be a group and suppose H is a nonempty ﬁnite sub-
set of G such that for every a, b ∈H, we have ab ∈H. Then H is a subgroup
of G.
Proof. Consider the mapping La : G →G deﬁned by left multiplication by
a. That is, La(g) = ag. This mapping is injective; for if ag = ag′, then left
multiplication by a−1 gives g = g′. By assumption, if a ∈H, then La(h) ∈H
for all h ∈H. Hence, since H is ﬁnite, the pigeonhole principle implies that
La is a bijection of H onto H. Now H contains an element a, so there exists
an h ∈H such that La(h) = ah = a. But since G is a group, it follows that
h = 1. Thus, H contains the identity of G. It follows that a has a right
inverse, since La(h) = 1 for some h ∈H. Hence every element a ∈H has an
inverse in H, so H satisﬁes the property that for every a, b ∈H, ab−1 ∈H.
Consequently, H is a subgroup, by deﬁnition.
□
Remark. Note that for a group G and a ∈G, the left-multiplication map-
ping La : G →G is a bijection. For La is injective by the proof of the above
proposition. It is also surjective, since if g ∈G, the equation La(x) = ax = g
has a solution, namely x = a−1g. The same remark holds for right multipli-
cation Ra : G →G, which is deﬁned by Ra(g) = ga.
2.1.7
Homomorphisms and Cayley’s Theorem
One often needs to compare or relate groups. The basic tool for this is given
by the notion of a homomorphism.
Deﬁnition 2.6. If G and H are groups, then a mapping ϕ : G →H is said
to be a homomorphism if and only if ϕ(gg′) = ϕ(g)ϕ(g′) for all g, g′ ∈G.
A bijective homomorphism is called an isomorphism. If there exists an iso-
morphism ϕ : G →H, we will say that G and H are isomorphic and write
G ∼= H. The kernel of a homomorphism ϕ : G →H is deﬁned to be
ker(ϕ) = {g ∈G | ϕ(g) = 1 ∈H}.
Here is an interesting example that we will generalize several times.
Example 2.8. Let R∗denote the nonzero real numbers. Multiplication by
a ∈R∗deﬁnes a bijection μa : R →R given by μa(r) = ar. The distributive
law for R says that
μa(r + s) = a(r + s) = ar + as = μa(r) + μa(s)
for all r, s ∈R. Thus μa : R →R is a homomorphism for the additive group
structure on R. Since a ̸= 0, μa is in fact an isomorphism. Furthermore, the

20
2
Groups and Fields: The Two Fundamental Notions of Algebra
associative and commutative laws for R imply that a(rs) = (ar)s = (ra)s =
r(as). Hence,
μa(rs) = rμa(s).
Combining these two identities says that μa is a linear mapping of R. (Linear
mappings will be studied in great detail later.) The linear mappings μa form
an important group, denoted by GL(1, R) called the general linear group
of R. The corresponding general linear group GL(n, R) of linear bijections
of Rn will be introduced in Section 4.2. I claim that R∗and GL(1, R) are
isomorphic via the homomorphism μ : R∗→GL(1, R) deﬁned by μ(a) = μa.
We leave this claim as an exercise. The cyclic subgroup < −1 >= {−1, 1} of
R∗is the one-dimensional case of an important subgroup O(n) of GL(n, R)
called the orthogonal group. We may thus denote < −1 > by O(1).
⊠
Example 2.9. If g ∈G, the mapping σg : G →G deﬁned by putting σg(h) =
ghg−1 is an isomorphism. The mapping σg is called conjugation by g. An
isomorphism of the form σg is called an inner automorphism of G. An iso-
morphism σ : G →G that is not of the form σg for some g ∈G is said to be
an outer automorphism of G.
⊠
Notice that if G is abelian, then its only inner automorphism is the identity
map IG(g) = g for all g ∈G.
Proposition 2.6. The image of a homomorphism ϕ : G →G′ is a subgroup
of G′, and its kernel is a subgroup of G.
We leave this as an exercise. An example of an outer automorphism ϕ :
G →G is given by letting G be any abelian group and putting ϕ(g) = g−1.
For example, if G = Z, then ϕ(m) = −m.
The next result, known as Cayley’s theorem, reveals a nontrivial funda-
mental property of the symmetric group.
Theorem 2.7. Every ﬁnite group G is isomorphic to a subgroup of Sym(G).
Hence if |G| = n, then G is isomorphic to a subgroup of S(n).
Proof. As already noted in the remark following Proposition 2.5, the mapping
La : G →G deﬁned by left multiplication by a ∈G is a bijection of G. Thus,
by deﬁnition, La ∈Sym(G). Now let ϕ : G →Sym(G) be deﬁned by ϕ(a) =
La. I claim that ϕ is a homomorphism. That is, ϕ(ab) = ϕ(a)ϕ(b). For if
a, b, c ∈G, then
ϕ(ab)(c) = Lab(c) = (ab)c = a(bc) = La(Lb(c))
by associativity. But
ϕ(a)ϕ(b)(c) = ϕ(a)(Lb(c)) = La(Lb(c)),

2.1
Groups and homomorphisms
21
so indeed ϕ is a homomorphism. Hence H = ϕ(G) is a subgroup of Sym(G).
To show that ϕ : G →H is an isomorphism, it suﬃces to show that ϕ is
injective. But if ϕ(a) = ϕ(b), then ag = bg for all g ∈G. This implies a = b,
so ϕ is indeed injective.
□
Exercises
Exercise 2.1.1. Let S be a ﬁnite set. Show that Sym(S) is a group.
Exercise 2.1.2. Show that S(n) is nonabelian for all n > 2.
Exercise 2.1.3. Let G = S(3) and let σ1, σ2 ∈G be given by
σ1 =
1
2
3
2
1
3

and σ2 =
1
2
3
1
3
2

.
(i) Compute σ1σ2 and σ2σ1 as arrays.
(ii) Likewise, compute σ1σ2σ−1
1 .
Exercise 2.1.4. The purpose of this exercise is to enumerate the elements of
S(3). Let σ1, σ2 ∈G be deﬁned as in Exercise 2.1.3. Show that every element
of S(3) can be expressed as a product of σ1 and σ2.
Exercise 2.1.5. In part (iii) of Example 2.7, we asserted that if G =< a >
is a ﬁnite cyclic group of order n and if m ∈Z, then H =< am > is a subgroup
of G, and H = G if and only if the greatest common divisor of m and n is
1. Prove this. Note: the greatest common divisor (gcd) of m and n is the
largest integer dividing both m and n. The key property of the gcd is that
the gcd of m and n is d if and only if there exist integers a and b such that
am + bn = d.
Exercise 2.1.6. For the notation in this exercise, see Example 2.8. Prove
that R∗and GL(1, R) are isomorphic via the mapping μ : R∗→GL(1, R)
deﬁned by μ(a) = μa.
Exercise 2.1.7. Suppose Alice, Bob, Carol, and Ted are seated in the same
row at a movie theater in the order ABCT. Suppose Alice and Ted switch
places, then Ted and Carol switch, and ﬁnally Bob and Ted switch. Putting
A = 1, B = 2, C = 3 and T = 4, Do the same if Ted and Carol switch ﬁrst,
then Bob and Ted switch, and Alice and Ted switch last. Compare the results.
Exercise 2.1.8. Suppose only Alice, Bob, and Carol are seated in a row.
Find the new seating arrangement if Alice and Bob switch, then Bob and
Carol switch, and ﬁnally Alice and Bob switch again. Now suppose Bob and
Carol switch ﬁrst, then Alice and Bob switch, and ﬁnally Bob and Carol
switch again.

22
2
Groups and Fields: The Two Fundamental Notions of Algebra
(i) Without computing the result, how do you think the seating arrangements
diﬀer?
(ii) Now compute the new arrangements and comment on the result.
Exercise 2.1.9. Prove Proposition 2.6. That is, show that if G and G′ are
groups and ϕ : G →G′ is a homomorphism, then the image of ϕ is a subgroup
of G′, and its kernel is a subgroup of G.
Exercise 2.1.10. Let G be a group and g ∈G. Prove that the inner auto-
morphism σg : G →G deﬁned by σg(h) = ghg−1 is an isomorphism.
Exercise 2.1.11. Let Aut(G) denote the set of all automorphisms of a
group G.
(i) Prove that Aut(G) is a group;
(ii) Prove that the inner automorphism σg : G →G deﬁned by σg(h) = ghg−1
is an element of Aut(G);
(iii) Prove that the mapping Φ : G →Aut(G) deﬁned by Φ(g) = σg is a homo-
morphism; and
(iv) Describe the kernel of Φ. The kernel is called the center of G.
(v) Show that if G is ﬁnite, then Aut(G) is also ﬁnite.

2.2
The Cosets of a Subgroup and Lagrange’s Theorem
23
2.2
The Cosets of a Subgroup and Lagrange’s
Theorem
Suppose G is a group and H is a subgroup of G. We are now going to use H
to partition G into mutually disjoint subsets called cosets. In general, cosets
are not subgroups; they are translates of H by elements of G. Hence there is
a bijection of H onto each of its cosets. Actually, cosets come in two varieties:
left cosets and right cosets. When G is a ﬁnite group, every pair of cosets
of both types of a subgroup H have the same number of elements. This is
the key fact in the proof of Lagrange’s Theorem, which was one of the ﬁrst
theorems in group theory and is still an extremely useful result.
2.2.1
The deﬁnition of a coset
Suppose G is a group and H is a subgroup of G.
Deﬁnition 2.7. For every x ∈G, the subset
xH = {g ∈G | g = xh ∃h ∈H}
is called the left coset of H containing x. Similarly, the subset
Hx = {g ∈G | g = hx ∃h ∈H}
of G is called the right coset of H containing x. Note, xH and Hx both
contain x, since 1 ∈H. The set of left cosets of H is denoted by G/H, while
the set of right cosets of H is denoted by H\G. We will call x a representative
of either coset xH or Hx.
Let us make some basic observations. Since x ∈xH, it follows that G is
the union of all the left cosets of H:
G =

x∈G
xH.
Of course, a similar assertion holds for the right cosets. Since two cosets xH
and yH may intersect or even coincide, the above expression for G needs to
be made more precise. To do so, we will consider how two cosets xH and yH
intersect. The answer might be a little surprising.
Proposition 2.8. Two left cosets xH and yH of the subgroup H of G are
either equal or disjoint. That is, either xH = yH or xH ∩yH is empty. Fur-
thermore, xH = yH if and only if x−1y ∈H. Consequently, G is the disjoint
union of all the left cosets of H.

24
2
Groups and Fields: The Two Fundamental Notions of Algebra
Proof. We will show that if xH and yH have at least one element in com-
mon, then they coincide. Suppose z ∈xH ∩yH. Then z = xh = yk. Now
we observe that xH ⊂yH. For if u ∈xH, then u = xj for some j ∈H.
Since x = ykh−1, u = ykh−1j. But kh−1j ∈H, since H is a subgroup. Thus
u ∈yH. Similarly, yH ⊂xH, so indeed, xH = yH. Thus, two left cosets
that have a nonempty intersection coincide. Consequently, two cosets are
either equal or disjoint. To prove the second statement, assume xH = yH.
Then xh = yk for some h, k ∈H, so x−1y = hk−1 ∈H. On the other hand,
if x−1y ∈H, then y = xh for some h ∈H. Thus xH and yH have y as a
common element. Therefore, xH = yH by the ﬁrst statement of the proposi-
tion. The ﬁnal statement follows from the fact that every element of G is in
a coset.
□
If G is abelian, then xH = Hx for all x ∈G and all subgroups H. In
nonabelian groups, it is often the case that xH ̸= Hx. Left and right cosets
may be diﬀerent. We will see that subgroups H such that xH = Hx for all
x ∈G play a special role in group theory. They are called normal subgroups.
Let us now consider some examples.
Example 2.10. Let us ﬁnd the cosets of the subgroup mZ of Z. By deﬁni-
tion, every coset has the form k + mZ = {k + mn | n ∈Z}. Suppose we begin
with m = 2. In this case, mZ = 2Z is the set of even integers. Then 2Z and
1 + 2Z are cosets. Similarly, 3 + 2Z is also a coset, as is 4 + 2Z. We could
continue, but it is better to check ﬁrst whether all these cosets are distinct.
For example, 4 + 2Z = 0 + 2Z, since 4 −0 is even. Similarly, 3 −1 ∈2Z, so
3 + 2Z = 1 + 2Z. Thus when m = 2, there are only two cosets: the even inte-
gers and the odd integers. Now suppose m is an arbitrary positive integer.
Then by similar reasoning, the cosets are
mZ, 1 + mZ, 2 + mZ, . . . , (m −1) + mZ.
In other words, mZ has m distinct cosets. Note that since mZ is an abelian
group,
the right cosets and the left cosets are the same: k + mZ = mZ + k for all
integers k.
⊠
In number theory, one says that two integers r and s are congruent mod-
ulo m if their diﬀerence is a multiple of m: r −s = tm. When r and s are
congruent modulo m, one usually writes r ≡s mod(m). Interpreted in terms
of congruence, Proposition 2.8 says that r ≡s mod(m) if and only if r and s
are in the same coset of mZ if and only if r + mZ = s + mZ.
Example 2.11 (Cosets in R2). Recall that R2 = R × R denotes the set
of all ordered pairs {(r, s) | r, s ∈R}. By the standard identiﬁcation, R2 is
the Cartesian plane. It is also an abelian group by componentwise addi-
tion: (a, b) + (c, d) = (a + c, b + d). The solution set ℓof a linear equation

2.2
The Cosets of a Subgroup and Lagrange’s Theorem
25
rx + sy = 0 is a line in R2 through the origin (0, 0). Note that (0, 0) is the
identity element of R2. Note also that every line ℓthrough the origin is a sub-
group of R2. By deﬁnition, every coset of ℓhas the form (a, b) + ℓ, and thus
the cosets of ℓare lines in R2 parallel to ℓ. For if (u, v) ∈ℓ, then ru + sv = 0;
hence r(a + u) + s(b + v) = (ra + sb) + (ru + sv) = ra + sb. Thus the coset
(a, b) + ℓis the line rx + sy = d, where d = ra + sb. If d ̸= 0, this is a line in
R2 parallel to ℓ, which veriﬁes our claim. Consequently, the coset decompo-
sition of R2 determined by a line ℓthrough the origin consists of ℓtogether
with the set of all lines in R2 parallel to ℓ. Finally, we remark that every
subgroup of R2 distinct from R2 and {(0, 0)} is a line through the origin, so
we have determined all possible cosets in R2: every coset is either R2, a point,
or a line. In Exercise 2.2.7, the reader is asked to carefully supply the details
of this example.
⊠
The ﬁnal example of this section veriﬁes our claim about the order of the
dihedral group.
Example 2.12 (The order of the dihedral group D(m) is 2m). Let m be
a positive integer. In this example, we will compute a coset decomposition
of the dihedral group D(m) and use it to show that |D(m)| = 2m. Recall
from Example 2.4 that D(m) is generated by two elements a and b satisfying
am = 1, b2 = 1, and ab = bam−1. Let H denote the cyclic subgroup < a >,
and let us compute the cosets of H. One is H, and another is bH, since b /∈H.
Now consider abH. Since ab = ba−1, it follows that abH = bH. Furthermore,
b2H = H. Thus the coset decomposition of D(m) into left cosets of H has to
be
D(m) = H

bH.
Since |H| = m, and |bH| = |H| due to the fact that Lb(h) = bh deﬁnes a
bijection of H to bH, it follows that |D(m)| = m + m = 2m.
⊠
2.2.2
Lagrange’s Theorem
The most important consequence of the fact that the cosets of a subgroup of
a ﬁnite group partition the group into subsets all having the same number
of elements is the famous theorem of Lagrange, which we will now prove. We
ﬁrst note the following result.
Proposition 2.9. If G is a group and H is a ﬁnite subset of G, then for
every a ∈G, |aH| = |Ha| = |H|.
Proof. Recall that the left multiplication mapping La : G →G deﬁned by
La(g) = ag is injective. Since La(H) = aH, we conclude that |aH| = |H|.
Similarly, |Ha| = |H|.
□

26
2
Groups and Fields: The Two Fundamental Notions of Algebra
We now prove Lagrange’s theorem. It is undoubtedly the most frequently
cited elementary result on ﬁnite groups.
Theorem 2.10. Suppose G is a ﬁnite group and H is a subgroup of G. Then
the order of H divides the order of G. In fact,
|G|/|H| = |G/H|.
Proof. Every element of G is in a unique left coset of H, and by the previous
lemma, any two cosets have the same number of elements. Since distinct
cosets are disjoint, we may conclude that |G| = |G/H||H|.
□
Deﬁnition 2.8. If |G| is ﬁnite, the common value of |G/H| and |H\G| is
called the index of H in G.
Since the left and right cosets in an abelian group are the same, it is also
possible to deﬁne the index of a subgroup of an inﬁnite abelian group.
Deﬁnition 2.9. If a subgroup H of an arbitrary abelian group G has only
ﬁnitely many cosets, then the index of H in G is deﬁned to be |G/H|. If G/H
is inﬁnite, we say that H has inﬁnite index in G.
For example, by Example 2.10, the index of H = mZ in G = Z is exactly
m. Let us now consider the order of an element x ̸= 1.
Deﬁnition 2.10. Let G be a group. An element x ̸= 1 ∈G is said to have
ﬁnite order if xm = 1 for some m > 0. The order of an element x of ﬁnite order
is the smallest integer n > 0 such that xn = 1. By assumption, the identity
has order one.
Notice that if x has order n > 0 and xm = 1 for some m > 0, then n
divides m. Reason: By deﬁnition, m ≥n. Dividing m by n, we can write
m = sn + r, where s and r are nonnegative integers and 0 ≤r < n. But then
xm = xsnxr = xr = 1. Since r < n, it follows by the deﬁnition of n that r = 0.
Of course, groups that are not ﬁnite need not have any elements of ﬁnite
order (example?). Another remark is that an element and its inverse have the
same order. Let us now derive the ﬁrst of several applications of Lagrange’s
theorem.
Proposition 2.11. Every element in a ﬁnite group G has ﬁnite order, and
the order of every element divides the order of the group.
Proof. If x ∈G, the powers x, x2, x3, . . . cannot all be distinct. Hence there
are positive integers r < s such that xr = xs. Consequently, xs−r = 1. Hence
every element of G has ﬁnite order. Let x have order n. Then the cyclic group
< x >= {1, x, . . . , xn−2, xn−1} is a subgroup of G of order n, so by Lagrange,
n divides |G|.
□

2.2
The Cosets of a Subgroup and Lagrange’s Theorem
27
Recall that an integer p > 1 is said to be prime if its only positive inte-
ger divisors are 1 and p. The next result is an immediate consequence of
Proposition 2.11.
Corollary 2.12. A group of prime order is cyclic.
Another useful result is the following.
Proposition 2.13. In a ﬁnite group, the number of elements of prime order
p is divisible by p −1.
Proof. By Lagrange, two distinct subgroups of prime order p meet exactly
at 1, for their intersection is a subgroup of both. But in a group of order
p, every element except the identity has order exactly p, since p is prime.
Thus the number of elements of order p is m(p −1) where m is the number
of subgroups of order p.
□
The reader may wonder whether Lagrange’s theorem has a converse: does a
group of order m have a subgroup of order k for every divisor k of m? It turns
out that the answer is no. For example, a group of order 12 need not have a
subgroup of order 4. However, there is a famous theorem of Cauchy that says
that if a prime p divides |G|, then G contains an element of order p. What
one can say in general about the converse of Lagrange’s theorem is partially
answered by the Sylow theorems, which describe the Sylow subgroups of G,
namely those subgroups whose order is the highest power pm of a prime p
dividing |G|. These results are all proved in Chap. 11.
Exercises
Exercise 2.2.1. Suppose G is a group and H is a subgroup of G. Show that
the relation on G given by x ≡y if and only if x−1y ∈H is an equivalence
relation whose equivalence classes are exactly the left cosets of H. Conclude
that G is the disjoint union of its left cosets. This gives an alternative proof
of Proposition 2.8.
Exercise 2.2.2. Let G have order 15 and let H have order 8. Does there
exist a surjective homomorphism ϕ : G →H?
Exercise 2.2.3. Let G have order 9 and let G′ have order 20. Describe the
set of all homomorphisms ϕ : G →G′.
Exercise 2.2.4. Suppose G is a ﬁnite cyclic group.
(i) Show that every subgroup of G is also cyclic.
(ii) Show that for every divisor k of m, there exists a cyclic subgroup of G
having order k.

28
2
Groups and Fields: The Two Fundamental Notions of Algebra
Exercise 2.2.5. Show that if ϕ is a homomorphism on a group G and if H
is the kernel of ϕ, then ϕ is constant on each coset of H, and ϕ takes diﬀerent
values on diﬀerent cosets.
Exercise 2.2.6. Let G be an abelian group, and suppose x and y are ele-
ments of G of ﬁnite orders m and n respectively.
(i) Show that the order of xy is the least common multiple of m and n.
(ii) Give an explicit example of a ﬁnite nonabelian group G containing non-
commuting elements x and y such that the order of xy is not the least common
multiple of m and n. (Try S(3).)
Exercise 2.2.7. Consider the plane R2 = {(x, y) | x, y ∈R}, and deﬁne addi-
tion on R2 by (x, y) + (u, v) = (x + u, y + v).
(i) Show that R2 is an abelian group under addition.
(ii) Let a, b ∈R, and put ℓ= {(x, y) | ax + by = 0}. Thus ℓis a line in R2
through the origin (0, 0). Show that ℓis a subgroup of R2, and conclude that
lines through the origin in R2 are subgroups.
(iii) Show that if (r, s) ∈R2, then the coset (r, s) + ℓof ℓis a line in R2. In
fact, show that (r, s) + ℓis the line ax + by = ar + bs. Conclude from this
that any two cosets of ℓcoincide or are disjoint.
Exercise 2.2.8. Consider the group Z24 as deﬁned in the 24-hour clock
example. Find the number of elements of order n for each divisor of 24.

2.3
Normal Subgroups and Quotient Groups
29
2.3
Normal Subgroups and Quotient Groups
Suppose G is a group and H is a subgroup of G. Recall that G/H is the set
of all left cosets of H in G. The plan in this section is to deﬁne what is called
a normal subgroup of G and to show that when H is a normal subgroup
of G, then G/H is also a group. The group G/H is called the quotient of
G by H.
2.3.1
Normal subgroups
Suppose H is a subgroup of an arbitrary group G. Let us consider what it
means to impose the condition that a left coset xH is also a right coset Hy.
Proposition 2.14. Suppose the left coset xH coincides with the right coset
Hy for a pair x, y ∈G. Then x = hy for some h ∈H, so Hy = Hx. Thus
xH = Hx, and consequently, xHx−1 = H.
Proof. Left to the reader.
It follows that the subgroups H of G such that every left coset of H is a
right coset are characterized by the property that xHx−1 = H for all x ∈G.
From now on, such subgroups will be called normal subgroups.
Deﬁnition 2.11. Let H be a subgroup of G. The normalizer of H in G is
deﬁned to be NG(H) = {g ∈G | gHg−1 = H}.
We leave it as an exercise to show that NG(H) is a subgroup of G and
H is a normal subgroup of NG(H). For example, if G is abelian, then every
subgroup is normal. By the above proposition, NG(H) consists of those x ∈G
such that the coset xH is also a right coset.
Example 2.13. Let m be a positive integer, and let G denote the dihedral
group D(m) with generators a and b satisfying am = b2 = 1 and ab = bam−1.
Let us show that the cyclic subgroup H =< a > of order m is normal. To
see this, note that since b = b−1, we know that bab−1 = am−1. It follows that
b ∈NG(H). Moreover, a ∈NG(H), so G = NG(H), since NG(H) contains
the generators a, b of G. Therefore, < a > is normal in D(m). Now consider
the subgroup < b >. Since aba−1 = bam−1a−1 = bam−2, aba−1 /∈< b > unless
m = 2. But in this case, as we have already seen, G is abelian. Therefore, for
m > 2, < b > is not normal in D(m).
⊠
The following proposition gives a well-known condition for a subgroup to
be normal.

30
2
Groups and Fields: The Two Fundamental Notions of Algebra
Proposition 2.15. Let G be a ﬁnite group and suppose H is a subgroup of
index two. Then H is normal in G.
Proof. Recall that G is the disjoint union of both its left cosets and its right
cosets. Hence if x /∈H, then
G = H ∪xH = H ∪Hx.
This implies that xH = Hx for every x ∈G. Therefore, H is normal.
□
Since |D(m)| = 2m (by Example 2.12) and the cyclic subgroup < a > has
order m, it follows that < a > has index two. This gives another proof that
< a > is normal.
The next proposition gives a method for producing normal subgroups.
Proposition 2.16. Let G and G′ be groups, and let ϕ : G →G′ be a homo-
morphism with kernel H. Then H is normal in G.
Proof. Let h ∈H and take any g ∈G. Then
ϕ(ghg−1) = ϕ(g)ϕ(h)ϕ(g−1) = ϕ(g)1ϕ(g−1) = ϕ(gg−1) = 1.
Thus, ghg−1 ∈H for all g ∈G and h ∈H. Therefore, G = NG(H), so H is
normal.
□
2.3.2
Constructing the quotient group G/H
We will now construct the quotient group G/H of G by a normal subgroup
H.
Proposition 2.17. Suppose H is a normal subgroup of G. Then the set
G/H of left cosets of H has a natural group structure under the binary
operation G/H × G/H →G/H deﬁned by (xH, yH) →xyH. Moreover, the
natural mapping π : G →G/H sending g ∈G to its coset gH is a homomor-
phism. Similarly, the set of right cosets of H admits a group structure deﬁned
analogously.
Proof. The ﬁrst step is to show that the rule (xH, yH) →xyH doesn’t
depend on the way the cosets are represented. Thus, suppose xH = uH
and yH = vH. I claim that xyH = uvH. To see this, we must show that
(xy)−1(uv) ∈H. Since xH = uH and vH = yH, x−1u = h1 ∈H and y−1v =
h2 ∈H, so
(xy)−1(uv) = y−1x−1uv = y−1x−1uyy−1v = y−1h1yh2 ∈H,

2.3
Normal Subgroups and Quotient Groups
31
since y−1h1y ∈H, due to the fact that H is a normal subgroup. There-
fore, xyH = uvH, as claimed. We leave it as an exercise to verify that
the group operation is associative. The other group properties are routine:
1 ∈G/H is the identity coset H and the inverse (xH)−1 is equal to x−1H.
Thus G/H satisﬁes the group axioms. Finally, it is evident that π is a
homomorphism.
□
The map π : G →G/H is called the quotient map. The quotient group
G/H is often referred to as G modulo H, or simply G mod H. The terminol-
ogy for the group of right cosets is the same.
Remark. It is worth noting that when H is normal in G, the product of
the two cosets xH and yH is the coset xyH, where by the product of xH
and yH, we mean the set (xH)(yH) ⊂G consisting of all elements of the
form xhyk, where h, k ∈H. We will leave the veriﬁcation of this claim as an
exercise.
Example 2.14 (Example 2.8 continued). Since R∗is abelian, O(1) = {±1}
is a normal subgroup. Each element of the quotient group R∗/O(1) is a coset
that can be uniquely written rO(1), where r is a positive real number. Note
that the set R>0 of all positive reals is also a subgroup of R∗. In fact, the
mapping ϕ : R>0 →R∗/O(1) given by ϕ(r) = rO(1) is an isomorphism, so
R>0 ∼= R∗/O(1).
⊠
Example 2.15. One of the most important examples of a quotient group is
the group Zm = Z/mZ of integers modulo m, where m is a positive integer.
Since Z is an abelian group under addition, the subgroup mZ is normal in Z,
so it follows that Zm is indeed a group. As we noted after Example 2.10, the
index of mZ in Z is m. Thus |Z/mZ| = |Zm| = m.
⊠
Here are two more interesting examples.
Example 2.16. Viewing R as an additive abelian group and Z as a normal
subgroup, the quotient S = R/Z of R modulo Z can be pictured by taking the
unit interval [0, 1] in R and identifying 0 and 1, thus producing a circle that
represents S. The quotient map π : R →R/Z wraps the real line R around
the circle S an inﬁnite number of times so that each interval [n, n + 1] repeats
what happened on [0, 1]. We will see below that the group operation on S can
be explicitly realized using complex numbers and the complex exponential.
In fact, S is then realized concretely as the unit circle in the complex plane.
⊠
Example 2.17. The plane R2, which consists of all pairs (x, y) of real num-
bers, is an abelian group under the addition (u, v) + (x, y) = (u + x, v + y).
Let Z2 denote the subgroup consisting of all pairs (m, n) of integers m and
n. The quotient group T = R2/Z2 is called a torus. One can visualize T by
taking the unit square S in R2 with vertices (0, 0), (1, 0), (0, 1), and (1, 1)

32
2
Groups and Fields: The Two Fundamental Notions of Algebra
and ﬁrst identifying the horizontal edge from (0, 0) to (1, 0) with the hori-
zontal edge from (0, 1) to (1, 1) placing (0, 0) on (0, 1) and (1, 0) on (1, 1).
The result is a horizontal cylinder of length one. Next, identify the left edge
circle with the right circle in the same way, placing the point (0, 0) on (1, 1).
This produces a curved surface that looks like the surface of a doughnut or,
in mathematical terminology, a torus.
⊠
When G is a ﬁnite group and H is normal in G, Lagrange’s theorem gives
us the following.
Proposition 2.18. The quotient group G/H has order |G/H| = |G|
|H|. Hence,
|H| and |G/H| both divide |G|.
By Cauchy’s theorem, which we mentioned above, if a prime p divides
|G|, then G contains an element of order exactly p. This tells us something
interesting about a group G of order 2p: G has an element a of order p and an
element b of order two. Moreover, the cyclic subgroup < a > is normal, since
its index in G is two. Thus, bab−1 = ar. Then G = D(2p) when r = p −1.
But there are other possibilities. For example, if r = 1, then ab = ba, so this
implies that G is abelian. In fact, in the abelian case, the element ab has
order 2p, provided p ̸= 2. Hence G =< ab >.
2.3.3
Euler’s Theorem via quotient groups
There are several beautiful applications of Lagrange’s theorem and quotient
groups to number theory. One of the nicest is the proof of Euler’s theorem that
we now give. First, let us recall the greatest common divisor, or gcd, (m, n) of
two integers m and n, already deﬁned in Exercise 2.1.5. By deﬁnition, (m, n)
is the largest positive integer d dividing both m and n, and (m, n) = d if and
only if there exist integers r and s such that rm + sn = d. When (m, n) = 1,
we say that m and n are relatively prime. The proof of this characterization
of (m, n) can be found in any book on elementary number theory. Euler’s
phi function φ : Z>0 →Z>0 is deﬁned as follows: if m ∈Z>0, then φ(m) is
the number of integers a ∈[1, m] such that (a, m) = 1.
Theorem 2.19 (Euler’s theorem). Let m and a be positive integers such
that (a, m) = 1. Then
aφ(m) ≡1 mod(m).
Euler’s theorem was published in 1736 and hence preceded groups and
Lagrange’s theorem. Thus the proof given here cannot be Euler’s original
proof. Despite its antiquity, the theorem has a modern application: it turns
out to be one of the ideas that RSA encryption is based on. After deﬁning a
certain ﬁnite abelian group Um known as the group of units modulo m, we

2.3
Normal Subgroups and Quotient Groups
33
will apply Lagrange’s theorem to deduce the result. The key turns out to be
the fact that the order of Um is φ(m).
Before deﬁning Um, we need to observe that the additive group Zm of inte-
gers modulo m also has an associative and commutative multiplication such
that 1 + mZ is a multiplicative identity. For those already familiar with the
notion of a ring, these claims follow because Zm is also a ring. The construc-
tion of the multiplication will be repeated (in more detail) in Section 2.6.1,
so we may omit some details. The product on Zm is given by the rule
(a + mZ)(b + mZ) = ab + mZ.
This deﬁnition is independent of the integers a and b representing their
corresponding cosets. Furthermore, this multiplication is associative and
commutative, and 1 + mZ is a multiplicative identity. We will say that
the coset a + mZ in Zm is a unit if there exists a coset b + mZ such
that (a + mZ)(b + mZ) = 1 + mZ. This is saying that there is a solution
to the equation rx = 1 in Zm. Now let Um ⊂Zm denote the set of unit
cosets. Then Um contains a multiplicative identity 1 + mZ, and each ele-
ment a + mZ of Um is invertible. That is, there exists b + mZ such that
(a + mZ)(b + mZ) = 1 + mZ. To ﬁnish showing that Um is a group, in fact
an abelian group, we must prove the following assertion.
Claim 1: The product of two units is a unit.
Proof. Let a + mZ and b + mZ be units with inverses c + mZ and d + mZ
respectively. Then, omitting parentheses, we have
(a + mZ)(b + mZ)(c + mZ)(d + mZ) = (a + mZ)(c + mZ)(b + mZ)(d + mZ)
= (1 + mZ)(1 + mZ) = 1 + mZ.
Consequently, Um is a group.
Claim 2: |Um| = φ(m).
Proof. Every element of Um is a coset a + mZ represented by an integer a
such that 0 < a < m. It will suﬃce to show that a + mZ is a unit if and
only if (a, m) = 1. Suppose ﬁrst that a + mZ is a unit. Then there exists
an integer b such that (a + mZ)(b + mZ) = 1 + mZ. Thus ab −1 = mn for
some n ∈Z, so ab −mn = 1. We conclude that (a, m) = 1. Conversely, if
(a, m) = 1, then there exist integers b, c such that ab + cm = 1, from which it
follows that (a + mZ)(b + mZ) = 1 + mZ. Thus, a + mZ ∈Um, ﬁnishing the
proof of Claim 2.
To prove Euler’s theorem, we must show that for every integer a such that
(a, m) = 1, aφ(m) ≡1 mod(m). Let a denote a + mZ. Then a ∈Um, so by
Lagrange’s theorem, aφ(m) = aφ(m) + mZ = 1 + mZ. This is equivalent to the
conclusion of the theorem, so we are done.
□

34
2
Groups and Fields: The Two Fundamental Notions of Algebra
The groups of units Um are interesting in themselves, because they give
nontrivial examples of ﬁnite abelian groups. Let us consider a couple of exam-
ples.
Example 2.18. Let us calculate Um for m = 8 and 10. As above, let a
denote a + mZ. Then U8 = {1, 3, 5, 7}. A simple check shows that every
element of U8 has order two. For example, 32 = 9 = 1. In particular, U8
cannot be cyclic. On the other hand, U10 = {1, 3, 7, 9} has at least one
element of order 4 (ﬁnd one!). Consequently, U10 is cyclic.
2.3.4
The First Isomorphism Theorem
Finally, let us prove one of the most fundamental results in group theory.
Theorem 2.20. (The ﬁrst isomorphism theorem) Let ϕ : G →G′ be a sur-
jective group homomorphism, and let H = ker(ϕ). Then ϕ induces an iso-
morphism Φ : G/H →G′ for which Φ(gH) = ϕ(g).
Proof. Since H = ker(ϕ), Proposition 2.16 implies that H is a normal
subgroup. To see that Φ is well deﬁned, we must show that its deﬁnition
is independent of the representative g of gH. But all representatives have the
form gh for some h ∈H, while ϕ(gh) = ϕ(g)ϕ(h) = ϕ(g), so Φ is well deﬁned.
Suppose Φ(gH) = Φ(kH). Then ϕ(g) = ϕ(k). Since ϕ(k−1) = ϕ(k)−1 (due to
the fact that ϕ(k)ϕ(k−1) = ϕ(kk−1) = 1), we thus have ϕ(k−1g) = ϕ(k−1)
ϕ(g) = 1. Therefore, k−1g ∈H, and so kH = gH. Therefore, Φ is injective.
It is clearly surjective, so Φ is an isomorphism.
□
There is a second isomorphism theorem, but it is not as useful, and we
will not mention it here.
Exercises
Exercise 2.3.1. Show that if H is a subgroup of G, then NG(H) is a sub-
group of G and H is a normal subgroup of NG(H).
Exercise 2.3.2. Show that S(3) has both normal subgroups and subgroups
that aren’t normal by showing that the subgroup H1 = {e, σ1σ2, σ2σ1} is
normal, while the subgroup H2 = {e, σ1} is not normal.
Exercise 2.3.3. The center of a group G is deﬁned as Z(G) = {g ∈G | gh =
hg ∀h ∈G}. Show that Z(G) is a normal subgroup.
Exercise 2.3.4. Show that the center of S(3) is the trivial subgroup.
Exercise 2.3.5. Show that S(3) contains a proper nontrivial normal sub-
group H.

2.3
Normal Subgroups and Quotient Groups
35
Exercise 2.3.6. Is U12 cyclic? What about U16?
Exercise 2.3.7. A primitive element of a cyclic group G is an element x ∈G
such that G = ⟨x⟩. Let G be a cyclic group of order m > 1. The purpose of
this exercise is to study the number of primitive elements of G. First apply the
fundamental theorem of arithmetic (Theorem 2.27) to factor m = pa1
1 · · · pak
k ,
where p1, . . . , pk are the prime factors of m and ai ≥1 for all i.
(i) Show that the number of primitive elements of G is φ(m), where φ is
Euler’s φ-function.
(ii) Next, prove that for every prime p and integer a > 0,
φ(pa) = pa −pa−1 = pa(1 −1/p).
(Just stare hard at 1, 2, . . . , pa.)
(iii) Now show that if m and n are relatively prime positive integers, then
φ(mn) = φ(m)φ(n). (This is elementary.)
(iv) Finally, conclude that the number of primitive elements of G is
φ(m) = m
k

i=1
(1 −1/pi).
This is a well-known expression for the Euler function.
Exercise 2.3.8. Deﬁne a mapping ϕ : Z4 →Z10 by ϕ(x) = 5x. That is, for
every a ∈Z, ϕ(a + 4Z) = (5a + 10Z).
(i) Show that ϕ is a well-deﬁned homomorphism.
(ii) Find the kernel and image of ϕ.
Exercise 2.3.9. Prove that if G is a ﬁnite group such that G/Z(G) is cyclic,
then G is abelian. (Recall that Z(G) is the center of G.)

36
2
Groups and Fields: The Two Fundamental Notions of Algebra
2.4
Fields
We now come to the second fundamental topic in this introduction to abstract
algebra, the notion of a ﬁeld. Roughly speaking, a ﬁeld is a set F with two
binary operations, addition and multiplication. The ﬁrst requirement is that
F be an abelian group under addition; the second is that if 0 denotes the
additive identity of F, then F∗= F −{0} is an abelian group under multipli-
cation. In addition, addition and multiplication are related by the distribu-
tive laws. The reader is undoubtedly already familiar with examples of ﬁelds.
For example, the rational numbers, Q, consisting of all quotients m/n with
m, n ∈Z and n ̸= 0, form a ﬁeld, as do the real numbers R. A third very
important ﬁeld is the complex numbers C, which allow one to solve an equa-
tion like x2 + 1 = 0 by adjoining the imaginary numbers to R. The complex
numbers are an indispensable tool for physicists, chemists, and engineers as
well as mathematicians. We will also consider another class of ﬁelds called
Galois ﬁelds. Galois ﬁelds are ﬁnite: as we will show, the rings Zp, where p
is a prime, form a class of Galois ﬁelds called the prime ﬁelds. The simplest
prime ﬁeld is Z2, in which 0 represents oﬀ, 1 represents on, and addition by 1
changes oﬀto on and on to oﬀ. Galois ﬁelds are a tool of coding theory and
computer science.
2.4.1
The deﬁnition of a ﬁeld
The deﬁnition of a ﬁeld is long but not diﬃcult, due to the fact that every
condition in the deﬁnition is a familiar arithmetic property of the real num-
bers.
Deﬁnition 2.12. A ﬁeld is a set F that has two binary operations F+(a, b) =
a + b and F·(a, b) = ab. These operations are called addition and multiplica-
tion respectively. They are required to satisfy the following conditions:
(i) F is an abelian group under addition.
(ii) Let 0 ∈F be the additive identity, and put
F∗= {x ∈F | x ̸= 0}.
Then F∗is an abelian group under multiplication. In particular, F∗contains
a multiplicative identity 1, and 0 ̸= 1.
(iii) The distributive law holds: a(b + c) = ab + ac for all a, b, c ∈F.
The associativity, commutativity, and distributivity of addition and mul-
tiplication are called the arithmetic properties of F. One of the consequences
of these properties is that the identity

2.4
Fields
37
a0 = 0
(2.2)
holds for all a ∈F. To see this, use the distributive law to infer a0 =
a(0 + 0) = a0 + a0. Then a0 + (−a0) = (a0 + a0) + (−a0). Now applying asso-
ciativity of addition gives the identity, since
0 = a0 + (a0 + (−a0)) = a0 + 0 = a0.
In the above deﬁnition, we referred to the additive and multiplicative iden-
tities as well as to the additive and multiplicative inverses. The uniqueness
follows from the fact that identities and inverses in groups are unique.
From now on, the additive inverse of a ∈F will be denoted by −a, and its
multiplicative inverse will be denoted by a−1, provided a ̸= 0. We next prove
a fundamental and useful property that all ﬁelds possess.
Proposition 2.21. Let F be a ﬁeld, and suppose a, b ∈F satisfy ab = 0.
Then either a = 0 or b = 0. Put another way, if neither a nor b is zero,
then ab ̸= 0.
Proof. Suppose ab = 0 but a ̸= 0. Then
0 = a−10 = a−1(ab) = (a−1a)b = 1b = b.
Therefore, b = 0.
□
We will consider the rational numbers Q, the real numbers R, and the
complex numbers C in the next section. After that, we will construct the
prime ﬁelds Fp having p elements, where p is a prime. To conclude this section,
let us describe the simplest prime ﬁeld F2 in more concrete terms than in the
introduction.
Example 2.19. Computers use two states O and I that interact. We will
call this setup O|I. We describe it as follows: let O|I = {0, 1}, with 0 denot-
ing the additive identity and 1 the multiplicative identity. To make O|I a
ﬁeld, we have to completely deﬁne the two binary operations. Multiplica-
tion is forced on us by the deﬁnition of a ﬁeld and (2.2). That is, 0 · 0 = 0,
0 · 1 = 0, and 1 · 1 = 1. (Here we are denoting multiplication ab by a · b for
clarity.) Furthermore, addition by 0 is also already determined. Thus it
remains to deﬁne 1 + 1. But if one puts 1 + 1 = 1, then necessarily 0 = 1,
so we are forced to set 1 + 1 = 0. With this stipulation, 1 is its own additive
inverse. Thus addition corresponds to the operation of changing the state
from 0 to 1 and 1 to 0. When p > 2, however, multiplication is nontrivial. We
leave it to the reader to show that in fact, O|I satisﬁes all the ﬁeld axioms
(see Exercise 2.4.2).
⊠
Finally, we make a useful though obvious deﬁnition.

38
2
Groups and Fields: The Two Fundamental Notions of Algebra
Deﬁnition 2.13. Let F be a ﬁeld. A subset F′ of F is said to be a subﬁeld
of F if F′ is a ﬁeld under the addition and multiplication of F.
2.4.2
Arbitrary sums and products
Just as for groups, one frequently has to express the sums and prod-
ucts of more than three elements in a ﬁeld. For a, b, c, d ∈F, we can put
a + b + c = a + (b + c) and then put a + b + c + d = a + (b + c + d). But is it
the case that a + b + c + d = ((a + b) + c) + d? The answer is yes. Moreover,
a + b + c + d = (a + b) + (c + d) too. For, if e = a + b, then (a + b) + (c +
d) = e + (c + d) = (e + c) + d = ((a + b) + c) + d. In fact, no matter how one
associates the terms a, b, c, d, their sum will always have the same value.
More generally, deﬁning the sum of any n elements a1, . . . , an ∈F induc-
tively by
a1 + a2 + · · · + an = (a1 + a2 + · · · + an−1) + an
as in Section 2.1.2 allows us to ignore parentheses. Moreover, if a′
1, . . . , a′
n are
the same elements as a1, . . . , an, but taken in a diﬀerent order, then
n

i=1
ai =
n

i=1
a′
i.
In other words, arbitrary ﬁnite sums of elements in a ﬁeld are well deﬁned
and may be computed by associating them in any manner or rearranging
them in any way. These claims can be proved via mathematical induction,
though their proofs are tedious (that is, no fun). Thus we will not attempt
them. The analogous results for products are also true and proved in exactly
the same way.
Exercises
Exercise 2.4.1. Show that in a ﬁeld, the additive identity cannot have a
multiplicative inverse.
Exercise 2.4.2. Finish Example 2.19 by showing that if O|I denotes the set
{0, 1} with addition 0 + 0 = 0, 0 + 1 = 1 + 0 = 1, 1 + 1 = 0 and multiplica-
tion 0 · 0 = 0, 0 · 1 = 1 · 0 = 0, and 1 · 1 = 1, then O|I is a ﬁeld.
Exercise 2.4.3. Let a1, . . . , an be elements of a ﬁeld F. Let a′
1, . . . , a′
n be
the same elements, but taken in a diﬀerent order. Use induction to show that
n

i=1
ai =
n

i=1
a′
i

2.4
Fields
39
and
n

i=1
ai =
n

i=1
a′
i.
Exercise 2.4.4. Consider the set Q of all pairs (a, b), where a, b ∈Z and
b ̸= 0. Consider two pairs (a, b) and (c, d) to be the same if ad = bc. Now
deﬁne operations of addition and multiplication on Q as follows:
(a, b) + (c, d) = (ad + bc, bd) and (a, b)(c, d) = (ac, bd).
Show that Q is a ﬁeld. Can you identify Q?
Exercise 2.4.5. Let F = {a + b
√
2 | a, b ∈Q}.
(i) Show that F is a subﬁeld of the real numbers R.
(ii) Find (1 −
√
2)−1 and (3 −4
√
2)−1.

40
2
Groups and Fields: The Two Fundamental Notions of Algebra
2.5
The Basic Number Fields Q, R, and C
We now describe the most familiar examples of ﬁelds: the rational numbers
Q, the real numbers R, and the complex numbers C. We will assume that the
real numbers exist and will not attempt to construct them.
2.5.1
The rational numbers Q
The set of rational numbers Q consists of all fractions a/b, where a, b ∈Z
and b ̸= 0. By assumption, b/b = 1, provided b ̸= 0. Addition is deﬁned by
the rule
a/b + c/d = (ad + bc)/bd,
and multiplication is deﬁned by
(a/b)(c/d) = ac/bd.
In particular, if c ̸= 0, then ac/bc = a/b. Moreover, a/b = 0 if and only if
a = 0. One easily checks that −(a/b) = −a/b. The multiplicative inverse of
a/b ̸= 0 is b/a. That is,
(a/b)−1 = b/a,
provided a, b ̸= 0.
That the rationals satisfy all the ﬁeld axioms follows from the arithmetic
properties of the integers. Of course, Z is not a ﬁeld, since the only nonzero
integers with multiplicative inverses are ±1. In fact, one can show that every
ﬁeld containing Z also contains a subﬁeld indistinguishable from Q. In other
words, Q is the smallest ﬁeld containing Z. The integers form a structure
known as a ring. In other words, a ﬁeld is a ring in which every nonzero
element has a multiplicative inverse. We have already encountered examples
of rings: for example, the integers modulo a composite number (see Exam-
ple 2.15). The n × n matrices over a ﬁeld deﬁned in the next chapter will give
other examples of rings.
2.5.2
The real numbers R
The construction of the real numbers involves some technical mathematics
that would require a lengthy digression. Thus we will simply view R as the
set of all decimal expansions

2.5
The Basic Number Fields Q, R, and C
41
±a1a2 · · · ar.b1b2 · · · ,
where all ai and bj are integers between 0 and 9 and a1 ̸= 0 unless r=1. Note
that there can be inﬁnitely many bj to the right of the decimal point but only
ﬁnitely many aj to the left. It is also necessary to identify certain decimal
expansions. For example, 1 = .999999 . . . . In these terms, Q is the set of real
numbers ±a1a2 · · · ar.b1b2 · · · such that the decimal part b1b2 · · · is either
ﬁnite (that is, all bi = 0 for i suﬃciently large) or eventually repeats itself ad
inﬁnitum. Examples are 1 = 1.000 . . . or 1/3 = .333 . . . .
The real numbers have the useful property of having an ordering; every
real number x is either positive, negative, or 0, and the product of two num-
bers with the same sign is positive. This makes it possible to solve linear
inequalities such as a1x1 + a2x2 + · · · + anxn > 0, although we will not need
to treat such ideas in this text. The reals also have the Archimedean prop-
erty: if a, b > 0, then there exists an x > 0 such that ax > b. In other words,
inequalities in R always have solutions.
2.5.3
The complex numbers C
The set of complex numbers C is a ﬁeld containing R and the square roots of
negative real numbers. Put another way, if a is a positive real number, then
the equation x2 + a = 0 has two complex roots. It is therefore possible to
give a meaning to the square root of a negative number such as √−a. These
numbers are said to be imaginary.
The deﬁnition of C starts with R2, namely the set of all ordered pairs (a, b)
of real numbers a and b with the usual componentwise addition:
(a, b) + (c, d) = (a + c, b + d).
The interesting feature is the deﬁnition of multiplication:
(a, b)(c, d) = (ac −bd, ad + bc).
(2.3)
Then we have the following proposition.
Proposition 2.22. The set R2 with addition and multiplication deﬁned as
above is a ﬁeld. The zero element 0 is (0, 0) and the multiplicative identity 1
is (1, 0). The additive inverse of (a, b) is
−(a, b) = (−a, −b),
and the multiplicative inverse of (a, b) ̸= (0, 0) is

42
2
Groups and Fields: The Two Fundamental Notions of Algebra
(a, b)−1 =
1
a2 + b2 (a, −b).
The proof is a straightforward calculation and will be omitted. We will
now simplify the notation by identifying the pair (a, 0) with the real number
a. (Note: in fact, the mapping R →R2 sending a →(a, 0) is an injective ﬁeld
homomorphism.) Since (a, 0)(r, s) = (ar, as), we obtain after this identiﬁca-
tion that
a(r, s) = (ar, as).
This operation is the usual scalar multiplication on R2. Later will we will say
that C is a vector space over R.
Next, denote (0, 1) by i. Then (a, b) may be written
(a, b) = a(1, 0) + b(0, 1) = a + bi.
Addition and multiplication are then given by the rules
(a + bi) + (c + di) = (a + c) + (b + d)i and (a + bi)(c + di) = (ac −bd) + (ad + bc)i.
In particular, i2 = −1, and if a + bi ̸= 0, then
(a + bi)−1 = a −bi
a2 + b2 .
We now make the following deﬁnition.
Deﬁnition 2.14. We will call a + bi as deﬁned above a complex number and
let C denote the set of all complex numbers a + bi with a and b arbitrary real
numbers.
Summarizing the above discussion, we have the following assertion.
Proposition 2.23. The set of complex numbers C = {a + bi | a, b ∈R} with
the deﬁnitions of addition and multiplication given above is a ﬁeld containing
R as a subﬁeld in which every element a ∈R diﬀerent from zero has two
distinct square roots ±√a.
The ﬁrst component a of a + bi is called its real part, and the second
component b is called its imaginary part. The points of C with imaginary
part zero are called real points, and the points of the form bi with real part
zero are called imaginary points.

2.5
The Basic Number Fields Q, R, and C
43
2.5.4
The geometry of C
We now make some deﬁnitions for complex numbers that lead to some beau-
tiful connections with the geometry of R2. First of all, the complex conjugate
ζ of ζ = a + ib is deﬁned by ζ = a −ib. It is straightforward to check the
following identities:
ω + ζ = ω + ζ
(2.4)
and
ωζ = ω ζ.
(2.5)
The real numbers are obviously the numbers ζ ∈C for which ζ = ζ. Geomet-
rically speaking, complex conjugation is a mapping with domain and target
R2 sending a point to its reﬂection through the real axis.
The length of the complex number ζ = a + ib is deﬁned as the length of
the point (a, b) ∈R2. That is, |ζ| = (a2 + b2)1/2. One calls |ζ| the modulus of
ζ. Since ζζ = (a + bi)(a −bi) = a2 + b2,
|ζ| = (ζζ)1/2.
Applying this to the formula for an inverse, we get the lovely formula
ζ−1 =
ζ
|ζ|2 ,
if ζ ̸= 0. This gives a nice geometric fact about inversion: if |ζ| = 1, then
ζ−1 = ζ. In other words, the inverse of a complex number of modulus one is
its reﬂection through the real axis.
The complex numbers ζ = x + yi of unit length are the points of R2 on
the unit circle
S1 = {(x, y) | x2 + y2 = 1}.
Since every point of S1 can be expressed in the form (cos θ, sin θ) for a unique
angle θ such that 0 ≤θ < 2π, we can parameterize the unit circle by intro-
ducing the complex exponential function
eiθ := cos θ + i sin θ.
(2.6)
The following proposition uses this observation.

44
2
Groups and Fields: The Two Fundamental Notions of Algebra
Proposition 2.24. Every ζ ∈C can be represented as ζ = |ζ|eiθ for some
θ ∈R. Two values of θ parameterize the same point of C if and only if their
diﬀerence is a multiple of 2π.
The unique value of θ in [0, 2π) such that ζ = |ζ|eiθ is called the argument
of ζ. A key property of the complex exponential is the identity
ei(θ+μ) = eiθeiμ,
(2.7)
which follows from the trigonometric formulas for the sine and cosine of the
sum of two angles. (We will give a simple geometric proof of this identity
using rotations in the plane.) The identity (2.7) can be interpreted group-
theoretically too.
Proposition 2.25. The complex exponential deﬁnes a homomorphism π
from the additive group R to the multiplicative group C∗of nonzero com-
plex numbers. The image π(R) is the subgroup S1 of C∗consisting of the
complex numbers of length one.
The identity (2.7) implies De Moivre’s identity: for every integer n > 0,
(cos θ + i sin θ)n = cos nθ + i sin nθ.
It also gives a geometric interpretation of complex multiplication: if ζ = |ζ|eiθ
and ω = |ω|eiμ, then
ωζ = (|ω|eiμ)(|ζ|eiθ) = (|ω||ζ|)ei(μ+θ).
(2.8)
In other words, the product ωζ is obtained by multiplying the lengths of ω
and ζ and adding their arguments (modulo 2π).
Example 2.20 (The mth roots of unity). Suppose m is a positive integer
and put θ = 2π/m. Let
Cm = {1, eiθ, . . . , ei(m−1)θ}.
The elements of Cm are distinct solutions of the polynomial equation zm −
1 = 0. We call Cm the set of mth roots of unity. We leave it as an exercise to
show that Cm is a cyclic subgroup of the multiplicative subgroup C∗of C of
order m. Since zm −1 = 0 has at most m solutions in C (see the discussion
in Section 2.5.5), Cm gives all mth roots of unity. The points of Cm are the
m equally spaced points on the unit circle S1 including 1.
⊠
Here is a surprisingly nice consequence of Proposition 2.9 (cf. Lagrange’s
theorem).
Proposition 2.26. Suppose G is a ﬁnite subgroup of C∗of order m. Then
G = Cm. Therefore, the only ﬁnite subgroups of C∗are the cyclic groups Cm
of mth roots of unity.

2.5
The Basic Number Fields Q, R, and C
45
Proof. Since G is ﬁnite of order m, it follows that for all z ∈G, zm = 1, by
Lagrange. In particular, G ⊂Cm. It follows immediately that G = Cm.
□
2.5.5
The Fundamental Theorem of Algebra
Suppose z denotes an arbitrary element of C and let n be a positive integer.
A function f : C →C of the form f(z) = α0zn + α1zn−1 + · · · + αn−1z + αn,
where the coeﬃcients α0, α1, . . . , αn ∈C, is called a complex polynomial. If
α0 ̸= 0, we say that f has degree n. The roots of f are the elements ζ ∈C such
that f(ζ) = 0. As noted earlier, the polynomial z2 + 1 has real coeﬃcients
but no real roots, However, ζ = ±i are a pair of roots in C. Consequently,
z2 + 1 = (z + i)(z −i). The fundamental theorem of algebra is the following
remarkable generalization of this example.
Theorem 2.27. Every complex polynomial
f(z) = zn + α1zn−1 + · · · + αn−1z + αn
with n > 0 has a complex root. That is, there exists ζ ∈C such that f(ζ) = 0.
The fundamental theorem of algebra poses a conundrum to algebraists: the
only known algebraic proof is long and complicated. The best proofs, in the
sense of explaining why in particular the theorem is true, come from complex
analysis and topology. The proof using complex analysis is extremely elemen-
tary and elegant, so it is invariably included in complex analysis courses.
It follows from division with remainders (see Proposition 2.36) that if f(z)
is a complex polynomial such that f(ζ) = 0, then there exists a complex
polynomial g(z) such that
f(z) = (z −ζ)g(z).
Applying the fundamental theorem of algebra to g(z) and so forth, it follows
that there are not necessarily distinct ζ1, . . . , ζn ∈C such that
f(z) = (z −ζ1)(z −ζ2) · · · (z −ζn).
Thus every complex polynomial f(z) can be expressed as a product of linear
polynomials. A ﬁeld F that has the property that every polynomial with
coeﬃcients in F of positive degree has a root in F is said to be algebraically
closed. For example, C is algebraically closed, but R isn’t. A fundamental
result in algebra, which is well beyond the scope of this discussion, says that
every ﬁeld is a subﬁeld of an algebraically closed ﬁeld.

46
2
Groups and Fields: The Two Fundamental Notions of Algebra
Exercises
Exercise 2.5.1. Express the following complex numbers in the form a + ib:
(i) 2 −3i
1 + 2i, and (ii) (2 + i)(3 −2i)
4 −2i
.
Exercise 2.5.2. Find the inverse of
(2 + i)(3 −2i)
4 −2i
without explicitly computing the fraction.
Exercise 2.5.3. Express all solutions of the equation x3 + 8 = 0 in the form
reiθ and graph them as elements of C = R2.
Exercise 2.5.4. Find α1, . . . , α4 such that
x4 −1 = (x −α1)(x −α2)(x −α3)(x −α4).
Exercise 2.5.5. Find all (x1, x2, x3) ∈C3 satisfying the equations
ix1 + 2x2 + (1 −i)x3 = 0,
−x1 + ix2 −(2 + i)x3 = 0.
Exercise 2.5.6. If necessary, look up formulas for cos(θ + μ) and sin(θ + μ),
and use them to prove formula De Moivre’s identity. That is, show that
ei(θ+μ) = eiθeiμ.
Exercise 2.5.7. Suppose p(x) is a polynomial with real coeﬃcients. Show
that all the roots of p(x) = 0 occur in conjugate pairs λ, λ ∈C. Conclude
that a real polynomial of odd degree has a real root.
Exercise 2.5.8. An nth root of unity α is called primitive if for every nth
root of unity β, there is an integer r ≥0 such that β = αr. Prove that for
every n > 0, there exists a primitive nth root of unity.
Exercise 2.5.9.
Let a, b, c, d be arbitrary integers. Show that there exist
integers m and n such that (a2 + b2)(c2 + d2) = m2 + n2.

2.6
Galois ﬁelds
47
2.6
Galois ﬁelds
A ﬁeld F which is ﬁnite is called a Galois ﬁeld. We have already encountered
an example of a Galois ﬁeld in Example 2.19, namely the two-element ﬁeld
O|I, or F2 as it is usually called. In this section, we will construct a Galois
ﬁeld with a prime number of elements for every prime.
2.6.1
The prime ﬁelds Fp
Let p denote an arbitrary prime. Our goal is to construct a ﬁeld Fp, called
a prime ﬁeld, having exactly p elements. We have already deﬁned a ﬁeld
with two elements, and since every ﬁeld has to contain at least two elements,
(namely the additive and multiplicative identities), F2 is the smallest ﬁeld.
Let us now deﬁne Fp. Putting Fp = Zp = Z/pZ gives us an additive abelian
group with p elements. It remains to deﬁne a multiplication on Fp such that
Fp −{0} is an abelian group. Given two cosets a = a + pZ and b = b + pZ,
deﬁne
ab = (a + pZ)(b + pZ) = ab + pZ.
Since
(a + mp)(b + np) = ab + p(an + bm + mnp),
this coset multiplication is well deﬁned. It is immediate that the coset 1 + pZ
is a multiplicative identity. Associativity of multiplication and the distributive
law follow easily from the arithmetic of Z.
Now recall the Euler group Up ⊂Z/pZ = Fp of units modulo p. We showed
that Up is an abelian group whose order is φ(p) = p −1. This says that in
fact, Fp −{0} = Up. Therefore, Fp −{0} is an abelian group, and hence we
have proved the following theorem.
Theorem 2.28. If p is a prime, then Fp is a ﬁeld.
There is another proof that doesn’t use Euler groups, which although
longer, is also instructive. First, one notes that a property of ﬁelds we already
proved holds for Fp. Claim. If p be a prime number and ab = 0 in Fp, then
either a = 0 or b = 0 (or both). For since ab = 0 in Fp is the same thing
as saying that p divides the product ab in Z, the claim follows from the fact
that if the prime number p divides ab, then either p divides a or p divides b.
This is immediate from the following theorem.
Theorem 2.29 (Fundamental theorem of arithmetic). Every integer m > 1
can be factored m = p1p2 · · · pk, where p1, p2, . . . , pk are not necessarily dis-
tinct primes. Moreover, the number of times each prime pi occurs in this
factorization is unique.

48
2
Groups and Fields: The Two Fundamental Notions of Algebra
To show that every nonzero element in Fp has an inverse, we will show
that multiplication by any a ∈Fp −0 induces an injective map
φa : Fp −0 −→Fp −0
deﬁned by φa(x) = ax. To see that φa is indeed injective, let φa(x) = φa(y),
that is, ax = ay. Then a(x −y) = 0, so x −y = 0, since a ̸= 0 (Proposi-
tion ??). Therefore, φa is indeed injective. Since Fp −0 is ﬁnite, the pigeonhole
principle says that φa is a bijection. In particular, there exists an x ∈Fp −0
such that φa(x) = ax = 1. Hence x is the required inverse of a. This com-
pletes the second proof that Fp is a ﬁeld.
□
Here is an explicit example. As we did earlier, we will use a dot to denote
multiplication.
Example 2.21 Consider F3 = {0, 1, 2}. Addition is given by 1 + 1 = 2,
1 + 2 = 0, and 2 + 2 = 1, the latter sum because 2 + 2 = 4 in Z, and 4 is
in the coset 1 + 3Z. Finding products is similar. For example, 2 · 2 = 4 = 1.
Thus 2−1 = 2 in F3. A good way to picture addition and multiplication is to
construct tables. The addition table for F3 is
+
0
1
2
0
0
0
0
1
1
2
0
2
2
0
1
.
Similarly, the multiplication table is
·
0
1
2
0
0
0
0
1
0
1
2
2
0
2
1
.
⊠
2.6.2
A four-element ﬁeld
We will now give an example of a Galois ﬁeld that is not an Fp by constructing
a ﬁeld F with four elements. We will deﬁne addition and multiplication by
tables and leave it to the reader to verify that F is a ﬁeld. Let 0, 1, α, and β
denote the elements of F, with 0 and 1 the usual identities. Ignoring addition
by 0, the addition table is deﬁned as follows:

2.6
Galois ﬁelds
49
+
1
α
β
1
0
β
α
α
β
0
1
β
α
1
0
.
The multiplication table (omitting the obvious cases 0 and 1) is
·
α
β
α
β
1
β
1
α
.
Proposition 2.30. The set F = {0, 1, α, β} having 0 and 1 as identities and
addition and multiplication deﬁned as above is a ﬁeld. Moreover, F2 is a
subﬁeld of F.
Notice that since α2 = β and β2 = α, it follows that α3 = β3 = 1. This
isn’t surprising, since F∗has order three and hence must be cyclic. Since
α4 = α and β4 = β, all elements of F satisfy the equation x4 −x = 0, since 0
and 1 trivially do. Using the deﬁnition of a polynomial over an arbitrary ﬁeld
given in Section 2.6.4, we can view x4 −x as a polynomial in the variable x
over the subﬁeld F2 of F, where we have the identity x4 −x = x4 + x, since
1 = −1. Thus,
x4 −x = x(x + 1)(x2 + x + 1).
(Recall that 1 + 1 = 0, so 2x = 2x2 = 0.) It can be veriﬁed directly from the
tables that the elements α and β are the two roots of x2 + x + 1 = 0. Since
x4 −x has distinct roots (by the multiple root test in Section 2.6.4), we have
shown that all the elements of F are roots of a polynomial over a subﬁeld of
F, namely x4 −x.
We will eventually show, using a theorem about ﬁnite-dimensional vector
spaces, that the number of elements in a Galois ﬁeld F is always a power pn of
a prime p. This prime is called the characteristic of the ﬁeld, the topic of the
next section. The integer n turns out to be interpreted as the dimension of
F as a vector space over Fp. It is a fundamental result in the theory of ﬁelds
that for every prime p and integer n > 0, there exists a Galois ﬁeld with pn
elements, and two Galois ﬁelds F and F′ with the same number of elements
are isomorphic.
2.6.3
The characteristic of a ﬁeld
If F is a Galois ﬁeld, then some multiple r1 of the identity 1 ∈F has to be 0.
(Note: by r1, we mean 1 + · · · + 1 with r summands.) The reason for this is

50
2
Groups and Fields: The Two Fundamental Notions of Algebra
that since F is ﬁnite, the multiples r1 of 1 cannot all be distinct. Hence there
have to be two distinct positive integers m and n such that m1 = n1 in F. This
implies m1 −n1 = 0. But by associativity of addition, m1 −n1 = (m −n)1.
Assuming without loss of generality that m > n, it follows that there exists
a positive integer r such that r1 = 0 in F.
I claim that the least positive integer r such that r1 = 0 is a prime. For
if r can be expressed as a product r = ab, where a, b are positive integers,
then r1 = (ab)1 = (a1)(b1) = 0. Thus, by Proposition 2.21, a1 = 0 or b1 = 0.
But by the minimality of r, one of a and b is r, so r is a prime, say r = p.
The prime p is the characteristic of F. In general, one makes the following
deﬁnition:
Deﬁnition 2.15. Let F be an arbitrary ﬁeld. If some nonzero multiple q1
of 1 equals 0, we say that F has positive characteristic. In that case, the
characteristic of F is deﬁned to be the smallest positive integer q such that
q1 = 0. If all multiples q1 are nonzero, we say that F has characteristic 0.
Example 2.22. The characteristic of the ﬁeld F4 deﬁned above is two. The
characteristic of Fp is p.
⊠
Summarizing the above discussion, we state the following proposition.
Proposition 2.31. If a ﬁeld F has positive characteristic, then its charac-
teristic is a prime p, and pa = 0 for all a ∈F.
Proof. We already proved that if the characteristic of F is positive, then it
has to be a prime. If p1 = 0, then by the distributive law,
pa = a + · · · + a = 1a + · · · + 1a = (1 + · · · + 1)a = (p1)a = 0a = 0
for all a ∈F.
□
Proposition 2.32. The characteristics of Q, R, and C are all 0. Moreover,
the characteristic of every subﬁeld of a ﬁeld of characteristic 0 is also 0.
The notion of the characteristic has a nice application.
Proposition 2.33. Suppose F is a ﬁeld of characteristic p > 0. Then for all
a1, . . . , an ∈F,
(a1 + · · · + an)p = ap
1 + · · · + ap
n.
This is an application of the binomial theorem. We will leave the proof as
an exercise. A consequence of the previous proposition is that ap = a for every
a ∈Fp. Thus ap−1 = 1. This also gives a formula for a−1 for every a ∈Fp,
namely, a−1 = ap−2.

2.6
Galois ﬁelds
51
Example 2.23. The formula a−1 = ap−2 for the inverse of a nonzero ele-
ment a of Fp actually isn’t so easy to apply without a computer. For exam-
ple, to compute the inverse of 5 in F23, one needs to ﬁnd 521, which is
476837158203125. After that, one has to reduce 476837158203125 modulo
23. The result is 14. Thus 5−1 = 14 in F23. But this can be seen easily with-
out having to do a long computation, since 5 · 14 = 70 = 69 + 1.
⊠
The result ap−1 = 1 in Fp translates into a well-known result from elemen-
tary number theory known as Fermat’s little theorem.
Proposition 2.34. Let p be prime. Then for every integer a ̸≡0 mod p,
a(p−1) ≡1 mod p.
We leave the proof as an exercise. Notice that Fermat’s little theorem is a
special case of Euler’s theorem, since if p is prime, then φ(p) = p −1. Here
is another interesting property of Fp.
Proposition 2.35. The product of all the nonzero elements of Fp is −1.
Proof. This follows by noting that {2, 3, . . . , p −2} can be partitioned into
pairs {a, b} such that a−1 = b. We will leave the proof of this to the reader.
Therefore, 2 · 3 · · · (p −2) = 1 in Fp. The result follows by multiplying by
−1 = p −1.
□
Proposition 2.35 stated in number-theoretic terms is one of the assertions
of Wilson’s theorem: (q −1)! ≡−1 mod q if and only if q is prime. Wilson’s
theorem gives a test for determining whether a number is prime, but the
problem is that implementing this test requires knowing (q −1)!. It turns out
that Fermat’s little theorem gives a much easier test, known as the Fermat
primality test. In this test, one checks whether aq−1 ≡1 mod q for some
“random” values of a. If the congruence fails for any value of a, then q isn’t
prime, while if it holds for several values, then the probability that q is a
prime is very high. Knowing large primes is useful, for example, in employing
RSA encryption.
2.6.4
Appendix: polynomials over a ﬁeld
The purpose of this appendix is to deﬁne the polynomials over a ﬁeld F.
For each integer i > 0, let xi denote a symbol, and suppose that every pair
of these symbols xi and xj can be multiplied with the result xixj = xi+j
for all i, j > 0. We will denote x1 simply by x. We will put x0 = 1 ∈F. The
symbol x is sometimes called an indeterminate. Note that xi = x · · · x (with
i factors). We assume that each symbol xi can be multiplied by an arbi-
trary element of F, so that 0xi = 0, 1xi = xi, and (axi)(bxj) = (ab)xi+j. Let
F[x] denote the set of all expressions f(x) = a0 + a1x + a2x2 + · · · + anxn,

52
2
Groups and Fields: The Two Fundamental Notions of Algebra
a0, a1, . . . , an ∈F and n ≥0. Then f(x) is called a polynomial over F. We
agree that two polynomials f(x) = a0 + a1x + a2x2 + · · · + anxn and g(x) =
b0 + a1x + b2x2 + · · · + bmxm are equal if ai = bi for every value of i. If
an ̸= 0, we say that f has degree n and write deg(f) = n. The value of f(x)
at r ∈F is by deﬁnition f(r) = a0 + a1r + a2r2 + · · · + anrn. Thus, a poly-
nomial f(x) determines a function f : F →F. However, if F is a Galois ﬁeld,
then there exist polynomials f(x) ∈F[x] such that the function f : F →F is
identically zero, so polynomials should not be thought of as the same thing
as functions.
Example 2.24. For example, if F = F2 and f(x) = x2 + x, then f(1) =
f(0) = 0. Hence the function corresponding to f(x) is identically zero on
F2. However, in the ﬁeld F4 containing F2 deﬁned in Section 2.6.2, the poly-
nomial f(x) satisﬁes f(α) = f(β) = 1. (Recall that α and β satisfy x2 + x +
1 = 0.
⊠
Addition and multiplication of polynomials are deﬁned as follows. Addition
amounts to adding together the coeﬃcients of each corresponding power xi
of x. For example,
(3x2 −2x + 1) + (x4 −x3 −3x2 + x) = x4 −x3 −x + 1.
Multiplication of two polynomials uses the above rules and the distributive
law. Thus,
(3x2 −2x + 1)(x4 −x3 −2x2 + x) = 3x6 −5x5 −7x4 + 6x3 −4x2 + x.
When the ﬁeld F has characteristic zero, for example F = Q, R, or C, there
is a more natural formulation of F[x] that avoids the problem encountered
in F2[x], where polynomials can deﬁne the zero function. In that case, let
x : F →F denote the identity function x(r) = r for all r ∈F. If i > 0, then
xi denotes the function xi(r) = ri. Then F[x] may be deﬁned as the set of all
functions f : F →F of the form
f(x) = a0 + a1x + a2x2 + · · · + anxn,
where a0, a1, . . . , an ∈F. Addition and multiplication are deﬁned pointwise
as above.
In the remainder of this section, we are going to consider two properties
of polynomials over an arbitrary ﬁeld. The ﬁrst is that F[x] admits division
with remainder.
Proposition 2.36 (Division with remainder for polynomials).
Suppose
f(x) and g(x) are in F[x]. Then there exists a unique expression
f(x) = q(x)g(x) + r(x),

2.6
Galois ﬁelds
53
where q(x), r(x) ∈F[x] and deg(r) < deg(g). In particular, if a ∈F is a root
of f, i.e., f(a) = 0, then f(x) = (x −a)g(x) for some g(x) ∈F[x].
Proof. This can be proved by induction on the degree of f(x). We will omit
the details.
The second property is a test for when a polynomial has a multiple root.
We say that a is a multiple root of p(x) ∈F[x] if there exists a polynomial
q(x) ∈F[x] such that p(x) = q(x)(x −a)2. To state the second property, we
need to deﬁne the derivative of a polynomial. Suppose n > 0 and p(x) =
anxn + an−1xn−1 + · · · + a1x + a0. Then the derivative of p(x) is deﬁned to
be the polynomial
p′(x) = nanxn−1 + (n −1)an−1xn−2 + · · · + 2a2x + a1.
The derivative of a constant polynomial is deﬁned to be zero. This deﬁnition
agrees with the classical formula for the derivative of a polynomial. The
derivative of a sum is the sum of the derivatives, and Leibniz’s formula for
the derivative of a product holds: (pq)′ = p′q + pq′.
Proposition 2.37. Let p(x) ∈F[x]. Then p(x) = (x −a)2q(x) for some q(x)
∈F[x] if and only if p(a) = p′(a) = 0.
Proof. Suppose p(x) = (x −a)2q(x). Then it is obvious from Leibniz that
p(a) = p′(a) = 0. Suppose, conversely, that p(a) = p′(a) = 0. Then p(x) =
(x −a)s(x), where s(x) ∈F[x], so p′(x) = (x −a)′s(x) + (x −a)s′(x) = s(x) +
(x −a)s′(x). Hence, 0 = p′(a) = s(a). This means that s(x) = (x −a)t(x) for
some t(x) ∈F[x], and thus p(x) = (x −a)2q(x), as claimed.
□
One says that a ∈F is a simple root of p(x) ∈F[x] if and only if (x −
a) divides p(x) but (x −a)2 does not. A root that is not simple is called
a multiple root. The next result formulates Proposition 2.37 as a test for
whether a root of p(x) is simple.
Corollary 2.38 (The simple root test). Let p(x) ∈F[x]. Then a is a simple
root if and only if p(a) = 0 but p′(a) ̸= 0, and a is a multiple root if and only
if p(a) = p′(a) = 0.
Exercises
Exercise 2.6.1. Write out the addition and multiplication tables for the
ﬁeld F7. Also, indicate the location of the multiplicative inverse for each
nonzero element.
Exercise 2.6.2. Find both −(6 + 6) and (6 + 6)−1 in F7.
Exercise 2.6.3. Show that Q, R, and C all have characteristic zero.

54
2
Groups and Fields: The Two Fundamental Notions of Algebra
Exercise 2.6.4. Let F be a ﬁeld and suppose that F′ ⊂F is a subﬁeld. Show
that F and F′ have the same characteristic.
Exercise 2.6.5. Show that the characteristic of Fp is p.
Exercise 2.6.6. Let F and F′ be ﬁelds. Show that every ﬁeld homomorphism
ϕ : F →F′ is injective.
Exercise 2.6.7. Suppose that the ﬁeld F contains Fp as a subﬁeld. Show
that the characteristic of F is p.
Exercise 2.6.8. Suppose that F is a ﬁeld of characteristic p > 0. Show that
all multiples of 1 including 0 form a subﬁeld of F with p elements. (This
subﬁeld is in fact a copy of Fp.)
Exercise 2.6.9. Prove Proposition 2.33. That is, show that if F is a ﬁnite
ﬁeld of characteristic p, then for all a1, . . . , an ∈F,
(a1 + · · · + an)p = ap
1 + · · · + ap
n.
Exercise 2.6.10. Use Proposition 2.33 to show that ap = a in Fp. Use this
to deduce Fermat’s little theorem.
Exercise 2.6.11. In the deﬁnition of the ﬁeld F4 in Section 2.6.2, can we
alter the deﬁnition of multiplication by putting α2 = β2 = 1 and still get a
ﬁeld?
Exercise 2.6.12. Suppose F is a ﬁeld of characteristic p. Show that if a, b ∈
F and ap = bp, then a = b.
Exercise 2.6.13. A ﬁeld of characteristic p is said to be perfect if every
element is a pth power. Show that every Galois ﬁeld is perfect. (Hint: use the
pigeonhole principle.)
Exercise 2.6.14. Use Fermat’s little theorem to ﬁnd 9−1 in Fp for p = 11,
13, 23, and 29. Use these results to solve the congruence equation 9x ≡15
mod p for the above values of p.
Exercise 2.6.15. A primitive element of Fp is an element β such that
Fp = {0, 1, β, β2, . . . , βp−2}.
It can be shown that for every prime p, Fp contains a primitive element. Find
at least one primitive element β for Fp when p = 5, 7, and 11.
Exercise 2.6.16. Write out the addition and multiplication tables for Z6.
Is Z6 a ﬁeld? If not, why not?

2.6
Galois ﬁelds
55
Exercise 2.6.17. Let p be a prime. Let φp : Z →Fp be the quotient map-
ping a →a, where a = a + pZ. Show that for all a, b ∈Z,
(1) φp(a + b) = φp(a) + φp(b), and
(2) φp(ab) = φp(a)φp(b).
Thus, φ is a homomorphism of rings. Use these two facts to deduce that
addition and multiplication in Fp are associative from the fact that they are
associative in Z.
Exercise 2.6.18. Using the deﬁnition of the derivative of a polynomial
f(x) ∈F[x] in Section 2.6.4, show that the product rule for diﬀerentiation
holds. That is, if f(x), g(x) ∈F[x], then (fg)′ = f ′g + fg′.
Exercise 2.6.19. Let F be a ﬁeld of characteristic p. Show that for every
n > 0, xpn −x = 0 has only simple roots in F.

Chapter 3
Matrices
Matrix theory is deeply embedded in the foundations of algebra. The idea of
a matrix is very simple, and useful examples and ideas present themselves
immediately, as we shall soon see. So it is surprising that their structure
turns out to be subtle. Matrices also represent abstract objects called linear
mappings, which we will treat after vector spaces. The main results in the
theory of linear mappings, including the Cayley–Hamilton theorem, Jordan
decomposition, and Jordan canonical form, are strikingly beautiful.
More down to earth, matrices represent systems of linear equations in
several variables. Such a linear system has the form Ax = b, where A, x,
and b are all matrices, and Ax is the product of A and x. In this form, a
linear system is a special case of the basic algebraic equation ax = b. Matrix
algebra arises from addition and multiplication in a ﬁeld, and it satisﬁes
some of the ﬁeld axioms: for example, the associative and distributive laws.
However, matrices usually do not have multiplicative inverses. When they do
is a topic that will be taken up in the next chapter. The ﬁrst goal of this
chapter is matrix algebra and the procedure known as row reduction, which
amounts to replacing a matrix A by a unique matrix in what is called reduced
row echelon form. The reduced row echelon form of a matrix is used to solve
linear systems. It also tells us the rank of the matrix and gives the inverse of
A when one exists. Prove the existence of an LPDU factorization, which is
how one understands how the matrix is constructed.
3.1
Introduction to matrices and matrix algebra
The purpose of this section is to introduce the notion of a matrix, give some
motivation, and make the basic deﬁnitions used in matrix algebra.
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 3
57

58
3
Matrices
3.1.1
What is a matrix?
Matrices arise from linear systems. Let F be a ﬁeld. A system of m linear
equations in n variables x1, . . . , xn with coeﬃcients aij and constants bi, all
of which lie in F, is a family of equations of the form
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
am1x1 + am2x2 + · · · + amnxn = bm.
(3.1)
Concentrating on the mn coeﬃcients aij, let us form the rectangular array
A =
⎛
⎜
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a23
. . .
a2n
...
...
· · ·
...
am1
am2
. . .
amn
⎞
⎟
⎟
⎟
⎠.
(3.2)
This array is called a matrix.
Deﬁnition 3.1. Let F be a ﬁeld and m and n positive integers. An m × n
matrix over F is a rectangular array of the form (3.2), where all the entries
aij are in F.
Notice that the elements in the ith row all have ﬁrst subscript i, and those
in the jth column have second subscript j. The set of all m × n matrices over
F will be denoted by Fm×n. In particular, those with real entries are denoted
by Rm×n, and those with complex entries are denoted by Cm×n.
Now let
x =
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠
and
b =
⎛
⎜
⎜
⎜
⎝
b1
b2
...
bm
⎞
⎟
⎟
⎟
⎠.
Then the system (3.1) will turn out to be represented by the compact equation
Ax = b. Here Ax is the column matrix on the left-hand side of the equalities,
and b is the column matrix on the right-hand side. We should think of Ax
as a matrix product. The general deﬁnition of a matrix product will be given
below.

3.1
Introduction to matrices and matrix algebra
59
3.1.2
Matrix addition
Our immediate goal is to deﬁne the algebraic operations for matrices: addi-
tion, multiplication, and scalar multiplication. We will begin with addition
and scalar multiplication.
Deﬁnition 3.2. Let A, B ∈Fm×n. The matrix sum (or simply the sum) A +
B is deﬁned as the matrix C ∈Fm×n such that cij = aij + bij for all pairs of
indices (i, j). The scalar multiple of A by α ∈F is the matrix αA of Fm×n
whose (i, j) entry is αaij.
Thus addition is a binary operation on Fm×n. It is clearly associative, since
addition is associative in F. The matrix O ∈Fm×n all of whose entries are
zero is called the zero matrix. The zero matrix is the additive identity for
Fm×n. That is, A + O = A for all A ∈Fm×n. The matrix −A = (−1)A is an
additive inverse of A, since A + (−A) = (−A) + A = O. Thus we have the
following result.
Proposition 3.1. Fm×n is an abelian group under matrix addition.
Example 3.1. Here are some examples with F = R:
A =
⎛
⎝
1
0
0
2
0
1
0
3
0
0
1
5
⎞
⎠,
and
B =
⎛
⎝
1
2
3
0
0
0
3
1
1
2
3
0
⎞
⎠.
Then
A + B =
⎛
⎝
2
2
3
2
0
1
3
4
1
2
4
5
⎞
⎠.
Doubling A, that is multiplying A by the scalar 2, gives
2A =
⎛
⎝
2
0
0
4
0
2
0
6
0
0
2
10
⎞
⎠.
⊠
A matrix in F1×n is called a row vector. Similarly, a matrix in Fn×1 is
called a column vector. As long as the context is clear, we will often use Fn
to denote either column vectors Fn×1 or row vectors F1×n. Vectors will be
written as boldface letters like x and their components by the same letter
in ordinary type. Thus, the ith component of x is xi. To conserve space, we
may write a column vector

60
3
Matrices
x =
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠= (x1, x2, . . . , xn)T .
(3.3)
The superscript T, which is called the transpose operator, changes rows into
columns. A full discussion of the transpose operator is given below in Section
3.1.6.
Several matrices A1, . . . , Am in Fm×n can be combined using addition and
scalar multiplication. The result is called a linear combination. That is, given
scalars a1, a2, . . . , am ∈F, the matrix
a1A1 + a2A2 + · · · + amAm ∈Fm×n
is the linear combination of A1, A2, . . . , Am with coeﬃcients a1, a2, . . . , am.
The set of all linear combinations a1A1 + a2A2 + · · · + amAm is called the
span of A1, . . . , Am. The set of m × n matrices over F with the above addition
and scalar multiplication is an important example of a vector space. The
theory of vector spaces is, of course, one of the basic topics of this text.
3.1.3
Examples: matrices over F2
Matrices over the ﬁeld F2 are themselves quite interesting. For example, they
are easy to enumerate: since F2 has only two elements, there are precisely
2mn m × n matrices. Addition of such matrices has some interesting features,
which the following example illustrates.
Example 3.2. Let
A =

1
0
1
0
1
1
	
and E =

1
1
1
1
1
1
	
.
Then
A + E =

0
1
0
1
0
0
	
.
Thus, the parity of every element of A is reversed by adding E. Adding A to
itself gives
A + A =

1
0
1
0
1
1
	
+

1
0
1
0
1
1
	
=

0
0
0
0
0
0
	
= O.
Thus a matrix over F2 is its own additive inverse.
⊠

3.1
Introduction to matrices and matrix algebra
61
Example 3.3. Here is how one might design a scanner to analyze black-
and-white photographs. A photo is a rectangular array consisting of many
black and white dots. By giving the white dots the value 0 and the black dots
the value 1, a photo is therefore transformed into a (very large) matrix over
F2. Now suppose one wants to compare two black-and-white photographs to
see whether they are in fact identical. Suppose they have each been encoded
as m × n matrices A and B. It turns out to be very ineﬃcient for a computer
to scan the two matrices to see in how many positions they agree. However,
when A and B are added, the sum A + B has a 1 in every component where
A and B diﬀer, and a 0 wherever they coincide. For example, the sum of two
identical photographs is the zero matrix, and the sum of a photograph and
its negative is the all-ones matrix. An obvious measure of how similar the
two matrices A and B are is the number of nonzero entries of A + B, i.e.,
Σi,j(aij + bij). This is an easily tabulated number, which is known as the
Hamming distance between A and B.
⊠
Example 3.4. Random Key Cryptography. Suppose Rocky the ﬂying
squirrel wants to send a message to his sidekick, Bullwinkle the moose, and
he wants to make sure that the notorious villains Boris and Natasha won’t
be able to learn what it says. Here is what the ever resourceful squirrel might
do. First he could assign the numbers 1 to a, 2 to b, and so forth up to 26
to z. He then computes the binary expansion of each integer between 1 and
26. Thus 1 = 1, 2 = 10, 3 = 11, 4 = 100, . . . , 26 = 11010. He now converts
his message into a sequence of ﬁve-digit strings (1 = 00001, etc.). The result
is an encoding of the message, which is referred to as the plaintext. To make
things more compact, he arranges the plaintext into an m × n matrix, call
it P. Now the deception begins. Rocky and Bullwinkle have already selected
an m × n matrix of ﬁve-digit strings of 0’s and 1’s, which we call Q. This
matrix is what cryptographers sometimes call a key. Rocky will send the
matrix P + Q to Bullwinkle, where the addition of strings is performed in
(F2)5×1. Bullwinkle will be able to recover P easily by adding Q. Indeed,
(P + Q) + Q = P + (Q + Q) = P + 2Q = P. This is good, since Bullwinkle,
being a moose, is somewhat mathematically challenged and ﬁnds subtraction
diﬃcult. Even if Boris and Natasha manage to intercept the ciphertext P +
Q, they need to know the key Q to recover P. However, the key Q must
be suﬃciently random so that neither Boris nor Natasha can guess it. The
squirrel’s encryption scheme is extremely secure if the key Q is a one-time
pad, that is, it is used only once.
⊠
3.1.4
Matrix multiplication
Matrix addition and scalar multiplication are simple and natural opera-
tions. The product of two matrices, on the other hand, is a little more

62
3
Matrices
complicated. We already mentioned that multiplication can be used to rep-
resent the unwieldy linear system (3.1) as Ax = b. Let us look at this more
carefully. Suppose A is a 1 × n row matrix and B is an n × 1 column matrix.
The product AB is then deﬁned as follows:
AB =

a1
· · ·
an

⎛
⎜
⎝
b1
...
bn
⎞
⎟
⎠=
n

i=1
aibi.
(3.4)
We will call this product the dot product of A and B. Note that it is very
important that the number of columns of A and the number of rows of B
agree. We next deﬁne the product of an m × n matrix A and an n × p matrix
B by generalizing the dot product.
Deﬁnition 3.3. Let A ∈Fm×n and B ∈Fn×p. Then the product AB of A
and B is deﬁned as the matrix C ∈Fm×p whose entry in the ith row and kth
column is the dot product of the ith row of A and the kth column of B. That
is,
cik =
n

j=1
aijbjk.
Thus
AB =

n

j=1
aijbjk

.
One can therefore express the eﬀect of multiplication as
Fm×n Fn×p ⊂Fm×p.
One can also formulate the product in terms of linear combinations. Namely,
if the columns of A are a1, . . . , an, then, since the scalar in the ith row of
each column of B multiplies an element in the ith column of A, we see that
the rth column of AB is
b1ra1 + b2ra2 + · · · + bnran.
(3.5)
Hence the rth column of AB is a linear combination of all n columns of A.
The entries in the rth column of B are the scalars. Similarly, one can express
AB as a linear combination of the rows of B. We will leave this as an exercise.
Example 3.5. Here are two examples in which F = Q:
1
3
2
4
	  6
0
−2
7
	
=
1 · 6 + 3 · (−2)
1 · 0 + 3 · 7
2 · 6 + 4 · (−2)
2 · 0 + 4 · 7
	
=
0
21
4
28
	
.

3.1
Introduction to matrices and matrix algebra
63
Note how the columns of the product are linear combinations. Computing
the product in the opposite order gives a diﬀerent result:

6
0
−2
7
	 
1
3
2
4
	
=

6 · 1 + 0 · 2
6 · 3 + 0 · 4
−2 · 1 + 7 · 2
−2 · 3 + 7 · 4
	
=

6
18
12
22
	
.
⊠
This example points out that matrix multiplication on Fn×n is in general
not commutative: if A, B ∈Fn×n, then in general, AB ̸= BA. In fact, if you
randomly choose two 2 × 2 matrices over Q, it is a safe bet that they won’t
commute.
3.1.5
The Algebra of Matrix Multiplication
We have now deﬁned the three basic algebraic operations on matrices. Let
us next see how they interact. Although matrix multiplication isn’t commu-
tative, matrix addition and multiplication behave as expected.
Proposition 3.2. Assuming that all the sums and products below are deﬁned,
matrix addition and multiplication satisfy the following conditions.
(i) the associative laws. addition and multiplication are associative:

A + B

+ C = A +

B + C

and

AB

C = A

BC

;
(ii) the distributive laws. addition and multiplication are distributive:
A

B + C

= AB + AC
and

A + B

C = AC + BC;
(iii) the scalar multiplication law. for every scalar r,

rA

B = A

rB

= r

AB

;
(iv) the commutative law for addition. addition is commutative:
A + B = B + A.
Verifying these properties is a routine exercise. We already commented
that addition is associative and commutative while showing that Fm×n is an
abelian group under addition. The reader should note that the validity of the
associative law for multiplication will be extremely useful. We will see this,
for example, when we consider matrix inverses.

64
3
Matrices
The n × n matrix In having ones everywhere on its diagonal and zeros
everywhere oﬀthe diagonal is called the identity matrix. One sometimes
writes In = (δij), where δij is the Kronecker delta, which is deﬁned by the
rule δii = 1 while δij = 0 if i ̸= j. For example,
I2 =

1
0
0
1
	
and
I3 =
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠.
A convenient way to deﬁne In is to write out its columns. Let ei denote the
column vector whose ith component is 1 and whose other components are 0.
Then
In = (e1 e2 · · · en).
The reason In is called the identity matrix is due to the following fact.
Proposition 3.3. Let A be an m × n matrix over F. Then ImA = A and
AIn = A. In particular, the ith column of A is Aei.
Proof. This
follows
immediately
from
the
deﬁnition
of
matrix
multiplication.
□
3.1.6
The transpose of a matrix
The transpose of an m × n matrix A is the n × m matrix AT whose ith row is
the ith column of A. That is, if A = (aij), then AT = (crs), where crs = asr.
The deﬁnition becomes clearer after working an example.
Example 3.6. If
A =
⎛
⎝
1
2
3
4
5
6
⎞
⎠,
then
AT =

1
3
5
2
4
6
	
.
⊠
For another example, note that ei = (0, . . . , 0, 1, 0, . . . , 0)T , where 1 is in
the ith component. Note that A and AT have the same entries on the diag-
onal. Also note that the transpose of AT is A:
(AT )T = A.

3.1
Introduction to matrices and matrix algebra
65
Deﬁnition 3.4. A matrix A that is equal to its own transpose is called
symmetric.
An example of a 2 × 2 symmetric matrix is

1
3
3
5
	
.
Clearly, a symmetric matrix is square (but not uninteresting). For example,
the symmetric matrices over R are all turn out to be diagonalizable due to
a fundamental result called the principal axes theorem. But this will not be
explained for quite a while.
The transpose of a sum is as expected, but the transpose of a product has
a twist, as we note in the next proposition.
Proposition 3.4. For every m × n matrices A and B,
(A + B)T = AT + BT .
Furthermore, if A is m × n and B is n × p, then

AB
T = BT AT .
Proof. The ﬁrst identity is immediate. To prove the product identity, note
that the (i, j) entry of BT AT is the dot product of the ith row of BT and
the jth column of AT . This is the same thing as the dot product of the jth
row of A and the ith column of B, which is the (j, i) entry of AB and hence
the (i, j) entry of (AB)T . Thus (AB)T = BT AT .
□
It is suggested that the reader try this proof on an example.
3.1.7
Matrices and linear mappings
One way to look at matrix multiplication is that it deﬁnes a mapping. Let
Fn = Fn×1 and Fm = Fm×1. Then for every A ∈Fm×n, say A = (a1 · · · an),
we obtain a mapping
TA : Fn →Fm,
where TA(x) = Ax = x1a1 + · · · + xnan.
Note that TA(ei) = ai for each index i, and TA is uniquely determined by
the TA(ei).
Proposition 3.5. For every A ∈Fm×n, the mapping TA : Fn →Fm associ-
ated to A is in fact a homomorphism with domain Fn and target Fm. That
is, A(x + y) = Ax + Ay for every x, y ∈Fn.

66
3
Matrices
Proof. This
is
simply
an
application
of
the
distributive
law
for
multiplication.
□
We will frequently use this proposition in geometric examples.
Example 3.7. (Rotations of R2). One such example is a rotation of R2 given
by the mapping
Rθ

x
y
	
=

x cos θ −y sin θ
x sin θ + y cos θ
	
.
Here, Rθ is the matrix mapping associated to the matrix
Rθ =
cos θ
−sin θ
sin θ
cos θ
	
.
(3.6)
Recalling that C = R2, multiplication by the complex exponential eiθ =
cos θ + i sin θ sends z = x + iy to
eiθz = (cos θ + i sin θ)(x + iy) = (x cos θ −y sin θ) + i(x sin θ + y cos θ).
Therefore, the matrix mapping given by Rθ is the same as multiplication by
the complex exponential eiθ.
⊠
Now suppose A ∈Fm×n and B ∈Fn×p. Then TAB is the mapping with
domain Fp and target Fm deﬁned by TAB(u) = (AB)u for every u ∈Fp. Since
by associativity (AB)u = A(Bu), it follows that TA ◦TB = TAB. Therefore,
we have the following.
Proposition 3.6. If A ∈Fm×n and B ∈Fn×p, then the matrix mapping
TAB : Fp →Fm satisﬁes
TAB = TA ◦TB.
That is, TAB is the composition of the matrix mappings TB : Fp →Fn and
TA : Fn →Fm. Hence matrix multiplication corresponds to the composition of
matrix mappings.
Exercises
Exercises 3.1.1. As an experiment, construct three matrices A, B, C of
dimensions such that AB and BC are deﬁned. Then compute AB and (AB)C.
Next compute BC and A(BC) and compare your results. If A and B are also
square, do AB and BA coincide?
Exercises 3.1.2. Prove
the
assertion
(A + B)T = AT + BT
in
Proposition 3.4 without writing down matrix entries.

3.1
Introduction to matrices and matrix algebra
67
Exercises 3.1.3. Suppose A and B are symmetric n × n matrices. (You can
even assume n = 2.)
(i) Must AB be symmetric? That is, are the n × n symmetric matrices closed
under multiplication?
(ii) If the answer to (i) is no, ﬁnd a condition that ensures that AB is
symmetric.
Exercises 3.1.4. Suppose B has a column of zeros. How does this aﬀect a
product of the form AB? What if A has a row of zeros?
Exercises 3.1.5. State a rule for expressing the rows of AB as linear com-
binations of the rows of B. (Suggestion: use the transpose identity and the
result expressing the columns of AB).
Exercises 3.1.6. Verify Proposition 3.3 for all A in Fm×n.
Exercises 3.1.7. Let F = R. Find all 2 × 2 matrices A =
a
b
c
d
	
such that
AB = BA, where B =

1
2
3
4
	
.
Exercises 3.1.8. Assume here that F = F3. Find all 2 × 2 matrices A =

a
b
c
d
	
such that AC = CA, where C =

1
2
1
2
	
. Is your result here any
diﬀerent from the result you obtained in Exercise 3.1.7.
Exercises 3.1.9. Let F be a ﬁeld. Prove that if S ∈F2×2 commutes with
every matrix A =

a
b
c
d
	
∈F2×2, then S = aI2 for some a ∈F. Matrices of
the form aIn are called scalar matrices.
Exercises 3.1.10. Let p be a prime, and let A be the 2 × 2 matrix over Fp
such that aij = 1 for each i, j. Compute Am for every integer m ≥1. (Note
that Am stands for the mth power of A.)
Exercises 3.1.11. Let A be the n × n matrix over Q such that aij = 2 for
all i, j. Find a formula for Aj for every positive integer j.
Exercises 3.1.12. Give an example of a 2 × 2 matrix A such that every
entry of A is either 0 or 1 and A2 = I2 as a matrix over F2, but A2 ̸= I2 as
a matrix over Q.

68
3
Matrices
3.2
Reduced Row Echelon Form
The standard procedure for solving a linear system is to use a process called
Gaussian elimination to put the system in a standard form in which it is
ready to solve. This process involves using row operations to put the coef-
ﬁcient matrix into a standard form called reduced row echelon form. It will
turn out that every matrix can be put into this standard form by a pre-
multiplication. We will later see that row operations preserve two important
quantities associated to A: the row space of A and its null space.
3.2.1
Reduced row echelon form and row
operations
Deﬁnition 3.5. A matrix A is said to be in row echelon form if
(i) the ﬁrst nonzero entry in each row of A is to the right of the ﬁrst nonzero
entry in the preceding row (and hence in all preceding rows), and
(ii) every entry above a ﬁrst nonzero entry is zero.
The ﬁrst nonzero entry in a row is called its pivot entry or corner entry.
A matrix A in row echelon form is said to be in reduced row echelon form,
or simply, to be reduced, if each corner entry is 1. Here are some examples of
reduced matrices:
⎛
⎝
1
0
0
2
0
1
0
3
0
0
1
5
⎞
⎠,
⎛
⎝
1
2
3
0
9
0
0
0
1
4
0
0
0
0
0
⎞
⎠,

0
1
3
0
9
0
0
0
1
0
	
.
The zero matrix O ∈Fm×n and the identity matrix In are reduced also.
A row operation on a matrix A replaces one of the rows of A by a new
row. The three row operations we will now deﬁne are called the elementary
row operations.
Deﬁnition 3.6. Let A be a matrix over F with rows a1, . . . , am. The ele-
mentary row operations over F on A are as follows:
(I) interchange the ith row ai and the jth row aj, where i ̸= j;
(II) replace the ith row ai with a nonzero scalar multiple rai, where r ∈F;
(III) replace the ith row ai by ai + raj, where r ∈F, r ̸= 0, and i ̸= j. That
is, replace ai by itself plus a nonzero multiple of some other row.
Row operations of type I are called row swaps. The type II operations are
called row dilations, and operations of type III are called transvections. The
next proposition gives the basic property of row operations.

3.2
Reduced Row Echelon Form
69
Proposition 3.7. Every matrix over a ﬁeld F can be put into reduced
row echelon form by a (not unique) sequence of elementary row operations
over F.
Before giving a proof, let us work an example. Each arrow below indicates
a single row operation, and the notation over the arrows indicates the row
operation in an obvious way.
Example 3.8. Consider the counting matrix
C =
⎛
⎝
1
2
3
4
5
6
7
8
9
⎞
⎠
as a matrix over Q. We can row reduce C as follows:
C
R2−4R1
−→
⎛
⎝
1
2
3
0
−3
−6
7
8
9
⎞
⎠R3−7R1
−→
⎛
⎝
1
2
3
0
−3
−6
0
−6
−12
⎞
⎠
R3−2R2
−→
⎛
⎝
1
2
3
0
−3
−6
0
0
0
⎞
⎠(−1/3)R2
−→
⎛
⎝
1
2
3
0
1
2
0
0
0
⎞
⎠R1−2R2
−→
⎛
⎝
1
0
−1
0
1
2
0
0
0
⎞
⎠.
⊠
Let us now prove Proposition 3.7.
Proof. We will induct on the number of columns of A. Assume ﬁrst that A
has just one column. If A is the zero matrix O, it is already in reduced row
echelon form, so suppose A ̸= O. If a11 ̸= 0, multiply the ﬁrst row by a−1
11 to
produce a 1 in the (1, 1) position. (This is why we require row operations
over a ﬁeld.) We can then use row operation s of type III to make all other
entries in the ﬁrst column zero. If a11 = 0, but the ﬁrst column has a nonzero
entry somewhere, say the ith row, then swap the ﬁrst row with the ith row to
create a nonzero entry in the (1, 1) position. Next, proceed as before, dividing
the new ﬁrst row by the inverse of its (1, 1) entry, getting a one in the (1, 1)
position. Then use row operations of type III to make all the other elements
in the ﬁrst column 0. Thus we have put A into reduced row echelon form by
row operations.
Now suppose that every m × n matrix over F can be put into reduced row
echelon form by row operations, and let A be an m × (n + 1) matrix over F.
Let A′ be the m × n matrix consisting of the ﬁrst n columns of A. By the
induction hypothesis, there exists a sequence of row operations that puts A′
in reduced row echelon form, say B′. After performing this sequence of row
operations on A, we obtain a matrix B whose the ﬁrst n columns have been

70
3
Matrices
put into reduced row echelon form. Assume that the lowest corner entry in
B′ is in the kth row. If the last column of B has only zeros below the kth
row, then B is already in reduced row echelon form, completing the proof.
Otherwise, B has a nonzero entry below the kth row, and we can repeat the
steps used in the case of one column to get a corner entry in the (k + 1, n + 1)
of B and zeros below. Since these row operations aﬀect only the rows below
the kth row, B′ is not changed. Thus, A can be put into reduced row echelon
form. This completes the induction step, so every matrix over F can be put
into reduced row echelon form by elementary row operations.
□
Remark. One can refer to performing a sequence of row operations on A as
row surgery. For every matrix A, there are many diﬀerent row surgeries lead-
ing to a matrix in reduced row echelon form. An interesting and important
point, often completely ignored, is that a matrix has only one reduced row
echelon form. That is, the reduced row echelon form of an arbitrary A ∈Fm×n
is unique. We will use Ared to denote this matrix, even though we have not
yet proved its uniqueness. The proof, which is not obvious, will be given in
Proposition 3.12. This result will let us assert that the number of nonzero
rows in Ared is unique. The term commonly used for this number is the row
rank of A. The row rank is important, because it gives us information such
as when the solution of a linear system is unique.
First we will introduce elementary matrices in order to get an eﬃcient
algorithm for row reducing a matrix.
3.2.2
Elementary matrices and row operations
We now introduce elementary matrices and explain their role in reduced row
echelon form.
Deﬁnition 3.7. An n × n matrix that is obtained by performing a single
row operation on In is called an elementary matrix.
Example 3.9. The elementary 2 × 2 matrices are illustrated as follows:
E1 =
0
1
1
0

,
E2 =
r
0
0
1

or
1
0
0
r

,
E3 =
1
s
0
1

or
1
0
s
1

.
Here, r and s are nonzero scalars.
⊠
The following computations show that premultiplication by one of the
above 2 × 2 elementary matrices Ei performs the same row operation on
A =
a
b
c
d
	
that is performed on the identity I2 to get Ei:

3.2
Reduced Row Echelon Form
71

0
1
1
0
	 
a
b
c
d
	
=

c
d
a
b
	
(row swap),

r
0
0
1
	 
a
b
c
d
	
=

ra
rb
c
d
	
(row dilation),
1
s
0
1
	 a
b
c
d
	
=
a + sc
b + sd
c
d
	
(row transvection).
One might call the next proposition the golden rule of matrix multiplication.
Proposition 3.8. Let A be of size m × n, and assume that E is an elemen-
tary m × m matrix. Then EA is the matrix obtained by performing the row
operation corresponding to E on A.
Proof. Recall that for every B ∈Fm×m, the rows of BA are linear combina-
tions of the rows of A using the entries of B = (bij) as scalars. In fact, if ai
is the ith row of A, then the ith row of BA is
bi1a1 + bi2a2 + · · · + bimam.
Thus, if B is the elementary matrix obtained by multiplying the ith row of In
by r, then the ith row of BA becomes rai, and all other rows are unchanged.
Likewise, if B is obtained by interchanging the ith and jth rows of In, then
the ith row of BA is aj, since bik = δjk, and similarly, the jth row is ai. The
argument for the third type of row operation is analogous, so it is left as an
exercise.
□
In fact, since EIn = E, the matrix E performing the desired row operation
is unique.
Thus row reduction can be expressed as follows: starting with A and replac-
ing it by A1 = E1A, A2 = E2(E1A), and so forth, we get the sequence
A →A1 = E1A →A2 = E2(E1A) →· · · →Ak = Ek(Ek−1(· · · (E1A) · · · )).
Assuming that the right-hand matrix Ak is Ared, we obtain by this process
a product of elementary matrices
B = EkEk−1 · · · E1
such that BA = Ared. Note: this assertion uses the associativity of matrix
multiplication. It should be emphasized that the way we choose the Ei isn’t
unique. Yet it will turn out that in certain cases, their product B is unique.
This seems to be a remarkable fact.

72
3
Matrices
Example 3.10. Let’s compute the matrix B produced by the sequence of
row operations in Example 3.8, which puts the counting matrix C in reduced
form. Examining the sequence of row operations, we see that B is the product
⎛
⎝
1
−2
0
0
1
0
0
0
1
⎞
⎠
⎛
⎝
1
0
0
0
−1/3
0
0
0
1
⎞
⎠
⎛
⎝
1
0
0
0
1
0
0
−2
1
⎞
⎠
⎛
⎝
1
0
0
0
1
0
−7
0
1
⎞
⎠
⎛
⎝
1
0
0
−4
1
0
0
0
1
⎞
⎠.
In computing the matrix B, the easy way is to start at the right and apply
the sequence of row operations working to the left. A convenient way of doing
this is to begin with the 3 × 6 matrix (A | I3) and carry out the sequence of
row operations; the ﬁnal result will be (Ared | B). Thus if we start with
(C | I3) =
⎛
⎝
1
2
3
1
0
0
4
5
6
0
1
0
7
8
9
0
0
1
⎞
⎠,
we end with
(Cred | B) =
⎛
⎝
1
0
−1
−5/3
2/3
0
0
1
2
4/3
−1/3
0
0
0
0
1
−2
1
⎞
⎠.
⊠
We may summarize this section by observing that elementary matrices
perform row operations, so Proposition 3.7 can be restated as follows.
Proposition 3.9. An arbitrary m × n matrix A over a ﬁeld F can be put
into reduced row echelon form by performing a sequence of left multiplications
on A using m × m elementary matrices over F. In other words, there exist
elementary matrices E1, . . . , Ek ∈Fm×m such that
Ared = EkEk−1 · · · E1A.
Proof. By Proposition 3.7, every matrix can be put into reduced form by a
sequence of row operations. But row operations are performed by left multi-
plication by elementary matrices.
□
3.2.3
The row space and uniqueness of reduced
row echelon form
After doing several row reductions, the reader may wonder whether Ared
is unique. Could choosing a diﬀerent sequence of row operations lead to a

3.2
Reduced Row Echelon Form
73
diﬀerent reduced matrix? As we remarked above, the answer is no. We now
prove this. We begin by noticing that every elementary row operation replaces
a row of A by a linear combination of its other rows. This leads us to consider
a fundamental notion, namely the row space of A.
Deﬁnition 3.8. Suppose A ∈Fm×n. The row space of A is deﬁned as the
span of the rows of A.
The row space of A is denoted by row(A). Thus, row(A) is the set of all
linear combinations of the rows of A. The basic result about row spaces and
the key to understanding the role of row operations is the following:
Proposition 3.10. If A′ is obtained from A by applying an elementary row
operation to one of the rows of A, then A′ and A have the same row space.
That is, row(A′) = row(A).
Proof. Let A and A′ be of size m × n, and let a1, . . . , am be the rows of A.
If the row operation is of type I, then A and A′ have the same row space,
because a type I operation just reorders the rows, hence does not change the
set of all linear combinations. Likewise, a type II row operation replaces some
ai by rai, where r is a nonzero scalar. Thus
c1a1 + · · · + ciai + · · · + cmam = c1a′
1 + · · · + ci
r a′
i + · · · + cma′
m.
Hence A and A′ also have the same row space. Finally, if A′ is obtained by
replacing ai by ai + raj, where j > i, then
c1a1 + · · · + ciai + · · · + cmam = c1a1 + · · · + ci(ai + raj)
+ · · · + (cj −rci)aj + · · · + cmam.
Again, it follows that A and A′ have the same row space.
□
We now prove the key step.
Proposition 3.11. Suppose A and B are two m × n matrices over F in
reduced row echelon form. Then A = B if and only if their row spaces are
equal.
Proof. If A = B, then obviously their row spaces are equal. The proof in the
other direction isn’t hard, but it’s a little hard to write out. First, suppose
that A has k nonzero rows and B has ℓof them. Since we don’t know the
relationship between k and ℓ, we may harmlessly suppose that k ≤ℓ. Let
a1, . . . , ak denote the nonzero rows of A. We will assume that they are labeled
so that the ﬁrst nonzero component of ai is to the right of that of ai+1. Hence
a1 is the last nonzero row of A. Assume that the nonzero rows b1, . . . , bℓof

74
3
Matrices
B are labeled in the same way. The ﬁrst step is to show that a1 = b1. This is
done as follows. Suppose the corner entries in a1 and b1 aren’t in the same
component; say the corner entry of a1 is to the left of the corner entry of b1.
Then obviously a1 can’t be a linear combination of the bi. By symmetry, the
corner entries of a1 and b1 are in the same component. Since b1 is a linear
combination of the ai, the only possibility is b1 = r1a1, and in fact, r1 = 1,
due to the fact that the corner entries of a1 and b1 are both one. Now, the
matrices
A1 =
a2
a1
	
and
B1 =
b2
b1
	
have the same second row and are, by assumption, in reduced row echelon
form. Suppose the corner entry in b2 is to the right of that of a2. Then b2
cannot be a linear combination of the ai, so once again a2 and b2 have to
have their corner entries in the same component. Hence, b2 = a2 + r1a1. But
since A1 is in reduced row echelon form, a2 + r1a1 has r1 in the component
corresponding to the corner entry of a1. But since B1 is also in reduced row
echelon form and b1 = a1, it follows that b2 has a zero in that component.
Consequently, r1 = 0. Thus, b2 = a2. One can now repeat this argument to
show that b3 = a3, b4 = a4, and so on, eventually arriving at the conclusion
bi = ai for 1 ≤i ≤k. Finally, we have to show that ℓ= k. If k < ℓ, then
bk+1 ̸= 0, and it lies in row(B) = row(A). Hence, bk+1 is a linear combination
of  riai with some ri ̸= 0. But this is impossible, because the component of
bk+1 corresponding to the corner entry is to the left of that of ak due to the
fact that ak = bk. This shows that k = ℓand hence ﬁnishes the proof that
A = B.
□
Finally, we answer the basic question.
Proposition 3.12. The reduced row echelon form of an arbitrary matrix
A ∈Fm×n is unique.
Proof. If two diﬀerent sequences of row operations yield two diﬀerent reduced
row echelon forms B and C for A, then by the previous proposition, we obtain
a contradiction to row(A) = row(B) = row(C). Hence B = C.
□
The fact that the reduced row echelon form of a matrix A is unique means
that the number of nonzero rows of Ared depends only on A. This leads to
the following important deﬁnition.
Deﬁnition 3.9. Suppose A ∈Fm×n. Then the number of nonzero rows in
Ared is called the rank of A.

3.2
Reduced Row Echelon Form
75
When we study vector spaces, we will show that the rank of A is actually
the dimension of the row space of A. For square matrices, it turns out that
rank is a measure of how far the matrix is from being invertible.
Finally, note that Proposition 3.9 and the above discussion imply the fol-
lowing.
Proposition 3.13. A matrix A ∈Fn×n has rank n if and only if there exist
elementary matrices E1, . . . , Ek ∈Fn×n such that
Ek · · · E1A = In.
Proof. By deﬁnition, A has rank n if and only if Ared has n nonzero rows.
But since A ∈Fn×n, Ared has n nonzero rows if and only if Ared = In. Thus,
A has rank n implies by Proposition 3.9 that Ek · · · E1A = In for suitable
elementary matrices E1, . . . , Ek. Conversely, if there exist elementary matri-
ces E1, . . . , Ek such that Ek · · · E1A = In, then Ared = In by the uniqueness
of the reduced form.
□
Deﬁnition 3.10. When the rank of an n × n matrix A over a ﬁeld F is n,
we will say that A is nonsingular. Otherwise, A is said to be singular.
Exercises
Exercises 3.2.1. Make a list of all the row reduced 2 × 3 matrices over F2.
Exercises 3.2.2. Let F = F3. Find the reduced row echelon form of the
following matrices:

2
1
1
2
	
,
⎛
⎝
0
1
2
1
1
2
0
1
1
0
0
0
⎞
⎠,
⎛
⎝
1
2
0
2
1
1
0
2
1
⎞
⎠.
Exercises 3.2.3. True or False (give a brief reason to justify your answer).
(i) Two square matrices that can be row reduced to the same reduced row
echelon form are equal.
(ii) If a 3 × 3 matrix A has the property that each row contains two zeros and
a one, and each column contains two zeros and a one, then A is nonsingular.
Exercises 3.2.4. If an n × n matrix has the property that each row and
each column has exactly one nonzero entry, is it nonsingular?
Exercises 3.2.5. The ﬁeld is F2. Consider the following matrices:
A1 =
⎛
⎝
1
1
0
1
0
1
1
1
1
⎞
⎠,
A2 =
⎛
⎜
⎜
⎝
1
0
1
0
0
1
1
0
1
1
0
0
1
0
0
1
⎞
⎟
⎟
⎠.

76
3
Matrices
Find matrices B1 and B2 that are products of elementary matrices such that
BiAi is reduced for i = 1, 2.
Exercises 3.2.6. Prove that if E is an elementary n × n matrix and F is
the elementary matrix that performs the inverse (i.e., reverse) operation, then
FE = EF = In.
Exercises 3.2.7. Write down all the 3 × 3 elementary matrices E over F2,
and for each E, ﬁnd the matrix F deﬁned in the previous exercise such that
FE = EF = I3.
Exercises 3.2.8. In this exercise, we will introduce column operations.
(i) Deﬁne the notion of reduced column echelon form for an m × n matrix.
(ii) Next, deﬁne the three types of column operations.
(iii) Show how to perform column operations using elementary matrices.

3.3
Linear Systems
77
3.3
Linear Systems
So far, we have deﬁned matrices, introduced matrix algebra, and studied
row operations and the reduced form of a matrix. Let us now go back and
consider a problem that motivated the introduction of matrices: how to ﬁnd
the solution set of a system of linear equations.
3.3.1
The coeﬃcient matrix of a linear system
Recall from (3.1) that a linear system of m equations in n variables with
coeﬃcients and constants in a ﬁeld F has the form
a11x1 + a12x2 + · · · + a1nxn = b1,
a21x1 + a22x2 + · · · + a2nxn = b2,
...
am1x1 + am2x2 + · · · + amnxn = bm.
(3.7)
These equations can be expressed compactly in an array called the coeﬃcient
matrix, consisting of the coeﬃcients and constants as follows:
(aij | b) =
⎛
⎜
⎜
⎜
⎝
a11
a12
· · ·
a1n
b1
a21
a22
· · ·
a2n
b2
...
...
· · ·
...
...
am1
am2
· · ·
amn
bm
⎞
⎟
⎟
⎟
⎠.
(3.8)
The solution set of the system (3.7) is the set of all x = (x1, x2, . . . , xn)T ∈Fn
that satisfy each equation.
The procedure for ﬁnding the solution set called Gaussian elimination
is to use row operations to bring (aij | b) into reduced form. Since row
operations can be performed by premultiplication using elementary matrices,
it is natural to use the matrix equation form of the system, namely Ax = b,
where A = (aij) is the coeﬃcient matrix and b is the column of constants.
The key point is the following: for every elementary matrix E ∈Fm×m, the
matrix equations Ax = b and EAx = Eb are equivalent in the sense that
they have exactly the same solution sets. For if Ax = b, then EAx = Eb.
Conversely, if EAx = Eb, then multiplying by the elementary matrix F such
that FE = In recovers the original system; that is,
Ax = (FE)Ax = F(EAx) = F(Eb) = (FE)b = b.
Thus we have the following statement.

78
3
Matrices
Proposition 3.14. Let A ∈Fm×n and suppose B is a product of elementary
matrices such that BA = Ared. Then the linear system Ax = b is equivalent
to the reduced system Aredx = c, where c = Bb
3.3.2
Writing the solutions: the homogeneous case
We now describe how to express the solutions. For this, we need to distinguish
between two types of systems. A linear system of the form Ax = 0 is said to
be homogeneous, while a linear system Ax = b, where b ̸= 0, is said to be
inhomogeneous. The solution set of a homogeneous system Ax = 0 is called
the null space of A and denoted by N(A). The previous proposition shows
that N(A) = N(Ared). That is,
Ax = 0 if and only if Aredx = 0.
The coeﬃcient matrix of a homogeneous system Ax = 0 is simply deﬁned as
A. Thus, the coeﬃcient matrix will be of size m × n instead of m × (n + 1).
Let us now consider a homogeneous example.
Example 3.11. The homogeneous system
x1 + x2 + 2x3 + 0x4 + 3x5 −x6 = 0,
x4 + 2x5 + 0x6 = 0,
with coeﬃcients in Q has coeﬃcient matrix
A =
1
1
2
0
3
−1
0
0
0
1
2
0
	
.
Note that A is already reduced and has corners in the ﬁrst and fourth
columns. Thus we can solve for the corresponding corner variables x1 and
x4 in terms of the noncorner variables x2, x3, x5, x6. This gives
x1 = −2x3 −3x5 + x6,
x4 = −2x5.
Notice that x2 doesn’t appear in these equations, but it will appear in the
solution. The upshot is that the variables corresponding to A’s corner columns
are functions of the remaining variables, which we will call the free variables.
Since there are six variables, we form the vector
s = (−2x3 −3x5 + x6, x2, x3, −2x5, x5, x6)T ∈Q6,

3.3
Linear Systems
79
where we have replaced x1 and x4 in x by their expressions in terms of
the free variables. Thus s contains only expressions in the free variables
x2, x3, x5, x6. We call s the general solution vector. We now deﬁne basic null
vectors f1, f2, f3, f4 such that
s = x2f1 + x3f2 + x5f3 + x6f4.
(3.9)
The basic null vectors are found by inspection. To get f1, set x2 = 1 and
x3 = x5 = x6 = 0. Repeating this for the other fi, we see that
f1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, f2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−2
0
1
0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, f3 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−3
0
0
−2
1
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
, f4 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Then (3.9) shows that every solution of Ax = 0 (as a vector in Q6) is a linear
combination of the basic null vectors fi with coeﬃcients in Q. Moreover, the
coeﬃcients x2, x3, x5, x6 are unique, since each of them occurs by itself in one
of the components of s.
⊠
The method of the above example can be used to ﬁnd basic null vectors
of any homogeneous linear system Ax = 0. In fact, since Ared is unique, the
basic solutions as deﬁned are also unique. Note, however, that basic null
vectors exist only when the rank of A is less than n. The next result summa-
rizes how a linear system Ax = 0 is solved. The proof imitates the previous
example and will be omitted.
Proposition 3.15. Suppose A ∈Fm×n has rank k, and assume ℓ= n −k >
0. Then every x ∈N(A) can be expressed in exactly one way as a linear
combination of basic null vectors f1, . . . , fℓof A. If k = n, then Ax = 0 has
the unique solution x = 0 (hence there are no basic null vectors).
3.3.3
The inhomogeneous case
The solution in the inhomogeneous case is described in the next proposition.
We ﬁrst note that an inhomogeneous linear system needn’t have any solutions
at all. The equation 0x = 1 is such an example. More generally, if A is the
m × n zero matrix, then Ax = 0 for every x ∈Fn, so if b ̸= 0, then Ax = b
cannot have a solution. A system Ax = b with no solutions is said to be
inconsistent. To take a less obvious example of an inconsistent system, note
that the equation ax + by = c represents a line in R2 (assuming that a, b, c
are real). Thus the system

80
3
Matrices
ax + by = c
dx + ey = f
represents the points where two lines in R2 intersect. If the lines are distinct
parallel lines (e.g., a = d, b = e, c ̸= f), then they have empty intersection, so
the system is inconsistent. If the lines are distinct and nonparallel, they meet
in a unique point. There is another way in which a system can be inconsistent.
If it is overdetermined, that is, if there are more equations than variables, then
it may be inconsistent. For example, three lines in R2 which are mutually
nonparallel will meet only if the third passes through the unique intersection
point of the ﬁrst two. We now state the criterion for an inhomogeneous linear
system to be consistent.
Proposition 3.16. The inhomogeneous linear system Ax = b is consistent
if and only if the rank of its coeﬃcient matrix (A | b) is the same as the rank
of A. In particular, if A is of size m × n and has rank m, then the system
Ax = b is always consistent.
Proof. If the ranks are diﬀerent, then the rank of (A | b) is larger than
the rank of A. This implies that if BA = Ared, then the equivalent system
BAx = Aredx = Aredb contains an equation of the form 0x1 + · · · + 0xn =
c, where c ̸= 0. Such an equation is clearly inconsistent. Therefore, if the
inhomogeneous linear system is consistent, the ranks of A and (A | b) are
the same. We leave the argument for the converse to the reader. If A has rank
m, then so does (A | b), so Ax = b is consistent.
□
Example 3.12. The system
3x + 3y = 1
x −y = 2
x + 2y = 0
.
has coeﬃcient matrix
⎛
⎝
3
3
1
1
−1
2
1
2
0
⎞
⎠.
This matrix has rank three, so the system is inconsistent.
⊠
Let us now summarize our discussion.
Proposition 3.17. Suppose that A ∈Fm×n has rank k, and consider an
inhomogeneous linear system Ax = b. This system is consistent if and only
if the rank of (A | b) is the same as the rank of A. Assume so and let p0 ∈Fn
be a particular solution. Then every solution can be written in the form
x = p0 + w, where w ∈N(A). If k = n, then N(A) = {0}, and consequently

3.3
Linear Systems
81
p0 is the unique solution. If k < n, let ℓ= n −k, and suppose f1, . . . , fℓare
A’s basic null vectors. Thus, every x ∈Fn of the form
x = p0 +
ℓ

i=1
aifi
(all ai ∈F)
(3.10)
is a solution, and every solution can be written in the form (3.10) for a unique
choice of the ai.
Proof. Suppose x satisﬁes Ax = b. Then
A(x −p0) = Ax −Ap0 = b −b = 0.
Thus, w = x −p0 ∈N(A), so x = p0 + w, as claimed. If k = n, we know
from Proposition 3.15 that N(A) = {0}, so w = 0. Thus p0 is the unique
solution. Now suppose k < n. There exist fundamental solutions f1, . . . , fℓ
with ℓ= n −k, and w can be uniquely written w = ℓ
i=1 aifi. This implies
the ﬁnal assertion.
□
Example 3.13. To illustrate this result, consider the inhomogeneous linear
system
x1 + x2 + 2x3 + 0x4 + 3x5 −x6 = 1,
x4 + 2x5 + 0x6 = −1.
Note that the coeﬃcient matrix A is taken from Example 3.11, so solving as
above, we see that
x1 = 1 −2x3 −3x5 + x6,
x4 = −1 −2x5.
Therefore, p0 = (1, 0, 0, −1, 0, 0)T is a particular solution of the inhomoge-
neous linear system. Hence, every solution has the form p0 + s, where s is
the general solution vector of the homogeneous linear system Ax = 0.
⊠
Remark. Notice that if A ∈Fm×n, then N(A) is an abelian group. In fact,
it is a subgroup of Fn. Proposition 3.17 says that the set of solutions of a
consistent linear system Ax = b is a coset p0 + N(A) of N(A). The coset
representative p0 is a particular solution.
Remark. Our ﬁnal remark is that Ax = b is consistent if and only if b is a
linear combination of the columns of A. This follows from the identity (3.5).
We will also discuss this fact in more detail when the column space of a
matrix is introduced.

82
3
Matrices
3.3.4
A useful identity
Suppose A ∈Fm×n. Since each corner of Ared occupies a unique row and a
unique column, the rank k of A satisﬁes both k ≤m and k ≤n. If k < n,
then there exists at least one free variable, so A will always have a basic null
vector. There is a simple but important relationship, already pointed out,
between the rank k and the number of basic null vectors, or equivalently, the
number of free variables:
rank(A) + # free variables = n.
(3.11)
We will restate this identity several times, each time in a more general form.
In its most general form, the identity (3.11) is the rank–nullity theorem. For
example, if m < n, then there exists at least one free variable. In particular,
a system Ax = b cannot have a unique solution.
The identity (3.11) has the following useful consequence for n × n matrices.
Proposition 3.18. Let A ∈Fn×n. Then A is nonsingular if and only if
N(A) = {0}.
Proof. Suppose A is nonsingular. By deﬁnition, A has rank n, so Ared = In.
This implies N(A) = N(Ared) = {0}. Conversely, if N(A) = {0}, then Ax =
0 implies x = 0. Hence there cannot be any free variables, so by the identity
(3.11), the rank of A is n. Thus A is nonsingular.
□
Remark. In the next chapter, we will develop some techniques for bringing
a matrix into reduced row echelon form that are useful for solving linear
systems.
Exercises
Exercises 3.3.1. Show that the null space of a matrix over a ﬁeld is an
abelian group.
Exercises 3.3.2. Let C denote the counting matrix of Example 3.8. Find
an equation in a, b, c that determines when the system C
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
a
b
c
⎞
⎠is
consistent. (Hint: row reduce.)
Exercises 3.3.3. The ﬁeld is F2. Consider the following matrices:
A1 =
⎛
⎝
1
1
0
1
0
1
0
1
1
⎞
⎠,
A2 =
⎛
⎜
⎜
⎝
1
0
1
0
0
1
1
0
1
1
0
0
1
0
0
1
⎞
⎟
⎟
⎠.
Find basic null vectors for both A1 and A2.

3.3
Linear Systems
83
Exercises 3.3.4. Which of the following can happen and which cannot?
Provide explanations.
(i) A is of size 15 × 24 and has exactly seven basic null vectors;
(ii) A is of size 3 × 3, and Ax = b is consistent for all b.
(iii) A is of size 12 × 12, three of A’s columns are zero, and A has 10 basic
null vectors.

Chapter 4
Matrix Inverses, Matrix Groups
and the LPDU Decomposition
In this chapter we continue our introduction to matrix theory beginning
with the notion of a matrix inverse and the deﬁnition of a matrix group. For
now, the main example of a matrix group is the group GL(n, F) of invertible
n × n matrices over a ﬁeld F and its subgroups. We will also show that every
matrix A ∈Fn×n can be factored as a product LPDU , where each of L, P,
D, and U is a matrix in an explicit subset of Fn×n. For example, P is a
partial permutation matrix, D is diagonal, and L and U are lower and upper
triangular respectively. The expression A = LPDU tells us a lot about A: for
example, D contains the pivots dii of A and also tells us the rank of A by
counting the number of nonzero pivots. In addition, A’s determinant (which
will be deﬁned in the next chapter) is determined by PD as ±d11 · · · dnn.
When A is invertible, each of L, P, D, and U lies in a certain subgroup of
GL(n, F). For example, P lies in the group of n × n permutation matrices,
which is a matrix group isomorphic to S(n).
4.1
The Inverse of a Square Matrix
We now come to the interesting question of when an n × n matrix over a ﬁeld
F has an inverse under matrix multiplication. Since every nonzero element of
a ﬁeld has an inverse, the question for 1 × 1 matrices is already settled. The
main fact turns out to be that an n × n matrix over F is invertible if and
only if its rank is n.
4.1.1
The deﬁnition of the inverse
We begin with an essential deﬁnition:
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 4
85

86
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Deﬁnition 4.1. Let F be a ﬁeld and suppose A ∈Fn×n. We say that A is
invertible if there exists B ∈Fn×n such that AB = BA = In. We will say that
B is an inverse of A, and likewise, that A is an inverse of B.
Example 4.1. All 2 × 2 elementary matrices are invertible. Their inverses
are given as follows:
E1 =

0
1
1
0

⇒E−1
1
=

0
1
1
0

,
E2 =

r
0
0
1

⇒E−1
2
=

r−1
0
0
1

,
and
E3 =

1
s
0
1

⇒E−1
3
=

1
−s
0
1

.
⊠
In fact, we already noted that if E ∈Fn×n is an elementary matrix and F
is the elementary matrix that reverses the row operation that E eﬀects, then
FE = EF = In. In other words, doing a row operation and then undoing it
produces the same result as ﬁrst undoing it and then doing it, and the result
is that nothing changes. Thus every elementary matrix is invertible. Note
that the deﬁnition can apply only to square matrices, but if A is square, we
can consider both left and right inverses of A. A left inverse of A ∈Fn×n is
a matrix B ∈Fn×n such that BA = In, and a right inverse of A is a matrix
B ∈Fn×n such that AB = In. It is useful to note (as was already noted for
groups) that when an inverse exists, it is unique. This is one of the nice
consequences of the associativity of matrix multiplication we have previously
mentioned.
Proposition 4.1. An invertible n × n matrix has a unique inverse.
Proof. Suppose A ∈Fn×n has two inverses B and C. Then B = BIn =
B(AC) = (BA)C = InC = C. Thus B = C.
□
4.1.2
Results on Inverses
Elementary matrices are the key to understanding when inverses exist and
how to ﬁnd them. As we already noted above, if E ∈Fn×n is elementary,
then there exists an elementary matrix F ∈Fn×n such that FE = EF = In.
Thus elementary matrices are invertible. In fact, the inverse of an elementary
matrix has the same type. Our ﬁrst result about inverses is that a nonsingular

4.1
The Inverse of a Square Matrix
87
matrix (that is, an element of Fn×n of rank n) is invertible. Recall that by
Proposition 3.13, A ∈Fn×n is nonsingular if and only if there exist elementary
matrices E1, . . . , Ek ∈Fn×n such that Ek · · · E1A = In. In particular, if A is
nonsingular, we can assert that A has a left inverse.
Proposition 4.2. Let A in Fn×n. If A is nonsingular, then A is invertible.
In fact, let E1, . . . , Ek ∈Fn×n be elementary matrices such that Ek · · · E1A =
In, and let F1, . . . , Fk ∈Fn×n denote the elementary matrices such that
FiEi = In. Then A = F1 · · · Fk and A−1 = Ek · · · E1. Consequently, if A is
nonsingular, then A−1 is also nonsingular.
Proof. Let B = Ek · · · E1 and C = F1 · · · Fk. Then by associativity of multi-
plication,
CB = (F1 · · · Fk)(Ek · · · E1) = In,
since FiEi = In for each i. Now BA = In by assumption, so
A = InA = (CB)A = C(BA) = CIn = C.
Thus A = F1 · · · Fk, as claimed. Since CB = In and A = C, we have AB = In,
which proves that A is invertible. We leave it to the reader to show that if A
is nonsingular, then so is A−1.
□
Therefore, nonsingular matrices are invertible. In fact, the converse is also
true, which will be important in our discussion of matrix groups.
Theorem 4.3. Let A ∈Fn×n. Then A is nonsingular if and only if A is
invertible if and only if A is a product of elementary matrices.
Proof. By Proposition 4.2, a nonsingular matrix is invertible. To show that
an invertible matrix is nonsingular, let A be invertible, and suppose BA = In.
Now suppose Ax = 0. Then x = (BA)x = B(Ax) = B0 = 0, so by Proposi-
tion 3.18, it follows that A is nonsingular. If A is nonsingular, then by Propo-
sition 4.2, A is a product of elementary matrices. Conversely, a product of
elementary matrices is invertible, so the proof is ﬁnished.
□
Notice that all that is required in the proof of the above theorem is that A
have a left inverse B. To expand on this observation, we will prove one last
result.
Proposition 4.4. Let A ∈Fn×n. Then if A has either a left inverse B (that
is, BA = In) or a right inverse C (that is, AC = In), then A is invertible.
Proof. Suppose A has a left inverse B. Arguing as in the proof of Theorem
4.3, we conclude that Ax = 0 implies x = 0, so A is nonsingular. Therefore,
A is invertible. Now suppose A has a right inverse C. Then C has a left
inverse A, so C is invertible. Thus, A is invertible too.
□

88
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Consequently, the invertible n × n matrices over F are exactly those for
which there is either a left or right inverse. This means that in an actual
calculation of A−1, it is necessary only to ﬁnd a B such that either AB or
BA is In. One ﬁnal result is the following.
Proposition 4.5. If A, B ∈Fn×n are both invertible, then so is AB. More-
over, (AB)−1 = B−1A−1. In addition, AT is also invertible, and (AT )−1 =
(A−1)T .
Proof. Both A and B are products of elementary matrices, so it follows that
AB is too. Therefore AB is also invertible. We leave it to the reader to check
the formula for (AB)−1. The assertion about AT is left as an exercise.
□
4.1.3
Computing inverses
We now consider the problem of actually inverting an n × n matrix A. One
method is the row reduction procedure used in Example 3.10. This starts
with the n × 2n matrix (A | In) and applies row reduction until the matrix
(In | B) is obtained. Then BA = In, so B = A−1 by Proposition 4.4. Oth-
erwise, A is singular and A−1 doesn’t exist. Alternatively, one can also ﬁnd
B by multiplying out a sequence of elementary matrices that row reduces A.
This is actually not as bad as it sounds, since multiplying elementary matri-
ces is elementary. In the following example, we will assume that the ﬁeld is
F2. Not surprisingly, this assumption makes the calculations a lot easier.
Example 4.2. Suppose the ﬁeld is F2. Let us ﬁnd the inverse of
A =
⎛
⎝
1
1
0
1
1
1
0
1
1
⎞
⎠,
if one exists. Now,
(A | I3) =
⎛
⎝
1
1
0
1
0
0
1
1
1
0
1
0
0
1
1
0
0
1
⎞
⎠→
⎛
⎝
1
1
0
1
0
0
0
0
1
1
1
0
0
1
1
0
0
1
⎞
⎠→
⎛
⎝
1
1
0
1
0
0
0
1
1
0
0
1
0
0
1
1
1
0
⎞
⎠→
⎛
⎝
1
1
0
1
0
0
0
1
0
1
1
1
0
0
1
1
1
0
⎞
⎠→
⎛
⎝
1
0
0
0
1
1
0
1
0
1
1
1
0
0
1
1
1
0
⎞
⎠.

4.1
The Inverse of a Square Matrix
89
Hence
A−1 =
⎛
⎝
0
1
1
1
1
1
1
1
0
⎞
⎠.
⊠
Example 4.3. For a slightly less simple example, let F = F2, but put
A =
⎛
⎜
⎜
⎝
1
0
0
1
1
1
0
0
0
1
1
1
1
1
1
1
⎞
⎟
⎟
⎠.
Following the above procedure, we obtain that
A−1 =
⎛
⎜
⎜
⎝
0
0
1
1
0
1
1
0
1
1
1
1
1
0
1
1
⎞
⎟
⎟
⎠.
Note that the correctness of this result should be checked by computing
directly that
I4 =
⎛
⎜
⎜
⎝
0
0
1
1
0
1
1
1
1
1
1
0
1
0
1
1
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
1
0
0
1
1
1
0
0
0
1
1
1
1
1
1
1
⎞
⎟
⎟
⎠.
⊠
There is a slightly less obvious third technique for inverting a matrix. If
A is of size n × n and we form the matrix (A | x), where x represents a
variable column vector with components x1, x2, . . . , xn, then row reducing
will produce a result of the form (Ared | c), where the components of c
are certain linear combinations of the components of x. The coeﬃcients in
these linear combinations turn out to be the entries of the matrix B such
that BA = Ared. Here is an example.
Example 4.4. Let the ﬁeld be Q and
A =
⎛
⎝
1
2
0
1
3
1
0
1
2
⎞
⎠.

90
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Now form
⎛
⎝
1
2
0
a
1
3
1
b
0
1
2
c
⎞
⎠
and row reduce. The result is
⎛
⎝
1
0
0
5a −4b + 2c
0
1
0
−2a + 2b −c
0
0
1
a −b + c
⎞
⎠.
Thus,
A−1 =
⎛
⎝
5
−4
2
−2
2
−1
1
−1
1
⎞
⎠.
⊠
Exercises
Exercise 4.1.1. Find the inverse of each of the following matrices over Q,
or show that the inverse does not exist:
(a)
⎛
⎝
1
0
1
0
1
−1
1
1
0
⎞
⎠; (b)
⎛
⎝
1
0
−2
0
1
1
1
1
0
⎞
⎠; (c)
⎛
⎜
⎜
⎝
1
0
1
0
0
1
0
−1
1
0
−1
0
0
1
0
1
⎞
⎟
⎟
⎠.
Exercise 4.1.2. If possible, invert
B =
⎛
⎜
⎜
⎝
1
2
−1
−1
−2
−1
3
1
−1
4
3
−1
0
3
1
−1
⎞
⎟
⎟
⎠.
Exercise 4.1.3. We saw that the 3 × 3 counting matrix is singular. Deter-
mine whether the 2 × 2 and 4 × 4 counting matrices are nonsingular.
Exercise 4.1.4. Consider the matrix of Exercise 4.1.2 as a matrix A over
F5. If possible, ﬁnd A−1.
Exercise 4.1.5. The following matrices are over F2. Determine which have
inverses and ﬁnd the inverses when they exist.
(a)
⎛
⎝
1
0
1
0
1
1
1
1
0
⎞
⎠; (b)
⎛
⎜
⎜
⎝
1
0
0
1
1
1
0
0
0
1
1
1
1
1
1
1
⎞
⎟
⎟
⎠; (c)
⎛
⎜
⎜
⎝
1
0
0
1
1
1
0
0
0
1
0
1
1
1
1
1
⎞
⎟
⎟
⎠.

4.1
The Inverse of a Square Matrix
91
Exercise 4.1.6. If possible, invert the following 5 × 5 matrix A over F2:
A =
⎛
⎜
⎜
⎜
⎜
⎝
1
0
1
1
1
1
1
0
1
0
1
1
0
0
1
1
0
1
1
0
1
1
1
0
1
⎞
⎟
⎟
⎟
⎟
⎠
.
Exercise 4.1.7. Suppose A =

 a b
c d

, and assume that Δ = ad −bc ̸= 0.
Show that A−1 = 1
Δ

 d
−b
−c a

. What does the condition Δ ̸= 0 mean in terms
of the rows of A?
Exercise 4.1.8. Verify that if A is invertible, then AT is also invertible.
Find a formula for (AT )−1 and verify it.
Exercise 4.1.9. Suppose A is invertible.
(i) Show that A−1 is also invertible.
(ii) Find a formula for the inverse of Am, where m is a positive integer.
(iii) True or false: A + AT is invertible. Include brief reasoning.
Exercise 4.1.10. True or false: If A is square and AAT is invertible, then
A is invertible. What if A isn’t square?
Exercise 4.1.11. True or false: Suppose A is of size n × n and A3 + 2A −
In = O. Then A invertible. Include brief reasoning.
Exercise 4.1.12. True or false: if an n × n matrix with integer entries has
an inverse, then the inverse also has integer entries. As usual, include brief
reasoning.
Exercise 4.1.13. Let C =

 1 a b
0 1 c
0 0 1

. Find a general formula for C−1.
Exercise 4.1.14. Show that if A and B are of size n × n and have inverses,
then (AB)−1 = B−1A−1. What is (ABCD)−1 if all four matrices are invert-
ible?
Exercise 4.1.15. Suppose A is an invertible m × m matrix and B is an
m × n matrix. Solve the equation AX = B.
Exercise 4.1.16. Suppose A and B are both of size n × n and AB is invert-
ible. Show that both A and B are invertible.
Exercise 4.1.17. Let A and B be two n × n matrices over R. Suppose A3 =
B3 and A2B = B2A. Show that if A2 + B2 is invertible, then A = B. (Hint:
consider (A2 + B2)A.)

92
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Exercise 4.1.18. Let A and B be n × n matrices over F.
(i) Show that if the inverse of A2 is B, then the inverse of A is AB.
(ii) Suppose A, B, and A + B are all invertible. Find the inverse of A−1 +
B−1 in terms of A, B, and A + B.
Exercise 4.1.19. You are a cryptographer assigned to crack the clever
cipher constructed as follows. First, let the sequence 01 represent A, 02 repre-
sent B, and so forth up to 26, which represents Z. For clarity, a space between
words is indicated by inserting 00. A text can thus be unambiguously encoded
as a sequence. For example, 1908040002090700041507 is the encoding of the
phrase “the big dog.” Since this string of integers has length 22, we will
think of it as a vector in Q22. Suppose a text has been encoded as a sequence
of length 14,212. Now, 14,212 = 44 × 323, so the sequence can be broken
into 323 consecutive intervals of length 44. Next, suppose each subinterval is
transposed and multiplied on the left by a single 44 × 44 matrix C. The new
sequence obtained by transposing again and laying the products end to end
will be the enciphered message, and it is your job to decipher it. Discuss the
following questions.
(i) How does one produce an invertible 44 × 44 matrix in an eﬃcient way,
and how does one ﬁnd its inverse?
(ii) How many of the subintervals will you need to decipher to break the
whole cipher by deducing the matrix C?

4.2
Matrix Groups
93
4.2
Matrix Groups
The purpose of this section is to introduce the concept of a matrix group and
produce a number of examples. A matrix group is essentially a collection of
matrices that lie in some Fn×n that forms a group under matrix multiplica-
tion, where the identity is In. Thus the axioms deﬁning a matrix group are
modest, but they are enough to ensure that matrix groups form an interest-
ing central part of algebra. In the ﬁnal chapter, we will outline the theory
of the structure of matrix groups that are deﬁned by some equations. These
groups are known as linear algebraic groups. The matrix group structure will
be useful in stating and proving some of the main results on matrices in later
chapters.
4.2.1
The deﬁnition of a matrix group
We begin with the main deﬁnition.
Deﬁnition 4.2. Let F be a ﬁeld. A subset G of Fn×n is called a matrix group
if it satisﬁes the following three conditions:
(i) if A, B ∈G, then AB ∈G (that is, G is closed under multiplication);
(ii) In ∈G; and
(iii) if A ∈G, then A is invertible, and A−1 ∈G.
Remark. Since matrix multiplication is associative, every matrix group G
is a group under matrix multiplication with identity the identity matrix, and
the inverse of every A ∈G given by A−1.
The most basic example of a matrix group G ⊂Fn×n is the general linear
group over F, which is denoted by GL(n, F). By deﬁnition,
GL(n, F) = {A ∈Fn×n | A−1 exists}.
(4.1)
Thus, GL(n, F) is the set of all invertible elements of Fn×n.
Proposition 4.6. The set GL(n, F) is a matrix group. Moreover, every
matrix group G ⊂Fn×n is a subgroup of GL(n, F).
Proof. By Proposition 4.5, GL(n, F) is closed under multiplication. Moreover,
In ∈GL(n, F). Finally, if A is invertible, so is A−1. Therefore, GL(n, F) is a
matrix group.
□
A subgroup of a matrix group is called a matrix subgroup. For example,
{In} is a subgroup of every matrix group G ⊂GL(n, F). One usually calls

94
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
{In} the trivial subgroup. The other basic deﬁnitions that apply to groups
such as normal subgroups, cosets, order (in the ﬁnite group case) all apply
without change to matrix groups.
4.2.2
Examples of matrix groups
We now give some examples of matrix groups.
Example 4.5 (Rotations). The ﬁrst example is the group Rot(2) consisting
of the 2 × 2 rotation matrices Rθ, 0 ≤θ ≤2π. As we mentioned in Example
3.7, the mapping Rθ : R2 →R2 deﬁned by
Rθ
x
y

=
x cos θ −y sin θ
x sin θ + y cos θ

is the rotation of R2 through θ. Note that Rθ is the matrix mapping associated
to the matrix
Rθ =
cos θ
−sin θ
sin θ
cos θ

,
(4.2)
which we will call a rotation matrix. Recall that in Example 3.7 we showed
that Rθ

x
y

is the same action on R2 as multiplication by the complex
exponential eiθ is on C. That is,
Rθ
x
y

= eiθz,
where z = x + yi. Since ei(θ+ψ) = eiθeiψ, it follows that Rθ+ψ = RθRψ for all
θ, ψ. Consequently, Rot(2) is closed under multiplication. It is also closed
under taking inverses. For using the formula for the inverse of a 2 × 2 matrix
in Exercise 4.1.7, we have
(Rθ)−1 =
 cos θ
sin θ
−sin θ
cos θ

=
cos(−θ)
−sin(−θ)
sin(−θ)
cos(−θ)

= R−θ.
Finally, note that I2 = R0. Hence, Rot(2) is a matrix group. In addition,
Rot(2) is abelian. For
RθRψ =
cos θ cos ψ −sin θ sin ψ
−cos θ sin ψ −sin θ cos ψ
cos θ sin ψ + sin θ cos ψ
cos θ cos ψ −sin θ sin ψ

= RψRθ,

4.2
Matrix Groups
95
as can be seen by noticing that each term in the matrix stays the same after
ﬂipping θ and ψ. This also follows from the observation that eiθeiψ = eiθeiθ,
since C∗is an abelian group. This is worth noting, since among other reasons,
abelian matrix groups are rather rare. Another fact to notice is that there
is an explicit group isomorphism φ : S1 →Rot(2), which is deﬁned by ﬁrst
setting z = eiθ and then putting φ(z) = Rθ. We leave it as an exercise to show
that φ is, as claimed, a well-deﬁned isomorphism.
⊠
The next example of a matrix group plays a fundamental role in the Euclid-
ean geometry of Rn.
Deﬁnition 4.3 (The orthogonal group). Let Q ∈Rn×n. We say that Q is
orthogonal if QT Q = In (i.e., Q−1 = QT ). The set of all n × n orthogonal
matrices is called the orthogonal group of degree n. The orthogonal group is
denoted by O(n, R).
Thus, Q ∈Rn×n is orthogonal if and only if when Q = (q1 q2 · · · qn),
then qT
i qj = δij for all 1 ≤i, j ≤n.
Proposition 4.7. O(n, R) is a subgroup of GL(n, R).
Proof. We ﬁrst show that if Q, R ∈O(n, R), then QR ∈O(n, R). We have to
check that (QR)T (QR) = In. But
(QR)T (QR) = (RT QT )(QR) = RT (QT Q)R = RT InR = In,
so O(n, R) is closed under multiplication. Clearly In ∈O(n, R), so it remains
to verify that Q ∈O(n, R) implies Q−1 ∈O(n, R). But Q−1 = QT , so we
have to check that (QT )T QT = In. This amounts to showing that QQT = In,
which
holds
due
to
the
fact
that
QT Q = In.
This
completes
the
veriﬁcation.
□
The alert reader may have noticed that the deﬁnition of the orthogonal
group O(n, R) actually had nothing to do with R. That is, we could just as
easily deﬁned O(n, F) in exactly the same way for any ﬁeld F. In fact, the
matrix groups of the type O(n, F) form an important class known as the
orthogonal groups over F. One frequently concentrates on O(n, R) because
its properties are related to the geometry of n-dimensional Euclidean space.
In the following section, we will give an example of a matrix group closely
related to the symmetric group.
4.2.3
The group of permutation matrices
Recall that ei is the column vector such that In = (e1 · · · en). If σ ∈S(n),
put Pσ = (eσ(1) · · · eσ(n)). Thus,

96
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Pσek = eσ(k).
Matrices of the form Pσ are called n × n permutation matrices. Let P(n)
denote the set of all n × n permutation matrices. Note that diﬀerent permu-
tations in S(n) give rise to diﬀerent permutation matrices. That is, if σ ̸= μ,
then Pσ ̸= Pμ. Thus there are exactly n! n × n permutation matrices.
Let us ﬁrst show that row-swap matrices are the permutation matrices
corresponding to transpositions. Suppose S is the n × n row-swap matrix
that interchanges rows i and j, where i < j. Let τ ∈S(n) be the transposition
interchanging i and j. Thus τ(i) = j, τ(j) = i, and τ(k) = k for k ̸= i, j. Then
Sei = ej = eτ(i) and Sej = ei = eτ(j). Also, for k ̸= i, j, Sek = ek. Therefore,
S = Pτ, as asserted. Now we want to study the relationship between the
permutation matrices and S(n). The key fact is contained in the following
proposition.
Proposition 4.8. Let σ, τ ∈S(n). Then we have
PτPσ = Pτσ.
Therefore, P(n) is closed under multiplication. Moreover, In and (Pσ)−1 are
permutation matrices for all σ ∈S(n). Consequently, P(n) is a matrix group.
Proof. For each index k, 1 ≤k ≤n, we have
PτPσek = Pτeσ(k) = eτ(σ(k)) = Pτσek.
Hence PτPσ = Pτσ, so P(n) is closed under multiplication. It also follows
from this formula that (Pσ)−1 = Pτ, where τ = σ−1, so every P ∈P(n) has
its inverse in P(n). Since Pidn = In, where idn is the identity, it follows that
P(n) is a matrix group.
□
Now let ϕ : S(n) →P(n) be the mapping deﬁned by ϕ(σ) = Pσ.
Proposition 4.9. ϕ is an isomorphism.
Proof. We already noted that ϕ is injective. Since S(n) and P(n) both have
order n!, it follows that ϕ is a bijection. Therefore, ϕ is an isomorphism. □
Every permutation matrix P can be put into reduced row echelon form.
But it is clear that type II row operations suﬃce for this, so P can be written
as a product of row swaps. This implies the following result.
Corollary 4.10. Every σ ∈S(n) can be written as a product of transpo-
sitions. These transpositions may be found by putting Pσ into reduced row
echelon form using row swaps.

4.2
Matrix Groups
97
Note, however, that there are in general many ways of writing a permuta-
tion matrix as a product of row swaps, and correspondingly, there are many
ways of writing a permutation as a product of transpositions. In fact, one
can always represent a permutation matrix as a product of row swaps that
interchange adjacent rows. These correspond to transpositions of the form
(i i + 1) and are called simple transpositions. The simple transpositions are
fundamental in the study of the combinatorial properties of S(n). The upshot
is the following.
Proposition 4.11. Every σ ∈S(n) can be expressed as a product of simple
transpositions.
Permutation matrices have the following beautiful property.
Proposition 4.12. If P ∈P(n), then P −1 = P T . In other words, every per-
mutation matrix is orthogonal.
We will leave the proof as an exercise. Thus, P(n) is a subgroup of O(n, R).
This gives an interesting example of an inﬁnite group, namely O(n, R), con-
taining a ﬁnite subgroup, which generalizes the example {±1} ⊂O(1, R).
Example 4.6. For instance, P(2) consists of two matrices, I2 and
S =
0
1
1
0

,
while P(3) consists of the following six 3 × 3 matrices;
I3,
⎛
⎝
1
0
0
0
0
1
0
1
0
⎞
⎠,
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠,
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠,
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠,
⎛
⎝
0
0
1
0
1
0
1
0
0
⎞
⎠.
The second, third, and sixth matrices are row swaps, and the others are
products of two row swaps.
⊠
Exercises
Exercise 4.2.1. Find the center of GL(n, F) for an arbitrary ﬁeld F.
Exercise 4.2.2. Find the center of O(n, F) for an arbitrary ﬁeld F.
Exercise 4.2.3. Show that Rot(2) is a subgroup of O(2, R). Is O(2, R) =
Rot(2)? If not, ﬁnd an element of O(2, R) that isn’t a rotation.
Exercise 4.2.4. Let G ⊂GL(2, R) denote the set of all matrices

a
b
−b
a

,
where a2 + b2 ̸= 0. Is G an abelian subgroup of GL(2, R)?

98
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Exercise 4.2.5. Show that Q ∈Rn×n is orthogonal if and only if its rows
q1, . . . , qn satisfy the condition qiqT
j = δij. That is, qiqT
i = 1, while qiqT
j =
0 if i ̸= j.
Exercise 4.2.6. Show that if Q ∈Rn×n is orthogonal, then so is QT . Con-
clude that a symmetric orthogonal matrix satisﬁes Q2 = In; hence Q is its
own inverse. Give an example of a 2 × 2 symmetric orthogonal matrix diﬀer-
ent from I2.
Exercise 4.2.7. Without computing, try to guess the inverse of the matrix
A =
⎛
⎜
⎜
⎝
1
0
1
0
0
1
0
−1
1
0
−1
0
0
1
0
1
⎞
⎟
⎟
⎠.
(Hint: consider the columns.)
Exercise 4.2.8. Let S1 =
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠and S2 =
⎛
⎝
1
0
0
0
0
1
0
1
0
⎞
⎠. Show that
every 3 × 3 permutation matrix is a product involving only S1 and S2.
Exercise 4.2.9. Let S1 and S2 be the permutation matrices deﬁned in Exer-
cise 4.2.8. Show that (S1S2)3 = I3.
Exercise 4.2.10. Show that every permutation matrix P satisﬁes the iden-
tity P −1 = P T , and conclude that P(n) is a subgroup of O(n, R).
Exercise 4.2.11. Show that the following two matrices are permutation
matrices and ﬁnd their inverses:
⎛
⎜
⎜
⎜
⎜
⎝
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
1
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
,
⎛
⎜
⎜
⎜
⎜
⎝
0
0
1
0
0
0
0
0
0
1
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
⎞
⎟
⎟
⎟
⎟
⎠
.
Exercise 4.2.12. A signed permutation matrix is a square matrix Q of the
form
Q = P(±e1 ± e2 · · · ± en),
where P ∈P(n).
(i) Show that the set SP(n) of n × n signed permutation matrices is a sub-
group of O(n, R) of order 2nn!.

4.2
Matrix Groups
99
(ii) Prove that P(n) is a normal subgroup of SP(n).
(iii) Describe the quotient group SP(n)/P(n).
Exercise 4.2.13. Prove that every ﬁnite group G of order n is isomorphic
to a subgroup of O(n, R).

100
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
4.3
The LPDU Factorization
Recall that an invertible n × n matrix A can be expressed as a product of
elementary n × n matrices. In this section, we will prove a much more explicit
result: every n × n matrix A over a ﬁeld F can be expressed in the form
A = LPDU, where each of the matrices L, P, D, and U is built up from a
single type of elementary matrix. This LPDU factorization is one of the most
basic tools in the theory of matrices. One nice application is a well-known
but nontrivial relationship between the ranks of A and AT for any matrix A,
which we prove below. The LPDU decomposition is widely used in applied
linear algebra for solving large systems of linear equations.
4.3.1
The basic ingredients: L, P , D, and U
The list of characters in the LPDU decomposition consists of matrices that
we have met before and matrices that we need to introduce. Let us begin
with L and U.
Deﬁnition 4.4. An n × n matrix L is called lower triangular if lij = 0 for
all j > i. That is, the nonzero entries of L are below or on the diagonal of
L. Similarly, an n × n matrix U is upper triangular if uij = 0 for i > j. A
matrix that is either lower or upper triangular is said to be unipotent if all
its diagonal entries are 1.
Clearly, the transpose of a lower triangular matrix is upper triangular.
The transpose of a unipotent matrix is also unipotent. In our discussion, the
matrices L and U will always be lower and upper triangular and both will be
unipotent.
Example 4.7. A lower triangular 3 × 3 unipotent matrix has the form
L =
⎛
⎝
1
0
0
a
1
0
b
c
1
⎞
⎠.
The transpose U of L is, of course,
U =
⎛
⎝
1
a
b
0
1
c
0
0
1
⎞
⎠,
which is upper triangular. One can easily check that

4.3
The LPDU Factorization
101
L−1 =
⎛
⎝
1
0
0
−a
1
0
ac −b
−c
1
⎞
⎠.
Thus L−1 is also lower triangular unipotent.
⊠
Recall that type III row operations are also called transvections. When a
lower row is replaced by itself plus a multiple of some higher row, a transvec-
tion is said to be
downward. Downward transvections are performed via
left multiplication by lower triangular unipotent matrices. Similarly, right-
ward transvections are performed by right multiplication by upper triangular
matrices. Here is a basic fact.
Proposition 4.13. Let Ln(F) and Un(F) denote respectively the set of all
lower triangular unipotent and upper triangular unipotent n × n matrices
over F. Then Ln(F) and Un(F) are matrix subgroups of GL(n, F) for all
n > 0.
Proof. It follows from the deﬁnition of matrix multiplication that the product
of two lower triangular matrices is also lower triangular. If A and B are lower
triangular unipotent, then the diagonal entries of AB are also all 1. Indeed,
if AB = (cij), then
cii =
n

k=1
aikbki = aiibii = 1,
since aij = bij = 0 if i < j. The identity In is also lower triangular unipotent,
so to show that Ln(F) is a subgroup of GL(n, F), it remains to show that the
inverse of an element A of Ln(F) is also in Ln(F). But this follows from the
explicit technique for inverting a matrix using row operations. Row swaps
are never needed, since A is already lower triangular. Row dilations are never
needed, since A is already unipotent. Thus, A−1 is obtained by a sequence of
downward transvections. But these correspond to taking products in Ln(F),
so A−1 ∈Ln(F). The result for Un(F) is proved in an analogous way. In fact,
one can simply transpose the above proof.
□
Continuing with the introduction of the basic ingredients, we now concen-
trate on P and D. We next describe P.
Deﬁnition 4.5. A partial permutation matrix is a matrix that is obtained
from a permutation matrix P by setting some of the rows of P equal to zero.
Let Πn denote the set of n × n partial permutation matrix matrices. To
be explicit, every element of Πn is either a permutation matrix or obtained
from a permutation matrix P by replacing some of the ones in P by zeros.
A matrix with a row of zeros is, of course, singular, so Πn is not a matrix
group. Nevertheless, it remains true that the product of two n × n partial
permutation matrices is also a partial permutation matrix.
Lastly, we describe D.

102
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Deﬁnition 4.6. A diagonal matrix is a square matrix D = (dij) all of whose
oﬀ-diagonal entries are zero; that is, dij = 0 for all i ̸= j.
Since a diagonal matrix D is invertible if and only if its diagonal entries
dii are all diﬀerent from 0 and the product of two diagonal matrices is also
diagonal, we obtain the following result.
Proposition 4.14. The set Dn(F) of all n × n invertible diagonal matrices
over F is a matrix subgroup of GL(n, F).
4.3.2
The main result
We now derive the LPDU decomposition.
Theorem 4.15. Every n × n matrix A over a ﬁeld F can be expressed in the
form A = LPDU, where L ∈Ln(F), P ∈Πn, D ∈Dn(F), and U ∈Un(F).
The partial permutation matrix P is always unique. If A is invertible, then
P is a full permutation matrix, and in that case, P and D are both unique.
This result can be expressed in the form of a product of sets, three of
which are matrix groups:
Fn×n = Ln(F) · Πn · Dn(F) · Un(F).
Thus every n × n matrix over F is the product of the four types of matrices
Ln(F), Πn, Dn(F), and Un(F). Specializing to invertible matrices, we have
GL(n, F) = Ln(F) · P(n) · Dn(F) · Un(F).
In particular, GL(n, F) is the product of four of its subgroups: Ln(F), P(n),
Dn(F), and Un(F). This is a fundamental result in the theory of matrix groups.
Remark. It is also possible to deﬁne an LPDU decomposition for nonsquare
matrices. If A ∈Fm×n, where say m < n, then we can augment A by adding
n −m rows of zeros to make A an n × n matrix. Of course, in this situation,
the last n −m rows of P will also be zero.
The proof of the theorem is in the same spirit as the proof of the result
that every matrix can be put into reduced row echelon form by a sequence
of row operations, but it is somewhat more complicated, since both row and
column operations are used. The reader may wish to look ﬁrst at the example
following the proof to get an idea of what is going on in an explicit case.
Proof of Theorem 4.15. We will ﬁrst prove the existence of the LPDU decom-
position. Let A ∈Fn×n be given. If A = O, put P = O and L = D = U = In.

4.3
The LPDU Factorization
103
Thus suppose A ̸= O. If A’s ﬁrst column is zero, go to the right until you
reach the ﬁrst nonzero column, say it’s the jth. Let δj be the ﬁrst nonzero
entry (from the top), and suppose δj occurs in the ith row. That is, δj = aij.
Perform a sequence of downward transvections to make the entries below δj
equal to zero. This transforms the jth column of A into
(0 · · · 0 δj 0 · · · 0)T .
(4.3)
Thus, we can premultiply A by a lower triangular unipotent matrix L1 to
bring the jth column of A into the form (4.3). (Of course, this requires that
the matrix entries lie in a ﬁeld.) The next step is to use δj to annihilate
all the entries in the ith row of A to the right of the jth column. Since
postmultiplying by elementary matrices performs column operations, this
amounts to multiplying L1A on the right by a sequence of unipotent upper
triangular elementary matrices. This produces an upper triangular unipotent
matrix U1 such that the ﬁrst j −1 columns of (L1A)U1 are zero, the jth has
the form (4.3), and the ith row is
(0 · · · 0 δj 0 · · · 0),
(4.4)
where δj ̸= 0. We now have the ﬁrst j columns and ith row of A in the desired
form, and from now on, they won’t change.
To continue, scan to the right until we ﬁnd the ﬁrst nonzero column in
L1AU1 to the right of the jth column, and suppose this column is the mth.
Let bkm be its ﬁrst nonzero entry, and put δm = bkm. Of course, k ̸= i. Now
repeat the previous process by forming L2L1AU1U2 with suitable lower and
upper triangular unipotent matrices L2 and U2. Continuing the process, we
eventually obtain a lower triangular unipotent matrix L′ and an upper tri-
angular unipotent matrix U ′ such that each row and column of L′AU ′ has
at most one nonzero entry. Thus we may write L′AU ′ = PD for some partial
permutation matrix P, where D is a diagonal matrix. The matrix D is not
unique, since some of the columns of PD may be zero. But the entry of D
in the nonzero columns is, of course, the corresponding entry in PD. Since
we can take any entries we want in the other columns of D, we can assume
that D is nonsingular. Unraveling, we get A = LPDU, where L = (L′)−1 and
U = (U ′)−1. This proves the existence of the LPDU decomposition.
We must now show that P is unique. So suppose A = LPDU = L′P ′D′U ′
are two LPDU decompositions of A. We have to show P = P ′. But we can
write L−1L′P ′ = PDU(D′U ′)−1, so P = P ′ will follow from the next lemma.
Lemma 4.16. Suppose P and Q are n × n partial permutation matrices
such that PN = MQ, where M is lower triangular unipotent and N is non-
singular and upper triangular. Then P = Q.
Proof. We claim that P and Q have the same zero columns. For if the jth
column of Q is zero, then the jth column of MQ, and hence of PN, is also

104
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
zero. If pij ̸= 0, then pir = 0 if r ̸= j. Thus the (i, j) entry of PN is pijnjj.
But njj ̸= 0, since N is nonsingular and upper triangular. This implies that
the jth column of PN has a nonzero entry, which is impossible. Thus the
jth column of P is also zero. But since QN −1 = M −1P, it follows from the
same argument that every zero column of P is also a zero column of Q. This
gives us the claim. Now suppose the jth columns of P and Q are nonzero.
Then prj = qsj = 1 for exactly one r and exactly one s. We have to show
that r = s. If r ̸= s, then since M does downward tranvections and the (r, j)
entry prjnjj of PN is nonzero, it follows that s ≤r. (Otherwise, qkj = 0 if
k ≤r, which implies prjnjj = 0.) But since QN −1 = M −1P, this argument
also shows that r ≤s. Consequently, r = s, and therefore P = Q.
□
The existence part of the proof of Theorem 4.15 in fact gives an algorithm
for ﬁnding the LPDU factorization. Let’s examine it in an example.
Example 4.8. To illustrate the proof, assume that the ﬁeld is Q and put
A =
⎛
⎝
0
2
−2
0
4
−5
−1
−2
−1
⎞
⎠.
Since the ﬁrst nonzero entry in the ﬁrst column of A is a13 = −1, we can
start by subtracting the ﬁrst column twice from the second and subtracting
it once from the third. The result is
AU1 =
⎛
⎝
0
2
−2
0
4
−5
−1
0
0
⎞
⎠,
where
U1 =
⎛
⎝
1
−2
−1
0
1
0
0
0
1
⎞
⎠.
Next we subtract twice the ﬁrst row of AU1 from the second, which gives
L1AU1 =
⎛
⎝
0
2
−2
0
0
−1
−1
0
0
⎞
⎠,
where
L1 =
⎛
⎝
1
0
0
−2
1
0
0
0
1
⎞
⎠.

4.3
The LPDU Factorization
105
Finally, we add the second column to the third, getting
L1AU1U2 =
⎛
⎝
0
2
−2
0
0
−1
−1
0
0
⎞
⎠
⎛
⎝
1
0
0
0
1
1
0
0
1
⎞
⎠=
⎛
⎝
0
2
0
0
0
−1
−1
0
0
⎞
⎠
=
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠
⎛
⎝
−1
0
0
0
2
0
0
0
−1
⎞
⎠= PD.
Now
U = U1U2 =
⎛
⎝
1
−2
−3
0
1
1
0
0
1
⎞
⎠.
After computing L = (L1)−1 and U = (U1U2)−1, we obtain the LPDU fac-
torization
A =
⎛
⎝
1
0
0
2
1
0
0
0
1
⎞
⎠
⎛
⎝
0
1
0
0
0
1
1
0
0
⎞
⎠
⎛
⎝
−1
0
0
0
2
0
0
0
−1
⎞
⎠
⎛
⎝
1
2
1
0
1
−1
0
0
1
⎞
⎠.
⊠
Notice that if A = LPDU is nonsingular, then A−1 = U −1D−1P −1L−1.
In theory, it is simpler to invert each of L, P, D, and U and to multiply them
than to compute A−1 directly. Indeed, D−1 is easy to ﬁnd, and P −1 = P T ,
so it boils down to computing L−1 and U −1. But the inverse of an upper or
lower triangular unipotent matrix can be expressed by an explicit formula,
although it is too complicated to write down here.
4.3.3
Matrices with an LDU decomposition
If A is invertible, we now know that in the expression A = LPDU, D and
P are unique. The diagonal entries of D also turn out to be important: they
are called the pivots of A. The purpose of this subsection is to determine
when A admits an LDU decomposition, that is, an LPDU decomposition in
which P is the identity matrix. This is answered by considering the matrices
Ak consisting of the k × k blocks in the upper left-hand corner of A. We ﬁrst
observe that if A = LBU, where B ∈Fn×n, then as long as L and U are lower
and upper triangular respectively, then Ak = LkBkUk. We leave this as an
exercise (see Exercise 4.3.15). We now determine when we can decompose A
as A = LDU.
Proposition 4.17. Let A ∈Fn×n be invertible. Then A can be written in
the form LDU if and only if Ak is invertible for all k = 1, . . . , n.

106
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Proof. If A = LDU, then Ak = LkDkUk for each index k. Since Lk, Dk, and
Uk are invertible for each k, each Ak is invertible. Conversely, suppose each
Ak is invertible, and write A = LPDU. Then Ak = Lk(PD)kUk for each k,
so if Ak is invertible for all k, it follows, in particular, that (PD)k is invertible
for each k. However, since P is a permutation matrix and D is an invertible
diagonal matrix, the only way this can happen is if each Pk is equal to Ik. In
particular, P = Pn = In.
□
We now prove an interesting result for matrices with an LDU decomposi-
tion and show that it doesn’t always hold if P ̸= In.
Proposition 4.18. If an invertible matrix A admits an LDU decomposition,
then L, D, and U are unique.
Proof. We already know that D is unique. Assume that A has two LDU
decompositions, say
A = L1DU1 = L2DU2.
Then
L−1
1 L2D = DU1U −1
2 .
(4.5)
But in (4.5), the left-hand side is lower triangular, and the right-hand side
is upper triangular. Thus, both sides are diagonal. Multiplying by D−1 on
the right immediately tells us that L−1
1 L2 is diagonal, since DU1U −1
2 D−1 is
diagonal. But L−1
1 L2 is also unipotent; hence L−1
1 L2 = In. Therefore, L1 =
L2. Canceling L1D on both sides, we also see that U1 = U2.
□
Thus, a natural question is whether L and U are unique for all LPDU
decompositions. We can answer this by considering the 2 × 2 case.
Example 4.9. Let A =

 a b
c d

be invertible. That is, suppose ad −bc ̸= 0. If
a ̸= 0, then the LPDU decomposition of A is
A =

1
0
−c/a
1
 1
0
0
1
 a
0
0
(ad −bc)/a
 1
−b/a
0
1

.
This is, of course, an LDU decomposition. If a = 0, then bc ̸= 0, and A can
be expressed either as
LPD =
 1
0
d/b
1
 0
1
1
0
 c
0
0
b

or as
PDU =
0
1
1
0
 c
0
0
b
 1
d/c
0
1

.
This tells us that if P ̸= I2, then L and U aren’t necessarily unique.
⊠

4.3
The LPDU Factorization
107
4.3.4
The Symmetric LDU Decomposition
Suppose A is an invertible symmetric matrix that has an LDU decomposition.
Then it turns out that L and U are not only unique, but they are related. In
fact, U = LT . This makes ﬁnding the LDU decomposition very simple. The
reasoning for this goes as follows. If A = AT and A = LDU, then
LDU = (LDU)T = U T DT LT = U T DLT ,
since D = DT . Therefore, the uniqueness of L, D, and U implies that U = LT .
The upshot is that to factor A = LDU in the general symmetric case, all
one needs to do is perform downward row operations on A until A is upper
triangular. This is expressed by the equality L′A = B, where B is upper
triangular. Then B = DU, where D is the diagonal matrix such that dii = bii
for all indices i, and (since all the bii are nonzero) U = D−1B. Thus by
construction, U is upper triangular unipotent, and we have A = LDU, where
L = U T by the result proved in the previous paragraph.
Example 4.10. Consider the symmetric matrix
A =
⎛
⎝
1
1
1
1
3
−1
1
1
2
⎞
⎠.
First bring A into upper triangular form, which is our DU. On doing so, we
ﬁnd that A reduces to
DU =
⎛
⎝
1
1
1
0
2
−2
0
0
1
⎞
⎠.
Hence
D =
⎛
⎝
1
0
0
0
2
0
0
0
1
⎞
⎠
and
U =
⎛
⎝
1
1
1
0
1
−1
0
0
1
⎞
⎠.
Thus A = LDU, where U is as above, L = U T , and D = diag(1, 2, 1).
Summarizing, we state the following result.
Proposition 4.19. If A is an invertible n × n symmetric matrix admitting
an LDU decomposition, then A can be written in the form A = LDLT for a
unique lower triangular unipotent matrix L. This factorization exists if and
only if each symmetric submatrix Ak, k = 1, . . . , n, is invertible.
The interested reader may wish to consider what happens when an invert-
ible symmetric matrix A has zero pivots (see Exercise 4.3.16).

108
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
4.3.5
The Ranks of A and AT
The LPDU decomposition turns out to tell us something a little surprising
about the rank of a matrix and the rank of its transpose. Recall that the
rank of A is the number of nonzero rows in Ared and is well deﬁned, since the
reduced form of A is unique. We shall now apply the LPDU decomposition
to ﬁnd another description of the rank.
Suppose A ∈Fm×n. We may as well assume the A is square (i.e., m = n),
since otherwise, we may add either rows or columns of zeros to make A square
without having any eﬀect on its rank. Thus assume that A ∈Fn×n. First of
all, we make the following assertion.
Proposition 4.20. If we write A = LPDU, the rank of A is the number of
nonzero rows in the partial permutation matrix P. Put another way, the rank
of A is the number of ones in P.
Proof. In fact, the proof of Theorem 4.15 shows that when we put A in LPDU
form, the nonzero entries of PD are the pivots of A, and they become the
corner entries of Ared after row permutations.
□
We will now prove a nice fact.
Theorem 4.21. For every matrix A over a ﬁeld F, A and AT have the same
rank.
Proof. As usual, we may assume that A is n × n. Writing A = LPDU, we
see that AT = U T DP T LT . This is almost the LPDU decomposition of AT .
All we need to do is notice that there exists another nonsingular diagonal
matrix D′ such that DP T = P T D′. But P and P T obviously have the same
number of ones, so A and AT have the same rank.
□
This result is indeed a little surprising, because without knowledge of
LPDU , there isn’t any obvious reason that A and AT are related in this way.
In fact, we needed to do quite a bit of work to obtain LPDU . In particular, we
ﬁrst needed to prove the uniqueness of Ared to deﬁne rank, then we needed
to show the existence of the LPDU decomposition, and ﬁnally, we had to
establish the uniqueness of the partial permutation matrix P. Recall that
the rank of a matrix tells us the number k of basic null vectors of a linear
system Ax = 0. Namely, if A ∈Fm×n, then k = n −rank(A).
The result on the rank of AT gives a simple proof of a well-known result
in the theory of matrices.
Corollary 4.22. Suppose A ∈Fm×n and assume that M ∈Fm×m and N ∈
Fn×n are both nonsingular. Then the rank of MAN equals the rank of A.

4.3
The LPDU Factorization
109
Proof. First, notice that A and MA have the same rank. Indeed, since M is
nonsingular, it is a product of elementary matrices, so A and MA have the
same reduced forms. Thus A, MA, and AT M T have the same rank. Simi-
larly, AT M T and N T AT M T have the same rank, since N T is nonsingular.
Therefore A and MAN also have the same rank.
□
Exercises
Exercise 4.3.1. Find the LPDU decompositions and ranks of the following
matrices over Q:
⎛
⎝
0
1
1
2
0
1
1
1
0
⎞
⎠,
⎛
⎝
0
0
3
0
2
1
1
1
1
⎞
⎠,
⎛
⎝
1
0
1
0
2
−1
1
−1
0
⎞
⎠.
Exercise 4.3.2. Find the LPDU decompositions and ranks of the trans-
poses of the matrices in Exercise 4.3.1.
Exercise 4.3.3. Suppose A = LPDU. What can you say about the LPDU
decomposition of A−1 if it exists? What about (A−1)T ?
Exercise 4.3.4. Let
A =
⎛
⎝
1
a
b
0
1
c
0
0
1
⎞
⎠.
Find a formula expressing A as a product of upper triangular transvections
(i.e., elementary matrices of type III).
Exercise 4.3.5. Find a general formula for the inverse of the general 4 × 4
upper triangular unipotent matrix
U =
⎛
⎜
⎜
⎝
1
a
b
c
0
1
d
e
0
0
1
f
0
0
0
1
⎞
⎟
⎟
⎠.
Exercise 4.3.6. Show directly that an invertible upper triangular matrix B
can be expressed as B = DU, where D is a diagonal matrix with nonzero
diagonal entries and U is an upper triangular matrix all of whose diagonal
entries are ones. Is this still true if B is singular?
Exercise 4.3.7. Find the LDU decomposition of the matrix
A =
⎛
⎝
1
1
1
1
−1
0
2
0
0
⎞
⎠.

110
4
Matrix Inverses, Matrix Groups and the LPDU Decomposition
Exercise 4.3.8. Let F = F3. Find the LPDU decomposition of
⎛
⎜
⎜
⎝
0
1
2
1
1
1
0
2
2
0
0
1
1
2
1
0
⎞
⎟
⎟
⎠.
Exercise 4.3.9. Find a 3 × 3 matrix A such that the matrix L in the A =
LPDU decomposition isn’t unique.
Exercise 4.3.10. Assume that A ∈Rn×n is symmetric and has an LDU
decomposition. Show that if all the diagonal entries of D are nonnegative,
then A can be written A = CCT , where C is lower triangular. This expression
is called the Cholesky decomposition of A.
Exercise 4.3.11. Find the number of 2 × 2 matrices over F2 having rank 2,
and do the same for 3 × 3 matrices of rank 3.
Exercise 4.3.12. Suppose p is prime. Find a formula for |GL(2, Fp)|. (We
will ﬁnd a formula for |GL(n, Fp)| for every n in Chap. 6.)
Exercise 4.3.13. Find the rank of each of the following matrices:
⎛
⎝
1
2
3
1
4
9
1
8
27
⎞
⎠,
⎛
⎝
1
2
2
1
4
4
1
8
8
⎞
⎠.
Can you see a general result?
Exercise 4.3.14. Write each of the matrices
⎛
⎜
⎜
⎝
1
1
2
1
1
−1
0
2
2
0
0
1
1
2
1
−1
⎞
⎟
⎟
⎠and
⎛
⎜
⎜
⎝
0
0
0
1
0
0
2
2
0
2
4
4
1
2
4
−3
⎞
⎟
⎟
⎠
in the form LPDU , where U = LT .
Exercise 4.3.15. This is an exercise in matrix multiplication. Let A ∈Fn×n
be expressed as A = LBU, where L and U are lower and upper triangular
elements of Fn×n respectively and B ∈Fn×n is arbitrary. Show that Ak =
LkBkUk for each k = 1, . . . , n. (Recall that Ak is the k × k matrix in the
upper left-hand corner of A.)
Exercise 4.3.16. Prove the following result.

4.3
The LPDU Factorization
111
Proposition 4.23. Let A be a symmetric invertible matrix. Then there
exists an expression A = LPDU with L, P, D, U as usual such that :
(i) U = LT ,
(ii) P = P T = P −1, and
(iii) PD = DP.
Conversely, if L, P, D, U satisfy the above three conditions, then LPDU is
symmetric.
Exercise 4.3.17. Suppose p is prime. Find a formula for
|{A ∈GL(n, Fp) | A = LDU}|.

Chapter 5
An Introduction to the Theory
of Determinants
In this chapter, we will introduce and study a remarkable function called
the determinant, which assigns to an n × n matrix A over a ﬁeld F a scalar
det(A) ∈F having two remarkable properties: det(A) ̸= 0 if and only if A
is invertible, and if B is also in Fn×n, then det(AB) = det(A) det(B). The
latter property is referred to as the product formula. From a group-theoretic
standpoint, the determinant is a group homomorphism det : GL(n, F) →F∗.
In particular, det(In) = 1. A further remarkable property of the determinant
is that det(AT ) = det(A). This implies, for example, that if A is orthogonal,
that is, AT A = In, then det(A)2 = 1. Thus the determinant of an orthogo-
nal matrix A satisﬁes det(A) = ±1. As can be imagined, the deﬁnition of a
function of n2 variables having all the properties claimed above is nontrivial.
We will deﬁne det(A) via the classical formula attributed to Leibniz in 1683.
This is not intended to imply that the notion of the determinant of a matrix
preceded the notion of a matrix, since in fact, Leibniz’s deﬁnition was applied
to the coeﬃcients of a linear system. If A ∈Fn×n, the determinant of A is
deﬁned to be
det(A) =

π∈S(n)
sgn(π) aπ(1)1aπ(2)2 · · · aπ(n)n.
For example, det
a b
c d

= ad −bc. Note that since there is a term for every
element π of the symmetric group S(n), the determinant of A contains n!
terms. The symbol sgn(π), which is known as the signature of π, is either 1
or −1, depending on whether π is an even or odd permutation. (One deﬁni-
tion is that π is even (respectively odd) if it can be expressed as a product
of an even number (respectively odd number) of transpositions. However, it
isn’t clear that permutations cannot be expressed both ways.) Thus, sgn(π)
must be deﬁned precisely. In fact, we will prove that the signature is a homo-
morphism from the group S(n) to the multiplicative cyclic group {± 1} such
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 5
113

114
5
An Introduction to the Theory of Determinants
that sgn(τ) = −1 if τ is a transposition. This justiﬁes the ad hoc deﬁnition
we gave above.
Of course, properties such as the product formula were not proved until
the introduction of matrices. The determinant function has proved to be such
a rich topic of research that between 1890 and 1929, Thomas Muir published
a ﬁve-volume treatise on it entitled The History of the Determinant. We will
discuss Charles Dodgson’s fascinating formula generalizing the formula for a
2 × 2 determinant. Interest in Dodgson’s formula has recently been revived,
and it is now known as Dodgson condensation. The reader will undoubtedly
recall that Charles Dodgson wrote Alice in Wonderland under the pen name
Lewis Carroll.
Before diving into the signature and the proofs of the properties of the
determinant mentioned above, we will state the main theorem (Theorem 5.1)
on determinants, which lists its important properties. Then we will derive an
eﬃcient method for computing a determinant. A reader who wants to know
only how to compute a determinant can skip the technical details of its deﬁn-
ition. After those remarks, we will introduce the signature of a permutation,
and ﬁnally, we will prove the main theorem.
Some further basic properties of the determinant such as the Laplace
expansion and Cramer’s rule are derived in the appendix. The Laplace expan-
sion is frequently used to deﬁne the determinant by induction. This approach
has the drawback that one is required to show that the Laplace expansions
along any two rows or columns give the same result (which is true, but messy
to prove). The appendix also contains a proof of Cramer’s rule and a char-
acterization of matrices with integer entries whose inverses have only integer
entries.
5.1
An Introduction to the Determinant Function
This section is an exposition on the determinant function, stating its proper-
ties and describing how the determinant is calculated. It is intended for the
reader who needs to know only the basic facts about the determinant. For
such readers, we recommend also perusing Section 5.3.1 for further compu-
tational techniques.
5.1.1
The main theorem
We will assume that all matrices are deﬁned over the same arbitrary ﬁeld
F. The essential properties of the determinant function are captured in the
following theorem.

5.1
An Introduction to the Determinant Function
115
Theorem 5.1. There exists a unique function det : Fn×n →F, called the
determinant, with the following properties.
(i) (The product formula) For all A, B ∈Fn×n,
det(AB) = det(A) det(B).
(5.1)
(ii) If A = (aij) is either upper or lower triangular, then
det(A) =
n

i=1
aii.
(5.2)
(iii) If E is a row swap matrix, then
det(E) = −1.
(5.3)
(iv) A is nonsingular if and only if det(A) ̸= 0.
One can deduce (iv) directly from (i), (ii), and (iii) (Exercise 5.1.2).
Example 5.1. Since A ∈Fn×n is nonsingular if and only if det(A) ̸= 0 and
det(AB) = det(A) det(B), it follows that det deﬁnes a group homomorphism
det : GL(n, F) →F∗. The kernel of this homomorphism, which is by deﬁnition
{A ∈Fn×n | det(A) = 1}, is an important normal subgroup of the matrix
group GL(n, F) called the special linear group. The special linear group is
denoted by SL(n, F). For example,
SL(2, C) = {
a
b
c
d

∈C2×2 | ad −bc = 1}.
⊠
Parts (ii) and (iii) tell us the values of det(E) for all elementary matrices.
If E is obtained by multiplying the ith row of In by r, then det(E) = r, and
if E is obtained from In by adding a multiple of its jth row to its ith row,
then det(E) = 1. The method for computing det(A) is thus to row reduce A,
making use of the identity det(EA) = det(E) det(A).
5.1.2
The computation of a determinant
We will assume that the determinant function exists and has the properties
listed in Theorem 5.1. Let us now see how to compute it. The 1 × 1 case is
easy. If A = (a), then det(A) = a. In the 2 × 2 case, the formula is given by

116
5
An Introduction to the Theory of Determinants
det

a
b
c
d

= ad −bc,
(5.4)
which can be proved by applying Theorem 5.1 (see Exercise 5.1.3). Notice
that this formula says that if A is 2 × 2, then det(A) = 0 if and only if its
rows are proportional.
The most eﬃcient general technique for computing det(A) is to use
row operations. Given A ∈Fn×n, one can always ﬁnd elementary matrices
E1,
. . . , Ek such that Ek · · · E1A is an upper triangular matrix, say U.
Repeated application of Theorem 5.1 then gives
det(U) = det(E1) · · · det(Ek) det(A).
Since det(U) = u11u22 · · · unn, where the uii are the diagonal entries of U,
det(A) =
u11u22 · · · unn
det(E1) · · · det(Ek),
(5.5)
which can be evaluated by applying Theorem 5.1.
Example 5.2. Let us compute det(A), where
A =
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
1
1
1
1
1
1
0
1
⎞
⎟
⎟
⎠,
taking the ﬁeld of coeﬃcients to be Q. We can make the following sequence
of row operations, all of type III except for the last, which is a row swap.
A →
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
1
0
0
1
1
0
1
⎞
⎟
⎟
⎠→
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
1
0
0
0
1
−1
0
⎞
⎟
⎟
⎠→
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
0
0
−1
0
1
−1
0
⎞
⎟
⎟
⎠→
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
0
0
−1
0
0
−1
0
⎞
⎟
⎟
⎠→
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
0
−1
0
0
0
0
−1
⎞
⎟
⎟
⎠.
Thus (5.5) implies det(A) = −1.

5.1
An Introduction to the Determinant Function
117
One curiosity is that if the ﬁeld F has characteristic 2, then row swaps
have determinant 1. Let us rework the previous example with F = F2 with
this in mind.
Example 5.3. First add the ﬁrst row to the third and fourth rows succes-
sively. Then we get
det(A) = det
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
1
0
1
0
0
0
1
1
0
⎞
⎟
⎟
⎠.
Since the ﬁeld is F2, row swaps also leave det(A) unchanged. Thus
det(A) = det
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
0
0
1
0
1
0
1
1
0
⎞
⎟
⎟
⎠.
Adding the second row to the third row and the fourth row successively, we
get
det(A) = det
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
0
0
0
0
1
0
0
1
0
⎞
⎟
⎟
⎠.
Finally, switching the last two rows, we get
det(A) = det
⎛
⎜
⎜
⎝
1
0
1
1
0
1
0
0
0
0
1
0
0
0
0
1
⎞
⎟
⎟
⎠= 1.
⊠
Exercises
Exercise 5.1.1. Assuming Theorem 5.1, show that if A ∈Fn×n has two
equal rows and the characteristic of F is not two, then det(A) = 0.
Exercise 5.1.2. Show that (iv) of Theorem 5.1 is a consequence of (i), (ii),
and (iii).
Exercise 5.1.3. The purpose of this exercise is to prove the identity (5.4).
Deﬁne a function F : F2×2 →F by
F
a b
c d

= ad −bc.

118
5
An Introduction to the Theory of Determinants
Show that F satisﬁes the conditions in Theorem 5.1, and conclude that
F(A) = det(A).
Exercise 5.1.4. Let A =

a b
c d

. Without appealing to Theorem 5.1, show
the following:
(i) det(A) = 0 if and only if the rows of A are proportional. Conclude that
A has rank 2 if and only if ad −bc ̸= 0.
(ii) A is invertible if and only if det(A) ̸= 0. In that case,
A−1 =
1
det(A)
 d −b
−c a

.
Exercise 5.1.5. Use Theorem 5.1 to prove that if B and C are square matri-
ces over F, then
det
B
∗
O
C

= det(B) det(C).
Exercise 5.1.6. Compute
det
⎛
⎜
⎜
⎝
1
1
−1
0
2
1
1
1
0
−1
2
0
1
1
−1
1
⎞
⎟
⎟
⎠
in two cases: ﬁrst when the ﬁeld is Q and second, when the ﬁeld is F3.
Exercise 5.1.7. Express
A =
⎛
⎝
1
2
−1
2
1
1
0
−1
2
⎞
⎠
in the form A = LPDU, and use your result to compute det(A) in the fol-
lowing cases:
(a) F = Q;
(b) F = F2; and
(c) F = F3.
Exercise 5.1.8. Show that the image of det : Fn×n →F∗is, in fact, all of
F∗.

5.2
The Deﬁnition of the Determinant
119
5.2
The Deﬁnition of the Determinant
Before formally deﬁning the determinant function, we have to deﬁne the sig-
nature sgn(σ) of a permutation σ ∈S(n). As already mentioned, the signature
is a homomorphism from S(n) to the multiplicative group {+1, −1}.
5.2.1
The signature of a permutation
Recall the isomorphism ϕ : S(n) →P(n) deﬁned by ϕ(σ) = Pσ, where Pσ is
the permutation matrix whose ith column is eσ(i). The signature sgn(σ) of
the permutation σ ∈S(n) will tell us whether one requires an even or odd
number of row swaps Si to write Pσ = S1 · · · Sk. Correspondingly, we will call
π even or odd.
Deﬁnition 5.1. Suppose σ ∈S(n). If n > 1, deﬁne the signature sgn(σ) of
σ to be
sgn(σ) =

i<j
σ(i) −σ(j)
i −j
.
(5.6)
If n = 1, put sgn(σ) = 1.
Clearly, sgn(σ) is a nonzero rational number. There is at least one example
in which the value of sgn(σ) is clear.
Example 5.4. The
identity
permutation
idn ∈S(n)
has
signature
sgn(idn) = 1.
⊠
As usual, we will denote idn by 1. We now establish the properties of sgn.
We ﬁrst show that sgn(σ) ∈{±1}.
Proposition 5.2. For every σ ∈S(n), sgn(σ) = ±1.
Proof. The case n = 1 is clear, so suppose n > 1. Since σ is a bijection of
{1, 2, . . . , n} and
σ(i) −σ(j)
i −j
= σ(j) −σ(j)
j −i
,
it follows that
(sgn(σ))2 =

i̸=j
σ(i) −σ(j)
i −j
.
Moreover, since
σ(σ−1(i)) −σ(σ−1(j)) = i −j,
each possible value of (i −j) occurs the same number of times in the numer-
ator and denominator. Hence sgn(σ)2 = 1, so the proof is done.
□

120
5
An Introduction to the Theory of Determinants
Let N(σ) = {(i, j) | i < j, σ(i) > σ(j)}, and put n(σ) = |N(σ)|. By the
deﬁnition of sgn(σ) and Proposition 5.2,
sgn(σ) = (−1)n(σ).
Here is an an example.
Example 5.5. Recall that σij ∈S(n) denotes the transposition that switches
i and j while leaving all other k, 1 ≤k ≤n, unchanged. Let us compute
sgn(σ12). Now σ12 interchanges 1 and 2 and leaves every k between 3 and
n unchanged. Thus, (1, 2) is the only pair (i, j) such that i < j for which
σ12(i) > σ12(j). Hence n(σ12) = 1, so sgn(σ12) = −1.
⊠
We now establish the main properties of the signature.
Proposition 5.3. The signature mapping sgn : S(n) →{±1} has the follow-
ing properties:
(i) sgn is a group homomorphism; that is, for all σ, τ ∈S(n),
sgn(τσ) = sgn(τ)sgn(σ);
(ii) if σ is any transposition, then sgn(σ) = −1; and
(iii) for all σ ∈S(n), sgn(σ−1) = sgn(σ).
Proof. To prove (i), it will suﬃce to show that n(τσ) = n(τ) + n(σ) for an
arbitrary pair τ, σ ∈S(n). Suppose i < j and write
τ(σ(i)) −τ(σ(j))
i −j
= τ(σ(i)) −τ(σ(j))
σ(i) −σ(j)
· σ(i) −σ(j)
i −j
.
The left-hand side is negative if and only if τ(σ(i)) −τ(σ(j))
σ(i) −σ(j)
and σ(i) −σ(j)
i −j
have diﬀerent signs. Thus, (i, j) ∈N(τσ) if and only if (i, j) ∈N(σ) or
(σ(i), σ(j)) ∈N(τ). Since there is no pair (i, j) ∈N(τσ) such that (i, j) ∈
N(σ) and (σ(i), σ(j)) ∈N(τ), it follows that n(τσ) = n(τ) + n(σ). This com-
pletes the proof of (i). For (ii), we can use the result of Example 5.5. Let
σab denote the transposition interchanging a ̸= b. As an exercise, the reader
should check that
σab = σ1bσ2aσ12σ2aσ1b.
(5.7)
Hence, by (i) and the result that sgn(σ12) = −1 (by Example 5.5),
sgn(σab) = sgn(σ1b)sgn(σ2a)sgn(σ12)sgn(σ2a)sgn(σ1b) = sgn(σ12) = −1.

5.2
The Deﬁnition of the Determinant
121
This gives (ii). For (iii), just note that for every σ ∈S(n), we have σ−1σ = 1,
and apply (i) and the fact that sgn(idn) = 1.
□
A permutation σ is said to be even if sgn(σ) = 1 and odd otherwise. In
particular, all transpositions are odd. Let A(n) denote the kernel of sgn. Then
A(n) consists of the even permutations, and since the kernel of a homomor-
phism is a normal subgroup, we see that A(n) is normal in S(n). The subgroup
A(n) is called the alternating group. It is a well-known classical result that
if n > 4, then A(n) has no nontrivial normal subgroups.
5.2.2
The determinant via Leibniz’s Formula
We now have everything needed to give the Leibniz deﬁnition of det(A).
Deﬁnition 5.2. Let A ∈Fn×n. Then the determinant of A, det(A), is the
scalar deﬁned by the identity
det(A) :=

π∈S(n)
sgn(π) aπ(1)1aπ(2)2 · · · aπ(n)n.
(5.8)
The above sum contains n! terms, so one is hardly ever going to use the
deﬁnition to compute a determinant except when n is small. The cases n = 1,
2, and 3 can be worked out, but even n = 4 is too diﬃcult to do by hand
without row operations. Fortunately, one seldom needs to actually compute
det(A).
Example 5.6. (1 × 1 and 2 × 2 determinants). If A = (a) is of size 1 × 1,
then since S(1) = {id1} and sgn(id1) = 1, it follows that det(A) = a. Now
suppose A = (aij) is 2 × 2. There are exactly two elements in S(2), namely
the identity id2 and the transposition σ12. Denoting id2 by σ and σ12 by τ,
we have, by deﬁnition,
det

a11 a12
a21 a22

= sgn(σ)aσ(1)1aσ(2)2 + sgn(τ)aτ(1)1aτ(2)2
= a11a22 −a21a12.
The properties of Theorem 5.1 are easy to check in these two cases.
⊠
Example 5.7. (3 × 3 determinants). For the 3 × 3 case, let us ﬁrst list the
elements σ ∈S(3) and their signatures. We will use the triple [σ(1), σ(2), σ(3)]
to represent each σ ∈S(3). Then the signatures for S(3) are given in the
following table:
σ
[1, 2, 3] [1, 3, 2] [2, 3, 1] [2, 1, 3] [3, 1, 2] [3, 2, 1]
sgn(σ)
1
−1
+1
−1
+1
−1
.

122
5
An Introduction to the Theory of Determinants
Hence,
det(A) = a11a22a33 −a11a32a23 + a21a32a13
−a21a12a33 + a31a12a23 −a31a22a13.
Rewriting this as
det(A) = a11a22a33 + a21a32a13 + a31a12a23 +
−a11a32a23 −a21a12a33 −a31a22a13,
one sees that det(A) is the sum of the products of three entries, either two
or three of which are on a diagonal of A parallel to its main diagonal, less
the sum of the products of three entries, either two or three of which are on
a diagonal of A parallel to its antidiagonal, that is, the diagonal from the
northeast corner of A to its southwest corner. This is a standard expression
for a 3 × 3 determinant known as the rule of Sarrus. Warning: the rule of
Sarrus does not generalize to the 4 × 4 case.
⊠
5.2.3
Consequences of the deﬁnition
Several parts of Theorem 5.1 follow directly from the deﬁnition. Let us ﬁrst
prove part (ii).
Proposition 5.4. Suppose A is of size n × n and either upper or lower tri-
angular. Then
det(A) = a11a22 · · · ann.
Proof. We will suppose that A is upper triangular and leave the lower tri-
angular case as an exercise (not a very hard one at that). The point is that
in this case, the only nonzero term in (5.2) is a11a22 · · · ann, which corre-
sponds to the identity permutation. For if σ ∈S(n) is diﬀerent from idn,
then σ(i) > i for some i; hence aσ(i)i = 0, since A is upper triangular. Thus,
sgn(σ)aσ(1)1aσ(2)2 · · · aσ(n)n = 0.
□
Suppose now that P ∈Fn×n is a permutation matrix. In this case, det(P)
has a very pretty interpretation.
Proposition 5.5. Assume that P ∈Fn×n is a permutation matrix, say P =
Pμ. Then det(P) = sgn(μ).
Proof. Recall that Pμ =

eμ(1) eμ(2) · · · eμ(n)

. Writing P = (pij), we have
pμ(i)i = 1 for all i, while all other prs are equal to 0. Therefore, the only
nonzero term in the expression for det(P) is

5.2
The Deﬁnition of the Determinant
123
sgn(μ)pμ(1)1pμ(2)2 · · · pμ(n)n = sgn(μ).
□
The following result about the determinant of the transpose is going to be
used in several places in the rest of this section.
Proposition 5.6. For every A ∈Fn×n,
det(AT ) = det(A).
Proof. Since AT = (bij), where bij = aji, formula (5.2) implies
det(AT ) :=

σ∈S(n)
sgn(σ) a1σ(1)a2σ(2) · · · anσ(n).
(5.9)
Now if σ(i) = j, then σ−1(j) = i, so aiσ(i) = aσ−1(j)j. Thus,
sgn(σ) a1σ(1)a2σ(2) · · · anσ(n) = sgn(σ) aσ−1(1)1aσ−1(2)2 · · · aσ−1(n)n
= sgn(σ−1) aσ−1(1)1aσ−1(2)2 · · · aσ−1(n)n,
since by Proposition 5.3, sgn(σ−1) = sgn(σ). But the correspondence σ →
σ−1 is a bijection of S(n), so we can conclude that

σ∈S(n)
sgn(σ) a1σ(1)a2σ(2) · · · anσ(n) =

τ∈S(n)
sgn(τ) aτ(1)1)aτ(2)2 · · · aτ(n)n.
Thus det(AT ) = det(A).
□
5.2.4
The eﬀect of row operations on the
determinant
In order to prove the product formula det(AB) = det(A) det(B) for all
A, B ∈Fn×n, we will demonstrate how a row operation changes the determi-
nant. In fact, we will show that for every elementary matrix E, det(EA) =
det(E) det(A). Since det(A) = det(AT ), it follows that the product formula
holds for AB if either A or B is a product of elementary matrices. First we
prove the following result.
Proposition 5.7. Suppose that E ∈Fn×n is the elementary matrix obtained
from In by multiplying the ith row of In by r ∈F. Then for every A ∈Fn×n,
det(EA) = r det(A) = det(E) det(A).

124
5
An Introduction to the Theory of Determinants
Proof. Certainly det(E) = r, while det(EA) = r det(A), since every term in
the expansion of det(A) is multiplied by r.
□
We next prove a result for swap matrices.
Proposition 5.8. Suppose A ∈Fn×n and S ∈Fn×n is a row swap matrix.
Then
det(SA) = −det(A) = det(S) det(A).
Proof. Suppose S is a row swap matrix. Put SA = B = (bij) and let τ
denote the transposition such that S = Pτ. Then bij = aτ(i)j. We will com-
pute det(B) using the result that det(B) = det(BT ). First of all, for every
σ ∈S(n),
b1σ(1)b2σ(2) · · · bnσ(n) = aτ(1)σ(1)aτ(2)σ(2) · · · aτ(n)σ(n)
= aτ(1)μτ(1)aτ(2)μτ(2) · · · aτ(n)μτ(n)
= a1μ(1)a2μ(2) · · · anμ(n),
where μ = στ. Thus,
det(B) =

σ∈S(n)
sgn(σ)b1σ(1)b2σ(2) · · · bnσ(n)
=

σ∈S(n)
sgn(σ)a1μ(1)a2μ(2) · · · anμ(n)
= −

μ∈S(n)
sgn(μ)a1μ(1)a2μ(2) · · · anμ(n)
= −det(A).
The third equality uses two facts; ﬁrst, if μ = στ, then sgn(μ) = −sgn(σ),
and second, if σ varies through all of S(n), then so does μ = στ. Thus,
det(SA) = −det(A). To ﬁnish the proof, we need to show that det(S) = −1.
But det(S) = det(SIn) = −det(In) = −1, since det(In) = 1. Alternatively,
since S = Pτ, it follows that det(S) = sgn(τ) = −1 by Proposition 5.5.
□
The next step is to show that det(EA) = det(A) if E is a transvection, that
is, an elementary matrix of type III. Suppose A ∈Fn×n and let a1, . . . , an
denote its rows. Let E be of type III, say E is obtained from In by replacing
the ith row of In by itself plus r times the jth row. Then the ith row of EA
is ai + raj, and the other rows are unchanged. Hence by (5.9), each term in
the expansion of det(EA) has the form
sgn(σ)a1σ(1) . . . a(i−1)σ(i−1)(aiσ(i) + rajσ(i))ai+1σ(i+1 . . . anσ(n).
Thus, det(EA) is of the form det(A) + r det(C), where C ∈Fn×n has the
property that its ith and jth rows both coincide with aj. If we apply the

5.2
The Deﬁnition of the Determinant
125
fact that det(SB) = −det(B) whenever S is a row swap matrix, then we see
that if S swaps the ith and jth rows, we get C = SC, so det(C) = det(SC) =
−det(C). Thus 2 det(C) = 0. It follows that det(C) = 0 as long as the char-
acteristic of the ﬁeld F is diﬀerent from two. The formula det(C) = 0 when F
has characteristic two and two rows of C coincide requires a special argument,
which is given in the appendix to this chapter.
□
The following proposition summarizes what was proved in this section
modulo showing that det(C) = 0 in characteristic two if C has two equal
rows.
Proposition 5.9. If E ∈Fn×n is any elementary matrix, then det(EA) =
det(E) det(A) for all A ∈Fn×n. Moreover, det(E) = r if E multiplies the ith
row of In by r, det(E) = −1 if E is a row swap, and det(E) = 1 if E is a
transvection.
5.2.5
The proof of the main theorem
We are now ready to complete the proof of the main theorem. Parts (ii) and
(iii) of the main theorem have already been veriﬁed. Let A ∈Fn×n, and let
us ﬁrst show that det(A) ̸= 0 if and only if A is nonsingular. There exist
elementary matrices E1, . . . , Ek such that Ek · · · E1A = Ared, so
det(A) = det(Ared)

i det(Ei).
Hence det(A) ̸= 0 if and only if det(Ared) ̸= 0. But since Ared is upper trian-
gular, det(Ared) ̸= 0 if and only if Ared = In if and only if A is nonsingular.
It remains to prove the product formula. If A and B are both nonsingular,
then the validity of the product formula is clear, for both A and B and hence
AB are products of elementary matrices. On the other hand, if either A or
B is singular, I claim that AB is singular. If not, then (AB)−1A is a left
inverse of B, so B is nonsingular. Similarly, B(AB)−1 is a right inverse of
A, so A is also nonsingular. Hence, if det(A) det(B) = 0, then det(AB) = 0.
This ﬁnishes the proof of the product formula.
□
5.2.6
Determinants and LP DU
Recall from Chap. 4 that every A ∈Fn×n can be written A = LPDU, where
L and U are respectively lower and upper triangular unipotent matrices, P is
a unique partial permutation matrix, and D is an invertible diagonal matrix.

126
5
An Introduction to the Theory of Determinants
Clearly, det(A) = 0 unless P is a full permutation matrix. Furthermore, we
have the following proposition.
Proposition 5.10. If A = LPDU is nonsingular, then
det(A) = det(P) det(D) = ± det(D).
Thus, up to sign, the determinant of an invertible matrix is the product of its
pivots. If A = LDU, then det(A) = det(D).
Proof. Just use the product rule and the fact that det(L) = det(U) = 1, since
L and U are triangular and have ones on their diagonals.
□
Recall from Proposition 4.17 that an invertible A ∈Fn×n has an LDU
decomposition if and only if each Ak is also invertible, where Ak is the k × k
submatrix in the upper left-hand corner of A, and that the pivots of A are the
diagonal entries of D. Let d1, . . . , dn be these pivots. Then Ak = LkDkUk, so
det(Ak) = det(Dk) = d1 · · · dk.
This gives us the following result.
Proposition 5.11. If A ∈Fn×n satisﬁes the condition that each Ak, 1 ≤
k ≤n, is invertible, then A has an LDU decomposition, and the kth pivot of
A in its LDU decomposition is
dk =
det(Ak)
det(Ak−1).
(5.10)
5.2.7
A beautiful formula: Lewis Carroll’s identity
The theory of determinants is a remarkably rich topic, and we have barely
scratched its surface. Many of the foremost mathematicians of the nine-
teenth century, among them Gauss, Laplace, Lagrange, Cayley, Sylvester,
and Jacobi, discovered some of its important properties and applications. An
example is the Jacobian of a mapping and the change of variables formula. In
this section, we will consider a determinantal curiosity from the nineteenth
century that has recently been found to have connections to contemporary
mathematics. The formula det

a
b
c
d

= ad −bc turns out to have a gener-
alization to n × n matrices that was discovered by Charles Dodgson, a profes-
sor of mathematics at Oxford who, as is well known, wrote Alice’s Adventures
in Wonderland and Through the Looking Glass under the pseudonym Lewis
Carroll. Dodgson found an identity involving the determinant of an n × n
matrix A that is analogous to the formula in the 2 × 2 case. Let AC be the

5.2
The Deﬁnition of the Determinant
127
(n −2) × (n −2) submatrix of A obtained by deleting the ﬁrst and last rows
and the ﬁrst and last columns. If n = 2, put det(AC) = 1. Also, let ANW
denote the (n −1) × (n −1) submatrix in the upper left-hand corner of A,
and deﬁne ANE, ASW , and ASE to be the (n −1) × (n −1) submatrices in
the other three corners of A. Dodgson’s formula asserts that
det(AC) det(A) = det(ANW ) det(ASE) −det(ANE) det(ASW )
(5.11)
(see C.L. Dodgson, Proc. Royal Soc. London 17, 555–560 (1860)). If det(AC) ̸=
0, this substantially cuts down on the diﬃculty of ﬁnding det(A). Of course,
if det(AC) = 0, then the identity says nothing about det(A), although it does
say something about the four determinants at the corners. This formula is
referred to as Dodgson condensation. Dodgson himself supplied the term con-
densation. In the 1980s, it was noticed that Dodgson condensation is related
to the problem of counting alternating sign matrices (ASMs). Eventually,
the problem of enumerating the ASMs was given an elegant solution using
statistical mechanics.

128
5
An Introduction to the Theory of Determinants
Exercises
Exercise 5.2.1. Two matrices A, B ∈Fn×n are said to be similar if A =
CBC−1 for some C ∈GL(n, F). Show that similar matrices have the same
determinant.
Exercise 5.2.2. Suppose P is an n × n matrix over C such that PP = P.
What is det(P)? What is det(Q) if Q4 = Q−1?
Exercise 5.2.3. Which of the following statements are true. Give your rea-
soning or supply a counter example.
(i) The determinant of a real symmetric matrix is always nonnegative.
(ii) If A is a 2 × 3 real matrix, then det(AAT )Gal(E/Q)0.
(iii) If A is a square real matrix, then det(AAT )Gal(E/Q)0.
Exercise 5.2.4. An n × n matrix A over R is called skew-symmetric
if
AT = −A. Show that if A is a skew-symmetric n × n matrix and n is odd,
then A cannot be invertible.
Exercise 5.2.5. Let H ∈Rn×n be a Hadamard matrix. That is, suppose
HHT = nIn. Find det(H).
Exercise 5.2.6. Recall that SL(n, F) denotes the set of all A ∈Fn×n such
that det(A) = 1. Prove that SL(n, F) is a matrix group and a proper normal
subgroup of GL(n, F) if the characteristic of F is diﬀerent from 2.
Exercise 5.2.7. Find the 3 × 3 permutation matrices that lie in SL(3, R).
Exercise 5.2.8. Let SO(n, R) denote the set of all n × n orthogonal matri-
ces Q such that det(Q) = 1. Show that SO(n, R) is a matrix group and a
proper normal subgroup of O(n, R).
Exercise 5.2.9. If A ∈Cm×n, deﬁne AH = A
T , where A is the matrix
obtained by conjugating each entry of A. An n × n matrix U over C is called
unitary
if U −1 = U H. What are the possible values of the determinant of
det(U) of a unitary matrix U?
Exercise 5.2.10. An n × n matrix K over C is called Hermitian
if K =
KH. (See the previous exercise for the deﬁnition of KH.) Show that if K is
Hermitian, then det(K) is a real number.
Exercise 5.2.11. Determine whether
A =
⎛
⎜
⎜
⎝
1
0
2
1
0
1
0
1
1
1
1
1
1
1
0
1
⎞
⎟
⎟
⎠
has an LDU decomposition over both Q and F3. If it does in either case, ﬁnd
its pivots.

5.2
The Deﬁnition of the Determinant
129
Exercise 5.2.12. Suppose A is a square matrix over F such that each row
sums to zero. Find det(A).
Exercise 5.2.13. Show that the condition in Proposition 7.27 that F(A) =
0 if two rows of A are equal is equivalent to the condition that F(SA) =
−F(A) if S is a row swap matrix.
Exercise 5.2.14. Find all values x ∈R for which det(A(x)) = 0 when
A(x) =
⎛
⎝
1
x
2
x
1
x
2
3
1
⎞
⎠.
Exercise 5.2.15. Repeat the previous exercise for the matrix
B(x) =
⎛
⎜
⎜
⎝
1
x
1
x
1
0
x
1
0
x
1
1
1
0
1
0
⎞
⎟
⎟
⎠.
Exercise 5.2.16. Why does condition (ii) in Theorem 7.27 imply that the
determinant changes sign under a row swap?
Exercise 5.2.17. Recall that SL(n, F) = {A ∈Fn×n such that det(A) = 1}.
Let SL(n, Z) denote Zn×n ∩SL(n, Q). Show that SL(n, Z) is a matrix group.
Exercise 5.2.18. Show that if P ∈Zn×n is a row swap, then GL(n, Z) =
SL(n, Z) ∪PSL(n, Z).

130
5
An Introduction to the Theory of Determinants
5.3
Appendix: Further Results on Determinants
The purpose of this appendix is to brieﬂy introduce the Laplace expansion
of a determinant and explain Cramer’s rule.
5.3.1
The Laplace expansion
In this section, we will obtain some further properties of the determinant,
beginning with the Laplace expansion, which is the classical way of calculating
an n × n determinant as a sum of (n −1) × (n −1) determinants. After the
Laplace expansion, we will state Cramer’s rule, which gives a closed form
for inverting a nonsingular matrix. The Laplace expansion also allows one to
show by induction that the determinant of a matrix with two equal rows is
zero.
Suppose A is of size n × n, and let Aij denote the (n −1) × (n −1) sub-
matrix obtained from A by deleting its ith row and jth column.
Theorem 5.12. For every A ∈Fn×n, we have
det(A) =
n

i=1
(−1)i+jaij det(Aij).
(5.12)
This is the Laplace expansion along the jth column. The corresponding
Laplace expansion along the ith row is
det(A) =
n

j=1
(−1)i+jaij det(Aij).
(5.13)
Proof. Since det(A) = det(AT ), it suﬃces to prove (5.12). For simplicity, we
will assume j = 1, the other cases being similar. Now,
det(A) =

σ∈S(n)
sgn(σ) aσ(1)1aσ(2)2 · · · aσ(n)n
= a11

σ(1)=1
sgn(σ)aσ(2)2 · · · aσ(n)n +
a21

σ(1)=2
sgn(σ)aσ(2)2 · · · aσ(n)n +
+ · · · + an1

σ(1)=n
sgn(σ)aσ(2)2 · · · aσ(n)n.

5.3
Appendix: Further Results on Determinants
131
Suppose σ(1) = r. Let us evaluate
ar1

σ(1)=r
sgn(σ)aσ(2)2 · · · aσ(n)n.
Let P ′
σ denote the element of F(n−1)×(n−1) obtained from Pσ by deleting the
ﬁrst column and the rth row. Then P ′
σ ∈P(n −1), so P ′
σ = Pσ′ for a unique
σ′ ∈S(n −1). Note that det(Pσ) = (−1)(r−1) det(P ′
σ), since if bringing P ′
σ to
In−1 by row swaps uses t steps, one needs t + σ(1) −1 row swaps to bring
Pσ to the identity. Thus,

σ(1)=r
sgn(σ)aσ(2)2 · · · aσ(n)n =

σ(1)=r
(−1)r−1sgn(σ′)aσ(2)2 · · · aσ(n)n
= (−1)r+1 det(Ar1).
Substituting this into the above calculation for r = 1, . . . , n gives the
result.
□
Example 5.8. If A is of size 3 × 3, expanding det(A) along the ﬁrst column
gives
det(A) = a11(a22a33 −a32a23) −a21(a12a23 −a13a32) + a31(a12a23 −a13a22).
This is the well-known formula for the triple product a1 · (a2 × a3) of the
rows of A.
Example 5.9. The Laplace expansion is useful for evaluating det(A) when
A has entries that are functions. In fact, this situation will arise when we
consider the characteristic polynomial of a square matrix A. Consider the
matrix
Cx =
⎛
⎝
1 −x
2
0
2
1 −x
−1
0
−1
2 −x
⎞
⎠.
Suppose we want to ﬁnd all values of x ∈C such that Cx has rank less that
3, i.e., Cx is singular. The obvious way to proceed is to solve the equation
det(Cx) = 0 for x. Clearly, row operations aren’t going to be of much help in
ﬁnding det(Cx), so we will use Laplace, as in the previous example. Expanding
along the ﬁrst column gives
det(Cx) = (1 −x)

(1 −x)(2 −x) −(−1)(−1)

−2

2(2 −x) −0(−1)

= −x3 + 4x −7.
Hence Cx is singular at the three complex roots of x3 −4x + 7 = 0.

132
5
An Introduction to the Theory of Determinants
We can now ﬁnish the proof that det(C) = 0 when two rows of C are
identical. This will complete the proof of Theorem 5.1.
Proposition 5.13. Suppose
C ∈Fn×n
has
two
equal
rows.
Then
det(C) = 0.
Proof. Suppose the ith and jth rows of C are equal and S is the matrix
that swaps these rows. Then SC = C, so det(SC) = det(C), while det(SC) =
−det(C), since det(S) = −1. Hence 2 det(C) = 0. Thus, as long as the char-
acteristic of F is diﬀerent from 2, det(C) = 0. The hard case is that in which
the characteristic of F is 2. In this case, the signatures do not contribute to
the determinant, since 1 = −1 in F, so
det(C) =

π∈S(n)
cπ(1)1cπ(2)2 · · · cπ(n)n.
(5.14)
Let us now assume that the characteristic of F is two. By the usual for-
mula, det(A) = ad −bc for A ∈F2×2, it follows that if a = c and b = d, then
det(A) = ad −ad = 0. Hence we may assume as our induction hypothesis
that proposition holds for n ≥2. Let A ∈F(n+1)×(n+1) have two equal rows,
say the ﬁrst two rows. Using the Laplace expansion for det(A) along the
ﬁrst column, we get that a11 det(A11) = a21 det(A21), and the other terms
are zero, since the induction hypothesis implies det(B) = 0 if B ∈Fn×n has
two equal rows. Consequently, det(A) = 2a11 det(A11) = 0. This ﬁnishes the
induction step, so the proposition is proved.
□
Remark. As mentioned above, algebra texts often deﬁne the determinant
inductively via the Laplace expansion. This avoids the problem of introducing
the signature of a permutation. Leibniz was aware of the Laplace expansion,
for example, well before the time of Laplace (1749–1827). The drawback of
this approach is that in order for it to be of use, one needs to know that all
possible row and column expansions have the same value. The only way to
show that fact is to appeal to a formula such as the Leibniz formula, which
avoids using rows and columns. The Laplace expansion is useful as a compu-
tational tool for matrices with few nonzero entries or when row operations are
impractical, such as for a characteristic polynomial. But it would be useless
to attempt to use Laplace for calculating even a 5 × 5 determinant, which
could be done easily with row operations.
5.3.2
Cramer’s Rule
Cramer’s rule is a closed formula for the inverse of a square matrix with
nonzero determinant. Recall that if A is of size 2 × 2, then

5.3
Appendix: Further Results on Determinants
133
A−1 =
1
det(A)

a22 −a12
−a21 a11

.
Inspecting this formula may suggest the correct formula for A−1 in the general
case.
Deﬁnition 5.3. Suppose A ∈Fn×n, and let Aij denote the (n −1) ×
(n −1) submatrix of A obtained by deleting A’s ith row and jth column.
Then the matrix
Adj(A) =

(−1)i+j det(Aji)

(5.15)
is called the adjoint of A.
Proposition 5.14. Suppose A ∈Fn×n. Then Adj(A)A = det(A)In. Thus if
det(A) ̸= 0, then
A−1 =
1
det(A)Adj(A).
Proof. The essential ideas are all contained in the 3 × 3 case, so for simplicity,
we will let n = 3. By deﬁnition,
Adj(A) =
⎛
⎝
det(A11)
−det(A21)
det(A31)
−det(A12)
det(A22)
−det(A23)
det(A13)
−det(A23)
det(A33)
⎞
⎠.
Put
C =
⎛
⎝
det(A11)
−det(A21)
det(A31)
−det(A12)
det(A22)
−det(A23)
det(A13)
−det(A23)
det(A33)
⎞
⎠
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠.
We have to show that C = det(A)In. But it follows immediately from Theo-
rem 5.12 that each diagonal entry of C is det(A). On the other hand, consider
one of C’s oﬀ-diagonal entries, say c21. Expanding the above product gives
c21 = −a11 det(A12) + a21 det(A22) −a31 det(A32).
But this is exactly the Laplace expansion along the ﬁrst column for the
determinant of the matrix
−
⎛
⎝
a11
a11
a13
a21
a21
a23
a31
a31
a33
⎞
⎠.
The determinant of this matrix is 0, since it has equal columns, so c21 = 0.
Similarly, all cij with i ̸= j vanish, so C = det(A)I3.
□

134
5
An Introduction to the Theory of Determinants
5.3.3
The inverse of a matrix over Z
In most of our examples of inverting a matrix A, the entries of A are integers.
But experience tells us that usually, at some time during the row operations,
denominators appear. In fact, Cramer’s rule, Proposition 5.14, tells us that
when det(A) = ±1, then A−1 also has integral entries. The question whether
this is the whole story will now be answered.
Proposition 5.15. Suppose A is an invertible matrix with integral entries.
Then A−1 has integral entries if and only if det(A) = ±1.
Proof. The if statement follows from Cramer’s rule. Conversely, suppose A−1
is integral. Then det(A) and det(A−1) both are integers. But
det(AA−1) = det(A) det(A−1) = det(In) = 1,
so the only possibility is that det(A) = det(A−1) = ±1.
□
A somewhat deeper fact is the following result.
Proposition 5.16. An n × n matrix over Z is invertible over Z if and only
if it can be expressed as a product of elementary matrices all of which are
deﬁned over Z.
We will skip the proof. Of course, row swap matrices are always integral.
The restriction of sticking to elementary matrices over Z means that one can
multiply a row only by ±1 and replace it by itself plus an integral multiple
of another row. Let GL(n, Z) = {A ∈Zn×n | det(A) = ±1} and SL(n, Z) =
GL(n, Z) ∩SL(n, R). The latter groups, especially SL(2, Z), are examples of
modular groups
and are very important in number theory, topology, and
complex analysis, to name a few areas where they are used.
Proposition 5.17. GL(n, Z) and SL(n, Z) are matrix groups.

Chapter 6
Vector Spaces
A vector space is a set V whose elements, called vectors, can be added and
subtracted: in fact, a vector space is an abelian group under addition. A
vector space also has an operation called scalar multiplication whereby the
elements of a ﬁeld F act on vectors. When we speak of a vector space, we also
mention the scalars by saying that V is a vector space over F. For example,
Fn, Fm×n are two examples of vector spaces over F, and the row space and
null space of a matrix over F are two more examples. Another example is the
set C([a, b]) of all continuous real-valued functions on a closed interval [a, b],
which is a vector space over R. Here, one needs the theorem that the sum of
two continuous real-valued functions on [a, b] is continuous in order to speak
of (vector) addition in this vector space.
One of the most important concepts associated with a vector space is its
dimension. The deﬁnition of dimension requires quite a bit of preliminary
groundwork. One must ﬁrst introduce linear independence and the notion of
a basis and then prove the fundamental result about bases: if a vector space
has a ﬁnite basis, then every two bases B and B′ have the same number
of elements; that is, |B| = |B′|. We then call |B| the dimension of V . This
deﬁnition turns out to coincide with one’s intuitive notion that a line is one-
dimensional, a plane is two-dimensional, space is three-dimensional, and after
that, you have to deal with objects such as spacetime, which you cannot
actually picture. After covering the basic topics, we will investigate some
special topics such as direct sums of subspaces, the Grassmann intersection
formula, and quotient vector spaces. The reader is also advised to look at the
appendix, which is an exposition of linear coding theory. This is an interesting
contemporary topic involving vector spaces over ﬁnite ﬁelds.
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 6
135

136
6
Vector Spaces
6.1
The Deﬁnition of a Vector Space and Examples
The purpose of this section is to introduce the deﬁnition of an abstract vector
space and to recall a few old examples as well as to add a few new ones. In
this section, it will be useful (but not absolutely necessary) to have studied
the material in Chap. 2 on groups and ﬁelds.
6.1.1
The vector space axioms
Put as succinctly as possible, a vector space over a ﬁeld F consists of an
abelian group V under addition whose elements are called vectors. Vectors
admit a multiplication by the elements of F, the ﬁeld of scalars. Addition
and scalar multiplication interact in a way that we will state precisely below.
The deﬁnition below of a vector space is given in full detail so that it is not
necessary to know the deﬁnition of an abelian group.
Deﬁnition 6.1. Let F be a ﬁeld and suppose V is a set with a binary oper-
ation + called addition assigning to any two elements a and b of V a unique
sum a + b ∈V . Suppose also that there is a second operation, called scalar
multiplication, assigning to every r ∈F and a ∈V a unique scalar multiple
ra ∈V . Addition and scalar multiplication have the following properties.
(1) Addition is commutative: a + b = b + a for all a, b ∈V .
(2) Addition is also associative: (a+b)+c = a+(b+c) for all a, b, c ∈V .
(3) V contains an additive identity 0: that is, 0 + a = a for all a ∈V .
(4) For every element v of V , there is an element −v such that
v + (−v) = 0.
Thus −v is an additive inverse of v.
(5) For all a ∈V , 1a = a, where 1 is the multiplicative identity of F.
(6) Scalar multiplication is associative: if r, s ∈F and a ∈V , then
(rs)a = r(sa).
(7) Scalar multiplication is distributive: if r, s ∈F and a, b ∈V , then
r(a + b) = ra + rb and (r + s)a = ra + sa.

6.1
The Deﬁnition of a Vector Space and Examples
137
Then V is called a vector space over F.
The ﬁrst four axioms say that a vector space is an abelian group V under
addition with identity 0. The properties that scalar multiplication must sat-
isfy along with addition are speciﬁed by properties (5), (6) and (7). As proved
in the group setting, the additive identity 0 and additive inverses are unique.
To remind the reader, we will repeat the proofs (word for word) for vector
spaces.
Proposition 6.1. In a vector space, there can be only one zero vector. Fur-
thermore, the additive inverse of a vector is always unique.
Proof. Let 0 and 0′ both be additive identities. Then
0 = 0 + 0′ = 0′,
by the deﬁnition of an additive identity. Hence the zero vector is unique. Now
suppose −v and −v′ are both additive inverses of v ∈V . Then
−v = −v + 0 = −v + (v −v′) = (−v + v) + (−v′) = 0 + (−v′) = −v′.
Hence, additive inverses are also unique.
□
Proposition 6.2. In a vector space V , 0v = 0 for all v ∈V , and r0 = 0
for every scalar r. Moreover, −v = (−1)v.
Proof. Let v be arbitrary. Now, by properties (4) and (7) of the deﬁnition,
v = 1v = (1 + 0)v = 1v + 0v = v + 0v.
Adding −v to both sides and using associativity gives 0v = 0. For the second
assertion, note that
0 = 0v = (1 + (−1))v = 1v + (−1)v = v + (−1)v.
Hence, (−1)v is an additive inverse of v, so (−1)v = −v for all v ∈V .
□
If v1, . . . , vk ∈V , then one can deﬁne the sum
v1 + · · · + vk =
k

i=1
vi
inductively as (v1 + · · · + vk−1) + vk. Just as for sums in a ﬁeld, the terms in
this sum can be associated in any convenient way. Similarly, the summands
vi can be taken in any order without changing the sum, since addition is
commutative. Recall that an expression k
i=1 rivi, where r1, . . . , rk ∈F, is
called a linear combination of v1, . . . , vk ∈V .

138
6
Vector Spaces
6.1.2
Examples
Example 6.1. As mentioned above, the basic example of a vector space
over the ﬁeld F is the set Fn of all column n-tuples of elements of F, where
addition and scalar multiplication are carried out componentwise:
a + b =
⎛
⎜
⎜
⎜
⎝
a1
a2
...
an
⎞
⎟
⎟
⎟
⎠+
⎛
⎜
⎜
⎜
⎝
b1
b2
...
bn
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
a1 + b1
a2 + b2
...
an + bn
⎞
⎟
⎟
⎟
⎠
and
ra = r
⎛
⎜
⎜
⎜
⎝
a1
a2
...
an
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
ra1
ra2
...
ran
⎞
⎟
⎟
⎟
⎠.
The elements of Fn are called column vectors.
⊠
Example 6.2. Let Fm×n denote the m × n matrices over F. In the previous
example, we considered Fn, which is the same as Fn×1. The elements of F1×n
are called row vectors. We have already deﬁned matrix addition and scalar
multiplication for m×n in an analogous way in Chap. 3, so we refer the reader
there. These operations make Fm×n a vector space over F. One can express the
elements of Fmn as matrices, so as a vector space, Fm×n is indistinguishable
from Fmn.
⊠
Example 6.3. (See Example 3.2.) When F = F2, the elements of Fn are
binary strings, which are called n-bit strings. Binary strings are usually writ-
ten as rows instead of columns, and the commas between components are
omitted. For example, there are 23 3-bit strings, 000, 100, 010, 001, 110, 101,
011, and 111. Binary strings are the fundamental objects of coding theory,
but one can just as well consider p-ary strings of length n, namely elements
of the vector space (Fp)n written as row vectors as in the binary case. A
common notation for (Fp)n is V (n, p). Thus V (n, p) consists of the pn strings
a1a2 . . . an where each ai ∈Fp.
⊠
Example 6.4. Let S be any set and deﬁne FS to be the set of all F-valued
functions whose domain is S. We deﬁne addition and scalar multiplication
pointwise as follows. If μ, φ ∈FS, then μ+φ ∈FS is deﬁned by the condition
(μ + φ)(s) = μ(s) + φ(s)
for all s ∈S. Also, if a ∈F, then aμ is deﬁned by

6.1
The Deﬁnition of a Vector Space and Examples
139
(aμ)(s) = aμ(s)
for all s ∈S. These operations make FS a vector space over F. Notice that
Fn is nothing but FS, where S = {1, 2, . . . , n}. Indeed, specifying the n-tuple
a = (a1, a2, . . . , an)T ∈Fn is the same as deﬁning the function fa : S →F by
fa(i) = ai.
⊠
Example 6.5. The set Pn of all polynomial functions with domain R and
degree at most n consists of all functions p : R →R such that for every r ∈R,
p(r) = anrn + an−1rn−1 + · · · + a1r + a0,
where the coeﬃcients a0, a1, . . . , an are ﬁxed elements of R. If we let x : R →R
be the identity function deﬁned by x(r) = r for all r ∈R, then the above
polynomial can be written p(x) = anxn + an−1xn−1 + · · · + a1x + a0. Then
Pn is a real vector space.
⊠
Example 6.6. (Polynomials over F) For an arbitrary ﬁeld F, let x denote a
quantity that admits multiplication by itself so that xixj = xi+j and scalar
multiples axk for all a ∈F and all integers i, j, k ≥0. Let F[x] denote the set
of all polynomial expressions
f(x) = anxn + an−1xn−1 + · · · + a1x + a0,
where the coeﬃcients a0, a1, . . . , an ∈F, and n is an arbitrary nonnegative
integer. The polynomials of degree at most n can be identiﬁed with Fn+1,
so there is a well deﬁned addition and scalar multiplication on F[x] so that
polynomials are added by adding the coeﬃcients of xi for all i.
⊠
Example 6.7. The set C[a, b] of all continuous real-valued functions on [a, b]
with the usual pointwise addition and scalar multiplication of Example 6.4 is
a slightly more exotic example of a vector space. To see that C[a, b] is closed
under addition and scalar multiplication requires knowing a basic theorem
from calculus: the sum of two continuous functions is continuous, and any
scalar multiple of a continuous function is continuous. Hence f + g and rf
belong to C[a, b] for all f and g in C[a, b] and real scalars r.
⊠
Example 6.8. Consider the diﬀerential equation
y′′ + ay′ + by = 0,
(6.1)
where a and b are real constants. This is an example of a homogeneous linear
second-order diﬀerential equation with constant coeﬃcients. The set of twice
diﬀerentiable functions on R that satisfy (6.1) is a real vector space.
⊠

140
6
Vector Spaces
Exercises
Exercise 6.1.1. Suppose V is a vector space over the ﬁeld F. Show that if
v is a nonzero element of V and a is a scalar such that av = 0, then a = 0.
Conclude that if av = bv, where a, b ∈F, then a = b.
Exercise 6.1.2. Suppose V is a vector space over the ﬁeld F, and a ∈F
is nonzero. Let μa : V →V be deﬁned by μa(v) = av. Show that μa is a
bijection.
Exercise 6.1.3. Consider the set S of all A ∈Fn×n for which A = AT . True
or false: S is a vector space.
Exercise 6.1.4. Let A ∈Fm×n.
(i) Show that N(A) = {v ∈Fn | Av = 0} is a vector space.
(ii) Suppose b ∈Fn. When is the solution set {x | Ax = b} a vector space?
Exercise 6.1.5. Is the unit circle x2 + y2 = 1 in R2 a vector space?
Exercise 6.1.6. When is a line ax + by = c in R2 a vector space?

6.2
Subspaces and Spanning Sets
141
6.2
Subspaces and Spanning Sets
The purpose of this section is to introduce, study, and give examples of sub-
spaces and spanning sets. Throughout this section, let V be a vector space
over an arbitrary ﬁeld F.
Deﬁnition 6.2. A nonempty subset W of V is called a linear subspace of
V , or simply a subspace, provided the following two conditions hold for all
a, b ∈W:
(i) a + b ∈W, and
(ii) ra ∈W whenever r ∈F.
In particular, every subspace of a vector space contains the zero vector
0. In fact, {0} is itself a subspace, called the trivial subspace. The following
proposition is an immediate consequence of this deﬁnition.
Proposition 6.3. A subspace W of V is also a vector space over F under
the addition and scalar multiplication induced from V .
We leave the proof to the reader.
Example 6.9. Suppose A ∈Fm×n. Then the solution set N(A) of the homo-
geneous equation Ax = 0 is a subspace of Fn. For if Axi = 0 for i = 1, 2,
then
A(x1 + x2) = Ax1 + Ax2 = 0 + 0 = 0,
while
A(rx) = rAx = r0 = 0.
Therefore, N(A) is indeed a subspace.
⊠
Example 6.10. The subspaces of R2 are easily described. They are {0},
every line through 0, and R2 itself. We will consider the subspaces of R3
below. This example shows that subspaces when viewed as geometric objects
need to be linear; that is, they need to be lines, planes, etc., through the zero
vector 0.
⊠
Example 6.11. Let F be a ﬁeld and suppose F′ is a subﬁeld of F. Then F
is a vector space over F′, vectors being the elements of F and scalars being
the elements of F′. Note that elements of F′ are also vectors, but that is
irrelevant. In particular, a ﬁeld is a vector space over itself. For example,
R is a vector space over Q, and C is a vector space over R and also a vec-
tor space over Q. Note that C as a vector space over R is very diﬀerent
from C as a vector space over Q. It will turn out that the dimension of
C as a vector space over R is two, while its dimension as a vector space
over Q is not ﬁnite (although we have yet to deﬁne what the dimension
of a vector space is).
⊠

142
6
Vector Spaces
6.2.1
Spanning sets
We will now consider the most basic method for constructing subspaces.
Deﬁnition 6.3. Let v1, . . . , vk be arbitrary elements of V . The span of
v1, . . . , vk is by deﬁnition the subset of V consisting of all linear combinations
k

i=1
aivi,
where a1, . . . , ak are arbitrary elements of F. The span of v1, . . . , vk is denoted
by span{v1, . . . , vk}.
Proposition 6.4. For all v1, . . . , vk in V , span{v1, . . . , vk} is a subspace
of V .
This follows readily from the deﬁnitions, so we will skip the details. How-
ever, the reader might beneﬁt from writing them all out.
Example 6.12 (lines and planes). The subspace spanned by a single nonzero
vector v is called the line spanned by v. Thus, the line spanned by v consists
of all scalar multiples av with a ∈F, so we will frequently denote it by
Fv instead of span{v}. A pair of vectors in V are said to be noncollinear
if neither lies on the line spanned by the other; in other words, their span
isn’t a line. A subspace spanned by two noncollinear vectors u, v is called
a plane. Thus a plane always has the form span{u, v}, but span{u, v} isn’t
necessarily a plane. It may be a line or even {0}.
⊠
Example 6.13. Let F = Fp. If v ∈Fn, one can ask how many elements Fv
has. If v = 0, the answer is clearly one. Otherwise, recall from Exercise 6.1.1
that if a, b ∈F and a ̸= b, then av ̸= bv. Consequently, the multiples of v are
all distinct, and therefore |Fv| = |F| = p. If a and b are noncollinear elements
of Fp, then the plane they span has p2 elements. More generally, we can ask
how many elements an arbitrary subspace of Fn has.
⊠
Example 6.14. We saw in Example 6.9 that the null space N(A) of a matrix
A ∈Fm×n is a subspace of Fn. Recall from Example 3.11 that N(A) is
spanned by the basic null vectors f1, . . . , fk obtained from the reduced row
echelon form Ared of A. Recall also that k = n −rank(A), where rank(A)
is the number of nonzero rows in Ared. Now suppose F = Fp. Since every
v ∈N(A) has an expansion v = a1f1 + · · · + akfk with all ai ∈F, it follows
that |N(A)| ≤pk.
⊠

6.2
Subspaces and Spanning Sets
143
The next two examples illustrate the other two subspaces associated with
a matrix.
Example 6.15. Recall from Section 3.2.3 that if A ∈Fm×n, then row(A) is
deﬁned as the set of all linear combinations of the rows of A. Hence row(A)
is a subspace of Fn = F1×n. Recall that if B is obtained from A by row
operations, then row(A) = row(B). One of our main results on matrix theory
is that two matrices in reduced row echelon form are equal if and only if
they have the same row space. Because of this fact, we called the number of
nonzero rows in A’s reduced row echelon form the rank of A.
⊠
Example 6.16. A matrix A ∈Fm×n also has a column space col(A): namely,
the set of all linear combinations of the columns of A. The column space is a
subspace of Fm = Fm×1. The column space has an important interpretation
in terms of linear systems. If A has columns a1, . . . , an, then by deﬁnition, b ∈
col(A) if and only if there are scalars c1, . . . , cn such that b = c1a1+· · ·+cnan.
Thus the column space of A consists of all b ∈Fm for which the linear system
Ax = b has a solution.
⊠
Example 6.17. Assume that a and b are vectors in F3. The cross product
a × b is deﬁned to be
a × b = (a2b3 −a3b2, −(a1b3 −a3b1), a1b2 −a2b1)T .
(6.2)
By direct calculation, aT (a×b) = bT (a×b) = 0. Furthermore, if a and b are
noncollinear, it can be seen that a × b ̸= 0. We thus obtain a homogeneous
equation satisﬁed by all vectors in span{a, b}: if a × b = (r, s, t)T , then such
an equation is rx + sy + tz = 0. In the case F = R, this is interpreted as
meaning that a × b is orthogonal to the plane spanned by a and b.
⊠
Exercises
Exercise 6.2.1. Which of the following subsets of R2 are not subspaces?
(i) The line x = y;
(ii) The unit circle;
(iii) The line 2x + y = 1;
(iv) The ﬁrst octant x, y ≥0.
Exercise 6.2.2. Prove that all lines through the origin and planes through
the origin in R3 are subspaces.
Exercise 6.2.3. Let p be a prime. Show that a subset of V (n, p) that is
closed under addition is a subspace.
Exercise 6.2.4. Assume n > 1. Find a subspace of V (n, p) that contains p
points. Next ﬁnd a subspace that contains p2 points.

144
6
Vector Spaces
Exercise 6.2.5. Show that the plane x + y + z = 0 in V (3, 2) has four
elements. Find a spanning set for this plane.
Exercise 6.2.6. Find an equation for the plane in V (3, 2) through the origin
containing both (1, 1, 1) and (0, 1, 1).
Exercise 6.2.7. Find a spanning set in V (4, 2) for the solution space of the
equation w +x+y +z = 0. How many solutions in V (4, 2) does this equation
have?
Exercise 6.2.8. Find a spanning set for the plane 3ix −y + (2 −i)z = 0
in C3.
Exercise 6.2.9. Find an equation for the plane in R3 through the origin
containing both (1, 2, −1)T and (3, 0, 1)T .
Exercise 6.2.10. Describe all subspaces of C3. What about C4?
Exercise 6.2.11. Find the number of subspaces of the vector space V (n, p)
in the following cases:
(i) n = p = 2;
(ii) n = 2, p = 3; and
(iii) n = 3, p = 2.
Exercise 6.2.12. Let F be an arbitrary ﬁeld. Show that if a, b ∈F3, then
a × b ̸= 0 if and only if a and b are not collinear.

6.3
Linear Independence and Bases
145
6.3
Linear Independence and Bases
As usual, let V denote a vector space over an arbitrary ﬁeld F. In order to
understand the structure of V , especially its dimension, we need to intro-
duce two new ideas: linear independence and bases. Linear independence is
about uniquely representing the elements of V as linear combinations, and the
notion of a basis concerns both linear independence and spanning the whole
of V . As we mentioned in the introduction, the notion of the dimension of V
depends on the existence of a basis.
6.3.1
The deﬁnition of linear independence
To put it informally, a nonempty set of vectors is linearly independent if no
one of them is a linear combination of the others. For example, two vectors
are linearly independent if they aren’t collinear, and three vectors are linearly
independent if they don’t all lie in a plane through the origin. That is, they
aren’t coplanar. In general, two, three, or any ﬁnite number of vectors fail to
be linearly independent when they are subject to a linear constraint. Let us
now state this formally.
Deﬁnition 6.4. Let v1, . . . , vk be in V . Then we say that v1, . . . , vk are
linearly independent (or simply independent) if the equation
a1v1 + a2v2 + · · · + akvk = 0,
(6.3)
with a1, a2, . . . , ak ∈F, is satisﬁed only when a1 = a2 = · · · = ak = 0. If
(6.3) has a nontrivial solution (i.e., some ai ̸= 0), we say that v1, . . . , vk are
linearly dependent (or simply dependent).
We will also say that a nonempty ﬁnite subset S of V is independent or
dependent if the vectors that are its elements are respectively independent or
dependent. Notice that if two of the vi coincide or if one of them is zero, then
v1, . . . , vk are dependent. Notice also that we are deﬁning linear independence
only for a ﬁnite number of vectors. The reader might want to contemplate
how to do this for inﬁnite sets. Another way to think about independence is
pointed out by the following proposition.
Proposition 6.5. A set of vectors is linearly dependent if and only if one
of them can be expressed as a linear combination of the others.
Proof. Let v1, . . . , vk be the vectors, and suppose one of the vectors, say v1,
is a linear combination of the others. Then
v1 = a2v2 + · · · + akvk.

146
6
Vector Spaces
Thus
v1 −a2v2 −· · · −akvk = 0,
so (6.3) has a nontrivial solution with a1 = 1. Therefore, v1, . . . , vk are depen-
dent. Conversely, suppose v1, . . . , vk are dependent. This means that there
is a nontrivial solution a1, a2, . . . , ak of (6.3). We can assume (by reindexing
the vectors) that a1 ̸= 0. Thus
v1 = b2v2 + · · · + bkvk,
where bi = −ai/a1, for i ≥2, so the proof is done.
□
Note how the fact that F is a ﬁeld was used in the above proof. The next
proposition gives one of the important properties of linearly independent sets.
Proposition 6.6. Assume that v1, . . . , vk ∈V are linearly independent, and
suppose v is in their span. Then v = k
i=1 aivi for exactly one linear com-
bination of v1, . . . , vk.
Proof. By assumption, there exists an expression
v = r1v1 + r2v2 + · · · + rkvk,
where r1, . . . , rk ∈F. Suppose there is another expression, say
v = s1v1 + s2v2 + · · · + skvk,
where the si are also elements of F. By subtracting the second expression
from the ﬁrst and collecting terms, we get that
0 = v −v = (r1 −s1)v1 + (r2 −s2)v2 + · · · + (rk −sk)vk.
Since the vi are independent, every coeﬃcient ri −si is equal to 0.
□
When V = Fm, checking linear independence involves solving a homoge-
neous linear system. Viewing vectors in Fm as column vectors, consider the
m × n matrix
A = (a1 · · · an).
By the theory of linear systems, we have the following result.
Proposition 6.7. The vectors a1, . . . , an in Fm are linearly independent
exactly when the system Ax = 0 has only the trivial solution, that is, when
N(A) = {0}. In particular, the columns a1, . . . , an of A are independent if
and only if rank(A) = n, so more than m vectors in Fm are linearly depen-
dent.

6.3
Linear Independence and Bases
147
Proof. The ﬁrst statement follows from the deﬁnitions. The second follows
from the identity rank(A) + # free variables = n. Since rank(A) ≤m, there
exist free variables whenever n > m, so N(A) ̸= {0}. Thus a1, . . . , an are
dependent when n > m.
□
6.3.2
The deﬁnition of a basis
We will now explain the second main ingredient in the notion of dimension.
A basis combines the notions of independence and spanning. From now on,
we will restrict our attention to vector spaces that have a ﬁnite spanning set.
Thus we make the following deﬁnition.
Deﬁnition 6.5. A vector space V is said to be ﬁnite-dimensional if V has
a ﬁnite spanning set.
Note that the trivial vector space is ﬁnite-dimensional, since it is spanned
by 0. Vector spaces such as the space of all continuous functions on [a, b] are
therefore excluded from our considerations. Here, ﬁnally, is the deﬁnition of
a basis.
Deﬁnition 6.6. A collection of vectors in V that is linearly independent
and spans V is called a basis of V .
One of the main results we will prove is that every ﬁnite-dimensional vector
space has a basis. In fact, every vector space has a basis, but the proof that a
spanning set exists in the inﬁnite-dimensional case is beyond our scope. For
the remainder of this section, we will consider examples of bases.
Example 6.18 (The standard basis of Fn). Recall that ei denotes the ith
column of In. Then {e1, . . . , en} is called the standard basis of Fn. Since
⎛
⎜
⎝
a1
...
an
⎞
⎟
⎠= a1e1 + · · · + anen
and In has rank n, it follows that e1, . . . , en indeed give a basis of Fn.
⊠
Example 6.19 (Lines and planes). A nonzero vector in Rn spans a line
through 0, and clearly a single nonzero vector is linearly independent. Hence
a line through 0 has a basis consisting of a single element. (The choice of basis
is not unique unless F = F2.) A plane P containing the origin is spanned by
any pair of noncollinear vectors in P, and two noncollinear vectors in P are
linearly independent. Thus P has a basis consisting of two vectors. (The
choice of basis is again not unique unless F = F2.)
⊠

148
6
Vector Spaces
It should be noted that the trivial vector space {0} does not have a basis,
since in order to contain a linearly independent subset it has to contain a
nonzero vector. The next result gives an elementary but useful property of
bases.
Proposition 6.8. The vectors v1, . . . , vn in V form a basis of V if and only
if every vector v in V admits a unique expression
v = a1v1 + a2v2 + · · · + anvn,
where a1, a2, . . . , an are elements of F.
Proof. We leave this as an exercise.
□
Remark. Proposition 6.8 shows that a basis B of V sets up a one-to-one
correspondence, or bijection, ΦB : V →Fn by ΦB(v) = (a1, a2, . . . , an)T .
The mapping ΦB also preserves linear combinations; that is, ΦB(av + bw) =
aΦB(v) + bΦB(w) for all a, b ∈F and v, w ∈V . In particular, ΦB is an
isomorphism in the sense of abelian groups between V and Fn. The n-tuple
(a1, a2, . . . , an)T assigns coordinates to v with respect to the basis B. We will
discuss coordinates in detail in the next chapter.
Proposition 6.9. Let A ∈Fm×n. If A has rank m, its rows are a basis of
row(A). If A has rank n, its columns are a basis of col(A).
Proof. We leave the proof to the reader.
□
If the rank of A is less than n, Proposition 6.7 tells us that its columns
are dependent. However, the columns still span col(A), so it is natural to ask
whether there is a basis of col(A) consisting of some of A’s columns. This is
the problem of ﬁnding a basis of V contained in a spanning set. The solution
is treated below.
Example 6.20 (Basis of the null space). Let A be an m × n matrix over F.
The basic null vectors span the null space N(A). They are also independent,
as can be seen by writing out the equation for independence and looking
at the corner components. Thus the basic null vectors determine a basis of
N(A).
⊠
Exercises
Exercise 6.3.1. Determine whether (0, 2, 1, 0)T , (1, 0, 0, 1)T , and (1, 0, 1, 1)T
are linearly independent as vectors in R4. If so, do they form a basis of R4?
Exercise 6.3.2. Are (0, 0, 1, 0)T , (1, 0, 0, 1)T , and (1, 0, 1, 1)T independent
in V (4, 2) = (F2)4?

6.3
Linear Independence and Bases
149
Exercise 6.3.3. Show that every nonempty subset of a linearly independent
set is linearly independent.
Exercise 6.3.4. We say that u1, u2, . . . , uk ∈Rn are mutually orthogonal
unit vectors if uT
i uj = 0 whenever i ̸= j and uT
i ui = 1 for all i. Show that
if u1, u2, . . . , uk are mutually orthogonal unit vectors, then they are linearly
independent.
Exercise 6.3.5. Show that m linearly independent vectors in Fm are a basis.
Exercise 6.3.6. Prove the assertion made in Example 6.20 that the basic
null vectors are a basis of N(A).
Exercise 6.3.7. Use the theory of linear systems to show the following:
(i) More than m vectors in Fm are dependent.
(ii) Fewer than m vectors in Fm cannot span Fm.
Exercise 6.3.8. Let u, v, and w be a basis of R3.
(i) Determine whether 3u + 2v + w, u + v + 0w, and −u + 2v −3w are
independent.
(ii) Do the vectors in part (i) span R3? Supply reasoning.
(iii) Find a general necessary and suﬃcient condition for the vectors a1u +
a2v + a3w, b1u + b2v + b3w, and c1u + c2v + c3w to be independent, where
the ai, bj, ck are arbitrary scalars.
Exercise 6.3.9. Suppose V is a vector space that contains an inﬁnite subset
S such that every ﬁnite nonempty subset of S is linearly independent. Show
that V cannot be a ﬁnite-dimensional vector space.
Exercise 6.3.10. Recall that R[x] denotes the space of all polynomials with
coeﬃcients in R. For each positive integer m, let R[x]m ⊂R[x] denote the
subset of all polynomials of degree at most m.
(i) Show that R[x]m is a subspace of R[x].
(ii) Show that the powers 1, x, . . . , xm are linearly independent for all positive
integers m. (Hint: use induction.)
(iii) Find a basis for each R[x]m.
(iv) Show that R[x] is not ﬁnite-dimensional.
(v) Exhibit (without proof) a basis for R[x].
Exercise 6.3.11. Suggest a deﬁnition for the notion of a basis of a vector
space that isn’t ﬁnite-dimensional.

150
6
Vector Spaces
Exercise 6.3.12. True or false: v1, . . . , vr ∈V (n, 2) are linearly indepen-
dent if and only if v1 + · · · + vr ̸= 0.
Exercise 6.3.13. Suppose B = {v1, . . . , vn} is a basis of V . Consider the
mapping ΦB : V →Fn deﬁned by ΦB(v) = (a1, a2, . . . , an)T if v = a1v1 +
· · · + anvn. Show that ΦB is a bijection that preserves linear combinations;
that is, ΦB(av + bw) = aΦB(v) + bΦB(w) for all a, b ∈F and v, w ∈V .
Conclude that ΦB is an isomorphism in the sense of abelian groups between
V and Fn.

6.4
Bases and Dimension
151
6.4
Bases and Dimension
Much of the groundwork for the deﬁnition of the dimension of a ﬁnite-
dimensional vector space has now been laid, and the ever alert reader has
undoubtedly guessed that the dimension is the number of vectors in a basis.
However, there is still the question whether a basis exists and whether all
bases have the same number of elements. It turns out that answering this
question is nontrivial.
6.4.1
The deﬁnition of dimension
Throughout this section, V denotes a ﬁnite-dimensional vector space over a
ﬁeld F. Here is one of the most important deﬁnitions in the theory of vector
spaces.
Deﬁnition 6.7. Suppose V ̸= {0}. Then the dimension of V is deﬁned to be
the number of elements in a basis of V . The dimension of V will be denoted
by dim V or by dimF V in case there is a chance of confusion about which
ﬁeld is being considered. When V = {0}, we will deﬁne the dimension of V
to be 0.
As already mentioned, this deﬁnition is based on two assertions: every
nontrivial ﬁnite-dimensional vector space V has a basis, and any two bases
have the same number of elements. These claims, being far from obvious, need
to be proved. They comprise the dimension theorem, which will be stated and
proved below.
As we already noted above (also see Exercise 6.3.7), Fn can’t contain more
than n independent vectors, and fewer than n vectors can’t span. This implies
that every basis of Fn has n elements. We also know that there is a basis with
n vectors, namely the standard basis. This implies the following.
Proposition 6.10. For every ﬁeld F, Fn has a basis, and every basis has n
elements. Thus, dim Fn = n.
An intuitive interpretation of dim V is either the maximal number of inde-
pendent vectors in V or the minimal number of spanning vectors, provided
these are the same. This deﬁnition certainly gives the correct result for Fn as
just noted.
There is a subtlety in the deﬁnition of dimension that is worth pointing
out. Namely, V can often be viewed as a vector space over diﬀerent ﬁelds,
so the ﬁeld has to be speciﬁed when talking about V ’s dimension. In fact,
suppose V is a ﬁnite-dimensional vector space over F. When F′ is a subﬁeld
of F, then V is automatically also a vector space over F′. In this case, dimF V

152
6
Vector Spaces
and dimF′ V will be diﬀerent if F ̸= F′. For example, let F = C and V = Cn.
By the previous proposition, dimC V = n. But R is a subﬁeld of C, and in
fact Cn is still a ﬁnite-dimensional vector space over R. Indeed, since C = R2,
Cn = R2n. Thus, dimR Cn = 2n. In general, if V is a ﬁnite-dimensional vector
space over C with dimC V = n, then dimR V = 2n.
6.4.2
Some examples
Before proving the dimension theorem, let’s consider some examples.
Example 6.21 (Lines and Planes). Let V = Fn. If v ̸= 0, then a line Fv in V
has a basis consisting of v and hence has dimension one. If P = span{a1, a2},
where a1 and a2 are noncollinear, then dim V = 2.
⊠
Example 6.22 (Dimension of a Hyperplane). Again, let V = Fn. Then we
saw that dim V = n. The dimension of the hyperplane a1x1 + · · · + anxn = 0
in V is n −1, provided some ai in nonzero, since the n −1 basic null vectors
form a basis of the hyperplane (see Example 6.20).
⊠
Example 6.23 (Dimension of Fm×n). As noted earlier, the vector space
Fm×n of m × n matrices over F is indistinguishable from Fmn, so we would
expect that dim Fm×n = mn. Now, the matrix analogue of the standard basis
of Fn is the set of m × n matrices Eij that have a 1 in the ith row and jth
column and a zero everywhere else. We leave the proof that they form a basis
as an exercise. Therefore, dim Fm×n = mn, as expected.
⊠
Example 6.24. By Exercise 6.3.10, we see that if dim R[x]m denotes the
space of real polynomials of degree at most m. then dim R[x]m = m + 1 for
all m ≥0, a basis being 1, x, . . . , xm.
⊠
Example 6.25. Let a1, . . . , am be real constants. Then the solution space
of the homogeneous linear diﬀerential equation
y(m) + a1y(m−1) + · · · + am−1y′ + amy = 0
is a vector space over R. It turns out, by a theorem on diﬀerential equations,
that the dimension of this space is m. For example, when m = 4 and ai = 0
for 1 ≤i ≤4, then the solution space is the vector space R[x]3 of the previous
example. The solution space W of the equation y′′+y = 0 consists of all linear
combinations of the functions sin x and cos x. We leave it as an exercise to
show that sin x and cos x are linearly independent, so dim W = 2.
⊠
Example 6.26 (Symmetric n × n matrices). Let Fn×n
s
denote the set of
symmetric n × n matrices over F. Now, Fn×n
s
is certainly a subspace of Fn×n
(exercise). The basis {Eij | 1 ≤i, j ≤n} of Fn×n doesn’t contain a basis of

6.4
Bases and Dimension
153
Fn×n
s
, however, since Eij isn’t symmetric if i ̸= j. To repair this problem, we
put Sij = Eij + Eji when i ̸= j. Then Sij ∈Fn×n
s
, and I claim that the Sij
(1 ≤i < j ≤n) together with the Eii (1 ≤i ≤n) are a basis of Fn×n
s
. They
certainly span Fn×n
s
, since if A = (aij) is symmetric, then
A =

i<j
aij(Eij + Eji) +

i
aiiEii.
We leave it as an exercise to verify that this spanning set is also independent.
In particular, counting the number of basis vectors, we see that
dim Fn×n
s
= (n −1) + (n −2) + · · · + 2 + 1 + n = n(n + 1)/2,
by the well-known formula for the sum of the ﬁrst n positive integers.
⊠
Example 6.27 (Skew-symmetric matrices). A square matrix A ∈Fn×n is
called skew-symmetric if AT = −A. The set Fn×n
ss
of skew-symmetric n × n
matrices over F is another interesting subspace of Fn×n. If the characteristic
of the ﬁeld F is two, then skew-symmetric and symmetric matrices are the
same thing, so for the rest of this example suppose char(F) ̸= 2. For example,
if

a b
c d
T
= −

a b
c d

,
then a = −a, d = −d, b = −c, and c = −b. Thus a 2 × 2 skew-symmetric
matrix has the form

0
b
−b 0

,
so E12 −E21 is a basis. We leave it as an exercise to show that dim Fn×n
ss
=
n(n −1)/2 for all n.
⊠
6.4.3
The Dimension Theorem
We will now prove the dimension theorem. This settles the question whether
a basis exists and the deﬁnition of dimension makes sense.
Theorem 6.11 (The dimension theorem). Let V denote a ﬁnite-dimensional
vector space with at least one nonzero element. Then V has a basis. In fact,
every spanning set for V contains a basis, and every linearly independent
subset of V is contained in a basis. Moreover, any two bases of V have the
same number of elements.

154
6
Vector Spaces
Proof. We’ll begin by showing that every ﬁnite spanning set contains a basis.
Let w1, . . . , wk span V , and consider the set of all subsets of {w1, . . . , wk}
that also span V . Let {v1, . . . , vr} be any such subset where r is minimal.
There is no problem showing that such a subset exists, since {w1, . . . , wk}
has only ﬁnitely many subsets. We now show that v1, . . . , vr are independent.
So suppose
a1v1 + · · · + arvr = 0,
but ai ̸= 0. Then
vi = −1
ai

j̸=i
ajvj,
so if vi is deleted from {v1, . . . , vr}, we still have a spanning set, contradicting
the minimality of r. Thus v1, . . . , vr are independent, so every spanning set
contains a basis. In particular, since V has a ﬁnite spanning set, it has a
basis.
We next show that every linearly independent set in V can be extended to
a basis. Let w1, . . . , wm be independent, and put W = span{w1, . . . , wm}.
I claim that if v /∈W, then w1, . . . , wm, v are independent. To see this,
suppose
a1w1 + · · · + amwm + bv = 0.
If b ̸= 0, it follows (as in the last argument) that v ∈W, contrary to the
choice of v. Thus b = 0. But then each ak is equal to zero as well, since the
wi are independent. Now suppose W ̸= V . We will use the basis v1, . . . , vr
of V obtained above to obtain a basis containing w1, . . . , wm. If each vi is
in W, then W = V , contrary to assumption. So let i be the ﬁrst index such
that vi /∈W. By the previous paragraph, w1, . . . , wm, vi are independent.
Hence they form a basis for W1 = span{w1, . . . , wm, vi}. Repeating this
construction with W1 replacing W and so on, we eventually obtain a subspace
Wk that contains all vj. Thus Wk = V , so the basis of Wk just constructed is a
basis of V containing w1, . . . , wm. This proves that every linearly independent
set is contained in a basis.
It remains to show that any two bases of V have the same number of
elements. Suppose u1, . . . , um and v1, . . . , vn are two bases of V . If m ̸= n,
we may assume without loss of generality that m ≤n. By deﬁnition, we can
certainly write
v1 = r1u1 + r2u2 · · · + rmum.
(6.4)
Since v1 ̸= 0, some ri is nonzero, so we may suppose, by renumbering the
indices of the uj if necessary, that r1 ̸= 0. I claim that v1, u2, . . . , um is also
a basis of V . To see this, we must show that v1, u2, . . . , um are independent
and span. Suppose that

6.4
Bases and Dimension
155
x1v1 + x2u2 · · · + xmum = 0.
If x1 ̸= 0, then
v1 = y2u2 + · · · + yjum,
where yi = −xi/x1. Since r1 ̸= 0, we get two distinct ways of expanding
v1 in terms of the ﬁrst basis, which contradicts the uniqueness statement in
Proposition 6.8. Hence x1 = 0. It follows immediately that all xi are equal to
zero (why?), so v1, u2, . . . , um are independent. The fact that v1, u2, . . . , um
span V follows from (6.4), since r1 ̸= 0. Hence we have produced a new basis
of V in which v1 replaces u1. Now write
v2 = x1v1 + x2u2 + · · · + um.
Since v1, . . . , vm are independent, there exists j ≥2 such that xj ̸= 0. Thus,
after reindexing again, we can assume that x2 ̸= 0. Repeating the above
argument, we see that u2 can be replaced by v2, giving another new basis
v1, v2, u3 . . . , um of V . Continuing this process, we will eventually replace all
the ui, which implies that v1, . . . , vm must be a basis of V . But if m < n,
it then follows that vn is a linear combination of v1, . . . , vm, contradicting
the linear independence of v1, . . . , vn. Thus m = n, and the proof of the
dimension theorem is ﬁnished.
□
The dimension theorem has several useful consequences.
Corollary 6.12. If W is a subspace of a ﬁnite-dimensional vector space V ,
then W is ﬁnite-dimensional, and dim W ≤dim V with equality exactly when
W = V . In particular, every subset of V containing more that dim V elements
is dependent.
Proof. This is an exercise.
□
Corollary 6.13. If dim V = m, then every set of m linearly independent
vectors in V forms a basis. Similarly, every set of m vectors that span V is
also a basis.
Proof. This is also an exercise.
□
Concentrating on linear systems, we may consider the following example.
Example 6.28 (Linear systems). By Example 6.20, we know that dim N(A)
is the number of free variables in the system Ax = 0. Thus the fundamental
identity (3.11) for an m × n homogeneous linear system Ax = 0 can now be
expressed in terms of dimension as follows:

156
6
Vector Spaces
dim N(A) + rank(A) = n.
(6.5)
However, we can say even more. The rows of Ared are certainly linearly
independent (why?), so they form a basis of row(A), the span of the rows of
A. Thus, rank(A) = dim row(A), so for every A ∈Fm×n, we also have
dim N(A) + dim row(A) = n.
(6.6)
This seems to be a more elegant statement than that of (3.11). There is yet
another improvement from knowing rank(A) = rank(AT ). Since rank(A) =
dim row(A), we have dim row(A) = dim row(AT ) = dim col(A). Thus,
dim N(A) + dim col(A) = n.
(6.7)
Since col(A) = {b ∈Fm | Ax = b ∃x ∈Fn}, dim N(A) and dim col(A)
refer to the linear system Ax = b. (Recall that understanding this equation
is one of our main motivations.) The identity (6.7) is sometimes called the
rank–nullity identity.
⊠
6.4.4
Finding a basis of the column space
Suppose {a1, . . . , an} is a spanning set for a subspace W of Fm. We know
that some subset of this set is a basis. Is there is an eﬃcient procedure for
extracting a basis of W? If we view the ai as column vectors and form the
m × n matrix A =
a1 a2 · · · an

, then W = col(A). Thus we know the
dimension of W: dim W = rank(A). Hence by Corollary 6.13, it suﬃces to
ﬁnd rank(A) independent columns. Somewhat surprisingly, one can proceed
by ﬁnding Ared. This follows from the next proposition.
Proposition 6.14 The columns of a matrix A ∈Fm×n that correspond to a
corner entry in Ared are a basis of col(A).
Proof We can assume that A has rank m. As shown above, it suﬃces to
show that the columns of A that correspond to a corner entry in Ared are
independent. Since Ared = BA with B invertible, it follows that Ax = 0 if
and only if Aredx = 0. In particular, every expression of linear dependence
among a subset of the columns of A is also an expression of linear dependence
among those columns of Ared. For example, if the ﬁfth column of A is the
sum of the ﬁrst four columns of A, this also holds for the columns of Ared.
of Ared containing a corner entry are standard basis vectors of Fm, hence

6.4
Bases and Dimension
157
are certainly independent. Therefore, these columns of A are also linearly
independent. Since dim col(A) = rank(A), Corollary 6.13 says that these
columns of A are a basis of W.
□
Example 6.29 For example, suppose
A =
⎛
⎝
1 2
2
4 5
8
7 8 14
⎞
⎠.
Then
Ared =
⎛
⎝
1 0 1
0 1 0
0 0 0
⎞
⎠.
Proposition 6.14 implies that the ﬁrst two columns are a basis of col(A).
Notice that the ﬁrst and third columns are dependent in both A and Ared,
as the proof shows must happen. Proposition 6.14 says that the ﬁrst two
columns are a basis of the column space, but it makes no assertion about the
second and third columns, which in fact are also a basis.
⊠
6.4.5
A Galois ﬁeld application
Let p be a prime and consider a vector space V over the prime ﬁeld Fp. Then
the dimension of V determines the number of elements of V as follows.
Proposition 6.15 If V is ﬁnite-dimensional, then the number of elements
of V is exactly pdim V .
Proof Let k = dim V and choose a basis w1, . . . , wk of V . By Proposition
6.8, every v ∈W has a unique expression
v = a1w1 + a2w2 + · · · + akwk,
where a1, a2, . . . , ak ∈Fp. Now it is simply a matter of counting such expres-
sions. In fact, since Fp has p elements, there are p choices for each ai, and
since uniqueness says that diﬀerent choices of the ai give diﬀerent elements of
V , it follows that there are exactly p·p · · · p = pk distinct linear combinations.
Thus V contains exactly pk elements.
□
For example, a line in (Fp)n has p elements, a plane has p2, and so forth.
We can apply the last result to deduce a beautiful fundamental fact about
Galois ﬁelds. Let F be a Galois ﬁeld. Then the characteristic of F is a prime,
say p. By Exercise 2.6.8, the multiples of 1 together with 0 form a subﬁeld

158
6
Vector Spaces
with p elements. This subﬁeld is indistinguishable from Fp, but we will denote
it by F′. It follows from the ﬁeld axioms that F is a vector space over F′ (see
Example 6.11). Moreover, since F itself is ﬁnite, it follows by deﬁnition that
F is ﬁnite-dimensional (over F′), since every a ∈F has the expression a = 1a,
so F spans itself over F′, since 1 ∈F′. Applying Proposition 6.15, we get the
following result.
Proposition 6.16 Let F be a ﬁnite ﬁeld of characteristic p. Then |F| = pn,
where n is the vector space dimension of F over the subﬁeld F′ of F consisting
of all multiples of 1: that is, n = dimF′ F.
Recall that in Section 2.6.2, we considered a ﬁeld F = {0, 1, α, β}, where
α + β = 1. Thus, for example, {1, α} is a basis, and F has 4 = 22 elements,
in agreement with the above result. It can be shown that for every prime p
and every n > 0, there is a unique Galois ﬁeld of order pn. Combining the
previous two results, we get the following corollary.
Corollary 6.17 Let F be a Galois ﬁeld of characteristic p, and let q = pn
denote |F|. Then if V is a ﬁnite-dimensional vector space over F, we have
|V | = qdimFV = pn dimF V .
Here is another corollary.
Corollary 6.18 Let F be a Galois ﬁeld with q = pm elements. Then the
general linear group GL(n, F) has order
|GL(n, F)| = (qn −1)(qn −q)(qn −q2) · · · (qn −qn−1).
(6.8)
Proof The elements of GL(n, F) may be described as the elements A of Fn×n
whose columns form a basis of Fn. Consequently, the ﬁrst i columns of A have
to form a basis of an i-dimensional subspace of Fn. Thus, there are qn −1
choices for A’s ﬁrst column, qn −q choices for its second column, since the
only restriction on the second column is that it not lie on the line spanned
by the ﬁrst column, and in general, there are qn −qi−1 choices for the ith
column, since the span of the ﬁrst i −1 columns has qi−1 elements.
□
Exercises
Exercise 6.4.1 Find a basis for the subspace of R4 spanned by
(1, 0, −2, 1)T , (2, −1, 2, 1)T , (1, 1, 1, 1)T , (0, 1, 0, 1)T , (0, 1, 1, 0)T
that contains the ﬁrst and ﬁfth vectors.

6.4
Bases and Dimension
159
Exercise 6.4.2 Consider the matrix A =
⎛
⎝
1
2
0
1
2
2
0
1
−1
2
1
1
−1
1
0
⎞
⎠as an
element of R3×5.
(i) Show that the basic null vectors of Ax = 0 are a basis of N(A).
(ii) Find a basis of col(A).
(iii) Repeat (i) and (ii) when A is considered as a matrix over F3.
Exercise 6.4.3 Prove that cos x and sin x are linearly independent on the
open interval (0, 2π).
Exercise 6.4.4 Let W be a subspace of V , and let w1, . . . , wk ∈W. Show
that if w1, . . . , wk are independent, then w1, w2, . . . , wk, v are also indepen-
dent for every v ∈V such that v /∈W.
Exercise 6.4.5 Prove Corollary 6.12. That is, suppose V is a ﬁnite-dimensional
vector space over a ﬁeld F, and let W be a subspace of V . Show the following:
(i) W is ﬁnite-dimensional.
(ii) In fact, dim W ≤dim V .
(iii) If dim W = dim V , then W = V .
Exercise 6.4.6 Prove Corollary 6.13. That is, show that if dim V = m, then
every set of m linearly independent vectors in V forms a basis, and similarly,
every set of m vectors that span V is also a basis.
Exercise 6.4.7 True or false: For every A ∈Fn×n, rank(A) ≥rank(A2).
Prove your answer.
Exercise 6.4.8 Consider the subspace W of V (4, 2) = (F2)4 spanned by
1011, 0110, and 1001.
(i) Find a basis of W and compute |W|.
(ii) Extend your basis to a basis of (F2)4.
Exercise 6.4.9 Construct at least two proofs of the statement that for every
matrix A, dim col(A) = dim row(A).
Exercise 6.4.10 Let A and B be n × n matrices.
(i) Show that N(A) ⊂N(BA). When is N(BA) = N(A)?
(ii) Show that col(A) ⊃col(AB). When is col(AB) = col(A)?
(iii) Show that AB = O if and only if col(B) ⊂N(A).

160
6
Vector Spaces
Exercise 6.4.11 Consider the set Fn×n
s
of symmetric n × n matrices
over F.
(i) Show that Fn×n
s
is a subspace of Fn×n.
(ii) Show that the set of matrices Sij with i < j deﬁned in Example 6.26
together with the Eii make up a basis of Fn×n
s
.
Exercise 6.4.12 Let Fn×n
ss
be the n × n skew-symmetric matrices over F.
(i) Show that Fn×n
ss
is a subspace of Fn×n.
(ii) Find a basis of Fn×n
ss
and compute its dimension.
(iii) Find a basis of Fn×n that uses only symmetric and skew-symmetric
matrices.
Exercise 6.4.13 Let F be a ﬁeld, and suppose V and W are subspaces
of Fn.
(i) Show that V ∩W is a subspace of Fn.
(ii) Let V + W = {u ∈Fn | u = v + w, where v ∈V, w ∈W}. Show that
V + W is a subspace of Fn.
(iii) Show that dim(V + W) ≤dim V + dim W.
Exercise 6.4.14 Let W and Y be subspaces of a vector space V of dimen-
sion n. What are the minimum and maximum dimensions that W ∩X can
have? Discuss the case that W is a hyperplane (i.e., dim W = n −1) and X
is a plane (i.e., dim X = 2).
Exercise 6.4.15 Suppose X is a ﬁnite set, say |X| = n, and let F = Fp. Let
V = FX. That is, V is the set of all maps with domain X and target F. Show
that V is an F-vector space, ﬁnd a basis, and compute dim V .
Exercise 6.4.16 Show that the set of n × n upper triangular matrices over
F is a subspace of Fn×n. Find a basis and its dimension.
Exercise 6.4.17 Let F be a Galois ﬁeld of characteristic p, and let F′ be
the subﬁeld of F consisting of all multiples of 1. If V is a ﬁnite-dimensional
vector space over F, show that dimF′ V = dimF′ F dimF V . Conclude that
|V | = pdimF′ F dimF V .
Exercise 6.4.18 Let |F| = q, where q = pn, p a prime. Show that every
element of F is a root of the polynomial xq −x ∈Fp[x].
Exercise 6.4.19 Let V = R as a vector space over Q. Is dim V ﬁnite or
inﬁnite? Discuss.

6.4
Bases and Dimension
161
Exercise 6.4.20 Let V be a vector space over Fp of dimension n. A linearly
independent subset of V with m elements is called an m-frame in V . Show
that the number of m-frames in V is exactly
(pn −1)(pn −p) · · · (pn −pm−2)(pn −pm−1).
(Use Proposition 6.15 and part of the proof of the dimension theorem.)
Exercise 6.4.21 Use Exercise 6.4.20 to show that the number of subspaces
of dimension m in an n-dimensional vector space V over Fp is
(pn −1)(pn −p) · · · (pn −pm−2)(pn −pm−1)
(pm −1)(pm −p) · · · (pn −pm−2)(pm −pm−1).
(The set of m-dimensional subspaces of a ﬁnite-dimensional vector space is
an important object, called a Grassmann variety.)
Exercise 6.4.22 Let V be a vector space. A complete ﬂag in V is a sequence
of subspaces
V1 ⊂V2 ⊂· · · ⊂Vn−1 ⊂V
such that dim Vi = i. The set Flag(V ) of all complete ﬂags in V is called the
ﬂag variety of V .
(i) Let B ⊂GL(n, F) denote the set of all invertible upper triangular elements
of Fn×n. Show that there exists a bijection
Φ : GL(n, F)/B →Flag(Fn).
(ii) Suppose F is Galois. Find a formula for the number of ﬂags in Fn. That
is, ﬁnd |Flag(Fn)|.
Exercise 6.4.23 (This is more challenging.) Consider a square array of
lights numbered 1 to 9 inclusively: that is,
1
2
3
4
5
6
7
8
9
Turning a light on or oﬀalso changes all the on–oﬀstates of the lights directly
above and below and to the left and right. For instance, if the center light
(light number 5) is turned on, then lights 2, 4, 5, 6, and 8 also change their
state.
(i) If all the lights are oﬀ, can they all be turned on?
(ii) How can you turn on just light number 1?
(iii) Is it possible to turn all the lights oﬀfrom every starting conﬁguration?

162
6
Vector Spaces
6.5
The Grassmann Intersection Formula
The intersection of a pair of nonparallel planes P1 and P2 in R3 through
the origin is a line through the origin. This may seem obvious, but what
would the answer be if instead of being planes in R3, P1 and P2 were three-
dimensional subspaces of R4? The purpose of this section is to answer this
question; that is, if W and Y are arbitrary subspaces of a ﬁnite-dimensional
vector space V , what is dim(W ∩Y )? The answer is given by the Grassmann
intersection formula, which gives dim(W ∩Y ) in terms of the dimensions of
W, Y , and a subspace W + Y known as the sum of W and Y . After that, we
will introduce direct sums of subspaces and will derive conditions for sums
of subspaces to be direct. These results on direct sums will be used when we
study eigentheory in Chap. 8.
6.5.1
Intersections and sums of subspaces
Let V be a vector space over a ﬁeld F with subspaces W and Y . The simplest
way of building a new subspace is by taking the intersection W ∩Y .
Proposition 6.19 The intersection W ∩Y of the subspaces W and Y of
V is also a subspace of V . More generally, the intersection of an arbitrary
collection of subspaces of V is also a subspace.
Proof This is an exercise.
□
Proposition 6.19 is a generalization of the fact that the solution space of
a homogeneous linear system is a subspace of Fn. The solution space of a
single homogeneous linear equation is a hyperplane in Fn, and so the solution
space of a homogeneous linear system is the intersection of a ﬁnite number
of hyperplanes in Fn.
Another simple way of forming a new subspace is to take the subspace
spanned by W and Y . This is deﬁned as follows.
Deﬁnition 6.8 The subspace spanned by W and Y or alternatively, the sum
of W and Y , is deﬁned to be the set of sums
W + Y = {w + y | w ∈W, y ∈Y }.
More generally, we can form the sum V1 + · · · + Vk of an arbitrary (ﬁnite)
number of subspaces V1, V2, . . . , Vk of V . The sum V1 +· · ·+Vk is also written
as k
i=1 Vi, or more simply  Vi.
Proposition 6.20 The sum k
i=1 Vi of the subspaces V1, V2, . . . , Vk of V is
also a subspace of V . It is, in fact, the smallest subspace of V containing
every Vi.

6.5
The Grassmann Intersection Formula
163
Proof We leave the proof as another exercise.
□
6.5.2
Proof of the Grassmann intersection
formula
We now return to the question of what one can say about the dimension of
the intersection of two subspaces. For example, what is the dimension of the
intersection of two three-dimensional subspaces of R4? The answer is given
by the Grassmann intersection formula, which relates the dimensions of W,
Y , W + Y , and W ∩Y . Before looking at the formula, the reader can try to
guess the answer.
Theorem 6.21 If W and Y are ﬁnite-dimensional subspaces of a vector
space V , then W + Y is ﬁnite-dimensional, and
dim(W + Y ) = dim W + dim Y −dim(W ∩Y ).
(6.9)
Proof Since W ∩Y is a subspace of W, it is ﬁnite-dimensional. Hence, we
know that W ∩Y has a basis, say x1, . . . , xk. The dimension theorem allows
us to extend this basis to a basis of W, say
x1, . . . , xk, wk+1, . . . , wk+r.
Likewise, since Y is also ﬁnite-dimensional, we can extend the basis of W ∩Y
to a basis of Y , say
x1, . . . , xk, yk+1, . . . , yk+s.
I claim that
B = {x1, . . . , xk, wk+1, . . . , wk+r, yk+1, . . . , yk+s}
is a basis of W + Y . It is not hard to see that B is a spanning set, so W + Y
is ﬁnite-dimensional even though V is an arbitrary vector space. To see that
B is independent, suppose
k

i=1
αixi +
k+r

j=k+1
βjwj +
k+s

m=k+1
γmym = 0.
(6.10)
Thus,

γmym = −
 
αixi +

βjwj

.
Since the left-hand term is in Y and the one on the right is in W, we have

164
6
Vector Spaces

γmym ∈Y ∩W.
Thus

γmym =

δixi
for some δi ∈F. Hence

δixi +

(−γm)ym = 0.
Therefore, all the δi and γm are zero. In particular, (6.10) becomes the expres-
sion

αixi +

βjwj = 0.
But this implies that all the αi and βj are 0 also. Consequently, B is inde-
pendent. Since B spans W + Y , it forms a basis of W + Y , so dim(W + Y ) =
k + r + s. It remains to count dimensions. We have
dim(W + Y ) = k + r + s = (k + r) + (k + s) −k,
and hence dim(W + Y ) = dim W + dim Y −dim(W ∩Y ).
□
Notice that the Grassmann intersection formula doesn’t mention dim V ,
which is why we don’t need to require that V be ﬁnite-dimensional. However,
if it is, then dim(W + Y ) ≤dim V , so we get a nice corollary.
Corollary 6.22 If W and Y are subspaces of a ﬁnite-dimensional vector
space V , then
dim(W ∩Y ) ≥dim W + dim Y −dim V.
(6.11)
In particular, if dim W + dim Y > dim V , then dim(Y ∩W) > 0.
Now let us see what can be said about two three-dimensional subspaces
W and Y of a ﬁve-dimensional space V . Since the sum of the dimensions of
W and Y is 6, they meet in at least a line, although the intersection can also
have dimension two or three. If dim V decreases, then intuitively, dim(Y ∩W)
should increase, since there is less room to maneuver. This is exactly what
the inequality tells us. If dim V = 4, then dim(W ∩Y ) ≥3 + 3 −4 = 2, so
W and Y contain a common plane. However, once dim V is at least 6, the
inequality no longer tells us anything.
Example 6.30 (Intersection of hyperplanes). Let H1 and H2 be distinct
hyperplanes in Fn. Then
dim(H1 ∩H2) ≥(n −1) + (n −1) −n = n −2.

6.5
The Grassmann Intersection Formula
165
But since the hyperplanes are distinct, dim(H1 ∩H2) < n −1, so dim(H1 ∩
H2) = n −2 exactly.
⊠
Here is a nice example.
Example 6.31. Recall that Fn×n
s
and Fn×n
ss
denote, respectively, the spaces
of of n × n symmetric and n × n skew-symmetric matrices (see Example
6.26). Let’s assume now that the characteristic of F is not equal to 2. Then
an arbitrary A ∈Fn×n can be expressed as the sum of a symmetric matrix
and a skew-symmetric matrix. Namely,
A = 1
2(A + AT ) + 1
2(A −AT ).
(6.12)
Thus Fn×n = Fn×n
s
+Fn×n
ss . Moreover, since char(F) ̸= 2, the only matrix that
is both symmetric and skew-symmetric is the zero matrix. That is, Fn×n
s
∩
Fn×n
ss
= {0}. Hence by the Grassmann intersection formula,
dim Fn×n = dim(Fn×n
s
) + dim(Fn×n
ss ).
Note that we already knew this result from Section 6.4.2, where we showed
that dim(Fn×n
s
) = n(n + 1)/2 and dim(Fn×n
ss ) = n(n −1)/2.
⊠
As we will see in the next section, this example shows that Fn×n is the
direct sum of Fn×n
s
and Fn×n
ss . This is a stronger assertion than simply saying
that Fn×n = Fn×n
s
+ Fn×n
ss , which is a consequence of (6.12). In particular, it
implies that every square matrix can be uniquely expressed via (6.12) as the
sum of a symmetric matrix and a skew-symmetric matrix, except when the
characteristic is two.
6.5.3
Direct sums of subspaces
By the Grassmann intersection formula, two subspaces W and Y of V such
that dim(W ∩Y ) = 0 have the property that dim(W + Y ) = dim W +
dim Y , and conversely. An explicit example of this was considered above.
This observation is related to the following deﬁnition.
Deﬁnition 6.9. We say that V is the direct sum of two subspaces W and
Y if V = W + Y and for every v ∈V , the expression v = w + y with w ∈W
and y ∈Y is unique. If V is the direct sum of W and Y , we write V = W ⊕Y .
More generally, we say that V is the direct sum of the subspaces V1, . . . , Vk
if V =  Vi and
for every v ∈V , the expression v =  vi, where each
vi ∈Vi, is unique. (Equivalently, if 0 =  vi, where each vi ∈Vi, then each
vi = 0.) In this case, we write V = k
i=1 Vi.

166
6
Vector Spaces
Proposition 6.23. Suppose V is a ﬁnite-dimensional vector space with sub-
spaces W and Y . Then the following conditions are all equivalent to the asser-
tion that V = W ⊕Y :
(i) V = W + Y and W ∩Y = {0}.
(ii) V = W + Y and dim V = dim W + dim Y .
(iii) dim V = dim W + dim Y and W ∩Y = {0}.
Proof. That conditions (i), (ii), and (iii) are equivalent follows from the
Grassmann intersection formula. Thus it suﬃces to show that (i) is equivalent
to V = W ⊕Y . Assume (i), and suppose v = w + y = w′ + y′. Then
w −w′ = y′ −y is an element of W ∩Y = {0}. Thus w = w′ and y′ = y.
Hence V = W ⊕Y . On the other hand, if V = W ⊕Y and W ∩Y ̸= {0},
then every nonzero v ∈W ∩Y has two expressions v = v + 0 = 0 + v. This
violates the deﬁnition of a direct sum, so W ∩Y = {0}.
□
Corollary 6.24. Suppose dim V = dim(W +Y ) and dim(W ∩Y ) = 0. Then
V = W ⊕Y .
Proof. If dim V = dim(W + Y ), then W + Y = V . Therefore, the result
follows from Proposition 6.23.
□
Referring back to Example 6.31, we get the following assertion.
Proposition 6.25. Assume char(F) ̸= 2. Then Fn×n = Fn×n
s
⊕Fn×n
ss . Thus,
every square matrix over F can be uniquely expressed as in (6.12) as the sum
of a symmetric matrix and a skew-symmetric matrix, both over F.
We will need the following extended version of Proposition 6.23.
Proposition 6.26. Suppose V is ﬁnite-dimensional and V1, . . . , Vk are sub-
spaces of V such that V = k
i=1 Vi. Then V
= k
i=1 Vi if and only if
dim V = k
i=1 dim Vi.
Proof. Suppose V = k
i=1 Vi, and dim V = k
i=1 dim Vi. Choose a basis of
each Vi and consider the union B of these bases. Then B clearly spans V ,
since the part in Vi spans Vi. Hence we get a spanning set in V with dim V
elements. It follows that B is a basis of V . Now suppose the sum isn’t direct.
Then there exists an element v with two diﬀerent decompositions
v =
k

i=1
xi =
k

i=1
yi,
where the xi and yi are in Vi for each i. Thus xj ̸= yj for some index j.
Hence v has to have two diﬀerent expansions in terms of B. This contradicts

6.5
The Grassmann Intersection Formula
167
the uniqueness of the expansion in a basis; hence the sum must be direct.
Conversely, suppose V = k
i=1 Vi. Forming B as in the previous case, it
follows that B is independent, since the sum is direct. Thus B is a basis, and
therefore, dim V = |B| = k
i=1 dim Vi.
□
6.5.4
External direct sums
Let V and W be arbitrary vector spaces over the same ﬁeld F. Then we can
form a new vector space V × W containing both V and W as subspaces.
Recall from Chap. 1 that V ×W denotes the Cartesian product of V and W,
namely the set of all ordered pairs (v, w), where v ∈V and w ∈W.
Deﬁnition 6.10. The external direct sum of V and W is the Cartesian prod-
uct V × W with addition deﬁned componentwise by
(v1, w1) + (v2, w2) = (v1 + v2, w1 + w2),
and scalar multiplication deﬁned similarly by
r(v, w) = (rv, rw).
The alert reader will have noted that Fk × Fm = Fk+m. Thus the external
direct sum is a generalization of the construction of Fn. This operation can
also be extended (inductively) to any ﬁnite number of vector spaces over F.
In fact, Fn is just the n-fold external direct sum of F.
We leave it to the reader to show that V and W can both be considered
subspaces of V × W.
Proposition 6.27. If V and W are ﬁnite-dimensional vector spaces over F,
then so is their external direct sum, and dim(V × W) = dim V + dim W.
Proof. We leave this as an exercise.
□
Exercises
Exercise 6.5.1. Suppose W and Y are two subspaces of a ﬁnite-dimensional
vector space V such that W ∩Y = {0}. Show that dim W + dim Y ≤dim V .
Exercise 6.5.2. Prove Proposition 6.27.
Exercise 6.5.3. If two 22-dimensional subspaces of Rn always meet in at
least a line, what can you say about n?

168
6
Vector Spaces
Exercise 6.5.4. Do two subspaces of R26 of dimensions 5 and 17 have to
meet in more than 0? What about a subspace of dimension 22 and a subspace
of dimension 13?
Exercise 6.5.5. Suppose W and Y are two subspaces of (Fp)n. Find expres-
sions for |W ∩Y | and |W + Y |.

6.6
Inner Product Spaces
169
6.6
Inner Product Spaces
In this section, we will study the notion of an inner product on a real vector
space. We will also study the notion of a Hermitian inner product on a com-
plex vector space, though not in as much detail. An inner product on a real
vector space V allows one to measure distances and angles between vectors.
Similarly, a Hermitian inner product permits one to do the same for vec-
tors in a complex vector space. The main result of this section is that every
ﬁnite-dimensional inner product space has a special type of basis known as
an orthonormal basis. This is a basis having the properties of the standard
basis of Rn.
6.6.1
The deﬁnition of an inner product
We will ﬁrst treat the real case.
Deﬁnition 6.11. Let V be a real vector space. An inner product on V is a
rule that associates to every a, b ∈V a unique scalar (a, b) ∈R having the
following properties for all a, b, c ∈V and r ∈R:
(i) (a, b) = (b, a),
(ii) (a + b, c) = (a, c) + (b, c),
(iii) (ra, b) = (a, rb) = r(a, b), and
(iv) if a ̸= 0, then (a, a) > 0.
The property in condition (iv) is called positive deﬁniteness. A real vector
space V with an inner product is called an inner product space. Clearly,
(0, 0) = 0, so (a, a) ≥0 for all a ∈V . Thus the following deﬁnition makes
sense.
Deﬁnition 6.12. Let V be an inner product space as in the previous deﬁ-
nition. Then the length of a ∈V is deﬁned by
|a| =

(a, a),
(6.13)
and the distance between a and b in V is deﬁned by
d(a, b) = |a −b|.
(6.14)
The basic example of an inner product space is Rn with the Euclidean
inner product deﬁned in the next example.

170
6
Vector Spaces
Example 6.32 (Euclidean n-space). The dot product on Rn deﬁned in
Section 3.1.6 by
a · b = aT b =
n

i=1
aibi
(6.15)
deﬁnes an inner product by setting (a, b) = a·b. The dot product is referred
to as the Euclidean inner product on Rn.
⊠
The next example gives an important inner product on Rn×n. First, we
have to deﬁne the trace of an n × n matrix. If A ∈Fn×n, deﬁne the trace of
A to be Tr(A) = n
i=1 aii.
Example 6.33. Let V = Rn×n. For A, B ∈V , put
(A, B) = Tr(ABT).
This deﬁnes an inner product on Rn×n known as the Killing form. The veri-
ﬁcation of the axioms for an inner product is left as an exercise. Notice, for
example, that if A and B are diagonal matrices, say A = diag(a1, . . . , an) and
B = diag(b1, . . . , bn), then
(A, B) = Tr(ABT ) = Tr(diag(a1b1, . . . , anbn)) =
n

i=1
aibi.
Thus the Killing form coincides with the Euclidean inner product on diagonal
matrices.
⊠
6.6.2
Orthogonality
As we just saw, an inner product on a real vector space V has natural length
and distance functions. We will now see that it allows one to imitate other
Euclidean properties of Rn, namely the notion of angles. The starting point
is orthogonality.
Deﬁnition 6.13. We say that a pair of vectors a, and b in an inner product
space V are orthogonal if
(a, b) = 0.
(6.16)
Here are a couple of simple properties.
Proposition 6.28. In an inner product space V , the zero vector is orthog-
onal to every vector. In fact, 0 is the only vector orthogonal to itself. Two
vectors a, b ∈Rn are orthogonal if and only if n
i=1 aibi = 0.

6.6
Inner Product Spaces
171
Proof. Left to the reader.
□
Orthogonality is a generalization of the notion of perpendicularity in R2.
Two vectors a = (a1, a2)T and b = (b1, b2)T in R2 are perpendicular exactly
when the triangle with sides a and b is a right triangle. This is the case
exactly when |a + b|2 = |a|2 + |b|2, by the Pythagorean theorem. The reader
should check that this identity holds if and only if a1b1 + a2b2 = 0, that is, if
a · b = 0. On an inner product space V , orthogonality can be characterized
in a similar manner.
Proposition 6.29. Two vectors a and b in an inner product space V are
orthogonal if and only if |a + b|2 = |a|2 + |b|2.
Proof. Since |a + b|2 = (a + b, a + b), one gets that
|a + b|2 = (a, a) + 2(a, b) + (b, b) = |a|2 + 2(a, b) + |b|2.
Hence (a, b) = 0 if and only if |a + b|2 = |a|2 + |b|2.
□
We now discuss orthogonal decomposition. If a, b ∈V and b ̸= 0, we claim
that there exists a unique λ ∈R such that a = λb + c and (b, c) = 0. To see
this, write c = a −λb. Using the properties of the inner product, we see that
(b, c) = 0 if and only if λ = (a, b)/(b, b). By the previous proposition, this
value of λ gives |a|2 = λ2|b|2 + |c|2. Hence we get the following result.
Proposition 6.30. Let a and b be elements of an inner product space V ,
and suppose b ̸= 0. Then a can be uniquely decomposed as a linear combina-
tion of two orthogonal vectors b and c as
a = λb + c,
(6.17)
where λ = (a, b)/(b, b) and c = a −λb.
In particular, since |c|2 ≥0, it follows that
|a|2 ≥((a, b)/(b, b))2|b|2.
Taking square roots gives a famous inequality.
Proposition 6.31 (Cauchy–Schwarz inequality). For every a, b ∈V ,
|(a, b)| ≤|a||b|,
(6.18)
with equality if and only if a is a multiple of b.

172
6
Vector Spaces
Proof. The inequality surely holds when b = 0. Thus suppose b ̸= 0. Then
(6.18) follows from the inequality before the proposition, and equality holds
if and only if c = 0, or equivalently, if and only if a is a multiple of b.
□
The Cauchy–Schwarz inequality for Rn says that
|
n

i=1
aibi| ≤

n

i=1
a2
i
1/2
n

i=1
b2
i
1/2
.
The vector
Pb(a) =

(a, b)/(b, b)

b
is called the orthogonal projection of a on b. Our ﬁnal deﬁnition is the
angle between two nonzero vectors. By the Cauchy–Schwarz inequality,
−1 ≤(a,b)
|a||b| ≤1. Hence there exists a unique θ ∈[0, π] such that
cos θ = (a, b)
|a||b| .
(6.19)
Deﬁnition 6.14. The angle between two nonzero vectors a and b in an
inner product space V is deﬁned to be the unique angle θ ∈[0, π] such that
(6.19) holds.
In particular, (a, b) = 0 if and only if the angle between a and b is π/2.
In physics books, the identity
a · b = |a||b| cos θ
is sometimes taken as a deﬁnition of the dot product. But this deﬁnition is
not as easy to work with as the usual one.
Inner products can exist on inﬁnite-dimensional vector spaces. This gives
a method for extending properties of Rn to the inﬁnite-dimensional setting.
A particularly important example is the following.
Example 6.34. (An inner product on C[a, b]) Let C[a, b] be the space of all
continuous real-valued functions on the closed interval [a, b] in R. The inner
product of f, g ∈C[a, b] is deﬁned by
(f, g) =
 b
a
f(t)g(t)dt.
The ﬁrst three axioms for the inner product on C[a, b] are veriﬁed by applying
standard facts about integration proved (or at least stated) in calculus. The
positive deﬁniteness property requires that we verify that (f, f) > 0 if f ̸= 0.

6.6
Inner Product Spaces
173
This involves a little bit of knowledge of how the Riemann integral is deﬁned,
and so we will skip the details.
⊠
6.6.3
Hermitian inner products
When we take up the principal axis theorem for Hermitian matrices in
Chap. 9, we will need the notion of a Hermitian inner product space. Her-
mitian inner products are also very important in physics. We will ﬁrst intro-
duce the standard Hermitian inner product on Cn, and then proceed to the
general deﬁnition.
Example 6.35 (Hermitian n-space). The Hermitian inner product of a pair
of vectors w, z ∈Cn is deﬁned to be the complex scalar
w • z = wT z =
w1 w2 · · · wn

⎛
⎜
⎜
⎜
⎝
z1
z2
...
zn
⎞
⎟
⎟
⎟
⎠=
n

i=1
wizi.
(6.20)
Let us put wH = wT ; wH is called the Hermitian transpose of w. Thus,
w • z = wHz.
Although w • z is not necessarily real, w • w is real, and in fact, w • w ≥0
for all w. Thus we may deﬁne the Hermitian length function by
|w| = (w • w)1/2 = (wHw)1/2 =

n

i=1
|wi|21/2.
⊠
We now make a general deﬁnition.
Deﬁnition 6.15. Let V be a complex vector space. A Hermitian inner prod-
uct on V is a rule assigning a scalar (w, z) ∈C to every pair of vectors
w, z ∈V such that
(i) (w + w′, z) = (w, z) + (w′, z) and (w, z + z′) = (w, z) + (w, z′),
(ii) (z, w) = (w, z),
(iii) (αw, z) = α(w, z) and (w, αz) = α(w, z), and ﬁnally,
(iv) if w ̸= 0, (w, w) > 0.
A complex vector space endowed with a Hermitian inner product is called
a Hermitian inner product space.

174
6
Vector Spaces
6.6.4
Orthonormal bases
In this section we will show that every ﬁnite-dimensional inner product space
admits a type of basis called an orthonormal basis, which is analogous to the
standard basis e1, . . . , en of Euclidean n-space Rn. Let V denote a real inner
product space.
Deﬁnition 6.16. A set U of unit vectors in V is called orthonormal if every
pair of distinct elements of U are orthogonal to each other.
Example 6.36. The standard basis e1, . . . , en determines an orthonormal
set in Euclidean n-space Rn.
⊠
Proposition 6.32. Every orthonormal subset U of V is linearly indepen-
dent. (That is, every ﬁnite subset of U is independent.) In particular, if V is
ﬁnite-dimensional, then every orthonormal set in V having dim V elements
is a basis of V .
Proof. Suppose u1, . . . , um are orthonormal, and assume that
m

i=1
aiui = 0.
Then for every index j,
 m

i=1
aiui, uj

= (0, uj) = 0.
Since (ui, uj) equals 0 if i ̸= j and equals 1 if i = j, the left-hand side is
m

i=1
ai(ui, uj) = aj,
so aj = 0 for all j. Therefore, u1, . . . , um are independent. Hence if |U| =
dim V, then U is a basis of V .
□
Deﬁnition 6.17. An orthonormal basis of V is a basis that is an orthonor-
mal set.
Proposition 6.33. A collection of vectors u1, u2, . . . , un in Rn is an ortho-
normal basis of Rn if and only if the matrix U = (u1 u2 . . . un) is orthogonal.
Thus the set of U ∈Fn×n whose columns are an orthonormal basis of Rn is
exactly the orthogonal group O(n, R).

6.6
Inner Product Spaces
175
Proof. Recall that U is orthogonal if and only if U T U = In. But U T U =
(uT
i uj) = (ui · uj) = In if and only if u1, u2, . . . , un are orthonormal.
□
For example, the standard basis vectors e1, . . . , en in Rn are the columns
of In. Here are some more examples.
Example 6.37. The vectors
u1 =
1
√
3(1, 1, 1)T , u2 =
1
√
6(1, −2, 1)T , u3 =
1
√
2(1, 0, −1)T
form an orthonormal basis of R3. Moreover, u1 and u2 constitute an ortho-
normal basis of the plane x −z = 0.
⊠
Example 6.38. The matrix
Q = 1
2
⎛
⎜
⎜
⎝
1
1
1
1
−1
1
−1
1
1
−1 −1
1
1
1
−1 −1
⎞
⎟
⎟
⎠
is orthogonal. Hence its columns form an orthonormal basis of R4. Since QT
is also orthogonal, the rows of B form another orthonormal basis of R4.
⊠
6.6.5
The existence of orthonormal bases
We now show that every ﬁnite-dimensional inner product space has an ortho-
normal basis. In fact, we show a little more.
Proposition 6.34. Let V be an inner product space. Then every nontrivial
ﬁnite-dimensional subspace W of V admits an orthonormal basis.
Proof. We prove this by induction on dim W. Note that every subspace of V
is also an inner product space via restricting the inner product on V to W.
If dim W = 1, the result is true, since a unit vector in W is an orthonormal
basis. Thus suppose dim W = m > 1 and that the result is true for every
subspace of W of dimension at most m −1. Let u be a unit vector in W
and let H = {x ∈W | (x, u) = 0}. Then H is a subspace of W. Since
|u| = 1, u /∈H. It follows from Corollary 6.12 that dim H < m. Thus, by the
induction hypothesis, H admits an orthonormal basis, say U. Now I claim
that U and u combine to give an orthonormal basis of W. Clearly, U and u
form an orthonormal set, so it suﬃces to check that they span W. Let x be
an arbitrary element of W, and let y = x −(x, u)u. Then
(y, u) = (x, u) −(x, u)(u, u) = 0,

176
6
Vector Spaces
since (u, u) = 1. Thus y ∈H, so y is a linear combination of the elements of
U. Since x = y + (x, u)u, x is in the span of u and U. Therefore W has an
orthonormal basis, so the result is proven.
□
The above proof gives us the following
Corollary 6.35. If u is a unit vector in a ﬁnite-dimensional inner product
space V , then
H = {x ∈V | (x, u) = 0}
is a hyperplane in V . That is, dim H = dim V −1.
Here is an example not involving Rn.
Example 6.39. Let V denote the set of functions f(x) = ax2 + bx + c on
[−1, 1], where a, b, c are arbitrary real numbers. For f, g ∈V , let (f, g) =
 1
−1 f(x)g(x)dx. In other words, V is a three-dimensional subspace of the
inner product space C[−1, 1]. The functions 1, x, and x2 are a basis of V ,
but unfortunately they aren’t orthonormal. For although x is orthogonal to 1
and x2, 1 and x2 aren’t orthogonal to each other:
 1
−1 1x2dx = 2/3. To correct
this, we replace x2 with x2 −r, where r is chosen such that (1, x2 −r) = 0.
Since (1, 1) = 2, it is easy to see that we should let r = 1/3. Thus 1, x, x2 −
1/3 are orthogonal on [−1, 1]. We therefore obtain the orthonormal basis by
normalizing. The result is u1 = 1/
√
2, u2 =

3/2 x, and u3 = c(x2 −1/3)
where c =
  1
−1(x2 −1/3)2dx
−1/2.
⊠
6.6.6
Fourier coeﬃcients
We are now going to see one of the reasons that an orthonormal basis is
very useful. If v1, v2, . . . , vn is a basis of a vector space V , how does one
express an arbitrary element v of V as a linear combination of these basis
vectors? If V = Fn, then this involves solving the linear system Ax = v, where
A =
v1 v2 · · · vn

. That is, x = A−1v. But if V is arbitrary, we don’t yet
have a general method. On the other hand, if V has an orthonormal basis,
there is a simple elegant solution.
Proposition 6.36. Let u1, u2, . . . , un be an orthonormal basis of V . Then
if w ∈V , we have
w =
n

1=i
(w, ui)ui.
(6.21)
Proof. Let w = n
1=i xiui. Then (w, uj) = n
1=i xi(ui, uj) = xj, since the
ui are orthonormal.
□

6.6
Inner Product Spaces
177
The coeﬃcients (w, ui) in (6.21) are called the Fourier coeﬃcients of w
with respect to the orthonormal basis u1, . . . , un. We may also refer to (6.21)
as the Fourier expansion of w in terms of the given orthonormal basis.
If V = Rn, then (6.21) can be expressed in matrix form In = QQT ; that
is,
In =
n

1=i
uiuT
i .
(6.22)
Example 6.40. In terms of the orthonormal basis of Example 6.38, one
gets, for example, that
⎛
⎜
⎜
⎝
1
0
0
0
⎞
⎟
⎟
⎠= 1
4
⎛
⎜
⎜
⎝
1
1
1
1
⎞
⎟
⎟
⎠−1
4
⎛
⎜
⎜
⎝
−1
1
−1
1
⎞
⎟
⎟
⎠+ 1
4
⎛
⎜
⎜
⎝
1
−1
−1
1
⎞
⎟
⎟
⎠+ 1
4
⎛
⎜
⎜
⎝
1
1
−1
−1
⎞
⎟
⎟
⎠.
⊠
6.6.7
The orthogonal complement of a subspace
Let U be a subset of an inner product space V .
Deﬁnition 6.18. The orthogonal complement of U is deﬁned to be the set
U ⊥consisting of all vectors v ∈V orthogonal to every vector in U.
Thus,
U ⊥= {v ∈V | (v, u) = 0 ∀u ∈U}.
(6.23)
Proposition 6.37. For every subset U of an inner product space V , U ⊥is
a subspace of V . Moreover, if W = span U, then W ∩U ⊥= {0}.
Proof. This is an exercise.
It may be instructive to visualize W ⊥in matrix terms. Let A ∈Rm×n,
and let U denote its columns. Then W = span U = col(A), the column space
of A, and W ⊥= N(AT ). By (6.6), dim N(AT ) + dim row(AT ) = m. But
row(AT ) and W = col(A) certainly have the same dimension, so we get the
identity
dimW + dimW ⊥= m.
(6.24)
Thus, the column space of a matrix and the null space of its transpose are
each the orthogonal complement of the other. We now prove a more general
version of this.

178
6
Vector Spaces
Proposition 6.38. Let W be a subspace of a ﬁnite-dimensional inner prod-
uct space V and W ⊥its orthogonal complement. Then V = W ⊕W ⊥. Thus,
dim V = dim W + dim W ⊥. In particular, every v ∈V can be orthogonally
decomposed in exactly one way as v = w + y, where w ∈W and y ∈W ⊥.
Proof. By Proposition 6.34, we may choose an orthonormal basis of W, say
u1, . . . , uk. Let v ∈V and put
y = v −
k

i=1
(v, ui)ui.
(6.25)
Since the ui are orthonormal, we have (y, ui) = 0 for all i. Thus, by deﬁnition,
y ∈W ⊥. But this says that if w = k
i=1(v, ui)ui, then v = w+y. Therefore,
V = W + W ⊥. Since W ∩W ⊥= {0}, we get V = W ⊕W ⊥by Proposition
6.23. Hence, dim W + dim W ⊥= dim V .
□
Deﬁnition 6.19. Let v = w + y be the above decomposition of v ∈V with
w ∈W. Then w is called the component of v in W.
Thus if u1, . . . , uk is an orthonormal basis of W, the component of an
arbitrary vector v ∈V is
w =
k

i=1
(v, ui)ui.
(6.26)
In particular, if W is a line, say W = Rw, then the component of an arbitrary
v in V can be easily worked out, since
u =
w
(w, w)1/2
is an orthonormal basis for W. In particular,
v = (v, u)u +

v −(v, u)u

.
6.6.8
Hermitian inner product spaces
The results about orthonormal bases in the case of an inner product space
all have analogues for the Hermitian inner product spaces that were intro-
duced in Section 6.6.3. Recall that the main example of a Hermitian inner
product space is Cn with the Hermitian inner product (w, z) = wHz. A basis
w1, . . . , wn of a Hermitian inner product space V is said to be a Hermitian
orthonormal basis, provided each |wi| is equal to 1 and (wi, wj) = 0 if i ̸= j.
By imitating the proof of Proposition 6.34, one can prove the following result.

6.6
Inner Product Spaces
179
Proposition 6.39. Every ﬁnite-dimensional Hermitian inner product space
has a Hermitian orthonormal basis. Moreover, the identity (6.21) holds for
every Hermitian orthonormal basis.
Orthogonal complements are deﬁned in the Hermitian case in exactly the
same way as in the real case, and the Hermitian version of Proposition 6.38
goes through without any change. Hermitian orthonormal bases of Cn are
related to unitary matrices in the same way that orthonormal bases of Rn
are related to orthogonal matrices. Recall that if U ∈Cn×n, then U H = (U)T .
Deﬁnition 6.20. A matrix U ∈Cn×n is said to be unitary if U HU = In.
The set of all n × n unitary matrices is denoted by U(n).
Thus, unitary matrices are to the Hermitian inner product on Cn as orthog-
onal matrices are to the Euclidean inner product on Rn.
Proposition 6.40. U(n) is a matrix group that is a subgroup of GL(n, C).
Proof. Exercise.
□
Thus U(n) is called the unitary group.
Exercises
Exercise 6.6.1. A nice application of Cauchy–Schwarz is the following fact:
if a and b are unit vectors in Rn such that a · b = 1, then a = b. Prove this.
Exercise 6.6.2. Prove the law of cosines: if a triangle has sides with lengths
a, b, c, and θ is the angle opposite the side of length c, then c2 = a2 + b2 −
2ab cos θ. (Hint: Consider c = b −a.)
Exercise 6.6.3. Show that the Killing form (A, B) = Tr(ABT ) introduced
in Example 6.33 is an inner product on the space R2×2 of real 2 × 2 matrices
and ﬁnd an orthonormal basis.
Exercise 6.6.4. Show that the orthogonal complement with respect to the
Killing form of the space of 2×2 symmetric real matrices is the space of 2×2
skew symmetric real matrices. Conclude R2×2 = R2×2
s
⊕R2×2
ss .
Exercise 6.6.5. The proof that the Killing form on Rn×n is an inner product
requires showing that (A, B) = (B, A). Show this by proving the following
statements.
(i) For all A, B ∈Rn×n, Tr(AB) = Tr(BA);
(ii) For all A, B ∈Rn×n, Tr(ABT ) = Tr(BAT )
Exercise 6.6.6. Orthogonally decompose the vector (1, 2, 2)T in R3 as p+q,
where p is required to be a multiple of (3, 1, 2)T and q is orthogonal to p.

180
6
Vector Spaces
Exercise 6.6.7. In this exercise, we consider the inner product space V =
C[−1, 1] of continuous real-valued functions on [−1, 1] with inner product
deﬁned by (f, g) =
 1
−1 f(t)g(t)dt.
(i) Show that the functions 1 and x are orthogonal. In fact, show that xk and
xm are orthogonal if k is even and m is odd.
(ii) Find the projection of x2 on the constant function 1.
(iii) Use this to obtain the orthogonal decomposition of x2 on [−1, 1] in which
one of the components has the form r1.
Exercise 6.6.8. Consider the real vector space V = C[0, 2π] with the inner
product deﬁned by (f, g) =
 2π
0
f(t)g(t)dt.
(i) Find the length of sin2 x in V .
(ii) Compute the inner product (cos x, sin2 x).
(iii) Find the projection of sin2 x on each of the functions 1, cos x, and sin x
in V .
(iv) Are 1, cos x, and sin x mutually orthogonal as elements of V ?
(v) Compute the orthogonal projection of sin2 x onto the subspace W of V
spanned by 1, cos x, and sin x.
Exercise 6.6.9. Assume f ∈C[a, b]. The average value of f over [a, b] is
deﬁned to be
1
b −a
 b
a
f(t)dt.
Show that the average value of f over [a, b] is the projection of f on 1. Does
this suggest an interpretation of the average value?
Exercise 6.6.10. Let f, g ∈C[a, b]. Give a formula for the scalar t that
minimizes
||f −tg||2 =
 b
a
(f(x) −tg(x))2dx.
Exercise 6.6.11. Show that the Hermitian inner product on Cn satisﬁes all
the conditions listed in Deﬁnition 6.15.
Exercise 6.6.12. Consider the plane P in R3 given by the equation x−y +
2z = 0.
(i) Find an orthonormal basis of P.
(ii) Find the expansion of (1, 1, 0)T in terms of this orthonormal basis.

6.6
Inner Product Spaces
181
Exercise 6.6.13. Find the expansion of (2, 0, 0)T in terms of the orthonor-
mal basis of Example 6.37.
Exercise 6.6.14. The Gram–Schmidt method gives an algorithm for pro-
ducing an orthonormal basis of an inner product space starting from a general
basis. Here is how it works for R3. Let v1, v2, v3 be a basis. First put
u1 = v1
|v1|.
Next, put
v′
2 = v2 −(v2 · u1)u1
and
u2 = v′
2
|v′
2|.
Finally, put
v′
3 = v3 −(v3 · u1)u1 −(v3 · u2)u2
and
u3 = v′
3
|v′
3|.
Verify that u1, u2, u3 form an orthonormal basis of R3 having the property
that span{u1} = span{v1} and span{u1, u2} = span{v1, v2}. Why are v′
2
and v′
3 both nonzero?
Exercise 6.6.15. Generalize the Gram–Schmidt method from R3 to R4.
Exercise 6.6.16. Let W denote the hyperplane w + x −y + z = 0 in R4.
(i) Find the component of (1, 1, 1, 1)T in W.
(ii) Find an orthonormal basis of W.
(iii) Find an orthonormal basis of R4 containing the orthonormal basis of
part (ii).
(iv) Expand (1, 1, 1, 1)T in terms of the basis in part (iii).
Exercise 6.6.17. Show that if W is a subspace of a ﬁnite-dimensional inner
product space V , then (W ⊥)⊥= W.
Exercise 6.6.18. Recall that the group P(n) of n×n permutation matrices
is a subgroup of O(n, R). Show that the set of all left cosets O(n, R)/P(n) is
in one-to-one correspondence with the set of all orthonormal bases of Rn.
Exercise 6.6.19. Let V be an inner product space. Show that the distance
function d(a, b) = |a −b| on V × V Deﬁnes a metric on V in the sense that
for all a, b, c ∈V , we have the following properties:
(i) d(a, b) ≥0 and d(a, b) = 0 if and only if a = b.
(ii) d(a, b) = d(b, a), and
(iii) d(a, c) ≤d(a, b) + d(b, c).
Another quite diﬀerent example of a metric is given in the Appendix.

182
6
Vector Spaces
Exercise 6.6.20. Let V be a ﬁnite-dimensional inner product space and
W a subspace. The distance d(v, W) from an arbitrary vector v ∈V to
the subspace W is deﬁned to be the minimum distance d(v, w) as w ranges
over W.
(i) If v ∈W, show that the distance from v to W is 0.
(ii) Suppose v = w + y is the orthogonal decomposition of v with w ∈W
and y ∈W ⊥. Show that d(v, w) ≤d(v, w′) for all w′ ∈W and conclude
that d(v, W) = |y|. (This fact is called the principle of least squares.)
Exercise 6.6.21. Find the distance from (1, 1, 1, 1)T to the subspace of R4
spanned by (2, 0, −1, 1)T and (0, 0, 1, 1)T .
Exercise 6.6.22. If K is a subspace of Cn with the Hermitian inner product,
show that dim K + dim K⊥= n, where K⊥is the orthogonal complement of
K with respect to the Hermitian inner product.
Exercise 6.6.23. Find a Hermitian orthonormal basis of the subspace of C3
spanned by (1, i, 0)T and (2, −i, 1)T , and then extend this basis to a Hermitian
orthonormal basis of C3.
Exercise 6.6.24. Prove Proposition 6.40. That is, show that U(n) is a
matrix group.
Exercise 6.6.25. The complex analogue of the Killing form on Cn×n is
(J, K) = Tr(JKH). Show that (J, K) deﬁnes a Hermitian inner product on
C2×2.

6.7
Vector Space Quotients
183
6.7
Vector Space Quotients
In the penultimate section of this chapter, we will construct the quotient
vector space V/W of a vector space V by a subspace W. Since V is an
abelian group and every subspace is a normal subgroup, this construction is
an application of the construction of the quotient group G/H of a group G
by a normal subgroup H (see Proposition 2.17). Thus, the space of cosets
V/W is also an abelian group, and the natural map π : V →V/W is a group
homomorphism.
Since we are not formally using results from group theory in this chapter,
we will instead construct V/W from scratch. We will then show that if
V is ﬁnite-dimensional, then V/W is a ﬁnite-dimensional vector space and
dim(V/W) = dim V −dim W. Unfortunately, the vector space V/W doesn’t
admit a useful geometric interpretation; one must think of V/W as an
abstract vector space.
6.7.1
Cosets of a subspace
Let V be a vector space over F and let W be a subspace. The cosets of W
were introduced in Section 2.2 in the setting of groups. Redeﬁning them from
scratch in the vector space setting goes as follows.
Deﬁnition 6.21. A coset of W is a subset of V of the form
v + W = {v + w | w ∈W},
(6.27)
where v ∈V .
The cosets of W are the equivalence classes of an equivalence relation EW
on V . Namely, if u, v ∈V , let us write uEW v if v −u ∈W. If uEW v, we
will say that u and v are equivalent modulo W.
Proposition 6.41. Let W be a subspace of a vector space V . Then EW is
an equivalence relation on V , and the equivalence classes of this equivalence
relation are exactly the cosets of W.
Proof. Clearly vEW v, since v −v = 0 ∈W. If uEW v, then vEW u, since
W is closed under scalar multiplication, and (u −v) = (−1)(v −u). Finally,
if uEW v and vEW w, then uEW w, since w −u = (w −v) + (v −u), and
W is closed under addition. Hence EW is an equivalence relation on V . Let
C denote the equivalence class of v and consider v + W. If y ∈C, then
y −v = w ∈W. Hence y = v +w, so y ∈v +W. Therefore, C ⊂v +W. On
the other hand, suppose y ∈v + W. Then y = v + w for some w ∈W. But
then y −v ∈W, so yEW v. Therefore, v + W ⊂C. Hence the equivalence
classes are exactly the cosets of W.
□

184
6
Vector Spaces
Example 6.41. For example, suppose V = R3 and W is a plane through
0. Then the coset v + W is simply the plane through v parallel to W. The
cosets are all the planes in R3 parallel to W.
⊠
6.7.2
The quotient V/W and the dimension
formula
We will refer to the set of cosets V/W as the quotient space of V modulo W.
Two cosets (v + W) and (y + W) may be added by putting
(v + W) + (y + W) = (v + y) + W.
(6.28)
To make V/W into a vector space over F, we also have to deﬁne scalar
multiplication, which we do in a natural way: for a ∈F and v ∈V , put
a(v + W) = av + W.
(6.29)
The proof that addition is a well-deﬁned binary operation uses the same
reasoning as for the quotient group. We need to show that the rule (6.28) is
independent of the way we write a coset. That is, suppose v + W = v′ + W
and y+W = y′+W. Then we have to show that (v+y)+W = (v′+y′)+W.
But this is so if and only if
(v + y) −(v′ + y′) ∈W,
which indeed holds, since
(v + y) −(v′ + y′) = (v −v′) + (y −y′) ∈W,
due to the fact that W is a subspace and both v −v′ and y −y′ are in
W. Therefore, addition on V/W is well deﬁned. The proof for scalar multi-
plication is analogous. The zero element is 0 + W, and the additive inverse
−(v + W) of v + W is −v + W. Properties such as associativity and commu-
tativity of addition follow from corresponding properties in V ; we will omit
all the details. Hence V/W is an F-vector space, which is the ﬁrst assertion of
the following proposition. The second assertion gives a formula for dim V/W
in the ﬁnite-dimensional setting.
Proposition 6.42. Let V be a vector space over a ﬁeld F and suppose W is
a subspace of V . Then the set V/W of cosets of W in V with addition and
scalar multiplication deﬁned as in (6.28) and (6.29) is a vector space over F.
If V is ﬁnite-dimensional, then

6.7
Vector Space Quotients
185
dim V/W = dim V −dim W.
(6.30)
Proof. To check the dimension formula (6.30), let w1, . . . , wk be a basis of
W, and extend this to a basis
w1, . . . , wk, v1, . . . , vn−k
of V . Then I claim that the cosets v1 +W, . . . , vn−k +W give a basis of V/W.
To see that they are independent, put vi + W = αi if 1 ≤i ≤n −k, and
suppose there exist a1, . . . , an−k ∈F such that n−k
i=1 aiαi = 0 + W. This
means that n−k
i=1 aivi ∈W. Hence there exist b1, . . . , bk ∈F such that
n−k

i=1
aivi =
k

j=1
bjwj.
But the fact that the vi and wj constitute a basis of V implies that all ai
and bj are zero. Therefore, α1, . . . , αn−k are linearly independent. We leave
the fact that they span V/W as an exercise.
□
Here is an example that shows how V/W can interpreted.
Example 6.42. Suppose A ∈Fm×n, and recall that N(A) ⊂Fn is the null
space of A. By Proposition 3.17, the elements of the quotient space Fn/N(A)
are the solution sets of the linear systems Ax = b, where b varies through
Fm. The zero element N(A) corresponds to the homogeneous linear system
Ax = 0, while the element p0 + N(A) corresponds to the inhomogeneous
linear system Ax = b, where A(p0) = b. Suppose p0, q0 ∈Fn, and suppose
A(p0) = b and A(q0) = c. By deﬁnition, (p0 + N(A)) + (q0 + N(A)) =
(p0 + q0) + N(A). But A(p0 + q0) = b + c, so here the addition of cosets of
N(A) corresponds to the addition of linear systems.
⊠
Exercises
Exercise 6.7.1. Prove that the cosets α1, . . . , αn−k deﬁned in the proof of
Proposition 6.42 span V/W.
Exercise 6.7.2. Let W be the subspace of V = (F2)4 spanned by 1001,
1101, and 0110. Write down all elements of W, and ﬁnd a complete set of
coset representatives for V/W. That is, ﬁnd an element in each coset.
Exercise 6.7.3. Let A and B be arbitrary subsets of a vector space V over
F. Deﬁne their Minkowski sum to be
A + B = {x + y | x ∈A,
y ∈B}.
Show that if A and B are cosets of a subspace W of V , then so is A + B.

186
6
Vector Spaces
Exercise 6.7.4. Let V and W be any two subspaces of Fn.
(i) Find a formula for dim(V + W)/W.
(ii) Are the dimensions of (V + W)/W and V/(V ∩W) the same?
Exercise 6.7.5. Find a basis of the quotient R4/W, where W is the sub-
space of R4 spanned by (1, 2, 0, 1) and (0, 1, 1, 0).
Exercise 6.7.6. Let V be a vector space over Fp of dimension n, and let W
be a subspace of dimension k.
(i) Show that every coset of W has pk elements. (Find a bijection from W to
x + W.)
(ii) Show that the number of cosets of W is p(n−k).

6.8
Appendix: Linear Coding Theory
187
6.8
Appendix: Linear Coding Theory
In 1948, a mathematician and electrical engineer named Claude Shannon
published a fundamental paper entitled “A Mathematical Theory of Commu-
nication.” This was followed in 1950 by a groundbreaking paper by Richard
Hamming on error-detecting codes. These papers laid the foundations for cod-
ing theory and the theory of error-detecting codes, both of which have been
crucial components of the electronic revolution brought about by computers
and the Internet. The purpose of this appendix is to give a brief introduc-
tion to linear coding theory and to explain how error detection operates. The
ideas we will introduce here depend on concepts from the theory of ﬁnite-
dimensional vector spaces over a Galois ﬁeld. In the ﬁnal section, we will play
the hat game.
6.8.1
The notion of a code
In mathematics, a code is just a subset of a vector space over a Galois ﬁeld.
Let p be a prime, and recall that V (n, p) denotes the vector space (Fp)n
over the ﬁeld Fp. Subsets of V (n, p) are called p-ary codes of length n. The
elements of a code C are called its codewords, and the number of codewords
is denoted by |C|.
A p-ary linear code of length n is a code C ⊂V (n, p) that is also a subspace.
By deﬁnition, all linear codes are ﬁnite-dimensional. For example, a code C ⊂
V (n, 2) consists of binary strings, i.e., strings of 0’s and 1’s, of length n. Such
a code C is linear if and only if the sum of two codewords is again a codeword.
Having the structure of a vector space gives a linear code some advantages
over nonlinear codes, one of them being that a linear code is completely
determined once any set of codewords that spans it is given. A basis of a
linear code is called a set of basic codewords. Recall from the dimension
theorem that two bases of a ﬁnite-dimensional vector space always have the
same number of elements, namely the dimension of C. If C ⊂V (n, p), then
dim C determines the number of codewords by the formula |C| = pdim C.
Example 6.43. The equation x1 + x2 + x3 + x4 = 0 over F2 deﬁnes a linear
code C = N(M), where M =

1 1 1 1

. Since dim(C) = 3, there are 8 = 23
codewords. Rewriting the deﬁning equation as x1+x2+x3 = x4 shows that x4
can be viewed as a check digit, since it is uniquely determined by x1, x2, x3,
which can be arbitrarily given. Here, the codewords are the 4-bit strings
with an even number of 1’s. A set of basic codewords is {1001, 0101, 0011},
although there are also other choices. (How many?)
⊠

188
6
Vector Spaces
6.8.2
Generating matrices
A generating matrix for a linear code C is a matrix of the form M = (Im | A)
whose row space is C. Notice that a generating matrix is in reduced row
echelon form. We know from Proposition 3.12 that the reduced row echelon
form of a matrix is unique, so the generating matrix for a linear code is
unique.
Example 6.44. Let p = 2. If
M =
⎛
⎝
1
0
0
1
1
1
0
1
0
1
0
1
0
0
1
0
1
1
⎞
⎠,
then C = row(M) has 8 elements. Besides the rows of M and the null word,
the elements of C are
110010, 101100, 011110, 111001.
⊠
Codes deﬁned by an m×n generating matrix have the following property:
every element of C = row(M) can be expressed as a matrix product of
the form (x1 . . . xm)M. (To see this, transpose the fact that the column
space of M T consists of all vectors of the form M T (x1 . . . xm)T .) Thus, to
every x = (x1 . . . xm) ∈Fm, there corresponds a unique codeword c(x) =
(x1 . . . xm)M ∈C. For a generating matrix M as above,
c(x) = x1 . . . xm
m

i=1
ai1xi · · ·
m

i=1
ai(n−m)xi ∈Fn.
Since x1, . . . , xm ∈F are arbitrary, the ﬁrst m entries x1 . . . xm are called the
message digits, and the last n −m digits are called the check digits.
6.8.3
Hamming distance
Hamming distance is a natural distance function on V (n, p) meant to solve the
following problem. If c is a codeword for a code C ⊂V (n, p), and another
string c′ that diﬀers from c only by a transposition is received during a
transmission, can one determine whether c′ is the result of an error in sending
or receiving c? Before making the deﬁnition, let us ﬁrst deﬁne the weight ω(v)
of an arbitrary v ∈V (n, p).

6.8
Appendix: Linear Coding Theory
189
Deﬁnition 6.22. Suppose v = v1 . . . vn ∈V (n, p). Deﬁne the weight ω(v)
of v to be the number of nonzero components of v. That is,
ω(v) = |{i | vi ̸= 0}|.
The Hamming distance d(u, v) between any pair u, v ∈V (n, p) is deﬁned as
d(u, v) = ω(u −v).
For example, ω(1010111) = 5. The only vector of weight zero is the zero
vector. Therefore, ω(u −v) = 0 exactly when u = v. The reason d(u, v) is
called the distance is due to the following result.
Proposition 6.43. Suppose u, v, w ∈V (n, p). Then:
(i) d(u, v) ≥0, and d(u, v) = 0 if and only if u ̸= v;
(ii) d(u, v) = d(v, u); and
(iii) d(u, w) ≤d(u, v) + d(v, w).
Property (iii) is called the triangle inequality. It says that the length of
one side of a triangle cannot exceed the sum of the lengths of the other two
sides. In general, if S is any set, then a function d : S × S →R satisfying (i),
(ii), and (iii) is called a metric on S, and d(s, t) is deﬁned to be the distance
between s, t ∈S. The notion of a metric is a natural generalization of the
metric on a ﬁnite-dimensional inner product space. See Exercise 6.6.19 for
this.
Proof. Properties (i) and (ii) are clear, but the triangle inequality requires
proof. For the triangle inequality, ﬁrst consider the case that u and v diﬀer
in every component. Thus d(u, v) = n. Let w be any vector in V (n, p), and
suppose d(u, w) = k. Then u and w agree in n −k components, which tells
us that v and w cannot agree in those n−k components, so d(v, w) ≥n−k.
Thus
d(u, v) = n = k + (n −k) ≤d(u, w) + d(v, w).
In the general case, let u, v, w be given, and let u′, v′ and w′ denote the
strings obtained by dropping the components where u and v agree. Thus we
are in the previous case, so
d(u, v) = d(u′, v′) ≤d(u′, w′) + d(u, w′).
But d(u′, w′) ≤d(u, w) and d(v′, w′) ≤d(v, w), since dropping components
decreases the Hamming distance. Therefore,

190
6
Vector Spaces
d(u, v) ≤d(u, w) + d(v, w),
and the triangle inequality is established.
□
For each C ⊂V (n, p), let d(C) denote the minimum value of d(u, v) as
u, v vary over C. As we will see below, one wants to maximize d(C) for a
given value of |C|. When C ⊂V (n, p) is linear, then d(C) is the minimum of
the weights of all the nonzero codewords. That is,
d(C) = min{ω(c) | c ∈C, c ̸= 0}.
(The proof is an exercise.) This demonstrates one of the nice properties of
linear codes: the minimum distance d(C) requires only |C| computations,
which is considerably fewer than the number needed for an arbitrary code.
6.8.4
Error-correcting codes
In the terminology of coding theory, a code C ⊂V (n, p) such that |C| = M
and d(C) = d is known as a p-ary (n, M, d)-code. As mentioned above, the
game is to make the minimal distance d(C) as large as possible for a given
M. The reason for this is the next result.
Proposition 6.44. An (n, M, d)-code C detects up to d −1 errors and cor-
rects up to e = (d −1)/2 errors. That is, if c ∈C and d(v, c) ≤d −1, then
either v = c or v /∈C. Moreover, if v is not a codeword, then there exists at
most one codeword c such that d(v, c) ≤e.
Thus, if v /∈C, but d(v, c) ≤e, then we say that c is c error-correcting
for v. The conclusion about error-correction implies that if all but e digits of a
codeword c are known, then every digit of c is known. Note that if d(C) ≥3,
then two codewords cannot diﬀer by a transposition.
Example 6.45. Suppose C is a 6-bit code with d = 3. Then e = 1. If
c = 100110 is a codeword, then v = 010110, which diﬀers from c by a
transposition, cannot be a codeword. Also, w = 000110 can’t be in C, since
d(c, w) = 1, but c is the unique codeword within Hamming distance 1 of w. If
x = 000010, then d(c, x) = 2, so there could be other codewords c′ such that
d(c′, x) = 2. Note, however, that the triangle inequality says that d(c, c′) ≤
d(c, x) + d(x, c′) = 4, so if d(C) = 5, then in fact, c = c′.
⊠
Let us now prove the proposition.
Proof. We will leave the ﬁrst assertion as an exercise. So assume d(v, c) ≤
(d −1)/2, and suppose for some c′ ∈C that we have d(v, c′) ≤d(v, c). Then
d(v, c′) ≤(d −1)/2 too. By the triangle inequality,

6.8
Appendix: Linear Coding Theory
191
d(c, c′) ≤d(c, v) + d(v, c′) ≤(d −1)/2 + (d −1)/2 = d −1,
so indeed c = c′.
□
Example 6.46. For the binary 4-bit code of Example 6.43 given by x1 +
x2 + x3 + x3 + x4 = 0, one can check that d(C) = 2. Thus C detects a single
error, but e = 1/2, so there is no error correction. However, some additional
information, such as the component where an error occurs, may allow error
correction. Here, the linear equation deﬁning the code enables that to be
possible.
Designing codes that maximize d(C) given |C| is a basic problem in coding
theory. The next example is a binary (n, M, d) = (8, 16, 4) linear code C that
maximizes d(C) for M = 16. This code is sometimes denoted by C8 and
called the extended Hamming code.
Example 6.47. Let
A =
⎛
⎜
⎜
⎝
1
0
0
0
1
1
1
0
0
1
0
0
1
1
0
1
0
0
1
0
1
0
1
1
0
0
0
1
0
1
1
1
⎞
⎟
⎟
⎠.
Then C8 is deﬁned to be row(A). Since every row of A has weight 4, the
minimum distance d(C8) is at most 4. But every sum of the rows of A can be
seen to have weight at most 4, so d(C8) = 4. Since A is a generating matrix,
M = |C8| = 24 = 16.
⊠
We will now show that d(C8) is maximal for M = 16 and n = 8.
Proposition 6.45. The code C8 with 16 codewords maximizes |C| among
all 8-bit binary linear codes with d(C) ≥4.
Proof. Since dim C8 = 4, we have to show that there are no 8-bit binary
linear codes C with d(C) ≥4 and dim C ≥5, hence 32 codewords. This
turns out to be a routine argument involving row reduction. Suppose C is
such a code. By taking a spanning set for C as the rows of a k × 8 matrix
A, we can use row operations to put A into reduced row echelon form Ared
without aﬀecting C. Note that k ≥5. By reordering the columns, we can
suppose that Ared is a generating matrix (Ir | M), where r ≥5. Hence M
has at most three columns. But the requirement d(C) ≥4 implies that all
entries of M are 1. This shows that there must exist codewords of weight
two, a contradiction. Thus dim C < 5, so |C| = 16 is the maximum.
□

192
6
Vector Spaces
6.8.5
Cosets and perfect codes
A code C ⊂V (n, p) with minimum distance d(C) = d ≥3 and e = (d−1)/2 ≥
1 is called perfect if every x ∈V (n, p) is within e of some codeword c. Since
Proposition 6.44 says that every x ∈V (n, p) is within distance e of at most
one codeword, every element in V (n, p) is within e of exactly one codeword.
It is convenient to state this condition in a geometric way by bringing in the
notion of a ball. Assume r > 0. The ball of radius r centered at v ∈V (n, p)
is deﬁned to be
Br(v) = {w ∈V (n, p) | d(w, v) ≤r}.
(6.31)
Thus, a code C ⊂V (n, p) with d(C) ≥3 and e ≥1 is perfect if and only if
V (n, p) is the disjoint union of the balls Be(c) as c varies over C. That is,
V (n, p) =

c∈C
Be(c) (disjoint union).
(6.32)
Example 6.48. Consider the binary linear code C = {000, 111}. Note that
d = 3, so e = 1. Now
V (3, 2) = {000, 100, 010, 001, 110, 101, 011, 111}.
The ﬁrst four elements are within 1 of 000, and the last four are within 1 of
111. Therefore, C is perfect.
⊠
What makes perfect codes so nice is that there is a simple numerical crite-
rion for deciding whether C is perfect. First note that for every v ∈V (n, p),
we have |Be(v)| = |Be(0)|. Hence we have the following.
Proposition 6.46. A code C ⊂V (n, p) is perfect if and only if
|C||Be(0)| = pn.
Thus, if C is linear of dimension k, then C is perfect if and only if |Be(0)| =
pn−k.
One can check this criterion in the previous example, since
Be(0) = {000, 100, 010, 001},
while |C| = 2. Notice that a necessary condition for a binary code C of
length n with e = 1 to be perfect is that n + 1 = 2m for some m, since
B1(0) = {0, e1, . . . , en}. Thus perfect codes with e = 1 must have length 3,
7, 15, and so on. Here is an example with n = 7.

6.8
Appendix: Linear Coding Theory
193
Example 6.49. The linear code C7 ⊂V (7, 2) with generating matrix
A =
⎛
⎜
⎜
⎝
1
0
0
0
1
1
1
0
1
0
0
1
1
0
0
0
1
0
1
0
1
0
0
0
1
0
1
1
⎞
⎟
⎟
⎠
is perfect. Indeed, enumerating the 16 elements of C7, one sees that d(C7) = 3,
so e = 1. Clearly, |Be(0)| = 23, while |C7| = 24. Thus the criterion for
perfection, |C7||Be(0)| = 27, holds.
⊠
There is also a connection between cosets and perfect linear codes. Recall
that a linear code C ⊂V (n, p) of dimension k has exactly pn−k cosets. Thus,
C is perfect exactly when the number of cosets of C is |Be(0)|. Furthermore,
we have the following proposition.
Proposition 6.47. A linear code C ⊂V (n, p) is perfect if and only if every
coset x + C of C meets Be(0).
Proof. Assume dim(C) = k and suppose that every coset x+C meets Be(0).
We claim that x + C cannot contain more than one element of Be(0). For if
x + c and x + c′ lie in Be(0), then
d(x + c, x + c′) = d(c, c′) ≤d(c, 0) + d(0, c′) ≤2e = d −1,
so c = c′, since d(C) = d. Thus, pn−k ≤|Be(0)|. But since V (n, p) is the union
of the cosets of C, every element of Be(0) lies in a coset. Thus |Be(0)| = pn−k,
so C is perfect. On the other hand, suppose C is perfect and consider a coset
x+C. By deﬁnition, x+C meets some ball Be(c), where c ∈C. Hence there
exists c′ ∈C such that d(x + c′, c) ≤e. But d(x + c′, c) = d(x + (c′ −c), 0),
so x + C meets Be(0), since C is linear.
□
6.8.6
The hat problem
The solution of the hat problem is an example of a surprising application
of mathematics, in this case coding theory. Consider the following problem:
Three people are each wearing either a white hat or a black hat. Each player
can see the other two hats but not their own. Although the players are not
allowed to communicate with each other, they are allowed to discuss before
getting hats what strategy they would use. Each person has a buzzer with
three buttons marked B, W, and A (for black, white, or abstain). At the
same time, each person presses B, W, or A according to whether they want
to guess their hat color or abstain from guessing. If at least one player guesses
their color correctly, and nobody guesses incorrectly, they win a huge prize.

194
6
Vector Spaces
A pretty good strategy would be to agree that two players abstain and
the third makes a random guess. The probability of winning is 0.5. But this
strategy doesn’t make any use of fact that each player can see the other
two hats. Instead, consider the following strategy. Suppose the players agree
that they will play under the assumption that the array is not either BBB
or WWW. Why does this help? First of all, there are eight possible arrays,
so if the players can ﬁnd an algorithm to guarantee that they avoid the
set F = {BBB, WWW} and do not make a mistake, then they will have a
probability of 6/8 = 0.75 of winning. Now let’s analyze what happens if the
hat array is BWB, for example. The ﬁrst player sees WB, and knows that
both BWB and WWB lie outside F but can’t take the chance of an incorrect
guess. So the ﬁrst player must hit A. The same is true of the third player.
The second player is the key. That player sees BB, so is forced to press W or
otherwise land in F. If the array is outside of F, this strategy is guaranteed
to produce a victory. The question is why, and the answer is that the 3-bit
code C2 = {000, 111} in V (3, 2) is a perfect code with e = 1.
Let us now see whether this strategy can be extended to seven players
using the perfect linear code C7 in the previous example. The players agree
in the strategy session to proceed as if the hat array is not in C7. Since
|C7| = 24, the probability that the hat array is in C7 is 24/27 = 1/8, so
the probability of this being a winning assumption is 7/8. They all need
to memorize the 16 codewords of C7. Suppose their assumption is correct.
Then in order to win, they proceed as follows. Since the hat array x1 . . . x7
diﬀers in exactly one place from a codeword c1 . . . c7, let us suppose that the
discrepancy occurs at x1. Then player #1 sees c2 . . . c7 and must make the
choice c1 + 1. Player #2 sees x1c3 . . . c7. But since d(C7) = 3, she knows that
whatever x2 is, x1x2c3 . . . c7 /∈C. Therefore, in order to not make a mistake,
she must abstain, as do the other ﬁve players. Assuming that the hat array
x1 . . . x7 is not in C7, they have won the game. The odds that they win the
million bucks are a pretty good 7/8.
Can you devise a strategy for how to proceed if there are four, ﬁve, or six
players? Since there are no perfect codes in V (n, 2) for n = 4, 5, 6, it isn’t clear
how to proceed. More information about this problem and related questions
can be found in the article “The Hat Problem and Hamming Codes,” by M.
Bernstein, in Focus Magazine, November 2001.
Exercises
Exercise 6.8.1. List all binary linear codes C of length 3 with four code-
words.
Exercise 6.8.2. Find a formula for the number of linear codes C ⊂V (n, p)
of dimension k. (Suggestion: count the number of linearly independent subsets
of V (n, p) having k elements.)

6.8
Appendix: Linear Coding Theory
195
Exercise 6.8.3. The international standard book number (ISBN) is a linear
code C ⊂V (10, 11) that consists of all solutions a1a2 · · · a9a10 of the equation
a1 + 2a2 + 3a3 + · · · + 10a10 = 0.
The digits are hyphenated to indicate the book’s language and publisher. So,
for example, the ISBN of Fermat’s Enigma, by Simon Singh, published by
Penguin Books (14) in 1997, is 1-14-026869-3. The actual ISBNs in use satisfy
the condition 0 ≤ai ≤9 for all i ≤9, while a10 is also allowed to take the
value 10, which is denoted by the Roman numeral X.
(i) How many ISBNs are in use?
(ii) Determine all x such that 0-13-832x4-4 is an ISBN.
(iii) Determine all x and y such that both 1-2-3832xy4-4 and 3-33-x2y377-6
are ISBNs.
Exercise 6.8.4. Show that if C is a linear code, then
d(C) = min{ω(x) | x ∈C, x ̸= 0}.
Use the result to ﬁnd d(C) for the code C used to deﬁne ISBNs. Is this code
error-correcting?
Exercise 6.8.5. Taking F = F11, compute the generating matrix for the
ISBN code.
Exercise 6.8.6. Consider the binary code C ⊂V (6, 2) that consists of
000000 and the following nonzero codewords:
100111, 010101, 001011, 110010, 101100, 011110, 111001.
(i) Determine whether C is linear.
(ii) Compute d(C).
(iii) How many elements of C are nearest to 011111?
(iv) Determine whether 111111 is a codeword. If not, is there a codeword
nearest 111111?
Exercise 6.8.7. Prove the ﬁrst part of Proposition 6.44.
Exercise 6.8.8. Consider the binary code C7 deﬁned as the row space of
the matrix
A =
⎛
⎜
⎜
⎝
1
0
0
0
1
1
1
0
1
0
0
1
1
0
0
0
1
0
1
0
1
0
0
0
1
0
1
1
⎞
⎟
⎟
⎠.

196
6
Vector Spaces
in V (7, 2).
(i) Compute d(C) and e.
(ii) Find the unique element of C that is nearest to 1010010. Do the same
for 1110001.
Exercise 6.8.9. Let r be a positive integer and let x ∈V (n, 2). Consider
the ball Br(x) of radius r about x, i.e., Br(x) = {y ∈V (n, 2) | d(x, y) ≤r}.
Show that
|Br(x)| =
r

i=0

n
i

.
Exercise 6.8.10. Generalize Exercise 6.8.9 from V (n, 2) to V (n, p).
Exercise 6.8.11. * Show that if C ⊂V (n, 2) is a linear code such that
dim(C) = k and C is e-error-correcting, then
e

i=0

n
i

≤2(n−k).
Conclude that if e = 1, then 1 + n ≤2n−k.
Exercise 6.8.12. Suppose C ⊂V (n, 2) is a linear code with dim C = k and
d ≥3. Prove that C is perfect if and only if
e

i=0

n
i

= 2(n−k).
(6.33)
In particular, if e = 1, then C is perfect if and only if
(1 + n)2k = 2n.
(6.34)
Exercise 6.8.13. Consider the code C = {00000, 11111} ⊂V (5, 2).
(i) Determine e.
(ii) Show that C is perfect.
Exercise 6.8.14. Show that every binary [2k −1, 2k −1−k]-code with d = 3
is perfect. Notice that C7 is of this type.
Exercise 6.8.15. Can there exist a perfect code with n = 5 and e = 2?
Exercise 6.8.16. Suppose n ≡2 mod(4). Show that there cannot be a
perfect binary [n, k]-code with e = 2.
(iii) Does C present any possibilities for a ﬁve-player hat game?
Exercise 6.8.17. Show that every binary [23, 12]-code with d = 7 is perfect.

Chapter 7
Linear Mappings
The purpose of this chapter is to introduce linear mappings. Let V and W
be vector spaces over a ﬁeld F. A linear mapping is a mapping T : V →W
with domain V and target W that preserves linear combinations. The basic
situation we will consider is that both V and W are ﬁnite-dimensional. Here
we already know quite a bit, since an m × n matrix A over F deﬁnes a linear
mapping TA : Fn →Fm by putting TA(x) = Ax (see Section 3.1.7). The
basic rules of matrix algebra tell us that TA preserves linear combinations.
Thus, linear mappings are a generalization of matrix theory, and many ideas
from matrix theory, such as the rank of a matrix A, its null space N(A),
and the column space col(A), have natural interpretations in terms of linear
mappings, as we shall see.
7.1
Deﬁnitions and Examples
In this section, we will deﬁne linear mappings and introduce several terms
that we will use to explain the basic theory of linear mappings. We will also
give several examples to illustrate some of the interesting types of linear
mappings.
7.1.1
Mappings
Recall from Chap. 1 that if X and Y are sets, then a mapping F : X →Y is
a rule that assigns to each element of the domain X a unique element F(x)
in the target Y . The image of F is the subset of the target F(X) = {y ∈Y |
y = F(x) ∃x ∈X}. A mapping F : Fn →Fm is completely determined by
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 7
197

198
7
Linear Mappings
its component functions f1, f2, . . . , fm, which are obtained by writing
F(x) =
m

i=1
fi(x)ei =
⎛
⎜
⎝
f1(x)
...
fm(x)
⎞
⎟
⎠,
where e1, . . . , em is the standard basis of Fm. If V and W are arbitrary vector
spaces over a ﬁeld F and W is ﬁnite-dimensional, we can also deﬁne compo-
nent functions h1, . . . , hm of F with respect to an arbitrary basis w1, . . . , wm
of W in the same way by writing
F(v) =
m

i=1
hi(v)wi.
The component functions with respect to a basis are uniquely determined
by F.
7.1.2
The deﬁnition of a linear mapping
In linear algebra, the most important mappings are those that preserve linear
combinations. These are called linear mappings.
Deﬁnition 7.1. Suppose V and W are vector spaces over a ﬁeld F. Then a
mapping T : V →W is said to be linear if
(i) for all x, y ∈V , T(x + y) = T(x) + T(y), and
(ii) for all r ∈F and all x ∈V , T(rx) = rT(x).
If W = F, then T is called a linear function.
By deﬁnition, a linear mapping T preserves linear combinations: for all
r, s ∈F and all x, y ∈V
T(rx + sy) = rT(x) + sT(y).
Thus a linear mapping also preserves linear combinations of an arbitrary num-
ber of vectors. Conversely, every mapping that preserves linear combinations
is a linear mapping.
7.1.3
Examples
We now present some basic examples of linear mappings. The reader should
note that some of the examples don’t require a particular basis or choice of
coordinates in their deﬁnition.

7.1
Deﬁnitions and Examples
199
Example 7.1 (Identity mapping). The mapping
IV : V →V
deﬁned by IV (x) = x is called the identity mapping. This mapping is clearly
linear. If V = Fn, then IV = TIn.
⊠
Example 7.2. If a ∈Rn, the dot product with a deﬁnes a linear function
Sa : Rn →R by Sa(x) = a · x = aT x. It turns out that every linear function
S : Rn →R has the form Sa for some a ∈Rn. For example, let aT = (1 2 0 1).
Then the linear function Sa : R4 →R has the explicit form
Sa(x) = (1 2 0 1)
⎛
⎜
⎜
⎝
x1
x2
x3
x4
⎞
⎟
⎟
⎠= x1 + 2x2 + x4.
We will consider the analogue of this function for an arbitrary inner product
space in the next example.
⊠
Example 7.3. More generally, suppose V is an inner product space of
dimension m. Then for every linear mapping T : V →R, there exists a
unique w ∈V such that T(v) = (v, w). In fact, if u1, . . . , um constitute an
orthonormal basis of V , then I claim that
w =
m

i=1
T(ui)ui.
The reader should check that T(v) = (v, w) does in fact hold for all v ∈V
and that this choice of w is unique.
⊠
Example 7.4 (Diagonal mappings). Let T : R2 →R2 be the mapping
T
	x1
x2

=
	μ1x1
μ2x2

,
(7.1)
where μ1 and μ2 are real scalars. We leave it as an exercise to show that T
is linear. In (7.1), T(e1) = μ1e1 and T(e2) = μ2e2. If both μ1 and μ2 are
nonzero, the image under T of a rectangle with sides parallel to e1 and e2 is
a parallel rectangle whose sides have been dilated by μ1 and μ2 and whose
area has been changed by the factor |μ1μ2|. If μ1μ2 ̸= 0, then T maps a circle
about the origin to an ellipse about the origin. For example, the image of the
unit circle x2 + y2 = 1 is the ellipse
(w1
μ1
)2 + (w2
μ2
)2 = 1,

200
7
Linear Mappings
as can be seen by putting w1 = μ1x1 and w2 = μ2x2.
⊠
In general, a linear mapping T : V →V is called semisimple if there exist
a basis v1, . . . , vn of V and scalars μ1, . . . , μn in F such that T(vi) = μivi
for all i. Note that we are not requiring that any or all μi be nonzero; but
if all μi = 0, then T is the zero linear mapping. The basis v1, . . . , vn of V
for which T(vi) = μivi is called an eigenbasis, and the scalars μ1, . . . , μn are
called the eigenvalues of T. The existence of an eigenbasis for T says a great
deal about how T acts. The question of when a linear mapping T admits an
eigenbasis is very basic. It will be solved when we study the classiﬁcation of
linear mappings. The solution is nontrivial, and not all linear mappings are
semisimple.
Example 7.5. Recall from Example 6.17 that the cross product of two vec-
tors a, b ∈R3 is deﬁned as
a × b = (a2b3 −a3b2, −(a1b3 −a3b1), a1b2 −a2b1)T .
(7.2)
The cross product deﬁnes a linear mapping Ca : R3 →R3 by
Ca(v) = a × v.
Notice that Ca(a) = 0. A basic property of Ca is that Ca(x) is orthogonal
to both a and x. It follows that Ca cannot be semisimple unless a = 0 (see
Example 7.4).
⊠
7.1.4
Matrix linear mappings
Recall from Section 3.1.7 that if A ∈Fm×n, then the mapping TA : Fn →Fm
deﬁned by TA(x) = Ax is called a matrix linear mapping. Every matrix
linear mapping is certainly linear. We now show that every linear mapping
T : Fn →Fm is a matrix linear mapping.
Proposition 7.1. Every linear mapping T : Fn →Fm is of the form TA for
a unique A ∈Fm×n. In fact,
A =

T(e1) T(e2) · · · T(en)

.
Conversely, if A ∈Fm×n, then TA is a linear mapping with domain Fn and
target Fm.
Proof. Since x = n
i=1 xiei,
T(x) = T

n

i=1
xiei

=
n

i=1
xiT(ei) =

T(e1) T(e2) · · · T(en)

x.

7.1
Deﬁnitions and Examples
201
Thus, T = TA, where A =

T(e1) T(e2) · · · T(en)

. Furthermore, A is
uniquely determined by T, since if two linear mappings S, T : Fn →Fm have
the property that S(ei) = T(ei) for i = 1, . . . , n, then S(x) = T(x) for all
x ∈Fn. The claim that every TA is linear has already been shown.
⊓⊔
In particular, a linear function T : Fn →F is given by a 1×n matrix. Thus,
there exists a unique a ∈Fn such that T(x) = aT x. Hence there exist unique
scalars a1, a2, . . . , an ∈F such that T(x) = n
i=1 aixi. When F = R, we may
express this fact in terms of the dot product as T(x) = a · x in Example 7.2.
Example 7.6. For example, the matrix of the identity mapping IFn on Fn
is the identity matrix In. That is, IFn = TIn. We will sometimes use In to
denote IFn, provided no confusion is possible.
⊠
Recall from Section 3.1.7 that if S : Fp →Fn and T : Fn →Fm are
linear mappings with matrices S = TA and T = TB respectively, then the
composition T ◦S : Rp →Rm is the matrix linear mapping associated to BA.
That is,
TB ◦TA = TBA.
Writing MT for the matrix of T etc., we therefore have the identity
MT ◦S = MT MS.
We will not repeat the proof, but it is short. Somewhat surprisingly, the key
fact is that matrix multiplication is associative.
7.1.5
An Application: rotations of the plane
The relationship between composition and matrix multiplication can be
applied to the group Rot(2) of rotations of R2 to give an extremely pretty
and simple proof of the sum formulas for the trigonometric functions sine
and cosine. Recall that Rθ : R2 →R2 is the rotation of R2 about the origin
through θ. Computing the images of R(e1) and R(e2), we have
Rθ(e1) = cos θe1 + sin θe2
and
Rθ(e2) = −sin θe1 + cos θe2.
I claim that rotations are linear. This can be seen as follows. Suppose x and
y are any two noncollinear vectors in R2, and let P be the parallelogram
they span. Then Rθ rotates the whole parallelogram P about 0 to a new

202
7
Linear Mappings
parallelogram Rθ(P) with edges Rθ(x) and Rθ(y) at 0. Since the diagonal
x + y of P is rotated to the diagonal of Rθ(P), it follows that
Rθ(x + y) = Rθ(x) + Rθ(y).
Similarly, for every scalar r,
Rθ(rx) = rRθ(x).
Therefore, Rθ is linear, as claimed. The matrix Rθ of Rθ, calculated via the
above formula, is Rθ = (Rθ(e1) Rθ(e2)). Hence
Rθ =
	cos θ −sin θ
sin θ
cos θ

.
(7.3)
(This veriﬁes the formula of Example 4.5.) Thus
Rθ
	x
y

=
	cos θ −sin θ
sin θ
cos θ

 	x
y

=
	x cos θ −y sin θ
x sin θ + y cos θ

.
Let us now apply this result to trigonometry. If one ﬁrst applies a rotation
Rψ and follows that by the rotation Rθ, the outcome is the rotation Rθ+ψ.
Hence,
Rθ+ψ = Rθ ◦Rψ = Rψ ◦Rθ.
Therefore, since composition of linear mappings corresponds to multiplication
of their matrices, Rθ+ψ = RθRψ = RψRθ. In particular,
	cos(θ + ψ) −sin(θ + ψ)
sin(θ + ψ) cos(θ + ψ)

=
	cos θ −sin θ
sin θ
cos θ

 	cos ψ −sin ψ
sin ψ
cos ψ

.
Expanding the product and comparing both sides gives us the trigonometric
formulas for the sine and cosine of θ + ψ:
cos(θ + ψ) = cos θ cos ψ −sin θ sin ψ
and
sin(θ + ψ) = sin θ cos ψ + cos θ sin ψ.
Exercises
Exercise 7.1.1. Let X and Y be sets and φ : X →Y a mapping. Recall
from Chap. 1 that φ is injective if and only if for x ∈X, φ(x) = φ(x′) implies
x = x′, φ is surjective if and only if φ(X) = Y , and φ is a bijection if and
only if it is both injective and surjective. Show the following:

7.1
Deﬁnitions and Examples
203
(i) φ is injective if and only if there exists a mapping ψ : F(X) →X such
that ψ ◦φ is the identity mapping IX : X →X.
(ii) φ is surjective if and only if there exists a mapping ψ : Y →X such that
φ ◦ψ is the identity mapping IY , and
(iii) φ is a bijection if and only if there exists a mapping ψ : Y →X such
that ψ ◦φ = IX and φ ◦ψ = IY .
Exercise 7.1.2. Show that every linear function T : R →R has the form
T(x) = ax for some a ∈R.
Exercise 7.1.3. Determine whether any of the following functions f : R2 →
R are linear:
(i) f(x, y) = xy,
(ii) f(x, y) = x −y,
(iii) f(x, y) = ex+y.
Exercise 7.1.4. Suppose T : Fn →Fm is an arbitrary mapping and write
T(v) = (f1(v), f2(v), . . . , fm(v))T .
Show that T is linear if and only if each component function fi is a linear
function.
Exercise 7.1.5. Find the matrix of the following linear mappings:
(i) S(x1, x2, x3) = (2x1 −3x3, x1 + x2 −x3, x1, x2 −x3)T .
(ii) T(x1, x2, x3, x4) = (x1 −x2 + x3 + x4, x2 + 2x3 −3x4)T .
(iii) T ◦S.
Exercise 7.1.6. Let V be a vector space over F, and let W be a subspace
of V . Let π : V →V/W be the quotient map deﬁned by π(v) = v +W. Show
that π is linear.
Exercise 7.1.7. Let S : U →V and T : V →W be linear mappings. Show
that T ◦S is also linear.
Exercise 7.1.8. Suppose A is a real m×n matrix. Show that when we view
both row(A) and N(A) as subspaces of Rn,
row(A) ∩N(A) = {0}.
Is this true for matrices over other ﬁelds, for example Fp or C?

204
7
Linear Mappings
Exercise 7.1.9. Let V = C, and consider the mapping S : V →V deﬁned
by S(z) = αz, where α ∈C.
(i) Describe S as a mapping S : R2 →R2.
(ii) Is S linear over R? If so, ﬁnd its matrix in R2×2.
Exercise 7.1.10. Show that every mapping S : C →C that is linear over
C has the form S(z) = αz for a unique A ∈C.
Exercise 7.1.11. Let TA : R2 →R2 be the matrix linear mapping associ-
ated to the matrix A =
	
a
b
c
d

. The purpose of this exercise is to determine
when T is linear over C. That is, since by deﬁnition, C = R2 (with complex
multiplication), we may ask when TA(αβ) = αTA(β) for all α, β ∈C. Show
that a necessary and suﬃcient condition for TA to be C-linear is that a = d
and b = −c.
Exercise 7.1.12. Show that every rotation Rθ deﬁnes a C-linear map Rθ :
C →C. Relate this map to the complex exponential eiθ.
Exercise 7.1.13. Let C∞(R) be the space of inﬁnitely diﬀerentiable func-
tions on the real line R. A function f ∈C∞(R) is said to be even if
f(−x) = f(x) for all x ∈R and odd if f(−x) = −f(x) for all x ∈R. Let
C∞(R)ev and C∞(R)odd denote the set of even and odd functions in C∞(R)
respectively.
(i) Show that C∞(R)ev and C∞(R)odd are subspaces of C∞(R).
(ii) Show that the mapping D : C∞(R) →C∞(R) deﬁned by D(f) = f ′
sends C∞(R)ev to C∞(R)odd.
Exercise 7.1.14. Let F be a Galois ﬁeld, and let p be the characteristic of
F. Let T : F →F be the mapping deﬁned by T(x) = xp. Recall that the set
of multiples m1 of 1 in F, where m = 0, 1, . . . , p −1, forms a subﬁeld F′ = Fp
of F and that F is a vector space over F′. Show that T is a linear mapping of
F with respect to this vector space structure. The linear mapping T is called
the Frobenius map.
Exercise 7.1.15. As in Exercise 7.1.14, let F be a Galois ﬁeld of character-
istic p, and let V = Fn×n. Show that the mapping F : V →V deﬁned by
F(A) = Ap is a linear mapping.

7.2
Theorems on Linear Mappings
205
7.2
Theorems on Linear Mappings
In this section, we will deﬁne some more terms and prove several results
about linear mappings, including a result that will generalize the rank–nullity
identity dim N(A) + dim col(A) = n derived in Example 6.28.
7.2.1
The kernel and image of a linear mapping
The kernel and image are two natural subspaces associated with a linear
mapping. Let T : V →W be linear. The image of T has been deﬁned in
Chap. 1. It will be denoted by im(T). The other natural subspace associated
with T is its kernel. First note that every linear mapping T maps 0 to 0,
since
T(0) = T(00) = 0T(0) = 0.
Deﬁnition 7.2. The kernel of T is deﬁned to be the set
ker(T) = {v ∈V | T(v) = 0}.
Since a linear mapping is a group homomorphism, the notion of the kernel
of a linear mapping is a special case of the notion of the kernel of a homo-
morphism. Suppose V = Fn, W = Fm, and T = TA, i.e., T(x) = Ax. Then
ker(TA) and im(TA) are related to linear systems. In fact, ker(TA) = N(A),
while im(TA) is the set of vectors b ∈Fm for which Ax = b has a solution.
Thus, im(TA) = col(A), so both ker(TA) and im(TA) are subspaces. This is
more generally true for arbitrary linear mappings.
Proposition 7.2. The kernel and image of a linear mapping T : V →W
are subspaces of V and W respectively.
Proof. We leave this as an exercise.
⊓⊔
The following result gives a very useful characterization of injective (equiv-
alently, one-to-one) linear mappings. The proof is almost word for word the
proof given in Chap. 2 for the group-theoretic analogue.
Proposition 7.3. A linear mapping T is injective if and only if ker(T) =
{0}.
Proof. Suppose ker(T) = {0} and T(x) = T(y). Then T(x −y) = 0, so
x −y ∈ker(T). But this says that x −y = 0, so T is injective. Conversely,
if T is injective and x ∈ker(T), then T(x) = 0 = T(0), so x = 0. Thus
ker(T) = {0}.
⊓⊔

206
7
Linear Mappings
7.2.2
The Rank–Nullity Theorem
Let A ∈Fm×n and recall the rank–nullity identity dim N(A)+dim col(A) = n
(see (6.7)). The general rank–nullity theorem generalizes this identity to a
linear mapping T : V →W, where V is ﬁnite-dimensional.
Theorem 7.4 (Rank–Nullity Theorem). Let V and W be vector spaces over
F, and suppose dim V is ﬁnite. Then for every linear mapping T : V →W,
dim ker(T) + dim im(T) = dim V.
(7.4)
Proof. If ker(T) = V , then im(T) = {0}, so there is nothing to prove. On the
other hand, if dim ker(T) = 0, then T is injective. Thus if v1, v2, . . . , vn is a
basis of V , it follows that T(v1), . . . , T(vn) is a basis of im(T). To verify this,
it suﬃces to show that the T(vi) are independent. But if  aiT(vi) = 0,
then T( aivi) = 0, so  aivi = 0, since T is injective. Hence all ai are
equal to zero, since v1, v2, . . . , vn are independent. Thus, dim im(T) = dim V .
Now suppose dim ker(T) = k > 0. By the dimension theorem (Theorem
6.11), we may choose a basis v1, v2, . . . vk of ker(T) and extend it to a basis
v1, v2, . . . , vn of V . Let wi = T(vi). I claim that wk+1, . . . , wn are a basis
of im(T). To see that they span, let w ∈im(T), say w = T(v). Write v =
 aivi. Then
w = T(v) =
n

i=1
aiT(vi) =
n

i=k+1
aiwi.
The proof that wk+1, . . . , wn are independent is identical to the proof in the
case dim ker(T) = 0, so we will leave it to the reader. Hence dim im(T) =
n −k, which proves the result.
⊓⊔
7.2.3
An existence theorem
The rank–nullity theorem tells us something about the behavior of a given
linear mapping, but we do not yet know how to construct linear mappings.
The following existence result will show that there is considerable ﬂexibility
in deﬁning a linear mapping T : V →W, provided we have a basis of V (for
example, if V is ﬁnite-dimensional). We will show that the values of T on a
basis of V can be arbitrarily described.
Proposition 7.5. Let V and W be vector spaces over F, and let v1, . . . , vn
be a basis of V . Suppose w1, . . . , wn are arbitrary vectors in W. Then there
exists a unique linear mapping T : V →W such that T(vi) = wi for each i.
In other words, a linear mapping is uniquely determined by giving its values
on a basis.

7.2
Theorems on Linear Mappings
207
Proof. The proof is surprisingly simple. Since every x ∈V has a unique
expression
x =
n

i=1
rivi,
we obtain a mapping T : V →W by setting
T(x) =
n

i=1
riwi.
In fact, T is linear. Indeed, if y ∈V , say
y =
n

i=1
sivi,
then x + y = (ri + si)vi, so
T(x + y) =
n

i=1
(ri + si)wi =
n

i=1
riwi +
n

i=1
siwi = T(x) + T(y).
Similarly, T(rv) = rT(v). Moreover, T is unique, since every linear mapping
is determined on a basis.
⊓⊔
If V = Fn and W = Fm, there is an even simpler proof by appealing to
matrix theory. Let B = (v1 v2 . . . vn) and C = (w1 w2 . . . wn). Then the
matrix A of T satisﬁes AB = C. But B is invertible, since v1, . . . , vn is a
basis of Fn, so A = CB−1.
7.2.4
Vector space isomorphisms
We will now answer the following question. When are two vector spaces indis-
tinguishable as far as their algebraic properties are concerned? The answer
is given by the notion of isomorphism.
Deﬁnition 7.3. We will say that two vector spaces V and W over the same
ﬁeld F are isomorphic if there exists a bijective linear mapping T : V →W.
Such a linear mapping T is called an isomorphism.
In other words, an isomorphism is a linear mapping that is both injective
and surjective. A vector space isomorphism is also a group isomorphism. The
point of the deﬁnition is that although two vector spaces V and W may look
quite diﬀerent, isomorphic vector spaces are indistinguishable if one consid-
ers only their internal algebraic properties (addition, scalar multiplication,

208
7
Linear Mappings
etc.). For example, one space V may be the solution space of a homogeneous
linear diﬀerential equation with real coeﬃcients, and the other, W, may be
a far less exotic vector space such as Rn. The question is, when are V and
W isomorphic? The quite simple answer, provided by the next result, is an
application of the dimension theorem and the fact that linear mappings are
determined by assigning arbitrary values to a basis.
Proposition 7.6. Two ﬁnite-dimensional vector spaces V and W over the
same ﬁeld are isomorphic if and only if they have the same dimension.
Proof. Suppose dim V = dim W. To construct an isomorphism T : V →W,
choose bases v1, . . . , vn of V and w1, . . . , wn of W, and let T : V →W be the
unique linear mapping (guaranteed by Proposition 7.5) such that T(vi) = wi
if 1 ≤i ≤n. Since im(T) is a subspace of W containing a basis of W, it follows
that T is surjective. By the rank–nullity theorem, dim ker(T) = 0; hence T
is injective by Proposition 7.3. The converse follows from the rank–nullity
theorem (Theorem 7.4) by similar reasoning.
⊓⊔
The set of isomorphisms T : V →V in fact forms a group, called the
general linear group of V , which is denoted by GL(V ). If V = Fn, then in
fact, GL(V ) = GL(n, F).
Exercises
Exercise 7.2.1. Let the ﬁeld be F2 and consider the matrix
A =
⎛
⎜
⎜
⎜
⎜
⎝
1
1
1
1
1
0
1
0
1
0
1
0
1
0
1
1
0
0
1
1
1
0
1
1
0
⎞
⎟
⎟
⎟
⎟
⎠
.
(i) Find a basis of im(A).
(ii) How many elements are in im(A)?
(iii) Is (01111)T in im(A)?
(iv) Without any further computation, ﬁnd a basis of im(AT ).
Exercise 7.2.2. Let A be a real 3 × 3 matrix such that the ﬁrst row of A is
a linear combination of A’s second and third rows.
(i) Show that N(A) is either a line through the origin or a plane containing
the origin.
(ii) Show that if the second and third rows of A span a plane P, then N(A)
is the line through the origin orthogonal to P.

7.2
Theorems on Linear Mappings
209
Exercise 7.2.3. Prove Proposition 7.2 using only the basic deﬁnition of a
linear mapping. That is, show that the kernel and image of a linear mapping
T are subspaces of the domain and target of T respectively.
Exercise 7.2.4. Consider the mapping Ca : R3 →R3 deﬁned by
Ca(v) = a × v,
where a × v is the cross product of a and v.
(i) Show that Ca is linear and ﬁnd its matrix.
(ii) Describe the kernel and image of Ca.
Exercise 7.2.5. Suppose a ∈R3. Find the kernel of the linear mapping
Ca + I3.
Exercise 7.2.6. Suppose T : R2 →R2 is a linear mapping that sends pair
of noncollinear vectors to noncollinear vectors. Suppose x and y in R2 are
noncollinear. Show that T sends every parallelogram with sides parallel to x
and y to another parallelogram with sides parallel to T(x) and T(y).
Exercise 7.2.7. Find the kernel and image of the linear mapping Sa : Rn →
R deﬁned by Sa(x) = a · x.
Exercise 7.2.8. Determine ker(Ca) ∩im(Ca) for the cross product linear
mapping Ca : R3 →R3 for every nonzero a.
Exercise 7.2.9. Suppose V is a ﬁnite-dimensional vector space and T : V →
V is a linear mapping such that T ◦T = O. Show that im(T) ⊂ker(T). Is
the converse true?
Exercise 7.2.10. Suppose V is a ﬁnite-dimensional vector space and T :
V →V is a linear mapping such that im(T) ⊂ker(T). Show that dim V is
an even integer.
Exercise 7.2.11. Suppose A is a symmetric real n × n matrix.
(i) Show that col(A) ∩N(A) = {0}.
(ii) Conclude that Rn = N(A) ⊕col(A).
(iii) Suppose A2 = O. Show that A = O.
Exercise 7.2.12. Find a nonzero 2 × 2 symmetric matrix A over C such
that A2 = O.
Exercise 7.2.13. Show that if V is a ﬁnite-dimensional vector space and T
is a linear mapping with domain V , then dim T(V ) ≤dim V .

210
7
Linear Mappings
Exercise 7.2.14. Suppose A is a square matrix over an arbitrary ﬁeld. Show
that if Ak = O for some positive integer k, then dim N(A) > 0.
Exercise 7.2.15. Recall that if x, y ∈Rn, then x·y = xT y. Use this to prove
that for every A ∈Rn×n, AT A and A have the same null space. Conclude
that AT A and A have the same rank.
Exercise 7.2.16. This exercise deals with the inverse of a linear mapping
T. Let V be a vector space over F, and let T : V →V be a linear mapping
such that ker(T) = 0 and im(T) = V . That is, T is an isomorphism. Prove
the following statements.
(i) There exists a linear mapping S : V →V with the property that S(y) = x
if and only if T(x) = y. Note: S is called the inverse of T.
(ii) Show that S is an isomorphism, and S ◦T = T ◦S = IV .
(iii) If V = Fn, A is the matrix of T, and B is the matrix of S, then
BA = AB = In.
Exercise 7.2.17. Let S : Rn →Rm and T : Rm →Rp be two linear map-
pings both of which are injective. Show that the composition T ◦S is also
injective. Conclude that if A is of size m × n and has N(A) = {0}, and B is
of size n × p and has N(B) = {0}, then N(BA) = {0} too.
Exercise 7.2.18. Suppose V and W are vector spaces of the same dimension
over a ﬁeld F, and let T : V →W be a linear mapping. Show that if T is
either injective or surjective, then T is an isomorphism.
Exercise 7.2.19. Let W be a subspace of a ﬁnite-dimensional vector space
V . Show there exists a linear mapping T : V →V such that ker(T) = W.
Exercise 7.2.20. Suppose T : V →W is linear. Show that there exists a
unique linear mapping T : V/ ker(T) →W such that T(v + ker(T)) = T(v).
Exercise 7.2.21. Let U, V , and W be ﬁnite-dimensional vector spaces over
the same ﬁeld F, and let S : U →V and T : V →W be linear. Show that:
(i) TS is injective if and only if S is injective and im(S) ∩ker(T) = {0}.
(ii) TS is surjective if and only if T is surjective and V = im(S) + ker(T).
(iii) Conclude that TS is an isomorphism if and only if S is injective, T is
surjective, and dim U = dim W.

7.3
Isometries and Orthogonal Mappings
211
7.3
Isometries and Orthogonal Mappings
Throughout this section, V will denote a real ﬁnite-dimensional inner prod-
uct space. Recall that the inner product on V deﬁnes a distance function
d(a, b) = |a −b|. Linear mappings T : V →V that preserve distances are
called isometries. (Actually, as we will see below, a distance-preserving map
is automatically linear, so the deﬁnition can be simpliﬁed.) The isometries of
V form an important group, called O(V ). When V = Rn, O(V ) is the matrix
group O(n, R) of all orthogonal n × n matrices over R. We will ﬁrst consider
isometries in general and then specialize to rotations and reﬂections. Finally,
we will consider the isometries of R2. Here we prove the O(2, R)-dichotomy:
every isometry of R2 is either a rotation or a reﬂection. We will also show
that the dihedral group D(m) can be realized as the group of all isometries
of a regular m-gon in R2. This will give another proof that the order of D(m)
is 2m.
7.3.1
Isometries and orthogonal linear mappings
A mapping S : V →V is said to be orthogonal if
(S(x), S(y)) = (x, y)
(7.5)
for all x, y ∈V . The term orthogonal comes from the fact that an orthogonal
mapping preserves the orthogonal relationship between orthogonal pairs of
vectors. Since orthogonal mappings preserve inner products, they preserve
lengths of vectors, distances between vectors, and angles between vectors,
since the angle θ between v and w is found by the identity
v · w = |v||w| cos θ.
It turns out that all orthogonal mappings are linear.
Proposition 7.7. Let S : V →V be orthogonal. Then S is linear. In fact,
the orthogonal mappings are exactly the isometries. In particular, all distance-
preserving mappings on V are linear. Conversely, every isometry is orthogo-
nal.
Proof. To show that S is linear, we ﬁrst show that for all a, b ∈V , |S(a +
b) −S(a) −S(b)|2 = 0. But
|S(a+b)−S(a)−S(b)|2 = (S(a+b)−S(a)−S(b), S(a+b)−S(a)−S(b))
= (S(a + b), S(a + b)) + (S(a), S(a)) + (S(b), S(b)) −2(S(a + b), S(a))

212
7
Linear Mappings
−2(S(a + b), S(b)) + 2(S(a), S(b)).
Using the fact that S is orthogonal, it follows that
|S(a + b) −S(a) −S(b)|2 = (a + b, a + b) + (a, a) + (b, b)
−2(a + b, a) −2(a + b, b) + 2(a, b).
Expanding further, one sees that the right-hand side is zero. Thus, S(a+b) =
S(a)+S(b). The proof that S(ra) = rS(a) for all r ∈F is similar. Therefore,
every orthogonal mapping is linear. To see that S is an isometry, note that
|S(b) −S(a)|2 = |S(b −a)|2
= (S(a −b), S(a −b))
= (b −a, b −a)
= |b −a|2.
This completes the proof that orthogonal mappings are isometries. We will
leave the rest of the proof as an exercise.
⊓⊔
More generally, every mapping F : V →V that preserves distances turns
out to be orthogonal, and hence is an isometry (see Exercise 7.3.3).
7.3.2
Orthogonal linear mappings on Rn
We are now going to show that the matrix of an isometry is orthogonal, and
conversely, that every orthogonal matrix deﬁnes an isometry.
Proposition 7.8. Every isometry T : Rn →Rn is the matrix linear map-
ping associated with a unique orthogonal matrix. Conversely, every orthogonal
matrix deﬁnes a unique isometry.
Proof. Suppose T : Rn →Rn is an isometry. Since T is linear, T = TQ for a
unique Q ∈Rn×n. But T is orthogonal, so TQ(x) · TQ(y)=Qx · Qy=x · y for
all x, y∈Rn. Thus
(Qx)T (Qy) = (xT QT )(Qy) = xT (QT Q)y = xT y
(7.6)
for all x, y ∈Rn. Setting Qei = qi, (7.6) implies qT
i qj = eT
i ej. Hence
QT Q = In, so Q is orthogonal. We leave the converse to the reader.
⊓⊔
Recall that the set of orthogonal n × n matrices is the orthogonal group
O(n, R), so O(n, R) is also the group of isometries of Rn.

7.3
Isometries and Orthogonal Mappings
213
7.3.3
Projections
Let a ∈R2 be nonzero, and consider the line Ra spanned by a. In Section
6.6.2, we called the mapping
Pa(x) =
a · x
a · a

a
the projection onto Ra. We leave it as an exercise to show that the projection
Pa is linear. Note that if u is the unit vector determined by a, then since
a · a = |a|2, it follows that
Pu(x) = Pa(x) = (uT x)u.
Thus,
Pu
	x1
x2

= (u1x1 + u2x2)
	u1
u2

,
so
Pu
	x1
x2

=
	 u2
1
u1u2
u1u2
u2
2

 	x1
x2

.
Hence the matrix of Pu is
	
u2
1
u1u2
u1u2
u2
2

.
(7.7)
Note that a projection matrix is symmetric. The image of the projection Pu
is the line Ru, while its kernel is the line orthogonal to Ru. A reﬂection Pu
has the property that Pu ◦Pu = Pu, since
Pu ◦Pu(v) = Pu((v · u)u)) = (v · u)Pu(u) = (v · u)u = Pu(v).
We will next apply projections to ﬁnd a general formula for the reﬂection Rn
through a hyperplane.
7.3.4
Reﬂections
We will ﬁrst ﬁnd an expression for a plane reﬂection and use that to suggest
how to deﬁne a reﬂection of Rn through a hyperplane. Let ℓbe a line through
the origin of R2. We want to consider how to reﬂect R2 through ℓ. The sought-
after mapping H : R2 →R2 ﬁxes every vector on ℓand sends every vector v
orthogonal to ℓto −v. (If a reﬂection is linear, this is enough information to
determine H.) Suppose v ∈R2 is on neither ℓnor ℓ⊥. Then v and its reﬂection
H(v) form an isosceles triangle with equal sides v and H(v), which H swaps.

214
7
Linear Mappings
Choosing a unit vector u on ℓ⊥, let us write
v = Pu(v) + c,
where c ∈ℓ. This is the orthogonal decomposition of v with respect to ℓand
ℓ⊥. Hence the component on ℓis c = v −Pu(v). By Euclidean geometry,
H(v) = −Pu(v) + c. Replacing c by v −Pu(v), we get the formula
H(v) = −Pu(v) + (v −Pu(v)) = v −2Pu(v).
Therefore,
H(v) = v −2Pu(v) = v −2(u · v)u = v −2(uT v)u.
(7.8)
This expression immediately establishes the following result.
Proposition 7.9. The reﬂection H of R2 through a line ℓpassing through
0 is a linear mapping.
Example 7.7. Let us ﬁnd the reﬂection H through the line ℓgiven by x =
−y. Now u = ( 1
√
2,
1
√
2)T is a unit vector on ℓ⊥. Thus,
H
	a
b

=
	a
b

−2(
	a
b

·
 1
√
2
1
√
2

)
 1
√
2
1
√
2

=
	
a −(a + b)
b −(a + b)

=
	−b
−a

.
Thus the matrix of H is
	
0
−1
−1
0

.
⊠
Now let Q denote the matrix of the reﬂection through the line orthogonal
to a unit vector u ∈R2. Applying (7.7) and (7.8), one sees that
Q =
	
1 −2u2
1
−2u1u2
−2u1u2
1 −2u2
2

=
	
u2
2 −u2
1
−2u1u2
−2u1u2
u2
1 −u2
2

.
(7.9)
Thus, every 2 × 2 reﬂection matrix has the form
Q =
	a
b
b
−a

,
(7.10)

7.3
Isometries and Orthogonal Mappings
215
where a2 + b2 = 1. Conversely, as we will show below, a matrix of the form
Q =
	
a
b
b
−a

, where a2 + b2 = 1, is a reﬂection. Hence, the symmetric
2 × 2 orthogonal matrices are exactly the matrices of reﬂections.
Let us now ask what the linear mapping H deﬁned by (7.8) does in the
case of Rn. The equation v·u = 0 deﬁnes an (n−1)-dimensional subspace W
of Rn, namely the hyperplane through the origin orthogonal to u. If v ∈W,
then H(v) = v. On the other hand, if v = ru, then H(v) = rH(u) =
r(u −2u) = −ru = −v. In particular, H leaves the hyperplane W pointwise
ﬁxed and reverses vectors on the line W ⊥orthogonal to this hyperplane.
Deﬁnition 7.4. Let u ∈Rn be a unit vector, and let W be the hyperplane
in Rn orthogonal to the line Ru. Then the linear mapping H : Rn →Rn
deﬁned by
H(v) = v −2(v · u)u = v −2(uT v)u
(7.11)
is called the reﬂection of Rn through W.
Since reﬂecting v ∈Rn twice through a hyperplane W returns v to itself, a
reﬂection H has the property that H ◦H = IRn. This can be checked directly
by matrix multiplication. Let P denote the matrix of the projection Pu. Then
the matrix Q of H is Q = In −2P. Thus,
Q2 = (In2 −2P)(In −2P) = In −4P + 4P 2 = In,
since P 2 = P.
Since Q is symmetric and Q2 = In, Q is by deﬁnition orthogonal. Thus we
get the following.
Proposition 7.10. The matrix of a reﬂection is orthogonal. Hence, reﬂec-
tions are orthogonal linear mappings.
7.3.5
Projections on a general subspace
As a ﬁnal example of a linear mapping, let us work out the projection of ﬁnite-
dimensional inner product space V onto an arbitrary subspace W. As we saw
in Proposition 6.38, every x ∈V admits a unique orthogonal decomposition
x = w + y,
where w ∈W and y ∈W ⊥. The projection of V onto W is the mapping
PW : V →V deﬁned by PW (x) = w. The following proposition justiﬁes
calling PW a projection.

216
7
Linear Mappings
Proposition 7.11. The mapping PW : V →V has the following properties:
(i) PW is linear,
(ii) PW (w) = w if w ∈W,
(iii) PW (W ⊥) = {0}, and ﬁnally,
(iv) PW + PW ⊥= I.
Proof. Since W is a ﬁnite-dimensional inner product space, we have shown
that it has an orthonormal basis, say u1, . . . , uk. Assume that w ∈W. By
Proposition 6.36,
w =
k

i=1
(w, ui)ui
=
k

i=1
(x −y, ui)ui
=
k

i=1
(x, ui)ui −
k

i=1
(y, ui)ui
=
k

i=1
(x, ui)ui.
The last identity holds, since y ∈W ⊥and all the ui are in in W. Hence,
PW (x) =
k

i=1
(x, ui)ui.
This shows that PW is linear and PW (w) = w if w ∈W. Thus, (i) and (ii)
hold. Statement (iii) follows from the fact that (y, ui) = 0 for all y ∈W ⊥,
and (iv) is a consequence of the decomposition x = w+y, since PW ⊥(x) = y.
⊓⊔
When V = Rn, Exercise 7.3.9 below gives an interesting alternative expres-
sion for PW that doesn’t require knowing an orthonormal basis of W.
7.3.6
Dimension two and the O(2, R)-dichotomy
The orthogonal group O(2, R) of all isometries of R2 contains the group
Rot(2) of all rotations of R2 as a normal subgroup. Recall that every rotation
matrix has the form Rθ =
	
cos θ
−sin θ
sin θ
cos θ

. We are now going to show that
O(2, R) has surprising decomposition.

7.3
Isometries and Orthogonal Mappings
217
Theorem 7.12 (The O(2, R)-dichotomy). Every 2 × 2 orthogonal matrix is
either a rotation matrix
	a
−b
b
a

or a reﬂection matrix
	a
b
b
−a

. Here
a2 + b2 = 1, so a = cos θ and b = sin θ for some θ. Consequently, every
isometry of R2 is either a rotation or a reﬂection.
Proof. We will ﬁrst use the coset decomposition of O(2, R). Note that by the
product formula, det : O(2, R) →R∗is a homomorphism. We have shown
that det(Q) = ±1 for all Q ∈O(2, R), since QT Q = I2. But by Exercise
2.2.5, the value of a homomorphism on a coset of its kernel is constant and
takes diﬀerent values on diﬀerent cosets. Let Q ∈O(2, R). Since the columns
of Q are orthogonal unit vectors, we have
Q =
	
a
c
b
d

,
where a2 + b2 = 1, c2 + d2 = 1 and ac + bd = 0. After some simpliﬁcation,
it follows that there are exactly two possibilities for Q:
Q1 =
	
a
−b
b
a

and Q2 =
	
a
b
b
−a

,
where a2 + b2 = 1.
Now, det(Q1) = a2 + b2 = 1, while det(Q2) = −a2 −b2 = −1. Thus Q1
constitutes the kernel of det, so it follows that the kernel is Rot(2). Now
every element of the form Q2 can be written
Q2 =
	
1
0
0
−1

 	
a
−b
b
a

=
	
a
c
c
−a

,
where c = −b. Thus we have to show that Q2 is a reﬂection. We have
	
a
c
c
−a

 	
−c
a + 1

= −
	
−c
a + 1

and
	
a
c
c
−a

 	
a + 1
c

=
	
a + 1
c

.
Put v =
	
−c
a + 1

and w =
	
a + 1
c

, and note that v and w are orthogonal.
Hence if v (and equivalently w) is nonzero, there exists an orthonormal basis
u1 = v/|v| and u2 = w/|w| of R2 such that Q2u1 = −u1 and Q2u2 = u2. If
v = w = 0, then a = −1 and c = 0. But in this case,
Q2 =
	−1
0
0
1

.

218
7
Linear Mappings
In either case, Q2 is the matrix of a reﬂection. This shows that every element
of O(2, R) is either a rotation or a reﬂection.
⊓⊔
The O(2, R)-dichotomy rests on the fact that the kernel of the determinant
homomorphism on O(2, R) has two cosets: the identity coset Rot(2) and its
complement, which is the coset consisting of reﬂection matrices. In general,
the determinant determines a surjective homomorphism of O(n, R) →{±1}
with kernel the group O(n, R) ∩SL(n, R). We will denote this group by
SO(n, R). In particular, Rot(2) = SO(2, R). (Also see Exercise 5.2.13.) Since
SO(n, R) is a normal subgroup of O(n, R), we may apply the ﬁrst isomor-
phism theorem (Theorem 2.20) to deduce that O(n, R)/SO(n, R) ∼= {±1}.
Thus, SO(n, R) also has exactly two left cosets, and therefore
O(n, R) = SO(n, R) ∪QSO(n, R),
(7.12)
where Q is any element of O(n, R) such that det(Q) = −1. However, it is not
as easy to describe this coset as in the case n = 2.
Since the product of a rotation matrix R and a reﬂection matrix Q is a
reﬂection matrix, one can ask how to determine the reﬂections RQ and QR.
Similarly, if Q1 and Q2 are reﬂections, how does one describe the rotation
Q1Q2? These questions are taken up in the exercises. There is another nice
fact.
Proposition 7.13. Every ﬁnite subgroup of SO(2, R) is cyclic.
Proof. In fact, SO(2, R) is isomorphic to the circle group S1 in C∗via the
isomorphism ϕ(Rθ) = eiθ (see Section 4.2.2). The fact that ϕ is an isomor-
phism is due to the formula Rθ
	
x
y

= eiθz, where z = x+iy. But we showed
that every ﬁnite subgroup of C∗is cyclic in Proposition 2.26, so the same
holds for SO(2, R).
⊓⊔
7.3.7
The dihedral group as a subgroup of O(2, R)
Recall that the dihedral group D(m) (m ≥1) was originally deﬁned by giving
generators a and b satisfying the relations am = b2 = 1 and ab = ba−1. In this
section we will show that D(m) can be realized geometrically as a subgroup
of O(2, R). Consider the rotation matrix a = R2π/m and the reﬂection matrix
b =
	
1
0
0
−1

. Then am = b2 = I2. Moreover, it can be checked directly
that aba = b, so ab = ba−1. Thus the subgroup D(m) of O(2, R) generated
by a and b is a copy of D(m).
Now suppose m > 1, and let {m} denote an m-sided regular polygon in R2
centered at the origin having a vertex at
	1
0

. For example, the vertices of

7.3
Isometries and Orthogonal Mappings
219
{m} can be placed at the mth roots of unity e2πi/m. Then {m} is symmetric
about the x-axis. If m = 1, we will assume {m} = {
	1
0

}. Let O({m}) ⊂
O(2, R) be the set of all orthogonal matrices that send {m} onto itself.
Proposition 7.14. Assume m ≥1. Then O({m}) = D(m).
Proof. We will leave it to the reader to verify that O({m}) is a subgroup
of O(2, R). By construction, it follows that {m} is sent onto itself by both
a and b. Thus D(m) ⊂O({m}). Since |D(m)| = 2m, it suﬃces to show
that |O({m})| = 2m also. Since the vertices of {m} are equidistant from
the origin and elements of O({m}) preserve lengths, it follows that O({m})
has to permute the vertices of {m}. But an element of O({m}) is a linear
mapping on R2, so it is determined by its values on two noncollinear vectors.
Thus, O({m}) has to be ﬁnite. Let O+ = O({m}) ∩SO(2, R). Being a ﬁnite
subgroup of SO(2, R), we know that O+ is cyclic. This implies that |O+| = m,
since because {m} has m vertices, no element of O+ can have order greater
than m. But a ∈O+, so O+ is the cyclic group generated by a. Since b ∈
O({m}), it follows that det : O({m}) →{±1} is surjective, so as above,
O({m}) = O+ ∪b O+.
Therefore, |O({m})| = 2m, as claimed.
⊓⊔
7.3.8
The ﬁnite subgroups of O(2, R)
We now classify the ﬁnite subgroups of O(2, R). It turns out that there are no
major surprises. Every ﬁnite subgroup of O(2, R) is either cyclic or dihedral.
Theorem 7.15. The only ﬁnite subgroups of O(2, R) are:
(i) the groups {I2, b}, where b is a reﬂection;
(ii) the cyclic groups Cm consisting of rotations Rθ, where θ = 2kπ/m with
0 ≤k ≤m −1; and
(iii) the dihedral groups D(m) of symmetries of {m}, where m > 1.
Proof. We have already shown that every ﬁnite subgroup of SO(2, R) is cyclic.
Suppose G is a subgroup of O(2, R) that is not contained in SO(2, R). Then
by the O(2, R)-dichotomy, G contains a reﬂection b. If G ̸= {1, b}, then G
also contains a rotation diﬀerent from I2. So let G+ = G ∩SO(2, R). Then
det : G →{±1} is surjective, so as above, G = G+ ∪bG+. Thus, G = D(m),
where m = |G+|.
⊓⊔

220
7
Linear Mappings
This theorem (or rather its content) is attributed to Leonardo da Vinci.
He was the ﬁrst to explicitly list the symmetries of a regular polyhedron
{m}. Leonardo’s interest in symmetries involved architecture, in particular
questions such as whether one can add structures at the corners of a building
without destroying the rotational or reﬂective symmetry. Many interesting
facts about symmetries can be found on the Internet as well as in a pair of
classic books: Symmetry, by H. Weyl, and Geometry, by H.S.M. Coxeter. In
the penultimate chapter, we will discuss the symmetry groups of the Platonic
solids and classify all the ﬁnite subgroups of SO(3, R). The classiﬁcation is a
considerably more diﬃcult proof.
Exercises
Exercise 7.3.1. Suppose a ∈Rn is nonzero.
(i) Show that the projection
Pa(x) =
a · x
a · a

a
onto the line Ra is linear.
(ii) Using the formula for Pa, verify that Pa ﬁxes every vector on Ra and
sends every vector orthogonal to a to 0.
(iii) Verify that Pa ◦Pa = Pa.
Exercise 7.3.2. Let u and v be an orthonormal basis of R2. Show directly
that the following formulas hold for all x ∈R2:
(i) Pu(x) + Pv(x) = x, and
(ii) Pu(Pv(x)) = Pv(Pu(x)) = 0.
Exercise 7.3.3. Let V be an inner product space, and suppose S : V →V
preserves distances. Show that S is orthogonal, and conclude that S : V →V
preserves distances if and only if S is an isometry.
Exercise 7.3.4. Find the matrix of each of the following linear mappings:
(i) the rotation R−π/4 of R2 through −π/4,
(ii) the reﬂection H of R2 through the line x = y,
(iii) the matrices of H ◦R−π/4 and R−π/4 ◦H, where H is the reﬂection of
part (ii),
(iv) the matrix of the rotation H ◦R−π/4 ◦H of R2.

7.3
Isometries and Orthogonal Mappings
221
Exercise 7.3.5. Let H be the reﬂection of R2 through the line ℓthrough
the origin, and let H′ be the reﬂection through ℓ⊥. Describe the following
linear mappings:
(i) HRθ and H′RθH,
(ii) HRθH, and
(iii) RθHR−θ.
Exercise 7.3.6. Let V = C and consider the mapping T : V →V deﬁned
by T(z) = z. Describe T as a linear mapping with domain and target R2,
and ﬁnd its matrix. Also, give a geometric interpretation of T.
Exercise 7.3.7. Show directly that a reﬂection H of Rn is orthogonal by
checking that for all x and y in Rn,
H(x) · H(y) = x · y.
Exercise 7.3.8. Find the reﬂection of R3 through the plane P if:
(i) P is the plane x + y + z = 0;
(ii) P is the plane ax + by + cz = 0.
Exercise 7.3.9. This exercise gives a formula for the projection on a sub-
space W of Rm that does not require having an orthonormal basis of W. Let
A ∈Rm×n be a matrix whose columns are a basis of W.
(i) Let x ∈Rn. Under what condition is Ax the projection of x on W?
(ii) Prove that AT A is invertible. (Hint: consider xT AT Ax.)
(iii) Prove that A(AT A)−1AT x ∈W if x ∈Rm, and A(AT A)−1AT x = x if
x ∈W.
(iv) Next show that for every v ∈Rm,
v −A(AT A)−1AT v
is orthogonal to W. Hint: show that AT (v −A(AT A)−1AT v) = 0.
(v) Conclude that A(AT A)−1AT x is the projection of x ∈Rm onto W. Thus,
PW has matrix A(AT A)−1AT .
Exercise 7.3.10. Let Q =
	
a
b
b
−a

, where a2 + b2 = 1. Show that
Q is a reﬂection by demonstrating that Q can be written in the form
	
1 −2u2
1
−2u1u2
−2u1u2
1 −2u2
2

, where u2
1 + u2
2 = 1, and verifying that Q is the reﬂec-
tion through the line R
	−u2
u1

.

222
7
Linear Mappings
7.4
Coordinates with Respect to a Basis
and Matrices of Linear Mappings
We now come to a technical question. Suppose V and W are ﬁnite-dimensional
vector spaces over the same ﬁeld F of dimensions n and m respectively, and
let T : V →W be a linear mapping. How can we represent T? For example,
if V and W are Fn and Fm respectively, then T is a matrix linear map-
ping, hence is determined by a unique element A ∈Fm×n. The assignment
T →A = MT ∈Fm×n depends on the fact that there are natural coordinates
on Fn and Fm. Recall that for a general vector space, coordinates depend on
choosing a basis. So, in order to represent a linear mapping T : V →W as
a matrix, we must ﬁrst choose bases for both V and W. The ﬁrst job in this
section is to show how to deﬁne the matrix of an arbitrary linear mapping
T with respect to a choice of bases for V and W, and the second job is to
investigate how a diﬀerent choice of these bases aﬀects the matrix of T. The
result is called the change of basis formula.
7.4.1
Coordinates with respect to a basis
As usual, let V be a ﬁnite-dimensional vector space over F, and let B =
{v1, v2, . . . , vn} be a basis of V . Recall that every v ∈V has a unique
expression
v = r1v1 + r2v2 + · · · + rnvn.
The scalars r1, . . . , rn are called the coordinates of v with respect to B. We
will write v = (r1, r2, . . . , rn)B. Notice that the notion of coordinates assumes
that the basis B is ordered. The term coordinate deserves some explanation.
A coordinate is actually a function on V with target F. A basis B as above
determines n coordinate functions x1, . . . , xn, which are deﬁned by putting
xi(v) = ri when v has the (above) expansion in the basis B. The coordinates
of v are the values of the coordinate functions on v. Note that the coordinate
functions are linear: xi(av + bw) = axi(v) + bxi(w).
Finding the coordinates of a vector in Fn with respect to a basis is a
familiar problem in matrix inversion. Here is a preliminary example.
Example 7.8. Let us choose two diﬀerent bases of R2, say
B = {
	1
2

,
	0
1

}
and
B′ = {
	1
1

,
	 1
−1

}.
Expanding e1 in terms of these two bases gives two diﬀerent sets of coordi-
nates. By inspection,

7.4
Coordinates with Respect to a Basis and Matrices of Linear Mappings
223
e1 =
	
1
0

= 1
	
1
2

−2
	
0
1

and
e1 =
	1
0

= 1
2
	1
1

+ 1
2
	 1
−1

.
Thus, e1 = (1, −2)B, while e1 = ( 1
2, 1
2)B′.
⊠
7.4.2
The change of basis matrix
The ﬁrst question is how the coordinates of v with respect to B and B′ are
related. This is answered by setting up a linear system as follows: expanding
the basis B′ in terms of the basis B gives
	1
1

= a
	1
2

+ b
	0
1

and
	 1
−1

= c
	1
2

+ d
	0
1

.
Expressed in matrix form, these equations become
	1
1
1
−1

=
	1
0
2
1

 	a
c
b
d

.
Thus,
	a
c
b
d

=
	1
0
2
1

−1 	1
1
1
−1

=
	 1
1
−1
−3

.
Now suppose v has coordinates (r, s)B and (x, y)B′ with respect to B and B′.
Then
v =
	1
0
2
1

 	r
s

=
	1
1
1
−1

 	x
y

.
Therefore,
	
r
s

=
	
1
1
−1
−3

 	
x
y

.
We can imitate this in the general case. Let
B = {v1, v2, . . . , vn}
and
B′ = {v′
1, v′
2, . . . , v′
n}

224
7
Linear Mappings
be two bases of V . Deﬁne the change of basis matrix MB
B′ ∈Fn×n to be the
matrix (aij) with entries determined by
v′
j =
n

i=1
aijvi.
For example, suppose n = 2. Then
v′
1 = a11v1 + a21v2,
v′
2 = a12v1 + a22v2.
In matrix form as above, this looks like
(v′
1 v′
2) = (v1 v2)
	a11
a12
a21
a22

= (v1 v2)MB
B′,
where
MB
B′ =
	a11
a12
a21
a22

.
Notice that (v1 v2) is a generalized matrix in the sense that it is a 1 × 2
matrix with vector entries. A nice general property of this notation is that
whenever v1, . . . , vn is a basis of V and (v1 · · · vn)A = (v1 · · · vn)B, then
A = B.
Returning to the general case, let B and B′ be the two bases of V deﬁned
above. Then,
(v′
1 v′
2 · · · v′
n) = (v1 v2 · · · vn)MB
B′.
(7.13)
Example 7.9. For B and B′ as in Example 7.8, we have
MB
B′ =
	 1
1
−1
−3

.
⊠
Proposition 7.16. Let B and B′ be bases of V . Then
MB′
B = (MB
B′)−1.
Also
MB
B = In.
Proof. First of all, the identity MB
B = In is clear. For the rest of the proof,
let us assume n = 2 for simplicity. Now,

7.4
Coordinates with Respect to a Basis and Matrices of Linear Mappings
225
(v1 v2) = (v′
1 v′
2)MB′
B = (v1 v2)MB
B′MB′
B .
Thus,
MB
B′MB′
B = MB
B = I2.
Hence, (MB
B′)−1 = MB′
B .
⊓⊔
Now let’s see what happens when a third basis B′′ = {v′′
1, . . . , v′′
n} is thrown
in. Iterating the expression in (7.13) gives
(v′′
1 · · · v′′
n) = (v′
1 · · · v′
n)MB′
B′′ = (v1 · · · vn)MB
B′MB′
B′′.
This gives the following result.
Proposition 7.17. Let B, B′, and B′′ be bases of V . Then
MB
B′′ = MB
B′MB′
B′′.
7.4.3
The matrix of a linear mapping
Now suppose T : V →W is a linear mapping. The purpose of this section is
to associate a matrix to T with respect to a pair of chosen bases of V and
W. Let these bases be
B = {v1, v2, . . . , vn}
for V and
B′ = {w1, w2, . . . , wm}
for W. Then one can write
T(vj) =
m

i=1
cijwi.
(7.14)
Note: the ith component of T(vj) with respect to B′ is denoted by cij. This
is exactly analogous to how we deﬁned the change of basis matrix. Now we
can deﬁne the matrix MB
B′(T) of T with respect to the bases B and B′.
Deﬁnition 7.5. The matrix of T with respect to the bases B and B′ is
deﬁned to be the m × n matrix (cij). In other words, MB
B′(T) = (cij).
Let us put T(v1 v2 · · · vn) = (T(v1) T(v2) · · · T(vn)). Expressing (7.14) in
matrix form gives
T(v1 v2 · · · vn) = (T(v1) T(v2) · · · T(vn)) = (w1 w2 · · · wm)MB
B′(T).
(7.15)

226
7
Linear Mappings
This notation is set up so that if V = Fn, W = Fm, and T = TA, where
A ∈Fm×n, then MB
B′(T) = A when B and B′ are the standard bases, since
TA(ej) is the jth column of A. For (7.15) says that
A = TAIn = ImMB
B′(TA) = MB
B′(TA).
We remark that
MB
B′(IV ) = MB
B′,
where IV : V →V is the identity mapping.
7.4.4
The Case V = W
Now suppose V = W. In this case, we want to express the matrix of T in a
single basis and then ﬁnd its expression in another basis. So let B and B′ be
bases of V . As above, for simplicity, we assume n = 2 and put B = {v1, v2}
and B′ = {v′
1, v′
2}. Hence (v′
1 v′
2) = (v1 v2)MB
B′. Since T is linear, we may
write
(T(v′
1) T(v′
2)) = (T(v1) T(v2))MB
B′
= (v1 v2)MB
B(T)MB
B′
= (v′
1 v′
2)MB′
B MB
B(T)MB
B′.
Hence,
MB′
B′(T) = MB′
B MB
B(T)MB
B′.
We have therefore proved the following proposition.
Proposition 7.18. Let T : V →V be linear and let B and B′ be bases of
V . Then
MB′
B′(T) = MB′
B MB
B(T)MB
B′.
(7.16)
Thus, if P = MB′
B , we have
MB′
B′(T) = PMB
B(T)P −1.
(7.17)
Example 7.10. Consider the linear mapping T : R2 →R2 whose matrix
with respect to the standard basis is
A =
	
1
0
−4
3

.
Let’s ﬁnd the matrix B of T with respect to the basis (1, 1)T and (1, −1)T .
Calling this basis B′ and the standard basis B, we have

7.4
Coordinates with Respect to a Basis and Matrices of Linear Mappings
227
MB
B′ =
	
1
1
1
−1

.
To compute B, we have to use the matrix equation
	
1
0
−4
3

 	
1
1
1
−1

=
	
1
1
1
−1

B,
so
B =
	
1
1
1
−1

−1 	
1
0
−4
3

 	
1
1
1
−1

.
Computing the product gives
B =
	0
−3
1
4

.
⊠
Example 7.11. Consider the reﬂection H of R2 through a line ℓ(containing
0). Let v1 be a nonzero element of ℓand v2 a nonzero element of the line ℓ⊥.
Then H(v1) = v1 and H(v2) = −v2. Since H is an isometry, it is natural
to switch to an orthonormal basis. So let B = {u1, u2} be the orthonormal
basis of R2 where ui =
1
|vi|vi for i = 1, 2. Then
MB
B(H) =
	1
0
0
−1

.
Now suppose that u1 =
1
√
2(1, 1)T and u2 =
1
√
2(1, −1)T . Then the matrix
(u1 u2) is orthogonal. Let us use this to ﬁnd the matrix A of H with respect
to to the standard basis B′ = {e1, e2} of R2. Since
MB′
B′(H) = MB′
B MB
B(H)MB
B′,
and since B is an orthonormal basis, it follows that MB
B′ = (u1 u2)T . Thus,
by (7.16),
A = MB′
B MB
B(H)MB′
B =
1
2
	1
1
1
−1

 	1
0
0
−1

 	1
1
1
−1

=
	0
1
1
0

.
This checks, since by deﬁnition, H sends e1 to e2 and sends e2 to e1.
⊠

228
7
Linear Mappings
7.4.5
Similar matrices
The relationship between the matrices of a linear mapping in diﬀerent bases
suggests that we recall a term introduced in Exercise 5.2.6.
Deﬁnition 7.6. Let A and B be n×n matrices over F. Then we say that A
is similar to B if there exists an invertible P ∈Fn×n such that B = PAP −1.
It is not hard to see that similarity is an equivalence relation on Fn×n
(exercise: check this). An equivalence class for this equivalence relation is
called a conjugacy class of Fn×n. The meaning of a conjugacy class is given
in the next proposition.
Proposition 7.19. Let V be a ﬁnite-dimensional vector space over F such
that dim V = n. Then the matrices that represent a given linear mapping
T : V →V form a conjugacy class of Fn×n.
Recall that a linear mapping T : V →V is semisimple if there exists a basis
B = {v1, . . . , vn} of V for which T(vi) = μivi for some scalars μ1, . . . , μn in
F. Thus a linear mapping T is semisimple if and only if the conjugacy class
of its matrix with respect to some basis of V contains a diagonal matrix. The
semisimple linear mappings T : V →V are classiﬁed in the next chapter.
Conjugacy classes are also important in group theory. We will say more about
this in Chap. 11.
7.4.6
The matrix of a composition T ◦S
Suppose S, T : V →V are linear mappings. Recall that we saw in Proposi-
tion 3.6 that when V = Fn, then MT ◦S = MT MS. We will now prove that
this fact also holds in general.
Proposition 7.20. Assume that V is a ﬁnite-dimensional vector space with
basis B. Then
MB
B(T ◦S) = MB
B(T)MB
B(S).
Proof. We leave this to the reader.
7.4.7
The determinant of a linear mapping
The product identity det(AB) = det(A) det(B) and the fact that det(A−1) =
det(A)−1 imply that two similar matrices always have the same determinant,
since

7.4
Coordinates with Respect to a Basis and Matrices of Linear Mappings
229
det(PAP −1) = det(P) det(A) det(P −1) = det(A).
Using this, one can deﬁne the determinant of a linear mapping T : V →V ,
provided V is ﬁnite-dimensional. The deﬁnition goes as follows.
Deﬁnition 7.7. Let V be a ﬁnite-dimensional vector space and suppose
T : V →V is linear. Then the determinant det(T) of T is deﬁned to be
det(A), where A ∈Fn×n is any matrix representing T with respect to some
basis of V .
In order to show that det(T) is well deﬁned, we need to show that det(T)
is independent of the choice of basis of V . But by Proposition 7.18, if A and B
are matrices of T with respect to diﬀerent bases, then A and B are similar,
i.e., B = PAP −1 for some invertible P ∈Fn×n. Hence, det(B) = det(A).
Thus, det(T) is indeed well deﬁned.
Example 7.12. Suppose T : V →V is semisimple. Then there exists a basis
for which the matrix of T is a diagonal matrix D = diag (μ1 · · · μn). Thus
det(T) = μ1 . . . μn, since the determinant of a diagonal matrix is the product
of the diagonal entries.
⊠
Example 7.13. Let V be a ﬁnite-dimensional inner product space, and let
T : V →V be an isometry. Since the determinant of an orthogonal matrix is
±1 (since QT Q = In), it follows that det(T) = ±1 also.
⊠
Proposition 7.21. If S, T
: V
→V are linear mappings on a ﬁnite-
dimensional vector space V , then det(T ◦S) = det(T) det(S).
Proof. Apply Proposition 7.20 and the product formula.
⊓⊔
For example, we have the following.
Proposition 7.22. Suppose T : V
→V is a linear mapping such that
det(T) ̸= 0. Then there exists a linear mapping S : V
→V such that
T ◦S = S ◦T = IV . In particular, T is a bijection.
Proof. Choose a basis B of V and let A be the matrix of T with respect
to B. Since det(T) ̸= 0, det(A) ̸= 0 too, so A has an inverse A−1. Now let
S : V →V be the linear mapping whose matrix with respect to B is A−1.
Then by Proposition 7.20,
MB
B(T ◦S) = MB
B(T)MB
B(S) = AA−1 = In.
Therefore, T ◦S = IV . Similarly, S ◦T = IV too. The assertion that T is a
bijection follows immediately from Proposition 1.1 or Exercise 7.1.1.
⊓⊔

230
7
Linear Mappings
Exercises
Exercise 7.4.1. Find the coordinates of the standard basis e1, e2, e3 of R3
in terms of the basis (1, 1, 1)T , (1, 0, 1)T , (0, 1, 1)T , and ﬁnd the matrix of the
linear mapping T((x1, x2, x3)T ) = (4x1 + x2 −x3, x1 + 3x3, x2 + 2x3)T with
respect to this basis.
Exercise 7.4.2. Consider the basis (1, 1, 1)T , (1, 0, 1)T , and (0, 1, 1)T of R3.
Find the matrix of the linear mapping T : R3 →R3 deﬁned by T(x) =
(1, 1, 1)T × x with respect to this basis.
Exercise 7.4.3. Let H : R2 →R2 be the reﬂection through the line 2x = y.
Find a basis of R2 such that the matrix of H is diagonal.
Exercise 7.4.4. Show that every projection Pa : R2 →R2 is semisimple by
explicitly ﬁnding a basis for which the matrix of Pa is diagonal. Also, ﬁnd
this diagonal matrix.
Exercise 7.4.5. Let Rθ be the usual rotation of R2. Does there exist a basis
of R2 for which the matrix of Rθ is diagonal?
Exercise 7.4.6. Show that matrix similarity is an equivalence relation on
Fn×n.
Exercise 7.4.7. Find the matrix MB
B(Ca) of the cross product Ca = a × x
in the following cases:
(i) a = e1 and B is the standard basis;
(ii) a = e1 and B is the basis {e1, e2 + e3, e2 −e3}.
Exercise 7.4.8. Let V = (F2)2×2, and let T : V →V be deﬁned by T(B) =
AB −BA, where A =
	0
1
1
0

as an element of V .
(i) Show that T is a linear mapping.
(ii) Find the matrix of T with respect to a suitable basis (of your choice)
of V .
(iii) Find det(T).
Exercise 7.4.9. Let V be a ﬁnite-dimensional vector space, and suppose
T : V →V is a linear mapping. Find the relationship between between
N

MB
B(T)

and N

MB′
B′(T)

, where B and B′ are any two bases of V .
Exercise 7.4.10. Let T : V →V be a linear mapping, where V has ﬁnite
dimension. Show that the kernel of T is nontrivial if and only if det(T) = 0.

7.4
Coordinates with Respect to a Basis and Matrices of Linear Mappings
231
Exercise 7.4.11. Suppose the characteristic of F is diﬀerent from 2, and let
V = Fn×n. Let T : V →V be the linear mapping given by T(A) = AT . Find
a basis of V for which the matrix of T is diagonal. (Hint: recall that A is the
sum of a symmetric matrix and a skew-symmetric matrix.)
Exercise 7.4.12. Let V = F4 = {0, 1, α, β} be the four-element Galois ﬁeld
considered as a vector space over the prime ﬁeld F2 (see Section 2.6.2).
(i) Show that 1 and α form a basis of V .
(ii) Show that the Frobenius map F : V →V given by F(x) = x2 is a linear
mapping.
(iii) Find the matrix of F with respect to the basis of (i).
(iv) Is F semi-simple?

232
7
Linear Mappings
7.5
Further Results on Mappings
As usual, all vector spaces will be over the ﬁeld F. If V and W are vector
spaces, the space L(V, W) denotes the space of all linear mappings T : V →
W. If V = Fn and W = Fm, then L(V, W) = Fm×n. The purpose of this
section is to study L(V, W) for various choices of V and W.
7.5.1
The space L(V, W )
Mappings F : V →W can be added using pointwise addition and can be
multiplied by scalars in a similar way. That is, if F, G : V →W are two
mappings, their sum F + G is the mapping formed by setting
(F + G)(v) = F(v) + G(v).
Scalar multiplication is deﬁned by putting
(aF)(v) = aF(v)
for any scalar a. Hence, one can form linear combinations of mappings. It
isn’t hard to see that the set of all mappings with domain V and target W is
a vector space over F. Now let L(V, W) denote the set of all linear mappings
with domain V and target W. Then L(V, W) is a vector space over F under
the pointwise addition and scalar multiplication deﬁned above. The following
result gives the dimension of L(V, W) in the ﬁnite-dimensional case.
Proposition 7.23. Suppose V and W are ﬁnite-dimensional vector spaces,
say dim V = n and dim W = m. Then dim L(V, W) = mn.
Proof. Choose bases B of V and B′ of W. Then T has matrix MT = MB
B′(T),
and putting φ(T) = MT deﬁnes a linear mapping Φ : L(V, W) →Fm×n.
In fact, Φ is a bijection, and therefore Φ is an isomorphism. This implies
dim L(V, W) = dim Fm×n = mn, since an isomorphism preserves dimension.
⊓⊔
7.5.2
The dual space
The space V ∗= L(V, F) of linear maps (or linear functions) from V to F is
called the dual space of V . If V is a ﬁnite-dimensional vector space, then the
previous result says that dim V ∗= dim V. It turns out that even though V
and V ∗have the same dimension, there usually is no natural isomorphism

7.5
Further Results on Mappings
233
between them unless there is some additional structure. For example, if V is
a ﬁnite-dimensional inner product space, then there is a natural isomorphism
(see below). Given a basis of V , however, there is a natural basis of V ∗known
as the dual basis, which we now describe. If v1, . . . , vn is a basis of V , then
the dual basis v∗
1, . . . , v∗
n of V ∗is deﬁned by specifying how each v∗
i acts on
the basis v1, . . . , vn. Since a linear mapping is uniquely deﬁned by giving its
values on a basis, this suﬃces to deﬁne a unique element of V ∗. Thus put
v∗
i (vj) =
1
if i = j,
0
if i ̸= j.
(7.18)
More succinctly, v∗
i (vj) = δij. To justify the term dual basis, we prove the
following proposition.
Proposition 7.24. If v1, . . . , vn is a basis of V , then v∗
1, . . . , v∗
n is a basis
of V ∗.
Proof. Since V and V ∗have the same dimension, it suﬃces to show that the
dual basis vectors are independent. Suppose n
i=1 aiv∗
i = 0. By (7.18),

n

i=1
aiv∗
i

(vj) =
n

i=1
aiv∗
i (vj) = aj.
Hence each aj is equal to zero, so v∗
1, . . . , v∗
n are indeed independent.
⊓⊔
Example 7.14. In fact, the dual basis for V = Fn is already quite familiar.
Recall that the ith component function xi : Fn →F is deﬁned by
xi(a1, · · · , an) = ai
for i = 1, . . . , n. Then each xi is in V ∗. In fact, x1, . . . , xn is the basis of V ∗
dual to the standard basis e1, . . . , en.
⊠
Example 7.15. Here is an inﬁnite-dimensional example. Let V
= F[x].
Then the evaluation at an element r ∈F is the map er : V →F deﬁned
by er(f) = f(r). Then er ∈V ∗for every r ∈F. The kernel of er consists of
all f ∈F[x] such that f(r) = 0.
Let us now consider the case mentioned above in which V is a ﬁnite-
dimensional inner product space.
Proposition 7.25. Let V be a ﬁnite-dimensional inner product space over
R. If v ∈V , let ϕv : V →R be the element of V ∗deﬁned by
ϕv(x) = (v, x),
where ( , ) is the inner product on V . Then the mapping Φ : V →V ∗deﬁned
by Φ(v) = ϕv is an isomorphism.

234
7
Linear Mappings
Proof. It is clear, by the properties of an inner product, that ϕv ∈V ∗. Since
dim V = dim V ∗, we have only to show that Φ is injective. But if Φ(v) = 0,
then (v, x) = 0 for all x ∈V . In particular, (v, v) = 0, so v = 0 by the
deﬁnition of an inner product, and hence by Proposition 7.3, Φ is injective.
⊓⊔
In a similar vein, a Hermitian inner product on a ﬁnite-dimensional vector
space V over the complex numbers enables one to deﬁne an isomorphism from
V to V ∗, although the deﬁnition is slightly diﬀerent, since (αx, v) = α(x, v)
for a Hermitian inner product (see Example 6.35).
It turns out that there is always a natural isomorphism between V and
V ∗∗. This is an interesting exercise.
7.5.3
Multilinear maps
Let V and W be vector spaces over F. A mapping
T : V × V × · · · × V →W
(k factors)
is called k-multilinear if for all i = 1, . . . , k, the mapping Si : V →W deﬁned
by
Si(x) = T(v1, . . . , vi−1, x, vi+1, . . . , vk)
is a linear mapping for every v1, . . . , vi−1, vi+1, . . . , vk ∈V .
Example 7.16. Let V = F and deﬁne T(r1, r2, . . . , rk) = r1r2 · · · rk. Then
T is k-multilinear on V .
⊠
More interestingly, let V
= Fn, where elements of Fn are viewed as
columns. Deﬁne D : Fn×n = Fn × · · · × Fn →F by
D(v1, v2, . . . , vn) = det(v1v2 . . . vn).
Proposition 7.26. The mapping D is n-multilinear.
Proof. Fix a1, . . . , an ∈Fn, and put Sj(x) = D(a1, . . . , aj−1, x, aj+1, . . . , an).
We must show that Sj(x + y) = Sj(x) + Sj(y), and Sj(rx) = rSj(x) for all
x, y ∈Fn and r ∈F. Let aj = (a1ja2j . . . anj)T . By (5.9),
D(a1, . . . , an) =

σ∈S(n)
sgn(σ) a1σ(1)a2σ(2) · · · anσ(n).
Put x = (x1j, x2j, . . . , xnj)T and y = (y1j, y2j, . . . , ynj)T . For each σ ∈S(n),
there exists exactly one index i such that σ(i) = j. Thus, each term in the
expansion of Sj(x + y) has the form

7.5
Further Results on Mappings
235
sgn(σ) a1σ(1) · · · (xiσ(i) + yiσ(i)) · · · anσ(n).
This shows that Sj(x + y) = Sj(x) + Sj(y). Similarly, Sj(rx) = rSj(x).
Therefore, D is n-multilinear.
⊓⊔
Example 7.17. Let us try an example. Suppose A =
	1
3
2
1

. Then,
det(A) = det
	
1
0
2
1

+ det
	
1
3
2
0

= 1 −6 = −5.
⊠
7.5.4
A characterization of the determinant
The determinant function on Fn×n is multilinear, has the value 1 on In, and
det(A) = 0 if two columns of A are equal. In the next result, we will prove
that the only function D : Fn×n →F having these three properties is the
determinant.
Proposition 7.27. Suppose D : Fn×n →F is a function satisfying the fol-
lowing properties:
(i) D is n-multilinear with respect to columns,
(ii) D(A) = 0 if two columns of A coincide; and
(iii) D(In) = 1.
Then D(A) = det(A) for all A ∈Fn×n.
Proof. I claim that for every elementary matrix E, D(E) = det(E). First,
if E is the elementary matrix obtained by dilating the ith column of In by
r ∈F, then by (i) and (iii), D(E) = rD(In) = r = det(E). Suppose E is
the elementary matrix obtained by swapping the ith and jth columns of In,
where i < j. Form the matrix F ∈Fn×n whose ith and jth columns are both
ei + ej and whose kth column, for each k ̸= i, j, is ek. (Note: in the 2 × 2
case, F is the all ones matrix.) By (ii), D(F) = 0. Expanding D(F) using (i)
and applying (ii) twice, we get
D(e1, . . . , ej, . . . , ei, . . . , en) + D(e1, . . . , ei, . . . , ej, . . . , en) = 0.
The ﬁrst term is D(E), while the second term is D(In) = 1, so D(E) = −1 =
det(E). Finally, if E is the transvection that adds a multiple of the ith column
of In to its jth column, then D(E) = 1 = det(E) by (i), (ii), and (iii).

236
7
Linear Mappings
The next step is to show that for every elementary matrix E and A ∈
Fn×n, D(AE) = D(E)D(A). (Recall that column operations are done via
right multiplication.) This follows from the previous proposition if E is a
row dilation. If E is a column swap, an argument similar to showing that
D(E) = −1 shows that D(AE) + D(A) = 0. Therefore, D(AE) = −D(A) =
D(E)D(A). Likewise, D(AE) = D(A) = D(E)D(A) when E is a transvection.
This completes the second step.
Now suppose A ∈Fn×n has rank n. Then there exist elementary matrices
E1, . . . , Ek such that A = E1 · · · Ek. Applying D(AE) = D(E)D(A) and
using the product formula (Theorem 5.1) for the determinant, one gets
D(A) = D(E1) · · · D(Ek) = det(E1) · · · det(Ek) = det(A).
On the other hand, if A has rank less than n, then the column reduced form
of A has a column of zeros, so for suitable elementary matrices E1, . . . , Ek,
AE1 · · · Ek has a column of zeros. Thus, D(A) = 0 = det(A). This completes
the proof that D(A) = det(A) for all A ∈Fn×n.
⊓⊔
Notice that what the proposition shows is that two functions D1 and D2
on Fn×n satisfying (i)–(iii) have to coincide. But it cannot actually be used
as a deﬁnition of the determinant.
Exercises
Exercise 7.5.1. Let V be a ﬁnite-dimensional inner product space. Show
how to deﬁne an inner product on V ∗in two ways:
(i) using a basis and the dual basis;
(ii) without appealing to a basis.
Exercise* 7.5.1. Let V be any ﬁnite-dimensional vector space. Deﬁne the
double dual V ∗∗of V to be (V ∗)∗. That is, V ∗∗is the dual of the dual space
of V . Show that the map Δ : V →V ∗∗deﬁned by the condition
Δ(v)(ϕ) = ϕ(v)
(7.19)
for all v ∈V and ϕ ∈V ∗is an isomorphism. Thus Δ is a natural isomorphism
from V onto V ∗∗.
Exercise* 7.5.2. Let V and W be a pair of ﬁnite-dimensional vector spaces
over F and let T : V →W be linear. Deﬁne the adjoint map T ∗: W ∗→V ∗
by
T ∗(ω)(v) = ω(T(v))
for all ω ∈W ∗and v ∈V .

7.5
Further Results on Mappings
237
(i) Show that T ∗is a well-deﬁned linear map.
(ii) Suppose v1, . . . vn is a basis of V , and w1, . . . , wm is a basis of W, and
let the matrix of T with respect to these bases be A. Find the matrix of T ∗
with respect to the dual bases of W ∗and V ∗.
(iii) Show that if T is injective, then T ∗is surjective. Also show the reverse:
if T is surjective, then T ∗is injective.
(iv) Show that dim im(T) = dim im(T ∗).
(v) Show that if V is a subspace of W, then there exists a natural surjective
linear map S : W ∗→V ∗.

Chapter 8
Eigentheory
Suppose V is a ﬁnite-dimensional vector space over F and T : V →V is a
linear mapping. An eigenpair for T consists of a pair (λ, v), where λ ∈F
and v ∈V is a nonzero vector such that T(v) = λv. The scalar λ is called
an eigenvalue of T, and v is called an eigenvector of T corresponding to λ.
We will say that the linear mapping T is semisimple if there exist eigen-
pairs (λ1, v1), . . . , (λn, vn) for T such that v1, . . . , vn form a basis of V . A
basis consisting of eigenvectors is known as an eigenbasis, and the problem of
ﬁnding an eigenbasis (or whether one exists) is an important step in under-
standing the structure of a linear mapping. The aim of this chapter is to
develop the theory of eigenpairs and eigenbases and to eventually obtain a
characterization the semisimple linear mappings. We will also introduce sev-
eral geometric notions, such as the deﬁnition of a dynamical system, and we
will give a number of group-theoretic applications of eigentheory. One of the
nicest applications is the proof of another classical theorem of Euler, which
in modern terms says that SO(3, R) consists of all rotations of R3 about
the origin. We will also discuss the symmetries of the Platonic solids, and
ﬁnally, we will prove the celebrated Cayley–Hamilton theorem, which will be
applied later in the proofs of Jordan canonical form and the Jordan–Chevalley
decomposition theorem.
8.1
The Eigenvalue Problem and the Characteristic
Polynomial
The purpose of this section is to introduce the basic terms and concepts con-
nected with eigentheory: eigenvalues, eigenvectors, eigenpairs, eigenspaces,
the characteristic polynomial, and the characteristic equation. As above, V
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 8
239

240
8
Eigentheory
is a ﬁnite-dimensional vector space over F, and T : V →V is a linear map-
ping. When V = Fn, then T = TA, where A ∈Fn×n is the matrix of T with
respect to the standard basis.
8.1.1
First considerations: the eigenvalue problem
for matrices
The problem of ﬁnding eigenpairs for a linear mapping T will be attacked
via matrix theory by solving it for the matrix A of T with respect to a
basis of V and then showing how an eigenpair for A determines an eigenpair
for T. Suppose dim V = n, so that A ∈Fn×n. On the level of matrix theory,
the eigenvalue problem is to ﬁnd the values of λ ∈F such that the linear
system Ax = λx has a nontrivial solution. The variables are λ and x, so this
is a nonlinear problem because of the λx term. The way to circumvent this
diﬃculty is by breaking the problem down into two separate problems. The
ﬁrst is to determine all λ ∈F such that N(A −λIn) ̸= {0}. Since the null
space of a square matrix is nonzero if and only if its determinant is zero, the
problem is to determine all λ ∈F such that
det(A −λIn) = 0.
(8.1)
One calls (8.1) the characteristic equation of A. The eigenvalues of A are the
solutions λ in F of the characteristic equation. The second problem, which
is straightforward, is to ﬁnd the null space N(A −λIn) corresponding to an
eigenvalue. The null space N(A −λIn) is called the eigenspace of A corre-
sponding to λ and denoted by Eλ(A). For every nonzero v ∈N(A −λIn),
(λ, v) is an eigenpair.
Remark. The properties of the determinant imply that det(A −λIn) is a
polynomial in λ of degree n. Hence ﬁnding eigenvalues of A requires ﬁnding
the roots of a polynomial. If n > 2, there is no easy way to do this, but there
are a few remarks we can and will make later. The roots of polynomials of
degree at most four can be found with great diﬃculty by radicals, but there
is no general formula for roots of a polynomial of degree ﬁve or more. This is
a famous result in Galois theory involving the Galois group of the equation.
There are analytic techniques for approximating roots. On the other hand,
the fundamental theorem of algebra guarantees that every A ∈Cn×n has n
complex eigenvalues. This does not mean, however, that square matrices over
C always admit an eigenbasis, as we will see.
Let us now consider an example.

8.1
The Eigenvalue Problem and the Characteristic Polynomial
241
Example 8.1. Consider the real matrix
A =
1
2
2
1

.
The real eigenvalues of A are the real numbers λ such that
A −λI2 =
1 −λ
2
2
1 −λ

has rank 0 or 1. The characteristic equation of A is
det(A −λI2) = (1 −λ)2 −2 · 2 = λ2 −2λ −3 = (λ −3)(λ + 1) = 0,
so λ = 3, −1 are the eigenvalues. Therefore, we seek N(A −3I2) and N(A +
I2). Clearly,
N(A −3I2) = N(

−2
2
2
−2

) = R

1
1

,
while
N(A + I2) = N(

2
2
2
2

) = R

1
−1

.
Thus, (3,

1
1

) and (−1,

1
−1

) are eigenpairs for A. Note that they deter-
mine an eigenbasis.
⊠
8.1.2
The characteristic polynomial
Let A ∈Fn×n. Instead of dealing with the characteristic equation of A, it is
more useful to consider det(A −λIn) as a function of λ. Let x be a variable
and put pA(x) = det(A −xIn). Then pA(x) ∈F[x]. In calculating pA(x), one
must expand a determinant containing a variable, so it turns out that row
operations are not very helpful. This will turn out not to be a problem,
because we will soon give a beautiful closed formula for det(A −xIn). Let us
now note some basic facts about pA(x).
Proposition 8.1. If A ∈Fn×n, then pA(x) = det(A −xIn) is a polynomial
in x over F. Its leading term is (−1)nxn, so the degree of pA(x) is n, and its
constant term is det(A). The eigenvalues of A are the roots of pA(x) = 0 in
F. In particular, an n × n matrix cannot have more than n eigenvalues.
Proof. That pA(x) is a polynomial is a consequence of the deﬁnition of the
determinant. The constant term is pA(0) = det(A). The leading term comes

242
8
Eigentheory
from the term
(a11 −x) · · · (ann −x)
in the Leibniz expansion, which is clearly (−1)nxn. The last claim follows
from the fact that a polynomial over F of degree n cannot have more than n
roots in F.
□
One calls pA(x) the characteristic polynomial of A. The characteristic poly-
nomial needn’t have any roots in F. However, one of the basic results in the
theory of ﬁelds is that given f(x) ∈F[x], there exists a ﬁeld F′ containing
F such that f(x) factors into linear terms in F′[x]. That is, F′ contains all
roots of f(x) = 0. We will give a proof of this result in the appendix (see
Section 8.7) at the end of this chapter. Although the fundamental theorem of
algebra (Theorem 2.27) guarantees that the characteristic polynomial of an
n × n matrix over R has n complex roots, none of these roots need be real,
as the next example points out.
Example 8.2. The characteristic polynomial of the matrix
J =
0
−1
1
0

is x2 + 1 = 0. Hence J is a real matrix that has no real eigenvalues. In fact,
there cannot be real eigenvalues, since J is the rotation of R2 through π/2:
no nonzero vector is rotated into a multiple of itself. On the other hand, if J
is treated as a 2 × 2 complex matrix with eigenvalues ±i, solving for corre-
sponding eigenvectors gives eigenpairs (i, (−1, i)T ) and (−i, (1, i)T ). Thus J
has two C-eigenvalues and two independent eigenvectors in C2. In particular,
J has an eigenbasis for C2. Thus,
0
−1
1
0
  1
1
−i
i

=
 1
1
−i
i
 i
0
0 −i

,
so J = MDM −1, where M =
 1
1
−i
i

.
⊠
The following example illustrates another possibility.
Example 8.3. Let K =
0
−i
i
0

. The characteristic polynomial of K is
x2 −1, so the eigenvalues of K are ±1. Thus K is a complex matrix with
real eigenvalues. Notice that K = iJ, which explains why its eigenvalues are
i times those of J.
⊠
The next Proposition gives an important property of the characteristic
polynomial.

8.1
The Eigenvalue Problem and the Characteristic Polynomial
243
Proposition 8.2. Similar matrices have the same characteristic polynomial.
Proof. Suppose A and B are similar, say B = MAM −1. Then
det(B −xIn) = det(MAM −1 −xIn)
= det(M(A −xIn)M −1)
= det(M) det(A −xIn) det(M −1).
Since det(M −1) = det(M)−1, the proof is done.
□
On the other hand, two matrices with the same characteristic polynomial
are not necessarily similar. Because of this proposition, we can extend the
deﬁnition of the characteristic polynomial of a matrix to the characteristic
polynomial of an arbitrary linear mapping T : V →V , provided V is ﬁnite-
dimensional.
Deﬁnition 8.1. If V is a ﬁnite-dimensional vector space and T : V →V is
linear, then we deﬁne the characteristic polynomial of T to be the polynomial
pT (x) deﬁned as pA(x) for each matrix A of T.
The eigenvalues of T are roots of its characteristic polynomial. The con-
nection between the eigentheory for linear mappings and matrices will be
made explicit in Proposition 8.4.
8.1.3
The characteristic polynomial of a 2 × 2
matrix
If A is of size 2 × 2, say
A =

a
b
c
d

,
then the characteristic polynomial pA(x) of A is easy to ﬁnd:
pA(x) = (a −x)(d −x) −bc = x2 −(a + d)x + (ad −bc).
As already noted, the constant term is det(A). The coeﬃcient a + d of x, that
is, the sum of the diagonal entries, is the trace of A, that is, Tr(A). Hence,
pA(x) = x2 −Tr(A)x + det(A).
(8.2)
The quadratic formula gives an elegant formula for the eigenvalues λ:
λ = 1
2

Tr(A) ±

Tr(A)2 −4 det(A)

.
(8.3)

244
8
Eigentheory
Hence if A ∈R2×2, it has real eigenvalues if and only if
Δ(A) := Tr(A)2 −4 det(A) = (a −d)2 + 4bc ≥0.
In particular, if bc ≥0, then A has real eigenvalues. If Δ(A) > 0, the roots are
real and unequal, and if Δ(A) = 0, they are real and identical. If Δ(A) < 0,
the roots are complex and unequal. In this case, the roots are conjugate
complex numbers, since pA(x) has real coeﬃcients.
Example 8.4. For example, if A ∈R2×2 is symmetric, then since b = c, A
has two real eigenvalues. If A is skew-symmetric (that is, AT = −A) and
A ̸= O, then pA(x) has two unequal complex roots.
⊠
If pA(x) = 0 has roots λ1, λ2, then pA(x) factors as
pA(x) = (x −λ1)(x −λ2) = x2 −(λ1 + λ2)x + λ1λ2,
so a comparison of the coeﬃcients gives the following:
(i) the trace of A is the sum of the eigenvalues of A:
Tr(A) = a + d = λ1 + λ2,
and
(ii) the determinant of A is the product of the eigenvalues of A:
det(A) = ad −bc = λ1λ2.
Thus the characteristic polynomial of a 2 × 2 matrix can be calculated with-
out pencil and paper. Our next task is to give a general formula extending
the 2 × 2 case.
8.1.4
A general formula for the characteristic
polynomial
As mentioned above, row operations are essentially of no use if one wants
to ﬁnd a characteristic polynomial by hand. The Laplace expansion is, in
general, the only tool that obviates the need to resort to Leibniz’s deﬁnition.
It turns out, however, that there is a beautiful formula for the character-
istic polynomial that reduces the computation to computing the principal
minors of A.
Let A ∈Fn×n. Since pA(x) is a polynomial in x of degree n with leading
coeﬃcient (−1)nxn and constant term det(A), one can write
pA(x) = (−1)nxn + (−1)n−1σ1(A)xn−1 + (−1)n−2σ2(A)xn−2+

8.1
The Eigenvalue Problem and the Characteristic Polynomial
245
+ · · · + (−1)σn−1(A)x + det(A),
(8.4)
where the σi(A), 1 ≤i ≤n −1, are scalars given by the next result.
Theorem 8.3. The coeﬃcients σi(A) for 1 ≤i ≤n are given by
σi(A) :=
 
all principal i × i minors of A

,
(8.5)
where the principal i × i minors of A are deﬁned to be the determinants of
the i × i submatrices of A obtained by deleting n −i rows of A and then the
same n −i columns.
We will omit the proof, since it would require us to take a lengthy detour
through exterior algebra. Note that by deﬁnition, the principal 1 × 1 minors
are just the diagonal entries of A. Hence
σ1(A) = a11 + a22 + · · · + ann,
so
σ1(A) = Tr(A).
Of course, σn(A) = det(A). In general, the number of j × j minors of A is
the binomial coeﬃcient

n
n−j

=
n!
j!(n−j)!. Thus, the characteristic polynomial
of a 4 × 4 matrix will involve four 1 × 1 principal minors, six 2 × 2 princi-
pal minors, four 3 × 3 principal minors, and a single 4 × 4 principal minor,
the determinant. In all, there are 2n terms involved, since by the Binomial
theorem, (1 + 1)n = n
j=0

n
j

.
Now suppose λ1, . . . , λn are the roots of pA(x) = 0. Then
pA(x) = (−1)n
(x −λ1)(x −λ2) · · · (x −λn)

= (−1)nxn + (−1)n−1(λ1 + λ2 + · · · + λn)xn−1 + · · · + λ1λ2 · · · λn.
This generalizes the result of the 2 × 2 case. For example,
σ1(A) = σ1(λ1, . . . , λn) = λ1 + λ2 + · · · + λn,
while
σn(A) = σn(λ1, . . . , λn) = λ1λ2 · · · λn.
Thus the trace of a matrix A is the sum of the roots of its characteristic poly-
nomial, and its determinant is the product of its roots. The other functions
σi(λ1, . . . , λn) = σi(A) can be expressed in a similar manner. For example,

246
8
Eigentheory
σ2(A) = σ2(λ1, . . . , λn) =

i<j
λiλj.
The functions σi(λ1, . . . , λn) are called the elementary symmetric functions
(symmetric because they remain unchanged after an arbitrary permutation
of λ1, . . . , λn). It turns out that all the coeﬃcients σi(A) of pA(x) can be
expressed in terms of the traces of powers of A. (This is a fact about sym-
metric functions due to Newton.) Consequently, there exist formulas for the
characteristic polynomial that avoid determinants altogether. For example,
if A is of size 3 × 3, then
σ2(A) = 1
2(Tr(A)2 −Tr(A2)),
while
det(A) = Tr(A)3 + 2Tr(A3) −3Tr(A)Tr(A2).
Consequently,
pA(x) = −x3 + Tr(A)x2 −1
2(Tr(A)2 −Tr(A2))x
+Tr(A)3 + 2Tr(A3) −3Tr(A)Tr(A2).
One can ﬁnd σi(A) for all i from the determinantal formula
σi(A) = 1
i! det
⎛
⎜
⎜
⎜
⎜
⎜
⎝
Tr(A)
i −1
0
· · ·
Tr(A2)
Tr(A)
i −2
· · ·
...
...
...
Tr(Ai−1)
Tr(Ai−2)
· · ·
1
Tr(Ai)
Tr(Ai−1)
· · ·
Tr(A)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Exercises
Exercise 8.1.1. Prove the following: Suppose A is a square matrix over F
and (λ, v) is an eigenpair for A. Then for every scalar r ∈F, (rλ, v) is an
eigenpair for rA. Moreover, for every positive integer k, (λk, v) is an eigenpair
for Ak. Finally, A has an eigenpair of the form (0, v) if and only if N(A) is
nontrivial.
Exercise 8.1.2. This exercise describes the rational root test, which is used
for ﬁnding rational roots of polynomials with integer coeﬃcients. The rational
root test says that the only rational roots of a polynomial
p(x) = anxn + an−1xn−1 + · · · + a1x + a0

8.1
The Eigenvalue Problem and the Characteristic Polynomial
247
with integer coeﬃcients can be expressed as p/q, where (p, q) = 1, q divides
an, and p divides a0.
(i) Give a proof of the rational root test.
(ii) Conclude that if A is a square matrix with integer entries, then the only
possible rational eigenvalues are the integers that divide det(A).
(iii) Using the rational root test, ﬁnd all integral eigenvalues of the matrix
A =
⎛
⎝
3
−2
−2
3
−1
−3
1
−2
0
⎞
⎠.
Exercise 8.1.3. Find the characteristic polynomial, eigenvalues, and if pos-
sible, a real eigenbasis for:
(i) the X-ﬁles matrix
X =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠,
(ii) the checkerboard matrix
C =
⎛
⎝
0
1
0
1
0
1
0
1
0
⎞
⎠,
(iii) the 4 × 4 X-ﬁles matrix
⎛
⎜
⎜
⎝
1
0
0
1
0
1
1
0
0
1
1
0
1
0
0
1
⎞
⎟
⎟
⎠,
(iv) the 4 × 4 checkerboard matrix
⎛
⎜
⎜
⎝
1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1
⎞
⎟
⎟
⎠.
Exercise 8.1.4. Find the characteristic polynomial and eigenvalues of
⎛
⎜
⎜
⎝
−3
0
−4
−4
0
2
1
1
4
0
5
4
−4
0
−4
−3
⎞
⎟
⎟
⎠

248
8
Eigentheory
in two ways, one using the Laplace expansion and the other using principal
minors.
Exercise 8.1.5. The following matrix A was on a blackboard in the movie
Good Will Hunting:
A =
⎛
⎜
⎜
⎝
0
1
0
1
1
0
2
1
0
2
0
0
1
1
0
0
⎞
⎟
⎟
⎠.
Find the characteristic polynomial of A and show that there are four real
eigenvalues.
Exercise 8.1.6. Using the determinantal formula for σi(A), ﬁnd a formula
for pA(x) when A is of size 4 × 4.
Exercise 8.1.7. Find the characteristic polynomial of a 4 × 4 matrix A if
you know that three eigenvalues of A are ±1 and 2 and that det(A) = 6.
Exercise 8.1.8. Suppose A ∈Fn×n has the property that A = A−1. Show
that if λ is an eigenvalue of A, then so is λ−1. Use this to ﬁnd the characteristic
polynomial of A−1 in terms of the characteristic polynomial of A.
Exercise 8.1.9. Show that two similar matrices have the same trace and
determinant.
Exercise 8.1.10. True or false: Two matrices with the same characteristic
polynomial are similar. If false, supply a 2 × 2 counter example.
Exercise 8.1.11. If A is a square matrix, determine whether A and AT have
the same characteristic polynomial, hence the same eigenvalues.
Exercise 8.1.12. Show that 0 is an eigenvalue of A if and only if A is
singular.
Exercise 8.1.13. True or false: If λ is an eigenvalue of A and μ is an eigen-
value of B, then λ + μ is an eigenvalue of A + B. If false, supply a 2 × 2
counter example.
Exercise 8.1.14. Suppose A and B are similar and v is an eigenvector of A.
Find an eigenvector of B.
Exercise 8.1.15. Let A be a real 3 × 3 matrix such that A and −A are
similar. Show that:
(i) det(A) = Tr(A) = 0,
(ii) 0 is an eigenvalue of A, and
(ii) if some eigenvalue of A is nonzero, then A has an eigenbasis for C3.

8.1
The Eigenvalue Problem and the Characteristic Polynomial
249
Exercise 8.1.16. Let A be a matrix whose characteristic polynomial has
the form −x3 + 7x2 −bx + 8. Suppose that the eigenvalues of A are integers.
(i) Find the eigenvalues of A.
(ii) Find the value of b.
Exercise 8.1.17. Let A ∈Fn×n. What is the characteristic polynomial of
A3 in terms of that of A?
Exercise 8.1.18. An n × n matrix such that Ak = O for some positive inte-
ger k is called nilpotent.
(i) Show all eigenvalues of a nilpotent matrix A are 0.
(ii) Conclude that the characteristic polynomial of A is (−1)nλn. In particu-
lar, the trace of a nilpotent matrix is 0.
(iii) Find a 3 × 3 matrix A such that A2 ̸= O but A3 = O. (Hint: look for an
upper triangular example.)
Exercise 8.1.19. Let the ﬁeld be F2. Find the characteristic polynomials
of the following X-matrices. Is either X-matrix similar to a diagonal matrix
over F2?
X1 =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠,
X2 =
⎛
⎜
⎜
⎜
⎜
⎝
1
0
0
0
1
0
1
0
1
0
0
0
1
0
0
0
1
0
1
0
1
0
0
0
1
⎞
⎟
⎟
⎟
⎟
⎠
.
Exercise 8.1.20. Show that the complex eigenvalues of a real n × n matrix
occur in conjugate pairs λ and λ. (Note: the proof of this that we gave for
n = 2 does not extend. First show that if p(x) is a polynomial with real
coeﬃcients, then p(z) = p(z) for every z ∈C.)
Exercise 8.1.21. Conclude from the previous exercise that a real n × n
matrix, where n is odd, has at least one real eigenvalue. In particular, every
3 × 3 real matrix has a real eigenvalue.
Exercise 8.1.22. Show that the only possible real eigenvalues of an n × n
real orthogonal matrix are ±1.
Exercise 8.1.23. Find eigenpairs for the two complex eigenvalues of the
rotation matrix Rθ if θ ̸= 0, π.
Exercise 8.1.24. Let F be a ﬁeld.
(i) Show that F is a one-dimensional vector space over itself.
(ii) Show that every linear mapping T : F →F is semisimple. That is, show
that there exists λ ∈F such that T(v) = λv for all v ∈F.

250
8
Eigentheory
Exercise 8.1.25. Let
J =

cos θ
−sin θ
sin θ
cos θ

.
Show that J determines a linear mapping T : C →C for every θ and ﬁnd the
unique complex eigenvalue of T.
Exercise 8.1.26. Suppose A, B ∈Fn×n and assume that A is invertible.
Show that B, AB, and BA all have the same characteristic polynomial.
Exercise 8.1.27. Find formulas for the elementary symmetric functions
σi(λ1, λ2, λ3, λ4)
(i = 1, 2, 3, 4) by expanding (x −λ1)(x −λ2)(x −λ3)(x −λ4). Deduce an
expression for all σi(A) for an arbitrary 4 × 4 matrix A.
Exercise 8.1.28. * Assume that a, b, c are real and consider the matrix
A =
⎛
⎝
a
b
c
b
c
a
c
a
b
⎞
⎠.
(i) Find the characteristic polynomial of A.
(ii) Show that Tr(A) is an eigenvalue of A.
(iii) Use (ii) to show that det(A) ≤0 and conclude that (a3 + b3 + c3) ≥
3abc.

8.2
Basic Results on Eigentheory
251
8.2
Basic Results on Eigentheory
We now return to the general problem of understanding the eigentheory of a
linear mapping T : V →V . We have already shown how to obtain eigenpairs
for a matrix, but it remains to show, ﬁrst, how an eigenpair for a matrix
determines an eigenpair for the linear mapping that the matrix represents,
and second, how an eigenbasis for a matrix determines an eigenbasis for this
linear mapping. We will also mention a suﬃcient condition for the existence of
an eigenbasis that will be generalized in the next section. Finally, we will give
a couple of nice applications of eigentheory. In particular, we will introduce
the notion of a dynamical system and use it to study the Fibonacci sequence.
8.2.1
Eigenpairs for linear mappings
Throughout this section, V will denote a ﬁnite-dimensional vector space over
a ﬁeld F, and T : V →V will be a linear mapping. Our ﬁrst objective is to
explain the correspondence between eigenpairs for matrices and eigenpairs
for linear mappings. Suppose dim V = n.
Claim. Assume that A = MB
B(T) is the matrix of T with respect to a
basis B = {v1, . . . , vn} of V . Let (μ, x) be an eigenpair for A, where x =
(x1, . . . , xn)T ∈Fn. If
v =
n

i=1
xivi = (v1 v2 · · · vn)x,
then (λ, v) is an eigenpair for T.
Proof. Recall that by (7.15), A is determined from
(T(v1) T(v2) · · · T(vn)) = (v1 v2 · · · vn)A.
Since Ax = μx,
T(v) = (T(v1) T(v2) · · · T(vn))x
= (v1 v2 · · · vn)Ax
= μ(v1 v2 · · · vn)x
= μv.
Thus (μ, v) is indeed an eigenpair for T. Summarizing, we have the following
result.

252
8
Eigentheory
Proposition 8.4. Let T : V →V
be linear, and let A = MB
B(T) be the
matrix of T with respect to a basis B of V . Then every eigenpair (μ, x) for A
gives an eigenpair (μ, v) for T, where v is the element of V whose coordinates
with respect to B are the components of x. Conversely, an eigenpair (μ, v)
for T gives a corresponding eigenpair (μ, x) for A in the same manner.
8.2.2
Diagonalizable matrices
Recall from Example 8.1 that the matrix
1
2
2
1

has eigenvalues 3 and
−1 and corresponding eigenspaces R

1
1

and R

1
−1

. When this data is
encoded in a matrix equation, we see that

1
2
2
1
 
1
1
1 −1

=

3
−1
3
1

=

1
1
1
−1
 
3
0
0
−1

.
This expression has the form AP = PD, where the columns of P are linearly
independent eigenvectors and the entries of D are the corresponding eigen-
values. Since P is invertible, we get the factorization A = PDP −1, where
D is diagonal. Thus, A is similar to a diagonal matrix over F. Let us now
introduce the following term.
Deﬁnition 8.2. A matrix A ∈Fn×n is said to be diagonalizable over F if A
can be written A = PDP −1, where D, P ∈Fn×n and D is diagonal. In other
words, A is diagonalizable over F if it is similar over F to a diagonal matrix
D ∈Fn×n.
We now describe what it means to say that a matrix is diagonalizable.
Assume A ∈Fn×n.
Proposition 8.5. Suppose w1, . . . , wn is an eigenbasis of Fn for A with
corresponding eigenvalues λ1, . . . , λn in F. Then A is diagonalizable over F.
In fact, A = PDP −1, where P = (w1 · · · wn) and D = diag(λ1, . . . , λn).
Conversely, if A = PDP −1, where P and D are in Fn×n, P is invertible, and
D is diagonal, then the columns of P are an eigenbasis of Fn for A, and the
diagonal entries of D are the corresponding eigenvalues. That is, if the ith
column of P is wi, then (λi, wi) is an eigenpair for A.
Proof. Let w1, . . . , wn be an eigenbasis, and put P = (w1 · · · wn). Then
AP = (Aw1 · · · Awn) = (λ1w1 · · · λnwn) = (w1 · · · wn)D,
(8.6)
where D = diag(λ1, . . . , λn). Thus AP = PD, so A is diagonalizable over
F. The converse is proved in a similar manner.
□

8.2
Basic Results on Eigentheory
253
Recall, a linear mapping having an eigenbasis is called semisimple. Finding
an eigenbasis for a linear mapping reduces to the problem of diagonalizing
its matrix.
Proposition 8.6. Suppose dim V = n, B = {v1, . . . , vn} is a basis of V
and A = MB
B(T) is the matrix of T with respect to B. Then T is semisimple
if and only if A is diagonalizable over F, say A = PDP −1, and an eigenbasis
w1, . . . , wn for T is given by
(w1 w2 · · · wn) = (v1 v2 · · · vn)P.
(8.7)
Proof. The proof is similar to the last, but we will give it anyway. Assume the
notation already in use above, and suppose A = PDP −1, where P, D ∈Fn×n
and D is diagonal. Then
(T(v1) T(v2) · · · T(vn)) = (v1 v2 · · · vn)PDP −1,
so
(T(v1) T(v2) · · · T(vn))P = (v1 v2 · · · vn)PD.
Thus if w1, . . . , wn are deﬁned by
(w1 w2 · · · wn) = (v1 v2 · · · vn)P,
then since P is nonsingular, w1, . . . , wn form a basis of V such that
T(w1 w2 · · · wn) = T(v1 v2 · · · vn)P
= (T(v1) T(v2) · · · T(vn))P
= (v1 v2 · · · vn)(PDP −1)P
= (v1 v2 · · · vn)PD
= (w1 w2 · · · wn)D.
Consequently, since D is diagonal, w1, . . . , wn form a basis of V such that
T(wi) = λiwi for each i. Hence T is semisimple. The converse is proved by
reversing the argument.
□
Consequently, we have the following corollary.
Corollary 8.7. A linear mapping T : V →V is semisimple if and only if
its matrix with respect to some basis of V is diagonal, and consequently its
matrix with respect to any basis of V is diagonalizable.

254
8
Eigentheory
8.2.3
A criterion for diagonalizability
The following proposition states a well-known criterion for diagonalizability.
Since we will prove a stronger result later, the proof will be omitted. We will
also give what amounts to a one-line proof in Example 8.10.
Proposition 8.8. An n × n matrix A over F with n distinct eigenvalues in
F is diagonalizable. More generally, if V is a ﬁnite-dimensional vector space
over F and T : V →V is a linear mapping with dim V distinct eigenvalues
in F, then T is semisimple.
The criterion for A ∈Fn×n to have distinct eigenvalues, given that its
eigenvalues lie in F, is that its characteristic polynomial have simple roots.
The multiple root test (Corollary 2.38) applied to pA(x) therefore gives the
following.
Proposition 8.9. A square matrix A over F has no repeated eigenvalues in
F if pA(x) and its derivative (pA)′(x) have no common roots in F.
Example 8.5. The counting matrix
C =
⎛
⎝
1
2
3
4
5
6
7
8
9
⎞
⎠
has characteristic polynomial pC(x) = −x3 + 15x2 −18x; hence C has three
distinct real eigenvalues. It follows that C is diagonalizable.
⊠
Example 8.6. The matrix
A =
⎛
⎜
⎜
⎝
0
1
0
1
1
0
2
1
0
2
0
0
1
1
0
0
⎞
⎟
⎟
⎠,
which is seen on a blackboard in the movie Good Will Hunting, has charac-
teristic polynomial
pA(x) = x4 −7x2 −2x + 4.
Now, −1 is a root, and hence
pA(x) = (x + 1)(x3 −x2 −6x + 4).
Since −1 is not a root of q(x) = x3 −x2 −6x + 4 = 0, it follows that pA(x)
has four distinct roots as long as q(x) has three distinct roots. But q′(x) =
3x2 −2x −6 has roots

8.2
Basic Results on Eigentheory
255
r = 2 ±
√
76
6
,
and
q(2 +
√
76
6
) < 0,
while q(2 −
√
76
6
) > 0.
Thus q and q′ have no roots in common, so q has three distinct roots. More-
over, it follows from the two inequalities above that q has three real roots.
(Reason: the coeﬃcient of x3 is positive.) Thus A has four distinct real eigen-
values; hence A is diagonalizable.
⊠
Note that the Good Will Hunting matrix A is real and symmetric. As
we will see later, the principal axis theorem therefore guarantees that A is
diagonalizable.
8.2.4
The powers of a diagonalizable matrix
In this section, we will consider the powers of a square matrix A. Let us begin
with the following observation. Suppose A ∈Fn×n has an eigenpair (λ, v) and
let k be a positive integer. Since Akv = Ak−1(Av) = Ak−1(λv) = λAk−1v, it
follows by iteration that
Akv = λkv.
Now suppose A can be diagonalized as A = PDP −1. Then for every positive
integer k,
Ak = (PDP −1)(PDP −1) · · · (PDP −1) = PDkP −1.
If D = diag(λ1, λ2, . . . , λn), then Dk = diag

(λ1)k, (λ2)k, . . . , (λn)k
. Let P =
(v1 v2 · · · vn). Setting v =  aivi, we have
Akv =

aiAkvi =

ai(λi)kvi.
(8.8)
If A ∈Rn×n and all its eigenvalues are real, one can say more:
(i) If A has only nonnegative eigenvalues, then A has a kth root for every
positive integer k. In fact, A
1
k = PD
1
k P −1.
(ii) If none of the eigenvalues of A are 0, then the negative powers of A are
found from the formula A−k = (A−1)k = PD−kP −1.
(iii) If all the eigenvalues λ of A satisfy |λ| < 1, then limm→∞Am = O.

256
8
Eigentheory
More generally, if A ∈Cn×n, there are corresponding statements. The
reader should attempt to formulate them. For nondiagonalizable matrices,
kth roots need not exist.
8.2.5
The Fibonacci sequence as a dynamical
system
The Fibonacci numbers ak are deﬁned by the Fibonacci sequence (ak)
as follows. Starting with arbitrary integers a0 and a1, put a2 = a0 + a1,
a3 = a2 + a1, and in general, put ak = ak−1 + ak−2 if k ≥2. The Fibonacci
sequence can also be deﬁned by the matrix identity

ak+1
ak

=

1
1
1
0
 
ak
ak−1

,
provided k ≥1. Hence putting
F =

1
1
1
0

,
we obtain that
ak+1
ak

= F
 ak
ak−1

= F 2
ak−1
ak−2

= · · · = F k
a1
a0

.
Thus putting
vk =

ak+1
ak

,
for k = 0, 1, 2, . . . , we can therefore express the Fibonacci sequence in the
form
vk = F kv0.
(8.9)
Such a sequence is called the dynamical system deﬁned by F. To analyze the
Fibonacci sequence, we will diagonalize F. The characteristic equation of F
is x2 −x −1 = 0, so the eigenvalues are
φ = 1 +
√
5
2
,
μ = 1 −
√
5
2
.
One checks that N(F −φI2) = R(φ, 1)T and N(F −μI2) = R(μ, 1)T . There-
fore, as in the previous example, F is diagonalized by

8.2
Basic Results on Eigentheory
257
F =

φ
μ
1
1
 
φ
0
0 μ
 
φ
μ
1
1
−1
.
Hence, by (8.9),
am+1
am

= F m
a1
a0

=
φ
μ
1
1
 φm
0
0
μm
 φ
μ
1
1
−1 a1
a0

.
Since

φ
μ
1
1
−1
=
1
φ −μ

1
−μ
−1
φ

, we get

am+1
am

=
1
φ −μ

φm+1 −μm+1
−μφm+1 + μm+1φ
φm −μm
μφm + μmφ
 
a1
a0

.
For example, if a0 = 0 and a1 = 1, we see that
am = φm −μm
φ −μ
=
1
√
5 · 2m ((1 +
√
5)m −(1 −
√
5)m).
(8.10)
Notice that
lim
m→∞
am+1
am
= lim
m→∞
φm+1 −μm+1
φm −μm
= φ,
since limm→∞(μ/φ)m = 0. Therefore, for large m, the ratio am+1/am is
approximately φ. A little further computation gives the precise formulas
a2m =
φ2m
√
5

and
a2m+1 =
φ2m+1
√
5

+ 1,
where [r] denotes the integer part of the real number r.
The eigenvalue φ = 1 +
√
5
2
is the so-called golden ratio. It comes up in
many unexpected and interesting ways. For more information, one can consult
The Story of φ, the World’s Most Astonishing Number, by Mario Livo.
Exercises
Exercise 8.2.1. Diagonalize the following matrices when possible:
A =
 1
−1
−1
1

,
B =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠,
C =
⎛
⎜
⎜
⎝
−3
0
−4
−4
0
2
1
1
4
0
5
4
−4
0
−4
−3
⎞
⎟
⎟
⎠.

258
8
Eigentheory
Exercise 8.2.2. A 4 × 4 matrix over R has eigenvalues ±1, trace 3, and
determinant 0. Can A be diagonalized over R? What about over Q?
Exercise 8.2.3. Determine which of the following matrices are diagonaliz-
able over the reals:
⎛
⎝
1
0
−1
−1
1
1
2
−1
−2
⎞
⎠,
⎛
⎝
0
1
1
1
0
1
1
−1
1
⎞
⎠,
⎛
⎝
2
−1
1
1
0
1
1
−1
−2
⎞
⎠,
⎛
⎝
0
1
0
1
0
−1
0
1
0
⎞
⎠.
Exercise 8.2.4. Find all possible square roots, if any exist, of the following
matrices:

2
1
1
2

,

1
2
2
1

,

1
−1
1
−1

.
Exercise 8.2.5. Do the same as in Problem 8.2.4 for the 4 × 4 all 1’s matrix.
Exercise 8.2.6. Compute the nth power of each matrix of Exercise 8.2.4
and also that of the 3 × 3 all 1’s matrix.
Exercise 8.2.7. Let F denote the Fibonacci matrix. Find F 4 in two ways,
once directly and once using eigentheory.
Exercise 8.2.8. Assuming a0 = 0 and a1 = 1, ﬁnd the thirteenth and
ﬁfteenth Fibonacci numbers using eigentheory.
Exercise 8.2.9. Show directly that
φm −μm
φ −μ
=
1
√
5 · 2m ((1 +
√
5)m −(1 −
√
5)m)
is an integer, thus explaining the strange expression in Section 5.1.
Exercise 8.2.10. Let A =

1
2
2
1

. Find A10.
Exercise 8.2.11. Give an example of a 2 × 2 matrix A over C that does not
have a square root.
Exercise 8.2.12. If possible, ﬁnd a square root for A =

1
1
0
1

.
Exercise 8.2.13. Suppose G is a ﬁnite subgroup of GL(n, C) of order m.
Show that if g ∈G, then every eigenvalue of g is an mth root of unity.

8.3
Two Characterizations of Diagonalizability
259
8.3
Two Characterizations of Diagonalizability
In this section, we ﬁrst prove the strong form of the diagonalizability crite-
rion stated in Proposition 8.10. Then we give two characterizations of the
diagonalizable matrices (equivalently semisimple linear mappings). The ﬁrst
involves the eigenspace decomposition. The second is more interesting, since
it gives a precise criterion for A ∈Fn×n to be diagonalizable in terms of
whether A satisﬁes a certain polynomial equation. This brings in the notion
of the minimal polynomial of a matrix.
8.3.1
Diagonalization via eigenspace
decomposition
In the previous section, we stated the result that every A ∈Fn×n with n
distinct eigenvalues in F is diagonalizable. We will now extend this result by
dropping the assumption that the eigenvalues are distinct. Suppose λ ∈F is
an eigenvalue of A and (x −λ)k divides pA(x) for some k > 1. Then we say
that λ is a repeated eigenvalue of A. We deﬁne the algebraic multiplicity of
λ as the largest value of k such that (x −λ)k divides pA(x). The geometric
multiplicity of λ is deﬁned to be dim Eλ(A). It turns out that the algebraic
multiplicity of an eigenvalue is always greater than or equal to its geometric
multiplicity. (The proof of this will have to wait until Chap. 10.)
Proposition 8.10. Suppose λ1, . . . , λm ∈F are distinct eigenvalues of A ∈
Fn×n, and choose a set of linearly independent eigenvectors in the eigenspace
Eλi(A) for each λi, 1 ≤i ≤m. Then the union of these linearly independent
sets is linearly independent.
Proof. First, notice that if i ̸= j, then Eλi(A) ∩Eλj(A) = {0}. For each i,
let Si denote a linearly independent subset of Eλi(A), and put S = m
i=1 Si.
Write S = {u1, . . . , us} in some way, and suppose

arur = 0,
(8.11)
where all ar ∈F. Let Mi, i = 1, . . . , m, denote the set of indices j such that
uj ∈Si, and put vi = 
j∈Mi ajuj. Thus, vi ∈Eλi(A), and by assumption,
m

i=1
vi = 0.
We will now show that all vi are equal to 0. Suppose some vi is not equal to
0. Without loss of generality, we suppose i = 1, so v1 = −
i>1 vi ̸= 0. Now

260
8
Eigentheory
let W denote the span of Y = {v2, . . . , vm}. By the dimension theorem, we
may select w1, . . . , wℓ∈Y that form a basis of W. Of course, each wi is an
eigenvector of A. Let μi denote the corresponding eigenvalue, and note that
λ1 ̸= μi for all i. Write v1 = 
i biwi. By applying A to v1, we get λ1v1 =

i biμiwi. By multiplying v1 by λ1, we also obtain that λ1v1 = 
i λ1biwi.
Subtracting the two expressions for λ1v1 gives

i
(λ1 −μi)biwi = 0.
But the wi are independent, so (λ1 −μi)bi = 0 for all i. Since λ1 −μi ̸= 0
for all i, it follows that all bi are zero. Hence v1 = 0, a contradiction. It
follows that all vi equal 0, so 
j∈Mi ajuj = 0 for all i. Consequently, all ar
are equal to zero in the original expression (8.11). Therefore, S is linearly
independent.
□
We now state the ﬁrst characterization of the diagonal matrices. Let A ∈
Fn×n.
Theorem 8.11. Let λ1, . . . , λm denote the distinct eigenvalues of A in F.
Then A is diagonalizable over F if and only if
m

i=1
dimEλi(A) = n.
(8.12)
In that case, if Bi is a basis of Eλi(A), then
B =

1≤i≤m
Bi
is an eigenbasis of Fn, and we have the direct sum decomposition
Fn = Eλ1(A) ⊕· · · ⊕Eλm(A).
(8.13)
Proof. Suppose (8.12) holds. Then Proposition 8.10 and the dimension the-
orem imply that A admits an eigenbasis. Therefore, A is diagonalizable. The
converse statement is immediate. The rest of the proof amounts to applying
results on direct sums, especially Proposition 6.26.
□
The following example with repeated eigenvalues is rather fun to analyze.
Example 8.7. Let B denote the 4 × 4 all-ones matrix
B =
⎛
⎜
⎜
⎝
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎞
⎟
⎟
⎠.

8.3
Two Characterizations of Diagonalizability
261
Now 0 is an eigenvalue of B. In fact, B has rank 1, so dim N(B) = 3. Thus
the eigenspace E0(B) of 0 has dimension 3. Every eigenvector for the eigen-
value 0 satisﬁes the equation x1 + x2 + x3 + x4 = 0. The basic null vectors
for this equation are f1 = (−1, 1, 0, 0)T , f2 = (−1, 0, 1, 0)T , f3 = (−1, 0, 0, 1)T ,
and they give a basis of N(B). By Proposition 8.10, there is at most one other
eigenvalue. The other eigenvalue can be found by inspection by noticing that
every row of B adds up to 4. Thus, f4 = (1, 1, 1, 1)T is an eigenvector for
λ = 4. By Proposition 8.10, we now have four linearly independent eigenvec-
tors, hence an eigenbasis. Therefore, B is diagonalizable; in fact, B is similar
to D = diag(0, 0, 0, 4).
⊠
In the above example, the eigenvalues of B were found by being a little
clever and using some special properties of B. A more methodical way to ﬁnd
the eigenvalues would have been to compute the characteristic polynomial of
B using principal minors. In fact, all principal minors of B are zero except
for the 1 × 1 principal minors, namely the diagonal elements. Hence, pB(x) =
x4 −Tr(A)x3 = x4 −4x3 = x3(x −4). Recall also that if all eigenvalues but
one are known, the ﬁnal eigenvalue can be found immediately from the trace.
The following result classifying the semisimple linear mappings follows
immediately from Theorem 8.11. Let V be a ﬁnite-dimensional vector space
over F.
Corollary 8.12. A linear mapping T : V →V is semisimple if and only if
m
i dim Eλi(T) = dim V , where λ1, . . . , λm denote the distinct eigenvalues
of T.
8.3.2
A test for diagonalizability
Is there a simple way of determining when a linear mapping is semisimple or
when a matrix is diagonalizable? It turns out the answer is yes, as long as all
its eigenvalues are known. Here, the algebraic and geometric multiplicities do
not play a role. Let T : V →V be a linear mapping whose distinct eigenvalues
(in F) are λ1, . . . , λm.
Claim: if T is semisimple, then
(T −λ1IV )(T −λ2IV ) · · · (T −λmIV ) = O.
(8.14)
Proof. Indeed, if T is semisimple, then
V = Eλ1(T) ⊕Eλ2(T) · · · ⊕Eλm(T).
Thus, to prove (8.14), it suﬃces to show that

262
8
Eigentheory
(T −λ1IV )(T −λ2IV ) · · · (T −λmIV )vi = 0,
provided vi ∈Eλi(T) for some i. But this is clear, since
(T −λiIV )(T −λjIV ) = (T −λjIV )(T −λiIV )
for all i and j. Thus, the left-hand side of (8.14) can be factored into the form
C(T −λiIV ), and since C(T −λiIV )vi = C0 = 0, (8.14) follows.
□
There is another way to prove (8.14). To do so, we need to evaluate a poly-
nomial f(x) = anxn + an−1xn−1 + · · · + a1x + a0 in F[x] on a matrix in Fn×n.
The value of f at A is deﬁned to be f(A) = anAn + an−1An−1 + · · · + a1A +
a0In. Thus, f(A) ∈Fn×n. Similarly, if and T : V →V is a linear mapping,
then by deﬁnition,
f(T) = anT n + an−1T n−1 + · · · + a1T + a0IV ,
where T i means T composed with itself i times. Thus, f(T) is also a linear
mapping with domain and target V . Note that if A = PDP −1, where A ∈
Fn×n, then f(A) = f(PDP −1) = Pf(A)P −1. Thus, if A = Pdiag(λ1, λ2 . . . ,
λn)P −1 and g(x) = (x −λ1)(x −λ2) · · · (x −λm), then
g(A) = g(PDP −1) = Pg(D)P −1 = Pdiag(g(λ1), g(λ2), . . . , g(λm))P −1.
But diag(g(λ1), g(λ2), . . . , g(λm)) = O, so g(A) = O also.
□
The useful fact is that the converse is also true. We state and prove that
next.
Theorem 8.13. Suppose the distinct eigenvalues of a linear mapping T :
V →V are λ1, . . . , λm. Then T is semisimple if and only if
(T −λ1IV )(T −λ2IV ) · · · (T −λmIV ) = O.
(8.15)
Proof. The “only if” implication was just proved. The “if” assertion, which
is harder to prove, is based on the following lemma.
Lemma 8.14. Suppose T1, T2, . . . , TN : V →V are linear mappings that sat-
isfy the following properties:
(i) T1 ◦T2 ◦· · · ◦TN = O,
(ii) Ti ◦Tj = Tj ◦Ti for all indices i, j,
(iii) for each i = 1, . . . , N −1, ker(Ti) ∩ker(Ti+1 ◦· · · ◦TN) = {0}.
Then V = ker(T1) ⊕ker(T2) ⊕· · · ⊕ker(TN).

8.3
Two Characterizations of Diagonalizability
263
Proof. The proof is based on the following principle. If P, Q : V →V are
linear mappings such that P ◦Q = O and ker(P) ∩ker(Q) = {0}, then V =
ker(P) ⊕ker(Q). This is proved by ﬁrst noting that by the rank–nullity the-
orem, dim V = dim ker(Q) + dim im(Q). Since P ◦Q = O, we have im(Q) ⊂
ker(P). Thus dim V ≤dim ker(Q) + dim ker(P). Now apply the Grassmann
intersection formula, which says that
dim(ker(P) + ker(Q)) = dim ker(P) + dim ker(Q) −dim(ker(P) ∩ker(Q)).
Since dim(ker(P) ∩ker(Q)) = 0, the previous inequality says that dim(ker
(P) + ker(Q)) ≥dim V . But ker(P) + ker(Q) is a subspace of V , so V =
ker(P) + ker(Q). Since ker(P) ∩ker(Q) = {0}, it follows from Proposition
6.26 that V = ker(P) ⊕ker(Q). Letting P = T1 and Q = T2 ◦· · · ◦TN, we
have shown that V = ker(T1) ⊕ker(T2 ◦· · · ◦TN). Now repeat the argument,
replacing V by ker(T2 ◦· · · ◦TN), P by T2, and Q by T3 ◦· · · ◦TN. This is
allowed, since by (ii), both T2 and T3 ◦· · · ◦TN map ker(T2 ◦· · · ◦TN) into
itself. Thus, ker(T2 ◦· · · ◦TN) = ker(T2) ⊕ker(T3 ◦· · · ◦TN). Hence
V = ker(T1) ⊕

ker(T2) ⊕ker(T3 ◦· · · ◦TN)

.
It follows that V = ker(T1) ⊕ker(T2) ⊕ker(T3 ◦· · · ◦TN). The hypotheses
allow us to iterate this argument until the conclusion is reached.
□
Clearly, (T −λiIV )(T −λjIV ) = (T −λjIV )(T −λiIV ), so to ﬁnish the
proof of Theorem 8.13, we just have to check that
ker(T −λiIV ) ∩ker

(T −λi+1IV ) · · · (T −λmIV )

= {0}
for i = 1, . . . , m −1. But if x ∈ker(T −λiIV ) and x ̸= 0, then
(T −λi+1IV ) · · · (T −λmIV )x = (λi −λi+1) · · · (λi −λm)x ̸= 0,
since λi ̸= λj if i ̸= j. Thus, by the lemma,
V = Eλ1(T) ⊕· · · ⊕Eλm(T),
which that proves T is semisimple.
□
For matrices, we obtain the following corollary.
Corollary 8.15. Suppose A ∈Fn×n, and let λ1, . . . , λm be the distinct eigen-
values of A in F. Then A is diagonalizable over F if and only if
(A −λ1In)(A −λ2In) · · · (A −λmIn) = O.

264
8
Eigentheory
Remark. Of course, it is possible that there exist distinct λi, 1 ≤i ≤m,
such that (A −λ1In)(A −λ2In) · · · (A −λmIn) = O, where all the λi lie in
a ﬁeld F′ containing F. Some may lie in F, of course. This simply means
that A isn’t diagonalizable over F, but it is over F′. An example of this is
a 2 × 2 rotation matrix that is diagonalizable over C but not over R. This
illustrates an advantage that matrices have over linear mappings. A matrix
over F may not be diagonalizable over F, but it can be diagonalizable over a
ﬁeld containing F. On the other hand, this concept does not make sense for
a linear mapping with domain a vector space over F, since we do not know
how to enlarge the ﬁeld over which V is deﬁned. In fact, it is possible to do
this, but it requires using tensor products.
Let us test the all-ones matrix with the second criterion.
Example 8.8. The 4 × 4 all-ones matrix B has eigenvalues 0 and 4. There-
fore, to test it for diagonalizabilty (over R), we have to show that B(B −
4I4) = O. Calculating the product, we see that
B(B −4I4) =
⎛
⎜
⎜
⎝
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
−3
1
1
1
1
−3
1
1
1
1
−3
1
1
1
1
−3
⎞
⎟
⎟
⎠= O;
hence B is diagonalizable.
⊠
This test works well on upper and lower triangular matrices, since the
eigenvalues are on the diagonal.
Example 8.9. Is the upper triangular matrix
A =
⎛
⎝
1
2
1
0
1
1
0
0
2
⎞
⎠
diagonalizable? Since the eigenvalues are 1 and 2, we have to show that
(A −I3)(A −2I3) = O. Now,
(A −I3)(A −2I3) =
⎛
⎝
0
2
1
0
0
1
0
0
1
⎞
⎠
⎛
⎝
−1
2
1
0
−1
1
0
0
0
⎞
⎠.
The product is clearly nonzero, so A isn’t diagonalizable.
⊠
This example shows that nondiagonalizable matrices exist. It also illus-
trates the general fact that the algebraic multiplicity of an eigenvalue is an
upper bound on the dimension of its corresponding eigenspace, that is, the
geometric multiplicity. Let us now reconsider a result we proved earlier.

8.3
Two Characterizations of Diagonalizability
265
Example 8.10 (Simple eigenvalues). Recall from Proposition 8.10 that
when A ∈Fn×n has n distinct eigenvalues in F, then A is diagonalizable
over F. The Cayley–Hamilton theorem, which we will take up and prove in
the next section, enables us to give an extremely short proof of this fact. Let
λ1, . . . , λn ∈F be the eigenvalues of A, which are assumed to all be diﬀerent.
Then
pA(A) = (A −λ1In)(A −λ2In) · · · (A −λnIn).
(8.16)
But the Cayley–Hamilton theorem says that pA(A) = O, so A is indeed diag-
onalizable by the diagonalizabilty test of Theorem 8.13.
⊠
Exercises
Exercise 8.3.1. Show from ﬁrst principles that if λ and μ are distinct eigen-
values of A, then Eλ(A) ∩Eμ(A) = {0}.
Exercise 8.3.2. Diagonalize the following matrices if possible:
A =

1
−1
−1
1

,
B =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠,
C =
⎛
⎜
⎜
⎝
−3
0
−4
−4
0
2
1
1
4
0
5
4
−4
0
−4
−3
⎞
⎟
⎟
⎠.
Exercise 8.3.3. Consider the real 2 × 2 matrix
A =

0
1
0
0

.
Determine whether A is diagonalizable.
Exercise 8.3.4. Determine which of the following matrices are diagonaliz-
able over the reals:
⎛
⎝
1
0
−1
−1
1
1
2
−1
−2
⎞
⎠,
⎛
⎝
0
1
1
1
0
1
1
−1
1
⎞
⎠,
⎛
⎝
2 −1 1
1 0
1
1 −1 −2
⎞
⎠,
⎛
⎝
0
1
0
1
0
−1
0
1
0
⎞
⎠.
Exercise 8.3.5. Does
C =
⎛
⎝
1
0
−1
−1
1
1
2
−1
−2
⎞
⎠
have distinct eigenvalues? Is it diagonalizable?
Exercise 8.3.6. Let A =
a
b
c
d

, where a, b, c, d are all positive real num-
bers. Show that A is diagonalizable.

266
8
Eigentheory
Exercise 8.3.7. Suppose A is the matrix in Exercise 8.3.6. Show that A has
an eigenvector in the ﬁrst quadrant and another in the third quadrant.
Exercise 8.3.8. Show that if A ∈Rn×n admits an eigenbasis for Rn, it also
admits an eigenbasis for Cn.
Exercise 8.3.9. Let a ∈R3, and let Ca : R3 →R3 be the cross product
map Ca(v) = a × v. Find the characteristic polynomial of Ca and determine
whether Ca is semisimple.
Exercise 8.3.10. Let V = Fn×n and let T : V →V be the linear map deﬁned
by sending A ∈V to AT . That is, T(A) = AT .
(i) Show that the only eigenvalues of T are ±1.
(ii) Prove that if the characteristic of F is diﬀerent from two, then T semi-
simple.
Exercise 8.3.11. Find an example of a nondiagonalizable 3 × 3 matrix A
with real entries that is neither upper nor lower triangular such that every
eigenvalue of A is 0.
Exercise 8.3.12. Let A be a 3 × 3 matrix with eigenvalues 0, 0, 1. Show
that A3 = A2.
Exercise 8.3.13. Let A be a 2 × 2 matrix such that A2 + 3A + 2I2 = O.
Show that −1, −2 are eigenvalues of A.
Exercise 8.3.14. Suppose A is a 2 × 2 matrix such that A2 + A −3I2 = O.
Show that A is diagonalizable.
Exercise 8.3.15. Let U be an upper triangular matrix over F with distinct
entries on its diagonal. Show that U is diagonalizable.
Exercise 8.3.16. Suppose that a 3 × 3 matrix A over R satisﬁes the equa-
tion A3 + A2 −A + 2I3 = O.
(i) Find the eigenvalues of A.
(ii) Is A diagonalizable? Explain.
Exercise 8.3.17. We say that two n × n matrices A and B are simultane-
ously diagonalizable if they are diagonalized by the same matrix M, that is,
if they share a common eigenbasis. Show that two simultaneously diagonal-
izable matrices A and B commute; that is, AB = BA.
Exercise 8.3.18. This is the converse to Exercise 8.3.17. Suppose A, B ∈
Fn×n commute.
(i) Show that for every eigenvalue λ of A, B(Eλ(A)) ⊂Eλ(A).
(ii) Conclude that if S is the subset of Fn×n consisting of diagonalizable
matrices such that AB = BA for all A, B ∈S, then the elements of S are
simultaneously diagonalizable.

8.3
Two Characterizations of Diagonalizability
267
Exercise 8.3.19. Let U be an arbitrary upper triangular matrix over F pos-
sibly having repeated diagonal entries. Show by example that U may not be
diagonalizable, and give a condition to guarantee that it will be diagonalizable
without any diagonal entries being changed.
Exercise 8.3.20. Let V be a ﬁnite dimensional vector space over Fp, p a
prime, and let μ be the mapping with domain and target L(V, V ) deﬁned by
μ(T) = T p. Show that μ is linear and ﬁnd its characteristic polynomial when
V = (Fp)2. Is μ diagonalizable?

268
8
Eigentheory
8.4
The Cayley–Hamilton Theorem
The Cayley–Hamilton theorem gives an interesting and fundamental rela-
tionship between a matrix and its characteristic polynomial, which is related
to our considerations about diagonalizabilty in the previous section. It is, in
fact, one of the most famous and useful results in matrix theory.
8.4.1
Statement of the theorem
The version of the Cayley–Hamilton theorem we will prove below is stated
as follows.
Theorem 8.16 (Cayley–Hamilton theorem). Let F be a ﬁeld and suppose
A ∈Fn×n. Then A satisﬁes its characteristic polynomial, that is, pA(A) = O.
Consequently, if V is a ﬁnite-dimensional vector space over F and T : V →V
is linear, then pT (T) = OV , where OV is the zero mapping on V .
The proof we give below in Section 8.4.4 was noticed by Jochen Kuttler
and myself. It is more straightforward than the usual proof based on Cramer’s
rule. We ﬁrst prove that PA(A) = O inductively just using matrix theory and
the assumption that F contains all the eigenvalues of A. The second step in
the proof, which doesn’t involve matrix theory, is to show that there exists
a ﬁeld F′ containing F such that all the roots of pA(x) = 0 lie in F. We say
F′ is a splitting ﬁeld for pA. Thus, pA(A) = O holds in F′n×n, and hence
pA(A) = O in Fn×n too. And since it holds for A, it also holds for T.
8.4.2
The real and complex cases
If A is a real n × n matrix, its eigenvalues all lie in C, and if A is diagonalizable
over C, say A = MDM −1, then
pA(A) = pA(MDM−1) = MpA(D)M−1 = Mdiag(pA(λ1), pA(λ2), . . . , pA(λn))M−1 = O,
by the argument in Section 8.3.2. With some ingenuity, one can fashion a
proof for arbitrary complex matrices (in particular, real matrices) by showing
that every square matrix over C is the limit of a sequence of diagonalizable
matrices.
Here is an important 2 × 2 example.
Example 8.11. For example, the characteristic polynomial of the matrix
J =
 0 −1
1
0

is x2 + 1. Cayley–Hamilton asserts that J2 + I2 = O, which is
easy to check directly. Notice that the eigenvalues of J are ±i, so J is diago-
nalizable over C, though not over R.
⊠

8.4
The Cayley–Hamilton Theorem
269
8.4.3
Nilpotent matrices
A square matrix A is said to be nilpotent if Am = O for some integer m > 0.
For example,
A =

1
−1
1
−1

is nilpotent, since A2 = O. It follows directly from the deﬁnition of an eigen-
value that a nilpotent matrix cannot have a nonzero eigenvalue. (Convince
yourself of this.) Thus if an n × n matrix A is nilpotent, its characteristic
polynomial pA(x) is equal to (−1)nxn.
Proposition 8.17. If an n × n matrix A is nilpotent, then An = O.
Proof. Just apply Cayley–Hamilton.
□
More generally, we also have the following deﬁnition.
Deﬁnition 8.3. A linear mapping T : V →V is said to be nilpotent if its
only eigenvalue is 0.
So the previous proposition implies the following result.
Proposition 8.18. If T : V →V is nilpotent, then (T)dim V = O.
8.4.4
A proof of the Cayley–Hamilton theorem
We will ﬁrst show that pA(A) = O for every A ∈Fn×n, provided all the eigen-
values of A lie in F. We will induct on n, the case n = 1 being trivial, since if
A = (a), then pA(x) = a −x and thus pA(A) = pA(a) = 0. Assume the result
for n −1, where n > 1. Let (λ1, v1) be an eigenpair for A, and extend v1 to
a basis B of Fn. Then A is similar to B = MB
B(TA), so pA(x) = pB(x), and
hence it suﬃces to show that pB(B) = O. Now
B =
⎛
⎜
⎜
⎜
⎝
λ1
∗
· · ·
∗
0
...
B1
0
⎞
⎟
⎟
⎟
⎠,
where B1 ∈F(n−1)×(n−1). It follows from this block decomposition that
det(B −xIn) = (λ1 −x) det(B1 −xIn−1).
Since
pA(x) = pB(x)
and
the
eigenvaluesλ1, λ2, . . . , λn of A all lie in F, it follows that the eigenvalues of
B are in F, so the eigenvalues λ2, . . . , λn of B1 are also in F. These all being
elements of F, we may apply the induction hypothesis to B1. First, notice

270
8
Eigentheory
the following fact about block matrix multiplication: if C1 and C2 are of size
(n −1) × (n −1), then
⎛
⎜
⎜
⎜
⎝
c1
∗
· · ·
∗
0
...
C1
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
c2
∗
· · ·
∗
0
...
C2
0
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
c1c2
∗
· · ·
∗
0
...
C1C2
0
⎞
⎟
⎟
⎟
⎠.
Now calculate pB(B). By the previous comment,
pB(B) = (−1)n(B −λ1In)(B −λ2In) · · · (B −λnIn)
= (−1)n
⎛
⎜
⎜
⎜
⎝
0
∗
· · ·
∗
0
...
B1 −λ1In−1
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
∗
∗
· · ·
∗
0
...
C
0
⎞
⎟
⎟
⎟
⎠,
where
C = (B1 −λ2In−1) · · · (B1 −λnIn−1).
But clearly C = pB1(B1), so by the induction hypothesis, it follows that
C = O. Thus
pB(B) = (−1)n
⎛
⎜
⎜
⎜
⎝
0
∗
· · ·
∗
0
...
B1 −λ1In−1
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
∗
∗
· · ·
∗
0
...
O
0
⎞
⎟
⎟
⎟
⎠.
Carrying out the multiplication, one sees immediately that pB(B) = O.
□
The assumption that all the roots of the characteristic polynomial of A lie
in F is actually unnecessary, due to the next result.
Lemma 8.19. Let F be a ﬁeld. Then for every polynomial f(x) ∈F[x], there
exists a ﬁeld F′ containing F and all the roots of f(x) = 0.
We will give a proof of this fact in Section 8.7. Thus the eigenvalues of A
lie in an extension F′ of F, so pA(A) = O. All that remains to be proved is
that a linear mapping also satisﬁes its characteristic polynomial. Let V be a
ﬁnite-dimensional vector space over F and T : V →V a linear mapping. Then
for every matrix A of T with respect to a basis of V , we have pA(A) = O.
But as noted already, the matrix of pT (T) with respect to this basis is pA(A).
Since pA(A) = O, it follows that pT (T) is zero also.
□

8.4
The Cayley–Hamilton Theorem
271
8.4.5
The minimal polynomial of a linear mapping
Let V be as usual and T : V →V a linear mapping. By the Cayley–Hamilton
theorem, pT (T) = OV , and therefore there exists a polynomial f(x) ∈F[x] of
least degree and leading coeﬃcient one such that f(T) = OV . This follows
by division with remainder. For if f1 and f2 satisfy the minimal polynomial
criterion, then f1 and f2 must have the same degree k. Then g = f1 −f2 is
a polynomial of degree at most k −1 such that g(T) = O. Thus g = 0. The
minimal polynomial of T will be denoted by μT . The minimal polynomial
μA of a matrix A ∈Fn×n is deﬁned in exactly the same way, and if A is the
matrix of T, then μA = μT .
Proposition 8.20. Suppose T : V →V is a linear mapping whose eigenval-
ues lie in F, and let λ1, . . . , λm denote T’s distinct eigenvalues in F. Then the
minimal polynomial μT (x) of T divides the characteristic polynomial pT (x),
and q(x) = (x −λ1) · · · (x −λm) divides μT (x).
Proof. Since the degree of μT (x) is at most n, division with remainder in F[x]
allows us to write pT (x) = h(x)μT (x) + r(x), where either r = 0 or deg r <
deg μT . But pT (T) = p(T) = O, r(T) = O also. By the deﬁnition of μT , it
follows that r = 0; hence polynomial p(x) divides pT (x). To conclude that
q(x) divides μT (x), use division with remainder again. Let (λj, v) be an
eigenpair for T, and write μT (x) = f(x)(x −λj) + r. Since μT (T)v = (T −
λj)v = 0, it follows that rIn = 0, so r = 0. Thus q divides μT .
□
The following corollary restates the diagonalization criterion of Theorem
8.13. As above, let q(x) = (x −λ1) · · · (x −λm), where λ1, . . . , λm denote T’s
distinct eigenvalues.
Corollary 8.21. The linear mapping T : V →V is semisimple if and only
if its minimal polynomial is q(x).
Proof. If T is semisimple, then all of its eigenvalues lie in F, and by Theorem
8.13, q(T) = O. Therefore, q(x) is the minimal polynomial of T. Conversely,
if q(x) is T’s minimal polynomial, then the criterion of Theorem 8.13 implies
that T is semisimple.
□
Exercises
Exercise 8.4.1. Verify the Cayley–Hamilton theorem directly for
A =
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠.

272
8
Eigentheory
Exercise 8.4.2. Give a direct proof for the 2 × 2 case. That is, show that
A =
a
b
c
d

= A2 −Tr(A)A + det(A)I2 = O.
Apparently, this was Cayley’s contribution to the theorem.
Exercise 8.4.3. The following false proof of the Cayley–Hamilton theorem
is well known and very persistent. In fact, it actually appears in an alge-
bra book. Since setting x = A in pA(x) = det(A −xIn) gives det(A −AIn) =
det(A −A) = 0, it follows that pA(A) = O. What is incorrect about this
“proof”?
Exercise 8.4.4. Show that a 2 × 2 matrix A is nilpotent if and only if
Tr(A) = det(A) = 0. Use this to ﬁnd a 2 × 2 nilpotent matrix with all entries
diﬀerent from 0.
Exercise 8.4.5. Use the Cayley–Hamilton theorem to deduce that A ∈
Fn×n is nilpotent if and only if all eigenvalues of A are 0.
Exercise 8.4.6. Show that a nonzero nilpotent matrix is not diagonalizable.
Exercise 8.4.7. Without using the Cayley–Hamilton theorem, show that if
an n × n matrix A is nilpotent, then in fact An = O. (Hint: use induction
on n. Choose a basis of N(A) and extend to a basis of Fn. Then apply the
induction to col(A).)
Exercise 8.4.8. * Prove that every A ∈Cn×n is the limit of a sequence of
diagonalizable matrices, and thus deduce the Cayley–Hamilton theorem over
C in another way.
Exercise 8.4.9. Test the following matrices to determine which are diago-
nalizable.
(i)

0
1
1
0

and

1
1
0
0

(F = F2).
(ii)
⎛
⎝
2
−1
1
0
1
1
0
0
2
⎞
⎠(F = Q).
(iii)
⎛
⎝
1
−1
2
0
2
0
0
0
2
⎞
⎠(F = Q).
(iv)
⎛
⎜
⎜
⎝
5
4
2
1
0
1
−1
−1
−1
−1
3
0
1
1
−1
2
⎞
⎟
⎟
⎠
(F = Q).
Note
that
the
characteristic
polynomial is x4 −11x3 + 42x2 −64x + 32 = (x −1)(x −2)(x −4)2.

8.4
The Cayley–Hamilton Theorem
273
(v)
⎛
⎜
⎜
⎝
2
1
2
1
0
1
2
2
2
2
0
0
1
1
2
2
⎞
⎟
⎟
⎠(F = F3).
Exercise 8.4.10. Find the minimal polynomials of the matrices in parts
(i)–(iii) of Exercise 8.4.9.
Exercise 8.4.11. Show that the characteristic polynomial of an n × n
matrix divides a power of its minimal polynomial.
Exercise 8.4.12. This exercise shows the minimal polynomial exists with-
out appealing to Cayley–Hamilton. Show that if V is a ﬁnite dimensional
vector space and T : V →V is linear, then the powers T 0, T, T 2, . . . T k are
linearly dependent for some k > 0. Conclude that the minimal polynomial of
T exists.

274
8
Eigentheory
8.5
Self Adjoint Mappings and the Principal Axis
Theorem
We now return to inner product spaces (both real and Hermitian) to treat one
of the most famous results in linear algebra: every matrix that is symmetric
over R or Hermitian over C is diagonalizable. Moreover, it has an ortho-
normal eigenbasis. This result is known as the principal axis theorem. It is
also commonly referred to as the spectral theorem. Our goal is to present
a geometric proof that explains some of the intuition underlying the ideas
behind symmetric and Hermitian matrices. We will ﬁrst treat self-adjoint
linear mappings. These are the linear mappings whose matrices with respect
to an orthonormal basis are symmetric in the real case and Hermitian in the
complex case.
8.5.1
The notion of self-adjointness
Let V be either a real or Hermitian ﬁnite-dimensional inner product space.
The inner product of x and y in V will be denoted by (x, y). Consider a
linear mapping T : V →V such that
(T(x), y) = (x, T(y))
for all x, y ∈V . In the case that V is a real vector space, we will say that
T is a self-adjoint linear mapping, and in the complex case, we will say that
T is a Hermitian self-adjoint linear mapping. The connection between self-
adjointness and matrix theory comes from the following result.
Proposition 8.22. Let V be a ﬁnite-dimensional Hermitian inner product
space (respectively inner product space) over C (respectively R). Then a linear
mapping T : V →V is self-adjoint if and only if its matrix with respect to an
arbitary Hermitian orthonormal basis (respectively orthonormal basis) of V
is Hermitian (respectively symmetric).
The proof is an exercise. A matrix K ∈Cn×n is said to be Hermitian if
KH = K. We remind the reader that AH = (A)T . Consequently, we get the
following corollary.
Corollary 8.23. A linear mapping T : Rn →Rn is self-adjoint if and only
if T = TA, where A is symmetric. Similarly, a linear mapping T : Cn →Cn
is Hermitian self-adjoint if and only if T = TK, where K is Hermitian.
Proof. Let us give a proof in the real case. If T is self-adjoint, then T(ei) ·
ej = ei · T(ej). This implies that aij = aji. Thus A = AT . The converse is
similar.
□

8.5
Self Adjoint Mappings and the Principal Axis Theorem
275
It is more natural to consider the Hermitian case ﬁrst. The reason for this
will be clear later. The geometric consequences of the condition that a linear
mapping is Hermitian self-adjoint are summed up in the following.
Proposition 8.24. Suppose T : V →V is a Hermitian self-adjoint linear
mapping. Then:
(i) all eigenvalues of T are real;
(ii) eigenvectors of T corresponding to distinct eigenvalues are orthogonal;
(iii) if W is a subspace of V such that T(W) ⊂W, then T(W ⊥) ⊂W ⊥;
(iv) im(T) = ker(T)⊥; and
(v) consequently, V = ker(T) ⊕im(T).
Proof. We will ﬁrst show that T has only real eigenvalues. Since C is alge-
braically closed, T has dim V complex eigenvalues. Let λ be any eigen-
value and suppose (λ, w) is a corresponding eigenpair. Since (T(w), w) =
(w, T(w)), we see that (λw, w) = (w, λw). This implies λ|w|2 = λ|w|2. Since
|w| ̸= 0, we have λ = λ, so all eigenvalues of T are real. For (ii), assume that λ
and μ are distinct eigenvalues of T with corresponding eigenvectors v and w.
Then (T(v), w) = (v, T(w)), so (λv, w) = (v, μw). Hence, (λ −μ)(v, w) =
0. (Recall that by (i), λ, μ ∈R.) Since λ ̸= μ, we get (ii). For (iii), let x ∈W
and y ∈W ⊥. Since T(x) ∈W, (T(x), y) = 0. But (x, T(y)) = (T(x), y), so
(x, T(y)) = 0. Since x is arbitrary, it follows that T(y) ∈W ⊥; hence (iii)
follows. For (iv), ﬁrst note that im(T) ⊂ker(T)⊥. Indeed, if y = T(x) and
w ∈ker(T), then
(w, y) = (w, T(x)) = (T(w), x) = 0.
By Proposition 6.38, dim V = dim ker(T) + dim ker(T)⊥, and by the rank–
nullity theorem (Theorem 7.4), dim V = dim ker(T) + dim im(T). Hence,
dim im(T) = dim ker(T)⊥, so im(T) = ker(T)⊥. Part (v) follows from (iv),
since V = W ⊕W ⊥for every subspace W of V .
□
8.5.2
Principal Axis Theorem for self-adjoint
linear mappings
We will now prove the principal axis theorem, starting with the Hermitian
version.
Theorem 8.25. (The Hermitian Principal Axis Theorem) Suppose V is a
ﬁnite-dimensional Hermitian vector space, and let T : V →V be Hermitian
self-adjoint. Then there exists a Hermitian orthonormal basis of V consisting
of eigenvectors of T.

276
8
Eigentheory
Proof. We will induct on dim V . The result is certainly true if dim V = 1.
Suppose it holds when dim V ≤n −1, and let dim V = n. Let λ ∈R be any
eigenvalue of T, and let us replace T by S = T −λIV . If V = ker(S), there is
nothing to prove, since every Hermitian orthonormal basis of V is an eigen-
basis. Thus we may assume that V ̸= ker(S). Note that S is also self-adjoint,
and ker(S) and im(S) are both stable under T, since S and T commute.
Moreover, by the previous proposition, V decomposes into the orthogonal
direct sum V = ker(S) ⊕im(S). Since 0 < dim ker(S) ≤n −1 and likewise,
0 < dim im(S) ≤n −1, it follows by induction that ker(S) and im(S) each
admits a Hermitian orthonormal eigenbasis for T. This ﬁnishes the proof. □
Corollary 8.26. If K ∈Cn×n is Hermitian, then there exists an eigenbasis
w1, w2, . . . , wn for K that is Hermitian orthonormal. Hence, there exists a
unitary matrix U ∈Cn×n such that KU = UD, where D is real diagonal.
Thus, K = UDU −1 = UDU H.
Proof. Put U = (w1 w2 . . . wn).
□
Similarly, we have the real principal axis theorem.
Theorem 8.27. (The real Principal Axis Theorem) Suppose V is a ﬁnite-
dimensional inner product space, and let T : V →V be self-adjoint. Then
there exists an orthonormal basis of V consisting of eigenvectors of T.
Proof. The proof is identical to that of the Hermitian principal axis theorem
once we prove that T has only real eigenvalues. For this, we appeal to the
matrix A of T with respect to an orthonormal basis of V . This matrix is
symmetric, hence also Hermitian. Therefore, A has only real eigenvalues.
But the eigenvalues of A are also the eigenvalues of T, so we have the desired
result.
□
Corollary 8.28. If A ∈Rn×n is symmetric, then there exists an orthonor-
mal eigenbasis u1, u2, . . . , un for A. Consequently, there exists an orthogonal
matrix Q such that A = QDQ−1 = ADQT , where D is real diagonal.
Proof. Take Q = (u1 u2 . . . un).
□
Remark. Symmetric and Hermitian matrices satisfy the condition that
AAH = AHA. Thus they are normal matrices, and hence automatically admit
a Hermitian orthonormal basis, by the normal matrix theorem, which we will
prove in Chap. 9. The proof is conceptually much simpler, but it doesn’t
shed any light on the geometry. There are inﬁnite-dimensional versions of the
spectral theorem for bounded self-adjoint operators on Hilbert space that the
reader can read about in a book on functional analysis. The ﬁnite-dimensional
spectral theorem is due to Cauchy.

8.5
Self Adjoint Mappings and the Principal Axis Theorem
277
8.5.3
Examples of self-adjoint linear mappings
Let us now consider some examples.
Example 8.12 (Projections).
Let W be a subspace of Rn. Recall from
Section 7.3.5 that the projection PW : Rn →Rn is deﬁned by choosing an
orthonormal basis u1, . . . , um of W and putting
PW (x) =
m

i=1
(x · ui)ui.
Certainly PW is linear. To see that it is self-adjoint, extend the orthonormal
basis of W to an orthonormal basis u1, . . . , un of Rn. If 1 ≤i, j ≤m, then
PW (ui) · uj = ui · PW (uj) = ui · uj. On the other hand, if one of the indices
i, j exceeds m, then PW (ui) · uj = 0 and ui · PW (uj) = 0. Thus, PW is self-
adjoint.
⊠
Example 8.13 (Reﬂections). Let H be a hyperplane in Rn, say H = (Ru)⊥,
where u is a unit vector. Recall that the reﬂection of Rn through H is the
linear mapping Q(v) = v −2(v, u)u. Then Q = In −2PW , where W = Ru.
Since the sum of two self-adjoint maps is evidently self-adjoint, all reﬂections
are also self-adjoint.
⊠
Example 8.14. Recall that in Example 6.33 we introduced the inner prod-
uct on Rn×n given by the Killing form
(A, B) = Tr(ABT ),
where Tr(A) is the trace of A. In this example, we show that the linear
mapping T : Rn×n →Rn×n deﬁned by T(A) = AT is self-adjoint with respect
to the Killing form. In other words, (AT , B) = (A, BT ) for all A, B. To see
this, note that
(AT , B) = Tr(AT BT ) = Tr((BA)T ) = Tr(BA),
while
(A, BT ) = Tr(A(BT )T ) = Tr(AB).
But Tr(AB) = Tr(BA) for all A, B ∈Rn×n, so T is self-adjoint.
⊠
In fact, the identity Tr(AB) = Tr(BA) holds for all A, B ∈Fn×n, where F
is any ﬁeld.
Example 8.15. Here is another way to show that T(A) = AT is self-adjoint
on Rn×n. We will show that T admits an orthonormal eigenbasis. Notice that
since T 2 = IW , where W = Rn×n, its only eigenvalues are ±1. An eigenbasis

278
8
Eigentheory
consists of the symmetric matrices Eii and (Eij + Eji)/
√
2, where i ̸= j with
eigenvalue λ = 1, and the skew-symmetric matrices (Eij −Eji)/
√
2 for i ̸= j
with eigenvalue λ = −1. We claim that this eigenbasis is orthonormal (the
veriﬁcation is an exercise), so T is self-adjoint. A consequence of this ortho-
normal basis is that every real matrix can be orthogonally decomposed as
the sum of a symmetric matrix and a skew-symmetric matrix. (Recall that
we already proved in Chap. 6 that every square matrix over an arbitrary ﬁeld
of characteristic diﬀerent from two can be uniquely expressed as the sum of
a symmetric matrix and a skew-symmetric matrix.)
⊠
The next example is an opportunity to diagonalize a 4 × 4 symmetric
matrix (one that was already diagonalized in Chap. 8) without any calcula-
tions.
Example 8.16. Let B be the 4 × 4 all-ones matrix. The rank of B is clearly
one, so 0 is an eigenvalue and N(B) = E0 has dimension three. In fact,
E0 = (R(1, 1, 1, 1)T )⊥. Thus, (1, 1, 1, 1)T is also an eigenvector. In fact, all
the rows sum to 4, so the eigenvalue for (1, 1, 1, 1)T is 4. Consequently, B
is orthogonally similar to D = diag(4, 0, 0, 0). To produce Q such that B =
QDQT , we need to ﬁnd an orthonormal basis of E0. One can simply look for
orthogonal vectors orthogonal to (1, 1, 1, 1)T . In fact, v1 = 1/2(1, 1, −1, −1)T ,
v2 = 1/2(1, −1, 1, −1)T , and v3 = 1/2(1, −1, −1, 1)T will give such an ortho-
normal basis after they are normalized. We can thus write
B = 1
2
⎛
⎜
⎜
⎝
1
1
1
1
1
1
−1
−1
1
−1
1
−1
1
−1
−1
1
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
4
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
1
1
1
1
1
1
−1
−1
1
−1
1
−1
1
−1
−1
1
⎞
⎟
⎟
⎠
1
2.
Notice
that
the
orthogonal
matrix
Q
used
here
is
symmetric,
so
Q−1 = Q.
⊠
8.5.4
A projection formula for symmetric
matrices
One of the nice applications of the principal axis theorem is that it enables
one to express any symmetric matrix as a linear combination of orthogonal
projections. Suppose A ∈Rn×n is symmetric, and let (λ1, u1), . . . , (λn, un) be
eigenpairs for A that give an orthonormal eigenbasis of Rn. Then if x ∈Rn,
the projection formula (6.21) gives
x = (uT
1 x)u1 + · · · + (uT
n x)un.
Hence

8.5
Self Adjoint Mappings and the Principal Axis Theorem
279
Ax = λ1(uT
1 x)u1 + · · · + λn(uT
n x)un.
Thus
A = λ1u1uT
1 + · · · + λnunuT
n .
(8.17)
Since uiuT
i is the matrix of the projection of Rn onto the line Rui, the identity
(8.17) indeed expresses A as a linear combination of orthogonal projections.
This formula holds in the Hermitian case as well, provided one uses the
Hermitian inner product.
The projection formula (8.17) can be put in a more elegant form. If
μ1, . . . , μk are the distinct eigenvalues of A and E1, . . . , Ek are the corre-
sponding eigenspaces, then
A = μ1PE1 + μ2PE2 + · · · + μkPEk.
(8.18)
For example, in the case of the all-ones matrix of Example 8.16,
A = 0PE0 + 4PE4 = 4PE4.
Exercises
Exercise 8.5.1. Let Ca : R3 →R3 be the cross product map Ca(v) = c × v.
True or false: Ca is self-adjoint.
Exercise 8.5.2. Prove Proposition 8.22.
Exercise 8.5.3. Are rotations of R3 self-adjoint?
Exercise 8.5.4. Orthogonally diagonalize the following matrices:
⎛
⎝
1
0
1
0
1
0
1
0
1
⎞
⎠,
⎛
⎝
1
1
3
1
3
1
3
1
1
⎞
⎠,
⎛
⎜
⎜
⎝
1
0
1
0
0
1
0
1
1
0
1
0
0
1
0
1
⎞
⎟
⎟
⎠.
(Try to diagonalize the ﬁrst and third matrices without pencil and paper.
You can also ﬁnd an eigenvalue of the second by inspection.)
Exercise 8.5.5. Let A =

a
b
b
c

∈R2×2.
(i) Show directly that both roots of the characteristic polynomial of A are
real.
(ii) Prove that A is orthogonally diagonalizable without appealing to the
principal axis theorem.

280
8
Eigentheory
Exercise 8.5.6. Suppose B is a real symmetric 3 × 3 matrix such that
(1, 0, 1)T ∈N(B −I3), and (1, 1,
−1)T ∈N(B −2I3). If det(B) = 12,
ﬁnd B.
Exercise 8.5.7. Answer each question true or false. If true, give a brief
reason. If false, give a counter example.
(i) The sum and product of two symmetric matrices are symmetric.
(ii) If two symmetric matrices A and B have the same eigenvalues, counting
multiplicities, then A and B are orthogonally similar (A = QBQ−1, where Q
is orthogonal).
Exercise 8.5.8. Suppose A is a 3 × 3 symmetric matrix such that the trace
of A is 4, the determinant of A is 0, and v1 =
⎛
⎝
1
0
1
⎞
⎠and v2 =
⎛
⎝
1
1
1
⎞
⎠are
eigenvectors of A that lie in the image of TA.
(i) Find the eigenvalues of A.
(ii) Find the eigenvalues corresponding to v1 and v2.
(iii) Finally, ﬁnd A itself.
Exercise 8.5.9. Suppose A and B in Rn×n are both symmetric and have a
common eigenbasis. Show that AB is symmetric.
Exercise 8.5.10. Suppose A, B, and AB are symmetric. Show that A and
B are simultaneously diagonalizable. Is BA symmetric?
Exercise 8.5.11. Let W be a subspace of Rn. Simultaneously orthogonally
diagonalize PW and PW ⊥.
Exercise 8.5.12. • Suppose A ∈R3×3 is symmetric and the trace of A is
an eigenvalue. Show that if A is invertible, then det(A)Tr(A) < 0.
Exercise 8.5.13. • Assume that a, b, c are all real. Let
A =
⎛
⎝
a
b
c
b
c
a
c
a
b
⎞
⎠.
(i) Show that if Tr(A) = 0, then det(A) = 0 too.
(ii) Diagonalize A if det(A) = 0 but Tr(A) ̸= 0.
Exercise 8.5.14. Using Exercise 8.5.12 and the matrix A in Exercise 8.5.13,
show that if a + b + c > 0, then
a3 + b3 + c3 −3abc > 0.

8.5
Self Adjoint Mappings and the Principal Axis Theorem
281
Exercise 8.5.15. • Diagonalize
A =
⎛
⎜
⎜
⎝
aa
ab
ac
ad
ba
bb
bc
bd
ca
cb
cc
cd
da
db
dc
dd
⎞
⎟
⎟
⎠,
where a, b, c, d are arbitrary real numbers. (Note: it may help to factor A.)
Exercise 8.5.16. Prove that a real symmetric matrix A whose only eigen-
values are ±1 is orthogonal.
Exercise 8.5.17. Suppose A ∈Rn is symmetric. Show that if Ak = O for
some positive integer k, then A = O.
Exercise 8.5.18. Give a direct proof of the principal axis theorem in the
2 × 2 Hermitian case.
Exercise 8.5.19. Show that two real symmetric matrices A and B having
the same characteristic polynomial are orthogonally similar. In other words,
A = QBQ−1 for some orthogonal matrix Q.
Exercise 8.5.20. • Let A ∈Rn be symmetric, and let λm and λM be its
minimum and maximum eigenvalues respectively.
(i) Use formula (8.17) to show that for every x ∈Rn, we have
λmxT x ≤xT Ax ≤λMxT x.
(ii) Use this inequality to ﬁnd the maximum and minimum values of |Ax| on
the ball |x| ≤1 in Rn.
(iii) Show that the maximum and minimum values of xT Ax for |x| = 1 are
eigenvalues of A.
Exercise 8.5.21. Show that if Q ∈Rn is orthogonal and symmetric, then
Q2 = In. Moreover, if 1 is not an eigenvalue of Q, then Q = −In.
Exercise 8.5.22. Find the eigenvalues of K =

2
3 + 4i
3 −4i
−2

and diag-
onalize K.
Exercise 8.5.23. Unitarily diagonalize the rotation Rθ =

cos θ
−sin θ
sin θ
cos θ

.
Exercise 8.5.24. Using only the deﬁnition, show that the trace and deter-
minant of a Hermitian matrix are real.
Exercise 8.5.25. Describe the relationship between U(1, C) and SO(2, R).

282
8
Eigentheory
Exercise 8.5.26. Let SU(2, C) ⊂U(2, C) denote the set of 2 × 2 unitary
matrices of determinant one.
(i) Show that SU(2, C) is a matrix group.
(ii) Describe the eigenvalues of the elements of SU(2, C).
Exercise 8.5.27. Consider a 2 × 2 unitary matrix U such that one of U’s
columns is in R2. Is U orthogonal?
Exercise 8.5.28. Suppose W is a complex subspace of Cn. Show that the
projection PW is Hermitian.
Exercise 8.5.29. Show how to alter the Killing form to deﬁne a Hermitian
inner product on Cn×n.
Exercise 8.5.30. Verify that the basis in Example 8.15 is indeed an
orthonormal basis of Rn×n.
Exercise 8.5.31. Find a one-to-one correspondence between the set of all
isometries Φ : R2 →R2 and O(2, R).
Exercise 8.5.32. Give a proof of the real principal axis theorem for a self-
adjoint T : V →V by reducing it to the case of a symmetric n × n matrix by
choosing an isometry Φ : V →Rn (n = dim V ).
Exercise 8.5.33. Let V be a ﬁnite-dimensional inner product space, and
suppose T : V →V is linear. Deﬁne the adjoint of T to be the map T ∗: V →
V determined by the condition that
(T ∗(x), y) = (x, T(y))
for all x, y ∈V .
(i) Show that the adjoint T ∗is a well-deﬁned linear mapping.
(ii) If V = Rn, ﬁnd the matrix of T ∗.

8.6
The Group of Rotations of R3 and the Platonic Solids
283
8.6
The Group of Rotations of R3 and the Platonic
Solids
The purpose of this section is give an application of eigentheory to group
theory. We will show that the set of rotations of R3 is the matrix group
SO(3, R) of 3 × 3 orthogonal matrices of determinant one. After that, we
will describe the Platonic solids and study their rotations.
8.6.1
Rotations of R3
The classical deﬁnition of a rotation of R3 originated with Euler. A mapping
ρ : R3 →R3 that ﬁxes every point on a line through the origin, called the
axis of ρ, and rotates every plane orthogonal to the axis through the same
ﬁxed angle θ is called a rotation of R3. We will let Rot(R3) denote the set of
all rotations of R3. Notice that it is not at all clear that Rot(R3) is a group.
It must be shown that the composition of two rotations with diﬀerent axes
is also a rotation.
Our ﬁrst objective is to show that rotations are linear. We will then show
that the matrix of a rotation is orthogonal and has determinant one. Thus,
Rot(R3) ⊂SO(3, R). It is clear from the deﬁnition that a rotation preserves
lengths and angles. Since the inner product on R3 has the property that
x · y = |x||y| cos α
for all nonzero x, y ∈R3, α being the angle between x and y, it follows that
every transformation of R3 preserving both lengths and angles also preserves
dot products. Thus if ρ ∈Rot(R3), then
ρ(x) · ρ(y) = x · y.
(8.19)
Therefore every rotation is orthogonal. Hence, by Proposition 7.7, we have
at once the following result.
Proposition 8.29. Every rotation ρ of R3 is an orthogonal linear mapping.
Consequently, the matrix of ρ with respect to the standard orthonormal basis
of R3 is orthogonal.
We will henceforth identify a rotation with its matrix with respect to the
standard orthonormal basis, so that Rot(R3) ⊂O(3, R). We will now classify
the elements of O(3, R) that are rotations.
Claim: every rotation ρ of R3 has determinant one. Indeed, a rotation ρ ﬁxes a
line L through the origin pointwise, so ρ has eigenvalue 1. Moreover, the plane

284
8
Eigentheory
orthogonal to L is rotated through an angle θ, so there exists an orthonormal
basis of R3 for which the matrix of ρ has the form
⎛
⎝
1
0
0
0
cos θ
−sin θ
0
sin θ
cos θ
⎞
⎠.
Hence det(ρ) = 1, which gives the claim. Therefore, Rot(R3) ⊂SO(3, R). We
will now prove a theorem.
Theorem 8.30. Rot(R3) = SO(3, R).
Proof. It remains to show that SO(3, R) ⊂Rot(R3), i.e., that every element
of SO(3, R) is a rotation. Note that by our deﬁnition, the identity mapping I3
is a rotation. Namely, I3 is the rotation that ﬁxes every line L through 0 and
rotates every plane parallel to L⊥through zero degrees. Let σ ∈SO(3, R).
I claim that 1 is an eigenvalue of σ, and moreover, if σ ̸= I3, the eigenspace
E1 of 1 is a line. To see this, we have to characterize σ’s eigenvalues. Since
complex eigenvalues occur in conjugate pairs, every 3 × 3 real matrix has
a real eigenvalue; and since the real eigenvalues of an orthogonal matrix
are either 1 or −1, the eigenvalues of σ are given by one of the following
possibilities (recall that det(σ) = 1):
(i) 1 of multiplicity three,
(ii) 1, −1, where −1 has multiplicity two, and
(iii) 1, λ, λ, where |λ| = 1 and λ ̸= λ (since the complex roots of the
characteristic polynomial of a real matrix occur in conjugate pairs).
In every case, 1 is an eigenvalue of σ, so dim E1(σ) ≥1. Suppose σ ̸= I3 but
dim E1(σ) > 1. Then dim E1(σ) = 3 is impossible, so dim E1(σ) = 2. Thus σ
ﬁxes the plane E1(σ) pointwise. Since σ preserves angles, it also has to send
the line L = E1(σ)⊥to itself. Thus L is an eigenspace. Since σ ̸= I3, the only
possible eigenvalue for σ on L is −1. In this case, R3 has a basis, so that the
matrix of σ is
⎛
⎝
1
0
0
0
1
0
0
0
−1
⎞
⎠,
contradicting the fact that det(σ) = 1. Thus, if σ ̸= I3, dim E1 = 1. Therefore,
σ ﬁxes every point on a unique line L through the origin and maps the plane
L⊥orthogonal to L into itself. For if u ∈L and u · v = 0, then
u · v = σ(u) · σ(v) = u · σ(v) = 0.
Thus if v ∈L⊥, then we must have σ(v) ∈L⊥as well. It remains to show
that σ rotates L⊥. Let u1, u2, u3 be an orthonormal basis in R3 such that

8.6
The Group of Rotations of R3 and the Platonic Solids
285
u1, u2 ∈L⊥and u3 ∈L, i.e., σ(u3) = u3. Since σu1 and σu2 are orthogonal
unit vectors on L⊥, we can choose an angle θ such that
σu1 = cos θu1 + sin θu2
and
σu2 = ±(sin θu1 −cos θu2).
In matrix terms, this says that if Q = (u1 u2 u3), then
σQ = Q
⎛
⎝
cos θ
± sin θ
0
sin θ
±(−cos θ)
0
0
0
1
⎞
⎠.
Since det(σ) = 1 and det(Q) ̸= 0, it follows that
det
⎛
⎝
cos θ
± sin θ
0
sin θ
±(−cos θ)
0
0
0
1
⎞
⎠= 1.
The only possibility is that
σQ = Q
⎛
⎝
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎞
⎠.
(8.20)
Thus σ rotates the plane L⊥through θ, so it follows that σ ∈Rot(R3). This
proves that SO(3, R) = Rot(R3).
□
The fact that Rot(R3) = SO(3, R) gives an interesting corollary.
Corollary 8.31. Rot(R3) is a matrix group. Hence, the composition of two
rotations of R3 is another rotation.
Remark. The fact that the composition of two rotations is a rotation is
certainly not obvious from the deﬁnition of a rotation. This result is due to
Euler. (Thus it may be said that Euler proved that Rot(R3) is a group before
groups were deﬁned. This is the second brush with groups associated with
Euler, the ﬁrst being the group Um of multiplicative units in Zm.) The axis of
the product of two rotations was found using what are called the Euler angles
of the rotation. The most eﬃcient way of ﬁnding the axis of the product of
two rotations is to represent each rotation as a unit quaternion, say q1 and
q2. Then the axis is read oﬀfrom the quaternionic product q = q1q2. The
reader is referred to any article on unit quaternions for further details.
Notice that the orthogonal matrix Q deﬁned above may be chosen to be
an element of SO(3, R). Therefore, the above argument gives another result.

286
8
Eigentheory
Proposition 8.32. Given σ∈SO(3, R), there exists an element Q∈SO(3, R)
such that
σ = Q
⎛
⎝
cos θ
−sin θ
0
sin θ
cos θ
0
0
0
1
⎞
⎠Q−1.
8.6.2
The Platonic solids
In order to apply our result about SO(3, R), we will ﬁrst deﬁne the Pla-
tonic solids and then determine their rotation groups. A half-space H in
R3 consists of all points lying on or on one side of a plane P in R3. If P
has the equation ax + by + cz = d, then there are two half-spaces, P+ and
P−, which are deﬁned respectively by the inequalities ax + by + cz ≥d and
ax + by + cz ≤d. A set R in R3 is called bounded if there exists M > 0 such
that |r| < M for all r ∈R. It is convex if for every two points x, y in R, the
straight line segment between x and y is also in R. The intersection of a
collection of half-spaces in R3 is always convex.
Deﬁnition 8.4. A convex polyhedron P in R3 is by deﬁnition a bounded
region in R3 that is the intersection of a ﬁnite number of half-spaces in R3.
It turns out that we can require that the half-spaces H1, . . . , Hk that deﬁne
P have the property that each Fi = P ∩Hi is a polygon in R3. These polygons
are the faces of P. Two distinct faces Fi and Fj are either disjoint, meet in
a common vertex, or meet along an edge common to both. The vertices and
edges of all the Fi constitute the sets vertices and edges of P. The boundary of
P is the union of all its faces Fi. For example, a cube is a convex polyhedron
whose boundary is made up of 6 faces, 12 edges, and 8 vertices. Notice that
the faces of a cube are squares of side 1. Hence they are regular polygons of
type {4} in the notation of Section 7.3.7. To give an idea of the startlingly
original ideas of Euler, yet another of his famous (and surprising) theorems
states that for every convex polyhedron in R3 with V vertices, E edges,
and F faces, F −E + V = 2. This number, which can be also be deﬁned for
the boundaries of arbitrary piecewise linear solids in R3, is called the Euler
characteristic. For example, for the surface of a piecewise linear doughnut in
R3, F −E + V = 0.
A convex polyhedron P with the property that all faces of P are congruent
regular polygons and every vertex is on the same number of faces is called a
Platonic solid. Up to position in R3 and volume, there are exactly ﬁve Platonic
solids: the tetrahedron, the cube, the octahedron, the dodecahedron, and the
icosahedron (or soccer ball). The Platonic solids were known to the classical
Greek mathematician-philosophers. Plato famously attempted to associate
four of them with the classical elements (earth, air, ﬁre, and water), and
later Kepler attempted to improve on Plato by associating them with the

8.6
The Group of Rotations of R3 and the Platonic Solids
287
known planets. Euclid proved in the Elements that the Platonic solids fall
into the ﬁve classes of convex polyhedra mentioned just above.
Not surprisingly, all convex polyhedra are determined by their vertices.
Hence the Platonic solids can be described by giving coordinates for their
vertices. All Platonic solids have Euler characteristic 2. The most famil-
iar one is the cube C, which has 8 vertices (±1, ±1, ±1), 12 edges, and
6 faces. (Note that F −E + V = 2.) A tetrahedron has four vertices and
four faces, which are equilateral triangles. Since F + V = 2 + E, it has six
edges. A convenient tetrahedron that we will call T has cubical vertices
(1, 1, 1), (1, −1, −1), (−1, 1, −1), (−1, −1, 1). The octahedron O has 6 ver-
tices, which we will take to be the midpoints of the faces of C, namely
(±1, 0, 0), (0, ±1, 0), (0, 0, ±1). It has 8 triangular faces, hence 12 edges. The
icosahedron I has 12 vertices (0, ±1, ±φ), (±1, ±φ, 0), (±φ, 0, ±1), 20 faces,
and 30 edges. Finally, the dodecahedron D has 20 vertices: 8 of which are
vertices of C, (0, ±φ−1, ±φ), (±φ, 0, ±φ−1), (±φ, 0, ±φ−1). Here, φ = 1+
√
5
2
is
the golden mean, which was encountered when we considered the Fibonacci
sequence. Notice that all ﬁve convex polyhedra listed above have the property
that their vertices are equidistant from the origin. Such a convex polyhedron
is said to be central.
8.6.3
The rotation group of a Platonic solid
The set Rot(P) consisting of rotations of R3 that preserve a convex polyhe-
dron P is called the rotation group of P.
Proposition 8.33. Suppose P is a convex polyhedron whose vertices span
R3. Then Rot(P) is a ﬁnite subgroup of SO(3, R).
Proof. A convex polyhedron P is uniquely determined by its vertices, and
every linear mapping of R3 sends each convex polyhedron P to another convex
polyhedron. Thus if a linear mapping sends P into itself, it has to preserve
the faces, edges and vertices of P. Since a rotation of R3 is linear and since
the vertices of P span R3, two rotations that coincide on the vertices are
the same. Since the vertex set is ﬁnite, it follows that Rot(P) has to be
ﬁnite too.
□
The ﬁnite subgroups of SO(3, R) are called the polyhedral groups. Thus,
Rot(P) is always a polyhedral group. Conversely, every polyhedral group is
the rotation group of a convex polyhedron. We will eventually classify all the
polyhedral groups. When P is a central Platonic solid, there is a beautiful
formula for the order |Rot(P)|:
Proposition 8.34. Suppose P is a central Platonic solid with f faces such
that each face has e edges. Then |Rot(P)| = ef.

288
8
Eigentheory
The proof will be given in Proposition 11.7 as an application of the orbit
stabilizer theorem. Thus the rotation group of a central cube C or octahedron
O has order 24. As we will prove next, both of these groups are isomorphic to
the symmetric group S(4). A central regular tetrahedron has four triangular
faces, so its rotation group has order 12. A central regular dodecahedron D
and icosahedron I both have rotation groups of order 60. In fact, Rot(I) =
Rot(D) ∼= A(5).
8.6.4
The cube and the octahedron
Since the rotation group of a central cube C has order 24, one might suspect
that it is isomorphic to S(4). We will verify this, but ﬁrst let us give an
explicit description of Rot(C). For convenience, suppose the six vertices of
C are (±1, ±1, ±1). Since all rotations of C send faces to faces, they also
permute the six midpoints of the faces. The midpoints being ±e1, ±e2, and
±e3, the orthogonal matrices that permute these vectors have the form
σ = (±eπ(1) ± eπ(2) ± eπ(3)),
where π ∈S(3). The matrices of this form are called signed permutation
matrices. They form the subgroup SP(3) of O(3, R). Note that |SP(3)| = 48.
Observe that if σ ∈SP(3), then
det σ = det (±eπ(1) ± eπ(2) ± eπ(3)) = (−1)rsgn(π),
where r is the number of −1’s. This implies that |SP(3) ∩SO(3, R)| = 24.
It is evident that Rot(C) = SP(3) ∩SO(3, R), so we get another proof that
|Rot(C)| = 24. For simplicity, let G denote Rot(C). To see that G ∼= S(4), let
D1, D2, D3, D4 denote the four diagonals of C. Since G leaves C invariant, it
follows that each element of G permutes the four diagonals. Thus if σ ∈G,
then σ(Di) = Dπ(i) for a unique π ∈S(4). We will show that the mapping
ϕ : G →S(4) given by ϕ(σ) = π is an isomorphism. The proof that ϕ is a
homomorphism is left to the reader. To show that ϕ is a bijection, it suf-
ﬁces, by the pigeonhole principle, to show that ϕ is injective, since G and
S(4) have the same order. Consider the four vectors d1 = e1 + e2 + e3, d2 =
−e1 + e2 + e3, d3 = e1 −e2 + e3, and d4 = −e1 −e2 + e3, which lie on the
four diagonals of C and point upward. Let Di be the diagonal determined
by di. Then for each i, σ(di) = ϵidπ(i), where each ϵi is either 1 or −1
depending on whether σ(di) points up or down. Since {d1, d2, d3} and
{ϵ1dπ(1), ϵ2dπ(2), ϵ3dπ(3)} are both bases of R3 for every choice of signs and
each σ ∈Rot(C) is a linear mapping, it follows that σ is uniquely determined
by the matrix identity

8.6
The Group of Rotations of R3 and the Platonic Solids
289
σ
d1 d2 d3

=
σ(d1) σ(d2) σ(d3)
=
ϵ1dπ(1) ϵ2dπ(2) ϵ3dπ(3)

.
Now assume ϕ(σ) = (1). Then
σ

d1 d2 d3

=

ϵ1d1 ϵ2d2 ϵ3d3

=

d1 d2 d3

diag(ϵ1, ϵ2, ϵ3).
Hence, σ = DED−1, where D =

d1 d2 d3

and E = diag(ϵ1, ϵ2, ϵ3). In order
to be in G, σ must be orthogonal, with det σ = 1. In particular, σ−1 = σT
and det E = 1. Since E−1 = E, it follows that σ = σ−1; hence to be orthog-
onal, σ must be symmetric. This implies that DED−1 = (DT )−1EDT , or
equivalently, DT DE = EDT D. By direct calculation,
DT D =
⎛
⎝
3
1
1
1
3
−1
1
−1
3
⎞
⎠,
so the only way DT DE = EDT D can occur with E = diag(±1, ±1, ±1) and
det E = 1 is if E = I3, for at least one entry of E, say ϵi, must be 1. If the
other diagonal entries are −1, then EDT D ̸= DT DE. For example, when ϵ1 =
ϵ3 = −1 and ϵ2 = 1, then the ﬁrst column of DT DE is
⎛
⎝
−3
−1
−1
⎞
⎠, while that of
EDT D is
⎛
⎝
−3
1
−1
⎞
⎠. We conclude that σ = I3, and consequently, ϕ is injective.
This proves that ϕ is an isomorphism, so Rot(C) ∼= S(4).
□
The octahedron O can be viewed as the unique convex polyhedron whose
vertices are the midpoints of the faces of the cube C, and every rotation of C
is thus a rotation of the octahedron and conversely. Thus Rot(C) = Rot(O).
Perhaps a more enlightening way to realize Rot(C) is to describe the rota-
tions directly by ﬁnding the axes about which C can be rotated. For example,
every coordinate axis Rei is such a line; C can be rotated by π/2, π, and
3π/2 about each coordinate axis. This gives a total of nine distinct rotations.
One can also rotate C through π around the lines x = y and x = −y in the
xy-plane. Repeating this for the other two coordinate planes gives six more
rotations, so we have now accounted for 15 rotations, 16 including the iden-
tity. There are four more lines of symmetry about which one might be able to
rotate C, namely the four diagonals joining opposite vertices. It seems to be
a little harder to visualize whether there any rotations through these lines,
so let us take a slightly diﬀerent approach. Recall that the alternating group
A(3) can be realized as the 3 × 3 permutation matrices of determinant 1. One
element σ ∈A(3) is the rotation

290
8
Eigentheory
σ =
⎛
⎝
0
0
1
1
0
0
0
1
0
⎞
⎠
sending e1 →e2 →e3 →e1. Clearly,
⎛
⎝
1
1
1
⎞
⎠is an eigenvector, so ϕ is in fact
a rotation about the line R
⎛
⎝
1
1
1
⎞
⎠. Since σ clearly has order 3, it is the rota-
tion through 2π/3. In this way, we account for another eight elements of
Rot(C), since there are four diagonals, so we now have constructed all 24
rotations of C.
8.6.5
Symmetry groups
The geometric side of group theory is symmetry. To understand this aspect,
suppose S is a subset of Rn. A symmetry of S is deﬁned to be an orthogonal
linear mapping ϕ : Rn →Rn such that ϕ(S) = S. The set of all symmetries
of S will be denoted by O(S). Recall that an orthogonal linear mapping is
a linear map ϕ that preserves the inner product on Rn. Since ϕ must also
preserve lengths, distances, and angles, it preserves the geometry of S. We
know that an orthogonal linear mapping ϕ : Rn →Rn is an orthogonal n × n
matrix, so the set of symmetries of S is thus
O(S) = {ϕ ∈O(n, R) | ϕ(S) = S}.
In particular, O(Rn) = O(n, R). The ﬁrst thing to note is the following.
Proposition 8.35. For every S ⊂Rn, the set of symmetries O(S) is a sub-
group of O(n, R).
Proof. If ϕ and ψ are elements of O(S), then ψϕ and ϕ−1 are also symmetries
of S. Hence ψϕ−1 ∈O(S), so O(S) is indeed a subgroup.
□
Of course, it is possible, in fact likely, that the only element of O(S) is In.
For example, let n = 2 and let S = {(1, 0), (2, 0), (3, 0)}. On the other hand,
if S is the unit circle x2 + y2 = 1 in R2, then O(S) = O(2, R). When S is
a convex polyhedron in R3 (or even in Rn), then O(S) has to permute the
vertices of S. But since elements of O(S) preserve lengths, O(S) can move a
vertex only into another vertex having the same distance from the origin. But
if S is contained in Rn and has n linearly independent vertices of diﬀerent
lengths, then O(S) is the trivial group.

8.6
The Group of Rotations of R3 and the Platonic Solids
291
Example 8.17. Let us consider the n-cube C(n) in Rn deﬁned by
C(n) = {(x1, x2, . . . , xn) | −1 ≤xi ≤1 for all i = 1, 2, . . . , n}.
The matrix group SP(n) consisting of all n × n matrices of the form
(±eπ(1) ± eπ(2) · · · ± eπ(n)),
where π ∈S(n), permutes the vectors ±e1, ±e2, . . . , ±en and hence sends
C(n) to C(n). In fact, SP(n) = O(C(n)).
⊠
Rotations in Rn for n > 3 are harder to deﬁne. We know that every rotation
of R3 is given by an element of SO(3, R). It can be shown by a similar
analysis that every element of SO(4, R) is given by a matrix that is a rotation
in two orthogonal planes in R4. In general, elements of O(n, R) satisfy the
normal matrix criterion AT A = AAT . This implies, by the normal matrix
theorem (see Theorem 9.2), that A is unitarily diagonalizable. That is, for
every A ∈O(n, R), there is a Hermitian orthonormal basis of Cn consisting
of eigenvectors of A. Then, associated to every pair of eigenvalues λ, λ of A,
there exists a two-plane V in Rn such that A is a rotation of V through eiλ.
(Recall that since A is orthogonal, its eigenvalues satisfy |λ| = 1.)
Remark. The study of symmetry via group theory has been successful in
several disciplines, e.g., chemistry, physics, and materials science. The symme-
tries of a class of pure carbon molecules called fullerenes oﬀer a prime exam-
ple. The most widely known of the fullerenes is a carbon molecule denoted
by C60 (not to be confused with the cyclic group of order 60), named buck-
minsterfullerene (or buckyball for short) after Buckminster Fuller for its sim-
ilarity to his famous geodesic domes, which is one of the most rigid molecules
ever discovered. Buckminsterfullerene is a truncated icosahedron. To obtain
a picture of the buckyball, we have to consider the truncated icosahedron,
which is the solid obtained by cutting oﬀeach vertex of the icosahedron by a
plane orthogonal to the line through the vertex and the center of the icosa-
hedron at the same distance from the center for each vertex. To get a model
for the truncated icosahedron, just take a close look at a soccer ball. Since
every vertex of the icosahedron lies on ﬁve faces, the vertices are replaced
by 12 regular pentagons. Hence, the truncated icosahedron has 60 vertices.
It also has 32 faces. The symmetry group of the vertices of the truncated
icosahedron is the same as for the icosahedron.
Exercises
Exercise 8.6.1. Prove the following: Suppose G is a subgroup of O(3, R)
and let
G+ = G ∩SO(3, R).

292
8
Eigentheory
Then either G+ = G or G has exactly two cosets, and |G| = 2|G+|.
Exercise 8.6.2. Let P be a central Platonic solid. Let O(P) be the set of
all orthogonal matrices preserving P.
(i) Verify that O(P) is a subgroup of O(3, R).
(ii) Show that |O(P)| = 2|Rot(P)|.
Exercise 8.6.3. Consider the cube C with vertices at (±1, ±1, ±1). For the
rotations of C through the diagonal along (1, 1, 1), where do the vertices go?
Even though this was worked out in the text, try to do it anyway without
looking back.
Exercise 8.6.4. Let H denote the reﬂection through the xy-plane in R3.
Show how to express the reﬂection through the yz-plane in the form Hσ,
where σ is a rotation.
Exercise 8.6.5. The symmetry group of the central 3-cube C of the previous
exercise permutes the diagonals of C, but it has order 48, which is twice the
order of S(4). Describe all the nonrotational symmetries.
Exercise 8.6.6. Show that
SP(n) = {σ ∈O(n, R) | σ = (±eπ(1) ± eπ(2) · · · ± eπ(n)), π ∈S(n)}
is a subgroup of O(n, R) of order 2nn!. Elements of SP(n) are called signed
permutation matrices.
Exercise 8.6.7. Consider the n-cube C(n) with its 2n vertices at the points
(±1, ±1, . . . , ±1). Show that the symmetry group of the set of vertices of
C(n) is the group SP(n). Does this imply that the symmetry group of C(n)
is SP(n)?
Exercise 8.6.8. The 4-cube C(4) has eight diagonals. They are represented
by the semidiagonals ±e1 + ±e2 + ±e3 + e4. Show that the symmetry group
of C(4) does not act transitively on the diagonals. Find a pair of diagonals
D1 and D2 such that no σ ∈Sym(C(4)) satisﬁes σ(D1) = D2.
Exercise 8.6.9. Let T be the central regular tetrahedron in R3 described
above. Show that the group O(T ) is isomorphic to S(4) and that Rot(T ) is
isomorphic to A(4).
Exercise 8.6.10. Construct a convex polyhedron P by taking two copies of
a tetrahedron, say T1 and T2, and gluing them together along two faces. The
result is a convex polyhedron with six faces, nine edges, and ﬁve vertices.
Assuming that P is central, compute the order of O(P).

8.6
The Group of Rotations of R3 and the Platonic Solids
293
Exercise 8.6.11. For a subset S of R3, let Rot(S) denote the group of all
σ ∈SO(3, R) such that σ(S) = S. Find Rot(S) in the following cases:
(a) S is the half-ball {x2 + y2 + z2 ≤1, z ≥0},
(b) S is the solid rectangle {−1 ≤x ≤1, −2 ≤y ≤2, −1 ≤z ≤1}.
Exercise 8.6.12. Suppose H is a reﬂection of R2. Show that there is a
rotation ρ of R3 such that ρ(x) = H(x) for all x ∈R2. (Hint: consider the
line through which H reﬂects R2.)
Exercise 8.6.13. Let G be a subgroup of O(n, R). True or false: if G is not
contained in SO(n, R), then G is normal in O(n, R).
Exercise 8.6.14. Describe how the alternating group A(4) acts on the cube
in R3 with vertices (±1, ±1, ±1).

294
8
Eigentheory
8.7
An Appendix on Field Extensions
The purpose of this Appendix is to prove that given a ﬁeld F and a polynomial
f ∈F[x], there exists a ﬁeld F′ containing both F and all the roots of f(x) = 0.
The ﬁeld F′ is called a splitting ﬁeld for the polynomial f.
To begin, we will construct a ﬁeld F′ containing F and at least one root
of f. Let x be a variable, and put V = F[x]. Then V is a vector space over F
with an inﬁnite basis 1, x, x2, . . . . Given a nonconstant polynomial f ∈V ,
let W be the subspace of V consisting of all polynomials of the form h = gf,
for some g ∈F[x]. We leave it to the reader to check that W is indeed a
subspace of V . Now form the quotient vector space V/W. Recall that the
elements of V/W are cosets g + W, where g ∈V , and that coset addition is
given by (g + W) + (h + W) = (g + h) + W. Scalar multiplication by a ∈F
is given in an analogous way: a(g + W) = ag + W. Recall also that two cosets
g + W and h + W are the same if and only if h −g ∈W. That is, h −g = qf
for some q ∈F[x]. Since both V and W are inﬁnite-dimensional, the following
result may at ﬁrst glance be surprising.
Proposition 8.36. The quotient vector space V/W is a ﬁnite-dimensional
vector space over F. In fact, dim V/W = deg(f).
Proof. To prove that dim V/W= deg(f), we exhibit a basis. Let k= deg(f)−1.
Given g ∈F[x], put g = g + W. For convenience, let us set α = x and αi = xi
for each nonnegative integer i. We claim that 1, α, α2, . . . , αk is a basis of
V/W. We ﬁrst show independence. Suppose there exist a0, a1, . . . , ak ∈F such
that
k

i=0
axαi = 0.
By deﬁnition, this means that
h(x) =
k

i=0
aixi ∈W.
Thus h = gf for some g ∈F[x]. This is impossible unless g = 0, so all ai are
equal to zero. To show that 1, α, . . . , αk span, let g ∈F[x] and apply division
with remainder to write g = qf + r, where q, r ∈F[x] and deg(r) < deg(f).
Then g = r. But r is in the span of 1, α, . . . , αk, so we have found a basis of
V/W. Thus dim V/W = deg(f), which ﬁnishes the proof.
□
An element f ∈F[x] is said to be irreducible if there is no factorization
f = gh in which both g and h lie in F[x] and both g and h have positive
degree. The next theorem gives an important and fundamental result in ﬁeld
theory.

8.7
An Appendix on Field Extensions
295
Theorem 8.37. If f ∈F[x] is irreducible, then V/W can be given the struc-
ture of a ﬁeld F′ such that F is a subﬁeld of F′. Moreover, α = x + W is a
root of f in F′.
Proof. We must ﬁrst deﬁne multiplication on V/W. Let g, h ∈F[x], and put
gh = gh. To prove that this deﬁnition makes sense, it is necessary to show that
if g1 = g2 and h1 = h2, then g1h1 = g2h2. This is analogous to the proof that
Zm admits an associative and commutative multiplication given in Chap. 2,
so we will omit the details. Note that 0 is the additive identity, and 1 is the
multiplicative identity. It remains to prove that if g ̸= 0, then g−1 exists. That
is, there exists h ∈F[x] such that hg = 1. Since f is irreducible and g ̸= 0, f
by deﬁnition doesn’t divide g. Therefore, f and g have no common factor of
positive degree. This means there exist polynomials a, b ∈F[x] such that af +
bg = 1, by the algorithm for computing the greatest common divisor of two
polynomials. Consequently, in F′, bg = 1. Hence V/W with this multiplication
is a ﬁeld. We now have F′. Note that F is contained in F′ as the subﬁeld
{r | r ∈F ⊂F[x]}. Finally, we must show that f(α) = 0. Let f(x) =  cixi.
Now,
f(α) =

ciαi =

cixi = f = 0,
so α is indeed a root. Therefore, the proof is ﬁnished.
□
Example 8.18. Let F = Q and notice that x2 + x −1 is irreducible in Q[x].
Its roots φ and μ were considered in Section 8.2.5. In particular, φ = 1+
√
5
2
.
The ﬁeld Q′ has dimension two over Q. A vector space basis of Q′ over Q is
1,
√
5.
⊠
Finally, we need to modify the above construction to get a splitting ﬁeld
F′ for f. Notice that f ∈F[x] ⊂F′[x], but f is no longer irreducible in F′[x],
since it has the root α = x in F′. Choose a new variable, say z, and consider
F′[z]. Dividing f(z) by (z −α) gives f(z) = g(z)(z −α), for some g ∈F′[z].
Now deg(g) = deg(f) −1, so if deg(f) = 2, then Proposition 8.36 implies that
F′ necessarily contains all roots of f, as in the above example. If deg(f) > 2,
we check whether g is irreducible in F′[z]. If so, we repeat the construction
with g to obtain a ﬁeld extension of F′ (and hence F) containing a root β of g
diﬀerent from α. Of course, β is also root of f. If g isn’t irreducible, factor it
until another irreducible factor is found and then perform another extension.
By continuing in this manner, one eventually obtains a ﬁeld extension of F
containing all roots of the original polynomial f. In fact, this process will
stop after at most deg(f) steps, since a polynomial of degree m has at most
m distinct roots.
□

Chapter 9
Unitary Diagonalization
and Quadratic Forms
As we saw in Chap. 8, when V is a ﬁnite-dimensional vector space over F, then
a linear mapping T : V →V is semisimple if and only if its eigenvalues lie in F
and its minimal polynomial has only simple roots. It would be useful to have
a result that would allow one to predict that T is semisimple on the basis of a
criterion that is simpler than ﬁnding the minimal polynomial, which, after all,
requires knowing the roots of the characteristic polynomial. In fact, we also
proved that when F = C or R, every self-adjoint operator is semisimple and
even admits a Hermitian orthonormal basis. The matrices associated to self-
adjoint operators, that is, Hermitian and symmetric matrices respectively,
happen to be in a larger class of matrices said to be normal consisting of
all A ∈Cn×n such that AAH = AHA. The normal matrix theorem asserts
that the normal matrices are exactly those A ∈Cn×n that can be unitarily
diagonalized, or equivalently, admit a Hermitian orthonormal basis. The ﬁrst
goal in this chapter is to prove this theorem. The second goal is to consider the
topic of quadratic forms to which our diagonalization results may be applied.
For example, we will classify the positive deﬁnite real quadratic forms or
equivalently, the positive deﬁnite symmetric matrices. We will also introduce
an equivalence relation on the set of all real quadratic forms called congruence
and classify the equivalence classes. This result is known as Sylvester’s law
of inertia.
9.1
Schur Triangularization and the Normal Matrix
Theorem
The key to understanding which matrices can be unitarily diagonalized is the
Schur triangularization theorem, which says that an arbitrary n × n matrix
over C is similar via a unitary matrix to an upper triangular matrix. Put
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 9
297

298
9
Unitary Diagonalization and Quadratic Forms
another way, if V is a ﬁnite-dimensional vector space over C with a Hermitian
inner product, then every linear mapping T : V →V can be represented in
a Hermitian orthonormal basis by an upper triangular matrix. One of the
interesting aspects of the proof of the Schur triangularization theorem is that
it uses the fact that the unitary group U(n) is closed under multiplication.
9.1.1
Upper triangularization via the unitary
group
We will now prove the Schur triangularization theorem.
Theorem 9.1 (Schur triangularization theorem). Suppose A ∈Cn×n. Then
there exist a unitary matrix U and an upper triangular matrix T ∈Cn×n such
that A = UTU H. Thus, every square matrix over C is unitarily similar to an
upper triangular matrix over C. If A is real and all its eigenvalues are also
real, then A is similar to an upper triangular matrix over R via an orthogonal
matrix.
Proof. We will induct on n. Since the result is trivial if n = 1, suppose n > 1
and that the proposition is true for all k × k matrices over C whenever k <
n. Since A’s eigenvalues lie in C, A has an eigenpair (λ1, u1) with λ1 ∈C
and u1 ∈Cn. Let W be the subspace (Cu1)⊥. By the Hermitian version of
Proposition 6.38, dim W = n −1. Hence by the above discussion, there exists
a Hermitian orthonormal basis {u2, . . . , un} of W. Adjoining u1 gives the
Hermitian orthonormal basis u1, u2, . . . , un of Cn. Thus U1 = (u1 u2 . . . un)
is unitary, and since (λ1, u1) is an eigenpair for A, we have
AU1 = (Au1 Au2 · · · Aun) = (λ1u1 Au2 · · · Aun).
Hence
U H
1 AU1 =
⎛
⎜
⎜
⎜
⎝
uH
1
uH
2
...
uH
n
⎞
⎟
⎟
⎟
⎠(λ1u1 Au2 · · · Aun) =
⎛
⎜
⎜
⎜
⎝
λ1uH
1 u1
∗
· · ·
∗
λ1uH
2 u1
∗
· · ·
∗
...
...
...
λ1uH
n u1
∗
· · ·
∗
⎞
⎟
⎟
⎟
⎠.
Therefore,
U H
1 AU1 =
⎛
⎜
⎜
⎜
⎝
λ1
∗
· · ·
∗
0
∗
· · ·
∗
...
...
...
0
∗
· · ·
∗
⎞
⎟
⎟
⎟
⎠.
(9.1)

9.1
Schur Triangularization and the Normal Matrix Theorem
299
Now apply the induction hypothesis to the (n −1) × (n −1) matrix B in the
lower right-hand corner of U H
1 AU1 to get an (n −1) × (n −1) unitary matrix
U ′ such that (U ′)HBU ′ is upper triangular. The matrix
U2 =
⎛
⎜
⎜
⎜
⎝
1
0
· · ·
0
0
...
U ′
0
⎞
⎟
⎟
⎟
⎠
is clearly unitary, and
U H
2 (U H
1 AU1)U2 =
⎛
⎜
⎜
⎜
⎝
1
0
· · ·
0
0
...
(U ′)H
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
λ1
∗
· · ·
∗
0
...
B
0
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
1
0
· · ·
0
0
...
U ′
0
⎞
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎝
λ1
∗
· · ·
∗
0
...
(U ′)HBU ′
0
⎞
⎟
⎟
⎟
⎠,
so U H
2 (U H
1 AU1)U2 is upper triangular. We are therefore done, since U = U1U2
is unitary due to the fact that U(n) is a matrix group (see Proposition 6.40).
If A and its eigenvalues are all real, then there exists a real eigenpair (λ1, u1);
hence U1 can be chosen to be orthogonal. The rest of the argument is the
same with orthogonal matrices replacing unitary matrices.
⊓⊔
9.1.2
The normal matrix theorem
We now determine when the upper triangular matrix T in Proposition 9.1
is actually diagonal. In other words, we classify those A ∈Cn×n such that
A = UDU H for some unitary matrix U, where D = diag(λ1, . . . , λn). To do
so, we make the following deﬁnition.
Deﬁnition 9.1. A matrix N ∈Cn×n is said to be normal if
NN H = N HN.
(9.2)
After proving the normal matrix theorem, which is next, we will give a
number of interesting examples of normal matrices. They fall into classes
depending on what conditions their eigenvalues satisfy.

300
9
Unitary Diagonalization and Quadratic Forms
Theorem 9.2 (The normal matrix theorem). A matrix A ∈Cn×n is unitar-
ily diagonalizable if and only if A is normal.
Proof. The “only if” part is straightforward and left as an exercise. Suppose
A is normal. Write A = UTU H, where U is unitary and T is upper triangular.
Since AHA = AAH and U is unitary, it follows that TT H = T HT. Hence it
suﬃces to show that an upper triangular normal matrix is diagonal. The key
is to compare the diagonal entries of T HT and TT H. The point is that for
every n × n complex matrix B, the kth diagonal entry of BHB is the square
of the length of the kth column of B, while the kth diagonal entry of BBH is
the square of the length of the kth row. But in an upper triangular matrix B,
if the square of the length of the kth row equals the square of the length of
the kth column for all k, then B is diagonal (the proof is left to the reader).
Thus an upper triangular normal matrix is in fact diagonal. Therefore, A is
indeed unitarily diagonalizable.
⊓⊔
Corollary 9.3. If A ∈Cn×n is normal, then two eigenvectors of A with dis-
tinct eigenvalues are Hermitian orthogonal.
Proof. We leave this as an exercise.
⊓⊔
9.1.3
The Principal axis theorem: the short proof
Recall that a matrix A ∈Cn×n is Hermitian if and only if AH = A. A real
Hermitian matrix is, of course, symmetric. It is clear that every Hermitian
matrix is normal. Furthermore, using the normal matrix theorem, we may
easily prove the following result.
Proposition 9.4. The eigenvalues of a Hermitian matrix are real. In fact,
the Hermitian matrices are exactly the normal matrices having real eigenval-
ues.
Proof. Let A be Hermitian, and write A = UDU H. Then A = AH if and only
if U HAU = U HAHU if and only if D = DH if and only if D is real.
⊓⊔
Of course, the fact that Hermitian matrices have real eigenvalues was
proved from ﬁrst principles in Chap. 8. The normal matrix theorem immedi-
ately implies the following.
Theorem 9.5 (Principal axis theorem). A complex matrix A is Hermitian
if and only if A can be unitarily diagonalized as A = UDU H, where D is a
real diagonal matrix. Similarly, a real matrix A is symmetric if and only if
A can be orthogonally diagonalized A = QDQT , where D is also real.

9.1
Schur Triangularization and the Normal Matrix Theorem
301
Proof. The Hermitian case is immediate, but if A is real symmetric, one
needs to argue a little more to show that U can be taken to be orthogonal.
By the Schur triangulation theorem, A = QTQT , where Q is orthogonal and
T is real and upper triangular. But since A is symmetric, it follows that T
is symmetric, and since T is also upper triangular, it has to be diagonal.
Therefore, A = QDQT , as desired.
⊓⊔
This proof is surprisingly brief, but it does not lead to any of the insights
of the ﬁrst version of the principal axis theorem.
9.1.4
Other examples of normal matrices
To obtain other classes of normal matrices, one can impose other conditions
on D in the expression A = UDU H. Here is another example obtained in this
way.
Example 9.1 (Skew-Hermitian matrices). A matrix J is said to be skew-
Hermitian if JH = −J. It is easy to see that J is skew-Hermitian if and only if
iJ is Hermitian. Since Hermitian matrices are normal, so are skew-Hermitian
matrices. Also, the nonzero eigenvalues of a skew-Hermitian matrix are pure
imaginary: they have the form iλ for a nonzero λ ∈R that is an eigenvalue
of iJ. A real skew-Hermitian matrix S is skew-symmetric, i.e., ST = −S. For
example,
J =
 0
1
−1
0
	
and
S =
⎛
⎝
0
1
2
−1
0
2
−2
−2
0
⎞
⎠
are both skew-symmetric. The diagonal entries of a skew-symmetric matrix
are zero, so the trace of a skew-symmetric is also zero. The determinant of a
skew-symmetric matrix of odd order is also 0 (see Exercise 9.1.3 below), so
a skew-symmetric matrix of odd order has 0 as an eigenvalue. The matrix J
above shows that the determinant of a skew-symmetric matrix of even order
can be nonzero. Note that the characteristic polynomial of S is −λ3 −9λ, so
the eigenvalues of S are 0, ±3i, conﬁrming the observation that all nonzero
eigenvalues of a skew-Hermitian matrix are purely imaginary. Moreover, the
nonzero eigenvalues of S occur in conjugate pairs, since S is real.
⊠
The normal matrix theorem gives the following structure theorem, which
describes the nonsingular skew-symmetric matrices.
Proposition 9.6. Assume that n is even, say n = 2m, and A ∈Rn×n is an
invertible skew-symmetric matrix. Then there exists an orthonormal basis
x1, y1, x2, y2, . . . , xm, ym of Rn such that A sends each real two-plane
Rxk + Ryk onto itself, and the matrix of A on this two-plane has the form

302
9
Unitary Diagonalization and Quadratic Forms
Jλk =

0
λk
−λk
0
	
,
where λk is a nonzero real number such that iλk is an eigenvalue of A. (This
basis is not necessarily unique, however.) Thus there exists an orthogonal
matrix Q such that A = Q diag(Jλ1, . . . , Jλm) QT . In particular, a nonsin-
gular real skew-symmetric matrix is similar via the matrix group O(n, R) to
a matrix that is the direct sum of nonzero two-dimensional skew-symmetric
blocks Jλk.
Proof. Since A is real, its eigenvalues occur in conjugate pairs. Moreover,
since A is skew-symmetric, iA is Hermitian, so iA has only real eigenval-
ues. Thus the eigenvalues of A can be sorted into pairs ± iλk, where λk is
a nonzero real number, since det(A) ̸= 0 and k varies from 1 to m. Note,
however, that we are not claiming that the λk are all distinct. We may
choose a Hermitian orthonormal basis of Cn consisting of eigenvectors of A,
and since eigenvectors for diﬀerent eigenvalues are Hermitian orthogonal by
Corollary 9.3, we may choose a Hermitian orthonormal basis of Cn consist-
ing of pairs {uk, uk}, where Auk = iλkuk. We now observe that this means
that xk = (uk + uk)/
√
2 and yk = i(uk −uk)/
√
2 also are a basis over C of
span{uk, uk}. Moreover, xk and yk have the additional property that both
lie in Rn and are Euclidean orthogonal: (xk)T yk = 0. Furthermore, A leaves
the real two-plane Pk = Rxk + Ryk that they span invariant, and the matrix
of A with respect to this basis of Pk is Jλk. Finally, note that the planes Pi
and Pj are also orthogonal if i ̸= j. Hence the matrix Q = (x1 y1 · · · xm ym)
is orthogonal and A = Q diag(Jλ1, . . . , Jλm) QT .
⊓⊔
One can extend the above result to odd skew-symmetric matrices. We will
leave this to the reader.
Another natural condition one can put on the eigenvalues of a normal
matrix is that they all have modulus one. This is investigated in the following
example.
Example 9.2. Let K = UDU H, where every diagonal entry of D is a unit
complex number. Then D is unitary, hence so is K. Conversely, every uni-
tary matrix is normal (see Exercise 9.1.10). Thus the unitary matrices are
exactly the normal matrices such that every eigenvalue has modulus one. For
example, the skew-symmetric matrix
J =

0
−1
1
0
	
is clearly orthogonal, hence unitary. The matrix J has eigenvalues ±i, and
we can easily compute that Ei = C(1, −i)T and E−i = C(1, i)T . Thus
J = U1DU H
1 =
1
√
2
1
1
i
−i
	 −i
0
0
i
	 1
√
2
1
−i
1
i
	
.

9.1
Schur Triangularization and the Normal Matrix Theorem
303
The basis constructed in the above proposition is u1 =
1
√
2

1
i
	
and u′
1 =
1
√
2
 1
−i
	
. The way U acts as a complex linear mapping of C2 can be inter-
preted geometrically as follows: U rotates vectors on the complex line C(1, i)T
spanned by (1, i)T (thought of as a real two-plane) through π
2 and rotates vec-
tors on the orthogonal axis by −π
2. Of course, as a mapping on R2, U is simply
the rotation Rπ/2.
⊠
Exercises
Exercise 9.1.1. Show that A ∈Cn×n is unitarily diagonalizable if and only
if AH is.
Exercise 9.1.2. True or false (discuss your reasoning):
(i) A complex symmetric matrix is normal.
(ii) The real part of a Hermitian matrix is symmetric and the imaginary
part is skew-symmetric.
(iii) The real part of a normal matrix is normal.
(iv) If a normal matrix N has real eigenvalues, then N is Hermitian.
Exercise 9.1.3. Unitarily diagonalize the skew-symmetric matrices
J =
 0
1
−1
0
	
and
S =
⎛
⎝
0
1
2
−1
0
2
−2
−2
0
⎞
⎠.
Exercise 9.1.4. Let
A =
⎛
⎜
⎜
⎝
0
−1
0
−2
1
0
0
0
0
0
0
1
2
0
−1
0
⎞
⎟
⎟
⎠.
Express A as Qdiag(Jλ, Jμ)QT as in Proposition 9.6.
Exercise 9.1.5. Formulate a result similar to Proposition 9.6 for skew-
symmetric matrices A ∈Rn×n, where n is odd, that have the property that
dim N(A) = 1.
Exercise 9.1.6. Let S be a skew-Hermitian n × n matrix. Show the follow-
ing:
(i) If n is odd, then det(S) is pure imaginary (but possibly zero), and if n
is even, then det(S) is real.
(ii) If S is skew-symmetric, then det(S) = 0 if n is odd, and det(S) ≥0 if n
is even.

304
9
Unitary Diagonalization and Quadratic Forms
Exercise 9.1.7. Show from the deﬁnition of the determinant that the deter-
minant of a Hermitian matrix is real.
Exercise 9.1.8. Prove Corollary 9.3. That is, show that two eigenvectors
for diﬀerent eigenvalues of a normal matrix are Hermitian orthogonal.
Exercise 9.1.9. If possible, ﬁnd an example of a 3 × 3 real matrix N such
that N is normal, but N is neither symmetric nor skew-symmetric.
Exercise 9.1.10. Let U ∈U(n). Show the following.
(i) Every eigenvalue of U also has modulus 1.
(ii) det(U) has modulus 1.
(iii) |Tr(U)| ≤n.
Exercise 9.1.11. Suppose Q is an n × n orthogonal matrix with no real
eigenvalues. True or false: n is even and det(Q) = 1.
Exercise 9.1.12. Suppose all eigenvalues of a unitary matrix Q are 1. True
or false: Q = In.
Exercise 9.1.13. Let N(n) denote the set of normal n × n complex matri-
ces. Prove that N(n) is not a subspace of Cn×n.
Exercise 9.1.14. Suppose A and B are commuting normal matrices. Prove
the following:
(i) A + B and AB are also normal, and
(ii) A and B are simultaneously diagonalizable.
Exercise 9.1.15. Suppose, as in Exercise 9.1.14, that A and B are commut-
ing normal matrices. What are the possible eigenvalues of A + B?
Exercise 9.1.16. Formulate the notion of a normal operator on a Hermitian
inner product space from the deﬁnition of a normal matrix.
Exercise 9.1.17. True or false (discuss your reasoning):
(i) If A is normal and U is unitary, then UAU H is normal.
(ii) If A is normal and invertible, then A−1 is normal.
(iii) If A ∈Rn×n, then AAT and AT A have the same eigenvalues.
(iv) If A is normal and k is a positive integer, then Ak is normal.

9.2
Quadratic Forms
305
9.2
Quadratic Forms
A quadratic form over a ﬁeld F is a function q : Fn →F of the form q(r1, . . . ,
rn) = 
i,j aijrirj, where all the aij are in F. Quadratic forms are used in
many areas. For example, Lagrange’s four squares theorem says that every
positive integer n is the sum of the squares of four integers. That is, there
exist a, b, c, d ∈Z such that n = a2 + b2 + c2 + d2. Every real quadratic form
can be expressed via a real symmetric matrix; hence every real quadratic form
can be written as a sum of squares. In this section, we will develop some of the
basic properties of quadratic forms over F such as diagonalization. We will
also consider an interesting equivalence relation on the symmetric matrices
over F called congruence, and we will prove Sylvester’s law of inertia, which
classiﬁes the equivalence classes of congruent symmetric matrices over the
reals R. A consequence of this is the classiﬁcation of positive deﬁnite matri-
ces. We will also treat the corresponding equivalence relation for Hermitian
matrices and Hermitian quadratic forms.
9.2.1
Quadratic forms and congruence
Throughout this section, we will assume that the ﬁeld F has characteristic
diﬀerent from two. A quadratic form on Fn is a function q : Fn →F such that
q(r1, . . . , rn) =
n

i,j=1
aijrirj,
where all aij are in F. Since the characteristic of F is not two, one can suppose
that the coeﬃcients aij of q are symmetric by replacing aij by bij = 1
2(aij +
aji). This leaves q unchanged, but by making its coeﬃcients symmetric, we
can associate a unique symmetric matrix to q in order to bring in our previous
results on symmetric matrices. Indeed, q is expressed in terms of B by the
identity
q(r1, . . . , rn) = rT Br
where r =
⎛
⎜
⎝
r1
...
rn
⎞
⎟
⎠.
Conversely, a symmetric matrix A ∈Fn×n deﬁnes the quadratic form
qA(r1, . . . , rn) = rT Ar.
Under a change of coordinates on Fn of the form r = Cs, where C ∈Fn×n
is invertible, the quadratic form qA is transformed into a new quadratic form

306
9
Unitary Diagonalization and Quadratic Forms
as follows:
qA(r1, . . . , rn) = qA((Cs)T ) = sT CT ACs.
Thus, the quadratic form qB(s1, s2, . . . , sn) associated to B = CT AC satisﬁes
qB(s1, s2, . . . , sn) = qA(r1, . . . , rn).
In other words, qB(sT ) and qA(rT ) are the same quadratic form expressed in
diﬀerent coordinate systems. This motivates the next deﬁnition.
Deﬁnition 9.2. Two symmetric n × n matrices A and B over F are said to
be congruent if there exists a nonsingular C ∈Fn×n such that A = CT BC.
Quadratic forms associated to congruent matrices are said to be equivalent.
Proposition 9.7. Congruence is an equivalence relation on the symmetric
matrices in Fn×n. Moreover, equivalence of quadratic forms is also an equiv-
alence relation on quadratic forms.
Proof. It suﬃces to show that equivalence is an equivalence relation. First,
A = (In)T AIn, which shows that A is equivalent to itself. If A = CT BCT ,
where C is nonsingular, then B = (C−1)T AC−1. Thus equivalence is sym-
metric. We leave the proof that equivalence is transitive as an exercise.
⊓⊔
9.2.2
Diagonalization of quadratic forms
The basic fact about quadratic forms is that every quadratic form q over a
ﬁeld F of characteristic diﬀerent from two is equivalent to a quadratic form
q′ that is a sum of squares. That is,
q′(s1, . . . , sn) =
n

i=1
ais2
i ,
where the ai lie in F. Equivalently, every symmetric matrix is congruent to
a diagonal matrix. We will omit the proof, but note that we have already
treated a special case. For example, by Proposition 4.19, a symmetric n × n
matrix A over F such that each k × k submatrix Ak in the upper left-hand
corner of A is invertible admits an LDLT decomposition, where L is lower
triangular unipotent. In that case, the kth diagonal entry of D is the kth
pivot of A, namely, dk = det Ak/ det Ak−1, where by deﬁnition, det A0 = 1.
Example 9.3. Suppose F = Q, and let A ∈F2×2 be symmetric. Put
A =

a
b
b
c
	
,

9.2
Quadratic Forms
307
where a ̸= 0. Then we have

1
0
−b/a
1
	 a
b
b
c
	 1
−b/a
0
1
	
=
a
0
0
c −b2/a
	
,
so A is congruent to a diagonal matrix (even if A is singular).
⊠
9.2.3
Diagonalization in the real case
The principal axis theorem says that a real quadratic form qA(r1, . . . , rn) =

n
i,j=1 aijrirj, where the matrix A = (aij) is symmetric, can be diagonalized
using orthogonal axes. To be speciﬁc, there exists an orthogonal matrix Q
such that A = QDQT . The columns of Q are the principal axes, and set-
ting s = QT r, the components of s are the coordinates with respect to the
principal axes. Thus,
qA(r1, . . . , rn) = qD(s1, . . . , sn) =

λis2
i .
Proposition 9.8. Every real quadratic form q(r1, . . . , rn) can be orthogo-
nally diagonalized as
q(r1, . . . , rn) =
n

i=1
λis2
i ,
where r = Qs and Q is the orthogonal matrix of principal axes.
Example 9.4. Consider the quadratic form q(x, y) = x2 + 4xy + y2. Its asso-
ciated symmetric matrix is
A =

1
2
2
1
	
.
The eigenvalues of A are 3 and −1. (Reason: both rows sum to 3, and the trace
of A is 2.) Noting that
1
1
	
and
 1
−1
	
are corresponding eigenvectors, we
get A = QDQT , where Q =
1
√
2

1
1
1
−1
	
and
D = diag(3, −1). Putting
u
v
	
= QT
x
y
	
gives new coordinates (u, v) such that q(x, y) is the diﬀer-
ence of squares q(x, y) = 3u2 −v2. Thus the equation q(x, y) = 1 represents
a hyperbola with axes
1
√
2
1
1
	
and
1
√
2
 1
−1
	
.
⊠

308
9
Unitary Diagonalization and Quadratic Forms
Remark. If q is a quadratic form on Rn and c ∈R is a constant, the level
set Vc = {(r1, . . . , rn) | q(r1, . . . , rn) = c} is called a quadratic variety. It is an
example of a real algebraic variety. When n = 2, a quadratic variety is called
a conic section. If q has matrix A and both eigenvalues of A are positive,
then Vc is an ellipse or a circle when c > 0. (It is a circle if A has equal
eigenvalues.) If both eigenvalues are positive and c < 0, then Vc is actually a
subset of C2 that does not meet R2. If A’s eigenvalues have diﬀerent signs,
then Vc is a hyperbola in R2. In R3, the quadratic varieties are surfaces whose
type is classiﬁed by the number of positive and negative eigenvalues of the
associated symmetric matrix.
9.2.4
Hermitian forms
When A ∈Cn×n is Hermitian, a quadratic function uA : Cn →C of the form
uA(z1, . . . , zn) =
n

i,j=1
aijzizj = zHAz,
where z =
⎛
⎜
⎝
z1
...
zn
⎞
⎟
⎠
is called a Hermitian form. Every Hermitian form can be unitarily diagonal-
ized. All Hermitian forms are real-valued, since
(zHAz)H = zHAHzHH = zHAz.
Similarly, a skew-Hermitian form takes only pure imaginary values. By an
argument similar to that in Proposition 9.9, we get the following.
Proposition 9.9. Let A ∈Cn×n be normal, so A = UDU H, where U is uni-
tary and D = diag(λ1, . . . , λn). Let w1, . . . , wn be the coordinates on Cn com-
ing from a Hermitian orthonormal eigenbasis of Cn consisting of the columns
of U. Then
uA(z1, . . . , zn) =
n

i=1
λiwiwi =
n

i=1
λi|wi|2.
9.2.5
Positive deﬁnite matrices
Given a real quadratic form q (or more generally, a Hermitian quadratic form
h), when does q (or h) have a strict maximum or minimum at the origin?
This corresponds to all the eigenvalues of its matrix being either positive or

9.2
Quadratic Forms
309
negative. A minimum is attained if all are positive, and a maximum occurs
if they are all negative. Thus one makes the following deﬁnition.
Deﬁnition 9.3. A real symmetric n × n matrix A is called positive deﬁnite
if
xT Ax > 0 for all nonzero x ∈Rn.
An n × n Hermitian matrix A is positive deﬁnite if and only if
zHAz > 0 for all nonzero z ∈Cn.
The following result describes the positive deﬁnite matrices in several
equivalent ways.
Proposition 9.10. For a real symmetric (respectively complex Hermitian)
matrix A, the following conditions are equivalent:
(i ) all eigenvalues of A are positive;
(ii) A is positive deﬁnite;
(iii) the upper left k × k submatrix Ak has det(Ak) > 0 for all k; and
(iv) A has an LDLT decomposition in which D has positive diagonal entries.
Proof. For simplicity, we will give the proof for only the real symmetric
case. The changes necessary to prove the Hermitian case are routine after
replacing xT Ax by zHAz. We will show that each statement implies the fol-
lowing one and (iv) implies (i). Assume that A ∈Rn×n is symmetric, and
(i) holds. Applying the principal axis theorem, we have A = QT DQ, where
Q = (q1 · · · qn) is orthogonal and D = diag(λ1, . . . , λn) with all λi > 0. Thus,
if x = 
n
i=1 xiqi, then xT Ax = 
n
i=1 λix2
i , which is positive unless x = 0.
Thus (i) implies (ii). Now suppose A is positive deﬁnite, and recall that if
k ≤n, then Rk ⊂Rn as Rk = {x ∈Rn | xi = 0
for all
i > k}. Being sym-
metric, it follows that Ak is positive deﬁnite on Rk for all k. Consequently, all
eigenvalues of Ak are positive, so det(Ak) > 0 for all k. We conclude that (ii)
implies (iii). Moreover, by Proposition 4.17, (iii) immediately implies (iv).
The last implication, (iv) implies (i), follows from the law of inertia, which
will be proved below.
⊓⊔
We say that a symmetric matrix A ∈Rn×n is negative deﬁnite if −A is pos-
itive deﬁnite, with a similar deﬁnition for the Hermitian case. Thus a negative
deﬁnite matrix has only negative eigenvalues and has an LDLT decompo-
sition with negative pivots. Note that this means that (−1)k det(Ak) > 0.
Finally, a symmetric matrix that has both positive and negative eigenvalues
is called indeﬁnite.

310
9
Unitary Diagonalization and Quadratic Forms
Example 9.5. Consider the matrix
A =
⎛
⎜
⎜
⎝
1
1
0
1
1
2
−1
0
0
−1
2
0
1
0
0
2
⎞
⎟
⎟
⎠.
By row operations using lower triangular elementary matrices of the third
kind, we get
L∗A =
⎛
⎜
⎜
⎝
1
1
0
1
0
1
−1
−1
0
0
1
−1
0
0
0
−1
⎞
⎟
⎟
⎠.
Hence A has an LDU decomposition, but three pivots are positive and one
is negative. Therefore, A cannot be positive deﬁnite or negative deﬁnite.
9.2.6
The positive semideﬁnite case
A quadratic form q on Rn is said to have a relative minimum at 0 if q(x) ≥0
for all x ∈Rn and q(x) = 0 for some nonzero x. The meaning of a rela-
tive maximum is similar, and the same deﬁnitions apply to Hermitian forms
on Cn. Here, k satisﬁes 0 < k ≤n. A real symmetric (respectively complex
Hermitian) n × n matrix A is said to be positive semideﬁnite if xT Ax ≥0
(respectively zHAz ≥0) for all x ∈Rn (respectively z ∈Cn). A positive
semideﬁnite matrix can have a nontrivial null space, but it can’t have any
negative eigenvalues, since if (λ, v) is an eigenpair for A with λ < 0, then
vT Av = λvT v = λ|v|2 < 0.
Proposition 9.11. Let A be positive semideﬁnite. Then all eigenvalues of
A are nonnegative.
This is immediate from the previous observation. Here is the main result
about positive semideﬁnite matrices.
Proposition 9.12. A real symmetric (respectively complex Hermitian) matrix
A is positive semideﬁnite if and only if it admits an LDLT (respectively
LDLH) decomposition in which D has only nonnegative entries.
Proof. Suppose that A is real symmetric. (The proof in the Hermitian case
is, as usual, similar.) Since it may happen that det(Ak) = 0 for some k, we
have to give a direct proof that A has an LDLT decomposition. But we know
that A has an LPDU decomposition of the form A = LPDLT , where L is
lower triangular unipotent, P is a (unique) partial permutation matrix, D
is diagonal, and PD is symmetric. It suﬃces to show that if PD is positive

9.2
Quadratic Forms
311
semideﬁnite, then PD is a diagonal matrix with nonnegative entries. Suppose
A is of size n × n and P ̸= In nor is P obtained by setting some rows of In
equal to 0. To simplify the notation, suppose the ﬁrst row of P is nonzero.
Let’s assume that the ith column is de1, where i > 1 and d ̸= 0. Since PD is
symmetric, its ﬁrst column is dei. Thus PD interchanges e1 and ei. Putting
y = y1e1 + yiei, we get
yT PDy = (y1e1 + yiei)T (dy1ei + dyie1) = 2dy1yi.
Hence, if PD is positive semideﬁnite, then dy1yi ≥0 for all y1, yi ∈R. Since
i > 1, this is clearly impossible, so if the ﬁrst row of PD is nonzero, it has
d in the (1, 1) entry. The argument is the same for all other rows, so PD is
diagonal. Moreover, the entries of D must be nonnegative, so every positive
semideﬁnite real symmetric matrix can be written LDLT . The converse is
left as an exercise.
⊓⊔
Example 9.6. Suppose A ∈Rm×n. Then AT A is symmetric, so we can ask
whether AT A is positive or positive semideﬁnite. In fact, for every x ∈Rn,
xT AT Ax = (Ax)T Ax = |Ax|2, so xT AT Ax ≥0. Hence AT A is positive semi-
deﬁnite. In particular, all eigenvalues of AT A are nonnegative. If N(A) = {0},
then |Ax| > 0, provided x ̸= 0, so AT A is positive deﬁnite. The same is true
for AAT .
⊠
Exercises
Exercise 9.2.1. Let A ∈R2×2 be symmetric.
(i) Show that if det(A) > 0, then A is either positive deﬁnite or negative
deﬁnite.
(ii) Also show that if Tr(A) = 0, then A is indeﬁnite.
Exercise 9.2.2. Suppose A is a symmetric matrix such that det(A) ̸= 0 and
A has both positive and negative diagonal entries. Explain why A has to be
indeﬁnite.
Exercise 9.2.3. Show that if A is a positive deﬁnite 3 × 3 matrix, then the
coeﬃcients of its characteristic polynomial alternate in sign. Also show that
if A is negative deﬁnite, the coeﬃcients are all negative.
Exercise 9.2.4. Give an example of a 3 × 3 symmetric matrix A such that
the coeﬃcients of the characteristic polynomial of A are all negative, but A
is not negative deﬁnite. (Could your answer be a diagonal matrix?)
Exercise 9.2.5. Let A ∈Rn×n be positive deﬁnite and suppose S ∈Rn×n is
nonsingular.
(i) When is SAS−1 positive deﬁnite?

312
9
Unitary Diagonalization and Quadratic Forms
(ii) Is Am positive deﬁnite for all integers m? (Note: A0 = In.)
Exercise 9.2.6. Describe the quadratic surface (x y z)A(x y z)T = 1 for
the following choices of A:
⎛
⎝
1
2
−1
2
0
3
3
−1
2
⎞
⎠
and
⎛
⎝
2
4
2
2
2
1
2
1
5
⎞
⎠.
Exercise 9.2.7. Decide whether g(x, y, z) = x2 + 6xy + 2xz + 3y2 −xz + z2
has a maximum, minimum, or neither at (0, 0, 0).
Exercise 9.2.8. Let A ∈Rn×n, and suppose Ai = 0 for some i < n. Does
this mean that A has a zero eigenvalue?
Exercise 9.2.9. Let A ∈Rm×n have the property that N(A) ̸= {0}. Is AT A
positive semideﬁnite? Can it be positive deﬁnite?
Exercise 9.2.10. For the following pairs A, B of symmetric matrices, deter-
mine whether A and B are congruent.
(i) A and B have the same characteristic polynomial.
(ii) det(A) < 0, det(B) > 0.
(iii) A =

1
2
2
1
	
and B =

1
2
2
−1
	
.
(iv) A and B are similar.
(v) A and B are positive deﬁnite.
(vi) AB = BA.
Exercise 9.2.11. Show that if a symmetric (respectively Hermitian) matrix
A is semipositive deﬁnite, then A has a symmetric (respectively Hermitian)
kth root M for all k > 0. (That is, Ak = M.) Moreover, if A is positive
deﬁnite, so is M.
Exercise 9.2.12. Show that the product AB of a positive deﬁnite matrix
A and a symmetric matrix B has real eigenvalues, even though AB is not
necessarily symmetric. (Hint: show that AB is similar to a symmetric matrix.
Exercise 9.2.11 may help.)

9.3
Sylvester’s Law of Inertia and Polar Decomposition
313
9.3
Sylvester’s Law of Inertia and Polar
Decomposition
In the ﬁnal section of this chapter, we will prove two interesting results about
congruence classes of Hermitian matrices. The ﬁrst is a famous result of
Sylvester that classiﬁes the congruence class of a Hermitian matrix in terms
of its signature. The signature of a Hermitian matrix is a triple that tabulates
the number of eigenvalues that are positive, negative, or zero. The law of
inertia says that two Hermitian matrices are congruent if and only if their
signatures coincide. This result tells us, for example, that the signs of the
pivots of a real symmetric matrix determine its signature. The second result
says that every element of GL(n, C) has a unique factorization as KU, where
K is positive deﬁnite Hermitian and U is unitary. This gives the structure
of the congruence class with signature (n, 0, 0). Namely, the class consisting
of positive deﬁnite Hermitian matrices is in one-to-one correspondence with
the coset space GL(n, C)/U(n). Similarly, the congruence class of positive
deﬁnite real symmetric matrices is in one-to-one correspondence with the
coset space GL(n, R)/O(n, R).
9.3.1
The law of inertia
Let A ∈Cn×n be Hermitian, and let n+(A), n−(A), and n0(A) denote, respec-
tively, the number of positive, negative, and zero eigenvalues of A. For exam-
ple, n0(A) = dim N(A). We will call the triple (n+(A), n−(A), n0(A)) the
signature of A. Sylvester’s law of inertia, to be proved next, gives an elegant
answer to the question of determining the signature of A. Since by Corollary
4.22, congruent matrices have the same rank, it follows from the rank–nullity
theorem that if A and B are congruent, then n0(A) = n0(B). For simplicity,
we will state and prove the law of inertia in the real symmetric case only.
The Hermitian version may be formulated without any surprises and proved
in essentially the same way.
Theorem 9.13 (Sylvester’s law of inertia). Let A and B be congruent real
symmetric matrices. Then A and B have the same signature. That is,
n+(A) = n+(B),
n0(A) = n0(B) and n−(A) = n−(B).
Conversely, two real symmetric matrices having the same signature are con-
gruent. In particular, if a real symmetric matrix A has a symmetric LDU
decomposition A = LDLT , then the signs of its eigenvalues are the same as
the signs on the diagonal of D (which are the pivots of A).

314
9
Unitary Diagonalization and Quadratic Forms
Proof. First choose orthogonal matrices P and Q such that A = PDP T and
B = QEQT , where D and E are diagonal. Notice that without aﬀecting
these expressions, we may assume that the positive diagonal entries of D
are d1, . . . , ds and that those of E are e1, . . . , et. This follows from the fact
that for every permutation matrix P, PDP T = PDP −1 and D have the same
diagonal up to the permutation corresponding to P. Notice that A and D, as
well as B and E, have the same eigenvalues, hence the same signature. Thus,
to show that if A and B are congruent, then they have the same signature, it
will suﬃce to show that D and E have the same signature. Write B = CACT ,
where C ∈Rn×n is invertible. Thus
B = CACT = CPDP T CT = QEQT .
Hence E = MDM T , where M = QT CP. To show that D and E have the
same signature, it suﬃces to show that n+(D) = n+(E), since D and E, being
congruent, have n0(D) = n0(E), as remarked above. Let us assume n+(D) =
s < n+(E) = t, and let fi(x) ∈(Rn)∗denote the ith component function of
M T x. Since xT Ex = xT MDM T x, when xT = (x1, . . . , xt, 0, . . . , 0), we have
t

i=1
eix2
i =
n

j=1
djfj(x1, . . . , xt, 0, . . . , 0)2.
(9.3)
Since t > s, there exist a1, . . . , at ∈R not all zero such that
fj(a1, . . . , at, 0, . . . , 0) = 0 for j = 1, . . . , s.
Indeed, fewer than t homogeneous equations in t variables have a nontrivial
solution. Thus,
0 <
t

i=1
eia2
i =
n

j=s+1
djfj(a1, . . . , at, 0, . . . , 0)2 ≤0,
since dj ≤0 if j > s. This is a contradiction, so we must have n+(D) =
n+(E). This shows that D and E have the same signature, and hence so
do A and B. We will leave the proof of the converse as an exercise.
⊓⊔
Example 9.7. Let
A =
⎛
⎝
1
1
2
1
1
3
2
3
2
⎞
⎠.
Then the quadratic form associated to A is
Q(x, y, z) = x2 + 2xy + 4xz + 2y2 + 6yz + 2z2.

9.3
Sylvester’s Law of Inertia and Polar Decomposition
315
A routine calculation gives
LA = DU =
⎛
⎝
1
1
2
0
1
1
0
0
−3
⎞
⎠=
⎛
⎝
1
0
0
0
1
0
0
0
−3
⎞
⎠
⎛
⎝
1
1
2
0
1
1
0
0
1
⎞
⎠.
Since A is nonsingular and symmetric, the expression LA = DU implies that
A = U T DU. Hence A is congruent to diag(1, 1, −3), so its signature is (2, 1, 0).
The quadratic surface
x2 + 2xy + 4xz + 2y2 + 6yz + 2z2 = 1
is a hyperboloid of one sheet.
⊠
Remark. The law of inertia also holds in the Hermitian case. In fact, the
proof is essentially the same, so we will leave the details to the reader.
9.3.2
The polar decomposition of a complex linear
mapping
Recall that every nonzero complex number has a unique polar representa-
tion z = |z|eiθ with 0 ≤θ < 2π. The purpose of this section is to generalize
this fact to linear mappings. Recall that eiθ is an element of the group of
unit complex numbers. By deﬁnition, this group is U(1) = {z ∈C | zzH = 1}.
The polar representation says that group-theoretically, C∗= GL(1, C) =
(R>0)U(1). This representation generalizes to the n × n case when we replace
U(1) by the n × n unitary group and R>0 by the set of positive deﬁnite n × n
Hermitian matrices (which is not a group).
Proposition 9.14. Every A ∈GL(n, C) can be uniquely expressed in either
of two ways as A = KU = UK′, where U ∈U(n), K is the unique Hermitian
positive deﬁnite matrix such that K2 = AAH, and K′ = U HKU.
Proof. Let H = AAH. Then H is Hermitian. Since A is nonsingular, H is pos-
itive deﬁnite. (Proof: let (λ, x) be an eigenpair for H. Since A is nonsingular,
AHx ̸= 0. Thus,
0 < |AHx|2 = (AHx)HAHx = xH(AAH)x = xH(λx) = λ|x|2,
so λ > 0.) By the result of Exercise 9.2.11, we can write H = K2, where K
is also positive deﬁnite Hermitian. Now put U = K−1A. Then A = KU, so
it suﬃces to show that U is unitary. But

316
9
Unitary Diagonalization and Quadratic Forms
UU H = K−1A(AH(K−1)H) = K−1H(K−1)H = K−1K2K−1 = In,
so U is indeed unitary. The U ′K′ factorization follows because
KU = (UU H)KU = U(U HKU) = UK′,
and K′ = U HKU is also positive deﬁnite Hermitian by the law of inertia.
Finally, to prove uniqueness, notice that if K is Hermitian positive deﬁnite
and A = KU, then necessarily K2 = AAH, since K = AU H, so KH = UAH,
and hence K2 = KKH = (AU H)(UAH) = AAH. But the square root of a
positive deﬁnite matrix is unique (verify this), so the polar representation
must be unique.
⊓⊔
The real version of the polar representation takes an expected similar form.
The proof is identical to the complex case.
Proposition 9.15. Every A ∈GL(n, R) can be uniquely expressed in either
of two ways as A = SQ = QS′, where Q ∈O(n, R), S is the unique positive
deﬁnite real matrix such that S2 = AAT , and S′ = QT SQ.
Polar decomposition gives rise to a nice interpretation of the coset spaces
GL(n, C)/U(n) and GL(n, R)/O(n, R). Let PD(n, C) (respectively PD(n, R))
denote the n × n positive deﬁnite Hermitian (respectively positive deﬁ-
nite real symmetric) matrices. Neither PD(n, C) nor PD(n, R) is a group,
because neither is closed under multiplication. However, PD(n, C) and
PD(n, R) are both closed under inverses. The upshot of polar decompo-
sition is that the quotient maps π : GL(n, C) →GL(n, C)/U(n) and π :
GL(n, R) →GL(n, R)/O(n, R) have the property that π(PD(n, C)) = GL(n,
C)/U(n) and π(PD(n, R)) = GL(n, R)/O(n, R). This gives a surjective map-
ping π : PD(n, C) →GL(n, C)/U(n) with a corresponding map in the real
case deﬁned in exactly the same way. I claim that π is injective in each case.
Here is the proof in the real case. (The complex case is essentially the same.)
Suppose R, T ∈PD(n, R) and π(R) = π(T). That is, RO(n, R) = TO(n, R).
By the criterion for equality of left cosets, it follows that R−1T ∈O(n, R).
Since R−1 is also positive deﬁnite, Exercise 9.2.12 implies that R−1T has real
eigenvalues. But if R−1T ∈O(n, R), this implies that the eigenvalues of R−1T
are either 1 or −1. But −1 is impossible, for if −1 is an eigenvalue, there exists
a nonzero x ∈Rn such that R−1T(x) = −x. Thus T(x) = R(−x) = −R(x),
so
xT T(x) = −xT R(x).

9.3
Sylvester’s Law of Inertia and Polar Decomposition
317
Since R and T are both positive deﬁnite, this is impossible. Hence 1 is the
only eigenvalue of R−1T. But since R−1T ∈O(n, R), R−1T is also normal,
hence is unitarily diagonalizable by the normal matrix theorem (Theorem
9.2). Hence R−1T = In, so R = T. Therefore, we have proved the following
result.
Proposition 9.16. The mappings π : PD(n, C) →GL(n, C)/U(n) and π :
PD(n, R) →GL(n, R)/O(n, R) are bijective.

Chapter 10
The Structure Theory of Linear
Mappings
Throughout this chapter, V will be a ﬁnite-dimensional vector space over F.
Our goal is to prove two theorems that describe the structure of an arbi-
trary linear mapping T : V →V having the property that all the roots of
its characteristic polynomial lie in F. To describe this situation, let us say
that F contains the eigenvalues of T. A linear mapping T : V →V is also
called an endomorphism of V , and in this chapter, we will usually use that
term. The structure theory for the endomorphisms of V is one of the nicest
chapters in the theory of ﬁnite-dimensional vector spaces. The ﬁrst result we
will prove, known as the Jordan–Chevalley decomposition, asserts that every
endomorphism T as above can be uniquely expressed as the sum T = S + N
of a semisimple endomorphism S and nilpotent endomorphism N such that
TS = ST and SN = NS (hence TN = NT). The endomorphisms S and N
are called, respectively, the semisimple part of T and the nilpotent part of T,
and the expression T = S + N is known as the Jordan–Chevalley decomposi-
tion of T.
The second structure theorem, known as the Jordan canonical form, is a
reﬁnement of the Jordan–Chevalley decomposition. It says that there exists
a basis of V such that the matrix of T is a direct sum of Jordan blocks.
A Jordan block is a matrix of the form J = μIn + N, where N is an upper
triangular n × n matrix with ones on the superdiagonal and zeros everywhere
else. In particular, N is nilpotent. Notice that μ is the unique eigenvalue of
J. A matrix J in this form is said to be in Jordan canonical form.
These structure theorems require that all the eigenvalues of the endomor-
phism T lie in F. If F is algebraically closed, for example if F = C, then they
apply to all endomorphisms. We can satisfy this eigenvalue assumption by
letting V = Fn, since then we are dealing with n × n matrices over F, and
by the appendix to Chap. 8, there exists an extension ﬁeld F′ of F containing
every eigenvalue of A. Thus A can be decomposed as A = S′ + N ′, where S′
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 10
319

320
10
The Structure Theory of Linear Mappings
and N ′ are matrices over F′ that have the same roles as S and N; that is, S′
is semisimple, N ′ is nilpotent, S′N ′ = N ′S′, and AS′ = S′A.
10.1
The Jordan–Chevalley Theorem
From now on, assume that V is a vector space over F such that dim V = n,
and suppose T : V →V is an arbitrary linear mapping whose characteristic
polynomial is pT (x) = (x −λ1)μ1 · · · (x −λm)μm, where λ1, . . . , λm are the
distinct eigenvalues of T and all λi lie in F. By the Cayley–Hamilton theorem,
pT (T) = (T −λ1IV )μ1 · · · (T −λmIV )μm = O.
(10.1)
By Theorem 8.13, a necessary and suﬃcient condition that T be semisimple is
that (T −λ1IV ) · · · (T −λmIV ) = O. The Jordan–Chevalley theorem, which
we will presently prove, will answer the question of how far T varies from
being semisimple.
10.1.1
The statement of the theorem
To state the theorem, we need to deﬁne the invariant subspaces of T.
Deﬁnition 10.1. The subspaces Ci = ker(T −λiIV )μi ⊂V are called the
invariant subspaces of T. If A is the matrix of T with respect to a basis of V ,
then the subspaces Ci = N((A −λiIn)μi) are the invariant subspaces for A.
Note that T(Ci) ⊂Ci, so each Ci is invariant under T and similarly for A.
The invariant subspaces are also called the cyclic subspaces of T or A.
Lemma 10.1. Let Ti = (T −λiIV )μi so that Ci = ker(Ti). Then
V = C1 ⊕C2 ⊕· · · ⊕Cm.
Proof. We will apply Lemma 8.14. Thus we have to check that Ti ◦Tj = Tj ◦
Ti for all i and j, T1 ◦· · · ◦Tm = O, and ker(Ti) ∩ker(Ti+1 ◦· · · ◦Tm) = {0}
for all i. In fact, the ﬁrst statement is clear, and the second is the Cayley–
Hamilton theorem. Thus, we need to check only that ker(Ti) ∩ker(Ti+1 ◦· · · ◦
Tm) = {0} for all i. Note ﬁrst that Eλi(T) ∩ker(Ti+1 ◦· · · ◦Tm) = {0}. For
if v ∈Eλi(T), then
Ti+1 ◦· · · ◦Tm(v) = (λi −λi+1)μi+1 · · · (λi −λm)μmv.

10.1
The Jordan–Chevalley Theorem
321
Thus if v ∈Eλi(T) ∩ker(Ti+1 ◦· · · ◦Tm), then v = 0. Now suppose μi > 1
and take v ∈ker(Ti) ∩ker(Ti+1 ◦· · · ◦Tm) such that v ̸= 0. Let a ≤μi be
the least positive integer such that (T −λiIV )av = 0. By the previous case,
we may assume a > 1, so let w = (T −λiIV )a−1v. Then by deﬁnition, w ∈
Eλi(T) and w ̸= 0. But w ∈ker(Ti+1 ◦· · · ◦Tm) also, which implies w = 0,
contradicting the choice of w. Hence v = 0, so the lemma is proved.
□
Let us now state the theorem.
Theorem 10.2 (Jordan–Chevalley decomposition theorem). Let T : V →V
be an endomorphism all of whose eigenvalues lie in F and whose characteristic
polynomial is given by (10.1), where λ1, . . . , λm are the distinct eigenvalues
of T. Suppose C1, C2, . . . , Cm are the invariant subspaces of T. Then
V = C1 ⊕C2 ⊕· · · ⊕Cm.
(10.2)
Let S : V →V be the unique endomorphism such that S(v) = λiv if v ∈Ci.
Then:
(i) S is semisimple, and the linear mapping N = T −S is nilpotent;
(ii) S and N commute, and both commute with T: SN = NS, NT = TN,
and ST = TS;
(iii) the decomposition T = S + N of T into the sum of a semisimple linear
mapping and a nilpotent linear mapping that commute is unique; and ﬁnally,
(iv) dim Ci = μi for all i.
The proof is given in the next section. The reader who wishes to see an
example in which the decomposition is explicitly computed can go directly
to Example 10.1.
Deﬁnition 10.2. The decomposition T = S + N is called the Jordan–
Chevalley decomposition of T. The mapping S is called the semisimple part
of T, and N is called its nilpotent part.
Corollary 10.3. An n × n matrix A over F whose eigenvalues all lie in F
can be expressed in exactly one way as a sum A = L + M of two commuting
matrices L and M over F such that L is diagonalizable and M is nilpotent.
Proof. First, let TA = TL + TM be the Jordan–Chevalley decomposition of
TA. This gives the decomposition A = L + M, as asserted.
□
The Jordan–Chevalley decomposition enables us ﬁnally to clear up an
intriguing question: is the algebraic multiplicity of an eigenvalue always an
upper bound for its geometric multiplicity?

322
10
The Structure Theory of Linear Mappings
Corollary 10.4. Let λ be an eigenvalue of an endomorphism T : V →V
having algebraic multiplicity μ. Then dim Eλ(T) ≤μ.
Proof. Let λi be an eigenvalue. By part (iv) of the Jordan–Chevalley theorem,
the algebraic multiplicity of λi, namely μi, is the dimension of the invariant
subspace Ci. But Eλi(T) ⊂Ci, so the geometric multiplicity of λi, namely
dim Eλi(T), is at most μi.
□
10.1.2
The multiplicative Jordan–Chevalley
decomposition
There is also a multiplicative version of the Jordan–Chevalley decomposition
for nonsingular matrices. Let us ﬁrst mention that the notion of a unipotent
matrix used in the LPDU decomposition is a special case of what is generally
known as a unipotent matrix: a matrix A ∈Fn×n is called unipotent if A −In
is nilpotent.
Proposition 10.5. Suppose A ∈GL(n, F) and all the eigenvalues of A lie in
F. Then one can write A uniquely in the form A = AsAu, where As is semi-
simple, Au is unipotent, and AsAu = AuAs. Moreover, this decomposition is
unique.
Proof. Let A = S + N be the Jordan–Chevalley decomposition of A. Since
the eigenvalues of A and S are the same, it follows that S is nonsingular.
Put As = S, so that As is semisimple. Next put Au = In + S−1N. Since S
and N commute, S−1N is nilpotent; hence Au is unipotent. This shows that
A = AsAu and AsAu = AuAs. To prove uniqueness, note that A = As(In +
N) = As + AsN. Since As and N commute, A = As + AsN is the Jordan–
Chevalley decomposition of A. Hence, As and Au are unique.
□
This product decomposition applies to all A ∈GL(n, C), for example, or
to every GL(n, F) with F algebraically closed. The matrices As and Au are
known as the semisimple and unipotent parts of A. The multiplicative version
of the Jordan–Chevalley decomposition implies an interesting fact about ﬁnite
subgroups of GL(n, C), or indeed GL(n, F) if F is algebraically closed of
characteristic zero.
Proposition 10.6. Let F be algebraically closed of characteristic zero. Then
every ﬁnite subgroup G < GL(n, F) consists of diagonalizable matrices.
Proof. Assume A ∈G, and write its Jordan–Chevalley decomposition as A =
AsAu. Since G is ﬁnite, A has ﬁnite order, say m. Then Am = (AsAu)m =
(As)m(Au)m = In. Now (As)m is semisimple, and (Au)m is unipotent, so
by the uniqueness of the multiplicative Jordan–Chevalley decomposition,

10.1
The Jordan–Chevalley Theorem
323
(As)m = (Au)m = In. To ﬁnish the proof, we have to show that (Au)m = In
implies Au = In. Write Au = In + N, where N is nilpotent. Then
(Au)m = In + mN +
m
2

N 2 + · · · + N m.
Hence, mN(In +
m
2

N + · · · + N m−1) = O. Since In +
m
2

N + · · · + N m−1
is unipotent, it is invertible, so N = O. Therefore Au = In, so A = As.
□
In fact, the above proof shows that there is a stronger conclusion.
Proposition 10.7. Suppose F is algebraically closed of characteristic p and
the order of G < GL(n, F) is prime to p. Then every element of G is diago-
nalizable.
Proof. By Lagrange’s theorem, the order m of every A ∈G is prime to p,
since |G| is. Therefore, by the above argument, mN = O, so N = O. Hence
Au = In, so A = As.
□
If F has characteristic p, then there exist ﬁnite subgroups of GL(n, F)
of order p all of whose elements are unipotent. For example, a transvection
matrix In + Eij, where i ̸= j, generates a subgroup of GL(n, F) of order p
isomorphic to the additive group of Fp.
Remark. When F = C, the proof of Proposition 10.6 can also be done using
the standard Hermitian inner product on Cn. If one averages the inner prod-
ucts (Av, Aw) over all A ∈G, then one obtains a Hermitian inner product
on Cn for which the matrices A ∈G are normal. Thus, every element of G is
unitarily diagonalizable, by the normal matrix theorem.
10.1.3
The proof of the Jordan–Chevalley theorem
As proved in Lemma 10.1,
V = C1 ⊕· · · ⊕Cm.
Thus each v ∈V has a unique expression v = m
i=1 ci with ci ∈Ci. Applying
Proposition 7.5, there exists a unique linear mapping S : V →V deﬁned by
setting
S(v) =
m

i=1
λici.
(10.3)

324
10
The Structure Theory of Linear Mappings
Moreover, by deﬁnition, S is semisimple. Now put N = T −S. We claim that
N is nilpotent. For this, it suﬃces to show that there exists an integer r > 0
such that N r = O on V . By deﬁnition, if v ∈Ci, then
N μi(v) = (T −S)μi(v) = (T −λiIV )μi(v) = 0.
But V is the sum of the Ci, so N r = O on V if r > μi for all i. This shows
that N is nilpotent and completes the veriﬁcation of part (i). We will leave
part (ii), that T, S, and N all commute with each other, to the reader.
We next prove part (iii): the decomposition T = S + N satisfying (i) and
(ii) is unique. Let T = S′ + N ′ be another decomposition of T, where S′ is
semisimple, N ′ is nilpotent, and S′N ′ = N ′S′. Since S′ is semisimple, we can
write V = W1 ⊕· · · ⊕Wk, where the Wi are the eigenspaces for S′, and since
TS′ = S′T, it follows that T(Wi) ⊂Wi for each i. Thus, N ′(Wi) ⊂Wi also.
But since N ′ is nilpotent, ker(N ′) ∩Wi ̸= {0}. Thus, there exists v ∈Wi
such that (T −S′)v = 0. This means that T(v) = S′(v) = νiv, where νi is
the eigenvalue of S on Wi. Thus, every eigenvalue of S′ is an eigenvalue of T.
Now it follows from the argument in Lemma 10.1 that Cj = ker(T −λjIn)μj+k
for all k ≥0. Thus if νi = μj, then Wi ⊂Cj. But this implies Wi = Cj, since
 dim Wj =  dim Ci = dim V . Hence S′ = S and N ′ = N.
It remains to prove (iv), that is, that μi = dim Ci for all i. Note that the
matrix A of T has the block form
A =
⎛
⎜
⎜
⎜
⎝
A1
O
· · ·
O
O
A2
· · ·
O
...
...
...
...
O
· · ·
O
Am
⎞
⎟
⎟
⎟
⎠,
where Ai is the matrix of T on Ci. It follows from the product rule for
determinants that
pA(x) = pA1(x) · · · pAm(x) = (x −λ1)ℓ1 · · · (x −λm)ℓm,
(10.4)
since the only eigenvalue of Ai is λi. Therefore, dim Ci = ℓi = μi. This
proves (iv).
□
10.1.4
An example
The following example shows that even the 3 × 3 case can be a little compli-
cated.

10.1
The Jordan–Chevalley Theorem
325
Example 10.1. Let V = C3, and consider the endomorphism TA, where
A =
⎛
⎝
1
2
1
0
1
1
0
0
2
⎞
⎠.
The characteristic polynomial of A is −(x −1)2(x −2), so the distinct eigen-
values are 2 and 1, which is repeated. Note that (A −I3)(A −2I3) ̸= O, so
A is not semisimple. The matrices (A −I3)2 and A −2I3 row reduce respec-
tively to
⎛
⎝
0
0
1
0
0
0
0
0
0
⎞
⎠
and
⎛
⎝
1
0
−3
0
1
−1
0
0
0
⎞
⎠.
Thus C1 = span{e1, e2}, and C2 = C
⎛
⎝
3
1
1
⎞
⎠. Hence the semisimple part of A
is the matrix S determined by
S
⎛
⎝
1
0
0
⎞
⎠=
⎛
⎝
1
0
0
⎞
⎠,
S
⎛
⎝
0
1
0
⎞
⎠=
⎛
⎝
0
1
0
⎞
⎠,
S
⎛
⎝
3
1
1
⎞
⎠= 2
⎛
⎝
3
1
1
⎞
⎠.
As usual, S is found by SP = PD. Therefore,
S =
⎛
⎝
1
0
3
0
1
1
0
0
2
⎞
⎠,
and we get N by subtraction:
N =
⎛
⎝
0
2
−2
0
0
0
0
0
0
⎞
⎠.
(As a check, make sure that SN = NS.) Thus A = S + N is the Jordan–
Chevalley decomposition.
⊠
Notice that if P is the matrix that diagonalizes S, i.e.,
P =
⎛
⎝
1
0
3
0
1
1
0
0
1
⎞
⎠,
then a change of basis using P puts the matrix of TA into block diagonal
form. Namely,

326
10
The Structure Theory of Linear Mappings
P −1AP =
⎛
⎝
1
2
0
0
1
0
0
0
2
⎞
⎠.
By choosing P more carefully, we can guarantee that P −1TP is in Jordan
canonical form. For this, see Example 10.4.
10.1.5
The Lie bracket
Given two endomorphisms S, T : V →V , their Lie bracket [S, T] is deﬁned
by [S, T] = S ◦T −T ◦S. Thus [S, T] is an endomorphism of V that mea-
sures how much S and T fail to commute. Now let gℓ(V ) denote the vector
space of all endomorphisms of V . Recall from Section 7.5.1 that gℓ(V ) has
dimension (dim V )2. By ﬁxing S, one obtains an endomorphism of gℓ(V ),
called the adjoint of S, which is denoted by ad(S) and deﬁned by the rule
ad(S)(T) = [S, T]. For example, if V = Fn, then gℓ(V ) = Fn×n, and we know
that a basis of gℓ(V ) is given by the matrices Eij. Then if i ̸= j, we have
ad(Eij)(Eji) = Eii −Ejj. We leave it as an exercise to compute ad(Eij)(Ers)
for all r, s (see Exercise 10.1.6) below). Now suppose F is algebraically closed.
Then a basic theorem about the adjoint mapping says that if we denote
the semisimple and nilpotent parts of S by Sss and Snilp, then the Jordan–
Chevalley decomposition of ad(S) is ad(S) = ad(Sss) + ad(Snilp). In other
words, ad(S)ss = ad(Sss) and ad(S)nilp = ad(Snilp). We ask the reader to
verify this when gℓ(V ) = F2×2 in the exercises below.
Exercises
Exercise 10.1.1. Show that invariant subspaces of a linear mapping T are
actually invariant under T. That is, T(Ci) ⊂Ci for each i.
Exercise 10.1.2. Discuss the Jordan–Chevalley decomposition of the fol-
lowing types of endomorphisms or matrices:
(i) rotations of R2 (i.e., elements of SO(2, R)),
(ii) rotations of R3,
(iii) the cross product mapping Ca(x) = a × x on R3, and
(iv) 2 × 2 complex matrices of determinant and trace zero.
Exercise 10.1.3. Let A ∈Fn×n. Show how to express the semisimple and
nilpotent parts of A2 and A3 in terms of those of A.

10.1
The Jordan–Chevalley Theorem
327
Exercise 10.1.4. Describe all real 2 × 2 matrices that are both symmetric
and nilpotent.
Exercise 10.1.5. Find the Jordan–Chevalley decomposition of the matrices
in parts (i)–(iii) of Exercise 8.5.9.
Exercise 10.1.6. Consider the standard basis Eij, 1 ≤i, j ≤2, of F2×2.
(i) Compute the matrices of ad(E11) and ad(E12) with respect to the standard
basis.
(ii) Calculate the Jordan–Chevalley decomposition of each of ad(E11) and
ad(E12).
Exercise 10.1.7. Show that an n × n matrix A is unipotent if and only if
its characteristic polynomial is (1 −x)n.
Exercise 10.1.8. Prove that the Lie bracket on gℓ(V ) satisﬁes the Jacobi
identity
[R, [S, T]] + [T, [R, S]] + [S, [T, R]] = 0.
Exercise 10.1.9. Show that the Jacobi identity on gℓ(V ) can be restated
as the identity
ad([S.T]) = [ad(S), ad(T)].
This says that if gℓ(V ) is made into a ring where the multiplication is R ◦S =
[R, S], then ad is a ring homomorphism.
Exercise 10.1.10. Let T : C9 →C9 be an endomorphism with characteris-
tic polynomial
(t + 1)2(t −1)3(t2 + 1)2 = (t + 1)(t −1)3(t −i)2(t + i)2,
and suppose the minimal polynomial of T is (t + 1)(t −1)2(t2 + 1)2. What is
the rank of the nilpotent part of T on each invariant subspace of T?

328
10
The Structure Theory of Linear Mappings
10.2
The Jordan Canonical Form
The Jordan canonical form is one of the central results in linear algebra. It
asserts that if all the eigenvalues of A ∈Fn×n lie in F, then A is similar to a
matrix over F that is the direct sum of Jordan blocks. In particular, for an
endomorphism T : V →V having all its eigenvalues in F, there exists a basis
of V that is an eigenbasis for the semisimple part of T and simultaneously a
string basis, as deﬁned below, for the nilpotent part.
10.2.1
Jordan blocks and string bases
A Jordan block is an n × n matrix J over F of the form
J = λIn + N,
where λ ∈F and N = (nij), where
nij =

1
if j = i+1,
0
otherwise.
In other words, N is an upper triangular matrix with ones on its superdiag-
onal and zeros elsewhere. The matrix N is called a nilpotent Jordan block.
Notice that if n = 1, then J = (λ). An n × n matrix A is said to be in Jordan
canonical form if it has the form
A =
⎛
⎜
⎜
⎜
⎝
J1
O
· · ·
O
O
J2
· · ·
O
...
...
...
...
O
· · ·
O
Jm
⎞
⎟
⎟
⎟
⎠,
(10.5)
where J1, . . . , Jm are Jordan blocks.
Example 10.2. There are four 3 × 3 matrices in Jordan canonical form hav-
ing eigenvalue λ:
J1 =
⎛
⎝
λ
1
0
0
λ
1
0
0
λ
⎞
⎠, J2 =
⎛
⎝
λ
1
0
0
λ
0
0
0
λ
⎞
⎠, J3 =
⎛
⎝
λ
0
0
0
λ
1
0
0
λ
⎞
⎠and J4 = λI3.

10.2
The Jordan Canonical Form
329
The ﬁrst matrix, J1, is itself a Jordan block; J2 and J3 have two Jordan
blocks; and J4 has three Jordan blocks. The reader can check that J2 and J3
are similar.
⊠
Example 10.3. The 5 × 5 nilpotent matrix
N =
⎛
⎜
⎜
⎜
⎜
⎝
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
⎞
⎟
⎟
⎟
⎟
⎠
is a sum of two nilpotent Jordan blocks, one of size 3 × 3 and the other of
size 2 × 2. As a linear mapping, N sends
e3 →e2 →e1 →0 and e5 →e4 →0.
(10.6)
⊠
Deﬁnition 10.3. Suppose N : V →V is a linear mapping. A basis of V that
can be partitioned into disjoint subsets {w1, . . . , wℓ} such that N(w1) = 0
and N(wi) = wi−1 for i = 2, . . . , ℓis called an N-string basis. The subsets
{w1, . . . , wℓ} will be called N-strings.
In Example 10.6, the sequences in (10.6) give the N-strings, and
{e1, e2, e3, e4, e5} is the N-string basis. Note that if N = O, then the N-
strings consisting of the singletons {ei} form a string basis. But in fact,
every basis determines a string basis for the zero matrix.
Put schematically, N acts on the N-strings as follows:
wℓ→wℓ−1 →· · · →w1 →0.
The matrix of N with respect to the above N-string is the nilpotent Jordan
block of size (ℓ+ 1) × (ℓ+ 1), and the matrix of N with respect to a string
basis is a direct sum of nilpotent Jordan blocks.
10.2.2
Jordan canonical form
Here is the main result.
Theorem 10.8 (The Jordan canonical form). Let V be a ﬁnite-dimensional
vector space over F, and suppose T : V →V is an endomorphism whose
eigenvalues all lie in F. Then there exists an eigenbasis for the semisimple

330
10
The Structure Theory of Linear Mappings
part of T that is also a string basis for the nilpotent part of T. Thus, there
exists a basis of V for which the matrix A of T has the form
A =
⎛
⎜
⎜
⎜
⎝
J1
O
· · ·
O
O
J2
· · ·
O
...
...
...
...
O
· · ·
O
Jm
⎞
⎟
⎟
⎟
⎠,
(10.7)
where each Ji is a Jordan block. Note: the Jordan blocks do not necessarily
have diﬀerent eigenvalues.
Let us review the situation. By the Jordan–Chevalley decomposition the-
orem, V is the direct sum of the invariant subspaces C1, . . . , Cm of T. Hence,
choosing a basis of each Ci and taking the union of these bases gives a
basis of V for which the semisimple part S of T is the diagonal matrix
diag(λ1Iμ1, . . . , λmIμm), where μi = dim Ci. Thus, if we show that each Ci
has a string basis for the nilpotent part N of T, then we obtain a basis of V
for which the matrix of T is in Jordan canonical form. This will be done in
the next section.
10.2.3
String bases and nilpotent endomorphisms
We will now prove that a nilpotent endomorphism N : V →V of an arbitrary
ﬁnite-dimensional vector space has a string basis. We begin by noticing that
string bases have the following property.
Proposition 10.9. In a string basis of V to N, the number of strings is
dim ker(N).
Proof. Let the strings be v(i)
ki →· · · →v(i)
1
→0 for i = 1, . . . , ℓ. Certainly the
terminal vectors v(i)
1 , 1 ≤i ≤ℓ, span ker(N). Let w ∈ker(N), and use the
string basis to write w = 
i,j aijv(i)
j . Then N(w) = 0, so 
i,j aijv(i)
j−1 =
0, where v(i)
j−1 = 0 if j = 1. Thus, ai,j = 0 if j > 1. Consequently, w =

i ai1v(i)
1 . This proves that the v(i)
1 , 1 ≤i ≤ℓ, span ker(N). Since the v(i)
1
are linearly independent, dim ker(N) = ℓ.
□
We now prove the main result.
Proposition 10.10. Suppose N : V →V
is a nilpotent linear mapping.
Then V admits a string basis for N.

10.2
The Jordan Canonical Form
331
Proof. We will induct on dim V . The case dim V = 1 is obvious, so assume
that the proposition is true if dim V < r, and suppose dim V = r. We may
as well assume N ̸= O, so N(V ) ̸= {0}. Let W = ker(N) and U = im(N).
By the rank–nullity theorem (cf. Theorem 7.4) we have dim U + dim W =
dim V . Since N is nilpotent, dim W > 0, so dim U < dim V . By deﬁnition,
N(U) ⊂U, so N is a nilpotent linear mapping on U. Hence, dim(U ∩W) >
0. Now apply the induction hypothesis to U to get a basis of N-strings.
By Proposition 10.9, there are exactly dim(U ∩W) N-strings in every string
basis of U. Let
v(i)
ki →· · · →v(i)
2
→v(i)
1
(i = 1, . . . , ℓ= dim(U ∩W)) be these strings. Since v(i)
ki ∈im(N), there exists
v(i)
ki+1 ∈V such that N(v(i)
ki+1) = v(i)
ki . Thus we can form a new N-string
v(i)
ki+1 →v(i)
ki →· · · →v(i)
2
→v(i)
1
(10.8)
in V . Now adjoin to the strings in (10.8) additional wj ∈W so that the v(i)
1
and the wk form a basis of W. The number of wk is exactly dim W −dim(U ∩
W). Now count the number of vectors. We have dim U vectors from the strings
(10.8), and there are dim(U ∩W) of these strings, each contributing a basis
vector of W. Hence because of how the string basis of U was obtained, we
have a total of
dim U + dim(U ∩W) + (dim W −dim(U ∩W)) = dim U + dim W
vectors. But as noted above, dim U + dim W = dim V , so it suﬃces to show
that these vectors are independent. Thus suppose

i,j
aijv(i)
j
+

bkwk = 0.
(10.9)
Applying N to (10.9) gives an expression

i,j
aijv(i)
j−1 = 0,
where for each i, 1 < j ≤ki + 1. But then all these aij are equal to zero.
Hence the original expression (10.9) becomes

ai1v(i)
1 +

bkwk = 0.
Thus the remaining coeﬃcients are also zero, since the v(i)
1
and the wk form
a basis of W. Hence, we have constructed an N-string basis of V .
□

332
10
The Structure Theory of Linear Mappings
There is another way to prove the existence of a string basis that the reader
is invited to consider in the exercises. Since N(ker(N)) ⊂ker(N), it follows
that N induces a linear mapping N : V/ ker(N) →V/ ker(N). One sees that
N is also nilpotent, so one can again use induction.
Remark. Each Jordan block of a matrix A corresponds to a subspace of
one of A’s invariant subspaces, and the sum of the sizes of all the Jordan
blocks for a ﬁxed eigenvalue μ of A is the dimension of the invariant subspace
corresponding to μ.
Corollary 10.11. Every A ∈Fn×n whose characteristic polynomial decom-
poses into linear factors over F is similar over F to a matrix in Jordan canon-
ical form (10.7). In particular, every square matrix over C is similar over C
to a matrix in Jordan canonical form.
Example 10.4. Let us ﬁnd the Jordan canonical form of
A =
⎛
⎝
1
2
1
0
1
1
0
0
2
⎞
⎠.
Recall from Example 10.1 that if
P =
⎛
⎝
1
0
3
0
1
1
0
0
1
⎞
⎠,
then A is similar using P to a matrix in block diagonal form. Namely,
A = P
⎛
⎝
1
2
0
0
1
0
0
0
2
⎞
⎠P −1.
Thus, we want to ﬁnd a 2 × 2 matrix R such that
R
1
2
0
1

R−1 =
1
1
0
1

.
Let R = diag(1/2, 2). Conjugation by R multiplies the ﬁrst row by 1/2 then
multiplies the ﬁrst column by 2. Thus, increasing R to a 3 × 3 matrix in an
obvious way, we put

10.2
The Jordan Canonical Form
333
Q = PR′ =
⎛
⎝
1
0
3
0
1
1
0
0
1
⎞
⎠
⎛
⎝
2
0
0
0
1
0
0
0
1
⎞
⎠=
⎛
⎝
2
0
3
0
1
1
0
0
1
⎞
⎠.
Then
A = Q
⎛
⎝
1
1
0
0
1
0
0
0
2
⎞
⎠Q−1,
giving the Jordan canonical form of A.
10.2.4
Jordan canonical form and the minimal
polynomial
Let T : V →V be a linear mapping, and suppose μT (x) ∈F[x] denotes the
minimal polynomial of T. Let λ1, . . . , λm be the distinct eigenvalues of T. As
usual, we assume λ1, . . . , λm ∈F. Let C1, · · · , Cm be the invariant subspaces
of V , and let μi = dim Ci. Now
μT (x) = (x −λ1)a1 · · · (x −λm)am,
where each ai is greater than zero. If we choose a basis of each Ci and consider
the Jordan–Chevalley decomposition T = S + N, where S is the semisimple
part and N is the nilpotent part of T, then μT (T) = O means that each N ai
i
equals O on Ci. But this means that no nilpotent Jordan block of Ni can be
larger than ai × ai.
Here is an example.
Example 10.5. Let T : C9 →C9 be an endomorphism with characteristic
polynomial
(t + 1)2(t −1)3(t2 + 1)2 = (t + 1)(t −1)3(t −i)2(t + i)2,
and suppose its minimal polynomial is (t + 1)(t −1)2(t2 + 1)2. Let us try to
ﬁnd the Jordan canonical form of T. Let C1, C2, C3, and C4 be the invariant
subspaces for the eigenvalues −1, 1, i, −i respectively of T. Then dim C1 =
2, dim C2 = 3, dim C3 = 2, and dim C4 = 2. By the above comments, the
nilpotent part N of T is zero on C1, so T has two 1 × 1 Jordan blocks with
eigenvalue −1 on C1. Now, T has a 3 × 3 Jordan block with eigenvalue 1 on
C2. Finally, T has a 2 × 2 Jordan block for i on C3 and a 2 × 2 Jordan block
for −i on C4.
⊠

334
10
The Structure Theory of Linear Mappings
10.2.5
The conjugacy class of a nilpotent matrix
Since the eigenvalues of a nilpotent matrix are zero, the Jordan canonical
form is similar to a matrix of the form
N =
⎛
⎜
⎜
⎜
⎝
Jn1
O
· · ·
O
O
Jn2
· · ·
O
...
...
...
...
O
· · ·
O
Jns
⎞
⎟
⎟
⎟
⎠,
(10.10)
where Jni is the ni × ni nilpotent Jordan block and n1, . . . , ns are the lengths
of the strings in an N-string basis of V . By conjugating N with a permu-
tation matrix, we may also assume that the blocks are arranged so that
n1 ≥n2 ≥· · · ≥ns. Such a sequence n1 ≥n2 ≥· · · ≥ns of positive integers
such that n1 + n2 + · · · + ns = n is called a partition of n. Thus every nilpo-
tent n × n matrix over F determines a unique partition of n. But Theo-
rem 10.8 says that if two nilpotent n × n matrices over F determine the same
partition of n, then they are conjugate or similar by an element of GL(n, F).
Thus the conjugacy classes of nilpotent n × n matrices over F are in one-to-
one correspondence with the partitions of n. The partition function π(n) is
the function that counts the number of partitions of n. It starts out slowly
and grows rapidly with n. For example, π(1) = 1, π(2) = 2, π(3) = 3, and
π(4) = 5, while π(100) = 190, 569, 292. This unexpected connection between
partitions, which lie in the domain of number theory, and the conjugacy
classes of nilpotent matrices, which lie in the domain of matrix theory, has
led to some interesting questions.
Exercises
Exercise 10.2.1. Suppose V is a ﬁnite-dimensional vector space and T :
V →V is an endomorphism. Show that if ker(T) = ker(T 2), then ker(T m) =
ker(T) for all m > 0. What does this say about the minimal polynomial of T?
Exercise 10.2.2. Suppose T : V →V is an endomorphism, and suppose
there exists a T-string basis of V . Show that T is nilpotent.
Exercise 10.2.3. Find the Jordan canonical form of the matrices in parts
(i)–(iii) of Exercise 8.5.9.
Exercise 10.2.4. Let A be the 4 × 4 all-ones matrix over F2. Find the Jor-
dan canonical form of A and resolve the paradox.
Exercise 10.2.5. Let a ∈R3. Find the Jordan canonical form over C of the
cross product Ca : R3 →R3.
Exercise 10.2.6. Compute π(5) and write down all π(5) 5 × 5 nilpotent
matrices in Jordan canonical form with decreasing blocks along the diagonal.

10.2
The Jordan Canonical Form
335
Exercise 10.2.7. Show that the nilpotent matrix N in (10.10) is similar via
a permutation matrix to a nilpotent matrix N ′ in Jordan canonical form such
that the block sizes form a decreasing sequence.
Exercise 10.2.8. True or false. State your reasoning.
(i) Two matrices over F with the same characteristic polynomial must be
similar.
(ii) Two matrices with the same minimal polynomial must be similar.
Exercise 10.2.9. Show without appealing to Jordan canonical form that if
A ∈Fn×n and all the roots of pA(x) lie in F, then A is similar over F to an
upper triangular matrix. (Hint: Assume ﬁrst that A is nilpotent. Let k be
the least positive integer for which Ak = O. Then
N(A) ⊂N(A2) ⊂· · · ⊂N(Ak) = Fn.
Now construct a basis B of Fn such that B = MB
B(A) is upper triangular.
Then show that B can’t have any nonzero entries on its diagonal. Finally,
consider the Jordan–Chevalley decomposition of A.)
Exercise 10.2.10. Let N : V →V be a nilpotent linear mapping, where V
is a ﬁnite-dimensional vector space.
(i) Show that N(ker(N)) ⊂ker(N), and conclude that N induces a nilpotent
linear mapping N : V/ ker(N) →V/ ker(N).
(ii) Show by induction that V admits an N-string basis subordinate to N
using induction on dim V with part (i).
Exercise 10.2.11. This exercise requires the use of limits. Let A ∈Cn×n.
Show that A is nilpotent if and only if there exists a homomorphism ϕ : C∗→
GL(n, C) such that limt→0 ϕ(t)Aϕ(t)−1 = O. This fact is a special case of the
Hilbert–Mumford criterion.

Chapter 11
Theorems on Group Theory
Our treatment of matrix theory and the theory of ﬁnite-dimensional vector
spaces and their linear mappings is ﬁnished, and we now return to the theory
of groups. The most important results in group theory are of two types: deep
results about ﬁnite groups, and results about linear algebraic groups. Linear
algebraic groups are matrix subgroups of some GL(n, F) that are solutions
of polynomial equations. We give an introduction to this subject in the last
chapter. One of the deepest results in ﬁnite group theory is the theorem
that all ﬁnite groups of odd order are solvable. Another result, which is truly
remarkable, is that all the ﬁnite simple groups have now been described. This
was accomplished by a joint eﬀort of many mathematicians who ﬁlled in the
details of an ambitious program that eventually consumed more than 10,000
pages in mathematics research journals. The ﬁnal step in this project was the
discovery of a so-called sporadic simple group known as the monster, whose
order is
80801742479451287588645990461710757005754368 × 109,
a number that is said to exceed the number of elementary particles in the
universe. But the discovery of this monstrous group did not close the subject.
In fact, it opened up several fascinating questions in areas such as number
theory that are usually not related to group theory. Moreover, there are even
conjectures that the monster has deep yet to be discovered connections to
physics. Linear algebraic groups, which we will brieﬂy introduce in the last
chapter, are closely related to two of the major areas of pure mathematics,
algebraic geometry and representation theory. It is harder to state results in
this area succinctly, but we will make an attempt in the next chapter. In the
last ﬁfty years, there has been a massive amount of important research in
these three areas.
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 11
337

338
11
Theorems on Group Theory
Our starting point is the theory of group actions and orbits, which is
the collection of ideas necessary for the orbit stabilizer theorem. The main
consequence of the theory of orbits is Cauchy’s theorem and a collection
of theorems known as the Sylow theorems, originally proved in 1872 by a
Norwegian high-school teacher, Ludwig Sylow, which still form one of the
most basic sets of tools in the theory of ﬁnite groups. If G has order mpn,
where p is prime, n ≥1, and m is prime to p, then a subgroup H of order pn
is called a Sylow p-subgroup. The Sylow theorems describe several properties
of the Sylow subgroups of a ﬁnite group G. For example, according to the
ﬁrst Sylow theorem, G has a Sylow p-subgroup for every prime p dividing the
order of G, and by the second Sylow theorem, any two Sylow p-subgroups
of G are conjugate. The ﬁnite abelian groups are completely classiﬁed by
the Sylow theorems. The orbit method will also be used to show that the
only ﬁnite subgroups of SO(3, R) are the dihedral groups, cyclic rotation
groups, and the symmetry groups of the Platonic solids. This extends our
classiﬁcation of the ﬁnite subgroups of SO(2).
11.1
Group Actions and the Orbit Stabilizer
Theorem
We will now lay the foundation for a number of results on the structure of
ﬁnite groups. The basic notion is the idea of a group action, which leads
directly to a simple yet extremely powerful observation known as the orbit
stabilizer theorem, which is the basis for several counting arguments employed
below. Following the usual practice in group theory, from now on we will write
H < G (or equivalently, G > H) whenever H is a proper subgroup of G and
write H ≤G if H is a subgroup that may coincide with G.
11.1.1
Group actions and G-sets
Let X be a set (ﬁnite or inﬁnite), and recall that Sym(X) denotes the group
of all bijections of X.
Deﬁnition 11.1. A group G is said to act on X if there exists a homomor-
phism ϕ : G →Sym(X). If such a homomorphism ϕ exists, then g ∈G acts
on X by sending each element x ∈X to the element g · x deﬁned by
g · x = ϕ(g)(x).
A mapping ϕ : G →Sym(X) is a homomorphism if and only if for every
g, h ∈G and x ∈X, we have
(gh) · x = g · (h · x).
(11.1)

11.1
Group Actions and the Orbit Stabilizer Theorem
339
The corresponding mapping G × X →X sending (g, x) →g · x will be
called an action of G on X, and we will say that X is a G-set. Note that if
G acts on X, then the identity of G acts as the identity bijection on X; that
is, 1 · x = x for all x ∈X. In general, there may be other elements of G that
act as the identity on X, namely the elements of ker(ϕ).
Deﬁnition 11.2. Let X be a G-set, and suppose x ∈X. The set
G · x = {y ∈X | y = g · x ∃g ∈G}
is called the G-orbit of x. The stabilizer Gx of x is deﬁned by
Gx = {g ∈G | g · x = x}.
For each x ∈X, the mapping G →X deﬁned by g →g · x of G onto G · x is
called the orbit map associated to x. Finally, the action of G on X is said to
be transitive if given any pair of elements x, y ∈X, there exists g ∈G such
that g · x = y.
For example, every orbit G · x is also a G-set and G acts transitively (in
the obvious way) on G·x. In particular, the orbit map g →g·x of G to G·x is
surjective. The symmetric group S(n) acts transitively on Xn = {1, 2, . . . , n}.
Another transitive action is given in the following example.
Example 11.1. A group G acts on itself by left multiplication as follows:
if g ∈G, let Lg : G →G be the map given by Lg(h) = gh. This map
is called left translation by g. Left translation is a bijection of G, and the
mapping ϕ : G →Sym(G) deﬁned by ϕ(g) = Lg is a homomorphism, by
associativity. Recall that left translation was already used in the proof of
Cayley’s theorem (Theorem 2.7). In the proof of Proposition 2.5 it was shown
that left translation by g is a bijection of G.
⊠
Proposition 11.1. Let X be a G-set, and suppose x ∈X. Then the stabi-
lizer Gx is a subgroup of G. Moreover, the stabilizer of g·x is gGxg−1. Conse-
quently, if Gx is ﬁnite and x and y lie in the same G-orbit, then |Gx| = |Gy|.
The proof is left to the reader. We next show that a group action on X
deﬁnes an equivalence relation on X whose equivalence classes are exactly
the orbits of G.
Deﬁnition 11.3. Let X be a G-set, and let x, y ∈X. Then we say that x
is congruent to y modulo G if x ∈G · y.
Proposition 11.2. Let X be a G-set. Then congruence modulo G is an
equivalence relation on X whose equivalence classes are the G-orbits. There-
fore, the G-orbits give a decomposition of X into disjoint subsets.

340
11
Theorems on Group Theory
Proof. Since 1 · x = x, x ∈G · x, so every element of X is congruent to itself.
If x is congruent to y, then x ∈G · y. Thus x = g · y, so y = g−1 · x. Hence
y is congruent to x. Finally, if x is congruent to y, and y is congruent to z,
then x = g · y and y = h · z, so x = g · (h · z) = (gh) · z. Hence x ∈G · z, so
x is congruent to z. It follows that congruence modulo G is an equivalence
relation.
□
Corollary 11.3 (The orbit identity). Suppose a group G acts on a ﬁnite set
X. Then G has ﬁnitely many orbits, say O1, . . . , Or, and |X| = |O1| + · · · +
|Or|.
Example 11.2 (Left and right cosets). Suppose H is a subgroup of G. Then
H acts on G on the left by h · a = ha (h ∈H, a ∈G). The orbits of this
action are the right cosets Ha of H. Similarly, H acts on G on the right by
h · a = ah−1. The orbits of the right action are the left cosets aH of H.
⊠
Example 11.3 (Double cosets). Let H and K be subgroups of G. Then the
product group H × K acts on G by (h, k) · a = hak−1. The orbits of this
action have the form HaK. An orbit of the H × K-action on G is called a
double coset of the pair (H, K), or sometimes an (H, K)-double coset. Thus
such double cosets are either disjoint or equal.
⊠
Recall that in a ﬁnite group G, all cosets aH of a subgroup H have exactly
|H| elements. This isn’t true for double cosets, however. For example, the
number of elements in HaH is |H| if a ∈H, while |HaH| > |H| if a /∈H.
The reader may wish to ﬁnd a formula for |HaH|. (See Exercise 11.1.16.)
Example 11.4. We have already seen that GL(n, F) has a decomposition
GL(n, F) = L(n, F) · P(n) · D(n, F) · U(n, F),
(11.2)
where L(n, F) and U(n, F) are respectively the lower triangular and upper
triangular unipotent n × n matrices over F. Let T (n, F) = D(n, F) · U(n, F).
Then T (n, F) is the subgroup of GL(n, F) consisting of all invertible upper
triangular n × n matrices over F. Thus,
GL(n, F) = L(n, C) · P(n) · T (n, F)
is a double coset decomposition of GL(n, F) consisting of the double cosets
L(n, F)σT (n, F), where σ varies over the group P(n) of n × n permutation
matrices. This particular decomposition is called the Birkhoﬀdecomposition.
There is a similar decomposition
GL(n, F) = T (n, F) · P(n) · T (n, F),
called the Bruhat decomposition. We will discuss the Bruhat decomposition
in the ﬁnal chapter.
⊠

11.1
Group Actions and the Orbit Stabilizer Theorem
341
11.1.2
The orbit stabilizer theorem
We now come to the orbit stabilizer theorem, which gives a powerful method
for counting the number of elements in an orbit of a ﬁnite group.
Theorem 11.4 (The orbit stabilizer theorem). Let G be a ﬁnite group and
let X be a G-set. Then for every x ∈X, the mapping g →g · x induces a
bijection πx : G/Gx →G · x from the coset space G/Gx to the orbit G · x.
Hence
|G · x| = |G|
|Gx|.
In particular, |G · x| divides |G|.
Proof. The proof is similar to the proof of Lagrange’s Theorem. Fix x ∈X
and let H denote the stabilizer Gx. For each a ∈G and h ∈H,
(ah) · x = a · (h · x) = a · x.
Hence, we can deﬁne a map πx : G/H →G · x by setting πx(aH) = a · x
for every aH ∈G/H. This map is certainly surjective, since the orbit map
G →G · x is surjective. To ﬁnish the proof, we need to show that πx is
injective. Suppose πx(aH) = πx(bH). Then a · x = b · x, so a−1b · x = x. Thus
a−1b ∈H, and therefore aH = bH. Hence πx is injective. Since |G/H| = |G|
|H|
by Lagrange, the proof is complete.
□
11.1.3
Cauchy’s theorem
Lagrange’s theorem, which says that the order of a subgroup of a ﬁnite group
G divides the order of G, does not have a converse. For example, the alter-
nating group A(4) has order 12, but it has no subgroup of order 6. Our ﬁrst
application of the orbit stabilizer theorem is Cauchy’s theorem, which guar-
antees, however, that a group whose order is divisible by a prime p has an
element of order p. Cauchy’s proof of this theorem, which appeared in 1845,
was unsatisfactory, being very long, hard to understand, and, as was noticed
around 1980, logically incorrect, although the ﬂaw was ﬁxable. The theorem
itself is historically important as a precursor of the Sylow theorems. As we
will now see, it also is an example of a nontrivial result whose proof is made
both elegant and brief by a clever application of the orbit stabilizer theorem.
Proposition 11.5 (Cauchy’s theorem). A ﬁnite group G whose order is
divisible by a prime p has an element of order p.

342
11
Theorems on Group Theory
Proof. Deﬁne
Σ = {(a1, a2, . . . , ap) | all ai ∈G and a1a2 · · · ap = 1}.
Since (a1, a2, . . . , ap) ∈Σ if and only if ap = (a1a2 · · · ap−1)−1, it follows that
|Σ| = |G|p−1. Thus p divides |Σ|. Note that the cyclic group Cp =< ζ > of
order p acts on Gp = G × G × · · · × G by rightward cyclic shifts. That is, for
each r = 1, . . . , p,
ζr · (g1, g2, . . . , gp) = (gp−r+1, . . . , gp, g1, . . . , gp−r).
This action is well deﬁned on Σ, since if a1a2 · · · ap = 1, then (ajaj+1 · · · ap)
(a1 · · · aj−1) = 1 also, due to the fact that in every group, if xy = 1, then
yx = 1 too. Since |Cp| = p, the stabilizer of an arbitrary element of Σ has
order one or p. Thus, the orbit stabilizer theorem implies that every orbit
of Cp has either one or p elements. Since Σ is the disjoint union of orbits,
it follows that the number of elements of Σ whose orbits consist of a single
element is divisible by p. But the orbit of σ = (1, 1, . . . , 1) has one element,
so there are at least p −1 more elements τ ∈Σ such that Cp · τ = {τ}. Every
such τ has the form (t, t, . . . , t), where t ̸= 1, so it follows that tp = 1.
□
From Cauchy’s theorem one immediately concludes the following.
Corollary 11.6. If G is ﬁnite and the order of every element of G is divisible
by p, then the order of G is pn for some n.
In the second application of the orbit stabilizer theorem, we will prove
the formula for the order of the rotation group of a Platonic solid stated in
Section 8.7.
Proposition 11.7. If P is a Platonic solid with f faces and if each face has
e edges, then |Rot(P)| = ef.
Proof. Let X denote the set consisting of the f faces of P. Now, Rot(P) acts
on X, and it can be shown that for every two faces, there is an element of
Rot(P) that takes one face into the other. To see this, one has only to verify
that every two adjacent faces can rotated one into the other in Rot(P), which
can be seen by inspection. The orbit stabilizer theorem says that |Rot(P)| =
f|H|, where H is the subgroup of Rot(P) consisting of all rotations that ﬁx a
face. But the faces are regular polygons with e edges, so H has order at most
e. In fact, the order of H is exactly e. This is evident for the cube, tetrahedron,
and octahedron. It also holds for the dodecahedron and icosahedron, but we
will skip the details. Therefore, |Rot(S)| = ef.
□

11.1
Group Actions and the Orbit Stabilizer Theorem
343
11.1.4
Conjugacy classes
The action of G on itself deﬁned by g · a = gag−1 is called the conjugation
action. We have already seen this action in a number of contexts, for example
in the question of when an invertible matrix is diagonalizable. Recall that
an orbit of this action is called a conjugacy class. Conjugation deﬁnes an
action, since the mapping ϕ : G →Sym(G) deﬁned by ϕ(g)(a) = gag−1 is a
homomorphism. Recall that ϕ(g) is in fact the inner automorphism σg. We
will denote the conjugacy class of a ∈G by Ga.
Deﬁnition 11.4. If a ∈G, the centralizer ZG(a) of a is deﬁned to be
ZG(a) = {g ∈G | gag−1 = a}.
In other words, the centralizer of a consists of all g ∈G commuting with
a. Note that the cyclic group < a > generated by a satisﬁes < a >≤ZG(a).
Clearly, ZG(a) is the stabilizer Ga of a ∈G for the conjugation action, so
ZG(a) is a subgroup of G. If a ∈Z(G), the center of G, then ZG(a) = G.
The orbit stabilizer theorem applied to the conjugation action of G on itself
has the following consequence.
Proposition 11.8. Let G be ﬁnite. Then for every a ∈G, the number of ele-
ments of the G-conjugacy class Ga is |G|/|ZG(a)|. Consequently, |Ga| divides
|G|.
To test the power of our results so far, let us consider what what can be
said about groups of order 15.
Example 11.5 (Groups of order 15). By Cauchy’s theorem, G contains ele-
ments a and b of orders 3 and 5 respectively. A natural question is whether
the center of G is trivial. If not, then |Z(G)| =3, 5, or 15. Any one of these
possibilities implies that G/Z(G) is cyclic, so G is abelian by the result in
Exercise 11.1.7. But if G is abelian, then ab has order 15, so G = C15. So sup-
pose Z(G) = (1). Since G is the union of the distinct conjugacy classes and
the order of a conjugacy class divides |G|, we have |G| = 15 = 1 + 3m + 5n,
where m is the number of conjugacy classes of order 3, n the number of order
5, and 1 is the order of the conjugacy class of the identity. The only solution
to this equation in nonnegative integers is m = 3 and n = 1. Thus, three con-
jugacy classes have order 3, and one has order 5. Consider a conjugacy class
Gx such that |Gx| = 3. By the orbit stabilizer theorem, |ZG(x)| = 5. Since
x ∈ZG(x), it follows that x has order 5, so all elements of Gx have order 5,
since the elements in a conjugacy class all have the same order. This gives
nine elements of order 5. But by Proposition 2.13, the number of elements
of order p in G is divisible by p −1. Since 4 does not divide 9, we obtain
a contradiction. Thus, |Z(G)| > 1, so by the remarks above, G is the cyclic
group C15.
⊠

344
11
Theorems on Group Theory
To summarize, we have
Proposition 11.9. If G has order 15, then G is cyclic.
11.1.5
Remarks on the center
Recall from Section 2.1 that the center of a group G is deﬁned to be the
subgroup
Z(G) =

g∈G
ZG(g) = {g ∈G | ag = ga ∀a ∈G}.
The center of G is a normal subgroup. Note that if G is ﬁnite, then the order
of its center is the number of elements of G whose conjugacy class consists
of a single element. We now consider a nontrivial example.
Proposition 11.10 (The center of GL(n, F)). The center of GL(n, F) con-
sists of the subgroup of scalar matrices {cIn | c ∈F∗}.
Proof. Let C belong to the center of GL(n, F). We will ﬁrst show that C is
diagonal. If the characteristic of F is greater then two, then any matrix C
commuting with all elementary matrices of type I is diagonal. If the char-
acteristic is two, then row operations of type three must also be used to
see that C is diagonal. But a nonsingular diagonal matrix that commutes
with any row swap matrix has the form C = cIn for some c ∈F∗, so
we are done.
□
Let Z denote the center of GL(n, F). The quotient group GL(n, F)/Z is
denoted by PGL(n, F) and is referred to as the projective general linear group.
By a similar argument, the center of SL(n, F) consists of all cIn such that
cn = 1. For example, if F = C, then Z is the group of nth roots of unity.
11.1.6
A ﬁxed-point theorem for p-groups
We have already shown that G is a p-group if and only if every subgroup of
G is also a p-group. We will now prove a well known result about p-groups
which is a consequence of a ﬁxed-point formula that we will also use in the
proof of the Sylow theorems.
Suppose G is a group acting on a set X. We say that x ∈X is a G-ﬁxed
point if Gx = G, that is, the stabilizer Gx of x is all of G. Let XG denote the
set of G-ﬁxed points in X. The ﬁxed-point formula goes as follows.

11.1
Group Actions and the Orbit Stabilizer Theorem
345
Proposition 11.11. Suppose G is a p-group acting on a ﬁnite set X such
that p divides |X|. Then |XG| is also divisible by p.
Proof. Let O1, . . . , Or be the orbits of G. Since |Oi| = |G|/|Gx| for every
x ∈Oi, it follows that either |Oi| = 1 or |Oi| is divisible by p. By the orbit
identity, |X| =  |Oi|, and the fact that p divides |X|, it follows that the
number of orbits such that |Oi| = 1 is also divisible by p. This is |XG|, which
gives the result.
□
Now G acts on itself by conjugation, and the set of ﬁxed points for this
action is the center Z(G) of G. Thus we have the following corollary.
Corollary 11.12. The center of a p-group G is nontrivial.
Proof. Since the ﬁxed-point set of the conjugation action is Z(G) and 1 ∈
Z(G), it follows that |Z(G)| = mp for some m ≥1.
□
Here is another corollary.
Proposition 11.13. If |G| = p2, then G is abelian.
The proof is left to the reader.
□
11.1.7
Conjugacy classes in the symmetric group
We will now derive a well-known result describing the conjugacy classes in
S(n). To this end, we need to introduce cycles and disjoint cycle notation.
Deﬁnition 11.5. An element σ of S(n) is called a k-cycle if the following
two conditions hold:
(i) There exists an integer i in [1, n] such that i, σ(i), σ2(i), . . . , σk−1(i) are
all distinct and σk(i) = i.
(ii) σ(j) = j for all j in [1, n] distinct from σm(i) for all m = 1, . . . , k.
The k-cycle σ as deﬁned above will be denoted by (i σ(i) σ2(i) · · · σk−1(i)).
There are other possible representations of this k-cycle, as the following exam-
ple illustrates.
Example 11.6. For example, a transposition (ij) is a 2-cycle. The permu-
tation σ = [2, 3, 4, 1] in S(4) that sends 1 to 2, 2 to 3, 3 to 4, and 4 to 1 is
the 4-cycle (1234). It can also be represented as (2341), (3412), or (4123). In
other words, a representation of a k-cycle in S(n) is not unique.
⊠

346
11
Theorems on Group Theory
The ambiguity in cycle notation pointed out in the above example can be
avoided by letting the leading entry of a cycle (a1a2 . . . ak) be the least integer
among the ai. The identity is represented by the cycle (1). Cycles are mul-
tiplied, like transpositions, by composing their permutations. For example,
(123)(13) = [1, 3, 2, 4] = (23). Two cycles that don’t share a common letter
such as (13) and (24) are said to be disjoint. Disjoint cycles have the property
that they commute, since they act on disjoint sets of letters. Hence the prod-
uct of two or more disjoint cycles can be written in any order. For example,
in S(6), we have (13)(24)(56) = (56)(24)(13). Nondisjoint cycles, however, do
not in general commute: for example, nondisjoint transpositions never com-
mute, since (ab)(bc) = (abc), while (bc)(ab) = (acb). Two more examples are
(123)(13) = (23) (as above), while (13)(123) = (12); also (123)(34) = (1234),
while (34)(123) = (1243). The important point is that every element of S(n)
can expressed as a product of disjoint cycles, as we now prove.
Proposition 11.14. Every element σ of S(n) diﬀerent from (1) can be writ-
ten as a product of one or more disjoint cycles of length greater than one.
The disjoint cycles are unique up to their order.
Proof. Given σ, we can construct a cycle decomposition as follows. Let i
be the ﬁrst integer in [1, n] such that σ(i) ̸= i. Now consider the sequence
i, σ(i), σ2(i), . . . . Since all σj(i) ∈[1, n], there has to be a least k > 1 such
that σk(i) ∈{i, σ(i), . . . , σk−1(i)}. I claim that σk(i) = i. For if σk(i) = σℓ(i),
where ℓsatisﬁes 1 ≤ℓ< k, then σ(σk−1(i)) = σ(σℓ−1(i)). Thus σk−1(i) =
σℓ−1(i), since σ is a bijection. Since ℓ< k, this contradicts the deﬁnition of
k. Hence σk(i) = i. It follows that (iσ(i) · · · σk−1(i)) is a k-cycle. Now repeat
this construction starting with the least j ∈[1, n] such that j > i and j does
not occur in the cycle we have just constructed. Proceeding in this way, we
eventually construct a family of disjoint cycles whose product is σ such that
each j ∈[1, n] such that σ(j) ̸= j belongs to exactly one cycle (and no j such
that σ(j) = j appears in any cycle). For the uniqueness, suppose σ has two
disjoint cycle representations, say σ = c1 · · · ck = c′
1 . . . c′
ℓ. Assuming that i is
the least integer such that σ(i) ̸= i, it follows that i occurs in some cj and
some c′
m. We can assume that j = m = 1, so by construction, c1 = c′
1. By
repeating the argument, we may conclude that each ci is a c′
j for a unique j
and conversely, so the disjoint cycle representations of σ coincide up to order
of the factors.
□
Example 11.7. Let’s express the element σ = [2, 3, 1, 5, 4] ∈S(5) as a prod-
uct of disjoint cycles. Now, σ(1) = 2, σ(2) = 3, σ(3) = 1. Thus, (123) will be
a cycle. The other will be (45), so σ = (123)(45). Similarly, τ = [5, 3, 2, 1, 4]
will have (154) and (23) as its disjoint cycles, so τ = (154)(23)
⊠
We can now describe the conjugacy classes in S(n).

11.1
Group Actions and the Orbit Stabilizer Theorem
347
Proposition 11.15. Two elements of S(n) are conjugate if and only if their
disjoint cycle representations have the same number of cycles of each length.
Proof. Suppose σ is a k-cycle, say σ = (i σ(i) · · · σk−1(i)). Then ψ = τστ −1
is also a k-cycle. To see this, let τ −1(j) = i. Since ψk = (τστ −1)k = τσkτ −1,
it follows that ψk(j) = τσk(i) = τ(i) = j. If m < k, then ψm(j) ̸= j, so
(j ψ(j) · · · ψk−1(j)) is a k-cycle. But if ψ contains another cycle, then revers-
ing this argument shows that σ also contains another cycle, which cannot
happen by assumption. Thus ψ is a k-cycle. It follows that two elements in
the same conjugacy class have the same cycle structure. For if σ = c1 · · · ck,
where the ci are mutually disjoint cycles, then
τστ −1 = (τc1τ −1)(τc2τ −1) · · · (τckτ −1),
and the cycles (τciτ −1) are also mutually disjoint. Conversely, if two per-
mutations ψ and σ have the same disjoint cycle structure, then they are
conjugate. To see this, let ψ = c1 · · · cs and σ = d1 · · · ds be disjoint cycle
representations of ψ and σ, where cr and dr have the same length for each r.
Write cr = (i ψ(i) · · · ψm(i)) and dr = (j σ(j) · · · σm(j)), and let τr ∈S(n)
be the product of the transpositions (σk(i) ψk(j)) for k = 0, . . . , m−1. These
transpositions commute pairwise, so the order in the product is irrelevant.
We leave it to the reader to check that τrcrτ −1
r
= dr, while τjcrτ −1
j
= cr for
j ̸= r. Consequently, if τ = τ1 · · · τs, then τψτ −1 = σ. Therefore, ψ and σ are
conjugate.
□
For example, (12)(34) and (13)(24) must be conjugate, since they are prod-
ucts of two disjoint 2-cycles. Clearly (23)(12)(34)(23) = (13)(24).
Remark. Therefore, each conjugacy class of S(n) corresponds to a unique
partition n = a1 + a2 + · · · + am, where a1 ≥a2 ≥· · · ≥am > 0, and
conversely, every such partition of n determines a unique conjugacy class.
Recall also that by Jordan canonical form, the conjugacy classes of nilpotent
n×n matrices over C are also in one-to-one correspondence with the partitions
of n. It follows that conjugacy classes of nilpotent matrices over C are in one-
to-one correspondence with the conjugacy classes of P(n), the group of n×n
permutation matrices.
Exercises
Exercise 11.1.1. Let G be an arbitrary group. The conjugation action is
the action of G on itself deﬁned by g · h = ghg−1. Show that the conjugation
action is indeed a group action.
Exercise 11.1.2. Show that if X is a G-set and y = g·x, then Gy = gGxg−1.

348
11
Theorems on Group Theory
Exercise 11.1.3. Does g · a = ag deﬁne an action of a group G on itself? If
not, adjust the deﬁnition so as to get an action.
Exercise 11.1.4. Prove Proposition 11.1.
Exercise 11.1.5. Show that a subgroup H of a group G is normal in G if
and only if H is a union of G-conjugacy classes.
Exercise 11.1.6. Show directly that ZG(a) is a subgroup of G.
Exercise 11.1.7. • Show that if G/Z(G) is cyclic, then G is abelian (and
hence G = Z(G)).
Exercise 11.1.8. Let G = S(3). Write out all three G-conjugacy classes.
Also, ﬁnd all six centralizers.
Exercise 11.1.9. Recall the signature homomorphism sgn : S(n) →{±1}.
Show that if σ ∈S(n) is a k-cycle, then sgn(σ) = (−1)k−1.
Exercise 11.1.10. Write out the conjugacy classes for the dihedral groups
D(3) and D(4). (Think geometrically.)
Exercise 11.1.11. Describe the centers of the dihedral groups D(n) for n =
3 and 4.
Exercise 11.1.12. Let p and q be distinct primes. Show that a group of
order pq is abelian and hence cyclic if and only if its center is nontrivial.
Show by example that there exist groups of order pq whose center is trivial.
Exercise 11.1.13. Generalize the fact that all groups of order 15 are cyclic
by showing that if p and p + 2 are both prime, then every group of order
p(p + 2) is cyclic.
Exercise 11.1.14. Show that if |G| = 65, then G is cyclic.
Exercise 11.1.15. Suppose |G| = 30. Show that there are two elements of
order 3 and four of order 5. How many elements of order 2 are there?
Exercise 11.1.16. •
Let G be a ﬁnite group and H a subgroup. Find a
formula for the order of the double coset |HaH| if a /∈H.
Exercise 11.1.17. Let σ = (1632457) ∈S(7).
(i) Find the disjoint cycle representation of σ.
(ii) What is the partition of 7 corresponding to σ?
(iii) Determine the sign sgn(σ) of σ.
(iv) Finally, express σ as the product of simple transpositions.

11.2
The Finite Subgroups of SO(3, R)
349
11.2
The Finite Subgroups of SO(3, R)
Before continuing our treatment of group theory, we will pause to give a
geometric application of the orbit method. Our plan is to determine the
orders of the polyhedral groups, that is, the ﬁnite subgroups of SO(3, R),
and give a partial classiﬁcation of these groups that will extend the result
that every ﬁnite subgroup of O(2, R) is either a cyclic group Cm consisting
of m rotations or a dihedral group D(m) consisting of m rotations and m
reﬂections. In both cases, Cm and D(m) act on the m-sided regular polygon
{m} centered at the origin in R2. The complete classiﬁcation of the polyhedral
groups is a natural extension to the Platonic solids of the two-dimensional
classiﬁcation, but the proof is much more complicated, and we will skip some
of the details. It turns out that the list of ﬁnite subgroups of SO(3, R) is brief
and easy to state. Here is the result.
Theorem 11.16. Every polyhedral group is isomorphic to one of Cm, D(m),
A(4), S(4), A(5), and each of these groups is the rotation group of a convex
polyhedron in R3.
Polyhedral groups of orders 12, 24, and 60 have already been discussed in
Section 8.7.2. They occur as the rotation groups of the Platonic solids (see
Section 8.7).
11.2.1
The order of a ﬁnite subgroup of SO(3, R)
Let G be polyhedral. Our plan is to apply the orbit stabilizer theorem to
count the number of elements in G in terms of the set of what are known
as its poles. We ﬁrst introduce some notation. Let G× = G \ {I3}, and let
S2 = {(x, y, z) ∈R3 | x2 + y2 + z2 = 1} be the unit sphere in R3. A point
p of S2 is called a pole of G if the stabilizer Gp satisﬁes Gp ̸= {I3}. Hence
each pole is on the axis of rotation of some σ ∈G×. Since G consists of linear
mappings, Gp = G−p for every p ∈S2. Let P denote the set of all poles of G,
and note that P = −P. Hence we can write P = P+∪P−, where P−= −P+,
and the union is disjoint. Since each σ ∈G× ﬁxes a unique doubleton, and
if p, q ∈S2 satisfy q ̸= ±p, then Gq ∩Gp is empty, and it follows that G×
can be expressed as a disjoint union
G× =

p∈P+
(Gp)×.
This leads to the observation that
|G×| =

p∈P+
|(Gp)×|.

350
11
Theorems on Group Theory
Consequently,
|G| −1 =

p∈P+
(|Gp| −1).
It is now convenient to reformulate this identity. First of all,
|G| −1 = 1
2

p∈P
(|Gp| −1).
(11.3)
We leave it to the reader to check that P is stable under G. Let us call the
G-orbit G · p of p ∈P a polar orbit. Since G is ﬁnite, we can choose distinct
polar orbits G · p1, G · p2, . . . , G · pk such that
P = G · p1 ∪G · p2 ∪· · · ∪G · pk.
We showed that |Gp| = |Gq| if p and q are in the same orbit. Moreover,
|G| = |G · p||Gp|. Thus,
2(|G| −1) =

p∈P
(|Gp| −1)
=
k

i=1
 
p∈G·pi
(|Gp| −1)

=
k

i=1
|G · pi|(|Gpi| −1)
=
k

i=1
(|G| −|G · pi|)
= |G|
k

i=1
(1 −
1
|Gpi|).
Therefore, we have proved the following proposition.
Proposition 11.17. Suppose G is a polyhedral group having polar orbits
G · p1, G · p2, . . . , G · pk. Then
2(1 −1
|G|) =
k

i=1
(1 −
1
|Gpi|).
(11.4)
The upshot of this elegant identity is that |G| depends only on the orders
|Gpi| of the stabilizers of its poles.

11.2
The Finite Subgroups of SO(3, R)
351
11.2.2
The order of a stabilizer Gp
Let us now apply (11.4) to investigate the possible values of |G|. First of all,
if p is a pole, then |Gp| ≥2, so
1
2 ≤1 −
1
|Gp| ≤1.
But
2(1 −1
|G|) < 2,
and hence there cannot be more than three poles. Thus, k = 1, 2, or 3. So
we need to consider these three cases.
(1) Suppose k = 1. Then G acts transitively on the poles. Let G · p be the
unique orbit. Thus |G| = |G · p||Gp|, so using the formula for |G| gives
2(|G · p||Gp| −1) = |G · p|(|Gp| −1).
Simplifying gives |G · p|(|Gp| + 1) = 2. This is impossible, since |Gp| + 1 ≥3,
so G cannot act transitively on the poles.
(2) Next, let k = 2 and let G · p and G · q be the two orbits. Then
2(1 −1
|G|) = 2 −
1
|Gp| −
1
|Gq|.
Canceling and cross multiplying by −|G| gives us that
2 = |G|
|Gp| + |G|
|Gq| = m + n,
where m and n are positive integers by Lagrange. Thus m = n = 1 and
G = Gp = Gq. In other words, every element of G ﬁxes both p and q. Since
every rotation has an axis, this means that G has just one axis, and q = −p.
Thus G consists of rotations about the line ℓ= Rp. In other words, G is
determined by a ﬁnite subgroup of rotations of the plane orthogonal to ℓ. In
this case, G is cyclic.
(3) Now suppose G has three polar orbits G · p, G · q, and G · r. Then the
order formula gives
2(1 −1
|G|) = 3 −
1
|Gp| −
1
|Gq| −
1
|Gr|.

352
11
Theorems on Group Theory
Consequently,
1 + 2
|G| =
1
|Gp| +
1
|Gq| +
1
|Gr| > 1.
This implies that the triple (|Gp|−1, |Gq|−1, |Gr|−1) has to be one of the
following:
(1
2, 1
2, 1
m),
(1
2, 1
3, 1
3),
(1
2, 1
3, 1
4),
(1
2, 1
3, 1
5).
The order formula (11.4) says that |G| is respectively 2m, 12, 24, or 60.
Let us analyze each case. Assume ﬁrst that (|Gp|−1, |Gq|−1, |Gr|−1)
= (1
2, 1
2, 1
m). If m = 1, then |G| = 2, so there can only be two poles.
Hence, this case cannot occur. Suppose m = 2. Then G has order four.
Thus G is abelian, and since G has three polar orbits, it cannot be cyclic.
Hence every element σ of G except 1 has order two. Suppose σ ̸= 1.
Then σ is a rotation through π about its axis. Thus, σ is diagonalizable
with orthonormal eigenvectors, say u, v, w and corresponding eigenvalues
1, −1, −1. It follows that every element of G is diagonalizable, and since
G is abelian, all elements of G are simultaneously diagonalizable. Hence
the only possibility is that G is conjugate in SO(3, R) to the subgroup
H = {I3, diag(1, −1, −1), diag(−1, 1, −1), diag(−1, −1, 1)}. Here, the polar
orbits for G are {±u}, {±v}, and {±w}. Notice that if α, β, γ denote the
three rotations in H, then α2 = β2 = γ2 and αβ = γ. Thus H is isomorphic
to the dihedral group D(2).
Now suppose m > 2. Since |G| = 2m, |G·p| = |G·q| = m, while |G·r| = 2.
But G·(−r) = −G·r, so |G·(−r)| = 2 also. Since m > 2, the only possibility
is that G · r = G · (−r) = {± r}, so σ(r) = ± r for every σ ∈G. Now let
P denote the subspace (Rr)⊥of R3 orthogonal to Rr. Then for every σ ∈G
and p ∈P, we have
0 = r · p = σ(r) · σ(p) = ± r · σ(p).
Thus the whole group G acts on P. Now choose an orthonormal basis u1, u2
of P, and deﬁne a linear mapping ϕ : P →R2 by requiring ϕ(ui) = ei for
i = 1, 2. Then ϕ is a isometry; that is, ϕ(x · y) = x · y for all x, y ∈P.
Next, put τ(σ) = ϕσϕ−1 for all σ ∈G. Then τ(σ) ∈O(2, R). To see this, put
μ = τ(σ). Then, as the reader should verify, μ(a)·μ(b) = a·b for all a, b ∈R2,
so μ is an orthogonal linear mapping by Proposition 7.7. Consequently, we
have deﬁned a mapping τ : G →O(2, R). The reader can check that τ is a
homomorphism, so the image τ(G) is a ﬁnite subgroup of O(2, R). By the
deﬁnition of τ, it follows that the eigenvalues of σ on P are the eigenvalues
of τ(σ) on R2. Thus τ(G) contains both rotations and reﬂections, so τ(G) is
a dihedral subgroup of O(2, R) consisting of all orthogonal linear mappings
ﬁxing a regular polygon {m} in R2. But observe that in fact, τ is injective.
For if τ(σ) = I2, then the eigenvalues of σ on P are both 1. But this means

11.2
The Finite Subgroups of SO(3, R)
353
that σ(r) = r too; for otherwise, σ(r) = −r, so det(σ) = −1, contradicting
the assumption that σ ∈SO(3, R). Hence, if τ(σ) = I2, then σ = I3, and
consequently τ is injective, since its kernel is trivial. We conclude that G is
isomorphic to the dihedral group D(m) of order 2m.
Next assume that G has the triple (1
2, 1
3, 1
3). Then the order of G is 12. If
one studies a list of all groups of order 12, it turns out that the only possibility
is that G is the rotation group of a central regular tetrahedron. The poles are
the four vertices v, the four midpoints e of the edges, and the four centers
c of the faces, and G acts transitively on each class of pole. Thus the polar
orbits have |G · v| = 4, |G · e| = 6, and |G · c| = 4|, which veriﬁes that the
orders of the stabilizers are 3, 2, 3.
For (1
2, 1
3, 1
4), the order of G is 24. We have already shown that the rotation
group of a central cube is isomorphic to S(4). As in the previous case, the
poles consist of the eight vertices v, the four midpoints e of the twelve edges,
and the centers c of the six faces, and G acts transitively on the set of poles of
each type. It can be shown that this realization of S(4) is the only polyhedral
group of order 24, but the proof is complicated.
The ﬁnal case (1
2, 1
3, 1
5) gives a group G of order 60. Recall from Remark
8.7.5 that an icosahedron is a regular solid with 12 vertices, 30 edges, and 20
triangular faces. Thus, the rotation group of a central icosahedron has order
60, which proves the existence of a polyhedral group of order 60. One may
take the vertices of I to be the points
(±1, ±φ, 0), (0, ±1, ±φ), (±φ, 0, ±1),
where φ = (1+
√
5)
2
is the golden ratio encountered in Chap. 8. At each of the
vertices, there are ﬁve adjoining triangular faces. Therefore, for each vertex
v, |Gv| = 5. The centers of the faces m are also poles, and clearly |Gm| = 3.
The other poles consist of the midpoints e of the edges with |Ge| = 2.
There is an amusing way of showing that G is isomorphic to A(5). It
turns out that one can color the 20 faces of I with ﬁve colors so that no two
adjoining faces have the same color and each color is used four times. If the
colors are labeled 1,2,3,4,5, then G, which permutes I’s 20 faces, becomes
the set of all permutations of {1, 2, 3, 4, 5} with signature +1. In other words,
G ∼= A(5).

354
11
Theorems on Group Theory
11.3
The Sylow Theorems
Given a ﬁnite group G, what can one say about its subgroups? Both
Lagrange’s theorem and Cauchy’s theorem make assertions about the sub-
groups of G in terms of the order of G, which is the most basic invariant of
a ﬁnite group. The Sylow theorems, which were discovered by Sylow in 1872
during his research on algebraic equations, give much more precise informa-
tion. Suppose |G| = pnm, where p is a prime that is prime to m and n ≥1.
A subgroup of G of order pn is called a Sylow p-subgroup. Sylow’s theorems
assert the following: if 1 ≤k ≤n, there exists a subgroup H of order pk,
and every such H is contained in a Sylow p-subgroup; all Sylow p-subgroups
for a given prime p are conjugate in G; and ﬁnally, the number of Sylow
p-subgroups of G is congruent to one modulo p and divides |G|. The proofs
given below are elegant and brief, and above all, they explain why the the-
orems are true. The tools used are Cauchy’s theorem, the orbit stabilizer
theorem, and the ﬁxed-point result for p-groups in Proposition 11.11.
11.3.1
The ﬁrst Sylow theorem
As above, let G be a ﬁnite group and p a prime p dividing |G|, say |G| = pnm,
where (p, m) = 1. We now prove the ﬁrst Sylow theorem, which ensures that
Sylow p-subgroups always exist and a bit more.
Theorem 11.18. For each k such that 1 ≤k ≤n, G contains a subgroup of
order pk, and every subgroup of order pk is contained in a Sylow p-subgroup
of G.
Proof. Suppose H is a subgroup of G of order pk, where 1 ≤k < n. We will
show that H is contained in a subgroup K of G of order pk+1. Iterating this
argument will yield a subgroup of G of order pn that contains H, giving the
required Sylow p-subgroup. The existence of a subgroup of order pk, for each
k ≤n, then follows by applying this result to the subgroup generated by an
element of order p, whose existence is guaranteed by Cauchy’s theorem. Now,
H acts on X = G/H by left translation. Moreover, H is a p-group, and |X|
is divisible by p. Therefore, Proposition 11.11 tells us that either the ﬁxed-
point set XH is empty or |XH| = jp for some j > 0. But 1H ∈XH, so the
latter holds. Note that gH ∈(G/H)H if and only if gH ∈NG(H)/H, since
HgH = gH if and only if gHg−1 = H. Thus, NG(H)/H is a group whose
order is divisible by p. By Cauchy’s theorem, NG(H)/H contains a subgroup
K of order p. Consider the inverse image H′ = π−1(K) of the quotient
map π : NG(H) →NG(H)/H. Then H′ is a subgroup of NG(H) containing
H, which π maps to K. Thus, by the isomorphism theorem, Theorem 2.20,
H′/H ∼= K, so [H′ : H] = |K| = p. But Lagrange’s theorem implies that
|H′| = pk+1, so the proof is complete.
□

11.3
The Sylow Theorems
355
11.3.2
The second Sylow theorem
The second Sylow theorem asserts that for a given prime p, every pair of
Sylow p-subgroups of G are conjugate. This says that a ﬁnite group G is
rigid in the sense that all subgroups of G of certain orders are isomorphic by
an inner automorphism of G.
Theorem 11.19. Every pair of Sylow p-subgroups of a ﬁnite group G are
conjugate, and if p divides |G|, then the number of Sylow p-subgroups of G
also divides |G|.
Proof. Let H and K be Sylow p-subgroups of G, and let H act on G/K. It
suﬃces to show that the ﬁxed-point set (G/K)H is nonempty, say HgK =
gK. For if HgK = gK, it follows that gKg−1 = H. Suppose (G/K)H is
empty. Then the orbit stabilizer theorem implies that for every H-orbit O,
|O| is a multiple of p, since H is a p-group. This implies that p divides |G/K|,
which contradicts the assumption that |K| = pn. Consequently, every two
Sylow p-subgroups are conjugate. Thus G acts transitively via conjugation
on the set of its Sylow p-subgroups, so by another application of the orbit
stabilizer theorem, the number of Sylow p-subgroups of G divides |G|.
□
The fact that Sylow p-subgroups are conjugate gives the following result.
Corollary 11.20. If G is a ﬁnite abelian group, then G contains a unique
Sylow p-subgroup for every prime p dividing |G|.
11.3.3
The third Sylow theorem
The third Sylow theorem is analogous to the result that in a ﬁnite group, the
number of elements of order p is divisible by p −1.
Theorem 11.21. If G is a ﬁnite group and p is a prime dividing |G|, then
the number of Sylow p-subgroups of G is congruent to 1 modulo p and divides
|G|.
Proof. Assume that the prime p divides |G|, and let Sp denote the set of Sylow
p-subgroups of G. Fix S ∈Sp, and let NS denote the normalizer NG(S).
I claim that the only Sylow p-subgroup of NS is S. For if R ∈Sp satisﬁes
R < NS, then there exists g ∈NS such that gSg−1 = R. But since gSg−1 = S
by the deﬁnition, it follows that R = S. Thus, if R, S ∈Sp and R ̸= S, then
|NR ∩NS| cannot be divisible by |S|. Now let NS act on Sp. Certainly,
S is a ﬁxed point; i.e. NS · S = S. On the other hand, if R ̸= S, then
|NS · R| is divisible by p. Indeed, by the orbit stabilizer theorem, |NS · R| =
|NS|/|NR ∩NS|, so this follows from the fact that the denominator is not
divisible by |S|. Therefore, the orbit identity implies that |Sp| = 1 + mp. □

356
11
Theorems on Group Theory
11.3.4
Groups of order 12, 15, and 24
We will now look at some examples.
Example 11.8. First consider groups of order 12. In such a group, the
number of Sylow 2-subgroups is either 1 or 3, and the number of Sylow
3-subgroups is either 1 or 4. The direct products C3 ×C4 = C12 and C3 ×U8,
where U8 is the group of units modulo 8, are both abelian groups of order
12; hence each has exactly one Sylow subgroup of order 3 and one of order
4. Note: the direct product of G and H is the group G × H consisting of all
the pairs (g, h) with g ∈G and H ∈H with the almost obvious group struc-
ture. The general deﬁnition is given in Deﬁnition 11.6 below. The alternating
group A(4) has four cyclic subgroups of order 3 generated respectively by
(123), (134), (124), and (234). Thus there are eight elements of order 3. This
leaves exactly three elements of order 2 or 4. Thus the Sylow 2-subgroup,
which has order 4, has to be normal. This group turns out to be the well-
known Klein 4-group
V4 = {(1), (12)(34), (14)(23), (13)(24)},
consisting of all elements of S(4) that are the product of two disjoint trans-
positions. In fact, V4 is normal in S(4). The dihedral group D(6) has six
rotations, which form a cyclic normal subgroup. The two rotations of order 3
determine a normal subgroup, which is therefore the unique Sylow 3-subgroup
of D(6). By the O(2, R) dichotomy, the remaining six elements are reﬂections.
Since reﬂections about orthogonal lines commute, and since a six-sided regu-
lar polygon P in R2 has three pairs of orthogonal lines through which P may
be reﬂected, there are three Sylow 2-subgroups. Thus D(6) has one Sylow
3-subgroup and three Sylow 2-subgroups.
⊠
Example 11.9. Returning to the group of order 15, we see that the number
of Sylow 3-subgroups has the form 1+3m and divides 15. The only possibility
is that there is exactly one Sylow 3-subgroup. Similarly, the number of Sylow
5-subgroups has the form 1 + 5m and divides 15. Hence there is exactly one
Sylow 5-subgroup. The order of an element has to divide 15, so the possible
orders are 3, 5, and 15. Since we just showed that there are two elements of
order 3 and four elements of order 5, there has to be an element of order 15
(in fact, eight of them). Thus we see once again that a group of order 15 is
cyclic.
⊠
Example 11.10. The symmetric group S(4) and the matrix group SL(2, F3)
both have order 24. Are they isomorphic? A necessary condition for two ﬁnite
groups to be isomorphic is that they have the same Sylow subgroups. By the
third Sylow theorem, the number of Sylow 3-subgroups in a group of order
24 is one or four, and the number of Sylow 2-subgroups is one or three. We
will now show that S(4) has three Sylow 2-subgroups. Recall that S(4) is

11.3
The Sylow Theorems
357
isomorphic to the group of all rotations of the unit cube C in R3 centered at
the origin. Let S1 denote the intersection of the cube with the xy-plane, and
label the vertices of S as 1, 2, 3, 4 in a clockwise manner, with 1 denoting
the vertex in the ﬁrst octant. The subgroup
H1 = {(1), (1234), (1432), (13)(24), (12)(34), (14)(23), (13), (24)}
is a copy of D(4). The ﬁrst four elements are rotations of S1, which rotate
C about the z-axis. The other four elements are reﬂections of S1, which are
rotations of the cube through π/2. The other two Sylow 2-subgroups are
copies of D(4), which act the same way on the squares S2 and S3 obtained
by intersecting the cube with the other two coordinate hyperplanes. It turns
out that SL(2, F3) has only one Sylow subgroup H of order 8, so H is normal.
The details are tedious, so we will omit them. For example, the elements of
order 4 are

1
1
1
2
	
,

2
1
1
1
	
,

1
2
2
2
	
,

2
2
1
2
	
.
Thus, S(4) and SL(2, F3) have diﬀerent Sylow 2-subgroups, so they cannot
be isomorphic.
⊠
Exercises
Exercise 11.3.1. Show that A(4) does not have a subgroup of order 6; hence
the converse of Lagrange’s theorem is false.
Exercise 11.3.2. Determine the Sylow subgroups for the dihedral group
D(6).
Exercise 11.3.3. Let p and q be distinct primes, and let G be a group of
order pq. Show that the following statements are equivalent:
(i) G is abelian.
(ii) The center Z(G) is nontrivial.
(iii) G is cyclic.
Give an example of a group of order pq that is not cyclic.
Exercise 11.3.4. True or false with reasoning: every group of order 10 is
abelian.
Exercise 11.3.5. Show that if p and p+2 are both prime, then every group
of order p(p + 2) is cyclic.
Exercise 11.3.6. Let p be a prime. Show that U(n, Fp) and L(n, Fp) are
Sylow p-subgroups of both GL(n, Fp) and SL(n, Fp). How is U(n, Fp) conju-
gated onto L(n, Fp)?

358
11
Theorems on Group Theory
Exercise 11.3.7. Suppose p = 5 and n = 3.
(i) How many Sylow p-subgroups does GL(n, Fp) have?
(ii) Answer part (i) for SL(n, Fp).
Exercise 11.3.8. Find the orders of all the Sylow p-subgroups of GL(n, Fp)
for primes p ≤7.
Exercise 11.3.9. Consider the group Un of units modulo n for n = 50.
(i) Discuss the Sylow subgroups of Un.
(ii) Determine whether Un is cyclic. (Suggestion: consider part (i).)
Exercise 11.3.10. The order of Un is 100 if n = 202. Is either of the Sylow
subgroups cyclic?
Exercise 11.3.11. • Let G be a ﬁnite group. Show that if |G| = mp, where
p is prime and p > m, then there is only one Sylow p-subgroup H, and H is
normal in G.
Exercise 11.3.12. • Show that a group of order pq, p and q distinct primes,
is cyclic whenever p does not divide q −1 and q does not divide p −1.

11.4
The Structure of Finite Abelian Groups
359
11.4
The Structure of Finite Abelian Groups
So far, we have encountered a few examples of ﬁnite abelian groups: the cyclic
groups Cm, the multiplicative group F∗of a Galois ﬁeld F, the center Z(G)
of an arbitrary ﬁnite group G, and the subgroups of all these groups. More
interestingly, as we saw in Section 2.1, the group of multiplicative units (i.e.,
invertible elements) in Z/nZ is an abelian group Un of order φ(n), where φ is
Euler’s phi-function. We will now describe the structure of all ﬁnite abelian
groups. The Sylow theorems imply a ﬁnite abelian group is the direct product
of its Sylow subgroups. After that, the job is to describe the abelian p-groups.
11.4.1
Direct products
The direct product of a ﬁnite number of groups is a generalization of the
direct sum of a ﬁnite number of vector spaces. And as for vector spaces, direct
products in the group setting come in two ﬂavors, internal and external. The
simpler case is the external direct product, so we will introduce it ﬁrst.
Deﬁnition 11.6. Let G1 and G2 be arbitrary groups. Their external direct
product is the usual Cartesian product G1 × G2 with the group operation
(g1, h1)(g2, h2) = (g1g2, h1h2).
It is easy to check that the external direct product G1 ×G2 is a group with
identity (1, 1), where 1 denotes both the identity of G1 and that of G2. The
notion of external direct product can be extended without diﬃculty from two
groups to any ﬁnite number of groups. We will leave the details to the reader.
Note that if V1 and V2 are vector spaces over diﬀerent ﬁelds, then V1 × V2 is
an abelian group but not a vector space. If V1 and V2 have the same scalar
ﬁeld F, then the external direct sum of vector spaces applies, and V1 × V2 is
a vector space over F.
Proposition 11.22. Suppose G = G1 × G2 × · · · × Gm is the external direct
product of the groups G1, G2, . . . , Gm. Then:
(i) G is abelian if each Gi is, and
(ii) for every σ ∈S(m), G ∼= Gσ(1) × Gσ(2) × · · · × Gσ(m).
Proof. We leave the proof to the reader.
□
We will now consider the internal direct product, which for groups is a
generalization of the internal direct sum of subspaces (keeping in mind that
subspaces of a vector space are normal subgroups). Let A1, A2, . . . , Ak be
subsets of the group G. Deﬁne A1A2 · · · Ak to be the totality of products

360
11
Theorems on Group Theory
a1a2 · · · ak, where each ai is in Ai. Also, deﬁne < A1, A2, . . . , Ak > to be the
smallest subgroup of G containing each Ai. Suppose each Ai is normalized
by G; that is, gAig−1 = Ai for all g ∈G. Then AiAj = AjAi for all i, j. For
if we choose any ai ∈Ai for each i, it follows that aiaj = aj(a−1
j aiaj), so
AiAj ⊂AjAi. Hence by symmetry, AiAj = AjAi. Consequently, the product
A1A2 · · · Ak is independent of how the sets Ai are ordered.
We now need the following lemma.
Lemma 11.23. If H1, H2, . . . , Hk are normal subgroups of G, then the prod-
uct H1H2 · · · Hk is a normal subgroup of G, and
< H1, H2, . . . , Hk >= H1H2 · · · Hk.
Proof. We will use induction to prove that H1H2 · · · Hk is a subgroup of
G, which will show that < H1, H2, . . . , Hk >= H1H2 · · · Hk, since H1H2 · · ·
Hk ⊂< H1, H2, . . . , Hk >. Notice that if H1H2 · · · Hk is a subgroup, then it
has to be normal, since for all g ∈G,
g(H1H2 · · · Hk)g−1 = (gH1g−1)(gH2g−1) · · · (gHkg−1),
and every Hi is a normal subgroup. The lemma holds for k = 1, so assume
k > 1 and that it holds for k −1. Thus H = H1H2 · · · Hk−1 is a subgroup of
G. We need to show that HHk is a subgroup, so we must show that if a, b ∈H
and g, h ∈Hk, then (ag)(bh)−1 ∈HHk. Now, (ag)(bh)−1 = a(gh−1)b−1 ∈
HHkH = HHk, since HHk = HkH by the remark preceding the lemma.
Hence H1H2 · · · Hk is a subgroup, so the proof is complete.
□
We now come to the internal direct product of an arbitrary number of
normal subgroups.
Deﬁnition 11.7. Let G be a group and let H1, H2, . . . , Hk be normal sub-
groups of G. We say that G is the internal direct product of H1, H2, . . . , Hk
if
G =< H1, H2, . . . , Hk >,
and for each i with 1 ≤i < k,
< H1, H2, . . . , Hi > ∩Hi+1 = (1).
If G is the internal direct product of H1, . . . , Hk, we will write G =
((H1, . . . , Hk)).
Proposition 11.24. Let G = ((H1, H2, . . . , Hk)). Then for each g ∈G,
there exists exactly one expression g = h1h2 · · · hm, where hi ∈Hi for each
i. In particular, if G is ﬁnite, then

11.4
The Structure of Finite Abelian Groups
361
|G| =
m

i=1
|Hi|.
(11.5)
Proof. An expression g = h1h2 · · · hm exists by Lemma 11.23. Assume
g = h1 · · · hm = k1 · · · km with hi, ki ∈Hi for all i. Then hmk−1
m
∈<
H1, . . . , Hm−1 >. Therefore, hmk−1
m = 1, so h1 · · · hm−1 = k1 · · · km−1. Con-
tinuing in this manner, it follows that hi = ki for each i. This shows that the
above expression for g is unique.
□
Proposition 11.25. Suppose G = ((H1, H2, . . . , Hk)). Then G ∼= H1×H2×
· · · × Hk.
Proof. Deﬁne a map ϕ : H1 × · · · × Hk →G by ϕ(h1, . . . , hk) = h1 · · · hk. By
Proposition 11.24, ϕ is a bijection. Since Hi and Hj commute elementwise
for all i and j, it also follows that ϕ is also a homomorphism, so ϕ is an
isomorphism.
□
Example 11.11. Let’s reconsider U8 (see Example 2.18). For simplicity, we
denote the coset m + 8Z by m. Thus U8 = {1, 3, 5, 7}. Since U8 is abelian,
every subgroup is normal. Using coset multiplication, we have 3 · 5 = 15 = 7.
Hence if H1 = {1, 3} and H2 = {1, 5}, then U8 = ((H1, H2)) ∼= H1 × H2.
⊠
Example 11.12. Let’s next consider U15 = {1, 2, 4, 7, 8, 11, 13, 14}. Since
|U15| = 8, the possible orders of its elements are 1, 2, 4, and 8. It is easy
to check that 2, 7, 8, and 13 have order 4, and 4, 11, and 14 have order 2.
Moreover, 2
2 = 7
2 = 8
2 = 13
2 = 4. Thus < a > ∩< 11 >=< a > ∩< 14 >=
(1) for a = 2, 7, 8, 13. Hence if H =< a >, K =< 11 >, and L =< 14 >, we
have
U15 = ((H, K)) ∼= H × K ∼= ((H, L)) ∼= H × L = C4 × C2.
⊠
11.4.2
The structure theorem for ﬁnite abelian
groups
Suppose G is a ﬁnite abelian group of order pn1
1 · · · pnm
m , where p1, p2, . . . , pm
are the distinct prime factors of |G|, and let Gi denote the unique Sylow
pi-subgroup G. The ﬁrst of two results about the structure of G is stated as
follows.
Proposition 11.26. Let G be as above. Then G is the direct product (both
internal and external) of its Sylow subgroups G1, . . . , Gm.

362
11
Theorems on Group Theory
Proof. Since G is abelian, its Sylow subgroups are normal and commute
elementwise. Let us induct on m. Assume that the product G1 · · · Gi is
direct for each i < m. By Proposition 11.24, |G1 · · · Gi| = pn1
1 · · · pni
i . Thus
G1 · · · Gm−1 ∩Gm = (1), since every element of Gm has order pk
m for
some k, and pm does not divide |G1 · · · Gm−1|. Therefore, the product
G1 · · · Gm is direct, and consequently, G = ((G1, . . . , Gm)). It follows that
G ∼= G1 × · · · × Gm.
□
It remains to give a complete description of the abelian p-groups in more
detail. This turns out to be surprisingly complicated, so we will simply outline
the proof. Here is the main result:
Theorem 11.27. A ﬁnite abelian group of order pn, p a prime, is isomor-
phic to a product of cyclic p-groups. In particular, every ﬁnite abelian group
G is the external direct sum of cyclic groups of prime power order.
Proof. The proof is by induction on n. Choose an element y ∈G of maximal
order, and let H =< y >. By induction, G/H is a product of cyclic groups,
say G/H =< a1H > × · · · × < akH >. Now G/H is also a p-group, so for
each i, apri
i
= ysi for some ri, si > 0. We assert that pri divides si. Put
ji = si/pri and ui = aiy−ji. Then it turns out that G =< u1 > × · · · × <
uk > ×H, which gives the induction step and ﬁnishes the proof (modulo
omitted details).
□
Example 11.13. According to the theorem, if G is abelian of order 16, then
the possibilities for G are
C2 × C2 × C2 × C2,
C4 × C2 × C2,
C4 × C4,
C8 × C2,
C16.
11.4.3
The Chinese Remainder Theorem
Applying the above results gives the following corollary.
Corollary 11.28. Suppose G is a ﬁnite cyclic group. Then its Sylow sub-
groups G1, . . . , Gm are cyclic of prime power order, and G = G1 × · · · × Gm.
Proof. Since G is cyclic, division with remainder tells us that every subgroup
of G is also cyclic. Hence the Sylow subgroups of G are cyclic, so the result
follows from Proposition 11.26.
□
In particular, let n = pn1
1 · · · pnm
m , as usual. Since the additive group Z/nZ
is cyclic, its Sylow subgroups are (Z/pn1
1 Z), . . . , (Z/pnm
m Z). Therefore,
Z/nZ ∼= (Z/pn1
1 Z) × · · · × (Z/pnm
m Z).

11.4
The Structure of Finite Abelian Groups
363
□
As a corollary, one gets the Chinese remainder theorem: given congruence
equations
x ≡a1 mod(pn1
1 ), . . . , x ≡am mod(pnm
m ),
there exists a unique class w + nZ in Z/nZ such that setting x = w solves all
the above congruences.
Exercises
Exercise 11.4.1. Find at least three values of n such that Un is cyclic. Is
there a general rule?
Exercise 11.4.2. • Show that in every cyclic group, the number of elements
of order m is divisible by φ(m). Give an example of a ﬁnite abelian group in
which the number of elements of order m is not divisible by φ(m).
Exercise 11.4.3. Suppose each Gi is cyclic. When is G = G1×G2×· · ·×Gm
cyclic?
Exercise 11.4.4. Suppose G can be expressed as a direct product HK.
Show that
(i) K ∼= G/H, and
(ii) G ∼= H × G/H.
Exercise 11.4.5. Find all the ways of decomposing U21 as a product of
cyclic subgroups.
Exercise 11.4.6. Let F be a Galois ﬁeld. It is a theorem that the multi-
plicative group F∗of F is always cyclic. Prove directly that F∗is cyclic in the
following cases:
(i) |F| = 16,
(ii) |F| = 32, and
(ii) |F| = 25.
Exercise 11.4.7. Let G be a group, H a subgroup, and N a normal sub-
group of G. Then G is said to be the semidirect product of H and N if at
least one of the following statements holds.
(i) G = NH and N ∩H = 1.
(ii) Every element of G can be written in a unique way as a product nh,
where n ∈N and h ∈H.
(iii) The natural inclusion homomorphism H →G composed with the natural
projection G →G/N induces an isomorphism H ∼= G/N.
(iv) There exists a homomorphism G →H that is the identity on H whose
kernel is N.
Show that the all four statements are equivalent.

364
11
Theorems on Group Theory
11.5
Solvable Groups and Simple Groups
One of the most celebrated results in classical algebra is the unsolvability of
the quintic. Roughly, this means that there exist polynomials f(x) of degree
ﬁve with rational coeﬃcients whose roots cannot be expressed in terms of
radicals involving those coeﬃcients. More precisely, there is no formula that
expresses the roots of an arbitrary ﬁfth-degree polynomial f(x) = x5+a1x4+
a2x3 + a3x2 + a4x + a5 starting from an algebraic combination α of the
coeﬃcients ai of f, takes an mth root α1/m for some m, then repeats the
process on the coeﬃcients of α1/m and so on until a closed formula that gives
all the roots of f(x) is obtained. Unsolvable quintics can be quite ordinary:
2x5 +10x+5 is an example. The deﬁnitive criterion for solvability by radicals
ﬁrst appeared in Galois’s posthumous 1846 paper: a polynomial is solvable
by radicals if and only if its Galois group is solvable. (For a deﬁnition of
the Galois group of a polynomial, see Section 11.5.5.) The Galois group of
2x5+10x+5 turns out to be S(5), which, as we will see below, is not a solvable
group. The simple groups lie at the opposite end of the group spectrum from
the solvable groups, and in fact, they serve as the main source of examples
of unsolvable groups. Recall that a nontrivial group G is said to be simple
if its only normal subgroups are G itself and the trivial subgroup (1). A
famous classical result is that the alternating groups A(n) for n > 4 are
simple. It follows that the symmetric groups S(n) for n > 4 are not solvable.
Simple groups play a role in the structure theory because of composition
series and the Jordan–H¨older theorem, which we will state without proof
below. As mentioned in the introduction to this chapter, the ﬁnite simple
groups have now been classiﬁed, the ﬁnal step not having come until 1982
with the construction of the Monster.
11.5.1
The deﬁnition of a solvable group
The process described above for solving by radicals hints at what the deﬁni-
tion of a solvable group might be. To deﬁne solvability, we must ﬁrst introduce
the notion of a subnormal series in a group.
Deﬁnition 11.8. A subnormal series for a group G is a ﬁnite sequence of
subgroups
G = H1 > H2 > · · · > Hr−1 > Hr = (1)
(11.6)
of G such that Hi+1 is normal in Hi for i = 1, . . . , r −1. The quotient groups
Hi/Hi+1 are called the factors of the subnormal series (11.6).
For example, if H ̸= (1) is a proper normal subgroup of G, then G > H >
(1) is an example of a subnormal series. A simple group G has no nontrivial

11.5
Solvable Groups and Simple Groups
365
normal subgroups, so G > (1) is the only subnormal series for G. A subnormal
series (11.6) is called a composition series if each factor is a simple group. If
H is normal in G, then G/H is simple if and only if there exists no normal
subgroup N of G such that H < N < G. (The proof of this assertion is an
exercise.) Thus a composition series is a subnormal series of maximal length.
It follows that every ﬁnite group has a composition series.
Now we can deﬁne what a solvable group is.
Deﬁnition 11.9. A group G is called solvable if it has a subnormal series
(11.6) such that each factor Hi/Hi+1 is abelian.
Of course, abelian groups are solvable. The next result points out an impor-
tant class of solvable groups.
Proposition 11.29. Every p-group is solvable.
Proof. Suppose |G| = pn. If n = 1, then surely G is solvable, so let us argue
by induction on n. Assume that all p-groups of order pn−1 are solvable. By the
ﬁrst Sylow theorem, G has a subgroup H of order pn−1, and by assumption,
H is solvable. To show that G is solvable, it suﬃces to show that H is normal
in G, since in that case, G/H has order p and hence is abelian. Let H act on
G/H by left multiplication. The orbit stabilizer theorem then says that for
every orbit O, either |O| = 1 or |O| = p. But the latter case cannot occur,
since the orbit of the coset H is itself. It follows that every orbit is a single
point. Thus HgH = gH for all g ∈G. This implies that H is normal in G. □
A famous result in ﬁnite group theory known as Burnside’s theorem (1904)
states that a ﬁnite group whose order is divisible by at most two primes is
solvable. Burnside’s proof used techniques from outside group theory, so it
remained an open problem to ﬁnd a purely group-theoretic proof. Such a
proof was not discovered until 1970. In 1963, Walter Feit and John Thompson
published a 255-page paper proving that every ﬁnite group of odd order is
solvable, hence ensuring that there are nonabelian solvable groups that are
not p-groups. Note that the Feit–Thompson theorem implies Burnside’s result
for two odd primes.
The symmetric groups S(3) and S(4) are both solvable. (For example,
apply Burnside’s theorem.) We will leave this claim as an exercise for S(3)
and give a proof for S(4) in the next example. It turns out, however, that
S(n) is not solvable for n > 4.
Example 11.14 (S(4) is solvable). We need to display a subnormal series
for S(4) whose derived factors are abelian. The alternating group A(4) (see
Section 5.2) is normal in S(4) and S(4)/A(4) ∼= C2, so it is abelian. Thus,
S(4) > A(4) is a ﬁrst step. As we have seen, the Klein 4-group V4 (see
Example 11.8) is a normal subgroup of A(4) of order 4. Since the order
of A(4) is 12, A(4)/V4 has order three, and hence it is also abelian. But
V4 is abelian, since it has order p2. It follows that S(4) > A(4) > V4 >
(1) is a subnormal series for S(4) whose factors are abelian. Hence S(4) is
solvable.
⊠

366
11
Theorems on Group Theory
11.5.2
The commutator subgroup
In this section, we will ﬁnd an explicit test for solvability. To do so, we need
to introduce the commutator subgroup of a group and its derived series. The
commutator subgroup of G is the smallest subgroup of G that contains all
products of the form ghg−1h−1, where g, h ∈G. This subgroup is denoted
by [G, G].
Proposition 11.30. The commutator subgroup [G, G] of G is normal in
G, and G/[G, G] is abelian. In particular, if G is simple, then G = [G, G].
Moreover, if N is a normal subgroup of G such that G/N is abelian, then N
contains [G, G].
Proof. We will leave the proof as an exercise.
□
The commutator groups for S(n) provide a nice example. Recall that
A(n) = ker(sgn), where sgn : S(n) →{±1} is the signature homomorphism
deﬁned by the expression
sgn(σ) =

i<j
σ(i) −σ(j)
i −j
.
It follows from the fact that sgn is a homomorphism that [S(n), S(n)] ≤A(n).
Proposition 11.31. The commutator subgroup of S(n) is A(n) for all n.
Proof. We just showed that [S(n), S(n)] ≤A(n). Notice that all 3-cycles in
S(n) are commutators, since
(abc) = (acb)(ab)(abc)(ab).
To show that A(n) = [S(n), S(n)], it thus suﬃces to show that A(n) is gener-
ated by 3-cycles. To see this, recall that S(n) is generated by transpositions;
hence elements of A(n) are products of an even number of transpositions.
These products can be either disjoint, such as (ab)(cd), or nondisjoint, such
as (ab)(bc). Thus the claim follows from the two identities
(ab)(cd) = (acb)(acd),
and (ab)(bc) = (abc).
□
The commutator subgroup of G has its own commutator subgroup. To
avoid making the notation too clumsy, let G(1) denote [G, G], and for each
i > 1 deﬁne G(i) = [G(i−1), G(i−1)]. The derived series of G is the sequence
G ≥G(1) ≥G(2) ≥· · · ≥G(i) ≥· · · .
(11.7)

11.5
Solvable Groups and Simple Groups
367
Proposition 11.32. A group G is solvable if and only if its derived series
has the property that G(k) is the identity subgroup for some k.
Proof. The if assertion follows immediately from Proposition 11.30 and the
deﬁnition of a solvable group. The only if assertion follows from the second
assertion of Proposition 11.30, since if H/N is abelian, then N contains
[H, H]. For if G = N1 > N2 > · · · > Nk−1 > Nk = 1 is a subnormal series
such that Ni/Ni+1 is abelian for all i, then G(i) < Ni+1. Thus G(k−1) = 1. □
Corollary 11.33. Subgroups and quotients of a solvable group are solvable.
Proof. Let G be solvable. If H is a subgroup of G, then its derived series
satisﬁes H(i) < G(i), so the derived series of H has to terminate at the
identity, since the derived series of G does. Likewise, if N is normal in G,
then the derived series of G/N is the image of the derived series of G, so it
likewise terminates.
□
We now give a nontrivial example of a solvable matrix group.
Proposition 11.34. The upper triangular subgroup T (n, F) < GL(n, F) is
solvable.
Proof. First notice that every element T ∈T (n, F) can be factored as T =
DU = V D, where D ∈D(n, F) and both U and V are in U(n, F). Moreover,
D(n, F) < NGL(n,F)(U(n, F)). This implies that the commutator of T (n, F) is
contained in U(n, F), for if A = DU and B = EV are in T (n, F), then there
exist W, Y ∈U(n, F) such that UE = EW and D−1V −1 = Y D−1. Thus,
[A, B] = ABA−1B−1
= (DU)(EV )(U −1D−1)(V −1E−1)
= (DE)(WV U −1Y )(D−1E−1).
Hence [A, B] ∈U(n, F). Thus it suﬃces to show that U(n, F) is solvable. For
each k with 1 ≤k < n, let Uk = {U ∈U(n, F) | uij = 0 if 0 < j −i ≤k}. In
other words, Uk consists of all U ∈U(n, F) that have zero on the ﬁrst through
kth superdiagonals. We leave it to the reader to check that [Uk, Uk] < Uk+1.
But [Un, Un] = {In}, so it follows from Proposition 11.32 that T (n, F) is
solvable.
□
11.5.3
An example: A(5) is simple
In this section, we will prove that the alternating group A(5) is simple. The
proof uses the following characterization of a normal subgroup, whose proof
is left to the reader.

368
11
Theorems on Group Theory
Proposition 11.35. A subgroup H of a group G is normal in G if and only
if H is a union of G-conjugacy classes.
Proposition 11.36. A(5) is simple.
Proof. It suﬃces to show that no subgroup of A(5) is a union of A(5)-
conjugacy classes. To prove this, we have to describe these conjugacy classes.
Since A(5) is normal in S(5), A(5) is the union of S(5)-conjugacy classes.
Note that the S(5)-conjugacy classes in A(5) are unions of A(5)-conjugacy
classes. By Proposition 11.15, the conjugacy classes in S(5) are represented
by
(1), (12), (123), (1234), (12345), (12)(34), and (123)(45).
Since the signature of a k-cycle is 1 if k is odd and −1 is k is even, it follows
that A(5) is the union of the S(5)-conjugacy classes of the elements
(1), (123), (12345), and (12)(34).
(11.8)
Let us ﬁrst compute the orders of the A(5)-conjugacy classes of these ele-
ments. By Proposition 11.8, we need to compute their centralizers in A(5).
But ZA(5)(1) = A(5), ZA(5)(123) = ⟨(123)⟩, ZA(5)(12345) = ⟨(12345)⟩, and
ZA(5)((12)(34)) = {1, (12), (34), (12)(34)}. Since |A(5)| = 60, Proposition
11.8 implies that the conjugacy classes in A(5) of the elements in (11.8) have
respectively 1, 20, 12, and 15 elements. Since these classes give a total of 48
elements, we have to account for another 12 elements. The reader can check
that (12)(12345)(12) = (13452) is not conjugate in A(5) to (12345). Thus its
A(5)-conjugacy class accounts for the remaining 12 elements. Therefore, the
order of a normal subgroup of A(5) must be a sum of 1 and some subset of
12, 12, 15, and 20. Hence the order of a nontrivial normal subgroup of A(5)
can be only one of 13, 16, 21, 25, 28, 33, 36, 40, 45, and 48. But none of these
numbers divides 60, so A(5) must be simple.
□
In fact, A(n) is simple for all n ≥5. The standard proof of this is to show
that every normal subgroup of A(n) for n ≥5 contains a 3-cycle. But the
only normal subgroup of A(n) that contains a 3-cycle is all of A(n), so A(n)
is simple. A complete proof can be found in Abstract Algebra, by Dummit
and Foote (pp. 128–130). The alternating groups also appear in another class
of ﬁnite simple groups. To describe this class, we ﬁrst consider the matrix
groups SL(2, Fp). Since the center Z(G) of a group G is always a normal
subgroup, the question is whether G/Z(G) is simple. Now, Z(SL(2, Fp)) =
{cI2 | c2 = 1} is nontrivial for p > 2. The quotient group PSL(2, p) =
SL(2, Fp)/Z(SL(2, Fp)) is called the projective linear group of degree two.

11.5
Solvable Groups and Simple Groups
369
Let us next compute |PSL(2, p)|. Since SL(2, Fp) is the kernel of the homo-
morphism det : GL(2, Fp) →(Fp)∗, and det is surjective, |SL(2, Fp)| =
|GL(2, Fp)|/(p −1). But from a computation from Chap. 6, |GL(2, Fp)| =
(p2 −1)(p2 −p) (since (p2 −1)(p2 −p) is the number of pairs of linearly
independent vectors in (F2)2). Thus if p > 2, then
|PSL(2, p)| = |SL(2, Fp)|
2
= (p2 −1)(p2 −p)
2(p −1)
= p(p2 −1)
2
.
It follows that |PSL(2, 2)| = 6, while |PSL(2, 3)| = 12 and |PSL(2, 5)| = 60.
In fact, using fractional linear transformations and projective geometry, one
can write down explicit isomorphisms PSL(2, 2) ∼= S(3), PSL(2, 3) ∼= A(4),
and PSL(2, 5) ∼= A(5). The general result is that if p > 3, then PSL(2, p) is
simple. The proof takes several pages to write down, and we will skip it.
11.5.4
Simple groups and the Jordan–H¨older
theorem
The Jordan–H¨older theorem is stated as follows:
Theorem 11.37. Any two composition series for a group G have the same
number of composition factors, and their composition factors are isomorphic
up to order.
The proof is somewhat long and complicated, and we will omit it. As we
noted earlier, a ﬁnite group always admits at least one composition series,
while inﬁnite groups need not admit any. The integers, for example, do not
have a composition series.
Example 11.15. For example, the cyclic group C12 gives a nice illustration
of the Jordan–H¨older Theorem. The group C12 has three composition series:
C12 > C6 > C3 > (1),
C12 > C6 > C2 > (1), and C12 > C4 > C2 > (1).
The corresponding composition factors taken in order are
{C2, C2, C3},
{C2, C3, C2},
and {C3, C2, C2}.
Two groups with isomorphic composition factors need not be isomorphic,
however, as the following example shows.
⊠

370
11
Theorems on Group Theory
Example 11.16. Let G be a cyclic group of order eight, and let H <
GL(3, F2) be the group of upper triangular unipotent matrices. Then G and
H have order eight, but G and H are not isomorphic, since G is abelian and
H isn’t. Now a composition series for G is G = C8 > C4 > C2 > (1) with
composition factors C2, C2, C2. To get a composition series for H, put
H1 =
 ⎛
⎝
1 0 c
0 1 b
0 0 1
⎞
⎠

,
and H2 =
 ⎛
⎝
1 0 c
0 1 0
0 0 1
⎞
⎠

.
Then the composition factors for the series H > H1 > H2 > {I3} are also
C2, C2, C2. Note that the composition factors for G are multiplicative groups,
while those for H are additive groups. Nevertheless, the composition factors
are isomorphic.
⊠
Remark. The previous example shows that a group isn’t determined by its
composition series. However, it does suggest the question of how groups can
be recovered from their composition series. This is known as the extension
problem.
For the ﬁnal result of this section, we classify the simple solvable ﬁnite
groups.
Proposition 11.38. The only ﬁnite groups that are both simple and solvable
are the cyclic groups of prime order.
Proof. Suppose G is simple and solvable. Since G has no nontrivial normal
subgroups, its only subnormal series is G > (1), which forces G to be abelian,
since it is assumed to be solvable. But the only simple abelian groups are the
cyclic groups of prime order.
□
11.5.5
A few brief remarks on Galois theory
To conclude our introduction to solvable groups, it seems necessary to give a
brief overview of Galois theory and the notion of the Galois group of a ﬁeld
extension, which is at the heart of Galois theory. Let F be a subﬁeld of a ﬁeld
E. Put another way, E is an extension ﬁeld of F. As we noted in Chap. 6,
E is a vector space over F; when the vector space dimension of E over F is
ﬁnite, it is customary to denote it by [E : F]. In that case, one calls E a ﬁnite
extension of F. For example, C is a ﬁnite extension of R with [C : R] = 2.
In the appendix to Chap. 8, we considered a method for extending a ﬁeld
F to a ﬁeld containing all the roots of a polynomial f(x) ∈F[x]. We will not
need to refer to that technique here, but the reader may wish to review it. Let
us consider an example. Assume F = Q and suppose f(x) = x4 −3. Letting

11.5
Solvable Groups and Simple Groups
371
α denote the positive real root
4√
3, it follows that the four roots of f(x) = 0
are ± α, ± iα. Now let E = Q(α, i) denote the smallest ﬁeld containing Q, α,
and i. Then E contains all four roots of f(x) = 0 as well as α2, α3, i, iα2, iα3.
It turns out that 1, α, α2, α3, i, iα, iα2, iα3 are linearly independent over Q,
and the set of all linear combinations
a1 + a2α + a3α2 + a4α3 + a5i + a6iα + a7iα2 + a8iα3,
where a1, . . . , a8 ∈Q, forms a ﬁeld that is a subﬁeld of C. We leave the
veriﬁcation of this assertion to the reader. This ﬁeld must evidently be E,
so [E : Q] = 8. Thus the smallest ﬁeld containing Q and all the roots of
x4 −3 = 0 has dimension eight over Q.
We now make the key deﬁnition.
Deﬁnition 11.10. Let E be an extension ﬁeld of F that is the smallest ﬁeld
containing all roots of a polynomial f(x) ∈F[x] without repeated roots. Then
the Galois group Gal(E/F) of E relative to F is the set of all ﬁeld isomorphisms
φ : E →E such that φ(a) = a for all a ∈F.
By deﬁnition, Gal(E/F) is a group under composition. Since φ(a) = a for
all a ∈F, it follows that each φ ∈Gal(E/F) is also an endomorphism of the
vector space E over F. Note that for every g(x) ∈F[x] and β ∈E such that
g(β) = 0, the deﬁnition of Gal(E/F) implies φ(g(β)) = g(φ(β)) = 0 for all
φ ∈Gal(E/F). In other words, elements of Gal(E/F) have to permute the
roots in E of arbitrary polynomials in F[x].
Let us return to the polynomial f(x) = x4 −3. Now, every φ ∈Gal(E/Q)
permutes the roots ±α, ±iα2 of f(x). Furthermore, if two elements φ, ψ ∈
Gal(E/Q) satisfy φ(μ) = ψ(μ) for each root μ, then φ = ψ. Since f(x) = 0
has four roots in E, Gal(E/Q) < S(4). Let us now determine Gal(E/Q). The
polynomial f has four distinct roots in E that are not in Q. Notice that since
±i satisfy x2+1 = 0, the above remark implies that every φ ∈Gal(E/Q) must
have φ(i) = ±i. However, φ can send α to any other root. Thus Gal(E/Q) has
an element of order four and an element of order two, and it can be shown
by a tedious calculation that they generate Gal(E/Q). To identify Gal(E/Q),
let ϕ ∈Gal(E/Q) satisfy ϕ(α) = α and ϕ(iα) = −iα. Then ϕ(i) = −i. Next
deﬁne τ ∈Gal(E/Q) by τ(α) = −iα and τ(i) = i. Then ϕ2 = τ 4 = 1 and
ϕτϕ = τ −1 in Gal(E/Q). We claim (again omitting the details) that ϕ and τ
also generate Gal(E/Q). Therefore, Gal(E/Q) is isomorphic to the dihedral
group D(4), of order eight.
In general, we have the following assertion.
Proposition 11.39. If f(x) ∈F[x] is a polynomial with only simple roots
and E is the smallest extension ﬁeld of F containing all the roots of f(x),
then |Gal(E/F)| ≤n!, where n = [E : F].

372
11
Theorems on Group Theory
The fundamental theorem of Galois theory stated below explains the cor-
respondence between the subﬁelds of E containing F and the subgroups of
Gal(E/F).
Theorem 11.40. Let E be an extension ﬁeld of the ﬁeld F that is obtained
by adjoining all the roots of an irreducible polynomial f(x) over F with sim-
ple roots. Then there is a one-to-one correspondence between subgroups of
Gal(E/F) and ﬁelds K such that F ⊂K ⊂E in which a subgroup H corre-
sponds to the subﬁeld K = EH of elements of E ﬁxed by all elements of H, and
a subﬁeld K such that F ⊂K ⊂E corresponds to Gal(E/K). Furthermore, if
H is a normal subgroup, then Gal(K/F) ∼= Gal(E/F)/Gal(E/K), where K is
the ﬁxed subﬁeld EH.
Returning to Q(α, i), let us determine what the theorem says. The group
Gal(E/Q(i)) is the subgroup of Gal(E/Q) containing all elements of Gal(E/Q)
ﬁxing i. The element τ ﬁxes i and generates a cyclic group < τ > of order four.
Therefore, Gal(E/Q(i)) is evidently < τ >. Notice that < τ > is a normal
subgroup of Gal(E/Q). Note also that Gal(Q(i)/Q) ∼= C2. The conclusion
from this is that |Gal(E/Q)| = 8, which bypasses the tedious calculations
mentioned above and shows that geq ∼= D(4).
Exercises
Exercise 11.5.1. Show directly that S(3) is solvable.
Exercise 11.5.2. True or false: a nonabelian simple group is its own com-
mutator.
Exercise 11.5.3. True or false: all dihedral groups are solvable.
Exercise 11.5.4. Show, without using the Sylow theorems, that the Klein
4-group is normal in A(4). (Suggestion: make a list of the conjugacy classes
of A(4) and show that V4 is a union of conjugacy classes.)
Exercise 11.5.5. Show that if H is normal in G, then G/H is simple if and
only if there does not exist a normal subgroup N of G such that H < N < G.
Exercise 11.5.6. Show that a solvable group has an abelian normal sub-
group.
Exercise 11.5.7. Prove that a ﬁnite group G is solvable if and only if G
has a subnormal series whose factors are cyclic.
Exercise 11.5.8. Suppose a ﬁnite group G contains a subgroup H that is
not solvable. Can G itself be solvable? If so, give an example.

11.5
Solvable Groups and Simple Groups
373
Exercise 11.5.9. Suppose G = H0 > H1 > · · · > Hk is a subnormal series.
Show that if G is ﬁnite, then |G| = h1 · · · hk, where hi = |Hi−1/Hi| is the
order of the ith factor.
Exercise 11.5.10. Show that the elementary matrix matrix E =

1
λ
0
1
	
over a ﬁeld F ̸= F2 is a commutator in GL(2, F). Use this to show that
GL(2, F) is its own commutator.
Exercise 11.5.11. Explain why Z does not have a composition series.
Exercise 11.5.12. Show that every ﬁnite group has a composition series.
Exercise 11.5.13. Does S(5) possess two distinct composition series?

374
11
Theorems on Group Theory
11.6
Appendix: S(n), Cryptography,
and the Enigma
One of the most interesting chapters in the history of World War 2 is how
the British cryptographers at Bletchley Park were able to solve the German
cipher machine known as the Enigma enabling the Allied military to read
virtually all the top secret military transmissions of the German military.
However, until just before the beginning of World War 2, the British had no
idea what sort of cipher the German military was using and were shocked
to learn that the mathematicians of the Polish cipher bureau had been able
to decipher the Enigma since 1932. Just before Germany invaded Poland in
1939, the Poles were able to give the British an actual German Enigma they
had reverse engineered along with their knowledge of how to operate it.
The main tool the Polish cryptographers used was group theory. Their
accomplishment still stands as both the ﬁrst and undoubtedly most important
use of abstract algebra in cryptography or any other endeavor outside of pure
mathematics. Since we are primarily interested in the role of group theory,
we will not mention many of the fascinating aspects of this story, such as,
for example, how Enigma led to the development of the computer. There is
an article by the principal character, Marian Rejewski, in the Annals of the
History of Computing, Vol. 3, Number 3, July, 1981, which gives a fascinating
ﬁrst-hand account. There are now many books and articles on the Enigma.
Enigma, by W. Kozachuk (published in 1984), is an excellent, though not
easy to ﬁnd, account. It is the only book that contains appendices written
by Rejewski himself explaining his breakthroughs. Kozachuk was himself a
Polish army oﬃcer and a military historian. Another excellent account is
given in Intercept, by Jozef Garlinski. Both books are fascinating.
11.6.1
Substitution ciphers via S(26)
A cipher is an algorithm for disguising a message so that only the sender
and the intended recipient for can read it. Cryptology, the mathematical
discipline of ciphers, consists of two areas, cryptography and cryptanalysis.
A cryptographer makes ciphers and a cryptanalyst tries to break them. A
substitution cipher is created by permuting the alphabet using an element
of S(26). A substitution cipher is one of the oldest ciphers in existence. Let
us consider an example. The following ciphertext can be deciphered in a few
minutes by analyzing the frequencies of the letters in the message. Guessing
the letters used in the one- and two-letter words is useful.
F KVZSDVS XNZVN
NSKZOFOSK OL ULWESO
ZOK ULHDRSWK ZK CLKO
FCUWSR DLWON XNZOSNSFR

11.6
Appendix: S(n), Cryptography, and the Enigma
375
Let us make a few observations. The frequencies of letters in the ciphertext
should roughly correspond to the frequencies of letters in plaintext, that is,
the message in English that has been enciphered. Notice, for example, that
there are eight S’s, so there is a strong probability that S represents E or
I. There are six Z’s, so Z is another candidate for E or I. But there are no
commonly used two-letter words that begin with E, and several that begin
with I, so Z very likely represents I. Since I has now been used, and the only
two one-letter words are A and I, we may infer that F represents A. This
is a start, but there is still work to do. One of the obvious ways of making
this cipher stronger would be to remove the spaces between words, since that
would conceal the one and two letter words.
11.6.2
The Enigma
A more sophisticated substitution cipher than the simple substitution
described above could employ several substitutions. For example, one might
encipher the ﬁrst letter by a permutation σ1, the second by another permu-
tation σ2, the third by σ3, and so on. Since there are 26! possible substi-
tutions, if the sequence of permutations was suﬃciently random and didn’t
repeat often, the cipher would be very hard to break, and statistics would
be of little help. This sort of variation of the substitution cipher has long
been incorporated in commercial cipher machines. The most famous of these
machines is the Enigma, which was manufactured in Germany and adapted
by the German military in 1929 for its military transmissions. In 1928, the
Polish Cipher Bureau was tipped oﬀthat the German military was interested
in the Enigma when an Enigma machine was inadvertently shipped to Poland
marked as radio parts. The cipher bureau learned of the misplaced package
through customs because of the anxiety of the German oﬃcials who had
mistakenly sent the wrong package, which they demanded to have returned
immediately. The mistake was discovered on a Saturday, so Polish customs
had time to allow the cipher bureau experts to inspect the contents of the
package, which they realized was an Enigma cipher machine. It was carefully
repackaged and returned, and apparently the Germans never suspected that
the Poles had learned about their error.
Around then, the Poles noticed that the German military began to use
an entirely new system of encipherment. They correctly surmised the source,
and the cipher bureau purchased a commercial Enigma for further study. The
chief of the cipher bureau made the astute observation that traditional cryp-
tological methods (linguistics and statistics) would not be of any use against
such a machine, and he organized a course in the mathematics department
at the University of Poznan to train cryptologists, hoping that he would ﬁnd
some brilliant students. There were indeed three outstanding students, who

376
11
Theorems on Group Theory
were recruited, and by 1932, they had actually succeeded in recreating a Ger-
man military Enigma, the so-called the Enigma double. This breakthrough
was based on a brilliant observation about permutation groups by Marian
Rejewski.
To explain the role group theory played in helping Rejewski and his col-
leagues duplicate the Enigma, we need to describe how it worked. It somewhat
resembled a portable typewriter. It had a keyboard with a key for each let-
ter but no space bar, no shift, and no keys for punctuation. Mounted above
and behind the keyboard where the keys would strike the paper was a lamp
board displaying 26 lights labeled a through z. Pressing a key had the aﬀect
of causing one of the lamps to light up. An Enigma required an operator,
who typed in the plaintext, and an assistant, who recorded the ciphertext as
the lamps were lit up in sequence. If, say, a was pressed ﬁve times in suc-
cession, a sequence of ﬁve lamps lit up. For example, pressing aaaaa might
produce bsfgt. The sequence would eventually reappear, but not for a long
time, in fact, not until a had been pressed (26)3 times. Curiously, due to the
way the keyboard was wired to the lamp board, if a was pressed, the lamp
corresponding to a could not light up. Eventually, the British cryptographers
ﬁgured out how to exploit this feature to their great beneﬁt. Once a message
was enciphered, it was sent in Morse code. A recieved enciphered message
was deciphered in exactly the same way. After it was decoded from the Morse
code to reveal the ciphertext, the operator typed the ciphertext on the key-
board, and the assistant read oﬀthe plaintext as the lamps lit up one after
another.
Now let us turn to how the machine functioned, which will explain why
enciphering and deciphering were the same process. The original version of
the Enigma contained three adjacent rotors on a horizontal axle. Each of the
rotors had 26 terminals equally spaced around both its left-hand and right-
hand circumferences. The terminals around each circumference represented
the alphabet arranged in the usual order, and each terminal on the left cir-
cumference of a rotor was wired internally to a single terminal on the right.
When a key was pressed, a current passed through the left rotor from the
left-hand terminal to one on the right, then through the middle rotor, and
ﬁnally through the right-hand rotor, thereby undergoing three permutations
σL, σM, σR in that order. The current then passed through a ﬁxed disk at
the end of the axle with 26 terminals around its inner circumference, each
wired to another terminal. This disk was called the reﬂector. The current then
returned through the right-hand rotor, middle rotor, and left-hand rotor to
the key that had caused its lamp to illuminate.
Let ρ denote the permutation of the reﬂector. Suppose the a key is struck.
Then the lamp that is illuminated by striking a is
σ−1
L σ−1
M σ−1
R ρσRσMσL(a).

11.6
Appendix: S(n), Cryptography, and the Enigma
377
Suppose this is w. Notice that ρ has the property that ρ = ρ−1. In a
group, such an element is called an idempotent. The permutation σ−1
L σ−1
M σ−1
R
ρσRσMσL, being conjugate to the idempotent ρ, is also idempotent. Thus if
pressing a lights up w, then pressing w lights up a. This was a most conve-
nient feature of Enigma and explains why encipherment and decipherment
were performed in the same way.
What complicated the encipherment is that each rotor could be indepen-
dently rotated through all 26 positions. Every time a key was pressed, the ﬁrst
rotor moved forward one terminal. This shift corresponded to the cyclic per-
mutation π = (abc . . . xyz), of order 26. Hence the second letter of plaintext
would be enciphered by
π−1σ−1
L πσ−1
M σ−1
R ρσRσMπ−1σLπ.
Notice that we have inserted π−1σLπ for σL, since the middle and right rotors
were stationary. Without conjugating by π, all three rotors would advance
1/26 revolution together. As soon as the ﬁrst 26 letters had been enciphered
and the left-hand rotor had made a complete revolution, the middle rotor
advanced 1/26th of a revolution. The 27th letter was thus enciphered by
σ−1
L π−1σ−1
M πσ−1
R ρσRπ−1σMπσL
since π26 = (1). As soon as 262 = 626 letters were enciphered, the right-
hand rotor moved forward 1/26th of a revolution, and so on. The rotors thus
kept cycling through diﬀerent permutations until 263 = 17576 keys had been
pressed, after which the cycle repeated.
11.6.3
Rejewski’s theorem on idempotents
in S(n)
We will now pause to analyze some properties of idempotents in S(n). To
take a quick example, note that (ab)(cd)(ef) is an idempotent, but (ab)(bd)
is not. Since an idempotent ρ has the property that ρ2 = 1, it follows that
ρ’s disjoint cycles must be transpositions, since a k-cycle has order k. The
reﬂector ρ on the Enigma was the product of 13 disjoint transpositions, since
every terminal had to be paired with a diﬀerent terminal, since pairing a
terminal with itself would mean that sometimes pressing a key would fail to
illuminate a lamp, since a key could not light up its own lamp. Thus, by the
binomial theorem, the total number of possible Enigma reﬂectors is
26!
213 = 49229914688306352 × 106.

378
11
Theorems on Group Theory
Although it was certainly not obvious at the time, it turned out that the key
to unlocking how the Enigma rotors were wired is what happens when two
idempotents are multiplied. This question was answered by Marian Rejewski
in 1932 in the following theorem, which has also been referred to as “the
theorem that won World War Two.”
Theorem 11.41 (Rejewski’s theorem). Let σ and τ be idempotents in S(n)
with the same ﬁxed points in {1, . . . , n}. Then the number of disjoint cycles
in στ of each length is even (including the possibility of length 0). Thus
if στ has a disjoint cycle of length k > 0, then it has an even number of
them. Conversely, an element of S(n) that has the property that there is an
even number of disjoint cycles of each possible length in its disjoint cycle
representation is a product of two idempotents (though possibly in several
ways).
Considering an example will give a good idea why the ﬁrst assertion is
true, but its converse is harder to justify.
Example 11.17. The permutations
σ = (a e)(b f)(c g)(h d) and τ = (b e)(f c)(h g)(a d)
are idempotents in S(8) with the same ﬁxed letters, namely i through z.
To see how to construct the disjoint cycles in στ, consider the following
arrangement:
a
h
c
b
a
d
g
f
e
.
Note that τ acts by reading diagonally down from left to right, while σ acts
by reading diagonally up from left to right. Thus the disjoint cycle decom-
position of στ is revealed by reading the top row from left to right to get
one cycle, and the bottom row from right to left to get the other. Thus
στ = (a h c b)(e f g d). Similarly, τσ = (a b c h)(d g f e). The cycles of each
length occur in pairs. In each case, there are only two cycles, and both are
of length four. This construction works for the product of any two idempo-
tents with the same ﬁxed letters. The converse statement is harder but more
important. It is this fact that led to Rejewski’s breakthrough.
⊠

11.7
Breaking the Enigma
379
11.7
Breaking the Enigma
As we noted above, for two Enigmas with the same initial rotor settings, enci-
phering and deciphering were the same operation. The operator who received
a message had only to type in the ciphertext as the assistant read oﬀthe plain-
text on the lamp board. To ensure that the starting positions were always
the same, a daily key schedule was compiled in a codebook issued to all the
Enigma operators. If on September 5, 1940, the daily key was xsf, then on
that day all Enigmas would begin sending and deciphering with the left rotor
set at x, the middle at s, and the right at f. To increase security, each oper-
ator also selected another three-letter key, a so-called telegram key, e.g., arf.
Then before enciphering took place, the operator, with the Enigma set to the
daily key xsf, enciphered the telegram key arf. As an error-detecting device,
the operator actually typed arfarf, producing a six-letter string, let us say
wkuygh. This six-letter string was then sent by Morse code as the ﬁrst six let-
ters of the enciphered message. After sending his doubly enciphered telegram
key, the operator set his rotors to arf and proceeded to encipher the plain-
text. The operator on the receiving end, with his Enigma set to the daily
key xsf, typed in wkuygh. The deciphered string arfarf told him to reset
his rotors to arf before typing in the ciphertext. Of course, if something
like arfark was received, this signaled a transmission error, and the message
wasn’t deciphered until the doubly enciphered telegram key was resent. The
double enciphering of the telegram key was necessary. Radio transmissions
could be disrupted by static, and there was always the possibility of human
error under the diﬃculties experienced in wartime. But it turned out to be
the weak link. To see why, see whether you can detect a pattern in the ﬁrst
six-letter groups from ﬁfteen messages all intercepted on September 5. Note
that all groupings are double encipherings of diﬀerent telegram keys.
wkuygh wctyuo qvtnno kophau evprmu
qmlnxz wvqymk dgybhj bxcdla mijwce
dbobth
yoeiaw ntplbu yugicf lhmqzp
The interesting feature of these six-letter groups is that whenever two have
the same ﬁrst letter, they have the same fourth letter, and conversely. This
also holds for the second and ﬁfth letters and the third and sixth letters. A
good cryptographer would notice this feature immediately, but an untrained
eye (such as the author’s) might not see it for quite a while, or ever. This
clearly suggested a double encryption hypothesis. Working on this assump-
tion, Rejewski had the wonderful insight to string together the ﬁrst and fourth
letters of all the ﬁrst six-letter groups for all the intercepted messages on a
particular day, since they were all enciphered with the same daily key: xsf
in the case of September 5. Here is what the above ﬁfteen intercepts give:

380
11
Theorems on Group Theory
dbd . . . mwyi . . . qnlq . . . odb . . . fv . . . er . . . kh . . . .
However, working with 60–80 daily intercepts, he was sometimes able to
string together the whole alphabet, getting an element π ∈S(26). Let us see
what the construction of π tells us. Looking at the ﬁrst grouping, we know
that the operator typed arfarf, which produced wkuygh. So typing a on
the keyboard produced w on the lamp board via an idempotent σ1. Now
let σ2 be the idempotent that sends a to y. Then since π(w) = y, we have
πσ1(a) = σ2(a). But since all the Enigmas were set up on September 5 with
the same daily key xsf, the pairings σ1 and σ2 would be the same for all
Enigmas. Hence πσ1 = σ2. Consequently, π = σ2σ1 in S(26)! Thus we see
the surprising way in which the product of two idempotents ﬁgured. It would
be possible, though not easy, to ﬁnd the disjoint cycle representation of π,
and from this, one might be able to deduce σ1 and σ2. Recall that neither
σ1 nor σ2 could have any ﬁxed letters. Let us call such idempotents pairings.
Thus, it might be possible to ﬁnd the pairings σ1 and σ2 from π. Repeating
this process for the second and ﬁfth and the third and sixth letters gave two
more elements of S(26) that were also products of two pairings. There was
an unavoidable problem, however: a factorization into pairings is not unique.
Let us take a couple of simple examples to illustrate how the pairings might
be found.
Example 11.18. Let us shorten the alphabet to a through h, and let π =
(ahc)(dgb). We want to write this as στ, where σ and τ are pairings. We
can clearly see that e →e and f →f. Imitating the procedure illustrated
after Rejewski’s theorem, consider the three possibilities taking the cyclic
permutations of b, d, g into account:
a
h
c
a
b
g
d
,
a
h
c
a
g
d
b
,
and
a
h
c
a
d
b
g
.
Thus there are three possible solutions:
σ = (a b)(h g)(c d)(e f),
τ = (a d)(c g)(b h)(e f),
σ = (a g)(d h)(b c)(e f),
τ = (a b)(c d)(g h)(e f),
σ = (a d)(b h)(c g)(e f),
τ = (a g)(c d)(d h)(e f).

11.7
Breaking the Enigma
381
Notice that we included (e f) in each solution in order to ensure that σ and
τ are pairings, not just idempotents. The transposition (e f) disappears in
the product.
Example 11.19. Consider the permutation
π = (d e p z v l y q)(a r o n j f m x)(b g k tu)(w s c i h).
Thus we consider pairs of arrays such as
d
e
p
z
v
l
y
q
d
x
m
f
j
n
o
r
a
and
b
g
k
t
u
b
h
i
c
s
w
.
One possible solution is therefore
σ = (d x)(e m)(f p)(j z)(n v)(l o)(r y)(a q)(b h)(g i)(k c)(s t)(u w)
and
τ = (a d)(q r)(o y)(ln )(j v)(f z)(m p)(e x)(b w)(u s)(t c)(k i)(g h).
Since we obtain all solutions by cyclicly permuting the second rows of the
two arrays, there are 128 solutions in all.
Rejewski’s solution helped reveal the some of the pairings. If he had known
the daily keys, then he would have gotten some real insight into the wiring of
the rotors. But it wasn’t always necessary to know the daily keys, because the
Enigma operators often chose easy to guess keys such as aaa or abc, or they
might always choose the same key. But what turned out to be a huge break
for the Poles was that French intelligence uncovered a disgruntled German
code clerk who sold them the daily keys for a two-month period in 1932.
They were given to the Poles, who used this windfall along with Rejewski’s
factorizations. Using several other clever and imaginative devices, the three
Polish cryptographers were able to decipher their ﬁrst Enigma message at the
end of 1932. By 1934, they had completely solved the puzzle of the wiring
of the rotors and were able to build an exact replica of the Enigma (an
Enigma double). What made this even more amazing was that Poland was
economically depressed, having become an independent country only at the
end of the First World War, and the ﬁnancial outlay for this project was
a serious strain on its national treasury. Yet because of the wisdom of the
head of its cipher bureau and the ingenuity of its cryptographers, the Poles

382
11
Theorems on Group Theory
were years ahead of the British and French, who despite their great economic
advantage, had been unable to unravel the Enigma’s mystery.
Rejewski had even constructed a primitive computer, which he called a
bomb, to test for the daily keys. Fortunately, a couple of months before Ger-
many’s shocking invasion of Poland in September 1939, two of the duplicate
Enigmas were handed over to the French, who gave one to the British. With
this windfall, the British cryptographers at Bletchley Park were immediately
able to decipher a certain amount of the intercepted radio traﬃc, somewhere
on the order of 150 intercepts per day. But after the war started, the Ger-
mans upgraded their system, so the cryptanalysts were stymied until they
ﬁgured out what modiﬁcations the Germans had made. In 1943, the British,
under the leadership of Alan Turing, built the ﬁrst true electronic computer
to test for the daily keys. They also called it the bomb, apparently in honor
of the Polish original. With the bomb, the Bletchley Park cryptanalysts were
eventually able to read virtually all of the top secret communications of the
German High Command, often before the oﬃcers for whom the communiqu´es
were intended.
The breaking of the Enigma surely shortened the war. In fact, after learn-
ing in the 1970s the extent to which the British had penetrated the Enigma
ciphers, the head of German Enigma security, a mathematical logician, stated
that it was a good thing, since it must have shortened the war. He was happy
to learn that a fellow logician, Alan Turing, had played such an important
role. A well-known German mathematician, who was responsible for the secu-
rity of the Enigmas for a branch of the German military, was quoted in an
obituary as saying that it was a job he hadn’t been very good at.
Acknowledgments The above account of how the Enigma was broken was originally
explained to me by my colleague Professor Hugh Thurston, who was a British cryptanalyst
at Bletchley Park during the war. He characterized the code-breaking activity at Bletchley
Park as “one of the few just war eﬀorts human history can boast of.”

Chapter 12
Linear Algebraic Groups: an
Introduction
The purpose of this chapter is to give a brief informal introduction, with very
few proofs, to the subject of linear algebraic groups, a far-reaching general-
ization of matrix theory and linear algebra. A very readable treatment with
much more information is contained in the book Linear Algebraic Groups
and Finite Groups of Lie Type, by Gunter Malle and Donna Testerman. A
linear algebraic group is a matrix group G contained in a general linear group
GL(n, F), for some ﬁeld F and positive integer n, whose elements are precisely
the roots, or zeros, of a ﬁnite set of polynomial equations in n2 variables.
Linear algebraic groups have proved to be indispensable in many areas of
mathematics, e.g., number theory, invariant theory, algebraic geometry, and
algebraic combinatorics, to name some. The results we will describe concern,
for the most part, the case F = C, but results that are valid for C are usually
true whenever F is algebraically closed and of characteristic zero. There is also
a great deal of interest in linear algebraic groups over a ﬁeld of characteristic
p > 0, since many such groups give examples of ﬁnite simple groups.
12.1
Linear Algebraic Groups
In order to deﬁne what a linear algebraic group is, we must ﬁrst make some
remarks about polynomials on Fn×n. Let V be a ﬁnite-dimensional vector
space over F with basis v1, . . . , vn, and let x1, . . . , xn be the dual basis of V ∗.
Thus xi : V →F is the linear function deﬁned by xi(vj) = δij, where δij is the
Kronecker delta function. A monomial in x1, . . . , xn is a function of the form
xa1
i1 xa2
i2 · · · xak
ik : V →F, where a1, . . . , ak are positive integers, 1 ≤ij ≤n for
all indices and i1 < i2 < · · · < ik. A polynomial function in x1, . . . , xn over F
is a linear combination over F of a ﬁnite set of monomials. Thus a typical
c⃝Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0 12
383

384
12
Linear Algebraic Groups: an Introduction
polynomial has the form
f(x1, . . . , xn) =

ci1i2...ikxa1
i1 xa2
i2 · · · xak
ik ,
where the coeﬃcients ci1i2...ik are in F, and only ﬁnitely many are nonzero. Let
F[x1, . . . , xn] denote the set of all such polynomials. If V = Fn×n with basis
Eij, 1 ≤i, j ≤n, then the dual basis is denoted by xij, where 1 ≤i, j ≤n. A
basic example of a polynomial in F[xij] is given by the determinant
det(xij) =

π∈S(n)
sgn(π) xπ(1)1xπ(2)2 · · · xπ(n)n.
We now deﬁne what it means for a matrix group to be closed.
Deﬁnition 12.1. A subgroup G of GL(n, F) is said to be closed if G consists
of the common zeros of a ﬁnite set of polynomials f1, . . . , fk ∈F[xij]. A closed
subgroup G of GL(n, F) is called a linear algebraic group.
The group GL(n, F) is by default a linear algebraic group, since GL(n, F)
is the set of common zeros of the empty set of polynomials on Fn×n. The spe-
cial linear group SL(n, F) is a closed subgroup of GL(n, F), since SL(n, F)
is deﬁned by the setting det(xij) −1 = 0. That is, SL(n, F) consists of the
matrices A such that det(A) −1 = 0. The groups P(n), U(n, F), L(n, F), and
D(n, F) involved in the LPDU are also closed subgroups of GL(n, F), as is
T (n, F). For example, T (n, F), the subgroup consisting of all upper triangu-
lar elements of GL(n, F), is the set of zeros of the polynomials xij, where
i > j. The elements of the group P(n) of n × n permutation matrices satisfy
the equations xij(xij −1) = 0 for all i, j, since every entry of a permutation
matrix is zero or one. But these equations don’t capture the fact that the
columns of P are orthogonal; for that, we use the identity P T P = In. Thus,
P ∈P(n) if and only if P satisﬁes the set of polynomial equations

k
xkixkj = δij and xij(xij −1) = 0 (i, j, k = 1, . . . , n).
Hence P(n) is a linear algebraic group; in fact, this example shows that linear
algebraic groups can be ﬁnite. (The reader may wish to show that in fact,
every ﬁnite subgroup of GL(n, F) is closed.)
From the deﬁnition of a linear algebraic group, it is clear that the inter-
section of two linear algebraic groups is a linear algebraic group, and the
product of two linear algebraic groups is also a linear algebraic group. In
order to understand the importance of the condition that the elements of a
linear algebraic group are cut out by polynomial equations, one must intro-
duce some basic concepts from algebraic geometry, in particular, the notion
of an aﬃne variety. What is very useful is that the important subgroups of a
linear algebraic group themselves also turn out to be linear algebraic groups.

12.1
Linear Algebraic Groups
385
We know make a general deﬁnition that applies to arbitrary groups. The
reason for the terminology should become clear later.
Deﬁnition 12.2. A group G, not necessarily a linear algebraic group, is said
to be connected if G has no normal subgroup of ﬁnite index greater than one.
By deﬁnition, a ﬁnite group diﬀerent from {1} cannot be connected. To
see another example of a nonconnected group, consider O(n, F) for which
the characteristic of F is diﬀerent from two. In that case, P(n) is naturally
embedded as a subgroup of O(n, F). Thus, the determinant homomorphism
det : O(n, F) →{±1} is surjective. Consequently, by the ﬁrst isomorphism
theorem, its kernel is a proper normal subgroup of index two. If the charac-
teristic of F is zero, then GL(n, F), SL(n, F), and SO(n, F) are all connected.
Exercise 12.1.1. Show that if F is a Galois ﬁeld, then GL(n, F) is not con-
nected.
12.1.1
Reductive and semisimple groups
We will now get a bit technical for a while in order to focus on two of the
most important classes of linear algebraic groups: reductive and semisim-
ple groups. We will concentrate on linear algebraic groups G < GL(n, C).
Each such G has a unique maximal, closed, normal, and connected solvable
subgroup Rad(G), called the radical of G. For example, if G = GL(n, C),
then Rad(G) = C∗In. (Note: C∗In is closed, since it is the set of all zeros in
GL(n, C) of the polynomials xij and xii −xjj, where i ̸= j). If G = T (n, C)
or D(n, C), then Rad(G) = G, since G is solvable. We now state the ﬁrst
main deﬁnition.
Deﬁnition 12.3. Let G < GL(n, C) denote a connected linear algebraic
group. Then G is called reductive if the only unipotent element of Rad(G)
is the identity. It is called semisimple if the only closed, normal, connected
abelian subgroup of G is the identity. It is called simple if G is nonabelian
and G has no nontrivial closed normal subgroup. Finally, G is almost simple
if every closed normal subgroup of G is ﬁnite.
It is not hard to see that a semisimple linear algebraic group is reduc-
tive, but the converse isn’t true. For example, by the above remarks, the
general linear group GL(n, C) is reductive. However, GL(n, C) is not semi-
simple, since its radical C∗In is a nontrivial closed normal connected abelian
subgroup. Another example of a reductive group that is not semisimple is
D(n, C). The special linear group SL(n, C) is semisimple. This is hard to
prove from ﬁrst principles, so we will omit the proof. Note that SL(n, C)
contains a normal abelian subgroup, namely its center Z = {ζIn | ζn = 1}.
But Z is not connected, since {In} is a normal subgroup of ﬁnite index.

386
12
Linear Algebraic Groups: an Introduction
12.1.2
The classical groups
A list of all semisimple linear algebraic groups over C has been known since
the early part of the twentieth century. There are four inﬁnite families known
as the classical groups, and ﬁve exceptional groups, which we will not describe
here. When C is replaced by a Galois ﬁeld, all the classical and exceptional
groups become what are called groups of Lie type. With the exception of 26
so-called sporadic groups, all the ﬁnite simple groups are of this type. We have
already encountered three of the families of classical groups: SL(n, C) (the
special linear groups), SO(2n, C) (the even orthogonal groups), and SO(2n +
1, C) (the odd orthogonal groups). It may seem artiﬁcial to distinguish the
even and odd special orthogonal groups, but the mechanism used to classify
the semisimple groups forces us to put these groups into diﬀerent categories.
The fourth family consists of the symplectic groups Sp(2n, C). By deﬁnition,
Sp(2n, C) = {A ∈GL(2n, C) | AΩAT = Ω},
where
Ω =
 O
In
−In
O

.
If A ∈Sp(2n, C), then A−1 = ΩAT Ω−1, and since Ω2 = −I2n, we get A−1 =
−ΩAT Ω. Also, by deﬁnition, det(AAT ) = 1, so det(A) = ±1. In fact, it can
be shown that det(A) = 1, so Sp(2n, C) < SL(n, C). This follows by showing
that Sp(2n, C) is connected, since the kernel of det is a normal subgroup. The
symplectic groups are used in symplectic geometry, physics, and the theory
of alternating quadratic forms.
12.1.3
Algebraic tori
A linear algebraic group T < GL(n, C) is called an algebraic torus, or simply
a torus, if T is connected and abelian and every element of T is semisimple.
Algebraic tori are basic examples of reductive groups. This follows from the
multiplicative Jordan–Chevalley decomposition, since the only unipotent ele-
ment of a torus is the identity. If T is a subgroup of a linear algebraic group
G < GL(n, C), we call T a subtorus of G if T is an algebraic torus in GL(n, C).
A subtorus T of G that is not a proper subgroup of any other subtorus in G
is called a maximal torus in G. For example, D(n, C) is a maximal torus in
GL(n, C). More generally, we have the following.
Proposition 12.1. Every maximal torus T < GL(n, C) is conjugate to
D(n, C).

12.1
Linear Algebraic Groups
387
Exercise 12.1.2. Show that T =

a
b
−b
a

| a, b ∈C, a2 + b2 ̸= 0

and
D(2, C) are both maximal tori in GL(2, C), and prove that they are conjugate
in GL(2, C).
Exercise 12.1.3. Let T be the algebraic torus deﬁned in the previous exer-
cise. Show that D(2, R) and T ∩GL(2, R) are not conjugate in GL(2, R).
We next state an important fact.
Theorem 12.2. Every reductive linear algebraic group G contains a maxi-
mal torus, and any two maximal tori in G are conjugate by an element of G.
Moreover, if T < G is an algebraic torus, then T is contained in a maximal
torus.
Let us now describe some maximal tori for classical groups.
Case 1: SL(n, C). This is the easiest case to describe. In fact, the subgroup
of all diagonal matrices in SL(n, C), that is, SL(n, C) ∩D(n, C), is a maximal
torus.
Case 2: SO(2n, C). Starting with the maximal torus T = {diag(t, t−1) | t ∈
C∗} in SL(2, C), one can conjugate T into SO(2, C), since
R = Pdiag(t, t−1)P −1 =
 a
b
−b
a

,
where a = (t2 + 1)/t, b = (−it2 + 1)/t, and P =
1
√
2
1
i
i
1

. We will call R
a rotation matrix. It turns out that the set of all matrices of the form
R =
⎛
⎜
⎜
⎜
⎝
R1
O
· · ·
O
O
R2
· · ·
O
...
...
...
...
O
O
· · ·
Rn
⎞
⎟
⎟
⎟
⎠,
where R1, . . . , Rn are arbitrary rotation matrices, is a maximal torus in
SO(2n, C).
Case 3: SO(2n + 1, C). To obtain a maximal torus in SO(2n + 1, C), con-
sider the natural inclusion homomorphism i : SO(2n, C) →SO(2n + 1, C)
deﬁned by
R →

R
0
0
1

.
Then the image of R is a maximal torus in SO(2n + 1, C).
Case 4: Sp(2n, C). Finally, the set of all T = diag(x1, . . . , xn, x−1
1 , . . . , x−1
n ),
where each xi is in C∗, is a maximal torus for Sp(2n, C).

388
12
Linear Algebraic Groups: an Introduction
A maximal torus T in a linear algebraic group G deﬁnes a group action
of T on G by conjugation, namely (t, g) →t · g = tgt−1. This action gives
rise to the so-called roots and weights of the pair (G, T), which completely
determine G up to isomorphism (in the sense of linear algebraic groups) when
G is semisimple. Space does not permit us to elaborate on this theme, since
it requires introducing the Lie algebra of G. Instead, we will concentrate on
another group that we have already seen in a special case. This is the Weyl
group of (G, T). We will then discuss the role of the Weyl group in the theory
of linear algebraic groups.
12.1.4
The Weyl group
The Weyl group of a pair (G, T) consisting of a linear algebraic group G
and a maximal torus T < G is the group W(G, T) = NG(T)/T. We will see
below that if G = GL(n, C) and T = D(n, C), then W(G, T) is isomorphic to
the group P(n) of n × n permutation matrices. Recall that P(n) is a basic
component of the LPDU decomposition in GL(n, C). The matrices P and D
are always unique, though L and U need not be. We will give some examples
of Weyl groups and then explain how they play a role similar to that of P(n)
for GL(n, C) for arbitrary reductive linear algebraic groups. Let us begin with
a fundamental result.
Theorem 12.3. If G is reductive, then the normalizer NG(T) is also a linear
algebraic group, and the Weyl group W(G, T) is ﬁnite.
For simplicity, we will denote W(G, T) by W as long as it is clear what G
and T are. Since any two maximal tori in G are conjugate, the Weyl group
of (G, T) is independent of the choice of T up to an isomorphism induced by
an inner automorphism of G. The following result computes W when G is
either GL(n, C) or SL(n, C).
Proposition 12.4. If G = GL(n, C) or SL(n, C), then W ∼= S(n).
Proof. Suppose ﬁrst that G = GL(n, C) and T = D(n, C). We will show
that NG(T) is the semidirect product P(n)T. Note that if P ∈P(n) and
D = diag(d1, . . . , dn), then PDP −1 = diag(dσ(1), . . . , dσ(n)), where σ ∈S(n)
is the unique permutation such that P = Pσ. To see this, it suﬃces to check
that PDP −1 = diag(dσ(1), . . . , dσ(n)) whenever σ = (i i + 1) is a simple trans-
position. Since (i i + 1) is conjugate to (12), it suﬃces to let σ = (12) and
assume that D is of size 2 × 2. Hence we get the result, since
PDP −1 =
0
1
1
0
 a
0
0
b
 0
1
1
0

=
b
0
0
a

.

12.1
Linear Algebraic Groups
389
Since P(n) ∩T = (1), the subgroup H generated by P(n) and T is the semi-
direct product of P(n) and T, and H ≤NG(T). Moreover, transvections,
or elementary matrices of type III, do not normalize T. For example, in the
2 × 2 case,
1
0
u
1
 a
0
0
b
  1
0
−u
1

=

a
0
u(a −b)
b

.
It follows that NG(T) = H = P(n)T, as claimed. It follows from Exercise
11.4.7 that W ∼= P(n) via the natural homomorphism P(n) →W deﬁned by
P →PT. Hence W ∼= S(n) also. When G = SL(n, C), the above argument
doesn’t work, because if P = Pσ and sgn(σ) = −1, then P /∈SL(n, C), since
det(Pσ) = sgn(σ). This is ﬁxed by replacing a Pσ with sgn(σ) = −1 by the
matrix RPσ, where R = diag(−1, 1, . . . , 1). It is clear that conjugation by
RPσ has the same eﬀect as conjugation by Pσ, so W ∼= S(n) in the SL(n, C)
case also.
□
We will skip the details for the computation of the Weyl groups of the
remaining classical groups. The complete result is contained in the following
theorem.
Theorem 12.5. The Weyl groups of the classical groups are as follows:
(i) for G = GL(n, C) or SL(n, C), W ∼= S(n);
(ii) for G = SO(2n + 1, C) or SP(2n, C), W ∼= SP(n), where SP(n) denotes
the group of signed permutation matrices; and
(iii) for G = SO(2n, C), W ∼= SP(n)+, the subgroup of SP(n) consisting of
all signed permutation matrices having an even number of minus signs.
The group SP(n) consists of all elements of O(n, C) having integer entries.
In other words, every element of SP(n) is obtained from a permutation matrix
P by allowing ± 1 wherever a 1 occurs in P. Thus the order of SP(n) is 2nn!.
Since det(Pσ) = sgn(σ) for the permutation matrix Pσ associated to σ ∈S(n),
it follows that SP(n)+ = SP(n) ∩SL(n, C). Thus |SP(n)+| = 2n−1n!.
Each of the Weyl groups above is generated by reﬂections of Rn. We will
give a minimal set of reﬂections that generate in each case. Such reﬂections
are called simple.
Case 1: W = P(n). For each i = 1, . . . , n −1, let Hi denote the reﬂection
through the hyperplane orthogonal to ei −ei+1. The matrix of Hi with
respect to the standard basis is the row swap matrix Pi obtained from In by
swapping the ith and (i + 1)st rows. Then P1, . . . , Pn−1 are the simple reﬂec-
tions generating W. Note that Pi is the permutation matrix corresponding
to the simple transposition (i i + 1).
Case 2: W = SP(n). Here the simple reﬂections are P1, . . . , Pn−1 together
with the reﬂection Pn = diag(1, . . . , 1, −1), which reﬂects Rn through the
hyperplane xn = 0.

390
12
Linear Algebraic Groups: an Introduction
Case 3: W = SP(n)+. In the ﬁnal case, the simple reﬂections consist of
P1, . . . , Pn−1 together with the reﬂection Hn through the hyperplane orthog-
onal to en−1 + en. The matrix of Hn is −Pn−1. Thus the simple reﬂections
are P1, . . . , Pn−2, ±Pn−1.
The simple reﬂections listed above arise from the root systems of types
A, B, C, D in the theory of reﬂection groups (that is, groups generated by
reﬂections). The simple reﬂections allow one to deﬁne the notion of the length
of an element w of W as the minimal k such that there exists an expression
w = Pi1 · · · Pik with each Pj a simple reﬂection. The length of w is denoted by
ℓ(w). We will mention below an interesting role that the length function plays.
An excellent source for information and further study on reﬂection groups,
including their root systems, is Reﬂection Groups and Coxeter Groups (Cam-
bridge Studies in Advanced Mathematics), by James E. Humphreys.
12.1.5
Borel subgroups
If G is a connected linear algebraic group, a closed, connected subgroup B of
G that is solvable and not properly contained in any other closed, connected
solvable subgroup of G is called a Borel subgroup of G. Borel subgroups play
a very important role in the structure theory of linear algebraic groups, as
we will presently see. We have already seen that the upper triangular sub-
groups T (n, C) and T (n, C) ∩SL(n, C) of GL(n, C) and SL(n, C) are solv-
able. It turns out that they are Borel subgroups of GL(n, C) and SL(n, C)
respectively. This is easy to see for n = 2. The commutator ABA−1B−1 of
A, B ∈GL(2, C) is in general not in T (2, C). In fact, let G be a closed sub-
group of GL(2, C) properly containing T (2, C). Then there exists A ∈G
such that the entry a21 is nonzero. Then one can ﬁnd B ∈T (2, C) such
that C = ABA−1B−1 has a nonvanishing (2, 1) entry. For example, assuming
ad −bc = 1 to avoid denominators, then

a
b
c
d
 
1
1
0
1
 
d
−b
−c
a
 
1
−1
0
1

=

1 −ac
a2 + ac −1
−c2
1 + ac + c2

.
It follows that every commutator subgroup G(i) is nontrivial, so G cannot
be solvable. Hence, T (2, C) is a Borel subgroup of GL(2, C). The reader can
then extend this reasoning to conclude that T (n, C) is a Borel subgroup of
GL(n, C). Similarly, T (n, C) ∩SL(n, C) is a Borel subgroup of SL(n, C).
The existence of a Borel subgroup B in a linear algebraic group G follows
from the fact that an algebraic torus is a connected solvable (in fact, abelian)
linear algebraic group. Now, if H is any closed connected solvable linear
algebraic group such that T ≤H ≤G, then either H is maximal or there

12.1
Linear Algebraic Groups
391
exists a closed connected solvable linear algebraic group H′ of G such that
H < H′. We need to know that iterating this remark will produce a maximal
closed connected solvable linear algebraic group B ≤G such that T ≤B. In
fact, if G < GL(n, C), then there cannot be a strictly increasing sequence of
linear algebraic groups
H1 < H2 < · · · < Hk < Hk+1 < · · · < G.
The tool needed to guarantee this is a fundamental theorem in abstract alge-
bra known as the Hilbert basis theorem, which is a result about ideals in
F[x1, . . . , xm]. The existence of a Borel subgroup B such that T ≤B thus
follows. It can also be shown that every Borel subgroup in a linear algebraic
group G < GL(n, C) can be obtained as G ∩B for some Borel B in GL(n, C).
Remark. The Borel subgroups of the orthogonal groups and the symplectic
groups are harder to describe, but a nice description is given in the above-
mentioned book by Malle and Testerman on page 38.
12.1.6
The conjugacy of Borel subgroups
Every subgroup H < GL(n, C) conjugate to a linear algebraic group G <
GL(n, C) is also a linear algebraic group. For if H = gGg−1, and p(X)
is a polynomial on Cn×n such that p(X) = 0 if X ∈G, then the function
q(X) = p(g−1Xg) is also a polynomial on Cn×n such that q(Y ) = 0 if Y ∈H,
since q(Y ) = p(g−1Y g) = p(g−1gXg−1g) = p(X) = 0. It follows that every
subgroup of G that is conjugate to a Borel subgroup is itself a Borel subgroup.
Thus the lower triangular matrices also give Borel subgroups in GL(n, C),
and those in SL(n, C) give a Borel subgroup in SL(n, C). Moreover, there
are also Borel subgroups that are quite hard to describe.
The Borel subgroups of a reductive linear algebraic group G have two
fundamental and deep properties: any two Borel subgroups of G are conjugate
by an element of G, and the normalizer of every Borel subgroup of B < G
is B itself (cf. Malle and Testerman). These two facts give rise to a nice
parameterization of the set of Borel subgroups of G.
Proposition 12.6. Let BG denote the set of all Borel subgroups of G, and
let B denote a ﬁxed Borel subgroup. Then the mapping gB →gBg−1 deﬁnes
a bijection between the space of left cosets G/B and BG.
Recall that we showed that every maximal torus T in G is contained in a
Borel subgroup B of G. We just remarked that NG(B) = B for every Borel
subgroup, and in fact, NG(T) ∩B = T. Since T < B, it makes sense to deﬁne
wBw−1 by nwBn−1
w
for every representative nw of w in the Weyl group W
of (G, T). The interesting point is that wBw−1 is also a Borel subgroup of G

392
12
Linear Algebraic Groups: an Introduction
containing T. Moreover, if w1, w2 ∈W, then w1Bw−1
1
= w2Bw−1
2
if and only
if w1 = w2.
Theorem 12.7. The correspondence sending w ∈W to wBw−1 is a bijec-
tion from the Weyl group of (G, T) onto the set of Borel subgroups of G
containing T. In particular, the number of Borel subgroups of G containing
T is |W|.
12.1.7
The ﬂag variety of a linear algebraic group
If B is a Borel subgroup of G, then the coset space G/B turns out to have
the structure of an algebraic variety, which is the fundamental concept in
the ﬁeld of algebraic geometry. This allows one to use results from algebraic
geometry to study the abstract set BG of all Borel subgroups in G. When
G = GL(n, C), we can explicitly describe G/B in terms from linear algebra.
Let us suppose B = T (n, C) and ﬁx A ∈GL(n, C). Let U be an arbitrary
element of B. Since multiplication on the right by each U performs rightward
column operations, the spans of the ﬁrst k columns of A and AU are the same
for all k. Let Vk denote this subspace. Since A is invertible, dim Vk = k for
all k. Thus the coset AB uniquely determines a strictly increasing sequence
of subspaces of Cn, namely
V1 ⊂V2 ⊂· · · ⊂Vn−1 ⊂Cn.
(12.1)
Conversely, every such sequence uniquely determines a coset of G/B. This
gives us a bijection from G/B onto the set of all sequences of the form (12.1).
These sequences are called complete ﬂags in Cn. The set of complete ﬂags in
Cn is often denoted by Flag(Cn). It has many applications in geometry and
related areas. If G is an arbitrary reductive group and B a Borel subgroup,
then the coset space G/B is known as the ﬂag variety of G.
Now let T be a maximal torus in a reductive group G and let B be a Borel
subgroup of G such that T < B. Consider the action of T on the ﬂag variety
G/B by left translation given explicitly by (t, gB) →tgB. As we have already
seen in the proofs of the Sylow theorems, it is often very useful to know the
ﬁxed-point set of a group action.
Proposition 12.8. The ﬁxed-point set (G/B)T of the left multiplication
action of T on G/B is precisely the set of cosets wB as w varies through
the Weyl group. In particular, T has exactly |W| ﬁxed points on G/B.
Proof. Suppose TgB = gB. Then g−1Tg < B, so T < gBg−1. Thus gBg−1 is a
Borel subgroup of G containing T. Therefore, by Cartan’s theorem, gBg−1 =
wBw−1 for some w ∈W. Consequently, gB = wB by Proposition 12.6.
□

12.1
Linear Algebraic Groups
393
The fact that the number of ﬁxed points of T on G/B is |W| translates
into a statement about the topology of the ﬂag variety G/B of G, namely,
the Euler characteristic of G/B is |W|, the order of the Weyl group. This is a
famous theorem of Andr´e Weil that was discovered around 1930. The Euler
characteristic is a topological invariant of a space that generalizes the number
F −E + V , which measures the number of holes in a two-dimensional surface
without boundary.
Example 12.1. It is interesting to compute (G/B)T when G = GL(n, C) in
two ways. First, let us view GL(n, C)/B as the complete ﬂags in Cn. Note
that if T = T (n, C), then every ﬂag of the form
Cei1 ⊂spanC{ei1, ei2} ⊂· · · ⊂spanC{ei1, ei2, . . . , ein−1} ⊂Cn
is ﬁxed by T. Here i1, i2, . . . , in−1 are n −1 distinct integers in [1, n]. Since
there are exactly n! such ﬂags, and |S(n)| = n!, these ﬂags comprise the
T-ﬁxed points. Now let us compute (G/B)T another way, this time using
the LPDU decomposition. The points (or cosets) PB, where P ∈P(n), are
ﬁxed under T, because TPB = PTB, since the normalizer in GL(n, C) of T
is P(n). Thus, P −1TP = T, so TP = PT, whence TPB = PB. Hence, the
points PB as P ranges over P(n) are ﬁxed under T. But in the LPDU
decomposition of an element of GL(n, C), we know that P and D are unique.
Thus if P, P ′ ∈P(n), then PB = P ′B if and only if P = P ′. Therefore, since
|P(n)| = n!, we have found (GL(n, C)/B)T in two (equivalent) ways.
⊠
12.1.8
The Bruhat decomposition of GL(n, F)
We are now going to discuss how the LPDU decomposition generalizes to
an arbitrary reductive linear algebraic group G < GL(n, F), where F is an
arbitrary ﬁeld. We will begin by ﬁnding a slightly diﬀerent version of LPDU
for G = GL(n, F). According to LPDU, we can decompose GL(n, F) as the
product of four subgroups, namely
GL(n, F) = L(n, F)P(n)D(n, F)U(n, F).
Of course, D(n, F)U(n, F) is the upper triangular Borel subgroup T (n, F).
Another Borel subgroup of GL(n, F) is, in fact, the set T (n, F)−of all lower
triangular elements of GL(n, F). Since T (n, F) and T (n, F)−are Borel sub-
groups containing the maximal torus D(n, F), they are conjugate by an ele-
ment of the Weyl group P(n). In fact, this element is
P0 = (en en−1 · · · e2 e1).

394
12
Linear Algebraic Groups: an Introduction
Put another way, if P0 is the permutation matrix with ones on the antidiago-
nal, then L(n, F) = P0T (n, F)P −1
0
. Now GL(n, F) = P0GL(n, F) and P −1
0
=
P0. Thus,
GL(n, F) = P0GL(n, F) = P0L(n, F)P0P0P(n)T (n, F) = T (n, F)P(n)T (n, F).
Recall that Pσ = (eσ(1) · · · eσ(n)). Hence
GL(n, C) =

σ∈S(n)
T (n, F)PσT (n, F).
(12.2)
This is a double coset decomposition of GL(n, C) in which the coset rep-
resentatives come from the subgroup P(n), which is isomorphic to S(n).
The double coset decomposition (12.2) is called the Bruhat decomposition of
GL(n, F). Applied to the ﬂag variety of GL(n, F), the Bruhat decomposition
implies
Flag(Fn) =

σ∈S(n)
T (n, F) · PσT (n, F).
Put another way, Flag(Fn) is the union of the T (n, F) orbits of the cosets
PσT (n, F) ∈Flag(Fn). These points are also the ﬁxed points of the action
of D(n, F) on Flag(Fn). The double cosets T (n, F)PσT (n, F) in GL(n, F) are
called Bruhat cells. The T (n, F)-orbits of the cosets PσT (n, F) in Flag(Fn) are
called Schubert cells. There are exactly n! Bruhat cells and the same number
of Schubert cells.
Let us now suppose F = C. Then the Bruhat cells have an interesting
connection with the length function on the Weyl group P(n). Suppose ﬁrst
that w = Pσ, and let the length be given by ℓ(w) = r. This means that
w has a minimal expression as w = s1s2 · · · sr, where each si is one of
the reﬂections P1, . . . , Pn−1 through the hyperplanes in Rn orthogonal to
e1 −e2, . . . , en−1 −en respectively. The length ℓ(w) is also the minimal num-
ber of simple transpositions (i i + 1) needed for an expression of σ as a
product of transpositions. (Note: if w = 1, then we agree that ℓ(w) = 0.)
It is a nice exercise to prove that if w = P0, then ℓ(w) = n(n −1)/2. Now
the Borel T (n, C) is deﬁned by setting the coordinate functions xij = 0
for i > j in GL(n, C). Intuitively, therefore, the dimension of T (n, C) is
n(n + 1)/2. In fact, we can parameterize T (n, C) using n free variables a1j,
n −1 free variables a2j, and so on. Thus, the dimension of T (n, C) should
be n + (n −1) + (n −2) + · · · + 2 + 1 = n(n + 1)/2. Now, the dimension of
T (n, C)P0T (n, C) is the same as the dimension of P −1
0
T (n, C)P0T (n, C) =
L(n, C)T (n, C). But recall that L(n, C)T (n, C) consists of all A ∈GL(n, C)
with LPDU decomposition having P = In. In fact, we showed that

12.1
Linear Algebraic Groups
395
L(n, C)T (n, C) = {A ∈GL(n, C) | det(Ai) ̸= 0, i = 1, . . . , n},
where Ai is the i × i matrix in the upper left-hand corner of A. Further-
more, if A ∈L(n, C)T (n, C), then in the factorization A = LDU, L, D,
and U are all unique. Now L(n, C) is described by n(n −1)/2 independent
variables aij, where i > j, D(n, C) is described by n independent variables
aii, and U(n, C) by another n(n −1)/2 independent variables aij, where
j > i. Thus L(n, C)T (n, C) is described by n(n −1)/2 + n(n −1)/2 + n =
n(n −1) + n = n2 independent variables. Similarly, if P = In, which corre-
sponds to P = Pw with w = 1, then dim T (n, C)PT (n, C) = dim T (n, C).
The general formula for the dimension of a Bruhat cell is as follows.
Proposition 12.9. If w = Pσ, then the Bruhat cell T (n, C)wT (n, C) has
dimension ℓ(w) + dim T (n, C) = ℓ(w) + n(n + 1)/2. The dimension of the
corresponding Schubert cell is ℓ(w).
The very interesting proof is beyond the scope of this introduction. The
reader may well want to verify it in some special cases. For example, when
n = 3 and σ is the transposition (12), then BPσB′ has the form
BPσB′ =
⎛
⎝
a
b
∗
0
c
∗
0
0
d
⎞
⎠
⎛
⎝
0
1
0
1
0
0
0
0
1
⎞
⎠
⎛
⎝
r
s
∗
0
t
∗
0
0
u
⎞
⎠=
⎛
⎝
br
at + bs
∗
cr
cs
∗
0
0
du
⎞
⎠,
(12.3)
where the asterisks stand for some entries in C. The (3, 1) and (3, 2) entries
of this matrix are zero, but the other entries are (essentially) not restricted
except for the condition that the determinant of BPσB′ is nonzero. This
(admittedly not rigorous) argument gives that the dimension of BPσB′ is 7,
which has the form ℓ(12) + dim B.
12.1.9
The Bruhat decomposition of a reductive
group
We now have the necessary ingredients to generalize the Bruhat decomposi-
tion to an arbitrary reductive linear algebraic group G, namely a maximal
torus T, the Weyl group W = NG(T)/T, and a Borel subgroup B of G con-
taining T. Recall that W is a ﬁnite group. It will play the role that P(n)
plays when G = GL(n, C). However, W isn’t in general a subgroup of G. But
if T ⊂B, we can still multiply the identity coset B by a coset of T. So let
w = nwT, where nw ∈NG(T), and deﬁne wB to be the coset nwB. This is
well deﬁned, since two representatives of w diﬀer by an element of T. Thus,
the Bruhat cell BwB in G is well deﬁned: it is the union of the cosets bnwB,
where b varies through B. The double coset BwB is called a Bruhat cell in G.

396
12
Linear Algebraic Groups: an Introduction
In the ﬂag variety G/B, the B-orbit B · wB of the coset wB is called a Schubert
cell. The LPDU decomposition of GL(n, C) generalizes as follows.
Theorem 12.10. Let G be a reductive linear algebraic group over C, B a
Borel subgroup of G, and T a maximal torus in G such that T < B. Then G
is the union of the Bruhat cells BwB as w varies through W, so G = BWB.
Moreover, NG(T) ∩B = T; hence if w ̸= w′ in W, then wB ̸= w′B. Thus, the
number of distinct Bruhat cells in G is the order of W. Finally, the dimension
of the Bruhat cell BwB is ℓ(w) + dim B.
12.1.10
Parabolic subgroups
A closed subgroup P of a linear algebraic group G is called parabolic if P
contains a Borel subgroup. We will classify the parabolic subgroups of G
after we give an example.
Example 12.2. Consider the standard case G = GL(n, C), B = T (n, C),
and T = D(n, C). As we have seen, W = S(n) ∼= P(n). Now suppose j + k =
n. Let P(j, k) < P(n) denote the subgroup consisting of permutation matri-
ces with block decomposition
P =

P1
O
O
P2

,
where P1 ∈P(j) and P2 ∈P(k). But P = Pτ for a unique τ ∈S(n). Since
P1 = Pμ and P2 = Pν for unique μ ∈S(j) and ν ∈S(k), we can deﬁne
an injective homomorphism ϕ : S(j) × S(k) →S(n) by ϕ(μ × ν) = τ. Let
S(j, k) < S(n) denote the image of ϕ. Then S(j, k) ∼= P(j, k). Then BP(j, k)B
is the set of all matrices of the form

A
∗
O
B

,
where A ∈GL(j, C) and B ∈GL(k, C). Hence, BP(j, k)B is a closed sub-
group of GL(n, C).
More generally, let P(j1, . . . , jk) denote all matrices of the form
⎛
⎜
⎜
⎜
⎝
A1
∗
· · ·
∗
O
A2
· · ·
∗
...
...
...
...
O
O
· · ·
Ak
⎞
⎟
⎟
⎟
⎠,
where Ai ∈GL(ji, C) and j1 + · · · + jk = n. Then the P(j1, . . . , jk) are the
parabolic subgroups of GL(n, C) containing T (n, C).
⊠

12.1
Linear Algebraic Groups
397
Now suppose G is reductive and ﬁx a maximal torus T and a Borel sub-
group B such that T < B < G. Let P be a parabolic subgroup in G containing
B, and deﬁne the Weyl group WP to be NP(T)/T. Then we have the following
theorem.
Theorem 12.11. The parabolic subgroup P is a union of certain Bruhat
cells. In particular, P = BWPB. Moreover, the number of Bruhat cells in P
is |WP|.
Not every subgroup Z < W gives a parabolic subgroup in this manner. For
example, let σ = (i j) be a transposition that is not simple; that is, |i −j| > 1.
Then if Z = {In, Pσ}, BZB is not a subgroup of GL(n, C). A natural question
then is which subgroups of Weyl groups have the form WP for some parabolic
P. In fact, these subgroups can be described in a simple way. An element
P ̸= 1 of W is called simple if BPB ∪B is a subgroup of G. Note that if P
is simple, then P 2 = 1. (We leave the proof to the reader.) Hence the simple
elements have order two. Let S ⊂W denote the set of all simple elements.
In the standard case, the simple elements are the reﬂections Pσ, where σ
is a simple transposition. The next result is a complete description of the
parabolic subgroups of the reductive group G.
Theorem 12.12. A subgroup Z < W has the property that BZB is a par-
abolic subgroup of G such that B < BZB if and only if there exists a subset
J of S such that Z =< σ | σ ∈J >. Moreover, every parabolic subgroup of G
is conjugate in G to a parabolic P containing B.
Corollary 12.13. The set of simple reﬂections in W generates W.
Proof. Let Z be the subgroup of W such that G = BZB. Since G = BWB,
it follows that W = Z.
□
Corollary 12.14. Every parabolic subgroup of GL(n, C) containing T (n, C)
has the form P(j1, . . . , jk) for some choice of the ji.

398
12
Linear Algebraic Groups: an Introduction
12.2
Linearly reductive groups
Let us now treat a new idea. Suppose G < GL(n, F) is a linear algebraic
group. How does one describe the G-sets in V = Fn? This is one of the ques-
tions treated in an area known as representation theory. The main question is
what one can say if we restrict the notion of G-sets to G-invariant subspaces.
A subspace W of V that is also a G-set is called a G-invariant subspace.
Two of the basic questions are which G-invariant subspaces have comple-
mentary G-invariant subspaces and which G-invariant subspaces do not have
any nontrivial proper G-invariant subspaces.
12.2.1
Invariant subspaces
Suppose G < GL(n, F) is a linear algebraic group, and let V denote Fn. A
G-invariant subspace W of V is called G-irreducible if there is no nontrivial
G-invariant subspace U of W. We say that G is linearly reductive or completely
reducible if whenever W is a nontrivial G-invariant subspace of V , there
exists a G-invariant subspace U of V such that V = U ⊕W. If V has no
proper G-invariant subspace except {0}, then we say that G acts irreducibly
on V . For example, GL(n, F) acts irreducibly on Fn.
Here are two basic examples. First of all, we have the following result.
Proposition 12.15. If T < GL(n, F) is an algebraic torus, then T is lin-
early reductive.
Proof. (sketch) By deﬁnition, every element of T is semisimple, and since T
is abelian, it follows that all elements of T are simultaneously diagonalizable.
Thus, T is conjugate to a subgroup of D(n, F). It follows from this that T is
linearly reductive.
□
The second basic example is the group P(n) of n × n permutation matri-
ces acting on Rn. Observe that the line ℓ= R(e1 + e2 + · · · + en) is stable
under P(n), so the hyperplane H orthogonal to ℓis also. This hyperplane
has equation x1 + x2 + · · · + xn = 0 and is clearly invariant under P(n).
Proposition 12.16. The only nontrivial P(n)-invariant subspaces of Rn are
H and ℓ. In particular, the action of P(n) on H is irreducible.
Exercise 12.2.1. Prove this proposition.
12.2.2
Maschke’s theorem
Maschke’s theorem says the following.

12.2
Linearly reductive groups
399
Theorem 12.17. A ﬁnite group G < GL(n, F) is linearly reductive if either
F is of characteristic zero or |G| is prime to the characteristic of F.
Proof. Let V = Fn and suppose W is a G-invariant subspace of V . Let T :
V →W be any linear mapping such that T(w) = w if w ∈W. We now alter
T using an averaging trick. In order to do this, we will use the fact that |G|
is invertible in F. This is guaranteed, since the characteristic of F either is
zero or is prime to |G|. Let ϕ : V →W be deﬁned by
ϕ(v) =
1
|G|

g∈G
g ◦T(g−1(v)).
(12.4)
Then ϕ(w) = w if w ∈W, and for every h ∈G and v ∈V , ϕ(h(v)) =
h(ϕ(v)). (The proof of this is left for the reader.) Now, ker ϕ ∩W = {0},
so by the rank–nullity theorem, V = W ⊕ker ϕ. But ker ϕ is G-invariant,
since if v ∈ker ϕ, then ϕ(g(v)) = g(ϕ(v)) = g0 = 0.
□
12.2.3
Reductive groups
When G < GL(n, F) is linearly reductive, it follows that V = Fn admits a
direct sum decomposition
V = W1 ⊕W2 ⊕· · · ⊕Wk,
where each Wi is a G-irreducible subspace. We just saw that if the char-
acteristic of F is zero, then every ﬁnite subgroup G < GL(n, F) is linearly
reductive. Reductive groups are fundamental partly because of the following
classical result.
Theorem 12.18. If F is algebraically closed of characteristic zero, then
every reductive subgroup G < GL(n, F) is linearly reductive.
The mapping ϕ used in the proof of Maschke’s theorem is known as
a Reynolds operator. When G is not ﬁnite but F = C, the proof uses a
Reynold’s-type operator deﬁned by the Haar integral over G. It was an open
question until the 1980s whether reductive subgroups of GL(n, F), F alge-
braically closed of positive characteristic, are linearly reductive. The answer
is yes if the notion of linearly reductive is replaced by a slightly weaker notion.

400
12
Linear Algebraic Groups: an Introduction
12.2.4
Invariant theory
Suppose G < GL(n, F), where F has characteristic zero. A polynomial
f ∈F[x1, . . . , xn] is said to be G-invariant if f is constant on every G-
orbit. In other words, for all (a1, . . . , an) ∈Fn, we have f(g(a1, . . . , an)) =
f(a1, . . . , an) for all g ∈G. For example, if G acts on Fn×n by conjugation,
then the determinant det ∈F[xij], 1 ≤i, j ≤n, is GL(n, F)-invariant. We
remark that G-invariants need to be deﬁned diﬀerently when F has positive
characteristic.
In the nineteenth century, mathematicians who worked in the ﬁeld of
invariant theory concentrated on the problem of constructing invariants, espe-
cially fundamental invariants, namely G-invariant polynomials f1, . . . , fk such
that every G-invariant f can be written uniquely as
f =

cα1,...,αkf α1
1 · · · f αk
k ,
(12.5)
where all cα1,...,αk are in F. David Hilbert, one of the greatest mathematicians
of the nineteenth and twentieth centuries, was the ﬁrst to realize that a new
approach had to be taken in order to further the ﬁeld, and he proved in 1888
that such invariants must exist in a certain general setting without explicitly
constructing them. This approach was initially condemned, but eventually
it was redeemed by his famous 1893 paper that established bounds on the
degrees of the (unknown) generators. Hilbert’s paper not only revolutionized
invariant theory, it established a new ﬁeld called commutative algebra, which
is still a very active area.
An important example illustrating invariant theory is the fundamental
theorem on symmetric polynomials. A polynomial f ∈F[x1, . . . , xn] is said
to be symmetric if
f(x1, . . . , xn) = f(xσ(1), . . . , xσ(n))
for all σ ∈S(n). Since S(n) and P(n) are isomorphic via the isomorphism
σ →Pσ, symmetric polynomials are exactly the P(n)-invariant polynomials
for the natural action of P(n) on Fn. Recall the elementary symmetric func-
tions σ1, σ2, . . . , σn, which were deﬁned when we discussed the characteristic
polynomial. Namely,
σ1 = x1 + · · · + xn,
σ2 =

i<j
xixj,
. . .
, σn = x1 · · · xn.
The fundamental theorem on symmetric polynomials is the following.
Theorem 12.19. Let F be a ﬁeld of characteristic zero. Then every sym-
metric polynomial f ∈F[x1, . . . , xn] can be expressed in exactly one way in
the form f = g(σ1, σ2, . . . , σn) for some g ∈F[x1, . . . , xn].

12.2
Linearly reductive groups
401
This basic fact has a beautiful generalization known as the Chevalley–
Shephard–Todd theorem. I will state a special case originally proved by
Chevalley in 1955. By a reﬂection of Fn, we mean an element of GL(n, F)
having order two that ﬁxes pointwise a hyperplane in Fn.
Theorem 12.20. Assume that the ﬁeld F has characteristic zero, and let
G < GL(n, F) be a ﬁnite group generated by reﬂections. Then there exist G-
invariants τ1, . . . , τn ∈F[x1, . . . , xn] such that every G-invariant f ∈F[x1, . . . ,
xn] can be expressed uniquely as f = g(τ1, . . . , τn) for some g ∈F[x1, . . . , xm].
Recall that Weyl groups are examples of ﬁnite groups generated by reﬂec-
tions. A very simple case of Chevalley’s theorem is illustrated by the following
example.
Example 12.3. Consider the group SP(n) of n × n signed permutation
matrices. Recall that a signed permutation matrix is an orthogonal matrix
whose only entries are 0 and ±1. The fundamental invariants of SP(n) are
easy to guess: they are τ1 = x2
1 + · · · + x2
n, τ2 = 
i<j x2
i x2
j, and in general,
τk(x1, . . . , xn) = σk(x2
1, . . . , x2
n) for all k = 1, . . . , n.
⊠
Finally, let us mention a result that connects invariant theory and reduc-
tive groups.
Theorem 12.21. Suppose F is algebraically closed and G < GL(n, F) is
reductive. Assume that v ̸= 0 is a vector in Fn such that g(v) = v for all
g ∈G. Then there exists a G-invariant f ∈F[x1, . . . , xn] such that f(v) ̸= 0
but f(0) = 0.
This implies, for example, that there exist invariant polynomials f1, . . . , fk
such that every G-invariant f can be represented as in (12.5). Invariant theory
for reductive groups is now called geometric invariant theory, or GIT for short.
GIT was founded in a famous 1965 paper by David Mumford in which he
conjectured the above theorem.

Bibliography
Groups and Fields
Alperin, J. L.; Bell, Rowen B. Groups and representations. Graduate Texts in Math-
ematics, 162. Springer-Verlag, New York, 1995.
Artin, Michael Algebra. Prentice Hall, Inc., Englewood Cliffs, NJ, 1991.
Dummit, David S.; Foote, Richard M. Abstract algebra. Third edition. John Wiley
and Sons, Inc., Hoboken, NJ, 2004.
Herstein, I. N. Abstract algebra. Third edition. With a preface by Barbara Cortzen
and David J. Winter. Prentice Hall, Inc., Upper Saddle River, NJ, 1996.
Humphreys, John F. A course in group theory. Oxford Science Publications. The
Clarendon Press, Oxford University Press, New York, 1996.
Rotman, Joseph J. An introduction to the theory of groups. Fourth edition. Graduate
Texts in Mathematics, 148. Springer-Verlag, New York, 1995.
van der Waerden, B. L. Modern Algebra. Vol. I. Translated from the second revised
German edition by Fred Blum. With revisions and additions by the author. Frederick
Ungar Publishing Co., New York, N. Y., 1949.
Matrix Theory
Gantmacher, F. R. The theory of matrices. Vol. 1. Translated from the Russian by K.
A. Hirsch. Reprint of the 1959 translation. AMS Chelsea Publishing, Providence,
RI, 1998.
Herstein, I. N.; Winter, David J. Matrix theory and linear algebra. Macmillan Pub-
lishing Company, New York; Collier Macmillan Publishers, London, 1988.
Strang, Gilbert Linear algebra and its applications. Second edition. Academic Press
[Harcourt Brace Jovanovich, Publishers], New York-London, 1980.
Determinants
Lang, Serge Linear algebra. Reprint of the third edition. Undergraduate Texts in
Mathematics. Springer-Verlag, New York, 1989.
© Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0
403

404
Bibliography
Muir, Thomas A treatise on the theory of determinants. Revised and enlarged by
William H. Metzler Dover Publications, Inc., New York 1960.
Shilov, Georgi E. Linear algebra. Revised English edition. Translated from the
Russian and edited by Richard A. Silverman. Dover Publications, Inc., New York,
1977.
Turnbull, H. W. The theory of determinants, matrices, and invariants. 3rd ed. Dover
Publications, Inc., New York 1960.
Vector Spaces
Artin, Michael Algebra. Prentice Hall, Inc., Englewood Cliffs, NJ, 1991.
Birkhoff, Garrett; MacLane, Saunders A Survey of Modern Algebra. Macmillan Com-
pany, New York, 1941.
Halmos,PaulR.Finite-dimensionalvectorspaces.Reprintingofthe1958secondedi-
tion. Undergraduate Texts in Mathematics. Springer-Verlag, New York-Heidelberg,
1974.
Hoffman, Kenneth; Kunze, Ray Linear algebra. Second edition Prentice-Hall, Inc.,
Englewood Cliffs, N.J. 1971.
Lang, Serge Linear algebra. Reprint of the third edition. Undergraduate Texts in
Mathematics. Springer-Verlag, New York, 1989.
Samelson, Hans An introduction to linear algebra. Pure and Applied Mathematics.
Wiley-Interscience [John Wiley and Sons], New York-London-Sydney, 1974.
Herstein, I. N.; Winter, David J. Matrix theory and linear algebra. Macmillan Pub-
lishing Company, New York; Collier Macmillan Publishers, London, 1988.
Linear Transformations
Coxeter, H. S. M. Introduction to geometry. Reprint of the 1969 edition. Wiley
Classics Library. John Wiley & Sons, Inc., New York, 1989.
Gelfand, I. M. Lectures on linear algebra. With the collaboration of Z. Ya. Shapiro.
Translated from the second Russian edition by A. Shenitzer. Reprint of the 1961
translation. Dover Books on Advanced Mathematics. Dover Publications, Inc., New
York, 1989.
Herstein, I. N.; Winter, David J. Matrix theory and linear algebra. Macmillan Pub-
lishing Company, New York; Collier Macmillan Publishers, London, 1988.
Weyl, H. Symmetry. Reprint of the 1952 original. Princeton Science Library. Prince-
ton University Press, Princeton, NJ, 1989.
Eigentheory
Lax, Peter D. Linear algebra. Pure and Applied Mathematics (New York). A Wiley-
Interscience Publication. John Wiley and Sons, Inc., New York, 1997.
Strang, Gilbert Linear algebra and its applications. Second edition. Academic Press
[Harcourt Brace Jovanovich, Publishers], New York-London, 1980.

Bibliography
405
Unitary Diagonalization and Quadratic Forms
Gelfand, I. M. Lectures on linear algebra. With the collaboration of Z. Ya. Shapiro.
Translated from the second Russian edition by A. Shenitzer. Reprint of the 1961
translation. Dover Books on Advanced Mathematics. Dover Publications, Inc., New
York, 1989.
Samelson, Hans An introduction to linear algebra. Pure and Applied Mathematics.
Wiley-Interscience [John Wiley and Sons], New York-London-Sydney, 1974.
Strang, Gilbert Linear algebra and its applications. Second edition. Academic Press
[Harcourt Brace Jovanovich, Publishers], New York-London, 1980.
Theory of Linear Mappings
Birkhoff, Garrett; MacLane, Saunders A Survey of Modern Algebra. Macmillan Com-
pany, New York, 1941.
Gelfand, I. M. Lectures on linear algebra. With the collaboration of Z. Ya. Shapiro.
Translated from the second Russian edition by A. Shenitzer. Reprint of the 1961
translation. Dover Books on Advanced Mathematics. Dover Publications, Inc., New
York, 1989.
Hoffman, Kenneth; Kunze, Ray Linear algebra. Second edition Prentice-Hall, Inc.,
Englewood Cliffs, N.J. 1971.
Strang, Gilbert Linear algebra and its applications. Second edition. Academic Press
[Harcourt Brace Jovanovich, Publishers], New York-London, 1980.
Linear Algebraic Groups
Alperin, J. L.; Bell, Rowen B. Groups and representations. Graduate Texts in Math-
ematics, 162. Springer-Verlag, New York, 1995.
Dieudonné, Jean: Carrell, James B. Invariant Theory, Old and New. Academic Press,
New York-London, 1971.
Humphreys, James E. Linear algebraic groups. Graduate Texts in Mathematics, No.
21. Springer-Verlag, New York-Heidelberg, 1975.
Humphreys, James E. Reﬂection groups and Coxeter groups. Cambridge Studies in
Advanced Mathematics, 29. Cambridge University Press, Cambridge, 1990.
Malle, Gunter; Testerman, Donna Linear algebraic groups and ﬁnite groups of Lie
type. Cambridge Studies in Advanced Mathematics, 133. Cambridge University
Press, Cambridge, 2011.

Index
Symbols
G-set, 339
k-cycle, 341
m-frame, 161
p-ary linear code of length n, 187
p-group, 344
A
Abelian group, 12
Adjoint, 133, 282
Adjoint map, 236
Algebraically closed ﬁeld, 45
Algebraic torus, 384
Alternating group, 121
Angle between vectors, 172
Associative law, 12
B
Basic codewords, 187
Basic null vectors, 79
Basis, 147
Bijective, 3
Binary operation, 2
Birkhoff decomposition, 340
Borel sgn, 392
Bounded set, 286
Bruhat cell, 396
Bruhat decomposition, 340, 395
C
Center, 340
Centralizer, 339
Characteristic equation, 240
Characteristic of a ﬁeld, 50
Characteristic polynomial, 243
Cipher, 375
Closed subgroup, 382
Code, 187
Codewords, 187
Commutator subgroup, 367
Complete ﬂag, 161
Completely reducible, 400
Complex conjugate, 43
Complex exponential, 43
Complex number, 42
Complex polynomial, 45
Composition series, 362
Congruent matrices, 306
Conjugacy class, 339
Conjugation, 20
Conjugation action, 339
Connected group, 383
Convex polyhedron, 286
Convex set, 286
Coordinates, 222
Corner entry, 68
Coset, 23
Coset of a subspace, 183
Cross product, 143, 200
Cyclic group, 16
Cyclic subspaces, 320
D
Daily key, 376
Derivative of a polynomial, 53
Derived series , 363
Diagonalizable matrix, 252
Direct sum, 165
Domain of a mapping, 2
Dot product, 62
© Springer Science+Business Media LLC 2017
J.B. Carrell, Groups, Matrices, and Vector Spaces,
DOI 10.1007/978-0-387-79428-0
407

408
subject Index
Double coset, 336
Double dual, 236
Dual basis, 233
Dual space, 232
Dynamical system, 256
E
Eigenpair, 239
Eigenvalue, 239
Eigenvector, 239
Equivalence class, 4
Equivalence relation, 4
Equivalent linear systems, 77
Euler’s theorem, 32
Extended Hamming code, 191
External direct product, 355
External direct sum, 167
F
Fermat’s little theorem, 51
Fibonacci numbers, 256
Field, 36
Finite-dimensional vector space, 147
Fixed point, 340
Flag variety, 161, 394
Fourier expansion, 177
Free variables, 78
Frobenius map, 204
Fundamental theorem of algebra, 45
G
Galois ﬁeld, 47
Galois group, 368
General linear group, 93
General solution vector, 79
Golden ratio, 349
Greatest common divisor, 21, 32
Group, 12
Group action, 338
H
Hamming distance, 61, 189
Hermitian inner product, 173
Hermitian matrix, 128
Hermitian transpose, 173
Homomorphism, 19
I
Idempotent, 374
Imaginary numbers, 41
Imaginary part, 42
Inconsistent system, 79
Index of a subgroup, 26
Injective, 3
Inner automorphism, 20
Inner product, 169
Inner product space, 169
Internal direct product, 356
Invariant subspace, 320
Inverse image, 3
Irreducible polynomial, 294
Irreducible subspace, 400
Isomorphism, 19
Isomorphism of vector spaces, 207
J
Jordan block, 328
Jordan–Chevalley decomposition, 321
K
Kernel, 19
Kernel of a linear mapping, 205
Killing form, 170
Klein 4-group, 352
Kronecker delta, 64
L
Length of a Weyl group element, 392
Lie bracket, 326
Linear algebraic group, 382
Linear combination, 60, 137
Linear function, 198
Linear mapping, 20
Linear subspace, 141
Linearly independence, 145
Linearly reductive, 400
M
Mapping, 2
Matrix, 58
Matrix linear mapping, 200
Metric, 181
Minimal polynomial, 271
Modular group, 134
Multiplicative unit, 33
N
Negative deﬁnite matrix, 309

subject Index
409
Nilpotent matrix, 269
Nilpotent part, 321
Noncollinear vectors, 142
Nonsingular matrix, 75
Normal matrix, 299
Normal subgroup, 24
Null space, 78
O
Orbit, 335
Order of a group, 13
Order of an element, 26
Orthogonal complement, 177
Orthogonal group, 95
Orthogonal group over F, 95
Orthogonal mapping, 211
Orthogonal projection, 172
Outer automorphism, 20
P
Pairings, 377
Parabolic subgroup, 398
Partial permutation matrix, 101
Partition, 334
Perfect code, 192
Perfect ﬁeld, 54
Permutation, 14
Phi function, 32
Plaintext, 61, 372
Plane, 142
Platonic solid, 286
Polar decomposition, 315
Polar orbit, 350
Pole, 345
Polyhedral group, 345
Polynomial, 52
Positive deﬁnite matrix, 309
Prime ﬁeld, 47
Primitive element, 35
Projection, 213
Projective linear group, 365
Q
Quadratic form, 305
Quadratic variety, 308
Quotient, 5
Quotient group, 29
R
Radical of a group, 383
Rank of a matrix, 74
Real part, 42
Reductive group, 383
Reﬂection, 213, 215
Relation, 4
Relative maximum, 310
Relative minimum, 310
Ring, 40
Roots of unity, 44
Rotation of R3, 283
S
Scalar multiplication, 42
Schubert cell, 396
Self adjoint, 274
Semidirect product, 359
Semisimple group, 383
Semisimple linear mapping, 200
Semisimple part, 321, 322
Signature of a permutation, 119
Signature of a quadratic form, 313
Signed permutation matrix, 98
Similar matrices, 128, 228
Simple group, 361
Simple reﬂection, 391, 399
Simple root, 53
Simple transposition, 97
Skew-Hermitian matrix, 301
Skew-symmetric matrix, 128
Solution set, 77
Solvable group, 362
Span, 60
Spanning set, 142
Special linear group, 115
Splitting ﬁeld, 294
Stabilizer, 339
Standard basis, 147
String basis, 329
Subﬁeld, 38
Subgroup, 18
Subnormal series, 361
Sum of subspaces, 162
Surjective, 3
Sylow subgroup, 350
Symplectic group, 388
T
Target, 2
Telegram key, 376
Torus, 31
Trace of a matrix, 170

410
Index
Transitive action, 339
Transpose, 64
Triangle inequality, 189
U
Unipotent, 322
Unipotent matrix, 100
Unipotent part, 322
Unitary matrix, 128, 179
W
Weight of a codeword, 189
Weyl group, 390

