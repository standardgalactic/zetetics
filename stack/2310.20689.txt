Learning From Mistakes Makes LLM Better Reasoner
Shengnan An∗♢,♣, Zexiong Ma∗♡,♣, Zeqi Lin†♣,
Nanning Zheng†♢, Jian-Guang Lou♣, Weizhu Chen♣
♢IAIR, Xi’an Jiaotong University,
♣Microsoft Corporation, ♡Peking University
♢{an1006634493@stu, nnzheng@mail}.xjtu.edu.cn, ♡mazexiong@stu.pku.edu.cn,
♣{Zeqi.Lin, jlou, wzchen}@microsoft.com
ABSTRACT
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on
solving math problems. To further improve this capability, this work proposes LEarning
from MistAkes (LEMA), akin to human learning processes. Consider a human student who
failed to solve a math problem, he will learn from what mistake he has made and how to
correct it. Mimicking this error-driven learning process, LEMA fine-tunes LLMs on mistake-
correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning
paths from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the mistake
step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the
final answer. Experimental results demonstrate the effectiveness of LEMA: across five
backbone LLMs and two mathematical reasoning tasks, LEMA consistently improves the
performance compared with fine-tuning on CoT data alone. Impressively, LEMA can also
benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1
accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved
by non-execution open-source models on these challenging tasks. Our code, data and models
will be publicly available at Github Link.
LLM
Question: Tina makes $18.00 an hour. If she works more than 8 hours per shift, she is 
eligible for overtime, which is paid by your hourly wage + 1/2 your hourly wage. If she works 
10 hours every day for 5 days, how much money does she make?
Rationales with Mistakes:
Step 1: Tina makes $18.00 an hour for 8 hours, which is 8 * $18.00 = $144.00.
Step 2: She makes $27.00 an hour for the 2 hours of overtime, which is 2 * $27.00 = $54.00.
Step 3: So for 5 days, she makes $144.00 + $54.00 = $198.00.
Step 4: The answer is 198.
GPT-4 As Corrector
Correction:
Incorrect Step: Step 3.
Explanation: Step 3 only calculates the earnings for one day, but not for the entire five days.
Correct Solution:
...
Step 3: For one day, she makes $144.00 + $54.00 = $198.00.
Step 4: For 5 days, she makes $198.00 * 5 = $990.00.
Step 5: The answer is 990.
Sampling Rationales
Identifying Mistake and Making Correction
LLM
Fine-Tuning on Mistake-Correction Data
85.4
84.2
83.5
77.9
30
50
70
90
Performance on GSM8K (%)
WizardMath
70B
LLaMA-2
70B
LLaMA
65B
Vanilla LLM
LLM + LEMA
Performance on MATH (%)
26.9
27.1
25.0
20.8
0
10
20
30
MetaMath
70B
WizardMath
70B
LLaMA-2
70B
LLaMA
65B
MetaMath
70B
Figure 1: Left: The overall process of LEarning from MistAkes (LEMA). Right: The performance of LEMA
on GSM8K (upper right) and MATH (lower right).
∗Work done during the internship at Microsoft.
† Corresponding authors.
arXiv:2310.20689v1  [cs.CL]  31 Oct 2023

Preprint.
1
Introduction
Mistakes are the portals of discovery. —James Joyce
With exponential growth in data size and model scale, contemporary large language models (Brown et al.,
2020; Zhang et al., 2022; Hoffmann et al., 2022; Smith et al., 2022; OpenAI, 2023b; Anil et al., 2023) have
demonstrated significant advancements on various NLP tasks, particularly in mathematical problem solving
that necessitates complex chain-of-thought (CoT) reasoning (Wei et al., 2022; Wang et al., 2022; Li et al.,
2023b; Shi et al., 2023; Qin et al., 2023; Lightman et al., 2023). In terms of performance on challenging
mathematical tasks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), proprietary large
language models, including GPT-4 (OpenAI, 2023b) and PaLM-2 (Anil et al., 2023), have attained notable
results. However, open-source LLMs, such as LLaMA-2 (Touvron et al., 2023b) and Baichuan-2 (Yang et al.,
2023), still have much room for improvement.
To further improve the CoT reasoning capabilities of open-source LLMs for tackling mathematical tasks, a
common approach is to fine-tune these models using annotated/generated question-rationale data pairs (referred
to as CoT data), which directly teach the model how to perform CoT reasoning on these tasks (Magister et al.,
2022; Huang et al., 2022; Ho et al., 2022; Li et al., 2022; Yuan et al., 2023; Luo et al., 2023; Yu et al., 2023; Li
et al., 2023a; Liang et al., 2023). While this straightforward learning process has exhibited its effectiveness,
this study investigates whether the reasoning capability of LLMs can be further improved through a backward
learning process, i.e., learning from the mistakes that LLMs have made. The insight of learning from mistakes
comes from the learning process of human students. Consider a student who is just beginning to learn math.
Although he learns from golden knowledge and examples in books, he also does exercises. After failing to
solve a problem, he will learn what mistakes he made and how to correct them. By learning from the mistakes
he has made, his reasoning capability will be further improved. Inspired by this error-driven learning process,
this work explores whether the reasoning ability of LLMs can also benefit from understanding and correcting
mistakes.
Specifically, we first generate mistake-correction data pairs (referred to as correction data) and then fine-tune
LLMs using correction data. For generating correction data, we employ multiple LLMs, including the LLaMA
and GPT series models, to collect inaccurate reasoning paths (i.e., with incorrect final answers). We then
use GPT-4 as the "corrector" to generate corrections for these inaccurate reasoning paths. The generated
corrections contain three pieces of information: (1) the incorrect step in the original solution, (2) an explanation
of why this step is incorrect, and (3) how to correct the original solution to arrive at the correct final answer.
After filtering out corrections with incorrect final answers, our human evaluation reveals that our correction
data exhibits adequate quality for the subsequent fine-tuning stage. We perform LEarning from MistAkes
(LEMA) through fine-tuning LLMs on both CoT data and correction data with QLoRA (Dettmers et al., 2023).
Our experimental results with five open-source LLMs and on two challenging mathematical reasoning tasks
demonstrate the effectiveness of LEMA. Compared to fine-tuning on CoT data alone, LEMA consistently
improves performance across various LLMs and tasks. For instance, LEMA with LLaMA-2-70B (Touvron
et al., 2023b) achieves 83.5% on GSM8K and 25.0% on MATH, while fine-tuning on CoT data alone yields
81.4% and 23.6%, respectively. Moreover, LEMA is also compatible with specialized LLMs: LEMA with
WizardMath-70B (Luo et al., 2023)/MetaMath-70B (Yu et al., 2023) achieves 84.2%/85.4% pass@1 accuracy
on GSM8K and 27.1%/26.9% on MATH. This surpasses the state-of-the-art performance achieved by non-
execution3 open-source models on these challenging tasks. Furthermore, our ablation study shows that LEMA
still outperforms CoT-alone fine-tuning under the same data size. It indicates that the effectiveness of CoT
data and correction data are not homogeneous, as the combination of the two data sources can yield additional
improvements rather than using a single source. These experimental results and analyses underscore the
potential of learning from mistakes in enhancing the reasoning capabilities of LLMs.
2
Related Work
LLMs with CoT reasoning.
Wei et al. (2022) uncovered the emergence of CoT reasoning capabilities
within extremely large models. Such reasoning capabilities have been examined in various reasoning-related
domains, such as logical reasoning (Creswell et al., 2022; Pan et al., 2023; Lei et al., 2023), commonsense
reasoning (Talmor et al., 2019; Geva et al., 2021; Ahn et al., 2022), and mathematical reasoning (Miao et al.,
3We mainly compare LEMA with non-execution models which do not rely on the external executor to obtain the final
answer.
2

Preprint.
2020; Koncel-Kedziorski et al., 2016; Patel et al., 2021; Cobbe et al., 2021; Hendrycks et al., 2021). The
impressive performance of LLMs in these domains has spurred the research community to further investigate
methods for effectively harnessing and enhancing CoT reasoning for LLMs (Wang et al., 2022; Zhou et al.,
2022; Creswell & Shanahan, 2022; Li et al., 2023b; Lightman et al., 2023).
Enhancing CoT reasoning for solving mathematical problems.
There has been much work dedicated
to enhancing the performance of LLMs in solving mathematical problems from various perspectives. Some
studies explored the voting or verification methods based on sampling multiple reasoning paths (Wang et al.,
2022; Li et al., 2023b; Lightman et al., 2023). Some methods considered to generate executable programs
to obtain the final answer or to integrate plug-in tools that facilitate the execution of external APIs during
intermediate steps (Jie & Lu, 2023; Wang et al., 2023a; Gou et al., 2023). Some work concentrated on data
augmentation, which involves expanding the training set or providing external annotations (Magister et al.,
2022; Huang et al., 2022; Ho et al., 2022; Li et al., 2022; Yuan et al., 2023; Luo et al., 2023; Yu et al., 2023; Li
et al., 2023a; Liang et al., 2023). This work aligns with the data augmentation approach.
Data augmentation for mathematical tasks.
With the help of advanced LLMs (e.g., GPT-4 and GPT-3.5-
Turbo), various methods have been proposed to generate more CoT data for mathematical tasks: Yuan et al.
(2023) proposed rejection sampling for augmenting CoT data; Xu et al. (2023) evolved the math questions in
the training set; Li et al. (2023a) applied both query augmentation and response augmentation; Yu et al. (2023)
used self-verification and FOBAR to generate CoT with high diversity. While the effectiveness of CoT data
has been well studied, how to improve mathematical reasoning with other auxiliary data is still under-explored.
This work takes a preliminary step toward leveraging auxiliary data: we propose to utilize mistake-correction
data and experimentally examine its effectiveness. More recently, Liu et al. (2023) and Wang et al. (2023c)
construct re-ranking data or verification data as auxiliary data to make the model judge the quality of reasoning
paths. Compared with this kind of auxiliary data, our correction data is more informative: beyond judging
the quality of reasoning steps, our correction data teaches the model to explain its judgment and to revise the
original reasoning path according to the judgement.
3
Method
Figure 1 (left) presents an overview of LEMA, which consists of two primary stages: generating correction
data and fine-tuning LLMs.
3.1
Correction Data Generation
Given a question-answer example (qi, ai) ∈Q, a corrector model Mc and a reasoning model Mr, we generate
the mistake-correction data pair (qi ⊕eri, ci) ∈C, where eri represents an inaccurate reasoning path to the
question qi, and ci denotes the correction for eri.
Collecting Inaccurate Reasoning Paths.
We first sample multiple reasoning paths for each question qi
using the reasoning model Mr and retain only those paths that do not lead to the correct final answer ai,
eri ∼Mr(Pr ⊕qi),
Ans(eri) ̸= ai,
(1)
where Pr is the few-shot prompt instructing the model to perform CoT reasoning, and Ans(·) extracts the final
answer from the reasoning path.
Generating Corrections for Mistakes.
For question qi and the inaccurate reasoning path eri, we employ the
corrector model Mc to generate a correction and then check the final answer in the correction,
ci ∼Mc(Pc ⊕qi ⊕eri),
Ans(ci) = ai,
(2)
where Pc contains 4 annotated mistake-correction examples to guide the corrector model what kind of
information should be contained in the generated corrections. Specifically, the annotated corrections comprises
three pieces of information:
• Incorrect Step: which step in the original reasoning path has a mistake.
• Explanation: what kind of mistake has been made in this step.
• Correct Solution: how to correct the inaccurate reasoning path to better solve the original question.
3

Preprint.
Example 1 briefly illustrates the prompt used for generating corrections.
Example 1: Prompt For Generating Corrections
For the following math word problems, the original solutions may contain errors. Please identify the incorrect step in
each solution, explain why it is incorrect, and provide the correct solution starting from that step.
Question: James creates a media empire. He creates a movie for $2000. Each DVD cost $6 to make. He sells it for 2.5
times that much. He sells 500 movies a day for 5 days a week. How much profit does he make in 20 weeks?
Original Solution:
Step 1: 500 movies a day, 5 days a week, for 20 weeks, he sells 500 * 5 * 20 = 50000 movies.
Step 2: Each movie he sells for $6 * 2.5 = $15.
Step 3: Thus, he makes a profit of $15 - $6 = $9 per movie.
Step 4: Therefore, he makes a profit of $9 * 50000 = $450000.
Step 5: The answer is 450000.
Incorrect Step: Step 4.
Explanation: The error in Step 4 is that it does not take into account the initial cost of creating the movie ($2000). The
correct solution should subtract this amount from the total profit calculated in Step 4.
Correct Solution:
...
Step 4: Therefore, he makes a profit of $9 * 50000 - $2000 = $448000.
Step 5: The answer is 448000.
... (Another 3 examples)
Question: qi
Original Solution: eri
Incorrect Step: {Completion}
Human Evaluation for Generated Corrections.
Before generating data on a large scale, we first manually
assess the quality of the generated corrections. We take LLaMA-2-70B as Mr, utilize GPT-4 as Mc, and
generate 50 mistake-correction data pairs based on the GSM8K training set. We classify the corrections into
three quality levels:
• Excellent: the corrector successfully identify the incorrect step in eri, provide a reasonable explanation,
and the corrected reasoning path exhibits high continuity with the pre-steps (i.e., steps before the identified
mistake) in the original reasoning path (e.g., Example 2).
• Good: the corrector successfully identify the incorrect step in eri, provide a reasonable explanation, while
the corrected reasoning path has minor issues in continuity (e.g., Example 3).
• Poor: the corrector fails to identify the incorrect step in eri or provides reasonable explanations (e.g.,
Example 4).
Example 2: Generated Correction (Excellent)
Question: Courtney liked to collect marbles. She kept them in mason jars. One jar had 80 marbles. Her second jar had
twice that amount. She just started her third jar which currently has 1/4 the amount of her first jar. How many marbles
does she have in total?
Original Solution:
Step 1: 80 marbles in the first jar, 2 x 80 = 160 marbles in the second jar.
Step 2: The third jar has 1/4 of the first jar, which is 80 / 4 = 20 marbles.
Step 3: So, Courtney has 160 + 20 = 180 marbles in total.
Step 4: The answer is 180.
Incorrect Step: Step 3.
Explanation: The calculation in Step 3 is incorrect because it only adds the marbles from the second and third jars.
The marbles from the first jar should also be included in the total.
Correct Solution:
...
Step 3: So, Courtney has 80 (first jar) + 160 (second jar) + 20 (third jar) = 260 marbles in total.
Step 4: The answer is 260.
4

Preprint.
Example 3: Generated Correction (Good)
Question: Johnny is a dog walker. He can walk 3 dogs at once. He gets paid $15 for a 30-minute walk and $20 for a
60-minute walk. Johnny works for 4 hours per day. If he always walks the maximum number of dogs possible and 6
dogs have 60-minute walks per day, how much money does he make in a week where he works 5 days?
Original Solution:
Step 1: 4 hours per day * 5 days = 20 hours per week.
Step 2: Johnny can walk 3 dogs at once, so he can walk 20 / 3 = 6 three-dog walks per day.
...
Step 6: The answer is $360.
Incorrect Step: Step 2.
Explanation: The incorrect step is Step 2. The calculation used in this step is not correct for finding the number of
three-dog walks per day. Instead, we should calculate the number of 30-minute and 60-minute walks Johnny does.
Correct Solution:
...
Step 2: Johnny works for 4 hours per day, which is 240 minutes. He walks 6 dogs for 60 minutes each, which takes 120
minutes. This leaves 120 minutes for 30-minute walks.
...
Step 6: The answer is 1500.
(Note: The explanation is reasonable and the final answer is correct. But the new solution has issues on continuity: the
new Step 2 does not exactly follow the original Step 1.)
Example 4: Generated Correction (Poor)
Question: Tom, Tim, and Paul are collecting photos of cars. Paul has 10 photos more than Tim. Tim has one hundred
photos less than the total amount of photos which is 152. How many photos does Tom have?
Original Solution:
Step 1: 152 - 100 = 52.
Step 2: Tom has 52 photos.
Step 3: The answer is 52.
Incorrect Step: Step 1.
Explanation: The step is incorrect because it subtracts 100 from the total number of photos (152) to find the number of
photos Tim has. However, the problem states that Tim has 100 photos less than the total amount of photos, not the total
amount of photos minus 100.
Correct Solution:
...
Step 5: The answer is 38.
(Note: Step 1 is correct and the mistake step is Step 2, because Tim, not Tom, has 52 photos.)
Our evaluation finds that 35 out of 50 generated corrections are of excellent quality, 11 are good, and 4 are
poor. Based on this human evaluation, we suppose the overall quality of corrections generated with GPT-4 is
sufficient for the further fine-tuning stage. Therefore, we generate corrections on a large scale and take all
corrections that have correct final answers for fine-tuning LLMs. We provide further analysis on the choice
and behavior of corrector model in Section 4.3.
3.2
Fine-Tuning LLMs
After generating the correction data, we fine-tune the LLMs to evaluate whether these models can learn from
mistakes. To this end, we mainly compare the performance of two fine-tuning settings:
• Fine-Tuning on CoT Data: We fine-tune the model on question-rationale data alone. Despite the
annotated data in each task, we additionally take CoT data augmentation following existing methods (Yuan
et al., 2023; Li et al., 2023a; Yu et al., 2023): we generate more reasoning paths for each question in
the training set with GPT-4 and filter out paths with wrong final answers. We apply this CoT data
augmentation to set up a strong fine-tuning baseline that only utilizes CoT data and also facilitates our
further ablation study on controlling data size for fine-tuning (detailed in Section 4.2).
• Fine-Tuning on CoT Data + Correction Data: Besides the CoT data, we add our generated mistake-
correction data for fine-tuning (this setting is referred to as LEMA). We also experiment with the
controlled data size as an ablation study to mitigate the effect of increments on data size.
Appendix A shows the input-output format of CoT data and correction data used for fine-tuning.
5

Preprint.
Table 1: Our main experimental results (%) on GSM8K and MATH. We report the performance of the best 3
checkpoints saved during the fine-tuning process and the average of three results.
Model
Training
GSM8K
MATH
1st / 2nd / 3rd
Avg.
1st / 2nd / 3rd
Avg.
LLaMA-2-70B (Touvron et al., 2023b)
CoT Fine-Tuning
81.4 / 81.3 / 81.1
81.3
23.6 / 23.2 / 23.2
23.2
+ Learning From Mistakes
83.5 / 83.4 / 83.2
83.4 (+2.1)
25.0 / 25.0 / 24.6
24.9 (+1.7)
LLaMA-65B (Touvron et al., 2023a)
CoT Fine-Tuning
76.2 / 76.2 / 75.7
76.0
19.7 / 19.7 / 19.2
19.5
+ Learning From Mistakes
77.9 / 77.3 / 77.2
77.5 (+1.5)
20.8 / 20.3 / 20.2
20.4 (+0.9)
CodeLLaMA-34B (Rozière et al., 2023)
CoT Fine-Tuning
68.8 / 68.5 / 68.2
68.5
19.1 / 19.0 / 18.9
19.0
+ Learning From Mistakes
71.7 / 71.0 / 70.9
71.2 (+2.7)
20.4 / 20.2 / 20.0
20.2 (+1.2)
LLaMA-2-13B (Touvron et al., 2023b)
CoT Fine-Tuning
62.9 / 62.7 / 62.7
62.8
12.2 / 11.9 / 11.8
12.0
+ Learning From Mistakes
65.7 / 65.2 / 65.0
65.3 (+2.5)
12.6 / 12.6 / 12.4
12.5 (+0.5)
LLaMA-2-7B (Touvron et al., 2023b)
CoT Fine-Tuning
52.6 / 52.5 / 52.5
52.5
8.7 / 8.5 / 8.5
8.6
+ Learning From Mistakes
54.1 / 53.7 / 53.6
53.8 (+1.3)
9.4 / 8.9 / 8.8
9.0 (+0.4)
250
500
750
1000
1250
1500
1750
2000
Training Steps
74
76
78
80
82
Pass@1 Accuracy (%)
CoT Data
CoT Data + Correction Data
(a) Performance curve on GSM8K.
250
500
750
1000
1250
1500
1750
2000
Training Steps
18
19
20
21
22
23
24
25
Pass@1 Accuracy (%)
CoT Data
CoT Data + Correction Data
(b) Performance curve on MATH.
Figure 2: The performance curves of LLaMA-2-70B during 2,000 fine-tuning steps.
4
Experiments and Analysis
4.1
Experimental Setup
Tasks and CoT Data.
We take experiments on two challenging mathematical tasks GSM8K (Cobbe et al.,
2021) and MATH (Hendrycks et al., 2021). GSM8K contains high quality linguistically diverse grade school
math word problems with 7,473 examples in the training set and 1,319 test cases. Our CoT data for GSM8K
contains all training set examples along with another 24,948 augmented reasoning paths. Specifically, we
first generate 30,000 reasoning paths with GPT-4 and then filter out 5,052 paths with wrong final answers or
unexpected format4. In total, there are 32,421 examples for CoT fine-tuning on GSM8K. MATH contains
challenging competition mathematics problems with 7,500 training examples and 5,000 test cases. Our CoT
data for MATH contains all training set examples along with another 12,509 augmented reasoning paths.
Specifically, we also sample 30,000 reasoning paths with GPT-4 and then filter out 17,491 paths. In total, there
are 20,009 examples for CoT fine-tuning on MATH.
Correction Data Generation.
We utilize multiple LLMs to collect inaccurate reasoning paths, including
LLaMA-2-70B (Touvron et al., 2023b), WizardLM-70B (Xu et al., 2023), WizardMath-70B (Luo et al., 2023),
Text-Davinci-003 (OpenAI, 2023c), GPT-3.5-Turbo (OpenAI, 2023a) and GPT-4 (OpenAI, 2023b). We take
GPT-4 to generate corrections. Finally, we collect 12,523 and 6,306 mistake-correction data pairs based on the
training set of GSM8K and MATH, respectively. Along with CoT data, there are totally 44,944 and 26,315
examples for LEMA on GSM8K and MATH, respectively.
4The unexpected format means that the final answer is failed to be extracted from the reasoning path with the regular
expression.
6

Preprint.
81.4
82.3
83.5
75
80
85
Performance on GSM8K (%)
LLaMA-2-70B
CoT Fine-Tuning
76.2
77.3
77.9
70
75
80
LLaMA-65B
62.9
64.1
65.7
60
65
70
LLaMA-2-13B
52.6
52.7
54.1
50
55
60
LLaMA-2-7B
LEMA (Controlled Data Size)
LEMA (Full Data Size)
68.8
70.8
71.7
65
70
75
CodeLLaMA-34B
Figure 3: Performance of LEMA with controlled data size on GSM8K. The data size for both CoT fine-tuning
and LEMA with controlled data size is 32,421 and the full data size for LEMA is 44,944.
Fine-Tuning and Evaluation.
We fine-tune multiple open-source LLMs in the LLaMA (Touvron et al.,
2023a), LLaMA-2 (Touvron et al., 2023b), CodeLLaMA (Rozière et al., 2023), WizardMath (Luo et al., 2023)
and MetaMath (Yu et al., 2023) families. We utilize QLoRA5 (Hu et al., 2022; Dettmers et al., 2023) to
fine-tune these models with 64 low-rank dimension and 0.05 dropout rate. We set learning rate as 0.0001 for
LLMs larger than (or equal to) 34B and 0.0002 for LLMs smaller than 34B. For all LLMs and tasks, we set
batch size as 96, train for 2,000 steps, and save checkpoints for every 100 training steps. For evaluation, we
evaluate the performance of all saved checkpoints based on vLLM library6 (Kwon et al., 2023). We report
the pass@1 accuracy of the best 3 checkpoints to clarify influence from random disturbances. We do not add
demonstration examples into the prompt for both fine-tuning and evaluation by default. Our experiments are
conducted on 4 x A100 GPU stations.
4.2
Results and Analysis
Correction data consistently improves the performance of various LLMs.
Table 1 show the main
experimental results on two mathematical reasoning tasks. Compared to fine-tuning on CoT data alone,
incorporating correction data during fine-tuning brings improvements across all five backbone LLMs and two
tasks. Moreover, we notice that the performance of all three checkpoints with correction data can consistently
outperforms the best performance with CoT data alone. Additionally, Figure 2 illustrates the performance
curves during the whole training process. It shows that our correction data leads to clear improvements during
training. These consistent improvements demonstrate that the effectiveness of our correction data is robust to
the random disturbances during training.
The effectiveness of CoT data and correction data are not homogeneous.
If the effectiveness of two kinds
of data are homogeneous, the improvements in Table 1 will be diminished if the data size of two fine-tuning
settings are controlled as the same. To further validate the effectiveness of correction data, we reduce the
data size of LEMA on GSM8K to be the same as that of the CoT fine-tuning. Specifically, we randomly
remove 12,523 examples from the augmented reasoning paths in our CoT data, leaving 32,421 examples for
LEMA (including 19,898 CoT data and 12,523 correction data). Figure 3 shows that after controlling the data
size, LEMA can still bring considerable gains for LLaMA-2-70B, LLaMA-65B and LLaMA-2-13B. It means
that these LLMs do obtain extra information from our correction data that is not available in the CoT data.
For LLaMA-2-7B, the performance of LEMA under controlled data size is close to that of CoT-data-alone
fine-tuning. It suggests that larger LLMs are better at learning from mistakes than smaller models. This is
further supported by our experimental results on specialized LLMs.
LEMA can further improve the performance of specialized LLMs.
To adapt generally pre-trained LLMs
into the math domain, there have been several specialized LLMs such as WizardMath (Luo et al., 2023) and
MetaMath (Yu et al., 2023). We apply LEMA on these specialized LLMs and report the performance in
Table 2. It shows that LEMA can also bring considerable gains for these specialized LLMs. Impressively,
LEMA with 70B specialized LLMs demonstrate state-of-the-art performance: LEMA achieves 85.4% pass@1
accuracy on GSM8K with MetaMath-70B, and achieves 27.1% on MATH with WizardMath-70B. Note that
these models are mainly trained on a large scale of CoT data. For instance, the training set of MetaMath
5https://github.com/artidoro/qlora.
6https://github.com/vllm-project/vllm.
7

Preprint.
Table 2: The pass@1 accuracy (%) of various LLMs on GSM8K and MATH.
Model
GSM8K
MATH
closed-source models
GPT-4 (OpenAI, 2023b)
92.0
42.5
Claude-2 (Anthropic, 2023)
88.0
-
Flan-PaLM-2 (Anil et al., 2023)
84.7
33.2
GPT-3.5-Turbo (OpenAI, 2023a)
80.8
34.1
PaLM-2 (Anil et al., 2023)
80.7
34.3
open-source models
LLaMA-2-7B (Touvron et al., 2023b)
14.6
2.5
Baichuan-2-7B (Yang et al., 2023)
24.5
5.6
SQ-VAE-7B (Wang et al., 2023b)
40.0
7.0
RFT-7B (Yuan et al., 2023)
50.3
-
Qwen-7B (Alibaba, 2023)
51.6
-
LLaMA-2-7B + LEMA (ours)
54.1
9.4
WizardMath-7B (Luo et al., 2023)
54.9
10.7
WizardMath-7B + LEMA (ours)
55.9
11.9
LLaMA-2-13B (Touvron et al., 2023b)
28.7
3.9
SQ-VAE-13B (Wang et al., 2023b)
50.6
8.5
Baichuan-2-13B (Yang et al., 2023)
52.8
10.1
RFT-13B (Yuan et al., 2023)
54.8
-
WizardMath-13B (Luo et al., 2023)
63.9
14.0
LLaMA-2-13B + LEMA (ours)
65.7
12.6
MetaMath-13B (Yu et al., 2023)
72.3
22.4
MetaMath-13B + LEMA (ours)
73.2
22.7
LLaMA-2-70B (Touvron et al., 2023b)
56.8
13.5
RFT-70B (Yuan et al., 2023)
64.8
-
WizardMath-70B (Luo et al., 2023)
81.6
22.7
MuggleMath-70B (Li et al., 2023a)
82.3
-
MetaMath-70B (Yu et al., 2023)
82.3
26.6
LLaMA-2-70B + LEMA (ours)
83.5
25.0
WizardMath-70B + LEMA (ours)
84.2
27.1
MetaMath-70B + LEMA (ours)
85.4
26.9
contains 240K question-rationale examples designed for GSM8K. The improvements with specialized LLMs
further demonstrate that learning from mistakes makes LLM better reasoner.
Larger LLMs can better benefit from LEMA.
For applying LEMA on specialized LLMs, we find that
larger specialized obtain more gains than smaller ones. As shown in Table 2, MetaMath-70B improves 3.1%
pass@1 accuracy score on GSM8K while MetaMath-13B only has 0.9% accuracy improvement; WizardMath-
70B improves 4.4% on MATH while WizardMath-7B only improves 1.2%. Along with the performance of
LLaMA-2-7B with controlled data size for LEMA (Figure 3), these comparisons suggest that larger LLMs are
better at learning from mistakes.
4.3
Further Analysis on Corrector
In our default setting, we take GPT-4 as the corrector model and our human evaluation in Section 3.1 supports
this choice. In the following, we provide further analysis on the choice and behavior of the corrector model.
Specifically, we explore the following three research questions: RQ1: Can we use a less powerful model as
the corrector model? RQ2: How well does GPT-4 perform in self-correction? RQ3: How well does GPT-4
correct inaccurate reasoning paths for challenging questions?
Less powerful models are not suitable for generating corrections.
Despite GPT-4, we have also tried
leveraging GPT-3.5-Turbo as the corrector model and assess the quality of generated corrections. We take
another round of human evaluation on 20 corrections generated by GPT-3.5-Turbo and find that nearly half are
8

Preprint.
0
2,000
4,000
6,000
8,000
Number of Examples
Level 1
Inaccurate Reasoning Paths
Corrections with Correct Final Answers
0
10
20
30
40
50
Success Rate (%)
Level 2
Level 3
Level 4
Level 5
Level 1
Level 2
Level 3
Level 4
Level 5
Success Rate
Figure 4: Statistics of generated correction data according to different difficulty levels in MATH. Left: The
number of collected inaccurate reasoning paths and generated corrections with correct final answers under
different difficulty levels. Right: The success rate for correcting inaccurate reasoning paths under different
difficulty levels.
of poor quality. Therefore, we just calling GPT-4 for correction generation although it is much more expensive
than GPT-3.5-Turbo. We believe it is a valuable research direction to explore how to generate high-quality
corrections without GPT-4.
GPT-4 can correct its own mistakes but with a low success rate.
Specifically, for 2,696 inaccurate
reasoning paths generated by GPT-4 on MATH training set, we finally get 217 corrections with correct final
answers. It means that GPT-4 only achieves 8.0% success rate for self-correction. Compared with this low
success rate for self-correction, GPT-4 can more effectively correct mistakes from less powerful models, such
as LLaMA-2-70B (37.5% success rate on MATH) and GPT-3.5-Turbo (26.9% success rate on MATH). One
possible reason for the low success rate of self-correction is that the mistakes generated by GPT-4 are from
more challenging questions, thus these mistakes are naturally harder for correcting.
GPT-4 still struggles to correct inaccurate reasoning paths for challenging questions.
The math problems
in MATH can be categorized into five levels of difficulty: Level 1 for the easiest problems and Level 5 for
the most challenging ones. Figure 4 shows statistics of our correction data on MATH according to different
difficulty levels. As the difficulty increased from Level 1 to Level 5, the number of collected inaccurate
reasoning paths increased, while the number of correct corrections (i.e., corrections for which the final answer
is correct) first increases and then decreases. We also calculate the success rate for correcting mistakes under
each difficulty level, dividing the number of correct corrections by the total number of collected reasoning
paths. Figure 4 shows that the success rate significantly drops with increasing the difficulty. These statistics
reveals that there is still huge room for improving contemporary LLMs on correcting mistakes.
5
Discussion
In this section, we delve deeper into the insights of learning from mistakes. Recent advancements in LLMs
have enabled them to perform a step-by-step approach in problem-solving. However, this multi-step generation
process does not inherently imply that LLMs possess strong reasoning capabilities, as they may merely emulate
the superficial behavior of human reasoning without genuinely comprehending the underlying logic and rules
necessary for precise reasoning. This incomprehension results in mistakes during the reasoning process and
necessitates the assistance of a "world model" that possesses a consciousness prior about the logic and rules
governing the real world. From this perspective, our LEMA framework employs GPT-4 as a "world model"
to teach smaller models in adhering to these logic and rules, rather than merely mimicking the step-by-step
behavior.
6
Conclusion
This work investigates whether LLMs can learn from mistakes. By generating correction data and fine-tuning
LLMs, our experimental results demonstrate that learning from mistakes can improve the performance of both
generally pre-trained and specialized LLMs on mathematical reasoning tasks.
9

Preprint.
Acknowledgments
Shengnan An and Nanning Zheng were supported in part by NSFC under grant No. 62088102. Thank Chen Li
for his valuable comments on this work.
References
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,
Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, Julian
Ibarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil J
Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, Linda
Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes,
Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao,
Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language in
robotic affordances, 2022.
Alibaba. Alibaba open sources qwen, a 7b parameter ai model, 2023. URL https://www.maginative.
com/article/alibaba-open-sources-qwen-a-7b-parameter-ai-model/.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,
Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,
Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan
Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele
Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément
Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer,
Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann,
Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey
Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,
Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li,
YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,
Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni,
Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao,
Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel,
Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter,
Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting,
Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng,
Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.
Anthropic. Model card and evaluations for claude models, 2023. URL https://www-files.anthropic.
com/production/images/Model-Card-Claude-2.pdf.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word
problems. arXiv preprint arXiv:2110.14168, 2021.
Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv preprint
arXiv:2208.14271, 2022.
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models
for interpretable logical reasoning. In The Eleventh International Conference on Learning Representations,
2022.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized
llms. arXiv preprint arXiv:2305.14314, 2023.
10

Preprint.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did Aristotle Use
a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the
Association for Computational Linguistics, 9:346–361, 04 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_
00370. URL https://doi.org/10.1162/tacl_a_00370.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu
Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving, 2023.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob
Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint
arXiv:2212.10071, 2022.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models. arXiv preprint arXiv:2203.15556, 2022.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu
Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning
Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large
language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.
Zhanming Jie and Wei Lu. Leveraging training data in few-shot prompting for numerical reasoning. arXiv
preprint arXiv:2305.18170, 2023.
Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math
word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 1152–1157, San Diego,
California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL
https://aclanthology.org/N16-1136.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gon-
zalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with
pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles,
2023.
Bin Lei, Chunhua Liao, Caiwen Ding, et al. Boosting logical reasoning in large language models through a
new framework: The graph of thought. arXiv preprint arXiv:2308.08614, 2023.
Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang
Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. arXiv
preprint arXiv:2310.05506, 2023a.
Shiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian, Baolin
Peng, Yi Mao, et al. Explanations from large language models make small reasoners better. arXiv preprint
arXiv:2210.06726, 2022.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making
language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315–5333, 2023b.
Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang Zhang, and Ashwin Kaylan. Let gpt
be a math tutor: Teaching math word problem solvers with customized exercise generation. arXiv preprint
arXiv:2305.14386, 2023.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John
Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step, 2023.
Yixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, and Peter J. Liu. Improving large language
model fine-tuning for solving math problems, 2023.
11

Preprint.
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin,
Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language
models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583, 2023.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching
small language models to reason. arXiv preprint arXiv:2212.08410, 2022.
Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.
A diverse corpus for evaluating and developing
English math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 975–984, Online, July 2020. Association for Computational Linguistics.
doi: 10.18653/v1/2020.acl-main.92. URL https://aclanthology.org/2020.acl-main.92.
OpenAI.
Gpt-3.5 turbo fine-tuning and api updates, 2023a.
URL https://openai.com/blog/
gpt-3-5-turbo-fine-tuning-and-api-updates.
OpenAI. Gpt-4 technical report, 2023b.
OpenAI. Openai documentation: Models, 2023c. URL https://platform.openai.com/docs/models/
gpt-3-5.
Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang. Logic-lm: Empowering large language
models with symbolic solvers for faithful logical reasoning. arXiv preprint arXiv:2305.12295, 2023.
Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math
word problems?
In Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, pp. 2080–2094, Online,
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL
https://aclanthology.org/2021.naacl-main.168.
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a
general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt,
Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar,
Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open
foundation models for code, 2023.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung,
Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual
chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, 2023.
URL https://openreview.net/forum?id=fR3wGCk-IXp.
Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,
Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to
train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,
2022.
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question an-
swering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers), pp. 4149–4158, Minneapolis, Minnesota, June 2019. Association for
Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-
lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned
chat models. arXiv preprint arXiv:2307.09288, 2023b.
12

Preprint.
Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie
Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical
reasoning, 2023a.
Xinyi Wang, Lucas Caccia, Oleksiy Ostapenko, Xingdi Yuan, and Alessandro Sordoni. Guiding language
model reasoning with planning tokens. arXiv preprint arXiv:2310.05707, 2023b.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The
Eleventh International Conference on Learning Representations, 2022.
Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang,
Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. Democratizing reasoning ability: Tailored learning from
large language model, 2023c.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information
Processing Systems, 35:24824–24837, 2022.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
Wizardlm: Empowering large language models to follow complex instructions, 2023.
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong
Yan, Fan Yang, et al. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305,
2023.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li,
Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language
models. arXiv preprint arXiv:2309.12284, 2023.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship
on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprint arXiv:2205.01068, 2022.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire
Cui, Olivier Bousquet, Quoc V Le, et al. Least-to-most prompting enables complex reasoning in large
language models. In The Eleventh International Conference on Learning Representations, 2022.
13

Preprint.
This is the Appendix of the paper: Learning From Mistakes Makes LLM Better Reasoner.
A
Input-Output Formats for Fine-Tuning
Example 5 and 6 illustrate the input-output formats of CoT data and correction data, respectively. Note that
during the fine-tuning process, the input part serves as a prompt and only the loss in the output part participates
in the back-propagation.
Example 5: Input-Output Format of CoT Data
Input:
Please solve the following math problem.
Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips
did Natalia sell altogether in April and May?
Answer: Let’s think step by step.
Output:
Natalia sold 48 / 2 = 24 clips in May. Natalia sold 48 + 24 = 72 clips altogether in April and May. The answer is 72.
Example 6: Input-Output Format of Correction Data
Input:
For the following math problem, the original solution is incorrect. Please identify the incorrect step, explain why it is
incorrect, and correct the original solution starting from the incorrect step.
Question: Courtney liked to collect marbles. She kept them in mason jars. One jar had 80 marbles. Her second jar had
twice that amount. She just started her third jar which currently has 1/4 the amount of her first jar. How many marbles
does she have in total?
Original Solution:
Step 1: 80 marbles in the first jar, 2 x 80 = 160 marbles in the second jar.
Step 2: The third jar has 1/4 of the first jar, which is 80 / 4 = 20 marbles.
Step 3: So, Courtney has 160 + 20 = 180 marbles in total.
Step 4: The answer is 180.
Incorrect Step:
Output:
Step 3.
Explanation: The calculation in Step 3 is incorrect because it only adds the marbles from the second and third jars. The
marbles from the first jar should also be included in the total.
Correct Solution:
{
...
Step 3: So, Courtney has 80 (first jar) + 160 (second jar) + 20 (third jar) = 260 marbles in total.
Step 4: The answer is 260.
}
Note: The ellipsis here are tokens that were actually used in the fine-tuning process, as we suppose that simply copying
pre-steps is not much informative for learning.
14

