Models for
Source
Separation
and Signal
Unmixing
MULTIVARIATE
BAYESIAN
STATISTICS
© 2003 by Chapman & Hall/CRC

CHAPMAN & HALL/CRC
A CRC Press Company
Boca Raton   London   New York   Washington, D.C.
Models for
Source
Separation
and Signal
Unmixing
MULTIVARIATE
BAYESIAN
STATISTICS
Daniel B. Rowe
© 2003 by Chapman & Hall/CRC

This book contains information obtained from authentic and highly regarded sources. Reprinted material
is quoted with permission, and sources are indicated. A wide variety of references are listed. Reasonable
efforts have been made to publish reliable data and information, but the author and the publisher cannot
assume responsibility for the validity of all materials or for the consequences of their use.
Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, microﬁlming, and recording, or by any information storage or
retrieval system, without prior permission in writing from the publisher.
The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for
creating new works, or for resale. Speciﬁc permission must be obtained in writing from CRC Press LLC
for such copying.
Direct all inquiries to CRC Press LLC, 2000 N.W. Corporate Blvd., Boca Raton, Florida 33431. 
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are
used only for identiﬁcation and explanation, without intent to infringe.
Visit the CRC Press Web site at www.crcpress.com
© 2003 by Chapman & Hall/CRC 
No claim to original U.S. Government works
International Standard Book Number 1-58488-318-9
Library of Congress Card Number 2002031598
Printed in the United States of America  1  2  3  4  5  6  7  8  9  0
Printed on acid-free paper
Library of Congress Cataloging-in-Publication Data
Rowe, Daniel B.
Multivariate Bayesian statistics : models for source separation and signal unmixing / 
    Daniel B. Rowe.
p. cm.
Includes bibliographical references and index.
ISBN 1-58488-318-9 (alk. paper)
1. Bayesian statistical decision theory. 2. Multivariate analysis. I. Title.
    QR749.H64G78 2002
    519.5¢42—dc21
2002031598
 
 

To Gretchen and Isabel
© 2003 by Chapman & Hall/CRC

Preface
This text addresses the Source Separation problem from a Bayesian sta-
tistical approach.
There are two possible approaches to solve the Source
Separation problem. The ﬁrst is to impose constraints on the model and like-
lihood such as independence of sources, while the second is to incorporate
available knowledge regarding the model parameters. One of these two ap-
proaches has to be followed because the Source Separation model is what is
called an overparameterized model. That is, there are more parameters than
can be uniquely estimated. In this text, the second approach which does not
impose potentially unreasonable model and likelihood constraints is followed.
The Bayesian statistical approach not only allows the sources and mixing
coeﬃcients to be estimated, but also inferences to be drawn on them.
There are many problems from diverse disciplines such as acoustics, EEG,
FMRI, genetics, MEG, portfolio allocation, radar, and surveillance, just to
name a few which can be cast into the Source Separation problem.
Any
problem where a signal is believed to be made up of a combination of elemen-
tary signals is a Source Separation problem. The real-world source separation
problem is more diﬃcult than it appears at ﬁrst glance.
The plan of the book is as follows. First, an introductory chapter describes
the Source Separation problem by motivating it with a description of a cocktail
party. In the description of the Source Separation problem, it is assumed that
the mixing process is instantaneous and constant over time.
Second, statistical material that is needed for the Bayesian Source Sepa-
ration model is introduced. This material includes statistical distributions,
introductory Bayesian Statistics, speciﬁcation of prior distributions, hyperpa-
rameter assessment, Bayesian estimation methods, and Multivariate Regres-
sion.
Third, the Bayesian Regression and Bayesian Factor Analysis models are
introduced to lead us to the Bayesian Source Separation model and then to the
Bayesian Source Separation model with unobservable and observable sources.
In all models except for the Bayesian Factor Analysis model which still
retains a priori uncorrelated factors from its Psychometric origins, the un-
observed source components are allowed to be correlated or dependent in-
stead of constrained to be independent. Models and likelihoods are described
with these speciﬁcations and then Bayesian statistical solutions are detailed
in which available prior knowledge regarding the parameters is quantiﬁed and
incorporated into the inferences.
Fourth, in the aforementioned models, it is speciﬁed that the observed
© 2003 by Chapman & Hall/CRC

mixed vectors and also the unobserved source vectors are independent over
time (but correlated within each vector). Models and likelihoods in which
the mixing process is allowed to be delayed and change over time are intro-
duced. In addition, the observed mixed vectors along with the unobserved
source vectors are allowed to be correlated over time (also correlated within
each vector). Available prior knowledge regarding the parameters is quantiﬁed
and incorporated into the inferences and then Bayesian statistical solutions
are described.
When quantifying available prior knowledge, both Conjugate and general-
ized Conjugate prior distributions are used. There may exist instances when
the covariance structure for the Conjugate prior distributions may not be
rich enough to quantify the prior information and thus generalized Conjugate
distributions should be used.
Formulas or algorithms for both marginal mean and joint modal or maxi-
mum a posteriori estimates are derived. In the Regression model, large sample
approximations are made in the derivation of the marginal distributions, and
hence, the marginal estimates when generalized Conjugate prior distributions
are speciﬁed. In this instance, a Gibbs sampling algorithm is also derived to
compute exact sampling based marginal mean estimates.
More formally, the outline of the book is as follows.
Chapter 1 introduces the Source Separation model by motivating it with the
“cocktail party” problem. The cocktail party is an easily understood example
of a Source Separation problem.
Part I is a succinct but necessary coverage of fundamental statistical knowl-
edge and skills for the Bayesian Source Separation model.
Chapter 2 contains needed background information on statistical distrib-
utions. Distributions are used in Statistics to model random variation and
uncertainty so that it can be understood and minimized. Several common
distributions are described which are also used in this text.
Chapter 3 gives a brief introduction to Bayesian Statistics. Bayesian Sta-
tistics is an approach in which inferences are made not only from information
contained in a set of data but also with available prior knowledge either from a
previous similar data set or from an expert in the form of a prior distribution.
Chapter 4 highlights the selection of diﬀerent common types of prior dis-
tributions used in Bayesian Statistics.
Knowledge regarding values of the
parameters from our available prior information is quantiﬁed through prior
distributions.
Chapter 5 elaborates on the assessment of hyperpameters of the prior dis-
tributions used in Bayesian Statistics to quantify our available knowledge.
Upon assessing the hyperparameters of the prior distribution, the entire prior
distribution is completely determined.
Chapter 6 describes two estimation methods commonly used for Bayesian
Statistics and in this text, namely, Gibbs sampling for marginal posterior
mean estimates and the iterated conditional modes algorithm for joint max-
imum a posteriori estimates.
After quantifying available knowledge about
© 2003 by Chapman & Hall/CRC

parameter values in the form of prior distributions, this knowledge is com-
bined with the information contained in a set of data through its likelihood.
A joint posterior distribution is obtained with the use of Bayes’ rule. This
joint distribution is evaluated to determine estimates of the model parameters.
Chapter 7 builds up from the Scalar Normal model to the Multivariate
(Non-Bayesian) Regression model.
The buildup includes Simple and Mul-
tiple Regression. The Regression model is preliminary knowledge which is
necessary to successfully understand the material of the text.
Part II considers the instantaneous constant mixing model where both the
observed vectors and unobserved sources are independent over time but al-
lowed to be dependent within each vector. The source components are corre-
lated or dependent.
Chapter 8 considers the sources to be known or observable for a description
of (Multivariate) Bayesian Regression. The Bayesian Regression model will
assist us in the progression toward the Bayesian Source Separation model.
Chapter 9 considers the sources to be unknown or unobservable and details
the Bayesian Factor Analysis model while pointing out its model diﬀerences
with Bayesian Regression.
Chapter 10 details the speciﬁcs of the Bayesian Source Separation model
and highlights its subtle but important diﬀerences from Bayesian Regression
and Factor Analysis.
Chapter 11 discusses the case when some sources are observed while others
are unobserved. This is a model which is a combination of Bayesian Regression
with observable sources and Bayesian Source Separation with unobservable
sources. Both the Bayesian Regression and Bayesian Source Separation mod-
els can be found by setting either the number of observable or unobservable
sources to be zero.
Chapter 12 consists of a case study example applying Bayesian Source Sep-
aration to functional magnetic resonance imaging (FMRI).
Part III details more general models in which sources are allowed to be
delayed and mixing coeﬃcients to change over time. This corresponds to the
speakers at the party being a physical distance from the microphones, thus
their conversation is not mixed instantaneously, and to speakers at a party
moving around the room, thus their mixing coeﬃcient increases and decreases
as they move closer or further away from the microphones. Also, observation
vectors as well as source vectors are allowed to be correlated over time. If a
person were talking (not talking) at a given time increment, then in the next
time increment this person is most likely talking (not talking).
Chapter 13 generalizes the model to delayed sources and dynamic mixing
as well as Regression coeﬃcients. Occasionally the speakers are a physical
distance from the microphones and thus their conversations do not instanta-
neously enter into the mixing process. Although this Chapter is presented
prior to the Chapter 14 on correlated vectors which is due to mathematical
coherence, a reader may wish to read Chapter 14 before this one.
© 2003 by Chapman & Hall/CRC

Chapter 14 expands the model to allow the observed mixed conversation
vectors in addition to observed and unobserved source vectors to be correlated
over time. There may be instances where the observation vectors and also the
source vectors are not independent over time.
Chapter 15 brings the text to an end with some concluding remarks on the
material of the text.
Appendix A presents methods for determining activation in FMRI.
Appendix B outlines methods for assessing hyperparameters in the Bayesian
Source Separation FMRI case study.
This text covers the basics of the necessary Multivariate Statistics and linear
algebra before delving into the substantive material. For each model, I give
two distinct ways to estimate the parameters.
It is my hope that I have provided enough information for the reader to
learn the fundamental statistical material. It is assumed that the reader has a
good knowledge of linear algebra, multivariable calculus, and calculus-based
Statistics. Those with suﬃcient breath and depth in the fundamental material
may skip directly to Chapter 8 and use the fundamental chapters as reference
material.
It is my belief that the most important topic in statistics is statistical distri-
bution theory. Everything can be derived from statistical distribution theory.
This text has many diﬀerent uses for many diﬀerent audiences. A short course
on Classical Multivariate Statistics can be put together by considering Chap-
ter 2 and Chapter 7. A larger course on Multivariate Bayesian Statistics can
be assembled by considering Part I and Chapter 8 of Part II. A one year long
course on Multivariate Bayesian Statistics can be made by considering the
whole text.
Daniel B. Rowe
Biophysics Research Institute
Medical College of Wisconsin
Milwaukee, Wisconsin
© 2003 by Chapman & Hall/CRC

Contents
List of Figures
List of Tables
1
Introduction
1.1
The Cocktail Party
1.2
The Source Separation Model
I 
Fundamentals 
2
Statistical Distributions
2.1
Scalar Distributions
2.1.1
Binomial
2.1.2
Beta
2.1.3
Normal
2.1.4
Gamma and Scalar Wishart
2.1.5
Inverted Gamma and Scalar Inverted Wishart
2.1.6
Student t
2.1.7
F-Distribution
2.2
Vector Distributions
2.2.1
Multivariate Normal
2.2.2
Multivariate Student t
2.3
Matrix Distributions
2.3.1
Matrix Normal
2.3.2
Wishart
2.3.3
Inverted Wishart
2.3.4
Matrix T
3
Introductory Bayesian Statistics
3.1
Discrete Scalar Variables
3.1.1
Bayes’ Rule and Two Simple Events
3.1.2
Bayes’ Rule and the Law of Total Probability
3.2
Continuous Scalar Variables
3.3
Continuous Vector Variables
3.4
Continuous Matrix Variables
© 2003 by Chapman & Hall/CRC

4
Prior Distributions
4.1
Vague Priors
4.1.1
Scalar Variates
4.1.2
Vector Variates
4.1.3
Matrix Variates
4.2
Conjugate Priors
4.2.1
Scalar Variates
4.2.2
Vector Variates
4.2.3
Matrix Variates
4.3
Generalized Priors
4.3.1
Scalar Variates
4.3.2
Vector Variates
4.3.3
Matrix Variates
4.4
Correlation Priors
4.4.1
Intraclass
4.4.2
Markov
5
Hyperparameter Assessment
5.1
Introduction
5.2
Binomial Likelihood
5.2.1
Scalar Beta
5.3
Scalar Normal Likelihood
5.3.1
Scalar Normal
5.3.2
Inverted Gamma or Scalar Inverted Wishart
5.4
Multivariate Normal Likelihood
5.4.1
Multivariate Normal
5.4.2
Inverted Wishart
5.5
Matrix Normal Likelihood
5.5.1
Matrix Normal
5.5.2
Inverted Wishart
6
Bayesian Estimation Methods
6.1
Marginal Posterior Mean
6.1.1
Matrix Integration
6.1.2
Gibbs Sampling
6.1.3
Gibbs Sampling Convergence
6.1.4
Normal Variate Generation
6.1.5
Wishart and Inverted Wishart Variate Generation
6.1.6
Factorization
6.1.7
Rejection Sampling
6.2
Maximum a Posteriori
6.2.1
Matrix Diﬀerentiation
6.2.2
Iterated Conditional Modes (ICM)
6.3
Advantages of ICM over Gibbs Sampling
6.4
Advantages of Gibbs Sampling over ICM
© 2003 by Chapman & Hall/CRC

7
Regression
7.1
Introduction
7.2
Normal Samples
7.3
Simple Linear Regression
7.4
Multiple Linear Regression
7.5
Multivariate Linear Regression
II 
Models
 
8
Bayesian Regression
8.1
Introduction
8.2
The Bayesian Regression Model
8.3
Likelihood
8.4
Conjugate Priors and Posterior
8.5
Conjugate Estimation and Inference
8.5.1
Marginalization
8.5.2
Maximum a Posteriori
8.6
Generalized Priors and Posterior
8.7
Generalized Estimation and Inference
8.7.1
Marginalization
8.7.2
Posterior Conditionals
8.7.3
Gibbs Sampling
8.7.4
Maximum a Posteriori
8.8
Interpretation
8.9
Discussion
9
Bayesian Factor Analysis
9.1
Introduction
9.2
The Bayesian Factor Analysis Model
9.3
Likelihood
9.4
Conjugate Priors and Posterior
9.5
Conjugate Estimation and Inference
9.5.1
Posterior Conditionals
9.5.2
Gibbs Sampling
9.5.3
Maximum a Posteriori
9.6
Generalized Priors and Posterior
9.7
Generalized Estimation and Inference
9.7.1
Posterior Conditionals
9.7.2
Gibbs Sampling
9.7.3
Maximum a Posteriori
9.8
Interpretation
9.9
Discussion
© 2003 by Chapman & Hall/CRC

10 Bayesian Source Separation
10.1
Introduction
10.2
Source Separation Model
10.3
Source Separation Likelihood
10.4
Conjugate Priors and Posterior
10.5
Conjugate Estimation and Inference
10.5.1
Posterior Conditionals
10.5.2
Gibbs Sampling
10.5.3
Maximum a Posteriori
10.6
Generalized Priors and Posterior
10.7
Generalized Estimation and Inference
10.7.1
Posterior Conditionals
10.7.2
Gibbs Sampling
10.7.3
Maximum a Posteriori
10.8
Interpretation
10.9
Discussion
11 Unobservable and Observable Source Separation
11.1
Introduction
11.2
Model
11.3
Likelihood
11.4
Conjugate Priors and Posterior
11.5
Conjugate Estimation and Inference
11.5.1
Posterior Conditionals
11.5.2
Gibbs Sampling
11.5.3
Maximum a Posteriori
11.6
Generalized Priors and Posterior
11.7
Generalized Estimation and Inference
11.7.1
Posterior Conditionals
11.7.2
Gibbs Sampling
11.7.3
Maximum a Posteriori
11.8
Interpretation
11.9
Discussion
12 FMRI Case Study
12.1
Introduction
12.2
Model
12.3
Priors and Posterior
12.4
Estimation and Inference
12.5
Simulated FMRI Experiment
12.6
Real FMRI Experiment
12.7
FMRI Conclusion
© 2003 by Chapman & Hall/CRC

13 Delayed Sources and Dynamic Coefficients
13.1
Introduction
13.2
Model
13.3
Delayed Constant Mixing
13.4
Delayed Nonconstant Mixing
13.5
Instantaneous Nonconstant Mixing
13.6
Likelihood
13.7
Conjugate Priors and Posterior
13.8
Conjugate Estimation and Inference
13.8.1
Posterior Conditionals
13.8.2
Gibbs Sampling
13.8.3
Maximum a Posteriori
13.9
Generalized Priors and Posterior
13.10 Generalized Estimation and Inference
13.10.1 Posterior Conditionals
13.10.2 Gibbs Sampling
13.10.3 Maximum a Posteriori
13.11 Interpretation
13.12 Discussion
14 Correlated Observation and Source Vectors
14.1
Introduction
14.2
Model
14.3
Likelihood
14.4
Conjugate Priors and Posterior
14.5
Conjugate Estimation and Inference
14.5.1
Posterior Conditionals
14.5.2
Gibbs Sampling
14.5.3
Maximum a Posteriori
14.6
Generalized Priors and Posterior
14.7
Generalized Estimation and Inference
14.7.1
Posterior Conditionals
14.7.2
Gibbs Sampling
14.7.3
Maximum a Posteriori
14.8
Interpretation
14.9
Discussion
15 Conclusion
Appendix A FMRI Activation Determination
A.1
Regression
A.2
Gibbs Sampling
A.3
ICM
Appendix B FMRI Hyperparameter Assessment
Bibliography
© 2003 by Chapman & Hall/CRC
III    Generalizations 

List of Figures
1.1
The cocktail party.
1.2
The unknown mixing process.
1.3
Sample mixing process.
1.4
The mixing process.
3.1
Pictorial representation of Bayes’ rule.
3.2
Pictorial representation of the Law of Total Probability.
10.1
Mixed and unmixed signals.
12.1
Experimental design: white task A, gray task B, and black
task C in seconds.
12.2
One trial: +1 when presented, -1 when not presented.
12.3
True source reference functions.
12.4
Simulated anatomical image.
12.5
Prior −−, Gibbs estimated −−, and ICM estimated ·· reference
functions for
12.6
Activations thresholded at 5 for prior reference functions.
12.7
Activations thresholded at 5 for Bayesian Gibbs reference func-
tions.
12.8
Activations thresholded at 5 Bayesian ICM reference func-
tions.
12.9
Prior −· and Bayesian ICM −−reference functions.
12.10 Activations for ﬁrst reference functions.
12.11 Activations for second reference functions.
© 2003 by Chapman & Hall/CRC

List of Tables
1.1
Sample data.
4.1
Scalar variate Conjugate priors.
4.2
Vector variate Conjugate priors.
4.3
Matrix variate Conjugate priors.
4.4
Scalar variate generalized Conjugate priors.
4.5
Vector variate generalized Conjugate priors.
4.6
Matrix variate generalized Conjugate priors.
6.1
Twelve independent random Uniform variates.
7.1
Regression data vector and design matrix.
7.2
Regression data and design matrices.
8.1
Variables for Bayesian Regression example.
8.2
Bayesian Regression data and design matrices.
8.3
Bayesian Regression prior data and design matrices.
8.4
Prior, Gibbs, and ICM Bayesian Regression coeﬃcients.
8.5
Prior, Gibbs, and ICM Regression covariances.
8.6
Statistics for coeﬃcients.
9.1
Variables for Bayesian Factor Analysis example.
9.2
Bayesian Factor Analysis data.
9.3
Gibbs sampling estimates of factor scores.
9.4
ICM estimates of factor scores.
9.5
Prior, Gibbs, and ICM means and loadings.
9.6
Factors in terms of strong observation loadings.
9.7
Gibbs estimates of Factor Analysis covariances.
9.8
ICM estimates of Factor Analysis covariances.
9.9
Statistics for means and loadings.
10.1
Variables for Bayesian Source Separation example.
10.2
Covariance hyperparameter for coeﬃcients.
10.3
Prior mode, Gibbs, and ICM source covariance.
10.4
Prior, Gibbs, and ICM mixing coeﬃcients.
10.5
Prior, Gibbs, and ICM covariances.
10.6
Statistics for means and loadings.
© 2003 by Chapman & Hall/CRC

10.7
Sources in terms of strong mixing coeﬃcients.
10.8
Plankton data.
10.9
Plankton prior data.
10.10 Prior sources.
10.11 Gibbs estimated sources.
10.12 ICM estimated sources.
12.1
True Regression and mixing coeﬃcients.
12.2
Covariance hyperparameter for coeﬃcients.
12.3
Prior, Gibbs, and ICM Regression coeﬃcients.
12.4
Prior, Gibbs, and ICM mixing coeﬃcients.
12.5
Prior, Gibbs, and ICM source covariances.
12.6
Error covariance prior mode.
12.7
Gibbs estimates of covariances.
12.8
ICM estimates of covariances.
12.9
Prior, Gibbs, and ICM coeﬃcient statistics.
© 2003 by Chapman & Hall/CRC

1
Introduction
1.1 
The Co cktail Pa rty
The Source Separation model will be easier to explain if the mechanics of a
“cocktail party” are ﬁrst described. The cocktail party is an easily understood
example of where the Source Separation model can be applied. There are
many other applications where the Source Separation model is appropriate. At
a cocktail party there are partygoers or speakers holding conversations while
at the same time there are microphones recording or observing the speakers
also called underlying sources. The cocktail party is illustrated in adapted
Figure 1.1. Partygoers, speakers, and sources will be used interchangeably as
will be recorded and observed. The cocktail party will be returned to often
when describing new concepts or material.
At the cocktail party there are typically several small groups of speakers
holding conversations. In each group, typically only one person is speaking
at a time. Consider the closest two speakers in Figure 1.1. In this group,
person one (left) speaks, then person two (right) speaks, then person one
again, and so on. The speakers are obviously negatively correlated. In the
Bayesian Source Separation model of this text, the speakers are allowed to be
correlated and not constrained to be independent.
At a cocktail party, there are p microphones that record or observe m
partygoers or speakers at n time increments.
This notation is consistent
with traditional Multivariate Statistics. The observed conversations consist
of mixtures of true unobservable conversations. A given microphone is not
placed to a given speakers’ mouth and is not shielded from the other speakers.
The microphones do not observe the speakers’ conversation in isolation. The
recorded conversations are mixed. The problem is to unmix or recover the
original conversations from the recorded mixed conversations.
Consider the following example.
There is a party with m = 4 speakers
and p = 3 microphones as seen in Figure 1.2. At time increment i, where
i = 1,...,n, the conversation emitted from speaker 1 is si1, speaker 2 is si2,
speaker 3 is si3, and speaker 4 is si4. Then, the recorded conversation at
microphone 1 is xi1, at microphone 2 is xi2, and at microphone 3 is xi3.
There is an unknown function f as illustrated in Figure 1.3 called the mixing
function which takes the emitted source signals and mixes them to produce
© 2003 by Chapman & Hall/CRC

FIGURE 1.1
The cocktail party.
the observed mixed signals.
1.2
The Source Separation Model
The p microphones are recording mixtures of the m speakers at each of the
n time increments. What is emitted from the m speakers at time i are m
distinct values collected as the rows of vector si and presented as
si =



si1
...
sim



(1.2.1)
and what is recorded at time i by the p microphones are p distinct values
collected as the rows of vector xi and presented as
© 2003 by Chapman & Hall/CRC

FIGURE 1.2
The unknown mixing process.
xi =



xi1
...
xip


.
(1.2.2)
Again, the goal is to separate or unmix these observed p-dimensional sig-
nal vectors into m-dimensional true underlying and unobserved source signal
vectors.
The process that mixes the speakers’ conversations is instantaneous, con-
stant, and independent over time. The number of speakers is known. Relax-
ation of these assumptions is discussed in Part III. The separation of sources
model for all time i is
(xi|si) = f(si) +
ǫi,
(p×1)
(p×1)
(p×1)
(1.2.3)
where f(si) is a function which is depicted in Figure 1.3 that mixes the source
signals and ǫi is the random error. Using a Taylor series expansion [11], the
function f, with appropriate smoothness conditions can be expanded about
the vector c, written as
© 2003 by Chapman & Hall/CRC

FIGURE 1.3
Sample mixing process.
f(si) = f(c)+f ′(c)(si −c)+... ,
(1.2.4)
and by considering the ﬁrst two terms (as in the familiar Regression model)
becomes
f(si) = f(c)+f ′(c)(si −c)
= [f(c)−f ′(c)c]+f ′(c)si
= µ+Λsi,
(1.2.5)
where f ′(c) and Λ are p × m matrices.
This is called the linear synthesis
model. As implied in Figure 1.4, the source signal emitted from each of the
speakers’ mouths gets multiplied by a mixing coeﬃcient which determines the
© 2003 by Chapman & Hall/CRC

strength of its contribution but there is also an overall mean background noise
level at each microphone and random error entering into the mixing process
which is recorded. More formally, the adopted model is
(xi|µ,Λ,si) =
µ
+
Λ
si
+
ǫi,
(p×1)
(p×1)
(p×m) (m×1)
(p×1)
(1.2.6)
FIGURE 1.4
The mixing process.
where xi is as previously described,
µ =



µ1
...
µp



(1.2.7)
is a p-dimensional unobserved population mean vector,
© 2003 by Chapman & Hall/CRC

Λ =



λ′
1
...
λ′
p



(1.2.8)
is a p×m matrix of unobserved mixing coeﬃcients, si is the ith m-dimensional
unobservable source vector as previously described, and
ǫi =



ǫi1
...
ǫip



(1.2.9)
is the p-dimensional vector of errors or noise terms of the ith observed signal
vector.
The observed mixed signal xij is the jth element of the observed mixed
signal vector i, which may be thought of as the recorded mixed conversation
signal at time increment i, i = 1,...,n for microphone j, j = 1,...,p. The
observed signal xij is a mixture of the sources or true unobserved speakers
conversations si with error, at time increment i, i = 1,...,n. The unobserved
source signal sik is the kth element of the unobserved source vector si, which
may be thought of as the unobserved source signal conversation of speaker k,
k = 1,...,m at time increment i, i = 1,...,n.
The model describes the mixing process by writing the observed signal xij
as the sum of an overall (background) mean part µj plus a linear combination
of the unobserved source signal components sik and the observation error ǫij.
Element j of xij may be found by simple matrix multiplication and addition
as the jth element of µ, µj; plus the element-wise multiplication and addition
of the jth row of Λ, λ′
j and si; and the addition of the jth element of ǫi, ǫij.
This is written in vector notation as
(xij|µj,λj,si) = µj +
m

k=1
λjk sik +ǫij
= µj +λ′
j si +ǫij.
(1.2.10)
Simply put, the observed conversation for a given microphone consists of
an overall background mean at that microphone plus contributions from each
of the speakers and random error. The contribution from the speakers to a
recorded conversation depends on the coeﬃcient for the speaker to the micro-
phone. The problem at hand is to unmix the unobserved sources and obtain
information regarding the mixing process by determining the remaining pa-
rameters in the model.
© 2003 by Chapman & Hall/CRC

Exercises
1. Describe in words xij, λjk, sik, µj, and ǫij including subscripts.
2. Assuming that the mixing process described in Equation 1.2.6 is known
to have values as listed in Table 1.1, where si is the vector valued speak-
ers signal at time i, compute the observed vector value xi.
TABLE 1.1
Sample data.
µ
Λ
si
ǫi
1
5 5 3 1
2
3
2
3 5 5 3
4
4
3
1 3 5 5
6
5
8
3. Assume that a quadratic term is also kept in the Taylor series expansion.
Write down the model for xij with a quadratic term.
© 2003 by Chapman & Hall/CRC

Part I
Fundamentals
© 2003 by Chapman & Hall/CRC

2
Statistical Distributions
In this Chapter of the text, various statistical distributions used in Bayesian
Statistics are described. Although scalar and vector distributions can be found
as special cases of their corresponding matrix distribution, they are treated
individually for clarity.
2.1
Scalar Distributions
2.1.1
Binomial
A Bernoulli trial or experiment is an experiment in which there are two
possible outcomes, one labeled “success” with probability ̺, the other labeled
“failure” with probability 1 −̺. The Binomial distribution [1, 22] gives the
probability of x successes out of n independent Bernoulli trials, where the
probability of success in each trial is ̺. A Bernoulli trial is an experiment
such as ﬂipping a coin where there are only two possible outcomes.
A random variable that follows a Binomial distribution is denoted
x|̺ ∼Bin(n,̺)
(2.1.1)
where (n,̺) parameterize the distribution which is given by
p(x|̺) =
n!
(n−x)!x!̺x(1−̺)n−x
(2.1.2)
with
x ∈{x : 0,1,...,n},
̺ ∈(0,1).
(2.1.3)
Properties
The mean, mode, and variance of the Binomial distribution are
E(x|̺) = n̺
(2.1.4)
Mode(x|̺) = x0 (as deﬁned below)
(2.1.5)
var(x|̺) = n̺(1−̺)
(2.1.6)
© 2003 by Chapman & Hall/CRC

which can be found by summation and diﬀerencing.
Since the Binomial distribution is a discrete distribution, diﬀerentiation in
order to determine the most probable value is not appropriate. However, it is
well known that the Binomial distribution increases monotonically and then
decreases monotonically [47]. The most probable value is when x = x0, where
x0 is an integer such that
(n+1)̺−1 < x0 ≤(n+1)̺
(2.1.7)
which can be found by diﬀerencing instead of taking the derivative.
2.1.2
Beta
The Beta distribution [1, 22] gives the probability of a random variable ̺
having a particular value between zero and one. The Beta distribution is often
used as the prior distribution (see the Conjugate procedure in Chapter 4) for
the probability of success ̺ in a Binomial experiment.
A random variable that follows a Beta distribution is denoted
̺|α,β ∼B(α,β),
(2.1.8)
where (α,β) parameterize the distribution which is given by
p(̺|α,β) = Γ(α+β)
Γ(α)Γ(β)̺α−1(1−̺)β−1,
(2.1.9)
where
̺ ∈(0,1),
α ∈(0,+∞),
β ∈(0,+∞)
(2.1.10)
and Γ(·) is the gamma function which is given by
Γ(α) =
 +∞
0
tα−1e−t dt
(2.1.11)
for α ∈R+, where R denotes the set of real numbers, R+ the set of positive
real numbers,
Γ(α) = αΓ(α−1)
(2.1.12)
and
Γ(α) = (α−1)!
(2.1.13)
for α ∈N, where N denotes the set of natural numbers. Another property of
the gamma function is that
Γ
	1
2

= π
1
2 .
(2.1.14)
© 2003 by Chapman & Hall/CRC

Properties
The mean, mode, and variance of the Beta distribution are
E(̺|α,β) =
α
α+β ,
(2.1.15)
Mode(̺|α,β) =
α−1
α+β −2,
(2.1.16)
var(̺|α,β) =
αβ
(α+β)2(α+β +1),
(2.1.17)
which can be found by integration and diﬀerentiation. The mode is deﬁned
for α+β > 2. Note that the Uniform distribution is a special case of the Beta
distribution with α = β = 1.
Generalized Beta
A random variable ρ = (b −a)̺ + a (where ̺ ∼B(α,β)) is said to have a
generalized Beta (type I) distribution [17, 41] given by
p(ρ|α,β) = Γ(α+β)
Γ(α)Γ(β)
	ρ−a
b−a

α−1 	
1−ρ−a
b−a

β−1
,
(2.1.18)
with
ρ ∈(a,b),
α ∈(0,+∞),
β ∈(0,+∞).
(2.1.19)
Note that the range of ρ is in the interval (a,b), and if (a,b) = (−1,1), then
this is a Beta distribution over the interval (−1,1), which has been used as
the prior distribution for a correlation. However, as shown in Chapter 4, a
Beta prior distribution is not the Conjugate prior distribution for the corre-
lation parameter ρ. The normalizing constant is often omitted in practice in
Bayesian Statistics.
Properties
The mean, mode, and variance of the generalized Beta (type I) distribution
are
E(ρ|α,β) = (b−a)
α
α+β +a,
(2.1.20)
Mode(ρ|α,β) = (α−1)b−(β −1)a
α+β −2
,
(2.1.21)
var(ρ|α,β) =
(b−a)2αβ
(α+β)2(α+β +1),
(2.1.22)
which can be found by integration and diﬀerentiation. The mode is deﬁned
for α+β > 2.
© 2003 by Chapman & Hall/CRC

2.1.3
Normal
The Normal or Gaussian distribution [1, 17, 22, 41] is used to describe
continuous real valued random variables.
A random variable that follows a Normal distribution is denoted
x|µ,σ2 ∼N(µ,σ2),
(2.1.23)
where (µ,σ2) parameterize the distribution which is given by
p(x|µ,σ2) = (2πσ2)−1
2 e−(x−µ)2
2σ2
(2.1.24)
with
x ∈(−∞,+∞),
µ ∈(−∞,+∞),
σ ∈(0,+∞).
(2.1.25)
Properties
The mean, mode, and variance of the Normal distribution are
E(x|µ,σ2) = µ,
(2.1.26)
Mode(x|µ,σ2) = µ,
(2.1.27)
var(x|µ,σ2) = σ2,
(2.1.28)
which can be found by integration and diﬀerentiation.
The Normal distribution, also called the bell curve, is that distribution
which any other distribution with ﬁnite ﬁrst and second moments tends to be
on average according to the central limit theorem [1].
2.1.4
Gamma and Scalar Wishart
A Gamma variate [1, 22] is found as a variate which is the sum of the
squares of ν0 centered independent Normal variates with common mean µ
and variance υ2, g = (x1 −µ)2 +···+(xν0 −µ)2. The variance of the Normal
random variates along with the number of random variates characterize the
distribution which is presented in a more familiar Multivariate parameteriza-
tion. A random variable that follows a Gamma distribution is denoted
g|α,β ∼G(α,β),
(2.1.29)
where (α,β) parameterize the distribution which is given by
p(g|α,β) = gα−1e−g/β
Γ(α)βα ,
(2.1.30)
with Γ(·) being the gamma function and
© 2003 by Chapman & Hall/CRC

g ∈R+,
α ∈R+,
β ∈R+,
(2.1.31)
where R denotes the set of real numbers and R+ the set of positive real
numbers.
Properties
The mean, mode, and variance of the Gamma distribution are
E(g|α,β) = αβ,
(2.1.32)
Mode(g|α,β) = (α−1)β,
(2.1.33)
var(g|α,β) = αβ2,
(2.1.34)
which can be found by integration and diﬀerentiation. The mode is deﬁned
for α > 1.
A more familiar parameterization used in Multivariate Statistics [17, 41]
which is the Scalar Wishart distribution is when
α = ν0
2 ,
β = 2υ2.
(2.1.35)
The Wishart distribution is the Multivariate (Matrix variate) generaliza-
tion of the Gamma distribution. A random variate g that follows the one-
dimensional or Scalar Wishart distribution is denoted by
g|υ2,ν0 ∼W(υ2,1,ν0),
(2.1.36)
where (υ2,ν0) parameterize the distribution which is given by
p(g|υ2,ν0) = (υ2)−ν0
2 g
ν0−2
2
e−
g
2υ2
Γ
 ν0
2

2
ν0
2
,
(2.1.37)
where
g ∈R+,
υ2 ∈R+,
ν0 ∈R+.
(2.1.38)
Although the Gamma and Scalar Wishart distributions were derived from
ν0 (an integer valued positive number) Normal variates, there is no restriction
that ν0 in these distributions be integer valued.
Properties
The mean, mode, and variance of the Scalar Wishart distribution are
E(g|υ2,ν0) = ν0υ2,
(2.1.39)
Mode(g|υ2,ν0) = (ν0 −2)υ2,
(2.1.40)
var(g|υ2,ν0) = 2ν0υ4,
(2.1.41)
© 2003 by Chapman & Hall/CRC

which can be found by integration and diﬀerentiation. The mode is deﬁned
for ν0 > 2.
This parameterization will be followed in this text. Note that the familiar
Chi-squared distribution with ν0 degrees of freedom results when
α = ν0
2 ,
β = 2.
(2.1.42)
2.1.5
Inverted Gamma and Scalar Inverted Wishart
An Inverted Gamma variate [1, 22] is found as a variate which is the recip-
rocal of a Gamma variate, σ2 = g−1. A random variable σ2 that follows an
Inverted Gamma distribution is denoted
σ2|α,β ∼IG(α,β),
(2.1.43)
where (α,β) parameterize the distribution which is given by
p(σ2|α,β) = (σ2)−(α+1)e
−
1
βσ2
Γ(α)βα
,
(2.1.44)
with Γ(·) being the gamma function,
σ2 ∈R+,
α ∈R+,
β ∈R+.
(2.1.45)
Properties
The mean, mode, and variance of the Inverted Gamma distribution are
E(σ2|α,β) =
1
(α−1)β ,
(2.1.46)
Mode(σ2|α,β) =
1
(α+1)β ,
(2.1.47)
var(σ2|α,β) =
1
(α−1)2(α−2)β2 ,
(2.1.48)
which can be found by integration and diﬀerentiation. The mean is deﬁned
for α > 1 and the variance for α > 2.
A more familiar parameterization used in Multivariate Statistics [17, 41]
which is the scalar version of the Inverted Wishart distribution is
α = ν −2
2

= ν0
2

,
β = 2
q

= 2υ2
.
(2.1.49)
A random variable which follows a Scalar Inverted Wishart distribution is
denoted by
σ2|q,ν0 ∼IW(q,1,ν),
(2.1.50)
© 2003 by Chapman & Hall/CRC

where (q,ν) parameterize the distribution which is given by
p(σ2|q,ν) = (σ2)−ν
2 q
ν−2
2 e−
q
2σ2
Γ
 ν−2
2

2
ν−2
2
,
(2.1.51)
where
σ2 ∈R+,
q ∈R+,
ν ∈R+.
(2.1.52)
Note: In the transformation of variable from g to σ2,
q = υ−2,
ν0 = ν −2,
(2.1.53)
and the Jacobian of the transformation is
J(g →σ2) = σ−4.
(2.1.54)
Although the Scalar Inverted Wishart distribution was derived from ν −2
(an integer valued positive number) Normal variates, there is no restriction
that ν in the Scalar Inverted Wishart distribution be integer valued.
Properties
The mean, mode, and variance of the Scalar Inverted Wishart distribution
are
E(σ2|q,ν) =
q
ν −2−2,
(2.1.55)
Mode(σ2|q,ν) = q
ν ,
(2.1.56)
var(σ2|q,ν) =
2q2
(ν −2−2)2(ν −2−4),
(2.1.57)
which can be found by integration and diﬀerentiation. The mean is deﬁned
for ν > 4 and the variance for ν > 6. Note the purposeful use of “ν −2 −2”
and “ν −2−4” which will become clear with the introduction of the Inverted
Wishart distribution.
This parameterization will be followed in this text.
Note that the less
familiar Inverted Chi-squared distribution results when
α = ν0
2 ,
β = 2.
(2.1.58)
2.1.6
Student t
The Scalar Student t-distribution [1, 17, 22, 41] is used to describe contin-
uous real-valued random variables with slightly heavier tails than the Normal
distribution. It is derived by taking
© 2003 by Chapman & Hall/CRC

x ∼N(µ,σ2)
and
g ∼W(σ−2,1,ν),
(2.1.59)
transforming variables to
t = ν
1
2 g−1
2 (x−µ)+t0
and
w = g,
(2.1.60)
with Jacobian
J(x,g →t,w) = ν−1
2 w
1
2 ,
(2.1.61)
and then integrating with respect to w. In the derivation, x could be the av-
erage of independent and identically distributed Scalar Normal variates with
common mean and variance, while g could be the sum of the squares of devi-
ations of these variates about their average.
A random variable that follows a Scalar Student t-distribution is denoted
t|ν,t0,σ2,φ2 ∼t(ν,t0,σ2,φ2),
(2.1.62)
where (ν,t0,σ2,φ2) are degrees of freedom, location, scale, and spread para-
meters which parameterize the distribution given by
p(t|ν,t0,σ2,φ2) =
Γ( ν+1
2 )
(νπ)
1
2 Γ( ν
2)
σ−1φ−ν

φ2 + 1
ν
 t−t0
σ
2 ν+1
2
,
(2.1.63)
with
t ∈R,
ν ∈R+
t0 ∈R,
σ ∈R+,
φ ∈R+.
(2.1.64)
Properties
The mean, mode, and variance of the Scalar Student t-distribution are
E(t|ν,t0,σ2,φ2) = t0,
(2.1.65)
Mode(t|ν,t0,σ2,φ2) = t0,
(2.1.66)
var(t|ν,t0,σ2,φ2) =
ν
ν −2φ2σ2,
(2.1.67)
which can be found by integration and diﬀerentiation. Note that this para-
meterization is a generalization of the typical one used which can be found
when φ2 = 1.
The mean of the Scalar Student t-distribution only exists for ν > 1 and
the variance only exists for ν > 2. If ν ∈(0,1], then neither the mean nor
the variance exists.
When ν = 1, the Scalar Student t-distribution is the
Cauchy distribution whose mean and variance or ﬁrst and second moments
do not exist. As the number of degrees of freedom increases, a random variate
which follows the Scalar Student t-distribution t ∼t(ν,t0,σ2,φ2) approaches
a Normal distribution t ∼N(t0,φ2σ2) [17, 41].
© 2003 by Chapman & Hall/CRC

2.1.7
F-Distribution
The F-distribution [1, 22, 66] is used to describe continuous random vari-
ables which are strictly positive. It is derived by taking
x1 ∼W(1,1,ν1)
and
x2 ∼W(1,1,ν2),
(2.1.68)
and transforming variables to
x = x1/ν1
x2/ν2
.
(2.1.69)
In the derivation, x1 and x2 could be independent sums or squared deviations
of standard Normal variates.
A random variable that follows an F-distribution is denoted
x|ν1,ν2 ∼F(ν1,ν2),
(2.1.70)
where (ν1,ν2) referred to as the numerator and denominator degrees of free-
dom respectively, which parameterize the distribution given by
p(x|ν1,ν2) =
Γ
 ν1+ν2
2

Γ
 ν1
2

Γ
 ν2
2

	ν1
ν2

 ν1
2
x
ν1
2 −1
	
1+ ν1
ν2
x

−ν1+ν2
2
,
(2.1.71)
with
x ∈R+,
ν1 ∈N
ν2 ∈N.
(2.1.72)
Properties
The mean, mode, and variance of the F-distribution are
E(x|ν1,ν2) =
ν2
ν2 −2,
(2.1.73)
Mode(x|ν1,ν2) = ν2(ν1 −2)
ν1(ν2 +2),
(2.1.74)
var(x|ν1,ν2) =
2ν2
2(ν1 +ν2 −2)
ν1(ν2 −2)2(ν2 −4),
(2.1.75)
which can be found by integration and diﬀerentiation.
The mean of the F-distribution only exists for ν2 > 2, and mode for ν1 > 2,
while the variance only exists for ν2 > 4. The square of a variate t which
follows a Scalar Student t-distribution, t ∼t(ν,0,0,1) is a variate which follows
an F-distribution with ν1 = 1 and ν2 = ν degrees of freedom. The result of
transforming a variate x which follows an F-distribution x ∼F(ν1,ν2) by
1/[1+(ν1/ν2)x] is a Beta variate with α = ν2/2 and β = ν1/2.
© 2003 by Chapman & Hall/CRC

2.2
Vector Distributions
A p-variate vector observation x is a collection of p scalar observations, say
x1,...,xp, arranged in a column.
2.2.1
Multivariate Normal
The p-variate Multivariate Normal distribution [17, 41] is used to simulta-
neously describe a collection of p continuous real-valued random variables.
A random variable that follows a p-variate Multivariate Normal distribution
with mean vector µ and covariance matrix Σ is denoted
x|µ,Σ ∼N(µ,Σ),
(2.2.1)
where (µ,Σ) parameterize the distribution which is given by
p(x|µ,Σ) = (2π)−p
2 |Σ|−1
2 e−1
2 (x−µ)′Σ−1(x−µ)
(2.2.2)
with
x ∈Rp,
µ ∈Rp,
Σ > 0,
(2.2.3)
where Rp denotes the set of p-dimensional real numbers and Σ > 0 that Σ
belongs to the set of p-dimensional positive deﬁnite matrices.
Properties
The mean, mode, and variance of the Multivariate Normal distribution are
E(x|µ,Σ) = µ,
(2.2.4)
Mode(x|µ,Σ) = µ,
(2.2.5)
var(x|µ,Σ) = Σ,
(2.2.6)
which can be found by integration and diﬀerentiation.
Since x follows a Multivariate Normal distribution, the conditional and
marginal distributions of any subset are Multivariate Normal distributions
[17, 41].
The p-variate Normal distribution is that distribution, which other with
ﬁnite ﬁrst and second moments tend to on average according to the central
limit theorem.
2.2.2
Multivariate Student t
The Multivariate Student t-distribution [17, 41] is used to describe contin-
uous real-valued random variables with slightly heavier tails than the Multi-
variate Normal distribution. It is derived by taking
© 2003 by Chapman & Hall/CRC

x ∼N(µ,φ−2)
and
G ∼W(Σ,p,ν),
(2.2.7)
transforming variables to
t = ν
1
2 G−1
2 (x−µ)+t0
and
W = G,
(2.2.8)
with Jacobian
J(x,G →t,W) = ν−p
2 W
p
2 ,
(2.2.9)
and then integrating with respect to W. In the derivation, x could be the
average of of independent and identically distributed Vector Normal variates
with common mean vector and covariance matrix, while G could be the sum
of the squares of deviations of these variates about their average.
A random variable that follows a p-variate Multivariate Student t-distribution
[17, 41] is denoted
t|ν,t0,Σ,φ2 ∼t(ν,t0,Σ,φ2)
(2.2.10)
where (ν,µ,Σ,φ2) parameterize the distribution which is given by
p(t|ν,t0,Σ,φ2) =
kt(φ2)−ν
2 |Σ|−1
2
[φ2 + 1
ν (t−t0)′Σ−1(t−t0)]
ν+p
2
,
(2.2.11)
where
kt =
Γ
 ν+p
2

(νπ)
p
2 Γ
 ν
2

(2.2.12)
with
t ∈Rp,
ν ∈R+,
t0 ∈Rp,
Σ > 0,
φ ∈R+.
(2.2.13)
Properties
The mean, mode, and variance of the Multivariate Student t-distribution
are
E(t|ν,t0,Σ,φ2) = t0,
(2.2.14)
Mode(t|ν,t0,Σ,φ2) = t0,
(2.2.15)
var(t|ν,0 ,Σ,φ2) =
ν
ν −2φ2Σ,
(2.2.16)
which can be found by integration and diﬀerentiation. Note that this para-
meterization is a generalization of the typical one used which can be found
when φ2 = 1.
© 2003 by Chapman & Hall/CRC

The mean of the Multivariate Student t-distribution exists for ν > 1 and
the variance for ν > 2. When ν = 1, the Multivariate Student t-distribution
is the Multivariate Cauchy distribution whose mean and variance or ﬁrst and
second moments do not exist.
As the number of degrees of freedom increases, a random variate which
follows the Multivariate Student t-distribution t ∼t(ν,t0,Σ,φ2) approaches a
Normal distribution t ∼N(t0,φ2Σ) [17, 41].
2.3
Matrix Distributions
2.3.1
Matrix Normal
The n×p Matrix Normal distribution [17, 31] can be derived as a special
case of the np-variate Multivariate Normal distribution when the covariance
matrix is separable. Denote an np-dimensional Multivariate Normal distrib-
ution with np-dimensional mean µ and np×np covariance matrix Ωby
p(x|µ,Ω) = (2π)−np
2 |Ω|−1
2 e−1
2 (x−µ)′Ω−1(x−µ).
(2.3.1)
A separable matrix is one of the form Ω= Φ⊗Σ where ⊗is the Kronecker
product which multiplies every entry of its ﬁrst matrix argument by its entire
second matrix argument.
The Kronecker product of Φ and Σ which are n- and p-dimensional matrices
respectively, is
Φ⊗Σ =



φ11Σ ··· φ1nΣ
...
φn1Σ ··· φnnΣ


.
(2.3.2)
Substituting the separable covariance matrix into the above distribution yields
p(x|µ,Σ,Φ) = (2π)−np
2 |Φ⊗Σ|−1
2 e−1
2 (x−µ)′(Φ⊗Σ)−1(x−µ)
(2.3.3)
which upon using the matrix identities
|Φ⊗Σ|−1
2 = |Φ|−p
2 |Σ|−n
2 ,
and
(x−µ)′(Φ⊗Σ)−1(x−µ) = trΦ−1(X −M)Σ−1(X −M)′,
where x = (X′) = (x′
1,...,x′
n)′, X′ = (x1,...,xn), µ = vec(M ′) = (µ′
1,...,µ′
n)′,
and M ′ = (µ1,...,µn), then Equation 2.3.3 becomes
© 2003 by Chapman & Hall/CRC

p(X|M,Σ,Φ) = (2π)−np
2 |Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′.
(2.3.4)
The “vec” operator vec(·) which stacks the columns of its matrix argument
from left to right into a single vector has been used as has the trace opera-
tor tr(·) which gives the sum of the diagonal elements of its square matrix
argument.
A random variable that follows an n × p Matrix Normal distribution is
denoted
X|M,Σ,Φ ∼N(M,Φ⊗Σ)
(2.3.5)
where (M,Σ,Φ) parameterize the above distribution with
X ∈Rn×p,
M ∈Rn×p,
Σ,Φ > 0.
(2.3.6)
The matrices Σ and Φ are commonly referred to as the within and between
covariance matrices.
Sometimes they are referred to as the right and left
covariance matrices.
Properties
The mean, mode, and variance of the Matrix Normal distribution are
E(X|M,Σ,Φ) = M,
(2.3.7)
Mode(X|M,Σ,Φ) = M,
(2.3.8)
var(vec(X′)|M,Σ,Φ) = Φ⊗Σ,
(2.3.9)
which can be found by integration and diﬀerentiation.
Since X follows a Matrix Normal distribution, the conditional and marginal
distributions of any row or column subset are Multivariate Normal distribu-
tions [17, 41]. It should also be noted that the mean of the ith row of X,
x′
i is the corresponding ith row of M, µ′
i, and the covariance of the ith row
of X is φiiΣ, where φii is the element in the ith row and ith column of Φ.
The covariance between the ith and i′th rows of X is φii′Σ, where φii′ is the
element in the ith row and i′th column of Φ. Similarly, the mean of the jth
column of X is the jth column of M and the covariance between the jth and
j′th columns of X is σjj′Φ.
Simply put, if
X =



x′
1
...
x′
n


= (X1,...,Xp),
(2.3.10)
M =



µ′
1
...
µ′
n


= (M1,...,Mp),
(2.3.11)
© 2003 by Chapman & Hall/CRC

φii′ denotes the ii′th element of Φ and σjj′ denotes the jj′th element of Σ then
var(xi|µi,φii,Σ) = φiiΣ,
(2.3.12)
cov(xi,xi′|µi,µi′,φii′,Σ) = φii′Σ,
(2.3.13)
var(Xj|Mj,σjj,Φ) = σjjΦ,
(2.3.14)
cov(Xj,Xj′|Mj,Mj′,σjj′,Φ) = σjj′Φ.
(2.3.15)
2.3.2
Wishart
A Wishart variate is found as a variate which is the transpose product
G = (X −M)′(X −M), where X is a ν0 ×p Matrix Normal variate with mean
matrix M and covariance matrix Iν0 ⊗Υ. Note that if p = 1, this is the sum of
the squares of ν0 centered independent Normal variates with common mean
µ and variance υ2, g = (x1 −µ)2 +···+(xν0 −µ)2. The covariance matrix Υ
enters into the Wishart distribution as follows. A p × p random symmetric
matrix G that follows a Wishart distribution [17, 41] is denoted
G|Υ,p,ν0 ∼W(Υ,p,ν0),
(2.3.16)
where (Υ,p,ν0) parameterize the distribution which is given by
p(G|Υ,p,ν0) = kW |Υ|−ν0
2 |G|
ν0−p−1
2
e−1
2 trΥ−1G,
(2.3.17)
where
k−1
W = 2
ν0p
2 π
p(p−1)
4
p

j=1
Γ
	ν0 +1−j
2

(2.3.18)
with
G > 0,
ν0 ∈R+,
Υ > 0,
(2.3.19)
and “> 0” is used to denote that both G and Υ belong to the set of positive
deﬁnite matrices. Although the Wishart distribution was derived from ν0 (an
integer valued positive number) vector Normal variates, there is no restriction
that ν0 in the Wishart distribution be integer valued.
Properties
The mean, mode, and variance of the Wishart distribution are
E(G|ν0,Υ) = ν0Υ,
(2.3.20)
Mode(G|ν0,Υ) = (ν0 −p−1)Υ,
(2.3.21)
var(gij|ν0,Υ) = ν0(υ2
ij +υiiυjj),
(2.3.22)
cov(gijgkl|ν0,Υ) = ν0(υikυjl +υilυjk),
(2.3.23)
© 2003 by Chapman & Hall/CRC

which can be found by integration and diﬀerentiation, where gij and υij de-
note the ijth elements of G and Υ respectively. The mode of the Wishart
distribution is deﬁned for ν0 > p+1.
The Wishart distribution is the Multivariate (Matrix variate) analog of the
univariate Gamma distribution.
2.3.3
Inverted Wishart
An Inverted Wishart variate Σ is found as a variate which is the reciprocal
of a Wishart variate, Σ = G−1. A p × p random matrix Σ that follows an
Inverted Wishart distribution [17, 41] is denoted
Σ|Q,p,ν ∼IW(Q,p,ν),
(2.3.24)
where (Q,p,ν) parameterize the distribution which is given by
p(Σ|ν,Q) = kIW |Q|
ν−p−1
2
|Σ|−ν
2 e−1
2 trΣ−1Q,
(2.3.25)
where
k−1
IW = 2
(ν−p−1)p
2
π
p(p−1)
4
p

j=1
Γ
	ν −p−j
2

(2.3.26)
with
Σ > 0,
ν ∈R+,
Q > 0.
(2.3.27)
Note: In the transformation of variable from G to Σ,
Q = Υ−1,
ν0 = ν −p−1,
(2.3.28)
and the Jacobian of the transformation is
J(G →Σ) = |Σ|−(p+1).
(2.3.29)
Although the Inverted Wishart distribution was derived from ν −p−1 (an
integer valued positive number) vector Normal variates, there is no restriction
that ν in the Inverted Wishart distribution be integer valued.
Properties
The mean, mode, and variance of the Inverted Wishart distribution are
E(Σ|ν,Q) =
Q
ν −2p−2,
(2.3.30)
Mode(Σ|ν,Q) = Q
ν ,
(2.3.31)
© 2003 by Chapman & Hall/CRC

var(σii|ν,Q) =
2q2
ii
(ν −2p−2)2(ν −2p−4),
(2.3.32)
var(σii′|ν,Q) =
qiiqi′i′ +
ν−2p
ν−2p−2q2
ii′
(ν −2p−1)(ν −2p−2)(ν −2p−4),
(2.3.33)
cov(σii′,σıı′|ν,Q) =
2
ν−2p−2qiiqıı′ +qiıqi′ı′ +qiı′qıi′
(ν −2p−1)(ν −2p−2)(ν −2p−4)
(2.3.34)
which can be found by integration and diﬀerentiation. The mean is deﬁned
for ν > 2p+2 while the variances and covariances are deﬁned for ν > 2p+4.
The variances are deﬁned for i ̸= i′. Where σij and qij denote the ijth element
of Σ and Q respectively.
2.3.4
Matrix T
The Matrix Student T-distribution [17, 41] is used to describe continuous
random variables with slightly heavier tails than the Normal distribution. It
is derived by taking
X ∼N(M,In ⊗Σ)
and
G ∼W(Φ−1,p,ν),
(2.3.35)
transforming variables to
T = ν
1
2 G−1
2 (X −M)+T0
and
W = G,
(2.3.36)
with Jacobian
J(X,G →T,W) = ν−np
2 W
p
2 ,
(2.3.37)
and then integrating with respect to W. In the derivation, X could be the
average of independent and identically distributed Matrix Normal variates
with common mean and variance, while G could be the sum of the squares of
deviations of these variates about their average.
A random variable T follows a n×p Matrix Student T-distribution [17, 41]
is denoted
T|ν,T0,Σ,Φ ∼T(ν,T0,Σ,Φ),
(2.3.38)
where (ν,T0,Σ,Φ) parameterize the distribution which is given by
p(T|ν,T0,Σ,Φ) = kT
|Φ|
ν
2 |Σ|−n
2
|Φ+ 1
ν (T −T0)Σ−1(T −T0)′|
ν+p
2
,
(2.3.39)
where
kT =
n
j=1 Γ
 ν+p+1−j
2

(νπ)
np
2 n
j=1 Γ
 ν+1−j
2

(2.3.40)
© 2003 by Chapman & Hall/CRC

with
T ∈Rn×p,
ν ∈R+,
T0 ∈Rn×p,
Σ,Φ > 0.
(2.3.41)
Properties
The mean, mode, and variance of the Matrix Student T-distribution are
E(T|ν,T0,Σ,Φ) = T0,
(2.3.42)
Mode(T|ν,T0,Σ,Φ) = T0,
(2.3.43)
var(vec(T ′)|ν,T0,Σ,Φ) =
ν
ν −2(Φ⊗Σ)
(2.3.44)
which can be found by integration and diﬀerentiation. Note that in typical
parameterizations [17, 41], the degrees of freedom ν and the matrix Φ are
grouped together as a single matrix.
Since T follows a Matrix Student T-distribution, the conditional and mar-
ginal distributions of any row or column of T are Multivariate Student t-
distribution [17, 41].
The mean of the Matrix Student T-distribution exists for ν > 1 and the
variance exists for ν > 2. When the hyperparameter ν = 1, the Matrix Student
T-distribution is the Matrix Cauchy distribution whose mean and variance or
ﬁrst and second moments do not exist. As the number of degrees of freedom
ν increases, a random matrix variate which follows the Matrix Student T-
distribution, T ∼T(ν,T0,Σ,Φ) approaches the Matrix Normal distribution
T ∼N(T0,Φ⊗Σ) [17].
Multivariate generalizations of the Binomial and Beta distribution also ex-
ist.
Since they are not used in this text, they have been omitted.
For a
description of the Multivariate Binomial distribution see [30] or [33] and for
the Multivariate Beta see [17].
© 2003 by Chapman & Hall/CRC

Exercises
1. Compute the mean, mode, and variance of the Scalar Normal distribu-
tion.
2. Compute the mean, mode, and variance of the Scalar Student t-distribution.
3. Compute the mean, mode, and variance of the Gamma distribution.
4. Compute the mean, mode, and variance of the Inverted Gamma distri-
bution.
5. Look at the mean, mode, and covariance matrix for the Multivariate
Normal distribution. Reason that the mean, mode, and variance of the
Scalar Normal distribution follows by letting p = 1.
6. Look at the mean, mode, and covariance matrix of the Multivariate
Student t-distribution. Reason that the mean, mode, and variance of
the Scalar Student t-distribution follows by letting p = 1.
7. Look at the mean, mode, and covariance matrix for the Matrix Nor-
mal distribution.
Reason that the mean, mode, and variance of the
Multivariate Normal distribution follows by letting n = 1 (and hence
the mean, mode, and variance of the Scalar Student t-distribution by
letting n = 1 and p = 1).
8. Look at the mean, mode, and covariance matrix of the Matrix Student
T-distribution. Reason that the mean, mode, and variance of the Mul-
tivariate Student t-distribution follows by letting n = 1 (and hence the
mean, mode, and variance of the Scalar Student t-distribution by letting
n = 1 and p = 1).
9. Look at the mean, mode, variances, and covariances of the Wishart
distribution. Reason that the mean, mode, and variance of the Gamma
distribution follows by letting p = 1.
10. Look at the mean, mode, variances, and covariances of the Inverted
Wishart distribution. Reason that the mean, mode, and variance of the
Gamma distribution follows by letting p = 1.
11. Look at the Multivariate Normal distribution. Reason that the Scalar
Normal distribution follows by letting p = 1.
12. Look at the Multivariate Student t-distribution. Reason that the Scalar
Student t-distribution follows by letting p = 1.
© 2003 by Chapman & Hall/CRC

13. Look at the Matrix Normal distribution. Reason that the Multivari-
ate Normal distribution follows by letting n = 1 (and hence the Scalar
Normal distribution by letting n = 1 and p = 1).
14. Look at the Matrix Student T-distribution. Reason that the Multivari-
ate Student t-distribution follows by letting n = 1 (and hence the Scalar
Student t-distribution by letting n = 1 and p = 1).
15. Look at the Wishart distribution. Reason that the Gamma distribution
follows by letting p = 1.
16. Look at the Inverted Wishart distribution.
Reason that the inverse
Gamma distribution follows by letting p = 1.
© 2003 by Chapman & Hall/CRC

3
Introductory Bayesian Statistics
Those persons who have experience with Bayesian Statistics [3, 42] can
skip this Chapter.
Bayesian Statistics quantiﬁes available prior knowledge
either using data from prior experiments or subjective beliefs from substantive
experts.
This prior knowledge is in the form of prior distributions which
quantify beliefs about various parameter values that are formally incorporated
into the inferences via Bayes’ rule.
3.1 
Discrete Scalar Variables
3.1.1
Bayes’ Rule and Two Simple Events
Bayesian Statistics is based on Bayes’ rule or conditional probability. It
is well known that the probability of events A and B both occurring can be
written as the probability of A occurring multiplied by the probability of B
occurring given that A has occurred. This is written as
P(A and B) = P(A)P(B|A)
(3.1.1)
which is the (general) rule for probability multiplication. If we rearrange the
terms then we get the formula for conditional probability
P(B|A) = P(A and B)
P(A)
(3.1.2)
which is Bayes’ rule or theorem. Pictorially this is represented in Figure 3.1
in what is called a Venn diagram.
Example:
Consider a standard deck of 52 cards.
Let A = the event of a King chosen randomly.
Let B = the event of a Heart chosen randomly.
What is the probability of selecting a Heart given we have selected a King?
P(B|A) = P (A and B)
P (A)
We use conditional probability. Then the probability of event B given that
event A has occurred is
© 2003 by Chapman & Hall/CRC

FIGURE 3.1
Pictorial representation of Bayes’ rule.
P(B|A) =
1
52
4
52
= 1
4.
3.1.2
Bayes’ Rule and the Law of Total Probability
Bayes’ rule can be extended to determine the probability of each of k events
given that event A has occurred.
The probability of one of the k events
occurring, say event Bi, given that event A has occurred is
P(Bi|A) = P(A and Bi)
P(A)
=
P(A and Bi)
k
i=1 P(A and Bi)
=
P(Bi)p(A|Bi)
k
i=1 P(Bi)p(A|Bi)
,
(3.1.3)
where the Bi’s are mutually exclusive events and
A ⊆
k
i=1
Bi.
(3.1.4)
The probabilities of the P(Bi)’s are (prior) probabilities for each of the k
events occurring.
The denominator of the above equation is called the Law of Total Proba-
bility. This version of Bayes’ rule is represented pictorially in Figure 3.2 as a
Venn diagram.
The law of total probability is deﬁned to be
P(A) =
k

i=1
P(Bi)p(A|Bi)
(3.1.5)
© 2003 by Chapman & Hall/CRC

FIGURE 3.2
Pictorial representation of the Law of Total Probability.
which states that if we sum over the probabilities of event A occurring given
that a particular event Bi has occurred multiplied by the probabilities of the
event Bi occurring, this results in the probability of event A occurring. Where
again, the events Bi are mutually exclusive and exhaustive.
Example:
To evaluate the eﬀectiveness of a medical testing procedure such as for
disease screening or illegal drug use, we will evaluate the probability of a false
negative or a false positive using the following notation
T +: The test is positive
T −: The test is negative
D+: The person has the disease
D−: The person does not have the disease.
The “sensitivity” of a test is the probability of a positive result given the
person has the disease. We will assume that a particular test has
P[T +|D+] = 0.99.
The “speciﬁcity” of the test is the probability the test is negative given the
person does not have the disease is
P[T −|D−] = 0.99.
If the proportion in the general public infected with the disease is 1 per
million or 0.000001, ﬁnd the probability of a false positive P[D−|T +].
From Bayes’ rule
P[D−|T +] = P(D−)P(T +|D−)
P(T +)
(3.1.6)
© 2003 by Chapman & Hall/CRC

and by the Law of Total Probability we get the probability of testing positive
P(T +) = P(D+)P(T +|D+)+P(D−)P(T +|D−)
= (0.000001)(0.99)+(0.999999)(0.01)
= 0.00000099+0.00999999
= 0.01000098.
The probability of a false positive or of the test for the disease giving a positive
result when the person does not in fact have the disease is
P[D−|T +] = 0.00999999
0.01000098 = 0.99990101.
3.2
Continuous Scalar Variables
Bayes’ rule also applies to continuous random variables. Let’s assume that
we have a continuous random variable x that is speciﬁed to come from a
distribution that is indexed by a parameter θ.
Prior
We can quantify our prior knowledge as to the parameter value and assess
a prior distribution
p(θ).
(3.2.1)
Likelihood
That is, the distribution (or likelihood) of the random variable x is
p(x|θ).
(3.2.2)
Posterior
We now apply Bayes’ rule to obtain the posterior distribution
p(θ|x) = p(θ)p(x|θ)
p(x)
,
(3.2.3)
where the denominator is given by
p(x) =

p(θ)p(x|θ) dθ
(3.2.4)
which is the continuous version of the discrete Law of Total Probability. Infer-
ences can be made from the posterior distribution of the model parameter θ
© 2003 by Chapman & Hall/CRC

given the data x which includes information from both the prior distribution
and the likelihood instead of only from the likelihood. From the posterior
distribution, mean and modal estimators as well as interval estimates can be
obtained for θ by integration or diﬀerentiation. This will be described later.
(It might help to make an analogy of x to A and θ to B.)
This is generalized to a random sample of size n, x1,...,xn from a distrib-
ution that depends on J parameters θ1,...,θJ.
Prior
We quantify available prior knowledge regarding the parameters in the form
of a joint prior distribution on the parameters (before taking the random
sample)
p(θ1,...,θJ),
(3.2.5)
where they are not necessarily independent.
Likelihood
With an independent sample of size n, the joint distribution of the obser-
vations is
p(x1,...,xn|θ1,...,θJ) =
n

i=1
p(xi|θ1,...,θJ).
(3.2.6)
Posterior
We apply Bayes’ rule to obtain a posterior distribution for the parameters.
The posterior distribution is
p(θ1,...,θJ|x1,...,xn) = p(θ1,...,θJ)p(x1,...,xn|θ1,...,θJ)
p(x1,...,xn)
,
(3.2.7)
where the denominator is given by
p(x1,...,xn) =

p(θ1,...,θJ)p(x1,...,xn|θ) dθ1 ...dθJ,
(3.2.8)
where θ = (θ1,...,θJ). (It might help to make an analogy of x1,...,xn to A
and θ1,...,θJ to B.)
Example:
Let’s consider a random sample of size n that is speciﬁed to come from a
population that is Normally distributed with mean µ and variance σ2. This
is denoted as xi ∼N(µ,σ2), where i = 1,...,n.
Prior
Before seeing the data, we quantify available prior knowledge regarding the
parameters µ and σ2 in the form of a joint prior distribution on the parameters
© 2003 by Chapman & Hall/CRC

p(µ,σ2),
(3.2.9)
where µ and σ2 are not necessarily independent.
Likelihood
The likelihood of all the n observations is
p(x1,...,xn|µ,σ2) = (2πσ2)−n
2 e−
Pn
i=1(xi−µ)2
2σ2
,
(3.2.10)
where x1,...,xn are the data.
Posterior
We apply Bayes’ rule to obtain a joint posterior distribution
p(µ,σ2|x1,...,xn) = p(µ,σ2)p(x1,...,xn|µ,σ2)
p(x1,...,xn)
(3.2.11)
for the mean and variance. From this joint posterior distribution which con-
tains information from the prior distribution and the likelihood, we can obtain
estimates of the parameters. This will be described later.
Bayesian statisticians usually neglect the denominator of the posterior dis-
tribution, as alluded to earlier, to get
p(θ1,...,θJ|x1,...,xn) = kp(θ1,...,θJ)p(x1,...,xn|θ1,...,θJ)
∝p(θ1,...,θJ)p(x1,...,xn|θ1,...,θJ), (3.2.12)
where “∝” denotes proportionality and the constant k, which does not depend
on the variates θ1,...,θJ, can be found by integration.
3.3
Continuous Vector Variables
We usually have several variables measured on an individual; thus, we
are rarely interested in a single random variable.
We are interested in p-
dimensional vector observations x1,...,xn where xi = (x1i,...,xpi)′ for i =
1,...,n.
The observations are speciﬁed to come from a distribution with
parameters θ1,...,θJ where the θ’s may be scalars, vectors, or matrices.
Prior
We quantify available prior knowledge (before performing the experiment
and obtaining the data) in the form of a joint prior distribution for the para-
meters
p(θ1,...,θJ),
(3.3.1)
© 2003 by Chapman & Hall/CRC

where they are not necessarily independent.
Likelihood
With an independent sample of size n, the joint distribution (likelihood)
of the observation vectors is the product of the individual distributions (like-
lihoods) and is given by
p(x1,...,xn|θ1,...,θJ) =
n

i=1
p(xi|θ1,...,θJ).
(3.3.2)
Posterior
We apply Bayes’ rule to obtain a posterior distribution for the parameters.
The posterior distribution is
p(θ1,...,θJ|x1,...,xn) = p(θ1,...,θJ)p(x1,...,xn|θ1,...,θJ)
p(x1,...,xn)
,
(3.3.3)
where the denominator is given by
p(x1,...,xn) =

p(θ1,...,θJ)p(x1,...,xn|θ) dθ1 ...dθJ,
(3.3.4)
where θ = (θ1,...,θJ). (It might help to make an analogy of x1,...,xn to A
and θ1,...,θJ to B.)
Remember we neglect the denominator to get
p(θ1,...,θJ|x1,...,xn) ∝p(θ1,...,θJ)p(x1,...,xn|θ1,...,θJ)
(3.3.5)
which states that the posterior distribution is proportional to the product of
the prior times the likelihood.
3.4
Continuous Matrix Variables
Just as we are able to observe scalar and vector valued variables, we can
also observe matrix valued variables X.
Prior
We can quantify available prior knowledge regarding the parameters with
the use of a joint prior distribution
p(θ1,...,θJ)
(3.4.1)
where the θ’s are possibly matrix valued and are not necessarily independent.
© 2003 by Chapman & Hall/CRC

Likelihood
With an independent sample of size n, from the joint distribution p(X|θ1,...,θJ)
of the observation matrices is
p(X1,...,Xn|θ1,...,θJ) =
n

i=1
p(Xi|θ1,...,θJ).
(3.4.2)
Posterior
We apply Bayes’ rule just as we have done for scalar and vector valued
variates to obtain a joint posterior distribution for the parameters. The joint
posterior distribution is
p(θ1,...,θJ|X1,...,Xn) = p(θ1,...,θJ)p(X1,...,Xn|θ1,...,θJ)
p(X1,...,Xn)
,
(3.4.3)
where the denominator is given by
p(X1,...,Xn) =

p(θ1,...,θJ)p(X1,...,Xn|θ) dθ1 ...dθJ.
(3.4.4)
(It might help to make an analogy of X1,...,Xn to A and θ1,...,θJ to B.)
Remember we neglect the denominator of the joint posterior distribution
to get
p(θ1,...,θJ|X1,...,Xn) ∝p(θ1,...,θJ)p(X1,...,Xn|θ1,...,θJ)
(3.4.5)
in which the joint posterior distribution is proportional to the product of the
prior distribution and the likelihood distribution.
From the posterior distribution, estimates of the parameters are obtained.
Estimation of the parameters is described later.
© 2003 by Chapman & Hall/CRC

Exercises
1. State Bayes’ rule for the probability of event B occurring given that
event A has occurred.
2. Assume that we select a Beta prior distribution
p(̺) ∝̺α−1(1−̺)β−1
for the probability of success in a Binomial experiment with likelihood
p(x|̺) ∝̺x(1−̺)n−x.
Write the posterior distribution p(̺|x) of ̺.
3. Assume that we select the joint prior distribution
p(µ,σ2) ∝p(µ|σ2)p(σ2),
where
p(µ|σ2) ∝(σ2)−1
2 e−(µ−µ0)2
2σ2
,
p(σ2) ∝(σ2)−ν
2 e−
q
2σ2 ,
and have a likelihood given by
p(x1,...,xn|µ,σ2) ∝(σ2)−n
2 e−Pn
i=1
(xi−¯x)2
2σ2
.
Write the joint posterior distribution p(µ,σ2|x1,...,xn) of µ and σ2.
© 2003 by Chapman & Hall/CRC

4
Prior Distributions
In this Chapter, we discuss the speciﬁcation of the form of the prior distri-
butions for our parameters which we are using to quantify our available prior
knowledge. We can specify any prior distribution that we like but our choice
is guided by the range of values of the parameters.
For example, the probability of success in a Binomial experiment has (0,1)
as its range of possible values, the mean of a Normal distribution has (−∞,+∞)
as its range of values, and the variance of a Normal distribution has (0,+∞)
as its range of values.
There are three common types of prior distributions.
1. Vague (uninformative or diﬀuse),
2. Conjugate, and
3. Generalized Conjugate.
Even though our choice is guided by the range of values for the parameters,
any distribution that is deﬁned solely in that range of values can be used.
However, the choice of Conjugate prior distributions have natural updating
properties and can simplify the estimation procedure.
Further, Conjugate
prior distributions are usually rich enough to quantify our available prior
information.
4.1
Vague Priors
4.1.1
Scalar Variates
The vague prior distribution can be placed on either a parameter that is
bounded (has a ﬁnite range of values) or unbounded (has an inﬁnite range of
values).
If a vague prior is placed on a parameter θ that has a ﬁnite range of values,
over the interval (a,b), then the prior distribution is a Uniform distribution
over (a,b) indicating that all values in this range are a priori equally likely.
That is,
© 2003 by Chapman & Hall/CRC

p(θ) =

1
b−a, if a < θ < b
0,
otherwise
,
(4.1.1)
the Uniform distribution and we write
p(θ) ∝(a constant).
(4.1.2)
A vague prior is a little diﬀerent when we place it on a parameter that is
unbounded.
Consider θ = µ to be the mean of a Normal distribution. If we wish to place
a Uniform prior on it, then we have
p(µ) =
 1
2a, if −a < µ < a
0,
otherwise,
(4.1.3)
where a →∞and again we write
p(µ) ∝(a constant).
(4.1.4)
Principle of Stable Estimation
As previously stated [41], in 1962 it was noted [10] that the posterior distrib-
ution is proportional to the product of the prior and the likelihood. Therefore,
to be vague about a parameter θ, we only have to place a Uniform prior over
the range of values for θ where the likelihood is non-negligible.
If the parameter θ = σ2 is the variance of a Normal distribution, then we
take log(σ2) to be uniform over the entire real line and by transforming back
to a distribution on σ2 we have
p(σ2) ∝1
σ2 .
(4.1.5)
A description of this can be found in [9]. Another justiﬁcation [25] is based
on a prior distribution which expresses minimal information. The vague prior
distribution in Equation 4.1.5 is an “improper” prior distribution. That is,
 ∞
0
p(σ2) dσ2
(4.1.6)
is not ﬁnite.
It should be noted that estimability problems may arise when using vague
prior distributions.
© 2003 by Chapman & Hall/CRC

4.1.2
Vector Variates
A vague prior distribution for a vector-valued mean such as for a Multivari-
ate Normal distribution is the same as that for a Scalar Normal distribution
p(µ) ∝(a constant),
(4.1.7)
where µ = (µ1,...,µp).
4.1.3
Matrix Variates
A vague prior distribution for a matrix-valued mean such as for a Matrix
Normal distribution is the same as for the scalar and vector versions
p(M) ∝(a constant),
(4.1.8)
where the matrix M is M = (µ1,...,µn)′. The rows of M are individual µ
vectors.
The generalization of the univariate vague prior distribution on a variance
to a covariance matrix is
p(Σ) ∝|Σ|−p+1
2
(4.1.9)
Which is often refered to as Jeﬀreys invariant prior distribution. Note that
this reduces to the scalar version when p = 1.
4.2
Conjugate Priors
Conjugate prior distributions are informative prior distributions. Conju-
gate prior distributions follow naturally from classical statistics. It is well
known that if a set of data were taken in two parts, then an analysis which
takes the ﬁrst part as a prior for the second part is equivalent to an analy-
sis which takes both parts together. The Conjugate prior distribution for a
parameter is of great utility and is obtained by writing down the likelihood,
interchanging the roles of the random variable and the parameter, and “en-
riching” the distribution so that it does not depend on the data set [41, 42].
The Conjugate prior distribution has the property that when combined with
the likelihood, the resulting posterior is in the same “family” of distributions.
© 2003 by Chapman & Hall/CRC

4.2.1
Scalar Variates
Beta
The number of heads x when a coin is ﬂipped n0 independent times with
y tails (n0 = x+y), follows a Binomial distribution
p(x|̺) ∝̺x(1−̺)n0−x.
(4.2.1)
We now implement the Conjugate procedure in order to obtain the prior
distribution for the probability of heads, θ = ̺. First, interchange the roles of
x and ̺
p(̺|x) ∝̺x(1−̺)n0−x,
(4.2.2)
and now “enrich” it so that it does not depend on the current data set to
obtain
p(̺) ∝̺α−1(1−̺)β−1.
(4.2.3)
This is the Beta distribution. This Conjugate procedure implies that a good
choice is to use the Beta distribution to quantify available prior information
regarding the probability of success in a Binomial experiment. The quantities
α and β are hyperparameters to be assessed. Hyperparameters are parameters
of the prior distribution.
As previously mentioned, the use of the Conjugate prior distribution has
the extra advantage that the resulting posterior distribution is in the same
family.
Example:
The prior distribution for the probability of heads when ﬂipping a certain
coin is
p(̺) ∝̺α−1(1−̺)β−1.
(4.2.4)
and the likelihood for a random sample subsequently taken is
p(x|̺) ∝̺x(1−̺)n0−x.
(4.2.5)
When these are combined to form the posterior distribution of ̺, the result
is
p(̺|x) ∝p(̺)p(x|̺)
∝̺(α+x)−1(1−̺)(β+n0−x)−1.
(4.2.6)
The prior distribution belongs to the family of Beta distributions as does
the posterior distribution. This is a feature of Conjugate prior distributions.
© 2003 by Chapman & Hall/CRC

Normal
The observation can be speciﬁed to have come from a Scalar Normal distri-
bution, x|µ,σ2 ∼N(µ,σ2) with σ2 either known or unknown. The likelihood
is
p(x|µ,σ2) ∝(σ2)−1
2 e−(x−µ)2
2σ2
,
(4.2.7)
which is often called the“kernel” of a Normal distribution.
If we interchange the roles of x and µ, then we obtain
p(µ) ∝(σ2)−1
2 e−(µ−x)2
2σ2
(4.2.8)
thus implying that we should select our prior distribution for µ from the
Normal family.
We then select as our prior distribution for µ to be
p(µ|σ2) ∝(σ2)−1
2 e−(µ−µ0)2
2σ2
,
(4.2.9)
where we have “enriched” the prior distribution with the use of µ0 so that
it does not depend on the data.
The quantity µ0 is a hyperparameter to
be assessed.
By specifying scalar quantities µ0 and σ2, the Normal prior
distribution is completely determined.
Inverted Gamma
If we interchange the roles of x and σ2 in the likelihood, then
p(σ2) ∝(σ2)−1
2 e−(x−µ)2
2σ2
(4.2.10)
thus implying that we should select our prior distribution for σ2 from the
Inverted Gamma family.
We then select as our prior distribution for σ2
p(σ2) ∝(σ2)−ν
2 e−
q
2σ2 ,
(4.2.11)
where we have “enriched” the prior distribution with the use of q so that
it does not depend on the data. The quantity q is a hyperparameter to be
assessed. By specifying scalar quantities q and ν, the Inverted Gamma prior
distribution is completely determined.
Using the Conjugate procedure to obtain prior distributions, we obtain
Table 4.1.
4.2.2
Vector Variates
Normal
The observation can be speciﬁed to have come from a Multivariate or Vector
© 2003 by Chapman & Hall/CRC

TABLE 4.1
Scalar variate Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Scalar Binomial
p
Beta
Scalar Normal σ2 known
µ
Normal
Scalar Normal µ
known
σ2
Inverted Gamma
Scalar Normal
(µ,σ2)
Normal-Inverted Gamma
Normal distribution, x|µ,Σ ∼N(µ,Σ) with Σ either known or unknown. The
likelihood is
p(x|µ,Σ) ∝|Σ|−1
2 e−1
2 (x−µ)′Σ−1(x−µ).
(4.2.12)
If we interchange the roles of x and µ, then
p(µ) ∝|Σ|−1
2 e−1
2 (µ−x)′Σ−1(µ−x)
(4.2.13)
thus implying that we should select our prior distribution for µ from the
Normal family.
We then select as our prior distribution for µ to be
p(µ|Σ) ∝|Σ|−1
2 e−1
2 (µ−µ0)′Σ−1(µ−µ0),
(4.2.14)
where we have “enriched” the prior distribution with the use of µ0 so that it
does not depend on the data. The vector quantity µ0 is a hyperparameter to
be assessed. By specifying the vector quantity µ0 and the matrix quantity Σ,
the Vector or Multivariate Normal prior distribution is completely determined.
Inverted Wishart
If we interchange the roles of x and Σ in the Vector or Multivariate Normal
likelihood and use the property of the trace operator, then
p(Σ) ∝|Σ|−1
2 e−1
2 trΣ−1(x−µ)(x−µ)′
(4.2.15)
thus implying that we should select our prior distribution for Σ from the
Inverted Wishart family.
We then select as our prior distribution for Σ
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(4.2.16)
where we have “enriched” the prior distribution with the use of Q and ν so that
it does not depend on the data. The quantities ν and Q are hyperparameters
to be assessed. By specifying the matrix quantity Q and the scalar quantity
ν, the Inverted Wishart prior distribution is completely determined.
Using the Conjugate procedure to obtain prior distributions, we obtain
Table 4.2 where “IW” is used to denote Inverted Wishart.
© 2003 by Chapman & Hall/CRC

TABLE 4.2
Vector variate Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Multivariate Normal Σ known
µ
Multivariate Normal
Multivariate Normal µ
known
Σ
Inverted Wishart
Multivariate Normal
(µ,Σ)
Normal-IW
4.2.3
Matrix Variates
Normal
The observation can be speciﬁed to have come from a Matrix Normal Dis-
tribution, X|M,Φ,Σ ∼N(M,Φ⊗Σ) with Φ and Σ either known or unknown.
The Matrix Normal likelihood is
p(X|M,Σ,Φ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′.
(4.2.17)
If we interchange the roles of X and M, then
p(M) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(M−X)Σ−1(M−X)′
(4.2.18)
thus implying that we should select our prior distribution for M from the
Matrix Normal family.
We then select as our prior distribution for M
p(M|Σ,Φ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(M−M0)Σ−1(M−M0)′,
(4.2.19)
where we have “enriched” the prior distribution with the use of M0 so that
it does not depend on the data. The quantity M0 is a hyperparameter to be
assessed. By specifying the matrix quantity M0 and the matrix quantities Φ
and Σ, the Matrix Normal prior distribution is completely determined.
Inverted Wishart
If we interchange the roles of X and Σ in the Matrix Normal likelihood
and use the property of the trace operator, then
p(Σ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΣ−1(X−M)′Φ−1(X−M)
(4.2.20)
thus implying that we should select our prior distribution for Σ from the
Inverted Wishart family.
We then select as our prior distribution for Σ
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(4.2.21)
where we have “enriched” the prior distribution with the use of Q and ν so that
it does not depend on the data. The quantities ν and Q are hyperparameters
to be assessed. By specifying the matrix quantity Q and the scalar quantity
ν, the Inverted Wishart prior distribution is completely determined.
© 2003 by Chapman & Hall/CRC

Taking the same Matrix Normal likelihood, and interchanging the role for
Φ,
p(Φ|X,M,Σ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′
(4.2.22)
thus implying that we should select our prior distribution for Σ from the
Inverted Wishart family
p(Φ|κ,Ψ) ∝|Φ|−η
2 e−1
2 trΦ−1Ψ,
(4.2.23)
where we have “enriched” the prior distribution with the use of Ψ and κ so that
it does not depend on the data. The quantities κ and Ψ are hyperparameters
to be assessed. By specifying the matrix quantity Ψ and the scalar quantity
κ, the Inverted Wishart prior distribution is completely determined.
Using the Conjugate procedure to obtain prior distributions, we obtain
Table 4.3 where “IW” is used to denote Inverted Wishart.
TABLE 4.3
Matrix variate Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Normal (Φ,Σ) known
M
Matrix Normal
Matrix Normal (M,Φ) known
Σ
Inverted Wishart
Matrix Normal (M,Σ) known
Φ
Inverted Wishart
Matrix Normal
(M,Φ,Σ)
Normal-IW-IW
4.3
Generalized Priors
At times, Conjugate prior distributions are not suﬃcient to quantify the
prior knowledge we have about the parameter values [49]. When this is the
case, generalized Conjugate prior distributions can be used. Generalized Con-
jugate prior distributions are found by writing down the likelihood, inter-
changing the roles of the random variable and the parameter, “enriching” the
distribution so that it does not depend on the data set, and assuming that
the priors on each of the parameters are independent [41].
4.3.1
Scalar Variates
Normal
The observation can be speciﬁed to have come from a Scalar Normal dis-
© 2003 by Chapman & Hall/CRC

tribution, x|µ,σ2 ∼N(µ,σ2) with σ2 either known or unknown. The Normal
likelihood is given by
p(x|µ,σ2) ∝(σ2)−1
2 e−(x−µ)2
2σ2
.
(4.3.1)
If we interchange the roles of x and µ, then we obtain
p(µ) ∝(σ2)−1
2 e−(µ−x)2
2σ2
(4.3.2)
thus implying that we should select our prior distribution for µ from the
Normal family.
We then select as our prior distribution for µ
p(µ) ∝(δ2)−1
2 e−(µ−µ0)2
2δ2
,
(4.3.3)
where we have “enriched” the prior distribution with the use of µ0 so that it
does not depend on the data and made it independent of the other parameter
σ2 through δ2. The quantities µ0 and δ2 are hyperparameters to be assessed.
By specifying scalar quantities µ0 and δ2, the Normal prior distribution is
completely determined.
Inverted Gamma
If we interchange the roles of x and σ2 in the Normal likelihood then
p(σ2) ∝(σ2)−1
2 e−(x−µ)2
2σ2
(4.3.4)
thus implying that we should select our prior distribution for σ2 from the
Inverted Gamma family.
We then select our prior distribution for σ2 to be
p(σ2) ∝(σ2)−ν
2 e−
q
2σ2 ,
(4.3.5)
where we have “enriched” the prior distribution with the use of q so that it
does not depend on the data. The quantities q and ν are hyperparameters to
be assessed. By specifying the scalar quantities q and ν, the Inverted Gamma
prior distribution is completely determined. The generalized Conjugate pro-
cedure yields the same prior distribution for the variance σ2 as the Conjugate
procedure.
Using the generalized Conjugate procedure to obtain prior distributions, we
obtain Table 4.4 where “IG” is used to denote Inverted Gamma.
4.3.2
Vector Variates
Normal
The observation can be speciﬁed to have come from a Multivariate or vector
© 2003 by Chapman & Hall/CRC

TABLE 4.4
Scalar variate generalized Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Scalar Normal σ2 known
µ
Generalized Normal
Scalar Normal µ
known
σ2
Inverted Gamma
Scalar Normal
(µ,σ2)
Generalized Normal-IG
Normal distribution, x|µ,Σ ∼N(µ,Σ) with Σ either known or unknown. The
Multivariate Normal likelihood is
p(x|µ,Σ) ∝|Σ|−1
2 e−1
2 (x−µ)′Σ−1(x−µ).
(4.3.6)
If we interchange the roles of x and µ in the likelihood, then
p(µ) ∝|Σ|−1
2 e−1
2 (µ−x)′Σ−1(µ−x)
(4.3.7)
thus implying that we should select our prior distribution for µ from the
Normal family.
We then select as our prior distribution for µ
p(µ) ∝|∆|−1
2 e−1
2 (µ−µ0)′∆−1(µ−µ0),
(4.3.8)
where we have “enriched” the prior distribution with the use of µ0 so that it
does not depend on the data and made it independent of the other parameter Σ
through ∆. The quantities µ0 and ∆are hyperparameters to be assessed. By
specifying the vector quantity µ0 and the matrix quantity ∆, the Multivariate
Normal prior distribution is completely determined.
Inverted Wishart
If we interchange the roles of x and Σ and use the property of the trace
operator, then
p(Σ) ∝|Σ|−1
2 e−1
2 trΣ−1(x−µ)(x−µ)′
(4.3.9)
thus implying that we should select our prior distribution for Σ from the
Inverted Wishart family.
We then select as our prior distribution for Σ
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(4.3.10)
where we have “enriched” the prior distribution with the use of Q and ν so that
it does not depend on the data. The quantities ν and Q are hyperparameters
to be assessed. By specifying the matrix quantity Q and the scalar quantity
ν, the Inverted Wishart prior distribution is completely determined.
The
generalized Conjugate procedure yields the same prior distribution for Σ as
the Conjugate procedure.
© 2003 by Chapman & Hall/CRC

Using the generalized Conjugate procedure to obtain prior distributions, we
obtain Table 4.5 where “IW” is used to denote Inverted Wishart and “GMN,”
Generalized Matrix Normal.
TABLE 4.5
Vector variate generalized Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Multivariate Normal Σ known
µ
GMN
Multivariate Normal µ
known
Σ
Inverted Wishart
Multivariate Normal
(µ,Σ)
GMN-IW
4.3.3
Matrix Variates
Normal
The observation can be speciﬁed to have come from a Matrix Normal Dis-
tribution, X|M,Φ,Σ ∼N(M,Φ⊗Σ) with Φ and Σ either known or unknown.
The Matrix Normal likelihood is
p(X|M,Σ,Φ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′.
(4.3.11)
If we interchange the roles of X and M, then
p(M) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(M−X)Σ−1(M−X)′
(4.3.12)
thus implying that we should select our prior distribution for M from the
Matrix Normal family.
We then select our prior distribution for M to be the Matrix Normal dis-
tribution
p(M) ∝|χ|−p
2 |Ξ|−n
2 e−1
2 trχ−1(M−M0)Ξ−1(M−M0)′,
(4.3.13)
where we have “enriched” the prior distribution with the use of M0 so that
it does not depend on the data X and made it independent of the other
parameters Φ and Σ through Ξ and χ. The quantities M0, Ξ, and χ are hy-
perparameters to be assessed. By specifying the matrix quantity M0 and the
matrix quantities Ξ and χ, the Matrix Normal prior distribution is completely
determined.
Inverted Wishart
If we interchange the roles of X and Σ in the Matrix Normal likelihood
and use the property of the trace operator, then
p(Σ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΣ−1(X−M)′Φ−1(X−M)
(4.3.14)
© 2003 by Chapman & Hall/CRC

thus implying that we should select our prior distribution for Σ from the
Inverted Wishart family.
We then select our prior distribution for Σ to be the Inverted Wishart
distribution
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(4.3.15)
where we have “enriched” the prior distribution with the use of Q and ν so that
it does not depend on the data. The quantities ν and Q are hyperparameters
to be assessed. By specifying the matrix quantity Q and the scalar quantity
ν, the Inverted Wishart prior distribution is completely determined.
Taking the same Matrix Normal likelihood, and interchanging the role for
Φ, we have
p(Φ|X,M,Σ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 Φ−1(X−M)trΣ−1(X−M)′
(4.3.16)
thus implying that we should select our prior distribution for Φ from the
Inverted Wishart family
p(Φ|κ,Ψ) ∝|Φ|−κ
2 e−1
2 trΦ−1Ψ,
(4.3.17)
where we have “enriched” the prior distribution with the use of Ψ and κ so that
it does not depend on the data. The quantities κ and Ψ are hyperparameters
to be assessed. By specifying the matrix quantity Ψ and the scalar quantity
κ, the Inverted Wishart prior distribution is completely determined.
Using the generalized Conjugate procedure to obtain prior distributions, we
obtain Table 4.6 where “IW” is used to denote Inverted Wishart and “GMN,”
Generalized Matrix Normal.
TABLE 4.6
Matrix variate generalized Conjugate priors.
Likelihood
Parameter(s)
Prior Family
Matrix Normal (Φ,Σ) known
M
GMN
Matrix Normal (M,Φ) known
Σ
Inverted Wishart
Matrix Normal (M,Σ) known
Φ
Inverted Wishart
Matrix Normal
(M,Φ,Σ)
GMN-IW-IW
4.4
Correlation Priors
In this section, Conjugate prior distributions are derived for the correlation
coeﬃcient between observation vectors.
In the context of Bayesian Factor
© 2003 by Chapman & Hall/CRC

Analysis, a Generalized Beta distribution has been used for the correlation
coeﬃcient ρ in the between vector correlation matrix Φ [50]. This was done
when Φ was either the intraclass correlation matrix
Φ =







1 ρ ρ ··· ρ
1 ρ ··· ρ
...
...
ρ
1







= (1−ρ)In +ρene′
n,
(4.4.1)
where en is a column vector of ones and −
1
n−1 < ρ < 1 or the ﬁrst order
Markov correlation matrix
Φ =





1
ρ
ρ2 ··· ρn−1
ρ
1
ρ ··· ρn−2
...
...
...
...
ρn−1 ρn−2
···
1




,
(4.4.2)
where 0 < |ρ| < 1.
When the Generalized Beta prior distribution and the likelihood are com-
bined, the result is a posterior distribution which is unfamiliar. This unfamil-
iar posterior distribution required a rejection sampling technique to generate
random variates as outlined in Chapter 6. A Conjugate prior distribution can
be derived and the rejection sampling avoided.
Given X which follows a Matrix Normal distribution
p(X|M,Σ,Φ) = (2π)−np
2 |Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′,
(4.4.3)
where x = (X′) = (x′
1,...,x′
n)′, X′ = (x1,...,xn), µ = vec(M ′) = (µ′
1,...,µ′
n)′,
M ′ = (µ1,...,µn), and Φ is either an intraclass or ﬁrst order Markov correlation
matrix; the Conjugate prior distribution for ρ in Φ is found as follows.
4.4.1
Intraclass
If we determine the intraclass structure in Equation 4.4.1 that has the
correlation between any two observations being the same, then we can use the
result that the determinant of Φ has the form
|Φ| = (1−ρ)n−1[1+ρ(n−1)]
(4.4.4)
and the result that the inverse of Φ has the form
Φ−1 =
In
1−ρ −
ρene′
n
(1−ρ)[1+(n−1)ρ]
(4.4.5)
which is again a matrix with intraclass correlation structure [41].
© 2003 by Chapman & Hall/CRC

With these results, the Matrix Normal distribution can be written as
p(X|M,Σ,Φ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′
∝|Φ|−p
2 e−1
2 trΦ−1Ψ
p(X|M,Σ,ρ) ∝(1−ρ)−(n−1)p
2
[1+(n−1)ρ]−p
2
×e
−
1
2(1−ρ)
h
k1−
ρk2
1+(n−1)ρ
i
,
(4.4.6)
where k1 = tr(Ψ), k2 = tr(ene′
nΨ), and Ψ = (X −M)Σ−1(X −M)′. It should
be noted that k1 and k2 can be written as
k1 =
n

i=1
Ψii
and
k2 =
n

i′=1
n

i=1
Ψii′.
(4.4.7)
We now implement the Conjugate procedure in order to obtain the prior
distribution for the between vector correlation coeﬃcient ρ. If we interchange
the roles of X and ρ, then we obtain
p(ρ) ∝(1−ρ)−αβ
2 [1+αρ]−β
2 e
−
1
2(1−ρ)
h
k1−ρk2
1+αρ
i
,
(4.4.8)
where α, β, and Ψ for k1 = tr(Ψ), k2 = tr(ene′
nΨ) are hyperparameters to be
assessed. With appropriate choices of α and β, for example, α = n −1 and
β = p, this prior distribution is Conjugate for ρ.
4.4.2
Markov
If we determine the ﬁrst order Markov structure in Equation 4.4.2 that has
the correlation between observations decrease with the power of the diﬀerence
between the observation numbers, then we can use the results [41] that the
determinant of Φ has the form
|Φ| = (1−ρ2)n−1
(4.4.9)
and that the inverse of such a patterned matrix has the form
Φ−1 =
1
1−ρ2







1
−ρ
0
−ρ (1+ρ2) −ρ
...
...
...
(1+ρ2) −ρ
0
−ρ
1







.
(4.4.10)
With these results, the Matrix Normal distribution can be written as
© 2003 by Chapman & Hall/CRC

p(X|M,Σ,Φ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−M)Σ−1(X−M)′,
∝|Φ|−p
2 e−1
2 trΦ−1Ψ
p(X|M,Σ,ρ) ∝(1−ρ2)−(n−1)p
2
e
−k1−ρk2+ρ2k3
2(1−ρ2)
,
(4.4.11)
where
Ψ1 = In,
Ψ2 =







0 1
0
1 0
1
... ... ...
0 1
0
1 0







,
Ψ3 =







0
0
1
...
1
0
0







,
(4.4.12)
k1 = tr(Ψ1Φ) =
n

i=1
Ψii,
(4.4.13)
k2 = tr(Φ2Ψ) =
n−1

i=1
(Ψi,i+1 +Ψi+1,i),
(4.4.14)
and
k3 = tr(Φ3Ψ) =
n−1

i=2
Ψii.
(4.4.15)
We now implement the Conjugate procedure in order to obtain the prior
distribution for the between vector correlation coeﬃcient ρ. If we interchange
the roles of X and ρ, then we obtain
p(ρ) ∝(1−ρ2)−αβ
2 e
−k1−ρk2+ρ2k3
2(1−ρ2)
,
(4.4.16)
where α, β, and Ψ for k1, k2, and k3 deﬁned above are hyperparameters to
be assessed. With appropriate choices of α and β, for example, α = n−1 and
β = p, this prior distribution is Conjugate for ρ.
The vague priors are used when there is little or no speciﬁc knowledge as to
various parameter values. The Conjugate prior distributions are used when
we have speciﬁc knowledge as to parameter values either in the form of a pre-
vious similar experiment or from substantive expert beliefs. The generalized
© 2003 by Chapman & Hall/CRC

Conjugate prior distributions are used in the rare situation where we believe
that the Conjugate prior distributions are too restrictive to correctly assess
prior information. Vague prior distributions are not used in this text because
they may lead to nonunique solutions for every model except for the Bayesian
Regression model.
© 2003 by Chapman & Hall/CRC

Exercises
1. What is the family of Conjugate prior distributions for ̺ corresponding
to a Scalar Binomial likelihood p(x|̺)?
2. What are the families of Conjugate prior distributions for µ and σ2
corresponding to a Scalar Normal likelihood p(x|µ,σ2)?
3. What are the families of Conjugate prior distributions for µ and Σ cor-
responding to a Multivariate Normal likelihood p(x|µ,Σ)?
© 2003 by Chapman & Hall/CRC

5
Hyperparameter Assessment
5.1
Introduction
This Chapter describes methods for assessing the hyperparameters for prior
distributions used to quantify available prior knowledge regarding parameter
values. When observed data arise from Binomial, Scalar Normal, Multivariate
Normal, and Matrix Normal distributions as in this text, the prior distribu-
tions of the parameters of these distributions contain parameters themselves
termed hyperparameters. These prior distributions are quite often the Scalar
Beta, Scalar Normal, Multivariate Normal, Matrix Normal, Inverted Gamma,
and Inverted Wishart distributions. The hyperparameters of these prior dis-
tributions need to be assessed so that the prior distribution can be identiﬁed.
There are two ways the hyperparameters can be assessed, either in a pure sub-
jective way which expresses expert knowledge and beliefs or by use of data
from a previous similar experiment.
Throughout this chapter, we will be in the predata acquisition stage of an
experiment. We will quantify available prior knowledge regarding values of
parameters of the model which is speciﬁed with a likelihood. We will quantify
how likely the values of the parameters in the likelihood are, prior to seeing
any current data. This can be accomplished by using data from a previous
similar experiment or by using subjective expert opinion in the form of a
virtual set of data.
5.2
Binomial Likelihood
Before performing a Binomial experiment and gathering data, we have
foresight in knowing that a similar experiment has been carried out and data
exist in the form of n0 observations x1,...,xn0. The likelihood of these n0
random variates is
p(x1,...,xn0|̺) ∝̺
Pn0
i=1 xi(1−̺)n0−Pn0
i=1 xi
© 2003 by Chapman & Hall/CRC

∝̺x(1−̺)n0−x,
(5.2.1)
where the new variable x = n0
i=1 xi is the number of successes (heads) and
the new variable y = n0 −x is the number of failures (tails). With the number
of successes x in n0 Bernoulli trials known, we now view ̺, the probability of
success, as the random variable with known parameters x and n0.
It can be recognized that the random variable ̺ is (Scalar) Beta distributed.
5.2.1
Scalar Beta
The probability of success ̺ has the Beta distribution
p(̺) ∝̺x(1−̺)n0−x,
(5.2.2)
which is compared to its typical parameterization
p(̺) ∝̺α−1(1−̺)β−1
(5.2.3)
and it is seen that the hyperparameters α and β of the prior distribution for
the probability of success ̺ are α = x+1 and β = y +1.
The scalar hyperparameters α and β can also be assessed by purely sub-
jective means. A substantive ﬁeld expert can assess them in the following
way.
Imagine that we are not able to visually inspect and have not observed any
realizations from the coin being ﬂipped. As described in [50], if we imagine a
virtual ﬂipping of the coin n0 times, then let x = α−1 denote the number of
virtual heads and y = β −1 the number of virtual tails such that x+y = n0.
If α = β = 1, then n0 = 0, implying the absence of virtual coin ﬂipping or the
absence of speciﬁc prior information. This corresponds to a vague or Uniform
prior distribution with mean 1
2. The parameter values α = 200 and β = 100
imply 199 heads and 99 tails which is strong prior information with a mean
of 2
3. The larger the virtual sample size n0, the stronger the prior information
we have and the more peaked the prior is around its mean.
5.3
Scalar Normal Likelihood
Before performing an experiment in which Scalar Normal random variates
will result, we have foresight in knowing that a similar experiment has been
carried out and data exist in the form of n0 observations x1,...,xn0. The
likelihood of these n0 random variates is
p(x1,...,xn0|µ,σ2) ∝(σ2)−n0
2 e
−Pn0
i=1(xi−µ)2
2σ2
.
(5.3.1)
© 2003 by Chapman & Hall/CRC

With the numerical values of the observations x1,...,xn0 known, we now view
the parameters µ and σ2 of this distribution as being the random variables
with known parameters involving n0 and x1,...,xn0.
By rearranging and performing some algebra on the above distribution, it
can be seen that µ and σ2 are Scalar Normal and Inverted Gamma distributed.
5.3.1
Scalar Normal
The random parameter µ from a sample of Scalar Normal random variates
has a Scalar Normal distribution
p(µ|σ2) ∝(σ2)−n0
2 e
−(µ−¯x)2
2σ2/n0
(5.3.2)
which is compared to its typical parameterization (where σ is used generically)
p(µ|σ2) ∝(σ2)−n0
2 e−(µ−µ0)2
2σ2
(5.3.3)
and it is seen that the hyperparameter µ0 of the prior distribution for the
mean is µ0 = ¯x.
5.3.2
Inverted Gamma or Scalar Inverted Wishart
The random parameter σ2 from a sample of Scalar Normal random variates
has an Inverted Gamma distribution
p(σ2) ∝(σ2)−n0
2 e−1
2
Pn0
i=1(xi−¯x)2
σ2
(5.3.4)
which is compared to its typical parameterization
p(σ2) ∝(σ2)−ν
2 e−1
2
q
σ2
(5.3.5)
and it is seen that the hyperparameters ν and q of the Inverted Gamma
distribution given in Chapter 2, which here is a prior distribution for the
variance σ2, are ν = n0 and q = n0
i=1(xi −¯x)2.
The scalar hyperparameters µ0, ν, and q can also be assessed by purely
subjective means. A substantive ﬁeld expert can assess them in the following
way.
If we imagine a virtual sample of size n0, x1,...,xn0, then a substantive
expert can determine a value of the mean of the sample data µ0 = ¯x which
would represent the most probable value to be the average (also a value he
would expect since the mean and mode of the Scalar Normal distribution are
identical). The substantive expert can also determine the most probable value
for the variance of this virtual sample σ2
0 and the hyperparameters ν and q
are ν = n0 and q = n0σ2
0.
© 2003 by Chapman & Hall/CRC

5.4
Multivariate Normal Likelihood
Before performing an experiment in which Multivariate Normal random
variates will result, we have foresight in knowing that a similar experiment
has been carried out and data exist in the form of n0 vector valued observations
x1,...,xn0. The likelihood of these n0 random variates is
p(x1,...,xn0|µ,Σ) ∝|Σ|−n0
2 e−1
2
Pn0
i=1(xi−µ)′Σ−1(xi−µ)
(5.4.1)
With the numerical values of the observations x1,...,xn0 known, we now view
the parameters µ and Σ of this distribution as being the random variables with
known parameters involving n0 and x1,...,xn0.
By rearranging and performing some algebra on the above distribution,
it can be seen that µ and Σ are Multivariate Normal and Inverted Wishart
distributed.
5.4.1
Multivariate Normal
The random parameter µ from a sample of Multivariate Normal random
variates of dimension p has a Multivariate Normal distribution
p(µ|Σ) ∝|Σ|−n0
2 e−1
2 (µ−¯x)′(Σ/n0)−1(µ−¯x)
(5.4.2)
which is compared to its typical parameterization (where Σ is used generically)
p(µ|Σ) ∝|Σ|−n0
2 e−1
2 (µ−µ0)′Σ−1(µ−µ0)
(5.4.3)
and it is seen that the hyperparameter µ0 of the prior distribution for the
mean is µ0 = ¯x.
5.4.2
Inverted Wishart
The random parameter Σ from a sample of Multivariate Normal random
variates has an Inverted Wishart distribution
p(Σ) ∝|Σ|−n0
2 e−1
2 trΣ−1 Pn0
i=1(xi−¯x)(xi−¯x)′
(5.4.4)
which is compared to its typical parameterization
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q
(5.4.5)
and it is seen that the hyperparameters ν and Q of the prior distribution for
the covariance matrix Σ are ν = n0 and Q = n0
i=1(xi −¯x)(xi −¯x)′.
© 2003 by Chapman & Hall/CRC

The vector, scalar, and matrix hyperparameters µ0, ν, and Q can also be
assessed by purely subjective means. A substantive ﬁeld expert can assess
them in the following way.
If we imagine a virtual sample of size n0, x1,...,xn0, then a substantive ex-
pert can determine a value of the mean of the sample data µ0 = ¯x which would
represent the most probable value to be the average (also a value he would
expect since the mean and mode of the Multivariate Normal distribution are
identical). The substantive expert can also determine the most probable value
for the covariance matrix of this virtual sample Σ0 and the hyperparameters
ν and Q are ν = n0 and Q = n0Σ0.
5.5
Matrix Normal Likelihood
Before performing an experiment in which Matrix Normal random vari-
ates will result, we have foresight in knowing that a similar experiment has
been carried out and data exist in the form of n0 matrix valued observations
X1,...,Xn0 of dimension n1 by p1. The likelihood of these n0 random variates
is
p(X1,...,Xn0|M,Σ,Φ) ∝|Σ|−n0n1
2
|Φ|−n0p1
2
e−1
2
Pn0
i=1 trΦ−1(Xi−M)Σ−1(Xi−M)′.
(5.5.1)
With the numerical values of the observations X1,...,Xn0 known, we now
view the parameters M, Σ, and Φ of this distribution as being the random
variables with known parameters involving n0 and X1,...,Xn0.
By rearranging and performing some algebra on the above distribution, it
can be seen that M, Σ, and Φ are Matrix Normal, Inverted Wishart, and
Inverted Wishart distributed.
5.5.1
Matrix Normal
The random parameter M from a sample of Matrix Normal random variates
has a Matrix Normal distribution
p(M|Σ,Φ) ∝e−1
2 trΦ−1(M−¯
X)(Σ/n0)−1(M−¯
X)′
(5.5.2)
which is compared to its typical parameterization (where Σ is used generically)
p(M|Σ,Φ) ∝e−1
2 trΦ−1(M−M0)Σ−1(M−M0)′
(5.5.3)
and it is seen that the hyperparameter M0 of the prior distribution for the
mean is M0 = ¯X.
© 2003 by Chapman & Hall/CRC

5.5.2
Inverted Wishart
The random parameter Σ from a sample of Matrix Normal random variates
has an Inverted Wishart distribution
p(Σ) ∝|Σ|−n0n1
2
|Φ|−n0p0
1
e−1
2 trΣ−1 Pn0
i=1(Xi−¯
X)′Φ−1(Xi−¯
X)
(5.5.4)
which is compared to its typical parameterization
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q
(5.5.5)
and it is seen that the hyperparameters ν and Q of the prior distribution for
the covariance matrix Σ are ν = n0n1 and Q = n0
i=1(Xi −¯X)′Φ−1(Xi −¯X).
Similarly, the random parameter Φ from a sample of Matrix Normal random
variates has an Inverted Wishart distribution
p(Φ) ∝|Σ|−n0n1
2
|Φ|−n0p0
1
e−1
2 trΦ−1 Pn0
i=1(Xi−¯
X)Σ−1(Xi−¯
X)′
(5.5.6)
which is compared to its typical parameterization
p(Φ) ∝|Φ|−κ
2 e−1
2 trΦ−1Ψ
(5.5.7)
and it is seen that the hyperparameters κ and Ψ of the prior distribution for
the covariance matrix Φ are κ = n0p1 and Ψ = n0
i=1(Xi −¯X)Σ−1(Xi −¯X)′.
Note that the equations for Q and Ψ are coupled. This means that there
is not a closed form analytic solution for estimating Φ and Σ. Their values
must be computed in an iterative fashion with an initial value similar to the
ICM algorithm which will be presented in Chapter 6.
The matrix, scalar, matrix, scalar, and matrix hyperparameters M0, ν, Q,
κ, Ψ, can also be assessed by purely subjective means. A substantive ﬁeld
expert can assess them in the following way.
If we imagine a virtual sample of size n0, X1,...,Xn0, then a substantive
expert can determine a value of the mean of the sample data M0 = ¯X which
would represent the most probable value to be the average (also a value he
would expect since the mean and mode of the Matrix Normal distribution are
identical). The substantive expert can also determine the most probable value
for the covariance matrix of this virtual sample Σ0 and the hyperparameters ν
and Q are ν = n0n1 and Q = n0Σ0. The substantive expert can also determine
the most probable value for the covariance matrix of this virtual sample Φ0
and the hyperparameters κ and Ψ are κ = n0p1 and Ψ = n0Φ0.
© 2003 by Chapman & Hall/CRC

Exercises
1. Assume that we have n0 = 25 (virtual or actual data) observations from
a Binomial experiment with x = 15 successes. What values would you
assess for the hyperparameters α and β for a Conjugate Beta prior
distribution on ̺, the probability of success?
2. Assume that we have n0 = 30 (virtual or actual data) observations for
a Scalar Normal variate with sample mean and sample sum of square
deviates given by
¯x = 50,
30

i=1
(xi −¯x)2 = 132.
What value would you assess for the hyperparameter µ0 of a Conjugate
Scalar Normal prior distribution for the mean µ and what hyperpara-
meters ν and q of the Conjugate Inverse Gamma prior distribution for
the variance σ2?
3. Assume that we have n0 = 50 (virtual or actual data) observations for a
Multivariate Normal variate with sample mean vector and sample sum
of square deviates matrix given by
¯x =


50
100
75

,
30

i=1
(xi −¯x)(xi −¯x)′ =


50.000 12.500 3.125
12.500 50.000 12.500
3.125 12.500 50.000

.
What value would you assess for the vector hyperparameter µ0 of a
Conjugate Multivariate Normal prior distribution for the mean vector µ
and what scalar and matrix hyperparameters ν and Q of the Conjugate
Inverse Gamma prior distribution for the covariance matrix Σ?
© 2003 by Chapman & Hall/CRC

6
Bayesian Estimation Methods
In this Chapter we deﬁne the two methods of parameter estimation which
are used in this text, namely, marginal posterior mean and joint maximum a
posteriori estimators. Typically these estimators are found by integration and
diﬀerentiation to arrive at explicit equations for their computation. There are
often instances where explicit closed form equations are not possible. In these
instances, numerical integration and maximization estimation procedures are
required. The typical explicit integration and diﬀerentiation procedures are
discussed as are the numerical estimation procedures used. The numerical
estimation procedures are Gibbs sampling for sampling based marginal pos-
terior means and the iterated conditional modes algorithm (ICM) for joint
maximum posterior (joint posterior modal) estimates.
6.1
Marginal Posterior Mean
Often we have a set of parameters, θ = (θ1,...,θJ) in our posterior distrib-
ution p(θ|X) where X represents the data which may be a collection of scalar,
vector, or matrix observations. The marginal posterior distribution of any of
the parameters, say θj, can be obtained by integrating p(θ|X) with respect to
all parameters except θj. That is, the marginal posterior distribution of θj is
p(θj|X) =

p(θ1,...,θJ|X) dθ1 ...dθj−1 dθj+1 ...dθJ
(6.1.1)
where the integral is evaluated over the appropriate range of the set para-
meters. After calculating the marginal posterior distribution for each of the
parameters, marginal posterior estimators such as
ˆθj = E(θj|X) =

θjp(θj|X)dθj
(6.1.2)
can be calculated which is the marginal mean estimator.
© 2003 by Chapman & Hall/CRC

6.1.1
Matrix Integration
When computing marginal distributions [42], joint posterior distributions
are integrated with respect to scalar, vector, or matrix variates. Let’s consider
the variates to be of the matrix form and perform integration. Scalar and
vector analogs follow as special cases. Integration of a posterior distribution
with respect to a matrix variate is typically carried out ﬁrst by algebraic
manipulation of the integrand and ﬁnally by recognition.
To motivate the integration with respect to matrices, consider the problem
of estimating the p dimensional mean vector µ and covariance matrix Σ from
a Multivariate Normal distribution p(x|µ,Σ). Available prior knowledge re-
garding the mean vector and covariance matrix is quantiﬁed in the form of the
joint (Conjugate) prior distribution p(µ,Σ) and a random sample x1,...,xn is
taken with likelihood
p(x1,...,xn|µ,Σ) =
n

i=1
p(xi|µ,Σ).
(6.1.3)
The posterior distribution is given by
p(µ,Σ|X) ∝p(µ,Σ)p(X|µ,Σ)
(6.1.4)
and marginal posterior distributions
p(µ|X) =

p(µ,Σ)p(X|µ,Σ) dΣ,
(6.1.5)
p(Σ|X) =

p(µ,Σ)p(X|µ,Σ) dµ,
(6.1.6)
where the random sample denoted by X′ = (x1,...,xn). The ﬁrst integral is
taken over the set of all p-dimensional positive deﬁnite symmetric matrices
and the second over p-dimensional real space.
In general, if we were presented with a joint posterior distribution p(θ|X)
that was a function of θ = (θ1,θ2), the marginal posterior distribution of θ1 is
found by integration with respect to θ2 as
p(θ1|X) =

p(θ|X) dθ2.
(6.1.7)
This integration is often carried out by algebraically manipulating the terms
in p(θ|X) to write it as
p(θ|X) = g(θ1|X)h(θ2|θ1,X),
(6.1.8)
where h(θ2|θ1,X) is recognized as being a known distribution except for a
multiplicative normalizing constant with respect to θ2.
The multiplicative
© 2003 by Chapman & Hall/CRC

normalizing constant k(θ1|X) can depend on the parameter θ1 and the data
X but not on θ2. The posterior distribution is such that
p(θ|X) = g(θ1|X)
k(θ1|X)k(θ1|X)h(θ2|θ1,X)
(6.1.9)
and the integration is carried out by taking those terms that do not depend
on θ2 out of the integrand and then recognizing that the integrand is unity.
Mathematically this procedure is described as
p(θ1|X) =
 g(θ1|X)
k(θ1|X)k(θ1|X)h(θ2|θ1,X) dθ2
(6.1.10)
= g(θ1|X)
k(θ1|X)

k(θ1|X)h(θ2|θ1,X) dθ2
(6.1.11)
= g(θ1|X)
k(θ1|X).
(6.1.12)
The integral will be the integral of a probability distribution function that
we recognize as unity.
Integration is similarly performed when integrating with respect to θ1 to
determine the marginal posterior distribution of θ2. This method also applies
when θ = (θ1,θ2,...,θJ).
In computing marginal posterior distributions for the mean vector and co-
variance matrix of a Multivariate Normal distribution with Conjugate priors,
integration of the joint posterior distribution will be carried out with respect
to the error covariance matrix Σ to ﬁnd the marginal posterior distribution
p(µ|X) of the mean vector µ. The integration is as follows.
The joint posterior distribution of the mean vector and covariance matrix
is
p(µ,Σ|X) ∝|Σ|−(n+ν+1)
2
e−1
2 trΣ−1[(X−enµ′)′(X−enµ′)+(µ−µ0)(µ−µ0)′+Q],
(6.1.13)
which upon inspection is an Inverted Wishart distribution except for a nor-
malizing constant
k(µ|X) = |(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|
ν∗−p−1
2
,
(6.1.14)
where ν∗= n+ν +1.
The joint posterior distribution is written as
© 2003 by Chapman & Hall/CRC

p(µ,Σ|X) ∝|(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|−ν∗−p−1
2
×|(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|
ν∗−p−1
2
×|Σ|−ν∗
2 e−1
2 trΣ−1[(X−enµ′)′(X−enµ′)+(µ−µ0)(µ−µ0)′+Q].
(6.1.15)
Upon integrating this joint posterior distribution, the marginal posterior dis-
tribution of the mean vector µ is
p(µ|X) ∝

p(µ,Σ|X) dΣ
∝|(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|−ν∗−p−1
2
×

|(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|
ν∗−p−1
2
×|Σ|−ν∗
2 e−1
2 trΣ−1[(X−enµ′)′(X−enµ′)+(µ−µ0)(µ−µ0)′+Q]dΣ
(6.1.16)
∝
1
|(X −enµ′)′(X −enµ′)+(µ−µ0)(µ−µ0)′ +Q|
ν∗−p−1
2
,
(6.1.17)
where the integral was recognized as being an Inverted Wishart distribution
except for its proportionality constant which did not depend on Σ. Upon
performing some matrix algebra, the above yields a marginal posterior distri-
bution which is recognized as being a Multivariate Student t-distribution.
Similarly, integration is performed with respect to the mean vector µ.
6.1.2
Gibbs Sampling
Gibbs sampling [13, 14] is a stochastic integration method that draws ran-
dom variates from the posterior conditional distribution for each of the para-
meters conditional on ﬁxed values of all the other parameters and the data X.
Let p(θ|X) be the posterior distribution of the parameters where θ is the set
of parameters and X is the data. Let θ be partitioned as θ = (θ1,θ2,...,θJ)
into J groups of parameters. Ideally, we would like to perform the integration
of the joint posterior distribution to obtain marginal posterior distributions
p(θj|X) =

p(θ1,...,θJ|X) dθ1 ...dθj−1 dθj+1 ...dθJ
(6.1.18)
and marginal posterior mean estimates
E(θj|X) =

θjp(θj|X)dθj.
(6.1.19)
© 2003 by Chapman & Hall/CRC

Unfortunately, these integrations are usually of very high dimension and not
always available in a closed form. This is why we need the Gibbs sampling
procedure. With the random variates drawn from the posterior conditional
distributions
p(θj|θ1,...,θj−1,θj+1,...,θJ,X) = p(θ1,...,θj−1,θj,θj+1,...,θJ,|X)
p(θ1,...,θj−1,θj+1,...,θJ|X)
∝p(θ1,...,θj−1,θj,θj+1,...,θJ,|X)
(6.1.20)
we can determine the marginal posterior distributions (Equation 6.1.18) and
any marginal posterior quantities such as the marginal posterior means (Equa-
tion 6.1.19).
For the Gibbs sampling, we begin with an initial value for the parameters
¯θ(0) = (¯θ(0)
1 , ¯θ(0)
2 ,..., ¯θ(0)
J ),
and at the lth iteration deﬁne
¯θ(l+1) = (¯θ(l+1)
1
, ¯θ(l+1)
2
,..., ¯θ(l+1)
J
)
by the values from
¯θ(l+1)
1
= a random variate from p(¯θ1|¯θ(l)
2 , ¯θ(l)
3 ,..., ¯θ(l)
J ,X),
(6.1.21)
¯θ(l+1)
2
= a random variate from p(¯θ2|¯θ(l+1)
1
, ¯θ(l)
3 ,..., ¯θ(l)
J ,X),
(6.1.22)
...
¯θ(l+1)
J
= a random variate from p(¯θJ|¯θ(l+1)
1
, ¯θ(l+1)
2
,..., ¯θ(l+1)
J−1 ,X), (6.1.23)
that is, at each step l drawing a random variate from the associated condi-
tional posterior distribution. To apply this method we need to determine the
posterior conditional of each θj, the posterior distribution of each θj condi-
tional on the ﬁxed values of all the other elements of θ and X from p(θ|X).
After drawing s + L random variates of each we will have ¯θ(1), ¯θ(2), ...,
¯θ(s+1),..., ¯θ(s+L). The ﬁrst s random variates called the “burn in” are dis-
carded and the remaining L variates are kept.
It has been shown [14] that under mild conditions the L randomly sampled
variates for each of the parameters constitute a random sample from the
corresponding marginal posterior distribution given the data and that for
any measurable function of the sample values whose expectation exists, the
average of the function of the sample values converges almost surely to the
expected value of the population parameter values.
The marginal posterior distributions (Equation 6.1.18) are computed to be
© 2003 by Chapman & Hall/CRC

¯p(θj|X) = 1
L
L

l=1
δ

θj −¯θ(s+l)
j

, j = 1,...,J,
(6.1.24)
where δ(·) denotes the Kronecker delta function
δ

θj −¯θ(s+l)
j

=

1, θj = ¯θ(s+l)
j
0, otherwise
(6.1.25)
and the marginal posterior mean estimators of the parameters (Equation 6.1.19)
are computed to be ¯θ = (¯θ1,..., ¯θJ) where
¯θj = ¯E(θj|X)
= 1
L
L

l=1
¯θ(s+l)
j
, j = 1,...,J.
(6.1.26)
The marginal posterior estimators of the variances of the parameter can be
similarly found as
var(θj|X) = 1
L
L

l=1

¯θ(s+l)
j
2
−

1
L
L

l=1
¯θ(s+l)
j
2
(6.1.27)
if θj is a scalar variate,
var(θj|X) = 1
L
L

l=1

¯θ(s+l)
j

¯θ(s+l)
j
′
−¯θj ¯θ′
j
(6.1.28)
if θj is a vector variate, and
var(θj|X) = 1
L
L

l=1
vec

¯θ(s+l)
j

vec

¯θ(s+l)
j
′
−vec
¯θj

vec
¯θj
′
(6.1.29)
if θj is a matrix variate. In fact, the posterior estimate of any function of the
parameters can be found.
Credibility interval estimates can also be found with the use of nonparamet-
ric techniques and all of the retained sample variates. In practice, a distribu-
tional speciﬁcation can be used with appropriate marginal posterior moments
to deﬁne it. For example, instead of retaining L random matrix variates of
dimension p × (q + 1) for the matrix of regression coeﬃcients, the marginal
posterior mean and covariance matrices are used in a Normal distribution.
This is reasonable since the Conjugate prior and the posterior conditional
distributions for the matrix of Regression coeﬃcients are both Normal.
© 2003 by Chapman & Hall/CRC

6.1.3
Gibbs Sampling Convergence
The Gibbs sampling procedure in the current form was developed [14, 19]
as a way to avoid direct multidimensional integration.
It is well known that the full posterior conditional distributions uniquely
determine the full joint distribution when the random variables have a joint
distribution whose distribution function is strictly positive over the sample
space [13]. Since the posterior conditionals uniquely determine the full joint
distribution, they also uniquely determine the posterior marginals.
It was
shown that under mild conditions, the following results are true [14].
Result 1 (Convergence)
The randomly generated variates from the posterior conditional distribu-
tions, (¯θ(l)
1 , ¯θ(l)
2 ,..., ¯θ(l)
J ) converge in distribution to the true parameter values
(θ1,θ2,...,θJ). This convergence is denoted by
(¯θ(l)
1 , ¯θ(l)
2 ,..., ¯θ(l)
J )
d→(θ1,θ2,...,θJ)
and hence for each j, the average of the random variates ¯θ(l)
j
converges to its
corresponding parameter value θj which has the distribution p(θj), written as
¯θ(l)
j
d→θj ∼p(θj) as l →∞.
Result 2 (Rate)
Using the sup norm, the joint posterior distribution of (¯θ(l)
1 , ¯θ(l)
2 ,..., ¯θ(l)
J ) con-
verges to the true joint posterior distribution p(θ1,θ2,...,θJ) at a geometric
rate in l, when visiting in order.
Result 3 (Ergodic Theorem)
For any measurable function T of the parameter values (¯θ1, ¯θ2,..., ¯θJ) whose
expectation exists, the average of the measurable functions of the sample
variates, as the number of sample variates tends toward inﬁnity, converges
almost surely to its expected value. This is expressed as
lim
L→∞
1
L
L

l=1
T(¯θ(l)
1 , ¯θ(l)
2 ,..., ¯θ(l)
J )
a.s.
→E(T(θ1,θ2,...,θJ)).
converges almost surely [47] to its expectation.
With these results, we are guaranteed convergence of the Gibbs sampling
estimation method.
6.1.4
Normal Variate Generation
The generation of random variates from Normal distributions is described
in terms of the Matrix Normal distribution with vector and scalar distributions
as special cases. The mathematical symbols here are used generically.
© 2003 by Chapman & Hall/CRC

An n×p random Matrix Normal variate X with mean matrix MX and
covariance matrix Φ⊗Σ can be generated from np independent scalar stan-
dard Normal variates with mean zero and variance one. This is performed
with the following Matrix Normal property.
If YX is an n×p random matrix whose elements are independent standard
Normal scalar variates, written
YX ∼N(0,In ⊗Ip)
(6.1.30)
and then using a transformation of variable result [17]
X = AXYXB′
X +MX ∼N(MX,AXA′
X ⊗BXB′
X),
(6.1.31)
where M is the mean matrix, Φ = AXA′
X, and Σ = BXB′
X. The covariance
matrices Σ and Φ have been factorized using a method such as a Cholesky
factorization also called decomposition [32] or a factorization by eigenvalues
and eigenvectors [53].
It was assumed that a method is available to generate standard Normal
scalar variates. If a Normal random variate generation method is not available,
these variates may be generated as follows [5, 22]. Generate two variates y1
and y2 which are Uniform on the unit interval. Deﬁne
x1 = (−2logy1)
1
2 cos(2πy2)
(6.1.32)
x2 = (−2logy1)
1
2 sin(2πy2).
(6.1.33)
Then, x1 and x2 are independent Scalar Normal variates with mean 0 and
variance 1. Now, only a method to generate Uniform random variates on the
unit interval is needed.
6.1.5
Wishart and Inverted Wishart Variate Generation
A p×p random Wishart matrix variate G or a p×p random Inverse Wishart
matrix variate Σ can be generated as follows. By generating a ν0 ×p standard
Matrix Normal variate YG as above and then using the transformation of
variable result [17], a Wishart distributed matrix variate
Y ′
GYG ∼W(Ip,p,ν0)
(6.1.34)
can be generated. Upon using the transformation of variable result [41]
G = AG(Y ′
GYG)A′
G ∼W(AGA′
G,p,ν0),
(6.1.35)
where Υ = AGA′
G has been factorized using technique such as the Cholesky
factorization [32] or using eigenvalues and eigenvectors [53].
© 2003 by Chapman & Hall/CRC

Inverted Wishart matrix variates can be generated by ﬁrst generating a
ν0 ×p standard Matrix Normal variate YΣ as above and then using the trans-
formation of variable result [17]
(Y ′
ΣYΣ)−1 ∼IW(Ip,p,ν)
(6.1.36)
and ﬁnally the transformation of variable result [41]
Σ = AΣ(Y ′
ΣYΣ)−1A′
Σ ∼IW(AΣA′
Σ,p,ν),
(6.1.37)
where ν0 = ν −p−1 and Q = AΣA′
Σ has been factorized.
If the degrees of freedom ν0 is not an integer, then random Wishart vari-
ates can be generated with the use of independent Gamma (Scalar Wishart)
distributed variates with real valued parameters.
6.1.6
Factorization
In implementing the Gibbs sampling algorithm, matrix factorizations have
to be computed. Two possibilities are the Cholesky and Eigen factorizations.
In the following, assume that we wish to factor the p×p covariance matrix Σ.
6.1.6.1
Cholesky Factorization
Cholesky’s method for factorizing a symmetric positive deﬁnite matrix Σ
of dimension p is very straightforward. This factorization Σ = AΣA′
Σ has the
property that AΣ be a lower triangular matrix. Denote the ijth element of Σ
and AΣ to be σij and aij respectively. Simple formulas [32] for the method
are
a11 = √σ11
(6.1.38)
aii =



σii −
i−1

k=1
a2
ik
i = 2,...,n
(6.1.39)
ai1 = σi1
a11
i = 1,...,n
(6.1.40)
aij =
1
ajj

σij −
j−1

k=1
aikajk

i = j +1,...,n;j ≥2.
(6.1.41)
6.1.6.2
Eigen Factorization
The matrix Σ can also be factorized using eigenvalues and eigenvectors as
Σ = (WD
1
2
θ )(WD
1
2
θ )′
(6.1.42)
= AΣA′
Σ,
(6.1.43)
© 2003 by Chapman & Hall/CRC

where the columns of W are the orthonormal eigenvectors which sequentially
maximize the percent of variation and Dθ is a diagonal matrix with elements
θj which are the eigenvalues.
The vector w1 is now determined to be that vector that maximizes the
variance subject to w′
1w1 = 1. The method of Lagrange multipliers is applied
∂
∂w1
[w′
1Σw1 −θ1(w′
1w1 −1)] = 2(Σ−θ1Ip)w1 = 0
and since w1 ̸= 0, there can only be a solution if
|Σ−θ1Ip| = 0.
It is apparent that θ1 must be an eigenvalue of Σ, and w1 is a normalized
eigenvector of Σ. There are p such eigenvalues that satisfy the equation. The
largest is selected. The other rows of W are found in a similar fashion with
the additional constraints that they are orthogonal to the previous ones. For
a more detailed account of the procedure refer to [41]. Previous work [53]
used this Eigen factorization in the context of factorizing separable matrices
Ω= Φ ⊗Σ in which the covariance matrices Φ and Σ were patterned with
exact known formulas for computing the eigenvectors and eigenvalues. Only
the eigenvalues and eigenvectors of Φ and Σ were needed and not of Ω.
Occasionally a matrix factorization will be represented as
Σ = (WD
1
2
θ )(WD
1
2
θ )′
(6.1.44)
= (Σ
1
2 )(Σ
1
2 )′
(6.1.45)
or the factorization of an inverse as
Σ−1 = (WD
−1
2
θ
)(WD
−1
2
θ
)′
(6.1.46)
= (Σ−1
2 )(Σ−1
2 )′.
(6.1.47)
These are refered to as the square root matrices.
6.1.7
Rejection Sampling
Random variates can also be generated from an arbitrary distribution func-
tion by using the rejection sampling method [16, 40, 48]. Occasionally the
(posterior conditional) distribution of a model parameter is not recognized as
one of the well-known standard distributions from which we can easily gen-
erate random variates. When this is the case, a rejection sampling technique
can be employed.
Assume that we are able to generate a random variate from the distribution
f(x) whose support (range of x values for which the distribution function is
© 2003 by Chapman & Hall/CRC

nonzero) is the same as p(x). If the support of f(x) is not the same as that
of p(x), then a truncated distribution function which restricts the range of
values of f(x) to that of p(x) can be used. Further assume that p(x) ≤cf(x)
for all x and a positive constant c.
The random variate from f(x) can be used to generate a random variate
from p(x). We generate a random variate y0 from f(y) and can retain this
generated variate as being from p(y). The rejection sampling is a simple two
step process [48] which proceeds as follows.
Step 1: Generate a random variate y0 from convenient distribution f(y) and
independently generate u0 from a Uniform distribution on the unit interval.
Step 2: If the Uniform random variate u0 ≤
p(y0)
cf(y0) where c is a constant,
then let x0 = y0. If not repeat step 1.
The retained random variate x0 generated by the above rejection sampling
process is a random variate from p(x). This can be shown to be true in the
following manner.
Denote the retained variate by x0. Let N denote the number of iterations
of the above steps required to retain x0, and yN denote a variate which took
N iterations to be retained. The probability of the retained value x0 being
less than another value x is
P[x0 ≤x] = P[yN ≤x]
= P

y0 ≤x|u0 ≤p(y0)
cf(y0)

= 1
kP

y0 ≤x,u0 ≤p(y0)
cf(y0)

= 1
k
 x
−∞
f(y)

p(y)
cf(y)
0
du dy
= 1
k
 x
−∞
p(y)
cf(y)f(y) dy
= 1
kc
 x
−∞
p(y) dy,
(6.1.48)
where k = P[u0 ≤
p(y0)
cf(y0)] (the probability of the event on the right side of
the conditioning in the second line of Equation 6.1.48). We can see that by
letting x →∞, k = 1
c. The constant c does not have to be 1
k. In fact, it can
be any number greater than 1
k, but the smaller the value of c, the greater the
probability of retaining a generated random variate.
The above concept is illustrated in the following example.
Example:
Let’s use the rejection sampling technique to generate a random variate
from the Beta distribution
© 2003 by Chapman & Hall/CRC

p(̺) = k̺α−1(1−̺)β−1,
(6.1.49)
where α > 1, β > 1, 0 < ̺ < 1, and k is the proportionality constant. Note
that when α and β are integers, k =
(α+β−1)!
(α−1)!(β−1)!.
Since the Beta distribution is conﬁned to the unit interval, a good choice
for the convenient distribution from which we can generate random variates
is the Uniform distribution
f(̺) = 1,
(6.1.50)
where 0 < ̺ < 1.
Without knowing the proportionality constant for p(̺), we can determine
the rejection criteria u0 ≤p(y0)
cf(y0) of step 2. This is done by ﬁnding the maxi-
mum value of
p(̺)
f(̺) = k̺α−1(1−̺)β−1
(6.1.51)
by diﬀerentiation with respect to ̺ which yields
d
d̺
p(̺)
f(̺) = k[(α−1)̺α−2(1−̺)β−1 −(β −1)̺α−1(1−̺)β−2].
(6.1.52)
Upon setting this derivative equal to zero, the maximum is seen to be the
mode of the Beta distribution
α−1
α+β −2
(6.1.53)
which gives
p(̺)
f(̺) ≤k
	
α−1
α+β −2

α−1 	
1−
α−1
α+β −2

β−1
= c.
(6.1.54)
Therefore the ratio,
p(̺)
cf(̺) =
	
α−1
α+β −2

α−1 	
1−
α−1
α+β −2

β−1−1
̺α−1(1−̺)β−1
(6.1.55)
and the rejection sampling procedure proceeds as follows.
Step 1: Generate random Uniform variates u1 and u2.
Step 2: If
u2 ≤
	
α−1
α+β −2

α−1 	
1−
α−1
α+β −2

β−1−1
uα−1
1
(1−u1)β−1,
(6.1.56)
© 2003 by Chapman & Hall/CRC

then let the retained random variate from p(̺) be x0 = u1. If not repeat step
1.
The average number of times that step 1 will be performed is c given above.
Thus, the smaller the value of c, the fewer times step 1 will be performed on
average.
The rejection sampling procedure can be generalized to a Multivariate for-
mulation to generate random vectors [40].
6.2
Maximum a Posteriori
The joint posterior distribution may also be jointly maximized with re-
spect to the parameters. If again θ = (θ1,...,θJ), then maximum a posteriori
(MAP) estimators (the analog of maximum likelihood estimators for posterior
distributions) can be found by diﬀerentiation.
To determine joint maximum a posteriori estimators, diﬀerentiate the joint
posterior distribution
p(θ1,...,θJ|X)
(6.2.1)
with respect to each of the parameters, set the result equal to zero as
∂
∂θ1
p(θ1,...,θJ|X)
θ1=ˆθ1,...,θJ =ˆθJ = ··· ∂
∂θJ
p(θ1,...,θJ|X)
θ1=ˆθ1,...,θJ =ˆθJ = 0,
(6.2.2)
and solve the resulting system of J equations with J unknowns to ﬁnd the
joint maximum a posteriori estimators
ˆθj =
Arg Max
θj
p(θj|ˆθ1,..., ˆθJ,X)
(6.2.3)
= ˆθj(ˆθ1,..., ˆθJ,X)
(6.2.4)
for each of the parameters. In addition to the joint posterior modal estimates,
conditional maximum a posteriori quantities such as variances can also be
found as
var(θj|ˆθ1,..., ˆθJ,X) =

(θj −¯θj)2p(θj|ˆθ1,..., ˆθJ,X) dθj,
(6.2.5)
where ¯θj is the conditional posterior mean which is often the mode.
© 2003 by Chapman & Hall/CRC

6.2.1
Matrix Diﬀerentiation
When determining maxima of scalar functions of matrices by diﬀerentia-
tion, the logarithm and trace are often used because maximizing a function
is equivalent to maximizing its logarithm due to monotonicity. Some matrix
(vector) derivative results that are often used [18, 41, 46] are
1. For general θ, |θ| > 0,
∂
∂θ log|θ| = (θ−1)′.
2. For symmetric θ = θ′, |θ| > 0,
∂
∂θ log|θ| = [2θ−1 −diag(θ−1)].
3. For general A and θ,|θ| > 0,
∂
∂θtr

θ−1A

= −θ−1Aθ−1.
4. For symmetric A = A′ and θ = θ′,|θ| > 0,
∂
∂θtr(θA) = −[2A−1 −diag(A−1)].
5. For symmetric A = A′, A : q×q, B = B′, B : p×p, and general θ,θ0 : p×q,
then
∂
∂θtr[A(θ −θ0)′B(θ −θ0)] = 2B(θ −θ0)A.
where diag(·) denotes the diagonalization operator which forms a diagonal
matrix consisting only of the diagonal elements of its matrix argument and θ
has been used generically to denote either a matrix, a vector, or a scalar.
To verify that these estimators are maxima and not minima, the Hessian
matrix of second derivatives is computed. If the Hessian matrix is negative
deﬁnite, then the estimators are maxima and not minima.
6.2.2
Iterated Conditional Modes (ICM)
Iterated Conditional Modes [36, 40] is a deterministic optimization method
that ﬁnds the joint posterior modal estimators also known as the maximum a
posteriori estimates of p(θ|X) where θ denotes the collection of scalar, vector,
or matrix parameters, and X denotes the data. It is useful when the system of
J equations from diﬀerentiation does not yield closed form analytic equations
for each of the maxima.
Assume that θ = (θ1,θ2) where θ1 and θ2 are scalars and the posterior
distribution of θ is p(θ1,θ2|X). We have a surface in 3-dimensional space.
We have θ1 along one axis and θ2 along the other with p(θ1,θ2|X) being the
height of the surface or hill.
We want to ﬁnd the top of the hill which is the same as ﬁnding the peak
or maximum of the function p(θ1,θ2|X) with respect to both θ1 and θ2. As
usual, the maximum of a surface is found by diﬀerentiating with respect to
each variable (direction) and setting the result equal to zero.
The maximum of the function p(θ1,θ2|X) with respect to each of the vari-
ables are
© 2003 by Chapman & Hall/CRC

˜θ(l+1)
1
=
Arg Max
θ1
p(θ1|˜θ(l)
2 ,X)
(6.2.6)
= ˜θ1(˜θ(l)
2 ,X)
(6.2.7)
˜θ(l+1)
2
=
Arg Max
θ2
p(θ2|˜θ(l+1)
1
,X)
(6.2.8)
= ˜θ2(˜θ(l+1)
1
,X)
(6.2.9)
which satisﬁes
∂
∂θ1
p(θ1,θ2|X)
θ1=˜θ1 = ∂
∂θ2
p(θ1,θ2)
θ2=˜θ2 = 0,
(6.2.10)
which is the same as
∂
∂θ1
p(θ1|θ2,X)p(θ2|X)
θ1=˜θ1 = ∂
∂θ2
p(θ2|θ1,X)p(θ1|X)
θ2=˜θ2 = 0
(6.2.11)
or
p(θ2|X) ∂
∂θ1
p(θ1|θ2,X)
θ1=˜θ1 = p(θ1|X) ∂
∂θ2
p(θ2|θ1,X)
θ2=˜θ2 = 0
(6.2.12)
assuming that p(θ1|X) ̸= 0 and p(θ2|X) ̸= 0.
We can obtain the posterior conditionals (functions) p(θ1|θ2,X) and p(θ2|θ1,X)
along with their respective modes (maximum) ˜θ1 = ˜θ1(θ2,X) and ˜θ2 = ˜θ2(θ1,X).
We have the maximum of θ1, ˜θ1 for a given value of (conditional on) θ2,
and the maximum of θ2, ˜θ2 for a given value of (conditional on) θ1.
The optimization procedure consists of
1. Select an initial value for θ2; call it ˜θ(0)
2 .
2. Calculate the modal (maximal) value of p(θ1|˜θ(0)
2 ,X), ˜θ(1)
1 .
3. Calculate the modal (maximal) value of p(θ2|˜θ(1)
1 ,X), ˜θ(1)
2 .
4. Continue to calculate the remainder of the sequence ˜θ(1)
1 , ˜θ(1)
2 , ˜θ(2)
1 , ˜θ(2)
2 ,...
until convergence is reached.
If the posterior conditional distributions are not unimodal, we may converge
to a local maximum and not the global maximum. If each of the posterior
conditionals are unimodal, then the Hessian matrix is negative deﬁnite and
the converged maximum is always the global maximum.
When convergence is reached, the point estimators (˜θ1, ˜θ2) are the maximum
a posteriori estimators.
© 2003 by Chapman & Hall/CRC

This method can be generalized to more than two parameters [40]. If θ is
partitioned by θ = (θ1,θ2,...,θJ) into J groups of parameters, we begin with
a starting point ˜θ(0) = (˜θ(0)
1 , ˜θ(0)
2 ,..., ˜θ(0)
J ) and at the lth iteration deﬁne ˜θ(l+1)
by
˜θ(l+1)
1
=
Arg Max
θ1
p(θ1|˜θ(l)
2 ,..., ˜θ(l)
J ,X)
(6.2.13)
= ˜θ1(˜θ(l)
2 , ˜θ(l)
3 ,..., ˜θ(l)
J ,X)
(6.2.14)
˜θ(l+1)
2
=
Arg Max
θ2
p(θ2|˜θ(l+1)
1
, ˜θ(l)
3 ,..., ˜θ(l)
J ,X)
(6.2.15)
= ˜θ2(˜θ(l+1)
1
, ˜θ(l)
3 ,..., ˜θ(l)
J ,X)
(6.2.16)
...
˜θ(l+1)
J
=
Arg Max
θJ
p(θJ|˜θ(l+1)
1
,..., ˜θ(l+1)
J−1 ,X)
(6.2.17)
= ˜θ1(˜θ(l+1)
2
, ˜θ(l+1)
3
,..., ˜θ(l+1)
J−1 ,X)
(6.2.18)
at each step computing the maximum or mode. To apply this method we
need to determine the functions ˜θj which give the maximum of p(θ|X) with
respect to θj, conditional on the ﬁxed values of all the other elements of θ.
6.3
Advantages of ICM over Gibbs Sampling
We will show that when Φ is a general symmetric covariance matrix (or
known), each of the posterior conditional distributions are unimodal. Thus
we do not have to worry about local maxima; we will ﬁnd global maxima.
The reason one would use a stochastic procedure like Gibbs sampling over a
deterministic procedure like ICM is to eliminate the possibility of converging
to a local mode when the conditional posterior distribution is multimodal.
ICM is slightly simpler to implement than Gibbs and less computationally
intensive because Gibbs sampling requires generation of random variates from
the conditionals which includes matrix factorizations. ICM simply has to cycle
through the posterior conditional modes and convergence is not uncertain as
it is with Gibbs sampling. With ICM, we can check for convergence, say every
1000 iterations, by computing the diﬀerence between θ(1000l)
j
and θ(1000(l+1))
j
for every j, and if each element is the same to the third decimal, we can claim
convergence and stop. This reduces computation time.
ICM should be implemented cautiously when Φ is a correlation matrix
such as a ﬁrst order Markov or an intraclass matrix with a single unknown
parameter. The posterior conditional is not necessarily unimodal. ICM might
© 2003 by Chapman & Hall/CRC

converge to a local maxima. This could however be combated by an exhaustive
search over the entire interval for which the single parameter is deﬁned.
6.4
Advantages of Gibbs Sampling over ICM
When the posterior conditionals are not recognizable as unimodal distri-
butions, we might prefer to use a stochastic procedure like Gibbs sampling
to eliminate the possibility of converging to a local maxima. Although Gibbs
sampling is more computationally intensive than ICM, it is a more general
method and gives us more information such as marginal posterior point and
interval estimates. Gibbs sampling allows us to make inferences regarding a
parameter unconditional of the other parameters.
© 2003 by Chapman & Hall/CRC

Exercises
1. Use the following 12 independent random Uniform variates in the inter-
val (0,1) to generate 12 independent random N(0,1) variates.
TABLE 6.1
Twelve independent random Uniform variates.
0.9501
0.4565
0.2311
0.0185
0.6068
0.8214
0.4860
0.4447
0.8913
0.6154
0.7621
0.7919
2. Compute the Cholesky factorization AΣA′
Σ of the positive deﬁnite sym-
metric covariance matrix
Σ =


2 0 1
0 4 2
1 2 3

.
3. Using the ﬁrst three independent random Scalar Normal variates (N(0,1)’s)
from Exercise 1, generate a 3-dimensional random Multivariate Normal
vector valued variate x with mean vector and covariance matrix
µ =


5
3
7


Σ = AΣA′
Σ,
where AΣ is computed from Exercise 2 above.
4. Compute the Eigen factorization AΣA′
Σ of
Q =


100 10
10
10 100 10
10 10 100

.
5. Using the last nine independent random Scalar Normal variates (N(0,1)’s)
from Exercise 1, generate a random 3 ×3 Inverse Wishart matrix vari-
ate Σ with scale matrix Q = AΣA′
Σ given in Exercise 4 above and ν = 5
degrees of freedom.
6. Using the ﬁrst two independent Uniformly distributed random variates
in Exercise 1, generate a random variate from a B(α = 2,β = 2) distri-
bution by using the rejection sampling technique.
© 2003 by Chapman & Hall/CRC

7. Assume that we have a posterior distribution
p(µ,σ2|x1,...,xn) ∝(σ2)
n+ν+1
2
e−
g
2σ2 ,
where
g =
n

i=1
(xi −¯x)2 +(µ−µ0)2 +q.
Derive an ICM algorithm for obtaining joint maximum a posteriori es-
timates of µ and σ2.
© 2003 by Chapman & Hall/CRC

7
Regression
7.1 
Intro duction
The purpose of this Chapter is to quickly review the Classical Multivariate
Regression model before the presentation of Multivariate Bayesian Regression
in Chapter 8. This is accomplished by a build up which starts with the Scalar
Normal Samples model through to the Simple and Multiple Regression models
and ﬁnally to the Multivariate Regression model. For each model, estimation
and inference is discussed. This is fundamental knowledge which is essential
to successfully understanding Part II of the text.
7.2
Normal Samples
Consider the following independent and identically distributed random
variables x1,...,xn from a Scalar Normal distribution with mean µ and vari-
ance σ2 denoted by N(µ,σ2). This model can be written in terms of a linear
model similar to Regression. This is identical to the linear model
xi = µ + ǫi,
(7.2.1)
where ǫi is the Scalar Normally distributed random error with mean zero and
variance σ2 denoted
ǫi ∼N(0,σ2)
(7.2.2)
for i = 1,...,n. The joint distribution of the variables or likelihood is
p(x1,...,xn|µ,σ2) =
n

i=1
p(xi|µ,σ2)
=
n

i=1
(2πσ2)−1
2 e−(xi−µ)2
2σ2
© 2003 by Chapman & Hall/CRC

= (2πσ2)−n
2 e−
1
2σ2
Pn
i=1(xi−µ)2
.
(7.2.3)
This model can also be written in terms of vectors
x =



x1
...
xn


,
en =



1
...
1


,
ǫ =



ǫ1
...
ǫn


,
(7.2.4)
so that the model is
x
=
en
µ
+
ǫ,
n×1
n×1 1×1
n×1
(7.2.5)
and likelihood is
p(x|µ,σ2) = (2πσ2)−n
2 e−(x−enµ)′(x−enµ)
2σ2
,
(7.2.6)
where “ ′ ” denotes the transpose.
The natural logarithm of p(x|µ,σ2) can be taken and diﬀerentiated with
respect to scalars µ and σ2 in order to obtain values which maximize the
likelihood. These are maximum likelihood estimates. However, some algebra
on the exponent can also be performed to ﬁnd the value of µ which yields the
maximal value of this likelihood.
(x−enµ)′(x−enµ) = x′x−x′µen −e′
nµx+µe′
nenµ
= µ[e′
nenµ−e′
nx]−x′enµ+x′x
= µ(e′
nen)[µ−(e′
nen)−1e′
nx]−x′enµ+x′x
= µ(n)[µ−(n)−1e′
nx]−x′enµ+x′x
= µ(n)[µ−ˆµ]−(n)ˆµµ+x′x
= (µ−ˆµ)(n)(µ−ˆµ)+(n)ˆµµ−ˆµ(n)ˆµ−(n)ˆµµ+x′x
= (µ−ˆµ)(n)(µ−ˆµ)−ˆµ(n)ˆµ+x′x
= (µ−ˆµ)(n)(µ−ˆµ)−(n)−1x′ene′
nx+x′x
= (µ−ˆµ)(n)(µ−ˆµ)+x′(In −ene′
n/n)x,
(7.2.7)
where ˆµ = (n)−1x′en = ¯x. This could have been written with the vector gen-
eralization µ′ but was not since µ′ = µ.
The likelihood is now
p(x|µ,σ2) = (2πσ2)−n
2 e
−
1
2σ2 [n(µ−ˆµ)2+x′

In−ene′n
n

x]
.
(7.2.8)
It is now obvious that the value of the mean µ which maximizes the likeli-
hood or minimizes the exponent in the likelihood is ˆµ = ¯x.
© 2003 by Chapman & Hall/CRC

Upon diﬀerentiating the natural logarithm of the likelihood denoted by
LL = log[p(x|µ,σ2)] with respect to σ2, then evaluating it at the maximal
values of the parameters, and setting it equal to zero
∂
∂σ2 LL

µ=ˆµ,σ2=ˆσ2
= 0,
(7.2.9)
we obtain the maximum likelihood estimate of the variance σ2,
ˆσ2 = (x−enˆµ)′(x−enˆµ)
n
.
(7.2.10)
Consider the numerator of the estimate ˆσ2.
NUM = (x−enˆµ)′(x−enˆµ)
= x′x−x′enˆµ−(enˆµ)′x+(enˆµ)′(enˆµ)
= ˆµ′[e′
nenˆµ−e′
nx]−x′enˆµ+x′x
= ˆµ′(e′
nen)[ˆµ−(e′
nen)−1e′
nx]−x′enˆµ+x′x
= ˆµ′(n)[ˆµ−(n)−1e′
nx]−x′enˆµ+x′x
= [ˆµ−(n)−1e′
nx]′(n)[ˆµ−(n)−1e′
nx]+(n)−1x′en(n)ˆµ
−(n)−1x′en(n)(n)−1e′
nx−x′enˆµ+x′x
= x′

In −ene′
n
n

x.
(7.2.11)
This is exactly the g term in the likelihood. Now the likelihood can be
written as
p(x|µ,σ2) = (2πσ2)−n
2 e−
1
2σ2 [n(µ−ˆµ)2+g].
(7.2.12)
In the likelihood for Normal observations as written immediately above, if
the normalization coeﬃcients are ignored by using proportionality, it can be
partitioned and viewed as a joint distribution of ˆµ and g. This then becomes
p(ˆµ,g|µ,σ2) ∝(σ2)−1
2 e
−(ˆµ−µ)2
2σ2/n (σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
=
e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
 
!"
#
ˆµ|µ,σ2∼N(µ,σ2/n)
(σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2
 
!"
#
g|σ2∼W (σ2,1,n−1)
.
(7.2.13)
© 2003 by Chapman & Hall/CRC

It should be noted that the estimates of the mean ˆµ and the variance ˆσ2 are
independent by the Neyman factorization criterion [22, 47]. This is proven by
showing that
p(ˆµ,g|µ,σ2) = p(ˆµ|µ,σ2)p(g|µ,σ2).
(7.2.14)
If the joint distribution p(ˆµ,g|µ,σ2) is integrated with respect to g, then
p(ˆµ|µ,σ2) =

p(ˆµ,g|µ,σ2) dg
=

e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
(σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2
dg
=
e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
 (σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2
dg
=
e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
,
(7.2.15)
and thus,
ˆµ|µ,σ2 ∼N

µ,σ2/n

.
(7.2.16)
If the joint distribution p(ˆµ,g|µ,σ2) is integrated with respect to ˆµ, then
p(g|σ2) =

p(ˆµ,g|µ,σ2) dˆµ
=

e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
(σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2
dˆµ
= (σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2

e
−(ˆµ−µ)2
2σ2/n
(2πσ2/n)
1
2
dˆµ
= (σ2)−n−1
2 g
n−1−2
2
e−
g
2σ2
Γ
 n−1
2

2
n−1
2
,
(7.2.17)
and thus,
g|σ2 ∼W(σ2,1,n−1).
(7.2.18)
© 2003 by Chapman & Hall/CRC

The joint distribution of µ and g is equal to the product of their marginal
distributions. Note that g/σ2 has the familiar Chi-squared distribution with
n−1 degrees of freedom.
By changing variables from ˆµ and g to t = (n −1)
1
2 g−1/2n
1
2 (ˆµ −µ) and
w = g, the joint distribution of t and w becomes
p(t,w|µ,σ2) = (σ2)−n
2 w
n−2
2 e
−w
2σ2

t2
n−1 +1

[(n−1)π]
1
2 Γ
 n−1
2

2
n
2
(7.2.19)
where the Jacobian of the transformation was
J(ˆµ,g →t,w) = (n−1)−1
2 w
1
2 n−1
2 .
(7.2.20)
Now integrate with respect to w to ﬁnd the distribution of t (unconditional
on w). This is done by making the integrand look like the Scalar Wishart
distribution. It is seen that the integrand needs to be multiplied and divided
by the same factor.
p(t|µ) =

p(t,w|µ,σ2) dw
∝

(σ2)−n
2 w
n−2
2 e
−w
2σ2

t2
n−1 +1

dw
∝
	 t2
n−1 +1

−n
2  	 t2
n−1 +1

 n
2
(σ2)−n
2 w
n−2
2 e
−w
2σ2

t2
n−1 +1

dw
∝
	 t2
n−1 +1

−n
2
(7.2.21)
and now transforming back while taking g to be known
p(ˆµ|µ,g) ∝

1+ng−1 (ˆµ−µ)2−n
2
∝

1+
1
n−1

ˆµ−µ
ˆσ/
&
(n−1)
2

−n
2
∝

1+
1
n−1

ˆµ−µ
ˆσ/
&
(n−1)
2

−(n−1)+1
2
.
(7.2.22)
Recall the Scalar Student t-distribution in the statistical distribution Chapter.
The estimate of the mean ˆµ given the estimated variance has the familiar
Scalar Student t-distribution with n−1 degrees of freedom.
© 2003 by Chapman & Hall/CRC

7.3
Simple Linear Regression
The Simple Linear Regression model is
xi = β0 +β1ui1 +ǫi
(7.3.1)
where ǫi is the Scalar Normally distributed random error term with mean zero
and variance σ2,
ǫi ∼N(0,σ2)
(7.3.2)
for i = 1,...,n. The joint distribution of the variables or likelihood is
p(x1,...,xn|β0,β1,σ2,u11,...,un1) =
n

i=1
p(xi|β0,β1,σ2,ui1)
=
n

i=1
(2πσ2)−1
2 e−(xi−β0−β1ui1)2
2σ2
= (2πσ2)−n
2 e−
1
2σ2
Pn
i=1(xi−β0−β1ui1)2
.
(7.3.3)
This can be written in terms of vectors and matrices
x =



x1
...
xn


,
ui =
	
1
ui1

,
U =



u′
1
...
u′
n


,
β =
	
β0
β1

,
ǫ =



ǫ1
...
ǫn


, (7.3.4)
so that the model is
x
=
U
β
+
ǫ
n×1
n×2 2×1
n×1
(7.3.5)
and the likelihood is
p(x|β,σ2,U) = (2πσ2)−n
2 e−(x−Uβ)′(x−Uβ)
2σ2
.
(7.3.6)
The natural logarithm of p(x|β,σ2,U) can be taken and diﬀerentiated with
respect to the vector β and the scalar σ2 in order to obtain values of β and
σ2 which maximize the likelihood. These are maximum likelihood estimates.
© 2003 by Chapman & Hall/CRC

However, some algebra on the exponent can also be performed to ﬁnd the
value of β which yields the maximal value of this likelihood.
(x−Uβ)′(x−Uβ) = x′x−x′Uβ −β′U ′x+β′U ′Uβ
= β′[U ′Uβ −U ′x]−x′Uβ +x′x
= β′(U ′U)[β −(U ′U)−1U ′x]−x′Uβ +x′x
= β′(U ′U)(β −ˆβ)−x′Uβ +x′x
= (β′ −ˆβ′)(U ′U)(β −ˆβ)+ ˆβ′(U ′U)(β −ˆβ)
−x′Uβ +x′x
= (β −ˆβ)′(U ′U)(β −ˆβ)+ ˆβ′(U ′U)β −ˆβ′(U ′U)ˆβ
−x′Uβ +x′x
= (β −ˆβ)′(U ′U)(β −ˆβ)+x′U(U ′U)−1(U ′U)β
−x′U(U ′U)−1(U ′U)(U ′U)−1U ′x−x′Uβ +x′x
= (β −ˆβ)′(U ′U)(β −ˆβ)
−x′U(U ′U)−1(U ′U)(U ′U)−1U ′x+x′x
= (β −ˆβ)′(U ′U)(β −ˆβ)
+x′[In −U(U ′U)−1U ′]x,
(7.3.7)
where ˆβ = (U ′U)−1U ′x.
The likelihood is now
p(x|β,σ2,U) = (2πσ2)−n
2 e−
1
2σ2 {(β−ˆβ)′(U′U)(β−ˆβ)+x′[In−U(U′U)−1U′]x}.
(7.3.8)
It is now obvious that the value of β which maximizes the likelihood or
minimizes the exponent in the likelihood is ˆβ = (U ′U)−1U ′x.
Upon diﬀerentiating the natural logarithm of the likelihood denoted by
LL = log[p(x|β,σ2,U)] with respect to σ2, then evaluating it at the maximal
values of the parameters, and setting it equal to zero
∂
∂σ2 LL

β= ˆβ,σ2=ˆσ2 = 0,
(7.3.9)
we obtain
ˆσ2 = (x−U ˆβ)′(x−U ˆβ)
n
.
(7.3.10)
Consider the numerator of the estimate ˆσ2.
© 2003 by Chapman & Hall/CRC

NUM = (x−U ˆβ)′(x−U ˆβ)
= x′x−x′U ˆβ −ˆβ′U ′x+ ˆβ′U ′U ˆβ
= ˆβ′(U ′U ˆβ −U ′x)−x′U ˆβ +x′x
= ˆβ′(U ′U)(ˆβ −(U ′U)−1U ′x)−x′U
ˆ
βBeta+x′x
= (ˆβ −(U ′U)−1U ′x)′(U ′U)(ˆβ −(U ′U)−1U ′x)
+x′U(U ′U)−1(U ′U)ˆβ −x′U(U ′U)−1(U ′U)(U ′U)−1U ′x
−x′U ˆβ +x′x
= x′x−x′U(U ′U)−1U ′x
= x′[In −U(U ′U)−1U ′]x,
(7.3.11)
This is exactly the g term in the likelihood. Now let’s write the likelihood
as
p(x|β,σ2,U) = (2πσ2)−n
2 e−
1
2σ2 [(β−ˆβ)′(U′U)(β−ˆβ)+g],
(7.3.12)
where g = (x−U ˆβ)′(x−U ˆβ).
In the likelihood for the simple linear regression model as written immedi-
ately above, if the normalization coeﬃcients are ignored by using proportion-
ality, it can be partitioned and viewed as a joint distribution of ˆβ and g. This
then becomes
p(ˆβ,g|β,σ2) ∝(σ2)−(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
(σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
= |σ2(U ′U)−1|
1
2
(2π)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
 
!"
#
ˆβ|β,σ2∼N(β,σ2(U′U)−1)
× (σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
 
!"
#
g|σ2∼W(σ2,1,n−(q+1))
,
(7.3.13)
where q which is unity is generically used.
It should be noted that the estimates of the vector of regression coeﬃcients
ˆβ and the variance ˆσ2 are independent by the Neyman factorization criterion
[22, 47]. This is proven by showing that
p(ˆβ,g|β,σ2) = p(ˆβ|β,σ2)p(g|β,σ2).
(7.3.14)
© 2003 by Chapman & Hall/CRC

If the joint distribution p(ˆβ,g|β,σ2) is integrated with respect to g, then
p(ˆβ|β,σ2) =

p(ˆβ,g|β,σ2) dg
=
 |σ2(U ′U)−1|
1
2
(2π)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
×(σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
dg
= |σ2(U ′U)−1|
1
2
(2π)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
×
 (σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
dg
= |σ2(U ′U)−1|
1
2
(2π)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
,
(7.3.15)
and thus,
ˆβ|β,σ2 ∼N

β,σ2(U ′U)−1
.
(7.3.16)
If the joint distribution p(ˆβ,g|β,σ2) is integrated with respect to ˆβ, then
p(g|σ2) =

p(ˆβ,g|β,σ2) dˆβ
=
 |σ2(U ′U)−1|
1
2
(2π)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
×(σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
dˆβ
= (σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
×
 |(U ′U)−1|
1
2
(2πσ2)
(q+1)
2
e−(β−ˆβ)′(U′U)(β−ˆβ)
2σ2
dˆβ
= (σ2)−n−(q+1)
2
g
n−(q+1)−2
2
e−
g
2σ2
Γ

n−(q+1)
2

2
n−(q+1)
2
,
(7.3.17)
© 2003 by Chapman & Hall/CRC

and thus,
g|σ2 ∼W

σ2,1,n−(q +1)

.
(7.3.18)
The joint distribution of ˆβ and g is equal to the product of their marginal
distributions. Note that g/σ2 has the familiar Chi-squared distribution with
n−(q +1) degrees of freedom.
By changing variables from ˆβ and g to t = [n−(q+1)]
1
2 g−1/2(U ′U)
1
2 (ˆβ −β)
and w = g, the joint distribution of t and w becomes
p(t,w|β,σ2) = (σ2)−n
2 w
n−2
2 e
−w
2σ2
h
t′t
n−(q+1) +1
i
[(n−(q +1))π]
1
2 Γ

n−(q+1)
2

2
n
2
,
(7.3.19)
where the Jacobian of the transformation was
J(ˆβ,g →t,w) = [n−(q +1)]−(q+1)
2
w
(q+1)
2
|U ′U|−1
2 .
(7.3.20)
Now integrate with respect to w to ﬁnd the distribution of t (unconditional
on w). This is done by making the integrand look like the Scalar Wishart
distribution. It is seen that the integrand needs to multiplied and divided by
the same factor.
p(t|β) =

p(t,w|β,σ2) dw
∝

(σ2)−n
2 w
n−(q+1)
2
e
−w
2σ2

t′t
n−(q+1) +1

dw
∝
	
t′t
n−(q +1) +1

−n
2
×
 	
t′t
n−(q +1) +1

 n
2
w
n−2
2 e
−w
2σ2

t′t
n−(q+1) +1

dw
∝
	
t′t
n−(q +1) +1

−n
2
(7.3.21)
and now transforming back while taking g to be known
p(ˆβ|β,g) ∝

g
n−(q+1)(U ′U)
−1
2

1+
1
n−(q+1)

ˆβ −β
′ 
g
n−(q+1)(U ′U)−1
−1 
ˆβ −β
) n
2 . (7.3.22)
© 2003 by Chapman & Hall/CRC

Recall the Multivariate Student t-distribution in the statistical distribution
Chapter. The estimate of the vector of regression coeﬃcients ˆβ has the familiar
Multivariate Student t-distribution with n−(q +1) degrees of freedom.
7.4
Multiple Linear Regression
Similarly, the multiple Regression model
xi = β0 +β1ui1 +···+βquiq +ǫi,
(7.4.1)
where ǫi is the Scalar Normally distributed random error with mean zero and
variance σ2,
ǫi ∼N(0,σ2)
(7.4.2)
for i = 1,...,n. The joint distribution of these variables or likelihood is
p(x1,...,xn|β,σ2,U) =
n

i=1
f(xi|β,σ2,ui)
=
n

i=1
(2πσ2)−1
2 e−
(xi−β0−β1ui1−···−βquiq)2
2σ2
= (2πσ2)−n
2 e−
1
2σ2
Pn
i=1(xi−β0−β1ui1−···−βquiq)2
.
(7.4.3)
This model can also be written in terms of vectors and matrices
x =



x1
...
xn


,
ui =





1
ui1
...
uiq




, U =



u′
1
...
u′
n


, β =



β0
...
βq


, ǫ =



ǫ1
...
ǫn


, (7.4.4)
so that the model is
x
=
U
β
+
ǫ
n×1
n×(q +1) (q +1)×1
n×1
(7.4.5)
and the likelihood is
© 2003 by Chapman & Hall/CRC

p(x|β,σ2,U) = (2πσ2)−n
2 e−(x−Uβ)′(x−Uβ)
2σ2
.
(7.4.6)
The value of β which maximizes the likelihood can be found the same way
as before, namely, ˆβ = (U ′U)−1U ′x. The value of σ2 that makes the likelihood
a maximum can also be found, namely, σ2 = ˆσ2 which is deﬁned as before.
By the Neyman factorization criterion [22, 47], ˆβ and ˆσ2 are independent
in a similar way as in the Normal Samples and simple linear Regression mod-
els. Also in a similar way as in the Normal Samples model and the simple
linear Regression model, the distribution of ˆβ|β,σ2, g|σ2, and ˆβ|β can also be
similarly found.
7.5
Multivariate Linear Regression
The Multivariate Regression model is a generalization of the multiple Re-
gression model to vector valued dependent variable observations.
Previously for the simple linear Regression, (xi,ui) pairs were observed,
i = 1,...,n, where xi was a scalar but ui was a (1 + 1) dimensional vector
containing a 1 and an observable u, ui1. We adopted the model
xi = β′ui +ǫi
(7.5.1)
which was adopted where
β =
	
β0
β1

,
ui =
	
1
ui1

,
(7.5.2)
and then ﬁt a simple line to the data.
In the multiple Regression model, (xi,ui) pairs were observed, i = 1,...,n
where xi was a scalar but ui was a (q +1) dimensional vector containing a 1
and q observable u’s, ui1,...,uiq. The model is
xi
=
β′
ui
+
ǫi
1×1
1×(q +1) (q +1)×1
1×1
(7.5.3)
which was adopted where
β =



β0
...
βq


,
ui =





1
ui1
...
uiq




,
(7.5.4)
© 2003 by Chapman & Hall/CRC

and then ﬁt a line in space to the data.
In the Multivariate Regression model, (xi,ui) pairs are observed, i = 1,...,n
where xi is a p-dimensional vector and ui is a (q + 1)-dimensional vector
containing a 1 and q observable u’s, ui1,...,uiq. The model
xi
=
B
ui
+
ǫi
p×1
p×(q +1) (q +1)×1
p×1
(7.5.5)
where ui is as in the multiple Regression model and
xi =



xi1
...
xip


,
βj =



βj0
...
βjq


,
B =



β′
1
...
β′
p


,
ǫi =



ǫi1
...
ǫip


.
(7.5.6)
Taking a closer look at this model,



xi1
...
xip


=



β10 ... β1q
...
βp0 ... βpq








1
ui1
...
uiq





+



ǫi1
...
ǫip


,
p×1
p×(q +1)
(q +1)×1
p×1
(7.5.7)
which means that for each observation, which is a row in the left-hand side of
the model, there is a Regression. Each row has its own Regression complete
with its own set of Regression coeﬃcients.
Just like in the simple and multiple Regression models, we specify that the
errors are normally distributed. However, the Multivariate Regression model
has vector-valued observations and vector-valued errors. The error vector ǫi
has a Multivariate Normal distribution with a zero mean vector and positive
deﬁnite covariance matrix Σ
ǫi ∼N(0,Σ)
(7.5.8)
for i = 1,...,n. The resulting distribution for a given observation vector has
a Multivariate Normal distribution
p(xi|B,Σ,ui) = (2π)−p
2 |Σ|−1
2 e−1
2 (xi−Bui)′Σ−1(xi−Bui),
(7.5.9)
because the Jacobian of the transformation is unity.
Note: If each of the elements of vector xi were independent, then Σ would
be diagonal. Since we assume that each variable has its own distinct error
term (i.e., its own σ2
j = Σjj), Σ = diag(σ2
1,...,σ2
p) and p(xi|B,Σ,ui) break
down into
© 2003 by Chapman & Hall/CRC

p(xi|B,Σ,ui) = p(xi1,...,xip|β,σ2,ui)
=
p

j=1
p(xij|βj,σ2
j ,uij).
(7.5.10)
This is what we would get if we just collected independent observations from
the multiple Regression model. This means add a subscript j to the multi-
ple Regression model for each of the j = 1,...,p variables and collect them
together.
Returning to the vector-valued observations with dependent elements, we
have observations for i = 1,...,n whose likelihood is
p(x1,...,xn|B,Σ,U) =
n

i=1
p(xi|B,Σ,U)
=
n

i=1
(2π)−p
2 |Σ|−1
2 e−1
2 (xi−Bui)′Σ−1(xi−Bui)
= (2π)−np
2 |Σ|−n
2 e−1
2
Pn
i=1(xi−Bui)′Σ−1(xi−Bui).
(7.5.11)
If all of the n observation and error vectors on p variables are collected
together as
X =



x′
1
...
x′
n


,
U =



u′
1
...
u′
n


,
E =



ǫ′
1
...
ǫ′
n


,
(7.5.12)
then the model can be written as
X
=
U
B′
+
E
n×p
n×(q +1) (q +1)×p
n×p
(7.5.13)
and the likelihood
p(X|B,Σ,U) = (2π)−np
2 |Σ|−n
2 e−1
2 tr(X−UB′)Σ−1(X−UB′)′
= (2π)−np
2 |Σ|−n
2 e−1
2 trΣ−1(X−UB′)′(X−UB′),
(7.5.14)
where tr(·) is the trace operator which gives the sum of the diagonal elements
of its matrix argument.
The natural logarithm of p(X|B,Σ,U) can be taken and diﬀerentiated with
respect to the matrix B and the matrix Σ. But just as with the simple and
© 2003 by Chapman & Hall/CRC

multiple Regression models, some algebra on the exponent can be performed
to ﬁnd the value of B which yields the maximal value of this likelihood.
(X −UB′)′(X −UB′) = X′X −X′UB′ −BU ′X +BU ′UB′
= B[U ′UB′ −U ′X]−X′UB′ +X′X
= B(U ′U)[B′ −(U ′U)−1U ′X]−X′UB′ +X′X
= B(U ′U)(B′ −ˆB′)−X′UB′ +X′X
= (B −ˆB)(U ′U)(B −ˆB)′ + ˆB(U ′U)(B −ˆB)′
−X′UB′ +X′X
= (B −ˆB)(U ′U)(B −ˆB)′ + ˆB(U ′U)B′ −ˆB(U ′U) ˆB′
−X′UB′ +X′X
= (B −ˆB)(U ′U)(B −ˆB)′ +X′U(U ′U)−1(U ′U)B′
−X′U(U ′U)−1(U ′U)(U ′U)−1U ′X −X′UB′ +X′X
= (B −ˆB)(U ′U)(B −ˆB)′
−X′U(U ′U)−1(U ′U)(U ′U)−1U ′X +X′X
= (B −ˆB)(U ′U)(B −ˆB)′ +X′[In −U(U ′U)−1U ′]X.
(7.5.15)
The likelihood is now
p(X|B,Σ,U) =
(2π)−np
2 |Σ|−n
2 e−1
2 trΣ−1[(B−ˆ
B)(U′U)(B−ˆ
B)′+X′[In−U(U′U)−1U′]X].
(7.5.16)
It is now obvious that the value of B which maximizes the likelihood or
minimizes the exponent in the likelihood is ˆB′ = (U ′U)−1U ′X.
It is also now evident by making an analogy to the simple and multiple
Regression models that the value of Σ which maximizes the likelihood is
ˆΣ = (X −U ˆB′)′(X −U ˆB′)
n
.
(7.5.17)
This could have also been proved by diﬀerentiation.
Recall that when the univariate or Scalar Normal distribution was intro-
duced, the estimates of the mean ˆµ and the variance ˆσ2 were independent.
Also in the simple and multiple linear Regression models the estimates of the
Regression coeﬃcients and the variance were independent. A similar thing is
true in Multivariate Regression, namely, that ˆB and ˆΣ are independent.
Consider the numerator of the estimate of Σ, ˆΣ.
© 2003 by Chapman & Hall/CRC

NUM = (X −U ˆB′)′(X −U ˆB′)
= X′X −X′U ˆB′ −ˆBU ′X + ˆBU ′U ˆB′
= ˆB(U ′U ˆB′ −U ′X)−X′U ˆB′ +X′X
= ˆB(U ′U)( ˆB′ −(U ′U)−1U ′X)−X′U ˆB′ +X′X
= ( ˆB −X′U(U ′U)−1)(U ′U)( ˆB −X′U(U ′U)−1)′
+X′U(U ′U)−1(U ′U) ˆB −X′U(U ′U)−1(U ′U)(U ′U)−1U ′X
−X′U ˆB +X′X
= X′X −X′U(U ′U)−1U ′X
= X′[In −U(U ′U)−1U ′]X.
(7.5.18)
This is exactly the G term in the likelihood. Now, the likelihood can be
written as
p(X|B,Σ,U) = (2π)−np
2 |Σ|−n
2 e−1
2 trΣ−1[(B−ˆ
B)(U′U)(B−ˆ
B)′+(X−U ˆ
B′)′(X−U ˆ
B′)],
(7.5.19)
where G = (X −U ˆB′)′(X −U ˆB′).
Just as in the Normal Samples and the multiple linear Regression models,
if we ignore the normalization coeﬃcients by using proportionality, partition
the likelihood, and view this as a distribution of ˆB and G, then
p( ˆB,G|B,Σ,U) ∝|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B)
 
!"
#
ˆ
B|B,Σ∼N(B,Σ⊗(U′U)−1)
×|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G
 
!"
#
G|Σ∼W (Σ,p,n−(q+1))
, (7.5.20)
where the ﬁrst term involves ˆB and the second G.
By the Neyman factorization criterion [22, 41, 47], ˆB|B,Σ and G|Σ are
independent. This is proven by showing that
p( ˆB,G|B,Σ) = p( ˆB|B,Σ)p(G|B,Σ).
(7.5.21)
If we integrate p( ˆB,G|B,Σ) with respect to G, then
p( ˆB|Σ,U) =

p( ˆB,G|B,Σ) dG
∝

|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B)
© 2003 by Chapman & Hall/CRC

×|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G dG
∝|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B)
×

|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G dG
∝|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B),
(7.5.22)
where the matrix ˆB follows a Matrix Normal distribution
ˆB|B,Σ ∼N(B,Σ⊗(U ′U)−1).
(7.5.23)
If we integrate p( ˆB,G|B,Σ) with respect to ˆB, then
p(G|Σ,U) =

p( ˆB,G|B,Σ) d ˆB
∝

|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B)
×|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G d ˆB
∝|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G
×

|Σ|−q+1
2 e−1
2 tr(U′U)( ˆ
B−B)′Σ−1( ˆ
B−B) d ˆB
∝|Σ|−n−(q+1)
2
|G|
n−(q+1)−p−1
2
e−1
2 trΣ−1G,
(7.5.24)
where the matrix G = nˆΣ = (X −U ˆB′)′(X −U ˆB′) follows a Wishart distrib-
ution
G|Σ ∼W(Σ,p,n−(q +1)).
(7.5.25)
By changing variables from ˆB and G to T = [n−(q+1)]
1
2 G−1
2 ( ˆB−B)(U ′U)
1
2
and W = G in a similar fashion as was done before, the distribution of
p(T,W|B,Σ,U) is
p(T,W|B,Σ,U) ∝|Σ|−n
2 |W|
n−p−1
2
e−1
2 trΣ−1[
1
n−q−1 W
1
2 T T ′W
1
2 +W ]
∝|Σ|−n
2 |W|
n−p−1
2
e
−1
2 tr
n
Σ−1[
1
n−q−1 T T ′+Ip]
o
W , (7.5.26)
where the Jacobian of the transformation [41] was
J( ˆB,G →T,W) = [n−(q +1)]
p(q+1)
2
|W|
q+1
2 |U ′U|−p
2 .
(7.5.27)
The distribution of T unconditional of Σ is found by integrating p(T,W|B,Σ,U)
with respect to W,
© 2003 by Chapman & Hall/CRC

p(T|B,U) ∝

p(T,W|B,Σ,U) dW
∝

|Σ|−n
2 |W|
n−p−1
2
e
−1
2 tr
n
Σ−1[
1
n−q−1 T T ′+Ip]
o
W dW
∝

1
n−q −1TT ′ +Ip
−n
2
×

|Σ−1[
1
n−q −1TT ′ +Ip]|−n
2 |W|
n−p−1
2
×e
−1
2 tr
n
Σ−1[
1
n−q−1 T T ′+Ip]
o
W dW
∝[Ip +
1
n−q −1TT ′]−n
2
(7.5.28)
and now transforming back while taking G to be known
p( ˆB|B,G,U) ∝
G+
1
n−(q +1)( ˆB −B)[n−(q +1)](U ′U)( ˆB −B)′

−n
2
(7.5.29)
which is a Matrix Student T-distribution. That is,
ˆB|B,G,U ∼T

n−q −1,B,[(n−q −1)(U ′U)]−1 ,G

.
(7.5.30)
Note that the exponent can be written as n−(q+1)+q+1 and compared to
the deﬁnition of the Matrix Student T-distribution given in Equation 2.3.39.
This is the conditional posterior distribution of ˆB given G. Conditional max-
imum a posteriori variance estimates can be found. The conditional modal
variance is
var( ˆB|G,X,U) =
n−(q +1)
n−(q +1)−2 G⊗[(n−q −1)(U ′U)]
−1
(7.5.31)
or equivalently
var(ˆβ|G,X,U) =
n−(q +1)
n−(q +1)−2 [(n−q −1)(U ′U)]−1 ⊗G
(7.5.32)
= ˆ∆,
(7.5.33)
where ˆβ = vec( ˆB).
Hypotheses can be performed regarding the entire coeﬃcient matrix, a sub-
matrix, a particular row or column, or a particular element.
Signiﬁcance for the entire coeﬃcient matrix can be evaluated with the use
of
© 2003 by Chapman & Hall/CRC

|I +G−1 ˆB(U ′U) ˆB′|−1
(7.5.34)
which follows a distribution of the product of independent Scalar Beta vari-
ates [12, 41]. General simultaneous hypotheses (which do not assume indepen-
dence) can be performed regarding the entire Regression coeﬃcient matrix.
A similar result holds for a submatrix.
The above distribution for the matrix of regression coeﬃcients can be writ-
ten in another form
p( ˆB|X,G,U,B) ∝|W +( ˆB −B)′G−1( ˆB −B)|−(n−q−1)+(q+1)
2
.
(7.5.35)
by using Sylvester’s theorem [41]
|In +Y IpY ′| = |Ip +Y ′InY |,
(7.5.36)
where W = (U ′U)−1 has been deﬁned.
It can be shown [17, 41] that the marginal distribution of any column, say
the kth of the matrix of ˆB, ˆBk is Multivariate Student t-distributed
p( ˆBk|Bk,G,X,U) ∝
1

Wkk +( ˆBk −Bk)′G−1( ˆBk −Bk)
 (n−q−p)+p
2
,
(7.5.37)
where Wkk is the kth diagonal element of W, k = 0,...,q.
It can be also be shown [17, 41] that the marginal distribution of any row,
say the jth of the matrix of ˆB, ˆβ′
j is Multivariate Student t-distributed
p(ˆβj|βj,G,X,U) ∝
1

Gjj +(ˆβj −βj)′W −1(ˆβj −βj)
 (n−q−p)+(q+1)
2
,
(7.5.38)
where Gjj is the jth diagonal element of G, j = 1,...,p.
With the marginal distribution of a column or row of ˆB, signiﬁcance can
be evaluated for the coeﬃcient of a particular independent variable (column
of ˆB) with the use of the statistic
Fp,n−q−p = n−q −p
p
W −1
kk ˆB′
kG−1 ˆBk,
(7.5.39)
and for the coeﬃcients of a particular dependent variable (row of ˆB) with the
use of the statistic
Fq+1,n−q−p = n−q −p
q +1
G−1
jj ˆβ′
jW −1 ˆβj.
(7.5.40)

It can be shown that these statistics follow F-distributions with (p,n−q−p) or
(q +1,n−q −p) numerator and denominator degrees of freedom respectively.
Signiﬁcance can be determined for a subset of variables by determining the
marginal distribution of the subset within ˆBk or ˆβ′
j which are also Multivariate
Student t-distributed.
It should also be noted that
b =

1+ ν1
ν2
Fν1,ν2
−1
(7.5.41)
follows the Scalar Beta distribution B
 ν2
2 , ν1
2

.
With the subset of variables being a singleton set, signiﬁcance can be de-
termined for a particular independent variable with the marginal distribution
of the scalar coeﬃcient which is
p( ˆBkj|Bkj,Gjj,X,U) ∝
1

Wkk +( ˆBkj −Bkj)G−1
jj ( ˆBkj −Bkj)
 (n−q−p)+1
2
,
(7.5.42)
where Gjj = (Xj −U ˆβj)′(Xj −U ˆβj) is the jth diagonal element of G. The
above can be rewritten in the more familiar form
p( ˆBkj|Bkj,Gjj,X,U) ∝
1

1+
1
(n−q−p)
( ˆ
Bkj−Bkj)2
Wkk[Gjj/(n−q−p)]
 (n−q−p)+1
2
(7.5.43)
which is readily recognizable as a Scalar Student t-distribution. Note that
ˆBkj = ˆβjk and that
t =
( ˆBkj −Bkj)
[WkkGjj(n−q −p)−1]
1
2
(7.5.44)
follows a Scalar Student t-distribution with n−q −p degrees of freedom, and
t2 follows an F distribution with 1 and n−q −p numerator and denominator
degrees of freedom, which is commonly used in Regression [1, 68], derived
from a likelihood ratio test of reduced and full models when testing a sin-
gle coeﬃcient. By using a t statistic instead of an F statistic, positive and
negative coeﬃcient values can be identiﬁed.
It should also be noted that
b =

1+
1
n−q −pt2
−1
(7.5.45)
follows the Scalar Beta distribution B
 n−q−p
2
, 1
2

.
© 2003 by Chapman & Hall/CRC

Exercises
1. Assume that we perform an experiment in which we have a multiple
Regression, p = 1, n = 8, with the data vector and design matrix as
given in Table 7.1.
TABLE 7.1
Regression data vector and design matrix.
x
1
U
en
1
2
1
12.0719
1
1
1
1
2
15.7134
2
1
2
1
3
20.2977
3
1
3
1
4
24.2973
4
1
4
1
5
15.9906
5
1
5
-1
6
20.0818
6
1
6
-1
7
24.0437
7
1
7
-1
8
27.9533
8
1
8
-1
We wish to ﬁt the multiple Regression model
xi = β0 +β1ui1 +β2ui2 +ǫi
x = Uβ +ǫ.
What is the estimate ˆβ of β? What is the estimate ˆσ2 of σ2?
2. What is the distribution and associated parameters of the vector of
Regression coeﬃcients?
3. Compute
Fq+1,n−(q+1) =
ˆβ′(U ′U)ˆβ/(q +1)
g/(n−q −1)
and
tk =
ˆβk
[Wkkg/(n−q −1)]
1
2
for k = 0,...,q from the data in Exercise 1.
4. Assume that we perform an experiment in which p = 3, q = 2, and n = 32
with data matrix X and design U matrix (including a column of ones
for the intercept term) as in Table 7.2.
© 2003 by Chapman & Hall/CRC

TABLE 7.2
Regression data and design matrices.
X
1
2
3
U
en
1
2
1
12.1814
9.3538
20.9415
1
1
1
1
2
15.8529
11.7987
28.0296
2
1
2
1
3
20.5458
15.1322
35.0787
3
1
3
1
4
23.9659
18.0548
42.3609
4
1
4
1
5
28.0285
20.7695
48.9123
5
1
5
1
6
32.2667
23.4573
56.1558
6
1
6
1
7
36.0148
26.9852
63.1998
7
1
7
1
8
39.9761
29.7473
70.2352
8
1
8
1
9
31.7919
23.1536
58.7520
9
1
9
-1
10
36.0736
26.1269
66.0530
10
1
10
-1
11
39.6660
29.4231
73.0595
11
1
11
-1
12
44.1786
32.1478
79.7481
12
1
12
-1
13
48.4059
34.8391
86.8145
13
1
13
-1
14
51.8271
38.0951
94.2706
14
1
14
-1
15
56.2145
40.7477
100.9671
15
1
15
-1
16
60.3135
43.9951
108.0975
16
1
16
-1
17
75.6016
56.9879
133.0220
17
1
17
1
18
79.6398
60.0000
139.8411
18
1
18
1
19
84.1428
62.9205
146.8601
19
1
19
1
20
87.9000
66.2738
154.1109
20
1
20
1
21
92.1725
68.5315
160.7625
21
1
21
1
22
96.2039
72.1070
168.1953
22
1
22
1
23
100.1780
75.2239
175.1422
23
1
23
1
24
104.3226
78.1827
181.7946
24
1
24
1
25
96.1672
71.1445
170.9336
25
1
25
-1
26
100.2977
74.0101
177.7031
26
1
26
-1
27
103.6994
77.1693
184.4494
27
1
27
-1
28
107.9951
80.1422
192.2466
28
1
28
-1
29
111.9608
82.9361
198.8703
29
1
29
-1
30
115.5990
85.9056
206.0818
30
1
30
-1
31
120.0643
88.9260
213.0585
31
1
31
-1
32
123.7359
91.6312
220.0054
32
1
32
-1
We wish to ﬁt the multiple Regression model
xi = Bui +ǫi
X = UB′ +E.
What is the estimate ˆB of B? What is the estimate ˆΣ of Σ?
5. What is the distribution and associated parameters of the matrix of
Regression coeﬃcients in Exercise 4?
© 2003 by Chapman & Hall/CRC

6. Compute
Fp,n−q−p = n−q −p
p
W −1
kk ˆB′
kG−1 ˆBk
for k=0,...,q,
Fq+1,n−q−p = n−q −p
q +1
G−1
jj ˆβ′
jW −1 ˆβj.
for j = 1,...,p, and
tkj =
ˆBkj
[WkkGjj/(n−q −p)]
1
2
(7.5.46)
for all kj combinations from the data in Exercise 4.
7. Recompute the statistics in Exercise 6 assuming p = 1, i.e. the observa-
tions are independent. Compare your results to those in Exercise 6.
© 2003 by Chapman & Hall/CRC

Part II
Models
© 2003 by Chapman & Hall/CRC

8
Bayesian Regression
8.1 
Intro duction
The Multivariate Bayesian Regression model [41] is introduced that re-
quires some knowledge of Multivariate as well as Bayesian Statistics. A concise
introduction to this material was presented in Part I. This Chapter consid-
ers the unobservable sources (the speakers conversations in the cocktail party
problem) to be observable or known. This is done so that the Bayesian Re-
gression model may be introduced which will help lead to the Bayesian Source
Separation models.
8.2 
The Bayesian Regression Mo del
A Regression model is used when it is believed that a set of dependent
variables may be represented as being linearly related to a set of independent
variables. The model may be motivated using a Taylor series expansion to
represent a functional relationship as was done in Chapter 1 for the Source
Separation model.
The p-dimensional recorded values are denoted as xi’s and the q emitted
values along with the overall mean by the (q +1)–dimensional ui’s. The ui’s
are of dimension (q +1) because they contain a 1 as their ﬁrst element for the
overall mean (constant or intercept). The q speciﬁed to be observable sources,
the uik’s are used for distinction from the m unobservable sources, the sik’s
in Chapter 1. The mixing coeﬃcients are denoted by β’s to denote that they
are coeﬃcients for observable variables (sources) and not λ’s for unobservable
variables (sources).
Given a set of independent and identically distributed vector-valued obser-
vations xi, i = 1,...,n, on p possibly correlated random variables, the Multi-
variate Regression model on the variable ui is
(xi|B,ui) =
B
ui
+
ǫi,
(p×1)
[p×(q +1)] [(q +1)×1]
(p×1)
(8.2.1)
© 2003 by Chapman & Hall/CRC

where the matrix of Regression coeﬃcients B is given by
B =



β′
1
...
β′
p


,
(8.2.2)
which describes the relationship between ui = (1,ui1,...,uiq)′ and xi while ǫi
is the p-dimensional Normally distributed error.
Note that if the coeﬃcient matrix B were written as B = (µ,B⋆) and ui =
(1,u′
⋆i)′, then
(xi|µ,B⋆,ui) =
µ
+
B⋆
u⋆i
+
ǫi,
(p×1)
(p×1)
(p×q) (q ×1)
(p×1)
(8.2.3)
which is the same basic model as in Source Separation.
More speciﬁcally for the Regression model, a given element of xi, say the
jth is
(xij|βj,ui) =
β′
j
ui
+
ǫij,
(1×1)
[1×(q +1)] [(q +1)×1]
(1×1)
(8.2.4)
where β′
j = (βj0,...,βjq). This is also represented as
(xij|βj,ui) =
q

t=0
βjt uit +ǫij.
(8.2.5)
Gathering all observed vectors into a matrix, the Regression model may be
written as
(X|B,U) =
U
B′
+
E,
(n×p)
[n×(q +1)] [(q +1)×p]
(n×p)
(8.2.6)
where the matrix of dependent variables (observed mixed sources) X, the
independent variables (observable sources) U, and the matrix of errors E are
deﬁned to be
X =



x′
1
...
x′
n


,
U =



u′
1
...
u′
n


,
E =



ǫ′
1
...
ǫ′
n


,
(8.2.7)
while B is as above.
The ijth element of X is the ith row of U multiplied by the jth column of
B′ plus the ijth element of E. The model may also be written in terms of
columns by parameterizing the dependent variables (observed mixed sources)
X, the independent variables (observable source) matrix U, the matrix of
Regression coeﬃcients B, and the matrix of errors E as
© 2003 by Chapman & Hall/CRC

X = (X1,...,Xp),
U = (en,U1,...,Uq),
B′ = (β1,β1,...,βp),
E = (E1,...,Ep),
(8.2.8)
where en is an n-dimensional column vector of ones.
This leads to the model
(Xj|βj,U) =
U
βj
+
Ei,
(n×1)
[n×(q +1)] [(q +1)×1]
(n×1)
(8.2.9)
which describes all the observations for a single microphone in the cocktail
party problem at all n time points.
8.3 
Likeliho o d
In observing variables, there is inherent random variability or error which
statistical models quantify.
The errors of observation are characterized to
have arisen from a particular distribution. It is speciﬁed that the errors of
observation are independent and Normally distributed random vectors with
p-dimensional mean 0 and p × p covariance matrix Σ. The Multivariate p-
dimensional Normal distribution for the errors described in Chapter 2 is de-
noted by
p(ǫi|Σ) ∝|Σ|−1
2 e−1
2 ǫ′
iΣ−1ǫi,
(8.3.1)
where ǫi is the p-dimensional error vector. It is common in Bayesian Statistics
to omit the normalization constant and use proportionality.
From this Multivariate Normal error speciﬁcation, the distribution of the
observation vectors is also Multivariate Normally distributed and given by
p(xi|B,ui,Σ) ∝|Σ|−1
2 e−1
2 (xi−Bui)′Σ−1(xi−Bui),
(8.3.2)
because the Jacobian of the transformation from ǫi to xi is unity.
With the matrix representation of the model given by Equation 8.2.6, the
distribution of the matrix of observations is a Matrix Normal distribution as
described in Chapter 2
p(X|B,Σ,U) ∝|Σ|−n
2 e−1
2 tr(X−UB′)Σ−1(X−UB′)′,
(8.3.3)
where “tr” denotes the trace operator which yields the sum of the diagonal
elements of its matrix argument.
© 2003 by Chapman & Hall/CRC

8.4 
Conjugate Priors a nd Po sterior
In the Bayesian approach to statistical inference, available prior informa-
tion regarding parameter values is quantiﬁed in terms of prior distributions
to represent the current state of knowledge before an experiment is performed
and data taken. In this Section, Conjugate prior distributions are used to
characterize our beliefs regarding the parameters, namely, the Normal and
the Inverted Wishart which are described in Chapters 2 and 4. Later in this
Chapter, generalized Conjugate prior distributions will be used.
Using the Conjugate procedure given in Chapter 4, the joint prior distri-
bution is p(B,Σ) for the parameters B, and Σ is the product of the prior
distribution of B given Σ, p(B|Σ), and the prior distribution of Σ, p(Σ). This
is expressed as
p(B,Σ) = p(B|Σ)p(Σ),
(8.4.1)
where the prior distribution for the Regression coeﬃcients B|Σ is Matrix
Normally distributed as
p(B|Σ) ∝|D|−p
2 |Σ|−(q+1)
2
e−1
2 trD−1(B−B0)′Σ−1(B−B0)
(8.4.2)
and the prior distribution for the error covariance matrix Σ is Inverse Wishart
distributed as
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q.
(8.4.3)
The quantities D, B0, ν, and Q are hyperparameters to be assessed. By speci-
fying these hyperparameters, the entire joint prior distribution is determined.
The joint posterior distribution of the model parameters B and Σ with
speciﬁed Conjugate priors is
p(B,Σ|X,U) ∝p(Σ)p(B|Σ)p(X|B,Σ,U)
(8.4.4)
and after inserting the aforementioned prior distributions and likelihood be-
comes
p(B,Σ|X,U) ∝|Σ|−(n+ν+q+1)
2
×e−1
2 trΣ−1[(X−UB′)′(X−UB′)+(B−B0)D−1(B−B0)′+Q],
(8.4.5)
where the property of the trace operator that the trace of the product of two
conformable matrices is equal to the trace of their product in the reverse order
was used.
© 2003 by Chapman & Hall/CRC

8.5 
Conjugate Estimation a nd Inference
The estimation of and inference on parameters as outlined in Chapter 6 in
a Bayesian Statistical procedure is often the most diﬃcult part of the analysis.
Two diﬀerent methods are used to estimate parameters and draw inferences.
The methods of estimation are marginal posterior mean and maximum a pos-
teriori estimates. Formulas for marginal mean and joint maximum a posteriori
estimates are derived.
8.5.1
Marginalization
Marginal posterior mean estimation of the parameters involves comput-
ing the marginal posterior distribution for each of the parameters and then
computing mean estimates from these marginal posterior distributions.
To ﬁnd the marginal posterior distribution of the matrix of Regression co-
eﬃcients B, the joint posterior distribution Equation 8.4.5 must be integrated
with respect to Σ. This can be performed easily by recognizing (as described
in Chapter 6) that the posterior distribution is exactly of the same form as
an Inverted Wishart distribution except for a proportionality constant. Inte-
gration can be easily performed using the deﬁnition of an Inverted Wishart
distribution. The marginal posterior distribution for the matrix of Regression
coeﬃcients B after integrating with respect to Σ is given by
p(B|X,U) ∝
1
|G+(B −¯B)(D−1 +U ′U)(B −¯B)′|
(n+ν−p−1+q+1)
2
,
(8.5.1)
where the matrix
¯B = (X′U +B0D−1)(D−1 +U ′U)−1,
(8.5.2)
and B0 is the prior mean while the p ×p matrix G (after some algebra) has
been written as
G = Q+X′X +B0D−1B′
0
−(X′U +B0D−1)(D−1 +U ′U)−1(X′U +B0D−1)′
(8.5.3)
have been deﬁned.
The mean and modal estimate of the matrix of Regression coeﬃcients from
this exact marginal posterior distribution is ¯B. The marginal posterior vari-
ance of the matrix of Regression coeﬃcients is
© 2003 by Chapman & Hall/CRC

var(B| ¯B,X,U) =
n−(q +1)
n−(q +1)−2 G⊗[(n−q −1)(U ′U)]−1
(8.5.4)
or equivalently
var(β|¯β,X,U) =
n+ν −p−1
n+ν −p−1−2 [(n−q −1)(U ′U)]−1 ⊗G,
(8.5.5)
= ˆ∆,
(8.5.6)
where β = vec(B).
This marginal posterior distribution is easily recognized as being a Matrix
Student T-distribution as described in Chapter 2. That is,
B| ¯B,U ∼T

n+ν −p−1, ¯B,
*
(n+ν −p−1)(D−1 +U ′U)
+−1 ,G

.
(8.5.7)
Inferences such as tests of hypothesis and credibility intervals can be eval-
uated on B as in the Regression Chapter. Hypotheses can be performed re-
garding the entire coeﬃcient matrix, a submatrix, a particular row or column,
or a particular element.
The above distribution for the matrix of regression coeﬃcients can be writ-
ten in another form
p(B|X,U, ¯B) ∝
1
|W +(B −¯B)′G−1(B −¯B)|
(n∗−q−p)+(q+1)
2
(8.5.8)
by using Sylvester’s theorem [41]
|In +Y IpY ′| = |Ip +Y ′InY |,
(8.5.9)
where W =

D−1 +U ′U
−1 and n∗= n+ν −p+2q +1 have been deﬁned.
It can be shown that the marginal distribution of any column, say the kth
of the matrix of B, Bk is Multivariate Student t-distributed
p(Bk| ¯Bk,X,U) ∝
1
*
Wkk +(Bk −¯Bk)′G−1(Bk −¯Bk)
+ (n∗−q−p)+p
2
,
(8.5.10)
where Wkk is the kth diagonal element of W.
It can be also be shown that the marginal distribution of any row, say the
jth of the matrix B, β′
j is Multivariate Student t-distributed
p(βj|¯βj,X,U) ∝
1
*
Gjj +(βj −¯βj)′W −1(βj −¯βj)
+ (n∗−q−p)+(q+1)
2
,
(8.5.11)
© 2003 by Chapman & Hall/CRC

where Gjj is the jth diagonal element of G.
With the marginal distribution of a column or row of B, signiﬁcance can
be evaluated for the coeﬃcients of a particular independent variable (column
of B) with the use of the statistic
Fp,n∗−q−p = n∗−q −p
p
W −1
kk ¯B′
kG−1 ¯Bk,
(8.5.12)
or for the coeﬃcients of a particular dependent variable (row of B) with the
use of the statistic
Fq+1,n∗−q−p = n∗−q −p
q +1
G−1
jj ¯β′
jW −1 ¯βj.
(8.5.13)
These statistics follow F-distributions with either (p,n∗−q −p) or (q +1,n∗−
q −p) numerator and denominator degrees of freedom respectively [39, 41].
Signiﬁcance can be determined for a subset of coeﬃcients by determining the
marginal distribution of the subset within Bk or β′
j which is also Multivariate
Student t-distributed.
With the subset of coeﬃcients being a singleton set, signiﬁcance can be
determined for a particular coeﬃcient with the marginal distribution of the
scalar coeﬃcient which is
p(Bkj| ¯Bkj,X,U) ∝
1
*
Wkk +(Bkj −¯Bkj)G−1
jj (Bkj −¯Bkj)
+ (n∗−q−p)+1
2
, (8.5.14)
where Gjj is the jth diagonal element of G. The above can be rewritten in
the more familiar form
p(Bkj| ¯Bkj,X,U) ∝
1

1+
1
(n∗−q−p)
(Bkj−¯
Bkj)2
Wkk[Gjj/(n∗−q−p)]
 (n∗−q−p)+1
2
(8.5.15)
which is readily recognizable as a Scalar Student t-distribution. Note that
¯Bkj = ¯βjk and that
t =
(Bkj −¯Bkj)
[WkkGjj(n∗−q −p)−1]
1
2
(8.5.16)
follows a Scalar Student t-distribution with n∗−q −p degrees of freedom, and
t2 follows an F-distribution with 1 and n∗−q −p numerator and denominator
degrees of freedom. The F-distribution is commonly used in Regression [1, 68]
and derived from a likelihood ratio test of reduced and full models when
testing coeﬃcients. By using a t statistic instead of an F statistic, positive
and negative coeﬃcient values can be identiﬁed. Even for a modest sample
size, this Scalar Student t-distribution typically has a large number of degrees
© 2003 by Chapman & Hall/CRC

of freedom (n+ν −q−2p+1) so that it is nearly equivalent to a Scalar Normal
distribution as noted in Chapter 2.
Note that the mean of this marginal posterior distribution ¯B can be written
as
¯B = X′U(D−1 +U ′U)−1 +B0D−1(D−1 +U ′U)−1
= ˆB[U ′U(D−1 +U ′U)−1]+B0[D−1(D−1 +U ′U)−1],
(8.5.17)
where we have deﬁned the matrix ˆB = X′U(U ′U)−1. This posterior mean is
a weighted combination of the prior mean B0 from the prior distribution and
the data mean ˆB from the likelihood.
The above mean for the matrix of Regression coeﬃcients can be written as
¯B = ˆB U ′U
n

(nD)−1 + U ′U
n
−1
+B0(nD)−1

(nD)−1 + U ′U
n
−1
,
(8.5.18)
and as the sample size n increases, (nD)−1 approaches the zero matrix, U′U
n
approaches a constant matrix, and the estimate of the matrix of Regression
coeﬃcients B is based only on the data mean ˆB from the likelihood. Thus,
the prior distribution has decreasing inﬂuence as the sample size increases.
This is a feature of Bayesian estimators.
To integrate the joint posterior distribution in order to ﬁnd the marginal
posterior distribution of Σ, rearrange the terms in the exponent of the poste-
rior distribution as in Chapter 6, and complete the square to ﬁnd
p(B,Σ|X,U) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1G
×|Σ|−(q+1)
2
e−1
2 trΣ−1(B−¯
B)(U′U+D−1)(B−¯
B)′, (8.5.19)
where ¯B and G are as previously deﬁned. In the above equation, the last line
with an additional multiplicative constant is a Matrix Normal distribution
(Chapter 2). Using the deﬁnition of a Matrix Normal distribution for the
purpose of integration as in Chapter 6, the marginal posterior distribution for
Σ is
p(Σ|X,U) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1G,
(8.5.20)
where the p×p matrix G is as deﬁned above. It is easily seen that the mean
of this marginal posterior distribution is
¯Σ =
G
n+ν −2p−2,
(8.5.21)
© 2003 by Chapman & Hall/CRC

while the mode is
¯Σmode =
G
n+ν .
(8.5.22)
The mean and mode of an Inverted Wishart distribution are given in Chap-
ter 2.
Since exact marginal estimates were found, a Gibbs sampling algorithm for
computing exact sampling based marginal posterior means and variances does
not need to be given.
8.5.2
Maximum a Posteriori
The joint posterior distribution may also be maximized with respect to B
and Σ by direct diﬀerentiation as described in Chapter 6 to obtain maximum
a posteriori estimates
˜B =
Arg Max
B
p(B|˜Σ,X,U)
(8.5.23)
= ˜B(˜Σ,X,U)
(8.5.24)
˜Σ =
Arg Max
Σ
p(Σ| ˜B,X,U)
(8.5.25)
= ˜Σ( ˜B,X,U).
(8.5.26)
The maximum a posteriori estimators are analogous to the maximum like-
lihood estimates found for the classical Regression model.
Upon performing some simplifying algebra, taking the natural logarithm,
diﬀerentiating the joint posterior distribution in Equation 8.5.19 with respect
to B, the result is
∂
∂B log(p(B,Σ|X)) =
∂
∂B tr(U ′U +D−1)(B −˜B)′Σ−1(B −˜B)
= 2Σ−1(B −˜B)(U ′U +D−1)
(8.5.27)
which upon evaluating this at (B,Σ) = ( ˜B, ˜Σ) and setting it equal to the null
matrix yields a maxima of ˜B = ¯B as given in Equation 8.5.2. Note that a
matrix derivative result as in Chapter 6 was used.
Upon diﬀerentiating the joint posterior distribution in Equation 8.5.19 with
respect to Σ, the result is
∂
∂Σ log(p(B,Σ|X)) = −(n+ν +q +1)
2
∂
∂Σ log|Σ|+ ∂
∂ΣtrΣ−1G
= −(n+ν +q +1)
2
[2Σ−1 −diag(Σ−1)]
+1
2[2G−1 −diag(G−1)],
(8.5.28)
© 2003 by Chapman & Hall/CRC

which upon setting this expression equal to the null matrix and evaluating at
(B,Σ) = ( ˜B, ˜Σ) yields a maxima of
˜Σ =
G
n+ν +q +1.
(8.5.29)
Note that the estimator of ˜B is identical to that of the marginal mean
¯B, but the one of Σ, ˜Σ is diﬀerent by a multiplicative factor. Conditional
maximum a posteriori variance estimates can also be found. The conditional
modal variance of the Regression coeﬃcients is
var(B| ˜B, ˜Σ,X,U) = ˜Σ⊗(D−1 +U ′U)−1
(8.5.30)
or equivalently
var(β|˜β, ˜Σ,X,U) = (D−1 +U ′U)−1 ⊗˜Σ,
= ˜∆,
where β = vec(B).
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
The posterior conditional distribution of the matrix of Regression coeﬃcients
B given the modal values of the other parameters and the data is
p(B| ˜B, ˜Σ,U,X) ∝|(D−1 +U ′U)|
p
2 |˜Σ|−q+1
2
×e−1
2 tr ˜Σ−1(B−˜
B)(D−1+U′U)(B−˜
B)′,
(8.5.31)
which may be also written in terms of vectors as
p(β|˜β, ˜Σ,U,X) ∝|(D−1 +U ′U)−1 ⊗˜Σ|−1
2
×e−1
2 (β−˜β)′[(D−1+U′U)−1⊗˜Σ]−1(β−˜β).
(8.5.32)
It can be shown [17, 41] that the marginal distribution of any column of
the matrix of B, Bk is Multivariate Normal
p(Bk| ˜Bk, ˜Σ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Bk−˜
Bk)′(Wkk ˜Σ)−1(Bk−˜
Bk),
(8.5.33)
where W = (D−1 +U ′U)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of B, signiﬁcance can be deter-
mined for the set of coeﬃcients of an independent variable. Signiﬁcance can
© 2003 by Chapman & Hall/CRC

be determined for a subset of coeﬃcients by determining the marginal distri-
bution of the subset within Bk which is also Multivariate Normal. With the
subset being a singleton set, signiﬁcance can be determined for a particular
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Bkj| ˜Bkj, ˜Σjj,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(Bkj−˜Bkj )2
2Wkk ˜Σjj
,
(8.5.34)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Bkj = ˜βjk and that
z = (Bkj −˜Bkj)
,
Wkk ˜Σjj
(8.5.35)
=
(Bkj −˜Bkj)
&
WkkGjj/(n+ν +q +1)
(8.5.36)
follows a Normal distribution with a mean of zero and variance of one.
8.6 
Generalized Priors and Posterior
Using the generalized Conjugate procedure given in Chapter 4, the joint
prior distribution p(β,Σ) for the parameters β = vec(B) and Σ, “vec” being
the vectorization operator that stacks the columns of its matrix argument, is
given by the product of the prior distribution p(β) for the Regression coeﬃ-
cients and that for the error covariance matrix p(Σ)
p(β,Σ) = p(β)p(Σ).
(8.6.1)
These prior distributions are found from the generalized Conjugate procedure
in Chapter 4 and given by
p(β) ∝|∆|−1
2 e−1
2 (β−β0)′∆−1(β−β0)
(8.6.2)
and
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(8.6.3)
where the quantities ∆, β0, ν, and Q are hyperparameters to be assessed. The
matrices ∆, Σ, and Q are all positive deﬁnite. By specifying the hyperpara-
meters, the entire joint prior distribution is determined.
By Bayes’ rule, the joint posterior distribution for the unknown model pa-
rameters with speciﬁed generalized Conjugate priors is given by
© 2003 by Chapman & Hall/CRC

p(β,Σ|X,U) ∝p(Σ)p(β)p(X|B,Σ,U),
(8.6.4)
which becomes
p(β,Σ|X,U) ∝|∆|−1
2 e−1
2 (β−β0)′∆−1(β−β0)
×|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−UB′)′(X−UB′)+Q]
(8.6.5)
after inserting the generalized Conjugate prior distributions and the likeli-
hood.
Now the joint posterior distribution is to be evaluated in order to obtain
estimates of the parameters of the model.
8.7 
Generalized Estimation and Inference
8.7.1
Marginalization
Marginal estimation of the parameters involves computing the marginal
posterior distribution for each of the parameters then computing estimates
from these marginal distributions.
To ﬁnd the marginal posterior distribution of B or β, the joint posterior
distribution Equation 8.6.5 must be integrated with respect to Σ. Integration
of the joint posterior distribution is performed as described in Chapter 6 by
ﬁrst multiplying and dividing by
|(X −UB′)′(X −UB′)+Q|−(n+ν−p−1)
2
and then using the deﬁnition of the Inverted Wishart distribution given in
Chapter 2 to get
p(β|X,U) ∝
|∆|−1
2 e−1
2 (β−β0)′∆−1(β−β0)
|(X −UB′)′(X −UB′)+Q|−(n+ν−p−1)
2
.
(8.7.1)
The above expression for the marginal distribution of the vector of Regression
coeﬃcients can be written as
p(β|X,U) ∝
|∆|−1
2 e−1
2 (β−β0)′∆−1(β−β0)
|Ip +Q−1
2 (X −UB′)′In(X −UB′)Q−1
2 |−(n+ν−p−1)
2
(8.7.2)
by performing some algebra.
© 2003 by Chapman & Hall/CRC

Sylvester’s theorem [41]
|In +Y IpY ′| = |Ip +Y ′InY |
(8.7.3)
is applied in the denominator of the marginal posterior distribution to obtain
p(β|X,U) ∝
|∆|−1
2 e−1
2 (β−β0)′∆−1(β−β0)
|In +(X −UB′)Q−1(X −UB′)′|−(n+ν−p−1)
2
.
(8.7.4)
Note the change that has occurred in the denominator. The large sample
approximation that |I +Ξ|α ∼= eαtrΞ is used [41] where α is a scalar and Ξ is
an n × n positive deﬁnite matrix with eigenvalues which lie within the unit
circle to obtain the approximate expression
p(β|X,U) ∝e−1
2 (β−β0)′∆−1(β−β0)e−1
2 tr[(X−UB′)(
Q
n+ν−p−1 )−1(X−UB′)′]
(8.7.5)
for the marginal posterior distribution of the vector of Regression coeﬃcients.
After some algebra, the above is written as
p(β|X,U) ∝e
−1
2 (β−˘β)′h
U′U⊗

Q
n+ν−p−1

−1+∆−1i
(β−˘β),
(8.7.6)
where the vector ˘β is deﬁned to be
˘β =

∆−1 +U ′U ⊗
	
Q
n+ν −p−1

−1−1
×

∆−1β0 +U ′U ⊗
	
Q
n+ν −p−1

−1
ˆβ

(8.7.7)
and the vector ˆβ to be
ˆβ = vec( ˆB)
= vec(X′U(U ′U)−1).
(8.7.8)
The exact marginal mean and modal estimator for β from this approximate
marginal posterior distribution is ˘β. Note that ˘β is a weighted average of the
prior mean from the prior distribution and the data mean from the likelihood.
To ﬁnd the marginal posterior distribution of Σ, rearrange the terms in the
exponent of the joint posterior distribution to arrive at
© 2003 by Chapman & Hall/CRC

p(β,Σ|X,U) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1Q
×e−1
2 [(β−β0)′∆−1(β−β0)+(β−ˆβ)′(U′U⊗Σ−1)(β−ˆβ)], (8.7.9)
where the vector ˆβ is as previously deﬁned.
Continuing on, complete the square of the terms in the exponent of the
term involving β to ﬁnd
p(β,Σ|X,U) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[Q+(X−U ˆ
B′)′(X−U ˆ
B′)]
×e−1
2 [(β−˜β)′(∆−1+U′U⊗Σ−1)(β−˜β)],
(8.7.10)
where again the vector ˜β has been deﬁned as
˜β = [∆−1 +U ′U ⊗Σ−1]−1[∆−1β0 +(U ′U ⊗Σ−1)ˆβ].
(8.7.11)
Now, integration will be performed by recognition as in Chapter 6. Multiply
and divide by the quantity
|∆−1 +U ′U ⊗Σ−1|−1
2
(8.7.12)
and by integrating with the deﬁnition of a Multivariate Normal distribution
as in Chapter 6, the marginal posterior distribution for Σ is given by
p(Σ|X,U) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1{Q+(X−U ˆ
B′)′(X−U ˆ
B′)}
|∆−1 +U ′U ⊗Σ−1|−1
2 ,
(8.7.13)
which by using a large sample result [41],
|∆−1 +U ′U ⊗Σ−1| = |Σ|−(q+1)|U ′U|−p
(8.7.14)
is approximately for large samples
p(Σ|X,U) ∝|Σ|−(n+ν−q−1)
2
e−1
2 trΣ−1{Q+(X−U ˆ
B′)′(X−U ˆ
B′)}.
(8.7.15)
From this approximate marginal posterior distribution which can easily be
recognized as an Inverted Wishart distribution, the exact marginal posterior
mean is
˘Σ = Q+(X −U ˆB′)′(X −U ˆB′)
n+ν −q −1−2p−2
,
(8.7.16)
while the exact mode is
˘Σmode = Q+(X −U ˆB′)′(X −U ˆB′)
n+ν −q −1
.
(8.7.17)
© 2003 by Chapman & Hall/CRC

8.7.2
Posterior Conditionals
Since approximations were made when ﬁnding the marginal posterior dis-
tributions for the vector of Regression coeﬃcients β and the error covariance
matrix Σ, a Gibbs sampling algorithm is given for computing exact sampling
based quantities such as marginal mean and variance estimates of the para-
meters. The posterior conditional distributions of β and Σ are needed for the
algorithm. The posterior conditional distribution for β is found by consider-
ing only those terms in the joint posterior distribution which involve β and is
given by
p(β|Σ,X,U) ∝p(β)p(X|B,Σ,X,U)
∝e−1
2 (β−˜β)′(∆−1+U′U⊗Σ−1)(β−˜β).
(8.7.18)
This is recognizable as a Multivariate Normal distribution for the vector of
Regression coeﬃcients for β whose mean and mode is
˜β = [∆−1 +U ′U ⊗Σ−1]−1[∆−1β0 +(U ′U ⊗Σ−1)ˆβ],
(8.7.19)
where the vector ˆβ is
ˆβ = vec(X′U(U ′U)−1)
(8.7.20)
which was found by completing the square in the exponent. Note that this is
a weighted combination of the prior mean from the prior distribution and the
data mean from the likelihood.
The posterior conditional distribution of the error covariance matrix Σ is
similarly found by considering only those terms in the joint posterior distrib-
ution which involve Σ and is given by
p(Σ|β,X,U) ∝p(Σ)p(X|B,Σ,X,U)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−UB′)′(X−UB′)+Q].
(8.7.21)
This is easily recognized as being an Inverted Wishart distribution with mode
˜Σ = (X −UB′)′(X −UB′)+Q
n+ν
(8.7.22)
as described in Chapter 2.
8.7.3
Gibbs Sampling
To obtain marginal mean and variance estimates for the model parameters
using the Gibbs sampling algorithm as described in Chapter 6, start with
© 2003 by Chapman & Hall/CRC

an initial value for the error covariance matrix Σ, say ¯Σ(0), and then cycle
through
¯β(l+1) = a random variate from p(β|¯Σ(l),X,U)
= AβYβ +Mβ
(8.7.23)
¯Σ(l+1) = a random variate from p(Σ| ¯B(l+1),X,U)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(8.7.24)
where
¯β(l+1) = vec( ¯B(l+1))
AβA′
β = (∆−1 +U ′U ⊗¯Σ−1
(l) )−1
Mβ = [∆−1 +U ′U ⊗¯Σ−1
(l) ]−1[∆−1β0 +(U ′U ⊗¯Σ−1
(l) )ˆβ]
AΣA′
Σ = (X −U ¯B′
(l+1))′(X −U ¯B′
(l+1))+Q
while Yβ is a p(q + 1) × 1 dimensional vector and YΣ is an (n + ν + p + 1) ×
p, dimensional matrix whose respective elements are random variates from
the standard Scalar Normal distribution. The formulas for the generation of
random variates from the conditional posterior distributions are easily found
from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯β = 1
L
L

l=1
¯β(l)
and
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling based marginal mean parameters estimates from
the posterior distribution. Exact sampling based marginal estimates of other
quantities can also be found.
Of interest is the estimate of the marginal
posterior variance of the regression coeﬃcients
var(β|X,U) = 1
L
L

l=1
¯β(l) ¯β′
(l) −¯β ¯β′
= ¯∆.
The covariance matrices of the other parameters follow similarly.
With
a speciﬁcation of Normality for the marginal posterior distribution of the
Regression coeﬃcients, their distribution is
p(β|X,U) ∝| ¯∆|−1
2 e−1
2 (β−¯β)′ ¯∆−1(β−¯β),
(8.7.25)
© 2003 by Chapman & Hall/CRC

where ¯β and ¯∆are as previously deﬁned.
To determine statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the vector of Regression coeﬃcients given above.
General simultaneous hypotheses can be evaluated on the entire vector or a
subset of it. Signiﬁcance regarding the coeﬃcient for a particular indepen-
dent variable can be evaluated by computing marginal distributions. It can
be shown that the marginal distribution of the kth column of the matrix of
Regression coeﬃcients B, Bk = β′
k is Multivariate Normal
p(Bk| ¯Bk,X,U) ∝| ¯∆k|−1
2 e−1
2 (Bk−¯
Bk)′ ¯∆−1
k
(Bk−¯
Bk),
(8.7.26)
where ¯∆k is the covariance matrix of Bk found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be determined for a subset of coeﬃcients of the kth column
of B by determining the marginal distribution of the subset within Bk which is
also Multivariate Normal. With the subset being a singleton set, signiﬁcance
can be determined for a particular Regression coeﬃcient with the marginal
distribution of the scalar coeﬃcient which is
p(Bkj| ¯Bkj,X,U) ∝( ¯∆kj)−1
2 e
−
(Bkj−¯Bkj)2
2 ¯
∆kj
,
(8.7.27)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Bkj = ¯βjk and that
z = (Bkj −¯Bkj)
,
¯∆kj
(8.7.28)
follows a Normal distribution with a mean of zero and variance of one.
8.7.4
Maximum a Posteriori
The joint posterior distribution may also be maximized with respect to the
vector of Regression coeﬃcients β and the error covariance matrix Σ to obtain
maximum a posteriori estimates. For maximization of the posterior, the ICM
algorithm is used. Using the posterior conditional distributions found for the
Gibbs sampling algorithm, the ICM algorithm for determining maximum a
posteriori estimates is to start with an initial value for the estimate of the
matrix of Regression coeﬃcients ˜B(0) and cycle through
˜β(l+1) =
Arg Max
β
p(β|˜Σ(l),X,U)
= [∆−1 +U ′U ⊗˜Σ−1
(l) ]−1[∆−1β0 +(U ′U ⊗˜Σ−1
(l) )ˆβ]
˜Σ(l+1) =
Arg Max
Σ
p(Σ|˜β(l+1),X,U)
=
(X −U ˜B′
(l+1))′(X −U ˜B′
(l+1))+Q
n+ν
© 2003 by Chapman & Hall/CRC

until convergence is reached. The variables ˆB = X′U(U ′U)−1, ˆβ = vec( ˆB),
and ˜β = vec( ˜B) have been deﬁned in the process.
Conditional maximum
a posteriori variance estimates can also be found.
The conditional modal
variance of the regression coeﬃcients is
var(β|˜β, ˜Σ,X,U) = (∆−1 +U ′U ⊗˜Σ−1)−1
= ˜∆,
where ˜β and ˜Σ are the converged value from the ICM algorithm.
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
The posterior conditional distribution of the vector of Regression coeﬃcients
β given the modal values of the other parameters and the data is
p(β|˜β, ˜Σ,X,U) ∝| ˜∆|−1
2 e−1
2 (β−˜β)′ ˜∆−1(β−˜β).
(8.7.29)
To determine statistical signiﬁcance with the ICM approach, use the mar-
ginal conditional distribution of the vector of Regression coeﬃcients given
above. General simultaneous hypotheses can be performed on the entire vec-
tor or a subset of it. Signiﬁcance regarding the coeﬃcient for a particular
independent variable can be evaluated by computing marginal distributions.
It can be shown that the marginal conditional distribution of the kth column
of the matrix of Regression coeﬃcients B, Bk is Multivariate Normal
p(Bk| ˜Bk, ˜Σ,X,U) ∝| ˜∆k|−1
2 e−1
2 (Bk−˜
Bk)′ ˜∆−1
k
(Bk−˜
Bk),
(8.7.30)
where ˜∆k is the covariance matrix of Bk found by taking the kth p ×p sub-
matrix along the diagonal of ˜∆.
Signiﬁcance can be evaluated for a subset of coeﬃcients of the kth column of
B by determining the marginal distribution of the subset within Bk which is
also Multivariate Normal. With the subset being a singleton set, signiﬁcance
can be evaluated for a particular Regression coeﬃcient with the marginal
distribution of the scalar coeﬃcient which is
p(Bkj| ¯Bkj,X) ∝( ˜∆kj)−1
2 e
−
(Bkj−˜Bkj )2
2 ˜
∆kj
,
(8.7.31)
where ˜∆kj is the jth diagonal element of ˜∆k. Note that ˜Bkj = ˜βjk and that
z = (Bkj −˜Bkj)
,
˜∆kj
(8.7.32)
follows a Normal distribution with a mean of zero and variance of one.
© 2003 by Chapman & Hall/CRC

8.8 
Interpretation
The main results of performing a Bayesian Regression are estimates of the
matrix of Regression coeﬃcients and the error covariance matrix. The results
of a Bayesian Regression are described with the use of an example.
TABLE 8.1
Variables for Bayesian Regression example.
X Variables
U Variables
X1
Concentration at Time 0
U1
Insulin Type
X2
Concentration at Time 1
U2
Dose Level
X3
Concentration at Time 2
U3
Insulin∗Dose Interaction
X4
Concentration at Time 3
X5
Concentration at Time 4
X6
Concentration at Time 5
As an illustrative example, consider data from a bioassay of insulin. The
blood sugar concentration in 36 rabbits was measured in mg/100 ml every hour
between 0 and 5 hours after administration of an insulin dose. There were
two types of insulin preparation (“standard,” U1 = −1 , and “test,” U1 = 1)
each with two dose levels (0.75 units, U2 = −1, and 1.50 units, U2 = 1), from a
study [67]. It is also believed that there is an interaction between preparation
type and dose level so the interaction term U3 = U1 ∗U2 is included.
The problem is to determine the relationship between the set of indepen-
dent variables (the U’s) and the dependent variables (the X’s) described in
Table 8.1. There are n = 36 observations of dimension p = 5 along with the
q = 3 independent variables.
The data X and the design matrix U (including a column of ones for the
intercept term) are given in Table 8.2. Hyperparameters for the prior distri-
butions were assessed by performing a regression on a previous data set X0
obtained from the same population on the previous day.
The estimate of the Regression coeﬃcient matrix B deﬁnes a “ﬁtted” line.
The ﬁtted line describes the linear relationship between the independent vari-
ables (the U’s), and the dependent variables (the X’s). The coeﬃcient matrix
has the interpretation that if all of the independent variables were held ﬁxed
except for one uij, which if increased to u∗
ij, the dependent variable xij in-
creases to an amount x∗
ij given by
x∗
ij = βi0 +···+βiju∗
ij +···+βiquiq.
(8.8.1)
Regression coeﬃcients are evaluated to determine whether they are statis-
tically “large” meaning that the associated independent variable contributes
© 2003 by Chapman & Hall/CRC

TABLE 8.2
Bayesian Regression data and design matrices.
X
1
2
3
4
5
6
U
en
1
2
3
1
96
37
31
33
35
41
1
1
1
1
1
2
90
47
48
55
68
89
2
1
1
1
1
3
99
49
55
64
74
97
3
1
1
1
1
4
95
33
37
43
63
92
4
1
1
1
1
5
107
62
62
85
110
117
5
1
1
1
1
6
81
40
43
45
49
55
6
1
1
1
1
7
95
49
56
63
68
88
7
1
1
1
1
8
105
53
57
69
103
106
8
1
1
1
1
9
97
50
53
59
82
96
9
1
1
1
1
10
97
54
57
66
80
89
10
1
1
-1
-1
11
105
66
83
95
97
100
11
1
1
-1
-1
12
105
49
54
56
70
90
12
1
1
-1
-1
13
106
79
92
95
99
100
13
1
1
-1
-1
14
92
46
51
57
73
91
14
1
1
-1
-1
15
91
61
64
71
80
90
15
1
1
-1
-1
16
101
51
63
91
95
96
16
1
1
-1
-1
17
87
53
55
57
78
89
17
1
1
-1
-1
18
94
57
70
81
94
96
18
1
1
-1
-1
19
98
48
55
71
91
96
19
1
-1
1
-1
20
98
41
43
61
89
101
20
1
-1
1
-1
21
103
60
56
61
76
97
21
1
-1
1
-1
22
99
36
43
57
89
102
22
1
-1
1
-1
23
97
44
51
58
85
105
23
1
-1
1
-1
24
95
41
45
49
59
78
24
1
-1
1
-1
25
109
65
62
72
93
104
25
1
-1
1
-1
26
91
57
60
61
67
83
26
1
-1
1
-1
27
99
43
48
52
61
86
27
1
-1
1
-1
28
102
51
56
81
97
103
28
1
-1
-1
1
29
96
57
55
72
85
89
29
1
-1
-1
1
30
111
84
83
91
101
102
30
1
-1
-1
1
31
105
57
67
83
100
103
31
1
-1
-1
1
32
105
57
61
70
90
98
32
1
-1
-1
1
33
98
55
67
88
94
95
33
1
-1
-1
1
34
98
69
72
89
98
98
34
1
-1
-1
1
35
90
53
61
78
94
95
35
1
-1
-1
1
36
100
60
63
67
77
104
36
1
-1
-1
1
to the dependent variable or statistically “small” meaning that the associated
independent variable does not contribute to the dependent variable.
Table 8.4 contains the matrix of regression coeﬃcients from an implemen-
tation of the aforementioned Conjugate prior model and used the data in Ta-
ble 8.2. Exact analytic equations to compute marginal mean and maximum
a posteriori estimates are used. The marginal mean and conditional modal
© 2003 by Chapman & Hall/CRC

TABLE 8.3
Bayesian Regression prior data and design matrices.
X0
1
2
3
4
5
6
U0
en
1
2
3
1
96
54
61
63
93
103
1
1
1
1
1
2
98
57
63
75
99
104
2
1
1
1
1
3
104
77
88
91
113
110
3
1
1
1
1
4
109
63
60
67
85
109
4
1
1
1
1
5
98
59
65
72
95
103
5
1
1
1
1
6
104
59
62
74
89
97
6
1
1
1
1
7
97
63
70
72
101
102
7
1
1
1
1
8
101
54
64
77
97
100
8
1
1
1
1
9
107
59
67
61
69
99
9
1
1
1
1
10
96
63
81
97
101
97
10
1
1
-1
-1
11
99
48
70
94
108
104
11
1
1
-1
-1
12
102
61
78
81
99
104
12
1
1
-1
-1
13
112
67
76
100
112
112
13
1
1
-1
-1
14
92
49
59
83
104
103
14
1
1
-1
-1
15
101
53
63
86
104
102
15
1
1
-1
-1
16
105
63
77
94
111
107
16
1
1
-1
-1
17
99
61
74
76
89
92
17
1
1
-1
-1
18
99
51
63
77
99
103
18
1
1
-1
-1
19
98
53
62
71
81
101
19
1
-1
1
-1
20
103
62
65
96
101
105
20
1
-1
1
-1
21
102
54
60
57
64
69
21
1
-1
1
-1
22
108
83
67
80
106
108
22
1
-1
1
-1
23
92
56
60
61
73
79
23
1
-1
1
-1
24
102
61
59
71
91
101
24
1
-1
1
-1
25
94
51
53
55
86
83
25
1
-1
1
-1
26
95
55
58
59
71
85
26
1
-1
1
-1
27
103
47
59
64
92
100
27
1
-1
1
-1
28
120
46
44
58
118
108
28
1
-1
-1
1
29
95
65
75
85
96
95
29
1
-1
-1
1
30
99
59
73
82
109
109
30
1
-1
-1
1
31
105
50
58
84
107
107
31
1
-1
-1
1
32
97
67
89
104
118
118
32
1
-1
-1
1
33
97
46
50
59
78
91
33
1
-1
-1
1
34
102
63
67
74
83
98
34
1
-1
-1
1
35
104
69
81
98
104
105
35
1
-1
-1
1
36
101
65
69
72
93
95
36
1
-1
-1
1
posterior distributions are known and as described previously in this Chapter.
With these posterior distributions, credibility intervals and hypotheses can be
evaluated to determine whether the set or a subset of independent variables
describe the observed relationship.
The prior mode, marginal mean, and maximum a posteriori values of the
observation error variances and covariances are the elements of Table 8.5.
In terms of the Source Separation model, if a coeﬃcient is statistically
© 2003 by Chapman & Hall/CRC

TABLE 8.4
Prior, Gibbs, and ICM Bayesian Regression coeﬃcients.
B0
0
1
2
3
1
101.0000
0.0556
-0.3889
0.8889
2
58.6944
0.2500
0.5833
1.0278
3
66.3889
2.5556
-2.8889
0.6111
4
76.9444
3.0556
-6.6111
-0.9444
5
95.5278
2.6944
-6.3056
1.5278
6
100.2222
2.6111
-2.5556
2.7222
¯B
0
1
2
3
1
99.6250
-0.6806
-0.5972
0.4861
2
55.9306
-0.4583
-2.5417
0.6806
3
62.0694
1.0417
-5.1806
-0.0417
4
72.4444
0.4722
-7.8889
-0.1389
5
88.9306
-0.4306
-6.4861
0.9306
6
96.7917
-0.3194
-2.5972
1.0139
˜B
0
1
2
3
1
99.6250
-0.6806
-0.5972
0.4861
2
55.9306
-0.4583
-2.5417
0.6806
3
62.0694
1.0417
-5.1806
-0.0417
4
72.4444
0.4722
-7.8889
-0.1389
5
88.9306
-0.4306
-6.4861
0.9306
6
96.7917
-0.3194
-2.5972
1.0139
“large,” then the associated observed source contributes signiﬁcantly to the
observed mixture of sources.
Table 8.6 contains the matrix of individual marginal statistics for the co-
eﬃcients. From this table, it is apparent which coeﬃcients are statistically
signiﬁcant.
8.9
Discussion
Returning to the cocktail party problem, the matrix of Regression coeﬃ-
cients B where B = (µ,B⋆) contains the matrix of mixing coeﬃcients B⋆for
the observed conversation (sources) U, and the population mean µ which is a
vector of the overall background mean level at each microphone.
© 2003 by Chapman & Hall/CRC

TABLE 8.5
Prior, Gibbs, and ICM Regression covariances.
Q/ν
1
2
3
4
5
6
1
30.3333
8.1327
-5.9198
2.8488
19.1790
18.5802
2
64.8086
55.9815
55.6574
29.8735
22.7747
3
80.3765
74.8858
29.8148
20.8056
4
128.1173
82.6636
57.0432
5
132.9506
75.2963
6
69.0247
¯Ψ
1
2
3
4
5
6
1
45.6351
29.9234
25.0795
36.3755
56.8027
47.6303
2
115.7423
110.1676
104.0680
85.9636
57.8525
3
141.7941
139.9377
114.5536
76.4875
4
205.5766
188.4320
122.8870
5
277.5163
184.3142
6
171.0833
˜Ψ
1
2
3
4
5
6
1
34.8268
22.8363
19.1396
27.7602
43.3494
36.3494
2
88.3297
84.0753
79.4203
65.6038
44.1506
3
108.2113
106.7946
87.4225
58.3721
4
156.8874
143.8034
93.7822
5
211.7887
140.6608
6
130.5636
TABLE 8.6
Statistics for coeﬃcients.
t65
0
1
2
3
1
125.1369
-0.8548
-0.7502
0.6106
2
44.1133
-0.3615
-2.0047
0.5368
3
44.2298
0.7423
-3.6916
-0.0297
4
42.8731
0.2795
-4.6687
-0.0822
5
45.2974
-0.2193
-3.3037
0.4740
6
62.7914
-0.2072
-1.6849
0.6577
z
0
1
2
3
1
143.2445
-0.9785
-0.8587
0.6989
2
50.4966
-0.4138
-2.2947
0.6144
3
50.6300
0.8497
-4.2258
-0.0340
4
49.0769
0.3199
-5.3443
-0.0941
5
51.8520
-0.2510
-3.7818
0.5426
6
71.8775
-0.2372
-1.9287
0.7529
© 2003 by Chapman & Hall/CRC

Exercises
1. Write the likelihood for all of the observations as
p(X|B,Σ,U) =
n

i=1
p(xi|B,Σ,ui)
and use the facts that tr(ΥΞ) = tr(ΞΥ) to derive Equation 8.3.3. Note
that the trace of a scalar is the scalar.
2. Given the posterior distribution, Equation 8.4.5, derive Gibbs sampling
and ICM algorithms. Show that for the ICM algorithm, convergence is
reached after one iteration when iterating in the order B,Σ.
3. Specify the prior distribution for the Regression coeﬃcients B and the
error covariance matrix Σ to be the vague priors
p(B) ∝(a constant)
and
p(Σ) ∝|Σ|−(p+1)
2
.
Combine these prior distributions with the likelihood in Equation 8.3.3
to obtain a posterior distribution and derive equations for marginal pa-
rameter estimates.
4. Using the priors speciﬁed in Exercise 2, derive Gibbs sampling and ICM
algorithms.
5. Specify the prior distribution for the Regression coeﬃcients and the
error covariance matrix to be the vague and Conjugate priors
p(B) ∝(a constant)
and
p(Σ) ∝|Σ|−ν
2 e−1
2 Σ−1Q.
Combine these prior distributions with the likelihood in Equation 8.3.3
to obtain a posterior distribution and derive equations for marginal pa-
rameter estimates.
© 2003 by Chapman & Hall/CRC

6. Using the priors speciﬁed in Exercise 4, derive Gibbs sampling and ICM
algorithms.
7. Derive the ICM algorithm for maximizing the posterior distribution,
Equation 8.4.5.
Do this by taking the logarithm and diﬀerentiating
with respect to B using the result [41] given in Chapter 6 that
∂tr(ΞB′ΥB)
∂B
= 2ΥBΞ
and either diﬀerentiating with respect to Σ or using the mode or maxi-
mum value of an Inverted Wishart distribution.
8. In the Conjugate prior model, exact marginal mean estimates were com-
puted. Derive the Gibbs sampling algorithm for the Conjugate prior
model.
© 2003 by Chapman & Hall/CRC

9
Bayesian Factor Analysis
9.1
Introduction
Now that the Bayesian Regression model has been discussed, the Bayesian
Factor Analysis model is described. The Bayesian Factor Analysis model is
similar to the Bayesian Source Separation model in that the factors, analogous
to sources, are unobservable. However, there are diﬀerences that outline the
Psychometric method.
The Factor Analysis model uses the correlations or covariances between a set
of observed variables to describe them in terms of a smaller set of unobservable
variables [50]. The unobserved variables called factors describe the underlying
relationship among the original variables.
There are two main reasons why one would perform a Factor Analysis. The
ﬁrst is to explain the observed relationship among a set of observed variables
in terms of a smaller number of unobserved variables or latent factors which
underlie the observations. This smaller number of variables can be used to
ﬁnd a meaningful structure in the observed variables. This structure will aid
in the interpretation and explanation of the process that has generated the
observations.
The second reason one would carry out a Factor Analysis is for data reduc-
tion. Since the observed variables are represented in terms of a smaller number
of unobserved or latent variables, the number of variables in the analysis is
reduced, and so are the storage requirements. By having a smaller number of
factors (vectors of smaller dimension) to work with that capture the essence
of the observed variables, only this smaller number of factors is required to be
stored. This smaller number of factors can also be used for further analysis
to reduce computational requirements.
The structure of the Factor Analysis model is strikingly similar to the Source
Separation model. However, its genesis comes from psychology and retains
some of the speciﬁcs for the Psychometric model. In reading about the Fac-
tor Analysis model, the unobservable factors correspond to the unobservable
sources.
© 2003 by Chapman & Hall/CRC

9.2
The Bayesian Factor Analysis Model
The development of Bayesian Factor Analysis has been recent.
Here is
a description of the Factor Analysis model and a Bayesian approach which
builds upon previous work [43, 44, 51, 54, 55, 65].
In the Bayesian Factor Analysis model, the p-dimensional observed values
are denoted as xi’s just as in the Bayesian Regression model and the m-
dimensional unobserved factor score vectors analogous to sources are denoted
by fi’s. The fi’s are used to distinguish the unobservable factors from the
observable regressors, the ui’s, and the unobservable sources, the si’s.
Given a set of independent and identically distributed vector-valued obser-
vations xi, i = 1,...,n, on p possibly correlated random variables, the Multi-
variate Factor Analysis model on the m < p, unobserved variable fi, is
(xi|µ,Λ,fi) =
µ
+
Λ
fi
+
ǫi,
(p×1)
(p×1)
(p×m) (m×1)
(p×1)
(9.2.1)
where the p-dimensional overall population mean vector is
µ =



µ1
...
µp


,
(9.2.2)
the matrix Λ of unobserved coeﬃcients called “factor loadings” (analogous to
the Regression coeﬃcient matrix B⋆in the Regression model and the mixing
matrix Λ in the Source Separation model) describing the relationship between
fi and xi,
Λ =



λ′
1
...
λ′
p


,
(9.2.3)
while ǫi is the error vector at time i is given by
ǫi =



ǫi1
...
ǫip


.
(9.2.4)
More speciﬁcally, a given element xij of observed vector xi is represented
by the model
(xij|µij,λj,fi) =
µij
+
λ′
j
fi
+
ǫij,
(1×1)
(1×1)
(1×m) (m×1)
(1×1)
(9.2.5)
© 2003 by Chapman & Hall/CRC

where λ′
j = (λj1,...,λjm) is a vector of factor loadings that connects the m
unobserved factors to the jth observation variable at observation number or
time i, xij. This is also represented as
(xij|µj,λj,fi) = µj +
m

k=1
λjk fik +ǫij.
(9.2.6)
Gathering all observed vectors into a matrix X, the Factor Analysis model
may be written as
(X|µ,Λ,F) =
enµ′
+
F
Λ′
+
E,
(n×p)
(n×p)
(n×m) (m×p)
(n×p)
(9.2.7)
where the matrix of observed vectors, the matrix of unobserved factors scores,
and the matrix of error vectors are given by
X =



x′
1
...
x′
n


,
F =



f ′
1
...
f ′
n


,
and
E =



ǫ′
1
...
ǫ′
n


.
(9.2.8)
The ijth element of the observed matrix X is given by the jth element of
µ, µj plus the ith row of F multiplied by the jth column (row) of Λ′ (Λ) plus
the ijth element of E.
9.3 
Likeliho o d
In statistical models, there is inherent random variability or error which
is characterized as having arisen from a distribution. As in the Regression
model [1, 41] it is speciﬁed that the errors of observation are independent
and Multivariate Normally distributed and represented by the Multivariate
Normal distribution (Chapter 2)
p(ǫi|Σ) ∝|Σ|−1
2 e−1
2 ǫ′
iΣ−1ǫi,
(9.3.1)
where ǫi is the ith p-dimensional error vector, and Σ is the p×p error covari-
ance matrix.
From this Multivariate Normal error speciﬁcation, the distribution of the
ith observation vector is the Multivariate Normal distribution
p(xi|µ,Λ,fi,Σ) ∝|Σ|−1
2 e−1
2 (xi−µ−Λ′fi)′Σ−1(xi−µ−Λ′fi).
(9.3.2)
© 2003 by Chapman & Hall/CRC

With the matrix representation of the model, the distribution of the matrix
of observations is given by the Matrix Normal distribution
p(X|µ,Λ,F,Σ) ∝|Σ|−n
2 e−1
2 tr(X−enµ′−F Λ′)Σ−1(X−enµ′−F Λ′)′,
(9.3.3)
where the observations are X′ = (x1,...,xn) and the factor scores are F ′ =
(f1,...,fn). The notation “tr” is the trace operator which yields the sum of
the diagonal elements of its matrix argument.
The Factor Analysis model can be written in a similar form as the Bayesian
Regression model. The overall mean vector µ and the factor loading matrix
Λ are joined into a single matrix as
C = (µ,Λ) = (C1,...,Cm+1) =



c′
1
...
c′
p


.
(9.3.4)
An n-dimensional vector of ones en and the factor scores matrix F are also
joined as Z = (en,F).
Having joined these vectors and matrices, the Factor Analysis model is now
written in the matrix formulation
(X|C,Z) =
Z
C′
+
E
n×p
n×(m+1) (m+1)×p
(n×p)
(9.3.5)
and the associated likelihood is the Matrix Normal distribution given by
p(X|C,Z,Σ) ∝|Σ|−n
2 e−1
2 tr(X−ZC′)Σ−1(X−ZC′)′,
(9.3.6)
where all variables are as deﬁned above.
Both Conjugate and generalized Conjugate distributions are used to quan-
tify our prior knowledge regarding various values of the model parameters.
9.4
Conjugate Priors and Posterior
In the model immediately described based on [60], which advances previous
work [43, 44, 51, 54, 55, 65], Conjugate families of prior distributions for
the model parameters are used. The joint prior distribution for the model
parameters C = (µ,Λ) the matrix of coeﬃcients, F the factor score matrix,
and Σ the error covariance matrix is the product of the prior distribution
for the factor score matrix multiplied by the prior distribution for the matrix
of coeﬃcients C given the error covariance matrix Σ multiplied by the prior
distribution for the error covariance matrix Σ
© 2003 by Chapman & Hall/CRC

p(F,C,Σ) = p(F)p(C|Σ)p(Σ),
(9.4.1)
where the prior distribution for the model parameters from the Conjugate
procedure outlined in Chapter 4 are the Matrix Normal distribution for the
matrix of coeﬃcients C, the Inverted Wishart distribution for the error covari-
ance matrix Σ, and the Matrix Normal distribution for the matrix of factor
scores F are given by
p(C|Σ) ∝|D|−p
2 |Σ|−m+1
2 e−1
2 trD−1(C−C0)′Σ−1(C−C0),
(9.4.2)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(9.4.3)
p(F) ∝e−1
2 trF ′F ,
(9.4.4)
with Σ, D, and Q positive deﬁnite symmetric matrices. Thus, C conditional
on Σ has elements which are jointly Normally distributed, and (C0,D) are
hyperparameters to be assessed; Σ follows an Inverted Wishart distribution,
and (ν,Q) are hyperparameters to be assessed. The factor score vectors are
independent and normally distributed random vectors with mean zero and
identity covariance matrix which is consistent with the traditional orthogonal
Factor Analysis model. The distributional speciﬁcation for the factor scores is
also present in non-Bayesian models as a model assumption [41, 50]. Note that
Q and consequently E(Σ) are diagonal, to represent traditional Psychometric
views of the factor model containing “common” and “speciﬁc” factors.
If the vector of coeﬃcients c is given by c = vec(C), then from the prior spec-
iﬁcation, var(c|Σ) = D ⊗Σ. By Bayes’ rule, the joint posterior distribution
for the unknown model parameters F, C, and Σ is given by
p(F,C,Σ|X) ∝e−1
2 trF ′F |Σ|−(n+ν+m+1)
2
e−1
2 trΣ−1G,
(9.4.5)
where the p×p matrix variable G has been deﬁned to be
G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q.
The joint posterior distribution must now be evaluated in order to obtain
estimates of the matrix of factor scores F, the matrix containing the over-
all mean µ with the factor loadings Λ, and the error covariance matrix Σ.
Marginal posterior mean and joint maximum a posteriori estimates of the
parameters F, C, and Σ are found by the Gibbs sampling and iterated con-
ditional modes (ICM) algorithms.
© 2003 by Chapman & Hall/CRC

9.5
Conjugate Estimation and Inference
With the above joint posterior distribution from the Bayesian Factor Analy-
sis model, it is not possible to obtain all or any of the marginal distributions
and thus marginal estimates in closed form or explicit formulas for maximum
a posteriori estimates from diﬀerentiation. For this reason, marginal mean
estimates using the Gibbs sampling algorithm and maximum a posteriori es-
timates using the ICM algorithm are found.
9.5.1
Posterior Conditionals
Both the Gibbs sampling and ICM estimation procedures require the pos-
terior conditional distributions. Gibbs sampling requires the posterior con-
ditionals for the generation of random variates while ICM requires them for
maximization by cycling through their modes or maxima.
The conditional posterior distribution of the matrix of factor scores F is
found by considering only those terms in the joint posterior distribution which
involve F and is given by
p(F|µ,Λ,Σ,X) ∝p(F)p(X|µ,F,Λ,Σ)
∝e−1
2 trF ′F |Σ|−n
2 e−1
2 trΣ−1(X−enµ−F Λ′)′(X−enµ−F Λ′)
∝e−1
2 trF ′F e−1
2 tr(X−enµ′−F Λ′)Σ−1(X−enµ′−F Λ′)′
which after performing some algebra in the exponent can be written as
p(F|µ,Λ,Σ,X) ∝e−1
2 tr(F −˜
F )(Im+Λ′Σ−1Λ)(F −˜
F)′,
(9.5.1)
where the matrix ˜F has been deﬁned to be
˜F = (X −enµ′)Σ−1Λ(Im +Λ′Σ−1Λ)−1.
(9.5.2)
That is, the matrix of factor scores given the overall mean µ, the matrix of
factor loadings Λ, the error covariance matrix Σ, and the data X is Matrix
Normally distributed.
The conditional posterior distribution of the matrix C of factor loadings Λ
and the overall mean µ is found by considering only those terms in the joint
posterior distribution which involve C and is given by
p(C|F,Σ,X) ∝p(C|Σ)p(X|F,C,Σ)
∝|Σ|−m+1
2 e−1
2 trΣ−1(C−C0)D−1(C−C0)′
© 2003 by Chapman & Hall/CRC

×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝e−1
2 trΣ−1[(C−C0)D−1(C−C0)′+(X−ZC′)′(X−ZC′)]
which after performing some algebra in the exponent becomes
p(C|F,Σ,X) ∝e−1
2 trΣ−1(C−˜
C)(D−1+Z′Z)(C−˜
C)′,
(9.5.3)
where the matrix ˜C has been deﬁned to be
˜C = [C0D−1 +X′Z](D−1 +Z′Z)−1.
Note that ˜C can be written as
˜C = C0[D−1(D−1 +Z′Z)−1]+ ˆC[(Z′Z)(D−1 +Z′Z)−1],
a weighted combination of the prior mean C0 from the prior distribution and
the data mean ˆC = X′Z(Z′Z)−1 from the likelihood.
That is, the conditional posterior distribution of the matrix C (containing
the mean vector µ and the factor loadings matrix Λ) given the factor scores F,
the error covariance matrix Σ, and the data X is Matrix Normally distributed.
The conditional posterior distribution of the disturbance covariance matrix
Σ is found by considering only those terms in the joint posterior distribution
which involve Σ, and is given by
p(Σ|F,C,X) ∝p(Σ)p(C|Σ)p(X|F,C,Σ)
∝|Σ|−ν
2 e−1
2 trΣ−1Q|Σ|−m+1
2 e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝|Σ|−(n+ν+m+1)
2
e−1
2 trΣ−1G,
(9.5.4)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)(X −ZC′)′ +(C −C0)D−1(C −C0)′ +Q.
(9.5.5)
That is, the conditional distribution of the error covariance matrix Σ given
the overall mean µ, the matrix of factor scores F, the matrix of factor loadings
Λ, and the data X has an Inverted Wishart distribution.
The modes of these posterior conditional distributions are as described in
Chapter 2 and are given by ˜F, ˜C, (both as deﬁned above) and
˜Σ =
G
n+ν +m+1,
(9.5.6)
respectively.
© 2003 by Chapman & Hall/CRC

9.5.2
Gibbs Sampling
To ﬁnd marginal mean estimates of the model parameters from the joint
posterior distribution using the Gibbs sampling algorithm, start with initial
values for the matrix of factor scores F and the error covariance matrix Σ,
say ¯F(0) and ¯Σ(0), and then cycle through
¯C(l+1) = a random variate from p(C| ¯F(l), ¯Σ(l),X)
= ACYCB′
C +MC,
(9.5.7)
¯Σ(l+1) = a random variate from p(Σ| ¯F(l), ¯C(l+1),X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(9.5.8)
¯F(l+1) = a random variate from p(F| ¯C(l+1), ¯Σ(l+1),X)
= YF B′
F +MF ,
(9.5.9)
where
ACA′
C = ¯Σ(l),
BCB′
C = (D−1 + ¯Z′
(l) ¯Z(l))−1,
¯Z(l) = (en, ¯F(l)),
MC = (X′ ¯Z(l) +C0D−1)(D−1 + ¯Z′
(l) ¯Z(l))−1,
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))
+( ¯C(l+1) −C0)D−1( ¯C(l+1) −C0)′ +Q,
BF B′
F = (Im + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MF = (X −en¯µ′
(l+1))¯Σ−1
(l+1)˜Λ(l+1)(Im + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while YC, YΣ, and YF are p ×(m +1), (n +ν +m + 1 +p +1) ×p, and n ×m
dimensional matrices respectively, whose elements are random variates from
the standard Scalar Normal distribution. The formulas for the generation of
random variates from the conditional posterior distributions are easily found
from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯F = 1
L
L

l=1
¯F(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters. Exact sampling-based estimates of other quantities can also be
found. Similar to Regression, there is interest in the estimate of the marginal
posterior variance of the matrix containing the means and factor loadings
© 2003 by Chapman & Hall/CRC

var(c|X) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C) and ¯c = vec( ¯C).
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the vector
containing the mean vector and factor loadings, their distribution is
p(c|X) ∝| ¯∆|−1
2 e−1
2 (c−¯c)′ ¯∆−1(c−¯c),
(9.5.10)
where ¯c and ¯∆are as previously deﬁned.
To evaluate statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the matrix containing the mean vector and factor
loading matrix given above. General simultaneous hypotheses can be evalu-
ated regarding the entire matrix containing the mean vector and the factor
loading matrix, a submatrix, or the mean vector or a particular factor, or
an element by computing marginal distributions. It can be shown that the
marginal distribution of the kth column of the matrix containing the mean
vector and factor loading matrix C, Ck is Multivariate Normal
p(Ck| ¯Ck,X) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(9.5.11)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be determined for a subset of coeﬃcients of the kth column
of C by determining the marginal distribution of the subset within Ck which
is also Multivariate Normal. With the subset being a singleton set, signiﬁ-
cance can be determined for a particular mean or loading with the marginal
distribution of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(9.5.12)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(9.5.13)
follows a Normal distribution with a mean of zero and variance of one.
© 2003 by Chapman & Hall/CRC

9.5.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
matrix of coeﬃcients C, the matrix of factor scores F, and the error covariance
matrix Σ by using the ICM algorithm. To jointly maximize the joint posterior
distribution using the ICM algorithm, start with an initial value for the matrix
of factor scores ˜F, say ˜F(0), and then cycle through
˜C(l+1) =
Arg Max
C
p(C| ˜F(l), ˜Σ(l),X)
= (X′ ˜Z(l) +C0D−1)(D−1 + ˜Z′
(l) ˜Z(l))−1,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜F(l),X)
= [(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))
+( ˜C(l+1) −C0)D−1( ˜C(l+1) −C0)′ +Q]/(n+ν +m+1),
˜F(l+1) =
Arg Max
F
p(F| ˜C(l+1), ˜Σ(l+1),X)
= (X −en˜µ′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)(Im + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
where the matrix ˜Z(l) = (en, ˜F(l)) has been deﬁned and cycling continues until
convergence is reached with the joint modal estimator for the unknown para-
meters ( ˜F, ˜C, ˜Σ). Conditional maximum a posteriori variance estimates can
also be found. The conditional modal variance of the matrix containing the
means and factor loadings is
var(C| ˜C, ˜F, ˜Σ,X) = ˜Σ⊗(D−1 + ˜Z′ ˜Z)−1
(9.5.14)
or equivalently
var(c|˜c, ˜F, ˜Σ,X) = (D−1 + ˜Z′ ˜Z)−1 ⊗˜Σ
(9.5.15)
= ˜∆,
(9.5.16)
where c = vec(C), while ˜C, ˜F, and ˜Σ are the converged value from the ICM
algorithm.
To determine statistical signiﬁcance with the ICM approach, use the condi-
tional distribution of the matrix containing the mean vector and factor loading
matrix which is
p(C| ˜C, ˜F, ˜Σ,X) ∝|D−1 + ˜Z′ ˜Z|
1
2 |˜Σ|−1
2 e−1
2 tr ˜Σ−1(C−˜
C)(D−1+ ˜Z′ ˜
Z)(C−˜
C)′.
(9.5.17)
That is,
© 2003 by Chapman & Hall/CRC

C| ˜C, ˜F, ˜Σ,X ∼N

˜C, ˜Σ⊗(D−1 + ˜Z′ ˜Z)−1
.
(9.5.18)
General simultaneous hypotheses can be evaluated regarding the entire ma-
trix containing the mean vector and the factor loading matrix, a submatrix, or
the mean vector or a particular factor, or an element by computing marginal
conditional distributions.
It can be shown [17, 41] that the marginal conditional distribution of any
column of the matrix containing the means and factor loadings C, Ck is
Multivariate Normal
p(Ck| ˜Ck, ˜F, ˜Σ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Ck−˜
Ck)′(Wkk ˜Σ)−1(Ck−˜
Ck),
(9.5.19)
where W = (D−1 +U ′U)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of C, signiﬁcance can be eval-
uated for the mean vector or a particular factor. Signiﬁcance can be deter-
mined for a subset of coeﬃcients by determining the marginal distribution
of the subset within Ck which is also Multivariate Normal. With the subset
being a singleton set, signiﬁcance can be evaluated for a particular mean or
loading with the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜F, ˜Σjj,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(Ckj−˜Ckj)2
2Wkk ˜Σjj ,
(9.5.20)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)
,
Wkk ˜Σjj
(9.5.21)
follows a Normal distribution with a mean of zero and variance of one.
9.6
Generalized Priors and Posterior
The Conjugate prior distributions can be expanded to generalized Conju-
gate priors which permit greater freedom of assessment [56]. This extends
previous work [51] in which available prior information regarding the parame-
ter values was quantiﬁed using these generalized Conjugate prior distributions;
however, independence was assumed between the overall mean and the factor
loadings matrix.
© 2003 by Chapman & Hall/CRC

The joint prior distribution p(F,c,Σ) for the matrix of factor scores F, the
vector containing the overall mean and factor loadings c = vec(C), and the
error covariance matrix Σ is given by the product of the prior distribution p(F)
for the factor loading matrix F with the prior distribution p(c) for the vector
of coeﬃcients c and with the prior distribution p(Σ) for the error covariance
matrix Σ and is given by
p(F,c,Σ) = p(F)p(c)p(Σ).
(9.6.1)
These prior distributions are found from the generalized Conjugate procedure
outlined in Chapter 4 and are given by
p(c) ∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(9.6.2)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(9.6.3)
p(F) ∝e−1
2 trF ′F .
(9.6.4)
The hyperparameters ∆, c0, ν, and Q are hyperparameters to be assessed.
The matrices ∆, Σ, and Q are positive deﬁnite. By specifying these hyperpa-
rameters the joint prior distribution is determined.
By Bayes’ rule, the joint posterior distribution for the unknown model pa-
rameters with speciﬁed generalized Conjugate prior distributions is given by
p(F,c,Σ|X) ∝p(F)p(c)p(Σ)p(X|F,C,Σ)
(9.6.5)
which is
p(F,c,Σ|X) ∝e−1
2 trF ′F |∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q]
(9.6.6)
after inserting the joint prior distribution and the likelihood.
The joint posterior distribution must now be evaluated in order to obtain
estimates of the parameters.
9.7
Generalized Estimation and Inference
With the generalized Conjugate prior distributions, it is not possible to
obtain all or any of the marginal distributions and thus marginal mean esti-
mates in closed form. It is also not possible to obtain explicit formulas for
maximum a posteriori estimates. For these reasons, marginal posterior mean
and joint maximum a posteriori estimates are found using the Gibbs sampling
and ICM algorithms.
© 2003 by Chapman & Hall/CRC

9.7.1
Posterior Conditionals
Both the Gibbs sampling and ICM algorithms require the posterior condi-
tionals. Gibbs sampling requires the conditionals for the generation of random
variates while ICM requires them for maximization by cycling through their
modes.
The conditional posterior distribution of the matrix of factor scores F is
found by considering only those terms in the joint posterior distribution which
only involve F and is given by
p(F|µ,Λ,Σ,X) ∝p(F)p(X|µ,F,Λ,Σ)
∝e−1
2 trF ′F |Σ|−n
2 e−1
2 trΣ−1(X−F Λ′)′(X−F Λ′)
∝e−1
2 trF ′F e−1
2 tr(X−enµ′−F Λ′)Σ−1(X−enµ′−F Λ′)′
which after performing some algebra in the exponent can be written as
p(F|µ,Λ,Σ,X) ∝e−1
2 tr(F −˜
F )(Im+Λ′Σ−1Λ)(F −˜
F)′,
(9.7.1)
where the matrix ˜F has been deﬁned to be
˜F = (X −enµ′)Σ−1Λ(Im +Λ′Σ−1Λ)−1.
(9.7.2)
That is, the matrix of factor scores F given the overall mean µ, the factor
loading matrix Λ, the error covariance matrix Σ, and the data X is Matrix
Normally distributed.
The conditional posterior distribution of the vector of means and factor
loadings is found by considering only those terms in the joint posterior distri-
bution which involve c or C and is given by
p(c|F,Σ,X) ∝p(c)p(X|F,C,Σ)
∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
(9.7.3)
which after performing some algebra in the exponent becomes
p(c|F,Σ,X) ∝e−1
2 (c−˜c)′[∆−1+Z′Z⊗Σ−1](c−˜c),
(9.7.4)
where the vector ˜c has been deﬁned to be
˜c = [∆−1 +Z′Z ⊗Σ−1]−1[∆−1c0 +(Z′Z ⊗Σ−1)ˆc]
(9.7.5)
and the vector ˆc has been deﬁned to be
© 2003 by Chapman & Hall/CRC

ˆc = vec[X′Z(Z′Z)−1].
(9.7.6)
Note that this is a weighted combination of the prior mean c0 from the
prior distribution and the data mean ˆc from the likelihood.
That is, the conditional posterior distribution of the vector containing the
overall mean vector µ and the factor loading vector λ = vec(Λ) given the
matrix of factor scores F, the error covariance matrix Σ, and the data X is
Multivariate Normally distributed.
The conditional posterior distribution of the error covariance matrix Σ is
found by considering only those terms in the joint posterior distribution which
involve Σ and is given by
p(Σ|F,C,X) ∝p(Σ)p(X|F,C,Σ)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q].
(9.7.7)
That is, the posterior conditional distribution of the error covariance matrix
Σ given the matrix of factor scores F, the overall mean µ, the matrix of factor
loadings Λ, and the data X has an Inverted Wishart distribution.
The modes of these conditional posterior distributions are as described in
Chapter 2 and given by ˜F, ˜c, (both as deﬁned above) and
˜Σ = (X −ZC′)′(X −ZC′)+Q
n+ν
,
(9.7.8)
respectively.
9.7.2
Gibbs Sampling
To ﬁnd marginal mean estimated of the parameters from the joint posterior
distribution using the Gibbs sampling algorithm, start with initial values for
the matrix of factor scores F and the error covariance matrix Σ, say ¯F(0) and
¯Σ(0), and then cycle through
¯c(l+1) = a random variate from p(c| ¯F(l), ¯Σ(l),X)
= AcYc +Mc,
(9.7.9)
¯Σ(l+1) = a random variate from p(Σ| ¯F(l),¯c(l+1),X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(9.7.10)
¯F(l+1) = a random variate from p(F|¯c(l+1), ¯Σ(l+1),X)
= YF B′
F +MF ,
(9.7.11)
where
© 2003 by Chapman & Hall/CRC

ˆc(l) = vec[X′ ¯Z(l)( ¯Z′
(l) ¯Z(l))−1],
¯c(l+1) = [∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc(l)],
AcA′
c = (∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )−1,
Mc = [∆−1 +
¯
¯Z′
(l) ¯Z(l) ⊗¯Σ
−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc],
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))+Q,
BF B′
F = (Im + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MF = (X −en¯µ′
(l+1))¯Σ−1
(l+1)¯Λ(l+1)(Im + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while Yc, YΣ, and YF are p(m + 1) × 1, (n + ν + p + 1) × p, and n × m di-
mensional matrices whose respective elements are random variates from the
standard Scalar Normal distribution. The formulas for the generation of ran-
dom variates from the conditional posterior distributions are easily found from
the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯F = 1
L
L

l=1
¯F(l)
¯c = 1
L
L

l=1
¯c(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters. Exact sampling-based estimates of other quantities can also be
found. Similar to Regression, there is interest in the estimate of the marginal
posterior variance of the vector containing the means and factor loadings
var(c|X) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆.
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the vector
containing the means and factor loadings, their distribution is
p(c|X) ∝| ¯∆|−1
2 e−1
2 (c−¯c)′ ¯∆−1(c−¯c),
(9.7.12)
where ¯c and ¯∆are as previously deﬁned.
To evaluate statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the vector c containing the means and factor
© 2003 by Chapman & Hall/CRC

loadings given above. General simultaneous hypotheses can be evaluated re-
garding the entire coeﬃcient vector of means and loadings, a subset of it, or
the coeﬃcients for a particular factor by computing marginal distributions. It
can be shown that the marginal distribution of the kth column of the matrix
containing the means and factor loadings C, Ck is Multivariate Normal
p(Ck| ¯Ck,X,U) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(9.7.13)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be evaluated for a subset of means or coeﬃcients of the kth
column of C by determining the marginal distribution of the subset within
Ck which is also Multivariate Normal. With the subset being a singleton set,
signiﬁcance can be determined for a particular mean or coeﬃcient with the
marginal distribution of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X,U) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(9.7.14)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(9.7.15)
follows a Normal distribution with a mean of zero and variance of one.
9.7.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
vector of coeﬃcients c, the matrix of factor scores F, and the error covariance
matrix Σ using the ICM algorithm. To maximize the joint posterior distrib-
ution using the ICM algorithm, start with initial values for the estimates of
the matrix of factor score matrix ˜F and the error covariance matrix Σ, say
˜F(0) and ˜Σ(0), and then cycle through
ˆc(l) = vec[X′ ˜Z(l)( ˜Z′
(l) ˜Z(l))−1],
˜c(l+1) =
Arg Max
c
p(c| ˜F(l), ˜Σ(l),X)
= [∆−1 + ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) ]−1[∆−1c0 +( ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) )ˆc(l)],
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜F(l),X)
=
(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))+Q
n+ν
,
˜F(l+1) =
Arg Max
F
p(F| ˜C(l+1), ˜Σ(l+1),X)
© 2003 by Chapman & Hall/CRC

= (X −en˜µ′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)(Im + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1
where the matrix ˜Z(l) = (en, ˜F(l)) has been deﬁned. Continue cycling until
convergence is reached with the joint modal estimator for the unknown pa-
rameters ( ˜F,˜c, ˜Σ). Conditional maximum a posteriori variance estimates can
also be found. The conditional modal variance of the matrix containing the
means and factor loadings is
var(c| ˜F, ˜Σ,X,U) = [∆−1 + ˜Z′ ˜Z ⊗˜Σ−1]−1
(9.7.16)
= ˜∆,
(9.7.17)
where c = vec(C), while ˜F and ˜Σ are the converged value from the ICM
algorithm.
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
The posterior conditional distribution of the matrix containing the means and
factor loadings C given the modal values of the other parameters and the data
is
p(c| ˜F, ˜Σ,X,U) ∝| ˜∆|−1
2 e−1
2 (c−˜c)′ ˜∆−1(c−˜c).
(9.7.18)
To determine statistical signiﬁcance with the ICM approach, use the mar-
ginal conditional distribution of the matrix containing the means and factor
loadings given above. General simultaneous hypotheses can be performed re-
garding the mean vector or the coeﬃcient for a particular factor by computing
marginal distributions. It can be shown that the marginal conditional distrib-
ution of the kth column Ck of the matrix C containing the overall mean vector
and factor loading matrix is Multivariate Normal
p(Ck| ¯Ck, ˜Σ,X,U) ∝| ˜∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ˜∆−1
k
(Ck−¯
Ck),
(9.7.19)
where ˜∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ˜∆.
Signiﬁcance can be determined for a subset of means or loadings of the kth
column of C by determining the marginal distribution of the subset within
Ck which is also Multivariate Normal. With the subset being a singleton set,
signiﬁcance can be determined for a particular mean or factor loading with
the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜F, ˜Σjj,X) ∝( ˜∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ˜
∆kj
,
(9.7.20)
where ˜∆kj is the jth diagonal element of ˜∆k. Note that ˜Ckj = ˜cjk and that
© 2003 by Chapman & Hall/CRC

z = (Ckj −˜Ckj)
,
˜∆kj
follows a Normal distribution with a mean of zero and variance of one.
9.8 
Interpretation
The main results of performing a Factor Analysis are estimates of the factor
score matrix, the factor loading matrix, and the error covariance matrix. The
results of a Factor Analysis are described with the use of an example.
Data are extracted from an example in [26] and have been used before in
Bayesian Factor Analysis [43, 44]. Applicants for a particular position have
been scored on ﬁfteen variables which are listed in Table 9.1.
The aim of performing this Factor Analysis is to determine an underlying
relationship between the original observed variables which is of lower dimen-
sion and to determine which applicants are candidates for being hired based
on these factors.
TABLE 9.1
Variables for Bayesian Factor Analysis example.
X
Variables
X1
Form of letter application
X2
Appearance
X3
Academic ability
X4
Likeabiliy
X5
Self-conﬁdence
X6
Lucidity
X7
Honesty
X8
Salesmanship
X9
Experience
X10
Drive
X11
Ambition
X12
Grasp
X13
Potential
X14
Keenness to join
X15
Suitability
The applicants were scored on a ten-point scale on ﬁfteen characteristics.
There are n = 48 observations on applicants which consists of p = 15 observed
variables. Table 9.2 contains the data for the applicant example. Note that
there are only X’s and not any U’s.
Tables 9.3 and 9.4 contain the Gibbs sampling and ICM estimates of the
factor scores. These are the new variables the applicants are rated on.
In a typical Factor Analysis, the sample size is usually large enough to esti-
mate the variances of the x’s as ˆσ2
1,..., ˆσ2
p, the maximum likelihood estimates,
© 2003 by Chapman & Hall/CRC

and scale the x’s to have unit variance. Having done this helps with the in-
terpretation of the factor loading matrix. The matrix of coeﬃcients, Λ which
was previously a matrix of covariances between x and f is now a matrix of
correlations.
Table 9.5 contains prior along with estimated Gibbs sampling and ICM
mean vectors with factor loading matrices.
The analysis implemented the
aforementioned Conjugate prior model. It has previously been determined to
use a model with m = 4 factors [43]. The rows of the factor loading matrices
have been rearranged for interpretation purposes.
It is seen that factor 1
“loads heavily” for variables 5, 6, 8, 10, 11, 12, and 13; factor 2, heavily on
variable 3; factor 3, heavily on variables 1, 9, and 15; while factor 4 loads
heavily on variables 4 and 7.
Since the observed vectors were scaled by their standard deviations and the
orthogonal factor model was used, the factor loading matrix is a matrix of
correlations between the p observed variables and the m unobserved factors.
For example, the correlation between observable variable 10 and unobservable
factor 1 is 0.6898 when estimated by Gibbs sampling.
© 2003 by Chapman & Hall/CRC

TABLE 9.2
Bayesian Factor Analysis data.
X
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1
6
7
2
5
8
7
8
8
3
8
9
7
5
7
10
2
9
10
5
8
10
9
9
10
5
9
9
8
8
8
10
3
7
8
3
6
9
8
9
7
4
9
9
8
6
8
10
4
5
6
8
5
6
5
9
2
8
4
5
8
7
6
5
5
6
8
8
8
4
5
9
2
8
5
5
8
8
7
7
6
7
7
7
6
8
7
10
5
9
6
5
8
6
6
6
7
9
9
8
8
8
8
8
8
10
8
10
8
9
8
10
8
9
9
9
8
9
9
8
8
10
9
10
9
9
9
10
9
9
9
7
8
8
8
8
5
9
8
9
8
8
8
10
10
4
7
10
2
10
10
7
10
3
10
10
10
9
3
10
11
4
7
10
0
10
8
3
9
5
9
10
8
10
2
5
12
4
7
10
4
10
10
7
8
2
8
8
10
10
3
7
13
6
9
8
10
5
4
9
4
4
4
5
4
7
6
8
14
8
9
8
9
6
3
8
2
5
2
6
6
7
5
6
15
4
8
8
7
5
4
10
2
7
5
3
6
6
4
6
16
6
9
6
7
8
9
8
9
8
8
7
6
8
6
10
17
8
7
7
7
9
5
8
6
6
7
8
6
6
7
8
18
6
8
8
4
8
8
6
4
3
3
6
7
2
6
4
19
6
7
8
4
7
8
5
4
4
2
6
8
3
5
4
20
4
8
7
8
8
9
10
5
2
6
7
9
8
8
9
21
3
8
6
8
8
8
10
5
3
6
7
8
8
5
8
22
9
8
7
8
9
10
10
10
3
10
8
10
8
10
8
23
7
10
7
9
9
9
10
10
3
9
9
10
9
10
8
24
9
8
7
10
8
10
10
10
2
9
7
9
9
10
8
25
6
9
7
7
4
5
9
3
2
4
4
4
4
5
4
26
7
8
7
8
5
4
8
2
3
4
5
6
5
5
6
27
2
10
7
9
8
9
10
5
3
5
6
7
6
4
5
28
6
3
5
3
5
3
5
0
0
3
3
0
0
5
0
29
4
3
4
3
3
0
0
0
0
4
4
0
0
5
0
30
4
6
5
6
9
4
10
3
1
3
3
2
2
7
3
31
5
5
4
7
8
4
10
3
2
5
5
3
4
8
3
32
3
3
5
7
7
9
10
3
2
5
3
7
5
5
2
33
2
3
5
7
7
9
10
3
2
2
3
6
4
5
2
34
3
4
6
4
3
3
8
1
1
3
3
3
2
5
2
35
6
7
4
3
3
0
9
0
1
0
2
3
1
5
3
36
9
8
5
5
6
6
8
2
2
2
4
5
6
6
3
37
4
9
6
4
10
8
8
9
1
3
9
7
5
3
2
38
4
9
6
6
9
9
7
9
1
2
10
8
5
5
2
39
10
6
9
10
9
10
10
10
10
10
8
10
10
10
10
40
10
6
9
10
9
10
10
10
10
10
10
10
10
10
10
41
10
7
8
0
2
1
2
0
10
2
0
3
0
0
10
42
10
3
8
0
1
1
0
0
10
0
0
0
0
0
10
43
3
4
9
8
2
4
5
3
6
2
1
3
3
3
8
44
7
7
7
6
9
8
8
6
8
8
10
8
8
6
5
45
9
6
10
9
7
7
10
2
1
5
5
7
8
4
5
46
9
8
10
10
7
9
10
3
1
5
7
9
9
4
4
47
0
7
10
3
5
0
10
0
0
2
2
0
0
0
0
48
0
6
10
1
5
0
10
0
0
2
2
0
0
0
0
© 2003 by Chapman & Hall/CRC

TABLE 9.3
Gibbs sampling estimates of factor scores.
¯F
1
2
3
4
1
0.3077
-3.3603
-0.2125
-0.5730
2
0.7924
-1.6057
0.1965
0.2117
3
0.5037
-2.7629
-0.0012
-0.1239
4
-0.7681
0.4642
-0.2068
0.1312
5
-1.1131
0.1111
-0.1170
0.6952
6
-0.2053
-0.1476
0.3176
0.5089
7
0.3489
0.2003
0.7723
0.0877
8
0.5909
0.8982
0.7203
0.0415
9
0.1043
-0.4084
0.7848
0.1326
10
1.7144
2.1089
-0.1417
-1.3574
11
1.2927
1.9320
-0.7402
-2.8422
12
1.3329
2.0431
-0.6587
-0.7475
13
-1.2701
0.2614
-0.5932
1.0057
14
-1.1671
0.3324
-0.1722
0.6009
15
-1.1925
0.3136
-0.4554
0.8168
16
0.2348
-1.0655
0.1624
-0.1061
17
-0.0942
-0.0641
0.1559
0.0120
18
-0.5280
0.7515
-0.9473
-1.2914
19
-0.5392
0.6903
-0.6783
-1.3880
20
0.1663
-0.1587
-0.8480
0.7384
21
0.1888
-0.8514
-0.8602
0.8835
22
0.8396
-0.1468
-0.2744
0.6413
23
0.7156
-0.3827
-0.8944
0.6872
24
0.5293
-0.2395
-0.4618
1.1675
25
-1.4216
-0.2283
-1.1918
0.3591
26
-1.1812
-0.2195
-0.4864
0.4257
27
-0.1117
-0.2813
-1.6328
1.0079
28
-1.9465
-0.9311
-1.5140
-1.2838
29
-2.4544
-1.9705
-2.2364
-2.6975
30
-1.2091
-1.0386
-1.6438
0.3865
31
-0.9775
-1.8423
-1.4264
0.7280
32
-0.3479
-1.1174
-1.3412
1.1795
33
-0.6294
-1.0550
-1.5186
1.1185
34
-1.8538
-0.5416
-1.7054
-0.2570
35
-2.3996
-1.9346
-1.0801
-0.4663
36
-1.0747
-1.3773
-0.5516
-0.2966
37
0.5600
-0.5530
-1.7533
-0.7706
38
0.3733
-0.7114
-2.0248
-0.6473
39
0.8246
1.0423
1.0749
1.3483
40
0.9801
1.0611
1.0504
1.3164
41
-2.4371
0.5326
2.0433
-2.8728
42
-2.7783
0.7600
2.2499
-3.1261
43
-2.1129
1.1138
-0.3480
-0.0329
44
0.5688
-0.1781
-0.1073
-0.0994
45
-0.2459
2.1388
-0.0444
1.5419
46
0.1637
1.9385
-0.3786
1.6403
47
-2.0181
2.2786
-2.4526
-0.0478
48
-1.9193
2.4105
-2.2961
-0.4600
© 2003 by Chapman & Hall/CRC

TABLE 9.4
ICM estimates of factor scores.
˜F
1
2
3
4
1
0.1606
-3.5584
-0.3575
-0.5349
2
0.7575
-1.6808
0.2195
0.3713
3
0.3921
-2.9206
-0.1091
-0.0416
4
-0.8869
0.5279
-0.4046
0.0312
5
-1.0895
0.2790
-0.1725
0.6625
6
-0.3319
-0.1462
0.1647
0.4682
7
0.3905
0.2531
0.8603
0.1704
8
0.6483
0.9351
0.8176
0.1400
9
0.1116
-0.3768
0.8034
0.2049
10
1.4590
2.0559
-0.4261
-1.3752
11
1.0815
1.9658
-0.9212
-2.9114
12
1.0971
2.0434
-0.9648
-0.8000
13
-1.2880
0.3323
-0.6155
1.0498
14
-1.2535
0.4241
-0.2638
0.6192
15
-1.2975
0.4498
-0.6034
0.7697
16
0.2113
-1.0312
0.1744
-0.0367
17
-0.2017
-0.1779
0.0809
0.0221
18
-0.7106
0.6368
-1.1290
-1.2534
19
-0.7226
0.6290
-0.9075
-1.4296
20
0.0829
-0.1970
-1.0102
0.8057
21
0.0443
-0.8392
-1.0544
0.9045
22
0.8245
-0.2741
-0.3225
0.7138
23
0.7634
-0.4100
-0.8217
0.8613
24
0.5545
-0.3338
-0.4949
1.2228
25
-1.5303
-0.2132
-1.3128
0.3743
26
-1.2976
-0.2034
-0.6532
0.3946
27
-0.2345
-0.2079
-1.7420
1.0966
28
-2.2280
-1.2265
-1.8600
-1.5166
29
-2.5958
-2.1770
-2.4250
-2.9581
30
-1.4420
-1.2948
-1.8597
0.3869
31
-1.1459
-2.0788
-1.6079
0.6640
32
-0.5964
-1.2671
-1.7646
0.9369
33
-0.8973
-1.2243
-1.9410
0.9053
34
-2.0689
-0.6754
-2.0180
-0.4497
35
-2.6429
-2.0414
-1.3609
-0.5105
36
-1.2520
-1.4339
-0.7874
-0.3159
37
0.2833
-0.6483
-1.9382
-0.6854
38
0.2040
-0.7895
-2.1215
-0.5485
39
0.8801
1.0039
1.0739
1.3098
40
1.0284
1.0084
1.0633
1.2868
41
-2.6775
0.6079
1.7383
-3.0415
42
-3.0521
0.7185
1.8513
-3.4691
43
-2.2026
1.1547
-0.5726
-0.2761
44
0.4625
-0.1958
-0.1921
-0.1354
45
-0.4851
2.0908
-0.4240
1.3945
46
-0.0210
1.9787
-0.6763
1.5681
47
-2.3946
2.2267
-2.7794
-0.1245
48
-2.3430
2.3154
-2.6884
-0.5719
© 2003 by Chapman & Hall/CRC

TABLE 9.5
Prior, Gibbs, and ICM means and loadings.
C0
0
1
2
3
4
5
7.5
.7
0
0
0
6
7.5
.7
0
0
0
8
7.5
.7
0
0
0
10
7.5
.7
0
0
0
11
7.5
.7
0
0
0
12
7.5
.7
0
0
0
13
7.5
.7
0
0
0
3
7.5
0
.7
0
0
1
7.5
0
0
.7
0
9
7.5
0
0
.7
0
15
7.5
0
0
.7
0
4
7.5
0
0
0
.7
7
7.5
0
0
0
.7
2
7.5
0
0
0
0
14
7.5
0
0
0
0
¯C
0
1
2
3
4
5
7.5851
0.7867
-0.0494
-0.1259
0.0099
6
7.4334
0.7396
-0.0335
0.0362
0.1036
8
6.3612
0.7841
-0.0739
0.0834
-0.0483
10
6.6714
0.6898
-0.0519
0.1883
0.0413
11
7.0915
0.7839
-0.0725
0.0183
-0.0640
12
7.4275
0.6937
0.0126
0.1423
0.1298
13
7.0281
0.6432
0.0613
0.1839
0.2076
3
7.2963
0.0292
0.6941
0.0875
0.0335
1
7.1472
0.0460
-0.0934
0.7389
0.0590
9
5.9576
0.0276
0.0211
0.7905
0.0069
15
7.4903
0.2225
-0.0688
0.7138
0.0465
4
6.7395
0.0980
-0.0685
0.1495
0.6966
7
7.9370
0.1019
-0.0121
-0.1473
0.6985
2
7.4223
0.2957
-0.0158
0.0523
0.1103
14
6.4301
0.3017
-0.2900
0.1879
0.2995
˜C
0
1
2
3
4
5
7.6941
0.7828
-0.0472
-0.1676
-0.0111
6
7.6225
0.7315
-0.0212
-0.0087
0.0742
8
6.6418
0.7869
-0.0664
0.0645
-0.0738
10
6.9201
0.6839
-0.0439
0.1716
0.0120
11
7.3092
0.7885
-0.0630
0.0004
-0.0844
12
7.6515
0.6792
0.0356
0.1070
0.1103
13
7.2853
0.6247
0.0887
0.1609
0.1898
3
7.3538
0.0277
0.7191
0.0638
0.0270
1
7.3959
0.0049
-0.0942
0.7606
0.0622
9
6.3038
-0.0090
0.0423
0.8184
-0.0002
15
7.8412
0.1817
-0.0526
0.7233
0.0453
4
6.8584
0.0557
-0.0544
0.1834
0.7227
7
7.9147
0.0499
-0.0016
-0.1483
0.7339
2
7.5305
0.2822
0.0111
0.0836
0.1548
14
6.5979
0.2903
-0.2993
0.2279
0.3115
© 2003 by Chapman & Hall/CRC

These factors which are based on Table 9.5 may be loosely interpreted as
factor 1 being a measure of personality, factor 2 being a measure of academic
ability, factor 3 being a measure of position match, and factor 4 being a
measure of what can be described as charisma as presented in Table 9.6.
TABLE 9.6
Factors in terms of strong observation loadings.
Factor 1:
5 Self-conﬁdence, 6 Lucidity, 8 Salesmanship, 10 Drive,
11 Ambition, 12 Grasp, 13 Potential
Factor 2:
3 Academic ability
Factor 3:
1 Form of letter application, 9 Experience, 15 Suitability
Factor 4:
4 Likeabiliy, 7 Honesty
The Gibbs sampling marginal values of the observation error variances and
covariances are the elements of Table 9.7 while the ICM values of the observa-
tion error variances and covariances are the elements of Table 9.8. Note that
the estimates from the two methods similar but with minor diﬀerences.
Table 9.9 contains the statistics for the individual posterior means and
loadings. Inspection can reveal which are “large.”
© 2003 by Chapman & Hall/CRC

TABLE 9.7
Gibbs estimates of Factor Analysis covariances.
¯Ψ
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1
.2166 .0603 -.0014 .0681
.0116
.0063 -.0274
.0215 -.0359
.0148
.0527
.0151
.0227
.1258 -.0357
2
.4472
.0458 .0637
.0154
-.0270
.0589
.0462
.0268
-.0247
.0874
.0459
.0487
.0185
.0909
3
.0504
0246 -.0138
-.0005 -.0107
.0126
.0315
.0137
.0144
.0165
.0303
.0084
.0143
4
.1451 -.0165
.0307 -.0595
.0513
.0419
.0325
.0537
.0258
.0534
.1035
.0452
5
.0876
-.0093
.0303
.0023
.0043
-.0123
.0138 -.0293 -.0320
.0181 -.0198
6
.0986 -.0317
.0112
.0003
-.0302 -.0240
.0439 -.0002
.0205 -.0051
7
.1206 -.0161 -.0115
-.0117 -.0189 -.0104 -.0303 -.0188
.0055
8
.1231
.0496
.0486
.0319 -.0083
.0067
.0773
.0538
9
.2205
.0511
.0424
.0194
.0398
.0710
.0220
10
.1771
.0297 -.0212
.0417
.1036
.0561
11
.1103
.0079
.0303
.0713
.0154
12
.1072
.0417
.0371
.0057
13
.1167
.0422
.0247
14
.2939
.0364
15
.1565
© 2003 by Chapman & Hall/CRC

TABLE 9.8
ICM estimates of Factor Analysis covariances.
˜Ψ
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1
.1731 .0287
.0009 .0359
.0097
.0028 -.0265 -.0017 -.0799 -.0100
.0315
.0043
.0040
.0808 -.0715
2
.4147
.0209 .0184
.0087 -.0425
.0318
.0275 -.0107 -.0481
.0718
.0214
.0166 -.0128
.0551
3
.0078 .0159 -.0093 -.0035 -.0101
.0085
.0123
.0091
.0086
.0029
.0098
.0185
.0054
4
.1006 -.0288
.0166 -.0830
.0271
.0104
.0100
.0306
.0042
.0263
.0643
.0090
5
.0817 -.0155
.0320 -.0088
.0002 -.0231
.0036 -.0352 -.0398 -.0033 -.0214
6
.0855 -.0250 -.0075 -.0081 -.0459 -.0404
.0312 -.0142 -.0035 -.0110
7
.0838 -.0087 -.0047 -.0029 -.0130 -.0086 -.0289 -.0300
.0048
8
.0882
.0186
.0162
.0047 -.0297 -.0198
.0384
.0271
9
.1531
.0186
.0147
.0000
.0099
.0300 -.0251
10
.1397
.0006 -.0419
.0140
.0634
.0277
11
.0827 -.0127
.0046
.0366 -.0070
12
.0867
.0189
.0085 -.0087
13
.0839
.0107
.0005
14
.2327 -.0080
15
.1136
© 2003 by Chapman & Hall/CRC

TABLE 9.9
Statistics for means and loadings.
zGibbs
0
1
2
3
4
5
121.1806
16.5189
-1.1570
-2.4151
0.1896
6
113.8556
14.7298
-0.7452
0.6357
1.9246
8
85.5536
15.2162
-1.5884
1.3252
-0.7857
10
81.8379
10.5490
-0.9250
2.6814
0.5879
11
100.1574
16.0036
-1.4903
0.2825
-1.0762
12
110.8510
13.0914
0.2640
2.4543
2.3607
13
99.0115
11.2402
1.2095
3.1098
3.8215
3
98.4057
0.5196
16.2240
1.5184
0.5861
1
75.5597
0.5372
-1.3360
8.4302
0.6707
9
57.7264
0.3361
0.3013
10.2866
0.0884
15
84.6348
3.0311
-1.0764
10.4162
0.6551
4
82.3772
1.2793
-1.1176
2.0990
10.7559
7
105.5962
1.5913
-0.2195
-2.2840
11.2131
2
70.1764
2.9582
-0.1531
0.4033
0.9846
14
67.9739
3.5476
-4.3737
1.9789
3.4330
zICM
0
1
2
3
4
5
73.3077
22.3229
-1.6097
-4.6372
-0.3237
6
54.3596
20.3900
-0.7057
-0.2361
2.1094
8
42.5136
21.5955
-2.1822
1.7176
-2.0642
10
41.3721
14.9170
-1.1464
3.6325
0.2657
11
57.0133
22.3481
-2.1352
0.0103
-2.4375
12
56.3954
18.8087
1.1811
2.8743
3.1123
13
52.0442
17.5842
2.9896
4.3949
5.4445
3
276.2472
2.5567
79.5316
5.7244
2.5460
1
43.7899
0.0962
-2.2092
14.4615
1.2418
9
32.0644
-0.1868
1.0536
16.5418
-0.0045
15
46.4278
4.3946
-1.5214
16.9757
1.1163
4
50.7638
1.4320
-1.6729
4.5750
18.9345
7
71.0275
1.4054
-0.0556
-4.0504
21.0602
2
39.1745
3.5729
0.1679
1.0275
1.9980
14
33.8990
4.9054
-6.0528
3.7362
5.3657
9.9
Discussion
There has been some recent work related to the Bayesian Factor Analysis
model. In a model which speciﬁes independence between the overall mean
µ vector and the factor loading matrix Λ with a vague prior distribution for
the overall mean µ, robustness of assessed hyperparameters was investigated
[34, 35] and the hyperparameters were found to be robust but most sensitive to
© 2003 by Chapman & Hall/CRC

the prior mean on the factor loadings Λ0. This indicates that the most care
should be taken when assessing this hyperparameter. For the same model,
methods for assessing the hyperparameters were presented [20, 21]. A prior
distribution was placed on the number of factors and then estimated a poste-
riori [45, 50]. For the same model, the process of estimating the overall mean
by the sample mean was evaluated [55] and the parameters were estimated
by Gibbs sampling and ICM [65]. The same Bayesian Factor Analysis model
was extended to the case of the observation vectors and also the factor score
vectors being correlated [50]. Bayesian Factor Analysis models which speciﬁed
independence between the overall mean vector and the factor loadings matrix
but took Conjugate and generalized Conjugate were introduced [51, 54].
Returning to the cocktail party problem, the matrix of factor loadings Λ is
the mixing matrix which determines the contribution of each of the speakers
to the mixed observations. The matrix of factor scores F is likened to the
matrix of unobserved conversations S. The overall population mean µ is the
overall background mean level at the microphones.
© 2003 by Chapman & Hall/CRC

Exercises
1. Specify that the overall mean µ and the factor loading matrix Λ are
independent with the prior distribution for the overall mean µ being the
vague prior
p(µ) ∝(a constant),
the distribution for the factor loading matrix being the Matrix Normal
distribution
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 9.4.3–9.4.4.
Combine these prior distributions with the likelihood in Equation 9.3.3
to obtain a joint posterior distribution. Integrate the joint posterior dis-
tribution with respect to Σ then Λ to obtain a marginal posterior distri-
bution for the matrix of factor scores. Use the large sample approxima-
tion F ′F
n
= Im to obtain an approximate marginal posterior distribution
for the matrix of factor scores which is a Matrix Student T-distribution.
Estimate the matrix of factor scores to be the mean of the approximate
marginal posterior distribution, the matrix of factor loadings given the
above mentioned factor scores, and then the error covariance matrix
given the factor scores and loadings [43, 44].
2. Specify that the overall mean µ and the factor loading matrix Λ are
independent with the prior distribution for the overall mean µ being the
vague prior
p(µ) ∝(a constant),
the prior distribution for the factor loading matrix being the Matrix
Normal distribution
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 9.4.3-9.4.4.
Combine these prior distributions with the likelihood in Equation 9.3.3
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal posterior mean and joint maximum a posteriori
parameter estimates [65].
© 2003 by Chapman & Hall/CRC

3. Specify that the overall mean µ and the factor loading matrix Λ are
independent with the prior distribution for the overall mean µ being the
Conjugate Normal prior
p(µ|Σ) ∝|hΣ|−1
2 e−1
2 trΣ−1(µ−µ0)(hΣ)−1(µ−µ0)′,
the distribution for the factor loading matrix being
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 Σ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 9.4.3-9.4.4.
Combine these prior distributions with the likelihood in Equation 9.3.3
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [54].
4. Specify that µ and Λ are independent with the prior distribution for the
overall mean µ to be the generalized Conjugate prior
p(µ) ∝|Γ|−1
2 e−1
2 (µ−µ0)′Γ−1(µ−µ0),
the distribution for the factor loading matrix being
p(λ) ∝|∆|−1
2 e−1
2 (λ−λ0)′∆−1(λ−λ0),
where λ = vec(Λ) and the other distributions are as in Equations 9.4.3–
9.4.4.
Combine these prior distributions with the likelihood in Equation 9.3.3
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal posterior mean and joint maximum a posteriori
parameter estimates [51].
© 2003 by Chapman & Hall/CRC

10
Bayesian Source Separation
10.1
Introduction
The Bayesian Source Separation model is diﬀerent from the Bayesian Re-
gression model in that the sources are unobserved and from the Bayesian
Factor Analysis model in that there may be more or less sources than the
observed dimension (the number of microphones). Further, in the Bayesian
Factor Analysis model, the variance of the unobserved factor score vectors
is a priori assumed to be unity and diagonal for Psychologic reasons. The
Bayesian Source Separation model [52, 57, 58, 59, 60, 62, 63] allows the co-
variance matrix for the unobserved sources to have arbitrary variances. That
is, the covariance matrix for the sources is not required to be diagonal and
also the sources are allowed to have a mean other than zero.
With a general covariance matrix (one that is not constrained to be diago-
nal), the sources or speakers at the cocktail party are allowed to be dependent
or correlated. There are other models which impose either the constraint of
orthogonal sources [23] or the constraint of independent sources [6]. If the
sources are truly orthogonal or independent, then such models would be ap-
propriate (independent sources can be obtained here by imposing constraints).
However, if the sources are not independent as in the “real-world” cocktail
party problem, then an independence constraint would not be appropriate.
10.2
Source Separation Model
In the Bayesian approach to statistical inference, available prior informa-
tion either from subjective expert experience, or prior experiments, is incor-
porated into the inferences. This prior information yields progressively less
inﬂuence in the ﬁnal results as the sample size increases, thus allowing the
data to “speak the truth.” The components of the source vectors are free to
be correlated, as is frequently the case and not constrained to be statistically
independent.
The constraint of independent sources models the situation where speakers
© 2003 by Chapman & Hall/CRC

at the cocktail party are talking without regard to the others at the party. The
independent source model implies that the people are “babbling” incoherently.
This is the case when we press play on several tape recorders with one speaker
on each and record on others. This is not how conversations work. This does
not model the true dynamics of a real cocktail party. Referring to Figure 1.1,
focus on the two people in the left foreground. When they speak, they do
not speak irrespective of each other. They speak interactively. For instance,
the person on the left will speak and then fade out while the person on the
right fades in to speak. They are not speaking at the same time. They are
obviously negatively correlated or dependent in a negative fashion.
The linear synthesis Source Separation model which was motivated in Chap-
ter 1 is given by
(xi|µ,Λ,si) =
µ
+
Λ
si
+
ǫi,
(p×1)
(p×1)
(p×m) (m×1)
(p×1)
(10.2.1)
where for observations xi at time increment i, i = 1,...,n; xi = a p-dimensional
observed vector, xi = (xi1,...,xip)′; µ is an overall unobserved mean vector,
µ = (µ1,...,µp)′; Λ = a p × m matrix of unobserved mixing constants, Λ =
(λ′
1,...,λ′
p)′; si = the ith m-dimensional true unobservable source vector, si =
(si1,...,s1m)′; and ǫi = the p-dimensional vector of errors or noise terms of
the ith observed signal vector ǫi = (ǫi1,...,ǫip)′.
Taking a closer look at the model, element (microphone) j in observed
vector (at time) i is represented by the model
(xij|µj,λj,si) =
µj
+
λ′
j
si
+
ǫij,
(1×1)
(1×1)
(1×m) (m×1)
(1×1)
(10.2.2)
in which the recorded or observed conversation xij for microphone j at time
increment i is a linear mixture of the m true unobservable conversations at
time increment i plus an overall (background) mean for microphone j and a
random noise term ǫij. This is also represented as
(xij|µj,λj,si) = µj +
m

k=1
λjk sik +ǫij.
(10.2.3)
Analogous to the Regression and Factor Analysis models, the Source Sep-
aration model can be written in terms of matrices as
(X|µ,Λ,S) =
enµ′
+
S
Λ′
+
E,
(n×p)
(n×p)
(n×m) (m×p)
(n×p)
(10.2.4)
where X′ = (x1,...,xn), en is an n-dimensional vector of ones, µ = (µ1,...,µp),
S′ = (s1,...,sn), Λ′ = (λ1,...,λp), and E′ = (ǫ1,...,ǫn).
© 2003 by Chapman & Hall/CRC

10.3
Source Separation Likelihood
Regarding the errors of the observations, it is speciﬁed that they are inde-
pendent Normally distributed random vectors with mean zero and full positive
deﬁnite covariance matrix Σ. From this error speciﬁcation, it is seen that the
observation vector xi given the overall background mean µ, the mixing ma-
trix Λ, the source vector si, and the error covariance matrix Σ is Multivariate
Normally distributed with likelihood given by
p(xi|µ,Λ,si,Σ) ∝|Σ|−1
2 e−1
2 (xi−µ−Λsi)′Σ−1(xi−µ−Λsi).
(10.3.1)
With the previously described matrix representation, the joint likelihood of
all n observation vectors collected into the matrix X is given by
p(X|µ,S,Λ,Σ) ∝|Σ|−n
2 e−1
2 tr(X−enµ′−SΛ′)Σ−1(X−enµ′−SΛ′)′,
(10.3.2)
where the variables are as previously deﬁned.
The overall background mean vector µ and the mixing matrix Λ are joined
into a single matrix as C = (µ,Λ). An n-dimensional vector of ones en and the
source matrix S are also joined as Z = (en,S). Having joined these vectors
and matrices, the Source Separation model is now in a matrix representation
given by
(X|C,Z) =
Z
C′
+
E,
n×p
n×(m+1) (m+1)×p
(n×p)
(10.3.3)
and its corresponding likelihood is given by the Matrix Normal distribution
p(X|C,Z,Σ) ∝|Σ|−n
2 e−1
2 tr(X−ZC′)Σ−1(X−ZC′)′,
(10.3.4)
where all variables are as previously deﬁned and tr(·) denotes the trace oper-
ator.
Again, the objective is to unmix the sources by estimating the matrix con-
taining them S, and to obtain knowledge about the mixing process by esti-
mating the overall mean µ, the mixing matrix Λ, and the error covariance
matrix Σ.
The advantage of the Bayesian statistical approach is that available prior
information regarding parameters are quantiﬁed through probability distrib-
utions describing degrees of belief for various values. This prior knowledge
is formally brought to bear in the problem through prior distributions and
Bayes’ rule. As stated earlier, the prior parameter values will have decreasing
inﬂuence in the posterior estimates with increasing sample size, thus allowing
the data to “speak the truth.”
© 2003 by Chapman & Hall/CRC

10.4 
Conjugate Priors a nd Po sterior
In the Bayesian Source Separation model [60] available information re-
garding values of the model parameters is quantiﬁed using Conjugate prior
distributions. The joint prior distribution for the model parameters which are
the matrix of coeﬃcients C, the matrix of sources S, the covariance matrix
for the sources R, and the error covariance matrix Σ is given by
p(S,R,C,Σ) = p(S|R)p(R)p(C|Σ)p(Σ),
(10.4.1)
where the prior distribution for the parameters from the Conjugate procedure
outlined in Chapter 4 are as follows
p(S|R) ∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′,
(10.4.2)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(10.4.3)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(10.4.4)
p(C|Σ) ∝|D|−p
2 |Σ|−m
2 e−1
2 trΣ−1(C−C0)D−1(C−C0)′,
(10.4.5)
where Σ, R, V , Q, and D are positive deﬁnite matrices. The hyperparameters
S0, η, V , ν, Q, C0, and D are to be assessed and having done so, completely
determine the joint prior distribution.
The prior distributions for the combined matrix containing the overall mean
µ with the mixing matrix Λ, the sources S, the source covariance matrix R,
and the error covariance matrix Σ follow Normal, Normal, Inverted Wishart,
and Inverted Wishart distributions respectively.
Note that both Σ and R are full positive deﬁnite symmetric covariance
matrices which allow both the observed mixed signals (elements in the xi’s)
and also the unobserved source components (elements in the si’s) to be cor-
related. The prior mean of the sources is often taken to be constant for all
observations and thus without loss of generality taken to be zero. Here an
observation (time) varying source mean is adopted.
Upon using Bayes’ rule, the joint posterior distribution for the unknown
parameters is proportional to the product of the joint prior distribution and
the likelihood and given by
p(S,R,C,Σ|X) ∝|Σ|−(n+ν+m+1)
2
e−1
2 trΣ−1G
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(10.4.6)
where the p×p matrix variable G has been deﬁned to be
© 2003 by Chapman & Hall/CRC

G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q.
(10.4.7)
This joint posterior distribution must now be evaluated in order to obtain
parameter estimates of the sources S, the overall background mean/mixing
matrix C, the source covariance matrix R, and the observation errors covari-
ance matrix Σ.
Marginal posterior mean and joint maximum a posteriori
estimates of the parameters S, R, C, and Σ are found by the Gibbs sampling
and ICM algorithms.
10.5
Conjugate Estimation and Inference
With the above joint posterior distribution for the Bayesian Source Separa-
tion model, it is not possible to obtain all or any of the marginal distributions
and thus marginal estimates of the parameters in an analytic closed form.
It is also not possible to obtain explicit formulas for maximum a posteriori
estimates from diﬀerentiation. It is possible to use both Gibbs sampling, to
obtain marginal parameter estimates and the ICM algorithm for maximum
a posteriori estimates. For both estimation procedures, the posterior condi-
tional distributions are required.
10.5.1
Posterior Conditionals
From the joint posterior distribution we can obtain the posterior condi-
tional distributions for each of the model parameters.
The conditional posterior distribution for the overall mean/mixing matrix
C is found by considering only the terms in the joint posterior distribution
which involve C and is given by
p(C|S,R,Σ,X) ∝p(C|Σ)p(X|C,S,Σ)
∝|Σ|−m+1
2 e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝e−1
2 trΣ−1[(C−C0)D−1(C−C0)′+(X−ZC′)′(X−ZC′)]
∝e−1
2 trΣ−1(C−˜
C)(D−1+Z′Z)(C−˜
C)′,
(10.5.1)
where the variable ˜C, the posterior conditional mean and mode, has been
deﬁned and is given by
˜C = (C0D−1 +X′Z)(D−1 +Z′Z)−1
(10.5.2)
© 2003 by Chapman & Hall/CRC

= C0[D−1(D−1 +Z′Z)−1]+ ˆC[(Z′Z)(D−1 +Z′Z)−1].
(10.5.3)
Note that the matrix ˜C can be written as a weighted combination of the
prior mean C0 from the prior distribution and the data mean ˆC = X′Z(Z′Z)−1
from the likelihood.
The posterior conditional distribution for the matrix of coeﬃcients C given
the matrix of sources S, the source covariance matrix R, the error covariance
matrix Σ, and the data matrix X is Matrix Normally distributed.
The conditional posterior distribution of the error covariance matrix Σ is
found by considering only those terms in the joint posterior distribution which
involve Σ and is given by
p(Σ|S,R,C,X) ∝p(Σ)p(C|Σ)p(X|S,C,Σ)
∝|Σ|−ν
2 e−1
2 trΣ−1Q|Σ|−m+1
2 e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝|Σ|−(n+ν+m+1)
2
e−1
2 trΣ−1G,
(10.5.4)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q
(10.5.5)
with a mode as described in Chapter 2 given by
˜Σ =
G
n+ν +m+1 .
(10.5.6)
The posterior conditional distribution of the observation error covariance
matrix Σ given the matrix of sources S, the source covariance matrix R, the
matrix of coeﬃcients C, and the data X is an Inverted Wishart.
The conditional posterior distribution for the sources S is found by consid-
ering only those terms in the joint posterior distribution which involve S and
is given by
p(S|µ,R,Λ,Σ,X) ∝p(S|R)p(X|µ,Λ,S,Σ)
∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−enµ′−SΛ′)′(X−enµ′−SΛ′)
∝e−1
2 tr(S−˜S)(R−1+Λ′Σ−1Λ)(S−˜S)′,
(10.5.7)
where the matrix ˜S has been deﬁned which is the posterior conditional mean
and mode given by
˜S = [S0R−1 +(X −enµ′)Σ−1Λ](R−1 +Λ′Σ−1Λ)−1.
(10.5.8)
© 2003 by Chapman & Hall/CRC

The conditional posterior distribution for the sources S given the overall
mean vector µ, the source covariance matrix R, the matrix of mixing coef-
ﬁcients Λ, the error covariance matrix Σ, and the data matrix X is Matrix
Normally distributed.
The conditional posterior distribution for the source covariance matrix R
is found by considering only those terms in the joint posterior distribution
which involve R and is given by
p(R|µ,Λ,S,Σ,X) ∝p(R)p(S|R)p(X|µ,Λ,S,Σ)
∝|R|−η
2 e−1
2 trR−1V |R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
∝|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(10.5.9)
with the posterior conditional mode as described in Chapter 2 given by
˜R = (S −S0)′(S −S0)+V
n+η
.
(10.5.10)
The conditional posterior distribution for the source covariance matrix R
given the matrix of sources S, the error covariance matrix Σ, the matrix of
means and mixing coeﬃcients C, and the data matrix X is Inverted Wishart
distributed.
10.5.2
Gibbs Sampling
To ﬁnd marginal posterior mean estimates of the model parameters from
the joint posterior distribution using the Gibbs sampling algorithm, start with
initial values for the matrix of sources S and the error covariance matrix Σ,
say ¯S(0) and ¯Σ(0), and then cycle through
¯C(l+1) = a random variate from p(C| ¯S(l), ¯R(l), ¯Σ(l),X)
= ACYCB′
C +MC,
(10.5.11)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l), ¯C(l+1),X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(10.5.12)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1),X)
= AR(Y ′
RYR)−1A′
R,
(10.5.13)
¯S(l+1) = a random variate from p(S| ¯R(l+1), ¯C(l+1), ¯Σ(l+1),X)
= YSB′
S +MS,
(10.5.14)
where
ACA′
C = ¯Σ(l),
© 2003 by Chapman & Hall/CRC

BCB′
C = (D−1 + ¯Z′
(l) ¯Z(l))−1,
¯Z(l) = (en, ¯S(l)),
MC = (X′ ¯Z(l) +C0D−1)(D−1 + ¯Z′
(l) ¯Z(l))−1
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))
+( ¯C(l+1) −C0)D−1( ¯C(l+1) −C0)′ +Q,
ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
BSB′
S = ( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MS = [S0R−1
(l+1)(X −en¯µ′
(l+1))¯Σ−1
(l+1)˜Λ(l+1)]
×( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while YC, YΣ, YR, and YS are p × (m + 1), (n + ν + m + 1 + p + 1) × p, (n +
η +m+1)×m, and n×m dimensional matrices respectively, whose elements
are random variates from the standard Scalar Normal distribution. The for-
mulas for the generation of random variates from the conditional posterior
distributions are easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters. Exact sampling-based estimates of other quantities can also be
found. Similar to Bayesian Regression and Bayesian Factor Analysis, there
is interest in the estimate of the marginal posterior variance of the vector
containing the means and mixing coeﬃcients
var(c|X) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C).
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the matrix
containing the mean vector and mixing matrix, their distribution is
p(c|X) ∝| ¯∆|−1
2 e−1
2 (c−¯c) ¯∆−1(c−¯c)′,
(10.5.15)
where ¯c and ¯∆are as previously deﬁned.
© 2003 by Chapman & Hall/CRC

To determine statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the matrix containing the mean vector and mix-
ing matrix given above. General simultaneous hypotheses can be evaluated
regarding the entire matrix containing the mean vector and the mixing ma-
trix, a submatrix, or the mean vector or a particular source, or an element by
computing marginal distributions. It can be shown that the marginal distrib-
ution of the kth column of the matrix containing the mean vector and mixing
matrix C, Ck is Multivariate Normal
p(Ck| ¯Ck,X) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(10.5.16)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be evaluated for a subset of coeﬃcients of the kth column of
C by determining the marginal distribution of the subset within Ck which is
also Multivariate Normal. With the subset being a singleton set, signiﬁcance
can be evaluated for a particular mean or mixing coeﬃcient with the marginal
distribution of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(10.5.17)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(10.5.18)
follows a Normal distribution with a mean of zero and variance of one.
10.5.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
matrix of coeﬃcients C, the matrix of sources S, the source covariance matrix
R, and the error covariance matrix Σ by the ICM algorithm. To maximize
the joint posterior distribution using the ICM algorithm, start with an initial
value for the matrix of sources S, say ˜S(0), and then cycle through
˜C(l+1) =
Arg Max
C
p(C| ˜S(l), ˜R(l), ˜Σ(l),X)
= [X′ ˜Z(l) +C0D−1](D−1 + ˜Z′
(l) ˜Z(l))−1,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜R(l), ˜S(l),X)
= [(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))
+( ˜C(l+1) −C0)D−1( ˜C(l+1) −C0)′ +Q]/(n+m+ν +1),
© 2003 by Chapman & Hall/CRC

˜R(l+1) =
Arg Max
R
p(R| ˜S(l), ˜C(l+1), ˜Σ(l+1),X)
= ( ˜S(l) −S0)′( ˜S(l) −S0)+V
n+η
,
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l+1), ˜Σ(l+1),X)
= [S0 ˜R−1
(l+1) +(X −en˜µ′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)]
×( ˜R−1
(l+1) + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
where the matrix ˜Z(l) = (en, ˜S(l)) until convergence is reached. The converged
values ( ˜S, ˜R, ˜C, ˜Σ) are joint posterior modal (maximum a posteriori) estima-
tors of the parameters. Conditional maximum a posteriori variance estimates
can also be found. The conditional modal variance of the matrix containing
the means and mixing coeﬃcients is
var(C| ˜C, ˜S, ˜R, ˜Σ,X) = ˜Σ⊗(D−1 ⊗˜Z′ ˜Z)−1
or equivalently
var(c|˜c, ˜S, ˜R, ˜Σ,X) = (D−1 ⊗˜Z′ ˜Z)−1 ⊗˜Σ
= ¯∆,
where c = vec(C), while ˜S, ˜R, and ˜Σ are the converged value from the ICM
algorithm.
To evaluate statistical signiﬁcance with the ICM approach, use the con-
ditional distribution of the matrix containing the mean vector and mixing
matrix which is
p(C| ˜C, ˜S, ˜R, ˜Σ,X,∝|D−1 + ˜Z′ ˜Z|
1
2 |˜Σ|−1
2 e−1
2 tr ˜Σ−1(C−˜
C)(D−1+ ˜
Z′ ˜
Z)(C−˜
C)′.
(10.5.19)
That is,
C| ˜C, ˜S, ˜R, ˜Σ,X,∼N

˜C, ˜Σ⊗(D−1 + ˜Z′ ˜Z)−1
.
(10.5.20)
General simultaneous hypotheses can be evaluated regarding the entire ma-
trix containing the mean vector and the mixing matrix, a submatrix, or the
mean vector or a particular source, or an element by computing marginal
conditional distributions.
It can be shown [17, 41] that the marginal conditional distribution of any
column of the matrix containing the means and mixing coeﬃcients C, Ck is
Multivariate Normal
p(Ck| ˜Ck, ˜S, ˜Σ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Ck−˜
Ck)′(Wkk ˜Σ)−1(Ck−˜
Ck),
(10.5.21)
© 2003 by Chapman & Hall/CRC

where W = (D−1 +U ′U)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of C, signiﬁcance can be evalu-
ated for the mean vector or a particular source. Signiﬁcance can be evaluated
for a subset of coeﬃcients by determining the marginal distribution of the
subset within Ck which is also Multivariate Normal. With the subset being a
singleton set, signiﬁcance can be determined for a particular mean or mixing
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜S, ˜Σjj,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(Ckj−˜Ckj)2
2Wkk ˜Σjj ,
(10.5.22)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)
,
Wkk ˜Σjj
(10.5.23)
follows a Normal distribution with a mean of zero and variance of one.
10.6 
Generalized Priors and Posterior
Generalized Conjugate prior distributions are assessed in order to quantify
available prior information regarding values of the model parameters.
The joint prior distribution for the sources S, the source covariance matrix
R, the error covariance matrix Σ, and the matrix of coeﬃcients c = vec(C) is
given by
p(S,R,Σ,c) = p(S|R)p(R)p(Σ)p(c),
(10.6.1)
where the prior distribution for the parameters from the generalized Conjugate
procedure outlined in Chapter 4 are as follows
p(S|R) ∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′,
(10.6.2)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(10.6.3)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(10.6.4)
p(c) ∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(10.6.5)
where Σ, R, V , Q, and ∆are positive deﬁnite matrices. The hyperparameters
S0, η, V , ν, Q, c0, and ∆are to be assessed, and having done so, completely
determine the joint prior distribution.
© 2003 by Chapman & Hall/CRC

The prior distribution for the matrix of sources S is Matrix Normally dis-
tributed, the prior distribution for the source vector covariance matrix R is
Inverted Wishart distributed, the vector c = vec(C), C = (µ,Λ) containing the
overall mean µ and the mixing matrix Λ is Multivariate Normal, and the prior
distribution for the error covariance matrix Σ is Inverted Wishart distributed.
Note that both Σ and R are full positive deﬁnite covariance matrices allow-
ing both the observed mixed signals (microphones) and also the unobserved
source components (speakers) to be correlated. The mean of the sources is
often taken to be constant for all observations and thus without loss of gener-
ality taken to be zero. An observation (time) varying source mean is adopted
here.
Upon using Bayes’ rule the joint posterior distribution for the unknown
parameters with generalized Conjugate prior distributions for the model pa-
rameters is given by
p(S,R,C,Σ|X) = p(S|R)p(R)p(Σ)p(c)p(X|C,S,Σ),
(10.6.6)
which is
p(S,R,C,Σ|X) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q]
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ]
×e−1
2 (c−c0)′∆−1(c−c0)
(10.6.7)
after inserting the joint prior distribution and the likelihood.
This joint posterior distribution must now be evaluated in order to obtain
parameter estimates of the sources S, the overall mean/mixing matrix C,
the covariance matrix for the sources R, and the observation error covariance
matrix Σ.
10.7
Generalized Estimation and Inference
With the generalized Conjugate prior distributions for the parameters, it is
not possible to obtain all or any of the marginal distributions or explicit maxi-
mum a posteriori estimates and thus marginal mean or maximum a posteriori
estimates in closed form. For these reasons, marginal mean and maximum a
posteriori estimates are found using the Gibbs sampling and ICM algorithms.
© 2003 by Chapman & Hall/CRC

10.7.1
Posterior Conditionals
Both Gibbs sampling and ICM require the posterior conditionals. Gibbs
sampling requires the conditionals for the generation of random variates while
ICM requires them for maximization by cycling through their modes.
The conditional posterior distribution of the matrix of sources S is found by
considering only those terms in the joint posterior distribution which involve
S and is given by
p(S|µ,R,Λ,Σ,X) ∝p(S|R)p(X|µ,S,Λ,Σ)
∝e−1
2 tr(S−S0)′R−1(S−S0)
×e−1
2 tr(X−enµ′−SΛ′)Σ−1(X−enµ′−SΛ′)′, (10.7.1)
which after performing some algebra in the exponent can be written as
p(S|µ,R,Λ,Σ,X) ∝e−1
2 tr(S−˜S)(R−1+Λ′Σ−1Λ)(S−˜S)′,
(10.7.2)
where the matrix ˜S has been deﬁned to be
˜S = [S0R−1 +(X −enµ′)Σ−1Λ](R−1 +Λ′Σ−1Λ)−1.
(10.7.3)
That is, the matrix of sources S given the source covariance matrix R, the
overall mean µ, the mixing matrix Λ, the error covariance matrix Σ, and the
data matrix X is Matrix Normally distributed.
The conditional posterior distribution of the source covariance matrix R is
found by considering only those terms in the joint posterior distribution which
involve R and is given by
p(R|µ,S,Λ,Σ,X) ∝p(R)p(S|R)
∝|R|−ν
2 e−1
2 trR−1V |R|−n
2 e−1
2 trR−1(S−S0)′(S−S0)
×|R|−(n+ν)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ].
(10.7.4)
That is, the posterior conditional distribution of the source covariance ma-
trix R given the overall mean µ, the matrix of sources S, the error covariance
matrix Σ, the mixing matrix Λ, and the data matrix X has an Inverted
Wishart distribution.
The conditional posterior distribution of the coeﬃcient vector c containing
the overall mean µ and mixing matrix Λ is found by considering only those
terms in the joint posterior distribution which involve c or C and is given by
p(c|S,R,Σ,X) ∝p(c)p(X|S,C,Σ)
∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′),
(10.7.5)
© 2003 by Chapman & Hall/CRC

which after performing some algebra in the exponent becomes
p(c|S,R,Σ,X) ∝e−1
2 (c−˜c)′(∆−1+Z′Z⊗Σ−1)(c−˜c),
(10.7.6)
where the vector ˜c has been deﬁned to be
˜c = (∆−1 +Z′Z ⊗Σ−1)−1[∆−1c0 +(Z′Z ⊗Σ−1)ˆc],
(10.7.7)
and the vector ˆc has been deﬁned to be
ˆc = vec[X′Z(Z′Z)−1].
(10.7.8)
Note that ˜c has been written as a weighted combination of the prior mean
c0 from the prior distribution and the data mean ˆc from the likelihood.
The conditional posterior distribution of the vector c containing the overall
mean µ and the mixing matrix Λ given the matrix of sources S, the source
covariance matrix R, the error covariance matrix Σ, and the data matrix X
is Multivariate Normally distributed.
The conditional posterior distribution of the error covariance matrix Σ is
found by considering only those terms in the joint posterior distribution which
involve Σ and is given by
p(Σ|S,R,C,X) ∝p(Σ)p(X|S,C,Σ)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q].
(10.7.9)
That is, the conditional distribution of the error covariance matrix Σ given
the matrix of sources S, the source covariance matrix R, the overall mean
µ, the mixing matrix Λ, and the data matrix X has an Inverted Wishart
distribution.
The modes of these posterior conditional distributions are as described in
Chapter 2 and given by ˜S, ˜c, (both as deﬁned above)
˜R = (S −S0)′(S −S0)+V
n+η
,
(10.7.10)
and
˜Σ = (X −ZC′)′(X −ZC′)+Q
n+ν
,
(10.7.11)
respectively.
© 2003 by Chapman & Hall/CRC

10.7.2
Gibbs Sampling
To ﬁnd marginal posterior mean estimates of the parameters from the joint
posterior distribution using the Gibbs sampling algorithm, start with initial
values for the matrix of sources S and the error covariance matrix Σ, say ¯S(0)
and ¯Σ(0), and then cycle through
¯c(l+1) = a random variate from p(c| ¯S(l), ¯Σ(l), ¯R(l+1),X)
= AcYc +Mc,
(10.7.12)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l+1), ¯C(l+1),X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(10.7.13)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1),X)
= AR(Y ′
RYR)−1A′
R,
(10.7.14)
¯S(l+1) = a random variate from p(S| ¯R(l+1), ¯C(l+1), ¯Σ(l+1),X)
= YSB′
S +MS,
(10.7.15)
where
ˆc(l) = vec[X′ ¯Z(l)( ¯Z′
(l) ¯Z(l))−1],
¯c(l+1) = [∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc(l)],
AcA′
c = (∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )−1,
Mc = [∆−1 +
¯
¯Z′
(l) ¯Z(l) ⊗¯Σ
−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc],
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))+Q,
ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
BSB′
S = ( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MS = [S0R−1
(l+1)(X −en¯µ′
(l+1))¯Σ−1
(l+1)¯Λ(l+1)]
×( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while Yc, YΣ, YR, and YS are p(m+1)×1, (n+ν +p+1)×p, (n+η+m+1)×
m, and n × m dimensional matrices whose respective elements are random
variates from the standard Scalar Normal distribution. The formulas for the
generation of random variates from the conditional posterior distributions is
easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯c = 1
L
L

l=1
¯c(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
© 2003 by Chapman & Hall/CRC

which are the exact sampling-based marginal posterior mean estimates of
the parameters. Exact sampling-based estimates of other quantities can also
be found. Similar to Regression and Factor Analysis, there is interest in the
estimate of the marginal posterior variance of the matrix containing the means
and mixing coeﬃcients
var(c|X) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆.
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the vector
containing the means and mixing coeﬃcients, their distribution is
p(c|X) ∝| ¯∆|−1
2 e−1
2 (c−¯c)′ ¯∆−1(c−¯c),
(10.7.16)
where ¯c and ¯∆are as previously deﬁned.
To evaluate statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the vector c containing the means and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the entire coeﬃcient vector of means and mixing coeﬃcients, a
subset of it, or the mean vector or the coeﬃcients for a particular source
by computing marginal distributions.
It can be shown that the marginal
distribution of the kth column of the matrix containing the means and mixing
coeﬃcients C, Ck is Multivariate Normal
p(Ck| ¯Ck,X) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(10.7.17)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be determined for a subset of means or coeﬃcients of the
kth column of C by determining the marginal distribution of the subset within
Ck which is also Multivariate Normal. With the subset being a singleton set,
signiﬁcance can be determined for a particular mean or coeﬃcient with the
marginal distribution of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(10.7.18)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(10.7.19)
follows a Normal distribution with a mean of zero and variance of one.
© 2003 by Chapman & Hall/CRC

10.7.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
vector of coeﬃcients c, the matrix of sources S, the source covariance matrix
R, and the error covariance matrix Σ using the ICM algorithm. To maximize
the joint posterior distribution using the ICM algorithm, start with initial
values for the matrix of sources ˜S and the error covariance matrix Σ, say ˜S(0)
and ˜Σ(0), and then cycle through
ˆc(l) = vec[X′ ˜Z(l)( ˜Z′
(l) ˜Z(l))−1],
˜c(l+1) =
Arg Max
c
p(c| ˜S(l), ˜R(l+1), ˜Σ(l),X)
= [∆−1 + ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) ]−1[∆−1c0 +( ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) )ˆc(l)],
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜R(l), ˜S(l),X)
=
(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))+Q
n+ν
,
˜R(l+1) =
Arg Max
R
p(R| ˜S(l), ˜C(l+1), ˜Σ(l+1),X)
= ( ˜S(l) −S0)′( ˜S(l) −S0)+V
n+η
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l+1), ˜Σ(l+1),X)
= [S0 ˜R−1
(l+1) +(X −en¯µ′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)]
×( ˜R−1
(l+1) + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
where the matrix ˜Z(l) = (en, ˜S(l)) until convergence is reached with the joint
modal (maximum a posteriori) estimator for the unknown model parameters
( ˜R, ˜S,˜c, ˜Σ). Conditional maximum a posteriori variance estimates can also be
found. The conditional modal variance of the matrix containing the means
and mixing coeﬃcients is
var(c| ˜S, ˜R, ˜Σ,X,U) = [∆−1 + ˜Z′ ˜Z ⊗˜Σ−1]−1
= ˜∆,
where c = vec(C), while ˜S, ˜R, and ˜Σ are the converged values from the ICM
algorithm.
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
The posterior conditional distribution of the matrix containing the means and
mixing coeﬃcients C given the modal values of the other parameters and the
data is
© 2003 by Chapman & Hall/CRC

p(c| ˜S, ˜Σ,X,U) ∝| ˜∆|−1
2 e−1
2 (c−˜c)′ ˜∆−1(c−˜c).
(10.7.20)
To evaluate statistical signiﬁcance with the ICM approach, use the mar-
ginal conditional distribution of the matrix containing the means and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the mean vector or the coeﬃcient for a particular source by com-
puting marginal distributions. It can be shown that the marginal conditional
distribution of the kth column Ck of the matrix C containing the overall mean
vector and mixing matrix is Multivariate Normal
p(Ck| ˜Ck, ˜Σ,X,U) ∝| ˜∆k|−1
2 e−1
2 (Ck−˜
Ck)′ ˜∆−1
k
(Ck−˜
Ck),
(10.7.21)
where ˜∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ˜∆.
Signiﬁcance can be determined for a subset of means or mixing coeﬃcients
of the kth column of C by determining the marginal distribution of the sub-
set within Ck which is also Multivariate Normal. With the subset being a
singleton set, signiﬁcance can be determined for a particular mean or mixing
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜S, ˜Σjj,X) ∝( ˜∆kj)−1
2 e
−
(Ckj−˜Ckj)2
2 ˜
∆kj
,
(10.7.22)
where ˜∆kj is the jth diagonal element of ˜∆k. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)

˜∆kj
follows a Normal distribution with a mean of zero and variance of one.
10.8
Interpretation
Although the main focus after having performed a Bayesian Source Sepa-
ration is the separated sources, there are others. The mixing coeﬃcients are
the amplitudes which determine the relative contribution of the sources. A
“small” coeﬃcient indicates that the particular source does not signiﬁcantly
contribute to the observed mixed signal. A “large” coeﬃcient indicates that
the particular source signiﬁcantly contributes to the observed mixed signal.
Whether a mixing coeﬃcient is large or small depends on its associated sta-
tistic.
© 2003 by Chapman & Hall/CRC

TABLE 10.1
Variables for Bayesian Source Separation example.
X
Variables
X1
Species 1
X2
Species 2
X3
Species 3
X4
Species 4
X5
Species 5
X6
Species 6
X7
Species 7
X8
Species 8
X9
Species 9
Consider the following data which consist of a core sample taken from the
ocean ﬂoor [24]. In the core sample, plankton content is measured for p = 9
species listed as species 1-9 in Table 10.1 at one hundred and ten depths. (The
seventh species has been dropped from the original data because nearly all
values were zero.)
FIGURE 10.1
Mixed and unmixed signals.
0
5
0
5
30
40
50
0
2
4
10
20
30
0
5
10
0
5
10
0
5
10
20
30
40
50
0
1
−10
0
10
−10
0
10
−5
0
5
−4
0
4
10
20
30
40
50
−2
0
2
The plankton content is used to infer approximate climatological conditions
which existed on Earth. The many species which coexist at diﬀerent times
© 2003 by Chapman & Hall/CRC

(core depths) consist of contributions from several diﬀerent sources.
The
Source Separation model will tell us what the sources are, and how they
contribute to each of the plankton types.
TABLE 10.2
Covariance hyperparameter for coeﬃcients.
D
1
2
3
4
5
6
1
0.0182
-0.0000
0.0000
-0.0000
0.0000
0.0000
2
0.0004
-0.0000
0.0000
-0.0000
-0.0000
3
0.0007
-0.0000
-0.0000
-0.0000
4
0.0018
-0.0000
0.0000
5
0.0054
-0.0000
6
0.0089
The n0 = 55 odd observations which are in Table 10.9 were used as prior
data to assess the model hyperparameters for the analysis of the n = 55 even
observations in Table 10.8.
The plankton species in the core samples are
believed to be made up from m = 5 diﬀerent sources. The ﬁrst ﬁve normalized
scores from a principal component analysis were used for the prior mean of
the sources. The remaining hyperparameters were assessed from the results
of a Regression of the prior data on the prior source mean.
TABLE 10.3
Prior mode, Gibbs, and ICM source covariance.
V/η
1
2
3
4
5
1
42.1132
0.0000
-0.0000
0.0000
0.0000
2
25.0674
0.0000
0.0000
0.0000
3
10.2049
0.0000
-0.0000
4
3.3642
0.0000
5
2.0436
¯R
1
2
3
4
5
1
36.8064
-1.0408
0.1357
-0.0161
-0.1414
2
21.1973
-0.5267
-0.1098
-0.0124
3
8.4600
-0.0989
0.0238
4
3.0521
0.0015
5
1.9396
˜R
1
2
3
4
5
1
25.5538
-0.0001
0.1350
-0.0783
0.2615
2
15.9102
-0.0426
0.0012
-0.0506
3
6.3911
0.0188
-0.0199
4
2.1210
0.0312
5
1.1658
© 2003 by Chapman & Hall/CRC

The assessed prior hyperparameter C0 for the mean vector and mixing
matrix was assessed from C0 = XZ0(Z′
0Z0)−1 where Z0 = (en,S0). The as-
sessed prior covariance matrix D for the coeﬃcient matrix was assessed from
D = (Z′
0Z0)−1 and presented in Table 10.2. The assessed values for η was
η = n0 = 55. The scale matrix Q for the error covariance matrix was assessed
from Q = (X −UC′
0)′(X −UC′
0) in Table 10.5 and ν = n0 = 55.
The observed mixed signals in Tables 10.8 and 10.9 along with the estimated
sources in Tables 10.11 and 10.12 are displayed in Figure 10.1.
Refer to Table 10.3 which contains the (top) prior mode, (middle) Gibbs,
and (bottom) ICM source covariance matrices. We can see the variances of
the sources are far from unity as is speciﬁed in the Factor Analysis model.
Table 10.4 has the matrix containing the prior mean vector and mixing ma-
trix along with the Gibbs sampling and ICM estimated ones using Conjugate
prior distributions. From the estimated mixing coeﬃcient values, it is diﬃcult
to discern which are “large” and which are “small.”
This motivates the need for relative mixing coeﬃcient statistics which are
a measure of relative size. In order to compute the statistics for the mixing
coeﬃcients, the error covariance matrices are needed.
The (top) prior mode, (middle) Gibbs, and (bottom) ICM error covariance
matrices are displayed in Table 10.5. The covariance values have been rounded
to two decimal places for presentation purposes. If these covariance matrices
were converted to correlation matrices, then it can be seen that several of the
values are above one half.
The statistics for the mean vector and mixing coeﬃcients are displayed in
Table 10.6. The rows of the mean/coeﬃcient matrix have been rearranged
for increased interpretability. It is seen that Species 3 primarily is made up
of Source 1, Species 7 contains a positive mixture of Source 3 and negative
mixtures of Sources 1 and 2, Species 5 consists primarily of Source 3, Species 2
consists of negative mixtures of Sources 3 and 4, Species 6 consists of Source
4 and possibly 5, Species 4 consists of a negative mixture of Source 5 and
possibly a negative one of 4, and Species 8 consists of Source 5. Table 10.7
lists the species which correspond to the sources.
© 2003 by Chapman & Hall/CRC

TABLE 10.4
Prior, Gibbs, and ICM mixing coeﬃcients.
C0
0
1
2
3
4
5
1
1.6991
-0.0248
0.0361
-0.0718
0.1107
0.0962
2
1.5746
-0.0455
0.0609
-0.2164
-0.4228
-0.0158
3
38.9093
0.9288
-0.0066
0.3409
0.0027
0.0184
4
1.6265
0.0116
-0.0689
0.0112
0.2694
-0.6050
5
13.1707
-0.1431
0.8980
0.3874
0.0870
0.0557
6
2.2433
-0.0111
-0.0807
-0.1280
0.8041
0.4115
7
3.3637
-0.3376
-0.4087
0.8122
0.0039
-0.0174
8
1.4693
-0.0025
-0.1012
0.0732
-0.2828
0.6719
9
0.2135
-0.0017
0.0030
-0.0170
0.0467
-0.0002
¯C
0
1
2
3
4
5
1
1.6804
-0.0241
0.0105
-0.0530
0.1297
-0.0360
2
1.6143
-0.0363
0.0500
-0.2098
-0.3291
-0.0128
3
38.6167
0.6343
-0.0391
0.1755
-0.0121
0.4518
4
1.6694
-0.0040
-0.0163
-0.0064
0.1581
-0.3663
5
13.2820
-0.0676
0.7008
0.3610
0.0802
-0.0502
6
2.4086
-0.0004
-0.0447
-0.1226
0.5575
0.3451
7
3.1186
-0.2359
-0.2633
0.5060
0.0289
-0.1600
8
1.4778
-0.0119
-0.0576
0.0405
-0.1631
0.2755
9
0.2297
0.0033
-0.0037
0.0089
-0.0089
-0.0222
˜C
0
1
2
3
4
5
1
1.6746
-0.0308
0.0152
-0.0695
0.1736
-0.0490
2
1.6040
-0.0469
0.0713
-0.2691
-0.4442
-0.0171
3
38.6004
0.8555
-0.0619
0.1944
0.0150
0.5738
4
1.6701
-0.0045
-0.0231
-0.0081
0.2130
-0.4976
5
13.2669
-0.1088
0.8972
0.4302
0.0958
-0.0596
6
2.3959
0.0015
-0.0580
-0.1567
0.7543
0.4597
7
3.1709
-0.3219
-0.3504
0.6878
0.0248
-0.1853
8
1.4857
-0.0153
-0.0751
0.0578
-0.2199
0.3809
9
0.2306
0.0042
-0.0049
0.0109
-0.0104
-0.0303
© 2003 by Chapman & Hall/CRC

TABLE 10.5
Prior, Gibbs, and ICM covariances.
Q
ν
1
2
3
4
5
6
7
8
9
1
1.53
-0.01
0.52
-0.34
0.21
0.05
-0.68
-0.03
0.12
2
2.03
-1.27
-0.45
1.23
-0.43
-0.74
-0.22
-0.02
3
54.94
0.46
-12.81
-0.60
-8.33
-2.27
-0.96
4
2.12
-1.67
0.31
0.60
-0.89
0.01
5
18.71
-2.00
-3.36
-0.84
-0.07
6
4.80
0.74
-0.32
0.25
7
12.95
1.81
-0.03
8
3.23
-0.09
9
0.27
¯Ψ
1
2
3
4
5
6
7
8
9
1
1.43
-0.01
0.80
-0.28
0.08
-0.00
-0.61
-0.06
0.11
2
1.76
-0.86
-0.37
0.97
-0.26
-0.55
-0.24
-0.01
3
47.42
0.20
-10.86
-0.43
-6.97
-2.15
-0.85
4
1.78
-1.35
0.24
0.53
-0.66
0.02
5
15.53
-1.71
-2.75
-0.65
-0.09
6
4.15
0.74
-0.30
0.22
7
10.63
1.44
0.02
8
2.76
-0.07
9
0.24
˜Ψ
1
2
3
4
5
6
7
8
9
1
1.33
-0.01
1.05
-0.30
-0.02
-0.11
-0.52
-0.01
0.11
2
1.38
-0.31
-0.28
0.68
-0.04
-0.16
-0.24
-0.00
3
38.67
0.34
-9.08
-0.49
-4.76
-2.17
-0.84
4
1.56
-1.18
0.19
0.41
-0.51
0.01
5
10.97
-1.28
-1.99
-0.34
-0.07
6
3.38
0.83
-0.25
0.22
7
7.72
1.17
0.01
8
2.50
-0.07
9
0.23
© 2003 by Chapman & Hall/CRC

TABLE 10.6
Statistics for means and loadings.
zGibbs
0
1
2
3
4
5
3
54.8773
5.6098
-0.2840
0.7928
-0.0318
0.9182
7
9.1505
-4.4542
-3.8932
4.6551
0.1601
-0.6934
5
31.8876
-1.0324
8.2695
2.7367
0.3548
-0.1722
2
11.7241
-1.6765
1.8071
-4.8271
-4.3483
-0.1311
6
11.6744
-0.0118
-1.0876
-1.8774
4.8424
2.3436
4
12.6162
-0.1925
-0.6144
-0.1538
2.1872
-3.8858
8
9.0991
-0.4708
-1.7873
0.7872
-1.8362
2.3665
1
14.3622
-1.2867
0.4391
-1.4076
1.9977
-0.4199
9
4.8283
0.4346
-0.3866
0.5967
-0.3387
-0.6567
zICM
0
1
2
3
4
5
3
65.0052
8.6843
-0.4979
0.9815
0.0443
1.3159
7
11.9498
-7.3128
-6.3027
7.7711
0.1638
-0.9512
5
41.9351
-2.0735
13.5378
4.0770
0.5316
-0.2563
2
14.2853
-2.5192
3.0324
-7.1856
-6.9418
-0.2071
6
13.6452
0.0506
-1.5773
-2.6756
7.5374
3.5650
4
13.9991
-0.2264
-0.9248
-0.2048
3.1326
-5.6794
8
9.8461
-0.6123
-2.3762
1.1479
-2.5572
3.4372
1
15.1846
-1.6856
0.6574
-1.8882
2.7613
-0.6056
9
5.0139
0.5510
-0.5104
0.7078
-0.3982
-0.8961
TABLE 10.7
Sources in terms of strong mixing coeﬃcients.
Source 1:
Species 3, 7
Source 2:
Species 5, 7
Source 3:
Species 2, 7, 5
Source 4:
Species 2, 6, 4
Source 5:
Species 6, 8
© 2003 by Chapman & Hall/CRC

TABLE 10.8
Plankton data.
X
1
2
3
4
5
6
7
8
9
1
3.203
0.712
37.722
0.356
30.961
0.712
0.356
0.000
0.000
2
1.124
0.562
47.191
1.124
12.360
2.247
3.933
0.562
0.562
3
1.149
0.766
52.874
0.766
12.261
0.000
0.383
2.299
0.000
4
2.222
2.222
45.926
2.222
13.333
2.963
1.481
1.481
1.481
5
0.621
0.621
36.025
2.484
10.519
0.621
1.242
1.863
0.000
6
0.000
0.000
38.298
0.709
11.348
2.837
1.418
5.674
0.000
7
1.379
1.034
42.069
0.690
8.621
2.069
2.759
1.724
0.690
8
3.429
1.143
45.714
1.143
14.286
1.714
0.571
3.429
0.571
9
1.198
1.796
50.299
1.198
8.383
2.994
0.599
0.599
0.599
10
5.143
2.857
38.286
0.000
13.714
1.143
1.143
1.143
0.000
11
1.961
2.614
41.830
3.268
11.765
1.307
1.307
0.654
0.000
12
1.422
2.844
38.389
1.422
16.114
0.948
0.000
0.474
0.000
13
1.571
1.571
37.696
1.571
10.995
4.188
2.094
2.618
1.047
14
0.926
3.241
28.241
0.463
12.037
0.926
0.463
1.852
0.463
15
1.036
6.218
34.197
1.036
14.508
0.518
0.000
1.554
0.518
16
1.485
7.426
29.208
2.475
15.842
1.485
2.970
1.485
0.000
17
3.404
0.426
32.766
4.255
13.191
2.128
3.830
0.851
1.700
18
1.449
3.623
36.957
0.000
15.942
3.623
0.725
1.449
0.720
19
0.772
0.386
40.927
0.772
15.444
2.703
0.000
0.772
0.380
20
3.627
0.518
41.451
1.554
16.580
0.518
2.591
1.554
0.000
21
3.509
2.456
42.105
2.105
12.281
1.053
2.456
0.000
0.000
22
1.449
0.483
43.961
3.865
12.560
1.449
2.899
0.000
0.000
23
0.000
0.741
33.333
2.222
22.222
2.222
0.741
0.000
0.000
24
1.026
0.513
42.051
2.051
16.410
2.051
0.513
2.051
0.000
25
1.523
0.000
34.518
2.030
20.305
2.030
1.523
1.015
0.000
26
0.000
2.703
28.649
1.622
24.324
3.784
2.162
3.243
0.000
27
0.800
2.400
50.400
1.600
11.200
2.400
4.800
0.000
0.000
28
0.000
0.543
32.609
1.087
11.413
4.891
3.804
2.717
0.000
29
1.762
0.000
33.921
0.000
16.740
2.643
9.251
2.643
0.000
30
1.136
2.841
49.432
2.273
11.932
2.273
0.568
0.000
0.000
31
3.636
1.212
35.758
2.424
6.061
6.061
3.030
0.000
0.000
32
1.342
2.685
34.228
3.356
12.081
2.685
2.685
4.027
0.000
33
2.158
2.158
34.532
2.158
15.826
5.036
0.719
2.158
0.000
34
1.235
0.000
41.975
0.000
12.346
1.852
0.617
2.469
0.000
35
3.550
2.367
47.337
2.367
5.917 10.059
0.000
0.592
0.000
36
5.455
0.606
43.636
1.818
10.303
7.273
0.605
0.000
0.000
37
2.609
1.304
33.043
1.739
9.130
3.913
3.478
0.435
0.000
38
1.899
0.000
34.177
2.532
12.025
4.430
2.532
1.266
0.000
39
0.595
2.976
50.000
0.000
7.738
6.548
2.381
0.595
0.000
40
0.372
5.576
37.918
0.372
15.613
0.743
0.000
0.372
0.000
41
2.362
2.362
36.220
3.150
14.173
1.969
0.787
1.575
0.000
42
2.381
3.175
32.143
1.190
17.460
1.587
0.397
1.190
0.000
43
0.858
3.863
31.760
1.717
21.888
7.296
4.721
0.858
0.000
44
0.658
1.316
52.632
0.000
3.289
1.974
3.947
0.658
0.000
45
1.689
0.676
26.689
2.027
8.108
4.392
13.176
2.027
1.689
© 2003 by Chapman & Hall/CRC

X
1
2
3
4
5
6
7
8
9
46
1.064
0.000
40.957
1.596
6.915 2.660
3.723
2.660
0.000
47
0.000
0.000
35.533
1.015
13.706 7.614
3.553
0.000
0.000
48
1.471
2.206
34.559
2.941
15.441 1.471
0.000
0.735
0.000
49
0.000
0.498
44.776
2.488
19.900 0.995
1.990
0.995
0.498
50
2.717
0.000
32.065
3.261
15.761 1.087
6.522
1.087
0.000
51
1.342
2.013
24.161
3.356
11.409 1.342
9.396
0.000
0.671
52
1.548
0.310
31.269
1.548
9.288 0.000
9.288
4.644
0.000
53
2.183
1.747
33.188
0.437
13.974 0.437
4.367
1.747
1.747
54
2.286
2.286
37.143
1.714
8.000 1.714
8.000
4.571
0.000
55
0.658
0.658
34.868
4.605
15.789 1.316
3.947
1.974
0.000
© 2003 by Chapman & Hall/CRC
TABLE 10.8
Plankton data.  (continued)

TABLE 10.9
Plankton prior data.
X0
1
2
3
4
5
6
7
8
9
1
1.792
0.489
43.485
0.814
25.570 0.651
0.163
0.000
0.163
2
2.364
1.709
47.009
0.855
20.513 1.709
1.282
0.427
0.000
3
0.671
1.007
43.624
3.020
15.436 1.007
0.336
0.671
0.336
4
1.990
0.498
53.234
3.980
6.965 0.000
0.498
0.995
0.000
5
1.786
1.190
49.405
1.786
10.714 1.786
0.595
0.595
0.000
6
1.418
0.000
46.099
2.837
9.220 4.255
0.709
2.836
0.000
7
0.498
0.498
48.756
0.000
5.970 1.990
0.498
2.985
0.000
8
0.662
0.000
46.358
0.000
11.921 0.000
1.987
3.311
0.000
9
2.899
2.899
42.995
0.000
14.010 1.449
2.415
2.415
0.483
10
1.887
2.516
38.994
3.145
7.547 2.516
1.258
1.258
0.000
11
3.067
0.613
37.423
1.227
13.497 2.761
1.227
0.000
0.307
12
1.515
2.020
37.374
1.010
12.626 2.020
0.000
0.505
0.000
13
1.630
1.630
36.957
2.174
10.870 2.174
0.000
0.000
0.000
14
1.826
3.196
36.073
0.913
12.329 2.283
0.457
0.913
0.457
15
1.379
2.414
35.517
0.345
11.679 0.345
0.000
4.828
0.000
16
0.649
3.896
39.610
3.896
13.636 1.299
0.543
0.649
0.000
17
1.087
0.000
42.391
1.630
15.761 1.630
2.174
1.087
0.000
18
1.429
0.476
42.381
2.857
10.952 1.905
0.476
0.952
1.900
19
1.685
1.685
48.315
2.809
10.674 1.124
1.124
1.124
0.000
10
1.266
1.266
37.975
2.532
18.143 3.376
2.110
0.422
0.000
21
1.869
1.402
37.850
2.804
12.617 2.336
9.813
0.467
0.930
22
0.904
0.904
44.578
1.205
14.759 0.602
1.506
0.602
0.000
23
1.299
0.649
38.961
0.325
17.208 1.945
4.545
1.948
0.000
24
2.513
4.523
35.176
1.005
20.603 0.000
0.000
0.000
0.000
25
0.565
0.565
44.068
3.955
10.169 1.695
9.605
3.390
0.000
26
0.508
0.000
40.609
0.508
21.827 0.508
3.046
0.000
0.000
27
0.629
4.403
39.623
0.629
10.063 3.145
5.660
5.031
0.000
28
1.630
0.543
54.348
2.174
7.609 3.804
1.630
2.717
0.000
29
1.622
1.081
32.973
2.162
11.892 3.784
9.780
0.541
0.000
30
1.418
0.000
36.879
0.709
11.348 4.255
4.965
4.965
0.709
31
0.893
3.561
33.036
5.357
13.393 2.679
4.464
0.893
0.893
32
3.448
1.478
29.064
3.448
14.778 4.433
2.955
0.000
0.000
33
4.435
2.419
33.468
0.806
17.742 3.226
0.000
4.032
0.000
34
0.000
4.545
38.636
0.000
15.152 1.515
2.273
2.273
0.758
35
1.508
1.508
38.191
0.503
3.518 1.508
1.508
2.010
0.503
36
5.344
0.000
39.695
1.527
13.740 6.870
0.763
0.000
0.000
37
0.000
0.000
38.095
3.571
4.762 9.524
3.571
0.000
1.190
38
1.604
1.604
33.690
0.000
19.251 2.139
3.209
3.209
0.535
39
2.041
0.816
36.327
2.041
20.000 2.449
2.449
1.224
0.408
40
0.000
6.130
35.249
0.000
10.728 0.000
0.383
0.383
0.000
41
3.582
5.373
38.209
0.896
17.015 0.896
0.000
0.896
0.299
42
2.105
4.211
26.842
1.053
13.684 4.737
5.263
2.105
0.000
43
0.455
0.909
37.273
0.455
24.091 3.182
0.455
0.455
0.909
44
2.769
1.231
43.385
1.231
2.769 4.000
6.462
3.077
0.000
45
3.448
0.575
35.632
1.149
14.368 0.000
4.598
0.575
0.000
© 2003 by Chapman & Hall/CRC

X0
1
2
3
4
5
6
7
8
9
46
1.533
0.000
35.249
0.383
9.195
2.682 13.793
1.533
0.000
47
1.394
0.348
36.585
1.045
8.014
3.833
6.969
1.394
0.000
48
1.970
2.463
39.901
0.493
15.764
3.941
0.985
0.493
0.493
49
1.613
0.403
42.742
1.210
16.129
2.823
2.823
0.403
0.000
50
0.448
0.448
40.359
4.484
12.556
2.242
6.278
0.897
0.000
51
1.887
1.887
34.906
1.415
12.264
1.415
3.302
1.415
0.472
52
1.633
0.816
24.898
2.449
6.531
0.408 12.245
2.041
0.000
53
1.093
0.546
31.694
1.639
14.208
0.000 19.672
4.372
0.000
54
1.878
0.469
24.883
1.878
14.085
1.408
9.390
0.939
0.000
55
3.911
2.793
32.961
1.117
14.525
1.117
2.793
0.559
0.000
© 2003 by Chapman & Hall/CRC
TABLE 10.9
Plankton prior data.  (continued)

TABLE 10.10
Prior sources.
S0
1
2
3
4
5
1
3.6149
12.6825
4.0799
0.4612
-0.2944
2
7.1521
7.6238
3.8249
0.3161
0.2023
3
5.1598
3.2538
0.3356
-0.0632
-1.7127
4
15.2554
-4.4849
0.6459
-1.0708
-2.6534
5
11.0592
-1.0505
0.4543
-0.1117
-0.7537
6
8.2063
-3.0009
-1.0149
1.8473
0.9696
7
11.2025
-5.4905
-1.3124
-1.3693
1.6291
8
7.6609
-0.6361
1.7594
-2.3160
1.3145
9
3.8920
1.3196
0.7227
-1.6788
1.5253
10
1.5624
-4.2317
-4.1069
-0.1962
-1.2202
11
-0.7028
1.3016
-2.1855
1.3025
-0.3576
12
-0.2300
1.0737
-3.5947
-0.3564
-0.3915
13
-0.3380
-0.5615
-4.3846
0.2477
-1.4599
14
-1.6171
0.6572
-4.0716
-0.7554
0.0124
15
-1.8337
-0.0017
-4.1467
-3.3739
2.1201
16
1.4962
1.6719
-2.2090
-0.9924
-2.3625
17
3.3596
2.7586
1.6629
0.4339
-0.3165
18
4.5899
-0.9119
-1.7738
0.5243
-1.2499
19
9.8718
-1.3394
0.5319
-0.7428
-1.3385
20
-1.1304
4.9008
0.4796
1.9486
-0.5406
21
-3.0642
-3.1156
4.6031
0.7750
-1.4913
22
5.7309
2.3271
1.3865
-0.8565
-0.8441
23
-0.8891
3.1361
2.8324
-0.0328
1.1677
24
-3.5279
8.6545
-1.6456
-2.0993
-1.0984
25
3.2165
-5.6941
6.2049
-0.2878
-0.6181
26
0.5586
8.1180
4.2066
-0.0011
-0.5954
27
0.2002
-3.9653
0.5069
-2.1345
3.0200
28
15.7550
-4.7438
1.8297
1.0191
1.1627
29
-7.4803
-3.8323
2.5490
1.7341
-0.6052
30
-2.1294
-2.8353
0.2431
0.8822
3.5626
31
-5.8898
-0.3510
-1.4622
0.6291
-2.6885
32
-9.2742
1.5786
-3.5336
3.0033
-1.1024
33
-4.7051
5.3833
-3.1385
0.1500
3.0859
34
-0.2758
2.4375
-0.6078
-2.5024
1.1381
35
1.3419
-7.8329
-5.3471
-1.8751
0.2044
36
1.4589
1.3861
-2.2417
5.2097
1.4438
37
0.4345
-8.3575
-3.9381
6.5848
0.2063
38
-5.6870
5.5015
0.5682
-0.5077
2.3459
39
-3.0381
6.4700
1.1192
1.2976
0.0628
40
-2.1995
-0.3325
-5.2858
-4.2932
-1.0557
41
-0.2766
5.3482
-2.3177
-2.1927
-0.1166
42
-12.0879
-0.2870
-3.2471
0.6110
1.5717
43
-2.0622
11.0301
1.3244
1.8362
0.9329
44
4.5609
-10.8968
-0.0979
0.2247
1.5995
45
-3.6077
0.8983
0.6598
-0.9732
-1.0658
© 2003 by Chapman & Hall/CRC

S0
1
2
3
4
5
46
-6.2946
-7.8674
5.9732
0.3227
0.5144
47
-2.5983
-6.2559
0.2136
1.1774
0.5527
48
1.2758
3.3990
-1.1071
1.2221
0.9443
49
3.3661
2.8668
2.1198
1.3968
0.0288
50
0.5678
-2.0059
2.9503
1.2206
-2.2747
51
-3.5799
-0.6490
-1.7519
-0.8967
-0.3592
52
-14.9961
-9.5153
0.3231
-1.6923
-1.6445
53
-12.2752
-5.8850
11.9566
-2.1282
0.6192
54
-15.1326
-1.5071
0.7682
0.0891
-1.1288
55
-5.6267
1.8602
-2.3135
-0.9663
-0.5969
© 2003 by Chapman & Hall/CRC
TABLE 10.10
Prior sources.  (continued)

TABLE 10.11
Gibbs estimated sources.
¯S
1
2
3
4
5
1
0.9807
15.6149
4.9057
1.1081
-0.1951
2
7.6055
2.8646
3.9698
-0.0716
0.2639
3
9.6302
1.6182
1.4932
-0.8031
-1.0248
4
15.2314
-1.0222
1.2876
-1.1253
-2.3789
5
8.0468
-3.0071
-0.7450
-0.2822
-1.0089
6
8.6775
-3.0436
-0.6280
1.0225
1.6290
7
9.4814
-6.0544
-0.2174
-1.3684
1.3358
8
7.7334
1.2350
1.6243
-1.5465
1.1107
9
7.2569
-1.0552
0.2612
-1.5892
1.3960
10
-1.9127
-1.9787
-3.9417
0.0928
-0.9757
11
0.0741
1.2153
-2.6727
0.6031
-0.9816
12
0.1553
2.7110
-2.9621
-0.7201
-0.4769
13
2.4942
-0.6772
-2.8096
0.1257
-0.8011
14
-2.8384
-0.9523
-4.7943
-1.6783
0.1043
15
-2.6866
2.4285
-4.9798
-4.3689
1.4187
16
-4.2748
3.8976
-5.1482
-2.3088
-2.3416
17
1.9142
1.6006
1.5756
0.6781
-1.5672
18
3.9248
1.3067
-2.0418
-0.2850
-0.3889
19
9.8123
-0.1879
0.9218
-0.1547
-0.6778
20
-1.1903
4.1789
1.1927
1.6698
-0.7015
21
-3.1564
-2.1541
1.5573
0.4897
-1.6518
22
5.2444
1.2583
1.3740
-0.0676
-1.4785
23
-0.6582
6.0503
2.4677
0.4769
0.7760
24
0.6486
7.1883
-0.3399
-1.0240
-0.6335
25
1.9693
-0.2651
4.5535
0.7289
-0.5858
26
-1.4475
10.4732
2.0957
0.0082
0.1105
27
2.0043
-2.3173
1.8399
-1.9020
2.3047
28
9.2866
-5.1963
0.3418
1.2520
1.4795
29
-7.7186
-2.3700
4.6203
1.5911
0.2233
30
1.4263
-0.4446
-0.3194
0.0516
2.5013
31
-6.4891
-4.1211
-4.0640
2.2281
-1.9756
32
-6.3127
1.8598
-3.9654
1.6800
-0.9687
33
-4.9825
5.7280
-3.6544
0.9995
2.6256
34
2.1692
0.2236
0.0881
-1.5464
1.5986
35
2.3613
-5.0694
-6.6207
1.2189
0.9052
36
-0.5200
0.0357
-3.7793
5.9551
1.2934
37
-2.4959
-7.6663
-4.1741
5.1509
-0.0393
38
-5.5737
2.0416
-0.7126
0.9923
1.8707
39
0.9322
1.2373
-0.9587
0.7867
1.3443
40
-1.5557
2.3431
-4.8692
-4.4719
-0.6164
41
-1.5259
4.6410
-3.1214
-1.1761
-0.5867
42
-10.5555
2.8372
-3.4738
0.1699
1.1763
43
-5.6385
11.3828
-0.1632
1.9829
1.3539
44
6.9574
-11.8097
0.5968
-0.6437
1.6987
45
-6.4213
-4.3600
2.7961
-0.6655
-0.9720
© 2003 by Chapman & Hall/CRC

¯S
1
2
3
4
5
46
-2.3506
-8.5414
3.2495
0.3405
0.7791
47
-1.4567
-3.9941
0.3060
2.5545
1.4352
48
-0.2274
3.5452
-2.3818
0.7657
0.1226
49
6.9381
5.5400
4.1900
0.5332
-0.2802
50
-3.0303
-1.1452
2.8620
1.6998
-2.5665
51
-8.5665
-2.8726
-0.5945
-0.8719
-1.3653
52
-12.1401
-8.6555
1.9367
-1.5648
-1.1537
53
-7.6439
-4.2261
8.3341
-2.8372
0.4667
54
-11.8889
-3.4816
0.3105
-0.6313
-0.6106
55
-3.6070
3.3189
-0.4605
-0.2526
-1.2679
© 2003 by Chapman & Hall/CRC
TABLE 10.11
Gibbs estimated sources.  (continued)

TABLE 10.12
ICM estimated sources.
˜S
1
2
3
4
5
1
0.4899
14.5145
4.5071
1.0927
-0.2315
2
6.9053
1.8323
3.5958
-0.1215
0.2405
3
9.7792
1.2122
1.4778
-0.8028
-1.0118
4
14.4503
-0.5136
1.2786
-0.9314
-2.4179
5
7.0907
-3.0951
-0.8586
-0.2755
-1.0750
6
8.2539
-2.8721
-0.5496
0.8661
1.4973
7
8.3046
-5.4934
0.0080
-1.2320
1.2450
8
7.0406
1.4542
1.4681
-1.3204
1.1038
9
6.8416
-1.1500
0.1584
-1.4614
1.4064
10
-1.8991
-1.3645
-3.5401
0.1384
-1.0253
11
0.1033
1.0660
-2.4731
0.4460
-0.9366
12
0.1227
2.7583
-2.5841
-0.6775
-0.4761
13
2.9057
-0.6468
-2.3662
0.2029
-0.8006
14
-2.7731
-0.9990
-4.4835
-1.6639
0.0846
15
-2.9552
2.7171
-4.5537
-4.1590
1.4409
16
-4.2508
3.7226
-5.0439
-2.2657
-2.3691
17
1.2587
1.2114
1.3578
0.6423
-1.4816
18
3.9268
1.5191
-1.9320
-0.2931
-0.4898
19
9.3672
-0.0612
0.8758
-0.0257
-0.7625
20
-1.0664
3.6590
1.1118
1.4867
-0.6761
21
-2.4913
-1.7258
0.8635
0.3771
-1.5980
22
4.6917
0.8820
1.3025
0.0330
-1.4372
23
-0.8445
5.9142
2.1302
0.4665
0.7865
24
1.1053
6.3389
-0.1244
-0.7830
-0.5295
25
1.9905
0.5701
3.8052
0.7934
-0.6001
26
-1.3647
9.6680
1.5065
0.0127
0.0810
27
1.6113
-1.6881
1.9844
-1.7399
2.3240
28
7.8789
-4.9566
0.1180
1.1929
1.2565
29
-6.8021
-1.9096
4.3907
1.4590
0.1934
30
1.2311
0.1059
-0.3359
-0.1058
2.5217
31
-5.6701
-4.3043
-4.1307
2.2532
-1.9239
32
-5.2401
1.6640
-3.7472
1.3927
-0.9051
33
-5.1508
5.3068
-3.3875
0.9692
2.6413
34
2.2469
0.0121
0.1681
-1.3107
1.6087
35
2.5091
-4.1341
-6.0621
1.5724
0.8760
36
-0.8734
-0.2395
-3.7032
5.5596
1.2169
37
-2.5916
-6.9029
-3.8893
4.5502
-0.1520
38
-5.6731
1.4270
-0.8668
1.0290
1.9274
39
1.5083
0.3557
-1.2108
0.6666
1.3105
40
-1.2330
2.6784
-4.2421
-4.0828
-0.6014
41
-1.6943
4.1259
-2.9150
-0.9705
-0.5250
42
-9.7477
3.2048
-3.2242
0.0645
1.2387
43
-5.7349
10.1939
-0.3842
1.8287
1.2970
44
6.7774
-10.7151
0.7029
-0.7140
1.6126
45
-6.3299
-4.7704
2.8196
-0.5370
-0.9357
© 2003 by Chapman & Hall/CRC

˜S
1
2
3
4
5
46
-1.4643
-7.7089
2.4053
0.2631
0.8422
47
-1.0564
-3.2776
0.2526
2.5518
1.3459
48
-0.6876
3.1744
-2.3780
0.6098
0.1191
49
6.7672
5.3878
4.0594
0.4279
-0.2528
50
-2.9901
-0.9787
2.5267
1.6197
-2.5275
51
-8.8425
-2.9188
-0.3269
-0.7978
-1.3129
52
-10.4148
-7.5808
1.9425
-1.4108
-1.0169
53
-6.2772
-3.3309
6.7783
-2.7691
0.6239
54
-10.2317
-3.3483
0.1529
-0.6803
-0.4715
55
-3.2689
3.2464
-0.1322
-0.1348
-1.1372
© 2003 by Chapman & Hall/CRC
TABLE 10.12
ICM estimated sources.  (continued)

10.9
Discussion
After having estimated the model parameters, the estimates of the sources
as well as the mixing matrix are now available.
The estimated matrix of
sources corresponds to the unobservable signals or conversations emitted from
the mouths of the speakers at the cocktail party. Row i of the estimated source
matrix is the estimate of the unobserved source vector at time i and column j
of the estimated source matrix is the estimate of the unobserved conversation
of speaker j at the party for all n time increments.
© 2003 by Chapman & Hall/CRC

Exercises
1. Specify that µ and Λ are independent with the prior distribution for the
overall mean µ being the vague prior
p(µ) ∝(a constant),
the distribution for the factor loading matrix being
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 10.4.2-10.4.4.
Combine these prior distributions with the likelihood in Equation 10.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [52, 59].
2. Specify that µ and Λ are independent with the prior distribution for the
overall mean µ being the Conjugate prior
p(µ|Σ) ∝|hΣ|−1
2 e−1
2 (µ−µ0)(hΣ)−1(µ−µ0)′,
the distribution for the factor loading matrix being
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 10.4.2-10.4.2.
Combine these prior distributions with the likelihood in Equation 10.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [61].
3. Specify that µ and Λ are independent with the prior distribution for the
overall mean µ being the generalized Conjugate prior
p(µ) ∝|Γ|−1
2 e−1
2 (µ−µ0)′Γ−1(µ−µ0),
the distribution for the factor loading matrix being
p(λ) ∝|∆|−1
2 e−1
2 (λ−λ0)′∆−1(λ−λ0),
© 2003 by Chapman & Hall/CRC

and the others as in Equations 10.4.2-10.4.4.
Combine these prior distributions with the likelihood in Equation 10.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [61].
4. Specify that the prior distribution for the matrix of sources S is
p(S) =

1 S = S0
0 S ̸= S0 ,
(10.9.1)
and as a result there is no variability or covariance matrix R and thus
no p(R). Show that by taking the (en,S0) = U, (µ,Λ) = B, and the
Conjugate prior distributions for the unknown model parameters, the
resulting model is the Bayesian Regression model given in Chapter 8.
© 2003 by Chapman & Hall/CRC

11
Unobservable and Observable Source Separation
11.1
Introduction
There may be instances where some sources may be speciﬁed to be observ-
able while others are not. An example of such a situation is when we recognize
that a stereo at a cocktail party is tuned to a particular radio station. We
record the radio station in isolation; thus the source is said to be observable,
but the associated mixing coeﬃcients for this source are still unknown. The
following model allows for such situations. The following model is a combi-
nation of the Bayesian Regression model for observable sources introduced in
Chapter 8 and the Bayesian Source Separation model for unobservable sources
described in Chapter 10. Either model may be obtained as a special case by
setting either the number unobservable or observable sources to be zero.
11.2
Model
Consider the model at time i, in which p-dimensional vector-valued obser-
vations xi, on p possibly correlated random variables are observed as well as
(q + 1)-dimensional vector-valued sources (including a vector of ones for the
overall mean) on ui, but m sources, at each time increment si, i = 1,...,n,
are unobservable. The (q +1)-dimensional observable sources are denoted by
ui as in Regression with coeﬃcients denoted by B and the m-dimensional un-
observable sources by si with coeﬃcients given by Λ. The mixing coeﬃcients
for the observable sources denoted by B will be referred to as Regression
coeﬃcients or the matrix of Regression coeﬃcients. The unobservable and
observable Source Separation model is given by
(xi|B,ui,Λ,si) =
B
ui
+
Λ
si
+
ǫi,
(p×1)
[p×(q +1)] [(q +1)×1]
(p×m) (m×1)
(p×1)
(11.2.1)
where the variables are as previously deﬁned.
© 2003 by Chapman & Hall/CRC

That is, the observed signal xij for microphone j at time (observation) i
contains a linear combination of the (q + 1) observable sources 1,ui1,...,uiq
(where the ﬁrst element of the ui’s is a 1 for the overall mean µj) in addition
to a linear combination of the m unobserved source components si1,...,sim
with amplitudes or mixing coeﬃcients λj1,...,λjm. This combined model can
be written in terms of vectors to describe the observed signal at microphone
j at time i as
xij = β′
jui +λ′
jsi +ǫij,
(11.2.2)
where ui = (1,ui1,...,uiq)′, βj = (βj0,...,βjq)′, λj = (λj1,...,λjm)′, and si =
(si1,...,sim)′.
If any or all of the unobservable sources were speciﬁed to
be observable, they could be grouped into the u’s and their Regression or
observed mixing coeﬃcients computed. This is also represented as
(xij|µj,λj,si,ui) = µj +
q

t=1
βjt uit +
m

k=1
λjk sik +ǫij.
(11.2.3)
The recorded or observed conversation for microphone j at time increment
i is a linear mixture of the (q +1) observable sources and the m unobservable
sources at time increment i and a random noise term. The observed sources
contains a 1 for an overall mean for microphone j.
The unobservable and observable Source Separation model that describes
all observations for all microphones can be written in terms of matrices as
(X|B,U,Λ,S) =
U
B′
+
S
Λ′
+
E,
(n×p)
[n×(q +1)] [(q +1)×p]
(n×m) (m×p)
(n×p)
(11.2.4)
where X′ = (x1,...,xn), U ′ = (u1,...,un), Λ′ = (λ1,...,λp) S′ = (s1,...,sn),
and E′ = (ǫ1,...,ǫn).
11.3
Likelihood
Regarding the errors of the observations, it is speciﬁed that they are in-
dependent Multivariate Normally distributed random vectors with mean zero
and full positive deﬁnite symmetric covariance matrix Σ.
From this error
speciﬁcation, it is seen that the observation vector xi given the source vector
si, the Regression coeﬃcient matrix B, the observable source vector ui, the
mixing matrix Λ, and the error covariance matrix Σ is Multivariate Normally
distributed with likelihood given by
© 2003 by Chapman & Hall/CRC

p(xi|B,ui,Λ,si,Σ) ∝|Σ|−1
2 e−1
2 (xi−Bui−Λsi)′Σ−1(xi−Bui−Λsi). (11.3.1)
With the previously described matrix representation, the joint likelihood of
all n observation vectors collected into the observations matrix X is given by
the Matrix Normal Distribution
p(X|U,B,S,Λ,Σ) ∝|Σ|−n
2 e−1
2 tr(X−UB′−SΛ′)Σ−1(X−UB′−SΛ′)′,
(11.3.2)
where the variables are as previously deﬁned.
The Regression and the mixing coeﬃcient matrices B and Λ are joined into
a single coeﬃcient matrix as C = (B,Λ). The observable and unobservable
source matrices U and S are also joined as Z = (U,S). Having joined these
matrices, the unobservable and observable Source Separation model is now in
a matrix representation given by
(X|C,Z) =
Z
C′
+
E,
n×p
n×(m+q +1) (m+q +1)×p
(n×p)
(11.3.3)
and its corresponding likelihood is given by the Matrix Normal distribution
p(X|C,Z,Σ) ∝|Σ|−n
2 e−1
2 tr(X−ZC′)Σ−1(X−ZC′)′,
(11.3.4)
where all variables are as deﬁned above and tr(·) denotes the trace operator.
Again, the objective is to unmix the unobservable sources by estimating
the matrix containing them S and to obtain knowledge about the mixing
process by estimating the Regression coeﬃcients (coeﬃcients for the observ-
able sources) B, the matrix of mixing coeﬃcients (coeﬃcients for the unob-
servable sources) Λ, and the error covariance matrix Σ.
Both Conjugate and generalized Conjugate distributions are utilized in or-
der to quantify our prior knowledge regarding value of the parameters.
11.4 
Conjugate Priors a nd Po sterior
The unobservable and observable Bayesian Source Separation model that
was just described, is based on previous work [52, 57, 59].
When quantifying available prior information regarding the parameters of
interest, Conjugate prior distributions are speciﬁed as described in Chapter 4.
The joint prior distribution for the model parameters which are the matrix
of (Regression/mixing) coeﬃcients C, the matrix of sources S, the source
covariance matrix R, and the error covariance matrix Σ is given by
© 2003 by Chapman & Hall/CRC

p(S,R,C,Σ) = p(S|R)p(R)p(C|Σ)p(Σ),
(11.4.1)
where the prior distributions for the model parameters from the Conjugate
procedure outlined in Chapter 4 are given by
p(S|R) ∝|R|−n
2 e−1
2 trR−1(S−S0)′(S−S0),
(11.4.2)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(11.4.3)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(11.4.4)
p(C|Σ) ∝|D|−p
2 |Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′,
(11.4.5)
where the matrices Σ, R, V , D, and Q are positive deﬁnite. The hyperpa-
rameters S0, η, V , C0, D, ν, and Q are to be assessed and having done so
completely determines the joint prior distribution. The prior distributions for
the parameters are Matrix Normal for the matrix of sources where the source
components are free to be correlated, Matrix Normal for the matrix of Re-
gression/mixing coeﬃcients, while the observation error and source covariance
matrices are taken to be Inverted Wishart distributed.
Note that both Σ and R are full positive deﬁnite symmetric covariance
matrices allowing both the observed mixed signals (the elements in the xi’s)
and also the unobserved source components (the elements in the si’s) to be
correlated. The mean of the sources is free to be general but often taken to
be constant for all observations and thus without loss of generality taken to
be zero. Here, an observation (time) varying source mean is adopted.
Upon using Bayes’ rule, the joint posterior distribution for the unknown
parameters is proportional to the product of the joint prior distribution and
the likelihood and is given by
p(S,R,C,Σ|U,X) ∝|Σ|−(n+ν+m+q+1)
2
e−1
2 trΣ−1G
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ], (11.4.6)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q.
(11.4.7)
This joint posterior distribution must now be evaluated in order to obtain
parameter estimates of the sources S, the Regression/mixing matrix C, the
errors of the sources R, and the errors of observation Σ. Marginal posterior
mean and joint maximum a posteriori estimates of the parameters S, R, C,
Σ are found by the Gibbs sampling and ICM algorithms.
© 2003 by Chapman & Hall/CRC

11.5 
Conjugate Estimation a nd Inference
With the above posterior distribution, it is not possible to obtain marginal
distributions and thus marginal estimates for any of the parameters in an
analytic closed form. It is also not possible to obtain explicit formulas for
maximum a posteriori estimates. It is possible to use both Gibbs sampling,
as described in Chapter 6 to obtain marginal parameter estimates and the ICM
algorithm for ﬁnding maximum a posteriori estimates. For both estimation
procedures, the posterior conditional distributions are needed.
11.5.1
Posterior Conditionals
From the joint posterior distribution we can obtain the posterior condi-
tional distribution for each of the model parameters.
The conditional posterior distribution for the Regression/mixing matrix C
is found by considering only the terms in the joint posterior distribution which
involve C and is given by
p(C|S,R,Σ,U,X) ∝p(C|Σ)p(X|C,S,Σ,U)
∝|Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝e−1
2 trΣ−1[(C−C0)D−1(C−C0)′+(X−ZC′)′(X−ZC′)]
∝e−1
2 trΣ−1(C−˜
C)(D−1+Z′Z)(C−˜
C)′,
(11.5.1)
where the variable ˜C, the posterior conditional mean and mode, has been
deﬁned and is given by
˜C = [C0D−1 +X′Z](D−1 +Z′Z)−1
(11.5.2)
= C0[D−1(D−1 +Z′Z)−1]+ ˆC[(Z′Z)(D−1 +Z′Z)−1].
(11.5.3)
Note that the matrix ˜C can be written as a weighted combination of the
prior mean C0 from the prior distribution and the data mean ˆC = X′Z(Z′Z)−1
from the likelihood.
The conditional distribution for the matrix of Regression and mixing coef-
ﬁcients C given the matrix of unobservable sources S, the source covariance
matrix R, the error covariance matrix Σ the matrix of observable sources U,
and the data matrix X is Matrix Normally distributed.
The conditional posterior distribution of the observation error covariance
matrix Σ is found by considering only the terms in the joint posterior distri-
bution which involve Σ and is given by
© 2003 by Chapman & Hall/CRC

p(Σ|S,R,C,U,X) ∝p(Σ)p(Λ|Σ)p(X|C,S,Σ,U)
∝|Σ|−ν
2 e−1
2 trΣ−1Q|Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′)
∝|Σ|−(n+ν+m+q+1)
2
e−1
2 trΣ−1G,
(11.5.4)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q,
(11.5.5)
with a mode as discussed in Chapter 2 given by
˜Σ =
G
n+ν +m+q +1 .
(11.5.6)
The posterior conditional distribution of the observation error covariance
matrix Σ given the matrix of unobservable sources S, the source covariance
matrix R, the matrix of Regression/mixing coeﬃcients C, the observable
sources U, and the data X is an Inverted Wishart.
The conditional posterior distribution for the sources S is found by consid-
ering only those terms in the joint posterior distribution which involve S and
is given by
p(S|B,Λ,R,Σ,U,X) ∝p(S|R)p(X|B,Λ,S,Σ,U)
∝|R|−n
2 e−1
2 trR−1(S−S0)′(S−S0)
×|Σ|−n
2 e−1
2 trΣ−1(X−UB′−SΛ′)′(X−UB′−SΛ′)
∝e−1
2 tr(S−˜S)(R−1+Λ′Σ−1Λ)(S−˜S)′,
(11.5.7)
where the matrix ˜S has been deﬁned which is the posterior conditional mean
and mode and is given by
˜S = [S0R−1 +(X −UB′)Σ−1Λ](R−1 +Λ′Σ−1Λ)−1.
(11.5.8)
The conditional posterior distribution for the sources S given the matrix
of Regression coeﬃcients B, the matrix of mixing coeﬃcients Λ, the source
covariance matrix R, the error covariance matrix Σ, the matrix of observable
sources U, and the matrix of data X is Matrix Normally distributed.
The conditional posterior distribution for the source covariance matrix R is
found by considering only the terms in the joint posterior distribution which
involve R and is given by
© 2003 by Chapman & Hall/CRC

p(R|C,S,Σ,U,X) ∝p(R)p(S|R)p(X|C,S,Σ,U)
∝|R|−η
2 e−1
2 trR−1V |R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
∝|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(11.5.9)
with the posterior conditional mode as described in Chapter 2 given by
˜R = (S −S0)′(S −S0)+V
n+η
.
(11.5.10)
The conditional posterior distribution for the source covariance matrix R
given the matrix of Regression/mixing coeﬃcients C, the matrix of sources
S, the error covariance matrix Σ, the matrix of observable sources U, and the
matrix of data X is Inverted Wishart distributed.
11.5.2
Gibbs Sampling
To ﬁnd marginal posterior mean estimates of the parameters from the joint
posterior distribution using the Gibbs sampling algorithm, start with initial
values for the matrix of sources S and the error covariance matrix Σ, say ¯S(0)
and ¯Σ(0), and then cycle through
¯C(l+1) = a random variate from p(C| ¯S(l), ¯R(l), ¯Σ(l),U,X)
= ACYCB′
C +MC,
(11.5.11)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l), ¯C(l+1),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(11.5.12)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1),U,X)
= AR(Y ′
RYR)−1A′
R,
(11.5.13)
¯S(l+1) = a random variate from p(S| ¯R(l+1), ¯C(l+1), ¯Σ(l+1),U,X)
= YSB′
S +MS,
(11.5.14)
where
ACA′
C = ¯Σ(l),
BCB′
C = (D−1 + ¯Z′
(l) ¯Z(l))−1,
¯Z(l) = (U, ¯S(l)),
MC = (X′ ¯Z(l) +C0D−1)(D−1 + ¯Z′
(l) ¯Z(l))−1
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))+
( ¯C(l+1) −C0)D−1( ¯C(l+1) −C0)′ +Q,
© 2003 by Chapman & Hall/CRC

ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
BSB′
S = ( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MS = [S0 ¯R−1
(l+1) +(X −U ¯B′
(l+1))¯Σ−1
(l+1)¯Λ(l+1)]
×( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while YC, YΣ, YR, and YS are p × (m + q + 1), (n + ν + m + 1 + p + 1) × p,
(n+η +m+1)×m, and n×m dimensional matrices respectively, whose ele-
ments are random variates from the standard Scalar Normal distribution. The
formulas for the generation of random variates from the conditional posterior
distributions are easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of
the parameters. Exact sampling-based estimates of other quantities can also
be found.
Similar to Bayesian Regression, Bayesian Factor Analysis, and
Bayesian Source Separation, there is interest in the estimate of the marginal
posterior variance of the matrix containing the Regression and mixing coeﬃ-
cients
var(c|¯c,X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C).
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the vector
containing the Regression and mixing coeﬃcients, their distribution is
p(c|¯c,X,U) ∝| ¯∆|−1
2 e−1
2 (c−¯c) ¯∆−1(c−¯c)′,
(11.5.15)
where ¯c and ¯∆are as previously deﬁned.
To evaluate statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the matrix containing the Regression and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the entire matrix containing the Regression and mixing coeﬃcients,
a submatrix, or a particular independent variable or source, or an element
by computing marginal distributions.
It can be shown that the marginal
© 2003 by Chapman & Hall/CRC

distribution of the kth column of the matrix containing the Regression and
mixing coeﬃcients C, Ck is Multivariate Normal
p(Ck| ¯Ck,X,U) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(11.5.16)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be evaluated for a subset of coeﬃcients of the kth column of
C by determining the marginal distribution of the subset within Ck which is
also Multivariate Normal. With the subset being a singleton set, signiﬁcance
can be evaluated for a particular coeﬃcient with the marginal distribution of
the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X,U) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(11.5.17)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(11.5.18)
follows a Normal distribution with a mean of zero and variance of one.
11.5.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
matrix of coeﬃcients C, the error covariance matrix Σ, the matrix of sources
S, and the source covariance matrix R by using the ICM algorithm. To jointly
maximize the joint posterior distribution using the ICM algorithm, start with
an initial value for the matrix of sources S, say ˜S(0), and then cycle through
˜C(l+1) =
Arg Max
C
p(C| ˜S(l), ˜R(l), ˜Σ(l),X)
= (X′ ˜Z(l) +C0D−1)(D−1 + ˜Z′
(l) ˜Z(l))−1,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜R(l), ˜S(l),X)
= [(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))
+( ˜C(l+1) −C0)D−1( ˜C(l+1) −C0)′ +Q]/(n+ν +m+q +1),
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l), ˜Σ(l+1),X)
= [S0 ˜R−1
(l) +(X −U ˜B′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)]
×( ˜R−1
(l) + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
˜R(l+1) =
Arg Max
R
p(R| ˜S(l+1), ˜C(l+1), ˜Σ(l+1),X)
© 2003 by Chapman & Hall/CRC

= ( ˜S(l+1) −S0)′( ˜S(l+1) −S0)+V
n+η
,
where the matrix ˜Z(l) = (U, ˜S(l)) until convergence is reached. The converged
values ( ˜S, ˜R, ˜C, ˜Σ) are joint posterior modal (maximum a posteriori) estimates
of the parameters. Conditional maximum a posteriori variance estimates can
also be found. The conditional modal variance of the matrix containing the
Regression and mixing coeﬃcients is
var(C| ˜C, ˜S, ˜R, ˜Σ,X,U) = ˜Σ⊗(D−1 ⊗˜Z′ ˜Z)−1
or equivalently
var(c|˜c, ˜S, ˜R, ˜Σ,X,U) = (D−1 ⊗˜Z′ ˜Z)−1 ⊗˜Σ
= ˜∆,
where c = vec(C), ˜S, ˜R, and ˜Σ are the converged value from the ICM algo-
rithm.
To evaluate statistical signiﬁcance with the ICM approach, use the condi-
tional distribution of the matrix containing the Regression and mixing coeﬃ-
cients which is
p(C| ˜C, ˜S, ˜R, ˜Σ,X,U) ∝|D−1 + ˜Z′ ˜Z|
1
2 |˜Σ|−1
2 e−1
2 tr ˜Σ−1(C−˜
C)(D−1+ ˜
Z′ ˜
Z)(C−˜
C)′,
(11.5.19)
That is,
C| ˜C, ˜S, ˜R, ˜Σ,X,U ∼N

˜C, ˜Σ⊗(D−1 + ˜Z′ ˜Z)−1
.
(11.5.20)
General simultaneous hypotheses can be evaluated regarding the entire ma-
trix containing the Regression and mixing coeﬃcients, a submatrix, or the
coeﬃcients of a particular independent variable or source, or an element by
computing marginal conditional distributions.
It can be shown [17, 41] that the marginal conditional distribution of any
column of the matrix containing the Regression and mixing coeﬃcients C, Ck
is Multivariate Normal
p(Ck| ˜Ck, ˜S, ˜Σ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Ck−˜
Ck)′(Wkk ˜Σ)−1(Ck−˜
Ck),
(11.5.21)
where W = (D−1 + ˜Z′ ˜Z)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of C, signiﬁcance can be de-
termined for a particular independent variable or source. Signiﬁcance can be
determined for a subset of coeﬃcients by determining the marginal distrib-
ution of the subset within Ck which is also Multivariate Normal. With the
© 2003 by Chapman & Hall/CRC

subset being a singleton set, signiﬁcance can be determined for a particular
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜S, ˜Σjj,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(Ckj−˜Ckj)2
2Wkk ˜Σjj ,
(11.5.22)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)
,
Wkk ˜Σjj
(11.5.23)
follows a Normal distribution with a mean of zero and variance of one.
11.6 
Generalized Priors and Posterior
Generalized Conjugate prior distributions are assessed in order to quantify
available prior information regarding values of the model parameters. The
joint prior distribution for the sources S, the source covariance matrix R, the
vector of coeﬃcients c = vec(C), and the error covariance matrix Σ is given
by
p(S,R,Σ,c) = p(S|R)p(R)p(Σ)p(c),
(11.6.1)
where the prior distribution for the parameters from the generalized Conjugate
procedure outlined in Chapter 4 are as follows
p(S|R) ∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′,
(11.6.2)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(11.6.3)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(11.6.4)
p(c) ∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(11.6.5)
where Σ, R, V , Q, and ∆are positive deﬁnite matrices.
The hyperpara-
meters S0, η, V , ν, Q, c0, and ∆are to be assessed.
Upon assessing the
hyperparameters, the joint prior distribution is completely determined.
The prior distribution for the matrix of sources S is Matrix Normally dis-
tributed, the prior distribution for the source vector covariance matrix R is
© 2003 by Chapman & Hall/CRC

Inverted Wishart distributed, the vector of combined Regression/mixing coef-
ﬁcients c = vec(C), C = (B,Λ) is Multivariate Normally distributed, the prior
distribution for the error covariance matrix Σ is Inverted Wishart distributed.
Note that both Σ and R are full covariance matrices allowing both the
observed mixed signals (microphones) and the unobserved source components
(speakers) to be correlated. The mean of the sources is often taken to be
constant for all observations and thus without loss of generality taken to be
zero. An observation (time) varying source mean is adopted here.
Upon using Bayes’ rule the joint posterior distribution for the unknown
parameters with generalized Conjugate prior distributions for the model pa-
rameters is given by
p(S,R,c,Σ|U,X) ∝p(S|R)p(R)p(Σ)p(c)p(X|C,Z,Σ),
(11.6.6)
which is
p(S,R,c,Σ|U,X) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q]
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ]
×|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(11.6.7)
after inserting the joint prior distribution and the likelihood.
This joint posterior distribution must now be evaluated in order to ob-
tain parameter estimates of the matrix of sources S, the vector of Regres-
sion/mixing coeﬃcients c, the sources covariance matrix R, and the error
covariance matrix Σ.
11.7
Generalized Estimation and Inference
With the generalized Conjugate prior distributions, it is not possible to ob-
tain all or any of the marginal distributions or explicit expressions for maxima
and thus marginal mean and joint maximum a posteriori estimates in closed
form. For these reasons, marginal mean and joint maximum a posteriori esti-
mates are found using the Gibbs sampling and ICM algorithms.
11.7.1
Posterior Conditionals
Both the Gibbs sampling and ICM require the posterior conditionals. Gibbs
sampling requires the conditionals for the generation of random variates while
ICM requires them for maximization by cycling through their modes or max-
ima.
© 2003 by Chapman & Hall/CRC

The conditional posterior distribution of the matrix of sources S is found
by considering only the terms in the joint posterior distribution which involve
S and is given by
p(S|B,R,Λ,Σ,U,X) ∝p(S|R)p(X|B,S,Λ,Σ,U)
∝e−1
2 tr(S−S0)′R−1(S−S0)
×e−1
2 tr(X−UB′−SΛ′)Σ−1(X−UB′−SΛ′)′,
which after performing some algebra in the exponent can be written as
p(S|B,R,Λ,Σ,U,X) ∝e−1
2 tr(S−˜S)(R−1+Λ′Σ−1Λ)(S−˜S)′,
(11.7.1)
where the matrix ˜S has been deﬁned to be
˜S = [S0R−1 +(X −UB′)Σ−1Λ](R−1 +Λ′Σ−1Λ)−1.
(11.7.2)
That is, the matrix of sources S given the matrix of Regression coeﬃcients
B, the source covariance matrix R, the mixing coeﬃcients Λ, the error covari-
ance matrix Σ the matrix observable sources U, and the matrix of observed
data X is Matrix Normally distributed.
The conditional posterior distribution of the source covariance matrix R is
found by considering only the terms in the joint posterior distribution which
involve R and is given by
p(R|C,S,Σ,U,X) ∝p(R)p(S|R)
∝|R|−ν
2 e−1
2 trR−1V |R|−n
2 e−1
2 trR−1(S−S0)′(S−S0)
∝|R|−(n+ν)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ].
(11.7.3)
That is, the posterior conditional distribution of the source covariance ma-
trix R given the matrix of Regression/mixing coeﬃcients C, the matrix of
sources S, the error covariance matrix Σ, the matrix of observable sources U,
and the matrix of data X has an Inverted Wishart distribution.
The conditional posterior distribution of the vector c containing the Re-
gression coeﬃcients B and the matrix of mixing coeﬃcients Λ is found by
considering only the terms in the joint posterior distribution which involve c
or C and is given by
p(c|S,R,Σ,U,X) ∝p(c)p(X|S,C,Σ,U)
∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′),
(11.7.4)
© 2003 by Chapman & Hall/CRC

which after performing some algebra in the exponent becomes
p(c|S,R,Σ,U,X) ∝e−1
2 (c−˜c)′(∆−1+Z′Z⊗Σ−1)(c−˜c),
(11.7.5)
where the vector ˜c has been deﬁned to be
˜c = (∆−1 +Z′Z ⊗Σ−1)−1[∆−1c0 +(Z′Z ⊗Σ−1)ˆc]
(11.7.6)
and the vector ˆc has been deﬁned to be
ˆc = vec[X′Z(Z′Z)−1].
(11.7.7)
Note that the vector ˜c can be written as a weighted combination of the prior
mean c0 from the prior distribution and the data mean ˆc from the likelihood.
The conditional posterior distribution of the vector c containing the matrix
of Regression coeﬃcients B and the matrix of mixing coeﬃcients Λ given the
matrix of sources S, the source covariance matrix R, the error covariance
matrix Σ, the matrix of observable sources U, and the matrix of observed
data X is Multivariate Normally distributed.
The conditional posterior distribution of the error covariance matrix Σ is
found by considering only the terms in the joint posterior distribution which
involve Σ and is given by
p(Σ|S,R,C,U,X) ∝p(Σ)p(X|S,C,Σ,U)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′(X−ZC′)+Q].
(11.7.8)
That is, the conditional posterior distribution of the error covariance ma-
trix Σ given the matrix of sources S, the source covariance matrix R, the
matrix of coeﬃcients C, the matrix of observable sources U, and the matrix
of observable data X has an Inverted Wishart distribution.
The modes of these posterior conditional distributions are as described in
Chapter 2 and given by ˜S, ˜c, (both as deﬁned above)
˜R = (S −S0)′(S −S0)+V
n+η
,
(11.7.9)
and
˜Σ = (X −ZC′)′(X −ZC′)+Q
n+ν
,
(11.7.10)
respectively.
© 2003 by Chapman & Hall/CRC

11.7.2
Gibbs Sampling
To ﬁnd marginal mean estimates of the parameters from the joint posterior
distribution using the Gibbs sampling algorithm, start with initial values for
the matrix of sources S and the error covariance matrix Σ,say ¯S(0) and ¯Σ(0),
and then cycle through
¯c(l+1) = a random variate from p(c| ¯S(l), ¯Σ(l), ¯R(l+1),U,X)
= AcYc +Mc,
(11.7.11)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l+1),¯c(l+1),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(11.7.12)
¯R(l+1) = a random variate from p(R| ¯S(l),¯c(l+1), ¯Σ(l+1),U,X)
= AR(Y ′
RYR)−1A′
R,
(11.7.13)
¯S(l+1) = a random variate from p(S| ¯R(l+1),¯c(l+1), ¯Σ(l+1),U,X)
= YSB′
S +MS,
(11.7.14)
where
ˆc(l) = vec[X′ ¯Z(l)( ¯Z′
(l) ¯Z(l))−1],
¯c(l+1) = [∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc(l)],
AcA′
c = (∆−1 + ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )−1,
Mc = [∆−1 +
¯
¯Z′
(l) ¯Z(l) ⊗¯Σ
−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Z(l) ⊗¯Σ−1
(l) )ˆc],
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′(X −¯Z(l) ¯C′
(l+1))+Q,
ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
BSB′
S = ( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MS = [S0 ¯R−1
(l+1) +(X −U ¯B′
(l+1))¯Σ−1
(l+1)¯Λ(l+1)]
×( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1
while Yc, YΣ, YR, and YS are p(m+1)×1, (n+ν +p+1)×p, (n+η+m+1)×
m, and n × m dimensional matrices whose respective elements are random
variates from the standard Scalar Normal distribution. The formulas for the
generation of random variates from the conditional posterior distributions is
easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯c = 1
L
L

l=1
¯c(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
© 2003 by Chapman & Hall/CRC

which are the exact sampling-based marginal posterior mean estimates of the
parameters. Exact sampling-based estimates of other quantities can also be
found. Similar to Regression, Factor Analysis, and Source Separation, there
is interest in the estimate of the marginal posterior variance of the matrix
containing the Regression and mixing coeﬃcients
var(c|¯c,X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C).
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the vector
containing the Regression and mixing coeﬃcients, their distribution is
p(c|¯c,X,U) ∝| ¯∆|−1
2 e−1
2 (c−¯c)′ ¯∆−1(c−¯c),
(11.7.15)
where ¯c and ¯∆are as previously deﬁned.
To evaluate statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the vector c containing the Regression and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the entire coeﬃcient vector of Regression and mixing coeﬃcients, a
subset of it, or the coeﬃcients for a particular independent variable or source
by computing marginal distributions.
It can be shown that the marginal
distribution of the kth column of the matrix containing the Regression and
mixing coeﬃcients C, Ck is Multivariate Normal
p(Ck| ¯Ck,X,U) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(11.7.16)
where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be evaluated for a subset of means or coeﬃcients of the kth
column of C by determining the marginal distribution of the subset within
Ck which is also Multivariate Normal. With the subset being a singleton set,
signiﬁcance can be evaluated for a particular mean or coeﬃcient with the
marginal distribution of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X,U) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(11.7.17)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(11.7.18)
follows a Normal distribution with a mean of zero and variance of one.
© 2003 by Chapman & Hall/CRC

11.7.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
vector of coeﬃcients c, the matrix of sources S, the source covariance matrix
R, and the error covariance matrix Σ using the ICM algorithm. To jointly
maximize the joint posterior distribution using the ICM algorithm, start with
initial values for the matrix of sources ˜S and the error covariance matrix Σ,
say ˜S(0) and ˜Σ(0), and then cycle through
ˆc(l) = vec[X′ ˜Z(l)( ˜Z′
(l) ˜Z(l))−1],
˜c(l+1) =
Arg Max
c
p(c| ˜S(l), ˜R(l), ˜Σ(l),X,U)
= (∆−1 + ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) )−1[∆−1c0 +( ˜Z′
(l) ˜Z(l) ⊗˜Σ−1
(l) )ˆc(l)],
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜R(l), ˜S(l),X,U)
=
(X −˜Z(l) ˜C′
(l+1))′(X −˜Z(l) ˜C′
(l+1))+Q
n+ν
,
˜R(l+1) =
Arg Max
R
p(R| ˜S(l), ˜C(l+1), ˜Σ(l+1),X,U)
= ( ˜S(l) −S0)′( ˜S(l) −S0)+V
n+η
,
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l+1), ˜Σ(l+1),X,U)
= [S0 ˜R−1
(l+1) +(X −U ˜B′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)]
×( ˜R−1
(l+1) + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
where the matrix ˜Z(l) = (U, ˜S(l)) has been deﬁned until convergence is reached
with the joint modal (maximum a posteriori) estimates for the unknown model
parameters ( ˜S, ˜R,˜c, ˜Σ). Conditional maximum a posteriori variance estimates
can also be found. The conditional modal variance of the matrix containing
the Regression and mixing coeﬃcients is
var(c|˜c, ˜S, ˜R, ˜Σ,X,U) = [∆−1 + ˜Z′ ˜Z ⊗˜Σ]−1
= ˜∆,
where c = vec(C), while ˜S, ˜R, and ˜Σ are the converged value from the ICM
algorithm.
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
The posterior conditional distribution of the matrix containing the Regression
and mixing coeﬃcients C given the modal values of the other parameters and
the data is
© 2003 by Chapman & Hall/CRC

p(c|˜c, ˜S, ˜Σ,X,U) ∝| ˜∆|−1
2 e−1
2 (c−˜c)′ ˜∆−1(c−˜c).
(11.7.19)
To evaluate statistical signiﬁcance with the ICM approach, use the marginal
conditional distribution of the matrix containing the Regression and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the entire vector, a subset of it, or the coeﬃcients of a particular
independent variable or source by computing marginal distributions. It can
be shown that the marginal conditional distribution of the kth column Ck of
the matrix C containing the Regression and mixing coeﬃcients is Multivariate
Normal
p(Ck| ˜Ck, ˜Σ,X,U) ∝| ˜∆k|−1
2 e−1
2 (Ck−˜
Ck)′ ˜∆−1
k
(Ck−˜
Ck),
(11.7.20)
where ˜∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ˜∆.
Signiﬁcance can be evaluated for a subset of Regression or mixing coeﬃ-
cients of the kth column of C by determining the marginal distribution of the
subset within Ck which is also Multivariate Normal. With the subset being
a singleton set, signiﬁcance can be evaluated for a particular coeﬃcient with
the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜S, ˜Σjj,X) ∝( ˜∆kj)−1
2 e
−
(Ckj−˜Ckj)2
2 ˜
∆kj
,
(11.7.21)
where ˜∆kj is the jth diagonal element of ˜∆k. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)
,
˜∆kj
follows a Normal distribution with a mean of zero and variance of one.
11.8
Interpretation
Although the main focus after having performed a Bayesian Source Sep-
aration is the separated sources, there are others. One focus as in Bayesian
Regression is on the estimate of the Regression coeﬃcient matrix B which de-
ﬁnes a “ﬁtted” line. Coeﬃcients are evaluated to determine whether they are
statistically “large” meaning that the associated independent variable con-
tributes to the dependent variable or statistically “small” meaning that the
associated independent variable does not contribute to the dependent variable.
© 2003 by Chapman & Hall/CRC

The coeﬃcient matrix also has the interpretation that if all of the indepen-
dent variables were held ﬁxed except for one uij which if increased to u∗
ij, the
dependent variable xij increases to an amount x∗
ij given by
x∗
ij = βi0 +···+βiju∗
ij +···+βiquiq.
(11.8.1)
Another focus after performing a Bayesian Source Separation is in the esti-
mated mixing coeﬃcients. The mixing coeﬃcients are the amplitudes which
determine the relative contribution of the sources. A particular mixing co-
eﬃcient which is relatively “small” indicates that the corresponding source
does not signiﬁcantly contribute to the associated observed mixed signal. If
a particular mixing coeﬃcient is relatively “large,” this indicates that the
corresponding source does signiﬁcantly contribute to the associated observed
mixed signal.
11.9
Discussion
Returning to the cocktail party problem, the matrix of Regression coeﬃ-
cients B where B = (µ,B⋆) contains the matrix of mixing coeﬃcients B⋆for
the observed conversation (sources) U, and the population mean µ which is a
vector of the overall background mean level at each microphone.
After having estimated the model parameters, the estimates of the sources
as well as the mixing matrix are now available.
The estimated matrix of
sources corresponds to the unobservable signals or conversations emitted from
the mouths of the speakers at the cocktail party. Row i of the estimated source
matrix is the estimate of the unobserved source vector at time i and column j
of the estimated source matrix is the estimate of the unobserved conversation
of speaker j at the party for all n time increments.
© 2003 by Chapman & Hall/CRC

Exercises
1. Specify that B and Λ are independent with the prior distribution for
Regression coeﬃcients B being the vague prior
p(B) ∝(a constant),
the distribution for the mixing matrix being
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others as in Equations 11.4.2-11.4.4.
Combine these prior distributions with the likelihood in Equation 11.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal posterior mean and joint maximum a posteriori
parameter estimates [52, 59].
2. Specify that B and Λ are independent with the prior distribution for
the vector of Regression coeﬃcients β to be the Conjugate prior
p(B|Σ) ∝|Σ|−1
2 e−1
2 trΣ−1(B−B0)H−1(B−B0)′,
the distribution for the vector of mixing coeﬃcients λ to be
p(Λ|Σ) ∝|A|−p
2 |Σ|−m
2 e−1
2 trΣ−1(Λ−Λ0)A−1(Λ−Λ0)′,
and the others to be as in Equations 11.4.2–11.4.4.
Combine these prior distributions with the likelihood in Equation 11.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [57].
3. Specify that B and Λ are independent with the prior distribution for
the overall mean µ being the generalized Conjugate prior
p(β) ∝|Γ|−1
2 e−1
2 (β−β0)′Γ−1(β−β0),
the distribution for the mixing matrix being
p(λ) ∝|∆|−1
2 e−1
2 (λ−λ0)′∆−1(λ−λ0),
© 2003 by Chapman & Hall/CRC

and the others as in Equations 11.4.2–11.4.4.
Combine these prior distributions with the likelihood in Equation 11.3.2
to obtain a posterior distribution.
Derive Gibbs sampling and ICM
algorithms for marginal mean and joint maximum a posteriori parameter
estimates [57].
4. Show that by (a) setting the number of observable sources q to be equal
to zero, the resulting model is the Source Separation model of Chapter 10
and (b) by setting the number of unobservable sources m to be equal to
zero, the resulting model is the Regression model of Chapter 8.
© 2003 by Chapman & Hall/CRC

12
FMRI Case Study
12.1
Introduction
Functional magnetic resonance imaging (FMRI) is a designed experiment
[15] which often consists of a patient being given a sequence of stimuli AB
or ABC. Imaging takes place while the patient is responding either passively
or actively to these stimuli. A model is used which views the observed time
courses as being made up of a linear (polynomial) trend, responses (possibly
zero valued) due to the presentation of the stimuli, and other cognitive ac-
tivites that are typically termed random and grouped into the error term. The
association between the observed time course in each voxel and the sequence
of stimuli is determined. Diﬀerent levels of activation (association) for the
stimuli are colored accordingly. This chapter focuses on block designs but is
readily adapted to event-related designs.
In computing the activation level in a given voxel, a standard method [68]
is to assume known reference functions (independent variable) corresponding
to the responses of the diﬀerent stimuli, often square waves based on the ex-
perimental sequence (but sometimes sine waves and functions which mimic
the “hemodynamic” response), and then to perform a multiple Regression of
the observed time courses on them, a linear trend, and any other independent
variables. In the multiple Regression, t or F Statistics are computed for the
coeﬃcient associated with the reference function and voxels colored accord-
ingly. But the most important question is: How do we choose the reference
functions? What if they change (possibly nonlinearly) over the course of the
experiment? What if we’re interested in observing an “ah ha” moment? An a
priori ﬁxed reference functions are not capable of showing an “ah ha” moment.
The choice of the reference functions in computing the activation of FMRI
has been somewhat arbitrary and subjective. This chapter uses a coherent
Bayesian statistical approach to determine the underlying responses (or refer-
ence functions) to the presentation of the stimuli and determine statistically
signiﬁcant activation. In this approach, all the voxels contribute to “telling
us” the underlying responses due to the experimental stimuli.
The model
is presented and applied to a simulated FMRI data in which available prior
information is quantiﬁed and dependent contributions (components) to the
observed hemodynamic response are determined (separated).
© 2003 by Chapman & Hall/CRC

The utility of the Bayesian Source Separation model for FMRI can be moti-
vated by returning to the classic “cocktail party” problem [27, 28, 52]. At the
cocktail party, there are microphones scattered about that record partygoers
or speakers at a given number of time increments. The observed conversa-
tions consist of mixtures of true unobservable conversations. The objective
is to separate these observed signal vectors into true unobservable source sig-
nal vectors. As previously mentioned, there may be instances where some
sources may be speciﬁed to be observable while others are not. This situation
in which some sources are observable while others are unobservable is exactly
the problem we are addressing in FMRI.
The Bayesian Source Separation model decomposes the observed time course
in a voxel into a linear (or polynomial) trend and a linear combination of
unobserved component sequences. The linear (or polynomial) trend corre-
sponds to the observable sources and the (unobservable) sources that make
up the observed time course corresponds to the unobservable speakers. If the
sources were assumed to be known and no priors speciﬁed for the remaining
parameters, then the Bayesian approach reduces to the standard model and
activations determined accordingly. In practice we do not know the true un-
derlying hemodynamic time response (source reference) functions due to the
presentation of the experimental stimuli.
The Bayesian Source Separation model assesses a prior distribution for the
response functions as well as for the other parameters, and combines them
with the data to form a joint posterior distribution. From the posterior dis-
tribution, values for the source response functions as well as for the other
parameters are computed and statistically signiﬁcant activation determined.
The Bayesian Source Separation model allows the source reference functions
to be correlated and can incorporate incidental cognitive processes (blood
ﬂow) such as that due to cardiac activity and respiration. Modeling them
instead of grouping them into the error term could prove useful.
12.2
Model
In describing the model, sometimes it will be parameterized in terms of
rows while other times in terms of columns. Considering the observed time
course in voxel j at time t, the model is
xtj = βj0 +βj1ut1 +···+βjmutq +λj1st1 +···+λjmstm +ǫtj, (12.2.1)
in which the observed signal in voxel j at time t, xtj is made up of an overall
mean βj0 (the intercept); a linear combination of q observed source reference
functions ut1 +··· +utq (the time trend and other conative processes) which
© 2003 by Chapman & Hall/CRC

characterize a change in the observed time course over time and includes other
observable source reference functions; in addition to a linear combination of
the m unobserved source reference functions st1,...,stm which characterize
the contributions due to the presentation of the stimuli that make up the
observed time course; and random error ǫtj.
Coeﬃcients of the observed
source reference functions are called Regression coeﬃcients and those of the
unobserved source reference functions called mixing coeﬃcients. This model
can be written in terms of vectors as
xtj = β′
jut +λ′
jst +ǫtj,
(12.2.2)
where a linear trend is speciﬁed to be observable so that ut = (1,t)′, βj =
(βj0,βj1)′, λj = (λj1,...,λjm)′, and st = (st1,...,stm)′. If any or all of the
sources were assumed to be observable, they could be grouped into the u’s
and their coeﬃcients computed.
Each voxel has its own slope and intercept in addition to a set of mixing
coeﬃcients that do not change over time. In contrast, the unobserved un-
derlying source reference functions are the same for all voxels (with possibly
zero-valued coeﬃcients) at a given time but do change over time.
Now, considering p voxels at time t, the model can be written as
xt = But +Λst +ǫt,
(12.2.3)
where xt is a p × 1 vector of observed values at time t, B = (β1,...,βp)′ =
(B0,...,Bq) is the p×(q +1) matrix of Regression coeﬃcients (slopes and in-
tercepts), and Λ = (λ1,...,λp)′ = (Λ1,...,Λm) is the p×m dimensional matrix
of mixing coeﬃcients.
Alternatively, considering a given voxel j, at all n time points, the model
can be written as
Xj = Uβj +Sλj +Ej,
(12.2.4)
where Xj is an n × 1 vector of observed values for voxel j, U = (en,cn) =
(u1,...,un)′ = (en,U1), en is a n × 1 vector of ones, cn = (1,...,n)′, S =
(s1,...,sn)′ = (S1,...,Sm), Ej is an n × 1 vector of errors, while βj and λj
are as previously deﬁned.
The model which considers all of the voxels at all time points can be written
in terms of matrices as
X = UB′ +SΛ′ +E,
(12.2.5)
where X = (x1,...,xn)′ = (X1,...,Xp), E = (ǫ1,...,ǫn)′ = (E1,...,Ep) while
B, Λ, U, and S are as before.
Motivated by the central limit theorem, the errors of observation at each
time increment are taken to be Multivariate Normally distributed, as (ǫt|Σ) ∼
N(0,Σ); thus the observations are also Normally distributed as
© 2003 by Chapman & Hall/CRC

p(X|U,B,S,Λ,Σ) ∝|Σ|−n
2 e−1
2 trΣ−1(X−UB′−SΛ′)′(X−UB′−SΛ′),
(12.2.6)
where Σ is the error covariance matrix and the remaining variables are as
previously deﬁned.
By letting Z = (U,S) = (z1,...,zn)′ = (Z0,...,Zm+q) and C = (B,Λ) =
(c1,...,cn)′ = (C0,...,Cm+q), the model and likelihood become
X = ZC′ +E
(12.2.7)
and
p(X|U,B,S,Λ,Σ) ∝|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′).
(12.2.8)
12.3
Priors and Posterior
The method of subjectively assigning source reference functions and per-
forming a multiple Regression is equivalent to assigning degenerate prior dis-
tributions for them and vague priors for the remaining parameters. That is
equivalent to assuming that the probability distribution for the source refer-
ence functions is equal to unity at these assigned values and zero otherwise.
Instead of subjectively choosing the source reference functions as being
ﬁxed, prior information as to their values in the form of prior distributions
are assessed as are priors for any other observable contributing source refer-
ence functions to the observed signal. These prior distributions are combined
with the data and the source reference functions are determined statistically
using information contributed from every voxel.
In addition, prior distri-
butions are assessed for the covariance matrix for the (dependent) source
reference functions, the Regression coeﬃcients (slopes and intercepts), the
mixing coeﬃcients, and the covariance matrix for the observation error. The
prior distribution for the source reference functions, the covariance matrix for
source reference functions, the Regression coeﬃcients, the mixing coeﬃcients,
and the error covariance matrix are taken to be Normal, Inverted Wishart,
Normal, Normal, and Inverted Wishart distributed respectively.
The prior distributions for the parameters are the Normal and Inverted
Wishart distributions
p(S|R) ∝|R|−n
2 e−1
2 trR−1(S−S0)′(S−S0),
(12.3.1)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(12.3.2)
p(C|Σ) ∝|D|−p
2 |Σ|−(m+q+1)
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′,
(12.3.3)
© 2003 by Chapman & Hall/CRC

p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(12.3.4)
where the prior mean for the source component reference functions is S0 =
(s01,...,s0n)′ = (S01,...,S0m) and R is the covariance matrix of the source
reference functions. The hyperparameters S0, V , η, M, C0 = (B0,Λ0), Q, and
ν which uniquely deﬁne the remaining prior distributions are to be assessed
(see Appendix B).
Note that both Σ and R are full covariance matrices allowing the observed
mixed signals (the voxels) and also the unobserved source reference functions
to be correlated. The Regression and mixing coeﬃcients are also allowed to
be correlated by specifying a joint distribution for them and not constraining
them to be independent.
Upon using Bayes’ rule the posterior distribution for the unknown para-
meters is written as being proportional to the product of the aforementioned
priors and likelihood
p(B,S,R,Λ,Σ|U,X) ∝|Σ|−(n+ν+m+q+1)
2
e−1
2 trΣ−1G
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(12.3.5)
where
G = (X −ZC′)′(X −ZC′)+(C −C0)D−1(C −C0)′ +Q.
(12.3.6)
This posterior distribution must now be evaluated in order to obtain para-
meter estimates of the source reference functions, the covariance matrix for
the source reference functions, the matrix of mixing coeﬃcients, the matrix
of Regression coeﬃcients, and the error covariance matrix. In addition, sta-
tistically signiﬁcant activation is determined from the posterior distribution.
12.4
Estimation and Inference
As stated in the estimation section of the Unobservable and Observable
source Separation model, the above posterior distribution cannot be inte-
grated or diﬀerentiated analytically to obtain marginal distributions for mar-
ginal estimates or maxima for maximum a posteriori estimates.
Marginal
and maximum a posteriori estimates can be obtained via Gibbs sampling and
iterated conditional modes (ICM) algorithms [13, 14, 36, 40, 52, 57]. These
algorithms use the posterior conditional distributions and either generate ran-
dom variates or cycle through their modes.
© 2003 by Chapman & Hall/CRC

The posterior conditional distributions are as in Chapter 11 with Conjugate
prior distributions.
From Chapter 11, the estimates
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
are sampling-based marginal posterior mean estimates of the parameters which
converge almost surely to their population values.
Interval estimates can also be obtained by computing marginal covariance
matrices from the sample variates. The marginal covariance for the matrix of
Regression/mixing coeﬃcients is
var(c|X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C), ¯c = vec( ¯C), and ¯c(l) = vec( ¯C(l)).
The covariance matrices of the other parameters follow similarly. With a
speciﬁcation of Normality for the marginal posterior distribution of the mixing
coeﬃcients, their distribution is
p(c|X,U) ∝| ¯∆|−1
2 e−1
2 (c−¯c) ¯∆−1(c−¯c)′,
(12.4.1)
where ¯c and ¯∆are as previously deﬁned.
The marginal posterior distribution of the mixing coeﬃcients is
p(λ|X,U) ∝|¯Υ|−1
2 e−1
2 (λ−¯λ) ¯Υ−1(λ−¯λ)′,
(12.4.2)
where ¯λ = vec(¯Λ) and ¯Υ is the lower right pm×pm (covariance) submatrix in
¯∆.
From Chapter 11, the ICM estimates found by cycling through the modes
˜C = [X′ ˜Z +C0D−1](D−1 + ˜Z′ ˜Z)−1,
˜Σ = [(X −˜Z ˜C′)′(X −˜Z ˜C′)+( ˜C −C0)D−1( ˜C −C0)′
+ Q]/(n+ν +m+q +1),
˜R = ( ˜S −S0)′( ˜S −S0)+V
n+η
,
˜S = (X −U ˜B′)˜Σ−1˜Λ( ˜R−1 + ˜Λ′ ˜Σ−1˜Λ)−1
are maximum a posteriori estimates via the ICM estimation procedure [36, 40].
© 2003 by Chapman & Hall/CRC

Conditional modal intervals may be computed by using the conditional dis-
tribution for a particular parameter given the modal values of the others. The
posterior conditional distribution of the matrix of Regression/mixing coeﬃ-
cients C given the modal values of the other parameters and the data is
p(C| ˜C, ˜S, ˜R, ˜Σ,U,X) ∝|(D−1 + ˜Z′ ˜Z)|
p
2 |˜Σ|−m
2
×e−1
2 tr ˜Σ−1(C−˜
C)(D−1+ ˜
Z′ ˜
Z)(C−˜
C)′,
(12.4.3)
which may be also written in terms of vectors as
p(c|˜c, ˜S, ˜R, ˜Σ,U,X) ∝|(D−1 + ˜Z′ ˜Z)−1 ⊗˜Σ|−1
2
×e−1
2 (c−˜c)′[(D−1+ ˜
Z′ ˜
Z)−1⊗˜Σ]−1(c−˜c).
(12.4.4)
The marginal posterior conditional distribution of the mixing coeﬃcients
λ = vec(Λ) which is the last mp rows of c written in terms of vectors as
p(λ|˜λ, ˜S, ˜R, ˜Σ,U,X) ∝|˜Γ⊗˜Σ|−1
2 e−1
2 (λ−˜λ)′(˜Γ⊗˜Σ)−1(λ−˜λ)
(12.4.5)
and in terms of the matrix Λ is
p(Λ|˜Λ, ˜S, ˜R, ˜Σ,U,X) ∝|˜Γ|−p
2 |˜Σ|−m
2 e−1
2 tr ˜Σ−1(Λ−˜Λ)˜Γ−1(Λ−˜Λ)′, (12.4.6)
where ˜Γ is the lower right m×m portion of (D−1 + ˜Z′ ˜Z)−1.
After determining the test statistics, a threshold or signiﬁcance level is set
and a one to one color mapping is performed. The image of the colored voxels
is superimposed onto an anatomical image.
12.5
Simulated FMRI Experiment
For an example, data were generated to mimic a scaled down version of a
real FMRI experiment. The simulated experiment was designed to have three
stimuli, A, B, and C, but this method is readily adapted to any number of
stimuli and to event related designs. Stimuli A, B, and C were 22, 10, and
32 seconds in length respectively, and eight trials were performed for a total
of 512 seconds as illustrated in Figure 12.1.
A single trial is focused upon in Figure 12.2 which shows stimulus A lasting
22 seconds, stimulus B lasting 10 seconds, and stimulus C lasting 32 sec-
onds. As in the real FMRI experiment in the next section, observations are
© 2003 by Chapman & Hall/CRC

FIGURE 12.1
Experimental design: white task A, gray task B, and black task C in seconds.
16
32
48
64
80
96
112
128
taken every 4 seconds so that there are n = 128 in each voxel. The simulated
functional data are created with true source reference functions. These true
reference functions, one trial of which is given in Figure 12.3, both start at −1
when the respective stimulus is ﬁrst presented, increasing to +1 just as the
ﬁrst quarter of a sinusoid with a period of 16 seconds and amplitude of 2. The
reference functions are at +1 when each of the respective stimuli are removed
and decrease to −1 just as the third quarter of a sinusoid with a period of 16
seconds and amplitude of 2. This sinusoidal increase and decrease take 4 sec-
onds and is assumed to simulate hemodynamic responses to the presentation
and removal of each of the stimuli.
FIGURE 12.2
One trial: +1 when presented, -1 when not presented.
A simulated anatomical 4×4 image is determined as in Figure 12.4. Statis-
tically signiﬁcant activation is to be superimposed onto the anatomical image.
The voxels are numbered from 1 to 16 starting from the top left, proceeding
across and then down. The functional data for these 16 voxels was created
according to the Source Separation model
xtj
=
β′
T j
uT t
+
λ′
T j
sT t
+
ǫtj,
(1×1)
(1×2) (2×1) + (1×3) (3×1)
(1×1)
(12.5.1)
where j denotes the voxel, t denotes the time increment, ǫtj denotes the ran-
© 2003 by Chapman & Hall/CRC

FIGURE 12.3
True source reference functions.
10
20
30
40
50
60
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
dom error term, and the T subscript denotes that these are the true values. In
each voxel, the simulated observed data at each time increment was generated
according to the above model with random error added to each of the parame-
ters as follows. The random errors were generated according to ǫtj ∼N(0,10).
Noise was added to the sources reference functions and the mixing coeﬃcients
according to st ∼N(sT t,0.2I2), λj ∼N(λT j,0.25I2), and βj ∼N(βT j,0.1I2)
where I2 is the 2×2 identity matrix. The true sources were sampled every 4
seconds from those in Figure 12.3.
TABLE 12.1
True Regression and mixing coeﬃcients.
BT
1
2
3
4
ΛT
1
2
3
4
1
.2, .5
.7, .1
.4, .9
.3, .2
1
15, 5
2, 1
1, 15
2, 15
2
.9, .6
.4, .8
.5, .3
.2, .7
2
1, 2
15, 1
-2, 1
2, 15
3
.9, .1
.1, .3
.5, .5
.1, .6
3
1, 15
-1, 2
15, 1
2, 2
4
.6, .4
.4, .2
.4, .5
.8, .9
4
1, 15
2, 15
1, 2
15, 1
The true slopes and intercepts for the voxels along with the true source
amplitudes (mixing coeﬃcients) are displayed in their voxels location as in
© 2003 by Chapman & Hall/CRC

FIGURE 12.4
Simulated anatomical image.
−3
−1.5
0
1.5
3
1
2
3
4
1
2
3
4
Table 12.1.
All hyperparameters were assessed according to the methods in Appendix B
and an empirical Bayes, approach was taken that uses the current data as the
prior data. For presentation purposes, all values have been rounded to either
one or two digits.
For the prior means of the source reference functions, square functions are
assessed with unit amplitude. Note that observations are taken at 20 and 24
seconds while the point where stimulus A ends and stimulus B begins is at
22 seconds. The prior means for the source reference function associated with
stimulus A is at +1 until 20 seconds (observation 5), 0 at 24 seconds (obser-
vation 6) and then at −1 thereafter; and the one associated with stimulus B
is at −1 until 20 seconds (observation 5), 0 at 24 seconds (observation 6), and
then +1 thereafter. Both are at −1 between 32 seconds and 64 seconds. The
prior, Gibbs, and ICM sources are displayed in Figure 12.5.
The assessed prior values for D were as in Table 12.2 while Q was as in
Table 12.6 and ν = n.
The assessed values for η and v0 were η = 6 and
v0 = 0.13.
The Regression and mixing coeﬃcients are as in Tables 12.3 and 12.4. The
top set of values are the prior means, the middle set are the Gibbs sampling
estimates, and the bottom set are the ICM estimates.
© 2003 by Chapman & Hall/CRC

FIGURE 12.5
Prior −−, Gibbs estimated −−, and ICM estimated ·· reference functions for
20
40
60
80
100
120
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
(a) source one
20
40
60
80
100
120
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
(b) source two.
TABLE 12.2
Covariance hyperparameter for coeﬃcients.
D
1
2
3
4
1
0.0405
-0.0004
0.0036
0.0124
2
0.0000
0.0000
0.0000
3
0.0100
0.0033
4
0.0179
TABLE 12.3
Prior, Gibbs, and ICM Regression coeﬃcients.
B0
1
2
3
4
1
-3.95, 0.50
0.95, 0.04
1.81, 0.83
-2.20, 0.23
2
3.56, 0.58
-4.09, 0.83
3.99, 0.26
1.90, 0.68
3
2.50, 0.08
2.00, 0.26
-6.38, 0.55
1.81, 0.58
4
2.31, 0.39
3.59, 0.17
-3.68, 0.57
-3.46, 0.89
¯B
1
2
3
4
1
-3.93, 0.50
0.95, 0.04
1.81, 0.83
-2.19, 0.22
2
3.58, 0.57
-4.08, 0.83
3.99, 0.26
1.90, 0.68
3
2.52, 0.08
2.00, 0.26
-6.40, 0.55
1.82, 0.58
4
2.31, 0.39
3.60, 0.17
-3.68, 0.57
-3.45, 0.89
˜B
1
2
3
4
1
-3.95, 0.50
0.95, 0.04
1.82, 0.83
-2.19, 0.22
2
3.57, 0.57
-4.08, 0.83
3.98, 0.26
1.91, 0.68
3
2.51, 0.08
2.00, 0.26
-6.38, 0.55
1.81, 0.58
4
2.32, 0.38
3.60, 0.17
-3.67, 0.57
-3.46, 0.89
© 2003 by Chapman & Hall/CRC

TABLE 12.4
Prior, Gibbs, and ICM mixing coeﬃcients.
Λ0
1
2
3
4
1
10.32, -1.63
0.48, 1.28
-2.28, 12.28
0.40, 12.61
2
2.65, 3.35
12.69, -0.07
-1.10, 2.09
0.52, 12.75
3
-1.45, 13.50
-1.47, 1.24
11.53, -1.84
1.82, 2.69
4
-0.83, 14.27
1.58, 15.18
1.39, 0.71
12.13, 0.37
¯Λ
1
2
3
4
1
10.35, -1.62
0.48, 1.29
-2.28, 12.30
0.39, 12.60
2
2.66, 3.36
12.71, -0.07
-1.10, 2.10
0.54, 12.78
3
-1.45, 13.53
-1.47, 1.24
11.55, -1.85
1.82, 2.70
4
-0.83, 14.29
1.59, 15.21
1.39, 0.70
12.15, 0.397
˜Λ
1
2
3
4
1
10.32, -1.63
0.48, 1.29
-2.28, 12.29
0.40, 12.62
2
2.65, 3.35
12.70, -0.07
-1.10, 2.09
0.53, 12.76
3
-1.45, 13.51
-1.47, 1.24
11.54, -1.84
1.82, 2.69
4
-0.83, 14.28
1.58, 15.52
1.39, 0.71
12.14, 0.38
The (left) prior mode along with the Bayesian (center) Gibbs sampling
marginal and Bayesian (right) ICM maximum a posteriori source covariance
matrices for the source reference functions are displayed in Table 12.5.
TABLE 12.5
Prior, Gibbs, and ICM source covariances.
V/η
1
2
¯R
1
2
˜R
1
2
1
0.02
0
1
0.04
0.00
1
0.35
0.05
2
0.02
2
0.03
2
0.30
The prior mode along with the Bayesian Gibbs sampling marginal and
Bayesian ICM maximum a posteriori error covariance matrices are displayed
in Tables 12.6-12.8.
© 2003 by Chapman & Hall/CRC

TABLE 12.6
Error covariance prior mode.
Q
ν
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
159.3
25.2
12.7
29.0
3.1
35.8
-11.2
19.0
15.6
10.2
45.7
66.1
13.8
6.1
-0.7
26.6
2
148.8
-.1
13.7
15.8
5.3
-7.3
8.9
3.8
-9.7
10.4
3.4
-10.9
7.8
-9.3
25.9
3
193.4
44.3
-18.3
29.1
12.2
23.9
18.9
7.4
29.8
32.9
21.2
18.9
-30.6
-9.8
4
174.7
-.7
3.5
-6.4
31.6
30.2
15.5
35.3
19.8
23.6
9.7
8.4
2.4
5
135.7
-23.8
1.9
8.3
23.2
-3.8
-2.2
-3.0
2.5
9.7
29.2
16.0
6
186.3
19.7
12.9
5.7
1.2
60.9
17.4
-30.0
.0
-22.6
19.4
7
105.7
-1.9
5.4
15.6
5.5
-11.3
.5
2.7
-9.9
-13.4
8
126.9
14.8
7.8
11.4
37.3
32.9
4.6
17.7
20.6
9
192.4
17.7
0.5
26.8
45.9
37.7
-15.0
-3.0
10
145.4
20.7
6.5
3.4
23.2
32.7
-2.1
11
188.3
16.8
1.7
14.0
11.9
7.3
12
170.8
14.3
-10.6
-3.1
-8.6
13
193.5
45.4
37.2
-4.7
14
208.2
7.2
14.1
15
145.9
5.1
16
191.2
© 2003 by Chapman & Hall/CRC

TABLE 12.7
Gibbs estimates of covariances.
¯Ψ
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
158.6
25.3
13.3
29.4
2.8
34.4
-11.1
19.1
16.2
10.4
44.3
66.1
14.1
6.12
-0.9
25.2
2
149.3
-0.3
13.6
15.9
5.2
-7.3
8.72
3.7
-9.7
10.4
3.4
-11.1
7.6
-9.3
26.0
3
192.5
42.7
-18.8
29.6
12.1
22.3
17.1
7.23
30.4
32.7
19.3
16.9
-30.9
-9.6
4
173.5
-1.1
3.5
-6.7
29.9
28.5
15.4
35.6
19.6
21.6
7.6
8.34
2.3
5
136.0
-24.3
1.9
7.8
22.8
-3.8
-2.5
-3.2
2.0
9.1
29.3
15.78
6
185.0
20.0
12.8
6.0
1.3
59.3
17.2
-30.0
-0.3
-23.0
17.6
7
106.0
-2.1
5.0
15.6
5.7
-11.3
0.3
2.4
-9.9
-13.2
8
125.5
13.1
7.6
11.4
37.0
31.0
2.4
17.6
20.4
9
191.2
17.5
1.0
26.6
44.0
35.5
-15.2
-2.8
10
145.8
20.9
6.5
3.34
23.1
32.9
-1.9
11
187.3
16.6
2.0
14.2
11.7
5.7
12
171.3
14.0
-11.2
-3.1
-9.0
13
191.9
43.1
37.24
-4.7
14
206.4
7.0
13.7
15
146.5
4.8
16
190.2
© 2003 by Chapman & Hall/CRC

TABLE 12.8
ICM estimates of covariances.
˜Ψ
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
1
157.4
24.9
12.6
28.7
3.0
35.3
-11.1
18.8
15.5
10.1
45.0
65.3
13.6
6.1
-0.7
26.1
2
147.0
-0.2
13.6
15.6
5.2
-7.2
8.8
3.7
-9.6
10.3
3.4
-10.8
7.7
-9.2
25.6
3
191.0
43.6
-18.1
28.8
12.1
23.5
18.5
7.3
29.5
32.5
20.8
18.5
-30.3
-9.7
4
172.5
-0.7
3.5
-6.4
31.1
29.7
15.3
34.9
19.5
23.1
9.4
8.3
2.3
5
134.2
-23.5
1.9
8.1
22.9
-3.8
-2.2
-3.0
2.4
9.6
28.9
15.8
6
184.0
19.5
12.8
5.6
1.2
60.1
17.2
-29.6
-0.0
-22.4
19.1
7
104.5
-1.9
5.3
15.4
5.4
-11.1
0.5
2.6
-9.8
-13.3
8
125.3
14.4
7.7
11.3
36.8
32.3
4.3
17.5
20.3
9
190.0
17.5
0.6
26.4
45.2
37.1
-14.9
-3.0
10
143.7
20.4
6.4
3.4
22.9
32.3
-2.0
11
186.0
16.6
1.7
13.9
11.7
7.1
12
168.8
14.1
-10.5
-3.0
-8.6
13
191.0
44.7
36.8
-4.6
14
205.6
7.1
13.9
15
144.2
5.0
16
188.8
© 2003 by Chapman & Hall/CRC

Activation as shown in Table 12.9 was determined using (top) Regression
with the prior source reference functions in addition to using (middle) Gibbs
sampling and (bottom) ICM estimation methods.
TABLE 12.9
Prior, Gibbs, and ICM coeﬃcient statistics.
tReg
1
2
3
4
1
8.04, -0.95
0.38, 0.77
-1.61, 6.49
0.30, 7.02
2
2.24, 2.11
9.14, -0.04
-1.06, 1.50
0.46, 8.33
3
-1.03, 7.16
-1.20, 0.76
8.26, -0.99
1.37, 1.51
4
-0.59, 7.54
1.08, 7.74
1.13, 0.43
8.63, 0.20
zGibbs
1
2
3
4
1
11.46, -1.36
0.55, 1.12
-2.34, 9.26
0.43, 10.08
2
3.23, 3.04
13.11, -0.05
-1.52, 2.16
0.67, 11.94
3
-1.47, 10.35
-1.73, 1.09
11.89, -1.42
1.97, 2.20
4
-0.84, 10.78
1.56, 11.15
1.62, 0.61
12.34, 0.29
zICM
1
2
3
4
1
11.64, -1.37
0.55, 1.12
-2.33, 9.40
0.43, 10.16
2
3.23, 3.06
13.22, -0.05
-1.53, 2.16
0.67, 12.06
3
-1.48, 10.36
-1.74, 1.10
11.95, -1.42
1.98, 2.190
4
-0.85, 10.92
1.56, 11.20
1.63, 0.62
12.48, 0.29
The Regression activations follow Scalar Student t-distributions with n −
m−q−1 = 124 degrees of freedom which is negligibly diﬀerent than the corre-
sponding Normal distributions. As by design, positive activations for source
reference function 1 are along the diagonal from upper left to lower right and
those for source reference function 2 are on the upper right and lower left. A
threshold was set at 5 and illustrated in Figure 12.6.
The same threshold is used for the Gibbs sampling and ICM activations.
For Gibbs sampling, all diagonal activations in Figure 12.7 are present and
more pronounced than those by the standard Regression method. The acti-
vation along the diagonal of the image increased by an average of 3.62 from
the standard Regression method while those for the corners increased by an
average of 3.13. For the ICM activations in Figure 12.8, all activations are
present and more pronounced than those by the standard Regression method.
The activation along the diagonal of the image increased by an average of 5.08
from the standard Regression method while those for the corners increased
by an average of 4.03.
These functional activations are to be superimposed onto the previously
shown anatomical image.
In this example, the Bayesian statistical Source
Separation model outperformed the common method of multiple Regression
for both estimation methods and ICM was the best.
© 2003 by Chapman & Hall/CRC

FIGURE 12.6
Activations thresholded at 5 for prior reference functions.
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
8.0
9.1
8.3
8.6
(a) one
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
7.2
7.5
7.7
6.5
7.0
8.3
(b) two
FIGURE 12.7
Activations thresholded at 5 for Bayesian Gibbs reference functions.
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
11.6
13.2
12.0
12.5
(a) one
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
10.4 
10.9 
11.2
9.4
10.2
12.1
(b) two
FIGURE 12.8
Activations thresholded at 5 Bayesian ICM reference functions.
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
11.5
13.1
11.9
12.3
(a) one
−15
−10
−5
0
5
10
15
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0.5
1
1.5
2
2.5
3
3.5
4
4.5
9.3
10.1
11.9
10.3
10.8
11.2
(b) two
© 2003 by Chapman & Hall/CRC

It can be seen that the Bayesian method of determining the reference func-
tion for computation of voxel activation performed well especially with only
sixteen voxels.
12.6
Real FMRI Experiment
Due to the fact that the number of voxels is large and that the ICM and
Gibbs sampling procedures requires the inversion of the voxels large covari-
ance matrix and additionally the Gibbs sampling procedure requires a matrix
factorization, spatial independence is assumed for computational simplicity.
The Gibbs sampling procedure is also very computationally intensive and
since the ICM procedure produced nearly identical simulation results, only
the ICM procedure is implemented. The software package AFNI [7] is used
to display the results.
The current FMRI data [59] provides the motivation for using Bayesian
Source Separation to determine the true underling unobserved source refer-
ence functions. These reference functions are the underlying responses due to
the presentation of the experimental stimuli. The data were collected from
an experiment in which a subject was given eight trials of stimuli A, B, and
C. The timing and trials were exactly the same as in the simulated example.
Experimental task A was an implementation of a common experimental eco-
nomic method for determining participants’ valuation of objects in the setting
of an auction [4]. The participant was given an item and told that the item
can be kept or sold. If the item is kept, the participant retains ownership at
the end of the experiment and is paid the items stated value.
Task A consisted of the participant reading text from a screen, determin-
ing a number, and entering the number using button response unit all in 22
seconds. Task B consisted of the subject receiving feedback displayed on a
screen for 10 seconds. Task C was a control stimulus which consisted of a
blank screen for 32 seconds.
For the functional data, 24 axial slices of size 64 × 64 were taken. Each
voxel has dimensions of 3 × 3 × 5 mm. Scanning was performed using a 1.5
Tesla Siemens Magneton with TE = 40 ms. Observations were taken every
4 seconds so that there are 128 in each voxel. All hyperparameters were as-
sessed according to the Regression technique in Appendix A with an empirical
Bayes’ approach. For the prior mean, a square function was assessed with unit
amplitude as discussed in the simulation example which mimics the experi-
ment. Due to space limitations, the prior and posterior parameter values for
the 98,304 voxel’s have been omitted.
In Figure 12.9 are the prior square and ICM Bayesian source reference func-
tions corresponding to the (a) ﬁrst and (b) second source reference functions.
© 2003 by Chapman & Hall/CRC

FIGURE 12.9
Prior −· and Bayesian ICM −−reference functions.
20
40
60
80
100
120
−5
−4
−3
−2
−1
0
1
2
3
4
5
(a) ﬁrst
20
40
60
80
100
120
−5
−4
−3
−2
−1
0
1
2
3
4
5
(b) second
The activations corresponding to the ﬁrst prior square reference function
was computed and displayed in Figure 12.10 (a). It is evident that the acti-
vation in Figure 12.10 (a) is buried in the noise. The threshold is set at 1.885
and if raised, the activation begins to disappear while noise remains.
The activations corresponding to the ﬁrst Bayesian ICM reference function
was computed and displayed in Figure 12.10 (b). It is evident that the acti-
vation in Figure 12.10 (b) is larger and is no longer buried in the noise. The
activations stand out above the noise. The threshold is set at 12.63 and if
raised, the activation begins to disappear.
The activations corresponding to the second prior square reference function
was computed and displayed in Figure 12.11(a). It is evident that the activa-
tion in Figure 12.11 (a) is buried in the noise. The threshold is set at 3.205
and if raised, the activation begins to disappear while noise remains.
The activations corresponding to the second ICM Bayesian square reference
function was computed and displayed in Figure 12.11 (b). It is evident that
the activation in Figure 12.11 (b) is much larger and is no longer buried in
the noise. Further, the activation is more localized. The activations stand
out above the noise. The threshold is set at 20.58 and if raised, the activation
begins to disappear.
The activations that were computed using the underlying reference func-
tions from Bayesian Source Separation were much larger and more distinct
than those using the prior square reference functions.
© 2003 by Chapman & Hall/CRC

FIGURE 12.10
Activations for ﬁrst reference functions.
(a) prior thresholded at 1.885
(b) Bayesian thresholded at 12.63
FIGURE 12.11
Activations for second reference functions.
(a) prior thresholded at 3.205
(b) Bayesian thresholded at 20.58
12.7
FMRI Conclusion
In computing the activations in FMRI, the choice of the source reference
function is subjective. It has been shown that the reference function need
not be assigned but may be determined statistically using Bayesian methods.
A dynamic (nonstatic or ﬁxed) source reference function can be determined
for each FMRI participant. It was further found in the simulation example,
© 2003 by Chapman & Hall/CRC

that when computing activations, the iterated conditional modes and Gibbs
sampling algorithms performed similarly.
© 2003 by Chapman & Hall/CRC

Part III
Generalizations
© 2003 by Chapman & Hall/CRC

13
Delayed Sources and Dynamic Coeﬃcients
13.1
Introduction
In Part II, the mixing of the sources was speciﬁed to be instantaneous and
the Regression/mixing coeﬃcients were speciﬁed to be constant over time.
In this Chapter, the sources are allowed to be delayed through the Regres-
sion/mixing coeﬃcients and the Regression/mixing coeﬃcients are allowed
to change over time. Delayed sources and nonconstant or dynamic mixing
coeﬃcients take into account the facts that speakers at a cocktail party are
a physical distance from the microphones, thus the sound from them taking
time to travel to the various microphones at diﬀerent distances, and that the
speakers at the party may be moving around.
13.2
Model
The observation and source vectors xi and si are stacked into single vectors
x and s which are np × 1 and nm × 1 respectively. The Source Separation
model is written
(x|B,u,L,s) =
B
u
+
L
s
+
ǫ,
(np×1)
[np×n(q +1)] [n(q +1)×1]
(np×nm) (nm×1)
(np×1)
(13.2.1)
where the observation vector x, the observed source vector u, the unobserved
source vector s, and the error vector ǫ given by
x =



x1
...
xn


,
u =



u1
...
un


,
s =



s1
...
sn


,
ǫ =



ǫ1
...
ǫn


,
(13.2.2)
have been deﬁned.
The matrices B and L are generalized Regression and
mixing coeﬃcients. The vectors x, u, s, and ǫ contain each of the observation,
© 2003 by Chapman & Hall/CRC

observed source, unobserved source, and error vectors stacked in time order
into single vectors.
Taking a closer look at the general mixing matrix L (similarly for B), it
is evident that it can be partitioned into p × m blocks and has the lower
triangular form
L =








L11
0
···
0
L21 L22
...
...
...
0
Ln1
···
Lnn








,
(13.2.3)
where the blocks above the diagonal are p×m zero matrices and each Lii′ are
p×m mixing matrices. Only blocks below the diagonal are nonzero because
only current and past sources that are delayed can enter into the mixing and
not future sources. Upon multiplying the observed source vector u by the
generalized Regression coeﬃcient matrix B and the observed source vector s
by the generalized mixing coeﬃcient matrix L or upon mixing the observed
and unobserved sources, the observed mixed signal vectors are
xi = i
i′=1 (Bii′ui′ +Lii′si′)+ǫi.
(13.2.4)
The matrix Lii is the instantaneous mixing matrix at time i and the ma-
trix Li,(i−d) is the mixing matrix at time i for signals delayed d time units.
The same is true for the generalized Regression coeﬃcient matrix B for the
observed sources.
For example, assume that there are m = 2 unobserved speakers’ and p = 1
microphones. Let Ds be a matrix of time delays for the unobservable sources
where djk is element (j,k) indicating the delay of unobserved source k to
microphone j. The general mixing matrix L for the case where the delays
are known to be described by Ds = (0,2) and the mixing process for the
unobserved sources which is allowed to vary over time is
L =











L11
0
0
···
0
0
L22
0
L31
0
L33
0
...
0
L42
0
L44
0
...
0
0
L53
0
L55 0
...
...











,
(13.2.5)
where at time increment i, Lii = (lii,0) indicating that the signal from speaker
1 is instantaneously mixed and the signal from speaker 2 has not yet reached
the microphone (thus not mixed), Li,(i−1) = (0,0) indicating that neither of
the speakers signals are delayed 1 time increment, and Li,(i−2) = (0,li,(i−2))
© 2003 by Chapman & Hall/CRC

indicating that the signal from speaker 2 takes 2 time units to enter the mixing
process, Lii′ = 0 for (i −i′) > 2. The same procedure for delayed mixing of
observed sources is true for the generalized Regression coeﬃcient matrix B.
The above very general delayed nonconstant mixing model can be divided
into three general cases. The ﬁrst is a delayed constant mixing process, the
second is a delayed nonconstant mixing process, and the third is an instanta-
neous nonconstant mixing process.
13.3
Delayed Constant Mixing
It is certainly reasonable that in many instances a delayed nonconstant
mixing process can be well approximated by a delayed constant one over
“short” periods of time. The mixing matrix L (similarly for B) for the delayed
constant mixing process is given by the matrix with n row and column blocks
L =











L0 0
0 ···
0
L1 L0 0
L2 L1 L0 0
...
L3 L2 L1 L0 0
...
L4 L3 L2 L1 L0 0
...
...











,
(13.3.1)
where Ld, d = 0,1,2,...,n −1 describes the delayed mixing process at d time
increments. The observed mixed vectors xi at time i are represented as linear
combinations of the observed and unobserved source vectors ui′ and si′ mul-
tiplied by the appropriate Regression and mixing coeﬃcient matrices Bi′−1
and Li′−1 (where i′ ranges from 1 to i) plus a random error vector ǫi which
is given by
xi = i
i′=1(Bi′−1 ui′ + Li′−1 si′) + ǫi.
(13.3.2)
In the above example with (m = 2) two unobserved speakers and (p = 1)
one microphone, the generalized mixing matrix L for the unobservabe sources
with a delayed constant mixing process is given by
© 2003 by Chapman & Hall/CRC

L =











L0 0
0 ···
0
L1 L0 0
L2 L1 L0 0
...
0 L2 L1 L0 0
...
0
0 L2 L1 L0 0
...
...











.
(13.3.3)
Note that at any given time increment i, a given source sik enters into
the mixing through only one mixing coeﬃcient lk in Lk, k = 1,...,m. For the
above example with p = 1 microphone and m = 2 speakers, the general mixing
matrix L has the form
L =











(l1,0)
0
0
···
0
(0,0) (l1,0)
0
(0,l2) (0,0) (l1,0)
0
...
0
(0,l2) (0,0) (l1,0)
0
...
0
0
(0,l2) (0,0) (l1,0) 0
...
...











,
(13.3.4)
(similarly for B) and the contribution of the unobserved source signals to the
observed mixed signals is
Ls =












(l1,l2)(s1,1,s1−2,2)′
(l1,l2)(s2,1,s2−2,2)′
(l1,l2)(s3,1,s3−2,2)′
...
(l1,l2)(si,1,si−2,2)′
...
(l1,l2)(sn,1,sn−2,2)′












,
(13.3.5)
where s1−2,2 and s2−2,2 are zero. With general delays, the elements are
(l1,l2)′(si−d11,1,si−d12,2)′,
(13.3.6)
where djk is the delay of source k to microphone j.
© 2003 by Chapman & Hall/CRC

13.4
Delayed Nonconstant Mixing
The mixing matrix L for the delayed nonconstant mixing process can be
divided a part that contains the mixing coeﬃcients In ⊗Λ and a part that
contains the delays Ps as L = (In ⊗Λ)Ps. The part that contains the delays Ps
is a permutation-like matrix. For the example, the permutation-like matrix
Ps
Ps =















1 0 0 0 0 0 0 0 0 0 · 0
0 0 0 0 0 0 0 0 0 0 · 0

0 0 1 0 0 0 0 0 0 0 · 0
0 0 0 0 0 0 0 0 0 0 · 0

0 0 0 0 1 0 0 0 0 0 · 0
0 1 0 0 0 0 0 0 0 0 · 0

0 0 0 0 0 0 1 0 0 0 · 0
0 0 0 1 0 0 0 0 0 0 · 0
· · · · · · · · · · · ·














,
(13.4.1)
where for the ith set of m rows, the rows are shifted (from the identity matrix)
kdk columns to the left, where k is the row number, m is the number of sources,
and dk is the delay for the kth source, so that
Pss =















s11
0
s21
0
s31
s32
s41
s42
...















.
(13.4.2)
This can easily be generalized to p microphones and m speakers.
With the generalized mixing coeﬃcient matrix L written as (In ⊗Λ)Ps and
the generalized Regression coeﬃcient matrix B written as (In ⊗B)Pu, the
linear synthesis model becomes
(x|B,u,Λ,s) =
(In ⊗B)
Puu
+ (In ⊗Λ)
Pss
+
ǫ,
(np×1)
[np×n(q +1)] [n(q +1)×1]
(np×nm) (nm×1)
(np×1)
(13.4.3)
where Pu and Ps are [n(q+1)×n(q+1)] and (nm×nm) matrices respectively.
If the delays were known or speciﬁed to be well explained by a given constant
delayed process, then the delayed constant mixing process is the instantaneous
constant one where Pss is generically replaced by s, Puu by u, and written as
© 2003 by Chapman & Hall/CRC

(x|B,u,Λ,s) =
(In ⊗B)
u
+ (In ⊗Λ)
s
+
ǫ.
(np×1)
[np×n(q +1)] [n(q +1)×1]
(np×nm) (nm×1)
(np×1)
(13.4.4)
Then the model can be written in the matrix formulation
(X|U,B,Λ,S) =
U
B′
+
S
Λ′
+
E,
(n×p)
[n×(q +1)] [(q +1)×p]
(n×m) (m×p)
(n×p)
(13.4.5)
and also the combined formulation
(X|C,Z) =
Z
C′
+
E,
(n×p)
n×(m+q +1) (m+q +1)×p
(n×p)
(13.4.6)
which is the previously model described from Part II.
The matrix of sources for the previous example with two sources and one
microphone is
S =







s11 0
s21 0
s31 s32
s41 s42
...
...







.
(13.4.7)
For this model, the parameters are estimated as before; then the columns
of U and S are shifted down by the appropriate amounts. If the delays were
unknown, an instantaneous constant mixing process could be assumed. Then
the estimates of the sources s would actually be Pss and similarly for the
mixing process for Puu, observed sources u. The mixing process is estimated
correctly except for shifts in the columns of U and S.
A model could constrain the appropriate number of rows in a column to
be zero and shift the columns, but this is not necessary with the current
robust model. Shifts are not necessary because we incorporate our prior be-
liefs regarding the values of the sources and the shifts by assessing a prior
mean for the sources S0 with the appropriate number of zeros in each column
representing our prior belief regarding the sources delays.
It has been shown that the delayed constant mixing process can be trans-
formed into the instantaneous constant one. The likelihood, prior distribu-
tions, joint posterior distribution, and estimation methods are as in Chap-
ter 11.
© 2003 by Chapman & Hall/CRC

13.5
Instantaneous Nonconstant Mixing
A delayed nonconstant mixing process can sometimes be approximated
by an instantaneous constant one due to the length of time it takes for a
given source component (signal from a speaker) to be observed (recorded
by the microphone).
Let r be the distance from a source (speaker) to an
observing device (microphone), v the speed that the source signal travels
in the medium (air for instance), and t be the number of time increments
per second (sampling rate).
Then, the distance a signal can be from the
recorder before there is a delay of 1 time increment is v/t. For example, if the
microphone sampled at 100 times per second with the speed of sound v = 343
m/s, then the speaker can be 3.43 m or 11.25 ft from the source before there
is a delay of 1 time increment.
The generalized mixing matrix L (similarly for the generalized Regression
coeﬃcient matrix B) for the instantaneous nonconstant mixing process is
L =











L1 0
0 ···
0
0 L2 0
0
0 L3 0
...
0
0
0 L4 0
...
0
0
0
0 L5 0
...
...











,
(13.5.1)
which yields observed mixed signals xi at time increment i of the form
xi = Biui +Lisi +ǫi
(13.5.2)
xi = Cizi +ǫi,
(13.5.3)
where Ci = (Bi,Li), zi = (u′
i,s′
i)′. For the usual instantaneous constant mixing
process of the unobserved and observed Source Separation model, described
in Part II, B = Bi and Λ = Li for all i.
The general Source Separation model describing the instantaneous noncon-
stant mixing process for all observations at all time increments is
(x|c,z) =
C
z
+
ǫ,
(np×1)
[np×n(m+q +1)] [n(m+q +1)×1]
(np×1)
(13.5.4)
where the general Regression/mixing coeﬃcient matrix C is given by
© 2003 by Chapman & Hall/CRC

C =











C1 0
0 ···
0
0 C2 0
0
0 C3 0
...
0
0
0 C4 0
...
0
0
0
0 C5 0
...
...











,
(13.5.5)
Ci = (Bi,Li), z = (z1,...,zn), and the remaining variables are as previously
deﬁned in this Chapter.
13.6
Likelihood
As in Part II, the observation vectors errors ǫi are speciﬁed to be indepen-
dent and Multivariate Normally distributed with mean zero and covariance
matrix Σ. The Multivariate Normal distributional likelihood for the instan-
taneous nonconstant model is given by
p(x|C,z,Σ) ∝|In ⊗Σ|−1
2 e−1
2 (x−Cz)′(In⊗Σ)−1(x−Cz),
(13.6.1)
which can be written by performing some algebra in the exponent and using
a determinant property of Kroneker products as
p(x|C,z,Σ) ∝|Σ|−n
2 e−1
2 trΣ−1  n
i=1(xi−Cizi)(xi−Cizi)′,
(13.6.2)
where the variables are as previously deﬁned.
To quantify available prior knowledge regarding our prior beliefs about the
model parameter values, both Conjugate and generalized Conjugate prior dis-
tributions are utilized.
13.7
Conjugate Priors and Posterior
When quantifying available prior information regarding the parameters of
interest, Conjugate prior distributions are speciﬁed as described in Chapter 4.
The joint prior distribution for the model parameters which are the matrix of
Regression/mixing coeﬃcients C, the matrix of sources S, the source covari-
ance matrix R, and the error covariance matrix Σ is given by
© 2003 by Chapman & Hall/CRC

p(S,R,C,Σ) = p(S|R)p(R)p(C|Σ)p(Σ),
(13.7.1)
where the prior distribution for the model parameters from the Conjugate
procedure outlined in Chapter 4 are given by
p(S|R) ∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′,
(13.7.2)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(13.7.3)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(13.7.4)
p(C|Σ) ∝|Σ|−n(m+q+1)
2
e−1
2 trΣ−1(C−C0)(In⊗D−1)(C−C0)′,
(13.7.5)
where the p × n(m + q + 1) matrix of coeﬃcients is C = (C1,...,Cn).
The
matrices Σ, R, Q, V , and D are positive deﬁnite. The hyperparameters S0,
ν, Q, η, V , D, and C0 are to be assessed and having done so completely
determine the joint prior distribution.
The prior distributions for the parameters are Matrix Normal for the ma-
trix of Regression/mixing coeﬃcients C, Matrix Normal for the matrix of
sources S, Inverted Wishart distributed for the source covariance matrix R,
and Inverted Wishart distributed for the error covariance matrix.
Upon using Bayes’ rule, the likelihood of the observed mixed signals and
the joint prior distribution for the unknown model parameters are combined
and their product is proportional to the joint posterior distribution
p(S,R,C,Σ|u,x) ∝|Σ|−ν+n(m+q+2)
2
e−1
2 trΣ−1G
×|R|−(n+η)
2
e−1
2 trR−1[V +(S−S0)′(S−S0)],
(13.7.6)
where the p×p matrix G has been deﬁned to be
G =
n
	
i=1
(xi −Cizi)(xi −Cizi)′ +(C −C0)(In ⊗D−1)(C −C0)′ +Q.
(13.7.7)
Note that the second term in G can be rewritten as
(C −C0)(In ⊗D−1)(C −C0)′ =
n
	
i=1
(Ci −C0i)D−1(Ci −C0i)′. (13.7.8)
This joint posterior distribution must now be evaluated in order to obtain
our parameter estimates of the matrix of sources S, the matrix of instan-
taneous nonconstant Regression/mixing coeﬃcients C, the source covariance
matrix R, and the error covariance matrix Σ. Marginal posterior mean and
joint maximum a posteriori estimates of the parameters S, R, C, and Σ are
found by the Gibbs sampling and ICM algorithms.
© 2003 by Chapman & Hall/CRC

13.8
Conjugate Estimation and Inference
With the above posterior distribution, it is not possible to obtain mar-
ginal distributions and thus marginal estimates for any of the parameters
in an analytic closed form or explicit maximum a posteriori estimates from
diﬀerentiation. It is possible to use both Gibbs sampling to obtain marginal
posterior parameter estimates and the ICM algorithm for joint modal or max-
imum a posteriori estimates. For both estimation procedures, the posterior
conditional distributions are required.
13.8.1
Posterior Conditionals
From the joint posterior distribution we can obtain the posterior condi-
tional distribution for each of the model parameters.
The conditional posterior distributions for the matrix of instantaneous non-
constant Regression/mixing coeﬃcients C is found by considering only the
terms in the joint posterior distribution which involve C and is given by
p(C|S,R,Σ,U,X) ∝p(C|Σ)p(X|S,C,Σ,U)
∝|Σ|−n(m+q+1)
2
e−1
2 trΣ−1  n
i=1(Ci−C0i)D−1(Ci−C0i)′
×|Σ|−n
2 e−1
2 trΣ−1  n
i=1(xi−Cizi)(xi−Cizi)′,
(13.8.1)
which after performing some algebra in the exponent can be written as
p(C|S,R,Σ,U,X) ∝e−1
2
 n
i=1 trΣ−1(Ci−˜Ci)(D−1+ziz′
i)(Ci−˜Ci)′, (13.8.2)
where the variable ˜C, the posterior conditional mean and mode, has been
deﬁned and is given by
˜Ci = [C0iD−1 +xiz′
i](D−1 +ziz′
i)−1
(13.8.3)
for all i, i = 1,...,n.
The posterior conditional distribution for the matrix of combined instanta-
neous nonconstant Regression/mixing coeﬃcients Ci can be written as
p(Ci|Si,R,Σ,ui,xi) ∝|Σ|−m+q+1
2
e−1
2 trΣ−1(Ci−˜Ci)(D−1+ziz′
i)(Ci−˜Ci)′. (13.8.4)
for all i, i = 1,...,n.
That is, the matrix of instantaneous nonconstant Regression/mixing Ci
coeﬃcients given the source vector si, the source covariance matrix R, the
© 2003 by Chapman & Hall/CRC

error covariance matrix Σ, the observed source vector ui, and the observed
data vector xi is Multivariate Normally distributed.
The conditional posterior distribution of the observation error covariance
matrix Σ is found by considering only the terms in the joint posterior distri-
bution which involve Σ and is given by
p(Σ|S,R,C,U,X) ∝p(Σ)p(C|Σ)p(X|Z,C,Σ)
∝|Σ|−ν
2 e−1
2 trΣ−1Q
×|Σ|−n(m+q+1)
2
e−1
2 tr  n
i=1 Σ−1(Ci−C0i)D−1(Ci−C0i)′
×|Σ|−n
2 e−1
2
 n
i=1(xi−Cizi)′Σ−1(xi−Cizi)
∝|Σ|−ν+n(m+q+2)
2
e−1
2 trΣ−1G,
(13.8.5)
where the p×p matrix G has been deﬁned to be
G =
n
	
i=1
[(xi −Cizi)(xi −Cizi)′ +(Ci −C0i)D−1(Ci −C0i)′]+Q (13.8.6)
with a mode as described in Chapter 2 given by
˜Σ =
G
ν +n(m+q +2) .
(13.8.7)
The conditional posterior distribution of the observation error covariance
matrix Σ given the matrix of sources S, the source covariance matrix R,
the matrix of instantaneous nonconstant Regression/mixing coeﬃcients C,
the matrix of observed sources U, and the matrix of data X is an Inverted
Wishart distribution.
The conditional posterior distribution for the matrix of sources S is found
by considering only the terms in the joint posterior distribution which involve
S and is given by
p(S|B,L,R,Σ,U,X) ∝p(S|R)p(X|B,L,S,Σ,U)
∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
×|Σ|−n
2 e−1
2
 n
i=1(xi−Biui−Lisi)′Σ−1(xi−Biui−Lisi),
(13.8.8)
which after performing some algebra in the exponent can be written as
p(S|B,L,R,Σ,U,X) ∝e−1
2
 n
i=1(si−˜si)′(R−1+L′
iΣ−1Li)(si−˜si), (13.8.9)
© 2003 by Chapman & Hall/CRC

where the vectors si have been deﬁned which are the posterior conditional
mean and mode as described in Chapter 2 and given by
˜si = (R−1 +L′
iΣ−1Li)−1[L′
iΣ−1(xi −Biui)+R−1s0i].
(13.8.10)
The posterior conditional distribution for the vectors of sources si can also
be written as
p(si|Bi,Li,R,Σ,ui,xi) ∝|R−1 +L′
iΣ−1Li|−1
2 e−1
2 (si−˜si)′(R−1+L′
iΣ−1Li)(si−˜si)
(13.8.11)
for all i, i = 1,...,n.
The conditional posterior distribution for the sources si given the instanta-
neous nonconstant Regression coeﬃcients Bi, the instantaneous nonconstant
mixing coeﬃcients Li, the source covariance matrix R, the error covariance
matrix Σ, the observed source vector ui, and the vector of data xi is Multi-
variate Normally distributed.
The conditional posterior distribution for the source covariance matrix R is
found by considering only the terms in the joint posterior distribution which
involve R and is given by
p(R|C,S,Σ,U,X) ∝p(R)p(S|R)p(X|B,L,S,Σ,U)
∝|R|−η
2 e−1
2 trR−1V |R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
∝|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(13.8.12)
with the posterior conditional mode as described in Chapter 2 given by
˜R = (S −S0)′(S −S0)+V
n+η
.
(13.8.13)
The conditional posterior distribution for the source covariance matrix R
given the instantaneous nonconstant matrix of Regression/mixing coeﬃcients
C, the matrix of sources S, the error covariance matrix Σ, the matrix of
observed sources U, and the matrix of data X is Inverted Wishart distributed.
13.8.2
Gibbs Sampling
To ﬁnd the marginal posterior mean estimates of the parameters from the
joint posterior distribution using the Gibbs sampling algorithm, start with
initial values for the matrix of sources S and error covariance matrix Σ, say
¯S(0) and ¯Σ(0), and then cycle through
¯C = a random variate from p(C| ¯S(l), ¯R(l), ¯Σ(l),U,X)
© 2003 by Chapman & Hall/CRC

¯Ci,(l+1) = ACiYCiB′
Ci +MCi,
i = 1,...,n,
(13.8.14)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l), ¯C(l+1),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(13.8.15)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1),U,X)
= AR(Y ′
RYR)−1A′
R,
(13.8.16)
¯S(l+1) = a random variate from p(S| ¯C(l+1), ¯Σ(l+1), ¯R(l+1),U,X),
si,(l+1) = AsiYsi +Msi,
i = 1,...,n,
(13.8.17)
where
ACiA′
Ci = ¯Σ(l),
BCiB′
Ci = (D−1 + ¯zi,(l)¯z′
i,(l))−1,
¯zi,(l) = (u′
i,(l),¯s′
i,(l))′,
MCi = [C0iD−1 +xi¯z′
i,(l)](D−1 + ¯zi,(l)¯z′
i,(l))−1,
AΣA′
Σ =
n

i=1
[(xi −¯Ci,(l+1)¯zi,(l))(xi −¯Ci,(l+1)¯zi,(l))′
+( ¯Ci,(l+1) −C0i)D−1( ¯Ci,(l+1) −C0i)′]+Q,
ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
AsiA′
si = ( ¯R−1
(l+1) + ¯L′
i,(l+1) ¯Σ
−1
(l+1) ¯Li,(l+1))−1,
Msi = ( ¯R−1
(l+1) + ¯L′
i,(l+1) ¯Σ
−1
(l+1) ¯Li,(l+1))−1
×[ ¯L′
i,(l+1) ¯Σ
−1
(l+1)(xi −¯Bi,(l+1)ui)+ ¯R−1
(l+1)s0i],
while YCi, YΣ, YR, and Ysi are p ×(m +q +1), (ν +n(m +q + 2) +p +1) ×p,
(n+η+m+1)×m, and (m×1)-dimensional matrices respectively, whose ele-
ments are random variates from the standard Scalar Normal distribution. The
formulas for the generation of random variates from the conditional posterior
distributions are easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters. In the above, C = (C1,...,Cn). Exact sampling-based estimates
of other quantities can also be found. Similar to Bayesian Regression and
Bayesian Factor Analysis there is interest in the estimate of the marginal pos-
terior variance of the matrix containing the Regression and mixing coeﬃcients
© 2003 by Chapman & Hall/CRC

var(c|X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′,
= ¯∆,
where c = vec(C) and ¯c = vec( ¯C).
The covariance matrices of the other parameters follow similarly.
13.8.3
Maximum a Posteriori
The joint posterior distribution can also be jointly maximized with respect
to the matrix of instantaneous nonconstant coeﬃcients C, the error covariance
matrix Σ, the matrix of sources S, and the source covariance matrix R by using
the ICM algorithm. To maximize the joint posterior distribution using the
ICM algorithm, start with an initial value for the matrix of sources S, say
˜S(0), and then cycle through
˜C(l+1) =
Arg Max
C
p(C| ˜S(l), ˜R(l), ˜Σ(l),X,U),
˜Ci,(l+1) = (D−1 + ˜zi,(l)˜z′
i,(l))−1[D−1C0i + ˜zi,(l)x′
i],
i = 1,...,n,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ¯C(l+1), ˜R(l), ˜S(l),X,U)
=
n

i=1
[(xi −˜Ci,(l+1)˜zi,(l))(xi −˜Ci,(l+1)˜zi,(l))′
+ ( ˜Ci,(l+1) −C0i)D−1( ˜Ci,(l+1) −C0i)′]+Q]/[ν +n(m+q +2)],
˜R(l+1) =
Arg Max
R
p(R| ˜S(l), ¯C(l+1), ˜Σ(l+1),X,U)
= ( ˜S(l) −S0)′( ˜S(l) −S0)+V
n+η
,
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l+1), ˜Σ(l+1),X,U),
˜si,(l+1) = ( ˜R−1
(l+1) + ˜L′
i,(l+1) ˜Σ
−1
(l+1) ˜Li,(l+1))−1
×[ ˜L′
i,(l+1) ˜Σ
−1
(l+1)(xi −˜Bi,(l+1)ui)+ ˜R−1
(l+1)s0i],
i = 1,...,n,
where the vector ˜zi = (u′
i,˜s′
i)′ has been deﬁned, until convergence is reached.
The converged values ( ˜S, ˜R, ˜C, ˜Σ) are joint posterior modal (maximum a poste-
riori) estimators of the parameters. Conditional maximum a posteriori vari-
ance estimates can also be found.
The conditional modal variance of the
matrix containing the Regression and mixing coeﬃcients is
var(Ci|˜Ci, ˜S, ˜R, ˜Σ,X,U) = ˜Σ⊗(D−1 + ˜zi˜z′
i)−1,
i = 1,...,n
© 2003 by Chapman & Hall/CRC

or equivalently
var(ci|˜ci, ˜S, ˜R, ˜Σ,X,U) = (D−1 + ˜zi˜z′
i)−1 ⊗˜Σ,
i = 1,...,n
= ˜∆i,
where ci = vec(Ci), ˜c, ˜S, ˜R, and ˜Σ are the converged value from the ICM
algorithm.
To determine statistical signiﬁcance with the ICM approach, use the con-
ditional distribution of the matrix containing the Regression and mixing co-
eﬃcients which is
p(Ci|˜Ci, ˜S, ˜R, ˜Σ,X,U) ∝|D−1 + ˜zi˜z′
i|
1
2 |˜Σ|−1
2 e−1
2 tr ˜Σ−1(Ci−˜Ci)(D−1+˜zi ˜z′
i)(Ci−˜Ci)′.
(13.8.18)
That is,
Ci|˜Ci, ˜S, ˜R, ˜Σ,X,U ∼N

˜Ci, ˜Σ⊗(D−1 + ˜zi˜z′
i)−1
.
(13.8.19)
General simultaneous hypotheses can be evaluated regarding the entire ma-
trix containing the Regression/mixing coeﬃcients, a submatrix, or the coeﬃ-
cients of a particular independent variable or source, or an element by com-
puting marginal conditional distributions.
It can be shown [17, 41] that the marginal conditional distribution of any
column of the matrix containing the Regression and mixing coeﬃcients Ci,
Ci,k is Multivariate Normal
p(Ci,k|˜Ci,k, ˜S, ˜Σ,U,X) ∝|Wi,kk ˜Σ|−1
2 e−1
2 (Ci,k−˜Ci,k)′(Wi,kk ˜Σ)−1(Ci,k−˜Ci,k),
(13.8.20)
where Wi = (D−1 + ˜zi˜z′
i)−1 and Wi,kk is its kth diagonal element.
With the marginal distribution of a column of Ci, signiﬁcance can be de-
termined for a particular independent variable or source. Signiﬁcance can be
determined for a subset of coeﬃcients by determining the marginal distrib-
ution of the subset within Ci,k which is also Multivariate Normal. With the
subset being a singleton set, signiﬁcance can be determined for a particular
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Ci,kj|˜Ci,kj, ˜S, ˜Σjj,U,X) ∝(Wi,kk ˜Σjj)−1
2 e
−
(Ci,kj−˜Ci,kj)2
2Wi,kk ˜Σjj
,
(13.8.21)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Ci,kj = ˜ci,jk and that
z = (Ci,kj −˜Ci,kj)
,
Wi,kk ˜Σjj
(13.8.22)
© 2003 by Chapman & Hall/CRC

follows a Normal distribution with a mean of zero and variance of one.
13.9 
Generalized Priors and Posterior
Generalized Conjugate prior distributions are assessed in order to quantify
available prior information regarding values of the model parameters. The
joint prior distribution for the sources S, the source covariance matrix R,
the vector of instantaneous nonconstant Regression/mixing coeﬃcients c =
vec(C), and the error covariance matrix Σ is given by
p(S,R,Σ,c) = p(S|R)p(R)p(Σ)p(c),
(13.9.1)
where the prior distribution for the parameters from the generalized Conjugate
procedure outlined in Chapter 4 are as follows
p(S|R) ∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′,
(13.9.2)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(13.9.3)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(13.9.4)
p(c) ∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(13.9.5)
where c = vec(C), R, Σ, Q, V , and ∆are positive deﬁnite matrices. The
hyperparameters S0, ν, Q, η, V , ∆, and c0 are to be assessed. Upon assessing
the hyperparameters, the joint prior distribution is completely determined.
The prior distribution for the matrix of sources S is Matrix Normally dis-
tributed, the prior distribution for the source vector covariance matrix R is
Inverted Wishart distributed, the vector of combined Regression/mixing co-
eﬃcients c = vec(C), C = (B,Λ) is Multivariate Normally distributed, and
the prior distribution for the error covariance matrix Σ is Inverted Wishart
distributed.
Using Bayes’ rule the joint posterior distribution for the unknown parame-
ters with generalized Conjugate prior distributions for the model parameters
is given by
p(c,S,R,Σ|U,X) = p(S|R)p(Σ)p(R)p(c)p(X|Z,C,Σ),
(13.9.6)
which is
p(c,S,R,Σ|u,x) ∝|Σ|−(n+ν)
2
|R|−(n+η)
2
e−1
2 trΣ−1Ge−1
2 (c−c0)′∆−1(c−c0)
×e−1
2 trR−1[V +(S−S0)′(S−S0)],
(13.9.7)
© 2003 by Chapman & Hall/CRC

where the p×p matrix G has been deﬁned to be
G =
n

i=1
(xi −Cizi)(xi −Cizi)′ +Q
(13.9.8)
after inserting the prior distributions and the likelihood.
This joint posterior distribution must now be evaluated in order to ob-
tain estimates of the sources S, the vector of instantaneous nonconstant Re-
gression/mixing coeﬃcients c, the source covariance matrix R, and the error
covariance matrix Σ.
13.10
Generalized Estimation and Inference
With the above joint posterior distribution that uses generalized Conjugate
prior distributions, it is not possible to obtain marginal distributions and thus
marginal estimates for all or any of the parameters in an analytic closed form
or explicit maximum a posteriori estimates from diﬀerentiation. It is possible
to use both Gibbs sampling, to obtain marginal mean parameter estimates and
the ICM algorithm for maximum a posteriori estimates. For both estimation
procedures, the posterior conditional distributions are required.
13.10.1
Posterior Conditionals
Both the Gibbs sampling and ICM algorithms require the posterior conditional
distributions.
Gibbs sampling requires them for the generation of random
variates while the ICM algorithm requires them for maximization by cycling
through their modes or maxima.
The conditional posterior distribution of the observation error covariance
matrix Σ is found by considering only those terms in the joint posterior dis-
tribution which involve Σ and is given by
p(Σ|S,R,C,U,X) ∝p(Σ)p(X|S,C,Σ,U)
∝|Σ|−ν
2 e−1
2 trΣ−1Q
×|Σ|−n
2 e−1
2
Pn
i=1(xi−Cizi)′Σ−1(xi−Cizi)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1G,
(13.10.1)
where the p×p matrix G has been deﬁned to be
© 2003 by Chapman & Hall/CRC

G =
n

i=1
(xi −Cizi)(xi −Cizi)′ +Q
(13.10.2)
and the mode of this posterior conditional distribution is as described in
Chapter 2 and is given by
˜Σ =
G
n+ν .
(13.10.3)
The posterior conditional distribution of the observation error covariance
matrix Σ given the matrix of sources S, the source covariance matrix R, the
matrix of instantaneous nonconstant Regression/mixing coeﬃcients C, the
matrix of observable sources U, and the matrix of data X is an Inverted
Wishart distribution.
The conditional posterior distribution for the matrix of sources S is found by
considering only those terms in the joint posterior distribution which involve
S and is given by
p(S|B,R,L,Σ,U,X) ∝p(S|R)p(X|B,L,S,Σ,U)
∝|R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
×|Σ|−n
2 e−1
2
Pn
i=1(xi−Biui−Lisi)′Σ−1(xi−Biui−Lisi)
∝e−1
2
Pn
i=1(si−˜si)′(R−1+L′
iΣ−1Li)(si−˜si),
(13.10.4)
where the vector ˜si has been deﬁned to be
˜si = (R−1 +L′
iΣ−1Li)−1[L′
iΣ−1(xi −Biui)+R−1s0i]
(13.10.5)
which is the posterior conditional mean and mode.
The posterior conditional distribution for the vectors of sources si can also
be written as
p(si|Bi,Li,R,Σ,ui,xi) ∝|R−1 +L′
iΣ−1Li|−1
2
×e−1
2 (si−˜si)′(R−1+L′
iΣ−1Li)(si−˜si) (13.10.6)
for all i, i = 1,...,n.
The conditional posterior distribution for the sources si given the instanta-
neous nonconstant Regression coeﬃcients Bi, the instantaneous nonconstant
mixing coeﬃcients Li, the source covariance matrix R, the error covariance
matrix Σ, the observed source vector ui, and the vector of data xi is Multi-
variate Normally distributed.
© 2003 by Chapman & Hall/CRC

The conditional posterior distribution for the source covariance matrix R
is found by considering only those terms in the joint posterior distribution
which involve R and is given by
p(R|B,L,S,Σ,U,X) ∝p(R)p(S|R)p(X|B,L,S,Σ,U)
∝|R|−η
2 e−1
2 trR−1V |R|−n
2 e−1
2 tr(S−S0)R−1(S−S0)′
∝|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′(S−S0)+V ],
(13.10.7)
with the posterior conditional mode as described in Chapter 2 given by
˜R = (S −S0)′(S −S0)+V
n+η
.
(13.10.8)
The conditional posterior distribution for the source covariance matrix R
given the matrix of sources S, the instantaneous nonconstant Regression co-
eﬃcients B, the matrix of instantaneous nonconstant mixing coeﬃcients L,
the error covariance matrix Σ, the matrix of observable sources U, and the
matrix of data X is Inverted Wishart distributed.
The conditional posterior distribution of the vector of instantaneous non-
constant Regression/mixing coeﬃcients is found by considering only those
terms in the joint posterior distribution which involve c, C, or C and is given
by
p(c|S,R,Σ,U,X) ∝p(c)p(X|C,S,Σ,U)
∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−n
2 e−1
2
Pn
i=1(xi−Cizi)′Σ−1(xi−Cizi), (13.10.9)
which after performing some algebra in the exponent can be written as
p(c|S,R,Σ,U,X) ∝e−1
2 (c−˜c)′(∆−1+I⊗Σ−1)(c−˜c),
(13.10.10)
where the vector ˜c has been deﬁned to be
˜c = (∆−1 +I ⊗Σ−1)−1(∆−1c0 +I ⊗Σ−1ˆc),
(13.10.11)
and the vector ˆc has been deﬁned by ﬁrst deﬁning ˆC to be
ˆC = (ˆc′
1,...,ˆc′
n)′,
(13.10.12)
where
ˆci = xiz′
i(ziz′
i)−1
(13.10.13)
© 2003 by Chapman & Hall/CRC

and then ˆc = vec( ˆC). That is, the vector of instantaneous nonconstant Regres-
sion/mixing coeﬃcients c given the matrix of sources S, the source covariance
matrix R, the error covariance matrix Σ, the matrix of observed sources U,
and the matrix of data X is Multivariate Normally distributed.
13.10.2
Gibbs Sampling
To ﬁnd marginal mean estimates of the parameters from the joint posterior
distribution using the Gibbs sampling algorithm, start with initial values for
the matrix of sources S and the error covariance matrix Σ, say ¯S(0) and ¯Σ(0),
and then cycle through
¯c(l+1) = a random variate from p(c| ¯S(l), ¯R(l), ¯Σ(l),U,X)
= AcYc +Mc,
(13.10.14)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l), ¯C(l+1),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(13.10.15)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1),U,X)
= AR(Y ′
RYR)−1A′
R,
(13.10.16)
¯S(l+1) = a random variate from p(S| ¯R(l+1), ¯C(l+1), ¯Σ(l+1),U,X)
si,(l+1) = AsiYsi +Msi,
(13.10.17)
where
AcA′
c = (∆−1 +I ⊗¯Σ−1
(l) )−1,
Mc = (∆−1 +I ⊗¯Σ−1
(l) )−1(∆−1c0 +I ⊗¯Σ−1
(l) ˆc(l)),
ˆci,(l) = xi¯z′
i,(l)(¯zi,(l)¯zi,(l))−1, i = 1,...,n,
ˆc(l) = (ˆc1,(l),...,ˆci,(l))′,
AΣA′
Σ =
n

i=1
[(xi −¯Ci,(l+1)¯zi,(l))(xi −¯Ci,(l+1)¯zi,(l))′ +Q,
ARA′
R = ( ¯S(l) −S0)′( ¯S(l) −S0)+V,
AsiA′
si = ( ¯R−1
(l+1) + ¯L′
i,(l+1) ¯Σ
−1
(l+1) ¯Li,(l+1))−1,
Msi = ( ¯R−1
(l+1) + ¯L′
i,(l+1) ¯Σ
−1
(l+1) ¯Li,(l+1))−1
×[ ¯L′
i,(l+1) ¯Σ
−1
(l+1)(xi −¯Bi,(l+1)ui)+ ¯R−1
(l+1)s0i],
while Yc, YΣ, YR, and Ysi are np(m+q +1)×1, (ν +n(m+q +2)+p+1)×p,
(n+η+m+1)×m, and (m×1)-dimensional matrices respectively, whose ele-
ments are random variates from the standard Scalar Normal distribution. The
formulas for the generation of random variates from the conditional posterior
© 2003 by Chapman & Hall/CRC

distributions is easily found from the methods in Chapter 6. The formulas for
the generation of random variates from the conditional posterior distributions
are easily found from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of each of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯c = 1
L
L

l=1
¯c(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters. Exact sampling-based estimates of other quantities can also be
found. Similar to Bayesian Regression and Bayesian Factor Analysis, there
is interest in the estimate of the marginal posterior variance of the matrix
containing the Regression and mixing coeﬃcients
var(c|X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C) and ¯c = vec( ¯C).
The covariance matrices of the other parameters follow similarly.
13.10.3
Maximum a Posteriori
The distribution can be maximized with respect to vector of instantaneous
nonconstant Regression/mixing coeﬃcients c, the matrix of sources S, the
source covariance matrix R, and the error covariance Σ using the ICM al-
gorithm. To jointly maximize the posterior distribution using the ICM al-
gorithm, start with initial values for the matrix of sources ˜S, and the error
covariance matrix Σ, say ˜S(0), and ˜Σ(0), and then cycle through
˜c(l+1) =
Arg Max
c
p(c| ˜S(l), ˜R(l), ˜Σ(l),X,U),
ˆci,(l) = xi˜z′
i,(l)(˜zi,(l)˜zi,(l))−1, i = 1,...,n,
ˆc(l) = (ˆc1,(l),...,ˆci,(l))′,
˜c(l+1) =

∆−1 +I ⊗˜Σ−1
(l)
−1 
∆−1c0 +

I ⊗˜Σ−1
(l)

ˆc(l)

,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜R(l), ˜S(l),X,U)
=
n
i=1(xi −˜Ci,(l+1)˜zi,(l))(xi −˜Ci,(l+1)˜zi,(l))′ +Q
n+ν
,
˜R(l+1) =
Arg Max
R
p(R| ˜S(l), ˜C(l+1), ˜Σ(l+1),X,U)
© 2003 by Chapman & Hall/CRC

= ( ˜S(l) −S0)′( ˜S(l) −S0)+V
n+η
,
˜S(l+1) =
Arg Max
S
p(S| ˜C(l+1), ˜R(l+1), ˜Σ(l+1),X,U),
˜si,(l+1) = ( ˜R−1
(l+1) + ˜L′
i,(l+1) ˜Σ−1
(l+1) ˜L(l+1))−1
×[ ˜L′
i,(l+1) ˜Σ−1
(l+1)(xi −˜Bi,(l+1)ui)+ ˜R−1
(l+1)s0i], i = 1,...,n
until convergence is reached. The converged values ( ˜S, ˜R,˜c, ˜Σ) are joint poste-
rior modal (maximum a posteriori) estimates of the unknown model parame-
ters. Conditional maximum a posteriori variance estimates can also be found.
The conditional modal variance of the matrix containing the Regression and
mixing coeﬃcients is
var(c|˜c, ˜S, ˜R, ˜Σ,X,U) = [∆−1 + ˜Z′ ˜Z ⊗˜Σ]−1
= ˜∆,
where c = vec(C), while ˜S, ˜R, and ˜Σ are the converged value from the ICM
algorithm.
Conditional modal intervals may be computed by using the conditional
distribution for a particular parameter given the modal values of the others.
13.11
Interpretation
Although the main focus after having performed a Bayesian Source Sep-
aration is the separated sources, there are others. One focus as in Bayesian
Regression is on the estimate of the Regression coeﬃcient matrix B which de-
ﬁnes a “ﬁtted” line. Coeﬃcients are evaluated to determine whether they are
statistically “large” meaning that the associated independent variable con-
tributes to the dependent variable or statistically “small” meaning that the
associated independent variable does not contribute to the dependent variable.
The coeﬃcient matrix also has the interpretation that if all of the indepen-
dent variables were held ﬁxed except for one uij which if increased to u∗
ij, the
dependent variable xij increases to an amount x∗
ij given by
x∗
ij = βi0 +···+βiju∗
ij +···+βiquiq.
(13.11.1)
Another focus after performing a Bayesian Source Separation is in the esti-
mated mixing coeﬃcients. The mixing coeﬃcients are the amplitudes which
determine the relative contribution of the sources. A particular mixing co-
eﬃcient which is relatively “small” indicates that the corresponding source
does not signiﬁcantly contribute to the associated observed mixed signal. If
© 2003 by Chapman & Hall/CRC

a particular mixing coeﬃcient is relatively “large,” this indicates that the
corresponding source does signiﬁcantly contribute to the associated observed
mixed signal.
A useful way to visualize the changing mixing coeﬃcients is to plot their
value over time for each of the sources.
13.12
Discussion
Returning to the cocktail party problem, the matrix of Regression coeﬃ-
cients B where B = (µ,B⋆) contains the matrix of mixing coeﬃcients B⋆for
the observed conversation (sources) U, and the population mean µ which is a
vector of the overall background mean level at each microphone.
After having estimated the model parameters, the estimates of the sources
as well as the mixing matrix are now available.
The estimated matrix of
sources corresponds to the unobservable signals or conversations emitted from
the mouths of the speakers at the cocktail party. Row i of the estimated source
matrix is the estimate of the unobserved source vector at time i and column j
of the estimated source matrix is the estimate of the unobserved conversation
of speaker j at the party for all n time increments.
© 2003 by Chapman & Hall/CRC

Exercises
1. Show that in the instantaneous nonconstant mixing process model for
both Conjugate and generalized Conjugate prior distributions that by
setting Bi = B and Ci = Λ, the resulting model is the unobservable/observable
Source Separation model of Chapter 11.
2. Derive a model in which up to time i0 the Regression and mixing coeﬃ-
cients are constant and after time i0, they are also constant but diﬀerent.
Assume an instantaneous constant mixing process.
© 2003 by Chapman & Hall/CRC

14
Correlated Observation and Source Vectors
14.1 
Intro duction
In Part II, the observed mixed source as well as the observed and unob-
served source vectors were speciﬁed to allow correlation within the vectors at
a given time, but to be uncorrelated over time. In the previous Chapter, the
sources were allowed to be delayed; in addition, the Regression and mixing
coeﬃcients were allowed to change over time. In this Chapter, the observed
mixed vectors as well as the unobserved source vectors are allowed to be cor-
related over time; in addition, the mixing is assumed to be instantaneous and
constant over time.
14.2 
Mo del
Just as in the previous Chapter, the observed mixed vectors, the observed
source vectors, and unobserved source vectors are stacked into singleton vec-
tors and the correlated vector Source Separation model is written as
(x|B,u,Λ,s) =
(In ⊗B)
u
+ (In ⊗Λ)
s
+
ǫ,
(np×1)
[np×n(q +1)] [n(q +1)×1]
(np×nm) (nm×1)
(np×1)
(14.2.1)
where the vector of observations x, the vector of observed sources u, the vector
of unobserved sources s, and the vector of errors ǫ are
x =



x1
...
xn


,
u =



u1
...
un


,
s =



s1
...
sn


,
ǫ =



ǫ1
...
ǫn


,
(14.2.2)
B and Λ are Regression and mixing coeﬃcients respectively, and an instanta-
neous constant mixing process has been assumed.
The errors of observation ǫ are speciﬁed to have distribution p(ǫ|Ω) with
mean zero and covariance matrix Ωwhich induces a distribution on the ob-
© 2003 by Chapman & Hall/CRC

servations x, namely, p(x|B,u,Λ,s) with mean (In ⊗B)u+(In ⊗Λ)s and co-
variance matrix Ω.
The above is a very general model.
The covariance matrix Ωdeﬁnes a
model in which all observations (xij and xi′j′) are distinctly correlated. With
this model in its full covariance generality, there are an enormous number of
distinct covariance parameters. To be exact, there are
np(np+1)
2
(14.2.3)
distinct error covariance parameters [50]. The full error covariance matrix for
the observation vector can be represented by the block partitioned matrix
Ω=







Ω11 Ω12
···
Ω1n
Ω22
...
...
Ωn−1,n
Ωnn







,
(14.2.4)
where only the diagonal and superdiagonal submatrices Ωii and Ωii′ are dis-
played. The subdiagonal submatrices are found by reﬂection. The variance
of observation vector i is given by the p×p submatrix
var(xi|B,ui,si,Λ,Ωii) = Ωii
(14.2.5)
and the covariance between observation vectors i and i′ is given by the p×p
submatrix
cov(xi,xi′|B,ui,ui′,si,si′,Λ,Ωii′) = Ωii′.
(14.2.6)
This generality is rarely needed. A simpliﬁed error covariance matrix is
usually rich enough to capture the covariance structure. The error covariance
matrix can be simpliﬁed by specifying a particular structure, thereby reducing
the number of distinct parameters and the required computation. In the con-
text of Bayesian Factor Analysis, separable and matrix intraclass covariance
matrices have been considered [50].
A separable covariance matrix for the errors is
Ω=







φ11Σ φ12Σ
··· φ1nΣ
φ22Σ
...
...
φnnΣ







= Φ⊗Σ
(14.2.7)
which is exactly the structure of a Matrix Normal distribution.
The covariance matrix Φ will be referred to as the between vector covari-
ance matrix and Σ the within vector covariance matrix. Separable covariance
© 2003 by Chapman & Hall/CRC

matrices have a wide variety of applications such as in time series. In applica-
tions such as the previous FMRI example, spatial vector valued observations
xi are observed over time. In such applications, Φ could be referred to as the
“temporal” covariance matrix and Σ as the “spatial” covariance. Previous
work has also considered matrix intraclass covariance structures, but points
out that the matrix intraclass covariance can be transformed to an indepen-
dent covariance model by an orthogonal transformation [50]. The covariance
matrix Φ has
n(n+1)
2
(14.2.8)
distinct covariance parameters. The number of distinct covariance parameters
has been reduced to
n(n+1)
2
+ p(p+1)
2
.
(14.2.9)
Covariance structures such as intraclass and ﬁrst order Markov (also known
as an AR(1)) which depend on a single parameter have been considered for Φ
to further reduce the number of parameters and computation [50].
The motivation to model covariation among the observations and also the
source vectors is that they are often taken in a time or spatial order, thus
possibly not independent. This covariance structure allows for both spatial
and temporal correlation.
The correlated vector Bayesian Source Separation model with the separable
matrix speciﬁcations can be written in the matrix form
(X|U,B,Λ,S) =
U
B′
+
S
Λ′
+
E,
(n×p)
[n×(q +1)] [(q +1)×p]
(n×m) (m×p)
(n×p)
(14.2.10)
which is the previously model described from Part II where X′ = (x1,...,xn),
U ′ = (u1,...,un), S′ = (s1,...,sn), and E′ = (ǫ1,...,ǫn).
14.3 
Likeliho o d
The variability among the observations is speciﬁed to have arisen from a
separable Multivariate Normal distribution. A Multivariate Normal distribu-
tion with a separable covariance structure Ω= Φ⊗Σ can be simpliﬁed to the
Matrix Normal distribution form (as in Chapter 2)
p(X|U,B,Λ,S,Φ,Σ) ∝|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−UB−SΛ′)Σ−1(X−UB′−SΛ′)′,
(14.3.1)
© 2003 by Chapman & Hall/CRC

where X′ = (x1,...,xn), S′ = (s1,...,sn), and E′ = (ǫ1,...,ǫn). If the between
vector covariance matrix Φ were speciﬁed to be the identity matrix In, then
this likelihood corresponds to the uncorrelated observation vector one as dis-
cussed in Chapter 11.
With the separable covariance matrix structure which is a Matrix Normal
distribution, the marginal likelihood of any row of the matrix of data X, say
x′
i, is Multivariate Normally distributed with mean
E(xi|B,ui,φii,Σ,si,Λ) = Bui +Λsi,
(14.3.2)
and the variance is given by
var(xi|B,ui,φii,Σ,si,Λ) = φiiΣ,
(14.3.3)
while the covariance between any two rows (observation vectors) xi and xi′ is
given by
cov(xi,xi′|B,ui,ui′,si,si′,Λ,φii′,Σ) = φii′Σ,
(14.3.4)
where φii′ is the ii′th element of Φ.
The model may also be written in terms of columns as in Chapter 8 by
parameterizing the data matrix X, the observable source matrix U, the matrix
of Regression coeﬃcients B, and the matrix of errors E in terms of columns
as
X = (X1,...Xp),
U = (en,U1,...Uq),
B = (B0,B1,...Bq),
E = (E1,...Ep).
(14.3.5)
This leads to the Bayesian Source Separation model being also written as
(Xj|U,Bj,S,Λj) =
U
Bj
+
S
Λj
+
Ej,
(n×1)
[n×(q +1)] [(q +1)×1]
(n×m) (m×1)
(n×1)
(14.3.6)
which describes all the observations for a single microphone j at all n time
points.
With the column representation of the Source Separation model, the mar-
ginal likelihood of any column of the matrix of data X, say Xj, is also Mul-
tivariate Normally distributed with mean
E(Xj|U,Bj,Φ,σjj,S,Λj) = UBj +SΛj,
(14.3.7)
and variance given by
var(Xj|U,Bj,Φ,σjj,S,Λj) = σjjΦ,
(14.3.8)
© 2003 by Chapman & Hall/CRC

while the covariance between any two columns (observation vectors) Xj and
Xj′ is given by
cov(xj,xj′|B,uj,uj′,sj,sj′,Λ,Φ,σjj′) = σjj′Φ,
(14.3.9)
where σjj′ is the jj′th element of Σ.
The Regression and the mixing coeﬃcient matrices are joined into a single
matrix as C = (B,Λ). The observable and unobservable source matrices are
also joined as Z = (U,S).
Having joined these matrices, the correlated vector Source Separation model
is now
(X|C,Z) =
Z
C′
+
E,
n×p
n×(m+q +1) (m+q +1)×p
(n×p)
(14.3.10)
and the corresponding likelihood is
p(X|C,Z,Σ) ∝|Σ|−n
2 |Φ|−p
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′,
(14.3.11)
where all variables are as previously deﬁned.
Available prior knowledge regarding the parameter values are quantiﬁed
through both Conjugate and generalized Conjugate prior distributions. For
the Conjugate prior distribution model, diﬀerent structures will be speciﬁed
for Φ along with prior distributions.
14.4 
Conjugate Priors a nd Po sterior
When quantifying available prior information regarding the parameters of
interest, Conjugate prior distributions can be speciﬁed. For the between vec-
tor covariance matrix Φ, structures will be considered along with the cor-
responding prior distribution.The joint posterior distribution for the model
parameters which are the matrix of Regression/mixing coeﬃcients C, the ma-
trix of sources S, the source covariance matrix R, the within observation
vector covariance matrix Σ, and the between vector covariance matrix Φ is
given by
p(S,R,C,Σ,Φ) = p(S|R,Φ)p(R)p(C|Σ)p(Σ)p(Φ),
(14.4.1)
where the prior distributions for the model parameters are from the Conjugate
procedure outlined in Chapter 4 and are given by
© 2003 by Chapman & Hall/CRC

p(S|R,Φ) ∝|R|−n
2 |Φ|−m
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′,
(14.4.2)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(14.4.3)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(14.4.4)
p(C|Σ) ∝|D|−p
2 |Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′,
(14.4.5)
p(Φ)
as below,
(14.4.6)
where Σ, Φ, R, Q, V , D, and Ψ, are positive deﬁnite matrices.
The hy-
perparameters S0, ν, Q, η, V , D, C0, and any for p(Φ) are to be assessed.
Upon assessing the hyperparameters, the joint prior distribution is completely
determined.
The Conjugate prior distributions are Matrix Normal for the combined
matrix of Regression/mixing coeﬃcients C, Matrix Normal for the matrix of
sources S, where the source vectors and components are free to be correlated,
while the observation within Σ and between Φ, as well as source covariance
matrices R are taken to be Inverted Wishart distributed.
Upon using Bayes’ rule, the joint posterior distribution for the unknown
model parameters is proportional to the product of the joint prior distribution
and the likelihood and given by
p(S,C,Σ,Φ|X) ∝|Σ|−(n+ν+m+q+1)
2
e−1
2 trΣ−1G|R|−(n+η)
2
e−1
2 trR−1V
× |Φ|−(p+m)
2
e−1
2 trΦ−1(S−S0)R−1(S−S0)′p(Φ),
(14.4.7)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)′Φ−1(X −ZC′)+(C −C0)D−1(C −C0)′ +Q.
(14.4.8)
Again, the objective is to unmix the unobservable sources by estimating S
and to obtain knowledge about the mixing process by estimating B, Λ, Σ,
and Φ.
This joint posterior distribution must now be evaluated in order to obtain
our parameter estimates of the matrix of sources S, the matrix of Regres-
sion/mixing coeﬃcients C, the within vector source covariance matrix R, the
within observation vector covariance matrix Σ, and the between vector co-
variance matrix Φ. Marginal posterior mean and joint maximum a posteriori
estimated of the parameters S, R, Φ, C, and Σ are found by the Gibbs sam-
pling and ICM algorithms.
There are four cases which will be considered for the between vector covari-
ance matrix Φ: (1) Φ, a known general covariance matrix with no unknown
parameters, (2) Φ, a general unknown covariance matrix with n(n+1)/2 un-
known parameters (3) Φ, an intraclass structured covariance matrix with one
© 2003 by Chapman & Hall/CRC

unknown parameter, and (4) Φ, a ﬁrst order Markov structured covariance
matrix with one unknown parameter.
Φ Known
In some instances, we know Φ, are able to assess Φ, or can estimate Φ using
previous data, so that
p(Φ) =

1, if Φ = Φ0
0, if Φ ̸= Φ0,
(14.4.9)
a degenerate distribution. If the observation vectors were independent, then
Φ0 = In.
For Gibbs sampling, we will need the conditional posterior distributions.
When the covariance matrix Φ is known to be Φ0, then the only change in
posterior conditional distributions for the parameters Σ, S, and C is that Φ
will be replaced by Φ0.
Φ General Covariance
If we determine that the observations are correlated according to a gen-
eral covariance matrix, then we assess the Conjugate Inverted Wishart prior
distribution
p(Φ) ∝|Φ|−κ
2 e−1
2 trΦ−1Ψ,
(14.4.10)
where Ψ and κ are hyperparameters to be assessed which completely deter-
mine the prior.
Φ Unknown Structured
It is often the case that Φ is unknown but structured.
When Φ is un-
known, the conditionals for Σ, S, and C do not change from when Φ is known
or unknown and general. Once the structure is determined, we need to as-
sess the prior distributions for the unknown parameters in Φ and calculate
the posterior conditional distribution for the unknown parameters in Φ. We
will specify that the observations are homoscedastic and consider Φ to be a
structured correlation matrix.
There are many possible structures that we are able to specify for Φ that
apply to a wide variety of situations. Given that we have homoscedasticity of
the observation vectors, then
Ω= Φ⊗Σ =







Σ φ12Σ
··· φ1nΣ
Σ
...
...
Σ







,
© 2003 by Chapman & Hall/CRC

where Φ is a correlation matrix.
One possibility is that there is a structure in the correlation matrix Φ so
that its elements only depend on a single parameter ρ; then the covariance
matrix becomes
Ω= Φ⊗Σ =







Σ φ12(ρ)Σ
··· φ1n(ρ)Σ
Σ
...
...
Σ







.
Two well-known examples of possible correlation structures for Φ are in-
traclass and ﬁrst order Markov. We will state these correlation structures
and derive the posterior conditionals for both of these correlations assuming
a Generalized Scalar Beta prior distribution.
Φ Intraclass Correlation
It could be determined that the observations are correlated according to an
intraclass correlation. An intraclass correlation is used when we have a set of
variables and we believe that any two are related in the same way. Any two
variables have the same correlation. Then the between observation correlation
matrix Φ is
Φ =







1 ρ ρ ··· ρ
1 ρ ··· ρ
...
...
ρ
1







= (1−ρ)In +ρene′
n,
(14.4.11)
where en is a column vector of ones and −
1
n−1 < ρ < 1.
If we determine that the observations are correlated according to an intra-
class correlation matrix, then the Conjugate prior distribution
p(ρ) ∝(1−ρ)−(n−1)(p+m)
2
[1+(n−1)ρ]−β
2 e
−
1
2(1−ρ)
h
k1−ρk2
1+αρ
i
(14.4.12)
is speciﬁed, where Ψ for
k1 =
n

i=1
Ψii
and
k2 =
n

i′=1
n

i=1
Ψii′
(14.4.13)
is a hyperparameter to be assessed which completely determines the prior.
Φ First Order Markov Correlation
© 2003 by Chapman & Hall/CRC

It could be determined that the observations are correlated according to a
ﬁrst order Markov structure. In a ﬁrst order Markov structure, the observa-
tions are related according to a vector auto regression with a time lag of one,
V AR(1). With this structure, the between observation correlation matrix Φ
is
Φ =





1
ρ
ρ2 ··· ρn−1
ρ
1
ρ ··· ρn−2
...
...
...
...
ρn−1 ρn−2
···
1




,
(14.4.14)
where 0 < |ρ| < 1.
If it is determined that the observations are correlated according to a ﬁrst
order Markov correlation matrix, then the Conjugate prior distribution
p(ρ) ∝(1−ρ2)−(n−1)(p+m)
2
e
−k1−ρk2+ρ2k3
2(1−ρ2)
(14.4.15)
is speciﬁed, where Ψ for
k1 =
n

i=1
Ψii,
k2 =
n−1

i=1
(Ψi,i+1 +Ψi+1,i),
and
k3 =
n−1

i=2
Ψii
(14.4.16)
is a hyperparameter to be assessed which completely determines the prior.
14.5 
Conjugate Estimation a nd Inference
With the above posterior distribution, it is not possible to obtain marginal
distributions and thus marginal estimates for all or any of the parameters
in an analytic closed form or explicit maximum a posteriori estimates from
diﬀerentiation. It is possible to use both Gibbs sampling to obtain marginal
parameter estimates and the ICM algorithm to ﬁnd maximum a posteriori
estimates. For both estimation procedures which are described in Chapter 6,
the posterior conditional distributions are required.
14.5.1
Posterior Conditionals
From the joint posterior distribution we can obtain the posterior condi-
tional distribution for each of the model parameters.
The conditional posterior distributions for the Regression/mixing matrix
C is found by considering only the terms in the joint posterior distribution
which involve C and is given by
© 2003 by Chapman & Hall/CRC

p(C|S,R,Σ,Φ,U,X) ∝p(C|Σ)p(X|C,Z,Σ,Φ)
∝|Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′
× |Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′Φ−1(X−ZC′)
∝e−1
2 trΣ−1[(C−C0)D−1(C−C0)′+(X−ZC′)′Φ−1(X−ZC′)]
∝e−1
2 trΣ−1(C−˜
C)(D−1+Z′Φ−1Z)(C−˜
C)′,
(14.5.1)
where the vector ˜C, the posterior conditional mean and mode, has been de-
ﬁned and is given by
˜C = [C0D−1 +X′Φ−1Z](D−1 +Z′Φ−1Z)−1
(14.5.2)
= [C0D−1 + ˆC(Z′Φ−1Z)](D−1 +Z′Φ−1Z)−1.
(14.5.3)
Note that the matrix of coeﬃcients C can be written as a weighted com-
bination of the prior mean C0 from the prior distribution and the data mean
ˆC = X′Φ−1Z(Z′Φ−1Z)−1 from the likelihood.
The conditional distribution for the combined Regression mixing matrix
given the matrix of unobservable sources S, the within vector source covari-
ance matrix R, the within observation vector covariance matrix Σ, the between
vector covariance matrix Φ, the matrix of observed sources U, and the matrix
of data X is Matrix Normally distributed.
The conditional posterior distribution of the within observation vector co-
variance matrix Σ is found by considering only the terms in the joint posterior
distribution which involve Σ and is given by
p(Σ|B,S,R,Λ,Φ,U,X) ∝p(Σ)p(Λ|Σ)p(X|Z,C,Σ,Φ)
∝|Σ|−m+q+1
2
e−1
2 trΣ−1(C−C0)D−1(C−C0)′
×|Σ|−ν
2 e−1
2 trΣ−1Q
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′Φ−1(X−ZC′)
∝|Σ|−(n+ν+m+q+1)
2
e−1
2 trΣ−1G,
(14.5.4)
where the p×p matrix G has been deﬁned to be
G = (X −ZC′)′Φ−1(X −ZC′)+(C −C0)D−1(C −C0)′ +Q
(14.5.5)
with a mode as described in Chapter 2 given by
˜Σ =
G
n+ν +m+q +1 .
(14.5.6)
© 2003 by Chapman & Hall/CRC

The posterior conditional distribution of the within observation vector co-
variance matrix Σ given the matrix of unobservable sources S, the within
source vector covariance matrix R, the matrix of Regression/mixing coeﬃ-
cients C, the between vector covariance matrix Φ, the matrix of observable
sources U, and the matrix of data X is an Inverted Wishart distribution.
The conditional posterior distribution for the matrix of sources S is found
by considering only the terms in the joint posterior distribution which involve
S and is given by
p(S|B,R,Λ,Σ,Φ,U,X) ∝p(S|R,Φ)p(X|B,Λ,S,Σ,Φ,U)
∝|Φ|−m
2 |R|−n
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
×|Σ|−n
2 e−1
2 trΣ−1(X−UB′−SΛ′)′Φ−1(X−UB′−SΛ′)
∝e−1
2 trΦ−1(S−˜S)(R−1+Λ′Σ−1Λ)(S−˜S)′,
(14.5.7)
where the matrix ˜S has been deﬁned which is the posterior conditional mean
and mode as described in Chapter 2 and is given by
˜S = [S0R−1 +(X −UB′)Σ−1Λ](R−1 +Λ′Σ−1Λ)−1.
(14.5.8)
The conditional posterior distribution for the matrix of sources S given
the matrix of Regression coeﬃcients B, the within source vector covariance
matrix R, the matrix of mixing coeﬃcients Λ, the within observation vector
covariance matrix Σ, the between vector covariance matrix Φ, the matrix of
observable sources U, and the matrix of data X is Normally Matrix distrib-
uted.
The conditional posterior distribution for the within source vector covari-
ance matrix R is found by considering only the terms in the joint posterior
distribution which involve R and is given by
p(R|C,Z,Σ,Φ,X) ∝p(R)p(S|R,Φ)p(X|C,Z,Σ,Φ)
∝|R|−η
2 e−1
2 trR−1V |Φ|−m
2 |R|−n
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
∝|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′Φ−1(S−S0)+V ],
(14.5.9)
with the posterior conditional mode as described in Chapter 2 given by
˜R = (S −S0)′Φ−1(S −S0)+V
n+η
.
(14.5.10)
The conditional posterior distribution for the within source vector covari-
ance matrix R given the matrix of Regression/mixing coeﬃcients C, the ma-
trix of unobservable sources S, the within observation vector covariance matrix
© 2003 by Chapman & Hall/CRC

Σ, the between vector covariance matrix Φ, the matrix of observable sources,
and the matrix of X data is Inverted Wishart distributed.
Φ Known
The conditional posterior distribution for the between vector covariance ma-
trix Φ is found by considering only the terms in the joint posterior distribution
which involve Φ and is given by
p(Φ|C,Z,R,Σ,X) ∝p(Φ)p(S|R,Φ)p(X|C,Z,Σ,Φ)
∝1Φ=Φ0|Φ|−m
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
×|Φ|−p
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′
= 1Φ=Φ0,
(14.5.11)
where 1Φ=Φ0 is used to denote the prior distribution for Φ which is one at Φ0
and zero otherwise.
Φ General Covariance
The conditional posterior distribution for the between vector covariance ma-
trix Φ is found by considering only the terms in the joint posterior distribution
which involve Φ and is given by
p(Φ|C,Z,R,Σ,X) ∝p(Φ)p(S|R,Φ)p(X|C,Z,Σ,Φ)
∝|Φ|−κ
2 e−1
2 trΦ−1Ψ|Φ|−m
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
×|Φ|−p
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′,
(14.5.12)
with the posterior conditional mode as described in Chapter 2 given by
˜Φ = (X −ZC′)Σ−1(X −ZC′)′ +(S −S0)R−1(S −S0)′ +Ψ
p+m+κ
.
(14.5.13)
The conditional posterior distribution for the between vector covariance
matrix Φ given the matrix of Regression/mixing coeﬃcients C, the matrix of
unobservable sources S, the matrix of observable sources U, the within source
vector covariance matrix R, the within observation vector covariance matrix
Σ, and the matrix of data X is Inverted Wishart distributed.
Φ Intraclass Correlation
As previously stated, the exact form of the conditional posterior distribution
depends on which structure is determined for the correlation matrix Φ. If
the intraclass structure that has the covariance between any two observations
being the same is determined, then we can use the result that the determinant
of Φ has the form
© 2003 by Chapman & Hall/CRC

|Φ| = (1−ρ)n−1[1+ρ(n−1)]
(14.5.14)
and the result that the inverse of Φ has the form
Φ−1 =
In
1−ρ −
ρene′
n
(1−ρ)[1+(n−1)ρ],
(14.5.15)
which is again a matrix with intraclass correlation structure. Using the afore-
mentioned likelihood, priors, and forms above the posterior conditional dis-
tribution is
p(ρ|S,C,Ψ,X,U) ∝p(ρ)p(S|Φ)p(X|Φ,S,C,Σ,U)
∝(1−ρ)−(n−1)(p+m)
2
[1+(n−1)ρ]−(p+m)
2
∝e
−
1
2(1−ρ)

tr(Ψ)−ρtr(ene′nΨ)
1+(n−1)ρ

×|Φ|−m
2 |R|−n
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
×|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′
∝(1−ρ)−(n−1)(p+m)[1+ρ(n−1)]−(p+m)
×e
−1
2
h k1
1−ρ −
k2ρ
(1−ρ)[1+(n−1)ρ]
i
,
(14.5.16)
where
Ξ = (X −ZC′)Σ−1(X −ZC′)′ +(S −S0)R−1(S −S0)′ +Ψ,
(14.5.17)
k1 = tr(Ξ) =
n

i=1
Ξii,
and
k2 = tr(ene′
nΞ) =
n

i′=1
n

i=1
Ξii′.
(14.5.18)
This is not recognizable as a common distribution.
Φ First Order Markov Correlation
If the ﬁrst order Markov structure is determined, then the result that the
determinant of a matrix with such structure has the form
|Φ| = (1−ρ2)n−1
(14.5.19)
and the result that the inverse of such a patterned matrix has the form
Φ−1 =
1
1−ρ2







1
−ρ
0
−ρ (1+ρ2) −ρ
...
...
...
(1+ρ2) −ρ
0
−ρ
1







.
(14.5.20)
© 2003 by Chapman & Hall/CRC

These results are used along with the aforementioned likelihood and prior
distributions to obtain
p(ρ|S,C,Σ,X,U) = p(ρ)p(S|Φ)p(X|Φ,S,C,Σ,U)
∝(1−ρ2)−(n−1)(p+m)
2
e
−k1−ρk2+ρ2k3
2(1−ρ2)
×|Φ|−m
2 |R|−n
2 e−1
2 trΦ−1(S−S0)R−1(S−S0)′
×|Φ|−p
2 |Σ|−n
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′
∝(1−ρ2)−(n−1)(p+m)e
−k1−ρk2+ρ2k3
2(1−ρ2)
,
(14.5.21)
where the matrices and constants used are Ξ as deﬁned previously,
Ψ1 = In,
Ψ2 =







0 1
0
1 0
1
... ... ...
0 1
0
1 0







,
Ψ3 =







0
0
1
...
1
0
0







,
k1 = tr(Ψ1Ξ) =
n

i=1
Ξii,
(14.5.22)
k2 = tr(Ψ2Ξ) =
n−1

i=1
(Ξi,i+1 +Ξi+1,i),
(14.5.23)
and
k3 = tr(Ψ3Ξ) =
n−1

i=2
Ξii.
(14.5.24)
Again, this is not recognizable as a common distribution.
These are two simple possible structures. There may be others that also
depend on a single parameter or on several parameters. The rejection sam-
pling technique is needed and is simple to carry out because one only needs
to generate random variates from a univariate distribution.
14.5.2
Gibbs Sampling
To ﬁnd marginal mean estimates of the model parameters from the joint
posterior distribution using the Gibbs sampling algorithm, start with initial
© 2003 by Chapman & Hall/CRC

values for the matrix of sources S, the within observation vector covariance
matrix Σ and the between vector covariance matrix Φ, say ¯S(0), ¯Σ(0), and ¯Φ(0)
or ¯ρ(0), then cycle through
¯C(l+1) = a random variate from p(C| ¯S(l), ¯R(l), ¯Σ(l), ¯Φ(l),U,X)
= ACYCB′
C +MC,
(14.5.25)
¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l), ¯C(l+1), ¯Φ(l),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(14.5.26)
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l+1), ¯Σ(l+1), ¯Φ(l),U,X)
= AR(Y ′
RYR)−1A′
R,
(14.5.27)
¯S(l+1) = a random variate from p(S| ¯R(l+1), ¯C(l+1), ¯Σ(l+1), ¯Φ(l),U,X)
= ASYSB′
S +MS,
(14.5.28)
¯Φ(l+1) = a random variate from p(Φ| ¯S(l+1), ¯R(l+1), ¯C(l+1), ¯Σ(l+1),U,X)
¯Φ(l+1) =

Φ0
if known
AΦ(Y ′
ΦYΦ)−1A′
Φ if general,
(14.5.29)
or
¯ρ(i+1) = a random variate from p(ρ| ¯S(l+1), ¯R(l+1), ¯C(l+1), ¯Σ(l+1),U,X),
where
ACA′
C = ¯Σ(l),
BCB′
C = (D−1 + ¯Z′
(l) ¯Φ−1
(l) ¯Z(l))−1,
¯Z(l) = (U, ¯S(l)),
MC = (X′ ¯Φ−1
(l) ¯Z(l) +C0D−1)(D−1 + ¯Z′
(l) ¯Φ−1
(l) ¯Z(l))−1,
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′ ¯Φ−1
(l) (X −¯Z(l) ¯C′
(l+1))
+ ( ¯C(l+1) −C0)D−1( ¯C(l+1) −C0)′ +Q,
ARA′
R = ( ¯S(l) −S0)′ ¯Φ−1
(l) ( ¯S(l) −S0)+V,
ASA′
S = ¯Φ(l),
BSB′
S = ( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
MS = [S0 ¯R−1
(l+1) +(X −U ¯B′
(l+1))¯Σ−1
(l+1)¯Λ(l+1)]
×( ¯R−1
(l+1) + ¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
AΦA′
Φ = (X −¯Z(l+1) ¯C′
(l+1))¯Σ−1
(l+1)(X −¯Z(l+1) ¯C′
(l+1))′ +
( ¯S(l+1) −S0) ¯R−1
(l+1)( ¯S(l+1) −S0)′ +Ψ,
while YC, YΣ, YR, YS, and YΦ are p×(m+q+1), (n+ν +m+q+1+p+1)×p,
(n+η +m+1)×m, n×m, and (p+m+κ+n+1)×n dimensional matrices
© 2003 by Chapman & Hall/CRC

respectively, whose elements are random variates from the standard Scalar
Normal distribution. The formulas for the generation of random variates from
the conditional posterior distributions are easily found from the methods in
Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of the parameters
¯S = 1
L
L

l=1
¯S(l)
¯R = 1
L
L

l=1
¯R(l)
¯C = 1
L
L

l=1
¯C(l)
¯Σ = 1
L
L

l=1
¯Σ(l)
¯Φ = 1
L
L

l=1
¯Φ(l) or ¯ρ = 1
L
L

l=1
¯ρ(l)
which are the exact sampling-based marginal posterior mean estimates of
the parameters. Exact sampling-based estimates of other quantities can also
be found.
Similar to Bayesian Regression, Bayesian Factor Analysis, and
Bayesian Source Separation, there is interest in the estimate of the marginal
posterior variance of the matrix containing the Regression and mixing coeﬃ-
cients
var(c|X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆,
where c = vec(C) and ¯c = vec( ¯C). The covariance matrices of the other pa-
rameters follow similarly. With a speciﬁcation of Normality for the marginal
posterior distribution of the vector containing the Regression and mixing co-
eﬃcients, their distribution is
p(c|X,U) ∝| ¯∆|−1
2 e−1
2 (c−¯c)′ ¯∆−1(c−¯c),
(14.5.30)
where ¯c and ¯∆are as previously deﬁned.
To determine statistical signiﬁcance with the Gibbs sampling approach, use
the marginal distribution of the matrix containing the Regression and mixing
coeﬃcients given above. General simultaneous hypotheses can be evaluated
regarding the entire matrix containing the Regression and mixing coeﬃcients,
a submatrix, or a particular independent variable or source, or an element
by computing marginal distributions.
It can be shown that the marginal
distribution of the kth column of the matrix containing the Regression and
mixing coeﬃcients C, Ck is Multivariate Normal
p(Ck| ¯Ck,X,U) ∝| ¯∆k|−1
2 e−1
2 (Ck−¯
Ck)′ ¯∆−1
k
(Ck−¯
Ck),
(14.5.31)
© 2003 by Chapman & Hall/CRC

where ¯∆k is the covariance matrix of Ck found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
Signiﬁcance can be determined for a subset of coeﬃcients of the kth column
of C by determining the marginal distribution of the subset within Ck which is
also Multivariate Normal. With the subset being a singleton set, signiﬁcance
can be determined for a particular coeﬃcient with the marginal distribution
of the scalar coeﬃcient which is
p(Ckj| ¯Ckj,X,U) ∝( ¯∆kj)−1
2 e
−
(Ckj−¯Ckj)2
2 ¯
∆kj
,
(14.5.32)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ¯Ckj = ¯cjk and that
z = (Ckj −¯Ckj)
,
¯∆kj
(14.5.33)
follows a Normal distribution with a mean of zero and variance of one.
14.5.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
model parameters.
To maximize the joint posterior distribution using the
ICM algorithm, start with initial values for S, and Φ, say ˜S(0) and ˜Φ(0), and
then cycle through
˜C(l+1) =
Arg Max
C
p(C| ˜S(l), ˜R(l), ˜Σ(l), ˜Φ(l),U,X)
= [X′ ˜Φ−1
(l) ˜Z(l) +C0D−1](D−1 + ˜Z′
(l) ˜Φ−1
(l) ˜Z(l))−1,
˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l+1), ˜S(l), ˜R(l), ˜Φ(l),U,X)
= [(X −˜Z(l) ˜C′
(l+1))′ ˜Φ−1
(l) (X −˜Z(l) ˜C′
(l+1))
+ ( ˜C(l+1) −C0)D−1( ˜C(l+1) −C0)′ +Q]/(n+ν +m+q +1),
˜R(l+1) =
Arg Max
R
p(R| ˜C(l+1), ˜Z(l), ˜Σ(l+1), ˜Φ(l),X)
=
( ˜S(l) −S0)′ ˜Φ−1
(l) ( ˜S(l) −S0)+V
n+η
,
˜S(l+1) =
Arg Max
S
p(S| ˜B(l+1), ˜R(l+1), ˜Λ(l+1), ˜Σ(l+1), ˜Φ(l),U,X),
= [S0 ˜R−1
(l+1) +(X −U ˜B′
(l+1))˜Σ−1
(l+1)˜Λ(l+1)]
×( ˜R−1
(l+1) + ˜Λ′
(l+1) ˜Σ−1
(l+1)˜Λ(l+1))−1,
˜Φ(l+1) =
Arg Max
Φ
p(Φ| ˜C(l+1), ˜Z(l+1), ˜R(l+1), ˜Σ(l+1),X),
˜Φ(l+1) = [(X −˜Z(l+1) ˜C′
(l+1))˜Σ−1
(l+1)(X −˜Z(l+1) ˜C′
(l+1))′
© 2003 by Chapman & Hall/CRC

+ ( ˜S(l+1) −S0) ˜R−1
(l+1)( ˜S(l+1) −S0)′ +Ψ]/(p+m+κ),
or
˜ρ(l+1) =
Arg Max
ρ
p(ρ| ˜C(l+1), ˜Z(l+1), ˜R(l+1), ˜Σ(l+1),X),
where the matrix ˜Z(l) = (U, ˜S(l)) until convergence is reached. The converged
values ( ˜C, ˜S, ˜R, ˜Σ, ˜Φ) are joint posterior modal (maximum a posteriori) esti-
mates of the model parameters. Conditional maximum a posteriori variance
estimates can also be found. The conditional modal variance of the matrix
containing the Regression and mixing coeﬃcients is
var(C| ˜C, ˜S, ˜R, ˜Σ, ˜Φ,,X,U) = ˜Σ⊗(D−1 ⊗˜Z′ ˜Φ−1 ˜Z)−1
or equivalently
var(c|˜c, ˜S, ˜R, ˜Σ, ˜Φ,X,U) = (D−1 ⊗˜Z′ ˜Φ−1 ˜Z)−1 ⊗˜Σ
= ˜∆,
where c = vec(C), ˜S, ˜R, and ˜Σ are the converged value from the ICM algo-
rithm.
To determine statistical signiﬁcance with the ICM approach, use the con-
ditional distribution of the matrix containing the Regression and mixing co-
eﬃcients which is
p(C| ˜C, ˜S, ˜R, ˜Σ, ˜Φ,X,U) ∝|D−1 + ˜Z′ ˜Φ−1 ˜Z|
1
2 |˜Σ|−1
2
×e−1
2 tr ˜Σ−1(C−˜
C)(D−1+ ˜
Z′ ˜Φ−1 ˜
Z)(C−˜
C)′.
(14.5.34)
That is,
C| ˜C, ˜S, ˜R, ˜Σ, ˜Φ,X,U ∼N

˜C, ˜Σ⊗(D−1 + ˜Z′ ˜Φ−1 ˜Z)−1
.
(14.5.35)
General simultaneous hypotheses can be evaluated regarding the entire ma-
trix containing the Regression and mixing coeﬃcients, a submatrix, or the
coeﬃcients of a particular independent variable or source, or an element by
computing marginal conditional distributions.
It can be shown [17, 41] that the marginal conditional distribution of any
column of the matrix containing the Regression and mixing coeﬃcients C, Ck
is Multivariate Normal
p(Ck| ˜Ck, ˜S, ˜Σ, ˜Φ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Ck−˜
Ck)′(Wkk ˜Σ)−1(Ck−˜
Ck),
(14.5.36)
© 2003 by Chapman & Hall/CRC

where W = (D−1 + ˜Z′ ˜Φ−1 ˜Z)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of C, signiﬁcance can be de-
termined for a particular independent variable or source. Signiﬁcance can be
determined for a subset of coeﬃcients by determining the marginal distrib-
ution of the subset within Ck which is also Multivariate Normal. With the
subset being a singleton set, signiﬁcance can be determined for a particular
coeﬃcient with the marginal distribution of the scalar coeﬃcient which is
p(Ckj| ˜Ckj, ˜S, ˜Σjj ˜Φ,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(Ckj−˜Ckj)2
2Wkk ˜Σjj ,
(14.5.37)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Ckj = ˜cjk and that
z = (Ckj −˜Ckj)
,
Wkk ˜Σjj
(14.5.38)
follows a Normal distribution with a mean of zero and variance of one.
14.6 
Generalized Priors and Posterior
Generalized Conjugate prior distributions are assessed in order to quantify
available prior information regarding values of the model parameters. The
joint prior distribution for the matrix of sources S, the within source vector
covariance matrix R, the between source vector covariance matrix χ, the vec-
tor of Regression/mixing coeﬃcients c = vec(C), the within observation vector
covariance matrix Σ, and the between observation vector Φ is given by
p(S,R,χ,c,Σ,Φ) = p(S|R,χ)p(R)p(χ)p(c)p(Σ)p(Φ),
(14.6.1)
where the prior distribution for the parameters from the generalized Conjugate
procedure outlined in Chapter 4 are as follows
p(S|R,χ) ∝|R|−n
2 |χ|−m
2 e−1
2 trχ−1(S−S0)R−1(S−S0)′,
(14.6.2)
p(R) ∝|R|−η
2 e−1
2 trR−1V ,
(14.6.3)
p(Σ) ∝|Σ|−ν
2 e−1
2 trΣ−1Q,
(14.6.4)
p(c) ∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0),
(14.6.5)
p(Φ) ∝|Φ|−κ
2 e−1
2 trΦ−1Ψ,
(14.6.6)
p(χ) ∝|χ|−ξ
2 e−1
2 trχ−1Ξ,
(14.6.7)

where χ, Ξ, Σ, R, V , Q, ∆, Φ, and Ψ are positive deﬁnite matrices. The
hyperparameters S0, η, V , ν, Q, c0, ∆, ξ, and Ξ are to be assessed. Upon
assessing the hyperparameters, the joint prior distribution is completely de-
termined.
The prior distribution for the matrix of sources S is Matrix Normally dis-
tributed, the prior distribution for the within source vector covariance matrix
R is Inverted Wishart distributed, the prior distribution for the between source
vector covariance matrix χ is Inverted Wishart distributed, the prior distrib-
ution for the vector of Regression/mixing coeﬃcients c = vec(C), C = (B,Λ)
is Multivariate Normally distributed, the prior distribution for the error co-
variance matrix Σ is Inverted Wishart distributed, and the prior distribution
for the between observation vector covariance matrix Φ is Inverted Wishart
distributed.
Note that R, χ, Σ, and Φ, are full covariance matrices allowing within and
between correlation for the observed mixed signals vectors (microphones) and
also for the unobserved source vectors (speakers). The mean of the sources
is often taken to be constant for all observations and thus without loss of
generality taken to be zero. An observation (time) varying source mean is
adopted here.
Upon using Bayes’ rule the joint posterior distribution for the unknown
parameters with generalized Conjugate prior distributions for the model pa-
rameters is given by
p(S,R,χ,c,Σ,Φ) = p(S|R,χ)p(R)p(χ)p(c)p(Σ)p(Φ),
(14.6.8)
which is
p(S,R,χ,c,Σ,Φ|U,X) ∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′Φ−1(X−ZC′)+Q]
×|R|−(n+η)
2
e−1
2 trR−1[(S−S0)′χ−1(S−S0)+V ]
×e−1
2 (c−c0)′∆−1(c−c0)|Φ|−(p+κ)
2
e−1
2 trΦ−1Ψ
×|χ|−ξ
2 e−1
2 trχ−1Ξ
(14.6.9)
after inserting the joint prior distribution and likelihood.
This joint posterior distribution is now to be evaluated in order to ob-
tain parameter estimates of the matrix of sources S, the vector of Regres-
sion/mixing coeﬃcients c, the within source vector covariance matrix R, the
between source vector covariance matrix χ, the within observation covariance
matrix Σ, and the between observation covariance matrix Φ.
© 2003 by Chapman & Hall/CRC

14.7
Generalized Estimation and Inference
With the above posterior distribution, it is not possible to obtain marginal
distributions and thus marginal estimates for all or any of the parameters
in an analytic closed form or explicit maximum a posteriori estimates from
diﬀerentiation. It is possible to use both Gibbs sampling to obtain marginal
parameter estimates and the ICM algorithm for maximum a posteriori esti-
mates. For both estimation procedures, the posterior conditional distributions
are required.
14.7.1
Posterior Conditionals
Both the Gibbs sampling and ICM algorithms require the posterior condi-
tional distributions. Gibbs sampling requires the conditionals for the genera-
tion of random variates while ICM requires them for maximization by cycling
through their modes or maxima.
The conditional posterior distribution of the matrix of sources S is found
by considering only the terms in the joint posterior distribution which involve
S and is given by
p(S|B,R,χ,Λ,Σ,Φ,U,X) ∝p(S|R,χ)p(X|B,S,Λ,Σ,Φ,U)
∝e−1
2 trχ−1(S−S0)R−1(S−S0)′
×e−1
2 trΦ−1(X−UB′−SΛ′)Σ−1(X−UB′−SΛ′)′,
(14.7.1)
which after performing some algebra in the exponent can be written as
p(s|B,R,χ,Λ,Σ,Φ,u,x) ∝e−1
2 (s−˜s)′(χ−1⊗R−1+Φ−1⊗Λ′Σ−1Λ)(s−˜s),
(14.7.2)
where the vector ˜s has been deﬁned to be
˜s = [χ−1 ⊗R−1 +Φ−1 ⊗Λ′Σ−1Λ]−1
×[(χ−1 ⊗R−1)s0 +(Φ−1 ⊗Λ′Σ−1Λ)ˆs],
(14.7.3)
and the matrix ˆS has been deﬁned to be
ˆS = (X −UB′)Σ−1Λ(Λ′Σ−1Λ)−1,
(14.7.4)
with the vector ˆs has been deﬁned to be
ˆs = vec( ˆS′).
(14.7.5)
© 2003 by Chapman & Hall/CRC

That is, the matrix of sources S given the matrix of Regression coeﬃcients
B, the within source vector covariance matrix R, the between source vector
covariance matrix χ, the matrix of mixing coeﬃcients Λ, the within observa-
tion vector covariance matrix Σ, the between observation vector covariance
matrix Φ, the matrix of observable sources U, and the matrix of data is Matrix
Normally distributed.
The conditional posterior distribution of the within source vector covari-
ance matrix R is found by considering only the terms in the joint posterior
distribution which involve R and is given by
p(R|B,S,χ,Λ,Σ,Φ,U,X) ∝p(R)p(S|R,χ)
∝|R|−ν
2 e−1
2 trR−1V |R|−n
2 e−1
2 trR−1(S−S0)′χ−1(S−S0)
∝|R|−(n+ν)
2
e−1
2 trR−1[(S−S0)′χ−1(S−S0)+V ]. (14.7.6)
That is, the conditional posterior distribution of the within source vector
covariance matrix R given the the matrix of Regression coeﬃcients B, the
matrix of sources S, the between source vector covariance matrix χ, the matrix
of mixing coeﬃcients Λ, the within observation vector covariance matrix Σ,
the between observation vector covariance matrix Φ, the matrix of observable
sources U, and the matrix of data X has an Inverted Wishart distribution.
The conditional posterior distribution of the vector of Regression/mixing
coeﬃcients c matrix is found by considering only the terms in the joint pos-
terior distribution which involve c or C and is given by
p(c|S,R,χ,Σ,Φ,U,X) ∝p(c)p(X|Z,C,Σ,U)
∝|∆|−1
2 e−1
2 (c−c0)′∆−1(c−c0)
×|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′Φ−1(X−ZC′), (14.7.7)
which after performing some algebra in the exponent becomes
p(c|S,R,χ,Σ,Φ,U,X) ∝e−1
2 (c−˜c)′[∆−1+Z′Φ−1Z⊗Σ−1](c−˜c),
(14.7.8)
where the vector ˜c has been deﬁned to be
˜c = [∆−1 +Z′Φ−1Z ⊗Σ−1]−1[∆−1c0 +(Z′Φ−1Z ⊗Σ−1)ˆc],
(14.7.9)
and the vector ˆc has been deﬁned to be
ˆc = vec[X′Z(Z′Φ−1Z)−1].
(14.7.10)
The conditional posterior distribution of the vector of Regression/mixing
coeﬃcients given the matrix of sources S, the within source vector covariance
© 2003 by Chapman & Hall/CRC

matrix R, the between source vector covariance matrix χ, the within obser-
vation vector covariance matrix Σ, the between observation vector covariance
matrix Φ, the matrix of observable sources U, and the matrix of data X is
Multivariate Normally distributed.
The conditional posterior distribution of the within observation vector co-
variance matrix Σ is found by considering only the terms in the joint posterior
distribution which involve Σ and is given by
p(Σ|C,Z,R,χ,Φ,U,X) ∝p(Σ)p(X|S,C,Z,Σ)
∝|Σ|−(n+ν)
2
e−1
2 trΣ−1[(X−ZC′)′Φ−1(X−ZC′)+Q].
(14.7.11)
That is, the conditional distribution of the within observation vector covari-
ance matrix Σ given matrix of Regression/mixing coeﬃcients C, the matrix of
sources S, the within source vector covariance matrix R, the between source
covariance matrix χ, the between observation covariance matrix Φ, the matrix
of observable sources U, and the matrix of data X has an Inverted Wishart
distribution.
The conditional posterior distribution for the between observation vector
covariance matrix Φ is found by considering only the terms in the joint pos-
terior distribution which involve Φ and is given by
p(Φ|S,C,R,χ,Σ,U,X) ∝p(Φ)p(X|S,C,Σ,Φ)
∝|Φ|−κ
2 e−1
2 trΦ−1Ψ
×|Φ|−p
2 e−1
2 trΦ−1(X−ZC′)Σ−1(X−ZC′)′
∝|Φ|−p+κ
2 e−1
2 trΦ−1[(X−ZC′)Σ−1(X−ZC′)′+Ψ].
(14.7.12)
The conditional posterior distribution for the between observation vector
covariance matrix Φ given the matrix of sources S, the matrix of Regres-
sion/mixing coeﬃcients C, the within source vector covariance matrix R, the
between source vector covariance matrix χ, the within observation vector co-
variance matrix Σ, the matrix of observable sources U, and the matrix of data
X is Inverted Wishart distributed.
The conditional posterior distribution for the between observation vector
covariance matrix χ is found by considering only the terms in the joint pos-
terior distribution which involve χ and is given by
p(χ|S,C,R,Σ,Φ,U,X) ∝p(χ)p(S|R,χ)
∝|χ|−ξ
2 e−1
2 trχ−1Ξ
© 2003 by Chapman & Hall/CRC

×|χ|−m
2 e−1
2 trχ−1(S−S0)R−1(S−S0)′
∝|χ|−m+ξ
2 e−1
2 trχ−1[(S−S0)R−1(S−S0)′+Ξ].
(14.7.13)
The conditional posterior distribution for the between source vector covari-
ance matrix χ given the matrix of sources S, the matrix of Regression/mixing
coeﬃcients C, the within source vector covariance matrix R, the within obser-
vation vector covariance matrix Σ, the between observation vector covariance
matrix Φ, the matrix of observable sources U, and the matrix of data X is
Inverted Wishart distributed.
The modes of these conditional distributions are ˜S, ˜c, (both as deﬁned
above)
˜R = (S −S0)′χ−1(S −S0)+V
n+η
,
(14.7.14)
˜Σ = (X −ZC′)′Φ−1(X −ZC′)+Q
n+ν
,
(14.7.15)
˜Φ = (X −ZC′)Σ−1(X −ZC′)′ +Ψ
p+κ
,
(14.7.16)
and
˜χ = (S −S0)R−1(S −S0)′ +Ξ
m+ξ
(14.7.17)
respectively.
14.7.2
Gibbs Sampling
To ﬁnd marginal mean estimates of the parameters from the joint posterior
distribution using the Gibbs sampling algorithm, start with initial values for
the vector of sources s, the within source vector covariance matrix R, and the
within observation vector covariance matrix Σ, say ¯s(0), ¯R(0) and ¯Σ(0), and
then cycle through
¯R(l+1) = a random variate from p(R| ¯S(l), ¯C(l), ¯Σ(l), ¯Φ(l), ¯χ(l),U,X)
= AR(Y ′
RYR)−1A′
R,
(14.7.18)
¯c(l+1) = a random variate from p(c| ¯S(l), ¯R(l+1), ¯Σ(l), ¯Φ(l), ¯χ(l),U,X)
= AcYc +Mc,
(14.7.19)
© 2003 by Chapman & Hall/CRC

¯Σ(l+1) = a random variate from p(Σ| ¯S(l), ¯R(l+1), ¯C(l+1), ¯Φ(l), ¯χ(l),U,X)
= AΣ(Y ′
ΣYΣ)−1A′
Σ,
(14.7.20)
¯s(l+1) = a random variate from p(s| ¯R(l+1), ¯C(l+1), ¯Σ(l+1), ¯Φ(l), ¯χ(l),u,x)
= AsYs +Ms,
(14.7.21)
¯Φ(l+1) = a random variate from p(Φ| ¯S(l+1), ¯R(l+1), ¯C(l+1), ¯Σ(l+1), ¯χ(l),U,X)
= AΦ(Y ′
ΦYΦ)−1A′
Φ,
(14.7.22)
¯χ(l+1) = a random variate from p(χ| ¯S(l+1), ¯R(l+1), ¯C(l+1), ¯Σ(l+1), ¯Φ(l+1),U,X)
= Aχ(Y ′
χYχ)−1A′
χ,
(14.7.23)
where
¯Z(l) = (U, ¯S(l)),
ˆc(l) = vec[X′ ¯Z(l)( ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l))−1],
¯c(l+1) = [∆−1 + ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l) ⊗¯Σ−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l) ⊗¯Σ−1
(l) )ˆc(l)],
AcA′
c = (∆−1 + ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l) ⊗¯Σ−1
(l) )−1,
Mc = [∆−1 + ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l) ⊗¯Σ−1
(l) ]−1[∆−1c0 +( ¯Z′
(l) ¯Φ−1
(l+1) ¯Z(l) ⊗¯Σ−1
(l) )ˆc],
AΣA′
Σ = (X −¯Z(l) ¯C′
(l+1))′ ¯Φ−1
(l) (X −¯Z(l) ¯C′
(l+1))
+ ( ¯C(l+1) −C0)D−1( ¯C(l+1) −C0)′ +Q,
ARA′
R = ( ¯S(l) −S0)′ ¯Φ−1
(l) ( ¯S(l) −S0)+V,
AsA′
s = (¯χ−1
(l) ⊗¯R−1
(l+1) + ¯Φ−1
(l) ⊗¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1,
ˆs(l) = vec[(¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))−1¯Λ′
(l+1) ¯Σ−1
(l+1)(X −U ¯B′
(l+1))′,
Ms = [¯χ−1
(l) ⊗¯R−1
(l+1) + ¯Φ−1
(l) ⊗¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1)]−1,
×[(¯χ−1
(l) ⊗¯R−1
(l+1))s0 +(¯Φ−1
(l) ⊗¯Λ′
(l+1) ¯Σ−1
(l+1)¯Λ(l+1))ˆs(l)]
AΦA′
Φ = (X −¯Z(l+1) ¯C′
(l+1))¯Σ−1
(l+1)(X −¯Z(l+1) ¯C′
(l+1))′ +Ψ,
AχA′
χ = [( ¯S(l+1) −S0) ¯R−1
(l+1)( ¯S(l+1) −S0)′ +Ξ,
while YC, YΣ, YR, YS, YΦ, and Yχ are p×(m+q +1), (n+ν +m+q +1+p+
1)×p, (n+η+m+1)×m, n×m, (p+m+κ+n+1)×n, and (m+ξ+n+1)×n
dimensional matrices respectively, whose elements are random variates from
the standard Scalar Normal distribution. The formulas for the generation of
random variates from the conditional posterior distributions are easily found
from the methods in Chapter 6.
The ﬁrst random variates called the “burn in” are discarded and after doing
so, compute from the next L variates means of the parameters
¯s = 1
L
L

l=1
¯s(l)
¯R = 1
L
L

l=1
¯R(l)
¯c = 1
L
L

l=1
¯c(l)
© 2003 by Chapman & Hall/CRC

¯Σ = 1
L
L

l=1
¯Σ(l)
¯Φ = 1
L
L

l=1
¯Φ(l)
¯χ = 1
L
L

l=1
¯χ(l)
which are the exact sampling-based marginal posterior mean estimates of the
parameters.
Exact sampling-based estimates of other quantities can also be found. Sim-
ilar to Bayesian Regression, Bayesian Factor Analysis, and Bayesian Source
Separation, there is interest in the estimate of the marginal posterior variance
of the matrix containing the Regression and mixing coeﬃcients
var(c|X,U) = 1
L
L

l=1
¯c(l)¯c′
(l) −¯c¯c′
= ¯∆
where c = vec(C) and ¯c = vec( ¯C).
The covariance matrices of the other parameters follow similarly.
14.7.3
Maximum a Posteriori
The joint posterior distribution can also be maximized with respect to the
vector of coeﬃcients c, the vector of sources s, the within source vector co-
variance matrix R, the between source vector covariance matrix χ, the within
observation vector covariance matrix Σ, and the between observation vector
covariance matrix Φ, using the ICM algorithm. To maximize the joint poste-
rior distribution using the ICM algorithm, start with initial values for S, Σ,
c, and R, say ˜S(0), ˜Σ(0), ˜c(0), ˜R(0), and then cycle through
˜χ(l+1) =
Arg Max
χ
p(χ| ˜C(l), ˜Z(l), ˜R(l), ˜Σ(l), ˜Φ(l),X)
=
[( ˜S(l) −S0) ˜R−1
(l) ( ˜S(l) −S0)′ +Ξ
m+ξ
,
˜Φ(l+1) =
Arg Max
Φ
p(Φ| ˜C(l), ˜Z(l), ˜R(l), ˜Σ(l), ˜χ(l+1),X)
=
(X −˜Z(l) ˜C′
(l))˜Σ−1
(l) (X −˜Z(l) ˜C′
(l))′ +Ψ
p+κ
,
˜S(l+1) =
Arg Max
S
p(S| ˜B(l), ˜R(l), ˜Λ(l), ˜Σ(l), ˜Φ(l+1), ˜χ(l+1),U,X),
ˆs(l) = vec[(˜Λ′
(l) ˜Σ−1
(l) ˜Λ(l))−1˜Λ′
(l) ˜Σ−1
(l) (X −U ˜B′
(l))′],
˜s(l+1) = [˜χ−1
(l+1) ⊗˜R−1
(l) + ˜Φ−1
(l+1) ⊗˜Λ′
(l) ˜Σ−1
(l) ˜Λ(l)]−1
×[(˜χ−1
(l+1) ⊗˜R−1
(l) )s0 +(˜Φ−1
(l+1) ⊗˜Λ′
(l) ˜Σ−1
(l) ˜Λ(l))ˆs(l)],
© 2003 by Chapman & Hall/CRC

˜Σ(l+1) =
Arg Max
Σ
p(Σ| ˜C(l), ˜S(l+1), ˜R(l), ˜Φ(l+1), ˜χ(l+1),U,X)
=
(X −˜Z(l+1) ˜C′
(l))′ ˜Φ−1
(l+1)(X −˜Z(l+1) ˜C′
(l))+Q
n+ν
,
˜c(l+1) =
Arg Max
c
p(c| ˜S(l+1), ˜R(l), ˜Σ(l+1), ˜Φ(l+1), ˜χ(l+1)U,X),
ˆc(l) = vec[X′ ˜Z(l+1)( ˜Z′
(l+1) ˜Φ−1
(l+1) ˜Z(l+1))−1],
˜c(l+1) = [∆−1 + ˜Z′
(l+1) ˜Φ−1
(l+1) ˜Z(l+1) ⊗˜Σ−1
(l+1)]−1
×{∆−1c0 +( ˜Z′
(l+1) ˜Φ−1
(l+1) ˜Z(l+1) ⊗˜Σ−1
(l+1))ˆc(l)},
˜R(l+1) =
Arg Max
R
p(R| ˜C(l+1), ˜Z(l+1), ˜Σ(l+1), ˜Φ(l+1), ˜χ(l+1),X)
=
( ˜S(l+1) −S0)′ ˜χ−1
(l+1)( ˜S(l+1) −S0)+V
n+η
,
until convergence is reached with the joint modal estimator for the unobserv-
able parameters (˜c, ˜S, ˜R, ˜χ, ˜Σ, ˜Φ).
14.8
Interpretation
Although the main focus after having performed a Bayesian Source Sepa-
ration is on the separated sources, there are others. One focus as in Bayesian
Regression is on the estimate of the Regression coeﬃcient matrix B which de-
ﬁnes a “ﬁtted” line. Coeﬃcients are evaluated to determine whether they are
statistically “large” meaning that the associated independent variable con-
tributes to the dependent variable or statistically “small” meaning that the
associated independent variable does not contribute to the dependent variable.
The coeﬃcient matrix also has the interpretation that if all of the indepen-
dent variables were held ﬁxed except for one uij which if increased to u∗
ij, the
dependent variable xij increases to an amount x∗
ij given by
x∗
ij = βi0 +···+βiju∗
ij +···+βiquiq.
(14.8.1)
Another focus after performing a Bayesian Source Separation is on the es-
timated mixing coeﬃcients. The mixing coeﬃcients are the amplitudes which
determine the relative contribution of the sources. A particular mixing co-
eﬃcient which is relatively “small” indicates that the corresponding source
does not signiﬁcantly contribute to the associated observed mixed signal. If
a particular mixing coeﬃcient is relatively “large,” this indicates that the
corresponding source does signiﬁcantly contribute to the associated observed
mixed signal.
© 2003 by Chapman & Hall/CRC

14.9
Discussion
Note that particular structures for Φ have been speciﬁed as has been done in
the context of Bayesian Factor Analysis [50] in order to capture more detailed
covariance structures and reduce computational complexity. This could have
also been done for Σ and χ.
© 2003 by Chapman & Hall/CRC

Exercises
1. For the Conjugate model, specify that Φ is a ﬁrst order Markov corre-
lation matrix with ii′th element given by ρ|i−i′|. Assess a generalized
Beta prior distribution for ρ. Derive Gibbs sampling and ICM algo-
rithms [50].
2. For the Conjugate model, specify that Φ is an intraclass correlation
matrix with oﬀdiagonal element given by ρ. Assess a generalized Beta
prior distribution for ρ. Derive Gibbs sampling and ICM algorithms
[50].
3. Specify that Φ and χ have degenerate distributions,
p(Φ) =

1, if Φ = Φ0
0, if Φ ̸= Φ0,
(14.9.1)
and
p(χ) =
 1, if χ = χ0
0, if χ ̸= χ0,
(14.9.2)
which means that Φ and χ are known. Derive Gibbs sampling and ICM
algorithms.
© 2003 by Chapman & Hall/CRC

15
Conclusion
There is a lot of material that needed to be covered in this text.
The
ﬁrst part on fundamential material was necessary in order to properly un-
derstand the Bayesian Source Separation model. I have tried to provide a
coherent description of the Bayesian Source Separation model and how it can
be understood by starting with the Regression model.
Throughout the text, Normal likelihoods with Conjugate and generalized
Conjugate prior distributions have been used. The coherent Bayesian Source
Separation model presented in this text provides the foundation for general-
izations to other distributions.
As stated in the Preface, the Bayesian Source Separation model incorporates
available prior knowledge regarding parameter values and incorporates it into
the inferences. This incorporation of knowledge avoids model and likelihood
constraints which are necessary without it.
© 2003 by Chapman & Hall/CRC

Appendix A
FMRI Activation Determination
A particular source reference function is deemed to signiﬁcantly contribute
to the observed signal if its (mixing) coeﬃcient is “large” in the statistical
sense. Statistically signiﬁcant activation is determined from the coeﬃcients
for source reference functions. If the coeﬃcient is “large,” then the associated
source reference function is signiﬁcant; if it is “small,” then the associated
source reference function is not signiﬁcant. The linear Regression model is
presented in its Multivariate Regression format as in Chapter 7 where voxels
are assumed to be spatially dependent.
Statistically signiﬁcant activation associated with a particular source ref-
erence function is found by considering its corresponding coeﬃcient. Signif-
icance of the coeﬃcients for the linear model is discussed in which spatially
dependent voxels are assumed (with independent voxels being a speciﬁc case)
when the source reference functions are assumed to be observable (known).
The joint distribution of the coeﬃcients for all source reference function for
all voxels is determined. From the joint distribution, the marginal distribu-
tion for the coeﬃcients of a particular source reference function for all voxels
is presented so that signiﬁcance of a particular source reference function can
be determined for all voxels. The marginal distribution of a subset of voxels
for a particular source reference functions coeﬃcients can be derived so that
signiﬁcant activation in a set of voxels can be determined for a given source
reference function. With the above mentioned subset consisting of a single
voxel, the marginal distribution is that of a particular source reference func-
tion coeﬃcient in a given voxel. From this signiﬁcance corresponding to a
particular source reference function in each voxel can be determined.
A.1
Regression
Consider the linear multiple Regression model
xtj = cj0 +cj1zt1 +cj2zt2 +···+cjτztτ +ǫtj
(A.1.1)
in which the observed signal in voxel j at time t is made up of a linear
© 2003 by Chapman & Hall/CRC

combination of the τ observed (known) source reference functions zt1,...,ztτ
plus an intercept term. In terms of vectors the model is
xtj = c′
jzt +ǫtj,
(A.1.2)
where for the Source Separation model c′
j = (β′
j,λ′
j) and z′
t = (u′
t,s′
t).
The linear Regression model for a given voxel j at all n time points is
written in vector form as
Xj = Zcj +Ej,
(A.1.3)
where Xj = (x1j,...,xnj)′ is a n × 1 vector of observed values for voxel j,
Z = (z1,...,zn)′ is an n × (τ + 1) design matrix, cj = (cj0,cj1,...,cjτ)′ is a
(τ + 1) × 1 matrix of Regression coeﬃcients, and Ej is an n × 1 vector of
errors. The model for all p voxels at all n time points is written in its matrix
form as
X = ZC′ +E,
(A.1.4)
where X = (X1,...,Xp) = (x1,...,xn)′ is an n×p matrix of the observed val-
ues, C = (c1,...,cp)′ is a p × (τ + 1) matrix of Regression coeﬃcients, and
E = (E1,...,Ep) = (ǫ1,...,ǫn)′ is an n×p matrix of errors.
With the distributional speciﬁcation that ǫt ∼N(0,Σ) as in the aforemen-
tioned Source Separation model, the likelihood of the observations is
p(X|Z,C,Σ) ∝|Σ|−n
2 e−1
2 trΣ−1(X−ZC′)′(X−ZC′).
(A.1.5)
It is readily seen by performing some algebra in the exponent of the afore-
mentioned likelihood that it can be written as
p(X|Z,C,Σ) ∝|Σ|−n
2 e−1
2 trΣ−1[(C−ˆ
C)Z′Z(C−ˆ
C)′+(X−Z ˆ
C′)′(X−Z ˆ
C′)],
(A.1.6)
where ˆC = X′Z(Z′Z)−1. By inspection or by diﬀerentiation with respect to C
it is seen that ˆC is the value of C which yields the maximum of the likelihood
and thus is the maximum likelihood estimator of the Regression coeﬃcients
C. It can further be seen by diﬀerentiation of Equation A.1.5 with respect to
Σ that
ˆΣ = (X −Z ˆC′)′(X −Z ˆC′)
n
(A.1.7)
is the maximum likelihood estimate of Σ.
It is readily seen that the matrix of coeﬃcients follows a Matrix Normal
distribution given by
© 2003 by Chapman & Hall/CRC

p( ˆC|X,Z,C,Σ) ∝|Z′Z|
p
2 |Σ|−(τ+1)
2
e−1
2 trΣ−1( ˆ
C−C)Z′Z( ˆ
C−C)′
(A.1.8)
and that G = nˆΣ = (X −Z ˆC′)′(X −Z ˆC′) follows a Wishart distribution given
by
p(G|X,Z,Σ) ∝|Σ|−(n−τ−1)
2
|G|
(n−τ−1−p−1)
2
e−1
2 trΣ−1G.
(A.1.9)
It can also be shown as in Chapter 7 that ˆC|Σ and G|Σ are independent.
The distribution of ˆC unconditional of Σ as in Chapter 7 is the Matrix
Student T-distribution given by
p( ˆC|X,Z,C) ∝|G+( ˆC −C)(Z′Z)( ˆC −C)′|−(n−τ+1)+(τ+1))
2
(A.1.10)
which can be written in the more familiar form
p( ˆC|X,Z,C) ∝
1
|W +( ˆC −C)′G−1( ˆC −C)|
(n−τ+1)+(τ+1)
2
,
(A.1.11)
where W = (Z′Z)−1.
General simultaneous hypotheses (which do not assume spatial indepen-
dence) can be performed regarding the coeﬃcient for a particular source ref-
erence function in all voxels (or a subset of voxels) by computing marginal
distributions. It can be shown [17, 41] that the marginal distribution of any
column of the matrix of ˆC, ˆCk is Multivariate Student t-distributed
p( ˆCk|Ck,X,Z) ∝
1
|Wkk +( ˆCk −Ck)′G−1( ˆCk −Ck)|
(n−τ−p)+p
2
(A.1.12)
where Wkk is the kth diagonal element of W. With the marginal distribution
of a column of ˆC, signiﬁcance can be determined for the coeﬃcient of a par-
ticular source reference function for all voxels. Signiﬁcance can be determined
for a subset of voxels for a particular source reference function by determining
the marginal distribution of the subset within ˆCk which is also Multivariate
Student t-distributed. With the subset of voxels being a singleton set, signiﬁ-
cance can be determined for a particular source reference function for a given
voxel with the marginal distribution of the scalar coeﬃcient which is
p( ˆCkj|Ckj,X,Z) ∝
1
|Wkk +( ˆCkj −Ckj)G−1
jj ( ˆCkj −Ckj)|
(n−τ−p)+(τ+1)
2
,
(A.1.13)
© 2003 by Chapman & Hall/CRC

where Gjj = (Xj −Zˆcj)′(Xj −Zˆcj) is the jth diagonal element of G. The
above can be rewritten in the more familiar form
p( ˆCkj|Ckj,X,Z) ∝
1

(n−τ −p)+
( ˆ
Ckj−Ckj)2
(n−τ−p)−1WkkGjj
 n−(τ−1)+1
2
(A.1.14)
which is readily recognizable as a Scalar Student t-distribution. Note that
ˆCkj = cjk and that
t =
( ˆCkj −Ckj)
&
WkkGjj(n−τ −p)−1
(A.1.15)
follows a Scalar Student t-distribution with n−τ −p degrees of freedom and
t2 follows an F distribution with 1 and n−τ −p numerator and denominator
degrees of freedom which is commonly used in Regression [39, 64, 68] derived
from a likelihood ratio test of reduced and full models when testing a single
coeﬃcient, thus allowing a t statistic instead of an F statistic.
To determine statistically signiﬁcant activation in voxels with the Source
Separation model using the standard Regression approach, join the Regression
coeﬃcient and source reference function matrices so that C = (B,Λ) are the
coeﬃcients and Z = (U,S) are the (observable or known) source reference
functions and τ = m +q. The model and likelihood are now in the standard
Regression formats given above.
A.2
Gibbs Sampling
To determine statistically signiﬁcant activation with the Gibbs sampling
approach, use the marginal distribution of the mixing coeﬃcients given in
Equation 12.4.2. General simultaneous hypotheses (which do not assume spa-
tial independence) can be performed regarding the coeﬃcient for a particular
source reference function in all voxels by computing marginal distributions.
It can be shown [17, 41] that the marginal distribution of the kth column of
the mixing matrix ¯Λ, ¯Λk is Multivariate Normal
p(Λk|¯Λk,X,U) ∝| ¯∆k|−1
2 e−1
2 (Λk−¯Λk)′ ¯∆−1
k
(Λk−¯Λk),
(A.2.1)
where ¯∆k is the covariance matrix of ¯Λk found by taking the kth p ×p sub-
matrix along the diagonal of ¯∆.
With the marginal distribution of a column of ¯Λ, signiﬁcance can be de-
termined for the coeﬃcient of a particular source reference function for all
voxels. Signiﬁcance can be determined for a subset of voxels for a particular
source reference function by determining the marginal distribution of the sub-
set within ¯Λk which is also Multivariate Normal. With the subset of voxels
© 2003 by Chapman & Hall/CRC

being a singleton set, signiﬁcance can be determined for a particular source
reference function for a given voxel with the marginal distribution of the scalar
coeﬃcient which is
p(¯Λkj|Λkj,X,U) ∝( ¯∆kj)−1
2 e
−
(Λkj−¯Λkj)2
2 ¯
∆kj
,
(A.2.2)
where ¯∆kj is the jth diagonal element of ¯∆k. Note that ˆΛkj = ˆλjk and that
z = (¯Λkj −Λkj)
,
¯∆kj
(A.2.3)
follows a Normal distribution with a mean of zero and variance of one.
A.3
ICM
To determine statistically signiﬁcant activation with the iterated condi-
tional modes (ICM) approach, use the conditional posterior distribution of
the mixing coeﬃcients given in Equation 12.4.6.
General simultaneous signiﬁcance (which does not assume spatial indepen-
dence) can be determined regarding the coeﬃcient for a particular source
reference function in all voxels by computing marginal distributions. It can
be shown [17, 41] that the marginal distribution of any column of the matrix
of ˜Λ, ˜Λk is Multivariate Normal
p(Λk|˜Λk, ˜B, ˜S, ˜R, ˜Σ,U,X) ∝|Wkk ˜Σ|−1
2 e−1
2 (Λk−˜Λk)′(Wkk ˜Σ)−1(Λj−˜Λj), (A.3.1)
where W = (D−1 + ˜Z′ ˜Z)−1 and Wkk is its kth diagonal element.
With the marginal distribution of a column of ˜Λ, signiﬁcance can be de-
termined for the coeﬃcient of a particular source reference function for all
voxels. Signiﬁcance can be determined for a subset of voxels for a particular
source reference function by determining the marginal distribution of the sub-
set within ˜Λk which is also Multivariate Normal. With the subset of voxels
being a singleton set, signiﬁcance can be determined for a particular source
reference function for a given voxel with the marginal distribution of the scalar
coeﬃcient which is
p(Λkj|˜Λkj, ˜B, ˜S, ˜R, ˜Σjj,U,X) ∝(Wkk ˜Σjj)−1
2 e
−
(˜Λkj−Λkj)2
2Wkk ˜Σjj ,
(A.3.2)
where ˜Σjj is the jth diagonal element of ˜Σ. Note that ˜Λkj = ˜λjk and that
© 2003 by Chapman & Hall/CRC

z = (˜Λkj −Λkj)
,
Wkk ˜Σjj
(A.3.3)
follows a Normal distribution with a mean of zero and variance of one.
After determining the test Statistics, a threshold or signiﬁcance level is set
and a one to one color mapping is performed. The image of the colored voxels
is superimposed onto an anatomical image.
© 2003 by Chapman & Hall/CRC

Appendix B
FMRI Hyperparameter Assessment
The hyperparameters of the prior distributions could be subjectively as-
sessed from a substantive ﬁeld expert, or by use of a previous similar set of
data from which the hyperparameters could be assessed as follows. Denote
the previous data by the n0 × p matrix X0 which has the same experimen-
tal design as the current data. The source reference functions corresponding
to the experimental stimuli are chosen to mimic the experiment with peaks
during the experimental stimuli and valleys during the control stimulus, typ-
ically a square, sine, or triangle wave function with unit amplitude and the
same timing as the experiment. Other source reference functions could be as-
sessed from a substantive ﬁeld expert or possibly from a cardiac or respiration
monitor.
Reparameterizing the prior source reference matrix in terms of columns
instead of rows as S0 = (S01,...,S0m), each of these column vectors is the
time course associated with a source reference function.
Using these a priori values for the source reference functions, the model for
the previous data is
X0 = UB′ +S0Λ′ +E
(B.1)
= Z0C′ +E
(B.2)
with Z0 = (U,S0), C = (B,Λ), and other variables as previously deﬁned. The
likelihood of the previous data is
p(X0|Z0,C,Σ) ∝|Σ|−n0
2 e−1
2 trΣ−1(X−Z0C′)′(X−Z0C′)
(B.3)
which after rearranging the terms in the exponent becomes
p(X0|Z0,C,Σ) ∝|(Z′
0Z0)−1|−(τ+1)
2
|Σ|−n0
2 e−1
2 trΣ−1(C−ˆ
C)(Z′
0Z0)(C−ˆ
C)′, (B.4)
where τ = m + q and ˆC = X′
0Z0(Z′
0Z0)−1. The above can be viewed as the
distribution of C (which is Normal) given the data and the other parameters
with
E(C|X0,Z0,Σ) = ˆC
(B.5)
var(c|X0,Z0,Σ) = (Z′
0Z0)−1 ⊗Σ,
(B.6)
© 2003 by Chapman & Hall/CRC

where c = vec(C). The likelihood can also be viewed as the distribution of Σ
(which is Inverted Wishart) given the data and the other parameters
p(X0|Z0,C,Σ) ∝|Σ|−n0
2 e−1
2 trΣ−1G,
(B.7)
where G = (X −Z0C′)′(X −Z0C′) with
E(Σ|X0,Z0,C) =
G
n0 −2p−2,
(B.8)
var(Σjj|X0,Z0,C) =
Gjj
(n0 −2p−2)2(n0 −2p−4).
(B.9)
Other second order moments are possible and slightly more complicated [41]
but are not be needed.
Hyperparameters are assessed from the previous data using the means and
variances given above, namely,
C0
=
(B0,Λ0)
(B.10)
=
X′
0Z0(Z′
0Z0)−1 = ˆC,
(B.11)
D
=
(Z′
0Z0)−1,
(B.12)
Q
=
(X −Z0 ˆC′)′(X −Z0 ˆC′) = G,
(B.13)
ν = n0.
(B.14)
Under the assumption of spatially independent voxels,
cj0 = (βj0,λj0)
(B.15)
= (Z′
0Z0)−1Z′
0Xj = ˆcj,
(B.16)
Qjj = (Xj −Z0ˆcj)′(Xj −Z0ˆcj) = Gjj,
(B.17)
while D, and ν are as deﬁned above.
The hyperparameters η and V which quantify the variability of the source
reference functions around their mean S0 must be assessed subjectively. The
mean and variance of the Inverted Wishart prior distribution as listed in
Chapter 2 for the covariance matrix of the source reference functions are
E(R) =
V
η −2m−2,
var(rkk) =
2v2
kk
(η−2m−2)2(η−2m−4).
(B.18)
For ease of assessing these parameters V is taken to be diagonal as V = v0Im
thus rkk = r and vkk = v0.
The above means and variances becomes
© 2003 by Chapman & Hall/CRC

E(r) =
v0
η −2m−2,
var(r) =
2v2
0
(η−2m−2)2(η−2m−4),
(B.19)
a system of two equations with two unknowns. Solving for η and v0 yields
η = [E(r)]2
2var(r) +6,
v0 = E(r)(η −4)
(B.20)
and the hyperparameter assessment has been transformed to assessing a prior
mean and variance for the variance of the source reference functions.
© 2003 by Chapman & Hall/CRC

Bibliography
[1] Lee J. Bain and Max Engelhardt. Introduction to Probability and Math-
ematical Statistics.
PWS-Kent Publishing Company, Boston, Massa-
chusetts, USA, second edition, 1992.
[2] Peter Bandettini, Andrzej Jesmanowicz, Eric Wong, and James Hyde.
Processing strategies for time-course data sets in functional mri of the
human brain. Magnetic Resonance in Medicine, 30, 1993.
[3] Thomas Bayes.
An essay towards solving a problem in the doctrine
of chance. Philosophical Transactions of the Royal Society of London,
53:370–418, 1763.
[4] G. M. Becker, M.H. De Groot, and M. Marshak. Measuring utility by a
single response sequential method. Behavioral Science, 9:226–232, 1964.
[5] George E. Box and M. A. Muller. A note on the generation of random
normal deviates. Annals of Mathematical Statistics, 29(610), 1958.
[6] Pierre Common. Independent component analysis: a new concept? Sig-
nal Processing, 36:11–20, 1994.
[7] Robert W. Cox. AFNI: Software for analysis and visualization of func-
tional magnetic resonance neuroimages. Computers and Biomedical Re-
search, 29, 1996.
[8] Robert W. Cox, Andrzej Jesmanowicz, and James S. Hyde. Real-time
functional magnetic resonance imaging. Magnetic Resonance in Medi-
cine, 33, 1995.
[9] Ward Edwards, Harold Lindman, and Leonard J. Savage. A note on
choosing the number of factors. Psychological Review, 70(3), 1963.
[10] Leonard J. Savage et al. The Foundations of Statistical Inference. John
Wiley and Sons (Methuen & Co. London), New York, New York, USA,
1962.
[11] Ross L. Finney and George B. Thomas.
Calculus.
Addison-Wesley
Publishing Company, Reading, Massachusetts, USA, 1990.
[12] Seymour Geisser. Bayesian estimation in multivariate analysis. Annals
of Mathematical Statistics, 36:150–159, 1965.
© 2003 by Chapman & Hall/CRC

[13] Alan E. Gelfand and Adrian F. M. Smith. Sampling based approaches
to calculating marginal densities. Journal of the American Statistical
Association, 85:398–409, 1990.
[14] Stuart Geman and Donald Geman. Stochastic relaxation, Gibbs distri-
butions and the Bayesian restoration of images. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 6:721–741, 1984.
[15] Christopher R. Genovese. A Bayesian time-course model for functional
magnetic resonance imaging. Journal of the American Statistical Asso-
ciation, 95(451):691–719, 2000.
[16] Wally R. Gilks and Pascal Wild. Adaptive rejection sampling for Gibbs
sampling. Journal of the Royal Statistical Society C, 41:337–348, 1992.
[17] Arjun K. Gupta and D. K. Nagar. Matrix Variate Distributions. Chap-
man & Hall/CRC Press, Boca Raton, Florida, USA, 2000.
[18] David A. Harville.
Matrix Algebra from a Statisticians Perspective.
Springer-Verlag Inc., New York, New York, USA, 1997.
[19] W. Keith Hastings. Monte Carlo sampling methods using Markov chains
and their applications. Biometrika, 87:97–109, 1970.
[20] Kentaro Hayashi. The Press-Shigemasu Bayesian Factor Analysis Model
with Estimated Hyperparameters. PhD thesis, Department of Psychol-
ogy, University of North Carolina, Chapel Hill, North Carolina, USA,
1997.
[21] Kentaro Hayashi and Pranab K. Sen. Bias-corrected estimator of factor
loadings in Bayesian factor analysis model. Educational and Psycholog-
ical Measurement, 2001.
[22] Robert V. Hogg and Allen T. Craig.
Introduction to Mathematical
Statistics. Macmillan, New York, New York, USA, fourth edition, 1978.
[23] Harold Hotelling.
Analysis of a complex of statistical variables into
principal components. Journal of Educational Psychology, 24:417–441,
1933.
[24] John Imbrie and Nilva Kipp.
A new micropaleontological method
for quantitative paleoclimatology:
Application to a late pleistocene
caribbean core.
In The Late Cenozoic Glacial Ages, chapter 5. Yale
University Press, New Haven, Connecticut, USA, 1971.
[25] Harold Jeﬀreys. Theory of Probability. Clarendon Press, Oxford, third
edition, 1961.
[26] Maurice Kendall. Multivariate Analysis. Charles Griﬃn & Company,
London, second edition, 1980.
© 2003 by Chapman & Hall/CRC

[27] Kevin Knuth.
Bayesian source separation and localization.
In
A. Mohammad-Djafari, editor, SPIE’98 Proceedings: Bayesian Infer-
ence for Inverse Problems, San Diego, California, pages 147–158, July
1998.
[28] Kevin Knuth. A Bayesian approach to source separation. In J. F. Car-
doso, C. Jutten, and P. Loubaton, editors, Proceedings of the First In-
ternational Workshop on Independent Component Analysis and Signal
Separation: ICA’99, Aussios, France, pages 283–288, 1999.
[29] Kevin Knuth and Herbert G. Vaughan. Convergent Bayesian formula-
tions of blind source separation and electromagnetic source estimation.
In W. von der Linden, V. Dose, R. Fischer, and R. Preuss, editors, Max-
imum Entropy and Bayesian Methods, Munich 1998. Kluwer Dordrecht,
1999, pages 217–226.
[30] Subrahmaniam Kocherlakota and Kathleen Kocherlakota. Bivariate Dis-
crete Distributions. Marcel Dekker, Inc., New York, New York, USA,
1992.
[31] Samuel Kotz and Norman Johnson, editors. Encyclopedia of Statistical
Science, volume 5. John Wiley and Sons, Inc., New York, New York,
USA, 1985, pages 326–333.
[32] Erwin Kreyszig. Advanced Engineering Mathematics. John Wiley &
Sons, Inc., New York, New York, USA, sixth edition, 1988.
[33] A. S. Krishnamoorthy. Multivariate Binomial and Poisson distributions.
Sankhya: The Indian Journal of Statistics, 11(3), 1951.
[34] Sang Eun Lee. Robustness of Bayesian Factor Analysis Estimates. PhD
thesis, Department of Statistics, University of California, Riverside, Cal-
ifornia, USA, December 1994.
[35] Sang Eun Lee and S. James Press. Robustness of Bayesian factor analysis
estimates. Communications in Statistics – Theory and Methods, 27(8),
1998.
[36] Dennis V. Lindley and Adrian F. M. Smith. Bayes estimates for the
linear model. Journal of the Royal Statistical Society B, 34(1), 1972.
[37] Ali Mohammad-Djafari. A Bayesian estimation method for detection,
localisation and estimation of superposed sources in remote sensing. In
SPIE 97 annual meeting, (San Diego, California, USA, July 27-August
1, 1997), 1997.
[38] Ali Mohammad-Djafari. A Bayesian approach to source separation. In
Proceedings of the Nineteenth International Conference on Maximum
Entropy and Bayesian Methods, (August 2-6, 1999, Boise, Idaho, USA),
1999.
© 2003 by Chapman & Hall/CRC

[39] Raymond H. Myers. Classical and Modern Regression with Applications.
Duxbury, Paciﬁc Grove, California, USA, 1990.
[40] Anthony O’Hagen. Kendalls’ Advanced Theory of Statistics, Volume 2B
Bayesian Inference. John Wiley and Sons, Inc., New York, New York,
USA, second edition, 1994.
[41] S. James Press. Applied Multivariate Analysis: Using Bayesian and Fre-
quentist Methods of Inference. Robert E. Krieger Publishing Company,
Malabar, Florida, USA, second edition, 1982.
[42] S. James Press. Bayesian Statistics: Principles, Models, and Applica-
tions. John Wiley and Sons, Inc., New York, New York, USA, 1989.
[43] S. James Press and K. Shigemasu. Bayesian inference in factor analysis.
In Leon J. Gleser and Michael D. Perlman, editors, Contributions to
Probability and Statistics: Essays in Honor of Ingram Olkin, chapter 15.
Springer-Verlag, New York, New York, USA, 1989.
[44] S. James Press and K. Shigemasu. Bayesian inference in factor analysis-
Revised. Technical Report No. 243, Department of Statistics, University
of California, Riverside, California, USA, May 1997.
[45] S. James Press and K. Shigemasu. A note on choosing the number of
factors. Communications in Statistics – Theory and Methods, 29(7),
1999.
[46] Gerald S. Rogers. Matrix Derivatives. Marcel Dekker, New York, New
York, USA, 1980.
[47] Vijay K. Rohatgi. An Introduction to Probability Theory and Math-
ematical Statistics. John Wiley and Sons, Inc., New York, New York,
USA, 1976.
[48] Sheldon M. Ross. Introduction to Probability Models. Academic Press,
Inc., San Diego, California, USA, fourth edition, 1989.
[49] T. J. Rothenberg. A Bayesian analysis of simultaneous equation systems.
Report 6315, Netherlands School of Economics, Econometric Institute,
1963.
[50] Daniel B. Rowe.
Correlated Bayesian Factor Analysis.
PhD thesis,
Department of Statistics, University of California, Riverside, California,
USA, December 1998.
[51] Daniel B. Rowe. A Bayesian factor analysis model with generalized prior
information. Social Science Working Paper 1099, Division of Humanities
and Social Sciences, Caltech, Pasadena, California, USA, August 2000.
[52] Daniel B. Rowe.
Bayesian source separation of fmri signals.
In
A. Mohammad-Djafari, editor, Maximum Entropy and Bayesian Meth-
ods. American Institute of Physics, College Park, Maryland, USA, 2000.
© 2003 by Chapman & Hall/CRC

[53] Daniel B. Rowe. Factorization of separable and patterned covariance
matrices for Gibbs sampling. Monte Carlo Methods and Applications,
6(3), 2000.
[54] Daniel B. Rowe. Incorporating prior knowledge regarding the mean in
Bayesian factor analysis. Social Science Working Paper 1097, Division
of Humanities and Social Sciences, Caltech, Pasadena, California, USA,
July 2000.
[55] Daniel B. Rowe. On estimating the mean in Bayesian factor analysis.
Social Science Working Paper 1096, Division of Humanities and Social
Sciences, Caltech, Pasadena, California, USA, July 2000.
[56] Daniel B. Rowe. A Bayesian model to incorporate jointly distributed
generalized prior information on means and loadings in factor analysis.
Social Science Working Paper 1110, Division of Humanities and Social
Sciences, Caltech, Pasadena, California, USA, February 2001.
[57] Daniel B. Rowe. A Bayesian observable/unobservable source separarion
model and activation determination in FMRI.
Social Science Work-
ing Paper 1120R, Division of Humanities and Social Sciences, Caltech,
Pasadena, California, USA, June 2001.
[58] Daniel B. Rowe.
Bayesian source separarion with jointly distributed
mean and mixing coeﬃcients via MCMC and ICM. Social Science Work-
ing Paper 1118, Division of Humanities and Social Sciences, Caltech,
Pasadena, California, USA, April 2001.
[59] Daniel B. Rowe. Bayesian source separation for reference function de-
termination in fmri. Magnetic Resonance in Medicine, 45(5), 2001.
[60] Daniel B. Rowe. A model for Bayesian factor analysis with jointly dis-
tributed means and loadings. Social Science Working Paper 1108, Divi-
sion of Humanities and Social Sciences, Caltech, Pasadena, California,
USA, January 2001.
[61] Daniel B. Rowe. A model for Bayesian source separarion with the overall
mean. Social Science Working Paper 1118, Division of Humanities and
Social Sciences, Caltech, Pasadena, California, USA, April 2001.
[62] Daniel B. Rowe. A Bayesian approach to blind source separation. Jour-
nal of Interdisciplinary Mathematics, 5(1):49–76, 2002.
[63] Daniel B. Rowe. Bayesian source separation of functional sources. Jour-
nal of Interdisciplinary Mathematics, 5(2), 2002.
[64] Daniel B. Rowe and Steven W. Morgan. Computing fmri activations:
Coeﬃcients and t-statistics by detrending and multiple regression. Tech-
nical Report 39, Division of Biostatistics, Medical College of Wisconsin,
Milwaukee, WI, USA, June 2002.
© 2003 by Chapman & Hall/CRC

[65] Daniel B. Rowe and S. James Press. Gibbs sampling and hill climb-
ing in Bayesian factor analysis. Technical Report No. 255, Department
of Statistics, University of California, Riverside, California, USA, May
1998.
[66] George W. Snedecor. Statistical Methods. Iowa University Press, Ames,
Iowa, USA, 1959.
[67] Aage Volund. Multivariate bioassay. Biometrics, 36:225–236, 1980.
[68] B. Douglas Ward. Deconvolution analysis of FMRI time series data.
AFNI 3dDeconvolve Documentation, Medical College of Wisconsin, Mil-
waukee, Wisconsin, USA, July 2000.
© 2003 by Chapman & Hall/CRC

