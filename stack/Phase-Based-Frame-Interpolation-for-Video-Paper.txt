Phase-Based Frame Interpolation for Video
Simone Meyer1
Oliver Wang2
Henning Zimmer2
Max Grosse2
Alexander Sorkine-Hornung2
1ETH Zurich
2Disney Research Zurich
Abstract
Standard approaches to computing interpolated (in-
between) frames in a video sequence require accurate pixel
correspondences between images e.g. using optical Ô¨Çow. We
present an efÔ¨Åcient alternative by leveraging recent devel-
opments in phase-based methods that represent motion in
the phase shift of individual pixels. This concept allows in-
between images to be generated by simple per-pixel phase
modiÔ¨Åcation, without the need for any form of explicit cor-
respondence estimation. Up until now, such methods have
been limited in the range of motion that can be interpolated,
which fundamentally restricts their usefulness. In order to
reduce these limitations, we introduce a novel, bounded
phase shift correction method that combines phase informa-
tion across the levels of a multi-scale pyramid. Additionally,
we propose extensions for phase-based image synthesis that
yield smoother transitions between the interpolated images.
Our approach avoids expensive global optimization typical
of optical Ô¨Çow methods, and is both simple to implement
and easy to parallelize. This allows us to interpolate frames
at a fraction of the computational cost of traditional op-
tical Ô¨Çow-based solutions, while achieving similar quality
and in some cases even superior results. Our method fails
gracefully in difÔ¨Åcult interpolation settings, e.g., signiÔ¨Åcant
appearance changes, where Ô¨Çow-based methods often in-
troduce serious visual artifacts. Due to its efÔ¨Åciency, our
method is especially well suited for frame interpolation and
retiming of high resolution, high frame rate video.
1. Introduction
Computing interpolated, in-between images is a classic
problem in image and video processing, and is a necessary
step in numerous applications such as frame rate conver-
sion (e.g. between broadcast standards), temporal upsam-
pling for generating slow motion video, image morphing,
as well as virtual view synthesis. Traditional solutions to
image interpolation Ô¨Årst compute correspondences (mostly
leveraging optical Ô¨Çow or stereo methods), followed by
correspondence-based image warping. Due to inherent am-
biguities in computing such correspondences, most meth-
ods are heavily dependent on computationally expensive
global optimization and require considerable parameter tun-
ing [24]. With today‚Äôs trend in the movie and broadcast-
ing industry to higher resolution, higher frame rate video
(e.g. current cameras support 4k resolution at 120 frames
per second and beyond), there is the need for interpolation
techniques that can deal efÔ¨Åciently with this considerably
larger data volume. Standard optical Ô¨Çow techniques based
on global optimization often become inefÔ¨Åcient for interpo-
lating this type of large scale, densely sampled input.
Recently, phase-based methods have shown promising
results in applications such as motion and view extrapola-
tion [10, 27]. These methods rely on the assumption that
small motions can be encoded in the phase shift of an in-
dividual pixel‚Äôs color. Currently, however, the spatial dis-
placement which can be encoded in the phase information
with these methods is highly limited, which narrows their
practical usability.
Contributions.
To overcome this fundamental issue, we
propose a method that propagates phase information across
oriented multi-scale pyramid levels using a novel bounded
shift correction strategy. Our algorithm estimates and ad-
justs the phase shift information using a coarse-to-Ô¨Åne ap-
proach, assuming that high frequency content moves in
a similar way to lower frequency content.
We propose
an adaptive upper bound on the phase shift that effec-
tively avoids artifacts for large motions, and an extension
to phase-based image synthesis that leads to smoother tran-
sitions between interpolated images. In combination, these
extensions considerably increase the amount of displace-
ment that can be successfully represented and interpolated,
rendering phase-based methods practical to be used for gen-
eral motion interpolation tasks, e.g., for high resolution,
high frame rate video.
Based on these extensions, we describe an efÔ¨Åcient
framework to synthesize in-between images, which is sim-
ple to implement and parallelize. Our implementation al-
lows us to interpolate a HD frame in a few seconds on the
CPU and in 1 second on a GPU, while scaling favorably
for larger image sizes. Thus our method only requires a
1

fraction of the time compared to typical Ô¨Çow-based meth-
ods, while having comparable memory requirements and
obtaining visually similar results. In the presence of severe
brightness variations, our approach can even give better re-
sults than optical Ô¨Çow. Furthermore, our method only has
a small set of parameters, most of which were Ô¨Åxed for all
experiments. Evaluations on ground truth data against state-
of-the art techniques using optical Ô¨Çow show similar quality
and favorable performance in terms of efÔ¨Åciency.
2. Related Work
Image interpolating techniques can roughly be classiÔ¨Åed
as either Lagrangian or Eulerian. While Lagrangian meth-
ods model motion as spatial pixel displacement, Eulerian
methods consider the change of color per pixel over time.
Lagrangian methods.
The most popular methods for
Ô¨Ånding pixel correspondences across images are based on
optical Ô¨Çow; see [4, 24] for extensive reviews.
Semi-
nal works include the Ô¨Årst global variational formation by
Horn and Schunck [14] and the local alternative by Lu-
cas and Kanade [16]. We compare to modern global meth-
ods that perform well when used for interpolation, includ-
ing the method of Brox et al. [6], and the currently best-
ranked method for interpolation on the Middlebury bench-
mark [29]. As most optical Ô¨Çow methods rely on global
optimization to resolve matching ambiguities they are dif-
Ô¨Åcult to implement and parallelize, and slow in conver-
gence. There has been some parallel Ô¨Çow implementations
on GPUs which allow to signiÔ¨Åcantly reduce running times
[28, 30]. However, they still do not scale well w.r.t. im-
age size and are restricted by GPU memory, limiting their
applicability for high resolution data. Tao et al. [26] pro-
posed a Ô¨Çow method that replaces global optimization by
edge-aware Ô¨Åltering. They achieve a favorable scaling in
practice, but at the cost of sacriÔ¨Åcing accuracy. Flow meth-
ods are in general sensitive to parameter settings and tend
to produce visual artifacts for strong brightness changes. As
an alternative to optical Ô¨Çow, Mahajan et al. [17] compute
paths in the source images and copy pixel gradients along
them to the in-between images, that are then obtained by
Poisson reconstruction. While alleviating some problems
of optical Ô¨Çow, this method still requires expensive global
optimization and careful parameter tuning. In contrast to
above approaches, our method is simple and local, allowing
for an easy and efÔ¨Åcient parallelization. It scales well w.r.t.
image size, does not require careful parameter tuning, and
fails gracefully in challenging cases.
Some earlier work [11, 12] used phase information di-
rectly in optical Ô¨Çow computations by replacing image
brightness with phase in the data term of standard optical
Ô¨Çow formulations. The main beneÔ¨Åt is that the phase is
more robust than the amplitude w.r.t smooth lighting vari-
ations. More recent methods include reliability measure-
ments and GPU implementations [13, 19]. These methods
use phase information only to match pixels in a standard op-
tical Ô¨Çow formulation, whereas our method interpolates the
phase information directly.
Eulerian methods.
There has been a recent interest in us-
ing Eulerian methods for enhancing subtle motions in a sim-
ple and efÔ¨Åcient way [9, 10, 27]. We aim at extend these
ideas in order to use them for frame interpolation, which
mainly requires to model larger amounts of motion.
Arguably the simplest Eulerian method for interpolating
images is weighted averaging. As no motion is interpo-
lated, ghosting of high frequency content is unavoidable. To
hide ghosting artifacts, multiband blending [7] or gradient
domain blending [20] can be used. Further improvements
can be obtained by matching the visual appearance of the
images before blending, e.g. by manipulating the scales of
a pyramid decomposition [25]. However, all these meth-
ods are designed to create composites of images where the
blending is done along seams or within a mask. Thus non-
trivial modiÔ¨Åcations would be needed to use them to inter-
polate the motion between two images. Image melding [8]
allows blending between images using patch based similar-
ity, but this approach is not guaranteed to yield smooth and
temporally consistent results.
Most related to our work is the method of Didyk et
al. [10] that proposes a phase-based approach to extrapo-
late a pair of stereo images to multi-view content for auto-
multiscopic 3D displays. As we show in our comparisons,
this method is restricted to very small displacements, and
cannot directly be applied for more general frame interpo-
lation for video. To deal with larger displacements as well
as fractional interpolation of views, we present several im-
provements including a novel, bounded shift correction al-
gorithm and modiÔ¨Åcations required for a smooth interpola-
tion between frames.
3. Method
3.1. Phase-based Motion Interpolation
Phase-based approaches build on the insight that the mo-
tion of certain signals can be represented as phase-shift. We
Ô¨Årst explain the basic concepts before introducing our gen-
eralization and modiÔ¨Åcations for image interpolation.
1D Case.
Consider a one dimensional sinusoidal function
shown in Figure 1 which is deÔ¨Åned as y = A sin(œâx ‚àíœÜ),
where A is the amplitude, œâ the angular frequency and œÜ
the phase. A translation of this function can be described
by modifying the phase, e.g. by subtracting œÄ/4 in our ex-
ample. The phase shift œÜshift, which corresponds to the ac-
tual spatial displacement between two translated functions,
is deÔ¨Åned as the phase difference œÜdiff between the two

Figure 1: The translation of two sinusoidal functions (left)
can be interpolated according to Equation (2) (right).
phases of the curves scaled by œâ:
œÜshift = œÜdiff
œâ
.
(1)
Let us now modify the phase difference according to a
weight Œ± ‚àà(0, 1) that describes an intermediate position
between the functions:
y = A sin(œâx‚àíŒ± œÜdiff) = A sin(œâ(x‚àíŒ± œÜshift)) . (2)
The resulting functions then correspond to intermediate si-
nusoids representing the translational motion, see Figure 1.
This idea can be extended to general functions f(x)
translated by a displacement function Œ¥(x) [27]. The shifted
function f(x+Œ¥(t)) can be represented in the Fourier do-
main as a sum of complex sinusoids over all frequencies œâ:
f(x + Œ¥(t)) =
œâ=‚àí‚àû
X
œâ=‚àí‚àû
Rœâ(x, t) ,
(3)
where each sinusoid represents one band Rœâ(x, t)
=
Aœâ eiœâ(x+Œ¥(t)). The corresponding phase œÜœâ = œâ(x+Œ¥(t))
can be directly modiÔ¨Åed w.r.t. Œ±, leading to modiÔ¨Åed bands
ÀÜRœâ(x, t) = Aœâ eiœâ(x+Œ±Œ¥(t)) .
(4)
The in-between functions are then obtained by integrating
the modiÔ¨Åed bands in accordance to (3).
2D Generalization.
For two dimensional functions one
can separate the sinusoids into bands not only according to
the frequency œâ, but also according to spatial orientation
Œ∏, using e.g. the complex-valued steerable pyramid [21, 22,
23]. The steerable pyramid Ô¨Ålters resemble Gabor wavelets
and, when applied to the discrete Fourier transform of an
image, they decompose the input images into a number of
oriented frequency bands Rœâ,Œ∏. The remaining frequency
content which has not been captured in the pyramid levels
is summarized in (real valued) high- and low-pass residuals.
An example for such a decomposition can be found in [27].
Phase computation.
The complex-valued response Rœâ,Œ∏
obtained by applying the steerable Ô¨Ålters Œ®œâ,Œ∏ [21] to an
image I can be written as:
Rœâ,Œ∏(x, y) = (I ‚àóŒ®œâ,Œ∏)(x, y)
(5)
= Aœâ,Œ∏(x, y) eiœÜœâ,Œ∏(x,y)
(6)
= Cœâ,Œ∏(x, y) + i Sœâ,Œ∏(x, y),
(7)
Figure 2: Interpolation of a large phase shift, with the same
initial phase difference as in Figure 1. Adding (in this case)
2œÄ to the phase difference enables correct interpolation of
larger motion. The challenge is how to reliably estimate
such large shifts.
where Cœâ,Œ∏ is the cosine part, representing the even-
symmetric Ô¨Ålter response, and Sœâ,Œ∏ is the sine part,
representing the odd-symmetric Ô¨Ålter response.
From
this we can compute the amplitude Aœâ,Œ∏(x, y)
=
p
Cœâ,Œ∏(x, y)2 + Sœâ,Œ∏(x, y)2, and the phase components
œÜœâ,Œ∏(x, y) = arctan(Sœâ,Œ∏(x, y)/Cœâ,Œ∏(x, y)).
Phase difference.
Based on the assumption that small
motion is encoded in the phase shift, interpolating it re-
quires the computation of the phase difference œÜdiff (see
Equation (1)) between the phases of the two input frames as
œÜdiff = atan2(sin(œÜ1 ‚àíœÜ2), cos(œÜ1 ‚àíœÜ2)) ,
(8)
where atan2 is the four-quadrant inverse tangent. This ap-
proach results in angular values between [‚àíœÄ, œÄ], which
correspond to the smaller angular difference between the
two input phases. It additionally determines the limit of
motion that can be represented, which is bounded by:
|œÜshift| = |œÜdiff|
œâ
‚â§œÄ
œâ ,
(9)
where œâ = 2œÄŒΩ, and ŒΩ being the spatial frequency.
In the multi-scale pyramid, each level represents a par-
ticular band of spatial frequencies ŒΩ ‚àà[ŒΩmin, ŒΩmax]. As-
suming ŒΩmax corresponds to the highest representable fre-
quency on that level, then a phase difference of œÄ corre-
sponds exactly to a shift of one pixel. While this is a rea-
sonable shift for low frequency content represented on the
coarser pyramid levels, it is too limiting for high frequency
content to achieve realistic interpolation results in the pres-
ence of larger motions. Our extensions described below
overcome this limitation.
3.2. Bounded Shift Correction
Large displacements corresponding to a phase difference
of more than œÄ lead to a phase ambiguity. Due to the peri-
odicity of the phase value, the phase difference is only de-
Ô¨Åned between [‚àíœÄ, œÄ] and corresponds to the smallest angu-
lar difference. An example is shown in Figure 2, where the

I
x
(a) Linear blending of the input images
I
x
(b) Shift correction based on absolute values [10]
I
x
(c) Our shift correction based on phase differences
Figure 3: Shift correction comparison. Simple linear blending (a) results in obvious ghosting. The approach of [10] (b)
interpolates, but leads to undesirable oscillations which manifest as ringing and blurring artifacts. Our approach (c) produces
a plausible interpolation and resolves these problems. ( c‚ÉùTom Guilmette [1]).
actual spatial displacement is larger than the original com-
puted phase by a multiple of ¬±2œÄ. Although the input func-
tions look identical to Figure 1, the intermediate sinusoids
differ depending on the assumed phase shift. Interpolation
only works as long as the shift computed on a particular
level mainly captures the frequency content corresponding
to the true motion.
Didyk et al. [10] propose to handle this problem by set-
ting the phase difference at pyramid level l to two times
its absolute value at the next coarser level l+1 whenever
the phase difference becomes greater than œÄ/2. Formally,
if |œÜl+1
diff| > œÄ/2, the corrected phase difference is given
by ÀúœÜ l
diff = 2 œÜl+1
diff. This essentially deÔ¨Ånes a level where
the phase difference of a pixel is assumed to correctly esti-
mate the motion, and then this value is simply copied to the
remaining Ô¨Åner resolution levels, ignoring their own respec-
tive phase values. This leads to a loss of detailed motion for
high frequency content, resulting in artifacts such as ring-
ing and blurring of detail (see Figure 3b). Additionally, this
approach is limited to pyramids constructed using a scal-
ing factor of two, which is known to be suboptimal for the
robustness of image-based multi-scale methods [6].
ConÔ¨Ådence based shifting.
To overcome the limitations
of the above approach, we developed a new conÔ¨Ådence es-
timate for the shift correction which takes all available shift
information into account and interpolates the motion of high
frequency content more robustly. Our approach is based on
the assumption that the phase difference between two reso-
lution levels does not differ arbitrarily, i.e. phase differences
between levels can be used as a conÔ¨Ådence measure that
quantiÔ¨Åes whether the computed phase shift is reliable.
More speciÔ¨Åcally, we try to Ô¨Årst resolve the 2œÄ ambigu-
ity (Figure 2) based on the information of the next coarser
level. Only if the computed shift on the current level l dif-
fers more than a threshold from the coarser level l + 1 we
perform shift correction on level l. To this end, we Ô¨Årst
add multiples of ¬±2œÄ to œÜdiff s.t. the absolute differences
between the phase values of consecutive levels are never
greater than a given tolerance. We use œÄ as a tolerance dis-
tance which modiÔ¨Åes the phase values in such a way that
the phase difference of a pixel between two levels is never
larger than œÄ. Because the original phase differences are
truncated to the range [‚àíœÄ, œÄ], this step allows the exten-
sion of the range in a meaningful way.
The actual shift correction depends on the difference be-
tween two levels, which serves as our conÔ¨Ådence estimate
œï = atan2

sin(œÜl
diff ‚àíŒª œÜl+1
diff), cos(œÜl
diff ‚àíŒª œÜl+1
diff)

,
(10)
where the phase value of the coarser level is scaled ac-
cording to an arbitrary pyramid scale factor Œª > 1 to get
a scale-independent estimate.
If |œï| > œÄ/2, we apply
shift correction and obtain the corrected phase difference
as ÀúœÜ l
diff = Œª œÜl+1
diff. As shown in Figure 3c and in the sup-
plementary video [2], this approach produces considerably
higher quality interpolation results compared to the simpler
correction based on absolute values [10].
Bounded phase shift.
While our shift correction allows
larger motions to be modeled, there is still a limit to the
motion that can be represented without introducing blur-
ring artifacts. We therefore propose an additional enhance-
ment, which limits the admissible phase shifts to well rep-
resentable motions.
To this end we limit the phase difference by a constant
œÜlimit. If the phase value is above this limit, i.e. |œÜl
diff| >
œÜlimit, the phase value from the next coarser level is used
as the corrected phase difference: ÀúœÜ l
diff = Œª œÜl+1
diff.
We deÔ¨Åne œÜlimit depending on the current level l, the
total number of levels L, and the scale factor Œª as
œÜlimit = œÑ œÄ ŒªL‚àíl ,
(11)

(a) Input image [18]
(b) Unbounded
(c) Bounded (œÑ = .1)
Figure 4: Bounding in the shift correction allows to retain
sharpness when interpolating large motion.
where the parameter œÑ ‚àà(0, 1) determines the percentage
of limitation. On the coarsest level we set the corrected
phase difference to zero if its magnitude exceeds œÜlimit.
The effect of this limiting is shown in Figure 4 where
blurring that extends beyond the moving car is avoided as
can be seen in the retained sharpness of the white stripe.
As an additional advantage over prior work [10], our for-
mulation is generalized to arbitrary pyramid scale factors Œª.
Similar to Ô¨Åndings for optical Ô¨Çow based techniques [6],
we found this to be important for high quality results as
with more pyramid levels it becomes possible to represent
sharper transitions, meaning that the phase differences for
higher frequencies are estimated more reliably and accu-
rately. Decreasing the scale factor naturally comes at the
cost of higher memory consumption and increased compu-
tation time. We found a good balance of result quality and
computation time, which we discuss in Section 4.
3.3. Phase Interpolation
For the rest of this section we omit the superscript l de-
noting the pyramid level to improve readability.
Matching phase differences.
We now explain how to
compute a smooth interpolation between phases œÜ1 and œÜ2.
Due to the shift correction, œÜ1 + ÀúœÜdiff is no longer guar-
anteed to match œÜ2, or any equivalent multiple œÜ2 ¬± Œ≥2œÄ,
where Œ≥ ‚ààN0.
In order for the resulting images to be
smoothly interpolated, we must preserve the original com-
puted phases œÜ1 and œÜ2, up to the 2œÄ ambiguity, while still
respecting the shift corrected phase difference ÀúœÜdiff. We do
this by searching for a phase difference ÀÜœÜdiff that is ¬±Œ≥2œÄ
the original phase difference œÜdiff from Equation (8) while
being as close as possible to ÀúœÜdiff, i.e.,
ÀÜœÜdiff = œÜdiff + Œ≥‚àó2œÄ ,
(12)
where Œ≥‚àóis determined as
Œ≥‚àó= argmin
Œ≥

ÀúœÜdiff ‚àí(œÜdiff + Œ≥ 2œÄ)
2
.
(13)
Due to the adjustment we can now compute the phase œÜŒ±
of the interpolated images based on the phase of one input
frame and a fraction of the Ô¨Ånal phase differences ÀÜœÜdiff as
œÜŒ± = œÜ1 + Œ± ÀÜœÜdiff .
(14)
Subtracting Œ± ÀÜœÜdiff from œÜ2 would give the same result.
Blending amplitudes.
To reconstruct the interpolated im-
ages we not only need to interpolate the phase, but also the
low-pass residual and the amplitude. For the amplitude we
found that using the approach of Didyk et al. [10], which
extrapolates using the amplitude value of the closer input
frame, leads to undesirable popping artifacts for interpola-
tion. To obtain a smoother transition, we propose to linearly
blend the amplitude as well as the low frequency residual.
On coarse resolutions these quantities mainly correspond to
the global luminance difference between the input frames,
hence linear blending does not create any visible artifacts.
In combination, both extensions proposed in this section
allow for smooth interpolation between the two images, as
demonstrated in the supplementary video [2]. Additionally
we add back the high-pass residual of the closer input frame
to retain as much high frequency content as possible.
3.4. Algorithm Summary
Algorithm 1 provides a summary of all the key steps
of our method. The handling of the residuals as described
above is omitted for reasons of brevity.
Algorithm 1 The inputs are two images I1 and I2 and inter-
polation parameter Œ±. The output is the interpolated image
IŒ±. P1 and P2 are the steerable pyramid decompositions.
(P1, P2) ‚Üêdecompose(I1, I2)
‚ä≤See [21]
(œÜ1, œÜ2) ‚Üêphase(P1, P2)
(A1, A2) ‚Üêamplitude(I1, I2)
œÜdiff ‚ÜêphaseDifference(œÜ1, œÜ2)
‚ä≤See Eq. 8
for all l = L ‚àí1 : 1 do
ÀúœÜl
diff ‚ÜêshiftCorrection(ÀúœÜl+1
diff)
‚ä≤See Sec. 3.2
end for
ÀÜœÜdiff ‚ÜêadjustPhase(œÜdiff, ÀúœÜdiff)
‚ä≤See Eq. 12
œÜŒ± ‚Üêinterpolate(œÜ1, ÀÜœÜdiff, Œ±)
‚ä≤See Eq. 14
AŒ± ‚Üêblend(A1, A2, Œ±)
‚ä≤See Sec. 3.3
PŒ± ‚Üêrecombine(œÜŒ±, AŒ±)
‚ä≤See [21]
IŒ± ‚Üêreconstruct(PŒ±)
4. Results
We evaluate our approach using images from the Mid-
dlebury dataset [4], from the challenging dataset for stereo
and optical Ô¨Çow [18], as well as ground truth data ob-
tained with high-speed cameras [1, 3]. We compare our

(a) Input images (only one shown) [1, 3]
(b) Optical Ô¨Çow (global) [6]
(c) Our phase-based method
Figure 5: Interpolated frames using optical Ô¨Çow (b) and our
approach (c). Our approach produces similar quality re-
sults while being faster and simpler. ( c‚ÉùTom Guilmette [1],
c‚ÉùVision Research [3])
results to interpolation methods using state-of-the art op-
tical Ô¨Çow, including MDP-Flow [29] which currently ranks
Ô¨Årst on the Middlebury interpolation benchmark, as well the
method of Brox et al. [6] and a GPU Ô¨Çow implementation
(Ô¨Çowlib) [28]. We also include comparisons to the Sim-
pleFlow method [26] that avoids global optimization and to
the local optical Ô¨Çow method of Lucas-Kanade [16] using
a multi-scale pyramid and iterative reÔ¨Ånement [5] imple-
mented in Matlab (and therefore removed from the timings).
For synthesizing the in-between images given the computed
Ô¨Çow Ô¨Åelds, we apply the interpolation algorithm used in the
Middlebury interpolation benchmark [4]. We also compare
to Eulerian methods, including simple linear blending, and
the work of Didyk et al. [10].
Qualitative comparisons.
Figure 5 visually compares
our method to a representative Ô¨Çow-based method and
shows that we obtain visually similar results, which be-
comes even more obvious in the supplementary video [2].
Besides being computationally more expensive, optical
Ô¨Çow-based approaches also fail when brightness constancy
is violated. In these cases, our approach reduces to simple
blending, which is preferable to the artifacts caused by op-
tical Ô¨Çow. See Figure 6 and the supplementary video [2] for
examples with strong lighting changes.
The robustness of our phase-based method in dealing
with brightness changes can also be used to interpolate
scenes with both motion and color change, as color changes
are often encoded in the lower frequencies, which our
(a) Input images [18, 3]
(b) Optical Ô¨Çow (global) [6]
(c) Our phase-based method
Figure 6: Some failure cases for optical Ô¨Çow due to strong
lighting changes that are gracefully handled by our method.
( c‚ÉùVision Research [3])
(a) Input images
(b) Flow [6]
(c) Ours
Figure 7: Interpolating between an images featuring both
scene motion and color change.
phase-based method handles well. An example is shown
in Figure 7 around the border of the sun.
Ground truth comparisons.
We conducted a number of
ground truth comparisons using the leave-some-out method,
i.e. we synthesize intermediate frames and compare these
to the original ones. We use one synthetic example (Roto)
that demonstrates typical artifacts. All other sequences are
extracted from real world footage taken from the Middle-
bury database [4], or from high speed videos recorded at
120+fps [3]. We focus on challenging scenes with many
moving parts as well as changing lighting conditions.
The plot in Figure 8 (left) visualizes the decay in qual-
ity when leaving out an increasing number of intermediate
images (causing larger displacements). We computed er-
ror measurements for different Lagrangian, i.e. Ô¨Çow-based
methods, as well as Eulerian methods. As error measure-
ments we use the sum of squared distances (SSD). The sup-
plementary material also reports results for the perceptually
motivated structural similarity (SSIM) measure. The plot
in Figure 8 (right) visualizes the SSD averaged over several
frames for various sequences when skipping a single frame.
Both plots show that our phase-based method performs
better than other Eulerian approaches, and is on par with
optical Ô¨Çow methods. This observation is also conÔ¨Årmed by
the SSIM error measures shown in the supplementary ma-

Number of Skipped Images
1
2
3
4
5
SSD
0
25
50
Brox et al.
Pyramid LK
Linear Blending
Didyk et al.
Our Method
Sequence
BarrierCouple Face
Hair Handk. Sand
Roto Firemen Light
SSD
0
5
10
15
20
25
30
35
40
45
MDP-Flow2
SimpleFlow
Brox et al.
Ô¨Çowlib
Pyramid LK
Linear Blending
Didyk et al.
Our Method
Figure 8: Error measurements (SSD) for different methods. On the RubberWhale sequence [4] showing quality degradation
with increased motion between frames (left). Averaged over several frames of different sequences, skipping one frame (right).
Example images from these datasets are shown in the supplementary material.
Method
Processing Synthesizing Total Time
MDP-Flow2 [29]
952.1
0.7
952.8
SimpleFlow [26]
‚àó70.2+3.1
0.7
74.0
Brox et al. [6]
25.2
0.7
25.9
Our method (CPU)
3.3
3.3
6.6
Ô¨Çowlib (GPU) [28]
3.1
0.7
3.8
Our method (GPU)
0.9
0.3
1.2
Table 1: Runtimes (in seconds) for different methods mea-
sured when interpolating a 1280x720 (HD) frame.
‚àóFor
SimpleFlow we report the sum of the preprocessing time
(done in Matlab) and the Ô¨Ånal Ô¨Çow computation.
terial. In general, optical Ô¨Çow-based methods give slightly
better error measurements, largely because they introduce
less blur, see Figure 10 for examples and the end of this
section for a detailed discussion of the limitations. In many
cases however, the numerical differences do not correspond
to differences in visual quality, which is our main goal, and
which is best visible in the supplementary video [2].
Concerning parameters, for Pyramid LK we optimized
the smoothness parameter deÔ¨Åned by the window size and
for the method of Brox et al. [6] we used the parameters
proposed by the author of the implementation [15]. Our
method has more intuitive parameters that can mostly be
Ô¨Åxed. We discuss parameters in more detail below.
Running times.
In Table 1 we report running times for
interpolating between a pair of 1280x720 (HD) images.
We use such a rather small resolution to be able to run
all competing methods, as the highly accurate MDP-Flow2
method [29] runs into memory limitations on larger images.
All measurements were performed on a standard desktop
PC (Intel Core i7 3.4GHz, 32GB memory). The GPU mea-
surements were run on a GeForce GTX 770 with 2GB mem-
Image Resolution
360
720
1080
1620 2160
Runtime in [s]
1
10
100
1000
MDP-Flow2
SimpleFlow
Brox et al.
Ô¨Çowlib (GPU)
Our Method (CPU)
Our Method (GPU)
Figure 9: Log-log plot of running time versus (vertical) im-
age resolution when interpolating one image. Our method
scales favorably compared to Ô¨Çow-based methods.
ory.
We report running times for processing the images
and synthesizing one output image. For the phase-based
method, processing includes the pyramid decomposition as
well as our bounded shift correction. Synthesizing an inter-
mediate image includes the smooth interpolation computa-
tions as well as the collapsing of the pyramid. For optical
Ô¨Çow these two steps correspond to computing the optical
Ô¨Çow Ô¨Åeld and the warping procedure, respectively.
For the Ô¨Çow-based methods we use available code;
for Brox et al. [6] we use the C++ implementation of
[15], whereas for MDP-Flow2 [29], SimpleFlow [26] and
Ô¨ÇowÔ¨Çib [28] we use the C / CUDA code provided by the
authors. The image warping using the Ô¨Çow Ô¨Åelds to inter-
polate the intermediate images is implemented in C++. Our
method is implemented and parallelized in C++ on the CPU
and using CUDA on the GPU.
Comparing the times in Table 1 clearly shows that our
method is signiÔ¨Åcantly and consistently faster than any of
the Ô¨Çow-based methods, both for CPU as well as GPU im-
plementations. Additionally, the log-log plot in Figure 9

(a) Input Images [4]
(b) Optical Ô¨Çow [29]
(c) Our phase-based method
Figure 10: Introduced blur of different methods. (a) shows the input images, compared to interpolated images generated by
optical Ô¨Çow (b), and our method (c), which are slightly more blurry.
shows that our method scales favorably w.r.t. the number of
pixels compared to Ô¨Çow-based methods, which is an impor-
tant criterion for processing high resolution data.
Implementation details.
Our phase-based approach has
very few, and intuitive parameters. The main parameters
control the number of orientations and levels corresponding
to the different scales of the pyramid. More levels and ori-
entations allow for a better separation of the motion at the
expense of increased memory and computation time. Un-
less otherwise speciÔ¨Åed we used the following parameters:
The pyramid was constructed using #Œ∏ = 8 number of ori-
entations, a scale factor Œª = 1.2, and the number of levels L
is determined s.t. the coarsest level has a minimal width of
10 pixels. For the limitation factor we use œÑ = 0.2. The size
of the coarsest level together with this choice of œÑ leads to
a theoretical limit of motion which can be modeled reliably
as 2% of the image width. We applied the pyramid decom-
position and the phase modiÔ¨Åcations to each color channel
in Lab color space independently.
Discussion and limitations.
While our method provides
an efÔ¨Åcient alternative to optical Ô¨Çow, it has some limita-
tions. Although we retain more sharpness compared to [10]
(see e.g. Figure 3), our approach still incurs some blurring
even in areas with small motion. This is due to the fact
that the high-pass residual contains the highest frequencies
whose motion is not represented in any pyramid levels and
consequently the motion of this frequency can currently
not be interpolated. In general we thus cannot reach the
same level of detail as Lagrangian approaches that explic-
itly match and warp pixels, see Figure 10. Note however
that the introduced blur is often very subtle. Reintroducing
or preserving high frequencies in phase-based methods is
still an interesting and promising area for future work.
Similar observations apply to large motions of high fre-
quency content that cannot be represented by our phase es-
timation. Here our algorithm degenerates to linear blend-
ing, which however often leads to less objectionable arti-
facts than the entirely wrong motion estimates produced by
optical Ô¨Çow in similar situations. An example is shown in
Figure 11 where interpolating an image between neighbor-
ing frames, i.e., with small motion, the detail of the water re-
(a) Original Frame
(b) Small motion
(c) Large motion
Figure 11: When interpolating between frames with small
displacements (b), our approach can reconstruct high fre-
quency details. For a separation of twenty frames (c), the
larger motion causes blurring. ( c‚ÉùVision Research [3])
mains, whereas when interpolating between images twenty
frames apart, the motion of the water is too large and it be-
comes blurred. On the other hand, the current trend towards
higher frame rate video will surely continue in the foresee-
able future, and thus the apparent motion between frames
will become smaller, and efÔ¨Åciency more important.
5. Conclusions
We have presented a novel method for frame interpo-
lation using a phase-based technique. We compared our
results with state-of-the-art optical Ô¨Çow-based approaches
and found similar visual quality over a number of real world
datasets. For strong illumination changes we even observed
a better performance. Important advantages of our method
are fewer, more intuitive parameters that can mostly be
Ô¨Åxed, as well as a graceful degradation for challenging sce-
narios. Most importantly, our method is computationally
very efÔ¨Åcient and simple to implement and parallelize. All
these make our method a practical tool for frame interpo-
lation on high resolution, high frame rate video, which is
generally challenging for optical Ô¨Çow based techniques.
We hope that this new perspective on a long standing
problem has many interesting areas for future work. Mak-
ing phase computation more robust using spatial similar-
ity would be an interesting direction.
Additionally, one
may combine phase-based with traditional Lagrangian ap-
proaches to generate higher quality results and handle illu-
mination changes more gracefully.

6. Acknowledgements
We would like to thank Tom Guilmette [1] and Vision
Research [3] for providing test footage.
References
[1] http://www.tomguilmette.com/archives/593. 4, 5, 6, 9
[2] http://www.disneyresearch.com/project/phasebased. 4, 5, 6,
7
[3] http://www.visionresearch.com/Gallery. 5, 6, 8, 9
[4] S. Baker, D. Scharstein, J. P. Lewis, S. Roth, M. J. Black,
and R. Szeliski. A database and evaluation methodology for
optical Ô¨Çow. IJCV, 92(1):1‚Äì31, 2011. 2, 5, 6, 7, 8
[5] J.-Y. Bouguet.
Pyramidal implementation of the Lucas
Kanade feature tracker. Intel Corporation, 2000. 6
[6] T. Brox, A. Bruhn, N. Papenberg, and J. Weickert. High ac-
curacy optical Ô¨Çow estimation based on a theory for warping.
In ECCV, pages 25‚Äì36, 2004. 2, 4, 5, 6, 7
[7] P. J. Burt and E. H. Adelson.
A multiresolution spline
with application to image mosaics.
ACM Trans. Graph.,
2(4):217‚Äì236, 1983. 2
[8] S. Darabi, E. Shechtman, C. Barnes, D. B. Goldman, and
P. Sen. Image melding: combining inconsistent images using
patch-based synthesis. ACM Trans. Graph., 31(4):82, 2012.
2
[9] A. Davis, M. Rubinstein, N. Wadhwa, G. J. Mysore, F. Du-
rand, and W. T. Freeman. The visual microphone: passive re-
covery of sound from video. ACM Trans. Graph., 33(4):79,
2014. 2
[10] P. Didyk, P. Sitthi-amorn, W. T. Freeman, F. Durand, and
W. Matusik. Joint view expansion and Ô¨Åltering for automul-
tiscopic 3D displays. ACM Trans. Graph., 32(6):221, 2013.
1, 2, 4, 5, 6, 8
[11] D. J. Fleet and A. D. Jepson. Computation of component
image velocity from local phase information. IJCV, 5(1):77‚Äì
104, 1990. 2
[12] D. J. Fleet and A. D. Jepson. Stability of phase information.
IEEE Trans. Pattern Anal. Mach. Intell., 15(12):1253‚Äì1268,
1993. 2
[13] T. Gautama and M. M. V. Hulle. A phase-based approach to
the estimation of the optical Ô¨Çow Ô¨Åeld using spatial Ô¨Åltering.
IEEE Transactions on Neural Networks, 13(5):1127‚Äì1136,
2002. 2
[14] B. K. P. Horn and B. G. Schunck. Determining optical Ô¨Çow.
Artif. Intell., 17(1-3):185‚Äì203, 1981. 2
[15] C. Liu. Beyond Pixels: Exploring New Representations and
Applications for Motion Analysis. PhD thesis, Massachusetts
Institute of Technology, 2009. 7
[16] B. D. Lucas and T. Kanade.
An iterative image registra-
tion technique with an application to stereo vision. In IJCAI,
pages 674‚Äì679, 1981. 2, 6
[17] D. Mahajan, F. Huang, W. Matusik, R. Ramamoorthi, and
P. N. Belhumeur. Moving gradients: a path-based method
for plausible image interpolation. ACM Trans. Graph., 28(3),
2009. 2
[18] S. Meister, B. J¬®ahne, and D. Kondermann. Outdoor stereo
camera system for the generation of real-world benchmark
data sets. Optical Engineering, 51(02):021107, 2012. 5, 6
[19] K. Pauwels and M. Van Hulle. Realtime phase-based optical
Ô¨Çow on the GPU. In CVPR Workshops, June 2008. 2
[20] P. P¬¥erez, M. Gangnet, and A. Blake. Poisson image editing.
ACM Trans. Graph., 22(3):313‚Äì318, 2003. 2
[21] J. Portilla and E. P. Simoncelli. A parametric texture model
based on joint statistics of complex wavelet coefÔ¨Åcients.
IJCV, 40(1):49‚Äì70, 2000. 3, 5
[22] E. P. Simoncelli and W. T. Freeman.
The steerable pyra-
mid: a Ô¨Çexible architecture for multi-scale derivative com-
putation. In International Conference on Image Processing,
pages 444‚Äì447. IEEE, 1995. 3
[23] E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J.
Heeger. Shiftable multiscale transforms. IEEE Transactions
on Information Theory, 38(2):587‚Äì607, 1992. 3
[24] D. Sun, S. Roth, and M. J. Black. Secrets of optical Ô¨Çow
estimation and their principles. In CVPR, pages 2432‚Äì2439.
IEEE, 2010. 1, 2
[25] K. Sunkavalli, M. K. Johnson, W. Matusik, and H. PÔ¨Ås-
ter. Multi-scale image harmonization. ACM Trans. Graph.,
29(4), 2010. 2
[26] M. W. Tao, J. Bai, P. Kohli, and S. Paris. SimpleÔ¨Çow: A non-
iterative, sublinear optical Ô¨Çow algorithm. Comput. Graph.
Forum, 31(2):345‚Äì353, 2012. 2, 6, 7
[27] N. Wadhwa, M. Rubinstein, F. Durand, and W. T. Freeman.
Phase-based video motion processing. ACM Trans. Graph.,
32(4):80, 2013. 1, 2, 3
[28] M. Werlberger, W. Trobin, T. Pock, A. Wedel, D. Cremers,
and H. Bischof.
Anisotropic Huber-L1 optical Ô¨Çow.
In
BMVC, pages 1‚Äì11, 2009. 2, 6, 7
[29] L. Xu, J. Jia, and Y. Matsushita. Motion detail preserving
optical Ô¨Çow estimation. IEEE Trans. Pattern Anal. Mach.
Intell., 34(9):1744‚Äì1757, 2012. 2, 6, 7, 8
[30] C. Zach, T. Pock, and H. Bischof. A duality based approach
for realtime TV-L1 optical Ô¨Çow. In DAGM, pages 214‚Äì223,
2007. 2

