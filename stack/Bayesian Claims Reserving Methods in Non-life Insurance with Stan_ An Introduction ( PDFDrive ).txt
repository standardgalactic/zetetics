Bayesian Claims 
Reserving Methods 
in Non-life 
Insurance with Stan
Guangyuan Gao   
An Introduction

Bayesian Claims Reserving Methods in Non-life
Insurance with Stan

Guangyuan Gao
Bayesian Claims Reserving
Methods in Non-life
Insurance with Stan
An Introduction
123

Guangyuan Gao
School of Statistics
Renmin University of China
Beijing, China
ISBN 978-981-13-3608-9
ISBN 978-981-13-3609-6
(eBook)
https://doi.org/10.1007/978-981-13-3609-6
Library of Congress Control Number: 2018963287
© Springer Nature Singapore Pte Ltd. 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore

Preface
Bayesian models are very popular in non-life claims reserving. This monograph
provides a review of Bayesian claims reserving models and their underlying
Bayesian inference theory. It investigates three types of claims reserving models in
Bayesian framework: chain ladder models, basis expansion models involving tail
factor, and multivariate copula models. One of the core techniques in Bayesian
modeling is inferential methods. This monograph largely relies on Stan, a spe-
cialized software environment which applies Hamiltonian Monte Carlo method and
variational Bayes. This monograph has the following three distinguishing features:
• It has a thorough review of various aspects of Bayesian statistics and relates
them to claims reserving problems.
• It addresses three important points in claims reserving: tail development,
stochastic version of payments per claim incurred method, and aggregation of
liabilities from correlated portfolios.
• It provides explicit Stan code for non-life insurance claims reserving.
Beijing, China
Guangyuan Gao
September 2018
v

Acknowledgements
I am very grateful to Borek Puza, Richard Cumpston, Hanlin Shang, Tim Higgins,
Bronwen Whiting, Steven Roberts, and Xu Shi at the Australian National
University. The discussion with them helps improve the quality of this monograph
signiﬁcantly. I am also very grateful to Chong It Tan and Yanlin Shi at the
Macquarie University, and Shengwang Meng at the Renmin University of China.
They provide constructive comments. I would like to thank the Research School of
Finance, Actuarial Studies and Statistics at the Australian National University for
the ﬁnancial support. I also would like to thank the School of Statistics at the
Renmin University of China for the research funds including National Social
Science Fund of China (Grant No. 16ZDA052), MOE National Key Research
Bases for Humanities and Social Sciences (Grant No. 16JJD910001), and the funds
for building world-class universities (disciplines).
vii

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Bayesian Inference and MCMC . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Bayesian Claims Reserving Methods . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Monograph Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.4
The General Notation Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2
Bayesian Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1
Bayesian Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.1
The Single-Parameter Case . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1.2
The Multi-parameter Case . . . . . . . . . . . . . . . . . . . . . . . .
15
2.1.3
Choice of Prior Distribution . . . . . . . . . . . . . . . . . . . . . . .
16
2.1.4
Asymptotic Normality of the Posterior Distribution . . . . . .
20
2.2
Model Assessment and Selection . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2.1
Posterior Predictive Checking . . . . . . . . . . . . . . . . . . . . . .
21
2.2.2
Residuals, Deviance and Deviance Residuals. . . . . . . . . . .
25
2.2.3
Bayesian Model Selection Methods . . . . . . . . . . . . . . . . .
28
2.2.4
Overﬁtting in the Bayesian Framework . . . . . . . . . . . . . . .
32
2.3
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
3
Advanced Bayesian Computation
. . . . . . . . . . . . . . . . . . . . . . . . . .
35
3.1
Markov Chain Monte Carlo (MCMC) Methods . . . . . . . . . . . . . .
35
3.1.1
Markov Chain and Its Stationary Distribution . . . . . . . . . .
36
3.1.2
Single-Component Metropolis-Hastings (M-H)
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.1.3
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.1.4
Hamiltonian Monte Carlo (HMC) . . . . . . . . . . . . . . . . . . .
43
ix

3.2
Convergence and Efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.2.1
Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.2.2
Efﬁciency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.3
OpenBUGS and Stan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.3.1
OpenBUGS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.3.2
Stan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.4
Modal and Distributional Approximations . . . . . . . . . . . . . . . . . .
58
3.4.1
Laplace Approximation . . . . . . . . . . . . . . . . . . . . . . . . . .
58
3.4.2
Variational Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
3.5
A Bayesian Hierarchical Model for Rats Data . . . . . . . . . . . . . . .
60
3.5.1
Classical Regression Models . . . . . . . . . . . . . . . . . . . . . .
60
3.5.2
A Bayesian Bivariate Normal Hierarchical Model . . . . . . .
63
3.5.3
A Bayesian Univariate Normal Hierarchical Model . . . . . .
66
3.5.4
Reparameterization in the Gibbs Sampler . . . . . . . . . . . . .
68
3.6
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
4
Bayesian Chain Ladder Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.1
Non-life Insurance Claims Reserving Background . . . . . . . . . . . .
73
4.1.1
Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.1.2
Run-Off Triangles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.1.3
Widely-Used Claims Reserving Methods . . . . . . . . . . . . .
76
4.2
Stochastic Chain Ladder Models . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.2.1
Frequentist Chain Ladder Models . . . . . . . . . . . . . . . . . . .
78
4.2.2
A Bayesian Over-Dispersed Poisson (ODP) Model . . . . . .
84
4.3
A Bayesian ODP Model with Tail Factor. . . . . . . . . . . . . . . . . . .
90
4.3.1
Reversible Jump Markov Chain Monte Carlo . . . . . . . . . .
92
4.3.2
RJMCMC for Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.4
Estimation of Claims Liability in WorkSafe VIC . . . . . . . . . . . . .
98
4.4.1
Background of WorkSafe Victoria . . . . . . . . . . . . . . . . . .
98
4.4.2
Estimation of the Weekly Beneﬁt Liability
Using Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
4.4.3
Estimation of the Doctor Beneﬁt Liability Using
a Compound Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
103
4.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
4.6
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
5
Bayesian Basis Expansion Models . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.1
Aspects of Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.1.1
Basis Functions of Splines . . . . . . . . . . . . . . . . . . . . . . . .
118
5.1.2
Smoothing Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
5.1.3
Low Rank Thin Plate Splines . . . . . . . . . . . . . . . . . . . . . .
122
5.1.4
Bayesian Splines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
x
Contents

5.2
Two Simulated Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
5.2.1
A Model with a Trigonometric Mean Function
and Normal Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
5.2.2
A Gamma Response Variable with a Log-Logistic
Growth Curve Mean Function . . . . . . . . . . . . . . . . . . . . .
133
5.3
Application to the Doctor Beneﬁt . . . . . . . . . . . . . . . . . . . . . . . .
143
5.3.1
Claims Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
5.3.2
PPCI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
5.3.3
Combining the Ultimate Claims Numbers
with the Outstanding PPCI . . . . . . . . . . . . . . . . . . . . . . . .
147
5.3.4
Computing Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
5.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
150
5.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
6
Multivariate Modelling Using Copulas . . . . . . . . . . . . . . . . . . . . . . .
153
6.1
Overview of Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
6.1.1
Sklar’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
6.1.2
Parametric Copulas . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
6.1.3
Measures of Bivariate Association . . . . . . . . . . . . . . . . . .
157
6.1.4
Inference Methods for Copulas . . . . . . . . . . . . . . . . . . . . .
159
6.2
Copulas in Modelling Risk Dependence . . . . . . . . . . . . . . . . . . . .
167
6.2.1
Structural and Empirical Dependence Between Risks . . . . .
168
6.2.2
The Effects of Empirical Dependence on Risk
Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6.3
Application to the Doctor and Hospital Beneﬁts . . . . . . . . . . . . . .
170
6.3.1
Preliminary GLM Analysis Using a Gaussian Copula . . . .
171
6.3.2
A Gaussian Copula with Marginal Bayesian Splines . . . . .
174
6.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
6.5
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
7
Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.1
The Three Claims Reserving Models . . . . . . . . . . . . . . . . . . . . . .
185
7.1.1
A Compound Model . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.1.2
A Bayesian Natural Cubic Spline Basis Expansion
Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
7.1.3
A Copula Model with Bayesian Margins. . . . . . . . . . . . . .
187
7.2
A Suggested Bayesian Modelling Procedure. . . . . . . . . . . . . . . . .
188
7.3
Other Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
7.3.1
Bayesian Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
7.3.2
Actuarial Applications . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
Contents
xi

Appendix A: Derivations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
Appendix B: Other Sampling Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
xii
Contents

Chapter 1
Introduction
Abstract This chapter brieﬂy reviews Bayesian statistics, Markov chain Monte
Carlo methods, and non-life insurance claims reserving methods. Some of the most
inﬂuential literature are listed in this chapter. Two Bayesian inferential engines,
BUGS and Stan, are introduced. At the end the monograph structure is given and the
general notation is introduced.
1.1
Bayesian Inference and MCMC
The foundation of Bayesian data analysis is Bayes’ theorem, which derives from
Bayes (1763). Although Bayes’ theorem is very useful in principle, Bayesian statis-
tics developed more slowly in the 18th and 19th centuries than in the 20th century.
Statistical analysis based on Bayes’ theorem was often daunting because of the
extensive calculations, such as numerical integrations, required. Perhaps the most
signiﬁcant advances to Bayesian statistics in the period just after Bayes’ death were
made by Laplace (1785, 1810).
In the 20th century, the development of Bayesian statistics continued, charac-
terised by Jeffreys (1961), Lindley (1965) and Box and Tiao (1973). At the time
these monographs were written, computer simulation methods were much less con-
venient than they are now, so they restricted their attention to conjugate families and
devoted much effort to deriving analytic forms of marginal posterior densities.
Thanks to advances in computing, millions of calculations can now be performed
easily in a single second. This removes the prohibitive computational burden involved
in much Bayesian data analysis. At the same time, computer-intensive sampling
methods have revolutionized statistical computing and hence the application of
Bayesian methods. They have profoundly impacted the practice of Bayesian statis-
tics by allowing intricate models to be posited and used in disciplines as diverse as
biostatistics and economics.
Bayesian inference
Compared with the frequentist approach, the Bayesian paradigm has the advantages
of intuitive interpretation of conﬁdence interval, fully deﬁned predictive distributions
and a formal mathematical way to incorporate the expert’s prior knowledge of the
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_1
1

2
1
Introduction
parameters. For example, a Bayesian interval for an unknown quantity of interest
can be directly regarded as having a high probability of containing the unknown
quantity. In contrast, a frequentist conﬁdence interval may strictly be interpreted
only in relation to a sequence of similar inferences that might be made in repeated
practice.
The central feature of Bayesian inference, the direct quantiﬁcation of uncertainty,
means that there is no impediment in principle to ﬁtting models with many parame-
ters and complicated multi-layered probability speciﬁcations. The freedom to set up
complex models arises in large part from the fact that the Bayesian paradigm provides
a conceptually simple method for dealing with multiple parameters. In practice, the
problems that do exist are ones of setting up and computing with such large mod-
els and we devote a large part of this monograph to recently developed, and still
developing, techniques for handling these modelling and computational challenges.
Markov chain Monte Carlo methods
Among Bayesian computational tools, Markov chain Monte Carlo (MCMC) meth-
ods (Metropolis et al. 1953; Hastings 1970) are the most popular. The Metropolis
algorithm (Metropolis et al. 1953) was ﬁrst used to simulate a liquid in equilibrium
with its gas phase. Hastings (1970) generalized the Metropolis algorithm, and simula-
tions following his scheme are said to use the Metropolis-Hastings (M-H) algorithm.
A special case of the Metropolis-Hastings algorithm was introduced by Geman and
Geman (1984). Simulations following their scheme are said to use the Gibbs sampler.
Gelfand and Smith (1990) made the wider Bayesian community aware of the Gibbs
sampler, which up to that time had been known only in the spatial statistics commu-
nity. It was rapidly realized that most Bayesian inference could be done by MCMC.
Green (1995) generalized the M-H algorithm, as much as it can be generalized.
In the context of a Bayesian model, MCMC methods can be used to generate
a Markov chain whose stationary distribution is the posterior distribution of the
quantity of interest. Statisticians and computer scientists have developed software
packages such as BUGS (Lunn et al. 2012) and Stan (Gelman et al. 2014) to imple-
ment MCMC methods for user-deﬁned Bayesian models. Hence, practitioners from
other areas without much knowledge of MCMC can create Bayesian models and
perform Bayesian inference with relative ease.
The BUGS project started in 1989 at the MRC Biostatistics Unit in Cambridge,
parallel to and independent of the classic MCMC work of Gelfand and Smith (1990).
Nowadays there are two versions of BUGS: WinBUGS and OpenBUGS. WinBUGS
is an older version and will not be further developed. OpenBUGS represents “the
future of the BUGS project”.
Stan is a relatively new computing environment which applies Hamiltonian Monte
Carlo (Duane et al. 1987; Neal 1994) and variational Bayes (Jordan et al. 1999). Stan
was ﬁrst introduced in Gelman et al. (2014). The BUGS examples (volume 1–3) are
translated into Stan as shown in the Stan GitHub Wiki. In this monograph, we largely
rely on Stan for doing Bayesian inference.

1.2 Bayesian Claims Reserving Methods
3
1.2
Bayesian Claims Reserving Methods
Recent attempts to apply enterprise risk management (ERM) principles to insurance
have placed a high degree of importance on quantifying the uncertainty in the various
necessary estimates, using stochastic models. For general insurers, the most impor-
tant liability is the reserve for unpaid claims. Over the years a number of stochastic
models have been developed to address this problem (Taylor 2000; Wüthrich and
Merz 2008, 2015).
In many countries, loss reserves are the single largest liability on the insurance
industry’s balance sheet. The delayed and stochastic nature of the timing and amount
of loss payments makes the insurance industry unique, and it effectively dominates or
deﬁnesmuchoftheﬁnancialmanagementandriskandopportunitymanagementofan
insurance company. For example, insurers are typically hesitant to utilize a signiﬁcant
amount of debt in their capital structure, as their capital is already leveraged by
reserves. Also, the characteristics of unpaid loss liabilities heavily inﬂuence insurer
investment policy.
The claims reserving problem is not only about the expected value of claims
liability, but also the distribution of claims liability (Taylor 2000; Wüthrich and Merz
2008). The predictive distribution of unpaid claims is vital for risk management, risk
capital allocation and meeting the requirements of Solvency II (Christiansen and
Niemeyer 2014) etc.
A feature of most loss reserve models is that they are complex, in the sense
that they have a relatively large number of parameters. It takes a fair amount of
effort to derive a formula for the predictive distribution of future claims from a
complex model with many parameters (Mack 1993, 1999, 2008). Taking advantage
of ever-increasing computer speeds, England and Verrall (2002) pass the work on
to computers using a bootstrapping methodology with the over-dispersed Poisson
model. With the relatively recent introduction of MCMC methods (Gelfand and
Smith 1990), complex Bayesian stochastic loss reserve models are now practical in
the current computing environment.
Bayesian inference can often be viewed in terms of credibility theory, where the
posterior distribution is a weighted average of the prior and likelihood. The idea
of credibility was widely used in actuarial science a long time ago (Whitney 1918;
Longley-Cook 1962; Bühlmann 1967). Often reasonable judgements by experienced
actuaries can override the signals in unstable data. Also, an insurance company
may not have enough “direct” data available to do a “credible” analysis. Bayesian
credibility theory provides a coherent framework for combining the “direct” data with
either subjective judgements or collateral data so as to produce a useful “credibility
estimate” (Mayerson 1964).
Setting a median reserve will lead to a half chance of insolvency, which deﬁnitely
violates the policyholders’ interest and will not meet the regulators’ requirements.
The insurers care more about the tail behaviour of future claims. Normally they hold
the economic capital deﬁned as a remote quantile of future claims distribution so as
to ensure a low probability of insolvency.

4
1
Introduction
Furthermore, the insurers may have several lines of business, such as automobile,
commercial general liability, commercial property, homeowners etc. It is good for
such multi-line insurers to know not only which lines have higher net proﬁt but also
which are riskier so they can compare the risk-adjusted return between lines. The risk
cannot be characterised just by standard errors, since the claims amounts are always
heavy-tailed. We are more interested in the tail-based risk measures such as value-
at-risk (Brehm et al. 2007), which can be estimated from the predictive distribution
of future claims.
Each line of insurance is typically modelled with its own parameters, but ulti-
mately the distribution of the sum of the lines is needed. To get the distribution of
the sum, the dependencies among the lines must be taken into account. For example,
if there are catastrophic events, all of the property damage lines could be hit at the
same time. Legislation changes could hit all of the liability lines. When there is the
possibility of correlated large losses across lines, the distribution of the sum of the
lines gets more probability in the right tail.
Unfortunately, even though the univariate distribution of the sum is the core
requirement, with dependent losses the multivariate distribution of the individual
lines is necessary to obtain the distribution of the sum. That quickly leads to the
realm of copulas (Joe 2014), which provide a convenient way to combine individual
distributions into a single multivariate distribution.
1.3
Monograph Structure
Two chapters of this monograph focus on Bayesian methodology and three chapters
on the application of Bayesian methods to claims reserving in non-life insurance.
In Chap. 2, we provide a broad overview of Bayesian inference, making compar-
isons with the frequentist approach where necessary. Model assessment and selection
in the Bayesian framework are reviewed. Some toy examples are used to illustrate
the main concepts.
In Chap. 3, Bayesian computational methods are reviewed. These computational
methods will be employed later in the monograph. As we mentioned before, the
popularity of Bayesian modelling is largely due to the development of Bayesian
computational methods and advances in computing. A knowledge of Bayesian com-
putational methods lets us feel more conﬁdent with using a “black box” such as
OpenBUGS or Stan. Moreover, with the computational methods at our disposal, we
may develop our own algorithm for some special models which cannot be solved by
any available package. To end this chapter, we do a full Bayesian analysis of a hier-
archical model for biology data in Gelfand et al. (1990). This model has a connection
with random effects models discussed in Chap. 4.
The next three chapters constitute an application of Bayesian methods to a data
set from WorkSafe Victoria which provides the compulsory workers compensation
insurance for all companies in Victoria except the self-insured ones. The data set
includes claims histories of various beneﬁt types from June 1987 to June 2012.

1.3 Monograph Structure
5
In Chap. 4, the parametric Bayesian models for the run-off triangle are inves-
tigated. We ﬁrst review the time-honoured Mack’s chain ladder models (Mack
1993, 1999) and Bornhuetter-Ferguson models (Bornhuetter and Ferguson 1972),
which have been widely used in actuarial science for decades. Then the more recent
Bayesian chain ladder models with an over-dispersed Poisson error structure (Eng-
land et al. 2012) are studied. Reversible jump Markov chain Monte Carlo (RJMCMC)
is discussed in this chapter for the purpose of dealing with the tail development com-
ponent in the models. Finally, we apply the models discussed above to estimate the
claims liabilities for the weekly beneﬁt and the doctor beneﬁt in WorkSafe Victoria.
For the doctor beneﬁt, we propose a compound model as a stochastic version of the
payments per claim incurred (PPCI) method.
Chapter 5 investigates Bayesian basis expansion models with shrinkage priors
and their applications to claims reserving. We ﬁrst summarize some aspects of basis
expansion models (Hastie et al. 2009). Among all the basis expansion models, the
Bayesian natural cubic spline basis expansion model with shrinkage priors is our
favourite. Two simulated examples are studied to illustrate two advantages of this
model: the shorter computational time and the better tail extrapolation. The second
simulated example is designed to mimic the mechanism of claims payments. Finally,
we reanalyze the doctor beneﬁt using the proposed Bayesian basis expansion model
and compare the results with those in Chap. 4 and the PwC report (Simpson and
McCourt 2012).
In Chap. 6, Bayesian copula models are used to aggregate the estimated claims
liabilities from two correlated run-off triangles. In the ﬁrst section, we review Sklar’s
theorem, several parametric copulas, and inferential methods. A simulated example
is used to demonstrate the inference functions for margins (IFM) method (Joe and
Xu 1996). In the second section, we discuss the usefulness of copulas in modelling
risk dependence. Ignorance of risk dependence does not affect the aggregated mean
too much, but it will affect the more interesting tail-based risk measures signiﬁcantly.
In the third section, we aggregate two correlated beneﬁts in WorkSafe Victoria: the
doctor beneﬁt and the hospital beneﬁt. The marginal regression for each beneﬁt is
the same as in Chap. 5.
Chapter 7 provides a summary of the monograph and discusses limitations and
further research topics. It includes remarks about the three most useful stochastic
claims reserving models in the monograph and suggests alternative Bayesian mod-
elling procedures.
There are two appendices. Appendix A supplies the technical complements to sup-
port the examples in Chaps. 2 and 3. Appendix B lists some Bayesian computational
methods not included in Chap. 3 and relevant proofs.
In each chapter, all ﬁgures and tables appear together at the end, in that order.

6
1
Introduction
1.4
The General Notation Used
By default, vectors are column vectors. If we write θ = (α, β), we mean θ is a column
vector with two elements. A lower case letter is a column vector or a scalar. A matrix
is denoted by a bold upper case letter.
Data
Bold and lower case Roman letters represent the observed data vector. For example,
yyy might be an n-vector of observed response values. A bold and upper case Roman
letter could represent a design matrix. For example, XXX might represent an n × p
matrix of observed predictors.
Parameters
Non-bold and lower case Greek letters represent the parameters. For example,
θ can be a vector containing p parameters. Bold and upper case Greek letters might
represent a covariance matrix. ΣΣΣ can be a p × p covariance matrix.
Functions
Unlessstatedotherwise,alltheprobabilitydensity(ormass)functionsarerepresented
by p and all the cumulative distribution functions are represented by F. Other generic
functions are typically represented by f, g, h, π.
Conditional distributions
The distribution of data is conditional on the parameters and the prior of parameters
is conditional on the hyperparameters. For example, a normal-normal-gamma model
with unknown mean and variance is formally written as follows:
y|μ, σ 2 ∼N(μ, σ 2)
μ|σ 2 ∼N(μ0, σ 2
0 )
σ 2 ∼Inv-Gamma(α, β).
For compactness, we will typically assume an implicit conditioning on the parameters
going down the page. For example the normal-normal-gamma model above could
also be written as follows:
y ∼N(μ, σ 2)
μ ∼N(μ0, σ 2
0 )
σ 2 ∼Inv-Gamma(α, β).
For the posterior distributions, we always include the conditioning parts to emphasize
the meaning of “posterior”. For example, the posterior distribution of μ is denoted
by p(μ|yyy), the full conditional posterior distribution of μ is denoted by p(μ|yyy, σ)
or p(μ|·), and the posterior predictive distribution is denoted by p(y′|yyy).

References
7
References
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society, 330–418.
Bornhuetter, R. L., & Ferguson, R. E. (1972). The actuary and IBNR. Proceedings of the Casualty
Actuarial Society, 59, 181–195.
Box, G. E., & Tiao, G. C. (1973). Bayesian inference in statistical analysis. New York: Wiley
Classics.
Brehm, P. J., Gluck, S., Kreps, R., Major, J., Mango, D., & Shaw, R., et al. (2007). Enterprise risk
analysis for property and liability insurance companies: A practical guide to standard models
and emerging solutions. New York: Guy Carpenter & Company.
Bühlmann, H. (1967). Experience rating and credibility. ASTIN Bulletin, 4, 199–207.
Christiansen, M. C., & Niemeyer, A. (2014). The fundamental deﬁnition of the solvency capital
requirement in Solvency II. ASTIN Bulletin, 44, 501–533.
Duane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid Monte Carlo. Physics
Letters B, 195, 216–222.
England, P. D., & Verrall, R. J. (2002). Stochastic claims reserving in general insurance. British
Actuarial Journal, 8, 443–518.
England, P. D., Verrall, R. J., & Wüthrich, M. V. (2012). Bayesian over-dispersed poisson model
and the Bornhuetter-Ferguson claims reserving method. Annals of Actuarial Science, 6, 258–283.
Gelfand, A. E., & Smith, A. F. M. (1990). Sampling-based approaches to calculating marginal
densities. Journal of the American Statistical Association, 85, 398–409.
Gelfand, A. E., Hills, S. E., Racinepoon, A., & Smith, A. F. M. (1990). Illustration of Bayesian-
inference in normal data models using Gibbs sampling. Journal of the American Statistical
Association, 85, 972–985.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.).
Boca Raton: Chapman & Hall.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6, 721–
741.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrika, 82, 711–732.
Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining,
inference, and prediction (2nd ed.). Springer, New York.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57, 97–109.
Jeffreys, H. (1961). Theory of probability (3rd ed.). London: Oxford University Press.
Joe, H., & Xu, J. J. (1996). The estimation method of inference functions for margins for multivariate
models. http://hdl.handle.net/2429/57078.
Joe, H. (2014). Dependence modeling with copulas. New York: Chapman & Hall.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational
methods for graphical models. Machine Learning, 37, 183–233.
Laplace, P. S. (1785). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres. In Memoires de l’Academie Royale des Sciences.
Laplace, P. S. (1810). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres, et sur leur application aux probabilites. In Memoires de l’Academie des Science
de Paris.
Lindley, D. V. (1965). Introduction to probability and statistics from Bayesian viewpoint (Vol. 2).
Cambridge: Cambridge University Press.
Longley-Cook, L. H. (1962). An introduction to credibility theory. Proceedings of the Casualty
Actuarial Society, 49, 194–221.
Lunn,D.,Jackson,C.,Best,N.,Thomas,A.,&Spiegelhalter,D.(2012).TheBUGSbook:Apractical
introduction to Bayesian analysis. Boca Raton: Chapman & Hall.

8
1
Introduction
Mack, T. (1993). Distribution-free calculation of the standard error of chain ladder reserve estimates.
ASTIN Bulletin, 23, 213–225.
Mack, T. (1999). The standard error of chain-ladder reserve estimates, recursive calculation and
inclusion of a tail factor. ASTIN Bulletin, 29, 361–366.
Mack, T. (2008). The prediction error of Bornhuetter-Ferguson. ASTIN Bulletin, 38, 87.
Mayerson, A. L. (1964). A Bayesian view of credibility. Proceedings of the Casualty Actuarial
Society, 51, 7–23.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation
of state calculations by fast computing machines. Journal of Chemical Physics, 21, 1087–1092.
Neal, R. M. (1994). An improved acceptance procedure for the hybrid Monte Carlo algorithm.
Journal of Computational Physics, 111, 194–203.
Simpson, L., & McCourt, P. (2012). Worksafe Victoria actuarial valuation of outstanding claims
liability for the scheme as at 30 June 2012, Technical report. PricewaterhouseCoopers Actuarial
Pty Ltd.
Taylor, G. (2000). Loss reserving: An actuarial perspective. Boston: Kluwer Academic Publishers.
Whitney, A. W. (1918). The theory of experience rating. Proceedings of the Casualty Actuarial
Society, 4, 274–292.
Wüthrich, M. V., & Merz, M. (2008). Stochastic claims reserving methods in insurance. Chichester:
Wiley & Sons.
Wüthrich, M. V., & Merz, M. (2015). Stochastic claims reserving manual: Advances in dynamic
modelling. SSRN, ID 2649057.

Chapter 2
Bayesian Fundamentals
Abstract Bayesian statistics is a ﬁeld of study with a long history (Bayes 1763).
It has the features of straightforward interpretation and simple underlying theory, at
least in principle. Analogous to the maximum likelihood estimates and conﬁdence
intervals in the frequentist framework, we have point estimates and interval esti-
mates based on posterior distributions in the Bayesian framework. We also have
similar diagnostic tools for model assessment and selections such as residual plots
and information criteria. In Sect. 2.1, we review Bayesian inference including the
posterior distribution, the posterior predictive distribution and the associated point
estimates and interval estimates. We also summarize the usefulness of different pri-
ors and state the asymptotic normality of the posterior distribution for large samples.
In Sect. 2.2, Bayesian model assessment and selections are discussed. For the model
assessment, the posterior predictive p-value is an alternative to the frequentist p-
value. For model selection, we turn to the several information criteria including DIC,
WAIC and LOO cross-validation.
2.1
Bayesian Inference
In contrast to frequentist statistics, where parameters are treated as unknown con-
stants, Bayesian statistics treats parameters as random variables with speciﬁed prior
distributions that reﬂect prior knowledge (information and subjective beliefs) about
the parameters before the observation of data. Given the observed data, the prior
distribution of the parameters is updated to the posterior distribution from which
Bayesian inference is made. In the following, the model with a single parameter is
considered ﬁrst, and then extensions are made to the multi-parameter case.
2.1.1
The Single-Parameter Case
Denote an observed sample of size n as yyy = (y1, y2, . . . , yn), the parameter as θ
(assumed to be a scalar), the prior density function of θ as p(θ), the parameter space
as Θ, the likelihood function (sometimes called sampling distribution) as p(yyy|θ),
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_2
9

10
2
Bayesian Fundamentals
and the posterior density function of θ as p(θ|yyy). According to Bayes’ theorem, the
three functions p(θ|yyy), p(yyy|θ) and p(θ) have the following relationship:
p(θ|yyy) = p(θ, yyy)
p(yyy)
=
p(yyy|θ)p(θ)

Θ p(yyy|θ)p(θ)dθ ∝p(yyy|θ)p(θ),
(2.1)
where p(θ, yyy) is the unconditional joint density function of parameters and obser-
vations, and p(yyy) is the unconditional density function (sometimes called marginal
distribution) of yyy which averages the likelihood function over the prior.
An important concept associated with the posterior distribution is conjugacy. If
the prior and posterior distributions are in the same family, we call them conjugate
distributions and the prior is called a conjugate prior for the likelihood. We will
see in Example 2.1 that the Beta distribution is the conjugate prior for the Bernoulli
likelihood.
An aim of frequentist inference is to seek the “best” estimates of ﬁxed unknown
parameters; for Bayesian statistics, the counterpart aim is to seek the “exact” distri-
bution for parameters and Eq. (2.1) has realized this aim.
2.1.1.1
Point Estimation
The fundamental assumption of Bayesian statistics is that parameters are random
variables, but we are still eager to ﬁnd a single value or an interval to summarize the
posterior distribution in Eq. (2.1). Intuitively, we want to use the mean, median or
mode of the posterior distribution to indicate an estimate of the parameter. We deﬁne
the posterior mean of θ as
ˆθ := E(θ|yyy) =

Θ
θp(θ|yyy)dθ,
where Θ is the domain of θ determined by the prior p(θ). The posterior median of
θ is deﬁned as
¨θ := median(θ|yyy) = {t : Pr(θ ≥t|yyy) ≥0.5 and Pr(θ ≤t|yyy) ≥0.5}.
The posterior mode of θ is deﬁned as
˜θ := mode(θ|yyy) = argmax
θ∈Θ
p(θ|yyy).
2.1.1.2
Interval Estimation
An interval covering the most likely values is called the highest posterior density
region (HPDR). It is deﬁned as

2.1 Bayesian Inference
11
HPDR(θ|yyy) := the shortest interval in S ,
where
S = {S : Pr(θ ∈S|yyy) ≥1 −α and p(θ = s|yyy) ≥p(θ = t|yyy) for any s ∈S, t ∈Sc}.
Another interval, called the central posterior density region (CPDR), covers the
central values of a distribution. It is deﬁned as the following interval:
CPDR(θ|yyy) := (sup{z : Pr(θ < z|yyy) ≤α/2}, inf{z : Pr(θ > z|yyy) ≤α/2}) ,
where α is the signiﬁcance level. Note that when θ is continuous, the above is
simpliﬁed as CPDR(θ|yyy) =

F−1
θ|yyy (α/2), F−1
θ|yyy (1 −α/2)

, where F−1
θ|yyy is the inverse
of the cumulative posterior distribution function of θ.
2.1.1.3
Decision Analysis/Theory
When selecting a point estimate, it is of interest and value to quantify the con-
sequences of that estimate being wrong to a certain degree. To this end, we may
consider a speciﬁed loss function L(θ∗, θ) as a measure of the information “cost”
due to using an estimate θ∗of the “true” value θ. We want θ∗to minimize the “overall
cost”, E(L(θ∗, θ)), namely the Bayes risk. According to the law of total expectation,
we have the following relationship:
E(L(θ∗, θ)) = Eyyy{Eθ|yyy

L

θ∗, θ

|yyy

} = Eθ{Eyyy|θ(L(θ∗, θ)|θ)}.
We deﬁne the posterior expected loss (PEL) and the risk function respectively as
follows:
PEL(θ∗) := Eθ|yyy(L(θ∗, θ)|yyy) =

Θ
L(θ∗, θ)p(θ|yyy)dθ
R(θ∗, θ) := Eyyy|θ(L(θ∗, θ)|θ) =

L(θ∗, θ)p(yyy|θ)dyyy.
Hence E(L(θ∗, θ)) = Eyyy(PEL(θ∗)) = Eθ(R(θ∗, θ)). If θ∗minimizes PEL(θ∗) for
all data yyy, then it also minimizes the Bayesian risk. Such θ∗is called the Bayesian
estimate with respect to the loss function L(θ∗, θ). Consider the following three loss
functions:
1. Quadratic error loss function: Lq(θ∗, θ) = (θ∗−θ)2.
2. Absolute error loss function: La(θ∗, θ) = |θ∗−θ|.
3. Zero-one error loss function: Lz = 1{0}c(θ∗−θ).

12
2
Bayesian Fundamentals
It can be proved that the posterior mean ˆθ minimizes the quadratic error loss function,
the posterior median ¨θ minimizes the absolute error loss function, and the posterior
mode ˜θ minimizes the zero-one error loss function. Hence, the point estimates dis-
cussed before are the Bayesian estimates with respect to these loss functions.
2.1.1.4
Prediction
Before the data yyy is observed, the distribution of the unknown but observable y is
p(y) =

Θ
p(y, θ)dθ =

Θ
p(y|θ)p(θ)dθ.
This is called the marginal distribution, the prior predictive distribution or the uncon-
ditional distribution of y since it is not conditional on a previous observation.
After the data yyy has been observed, we can predict an unknown observable y′. The
distribution of y′ is called the posterior predictive distribution, since it is conditional
on the data yyy:
p(y′|yyy) =

Θ
p(y′, θ|yyy)dθ =

Θ
p(y′|θ)p(θ|yyy)dθ.
Example 2.1 (A single-parameter Bernoulli-Beta model) Consider the following
Bayesian Bernoulli-Beta model:
yi ∼Bern(θ), i = 1, . . . , n
θ ∼Beta(α, β).
According to Bayes’ theorem, the posterior distribution of θ is
p(θ|yyy) ∝θ
α−1+
n
i=1
yi(1 −θ)
β−1+n−
n
i=1
yi,
(2.2)
which implies the posterior distribution of θ is Beta(α + n
i=1 yi, β + n −n
i=1 yi).
The posterior mean of θ is ˆθ = (α + n
i=1 yi)/(α + β + n), and it can be interpreted
as an upgrade from the prior mean of α/α + β due to observation yyy. And we can
continually upgrade ˆθ as more observations become available.
If we choose α = 1, β = 1, i.e., the prior of θ is an uniform distribution on [0, 1]
reﬂecting no favourite of a particular value of θ, then the posterior mean ˆθ = (1 +
n
i=1 yi)/(2 + n). In the case when α = 0, β = 0, the prior is improper (discussed
later). However, the resulting posterior is still proper and ˆθ = n−1 n
i=1 yi, which is
equal to the MLE.
To illustrate the point estimates and interval estimates in the Bayesian framework,
we assume the true underlying parameter as θTrue = 0.3, then simulate a data set
yyy = (0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0). The prior of θ is assumed

2.1 Bayesian Inference
13
●
●●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0
1
2
3
4
θ
Density
0.1
0.3
0.5
0.7
0.9
●
●
Posterior density
Prior density
Likelihood
θ0
Posterior mean
Posterior mode
Posterior median
95% CPDR
Prior mean
MLE
95% CI
Fig. 2.1 The prior, posterior and likelihood of θ
●
●
●
●
0.00
0.05
0.10
0.15
0.20
∑y m
pmf
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.033
0.111
0.191
0.225
0.197
0.133
0.071
0.029
0.009 0.002
0
Predictive mean
Predictive mode/median
Fig. 2.2 The posterior predictive distribution of 10
j=1 y′
j/10
to be Beta(2, 5), because suppose we had previously observed 2 successes in 7 trials
before our yyy was observed. In Fig. 2.1, we show the prior distribution, the likelihood,
the posterior distribution, three point estimates, the 95% CPDR, the MLE and the
95% conﬁdence interval. The posterior distribution is a kind of weighting between
the prior distribution and the likelihood. The predictive distribution of the proportion
of successes in the next 10 trials, 10
j=1 y′
j/10, is given in Fig. 2.2, together with the
predictive mean, mode and median.

14
2
Bayesian Fundamentals
0.0
0.1
0.2
0.3
0.4
0
1
2
3
4
5
6
θ
Density
●
●
●
●
●
●
Posterior density
Prior density
Likelihood
Posterior mean
95% CPDR
Prior mean
MLE
Fig. 2.3 The prior, posterior and likelihood of θ
Example 2.2 (Number of positive lymph nodes) This example is adjusted from Berry
and Stangl (1996). About 75% of the lymph from the breasts drains into the axillary
lymph nodes, making them important in the diagnosis of breast cancer. A doctor will
usually refer a patient to a surgeon to have an axillary lymph node dissection to see
if cancer cells have been trapped in the nodes. The presence of cancer cells in the
nodes increases the risk of metastatic breast cancer.
Suppose a surgeon removes four axillary lymph nodes from a woman with breast
cancer and none tests positive (i.e., no cancer cells). Suppose also that the probability
of a node testing positive has a distribution of Beta(0.14, 4.56) Berry and Stangl
(1996). The question is, what is the probability that the next four nodes are all
negative?
Denote a random variable by y with the sample space of {0, 1}, where 0 repre-
sents negative and 1 represents positive for a tested node. We know y ∼Bern(θ).
Now we have a data set yyy = (0, 0, 0, 0), so according to Eq. (2.2) our knowledge
of θ is upgraded as the posterior distribution of Beta(0.14 + 4
i=1 yi, 4.56 + 4 −
4
i=1 yi) = Beta(0.14, 8.56). Figure 2.3 shows how the observation shifts the prior
to the posterior. In this example, the number of successes is zero, so the 95% CI is not
well deﬁned while the 95% CPDR still exists. The posterior mean is ˆθ = 0.01609,
the posterior median is ¨θ = 0.0005460, the posterior mode is ˜θ = 0 and the 95%
CPDR of θ is (0, 0.14).
The posterior predictive distribution of y′ is given by:
Pr(y′ = 1|yyy) =
 1
0
θp(θ|yyy)dθ = ˆθ = 0.016
Pr(y′ = 0|yyy) =
 1
0
(1 −θ)p(θ|yyy)dθ = 1 −ˆθ = 0.984,

2.1 Bayesian Inference
15
where p(θ|yyy) is the density function of Beta(0.14, 8.56). Hence y′|yyy ∼Bern(0.016).
Now denote the status of next four nodes by y5, y6, y7, y8. The probability that the
next four nodes are all negative is
Pr(y5, y6, y7, y8 = 0|yyy)
= Pr(y8 = 0|y5, y6, y7 = 0, yyy) Pr(y7 = 0|y5, y6 = 0, yyy) Pr(y6 = 0|y5 = 0, yyy)
Pr(y5 = 0|yyy)
=0.946.
Note that Pr(y5 = 0|yyy) = 0.984 and the other terms are obtained from the updating
procedure just described in two previous paragraphs.
2.1.2
The Multi-parameter Case
We extend a single parameter θ to multiple parameters θ and assume the parameter
vector θ = (θ1, . . . , θm) distributed as a joint prior p(θ) with parameter space θ ⊆
Rm. The left hand side of Eq. (2.1) becomes a joint posterior distribution of θ =
(θ1, . . . , θm).
Unlike the single parameter case, we cannot make inferences about a parameter
directly from Eq. (2.1). We need to further ﬁnd the marginal posterior distribution
by integrating the joint posterior distribution p(θ|yyy) over all the parameters except
the parameter of interest, θk, as follows:
p(θk|yyy) =

p(θ|yyy)dθ−k,
(2.3)
where θ−k = (θ1, . . . , θk−1, θk+1, . . . , θm). Now the deﬁnitions of posterior mean,
median, mode, HPDR and CPDR from the previous section can be applied to p(θk|yyy).
For the posterior predictive distribution, multiple integration is required since p(θ|yyy)
is a joint distribution. We also deﬁne the full conditional posterior distribution of θk
as p(θk|yyy, θ−k) ∝p(θ|yyy) for 1 ≤k ≤m.
Example 2.3 (An autoregressive process of order one) Consider the following
Bayesian model for an autoregressive process of order one:
xt = αxt−1 + et, t = 1, . . . , n
et ∼N(0, λ−1)
α ∼U(−1, 1)
p(λ) ∝1/λ,
where λ is the precision parameter. We simulate a sample of size n, assuming
α0 = 0.7, λ0 = 0.25 and n = 20. The joint posterior density of α and λ is

16
2
Bayesian Fundamentals
Fig. 2.4 The joint posterior
distribution of α and λ
alpha
0.0
0.2
0.4
0.6
0.8
1.0
lambda
0.0
0.2
0.4
0.6
0.8
1.0
Joint density
0
2
4
6
●
●
●
Joint mode (0.493, 0.389)
Marginal mode 0.496
Marginal mode 0.370
p(α, λ) = h0λn/2−1(1 −α2)1/2 exp
	
−λ
2 h(xxx, α)

,
where h0 is called the normalizing constant and h(xxx, α) = (xn −αxn−1)2 + · · · +
(x2 −αx1)2 + (1 −α2)x2
1.
In Fig. 2.4 we show the joint posterior distribution, two marginal distributions,
the joint mode and two marginal modes. There is a slight difference between joint
modes and marginal modes. Similar to the single parameter case, in Fig. 2.5 we
show the inferences made from two marginal posterior distributions. Under the non-
informative priors, Bayesian inference is quite close to the frequentist inference. This
is guaranteed by the asymptotic theory, which will be discussed in Sect. 2.1.4.
Finallyfortheprediction, ˆx20+1 = E(x20+1|xxx) = E(αx20|xxx) = x20E(α|xxx) = x20 ˆα
= 0.3517. The analytic solution to the predictive distribution requires a double inte-
gral with respect to α and λ. We will estimate the posterior predictive distribution in
Sect. 3.1.2 using the MCMC methods. See details in Appendix A on page 187.
2.1.3
Choice of Prior Distribution
Here we will discuss three types of priors: informative priors, non-informative priors
and weakly informative priors Gelman et al. (2014).
2.1.3.1
Informative Priors
In Example 2.1, comparing p(θ) and p(θ|yyy) suggests that the prior is equivalent
to α −1 prior successes and β −1 prior failures. The parameters of the prior dis-
tribution are often referred to as hyperparameters. If we had past trials, we could

2.1 Bayesian Inference
17
0.0
0.2
0.4
0.6
0.8
1.0
α
Density
●
●
●
●
●
●
Posterior mean
Posterior mode
Posterior median
95% CPDR
α0
MLE
95% CI
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
λ
Density
●
●
●
●
●
●
●
●
●
●
Posterior mean
Posterior mode
Posterior median
95% CPDR
λ0
MLE
95% CI
●●●●
Fig. 2.5 The marginal posterior distributions of α and λ
summarize the past information about θ into an informative prior. Every time we
use an informative prior we can treat the prior as the summary from past data. An
informative prior is equivalent to adding some observations to a non-informative
prior.
Sometimes informative priors are called strong priors, in the sense that they affect
the posterior distribution more strongly, relative to the data, than other priors. The
distinction between strong priors and weak priors is vague, and a strong prior may
become a weak prior as more data comes in to counterbalance the strong prior. It is
better to look at the prior together with the likelihood of data.

18
2
Bayesian Fundamentals
2.1.3.2
Non-informative Priors
There has been a desire for priors that can be guaranteed to play a minimal role,
ideally no role at all, in the posterior distribution. Such priors are variously called
non-informative priors, reference priors (Berger et al. 2009), vague priors, ﬂat priors,
or diffuse priors. The rationale for using a non-informative prior is often given as
letting the data speak for themselves.
The Bernoulli-Beta model
In Example 2.1, Beta(1, 1) is a non-informative prior, since it assumes that θ is
distributed uniformly on [0, 1]. The posterior distribution under this prior is the
same as the likelihood. The posterior mode will be equal to the maximum likelihood
estimate n
i=1 yi/n. Note that the posterior mean is not equal to the posterior mode.
If we want the posterior mean equal to the MLE, we need to specify α, β = 0.
This prior is called a improper non-informative prior since the integral of this prior’s
pdf is not 1. When we use an improper non-informative prior, we need to check
whether the resulting posterior is proper. Fortunately, the posterior here is proper.
The normal-normal model with known variance
Another example is the normal model with unknown mean but known variance,
shown as follows:
yi ∼N(μ, σ 2), i = 1, . . . , n
μ ∼N(μ0, τ 2
0 ).
If τ 2
0 →∞, the prior is proportional to a constant, and is improper. But the posterior
is still proper, p(μ|yyy) ≈N(μ|¯yyy, σ 2/n). Here N(μ|¯yyy, σ 2/n) is used to represent the
probability density function for variable μ, a normal distribution with mean of ¯yyy and
variance of σ 2/n.
The normal-normal model with known mean
Now assume the mean is known and variance is unknown. We know that the con-
jugate prior for variance is inverse-gamma distribution, i.e., σ −2 follows a gamma
distribution, Gamma(α, β). The non-informative prior is obtained as α, β →0.
Here we parameterize it as a scaled inverse−χ2 distribution with scale σ 2
0 and ν0
degrees of freedom; i.e., the prior distribution of σ 2 is taken to be the distribution of
σ 2
0 ν0/X, where X is a χ2
ν0 random variable. The model can be written as follows:
yi ∼N(μ, σ 2), i = 1, . . . , n
σ 2 ∼Inv-χ2(ν0, σ 2
0 ).
The resulting posterior distribution of σ 2 can be shown as
σ 2|yyy ∼Inv-χ2
	
ν0 + n, ν0σ 2
0 + nν
ν0 + n

,
where ν = 1/n n
i=1(yi −μ)2.

2.1 Bayesian Inference
19
The non-informative prior is obtained as ν0 →0, which is improper and propor-
tional to the inverse of the variance parameter. This non-informative prior is some-
times written as p(log σ 2) ∝1. The resulting posterior distribution is proper, with
the density function of p(σ 2|yyy) ≈Inv-χ2(σ 2|n, ν). The uniform prior distribution
on σ 2, i.e., p(σ 2) ∝1, will lead to an improper posterior.
Jeffreys’ priors
Finally, there is a family of non-informative priors called Jeffreys’ priors. The idea
is that the non-informative priors should have the same inﬂuence as likelihood on
the parameters. It can be shown that the Jeffreys’ prior of θ is proportional to the
squared root of Fisher information; i.e., p(θ) ∝J(θ)1/2, where
J(θ) = E
	d log p(yyy|θ)
dθ

2θ

= −E
	d2 log p(yyy|θ)
dθ2
θ

.
(2.4)
As a simple justiﬁcation, the Fisher information measures the curvature of the log-
likelihood, and high curvature occurs wherever small changes in parameter values
are associated with large changes in the likelihood. So the proportional relationship
ensures that Jeffreys’ prior gives more weight to these parameters. In Example 2.1,
the Fisher information is J(θ) = n/(θ(1 −θ)). Hence, Jeffreys’ prior is p(θ) ∝
θ−1/2(1 −θ)−1/2, which is Beta(0.5, 0.5).
2.1.3.3
Weakly Informative Priors
A weakly informative prior lies between informative priors and non-informative
priors. It is proper, but is set up so that the information it provides is intentionally
weaker than whatever actual prior knowledge is available. We do not use weakly
informative priors here. For more discussion, please refer to Gelman et al. (2014) on
page 55.
Example 2.4 (A single-parameter Bernoulli-Beta model) We continue with
Example 2.1 and consider the effects of two non-informative priors, Beta(1, 1)
and Beta(0.5, 0.5), on the posterior distributions. Under the uniform distribution
Beta(1, 1), the posterior distribution is equal to the scaled likelihood, so the poste-
rior mode is equal to the MLE. Under the Jeffreys’ prior Beta(0.5, 0.5), the posterior
distribution is quite close to the scaled likelihood. In both cases, the effect of the
priors on the posterior distribution is negligible. In Fig. 2.6, we plot the likelihood,
the prior, and the posterior distribution. As we expect, under the two non-informative
priors the scaled likelihood is quite close to the posterior distribution.

20
2
Bayesian Fundamentals
θ
Density
●
●●
●
●
●
●
●
●
●
●
●
Posterior density
Prior density
Likelihood
θ0
Posterior mean
Posterior mode
Posterior median
95% CPDR
Prior mean
MLE
95% CI
θ
Density
0
3
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0
1
2
1
2
3
4
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
●
●●
●
●
●
●
●
●
●
●
●
Posterior density
Prior density
Likelihood
θ0
Posterior mean
Posterior mode
Posterior median
95% CPDR
Prior mean
MLE
95% CI
Fig. 2.6 The effect of two non-informative priors, Beta(1, 1) and Beta(0.5, 0.5), on the posterior
distribution
2.1.4
Asymptotic Normality of the Posterior Distribution
Suppose y1, . . . , yn are outcomes sampled from a distribution f (y). We model the
data by a parametric family p(y|θ) : θ ∈Θ, where θ is distributed as p(θ). The
result of large-sample Bayesian inference is that as more and more data arrive, i.e.,
n →∞, the posterior distribution of the parameter vector approaches multivariate
normal distribution.
We label θ0 as the value of θ that minimizes the Kullback-Leibler divergence
KL(θ) of the likelihood p(yyy|θ) relative to the true distribution f (yyy). The Kullback-
Leibler divergence is deﬁned as a function of θ as follows:

2.1 Bayesian Inference
21
KL(θ) := E f
	
log
	
f (yyy)
p (yyy|θ)


= −

log
	
f (yyy)
p (yyy|θ)

f (yyy)dyyy.
2.1.4.1
When the True Distribution is in the Parametric Family
If the true data distribution is included in the parametric family, i.e., f (y) =
p(y|θTrue) for some θTrue, then θ0 will approach θTrue as n →∞. The posterior distri-
bution of θ approaches normality with mean θ0 and variance nJ(θ0)−1, where J(θ0)
is the Fisher information deﬁned in Eq. (2.4).
The proof of asymptotic normality is based on the Taylor series expansion of log
posterior distribution, log p(θ|yyy), centred at the posterior mode up to the quadratic
term. As n →∞, the likelihood dominates the prior, so we can just use the likelihood
to obtain the mean and variance of the normal approximation.
2.1.4.2
When the True Distribution is Not in the Parametric Family
The above discussion is based on the assumption that the true distribution is included
in the parametric family, i.e., f (y) ∈{p(y|θ) : θ ∈Θ}. When this assumption fails,
there is no true value θTrue ∈Θ, and its role in the theoretical result is replaced
by the value θ0 which minimizes the Kullback-Leibler divergence. Hence, we still
have the similar asymptotic normality that the posterior distribution of θ approaches
normality with mean θ0 and variance nJ(θ0)−1. But now p(y|θ0) is no longer the
true distribution f (y).
2.2
Model Assessment and Selection
In this section, we review the model diagnostic tools including posterior predictive
checking and residual plots. We also discuss the model selection criteria including
several information criteria and cross-validation.
2.2.1
Posterior Predictive Checking
In the classical framework, the testing error is preferred since it is calculated on a
testing data set which is not used to train the model. In the Bayesian framework,
ideally we want to split the data into a training set and a testing set and do the
posterior predictive checking on the testing data set. Alternatively, we can choose a
test statistic whose predictive distribution does not depend on unknown parameters
in the model but primarily on the assumption being checked. Then there is no need
to have a separate testing data set. Nevertheless, when the same data are used for

22
2
Bayesian Fundamentals
both ﬁtting and checking the model, this needs to be carried out with caution, as the
procedure can be conservative.
In frequentist statistics, p-value is typically deﬁned as
p := Pr(T (yyy′) ≥T (yyy)|H0),
where T is the function of data that generates the test statistic, yyy′ is the future
observation (random variable), and yyy is the observed sample. Note that T (yyy) is
regarded as a constant. The probability is calculated over the sampling distribution
of y under the null hypothesis. It is well known that p can be calculated exactly only
in the sense that T (yyy) is a pivotal quantity.
Meng (1994) explored the posterior predictive p-value (pB), a Bayesian version
of the classical p-value. pB is deﬁned as the probability
pB := Pr{T (yyy′, θ) ≥T (yyy, θ)|yyy, H0},
where yyy′ is the future data, and T (yyy, θ) is a discrepancy measure that possibly
depends on θ. This probability is calculated over the following distribution:
p(yyy′, θ|yyy, H0) = p(yyy′|θ)p(θ|yyy, H0),
where the form of p(θ|yyy, H0) depends on the nature of the null hypothesis. Following
Meng (1994), we consider the two null hypotheses: a point hypothesis and a compos-
ite hypothesis. For the completion of discussion, please refer to Robins et al. (2000).
They mentioned some problems associated with the posterior predictive p-value
under a composite hypothesis.
2.2.1.1
When the Null Hypothesis is a Point Hypothesis
Suppose the null hypothesis is θk = a and the prior under the null hypothesis is
p(θ−k|θk = a) with the parameter space Θ ⊂Rm−1. Then the posterior density of θ
under the null hypothesis is
p (θ|yyy, H0) =
p (yyy|θ−k, θk = a) p (θ−k|θk = a)

Θ p (yyy|θ−k, θk = a) p (θ−k|θk = a) dθ−k
.
The posterior predictive p-value is calculated as
pB = Pr

T

yyy′, θ

≥T (yyy, θ) |y, H0

=

Θ
Pr

T

yyy′, θ

≥T (yyy, θ) |θ

p (θ|yyy, H0) dθ−k.

2.2 Model Assessment and Selection
23
2.2.1.2
When the Null Hypothesis is a Composite Hypothesis
Suppose the null hypothesis is θk ∈A and the prior under the null hypothesis is
p(θ−k|θk)p(θk). Then the posterior density of θ under the null hypothesis is
p (θ|yyy, H0) = p (θ−k|yyy, θk) p (θk) =
p (yyy|θ) p (θ−k|θk)

Θ p (yyy|θ) p (θ−k|θk) dθ−k
p (θk) .
The posterior predictive p-value is calculated as
pB = Pr

T

yyy′, θ

≥T (yyy, θ) |yyy, H0

=

Θ

A
Pr

T

yyy′, θ

≥T (yyy, θ) |θ

p (θ−k|yyy, θk) p (θk) dθkdθ−k.
2.2.1.3
Choice of T(yyy, θ)
Recall that in the frequentist theory, the most powerful test in a composite test,
H0 : θk ∈A versus H1 : θk /∈A, is based on the generalized likelihood ratio deﬁned
as follows:
g (yyy) := supθk /∈A p(yyy|θk)
supθk∈A p(yyy|θk).
Meng (1994) suggested using the conditional likelihood ratio and the generalized
likelihood ratio, deﬁned respectively as follows:
CLR (yyy, θ) = T C (yyy, θ) := supθk /∈A p (yyy|θ)
supθk∈A p (yyy|θ)
GLR (yyy) = T G (yyy) :=
supθk /∈Asupθ−k p (yyy|θ)
supθk∈Asupθ−k p (yyy|θ).
Because a probability model can fail to reﬂect the process that generated the data in
any number of ways, pB can be computed for a variety of discrepancy measures T
in order to evaluate more than one possible model failure.
Example 2.5 (A one-sample normal mean test using pB) This example is extracted
from Meng (1994). Suppose we have a sample of size n from N(μ, σ 2), and we test
the null hypothesis that μ = μ0 with σ 2 unknown. Recall that in classical testing,
the pivotal test statistic is √n(¯xxx −μ0)/s, where ¯xxx is the sample mean and s2 is
the sample variance. We know this test statistic follows a tn−1 distribution. So p =
Pr(tn−1 ≥√n(¯xxx −μ0)/s).
In the Bayesian framework, we assume μ and σ 2 are independent and σ 2 has a
non-informative prior (i.e., p(σ 2) ∝1/σ 2). We can ﬁnd CLR and GLR as

24
2
Bayesian Fundamentals
CLR

xxx, σ 2
= T C 
xxx, σ 2
= n(¯xxx −μ0)2
σ 2
GLR (xxx) = T G (xxx) = n(¯xxx −μ0)2
s2
.
Using the two discrepancy measures, we calculate pB as
pC
B = Pr{T C 
xxx′, σ 2
⩾T C 
xxx, σ 2
|xxx, μ0} = Pr

F1,n ⩾n(¯xxx −μ0)2
s2
0

pG
B = Pr{T G 
xxx′
⩾T G (xxx) |xxx, μ0} = Pr{F1,n−1 ⩾T G (xxx)},
where s2
0 = n
i=1(xi −μ0)2. Note that p = pG
B ̸= pC
B; pB is equal to the classical
p-value when using GLR. See details in Appendix A on page 189.
Example 2.6 (Number of runs) Suppose we have a data set xxx = (x1, x2, . . . , x10) =
(1, 1, 1, 0, 0, 0, 0, 0, 1, 1), resulting from n = 10 Bernoulli trials with success prob-
ability θ which has an non-informative improper prior, Beta (0, 0). Now we want to
test the null hypothesis that the trials are independent of each other.
We use the number of runs in xxx as the test statistic, denoted by r(xxx). Note that a
run is deﬁned as a subsequence of elements of one kind immediately preceded and
succeeded by elements of the other kind. So in this example we have r(xxx) = 3, and θ
is treated as a nuisance parameter. It is easy to ﬁnd that the posterior distribution of
θ is Beta (6, 6) under H0. To calculate pB = Pr{r(xxx′) ≤3|H0}, we apply the exact
density of r(xxx′).
According to Kendall and Stuart (1961), assuming n1 1’s and n2 0’s are ran-
domly placed in a row, the number of runs, denoted by R (n1, n2), has the following
probability mass functions for 0 ≤n2 ≤n1 and 2 ≤R ≤n1 + n2 :
Pr {R = 2s} =
2
n1−1
s
n2−1
s−1

n2−1
s−1

,
for s = 1, . . . , n2
Pr {R = 2s −1} =
n1−1
s−2
n2−1
s−1

+
n1−1
s−1
n2−1
s−2

n2−1
s−1

,
for s = 2, 3, . . . , n2.
However, this probability mass function is not complete, missing the case when
R = 2n2 + 1 (i.e., R is odd and s = n2 + 1). For completeness, we add the two
special cases and their associated probabilities as in Table 2.1. Applying the exact
density of R (n1, n2), pB is calculated as
pB =
 1
0
⎛
⎝
10

i=0
3

j=1
Pr{R (i, 10 −i) = j} Pr (n1 = i|θ)
⎞
⎠p(θ|xxx)dθ = 0.1630,
(2.5)

2.2 Model Assessment and Selection
25
Table 2.1 Special cases for the probability of R(n1, n2)
n1
n2
s
Pr(R(n1, n2) =
2s −1)
≥1
0
1
1
≥n2 + 1
n2 ≥1
n2 + 1
n1−1
n2

/
n1+n2
n1

Table 2.2
pB’s for other observations
Case
Sample xxx
n
r(xxx)
pB
(i)
(1,1,1,1,1, 1,1,1,1,1)
10
1
0.5293
(ii)
(0,0,0,0,0, 1,1,1,1,1)
10
2
0.0462
(iii)
(0,1,1,0,1, 1,0,1,1,1)
10
6
0.6066
(iv)
(0,1,0,1,0, 1,0,1,0,1)
10
10
1
(v)
(1,0,1,1,1, 0,0,1,1,0, 1,0,1,1,0)
15
10
0.9354
(vi)
(1,1,1,1,1, 0,0,0,0,0, 1,1,1,1,1, 1,1,1,1,1, 1,1,1,0,0)
25
4
0.0248
(vii)
(1,0,1,0,1, 0,1,0,1,0, 1,0,1,0,1, 0,1,0,1,0, 1,0,1,0,1)
25
25
1
which implies that under H0 the number of runs of a future observed sample would
be smaller than 3 with probability of 0.163. See details in Appendix A on page 192.
Furthermore, we list pB’s calculated for other observations in Table 2.2. Note that
the sample test statistics in cases (iv) and (vii) reach the maximum number of runs, so
pB is deﬁnitely 1. However, we cannot conclude that x’s are deﬁnitely independent
of each other, as these observations indicate that 1’s are most likely followed by 0’s.
We consider any pB smaller than 0.1 or larger than 0.9 as indicating the violation
of H0.
2.2.2
Residuals, Deviance and Deviance Residuals
In the Bayesian framework, we can generate a set of residuals for one realization of
posterior parameters. So there are four choices of residuals:
• Choose the posterior mean of parameters and ﬁnd one set of residuals.
• Randomly choose a realization of parameters and ﬁnd one set of the residuals.
• Get the posterior mean of residuals.
• Get the posterior distribution of residuals.
In the following, we will review Pearson residuals, deviance and deviance residuals.
A Pearson residual is deﬁned as
ri (θ) := yi −E(yi|θ)
√Var(yi|θ) .
The deviance is deﬁned as
D (θ) := −2 log p(yyy|θ) = −2
n

i=1
log p (yi|θ) ,
(2.6)

26
2
Bayesian Fundamentals
and the contribution of each data point to the deviance is Di (θ) = −2 log p (yi|θ) .
We will deﬁne and use D( ˆθ) and 
D (θ) in the next section.
The deviance residuals are based on a standardized or “saturated” version of the
deviance, deﬁned as
DS (θ) := −2
n

i=1
log p (yi|θ) + 2
n

i=1
log p

yi
 ˆθS (yyy)

,
where ˆθS (yyy) are appropriate “saturated” estimates, e.g., we set ˆθS (yyy) = yyy. The con-
tribution of each data point to the standardized deviance is
DSi (θ) = −2 log p (yi|θ) + 2 log p

yi
 ˆθS (yyy)

.
The deviance residual is deﬁned as
dri := signi

DSi (θ),
where signi is the sign of yi −E(y′
i|θ).
Example 2.7 (Three error structures for stack-loss data) The stack-loss data set
contains 21 daily responses of stack loss yyy, the amount of ammonia escaping, with
covariates being air ﬂow x1, temperature x2 and acid concentration x3. We assume a
linear regression on the expectation of y, i.e., E (yi) = μi = β0 + β1zi1 + β2zi2 +
β3zi3, i = 1, . . . , 21. We consider three error structures: normal, double exponential
and t4, as follows:
yi ∼N(μi, τ −1)
yi ∼DoubleExp(μi, τ −1)
yi ∼t4(μi, τ −1),
where zi j =

xi j −xxx j

/sd

xxx j

for j = 1, 2, 3 are covariates standardized to have
zeromeanandunitvariance,andβ0, β1, β2, β3 aregivenindependentnon-informative
priors.
The deviance residuals of the three models have the following forms respectively:
DSi = √τ (yi −μi)
DSi = signi

2τ |yi −μi|
DSi = signi

5 log
	
1 + (yi −μi)2
4

.
We plot the posterior distributions of deviance residuals for each model in Fig. 2.7.
The three residual plots agree on four outliers: 1, 3, 4 and 21.

2.2 Model Assessment and Selection
27
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●●
●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●●●●
●●
●
●
●
●●
●
●
●
●●
●●
●
●●
●
●
●
●●●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●●
●
●●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●●
●
●●●●
●
●●
●
●
●
●
●
●
●●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●●
●●●●●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●●●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●●
●
●
●
●●●
●●
●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●●
●
●●●●
●
●
Normal
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●
●●●
●
●●●
●
●
●
●●
●
●●
●
●
●●
●●
●
●
●●
●
●
●
●●●●
●●●●●●●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●●
●●●
●
●●●●
●●
●
●●●●●
●
●●●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●●●●
●●●
●●
●
●●
●
●
●
●●
●
●●●
●
●
●●
●
●●●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●
●
●
●●
●●●●●
●●
●
●●●●
●
●
●
●
●
●
●●●●
●
●●●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●●●●●
●●●●●●●●●
●
●●
●
●●
●●●
●●
●
●
●●●
●●
●
●
●●●●●●●
●●
●
●
●●
●
●●
●
●●●●
●
●
●●●●●●
●
●
●
●
●●
●
●●
●●
●●
●
●
●
●
●
●
●
●
●●●●
●●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●●●●
●
●
●●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●●
●
●
●●
●
●●●●
●●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●●●●●
●
●
●
●●●●
●●●
●●
●
●●
●●●
●
●●
●
●
●●●●●
●
●
●
●●●
●
●
●
●●●
●
●
●
●●
●
●●●
●
●●●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●●
●●●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●●●
●●
●
●●●
●●
●
●
●●●●
●●●
●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●●
●●●●●
●
●
●●●●●
●●●●●●●●
●●
●●●
●
●●
●●
●
●●
●
●●
●●●●
●
●●●●
●
●●
●
●
●●●●
●
●●●●
●
●
●●●●
●
●
●●
●●●
●
●
●●
●
●
●
●
●
●●
●●
●●
●
●●
●
●
●
●
●●
●●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●●
●
●●
●●
●●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●●●●
●
●●●
●
●
●
●
●●●
●●●●
●
●
●
●
●●
●
●●
●●
●
●●
●
●
●
●
●●
●
●
●●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●●●●●●●●●●
●
●●●●
●●
●●
Double Exp
●
●●●●
●●
●
●
●
●●●
●
●
●●
●
●
●
●●●
●●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●●●
●
●
●
●●
●●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●●
●
●
●●●●
●
●●
●
●
●●
●
●
●
●
●
●
●
●●●●●
●●
●
●●●●
●●●
●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●●●
●
●
●
●●
●
●
●●
●●●●●
●
●
●
●●
●
●
●●
●●
●
●
●
●●●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●●
●
●●●●●
●●
●●
●●
●
●
●
●
●
●
●●●
●
●●
●
●
●●
●●●
●●●
●●
●
●
●
●●●
●
●
●
●●
●
●●
●
●●●●
●●●●
●
●
●
●●●●
●
●
●
●●
●
●●
●
●●●
●
●
●
●●●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●●
●●●
●
●●
●
●
●
●●●●
●
●●
●
●
●●●
●
●
●
●
●
●●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●●
●
●●●●●●●
●●●
●
●
●
●●
●
●●
●
●●
●
●
●●●
●●●
●
●
●
●
●
●
●●●
●
●●●●
●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●
●●
●●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●●●
●●
●
●
●
●●●
●●
●
●
●●
●●●●●
●
●
●
●
●●
●●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●●●
●
●●
●
●●
●●●●●●●
●
●
●
●
●●●
●
●●
●
●
●
●
●
●
●●
●
●●●
●
●
●●●●●
●
●●
●
●
●●●●●●
3
1
2
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21
−4
−2
0
2
4
3
1
2
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21
−4
−2
0
2
4
3
1
2
4
5
6
7
8
9
10 11 12 13 14 15 16 17 18 19 20 21
−4
−2
0
2
4
t(4)
Fig. 2.7 The deviance residual plots of the three models

28
2
Bayesian Fundamentals
2.2.3
Bayesian Model Selection Methods
The model selection problem is a trade-off between a simple model and good ﬁtting.
Ideally, we want to choose the simplest model with best ﬁtting. However good ﬁtting
models tend to be more complicated while simpler models tend to be underﬁt. The
model selection methods used in frequentist statistics are typically cross-validation
and information criteria, which are the modiﬁed residual sum of squares with respect
to the model complexity and overﬁtting.
Cross-validation measures the ﬁt of a model on the testing data set, which is not
used to ﬁt the model, while the information criteria adjust the measure of ﬁt on the
training data set by adding a penalty for model complexity.
2.2.3.1
The Predictive Accuracy of a Model
In the Bayesian framework, the ﬁt of a model is sometimes called the predictive
accuracy of a model (Gelman et al. 2014). We measure the predictive accuracy of
a model to a data set yyy′ by log point wise predictive density (lppd), calculated as
follows:
lppd:= log
n′

i=1
Eθ|yyy p

y′
i|θ

=
n′

i=1
log

Eθ|yyy p

y′
i|θ

=
n′

i=1
log
	
p(y′
i|θ)p(θ|yyy)dθ

.
Ideally, yyy′ should not be used to ﬁt the model. If we choose yyy′ = yyy, we get the
within-sample lppd (denoted by lppdtrain), which is typically larger than the out-of-
sample lppd (denoted by lppdtest). To compute lppd in practice, we can evaluate the
expectation using draws from the posterior distribution p(θ|yyy), which we label as
θt, t = 1, . . . , T . The computed lppd is deﬁned as follows:
computed lppd:=
n′

i=1
log

1
T
n′

i=1
p(y′
i|θt)

.
2.2.3.2
Cross-validation
In Bayesian cross-validation, the data are repeatedly partitioned into a training set
ytrain and a testing set ytest. For simplicity, we restrict our attention to leave-one-out
cross-validation (LOO-CV), where ytest only contains a data point. The Bayesian
LOO-CV estimate of out-of-sample lppd is deﬁned as follows:
lppdloo-cv :=
n

i=1
log
	
p (yi|θ) p (θ|yyy−i) dθ

,
(2.7)

2.2 Model Assessment and Selection
29
where yyy−i is the data set without the ith point. The lppdloo-cv can be computed as
computed lppdloo-cv =
n

i=1
log

1
T
T

t=1
p

yi|θit

,
whereθit, t = 1, . . . , T arethesimulationsfromtheposteriordistribution p

θ|yyy−i

.
2.2.3.3
Deviance Information Criterion (DIC)
AIC and BIC
Before describing the DIC, we review another two information criteria employed in
frequentist statistics. The Akaike information criterion (AIC) by Akaike (1973) is
deﬁned as
AIC:= −2
n

i=1
log p (yi|θMLE) + 2p.
The Bayesian information criterion (BIC) by Schwarz (1978) is deﬁned as
BIC:= −2
n

i=1
log p (yi|θMLE) + p log n,
where p isthenumberofparameters.Theﬁrstcommonterm−2 n
i=1 log p(yi|θMLE)
measures the discrepancy between the ﬁtted model and the data. The second term
measures the model complexity.
DIC
In the Bayesian framework, we deﬁne a similar quantity to measure the discrepancy,
−2 n
i=1 log p(yi| ˆθ), where ˆθ is the posterior mean. Spiegelhalter et al. (2002) pro-
posed a measure of number of effective parameters, which is deﬁned as the difference
between the posterior mean of the deviance and the deviance at the posterior means,
as follows:
pD := 
D (θ) −D( ˆθ) = −2Eθ|yyy
 n

i=1
log p (yi|θ)

+ 2
n

i=1
log p

yi
 ˆθ

,
where D is the deviance deﬁned in Eq. (2.6).
Furthermore, they proposed a deviance information criterion (DIC), deﬁned as
the deviance at the posterior means plus twice the effective number of parameters,
to give
DIC := D( ˆθ) + 2pD.
DIC is viewed as a Bayesian analogue of AIC. We prefer the model with smaller
DIC. Note that DIC and pD are sensitive to the level of a hierarchical model. They

30
2
Bayesian Fundamentals
are appropriate when we are interested in the parameters directly related to the
data. DIC and pD can be calculated using OpenBUGS, which will be discussed in
Sect. 3.3.
2.2.3.4
Watanabe-Akaike or widely available information criterion
(WAIC)
Watanabe (2010) proposed another measure of number of effective parameters as
follows:
pWAIC := 
D(θ) + 2lppdtrain = −2Eθ|yyy
 n

i=1
log p (yi|θ)

+ 2
n

i=1
log

Eθ|yyy p (yi|θ)

,
where −2lppdtrain plays a role as D( ˆθ) as in pD. As with AIC and DIC, the Watanabe-
Akaike information criterion (WAIC) is deﬁned as
WAIC:= −2lppdtrain + 2pWAIC.
2.2.3.5
Leave-One-Out Information Criterion (LOOIC)
Different from the deﬁnition of number of effective parameters in AIC, DIC, and
WAIC, we deﬁne
ploo := lppdtrain −lppdloo-cv,
where lppdloo-cv comes from Eq. (2.7). The leave-one-out information criterion
(LOOIC) is deﬁned as
LOOIC:= −2lppdtrain + 2ploo = −2lppdloo-cv,
which is reasonable since lppdloo-cv already penalizes the overﬁtting (or equivalently
the model complexity).
Example 2.8 (pD in a random effects model) This example follows Spiegelhalter
et al. (2002). Consider the following random effects Bayesian model:
yi j ∼N(θi, τ −1
i
), i = 1, . . . , m, j = 1, . . . , n
θi ∼N(μ, λ−1)
μ ∼N(0, ∞)
where τi, i = 1, . . . , m, and λ are known precision parameters. τi is termed as the
within-group precision, and λ is termed as the between-group precision. It can be
shown that the posterior distribution of population mean is

2.2 Model Assessment and Selection
31
μ|yyy ∼N
⎛
⎝¯yyy,

λ
m

i=1
ρi
−1⎞
⎠,
where
¯yyy =
m
i=1 ρi ¯yi
m
i=1 ρi
, ρi =
τi
τi + λ, yyyi =
n
j=1 yi j
n
.
Assuming θ = (θ1, . . . , θm), we will have the following equations:

D (θ) =

ρi + λ

ρi (1 −ρi) ( ¯yi −¯yyy)2 +
 ρi (1 −ρi)
 ρi
D( ˆθ) = λ

ρi (1 −ρi) ( ¯yi −¯yyy)2
pD =

ρi +
 ρi (1 −ρi)
 ρi
.
Consider the number of effective parameters pD under the following three cases:
• If λ →∞, then ρi →0, and pD = 1. All the groups have the same mean μ, which
is the only effective parameter. The model is equivalent to:
yi j ∼N

μ, τ −1
i

, i = 1, . . . , m, j = 1, . . . , n
μ ∼N (0, ∞) .
• If λ →0, then ρi →1, and pD = m. All the groups are independent and have
different means. The model is equivalent to:
yi j ∼N

θi, τ −1
i

, i = 1, . . . , m, j = 1, . . . , n
θi ∼N (0, ∞) .
• If τi are equal, then ρ1 = · · · = ρm = ρ and pD = 1 + (m −1) ρ.
In summary, if we assign the majority of variation in yyy to the within-group variation
rather than between-group variation (i.e., λ is much larger than τi), then the group
means θi tend to converge to the population mean μ, and we have only one parameter
(i.e., θi cannot be effectively estimated distinguishably).
On the other hand, if we assign the majority of variation in yyy to the between-group
variation (i.e., τi is much larger than λ), then there is no accurate estimate of μ, and
every θi tends to “escape” from the “trap” distribution θi ∼N

μ, λ−1
. Every θi can
be effectively estimated by its group mean, and there are m effective parameters.
Example 2.9 (Three error structures for stack-loss data) We continue with Example
2.7 and calculate lppdloo-cv, DIC, pD, WAIC and pWAIC for the three models discussed
on page 26. As shown in Table 2.3, lppdloo-cv, DIC, and WAIC agree on the model
with double exponential error distribution.

32
2
Bayesian Fundamentals
Table 2.3 lppdloo-cv, DIC and WAIC for the three models
Error structures
lppdloo-cv
DIC
pD
WAIC
pWAIC
Normal
−59.0
115.5
5.23
116.5
4.8
DoubleExp
−57.3
113.3
5.53
114.5
5.7
t4
−57.8
114.2
5.53
115.6
5.8
2.2.4
Overﬁtting in the Bayesian Framework
Suppose that we have a sample of size n from a common normal distribution
with unknown mean and known precision, yi ∼N

μ, τ −1
, i = 1, . . . , m. In the
Bayesian framework we can assume m parameters, each of which is for one data
value. Such a Bayesian model can be written as follows:
yi ∼N

μi, τ −1
, i = 1, . . . , m
μi ∼N

μ0, τ −1
0

,
where τ is known and p(μ0, τ −1
0 ) ∝τ0 is a non-informative improper hyperprior.
This is a special case when n = 1 in Example 2.8.
This random effects model can also be viewed as a hierarchical model with three
levels. We refer to the top level distribution related to the data as the sampling
distribution or likelihood, the second level distribution as the prior and the third level
distribution as the hyperprior. Accordingly, μi, τ are called parameters and μ0, τ0
are called hyperparameters.
The model has m data values and m + 2 parameters, which presents an overﬁtting
issue in the frequentist framework on account of parameters treated as unknown ﬁxed
constants. However, it is quite common for the number of parameters to be larger
than the number of data values in the Bayesian framework, where the number of
effective parameters would be smaller than m as shown in Example 2.8.
2.3
Bibliographic Notes
Bayesian statistics derives from Bayes’ famous 1763 essay, which has been reprinted
as Bayes (1763). For other early contributions, see also Laplace (1785, 1810). Gel-
man et al. (2014) contains most of the current developments in Bayesian statistics.
Jeffreys’ priors and the invariance principles for non-informative priors are studied
in Jeffreys (1961). The asymptotic normality of the posterior distribution was known
by Laplace (1810).
ThemethodofposteriorpredictivecheckingwasproposedbyRubin(1981, 1984).
The posterior predictive p-value was studied by Meng (1994). Akaike (1973) intro-
duced the expected predictive deviance and AIC. Schwarz (1978) introduced BIC.

2.3 Bibliographic Notes
33
Spiegelhalter et al. (2002) introduced the DIC. Watanabe (2010, 2013) presented
WAIC. RJMCMC was introduced by Green (1995). A recent work summarizing cri-
teria for evaluation of Bayesian model selection procedures is Bayarri et al. (2012).
References
Akaike, H. (1973). Information theory and an extension of the maximum likelihood principle. In
Second International Symposium on Information Theory, 267–281.
Bayarri, M. J., Berger, J. O., Forte, A., & Garcia-Donato, G. (2012). Criteria for Bayesian model
choice with application to variable selection. The Annals of Statistics, 40, 1550–1577.
Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. Philosophical
Transactions of the Royal Society, 330–418.
Berger, J. O., Bernardo, J. M., & Sun, D. (2009). The formal deﬁnition of reference priors. The
Annals of Statistics, 37, 905–938.
Berry, D. A., & Stangl, D. (1996). Bayesian biostatistics. New York: Marcel Dekker.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.).
Boca Raton: Chapman & Hall.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrika, 82, 711–732.
Jeffreys, H. (1961). Theory of probability (3rd ed.). London: Oxford University Press.
Kendall, M. G., & Stuart, A. (1961). The advanced theory of statistics: Inference and relationship.
London: Charles Grifﬁn.
Laplace, P. S. (1785). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres. In Memoires de l’Academie Royale des Sciences.
Laplace, P. S. (1810). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres, et sur leur application aux probabilites. In Memoires de l’Academie des Science
de Paris.
Meng, X. L. (1994). Posterior predictive p-values. The Annals of Statistics, 22, 1142–1160.
Robins, J. M., van der Vaart, A. W., & Ventura, V. (2000). Asymptotic distribution of p-values in
composite null models. Journal of the American Statistical Association, 95, 1143–1156.
Rubin, D. B. (1981). Estimation in parallel randomized experiments. Journal of Educational and
Behavioral Statistics, 6, 377–401.
Rubin, D. B. (1984). Bayesianly justiﬁable and relevant frequency calculations for the applied
statistician. The Annals of Statistics, 12, 1151–1172.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of Statistics, 6, 461–464.
Spiegelhalter, D. J., Best, N. G., Carlin, B. R., & van der Linde, A. (2002). Bayesian measures of
model complexity and ﬁt. Journal of the Royal Statistical Society B, 64, 583–616.
Watanabe, S. (2010). Asymptotic equivalence of Bayes cross validation and widely applicable
information criterion in singular learning theory. Journal of Machine Learning Research, 11,
3571–3594.
Watanabe, S. (2013). A widely applicable Bayesian information criterion. Journal of Machine
Learning Research, 14, 867–897.

Chapter 3
Advanced Bayesian Computation
Abstract The popularity of Bayesian statistics is largely due to advances in com-
puting and developments in computational methods. Currently, there are two main
types of Bayesian computational methods. The ﬁrst type involves iterative Monte
Carlo simulation and includes the Gibbs sampler, the Metropolis-Hastings algorithm,
Hamiltonian sampling etc. These ﬁrst type methods typically generate a Markov
chain whose stationary distribution is the target distribution. The second type involves
distributional approximation and includes Laplace approximation (Laplace 1785,
1810), variational Bayes (Jordan et al. 1999), etc. These second type methods try to
ﬁnd a distribution with the analytical form that best approximates the target distribu-
tion. In Sect. 3.1, we review Markov chain Monte Carlo (MCMC) methods including
the general Metropolis-Hastings algorithm (M-H), Gibbs sampler with conjugacy,
and Hamiltonian Monte Carlo (HMC) algorithm (Neal 1994). Section 3.2 discusses
the convergence and efﬁciency of the above sampling methods. We then show how
to specify a Bayesian model and draw model inferences using OpenBUGS and Stan
in Sect. 3.3. Section 3.4 provides a brief summary on the mode-based approxima-
tion methods including Laplace approximation and Bayesian variational inference.
Finally, in Sect. 3.5, a full Bayesian analysis is performed on a biological data set
from Gelfand et al. (1990). The key concepts and the computational tools discussed
in this chapter are demonstrated in this section.
3.1
Markov Chain Monte Carlo (MCMC) Methods
InSect.2.1,wediscussedhowtomakeinferencesaboutparametersfromtheposterior
distribution. When the posterior distribution is complicated, it is tedious to make any
inferences analytically. We have seen that in Example 2.3 the marginal posterior
distribution p(λ|yyy) contains a complicated integral. Even if p(λ|yyy) can be found
analytically, it still requires some effort to get the exact posterior mean and the
CPDR of λ. This motivates us to explore other methods.
Monte Carlo simulation is a sampling process from a target distribution. Once
sufﬁcient samples are obtained, the inferences of target distribution can be approx-
imated by sample statistics, such as sample mean, sample standard error, sample
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_3
35

36
3
Advanced Bayesian Computation
percentile etc. The traditional Monte Carlo simulation methods involve inversing the
cumulative distribution function, the rejection sampling method, etc. These methods
generate independent samples. In contrast, Markov chain Monte Carlo (MCMC)
methods generate a Markov chain whose stationary distribution is equivalent to
the target distribution. In MCMC, the next sampled value typically depends on the
previous sampled value.
In this section, we ﬁrst brieﬂy state some properties of a Markov chain with a
stationary distribution. Then the Metropolis-Hastings (M-H) algorithm, Gibbs sam-
pler and Hamiltonian Monte Carlo (HMC) are reviewed. Throughout this section,
we continue with Example 2.3. We compare the MC-based inferences to analytical
inferences.
3.1.1
Markov Chain and Its Stationary Distribution
Let S be a ﬁnite set. A Markov chain is characterized by a transition matrix KKK(s, s′)
with KKK(s, s′) ≥0 for any s, s′ ∈S , and 
s′∈S KKK(s, s′) = 1 for any s′ ∈S . All
of the Markov chains considered in this chapter have a stationary distribution π(s)
which satisﬁes the equation

s∈S
π(s)KKK(s, s′) = π(s′).
The stationary theorem of Markov chains says, under a simple connectedness condi-
tion, π is unique, and high powers of KKK converge to a rank one matrix with all rows
equal to π. That is
KKK n(s, s′) →π(s′) for s, s′ ∈S .
The probabilistic content of the theorem is that from any starting state s, the nth step
of a run of the Markov chain has a chance close to π(s′) of being at s′ if n is large. In
computational settings, when the cardinality of S is large, it is easy to move from s
to s′ according to KKK(s, s′), but it is hard to sample from π directly.
Example 3.1 (The stationary distribution of a Markov chain process). Suppose a
Markov chain with the sample space S = {0, 1, 2, 3}, and a transition matrix as
follows:
KKK =
⎛
⎜⎜⎝
0.9 0.1 0
0
0.9 0 0.1 0
0.9 0
0 0.1
0.9 0
0 0.1
⎞
⎟⎟⎠.
A little more calculation shows that

3.1 Markov Chain Monte Carlo (MCMC) Methods
37
KKK 4 =
⎛
⎜⎜⎝
0.9 0.09 0.009 0.001
0.9 0.09 0.009 0.001
0.9 0.09 0.009 0.001
0.9 0.09 0.009 0.001
⎞
⎟⎟⎠,
so that for some m ≥4, KKK m(s1, s′) = KKK m(s2, s′) for all s1, s2 ∈S . It follows that
KKK m+1 = KKK m, since
KKK m+1(s, s′) =

v∈S
KKK (s, v) KKK m 
v, s′
= KKK m(s, s′)

v∈S
KKK (s, v) = KKK m(s, s′).
Therefore, limn→∞KKK n(s, s′) = KKK m(s, s′) = π(s′), where we write the ﬁnal equality
without reference to s since all the rows of KKK m(s, s′) are identical. π(s′) is the
stationary distribution.
3.1.2
Single-Component Metropolis-Hastings (M-H)
Algorithm
Suppose we want to simulate a sample of θ. When θ contains multiple variables,
instead of sampling the whole θ at a time, it is often more convenient and com-
putationally efﬁcient to divide θ into components as {θ1, θ2, . . . , θh}, and sample
these components one by one, i.e., using single-component Metropolis-Hastings
algorithm.
An iteration of the single-component Metropolis-Hastings algorithm comprises h
updating processes. Suppose θ is updated sequentially according to the component
index and the target multivariate distribution as π. The ith updating process for θi at
the tth iteration in the M-H algorithm works as follows:
1. Draw a value from a proposal distribution of θi, gi

θ∗
i |θt−1
i
, θt
−i

, where θt
−i =

θt+1
1
, . . . , θt+1
i−1 , θt
i+1, . . . , θt
h

, and θt−1
i
denotes the value of θi at the end of
iteration t −1 or denotes the initial value when t = 1.
2. Calculate the acceptance ratio
Ai

θ∗
i , θt−1
i

= π

θ∗
i |θt
−i

gi

θt−1
i
|θ∗
i , θt
−i

π

θt−1
i
|θt
−i

gi

θ∗
i |θt−1
i
, θt
−i
,
where π

·|θt
−i

is the full conditional distribution of θi.
3. Accept θ∗
i and set θt
i = θ∗
i with probability Ai

θ∗
i , θt−1
i

. Otherwise, reject θ∗
i
and set θt
i = θt−1
i
.
Note that the parameters in the proposal distribution gi are called tuning parameter;
these are speciﬁed in advance and will affect the acceptance rates and the conver-
gence. In Sect. 3.3, we will see that OpenBUGS has a phase called “adapting”,

38
3
Advanced Bayesian Computation
when the program automatically chooses the appropriate tuning parameters. In the
M-H algorithm, we need to discard the ﬁrst few iterations, which are called burn-in.
We judge the length of burn-in by looking at trace plots, BGR plots (Gelman and
Rubin 1992) or potential scale reduction factor (Gelman et al. 2014), which will be
discussed in Sect. 3.2.1.
Example 3.2 (An autoregressive process of order one). We continue with Example
2.3 on page 15. Now we complete the following tasks:
1. Write a M-H algorithm to generate a sample of size T = 1000 from the joint pos-
terior distribution p (α, λ|xxx), and produce trace plots for α and λ. Also calculate
the acceptance rates for both parameters.
2. Draw histograms for the sampled values in (1) and superimpose density estimates
of marginal posterior distributions, p (α|xxx) and p (λ|xxx). Estimate the posterior
means ˆα and ˆλ and give the 95% conﬁdence intervals for them. Also report the
95% CPDR estimates for α and λ.
Solutions to (1):
Instead of using p (α, λ|xxx) directly, we take the logarithm of it, denoted by l (α, λ|xxx),
and calculate the acceptance ratio on a logarithm scale. The tth iteration in the M-H
algorithm is as follows:
1. Draw a proposed value α∗∼U (α −c, α + c) . If α∗/∈[−1, 1] , reject it and
redraw. Otherwise, calculate the acceptance ratio:
Aα

α∗, αt−1
= exp

l

α∗|xxx, λt−1
−l

αt−1|xxx, λt−1
,
where αt−1 and λt−1 are the values at the end of the (t −1)th iteration or the
initial values when t = 1. If Aα

α∗, αt−1
≥1, accept α∗and set αt = α∗. If
Aα

α∗, αt−1
≤1, accept α∗and set αt = α∗with probability of Aα

α∗, αt−1
;
otherwise, set αt = αt−1.
2. Draw a proposed value λ∗∼U (α −d, α + d). If λ∗< 0, reject it and redraw.
Otherwise, calculate the acceptance ratio:
Aλ

λ∗, λt−1
= exp

l

λ∗|xxx, αt
−l

λt−1|xxx, αt
,
where αt comes from step 1. If Aλ

λ∗, λt−1
≥1, accept λ∗and set λt = λ∗. If
Aλ

λ∗, λt−1
≤1, accept λ∗and set λt = λ∗with probability of Aλ

λ∗, λt−1
;
otherwise, set λt = λt−1.
With c = 0.3, d = 0.2, α0 = 0, λ0 = 1, the M-H algorithm converges within 100
iterations with acceptance rate of 71% for α and 69% for λ over a total of 10, 000
iterations. The trace plots for α and λ are shown in Fig. 3.1.
Solutions to (2):
The last 9,900 sampled values are used for inference. The MC estimate of posterior
mean ˆα is ¯α = (10000
t=101 αt)/9900 = 0.4721, with the 95% CI

3.1 Markov Chain Monte Carlo (MCMC) Methods
39
0
2000
4000
6000
8000
10000
−0.5
0.0
0.5
1.0
Iterations
α
0
2000
4000
6000
8000
10000
0.2
0.4
0.6
0.8
1.0
Iterations
λ
Fig. 3.1 The trace plots of α and λ

¯α −1.96

Var (α)
9900 ,
¯α + 1.96

Var (α)
9900

= (0.4683, 0.4759) ,
where Var (α) is the MC sample variance (i.e., the sample variance of αt, t =
101, . . . , 10000). The MC estimate of 95% CPDR for α is (0.0726, 0.8188).
Similarly, the MC estimate of posterior mean ˆλ is ¯λ = (10000
t=101 λt)/9900 =
0.4101, with the 95% CI (0.4075, 0.4126). The MC estimate of 95% CPDR for
λ is (0.1947, 0.6959). We show the MC histograms and the MC density estimates
comparing with the exact densities in Fig. 3.2. We can see the MC estimates are quite
close to the exact values.

40
3
Advanced Bayesian Computation
α
Density
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
●●
●
●
●
●
M−H density estimate
M−H posterior mean estimate
M−H 95% CPDR estimate
Exact density
Exact posterior mean
Exact 95% CPDR
α0
λ
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
●●
●
●
●
●
M−H density estimate
M−H posterior mean estimate
M−H 95% CPDR estimate
Exact density
Exact posterior mean
Exact 95% CPDR
λ0
Fig. 3.2 The MC estimates of α and λ using M-H
Since there is strong series dependence in a Markov chain, it is not good to make
inferences directly from the original MCMC sample. Two methods can be applied to
reduce the dependence: the batch means (BM) method and the thinning sample (TS)
method. We will discuss these two methods in more detail in Sect. 3.2.2. In the batch
means method we place 20 bins and in the thinning sample method we extract one
value from every 20 successive samples. Table 3.1 lists the inferences made from the
two methods. Note that * indicates the exact posterior mean is in the 95% CI.

3.1 Markov Chain Monte Carlo (MCMC) Methods
41
Table 3.1 The MC, BM, TS estimates of the posterior means and the associated 95% CIs using
the M-H algorithm
MC est.
MC CI
BM CI
TS CI
Exact
ˆα
0.4721
(0.4683, 0.4759)
(0.4598, 0.4845)∗(0.4461, 0.4800)
0.4814
ˆλ
0.4101
(0.4075, 0.4126)
(0.4047, 0.4154)∗(0.3982, 0.4208)
0.4129
3.1.3
Gibbs Sampler
The Gibbs sampler is another MCMC method which simulates the joint distribution
via full conditional distributions. In fact, if we choose the full conditional distribution
of each component in single-component M-H algorithm as the proposal distribution
for this component, i.e., gi

θ∗
i
θt−1
i
, θt
−i

= π

θ∗
i
θt
−i

, the acceptance ratio will be
Ai

θ∗
i , θt−1
i

= π

θ∗
i
θt
−i

π

θt−1
i
θt
−i

π

θt−1
i
θt
−i

π

θ∗
i
θt
−i
 = 1,
which guarantees the proposed value θ∗
i being accepted. So the Gibbs sampler is a
special case of the M-H algorithm.
Compared with the M-H algorithm, the Gibbs sampler does not have the accept-
reject step and tuning parameters. However, the main difﬁculty with the Gibbs sam-
pler is simulating from the full conditional distribution which sometimes does not
have a recognizable form. In that case, we may turn to other sampling methods such
as adaptive rejection sampling (Gilks and Wild 1992); see details in Appendix B on
page 196.
Adaptive rejection sampling is a generalized rejection sampling method that can
be used to simulate for any univariate log-concave probability density function. As
sampling proceeds, the rejection envelope and the squeezing function converge to
the target function. The adaptive rejection sampling and the M-H algorithm are both
intended for the situation where there is non-conjugacy of the Gibbs sampler in a
Bayesian model.
Example 3.3 (An autoregressive process of order one). We continue with Example
2.3. The Gibbs sampler is applied to the joint posterior distribution of α and λ. The
full conditional distributions are
p (α|xxx, λ) ∝

1 −α2 1
2 exp

−λ
2 h (xxx, α)

λ|xxx, α ∼Gamma
n
2, h (xxx, α)
2

.
The full conditional distribution of α is unrecognisable. We can write a Gibbs sampler
for λ and keep the M-H algorithm for α.

42
3
Advanced Bayesian Computation
Table 3.2 The MC, BM, TS estimates of the posterior means and the associated 95% CIs using a
Gibbs sampler
MC est.
MC CI
BM CI
TS CI
Exact
ˆα
0.477
(0.473, 0.480)
(0.466, 0.487)
(0.451, 0.484)
0.481
ˆλ
0.413
(0.411, 0.416)
(0.411, 0.416)
(0.398, 0.420)
0.413
ˆx21
0.363
(0.331, 0.395)
(0.329, 0.396)
(0.210, 0.491)
0.352
To simulate x21, we add an extra step to every iteration: draw a value from
N

αtx20, 1/λ j t
, where α j t, λ j t are the ending values at the tth iteration. Simi-
lar to Table 3.1, we can obtain the new MC estimates based on the Gibbs sampler
as shown in Table 3.2. Another way to ﬁnd the posterior mean and the posterior
marginal density is to apply the Rao-Blackwell (RB) method. We can estimate the
marginal posterior distribution of λ as
¯p (λ|xxx) = 1
T
T

t=1
Gamma

λ

n
2, h

xxx, αt
2

,
where αt is the tth sampled value from the posterior distribution p (α|xxx). The pos-
terior mean is estimated as
¯λ = 1
T
T

t=1
n
h (xxx, αt).
The 95% CI for posterior mean is calculated as (¯λ ± 1.96s/
√
T ), where s is the
sample standard deviation of n/h

xxx, αt
, t = 1, . . . , T.
Similarly, we can estimate the posterior predictive distribution of x21 as
¯p (x21|xxx) = 1
T
T

t=1
p

x20
xxx, αt, λt
= 1
T
T

t=1
N

x21
αtx20, 1
λt

.
The posterior mean of x21 is estimated as
¯x21 = 1
T
T

t=1
αtx20.
The 95% CI for the posterior mean is calculated as

¯x21 ± 1.96s/
√
T

, where s is the
sample standard deviation of αtx20, t = 1, . . . , T. We summarize the Rao-Blackwell
estimates in Fig. 3.3. We see that the 95% RB CIs cover the exact posterior means,
and the RB density estimate of λ is almost equal to its exact density.

3.1 Markov Chain Monte Carlo (MCMC) Methods
43
λ
Density
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
●●
●
●
●
●
RB density estimate
RB posterior mean estimate
RB 95% CI
Exact density
Exact posterior mean
λ0
x21
Density
−4
−2
0
2
4
6
0.00
0.05
0.10
0.15
0.20
0.25
●●
●
●
RB density estimate
RB posterior mean estimate
RB 95% CI
Exact posterior mean
Fig. 3.3 The Rao-Blackwell estimates of λ and x21
3.1.4
Hamiltonian Monte Carlo (HMC)
Hamiltonian Monte Carlo (HMC) was introduced to physics by Duane et al. (1987)
and to statistical problems by Neal (1994, 2011). In contrast to the random-walk
Metropolisalgorithmwheretheproposedvalueisnotrelatedtothetargetdistribution,
HMCproposes a value by computing a trajectory according to Hamiltonian dynamics
that takes account of the target distribution.

44
3
Advanced Bayesian Computation
3.1.4.1
Hamiltonian Dynamics
Suppose we have a Hamiltonian dynamics scenario in which a frictionless ball slides
over a surface of varying height. The state of the ball at any time consists of the
position and the momentum. Denote the position by a h vector θ and the momentum
by a same length vector φ.
Hamiltonian functions can be written as
H(θ, φ) = U(θ) + K(φ),
where U(θ) is called the potential energy and will be deﬁned to be minus the log
probability density of the distribution of θ we wish to simulate, K(φ) is called the
kinetic energy and is usually deﬁned as
K(φ) = φTΣΣΣ−1φ/2,
whereΣΣΣ−1 is a symmetric positive-deﬁnite “mass matrix” which is typically diagonal
and is often a scalar multiple of the identity matrix. This form of K(φ) corresponds
to the minus log probability density of the zero-mean Gaussian distribution with
covariance matrix ΣΣΣ.
The state of the ball in the next inﬁnitesimal time is determined by Hamilton’s
equations of motion:
dθi
dt = ∂H
∂φi
= [ΣΣΣ−1φ]i
dφi
dt = −∂H
∂θi
= −∂U
∂θi
.
For computer implementation, Hamilton’s equation must be approximated by
discretizing time, using some small step size, ε. The most straightforward method
is Euler’s method. The solution to the above system of differential equations can be
approximated by Euler’s method as follows:
φi(t + ε) = φi(t) + εdφi
dt (t) = φi(t) −ε∂U
∂θi
(θi(t))
θi(t + ε) = θi(t) + εdθi
dt (t) = θi(t) + ε[ΣΣΣ−1φ]i.
However, Euler’s method does not preserve the volume and the resulting trajectory
would diverge from the exact trajectory to inﬁnity. A better trajectory may be gen-
erated by using the leapfrog method as follows:

3.1 Markov Chain Monte Carlo (MCMC) Methods
45
φi(t + ε/2) = φi(t) −(ε/2)∂U
∂θi
(θi(t))
θi(t + ε) = θi(t) + ε[ΣΣΣ−1φ(t + ε/2)]i
φi(t + ε) = φi(t + ε/2) −(ε/2)∂U
∂θi
(θi(t + ε)).
The leapfrog method preserves volume exactly.
3.1.4.2
MCMC from Hamiltonian Dynamics
Suppose we want to simulate a sample from the target density p(θ). HMC intro-
duces auxiliary momentum variables φ and draws from a joint density p(θ, φ). We
assume the auxiliary density is a multivariate Gaussian distribution, independent of
the parameter θ. The covariance matrix ΣΣΣ acts as a Euclidean metric to rotate and
scale the target distribution. The joint density p(θ, φ) deﬁnes a Hamiltonian function
as follows:
H(θ, φ) = −log p(θ, φ) = −log p(θ) −log p(φ) = U(θ) + K(φ).
Starting from the value of the parameters at the end of the t −1th iteration, θt−1, a
new value θ∗is proposed by two steps before being subjected to a Metropolis accept
step.
First, a value φt−1 for the momentum is drawn from the multivariate Gaussian
distribution, N(0,ΣΣΣ). Next the joint system (θt−1, φt−1) is evolved via the following
leapfrog method for L steps to get the proposed value (θ∗, φ∗):
φt−1+ε/2
i
= φt−1
i
+ (ε/2)∂log p(θt−1)
∂θi
θt−1+ε
i
= θt−1
i
+ ε[ΣΣΣ−1φt−1+ε/2]i
φt−1+ε
i
= φt−1+ε/2
i
+ (ε/2)∂log p(θt−1+ε)
∂θi
.
Note that θ∗= θt−1+εL, φ∗= φt−1+εL. If there were no numerical errors in the
leapfrog step (i.e., the leapfrog trajectory followed the exact trajectory), we would
accept (θ∗, φ∗) deﬁnitely. However, there are always errors given the non-zero step
size. Hence, we conduct a Metropolis accept step with the acceptance rate as
min{1, exp[H(θt−1, φt−1) −H(θ∗, φ∗)]}.
Neal (1994) suggests that HMC is optimally efﬁcient when its acceptance rate is
approximately 65% while the multi-dimensional M-H algorithm is optimal at an
acceptance rate of around 23%.

46
3
Advanced Bayesian Computation
3.1.4.3
The No-U-Turn Sampler (NUTS)
There are three tuning parameters in HMC: the mass matrix ΣΣΣ, the step size ε and
the number of steps L. If ε is too large, the resulting trajectory will be inaccurate and
too many proposals will be rejected. If ε is too small, too many steps will be taken by
the leapfrog method, leading to long simulation times per iteration. If L is too small,
the trajectory traced out will be too short and sampling will devolve to a random
walk. If L is too large, the algorithm will spend too much time in one iteration. The
mass matrix ΣΣΣ needs to be comparable with the covariance of the posterior.
In MCMC, all the tuning parameters should be ﬁxed during the simulation that
will be used for inference; otherwise the algorithm may converge to the wrong
distribution. BUGS has an adaptive period during which suitable tuning parameters
are selected.
NUTS (Homan and Gelman 2014) can dynamically adjust the number of leapfrog
steps at each iteration to send the trajectory as far as it can go during that iteration.
If such a rule is applied alone, the simulation will not converge to the desired target
distribution. The full NUTS is more complicated, going backward and forward along
the trajectory in a way that satisﬁes detailed balance (Gelman et al. 2014).
The programming of NUTS is much more complicated than a M-H algorithm.
We rely on Stan to implement NUTS inferential engine. More details of Stan are
provided in Sect. 3.3. Along with this algorithm, Stan can automatically optimize
ε to match an acceptance rate target and estimate ΣΣΣ based on warm up iterations.
Hence we do not need to specify any tuning parameters in Stan.
3.2
Convergence and Efﬁciency
Two concerns in MCMC methods are checking the convergence of sampled values
and designing an efﬁcient algorithm.
3.2.1
Convergence
We can detect the convergence by eye, relying on the trace plots such as Fig. 3.1.
Informally speaking, a “fat hairy caterpillar” appearance indicates the convergence.
For numerical diagnosis, we use the Brooks-Gelman-Rubin (BGR) ratio and potential
scale reduction factor, both of which are based on the mixture and stationarity of
simulated multiple chains starting from diversiﬁed initial values.

3.2 Convergence and Efﬁciency
47
3.2.1.1
The Brooks-Gelman-Rubin (BGR) Ratio
The numerical diagnosis for convergence in OpenBUGS is based on comparing
within- and between- chain variability (Gelman and Rubin 1992). Suppose we sim-
ulate I chains, each of length J, with a view to assessing the degree of stationarity
in the ﬁnal J/2 iterations. We take the width of 100 (1 −α) % credible interval for
the parameter of interest as a measure of posterior variability.
From the ﬁnal J/2 iterations we calculate the width of empirical 100 (1 −α) %
credible interval for each chain as Wi,
i = 1, . . . , I, then ﬁnd the average width
across these chains as W = I
i=1 Wi/m. We also pool I J iterations together and
ﬁnd the pooled width B.
The BGR ratio is deﬁned as the ratio of pooled interval widths to average interval
widths, ˆRBGR = B/W. It should be larger than 1 if the starting values are suitably
diversiﬁed and will tend to be 1 as convergence is approached. So we can assume
convergence for practical purposes if ˆRBGR < 1.05.
Brooks and Gelman (1998) further suggested splitting the total iteration range
of each chain into M batches of length a = J/M and calculating B(m), W(m) and
ˆRBGR(m) based on the latter halves of iterations (1, . . . , ma) for m = 1, . . . , M.
3.2.1.2
The Potential Scale Reduction Factor
Gelman et al. (2014) propose a similar quantity to monitor the convergence, namely
potential scale reduction factor. This factor is automatically monitored in Stan.
Again, suppose we simulate I chains, each of length J (this is all after discarding the
burn-in iterations). We split each chain into two parts to get 2I batches, each of length
J/2. We label the simulations as θi, j, i = 1, . . . , 2I, j = 1, . . . , J/2 and calculate
the between- and within- batch variances as a measure of posterior variability rather
than the width of credible interval as in BGR.
The average within-batch variance is WVar = 2I
i=1 s2
i , where s2
i is the sample
variance of the ith batch. The between-batch variance is
BVar =
J/2
2I −1
2I

i=1

 ¯θi· −¯θ
2 ,
where ¯θi· is the sample mean of ith batch and ¯θ is the pooled sample mean. The
reason for containing a factor of J/2 is that BVar is based on the sample variance of
batch means ¯θi·. Note that WVar and BVar are both estimates of the posterior variance
Var (θ|yyy). Later we will show that √BVar/I J is the MC standard error of the posterior
mean estimate using batch-mean method.
Gelman et al. (2014) proposes an estimate of Var (θ|yyy) as a weighted average of
WVar and BVar:

Var (θ|yyy) = J/2 −1
J
WVar + 1
J BVar,

48
3
Advanced Bayesian Computation
which is also an unbiased estimate under stationarity, but an overestimate if involving
the burn-in iterations. On the other hand, WVar always underestimates Var (θ|yyy) due
to limited sample size J/2 and dependent iterations. So we monitor convergence by
estimating the potential scale reduction factor by
ˆR =


Var (θ|y)
WVar
,
(3.1)
which declines to 1 as J →∞. If ˆR is high, we believe that more iterations are
needed to guarantee the convergence.
3.2.2
Efﬁciency
For a given sample size, the accuracy of our inferences is dependent on the efﬁciency
of the posterior sample, which decreases with an increasing level of autocorrelation.
We can improve the efﬁciency by reﬁning the algorithm or resampling from the MC
sample to reduce the correlation.
3.2.2.1
Reparameterization, Thinning and Adding Auxiliary Variables
One way of increasing efﬁciency is to reparameterize the model so that the posterior
correlation among parameters is reduced, as shown in Example 3.4 and Sect. 3.5.4.
Another way to improve efﬁciency is to perform a process known as thinning,
whereby only every vth value from the MC sample is actually retained for inference.
In Sect. 3.3, we will see there is an option of “thin” in the OpenBUGS Update Tool
window.
Finally, the Gibbs sampler can often be simpliﬁed or the convergence can be
accelerated by adding an auxiliary variable (Gelman et al. 2014).
3.2.2.2
The Batch Means Method
In Example 2.3 we are interested in the 95% CI of the posterior mean. The standard
error of the posterior mean estimate (i.e., the MC sample mean) is calculated by
the sample standard deviation over the squared root of sample size. This follows the
central limit theorem (CLT) under the condition of independent samples. However,
the MC sample is from a Markov chain and each sampled value depends on the
previous sampled value. The MC sample variance is not an accurate estimate of the
posterior variance Var (θ|yyy). We will turn to the batch means method to get a more
accurate estimate.

3.2 Convergence and Efﬁciency
49
Suppose we have I chains, each of length J, and split every chain into M batches,
each of length J/M, and J/M is sufﬁciently large that CLT holds for each batch.
We label the simulations as θi j, i = 1, . . . , I M, j = 1, . . . , J/M.
We calculate the batch means ¯θi·, which are roughly independent and identically
distributed with a mean of posterior mean and a variance of posterior variance over
J/M. Then we can use the sample variance of batch means to estimate the posterior
variance as

Var (θ|yyy) =
J/M
I M −1
I M

i=1

 ¯θi· −¯θ
2.
The standard error of the posterior mean estimate ¯θ = 
i j θi j/(I J) can be approx-
imated more accurately by


Var (θ|yyy)
I J
=




1
I M (I M −1)
I M

i=1

 ¯θi· −¯θ
2,
(3.2)
which is also called the Monte Carlo standard error given in the “MC_error” column
in OpenBUGS output and the “se_mean” column in Stan. Using the batch means
method, the 95% CI of ˆθ is modiﬁed as

¯θ ± 1.96


Var (θ|yyy)/(I J)

.
3.2.2.3
Effective Sample Size
Gelman et al. (2014) deﬁned an estimate of effective sample size as
neff =
IJ
1 + 2 ∞
t=1 ρt
,
(3.3)
where I, J follow the notation in batch means method and ρt is the autocorrelation
of the MC sample at lag t. Stan automatically monitors neff for each parameter of
interest and gives them in the column of n_eff.
Example 3.4 (Reparameterize a simple linear regression model). Consider the
simple linear regression model: yi ∼N

a + bxi, σ 2
, i = 1, . . . , 30, with true
parameters a = 17, b = 3, σ 2 = 16. Assume xxx = (0.5, 1.0, . . . , 15) and generate
a response vector yyy = (y1, y2, . . . , y30) . We assume a non-informative prior, i.e.,
p

a, b, σ 2
∝1/σ 2.
Gibbs sampler (1):
A Gibbs sampler which could be applied here is based on the following full condi-
tional distributions:

50
3
Advanced Bayesian Computation
a|· ∼N

1
n
n

i=1
(yi −bxi) , σ 2
n

b|· ∼N
n
i=1 xi (yi −a)
n
i=1 xi 2
,
σ 2
n
i=1 xi 2

σ 2|· ∼Inv-Gamma
n
2,
n
i=1 (yi −a −bxi)2
2

.
The dependence of p (a|·) on b makes the Gibbs sampler (1) ineffective, especially
for σ 2. We reparameterize the simple linear regression model as
yi ∼N

c + b (xi −¯xxx) , σ 2
,
where c = a + b¯xxx. The prior for c can be shown as N (a + b¯xxx, ∞). So c also has a
non-informative ﬂat prior.
Gibbs sampler (2):
An alternative Gibbs sampler is based on the following full conditional distributions:
c|· ∼N

1
n
n

i=1
yi, σ 2
n

b|· ∼N
n
i=1 (xi −¯xxx) yi
n
i=1 (xi −¯xxx)2 ,
σ 2
n
i=1 (xi −¯xxx)2

σ 2|· ∼Inv-Gamma

n
2,
n
i=1 (yi −c −bxi + b¯xxx)2
2

,
where p (c|·) does not depend on b and p(b|·) is not dependent on c. The inde-
pendence between full conditional distributions will make Gibbs sampler (2) more
effective than Gibbs sampler (1).
We compare the MC estimates and the least-squares estimates in Table 3.3. Gibbs
sampler (2) improves the MC estimates of posterior means ˆσ 2, ˆy′, while performing
equally well for ˆa and ˆb as Gibbs sampler (1).
Table 3.3 Comparison of the least-squared estimates with the MC estimates using different Gibbs
samplers
Estimation method
ˆσ 2
95% CPDR
ˆy′
95% CI/CPDR
L-S estimates
22.81
NA
32.83
(22.82, 42.84)
Gibbs sampler (1)
34.35
(15.26, 86.18)
32.95
(20.56, 45.44)
Gibbs sampler (2)
24.56
(14.30, 41.55)
32.81
(22.67, 43.10)

3.3 OpenBUGS and Stan
51
3.3
OpenBUGS and Stan
The MCMC methods are useful Bayesian model computation tools, especially when
the posterior distribution does not have a closed form. The programming of MCMC
requires a lot of effort, even for a simple linear regression model as in Example 3.4.
Moreover, we need to customize a MCMC algorithm for every model. To relieve the
burden of programming MCMC, several packages have been developed. The two
main statistical packages are BUGS and Stan. We will see in this section how to use
these two packages to do a Bayesian analysis.
3.3.1
OpenBUGS
BUGS stands for Bayesian inference Using Gibbs Sampler. The BUGS project began
in 1989 and has developed into two versions: WinBUGS and OpenBUGS. Currently
all development is focused on OpenBUGS. As its name suggests, OpenBUGS uses
a Gibbs sampler which updates unknown quantities one by one, based on their full
conditional distribution.
The MCMC building blocks include the conjugacy Gibbs sampler, the M-H algo-
rithm, various types of rejection sampling and slice sampling; see details in Appendix
B on page 197. Such methods are used only as a means of updating full conditional
distributions within a Gibbs sampler. OpenBUGS has an “expert system”, which
determines an appropriate MCMC method for analysing a speciﬁed model.
3.3.1.1
Directed Graphical Models
Suppose we have a set of quantities G arranged as a direct acyclic graph, in which
each quantity υ ∈G represents a node in the graph. The “intermediate” nodes always
have “parents” and “descendants”. The relationship between parent and child can be
logical or stochastic. If it is a logical relationship, the value of the node is determined
exactly by its parents. If it is a stochastic relationship, the value of the node is
generated by a distribution which is determined only by its parents.
Conditional on its parents, denoted by pa [υ], υ is independent of all the other
nodes except its descendants, denoted by ch [υ]. This conditional independence
assumption implies the joint distribution of all the quantities G has a simple fac-
torization in terms of the conditional distribution p(υ|pa [υ]), as follows:
p (G ) =
 
υ∈G
p(υ|pa [υ]),

52
3
Advanced Bayesian Computation
where the conditional distribution may degenerate to a logical function of its parents
if the relationship is logical. The full joint distribution p (G ) can be fully speciﬁed
by the parent-child relationships.
The crucial idea behind BUGS is that this factorization forms the basis for both
the model description and the computational methods. The Gibbs sampler for each
unknown quantity θi, is based on the following full conditional distribution:
p (θi|θ−i, yyy) ∝p (θi|pa [θi]) ×
 
υ∈ch[θi]
p (υ|pa [υ]) .
Note that θi can be any unknown quantities, not just unknown parameters. An impor-
tant implication of directed graphical models is that every node should appear in the
left side of an assignment sign only once. This implication can be used as a debugging
tool of the BUGS language.
3.3.1.2
The BUGS Language
For a complex model, it is better to use the BUGS language to specify the model
rather than using a graphical model. It takes time for R users to get used to BUGS.
The fundamental difference is the declarative language in BUGS, so it does not
matter in which order the statements come in BUGS.
3.3.2
Stan
Stan stands for Sampling Through Adaptive Neighbourhoods, which applies the no-
U-turn sampler (NUTS). Besides the no-U-turn sampler, Stan can also approximate
Bayesian inference using variational Bayes, which will be discussed in Sect. 3.4.2,
and do penalized maximum likelihood estimation if we specify the priors as the
penalized term.
The key steps of the algorithm include data and model input, computation of the
log posterior density (up to an arbitrary constant that cannot depend on the parameters
in the model) and its gradients, a warm-up phase in which the tuning parameters, ε
and M, are set, an implementation of NUTS to move through the parameter space,
convergence monitoring, and inferential summaries at the end.
Compared with OpenBUGS, Stan works seamlessly with R. Stan is installed as a
package in R. The output from Stan is stored in R automatically and can be analyzed
and plotted in R directly. Instead, BUGS works by itself. BUGS has its own graph
tools and output form. The output from BUGS needs to be transferred into another
package such as R before it can be used for further analysis.

3.3 OpenBUGS and Stan
53
Fig. 3.4 The graphical
model for AR(1)
Stan can analyze all the BUGS examples. It provides more instructive error mes-
sages than BUGS. This is particularly helpful when we work with a “black box”
inferential engine. Stan can solve the multi-level models with unknown covariance
matrices which BUGS can not easily deal with. Moreover, it is easier to specify the
constraints of parameters in Stan.
Example 3.5 (An autoregressive process of order one). We continue with Example
3.2. Rather than programming the MCMC, we rely on BUGS and Stan to make
inference.
BUG:
A graphical model (also called a Doodle) representation is shown in Fig. 3.4. For
the simplicity, we only assume 6 observations. The single arrows imply stochastic
relationship, while double arrows imply logical relationship. A “parent” constant is
denotedbyasquaredplate,whileothernodesaredenotedbyellipseplates.TheBUGS
can generate codes from graphical model by using “pretty print” under “model”
menu.
The modelling procedure using BUGS language typically includes the following
steps:
1. Check the syntax of the model speciﬁcation by “Speciﬁcation Tool”; if the model
is correctly speciﬁed, the message “model is syntactically correct” will appear
on the bottom left of the screen.

54
3
Advanced Bayesian Computation
2. Read in the following data by clicking “load data”:
list (K=20 ,x=
c ( -0.58196581 , -1.70339058 , -4.29434356 , -2.00495593 ,
-0.09234224 , -1.56433489 , -0.49151508 , -1.55912920 ,
-0.90546327 , -1.31576285 , -1.12240668 ,
0.50931757 ,
0.54899741 , -1.87582922 , -4.54187225 , -0.41553845 ,
0.31656492 , -0.32832899 ,
1.69457825 ,
0.73050020) ).
The message “data loaded” will appear.
3. Specify the number of chains as 2 and compile the model. The message “model
compiled” will appear.
4. Load the following initial values:
list ( alpha = -0.99 , lambda =100)
list ( alpha =0.99 ,
lambda =0.001) .
The message “model initialised” or “initial values loaded but chain contains
uninitialised variables” will appear. In the second case, we need to click “gen
inits”, which will generate initial values from priors.
After compiling and loading data, BUGS will choose an appropriate MCMC
methodforeachunknownquantity,whichisshownunderthemenu“Info/Updater
types”.
5. Start the simulation using “Update Tool”. We have the following options:
• Thin: Every kth iteration will be used for inference.
• Adapting: This will be ticked while the M-H or slice sampling is in its initial
tuning phase where some optimization parameters are tuned.
• Over relax: This generates multiple samples at each iteration and then selects
one that is negatively correlated with the current value. The within-chain
correlations should be reduced.
6. Monitor the interested unknown quantities using “Sample Monitor Tool”. Typing
* into the node box means all monitored nodes.
7. Diagnose the convergence via “bgr diag” plots and trace plots shown in Fig. 3.5.
MCMC converges after 750 iterations, so we can rely on the subsequent iterations
to make inferences.
8. Report the inferences. We can get the inference by clicking “stats” in “Sample
Monitor Tool” window. OpenBUGS also automatically reports DIC, pD, 
D (θ)
(shown as “Dbar”), and D( ˆθ) (shown as “Dhat”). In this example, pD is close to
the number of parameters. See the following output:
Dbar
Dhat
DIC
pD
x
75.83
73.86
77.81
1.975
total
75.83
73.86
77.81
1.975

3.3 OpenBUGS and Stan
55
Fig. 3.5 The BGR plots and the trace plots of α and λ from OpenBUGS

56
3
Advanced Bayesian Computation
α
Density
−0.4
−0.2
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
●
●Posterior mean estimate
λ
Density
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
1.5
2.0
2.5
3.0
●
●Posterior mean estimate
x21
Density
−4
−2
0
2
4
0.00
0.05
0.10
0.15
0.20
0.25
●
●Posterior mean estimate
log posterior density
Density
−26
−24
−22
−20
−18
0.0
0.1
0.2
0.3
0.4
Fig. 3.6 The MC estimates of α, λ and log posterior density from Stan

3.3 OpenBUGS and Stan
57
Stan:
Programming in Stan is more ﬂexible and easier than in BUGS. For example, there is
no need to specify ﬂat priors, logical operators are allowed in stochastic expressions,
constraints are easily incorporated, and there are more instructive error messages.
The Stan code is as follows:
1
modelcode <-"
2
data {
3
int < lower =0>
J;
4
real
x[J];
5
}
6
parameters {
7
real < lower =-1, upper =1>
alpha ;
8
real < lower =0>
sigma ;
9
real
x 21;
10
}
11
transformed
parameters {
12
real < lower =0>
sigma 1;
13
sigma 1<- sigma /(1 - alpha ^2);
14
}
15
model {
16
x[1]
~
normal (0, sigma 1);
17
for
(j
in
2:J)
x[j]
~
normal
( alpha *x[j -1] , sigma );
18
x21
~
normal ( alpha *x[J], sigma );
19
}
20
generated
quantities {
21
real
<lower =0>
lambda ;
22
lambda < -1/ sigma ^2;
23
}
24
"
25
stanmodel <- stan _ model ( model _ code = modelcode )
26
J= length (x20)
27
dat <- list (J=J,x=x 20)
28
fit <- stan ( model _ code = modelcode ,
data =dat ,
iter =1000 ,
chains =4)
29
print (fit ,par=c(" alpha "," lambda ","x 21") )
30
##
Inference
for
Stan
model :
modelcode .
31
##
4
chains ,
each
with
iter =1000;
warmup =500;
thin =1;
32
##
post - warmup
draws
per
chain =500 ,
total
post - warmup
draws =2000.
33
##
34
##
mean
se_ mean
sd
2.5%
25%
50%
75%
97.5%
n_eff
Rhat
35
##
alpha
0.45
0.01
0.19
0.05
0.33
0.47
0.59
0.80
853
1
36
##
lambda
0.39
0.00
0.13
0.18
0.30
0.38
0.47
0.70
1079
1
37
##
x21
0.39
0.05
1.70
-2.96
-0.71
0.35
1.53
3.73
962
1
38
##
39
##
Samples
were
drawn
using
NUTS ( diag _e)
at
Thu
Sep
10
23:23:28
2018.
40
##
For
each
parameter ,
n_ eff
is
a
crude
measure
of
effective
sample
size ,
41
##
and
Rhat
is
the
potential
scale
reduction
factor
on
split
chains
(at
42
##
convergence ,
Rhat =1).
Note that x20 is the data. We run iter=1000 iterations for each of four chains. By
default, Stan discards the ﬁrst half of each chain as burn-in. In the output, the last

58
3
Advanced Bayesian Computation
row is normalized log posterior density. The se_mean column contains the MC errors
deﬁned in Eq. (3.2). The last two columns correspond to n_eff and Rhat, which we
deﬁned in Eqs. (3.3) and (3.1). The posterior densities of α, λ, x21 and log posterior
density are shown in Fig. 3.6, which are similar to Figs. 3.2 and 3.3.
3.4
Modal and Distributional Approximations
The joint posterior modes can be found using the optimizing( ) function in Stan,
which applies the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm (Nocedal
and Wright 2006). Conditional maximization Newton’s method can also ﬁnd the pos-
terior joint modes. For the marginal modes, a well-known method is the expectation-
maximization (EM) algorithm.
3.4.1
Laplace Approximation
Once the posterior mode is found, we can approximate the target distribution by a
multivariate Gaussian distribution with the same mode and covariance matrix as the
inverse of the log posterior density curvature at the mode. This approximation works
well for large sample sizes following the asymptotic theory discussed in Sect. 2.1.4.
3.4.2
Variational Inference
When facing a difﬁcult problem for which we cannot give an exact solution, we
typically have two alternatives. One is to stick to this problem and give an approxi-
mation to the exact answer. That is what the MCMC methods do. We approximate
the exact posterior distribution using a Markov chain. The other is to introduce a
closely similar problem for which we can give an exact answer. That is what the
variational inference tries to do.
We introduce an approximate distribution family q that is easier to deal with than
p(θ|yyy). The log model evidence log p(yyy) can be written as follows:
log p(yyy) = log p(yyy, θ)
p(θ|yyy)
=
!
q(θ) log p(yyy, θ)
p(θ|yyy) dθ
=
!
q(θ)

log q(θ)
p(θ|yyy) + log p(yyy, θ)
q(θ)

dθ
=
!
q(θ) log q(θ)
p(θ|yyy)dθ +
!
q(θ) log p(yyy, θ)
q(θ) dθ
= KL[q||p] + F(q, yyy),

3.4 Modal and Distributional Approximations
59
where the ﬁrst term in the last line is called the Kullback-Leibler divergence between
q(θ) and p(θ|yyy), and the second term is called free energy. If we want to ﬁnd an
approximate distribution q to minimize KL[q||p], we can just maximize the free
energy since the model evidence is a constant given the sample.
3.4.2.1
Mean Field Variational Inference
Acommonchoiceofq(θ)istoassumeitcanbefactorizedintoindependentpartitions:
q(θ) =
h 
i=1
qi(θi).
This assumption is called mean ﬁeld assumption. Under this assumption, if we dissect
out the dependence on qk(θk), then the free energy can be written as
F(q, yyy) =
!
q(θ) log p(yyy, θ)
q(θ) dθ
=
!
h 
i=1
qi(θi) ×

log p(yyy, θ) −
h

i=1
log qi(θi)

dθ
=
!
qk(θk)
 
i̸=k
qi(θi) ×

log p(yyy, θ) −log qk(θk)

dθ
−
!
qk(θk)
 
i̸=k
qi(θi)

i̸=k
log qi(θi)dθ
=
!
qk(θk)
⎛
⎝
!  
i̸=k
qi(θi) log p(yyy, θ)dθ−k −log qk(θk)
⎞
⎠dθk
−
!
qk(θk)
⎛
⎝
!  
i̸=k
qi(θi)

i̸=k
log qi(θi)dθ−k
⎞
⎠dθk
=
!
qk(θk) log exp{Eθ−k log p(yyy, θ)}
qk(θk)
dθk + C
= −KL

qk(θk)|| exp{Eθ−k log p(yyy, θ)}

+ C.
Then the approximate distribution qk(θk) that maximizes the free energy is given by
q∗
k = argmax
qk
F(q, yyy) = exp{Eθ−k log p(yyy, θ)}
Z
.
Thisimpliesastraightforwardalgorithmforvariationalinference.Assumetheparam-
eters in distribution qk are φk. The algorithm consists of the following two steps:

60
3
Advanced Bayesian Computation
• Determine the form of the approximating distribution. Average log p (yyy, θ) over
q−k (θ−k) to ﬁnd the marginal approximate distribution q∗
k , whose parameters are
some function of parameters in q−k, φ−k.
• Iterative update φ. The ﬁrst step establishes the a circular dependence among φi.
We iterate φ until there are no more visible changes and use the last update q (θ|φ)
as an approximation to p (θ|yyy).
3.5
A Bayesian Hierarchical Model for Rats Data
We have seen a hierarchical model in Example 2.8. A hierarchical model is often used
when considering the variations on different levels. For most hierarchical models, the
posterior distribution does not have a closed form. We compute Bayesian inference
via programming a MCMC algorithm or using BUGS/Stan.
In this section, we reanalyze the rats’ weights data set shown in Table 3.4, and
extend the work by Gelfand et al. (1990) and Lunn et al. (2000). The data set contains
the weights of 60 rats measured weekly for 5 weeks. The ﬁrst 30 rats are under control
while the rest are under treatment. Our interest is the effect of treatment on the growth
rates and on the growth volatility.
In Sect. 3.5.1, a classical ﬁxed effects model and a random effects model are
considered. In Sects. 3.5.2 and 3.5.3, two Bayesian hierarchical models are used. The
advantages of Bayesian models are the accommodation of parameters uncertainties
and the inherent hierarchical structure. We turn to Stan to do model inference in this
section. In Sect. 3.5.4, we reparameterize the univariate normal hierarchical model
to propose a more efﬁcient Gibbs sampler as we did in Example 3.4.
3.5.1
Classical Regression Models
We ﬁrst ﬁt a ﬁxed effects model, then a random effects model with rat IDs as group
levels. We will see that the random effects model is better at capturing the two levels
of variation: between-rat variation and within-rat variation.
Table 3.4 The rats’ weights measured at the end of each week (Gelfand et al. 1990)
Rat id.
8 days
15 days
22 days
29 days
36 days
1
151
199
246
283
320
· · ·
· · ·
· · ·
· · ·
· · ·
· · ·
60
136
177
223
256
287

3.5 A Bayesian Hierarchical Model for Rats Data
61
10
15
20
25
30
35
100
150
200
250
300
350
Days
Weights
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Control
Treatment
Fig. 3.7 Two regression lines for the control and treatment groups
3.5.1.1
Two Regression Lines
We ﬁt one regression line to each of the control group and the treatment group
respectively. Figure 3.7 roughly shows the negative effect of the treatment on the
weights.
3.5.1.2
A Random Effects Model
As we saw in Fig. 3.7, after considering the effect of treatment, there still remains
variation between different rats, so we may ﬁt the following random effects model:
yi j = α0 + β0x j + α11treat (i) + β1x j1treat (i) + ai + bix j + εi j
ai ∼N

0, σ 2
α

, bi ∼N

0, σ 2
β

, εi j ∼N

0, σ 2
,
where i indicates the ith rat, j indicates the jth week; α0, β0 are the population inter-
cept and slope for the control group; α1, β1 are the incremental population intercept
and the slope for the treatment group; ai, bi are the random intercepts and the slopes
for the ith rats; and x j is the days until the jth week (i.e., x1 = 8, . . . , x5 = 36).
In the random effects model, we effectively separate the residual variation from
the ﬁxed effects model into two parts: the variation in random effects, measured by
σ 2
α, σ 2
β, and the variation in residuals, measured by σ 2. We compare the residuals
from the ﬁxed effects model and the random effects model in Fig. 3.8. In the random
effects model, the variation of residuals is largely reduced, and the residuals for
each rat are closer to a normal distribution. Note that red dots indicate the means of
residuals for each rat.

62
3
Advanced Bayesian Computation
●●●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●●●
●
●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●●
●
●●●
●
●
●●●
●
●●●●●
●●●●●
●●
●
●●
●
●●
●●
●●
●
●●
●
●
●
●●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
−40
0
20
40
60
0
10
20
30
40
50
60
Residuals from fixed effects model
Rat ID
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●●●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−15
−5
0
5
10
15
0
10
20
30
40
50
60
Residuals from random effects model
Rat ID
●
●
●
●
●●
●
●●●
●
●
●
●
●●
●
●●
●
●
●●
●●
●●
●
●●
●
●
●
●
●
●
●●
●●●●
●
●●
●
●
●
●
●●●
●●
●
●●●●
●
Fig. 3.8 Residuals from the ﬁxed effects model and the random effects model
10
15
20
25
30
35
100
150
200
250
300
350
Days
Weights
●
●
Control
Treatment
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Fig. 3.9 Fitted lines in the random effects model
We draw the ﬁtted lines for each rat in Fig. 3.9. In the random effects model, the
ﬁtted values for the ith rat are obtained by adding the population ﬁtted values (based
only on the ﬁxed effects estimates) and the estimated contributions of the random
effects to the ﬁtted values. The resulting values estimate the best linear unbiased
predictions (BLUPs) for the ith rat.

3.5 A Bayesian Hierarchical Model for Rats Data
63
One interest is the effect of treatment on the growth rate, measured by β1. The
summary output shows the signiﬁcant negative effect of treatment on the growth rate.
Another interest is whether a rat with higher birth weight will grow faster. A Pearson
correlation test of intercepts and slopes shows there is no signiﬁcant relationship
between birth weights and growth rates.
3.5.2
A Bayesian Bivariate Normal Hierarchical Model
A Bayesian bivariate normal hierarchical model is used to ﬁt both control and treat-
ment groups as follows:
yi j ∼N

αi + βix j, σ 2
c

, i = 1, . . . , 30
yi j ∼N

αi + βix j, σ 2
t

, i = 31, . . . , 60
αi
βi

∼N
α
β

, Σccc

, i = 1, . . . , 30
αi
βi

∼N
α + Δα
β + Δβ

,ΣΣΣttt

, i = 31, . . . , 60
ΣΣΣccc,ΣΣΣttt ∼Inv-Wishart
200 0
0
0.2
−1
, 2

,
(3.4)
where α, β, Δα, Δβ, σ 2
c , σ 2
t have non-informative priors.
We are interested in the effect of treatment on the growth rate, Δβ, the variation
ratio of treatment group to control group, σt/σc, and the correlation between growth
rates and born weights (for either control group, i.e., ρc, or treatment group, i.e., ρt),
ρc/t = ΣΣΣc/t [1, 2] /
"
ΣΣΣc/t [1, 1]ΣΣΣc/t [2, 2]. The Stan code is as follows:
1
rats _ code 1<-"
2
data {
3
int
<lower =8,
upper =36 >
day [5];
4
real
<lower =0>
weights [60 ,5];
5
}
6
parameters {
7
vector [2]
ab [60];
8
vector [2]
ab_ave;
9
vector [2]
ab_ treat ;
10
real
<lower =0>
sigmaC ;
11
real
<lower =0>
sigmaT ;
12
cov_ matrix [2]
cov_ ave;
13
cov_ matrix [2]
cov_ treat ;
14
}
15
model {
16
for
(j
in
1:30)
ab[j]
~
multi _ normal (ab_ave ,
cov_ave);
17
for
(j
in
31:60)
ab[j]
~
multi _ normal (ab_ave+ab_treat ,
cov_
treat );
18
for
(j
in
1:30)

64
3
Advanced Bayesian Computation
19
for
(t
in
1:5)
weights [j,t]
~
normal (ab[j ,1]+ ab[j ,2]* day[t],
sigmaC );
20
for
(j
in
31:60)
21
for
(t
in
1:5)
weights [j,t]
~
normal (ab[j ,1]+ ab[j ,2]* day[t],
sigmaT );
22
}
23
generated
quantities {
24
real
TC_ sigma ;
25
matrix [2 ,2]
TC_ab;
26
vector [300]
log_lik;
27
vector [300]
dev_res;
28
vector [300]
fitted ;
29
real
rhoC ;
30
real
rhoT ;
31
real
D;
32
TC_ sigma
<-
sigmaT / sigmaC ;
33
TC_ab
<-
cov_ treat
./
cov_ ave;
34
rhoC
<-
cov_ave [1 ,2]/ sqrt ( cov_ave [1 ,1]* cov_ ave [2 ,2]);
35
rhoT
<-
cov_ treat [1 ,2]/ sqrt (cov_ treat [1 ,1]* cov_ treat [2 ,2])
;
36
for
(j
in
1:30) {
37
for
(t
in
1:5)
{
38
log_lik [5*(j -1)+t]
<-
normal _log( weights [j,t],
ab[j ,1]
+
ab
[j ,2]
*
day[t],
sigmaC );
39
dev_res [5*(j -1)+t]
<-
( weights [j,t]
-
ab[j ,1]
-
ab[j ,2]
*
day[t])
/
sigmaC ;
40
fitted [5*(j -1)+t]
<-
ab[j ,1]
+
ab[j ,2]
*
day[t];
41
}
42
}
43
for
(j
in
31:60) {
44
for
(t
in
1:5)
{
45
log_lik [5*(j -1)+t]
<-
normal _log( weights [j,t],
ab[j ,1]
+
ab
[j ,2]
*
day[t],
sigmaT );
46
dev_res [5*(j -1)+t]
<-
( weights [j,t]
-
ab[j ,1]
-
ab[j ,2]
*
day[t])
/
sigmaC ;
47
fitted [5*(j -1)+t]
<-
ab[j ,1]
+
ab[j ,2]
*
day[t];
48
}
49
}
50
D
<-
sum ( -2* log_lik);
51
}
52
"
In Stan, we simulate four chains, each of 400 iterations, and discard the ﬁrst halves.
The MC estimates are shown in Table 3.5. The MC estimated posterior densities of
interested quantities are drawn in Fig. 3.10. According to Table 3.5 and Fig. 3.10,
we make the following conclusion: the effect of treatment on the growth rates is
negative, i.e., the CPDR of Δβ = βt −βc is negative; the treatment group is less
volatile, i.e., the CPDR of σt/σc is less than 1; there is no signiﬁcant relationship
between born weights and growth rates for either group, i.e., the CPDRs of ρc and
ρt contain 0. Finally, Fig. 3.11 validates the assumption of normal error distribution.

3.5 A Bayesian Hierarchical Model for Rats Data
65
βt−βc
Density
−1.8
−1.6
−1.4
−1.2
−1.0
−0.8
0.0
0.5
1.0
1.5
2.0
2.5
●
ρc
Density
−1.0
−0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
●
σt/σc
Density
0.5
0.6
0.7
0.8
0.9
0
1
2
3
4
5
●
ρt
Density
−1.0
−0.5
0.0
0.5
1.0
0.0
0.5
1.0
1.5
2.0
●
Fig. 3.10 The posterior density plots of interested parameters in the Bayesian bivariate model

66
3
Advanced Bayesian Computation
Table 3.5 The MC estimates made by Stan
Parameter
Post. mean
Mean err.
2.5%
Median
97.5%
Eff. size
ˆR
Δβ
−1.33
0.01
−1.64
−1.32
−1.01
800
1.00
σt/σc
0.72
0.00
0.58
0.72
0.89
661
1.00
ρc
−0.17
0.01
−0.59
−0.19
0.32
428
1.01
ρt
0.00
0.01
−0.43
−0.01
0.40
800
1.00
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
100
200
300
−2
−1
0
1
2
Scatter plot of residuals
Fitted posterior mean
Posterior residuals mean 
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−3
−2
−1
0
1
2
3
−2
−1
0
1
2
Normal Q−Q plot
Theoretical Quantiles
Sample Quantiles
Fig. 3.11 The deviance residual plots of the Bayesian bivariate model
3.5.3
A Bayesian Univariate Normal Hierarchical Model
In the previous section, ρ is not signiﬁcantly different from 0. If we can assume that
ρ = 0, the bivariate normal hierarchical model (3.4) can be simpliﬁed to a univariate
normal hierarchical model, as follows:
yi j ∼N

αi + βix j, σ 2
c

, i = 1, . . . , 30
yi j ∼N

αi + βix j, σ 2
t

, i = 31, . . . , 60
αi ∼N

α, σ 2
αc

, i = 1, . . . , 30
βi ∼N

β, σ 2
βc

, i = 1, . . . , 30
αi ∼N

α + Δα, σ 2
αt

, i = 31, . . . , 60
βi ∼N

β + Δβ, σ 2
βt

, i = 31, . . . , 60,
(3.5)

3.5 A Bayesian Hierarchical Model for Rats Data
67
where α, β, Δα, Δβ, σ 2
αc, σ 2
αt, σ 2
βc, σ 2
βt, σ 2
c , σ 2
t are assumed to have non-informative
priors. The Stan code is as follows:
1
rats . code 2<-"
2
data {
3
int
<lower =8,
upper =36 >
day [5];
4
real
<lower =0>
weights [60 ,5];
5
}
6
parameters {
7
real
alpha [60];
8
real
beta [60];
9
real
alpha _ ave;
10
real
alpha _ treat ;
11
real
beta _ ave;
12
real
beta _ treat ;
13
real
<lower =0>
sigmaC ;
14
real
<lower =0>
sigmaT ;
15
real
<lower =0>
sigma _ alphaC ;
16
real
<lower =0>
sigma _ alphaT ;
17
real
<lower =0>
sigma _ betaC ;
18
real
<lower =0>
sigma _ betaT ;
19
}
20
model {
21
for
(j
in
1:30)
alpha [j]
~
normal ( alpha _ave ,
sigma _ alphaC );
22
for
(j
in
31:60)
alpha [j]
~
normal ( alpha _ave+ alpha _treat ,
sigma _ alphaT );
23
for
(j
in
1:30)
beta [j]
~
normal ( beta _ave ,
sigma _ betaC );
24
for
(j
in
31:60)
beta [j]
~
normal ( beta _ave+ beta _treat ,
sigma _ betaT );
25
for
(j
in
1:30)
26
for
(t
in
1:5)
weights [j,t]
~
normal ( alpha [j]+ beta [j]* day[t
], sigmaC );
27
for
(j
in
31:60)
28
for
(t
in
1:5)
weights [j,t]
~
normal ( alpha [j]+ beta [j]* day[t
], sigmaT );
29
}
30
generated
quantities {
31
real
TC_ sigma ;
32
real
TC_ bsigma ;
33
real
TC_ asigma ;
34
vector [300]
log_lik;
35
vector [300]
dev_res;
36
vector [300]
fitted ;
37
real
D;
38
TC_sigma <- sigmaT / sigmaC ;
39
TC_ asigma <- sigma _ alphaT / sigma _ alphaC ;
40
TC_ bsigma <- sigma _ betaT / sigma _ betaC ;
41
for
(j
in
1:30) {
42
for
(t
in
1:5)
{
43
log_lik [5*(j -1)+t]
<-
normal _log( weights [j,t],
alpha [j]
+
beta [j]
*
day[t],
sigmaC );
44
dev_res [5*(j -1)+t]
<-
( weights [j,t]
-
alpha [j]
-
beta [j]
*
day[t])
/
sigmaC ;
45
fitted [5*(j -1)+t]
<-
alpha [j]
+
beta [j]
*
day[t];
46
}
47
}
48
for
(j
in
31:60) {

68
3
Advanced Bayesian Computation
49
for
(t
in
1:5)
{
50
log_lik [5*(j -1)+t]
<-
normal _log( weights [j,t],
alpha [j]
+
beta [j]
*
day[t],
sigmaT );
51
dev_res [5*(j -1)+t]
<-
( weights [j,t]
-
alpha [j]
-
beta [j]
*
day[t])
/
sigmaT ;
52
fitted [5*(j -1)+t]
<-
alpha [j]
+
beta [j]
*
day[t];
53
}
54
}
55
D
<-
sum ( -2* log_lik);
56
}
57
"
We get similar estimates of Δβ and σt/σc as in model (3.4). We display the model
selection criteria in Table 3.6. Both DIC and WAIC agree on the best model (3.5).
The Stan code for information criteria is as follows:
1
rats . stanfit 1<- stan ( model _ code = rats . code 1,
data =c(" weights "," day
") ,iter =1000 , chains =4, seed =20)
#or
model _ code = rats . code 2
2
rats . sim1<- extract ( rats . stanfit 1, permuted
=T)
3
#
loo
and
WAIC
4
loo( extract _log_ lik( rats . stanfit 1," log_ lik "))
5
#
pD
and
DIC
6
Dbar 1<- mean ( rats . sim 1$D)
7
Dhat 1<-0
8
for
(j
in
1:30) {
9
for
(t
in
1:5)
10
Dhat 1<- Dhat 1 -2* dnorm ( weights [j,t],
mean ( rats .sim 1$ alpha [,j])
+
mean ( rats .sim 1$ beta [,j])
*
day[t],
mean ( rats . sim 1$ sigmaC ),
log=T);
11
}
12
for
(j
in
31:60) {
13
for
(t
in
1:5)
14
Dhat 1<- Dhat 1 -2* dnorm ( weights [j,t],
mean ( rats .sim 1$ alpha [,j])
+
mean ( rats .sim 1$ beta [,j])
*
day[t],
mean ( rats . sim 1$ sigmaT ),
log=T);;
15
}
3.5.4
Reparameterization in the Gibbs Sampler
An issue arises when R is used to reproduce the results from the Stan analysis. Table
3.7 shows the posterior mean estimates of scale parameters in model (3.5) using a
Gibbs sampler coded in R, compared with the estimates from Stan. The estimates of
#
σc and #σt using the R Gibbs sampler are unduly large.
Table 3.6 Information criteria of models (3.4) and (3.5)
Model
lppdloo-cv
DIC
pD
WAIC
pWAIC
(3.4)
−988.6
1938.7
107.3
1948.0
91.9
(3.5)
−988.6
1937.2
103.2
1946.2
88.8

3.5 A Bayesian Hierarchical Model for Rats Data
69
Table 3.7 Comparison of the MC estimates of scale parameters via different sampling methods
Estimation method
ˆσc
ˆσαc
ˆσβc
ˆσt
ˆσαt
ˆσβt
Stan
6.2
10.7
0.52
4.3
13.8
0.55
Gibbs sampler
13.2
11.1
0.5
14.2
13.6
0.56
New Gibbs sampler
5.6
12.7
0.46
3.9
14.5
0.52
The effectiveness of the Gibbs sampler crucially depends on the choice of param-
eters to be simulated. Gelman et al. (2014) suggested parameterization in terms
of independent components as an approach to constructing an efﬁcient simulation
algorithm. Following the suggestion, model (3.5) is reparameterized as follows:
yi j ∼N

γi + βi

xi j −xxxi

, σ 2
c

, i = 1, . . . , 30
yi j ∼N

γi + βi

xi j −xxxi

, σ 2
t

, i = 31, . . . , 60
γi ∼N

α + βxxxi, σ 2
αc + σ 2
βcxxx2
i

, i = 1, . . . , 30
βi ∼N

β, σ 2
βc

, i = 1, . . . , 30
γi ∼N

α + Δα + (β + Δβ)xxxi, σ 2
αt + σ 2
βtxxx2
i

, i = 31, . . . , 60
βi ∼N

β + Δβ, σ 2
βt

, i = 31, . . . , 60,
where the prior of γi is derived based on the relationship γi = αi + βi ¯xi.
For i = 1, . . . , 30, the full conditional distributions of γi and βi are
γi|· ∼N
⎛
⎝
5
j=1 yi j

σ 2
αc + σ 2
βcxxx2
i

+ (α + βxxxi) σ 2
c
5

σ 2αc + σ 2
βcxxx2
i

+ σ 2c
,

σ 2
αc + σ 2
βcxxx2
i

σ 2
c
5

σ 2αc + σ 2
βcxxx2
i

+ σ 2c
⎞
⎠
βi|· ∼N
5
j=1 yi j

xi j −xxxi

σ 2
βc + βσ 2
c
5
j=1

xi j −xxxi
2σ 2
βc + σ 2c
,
σ 2
βcσ 2
c
5
j=1

xi j −xxxi
2σ 2
βc + σ 2c

,
where p(γi| ·) does not depend on βi and p(βi|·) does not depend on γi. We use these
full conditional distributions to update γi, βi, and then recover αi as γi −βixxxi. This
new Gibbs sampler gives more accurate posterior mean estimates of scale parameters,
as shown in Table 3.7.
3.6
Bibliographic Notes
Metropolis et al. (1953) were the ﬁrst to describe the Metropolis algorithm. This was
generalized by Hastings (1970). The Gibbs sampler was ﬁrst so-named by Geman

70
3
Advanced Bayesian Computation
and Geman (1984). HMC was introduced by Duane et al. (1987) in the physics
literature and Neal (1994) for statistics problems.
Gelman and Rubin (1992) and Brooks and Gelman (1998) provided a theoretical
justiﬁcation of the convergence checking methods presented in Sect. 3.2.1 and 3.2.2.
For improving the efﬁciency of MCMC, Tanner and Wong (1987) discussed data
augmentation and auxiliary variables. Hills and Smith (1992) and Roberts and Sahu
(1997) discussed different parameterizations for the Gibbs sampler.
Lunn et al. (2012) is the ﬁrst book about the BUGS project. Other references to
BUGS include Lunn et al. (2000) and Spiegelhalter et al. (2003). The references to
Stan include Stan Development Team (2014), Carpenter et al. (2017), Gelman et al.
(2015), Homan and Gelman (2014) and Kucukelbir et al. (2015). Vehtari et al. (2015)
demonstrated the calculation of WAIC and LOO cross-validation in Stan.
The EM algorithm was ﬁrst presented in full generality by Dempster et al. (1977).
Some references on variational Bayes include Jordan et al. (1999), Jaakkola and
Jordan (2000), Blei et al. (2003) and Gershman et al. (2012). Hoffman et al. (2013)
presented a stochastic variational algorithm that is computable for large datasets.
Gilks et al. (1996) is a book full of examples and applications of MCMC methods.
The data and model investigated in Sect. 3.5 are from Gelfand et al. (1990).
For other sampling methods, Neal (2003) discussed slice sampling, and Gilks and
Wild (1992) introduced adaptive rejection sampling.
References
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine
Learning Research, 3, 993–1022.
Brooks, S. P., & Gelman, A. (1998). General methods for monitoring convergence of iterative
simulations. Journal of Computational and Graphical Statistics, 7, 434–455.
Carpenter, B., Gelman, A., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M., et al. (2017). Stan:
A probabilistic programming language. Journal of Statistical Software, 76, 1–32.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical Society B, 39, 1–38.
Duane, S., Kennedy, A. D., Pendleton, B. J., & Roweth, D. (1987). Hybrid Monte Carlo. Physics
Letters B, 195, 216–222.
Gelfand, A. E., Hills, S. E., Racinepoon, A., & Smith, A. F. M. (1990). Illustration of Bayesian-
inference in normal data models using Gibbs sampling. Journal of the American Statistical
Association, 85, 972–985.
Gelman, A., Lee, D., & Guo, J. (2015). Stan: A probabilistic programming language for Bayesian
inference and optimization. Journal of Educational and Behavioral Statistics, 40, 530–543 .
Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences.
Statistical Science, 7, 457–472.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.).
Boca Raton: Chapman & Hall.
Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6, 721–
741.
Gershman, S., Hoffman, M., & Blei, D. (2012). Nonparametric variational inference. In 29th Inter-
national Conference on Machine Learning.

References
71
Gilks, W. R., & Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Journal of the
Royal Statistical Society C, 41, 337–348.
Gilks, W. R., Richardson, S., & Spiegelhalter, D. J. (1996). Monte Carlo Markov chain in practice.
New York: Chapman & Hall.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their applications.
Biometrika, 57, 97–109.
Hills, S. E., & Smith, A. F. M. (1992). Parameterization issues in Bayesian inference. London:
Oxford University Press.
Hoffman, M. D., Blei, D. M., Wang, C., & Paisley, J. (2013). Stochastic variational inference.
Journal of Machine Learning Research, 14, 1303–1347.
Homan, M. D., & Gelman, A. (2014). The no-u-turn sampler: Adaptively setting path lengths in
Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 15, 1593–1623.
Jaakkola, T. S., & Jordan, M. I. (2000). Bayesian parameter estimation via variational methods.
Statistics and Computing, 10, 25–37.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to variational
methods for graphical models. Machine Learning, 37, 183–233.
Kucukelbir, A., Ranganath, R., Gelman, A., & Blei, D. M. (2015). Automatic variational inference
in Stan. arXiv:1506.03431.
Laplace, P. S. (1785). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres. In Memoires de l’Academie Royale des Sciences.
Laplace, P. S. (1810). Memoire sur les approximations des formules qui sont fonctions de tres
grands nombres, et sur leur application aux probabilites. In Memoires de l’Academie des Science
de Paris.
Lunn, D. J., Thomas, A., Best, N., & Spiegelhalter, D. (2000). WinBUGS–A Bayesian modelling
framework: Concepts, structure, and extensibility. Statistics and Computing, 10, 325–337.
Lunn,D.,Jackson,C.,Best,N.,Thomas,A.,&Spiegelhalter,D.(2012).TheBUGSbook:Apractical
introduction to Bayesian analysis. Boca Raton: Chapman & Hall.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., & Teller, E. (1953). Equation
of state calculations by fast computing machines. Journal of Chemical Physics, 21, 1087–1092.
Neal, R. M. (2011). MCMC using Hamiltonian dynamics. In Handbook of Markov chain Monte
Carlo.
Neal, R. M. (1994). An improved acceptance procedure for the hybrid Monte Carlo algorithm.
Journal of Computational Physics, 111, 194–203.
Neal, R. M. (2003). Slice sampling. The Annals of Statistics, 31, 705–741.
Nocedal, J., & Wright, S. (2006). Numerical optimization. New York: Springer Science & Business
Media.
Roberts, G. O., & Sahu, S. K. (1997). Updating schemes, correlation structure, blocking and param-
eterization for the Gibbs sampler. Journal of the Royal Statistical Society B, 59, 291–317.
Spiegelhalter, D., Thomas, A., Best, N., and Lunn, D. (2003). WinBUGS user manual. http://www.
mrc-bsu.cam.ac.uk/wp-content/uploads/manual14.pdf.
Stan Development Team (2014). Stan modeling language: User’s guide and reference manual.
http://mc-stan.org/users/documentation/
Tanner, M. A., & Wong, W. H. (1987). The calculation of posterior distributions by data augmen-
tation. Journal of the American statistical Association, 82, 528–540.
Vehtari, A., Gelman, A., & Gabry, J. (2015). Efﬁcient implementation of leave-one-out cross-
validation and WAIC for evaluating ﬁtted Bayesian models. arXiv:1507.04544.

Chapter 4
Bayesian Chain Ladder Models
Abstract We study the Bayesian chain ladder models and their extensions in this
chapter. In Sect. 4.1, the non-life insurance claims reserving background is reviewed.
Therearetwopartsinthissection.Theﬁrstpartreviewsclaimsreservingterminology.
The second part summarizes widely used traditional reserving methods, including
the chain ladder (CL) method and the Bornhuetter-Ferguson (BF) method. Stochastic
models are discussed in Sects. 4.2 and 4.3. We focus on a Bayesian over-dispersed
Poisson (ODP) model with an exponential decay curve component (Verrall et al.
2012). Reversible jump MCMC is used to simulate a sample from this model. In
Sect. 4.4, we propose a compound model based on the payments per claim incurred
(PPCI) method. A fully Bayesian analysis blending with preliminary classical model
checking is performed on the weekly beneﬁt data set and the doctor beneﬁt data
set from WorkSafe Victoria, a workers compensation scheme in Victoria state of
Australia. We compare our results with the PwC evaluation (Simpson and McCourt
2012).
4.1
Non-life Insurance Claims Reserving Background
Non-life insurance is also known as property and casualty (P&C) insurance in the
United States and general insurance in Australia. There have been much stochastic
claims reserving literature proposed in recent decades. England and Verrall (2002)
is a good summary of stochastic models up to 2002. Wüthrich and Merz (2008,
2015) are very much mathematically driven. The literature using Bayesian methods
include Taylor (2000), England et al. (2012), Verrall and Wüthrich (2012), Zhang
et al. (2012), Meyers (2015), etc. We follow Taylor (2000) to review the claims
reserving terminology and the traditional claims reserving methods.
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_4
73

74
4
Bayesian Chain Ladder Models
4.1.1
Terminology
A non-life insurance policy is a contract between two parties, the insurer and the
insured, providing for the insurer to pay an amount of money to the insured on the
occurrence of speciﬁed events.
A claim is the right of the insured to these amounts and the aggregate of facts
establishing that right and the insurer’s fulﬁllment of it. These facts are also called
trigger events. For a personal automobile policy, the trigger event is usually a car
accident. For a workers compensation policy, the trigger event is usually a work-place
accident. For a homeowners policy, it can be a ﬁre or storm.
The date on which the events generating the claim took place is called date of
occurrence. Most non-life insurance policies are occurrence policies, which limit the
insurer’s liability to the trigger events within the policy period. In contrast, claims-
made policies cover the claims made during the policy period even if these claims
arise from an event that happened before policy inception. Most malpractice insur-
ance policies are belong to this type. Claim amount is the amount which the insurer
is obliged to pay with respect to a claim. It is also called loss amount, claim payment,
loss payment, paid claim, paid loss etc.
4.1.1.1
The Claims Process
Figure 4.1 shows the time line of a claim. The period A to B is the policy effective
period, during which accidents fulﬁlling other policy conditions will be covered. t1
is the date of occurrence. The claim is not notiﬁed to the insurer until t2, when the
policy is already expired.
Typically, the claim will not be paid immediately. At the very least there will be
administrative delays. For more complicated claims, investigation, dispute, litigation
or other processes are needed before determination of any payments. It may be in the
nature of the policy that the payments extend over years, e.g., when the beneﬁts are
income replacement under workers compensation. At time t5, the insurer considered
the action on the claim was complete and closed it. At time t6, the early closure
decision was found to be wrong and claim was reopened, further payments made,
and it was closed again at t8.
Fig. 4.1 Time line of a claim

4.1 Non-life Insurance Claims Reserving Background
75
4.1.1.2
The Components of Unpaid Claims
Unpaid claims as of a particular time are deﬁned as the outstanding loss liability with
regarding to the past exposure period. For the claim in Fig. 4.1, the unpaid claims as
of time B are called the incurred but not reported claim (IBNR), since there is no
notiﬁcation of the claim.
At t2, when the claim is notiﬁed, the unpaid claims consist of case estimates,
future development of case estimates and estimates for reopened claim. Case esti-
mate is established by the claim department or independent adjusters. The sum of
future development of case estimates and estimates of the re-opened claim are called
incurred but not enough reported (IBNER).
Aggregately, at any particular time point, the unpaid claims of an insurer consist
of IBNR, case estimates for reported claims, and IBNER. The case estimates and
IBNER are set up individually according to the characteristics of a particular claim,
while IBNR must be estimated aggregately since it comes from the existing claims
not yet reported to the insurer. Actuaries rely on the historical aggregate claims data
to estimate IBNR, which is also one of the main tasks of this monograph.
4.1.1.3
Loss Reserving
The outstanding loss liability is distinct from loss reserve. The outstanding loss
liability is an unknown random variable which would be recognized after all the
claims are paid. Before all the claims are closed, an unbiased estimate of unpaid
claims liability as of a valuation date is called expected outstanding loss liability.
A reserve set at this level would have a roughly 50% chance of ultimate adequacy.
Often an insurer will wish to reserve more strongly than this and will add a margin to
the expected liability. This margin is also referred to as the prudential margin or pro-
vision for adverse deviation. To quantify the margin, the uncertainty of outstanding
loss liability or, ideally, its predictive distribution needs to be estimated.
4.1.2
Run-Off Triangles
As mentioned before, the estimation of IBNR is impossible for a single claim. So we
need to rely on the aggregate claims history. The claims are usually cross-aggregated
by two factors: period of occurrence and period of development. We treat all the
claims with the same occurrence period as a group and track the group’s development
in the future. This structure is analogous to the rats growth data in Sect. 3.5. The only
difference is that the claims groups have varying development periods at a particular
time.

76
4
Bayesian Chain Ladder Models
Table 4.1 An incremental
claims run-off triangle
Occurrence period
Development period
1
2
. . .
I
1
y1,1
y1,2
. . .
y1,I
2
y2,1
y2,2
. . .
...
...
I
yI,1
4.1.2.1
Notation for a Run-Off Triangle
We denote the occurrence periods (or accident periods) by i = 1, . . . , I, and the
development periods by j = 1, . . . , J. The unit can be a quarter, half or full year,
but the occurrence periods and development periods should use the same units and the
intervals should be equal. The experience periods (or calendar periods) are denoted
by k = i + j, which contains a cross-section of experience from various periods of
occurrence lying on a diagonal line, and the incremental claims of occurrence period
i during the development period j as yi, j.
In the case of I > J, the run-off triangle becomes a trapezoid where the early
occurrence periods i = 1, . . . , J −I are assumed fully run-off by the development
period J. A trapezoid can be converted to a triangle by adding J −I development
periods and assuming yi, j = 0 for J < j ≤I. So we always consider the case when
I = J. Table 4.1 shows a typical structure of incremental claims run-off triangle,
where the upper triangle

yi, j : i + j ≤I + 1

is available by the end of most recent
accident year I (or by the end of most recent experience period I + 1). The loss
reserving problem is to predict the lower triangle {yi, j : i + j > I + 1, j ≤I}, and
tail development {yi, j : j > I} if not fully run-off by the end of development period
I. The ﬁnal reserve is not equal to the summation of predicted lower triangle and
possible tails development but depends on the uncertainty around them.
We deﬁne the cumulative claims for occurrence year i as of development period j
as ci, j =  j
l=1 yi,l, and the ultimate claims of occurrence year i as ci,∞or ui, which
is equal to ci,I when the claims are fully run-off by the development period I. The
unpaid claims of accident year i are deﬁned as Ri = ∞
j=I−i+2 yi, j. In the case of
no development after I, Ri = ci,I −ci,I−i+1. The total unpaid claims are deﬁned as
R = I
i=1 Ri.
4.1.3
Widely-Used Claims Reserving Methods
Here we list two methods: the chain ladder (CL) method and the Bornhuetter-
Ferguson (BF) method. Friedland (2010) discusses other popular methods such as
the Cape Cod method, frequency-severity method, case development method etc.
But the CL and BF methods are the building blocks of all the other methods.

4.1 Non-life Insurance Claims Reserving Background
77
Table 4.2 An age-to-age factors triangle
Occurrence period
Age-to-age factor
1 to 2
2 to 3
. . .
I −1 to I
1
f1,1 = c1,2/c1,1
f1,2 = c1,3/c1,2
. . .
f1,I−1 = c1,I /c1,I−1
2
f2,1 = c2,2/c2,1
f2,2 = c2,3/c2,2
...
...
I −1
fI−1,1 =
cI−1,2/cI−1,1
4.1.3.1
The Chain Ladder Method
The CL method is the most popular and basic technique. The key assumption is that
the future claims development is similar to prior years’ development. An implicit
assumption is that, for an immature accident year, the claims observed so far tell
something about the claims yet to be observed. This is in contrast to the assumption
underlying the BF method. Other important assumptions include a consistent claim
processing and a stable mix of claim types.
The CL method ﬁrst calculates the observed age-to-age factor (also called the
development factor) triangle as in Table 4.2. The CL method requires the judgemen-
tally selected age-to-age factors among the candidates including all-year average,
last three-year average, volume-weighted average etc. We deﬁne the CL estimate of
development factor of j to j + 1 as the volume-weighted average:
ˆf j =
I−j
i=1 ci, j+1
I−j
i=1 ci, j
for j = 1, . . . , I −1.
Assume the tail factor as fI. In the case of no development after I, fI = 1. The
CL estimate of ultimate claim of occurrence period i is
ˆui = ˆci,∞= ci,I+1−i ˆfI+1−i . . . ˆfI.
The expected outstanding liability of occurrence period i is
ˆRi = ci,I+1−i

ˆfI+1−i · · · ˆfI −1

.
4.1.3.2
The Bornhuetter-Ferguson Method
The Bornhuetter-Ferguson (BF) method (Bornhuetter and Ferguson 1972) assumes
that unpaid claims will develop based on a prior ultimate claim estimate. In other
words, the claims reported to date contain no informational value as to the amount

78
4
Bayesian Chain Ladder Models
of claim yet to be reported. The BF method is rather robust against the unreliable
immature claim in the recent accident years.
The BF method applies the same estimate of development pattern as the CL
method, but uses a prior estimate of ultimate claims ˜ui. The BF reserve is ˜Ri =
˜ui

1 −ˆzI+1−i

, where ˆzI+1−i is the estimated percentage of the ultimate claims
amount that is expected to be known by the end of the most recent development period
I + 1 −i for the occurrence period i (i.e. by the end of the most recent experience
period I + 1). The BF method simply uses the CL estimates ˆf j to estimate z as
follows:
ˆz1 =

ˆf1 . . . ˆfI−1 ˆfI
−1
, . . . , ˆzI−1 =

ˆfI−1 ˆfI
−1
, ˆzI = ˆf −1
I
.
4.2
Stochastic Chain Ladder Models
Wüthrich and Merz (2008) commented on the development of claim reserving meth-
ods that:
Reserving actuaries now have to not only estimate reserves for the outstanding loss liabilities
but also to quantify possible shortfalls in these reserves that may lead to potential losses.
Such an analysis requires stochastic modelling of loss liability cash ﬂows and it can only be
done within a stochastic framework.
This section summarizes the recent literature on stochastic claims reserving models.
They can be divided into two categories according to the mean functions: multi-
plicative (cross-classifed) structure using occurrence period and development period
as factor covariates; parametric curve using development period as a continuous
variable.
The ﬁrst type of models can give the CL estimates when using over-dispersed
error structure but they cannot accommodate the tail development. The second type
of models have far fewer parameters and can accommodate the tail development. We
will turn to the bootstrap or the MCMC methods to get the predictive distribution
of unpaid claims. RJMCMC is discussed in this section as a way of combining the
MCMC methods with the model selection.
4.2.1
Frequentist Chain Ladder Models
The distribution-free model by Mack (1993) and the over-dispersed Poisson (ODP)
model by Renshaw and Verrall (1998) use the same mean function to ﬁt the incre-
mental claims. The mean function is the multiplication of two parameters, which cor-
respond to the occurrence periods and the development periods respectively. Besides
having the same response variable and mean function, they both assume the variance

4.2 Stochastic Chain Ladder Models
79
of the response variable is proportional to its mean. It is not surprising that both of
them give the CL estimates.
The distribution-free model does not assume a distribution family and relies on
the unbiased estimators, while the ODP model assumes a Poisson distribution and
relies on the MLE. They have different prediction errors and predictive distributions
which can be estimated via the bootstrap.
4.2.1.1
The Distribution-Free Model
Mack(1993)proposedadistribution-freemodelassumingonlytheﬁrsttwomoments,
as follows:
E

ci, j|ci, j−1

= f j−1ci, j−1, i = 1, . . . , I, j = 2, . . . , I
Var

ci, j|ci, j−1

= σ2
j−1ci, j−1, i = 1, . . . , I, j = 2, . . . , I.
(4.1)
It can be shown that the CL estimators ˆf j are the unbiased estimator of f j. Using
the CL estimators ˆf j, the unpaid claims estimate is the same as the CL estimate.
Furthermore, an unbiased estimator for σ2
j is
ˆσ2
j =
1
I −j −1
I−j
	
i=1
ci, j

ci, j+1
ci, j
−ˆf j
2
,
j = 1, . . . , I −2
ˆσ2
I−1 = min

ˆσ2
I−2/ˆσ2
I−3, min

ˆσ2
I−3, ˆσ2
I−2

.
The conditional mean squared error of prediction (MSEP) for ˆRi is
MSEPc

ˆRi
yyy

= E


Ri −ˆRi
2yyy

= Var (Ri) +

E (Ri) −ˆRi
2
,
where yyy =

yi j : i = 1, . . . I, j = 1, . . . , I −i + 1

is the upper triangle. In words,
the conditional prediction variance is equal to the sum of process variance and esti-
mation bias squared. Note that E (Ri) ̸= ˆR; see Mack (1993). The analytical results
of conditional MSEP of individual occurrence period reserve and total reserve are
available. As a ﬁnal remark, Mack (1999) extends this model to involve the tail factor.
4.2.1.2
The Over-Dispersed Poisson (ODP) Model
One of the most popular generalized linear models in the claims reserving problem
is the ODP model which has the following form:
yi, j
ϕ ∼Poisson

μiγ j
ϕ

, i = 1, . . . , I, j = 1, . . . , I,
(4.2)

80
4
Bayesian Chain Ladder Models
with the constraint J
j=1 γ j = 1. Here μi is interpreted as the expected ultimate
claims of occurrence period i and γ j as the expected proportion of incremental
claims to the ultimate claims during development period j. This model has been
intensively studied, including by Renshaw and Verrall (1998), Verrall (2000, 2004),
England and Verrall (2002, 2006), England et al. (2012), Verrall et al. (2012) and
Wüthrich (2013b).
An implicit assumption of this model is that the variance of the response variable
is proportional to its mean. We can check this assumption by inspecting the residual
plots. When it fails, other error structures such as a Tweedie distribution can be used.
It can be shown that the MLEs for μi and γ j are equal to the CL estimates using
the weighted averages of age-to-age factors. The ODP model can be extended to
non-integer, and negative data (i.e., when recoveries are possible) via the quasi-
likelihood method (Faraway 2015). The quasi-likelihood method is easily applied in
R by specifying the argument family as quasi in the function glm().
We deﬁne the unscaled Pearson residuals as
ri, j = yi, j −ˆmi, j

ˆmi, j
,
where ˆmi, j is the MLE for E

yi, j

(i.e., the ﬁtted value). The dispersion parameter
ϕ is estimated by
ˆϕ =

i+ j≤I+1 ri, j 2
N −p
,
where N = (I + 1)I/2 is the number of observations, and p = 2I −1 is the number
of parameters. Fortunately, R can calculate all of these estimates in a second. England
andVerrall(2006)alsoconsiderthenon-constantdispersionfordevelopmentperiods,
which is the assumption of the distribution-free model (4.1).
The mean squared error of prediction for ˆRi is
MSEP( ˆRi) = E

Ri −ˆRi
2
= Var

Ri −ˆRi

+

E (Ri) −E( ˆRi)
2
.
The second term is approximately zero. Hence,
MSEP( ˆRi) ≈Var

Ri −ˆRi

= Var (Ri) + Var( ˆRi).
(4.3)
In words, the prediction variance is roughly equal to the sum of process variance
and estimation variance. R cannot provide the MSEP( ˆRi) directly since it is a com-
plicated function of parameters. From Renshaw and Verrall (1998), MSEP( ˆRi) is
estimated as
I
	
j=I−i+2
ˆϕ ˆmi, j +
I
	
j=I−i+2
ˆm2
i, jVar

ˆηi, j

+ 2
	
k>l
ˆmi,k ˆmi,lCov

ˆηi,k, ˆηi,l

,

4.2 Stochastic Chain Ladder Models
81
where η is the linear predictor and its covariance matrix is available directly from R.
England and Verrall (2002) also give the MSEP of total reserve with an additional
covariance term for different occurrence periods (i.e., Cov ( ˆηm,k, ˆηn,l)). We can rely
on ChainLadder package to get MSEP( ˆRi). Later, we will use the bootstrap or
MCMC to simulate Ri and estimate its MSEP based on the simulated sample.
4.2.1.3
The Predictive Distribution via the Bootstrap
Bootstrapping (Efron and Tibshirani 1994) is a powerful, yet simple, technique for
obtaining information from a single sample of data. In a standard application of
the bootstrap, where data are assumed to be independent and identically distributed,
resampling with replacement takes place of the data themselves.
In regression problems the data are usually assumed to be independent but not
identically distributed due to the existence of covariates. Therefore, with regression
problems it is common to bootstrap residuals, rather than data themselves, since the
residuals are approximately independent and identically distributed. For model (4.1)
and model (4.2), we use the scaled Pearson residuals for bootstrapping.
The bootstrap for model (4.1).
Model (4.1) is in a recursive structure. England and Verrall (2002) showed that
an equivalent model can be obtained using the observed factors fi, j as a response
variable with the following mean and variance:
E

fi, j|ci, j

= f j
Var

fi, j|ci, j

=
σ2
j
ci, j
.
The scaled Pearson residuals are deﬁned as
rs
i, j = fi, j −ˆf j
ˆσ j/√ci, j
.
The bootstrap algorithm for model (4.1) is as follows:
1. Sample with replacement, from the set of scaled Pearson residuals, to get a sample
of residuals for a single bootstrap iteration

r B
i, j : i + j ≤I

.
2. Back out the residual deﬁnition to obtain a pseudo run-off triangle of development
factor as follows:
f B
i, j =
r B
i, j ˆσ j
√ci, j
+ ˆf j.
3. Obtain the new volume-weighted development factor

82
4
Bayesian Chain Ladder Models
˜f j =
I−j
i=1 f B
i, j ci, j
I−j
i=1 ci, j
.
4. Simulate the future claims. Starting from the latest cumulative claims ci,I+1−i,
forecast the next cumulative claims by sampling a value from a gamma distribu-
tion:
˜ci,I+2−i|ci,I+1−i ∼Gamma
 ˜f 2
I+1−i ci,I+1−i
ˆσ2
I+1−i
,
˜fI+1−i
ˆσ2
I+1−i

for i = 2, . . . , I.
5. Recursively predict the future cumulative claims by sampling from
˜ci, j+1|˜ci, j ∼Gamma
 ˜f 2
j ˜ci, j
ˆσ2
j
,
˜f j
ˆσ2
j

for i = 3, . . . , I and j = I −i + 3, . . . , I.
6. Calculate each accident year future claims and total future claims as
˜Ri = ˜ci,I −ci,I+1−i, for i = 2, . . . , I
˜R = ˜R2 + ˜R3 + · · · + ˜RI.
7. Repeat steps 1–6 to get a sample of ˜Ri and ˜R.
The empirical distribution of the bootstrap sample approximates the predictive dis-
tribution. The prediction variance of total liability can be estimated by the sample
variance of the bootstrap sample of total liability. Note that the bootstrap sample
variation consists of variation due to bootstrapping in step 1 (i.e., estimation vari-
ance) and variation due to forecasting in step 4 and 5 (i.e., process variance), which
correspond to the two terms on the right side of Eq. (4.3).
The bootstrap for model (4.2).
The scaled Pearson residuals of model (4.2) are
rs
i, j = yi, j −ˆmi, j

ˆϕ ˆmi, j
.
The bootstrap algorithm for model (4.2) is as follows:
1. Sample with replacement from the set of scaled Pearson residuals to get a sample
of residuals for a single bootstrap iteration

r B
i, j : i + j ≤I

.
2. Back out the residual deﬁnition to obtain a pseudo run-off triangle of incremental
claims as follows:
yB
i, j = r B
i, j

ˆϕ ˆmi, j + ˆmi, j.

4.2 Stochastic Chain Ladder Models
83
Table 4.3 The total outstanding liability estimates from models (4.1) and (4.2)
Model
Estimate
No tail factor
With tail factor
(4.1)
ˆR
1,463,076
1,599,558

MSEPc( ˆR)
55,300
58,528
(4.2)
ˆR
1,463,076 (1,471,906)
NA

MSEP( ˆR)
60,444 (60,087)
NA
3. Use the CL method to get the new estimate ˜μi, ˜γ j based on the pseudo incremental
claims run-off triangle from step 2.
4. Simulate the future claims from the following ODP model:
˜Ri ∼ˆϕPoisson

˜μi
I
j=I−i+2 ˜γ j
ˆϕ

for i = 2, 3, . . . , I.
Calculate the total future claims as ˜R = ˜R2 + ˜R3 + · · · + ˜RI.
5. Repeat steps 1–4 to get a sample of ˜Ri and ˜R.
In the case when ˆϕ is large, e.g., ˆϕ = 1000, ˜Ri will be sampled from {0, 1000, . . .},
which is undesirable. We can use an alternative gamma distribution with the target
mean and variance in step 4.
Example 4.1 (Liability insurance claims data) We use the liability claims run-off
data with 22 accident years and 22 development years from Verrall and Wüthrich
(2012). The R package ChainLadder by Gesmann et al. (2015) can estimate all the
quantities we have previously mentioned.The residual plots are needed to validate
the model assumptions.
Table 4.3 shows that models (4.1) and (4.2) both give the same point estimate of
total liability, which are also equal to the CL estimate. The numbers in parentheses are
from the bootstrap method. The distribution-free model (4.1) can accommodate tail
development, which consists of nearly 10% of total liability. The conditional mean
squared error is smaller than the unconditional mean squared error since the latter
involves the extra uncertainty induced by the historical claims data (i.e., estimation
error).
The function BootChainLadder in the R package ChainLadder performs the
bootstrap for model (4.2). Here we bootstrap 1, 000 times. We show the histogram
of the bootstrap sample of total outstanding liability in Fig. 4.2, and we get the
bootstrap estimate of total outstanding liability and the standard error in Table 4.3
(stated in parentheses).

84
4
Bayesian Chain Ladder Models
Total outstanding liability (in millions)
Frequency
1.3
1.4
1.5
1.6
1.7
0
1
2
3
4
5
6
●
●
●
●
CL estimate
Bootstrap mean
Bootstrap 90% CI
Fig. 4.2 The histogram of the total outstanding claims liability via the bootstrap
4.2.2
A Bayesian Over-Dispersed Poisson (ODP) Model
The model (4.2) in a Bayesian framework has the following form:
yi, j
ϕ ∼Poisson

μiγ j
ϕ

μi ∼Gamma (ai, bi)
γ j ∼Gamma

c j, d j

,
(4.4)
where μi is related to the ultimate claim of accident year i, γ j is related to the
incremental claims percentage during development year j, and ai, bi, c j, d j are
constant hyperparameters whose values are adjusted according to prior knowledge.
In the case where there is no prior knowledge, we assume μi and γ j follow the
same non-informative prior. ϕ is a plug-in estimate via GLM (see Sect. 4.2.1.2). We
can assume a prior for ϕ, see Example 3.17 of Wüthrich and Merz (2015). Note
that the uncertainty around ϕ doesn’t have a signiﬁcant inﬂuence on the predictive
uncertainty of unpaid claims. Hence, it is reasonable to assume a constant ϕ as we
did here.
The joint posterior distribution of μ = (μ1, . . . , μI) and γ = (γ1, . . . , γI) is
p (μ, γ|yyy) =
p (yyy|μ, γ) p (μ, γ)

μ,γ p (yyy|μ, γ) p (μ, γ) dμdγ ∝p (yyy|μ, γ) p (μ, γ)

4.2 Stochastic Chain Ladder Models
85
∝

i+ j≤I+1
exp

−μiγ j
ϕ
 
μiγ j
ϕ
 yi, j
ϕ
I
i=1
μi ai−1 exp (−biμi)
I
j=1
γ j c j−1 exp

−d jγ j

.
Our interest is in not only the parameter μ, γ but also the future claims. We have the
following posterior predictive distribution of future claims:
p(y′|yyy) =

μ,γ
p

y′|μ, γ

p (μ, γ|yyy) dμdγ,
where y′ is the set of lower triangle. It is hard to solve p

y′|yyy

analytically. The
conditional mean squared error of prediction for a predictor ˆR is
MSEPc( ˆR) = E


ˆR −R
2yyy

= Var (R|yyy) +

ˆR −E (R|yyy)
2
.
We prefer the predictor ˆR = E (R|yyy) (i.e., the posterior mean) which minimizes
MSEPc( ˆR). The MSEP of the posterior mean is Var (R|yyy), which can be estimated
from a MC sample.
4.2.2.1
A Gibbs Sampler for Model (4.4)
The Gibbs sampler is a special case of the Metropolis-Hastings (M-H) algorithm.
In the M-H algorithm if we choose the full conditional distribution as the proposed
distribution, the acceptance rate will be 1. The use of the Gibbs sampler implicitly
requires that the full conditional distribution is recognisable; otherwise, we need to
turn to the general M-H algorithm or adaptive rejection sampling (Gilks and Wild
1992).
The full conditional distribution of μi is obtained from p (μ, γ|yyy), assuming all
the other parameters constant, as follows:
p (μi|yyy, γ, μ−i) ∝exp

−
μi
I+1−i
j=1
γ j
ϕ

μi
I+1−i
j=1
yi, j
ϕ
μi
ai−1 exp (−biμi) ,
where μ−i is the vector μ excluding μi. It can be recognized as a gamma distribution
μi|yyy, γ ∼Gamma

ai +
I+1−i
j=1
yi, j
ϕ
, bi +
I+1−i
j=1
γ j
ϕ

.
(4.5)
Symmetrically, the full conditional distribution of γ j for j = 1, . . . , I is
γ j|yyy, μ ∼Gamma

c j +
I+1−j
i=1
yi, j
ϕ
, d j +
I+1−j
i=1
μi
ϕ

.
(4.6)

86
4
Bayesian Chain Ladder Models
A Gibbs sampler based on the above full conditional distributions has the following
steps:
1. Initialize μ0, γ0. For t ≥1, repeat the steps 2–4.
2. For 1 ≤i ≤I, draw a value μt
i from distribution (4.5) with γ = γt−1, and set
μt =

μt
1, . . . , μt
I

.
3. For 1 ≤j ≤I, draw a value γt
j from distribution (4.6) with μ = μt, and set
γt =

γt
1, . . . , γt
I

.
4. For 1 ≤i ≤I, draw a value Rt
i from the distribution
ϕPoisson

μt
i
I
j=I−i+1 γt
j
ϕ

,
and set Rt = Rt
2 + · · · + Rt
I.
Steps 2 and 3 provide a Markov chain

μt, γt
t≥0 whose stationary distribution is
p (μ, γ|yyy). Step 4 provides a sample of the total outstanding liability. The prediction
error of future claims consists of estimation error via steps 2 and 3 and process error
via step 4, which correspond to the bootstrap resampling step and forecasting step
respectively.
Note that parameters μ and γ are not uniquely deﬁned. In Example 4.2, we will
see that the multiplication μiγ j is converged rather than μi,γ j by themselves. In
other words, μi, γ j cannot be estimated accurately individually. For interpretation
purposes, we deﬁne the normalized μi, γ j as
μ*
i = μi
I
	
j=1
γ j, γ*
j =
γ j
I
k=1 γk
.
Inferences under non-informative priors
Under the non-informative priors, i.e., a →0, b →0, c →0, d →0, distributions
(4.5) and (4.6) deﬁne the following conditional expectations:
E (μi|yyy, γ) =
I+1−i
j=1
yi, j
I+1−i
j=1
γ j
, E

γ j|yyy, μ

=
I+1−j
i=1
yi, j
I+1−j
i=1
μi
.
If we substitute the left sides with μi and γ j, the above equations deﬁne a system of
equations whose solutions will be consistent with the CL estimates. Strictly, the pos-
terior mean of outstanding liability is close but not exactly equal to the CL estimate.

4.2 Stochastic Chain Ladder Models
87
In Example 4.1, we use the plug-in estimate ˆϕ = 631.8, and non-informative prior
for μ, γ, (i.e., a, b, c, d →0). We iterate for T = 1000 times and get the MC esti-
mate of posterior mean of total outstanding liability as 1,461,958 dollars, with the
standard error of 60,902 dollars. These values are quite close to the result in Table 4.3.
Inferences under strong priors for μ
Assume the prior knowledge of μ is some value around m with small variation,
i.e., b/mi →∞and ai = mib. Distributions (4.5) and (4.6) deﬁne the following
conditional expectations:
E (μi|yyy, γ) ≈mi, E

γ j|yyy, μ

≈
I+1−j
i=1
yi, j
I+1−j
i=1
mi
,
which follows the BF predictor proposed by Mack (2008). The estimation error of
μ is close to 0, and the standard error of claims liability will be largely reduced.
Example 4.2 (A Monte Carlo study of model (4.4) using simulated data) We assume
the parameters in model (4.4) as μ =

107, 1.02 × 107, . . . , 1.029 × 107
, γ =
(0.30, 0.21, 0.15, 0.10, 0.08, 0.06, 0.04, 0.03, 0.02, 0.01) , ϕ = 25000,where
the sum of γ is 1 implying no claims development beyond age 10. We simulate a
sample of incremental claims in the upper triangle.
Inferences under non-informative priors
We use the plug-in estimate ˆϕ = 23,488, and choose a = 0, b = 0. We iterate
for T = 1000 times. The trace plots in Fig. 4.3 show that μ*
6, γ*
6 converge rather
than μ6, γ6. The MC estimates of posterior means of μ*, γ* are close to the CL esti-
mates as shown in Fig. 4.4. The predictive distributions of outstanding liability are
shown in Fig. 4.5. We check whether the 95% CPDRs have 95% chances to cover
the true parameters if we replicate the above process (i.e., simulate the data then
estimate the 95% CPDR) for 100 times. Table 4.4 conﬁrms our expectation except
for the last accident year and the last development period, due to the sparse data for
these two periods.
Table 4.4 The proportions of the 95% CPDRs containing the true values
μ1
μ2
μ3
μ4
μ5
μ6
μ7
μ8
μ9
μ10
0.93
0.91
0.95
0.92
0.93
0.89
0.91
0.96
0.95
0.89
γ1
γ2
γ3
γ4
γ5
γ6
γ7
γ8
γ9
γ10
0.94
0.94
0.94
0.94
0.95
0.92
0.97
0.92
0.96
0.78
R
R2
R3
R4
R5
R6
R7
R8
R9
R10
0.94
0.90
0.94
0.98
0.95
0.98
0.97
0.97
0.95
0.88

88
4
Bayesian Chain Ladder Models
0
200
400
600
800
1000
4.0e+06
8.0e+06
1.2e+07
Iterations
μ6
0
200
400
600
800
1000
8500000
10000000
11500000
Iterations
Normalized μ6
0
200
400
600
800
1000
0.04
0.08
0.12
0.16
Iterations
γ6
0
200
400
600
800
1000
0.045
0.055
0.065
0.075
Iterations
Normalized γ6
Fig. 4.3 The trace plots of the ﬁrst 10,000 iterations

4.2 Stochastic Chain Ladder Models
89
●●
●
●●●●●●
●
●
●
●
●●●●
●
●
●
●
●●
●
●
●
●●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●●●●●
●
●
●
●
●
●●●
●
●
●
●
●
●
1
2
3
4
5
6
7
8
9
10
8
10
12
14
Accident Year
Ultimate Claims (in millions)
●
●
●
●
●
●
●
●
●
●
●
True ultimate
CL estimates
Posterior means
95% CPDR
●●●
●
●
●
●●●●
●
●●
●
●●
●
●
●●
●
●
●●
●●●
●
●
●
●
●●●●●●●●●
●●●●●●
●●●●●
●
●●●
●
●
●
●
●●●●●●●●●●
●●●●●●●●●●●●●
●●●
●●●●
●●
1
2
3
4
5
6
7
8
9
10
0.00
0.05
0.10
0.15
0.20
0.25
0.30
Development Year
Incremental Percentage
●
●
●
●
●
●
●
●
●
●
●
True incremental percentage
CL estimates
Posterior means
95% CPDR
Fig. 4.4 The MC estimates of the ultimate claims μ∗and the incremental claims percentages γ∗
Inferences under strong priors for μ
We choose the following strong priors for μ: a = 1012, m =

107, 1.02 × 107, . . .

,
b = 0.Weiteratefor T = 1,000 timesandgettheMCestimatesasinTable4.5.Aswe
expected, the variations of outstanding liability under strong priors are substantially
smaller than those under a non-informative prior.

90
4
Bayesian Chain Ladder Models
●●
●●●
●●●
●●●●●●●●●●●●●●●
●●●●●●
●●●●●●●●●●
●●●●●
●●
●
●●●●●
●●
●
●●●●
●●●
●
●●●●
●●●●●●
●
●
●●
●
●●●●
●
●
●
●●●●
●●
●
●●
●
●
●
●●
●
●
●
●
●
●●
●
●●
●●●●
1
2
3
4
5
6
7
8
9
10
0
2
4
6
8
10
Accident Year
Unpaid claims (in millions)
●
●
●
●
●
●
●
●
●
●
●
True liability
CL estimates
Posterior means
95% CPDR
Total outstanding liability in millions
Density
20
22
24
26
28
30
0.00
0.05
0.10
0.15
0.20
0.25
●
●
●
●
●
●
True total outstanding liability
CL estimate
Posterior Mean
95% CPDR
Fig. 4.5 The predictive distributions of outstanding claims liability for each accident year and the
predictive distribution of the total outstanding claims liability
4.3
A Bayesian ODP Model with Tail Factor
In this section we will focus on the following model:

4.3 A Bayesian ODP Model with Tail Factor
91
yi, j
ϕ ∼Poisson

μiγ j
ϕ

, i = 1, .., I, j = 1, . . . , I
μi ∼Gamma (ai, bi)
γ j ∼Gamma

c j, d j

, j = 1, . . . , k −1
γ j = exp (α −jβ) , j = k, . . . , I
α ∼N

e, σ2
1

β ∼N

f, σ2
2

Pr (k = i) =
1
I −1, i = 2, . . . , I,
(4.7)
where a, b, c, d, e, f, σ2
1, σ2
2 are the speciﬁed hyperparameters and ϕ is a plug-in
estimate. This is the same Bayesian ODP model as model (4.4) but extended to
include a suitable tail factor.
To illustrate this model, we specify ai = 100, bi = ai/mi, c j = 1, d j = c j/h j,
e = 0, f = 0, σ2
1 = 100, σ2
2 = 100, where mi and h j are the CL ultimate claims
estimates and the CL incremental claims proportion estimates. The choice of these
hyperparameters ensures the convergence of the RJMCMC algorithm while allowing
sufﬁcient ﬂexibility. Denote θk = {α, β, μ, γ1, . . . , γk−1}. This model reduces the
number of parameters from 2I in model (4.4) to k + 2. Note that k is usually much
smaller than I.
Model (4.7) implicitly includes a tail factor
γJ =
J
	
j=I+1
exp (α −jβ) ,
where J is chosen judgementally. J →∞leads
γ∞= exp (α −(I + 1) β)
1 −exp (−β)
.
The main task of this section is to determine which k leads to the optimal model
ﬁt. Since different ks will lead to different parameter dimensions, this problem is
Table 4.5 The outstanding liability estimates under different priors
Estimate
Strong prior case
Non-informative prior case
Post. mean
Sd. error
CV (%)
Post. mean
Sd. error
CV (%)
R
24,244,540
1,006,232
4.2
23,867,671
1,524,567
6.4
R10
8,340,955
453,191
5.4
8,206,132
857,982
10.5
R9
5,706,840
383,309
6.7
5,189,284
528,886
10.2
R8
3,862,495
321,367
8.3
4,040,881
422,256
10.5

92
4
Bayesian Chain Ladder Models
equivalent to model selection. Here we investigate two methods: deviance infor-
mation criteria (DIC) (Spiegelhalter et al. 2002) and reversible jump Markov chain
Monte Carlo (RJMCMC) method (Green 1995). There are other methods to compare
and evaluate Bayesian models such as BIC, cross-validation and posterior predictive
checking (see Sect. 2.2).
4.3.1
Reversible Jump Markov Chain Monte Carlo
RJMCMC generalizes the Metropolis-Hastings (M-H) algorithm to include a model
indicator. The joint state space (θl,l) is deﬁned by both model parameters θl and the
model index l, where l ∈{1, 2, . . . , L}. The joint posterior distribution of θl,l can
be factorized as
p (l, θl|yyy) ∝p (θl|yyy) p (l|yyy) ∝p (yyy|θl,l) p (θl) p (l) ,
which is the product of the likelihood, the prior of θl and the prior of l.
Before turning to the RJMCMC algorithm, we review the M-H algorithm. In the
M-H algorithm, a proposal distribution from θ to θ∗is q(θ∗|θ), and the acceptance
rate is
min

1, p

θ*|yyy

q(θ|θ*)
p (θ|yyy) q(θ*|θ)

.
For RJMCMC, we need a model index proposal distribution from l to l∗, q(l∗|l), and
a parameter proposal distribution from θl to θl∗. Since θl and θl∗may have different
dimensions, the parameter proposal process involves two steps: generate u ∼ql→l*,
and then set

θl*, u*
:= Tl→l* (θl, u) , where Tl→l* is a one-to-one mapping with
Tl→l* = T −1
l*→l.
Note that (θl, u) must have the same dimension as

θl*, u*
. It is possible that
u is zero-dimensional, e.g., θl has more parameters than θl*. Similar to the M-H
algorithm, the acceptance rate is calculated as
min

1, p

l*, θl*|yyy

p (l, θl|yyy)
q(l|l*)
q(l*|l)
ql*→l

u*
ql→l* (u)

∂Tl→l* (θl, u)
∂(θl, u)


,
where the ﬁnal term is the determinant of the Jacobian matrix.
The RJMCMC algorithm typically has the following steps:
1. Initialize l0 and θ0
l0. In the following we use the shortened notation θt
lt for θt. For
t ≥1, repeat the following steps.
2. Propose a new model index l∗from the distribution q

l∗|lt
.
3. If l∗= lt, do the following within-model update. Otherwise, jump to step 4.
a. Update the current model lt by one iteration (i.e., via normal MCMC).

4.3 A Bayesian ODP Model with Tail Factor
93
b. Set lt+1 = l∗and θt+1 as the updated parameters.
c. Go to step 2.
4. If l* ̸= lt, do the following between-model update.
a. Generate ut ∼qlt→l*.
b. Set

θ*, u*
:= Tlt→l* 
θt, ut
.
c. Compute the acceptance rate as
min

1, p

l*, θ*|yyy

p (lt, θt|yyy)
q

lt|l*
q

l*|lt ql*→lt 
u*
qlt→l* (ut)

∂Tlt→l* 
θt, ut
∂(θt, ut)


.
d. With this acceptance rate, set lt+1 = l∗and θt+1 = θ∗. Otherwise keep
lt+1 = lt and θt+1 = θt.
e. Go to step 2.
The RJMCMC algorithm provides a Markov chain

lt, θt
t≥0 whose stationary distri-
bution is p(l, θl|yyy). We can either choose the model l0 which has the highest posterior
probability p(l|yyy), or perform model averaging over p(l, θl|yyy).
4.3.2
RJMCMC for Model (4.7)
In model (4.7), k is a model index variable whose value determines the param-
eter dimension. The joint posterior of k and θk is simpliﬁed as p (k, θk|yyy) ∝
p (yyy|θk) p (θk) . We use the following model index proposal distributions:
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
q

k* = k|k

= q

k* = k + 1|k

= q

k* = k −1|k

= 1
3 for k = 3, 4, . . . , I −1
q

k* = k|k

= 2
3 and q

k* = k + 1|k

= 1
3 for k = 2
q

k* = k|k

= 2
3 and q

k* = k −1|k

= 1
3 for k = I
(4.8)
which implies that k can equally jump to the nearest neighbourhood or stay in the
current state. The RJMCMC algorithm for model (4.7) consists of a within-model
update and a between-model update.
4.3.2.1
Within-Model Update
Suppose at the t + 1th iteration we propose k∗= kt from (4.8). The parameters at the
end of tth iteration are denoted by θt =

αt, βt, μt, γt
1, . . . , γt
kt−1

. The following
steps update θt to θt+1:
1. For μt+1, γt+1
1
, . . . , γt+1
kt−1, we apply the Gibbs sampler algorithm from Sect. 4.2.2.
2. For αt+1, βt+1, we apply the following M-H algorithm:

94
4
Bayesian Chain Ladder Models
a. Propose α* ∼N

αt, 0.022
, β* ∼N

βt, 0.022
.
b. Set θ* =

α*, β*, μt+1, γt+1
1
, . . . , γt+1
kt−1

.
c. Calculate the acceptance as
min

1, p

yyy|θ*
N

αt|α*, 0.022
N

βt|β*, 0.022
p (yyy|θt) N

α*|αt, 0.022
N

β*|βt, 0.022

,
where N (x|a, b) is the normal density at x with mean a and variance b.
d. Withthis acceptancerate, set αt+1 = α*, βt+1 = β*. Otherwisekeepαt+1 =
αt, βt+1 = βt.
3. Set kt+1 = k*, θt+1 =

αt+1, βt+1, μt+1, γt+1
1
, . . . , γt+1
kt−1

. Note that the within-
model acceptance rate of k∗is always 1.
4.3.2.2
Between-Model Update
Between-model update case 1:
Suppose at the t + 1th iteration, we propose k∗= kt + 1 from (4.8). The parameters
at the end of the tth iteration are denoted by θt =

αt, βt, μt, γt
1, . . . , γt
kt−1

. The
following steps update θt to θt+1:
1. Propose a value ut from a gamma distribution with shape of 100 and mean of
exp

αt −ktβt
, as follows:
ut ∼qkt→k* = Gamma

100,
100
exp (αt −ktβt)

.
2. Set

θ*, u*
:= Tkt→k* 
θt, ut
=

θt, ut
, where u* has zero-dimension. Tkt→k*
is an identity mapping matrix with the Jacobian of 1.
3. Calculate the acceptance rate as
min
⎛
⎝1,
p

yyy|θ*
p

θ*
p (yyy|θt) p(θt)Gamma

ut
100,
100
exp(αt−ktβt)

⎞
⎠.
4. With this acceptance rate, set

kt+1, θt+1
=

k*, θ*
. Otherwise keep

kt+1, θt+1
=

kt, θt
.
Between-model update case 2:
Suppose at the t + 1th iteration, we propose k∗= kt −1 from (4.8). The parameters
at the end of the tth iteration are denoted by θt =

αt, βt, μt, γt
1, . . . , γt
kt−1

. The
following steps update θt to θt+1:

4.3 A Bayesian ODP Model with Tail Factor
95
●
●
●
●
●
●
●
●
●
2
4
6
8
10
9
10
11
12
13
14
15
16
k
pD
●
●
●
●
●
●
●
●
●
−15345
−15340
−15335
DIC
●
●
pD
DIC
Fig. 4.6 DIC’s and pD’s for the simulated data with respect to k
1. Set

θ*, u*
:= Tkt→k* 
θt, ut
=

θt, ut
,
where
ut
has
zero-dimension,
u∗= γt
kt−1. Tkt→k∗is an identity mapping matrix with the Jacobian of 1.
2. Calculate the acceptance rate as
min
⎛
⎝1,
p

yyy|θ*
p

θ*
Gamma

u* 100,
100
exp(αt−(kt−1)βt)

p (yyy|θt) p(θt)
⎞
⎠.
3. With this acceptance rate, set

kt+1, θt+1
=

k*, θ*
. Otherwise keep

kt+1, θt+1
=

kt, θt
.
Example 4.3 (A Monte Carlo study of model (4.7)) We specify the true parameters
as follows:
I = 10, k = 5, α = −1.4, β = 0.2, ϕ = 25,000
μ =

107, 1.02 × 107, . . . , 1.029 × 107
γ = (0.159, 0.179, 0.179, 0.139) ,
and simulate a sample from model (4.7).
DIC method:
We want to determine which k leads to the optimal model ﬁt. Applying MCMC to
different models indexed by k gives the corresponding DIC and pD. We prefer the
model with smaller DIC, thus k = 5 is preferred as shown in Fig. 4.6. Also note that
pD is always less than the length of θk, since pD depends on the strength of priors,
the structure of the Bayesian model and the data (Spiegelhalter et al. 2002).

96
4
Bayesian Chain Ladder Models
80000
85000
90000
95000
100000
2
4
6
8
10
Iteration
k
0
40000
k
Frequency
2
3
4
5
6
7
8
9
10
Fig. 4.7 The trace plot and the histogram of k
RJMCMC method:
We iterate for 105 times. The within-model acceptance rate is 0.37 and the between-
model acceptance rate is 0.11. We plot the trace plot and the histogram of k in Fig. 4.7.
In this example, DIC and RJMCMC suggest the same best model, k = 5. However,
the DIC method takes a much longer time than RJMCMC. The reason is that the DIC
method spends equal time on every model while RJMCMC always tends to jump to a
more “accepted” model. Hence, in term of running time, RJMCMC is more efﬁcient.
Example 4.4 (Liability insurance claims data) We continue with Example 4.1. DIC
method suggests that the models with k larger than 7 perform equally well as shown in
Fig. 4.8. We choose k = 8 to keep pD as small as possible. RJMCMC is then applied
starting from k0 = 3 and iterating for 105 times. The trace plot and histogram of k are
shown in Fig. 4.9. Again, the model with k = 8 is preferred. RJMCMC outperforms
DIC in terms of distinguishing the “best” model from the other candidates.
We set k = 8 and estimate the posterior mean and the 95% CPDR of γ, comparing
with the CL estimates (in logarithm scale) shown in Fig. 4.10. The development
pattern after age 8 is smoothed to a straight line due to an exponential decay curve
being used. The big jump at development period 23 represents a large proportion of
tail development to the ultimate claims. In fact, the last point is valued as
log
⎛
⎝
J
	
j=I+1
exp (α −jβ)
⎞
⎠.

4.3 A Bayesian ODP Model with Tail Factor
97
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
20
25
30
35
k
pD
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−78600
−78400
−78200
−78000
−77800
DIC
●
●
pD
DIC
Fig. 4.8 DIC’s and pD’s for Verrall and Wüthrich (2012) data with respect to k
80000
85000
90000
95000
100000
7
8
9
11
Iteration
k
0
20000
50000
k
Frequency
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Fig. 4.9 The trace plot and the histogram of k for Verrall and Wüthrich (2012) data
We close this subsection by summarizing the total outstanding liability estimates
from different models in Table 4.6. For model (4.1) and (4.2), ˆR is an unbiased
estimate and equal to the CL estimate. For model (4.4) and (4.7), ˆR is an estimate of
posterior mean.

98
4
Bayesian Chain Ladder Models
5
10
15
20
−6
−5
−4
−3
−2
−1
Development Year
Log of development parameters
CL Estimate
Posterior Mean
95% CPDR
Fig. 4.10 The Logarithm of development parameters γ’s including the tail factor
Table 4.6 Comparison of the
total outstanding liability
estimates from four different
models
Model
Estimate
No tail
With tail
(4.1)
ˆR
1,463,076
1,599,558
se(R)
55,300
58,528
(4.2)
ˆR
1,463,076
NA
se(R)
60,444
NA
(4.4)
ˆR
1,463,312
NA
se(R)
60,428
NA
(4.7)
ˆR
1,475,336
1,610,734
se(R)
54,060
56,746
4.4
Estimation of Claims Liability in WorkSafe VIC
In this section, we analyze WorkSafe Victoria claims data to estimate the claims
liabilities of the weekly beneﬁt and doctor beneﬁt. The data are from the actuarial
valuation reports of outstanding claims liability for the scheme as of 30 June 2012 by
Pricewaterhouse Coopers (PwC) Actuarial Pty Ltd (Simpson and McCourt 2012).
4.4.1
Background of WorkSafe Victoria
A company operating in Victoria must take out WorkSafe insurance if it pays more
than $7,500 a year in rateable remuneration. WorkSafe insurance covers employee’s

4.4 Estimation of Claims Liability in WorkSafe VIC
99
Table 4.7 Summary of the PwC report
Beneﬁt
Sub-beneﬁt
Method
Key note
Weekly
Weekly
PPAC
34% of the total liability
Occupational
rehabilitation
Relate to
income
Help workers back to work
Medical
and like
Doctor
PPCI
Shorter tail than weekly beneﬁt
Hospital
PPCI
Correlated with doctor
Paramedical
PPAC
Generally ceases one year after weekly
beneﬁt
Hearing aids
PPCI
Missing data before experience year 1994
Personal and household
services
PPAC
Including attendant care, personal
services, home care, case management,
home and vehicle modiﬁcation payments
Community integration
program
CL on
amounts
Personal & household services for
catastrophically injured workers
Medical reports
PPCI
Refers to independent medical
examinations and treating health
practitioners’ reports
Common
law
Common law damages
and legal costs
PPCR
Relates to damages and costs arising from
common law claims with respect to
injuries occurring on or after 20 Oct 1999
Old common law
PPCR
Date of injury prior to 12 Nov 1997
Impairment
and death
beneﬁts
Impairment
PPCR
Injured workers can access impairment
beneﬁt if their whole person impairment is
assessed as being 10% or more
Maim
PPCR
The maim beneﬁt is in run-off, being
applicable only for injuries occurring
prior to 12 Nov 1997
Death lump sum
PPCR
Includes payments of statutory lump sum
and interest payments on it
Death pension
PPAC
Payment pattern determines the method
used
Disputes,
recoveries
and others
Statutory legal
PPCR
All legal costs, other than those associated
with common law cases, arising from
workers and employers appealing
decisions relating to eligibility of
payments or continuance of beneﬁts
Investigation costs
PPCI
Can be incurred before any claims
payments
Recoveries
PPCI
Relates to recoveries from negligent third
parties or recoveries of amounts where
agents have paid injured workers in excess
of the required amount
Other
PPCI
Travel and accommodation costs

100
4
Bayesian Chain Ladder Models
work related claims, such as back-injury during work. The beneﬁts include income
replacement, medical costs, rehabilitation etc. The premiums depend on the remuner-
ation, the industry classiﬁcation, industry claims history or its own business claims
history, capping etc. Most of the functions associated with premium and claims
management are performed by WorkSafe agents appointed by WorkSafe, including
Allianz Australia Workers’ Compensation Ltd., CGU Workers Compensation Ltd.
etc.
4.4.1.1
Beneﬁts
Depending on the features of a claim, one beneﬁt or several beneﬁts may be paid.
A beneﬁt can be a stream of payments extending for years or a lump sum. In the
claims reserving problem, it is desirable to distinguish beneﬁts in terms of payment
period, settlement rate, average size etc. The PwC report divides claims payments
into ﬁve beneﬁts shown in Table 4.7, each of which has several sub-beneﬁts. The
reserving method is chosen for each sub-beneﬁt depending on the beneﬁt features
and the data available. The last column in Table 4.7 provides some key information
about each sub-beneﬁt.
4.4.1.2
Reserving Methods Used by the PwC Report
The methods used in the PwC report mainly include payments per active claim
(PPAC), payments per claim incurred (PPCI) and payments per claim resolved.
For example, it is suitable to use PPAC to model the weekly beneﬁt. The weekly
beneﬁt is to compensate the loss of salary. So PPAC during a development year
should be stably proportional to average weekly salary for that period. In contrast,
PPCI is not suitable for the weekly beneﬁt since PPCI does not take account of the
duration of a claim, a main factor determining the weekly beneﬁt.
4.4.2
Estimation of the Weekly Beneﬁt Liability Using
Models (4.1) and (4.7)
We analyze the weekly beneﬁt using the distribution-free model with tail factor
(4.1) and the Bayesian ODP model with tail factor (4.7). We will show that the tail
development consists of a large percentage of total outstanding liability.

4.4 Estimation of Claims Liability in WorkSafe VIC
101
Table 4.8 The outstanding claims liability estimates of the weekly beneﬁt from different models
Model
Expected value
Standard deviation
95% PI/CPDR
(4.1)
2,902,875,000
172,396,900
(2,558,081,200, 3,247,668,800)
(4.7)
3,127,649,615
145,385,671
(2,849,161,960, 3,417,721,458)
PwC
2,831,072,753
NA
NA
4.4.2.1
The Distribution-Free Model (4.1)
We apply this model to the incremental payments run-off triangle. The total out-
standing liability is estimated as 2,902,875,000 dollars with the standard error of
172,396,900 dollars (CV = 6.0%). The PwC estimate of 2,831,072,753 dollars is
within the 95% prediction interval (2,558,081,200, 3,247,668,800).
From the diagnostic plots in Fig. 4.11, we can see an obvious pattern in the
standardized residuals vs. original years plot, which implies that the distribution-
free model does not ﬁt the data well (i.e. the model assumptions do not hold). The
PwC report mentioned that the scheme structure changed in 2010, 2006, 1999 and
1997. These changes affected the weekly beneﬁt, which more or less explains the
pattern observed.
4.4.2.2
The Bayesian Over-Dispersed Poisson Model with Tail Factor
(4.7)
First we apply the RJMCMC algorithm. The trace plot and the histogram of k are
plotted in Fig. 4.12. Then we apply the M-H algorithm with k = 8 to estimate the
outstanding liability. The tail factor is considered and J is assumed to be 37. The
posterior mean of total outstanding liability is estimated as 3,127,649,615 dollars
with the standard error of 145,385,671 dollars (CV = 4.6%). The 95% CPDR is
(2,849,161,960, 3,417,721,458) as shown in Table 4.8.
4.4.2.3
Limitations
The above analysis demonstrates that real world problems are always more complex
than our models. In the actuarial area, we typically use a statistical model to identify
and quantify the independent risk. Other risks, such as event risk, strategic risk,
operational risk, legal risk etc, are difﬁcult to be quantiﬁed by a statistical model.
The models discussed in this chapter all assume that historical experience can
predict the future. When the assumption does not hold, actuarial judgement is neces-
sary to adjust the prediction inferred from the model. Nevertheless, a comprehensive
understanding of model assumptions, historical events and possible future events is
required before making any judgements.

102
4
Bayesian Chain Ladder Models
1
3
5
7
9
12
15
18
21
24
27
Forecast
Latest
Mack Chain Ladder Results
Origin period
Amount
0.0e+00
5.0e+08
1.0e+09
1.5e+09
●
●
●
●
●
●
●
●
●
●
●●●
●●●
●
●●●●●●●
●●●
0
5
10
15
20
25
0.0e+00
5.0e+08
1.0e+09
1.5e+09
Chain ladder developments by origin period
Development period
Amount
1
1
1
1
1
1
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
2
2
2
2
2
2
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
3
3
3
3
3
3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
4
4
4
4
4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4
5
5
5
5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5
6
6
6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6
7
7
7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7
8
8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8
9
9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9
0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
a
a a a a a a a a a a a a a a a a
b
b b b b b b b b b b b b b b b
c
c c c c c c c c c c c c c c
d
d
d d d d d d d d d d d d
e
e
e e e e e e e e e e e
f
f
f f f f f f f f f f
g
g
g g g g g g g g g
h
h h h h h h h h h
i
i i i i i i i i
j
j j j j j j j
k
k k k k k k
l
l
l l l l
m
m
mmm
n
n
n n
o
o
o
p
p
q
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5.0e+08
1.0e+09
1.5e+09
−2
−1
0
1
2
3
4
Fitted
Standardised residuals
●
●
●
●
●
●●
●
●
●
●●
●
●
●●●●●●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
−2
−1
0
1
2
3
4
Origin period
Standardised residuals
●
●
●
●
●
●●
●
●
●
●●
●
●
●●●●●●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
−2
−1
0
1
2
3
4
Calendar period
Standardised residuals
●
●
●
●
●
●●
●
●
●
●●
●
●
●●●●●●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●●
●
●●
●
●
●
●
●
●●
●●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●●
●
●
●
●●
●●
●
●
●
●
●
●●●●●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●●
●●●●
●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●●●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●●●
●
●
●
●
●
●
●●
●
●●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
5
10
15
20
25
−2
−1
0
1
2
3
4
Development period
Standardised residuals
Fig. 4.11 The diagnostic plots for the distribution-free model applied to the weekly beneﬁt

4.4 Estimation of Claims Liability in WorkSafe VIC
103
0e+00
2e+04
4e+04
6e+04
8e+04
1e+05
10
15
20
25
Iterations
k
0
20000
k
Frequency
6
7
8
9
11
13
15
17
19
21
23
25
Fig. 4.12 The trace plot and the histogram of k for the weekly beneﬁt data
4.4.3
Estimation of the Doctor Beneﬁt Liability Using
a Compound Model
The doctor beneﬁt is not subject to changes in legislation as frequently as the weekly
beneﬁt, hence the historical claims data are much more instructive for the future
claims. The PPCI method is used to analyze the doctor beneﬁt. Compared to the
CL method applied to the claims amounts directly, the PPCI method provides more
information, such as the total incurred claims number and the average claim size.
There are three steps in the PPCI method:
1. Project the ultimate incurred claims number for each accident year.
2. Divide the incremental claims amounts by the ultimate claims number to get the
PPCI triangle, and project the PPCI triangle to get the outstanding PPCI.
3. Combine the ultimate claims number with the outstanding PPCI to get the out-
standing liability.
Here we apply the Bayesian ODP model without tail factor model (4.4) to both the
claims number and PPCI triangles, since the doctor beneﬁt is not a long-tailed beneﬁt.
We then aggregate them using a compound model.
4.4.3.1
Preliminary GLM Analysis
Before going to the Bayesian analysis, we apply a quasi-likelihood GLM to the
incremental claims number, in which a log link function and variance proportional
to mean are speciﬁed. It is equivalent to ﬁtting an ODP model (4.2). We get the scaled
Pearson residual plot in Fig. 4.13.

104
4
Bayesian Chain Ladder Models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
−8
−6
−4
−2
0
2
4
6
Development period
Scaled Pearson residuals
●
●
●
●
●
●●
●
●
●●●
●
●●
●●
●●●●●●●●
●●
●
●
●
●
●●
●
●
●●●●●●●●●●●●●●●●●
●
●
●
●
●
●
●
●
●●●●●●●●
●
●●●●●●●●●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●
●
●
●●
●●●
●●●●●●●●●●●●●●●●
●
●
●●●●●●●
●●●●●●●●●●●
●●
●
●●
●
●
●●●●●●
●
●●●●●●●
●●
●
●
●
●
●●●
●●●
●
●●●●●●●●●
●
●●
●●●●●●●
●●
●●●●●●●
●
●●●●●
●●
●
●●
●●●●●
●
●
●●●●●●●●●●●●●●●●●
●●●●●●
●
●●●●●●●
●
●
●
●●
●●
●●●●●●●●●
●
●
●
●●●●●●●●●●●●
●
●●●
●●●●●
●●
●
●
●
●●
●
●●●●●●●●
●
●
●
●●●●●●●●
●
●
●●●●●●●
●
●
●
●
●
●●
●●●
●
●
●●
●●●●
●
●●●●●●
●
●
●●●●
●
●
●●●
●
●●●
●
●
●
●
●
●
0
5
10
15
20
25
−8
−6
−4
−2
0
2
4
6
Accident period
Scaled Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
25
−8
−6
−4
−2
0
2
4
6
Experience period
Scaled Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
−8
−6
−4
−2
0
2
4
6
Fitted values
Scaled Pearson residuals
Fig. 4.13 The scaled Pearson residuals of the ODP model

4.4 Estimation of Claims Liability in WorkSafe VIC
105
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
Development period
Scaled Pearson residuals
●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●●
●●
●●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●
●
●
●●●
●●
●●
●
●
●
●
●●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
−2
−1
0
1
2
3
−2
−1
0
1
2
3
Accident period
Scaled Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
25
−2
−1
0
1
2
3
Experience period
Scaled Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
−2
−1
0
1
2
3
Fitted values
Scaled Pearson residuals
Fig. 4.14 The scaled Pearson residuals of the GLM with a gamma error and a log link function

106
4
Bayesian Chain Ladder Models
It displays heteroscedasticity, implying the variance is proportional to the mean
powered to more than one. We then try a GLM with the same link function but with
variance proportional to mean squared, as follows:
ni, j ∼Gamma

α,
α
μiγ j

, i = 1, . . . , 27, j = 1, . . . , 27.
A better residual plot is obtained as in Fig. 4.14. The scaled Pearson residual in this
model is deﬁned as
ri j =
ei j

ˆφV

ˆni j
 = ni j −ˆni j
ˆni j
√
ˆα.
By dividing the incremental payments triangle by the ultimate claims number
predicted from the above model, we get the PPCI triangle. The same process is
applied to the PPCI triangle as to the claims number. Similarly, a gamma error
distribution does a better ﬁt than an ODP error structure.
This preliminary GLM ﬁtting provides valuable information about the further
Bayesian analysis. In the following, we will use the gamma error distribution for
both claims numbers and PPCI.
4.4.3.2
A Bayesian Gamma Model for the Claims Numbers and PPCI
According to the preliminary GLM analysis, a Bayesian gamma model (similar to
model (4.4)) is used here, as follows:
ni, j ∼Gamma

α,
α
μiγ j

μi ∼Gamma (ai, bi)
γ j ∼Gamma

c j, d j

.
The prior N (20000, 1000) is assumed for the ultimate claims numbers of the three
most recent accident years, μi, i = 25,26,27. The strong prior works as the BF
method to reduce the leverage effect of the immature claims numbers. The Stan code
is as follows:
1
number . code <-"
2
data {
3
int
N;
//
Number
of
observations
4
int
K;
//
Number
of
accident
years
5
int
M;
//
Number
of
development
years
6
int
acc[N];
//
Accident
years
in
upper
triange
7
int
dev[N];
//
Development
years
in
upper
triangle
8
real
first _inc[N];
//
Number
of
claims
in
upper
triangle
9
int
n;
//
Number
of
future
prediction
10
int
acc_p[n];
//
Accident
years
in
lower
triangle
11
int
dev_p[n];
//
Development
years
in
lower
triangle
12
}

4.4 Estimation of Claims Liability in WorkSafe VIC
107
13
parameters {
14
vector < lower =0, upper =50000 >[ K]
ult;
15
vector [M]
dev_raw;
16
real < lower =0>
alpha ;
17
}
18
transformed
parameters {
19
vector [N]
means ;
20
vector [M]
dev_ norm ;
// Normalized
development
pattern
21
dev_ norm <-exp(dev_raw)/sum(exp( dev_raw));
22
for
(i
in
1:N){
23
means [i] <-dev_ norm [dev[i]]*ult[acc[i]];
24
}
25
}
26
model {
27
for
(i
in
1:N)
28
first _ inc[i]
~
gamma (alpha , alpha / means [i]);
29
for
(i
in
25:27)
30
ult[i]
~
normal (20000 ,1000) ;
31
}
32
generated
quantities
33
{
34
real
pearson _res[N];
35
real
means _p[n];
36
for
(i
in
1:N)
37
pearson _res[i] <-( first _inc[i]- means [i])/ means [i]* sqrt ( alpha );
38
for
(i
in
1:n)
39
means _p[i] <-dev_ norm [dev_p[i]]*ult[acc_p[i]];
40
}
41
"
The posterior mean of residuals vs. linear predictors is plotted in Fig. 4.15, which
shows a similar pattern to Fig. 4.14. It seems that the variance is proportional to
the mean powered to some value between 1 and 2. We could use a Tweedie family
in glm( ) function in R, but Stan does not have such a distribution. The predictive
distribution of outstanding claims numbers is positively skewed. The posterior mean
of outstanding claims number is estimated as 13,923, which is higher than the PwC
estimate of 12,811. It takes one minute to run 1,600 iterations. We use the posterior
means of ultimate claims numbers to derive the PPCI triangle and ﬁt the same model
as for the claims numbers. The residual plot and the histogram of total outstanding
PPCI are shown in Fig. 4.16. The predictive distribution of outstanding PPCI is
roughly symmetric with the posterior mean of 18,012 dollars, compared with the
PwC estimate of 17,827 dollars.

108
4
Bayesian Chain Ladder Models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
−2
−1
0
1
2
3
4
Linear predictor
Scaled Pearson residuals
Outstanding claims numbers
Density
10000
15000
20000
25000
0.00000
0.00005
0.00010
0.00015
●
●Posterior mean
95% CPDR
Min and Max
Fig. 4.15 The residual plot and the histogram of total outstanding claims number

4.4 Estimation of Claims Liability in WorkSafe VIC
109
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
2
3
4
5
6
7
−2
0
2
4
Linear predictor
Scaled Pearson residuals
Outstanding PPCI
Density
16500
17000
17500
18000
18500
19000
19500
0e+00
2e−04
4e−04
6e−04
8e−04
●
●Posterior mean
95% CPDR
Min and Max
Fig. 4.16 The residual plot and the histogram of total outstanding PPCI
4.4.3.3
A Compound Model to Combine the Ultimate Claims Numbers
and the Outstanding PPCI
Ideally, we should use the predictive distribution of ultimate claims numbers to derive
the PPCI triangle, then combine the predictive distribution of the outstanding PPCI
with the corresponding ultimate claims numbers to get the predictive distribution of
outstanding liability. This method requires a large amount of computing time.

110
4
Bayesian Chain Ladder Models
Here we propose a compound model to get the predictive distribution of outstand-
ing liability. The model is speciﬁed as follows:
yi j =
μi
	
k=1
xi jk, i = 1, . . . , 27, j = 1, . . . , 27
μi ∼Distributioni
xi jk ∼Gamma

αi j, βi j

, k = 1, . . . , μi,
where μi is the ultimate claims number of accident year i whose distribution is
approximated by a Bayesian model, and xi jk is the payment for the kth claim during
the development year j, with the distribution depending on both accident year and
development year.
The payments per claim incurred (PPCI) during the development period j of
accident year i is deﬁned as
PPCIi j := yi j/E (μi).
Note that E(PPCIi j) = E(xi jk). The posterior mean of μi is an estimate of E (μi).
The relationship between the variance of PPCIi j and the variance of xi jk is as
follows:
Var

PPCIi j

= Var

μi
k=1 xi jk
E (μi)

= Var

xi jk

E (μi) +

E

xi jk
2Var (μi)
(E (μi))2
= Var

xi jk

E (μi) +

E

PPCIi j
2Var (μi)
(E (μi))2
.
We can solve Var

xi jk

as
Var

xi jk

= (E (μi))2Var

PPCIi j

−Var (μi)

E

PPCIi j
2
E (μi)
,
(4.9)
where all the quantities on the right hand side can be estimated by a MC sam-
ple. The distribution of yi j conditional on μi is Gamma

μiαi j, βi j

, where αi j =
E

xi jk
2/Var

xi jk

, βi j = αi j/E

xi jk

.

4.4 Estimation of Claims Liability in WorkSafe VIC
111
The outstanding claims liability of accident year i is Ri|μi = I
j=I−i+1 yi j. The
predictive distribution of total claims liability is shown in Fig. 4.17. The poste-
rior mean of total claims liability is estimated as 391,761,803 dollars with the
standard deviation of 10,195,111 (CV = 2.6%), compared with 396,827,792 dol-
lars estimated by PwC. The 95% CPDR of total claims liability is estimated as
(373,902,941, 414,549,267). We summarize the predictions made from the com-
pound model in Table 4.9.
Total outstanding claims liability
Density
380
400
420
440
0.00
0.01
0.02
0.03
0.04
●
●Posterior mean
95% CPDR
Min and Max
Fig. 4.17 The predictive distribution of total outstanding liability of the doctor beneﬁt
Table 4.9 Summary of the predictions made from the compound model
Post. mean
Std. deviation
95% CPDR
PwC estimate
O/S claims no.
13,923
2,407
(9,742, 19,117)
12,811
O/S PPCI
18,012
474
(17,056, 18,901)
17,827
O/S liability
391,761,803
10,195,111
(373,902,941,
414,549,267)
396,827,792

112
4
Bayesian Chain Ladder Models
4.4.3.4
Other Ways to Combine the Ultimate Claims Numbers
with the Outstanding PPCI
As a ﬁnal remark, we point out that the PPCI triangle is conditional on the posterior
mean of ultimate claims number, i.e., E(μi|yyy). If we only consider the variation in
PPCI and keep the ultimate claims numbers ﬁxed at the posterior mean, we would
underestimate the variation of outstanding liability, i.e., we ignore the estimation
error in E(μi|yyy).
The key point of the compound model is Eq. (4.9), which recovers the variation
in a single claim payment xi jk, which is assumed to be independent of the ultimate
claims number μi.
4.5
Discussion
Occasionally, we see some abnormal values in a particular diagonal line or some
pattern in the residuals vs. experience periods plot. This is called the experience
period effect or calendar period effect. It can be due to the uncommon inﬂation
rates in a particular calendar year. The straightforward way to address this problem
is to involve an experience period covariate. This covariate effectively isolate the
outliers in the diagonal lines, so the estimation of accident period parameters and
development period parameters are not affected.
For the run-off triangle data, the experience period parameters are not used in
the prediction of future claims since all future claims correspond to new experience
periods. So the main purpose of introducing the experience period covariate is to
remove the discontinuous abnormal calendar year effect.
An innovative contribution made in this chapter is using a compound model to
quantify the uncertainty associated with the estimates from the PPCI method. The
distributional assumption of xi jk has not been checked. To check this assumption,
we need the payments data during the whole life of individual claims.
We also stress the importance of preliminary GLM ﬁtting. Bayesian modelling
needs time-consuming inferential tools. We normally cannot get the inference and
do the goodness-of-ﬁt check of a Bayesian model as easily as a GLM. So a prelim-
inary GLM ﬁtting can help us set up the Bayesian model with regards to the error
distribution, the mean function, the priors for parameters etc.
Finally, we point out that it is hard to program RJMCMC and there are no statistical
packages available to do RJMCMC directly. To avoid RJMCMC but still incorporate
a tail factor, a non-linear curve mean function, such as log-logistic curve and Hoerl
curve (Taylor 2000), can be used. If these non-linear curves are used, GLM will not
work, which demonstrates an advantage of Bayesian models. In the next chapter,
rather than using curves, we go a step further to use a basis expansion model, which
is a non-parametric approach.

4.6 Bibliographic Notes
113
4.6
Bibliographic Notes
The Bornhuetter-Ferguson method derives from Bornhuetter and Ferguson (1972).
Friedland (2010) is the reading material of the CAS Exam 5 and provides an overview
of basic techniques of estimating unpaid claims. For the stochastic claims reserving
methods, Mack (1993, 1999, 2008) established the Mack’s models. Australian actu-
aries are largely inﬂuenced by Taylor (2000). England and Verrall (2002, 2006) and
Wüthrich and Merz (2008, 2015) are summaries of stochastic reserving models.
An excellent GLM reference is McCullagh and Nelder (1989). The references of
ODP model in claims reserving problem include Renshaw and Verrall (1998), Verrall
(2000, 2004), Alai et al. (2009), Saluz et al. (2011), England et al. (2012), Verrall
and Wüthrich (2012) and Wüthrich (2013a).
Other papers using a Bayesian approach include Scollnik (2001), De Alba (2002),
Ntzoufras and Dellaportas (2002) and Meyers (2009, 2015).
Clark (2003) and Zhang et al. (2012) used the stochastic curve models. Brydon and
Verrall (2009) and Wüthrich (2013a) considered the calendar year effect. Piwcewicz
(2008) and Beens et al. (2010) are two presentations about Bayesian claims reserving
method in IAA’s non-life insurance seminars.
Verrall et al. (2012) and Verrall and Wüthrich (2012) used RJMCMC. RJMCMC
is proposed by Green (1995). The collective risk model (or aggregate risk model)
have been much studied in the standard risk modelling text books such as Klugman
et al. (2012) and Gray and Pitts (2012). Gao et al. (2018) investigates the uncertainty
associated with the PPCI method from a different perspective.
References
Alai, D. H., Merz, M., & Wüthrich, M. V. (2009). Mean square error of prediction in the Bornhuetter-
Ferguson claims reserving method. Annals of Actuarial Science, 4, 7–31.
Beens, F., Bui, L., Collings, S., & Gill, A. (2010). Stochastic reserving using Bayesian models: Can
it add value? In Institute of Actuaries of Australia 17th General Insurance Seminar.
Bornhuetter, R. L., & Ferguson, R. (1972). The actuary and IBNR. Proceedings of the Casualty
Actuarial Society, 59, 181–195.
Brydon, D., & Verrall, R. J. (2009). Calendar year effects, claims inﬂation and the chain-ladder
technique. Annals of Actuarial Science, 4, 287–301.
Clark, D. R. (2003). LDF curve-ﬁtting and stochastic reserving: A maximum likelihood approach.
Casualty Actuarial Society Forum (Fall 2003), 41–92.
De Alba, E. (2002). Bayesian estimation of outstanding claim reserves. North American Actuarial
Journal, 6, 1–20.
Efron, B., & Tibshirani, R. J. (1994). An Introduction to the Bootstrap. New York: Chapman &
Hall.
England, P. D., & Verrall, R. J. (2002). Stochastic claims reserving in general insurance. British
Actuarial Journal, 8, 443–518.
England, P. D., & Verrall, R. J. (2006). Predictive distributions of outstanding liabilities in general
insurance. Annals of Actuarial Science, 1, 221–270.
England, P. D., Verrall, R. J., & Wüthrich, M. V. (2012). Bayesian over-dispersed poisson model
and the Bornhuetter-Ferguson claims reserving method. Annals of Actuarial Science, 6, 258–283.

114
4
Bayesian Chain Ladder Models
Faraway, J. J. (2015). Linear models with R (2nd ed.). Boca Raton: Chapman & Hall.
Friedland, J. (2010). Estimating unpaid claims using basic techniques. Casualty Actuarial Society
Study Notes.
Gao, G., Meng, S., & Shi, Y. (2018). Stochastic payments per claim incurred. North American
Actuarial Journal, (forthcoming).
Gesmann, M., Murphy, D., Zhang, W., Carrato, A., Crupi, G., Wüthrich, M. V., et al. (2015). Chain
ladder: statistical methods and models for claims reserving in general insurance. R package
version 0.2.1.
Gilks, W. R., & Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Journal of the
Royal Statistical Society C, 41, 337–348.
Gray, R. J., & Pitts, S. M. (2012). Risk modelling in general insurance: From principles to practice.
Cambridge: Cambridge University Press.
Green, P. J. (1995). Reversible jump Markov chain Monte Carlo computation and Bayesian model
determination. Biometrika, 82, 711–732.
Klugman, S. A., Panjer, H. H., & Willmot, G. E. (2012). Loss models: From data to decisions (4th
ed.). New York: Wiley.
Mack, T. (1993). Distribution-free calculation of the standard error of chain ladder reserve estimates.
ASTIN Bulletin, 23, 213–225.
Mack, T. (1999). The standard error of chain-ladder reserve estimates, recursive calculation and
inclusion of a tail factor. ASTIN Bulletin, 29, 361–366.
Mack, T. (2008). The prediction error of Bornhuetter-Ferguson. ASTIN Bulletin, 38, 87.
McCullagh, P., & Nelder, J. A. (1989). Generalized linear models (2nd ed.). New York: Chapman
& Hall.
Meyers, G. (2009). Stochastic loss reserving with the collective risk model. Variance, 3, 239–269.
Meyers, G. (2015). Stochastic loss reserving using Bayesian MCMC models. CAS Monograph
Series, 1, 1–64.
Ntzoufras, I., & Dellaportas, P. (2002). Bayesian modelling of outstanding liabilities incorporating
claim count uncertainty. North American Actuarial Journal, 6, 113–125.
Piwcewicz, B. (2008). Stochastic reserving: case study using a Bayesian approach. In Institute of
Actuaries of Australia 16th General Insurance Seminar.
Renshaw, A. E., & Verrall, R. J. (1998). A stochastic model underlying the chain-ladder technique.
British Actuarial Journal, 4, 903–923.
Saluz, A., Gisler, A., & Wüthrich, M. V. (2011). Development pattern and prediction error for the
stochastic Bornhuetter-Ferguson claims reserving method. ASTIN Bulletin, 41, 279–313.
Scollnik, D. P. (2001). Actuarial modeling with MCMC and BUGS. North American Actuarial
Journal, 5, 96–124.
Simpson, L., & McCourt, P. (2012). Worksafe Victoria actuarial valuation of outstanding claims
liability for the scheme as at 30 June 2012, Technical report. PricewaterhouseCoopers Actuarial
Pty Ltd.
Spiegelhalter, D. J., Best, N. G., Carlin, B. R., & van der Linde, A. (2002). Bayesian measures of
model complexity and ﬁt. Journal of the Royal Statistical Society B, 64, 583–616.
Taylor, G. (2000). Loss reserving: An actuarial perspective. Boston: Kluwer Academic Publishers.
Verrall, R. J. (2000). An investigation into stochastic claims reserving models and the chain-ladder
technique. Insurance: Mathematics and Economics 26, 91–99.
Verrall, R. J. (2004). A Bayesian generalized linear model for the Bornhuetter-Ferguson method of
claims reserving. North American Actuarial Journal, 8, 67–89.
Verrall, R. J., & Wüthrich, M. V. (2012). Reversible jump Markov chain Monte Carlo method for
parameter reduction in claims reserving. North American Actuarial Journal, 16, 240–259.
Verrall, R. J., Hössjer, O., & Björkwall, S. (2012). Modelling claims run-off with reversible jump
Markov chain Monte Carlo methods. ASTIN Bulletin, 42, 35–58.
Wüthrich, M. V. (2013a). Calendar year dependence modeling in run-off triangles. In ASTIN Col-
loquium, 21–24.

References
115
Wüthrich, M. V. (2013b). Challenges with non-informative gamma priors in the Bayesian over-
dispersed Poisson reserving model. Insurance: Mathematics and Economics, 52, 352–358.
Wüthrich, M. V., & Merz, M. (2008). Stochastic claims reserving methods in insurance. Chichester:
Wiley.
Wüthrich, M. V., & Merz, M. (2015). Stochastic claims reserving manual: Advances in dynamic
modelling. SSRN, ID 2649057.
Zhang, Y., Dukic, V., & Guszcza, J. (2012). A Bayesian non-linear model for forecasting insurance
loss payments. Journal of the Royal Statistical Society A, 175, 637–656.

Chapter 5
Bayesian Basis Expansion Models
Abstract In this chapter, Bayesian basis expansion models are used to ﬁt various
development patterns and accommodate the tail factor. A parametric model is typ-
ically characterized by a parametric mean function and an error distribution. The
shape of the mean function is restricted by the space of parameters. Non-parametric
models such as basis expansion models are able to automatically adjust to ﬁt any
shape of data. In Sect. 5.1, the aspects of splines are reviewed, including spline basis
functions, smoothing splines, low rank smoothing splines and Bayesian shrinkage
splines. In Sect. 5.2, we study two simulated examples. The ﬁrst simulated example
is based on a trigonometric mean function, while the second simulated example is
based on the claims payments process. Both examples illustrate the usefulness of
natural cubic spline basis in the extrapolation beyond the range of data. Section 5.3
is the application of above methodology to the doctor beneﬁt in WorkSafe Victoria.
The basis expansion model used to ﬁt the PPCI triangle induces a tail development.
5.1
Aspects of Splines
There is a trade-off between ﬂexibility and simplicity in model ﬁtting. Basis expan-
sion models on one hand are more ﬂexible, able to be adjusted to ﬁt various shapes of
data, while on the other hand, they are more complicated (i.e., involve more param-
eters). Before using a non-parametric model, we should consider whether there is a
capable parametric model. The log-logistic curve and Hoerl curve together with the
models in the previous chapter can tackle many claims reserving problems.
Consider the following underlying true model:
yi ∼f (xi) + εi, i = 1, . . . , n,
where εi are i.i.d N

0, σ 2
ε

. A non-parametric approach is to approximate f by a
non-parametric function m. Basis expansion is a way to express the form of m. The
core idea of basis expansion is to expand the input x with additional variables, which
are transformations of x, and then to apply linear models to this newly expanded
space of input x. In basis expansion models, m is written as a linear combination of
basis functions, as follows:
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_5
117

118
5
Bayesian Basis Expansion Models
m (x) =
H

h=1
βhbh (x) ,
where bh is called a basis function. A common choice of bh is a polynomial. The
mechanism of deﬁning bh determines the behaviour of m. Here we consider m as
splines, which use polynomials as basis functions with some constraints. Splines
are a combination of polynomials and step functions. In polynomial models, the
basis functions have the form of bh (x) = xh. Polynomial models tend to capture the
shape of the data as long as there are high-degree polynomials. A disadvantage of
polynomial models is the global representation of basis functions, which means all
the data points can affect parameter estimation and every parameter can affect the
mean function. A step function model partitions the data into H parts and ﬁts the
hth part using a basis function bh (x) whose value is zero for the remaining parts of
data. Step function models have a disadvantage of discontinuity at the boundaries of
partition. Spline models are a combination of polynomial models and step function
models. For example, a cubic spline is a series of piecewise-cubic polynomials joined
continuously up to the second derivatives. The properties of continuity and being
piecewise are realised by using a particular set of basis functions.
5.1.1
Basis Functions of Splines
5.1.1.1
Truncated Power Basis
One intuitive choice is truncated power basis of degree p, which contains K + p + 1
basis functions as follows:
1, x, . . . , x p, (x −κ1)p
+ , . . . , (x −κK)p
+ ,
where(x −κi)p
+ = (x −κi)p for x > κi and0elsewhere,κi, i = 1, . . . , K arecalled
knots. The basis functions consist of two parts: the global polynomials up to degree p,
and the truncated degree p polynomials which have the local representation property.
It can be shown that any linear combination of these basis functions has continuous
derivatives up to order p −1 at every knot.
The degrees of freedom of a spline is the number of parameters in the mean
function. Truncated power basis of degree p has K + p + 1 degrees of freedom,
which is intuitive to join K + 1 pieces of degree p polynomials smoothly (up to p −
1th derivatives at knots), K p degrees of freedom are lost, leaving K + p + 1 degrees
of freedom, i.e., K + p + 1 = (K + 1) (p + 1) −K p. In the GLM setting, we write
the design matrix as
XXX =
⎡
⎣
1 x1 · · · x p
1 (x1 −κ1)p
+ · · · (x1 −κK)p
+
1 · · · · · · · · ·
· · ·
· · ·
· · ·
1 xn · · · x p
n (xn −κ1)p
+ · · · (xn −κK)p
+
⎤
⎦.

5.1 Aspects of Splines
119
At ﬁrst glance, it seems that spline models are more complicated than either poly-
nomial models or step function models. This is not true. Compared with polynomial
models, we do not need the higher degree polynomials to capture all the curvatures
of the data, since we have the local basis functions. Compared with step function
models, we overcome the problem of discontinuity via the mechanism of basis func-
tions. Spline models combine the advantages of both polynomial and step functions
models, and get rid of the ﬂaws of both models when they are used alone.
A truncated power basis has a practical disadvantage in that it is far from orthog-
onal, i.e., the columns of design matrix XXX are not orthogonal. It is better to work
with an equivalent basis with more stable numerical properties. Note that two bases
are equivalent if they span the same set of functions.
5.1.1.2
B-Spline Basis
The most common choice for spline basis is the B-spline basis of degree p, which
consists of piecewise continuous functions only non-zero over the intervals between
p + 2 adjacent knots. The degrees of freedom of a K-knot degree p B-spline basis
is K −p + 1, since the spline is to be evaluated only over the interval
	
κp+1, κK−p

.
To span the same function space as truncated power basis of degree p with K
knots, we need to add p arbitrary knots to the ends of [κ1, κK], i.e., we usually choose
the knots {κ1, κ1, κ1, κ1, κ2, . . . , κK−1, κK, κK, κK, κK} in a cubic B-spline basis. A
B-spline basis is an orthogonal set.
5.1.1.3
Radial Basis
Another set of basis functions equivalent to a truncated power basis of degree p (odd)
is a radial basis, as follows:
1, x, . . . , x p, |x −κ1|p, . . . , |x −κK|p.
We will come back to radial basis functions in smoothing splines and Bayesian
shrinkage splines.
5.1.2
Smoothing Splines
Smoothing splines come from the solution to the optimal problem of ﬁnding a func-
tion g to minimizes the residual sum of squares (RSS) plus a penalty on the integral
of the squared second derivatives of g. This penalized residual sum of squares is
RSS (g, λ) =
n

i=1
[yi −g (xi)]2 + λ
 	
g′′ (t)

2dt,
(5.1)

120
5
Bayesian Basis Expansion Models
where λ is a ﬁxed smoothing parameter. The ﬁrst term measures closeness to the
data, while the second term penalizes curvature in the function and λ establishes
a trade-off between the two. Two special cases are: λ →0, ˆg can be any function
that interpolates the data (i.e., RSS = 0); λ →∞, ˆg is the simple linear regression
ﬁt since no second derivative can be tolerated. Note that a smoothing spline is a
one-dimensional thin plate spline.
Remarkably, even without the constraint of g as splines, it can be shown, for
0 < λ < ∞, that ˆg is a natural cubic spline with knots placed at the unique values
of xi, i = 1, . . . , n (Hastie and Tibshirani 1990). Natural cubic splines are cubic
splines with the constraint that they are linear beyond the boundary knots. Hence,
the degrees of freedom of a smoothing spline ˆg are n (i.e., n = n + 3 + 1 −2 −2),
since 4 degrees of freedom are lost due to the linear constraints at two boundary
knots.
We can write this natural cubic spline as
ˆg (x) =
n

h=1
ˆβhbh (x) ,
where {bh : h = 1, . . . , n} is a set of n basis functions for representing this natural
cubic spline. We write the design matrix as
XXX =
⎡
⎣
b1 (x1) · · · bn (x1)
· · ·
· · ·
· · ·
b1 (xn) · · · bn (xn)
⎤
⎦,
which is an n × n matrix. RSS in (5.1) can be written as
RSS (β, λ) = yyy −XXXβ + λβTΩ
Ω
Ωβ,
(5.2)
where Ω
Ω
Ω [i, j] =

bi ′′ (t) b j ′′ (t) dt. The solution is ˆβ =

XXX T XXX + λΩ
Ω
Ω
−1XXX T yyy,
which has a additional penalty term λΩ
Ω
Ω compared with the ordinary least squares
solution.
5.1.2.1
Rank of a Smoother and Effective Degrees of Freedom
The ﬁtted values of smoothing splines are
ˆyˆyˆy = XXX

XXX T XXX + λΩ
Ω
Ω
−1XXX T yyy = SSSλyyy,
where SSSλ is known as the smoother matrix or hat matrix. We list some features of
SSSλ as follows:

5.1 Aspects of Splines
121
• SSSλ is a symmetric positive semi-deﬁnite matrix with rank n.
• SSSλ has n eigenvectors and n non-zero eigenvalues.
• λ cannot affect the eigenvectors of SSSλ.
• λ affects the eigenvalues of SSSλ negatively, except the ﬁrst two which are always 1
corresponding to the two-dimensional eigenspace of functions linear in x. Other
eigenvalues are between 0 and 1 depending on λ.
• The degree of freedom of SSSλ is dfλ = trace (SSSλ) = sum of eigenvalues, which is
always between 2 and n.
• When λ →0, all the eigenvalues are 1. dfλ = trace (SSSλ) = sum of eigenvalues
= n, corresponding to any functions interpolating the data.
• When λ →∞, all the eigenvalues are 0 except the ﬁrst two. dfλ = trace (SSSλ) =
sum of eigenvalues = 2, corresponding to a straight line.
5.1.2.2
Radial Basis Functions for Smoothing Splines
Smoothing splines have a natural representation in terms of radial basis functions.
For a given λ, a smoothing spline can be written as
ˆg (x) = ˆγ0 + ˆγ1x +
n

k=1
ˆδk|x −xk|3,
where ˆθ =

ˆγ0, ˆγ1, ˆδ1, . . . , ˆδn

minimizes the penalized residual sum of squares
n

i=1

yi −ˆγ0 −ˆγ1xi −
n

k=1
ˆδk|xi −xk|3
2
+ λ
n

i=1
ˆδi
n

k=1
ˆδk|xi −xk|3,
(5.3)
subject to the constraints n
k=1 ˆδk = n
k=1 ˆδkxk = 0. The constraints make the num-
ber of parameters n rather than n + 2 which is consistent with the degrees of freedom
of a smoothing spline.
The criterion (5.3) is connected with the criterion of best linear unbiased pre-
diction (BLUP) in a mixed effects model, which opens a gate for a Bayesian mixed
effects model representing a smoothing spline.
5.1.2.3
Choice of λ
The above discussion is based on a given λ. We can treat λ as a tuning parameter
which indexes different smoothing models. The choice of λ can be thought as a model
selection problem. The selection criterion relates to a model’s prediction capability
on an independent test data set. Typically, we use test error as a measure of prediction
capability, deﬁned as the prediction squared error over an independent test sample.

122
5
Bayesian Basis Expansion Models
The most widely used method for estimating the test error is cross-validation (see
Sect. 2.2). λ is chosen by minimizing CV or generalized CV (Hastie et al. 2009).
5.1.3
Low Rank Thin Plate Splines
The rank of smoother SSSλ is the number of distinct x. Sometimes it is called a full
rank smoother. Wood (2003, 2006) uses the truncated eigen-decomposition of XXX
to achieve a low rank smoother approximating the full rank smoother. A simpler
approximation is to set up a new natural cubic spline basis with speciﬁed knots
κi, i = 1, . . . , K, rather than at every distinctive x.
It can be shown that a natural cubic spline with speciﬁed knots ﬁtted by minimizing
(5.1) can approximate the full rank smoothing spline well (Ruppert et al. 2003). A
spline with ﬁxed knots is called a spline regression. If it is ﬁtted by minimizing (5.1),
it is called a penalized spline regression, or more generally a low rank thin plate
spline.
5.1.3.1
Rank of a Fixed-Knot Thin Plate Spline and Effective Degrees
of Freedom
Some features of a K-knot thin plate spline smoother SSSλ are as follows:
• SSSλ is a symmetric positive semi-deﬁnite matrix with rank of K.
• SSSλ has K eigenvectors and K non-zero eigenvalues.
• λ cannot affect the eigenvectors of SSSλ.
• λ affects the eigenvalues of SSSλ negatively, except the ﬁrst two which are always 1
corresponding to the two-dimensional eigenspace of functions linear in x. Other
eigenvalues are between 0 and 1 depending on λ.
• The degrees of freedom of SSSλ is dfλ = trace (SSSλ) = sum of eigenvalues, which is
always between 2 and K.
• When λ →0, all the eigenvalues are 1, SSSλ →I. dfλ = trace (SSSλ) = sum of eigen-
values = K, corresponding to any functions interpolating the K knots.
• When λ →∞, all the eigenvalues are 0 except the ﬁrst two. dfλ = trace (SSSλ) =
sum of eigenvalues = 2, corresponding to a straight line.
5.1.3.2
Radial Basis Functions for a Fixed-Knot Thin Plate Spline
For a given λ and ﬁxed knots κi, i = 1, . . . , K, the ﬁxed-knot thin plate spline can
be written as
ˆg (x) = ˆγ0 + ˆγ1x +
K

k=1
ˆδk|x −κk|3,

5.1 Aspects of Splines
123
where ˆθ =

ˆγ0, ˆγ1, ˆδ1, . . . , ˆδK

minimizes the following penalized residual sum of
squares,
n

i=1

yi −ˆγ0 −ˆγ1xi −
K

k=1
ˆδk|xi −κk|3
2
+ λ
K

l=1
ˆδl
K

k=1
ˆδk|κl −κk|3,
subject to the constraints K
k=1 ˆδk = K
k=1 ˆδkκk = 0. The constraint makes the num-
berofparameters K ratherthan K + 2whichisconsistentwiththedegreesoffreedom
of a natural cubic spline with K knots. For compact notation and programming, we
can write the above equation in terms of matrices, as follows:
RSS = ∥yyy −XXX ˆγ −ZZZ ˆδ∥+ λˆδT KKK ˆδ,
(5.4)
where XXX [i, ] = (1, xi)T , ZZZ [i, k] = |xi −κk|3, ˆγ =

ˆγ0, ˆγ1

, ˆδ =

ˆδ1, . . . , ˆδK

and
KKK [l, k] = |κl −κk|3,l = 1, . . . , K, k = 1, . . . , K.
5.1.3.3
Linkage to a Mixed Effects Model
As already mentioned at the end of Sect. 5.1.2, the criterion of minimizing (5.4) is
related to the criterion for calculating the best linear unbiased prediction (BLUP) in
a mixed effects model. Suppose we have the following mixed effects model:
yi = γ0 + γ1xi +
K

k=1
δk|xi −κk|3 + εi
E (δk) = 0; Var (δ) = σ 2
δ KKK −1
E (εi) = 0; Var(εεε) = σ 2
ε III.
BLUP of γ and δ is deﬁned as follows:
˜γ , ˜δ = argmin
γ ′,δ′
E

sT XXXγ ′ + t T ZZZδ′
−

sT XXXγ + t T ZZZδ
2,
for any arbitrary s and t, and subject to the unbiasedness constraint
E

sT XXXγ ′ + t T ZZZδ′
= E

sT XXXγ + t T ZZZδ

.
It can be shown that ˜γ and ˜δ also minimize the following penalized RSS:

yyy −XXXγ ′ −ZZZδ′T 
σ 2
ε III
−1 
yyy −XXXγ ′ −ZZZδ′
+ ˆδT 
σ 2
δ KKK −1−1 ˆδ,

124
5
Bayesian Basis Expansion Models
which is equivalent to minimizing (5.4) with λ = σ 2
ε /σ 2
δ . ˜γ and ˜δ have the following
expression:
 ˜γ
˜δ

=

CCCTCCC + λBBB
−1CCCT yyy,
where
CCC = [X, Z
X, Z
X, Z] , BBB =
0 0
0 KKK

.
The ﬁtted values are ˆy = CCC

CCCTCCC + λBBB
−1CCCT yyy. Note that ˜γ and ˜δ depend on the
variance parameters σ 2
δ and σ 2
ε , which can be estimated via maximum likelihood or
restricted maximum likelihood (REML).
The connection of a ﬁxed-knot thin plate spline with a mixed effects linear model
makes it possible to analyze a smoothing spline in the framework of a Bayesian
mixed effects linear model. Bayesian mixed effects linear models can quantify the
estimation uncertainties of variance parameters which are ignored in the REML
approach.
5.1.4
Bayesian Splines
Rather than using the equivalence of a smoothing spline to a mixed effects linear
model, we can set up a mixed effects model structure directly on the basis expansion
functions. The core idea of a smoothing spline is to shrink the parameters δi, i =
1, . . . , n towards 0 in Eq. (5.3), where the shrinkage force and style are controlled
by the smoothing parameter λ.
In the Bayesian framework, we can assume shrinkage priors, which perform the
role of the smoothing parameter. Generally, we use the following Bayesian shrinkage
spline model:
yi =
H

h=1
βhbh (xi) + εi
εi ∼N

0, σ 2
βh ∼Gh, h = 1, . . . , H,
where Gh is a shrinkage prior having high density at zero and heavy tails to avoid
over-shrinking. Gh can be a t distribution with small degrees of freedom, or a double
exponential distribution (Laplace distribution) which is related to the lasso method.
The Laplace prior induces sparsity in the posterior mode, in that the posterior mode
˜βh can be exactly equal to zero. The Laplace prior is the prior having heaviest tails
which still produces a computationally convenient uni-modal posterior density.

5.1 Aspects of Splines
125
An alternative is to use a generalized double Pareto prior distribution (Gelman
et al. 2014), which resembles the double exponential near the origin while having
arbitrarily heavy tails.
One can sample from a generalized double Pareto with scale parameter of ξ and
shape parameter of α by instead drawing βh ∼N

0, σ 2τh

, with τh ∼Exp

λ2
h/2

and λh ∼Gamma (α, αξ). Placing the prior p (σ) ∝1/σ, we then obtain a simple
block Gibbs sampler having the following full conditional posterior distributions:
β|· ∼N

XXX T XXX + TTT −1−1XXX T yyy, σ 2
XXX T XXX + TTT −1−1
σ 2|· ∼Inv-gamma

n + k
2
, (yyy −XXXβ)T (yyy −XXXβ)
2
+ βTTTT −1β
2

λh|· ∼Gamma

α + 1, |βh|
σ
+ η

, h = 1, . . . , H
τ −1
h |· ∼Inv-Gaussian

μ = λhσ
βh
, ρ = λ2
h

, h = 1, . . . , H,
where
XXX =
⎡
⎣
b1 (x1) · · · bH (x1)
· · ·
· · ·
· · ·
b1 (xn) · · · bH (xn)
⎤
⎦,TTT = Diag (τ1, · · · , τH) .
5.2
Two Simulated Examples
Now we turn to two simulated examples. It is always good to ﬁrst check our methods
using some simulated data to see whether these methods work before we go into
the more complicated application. In these two examples, even though we know
the underlying true mean function, estimating the coefﬁcients in the mean function
is not straightforward. We use smoothing splines, low rank smoothing splines and
Bayesian shrinkage splines to estimate the mean function.
The ﬁrst simulated example uses a trigonometric mean function with normal
errors. It is an example used by Faraway (2015). Here we are more interested in
prediction beyond the boundary. Besides the methods used by Faraway (2015), we
also study this example in the Bayesian framework. The second simulated example
assumes the response variable following a gamma distribution with a log-logistic
curve mean function. We design the second example to mimic the claims payment
process in general insurance. This prepares for application to real claims data in
Sect. 5.3.

126
5
Bayesian Basis Expansion Models
5.2.1
A Model with a Trigonometric Mean Function
and Normal Errors
We generate the data from the following model:
yi = sin3(2πx3
i ) + εi, i = 1, . . . , 100
xi ∼U (0, 1)
εi ∼N (0, 0.01) .
5.2.1.1
Polynomial Basis Expansion Regression Models
TheRfunction poly( )generatesanorthogonalpolynomialbasismatrixofspeciﬁed
degree at speciﬁed values. In Fig. 5.1, the ﬁrst plot shows the raw polynomial basis
of degree 4 at values from 0 to 1, where each line corresponds to a polynomial. The
second plot shows the orthogonal polynomial basis of degree 4 at values from 0 to 1,
where each line corresponds to a linear combination of polynomials, x, x2, x3, x4.
The third plot shows the orthogonal polynomial basis of degree 11 at values from 0
to 1.
We use the orthogonal polynomial basis of degrees 4, 7, and 11 to ﬁt the simulated
data. The ﬁtted lines are shown in Fig. 5.2. Note that the degrees of freedom (df)
shown in the legend box include the intercept term. None of the ﬁtted lines can
capture the shape of data adequately.
5.2.1.2
Spline Regression Models
The R function bs( ) works similarly to poly( ). It generates the B-spline basis
matrix of speciﬁed degree and knots. The number of rows of the B-spline basis matrix
is equal to the number of values to be calculated. The number of columns of B-spline
basis matrix is equal to the degrees of freedom of this spline. Here we use a cubic
B-spline with 8 knots at (0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9). So the degrees of
freedom (or equivalently the number of columns) is 12, including the intercept term.
Using the R function ns( ), we generate a natural cubic B-spline with 8 interior
knots at (0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9) and the boundary knots at the ending
points of x. A natural cubic B-spline has the property of orthogonality and linearity
beyond the boundary knots, so the degrees of freedom of this natural cubic B-spline
are 10 (i.e., 8 + 2 + 3 + 1 −4), including the intercept term.
The comparison of a normal cubic B-spline basis with a natural cubic B-spline
basis is shown in Fig. 5.3. There are 12 lines in the ﬁrst plot corresponding to 12
columns of a cubic B-spline basis matrix. Except the marginal lines, all the lines are
non-zero over the interval between 5 adjacent knots. There are 10 lines in the second
plot, corresponding to 10 columns of the natural cubic B-spline basis matrix.

5.2 Two Simulated Examples
127
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
x
Polynomials of x
0.0
0.2
0.4
0.6
0.8
1.0
−0.2
−0.1
0.0
0.1
0.2
x
Polynomials of x
0.0
0.2
0.4
0.6
0.8
1.0
−0.3
−0.2
−0.1
0.0
0.1
0.2
0.3
x
Polynomials of x
Fig. 5.1 Three polynomial basis functions in the interval [0, 1]: a raw polynomial basis of 4◦, an
orthogonal polynomial basis of 4◦and an orthogonal polynomial basis of 11◦

128
5
Bayesian Basis Expansion Models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0.0
0.5
1.0
x
y
True y = sin3(2πx3)
Polynomial fit with df=5
Polynomial fit with df=8
Polynomial fit with df=12
Fig. 5.2 The ﬁtted lines of three polynomial models with df = 5, 8, 12
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.5
1.0
x
Cubic B−spline
 basis
Knots
0.0
0.2
0.4
0.6
0.8
1.0
−0.4
0.2
0.6
x
Natural cubic spline
 basis
Interior Knots
Boundary knots
Fig. 5.3 A cubic B-spline basis and a natural cubic B-spline basis
Figure5.4showstheﬁttedlinesfromsplinebasisexpansionregressions.Thecubic
spline with 12 degrees of freedom is less wiggly than the polynomial regression with
the same degrees of freedom. This is mainly due to the local representation of spline
basis functions. However, the cubic spline spreads weirdly outside the range of data,
especially for x > 1.
The natural cubic spline regression with 10 degrees of freedom has similar per-
formance to the cubic spline within the range of data. Moreover, it has a better
extrapolation outside the range of data due to the linear constraints.

5.2 Two Simulated Examples
129
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0.0
0.5
1.0
x
y
True y = sin3(2πx3)
Cubic spline regression with df=12
Natural cubic spline regression with df=10
Smoothing spline with df=12
(Interior) knots of (natural) cubic spline
Boundary knots of natural cubic spline
Fig. 5.4 The ﬁtted lines of two spline regressions and the smoothing spline
5.2.1.3
A Full Rank Thin Plate Spline
The full rank smoothing spline is as good as the natural cubic spline since the smooth-
ing spline also puts linear constraints beyond the range of data. However, the ﬁtting
process of a smoothing spline is quite different.
A smoothing spline uses the natural basis functions with knots at every unique x
and shrink the coefﬁcients by a penalty matrix based on (5.2), while the natural cubic
spline regression does not shrink the coefﬁcients but uses the least squares estimates.
5.2.1.4
Low Rank Thin Plate Splines
Rather than using the full rank basis matrix as in a smoothing spline, Wood (2003,
2006) uses the truncated eigen-decomposition of a full rank basis matrix to achieve
a low rank smoother approximating the full rank smoother. Package mgcv can ﬁt a
low rank smoothing spline by smoothing function s. A disadvantage of this package
is that we cannot specify the degrees of freedom or the location of knots. They are
chosen automatically by generalized cross-validation criteria.
Another approach to solving the low rank smoothing spline is using a set of radial
basis functions with speciﬁed knots as in Sect. 5.1.3. Due to the equivalence of a low
rank thin plate spline to a mixed effects model, we can set up a low rank thin plate
spline model as a Bayesian mixed effects model:

130
5
Bayesian Basis Expansion Models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0.0
0.5
1.0
x
y
True
Posterior mean
95% CPDR
Knots
Fig. 5.5 A Bayesian mixed effects model using radial basis functions
yyy = XXXγ + ZZZδ + ε
δ ∼N

0, σ 2
δ KKK −1
ε ∼N

0, σ 2
ε III

,
(5.5)
where XXX [i, ] = (1, xi)T , ZZZ [i, k] = |xi −κk|3, γ = (γ0, γ1) , δ = (δ1, . . . , δK) and
KKK [l, k] = |κl −κk|3,l = 1, . . . , K, k = 1, . . . , K. Here we specify a set of 20
equally located knots spreading the range of x. We give non-informative priors for
γ, σ 2
δ , σ 2
ε . The smoothing parameter λ = σ 2
ε /σ 2
δ is not ﬁxed, and we can get the
posterior distribution of it.
We use Stan to simulate from the posterior distribution. It takes approximately 5
min to generate 1,600 iterations of which the ﬁrst half are discarded as burn-in.
The posterior mean of λ is estimated as 0.000121 compared with 0.000102
from smoothing spline ﬁt. We also plot the posterior predictive distribution for
x ∈(−0.05, 1.05) in Fig. 5.5. The estimated number of effective parameters is
pD = 17.9, pWAIC = 16.5, ploo = 17.5, which indicates around 16 degrees of free-
dom for the smoothing line (i.e., 18 minus two scale parameters, σ 2
δ and σ 2
ε ).
5.2.1.5
A Bayesian Spline Model
We apply the method in Sect. 5.1.4. A natural cubic spline basis is used with 20
equally located interior knots spreading the range of x and the boundary knots at
the ending points of xxx. We compare the goodness-of-ﬁt of three shrinkage priors:
generalized double Pareto (gdP) prior, Laplace prior (double exponential prior) and
Cauchy prior. The Stan code when using gdP prior is as follows:

5.2 Two Simulated Examples
131
1
E1. code <-"
2
data {
3
int
H;
//
Number
of
basis
functions
4
int
N;
//
Number
of
observations
5
int
n;
//
Number
of
predicted
values
6
vector [N]
y;
//
Observations
7
matrix [N,H]
basis ;
//
Basis
functions
at
observed
x
8
matrix [n,H]
basis _ hat ;//
Basis
functions
at
some
fixed
points
9
x_ hat
10
}
11
parameters {
12
vector [H]
b;
//
Parameters
of
basis
functions
13
real < lower =0>
sigmaE ;
//
Standard
deviation
of
observations
y
14
vector < lower =0 >[H]
tau ;
//
Hyperparameter
in
gdP
prior
15
vector < lower =0 >[H]
lambda ;
//
Hyperparameter
in
gdP
prior
16
}
17
transformed
parameters {
18
vector [N]
means ;
19
means <- basis *b;
20
}
21
model {
22
for
(i
in
1:H){
23
b[i]
~
normal
(0,
sigmaE * tau [i ]^0.5) ;
24
tau [i]
~
exponential
( lambda [i ]^2/2) ;
25
lambda [i]
~
gamma
(1 ,1);
26
}
27
y
~
normal
(means , sigmaE );
28
}
29
generated
quantities {
30
vector [n]
means _hat ;
31
vector [N]
log _ lik;
32
real
D;
33
means _hat <- basis _hat *b;
34
for
(i
in
1:N)
35
log _ lik [i]<- normal _ log(y[i], means [i], sigmaE );
36
D<- sum ( -2* log _ lik);
37
}"
38
funky
<-
function (x)
sin (2* pi*x ^3) ^3
39
set. seed (1) ;
x
<-
sort ( runif (100 ,0 ,1))
40
y
<-
funky (x)
+
0.1* rnorm ( length (x))
41
knots <- seq ( min (x),max (x),length . out =20) [-c (1 ,20) ]
42
basis <-ns(x, knots =knots , intercept
=
T)
#
using
the
default
43
boundary
knots
of
range
of
data
44
H<- ncol ( basis );
N<- nrow ( basis )
45
x_hat <- seq ( -0.05 ,1.05 ,0.01)
46
basis _hat <-ns(x_hat , knots =knots , intercept =T, Boundary . knots
=
47
range (x))
#
make
sure
to
use
the
same
knots
as
design
matrix
48
n<- length (x_ hat )
49
E1. stanfit <- stan ( model _ code
=
E1. code ,
data =c("H","N"," basis ","y"
50
," basis _ hat ","n") ,
iter =800 , chains =4, seed =10)
The smoothness depends on the hyperparameters in the shrinkage priors, which
can be speciﬁed as ﬁxed constants or left to be estimated from the data. We list
several information criteria in Table 5.1. Generally, all the shrinkage priors perform

132
5
Bayesian Basis Expansion Models
Table 5.1 Comparison of Bayesian spline models using different shrinkage priors in the ﬁrst
simulated example. The computing time for 4 × 800 iterations is on a PC of 6G RAM with 2.8
GHz dual CPU. We assume the scale and shape parameters for gdP prior, and assume the mean and
standard variance parameters for the Laplace prior and the Cauchy prior
Shrinkage prior
Computing time
pD
pWAIC
ploo
DIC
WAIC
LOOIC
gdP
(1, 1)
13 s
17.5
15.5
16.5
−168.5
−168.1
−166.0
(?, ?)
5 min
17.6
16.1
22.6
−171.7
−171.0
−158.0
Laplace (0, 0.1)
1 s
16.6
15.6
16.5
−168.7
−167.1
−165.3
(0, ?)
1 s
20.0
18.0
19.3
−167.9
−166.7
−164.2
Cauchy
(0, 0.1)
1 s
17.6
16.0
16.9
−169.2
−168.2
−166.5
(0, ?)
1 s
19.1
17.3
18.6
−168.3
−167.2
−164.5
Model (5.5)
5 min
17.9
16.5
17.5
−165.4
−164.1
−162.0
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●●
●
●
●●
●
●●●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●●
●
●●
●●●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
−1.0
−0.5
0.0
0.5
1.0
x
y
True
Posterior mean
95% CPDR
Knots
Fig. 5.6 A Bayesian natural cubic spline model using Cauchy (0, 0.01) prior
equally well. The ﬁtted line is not sensitive to shrinkage priors. Hence we only give
the posterior mean of the ﬁtted line with the 95% CPDR under Cauchy (0, 0.1)
shrinkage prior in Fig. 5.6. Note that the information criteria can be calculated using
the following code:

5.2 Two Simulated Examples
133
1
E1. sim <- extract (E1. stanfit , permuted =T)
2
#
loo
and
WAIC
3
loo( extract _ log _ lik(E1. stanfit ," log _lik "))
4
waic ( extract _ log _ lik(E1. stanfit ," log_ lik "))
5
#
pD
and
DIC
6
Dbar <- mean (E1. sim $D)
7
Dhat <-0
8
for
(i
in
1:N){
9
Dhat <-Dhat -2* dnorm (y[i],
basis [i ,]%*% apply (E1. sim $b ,2, mean ),
10
mean (E1. sim $ sigmaE ),log =T);
11
}
12
list ( Dhat =Dhat , Dbar =Dbar ,pD=Dbar -Dhat , DIC =2* Dbar - Dhat )
5.2.2
A Gamma Response Variable with a Log-Logistic
Growth Curve Mean Function
We assume the cumulative claims following a log-logistic growth curve, and generate
the incremental claims from a gamma distribution. More speciﬁcally, we use the
following model to generate the incremental claims:
yi j ∼Gamma

100, 100
μi j

, i = 1, . . . , 30, j = 1, . . . , 40
μi j = Pi × L Ri × (G ( j; θi, ωi) −G ( j −1; θi, ωi))
Pi = (1.00 + i × 0.01) × 106
L Ri ∼N

0.8, 0.12
θi ∼N

7.5, 0.052
ωi ∼N

2.5, 0.032
G (l; θ, ω) =
lω
lω + θω ,l = 0, . . . , 40,
where Pi is the earned premium of accident year i, L Ri is the loss ratio of accident
year i and G is a log-logistic function. Note that the earned premiums are always
available and are used as the offset later. We choose the shape parameter of the
gamma distribution to be 100, implying the coefﬁcient of variation of yi j as 0.1.
We deﬁne the cumulative claims at the end of development year j for the accident
year i as ci j =  j
l=1 yil. We assume that there is no development after 40 years since
G (40; 7.5, 2.5) = 0.985.
Suppose the evaluation time of outstanding liability is at the end of ﬁrst develop-
ment year of accident year 30. We have the triangle data set yyy =

yi, j : i + j ≤31,
i = 1, . . . , 30} available. The task is to predict the future claims up to the develop-
ment year 40, y′ =

yi, j : i + j > 31, i = 1, . . . , 30, j ≤40

. The simulated data
is plotted in Fig. 5.7.

134
5
Bayesian Basis Expansion Models
0
10
20
30
40
0.00
0.06
0.12
Development year
Incremental claims in
 million
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Available data (the upper left triangle)
To be predicted values (bottom right)
0
10
20
30
40
0.0
0.4
0.8
Development year
Cumulative claims in 
million
●
Available data (the upper left triangle)
To be predicted values (bottom right)
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Fig. 5.7 Simulated incremental and cumulative claims
In the following, we ﬁt four models: a polynomial basis expansion regression
model, a natural cubic spline regression model, a low rank smoothing spline model,
and a Bayesian shrinkage natural cubic spline model. All the models have the fol-
lowing common structure:
yi j ∼Gamma

α, α
μi j

, i = 1, . . . , 30, j = 1, . . . , 40
μi j = Pi × L Ri × exp
 H

h=1
βhbh ( j)

Pi = (1.00 + i × 0.01) × 106.
(5.6)
5.2.2.1
A Polynomial Basis Expansion Regression Model
We ﬁt a GLM with a gamma family and a logarithm link function. The offset term
is log Pi. The number of parameters is 31 + H ′, where H ′ is the degrees of freedom
of polynomial basis without intercept. H ′ is chosen according to AIC. Figure 5.8
shows that H ′ = 10 is optimal.
For this model, we make the prediction of the lower triangle and tail development
during development years 31–40. The predicted values are shown as lines and simu-
lated data of the same accident year are shown as dots in the same colour. We separate
the prediction of the lower triangle from the prediction of the tail development in
Fig. 5.9.
As in the ﬁrst simulated example, the polynomial basis expansion model cannot
make good prediction beyond the range of data.

5.2 Two Simulated Examples
135
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
9000
9500
10000
10500
H'
AIC
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
30
35
40
45
50
Equivalent degrees of freedom
Fig. 5.8 AIC versus H′ of polynomial basis expansion models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
30
0
60000
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted lower triangle
32
34
36
38
40
0
1000
2500
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted tail development
Fig. 5.9 Prediction of future claims from a polynomial basis expansion model
5.2.2.2
A Natural Cubic Spline Regression Model
We choose 5 interior knots at 2, 3, 5, 10, 20 and 2 boundary knots at 1 and 30. This
induces a 7 degrees of freedom smoothing development curve. The prediction of
future claims is shown in Fig. 5.10. The model can predict the tail development more
accurately compared with the polynomial basis expansion model.

136
5
Bayesian Basis Expansion Models
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
30
0
60000
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted lower triangle
32
34
36
38
40
0
1000
2500
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted  tail development
Fig. 5.10 Prediction of future claims from a natural cubic spline regression model
5.2.2.3
A Low Rank Thin Plate Spline
We rely on the mgcv package by Wood (2006) to ﬁt a generalized additive model
(GAM) with a low rank smoothing spline for the development year covariate. The
degrees of freedom of the smoothing spline cannot be speciﬁed but are chosen using
the criterion of generalized cross-validation. The rank reduction is achieved by a
truncated eigen-decomposition rather than the choice of knots.
The predicted lower triangle is quite close to those predicted by the previous two
models. Here we compare the tail development predictions made by the three models:
thepolynomialbasisexpansionmodel,thenaturalcubicsplinebasisexpansionmodel
and the low rank smoothing spline model. As shown in Fig. 5.11, the natural cubic
spline regression model can best capture the tail development. Next we will set up a
natural cubic spline basis expansion model in the Bayesian framework.
5.2.2.4
A Bayesian Natural Cubic Spline
In the previous simulated example, we saw that a Bayesian mixed effects model is
more computationally expensive but no better ﬁt than a Bayesian shrinkage spline
model (see Table 5.1). Here we consider only a Bayesian full rank natural cubic spline
model with shrinkage priors. An alternative is to use a ﬁxed-knot natural cubic spline
model which leads to similar prediction given the knots are chosen properly.
The Bayesian shrinkage spline model we will focus on is as follows:

5.2 Two Simulated Examples
137
32
34
36
38
40
0
500
1000
1500
2000
2500
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted tial development by polynomial expansion model
32
34
36
38
40
500
1000
1500
2000
2500
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted tial development by natural cubic spline
32
34
36
38
40
1000
2000
3000
4000
Development years
Incremental claims
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
Simulated data
Predicted tial development by GAM using 'mgcv'
Fig. 5.11 Comparison of tail development predictions by three models: a polynomial regression,
a natural cubic spline regression and a GAM

138
5
Bayesian Basis Expansion Models
yi j ∼Gamma

α, α
μi j

, i = 1, . . . , 30, j = 1, . . . , 40
μi j = Pi × L Ri × exp
 30

h=1
βhbh ( j)

βh ∼DoubleExp (0, 1) , h = 1, . . . , 30
Pi = (1.00 + i × 0.01) × 106,
(5.7)
where{bh : h = 1, . . . , 30} isasetofnaturalcubicsplinebasisfunctionswithinterior
knots placed at 2, . . . , 29 and boundary knots placed at 1 and 30.
Denote the natural cubic spline basis matrix (40 × 30) by BBB. Hence
30

h=1
βhbh ( j) = (BBBβ) [, j] ,
where β = (β1, . . . , β30). We use the double exponential (Laplace) shrinkage priors
with mean zero and variance 1. We assume non-informative priors for L Ri, α.
Model inference
We use Stan to estimate parameters and predict future claims. The code is as follows:
1
E2. code <-"
2
data {
3
int
N;
//
Number
of
obs .
4
int
H;
//
Number
of
basis
functions
5
int
n;
//
Number
of
future
values
6
int
K;
//
Number
of
accident
year
7
int
M;
//
Number
of
develop
year
8
vector [N]
inc;
//
Incremental
claims
in
upper
triangle
9
matrix [M,H]
dev_ basis ;
//
Basis
expansion
matrix
10
int
acc [N];
//
Accident
years
in
upper
triangle
11
int
dev [N];
//
Development
years
in
upper
triangle
12
vector [N]
pre ;
//
Premiums
in
upper
triangle
13
int
acc _p[n];
// Accident
years
in
lower
triangle
14
int
dev _p[n];
// Development
years
in
lower
triangle
15
vector [n]
pre_p;
// Premiums
in
lower
triangle
16
}
17
parameters {
18
vector [H]
b;
19
vector < lower =0.6 , upper =1 >[K]
ratio ;
20
real < lower =0>
alpha ;
21
}
22
transformed
parameters {
23
vector [N]
means ;
24
vector [M]
dev _ raw;
25
dev _raw <- exp ( dev _ basis
*
b);
26
for
(i
in
1:N){
27
means [i]<- pre [i]* ratio [ acc [i ]]* dev _ raw [ dev [i ]];
28
}
29
}

5.2 Two Simulated Examples
139
30
model {
31
b
~
double _ exponential (0 ,1);
32
for
(i
in
1:N){
33
inc [i]
~
gamma (alpha ,
alpha / means [i]);
34
}
35
}
36
generated
quantities {
37
vector [n]
means _p;
38
vector [N]
residuals ;
39
vector [N]
log _ lik;
40
real
D;
41
for
(i
in
1:n){
42
means _p[i]<- pre_p[i]* ratio [ acc _p[i ]]* dev _ raw [ dev _p[i ]];
43
}
44
for
(i
in
1:N){
45
residuals [i]<-( inc [i]- means [i])/ means [i]* sqrt ( alpha );
46
log _ lik [i]<- gamma _ log ( inc [i],alpha , alpha / means [i]);
47
}
48
D<- sum ( -2* log _ lik);
49
}
50
"
51
knots <-c (2:29)
52
dev_basis <-ns (1:40 , knots =knots , Boundary . knots
=
c (1 ,30) ,intercept
53
=T)
54
E2. stanfit <- stan ( model _ code
=
E2. code , data =c("N","n","H","K","M",
55
" inc "," acc "," dev "," dev _ basis "," pre "," acc _p"," dev_p"," pre _p") ,
56
iter =400 , chains =4, seed =1)
It takes 40 s for 1600 iterations. After checking convergence, we plot the posterior
mean of Pearson residuals versus the posterior mean of ﬁtted values in Fig. 5.12. Not
surprisingly, it shows a randomly spread, since the gamma distribution assumption
is the same as the underlying error structure generating the data.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0e+00
2e+04
4e+04
6e+04
8e+04
1e+05
−2
−1
0
1
2
Posterior mean of fitted values
Posterior mean of Pearson residuals
Fig. 5.12 The residual plot of a Bayesian natural cubic spline model

140
5
Bayesian Basis Expansion Models
0
10
20
30
40
0.00
0.02
0.04
0.06
0.08
0.10
Development years
Incremental payment proportion
True underlying
Posterior mean
95% CPDR
Fig. 5.13 Proportions of the incremental claims to the ultimate claims
The posterior mean and the 95% CPDR of the proportion of the incremental
claims to the ultimate claims (i.e., the term exp
30
h=1 βhbh ( j)

in Eq. (5.7)) is
shown in Fig. 5.13. The posterior mean is close to the true underlying log-logistic
curve. The 95% CPDR covers most of the true underlying curve. As we expected,
the 95% CPDR spreads out after development year 30, since there are no data after
development year 30.
We plot the posterior distribution of cumulative claims up to development year 40
for 9 accident years shown in Fig. 5.14. The ultimate claims distributions are posi-
tively skewed due to the assumption of gamma likelihood. The posterior distribution
of total outstanding unpaid claims liability is plotted in Fig. 5.15. We also plot the
result using a Cauchy shrinkage prior for comparison in Fig. 5.15. Both models lead
to similar posterior distributions that are positively skewed.
Advantages of using a Bayesian model
An important advantage of Bayesian modelling is the ability to evaluate the uncer-
tainty via simulation from the posterior distribution. Frequentist models typically use
the asymptotic property of parameters under resampling to estimate the uncertainty
associated with parameters and future values. This method becomes problematic for
some complicated functions of direct predictions.
For the claims reserving problem, the response variable is the incremental claims,
but our interest is the cumulative claims whose uncertainty is difﬁcult to estimate. The
bootstrap method can tackle this task through resampling residuals and generating
the pseudo-data. In the Bayesian framework, we use MCMC or HMC to simulate
the joint posterior distribution of parameters and perform a further step to generate
the future claims. Essentially, the distribution of any functions of response variable
can be simulated through this process.

5.2 Two Simulated Examples
141
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 4
●
●
Simulated past data
Simulated future data
Posterior interval
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 7
●
●
Simulated past data
Simulated future data
Posterior interval
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 10
●
●
Simulated past data
Simulated future data
Posterior interval
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 13
●
●
Simulated past data
Simulated future data
Posterior interval
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 16
●
●
Simulated past data
Simulated future data
Posterior interval
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 19
●
●
Simulated past data
Simulated future data
Posterior interval
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 22
●
●
Simulated past data
Simulated future data
Posterior interval
0.0
0.2
0.4
0.6
0.8
1.0
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 25
●
●
Simulated past data
Simulated future data
Posterior interval
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
1.2
0
10
20
30
40
0
10
20
30
40
0.0
0.2
0.4
0.6
0.8
1.0
Development years
Cumulative claims in millions
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●●●●●●●●●●●●●●●●●●●●●●●●
Accident year 28
●
●
Simulated past data
Simulated future data
Posterior interval
Fig. 5.14 The predictive distributions of cumulative claims for 9 accident years

142
5
Bayesian Basis Expansion Models
Outstanding unpaid claims in millions (using Laplace shrinkage prior)
Density
7
8
9
10
11
12
0.0
0.5
1.0
1.5
●
●Posterior mean
95% CPDR
Min and Max
Outstanding unpaid claims in millions (using Cauchy shrinkage prior)
Density
7
8
9
10
11
12
0.0
0.5
1.0
1.5
●
●Posterior mean
95% CPDR
Min and Max
Fig. 5.15 The predictive distribution of the total outstanding liability using different shrinkage
priors

5.2 Two Simulated Examples
143
Table 5.2 Comparison of Bayesian spline models using different shrinkage priors in the second
simulated example. The computing time for 4 × 800 iterations is on a PC of 6G RAM with 2.8
GHz dual CPU. We assume the scale and shape parameters for gdP prior, and assume the mean and
standard variance parameters for the Laplace prior and the Cauchy prior
Shrinkage prior
Computing time (s)
pD
pWAIC
ploo
DIC
WAIC
LOOIC
Laplace (0, 1)
35
57.1
51.6
52.8
8783.8
8784.4
8786.8
(0, ?)
35
55.7
49.8
51.1
8780.5
8780.3
8782.9
Cauchy (0, 1)
34
58.2
51.9
53.4
8786.0
8785.9
8789.0
(0, ?)
34
57.3
51.2
52.7
8784.1
8784.1
8787.1
Model selection
Finally, we compare four models in terms of the three information criteria discussed
in Sect. 2.2. As shown in Table 5.2, these four models have similar goodness-of-ﬁt
values. The differences are mainly due to the randomness.
5.3
Application to the Doctor Beneﬁt
In the previous chapter, the analysis of doctor beneﬁt did not accommodate the tail
development. While all the claims seem to be reported by the development year 27,
the beneﬁt payments seem to continue beyond the development year 27. So we need
to consider the tail development of PPCI.
Abasis expansionmodel is appliedtoextrapolatethetail development. Thenatural
cubic spline is at the top of our option list, since it comes from an optimal problem
and has the linear constraint beyond the boundary knots.
As in the previous chapter, we have three steps to ﬁt a compound model. The
ﬁrst step is to ﬁt a Bayesian natural cubic spline model to the claims numbers. The
posterior mean of ultimate claims number is used to calculate the PPCI triangle.
Next, we ﬁt a Bayesian natural cubic spline model to the PPCI triangle to get the
posterior distribution of outstanding PPCI. The payments are assumed to continue
up to the development year 30. Finally, we apply a compound model to combine the
ultimate claims numbers with the outstanding PPCI to get the claims liability.
5.3.1
Claims Numbers
A Bayesian natural cubic spline model with Cauchy shrinkage priors and a gamma
distribution is ﬁtted to the claims numbers triangle. The boundary knots are placed
at the ﬁrst and last available development years, i.e., the development years 1 and
27. The development years 2–26 are interior knots. The basis matrix for prediction
must use the same knots. The Stan code is as follows:

144
5
Bayesian Basis Expansion Models
1
number .code <-"
2
data {
3
int
N;
//
Number
of
observations
4
int
n;
//
Number
of
future
values
5
int
K;
//
Number
of
accident
years
6
int
M;
//
Number
of
develop
years
(27)
7
int
H;
//
Number
of
basis
functions
8
vector [N]
first _ inc ;
//
Number
of
claims
in
upper
triangle
9
matrix [M,H]
dev _ basis ;
//
The
basis
functions
10
int
acc [N];
//
Accident
years
in
upper
triangle
11
int
dev [N];
//
Development
years
in
lower
triangle
12
int
acc _p[n];
//
Accident
years
in
lower
triangle
13
int
dev _p[n];
//
Development
years
in
lower
triangle
14
}
15
parameters {
16
vector [H]
b;
17
vector < lower =0, upper =55000 >[ K]
ult ;
18
real < lower =0>
alpha ;
19
real < lower =0>
sigma ;
20
}
21
transformed
parameters {
22
vector [N]
means ;
23
vector [M]
dev _ raw;
24
vector [M]
dev _ norm ;
25
dev _raw <- exp ( dev _ basis
*
b);
26
dev _norm <- dev _ raw/ sum ( dev _ raw );
27
for
(i
in
1:N){
28
means [i]<- ult [acc [i ]]* dev _ norm [dev [i ]];
29
}
30
}
31
model {
32
b
~
cauchy (0, sigma );
33
for
(i
in
1:N){
34
first _ inc [i]
~
gamma (alpha ,
alpha / means [i]);
35
}
36
for
(i
in
25:27)
37
ult [i]
~
normal (20000 ,2000) ;
38
}
39
generated
quantities {
40
vector [n]
means _p;
41
vector [N]
residuals ;
42
vector [N]
log _ lik;
43
real
D;
44
for
(i
in
1:n){
45
means _p[i]<- ult[ acc _p[i ]]* dev _ norm [ dev _p[i ]];
46
}
47
for
(i
in
1:N){
48
residuals [i]<-( first _ inc [i]- means [i])/ means [i]* sqrt ( alpha );
49
log _ lik [i]<- gamma _ log ( first _ inc [i],alpha , alpha / means [i]);
50
}
51
D<- sum ( -2* log _ lik);
52
}"
53
M < -27;
knots <-c (2:26)
54
dev_basis <-ns(c (1: M),knots =knots , Boundary . knots
=
c (1 ,27) ,
55
intercept
=
T)
56
number . stanfit <- stan ( model _ code
=
number .code ,
data =c(" first _ inc "
57
,"N","n","K","M","H"," acc "," dev "," acc_p"," dev _p"," dev_ basis ") ,
58
iter =800 , chains =4, seed =1)

5.3 Application to the Doctor Beneﬁt
145
0
5
10
15
20
25
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Development years
Incremental proportion of number of
 reported claims
Posterior mean
95% CPDR
Fig. 5.16 Proportions of incremental claims numbers to ultimate claims numbers
The residual plot shows a quite similar pattern to Fig. 4.17 so we did not present
it here. The posterior mean and the 95% CPDR for the proportion of incremental
reported claims to the ultimate claims numbers are plotted in Fig. 5.16. It shows that
nearly all the claims are reported by the development year 3, hence the assumption
of no tail development after development year 27 is reasonable.
We plot the posterior distributions of cumulative claims numbers for the accident
years 8, 10, 12, 14, 16, 18, 20, 22 and 24 in Fig. 5.17. It shows that the ultimate
claims numbers for the older accident years can be estimated more accurately. For
the recent accident years, the large uncertainties in the ﬁrst three development years
are carried forward to the ultimate claims numbers. We use the posterior mean of the
ultimate claims number as a proxy to derive the PPCI triangle.
5.3.2
PPCI
Similar to the claims numbers, we ﬁt a natural cubic spline model with Cauchy
shrinkage priors to the PPCI triangle. The choice of knots is the same as for the
claims numbers, and we assume the payments are ﬁnalized by the development year
30. The Stan code is similar to number.code, but with the following changes:
1
M < -30;
knots <-c (2:26)
2
dev_basis <-ns(c (1: M),knots =knots , Boundary . knots
=
c (1 ,27) ,
3
intercept
=
T)

146
5
Bayesian Basis Expansion Models
Development years
Cumulative number of claims
●
●
●●●●●●●●●●●●●●●●●●
Accident year 8
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●●●●●●●●●●●●●●●
Accident year 10
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●●●●●●●●●●●●●
Accident year 12
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●
●●●●●●●●●
Accident year 16
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●●●●●●●
Accident year 18
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●●●●●
Accident year 20
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●●●
Accident year 22
●
Data
Posterior interval
Development years
Cumulative number of claims
●
●
●●
Accident year 24
●
Data
Posterior interval
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
0
5
10
15
20
25
0
5000
10000
15000
20000
25000
30000
Development years
Cumulative number of claims
●
●
Accident year 26
●
Data
Posterior interval
Fig. 5.17 The predictive distributions of cumulative claims numbers for 9 accident years

5.3 Application to the Doctor Beneﬁt
147
0
5
10
15
20
25
30
0.00
0.05
0.10
0.15
0.20
0.25
Development years
Incremental proportion of PPCI
Posterior mean
95% CPDR
Fig. 5.18 Proportions of the incremental PPCI’s to the ultimate PPCI’s
The posterior inference of the proportion of incremental PPCI to the ultimate
PPCI is shown in Fig. 5.18. The 95% CPDR spreads out in the tail area due to the
lack of data. The development of PPCI for accident years 8, 10, 12, 14, 16, 18, 20,
22 and 24 is plotted in Fig. 5.19. As expected, less developed accident years show
more variation.
Here we saw the advantage of the basis expansion model compared with model
(4.7). Model (4.7) separates the development curve into two parts: the ﬁrst few devel-
opment years, characterized by a factor covariate, and the last mature development
years, characterized by an exponential curve. The RJMCMC method is used to sim-
ulate from the posterior distribution, which is a joint distribution of the model index
and parameters. By using a basis expansion model, only one model is focused and
non-signiﬁcant coefﬁcients are shrunk to zero.
5.3.3
Combining the Ultimate Claims Numbers with the
Outstanding PPCI
A compound model discussed in the previous chapter (see Sect. 4.4) is applied to
calculate the posterior distribution of total outstanding claims liability as shown
in Fig. 5.20. Table 5.3 lists the predictions made from the compound model. The
posterior mean of total outstanding liability is 419,770,032 dollars (7% higher than
inthepreviouschapter)withstandardvarianceof10,492,327dollars.The95%CPDR

148
5
Bayesian Basis Expansion Models
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●●●●●●●●●●●●●
Accident year 8
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●
●
●●●●●●●●●
Accident year 10
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●
●
●●●●●●●
Accident year 12
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●
●
●●●
Accident year 16
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●
●
●
Accident year 18
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
●
●
Accident year 20
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
●
●
Accident year 22
●
Data
Posterior interval
Development years
Cumulative PPCI
●
●
●
●
Accident year 24
●
Data
Posterior interval
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
0
5
10
15
20
25
30
0
1000
2000
3000
4000
5000
Development years
Cumulative PPCI
●
●
Accident year 26
●
Data
Posterior interval
Fig. 5.19 The predictive distributions of cumulative PPCI’s for 9 accident years

5.3 Application to the Doctor Beneﬁt
149
Outstanding liability in millions
Density
400
420
440
460
480
0.000
0.010
0.020
0.030
●
●Posterior mean
95% CPDR
Min and Max
Fig. 5.20 The predictive distribution of total outstanding claims liability of the doctor beneﬁt
Table 5.3 The predictions made from the compound model for the doctor beneﬁt
Post. mean
Std. deviation
95% CPDR
O/S claims no.
13,693
2,397
(9,846, 19,060)
O/S PPCI
18,320
386
(17,548, 19,059)
O/S liability
419,770,032
10,492,327
(401,778,990, 442,281,893)
Table 5.4 The outstanding claims liability estimates of the doctor beneﬁt from different models
Model
Post. mean
Std. deviation
95% CPDR
Previous chapter
391,761,803
10,195,111
(373,902,941, 414,549,267)
This chapter
419,770,032
10,492,327
(401,778,990, 442,281,893)
PwC
396,827,792
NA
NA
is (401,778,990, 442,281,893). These estimates should be compared with those from
the previous chapter in Table 5.4.
5.3.4
Computing Time
Finally, we point out that the computing time for the Bayesian basis expansion model
is much less than for the Bayesian chain ladder model in the previous chapter, since

150
5
Bayesian Basis Expansion Models
Table 5.5 Comparison of the computing times for the Bayesian chain ladder model and the
Bayesian spline model. The computing time is on a Mac of 4 GB 1600 MHz DDR3 with 1.3
GHz Intel Core i5
Model
Response variable
Iterations
Computing time (s)
Bayesian chain ladder
Claims no.
4 × 400
86
PPCI
4 × 400
364
Bayesian basis expansion
Claims no.
4 × 800
73
PPCI
4 × 800
65
we use the orthogonal basis functions in the basis expansion model. The computing
times for the models used in this section and for those used in Sect. 4.4.3 are displayed
in Table 5.5.
5.4
Discussion
To the best of our knowledge of the actuarial science literature, the contribution of
this chapter is to introduce a Bayesian basis expansion model to the claims reserving
problem. Compared with a stochastic chain ladder model, a Bayesian basis expan-
sion model has the advantages of reducing the number of parameters via shrinkage
priors and incorporating the tail factor via interpolation. Due to the orthogonality of
basis functions, the running time of MCMC is largely reduced. Unlike a non-linear
curve model, a Bayesian basis expansion model can accommodate all the shapes of
data. Hence, the Bayesian basis expansion model is one of the most powerful tools
according to our research.
This chapter considers the basis expansion of the development year covariate,
and it is typically enough for the claims reserving problem. We can address the non-
linear effect of both accident years and development years simultaneously (and their
interaction) using the multivariate adaptive regression splines (MARS), see Hastie
et al. (2009) and Section 3.2 of Wüthrich and Buser (2018). Another related work
is Gabrielli et al. (2018) which embeds the MLEs from GLM into a neural network.
Further research can consider the basis expansion of two or more covariates, which
is more common in the insurance rating problem.
Finally, we point out a problem in Fig. 5.18. From a statistical point of view,
since there is no data in the tail development, more variability is expected. However,
from an actuarial point of view, the claims paid in the tail development period should
be subjected to less variability since almost all the claims have been closed by this
period. We do expect less variation associated with the tail development. To realize
this expectation, a strong prior for the tail development can be assumed to limit its
posterior variability. This method will be applied in the next chapter (see Figs. 6.12
and 6.13). This is a situation when the actuarial judgements override the data.

5.5 Bibliographic Notes
151
5.5
Bibliographic Notes
There are several books covering the topic of spline models: Hastie and Tibshirani
(1990), Ruppert et al. (2003), Wood (2006), Hastie et al. (2009) and James et al.
(2013).
Wood (2003) discusses low rank thin plate splines. Ruppert (2012) discusses
selecting the number of knots. DiMatteo et al. (2001) apply RJMCMC to allocate
the knots. Crainiceanu et al. (2005) ﬁt a penalized spline model via WinBUGS and
give several examples. Hall and Opsomer (2005) give some theoretical properties
of penalized spline regression. Lay (2012) is an excellent reference book for matrix
concepts such as orthogonality, rank, basis etc.
Bishop (2006) provides a useful review of basis function models. Park and Casella
(2008) discuss inference using the Laplace prior distribution. References on gen-
eralized double Pareto shrinkage include Armagan et al. (2013). Komaki (2006)
investigates the shrinkage predictive distributions based on vague priors.
There is little literature about non-parametric claims reserving models. England
and Verrall (2001) apply the generalized additive model. Zhang and Dukic (2013)
apply a semi-parametric Bayesian model proposed by Crainiceanu et al. (2005). Gao
and Meng (2018) propose the Bayesian basis expansion claims reserving model.
References
Armagan, A., Dunson, D. B., & Lee, J. (2013). Generalized double Pareto shrinkage. Statistica
Sinica, 23, 119–143.
Bishop, C. M. (2006). Pattern recognition and machine learning. New York: Springer.
Crainiceanu, C. M., Ruppert, D., & Wand, M. P. (2005). Bayesian analysis for penalized spline
regression using WinBUGS. Journal of Statistical Software, 14, 1–14.
DiMatteo, I., Genovese, C. R., & Kass, R. E. (2001). Bayesian curve-ﬁtting with free-knot splines.
Biometrika, 88, 1055–1071.
England, P. D., & Verrall, R. J. (2001). A ﬂexible framework for stochastic claims reserving.
Proceedings of the Casualty Actuarial Society, 88, 1–38.
Faraway, J. J. (2015). Linear models with R (2nd ed.). Boca Raton: Chapman & Hall.
Gao, G., & Meng, S. (2018). Stochastic claims reserving via a bayesian spline model with random
loss ratio effects. ASTIN Bulletin, 48, 55–88.
Gabrielli, A., Richman, R., & Wüthrich, M. V. (2018). Neural network embedding of the over-
dispersed Poisson reserving model. SSRN, ID 3288454.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2014). Bayesian data analysis (3rd ed.).
Boca Raton: Chapman & Hall.
Hall, P., & Opsomer, J. D. (2005). Theory for penalised spline regression. Biometrika, 92, 105–118.
Hastie, T. J., Tibshirani, R. J., & Friedman, J. H. (2009). The elements of statistical learning: Data
mining, inference, and prediction (2nd ed.). New York: Springer.
Hastie, T. J., & Tibshirani, R. J. (1990). Generalized additive models. New York: Chapman & Hall.
James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning.
New York: Springer.
Komaki, F. (2006). Shrinkage priors for Bayesian prediction. The Annals of Statistics, 34, 808–819.
Lay, D. C. (2012). Linear algebra and its application (4th ed.). Boston: Addison Wesley.

152
5
Bayesian Basis Expansion Models
Park, T., & Casella, G. (2008). The Bayesian lasso. Journal of the American Statistical Association,
103, 681–686.
Ruppert, D. (2012). Selecting the number of knots for penalized splines. Journal of Computational
and Graphical Statistics, 11, 735–757.
Ruppert, D., Wand, M. P., & Carroll, R. J. (2003). Semiparametric regression. New York: Cambridge
University Press.
Wood, S. N. (2003). Thin plate regression splines. Journal of the Royal Statistical Society B, 65,
95–114.
Wood, S. (2006). Generalized additive models: An introduction with R. New York: Chapman &
Hall.
Wüthrich, M. V., & Buser, C. (2018). Data analytics for non-life insurance pricing. SSRN, ID
2870308.
Zhang, Y. W., & Dukic, V. (2013). Predicting multivariate insurance loss payments under the
Bayesian copula framework. Journal of Risk and Insurance, 80, 891–919.

Chapter 6
Multivariate Modelling Using Copulas
Abstract Copulas are a family of multivariate distributions whose marginal distri-
butions are uniform. At the end of reserving problems, we need to aggregate the
outstanding liability distribution of each line of business or each type of beneﬁt to
get the total outstanding liability distribution. The dependence between them must
be considered. In the Bayesian copulas framework, all the uncertainties and correla-
tions are considered during the inferential process which is an advantage compared
with the likelihood-based frequentist inference. In Sect. 6.1, the elements of copulas
are reviewed, including Sklar’s theorem, parametric copulas, inference methods, etc.
In Sect. 6.2, we discuss the usefulness of copulas in risk modelling generally. The
copula is used to model the empirical dependence between risks while the marginal
regression model is used to model the structural dependence. In Sect. 6.3, a bivari-
ate Gaussian copula is used to aggregate the liabilities of the doctor beneﬁt and the
hospital beneﬁt in WorkSafe Victoria. These two beneﬁts are correlated positively
even after removing the structural effects of the development periods.
6.1
Overview of Copulas
All the models we discussed before are univariate models, i.e., there is one response.
However, for many applications, it is more appropriate to apply a multivariate model
which captures important relationships. Property damage lines could be positively
correlated, e.g., homeowners property damage insurance and personal auto damage
insurance could be hit at the same time in catastrophic events. Liability lines could
be positively correlated due to changes in litigation. It is important to consider the
impacts of correlation between lines or beneﬁts on the distribution of aggregated
liability.
Typical multivariate distributions include multivariate Gaussian distribution, mul-
tivariate t-distribution, Wishart distribution etc. These multivariate distributions also
determine the marginal distributions. Copulas are a family of multivariate distri-
butions whose marginal distributions are uniform. In this section, we summarize
the elements of copulas in four parts: the mechanism of copulas to join arbitrary
marginal distributions, two copula families, measures of bivariate association, and
the inferential methods.
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_6
153

154
6
Multivariate Modelling Using Copulas
6.1.1
Sklar’s Theorem
Sklar’stheorem (Sklar1959)isperhapsthemostimportantresultregardingcopulas.It
establishes the general connection between any multivariate distribution and copulas
and is used essentially in all copula applications. Sklar’s theorem states that for
an m-dimensional multivariate distribution function F with marginal distributions,
F1, . . . , Fm, there always exists an m-dimensional copula C such that
F (y1, . . . , ym) = C [F1 (y1) , . . . , Fm (ym)] .
Conversely, if C is an m-dimensional copula and F1, . . . , Fm are distribution func-
tions, then the function F deﬁned above is an m-dimensional multivariate distribution
function with marginal distribution functions, F1, . . . , Fm.
From Sklar’s theorem, we see that for any multivariate distributions, the marginal
distributions can be separated from the multivariate dependence which can then be
represented by a copula. A direct implication of Sklar’s theorem is deriving a copula
from a multivariate distribution as follows:
C (u1, . . . , um) = F

F−1
1
(u1) , . . . , F−1
m (um)

,
where u1, . . . , um follow marginal uniform distributions on the interval [0, 1].
6.1.1.1
Invariance to Monotone Transformation
While a joint distribution is affected by the monotone transformation of variables, a
copula is invariant to the monotone transformation of variables. Let (y1, . . . , ym) be a
vector of continuous random variables with a copula C. Deﬁne x1 = h1 (y1) , . . . , xm
= hm (ym). If h1, . . . , hm are strictly increasing functions, then (x1, . . . , xm) also has
the same copula C.
6.1.1.2
The Fréchet-Hoeffding Bounds for Bivariate Copulas
Fréchet(1935)foundthatanybivariatecopulaC isboundedbytheFréchet-Hoeffding
lower bound W and the Fréchet-Hoeffding upper bound M, as follows:
W (u1, u2) ≤C (u1, u2) ≤M (u1, u2) ,
where W (u1, u2) = max (u1 + u2 −1, 0) , M (u1, u2) = min (u1, u2). Figure 6.1
shows the surfaces and contours of W and M compared with the independent copula
whose variables are independent with each other.

6.1 Overview of Copulas
155
u1
u2
C(u1,u2)
Independent copula
u1
u2
C(u1,u2)
Minimum copula
u1
u2
C(u1,u2)
Maximum copula
Contour of Independent copula
u1
u2
 0.1 
 0.2 
 0.3 
 0.4 
 0.5 
 0.6 
 0.7 
 0.8 
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of Minimum copula
u1
u2
 0.1 
 0.2 
 0.2 
 0.2 
 0.3 
 0.4 
 0.5 
 0.6 
 0.7 
 0.8 
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of Maximum copula
u1
u2
 0.1 
 0.2 
 0.3 
 0.4 
 0.5 
 0.6 
 0.7 
 0.8 
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Fig. 6.1 The surfaces and contour plots of the independent, minimum, and maximum copulas
6.1.2
Parametric Copulas
We will investigate two parametric copula families: elliptical copulas and Archi-
medean copulas. Elliptical copulas are simply the copulas of elliptical distributions
such as multivariate Gaussian distribution and multivariate t-distribution.
Rather than deriving from multivariate distributions, Archimedean copulas are
functions of a convex generator and the dependence strength is governed by only
oneparameter.ArchimedeancopulasincludetheClayton,Gumbel,Frank,andothers.
6.1.2.1
Elliptical Copulas
The copula of an m-dimensional normal distributed random vector z with mean zero
and correlation matrix  is
C (u) = m

−1 (u1) , . . . , −1 (um) ;

,
where −1 is the inverse of the standard normal distribution function and m is
the joint distribution function of z. The connection between elliptical copulas and
elliptical distributions provides an easy way to simulate from elliptical copulas: ﬁrst
simulate z ∼m, then let ui = −1 (zi) for i = 1, . . . , m.
The copula of an m-dimensional t-distributed random vector x with mean zero,
degrees of freedom ν and correlation matrix  is

156
6
Multivariate Modelling Using Copulas
u1
u2
C(u1,u2)
Normal copula
u1
u2
C(u1,u2)
t−copula with df=1
u1
u2
C(u1,u2)
t−copula with df=10
u1
u2
c(u1,u2)
Normal copula density
u1
u2
c(u1,u2)
t−copula density with df=1
u1
u2
c(u1,u2)
t−copula density with df=10
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of normal copula
u1
u2
 0.4 
 0.4 
 0.6 
 0.6 
 0.8 
 0.8 
 1 
 1 
 1.2 
 1.2 
 1.4 
 1.4 
 2 
 2 
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of t−copula (df=1)
u1
u2
 0.5 
 0.5 
 1 
 1 
 1.5 
 1.5 
 2 
 2 
 2.5 
 2.5 
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of t−copula(df=10)
u1
u2
 0.4 
 0.4 
 0.6 
 0.6 
 0.8 
 0.8 
 1 
 1 
 1.2 
 1.2 
 1.4 
 1.4 
 1.6 
 1.6 
Fig. 6.2 A bivariate Gaussian copula and t-copulas with df = 1, 10, which have the same Pearson
correlation of 0.8 and Kendall’s tau of 0.5903
C (u) = tm,ν

t−1
ν
(u1) , . . . , t−1
ν
(um) ;

,
where t−1
ν
is the inverse of the t-distribution function with ν degrees of freedom and
tm,ν is the joint distribution function of x.
Figure 6.2 shows a bivariate Gaussian copula and a bivariate t-copula, both of
which have the same Pearson correlation of 0.8 and Kendall’s tau of 0.5903 (deﬁned
in Sect. 6.1.3). Kendall’s tau of a t-copula does not depend on the degrees of freedom.
With the degrees of freedom increasing, a t-copula approaches a normal copula.

6.1 Overview of Copulas
157
Table 6.1 The generators, Kendall’s tau and tail dependence for two elliptical copulas and three
Archimedean copulas
Copula
Generator
Kendall’s tau
Upper
Lower
Gaussian
NA
2 arcsin (12) /π
0
0
t
NA
2 arcsin (12) /π
0
0
Clayton
1
θ

u−θ −1

θ/(θ + 2)
0
2−1/θ
Gumbel
(−log u)θ
1 −1/θ
2 −21/θ
0
Frank
−log

exp(−θu)−1
exp(−θ)−1

1 −4θ−4
 a
0 t/(et−1)dt
θ2
0
0
6.1.2.2
Archimedean Copulas
A general deﬁnition of Archimedean copulas can be found in Nelsen (2013). An
Archimedean m-dimensional copula has the following form:
C (u) = ϕ[−1] [ϕ (u1; θ) + · · · + ϕ (um; θ) ; θ] ,
where ϕ is called the generator of copula C and ϕ[−1] is the pseudo-inverse of ϕ.
The function ϕ is a continuous, strictly decreasing convex function mapping from
[0, 1] to [0, ∞], such that ϕ (1) = 0.
Table 6.1 shows the generators of three popular Archimedean copulas. We plot the
cumulative distribution functions, the probability density functions and the contours
of probability density functions for the three Archimedean copulas in Fig. 6.3.
6.1.3
Measures of Bivariate Association
Copulas are invariant under monotone transformation, so we want the measures of
association to also be invariant to monotone transformation. Pearson correlation (or
linear correlation) is invariant under linear transformation but not invariant under
non-linear transformation.
In the following we will review two measures of association known as Kendall’s
tau and Spearman’s rho, both of which depend on the variable ranks rather than their
values (and hence are invariant under monotone transformations).
Moreover, we will discuss the tail dependence relating to the amount of depen-
dence in the upper-right-quadrant tail or lower-left-quadrant tail of a bivariate distri-
bution. It turns out that tail dependence is also a copula-based association measure
that is invariant under monotone transformations.

158
6
Multivariate Modelling Using Copulas
u1
u2
C(u1,u2)
Clayton copula
u1
u2
C(u1,u2)
Gumbel copula
u1
u2
C(u1,u2)
Frank copula
u1
u2
c(u1,u2)
Clayton copula density
u1
u2
c(u1,u2)
Gumbel copula density
u1
u2
c(u1,u2)
Frank copula density
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of Clayton density
u1
u2
 0.5 
 0.5 
 1 
 1 
 1.5 
 1.5 
 2 
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of Gumbel density
u1
u2
 0.5 
 0.5 
 1 
 1 
 1.5 
 1.5 
 2 
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Contour of Frank density
u1
u2
 0.4 
 0.4 
 0.6 
 0.6 
 0.8 
 0.8 
 1 
 1 
 1.2 
 1.2 
 1.4 
 1.4 
 1.6 
 1.6 
 2 
Fig. 6.3 Clayton, Gumbel and Frank copulas with the same Kendall’s tau of 0.5903
6.1.3.1
Kendall’s Tau and Spearman’s Rho
Kendall’s tau for two random variables is deﬁned as the probability of concordance
minus the probability of discordance. Assuming the two variables y1, y2 have a
copula C, then Kendall’s tau for y1, y2 is given by
τ (y1, y2) := 4
		
[0,1]2 C (u1, u2) dC (u1, u2) −1 = 4E [C (u1, u2)] −1.
Spearman’s rho for y1, y2 is given by
ρS (y1, y2) = 12
		
[0,1]2 u1u2dC (u1, u2) −3 = 12E (u1u2) −3.

6.1 Overview of Copulas
159
If the marginal distributions are F1 and F2, and u1 = F1 (y1) and u2 = F2 (y2), then
ρS (y1, y2) = E (u1u2) −1/4
1/12
=
Cov (u1, u2)
√Var (u1)√Var (u2) = ρ (F1 (y1) , F2 (y2)) .
Table 6.1 lists Kendall’s tau for two elliptical copulas and three Archimedean copulas
discussed before. Figure 6.3 shows three Archimedean copulas, all of which have
the same Kendall’s tau of 0.5903.
6.1.3.2
Tail Dependence
The coefﬁcient of upper tail dependence of the two variables y1, y2 with the copula
C is deﬁned as
λU := lim
u→1 Pr

y2 > F−1
2
(u) |y1 > F−1
1
(u)

.
It can be shown that λU is a copula property which has the following equivalent
form:
λU = lim
u→1
1 −2u + C (u, u)
1 −u
.
The coefﬁcient of lower tail dependence λL is deﬁned in a similar way:
λL := lim
u→0
C (u, u)
u
.
Table 6.1 lists the coefﬁcients of upper and lower tail dependence for bivariate cop-
ulas. None of the copulas exhibit tail dependence except the Clayton copula and the
Gumbel copula. The Clayton copula has a lower tail dependence while the Gumbel
copula has an upper tail dependence.
6.1.4
Inference Methods for Copulas
In this section, we follow the model speciﬁcation as in Pitt et al. (2006). Consider
an m-element response variable y = (y1, . . . , ym). It is observed for n times, so the
data is
yyy = (y1, . . . , yn) =

(y11, . . . , y1m)T , . . . , (yn1, . . . , ynm)T 
= (yyy1, . . . , yyym)T ,
where yi is an m-row-vector of the ith observation, yyy j is an n-column-vector of the
jth response variable.
For the jth element in the ith observation yi j, we have a k-vector covariate xi j.
Marginally, we ﬁt a generalized linear model Fj to y j. We denote the associated

160
6
Multivariate Modelling Using Copulas
parameters as θ j =

β j, ϕ j

, where β j is a k-vector of coefﬁcients of xi j and ϕ j is
a vector of all the other parameters in Fj.
The joint distribution of the ith observation yi = (yi1, . . . , yim) is modelled by a
copula with parameters θc as follows:
F (yi) = C [F1 (yi1) , . . . , Fm (yim) ; θc] ,
(6.1)
which can be seen as a joint distribution of residual ranks of response variables (after
removing the systematic effects of covariates). In a Gaussian copula setting, we can
write the above copula as
m{−1 [F1 (yi1)] , . . . , −1 [Fm (yim)] ;},
where −1 is the inverse of a standard normal distribution function and m is an
m-multivariate Gaussian distribution function with mean zero.
In the following, we discuss two likelihood-based estimations: the maximum like-
lihood estimation (MLE) and the inference functions for margins estimator (IFME).
Bootstrap methods and MCMC methods can be applied to estimate the estimation
error and the prediction error in IFME.
6.1.4.1
Maximum Likelihood Estimation (MLE)
The density function of yi is the derivative of Eq. (6.1), as follows:
f (yi) = ∂mC [F1 (yi1) , . . . , Fm (yim)]
∂yi1 . . . ∂yim
= c [F1 (yi1) , . . . , Fm (yim)] f1 (yi1) · · · fm (yim) ,
where c is the density function of C and fi is the density function of yi. The likelihood
function of yyy = (y1, . . . , yn) is
L (θ; yyy) =
n
i=1
c [F1 (yi1) , . . . , Fm (yim)]
m

j=1
n
i=1
f j

yi j

.
The MLE is then deﬁned as
ˆθMLE = argmax
θ
L (θ; yyy) .
Note that the optimization of global likelihood can be quite demanding since the cop-
ula likelihood part also contains marginal regression parameters θ j, j = 1, . . . , m.

6.1 Overview of Copulas
161
6.1.4.2
Inference Functions for Margins Estimator (IFME)
Joe (2014) suggested ﬁrst estimating θ j for each jth marginal regression model, and
then estimating the copula parameter θc via
ˆθIFME
c
= argmax
θc
n
i=1
c

F1

yi1; ˆθ1

, . . . , Fm

yim; ˆθm

; θc

,
where ˆθ j, j = 1, . . . , m are the MLEs of the marginal models. IFME is always easier
to compute than the global MLE.
Predictive distributions via parametric bootstrap
Suppose we want to get the predictive distribution of R = g

yn+1,1, . . . , yn+1,m

given the covariates xn+1 = (xn+1,1, . . . , xn+1,m), where g is a generic function. The
bootstrap algorithm is as follows:
1. Fit a marginal regression to yyy j to get the estimated parameters ˆθ j for j = 1, . . . m.
2. Calculate the cdfs given the estimated parameters in step 1 as
ˆui j = Fj

yi j; ˆθ j

for i = 1, . . . n, j = 1 . . . , m.
3. Calculate the IFME of θc:
ˆθIFME
c
= argmax
θc
n
i=1
c

F1

yi1; ˆθ1

, . . . , Fm

yim; ˆθm

; θc

.
4. Generate a bootstrap sample us
i j, i = 1, . . . , n, j = 1, . . . , m from the copula
C(u; ˆθc).
5. Inverse the cdfs to get a bootstrap data sample ys
i j = F−1
j

us
i j; ˆθ j

, i = 1, . . . , n,
j = 1, . . . , m, where ˆθ j is from step 1.
6. Fit a marginal regression to yyys
j to get the estimated parameters ˆθs
j, j = 1, . . . m.
7. Calculate the prediction as Rs = g

ys
n+1,1, . . . , ys
n+1,m

, where ys
n+1, j = F−1
j

us
n+1, j; ˆθs
j

. us
n+1 is a realized sample from C(u; ˆθc).
8. Redo steps 4 to 7 for S times to get a bootstrap sample Rs, s = 1, . . . , S.
The key steps are 4 and 7 which establish the correlation between the estimated
parameters and the correlation between the predicted values.

162
6
Multivariate Modelling Using Copulas
Predictive distributions via MCMC
Again, suppose we want to get the predictive distribution of R = g

yn+1,1, . . . ,
yn+1,m

given the covariates xn+1 = (xn+1,1, . . . , xn+1,m), where g is a generic func-
tion. The MCMC algorithm is as follows:
1. Apply the MCMC methods to each marginal model to generate a Markov chain
whose stationary distribution is the posterior distribution of θ j for j = 1, . . . , m.
2. For the tth MC sampled parameters θt
j, calculate the corresponding cumulative
probabilities ut
i j = Fj

yi j; θt
j

, which will be used as the “observed” data of
the copula.
3. Calculate the MLE of copula parameter θt
c, and generate a sample ut
n+1 ∼
C(u|θt
c).
4. Calculate the prediction values as
Rt = g

F−1
1

ut
n+1,1; θt
1

, . . . , F−1
m

ut
n+1,m; θt
m

.
5. Repeat steps 2 to 4 for T times to get a MC sample Rt, t = 1, . . . , T.
Example 6.1 (A simulated example using a Gumbel copula) Suppose the joint dis-
tribution of two response variables have a Gumbel copula and each variable is
marginally modelled by a linear regression model:
yi1, yi2 ∼C

F1 (yi1; α, β01, β11) , F2

yi2; σ2, β02, β12

; θc

yi1 ∼Gamma

α,
α
β01 + β11xi1

log yi2 ∼N

β02 + β12xi2, σ2
.
The following true parameters are speciﬁed: n = 100, β01 = 1, β11 = 2, α = 10,
β02 = 0.1, β12 = 0.3, σ2 = 0.5, θc = 2 (Kendall’s tau is 0.5). xi1, xi2 are generated
independently from a uniform distribution U [0, 10]. yi1 and yi2 are associated via
the same index i which can indicate the same time, the same place or other common
features. Figure 6.4 shows the relationships between the variables. Due to the effects
of covariates, there is no signiﬁcant relationship between the two response variables.
Inference functions for margins estimator (IFME)
Two linear regression models are ﬁtted to two response variables respectively. The
estimated parameters of two models are shown in Table 6.2. We then calculate the
cdfs of the response variables given the estimated regression parameters as
F1

yi1; ˆβ01, ˆβ11, ˆα

, F2

yi2; ˆβ02, ˆβ12, ˆσ

,
which are denoted by ˆui1, ˆui2, i = 1, . . . , 100.

6.1 Overview of Copulas
163
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
0
5
10
15
20
25
30
x1
y1
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
0
10
20
30
40
50
60
x2
y2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
2
4
6
8
10
0
2
4
6
8
10
x1
x2
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0
5
10
15
20
25
30
0
10
20
30
40
50
60
y1
y2
Fig. 6.4 The scatter plots of the simulated data
Table 6.2 The inferences made for two marginal linear regressions in Example 6.1.
Model
ˆβ0
ˆβ1
1/ ˆα or ˆσ
yi1 ∼Gamma

α,
α
β01+β11xi1

0.96
1.95
0.11
log yi2 ∼N

β02 + β12xi2, σ2
0.07
0.31
0.53
The scatter plot of

ˆui1, ˆui2

, i = 1, . . . , 100 is shown in Fig. 6.5, indicating a
signiﬁcant positive relationship with an empirical Kendall’s tau of 0.51. The rugs
indicate that the marginal distributions of ˆui1, ˆui2 are close to a uniform distribution
as expected.

164
6
Multivariate Modelling Using Copulas
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
u^i1
u^i2
Fig. 6.5
ˆui1 versus ˆui2
5
10
15
20
25
30
0
5
10
15
20
25
30
yi1|x=5
yi2|x=5
yi1 + yi2
Density
0
10
20
30
40
50
60
0.00
0.02
0.04
0.06
●
●Bootstrap mean
95% PI
Fig. 6.6 y101,1 versus y101,2 and the predictive distribution of y101,1 + y101,2 via the bootstrap
methods
The predictive distribution via bootstrap methods
Suppose we want to predict the sum of y101,1 and y101,2, both of which have the
same covariate of 5. The bootstrap algorithm discussed before is used to simulate
the predictive distribution of y101,1 + y101,2. Figure 6.6 shows a signiﬁcant positive
correlation between y101,1 and y101,2. The bootstrap estimate is 16.42 with the 95%
PI of (7.24, 32.23).

6.1 Overview of Copulas
165
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
Posterior mean of u1
Posterior mean of u2
θc
Density
0.67
0.68
0.69
0.70
0.71
0
20
40
60
●
●Mean
95% credible interval
Fig. 6.7
¯ui1 versus ¯ui2 and the posterior distribution of θc via the MCMC
The predictive distribution via MCMC methods
We ﬁt two Bayesian linear models separately to the two response variables. HMC is
applied to simulate from the posterior distribution. At the end of Bayesian infer-
ential simulation, a sample of parameters is obtained. Assuming the tth sam-
pled parameters as βt
01, βt
11, αt, βt
02, βt
12, σt, we can calculate the corresponding
ut
i1, ut
i2, i = 1, . . . , n. The Stan code for sampling ut
i1, ut
i2 is as follows
1
E1. code <-"
2
data {
3
int
n;
4
real
x1[n];
5
real
x2[n];
6
real
y1[n];
7
real
y2[n];
8
}
9
parameters {
10
real
b01;
11
real
b11;
12
real
b02;
13
real
b12;
14
real < lower =0>
alpha ;
15
real < lower =0>
sigma ;
16
}
17
transformed
parameters {
18
real
mu 1[n];
19
real
mu 2[n];
20
for
(i
in
1:n){
21
mu 1[i]<-b01+b11*x1[i];
22
mu 2[i]<-b02+b12*x2[i];
23
}
24
}
25
model {
26
for
(i
in
1:n){
27
y1[i]
~
gamma
(alpha ,
alpha /mu 1[i]);
28
log(y2[i])
~
normal
(mu 2[i],
sigma );

166
6
Multivariate Modelling Using Copulas
29
}
30
}
31
generated
quantities {
32
real
F1[n];
33
real
F2[n];
34
real
res 1[n];
35
real
res 2[n];
36
for
(i
in
1:n){
37
F1[i]<- gamma _cdf(y1[i],alpha , alpha /mu 1[i]);
38
F2[i]<- normal _cdf( log(y2[i]) ,mu 2[i],
sigma );
39
res 1[i]<-(y1[i]-mu 1[i])/mu 1[i]* sqrt ( alpha );
40
res 2[i]<-( log(y2[i]) -mu 2[i])/ sigma ;
41
}
42
}
43
"
44
E1. stanfit <- stan ( model _ code =E1. code , data =c("n","x1" ,"x2" ,"y1" ,"y2
45
") ,iter =400 , chains =4, seed =2)
46
E1. sim <- extract (E1. stanfit , permuted =T)
For the ease of copula parameter estimation, a bivariate Gaussian copula is chosen.
The MLE of a bivariate normal copula parameter is just the sample correlation of
−1(ut
i1) and −1(ut
i2), denoted by θt
c. Figure 6.7 shows the scatter plot of posterior
means, ¯ui1 versus ¯ui2, which is quite similar to Fig. 6.5 indicating the suitability
of using a bivariate Gaussian copula. The histogram of θc is shown in Fig. 6.7,
which also conﬁrms the signiﬁcant positive relationship between ui1 and ui2. Again,
suppose we want to predict the sum of y101,1 and y101,2, both of which have the
same covariate value of 5. We compare two approaches: the independent predic-
tion and the dependent prediction using a copula. The independent prediction is the
sum of posterior predictions yt
101,1, ys
101,2 without considering the permutation. For
the dependent prediction using a copula, ﬁrst a pair of (ut
101,1, ut
101,2) is generated
from a bivariate Gaussian copula with parameter θt
c. Then we inverse two functions,
ut
101,1 = F1(yt
101,1; βt
01, βt
11, αt) and ut
101,2 = F2(yt
101,2; βt
02, βt
12, σt), to get a pair of
(yt
101,1, yt
101,2). Figure 6.8 shows a positive correlation between y101,1, y101,2 under
the dependent prediction and this positive correlation affects the 97.5% percentile
signiﬁcantly compared with independent prediction. The posterior mean is 16.74
with the 95% CPDR of (7.80, 33.20) under the dependent prediction. The R code
for estimating θt
c and predicting y101,1, y101,2 is as follows:
1
rho <-rep(NA , nrow (E1. sim$F1))
2
y1_5<- rep(NA , nrow (E1. sim$F1))
3
y2_5<- rep(NA , nrow (E1. sim$F1))
4
y 1_5_ ind <-rep(NA , nrow (E1. sim$F1))
5
y 2_5_ ind <-rep(NA , nrow (E1. sim$F1))
6
for
(i
in
1: nrow (E1. sim$F1))
7
{
8
rho[i]<-cor( qnorm (E1. sim$F1[i ,]) ,qnorm (E1. sim$F2[i ,]))
9
sigma <- matrix (c(1, rho[i],rho[i] ,1) ,ncol =2)
10
F12<- rmvnorm (1, mean = rep (0 ,2) ,sigma )
11
F1<- pnorm (F 12[1]) ;F2<- pnorm (F 12[2])
12
y 1_5[ i]<- qgamma (F1, shape =E1. sim$ alpha [i], rate =E1. sim$ alpha [i]/(
13
E1. sim$b01[i ]+5* E1. sim$b11[i]))
14
y 2_5[ i]<-exp( qnorm (F2,E1. sim$b02[i ]+5* E1. sim$b12[i],E1. sim$
15
sigma [i]))
16
y 1_5_ ind[i]<- rgamma (1, shape =E1. sim$ alpha [i], rate =E1. sim$ alpha [i]

6.1 Overview of Copulas
167
17
/(E1. sim$b 01[i ]+5* E1. sim$b11[i]))
18
y 2_5_ ind[i]<-exp( rnorm (1,E1. sim$b02[i ]+5* E1. sim$b12[i],E1. sim$
19
sigma [i]))
20
}
6.2
Copulas in Modelling Risk Dependence
We focus on the models for multiple run-off triangles. There are several papers on
this topic. Shi and Frees (2011) and Shi (2014) use the elliptical copulas to address
the dependencies introduced by various sources. They use the parametric bootstrap
method to simulate the predictive distribution of outstanding liabilities. De Jong
(2012) uses a Gaussian copula to model the dependence of payments from different
triangles in the same calendar year.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
25
0
5
10
15
20
25
30
35
y1
y2
y1 + y2
Density
0
10
20
30
40
50
0.00
0.01
0.02
0.03
0.04
0.05
0.06
●
●Posterior mean
VaR
TVaR
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
5
10
15
20
25
30
0
5
10
15
20
25
y1
y2
y1 + y2
Density
0
10
20
30
40
50
0.00
0.02
0.04
0.06
●
●Posterior mean
VaR
TVaR
Fig. 6.8 y101,1 versus y101,2 and the predictive distribution of y101,1 + y101,2 via the MCMC. The
ﬁrst row is from the desirable copula model. The second row is from the inappropriate independent
model for the purpose of comparison. VaR and TVaR will be discussed in Sect. 6.2.2

168
6
Multivariate Modelling Using Copulas
One of the most important works is Zhang et al. (2012) which was awarded
the ARIA prize by the Casualty Actuary Society. This annual prize, ﬁrst awarded in
1997, is made to the author or authors of a paper published by the Journal of Risk and
Insurance that provides the most valuable contribution to casualty actuarial science.
This paper uses a Bayesian copula model to address the dependence between the
different triangle payments in the same accident year and development year. This
paper compares the goodness-of-ﬁt of Clayton, Gumbel, Frank and Gaussian copulas
and uses three different marginal regressions: a generalized linear regression, a non-
linear growth curve model and a semi-parametric model.
6.2.1
Structural and Empirical Dependence Between Risks
We distinguish the two types of dependence since two different approaches are used
to tackle them. In general, the risks an insurer faces often exhibit co-movement or
dependencies. This means that knowledge about results for one risk can be used to
better predict the results for another risk. Dependence between two risks may be
due to known relationships (structural dependence), or simply due to the historically
observed correlations (empirical dependence).
Structural dependence modelling
The structural co-movements can be accounted for in a regression modelling process.
Structural dependencies include situations where loss variables are driven by com-
mon variables: for example, the cumulative claims of two beneﬁts are both increasing
with the development periods. This positive dependence can be modelled by using
the covariate of development periods.
Empirical dependence modelling
The empirical co-movements are simply observed without any known (or capable
of being modelled) relationships, i.e., the positive relationship of residuals from
two models. For many types of risks, particularly in property and liability areas, co-
movements are observed, but may not be easily explained. It is more likely necessary
to construct dependency models that reﬂect observed and expected dependencies
without formalising the structure of those dependencies with cause-effect models.
The theory of copulas provides a comprehensive modelling tool that can reﬂect
dependencies in a very ﬂexible way.

6.2 Copulas in Modelling Risk Dependence
169
6.2.2
The Effects of Empirical Dependence on Risk Measures
An insurer needs to hold much more than the expected value of unpaid claims liability
to ensure the company’s solvency with a quite large probability. In Australia, insurers
typically add a risk margin to the mean of liability to get the estimation of reserve
amount.
A risk margin is set consistently with risk measures. A risk measure is not cal-
culated by summing up the contributions of different business lines, but more likely
from the distribution of all risks combined. So it is necessary to consider the empirical
dependence between different lines.
6.2.2.1
Risk Measures
Most risk measures can be classiﬁed as moment-based, tail-based, or probability
transforms. The moment-based risk measures (including the standard deviation and
semi-standard deviation) are not often used since they are not directly related to the
solvency concept.
The most used risk measures are tail-based risk measures which emphasize large
losses. The four tail-based risk measures, value at risk (VaR), tail value at risk
(TVaR), excess tail value at risk (XTVaR), and expected policyholder deﬁcit (EPD),
are deﬁned as follows:
• VaR is a percentile of a loss distribution.
• TVaR is the expected loss at a speciﬁed probability level and beyond.
• XTVaR is TVaR less the mean. When the mean is ﬁnanced by other funding,
capital is needed for losses above the mean, so subtracting the mean can capture
this need.
• EPD is calculated by multiplying TVaR minus VaR by the probability level. If
the probability level is chosen so that capital is VaR at that level, then TVaR
minus VaR is the expected value of defaulted losses if there is default. Multiplying
this quantity by the complement of the probability level yields the unconditional
expected value of defaulted losses.
Probability transforms measure risk by shifting the probability towards the
unfavourable outcomes and then computing a risk measure from the transformed
probabilities. Most of the usual asset pricing formulas, like the capital asset pricing
model and the Black-Scholes options pricing formula, can be expressed as trans-
formed mean.

170
6
Multivariate Modelling Using Copulas
Table 6.3 The tail-based risk measures under different copula parameters in Example 6.2
Copula parameters
Loss
Mean
VaR
TVaR
XTVaR
EPD
x1
200.00
366.14
438.23
234.32
3.02
x2
147.31
295.88
369.13
223.07
3.80
θc = 1, τ = 0
x1 + x2
347.31
566.10
654.03
304.07
4.40
θc = 2, τ = 0.5
x1 + x2
347.31
667.59
826.50
473.59
7.95
θc = 4, τ = 0.75
x1 + x2
347.31
687.10
852.75
501.18
8.28
Example 6.2 (Empirical dependence) We illustrate the effects of empirical depen-
dence on the risk measures by a hypothetical example. Consider two correlated loss
variables x1 and x2 with the following distribution:
F (x1, x2) = C

FG (x1; α, μ1) , FLN

x2; μ2, σ2
; θc

x1 ∼Gamma

α, α
μ1

log x2 ∼N

μ2, σ2
,
where C is a Gumbel copula. The underlying parameters are speciﬁed as μ1 =
200, α = 5, μ2 = log 130 and σ2 = 0.25. Consider three cases: θc = 1 (i.e., the two
risks are independent), θc = 2 and θc = 4. The two marginal distributions are posi-
tively skewed.
By doing simulation, we estimate the four tail-based risk measures for individual
loss and the aggregated loss. Table 6.3 shows the results, implying the signiﬁcant
effects of empirical dependence on the tail-based risk measures. Figure 6.9 shows
that when θc = 2, larger x1 and x2 are more likely to be correlated with each other.
This is because a Gumbel copula has a non-zero upper tail dependence as shown in
Table 6.1.
6.3
Application to the Doctor and Hospital Beneﬁts
Recall that Table 4.9 lists all the beneﬁts in the WorkSafe Victoria. In “medical and
like” beneﬁt category, we have two sub-beneﬁts: doctor and hospital. Intuitively,
these two sub-beneﬁts should be positively correlated. Here we focus on the models
applied to the claims amounts rather than the PPCI method as in the previous two
chapters.

6.3 Application to the Doctor and Hospital Beneﬁts
171
100
200
300
400
500
600
0
200
400
600
800
X1
X2
X1 + X2
Density
0
200
400
600
800
1000
1200
0.0000
0.0010
0.0020
0.0030
●
●Mean
95% VaR
100
200
300
400
500
600
100
200
300
400
500
600
X1
X2
X1 + X2
Density
0
200
400
600
800
1000
1200
0.0000
0.0010
0.0020
●
●Mean
95% VaR
Fig. 6.9 x1 versus x2 and the distribution of x1 + x2. The ﬁrst row is for θc = 1. The second row
is for θc = 2
6.3.1
Preliminary GLM Analysis Using a Gaussian Copula
As a quick check of correlation between two triangles, we recommend starting from
the least complicated models. We ﬁt two chain ladder GLMs with a gamma error and
a log link to the doctor beneﬁt x and the hospital beneﬁt y. The model is as follows:
F

xi j, yi j

= C

F1

xi j; α1, μ1i, γ1 j

, F2

yi j; α2, μ2i, γ2 j

; θc

xi j ∼Gamma

α1,
α1
μ1iγ1 j

, i = 1, . . . , 27, j = 1, . . . , 27
yi j ∼Gamma

α2,
α2
μ2iγ2 j

, i = 1, . . . , 27, j = 1, . . . , 27,

172
6
Multivariate Modelling Using Copulas
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
13
14
15
16
17
−2
0
2
4
Linear predictors of doctor benefit
Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
13
14
15
16
17
−4
−2
0
2
4
Linear predictors of hospital benefit
Pearson residuals
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
−2
0
2
4
−4
−2
0
2
4
Pearson residuals from doctor model
Pearson residuals from hospital model
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
0.0
0.2
0.4
0.6
0.8
1.0
0.0
0.2
0.4
0.6
0.8
1.0
u^
v^
Fig. 6.10 The top two: the residual plots of two marginal regressions. The bottom two: the scatter
plot of residuals and the scatter plot of ˆui j versus ˆvi j.
where F1, F2 are the cdfs of gamma distributions and C is a bivariate Gaussian
copula.
For model inference, the IFME method is applied. We calculate the empirical
cdfs, ˆui j = F1

xi j; ˆα1, ˆμ1i, ˆγ1 j

and ˆvi j = F2

yi j; ˆα2, ˆμ2i, ˆγ2 j

. Note that ˆα1, ˆμ1i,
ˆγ1 j, ˆα2, ˆμ2i, ˆγ2 j are the MLEs. We draw four Pearson residual plots: two scatter
plots of residuals from marginal GLMs, the plot showing the relationship between
two residuals, and the plot of ˆui j versus ˆvi j in Fig. 6.10. It shows a signiﬁcant positive
empirical relationship.

6.3 Application to the Doctor and Hospital Beneﬁts
173
6.3.1.1
The Predictive Distribution via a Parametric Bootstrap
The claims liability is simulated via the bootstrap method. The IFME of θc is ˆθc =
cor

−1 
ˆu

, −1 
ˆv

= 0.5530.Wecomparethebootstrapsamplefromthecopula
model (ﬁrst row in Fig. 6.11) with the bootstrap sample from an independent model
(second row in Fig. 6.11).
The 95% VaR from the copula model is larger than that from the independent
model. We also list other tail-based risk measures in Table 6.4. Note that the estimated
aggregated liability of both beneﬁts is 707,407,135 dollars in the PwC report.
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●●
●
●
●
●
360
380
400
420
440
460
260
280
300
320
Outstanding liability of doctor benefit
Outstanding liability of hospital benefit
Total liabilities of both doctor and hospital
benefit 
Density
650
700
750
0.000
0.005
0.010
0.015
●
●Bootstrap mean
95% VaR
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
360
380
400
420
440
260
280
300
320
Outstanding liability of doctor benefit
Outstanding liability of hospital benefit
Total liabilities of both doctor and hospital 
benefit
Density
650
700
750
0.000
0.005
0.010
0.015
●
●Bootstrap mean
95% VaR
Fig. 6.11 The top two: the prediction of claims liability of two beneﬁts made from the desirable
copula model. The bottom two: the prediction of claims liability of two beneﬁts made from the
inappropriate independent model. The simulation is performed using bootstrap methods

174
6
Multivariate Modelling Using Copulas
Table 6.4 The tail-based risk measures of the aggregated liability via bootstrap methods
Model
Mean
VaR
TVaR
XTVaR
EPD
Copula model
692,205,659
737,515,967
747,729,301
55,523,642
510,667
Independent
693,343,113
727,254,943
737,508,270
44,165,157
512,666
Differences (%)
−0.2
1.4
1.4
25.7
−0.4
6.3.2
A Gaussian Copula with Marginal Bayesian Splines
We apply a Gaussian copula model with two marginal Bayesian natural cubic spline
models to the two beneﬁts, as follows:
F

xi j, yi j

= C

F1

xi j; α1, θi j

, F2

yi j; α2, ϕi j

; θc

xi j ∼Gamma

α1, α1
θi j

yi j ∼Gamma

α2, α2
ϕi j

θi j = Ai × exp
 27

h=1
βhbh ( j)

ϕi j = Bi × exp
 27

h=1
γhbh ( j)

βh ∼DoubleExp

0, σ2
1

, h = 1, . . . , 27
γh ∼DoubleExp

0, σ2
2

, h = 1, . . . , 27
θc ∼U (0, 1) ,
where F1, F2 are the cdfs of gamma distributions and C is a bivariate Gaussian
copula. All the claims are assumed to be settled by the development year 30. The
IFME method is applied for the copula parameter estimation.
6.3.2.1
The Inferences for the Marginal Bayesian Splines
We draw the posterior mean and the 95% CPDR of the proportions of incremental
paymentstotheultimateclaimspaymentsfortwobeneﬁtsinFig.6.12.Theincreasing
uncertainty in the tail developments is due to the lack of data. However, as stated
in the discussion of previous chapter, we believe that the uncertainties should not
increase dramatically. One approach to solving this problem is to assume strong
priors for the tail developments.

6.3 Application to the Doctor and Hospital Beneﬁts
175
0
5
10
15
20
25
30
0.00
0.05
0.10
0.15
0.20
0.25
Development years
Incremental proportion (doctor)
Posterior mean
95% CPDR
0
5
10
15
20
25
30
0.0
0.1
0.2
0.3
0.4
Development years
Incremental proportion (hospital)
Posterior mean
95% CPDR
Fig. 6.12 Proportions of the incremental claims to the ultimate claims under non-informative priors
Under the non-informative priors, the posterior mean of exp
27
h=1 βhbh (27)

was 0.003 with posterior standard deviation of 0.0004 and the posterior mean of
exp
27
h=1 γhbh (27)

is 0.003 with posterior standard deviation of 0.0006. Accord-
ingly, we assume the following strong priors for the tail developments in the devel-
opment years 28, 29, 30:
exp
 27

h=1
βhbh ( j)

∼N (0.003, 0.0003) , j = 28, . . . , 30

176
6
Multivariate Modelling Using Copulas
exp
 27

h=1
γhbh ( j)

∼N (0.003, 0.0006) , j = 28, . . . , 30.
The resulting posterior distributions of proportions of incremental claims for both
beneﬁts are plotted in Fig. 6.13. Now the tail developments do not show as much
volatility as in the model with non-informative priors. The Stan code is as follows:
1
amount .code <-"
2
data {
3
int
N;
//
Number
of
observations
4
int
n;
//
Number
of
future
values
5
int
K;
//
Number
of
accident
year
6
int
M;
//
Number
of
develop
year
7
int
H;
//
Number
of
basis
functions
8
vector [N]
amount _ doc;
//
Claims
of
doctor
benefit
9
vector [N]
amount _ hos;
//
Claims
of
hospital
benefit
10
matrix [M,H]
dev_ basis ;
//
Basis
functions
11
int
acc[N];
//
Accident
years
in
upper
triangle
12
int
dev[N];
//
Development
years
in
upper
triangle
13
int
acc_p[n];
//
Accident
years
in
lower
triangle
14
int
dev_p[n];
//
Development
years
in
lower
triangle
15
}
16
parameters {
17
vector [H]
b1;
18
vector [H]
b2;
19
vector < lower =60*10^6 , upper =150*10^6 >[ K]
ult 1;
20
vector < lower =50*10^6 , upper =150*10^6 >[ K]
ult 2;
21
real < lower =0>
alpha 1;
22
real < lower =0>
alpha 2;
23
real < lower =0>
sigma 1;
24
real < lower =0>
sigma 2;
25
}
26
transformed
parameters {
27
vector [N]
means 1;
28
vector [N]
means 2;
29
vector [M]
dev_raw 1;
30
vector [M]
dev_raw 2;
31
vector < lower =0 >[M]
dev_ norm 1;
32
vector < lower =0 >[M]
dev_ norm 2;
33
dev_raw1<-exp(dev_ basis
*
b1);
34
dev_raw2<-exp(dev_ basis
*
b2);
35
dev_ norm 1<-dev_raw 1/ sum(dev_raw 1);
36
dev_ norm 2<-dev_raw 2/ sum(dev_raw 2);
37
for
(i
in
1:N){
38
means 1[i]<-ult 1[ acc[i]]* dev_ norm 1[ dev[i]];
39
means 2[i]<-ult 2[ acc[i]]* dev_ norm 2[ dev[i]];
40
}
41
}
42
model {
43
b1
~
cauchy (0, sigma 1);
// sigma
is
a
tuning
parameters
44
b2
~
cauchy (0, sigma 2);
// sigma
is
a
tuning
parameters
45
for(i
in
28:M){
46
dev_ norm 1[i]
~
normal (0.003 ,
0.0003) ;
47
dev_ norm 2[i]
~
normal (0.003 ,
0.0006) ;
48
}

6.3 Application to the Doctor and Hospital Beneﬁts
177
49
for
(i
in
1:N){
50
amount _ doc[i]
~
gamma ( alpha 1,
alpha 1/ means 1[i]);
51
amount _ hos[i]
~
gamma ( alpha 2,
alpha 2/ means 2[i]);
52
}
53
}
54
generated
quantities {
55
vector [n]
means _p1;
56
vector [n]
means _p2;
57
vector [N]
u;
58
vector [N]
v;
59
vector [N]
residuals 1;
60
vector [N]
residuals 2;
61
vector [N]
log_lik 1;
62
vector [N]
log_lik 2;
63
real
D1;
64
real
D2;
65
for
(i
in
1:n){
66
means _p1[i]<-ult 1[ acc_p[i]]* dev_ norm 1[ dev_p[i]];
67
means _p2[i]<-ult 2[ acc_p[i]]* dev_ norm 2[ dev_p[i]];
68
}
69
for
(i
in
1:N){
70
u[i]<- gamma _cdf( amount _doc[i], alpha 1, alpha 1/ means 1[i]);
71
v[i]<- gamma _cdf( amount _hos[i], alpha 2, alpha 2/ means 2[i]);
72
residuals 1[i]<-( amount _doc[i]- means 1[i])/ means 1[i]* sqrt ( alpha
1);
73
residuals 2[i]<-( amount _hos[i]- means 2[i])/ means 2[i]* sqrt ( alpha
2);
74
log_lik 1[i]<- gamma _log( amount _ doc[i], alpha 1, alpha 1/ means 1[i]);
75
log_lik 2[i]<- gamma _log( amount _ hos[i], alpha 2, alpha 2/ means 2[i]);
76
}
77
D1<-sum ( -2* log_lik 1);
78
D2<-sum ( -2* log_lik 2);
79
}"
80
knots <-c (2:26)
81
dev_basis <-ns(c (1:30) ,knots =knots , Boundary . knots
=
c(1 ,27) ,
intercept
=
T)
82
H<- ncol (dev_ basis )
83
M<- nrow (dev_ basis )
84
amount . stanfit <- stan ( model _ code
=
amount .code ,
data =c(" amount _doc
85
"," amount _ hos ","N","n","K","M","H"," acc "," dev "," acc_p"," dev_p
86
"," dev_ basis ") ,iter =800 , chains =4,
seed =10)
6.3.2.2
The Predictive Distribution via MCMC Methods
We aggregate the liabilities of two beneﬁts via a bivariate Gaussian copula. Surpris-
ingly, there is no signiﬁcant difference between simulations of total liability from
the copula model and from the independent model as shown in Fig. 6.14.
There are two reasons for this: one is that the marginal Bayesian model uncertainty
overwhelms the dependence between them; the other is that the copula is used to
model the dependence of incremental claims and the sum of incremental claims
may display less dependence. We list the tail-based risk measures of the aggregated
liability in Table 6.5.

178
6
Multivariate Modelling Using Copulas
0
5
10
15
20
25
30
0.00
0.05
0.10
0.15
0.20
0.25
Development years
Incremental proportion (doctor)
Posterior mean
95% CPDR
0
5
10
15
20
25
30
0.0
0.1
0.2
0.3
0.4
Development years
Incremental proportion (hospital)
Posterior mean
95% CPDR
Fig. 6.13 Proportions of the incremental claims to the ultimate claims under strong priors
Table 6.5 The tail-based risk measures of the aggregated liability via MCMC methods. The PwC
estimate is 707,407,135
Model
Mean
VaR
TVaR
XTVaR
EPD
Copula model
706,344,715
745,713,292
756,101,056
49,756,341
519,388
Independent
706,302,106
742,610,194
753,119,350
46,817,244
525,458
Differences (%)
0.01
0.42
0.40
6.28
1.16

6.3 Application to the Doctor and Hospital Beneﬁts
179
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
380
400
420
440
460
260
280
300
320
340
Outstanding liability of doctor benefit (copula)
Outstanding liability of hospital benefit
Total liabilities in millions (copula model)
Density
650
700
750
800
0.000
0.005
0.010
0.015
●
●Posterior mean
95% VaR
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
380
400
420
440
460
260
280
300
320
340
Outstanding liability of doctor benefit 
(independent)
Outstanding liability of hospital benefit
Total liabilities in millions 
(independent model)
Density
650
700
750
0.000
0.005
0.010
0.015
●
●Posterior mean
95% VaR
Fig. 6.14 The top two: the prediction of claims liability of two beneﬁts made from the desirable
copula model. The bottom two: the prediction of claims liability of two beneﬁts made from the
inappropriate independent model. The simulation is performed using MCMC methods
To end of this section, we point out that the copula model makes a difference if
the claims payments in the next calendar year are predicted. As we did for the total
claims liability, we simulate the claims payments in the next calendar year for both
beneﬁts from the copula model and from the independent model. The results are
shown in Fig. 6.15 and Table 6.6. The empirical positive correlation is more obvious
and it affects the XTVaR most signiﬁcantly.

180
6
Multivariate Modelling Using Copulas
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
70
80
90
100
40
45
50
55
60
65
70
Doctor benefit payment in the next year
(copula)
Hospital benefit payment in the next year
Total payments in the next year in millions
(copula model)
Density
110
120
130
140
150
160
170
0.00
0.01
0.02
0.03
0.04
●
●Posterior mean
95% VaR
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
70
80
90
100
40
45
50
55
60
65
70
Doctor benefit payment in the next year 
(independent)
Hospital benefit payment in the next year
Total payments in the next year in millions 
(independent model)
Density
110
120
130
140
150
160
0.00
0.01
0.02
0.03
0.04
0.05
●
●Posterior mean
95% VaR
Fig. 6.15 The top two: the prediction of next year claims payment of two beneﬁts made from
the desirable copula model. The bottom two: the prediction of next year claims payment of two
beneﬁts made from the inappropriate independent model. The simulation is performed using MCMC
methods
Table 6.6 The tail-based risk measures of the aggregated claims payments in the next calendar
year via MCMC methods
Model
Mean
VaR
TVaR
XTVaR
EPD
Copula model
133,988,676
149,919,590
154,493,898
20,505,222
228,715
Independent
133,956,740
147,246,196
151,426,112
17,469,373
208,996
Differences (%)
0.02
1.82
2.03
17.38
9.44

6.4 Discussion
181
6.4
Discussion
Copulas have a wide range of applications in ﬁnance, risk management, insurance
etc. This chapter uses copulas to model the contemporaneous correlation, i.e., the
dependence among different run-off triangles at the same development year and the
same accident year. There are several actuarial papers considering using copulas to
model other types of dependence, such as the common calendar years dependence
due to claims inﬂation.
Anotherconcernistheestimationmethodforthecopulamodels.Hereweapplythe
IFME method involving two consecutive steps: ﬁrst make inference of the marginal
regressions, then ﬁx the parameters of the marginal regressions and infer the cop-
ula parameters. We have done some experiments to compare the Bayesian IFME
method (applying MCMC to the marginal distribution and MLE to the copula con-
secutively) and the full Bayesian method (applying MCMC to the multivariate likeli-
hood directly). They show that the Bayesian IFME method takes much less time with
better convergence and similar inferences compared with the full Bayesian method.
So we are conﬁdent with the Bayesian IFME method. Nevertheless, several papers
develop a MCMC algorithm for the full Bayesian copula models (see the relevant
literature in the next section).
In this chapter, we do not consider the selection of the optimal copula family,
since a Gaussian copula ﬁts well (at least visually) in all the problems considered.
Genest and Rivest (1993) provide estimation and selection methods for Archimedean
copulas. The tail dependence can be used to select a copula if the interest is in the
tail behaviour.
6.5
Bibliographic Notes
A thorough discussion of copulas can be found in Joe (2014). An introduction to
copulas is available in Nelsen (2013), which does not, however, contain the inference
methods. Sklar (1959) introduces Sklar’s theorem. Trivedi and Zimmer (2007) cover
themainimplementationandestimationofcopulas.GenestandRivest(1993)provide
estimation and selection methods for Archimedean copulas. Embrechts and Hofert
(2013) address the inference methods and goodness-of-ﬁt tests for high-dimensional
copulas. Kruskal (1958) discusses the measures of association in detail.
Pitt et al. (2006), Hoff (2007), Danaher and Smith (2011) and Smith (2011) discuss
the Bayesian copula models and design efﬁcient MCMC methods accordingly. All
of them also consider the special case where there are discrete response variables.
Frees and Valdez (1998) introduced copulas to actuarial science. A general
overview of copulas and their applications in actuarial science is provided by
Embrechts et al. (2001), Venter (2002), Brehm et al. (2007) and Feldblum (2010).
Literature considering the dependence among run-off triangles includes Shi and
Frees (2011) and Zhang et al. (2012), both of which use copulas to model the con-

182
6
Multivariate Modelling Using Copulas
temporaneous correlations among various lines of business: the former apply the
bootstrap to estimate the predictive distribution of unpaid claims, while the latter
apply the MCMC methods, which is closer to what we did in this chapter. De Jong
(2012) uses copulas to accommodate the common calendar effect between triangles.
Shi et al. (2012) and Merz et al. (2013) model the contemporaneous dependence
between run-off triangles and the common calendar effect within a run-off triangle
via a Bayesian hierarchical log-normal model, which is equivalent to a Gaussian
copula model with marginal log-normal regressions. Shi (2014) relaxes the marginal
log-normal regression using elliptical copulas. Anas et al. (2015) use a hierarchical
Archimedean copula to analyze the data from Shi and Frees (2011).
Czado et al. (2012) and Krämer et al. (2013) use copulas to model the dependence
between claims occurrences and claims sizes. Meng and Gao (2018) discuss the
claims reserving methods using both claims numbers and claims amounts but not in
a copula framework.
References
Anas, A., Boucher, J. P., & Cossette, H. (2015). Modeling dependence between loss triangles with
hierarchical Archimedean copulas. ASTIN Bulletin, 45, 577–599.
Brehm, P. J., Perry, G., Venter, G. G., & Witcraft, S. (2007). Enterprise risk analysis for property
and liability insurance companies: A practical guide to standard models and emerging solutions.
New York: Guy Carpenter & Company.
Czado, C., Kastenmeier, R., Brechmann, E. C., & Min, A. (2012). A mixed copula model for
insurance claims and claim sizes. Scandinavian Actuarial Journal, 2012, 278–305.
Danaher, P. J., & Smith, M. S. (2011). Modeling multivariate distributions using copulas: Applica-
tions in marketing. Marketing Science, 30, 4–21.
DeJong,P.(2012).Modelingdependencebetweenlosstriangles.NorthAmericanActuarialJournal,
16, 74–86.
Embrechts, P., Lindskog, F., & McNeil, A. (2001). Modelling dependence with copulas and appli-
cations to risk management. https://people.math.ethz.ch/~embrecht/ftp/copchapter.pdf.
Embrechts, P., & Hofert, M. (2013). Statistical inference for copulas in high dimensions: A simu-
lation study. ASTIN Bulletin, 43, 81–95.
Feldblum, S. (2010). Dependency modeling. Casualty Actuarial Society Study Notes.
Fréchet, M. (1935). Generalisations du theoreme des probabilites totales. Fundamenta Mathemat-
icae, 25, 379–387.
Frees, E. W., & Valdez, E. A. (1998). Understanding relationships using copulas. North American
Actuarial Journal, 2, 1–25.
Genest, C., & Rivest, L.-P. (1993). Statistical inference procedures for bivariate Archimedean cop-
ulas. Journal of the American statistical Association, 88, 1034–1043.
Hoff, P. D. (2007). Extending the rank likelihood for semiparametric copula estimation. The Annals
of Applied Statistics, 1, 265–283.
Joe, H. (2014). Dependence modeling with copulas. New York: Chapman & Hall.
Krämer, N., Brechmann, E. C., Silvestrini, D., and Czado, C. (2013). Total loss estimation using
copula-based regression models. Insurance: Mathematics and Economics, 53, 829–839.
Kruskal, W. H. (1958). Ordinal measures of association. Journal of the American Statistical Asso-
ciation, 53, 814–861.
Meng, S. and Gao, G. (2018). Compound Poisson claims reserving models: Extensions and infer-
ence. ASTIN Bulletin, 48, 1137–1156.

References
183
Merz, M., Wüthrich, M. V., & Hashorva, E. (2013). Dependence modelling in multivariate claims
run-off triangles. Annals of Actuarial Science, 7, 3–25.
Nelsen, R. B. (2013). An introduction to copulas. New York: Springer.
Pitt, M., Chan, D., & Kohn, R. (2006). Efﬁcient Bayesian inference for Gaussian copula regression
models. Biometrika, 93, 537–554.
Shi, P. (2014). A copula regression for modeling multivariate loss triangles and quantifying reserving
variability. ASTIN Bulletin, 44, 85–102.
Shi, P., & Frees, E. W. (2011). Dependent loss reserving using copulas. ASTIN Bulletin, 41, 449–486.
Shi, P., Basu, S., & Meyers, G. G. (2012). A Bayesian log-normal model for multivariate loss
reserving. North American Actuarial Journal, 16, 29–51.
Sklar, M. (1959). Fonctions de répartition à n dimensions et leurs marges. Publications de l’Institut
de Statistique de l’Université Paris, 8, 229–231.
Smith, M. S. (2011). Bayesian approaches to copula modelling. SSRN, ID 1974297.
Trivedi, P. K., & Zimmer, D. M. (2007). Copula Modeling: An Introduction for Practitioners.
Boston: Now Publishers.
Venter, G. G. (2002). Tails of copulas. Proceedings of the Casualty Actuarial Society, 89, 68–113.
Zhang, Y., Dukic, V., & Guszcza, J. (2012). A Bayesian non-linear model for forecasting insurance
loss payments. Journal of the Royal Statistical Society A, 175, 637–656.

Chapter 7
Epilogue
Abstract In this ﬁnal chapter, we summarize the three proposed Bayesian claims
reserving models and suggest a Bayesian modelling procedure for use when facing
a real problem. Finally, other considerations with respect to Bayesian methodology
and actuarial applications are discussed.
7.1
The Three Claims Reserving Models
This monograph presents several Bayesian models to tackle the claims reserving
problem in general insurance. These models are used to analyze the WorkSafe Vic-
toria data set. Bayesian models provide a coherent way to incorporate the prior
knowledge and combine it with the evidence from the data. This property is particu-
larly useful when the actuarial judgements override the data. Another advantage of
Bayesian models is that the Bayesian inferential engine can simulate the posterior
distribution of parameters and the predictive distribution of the future value. This
property is very important for application to the claims reserving problem, since
claims reserving models are always complicated in terms of number of parameters
and insurers are more interested in the distribution of unpaid claims than the point
estimates.
We point out that the three proposed claims reserving models in this monograph
are a compound model, a Bayesian natural cubic spline basis expansion model and
a copula model with Bayesian margins. For the model inference, we rely on Stan,
which implements the HMC method. Like MCMC, HMC simulates a Markov chain
whose stationary distribution is the same as the target distribution. Compared with
MCMC, HMC has a higher acceptance rate due to the “Hamiltonian dynamics”
proposal.
7.1.1
A Compound Model
The PPCI method is used in the PwC report for the doctor beneﬁt in WorkSafe
Victoria, and we propose a compound model as a stochastic version of the PPCI
method. The key point is to establish the relationship between the variance in a
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6_7
185

186
7
Epilogue
single claim payment and the variance in PPCI. The distributional assumption of a
single claim payment could be checked if we had the individual claims data.
The compound model discussed in Chaps. 5 and 6 is as follows:
yi j =
μi

k=1
xi jk
μi ∼Distributioni
xi jk ∼Gamma

αi j, βi j

, k = 1, . . . , μi,
where μi is the ultimate claims number of accident year i whose distribution is
approximated by a Bayesian model, and xi jk is the payment for the kth claim dur-
ing the development year j whose distribution depends on both accident year and
development year.
We deﬁne the payments per claim incurred during the development period j of
accident year i as PPCIi j := yi j/E (μi). Note that E(PPCIi j) = E(xi jk). We use the
posterior mean of μi as an estimate of E (μi). The relationship between the variance
of PPCIi j and the variance of xi jk is
Var

xi jk

= (E (μi))2Var

PPCIi j

−Var (μi)

E

PPCIi j
2
E (μi)
,
where all the quantities on the right hand side can be estimated by a MC sam-
ple. The distribution of yi j conditional on μi is Gamma

μiαi j, βi j

, where αi j =
E

xi jk
2/Var

xi jk

and βi j = αi j/E

xi jk

.
7.1.2
A Bayesian Natural Cubic Spline Basis Expansion
Model
In the claims reserving models, the two challenging tasks are the derivation of the
predictive distribution and the ﬁt to the various shapes of development patterns. In
the Bayesian framework, the ﬁrst task is easily tackled by either the MCMC method
or the HMC method. To deal with the second task, we rely on the chain ladder model
or the basis expansion model.
The stochastic chain ladder model treats the development year as a factor variable,
effectively introducing the same number of parameters as the number of development
periods. So it can accommodate all the shapes of development patterns. However,
the stochastic chain ladder model does not introduce the tail development.
The basis expansion model treats the development year as a continuous vari-
able and expands the predictor space by including transformation of the predictor
variable. In the Bayesian framework, we can shrink the non-signiﬁcant parameters
and interpolate the tail development.
Consider the Bayesian basis expansion model as discussed in Chap. 5:

7.1 The Three Claims Reserving Models
187
yi j ∼Gamma

α, α
μi j

, i = 1, . . . , I, j = 1, . . . , J
μi j = Pi × L Ri × exp
 H

h=1
βhbh ( j)

βh ∼DoubleExp

0, σ2
h

, h = 1, . . . , H.
The key part of this model is the natural cubic spline basis functions {bh: h =
1, . . . , H} which expand the predictor space. We use the B-spline basis, an orthog-
onal set, generated by R function ns( ). We normally choose the knots at every
unique value of x, which is analogous to the full rank smoothing splines.
Here we choose a gamma error distribution which could be replaced by other
distributions such as a more general Tweedie distribution.
7.1.3
A Copula Model with Bayesian Margins
The copula model is used to aggregate the outstanding claims liabilities estimated
from multiple triangles. We could assume any marginal regression for each triangle
in the copula framework. In this monograph, we use the Gaussian copula which
offers computational simplicity.
The copula model with Bayesian marginal regressions discussed in Chap. 6 is as
follows:
F

xi j, yi j

= C
	
F1

xi j; α1, θi j

, F2

yi j; α2, ϕi j

; θc

xi j ∼Gamma

α1, α1
θi j

yi j ∼Gamma

α2, α2
ϕi j

θi j = Ai × exp
 H

h=1
βhbh ( j)

ϕi j = Bi × exp
 H

h=1
γhbh ( j)

βh ∼DoubleExp

0, σ2
1

γh ∼DoubleExp

0, σ2
2

θc ∼U (0, 1) ,
with non-informative priors for α1, α2, Ai, Bi, σ2
1, σ2
2. We ﬁt the model using the
IFME method (Joe 2014) which is not a full Bayesian analysis. It is possible to do a
full Bayesian analysis by using a user-deﬁned MCMC algorithm.

188
7
Epilogue
7.2
A Suggested Bayesian Modelling Procedure
A typical Bayesian modelling procedure includes: proposing a full probability model,
calculating the posterior inference conditional on the data, modelling evaluation, and
reﬁnement. We suggest that a typical Bayesian modelling procedure should involve
the following steps:
1. Deﬁne the problem. Different problems need different levels of effort. If we just
need to get a point estimate of unpaid claims, the deterministic CL method or BF
method may solve this problem well enough.
2. Visualize the data. We cannot change the data which is a reﬂection of real world,
but we could change a model. Visualising the data helps us detect abnormal
observations and choose a suitable model to analyze the data.
3. Fit a classical model, usually a GLM. This includes choosing the covariates,
the mean function and the error distribution. It is good to try a simple model
ﬁrst, then go deeper into a more complicated model. In the GLM setting, lots
of diagnostic tools are available and easily accessed. The mean function and the
error distribution can be used in the next step.
4. Set up a Bayesian model and simulate from the posterior distribution. We turn
to Bayesian modelling software such as BUGS or Stan to simulate from the
posterior distributions. The detection of convergence was discussed in Sect. 3.2.1
and strategies for improving the convergence and efﬁciency were discussed in
Sect. 3.2.2.
5. Make inferences from the MCMC or HMC sample. If the predictive distribution
is required, we need to perform one further step to simulate the future values from
the likelihood.
6. Model assessment and selection. We can compare different models using several
information criteria. LOO cross-validation and WAIC can be easily derived using
Stan, while DIC can be calculated automatically in BUGS.
We followed these six steps (though not strictly) in all the Bayesian modelling pre-
sented in this monograph. A variation to step 4 is to use a user-deﬁned MCMC or
HMC algorithm. We did this in the early stage of research for the examples discussed
in Chaps. 2 and 3. We also used a user-deﬁned RJMCMC algorithm in Sect. 4.3.1.
7.3
Other Considerations
We list some other considerations in Bayesian methodology and actuarial applica-
tions.

7.3 Other Considerations
189
7.3.1
Bayesian Methodology
7.3.1.1
ODP Models and Tweedie Models in Stan
ODP models can be speciﬁed in BUGS via the zero trick. Indeed the zero trick can
be used to deﬁne arbitrary likelihood function in BUGS (Lunn et al. 2000). However,
Stan does not accept the zero trick and we have not yet worked out how to make
a statement of the ODP model in Stan. In addition, Tweedie distributions are not
built-in distributions in Stan.
7.3.1.2
Other Non-parametric Bayesian Models
We have seen the power of basis expansion models. Other Bayesian non-parametric
models include Gaussian process models, Dirichlet process models etc. Further
research could be done on these models and their applications.
7.3.1.3
Copulas Comparison and Selection
As we mentioned in Sect. 6.4, the comparison of different copulas is ignored in that
chapter. The selection of a suitable copula could be based on the information criteria
or the tail dependence. The goodness-of-ﬁt for copulas is discussed in Genest et al.
(2009).
7.3.1.4
Distributional Approximation
In Sect. 3.4, we have brieﬂy reviewed variational Bayes methods, which are promis-
ing when dealing with large data sets. Other distributional approximation methods,
such as pragmatic expectation (Minka 2001), are discussed in Bishop (2006). These
methods deserve more attention in future research.
7.3.2
Actuarial Applications
7.3.2.1
Calendar Year Effect
The calendar year effect is not considered in this monograph. The obvious pattern
in Fig. 4.17 indicates a signiﬁcant calendar year effect. A possible approach is to
incorporate a calendar year covariate (see Sect. 4.5).

190
7
Epilogue
7.3.2.2
Stochastic Reserving Methods for Other Beneﬁts in WorkSafe
Victoria
Three beneﬁts in WorkSafe Victoria are investigated: the weekly beneﬁt, the doctor
beneﬁt and the hospital beneﬁt. These beneﬁts are chosen since they are stable and
less subject to changes in legislation than many others. It is desirable to propose
stochastic versions of other reserving methods in the PwC report (Simpson and
McCourt 2012) such as PPAC and PPCR.
7.3.2.3
One-Year Reserve Volatility
One key issue relating to the actual implementation of Solvency II is the estimation of
the one-year reserve volatility (or claims development results). This issue is discussed
in some recent literature, including Saluz et al. (2011), Christiansen and Niemeyer
(2014) and Saluz (2015).
References
Bishop, C. M. (2006). Pattern recognition and machine learning. New York: Springer.
Christiansen,M.,&Niemeyer,A.(2014).Thefundamentaldeﬁnitionofthesolvencycapitalrequire-
ment in Solvency II. ASTIN Bulletin, 44, 501–533.
Genest, C., Rémillard, B., & Beaudoin, D. (2009). Goodness-of-ﬁt tests for copulas: A review and
a power study. Insurance: Mathematics and Economics, 44, 199–213.
Joe, H. (2014). Dependence modeling with copulas. New York: Chapman & Hall.
Lunn, D. J., Thomas, A., Best, N., & Spiegelhalter, D. (2000). WinBUGS—A Bayesian modelling
framework: Concepts, structure, and extensibility. Statistics and Computing, 10, 325–337.
Minka, T. P. (2001). Expectation propagation for approximate Bayesian inference. In Proceedings
of the Seventeenth Conference on Uncertainty in Artiﬁcial Intelligence (pp. 362–369). Morgan
Kaufmann Publishers Inc.
Saluz, A. (2015). Prediction uncertainties in the Cape Cod reserving method. Annals of Actuarial
Science, 9, 239–263.
Saluz, A., Gisler, A., & Wüthrich, M. V. (2011). Development pattern and prediction error for the
stochastic Bornhuetter-Ferguson claims reserving method. ASTIN Bulletin, 41, 279–313.
Simpson, L., & McCourt, P. (2012). Worksafe Victoria actuarial valuation of outstanding claims
liability for the scheme as at 30 June 2012, Technical report. PricewaterhouseCoopers Actuarial
Pty Ltd.

Appendix A
Derivations
A.1
Example 2.3
Since E (xt) = E (xt−1) and Var (xt) = Var (xt−1), we can easily get
E (x1) = 0 and Var (x1) =
1
λ

1 −α2.
Hence, this autoregressive process is uniquely determined by the following two
distributions:
x1|α, λ ∼N

0,
1
λ

1 −α2

xt|xt−1, α, λ ∼N

αxt−1, 1
λ

, t = 2, 3, . . . , n.
A.1.1
The Joint Posterior Distribution
The joint posterior distribution of α and λ is
p (α, λ|xxx) ∝p (xxx|α, λ) p (α) p (λ)
∝p(xxx|α, λ)1
λ
= p (xn|xn−1, xn−2, . . . , x1, α, λ) p(xn−1, xn−2, . . . , x1|α, λ)1
λ
= p (xn|xn−1, α, λ) p (xn−1|xn−2, α, λ) · · · p (x1|α, λ) 1
λ
∝
√
λ exp

−λ
2(xn −xn−1)2
	
· · ·

λ

1 −α2
exp

−λ

1 −α2
2
x1
2

1
λ
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6
191

192
Appendix A: Derivations
= λ
n
2 −1
1 −α2 1
2
exp

−λ
2

(xn −αxn−1)2 + · · · + (x2 −αx1)2 +

1 −α2
x1
2
.
Thus,
p (α, λ|xxx) = h0λ
n
2 −1
1 −α2 1
2 exp

−λ
2 h (xxx, α)
	
,
where
h0 =
1
 ∞
0
 1
−1 λ
n
2 −1
1 −α2 1
2 exp

−λ
2h (xxx, α)

dαdλ
is called the normalizing constant and
h (xxx, α) = (xn −αxn−1)2 + (xn−1 −αxn−2)2 + · · · + (x2 −αx1)2 +

1 −α2
x1
2.
A.1.2
Two Marginal Posterior Distributions
The marginal posterior distribution of α is
p (α|xxx) =
∞

0
p (α, λ|xxx) dλ
∝
∞

0
λ
n
2 −1
1 −α2 1
2 exp

−λ
2 h (xxx, α)
	
dλ
=

1 −α2 1
2

 n
2


h(xxx,α)
2
 n
2
∝

1 −α2 1
2
h(xxx, α)
n
2 .
Thus,
p (α|xxx) = h1

1 −α2 1
2
h(xxx, α)
n
2 ,
where
h1 =
1

−1
h(xxx, α)
n
2

1 −α2 1
2 dα.

Appendix A: Derivations
193
The marginal posterior distribution of λ is
p (λ|xxx) =
1

−1
p (α, λ|xxx) dα
∝
1

−1
λ
n
2 −1
1 −α2 1
2 exp

−λ
2 h (xxx, α)
	
dα
∝λ
n
2 −1
1

−1
exp

−λ
2 h (xxx, α)
	
dα
≡π (λ) .
Thus p (λ|xxx) = π0π (λ), where π0 = 1/
∞
0
π (λ) dλ.
A.1.3
Full Conditional Distribution of λ
It is easy to note that the full conditional distribution of λ is given by
λ|xxx, α ∼Gamma
n
2, h (xxx, α)
2

.
So
ˆλ = E (λ|xxx) = E (E (λ|α,xxx) |xxx) = E

n
h (xxx, α)
xxx

=
1

−1
n
h (xxx, α) p(α|xxx)dα.
In Sect. 3.1.3 we show that the Rao-Blackwell estimate of ˆλ is just based on the
above argument.
A.2
Example 2.5
Consider a sample of size n from N

μ, σ 2
, denoted by xxx. We want to test H0 : μ =
μ0 versus H1 : μ ̸= μ0 with σ 2 unspeciﬁed.

194
Appendix A: Derivations
A.2.1
CLR and GLR
The conditional likelihood ratio (CLR) is
T C (xxx, θ) =
supμ̸=μ0 p(xxx|μ, σ 2)
supμ=μ0 p(xxx|μ, σ 2) = p(xxx|μ = ¯xxx, σ 2)
p(xxx|μ = μ0, σ 2) = exp

n(¯xxx −μ0)2
−2σ 2

.
Since the posterior predictive p-value, pB, is invariant under any strictly monotone
data-free transformation of a discrepancy variable, we can use n(¯xxx −μ0)2/σ 2 as the
CLR. Similarly, the generalized likelihood ratio (GLR), T G (xxx), can be calculated as
n(¯xxx −μ0)2/s2, where ¯xxx and s2 are the sample mean and the sample variance.
A.2.2
pB Using CLR
The posterior predictive p-value, pB, conditional on σ 2 is
pC
B

σ 2
= Pr

n
 ¯X −μ0
2
σ 2
≥n(¯xxx −μ0)2
σ 2
H0, σ 2

,
which depends on the choice of the conditional prior p(σ 2). Under the non-
informative prior, p(σ 2) ∝1/σ 2, the posterior distribution of σ 2 can be calculated
as
σ 2|xxx ∼ns2
0
χ2n
,
where s2
0 = n
i=1 (xi −μ0)2/n is the MLE of σ 2 under the null hypothesis H0.
We have the following equation:
pC
B = E

pC
B

σ 2
|xxx, H0

= E

Pr

n
 ¯X −μ0
2
σ 2
≥n(¯xxx −μ0)2
σ 2
μ0, σ 2,xxx
 μ0,xxx

= E
⎡
⎣Pr
⎛
⎝
n( ¯X−μ0)
2
σ 2
ns2
0
σ 2 /n
≥n(¯xxx −μ0)2
s2
0

μ0, σ 2,xxx
⎞
⎠

μ0,xxx
⎤
⎦.
(A.1)
Let:
u = n
 ¯X −μ0
2
σ 2
, v = ns2
0
σ 2 , T0 (xxx) = n(¯xxx −μ0)2
s2
0
.
Since u|μ0, σ 2,xxx ∼χ2
1 does not depend on σ or xxx, we have

Appendix A: Derivations
195
u|μ0,xxx ∼χ2
1 and (u|μ0,xxx) ⊥

σ 2|μ0,xxx

.
Similarly we have
v|μ0,xxx ∼χ2
n , (v |μ0,xxx)⊥(u| μ0,xxx),
 u
v
n
μ0,xxx

⊥(σ 2|μ0,xxx).
It follows, by continuation of (A.1), that
pC
B = E

Pr
 u
v
n
≥T0 (xxx)
μ0, σ 2,xxx
 μ0,xxx
	
= E
⎡
⎣
Pr

u
v
n ≥T0 (xxx) , σ 2|μ0,xxx

p

σ 2|μ0,xxx


μ0,xxx
⎤
⎦
=

σ 2
Pr

u
v
n ≥T0 (xxx) , σ 2|μ0,xxx

p

σ 2|μ0,xxx

p

σ 2|μ0,xxx

dσ 2
=

σ 2
Pr
 u
v
n
≥T0 (xxx) , σ 2
μ0,xxx

dσ 2
= Pr
 u
v
n
≥T0 (xxx)
μ0,xxx
 
σ 2
p

σ 2|μ0,xxx

dσ 2
= Pr

F1,n ≥T0 (xxx) |xxx, μ0

.
A.2.3
pB Using GLR
The posterior predictive p-value using GLR is
pG
B = Pr

n
 ¯X −μ0
2
s2
≥n(¯xxx −μ0)2
s2
μ0,xxx

= Pr

F1,n−1 ≥n(¯xxx −μ0)2
s2
μ0,xxx

= Pr

F1,n−1 ≥T G (xxx)

= Pr

tn−1 >

T G (xxx)

.
Notice that T G is a pivotal quantity, and pB based on GLR is identical to the classical
p-value based on the t-test.

196
Appendix A: Derivations
A.3
Calculation of Equation (2.5)
To calculate pB, we will ﬁrst verify that 1’s are uniformly placed given n1. It can be
shown that Pr

xk = 1
n
i=1 xi = n1

= n1/n, as follows:
Pr

xk = 1

n
 
i=1
xi = n1

=
1

0
Pr

xk = 1

n
 
i=1
xi = n1, θ

p

θ

n
 
i=1
xi = n1

dθ
=
1

0
Pr (xk = 1, ˙x−k = n1 −1|θ)
Pr
n
i=1 xi = n1
θ

p

θ

n
 
i=1
xi = n1

dθ
=
1

0
θ
 n−1
n1−1

θn1−1(1 −θ)(n−1)−(n1−1)
 n
n1

θn1(1 −θ)n−n1
p

θ

n
 
i=1
xi = n1

dθ
=
1

0
n1
n p

θ

n
 
i=1
xi = n1

dθ = n1
n ,
where ˙x−k = n
i=1 xi −xk.
Next,
pB =
10
 
i=0
Pr (R (i, 10 −i) ≤3) Pr(n1 = i|xxx).
(A.2)
It follows that
pB = Pr

r

x′
≤r (xxx)
xxx

=
1

0
⎛
⎝
10
 
i=0
3
 
j=1
Pr (R (i, 10 −i) = j) Pr (n1 = i|θ)
⎞
⎠p(θ|xxx)dθ
=
10
 
i=0
1

0
Pr (R (i, 10 −i) ≤3) p (n1 = i|θ) p (θ|xxx) dθ
=
10
 
i=0
1

0
Pr (R (i, 10 −i) ≤3) p (n1 = i, θ|xxx) dθ
=
10
 
i=0
Pr (R (i, 10 −i) ≤3) Pr(n1 = i|xxx).

Appendix A: Derivations
197
We next calculate p (n1|xxx) as follows:
p (n1|xxx) =
1

0
p(n1, θ|xxx)dθ
=
1

0
p (n1|θ,xxx) p(θ|xxx)dθ
=
1

0
p (n1|θ) p(θ|xxx)dθ
∝
1

0
 n
n1

θn1+5(1 −θ)n−n1+5dθ
∝
 n
n1

 (n1 + 6)  (n + 6 −n1)
∝(n1 + 5)! (n + 5 −n1)!
n1! (n −n1)!
,
which implies that
p (n1|xxx) =
(n1+5)!(n+5−n1)!
n1!(n−n1)!
n
i=0
(i+5)!(n+5−i)!
i!(n−i)!
.
Now the pmfs of R (i, 10 −i) and n1|xxx are know. Finally, according to Eq. (A.2),
pB can be calculated as
pB =
10
 
i=0
Pr (R (i, 10 −i) ≤3) Pr (n1 = i|xxx) = 0.1630.

Appendix B
Other Sampling Methods
B.1
A Simple Proof of the M-H Algorithm
The Metropolis-Hastings (M-H) algorithm is used to simulate a Markov chain whose
stationary distribution is the target distribution. This Markov chain has a certain
transactionmatrixdeterminedbythetargetdistributionandbyaproposaldistribution.
Let X be a ﬁnite sample space and π (x) a probability of interest on X (perhaps
speciﬁed up to an unknown normalizing constant). The M-H algorithm at the tth
iteration works as follows:
1. Propose a value from a proposal distribution g(x∗|xt−1), where xt−1 is the state
of x at the end of t −1 iteration or the initial value when t = 1.
2. Calculate the acceptance ratio
A

x∗, xt−1
=
π (x∗) g(xt−1|x∗)
π

xt−1
g(x∗|xt−1).
3. Accept x∗and set xt = x∗with probability A

x∗, xt−1
if A

x∗, xt−1
≤1.
Otherwise, reject x∗and set xt = xt−1.
The above M-H algorithm deﬁnes a Markov transaction matrix K, whose entry,
K

xt−1, xt
, has the following expression:
⎧
⎪⎨
⎪⎩
g

xt|xt−1
,
if xt ̸= xt−1, A

xt−1, xt
≥1
g

xt|xt−1
A

xt−1, xt
,
if xt ̸= xt−1, A

xt−1, xt
< 1
g

xt|xt−1
+  g

xt|xt−1 
1 −A

xt−1, xt
, if xt = xt−1,
where A

xt−1, xt
is the acceptance ratio. Note that the normalizing constant of π
cancels out in all calculations. It is easy to show that the following equation holds:
π

xt−1
K

xt−1, xt
= π

xt
K

xt, xt−1
.
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6
199

200
Appendix B: Other Sampling Methods
Thus
 
xt−1
π

xt−1
K

xt−1, xt
=
 
xt−1
π

xt
K

xt, xt−1
= π

xt  
xt−1
K

xt, xt−1
= π

xt
.
The above equation says that no matter what the starting value is, after many itera-
tions, the chance of being at xt is approximately π

xt
.
When X extends to the general space, many results are analogous to the results
for discrete state-space space chains as we have shown here (see Robert and Casella
2013). Hence the M-H algorithm can be applied to most target distributions.
B.2
Adaptive Rejection Sampling
In adaptive rejection sampling, we assume π (x) is log-concavity and denote h (x) =
log (π (x)). Suppose that h (x) and h′ (x) have been evaluated at k abscissae in
X : x1 ≤x2 ≤. . . ≤xk. Let Tk = {xi : i = 1, 2, . . . , k}.
Deﬁne the envelope function on Tk as exp uk (x) where uk (x) is a piecewise linear
upper hull formed from the tangents to h (x) at the abscissae in Tk. The tangents at
xi and xi+1 intersect at
zi = h (xi+1) −h (xi) −xi+1h′ (xi+1) + xih′ (xi)
h′ (xi) −h′ (xi+1)
, for i = 1, . . . , k −1.
We add z0 as the lower bound of X and zk as the upper bound of X . Then uk (x) =
h (xi) + (x −xi) h′ (xi) for x ∈

zi−1, zi

, i = 1, . . . , k.
Deﬁne the squeezing function on Tk as explk (x), where lk (x) is a piecewise
linear lower hull formed from the chords between adjacent abscissae in Tk. For
x ∈

x j, x j+1

, j = 1, 2, . . . , k −1,
lk (x) =

x j+1 −x

h

x j

+

x −x j

h

x j+1

x j+1 −x j
.
For x < x1 and x > xk, we deﬁne lk (x) = −∞.
Note that the envelope and squeezing functions are piecewise exponential func-
tions. The concavity of h (x) ensures that lk (x) ≤h (x) ≤uk (x) for all x in X . To
independently simulate n values from π (x) by the adaptive rejection sampling, we
perform the following steps until n values are accepted:
1. Initialisation step. Initialize the abscissae in Tk. If X is unbounded, make sure
h′ (x1) > 0 and h′ (xk) < 0. Calculate the functions uk (x) and lk (x). Also cal-
culate
sk (x) =
exp (uk (x))

X exp (uk (x)) dx .

Appendix B: Other Sampling Methods
201
2. Sampling step. Sample a value x∗from sk (x) (a piecewise exponential distribu-
tion) and a value u from U (0, 1). Accept it if u ≤exp [lk (x∗) −uk (x∗)]. Other-
wise, calculate h (x∗) and h′ (x∗), accept it if u ≤exp [h (x∗) −uk (x∗)].
3. Updating step. If h (x∗) and h′ (x∗) were evaluated at the sampling step, include
x∗in Tk to form Tk+1, relabel the elements of Tk+1 in ascending order, construct
functions uk+1 (x) ,lk+1 (x) and sk+1 (x) on the basis of Tk+1. Return to the sam-
pling step if n values have not yet been accepted.
In a Gibbs sampler, the full conditional distribution of a particular parameter θ can
be written as
h (θ|·) ∝
%
j
g j

θ j|Ω j

,
where g j(θ j|Ω j) is a function containing θ j, and Ω j is a set of other parameters and
data. When h (θ|·) is not a standard distribution but every g j(θ j|Ω j) is log-concave,
we can apply the adaptive rejection sampling to h (θ|·).
B.3
Slice Sampling
Slice sampling is another MCMC method. This was introduced by Neal (2003) and
it is one of the building blocks of BUGS. Slice sampling simulates a value uniformly
from underneath the pdf curve π (x) without need to reject any points. Here we give
a brief summary of how slice sampling works. The tth iteration of a slice sampling
consists of the following three steps:
1. Draw a value y from U

0, g

xt−1
(i.e., a vertical line under g

xt−1
), where
xt−1 is the ending value of t −1th iteration, and g is a function proportional to
the target distribution π (x). Deﬁne a horizontal slice S = {x : g (x) > y}.
2. Find a suitable interval I containing much of the slice S. Ideally, we can solve
g (x) > y and ﬁnd the exact slice. But this is not always feasible. Generally, we
use a “stepping out” procedure to ﬁnd an interval containing much of the slice.
We assume w as a typical length of a unit interval, m as an integer limiting the
length of interval to mw.
a. Randomlyplaceaunitintervaloflengthw around xt−1.Firstchooseavalueu
from U (0, 1), then set L = xt−1 −wu and R = L + w. The interval (L, R)
covers xt−1.
b. Expand the unit interval. Choose a value v from U (0, 1), then set the maxi-
mum number of unit intervals on the right side as the largest integral smaller
than mv, denoted by J, and the maximum number of unit intervals in the
left side as K = m −1 −J. Calculate the ending points of the expanded
interval as follows:
L = xt−1 −wu −wJ, R = L + w.

202
Appendix B: Other Sampling Methods
c. Adjust the interval. If J > 0 and y < g (L), repeat set the new L as L −w
and the new J as J −1 until J = 0 or y > g (L); if K > 0 and y < g (R),
repeat set the new R as R + w and the new K as K −1 until K = 0 or
y > g (R). Return the ﬁnal interval I = (L, R).
3. Draw a new value xt uniformly from S. Repeatedly draw a value uniformly from
an interval which is initially equal to I but shrinks each time when a draw is not
in the slice S, until a value is found within S ∩I. Note that the interval I found
from “stepping out” procedure may overlap S.
Neal (2003) gave detailed proof of slice sampling which is not discussed here.
References
Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31, 705–741.
Robert, C., & Casella, G. (2013). Monte Carlo statistical methods. New York: Springer Science &
Business Media.

Index
A
Adaptive rejection sampling, 41, 200
Akaike information criterion (AIC), 29
Archimedean copulas, 155
Auxiliary momentum, 45
B
Basis function, 118
Batch means method, 40
Bayesian estimate, 11
Bayesian information criterion (BIC), 29
Bayesian model assessment, 21
Bayesian over-dispersed Poisson model, 84,
101
Bayesian spline, 124, 130, 174
Bayes risk, 11
Bayes’ theorem, 10
Best linear unbiased prediction (BLUP), 62,
121
Bootstrap, 81, 161, 173
Bornhuetter-Ferguson method, 78
Brooks-Gelman-Rubin (BGR) ratio, 46
B-spline, 119, 126
BUGS, 46, 51
Burn-in, 38
C
Calendar period effect, 112
Central limit theorem, 48
Central posterior density region (CPDR), 11
Chain ladder method, 73, 77
Conditional likelihood ratio, 23
Conjugacy, 10
Conjugate prior, 10
Connectedness condition, 36
Convergence, 46
Copula, 153
Cross-validation, 28
Cumulative claims, 76
D
Detailed balance, 46
Deviance, 25
Deviance information criterion (DIC), 29,
54, 68, 95
Deviance residuals, 26
Diffuse priors, 18
Discrepancy measure, 22
Doctor beneﬁt, 103, 143, 170
Doodle, 53
E
Effective degrees of freedom, 120
Effective sample size, 49
Efﬁciency, 48
Elliptical copulas, 155
Empirical dependence, 168
Euler’s method, 44
F
Fisher information, 19
Flat priors, 18
Full conditional posterior distribution, 15,
125
Full rank smoother, 122
Full rank thin plate spline, 129
© Springer Nature Singapore Pte Ltd. 2018
G. Gao, Bayesian Claims Reserving Methods in Non-life Insurance with Stan,
https://doi.org/10.1007/978-981-13-3609-6
203

204
Index
G
Generalized double Pareto distribution, 125,
130
Generalized likelihood ratio, 23
Generalized linear model, 103, 171
Gibbs sampler, 41, 85
H
Hamiltonian Monte Carlo (HMC), 43
Hierarchical model, 32
Highest posterior density region (HPDR), 10
Hoerl curve, 117
Hospital beneﬁt, 170
Hyperparameters, 16
I
Improper, 12, 18
Incremental claims, 76
Incurred but not enough reported (IBNER),
75
Incurred but not reported claim (IBNR), 75
Independent risk, 101
Inference functions for margins estimator
(IFME), 161, 162
Information criteria, 28, 68, 132, 143
J
Jeffrey’s priors, 19
K
Kendall’s tau, 158
Knots, 118
Kullback-Leibler divergence, 59
L
Leapfrog method, 44
Leave-one-out
information
criterion
(LOOIC), 28, 30, 68
Likelihood, 9
Log-logistic curve, 133
Log point wise predictive density (lppd), 28
Loss function, 11
Low rank thin plate spline, 122, 129, 136
M
Mack’s model, 77, 79, 101
Marginal distribution, 10, 12
Marginal posterior distribution, 15
Markov chain, 36
Markov chain Monte Carlo (MCMC), 36
Mean ﬁeld assumption, 59
Mean squared error of prediction, 80
Metropolis accept step, 45
Metropolis-Hastings (M-H) algorithm, 37,
92, 199
Mixed effects model, 123
Mode approximation, 58
Monte Carlo simulation, 35
N
Natural cubic spline, 120, 135
Non-informative prior, 18, 86, 175
Normalizing constant, 16
Nuisance parameter, 24
O
OpenBUGS, 47, 51
Out-of-sample, 28
Over-dispersed Poisson (ODP) model, 79
Overﬁtting, 28
P
Payments per claim incurred (PPCI), 100,
103, 143
Pearson residuals, 25, 81
Penalized spline regression, 122
Pivotal quantity, 22
Posterior, 10
Posterior expected loss (PEL), 11
Posterior predictive distribution, 12
Potential scale reduction factor, 47
Precision parameter, 15
Predictive accuracy, 28
Prior, 9
Prior predictive distribution, 12
Proper, 12
Prudential margin, 75
R
Radial basis, 119
Random effects model, 61
Rao-Blackwell method, 42
Reference priors, 18
Rejection sampling, 36, 51
Residual sum of squares, 28
Reversible jump Markov chain Monte Carlo
(RJMCMC), 91, 92
Risk function, 11

Index
205
Risk margin, 169
Risk measure, 169
Run-off triangle, 75
S
Sampling distribution, 9
Shrinkage prior, 124
Sklar’s theorem, 154
Slice sampling, 51, 201
Smoother matirx, 120
Smoothing spline, 120
Spearman’s rho, 158
Spline regression, 122, 126
Stan, 46, 52, 57, 63, 106, 130, 138, 143, 165,
176
Stationary distribution, 36
Strong prior, 87, 106, 174
Structural dependence, 168
T
Tail dependence, 159
Tail factor, 90, 101, 143, 174
Taylor series expansion, 21
The No-U-Turn Sampler (NUTS), 46, 52
Thinning sample method, 40
Thin plate spline, 120
Transition matrix, 36
Tuning parameter, 37, 122
U
Ultimate claims, 76
Unconditional distribution, 12
Underﬁt, 28
V
Vague priors, 18
Variational Bayes, 52
W
Watanabe-Akaike
information
criterion
(WAIC), 30, 68
Weekly beneﬁt, 100
WinBUGS, 51
Within-sample, 28
WorkSafe VIC, 98, 143, 170

