TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation
Samuel G. M¨uller
University of Freiburg
muellesa@cs.uni-freiburg.de
Frank Hutter
University of Freiburg &
Bosch Center for Artificial Intelligence, Germany
fh@cs.uni-freiburg.de
Abstract
Automatic augmentation methods have recently become
a crucial pillar for strong model performance in vision
tasks.
While existing automatic augmentation methods
need to trade off simplicity, cost and performance, we
present a most simple baseline, TrivialAugment, that out-
performs previous methods for almost free. TrivialAugment
is parameter-free and only applies a single augmentation
to each image. Thus, TrivialAugment's effectiveness is very
unexpected to us and we performed very thorough exper-
iments to study its performance. First, we compare Triv-
ialAugment to previous state-of-the-art methods in a variety
of image classification scenarios. Then, we perform mul-
tiple ablation studies with different augmentation spaces,
augmentation methods and setups to understand the crucial
requirements for its performance. Additionally, we provide
a simple interface to facilitate the widespread adoption of
automatic augmentation methods, as well as our full code
base for reproducibility1. Since our work reveals a stag-
nation in many parts of automatic augmentation research,
we end with a short proposal of best practices for sustained
future progress in automatic augmentation methods.
Sample strength
Input image
Sample augmentation
and apply it
Figure 1: A visualization of TA. For each image, TA (uni-
formly) samples an augmentation strength and an augmen-
tation. This augmentation is then applied to the image with
the sampled strength.
Method
Search
CIFAR-10 CIFAR-100 SVHN ImageNet
Overhead ShakeShake
WRN
WRN
ResNet
AA
40 - 800×
98.0
82.9
98.9
77.6
RA
4 - 80×
98.0
83.3
99.0
77.6
Fast AA
1×
98.0
82.7
98.8
77.6
TA (ours)
0×
98.2
84.3
98.9
78.1
Table 1: TrivialAugment compares very favourably to
previous augmentation methods. In this table we sum-
marize some results from Table 2 and present augmentation
search overhead estimates.
1. Introduction
Data Augmentation is a very popular approach to in-
crease generalization of machine learning models by gen-
erating additional data. It is applied in many areas, such
as machine translation [4], object detection [6] or semi-
supervised learning [20]. In this work, we focus on the
application of data augmentation to image classification
[3, 12].
Image augmentations for image classification generate
novel images based on images in a dataset, which are likely
to still belong to the same classification category. This way
the dataset can grow based on the biases that come with the
augmentations. While data augmentations can yield consid-
erable performance improvements, they do require domain
knowledge. An example of an augmentation, with a likely
class-preserving behaviour, is the rotation of an image by
some small number of degrees. The image’s class is still
recognized by humans and so this allows the model to gen-
eralize in a way humans expect it to generalize.
Automatic augmentation methods are a set of methods
that design augmentation policies automatically. They have
been shown to improve model performance significantly
across tasks [2, 23, 20].
Automatic augmentation methods have flourished espe-
cially for image classification in recent years [2, 13, 14, 9]
with many different approaches that learn policies over aug-
1. https://github.com/automl/trivialaugment

mentation combinations.
The promise of this field is to
learn custom augmentation policies that are strong for a par-
ticular model and dataset. While the application of an aug-
mentation policy found automatically is cheap, the search
for it can be much more expensive than the training itself.
In this work, we challenge the belief that the resulting
augmentation policies of current automatic augmentation
methods are actually particularly well fit to the model and
dataset. We do this by introducing a trivial baseline method
that performs comparably to more expensive augmentation
methods without learning a specific augmentation policy
per task. Our method does not even combine augmentations
in any way. We fittingly call it TrivialAugment (TA).
The contributions of this paper are threefold:
• We analyze the minimal requirements for well-
performing automatic augmentation methods and pro-
pose TrivialAugment (TA), a trivial augmentation
baseline that poses state-of-the-art performance in
most setups. At the same time, TA is the most prac-
tical automatic augmentation method to date.
• We comprehensively analyze the performance of TA
and multiple other automatic augmentation methods in
many setups, using a unified open-source codebase to
compare apples to apples.
• We make recommendations on the practical usage of
automatic augmentation methods and collect best prac-
tices for automatic augmentation research. Addition-
ally, we provide our code for easy application and fu-
ture research.
2. Related Work
Many automatic augmentation methods have been pro-
posed in recent years with multiple different setups. Still,
all automatic augmentation methods we consider share one
property: They work on augmentation spaces that consist
of i) a set of prespecified augmentations A and ii) a set of
possible strength settings with which augmentations in A
can be called (in this work {0, . . . , 30}). One member of A
might, for example, be the aforementioned rotation opera-
tion, where the strength would correspond to the number of
degrees. Automatic augmentation methods now learn how
to use these augmentations together on training data to yield
a well-performing final classifier.
In this section, we provide a thorough overview of rel-
evant previous methods. As the compute requirements for
automatic augmentation methods can dominate the training
costs, we order this recount by the total cost of each method.
We begin with the first automatic augmentation method,
AutoAugment (AA) [1], which also happens to be the most
expensive, spending over half a GPU-year of compute to
yield a classifier on CIFAR-10. AA uses a recurrent neural
network (RNN), which is trained with reinforcement learn-
ing methods, to predict a parameterization of augmentation
policies. Reward is given for the validation accuracy of a
particular model trained on a particular dataset with the pre-
dicted policy. AA makes use of multiple sub-policies each
consisting of multiple augmentations, which in turn are ap-
plied sequentially to an input image. Additionally, augmen-
tations are left out with a specified probability. This allows
one sub-policy to represent multiple combinations of aug-
mentations. Since AA is costly it uses not the task at hand
for augmentation search, but a reduced dataset and a smaller
model variant.
The second most expensive method is Augmentation-
wise Sharing for AutoAugment (AWS) [19]. It builds on
the same optimization procedure as AA, but uses a simpler
search space. The search space consists of a distribution
over pairs of augmentations that are applied together. Dif-
ferent from AA, AWS learns the augmentation policy for
the last few epochs of training only. It does this on the full
dataset with a small model.
A very different approach, called Population-based Aug-
mentation (PBA) [9], is to learn the augmentation policy
online as the training goes. PBA does so by using multi-
ple workers that each use a different policy and are updated
in an evolutionary fashion. It uses yet another policy pa-
rameterization: a vector of augmentations where each aug-
mentation has an attached strength and leave-out probabil-
ity. From this vector augmentations are sampled uniformly
at random and applied with the given strength or left out,
depending on the leave-out probability.
Another method based on multiple parallel workers is
Online Hyper-Parameter Learning for Auto-Augmentation
(OHL) [14]. Here, the policy is defined like for AWS and
its parameters are trained using reinforcement learning. The
major difference with AWS is that its reward is the accu-
racy on held-out data after a part of training like for PBA,
rather than final accuracy. As an additional way of tuning
the neural network weights in the parallel run, the weights
of the worker with maximal accuracy are used to initialize
all workers in the next part of training.
Adversarial AutoAugment (Adv. AA) [22] is another
slightly cheaper method that uses multiple workers and
learns the augmentation policy online. It trains only a sin-
gle model, though. Here, a single batch is copied to eight
different workers and each worker applies its own policies
to it, similar to the work by Hoffer et al. [10]. The worker
policies are sampled at the beginning of each epoch from
a policy distribution. The policy distribution has a simi-
lar form to that of AA. After each epoch, Adv. AA makes a
reinforcement-learning based update and rewards the policy
yielding the lowest accuracy training accuracy, causing the
policy distribution to shift towards progressively stronger
augmentations over the course of training.

Recently, Cubuk et al. proposed RandAugment (RA) [2].
It is much simpler, but only slightly cheaper, compared
to the previous methods.
RA only tunes two scalar pa-
rameters for each task: (i) a single augmentation strength
m ∈{0, . . . , 30} which is applied to all augmentations and
(ii) the number of augmentations to combine for each image
n ∈{1, 2, 3}. RA therefore reduces the number of hyper-
parameters from all the weights of an RNN (for AA) or a
distribution over more than a thousand augmentation com-
binations (for AWS and OHL) to just two. This radical sim-
plification, contrary to expectations, does not hurt accuracy
scores compared to many other methods. The authors give
indication that the strong performance might be due to the
fact that n and m are tuned for the exact task at hand and not
for a pruned dataset, as is done, for example, in AA. The big
downside of RA is that it ends up performing an exhaustive
search over a set of options for n and m incurring up to 80×
overhead over a single training2.
Fast AutoAugment (Fast AA) [13] is the cheapest of
the learned methods.
It is based on AA, but does not
directly search for policies with strong validation perfor-
mance. Rather, it searches for augmentation policies by
finding well-performing inference augmentation policies
for networks trained on a split of raw, non-augmented, im-
ages. All inference augmentations found on different splits
are then joined to build a training time augmentation policy.
The intuition behind this can be summarized as follows: If a
neural network trained on real data generalizes to examples
augmented with some policy then this policy produces im-
ages that lie in the domain of the class, as approximated by
the neural network. The augmentations therefore are class-
preserving and useful. This objective stands in contrasts
to the approach followed by Adv. AA. Fast AA tries to find
augmentations that yield high accuracy when applied to val-
idation data, while Adv. AA tries to find augmentations that
yield low accuracy when applied to training data.
Finally, in an unpublished arXiv paper, Lingchen et al.
[15] very recently suggested UniformAugment (UA), which
works almost like RA. Unlike RA, it fixes the number of
augmentations to N = 2 and drops each augmentation with
a fixed probability of 0.5. Furthermore, the strength m is
sampled uniformly at random for each applied operation.
In contrast to all above methods methods, we propose
TrivialAugment (TA), an augmentation algorithm that is
parameter-free like UA, but even simpler. At the same time,
TA performs better than any of the comparatively cheap
augmentation strategies, making it the most practical auto-
matic augmentation method to date.
Different from all of the work discussed above, which
improves final in-distribution test performance with aug-
mentation strategies, AugMix [8] aims to improve model
robustness by combining multiple augmentations in appli-
cation chains, mixing their outputs, and applying a consis-
Figure 2: An exemplary visualization of a 2-D dataset with
two classes, crosses and circles, separated by a decision
boundary, the dotted line. The colored crosses represent
deterministic augmentations of the cross class. TA now uni-
formly samples from all crosses.
tency loss to several augmented images. The only metric we
evaluated for which AugMix was evaluated, too, is ResNet-
50 performance on the ImageNet test set. Here, TA outper-
forms AugMix.
3. TrivialAugment
In this section, we present the simplest augmentation al-
gorithm we could come up with that still performs well:
TrivialAugment (TA). TA employs the same augmentation
style that previous work [2, 15] used: An augmentation is
defined as a function a mapping an image x and a discrete
strength parameter m to an augmented image. The strength
parameter is not used by all augmentations, but most use it
to define how strongly to distort the image.
TA works as follows.
It takes an image x and a set
of augmentations A as input. It then simply samples an
augmentation from A uniformly at random and applies this
augmentation to the given image x with a strength m, sam-
pled uniformly at random from the set of possible strengths
{0, . . . , 30}, and returns the augmented image. We outline
this very simple and parameter-free procedure as pseudo-
code in Algorithm 1 and visualize it in Figure 1. We em-
phasize that TA is not a special case of RandAugment (RA),
since RA uses a fixed optimized strength for all images
while TA samples this strength anew for each image.
While previous methods used multiple subsequent aug-
mentations, TA only applies a single augmentation to each
image.
This allows viewing the distribution of the TA-
augmented dataset as an average of the |A| data distribu-
tions generated by each of the augmentations applied to the
full dataset. In Figure 2 we visualize this notion for deter-
ministic augmentations without a strength parameter. Un-
2. In the original setups, the authors also used a different choice of n
and m for the search on each task. This can be hard to do for new tasks or
with less intuition for a task.

Algorithm 1 TrivialAugment Procedure
1: procedure TA(x: image)
2:
Sample an augmentation a from A
3:
Sample a strength m from {0, . . . , 30}
4:
Return a(x, m)
5: end procedure
like previous work, we do not generate complex distribu-
tions out of stochastic combinations of augmentation meth-
ods, but simply mean the data distributions of the augmen-
tations applied to the given dataset.
4. Experiments
In this section, we empirically demonstrate TA's surpris-
ingly strong performance, as well as its behaviour across
many ablation settings. In all non-ablation experiments we
use either the RA augmentation space (RA), i.e., the set of
augmentations and their strength parameterization from the
RA paper [2], or the wide augmentation space (Wide) for
TA. We list the augmentations and their arguments for all
augmentation spaces in the appendix in Table 8. We run
each experiment ten times, if not stated otherwise. In ad-
dition to the average over runs we report a confidence in-
terval, which will contain the true mean with probability
p = 95%, under the assumption of normal distributed accu-
racies. In our code we provide a function to compute this
interval. Results that lie within the confidence interval of
the best performer for each task are typeset in bold font.
We evaluate our method on five different datasets. i)
CIFAR-10 and CIFAR-100 [11] are standard datasets for
image classification and each contain 50K training images.
We trained Wide-ResNets [21] as well as a ShakeShake
model [5]. We follow previous work [1, 2] with our setup.
ii) SVHN [17] consists of images of house numbers.
It
comes with a core set of 73K training images, but offers an
additional 531K simpler images as extension of the dataset.
We perform experiments with and without the additional
images on a Wide-ResNet-28-10. iii) Finally, we perform
experiments on ImageNet, a very large image classification
corpus with 1000 classes and over 1.2 million images. This
experiment is particularly interesting, since it was shown
previously that there are augmentations, such as cutout, that
do not generalize well to ImageNet. We train a ResNet-
50 [7] following the setup of [1]. We use warmup and 32
workers due to cluster limitations, which is less than [1].
We scale the learning rate appropriately. See Appendix A
for more details.
4.1. Comparison to State-of-the-Art
It is non-trivial to compare automatic augmentation
methods fairly. We therefore compare our method with the
previous state-of-the-art in three different setups.
In Section 4.1.1, we follow the majority of previous work
[1, 2, 9, 13, 15] and perform a comparison with other meth-
ods that use the same model and training pipeline. This
setup allows for different search costs of different methods
and compares methods with the same inference and training
costs.
In Section 4.1.2, we compare in a similar way as above,
but against reproductions of other methods in our codebase.
This avoids confounding factors, making sure that the meth-
ods, and not setup details, explain the differences between
results. We reproduced a total of four other methods in our
codebase, including the cheapest three previous methods.
In Section 4.1.3, we compare the total cost of each
method, both search and model training, with the final ac-
curacy. This comparison has the upside that it can consider
work with different pipelines and models more fairly.
4.1.1
Comparison to Published Results
In Table 2, we compare TA to all methods that used the
setup of AutoAugment [1] or a very similar setup in terms
of hyper-parameters, number of epochs and models.
TA performs as well or better than previous methods in
almost all tasks. The SVHN datasets are the only exception,
with RA performing somewhat better. This might, however,
be due to our training pipeline, since, as we show in Sec-
tion 4.1.2, we were not able to reproduce RA's performance
for SVHN Core with our pipeline and the original training
pipeline is not available.
For ImageNet, TA outperformed all other methods in
terms of both top-1 accuracy and top-5 accuracy. We used
an image width of 244 like RA [2], but even with a lower
width of 224 (as was used for AA [1]), TA outperformed the
previously best methods (with a 77.97 ± .21 top-1 accuracy
and 93.98 ± .07 top-5 accuracy; not listed in the table).
In this comparison, we cannot compare to all previous
methods, since some use different setups. The best-known
setup we had to leave out is Adv. AA. Therefore, we per-
form an extra set of experiments following its setup closely.
Adv. AA uses eight times the compute for its final train-
ing compared to other methods and therefore has a signif-
icant advantage compared to other methods. Adv. AA is
based on batch augmentation [10], where a set of work-
ers in a data parallel setting each compute gradients with
respect to the same batch of examples, but apply differ-
ent augmentations to the images in it. We re-created this
setup, including all hyper-parameters and batch augmenta-
tion, for TA. In Table 3, we compare TA with Adv. AA with
a Wide-ResNet-28-10 and a ShakeShake-26-2x96d for both
CIFAR-10 and CIFAR-100. We show that TA's trivial uni-
form sampling of a single augmentation achieves the same
performance as their complex (and unavailable) reinforce-
ment learning pipeline.

Default
PBA
Fast AA
AA
RA
UA
TA (Wide)
CIFAR-10
Wide-ResNet-40-2
96.16 ± .08
-
96.4
96.3
-
96.25
96.32 ± .05
Wide-ResNet-28-10
97.03 ± .07
97.4
97.3
97.4
97.3
97.33
97.46 ± .06
ShakeShake-26-2x96d
97.54 ± .07
98.0
98.0
98.0
98.0
98.10
98.21 ± .06
PyramidNet
97.95 ± .05
98.5
98.5
98.3
98.5
98.5
98.58 ± .04
CIFAR-100
Wide-ResNet-40-2
78.42 ± .31
-
79.4
79.3
-
79.01
79.86 ± .19
Wide-ResNet-28-10
82.22 ± .25
83.3
82.7
82.9
83.3
82.82
84.33 ± .17
ShakeShake-26-2x96d
83.28 ± .14
84.7
85.4
85.7
-
85.00
86.19 ± .15
SVHN Core
Wide-ResNet-28-10
97.12 ± .05
-
-
98.0
98.3
-
98.11 ± .03
SVHN
Wide-ResNet-28-10
98.67 ± .02
98.9
98.8
98.9
99.0
-
98.9 ± .02
ImageNet
ResNet-50
77.20 ± .32
(93.43 ± .11)
-
77.6
(93.7)
77.6
(93.8)
77.6
(93.8)
77.63
(-)
78.07 ± .27
(93.92 ± .09)
Table 2: The average test accuracies from ten runs, besides for ImageNet, where we used five runs. The 95% confidence
interval is noted with ±. The trivial TA is in all benchmarks among the top-performers. The only exception is the comparison
to RA's performance on the SVHN benchmarks, but this difference was non-existent in our reimplementation in 4.1.2.
Adv. AA
TA (Wide)
CIFAR-10
Wide-ResNet-28-10
98.10 ± .15
98.04 ± .06
ShakeShake-26-2x96d
98.15 ± .12
98.12 ± .12
CIFAR-100
Wide-ResNet-28-10
84.51 ± .18
84.62 ± .14
ShakeShake-26-2x96d
85.90 ± .15
86.02 ± .13
Table 3: A comparison of TA with Adv. AA in the aug-
mented batch setting on a Wide-ResNet-28-10. We report
the average over five runs.
We conclude from this section that, for almost all consid-
ered benchmarks across datasets, models and even the way
augmentations are applied, TA is among the top-performing
methods.
4.1.2
Comparison of Reproduced Results in a Fixed
Training Setup
While in the previous section, we tried to mitigate con-
founding factors by comparing results obtained with very
similar setups with each other, in this section, we go one
step further. We reproduce the results of four methods and
compare our baseline method with these reproductions in
order to yield a true apples-to-apples comparison.
As we present a very cheap and simple augmentation
method we picked RA, Fast AA and UA as other cheap
and simple augmentation methods for our comparison. Ad-
ditionally, we compare to AA, as an important, common
baseline. Moreover, for all of these methods relevant infor-
mation for reproduction was published3.
For RA, AA and Fast AA we used the published policies
and did not search for an augmentation policy from scratch.
We based both our RA and AA implementations on a pub-
lic codebase4 by the authors of both RA and AA that im-
plements AA for the CIFAR datasets. Likewise, for Fast
AA we based our implementation on a public codebase.
No code is published for UA, and there are multiple hyper-
parameters missing in the paper; in these cases, we used the
hyper-parameters from RA. For our reproduction of UA,
we also adopted the same discretization of the augmenta-
tion strengths into 31 values used by the other methods. In
addition to the original augmentation space of UA we also
perform experiments with the RA augmentation space.
We reran experiments for CIFAR-10, CIFAR-100 and
SVHN Core, and present the results in Table 4. For each
method we ran the benchmarks included in the original
work. Generally, we could reproduce most results or even
improve upon published results. The only severe exception
is RA for which we tried multiple changes to the setup, but
were not able to reach their published scores – neither for
CIFAR nor for SVHN Core.
In this evaluation, TA (Wide) performed best across all
methods for each benchmark with a Wide-Resnet-28-10,
and TA (RA) performed best for both Wide-Resnet-40-2
benchmarks.
In addition to the reproductions of published policies, we
applied RandAugment to the Wide-ResNet-40-2 on CIFAR-
3. See Table 12 in the appendix for an overview of the published mate-
rials of different methods
4. https://github.com/tensorflow/models/tree/
fd34f711f319d8c6fe85110d9df6e1784cc5a6ca/
research/autoaugment

10, which was originally not considered in the RA pa-
per.
We therefore had to search for a policy first.
De-
pending on the task, Cubuk et al. [2] considered differ-
ent subsets of the full range of the augmentation strengths
M ⊂{1, . . . , 30} and the number of consecutive augmen-
tations N ⊂{1, . . . , 3}. In order to avoid missing the best
candidates and to not require human intuition we searched
on all 90 resulting combinations of RA's parameters. We
split up a validation set of 10000 examples like in the orig-
inal RandAugment method to evaluate the settings.
We
then picked the best setting and compared it to TA. Table
5 clearly shows that TA performs better than the costly RA
setup, even though the RA setup in total required 91 full
trainings, compared to a single training for TA.
Finally, we consider three more evaluations in the ap-
pendix: (i) We show that TA performs comparably or better
on the same augmentation space with other automatic aug-
mentation methods (see Appendix B), (ii) we show that TA
generalizes to more peculiar datasets (see Appendix C) and
(iii) we show TA's effectiveness with the EfficientNet Ar-
chitecture [18] (see Appendix D).
WRN-28-10
CIFAR-10
CIFAR-100
AA
97.31 ± .22 (-.09)
82.91 ± .41 (+.01)
FAA
97.43 ± .09 (+.13)
83.27 ± .13 (+.57)
RA
97.12 ± .14 (-.18)
83.1 ± .32 (-.20)
UA (UA)
97.46 ± .14 (+.13)
83.08 ± .27 (+.26)
UA (RA)
97.44 ± .09
83.36 ± .18
TA (RA)
97.46 ± .09
83.54 ± .12
TA (Wide)
97.46 ± .06
84.33 ± .17
(a)
WRN-40-2
CIFAR-10
CIFAR-100
AA
96.38 ± .10 (+.08)
79.66 ± .17 (+.36)
FAA
96.39 ± .06 (-.01)
79.79 ± .21 (+.39)
UA (UA)
96.42 ± .04 (+.17)
79.74 ± .15 (+.73)
UA (RA)
96.45 ± .06
79.95 ± .20
TA (RA)
96.62 ± .09
79.99 ± .16
TA (Wide)
96.32 ± .05
79.86 ± .19
(b)
WRN-28-10
SVHN Core
AA
97.99 ± .06 (-.01)
RA
98.06 ± .04 (-.24)
TA (RA)
98.05 ± .02
TA (Wide)
98.11 ± .03
(c)
Table 4: A reproduction of the results of previous work with
a Wide-ResNet-28-10 on CIFAR (a) and SVHN Core (c),
and with a Wide-ResNet-40-2 on CIFAR (b). We report the
relative performance difference to the published results in
parentheses.
Method
Acc.
Brute-Force RA
96.42 ± .09
TA (RA)
96.62 ± .09
Table 5: Average over ten runs on CIFAR-10 with a Wide-
ResNet-40-2. TA performs better than the over 80-times
more expensive exhaustive search over RA's parameters.
4.1.3
Comparison by Total Compute Costs
In the previous sections, we compared different augmenta-
tion methods for a fixed training setup. We now consider the
other extreme, comparing all methods across models and
setups by their compute requirements.
In Figure 3, we plot this comparison for many CIFAR-
100 setups across the literature. The question this plot an-
swers is: given some compute budget, what method should
we choose for the best final accuracy? For this plot, we used
the accuracy numbers published in the literature and esti-
mated the compute costs in RTX 2080 Ti GPU-hours. See
Appendix E for a detailed account of the information used
to calculate the compute cost approximations for all setups.
We had to restrict the set of models we considered to the set
of models for which we know from our experiments how
expensive they are to run, namely all Wide-ResNet setups
and the ShakeShake-26-2x96d. We tried to be as conserva-
tive as possible regarding the compute requirements of other
methods, to not give TA an unfair advantage.
In the figure, for all considered budgets, TA and its vari-
10
100
1,000
78
80
82
84
86
GPU hours
Accuracy (%)
AA
Fast AA
RA
AWS
UA
AWS x8
TA (RA)
Adv. AA x8
TA (Wide)
TA (Wide) x8
Figure 3: Comparison of the final test accuracy on CIFAR-
100 in comparison to RTX2080ti GPU-hours compute in-
vested for augmentation search and final model training
across a set of models. Methods marked with x8 use batch
augmentations[10].

Augmentation space
SVHN Core
CIFAR-10
Full
97.63 ± .06
97.24 ± .03
AA
98.04 ± .02
97.47 ± .11
AA - {Invert}
97.97 ± .08
97.55 ± .06
RA
98.05 ± .02
97.46 ± .09
Wide
98.11 ± .03
97.46 ± .06
UA
98.06 ± .04
97.42 ± .07
OHL
98.10 ± .02
97.45 ± .05
Table 6: Evaluation of TA on SVHN Core and CIFAR-10
with a set of 7 different augmentation spaces. Note that
RA = AA −{SamplePairing, Invert, Cutout} and UA =
AA −{SamplePairing}.
ant with augmented batch (TA x8) perform among the best
methods. TA also has a clear benefit compared to the popu-
lar cheap methods Fast AA and RA for all compute budgets;
finally, it is dramatically cheaper than AA.
4.2. Understanding the Minimal Requirements of
TA
While so far, we have demonstrated that in many circum-
stances TA's approach of only using a single augmentation
per image is enough or yields even better performance than
more complicated methods, in this section we will dissect
other properties of TA.
We first analyse how TA behaves across augmentation
spaces from the literature.
We then look at its perfor-
mance after we apply random changes to its augmentation
space. Finally, we consider sets of different augmentation
strengths from which TA samples.
4.2.1
TA with Different Hand-Picked Augmentation
Spaces
For this evaluation, we carefully reimplemented the aug-
mentation spaces of AA, UA and OHL, besides the one of
RA. Additionally, we consider a larger augmentation space
(Full), which is a super set of AA, and additionally contains
a blur, a smooth, a horizontal and a vertical flip. Especially
the vertical flip is likely not useful for very many classifica-
tion tasks. See Table 8 in the appendix for an overview of
the augmentation spaces.
Table 6 indeed shows that TA performs worse on the full
augmentation space than on all other augmentation spaces
for a Wide-ResNet-28-10 on both SVHN Core and CIFAR-
10.
We also included another augmentation space not con-
sidered in the previous literature: a variant of the AA aug-
mentation space, where we removed the extreme invert op-
eration, which maps each pixel x to 255−x. We can see that
this augmentation space performs very well for CIFAR-10,
but not great for SVHN Core. This aligns well with ob-
servations made by earlier work, indicating that the invert
4
8
10
11
12
13
14
95.75
96
96.25
96.5
96.75
Number of Augmentations
Accuracy (%)
Figure 4: The performance of WRN-40-2 models depend-
ing on the size of sampled subsets of the RA augmentations
on CIFAR-10. We performed 10 evaluations per subset size.
augmentation fosters generalization on SVHN, but not on
the other datasets[1]. A peculiarity of the OHL augmenta-
tion space is that it only uses three strengths, unlike all other
methods which consider 31 strengths. Interestingly, this is
not harmful and OHL yields the best score for SVHN Core.
We can see that the performance of TA is rather stable
between augmentation spaces, but still there seems to be
room for improvement by a more sophisticated method to
choose the augmentation space for TA depending on the
task.
4.2.2
TA's Behavior With Randomly Pruned Augmen-
tation Spaces
While we assessed performance with different hand-crafted
augmentation spaces above, now we want to analyze how
performance is impacted if we only use random subsets of
the 14 augmentations in the RA augmentation space (which
we used in the other experiments unless otherwise stated).
In Figure 4, we analyze the performance and its variance
for multiple augmentation subset sizes for a Wide-ResNet-
40-2 on CIFAR-10. We performed 10 evaluations per sam-
ple size, where in each evaluation we picked a random
sample of augmentations. While performance decreases as
fewer and fewer augmentations are considered, we can see
that it drops very slowly. We can throw away 4 of 14 aug-
mentations and still obtain performance close to the original
performance. Another trend is that with fewer augmenta-
tions the variance increases. This is likely due to the ran-
domness of the subset choice per run, which increases for
smaller subsets.
4.2.3
The Impact of the Set of Strengths on TA's Per-
formance
Before, we mostly considered the impact of different sets
of augmentations; now we consider the other component of
the augmentation space: the set of strengths.

Strengths
CIFAR-10
CIFAR-100
SVHN Core
{30}
97.45 ± .05
82.98 ± .22
98.16 ± .03
{0, 30}
97.51 ± .08
83.46 ± .10
98.02 ± .02
{0, 15, 30}
97.46 ± .06
83.43 ± .24
98.04 ± .03
{0, . . . , 30}
97.46 ± .09
83.54 ± .12
98.05 ± .02
Table 7: A comparison of the performance of TA (RA) on
different datasets with a Wide-ResNet-28-10 using different
subsets of strengths.
In Table 7, we analyze the performance of TA with a
Wide-ResNet-28-10 and different subsets of the original set
of possible strengths {0, . . . , 30} on the RA augmentation
space. We can see that the CIFAR-10 setup seems to be
relatively agnostic to the set of strengths. Performance on
CIFAR-100, on the other hand, is very negatively impacted
by choosing the subset {30}. In general, performance im-
proves on CIFAR-100 with larger sets. For SVHN Core, the
opposite is the case: performance improves when only con-
sidering {30}. A reason for this could be that the majority
of the augmentations are color based and changing the col-
ors of a single-color background and a single-color number
drastically, still in most cases yields valid house numbers.
Another observation we made is that it does not matter
so much for any setup whether we reduce to three or just
two augmentation strengths, compared to all 31. This seems
to point towards the importance of a mixture of strong
and weak augmentations. At the same time three differ-
ent strengths, compared to 31, seem to be enough for these
settings.
5. Automatic Augmentation Methods in
Practice
While there are many expensive or hard to reproduce
automatic augmentation methods, it is important that aug-
mentation methods are practical: the impact of automatic
augmentation methods unfolds in the application to new se-
tups and problems. We evaluated many different settings
and augmentation methods and we would like to pass on
the gained knowledge.
First, we have compiled a short summary of learnings for
the application of augmentation methods in Appendix F.
Second, in addition to our full codebase, we provide
a simple one-file python library that implements the more
practical augmentation methods: RA, UA and TA. It even
allows choosing from all augmentation spaces considered
in this work. For example, to get an image augmenter for
TA and transform a PIL image img, one can call
1 aug = TrivialAugment(n,m)
2 augmented_img = aug(img)
6. Best Practices Proposal for Research
We found that it is difficult to reimplement many of the
published methods, see Table 12 in the appendix. We also
found that many methods performed similarly to the simple
TA baseline, when we follow their setup. Here, we com-
pile a short bullet point list of best practices we believe are
important for sustainable research in this field.
• Share code as much as possible for easy entry of be-
ginners and to make sure that setups are similar across
papers. Otherwise, differences between the actual im-
plementation and its description in the paper can im-
pair reproducibility.
• Compare fairly to other methods and baselines with
the same setup, train budget and augmentation space,
or reproduce results of previous methods in your setup
and mention differences.
• Report confidence intervals to discern “outperform-
ing” from “performing comparably”.
7. Limitations
While we could not find settings where TA failed for im-
age classification, we found that TA does not work out-of-
the-box for object detection setups and also needs tuning to
work for this task. So far, we can only wholeheartedly rec-
ommend the use of TA for image classification; its applica-
tion to other computer vision tasks requires further study.
8. Conclusion
Most of the approaches considered as automatic aug-
mentation methods are complicated. In this work, we pre-
sented TA, a very simple augmentation algorithm from
which we can learn three main things.
First, TA teaches us about a crucial baseline missing for
automatic augmentation methods.
Second, TA teaches us to never overlook the simplest so-
lutions. There are a lot of complicated methods to automat-
ically find augmentation policies, but the simplest method
was so-far overlooked, even though it performs comparably
or better.
Third, randomness in the chosen strengths appears to be
very important for good performance.
Acknowledgements
We want to thank Ildoo Kim for his open-source code-
base which ours forks from, and the reviewers for their in-
sightful comments. We acknowledge funding by the Robert
Bosch GmbH and the European Research Council (ERC)
under the European Union Horizon 2020 research and in-
novation programme through grant no. 716721.

References
[1] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-
van, and Quoc V Le. Autoaugment: Learning augmentation
strategies from data. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
113–123, 2019.
[2] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le.
Randaugment:
Practical automated data augmenta-
tion with a reduced search space.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops, pages 702–703, 2020.
[3] Terrance DeVries and Graham W. Taylor. Improved regular-
ization of convolutional neural networks with cutout, 2017.
[4] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. Data
augmentation for low-resource neural machine translation.
In Proceedings of the 55th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 2: Short Pa-
pers), pages 567–573, 2017.
[5] Xavier Gastaldi. Shake-shake regularization, 2017.
[6] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr
Doll´ar, and Kaiming He. Detectron. https://github.
com/facebookresearch/detectron, 2018.
[7] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016.
[8] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph,
Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A
simple data processing method to improve robustness and
uncertainty. Proceedings of the International Conference on
Learning Representations (ICLR), 2020.
[9] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter
Abbeel. Population based augmentation: Efficient learning
of augmentation policy schedules. In International Confer-
ence on Machine Learning, pages 2731–2741. PMLR, 2019.
[10] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten
Hoefler, and Daniel Soudry. Augment your batch: Improving
generalization through instance repetition. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), June 2020.
[11] Alex Krizhevsky et al. Learning multiple layers of features
from tiny images. 2009.
[12] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-
sification with deep convolutional neural networks.
In P.
Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Wein-
berger, editors, Proceedings of the 26th International Con-
ference on Advances in Neural Information Processing Sys-
tems (NeurIPS’12), pages 1097–1105, 2012.
[13] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and
Sungwoong Kim.
Fast autoaugment.
In H. Wallach, H.
Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R.
Garnett, editors, Advances in Neural Information Processing
Systems, volume 32, pages 6665–6675. Curran Associates,
Inc., 2019.
[14] Chen Lin, Minghao Guo, Chuming Li, Xin Yuan, Wei
Wu, Junjie Yan, Dahua Lin, and Wanli Ouyang.
Online
hyper-parameter learning for auto-augmentation strategy. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV), October 2019.
[15] Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari,
Mina Rafi Nazari, Jaspreet Singh Sambee, and Mario A.
Nascimento. Uniformaugment: A search-free probabilistic
data augmentation approach, 2020.
[16] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient de-
scent with warm restarts. In Proceedings of the International
Conference on Learning Representations (ICLR’17), 2017.
[17] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bis-
sacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural
images with unsupervised feature learning. In NIPS Work-
shop on Deep Learning and Unsupervised Feature Learning
2011, 2011.
[18] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model
scaling for convolutional neural networks. In International
Conference on Machine Learning, pages 6105–6114. PMLR,
2019.
[19] Keyu Tian, Chen Lin, Ming Sun, Luping Zhou, Junjie
Yan, and Wanli Ouyang.
Improving auto-augment via
augmentation-wise weight sharing. Advances in Neural In-
formation Processing Systems, 33, 2020.
[20] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and
Quoc Le. Unsupervised data augmentation for consistency
training. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information
Processing Systems, volume 33, pages 6256–6268. Curran
Associates, Inc., 2020.
[21] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-
works. In In Proceedings of BMCV’16, 2016.
[22] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong.
Adversarial autoaugment.
In International Conference on
Learning Representations, 2020.
[23] Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin,
Jonathon Shlens, and Quoc V Le. Learning data augmenta-
tion strategies for object detection. In European Conference
on Computer Vision, pages 566–583. Springer, 2020.

A. Training Settings
For all setups we normalize the images by training set
mean and standard deviation after the application of all aug-
mentations, besides a final cutout, if applicable.
A.1. CIFAR
Following previous work we apply the vertical flip and
the pad-and-crop augmentations and finally a 16 pixel
cutout [3] after TA or generally any augmentation method.
We trained Wide-ResNet models [21] in the Wide-ResNet-
40-2 and the larger Wide-ResNet-28-10 settings. We trained
these models for 200 epochs using SGD with Nesterov Mo-
mentum and a learning rate of 0.1, a batch size of 128, a
5e-4 weight decay, cosine learning rate decay [16].
We trained ShakeShake-26-2x96d for 1600 epochs using
SGD with Nesterov Momentum, a learning rate of 0.01, a
batch size of 128, 1e-3 weight decay and a cosine learning
rate decay.
For the augmented batch setups we followed Zhang et al.
[22]. We used the settings above for the Wide-ResNet-28-
10 evaluations. And like Zhang slightly different settings
for ShakeShake. We use 600 epochs, with a 0.2 learning
rate and a 1e-4 weight decay.
A.2. SVHN
Unlike for CIFAR we do not apply extra augmentations
for SVHN, besides a final 16 pixel cutout [3]. For the full
dataset we trained for 160 epochs using SGD with Nesterov
Momentum of 0.9, a learning rate of 0.005, a batch size of
128, a 1e-3 weight decay and cosine learning rate decay.
For SVHN Core we train with the same settings, except that
we trained for 200 epochs and used a larger weight decay
of 5e-3.
A.3. ImageNet
Like for the other datasets we performed the standard
augmentations of the dataset after the learned augmenta-
tions. That is we performed a randomly resized crop and
scales between 0.08 and 1.0 using bicubic interpolation. We
augmented with horizontal flips, applied a color jitter with
brightness, contrast and saturation strength set to 0.4 and we
applied lighting noise with an alpha of 0.1.
We trained on Imagenet with a ResNet-50 [7] and fol-
lowed the setup of AA [1]. We train for 270 epochs with a
batch size of 2048 distributed among 32 workers. We use
image crops of height 224 considered both a 244 width of
the images, like RA, and a 224 width, like AA. The initial
learning rate of 0.1 is scaled proportional to the batch size
divided by 256. As learning rate schedule we apply a step-
wise 10-fold reduction after 90, 180 and 240 epochs with a
linear warmup of factor 4 over the first 3 epochs. We use
Nesterov Momenutm with a momentum parameter of 0.9
and a weight decay of 1e-4.
Unlike [1] we only use 32 instead of 64 workers out of
cluster limitations and scale the learning rate accordingly.
PIL operation
range
PIL operation
range
identity
-
auto contrast
-
equalize
-
rotate
−30◦- +30◦
( −135◦- +135◦)
solarize
0 - 256
(0 - 256)
color
0.1 - 1.9.
(0.01 - 2.)
posterize
4 - 8
(2 - 8)
contrast
0.1 - 1.9.
(0.01 - 2.)
brightness
0.1 - 1.9.
(0.01 - 2.)
sharpness
0.1 - 1.9.
(0.01 - 2.)
shear x
0.0 - 0.3
0.0 - 0.99
shear y
0.0 - 0.3
0.0 - 0.99
translate x
0 - 10
(0 - 32)
translate y
0 - 10
(0 - 32)
cutout
0 - 0.2
.......
invert
-
flip lr
-
flip ud
-
sample pairing
0.0 - 0.4
blur
-
smooth
-
Table 8: An overview of the augmentation spaces. The un-
marked operations are shared by all augmentation spaces
and make up the RA augmentation space. The UA augmen-
tation space additionally contains the dash underlined oper-
ations and the ......
OHL augmentation space additionally con-
tains the dotted underlined operations. The ranges given
here are the ones used for AA and RA with a discretiza-
tion to thirty values.
The UA augmentation space al-
lows translation up to 14 pixels, but inherits all other set-
tings from RA. The Wide augmentation space we use for
batch augmentation has the same operations as RA, but
uses the strength ranges in parantheses.
The OHL aug-
mentation space uses different ranges and a discretization
to three values, see [14] for more details.
All methods
are defined as part of Pillow (https://github.com/
python-pillow/Pillow), as part of ImageEnhance,
ImageOps or as image attribute, besides cutout [3]. We also
provide operations with the exact same names in our code.
B. Comparison of Different Methods on the
Same Augmentation Space
While in the above experiments we used the augmenta-
tion space corresponding to each method in the evaluations,
in this section, we probe the impact of these differences.
We follow the setup of section 4.1.2 and compare our re-
produced results of each method to TA on the exact same
augmentation space as in the paper introducing the respec-
tive method. Table 9 shows that TA's improvements gener-
alize across augmentation spaces and methods.

Dataset
Setup
AA
FAA
RA
UA
CIFAR-10
Method 97.31 ± .22 97.43 ± .09 97.12 ± .14 97.46 ± .14
TA
97.55 ± .06 97.51 ± .09 97.46 ± .09 97.42 ± .07
CIFAR-100 Method 82.91 ± .41 83.27 ± .13
83.1 ± .32
83.08 ± .27
TA
83.34 ± .10 83.36 ± .15 83.54 ± .12 83.33 ± .14
SVHN
Method 97.99 ± .06
-
98.06 ± .04 98.05 ± .04
Core
TA
98.04 ± .02 97.84 ± .03 98.05 ± .02 98.06 ± .04
Table 9: Comparisons of various methods (in our reimplementation) to TA, using the exact same augmentation space. E.g.,
for CIFAR-10 on the AA space, AA reached 97.31 ± .22 and TA reached 97.55 ± .06. No policy is published for FAA on
SVHN Core, since this setup was not part of the FAA paper. Therefore, we do not reproduce FAA on SVHN Core.
C. Evaluation on Special Datasets
To further show that this method generalizes to more par-
ticular image classification datasets without fine-tuning, we
considered two more datasets, following the settings of Sec-
tion 4.1.2. (i) Since we are not aware of an image recogni-
tion dataset that contains occlusions, we created an occlu-
sion variant of CIFAR-10 (Occ. CIFAR-10), where a 14x14
square is occluded by a black box, in each image including
the test images; we evaluate a WRN-28-10 on Occ. CIFAR-
10. We follow the settings for CIFAR-10 closely for this
experiment. (ii) Additionally, we evaluate an RN-50 on the
Stanford Cars dataset, which is a dataset in which visual de-
tails are important to distinguish car models. We train for
1000 epochs. Table 10 shows that TA continues to perform
well in these settings, outperforming even brute-force tuned
RA.
Method
Baseline
Transfer-RA
BF-RA
TA (RA)
Occ. CIFAR-10 94.99 ± .11
95.2 ± .08
95.52 ± .18 95.72 ± .09
Stanford Cars
90.21 ± .16 92.47 ± .17
-
92.77 ± .12
Table 10: A comparison on non-standard datasets. The RA
setting of BruteForce-RA is searched in the same large set
as in Section 4.1.2. Transfer-RA is transferred from Wide-
ResNet-28-10 on CIFAR-10 and ResNet-50 on ImageNet,
respectively. BF-RA for S. Cars was not feasible in the
given time.
D. Evaluation on EfficientNet-B1
While we tried to evaluatate on as relevant setups as
possible, we also had to make sure that we can compare
with previous work for the main evaluation in Section 4.1.1.
Here, we add an Evaluation of an EfficientNet-B1 follow-
ing the ImageNet setup described in the original paper [18]
closely.
None of the methods we compare to compares
on this task, thus we re-implemented UA and RA as base-
lines. For RA we performed a search over 3 settings for m,
namely 8, 14 and 21, and fixed the number of augmentations
n to 2 following the EfficientNet evaluations in [2].
RA
UA
TA (Wide)
EfficientNet-B1
78.75 ± .16
78.83 ± .23
78.99 ± .12
Table 11: The average performance of an EfficientNet-B1
across 5 re-runs on ImageNet with different augmentation
methods.
E. Approximation of the Compute Costs for
Different Methods
In this section, we discuss the data underlying our perfor-
mance per compute comparison. To fairly compare meth-
ods, we do not rely on published GPU times as much as
possible, but instead calculate all costs for a RTX 2080 Ti
for which we know many training times. Therefore, we can
only compare methods for which we ran the models. That
means for CIFAR-100 we consider only, the consider the
Wide-ResNets as well as Shake-Shake-26-2x96d.
Our estimates for the cost of one epoch on the full
CIFAR-10 dataset with 50,000 examples for each model:
• Wide-ResNet-28-2: 16s
• Wide-ResNet-40-2: 40s
• Wide-ResNet-28-10: 101s
• Shake-Shake-26-2x96d: 83s
AA
In the AA paper [1] the policy is trained over 15’000
evaluations of a Wide-ResNet-40-2 on 120 epochs of 4000
examples from CIFAR-10 for all models.
We therefore
estimate the search cost of AA as 15000 · 4000/50000 ·
120 · 40s = 1600h. Additionally we add the standard time
for 200 epochs of standard training for each mode. Wide-
ResNet-40-2: 1600h + 40s · 200/60/60, Wide-ResNet-28-
10: 1600h + 101s · 200/60/60, Shake96: 1600h + 83s ·
1800/60/60.
Fast AA
For Fast AA [13], we estimate, based on the
GPU times in the paper that the search costs more than one

full training. We therefore estimate the compute cost as one
training.
UA and TA
No search costs.
Therefore the total cost
simply is the cost of a single training. This is #epochs ·
costperepoch.
Adv. AA and TA x8
We assume for both setups no costs,
even though this of course is only a lower bound on the
compute requirements of Adv. AA. We simply multiply the
number of epochs with the cost per epoch and 8, the number
of workers.
RA
The authors of RA use a search space of 5 settings
each is evaluated on 90% of the full dataset with the same
number of epochs and model. So we have a factor of 5·9/10
with which we multiply the standard costs to get the search
costs.
OHL
OHL uses 300 epochs for the Wide-ResNets and
trains with 8 parallel workers. We therefore have a factor
of 8 · 300/200 compared to standard costs for search and
training combined.
AWS
For AWS the data is not completely clear. First, we
have an earlier version of the paper that says it evaluates
800 policies, but in a later version this was corrected down
to 500. We therefore assume only 500 policy evaluations
to be conservative. They used a Wide-ResNet-28-10 for
the augmentation search CIFAR-100 experiments. During
augmentation search they train on 80% of the training set
for 200 epochs first, and then for 10 epochs for each policy
evaluation. This yields 0.8 · (200 + 500 · 10) · 101 = 117h.
For AWS's x8 setting (8-times augmented batches), we
assume the same search costs as above and 8-times the train-
ing costs.
F. Recommendations for the Application of
Automatic Augmentation Methods
Based on our intense study of automatic augmentation
methods for different image classification tasks using dif-
ferent models we recommend the following steps when ap-
plying automatic augmentation methods. In the application
of an automatic augmentation method it is of course impor-
tant to know, whether a method is easy to reimplement. We
thus put together Table 12 for easy guidance.
Standard Model and Dataset
If the model and dataset
combination you are using is part of automatic augmenta-
tion literature, we recommend to simply use the best pub-
lished method for your setup with published code and poli-
cies.
Novel model or Novel dataset
If you are using a setup
not evaluated in the automatic augmentation literature, it
is a good approach to try both the best performing model
on a similar task as well as a parameter-free baseline.
The parameter-free baseline, like UniformAugment or Triv-
ialAugment, especially can be expected to generalize to the
new task, since they generalized to all standard automatic
evluation benchmarks without any tuning. If you have tun-
ing budget, you can of course tune something like PBA
to your particular task. This likely is a good idea if your
images are very dissimilar to the automatic augmentation
benchmarks.

Method
Policies for Training
Code for training
Code for meta-training
Cheap Search
TA
✓
✓
-
UA
✓
✗
-
Fast AA
✓p
✓
✓
Expensive Search (> 2×)
RA
✓
✗
✗
Adv. AA
✗
✗
✗
OHL
✗
✗
✗p
Very Expensive Search (> 10×)
PBA
✓s
✓i
✓i
AWS
✗
✗
✗p
AA
✓
✓i
✗
Table 12: In this table we compare the reproducibility of different methods in three categories. (i) Whether the augmentation
policies used for model trainings are available, (ii) whether the authors provide code for training a model with the policies on
which they report their performance and (iii) whether there is code available to run the search for training policies, code for a
meta-training. We mark entries with - if it is not an applicable category for the given augmentation method and additionally
use the following symbols. ✓s: Only available for subset of experiments, ✓i: Not available for ImageNet trainings, which
for PBA was also not considered in the paper, ✗p: there is publicly work in progress.

