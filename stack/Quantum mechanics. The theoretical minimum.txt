
This book is the second volume of the Theoretical Minimum series. 
The first volume, The Theoretical Minimum: What You Need to Know 
to Start Doing Physics, covered classical mechanics, which is the core 
of any physics education. We will refer to it from time to time simply 
as Volume I. This second book explains quantum mechanics and its re-
lationship to classical mechanics. The books in this series run parallel 
to Leonard Susskind‚Äôs videos, available on the Web through Stanford 
University (see www.theoreticalminimum.com for a listing). While cov-
ering the same general topics as the videos, the books contain additional 
details, and topics that don‚Äôt appear in the videos.


QUANTUM  
MECHANICS

Also by Leonard Susskind
The Theoretical Minimum
What You Need To Know to Start Doing Physics 
(with George Hrabovsky)
The Black Hole War
The Cosmic Landscape

QUANTUM  
MECHANICS
The Theoretical Minimum
LEONARD SUSSKIND  
and ART FRIEDMAN
A Member of the Perseus Books Group  
New York

Copyright ¬© 2014 by Leonard Susskind and Art Friedman
Published by Basic Books,
A Member of the Perseus Books Group
All rights reserved. Printed in the United States of America. No part of this book may 
be reproduced in any manner whatsoever without written permission except in the 
case of brief quotations embodied in critical articles and reviews. For information, 
address Basic Books, 250 West 57th Street, 15th Floor, New York, NY 10107‚Äì1307.
Books published by Basic Books are available at special discounts for bulk purchases 
in the United States by corporations, institutions, and other organizations. For more 
information, please contact the Special Markets Department at the Perseus Books 
Group, 2300 Chestnut Street, Suite 200, Philadelphia, PA 19103, or call (800) 810‚Äì
4145, ext. 5000, or e-mail special.markets@perseusbooks.com.
Designed by Art Friedman and Leonard Susskind
Hilbert‚Äôs Place drawings were created by Margaret Sloan.
A CIP catalog record for this book is available from the Library of Congress.
ISBN (hardcover): 978-0-465-03667-7
ISBN (ebook): 978-0-465-08061-8
10 9 8 7 6 5 4 3 2 1

For our parents,
who made it all possible:
Irene and Benjamin Susskind
George and Trudy Friedman


Contents
 
Preface 
xi
 
Prologue 
xv
 
Introduction 
xix
  1 Systems and Experiments 
1
  2 Quantum States 
35
  3 Principles of Quantum Mechanics 
51
  4 Time and Change 
93
  5 Uncertainty and Time Dependence 
129
  6 Combining Systems: Entanglement 
149
  7 More on Entanglement 
183
  8 Particles and Waves 
235
  9 Particle Dynamics 
273
10 The Harmonic Oscillator 
311
 
Appendix 
347
 
Index 
353


Preface
Albert Einstein, who was in many ways the father of quan-
tum mechanics, had a notorious love-hate relation with the
subject. His debates with Niels Bohr‚ÄîBohr completely ac-
cepting of quantum mechanics and Einstein deeply skeptical‚Äî
are famous in the history of science. It was generally ac-
cepted by most physicists that Bohr won and Einstein lost.
My own feeling, I think shared by a growing number of physi-
cists, is that this attitude does not do justice to Einstein‚Äôs
views.
Both Bohr and Einstein were subtle men. Einstein tried
very hard to show that quantum mechanics was inconsis-
tent; Bohr, however, was always able to counter his argu-
ments. But in his Ô¨Ånal attack Einstein pointed to something
so deep, so counterintuitive, so troubling, and yet so ex-
citing, that at the beginning of the twenty-Ô¨Årst century it
has returned to fascinate theoretical physicists. Bohr‚Äôs only
answer to Einstein‚Äôs last great discovery‚Äîthe discovery of
entanglement‚Äîwas to ignore it.
The phenomenon of entanglement is the essential fact
of quantum mechanics, the fact that makes it so diÔ¨Äerent
from classical physics. It brings into question our entire un-
xi

xii
PREFACE
derstanding about what is real in the physical world. Our
ordinary intuition about physical systems is that if we know
everything about a system, that is, everything that can in
principle be known, then we know everything about its parts.
If we have complete knowledge of the condition of an auto-
mobile, then we know everything about its wheels, its engine,
its transmission, right down to the screws that hold the up-
holstery in place. It would not make sense for a mechanic to
say, ‚ÄúI know everything about your car but unfortunately I
can‚Äôt tell you anything about any of its parts.‚Äù
But that‚Äôs exactly what Einstein explained to Bohr‚Äî
in quantum mechanics, one can know everything about a
system and nothing about its individual parts‚Äîbut Bohr
failed to appreciate this fact. I might add that generations
of quantum textbooks blithely ignored it.
Everyone knows that quantum mechanics is strange, but
I suspect very few people could tell you exactly in what way.
This book is a technical course of lectures on quantum me-
chanics, but it is diÔ¨Äerent than most courses or most text-
books. The focus is on the logical principles and the goal
is not to hide the utter strangeness of quantum logic but to
bring it out into the light of day.
I remind you that this book is one of several that closely
follow my Internet course series, the Theoretical Minimum.
My coauthor, Art Friedman, was a student in these courses.
The book beneÔ¨Åted from the fact that Art was learning the
subject and was therefore very sensitive to the issues that
might be confusing to the beginner. During the course of
writing, we had a lot of fun, and we‚Äôve tried to convey some

xiii
of that spirit with a bit of humor. If you don‚Äôt get it, ignore
it.
Leonard Susskind
When I completed my master‚Äôs degree in computer science at
Stanford, I could not have guessed that I‚Äôd return some years
later to attend Leonard‚Äôs physics lectures. My short ‚Äúcareer‚Äù
in physics ended many years earlier, with the completion of
my bachelor‚Äôs degree. But my interest in the subject has
remained very much alive.
It appears that I have lots of company‚Äîthe world seems
Ô¨Ålled with people who are genuinely, deeply interested in
physics but whose lives have taken them in diÔ¨Äerent direc-
tions. This book is for all of us.
Quantum mechanics can be appreciated, to some degree,
on a purely qualitative level. But mathematics is what brings
its beauty into sharp focus. We have tried to make this amaz-
ing body of work fully accessible to mathematically literate
nonphysicists. I think we‚Äôve done a fairly good job, and I
hope you‚Äôll agree.
No one completes a project like this without lots of help.
The people at Brockman, Inc., have made the business end of
things seem easy, and the production team at Perseus Books
has been top-notch. My sincere thanks go to TJ Kelleher,
Rachel King, and Tisse Takagi. It was our good fortune to
work with a talented copy editor, John Searcy.
I‚Äôm grateful to Leonard‚Äôs (other) continuing education
students for routinely raising thoughtful, provocative ques-
tions, and for many stimulating after-class conversations.
PREFACE

Rob Colwell, Todd Craig, Monty Frost, and John Nash of-
fered constructive comments on the manuscript.
Jeremy
Branscome and Russ Bryan reviewed the entire manuscript
in detail, and identiÔ¨Åed a number of problems.
I thank my family and friends for their kind support and
enthusiasm. I especially thank my daughter, Hannah, for
minding the store.
Besides her love, encouragement, insight, and sense of
humor, my amazing wife, Margaret Sloan, contributed about
a third of the diagrams and both Hilbert‚Äôs Place illustrations.
Thanks, Maggie.
At the start of this project, Leonard, sensing my real mo-
tivation, remarked that one of the best ways to learn physics
is to write about it.
True, of course, but I had no idea
how true, and I‚Äôm grateful that I had a chance to Ô¨Ånd out.
Thanks a million, Leonard.
Art Friedman
PREFACE
xiv

Prologue
Art looks over his beer and says, ‚ÄúLenny, let‚Äôs play a round
of the Einstein-Bohr game.‚Äù
‚ÄúOK, but I‚Äôm tired of losing. This time, you be Artstein and
I‚Äôll be L-Bore. You start.‚Äù
‚ÄúFair enough. Here‚Äôs my Ô¨Årst shot: God doesn‚Äôt play dice.
Ha-ha, L-Bore, that‚Äôs one point for me.‚Äù
‚ÄúNot so fast, Artstein, not so fast. You, my friend, were
the Ô¨Årst one to point out that quantum theory is inherently
probabilistic. Heh heh heh, that‚Äôs a two-pointer!‚Äù
‚ÄúWell, I take it back.‚Äù
‚ÄúYou can‚Äôt.‚Äù
‚ÄúI can.‚Äù
‚ÄúYou can‚Äôt.‚Äù
Few people realize that Einstein, in his 1917 paper, ‚ÄùOn the
Quantum Theory of Radiation,‚Äù argues that the emission of
gamma rays is governed by a statistical law.
xv

xvi
PROLOGUE
A Professor and a Fiddler Walk into a Bar
Volume I was punctuated by short conversations between
Lenny and George, Ô¨Åctional personas who were loosely based
on two John Steinbeck characters.The setting for this volume
of the Theoretical Minimum series is inspired by the stories
of Damon Runyon. It‚Äôs a world Ô¨Ålled with crooks, con artists,
degenerates, smooth operators, and do-gooders. Plus a few
ordinary folks, just trying to get through the day. The action
unfolds at a popular watering hole called Hilbert‚Äôs Place.
Into this setting stroll Lenny and Art, two greenhorns
from California who somehow got separated from their tour
bus. Wish them luck. They will need it.
What to Bring
You don‚Äôt need to be a physicist to take this journey, but
you should have some basic knowledge of calculus and linear
algebra. You should also know something about the material
covered in Volume I. It‚Äôs OK if your math is a bit rusty.
We‚Äôll review and explain much of it as we go, especially the
material on linear algebra. Volume I reviews the basic ideas
of calculus.
Don‚Äôt let our lighthearted humor fool you into thinking
that we‚Äôre writing for airheads. We‚Äôre not. Our goal is to
make a diÔ¨Écult subject ‚Äúas simple as possible, but no sim-
pler,‚Äù and we hope to have a little fun along the way. See
you at Hilbert‚Äôs Place.

¬© Margaret  Sloan 


Introduction
Classical mechanics is intuitive; things move in predictable
ways. An experienced ballplayer can take a quick look at a
Ô¨Çy ball, and from its location and its velocity, know where
to run in order to be there just in time to catch the ball. Of
course a sudden unexpected gust of wind might fool him, but
that‚Äôs only because he didn‚Äôt take into account all the vari-
ables. There is an obvious reason why classical mechanics is
intuitive: humans, and animals before them, have been using
it many times every day for survival. But no one ever used
quantum mechanics before the twentieth century. Quantum
mechanics describes things so small that they are completely
beyond the range of the human senses. So it stands to reason
that we did not evolve an intuition for the quantum world.
The only way we can comprehend it is by rewiring our intu-
itions with abstract mathematics. Fortunately, for some odd
reason, we did evolve the capacity for such rewiring.
Ordinarily, we learn classical mechanics Ô¨Årst, before even
attempting quantum mechanics.
But quantum physics is
much more fundamental than classical physics. As far as we
know, quantum mechanics provides an exact description of
every physical system, but some things are massive enough
xix

xx
INTRODUCTION
that quantum mechanics can be reliably approximated by
classical mechanics. That‚Äôs all that classical mechanics is:
an approximation. From a logical point of view, we should
learn quantum mechanics Ô¨Årst, but very few physics teach-
ers would recommend that. Even this course of lectures‚Äîthe
Theoretical Minimum series‚Äîbegan with classical mechan-
ics. Nevertheless, in these quantum lectures, classical me-
chanics will play almost no role except near the end, well
after the basic principles of quantum mechanics have been
explained. I think this is really the right way to do it, not
just logically but pedagogically as well. That way we don‚Äôt
fall into the trap of thinking that quantum mechanics is basi-
cally just classical mechanics with a couple of new gimmicks
thrown in. By the way, quantum mechanics is technically
much easier than classical mechanics.
The simplest classical system‚Äîthe basic logical unit for
computer science‚Äîis the two-state system. Sometimes it‚Äôs
called a bit. It can represent anything that has only two
states: a coin that can show heads or tails, a switch that
is on or oÔ¨Ä, or a tiny magnet that is constrained to point
either north or south. As you might expect, especially if you
studied the Ô¨Årst lecture of Volume I, the theory of classical
two-state systems is extremely simple‚Äîboring, in fact. In
this volume, we‚Äôre going to begin with the quantum version
of the two-state system, called a qubit, which is far more
interesting. To understand it, we will need a whole new way
of thinking‚Äîa new foundation of logic.

Lecture 1
Systems and
Experiments
Lenny and Art wander into Hilbert‚Äôs Place.
Art: What is this, the Twilight Zone? Or some kind of fun
house? I can‚Äôt get my bearings.
Lenny: Take a breath. You‚Äôll get used to it.
Art: Which way is up?
1.1
Quantum Mechanics Is
DiÔ¨Äerent
What is so special about quantum mechanics? Why is it so
hard to understand? It would be easy to blame the ‚Äúhard
mathematics,‚Äù and there may be some truth in that idea.
But that can‚Äôt be the whole story. Lots of nonphysicists are
1

2
LECTURE 1. SYSTEMS AND EXPERIMENTS
able to master classical mechanics and Ô¨Åeld theory, which
also require hard mathematics.
Quantum mechanics deals with the behavior of objects
so small that we humans are ill equipped to visualize them
at all. Individual atoms are near the upper end of this scale
in terms of size. Electrons are frequently used as objects of
study. Our sensory organs are simply not built to perceive
the motion of an electron.
The best we can do is to try
to understand electrons and their motion as mathematical
abstractions.
‚ÄúSo what?‚Äù says the skeptic. ‚ÄúClassical mechanics is Ô¨Ålled
to the brim with mathematical abstractions‚Äîpoint masses,
rigid bodies, inertial reference frames, positions, momenta,
Ô¨Åelds, waves‚Äîthe list goes on and on. There‚Äôs nothing new
about mathematical abstractions.‚Äù This is actually a fair
point, and indeed the classical and quantum worlds have
some important things in common.
Quantum mechanics,
however, is diÔ¨Äerent in two ways:
1. DiÔ¨Äerent Abstractions. Quantum abstractions are fun-
damentally diÔ¨Äerent from classical ones. For example,
we‚Äôll see that the idea of a state in quantum mechanics
is conceptually very diÔ¨Äerent from its classical counter-
part. States are represented by diÔ¨Äerent mathematical
objects and have a diÔ¨Äerent logical structure.
2. States and Measurements. In the classical world, the
relationship between the state of a system and the re-
sult of a measurement on that system is very straight-
forward. In fact, it‚Äôs trivial. The labels that describe

1.2. SPINS AND QUBITS
3
a state (the position and momentum of a particle, for
example) are the same labels that characterize mea-
surements of that state. To put it another way, one
can perform an experiment to determine the state of a
system. In the quantum world, this is not true. States
and measurements are two diÔ¨Äerent things, and the
relationship between them is subtle and nonintuitive.
These ideas are crucial, and we‚Äôll come back to them again
and again.
1.2
Spins and Qubits
The concept of spin is derived from particle physics. Par-
ticles have properties in addition to their location in space.
For example, they may or may not have electric charge, or
mass. An electron is not the same as a quark or a neutrino.
But even a speciÔ¨Åc type of particle, such as an electron, is
not completely speciÔ¨Åed by its location. Attached to the elec-
tron is an extra degree of freedom called its spin. Naively,
the spin can be pictured as a little arrow that points in some
direction, but that naive picture is too classical to accurately
represent the real situation. The spin of an electron is about
as quantum mechanical as a system can be, and any attempt
to visualize it classically will badly miss the point.
We can and will abstract the idea of a spin, and for-
get that it is attached to an electron. The quantum spin
is a system that can be studied in its own right. In fact,
the quantum spin, isolated from the electron that carries it

4
LECTURE 1. SYSTEMS AND EXPERIMENTS
through space, is both the simplest and the most quantum
of systems.
The isolated quantum spin is an example of the gen-
eral class of simple systems we call qubits‚Äîquantum bits‚Äî
that play the same role in the quantum world as logical
bits play in deÔ¨Åning the state of your computer.
Many
systems‚Äîmaybe even all systems‚Äîcan be built up by com-
bining qubits. Thus in learning about them, we are learning
about a great deal more.
1.3
An Experiment
Let‚Äôs make these ideas concrete, using the simplest example
we can Ô¨Ånd. In the Ô¨Årst lecture of Volume I, we began by
discussing a very simple deterministic system: a coin that
can show either heads (H) or tails (T). We can call this a
two-state system, or a bit, with the two states being H and
T. More formally we invent a ‚Äúdegree of freedom‚Äù called œÉ
that can take on two values, namely +1 and ‚àí1. The state
H is replaced by
œÉ = +1
and the state T by
œÉ = ‚àí1.
Classically, that‚Äôs all there is to the space of states. The
system is either in state œÉ = +1 or œÉ = ‚àí1 and there is

1.3. AN EXPERIMENT
5
nothing in between. In quantum mechanics, we‚Äôll think of
this system as a qubit.
Volume I also discussed simple evolution laws that tell
us how to update the state from instant to instant.
The
simplest law is just that nothing happens. In that case, if
we go from one discrete instant (n) to the next (n + 1), the
law of evolution is
œÉ(n + 1) = œÉ(n).
(1.1)
Let‚Äôs expose a hidden assumption that we were careless
about in Volume I. An experiment involves more than just
a system to study. It also involves an apparatus A to make
measurements and record the results of the measurements.
In the case of the two-state system, the apparatus interacts
with the system (the spin) and records the value of œÉ. Think
of the apparatus as a black box1 with a window that displays
the result of a measurement. There is also a ‚Äúthis end up‚Äù
arrow on the apparatus. The up-arrow is important because
it shows how the apparatus is oriented in space, and its di-
rection will aÔ¨Äect the outcomes of our measurements. We
begin by pointing it along the z axis (Fig. 1.1). Initially,
we have no knowledge of whether œÉ = +1 or œÉ = ‚àí1. Our
purpose is to do an experiment to Ô¨Ånd out the value of œÉ.
Before the apparatus interacts with the spin, the window
is blank (labeled with a question mark in our diagrams).
After it measures œÉ, the window shows a +1 or a ‚àí1. By
1‚ÄúBlack box‚Äù means we have no knowledge of what‚Äôs inside the
apparatus or how it works. But rest assured, it does not contain a cat.

6
LECTURE 1. SYSTEMS AND EXPERIMENTS
Spin
up
?
Apparatus
Before Measurement (A)
Spin
up
+1
Apparatus
After Measurement (B)
Z
X
Y
Figure 1.1: (A) Spin and cat-free apparatus before any mea-
surement is made. (B) Spin and apparatus after one mea-
surement has been made, resulting in œÉz = +1. The spin
is now prepared in the œÉz = +1 state. If the spin is not
disturbed and the apparatus keeps the same orientation, all
subsequent measurements will give the same result. Coordi-
nate axes show our convention for labeling the directions of
space.
looking at the apparatus, we determine the value of œÉ. That
whole process constitutes a very simple experiment designed
to measure œÉ.
Now that we‚Äôve measured œÉ, let‚Äôs reset the apparatus to
neutral and, without disturbing the spin, measure œÉ again.
Assuming the simple law of Eq. 1.1, we should get the same
answer as we did the Ô¨Årst time. The result œÉ = +1 will be

1.3. AN EXPERIMENT
7
followed by œÉ = +1. Likewise for œÉ = ‚àí1. The same will be
true for any number of repetitions. This is good because it
allows us to conÔ¨Årm the result of an experiment. We can also
say this in the following way: The Ô¨Årst interaction with the
apparatus A prepares the system in one of the two states.
Subsequent experiments conÔ¨Årm that state. So far, there is
no diÔ¨Äerence between classical and quantum physics.
Spin
up
-1
Apparatus
Apparatus Flipped 180¬∞
Z
X
Y
Figure 1.2: The apparatus is Ô¨Çipped without disturbing the
previously measured spin.
A new measurement results in
œÉz = ‚àí1.
Now let‚Äôs do something new. After preparing the spin
by measuring it with A, we turn the apparatus upside down
and then measure œÉ again (Fig. 1.2). What we Ô¨Ånd is that if
we originally prepared œÉ = +1, the upside down apparatus
records œÉ = ‚àí1. Similarly, if we originally prepared œÉ = ‚àí1,
the upside down apparatus records œÉ = +1. In other words,

8
LECTURE 1. SYSTEMS AND EXPERIMENTS
turning the apparatus over interchanges œÉ = +1 and œÉ = ‚àí1.
From these results, we might conclude that œÉ is a degree of
freedom that is associated with a sense of direction in space.
For example, if œÉ were an oriented vector of some sort, then
it would be natural to expect that turning the apparatus over
would reverse the reading. A simple explanation is that the
apparatus measures the component of the vector along an
axis embedded in the apparatus. Is this explanation correct
for all conÔ¨Ågurations?
If we are convinced that the spin is a vector, we would
naturally describe it by three components: œÉz, œÉx, and œÉy.
When the apparatus is upright along the z axis, it is posi-
tioned to measure œÉz.
Spin
up
-1 or +1
Apparatus
Apparatus Rotated 90¬∞
    
Z
X
Y
Figure 1.3: The apparatus rotated by 90‚ó¶. A new measure-
ment results in œÉz = ‚àí1 with 50 percent probability.
So far, there is still no diÔ¨Äerence between classical physics

1.3. AN EXPERIMENT
9
and quantum physics. The diÔ¨Äerence only becomes apparent
when we rotate the apparatus through an arbitrary angle,
say
œÄ
2 radians (90 degrees).
The apparatus begins in the
upright position (with the up-arrow along the z axis).
A
spin is prepared with œÉ = +1. Next, rotate A so that the
up-arrow points along the x axis (Fig. 1.3), and then make a
measurement of what is presumably the x component of the
spin, œÉx.
If in fact œÉ really represents the component of a vector
along the up-arrow, one would expect to get zero. Why?
Initially, we conÔ¨Årmed that œÉ was directed along the z axis,
suggesting that its component along x must be zero. But we
get a surprise when we measure œÉx: Instead of giving œÉx = 0,
the apparatus gives either œÉx = +1 or œÉx = ‚àí1. A is very
stubborn‚Äîno matter which way it is oriented, it refuses to
give any answer other than œÉ = ¬±1. If the spin really is a
vector, it is a very peculiar one indeed.
Nevertheless, we do Ô¨Ånd something interesting. Suppose
we repeat the operation many times, each time following the
same procedure, that is:
‚Ä¢ Beginning with A along the z axis, prepare œÉ = +1.
‚Ä¢ Rotate the apparatus so that it is oriented along the x
axis.
‚Ä¢ Measure œÉ.
The repeated experiment spits out a random series of plus-
ones and minus-ones. Determinism has broken down, but
in a particular way. If we do many repetitions, we will Ô¨Ånd

10
LECTURE 1. SYSTEMS AND EXPERIMENTS
that the numbers of œÉ = +1 events and œÉ = ‚àí1 events
are statistically equal. In other words, the average value of
œÉ is zero. Instead of the classical result‚Äînamely, that the
component of œÉ along the x axis is zero‚Äîwe Ô¨Ånd that the
average of these repeated measurements is zero.
Spin
up
-1 or +1
Apparatus
Apparatus Rotated
by an Arbitrary Angle
m
^
n^
”®
Figure 1.4: The apparatus rotated by an arbitrary angle
within the x‚Äìz plane. Average measurement result is ÀÜn ¬∑ ÀÜm.
Now let‚Äôs do the whole thing over again, but instead of
rotating A
to lie on the x axis, rotate it to an arbitrary
direction along the unit vector2 ÀÜn. Classically, if œÉ were a
vector, we would expect the result of the experiment to be
the component of œÉ along the ÀÜn axis. If ÀÜn lies at an angle Œ∏
2The standard notation for a unit vector (one of unit length) is to
place a ‚Äúhat‚Äù above the symbol representing the vector.

1.3. AN EXPERIMENT
11
with respect to z, the classical answer would be œÉ = cos Œ∏.
But as you might guess, each time we do the experiment we
get œÉ = +1 or œÉ = ‚àí1. However, the result is statistically
biased so that the average value is cos Œ∏.
The situation is of course more general. We did not have
to start with A oriented along z. Pick any direction ÀÜm and
start with the up-arrow pointing along ÀÜm. Prepare a spin
so that the apparatus reads +1. Then, without disturbing
the spin, rotate the apparatus to the direction ÀÜn, as shown
in Fig. 1.4. A new experiment on the same spin will give
random results ¬±1, but with an average value equal to the
cosine of the angle between ÀÜn and ÀÜm. In other words, the
average will be ÀÜn ¬∑ ÀÜm.
The quantum mechanical notation for the statistical av-
erage of a quantity Q is Dirac‚Äôs bracket notation ‚ü®Q‚ü©. We
may summarize the results of our experimental investigation
as follows: If we begin with A oriented along ÀÜm and conÔ¨Årm
that œÉ = +1, then subsequent measurement with A oriented
along ÀÜn gives the statistical result
‚ü®œÉ‚ü©= ÀÜn ¬∑ ÀÜm.
What we are learning is that quantum mechanical systems
are not deterministic‚Äîthe results of experiments can be sta-
tistically random‚Äîbut if we repeat an experiment many
times, average quantities can follow the expectations of clas-
sical physics, at least up to a point.

12
LECTURE 1. SYSTEMS AND EXPERIMENTS
1.4
Experiments Are Never Gentle
Every experiment involves an outside system‚Äîan apparatus‚Äî
that must interact with the system in order to record a re-
sult. In that sense, every experiment is invasive. This is
true in both classical and quantum physics, but only quan-
tum physics makes a big deal out of it. Why is that so?
Classically, an ideal measuring apparatus has a vanishingly
small eÔ¨Äect on the system it is measuring. Classical experi-
ments can be arbitrarily gentle and still accurately and repro-
ducibly record the results of the experiment. For example,
the direction of an arrow can be determined by reÔ¨Çecting
light oÔ¨Äthe arrow and focusing it to form an image. While
it is true that the light must have a small enough wavelength
to form an image, there is nothing in classical physics that
prevents the image from being made with arbitrarily weak
light. In other words, the light can have an arbitrarily small
energy content.
In quantum mechanics, the situation is fundamentally
diÔ¨Äerent. Any interaction that is strong enough to measure
some aspect of a system is necessarily strong enough to dis-
rupt some other aspect of the same system. Thus, you can
learn nothing about a quantum system without changing
something else.
This should be evident in the examples involving A and
œÉ. Suppose we begin with œÉ = +1 along the z axis. If we
measure œÉ again with A oriented along z, we will conÔ¨Årm the
previous value. We can do this over and over without chang-
ing the result. But consider this possibility: Between suc-
cessive measurements along the z axis, we turn A through

1.5. PROPOSITIONS
13
90 degrees, make an intermediate measurement, and turn it
back to its original direction. Will a subsequent measure-
ment along the z axis conÔ¨Årm the original measurement?
The answer is no. The intermediate measurement along the
x axis will leave the spin in a completely random conÔ¨Ågura-
tion as far as the next measurement is concerned. There is
no way to make the intermediate determination of the spin
without completely disrupting the Ô¨Ånal measurement. One
might say that measuring one component of the spin destroys
the information about another component. In fact, one sim-
ply cannot simultaneously know the components of the spin
along two diÔ¨Äerent axes, not in a reproducible way in any
case. There is something fundamentally diÔ¨Äerent about the
state of a quantum system and the state of a classical system.
1.5
Propositions
The space of states of a classical system is a mathematical
set. If the system is a coin, the space of states is a set of
two elements, H and T. Using set notation, we would write
{H, T}. If the system is a six-sided die, the space of states
has six elements labeled {1, 2, 3, 4, 5, 6}. The logic of set the-
ory is called Boolean logic. Boolean logic is just a formalized
version of the familiar classical logic of propositions.
A fundamental idea in Boolean logic is the notion of a
truth-value. The truth-value of a proposition is either true
or false. Nothing in between is allowed. The related set
theory concept is a subset. Roughly speaking, a proposition
is true for all the elements in its corresponding subset and
false for all the elements not in this subset. For example,

14
LECTURE 1. SYSTEMS AND EXPERIMENTS
if the set represents the possible states of a die, one can
consider the proposition
A: The die shows an odd-numbered face.
The corresponding subset contains the three elements
{1, 3, 5}.
Another proposition states
B: The die shows a number less than 4.
The corresponding subset contains the states {1, 2, 3}.
Every proposition has its opposite (also called its negation).
For example,
not A: The die does not show an odd-numbered face.
The subset for this negated proposition is {2, 4, 6}.
There are rules for combining propositions into more com-
plex propositions, the most important being or, and, and
not. We just saw an example of not, which gets applied to
a single subset or proposition. And is straightforward, and
applies to a pair of propositions.3 It says they are both true.
Applied to two subsets, and gives the elements common to
both, that is, the intersection of the two subsets. In the die
example, the intersection of subsets A and B is the subset
of elements that are both odd and less than 4. Fig. 1.5 uses
a Venn diagram to show how this works.
3And may be deÔ¨Åned for multiple propositions, but we‚Äôll only con-
sider two. The same goes for or.

1.5. PROPOSITIONS
15
The or rule is similar to and, but has one additional
subtlety. In everyday speech, the word or is generally used
in the exclusive sense‚Äîthe exclusive version is true if one or
the other of two propositions is true, but not both. However,
Boolean logic uses the inclusive version of or, which is true if
either or both of the propositions are true. Thus, according
to the inclusive or, the proposition
Albert Einstein discovered relativity or Isaac Newton was
English
is true. So is
Albert Einstein discovered relativity or Isaac Newton was
Russian.
The inclusive or is only wrong if both propositions are false.
For example,
Albert Einstein discovered America4 or Isaac Newton was
Russian.
The inclusive or has a set theoretic interpretation as the
union of two sets: it denotes the subset containing anything
in either or both of the component subsets. In the die ex-
ample, (A or B) denotes the subset {1, 2, 3, 5}.
4OK, perhaps Einstein did discover America. But he was not the
Ô¨Årst.

16
LECTURE 1. SYSTEMS AND EXPERIMENTS
1              3              5
2              4              6
Subset A:
Die shows an
odd-numbered
face.
Subset B:
Die shows a
number < 4
Space of States for a Single Die
.
Figure 1.5: An Example of the Classical model of State
Space. Subset A represents the proposition ‚Äúthe die shows
an odd-numbered face.‚Äù Subset B: ‚ÄúThe die shows a num-
ber < 4.‚Äù Dark shading shows the intersection of A and B,
which represents the proposition (A and B). White num-
bers are elements of the union of A with B, representing the
proposition (A or B).
1.6
Testing Classical Propositions
Let‚Äôs return to the simple quantum system consisting of a
single spin, and the various propositions whose truth we
could test using the apparatus A. Consider the following
two propositions:
A: The z component of the spin is +1.

1.6. TESTING CLASSICAL PROPOSITIONS
17
B: The x component of the spin is +1.
Each of these is meaningful and can be tested by orienting
A along the appropriate axis. The negation of each is also
meaningful. For example, the negation of the Ô¨Årst proposi-
tion is
not A: The z component of the spin is ‚àí1.
But now consider the composite propositions
(A or B): The z component of the spin is +1 or the x
component of the spin is +1.
(A and B): The z component of the spin is +1 and the
x component of the spin is +1.
Consider how we would test the proposition (A or B).
If spins behaved classically (and of course they don‚Äôt), we
would proceed as follows:5
‚Ä¢ Gently measure œÉz and record the value. If it is +1,
we are Ô¨Ånished: the proposition (A or B) is true. If œÉz
is ‚àí1, continue to the next step.
‚Ä¢ Gently measure œÉx. If it is +1, then the proposition
(A or B) is true. If not, this means that neither œÉz
nor œÉx was equal to +1, and (A or B) is false.
5Recall that the classical meaning of œÉ is diÔ¨Äerent from the quantum
mechanical meaning. Classically, œÉ is a straightforward 3-vector; œÉx
and œÉz represent its spatial components.

18
LECTURE 1. SYSTEMS AND EXPERIMENTS
There is an alternative procedure, which is to interchange the
order of the two measurements. To emphasize this reversal
of ordering, we‚Äôll call the new procedure (B or A):
‚Ä¢ Gently measure œÉx and record the value. If it is +1 we
are Ô¨Ånished: The proposition (B or A) is true. If œÉx is
‚àí1 continue to the next step.
‚Ä¢ Gently measure œÉz. If it is +1, then (B or A) is true.
If not, it means that neither œÉx nor œÉz
was equal to
+1, and (B or A) is false.
In classical physics, the two orders of operation give the same
answer. The reason for this is that measurements can be
arbitrarily gentle‚Äîso gentle that they do not aÔ¨Äect the re-
sults of subsequent measurements.
Therefore, the propo-
sition (A or B) has the same meaning as the proposition
(B or A).
1.7
Testing Quantum Propositions
Now we come to the quantum world that I described earlier.
Let us imagine a situation in which someone (or something)
unknown to us has secretly prepared a spin in the œÉz = +1
state.
Our job is to use the apparatus A
to determine
whether the proposition (A or B) is true or false. We will
try using the procedures outlined above.
We begin by measuring œÉz. Since the unknown agent has
set things up, we will discover that œÉz = +1. It is unnecessary
to go on: (A or B) is true. Nevertheless, we could test œÉx

1.7. TESTING QUANTUM PROPOSITIONS
19
just to see what happens. The answer is unpredictable. We
randomly Ô¨Ånd that œÉx = +1 or œÉx = ‚àí1. But neither of these
outcomes aÔ¨Äects the truth of proposition (A or B).
But now let‚Äôs reverse the order of measurement. As be-
fore, we‚Äôll call the reversed procedure (B or A), and this time
we‚Äôll measure œÉx Ô¨Årst. Because the unknown agent set the
spin to +1 along the z axis, the measurement of œÉx is ran-
dom. If it turns out that œÉx = +1, we are Ô¨Ånished: (B or A)
is true. But suppose we Ô¨Ånd the opposite result, œÉx = ‚àí1.
The spin is oriented along the ‚àíx direction. Let‚Äôs pause here
brieÔ¨Çy, to make sure we understand what just happened. As
a result of our Ô¨Årst measurement, the spin is no longer in its
original state œÉz = +1. It is in a new state, which is either
œÉx = +1 or œÉx = ‚àí1. Please take a moment to let this idea
sink in. We cannot overstate its importance.
Now we‚Äôre ready to test the second half of proposition
(B or A). Rotate the apparatus A to the z axis and mea-
sure œÉz. According to quantum mechanics, the result will be
randomly ¬±1. This means that there is a 25 percent probabil-
ity that the experiment produces œÉx = ‚àí1 and œÉz = ‚àí1. In
other words, with a probability of 1
4, we Ô¨Ånd that (B or A)
is false; this occurs despite the fact that the hidden agent
had originally made sure that œÉz = +1.
Evidently, in this example, the inclusive or is not sym-
metric. The truth of (A or B) may depend on the order in
which we conÔ¨Årm the two propositions. This is not a small
thing; it means not only that the laws of quantum physics
are diÔ¨Äerent from their classical counterparts, but that the
very foundations of logic are diÔ¨Äerent in quantum physics as
well.

20
LECTURE 1. SYSTEMS AND EXPERIMENTS
What about (A and B)?
Suppose our Ô¨Årst measure-
ment yields œÉz = +1 and the second, œÉx = +1. This is of
course a possible outcome. We would be inclined to say that
(A and B) is true.
But in science, especially in physics,
the truth of a proposition implies that the proposition can
be veriÔ¨Åed by subsequent observation. In classical physics,
the gentleness of observations implies that subsequent exper-
iments are unaÔ¨Äected and will conÔ¨Årm an earlier experiment.
A coin that turns up Heads will not be Ô¨Çipped to Tails by
the act of observing it‚Äîat least not classically. Quantum
mechanically, the second measurement (œÉx = +1) ruins the
possibility of verifying the Ô¨Årst. Once œÉx has been prepared
along the x axis, another mesurement of œÉz will give a ran-
dom answer. Thus (A and B) is not conÔ¨Årmable: the second
piece of the experiment interferes with the possibility of con-
Ô¨Årming the Ô¨Årst piece.
If you know a bit about quantum mechanics, you proba-
bly recognize that we are talking about the uncertainty prin-
ciple. The uncertainty principle doesn‚Äôt apply only to posi-
tion and momentum (or velocity); it applies to many pairs
of measurable quantities. In the case of the spin, it applies
to propositions involving two diÔ¨Äerent components of œÉ. In
the case of position and momentum, the two propositions we
might consider are:
A certain particle has position x.
That same particle has momentum p.
From these, we can form the two composite propositions

1.8. INTERLUDE: COMPLEX NUMBERS
21
The particle has position x and the particle has
momentum p.
The particle has position x or the particle has
momentum p.
Awkward as they are, both of these propositions have mean-
ing in the English language, and in classical physics as well.
However, in quantum physics, the Ô¨Årst of these propositions
is completely meaningless (not even wrong), and the second
one means something quite diÔ¨Äerent from what you might
think. It all comes down to a deep logical diÔ¨Äerence between
the classical and quantum concepts of the state of a system.
Explaining the quantum concept of state will require some
abstract mathematics, so let‚Äôs pause for a brief interlude on
complex numbers and vector spaces. The need for complex
quantities will become clear later on, when we study the
mathematical representation of spin states.
1.8
Mathematical Interlude:
Complex Numbers
Everyone who has gotten this far in the Theoretical Mini-
mum series knows about complex numbers. Nevertheless, I
will spend a few lines reminding you of the essentials. Fig.
1.6 shows some of their basic elements.
A complex number z is the sum of a real number and an
imaginary number. We can write it as
z = x + iy,

22
LECTURE 1. SYSTEMS AND EXPERIMENTS
Complex Numbers
∆µ
r cos (∆µ)
r sin ∆µ
Imaginary Axis
Real Axis
Y
X
r
z = x + iy
Figure 1.6: Two Common Ways to Represent Complex Num-
bers. In the Cartesian representation, x and y are the hor-
izontal (real) and vertical (imaginary) components. In the
polar representation, r is the radius, and Œ∏ is the angle made
with the x axis. In each case, it takes two real numbers to
represent a single complex number.
where x and y are real and i2 = ‚àí1. Complex numbers can
be added, multiplied, and divided by the standard rules of
arithmetic. They can be visualized as points on the complex
plane with coordinates x, y. They can also be represented in
polar coordinates:

1.8. INTERLUDE: COMPLEX NUMBERS
23
z = reiŒ∏ = r(cos Œ∏ + i sin Œ∏).
Adding complex numbers is easy in component form: just
add the components.
Similarly, multiplying them is easy
in their polar form: Simply multiply the radii and add the
angles:

r1eiŒ∏1 
r2eiŒ∏2
= (r1r2) ei(Œ∏1+Œ∏2).
Every complex number z has a complex conjugate z‚àóthat is
obtained by simply reversing the sign of the imaginary part.
If
z = x + iy = reiŒ∏,
then
z‚àó= x ‚àíiy = re‚àíiŒ∏.
Multiplying a complex number and its conjugate always gives
a positive real result:
z‚àóz = r2.
It is of course true that every complex conjugate is itself a
complex number, but it‚Äôs often helpful to think of z and z‚àó
as belonging to separate ‚Äúdual‚Äù number systems. Dual here
means that for every z there is a unique z‚àóand vice versa.

24
LECTURE 1. SYSTEMS AND EXPERIMENTS
There is a special class of complex numbers that I‚Äôll call
‚Äúphase-factors.‚Äù A phase-factor is simply a complex number
whose r-component is 1.
If z is a phase-factor, then the
following hold:
z‚àóz = 1
z = eiŒ∏
z = cos Œ∏ + i sin Œ∏.
1.9
Mathematical Interlude:
Vector Spaces
1.9.1
Axioms
For a classical system, the space of states is a set (the set of
possible states), and the logic of classical physics is Boolean.
That seems obvious and it is diÔ¨Écult to imagine any other
possibility. Nevertheless, the real world operates along en-
tirely diÔ¨Äerent lines, at least whenever quantum mechanics
is important. The space of states of a quantum system is not
a mathematical set;6 it is a vector space. Relations between
the elements of a vector space are diÔ¨Äerent from those be-
tween the elements of a set, and the logic of propositions is
diÔ¨Äerent as well.
Before I tell you about vector spaces, I need to clarify the
term vector. As you know, we use this term to indicate an
6To be a little more precise, we will not focus on the set-theoretic
properties of state spaces, even though they may of course be regarded
as sets.

1.9. INTERLUDE: VECTOR SPACES
25
object in ordinary space that has a magnitude and a direc-
tion. Such vectors have three components, corresponding to
the three dimensions of space. I want you to completely for-
get about that concept of a vector. From now on, whenever
I want to talk about a thing with magnitude and direction
in ordinary space, I will explicitly call it a 3-vector. A math-
ematical vector space is an abstract construction that may
or may not have anything to do with ordinary space. It may
have any number of dimensions from 1 to ‚àûand it may
have components that are integers, real numbers, or even
more general things.
The vector spaces we use to deÔ¨Åne quantum mechanical
states are called Hilbert spaces. We won‚Äôt give the mathe-
matical deÔ¨Ånition here, but you may as well add this term
to your vocabulary. When you come across the term Hilbert
space in quantum mechanics, it refers to the space of states.
A Hilbert space may have either a Ô¨Ånite or an inÔ¨Ånite number
of dimensions.
In quantum mechanics, a vector space is composed of
elements |A‚ü©called ket-vectors or just kets.
Here are the
axioms we will use to deÔ¨Åne the vector space of states of a
quantum system (z and w are complex numbers):
1. The sum of any two ket-vectors is also a ket-vector:
|A‚ü©+ |B‚ü©= |C‚ü©.

26
LECTURE 1. SYSTEMS AND EXPERIMENTS
2. Vector addition is commutative:
|A‚ü©+ |B‚ü©= |B‚ü©+ |A‚ü©.
3. Vector addition is associative:
{|A‚ü©+ |B‚ü©} + |C‚ü©= |A‚ü©+ {|B‚ü©+ |C‚ü©} .
4. There is a unique vector 0 such that when you add it
to any ket, it gives the same ket back:
|A‚ü©+ 0 = |A‚ü©.
5. Given any ket |A‚ü©, there is a unique ket ‚àí|A‚ü©such that
|A‚ü©+ (‚àí|A‚ü©) = 0.
6. Given any ket |A‚ü©and any complex number z, you can
multiply them to get a new ket. Also, multiplication
by a scalar is linear:
|zA‚ü©= z|A‚ü©= |B‚ü©.
7. The distributive property holds:
z {|A‚ü©+ |B‚ü©} = z|A‚ü©+ z|B‚ü©
{z + w} |A‚ü©= z|A‚ü©+ w|A‚ü©.

1.9. INTERLUDE: VECTOR SPACES
27
Axioms 6 and 7 taken together are often called linearity.
Ordinary 3-vectors would satisfy these axioms except for
one thing: Axiom 6 allows a vector to be multiplied by any
complex number. Ordinary 3-vectors can be multiplied by
real numbers (positive, negative, or zero) but multiplication
by complex numbers is not deÔ¨Åned.
One can think of 3-
vectors as forming a real vector space, and kets as forming a
complex vector space. Our deÔ¨Ånition of ket-vectors is fairly
abstract. As we will see, there are various concrete ways to
represent ket-vectors as well.
1.9.2
Functions and Column Vectors
Let‚Äôs look at some concrete examples of complex vector spaces.
First of all, consider the set of continuous complex-valued
functions of a variable x. Call the functions A(x). You can
add any two such functions and multiply them by complex
numbers. You can check that they satisfy all seven axioms.
This example should make it obvious that we are talking
about something much more general than three-dimensional
arrows.
Two-dimensional column vectors provide another con-
crete example.
We construct them by stacking up a pair
of complex numbers, Œ±1 and Œ±2, in the form
 Œ±1
Œ±2

and identifying this ‚Äústack‚Äù with the ket-vector |A‚ü©. The
complex numbers Œ± are the components of |A‚ü©. You can add
two column vectors by adding their components:

28
LECTURE 1. SYSTEMS AND EXPERIMENTS
 Œ±1
Œ±2

+
 Œ≤1
Œ≤2

=
 Œ±1 + Œ≤1
Œ±2 + Œ≤2

.
Moreover, you can multiply the column vector by a complex
number z just by multiplying the components,
z
 Œ±1
Œ±2

=
 zŒ±1
zŒ±2

.
Column vector spaces of any number of dimensions can be
constructed. For example, here is a Ô¨Åve-dimensional column
vector:
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
Œ±1
Œ±2
Œ±3
Œ±4
Œ±5
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.
Normally, we do not mix vectors of diÔ¨Äerent dimensionality.
1.9.3
Bras and Kets
As we have seen, the complex numbers have a dual version:
in the form of complex conjugate numbers. In the same way,
a complex vector space has a dual version that is essentially
the complex conjugate vector space.
For every ket-vector
|A‚ü©, there is a ‚Äúbra‚Äù vector in the dual space, denoted by ‚ü®A|.
Why the strange terms bra and ket? Shortly, we will deÔ¨Åne
inner products of bras and kets, using expressions like ‚ü®B|A‚ü©
to form bra-kets or brackets. Inner products are extremely

1.9. INTERLUDE: VECTOR SPACES
29
important in the mathematical machinery of quantum me-
chanics, and for characterizing vector spaces in general.
Bra vectors satisfy the same axioms as the ket-vectors,
but there are two things to keep in mind about the corre-
spondence between kets and bras:
1. Suppose ‚ü®A| is the bra corresponding to the ket |A‚ü©,
and ‚ü®B| is the bra corresponding to the ket |B‚ü©. Then
the bra corresponding to
|A‚ü©+ |B‚ü©
is
‚ü®A| + ‚ü®B|.
2. If z is a complex number, then it is not true that the bra
corresponding to z|A‚ü©is ‚ü®A|z. You have to remember
to complex-conjugate. Thus, the bra corresponding to
z|A‚ü©
is
‚ü®A|z‚àó.
In the concrete example where kets are represented by col-
umn vectors, the dual bras are represented by row vectors,

30
LECTURE 1. SYSTEMS AND EXPERIMENTS
with the entries being drawn from the complex conjugate
numbers. Thus, if the ket |A‚ü©is represented by the column
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
Œ±1
Œ±2
Œ±3
Œ±4
Œ±5
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
,
then the corresponding bra ‚ü®A| is represented by the row

Œ±‚àó
1
Œ±‚àó
2
Œ±‚àó
3
Œ±‚àó
4
Œ±‚àó
5

.
1.9.4
Inner Products
You are no doubt familiar with the dot product deÔ¨Åned for
ordinary 3-vectors. The analogous operation for bras and
kets is the inner product. The inner product is always the
product of a bra and a ket and it is written this way:
‚ü®B|A‚ü©.
The result of this operation is a complex number. The ax-
ioms for inner products are not too hard to guess:
1. They are linear:
‚ü®C| { |A‚ü©+ |B‚ü©} = ‚ü®C|A‚ü©+ ‚ü®C|B‚ü©.

1.9. INTERLUDE: VECTOR SPACES
31
2. Interchanging bras and kets corresponds to complex
conjugation:
‚ü®B|A‚ü©= ‚ü®A|B‚ü©‚àó.
Exercise 1.1:
a) Using the axioms for inner products, prove
{‚ü®A| + ‚ü®B|} |C‚ü©= ‚ü®A|C‚ü©+ ‚ü®B|C‚ü©.
b) Prove ‚ü®A|A‚ü©is a real number.
In the concrete representation of bras and kets by row and
column vectors, the inner product is deÔ¨Åned in terms of com-
ponents:
‚ü®B|A‚ü©
=

Œ≤‚àó
1
Œ≤‚àó
2
Œ≤‚àó
3
Œ≤‚àó
4
Œ≤‚àó
5

‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
Œ±1
Œ±2
Œ±3
Œ±4
Œ±5
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
=
Œ≤‚àó
1Œ±1 + Œ≤‚àó
2Œ±2 + Œ≤‚àó
3Œ±3 + Œ≤‚àó
4Œ±4 + Œ≤‚àó
5Œ±5. (1.2)
The rule for inner products is essentially the same as for dot
products: add the products of corresponding components of
the vectors whose inner product is being calculated.

32
LECTURE 1. SYSTEMS AND EXPERIMENTS
Exercise 1.2: Show that the inner product deÔ¨Åned by Eq.
1.2 satisÔ¨Åes all the axioms of inner products.
Using the inner product, we can deÔ¨Åne some concepts that
are familiar from ordinary 3-vectors:
‚Ä¢ Normalized Vector: A vector is said to be normalized
if its inner product with itself is 1. Normalized vectors
satisfy,
‚ü®A|A‚ü©= 1.
For ordinary 3-vectors, the term normalized vector is
usually replaced by unit vector, that is, a vector of unit
length.
‚Ä¢ Orthogonal Vectors: Two vectors are said to be or-
thogonal if their inner product is zero. |A‚ü©and |B‚ü©are
orthogonal if
‚ü®B|A‚ü©= 0.
This is the analog of saying that two 3-vectors are or-
thogonal if their dot product is zero.
1.9.5
Orthonormal Bases
When working with ordinary 3-vectors, it is extremely useful
to introduce a set of three mutually orthogonal unit vectors
and use them as a basis to construct any vector. A simple

1.9. INTERLUDE: VECTOR SPACES
33
example would be the unit 3-vectors that point along the x,
y, and z axes. They are usually called ÀÜi, ÀÜj, and ÀÜk. Each is of
unit length and orthogonal to the others. If you tried to Ô¨Ånd
a fourth vector orthogonal to these three, there wouldn‚Äôt be
any‚Äînot in three dimensions anyway. However, if there were
more dimensions of space, there would be more basis vectors.
The dimension of a space can be deÔ¨Åned as the maximum
number of mutually orthogonal vectors in that space.
Obviously, there is nothing special about the particular
axes x, y, and z. As long as the basis vectors are of unit length
and are mutually orthogonal, they comprise an orthonormal
basis.
The same principle is true for complex vector spaces. One
can begin with any normalized vector and then look for a
second one, orthogonal to the Ô¨Årst. If you Ô¨Ånd one, then
the space is at least two-dimensional. Then look for a third,
fourth, and so on. Eventually, you may run out of new direc-
tions and there will not be any more orthogonal candidates.
The maximum number of mutually orthogonal vectors is the
dimension of the space. For column vectors, the dimension
is simply the number of entries in the column.
Let‚Äôs consider an N-dimensional space and a particular
orthonormal basis of ket-vectors labeled |i‚ü©.7 The label i runs
from 1 to N. Consider a vector |A‚ü©, written as a sum of basis
7Mathematically, basis vectors are not required to be orthonor-
mal. However, in quantum mechanics they generally are. In this book,
whenever we say basis, we mean an orthonormal basis.

34
LECTURE 1. SYSTEMS AND EXPERIMENTS
vectors:
|A‚ü©=

i
Œ±i|i‚ü©.
(1.3)
The Œ±i are complex numbers called the components of the
vector, and to calculate them we take the inner product of
both sides with a basis bra ‚ü®j|:
‚ü®j|A‚ü©=

i
Œ±i‚ü®j|i‚ü©.
(1.4)
Next, we use the fact that the basis vectors are orthonormal.
This implies that ‚ü®j|i‚ü©= 0 if i is not equal to j, and ‚ü®j|i‚ü©= 1
if i = j. In other words, ‚ü®j|i‚ü©= Œ¥ij. This makes the sum in
Eq. 1.4 collapse to one term:
‚ü®j|A‚ü©= Œ±j.
(1.5)
Thus, we see that the components of a vector are just its
inner products with the basis vectors. We can rewrite Eq.
1.3 in the elegant form
|A‚ü©=

i
|i‚ü©‚ü®i|A‚ü©.

Lecture 2
Quantum States
Art: Oddly enough, that beer made my head stop spinning.
What state are we in?
Lenny: I wish I knew. Does it matter?
Art: It might. I don‚Äôt think we‚Äôre in California anymore.
2.1
States and Vectors
In classical physics, knowing the state of a system implies
knowing everything that is necessary to predict the future
of that system. As we‚Äôve seen in the last lecture, quantum
systems are not completely predictable. Evidently, quantum
states have a diÔ¨Äerent meaning than classical states. Very
roughly, knowing a quantum state means knowing as much
as can be known about how the system was prepared. In the
last chapter, we talked about using an apparatus to prepare
the state of a spin. In fact, we implicitly assumed that there
35

36
LECTURE 2. QUANTUM STATES
was no more Ô¨Åne detail to specify or that could be speciÔ¨Åed
about the state of the spin.
The obvious question to ask is whether the unpredictabil-
ity is due to an incompleteness in what we call a quantum
state. There are various opinions about this matter. Here is
a sampling:
‚Ä¢ Yes, the usual notion of quantum state is incomplete.
There are ‚Äúhidden variables‚Äù that, if only we could ac-
cess them, would allow complete predictability. There
are two versions of this view. In version A, the hidden
variables are hard to measure but in principle they are
experimentally available to us. In version B, because
we are made of quantum mechanical matter and there-
fore subject to the restrictions of quantum mechanics,
the hidden variables are, in principle, not detectable.
‚Ä¢ No, the hidden variables concept does not lead us in
a proÔ¨Åtable direction. Quantum mechanics is unavoid-
ably unpredictable. Quantum mechanics is as complete
a calculus of probabilities as is possible. The job of a
physicist is to learn and use this calculus.
I don‚Äôt know what the ultimate answer to this question will
be, or even if it will prove to be a useful question. But for our
purposes, it‚Äôs not important what any particular physicist
believes about the ultimate meaning of the quantum state.
For practical reasons, we will adopt the second view.
In practice, what this means for the quantum spin of
Lecture 1 is that, when the apparatus A acts and tells us
that œÉz = +1 or œÉz = ‚àí1, there is no more to know, or

2.2. REPRESENTING SPIN STATES
37
that can be known. Likewise, if we rotate A and measure
œÉx = +1 or œÉx = ‚àí1, there is no more to know. Likewise for
œÉy or any other component of the spin.
2.2
Representing Spin States
Now it‚Äôs time to try our hand at representing spin states us-
ing state-vectors. Our goal is to build a representation that
captures everything we know about the behavior of spins.
At this point, the process will be more intuitive than formal.
We will try to Ô¨Åt things together the best we can, based on
what we‚Äôve already learned. Please read this section care-
fully. Believe me, it will pay oÔ¨Ä.
Let‚Äôs begin by labeling the possible spin states along the
three coordinate axes.
If A is oriented along the z axis,
the two possible states that can be prepared correspond to
œÉz = ¬±1. Let‚Äôs call them up and down and denote them by
ket-vectors |u‚ü©and |d‚ü©. Thus, when the apparatus is oriented
along the z axis and registers +1, the state |u‚ü©has been
prepared.
On the other hand, if the apparatus is oriented along
the x axis and registers ‚àí1, the state |l‚ü©has been prepared.
We‚Äôll call it left. If A is along the y axis, it can prepare the
states |i‚ü©and |o‚ü©(in and out). You get the idea.
The idea that there are no hidden variables has a very
simple mathematical representation: the space of states for
a single spin has only two dimensions. This point deserves
emphasis:

38
LECTURE 2. QUANTUM STATES
All possible spin states can be represented in a two-
dimensional vector space.
We could, somewhat arbitrarily,1 choose |u‚ü©and |d‚ü©as
the two basis vectors and write any state as a linear super-
position of these two. We‚Äôll adopt that choice for now. Let‚Äôs
use the symbol |A‚ü©for a generic state. We can write this as
an equation,
|A‚ü©= Œ±u|u‚ü©+ Œ±d|d‚ü©,
where Œ±u and Œ±d are the components of |A‚ü©along the basis
directions |u‚ü©and |d‚ü©. Mathematically, we can identify the
components of |A‚ü©as
Œ±u
=
‚ü®u|A‚ü©
Œ±d
=
‚ü®d|A‚ü©.
(2.1)
These equations are extremely abstract, and it is not at all
obvious what their physical signiÔ¨Åcance is. I am going to
tell you right now what they mean: First of all, |A‚ü©can
represent any state of the spin, prepared in any manner. The
components Œ±u and Œ±d are complex numbers; by themselves,
they have no experimental meaning, but their magnitudes
do. In particular, Œ±‚àó
uŒ±u and Œ±‚àó
dŒ±d have the following meaning:
‚Ä¢ Given that the spin has been prepared in the state
|A‚ü©, and that the apparatus is oriented along z, the
1The choice is not totally arbitrary.
The basis vectors must be
orthogonal to each other.

2.2. REPRESENTING SPIN STATES
39
quantity Œ±‚àó
uŒ±u is the probability that the spin would
be measured as œÉz = +1. In other words, it is the
probability of the spin being up if measured along the
z axis.
‚Ä¢ Likewise, Œ±‚àó
dŒ±d is the probability that œÉz would be down
if measured.
The Œ± values, or equivalently ‚ü®u|A‚ü©and ‚ü®d|A‚ü©, are called
probability amplitudes. They are themselves not probabil-
ities. To compute a probability, their magnitudes must be
squared. In other words, the probabilities for measurements
of up and down are given by
Pu
=
‚ü®A|u‚ü©‚ü®u|A‚ü©
Pd
=
‚ü®A|d‚ü©‚ü®d|A‚ü©.
(2.2)
Notice that I have said nothing about what œÉz is before it is
measured. Before the measurement, all we have is the vector
|A‚ü©, which represents the potential possibilities but not the
actual values of our measurements.
Two other points are important: First, note that |u‚ü©and
|d‚ü©are mutually orthogonal. In other words,
‚ü®u|d‚ü©
=
0
‚ü®d|u‚ü©
=
0.
(2.3)

40
LECTURE 2. QUANTUM STATES
The physical meaning of this is that, if the spin is prepared
up, then the probability to detect it down is zero, and vice
versa.
This point is so important, I‚Äôll say it again: Two
orthogonal states are physically distinct and mutually exclu-
sive. If the spin is in one of these states, it cannot be (has
zero probability to be) in the other one. This idea applies to
all quantum systems, not just spin.
But don‚Äôt mistake the orthogonality of state-vectors for
orthogonal directions in space. In fact, the directions up and
down are not orthogonal directions in space, even though
their associated state-vectors are orthogonal in state space.
The second important point is that for the total proba-
bility to come out equal to unity, we must have
Œ±‚àó
uŒ±u + Œ±‚àó
dŒ±d = 1.
(2.4)
This is equivalent to saying that the vector |A‚ü©is normalized
to a unit vector:
‚ü®A|A‚ü©= 1.
This is a very general principle of quantum mechanics that
extends to all quantum systems: the state of a system is
represented by a unit (normalized) vector in a vector space of
states. Moreover, the squared magnitudes of the components
of the state-vector, along particular basis vectors, represent
probabilities for various experimental outcomes.

2.3. ALONG THE X AXIS
41
2.3
Along the x Axis
We said before that we can represent any spin state as a
linear combination of the basis vectors |u‚ü©and |d‚ü©. Let‚Äôs try
doing this now for the vectors |r‚ü©and |l‚ü©, which represent
spins prepared along the x axis. We‚Äôll start with |r‚ü©. As you
recall from Lecture 1, if A initially prepares |r‚ü©, and is then
rotated to measure œÉz, there will be equal probabilities for
up and down. Thus, Œ±‚àó
uŒ±u and Œ±‚àó
dŒ±d must both be equal to
1
2. A simple vector that satisÔ¨Åes this rule is
|r‚ü©= 1
‚àö
2|u‚ü©+ 1
‚àö
2|d‚ü©.
(2.5)
There is some ambiguity in this choice, but as we will see
later, it is nothing more than the ambiguity in our choice of
exact directions for the x and y axes.
Next, let‚Äôs look at the vector |l‚ü©. Here is what we know:
when the spin has been prepared in the left conÔ¨Åguration,
the probabilities for œÉz are again equal to
1
2. That is not
enough to determine the values Œ±‚àó
uŒ±u and Œ±‚àó
dŒ±d, but there
is another condition that we can infer. Earlier, I told you
that |u‚ü©and |d‚ü©are orthogonal for the simple reason that, if
the spin is up, it‚Äôs deÔ¨Ånitely not down. But there is nothing
special about up and down that is not also true of right and
left. In particular, if the spin is right, it has zero probability
of being left. Thus, by analogy with Eq. 2.3,
‚ü®r|l‚ü©= 0
‚ü®l|r‚ü©= 0.

42
LECTURE 2. QUANTUM STATES
This pretty much Ô¨Åxes |l‚ü©in the form
|l‚ü©= 1
‚àö
2|u‚ü©‚àí1
‚àö
2|d‚ü©.
(2.6)
Exercise 2.1: Prove that the vector |r‚ü©in Eq. 2.5 is orthog-
onal to vector |l‚ü©in Eq. 2.6.
Again, there is some ambiguity in the choice of |l‚ü©. This is
called the phase ambiguity. Suppose we multiply |l‚ü©by any
complex number z. That will have no eÔ¨Äect on whether it is
orthogonal to |r‚ü©, though in general the result will no longer
be normalized (have unit length). But if we choose z = eiŒ∏
(where Œ∏ can be any real number), then there will be no
eÔ¨Äect on the normalization because eiŒ∏ has unit magnitude.
In other words, Œ±‚àó
uŒ±u + Œ±‚àó
dŒ±d will remain equal to 1. Since
a number of the form z = eiŒ∏ is called a phase-factor, the
ambiguity is called the phase ambiguity. Later, we will Ô¨Ånd
out that no measurable quantity is sensitive to the overall
phase-factor, and therefore we can ignore it when specifying
states.
2.4
Along the y Axis
Finally, this brings us to |i‚ü©and |o‚ü©, the vectors representing
spins oriented along the y axis. Let‚Äôs look at the conditions
they need to satisfy. First,
‚ü®i|o‚ü©= 0.
(2.7)

2.4. ALONG THE Y AXIS
43
This condition states that in and out are represented by or-
thogonal vectors in the same way that up and down are.
Physically, this means that if the spin is in, it is deÔ¨Ånitely
not out.
There are additional restrictions on the vectors |i‚ü©and
|o‚ü©. Using the relationships expressed in Eqs. 2.1 and 2.2,
and the statistical results of our experiments, we can write
the following:
‚ü®o|u‚ü©‚ü®u|o‚ü©
=
1
2
‚ü®o|d‚ü©‚ü®d|o‚ü©
=
1
2
‚ü®i|u‚ü©‚ü®u|i‚ü©
=
1
2
‚ü®i|d‚ü©‚ü®d|i‚ü©
=
1
2.
(2.8)
In the Ô¨Årst two equations, |o‚ü©takes the role of |A‚ü©from Eqs.
2.1 and 2.2. In the second two, |i‚ü©takes that role. These
conditions state that if the spin is oriented along y, and is
then measured along z, it is equally likely to be up or down.
We should also expect that if the spin were measured along
the x axis, it would be equally likely to be right or left. This
leads to additional conditions:
‚ü®o|r‚ü©‚ü®r|o‚ü©
=
1
2
‚ü®o|l‚ü©‚ü®l|o‚ü©
=
1
2

44
LECTURE 2. QUANTUM STATES
‚ü®i|r‚ü©‚ü®r|i‚ü©
=
1
2
‚ü®i|l‚ü©‚ü®l|i‚ü©
=
1
2.
(2.9)
These conditions are suÔ¨Écient to determine the form of the
vectors |i‚ü©and |o‚ü©, apart from the phase ambiguity. Here is
the result:
|i‚ü©
=
1
‚àö
2|u‚ü©+
i
‚àö
2|d‚ü©
|o‚ü©
=
1
‚àö
2|u‚ü©‚àí
i
‚àö
2|d‚ü©.
(2.10)
Exercise 2.2: Prove that |i‚ü©and |o‚ü©satisfy all of the con-
ditions in Eqs. 2.7, 2.8, and 2.9. Are they unique in that
respect?
It‚Äôs interesting that two of the components in Eqs. 2.10 are
imaginary. Of course, we‚Äôve said all along that the space of
states is a complex vector space, but until now we have not
had to use complex numbers in our calculations. Are the
complex numbers in Eqs. 2.10 a convenience or a necessity?
Given our framework for spin states, there is no way around
them. It‚Äôs somewhat tedious to demonstrate this, but the
steps are straightforward. The following exercise gives you
a road map.
The need for complex numbers is a general
feature of quantum mechanics, and we‚Äôll see more examples
as we go.

2.5. COUNTING PARAMETERS
45
Exercise 2.3: For the moment, forget that Eqs. 2.10 give us
working deÔ¨Ånitions for |i‚ü©and |o‚ü©in terms of |u‚ü©and |d‚ü©, and
assume that the components Œ±, Œ≤, Œ≥, and Œ¥ are unknown:
|i‚ü©= Œ±|u‚ü©+ Œ≤|d‚ü©
|o‚ü©= Œ≥|u‚ü©+ Œ¥|d‚ü©.
a) Use Eqs. 2.8 to show that
Œ±‚àóŒ± = Œ≤‚àóŒ≤ = Œ≥‚àóŒ≥ = Œ¥‚àóŒ¥ = 1
2.
b) Use the above result and Eqs. 2.9 to show that
Œ±‚àóŒ≤ + Œ±Œ≤‚àó= Œ≥‚àóŒ¥ + Œ≥Œ¥‚àó= 0.
c) Show that Œ±‚àóŒ≤ and Œ≥‚àóŒ¥ must each be pure imaginary.
If Œ±‚àóŒ≤ is pure imaginary, then Œ± and Œ≤ cannot both be real.
The same reasoning applies to Œ≥‚àóŒ¥.
2.5
Counting Parameters
It‚Äôs always important to know how many independent pa-
rameters it takes to characterize a system.
For example,
the generalized coordinates we used in Volume I (referred to
as qi) each represented an independent degree of freedom.
That approach freed us from the diÔ¨Écult job of writing ex-
plicit equations to describe physical constraints. Along sim-
ilar lines, our next task is to count the number of physically
distinct states there are for a spin. I will do it in two ways,
to show that you get the same answer either way.

46
LECTURE 2. QUANTUM STATES
The Ô¨Årst way is simple. Point the apparatus along any
unit 3-vector2 ÀÜn and prepare a spin with œÉ = +1 along that
axis. If œÉ = ‚àí1, you can think of the spin as being oriented
along the ‚àíÀÜn axis. Thus, there must be a state for every
orientation of the unit 3-vector ÀÜn. How many parameters
does it take to specify such an orientation? The answer is
of course two. It takes two angles to deÔ¨Åne a direction in
three-dimensional space.3
Now, let‚Äôs consider the same question from another per-
spective. The general spin state is deÔ¨Åned by two complex
numbers, Œ±u and Œ±d. That seems to add up to four real pa-
rameters, with each complex parameter counting as two real
ones. But recall that the vector has to be normalized as in
Eq. 2.4. The normalization condition gives us one equation
involving real variables, and cuts the number of parameters
down to three.
As I said earlier, we will eventually see that the physical
properties of a state-vector do not depend on the overall
phase-factor. This means that one of the three remaining
parameters is redundant, leaving only two‚Äîthe same as the
number of parameters we need to specify a direction in three-
dimensional space.
Thus, there is enough freedom in the
expression
Œ±u|u‚ü©+ Œ±d|d‚ü©
2Keep in mind that 3-vectors are not bras or kets.
3Recall that spherical coordinates use two angles to represent the
orientation of a point in relation to the origin. Latitude and longitude
provide another example.

2.6. REPRESENTING SPIN STATES
47
to describe all the possible orientations of a spin, even though
there are only two possible outcomes of an experiment along
any axis.
2.6
Representing Spin States as
Column Vectors
So far, we have been able to learn a lot by using the abstract
forms of our state-vectors, that is, |u‚ü©and |d‚ü©and so forth.
These abstractions help us focus on mathematical relation-
ships without worrying about unnecessary details. However,
soon we will need to perform detailed calculations on spin
states, and for that we‚Äôll need to write our state-vectors in
column form. Because of ‚Äúphase indiÔ¨Äerence,‚Äù the column
representations are not unique, and we‚Äôll try to choose the
simplest and most convenient ones we can Ô¨Ånd.
As usual, we‚Äôll start with |u‚ü©and |d‚ü©. We need them to
have unit length, and to be mutually orthogonal. A pair of
columns that satisÔ¨Åes these requirements is
|u‚ü©=
 1
0

(2.11)
|d‚ü©=
 0
1

.
(2.12)
With these column vectors in hand, it will be easy to create
column vectors for |r‚ü©and |l‚ü©using Eqs. 2.5 and 2.6, and for
|i‚ü©and |o‚ü©using Eqs. 2.10. We‚Äôll do that in the next lecture,
where these results are needed.

48
LECTURE 2. QUANTUM STATES
2.7
Putting It All Together
We have covered a lot of ground in this lecture. Before mov-
ing on, let‚Äôs take stock of what we‚Äôve done. Our goal was to
synthesize what we know about spins and vector spaces. We
Ô¨Ågured out how to use vectors to represent spin states, and
in the process we got a glimpse of the kind of information a
state-vector contains (and does not contain!). Here is a brief
outline of what we did:
‚Ä¢ Based on our knowledge of spin measurements, we chose
three pairs of mutually orthogonal basis vectors. Pair-
wise, we named them |u‚ü©and |d‚ü©, |r‚ü©and |l‚ü©, and |i‚ü©
and |o‚ü©. Because the basis vectors |u‚ü©and |d‚ü©repre-
sent physically distinct states, we were able to assert
that they are mutually orthogonal.
In other words,
‚ü®u|d‚ü©= 0. The same holds for |r‚ü©and |l‚ü©, and also for
|i‚ü©and |o‚ü©.
‚Ä¢ We found that it takes two independent parameters to
specify a spin state, and then we arbitrarily chose one
of the orthogonal pairs, |u‚ü©and |d‚ü©, as our basis vec-
tors for representing all spin states‚Äîeven though the
two complex numbers in a state-vector require four real
numbers to specify them. How did we get away with
this? We were clever enough to notice that these four
numbers are not all independent.4 The normalization
constraint (total probability must equal 1) eliminates
one independent parameter, and ‚Äúphase indiÔ¨Äerence‚Äù
4Please indulge in a self-satisÔ¨Åed grin.

2.7. PUTTING IT ALL TOGETHER
49
(the physics of a state-vector is unaÔ¨Äected by its overall
phase-factor) eliminates a second.
‚Ä¢ Having chosen |u‚ü©and |d‚ü©as our main basis vectors,
we Ô¨Ågured out how to represent the other two pairs
of basis vectors as linear combinations of |u‚ü©and |d‚ü©,
using additional orthogonality and probability-based
constraints.
‚Ä¢ Finally, we established a way to represent our main
basis vectors as columns. This representation is not
unique. In the next lecture, we‚Äôll use our |u‚ü©and |d‚ü©
column vectors to derive column vectors for the two
other bases.
While achieving these concrete results, we got a chance to
see some state-vector mathematics in action and learn some-
thing about how these mathematical objects correspond to
physical spins. Although we will focus on spin, the same
concepts and techniques apply to other quantum systems as
well. Please take a little time to assimilate the material we‚Äôve
covered so far before moving on to the next lecture. As I said
at the beginning, it will really pay oÔ¨Ä.


Lecture 3
Principles of Quantum
Mechanics
Art: I‚Äôm not like you, Lenny. My brain just wasn‚Äôt built for
quantum mechanics.
Lenny: Nah, mine wasn‚Äôt either. Just can‚Äôt really visualize
the stuÔ¨Ä. But I‚Äôll tell you, I once knew a guy who thought
just like an electron.
Art: What happened to him?
Lenny: Art, all I‚Äôm gonna tell you is that it sure wasn‚Äôt
pretty.
Art: Hmm, I guess that gene didn‚Äôt Ô¨Çy.
No, we were not built to sense quantum phenomena; not
the same way we were built to sense classical things like
force and temperature. But we are very adaptable creatures
and we‚Äôve been able to substitute abstract mathematics for
51

52
LECTURE 3. PRINCIPLES OF QUANTUM MECH
the missing senses that might have allowed us to directly
visualize quantum mechanics. And eventually we do develop
new kinds of intuition.
This lecture introduces the principles of quantum me-
chanics.
In order to describe those principles, we‚Äôll need
some new mathematical tools. Let‚Äôs get started.
3.1
Mathematical Interlude:
Linear Operators
3.1.1
Machines and Matrices
States in quantum mechanics are mathematically described
as vectors in a vector space. Physical observables‚Äîthe things
that you can measure‚Äîare described by linear operators.
We‚Äôll take that as an axiom, and we‚Äôll Ô¨Ånd out later (in
Section 3.1.5) that operators corresponding to physical ob-
servables must be Hermitian as well as linear. The corre-
spondence between operators and observables is subtle, and
understanding it will take some eÔ¨Äort.
Observables are the things you measure. For example,
we can make direct measurements of the coordinates of a
particle; the energy, momentum, or angular momentum of a
system; or the electric Ô¨Åeld at a point in space. Observables
are also associated with a vector space, but they are not
state-vectors. They are the things you measure‚ÄîœÉx would
be an example‚Äîand they are represented by linear opera-
tors. John Wheeler liked to call such mathematical objects
machines. He imagined a machine with two ports: an input

3.1. INTERLUDE: LINEAR OPERATORS
53
port and an output port. In the input port you insert a vec-
tor, such as |A‚ü©. The gears turn and the machine delivers a
result in the output port. This result is another vector, say
|B‚ü©.
Let‚Äôs denote the operator by the boldface letter M (for
‚Äúmachine‚Äù). Here is the equation to express the fact that M
acts on the vector |A‚ü©to give |B‚ü©:
M|A‚ü©= |B‚ü©.
Not every machine is a linear operator. Linearity implies a
few simple properties. To begin with, a linear operator must
give a unique output for every vector in the space. We can
imagine a machine that gives an output for some vectors,
but just grinds up others and gives nothing. This machine
would not be a linear operator. Something must come out
for anything you put in.
The next property states that when a linear operator
M acts on a multiple of an input vector, it gives the same
multiple of the output vector. Thus, if M|A‚ü©= |B‚ü©, and z
is any complex number, then
Mz|A‚ü©= z|B‚ü©.
The only other rule is that, when M acts on a sum of vectors,
the results are simply added together:
M{|A‚ü©+ |B‚ü©} = M|A‚ü©+ M|B‚ü©.
To give a concrete representation of linear operators, we re-
turn to the row and column vector representation of bra-

54
LECTURE 3. PRINCIPLES OF QUANTUM MECH
and ket-vectors that we used in Lecture 1. The row-column
notation depends on our choice of basis vectors. If the vector
space is N-dimensional, we choose a set of N orthonormal
(orthogonal and normalized) ket-vectors. Let‚Äôs label them
|j‚ü©, and their dual bra-vectors ‚ü®j|.
We are now going to take the equation
M|A‚ü©= |B‚ü©
and write it in component form. As we did in Eq. 1.3, we‚Äôll
represent an arbitrary ket |A‚ü©as a sum over basis vectors:
|A‚ü©=

j
Œ±j|j‚ü©.
Here, we‚Äôre using j as an index rather than i so you won‚Äôt be
tempted to think that we‚Äôre talking about the in spin state.
Now, we‚Äôll represent |B‚ü©in the same way and plug both of
these substitutions into M|A‚ü©= |B‚ü©. That gives

j
M|j‚ü©Œ±j =

j
Œ≤j|j‚ü©.
The last step is to take the inner product of both sides with
a particular basis vector ‚ü®k|, resulting in

j
‚ü®k|M|j‚ü©Œ±j =

j
Œ≤j‚ü®k|j‚ü©.
(3.1)
To make sense of this result, remember that ‚ü®k|j‚ü©is zero if
j and k are not equal, and 1 if they are equal. That means
that the sum on the right side collapses to a single term, Œ≤k.

3.1. INTERLUDE: LINEAR OPERATORS
55
On the left side, we see a set of quantities ‚ü®k|M|j‚ü©Œ±j. We
can abbreviate ‚ü®k|M|j‚ü©with the symbol mkj. Notice that
each mkj is just a complex number. To see why, think of
M operating on |j‚ü©to give some new ket-vector. The inner
product of ‚ü®k| with this new ket-vector must be a complex
number. The quantities mkj are called the matrix elements
of M and are often arranged into a square N √ó N matrix.
For example, if N = 3, we can write the symbolic equation
M =
‚éõ
‚éù
m11
m12
m13
m21
m22
m23
m31
m32
m33
‚éû
‚é†.
(3.2)
This equation involves a slight abuse of notation that would
give a purist indigestion. The left side is an abstract linear
operator and the right side is a concrete representation of it
in a particular basis. Equating them is sloppy but it should
not cause confusion.
Now let‚Äôs revisit Eq. 3.1 and replace ‚ü®k|M|j‚ü©with mkj.
We get

j
mkjŒ±j = Œ≤k.
(3.3)
We can write this in matrix form as well. Eq. 3.3 becomes
‚éõ
‚éù
m11
m12
m13
m21
m22
m23
m31
m32
m33
‚éû
‚é†
‚éõ
‚éù
Œ±1
Œ±2
Œ±3
‚éû
‚é†=
‚éõ
‚éù
Œ≤1
Œ≤2
Œ≤3
‚éû
‚é†.
(3.4)

56
LECTURE 3. PRINCIPLES OF QUANTUM MECH
You‚Äôre probably familiar with the rule for matrix multipli-
cation, but I will remind you just in case. To compute the
Ô¨Årst entry on the right, Œ≤1, take the Ô¨Årst row of the matrix
and ‚Äúdot‚Äù it into the Œ± column:
Œ≤1 = m11Œ±1 + m12Œ±2 + m13Œ±3.
For the second entry, dot the second row of the matrix with
the Œ± column:
Œ≤2 = m21Œ±1 + m22Œ±2 + m23Œ±3.
And so on. If you are not familiar with matrix multiplication,
run to your computer and look it up right away. It‚Äôs a crucial
part of our tool kit, and I will assume you know it from now
on.
There are both advantages and disadvantages to repre-
senting vectors and linear operators concretely with columns,
rows, and matrices (known collectively as components). The
advantages are obvious. Components provide a completely
explicit set of arithmetic rules for working the machine. The
disadvantage is that they depend on a speciÔ¨Åc choice of basis
vectors. The underlying relationships between vectors and
operators is independent of the particular basis we choose,
and the concrete representation obscures that fact.
3.1.2
Eigenvalues and Eigenvectors
In general, when a linear operator acts on a vector, it will
change the direction of the vector. This means that what

3.1. INTERLUDE: LINEAR OPERATORS
57
comes out of the machine will not just be the input vector
multiplied by a number. But for a particular linear operator,
there will be certain vectors whose directions are the same
when they come out as they were when they went in. These
special vectors are called eigenvectors. The deÔ¨Ånition of an
eigenvector of M is a vector |Œª‚ü©such that
M|Œª‚ü©= Œª|Œª‚ü©.
(3.5)
The double use of Œª is admittedly a little confusing. First
of all, Œª (as opposed to |Œª‚ü©) is a number‚Äîgenerally a com-
plex one, but still a number. On the other hand, |Œª‚ü©is a
ket-vector. Furthermore, it is a ket with a very special rela-
tionship to M. When |Œª‚ü©is fed into the machine M, all that
happens is that it gets multiplied by the number Œª. I‚Äôll give
you an example. If M is the 2 √ó 2 matrix
 1
2
2
1

,
then it‚Äôs easy to see that the vector
 1
1

just gets multiplied by 3 when M acts on it. Try it out. M
also happens to have another eigenvector:

1
‚àí1

.

58
LECTURE 3. PRINCIPLES OF QUANTUM MECH
When M acts on this eigenvector, it multiplies the vector
by a diÔ¨Äerent number, namely ‚àí1. On the other hand, if M
acts on the vector
 1
0

,
the vector is not simply multiplied by a number. M alters
the direction of the vector as well as its magnitude.
Just as the vectors that get multiplied by numbers when
M acts on them are called eigenvectors of M, the constants
that multiply them are called eigenvalues. In general, the
eigenvalues are complex numbers. Here is an example that
you can work out for yourself. Take the matrix
M =
 0
‚àí1
1
0

and show that the vector
 1
i

is an eigenvector with eigenvalue ‚àíi.
Linear operators can also act on bra-vectors. The nota-
tion for multiplying ‚ü®B| by M is
‚ü®B|M.
I will keep the discussion short by telling you the rule for this
type of multiplication. It is most simple in component form.

3.1. INTERLUDE: LINEAR OPERATORS
59
Remember that bra-vectors are represented in component
form as row vectors.
For example, the bra ‚ü®B| might be
represented by
‚ü®B| =

Œ≤‚àó
1
Œ≤‚àó
2
Œ≤‚àó
3

.
The rule is again just matrix multiplication. With a slight
abuse of notation,
‚ü®B|M =

Œ≤‚àó
1
Œ≤‚àó
2
Œ≤‚àó
3

‚éõ
‚éù
m11
m12
m13
m21
m22
m23
m31
m32
m33
‚éû
‚é†.
(3.6)
3.1.3
Hermitian Conjugation
You might think that if M|A‚ü©= |B‚ü©then ‚ü®A|M = ‚ü®B|, but if
you do you are wrong. The problem is complex conjugation.
Even when Z is just a complex number, if Z|A‚ü©= |B‚ü©, it is
not generally true that ‚ü®A|Z = ‚ü®B|. You have to complex-
conjugate Z when going from kets to bras: ‚ü®A|Z‚àó= ‚ü®B|.
Of course, if Z happens to be a real number, then complex
conjugation has no eÔ¨Äect‚Äîevery real number is equal to its
own complex conjugate.
What we need is a concept of complex conjugation for
operators. Let‚Äôs look at the equation M|A‚ü©= |B‚ü©in com-
ponent notation,

i
mjiŒ±i = Œ≤j,
and form its complex conjugate,

60
LECTURE 3. PRINCIPLES OF QUANTUM MECH

i
m‚àó
jiŒ±‚àó
i = Œ≤‚àó
j .
We would like to write this equation in matrix form, using
bras instead of kets. In doing this, we have to remember that
bra-vectors are represented by rows, not columns. For the
result to work out correctly, we also need to rearrange the
complex conjugate elements of the matrix M. The notation
for this rearrangement is M‚Ä†, as explained below. Our new
equation is
‚ü®A|M‚Ä† =

Œ±‚àó
1
Œ±‚àó
2
Œ±‚àó
3

‚éõ
‚éù
m‚àó
11
m‚àó
21
m‚àó
31
m‚àó
12
m‚àó
22
m‚àó
32
m‚àó
13
m‚àó
23
m‚àó
33
‚éû
‚é†.
(3.7)
Look carefully at the diÔ¨Äerence between the matrix in this
equation and the matrix in Eq. 3.6. You will see two diÔ¨Äer-
ences. The most obvious is the complex conjugation of each
element, but you can also see a diÔ¨Äerence in the element in-
dices. For example, where you see m23 in Eq. 3.6, you see
m‚àó
32 in Eq. 3.7. In other words, the rows and columns have
been interchanged.
When we change an equation from the ket form to the
bra form, we must modify the matrix in two steps:
1. Interchange the rows and the columns.
2. Complex-conjugate each matrix element.
In matrix notation, interchanging rows and columns is called
transposing and is indicated by a superscript T. Thus, the
transpose of the matrix M is

3.1. INTERLUDE: LINEAR OPERATORS
61
‚éõ
‚éù
m11
m12
m13
m21
m22
m23
m31
m32
m33
‚éû
‚é†
T
=
‚éõ
‚éù
m11
m21
m31
m12
m22
m32
m13
m23
m33
‚éû
‚é†.
Notice that transposing a matrix Ô¨Çips it about the main di-
agonal (the diagonal from the upper left to the lower right).
The complex conjugate of a transposed matrix is called
its Hermitian conjugate, denoted by a dagger. You could
think of the dagger as a hybrid of the star-notation used in
complex conjugation and the T used in transposition.
In
symbols,
M‚Ä† =

MT‚àó.
To summarize: if M acts on the ket |A‚ü©to give |B‚ü©, then it
follows that M‚Ä† acts on the bra ‚ü®A| to give ‚ü®B|. In symbols:
If
M|A‚ü©= |B‚ü©,
then
‚ü®A|M‚Ä† = ‚ü®B|.
3.1.4
Hermitian Operators
Real numbers play a special role in physics. The results of
any measurements are real numbers. Sometimes, we mea-
sure two quantities, put them together with an i (forming a

62
LECTURE 3. PRINCIPLES OF QUANTUM MECH
complex number), and call this number the result of a mea-
surement. But it‚Äôs actually just a way of combining two real
measurements. If we want to be pedantic, we might say that
observable quantities are equal to their own complex con-
jugates. That‚Äôs of course just a fancy way of saying they
are real. We are going to Ô¨Ånd out very soon that quantum
mechanical observables are represented by linear operators.
What kind of linear operators? The kind that are the clos-
est thing to a real operator. Observables in quantum me-
chanics are represented by linear operators that are equal to
their own Hermitian conjugates. They are called Hermitian
operators after the French mathematician Charles Hermite.
Hermitian operators satisfy the property
M = M‚Ä†.
In terms of matrix elements, this can be stated as
mji = m‚àó
ij.
In other words, if you Ô¨Çip a Hermitian matrix about the main
diagonal and then take its complex conjugate, the result is
the same as the original matrix. Hermitian operators (and
matrices) have some special properties. The Ô¨Årst is that their
eigenvalues are all real. Let‚Äôs prove it.
Suppose Œª and |Œª‚ü©represent an eigenvalue and the corre-
sponding eigenvector of the Hermitian operator L. In sym-
bols,

3.1. INTERLUDE: LINEAR OPERATORS
63
L|Œª‚ü©= Œª|Œª‚ü©.
Then, by the deÔ¨Ånition of Hermitian conjugation,
‚ü®Œª|L‚Ä† = ‚ü®Œª|Œª‚àó.
However, since L is Hermitian, it is equal to L‚Ä†. Thus, we
can rewrite the two equations as
L|Œª‚ü©= Œª|Œª‚ü©
(3.8)
and
‚ü®Œª|L = ‚ü®Œª|Œª‚àó.
(3.9)
Now multiply Eq. 3.8 by ‚ü®Œª| and Eq. 3.9 by |Œª‚ü©. They become
‚ü®Œª|L|Œª‚ü©= Œª ‚ü®Œª|Œª‚ü©
and
‚ü®Œª|L|Œª‚ü©= Œª‚àó‚ü®Œª|Œª‚ü©.
Obviously, for both equations to be true, Œª must equal Œª‚àó. In
other words, Œª (and therefore any eigenvalue of a Hermitian
operator) must be real.

64
LECTURE 3. PRINCIPLES OF QUANTUM MECH
3.1.5
Hermitian Operators and
Orthonormal Bases
We come now to the basic mathematical theorem‚ÄîI will call
it the fundamental theorem‚Äîthat serves as a foundation of
quantum mechanics. The basic idea is that observable quan-
tities in quantum mechanics are represented by Hermitian
operators. It‚Äôs a very simple theorem, but it‚Äôs an extremely
important one. We can state it more precisely as follows:
The Fundamental Theorem
‚Ä¢ The eigenvectors of a Hermitian operator are a com-
plete set. This means that any vector the operator
can generate can be expanded as a sum of its eigen-
vectors.
‚Ä¢ If Œª1 and Œª2 are two unequal eigenvalues of a Hermi-
tian operator, then the corresponding eigenvectors are
orthogonal.
‚Ä¢ Even if the two eigenvalues are equal, the correspond-
ing eigenvectors can be chosen to be orthogonal. This
situation, where two diÔ¨Äerent eigenvectors have the
same eigenvalue, has a name: it‚Äôs called degeneracy.
Degeneracy comes into play when two operators have
simultaneous eigenvectors, as discussed later on in Sec-
tion 5.1.
One can summarize the fundamental theorem as follows: The
eigenvectors of a Hermitian operator form an orthonormal
basis. Let‚Äôs prove it, beginning with the second bullet item.

3.1. INTERLUDE: LINEAR OPERATORS
65
According to the deÔ¨Ånition of eigenvectors and eigenval-
ues, we can write
L|Œª1‚ü©= Œª1|Œª1‚ü©
L|Œª2‚ü©= Œª2|Œª2‚ü©.
Now, using the fact that L is Hermitian (its own Hermitian
conjugate), we can Ô¨Çip the Ô¨Årst equation into a bra equation.
Thus,
‚ü®Œª1|L = Œª1‚ü®Œª1|
L|Œª2‚ü©= Œª2|Œª2‚ü©.
By now, the trick should be obvious, but I will spell it out.
Take the Ô¨Årst equation and form its inner product with |Œª2‚ü©.
Then, take the second equation and form its inner product
with ‚ü®Œª1|. The result is
‚ü®Œª1|L|Œª2‚ü©= Œª1‚ü®Œª1|Œª2‚ü©
‚ü®Œª1|L|Œª2‚ü©= Œª2‚ü®Œª1|Œª2‚ü©.
By subtracting, we get
(Œª1 ‚àíŒª2)‚ü®Œª1|Œª2‚ü©= 0.
Therefore, if Œª1 and Œª2 are diÔ¨Äerent, the inner product ‚ü®Œª1|Œª2‚ü©
must be zero. In other words, the two eigenvectors must be
orthogonal.

66
LECTURE 3. PRINCIPLES OF QUANTUM MECH
Next, let‚Äôs prove that even if Œª1 = Œª2, the two eigenvec-
tors can be chosen to be orthogonal. Suppose
L|Œª1‚ü©
=
Œª|Œª1‚ü©
L|Œª2‚ü©
=
Œª|Œª2‚ü©.
(3.10)
In other words, there are two distinct eigenvectors with the
same eigenvalue. It should be clear that any linear combi-
nation of the two eigenvectors is also an eigenvector with
the same eigenvalue. With this much freedom, it is always
possible to Ô¨Ånd two orthogonal linear combinations.
Let‚Äôs see how. Consider an arbitrary linear combination
of these two eigenvectors:
|A‚ü©= Œ±|Œª1‚ü©+ Œ≤|Œª2‚ü©.
Operating on both sides with L, we get
L|A‚ü©= Œ±L|Œª1‚ü©+ Œ≤L|Œª2‚ü©,
L|A‚ü©= Œ±Œª|Œª1‚ü©+ Œ≤Œª|Œª2‚ü©,
and Ô¨Ånally
L|A‚ü©= Œª(Œ±|Œª1‚ü©+ Œ≤|Œª2‚ü©) = Œª|A‚ü©.
This equation demonstrates that any linear combination of
|Œª1‚ü©and |Œª2‚ü©is also an eigenvector of L, with the same

3.1. INTERLUDE: LINEAR OPERATORS
67
eigenvalue. By assumption, these two vectors are linearly
independent‚Äîotherwise, they would not represent distinct
states. We will also suppose that they span the subspace
of eigenvectors of L that have eigenvalue Œª.
There is a
straightforward process, called the Gram-Schmidt procedure,
for Ô¨Ånding an orthonormal basis for a subspace, given a set
of independent vectors that spans the subspace. In plain En-
glish, we can Ô¨Ånd two orthonormal eigenvectors by writing
them as a linear combination of |Œª1‚ü©and |Œª2‚ü©. We outline
the Gram-Schmidt procedure below, in Section 3.1.6.
The Ô¨Ånal part of the theorem states that the eigenvectors
are complete. In other words, if the space is N-dimensional,
there will be N orthonormal eigenvectors. The proof is easy
and I will leave it to you.
Exercise 3.1: Prove the following: If a vector space is N-
dimensional, an orthonormal basis of N vectors can be con-
structed from the eigenvectors of a Hermitian operator.
3.1.6
The Gram-Schmidt Procedure
Sometimes we encounter a set of linearly independent eigen-
vectors that do not form an orthonormal set.
This typi-
cally happens when a system has degenerate states‚Äîdistinct
states that have the same eigenvalue. In that situation, we
can always use the linearly independent vectors we have, to
create an orthonormal set that spans the same space. The
method is the Gram-Schmidt procedure I alluded to earlier.
Fig. 3.1 illustrates how it works for the simple case of two

68
LECTURE 3. PRINCIPLES OF QUANTUM MECH
linearly independent vectors. We start with the two vectors
‚ÉóV1 and ‚ÉóV2, and from these we construct two orthonormal
vectors, ÀÜv1 and ÀÜv2.
Figure 3.1: The Gram-Schmidt Procedure. Given two lin-
early independent vectors, ‚ÉóV1 and ‚ÉóV2, that are not necessar-
ily orthogonal, we can construct two orthonormal vectors, ÀÜv1
and ÀÜv2. ‚ÉóV2‚ä•is an intermediate result used in the construc-
tion process. We can extend this procedure to larger sets of
linearly independent vectors.
The Ô¨Årst step is to divide ‚ÉóV1 by its own length, |‚ÉóV1|, which
gives us a unit vector parallel to ‚ÉóV1.
We‚Äôll call that unit
vector ÀÜv1, and ÀÜv1 becomes the Ô¨Årst vector in our orthonormal
set. Next, we project ‚ÉóV2 onto the direction of ÀÜv1 by forming
the inner product ‚ü®‚ÉóV2|ÀÜv1‚ü©. Now, we subtract ‚ü®‚ÉóV2|ÀÜv1‚ü©from
‚ÉóV2. We‚Äôll call the result of this subtraction ‚ÉóV2‚ä•. You can see
in Fig. 3.1 that ‚ÉóV2‚ä•is orthogonal to ÀÜv1. Lastly, we divide
‚ÉóV2‚ä•by its own length to form the second member of our

3.2. THE PRINCIPLES
69
orthonormal set, ÀÜv2. It should be clear that we can extend
this procedure to larger sets of linearly independent vectors
in more dimensions. For instance, if we had a third linearly
independent vector, say ‚ÉóV3, pointing out of the page, we
would subtract its projections onto each of the unit vectors
ÀÜv1 and ÀÜv2, and then divide the result by its own length.1
3.2
The Principles
We are now fully prepared to state the principles of quantum
mechanics, so without further ado, let‚Äôs do it.
The principles all involve the idea of an observable, and
they presuppose the existence of an underlying complex vec-
tor space whose vectors represent system states. In this lec-
ture, we present the four principles that do not involve the
evolution of state-vectors with time. In Lecture 4, we will
add a Ô¨Åfth principle that addresses the time development of
system states.
An observable could also be called a measurable.
It‚Äôs
a thing that you can measure with a suitable apparatus.
Earlier, we spoke about measuring the components of a spin,
œÉx, œÉy, and œÉz. These are examples of observables. We‚Äôll
come back to them, but Ô¨Årst let‚Äôs look at the principles:
‚Ä¢ Principle 1: The observable or measurable quantities of
quantum mechanics are represented by linear operators
L.
1In this example, the term out of the page does not mean ‚ÉóV3 is
necessarily orthogonal to the plane of the page.
The ability to use
nonorthogonal vectors as a starting point is the main feature of the
Gram-Schmidt Procedure.

70
LECTURE 3. PRINCIPLES OF QUANTUM MECH
I realize that this is the kind of hopelessly abstract
statement that makes people give up on quantum me-
chanics and take up surÔ¨Ång instead. Don‚Äôt worry‚Äîits
meaning will become clear by the end of the lecture.
We‚Äôll soon see that L must also be Hermitian. Some
authors regard this as a postulate, or basic principle.
We have chosen instead to derive it from the other
principles. The end result is the same either way: the
operators that represent observables are Hermitian.
‚Ä¢ Principle 2: The possible results of a measurement are
the eigenvalues of the operator that represents the ob-
servable. We‚Äôll call these eigenvalues Œªi. The state for
which the result of a measurement is unambiguously
Œªi is the corresponding eigenvector |Œªi‚ü©. Don‚Äôt unpack
your surfboard just yet.
Here‚Äôs another way to say it: if the system is in the
eigenstate |Œªi‚ü©, the result of a measurement is guaran-
teed to be Œªi.
‚Ä¢ Principle 3: Unambiguously distinguishable states are
represented by orthogonal vectors.
‚Ä¢ Principle 4: If |A‚ü©is the state-vector of a system, and
the observable L is measured, the probability to ob-
serve value Œªi is
P(Œªi) = ‚ü®A|Œªi‚ü©‚ü®Œªi|A‚ü©.
(3.11)
I‚Äôll remind you that the Œªi are the eigenvalues of L,
and |Œªi‚ü©are the corresponding eigenvectors.

3.2. THE PRINCIPLES
71
These brief statements are hardly self-explanatory, and we‚Äôll
need to Ô¨Çesh them out. For the moment, let‚Äôs accept the Ô¨Årst
item, namely that every observable is identiÔ¨Åed with a lin-
ear operator. We can already begin to see that an operator
is a way of packaging up states along with their eigenval-
ues, which are the possible results of measuring those states.
These ideas should become clear as we move forward.
Let‚Äôs recall some important points from our earlier dis-
cussion of spins. First of all, the result of a measurement
is generally statistically uncertain. However, for any given
observable, there are particular states for which the result is
absolutely certain. For example, if the spin-measuring ap-
paratus A is oriented along the z axis, the state |u‚ü©always
leads to the value œÉz = +1. Likewise, the state |d‚ü©never gives
anything but œÉz = ‚àí1. Principle 1 gives us a new way to look
at these facts. It implies that each observable (œÉx, œÉy, and
œÉz) is identiÔ¨Åed with a speciÔ¨Åc linear operator in the two-
dimensional space of states describing the spin.
When an observable is measured, the result is always a
real number drawn from a set of possible results. For exam-
ple, if the energy of an atom is measured, the result will be
one of the established energy levels of the atom. For the fa-
miliar case of the spin, the possible values of any of the com-
ponents are ¬±1. The apparatus never gives any other result.
Principle 2 deÔ¨Ånes the relation between the operator repre-
senting an observable and the possible numerical outputs of
the measurement. Namely, the result of a measurement is
always one of the eigenvalues of the corresponding operator.
Thus, each component of the spin operator must have two

72
LECTURE 3. PRINCIPLES OF QUANTUM MECH
eigenvalues equal to ¬±1.2
Principle 3 is the most interesting. At least I Ô¨Ånd it so. It
speaks of unambiguously distinct states, a key idea that we
have already encountered. Two states are physically distinct
if there is a measurement that can tell them apart without
ambiguity. For example, |u‚ü©and |d‚ü©can be distinguished by
measuring œÉz. If you are handed a spin and told that it is
either in the state |u‚ü©or the state |d‚ü©, to Ô¨Ånd out which of
the two states is the right one, all you have to do is align A
with the z axis and measure œÉz. There is no possibility of a
mistake. The same is true for |l‚ü©and |r‚ü©. You can distinguish
them by measuring œÉx.
But suppose instead that you are told the spin is in one
of the two states, |u‚ü©or |r‚ü©(up or right). There is nothing
you can measure that will unambiguously tell you the spin‚Äôs
true state. Measuring œÉz won‚Äôt do it. If you get œÉz = +1,
it is possible that the initial state was |r‚ü©since there is a 50
percent probability of getting this answer in the state |r‚ü©. For
that reason, |u‚ü©and |d‚ü©are said to be physically distinguish-
able, but |u‚ü©and |r‚ü©are not. One might say that the inner
product of two states is a measure of the inability to dis-
tinguish them with certainty. Sometimes this inner product
is called the overlap. Principle 3 requires physically distinct
states to be represented by orthogonal state-vectors, that is,
vectors with no overlap. Thus, for spin states, ‚ü®u|d‚ü©= 0 but
‚ü®u|r‚ü©=
1
‚àö
2.
2We have not yet explained what we mean by a ‚Äúcomponent‚Äù of
the spin operator. We will do so shortly.

3.2. THE PRINCIPLES
73
Finally, Principle 4 quantiÔ¨Åes these ideas in a rule that
expresses the probabilities for various outcomes of an experi-
ment. If we assume that a system has been prepared in state
|A‚ü©, and subsequently the observable L is measured, then the
outcome will be one of the eigenvalues Œªi of the operator L.
But, in general, there is no way to tell for certain which of
these values will be observed. There is only a probability‚Äî
let us call it P(Œªi)‚Äîthat the outcome will be Œªi. Principle 4
tells us how to calculate that probability, and it is expressed
in terms of the overlap of |A‚ü©and |Œªi‚ü©. More precisely, the
probability is the square of the magnitude of the overlap:
P(Œªi) = |‚ü®A|Œªi‚ü©|2
or, equivalently,
P(Œªi) = ‚ü®A|Œªi‚ü©‚ü®Œªi|A‚ü©.
You might be wondering why the probability is not the over-
lap itself. Why the square of the overlap? Keep in mind
that the inner product of two vectors is not always positive,
or even real. Probabilities, on the other hand, are both pos-
itive and real. So it would not make sense to identify P(Œªi)
with ‚ü®A|Œªi‚ü©. But the square of the magnitude, ‚ü®A|Œªi‚ü©‚ü®Œªi|A‚ü©,
is always positive and real and thus can be identiÔ¨Åed with
the probability of a given outcome.
An important consequence of the principles is as follows:
The operators that represent observables are Hermitian.

74
LECTURE 3. PRINCIPLES OF QUANTUM MECH
The reason for this is twofold. First, since the result of an
experiment must be a real number, the eigenvalues of an
operator L must also be real. Secondly, the eigenvectors that
represent unambiguously distinguishable results must have
diÔ¨Äerent eigenvalues, and must also be orthogonal. These
conditions are suÔ¨Écient to prove that L must be Hermitian.
3.3
An Example: Spin Operators
It may be hard to believe, but single spins‚Äîas simple as
they are‚Äîstill have a lot more to teach us about quantum
mechanics, and we plan to milk them for all they‚Äôre worth.
Our goal in this section is to write down the spin operators
in concrete form, as 2 √ó 2 matrices. Then, we‚Äôll get to see
how they work in speciÔ¨Åc situations. We‚Äôll build up our spin
operators and state-vectors shortly. But before we dive into
the details, I‚Äôd like to say a little more about how operators
are related to physical measurements. The relationship is a
subtle one, and we‚Äôll say more about it as we go.
As you know, physicists recognize various types of physi-
cal quantities, such as scalars and vectors. It should come as
no surprise, then, that an operator associated with the mea-
surement of a vector (such as spin) has a vector character of
its own.
In our travels so far, we have seen more than one kind of
vector. The 3-vector is the most straightforward and serves
as a prototype.
It‚Äôs a mathematical representation of an
arrow in three-dimensional space, and is often represented by
three real numbers, written out as a column matrix. Because
their components are real-valued, 3-vectors are not quite rich

3.4. CONSTRUCTING SPIN OPERATORS
75
enough to represent quantum states. For that, we need bras
and kets, which have complex-valued components.
What sort of vector is the spin operator œÉ? It is deÔ¨Ånitely
not a state-vector (a bra or a ket).
It‚Äôs not exactly a 3-
vector either, but it does have a strong family resemblance
because it‚Äôs associated with a direction in space. In fact, we
will frequently use œÉ as though it were a simple 3-vector.
However, we‚Äôll try to keep things straight by calling œÉ a 3-
vector operator.
But what does that actually mean? In physical terms,
it means this: Just as a spin-measuring apparatus can only
answer questions about a spin‚Äôs orientation in a speciÔ¨Åc di-
rection, a spin operator can only provide information about
the spin component in a speciÔ¨Åc direction.
To physically
measure spin in a diÔ¨Äerent direction, we need to rotate the
apparatus to point in the new direction. The same idea ap-
plies to the spin operator‚Äîif we want it to tell us about
the spin component in a new direction, it too must be ‚Äúro-
tated,‚Äù but this kind of rotation is accomplished mathemat-
ically. The bottom line is that there is a spin operator for
each direction in which the apparatus can be oriented.
3.4
Constructing Spin Operators
Now, let‚Äôs work out the details of spin operators. The Ô¨Årst
goal is to construct operators to represent the components
of spin, œÉx, œÉy, and œÉz. Then we‚Äôll build on those results to
construct an operator that represents a spin component in
any direction. As usual, we begin with œÉz. We know that œÉz
has deÔ¨Ånite, unambiguous values for the states |u‚ü©and |d‚ü©,

76
LECTURE 3. PRINCIPLES OF QUANTUM MECH
and that the corresponding measurement values are œÉz = +1
and œÉz = ‚àí1. Here is what the Ô¨Årst three principles tell us:
‚Ä¢ Principle 1: Each component of œÉ is represented by a
linear operator.
‚Ä¢ Principle 2: The eigenvectors of œÉz are |u‚ü©and |d‚ü©.
The corresponding eigenvalues are +1 and ‚àí1.
We
can express this with the abstract equations
œÉz|u‚ü©
=
|u‚ü©
œÉz|d‚ü©
=
‚àí|d‚ü©.
(3.12)
‚Ä¢ Principle 3: States |u‚ü©and |d‚ü©are orthogonal to each
other. This can be expressed as
‚ü®u|d‚ü©= 0.
(3.13)
Recalling our column representations of |u‚ü©and |d‚ü©from Eqs.
2.11 and 2.12, we can write Eqs. 3.12 in matrix form as
 (œÉz)11
(œÉz)12
(œÉz)21
(œÉz)22
  1
0

=
 1
0

(3.14)
and
 (œÉz)11
(œÉz)12
(œÉz)21
(œÉz)22
  0
1

= ‚àí
 0
1

.
(3.15)

3.4. CONSTRUCTING SPIN OPERATORS
77
There is only one matrix that satisÔ¨Åes these equations. I
leave it as an exercise to prove
 (œÉz)11
(œÉz)12
(œÉz)21
(œÉz)22

=
 1
0
0
‚àí1

(3.16)
or, more concisely,
œÉz =
 1
0
0
‚àí1

.
(3.17)
Exercise 3.2:
Prove that Eq. 3.16 is the unique solution
to Eqs. 3.14 and 3.15.
This is our very Ô¨Årst example of a quantum mechanical
operator. Let‚Äôs summarize what went into it. First, some ex-
perimental data: there are certain states that we called |u‚ü©
and |d‚ü©, in which the measurement of œÉz gives unambiguous
results ¬±1. Next, the principles told us that |u‚ü©and |d‚ü©are
orthogonal and are eigenvectors of a linear operator œÉz. Fi-
nally, we learned from the principles that the corresponding
eigenvalues are the observed (or measured) values, again ¬±1.
That‚Äôs all it takes to derive Eq. 3.17.
Can we do the same for the other two components of spin,
œÉx and œÉy? Yes, we can.3 The eigenvectors of œÉx are |r‚ü©and
|l‚ü©, with eigenvalues +1 and ‚àí1 respectively. In equation
form,
3We are not trying to slip in a political slogan. Really. Just say no
to slogans.

78
LECTURE 3. PRINCIPLES OF QUANTUM MECH
œÉx|r‚ü©
=
|r‚ü©
œÉx|l‚ü©
=
‚àí|l‚ü©.
(3.18)
Recall that |r‚ü©and |l‚ü©are linear superpositions of |u‚ü©and
|d‚ü©:
|r‚ü©
=
1
‚àö
2|u‚ü©+ 1
‚àö
2|d‚ü©
|l‚ü©
=
1
‚àö
2|u‚ü©‚àí1
‚àö
2|d‚ü©.
(3.19)
Substituting the appropriate column vectors for |u‚ü©and |d‚ü©,
we get
|r‚ü©=

1
‚àö
2
1
‚àö
2

|l‚ü©=

1
‚àö
2
‚àí1
‚àö
2

.
To make Eqs. 3.18 concrete, we can write them in matrix
form:
 (œÉx)11
(œÉx)12
(œÉx)21
(œÉx)22
 
1
‚àö
2
1
‚àö
2

=

1
‚àö
2
1
‚àö
2

and

3.4. CONSTRUCTING SPIN OPERATORS
79
 (œÉx)11
(œÉx)12
(œÉx)21
(œÉx)22
 
1
‚àö
2
‚àí1
‚àö
2

= ‚àí

1
‚àö
2
‚àí1
‚àö
2

.
If you write these equations out in longhand form, they turn
into four easily solved equations for the matrix elements
(œÉx)11, (œÉx)12, (œÉx)21, and (œÉx)22. Here is the solution:
 (œÉx)11
(œÉx)12
(œÉx)21
(œÉx)22

=
 0
1
1
0

or
œÉx =
 0
1
1
0

.
Finally, we can do the same for œÉy. The eigenvectors of œÉy
are the in and out states |i‚ü©and |o‚ü©:
|i‚ü©= 1
‚àö
2|u‚ü©+
i
‚àö
2|d‚ü©
|o‚ü©= 1
‚àö
2|u‚ü©‚àí
i
‚àö
2|d‚ü©.
In component form, these equations become
|i‚ü©=

1
‚àö
2
i
‚àö
2

|o‚ü©=

1
‚àö
2
‚àíi
‚àö
2

,
and an easy calculation gives

80
LECTURE 3. PRINCIPLES OF QUANTUM MECH
œÉy =
 0
‚àíi
i
0

.
To summarize, the three operators œÉx, œÉy, and œÉz are repre-
sented by the three matrices
œÉz
=
 1
0
0
‚àí1

œÉx
=
 0
1
1
0

œÉy
=
 0
‚àíi
i
0

.
(3.20)
These three matrices are very famous and carry the name of
their discoverer. They are the Pauli matrices.4
3.5
A Common Misconception
This is a convenient time to warn you about a potential haz-
ard. The correspondence between operators and measure-
ments is fundamental in quantum mechanics. It is also very
easy to misunderstand. Here‚Äôs what is true about operators
in quantum mechanics:
1. Operators are the things we use to calculate eigenvalues
and eigenvectors.
2. Operators act on state-vectors (which are abstract math-
ematical objects), not on actual physical systems.
4Along with the 2 √ó 2 identity matrix, they are also quaternions.

3.5. A COMMON MISCONCEPTION
81
3. When an operator acts on a state-vector, it produces
a new state-vector.
Having said what is true about operators, I want to warn
you about a common misconception. It is often thought that
measuring an observable is the same as operating with the
corresponding operator on the state. For example, suppose
we are interested in measuring an observable L. The mea-
surement is some kind of operation that the apparatus does
to the system, but that operation is in no way the same as
acting on the state with the operator L. For example, if the
state of the system before we do the measurement is |A‚ü©, it
is not correct to say that the measurement of L changes the
state to L|A‚ü©.
To make sense of this, let‚Äôs look closely at an example.
Fortunately, the spin example of the previous subsection is
just what we need. Recall Eqs. 3.12:
œÉz|u‚ü©= |u‚ü©
œÉz|d‚ü©= ‚àí|d‚ü©.
In these situations, there is no trap because |u‚ü©and |d‚ü©are
eigenvectors of œÉz. If the system is prepared in, say, the |d‚ü©
state, a measurement will deÔ¨Ånitely give the result ‚àí1, and
the œÉz operator transforms the prepared state into the cor-
responding post-measurement state, ‚àí|d‚ü©. The state ‚àí|d‚ü©is
the same as |d‚ü©except for a multiplicative constant, so the
two states are really the same. No problems here.

82
LECTURE 3. PRINCIPLES OF QUANTUM MECH
But now let‚Äôs review the action of œÉz on the prepared
state |r‚ü©, which is not one of its eigenvectors. From Eq. 3.19,
we know that
|r‚ü©= 1
‚àö
2|u‚ü©+ 1
‚àö
2|d‚ü©.
Acting on this state-vector with œÉz gives the result
œÉz|r‚ü©= 1
‚àö
2œÉz|u‚ü©+ 1
‚àö
2œÉz|d‚ü©
or
œÉz|r‚ü©= 1
‚àö
2|u‚ü©‚àí1
‚àö
2|d‚ü©.
(3.21)
OK, here is our trap. Despite what you might think, the
state-vector on the right-hand side of Eq. 3.21 is deÔ¨Ånitely
not the state that would result from a measurement of œÉz.
That measurement result would be either +1, leaving the
system in state |u‚ü©, or ‚àí1, leaving it in state |d‚ü©. Neither
of these results would leave the system state-vector in the
superposition represented by Eq. 3.21.
But surely that state-vector must have something to do
with the measurement result? In fact, it does. We‚Äôll Ô¨Ånd
part of the answer in Lecture 4, where we‚Äôll see how the new
state-vector allows us to calculate the probabilities of each
possible outcome of the measurement. However, the result of
a measurement cannot be properly described without taking
the apparatus into account as part of the system.
What
actually does happen during a measurement is the subject
of Section 7.8.

3.6. 3-VECTOR OPERATORS REVISITED
83
3.6
3-Vector Operators Revisited
Now, let‚Äôs revisit the idea of a 3-vector operator.
I have
called œÉx, œÉy, and œÉz the components of spin along the three
axes, implying that they are the components of some kind
of 3-vector. This is a good time to return to the two notions
of vectors that come up all the time in physics. First, there
is your garden-variety vector in ordinary three-dimensional
space, which we‚Äôve decided to call a 3-vector.
As we‚Äôve
seen, a 3-vector has components along the three directions
of space.
The other completely distinct meaning of the term vector
is the state-vector of a system. Thus, |u‚ü©and |d‚ü©, |r‚ü©and |l‚ü©,
and |i‚ü©and |o‚ü©are state-vectors in a two-dimensional space
of spin states. What about œÉx, œÉy, and œÉz? Are they vectors,
and if so, what kind?
Clearly, they are not state-vectors; they are operators
(written as matrices) that correspond to the three measur-
able components of spin. In fact, these 3-vector operators
represent a new type of vector. They are diÔ¨Äerent both from
state-vectors, and from ordinary 3-vectors.
However, be-
cause spin operators behave so much like 3-vectors, it does
no harm to think of them in that way, and that‚Äôs what we‚Äôll
do here.
We measure spin components by orienting the apparatus
A along any one of the three axes and then activating it.
But then why not orient A along any axis and measure the
component of œÉ along that axis? In other words, take any
unit 3-vector ÀÜn with components nx, ny, and nz, and orient

84
LECTURE 3. PRINCIPLES OF QUANTUM MECH
the apparatus A with its arrow along ÀÜn. Activating A would
then measure the component of œÉ along the axis ÀÜn. There
must be an operator that corresponds to this measurable
quantity.
If œÉ really behaves like a 3-vector, then the component of
œÉ along ÀÜn is nothing but the ordinary dot product of œÉ and
ÀÜn.5,6 Let‚Äôs denote that component of œÉ by œÉn, so that
œÉn = ‚ÉóœÉ ¬∑ ÀÜn
or, in expanded form,
œÉn = œÉxnx + œÉyny + œÉznz.
(3.22)
To clarify the meaning of this equation, keep in mind that
the components of ÀÜn are just numbers. They themselves are
not operators. Eq. 3.22 describes a vector-operator that is
constructed as the sum of three terms, each containing a
numerical coeÔ¨Écient nx, ny, or nz. To be more concrete, we
can write Eq. 3.22 in matrix form:
œÉn = nx
 0
1
1
0

+ ny
 0
‚àíi
i
0

+ nz
 1
0
0
‚àí1

.
5We‚Äôll start using the notation ‚ÉóœÉ, except when referring to compo-
nents, such as œÉx.
6The careful reader may object, because the result of this ‚Äúordi-
nary‚Äù dot product is a 2 √ó 2 matrix rather than a scalar, so it‚Äôs not
quite ordinary. Perhaps there is some comfort in the fact that the re-
sulting matrix operator corresponds to a vector component, which is a
scalar. It all works out in the end.

3.7. REAPING THE RESULTS
85
Or even more explicitly, we can combine these three terms
into a single matrix:
œÉn =

nz
(nx ‚àíiny)
(nx + iny)
‚àínz

.
(3.23)
What is this good for? Not much, until we Ô¨Ånd the eigenvec-
tors and eigenvalues of œÉn. But once we do that, we will know
the possible outcomes of a measurement along the direction
of ÀÜn. And we will also be able to calculate probabilities for
those outcomes. In other words, we will have a complete pic-
ture of spin measurements in three-dimensional space. That
is pretty darn cool, if I say so myself.
3.7
Reaping the Results
We are now positioned to make some real calculations, some-
thing that should make your inner physicist jump for joy.
Let‚Äôs look at the special case where ÀÜn lies in the x‚Äìz plane,
which is the plane of this page. Since ÀÜn is a unit vector, we
can write
nz = cos Œ∏
nx = sin Œ∏
ny = 0,
where Œ∏ is the angle between the z axis and the ÀÜn axis. Plug-
ging these values into Eq. 3.23, we can write
œÉn =
 cos Œ∏
sin Œ∏
sin Œ∏
‚àícos Œ∏

.

86
LECTURE 3. PRINCIPLES OF QUANTUM MECH
Exercise 3.3: Calculate the eigenvectors and eigenvalues of
œÉn. Hint: Assume the eigenvector Œª1 has the form
 cos Œ±
sin Œ±

,
where Œ± is an unknown parameter.
Plug this vector into
the eigenvalue equation and solve for Œ± in terms of Œ∏. Why
did we use a single parameter Œ±? Notice that our suggested
column vector must have unit length.
Here are the results:
Œª1 = 1
|Œª1‚ü©=
‚éõ
‚éù
cos Œ∏
2
sin Œ∏
2
‚éû
‚é†
and
Œª2 = ‚àí1
|Œª2‚ü©=
‚éõ
‚éù
‚àísin Œ∏
2
cos Œ∏
2
‚éû
‚é†.
Notice some important facts. First, the two eigenvalues are
again +1 and ‚àí1. This should come as no surprise; the ap-
paratus A can only give one of these two answers no matter
which way it points. But it‚Äôs good to see this come out of
the equations. The second fact is that the two eigenvectors
are orthogonal.

3.7. REAPING THE RESULTS
87
We are now ready to make an experimental prediction.
Suppose A initially points along the z axis and that we pre-
pare a spin in the up state |u‚ü©. Then, we rotate A so that it
lies along the ÀÜn axis. What is the probability of observing
œÉn = +1? According to Principle 4, and using the row and
column expansions of ‚ü®u| and |Œª1‚ü©, the answer is
P(+1) = |‚ü®u|Œª1‚ü©|2 = cos2 Œ∏
2.
(3.24)
Similarly, for the same setup,
P(‚àí1) = |‚ü®u|Œª2‚ü©|2 = sin2 Œ∏
2.
(3.25)
With this result, we have come nearly full circle.
When
introducing spins, we made the claim that if we prepare a
large number of them in the up state and then measure their
component along ÀÜn, at angle Œ∏ to the z axis, then the average
value of the measured results would be cos Œ∏‚Äîthe same result
we would get for a simple 3-vector in classical physics. Does
our mathematical framework give the same result? It had
better! If a theory disagrees with experiment, it‚Äôs the theory
that has to leave town. Let‚Äôs see how well our theory holds
up so far.
Unfortunately, we need to cheat a little by using an equa-
tion that we will not fully explain until the next lecture. This
is the equation that tells us how to calculate the average
value (also called the expectation value) of a measurement.
Here it is:
‚ü®L‚ü©=

i
ŒªiP(Œªi).
(3.26)

88
LECTURE 3. PRINCIPLES OF QUANTUM MECH
It‚Äôs worth mentioning that Eq. 3.26 is just a standard formula
for an average value. It‚Äôs not unique to quantum mechanics.
To calculate the expectation value of a measurement cor-
responding to the operator L, we multiply each eigenvalue
by its probability, and then sum the results. Of course, the
operator we‚Äôre looking at now is just œÉn, and we already have
all the values we need. Let‚Äôs plug them in. Using Eqs. 3.24
and 3.25, along with our known eigenvalues, we can write
‚ü®œÉn‚ü©= (+1) cos2 Œ∏
2 + (‚àí1) sin2 Œ∏
2
or
‚ü®œÉn‚ü©= cos2 Œ∏
2 ‚àísin2 Œ∏
2.
If you remember your trigonometry, this gives
‚ü®œÉn‚ü©= cos Œ∏,
which agrees perfectly with experiment. Yes! We‚Äôve done it!
Having come this far, you might want to try your hand
on a slightly more general problem. As before, we start with
the apparatus A pointing in the z direction. But now, once
the spin has been prepared in the up state, we can rotate
A to an arbitrary direction in space for the second set of
measurements. In this situation, ny Ã∏= 0. Go ahead and try
it.
Exercise 3.4: Let nz = cos Œ∏, nx = sin Œ∏ cos œÜ, and ny =
sin Œ∏ sin œÜ. Angles Œ∏ and œÜ are deÔ¨Åned according to the usual
conventions for spherical coordinates (Fig. 3.2). Compute
the eigenvalues and eigenvectors for the matrix of Eq. 3.23.

3.7. REAPING THE RESULTS
89
Figure 3.2:
Spherical Coordinates.
This diagram illus-
trates conventional spherical coordinate labels r, Œ∏, and œÜ.
It also illustrates the conversion to Cartesian coordinates:
x = r sin Œ∏ cos œÜ, y = r sin Œ∏ sin œÜ, and z = r cos Œ∏.

90
LECTURE 3. PRINCIPLES OF QUANTUM MECH
You could also try working out a much more elaborate
example involving two directions, ÀÜn and ÀÜm. In this setup, A
not only ends up in an arbitrary direction; it also starts out
in a (diÔ¨Äerent) arbitrary direction.
Exercise 3.5: Suppose that a spin is prepared so that œÉm =
+1. The apparatus is then rotated to the ÀÜn direction and œÉn
is measured. What is the probability that the result is +1?
Note that œÉm = œÉ ¬∑ ÀÜm, using the same convention we used
for œÉn.
The answer is the square of the cosine of half the angle be-
tween ÀÜm and ÀÜn. Can you show it?
3.8
The Spin-Polarization
Principle
There is an important theorem that you can try to prove. I
will call it
The Spin-Polarization Principle: Any state of a
single spin is an eigenvector of some component of the
spin.
In other words, given any state
|A‚ü©= Œ±u|u‚ü©+ Œ±d|d‚ü©,
there exists some direction ÀÜn, such that
‚ÉóœÉ ¬∑ ‚Éón |A‚ü©= |A‚ü©.

3.8. THE SPIN-POLARIZATION PRINCIPLE
91
This means that for any spin state, there is some orientation
of the apparatus A such that A will register +1 when it
acts. In physics language, we say that the states of a spin
are characterized by a polarization vector, and along that
polarization vector the component of spin is predictably +1,
assuming of course that you know the state-vector.
An interesting consequence of this theorem is that there
is no state for which the expectation values of all three com-
ponents of spin are zero. There is a quantitative way to ex-
press this. Consider the expectation value of the spin along
the direction ÀÜn. Since |A‚ü©is an eigenvector of ‚ÉóœÉ ¬∑ ‚Éón (with
eigenvalue +1), it follows that the expectation value can be
expressed as
‚ü®‚ÉóœÉ ¬∑ ‚Éón‚ü©= 1.
On the other hand, the expectation value of the perpendicu-
lar components of œÉ are zero in the state |A‚ü©. It follows that
the squares of the expectation values of all three components
of œÉ sum to 1. Moreover, this is true for any state:
‚ü®œÉx‚ü©2 + ‚ü®œÉy‚ü©2 + ‚ü®œÉz‚ü©2 = 1.
(3.27)
Remember this fact. We will come back to it in Lecture 6.


Lecture 4
Time and Change
There is a massive, quiet, intimidating man sitting alone at
the end of the bar. His T-shirt says ‚Äú‚àí1.‚Äù
Art: Who is that ‚ÄúMinus One‚Äù guy over in the corner? The
bouncer?
Lenny: He‚Äôs way more than a bouncer. He‚Äôs
THE LAW.
Without him, this whole place would fall apart.
4.1
A Classical Reminder
In Volume I, it took a little more than a page to explain
what a state is in classical mechanics. The quantum version
has taken three lectures, three mathematical interludes, and
according to my rough count, about 17,000 words to get to
the same place.
But I think the worst is over.
We now
know what a state is. However, just as in classical physics,
93

94
LECTURE 4. TIME AND CHANGE
knowing the states of a system is only half the story. The
other half involves a rule about how states change with time.
That‚Äôs our next job.
Let me just give you a quick reminder about the na-
ture of change in classical physics. In classical physics, the
space of states is a mathematical set. The logic is Boolean,
and the evolution of states over time is deterministic and re-
versible. In the simplest examples we considered, the state-
space consisted of a few points: Heads and Tails for a coin,
{1, 2, 3, 4, 5, 6} for a die. The states were pictured as a set
of points on the page, and the time evolution was just a
rule telling you where to go next. A law of motion consisted
of a graph with arrows connecting the states.
The main
rule‚Äîdeterminism‚Äîwas that wherever you are in the state-
space, the next state is completely speciÔ¨Åed by the law of
motion. But there was also another rule called reversibility.
Reversibility is the requirement that a properly formulated
law must also tell you where you were last. A good law cor-
responds to a graph with exactly one arrow in and one arrow
out at each state.
There is another way to describe these requirements. I
called it the minus Ô¨Årst law, because it underlies everything
else. It says that information is never lost. If two identical
isolated systems start out in diÔ¨Äerent states, they stay in
diÔ¨Äerent states. Moreover, in the past they were also in dif-
ferent states. On the other hand, if two identical systems are
in the same state at some point in time, then their histories
and their future evolutions must also be identical. Distinc-
tions are conserved. The quantum version of the minus Ô¨Årst
law has a name‚Äîunitarity.

4.2. UNITARITY
95
4.2
Unitarity
Let us consider a closed system that at time t is in the quan-
tum state |Œ®‚ü©. (The use of the Greek letter Œ® [psi] for quan-
tum states is traditional when considering the evolution of
systems.) To indicate that the state was |Œ®‚ü©at the speciÔ¨Åc
time t, let‚Äôs complicate the notation a bit and call the state
|Œ®(t)‚ü©. Of course, this notation suggests a bit more than just
‚Äúthe state was |Œ®‚ü©at time t.‚Äù It also suggests that the state
may be diÔ¨Äerent at diÔ¨Äerent times. Thus, we think of |Œ®(t)‚ü©
as representing the entire history of the system.
The basic dynamical assumption of quantum mechanics
is that if you know the state at one time, then the quantum
equations of motion tell you what it will be later. Without
loss of generality, we can take the initial time to be zero
and the later time to be t. The state at time t is given by
some operation that we call U(t), acting on the state at time
zero. Without further specifying the properties of U(t), this
tells us very little except that |Œ®(t)‚ü©is determined by |Œ®(0)‚ü©.
Let‚Äôs express this relation with the equation
|Œ®(t)‚ü©= U(t)|Œ®(0)‚ü©.
(4.1)
The operation U is called the time-development operator for
the system.

96
LECTURE 4. TIME AND CHANGE
4.3
Determinism in Quantum
Mechanics
At this point, we need to draw some careful distinctions.
We are setting up U(t) in such a way that the state-vector
will evolve in a deterministic manner. Yes, you heard me
correctly‚Äîthe time evolution of the state-vector is deter-
ministic. This is nice because it provides us with something
we can try to predict. But how does that square with the
statistical character of our measurement results?
As we‚Äôve seen, knowing the quantum state does not mean
that you can predict the result of an experiment with cer-
tainty. For example, knowing that the state of a spin is |r‚ü©
may tell you the outcome of a measurement of œÉx but tells
you nothing about a measurement of œÉz or œÉy. For this rea-
son, Eq. 4.1 is not the same as classical determinism. Clas-
sical determinism allows us to predict the results of experi-
ments. The quantum evolution of states allows us to com-
pute the probabilities of the outcomes of later experiments.
This is one of the core diÔ¨Äerences between classical and
quantum mechanics. It goes back to the relationship between
states and measurements we mentioned at the very beginning
of this book. In classical mechanics, there‚Äôs no real diÔ¨Äerence
between states and measurements. In quantum mechanics,
the diÔ¨Äerence is profound.

4.4. A CLOSER LOOK AT U(T)
97
4.4
A Closer Look at U(t)
Conventional quantum mechanics places a couple of require-
ments on U(t). First, it requires U(t) to be a linear operator.
That is not very surprising. The relationships between states
in quantum mechanics are always linear. It goes along with
the idea that the state-space is a vector space. But linearity
is not the only thing that quantum mechanics requires of
U(t). It also requires the quantum analog of the minus Ô¨Årst
law: the conservation of distinctions.
Recall from the last lecture that two states are distin-
guishable if they are orthogonal.
Being orthogonal, two
diÔ¨Äerent basis vectors represent two distinguishable states.
Suppose that |Œ®(0)‚ü©and |Œ¶(0)‚ü©are two distinguishable states;
in other words, there is a precise experiment that can tell
them apart, and therefore they must be orthogonal:
‚ü®Œ®(0)|Œ¶(0)‚ü©= 0.
The conservation of distinctions implies that they will con-
tinue to be orthogonal for all time. We can express this as
‚ü®Œ®(t)|Œ¶(t)‚ü©= 0
(4.2)
for all values of t. This principle has consequences for the
time-development operator U(t). To see what they are, let‚Äôs
Ô¨Çip the ket-vector Eq. 4.1 to its bra-vector counterpart:
‚ü®Œ®(t)| = ‚ü®Œ®(0)|U‚Ä†(t).
(4.3)

98
LECTURE 4. TIME AND CHANGE
Notice the dagger that indicates Hermitian conjugation. Now,
let‚Äôs plug Eqs. 4.1 and 4.3 into Eq. 4.2:
‚ü®Œ®(0)|U‚Ä†(t)U(t)|Œ¶(0)‚ü©= 0.
(4.4)
To examine the consequences of this equation, consider an
orthonormal basis of vectors |i‚ü©. Any basis will do.
The
orthonormality is expressed in equation form as
‚ü®i|j‚ü©= Œ¥ij,
where Œ¥ij is the usual Kronecker symbol.
Next, let‚Äôs take |Œ¶(0)‚ü©and |Œ®(0)‚ü©to be members of this
orthonormal basis. Substituting into Eq. 4.4 gives
‚ü®i|U‚Ä†(t)U(t)|j‚ü©= 0
(i Ã∏= j)
whenever i and j are not the same. On the other hand, if i
and j are the same, then so are the output vectors U(t)|i‚ü©
and U(t)|j‚ü©. In that case, the inner product between them
should be 1. Therefore, the general relation takes the form
‚ü®i|U‚Ä†(t)U(t)|j‚ü©= Œ¥ij.
In other words, the operator U‚Ä†(t)U(t) behaves like the unit
operator I when it acts between any members of a basis
set. From here it is an easy matter to prove that U‚Ä†(t)U(t)
acts like the unit operator I when it acts on any state. An
operator U that satisÔ¨Åes

4.5. THE HAMILTONIAN
99
U‚Ä†U = I
is called unitary. In physics lingo, time evolution is unitary.
Unitary operators play an enormous role in quantum
mechanics, representing all sorts of transformations on the
state-space. Time evolution is just one example. Thus, we
conclude this section with a Ô¨Åfth principle of quantum me-
chanics:
‚Ä¢ Principle 5: The evolution of state-vectors with time
is unitary.
Exercise 4.1: Prove that if U is unitary, and if |A‚ü©and |B‚ü©
are any two state-vectors, then the inner product of U|A‚ü©
and U|B‚ü©is the same as the inner product of |A‚ü©and |B‚ü©.
One could call this the conservation of overlaps. It expresses
the fact that the logical relation between states is preserved
with time.
4.5
The Hamiltonian
In the study of classical mechanics, we became familiar with
the idea of an incremental change in time. Quantum mechan-
ics is no diÔ¨Äerent in this respect: we may build up Ô¨Ånite time
intervals by combining many inÔ¨Ånitesimal intervals. Doing
so will lead to a diÔ¨Äerential equation for the evolution of
the state-vector.
To that end, we replace the time inter-
val t with an inÔ¨Ånitesimal time interval œµ and consider the
time-evolution operator for this small interval.

100
LECTURE 4. TIME AND CHANGE
There are two principles that go into the study of incre-
mental changes. The Ô¨Årst principle is unitarity:
U‚Ä†(œµ)U(œµ) = I .
(4.5)
The second principle is continuity. This means that the
state-vector changes smoothly. To make this precise, Ô¨Årst
consider the case in which œµ is zero. It should be obvious that
in this case the time-evolution operator is merely the unit
operator I. Continuity means that when œµ is very small, U(œµ)
is close to the unit operator, diÔ¨Äering from it by something
of order œµ. Thus, we write
U(œµ) = I ‚àíiœµH.
(4.6)
You may wonder why I put a minus sign and an i in front
of H. These factors are completely arbitrary at this stage.
In other words, they are a convention that has no content.
I used them with an eye toward the future, when we will
recognize H as something familiar from classical physics.
We will also need an expression for U‚Ä†. Remembering
that Hermitian conjugation requires the complex conjuga-
tion of coeÔ¨Écients, we Ô¨Ånd that
U‚Ä†(œµ) = I + iœµH‚Ä†.
(4.7)
Now we plug Eqs. 4.6 and 4.7 into the unitarity condition of
Eq. 4.5:
(I + iœµH‚Ä†)(I ‚àíiœµH) = I .

4.5. THE HAMILTONIAN
101
Expanding to Ô¨Årst order in œµ, we Ô¨Ånd
H‚Ä† ‚àíH = 0
or, in a format that is more illuminating,
H‚Ä† = H.
(4.8)
This last equation expresses the unitarity condition. But it
also says that H is a Hermitian operator. This has great sig-
niÔ¨Åcance. We can now say that H is an observable, and has
a complete set of orthonormal eigenvectors and eigenvalues.
As we proceed, H will become a very familiar object, namely
the quantum Hamiltonian. Its eigenvalues are the values that
would result from measuring the energy of a quantum sys-
tem. Exactly why we identify H with the classical concept of
a Hamiltonian, and its eigenvalues with energy, will become
clear shortly.
Let‚Äôs return now to Eq. 4.1 and specialize it to the in-
Ô¨Ånitesimal case t = œµ. Using Eq. 4.6, we Ô¨Ånd
|Œ®(œµ)‚ü©= |Œ®(0)‚ü©‚àíiœµH|Œ®(0)‚ü©.
This is just the kind of equation that we can easily turn into
a diÔ¨Äerential equation. First, we transpose the Ô¨Årst term on
the right side over to the left side, and then divide by œµ:
|Œ®(œµ)‚ü©‚àí|Œ®(0)‚ü©
œµ
= ‚àíiH|Œ®(0)‚ü©.

102
LECTURE 4. TIME AND CHANGE
If you remember your calculus (see Volume I for a quick
review), you‚Äôll recognize that the left-hand side of this equa-
tion looks exactly like the deÔ¨Ånition of a derivative. If we
take the limit as œµ ‚Üí0, it becomes the time derivative of the
state-vector:
‚àÇ|Œ®‚ü©
‚àÇt
= ‚àíiH|Œ®‚ü©.
(4.9)
We originally set things up so that the time variable was
zero, but there was nothing special about t = 0. Had we
chosen another time and done the same thing, we would
have gotten exactly the same result, namely, Eq. 4.9. This
equation tells us how the state-vector changes: if we know
the state-vector at one instant, the equation tells us what it
will be at the next. Eq. 4.9 is important enough to have a
name. It is called the generalized Schr¬®odinger equation, or
more commonly, the time-dependent Schr¬®odinger equation.
If we know the Hamiltonian, it tells us how the state of an
undisturbed system evolves with time. Art likes to call this
state-vector Schr¬®odinger‚Äôs Ket. He even wanted to render
the Greek symbol with little whiskers,1 but I had to draw
the line somewhere.
4.6
What Ever Happened to ¬Øh?
I‚Äôm sure you have all heard of Planck‚Äôs constant. Planck him-
self called it h and gave it a value of about 6.6√ó10‚àí34 kg m2 /s.
1OK, not really.

4.6. WHAT EVER HAPPENED TO ¬ØH?
103
Later generations redeÔ¨Åned it, dividing by a factor of 2œÄ and
calling the result ¬Øh:
¬Øh = h
2œÄ = 1.054571726 ¬∑ ¬∑ ¬∑ √ó 10‚àí34 kg m2/s.
Why divide by 2œÄ? Because it saves us from having to write
2œÄ in lots of other places. Considering the importance of
Planck‚Äôs constant in quantum mechanics, it seems a little
odd that it hasn‚Äôt come up yet. We‚Äôre going to correct that
now.
In quantum mechanics, as in classical physics, the Hamil-
tonian is the mathematical object that represents the energy
of a system. This raises a question that, if you are very alert,
may have been a source of confusion. Take a good look at
Eq. 4.9. It doesn‚Äôt make dimensional sense. If you ignore
|Œ®‚ü©on both sides of the equation, the units on the left side
are inverse time. If the quantum Hamiltonian is really to be
identiÔ¨Åed with energy, then the units on the right side are
energy. Energy is measured in units of joules, or kg ¬∑ m2/s2.
Evidently, I‚Äôve been cheating a little bit.
The resolution
of this dilemma involves ¬Øh, a universal constant of nature,
which happens to have units of kg ¬∑ m2/s. A constant with
these units is exactly what we need to make Eq. 4.9 consis-
tent. Let‚Äôs rewrite it with Planck‚Äôs constant inserted in a
way that makes it dimensionally consistent:
¬Øh‚àÇ|Œ®‚ü©
‚àÇt
= ‚àíiH|Œ®‚ü©.
(4.10)
Why is it that ¬Øh is such a ridiculously small number? The
answer has much more to do with biology than with physics.

104
LECTURE 4. TIME AND CHANGE
The real question is not why ¬Øh is so small; it‚Äôs why you are
so big.
The units that we use reÔ¨Çect our own size.
The
origin of the meter seems to be that it was used to measure
rope or cloth: it‚Äôs about the distance from a person‚Äôs nose
to his or her outstretched Ô¨Ångers. A second is about as long
as a heartbeat. And a kilogram is a nice weight to carry
around.
We use these units because they are convenient,
but fundamental physics doesn‚Äôt care that much about us.
The size of an atom is about 10‚àí10 meters. Why so small?
That‚Äôs the wrong question. The right one is: Why are there
so many atoms in an arm? The reason is simply that to make
a functioning, intelligent, unit-using creature, you need to
put together a lot of atoms. Similarly, the kilogram is many
times larger than an atomic mass because people don‚Äôt carry
around single atoms; they get lost too easily. The same goes
for time, and our long, plodding second.
In the end, the
reason that Planck‚Äôs constant is so small is that we are so
big and heavy and slow.
Physicists who are interested in the microscopic world are
likely to use units that are more tailored to the phenomena
that they study. If we used atomic length scales, time scales,
and mass scales, then Planck‚Äôs constant would not be such an
unwieldy number; it would be much closer to 1. In fact, units
for which Planck‚Äôs constant equals 1 are a natural choice
for quantum mechanics, and it‚Äôs a common practice to use
them. However, in this book, we will usually retain ¬Øh in our
equations.

4.7. EXPECTATION VALUES
105
4.7
Expectation Values
Let‚Äôs take a short break to discuss an important aspect of
statistics, namely the idea of an average value or mean value.
We mentioned this idea brieÔ¨Çy in the previous lecture, but
now it‚Äôs time to take a closer look.
In quantum mechanics, average values are called expecta-
tion values. (In some ways, this is a poor choice of words; I‚Äôll
tell you why later.) Suppose we have a probability function
for the outcome of an experiment that measures an observ-
able L. The outcome must be one of L‚Äôs eigenvalues, Œªi, and
the probability function is P(Œªi). In statistics, that average
(or mean) value is denoted by a bar over the quantity being
measured. The average of the observable L would be ¬ØL. In
quantum mechanics, the standard notation is diÔ¨Äerent, hav-
ing grown out of Paul Dirac‚Äôs clever bra-ket notation. We
represent the average value of L with the notation ‚ü®L‚ü©. We‚Äôll
soon see why the bra-ket notation is so natural, but Ô¨Årst let‚Äôs
discuss the meaning of the term average.
From a mathematical point of view, an average is deÔ¨Åned
by the equation
‚ü®L‚ü©=

i
ŒªiP(Œªi).
(4.11)
In other words, it is a weighted sum, weighted with the prob-
ability function P.
Alternatively, the average can be deÔ¨Åned in an experi-
mental way. Suppose a very large number of identical exper-
iments is made, and the outcomes are recorded. Let‚Äôs deÔ¨Åne

106
LECTURE 4. TIME AND CHANGE
the probability function in a direct observational manner.
We identify P(Œªi) as the fraction of observations whose re-
sult was Œªi. The deÔ¨Ånition 4.11 is then identiÔ¨Åed with the
experimental average of the observations. The basic hypoth-
esis of any statistical theory is that if the number of trials
is large enough, the mathematical and experimental notions
of probability and average will agree. We will not question
this hypothesis.
I‚Äôll now prove an elegant little theorem that explains the
bra-ket notation for averages. Suppose that the normalized
state of a quantum system is |A‚ü©. Expand |A‚ü©in the or-
thonormal basis of eigenvectors of L:
|A‚ü©=

i
Œ±i|Œªi‚ü©.
(4.12)
Just for fun, with no particular agenda in mind, let‚Äôs com-
pute the quantity ‚ü®A|L|A‚ü©. The meaning of this should be
clear: First act on |A‚ü©with the linear operator L.2 Then,
take the inner product of the result with the bra ‚ü®A|. Let‚Äôs
do the Ô¨Årst step by letting L operate on both sides of Eq.
4.12:
L|A‚ü©=

i
Œ±iL|Œªi‚ü©.
Remember that the vectors |Œªi‚ü©are eigenvectors of L. Using
the fact that L|Œªi‚ü©= Œªi|Œªi‚ü©, we can write
2We would get the same result if we had let L act on ‚ü®A| Ô¨Årst.

4.7. EXPECTATION VALUES
107
L|A‚ü©=

i
Œ±iŒªi|Œªi‚ü©.
The last step is to take the inner product with ‚ü®A|. We do
that by expanding the bra ‚ü®A| in eigenvectors on the right-
hand side, and then using the orthonormality of the eigen-
vectors. The result is
‚ü®A|L|A‚ü©=

i
(Œ±‚àó
i Œ±i)Œªi.
(4.13)
Using the probability principle (Principle 4) to identify (Œ±‚àó
i Œ±i)
with the probability P(Œªi), we immediately see that the ex-
pression on the right side of Eq. 4.13 is the same as the
expression on the right side of Eq. 4.11. That is to say,
‚ü®L‚ü©= ‚ü®A|L|A‚ü©.
(4.14)
Thus, we have a quick rule to compute averages. Just sand-
wich the observable between the bra and ket representations
of the state-vector.
In the previous lecture (Section 3.5), we promised to ex-
plain how the action of a Hermitian operator on a state-
vector is related to the results of physical measurements.
Armed with our knowledge of expectation values, we can now
keep that promise. If we look back at Eq. 3.21, we see an
example of an operator, œÉz, acting on state-vector |r‚ü©to pro-
duce a new state-vector. We can view this equation as half
of the calculation for the expectation value of the measure-
ment œÉz‚Äîthe right-hand part of the sandwich, if you will.

108
LECTURE 4. TIME AND CHANGE
The rest of that calculation involves taking the inner prod-
uct of this state-vector with the dual vector ‚ü®r|. So when œÉz
acts on |r‚ü©in Eq. 3.21, it produces a state-vector from which
we can calculate the probabilities of each œÉz measurement
outcome.
4.8
Ignoring the Phase-Factor
In previous lectures, we said that we can ignore the overall
phase-factor of a state-vector, and promised to explain why
in a later section. Having worked out the rule for averages,
we‚Äôll take a short detour to keep that promise.
What does it mean to ‚Äúignore the overall phase-factor‚Äù?
It means we can multiply any state-vector by a constant
factor eiŒ∏, where Œ∏ is a real number, without changing the
state-vector‚Äôs physical meaning. To see this, let‚Äôs multiply
Eq. 4.12 by eiŒ∏ and call the result |B‚ü©:
|B‚ü©= eiŒ∏|A‚ü©= eiŒ∏ 
j
Œ±j|Œªj‚ü©.
(4.15)
Note that we changed the index in the summation from i to
j to avoid confusion. It‚Äôs easy to see that |B‚ü©has the same
magnitude as |A‚ü©, because eiŒ∏ has a magnitude of one:
‚ü®B|B‚ü©= ‚ü®Ae‚àíiŒ∏|eiŒ∏A‚ü©= ‚ü®A|A‚ü©.
The same pattern of cancellation preserves other quantities
as well. For example, |A‚ü©‚Äôs probability amplitudes Œ±j be-
come eiŒ∏Œ±j for |B‚ü©, so the probability amplitudes are diÔ¨Äer-
ent. However, it‚Äôs the actual probability, not the amplitude,

4.9. CONNECTIONS TO CLASSICAL MECHANICS 109
that has physical meaning. If a system is in state |B‚ü©, and
we perform a measurement, the result will be the eigenvalue
of |Œªj‚ü©with probability
Œ±‚àó
je‚àíiŒ∏eiŒ∏Œ±j = Œ±‚àó
jŒ±j,
which is the same result we would get for state |A‚ü©. Finally,
let‚Äôs use the same trick for the expectation value of a Her-
mitian operator L. Applying Eq. 4.14 to state |B‚ü©, we can
write
‚ü®L‚ü©= ‚ü®B|L|B‚ü©.
Using Eq. 4.15 for |B‚ü©, we get
‚ü®L‚ü©= ‚ü®Ae‚àíiŒ∏|L|eiŒ∏A‚ü©
or
‚ü®L‚ü©= ‚ü®A|L|A‚ü©.
In other words, L has the same expectation value in state
|B‚ü©as it does in state |A‚ü©. Promise kept.
4.9
Connections to Classical
Mechanics
The average, or expectation value, of an observable is the
closest thing in quantum mechanics to a classical value. If

110
LECTURE 4. TIME AND CHANGE
the probability distribution for an observable is a nice bell-
shaped curve, and not too broad, then the expectation value
really is the value that you expect to measure. If a system is
so big and heavy that quantum mechanics is not too impor-
tant, then the expectation value of an observable behaves
almost exactly according to classical equations of motion.
For this reason, it is interesting and important to Ô¨Ånd out
how expectation values change with time.
First of all, why do they change with time? They change
with time because the state of the system changes with time.
Suppose the state at time t is represented by ket |Œ®(t)‚ü©and
bra ‚ü®Œ®(t)|. The expectation value of the observable L at time
t is
‚ü®Œ®(t)|L|Œ®(t)‚ü©.
Let‚Äôs see how this changes by diÔ¨Äerentiating it with respect to
t and using the Schr¬®odinger equation for the time derivatives
of |Œ®(t)‚ü©and ‚ü®Œ®(t)|. Using the product rule for derivatives,
we Ô¨Ånd that
d
dt‚ü®Œ®(t)|L|Œ®(t)‚ü©= ‚ü®ÀôŒ®(t)|L|Œ®(t)‚ü©+ ‚ü®Œ®(t)|L| ÀôŒ®(t)‚ü©,
where, as usual, the dot means time derivative.
L itself
has no explicit time dependency, so it just comes along for
the ride.
Now, plugging in the bra and ket versions of
Schr¬®odinger‚Äôs equation (Eq. 4.10), we get
d
dt‚ü®Œ®(t)|L|Œ®(t)‚ü©= i
¬Øh ‚ü®Œ®(t)|HL|Œ®(t)‚ü©‚àíi
¬Øh ‚ü®Œ®(t)|LH|Œ®(t)‚ü©

4.9. CONNECTIONS TO CLASSICAL MECHANICS 111
or, more concisely,
d
dt‚ü®Œ®(t)|L|Œ®(t)‚ü©= i
¬Øh ‚ü®Œ®(t)| [HL ‚àíLH] |Œ®(t)‚ü©.
(4.16)
If you are used to ordinary algebra, Eq. 4.16 has a strange
appearance. The right-hand side contains the combination
HL‚àíLH, a combination that would ordinarily be zero. But
linear operators are not ordinary numbers: when they are
multiplied (or applied sequentially), the order counts.
In
general, when H acts on L|Œ®‚ü©, the result is not the same
as when L acts on H|Œ®‚ü©. In other words, except for spe-
cial cases, HL Ã∏= LH. Given two operators or matrices, the
combination
LM ‚àíML
is called the commutator of L with M, and it is denoted by
a special symbol:
LM ‚àíML = [L, M].
It‚Äôs worth noticing that [L, M] = ‚àí[M, L] for any pair of
operators. Armed with the notation for commutators, we
can now write Eq. 4.16 in a simple form:
d
dt‚ü®L‚ü©= i
¬Øh ‚ü®[H, L]‚ü©
(4.17)
or, equivalently,
d
dt‚ü®L‚ü©= ‚àíi
¬Øh ‚ü®[L, H]‚ü©.
(4.18)

112
LECTURE 4. TIME AND CHANGE
This is a very interesting and important equation. It relates
the time derivative of the expectation value of an observable
L to the expectation value of another observable, namely
‚àíi
¬Øh[L, H].
Exercise 4.2: Prove that if M and L are both Hermitian,
i[M, L] is also Hermitian. Note that the i is important. The
commutator is, by itself, not Hermitian.
If we assume that the probabilities are nice, narrow, bell-
shaped curves, then Eq. 4.18 tells us how the peaks of the
curves move with time. Equations like this are the closest
thing in quantum mechanics to the equations of classical
physics. Sometimes we even omit the angle brackets in such
equations and write them in a shorthand form:
dL
dt = ‚àíi
¬Øh[L, H].
(4.19)
But keep in mind that a quantum equation of this type
should be in the middle of a sandwich, with a bra ‚ü®Œ®| on
one side, and a ket |Œ®‚ü©on the other. Alternatively, we can
think of it as an equation that tells us how the centers of
probability distributions move around.
Does Eq. 4.19 have a familiar look to it? If not, go back
to Lectures 9 and 10 in Volume I, where we learned about
the Poisson bracket formulation of classical mechanics. On

4.9. CONNECTIONS TO CLASSICAL MECHANICS 113
page 172, the following equation can be found:3
ÀôF = {F, H}.
(4.20)
In this equation, {F, H} is not a commutator; it is a Pois-
son bracket. But still, Eq. 4.20 is suspiciously similar to Eq.
4.19. In fact, there is a close parallel between commutators
and Poisson brackets, and their algebraic properties are quite
similar. For example, if F and G represent operators, both
commutators and Poisson brackets change their sign when
F and G are interchanged. Dirac discovered this, and re-
alized that it represents an important structural connection
between the mathematics of classical mechanics and that of
quantum mechanics. The formal identiÔ¨Åcation between com-
mutators and Poisson brackets is
[F, G]
‚áê‚áí
i¬Øh{F, G}.
(4.21)
To facilitate comparison with Eq. 4.19, we can substitute the
symbols L and H that we‚Äôve been using in this section.
[L, H]
‚áê‚áí
i¬Øh{L, H}.
(4.22)
Let‚Äôs try and make this identiÔ¨Åcation as clear as possible. If
we start with Eq. 4.19,
dL
dt = ‚àíi
¬Øh[L, H],
3Volume I, Lecture 9, Eq. 10. Another one of those elegant French
inventions.

114
LECTURE 4. TIME AND CHANGE
and then use the identiÔ¨Åcation of Eq. 4.22 to write the clas-
sical analog, the result is
dL
dt = ‚àíi
¬Øh(i¬Øh{L, H})
or
dL
dt = {L, H},
which matches the pattern of Eq. 4.20 exactly.
Exercise 4.3: Go back to the deÔ¨Ånition of Poisson brackets
in Volume I and check that the identiÔ¨Åcation in Eq. 4.21 is
dimensionally consistent. Show that without the factor ¬Øh, it
would not be.
Equation 4.21 solves a riddle. In classical physics, there
is no diÔ¨Äerence between FG and GF. In other words: clas-
sically, commutators between ordinary observables are zero.
From Eq. 4.21, we see that commutators in quantum me-
chanics are not zero, but that they are very small. The clas-
sical limit (the limit at which classical mechanics is accurate)
is also the limit at which ¬Øh is negligibly small. Therefore, it is
also the limit at which commutators are very small in human
units.
4.10
Conservation of Energy
How can we tell whether something is conserved in quan-
tum mechanics? What do we even mean by saying that an
observable‚Äîcall it Q‚Äîis conserved? At the very minimum,

4.10. CONSERVATION OF ENERGY
115
we mean that its expectation value ‚ü®Q‚ü©does not change with
time (unless of course the system is disturbed).
An even
stronger condition is that ‚ü®Q2‚ü©(or the expectation value of
any power of Q) does not change with time.
Looking at Eq. 4.19, we can see that the condition for
‚ü®Q‚ü©not to change is
[Q, H] = 0.
In other words, if a quantity commutes with the Hamilto-
nian, its expectation value is conserved. We can make this
statement stronger. Using the properties of commutators,
it‚Äôs easy to see that if [H, Q] = 0, then [Q2, H] = 0, or
even more generally, [Qn, H] = 0, for any n. It turns out
that we can make a stronger claim: if Q commutes with the
Hamiltonian, the expectation values of all functions of Q
are conserved. That‚Äôs what conservation means in quantum
mechanics.
The most obvious conserved quantity is the Hamiltonian
itself. Since any operator commutes with itself, one can write
[H, H] = 0,
which is exactly the condition that H is conserved. As in
classical mechanics, the Hamiltonian is another word for the
energy of a system‚Äîit‚Äôs a deÔ¨Ånition of energy. We see that
under very general conditions, energy is conserved in quan-
tum mechanics.

116
LECTURE 4. TIME AND CHANGE
4.11
Spin in a Magnetic Field
Let‚Äôs try out the Hamiltonian equations of motion for a sin-
gle spin. We will Ô¨Årst need to specify a Hamiltonian. Where
do we get it from? In general, the answer is the same as in
classical physics: derive it from experiment, borrow it from
some theory that we like, or just pick one and see what it
does. But in the case of a single spin, we don‚Äôt have many
options. Let‚Äôs start with the unit operator I. Since I com-
mutes with all operators, if it were the Hamiltonian, nothing
would change with time. Remember, the time-dependence of
an observable is given by the commutator of the observable
with the Hamiltonian.
The only other choice is a sum of the spin components.
In fact, that‚Äôs exactly what we would get from experimen-
tal observation of a real spin‚Äîsay an electron‚Äôs spin‚Äîin a
magnetic Ô¨Åeld. A magnetic Ô¨Åeld ‚ÉóB is a 3-vector‚Äîordinary
vector in space‚Äîand is speciÔ¨Åed by three Cartesian compo-
nents, Bx, By, and Bz. When a classical spin (a charged
rotor) is put into a magnetic Ô¨Åeld, it has an energy that de-
pends on its orientation. The energy is proportional to the
dot product of the spin and the magnetic Ô¨Åeld. The quantum
version of this is
H ‚àº‚ÉóœÉ ¬∑ ‚ÉóB = œÉxBx + œÉyBy + œÉzBz,
where the symbol ‚àºmeans ‚Äúproportional to.‚Äù Remember
that œÉx, œÉy, and œÉz represent the components of the spin
operator in the above quantum version.

4.11. SPIN IN A MAGNETIC FIELD
117
Let‚Äôs take a simple example in which the magnetic Ô¨Åeld
lies along the z axis. In that case, the Hamiltonian is propor-
tional to œÉz. For convenience, we‚Äôll absorb all the numerical
constants, including the magnitude of the Ô¨Åeld (but not ¬Øh),
into a single constant œâ and write
H = ¬Øhœâ
2 œÉz.
(4.23)
The reason for the 2 in the denominator will become clear
soon.
Our goal is to Ô¨Ånd out how the expectation value of the
spin varies with time‚Äîin other words, to determine ‚ü®œÉx(t)‚ü©,
‚ü®œÉy(t)‚ü©, and ‚ü®œÉz(t)‚ü©. To do this, we just go back to Eq. 4.19,
and plug in these components of L. We get
Àô
‚ü®œÉx‚ü©
=
‚àíi
¬Øh‚ü®[œÉx, H]‚ü©
Àô
‚ü®œÉy‚ü©
=
‚àíi
¬Øh‚ü®[œÉy, H]‚ü©
Àô
‚ü®œÉz‚ü©
=
‚àíi
¬Øh‚ü®[œÉz, H]‚ü©.
(4.24)
Plugging in H = ¬Øhœâ
2 œÉz from Eq. 4.23, we get
Àô
‚ü®œÉx‚ü©
=
‚àíiœâ
2 ‚ü®[œÉx, œÉz]‚ü©
Àô
‚ü®œÉy‚ü©
=
‚àíiœâ
2 ‚ü®[œÉy, œÉz]‚ü©

118
LECTURE 4. TIME AND CHANGE
Àô
‚ü®œÉz‚ü©
=
‚àíiœâ
2 ‚ü®[œÉz, œÉz]‚ü©.
(4.25)
The things we are computing on the left side of the equa-
tions are supposed to be real quantities. The factor i in these
equations seems like trouble. Fortunately, the commutation
relations between œÉx, œÉy, and œÉz will save the day. By plug-
ging in the Pauli matrices from Eq. 3.20, it‚Äôs easy to verify
that
[œÉx, œÉy]
=
2iœÉz
[œÉy, œÉz]
=
2iœÉx
[œÉz, œÉx]
=
2iœÉy.
(4.26)
Each of these equations also has an i, which will cancel the i
in Eqs. 4.25. Notice that the factors of 2 also cancel, resulting
in some very simple equations:
Àô
‚ü®œÉx‚ü©
=
‚àíœâ‚ü®œÉy‚ü©
Àô
‚ü®œÉy‚ü©
=
œâ‚ü®œÉx‚ü©
Àô
‚ü®œÉz‚ü©
=
0.
(4.27)
Does this look familiar? If not, go back to Volume I, Lecture
10. There, we studied the classical rotor in a magnetic Ô¨Åeld.

4.12. SOLVING THE SCHR ¬®ODINGER EQUATION
119
The equations were exactly the same, except that instead of
expectation values, we were studying the actual motion of a
deterministic system. Both there and here, the solution is
that the 3-vector-operator ‚ÉóœÉ (or the 3-vector ‚ÉóL in Volume I)
precesses like a gyroscope around the direction of the mag-
netic Ô¨Åeld. The precession is uniform, with angular velocity
œâ.
This similarity to classical mechanics is very pleasing, but
it‚Äôs important to take note of the diÔ¨Äerence. Exactly what
is precessing? In classical mechanics, it‚Äôs just the x and y
components of angular momentum. In quantum mechanics,
it‚Äôs an expectation value. The expectation value for a œÉz
measurement does not change with time, but the other two
expectation values do. Regardless, the result of each individ-
ual measurement of each spin component is still either +1 or
‚àí1.
Exercise 4.4:
Verify the commutation relations of Eqs.
4.26.
4.12
Solving the Schr¬®odinger
Equation
The iconic Schr¬®odinger equation that appears on T-shirts
has this form:
i¬Øh‚àÇŒ®(x)
‚àÇt
= ‚àí¬Øh2
2m
‚àÇ2Œ®(x)
‚àÇx2
+ U(x)Œ®(x).
At this point, let‚Äôs not worry about the meaning of the sym-

120
LECTURE 4. TIME AND CHANGE
bols except to note that it is an equation that tells you how
something changes with time. (The ‚Äúsomething‚Äù is a repre-
sentation of the state-vector of a particle.)
The iconic Schr¬®odinger equation is a special case of a
more general equation that we‚Äôve already met in Eq. 4.9. It is
part deÔ¨Ånition and part principle of quantum mechanics. As
a principle, it says that the state-vector changes continuously
with time, in a unitary way. As a deÔ¨Ånition, it deÔ¨Ånes the
Hamiltonian, and therefore the observable called energy. Eq.
4.10,
¬Øh‚àÇ|Œ®‚ü©
‚àÇt
= ‚àíiH|Œ®‚ü©,
is sometimes called the time-dependent Schr¬®odinger equa-
tion. Because the Hamiltonian operator H represents en-
ergy, the observable values of energy are just the eigenvalues
of H. Let‚Äôs call these eigenvalues Ej and the corresponding
eigenvectors |Ej‚ü©. By deÔ¨Ånition, the relation between H, Ej,
and |Ej‚ü©is the eigenvalue equation
H|Ej‚ü©= Ej|Ej‚ü©.
(4.28)
This is the time-independent Schr¬®odinger equation, and it‚Äôs
used in two diÔ¨Äerent ways.
If we work in a particular matrix basis, then the equation
determines the eigenvectors of H. One puts in a particular
value of the energy Ej and looks for the ket-vector |Ej‚ü©that
solves the equation.
It is also an equation that determines the eigenvalues Ej.
If you put in an arbitrary value of Ej, in general there will

4.12. SOLVING THE SCHR ¬®ODINGER EQUATION
121
not be a solution for the eigenvector. Let‚Äôs take a very sim-
ple example: Suppose the Hamiltonian is the matrix ¬Øhœâ
2 œÉz.
Since œÉz has only two eigenvalues, namely ¬±1, the Hamil-
tonian also has only two eigenvalues, ¬± ¬Øhœâ
2 . If you put any
other value on the right-hand side of Eq. 4.28, there will not
be a solution. Because the operator H represents energy,
we often call Ej the energy eigenvalues and |Ej‚ü©the energy
eigenvectors of the system.
Exercise 4.5: Take any unit 3-vector ‚Éón and form the oper-
ator
H = ¬Øhœâ
2 œÉ ¬∑ ‚Éón.
Find the energy eigenvalues and eigenvectors by solving the
time-independent Schr¬®odinger equation. Recall that Eq. 3.23
gives œÉ ¬∑ ‚Éón in component form.
Let‚Äôs suppose we have found all the energy eigenvalues
Ej and the corresponding eigenvectors |Ej‚ü©. We can now use
that information to solve the time-dependent Schr¬®odinger
equation. The trick is to use the fact that the eigenvectors
form an orthonormal basis and then expand the state-vector
in that basis. Let the state-vector be called |Œ®‚ü©and write
|Œ®‚ü©=

j
Œ±j|Ej‚ü©.
Since the state-vector |Œ®‚ü©changes with time and the basis
vectors |Ej‚ü©do not, it follows that the coeÔ¨Écients Œ±j must

122
LECTURE 4. TIME AND CHANGE
also depend on time:
|Œ®(t)‚ü©=

j
Œ±j(t)|Ej‚ü©.
(4.29)
Now feed Eq. 4.29 into the time-dependent equation. The
result is

j
ÀôŒ±j(t)|Ej‚ü©= ‚àíi
¬ØhH

j
Œ±j(t)|Ej‚ü©.
Next, we use the fact that H|Ej‚ü©= Ej|Ej‚ü©to get

j
ÀôŒ±j(t)|Ej‚ü©= ‚àíi
¬Øh

j
EjŒ±j(t)|Ej‚ü©
or, regrouping,

j

ÀôŒ±j(t) + i
¬ØhEjŒ±j(t)

|Ej‚ü©= 0.
The Ô¨Ånal step should be easy to see. If a sum of basis vec-
tors equals zero, every coeÔ¨Écient must be zero. Hence, for
each eigenvalue Ej, Œ±j(t) must satisfy the simple diÔ¨Äerential
equation
dŒ±j(t)
dt
= ‚àíi
¬ØhEjŒ±j(t).
This, of course, is the familiar diÔ¨Äerential equation for an
exponential function of time, in this case with an imaginary
exponent. The solution is
Œ±j(t) = Œ±j(0)e‚àíi
¬Øh Ejt.
(4.30)

4.12. SOLVING THE SCHR ¬®ODINGER EQUATION
123
This equation tells us how the Œ±j change with time. It is
quite general and not restricted to spins, provided that the
Hamiltonian does not depend explicitly on time.
This is
our Ô¨Årst example of the deep connection between energy and
frequency, which recurs over and over throughout quantum
mechanics and quantum Ô¨Åeld theory. We will return to it
often.
In Eq. 4.30, the factors Œ±j(0) are the values of the coeÔ¨É-
cients at time zero. If we know the state-vector |Œ®‚ü©at time
zero, then the coeÔ¨Écients are given by the projections of |Œ®‚ü©
on the basis eigenvectors. We can write this as
Œ±j(0) = ‚ü®Ej|Œ®(0)‚ü©.
(4.31)
Now let‚Äôs put the whole thing together and write the full
solution of the time-dependent Schr¬®odinger equation:
|Œ®(t)‚ü©=

j
Œ±j(0) e‚àíi
¬Øh Ejt |Ej‚ü©.
When we use Eq. 4.31 to replace Œ±j(0), this equation be-
comes
|Œ®(t)‚ü©=

j
‚ü®Ej|Œ®(0)‚ü©e‚àíi
¬Øh Ejt |Ej‚ü©.
(4.32)
Eq. 4.32 can be written in the more elegant form,
|Œ®(t)‚ü©=

j
|Ej‚ü©‚ü®Ej|Œ®(0)‚ü©e‚àíi
¬Øh Ejt,
(4.33)

124
LECTURE 4. TIME AND CHANGE
which emphasizes that we‚Äôre summing over the basis vectors.
You may wonder how we just happen to ‚Äúknow‚Äù |Œ®(0)‚ü©. The
answer depends on the circumstances, but usually, we as-
sume we can use some apparatus to prepare the system in a
known state.
Before we discuss the bigger meaning of these equations,
I want to restate them as a recipe. I‚Äôll assume you already
know enough about the system and its space of states to get
started.
4.13
Recipe for a Schr¬®odinger Ket
1. Derive, look up, guess, borrow, or steal the Hamilto-
nian operator H.
2. Prepare an initial state |Œ®(0)‚ü©.
3. Find the eigenvalues and eigenvectors of H by solving
the time-independent Schr¬®odinger equation,
H|Ej‚ü©= Ej|Ej‚ü©.
4. Use the initial state-vector |Œ®(0)‚ü©, along with the eigen-
vectors |Ej‚ü©from step 3, to calculate the initial coeÔ¨É-
cients Œ±j(0):
Œ±j(0) = ‚ü®Ej|Œ®(0)‚ü©.
5. Rewrite |Œ®(0)‚ü©in terms of the eigenvectors |Ej‚ü©and
the initial coeÔ¨Écients Œ±j(0):
|Œ®(0)‚ü©=

j
Œ±j(0)|Ej‚ü©.

4.13. RECIPE FOR A SCHR ¬®ODINGER KET
125
What we‚Äôve done so far is to expand the initial state-vector
|Œ®(0)‚ü©in terms of the eigenvectors |Ej‚ü©of H. Why is that
basis better than any other? Because H tells us how things
evolve with time. We will use that knowledge now.
6. In the above equation, replace each Œ±j(0) with Œ±j(t)
to capture its time-dependence.
As a result, |Œ®(0)‚ü©
becomes |Œ®(t)‚ü©:
|Œ®(t)‚ü©=

j
Œ±j(t)|Ej‚ü©.
7. Using Eq. 4.30, replace each Œ±j(t) with Œ±j(0)e‚àíi
¬Øh Ejt:
|Œ®(t)‚ü©=

j
Œ±j(0)e‚àíi
¬Øh Ejt|Ej‚ü©.
(4.34)
8. Season according to taste.
We can now predict the probabilities for each possible
outcome of an experiment as a function of time, and we
are not restricted to energy measurements. Suppose L has
eigenvalues Œªj and eigenvectors |Œªj‚ü©.
The probability for
outcome Œª is
PŒª(t) = |‚ü®Œª|Œ®(t)‚ü©|2.

126
LECTURE 4. TIME AND CHANGE
Exercise 4.6: Carry out the Schr¬®odinger Ket recipe for a
single spin.
The Hamiltonian is H =
œâ¬Øh
2 œÉz and the Ô¨Ånal
observable is œÉx. The initial state is given as |u‚ü©(the state
in which œÉz = +1).
After time t, an experiment is done to measure œÉy. What
are the possible outcomes and what are the probabilities for
those outcomes?
Congratulations! You have now solved a real quantum me-
chanics problem for an experiment that can actually be car-
ried out in the laboratory. Feel free to pat yourself on the
back.
4.14
Collapse
We‚Äôve seen how the state-vector evolves between the time
that a system is prepared in a given state and the time that
it is brought into contact with an apparatus and measured. If
the state-vector were main focus of observational physics, we
would say that quantum mechanics is deterministic. But ex-
perimental physics is not about measuring the state-vector.
It is about measuring observables. Even if we know the state-
vector exactly, we don‚Äôt know the result of any given mea-
surement. Nevertheless, it is fair to say that between obser-
vations, the state of a system evolves in a perfectly deÔ¨Ånite
way, according to the time-dependent Schr¬®odinger equation.
But something diÔ¨Äerent happens when an observation is
made.
An experiment to measure L will have an unpre-
dictable outcome, but after the measurement is made, the
system is left in an eigenstate of L. Which eigenstate? The

4.14. COLLAPSE
127
one corresponding to the outcome of the measurement. But
this outcome is unpredictable. So it follows that during an
experiment the state of a system jumps unpredictably to an
eigenstate of the observable that was measured. This phe-
nomenon is called the collapse of the wave function.4
To put it another way, suppose the state-vector is

j
Œ±j|Œªj‚ü©
just before the measurement of L. Randomly, with probabil-
ity |Œ±j|2, the apparatus measures a value Œªj and leaves the
system in a single eigenstate of L, namely |Œªj‚ü©. The entire
superposition of states collapses to a single term.
This strange fact‚Äîthat the system evolves one way be-
tween measurements and another way during a measure-
ment‚Äîhas been a source of contention and confusion for
decades. It raises a question: Shouldn‚Äôt the act of measure-
ment itself be described by the laws of quantum mechanics?
The answer is yes. The laws of quantum mechanics are
not suspended during measurement. However, to examine
the measurement process itself as a quantum mechanical
evolution, we must consider the entire experimental setup,
including the apparatus, as part of a single quantum sys-
tem. We‚Äôll discuss that topic‚Äîhow systems are combined
into composite systems‚Äîin Lecture 6. But Ô¨Årst, a few words
about uncertainty.
4We have not yet explained what a wave function is, but we‚Äôll do
so shortly, in Section 5.1.2.


Lecture 5
Uncertainty and Time
Dependence
Lenny: Good evening, General. Nice to see you again.
The General: Lenny? Is that you? It‚Äôs been forever. Well,
a long time anyway. Who‚Äôs your friend?
Lenny: His name is Art.
Art, shake hands with General
Uncertainty.
5.1
Mathematical Interlude:
Complete Sets of Commuting
Variables
5.1.1
States That Depend On More Than
One Measurable
The physics of a single spin is extremely simple, and that‚Äôs
what makes it so attractive as an illustrative example. But
129

130
LECTURE 5. UNCERTAINTY & TIME DEP
that also means there‚Äôs a lot it can‚Äôt illustrate. One property
of a single spin is that its state can be fully speciÔ¨Åed by the
eigenvalue of a single operator, say œÉz. If the value of œÉz
is known, then no other observable‚Äîsuch as œÉx‚Äîcan also
be speciÔ¨Åed.
As we have seen, measuring either of these
quantities destroys any information we may have had about
the other one.
But in more complicated systems, we may have multiple
observables that are compatible; that is, their values can be
known simultaneously. Here are two examples:
‚Ä¢ A particle moving in three-dimensional space. A basis
of states for this system is speciÔ¨Åed by the position of
the particle, but this takes three position coordinates.
Thus, we have states that are speciÔ¨Åed by three num-
bers, |x, y, z‚ü©. We will see later that all three spatial
coordinates of a particle can be simultaneously speci-
Ô¨Åed.
‚Ä¢ A system composed of two physically independent spins;
in other words, a system of two qubits. Later, we will
see how to combine systems to form bigger systems.
But for now we can just say that the two-spin system
can be described by two observables. Namely, we have
a state in which both spins are up, another in which
both are down, another in which the Ô¨Årst is up while
the second is down, and another in which these spins
are reversed. To put it more brieÔ¨Çy, we can charac-
terize the two-spin system by two observables: the z
component of the Ô¨Årst spin and the z component of
the second spin. Quantum mechanics does not forbid

5.1. INTERLUDE: COMMUTING VARIABLES
131
simultaneous knowledge of these two observables. In
fact, one can choose any component of one spin and
any component of the other spin. Quantum mechanics
allows simultaneous knowledge of both.
In these situations, we need multiple measurements to fully
characterize the state of the system. For example, in our two-
spin system, we measure each spin separately and associate
these measurements with two diÔ¨Äerent operators. We‚Äôll call
these operators L and M.
A measurement leaves the system in an eigenstate (con-
sisting of a single eigenvector), corresponding to the value
(an eigenvalue) that was measured. If we measure both spins
in a two-spin system, the system winds up in a state that
is simultaneously an eigenvector of L and an eigenvector of
M. We call this a simultaneous eigenvector of the operators
L and M.
The two-spin example gives us something concrete to
think about, but keep in mind that our results will be far
more general‚Äîthey will apply to any system that is charac-
terized by two diÔ¨Äerent operators. And as you might guess,
there is nothing magic about the number two. The ideas pre-
sented here generalize to larger systems that require many
operators to characterize them.
To work with two diÔ¨Äerent compatible operators, we‚Äôll
need two sets of labels for their basis vectors. We‚Äôll use the
labels Œªi and Œºa. The symbols Œªi and Œºa are the eigenvalues
of L and M. The subscripts i and a run over all the possible
outcomes of measurements of L and M. We assume that

132
LECTURE 5. UNCERTAINTY & TIME DEP
there is a basis of state-vectors |Œªi, Œºa‚ü©that are simultaneous
eigenvectors of both observables. In other words,
L|Œªi, Œºa‚ü©= Œªi|Œªi, Œºa‚ü©
M|Œªi, Œºa‚ü©= Œºa|Œªi, Œºa‚ü©.
To make these equations a little less precise but a little easier
to read, I will sometimes leave out the subscripts:
L|Œª, Œº‚ü©= Œª|Œª, Œº‚ü©
M|Œª, Œº‚ü©= Œº|Œª, Œº‚ü©.
In order to have a basis of simultaneous eigenvectors, the
operators L and M must commute.
This is easy to see.
We begin by acting on any of the basis vectors with the
product LM, and then use the fact that the basis vector is
an eigenvector of both:
LM|Œª, Œº‚ü©= LŒº|Œª, Œº‚ü©,
or
LM|Œª, Œº‚ü©= ŒªŒº|Œª, Œº‚ü©.
The eigenvalues Œª, Œº are of course just numbers and it doesn‚Äôt
matter which one appears Ô¨Årst when we multiply them. Thus,
if we reverse the order of these operators, and let the opera-
tor ML act on the same basis vector, we get the same result:

5.1. INTERLUDE: COMMUTING VARIABLES
133
LM|Œª, Œº‚ü©= ML|Œª, Œº‚ü©,
or, more succinctly,
[L, M] |Œª, Œº‚ü©= 0,
(5.1)
where the right-hand side represents the zero vector. This
result would not be very helpful if it were only true for a
particular basis vector. But the reasoning that leads us to
Eq. 5.1 is valid for any of the basis vectors. That‚Äôs enough to
ensure that the operator [L, M] = 0. If an operator annihi-
lates every member of a basis, it must also annihilate every
vector in the vector space.1
An operator that annihilates
every vector is exactly what we mean by the zero operator.
Thus, we prove that if there is a complete basis of simulta-
neous eigenvectors of two observables, the two observables
must commute. It turns out that the converse of this theo-
rem is also true: if two observables commute, then there is
a complete basis of simultaneous eigenvectors of the two ob-
servables. To put it simply, the condition for two observables
to be simultaneously measurable is that they commute.
As we mentioned earlier, this theorem is more general.
One may need to specify a larger number of observables to
completely label a basis. Regardless of the number of ob-
servables that are needed, they must all commute among
themselves. We call this collection a complete set of com-
muting observables.
1Do you see why?

134
LECTURE 5. UNCERTAINTY & TIME DEP
5.1.2
Wave Functions
Now we‚Äôll introduce a concept called the wave function. For
now, ignore the name; in general, the quantum wave function
may have nothing to do with waves. Later, when we study
the quantum mechanics of particles (Lectures 8‚Äì10), we‚Äôll
Ô¨Ånd out about the connection between wave functions and
waves.
Suppose we have a basis of states for some quantum sys-
tem. Let the orthonormal basis vectors be called |a, b, c, . . .‚ü©,
where a, b, c, . . . are the eigenvalues of some complete set of
commuting observables A, B, C, . . . . Now, consider an arbi-
trary state vector |Œ®‚ü©. Since the vectors |a, b, c, . . .‚ü©are an
orthonormal basis, |Œ®‚ü©can be expanded in terms of them:
|Œ®‚ü©=

a,b,c,...
œà(a, b, c, . . . )|a, b, c, . . .‚ü©.
The quantities œà(a, b, c, . . . ) are the coeÔ¨Écients that enter
the expansion. Each of them is also equal to the inner prod-
uct of |Œ®‚ü©with one of the basis vectors:
œà(a, b, c, . . . ) = ‚ü®a, b, c, . . . |Œ®‚ü©.
(5.2)
The set of coeÔ¨Écients œà(a, b, c, . . . ) is called the wave func-
tion of the system in the basis deÔ¨Åned by the observables
A, B, C, . . . . The mathematical deÔ¨Ånition of a wave function
is given by Eq. 5.2, which seems formal and abstract, but the
physical meaning of the wave function is profoundly impor-
tant. According to the basic probability principle of quan-
tum mechanics, the squared magnitude of the wave function

5.1. INTERLUDE: COMMUTING VARIABLES
135
is the probability for the commuting observables to have val-
ues a, b, c, . . . :
P(a, b, c, . . . ) = œà‚àó(a, b, c, . . . ) œà(a, b, c, . . . ).
The form of the wave function depends on which observables
we choose to focus on. That‚Äôs because calculations for two
diÔ¨Äerent observables rely on diÔ¨Äerent sets of basis vectors.
For example, in the case of a single spin, the inner products
œà(u) = ‚ü®u|Œ®‚ü©
and
œà(d) = ‚ü®d|Œ®‚ü©
deÔ¨Åne the wave function in the œÉz basis, while
œà(r) = ‚ü®r|Œ®‚ü©
and
œà(l) = ‚ü®l|Œ®‚ü©
deÔ¨Åne the wave function in the œÉx basis.
An important feature of the wave function follows from
the fact that the total probability sums to one:

a,b,c,...
œà‚àó(a, b, c, . . . ) œà(a, b, c, . . . ) = 1.

136
LECTURE 5. UNCERTAINTY & TIME DEP
5.1.3
A Note About Terminology
The term wave function, as used in this book, refers to the
collection of coeÔ¨Écients (also called components) that mul-
tiply the basis vectors in an eigenfunction expansion. For
example, if we expand a state-vector |Œ®‚ü©as follows,
|Œ®‚ü©=

j
Œ±j|œàj‚ü©,
where the |œàj‚ü©are the orthonormal eigenvectors of a Her-
mitian operator, the collection of coeÔ¨Écients Œ±j‚Äîthe things
we called œà(a, b, c, . . . ) just above‚Äîis what we mean by the
wave function. In situations where the state-vector is ex-
pressed as an integral rather than a sum, the wave function
is continuous rather than discrete.
So far, we have been careful to distinguish the wave func-
tion from the state-vectors |œàj‚ü©, and this is a common con-
vention. However, some authors refer to wave functions as
though they are the state-vectors. This ambiguous use of ter-
minology can be confusing. It becomes less confusing when
you realize that a wave function really can represent a state-
vector. It is reasonable to think of the Œ±j coeÔ¨Écients as the
coordinates of the state-vector in a speciÔ¨Åc basis of eigen-
vectors.
This is similar to saying that a set of Cartesian
coordinates represents a particular point in 3-space relative
to a speciÔ¨Åc coordinate frame. To avoid confusion, just try
to be aware of which convention is being followed. In this
book, we will generally use uppercase symbols, such as Œ®, to
represent state-vectors, and lowercase symbols, such as œà, to
represent wave functions.

5.2. MEASUREMENT
137
5.2
Measurement
Let‚Äôs return to the concept of measurement.
Suppose we
measure two observables L and M in a single experiment,
and the system is left in a simultaneous eigenvector of these
two observables. As we learned in Section 5.1.1, this means
that L and M must commute.
But what if they don‚Äôt commute? Then, in general, it is
not possible to have unambiguous knowledge of both. Later
on, we will make this more quantitative in the form of the
uncertainty principle, Heisenberg‚Äôs being a special case.
Let‚Äôs go back to our touchstone, the problem of a single
spin.
Any observable of a spin is represented by a 2 √ó 2
Hermitian matrix, and any such matrix has the form
 r
w
w‚àó
r‚Ä≤

,
with the diagonal elements being real and the other two be-
ing complex conjugates. The implication is that it takes ex-
actly four real parameters to specify this observable. In fact,
there is a neat way to write any spin observable in terms of
the Pauli matrices, œÉx, œÉy, and œÉz, and one more matrix: the
unit matrix I. As you recall,
œÉx =
 0
1
1
0

œÉy =
 0
‚àíi
i
0


138
LECTURE 5. UNCERTAINTY & TIME DEP
œÉz =
 1
0
0
‚àí1

I =
 1
0
0
1

.
Any 2 √ó 2 Hermitian matrix L can be written as a sum of
four terms,
L = aœÉx + bœÉy + cœÉz + dI ,
where a, b, c, and d are real numbers.
Exercise 5.1: Verify this claim.
The unit operator I is oÔ¨Écially an observable because it
is Hermitian, but it‚Äôs a very boring one. There is only one
possible value this trivial observable can have, namely 1, and
every state-vector is an eigenvector. If we ignore I, then the
most general observable is a superposition of the three spin
components œÉx, œÉy, and œÉz. Can any pair of spin components
be simultaneously measured? Only if they commute. But it
is easy to calculate the commutators for these spin compo-
nents. Just use the matrix representation to multiply them
in both orders, and then subtract.
The commutation relations we listed in Eqs. 4.26,
[œÉx, œÉy]
=
2iœÉz

5.3. THE UNCERTAINTY PRINCIPLE
139
[œÉy, œÉz]
=
2iœÉx
[œÉz, œÉx]
=
2iœÉy,
tell us straightaway that no two spin components can be
simultaneously measured, because the right-hand sides are
not zero. In fact, no two components of the spin along any
axes can be simultaneously measured.
5.3
The Uncertainty Principle
Uncertainty is one of the hallmarks of quantum mechanics,
but it is not always the case that the result of an experiment
is uncertain. If a system is in an eigenstate of an observable,
then there is no uncertainty about the result of measuring
that observable.
But whatever the state, there is always
uncertainty about some observable. If the state happens to
be an eigenvector of one Hermitian operator‚Äîcall it A‚Äî
then it will not be an eigenvector of other operators that
don‚Äôt commute with A. Thus, as a rule, if A and B do not
commute, then there must be uncertainty in one or the other,
if not both.
The iconic example of this mutual uncertainty is the
Heisenberg Uncertainty Principle, which in its original form
had to do with the position and momentum of a particle.
But Heisenberg‚Äôs ideas can be expanded into a much more
general principle that applies to any two observables that
happen not to commute. An example would be two compo-
nents of a spin. We now have all the ingredients necessary
to derive the general form of the uncertainty principle.

140
LECTURE 5. UNCERTAINTY & TIME DEP
5.4
The Meaning of Uncertainty
We need to be very certain about what we mean by uncer-
tainty if we want to quantify it. Let‚Äôs suppose the eigenval-
ues of the observable A are called a. Then, given a state |Œ®‚ü©,
there is a probability distribution P(a) with the usual prop-
erties. The expectation value of A is the ordinary average:
‚ü®Œ®|A|Œ®‚ü©=

a
aP(a).
Roughly speaking, this means that P(a) is centered around
the expectation value. What we will mean by ‚Äúthe uncer-
tainty in A‚Äù is the so-called standard deviation. To compute
the standard deviation, begin by subtracting from A its ex-
pectation value. We deÔ¨Åne the operator ¬ØA to be:
¬ØA = A ‚àí‚ü®A‚ü©.
By deÔ¨Åning ¬ØA in this way, we have subtracted an expecta-
tion value from an operator, and it‚Äôs not completely clear
what that means. Let‚Äôs take a closer look. The expectation
value is itself a real number. Every real number is also an
operator, namely an operator proportional to the identity or
unit operator I. To make the meaning clear, we can write ¬ØA
in a more complete form:
¬ØA = A ‚àí‚ü®A‚ü©I.
The probability distribution for ¬ØA is exactly the same as the
distribution for A except that it is shifted so that the average

5.4. THE MEANING OF UNCERTAINTY
141
of ¬ØA is zero. The eigenvectors of ¬ØA are the same as those of
A and the eigenvalues are just shifted so that their average
is zero as well. In other words, the eigenvalues of ¬ØA are
¬Øa = a ‚àí‚ü®A‚ü©.
The square of the uncertainty (or standard deviation) of A,
which we call (ŒîA)2, is deÔ¨Åned by
(ŒîA)2 =

a
¬Øa2P(a)
(5.3)
or
(ŒîA)2 =

a
(a ‚àí‚ü®A‚ü©)2P(a).
(5.4)
This may also be written as
(ŒîA)2 = ‚ü®Œ®| ¬ØA2|Œ®‚ü©.
If the expectation value of A is zero, then the uncertainty
ŒîA takes the simpler form
(ŒîA)2 = ‚ü®Œ®|A2|Œ®‚ü©.
In other words the square of the uncertainty is the average
value of the operator A2.

142
LECTURE 5. UNCERTAINTY & TIME DEP
5.5
Cauchy-Schwarz Inequality
The uncertainty principle is an inequality that says the prod-
uct of the uncertainties of A and B is larger than something
that involves their commutator. The basic mathematical in-
equality is the familiar triangle inequality. It says that in
any vector space, the magnitude of one side of a triangle is
less than the sum of the magnitudes of the other two sides.
For real vector spaces, we derive
|X||Y | ‚â•|X ¬∑ Y |
(5.5)
from the triangle inequality,
|X| + |Y | ‚â•|X + Y |.
5.6
The Triangle Inequality and
the Cauchy-Schwarz Inequality
The triangle inequality is motivated, of course, by the prop-
erties of ordinary triangles, but it‚Äôs actually far more general
and applies to a large class of vector spaces. You can get the
basic idea by looking at Fig. 5.1, where the sides of the tri-
angle are taken to be ordinary geometric vectors in a plane.
The triangle inequality is just the statement that the sum of
any two sides is bigger than the third side, and the under-
lying idea is that the shortest path between two points is a
straight line. The shortest path between point 1 and point
3 is side Z, and the sum of the other two sides is certainly
bigger.

5.6. TRIANGLE & C-S INEQUALITIES
143
Figure 5.1: The Triangle Inequality. The sum of the lengths
of vectors ‚ÉóX and ‚ÉóY is greater than or equal to the length of
vector ‚ÉóZ. (The shortest path between two points is a straight
line.)
The triangle inequality can be expressed in more than one
way. We‚Äôll start with the basic deÔ¨Ånition and then massage
it into the form we need. We know that
|X| + |Y | ‚â•|Z|.
If we think of X and Y as vectors that can be added, we can
write the above as
| ‚ÉóX| + |‚ÉóY | ‚â•| ‚ÉóX + ‚ÉóY |.
If we square this equation, it becomes
| ‚ÉóX|2 + |‚ÉóY |2 + 2| ‚ÉóX||‚ÉóY | ‚â•| ‚ÉóX + ‚ÉóY |2.

144
LECTURE 5. UNCERTAINTY & TIME DEP
But the right-hand side can be expanded as
| ‚ÉóX + ‚ÉóY |2 = | ‚ÉóX|2 + |‚ÉóY |2 + 2( ‚ÉóX ¬∑ ‚ÉóY ).
Why? Because | ‚ÉóX + ‚ÉóY |2 is just ( ‚ÉóX + ‚ÉóY )¬∑( ‚ÉóX + ‚ÉóY ). Collecting
these results, we get
| ‚ÉóX|2 + |‚ÉóY |2 + 2| ‚ÉóX||‚ÉóY | ‚â•| ‚ÉóX|2 + |‚ÉóY |2 + 2( ‚ÉóX ¬∑ ‚ÉóY ).
Now, we just subtract | ‚ÉóX|2 + |‚ÉóY |2 from each side and then
divide by 2, leaving us with
| ‚ÉóX||‚ÉóY | ‚â•‚ÉóX ¬∑ ‚ÉóY .
(5.6)
This is another form of the triangle inequality. It says that,
given any two vectors ‚ÉóX and ‚ÉóY , the product of their lengths
is greater than or equal to their dot product.
This is no
surprise‚Äîthe dot product is often deÔ¨Åned as
‚ÉóX ¬∑ ‚ÉóY = | ‚ÉóX||‚ÉóY | cos Œ∏,
where Œ∏ is the angle between the two vectors. But we know
that the cosine of an angle always stays in the range ‚àí1
to +1, so the right-hand side must always be less than or
equal to | ‚ÉóX||‚ÉóY |. This relationship is true for vectors in two
dimensions, three dimensions, or an arbitrary number of di-
mensions. It‚Äôs even true for vectors in complex vector spaces.
It‚Äôs generally true for vectors in any vector space, provided

5.6. TRIANGLE & C-S INEQUALITIES
145
the length of the vector is deÔ¨Åned as the square root of the
vector‚Äôs inner product with itself. As we go forward, we plan
to use Inequality 5.6 in the squared form, that is,
| ‚ÉóX|2|‚ÉóY |2 ‚â•( ‚ÉóX ¬∑ ‚ÉóY )2
or
| ‚ÉóX|2|‚ÉóY |2 ‚â•| ‚ÉóX ¬∑ ‚ÉóY |2.
(5.7)
In this form, it‚Äôs called the Cauchy-Schwarz inequality.
For complex vector spaces, the triangle inequality takes
a slightly more complicated form. Let |X‚ü©and |Y ‚ü©be any
two vectors in a complex vector space. The magnitudes of
the three vectors |X‚ü©, |Y ‚ü©, and |X‚ü©+ |Y ‚ü©are
|X|
=

‚ü®X|X‚ü©
|Y |
=

‚ü®Y |Y ‚ü©
|X + Y |
=

(‚ü®X| + ‚ü®Y |)(|X‚ü©+ |Y ‚ü©)
(5.8)
We now follow the same steps as we did for the real case:
First write
|X| + |Y | ‚â•|X + Y |.

146
LECTURE 5. UNCERTAINTY & TIME DEP
Then square it and simplify:
2|X||Y | ‚â•|‚ü®X|Y ‚ü©+ ‚ü®Y |X‚ü©|.
(5.9)
This is the form of the Cauchy-Schwarz inequality that will
lead to the uncertainty principle. But what does it have to
do with the two observables A and B? We‚Äôll Ô¨Ånd out by
cleverly deÔ¨Åning |X‚ü©and |Y ‚ü©.
5.7
The General Uncertainty
Principle
Let |Œ®‚ü©be any ket and let A and B be any two observables.
We now deÔ¨Åne |X‚ü©and |Y ‚ü©as follows:
|X‚ü©
=
A|Œ®‚ü©
|Y ‚ü©
=
iB|Œ®‚ü©.
(5.10)
Notice the i in the second deÔ¨Ånition. Now, substitute 5.10
into 5.9 to get
2

‚ü®A2‚ü©‚ü®B2‚ü©‚â•|‚ü®Œ®|AB|Œ®‚ü©‚àí‚ü®Œ®|BA|Œ®‚ü©|.
(5.11)
The minus sign is due to the factor of i in the second deÔ¨Åni-
tion in 5.10. Using the deÔ¨Ånition of a commutator, we Ô¨Ånd
that
2

‚ü®A2‚ü©‚ü®B2‚ü©‚â•|‚ü®Œ®|[A, B]|Œ®‚ü©|.
(5.12)

5.7. GENERAL UNCERTAINTY PRINCIPLE
147
Let‚Äôs suppose for the moment that A and B have expectation
values of zero. In that case, ‚ü®A2‚ü©is just the square of the
uncertainty in A, that is, (ŒîA)2, and ‚ü®B2‚ü©is just (ŒîB)2.
Thus we can rewrite Eq. 5.12 as
ŒîA ŒîB ‚â•1
2|‚ü®Œ®|[A, B]|Œ®‚ü©|.
(5.13)
ReÔ¨Çect on this mathematical inequality for a moment. On
the left side, we see the product of the uncertainties of the
two observables A and B in the state Œ®. The inequality
says that this product cannot be smaller than the right side,
which involves the commutator of A and B. SpeciÔ¨Åcally, it
says that the product of the uncertainties cannot be smaller
than half the magnitude of the expectation value of the com-
mutator.
The general uncertainty principle is a quantitative ex-
pression of something we already suspected: if the commu-
tator of A and B is not zero, then both observables cannot
simultaneously be certain.
But what if the expectation value of A or B is not zero?
In that case, the trick is to redeÔ¨Åne two new operators in
which the expectation values have been subtracted oÔ¨Ä:
¬ØA = A ‚àí‚ü®A‚ü©
¬ØB = B ‚àí‚ü®B‚ü©.
Then repeat the whole process, replacing A and B with ¬ØA
and ¬ØB. The following exercise serves as a guide.

148
LECTURE 5. UNCERTAINTY & TIME DEP
Exercise 5.2:
1) Show that ŒîA2 = ‚ü®¬ØA2‚ü©and ŒîB2 = ‚ü®¬ØB2‚ü©.
2) Show that [ ¬ØA, ¬ØB] = [A, B].
3) Using these relations, show that
ŒîA ŒîB ‚â•1
2|‚ü®Œ®|[A, B]|Œ®‚ü©|.
Later, in Lecture 8, we will use this very general version of
the uncertainty principle to prove the original form of Heisen-
berg‚Äôs Uncertainty Principle: The product of the uncertain-
ties of the position and momentum of a particle cannot be
less than half of Planck‚Äôs constant.

Lecture 6
Combining Systems:
Entanglement
Art: This is a pretty friendly place after all.
Except for
Minus One, I don‚Äôt see too many loners.
Lenny: Mingling is only natural at a place like this. And not
just because it‚Äôs cramped. Just keep track of your wallet and
don‚Äôt get too entangled.
6.1
Mathematical Interlude:
Tensor Products
6.1.1
Meet Alice and Bob
Figuring out how systems combine to make bigger systems
is a large part of what we do in physics. I hardly need to tell
you that an atom is a collection of nucleons and electrons,
each of which could be considered a quantum system in its
own right.
149

150
LECTURE 6. ENTANGLEMENT
When talking about composite systems, it‚Äôs easy to get
bogged down in formal language like System A and System
B. Most physicists prefer lighter-weight, informal language
instead, and Alice and Bob have become near-universal sub-
stitutes for A and B. We can think of Alice and Bob as pur-
veyors of composite systems and laboratory setups of every
description. Their inventory and expertise are limited only
by our imaginations, and they gladly tackle diÔ¨Écult or dan-
gerous assignments like jumping into black holes. They‚Äôre
true geek superheroes!
Let‚Äôs say that Alice and Bob have provided two systems‚Äî
Alice‚Äôs system and Bob‚Äôs system. Alice‚Äôs system‚Äîwhatever
it is‚Äîis described by a space of states called SA, and similarly
Bob‚Äôs system is described by a space of states called SB.
Now let‚Äôs say that we want to combine the two systems
into a single composite system. Before going any further,
let‚Äôs be more speciÔ¨Åc about the systems we‚Äôre starting with.
For example, Alice‚Äôs system could be a quantum mechanical
coin with two basis states H and T. Of course, a classical
coin must be in either one state or the other, but a quantum
coin can exist in a superposition:
Œ±H|H

+ Œ±T|T

.
You‚Äôll notice that I‚Äôve used an unusual notation for Alice‚Äôs
ket-vectors.
This is to distinguish them from Bob‚Äôs kets.
The new notation is intended to discourage us from adding
vectors in Alice‚Äôs space SA to vectors in Bob‚Äôs space SB.
Alice‚Äôs SA is a two-dimensional vector space‚Äîit is deÔ¨Åned
by the two basis vectors |H

and |T

.

6.1. INTERLUDE: TENSOR PRODUCTS
151
Bob‚Äôs system might also be a coin, but then again it
might be something else. Let‚Äôs assume it‚Äôs a quantum die.
Bob‚Äôs space of states SB would then be six-dimensional, with
the basis
|1‚ü©
|2‚ü©
|3‚ü©
|4‚ü©
|5‚ü©
|6‚ü©
denoting the six faces of the die. Just like Alice‚Äôs coin, Bob‚Äôs
die is quantum mechanical, and the six states can be super-
posed in a similar way.
6.1.2
Representing the Combined System
Now imagine that Bob‚Äôs and Alice‚Äôs systems both exist, and
form a single composite system. The Ô¨Årst question is: How
could we construct the state-space‚Äîcall it SAB‚Äîfor the com-
bined system? The answer is to form the tensor product of
SA and SB. The notation for this operation is
SAB = SA ‚äóSB.
To deÔ¨Åne SAB, it is enough to specify its basis vectors. The
basis vectors are exactly what you might expect. The top

152
LECTURE 6. ENTANGLEMENT
Bob‚Äôs state-labels
Alice‚Äôs
state
labels
Alice‚Äôs
system
Bob‚Äôs
system
-
1
2
3
4
5
6
H
T
H1
T1
T2
T3
T4
T5
T6
H2
H3
H4
H5
H6
State-Labels for Combined System SAB
H
Figure 6.1: The basis states of the composite system SAB,
shown as a table.
Across the top are the state-labels for
Bob‚Äôs die.
The state-labels for Alice‚Äôs coin are shown on
the left. The state-labels for the combined system are the
table entries. Each combined state-label shows the state of
each of the two subsystems. For example, the state-label H4
denotes a state in which Alice‚Äôs coin shows H and Bob‚Äôs die
shows 4.

6.1. INTERLUDE: TENSOR PRODUCTS
153
half of Fig. 6.1 shows a table whose columns correspond to
Bob‚Äôs six basis vectors and whose rows correspond to Alice‚Äôs
two basis vectors.
Each box in the table denotes a basis
vector for the SAB system. For example, the box labeled H4
represents a state in SAB in which the coin shows Heads and
the die shows the number 4. In the combined system, there
are twelve basis vectors altogether.
There are various ways to represent these states symbol-
ically. We could represent the H4 state using explicit nota-
tion, as |H} ‚äó|4‚ü©or |H}|4‚ü©. Usually, it‚Äôs more convenient
to use the composite notation |H4‚ü©. This emphasizes that
we‚Äôre talking about a single state with a two-part label. The
left half labels Alice‚Äôs subsystem, and the right half labels
Bob‚Äôs. The explicit and composite notations both have the
same meaning‚Äîthey refer to the same state.
Once the basis vectors are listed‚Äîin this case, twelve
of them‚Äîwe can combine them linearly to form arbitrary
superpositions. Thus, the tensor product space in this case
is twelve-dimensional. A superposition of two of these basis
vectors might look like
Œ±h3|H3‚ü©+ Œ±t4|T4‚ü©.
In each case, the Ô¨Årst half of the state-label describes the
state of Alice‚Äôs coin, and the second half describes the state
of Bob‚Äôs die.
Sometimes, we‚Äôll need to refer to an arbitrary basis vector
in SAB. To do that, we‚Äôll use ket-vectors that look like this,
|ab‚ü©,

154
LECTURE 6. ENTANGLEMENT
or like this,
|a‚Ä≤b‚Ä≤‚ü©.
In this notation, the a or a‚Ä≤ (or whatever the left-hand char-
acter of the label happens to be) represents one of Alice‚Äôs
states, and the b or b‚Ä≤ represents one of Bob‚Äôs states.
There is one aspect of this notation that is tricky. Even
though our SAB state-labels are doubly indexed, ket-vectors
like |ab‚ü©or |H3‚ü©represent a single state of the combined
system. In other words, we‚Äôre using a double index to label
a single state. This will take some getting used to. Alice‚Äôs
part of the state-label is always on the left and Bob‚Äôs part is
always on the right‚Äîkeeping Alice and Bob in alphabetical
order makes this convention easy to remember.
The rules are the same for more general systems. The
only diÔ¨Äerence is that the two A-states and the six B-states
would be replaced by NA and NB states respectively, and the
tensor product would have dimension
NAB = NANB.
Systems with three or more components can be represented
by tensor products of three or more state spaces, but we
won‚Äôt do that here.
Now that we‚Äôve described Alice‚Äôs and Bob‚Äôs separate spac-
es SA and SB, as well as the combined space SAB, there‚Äôs still
one more bit of notation to set up. Alice has a set of oper-
ators, labeled œÉ, that act on her system. Bob has a similar

6.2. CLASSICAL CORRELATION
155
set for his system, which we can label œÑ, so we don‚Äôt mix
them up with Alice‚Äôs. Alice may have several œÉ operators,
and likewise Bob may have several œÑ operators. With this
framework in hand, we‚Äôre ready to explore composite sys-
tems in greater depth. Later on, in Lecture 7, we‚Äôll explain
how to work with tensor product operators in component
form‚Äîexpressed as matrices and column vectors.
By now, there should be no doubt in your mind that
quantum physics is diÔ¨Äerent from classical physics, right
down to its logical roots. In this lecture and the next one,
I am going to hit you even harder with this idea. We are
going to discuss an aspect of quantum physics that is so
diÔ¨Äerent from classical physics that, as of this writing, it
has puzzled‚Äîand aggravated‚Äîphysicists and philosophers
for almost 80 years. It drove its discoverer, Einstein, to the
conclusion that something very deep is missing from quan-
tum mechanics, and physicists have been arguing about it
ever since. As Einstein realized, in accepting quantum me-
chanics, we are buying into a view of reality that is radically
diÔ¨Äerent from the classical view.
6.2
Classical Correlation
Before we get to quantum entanglement, let‚Äôs spend a few
minutes on what we might call classical entanglement. In the
following experiment, Alice (A) and Bob (B) will get some
help from Charlie (C).
Charlie has two coins in his hands‚Äîa penny and a dime.
He mixes them up and holds them out, one in each hand, to

156
LECTURE 6. ENTANGLEMENT
Alice and Bob, and gives one coin to each of them. No one
looks at the coins and no one knows who has which. Then,
Alice gets on the shuttle to Alpha Centauri while Bob stays
in Palo Alto. Charlie has done his job and doesn‚Äôt matter
anymore (sorry, Charlie).
Before Alice‚Äôs big trip, Alice and Bob synchronize their
clocks‚Äîthey have done their relativity homework and ac-
counted for time dilation and all that. They agree that Alice
will look at her coin just a second or two before Bob looks
at his.
Everything proceeds smoothly, and when Alice gets to
Alpha Centauri she indeed looks at her coin.
Amazingly,
the instant she looks at it, she immediately knows exactly
what coin Bob will see, even before he looks. Is this crazy?
Have Alice and Bob succeeded in breaking relativity‚Äôs most
fundamental rule, which states that information cannot go
faster than the speed of light?
Of course not. What would violate relativity would be
for Alice‚Äôs observation to instantly tell Bob what to expect.
Alice may know what coin Bob will see but she has no way
to tell him‚Äînot without sending him a real message from
Alpha Centauri, and that would take at least the four years
required for light to make the trip.
Let‚Äôs do this experiment many times, either with many
Alice-Bob pairs or with the same pair spread out over time.
In order to be quantitative, Charlie (he‚Äôs back now, having
accepted our apology) paints a ‚ÄúœÉ = +1‚Äù on each penny and
a ‚ÄúœÉ = ‚àí1‚Äù on each dime. If we assume that Charlie really
is random in the way he shuÔ¨Ñes the coins, then the following
facts will emerge:

6.2. CLASSICAL CORRELATION
157
‚Ä¢ On average, both A and B will get as many pennies as
dimes. Calling the values of A‚Äôs observations œÉA and
B‚Äôs observations œÉB, we can express this fact mathe-
matically as
‚ü®œÉA‚ü©
=
0
‚ü®œÉB‚ü©
=
0.
(6.1)
‚Ä¢ If A and B record their observations and then get to-
gether back in Palo Alto to compare them, they will
Ô¨Ånd a strong correlation.1 For each trial, if A observed
œÉA = +1, then B observed œÉB = ‚àí1, and vice versa.
In other words, the product œÉAœÉB always equals ‚àí1:
‚ü®œÉAœÉB‚ü©= ‚àí1.
Notice that the average of the products (of œÉA and œÉB) is not
equal to the product of the averages‚ÄîEqs. 6.1 tell us that
‚ü®œÉA‚ü©‚ü®œÉB‚ü©is zero. In symbols,
‚ü®œÉA‚ü©‚ü®œÉB‚ü©Ã∏= ‚ü®œÉAœÉB‚ü©,
or
‚ü®œÉAœÉB‚ü©‚àí‚ü®œÉA‚ü©‚ü®œÉB‚ü©Ã∏= 0.
(6.2)
1Actually, it‚Äôs a perfect correlation in this example.

158
LECTURE 6. ENTANGLEMENT
This indicates that Alice‚Äôs and Bob‚Äôs observations are corre-
lated. In fact, the quantity
‚ü®œÉAœÉB‚ü©‚àí‚ü®œÉA‚ü©‚ü®œÉB‚ü©
is called the statistical correlation between Bob‚Äôs and Alice‚Äôs
observations. It‚Äôs called the statistical correlation even if it is
zero. When the statistical correlation is nonzero, we say the
observations are correlated. The source of this correlation
is the fact that originally Alice and Bob were in the same
location and Charlie had one of each type of coin. The cor-
relation remained when Alice went to Alpha Centauri simply
because the coins didn‚Äôt change during the trip. There is ab-
solutely nothing strange about this or about Inequality 6.2.
It is a very common property of statistical distributions.
Suppose you have a probability distribution P(a, b) for
two variables a and b. If the variables are completely uncor-
related, then the probability will factorize:
P(a, b) = PA(a)PB(b),
(6.3)
where PA(a) and PB(b) are the individual probabilities for
a and b. (I added subscripts to the function symbols as a
reminder that they could be diÔ¨Äerent functions of their ar-
guments.) It is easy to see that if the probability factorizes
in this fashion, then there is no correlation; in other words,
the average of the product is the product of the averages.
Exercise 6.1: Prove that if P(a, b) factorizes, then the cor-
relation between a and b is zero.

6.2. CLASSICAL CORRELATION
159
Let me use an example to illustrate the kind of situation
that leads to factorized probabilities. Suppose that instead
of a single Charlie, there are two Charlies‚ÄîCharlie-A and
Charlie-B‚Äîwho have never communicated. Charlie-B mixes
up his two coins and gives one to Bob‚Äîthe other one is
discarded.
Charlie-A does exactly the same thing except that he
gives a coin to Alice instead. This is the type of situation that
leads to factorized product probabilities with no correlation.
In classical physics we use statistics and probability the-
ory when we are ignorant about something that is, in princi-
ple, knowable. For example, after mixing up the coins in the
Ô¨Årst experiment, Charlie could have made a gentle observa-
tion (a quick peek) and then let Alice and Bob have their
coins.
This would have made no diÔ¨Äerence in the result.
In classical mechanics, the probability distribution P(a, b)
represents an incomplete speciÔ¨Åcation of the system state.
There is more to know‚Äîmore that could be known‚Äîabout
the system.
In classical physics, the use of probability is
always associated with an incompleteness of knowledge rel-
ative to all that could be known.
A related point is that complete knowledge of a system in
classical physics implies complete knowledge of every part of
the system. It would not make any sense to say that Charlie
knew everything that could be known about the system of
two coins but was missing information about the individual
coins.
These classical concepts are deeply ingrained in our think-
ing. They are the foundation of our instinctual understand-
ing of the physical world, and it‚Äôs very hard to get past

160
LECTURE 6. ENTANGLEMENT
them. But get past them we must, if we are to understand
the quantum world.
6.3
Combining Quantum Systems
Charlie‚Äôs two coins formed a single classical system, com-
posed of two classical subsystems. Quantum mechanics also
allows us to combine systems, as we found out in the Math-
ematical Interlude on tensor products (Section 6.1).
Alice and Bob have kindly agreed to provide a variant
of the coin/die system they loaned us for the Interlude on
tensor products. Instead of a coin and a die, the new system
is built up from two spins, meaning that we‚Äôll have a chance
to put our knowledge of single spins to work.
As before, we will sometimes use the oddball notation |a

to remind us that Alice‚Äôs state-vectors are not in the same
state-space as Bob‚Äôs, and that we‚Äôre not allowed to add them
together. On the other hand, recall that each member of an
orthonormal basis for SAB is labeled by a pair of vectors, one
from SA and one from SB. We will make frequent use of the
notation |ab‚ü©to label a single basis vector of the combined
system. These doubly indexed basis vectors can be added
together, and we‚Äôll be doing that a lot.
As we explained in the Interlude, labeling a basis vector
with a pair of indices takes some getting used to. You should
think of the pair ab as a single index labeling a single state.
Let‚Äôs look at an example. Consider some linear operator
M acting on the space of states of the composite system. As
usual, it can be represented as a matrix. The matrix ele-
ments are constructed by sandwiching the operator between

6.4. TWO SPINS
161
basis vectors. Thus, the matrix elements of M are expressed
as
‚ü®a‚Ä≤b‚Ä≤|M|ab‚ü©= Ma‚Ä≤b‚Ä≤,ab.
Each row of the matrix is labeled with a single index (a‚Ä≤b‚Ä≤)
of the composite system and each column with (ab).
The vectors |ab‚ü©are taken to be orthonormal, which
means that their inner products are zero unless both labels
match. This does not mean that a matches b, but rather
that ab matches a‚Ä≤b‚Ä≤. We can also express this idea using the
Kronecker delta symbol:
‚ü®ab|a‚Ä≤b‚Ä≤‚ü©= Œ¥aa‚Ä≤Œ¥bb‚Ä≤.
The right side is zero unless a = a‚Ä≤ and b = b‚Ä≤. If the labels
do match, the inner product is one.
Now that we have the basis vectors, any linear superpo-
sition of them is allowed. Thus, any state in the composite
system can be expanded as
|Œ®‚ü©=

a,b
œà(a, b)|ab‚ü©.
6.4
Two Spins
Returning to our example, let‚Äôs imagine two spins: Alice‚Äôs
and Bob‚Äôs. To put it in a context that we can visualize, imag-
ine that the spins are attached to two particles and that the
two particles are Ô¨Åxed in space at two nearby but diÔ¨Äerent
locations.

162
LECTURE 6. ENTANGLEMENT
Alice and Bob each have their own apparatuses, called
A and B respectively, that they can use to prepare states
and measure spin components. Each can be independently
oriented along any axis.
We are going to need names for the two spins. When
we only had one spin, we simply called it œÉ, and it had
three components along the x, y, and z axes. Now we have
two spins, and the question is how to label them without
cluttering the symbols with too many sub- and superscripts.
We could call them œÉA and œÉB, and the components, œÉA
x , œÉB
y ,
and so on. For me, that‚Äôs just too many subscripts to keep
track of, especially on the blackboard. Instead, I‚Äôll follow the
same convention we used in the Interlude on tensor products.
I‚Äôll call Alice‚Äôs spin œÉ and assign the next letter in the Greek
alphabet, œÑ, to Bob‚Äôs spin. The full sets of components for
Alice‚Äôs and Bob‚Äôs spins are
œÉx, œÉy, œÉz
and
œÑx, œÑy, œÑz.
According to the principles that we laid out earlier, the space
of states for the two-spin system is a tensor product. We can
make a table of the four states, just as we did in the Interlude.
This time, it‚Äôs a 2 √ó 2 square, comprising four basis states.
Let‚Äôs work in a basis in which the z components of both
spins are speciÔ¨Åed. The basis vectors are

6.5. PRODUCT STATES
163
|uu‚ü©, |ud‚ü©, |du‚ü©, |dd‚ü©,
where the Ô¨Årst part of each label represents the state of œÉ,
and the second part represents œÑ. For example, the Ô¨Årst basis
vector |uu‚ü©represents the state in which both spins are up.
The vector |du‚ü©is the state in which Alice‚Äôs spin is down
and Bob‚Äôs spin is up.
6.5
Product States
The simplest type of state for the composite system is called
a product state. A product state is the result of completely
independent preparations by Alice and Bob, in which each
uses his or her own apparatus to prepare a spin. Using ex-
plicit notation, suppose Alice prepares her spin in state
Œ±u|u

+ Œ±d|d

and Bob prepares his in the state
Œ≤u|u‚ü©+ Œ≤d|d‚ü©.
We assume each state is normalized:
Œ±‚àó
uŒ±u + Œ±‚àó
dŒ±d
=
1
Œ≤‚àó
uŒ≤u + Œ≤‚àó
dŒ≤d
=
1.
(6.4)

164
LECTURE 6. ENTANGLEMENT
And in fact these separate normalization equations for each
subsystem play a crucial role in deÔ¨Åning product states. If
they did not hold, we would not have a product state. The
product state describing the combined system is
|product state‚ü©=

Œ±u|u

+ Œ±d|d

‚äó

Œ≤u|u

+ Œ≤d|d

,
where the Ô¨Årst factor represents Alice‚Äôs state and the second
factor represents Bob‚Äôs. Expanding the product and switch-
ing to composite notation, the right-hand side becomes
Œ±uŒ≤u|uu‚ü©+ Œ±uŒ≤d|ud‚ü©+ Œ±dŒ≤u|du‚ü©+ Œ±dŒ≤d|dd‚ü©.
(6.5)
The main feature of a product state is that each subsystem
behaves independently of the other. If Bob does an exper-
iment on his own subsystem, the result is exactly the same
as it would be if Alice‚Äôs subsystem did not exist. The same
is true for Alice, of course.
Exercise 6.2: Show that if the two normalization conditions
of Eqs. 6.4 are satisÔ¨Åed, then the state-vector of Eq. 6.5 is
automatically normalized as well. In other words, show that
for this product state, normalizing the overall state-vector
does not put any additional constraints on the Œ±‚Äôs and Œ≤‚Äôs.
I‚Äôll mention here that tensor products and product states
are two diÔ¨Äerent things, despite their similar-sounding names.2
2Sometimes, we‚Äôll use the term tensor product space, or just product
space, instead of tensor product.

6.6. PRODUCT STATE PARAMETERS
165
A tensor product is a vector space for studying composite
systems. A product state is a state-vector. It‚Äôs one of the
many state-vectors that inhabit a product space. As we will
see, most of the state-vectors in the product space are not
product states.
6.6
Counting Parameters for the
Product State
Let‚Äôs consider the number of parameters it takes to specify
such a product state. Each factor requires two complex num-
bers (Œ±u and Œ±d for Alice, Œ≤u and Œ≤d for Bob), which means
we need four complex numbers altogether. That‚Äôs equivalent
to eight real parameters. But recall that the normalization
conditions in Eqs. 6.4 reduce this by two. Furthermore, the
overall phases of each state have no physical signiÔ¨Åcance, so
the total number of real parameters is four. That‚Äôs hardly
surprising: it took two parameters to describe the state of a
single spin, so two independent spins require four.
6.7
Entangled States
The principles of quantum mechanics allow us to superpose
basis vectors in more general ways than just product states.
The most general vector in the composite space of states is
œàuu|uu‚ü©+ œàud|ud‚ü©+ œàdu|du‚ü©+ œàdd|dd‚ü©,
where we have used the subscripted symbols œà (instead of

166
LECTURE 6. ENTANGLEMENT
Œ± and Œ≤) to represent the complex coeÔ¨Écients. Again, we
have four complex numbers, but this time we only have one
normalization condition,
œà‚àó
uuœàuu + œà‚àó
udœàud + œà‚àó
duœàdu + œà‚àó
ddœàdd = 1,
and only one overall phase to ignore.
The result is that
the most general state for a two-spin system has six real
parameters. Evidently, the space of states is richer than just
those product states that can be prepared independently by
Bob and Alice. Something new is going on. The new thing
is called entanglement.
Entanglement is not an all-or-nothing proposition. Some
states are more entangled than others. Here is an example of
a maximally entangled state‚Äîa state that‚Äôs as entangled as
it can be. It is called the singlet state, and it can be written
as
|sing‚ü©= 1
‚àö
2

|ud‚ü©‚àí|du‚ü©

.
The singlet state cannot be written as a product state. The
same is true for the triplet states,
1
‚àö
2

|ud‚ü©+ |du‚ü©

1
‚àö
2

|uu‚ü©+ |dd‚ü©

1
‚àö
2

|uu‚ü©‚àí|dd‚ü©

,

6.8. ALICE AND BOB‚ÄôS OBSERVABLES
167
which are also maximally entangled. The reason for calling
them singlet and triplet will be explained later.
Exercise 6.3: Prove that the state |sing‚ü©cannot be written
as a product state.
What is it about maximally entangled states that is so fas-
cinating? I can sum this up in two statements:
‚Ä¢ An entangled state is a complete description of the
combined system. No more can be known about it.
‚Ä¢ In a maximally entangled state, nothing is known about
the individual subsystems.
How can that be? How could we know as much as can pos-
sibly be known about the Alice-Bob system of two spins, and
yet know nothing about the individual spins that are its sub-
components?
That‚Äôs the mystery of entanglement, and I
hope that by the end of this lecture you will understand the
rules of the game, even if the deeper nature of entanglement
remains a paradox.
6.8
Alice and Bob‚Äôs Observables
So far, we‚Äôve discussed the space of states of the Alice-Bob
two-spin system, but not its observables. Some of these ob-
servables are obvious, even if their mathematical representa-
tion is not. In particular, using their apparatuses A and B,
Alice and Bob can measure the components of their spins:

168
LECTURE 6. ENTANGLEMENT
œÉx, œÉy, œÉz
and
œÑx, œÑy, œÑz.
How are these observables represented as Hermitian opera-
tors in the composite space of states? The answer is sim-
ple. Bob‚Äôs operators act on Bob‚Äôs spin states exactly as they
would if Alice had never shown up. The same goes for Alice.
Let‚Äôs review how the spin operators act on the states of a
single spin. First, let‚Äôs look at Alice‚Äôs spin:
œÉz|u

=
|u

œÉz|d

=
‚àí|d

œÉx|u

=
|d

œÉx|d

=
|u

œÉy|u

=
i|d

œÉy|d

=
‚àíi|u

.
(6.6)
Of course, Bob‚Äôs setup is identical to Alice‚Äôs, so we can write
a parallel set of equations showing how the components of œÑ
act on Bob‚Äôs states:

6.8. ALICE AND BOB‚ÄôS OBSERVABLES
169
œÑz|u‚ü©
=
|u‚ü©
œÑz|d‚ü©
=
‚àí|d‚ü©
œÑx|u‚ü©
=
|d‚ü©
œÑx|d‚ü©
=
|u‚ü©
œÑy|u‚ü©
=
i|d‚ü©
œÑy|d‚ü©
=
‚àíi|u‚ü©.
(6.7)
Now let‚Äôs consider how the operators should be deÔ¨Åned when
acting on the tensor product states, |uu‚ü©, |ud‚ü©, |du‚ü©, and |dd‚ü©.
The answer is that when œÉ acts, it just ignores Bob‚Äôs half
of the state label. There are many possible combinations
of operators and states, but I will pick a few at random.
You can Ô¨Åll in the others, or look them up in the appendix.
Starting with Alice‚Äôs operators, we Ô¨Ånd that
œÉz|uu‚ü©
=
|uu‚ü©
œÉz|du‚ü©
=
‚àí|du‚ü©
œÉx|ud‚ü©
=
|dd‚ü©
œÉx|dd‚ü©
=
|ud‚ü©

170
LECTURE 6. ENTANGLEMENT
œÉy|uu‚ü©
=
i|du‚ü©
œÉy|du‚ü©
=
‚àíi|uu‚ü©
œÑz|uu‚ü©
=
|uu‚ü©
œÑz|du‚ü©
=
|du‚ü©
œÑx|ud‚ü©
=
|uu‚ü©
œÑx|du‚ü©
=
|dd‚ü©
œÑy|uu‚ü©
=
i|ud‚ü©
œÑy|dd‚ü©
=
‚àíi|du‚ü©.
(6.8)
Again, the rule is that Alice‚Äôs spin components act only on
the Alice half of the composite system.
The Bob half is
a passive spectator that does not participate. In terms of
symbols, when œÉx, œÉy, or œÉz acts, Bob‚Äôs half of the spin state
does not change.
And when Bob‚Äôs œÑ spin operators act,
Alice‚Äôs half is similarly passive.
We are being a little loose with our notation. The vectors
of a tensor product space are new vectors, built up from the
vectors of two smaller spaces. Technically, the same is true
for the operators. If we were being pedantic, we would insist
on writing the tensor product versions of œÉz and œÑx as œÉz ‚äóI
and I ‚äóœÑx, respectively, where I is the identity operator.
In fact, we can highlight two important properties of tensor

6.8. ALICE AND BOB‚ÄôS OBSERVABLES
171
product operators by rewriting the equation
œÉz|du‚ü©= ‚àí|du‚ü©
(6.9)
as
(œÉz ‚äóI) (|d‚ü©‚äó|u‚ü©)
=
(œÉz|d‚ü©‚äóI|u‚ü©)
=
(‚àí|d‚ü©‚äó|u‚ü©).
(6.10)
This notation is cumbersome, and we‚Äôll usually stick to the
simpler language of Eq. 6.9. However, the language of Eq.
6.10 makes two things clear:
1. A composite operator œÉz ‚äóI is operating on a compos-
ite vector |d‚ü©‚äó|u‚ü©to produce a new composite vector
‚àí|d‚ü©‚äó|u‚ü©.
2. Alice‚Äôs half (the left half) of the composite operator
only aÔ¨Äects her half of the composite vector. Likewise,
Bob‚Äôs half of the operator only aÔ¨Äects his half of the
vector.
We‚Äôll have more to say about composite operators in the
next section. Furthermore, in Lecture 7, the language of Eq.
6.10 will help us see how to work with tensor products in
component form.

172
LECTURE 6. ENTANGLEMENT
Exercise 6.4: Use the matrix forms of œÉz, œÉx, and œÉy and
the column vectors for |u

and |d

to verify Eqs. 6.6. Then,
use Eqs. 6.6 and 6.7 to write the equations that were left out
of Eqs. 6.8. Use the appendix to check your answers.
Exercise 6.5: Prove the following theorem:
When any one of Alice‚Äôs or Bob‚Äôs spin operators acts on a
product state, the result is still a product state.
Show that in a product state, the expectation value of any
component of ‚ÉóœÉ or ‚ÉóœÑ is exactly the same as it would be in
the individual single-spin states.
This last exercise proves something important about product
states. In a product state, every prediction about Bob‚Äôs half
of the system is exactly the same as it would have been in the
corresponding single-spin theory. The same goes for Alice.
An example of this property of product states involves
what I called the Spin-Polarization Principle in Lecture 3.
A useful way to state that principle is:
For any state of a single spin, there is some direction for
which the spin is +1.
As I explained, this means that the expectation values of the
components satisfy the equation
‚ü®œÉx‚ü©2 + ‚ü®œÉy‚ü©2 + ‚ü®œÉz‚ü©2 = 1,
(6.11)
which tells us that not all the expectation values can be zero.
This fact continues to hold for all product states. However,

6.8. ALICE AND BOB‚ÄôS OBSERVABLES
173
it does not hold for the entangled state |sing‚ü©. In fact, for
the |sing‚ü©state the right-hand side of Eq. 6.11 becomes zero,
as we‚Äôll show next.
Recall that the entangled state |sing‚ü©is deÔ¨Åned as
|sing‚ü©= 1
‚àö
2

|ud‚ü©‚àí|du‚ü©

.
Let‚Äôs look at the expectation values of œÉ in this state. We
have all the machinery we need to compute them. First, let‚Äôs
consider ‚ü®œÉz‚ü©:
‚ü®œÉz‚ü©= ‚ü®sing|œÉz|sing‚ü©
=
‚ü®sing|œÉz
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

.
Here is where Eqs. 6.8 come in (along with Exercise 6.4,
which completes this set of equations!). They tell us how œÉz
acts on each basis vector. The result is
‚ü®sing|œÉz|sing‚ü©= ‚ü®sing| 1
‚àö
2

|ud‚ü©+ |du‚ü©

or
‚ü®œÉz‚ü©= 1
2

‚ü®ud| ‚àí‚ü®du|

|ud‚ü©+ |du‚ü©

.
A quick inspection shows that this is equal to zero. Next,
let‚Äôs consider ‚ü®œÉx‚ü©:

174
LECTURE 6. ENTANGLEMENT
‚ü®œÉx‚ü©= ‚ü®sing|œÉx|sing‚ü©
=
‚ü®sing|œÉx
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

or
‚ü®œÉx‚ü©= 1
2

‚ü®ud| ‚àí‚ü®du|

|dd‚ü©‚àí|uu‚ü©

.
Again, this equation gives us zero. Finally, let‚Äôs look at ‚ü®œÉy‚ü©:
‚ü®œÉy‚ü©= ‚ü®sing|œÉy|sing‚ü©
=
1
2

‚ü®ud| ‚àí‚ü®du|

i|dd‚ü©+ i|uu‚ü©

.
As you may have guessed, we are left with zero once more.
Thus, we have shown that for the state |sing‚ü©,
‚ü®œÉz‚ü©= ‚ü®œÉx‚ü©= ‚ü®œÉy‚ü©= 0,
and indeed all expectation values of œÉ are zero. Needless to
say, the same is true for the expectation values of œÑ. Clearly,
|sing‚ü©is very diÔ¨Äerent from a product state. What does all
this say about the measurements we can make?
If the expectation value of a component of œÉ is zero, it
means that the experimental outcome is equally likely to be

6.9. COMPOSITE OBSERVABLES
175
+1 or ‚àí1. In other words, the outcome is completely uncer-
tain. Even though we know the exact state-vector, |sing‚ü©, we
know nothing at all about the outcome of any measurement
of any component of either spin.
Perhaps this means that the state |sing‚ü©is somehow
incomplete‚Äîthat there are details of the system that we were
sloppy about and didn‚Äôt measure. After all, earlier we saw
a perfectly classical example in which Alice and Bob knew
nothing about their coins until they actually looked at them.
How is the quantum version diÔ¨Äerent?
In our ‚Äúclassical entanglement‚Äù example involving Alice,
Bob, and Charlie, it is perfectly clear that there was more
to know. Charlie could have sneaked a peek at the coins
without changing anything, because classical measurements
can be arbitrarily gentle.
Might there be so-called hidden variables in the quantum
system? The answer is that according to the rules of quan-
tum mechanics, there is nothing to know beyond what is
encoded in the state-vector‚Äîin the present case, |sing‚ü©. The
state-vector is as complete a description of a system as it is
possible to make. So it seems that in quantum mechanics, we
can know everything about a composite system‚Äîeverything
there is to know, anyway‚Äîand still know nothing about its
constituent parts.
This is the true weirdness of entangle-
ment, which so disturbed Einstein.
6.9
Composite Observables
Let‚Äôs imagine a quantum mechanical Alice-Bob-Charlie setup.
Charlie‚Äôs role is to prepare two spins in the entangled state

176
LECTURE 6. ENTANGLEMENT
|sing‚ü©. Then, without looking at the spins (remember, quan-
tum measurements are not gentle), he gives one spin to Alice
and one to Bob. Although Alice and Bob know exactly what
state the combined system is in, they can predict nothing
about the outcome of their individual measurements.
But surely knowing the exact state of the composite sys-
tem must tell them something, even if the state is highly
entangled.
And in fact it does.
However, to understand
what it tells them, we have to consider a wider family of
observables than the ones that Alice and Bob can measure
separately, each using only his or her own detector. As it
turns out, there are observables that can only be measured
by using both detectors. The results of such experiments
can only be known to Alice or Bob if they come together
and compare notes.
The Ô¨Årst question is whether Alice and Bob can simulta-
neously measure their own observables. We have seen that
there are quantities that cannot be simultaneously measured.
In particular, two observables that do not commute cannot
both be measured without the measurements interfering with
each other. But for Alice and Bob, it is easy to see that ev-
ery component of œÉ commutes with every component of œÑ.
This is a general fact about tensor products. The operators
that act on the two separate factors commute with one an-
other. Therefore, Alice can make any measurement on her
spin and Bob can make any measurement on his, without
either interfering with the other‚Äôs experiment.
Let‚Äôs suppose Alice measures œÉz and Bob measures œÑz,
and then they multiply the results.
In other words, they
conspire to measure the product œÑzœÉz.

6.9. COMPOSITE OBSERVABLES
177
The product œÑzœÉz is an observable that is mathematically
represented by Ô¨Årst applying œÉz to a ket and then subse-
quently applying œÑz. Keep in mind that these are just the
mathematical operations that deÔ¨Åne a new operator: they
are diÔ¨Äerent from the act of performing a physical measure-
ment.
You don‚Äôt need an apparatus to multiply two op-
erators; you just need a pencil and paper. Let‚Äôs see what
happens if we apply the product œÑzœÉz to the state |sing‚ü©:
œÑzœÉz
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

.
First, using the table in Eqs. 6.8, apply œÉz:
œÑzœÉz
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

= œÑz
1
‚àö
2

|ud‚ü©+ |du‚ü©

.
Now, apply œÑz to get
œÑzœÉz
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

= 1
‚àö
2

‚àí|ud‚ü©+ |du‚ü©

.
Notice that the end result is just to change the sign of |sing‚ü©:
œÑzœÉz|sing‚ü©= ‚àí|sing‚ü©.
Evidently, |sing‚ü©is an eigenvector of the observable œÑzœÉz
with eigenvalue ‚àí1. Let‚Äôs examine the signiÔ¨Åcance of this
result. Alice measures œÉz and Bob measures œÑz; when they
come together and compare results, they Ô¨Ånd they‚Äôve mea-
sured opposite values.
Sometimes, Bob measures +1 and

178
LECTURE 6. ENTANGLEMENT
Alice measures ‚àí1.
Other times, Alice measures +1 and
Bob measures ‚àí1. The product of the two measurements is
always ‚àí1.
There should be nothing surprising in this result. The
state-vector |sing‚ü©is a superposition of two vectors, |ud‚ü©
and |du‚ü©, both of which comprise two spins with opposite
z components.
The situation is altogether similar to the
classical example involving Charlie and his two coins.
But now we come to something that has no classical ana-
log. Suppose that instead of measuring the z components of
their spins, Alice and Bob measure the x components. To
Ô¨Ånd out how their outcomes are correlated, we must study
the observable œÑxœÉx.
Let‚Äôs act on |sing‚ü©with this product. Here are the steps:
œÑxœÉx|sing‚ü©
=
œÑxœÉx
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

=
œÑx
1
‚àö
2

|dd‚ü©‚àí|uu‚ü©

=
1
‚àö
2

|du‚ü©‚àí|ud‚ü©

or, more simply,
œÑxœÉx|sing‚ü©= ‚àí|sing‚ü©.
Now this is a bit surprising: |sing‚ü©is also an eigenvector
of œÑxœÉx with eigenvalue ‚àí1. It is far less obvious from just
looking at |sing‚ü©that the x components of the two spins

6.9. COMPOSITE OBSERVABLES
179
are always opposite. Nevertheless, every time Alice and Bob
measure them, they Ô¨Ånd that œÉx and œÑx have opposite values.
At this point, you will probably not be surprised to learn that
the same thing is true for the y components.
Exercise 6.6: Assume Charlie has prepared the two spins
in the singlet state. This time, Bob measures œÑy and Alice
measures œÉx. What is the expectation value of œÉxœÑy?
What does this say about the correlation between the two
measurements?
Exercise 6.7: Next, Charlie prepares the spins in a diÔ¨Äerent
state, called |T1‚ü©, where
|T1‚ü©= 1
‚àö
2

|ud‚ü©+ |du‚ü©

.
In these examples, T stands for triplet. These triplet states
are completely diÔ¨Äerent from the states in the coin and die
examples. What are the expectation values of the operators
œÉzœÑz, œÉxœÑx, and œÉyœÑy?
What a diÔ¨Äerence a sign can make!
Exercise 6.8:
Do the same for the other two entangled
triplet states,
|T2‚ü©= 1
‚àö
2

|uu‚ü©+ |dd‚ü©

|T3‚ü©= 1
‚àö
2

|uu‚ü©‚àí|dd‚ü©

,
and interpret.

180
LECTURE 6. ENTANGLEMENT
Finally, let‚Äôs consider one more observable. This one can-
not be measured by Alice and Bob making separate measure-
ments with their individual apparatuses, even if they come
together and compare notes.
Nevertheless, quantum me-
chanics insists that some kind of apparatus can be built to
measure the observable.
The observable I am referring to can be thought of as the
ordinary dot product of the vector-operators ‚ÉóœÉ and ‚ÉóœÑ:
‚ÉóœÉ ¬∑ ‚ÉóœÑ = œÉxœÑx + œÉyœÑy + œÉzœÑz.
One might think that a value for this observable can be found
if Bob measures all components of œÑ, while Alice measures all
components of œÉ; then they could multiply the components
and add them up. The problem is that Bob cannot simul-
taneously measure the individual components of œÑ, because
they don‚Äôt commute. Likewise, Alice cannot measure more
than one component of œÉ at a time. To measure ‚ÉóœÉ ¬∑‚ÉóœÑ, a new
kind of apparatus must be built, one that measures ‚ÉóœÉ¬∑‚ÉóœÑ with-
out measuring any individual component. It‚Äôs far from ob-
vious how that could be done. Here is a concrete example of
how such a measurement could be carried out: Some atoms
have spins that are described in the same way as electron
spins. When two of these atoms are close to each other‚Äî
for example, two neighboring atoms in a crystal lattice‚Äîthe
Hamiltonian will depend on the spins. In some situations,
the neighboring spins‚Äô Hamiltonian is proportional to ‚ÉóœÉ ¬∑ ‚ÉóœÑ.
If that happens to be the case, then measuring ‚ÉóœÉ ¬∑‚ÉóœÑ is equiv-
alent to measuring the energy of the atomic pair. Measuring
this energy is a single measurement of the composite opera-

6.9. COMPOSITE OBSERVABLES
181
tor and does not entail measuring the individual components
of either spin.
Exercise 6.9: Prove that the four vectors |sing‚ü©, |T1‚ü©, |T2‚ü©,
and |T3‚ü©are eigenvectors of ‚ÉóœÉ¬∑‚ÉóœÑ. What are their eigenvalues?
Take a look at your results from this last exercise. Do
you see why one of these state-vectors is called the singlet,
while the other three are called triplets? The reason is that
if you look at their relation to the operator ‚ÉóœÉ ¬∑ ‚ÉóœÑ, the singlet
is an eigenvector with one eigenvalue, and the triplets are all
eigenvectors with a diÔ¨Äerent degenerate eigenvalue.
Here is a good exercise that combines the concept of en-
tanglement with the concepts of time and change from Lec-
ture 4. Use it to review the ideas of unitary time evolution
and the meaning of the Hamiltonian.
Exercise 6.10: A system of two spins has the Hamiltonian
H = œâ
2 ‚ÉóœÉ ¬∑ ‚ÉóœÑ.
What are the possible energies of the system, and what are
the eigenvectors of the Hamiltonian?
Suppose the system starts in the state |uu‚ü©. What is the
state at any later time? Answer the same question for initial
states of |ud‚ü©, |du‚ü©, and |dd‚ü©.


Lecture 7
More on Entanglement
Hilbert‚Äôs Place, summer 1935:
Two scruÔ¨Äy regulars come through the swinging doors, in the
midst of an intense conversation. The one with the wild gray-
ish hair and frayed sweater says, ‚ÄúNo, I will not accept your
theory unless you can tell me what the elements of physical
reality are.‚Äù
The other one looks around, throws up his hands in obvi-
ous frustration, and says to Art and Lenny, ‚ÄúThere he goes
again. Elements of physical reality, EPRs, EPRs, that‚Äôs all
he ever thinks about. Albert, stop being obsessive and just
accept the facts.‚Äù
‚ÄúNever! I cannot accept that one can know everything there
is to know about a thing, and still know nothing about its
parts. That‚Äôs utter nonsense, Niels.‚Äù
‚ÄúSorry, Albert. That‚Äôs just the way it is. Here, let me buy
you a beer.‚Äù
183

184
LECTURE 7. MORE ON ENTANGLEMENT
In this lecture, we will look at entanglement in greater depth.
To do that, we‚Äôll need some additional mathematical tools.
First, we‚Äôll Ô¨Ånd out how to work with tensor products in
component form. Then, we‚Äôll learn about a new operator
called the density matrix.
These tools are not inherently
hard to master, but they do require some patience and a fair
amount of index wrangling.
7.1
Mathematical Interlude:
Tensor Products in
Component Form
In Lecture 6, we explained how to form the tensor product of
two vector spaces using the abstract notation of bras, kets,
and operator symbols like œÉz. How does that translate into
columns, rows, and matrices?
Building tensor products from matrices and column vec-
tors is not hard. The rules are straightforward, as we‚Äôll see
below.
The tricky part is understanding why these rules
work‚Äîwhy they allow us to build matrices and column vec-
tors that have the properties we want. We‚Äôll tackle the issue
in two diÔ¨Äerent ways. First, we‚Äôll build composite operators
using the tried-and-true method we developed in Lecture 3.
Then we‚Äôll show you how to build composite operators di-
rectly from their component operators.

7.1. INTERLUDE: TENSOR PRODUCT MATRICES 185
7.1.1
Building Tensor Product Matrices
from Basic Principles
Back in Lecture 3, we showed you how to write any observ-
able M in matrix form, relative to a speciÔ¨Åc basis. Take a
moment to review Eqs. 3.1 through 3.4. In that section, we
calculated the numerical values mjk of M‚Äôs matrix elements
with the expression
mjk = ‚ü®j|M|k‚ü©,
(7.1)
where |j‚ü©and |k‚ü©represent the basis vectors. Each |j‚ü©, |k‚ü©
combination generates a diÔ¨Äerent matrix element.1
Our plan is to apply this formula to some tensor prod-
uct operators and see what we get. Because of our double-
indexing convention for tensor product basis vectors, the
‚Äúsandwiches‚Äù in these equations will look a little diÔ¨Äerent
from the ones in Eq. 7.1. On each end of the sandwich, we
will cycle through the basis vectors |uu‚ü©, |ud‚ü©, |du‚ü©, and |dd‚ü©.2
To keep things simple, we‚Äôll use the operator œÉz ‚äóI as an
example, where I is the identity operator. As we have seen,
œÉz ‚äóI acts on Alice‚Äôs half of the state-vector with œÉz, and
does absolutely nothing to Bob‚Äôs half. Because we are work-
ing in a four-dimensional vector space, the resulting matrix
1In Lecture 3, we happened to write the index j on the left side of
M, and k on the right, the opposite of what we‚Äôre doing here. Because
j and k are index variables, this makes no diÔ¨Äerence as long as we
maintain consistency within a group of equations.
2Of course, we could have used a diÔ¨Äerent set of basis vectors, such
as |rr‚ü©, |rl‚ü©, etc. Doing so would result in a diÔ¨Äerent set of matrix
elements.

186
LECTURE 7. MORE ON ENTANGLEMENT
will be 4 √ó 4. Omitting multiple ‚äósymbols to avoid visual
clutter, we can write the matrix like this:
œÉz ‚äóI =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚ü®uu|œÉzI|uu‚ü©
‚ü®uu|œÉzI|ud‚ü©
‚ü®uu|œÉzI|du‚ü©
‚ü®uu|œÉzI|dd‚ü©
‚ü®ud|œÉzI|uu‚ü©
‚ü®ud|œÉzI|ud‚ü©
‚ü®ud|œÉzI|du‚ü©
‚ü®ud|œÉzI|dd‚ü©
‚ü®du|œÉzI|uu‚ü©
‚ü®du|œÉzI|ud‚ü©
‚ü®du|œÉzI|du‚ü©
‚ü®du|œÉzI|dd‚ü©
‚ü®dd|œÉzI|uu‚ü©
‚ü®dd|œÉzI|ud‚ü©
‚ü®dd|œÉzI|du‚ü©
‚ü®dd|œÉzI|dd‚ü©
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(7.2)
To evaluate these matrix elements, we could allow œÉz and I
to operate either to the left or to the right. Let‚Äôs assume
œÉz operates to the left and I operates to the right. Since
I does nothing, all we care about is what œÉz does to the
bra vector on its left. And within that bra vector, œÉz only
acts on the leftmost (that is, Alice‚Äôs) state-label. Using the
rules we‚Äôve already worked out (see Eqs. 6.6 and 6.7), we
can carry out all of these œÉz operations to obtain a matrix
of inner products:
œÉz ‚äóI =
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
‚ü®uu|uu‚ü©
‚ü®uu|ud‚ü©
‚ü®uu|du‚ü©
‚ü®uu|dd‚ü©
‚ü®ud|uu‚ü©
‚ü®ud|ud‚ü©
‚ü®ud|du‚ü©
‚ü®ud|dd‚ü©
‚àí‚ü®du|uu‚ü©
‚àí‚ü®du|ud‚ü©
‚àí‚ü®du|du‚ü©
‚àí‚ü®du|dd‚ü©
‚àí‚ü®dd|uu‚ü©
‚àí‚ü®dd|ud‚ü©
‚àí‚ü®dd|du‚ü©
‚àí‚ü®dd|dd‚ü©
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
.
(7.3)

7.1. INTERLUDE: TENSOR PRODUCT MATRICES 187
Because these eigenvectors are orthonormal, the matrix re-
duces to
œÉz ‚äóI =
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†.
(7.4)
How do we write the eigenvectors |uu‚ü©, |ud‚ü©, |du‚ü©, and |dd‚ü©
as column vectors?
For now, I‚Äôll just tell you that we‚Äôll
represent |uu‚ü©and |du‚ü©as
|uu‚ü©=
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†,
|du‚ü©=
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
‚éû
‚éü
‚éü
‚é†.
(7.5)
Let‚Äôs see what happens when œÉz‚äóI operates on these column
vectors. Applying the matrix to |uu‚ü©results in
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
0
1
0
0
0
0
‚àí1
0
0
0
0
‚àí1
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†.
In other words,
(œÉz ‚äóI)|uu‚ü©= |uu‚ü©,
just as we expect. What if we apply the same matrix to the
column vector |du‚ü©in Eqs. 7.5? Carrying out the matrix
multiplication results in ‚àí|du‚ü©, just as it should.

188
LECTURE 7. MORE ON ENTANGLEMENT
7.1.2
Building Tensor Product Matrices
from Component Matrices
The above method for calculating matrix elements is very
general‚Äîit works for all observables. If we need to construct
the tensor product of two operators, and we already know the
matrix elements of the building blocks, we can combine them
directly. Here is the rule for combining 2√ó2 matrices to form
4 √ó 4 matrices:
A ‚äóB =
 A11B
A12B
A21B
A22B

(7.6)
or
A ‚äóB =
‚éõ
‚éú
‚éú
‚éù
A11B11
A11B12
A12B11
A12B12
A11B21
A11B22
A12B21
A12B22
A21B11
A21B12
A22B11
A22B12
A21B21
A21B22
A22B21
A22B22
‚éû
‚éü
‚éü
‚é†.
(7.7)
The same pattern works for matrices of any size. This kind
of matrix multiplication is sometimes called the Kronecker
product, a term that only applies to matrices‚Äîit‚Äôs the matrix
version of the tensor product. The Kronecker product of two
2√ó2 matrices is a 4√ó4 matrix, and the pattern is similar for
matrices of arbitrary size. In general, the Kronecker product
of an m√ón matrix and a p√óq matrix is an mp√ónq matrix.
All of this applies perfectly well to column and row vec-
tors, which are just specialized matrices. The tensor product
of two 2 √ó 1 column vectors is a 4 √ó 1 column vector. If a
and b are 2 √ó 1 column vectors, their tensor product looks
like this:

7.1. INTERLUDE: TENSOR PRODUCT MATRICES 189
 a11
a21

‚äó
 b11
b21

=
‚éõ
‚éú
‚éú
‚éù
a11b11
a11b21
a21b11
a21b21
‚éû
‚éü
‚éü
‚é†.
(7.8)
Let‚Äôs see how this works out for Alice and Bob. First, we‚Äôll
construct the four tensor product basis vectors, using |u‚ü©
and |d‚ü©as building blocks. Recall Eqs. 2.11 and 2.12 from
Lecture 2,
|u‚ü©=
 1
0

|d‚ü©=
 0
1

.
If we plug the appropriate combinations of |u‚ü©and |d‚ü©into
Eq. 7.8, our four 4 √ó 1 column vectors are
|uu‚ü©=
 1
0

‚äó
 1
0

=
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†
|ud‚ü©=
 1
0

‚äó
 0
1

=
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
‚éû
‚éü
‚éü
‚é†
|du‚ü©=
 0
1

‚äó
 1
0

=
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
‚éû
‚éü
‚éü
‚é†

190
LECTURE 7. MORE ON ENTANGLEMENT
|dd‚ü©=
 0
1

‚äó
 0
1

=
‚éõ
‚éú
‚éú
‚éù
0
0
0
1
‚éû
‚éü
‚éü
‚é†.
(7.9)
Next, we‚Äôll use the rule from Eq. 7.7 to combine the operators
œÉz and œÑx. Using Eqs. 3.20 to deÔ¨Åne matrices œÉz and œÑx, this
rule gives the tensor product matrix
œÉz ‚äóœÑx =
 1
0
0
‚àí1

‚äó
 0
1
1
0

=
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
1
0
0
0
0
0
0
‚àí1
0
0
‚àí1
0
‚éû
‚éü
‚éü
‚é†.
Let‚Äôs compare this result with the product of œÉx and œÑz,
œÉx ‚äóœÑz =
 0
1
1
0

‚äó
 1
0
0
‚àí1

=
‚éõ
‚éú
‚éú
‚éù
0
0
1
0
0
0
0
‚àí1
1
0
0
0
0
‚àí1
0
0
‚éû
‚éü
‚éü
‚é†.
Notice that œÉx‚äóœÑz is not the same as œÉz‚äóœÑx. That is natural,
because they represent diÔ¨Äerent observables.
So far, so good. But next, we‚Äôll see something a little
more interesting. With the help of a few exercises, we‚Äôll try to
convince you that the Kronecker product really is the tensor
product for matrices‚Äîin other words, that Alice‚Äôs half of the
matrix only aÔ¨Äects her half of the column vector, and likewise
for Bob. This is tricky because of the way the Kronecker
product mixes up the elements of its building blocks.

7.1. INTERLUDE: TENSOR PRODUCT MATRICES 191
As an example, let‚Äôs look at how œÉz ‚äóœÑx acts on |ud‚ü©.
Translating the abstract symbols into components, we can
write
(œÉz ‚äóœÑx)|ud‚ü©=
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
1
0
0
0
0
0
0
‚àí1
0
0
‚àí1
0
‚éû
‚éü
‚éü
‚é†
‚éõ
‚éú
‚éú
‚éù
0
1
0
0
‚éû
‚éü
‚éü
‚é†=
‚éõ
‚éú
‚éú
‚éù
1
0
0
0
‚éû
‚éü
‚éü
‚é†.
But the column vector on the right-hand side corresponds
to |uu‚ü©in Eqs. 7.9. Translated back into abstract notation,
this becomes
(œÉz ‚äóœÑx)|ud‚ü©= |uu‚ü©.
This is exactly what we want‚Äîa matrix representation of
our abstract operators and state-vectors that replicates their
known behavior.
The following exercise will help crystallize the idea that
the œÉ-half of œÉ‚äóœÑ only aÔ¨Äects Alice‚Äôs half of the state-vector,
and that the œÑ-half only aÔ¨Äects Bob‚Äôs. The one after that
provides some practice working out the matrix elements of an
operator, assuming that we already know what the operator
does to each basis vector.
Exercise 7.1:
Write the tensor product I ‚äóœÑx as a matrix,
and apply that matrix to each of the |uu‚ü©, |ud‚ü©, |du‚ü©, and
|dd‚ü©column vectors.
Show that Alice‚Äôs half of the state-
vector is unchanged in each case. Recall that I is the 2 √ó 2
unit matrix.
Exercise 7.2:
Calculate the matrix elements of œÉz ‚äóœÑx by
forming inner products as we did in Eq. 7.2.

192
LECTURE 7. MORE ON ENTANGLEMENT
The third exercise is a bit tedious, but it really nails things
down. Consider the equation
(A ‚äóB) (a ‚äób) = (Aa ‚äóBb).
(7.10)
As in Eqs. 7.7 and 7.8, A and B represent 2 √ó 2 matrices (or
operators), and a and b represent 2 √ó 1 column vectors. The
exercise asks you to expand the equation into components
and show that the left side matches the right side.
Exercise 7.3:
a) Rewrite Eq. 7.10 in component form, replacing the sym-
bols A, B, a, and b with the matrices and column vectors
from Eqs. 7.7 and 7.8.
b) Perform the matrix multiplications Aa and Bb on the
right-hand side. Verify that each result is a 4 √ó 1 matrix.
c) Expand all three Kronecker products.
d) Verify the row and column sizes of each Kronecker prod-
uct:
‚Ä¢ A ‚äóB: 4 √ó 4
‚Ä¢ a ‚äób: 4 √ó 1
‚Ä¢ Aa ‚äóBb: 4 √ó 4
e) Perform the matrix multiplication on the left-hand side,
resulting in a 4 √ó 1 column vector. Each row should be the
sum of four separate terms.
f) Finally, verify that the resulting column vectors on the
left and right sides are identical.

7.2. INTERLUDE: OUTER PRODUCTS
193
7.2
Mathematical Interlude:
Outer Products
Given a bra ‚ü®œÜ| and a ket |œà‚ü©, we can form the inner product
‚ü®œÜ|œà‚ü©. As we‚Äôve seen, the inner product is a complex number.
However, there is another kind of product called the outer
product, written
|œà‚ü©‚ü®œÜ|.
The outer product is not a number; it is a linear operator.
Let‚Äôs consider what happens when |œà‚ü©‚ü®œÜ| acts on another ket
|A‚ü©:
|œà‚ü©‚ü®œÜ| |A‚ü©.
In these examples, we‚Äôre using spacing instead of parenthe-
ses to show the grouping of operations. Remember that all
operations with bras, kets, and linear operators are associa-
tive, which means we‚Äôre allowed to group them any way we
like, as long as we keep the same ordering from left to right.3
The action of the outer product operator is very simple and
can be deÔ¨Åned as
|œà‚ü©‚ü®œÜ| |A‚ü©‚â°|œà‚ü©‚ü®œÜ|A‚ü©.
3Sometimes we can change left-to-right ordering as well, but that
requires more care.

194
LECTURE 7. MORE ON ENTANGLEMENT
In other words, we take the inner product of ‚ü®œÜ| with |A‚ü©(the
result is a complex number) and multiply it by the ket |œà‚ü©.
The bra-ket notation is so eÔ¨Écient that it practically forces
the deÔ¨Ånition on us. That was the genius of Paul Dirac. It‚Äôs
easy to prove that the outer product can also act on bras:
‚ü®B| |œà‚ü©‚ü®œÜ| ‚â°‚ü®B|œà‚ü©‚ü®œÜ|.
A special case is the outer product of a ket with its corre-
sponding bra, |œà‚ü©‚ü®œà|. Assuming that |œà‚ü©is normalized, this
operator is called a projection operator. Here is how it acts:
|œà‚ü©‚ü®œà| |A‚ü©= |œà‚ü©‚ü®œà|A‚ü©
Note that the result is always proportional to |œà‚ü©. A pro-
jection operator can be said to project a vector onto the
direction deÔ¨Åned by |œà‚ü©. Here are some properties of projec-
tion operators that you can easily prove (remember that |œà‚ü©
is normalized to 1):
‚Ä¢ Projection operators are Hermitian.
‚Ä¢ The vector |œà‚ü©is an eigenvector of its projection oper-
ator with eigenvalue 1:
|œà‚ü©‚ü®œà| |œà‚ü©= |œà‚ü©
‚Ä¢ Any vector orthogonal to |œà‚ü©is an eigenvector with
eigenvalue zero. Thus, the eigenvalues of |œà‚ü©‚ü®œà| are all
either 0 or 1, and there is only one eigenvector with
unit eigenvalue. That eigenvector is |œà‚ü©itself.

7.2. INTERLUDE: OUTER PRODUCTS
195
‚Ä¢ The square of a projection operator is the same as the
projection operator itself:
|œà‚ü©‚ü®œà|2 = |œà‚ü©‚ü®œà|.
‚Ä¢ The trace of an operator (or any square matrix) is de-
Ô¨Åned as the sum of its diagonal elements. Using the
symbol Tr for trace, we can deÔ¨Åne the trace of an op-
erator L to be
Tr L =

i
‚ü®i|L|i‚ü©,
which is just the sum of L‚Äôs diagonal matrix elements.
The trace of a projection operator is 1. This follows
from the fact that the trace of a Hermitian operator is
the sum of its eigenvalues.4
‚Ä¢ If we add all the projection operators for a basis sys-
tem, we obtain the identity operator:

i
|i‚ü©‚ü®i| = I.
(7.11)
Finally, here is a very important theorem about projection
operators and expectation values. The expectation value of
4A Hermitian matrix M can be diagonalized by a transformation
P‚Ä†MP, where P is a unitary matrix whose columns are the normalized
eigenvectors of M. The trace of M is invariant under this transforma-
tion. We have not proved this well-known result.

196
LECTURE 7. MORE ON ENTANGLEMENT
any observable L in state |œà‚ü©is given by
‚ü®œà|L|œà‚ü©= Tr |œà‚ü©‚ü®œà| L.
(7.12)
Here are the steps to prove it. Pick any basis |i‚ü©. Then, using
the deÔ¨Ånition of trace, write
Tr |œà‚ü©‚ü®œà| L =

i
‚ü®i|œà‚ü©‚ü®œà|L|i‚ü©.
The two factors in the summation are just numbers, so we
can reverse their ordering,
Tr |œà‚ü©‚ü®œà| L =

i
‚ü®œà|L|i‚ü©‚ü®i|œà‚ü©.
Carrying out the sum and using  |i‚ü©‚ü®i| = I, we get
Tr |œà‚ü©‚ü®œà| L = ‚ü®œà|L|œà‚ü©.
The right side is just the expectation value of L.
7.3
Density Matrices: A New Tool
Up to now, we have learned how to make predictions about
a system when we know the system‚Äôs exact quantum state.
But more often than not, we don‚Äôt have complete knowledge
of the state. For example, suppose Alice has prepared a spin
using an apparatus oriented along some axis. She gives the
spin to Bob but doesn‚Äôt tell him the axis along which the
apparatus was oriented. Perhaps she gives him some partial

7.3. DENSITY MATRICES
197
information, such as the fact that the axis was either along
the z axis or the x axis, but she refuses to tell him more than
that. What does Bob do? How does he use this information
to make predictions?
Bob reasons as follows: If Alice prepared the spin in the
state |œà‚ü©, then the expectation value of any observable L is
Tr |œà‚ü©‚ü®œà|L = ‚ü®œà|L|œà‚ü©.
On the other hand, if Alice prepared the spin in state |œÜ‚ü©,
then the expectation value of L is
Tr |œÜ‚ü©‚ü®œÜ|L = ‚ü®œÜ|L|œÜ‚ü©.
What if there is a 50 percent probability that she prepared
|œà‚ü©and a 50 percent probability that she prepared |œÜ‚ü©? Ob-
viously, the expectation value is
‚ü®L‚ü©= 1
2Tr |œà‚ü©‚ü®œà|L + 1
2Tr |œÜ‚ü©‚ü®œÜ|L.
All we are doing is averaging over Bob‚Äôs ignorance of the
state prepared by Alice.
But now we can combine the terms into a single expres-
sion by deÔ¨Åning a density matrix œÅ that encodes Bob‚Äôs knowl-
edge. In this case the density matrix is half the projection
operator onto |œÜ‚ü©plus half the projection operator onto |œà‚ü©,
œÅ = 1
2|œà‚ü©‚ü®œà| + 1
2|œÜ‚ü©‚ü®œÜ|.

198
LECTURE 7. MORE ON ENTANGLEMENT
We‚Äôve now packaged all of Bob‚Äôs knowledge of the system
into a single operator œÅ. At this point, the rule to compute
expectation values becomes very simple:
‚ü®L‚ü©= Tr œÅL.
(7.13)
We can generalize this. Suppose that Alice tells Bob that she
has prepared one of several states‚Äîcall them |œÜ1‚ü©, |œÜ2‚ü©, |œÜ3‚ü©,
and so on. Moreover, she speciÔ¨Åes probabilities P1, P2, P3, . . .
for each of these states. Bob can still package all his knowl-
edge into a density matrix:
œÅ = P1|œÜ1‚ü©‚ü®œÜ1| + P2|œÜ2‚ü©‚ü®œÜ2| + P3|œÜ3‚ü©‚ü®œÜ3| + . . . .
Furthermore, he can use exactly the same rule, Eq. 7.13, to
compute the expectation value.
When the density matrix corresponds to a single state, it
is a projection operator that projects onto that state. In this
case, we say that the state is pure. A pure state represents
the maximum amount of knowledge that Bob can have of a
quantum system. But in the more general case, the density
matrix is a mix of several projection operators. We then say
that the density matrix represents a mixed state.
I have used the term density matrix, but strictly speaking,
œÅ is an operator. It only becomes a matrix when a basis is
chosen. Suppose we choose the basis |a‚ü©. The density matrix
is just the matrix representation of œÅ with respect to this
basis:

7.4. ENTANGLEMENT AND DENSITY MATRICES 199
œÅaa‚Ä≤ = ‚ü®a|œÅ|a‚Ä≤‚ü©.
If the matrix representation of L is La‚Ä≤,a then 7.13 takes the
form
‚ü®L‚ü©=

a,a‚Ä≤
La‚Ä≤,aœÅa,a‚Ä≤.
(7.14)
7.4
Entanglement and Density
Matrices
Classical physics also has its notion of pure and mixed states,
although they are not called by those names. Just to illus-
trate, let‚Äôs consider a system of two particles moving along
a line. According to the rules of classical mechanics, we can
calculate the orbits of the particles if we know the values
of their positions (x1 and x2) and momenta (p1 and p2) at
a certain instant in time. The state of the system is thus
speciÔ¨Åed by four numbers: x1, x2, p1, and p2. If we know
these four numbers, we have as complete a description of the
two-particle system as it is possible to have: there is no more
to know. We can call this a pure classical state.
Often, however, we don‚Äôt know the exact state, but only
some probabilistic information.
That information can be
encoded in a probability density
œÅ(x1, x2, p1, p2).
A classical pure state is just a special case of a probability

200
LECTURE 7. MORE ON ENTANGLEMENT
density, in which œÅ is nonzero at only one point. But more
generally, œÅ will be smeared out, in which case we could call
it a classical mixed state.5 When œÅ is smeared out, it means
our knowledge of the system state is incomplete. The more
smeared out it is, the greater our ignorance.
One thing should be completely obvious from this exam-
ple: if you know the pure state for the combined two-particle
system, then you know everything about each particle. In
other words, a pure state for two classical particles implies
a pure state for each of the individual particles.
But this is exactly what is not true in quantum mechanics
when a system is entangled. The state of a composite system
can be absolutely pure, but each of its constituents must be
described by a mixed state.
Let‚Äôs take a system composed of two parts, A and B. It
could be two spins or any other composite system.
In this case, we will suppose that Alice has complete
knowledge of the state of the combined system.
In other
words, she knows the wave function
Œ®(a, b).
There is nothing missing from her knowledge of the combined
system. Nevertheless, Alice is not interested in B. Instead,
she wishes to Ô¨Ånd out as much as she can about A without
looking at B. She selects an observable L that belongs to
A, and that does nothing to B when it acts. The rule for
5By smeared out, we mean that œÅ(x1, x2, p1, p2) will be nonzero for
a range of values of its arguments, not just one value. The greater this
range, the more smeared out œÅ becomes.

7.4. ENTANGLEMENT AND DENSITY MATRICES 201
calculating the expectation value of L is
‚ü®L‚ü©=

ab,a‚Ä≤b‚Ä≤
Œ®‚àó(a‚Ä≤b‚Ä≤)La‚Ä≤b‚Ä≤,abŒ®(ab).
(7.15)
So far, this is entirely general. However, if the observable L
is associated only with A, then it acts trivially on the b-index
and we can write the expectation value as
‚ü®L‚ü©=

a,b,a‚Ä≤
Œ®‚àó(a‚Ä≤b)La‚Ä≤,aŒ®(ab).
(7.16)
Now, Alice can summarize all of her knowledge, at least for
the purpose of studying A, in terms of a matrix œÅ:
œÅaa‚Ä≤ =

b
Œ®‚àó(a‚Ä≤b)Œ®(ab).
(7.17)
Surprisingly, Eq. 7.16 has exactly the same form as Eq. 7.14
for expectation value of a mixed state. Indeed, only in the
very special case of a product state will œÅ have the form of
a projection operator. In other words, despite the fact that
the composite system is described by a perfectly pure state,
the subsystem A must be described by a mixed state.
There‚Äôs a subtle point about our notation for density ma-
trices that‚Äôs worth noticing: in Eq. 7.17, the right-hand in-
dex of œÅ, that is, the a‚Ä≤ index, corresponds to the complex
conjugage state-vector Œ®‚àó(a‚Ä≤b) in the summation. This is a
consequence of our convention
Laa‚Ä≤ = ‚ü®a|L|a‚Ä≤‚ü©

202
LECTURE 7. MORE ON ENTANGLEMENT
for labeling the matrix elements of an operator L. Applying
this convention to
œÅ = |Œ®‚ü©‚ü®Œ®|
results in
œÅaa‚Ä≤ = ‚ü®a|Œ®‚ü©‚ü®Œ®|a‚Ä≤‚ü©,
or
œÅaa‚Ä≤ = Œ®(a)Œ®‚àó(a‚Ä≤).
7.5
Entanglement for Two Spins
Before leading you further into the world of entanglement,
I‚Äôll give you a simple deÔ¨Ånition and a quick warm-up exercise.
If Alice only has a single spin in a known state, her density
matrix is deÔ¨Åned to be
œÅaa‚Ä≤ = œà‚àó(a‚Ä≤)œà(a).
This equation tells you how to calculate an element of Alice‚Äôs
density matrix. If we stick with our familiar œÉz basis, each
index a and a‚Ä≤ can take the values up and down, so Alice has
a 2 √ó 2 density matrix.

7.5. ENTANGLEMENT FOR TWO SPINS
203
Exercise 7.4:
Calculate the density matrix for
|Œ®‚ü©= Œ±|u‚ü©+ Œ≤|d‚ü©.
Answer:
œà(u) = Œ±;
œà‚àó(u) = Œ±‚àó
œà(d) = Œ≤;
œà‚àó(d) = Œ≤‚àó
œÅa‚Ä≤a =
 Œ±‚àóŒ±
Œ±‚àóŒ≤
Œ≤‚àóŒ±
Œ≤‚àóŒ≤

.
Now try plugging in some numbers for Œ± and Œ≤. Make sure
they are normalized to 1. For example, Œ± =
1
‚àö
2, Œ≤ =
1
‚àö
2.
This simple example is a good way to understand the prop-
erties of density matrices. You can refer back to it as we look
at the more complex example of an entangled state.
Suppose we know the wave function of a composite sys-
tem, for example
œà(a, b),
but we are only interested in Alice‚Äôs subsystem. In other
words, we want to keep track of everything that Alice can
ever measure. Do we have to know the whole wave function?
Or is there some way to get rid of Bob‚Äôs variables?
The
answer to the latter question is yes; we can capture Alice‚Äôs
complete description in terms of a density matrix œÅ.
Let‚Äôs consider an observable L of Alice‚Äôs system. Like
any observable, it can of course be represented as a matrix:

204
LECTURE 7. MORE ON ENTANGLEMENT
La‚Ä≤b‚Ä≤,ab = ‚ü®a‚Ä≤b‚Ä≤|L|ab‚ü©.
Remember, for the composite system, the pair ab is really a
single index labeling a basis vector.
When we say, ‚ÄúL is an Alice-observable,‚Äù what we mean
is that L does nothing to Bob‚Äôs half of the state-label. This
forces some restrictions on the form of L. The idea is to Ô¨Ålter
out (set equal to zero) any of L‚Äôs matrix elements that have
the eÔ¨Äect of changing Bob‚Äôs half of the state-label. In other
words, L has the special form
La‚Ä≤b‚Ä≤,ab = La‚Ä≤a Œ¥b‚Ä≤b.
(7.18)
This simple-looking equation requires some explanation, and
you may want to review the material on tensor products in
component form, in the Interlude on tensor products (Sec-
tion 6.1). The left-hand side of the equation is an element of
a 4 √ó 4 matrix. Each of its two indices can take four distinct
values: uu, ud, du, or dd. What about the right-hand side?
The matrix element La‚Ä≤a also has two indices, but each of
them can take only two distinct values: u or d. In fact, the
same symbol L refers to two diÔ¨Äerent matrices on each side
of Eq. 7.18.
At Ô¨Årst glance, it appears as though we have equated a
4 √ó 4 matrix to a 2 √ó 2 matrix, and indeed that would be
a problem. However, the factor Œ¥b‚Ä≤b makes everything work
out. The term La‚Ä≤a Œ¥b‚Ä≤b is an element of the tensor product
of two 2 √ó 2 matrices, and that tensor product is a 4 √ó 4

7.5. ENTANGLEMENT FOR TWO SPINS
205
matrix.6 Here is the way to read Eq. 7.18:
The 4 √ó 4 matrix La‚Ä≤b‚Ä≤,ab can be factored into a tensor
product of the two 2 √ó 2 matrices La‚Ä≤a and Œ¥b‚Ä≤b, where Œ¥b‚Ä≤b
is equivalent to the 2 √ó 2 identity matrix.
Now, let‚Äôs calculate the expectation value of L (the 4 √ó 4
version) using the full apparatus of the composite system:
‚ü®Œ®|L|Œ®‚ü©=

a,b,a‚Ä≤,b‚Ä≤
œà‚àó(a‚Ä≤, b‚Ä≤) La‚Ä≤b‚Ä≤,ab œà(a, b).
As I warned, there are lots of indices. But it gets simpler if
we use the special form of the matrix L. The factor Œ¥b‚Ä≤b in
Eq. 7.18‚Äîa Kronecker delta‚ÄîÔ¨Ålters out any elements that
change Bob‚Äôs half of the label, and leaves the others intact.
It tells us to set b‚Ä≤ = b to get
‚ü®Œ®|L|Œ®‚ü©=

a‚Ä≤,b,a
œà‚àó(a‚Ä≤, b) La‚Ä≤,a œà(a, b).
(7.19)
For the moment, let‚Äôs ignore the sums over a and a‚Ä≤, and
concentrate instead on the sum over b. We encounter the
quantity
œÅa‚Ä≤a =

b
œà‚àó(a, b) œà(a‚Ä≤, b).
(7.20)
The 2 √ó 2 matrix œÅa‚Ä≤a is Alice‚Äôs density matrix. Notice that
œÅa‚Ä≤a does not depend on any b-index since it has already been
6We could also call it a Kronecker product, since we‚Äôre talking about
matrices. The formal distinction is not important for our purposes.

206
LECTURE 7. MORE ON ENTANGLEMENT
summed over b. It is purely a function of Alice variables a
and a‚Ä≤. In fact, we only kept the b‚Äôs in the equation to make
the example in the next section easier to follow.
We can simplify Eq. 7.19 by plugging in œÅa‚Ä≤a from Eq.
7.20. The expectation value of L (the 2 √ó 2 version) then
becomes
‚ü®L‚ü©=

a‚Ä≤a
œÅa‚Ä≤a La,a‚Ä≤.
(7.21)
In summing over b, we have collapsed a 4 √ó 4 matrix down
to a 2 √ó 2 matrix. This makes sense. We expect an operator
that acts on the composite system to be a 4 √ó 4 matrix, and
we expect an Alice operator to be 2 √ó 2.
Notice that the right side of Eq. 7.21 is a sum of diagonal
matrix elements. In other words, it‚Äôs the trace of the matrix
œÅL, which we can write as
‚ü®L‚ü©= Tr œÅL.
The lesson is this: To calculate Alice‚Äôs density matrix œÅ,
we may need to know the full wave function, including the
dependence on Bob‚Äôs variables. But once we know œÅ, we can
forget where it came from, and use it to calculate anything
about Alice‚Äôs observations. As a simple example, we can use
œÅ to calculate the probability P(a) that Alice‚Äôs system will
be left in the state a if a measurement is performed.
To
determine P(a), we begin with P(a, b), the probability that
the combined system is in state |ab‚ü©. That‚Äôs just
P(a, b) = œà‚àó(a, b)œà(a, b).

7.5. ENTANGLEMENT FOR TWO SPINS
207
By the standard rules of probability, if we sum over b, we get
the probability for a:
P(a) =

b
œà‚àó(a, b)œà(a, b).
This is just a diagonal entry in the density matrix:
P(a) = œÅaa.
(7.22)
Here are some properties of density matrices:
‚Ä¢ Density matrices are Hermitian:
œÅaa‚Ä≤ = œÅ‚àó
a‚Ä≤a.
‚Ä¢ The trace of a density matrix is 1:
Tr(œÅ) = 1.
Eq. 7.22 should help make this clear because the left
side is a probability.
‚Ä¢ The eigenvalues of the density matrix are all positive
and lie between 0 and 1. It follows that if any eigenvalue
is 1, all the others are 0. Can you interpret this result?
‚Ä¢ For a pure state:
œÅ2 = œÅ
Tr(œÅ2) = 1

208
LECTURE 7. MORE ON ENTANGLEMENT
‚Ä¢ For a mixed or entangled state:
œÅ2 Ã∏= œÅ
Tr(œÅ2) < 1
The last two properties give us a clear way to distinguish
mathematically between pure and mixed states. A subsys-
tem of an entangled state (such as Alice‚Äôs half of the singlet
state) is considered a mixed state.
It‚Äôs worth taking a moment to understand these two prop-
erties a little better. To simplify things, we will assume that
œÅ is a diagonal matrix‚Äîin other words, all of its oÔ¨Ä-diagonal
elements are zero. This simpliÔ¨Åcation costs us nothing be-
cause œÅ is Hermitian, and it turns out that every Hermitian
matrix can be expressed in diagonal form in some basis.7
Taking the square of a diagonal matrix is quite simple: all
you need to do is square each individual element. Since œÅ rep-
resents a mixed state, and the diagonal elements of œÅ must
add up to 1, none of the diagonal elements of œÅ can equal
1. Otherwise, œÅ would represent a pure state. Therefore, œÅ
must have at least two positive diagonal elements that are
less than 1. Squaring these elements gives a new matrix œÅ2
whose elements are even smaller. This accounts for both of
the mixed-state properties of œÅ.
Before you try the next exercises, I‚Äôll mention one more
thing about the trace. It turns out that the trace has many
7As we mentioned earlier, in Section 7.2, a Hermitian matrix M
can be diagonalized by a transformation P‚Ä†MP, where P is a unitary
matrix whose columns are the normalized eigenvectors of M.

7.5. ENTANGLEMENT FOR TWO SPINS
209
interesting mathematical properties. One of its more useful
properties is that the trace of a product of two matrices does
not depend on their order of multiplication. In other words,
TrAB = TrBA,
even if
AB Ã∏= BA.
I mention this because you will sometimes see the trace of the
density matrix written as Tr LœÅ, instead of Tr œÅL. These
two expressions are equivalent.
Exercise 7.5:
a) Show that
 a
0
0
b
2
=
 a2
0
0
b2

.
b) Now, suppose
œÅ =
 1
3
0
0
2
3

.
Calculate
œÅ2
Tr(œÅ)
Tr(œÅ2).
c) If œÅ is a density matrix, does it represent a pure state or
a mixed state?

210
LECTURE 7. MORE ON ENTANGLEMENT
Exercise 7.6:
Use Eq. 7.22 to show that if œÅ is a density
matrix, then
Tr(œÅ) = 1.
7.6
A Concrete Example:
Calculating Alice‚Äôs Density
Matrix
So far, the discussion of density matrices may have been a
little abstract for some readers. Here is a worked-out ex-
ample that should help bring density matrices into sharper
focus. Recall the deÔ¨Ånition of Alice‚Äôs density matrix from
Eq. 7.20:
œÅa‚Ä≤a =

b
œà‚àó(a, b) œà(a‚Ä≤, b).
(7.23)
Now, consider the state-vector
|Œ®‚ü©
=
1
‚àö
2

|ud‚ü©+ |du‚ü©

.
Notice that two of the basis vectors have a coeÔ¨Écient of
1
‚àö
2,
while the other two have coeÔ¨Écients of zero. The state is
normalized because the sum of the squared coeÔ¨Écients is 1.
Also, all four coeÔ¨Écients happen to be real, which simpliÔ¨Åes
the process of complex conjugation.

7.6. CALCULATING ALICE‚ÄôS DENSITY MATRIX
211
Let‚Äôs calculate Alice‚Äôs density matrix for this state. First,
for all possible inputs a and b, we‚Äôll list the values of œà(a, b).
Recall that these are just the basis vector coeÔ¨Écients:
œà(u, u)
=
0
œà(u, d)
=
1
‚àö
2
œà(d, u)
=
1
‚àö
2
œà(d, d)
=
0.
Next, we‚Äôll use these four equations to calculate each element
of Alice‚Äôs density matrix by expanding the summation of Eq.
7.23. In the expansion, notice that for every factor of the
form œà‚àó(a, b)œà(a‚Ä≤, b), Bob‚Äôs input is the same for both factors.
We discard any terms that do not have this property. This is
what we mean by ‚Äúsetting b‚Ä≤ equal to b in the summation.‚Äù
Here is the expansion:
œÅuu
=
œà‚àó(u, u)œà(u, u) + œà‚àó(u, d)œà(u, d) = 1
2
œÅud
=
œà‚àó(u, u)œà(d, u) + œà‚àó(u, d)œà(d, d) = 0
œÅdu
=
œà‚àó(d, u)œà(u, u) + œà‚àó(d, d)œà(u, d) = 0
œÅdd
=
œà‚àó(d, u)œà(d, u) + œà‚àó(d, d)œà(d, d) = 1
2.
These values are the elements of a 2 √ó 2 matrix:

212
LECTURE 7. MORE ON ENTANGLEMENT
œÅ
=
 1
2
0
0
1
2

.
(7.24)
The trace of our matrix is 1. And our density matrix is done.8
Exercise 7.7: Use Eq. 7.24 to calculate œÅ2. How does this
result conÔ¨Årm that œÅ represents an entangled state? We‚Äôll
soon discover that there are other ways to check for entan-
glement.
Exercise 7.8: Consider the following states:
|œà1‚ü©
=
1
2

|uu‚ü©+ |ud‚ü©+ |du‚ü©+ |dd‚ü©

|œà2‚ü©
=
1
‚àö
2

|uu‚ü©+ |dd‚ü©

|œà3‚ü©
=
1
5

3|uu‚ü©+ 4|ud‚ü©

.
For each one, calculate Alice‚Äôs density matrix and Bob‚Äôs den-
sity matrix. Check their properties.
7.7
Tests for Entanglement
Suppose I gave you a wave function
œà(a, b)
8Art‚Äôs a poet, and he‚Äôs not even aware of it.

7.7. TESTS FOR ENTANGLEMENT
213
for the composite SAB system. How could you tell whether
the corresponding state is entangled?
I am not referring
to an experimental test but to a mathematical procedure.
A related question is whether there are varying degrees of
entanglement. If there are, how could you quantify them?
Entanglement is the quantum mechanical generalization
of correlation. In other words, it indicates that Alice can
learn something about Bob‚Äôs half of the system by measur-
ing her own. In the classical example of the previous lecture,
I illustrated the idea of correlation using coins. If Alice ob-
serves the coin that Charlie gave her, she not only knows
whether her own coin is a penny or a dime; she also knows
which coin Bob has. That‚Äôs the experimental picture. The
mathematical indication of correlation is that the probability
function P(a, b) does not factorize (that is, it does not look
like Eq. 6.3).
Whenever the probability distribution does
not factorize, there are nonzero correlations as I described in
Inequality 6.2.
7.7.1
The Correlation Test for
Entanglement
Let‚Äôs assume that A is an Alice observable and B is a Bob
observable. The correlation between them is deÔ¨Åned in terms
of the average values (also known as the expectation values)
of the individual observables, and of their product. Suppose
that
‚ü®A‚ü©

214
LECTURE 7. MORE ON ENTANGLEMENT
‚ü®B‚ü©
‚ü®AB‚ü©
are these expectation values. The correlation C(A, B) be-
tween A and B is deÔ¨Åned as
C(A, B) = ‚ü®AB‚ü©‚àí‚ü®A‚ü©‚ü®B‚ü©.
Exercise 7.9: Given any Alice observable A and Bob ob-
servable B, show that for a product state, the correlation
C(A, B) is zero.
From this exercise, we can learn something about entan-
glement. If a system is in a state where one can Ô¨Ånd any
two observables A and B that are correlated‚Äîmeaning that
C(A, B) Ã∏= 0‚Äîthen the state is entangled. Correlations are
deÔ¨Åned to lie in the range ‚àí1 to +1. These extreme values
represent the greatest possible negative and positive corre-
lations. The greater the magnitude of C(A, B), the more
entangled is the state.
If C(A, B) = 0, then there is no
correlation (and no entanglement) at all.
7.7.2
The Density Matrix Test for
Entanglement
To calculate correlations, you have to know about both Bob‚Äôs
part and Alice‚Äôs part of the system, along with the system
wave function. But there is another test for entanglement

7.7. TESTS FOR ENTANGLEMENT
215
that only requires us to know Alice‚Äôs (or Bob‚Äôs) density ma-
trix. Let‚Äôs suppose that the state |Œ®‚ü©is a product state of
a Bob factor |œÜ‚ü©and an Alice factor |œà

. That means the
composite wave function is also the product of a Bob factor
and an Alice factor:
œà(a, b) = œà(a)œÜ(b).
Now, let‚Äôs work out Alice‚Äôs density matrix. We use the deÔ¨Å-
nition in Eq. 7.20 to get
œÅa‚Ä≤a = œà‚àó(a)œà(a‚Ä≤)

b
œÜ‚àó(b)œÜ(b).
But if Bob‚Äôs state is normalized, then

b
œÜ‚àó(b)œÜ(b) = 1,
which makes Alice‚Äôs density matrix particularly simple:
œÅa‚Ä≤a = œà‚àó(a)œà(a‚Ä≤).
(7.25)
Notice that it only depends on the Alice variables. Perhaps
it‚Äôs not very surprising that everything we need to know
about Alice‚Äôs system is contained in Alice‚Äôs wave function.
Now, I‚Äôm going to prove a key theorem about the eigen-
values of Alice‚Äôs density matrix, under the assumption of
a product state. It is true only for unentangled states and
serves to identify them. The theorem says that for any prod-
uct state, Alice‚Äôs (or Bob‚Äôs) density matrix has exactly one

216
LECTURE 7. MORE ON ENTANGLEMENT
nonzero eigenvalue, and that eigenvalue is exactly 1. We be-
gin the theorem by writing the eigenvalue equation for the
matrix œÅ:

a‚Ä≤
œÅa‚Ä≤aŒ±a‚Ä≤ = ŒªŒ±a.
In other words, the matrix œÅ acting on the column vector
Œ± gives back the same vector multiplied by an eigenvalue Œª.
Using the simple form of œÅ in Eq. 7.25, we can write
œà(a‚Ä≤)

a
œà‚àó(a)Œ±a = ŒªŒ±a‚Ä≤.
(7.26)
Now, you may notice a couple of things. First, the quantity

a
œà‚àó(a)Œ±a
has the form of an inner product. If the column vector Œ± is
orthogonal to œà, then the left side of Eq. 7.26 is zero. Such
a vector is an eigenvector of œÅ with eigenvalue zero.
If the dimension of Alice‚Äôs space of states is NA, then
there are NA ‚àí1 vectors orthogonal to œà. Each one of them
is an eigenvector of œÅ with eigenvalue 0. That leaves only one
possible direction for an eigenvector with a nonzero eigen-
value, namely the vector œà(a). In fact, if we plug in Œ±a =
œà(a), we do indeed Ô¨Ånd that it is an eigenvector of œÅ with
eigenvalue 1.
To summarize the theorem: If the composite Alice-Bob
system is in a product state, then Alice‚Äôs (or Bob‚Äôs) density

7.7. TESTS FOR ENTANGLEMENT
217
matrix has one and only one eigenvalue equal to 1, and all
the rest are zero. Moreover, the eigenvector with a nonzero
eigenvalue is nothing but the wave function of Alice‚Äôs half of
the system.
In this situation, Alice‚Äôs system is in a pure state. All of
Alice‚Äôs observations are described as if Bob and his system
never existed and Alice had an isolated system described by
the wave function œà(a‚Ä≤).
The opposite extreme of a pure state is a maximally en-
tangled state.
Maximally entangled states are states of a
combined system in which nothing is known about either
subsystem, even though they are complete descriptions of
the system as a whole‚Äîas complete as quantum mechanics
allows. The state |sing‚ü©is a maximally entangled state.
When Alice calculates her density matrix for a maximally
entangled state, she Ô¨Ånds something very disappointing: the
density matrix is proportional to the unit matrix. All the
eigenvalues are equal, and given that they all sum to unity,
each eigenvalue is equal to 1/NA. In other words,
œÅa‚Ä≤a =
1
NA
Œ¥a‚Ä≤a.
(7.27)
Why is Alice disappointed? Go back to Eq. 7.22. This equa-
tion says that the probability for a particular state a is the
diagonal element of œÅ, but Eq. 7.27 tells us that all the prob-
abilities are equal. What could be less informative than a
probability distribution so structureless that every possible
outcome is equally probable?

218
LECTURE 7. MORE ON ENTANGLEMENT
Maximal entanglement implies a complete lack of infor-
mation about Alice‚Äôs subsystem for experiments that only
involve that one subsystem. On the other hand, it implies
a large correlation between Alice‚Äôs and Bob‚Äôs measurements.
For the singlet state, if Alice measures any component of her
spin, she automatically knows the result Bob would get if
he were to measure the same component of his spin. This is
exactly the kind of knowledge that is precluded in a product
state.
So in each type of state, some things are predictable and
some are not. In a product state, we can make statistical pre-
dictions about measurements made on each separate subsys-
tem, but Alice‚Äôs measurements tell her nothing about Bob‚Äôs
system. In a maximally entangled state, on the other hand,
Alice can predict nothing about her own measurements, but
she knows a great deal about the relation between her out-
comes and Bob‚Äôs.
7.8
The Process of Measurement
We have seen that quantum systems evolve in what look
like irreconcilably diÔ¨Äerent ways: by unitary evolution be-
tween measurements, and by wave function collapse when
measurements take place. This circumstance has led to some
of the most contentious debates and confusing claims about
so-called reality. I‚Äôm going to steer away from those debates
and stick to the facts. Once you know how quantum me-
chanics works, you can decide for yourself whether you think
there is a problem.
Let‚Äôs begin by noting that every measurement involves

7.8. THE PROCESS OF MEASUREMENT
219
a system and an apparatus. But if quantum mechanics is
a consistent theory, then it should be possible to combine
the system and apparatus into a single bigger system. For
simplicity let‚Äôs take the system to be a single spin. The appa-
ratus A is the same one that we used in the very Ô¨Årst lecture.
The window in the apparatus can show three possible read-
ings. The Ô¨Årst is blank‚Äîit represents the neutral state of
the apparatus before it comes in contact with the spin. The
two other readings record the two possible outcomes of the
measurement: +1 or ‚àí1.
If the apparatus is a quantum system (of course, it must
be), then it is described by a space of states. In the simplest
description, the apparatus has exactly three states: a blank
state and two outcome states. Thus, the basis vectors for
the apparatus are
|b

| + 1

| ‚àí1

.
Meanwhile, the basis states of the spin can be taken to be
the usual up and down states:
|u‚ü©
|d‚ü©.
From these two sets of basis vectors, we can build up a com-
posite (tensor product) space of states that has the six basis
vectors

220
LECTURE 7. MORE ON ENTANGLEMENT
|u, b‚ü©
|u, +1‚ü©
|u, ‚àí1‚ü©
|d, b‚ü©
|d, +1‚ü©
|d, ‚àí1‚ü©.
The detailed mechanics of what takes place when system
meets apparatus may be complicated, but we are free to
make some assumptions about how the combined system
evolves. Let‚Äôs assume the apparatus starts in the blank state
and the spin starts in the up state. After the apparatus in-
teracts with the spin, the Ô¨Ånal state (by assumption) is
|u, +1‚ü©.
In other words, the interaction leaves the spin unchanged
but Ô¨Çips the apparatus to the +1 state. We write this as
|u, b‚ü©‚Üí|u, +1‚ü©.
(7.28)
Similarly, we can require that if the spin is in the down state,
it Ô¨Çips the apparatus to the ‚àí1 state:
|d, b‚ü©‚Üí|d, ‚àí1‚ü©.
(7.29)

7.8. THE PROCESS OF MEASUREMENT
221
So by looking at the apparatus after it interacts with the
spin, you can tell what the spin was initially.
Now, let‚Äôs
assume that the initial spin state is more general, namely
Œ±u|u‚ü©+ Œ±d|d‚ü©.
If we include the apparatus as part of the system, the initial
state is
Œ±u|u, b‚ü©+ Œ±d|d, b‚ü©.
(7.30)
This initial state is a product state, speciÔ¨Åcally a product of
the initial spin state and the blank apparatus state. You can
check that it is completely unentangled.
Exercise 7.10: Verify that the state-vector in 7.30 repre-
sents a completely unentangled state.
Because we know from Eqs. 7.28 and 7.29 how the individual
terms in 7.30 evolve, we can easily determine the Ô¨Ånal state:
Œ±u|u, b‚ü©+ Œ±d|d, b‚ü©‚ÜíŒ±u|u, +1‚ü©+ Œ±d|d, ‚àí1‚ü©.
This Ô¨Ånal state is an entangled state. In fact, if Œ±u = ‚àíŒ±d,
it is the maximally entangled singlet state. Indeed, one can
look at the apparatus and immediately tell what the spin
state is: if the apparatus reads +1 ,the spin is up, and if it
reads ‚àí1, the spin is down. Moreover, the probability that
the Ô¨Ånal apparatus shows +1 is

222
LECTURE 7. MORE ON ENTANGLEMENT
Œ±‚àó
uŒ±u.
This number represents a probability‚Äîit‚Äôs exactly the same
as the original probability that the spin was up. In this de-
scription of a measurement, no collapse of the wave function
takes place. Instead, entanglement between the apparatus
and the system just happens by unitary evolution of the
state-vector.
The only problem is that, in a certain sense, we have
merely delayed the diÔ¨Éculty.
It is not very satisfying to
be told that the apparatus ‚Äúknows‚Äù the spin state unless
the experimenter‚Äîlet‚Äôs say Alice‚Äîis allowed to look at the
apparatus. Isn‚Äôt it true that when she does so, she will col-
lapse the wave function of the composite system? Yes and
no. For all of Alice‚Äôs purposes, yes; she will conclude that
the apparatus, and the spin, are in one of the two possible
conÔ¨Ågurations and will proceed accordingly.
But now let‚Äôs bring Bob into the picture. So far, he has
not interacted with the spin, the apparatus, or Alice. From
his point of view, all three form a single quantum system. No
wave function collapse took place when Alice looked at the
apparatus. Instead, Bob says that Alice became entangled
with the other two component systems.
That‚Äôs all well and good, but what happens when Bob
looks at Alice? For his purposes, he has collapsed the wave
function. But then there is good old Charlie . . .
Does the last entity to look at the system collapse the
wave function, or does it just get entangled?
Or is there

7.9. ENTANGLEMENT AND LOCALITY
223
a last looker?
I won‚Äôt try to answer these questions, but
what should be apparent is that quantum mechanics is a
consistent calculus of probabilities for a certain kind of ex-
periment involving a system and an apparatus. We use it,
and it works, but when we try to ask questions about the
underlying ‚Äúreality,‚Äù we get confused.
7.9
Entanglement and Locality
Does quantum mechanics violate locality? Some people think
so.
Einstein railed against the ‚Äúspooky action at a dis-
tance‚Äù (spukhafte Fernwirkung) that he claimed was implied
by quantum mechanics. And John Bell became almost a cult
Ô¨Ågure by proving that quantum mechanics is nonlocal.
On the other hand, most theoretical physicists, particu-
larly those who study quantum Ô¨Åeld theory, which is riddled
with entanglement, would claim the opposite: quantum me-
chanics done correctly ensures locality.
The problem, of course, is that the two groups mean dif-
ferent things by locality. Let‚Äôs begin with the quantum Ô¨Åeld
theorist‚Äôs understanding of the term.
From this point of
view, locality has only one meaning: it is impossible to send
a signal faster than the speed of light. I will show you how
quantum mechanics enforces this rule.
First, let me expand the deÔ¨Ånition of Alice‚Äôs system and
Bob‚Äôs system. So far, I have used the term Alice‚Äôs system
to mean some system that Alice carries with her and can do
experiments on. For the rest of this section, I will use the
term to mean something else: Alice‚Äôs system consists not
only of some system that she carries, but also the apparatus

224
LECTURE 7. MORE ON ENTANGLEMENT
that she uses, and even herself. The same thing, of course,
goes for Bob‚Äôs system. The basis ket-vectors
|a

describe everything that Alice can interact with. Likewise,
the ket-vectors
|b‚ü©
describe everything that Bob can interact with.
And the
tensor product states
|ab‚ü©
describe the combination of Alice‚Äôs and Bob‚Äôs worlds.
We will assume that Alice and Bob may have been close
enough to interact sometime in the past, but at present Alice
is on Alpha Centauri and Bob is in Palo Alto. The Alice-Bob
wave function is
œà(ab),
and it may be entangled. Alice‚Äôs complete description of her
system, her apparatus, and herself is contained in her density
matrix œÅ:
œÅaa‚Ä≤ =

b
œà‚àó(a‚Ä≤b) œà(ab).
(7.31)

7.9. ENTANGLEMENT AND LOCALITY
225
Consider this question: Can Bob, at his end, do anything to
instantly change Alice‚Äôs density matrix? Keep in mind that
Bob can only do things that the laws of quantum mechanics
allow.
In particular, Bob‚Äôs evolution, whatever causes it,
must be unitary. In other words, it must be described by a
unitary matrix
Ubb‚Ä≤.
The matrix U represents whatever happens to Bob‚Äôs system,
whether or not Bob does an experiment. It acts on the wave
function to produce a new wave function, which we‚Äôll call
the ‚ÄúÔ¨Ånal‚Äù wave function:
œàfinal(ab) =

b‚Ä≤
Ubb‚Ä≤ œà(ab‚Ä≤).
We can also write the complex conjugate of this wave func-
tion:
œà‚àó
final(a‚Ä≤b) =

b‚Ä≤‚Ä≤
œà‚àó(a‚Ä≤b‚Ä≤‚Ä≤) U‚Ä†
b‚Ä≤‚Ä≤b.
Notice that we added primes to some of the symbols to avoid
mixing them up in the next step. Now, let‚Äôs calculate Alice‚Äôs
new density matrix. We‚Äôll use Eq. 7.31, but we‚Äôll replace the
original wave functions with the Ô¨Ånal ones:
œÅaa‚Ä≤ =

b,b‚Ä≤,b‚Ä≤‚Ä≤
œà‚àó(a‚Ä≤b‚Ä≤‚Ä≤) U‚Ä†
b‚Ä≤‚Ä≤b Ubb‚Ä≤ œà(ab‚Ä≤).

226
LECTURE 7. MORE ON ENTANGLEMENT
There are lots of indices Ô¨Çying around now, but the math
isn‚Äôt as hard as it looks. In fact, look at how the U matrices
enter through the combination
U‚Ä†
b‚Ä≤‚Ä≤b Ubb‚Ä≤.
This combination is just the matrix product U‚Ä†U. But recall
that U is unitary. This tells you that the product U‚Ä†U is the
unit matrix Œ¥b‚Ä≤‚Ä≤b‚Ä≤. As before, this amounts to an instruction
to include all the terms where b‚Ä≤‚Ä≤ = b‚Ä≤, and to ignore all the
others. With this simpliÔ¨Åcation, we get
œÅaa‚Ä≤ =

b
œà‚àó(a‚Ä≤b) œà(ab).
This is exactly the same as Eq. 7.31. In other words, œÅaa‚Ä≤
is exactly the same as it was before U acted. Nothing that
happens at Bob‚Äôs end has any immediate eÔ¨Äect on Alice‚Äôs
density matrix, even if Bob and Alice are maximally entan-
gled. This means that Alice‚Äôs view of her subsystem (her
statistical model) remains exactly as it was. This remark-
able result may seem surprising for a maximally entangled
system, but it also guarantees that no faster-than-light signal
has been sent.

7.10. QUANTUM SIM: INTRO TO BELL‚ÄôS THM
227
7.10
The Quantum Sim: An
Introduction to Bell‚Äôs
Theorem
It‚Äôs interesting that unitarity played a prominent role in
guaranteeing that no signal can be sent instantaneously. If
U had not been unitary, Alice‚Äôs Ô¨Ånal density matrix would
indeed have been aÔ¨Äected by Bob.
What was it, then, that disturbed Einstein so much that
he spoke of spooky action at a distance?
To answer this
question, it‚Äôs important to understand that he and Bell were
talking about a totally diÔ¨Äerent notion of locality. To illus-
trate this, I am going to invent a computer game. What
my new computer game does is try to fool you into thinking
there is a quantum spin in a magnetic Ô¨Åeld inside the com-
puter. You get to do experiments to test this possibility. See
Fig. 7.1 for a schematic.
Here‚Äôs how it works: Inside the computer, the memory
stores two complex numbers, Œ±u and Œ±d, subject to the usual
normalization rule,
Œ±‚àó
uŒ±u + Œ±‚àó
dŒ±d = 1.
At the beginning of the game, the Œ± coeÔ¨Écients are initialized
at some value. The computer then solves the Schr¬®odinger
equation to update the Œ±‚Äôs exactly as if they were the com-
ponents of the spin‚Äôs state-vector.
The computer also stores the classical three-dimensional
orientation of the apparatus in the form of two angles or a

228
LECTURE 7. MORE ON ENTANGLEMENT
Figure 7.1: Quantum Sim. The computer screen displays the
user-controlled orientation of the apparatus. For simplicity,
only the two-dimensional orientation is shown here. The user
can press the M button whenever she or he wants to measure
the spin (not shown). Between measurements, the spin state
evolves according to the Schr¬®odinger equation.
unit vector.
The keyboard allows you to set these angles
and change them at will. One more element is stored in the
memory, namely the value (either +1 or ‚àí1) representing
the number in the window of the apparatus. The computer
screen shows the apparatus. As the experimenter, you get
to choose how your apparatus will be oriented. There is also

7.10. QUANTUM SIM: INTRO TO BELL‚ÄôS THM
229
a measure button M that activates the apparatus.
The Ô¨Ånal element of the program is a random number
generator that produces the measurement results +1 or ‚àí1
with probabilities Œ±‚àó
uŒ±u and
Œ±‚àó
dŒ±d, respectively.
Keep in
mind that random number generators are not really gen-
erators of random numbers; they are random number sim-
ulators. They are based on entirely classical deterministic
mechanisms, using things like the digits of œÄ to generate
numbers. Nevertheless, they are good enough to fool you.
The game begins, and the computer continually updates
the values of Œ±u and Œ±d. You wait as long as you want and
then hit the M button. Then, with the aid of the random
number generator, the game produces an outcome that is
displayed on the screen. Based on this outcome, the com-
puter updates the state by collapse. If the outcome is +1,
the value of Œ±d is reset to zero, and the value of Œ±u is reset to
unity. If the outcome is ‚àí1, the value of Œ±d is reset to unity,
and the value of Œ±d is reset to zero. Then, the Schr¬®odinger
equation takes over until you hit M again.
Being a good experimenter, you do many trials and col-
lect statistics, which you compare with quantum mechanical
predictions. If everything works properly, you conclude that
quantum mechanics is the correct description of whatever is
taking place in the computer. Of course, the computer is still
entirely classical, but it simulates a quantum spin without
much diÔ¨Éculty.
Next, let‚Äôs try the same thing with two computers, A
and B, simulating two quantum spins.
If the spins start
in a product state and never interact, we can simply play

230
LECTURE 7. MORE ON ENTANGLEMENT
the game on each of the two computers without any cross
talk. But now, Alice, Bob, and Charlie return to help us
out. Charlie, of course, wants to create an entangled pair.
He begins by connecting the two computers with a cable
to form a single computer, and we assume the cable can
send instantaneous signals.
In its memory, the combined
computer now stores four complex numbers,
Œ±uu, Œ±ud, Œ±du, Œ±dd,
and it updates these numbers using the Schr¬®odinger equa-
tion.
Each computer screen shows an apparatus.
Alice‚Äôs
screen shows A and Bob‚Äôs screen shows B.
Each virtual
apparatus can be independently oriented, and each can be
independently activated by its own M button. When either
M button is pressed, the joint memory (with the aid of the
random number generator) sends a signal to the correspond-
ing apparatus and produces an outcome.
Can this device simulate the quantum mechanics of the
two-spin system? Yes, it can‚Äîas long as the cable connect-
ing the computers is not disconnected, and as long as it can
send messages instantaneously. But unless the system is in
a product state and stays in a product state, disconnecting
the two computers will destroy the simulation.
Can we prove this? Again, the answer is yes‚Äîand that is
the essential content of Bell‚Äôs theorem. Any classical simu-
lation of quantum mechanics that tries to spatially separate
Alice‚Äôs and Bob‚Äôs apparatuses must have an instantaneous
cable connecting the separate computers with a central mem-
ory that stores and updates the state-vector.

7.11. ENTANGLEMENT SUMMARY
231
But doesn‚Äôt this mean that locality-violating information
can be sent through the cable? It would, if Alice, Bob, and
Charlie were allowed to do anything that nonrelativistic clas-
sical systems can do.9 But if the only operations that are
allowed are those that simulate quantum operations, then
the answer is no. As we‚Äôve seen, quantum mechanics does
not allow Alice‚Äôs density matrix to be aÔ¨Äected by Bob‚Äôs ac-
tions.
This problem is not a problem for quantum mechanics.
It‚Äôs a problem for simulating quantum mechanics with a clas-
sical Boolean computer. That‚Äôs the content of Bell‚Äôs theo-
rem: The classical computers have to be connected with an
instantaneous cable to simulate entanglement.
7.11
Entanglement Summary
Of all the counterintuitive ideas quantum mechanics forces
upon us, entanglement may be the hardest one to accept.
There is no classical analog for a system whose full state de-
scription contains no information about its individual sub-
components. Nonlocality is surprisingly diÔ¨Écult to even de-
Ô¨Åne. The best way to come to terms with these issues is
to internalize the mathematics. What follows is a compact
summary of what we‚Äôve learned about entanglement. In par-
ticular, we‚Äôve tried to map out the diÔ¨Äerences between entan-
gled, unentangled, and partially entangled states by creating
‚Äúrap sheets‚Äù for three speciÔ¨Åc examples‚Äîthe singlet state,
a product state, and a ‚Äúnear singlet‚Äù state. We hope this
9In other words, systems that permit signals to be sent instantly.

232
LECTURE 7. MORE ON ENTANGLEMENT
format will help clarify the mathematical similarities and
diÔ¨Äerences. Please take some time to review this material
and work the exercises before moving on.
State-Vector Rap Sheet 1
Name: Product State (No Entanglement)
Wanted for: Excessive Locality, Impersonating a Classical
System
Description: Each subsystem is fully characterized. There
are no correlations between Alice‚Äôs and Bob‚Äôs systems.
State-Vector: Œ±uŒ≤u|uu‚ü©+ Œ±uŒ≤d|ud‚ü©+ Œ±dŒ≤u|du‚ü©+ Œ±dŒ≤d|dd‚ü©
Normalization: Œ±‚àó
uŒ±u + Œ±‚àó
dŒ±d = 1,
Œ≤‚àó
uŒ≤u + Œ≤‚àó
dŒ≤d = 1
Density Matrix: Alice‚Äôs density matrix has exactly one
nonzero eigenvalue, which equals 1. The eigenvector with
this nonzero eigenvalue is the wave function of Alice‚Äôs sub-
system. The same goes for Bob.
Wave Function: Factorized: œà(a)œÜ(b)
Expectation Values:
‚ü®œÉx‚ü©2 + ‚ü®œÉy‚ü©2 + ‚ü®œÉz‚ü©2 = 1
‚ü®œÑx‚ü©2 + ‚ü®œÑy‚ü©2 + ‚ü®œÑz‚ü©2 = 1
Correlation: ‚ü®œÉzœÑz‚ü©‚àí‚ü®œÉz‚ü©‚ü®œÑz‚ü©= 0

7.11. ENTANGLEMENT SUMMARY
233
State-Vector Rap Sheet 2
Name: Singlet State (Maximum Entanglement)
Wanted for: Nonlocality, Complete Quantum Weirdness
Description: The composite system as a whole is fully char-
acterized. There is no information about Alice‚Äôs or Bob‚Äôs
subsystems.
State-Vector:
1
‚àö
2

|ud‚ü©‚àí|du‚ü©

Normalization: œà‚àó
uuœàuu + œà‚àó
udœàud + œà‚àó
duœàdu + œà‚àó
ddœàdd = 1
Density Matrix:
Full Composite System: œÅ2 = œÅ, and Tr(œÅ2) = 1.
Alice‚Äôs Subsystem: Density matrix is proportional to the unit
matrix, having equal eigenvalues that add up to 1. Hence,
each measurement outcome is equally likely. œÅ2 Ã∏= œÅ, and
Tr(œÅ2) < 1.
Wave Function: Not Factorized: œà(a, b)
Expectation Values:
‚ü®œÉz‚ü©,‚ü®œÉx‚ü©,‚ü®œÉy‚ü©= 0
‚ü®œÑz‚ü©,‚ü®œÑx‚ü©,‚ü®œÑy‚ü©= 0
‚ü®œÑzœÉz‚ü©,‚ü®œÑxœÉx‚ü©,‚ü®œÑyœÉy‚ü©= ‚àí1
Correlation: ‚ü®œÉzœÑz‚ü©‚àí‚ü®œÉz‚ü©‚ü®œÑz‚ü©= ‚àí1

234
LECTURE 7. MORE ON ENTANGLEMENT
State-Vector Rap Sheet 3
Name: ‚ÄúNear-Singlet‚Äù (Partial Entanglement)
Wanted for: Indecision, General Wishy-Washiness, Trou-
ble Telling up from down
Description: There is some information about the compos-
ite system, and some about each subsystem. Incomplete in
each case.
State-Vector:
‚àö
0.6|ud‚ü©‚àí
‚àö
0.4|du‚ü©
Normalization:
œà‚àó
uuœàuu + œà‚àó
udœàud + œà‚àó
duœàdu + œà‚àó
ddœàdd = 1
Density Matrix:
Full Composite System: œÅ2 Ã∏= œÅ, and Tr(œÅ2) < 1.
Alice‚Äôs Subsystem: œÅ2 Ã∏= œÅ, and Tr(œÅ2) < 1.
Wave Function: Not Factorized: œà(a, b)
Expectation Values:
‚ü®œÉz‚ü©= 0.2
‚ü®œÉx‚ü©, ‚ü®œÉy‚ü©= 0; ‚ü®œÑz‚ü©= ‚àí0.2
‚ü®œÑx‚ü©, ‚ü®œÑy‚ü©= 0
‚ü®œÑzœÉz‚ü©= ‚àí1
‚ü®œÑxœÉx‚ü©= ‚àí2
‚àö
0.24
Correlation: ‚ü®œÉzœÑz‚ü©‚àí‚ü®œÉz‚ü©‚ü®œÑz‚ü©= ‚àí0.96 for this example.
For partially entangled states in general, correlation is be-
tween ‚àí1 and +1, but not exactly 0.
Exercise 7.11: Calculate Alice‚Äôs density matrix for œÉz for
the ‚Äúnear-singlet‚Äù state.
Exercise 7.12:
Verify the numerical values in each rap
sheet.

Lecture 8
Particles and Waves
Art and Lenny have had enough entanglement for now. They‚Äôre
ready for something simpler.
Lenny: Hey Hilbert, do you have anything in one dimension?
Hilbert: Let me check. Single dimensions are very popular
lately. Sometimes we run out.
Art: I‚Äôd settle for something classical, if that‚Äôs all you have.
Hilbert: Not here, friend. We‚Äôd lose our license.
Art: Good point.
To the person in the street, quantum mechanics is all about
light being particles and electrons being waves. But up until
now, I‚Äôve hardly mentioned particles, and the only mention
of waves has been the wave function, which so far has had
nothing to do with waves. So when do we get to the ‚Äúreal‚Äù
quantum mechanics?
235

236
LECTURE 8. PARTICLES AND WAVES
The answer, of course, is that real quantum mechanics
is not so much about particles and waves as it is about
the nonclassical logical principles that govern their behav-
ior. Particle-wave duality is an easy extension of the things
you‚Äôve already learned, as we‚Äôll see in this lecture. But before
we get into the physics, I want to review some mathematics,
some of which is old‚Äîit appeared in earlier lectures‚Äîand
some of which is new.
8.1
Mathematical Interlude:
Working with Continuous
Functions
8.1.1
Wave Function Review
We‚Äôll be using the language of wave functions in this lecture,
so let‚Äôs review some of that material before we dive in. In
Lecture 5, we discussed wave functions as abstract objects,
without explaining what they had to do with either waves
or functions. Before correcting this omission, I will review
what we discussed earlier.
Begin by picking an observable L, with eigenvalues Œª and
eigenvectors |Œª‚ü©. Let |Œ®‚ü©be a state-vector. Since the eigen-
vectors of a Hermitian operator form a complete orthonormal
basis, the vector |Œ®‚ü©can be expanded as
|Œ®‚ü©=

Œª
œà(Œª) |Œª‚ü©.
(8.1)
As you recall from Sections 5.1.2 and 5.1.3, the quantities

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
237
œà(Œª)
are called the wave function of the system. But notice: the
speciÔ¨Åc form of œà(Œª) depends on the speciÔ¨Åc observable L
that we initially choose. If we pick a diÔ¨Äerent observable, the
wave function (along with the basis vectors and eigenvalues)
will be diÔ¨Äerent, even though we‚Äôre still talking about the
same state. Therefore, we should qualify the statement that
œà(Œª) is the wave function associated with |Œ®‚ü©. To be more
precise, we should say that œà(Œª) is the wave function in the
L-basis. If we use the orthonormality properties of the basis
vectors,
‚ü®Œªi|Œªj‚ü©= Œ¥ij,
then the wave function in the L-basis may also be identiÔ¨Åed
with the inner products (or projections) of the state-vector
|Œ®‚ü©onto the eigenvectors |Œª‚ü©:
œà(Œª) = ‚ü®Œª|Œ®‚ü©.
You can think of the wave function in two ways. First of all,
it is the set of components of the state-vector in a particular
basis. These components can be stacked up to form a column
vector:
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éù
œà(Œª1)
œà(Œª2)
œà(Œª3)
œà(Œª4)
œà(Œª5)
‚éû
‚éü
‚éü
‚éü
‚éü
‚é†
.

238
LECTURE 8. PARTICLES AND WAVES
Another way to think of the wave function is as a function of
Œª. If you specify any allowable value of Œª, the function œà(Œª)
produces a complex number. One can therefore say that
œà(Œª)
is a complex-valued function of the discrete variable Œª. When
thought of in this way, linear operators become operations
that are applied to functions, and give back new functions.
One last reminder: the probability for an experiment to
have outcome Œª is
P(Œª) = œà‚àó(Œª)œà(Œª).
8.1.2
Functions as Vectors
Up until now, the systems we have studied have had Ô¨Ånite
dimensional state-vectors. For example, the simple spin is
described by a two-dimensional space of states. For this rea-
son, the observables have had only a Ô¨Ånite number of possible
observable values. But there are more complicated observ-
ables that can have an inÔ¨Ånite number of values. An example
is a particle. The coordinates of a particle are observables,
but, unlike spin, the coordinates have an inÔ¨Ånite number of
possible values. For instance, a particle moving along the x
axis can be found at any real value of x. In other words, x
is a continuously inÔ¨Ånite variable. When the observables of
a system are continuous, the wave function truly becomes
a function of a continuous variable. To apply quantum me-
chanics to this kind of system, we have to expand the idea

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
239
of vectors to include functions.
Functions are functions, and vectors are vectors‚Äîthey
seem like diÔ¨Äerent things, so in what sense are functions
vectors? If you think of vectors as arrows pointing in three-
dimensional space, then they are not the same as functions.
But if you take the broader view of vectors as a set of math-
ematical objects satisfying certain postulates, then functions
can indeed form a vector space. Such a vector space is of-
ten called a Hilbert space after the mathematician David
Hilbert.
Let‚Äôs consider the set of complex functions œà(x) of a sin-
gle real variable x. By complex functions, I mean that for
each x, œà(x) is a complex number. On the other hand, the
independent variable x is an ordinary real variable. It can
take on any real value from ‚àí‚àûto +‚àû.
Now, let‚Äôs nail down what we mean when we say ‚ÄúFunc-
tions are vectors.‚Äù This is not a loose analogy or a metaphor.
With appropriate restrictions (that we‚Äôll come back to), func-
tions like œà(x) satisfy the mathematical axioms that deÔ¨Åne
a vector space. We mentioned this idea brieÔ¨Çy in Section
1.9.2, and now we‚Äôll make full use of it. Looking back at the
axioms that deÔ¨Åne a complex vector space (in Section 1.9.1),
we can see that complex functions satisfy all of them:
1. The sum of any two functions is a function.
2. The addition of functions is commutative.
3. The addition of functions is associative.
4. There is a unique zero function such that when you add
it to any function, you get the same function back.

240
LECTURE 8. PARTICLES AND WAVES
5. Given any function œà(x), there is a unique function
‚àíœà(x) such that œà(x) + (‚àíœà(x)) = 0.
6. Multiplying a function by any complex number gives a
function and is linear.
7. The distributive property holds, which means that
z[œà(x) + œÜ(x)] = zœà(x) + zœÜ(x)
[z + w]œà(x) = zœà(x) + wœà(x),
where z and w are complex numbers.
All of this implies that we can identify the functions œà(x)
with the ket-vectors |Œ®‚ü©in an abstract vector space. Not sur-
prisingly, we can also deÔ¨Åne bra vectors. The bra vector ‚ü®Œ®|
corresponding to the ket |Œ®‚ü©is identiÔ¨Åed with the complex
conjugate function œà‚àó(x).
To use this idea eÔ¨Äectively, we‚Äôll need to generalize some
of the items in our mathematical tool kit.
In earlier lec-
tures, the labels that identiÔ¨Åed wave functions were mem-
bers of some Ô¨Ånite discrete set‚Äîfor example, the eigenvalues
of some observable.
But now the independent variable is
continuous. Among other things, this means that we cannot
sum over it using ordinary sums. I think you know what
to do, though. Here are function-oriented replacements for
three of our vector-based concepts, two of which you will
easily recognize:
‚Ä¢ Integrals replace sums.

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
241
‚Ä¢ Probability densities replace probabilities.
‚Ä¢ Dirac delta functions replace Kronecker deltas.
Let‚Äôs look at these items more closely.
Integrals Replace Sums:
If we really wanted to be rigor-
ous, we would begin by replacing the x axis by a discrete set
of points separated by a very small distance œµ, and then take
the limit œµ ‚Üí0. It would take several pages to justify each
step. But we can avoid this trouble by a few intuitive deÔ¨Åni-
tions, such as replacing sums with integrals. Schematically,
this concept can be written as

i
‚Üí

dx.
For example, if we want to compute the area under a curve,
we divide the x axis up into tiny segments and then add up
the areas of a large number of rectangles, exactly as we do
in elementary calculus. When we let the segments shrink to
zero size, the sum becomes an integral.
Let‚Äôs consider a bra ‚ü®Œ®| and a ket |Œ¶‚ü©and deÔ¨Åne their
inner product. The obvious way to do this is to replace the
summation in Eq. 1.2 with an integral. We deÔ¨Åne the inner
product to be
‚ü®Œ®|Œ¶‚ü©=
 ‚àû
‚àí‚àû
œà‚àó(x)œÜ(x)dx.
(8.2)

242
LECTURE 8. PARTICLES AND WAVES
Probability Densities Replace Probabilities:
Later,
we will identify
P(x) = œà‚àó(x)œà(x)
as a probability density for the variable x. Why a probability
density and not just a probability? If x is a continuous vari-
able, then the probability that it will have any exact value is
typically zero. A more useful question to ask is: What is the
probability that x lies between two values, x = a and x = b?
Probability densities are deÔ¨Åned so that this probability is
given by an integral:
P(a, b) =
 b
a
P(x) dx =
 b
a
œà‚àó(x)œà(x) dx.
Because the total probability should be 1, we can deÔ¨Åne a
normalized vector by
 ‚àû
‚àí‚àû
œà‚àó(x)œà(x) dx = 1.
(8.3)
Dirac Delta Functions Replace Kronecker Deltas:
So
far, this should be very familiar. The Dirac delta function
may be less so. The delta function is the analog of the Kro-
necker delta, Œ¥ij. The Kronecker delta is deÔ¨Åned to be 0 for
i Ã∏= j and 1 for i = j. But it can also be deÔ¨Åned another way.
Consider any vector Fi in a Ô¨Ånite dimensional space. It is
easy to see that the Kronecker delta satisÔ¨Åes

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
243

j
Œ¥ijFj = Fi.
That‚Äôs because the only nonzero term in the sum is the one
where j = i. Within the summation, the Kronecker symbol
Ô¨Ålters out all the F‚Äôs except Fi. The obvious generalization is
to deÔ¨Åne a new function that has similar Ô¨Åltering properties
when used inside an integral. In other words, we want a new
entity
Œ¥(x ‚àíx‚Ä≤)
with the property that, for any function F(x),
 ‚àû
‚àí‚àû
Œ¥(x ‚àíx‚Ä≤) F(x‚Ä≤)dx‚Ä≤ = F(x).
(8.4)
Eq. 8.4 deÔ¨Ånes this new entity, called the Dirac delta func-
tion, which turns out to be an essential tool in quantum
mechanics. But despite its name, it isn‚Äôt really a function
in the usual sense. It is zero whenever x Ã∏= x‚Ä≤, but when
x = x‚Ä≤ it is inÔ¨Ånite. In fact it is just inÔ¨Ånite enough that the
area under Œ¥(x) equals 1. Roughly speaking, it is a function
that is nonzero over an inÔ¨Ånitesimal interval œµ, but on that
interval it has the value 1/œµ. Thus, its area is 1, and, more
importantly, it satisÔ¨Åes Eq. 8.4. The function
n
‚àöœÄe‚àí(nx)2

244
LECTURE 8. PARTICLES AND WAVES
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =1
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =2
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =3
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =4
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =5
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =6
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =7
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =8
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =9
8
6
4
2
0
2
4
6
8
0
1
2
3
4
5
n =10
Figure 8.1: Dirac Delta Function Approximations.
These
approximations are based on
n
‚àöœÄe‚àí(nx)2 and plotted for in-
creasing values of n.

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
245
approximates the delta function reasonably well as n be-
comes very large. Fig. 8.1 plots this approximation for in-
creasing values of n. Even though we stop at n = 10, a very
small value, notice that the graph has already become very
narrow and sharply peaked.
8.1.3
Integration by Parts
Before discussing linear operators, we‚Äôll take a short detour
to remind you of a technique called integration by parts. It‚Äôs
fairly simple, and indispensable for our purposes. We‚Äôll be
using it again and again. Suppose we take two functions,
F and G, and consider the diÔ¨Äerential of their product FG.
We can write
d(FG) = FdG + GdF
or
d(FG) ‚àíGdF = FdG.
Taking the deÔ¨Ånite integral gives us
 b
a
d(FG) ‚àí
 b
a
GdF =
 b
a
FdG
or
FG

b
a ‚àí
 b
a
GdF =
 b
a
FdG.

246
LECTURE 8. PARTICLES AND WAVES
This is the standard formula that you may remember from
calculus. But in quantum mechanics the limits of integration
tend to span the entire axis, and our wave functions must
go to zero at inÔ¨Ånity to be properly normalized. Therefore,
the Ô¨Årst term of this expression will always evaluate to zero.
With that in mind, we can use a simpliÔ¨Åed version of inte-
gration by parts:
 ‚àû
‚àí‚àû
F dG
dx dx = ‚àí
 ‚àû
‚àí‚àû
dF
dx Gdx.
This form is correct as long as F and G go to zero appropri-
ately at inÔ¨Ånity, so that the boundary term becomes zero.
You will do yourself a big favor if you just memorize this pat-
tern: Switch the derivative from one factor of the integrand
to the other at the cost of a minus sign.
8.1.4
Linear Operators
Bras and kets are half the story in quantum mechanics; the
other half is the concept of linear operators and, in particu-
lar, Hermitian operators. This raises two questions:
‚Ä¢ What is meant by a linear operator on a space of func-
tions?
‚Ä¢ What is the condition for a linear operator to be Her-
mitian?
The concept of a linear operator is simple enough: it‚Äôs a
machine that acts on a function and gives another function.

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
247
When it acts on the sum of two functions, it gives the sum
of the individual results. When it acts on a complex numer-
ical multiple of a function, it gives the same multiple of the
original result. In other words, it is (surprise!) linear.
Let‚Äôs look at some examples. One simple operation we
can perform on a function œà(x) is to multiply it by x. That
gives a new function xœà(x), and you can easily check that the
action is linear. We‚Äôll represent the ‚Äúmultiply by x‚Äù operator
with the symbol X. By deÔ¨Ånition, then,
X œà(x) = xœà(x).
(8.5)
Here‚Äôs another example. DeÔ¨Åne D to be the diÔ¨Äerentiation
operator:
D œà(x) = dœà(x)
dx
.
(8.6)
Exercise 8.1: Prove that X and D are linear operators.
This, of course, is a minute subset of the possible linear op-
erators that can be constructed, but we will soon see that X
and D play a very central role in the quantum mechanics of
particles.
Now, let‚Äôs consider the property of Hermiticity. A con-
venient way to deÔ¨Åne a Hermitian operator is through its
matrix elements, by sandwiching it between a bra and a ket.
You can sandwich an operator L in two diÔ¨Äerent ways:

248
LECTURE 8. PARTICLES AND WAVES
‚ü®Œ®|L|Œ¶‚ü©
or
‚ü®Œ¶|L|Œ®‚ü©.
In general, there is no simple relation between these two
sandwiches. But in the case of a Hermitian operator (for
which, by deÔ¨Ånition, L‚Ä† = L) there is a simple relation: the
two sandwiches are complex conjugates of each other:
‚ü®Œ®|L|Œ¶‚ü©= ‚ü®Œ¶|L|Œ®‚ü©‚àó.
Let‚Äôs see whether the operators X and D are Hermitian.
Recalling that
X œà(x) = xœà(x),
and using the inner product formula Eq. 8.2, we can write
‚ü®Œ®|X|Œ¶‚ü©=

œà‚àó(x)xœÜ(x)dx
and
‚ü®Œ¶|X|Œ®‚ü©=

œÜ‚àó(x)xœà(x)dx.
Because x is real, it‚Äôs easy to see that these two integrals are
complex conjugates of each other, and therefore that X is
Hermitian.

8.1. INTERLUDE: CONTINUOUS FUNCTIONS
249
What about the operator D? In this case, the two sand-
wiches are
‚ü®Œ®|D|Œ¶‚ü©=

œà‚àó(x)dœÜ(x)
dx
dx
(8.7)
and
‚ü®Œ¶|D|Œ®‚ü©=

œÜ‚àó(x)dœà(x)
dx
dx.
(8.8)
To determine if D is Hermitian, we need to compare these
two integrals and see if they are complex conjugates of each
other. In this form, it‚Äôs a bit diÔ¨Écult to tell. The trick is to
do the second integral by parts. As we explained, integration
by parts allows you to switch the derivative from one factor
in the integrand to the other, as long as you change the sign
at the same time. Therefore, the integral in Eq. 8.8 can be
rewritten as
‚ü®Œ¶|D|Œ®‚ü©= ‚àí

œà(x)dœÜ‚àó(x)
dx
dx.
(8.9)
Now, we just need to compare the two expressions in Eqs. 8.7
and 8.9, which turns out to be easy. Because of the minus
sign, it‚Äôs clear that they are deÔ¨Ånitely not complex conju-
gates of each other. Instead, their relationship is captured
by
‚ü®Œ®|D|Œ¶‚ü©= ‚àí‚ü®Œ¶|D|Œ®‚ü©‚àó,
which is the diametric opposite of what we wanted. Unlike
the X operator, D is not Hermitian. Instead, it satisÔ¨Åes

250
LECTURE 8. PARTICLES AND WAVES
D‚Ä† = ‚àíD.
An operator with this property is called anti-Hermitian.
Although anti-Hermitian and Hermitian operators are
opposites, it‚Äôs very easy to go from one to the other. All
you have to do is multiply by the imaginary number i or
‚àíi. Therefore we can use D to construct an operator that is
Hermitian, namely
‚àíi¬ØhD.
If we look at the action of this new Hermitian operator on
wave functions, we Ô¨Ånd that
‚àíi¬ØhDœà(x) = ‚àíi¬Øh dœà(x)
dx
.
(8.10)
Keep this formula in mind. It will soon play a leading role
in deÔ¨Åning a very important property of particles‚Äîtheir mo-
mentum.
8.2
The State of a Particle
In classical mechanics, the ‚Äústate of a system‚Äù means every-
thing you need to know to predict the system‚Äôs future, given
the forces acting on it. That, of course, means the positions
of all the particles comprising the system, as well as the mo-
menta of those particles. From a classical perspective, the
instantaneous positions and momenta are entirely indepen-
dent variables. For example, for a particle of mass m moving

8.2. THE STATE OF A PARTICLE
251
along a one-dimensional axis x, the momentary state of the
system is described by the pair (x, p). The coordinate x is
the location of the particle, and p = m Àôx is its momentum.
Taken together, these two variables deÔ¨Åne the phase space
of the system. If we also know the force on the particle as
a function of its position, Hamilton‚Äôs equations permit us to
calculate its position and momentum at all later times. They
deÔ¨Åne a Ô¨Çow through the phase space.
Given this, one might guess that the quantum state of
a particle would be spanned by a basis of states labeled by
position and momentum:
|x, p‚ü©.
The wave function would then be a function of both vari-
ables:
œà(x, p) = ‚ü®x, p|Œ®‚ü©.
However, this is incorrect. We‚Äôve already seen that things
that would be simultaneously knowable in classical physics
may not be in quantum mechanics. DiÔ¨Äerent components of
a spin, say œÉz and œÉx, are an example. One cannot know
both components simultaneously; therefore, one does not
have states in which both components are speciÔ¨Åed.
The
same is true for x and p: specifying both values is too much.
Whether we‚Äôre talking about spins (œÉz, œÉx) or positions and
momenta (x, p), the incompatibility is ultimately an experi-
mental fact.

252
LECTURE 8. PARTICLES AND WAVES
What then can we know about the particle on the x axis,
if not x and p? The answer is x or p; for according to the
mathematics of position and momentum operators, the two
do not commute. But I emphasize that this is not something
you could have predicted in advance; it is the distillation of
many decades of experimental observations.
If the position of a particle is an observable, there must
be a Hermitian operator associated with it.
The obvious
candidate is the operator X. The Ô¨Årst step in understanding
this fundamental connection between the intuitive concept
of position and the mathematical operator X is to work out
the eigenvectors and eigenvalues of X. The eigenvalues are
the possible values of position that can be observed, and the
eigenvectors represent the states of deÔ¨Ånite position.
8.2.1
The Eigenvalues and Eigenvectors
of Position
The obvious next question is: What are the possible out-
comes of measuring X, and what are the states in which it
has a deÔ¨Ånite (predictable) value? In other words, what are
its eigenvalues and eigenvectors? We‚Äôll start with X. The
eigen-equation for X is
X|Œ®‚ü©= x0|Œ®‚ü©,
where the eigenvalue is denoted by x0. In terms of wave func-
tions, this becomes
xœà(x) = x0œà(x).
(8.11)

8.2. THE STATE OF A PARTICLE
253
This last equation seems strange. How can x times a function
be proportional to the same function? On the face of it, this
seems impossible. But let‚Äôs pursue it. We can rewrite Eq.
8.11 in the form
(x ‚àíx0)œà(x) = 0.
Of course, if a product is zero, then at least one of the factors
must be zero. But the other factors may be diÔ¨Äerent from
zero. Thus, if x Ã∏= x0, then œà(x) = 0. That‚Äôs a very strong
condition. It says that for a given eigenvalue x0, the function
œà(x) can be nonzero at only one point, namely at
x = x0.
For an ordinary continuous function this condition would be
deadly: no sensible function can be zero everywhere except
at one point, and be nonzero only at that point. But that is
exactly the property of the Dirac delta function
Œ¥(x ‚àíx0).
Evidently, then, every real number x0 is an eigenvalue of X,
and the corresponding eigenvectors are functions (we often
call them eigenfunctions) that are inÔ¨Ånitely concentrated at
x = x0. The meaning of this is clear: the wave functions
œà(x) = Œ¥(x ‚àíx0)

254
LECTURE 8. PARTICLES AND WAVES
represent states in which the particle is located right at the
point x0 on the x axis.
It of course makes a lot of sense that the wave function
representing a particle known to be at x0 is zero everywhere
except at x0. How could it be otherwise? But it is gratifying
to see the mathematics conÔ¨Årm this intuition.
Consider the inner product of a state |Œ®‚ü©and a position
eigenstate |x0‚ü©:
‚ü®x0|Œ®‚ü©.
Using Eq. 8.2, we get
‚ü®x0|Œ®‚ü©=
 ‚àû
‚àí‚àû
Œ¥(x ‚àíx0)œà(x).
By the deÔ¨Ånition of delta functions given in Eq. 8.4, this
integral evaluates to
‚ü®x0|Œ®‚ü©= œà(x0).
(8.12)
Because this is true for any x0, we can drop the subscript
and write the general equation
‚ü®x|Œ®‚ü©= œà(x).
(8.13)
In other words, the wave function, œà(x), of a particle moving
in the x direction is the projection of a state-vector |Œ®‚ü©onto
the eigenvectors of position. We will also refer to œà(x) as
the wave function in the position representation.

8.2. THE STATE OF A PARTICLE
255
8.2.2
Momentum and Its Eigenvectors
Position is intuitive; momentum is less so, particularly in
quantum mechanics. It will only be later that we see the
connection between the operator that we identify with mo-
mentum and the familiar classical concept of mass times ve-
locity. But I assure you that we will make the connection.
For now, let‚Äôs take the abstract mathematical route. The
momentum operator in quantum mechanics is called P, and
it is deÔ¨Åned in terms of the operator ‚àíiD:
‚àíiD = ‚àíi d
dx .
As we saw earlier in Eq. 8.10, we need the factor ‚àíi to make
this operator Hermitian.
We could just deÔ¨Åne P to be ‚àíiD, but if we did, we
would run into a problem later when we connect these ideas
to those of classical physics. The reason should be clear‚Äî
there‚Äôs a dimensional mismatch.
In classical physics, the
units of momentum are mass times velocity‚Äîin other words,
mass times length divided by time (ML/T). On the other
hand, the operator D has units of inverse length, or 1/L. The
resolution of the mismatch is provided by Planck‚Äôs constant
¬Øh, which has units of ML2/T. The correct relation between
P and D is therefore
P = ‚àíi¬ØhD
(8.14)
or, in terms of its action on wave functions,
Pœà(x) = ‚àíi¬Øh dœà(x)
dx
.
(8.15)

256
LECTURE 8. PARTICLES AND WAVES
Quantum physicists often use units in which ¬Øh is exactly one,
and in that way simplify the equations. As tempting as it is,
we won‚Äôt do that here.
Let‚Äôs work out the eigenvectors and eigenvalues of P. The
eigen-equation in abstract vector notation is
P|Œ®‚ü©= p|Œ®‚ü©,
(8.16)
where the symbol p is an eigenvalue of P. Eq. 8.16 can also be
expressed in terms of wave functions. Using the identiÔ¨Åcation
P = ‚àíi¬Øh d
dx ,
we can write the eigen-equation as
‚àíi¬Øhdœà(x)
dx
= pœà(x)
or
dœà(x)
dx
= ip
¬Øh œà(x).
This is a type of equation that we‚Äôve run into before. The
solution has the form of an exponential:
œàp(x) = Ae
ipx
¬Øh .
The subscript p is just a reminder that œàp(x) is the eigen-
vector of P with the speciÔ¨Åc eigenvalue p. It is a function of
x, but it is labeled by an eigenvalue of P.

8.2. THE STATE OF A PARTICLE
257
The constant A multiplying the exponential is not deter-
mined by the eigenvector equation. That‚Äôs nothing new; the
eigenvalue equation never tells us the overall normalization
of the wave function. As a rule, we Ô¨Åx the constant by requir-
ing the wave function to be normalized to unit probability.
An example that goes all the way back to Section 2.3 is the
eigenvector of the x component of spin:
|r‚ü©= 1
‚àö
2|u‚ü©+ 1
‚àö
2|d‚ü©.
The factor 1/
‚àö
2 is there to make sure the total probability
is 1.
Normalizing the eigenvectors of P is a more subtle oper-
ation, but the result is simple. The factor A is only slightly
more complicated than in the spin case. To save time, I will
tell you the answer and leave it for you to prove later. The
correct factor is A = 1/
‚àö
2œÄ. Thus,
œàp(x) =
1
‚àö
2œÄe
ipx
¬Øh .
(8.17)
A point of some interest follows from Eqs. 8.13 and 8.17. The
inner product of a position eigenvector |x‚ü©and a momentum
eigenvector |p‚ü©has a very simple and symmetric form:
‚ü®x|p‚ü©
=
1
‚àö
2œÄe
ipx
¬Øh
‚ü®p|x‚ü©
=
1
‚àö
2œÄe
‚àíipx
¬Øh .
(8.18)

258
LECTURE 8. PARTICLES AND WAVES
The second equation is simply the complex conjugate of the
Ô¨Årst. These results are easy to verify if you keep in mind that
|x‚ü©is represented by a delta function. I‚Äôd like to mention two
important points before moving further:
1. Eq. 8.17 represents a momentum eigenfunction in the
position basis. In other words, although it represents
a momentum eigenstate, it is a function of x, and not
an explicit function of p.
2. We‚Äôve been using the symbol œà for both position and
momentum eigenstates. A mathematician might not
approve of using the same symbol for two diÔ¨Äerent
functions, but physicists do it all the time. œà(x) is just
the generic symbol for whatever function we happen to
be discussing.
At this juncture, we begin to get a glimmer of why the wave
function is called the wave function. What you should notice
is that the eigenfunctions (wave functions representing eigen-
vectors) of the momentum operator have the form of waves‚Äî
sine waves and cosine waves, to be precise. In fact, we can
now see one of the most fundamental aspects of the wave-
particle duality of quantum mechanics. The wavelength of
the function
e
ipx
¬Øh
is given by

8.2. THE STATE OF A PARTICLE
259
Œª = 2œÄ¬Øh
p
because the value of the function is unchanged if we add 2œÄ¬Øh
p
to the variable x:
e
ip(x+ 2œÄ¬Øh
p
)
¬Øh
= e
ipx
¬Øh e2œÄi = e
ipx
¬Øh .
Let‚Äôs pause for a moment to discuss the importance of this
connection between momentum and wavelength.
It‚Äôs not
just important: in many ways, it is the relationship that
deÔ¨Åned twentieth-century physics.
Over the last hundred
years, physicists have primarily been concerned with uncov-
ering the laws of the microscopic world.
This has meant
Ô¨Åguring out how objects are built out of smaller objects.
The examples are obvious: molecules are made from atoms;
atoms from electrons and nuclei; nuclei from protons and
neutrons. These subnuclear particles are constructed out of
quarks and gluons. And the game goes on as scientists search
for ever smaller and more hidden entities.
All of these objects are too small to see with the best
optical microscopes, let alone the naked eye. The reason is
not just that our eyes are insuÔ¨Éciently sensitive. The more
important fact is that eyes and optical microscopes are sensi-
tive to the visible spectrum, which comprises wavelengths at
least a few thousand times longer than the size of an atom.
As a rule, you can‚Äôt resolve objects much smaller than the
wavelength you‚Äôre using to look at them. For this reason, the
story of twentieth-century physics was in large part a quest

260
LECTURE 8. PARTICLES AND WAVES
for smaller and smaller wavelengths of light‚Äîor any other
kind of wave. In Lecture 10, we will discover that light of a
given wavelength is composed of photons whose momentum
is related to the wavelength by exactly the relation
Œª = 2œÄ¬Øh
p .
The implication is that to probe objects of ever smaller size
one needs photons (or other objects) of ever larger momen-
tum. Large momentum inevitably means large energy. It‚Äôs
for that reason that the discovery of the microscopic proper-
ties of matter required increasingly powerful particle accel-
erators.
8.3
Fourier Transforms and the
Momentum Basis
The wave function œà(x) has the important role of determin-
ing the probability for Ô¨Ånding the particle at position x:
P(x) = œà‚àó(x)œà(x).
As we will see, no experiment can determine both the posi-
tion and momentum of a particle simultaneously. But if we
forego determining anything about the position, momentum
can be measured precisely. The situation is quite analogous
to that of the x and z components of a spin. Either value
can be measured, but not both.

8.3. FOURIER TRANSF/MOMENTUM BASIS
261
What is the probability that a particle has momentum
p if we choose to measure it? The answer is a straightfor-
ward generalization of the principles laid down in Lecture
3. The probability that a momentum measurement will give
momentum p is
P(p) = | ‚ü®P|Œ®‚ü©|2.
(8.19)
The entity ‚ü®P|Œ®‚ü©is called the wave function of |Œ®‚ü©in the
momentum representation. Naturally, it is a function of p
and is denoted by a new symbol:
Àúœà(p) = ‚ü®P|Œ®‚ü©.
(8.20)
It is now clear that there are two ways to represent a state-
vector. One way is in the position basis and the other is
in the momentum basis. Both wave functions‚Äîthe position
wave function œà(x) and the momentum wave function Àúœà(p)‚Äî
represent exactly the same state-vector |Œ®‚ü©. It follows that
there must be some transformation between them such that
if you know œà(x), the transformation produces Àúœà(p), and vice
versa. In fact, the two representations are Fourier transforms
of each other.
8.3.1
Resolving the Identity
We are about to see the great power of the Dirac bra-ket
notation in simplifying complicated things. First, let‚Äôs recall
an important idea from earlier lectures. Suppose we deÔ¨Åne
an orthonormal basis of states through the eigenvectors of

262
LECTURE 8. PARTICLES AND WAVES
some Hermitian observable.
Call the basis vectors |i‚ü©. In
Lecture 7, I explained a very useful trick, and now we are
going to see just how useful it is. It‚Äôs called resolving the
identity. The trick given in (Eq. 7.11) is to write the identity
operator I (the operator that acts on any vector to give the
same vector) in the form
I =

i
|i‚ü©‚ü®i|.
Because momentum and position are both Hermitian, the
sets of vectors |x‚ü©and set |p‚ü©each deÔ¨Åne basis vectors. By
replacing summation with integration we discover two ways
to resolve the identity:
I =

dx|x‚ü©‚ü®x|
(8.21)
and
I =

dp|p‚ü©‚ü®p|.
(8.22)
Let‚Äôs suppose that we know the wave function of the abstract
vector |Œ®‚ü©in the position representation. By deÔ¨Ånition, it is
equal to
œà(x) = ‚ü®x|Œ®‚ü©.
(8.23)
Now suppose we want to know the wave function Àúœà(p) in
the momentum representation. Here are the steps laid out
in detail:

8.3. FOURIER TRANSF/MOMENTUM BASIS
263
‚Ä¢ First, use the deÔ¨Ånition of the momentum-representation
wave function:
Àúœà(p) = ‚ü®p|Œ®‚ü©.
‚Ä¢ Now, insert the unit operator between the bra- and
ket-vectors, in the form given in Eq. 8.21:
Àúœà(p) =

dx‚ü®p|x‚ü©‚ü®x|Œ®‚ü©.
‚Ä¢ The expression ‚ü®x|Œ®‚ü©is just the wave function œà(x),
and ‚ü®p|x‚ü©is given to us by the second equation of Eqs.
8.18:
‚ü®p|x‚ü©=
1
‚àö
2œÄe
‚àíipx
¬Øh .
‚Ä¢ Putting it all together, we Ô¨Ånd that
Àúœà(p) =
1
‚àö
2œÄ

dxe
‚àíipx
¬Øh œà(x).
(8.24)
This equation shows us exactly how to transform a given
wave function in the position representation into the cor-
responding wave function in the momentum representation.
What is it good for? Suppose the position wave function
for some particle is known; however, the goal of your exper-
iment is to measure the momentum, and you want to know

264
LECTURE 8. PARTICLES AND WAVES
the probability of observing momentum p. The procedure is
to Ô¨Årst calculate Àúœà(p) by using Eq. 8.24 and then compute
the probability
P(p) = Àúœà‚àó(p) Àúœà(p).
It‚Äôs just as easy to go the other way.
Suppose we know
Àúœà(p) and wish to recover œà(x). This time, we use Eq. 8.22
to resolve the identity. Here are the steps (notice that they
look suspiciously similar to the earlier ones):
‚Ä¢ First, use the deÔ¨Ånition of the position-representation
wave function:
œà(x) = ‚ü®x|Œ®‚ü©
‚Ä¢ Now, insert the unit operator between the bra- and
ket-vectors, in the form given in Eq. 8.22:
œà(x) =

dp‚ü®x|p‚ü©‚ü®p|Œ®‚ü©.
‚Ä¢ The expression ‚ü®p|Œ®‚ü©is just the wave function Àúœà(p),
and ‚ü®x|p‚ü©is given to us by Eqs. 8.18. But this time,
it‚Äôs the Ô¨Årst of the two equations.
‚ü®x|p‚ü©=
1
‚àö
2œÄe
ipx
¬Øh .

8.4. COMMUTATORS AND POISSON BRACKETS 265
‚Ä¢ Putting it all together, we Ô¨Ånd that
œà(x) =
1
‚àö
2œÄ

dpe
ipx
¬Øh Àúœà(p).
Let‚Äôs take another look at the two equations for going back
and forth from position to momentum. Notice how symmet-
rical they are.
The only asymmetry is that one equation
contains e
ipx
¬Øh and the other contains e
‚àíipx
¬Øh :
Àúœà(p)
=
1
‚àö
2œÄ

dxe
‚àíipx
¬Øh œà(x)
œà(x)
=
1
‚àö
2œÄ

dpe
ipx
¬Øh Àúœà(p).
(8.25)
The relation between the position and momentum represen-
tations summarized by Eqs. 8.25 is that they are reciprocal
Fourier transforms of one another.
In fact, these are the
central equations in the Ô¨Åeld of Fourier analysis. I want you
to notice how easy it was to derive those equations using
Dirac‚Äôs elegant notation.
8.4
Commutators and Poisson
Brackets
Earlier, in Lecture 4, we formulated two important principles
about commutators. The Ô¨Årst had to do with the connec-
tion between classical mechanics and quantum mechanics;

266
LECTURE 8. PARTICLES AND WAVES
the second had to do with uncertainty. I now will Ô¨Ånish up
this very long lecture by showing you what these principles
have to do with X and P.
We‚Äôll start with the connection between commutators
and classical physics.
As you may recall, we found that
commutators have a great similarity to Poisson brackets, a
relationship we made explicit in Eq. 4.21. If we plug in the
operator symbols L and M that we‚Äôve been using in this
lecture, we get
[L, M]
‚áê‚áí
i¬Øh{L, M},
(8.26)
and we‚Äôre reminded that the equations for quantum motion
strongly resemble their classical equivalents. This suggests
that we may learn something by computing the commutator
of the observables X and P. Fortunately, this is easy to do.
First, let‚Äôs see what the product XP does when it acts
as an operator on an arbitrary wave function œà(x). Recalling
Eqs. 8.5 and 8.15, we can write
Xœà(x) =
xœà(x)
Pœà(x) =
‚àíi¬Øhdœà(x)
dx
.
Together, these equations tell us how the product XP acts
on œà(x):
XPœà(x) = ‚àíi¬Øhx dœà(x)
dx
(8.27)
Now, let‚Äôs try it with X and P in the opposite order:

8.4. COMMUTATORS AND POISSON BRACKETS 267
PXœà(x) = ‚àíi¬Øh d

xœà(x)

dx
.
To calculate this last expression, we just use the standard
rule for diÔ¨Äerentiating the product xœà(x). Using this rule,
it‚Äôs easy to see that
PXœà(x) = ‚àíi¬Øhx dœà(x)
dx
‚àíi¬Øhœà(x).
(8.28)
Now, we‚Äôll subtract Eq. 8.28 from Eq. 8.27 to show how the
commutator acts on the wave function:
[X, P]œà(x) = XPœà(x) ‚àíPXœà(x)
or
[X, P]œà(x) = i¬Øhœà(x).
In other words, when the commutator [X, P] acts on any
wave function œà(x), all it does is multiply œà(x) by the num-
ber i¬Øh. We can express this by writing
[X, P] = i¬Øh.
(8.29)
This in itself is extremely important. The fact that X and P
don‚Äôt commute is the key to understanding why they are not
simultaneously measurable. But things get even more inter-
esting when we compare this equation with Equivalence 8.26,
which relates commutators to classical Poisson brackets. In

268
LECTURE 8. PARTICLES AND WAVES
fact, Eq. 8.29 suggests that the corresponding classical Pois-
son bracket is
{x, p} = 1,
which is exactly the classical relation between coordinates
and their conjugate momenta (see Volume I, Lecture 10, Eq.
8). Ultimately, it is this connection that explains why the
quantum concept of momentum is connected to the classical
concept.
Using the general uncertainty principle from Lecture 5,
we can now specialize to the case
[X, P] = i¬Øh.
and
ŒîXŒîP ‚â•¬Øh
2.
We‚Äôll do that in the next section.
Now let‚Äôs recall the second principle involving commuta-
tors. In Lecture 4, we found that two observables L and M
cannot be determined simultaneously unless they commute.
If they don‚Äôt commute, you cannot measure L without in-
terfering with a measurement of M. It is not possible to Ô¨Ånd
simultaneous eigenvectors of two noncommuting observables.
This led to the general uncertainty principle.

8.5.
HEISENBERG UNCERTAINTY PRINCIPLE
269
8.5
The Heisenberg Uncertainty
Principle
And now, ladies and gentlemen, here‚Äôs what you‚Äôve all been
waiting for. At long last: the Heisenberg Uncertainty Prin-
ciple.
The Heisenberg Uncertainty Principle is one of the most
famous results of quantum mechanics: it not only asserts
that the position and momentum of a particle cannot be
simultaneously known, but it also provides an exact quan-
titative limit for their mutual uncertainties. At this point,
I suggest that you revisit Lecture 5, where I explained the
general uncertainty principle. We did all the work there, and
now we get to reap the beneÔ¨Åts.
As we‚Äôve seen, the general uncertainty principle puts a
quantitative limit on the simultaneous uncertainties of two
observables A and B. This idea was captured in Inequality
5.13:
ŒîA ŒîB ‚â•1
2|‚ü®Œ®|[A, B]|Œ®‚ü©|.
Now let‚Äôs apply this principle directly to the position and
momentum operators X and P. In this case, the commutator
is just a number and its expectation is that same number.
Replacing A and B with X and P gives
ŒîX ŒîP ‚â•1
2|‚ü®Œ®|[X, P]|Œ®‚ü©|,
and replacing [X, P] with i¬Øh results in

270
LECTURE 8. PARTICLES AND WAVES
ŒîX ŒîP ‚â•1
2|i¬Øh‚ü®Œ®|Œ®‚ü©|.
But ‚ü®Œ®|Œ®‚ü©equals 1, and the end result is
ŒîX ŒîP ‚â•1
2¬Øh.
No experiment can ever beat this limitation. You can try
your best to determine a particle‚Äôs momentum and position
simultaneously in a reproducible manner, but no matter how
careful you are, the uncertainty in the position times the
uncertainty in the momentum will never be less than 1
2¬Øh.
As we saw in Section 8.2.1, the wave function of an eigen-
state of X is highly concentrated about some point x0; in this
eigenstate, the probability is also perfectly localized. On the
other hand, the probability P(x) for a momentum eigenstate
is uniformly spread over the entire x axis. To see this, let‚Äôs
take the wave function in Eq. 8.17 and multiply it by its
complex conjugate:
œà‚àó
p(x)œàp(x) =

1
‚àö
2œÄe
‚àíipx
¬Øh 
1
‚àö
2œÄe
ipx
¬Øh 
= 1
2œÄ.
The result is completely uniform, with no peaks anywhere
on the x axis. Evidently, a state with deÔ¨Ånite momentum is
completely uncertain in its position.
Fig. 8.2 illustrates the deÔ¨Ånition of uncertainty for the
position variable x. In the top half of the Ô¨Ågure, you can
see that the uncertainty Œîx is a measure of how spread out
the function is in relation to its expectation value ‚ü®x‚ü©. The

8.5.
HEISENBERG UNCERTAINTY PRINCIPLE
271
label d shows the deviation of one point in relation to ‚ü®x‚ü©;
this may be a positive or negative quantity. The uncertainty
Œîx is the result of an averaging process over all possible d‚Äôs
and characterizes the function as a whole. To prevent the
positive d‚Äôs from canceling the negative ones, each d value is
squared during this averaging process.
The bottom half of Fig. 8.2 shows how the calculation
can be simpliÔ¨Åed by shifting the origin to coincide with ‚ü®x‚ü©.
The numerical value of Œîx is unchanged by this shift.
Figure 8.2: Uncertainty Basics.
Top: ‚ü®x‚ü©to right of ori-
gin. Deviations d may be positive or negative. Overall un-
certainty Œîx (> 0) derived from the average value of d2.
Bottom: Origin shifted right, ‚ü®x‚ü©= 0, Œîx has same value.


Lecture 9
Particle Dynamics
Art and Lenny expected some action at Hilbert‚Äôs Place. But
all the state-vectors were absolutely still‚Äîfrozen, you might
say.
Lenny: This is boring, Art. Doesn‚Äôt anything ever happen
around here? Hey Hilbert, why is this joint so still?
Hilbert: Oh, don‚Äôt worry. Things will pick up as soon as the
Hamiltonian gets here.
Art: The Hamiltonian? He sounds like a real operator.
9.1
A Simple Example
The Ô¨Årst two volumes of the Theoretical Minimum series
have largely focused on two questions. The Ô¨Årst is: What do
we mean by a system and how do we describe the momentary
states of a system? As we‚Äôve seen, the classical and quantum
answers to this question are very diÔ¨Äerent. Classical phase
273

274
LECTURE 9. PARTICLE DYNAMICS
space‚Äîthe space of coordinates and momenta‚Äîis replaced
in quantum theory by the linear vector space of states.
The second big question is: How do states change with
time?
In both classical mechanics and quantum mechan-
ics, the answer is according to the minus Ô¨Årst law. In other
words, states change so that information and distinctions
are never erased. In classical mechanics, this principle led
to Hamilton‚Äôs equations and Liouville‚Äôs theorem. Earlier, in
Lecture 4, I explained how in quantum mechanics this law
led to the principle of unitarity, which in turn led to the
general Schr¬®odinger equation.
Lecture 8 was all about the Ô¨Årst question: How do we
describe the state of a particle? Now, in the current lecture,
we come to the second question, which we might rephrase:
How do particles move in quantum mechanics?
In Lecture 4, I laid out the basic rules for how quan-
tum states change with time. The essential ingredient is the
Hamiltonian H, which in both classical and quantum me-
chanics represents the total energy of a system. In quantum
mechanics, the Hamiltonian controls the time evolution of a
system through the time-dependent Schr¬®odinger equation:
i¬Øh‚àÇ|Œ®‚ü©
‚àÇt
= H|Œ®‚ü©.
(9.1)
This lecture is all about the Original Schr¬®odinger Equation‚Äî
the equation that Schr¬®odinger wrote down to describe a quan-
tum mechanical particle. The Original Schr¬®odinger Equation
is a special case of Eq. 9.1.
The motion of ordinary (nonrelativistic) particles in clas-
sical mechanics is governed by a Hamiltonian, equal to the

9.1. A SIMPLE EXAMPLE
275
kinetic energy plus the potential energy. We will soon come
to the quantum version of this Hamiltonian, but Ô¨Årst let‚Äôs
look at a Hamiltonian that‚Äôs even simpler.
We‚Äôll start with the simplest Hamiltonian I can think of.
In this case, the Hamiltonian operator H is a Ô¨Åxed constant
times the momentum operator P:
H = cP.
(9.2)
This example is rarely written down, though it turns out to
be quite instructive. The constant c is a Ô¨Åxed number. Is cP
a reasonable Hamiltonian for a particle? Yes it is, and in a
moment we‚Äôll Ô¨Ånd out what kind of particle it describes. For
now, just notice that Eq. 9.2 is diÔ¨Äerent from what we might
expect for a nonrelativistic particle. In other words, it‚Äôs not
P2/2m. This simpler example is worth exploring Ô¨Årst, just
to see how the mathematical apparatus works.
How do we represent this example in terms of wave func-
tions œà(x) in the position basis?
We‚Äôll start by plugging
our operators into the time-dependent Schr¬®odinger equation
(Eq. 9.1):
i¬Øh‚àÇœà(x, t)
‚àÇt
= ‚àíci¬Øh‚àÇœà(x, t)
‚àÇx
.
Notice that we‚Äôre now writing œà as a function of both x and
t. Canceling the i¬Øh terms gives us
‚àÇœà(x, t)
‚àÇt
= ‚àíc‚àÇœà(x, t)
‚àÇx
,
(9.3)

276
LECTURE 9. PARTICLE DYNAMICS
which is a pretty simple equation. In fact, any function of
(x ‚àíct) is a solution. By ‚Äúfunction of (x ‚àíct),‚Äù I mean any
function that depends not on x and t separately, but only on
the combination (x‚àíct). To see how this works, just consider
an arbitrary function œà(x ‚àíct) and look at its derivatives.
If you take the partial with respect to x, you just get
‚àÇœà(x ‚àíct)
x
because the derivative of (x ‚àíct) with respect to x is 1. But
if you take the partial with respect to t, you get
‚àíc‚àÇœà(x ‚àíct)
‚àÇt
.
It‚Äôs clear that this combination of derivatives satisÔ¨Åes Eq.
9.3; therefore any function of this form solves the Schr¬®odinger
equation.
Now, let‚Äôs see how a function œà(x ‚àíct) behaves. What does
it look like? How does it evolve with time? Suppose we start
by looking at a snapshot at t = 0. We can call the snapshot
œà(x) because it tells us what œà looks like at every point in
space at the speciÔ¨Åc time t = 0. Of course, we don‚Äôt want
just any function of (x ‚àíct). We want the total probability
 ‚àû
‚àí‚àû
œà‚àó(x)œà(x)dx
to equal 1. In other words, we want œà(x) to fall oÔ¨Änicely
to zero at inÔ¨Ånity so that the integral doesn‚Äôt blow up. Fig.

9.1. A SIMPLE EXAMPLE
277
Figure 9.1: Fixed Shape Wave Packet Moving at Fixed Speed
c
9.1 shows œà(x) schematically. With these characteristics, it
makes sense to call œà(x) a wave packet.
Now that we‚Äôve described the snapshot œà(x) at t = 0,
what happens if we let time move forward? As t increases,
the wave packet keeps the exact same shape. Every feature
of the complex-valued function œà(x, t) moves with uniform
velocity c to the right.1
I had a reason for giving the name c to our constant‚Äîthe
symbol c often stands for the speed of light. So is this par-
ticle a photon? No, not really. But our description of this
hypothetical particle is pretty close to the correct descrip-
tion of a neutrino that moves at the speed of light. (Real
1This includes both the real and the imaginary parts of œà(x).

278
LECTURE 9. PARTICLE DYNAMICS
neutrinos probably move at a speed that is immeasurably
smaller than the speed of light.) This Hamiltonian would be
a very good description of a one-dimensional neutrino except
for one problem: the particle described by our wave function
can only move to the right. To round out this description,
we would have to add another possibility‚Äîthat the particle
could also move to the left!2
Our right-going zaxon3 has another oddball feature‚Äîits
energy can be either positive or negative. This is because
the P operator, as a vector, can take on positive or negative
values.
In general, the energy of a particle with negative
momentum is negative, and the energy of a particle with
positive momentum is positive. I won‚Äôt say more about this
except that the problem of negative energy for this kind of
particle was solved by Dirac, who used it to establish the
theoretical basis for antiparticles. For our purposes, we can
ignore this issue and simply allow the energy of our particle
to be either positive or negative.
Since the wave function of our particle moves rigidly
down the x axis, so does the probability distribution. As
a result, the expectation value of x moves in exactly the
same way, which is to say that it moves to the right with
velocity c. That‚Äôs the essential quantum mechanics of this
system. However, there is another important thing to keep
in mind. When we said the velocity c is a Ô¨Åxed constant, we
2Our right-going particles remind me of Dr. Seuss‚Äôs classic story
‚ÄúThe Zax,‚Äù and I‚Äôm tempted to call them ‚Äúright-going zaxons.‚Äù There‚Äôs
no telling how the story would have turned out if Theodor Geisel had
known more about neutrinos.
3There. I‚Äôve said it.

9.1. A SIMPLE EXAMPLE
279
weren‚Äôt kidding. Our particle can only exist in a state where
it moves at this particular velocity. It can never slow down
or speed up.
How does this compare with the classical description of
such a particle? Starting with the same Hamiltonian, a clas-
sical physicist would just write Hamilton‚Äôs equations. With
H = cP, Hamilton‚Äôs equations are
‚àÇH
‚àÇp = Àôx
and
‚àÇH
‚àÇx = ‚àíÀôp.
Carrying out the partial derivatives, these become
‚àÇH
‚àÇp = Àôx = c
and
‚àÇH
‚àÇx = ‚àíÀôp = 0.
Thus, in the classical description of our particle, the momen-
tum is conserved, and the position moves with Ô¨Åxed velocity
c. In the quantum mechanical description, the whole proba-
bility distribution and the expectation value move with ve-
locity c. In other words, the expectation value of position
behaves according to the classical equations of motion.

280
LECTURE 9. PARTICLE DYNAMICS
9.2
Nonrelativistic Free Particles
Only massless particles can move at the velocity of light, and
I might add, they can only move at that velocity. All known
particles other than photons and gravitons are massive and
can move at any velocity less than c. When they move with a
velocity much less than c, they are said to be nonrelativistic
and their motion is governed by ordinary Newtonian mechan-
ics, at least classically. The earliest application of quantum
mechanics was to the motion of nonrelativistic particles.
I showed earlier (in Lectures 4 and 8) that Poisson brack-
ets play the same mathematical role in classical mechanics as
commutators do in quantum mechanics. Written with these
constructs, the classical and quantum mechanical equations
of motion are almost identical in form. In particular, the
Hamiltonian comes into play in the same way with Poisson
brackets as it does with commutators. So, if you want to
write down the quantum mechanical equations of a system
whose classical physics you already know, it‚Äôs very reason-
able to try using the classical Hamiltonian, translated into
operator form.
For a nonrelativistic free particle, the natural Hamilto-
nian to try is p2/2m. When we say the particle is free, what
we really mean is that no forces are acting on it, and there-
fore we can ignore potential energy. All we care about is the
kinetic energy, which is deÔ¨Åned as
T = 1
2mv2.
As you recall, the momentum for a classical particle is

9.2. NONRELATIVISTIC FREE PARTICLES
281
p = mv.
The Hamiltonian is just the kinetic energy, which we can
write in terms of the momentum p. This gives us
H = 1
2mv2 = p2
2m
for the Hamiltonian of a classical nonrelativistic free parti-
cle. Unlike the right-going zaxon of the previous example,
the energy of this particle does not depend on its direction
of motion. That‚Äôs because the energy is proportional to p2
rather than p itself. So we‚Äôll start with a particle whose en-
ergy is p2/2m and work out the Schr¬®odinger equation (the
original one that Schr¬®odinger discovered) for a free particle.
Our plan is to follow the same process we used in the
previous example, using the Hamiltonian to write a time-
dependent Schr¬®odinger equation. As usual, the left side of
the equation is
i¬Øh‚àÇœà
‚àÇt .
We‚Äôll derive the right-hand side by rewriting the classical
Hamiltonian‚Äîthe kinetic energy‚Äîas an operator. The clas-
sical kinetic energy is
p2/2m.
The quantum version replaces p with P:

282
LECTURE 9. PARTICLE DYNAMICS
H = P2/2m.
What is the meaning of this? As we‚Äôve seen, the operator P
is deÔ¨Åned as
P = ‚àíi¬Øh ‚àÇ
‚àÇx.
The square of P is just the operator that you get by allowing
P to act twice in succession. Thus,
P2 = (‚àíi¬Øh ‚àÇ
‚àÇx)(‚àíi¬Øh ‚àÇ
‚àÇx),
or
P2 = ‚àí¬Øh2 ‚àÇ2
‚àÇx2,
and the Hamiltonian becomes
H = ‚àí¬Øh2
2m
‚àÇ2
‚àÇx2.
Finally, if we equate the left- and right-hand sides of the
time-dependent Schr¬®odinger equation, we get
i¬Øh‚àÇœà
‚àÇt = ‚àí¬Øh2
2m
‚àÇ2œà
‚àÇx2 .
(9.4)
This is the traditional Schr¬®odinger equation for an ordinary
nonrelativistic free particle. It is a particular kind of wave
equation, but, in contrast to the previous example, waves

9.3. TIME-INDEPENDENT SCHR ¬®ODINGER EQ
283
of diÔ¨Äerent wavelength (and momenta) move with diÔ¨Äerent
velocities. Because of this, the wave function does not main-
tain its shape. Unlike the zaxon wave function, it tends to
spread out and fall apart. This is shown schematically in
Fig. 9.2.
Figure 9.2: Typical Wave Packet for a Nonrelativistic Free
Particle. Top: The initial wave packet is compact and highly
localized. Bottom: Over time, the wave packet moves to the
right and spreads out.
9.3
Time-Independent
Schr¬®odinger Equation
We are going to solve the time-dependent Schr¬®odinger equa-
tion for nonrelativistic free particles, but Ô¨Årst we need to

284
LECTURE 9. PARTICLE DYNAMICS
solve the time-independent version. The time-independent
equation is essentially the eigenvector equation for the Hamil-
tonian,
H|Œ®‚ü©= E|Œ®‚ü©,
written explicitly in terms of the wave function œà(x):
‚àí¬Øh2
2m
‚àÇ2œà(x)
‚àÇx2
= Eœà(x).
(9.5)
It‚Äôs very easy to Ô¨Ånd a complete set of eigenvectors that
satisfy this equation.
In fact, momentum eigenvectors do
the job. Let‚Äôs try the function
œà(x) = e
ipx
¬Øh
(9.6)
as a possible solution. Carrying out the derivatives, we Ô¨Ånd
that this function is indeed a solution to Eq. 9.5, as long as
we set
E = p2/2m.
(9.7)
This should come as no surprise‚Äîafter all, E represents an
energy eigenvalue in Eq. 9.5.
Exercise 9.1: Derive Eq. 9.7 by plugging Eq. 9.6 into Eq.
9.5.
As we saw in Section 4.13, every solution to the time-independent
Schr¬®odinger equation allows us to construct to a time-dependent

9.3. TIME-INDEPENDENT SCHR ¬®ODINGER EQ
285
solution. All we need to do is multiply the time-independent
solution‚Äîin this case e
ipx
¬Øh ‚Äîby e‚àíi Et
¬Øh = e‚àíi p2t
2m¬Øh. Thus, a com-
plete set of solutions can be written as
œà(x, t) = exp i(px ‚àíp2t
2m)
¬Øh
.
Any solution is a sum, or integral, of these solutions:
œà(x, t) =

Àúœà(p)

exp i(px ‚àíp2t
2m)
¬Øh

dp.
You can start with any wave function at t = 0, Ô¨Ånd Àúœà(p) by
Fourier transform, and let it evolve. The shape will change
because the waves for diÔ¨Äerent p values travel at diÔ¨Äerent
velocities. But, as we will soon see, the overall wave packet
will travel at velocity ‚ü®p/m‚ü©just as a classical particle would.
This simple general solution has an important implica-
tion.
Among other things, it says that the momentum-
representation wave function changes with time in a very
simple way:
Àúœà(p, t) = Àúœà(p) exp i(px ‚àíp2t
2m)
¬Øh
.
In other words, only the phase changes with time, while the
magnitude remains constant. What makes this so interesting
is that the probability P(p) does not change at all with time.
This, of course, is a consequence of momentum conservation,
but it only holds if there are no forces acting on the particle.

286
LECTURE 9. PARTICLE DYNAMICS
9.4
Velocity and Momentum
So far, I haven‚Äôt explained the connection between the oper-
ator P and the classical notion of momentum‚Äînamely, mass
times velocity, or
v = p/m.
(9.8)
What do we mean by the velocity of a quantum mechanical
particle?
The simplest answer is that we mean the time
derivative of the average position ‚ü®Œ®|X|Œ®‚ü©:
v = d‚ü®Œ®|X|Œ®‚ü©
dt
or, more concretely, in terms of wave functions,
v = d
dt

œà‚àó(x, t) x œà(x, t).
Why does ‚ü®Œ®|X|Œ®‚ü©vary with time? Because œà depends on
time, and in fact we know just how. The time dependence of
œà is governed by the time-dependent Schr¬®odinger equation.
We could use that fact to work out how ‚ü®Œ®|X|Œ®‚ü©varies with
time. I‚Äôve done it this way‚Äîby brute force‚Äîand it takes sev-
eral pages. Fortunately, the abstract methods you learned in
earlier lectures make it easier; in fact, we have already done
most of the work in Lecture 4. In fact, before we proceed,
I recommend that you review Lecture 4, especially Section
4.9, from the beginning to the appearance of Eq. 4.17. To
restate Eq. 4.17,

9.4. VELOCITY AND MOMENTUM
287
d
dt‚ü®L‚ü©= i
¬Øh ‚ü®[ H, L ] ‚ü©.
In words: the time derivative of the expectation value of any
observable L is given by i/¬Øh times the expectation value of
the commutator of the Hamiltonian with L. Applying this
principle to the velocity v, we Ô¨Ånd that
v =
i
2m¬Øh‚ü®[ P2, X ] ‚ü©.
(9.9)
Now, all we have to do is compute the commutator of P2
and X. A couple of simple steps shows that
[ P2, X ] = P[ P, X ] + [ P, X ]P.
(9.10)
This relation can be conÔ¨Årmed by expanding each commu-
tator and spotting some obvious cancellations.
Exercise 9.2: Prove Eq. 9.10 by expanding each side and
comparing the results.
The last step uses the standard commutation relation
[ P, X ] = ‚àíi¬Øh.
Substituting this into Eq. 9.10 and plugging that result into
Eq. 9.9, we Ô¨Ånd that
v = ‚ü®P‚ü©
m

288
LECTURE 9. PARTICLE DYNAMICS
or, perhaps more familiarly,
‚ü®P‚ü©= mv.
(9.11)
We have proved exactly what we set out to prove: the mo-
mentum is equal to the mass times the velocity, or, more
exactly, the average momentum equals the mass times the
velocity.
To get a better idea of what this means, let‚Äôs suppose
the wave function has the form of a packet, or fairly narrow
lump. The expectation value of x will be approximately at
the center of the lump. What Eq. 9.11 tells us is that the
center of the wave packet travels according to the classical
rule p = mv.
9.5
Quantization
Before moving on to the subject of forces in quantum me-
chanics, I want to pause and discuss what we have done. We
started with a well-known and well-trusted classical system‚Äî
the free particle‚Äîand quantized it. We can codify this pro-
cedure as follows:
1. Start with a classical system. This means a set of co-
ordinates x and momenta p. In our example, there was
only one coordinate and one momentum, but the pro-
cedure is easy to generalize. The coordinates and mo-
menta come in pairs, xi and pi. The classical system
also has a Hamiltonian, which is a function of the x‚Äôs
and p‚Äôs.

9.5. QUANTIZATION
289
2. Replace the classical phase space with a linear vec-
tor space.
In the position representation, the space
of states is represented by a wave function œà(x) that
depends on the coordinates‚Äîin general, all of them.
3. Replace the x‚Äôs and p‚Äôs with operators Xi and Pi. Each
Xi acts on the wave function to multiply it by xi. Each
Pi acts according to the rule
Pi ‚Üí‚àíi¬Øh ‚àÇ
‚àÇxi
.
4. When these replacements are made, the Hamiltonian
becomes an operator that can be used in either the
time-dependent or time-independent Schr¬®odinger equa-
tion. The time-dependent equation tells us how the
wave function changes with time. The time-independent
form allows us to Ô¨Ånd the eigenvectors and eigenvalues
of the Hamiltonian.
This procedure of quantization is the means by which the
classical equations of a system converted to quantum equa-
tions. It has been used over and over, in Ô¨Åelds ranging from
the motion of particles to quantum electrodynamics; there
have even been (not so successful) attempts to quantize Ein-
stein‚Äôs theory of gravity. As we saw in one simple case, the
procedure guarantees that the motion of expectation values
is closely related to classical motion.
All of this raises a ‚Äúchicken and egg‚Äù question: Which
comes Ô¨Årst‚Äîclassical theory or quantum theory?
Should

290
LECTURE 9. PARTICLE DYNAMICS
the logical starting point of physics be classical or quantum
mechanical? I think the answer is obvious. Quantum me-
chanics is the real description of nature. Classical mechanics,
while beautiful and elegant, is nevertheless an approxima-
tion. Roughly speaking, it holds true when wave functions
maintain their shape as packets.
Sometimes, we‚Äôre lucky
and the quantum theory of a system can be guessed‚Äîand
that‚Äôs all it is, a guess‚Äîby starting with a familiar clas-
sical system and quantizing it. Sometimes this works. The
quantum motion of electrons, deduced from the classical me-
chanics of particles, is a case in point. Quantum electrody-
namics, deduced from Maxwell‚Äôs equations, is another. But
sometimes there is no classical theory to use as a starting
point. The spin of a particle has no real classical counter-
part. And the quantization of general relativity has largely
failed. Quantum theory is probably much more fundamental
than classical theory, which generally should be understood
as an approximation.
That being said, I will now continue to quantize the mo-
tion of particles, but this time incorporating the eÔ¨Äects of
forces.
9.6
Forces
The world would be a dull place if all particles were free.
Forces are what make particles do interesting things, such as
assembling themselves into atoms, molecules, chocolate bars,
and black holes. The force on any given particle is the sum
total of the forces exerted on it by all the other particles in
the universe. In practice, we usually assume that we know

9.6. FORCES
291
what all the other particles are doing and replace their eÔ¨Äect
by a potential energy function for the particle that we are
studying. This much is true in both classical and quantum
mechanics.
The potential energy function is denoted by V (x). In clas-
sical mechanics it‚Äôs related to the force on a particle by the
equation
F(x) = ‚àí‚àÇV
‚àÇx .
If the motion is one-dimensional, the partial derivative can
be replaced by an ordinary derivative, but I will leave it as
is. If we then combine this equation with Newton‚Äôs second
law, F = ma, we get
md2x
dt2 = ‚àí‚àÇV
‚àÇx .
In quantum mechanics, we proceed diÔ¨Äerently; we write a
Hamiltonian and solve the Schr¬®odinger equation. Incorpo-
rating the potential energy into this program is straightfor-
ward. The potential energy V (x) becomes an operator V
that gets added to the Hamiltonian.
What kind of operator is V? The answer is easiest to
express if we think in the language of wave functions rather
than in terms of abstract bras and kets. When the operator
V acts on any wave function œà(x), it multiplies the wave
function by the function V (x).
V|Œ®‚ü©‚ÜíV (x)œà(x).

292
LECTURE 9. PARTICLE DYNAMICS
Just as in classical mechanics, once forces are included, the
momentum of a particle is not conserved. In fact, Newton‚Äôs
laws of motion can be stated in the form
dp
dt = F
or
dp
dt = ‚àí‚àÇV
‚àÇx .
(9.12)
The rules of quantization require us to add V(x) to the
Hamiltonian,4
H = P2
2m + V(x),
(9.13)
and modify the Schr¬®odinger equations in the obvious way:
i¬Øh‚àÇœà
‚àÇt
=
‚àí¬Øh2
2m
‚àÇ2œà
‚àÇx2 + V (x)œà
Eœà
=
‚àí¬Øh2
2m
‚àÇ2œà
‚àÇx2 + V (x)œà.
(9.14)
What eÔ¨Äect does this have? The additional term certainly
aÔ¨Äects the way œà changes with time. That of course must
be so if the average position of a wave packet is to follow
a classical trajectory.
To check our reasoning, let‚Äôs see if
it does.
First of all, does Eq. 9.11 still hold?
It should,
4Technically, this is true for free particles as well. However, in the
case of free particles we set V (x) equal to 0.

9.6. FORCES
293
because the connection between momentum and velocity is
unaÔ¨Äected by the presence of forces.
Because a new term has been added to H, there will be a
new term in the commutator of X and H. Potentially, that
could modify the expression for velocity in Eq. 9.9, but it‚Äôs
easy to see that this doesn‚Äôt happen. The new term involves
the commutator of X with V(x). But multiplying by x and
multiplying by a function of x are operations that commute.
In other words,
[X, V(x)] = 0.
Therefore, the connection between velocity and momentum
is unaÔ¨Äected by forces in quantum mechanics, as is the case
in classical mechanics.
The more interesting question is: Can we understand the
quantum version of Newton‚Äôs law? As stated above, this law
can be written as
dp
dt = F.
Let‚Äôs calculate the time derivative of the expectation value of
P. Again, the trick is to commute P with the Hamiltonian:
d
dt‚ü®P‚ü©=
i
2m¬Øh ‚ü®[P2,P]‚ü©+ i
¬Øh ‚ü®[V, P]‚ü©.
(9.15)
The Ô¨Årst term is zero because an operator commutes with
any function of itself. To compute the second term, we‚Äôll use

294
LECTURE 9. PARTICLE DYNAMICS
an equation that we haven‚Äôt proved yet:
[V(x), P] = i¬Øh dV (x)
dx
.
(9.16)
Plugging Eq. 9.16 into Eq. 9.15, we get
d
dt‚ü®P‚ü©= ‚àí‚ü®dV
dx ‚ü©.
Now, let‚Äôs prove Eq. 9.16. Letting the commutator act on a
wave function, we can write
[V(x), P]œà(x) = V (x)(‚àíi¬Øh d
dx )œà(x) ‚àí(‚àíi¬Øh d
dx )V (x)œà(x).
(9.17)
This is easily simpliÔ¨Åed and results in Eq. 9.16. Thus, we
have shown that
d
dt‚ü®P‚ü©= ‚àí‚ü®dV
dx ‚ü©,
(9.18)
which is the quantum analog of Newton‚Äôs equation for the
time rate of change of momentum.
Exercise 9.3: Show that the right-hand side of Eq. 9.17
simpliÔ¨Åes to the right-hand side of Eq. 9.16. Hint: First ex-
pand the second term by taking the derivative of the product.
Then look for cancellations.

9.7. LINEAR MOTION/CLASSICAL LIMIT
295
9.7
Linear Motion and the
Classical Limit
You might think we have proved that the expectation value
of X exactly follows the classical trajectory. But what we‚Äôve
actually proved is quite diÔ¨Äerent. This diÔ¨Äerence exists be-
cause the average of a function of x is not the same as the
function of the average of x. If Eq. 9.18 had read
d
dt‚ü®P‚ü©= ‚àídV (‚ü®x‚ü©)
d‚ü®x‚ü©
[This is wrong]
(and, let me emphasize, it does not), then indeed we would
say that the average position and momentum satisfy the clas-
sical equations. But in reality the classical equations are only
approximations, good whenever we can replace the average
of dV/dx by the function of the average of x. When is it
reasonable to do this?
The answer is whenever the V (x)
varies slowly compared to the size of the wave packet. If V
varies rapidly across the wave packet, the classical approx-
imation will break down. In fact, in that situation a nice,
narrow wave packet will get broken up into a badly scattered
wave that has no resemblance to the original wave packet.
The probability function will also get scattered. Then you‚Äôll
have no choice but to solve the Schr¬®odinger equation.
Let‚Äôs look at this point more closely.
Mathematically,
we‚Äôve made no assumptions about the shapes of our wave
packets. But we have tacitly thought of them as being nicely
shaped functions with a single maximum, smoothly trailing
oÔ¨Äto zero in the positive and negative directions. This con-

296
LECTURE 9. PARTICLE DYNAMICS
dition, though not explicit in our mathematical assumptions,
does have a real impact on whether a particle behaves the
way classical mechanics would lead us to expect.
Figure 9.3: Bimodal (Two-Humped) Function, Centered at
x = 0. Note that ‚ü®x‚ü©= 0, but Œîx > 0.
To illustrate this point, let‚Äôs consider a slightly ‚Äúweird‚Äù
wave packet. Fig. 9.3 shows a bimodal wave packet (having
two maxima), centered at the origin of the x axis. Now, let‚Äôs
consider some function of x, say F(x), where F represents
force. The expectation value of F(x) is not the same as the
function F of the expectation value of x. In other words,
‚ü®F(x)‚ü©Ã∏= F(‚ü®x‚ü©).
The right-hand side is a function of the center of the wave
packet. It is not the same as the left-hand side, which corre-

9.7. LINEAR MOTION/CLASSICAL LIMIT
297
sponds to our results from the previous section‚Äî‚ü®F(x)‚ü©has
the same form as the right-hand side of Eq. 9.18.5
Let me give you an example where these two expressions
could be extremely diÔ¨Äerent. Suppose that F is equal to x
squared:
F = x2.
And suppose the wave packet looks like Fig. 9.3. What‚Äôs the
expectation value of x? It‚Äôs zero, and so is F(‚ü®x‚ü©), because
F(0) = 02 = 0. On the other hand, what is the expectation
value of x2? It‚Äôs greater than zero. So when a wave packet
is not a nice, single bump that is mainly characterized by its
center, it‚Äôs not always true that the time rate of change of the
momentum is the force evaluated at the expectation value of
x. It‚Äôs only when the wave function is concentrated over a
fairly narrow range that the expectation value of F(x) is the
same as F(‚ü®x‚ü©). So we have cheated a little in saying our
quantum equation of motion looks classical. That depends
on the wave packet being coherent and well localized.
Everything else being equal, when the mass of a particle
is large, the wave function tends to be very well concen-
trated.
If there are no very sharp spikes in the potential
function V (x), then it will be a good approximation to re-
place ‚ü®F(x)‚ü©with F(‚ü®x‚ü©). When V (x) has spikes, however,
the wave packet tends to break up. For example, suppose we
have a nice wave packet moving to the right, and it hits a
point structure, like an atom, with a potential function sim-
5Recall that ‚àí‚ü®dV
dx ‚ü©represents force in that equation.

298
LECTURE 9. PARTICLE DYNAMICS
ilar to Fig. 9.4. The wave packet will spread out and disinte-
grate. If, on the other hand, it hits a very smooth potential,
then it will go through the smooth potential, moving more or
less according to the classical equations of motion. We don‚Äôt
expect quantum mechanics to reproduce classical mechanics
in every possible circumstance. We expect it to reproduce
classical mechanics in circumstances where it should‚Äîwhere
the particles are heavy, the potentials are smooth, and noth-
ing causes the wave function to disintegrate or scatter.6

Figure 9.4: Spiky Potential Function. Potential functions
with sharp peaks tend to cause wave functions to scatter.
The smaller these features are in relation to the wave packet,
the more the wave packet will scatter, and the less ‚Äúclassical‚Äù
it will become.
What physical situations lead to ‚Äúbad potentials‚Äù that
break up the wave function? Suppose a potential has fea-
6Not as eloquent as Garrison Keillor‚Äôs tagline, but true all the same.

9.7. LINEAR MOTION/CLASSICAL LIMIT
299
tures that have a certain size associated with them. Think of
Fig. 9.4 on steroids, with lots of large, closely packed spikes.
Suppose we call the size of these features Œ¥x, and that Œ¥x is
signiÔ¨Åcantly smaller than the incoming particle‚Äôs uncertainty
in position:
Œ¥x < Œîx.
If the sharp features of V (x) exist on a scale that is much
smaller than the size of the incoming wave packet, the packet
will break into a lot of little pieces. Each one will scatter oÔ¨Ä
in a diÔ¨Äerent direction.
Roughly speaking, when the fea-
tures of the potential are shorter than the wavelength of the
incoming particle, the wave function will tend to break up.
Let‚Äôs say you take a bowling ball and ask, ‚ÄúWhat is Œîx?‚Äù
We can use the uncertainty principle to gain some intuition
about this question. Typically, Œîp √ó Œîx is bigger than ¬Øh.
But in many reasonable cases it‚Äôs of order ¬Øh:
ŒîpŒîx ‚àº¬Øh.
Now, p is about as concentrated as it can be, but for an or-
dinary macroscopic object, the uncertainty relation is pretty
much saturated‚Äîthe left-hand side is roughly equal to ¬Øh.
The reasons for this are very complicated, and I won‚Äôt go
into them here. Instead, let‚Äôs assume this is true and work
out the implications. What is Œîp? It‚Äôs mŒîv, which gives us
mŒîvŒîx ‚àº¬Øh.

300
LECTURE 9. PARTICLE DYNAMICS
Rearranging the symbols, we can then write
ŒîvŒîx ‚àº
¬Øh
m
or
Œîx ‚àº
¬Øh
mŒîv.
Now, if I put a bowling ball on the ground, I know very well
that the uncertainty in its velocity is not very big. As the
ball gets heavier and heavier, you might expect the uncer-
tainty in velocity to get smaller and smaller. But, in any
case, the right-hand side has an m in the denominator, and
regardless of Œîv, as m gets smaller, Œîx will get bigger. And
in particular, it will tend to get bigger than the features in
the potential.
In the quantum mechanical limit where m is very small
and Œîx tends to be big, the wave function will move under
the inÔ¨Çuence of a ragged potential, which it sees as being
much sharper and more featured than the wave function it-
self. That‚Äôs when the wave function breaks up. On the other
hand, as m gets very large, Œîx gets small. For a large bowl-
ing ball, the wave packet might be very concentrated. When
it moves through a spiky potential, this tiny wave function
encounters a potential whose features are (comparatively)
very broad.
Moving through broad smooth features does
not break the wave function into pieces. Large masses and
smooth potentials characterize the classical limit. A particle
with low mass, moving through an abrupt potential, behaves
like a quantum mechanical system.

9.8. PATH INTEGRALS
301
What about electrons? Are they massive enough to be-
have classically? The answer depends on the interplay be-
tween the potential and the mass. For example, if you have
two capacitor plates separated by a centimeter, with a smooth
electric Ô¨Åeld between them, then the electron will move across
the gap like a nice, coherent, almost classical particle. On
the other hand, the potential associated with the nucleus of
an atom always has a sharp feature in it. If an electron wave
packet hits this potential, it will scatter all over the place.
Before leaving this topic, I‚Äôd like to mention minimum-
uncertainty wave packets.
These are wave packets where
ŒîxŒîp is equal to ¬Øh/2 (as opposed to being greater). In other
words, in these cases, ŒîxŒîp is as small as quantum mechan-
ics allows. These wave packets have the form of a Gaussian
curve, and they‚Äôre often called Gaussian wave packets. Over
time, they spread out and Ô¨Çatten. Such wave packets are not
that common, but they do exist. A bowling ball at rest is a
good approximation. In Lecture 10, we‚Äôll see that the ground
state of a harmonic oscillator is a Gaussian wave packet.
9.8
Path Integrals
Classical Hamiltonian mechanics focuses on the step-by-step
incremental changes in the state of a system. But there is
another way to formulate mechanics‚Äîthe Principle of Least
Action‚Äîin which the focus is on entire histories. For a par-
ticle, this means looking at the full trajectory of the particle
from some initial time to some Ô¨Ånal time. The content of
the two approaches is the same, but the emphasis is diÔ¨Äer-

302
LECTURE 9. PARTICLE DYNAMICS
ent. Hamiltonian mechanics zeros in on some instant and
tells you how the system changes between that instant and
the next. The least action principle steps back and takes a
global look. One can imagine nature sampling all possible
trajectories and picking the one that minimizes the action
between a pair of Ô¨Åxed initial and Ô¨Ånal points.7
Quantum mechanics also has a Hamiltonian description
that concentrates on incremental changes.
It‚Äôs called the
time-dependent Schr¬®odinger equation, and it‚Äôs very general.
As far as we know, it can be used to describe all physical
systems. Still, it seems fair to ask, as Richard Feynman did
almost seventy years ago, whether there is a way to look at
quantum mechanics that pictures whole histories. In other
words, is there a formulation that parallels the Principle of
Least Action? I will not explain Feynman‚Äôs path integral
description in detail in this lecture, but just to whet your
appetite I‚Äôll give you a hint of how it works.
First, let me very brieÔ¨Çy remind you of the classical least
action principle as I explained it in Volume I. Suppose that
a classical particle starts at position x1 at time t1 and arrives
at position x2 at time t2 (Fig. 9.5). The question is: What
is the trajectory that it took between t1 and t2?
According to the least action principle, the actual trajec-
tory is the one of minimum action. Action is of course a tech-
nical term, and it stands for the integral of the Lagrangian
between the end points of the trajectory. For simple systems,
7Strictly speaking, the principle should be called the Principle of
Stationary Action. Actual trajectories are stationary points of the ac-
tion and not always minima. For our purposes, this Ô¨Åne point is not
important.

9.8. PATH INTEGRALS
303
Figure 9.5: Classical Trajectory.
This shows one path a
particle may take when moving from point 1 (x1, t1) to point
2 (x2, t2). To keep things simple, the Àôx axis, representing the
particle‚Äôs velocity in the x direction, is not shown.
the Lagrangian is the kinetic energy minus the potential en-
ergy. Thus, for a particle that moves in one dimension, the
action is
A =
 t2
t1
L(x, Àôx)dt
(9.19)
or

304
LECTURE 9. PARTICLE DYNAMICS
Figure 9.6: First Step Toward Quantizing the Trajectory.
Break the particle‚Äôs path into two equal parts (equal in time,
that is).
The particle has the same starting and ending
points, but now its trajectory passes through the interme-
diate point x.
A =
 t2
t1
m Àôx2
2
‚àíV (x)

dt.
The idea is to try out all possible trajectories connecting the
two end points, and calculate A for each one of them. The

9.8. PATH INTEGRALS
305
Figure 9.7: Further Steps Toward Constructing the Path
Integral. Keeping the same starting and ending points, break
the path up into a large number of equally sized segments.
winner is the one that has the least action.8,9
Now, let‚Äôs turn to quantum mechanics. The idea of a
well-deÔ¨Åned trajectory between two points makes no sense
in quantum mechanics because of the uncertainty principle.
8That‚Äôs how it works conceptually, anyway. In practice, the Euler-
Lagrange equations provide a shortcut, as explained in Volume I.
9To keep our diagrams simple, we don‚Äôt display an Àôx axis even
though the Lagrangian clearly depends on Àôx.

306
LECTURE 9. PARTICLE DYNAMICS
However, a question that we can ask is: Given that a particle
starts out at (x1, t1), what is the probability that it will show
up at (x2, t2) if an observation of its position is made?
As always in quantum mechanics, the probability is the
square of the absolute value of a complex amplitude. The
global version of quantum mechanics asks:
Given that a particle starts out at (x1, t1), what is the
amplitude that it will show up at (x2, t2)?
Let‚Äôs call that amplitude C(x1, t1; x2, t2) or, more simply, just
C1,2. The initial state of the particle is |Œ®(t1)‚ü©= |x1‚ü©. Over
the time interval between t1 and t2, the state evolves to
|Œ®(t2)‚ü©= e‚àíiH(t2‚àít1)|x1‚ü©.
(9.20)
The amplitude to detect the particle at |x2‚ü©is just the inner
product of |Œ®(t2)‚ü©with |x2‚ü©. Its value is
C1,2 = ‚ü®x2|e‚àíiH(t2‚àít1)|x1‚ü©.
(9.21)
In other words, the amplitude to go from x1 to x2 over the
time interval t2 ‚àít1 is constructed by sandwiching e‚àíiH(t2‚àít1)
between the initial and Ô¨Ånal positions. To simplify the for-
mula, let‚Äôs deÔ¨Åne t2 ‚àít1 to be t. Then the amplitude is
C1,2 = ‚ü®x2|e‚àíiHt|x1‚ü©.
(9.22)
Now, let‚Äôs break the time interval t into two smaller intervals
of size t/2 (see Fig. 9.6). The operator e‚àíiHt can be written

9.8. PATH INTEGRALS
307
as the product of two operators:
e‚àíiHt = e‚àíiHt/2e‚àíiHt/2.
(9.23)
By inserting the identity operator in the form
I =

dx|x‚ü©‚ü®x|,
(9.24)
we can rewrite the amplitude as
C1,2 =

dx‚ü®x2|e‚àíiHt/2|x‚ü©‚ü®x|e‚àíiHt/2|x1‚ü©.
(9.25)
This form of the equation looks more complicated, but has
a very interesting interpretation. Let me put it in words.
The amplitude to get from x1 to x2 over time interval t is
an integral over an intermediate position x. The integrand is
the amplitude to go from x1 to x over the time interval t/2
multiplied by the amplitude to go from x to x2 over another
time interval t/2.
Fig. 9.6 shows the same idea in visual terms. Classically,
to go from x1 to x2, the particle must pass through an inter-
mediate point x. But in quantum mechanics the amplitude to
go from x1 to x2 is an integral over all possible intermediate
points.
We can carry this idea further and divide the time in-
terval into a great many tiny intervals, as illustrated in Fig.
9.7. I won‚Äôt write out the complicated formulas, but the idea
should be clear. For each tiny time interval, say of size
we
include a factor
e,

308
LECTURE 9. PARTICLE DYNAMICS
e‚àíiœµH.
Then, between each pair of factors, we insert the identity
so that the amplitude C1,2 becomes a multiple integral over
all the intermediate locations. The integrand is built from
products of expressions with the form
‚ü®xi|e‚àíiœµH|xi+1‚ü©.
If we deÔ¨Åne U(œµ) as
U(œµ) = e‚àíiœµH,
then we can write the entire product as
‚ü®x2|U N|x1‚ü©
or
‚ü®x2|UUUU . . . |x1‚ü©.
In this equation, U appears N times as a factor, where N
is the number of epsilon steps. We can then insert identity
operators between the U‚Äôs.
Such an expression can be called the amplitude for the
given path. But the particle does not travel along a par-
ticular path. Instead, in the limit of a large number of in-
Ô¨Ånitesimal time intervals, the amplitude is an integral over

9.8. PATH INTEGRALS
309
all possible paths between the end points. The elegant fact
that Feynman discovered is that the amplitude for each path
bears a simple relation to a familiar expression from classical
mechanics‚Äîthe action for that path. The exact expression
for each path is
eiA/¬Øh,
where A is the action for the individual path.
Feynman‚Äôs formulation can be summarized by a single
equation:
C1,2 =

paths
eiA/¬Øh.
(9.26)
The path integral formulation is not merely an elegant math-
ematical trick; it has real power. In fact, it can be used to
derive both Schr¬®odinger equations, and all the commutation
relations of quantum mechanics. But it really comes into its
own in the context of quantum Ô¨Åeld theory, where it is the
principal tool for formulating the laws of elementary particle
physics.


Lecture 10
The Harmonic Oscillator
Art: I think I see it, Lenny. The whole picture is slowly com-
ing into focus. Minus One, General Uncertainty, entangled
pairs, the Hamiltonian‚Äîeven the degenerates. What‚Äôs next?
Lenny: Oscillations, Art. Vibrations. You‚Äôre a Ô¨Åddler‚Äîplay
us a last tune tonight. Something with good vibes.
Of all the ingredients that go into building a quantum de-
scription of the world, two stand out as especially fundamen-
tal. The spin, or qubit, of course is one of them. In classical
logic, everything can be built out of yes-no questions. Sim-
ilarly, in quantum mechanics, every logical question boils
down to a question about qubits. We spent a lot of time
in earlier lectures learning about qubits.
In this lecture,
we‚Äôll learn about the second basic ingredient of quantum
mechanics‚Äîthe harmonic oscillator.
The harmonic oscillator isn‚Äôt a particular object like a
hydrogen atom or a quark. It‚Äôs really a mathematical frame-
work for understanding a huge number of phenomena. This
311

312
LECTURE 10. THE HARMONIC OSCILLATOR
concept of the harmonic oscillator also exists in classical
physics, but it really comes to the fore in quantum theory.
One example of a harmonic oscillator is a particle moving
under a linear restoring force; for example, the iconic weight
on the end of a spring. An idealized spring satisÔ¨Åes Hooke‚Äôs
law: the force on the displaced mass is proportional to the
distance it has been displaced. We call the force a restoring
force because it pulls the mass back toward the equilibrium
position.
Another example is a marble rolling back and forth at the
bottom of a bowl, with no energy being lost to friction. What
characterizes these systems is a potential energy function
that looks like a parabola:
V (x) = k
2x2.
(10.1)
The constant k is called the spring constant. If we recall
that the force on an object is minus the gradient of V, we
Ô¨Ånd that the force on the object is
F = ‚àíkx.
(10.2)
The negative sign tells us that the force acts opposite to the
displacement and pulls the mass back toward the origin.
Why are harmonic oscillators so prevalent in physics? Be-
cause almost any smooth function looks like a parabola close
to a minimum of the function. Indeed, many kinds of sys-
tems are characterized by an energy function that can be
approximated by a quadratic function of some variable rep-
resenting a displacement from equilibrium. When disturbed,

313
these systems will all oscillate about the equilibrium point.
Here are some other examples:
‚Ä¢ An atom situated in a crystal lattice.
If the atom
is displaced slightly from its equilibrium position, it
gets pushed back with an approximately linear restor-
ing force. This motion is three-dimensional and really
consists of three independent oscillations.
‚Ä¢ The electric current in a circuit of low resistance often
oscillates with a characteristic frequency. The math-
ematics of circuits is identical to the mathematics of
masses attached to springs.
‚Ä¢ Waves. If the surface of a pond is disturbed, it sends
out waves. Someone watching at a particular location
will see the surface oscillate as the wave passes by. This
motion can be described as simple harmonic motion.
The same goes for sound waves.
‚Ä¢ Electromagnetic waves.
Just like any other wave, a
light wave or a radio wave oscillates when it passes you.
The same mathematics that describes the oscillating
particle also applies to electromagnetic waves.
The list goes on and on but the math is always the same.
Just to have an example in mind, let‚Äôs picture the oscillator
as a weight hanging from a spring. Needless to say, we hardly
need quantum mechanics to describe an ordinary weight and
spring, so let‚Äôs imagine a very tiny version of this same sys-
tem and then quantize it.

314
LECTURE 10. THE HARMONIC OSCILLATOR
10.1
The Classical Description
Let‚Äôs use y to denote the height of the hanging weight. We‚Äôll
choose the origin so that the weight is at y = 0 when it‚Äôs
in equilibrium‚Äîthat is when the weight is hanging at rest.
To study this system classically, we can use the Lagrangian
method that we learned about in Volume I. The kinetic and
potential energies are 1
2m Àôy2 and 1
2ky2 respectively.
As you recall, the Lagrangian is the kinetic energy minus
the potential energy:
L = 1
2m Àôy2 ‚àí1
2ky2.
First, we‚Äôll put the Lagrangian into a certain standard form
by changing from y to another variable that we will call x.
This coordinate is not something new. It still represents the
displacement of the mass. By switching from y to x, we‚Äôre
just making a convenient change of units. Let‚Äôs deÔ¨Åne the
new variable as
x = ‚àöm y.
In terms of x, the Lagrangian becomes
L = 1
2 Àôx2 ‚àí1
2œâ2x2.
(10.3)
The constant œâ is deÔ¨Åned as œâ =

k
m and happens to be
the frequency of the oscillator.

10.1. THE CLASSICAL DESCRIPTION
315
By making this change of variables, we can describe every
oscillator in exactly the same form. In this form, oscillators
are distinguished from each other only by their frequency œâ.
Now, let‚Äôs use Lagrange‚Äôs equations to work out the equa-
tions of motion. For this one-dimensional system, there is
only one Lagrange equation, namely
‚àÇL
‚àÇx = d
dt
‚àÇL
‚àÇÀôx .
(10.4)
Carrying out these operations on Eq. 10.3, we Ô¨Ånd that
‚àÇL
‚àÇÀôx = Àôx.
(10.5)
This is called the canonical momentum conjugate to x. Dif-
ferentiating with respect to time gives
d
dt
‚àÇL
‚àÇÀôx = ¬®x,
(10.6)
and now we have the right-hand side of Eq. 10.4. Turning
to the left-hand side, we Ô¨Ånd that
‚àÇL
‚àÇx = ‚àíœâ2x.
(10.7)
Setting the left and right sides (Eqs. 10.7 and 10.6) of the
Lagrange equation equal to each other, we get
‚àíœâ2x = ¬®x.
(10.8)

316
LECTURE 10. THE HARMONIC OSCILLATOR
This equation is, of course, equivalent to F = ma. Why is
there a minus sign? Because the force is a restoring force‚Äî
its direction is opposite to the direction of the displacement.
By now you have seen this type of equation enough to know
that the solution contains sines and cosines.
The general
solution is
x = A cos(œât) + B sin(œât),
(10.9)
which shows us that œâ is indeed the frequency of the oscil-
lator. When we diÔ¨Äerentiate twice, we pull out a factor of
œâ2.
Exercise 10.1: Find the second time derivative of x in Eq.
10.9, and thereby show that it solves Eq. 10.8.
10.2
The Quantum Mechanical
Description
Now, let‚Äôs return to our microscopic version of the weight-
and-spring system‚Äîlet‚Äôs say no bigger than a single molecule.
At Ô¨Årst, this seems ridiculous. How could we ever build a
spring that small? But in fact nature provides all sorts of
microscopic springs. Many molecules consist of two atoms‚Äî
for example, a heavy atom and a light one. There are forces
holding the molecule in equilibrium with the atoms separated
by a certain distance.
When the light atom is displaced,
it will be attracted back to the equilibrium location. The

10.2. QUANTUM MECHANICAL DESCRIPTION
317
molecule is a miniature version of the weight-and-spring sys-
tem, but is so small that we have to use quantum mechanics
to understand it.
Having worked out the classical Lagrangian, let‚Äôs try to
build a quantum mechanical description of our system. The
Ô¨Årst thing we need is a space of states. As we‚Äôve seen, the
state of a particle moving on a line is represented by a wave
function œà(x). There are many possible system states, and
each one is represented by a diÔ¨Äerent wave function. A func-
tion œà(x) is deÔ¨Åned in such a way that œà‚àó(x)œà(x) is the
probability density (the probability per unit interval) to Ô¨Ånd
the particle at position x:
œà‚àó(x)œà(x) = P(x).
In this equation, P(x) represents the probability density. We
now have a sort of kinematics‚Äîa speciÔ¨Åcation of what the
system states are.
Can œà(x) be any function at all? Aside from the require-
ment that it must be continuous and diÔ¨Äerentiable, the only
extra condition is that the total probability of Ô¨Ånding the
particle at any position must be 1:
 +‚àû
‚àí‚àû
œà‚àó(x)œà(x)dx = 1.
(10.10)
This would not seem to be much of a restriction. Whatever
the right-hand side of this equation is, we could always mul-
tiply œà by some constant to make the integral equal to 1‚Äî
unless the integral is either zero or inÔ¨Ånity. Since œà‚àó(x)œà(x)

318
LECTURE 10. THE HARMONIC OSCILLATOR
is positive, we don‚Äôt have to worry about zero, but inÔ¨Ånity
is a diÔ¨Äerent matter altogether; there are lots of functions
that would make the integral in Eq. 10.10 blow up.
The
conditions for a sensible wave function thus include the re-
quirement that œà falls to zero fast enough that the integral
converges. Functions that meet this condition are called nor-
malizable.
There are two questions we might ask about our harmonic
oscillator:
‚Ä¢ How does the state-vector change as a function of time?
To answer this question, we need to know the Hamil-
tonian.
‚Ä¢ What are the oscillator‚Äôs possible energies? These are
also determined by the Hamiltonian.
So to know anything useful we need the Hamiltonian. Fortu-
nately, we can derive it from the Lagrangian, and I‚Äôll remind
you how in a moment. But Ô¨Årst recall that the canonical
momentum conjugate to x is deÔ¨Åned as ‚àÇL/‚àÇÀôx.1 Combining
this with Eq. 10.5, we get
p = ‚àÇL
‚àÇÀôx = Àôx.
Using the straightforward deÔ¨Ånition from classical mechan-
ics, we Ô¨Ånd that the Hamiltonian for the harmonic oscillator
is
1This idea is explained in Volume I.

10.2. QUANTUM MECHANICAL DESCRIPTION
319
H = p Àôx ‚àíL,
where p is the canonical momentum conjugate to x, and L
represents the Lagrangian.2
We could work directly from
this deÔ¨Ånition, but instead we‚Äôll take a shortcut. Because
the Lagrangian is the kinetic energy minus the potential en-
ergy, the Hamiltonian is the kinetic energy plus the potential
energy‚Äîin other words, the total energy. The Hamiltonian
for the oscillator can therefore be written
H = 1
2 Àôx2 + 1
2œâ2x2.
So far, so good, but we‚Äôre not quite Ô¨Ånished. We‚Äôve expressed
kinetic energy in terms of velocity; in quantum mechanics,
however, we need to represent our observables as operators,
and we don‚Äôt have a velocity operator. To take care of this,
we‚Äôll have to recast things in terms of position and canoni-
cal momentum, which does have a standard operator form.
Rewriting the Hamiltonian in terms of canonical momentum
is easy because
p = ‚àÇL
‚àÇÀôx = Àôx,
which allows us to write
H = 1
2p2 + 1
2œâ2x2.
(10.11)
2We don‚Äôt need to use a summation sign because there‚Äôs only one
degree of freedom.

320
LECTURE 10. THE HARMONIC OSCILLATOR
That‚Äôs the classical Hamiltonian. We can now turn it into a
quantum mechanical equation by reinterpreting x and p as
operators, deÔ¨Åned by their action on œà(x). As we‚Äôve done be-
fore, we‚Äôll use the boldface symbols, X and P, to distinguish
our quantum operators from their classical counterparts, x
and p. From previous lectures, we know exactly how these
operators work. X just multiplies the wave function by the
position variable:
X|œà(x)‚ü©
=‚áí
xœà(x).
And P takes the same form it does for other one-dimensional
problems:
P|œà(x)‚ü©
=‚áí
‚àíi¬Øh d
dx œà(x).
Now, we can Ô¨Ågure out the action of the Hamiltonian on a
wave function by letting P act twice on the wave function.
This is the same procedure we followed in Lecture 9. In other
words,
H|œà(x)‚ü©
=‚áí
1
2

‚àíi¬Øh ‚àÇ
‚àÇx

‚àíi¬Øh ‚àÇœà(x)
‚àÇx

+ 1
2œâ2x2œà(x),
or
H|œà(x)‚ü©
=‚áí
‚àí¬Øh2
2
‚àÇ2œà(x)
‚àÇx 2
+ 1
2œâ2x 2œà(x).
(10.12)
We‚Äôre using partial derivatives because in general œà also de-
pends on another variable, time. Time is not an operator and

10.3. THE SCHR ¬®ODINGER EQUATION
321
does not have the same status as x, but the state-vector does
change with time, and we therefore treat time as a param-
eter. The partial derivative indicates that we‚Äôre describing
the system ‚Äúat a Ô¨Åxed time.‚Äù
10.3
The Schr¬®odinger Equation
Eq. 10.12 shows how the Hamiltonian operates on œà. Now,
let‚Äôs put it to work. As we said in the previous section, one of
its jobs is to tell you how the state-vector changes with time.
So let‚Äôs write out the time-dependent Schr¬®odinger equation:
i‚àÇœà
‚àÇt = 1
¬ØhHœà.
Substituting for H using 10.12, we get
i‚àÇœà
‚àÇt = ‚àí¬Øh
2
‚àÇ2œà
‚àÇx2 + 1
2¬Øhœâ2x2œà.
(10.13)
This equation says that if you know œà (both the real and
imaginary parts) at some particular time, you can predict
what it will be at a future time. Notice that the equation is
complex‚Äîit contains i as a factor. This means that even if œà
starts out being real-valued at time t = 0, it will very shortly
develop an imaginary part. Any solution œà must therefore
be a complex function of x and t.
You can solve this equation in a number of ways. For
example, you can solve it numerically on a computer. Start
with a known value of œà(x) and update it slightly by calcu-
lating the derivative. Once you have the derivative, calcu-
late how œà(x) changes in a small increment of time. Then,

322
LECTURE 10. THE HARMONIC OSCILLATOR
add this incremental change to œà(x) and keep doing it over
and over. It turns out that œà(x) will do some interesting
things‚Äîit will move around somehow. In fact, under certain
circumstances, it will form a wave packet that moves around
very much like a harmonic oscillator.
10.4
Energy Levels
The other thing you can do with the Hamiltonian is calcu-
late the energy levels of the oscillator, by Ô¨Ånding the energy
eigenvectors and eigenvalues. As we learned in Lecture 4,
once you know these eigenvectors and eigenvalues, you can
Ô¨Ågure out the time dependence without solving any diÔ¨Äer-
ential equations. That‚Äôs because you already know the time
dependence of each energy eigenvector. You may want to
review the Schr¬®odinger‚Äôs Ket recipe we gave in Section 4.13.
For now, let‚Äôs concentrate on Ô¨Ånding the energy eigen-
vectors themselves, using the time-independent Schr¬®odinger
equation:
H|œàE‚ü©= E|œàE‚ü©.
The subscript E indicates that œàE is the eigenvector for a
particular eigenvalue E. This equation deÔ¨Ånes two things:
the wave functions œàE(x) and the energy levels E.
Let‚Äôs
make things less abstract by expanding H using Eq. 10.12:
‚àí¬Øh2
2
‚àÇ2œàE(x)
‚àÇx2
+ 1
2œâ2x2œàE(x) = EœàE(x).
(10.14)
To solve this equation, we must:

10.4. ENERGY LEVELS
323
‚Ä¢ Find the allowable values of E that permit a mathe-
matical solution.
‚Ä¢ Find the eigenvectors and possible eigenvalues of the
energy.
This is a little trickier than you might think. There turns
out to be a solution to the equation for every value of E,
including all the complex numbers, but most solutions are
physically absurd. If we just start at some point and solve
the Schr¬®odinger equation by making little incremental steps,
we will almost always Ô¨Ånd that œà(x) grows or ‚Äúblows up‚Äù
as x becomes large. In other words, we may be able to Ô¨Ånd
solutions to the equation, but only very rarely will we Ô¨Ånd a
normalizable solution.
In fact, for most values of E, including all the complex
numbers, the solutions of Eq. 10.14 grow exponentially as x
approaches ‚àû, ‚àí‚àû, or both. This type of solution makes
no physical sense; it tells us that there is an overwhelm-
ing probability that the oscillator coordinate is inÔ¨Ånitely far
away. Clearly, we want to impose some condition that gets
rid of such solutions. So let‚Äôs impose one:
Physical solutions of the Schr¬®odinger equation must be
normalizable.
This is a very powerful constraint. In fact, for almost
all values of E, there are no normalizable solutions. But for
certain very special values of E such solutions do exist, and
we will Ô¨Ånd them.

324
LECTURE 10. THE HARMONIC OSCILLATOR
10.5
The Ground State
What is the lowest possible energy level for a harmonic oscil-
lator? In classical physics, the energy can never be negative
because the Hamiltonian has an x2 term and a p2 term; to
minimize energy, we just set p and x equal to zero.
But
in quantum mechanics, that‚Äôs asking too much. The uncer-
tainty principle says that you can‚Äôt set both x and p equal
to zero. The best you can do is Ô¨Ånd a compromise state in
which x and p are not too spread out. Because you have
to compromise, the lowest possible energy will not be zero.
Neither p2 nor x2 will be zero. Because the operators X2
and P2 can have only positive eigenvalues, the harmonic os-
cillator has no negative energy levels, and in fact, it has no
state with zero energy either.
If all the energy levels of a system must be positive, there
must be a lowest allowable energy and a wave function to go
with it. This lowest energy level is called the ground state
and is denoted by œà0(x). Keep in mind that the subscript
0 does not mean that the energy is zero; it means that it is
the lowest allowable energy.
There is a very useful mathematical theorem that helps
identify the ground state. We won‚Äôt prove it here, but it is
very simple to state:
The ground-state wave function for any potential has no
zeros and it‚Äôs the only energy eigenstate that has no
nodes.
So all we have to do to Ô¨Ånd the ground state of our har-
monic oscillator is to Ô¨Ånd a nodeless solution for some value

10.5. THE GROUND STATE
325
of E. It doesn‚Äôt matter how we Ô¨Ånd it‚Äîwe can use mathe-
matical tricks, make guesses, or just ask the professor. Let‚Äôs
use the latter method. (I‚Äôll play the role of the professor.)
Figure 10.1: Harmonic Oscillator Ground State
Here is a function that works:
œà(x) = e‚àíœâ
2¬Øh x2.
(10.15)
This function is shown schematically in Fig. 10.1. As you
can see, it‚Äôs concentrated near the origin, where we expect
the lowest energy state to be concentrated. It goes to zero
very quickly as it moves away from the origin, so the integral
of the probability density is Ô¨Ånite. And, importantly, it has
no nodes. So it has a chance of being our ground state.
Let‚Äôs see if we can Ô¨Ågure out what the Hamiltonian does
to this function. The Ô¨Årst term of the Hamiltonian (the left
side of Eq. 10.14) tells us to apply the operator

326
LECTURE 10. THE HARMONIC OSCILLATOR
‚àí¬Øh2
2
‚àÇ
‚àÇx2
to œà(x). Let‚Äôs calculate that term, one derivative at a time.
The Ô¨Årst step is
‚àÇœà(x)
‚àÇx
= ‚àíœâ
2¬Øh(2x)e‚àíœâ
2¬Øh x2,
which simpliÔ¨Åes to
‚àÇœà(x)
‚àÇx
= ‚àíœâ
¬Øhxe‚àíœâ
2¬Øh x2.
When we take the second derivative, there will be two terms
because of the product rule:
‚àÇ2œà(x)
‚àÇx2
= ‚àíœâ
¬Øhe‚àíœâ
2¬Øh x2 + œâ2
¬Øh2 x2e‚àíœâ
2¬Øh x2.
Let‚Äôs plug this result back into Eq. 10.14, and at the same
time replace œà on the right side with our guess, e‚àíœâ
2¬Øh x2:
¬Øh
2œâe‚àíœâ
2¬Øh x2 ‚àí1
2œâ2x2e‚àíœâ
2¬Øh x2 + 1
2œâ2x2e‚àíœâ2
2¬Øh x2 = Ee‚àíœâ
2¬Øh x2.
After canceling the terms proportional to x2e‚àíœâ
2¬Øh x2, we dis-
cover the remarkable fact that solving the Schr¬®odinger equa-
tion just reduces to solving
¬Øh
2œâe‚àíœâ
2¬Øh x2 = Ee‚àíœâ
2¬Øh x2.

10.6. CREATION/ANNIHILATION OPERATORS
327
As you can see, the only way we can solve this equation is to
set the energy E equal to œâ¬Øh
2 . In other words, we‚Äôve found not
only the wave function but also the value of the ground-state
energy. Calling the ground-state energy E0, we can write
E0 = œâ¬Øh
2 .
(10.16)
The ground-state wave function, meanwhile, is just the Gaus-
sian function the professor gave us:
œà0(x) = e‚àíœâ
2¬Øh x2.
He‚Äôs a clever fellow, that professor.
10.6
Creation and Annihilation
Operators
Over the course of these lectures, we have seen two ways of
thinking about quantum mechanics. They go all the way
back to Heisenberg and Schr¬®odinger. Heisenberg liked alge-
bra, matrices, and, had he known what to call them, linear
operators.
Schr¬®odinger, by contrast, thought in terms of
wave functions and wave equations, the Schr¬®odinger equa-
tion being one famous example. Of course, the two ways of
thinking are not contradictory; functions form a vector space
and derivatives are operators.
So far, in our study of the harmonic oscillator we have fo-
cused on functions and diÔ¨Äerential equations. But the more
powerful tool in many cases‚Äîparticularly for the harmonic

328
LECTURE 10. THE HARMONIC OSCILLATOR
oscillator‚Äîis the operator method.
It reduces the entire
study of wave functions and wave equations to a very small
number of algebraic tricks, which almost always involve the
commutation relations. In fact, whenever you see a pair of
operators, my advice is to Ô¨Ågure out their commutator. If
the commutator is a new operator that you haven‚Äôt seen be-
fore, Ô¨Ånd its commutator with the original pair. That‚Äôs when
the fun happens.
Obviously, this advice can lead to an unending chain of
boring computations. But once in a while you may get lucky
and Ô¨Ånd a set of operators that close under commutation.
Whenever that happens, you‚Äôre in business; as we will see,
operator methods have tremendous power.
Now, let‚Äôs apply this approach to our harmonic oscillator.
We begin with the Hamiltonian expressed in terms of the
operators P and X:
H = P2 + œâ2X2
2
.
(10.17)
To Ô¨Ågure out the rest of the energy levels, we‚Äôll use some
tricks. The idea is to cleverly use the properties of X and
P (in particular, the commutation relation [X, P] = i¬Øh) to
construct two new operators, called creation and annihila-
tion operators. When a creation operator acts on an energy
eigenvector (or eigenfunction), it produces a new eigenvector
that has the next higher energy level. An annihilation opera-
tor does just the opposite: it produces an eigenvector whose
energy is one level lower than the energy of the eigenvector
it started with. So, roughly speaking, the thing that they
create and annihilate is energy. They‚Äôre also called raising

10.6. CREATION/ANNIHILATION OPERATORS
329
and lowering operators. But remember: operators act on
state vectors, not on systems. To see how these operators
work, let‚Äôs rewrite the Hamiltonian in the form
H = 1
2(P2 + œâ2X2).
(10.18)
This is a classical as well as a quantum mechanical Hamil-
tonian, and it would be just as correct to use the lowercase
symbols p and x. However, we‚Äôre using the boldface P and X
because we plan to focus on the quantum mechanical Hamil-
tonian.
Let‚Äôs start by doing a manipulation that is correct for
classical physics but will require some modiÔ¨Åcation for quan-
tum mechanics. In the parentheses above, we have a sum of
squares. Using the formula
a2 + b2 = (a + ib)(a ‚àíib),
it seems that we can rewrite the Hamiltonian as
H ‚Äú = ‚Äù 1
2(P + iœâX)(P ‚àíiœâX),
(10.19)
and that‚Äôs almost correct. Why almost? Because quantum
mechanically, P and X do not commute, and we need to be
careful about the order of operations. Let‚Äôs expand our fac-
tored expression and see how it might diÔ¨Äer from the original
Hamiltonian in Eq. 10.18. Keeping careful track of the order
of factors, we can expand the expression as follows:

330
LECTURE 10. THE HARMONIC OSCILLATOR
1
2(P + iœâX)(P ‚àíiœâX) = 1
2(P2 + iœâXP ‚àíiœâPX ‚àíi2œâ2X2)
= 1
2(P2 + iœâ(XP ‚àíPX) ‚àíi2œâ2X2)
= 1
2(P2 + iœâ(XP ‚àíPX) + œâ2X2)
= 1
2(P2 + œâ2X2) + 1
2iœâ(XP ‚àíPX).
Look at the right-hand set of parentheses in the Ô¨Ånal line.
We have seen that expression before‚Äîit‚Äôs the commutator
of X and P. In fact, we already know its value:
(XP ‚àíPX) = [X, P] = i¬Øh.
Thus, the expression for our factored Hamiltonian becomes
1
2(P2 + œâ2X2) + 1
2iœâi¬Øh
or
1
2(P2 + œâ2X2) ‚àí1
2œâ¬Øh.
In other words, the factored expression we started out with
in Eq. 10.19 is actually smaller than the Hamiltonian by œâ¬Øh
2 .
To recover the actual Hamiltonian, we need to add the œâ¬Øh
2
back in:
H = 1
2(P + iœâX)(P ‚àíiœâX) + œâ¬Øh
2 .

10.6. CREATION/ANNIHILATION OPERATORS
331
Rewriting the Hamiltonian this way and that way may seem
like an exercise in futility, but trust me, it‚Äôs not. First of
all, the last term is just an additive constant that adds the
numerical value œâ¬Øh
2 to every energy eigenvalue. We can ignore
it for now. Later, after we‚Äôve solved the rest of the problem,
we can add it back in. The guts of the problem are found
in the expression (P + iœâX)(P ‚àíiœâX). It turns out that
these two factors, (P + iœâX) and (P ‚àíiœâX), have some
very remarkable properties. In fact, they are the raising and
lowering operators (or creation and annihilation operators)
that I told you about earlier. For now, these are just names,
but as we go along we‚Äôll see that the names were well chosen.
The obvious deÔ¨Ånitions would be
a‚àí= (P ‚àíiœâX)
for the lowering operator, and
a+ = (P + iœâX)
for the raising operator. But history sometimes preempts
the obvious. Historically, the raising and lowering operators
have been deÔ¨Åned with an extra factor in front of them. Here
are the oÔ¨Écial deÔ¨Ånitions:
a‚àí=
i
‚àö
2œâ¬Øh
(P ‚àíiœâX),
(10.20)
a+ =
‚àíi
‚àö
2œâ¬Øh
(P + iœâX),
(10.21)

332
LECTURE 10. THE HARMONIC OSCILLATOR
If we use these deÔ¨Ånitions, the Hamiltonian starts to look
very simple:
H = œâ¬Øh(a+a‚àí+ 1/2).
(10.22)
There are only two properties of a+ and a‚àíthat we need
to know. The Ô¨Årst is that they are Hermitian conjugates of
each other. That follows from their deÔ¨Ånitions. The other
property is what really gives them juice. The commutator
of a+ and a‚àíis
[a‚àí, a+] = 1.
This is easy to prove. First, we use the deÔ¨Ånitions to write
[a‚àí, a+] =
1
2œâ¬Øh [(P ‚àíiœâX), P + iœâX)]
The next step is to use the commutation relations [X, X] = 0,
[P, P] = 0, and [X, P] = i¬Øh.
Apply these to the above
equation, and you will quickly Ô¨Ånd that [a‚àí, a+] = 1.
We can make the Hamiltonian in Eq. 10.22 even simpler
by deÔ¨Åning a new operator,
N = a+a‚àí,
called the number operator. Once again, this is just a name,
but as we‚Äôll see, it‚Äôs a very good name. Stated in terms of
the number operator, the Hamiltonian becomes
H = œâ¬Øh(N + 1/2).
(10.23)

10.6. CREATION/ANNIHILATION OPERATORS
333
So far, all we‚Äôve done is deÔ¨Åne some symbols, a+, a‚àí, and
N, that make the Hamiltonian look deceptively simple; it‚Äôs
not clear that we are actually any closer to Ô¨Åguring out the
energy eigenvalues. To proceed further, let‚Äôs recall my earlier
advice: whenever you see two operators, commute them. In
this case, we already know one commutator:
[a‚àí, a+] = 1.
(10.24)
Next, let‚Äôs Ô¨Ånd the commutator of the raising and lowering
operators with the number operator N.
We‚Äôll do this by
brute force. Here are the steps:
[a‚àí, N] = a‚àíN ‚àíNa‚àí= a‚àía+a‚àí‚àía+a‚àía‚àí.
Now, we‚Äôll combine the terms in the form
[a‚àí, N] = (a‚àía+ ‚àía+a‚àí)a‚àí.
This looks complicated until we notice that the expression
in the parentheses is just [a‚àí, a+], which just happens to be
1. Using this fact to simplify, we get
[a‚àí, N] = a‚àí.
We can do the same thing with a+ and N. The result is
almost the same except for the sign. Here is the whole list
of commutators in one neat package:

334
LECTURE 10. THE HARMONIC OSCILLATOR
[a‚àí, a+]
=
1
[a‚àí, N]
=
a‚àí
[a+, N]
=
‚àía+.
(10.25)
This is what you might call a commutator algebra: a set of
operators that closes under commutation. Commutator al-
gebras have wonderful properties that make them one of the
theoretical physicist‚Äôs favorite tools. We are now going to see
the power of this commutator algebra in the iconic example
of the harmonic oscillator, using it to Ô¨Ånd the eigenvalues
and eigenvectors of N. Once we know these, we can imme-
diately read oÔ¨Äthe eigenvalues of H from Eq. 10.23. The
trick is to use a kind of induction procedure: we begin by
supposing we have an eigenvalue and eigenvector of N. Call
the eigenvalue n and the eigenvector |n‚ü©. By deÔ¨Ånition,
N|n‚ü©= n|n‚ü©.
Now, let‚Äôs consider a new vector, obtained by acting on |n‚ü©
with a+. Let‚Äôs prove that the result is a diÔ¨Äerent eigenvector
of N, with a diÔ¨Äerent eigenvalue. Again, we accomplish this
by straightforward application of the commutation relations.
We‚Äôll start by writing the expression N(a+|n‚ü©) in a slightly
more complicated form,
N(a+|n‚ü©) = [a+N ‚àí(a+N ‚àíNa+)]|n‚ü©.
The expression in brackets on the right-hand side is the same
as Na+, with the term a+N added and then subtracted. But

10.6. CREATION/ANNIHILATION OPERATORS
335
notice that the expression in parentheses is the last of the
commutators from Eqs. 10.25. If we plug that in, we get
N(a+|n‚ü©) = a+(N + 1)|n‚ü©.
The last step is to use the fact that |n‚ü©is an eigenvector of
N with eigenvalue n. That means we can replace (N + 1)
with (n + 1):
N(a+|n‚ü©) = (n + 1)(a+|n‚ü©).
(10.26)
As always, when we run on autopilot, we have to keep our
eyes open for interesting results. Eq. 10.26 is interesting. It
says that the vector a+|n‚ü©is a new eigenvector of N with
eigenvalue (n+1). In other words, given the eigenvector |n‚ü©,
we have discovered another eigenvector whose eigenvalue is
increased by 1. All of this can be summarized by the equa-
tion
a+|n‚ü©= |n + 1‚ü©.
(10.27)
Obviously, we can do this again and again to Ô¨Ånd the eigen-
vectors |n+2‚ü©, |n+3‚ü©, and so on. Remarkably, we Ô¨Ånd that
if there is an eigenvalue n, there must be an inÔ¨Ånite sequence
of eigenvalues above it, spaced by integers. The name raising
operator seems well chosen.
What about the lowering operator? Not surprisingly, we
Ô¨Ånd that a‚àí|n‚ü©produces an eigenvector whose eigenvalue is
one unit lower:
a‚àí|n‚ü©= |n ‚àí1‚ü©.
(10.28)

336
LECTURE 10. THE HARMONIC OSCILLATOR
This suggests that there must be an unending sequence of
eigenvalues below n, but that can‚Äôt be correct. We already
know that the ground state has positive energy, and because
H = œâ¬Øh(N+1/2) the downward sequence must end. But the
only possible way it can end is for there to be an eigenvector
|0‚ü©such that when a‚àíacts on it, the result is zero. (We
should not confuse |0‚ü©with the zero vector.3) Symbolically,
this can be expressed as
a‚àí|0‚ü©= 0.
(10.29)
Being the lowest energy state, |0‚ü©is the ground state, and its
energy is E0 = œâ¬Øh/2. It is an eigenvector of N with an eigen-
value 0. We often say that the ground state is annihilated
by a‚àí.
So you see, the abstract construction of a+, a‚àí, and N
paid oÔ¨Ä. It allowed us to Ô¨Ånd the entire spectrum of har-
monic oscillator energy levels without solving a single diÔ¨É-
cult equation. This spectrum consists of the energy values,
En
=
œâ¬Øh(n + 1/2)
=
œâ¬Øh ( 1/2, 3/2, 5/2, . . . ).
(10.30)
This quantization of harmonic oscillator energy levels was
one of the Ô¨Årst results of quantum mechanics, and arguably
the most important. The hydrogen atom is a wonderful ex-
ample of quantum mechanics, but it is, after all, just the
3The 0 vector is the vector whose components are all zero. The
vector |0‚ü©, on the other hand, is a state-vector with nonzero compo-
nents.

10.7. BACK TO WAVE FUNCTIONS
337
hydrogen atom. The harmonic oscillator, on the other hand,
shows up everywhere, from crystal vibrations to electric cir-
cuits to electromagnetic waves.
The list goes on.
Even
macroscopic oscillators, like a child on a swing, have quan-
tized energy levels, but the presence of Planck‚Äôs constant in
Eq. 10.30 means that the spacing between levels is so tiny
that they are completely undetectable.
The unending spectrum of positive energy levels for a
harmonic oscillator is sometimes called a tower, and some-
times called a ladder. It is illustrated schematically in Fig.
10.2.
10.7
Back to Wave Functions
This exercise has amply demonstrated the remarkable power
of operator algebras, and the operator method is indeed re-
markable. But it‚Äôs also very abstract. Is it useful in helping
us Ô¨Ånd wave functions, which are more concrete and easier
to visualize? Absolutely.
Let‚Äôs begin with the ground state. We just saw in Eq.
10.29 that the ground state is the unique state that is annihi-
lated by a‚àí. Now, let‚Äôs rewrite Eq. 10.29 in terms of the po-
sition and momentum operators, and the ground-state wave
function œà0(x):
i
‚àö
2œâ¬Øh
(P ‚àíiœâX)œà0(x) = 0,
or, dividing by the constant factor,
(P ‚àíiœâX)œà0(x) = 0.

338
LECTURE 10. THE HARMONIC OSCILLATOR
Figure 10.2: Harmonic Oscillator Energy Level Ladder. En-
ergy levels are evenly spaced. a+ and a‚àíraise and lower the
energy level respectively. N has a lower limit of zero (the
ground state), but no upper limit.
If we now replace P with ‚àíi¬Øh d
dx, we get a Ô¨Årst-order diÔ¨Äer-
ential equation that is much simpler than the second-order
Schr¬®odinger equation:
dœà0
dx = ‚àíœâx
¬Øh œà0(x).
This is a simple diÔ¨Äerential equation that you can easily

10.7. BACK TO WAVE FUNCTIONS
339
solve. Or, you can just check that the ground-state wave
function
e‚àíœâ
2¬Øh x2
in Eq. 10.15 solves it. Calculating the wave functions for
the excited (nonground) states is even easier‚Äîwe don‚Äôt even
have to solve any equations. Let‚Äôs go up the ladder to n =
+1. We can do that by applying a+ to the ground state.
Let‚Äôs call the wave function of this new state œà1(x).
To avoid dragging the constant ‚àíi/
‚àö
2œâ¬Øh around in our
calculations, we‚Äôll just drop it in our deÔ¨Ånition of a+. This
only aÔ¨Äects the numerical coeÔ¨Écient. The resulting equation
is
œà1(x) = (P + iœâX)œà0(x)
or
œà1(x) =

‚àíi¬Øh ‚àÇ
‚àÇx + iœâx

e‚àíœâ
2¬Øh x2.
Factoring out the i, we get
œà1(x) = i

‚àí¬Øh ‚àÇ
‚àÇx + œâx

e‚àíœâ
2¬Øh x2.
The ‚Äúhardest‚Äù part of working this out is performing an easy
derivative of e‚àíœâ
2¬Øh x2. Here is the result:
œà1(x) = 2iœâxe‚àíœâ
2¬Øh x2,

340
LECTURE 10. THE HARMONIC OSCILLATOR
or
œà1(x) = 2iœâxœà0(x).
The only important diÔ¨Äerence between œà0 and œà1 is the pres-
ence of the factor x in œà1. This has an eÔ¨Äect: it causes the
wave function of the Ô¨Årst excited state to have a zero, or
node, at x = 0. This is a pattern that continues as we go up
the ladder: each successive excited state has an additional
node. We can see this pattern emerge by calculating the sec-
ond excited state at n = 2. All we have to do is apply a+
again:
œà2(x) = i

‚àí¬Øh ‚àÇ
‚àÇx + œâx

xe‚àíœâ
2¬Øh x2
.
We can see right away that the œâx term will result in an œâx2
term. The ‚àí‚àÇ
‚àÇx, meanwhile, will result in two terms because
of the product rule for derivatives. One of these terms will
come from the exponential (producing another œâx).
The
other will come from taking the derivative of x. It‚Äôs clear
that what we‚Äôll end up with is a quadratic polynomial. If we
work out these derivatives, the resulting wave function is
œà2(x) = (‚àí¬Øh + 2œâx2)e‚àíœâ
2¬Øh x2.
And so it goes, all the way up the ladder. We can see another
pattern here: each eigenfunction is a polynomial in x multi-
plied by e‚àíœâ
2¬Øh x2. Because the exponential goes to zero faster

10.7. BACK TO WAVE FUNCTIONS
341
than any of these polynomials grows, each eigenfunction ap-
proaches zero asymptotically as x goes to plus or minus in-
Ô¨Ånity. Also, because the degree of each polynomial is one
greater than the degree of the previous one, each eigenfunc-
tion has one more zero than the previous one.4 This also ex-
plains why successive eigenfunctions alternate between being
symmetric and antisymmetric. SpeciÔ¨Åcally, eigenfunctions
with polynomials of even degree are symmetric, while those
with polynomials of odd degree are antisymmetric.
The
polynomials in this sequence are very well-known. They‚Äôre
called the Hermite polynomials.
The ground-state eigen-
function e‚àíœâ
2 x2, which appears in all of these higher-energy
eigenfunctions, is symmetric in x.
Fig. 10.3 displays the eigenfunctions for several diÔ¨Äer-
ent energy levels.
Each successive eigenfunction oscillates
more rapidly than the one before it. This corresponds to
an increase in momentum. The more rapidly the wave func-
tion oscillates, the greater the momentum of the system. At
higher energy levels, the wave function also becomes more
spread out. In physical terms, this means the mass is mov-
ing farther from the equilibrium point, and moving faster.
These eigenfunctions contain another important lesson.
Although they approach zero asymptotically (quite rapidly)
they never quite reach zero. That means there is a small
but Ô¨Ånite chance of Ô¨Ånding the particle ‚Äúoutside the bowl‚Äù
that deÔ¨Ånes its potential energy function. This phenomenon,
4It turns out that these zeros occur for real values of x, but that‚Äôs
not obvious from what we‚Äôve seen. In a physical sense, the zeros seem a
little weird, because they are points where the moving mass will never
be found, even though it‚Äôs merrily whizzing back and forth.

342
LECTURE 10. THE HARMONIC OSCILLATOR
known as quantum tunneling, is completely unknown in clas-
sical physics.
10.8
The Importance of
Quantization
We‚Äôve climbed a high mountain in these lectures, but it‚Äôs not
the last mountain. Looking out from the present vantage
point, we can get a glimpse of the enormous landscape of
quantum Ô¨Åeld theory. That‚Äôs material for another book. Or
maybe three. But still, we can see a bit of the terrain from
where we are.
Consider the example of electromagnetic radiation in a
cavity, as shown in Fig. 10.4. In this context, a cavity is
a region of space bracketed by a pair of perfectly reÔ¨Çecting
mirrors that keep the radiation bouncing endlessly back and
forth. Think of the cavity as a long metallic tube that the
radiation can travel along in both directions.
There are many wavelengths that can Ô¨Åt into the cavity.
Let‚Äôs consider waves of length Œª. Like all waves, these waves
oscillate, very much like a mass on the end of a spring. But
it‚Äôs important not to get confused here: the oscillators are
not masses attached to springs.
What‚Äôs really oscillating
are the electric and magnetic Ô¨Åelds. For each wavelength,
there is a mathematical harmonic oscillator describing the
amplitude or strength of the Ô¨Åeld. That‚Äôs a lot of harmonic
oscillators all running simultaneously. Fortunately, however,
they all oscillate independently, so we can focus our attention
on waves of one particular wavelength and ignore all the

10.8. THE IMPORTANCE OF QUANTIZATION
343
œà0(x)
œà 2
0 (x)
œà1(x)
œà 2
1 (x)
œà2(x)
œà 2
2 (x)
œà3(x)
œà 2
3 (x)
œà4(x)
œà 2
4 (x)
œà20(x)
œà 2
20(x)
Figure 10.3: Harmonic Oscillator Eigenfunctions.
Ampli-
tudes are shown on the left, probabilities on the right. The
higher-energy wave functions oscillate more rapidly and are
more spread out.

344
LECTURE 10. THE HARMONIC OSCILLATOR
Figure 10.4: Electromagnetic Radiation in a Cavity
others.
There is only one important number associated with a
harmonic oscillator‚Äînamely, its frequency.
You probably
already know how to calculate the frequency of a wave of
length Œª:
œâ = 2œÄc
Œª .
In classical physics, of course, the frequency is just the fre-
quency.
But in quantum mechanics, the frequency deter-
mines the quantum of energy of the oscillator.
In other
words, the energy contained in waves of length Œª has to be
(n + 1/2)¬Øhœâ.
The term (1/2)¬Øhœâ is not important for our purposes. It‚Äôs
called the zero-point energy, and we can ignore it. If we do,
the energy of waves of length Œª becomes
2œÄ¬Øhc
Œª
n,
where n can be any integer from zero on up. In other words,

10.8. THE IMPORTANCE OF QUANTIZATION
345
the energy of an electromagnetic wave is quantized in indi-
visible units of
2œÄ¬Øhc
Œª
.
For a classical physicist this is very odd. No matter what
you do, the energy always comes in unbreakable units.
You may already know that these units are called pho-
tons. In fact, photon is just another name for the quantized
unit of energy in a quantum harmonic oscillator. But we
can also describe the same facts another way. Being indivis-
ible, photons can be thought of as elementary particles. A
wave excited to its nth quantum state can be thought of as
a collection of n photons.
What is the energy of a single photon? That‚Äôs easy. It‚Äôs
just the energy that it takes to add one more unit, namely
E(Œª) = 2œÄ¬Øhc
Œª
.
Here, we can see something that has dominated physics for
well over a century: the shorter the wavelength of a photon,
the higher its energy. Why would a physicist be interested in
making short-wavelength photons, given that they are costly
in energy? The answer is to see more clearly. As discussed
in Lecture 1, to resolve an object of a given size, you must
use waves of that size or smaller. To see a human Ô¨Ågure, a
wavelength of a few inches is good enough. To see a tiny
speck of dust, you may need visible light of a much smaller
wavelength. To resolve the parts of a proton, the wavelength
must be smaller than 10‚àí15 meters, and the corresponding

346
LECTURE 10. THE HARMONIC OSCILLATOR
photons must be very energetic. In the end, it all goes back
to the harmonic oscillator.
On that note, my friends, we conclude this volume of the
Theoretical Minimum series. I look forward to seeing you in
Special Relativity.
¬© Margaret  Sloan 

Appendix
Pauli Matrices
œÉz
=
 1
0
0
‚àí1

œÉx
=
 0
1
1
0

œÉy
=
 0
‚àíi
i
0

347

348
APPENDIX
Action of Spin Operators
|u‚ü©=
 1
0

‚áê‚áí
œÉz|u‚ü©= |u‚ü©
œÉx|u‚ü©= |d‚ü©
œÉy|u‚ü©= i|d‚ü©
|d‚ü©=
 0
1

‚áê‚áí
œÉz|d‚ü©= ‚àí|d‚ü©
œÉx|d‚ü©= |u‚ü©
œÉy|d‚ü©= ‚àíi|u‚ü©
|r‚ü©=
 
1
‚àö
2
1
‚àö
2
!
‚áê‚áí
œÉz|r‚ü©= |l‚ü©
œÉx|r‚ü©= |r‚ü©
œÉy|r‚ü©= ‚àíi|l‚ü©
|l‚ü©=
 
1
‚àö
2
‚àí1
‚àö
2
!
‚áê‚áí
œÉz|l‚ü©= |r‚ü©
œÉx|l‚ü©= ‚àí|l‚ü©
œÉy|l‚ü©= i|r‚ü©
|i‚ü©=
 
1
‚àö
2
i
‚àö
2
!
‚áê‚áí
œÉz|i‚ü©= |o‚ü©
œÉx|i‚ü©= i|o‚ü©
œÉy|i‚ü©= |i‚ü©
|o‚ü©=
 
1
‚àö
2
‚àíi
‚àö
2
!
‚áê‚áí
œÉz|o‚ü©= |i‚ü©
œÉx|o‚ü©= ‚àíi|i‚ü©
œÉy|o‚ü©= ‚àí|o‚ü©

APPENDIX
349
Change of Basis
|r‚ü©
=
1
‚àö
2|u‚ü©+ 1
‚àö
2|d‚ü©
|l‚ü©
=
1
‚àö
2|u‚ü©‚àí1
‚àö
2|d‚ü©
|i‚ü©
=
1
‚àö
2|u‚ü©+
i
‚àö
2|d‚ü©
|o‚ü©
=
1
‚àö
2|u‚ü©‚àí
i
‚àö
2|d‚ü©
Spin Component in the ÀÜn Direction
Vector Notation
œÉn = ‚ÉóœÉ ¬∑ ÀÜn
Component Form
œÉn = œÉxnx + œÉyny + œÉznz
More Concretely
œÉn = nx
 0
1
1
0

+ ny
 0
‚àíi
i
0

+ nz
 1
0
0
‚àí1

Combined in a Single Matrix
œÉn =

nz
(nx ‚àíiny)
(nx + iny)
‚àínz


350
APPENDIX
Spin Operator Multiplication
Tables
A word about notation: Table 3 below uses the symbol i in
two diÔ¨Äerent ways. Inside a ket, such as |io‚ü©, it is part of a
state-label‚Äîio signiÔ¨Åes ‚Äúin-out.‚Äù But when i appears outside
of a ket symbol, as in i|oo‚ü©, it signiÔ¨Åes the unit imaginary
number.
Table 1: Up-Down Basis
2-Spin Eigenvectors
|uu‚ü©
|ud‚ü©
|du‚ü©
|dd‚ü©
œÉz
|uu‚ü©
|ud‚ü©
‚àí|du‚ü©
‚àí|dd‚ü©
œÉx
|du‚ü©
|dd‚ü©
|uu‚ü©
|ud‚ü©
œÉy
i|du‚ü©
i|dd‚ü©
‚àíi|uu‚ü©
‚àíi|ud‚ü©
œÑz
|uu‚ü©
‚àí|ud‚ü©
|du‚ü©
‚àí|dd‚ü©
œÑx
|ud‚ü©
|uu‚ü©
|dd‚ü©
|du‚ü©
œÑy
i|ud‚ü©
‚àíi|uu‚ü©
i|dd‚ü©
‚àíi|du‚ü©

APPENDIX
351
Table 2: Right-Left Basis
2-Spin Eigenvectors
|rr‚ü©
|rl‚ü©
|lr‚ü©
|ll‚ü©
œÉz
|lr‚ü©
|ll‚ü©
|rr‚ü©
|rl‚ü©
œÉx
|rr‚ü©
|rl‚ü©
‚àí|lr‚ü©
‚àí|ll‚ü©
œÉy
‚àíi|lr‚ü©
‚àíi|ll‚ü©
i|rr‚ü©
i|rl‚ü©
œÑz
|rl‚ü©
|rr‚ü©
|ll‚ü©
|lr‚ü©
œÑx
|rr‚ü©
‚àí|rl‚ü©
|lr‚ü©
‚àí|ll‚ü©
œÑy
‚àíi|rl‚ü©
i|rr‚ü©
‚àíi|ll‚ü©
i|lr‚ü©
Table 3: In-Out Basis
2-Spin Eigenvectors
|ii‚ü©
|io‚ü©
|oi‚ü©
|oo‚ü©
œÉz
|oi‚ü©
|oo‚ü©
|ii‚ü©
|io‚ü©
œÉx
i|oi‚ü©
i|oo‚ü©
‚àí|ii‚ü©
‚àí|io‚ü©
œÉy
|ii‚ü©
|io‚ü©
‚àí|oi‚ü©
‚àí|oo‚ü©
œÑz
|io‚ü©
|ii‚ü©
|oo‚ü©
|oi‚ü©
œÑx
i|io‚ü©
‚àíi|ii‚ü©
i|oo‚ü©
‚àíi|oi‚ü©
œÑy
|ii‚ü©
‚àí|io‚ü©
|oi‚ü©
‚àí|oo‚ü©


2 √ó 2 matrices, combining, 188
3-vector operators, 75, 83‚Äì85, 119
3-vectors, 25, 27, 74‚Äì75, 83
orthogonal unit vectors and, 
32‚Äì33
4 √ó 4 matrices, from combined 2 √ó 2 
matrices, 188
Addition
of complex numbers, 23
vector, 26
Amplitude, 39, 108, 342, 343
for paths, 306‚Äì309
and rule, 14, 15, 20
Annihilation operators, 327‚Äì337
Anti-Hermitian operator, 250
Antisymmetric eigenfunctions, 341
Apparatus, measurement and, 5‚Äì13, 
37‚Äì38, 71, 75, 81‚Äì82, 83‚Äì84, 91, 
126‚Äì127, 180, 219‚Äì224, 227‚Äì230
Associative property, 26, 193, 239
Atoms, 259, 290, 311
in crystal lattice, 313
hydrogen, 336‚Äì337
quantum mechanics and, 2, 71, 
149, 316
size of, 104
spins of, 180‚Äì181
wave packets and, 297, 301
Average,140‚Äì141, 157‚Äì158, 213, 271, 
286, 288, 292, 295
bra-ket notation for, 106‚Äì107
defining, 105‚Äì106
See also Expectation values
Average value, 105
Axioms, vector space, 24‚Äì27
Basis of simultaneous eigenvectors, 
131‚Äì133
Basic vectors, 32‚Äì34, 38, 40, 41, 
48‚Äì49, 54, 55, 64, 67, 97, 
98, 106, 120‚Äì125, 130‚Äì136, 
173, 185, 189, 191,195, 196, 
198, 202, 204, 208, 210, 211, 
219, 224, 236, 237, 251, 258, 
260‚Äì263, 275
components, 56
entangled states, 165‚Äì167
labeling, 150.151, 152, 153, 154, 
160‚Äì163
product states, 163‚Äì165
Bell, John, 223, 227
Bell‚Äôs theorem, 227‚Äì231
Boolean logic, 13‚Äì18
Bracket notation, 11
Bra-ket notation, 105
for averages, 106‚Äì107
Bras (bra vectors), 28‚Äì30, 240
inner product and, 30‚Äì32
linear operators and, 58‚Äì59
outer products and, 194
Index
353

354 
INDEX
Canonical momentum, 315, 318‚Äì320
Canonical momentum conjugate to 
x, 315, 318‚Äì320
Cartesian coordinates, 89, 116,  
136
Cartesian representation, of complex 
number, 22
Cauchy-Schwarz inequality, 142
triangle inequality and, 142‚Äì146
Change
in classical physics, 94
continuity and, 100
unitarity and incremental, 100
Classical entanglement, 155‚Äì160
Classical equations, quantization 
and, 289‚Äì290
Classical limit, 295‚Äì301
Classical physics
change in, 94
change in expectation values over 
time and, 109‚Äì114
commutators and, 266‚Äì268
momentum in, 255
particle dynamics and, 279
pure and mixed states and, 
199‚Äì200
quantum mechanics vs., 2‚Äì3
testing propositions of, 16‚Äì18
Collapse of the wave function, 
126‚Äì127
Column vectors, 27‚Äì28, 49
kets and, 29
spin states as, 47
Commutation relations, 118, 119, 
138‚Äì139, 287, 309, 328, 332, 
334
Commutative property, 26
Commutator algebra, 334‚Äì337
Commutators, 111‚Äì116, 138, 142, 
146, 147, 269, 280, 287, 293, 
294,
classical physics and, 266‚Äì268
operators and, 328, 330, 332, 333, 
334, 335
Poisson brackets and, 112‚Äì114, 
265‚Äì268
Commuting variables, complete sets 
of, 129‚Äì136
wave functions, 134‚Äì136
Complex conjugate, 23
Complex conjugate numbers, 28, 30
Complex conjugation, for operators, 
59‚Äì61
Complex numbers, 21‚Äì30, 34, 38, 
42, 44
addition of, 23
eigenvalues and, 58
multiplication of, 23
phase-factors, 24
representations of, 22
Complex vector spaces, orthonormal 
basis and, 33
Component matrices, building 
tensor product matrices from, 
188‚Äì192
Component, 56
of 3-vector, 25, 74‚Äì75, 83, 116
addition of, 27
of angular momentum, 119
of basis vector, 56
of generic state, 38
inner products and, 31, 34
multiplication of, 28
of phase factor, 24
of spin, 9, 13, 16‚Äì17, 20, 37, 69, 
71, 75, 77, 83‚Äì84, 87, 90‚Äì91, 
116‚Äì117, 119, 130‚Äì131, 138‚Äì
139, 162, 167‚Äì168, 170, 174‚Äì
175, 176, 178‚Äì179, 180‚Äì181, 
218, 222, 251, 257, 260, 349
of spin operator, 71‚Äì72, 75, 116
of state-vector, 40, 227, 237, 336n
of system, 154, 222
of vector, 8, 9‚Äì10
wave functions and, 136
Component form
of addition, 23, 27‚Äì28
of bra-vectors, 59
equation in, 54, 59, 79
 of multiplication, 58‚Äì59
of tensor product operators, 155, 
171‚Äì172, 184, 188, 204

INDEX 
355
Component matrices, 188‚Äì192
Composite observables, 175‚Äì181
Composite operator
composite vectors and, 171
energy and measurement of, 
180‚Äì181
Composite state, two spin, 161‚Äì181
Composite systems
mixed and pure states and, 
200‚Äì201
observables in, 167‚Äì175
product states, 163‚Äì165
representing, 151‚Äì155
tensor products and, 150‚Äì155
See also Entanglement
Composite vectors, composite 
operators and, 171
Conservation of distinctions,  
97‚Äì99
Conservation of energy, 114‚Äì115
Conservation of overlaps, 99
Continuity, 100‚Äì101
Continuous functions, 236‚Äì250
functions as vectors, 238‚Äì245
integration by parts, 245‚Äì246
linear operators, 246‚Äì250
wave functions and, 236‚Äì238
Correlation
of near-singlet state, 234
of product state, 232
of singlet state, 233
Correlation test for entanglement, 
213‚Äì214
Creation operators, 327‚Äì337
Crystal lattice, atom in, 313
Degeneracy, 64
Density matrices, 184, 196‚Äì199
calculating, 210‚Äì212
entanglement and, 199‚Äì202
of near-singlet state, 234
notation for, 201‚Äì202
of product state, 232
properties of, 207
for single spin, 202‚Äì203
of singlet state, 233
two-spin system and, 203‚Äì217, 
231
Density matrix test for 
entanglement, 214‚Äì218
Determinism
in classical physics, 94
in quantum mechanics, 9‚Äì11, 96
Dirac, Paul, 105, 113, 194, 278
Dirac delta functions, 241, 242‚Äì245, 
253
Dirac‚Äôs bracket notation, 11
Distributive property, 26
Dot product, 30, 31, 144, 180
Down states, 219‚Äì221
Dual number systems, 23
Eigen-equation, 256
Eigenfunctions, 253
alternation between being 
symmetric and antisymmetric, 
340‚Äì341
for energy levels, 341, 343
harmonic oscillator, 341, 343
Eigenstate, collapse of the wave 
function and, 126‚Äì127
Eigenvalues, 56‚Äì59, 70, 71‚Äì72
of density matrix, 207, 215‚Äì217
energy, 121, 322‚Äì323
of Hermitian operators, 62‚Äì63
of operators, 80
of position, 252‚Äì254
of spin operator, 76, 77‚Äì78
Eigenvectors, 56‚Äì59, 70
of annihilation operator, 328
of creation operator, 328
defined, 57
energy, 121, 322‚Äì323
of Hermitian operator, 64‚Äì67
of momentum, 255‚Äì260
of operators, 80
of position, 252‚Äì254
of projection operator, 194
simultaneous, 131‚Äì133
of spin operator, 76, 77‚Äì80
Einstein, Albert, 155, 175, 223, 227
Electric current, 313

356 
INDEX
Electromagnetic radiation in cavity, 
342‚Äì345
Electromagnetic waves, 313
Electrons, 2, 149, 259, 301
spin of, 3‚Äì4, 116, 180, 290
wave packets and, 301
waves and, 235
Energy
composite operator and, 180‚Äì181
conservation of, 114‚Äì115
creation and annihilation 
operators and, 328‚Äì337
frequency and, 123
harmonic oscillator and, 314‚Äì316, 
317‚Äì319
of particle with negative 
momentum, 278
of photon, 345
See also Hamiltonian
Energy eigenvalues, 121, 322‚Äì323
Energy eigenvectors, 121, 322‚Äì323
Energy levels
eigenfunctions for, 341, 343
harmonic oscillators and, 322‚Äì323, 
336‚Äì337, 338
Entangled states, 165‚Äì167
Entanglement, 149‚Äì181
Bell‚Äôs Theorem and, 227‚Äì231
classical, 155‚Äì160
combining quantum systems, 
160‚Äì161
composite observables, 175‚Äì181
correlation test for, 213‚Äì214
density matrices and, 184, 
199‚Äì202, 210‚Äì212
density matrix test for, 214‚Äì218
entangled states, 165‚Äì167
example: calculating a density 
matrix, 210‚Äì212
locality and, 223‚Äì226
of near-singlet state, 234
observables and, 167‚Äì175
process of measurement and, 
218‚Äì223
of product state, 163‚Äì165, 232
of singlet state, 233
summary of, 231‚Äì234
tests for, 212‚Äì218
for two spins, 161‚Äì163, 202‚Äì210
Euler-Lagrange equations, 305n
Expectation values, 87‚Äì88, 91, 
105‚Äì108
change over time in, 109‚Äì114
conservation of, 115
correlation test for entanglement 
and, 213‚Äì214
for density matrix, 198
of entangled state, 172‚Äì175
of near-singlet state, 234
particle dynamics and, 278‚Äì279
of product state, 232
of projection operator, 195‚Äì196
of singlet state, 233
in spin over time, 116‚Äì119
Experiments
apparatus and, 5‚Äì13
invasiveness of, 12‚Äì13
probabilities for outcomes of (see 
Probabilities for experimental 
outcomes)
two-state system, 4‚Äì11
Feynman, Richard, 302, 309
Forces, 290‚Äì294
Fourier transforms, 260‚Äì261, 265, 
285
Frequency
energy and, 123
of harmonic oscillator, 344‚Äì345
Functions
Dirac delta, 241, 242‚Äì245, 253
Gaussian, 327
normalizable, 318
potential, 291, 297‚Äì298
probability, 105‚Äì106, 213, 295
as vectors, 238‚Äì245
vector space, 27‚Äì28
zero, 239
See also Continuous functions; 
Eigenfunctions; Wave functions
Fundamental theorem of quantum 
mechanics, 64

INDEX 
357
Gaussian curve, 301
Gaussian function, 327
Gaussian wave packets, 301
General Schr√∂dinger equation, 102, 
274
General uncertainty principle, 
146‚Äì148, 268, 269‚Äì270
Gluons, 259
Gram-Schmidt procedure, 67‚Äì69
Gravitons, 280
Ground states, 324‚Äì327
annihilation of, 336
wave functions for, 337‚Äì339
Hamiltonian, 99‚Äì102
canonical momentum and, 
319‚Äì320
conservation of, 115
entanglement and, 181
for harmonic oscillator, 318‚Äì320, 
321, 322‚Äì323, 324‚Äì326, 329‚Äì
334, 336
motion of particles and, 274‚Äì278
nonrelativistic free particles and, 
280‚Äì283
quantum, 101, 103
spin in magnetic field, 116‚Äì119
time evolution of system and, 274
Hamiltonian operator, Schr√∂dinger 
ket and, 124
Hamilton‚Äôs equations, 274, 279
Harmonic oscillator, 311‚Äì346
annihilation operators, 327‚Äì337
classical description, 314‚Äì316
creation operators, 327‚Äì337
energy levels, 322‚Äì323
ground state, 324‚Äì327
prevalence in physics, 311‚Äì313
quantization and, 342‚Äì346
quantum mechanical description, 
316‚Äì321
Schr√∂dinger equation, 321‚Äì322
wave functions, 337‚Äì342
Harmonic oscillator energy level 
ladder/tower, 337, 338
Heisenberg, Werner, 327
Heisenberg Uncertainty Principle, 
139‚Äì140, 148, 269‚Äì271
Hermite, Charles, 62
Hermite polynomials, 341
Hermitian
density matrices as, 207, 208
momentum as, 262
position as, 262
projection operators as, 194
Hermitian conjugation/conjugate, 
59‚Äì61, 62, 63, 65, 97‚Äì98, 100, 
332
Hermitian matrix, 62, 137‚Äì138, 
195n, 208
Hermitian observable, 262
Hermitian operators, 52, 101, 112, 
138, 255
action on state-vector, 107‚Äì108
in composite space of states, 168
eigenvector of, 139, 236, 262
expectation value of, 109
linear operators as, 70, 73‚Äì74, 
246‚Äì250
orthonormal bases and, 64‚Äì67
orthonormal edge vectors of, 136
overview, 61‚Äì63
particles and, 252
trace of, 196
Hilbert, David, 239
Hilbert spaces, 25, 239
Hooke‚Äôs law, 312
Hydrogen atom, 336‚Äì337
Identity, resolving, 261‚Äì264
Identity operator, from projection 
operators, 195
Inner products, 28‚Äì29, 30‚Äì32, 193
Integrals, replacing sums, 240, 241
Integration by parts, 245‚Äì246
Kets (ket vectors), 28‚Äì30
axioms of, 25‚Äì27
composite systems and, 153‚Äì154
inner product, 30‚Äì32
Schr√∂dinger, 124‚Äì126
Kinematics, 317

358 
INDEX
Kronecker delta, 205
replaced by Dirac delta functions, 
241, 242‚Äì245
Kronecker product, 188‚Äì192, 205n
Kronecker symbol, 98, 161
Lagrange equation, 314‚Äì316
Lagrangian, 302‚Äì303, 314‚Äì316, 318, 
319
Law of evolution, 5
Least action principle, 301‚Äì305
Linearity, 27, 53
Linear motion, 295‚Äì301
Linear operators, 52‚Äì69, 246‚Äì250
eigenvalues, 56‚Äì59
eigenvectors, 56‚Äì59
Gram-Schmidt procedure, 67‚Äì69
Hermitian conjugation, 59‚Äì61
Hermition operators, 61‚Äì63
Hermition operators, orthonormal 
bases and, 64‚Äì67
machines and matrices, 52‚Äì56
observables and, 69‚Äì70, 73
outer product as, 193‚Äì196
properties of, 53
time-development operator, 97
Liouville‚Äôs theorem, 274
Locality
defined, 223‚Äì224
Einstein vs. Bell and, 227
entanglement and, 223‚Äì226
Lowering operators (annihilation 
operators), 327‚Äì337
Machines, matrices and, 52‚Äì56
Magnetic field, spin in, 116‚Äì119
Mathematical concepts
complete sets of commuting 
variables, 129‚Äì136
complex numbers, 21‚Äì24
continuous functions, 236‚Äì250
functions as vectors, 238‚Äì245
integration by parts, 245‚Äì246
linear operators, 52‚Äì69, 246‚Äì250
outer products, 193‚Äì196
tensor products, 149‚Äì155
tensor products in component 
form, 184‚Äì192
vector spaces, 24‚Äì34
Matrices
4 ‚ãÖ 4, 188
machines and, 52‚Äì56
Pauli, 80, 118, 137
tensor product, building, 185‚Äì192
2 ‚ãÖ 2, 188 [is this entry out of 
lexical sequence?]
Matrix elements, 55
Matrix multiplication, 56, 59
Matrix notation, transposing in, 
60‚Äì61
Maximally entangled state, 217, 221
Maxwell‚Äôs equations, 290
Mean value, 105
Measurables, states that depend on 
more than one, 129‚Äì133
Measurement, 137‚Äì139
apparatus and, 5‚Äì11, 219‚Äì223
collapse of the wave function and, 
126‚Äì127
multiple, 129‚Äì133
operators and, 80‚Äì82
process of, 218‚Äì223
states and, 2‚Äì3
Minimum-uncertainty wave packets, 
301
Minus first law, 94, 274
quantum version of, 94‚Äì95, 97
Mixed states, 198, 199‚Äì200
composite system and, 200‚Äì201
density matrices and, 208‚Äì209
Momentum
canonical, 315, 318‚Äì320
connection between quantum and 
classical physics, 268
eigenfunctions and, 341
eigenvectors of, 255‚Äì260
forces and, 292‚Äì294
Heisenberg Uncertainty Principle 
and, 269
proposition for, 20‚Äì21
velocity and, 286‚Äì288, 293
wavelength and, 259‚Äì260

INDEX 
359
Momentum basis, 260‚Äì265
Momentum operator, 255‚Äì257
Momentum representation, of wave 
function, 260‚Äì265
Motion of particles. See Particle 
dynamics
Multiplication
of column vector, 28
of complex numbers, 23
matrix, 56, 59
vector, 26
Near-singlet state
correlation, 234
density matrix, 234
description of, 234
entanglement status of, 234
expectation values, 234
normalization, 234
state-vector, 234
wave function, 234
Negation, 14
Neutrino, 3
moving at speed of light,  
277‚Äì278
Newton‚Äôs law, 291, 292
quantum version of, 293‚Äì294
Nonlocality, 231
Nonrelativistic free particles, 
280‚Äì283
Normalizable functions, 318
Normalization
of near-singlet state, 234
of product state, 232
of singlet state, 233
Normalized vector, 32, 40
not rule, 14
Number operator, 332‚Äì333
Observables
complete set of commuting, 133
composite, 175‚Äì181
composite system, 167‚Äì175
defined, 52
linear operators and, 69‚Äì70, 73
multiple, 130‚Äì131
Observations, collapse of the wave 
function and, 126‚Äì127
Operator method
harmonic oscillator and, 328‚Äì337
wave functions and, 337‚Äì342
Operators
3-vector, 75, 83‚Äì85, 119
annihilation, 327‚Äì337
anti-Hermitian, 250
commutators and, 328, 334
composite, 171, 180‚Äì181
creation, 327‚Äì337
Hamiltonian, 124
Hermitian (see Hermitian 
operators)
identity, 195
linear (see Linear operators)
measurement and, 80‚Äì82
misconception regarding, 81‚Äì82
momentum, 255‚Äì257
number, 332‚Äì333
projection, 194‚Äì195
spin, 74‚Äì80
state-vectors and, 80‚Äì81
time-development, 95, 97‚Äì99
time-evolution, 99‚Äì102
unitary, 95, 97‚Äì99
zero, 133
Original Schr√∂dinger equation,  
274
nonrelativistic free particle and, 
281‚Äì283
or rule, 14, 15, 19
Orthogonal basis vectors, 48
Orthogonal states, 39‚Äì40, 97
Orthogonal state-vectors, 70, 72
Orthogonal vectors, 32, 64‚Äì67, 70
Orthonormal bases, 32‚Äì34
Gram-Schmidt procedure, 67‚Äì69
Hermitian operators and, 64‚Äì67
Outer products, 193‚Äì196
Overlap, 72, 73
Parameters, counting, 45‚Äì47
Partial derivatives, time and, 
320‚Äì321

360 
INDEX
Particle dynamics, 273‚Äì309
example, 273‚Äì279
forces, 290‚Äì294
linear motion and classical limit, 
295‚Äì301
nonrelativistic free particles, 
280‚Äì283
path integrals, 301‚Äì309
quantization, 288‚Äì290
time-independent Schr√∂dinger 
equation, 283‚Äì285
velocity and momentum, 286‚Äì288
Particle moving in three-dimensional 
space, measuring, 130
Particles, 235‚Äì236
coordinates of, 238
Heisenberg Uncertainty Principle 
and, 269‚Äì271
Hermitian operators and, 252
wave function and probability for 
finding position of, 260‚Äì265
Particles, state of, 250‚Äì260
eigenvalues and eigenvectors of 
position, 252‚Äì254
momentum and its eigenvectors, 
255‚Äì260
Particle-wave duality, 236
Path integrals, 301‚Äì309
Pauli matrices, 80, 118, 137
Phase ambiguity, 42
Phase-factors, 24, 42, 46, 108‚Äì109
Phase indifference, 47, 48‚Äì49
Photons, 260, 277, 280, 345‚Äì346
Planck‚Äôs constant, 102‚Äì104, 148, 
255, 337
Poisson brackets, 280
commutators and, 112‚Äì114, 265‚Äì268
Polarization vector, 91
Polar representation of complex 
number, 22‚Äì23
Position
eigenvalues and eigenvectors of, 
252‚Äì254
Heisenberg Uncertainty Principle 
and, 269
proposition for, 20‚Äì21
Position representation, of wave 
function, 260‚Äì262, 263‚Äì265
Potential functions, 291
spiky, 297‚Äì298
Precession, of spin in magnetic field, 
119
Principle of Least Action, 301‚Äì305
Principle of Stationary Action, 302n
Probabilities for experimental 
outcomes, 8, 19, 48‚Äì49, 70, 
72‚Äì73, 87‚Äì90, 238, 306
replaced by probability densities, 
241, 242
Schr√∂dinger ket and, 124‚Äì126
Probability
entanglement and, 206‚Äì207, 222
wave function and, 260‚Äì261, 264, 
270
Probability amplitudes, 39, 108‚Äì109
Probability density, 199, 317, 325
replacing probabilities, 241, 242
Probability distribution, 110, 112, 
213
in classical mechanics, 158‚Äì159
particle dynamics and, 278‚Äì279
uncertainty and, 140‚Äì141
Probability function, 105‚Äì106, 213, 
295
Product states, 163‚Äì165
correlation, 232
counting parameters for, 165
density matrix, 232
density matrix test for 
entanglement and, 215‚Äì218
description of, 232
entanglement status, 232
expectation values, 232
normalization, 232
state-vector, 232
wave function, 232
Projection operators, 194
properties of, 194‚Äì195
Propositions
classical, 13‚Äì16
classical, testing, 16‚Äì18
quantum, testing, 18‚Äì21

INDEX 
361
Pure states, 198, 199‚Äì200
composite system and, 200‚Äì201
density matrices and, 207‚Äì209, 
217
Quantization, 288‚Äì290
importance of, 342‚Äì346
Quantum abstractions, 2
Quantum electrodynamics, 290
Quantum field theory, 342
path integrals and, 309
Quantum Hamiltonian, 101, 103
Quantum mechanics
as calculus of probabilities, 36
classical mechanics vs., 2‚Äì3
conservation of energy and, 
114‚Äì115
focus of, 1‚Äì3
fundamental theorem of, 64
Planck‚Äôs constant and, 102‚Äì104
testing propositions of, 18‚Äì21
Quantum mechanics, principles of, 
69‚Äì74, 99
3-vector operators, 83‚Äì85
application, 85‚Äì90
operators, measurement and, 
80‚Äì82
spin operators, 74‚Äì75
spin operators, constructing, 
75‚Äì80
spin-polarization principle, 90‚Äì91
Quantum Sim, 227‚Äì231
Quantum spins, 3‚Äì4, 36‚Äì37, 227, 
229‚Äì230
Quantum states, 35‚Äì49
along the x axis, 41‚Äì42
along the y axis, 42‚Äì45
counting parameters, 45‚Äì47
incompleteness of, 36
representing spin states as column 
vectors, 47
spin states, 37‚Äì40
states and vectors, 35‚Äì37
Quantum systems, combining, 
160‚Äì161
Quantum tunneling, 341‚Äì342
Quarks, 3, 259, 311
Qubits, 3‚Äì4, 5, 311
measuring system of two, 130‚Äì131
Raising operators (creation 
operators), 327‚Äì337
Real numbers, quantum mechanics 
and, 61‚Äì63
Reversibility, 94
Row vectors, bras and, 29‚Äì30
Schr√∂dinger, Erwin, 327
Schr√∂dinger equations
generalized (see Time-dependent 
Schr√∂dinger equation)
original, 274, 281‚Äì283
path integrals and, 309
solving, 119‚Äì124
spin state evolution and,  
227‚Äì230
time-dependent (see Time-
dependent Schr√∂dinger 
equation)
for time derivatives, 110‚Äì112
time-independent, 120‚Äì121, 124, 
283‚Äì285, 286, 289
Schr√∂dinger ket, 124‚Äì126
Schr√∂dinger‚Äôs Ket, 102
Sets, Boolean logic and, 13‚Äì16
Simultaneous eigenvectors, 131‚Äì133
Singlet state, 166‚Äì167, 181
correlation, 233
density matrix, 233
description of, 233
entanglement status of, 233
expectation values, 233
normalization, 233
state-vector, 233
wave function, 233
Space of states, 4‚Äì5, 13, 16, 24, 25, 
37, 40, 44, 71, 94, 124, 150‚Äì151, 
160, 162, 165, 166, 167‚Äì168, 
216, 219, 238, 274, 289, 317
Speed of light, particles moving at, 
277‚Äì278
Spherical coordinates, 89‚Äì90

362 
INDEX
Spin
3-vector operators and, 83‚Äì85
along the x axis, 41‚Äì42
along the y axis, 42‚Äì45
density matrix for, 202‚Äì203
expectation values of, 87‚Äì88, 91
interaction with apparatus,  
5‚Äì13
in magnetic field, 116‚Äì119
number of distinct states for, 
45‚Äì47
quantum, 3‚Äì4, 36‚Äì37, 227, 
229‚Äì340
uncertainty principle and, 20
See also Qubits; Two spins
Spin components, simultaneous 
measurement of, 138‚Äì139
Spin operators, 74‚Äì75
constructing, 75‚Äì80
Spin-Polarization Principle, 90‚Äì91, 
172
Spin states
as column vectors, 47
representing, 37‚Äì40
Schr√∂dinger‚Äôs equation and 
evolution of, 227‚Äì230
Spring constant, 312
Standard deviation, 140, 141
State
of apparatus, 219‚Äì220
change over time, 94, 274
maximally entangled, 217, 221
measurement and, 2‚Äì3
mixed (see Mixed states)
near-singlet (see Near-singlet 
state)
of particles, 250‚Äì260
pure (see Pure states)
quantum (see Quantum states)
in quantum mechanics, 2
singlet (see Singlet state)
that depend on more than one 
measurable, 129‚Äì133
triplet, 166‚Äì167, 179, 181
unambiguously distinct, 70, 72
State-labels, for composite system, 
152, 153, 154, 160‚Äì161
State of system, in classical vs. 
quantum physics, 21, 273‚Äì274
State space, Boolean logic and, 
13‚Äì16
State-vectors, 70
action of Hermitian operator on, 
107‚Äì108
as complete description of system, 
175
evolution of with time, 99
of near-singlet state, 234
operators and, 80‚Äì81
phase-factor and, 108‚Äì109
physical properties of, 46
of product state, 163‚Äì165, 232
representing spin states using, 
37‚Äì40
of singlet state, 233
time derivative of, 102
time evolution of, 95‚Äì96
wave functions and, 136
See also Bras (bra vectors); Kets 
(ket vectors); Singlet state; 
Triplet states
Statistical correlation, 158
Subset, 13, 14, 15‚Äì16
Sums, integrals replacing, 240
Symmetric eigenfunctions, 340‚Äì341
Systems
number of parameters 
characterizing, 45‚Äì47
quantum, combining, 160‚Äì161
See also Composite systems; Two-
spin system
Tensor products, 149‚Äì155, 165, 176
Tensor products in composite form, 
184‚Äì192
building tensor product matrices 
from basic principles, 185‚Äì187
building tensor product matrices 
from component matrices, 
188‚Äì192

INDEX 
363
Tests for entanglement, 212‚Äì218
Time
change in expectation values over, 
109‚Äì114
conservation of distinctions and, 
97‚Äì99
determinism and, 96
partial derivatives and, 320‚Äì321
time-evolution operator, 99‚Äì102
unitarity, 95, 98‚Äì99
See also Schr√∂dinger equations
Time dependence, 116, 125, 286, 
322. See also Uncertainty
Time-dependent Schr√∂dinger 
equation, 102
harmonic oscillation and, 321‚Äì323
particle dynamics and, 274, 
275‚Äì276, 289, 302
solving, 120, 121‚Äì124
state of system and, 126
Time derivatives, 102
Schr√∂dinger equation for, 110‚Äì112
Time-development operator, 95
conservation of distinctions and, 
97‚Äì99
Time evolution, 274
determinism and, 96
entanglement and, 181
unitary operators and, 98‚Äì99
Time-evolution operator, 99‚Äì102
Time-independent Schr√∂dinger 
equation, 120‚Äì121, 124
particle dynamics and, 283‚Äì285, 
286, 289
Trace
of density matrix, 206, 207, 209
of projection operator, 195, 196
properties of, 209
Trajectories, path integrals, 301‚Äì309
Transposing, 60‚Äì61
Triangle inequality, 142‚Äì146
Triplet states, 166‚Äì167, 179, 181
Truth-value, 13‚Äì14
Two spins, 161‚Äì181
entanglement for, 202‚Äì210
Two-spin system
Bell‚Äôs theorem and, 230‚Äì231
density matrix of, 202‚Äì212, 
214‚Äì218, 226, 231
Two-state system, experiment on, 
4‚Äì11
Uncertainty
Cauchy-Schwarz inequality, 142
defined, 140‚Äì141
triangle inequality and Cauchy-
Schwarz inequality, 142‚Äì146
Uncertainty principle, 20, 139‚Äì140, 
146‚Äì148
Heisenberg, 139‚Äì140, 148
Unitarity, 95, 98‚Äì99, 100
Unitary evolution, 218, 222, 225
Unitary matrix, 225
Unitary operators, 95, 97‚Äì99
Unitary time evolution, 181
Unit matrix, 137
density matrix and, 217
Unit (normalized) vector, 32
state of system and, 40
Unit operator, as observable, 138
Up states, 71, 87‚Äì88, 219‚Äì220, 
221‚Äì222
Vector addition, 26
Vectors
basis (see Basis vectors)
column, 27‚Äì28, 29, 47, 49
concept of, 24‚Äì25
functions as, 238‚Äì245
normalized, 32, 40
orthogonal, 32, 64‚Äì67, 70
polarization, 91
quantum states and, 35‚Äì37
row, 29‚Äì30
three-(3-vector), 25, 27, 32‚Äì33, 
74‚Äì75, 83
unit, 32, 40
See also Bras (bra vectors); 
Eigenvectors; Kets (ket 
vectors)

364 
INDEX
Vector space, 24‚Äì34
axioms, 24‚Äì27
bras, 28‚Äì30
column vectors, 27‚Äì28
functions and, 27‚Äì28, 239‚Äì240
inner products, 30‚Äì32
kets, 28‚Äì30
orthonormal bases, 32‚Äì34
tensor product as, 165
triangle inequality and, 142‚Äì146
Velocity
momentum and, 286‚Äì288, 293
of quantum mechanical particle, 
286‚Äì288
Venn diagram, 14, 16
Wave functions, 134‚Äì135, 236‚Äì238
action of Hamiltonian on, 320‚Äì321
calculating density matrices and, 
206‚Äì207
collapse of, 126‚Äì127
entanglement and, 212‚Äì213
ground-state, 324‚Äì327
locality and, 225‚Äì226
measurement and collapsing, 218, 
222‚Äì223
momentum and, 255‚Äì259
momentum representation, 
260‚Äì265
of near-singlet state, 234
operator method and, 337‚Äì342
position representation, 254, 
260‚Äì262, 263‚Äì265
of product state, 232
representing particles, 253‚Äì254
of singlet state, 233
state-vectors and, 136
Wavelength, momentum and, 
259‚Äì260
Wave packets, 295‚Äì301
bimodal, 296‚Äì297
Gaussian, 301
harmonic oscillation and, 322
minimum-uncertainty, 301
moving at fixed speed, 276‚Äì277
for nonrelativistic free particle, 
283
Waves, 235‚Äì236
harmonic oscillator and, 313
Wheeler, John, 52
x axis, spins along, 41‚Äì42
y axis, spins along, 42‚Äì45
Zaxon, 278
Zero function, 239
Zero operator, 133

