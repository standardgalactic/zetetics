Probabilistic solution of diﬀerential equations for Bayesian
uncertainty quantiﬁcation and inference
by
Oksana A. Chkrebtii
M.Sc., Carleton University, 2008
B.Math. (Hons.), Carleton University, 2006
a Thesis submitted in partial fulfillment
of the requirements for the degree of
Doctor of Philosophy
in the
Department of Statistics & Actuarial Science
Faculty of Science
c⃝Oksana A. Chkrebtii 2013
SIMON FRASER UNIVERSITY
Fall 2013
All rights reserved.
However, in accordance with the Copyright Act of Canada, this work may be
reproduced without authorization under the conditions for “Fair Dealing.”
Therefore, limited reproduction of this work for the purposes of private study,
research, criticism, review and news reporting is likely to be in accordance
with the law, particularly if cited appropriately.

APPROVAL
Name:
Oksana A. Chkrebtii
Degree:
Doctor of Philosophy
Title of Thesis:
Probabilistic solution of diﬀerential equations for Bayesian
uncertainty quantiﬁcation and inference
Examining Committee:
Dr. Tim Swartz, Professor
Chair
Dr. David Campbell, Associate Professor
Senior Supervisor
Dr. Derek Bingham, Professor
Supervisor
Dr. Richard Lockhart, Professor
Internal Examiner
Dr. Michael Dowd, Associate Professor
External Examiner, Dalhousie University
Date Approved:
December 9th, 2013
ii

Partial Copyright Licence 
 
 
 
 
 
iii

Abstract
In many areas of applied science the time and space evolution of variables can be naturally
described by diﬀerential equation models, which deﬁne states implicitly as functions of
their own rates of change. Inference for diﬀerential equation models requires an explicit
representation of the states (the solution), which is typically not known in closed form,
but can be approximated by a variety of discretization-based numerical methods. However,
numerical error analysis is not well-suited for describing functional discretization error in a
way that can be propagated through the inverse problem, and is consequently ignored in
practice. Because its impact can be substantial, characterizing the eﬀect of discretization
uncertainty propagation on inference has been an important open problem.
We develop a probability model for the systematic uncertainty introduced by a ﬁnite-
dimensional representation of the inﬁnite-dimensional solution of ordinary and partial dif-
ferential equation problems. The result is a probability distribution over the space of pos-
sible state trajectories, describing our belief about the unknown solution given information
generated from the model over a discrete grid. Our probabilistic approach provides a use-
ful alternative to deterministic numerical integration techniques in cases when models are
chaotic, ill-conditioned, or contain unmodelled functional variability. Based on these re-
sults, we develop a fully probabilistic Bayesian approach for the statistical inverse problem
of inference and prediction for intractable diﬀerential equation models from data, which
characterizes and propagates discretization uncertainty in the estimation. Our approach is
demonstrated on a number of challenging forward and inverse problems.
iv

To my family and friends, near and far.
v

“But I have swam through libraries and sailed through oceans...”
—Herman Melville, Moby-Dick, 1851
vi

Acknowledgments
I would like to thank all the people who made this work possible and made my years at
Simon Fraser University so wonderful. Many thanks to collaborators Mark Girolami and Ben
Calderhead, and also to all the researchers who oﬀered valuable comments and suggestions
to the unbelievably many iterations of our paper. Also thank you to Erin Cameron for taking
my mind away from diﬀerential equations with research on the introduction and spread of
invasive earthworms. I am grateful to the Bamﬁeld Marine Sciences Centre for oﬀering
the course Models in Ecology with the help of Dr. Mark Lewis, Dr. Marty Krkosek, and
Stephanie Peacock, where I met some really great people. I would like to thank very much
the Paciﬁc Institute for Mathematical Sciences International Graduate Training Centre in
Mathematical Biology for opportunities to connect with many researchers and for organizing
some very fun summits in Naramata and Banﬀ. And ﬁnally, I gratefully acknowledge the
Natural Sciences and Engineering Research Council of Canada whose funding allowed me
to spend most of my time learning, asking questions, and exploring my interests.
To all the staﬀand faculty at the Department of Statistics & Actuarial Science at
Simon Fraser University, I want to say thank you for providing the best atmosphere to grow
both personally and professionally. I am also grateful to all the people at the School of
Mathematics and Statistics at Carleton University, and especially to Dr. Natalia Stepanova
for the invaluable guidance and supervision during my undergraduate and M.Sc. studies,
and for providing a strong foundation for my research. The oﬃce(s) would not have been the
same without many amazing oﬃcemates. A special mention for making my PhD studies so
much fun go to Ryan Lekivetz, Shirin Golchi, Audrey B´eliveau, Ruth Joy, Andrew Henrey,
Jinny Lim, Zheng Sun, Ararat Harutyunyan, and Krystal Guo, who shared not just the
oﬃces, but a passion for unreasonable running on Burnaby Mountain, nights out at the
Orpheum, dinner parties, pub nights, hiking adventures, and trips all around Canada. There
vii

are also a group of people without whom this experience would not have been complete:
they are my friends, who have become my second family and have stuck with me throughout
my years away from home. Thank you to Katrina Rogers-Stewart, Matt Lemire, Phil Munz,
Marjolaine Jerry, and Dale Powell, for your friendship.
Most importantly, I want to thank Dave Campbell for being such a great supervisor: for
teaching me to understand the subtleties of Bayesian statistics and for being patient when
I didn’t or when I managed to crash his oﬃce computer from a remote research station on
Vancouver Island (long story, also involving invasive earthworms); for sending me on many
very interesting conferences; and for a summer of research in London, Eindhoven, and Paris,
which I will never forget.
To my family: Mom, Dad, Anna, Fran¸cois, I want to say how much your love and
support has meant to me throughout these years. Thank you for listening to my whining
when my research wasn’t going as it should, for the wonderful vacations, for the visits and
parties, and by considering my need to run code remotely in choosing resorts with Wi-Fi
connection on the beach.
viii

Contents
Approval
ii
Partial Copyright License
iii
Abstract
iv
Dedication
v
Quotation
vi
Acknowledgments
vii
Contents
ix
List of Figures
xii
List of Symbols and Notation
xv
Preface
xviii
1
Introduction
1
1.0.1
Inference for diﬀerential equation models
. . . . . . . . . . . . . . . .
2
1.1
Numerical methods and the role of probability
. . . . . . . . . . . . . . . . .
3
1.1.1
History of probability in numerical analysis . . . . . . . . . . . . . . .
3
1.2
Contribution and organization of this thesis . . . . . . . . . . . . . . . . . . .
4
2
Diﬀerential Equation Models
6
2.1
Ordinary diﬀerential equation models
. . . . . . . . . . . . . . . . . . . . . .
7
ix

2.1.1
Initial value problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1.2
Multi-point boundary value problems
. . . . . . . . . . . . . . . . . .
13
2.1.3
Delay initial function problems . . . . . . . . . . . . . . . . . . . . . .
16
2.2
Partial diﬀerential equation models . . . . . . . . . . . . . . . . . . . . . . . .
18
2.2.1
Initial boundary value problems
. . . . . . . . . . . . . . . . . . . . .
18
3
Probability Model for the Unknown Solution
24
3.1
Review of selected measure theoretic concepts . . . . . . . . . . . . . . . . . .
25
3.1.1
Radon measures on Hilbert spaces . . . . . . . . . . . . . . . . . . . .
25
3.1.2
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.1.3
The Gaussian measure . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.2
Model for the probabilistic solution and its derivative . . . . . . . . . . . . . .
29
3.2.1
Univariate solution model . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2.2
Multivariate solutions . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.3
The probabilistic solution is well-deﬁned . . . . . . . . . . . . . . . . . . . . .
34
4
Probabilistic Solution of Diﬀerential Equations
37
4.1
Solving ODE initial value problems . . . . . . . . . . . . . . . . . . . . . . . .
38
4.1.1
Generating derivative realizations from the model . . . . . . . . . . . .
38
4.1.2
Relationship with numerical solvers
. . . . . . . . . . . . . . . . . . .
41
4.1.3
Recursive formulation of probabilistic solution
. . . . . . . . . . . . .
42
4.1.4
Posterior consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
4.2
Solving ODE Boundary value problems
. . . . . . . . . . . . . . . . . . . . .
48
4.3
Solving ODE delay initial function problems . . . . . . . . . . . . . . . . . . .
53
4.4
Solving PDE boundary value problems . . . . . . . . . . . . . . . . . . . . . .
57
4.4.1
Indirect probabilistic solution method . . . . . . . . . . . . . . . . . .
58
4.4.2
Direct probabilistic solution method . . . . . . . . . . . . . . . . . . .
62
5
Model Uncertainty in the Inverse Problem
63
5.1
Statistical inverse problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
5.1.1
Inference for diﬀerential equation models
. . . . . . . . . . . . . . . .
64
5.1.2
Bayesian approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
5.1.3
Approximate inference under an unknown DE solution . . . . . . . . .
66
5.2
A fully probabilistic approach . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
x

5.2.1
Exact posterior density
. . . . . . . . . . . . . . . . . . . . . . . . . .
67
5.3
Sampling from the posterior distribution over model parameters
. . . . . . .
67
5.3.1
Inference for the JAK-STAT protein network model
. . . . . . . . . .
69
6
Choice of Covariance Structure
72
6.1
Role and properties of prior covariances . . . . . . . . . . . . . . . . . . . . .
72
6.1.1
Positive-deﬁniteness
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
6.1.2
Regularity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
6.1.3
Role of kernel functions . . . . . . . . . . . . . . . . . . . . . . . . . .
74
6.2
Some useful covariances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
6.2.1
Squared exponential covariance . . . . . . . . . . . . . . . . . . . . . .
75
6.2.2
Uniform covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
6.2.3
Diagonal boundary covariance . . . . . . . . . . . . . . . . . . . . . . .
78
7
Probabilistic Mesh Selection
82
7.1
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
7.2
Numerical step length selection . . . . . . . . . . . . . . . . . . . . . . . . . .
83
7.2.1
Kullback-Leibler divergence criterion . . . . . . . . . . . . . . . . . . .
83
7.3
Probabilistic sequential step length selection . . . . . . . . . . . . . . . . . . .
84
7.3.1
KL divergence between current and step-ahead estimated solution
. .
84
7.3.2
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
8
Conclusion
88
8.1
Impact and Recommendations
. . . . . . . . . . . . . . . . . . . . . . . . . .
88
Bibliography
90
xi

List of Figures
2.1
Two numerical solutions to the Lorenz IVP (2.4), for each state in the system (top, mid-
dle, bottom rows), computed under almost identical initial conditions. The trajectories
correspond to the initial function (−10, −5, 36) (red), and the initial function perturbed by
adding 10−3 (blue). Numerical solutions were obtained via the ode45 (MATLAB) numerical
solver with an error tolerance of 10−4.
. . . . . . . . . . . . . . . . . . . . . . . . .
13
2.2
Numerical solutions to the Lane-Emden MBVP with parameters θ = (2, 1) and boundary
conditions
(
ub, va
)
=
( √
3
2 , −288
2197
)
. The bvp4c (MATLAB) numerical solver was used with
two diﬀerent starting points: ua ∈{1, 2}. Two distinct solutions were obtained (red, blue).
16
2.3
Top: analytical (black) and numerical (red) solution to the DIFP (2.9) with fully-speciﬁed
initial function ϕ(t) = 1, t ∈[−1, 0]. Bottom: analytical (black) solution to the DIFP
(2.9) with initial function ϕ(t) = 1 + sin(4πt)
4
, t ∈[−1, 0]. The numerical solution (red) is
obtained for the same system with only a partially speciﬁed initial function, given without
error at a set of six knots (circled in red).
. . . . . . . . . . . . . . . . . . . . . . .
19
2.4
Top view of a numerical solution of the Kuramoto-Sivashinsky PDE boundary value problem
on the domain D = [0, 32π] × [0, 150]. The numerical solution on the left was obtained
under the initial function uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
, while the numerical solution on
the right was obtained under the initial function uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
+ 10−2.
.
22
4.1
Directed acyclic graph diagram for Algorithm 1, producing a sample from the probability
density p
(
uN(t, θ), f1:N
θ, ua, ΨN
)
. The grey nodes represent values that are returned by
the algorithm, and all others are discarded. . . . . . . . . . . . . . . . . . . . . . . .
39
4.2
Sample of 100 realizations from the probabilistic solution for the Lorenz system under a
ﬁxed initial state.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
xii

4.3
Sample of 100 probabilistic solution realizations for states u(·, θ) (left, above) and v(·, θ)
(left, below); the estimated marginal unnormalized log-density of the unknown initial state
ua (right).
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
4.4
Mean probabilistic solution (blue dashed line) constructed using 500 solver knots compared
with numerical (red dashed line) and analytical (black solid line) solutions for the system
(2.9) with initial function ϕ(t) = 1, t ∈[−1, 0] fully speciﬁed (top), and ϕ(t) = sin(4πt)/4+1
(below) fully speciﬁed (black) and estimated (dotted red line) from six nodal points (red
circles). The grey bands show ± 100 standard deviations around the mean probabilistic
solution for exposition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
4.5
A sample of 100 realizations of the probabilistic solution of the Kuramoto-Sivashinsky
PDE using a ﬁxed initial function. The spatial and temporal dimensions are shown on
the horizontal and vertical axes respectively. The solution is known to exhibit temporal
chaos, as evidenced by the variety of dynamics observed due to uncertainty introduced in
its estimation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
4.6
Vorticities for a sample of 6 realizations of the probabilistic solution of the Navier-Stokes
equation on a two-dimensional torus at time t = 30 units, under a ﬁxed initial ﬁeld. Vorticity
of the 6 realizations are very similar at this stage. . . . . . . . . . . . . . . . . . . . .
60
4.7
Vorticities for a sample of 6 realizations of the probabilistic solution of the Navier-Stokes
equation on a two-dimensional torus at time t = 100 units, under a ﬁxed initial ﬁeld. Vortic-
ity of the 6 realizations have begun to diverge from one another as a result of discretization
error accumulation along the temporal domain. . . . . . . . . . . . . . . . . . . . . .
61
5.1
Experimental data (red circles) and sample paths (lines) of the observation processes.
They are obtained by transforming a sample from the marginal posterior distribution of
the states by the observation function (5.6). . . . . . . . . . . . . . . . . . . . . . . .
70
5.2
Marginal fully probabilistic posterior distribution in the model parameters based on a
sample of size 100,000 generated by a parallel tempering algorithm utilizing seven chains,
with the ﬁrst 10,000 samples removed. Prior densities are shown in red.
. . . . . . . . .
71
6.1
Top row: contour plots of Gaussian kernel (left), and its integrated version (right). Bottom
row: contour plots of squared exponential derivative covariance (left) and associated state
covariance (right). Functions are evaluated over t, ˜t ∈[0, 1], with (α, λ) = (1, 0.1). . . . . .
77
xiii

6.2
Top row: contour plots of uniform kernel (left), and its integrated version (right). Bottom
row: contour plots of derivative covariance (left) and state covariance (right). Functions
are evaluated over t, ˜t ∈[0, 1], with (α, λ) = (1, 0.1). . . . . . . . . . . . . . . . . . . .
79
6.3
Top row: contour plots of diagonal kernel (left), and its integrated version (right). Bottom
row: contour plots of derivative covariance (left) and state covariance (right). Functions
are evaluated over t, ˜t ∈[0, 1], with (α, λ) = (1, 0.1). . . . . . . . . . . . . . . . . . . .
81
7.1
Realizations of the states (top, solid lines) and derivatives (middle, solid lines) of a single
draw from the probabilistic solution of the Lorenz system on the interval [0, 5]. State and
derivative realizations obtained at each step are shown as dots. The mesh was selected
adaptively, and the resulting step lengths are shown in the lower panel in dark blue. Light
blue dotted lines represent the possible step lengths {gh∗}g=1 ...,4.
. . . . . . . . . . . .
87
xiv

List of Symbols and Notation
Spaces
(Ω, A, µ)
probability triple with sample space Ω, σ-algebra A, and
probability measure µ
H
Hilbert space
H∗
dual space of linear functionals of H
L2(
D
)
space of square integrable functions on the domain D
L2(
D; (Ω, A, µ)
)
space of square integrable random functions deﬁned on the
domain D and the probability space (Ω, A, µ)
Cm(
D
)
space of m-times continuously diﬀerentiable functions on
the domain D
Mr,c(
R
)
r × c matrix space over the real numbers
Mn(
R
)
n × n matrix space over the real numbers
xv

Norms, Vector Products
⟨·, ·⟩
vector product
|| · ||
vector norm
| · |
Euclidian norm
Operators
⊕
direct sum
⊗
tensor product
◦
composition of functions
˙u
derivative of u with respect to t
Dα
diﬀerential operator with multi-index |α| = α0, . . . , αq
∆
Laplacian operator
∇
gradient operator
(∇·)
divergence operator
xvi

Other symbols
B(Ω)
Borel σ-algebra of subsets of Ω
P(S)
power set of the set S
f = o (g)
“small oh” notation, f(x)/g(x) →0 as x →a
f = O (g)
“big oh” notation, ∃C > 0 : |f(x)/g(x)| < C as x →a
δu(·)
Dirac delta function centered at u
⌈q⌉
ceiling function: one plus the smallest integer part of q
N(m, C)
Gaussian measure on an inﬁnite-dimensional space with
mean function m and covariance operator C
Nn(m, Λ)
Gaussian measure on an n-dimensional space with mean
vector m and covariance matrix Λ
xvii

Preface
Much of the work presented in this thesis has been summarized in the manuscript Bayesian
Uncertainty Quantiﬁcation for Diﬀerential Equations, jointly written with David A. Camp-
bell, Mark A. Girolami, and Ben Calderhead.
The preprint is available from arXiv at
http://arxiv.org/abs/1306.2365. This thesis also contains a variety of unpublished results,
extensions, and implementation details that should be of interest to practitioners or those
wishing to study this methodology in greater detail.
Complete code for replicating all results presented in this thesis can be downloaded from
http://people.stat.sfu.ca/∼ochkrebt/PODES.html.
xviii

Chapter 1
Introduction
Variables of interest in many physical, chemical, and biological systems evolve over time and
space in a way that is intrinsically linked to their rates of change. Consider, for example, the
concentration of a reagent, such as a toxic compound in the liver, metabolized by reacting
with enzymes into a product. On a molecular level, the rate at which the toxic substance
and the enzymes come into contact depends on their relative concentrations. That is, the
instantaneous rate of change of the concentration is a function of the current concentration
in the system. Another example might be the velocity of an object, such as a meteorite or
re-entering spacecraft, falling in the atmosphere, whose velocity over time is related to its
rate of change, or acceleration.
Diﬀerential equations (DEs) provide a convenient modelling framework for describing
the natural dependence of variables on their derivatives. Such models are widely used in the
physical, biological, and social sciences. Frequently, quantities parameterizing DE models
are unknown and interest lies in estimating them from measurement data.
Statistical inference for diﬀerential equations is made challenging by the fact that the
variables of interest, or system states, are implicitly deﬁned in terms of their derivatives,
while the data is typically only directly measured on the states. In our metabolism example,
one can only measure concentrations of each compound, and not their rates of change.
Similarly, while acceleration in our physics example can be computed indirectly from the
velocity, we can directly only measure the displacement of a falling object.
Therefore, statistical inference about a DE model requires obtaining its equivalent rep-
resentation in terms of the states alone, also known as the solution of the DE. However,
1

CHAPTER 1. INTRODUCTION
2
solutions in closed form are typically only available for relatively simple systems, requir-
ing modellers to frequently rely on numerical approximations. Pointwise error bounds on
numerical solutions are not well-suited for a probabilistic analysis so that, in practice, dis-
cretization error is ignored for the purposes of inference.
1.0.1
Inference for diﬀerential equation models
Consider the problem of inferring unknown parameters θ ∈Θ, deﬁning a nonlinear DE
model with unknown closed-form solution u(t, θ), t ∈D, from discretely observed data
y(t), t ∈DT . The likelihood function Ly(t)
(
u(t, θ)
)
provides a measure of distance between
the data and the model under a given parameter value. As this likelihood depends on the
unknown DE solution, the conventional inference approach substitutes the solution with its
N-dimensional numerical approximation uN(t, θ). Inference then proceeds based on the
approximation,
Ly(t)
(
u(t, θ)
)
≈Ly(t)
(
uN(t, θ)
)
.
As will be illustrated in this thesis, there are many cases in which the mismatch between the
true DE solution and its discrete approximation uN(t, θ) has a well-deﬁned, non-negligible
structure, leading to serious inferential bias under the conventional inferential approach.
However, this approach remains popular for a number of possible reasons.
1. A satisfactory probability model of discretization uncertainty has been unavailable.
Numerical error analysis is a well-established ﬁeld studying the accuracy of numer-
ical techniques (see, for example, Butcher, 2008). However, because error estimates
are typically computed pointwise, they are not well-suited for quantifying functional
uncertainty in the solution. Moreover, it is unclear how classical numerical error anal-
ysis can be considered and propagated forward through the inference methodology to
characterize model uncertainty.
2. Incorrect interpretation of a probabilistic uncertainty model. A probabilistic descrip-
tion of the discretization unvertainty for an unknown DE solution represents our
knowledge about the states given a ﬁnite-dimensional discretization grid by a distri-
bution over the space of possible trajectories. This can incorrectly be interpreted as a
stochastic model for the deterministic solution itself, which would then be inconsistent

CHAPTER 1. INTRODUCTION
3
with the theory of deterministic diﬀerential equations.
3. Convenience. Many statistical procedures rely on approximations. When the approx-
imation error is believed to be negligible and not systematic, it may be diﬃcult to
justify adding an additional layer of uncertainty to an already challenging analysis.
The present work develops a probabilistic formalism for describing model uncertainty
resulting from discretization of an unknown DE solution, and naturally incorporates the
associated functional variability into a fully probabilistic inferential framework. We show
that such a probability model can characterize solution uncertainty in a consistent way,
and allows us to distinguish discretization uncertainty from other sources of variability. We
develop eﬃcient Monte Carlo methods to generate approximate samples from posterior dis-
tributions deﬁned by introducing the additional layer of uncertainty associated with model
error in the solution.
1.1
Numerical methods and the role of probability
Solving diﬀerential equations, either exactly or approximately, is a critical tool for modelling
in the natural, applied, and social sciences. Despite the availability of pointwise error bounds
for many numerical methods, these are typically assumed negligible for convenience when
doing statistical inference. This approach is not limited to diﬀerential equation models.
Indeed, numerical approximations are commonly used in statistical procedures (see, for
example, Lange, 1999), and the associated numerical error is frequently ignored. But should
such approximations be considered from a statistical perspective, or should they remain
ﬁrmly in the area of numerical analysis?
1.1.1
History of probability in numerical analysis
The earliest explicit use of probability to address a numerical problem, pointed out in Di-
aconis (1988), was due to Poincar´e (1896). Poincar´e considered the problem of polynomial
interpolation of a ﬁnite number of function evaluations g = [g(t1), . . . , g(tN)] based on a
power series expansion. Uncertainty about the coeﬃcients of the expansion was modelled
through zero-mean Gaussian priors, updated given the function evaluations g. Uncertainty
in the coeﬃcients thus induces a distribution over the space of polynomials. This approach

CHAPTER 1. INTRODUCTION
4
predated Gaussian process (GP) regression, but provided an equivalent estimate of the pos-
terior distribution under a speciﬁc covariance speciﬁcation (e.g., Rasmussen and Williams,
2006).
With advances in computational power and sampling methodology, the Bayesian ap-
proach to function estimation became more generally feasible. This led a number of re-
searchers to consider problems of numerical analysis from the point of view of probability
theory. O’Hagan (1992) proposed a Gaussian process based approach to contour estimation
and function interpolation, and consequently quadrature in low-dimensions exploiting the
integrability of GPs. In the last several decades, the Markov chain Monte Carlo (MCMC)
approach (Hastings, 1970; Metropolis and Ulam, 1949) has become a standard way to es-
timate integrals, even in high-dimensional problems when numerical quadrature alone is
infeasible. This feature is very useful because many problems reduce to complex integra-
tion. For example, Doucet et al. (2010) show how to use two types of Monte Carlo methods
to approximate the solution to a class of integral equations. Fredholm equations of the
second kind are restated as high-dimensional integrals via their Von Neumann representa-
tion and estimated from a sample obtained via sequential or reversible-jump Monte Carlo
sampling.
Models deﬁned by diﬀerential equations are very ﬂexible and simple to formulate but
often diﬃcult to solve. The challenge, as will be illustrated in this thesis, is that the implicit
dependence between states and their derivatives is typically nonlinear so that the solution
does not reduce to a simple quadrature problem. A very interesting, although not well-
known suggestion in Skilling (1991) considered the question of solving ordinary diﬀerential
equation initial value problems by modelling solutions and their derivatives as convolutions
of an underlying latent process that could then be integrated or diﬀerentiated as required.
The diﬀerence with the problem studied by O’Hagan (1992) is that function evaluations
must be made sequentially from the updated model because of the implicit dependence of
the states on their derivatives.
1.2
Contribution and organization of this thesis
This thesis develops the ideas proposed by Skilling (1991) into a fully usable and widely
applicable framework for solving general classes of analytically intractable ordinary and
partial diﬀerential equation problems. We formalize the model and show that these ideas

CHAPTER 1. INTRODUCTION
5
can be made computationally feasible, proposing sequential sampling methodologies that
capture fast-changing, or even chaotic, dynamics. For these methods, convergence results
are obtained under relatively mild conditions. Additionally, we investigate methodologies to
make probabilistic solutions more eﬃcient, through the development of recursive algorithm
formulations and statistical step size selection techniques.
Using this probabilistic perspective on the forward problem, we then incorporate the
model uncertainty into a fully probabilistic inferential framework. We design Monte Carlo
algorithms capable of generating samples from posterior distributions for unknown model
parameters and state functions, taking into account the additional hierarchical layer intro-
duced by model uncertainty. Our contribution allows us to characterize and separate model
uncertainty from other sources of error.
The thesis is organized as follows. Chapter 2 provides a basic review of ordinary and par-
tial diﬀerential equations (ODEs and PDEs), deﬁning a number of ODE and PDE problems
that often arise in practical applications. We also discuss the types of numerical techniques
available to provide approximate solutions to these problems, and point out cases where
they fail to reasonably approximate the solution due to model error. Chapter 3 formalizes
the Bayesian uncertainty model for unknown univariate ODE solutions, and further extends
this framework to multivariate PDE solutions. The stochastic process model for the un-
certainty in the system states, which we call the probabilistic solution, given a number of
model-based derivative evaluations is shown to have a well-deﬁned density with respect to
the prior measure under some conditions. Then, Chapter 4 develops sequential sampling
strategies that allow us to generate derivative evaluations in a self-consistent model-based
way to build up a picture of the implicitly deﬁned DE solution. Furthermore, this chap-
ter provides techniques and algorithms for sampling functional realizations of the proba-
bilistic solution. Next, Chapter 5 considers the statistical inverse problem of estimating
model parameters and states from measurement data. We develop a Monte Carlo sampling
methodology to obtain realizations of the posterior distribution of the states and the un-
known solution. Chapter 6 describes three important kernel functions, two of which were
speciﬁcally designed to model DE solutions with certain properties. We also compute ana-
lytically the pairwise convolutions between each kernel and its integrated version, required
for the computational implementation of our probabilistic algorithm. Finally, Chapter 7
discusses adaptive sequential design for discretizing the domain of integration by optimizing
an information theoretic criterion.

Chapter 2
Background on Diﬀerential
Equation Models and Their
Solutions
Diﬀerential equations are a class of models that describe the relationship, common in nature,
between system states and their rates of change with respect to one or several, usually spatio-
temporal, variables. In addition to describing the evolution of state variables over time
or space, diﬀerential equation models also provide some basic constraints on the states,
to ensure that a ﬁnite number of solutions can exist, and to incorporate any additional
information into the model that is known about a particular system.
The versatility of diﬀerential equation models, and the variety of dynamics they describe,
is best reviewed by examining a number of model types. Therefore, this chapter brieﬂy
introduces some commonly used ordinary and partial diﬀerential equation (ODE and PDE)
problems, and provides examples of their use in modelling real-world phenomena.
The ﬂexibility and intuitive appeal of diﬀerential equation models usually comes at a
price, stemming from the diﬃculty in obtaining an explicit representation of the states, or
solution, that is independent of the derivatives. This chapter gives a broad description of
numerical techniques for obtaining approximate solutions given model parameters for each
of the ODE and PDE problems presented. We also point out situations in which numerical
approximations may give a misleading representation of the true solution.
6

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
7
Throughout this thesis we use examples of diﬀerential equation problems which illus-
trate the systematic nature of discretization error, and showcase the probabilistic posterior
solution developed in this work. These models and their interpretation, as well as relevant
references, are provided in this section.
We shall refer to the task of solving these sys-
tems, given any parameters or inputs, as the forward problem, which will then be applied
in Chapter 5 to the inverse problem of inferring unknown parameters from data.
2.1
Ordinary diﬀerential equation models
Ordinary diﬀerential equations (ODEs) describe the relationship between the states (or
dependent variables), u( · , θ) : [a, b] →RP , and their derivatives, ˙u( · , θ) : [a, b] →RP , with
respect to a single indexing variable (or independent variable), t ∈[a, b] ⊂R, which we
shall call “time” for convenience. We refer to P as the dimension of the system. The order
of the system refers to the highest-order derivative in the expression. Nevertheless, ODEs
of order greater than one can be reformulated by deﬁning derivatives as additional states.
Therefore, without loss of generality, we shall consider ﬁrst order ODE systems, written in
the explicit form:
˙u(t, θ) = fθ
(
t, u(t, θ)
)
,
t ∈[a, b],
where the vector ﬁeld function, fθ : [a, b] × RP →RP , is fully speciﬁed given the inputs (or
model parameters), θ ∈Θ. The space containing all possible state trajectories is called the
phase space.
For the purposes of modelling real systems, and in order to obtain a ﬁnite number of
solutions satisfying the system dynamics, some constraints on the states or derivatives must
be imposed. Broadly speaking, the type of constraints and their relationship to the model
deﬁne diﬀerent classes of ODE problems discussed below.
2.1.1
Initial value problems
Initial value problems (IVPs) are well suited for describing systems that begin at known
initial conditions and evolve according to the dynamics described by the ODE model. Many
problems in the natural sciences are formulated as IVPs, such as the trajectory of a single
object orbiting a large stationary body; the movement of a simple pendulum; atmospheric

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
8
convection; chemical kinetics; and the interaction of animal populations in a predator-prey
system.
Deﬁnition 1. A P-dimensional, ﬁrst-order initial value problem (IVP) in explicit form is
given by:







˙u(t, θ)
=
fθ
(
t, u(t, θ)
)
,
t ∈[a, b],
u(a, θ)
=
ua,
(2.1)
on the interval [a, b], where u( · , θ) : [a, b] →RP , and fθ : [a, b] × RP →RP .
A unique solution for this system exists under a relatively mild assumption on the vector
ﬁeld, known as the Lipschitz condition.
Deﬁnition 2. Let | · | denote the Euclidian vector norm. A function f : [a, b] × RP →RP
is Lipschitz continuous with respect to the second variable if there exists a constant L < ∞
such that, for any t ∈[a, b] and u, v ∈RP , the inequality:
|f (t, u) −f (t, v) | ≤L|u −v|,
holds.
Roughly, this condition ensures that small deviations between trajectories do not prop-
agate into disproportionately large diﬀerences in their respective vector ﬁeld evaluations.
This allows the use of a result known as the contraction mapping lemma on metric spaces
(e.g. Butcher, 2008, p. 22) to show that a unique solution satisfying (2.1) exists. We refer
the reader to Butcher (2008, p.23) for the proof of the following well-known result.
Theorem 2.1.1. If fθ : [a, b] × RP →RP is Lipschitz continuous in the second argument,
then there exists a unique solution, u(·, θ) : [a, b] →RP , satisfying the initial value problem
(2.1).
Although a unique solution for (2.1) exists under this relatively mild condition, it is
typically not available in closed form. In such cases, modellers rely on numerical integration
(or numerical solution) techniques to provide an approximation to the ODE solution, based
on discretizing the time domain by an ordered partition s = [s1, · · · , sN] ∈[a, b]N, where

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
9
each point sn is called a knot. Many widely-used numerical methods are variants of the
approximation obtained by replacing the integrals in the equations:







u(sn+1, θ)
=
u(sn+1−k, θ) +
∫sn+1
sn+1−k fθ
(
t, u(t, θ)
)
dt,
k −1 ≤n ≤N,
u(s1, θ)
=
ua,
(2.2)
with a quadrature rule deﬁned by quadrature weights, {wj}J
j=1, intermediate quadrature
nodes, tj ∈[sn−k+1, sn+1], 1 ≤j ≤J, and step number, k. The result is a set of nonlinear
algebraic equations:







u(sn+1, θ)
=
u(sn+1−k, θ) + ∑J
j=1 wj fθ
(
tj, u(tj, θ)
)
,
k −1 ≤n ≤N,
u(s1, θ)
=
ua.
(2.3)
The diﬃculty in solving system (2.3) lies in the implicit dependence of the vector ﬁeld,
fθ, on the state, u.
For this reason, each subsequent state, u(sn+1, θ), must either be
approximated recursively, or via deterministic nonlinear optimization techniques. By their
nature, such numerical solutions, are deterministic, in the sense that a numerical solver
algorithm, implemented under the same conditions, will always yield the same solution for
a given ODE system.
Algorithms deﬁned with k = 1 steps have no evaluation points within each interval of
integration, [sn, sn+1], so a temporary approximation for u(t) is used to evaluate fθ at the J
quadrature nodes within each interval. After the approximation for u(sn+1, θ) is computed,
these temporary values are discarded. Such methods are called one-step quadrature schemes,
and include the well-known Euler and Runge-Kutta algorithms. Multistep methods allow a
step number of k > 1, so that approximation of u(sn+1, θ) can be based on evaluations of
the vector ﬁeld, fθ, at k previously computed states, u(sn−k+1, θ), . . . , u(sn, θ). Examples
of multistep algorithms include the k-step Adams-Bashforth and Adams-Moulton solvers.
For a given discretization grid, s, the quality of the approximation is related to the
choice of quadrature method. Error propagation through the solver occurs largely as a result
of replacing the true solution evaluations with approximations, ˜u(s2, θ), . . . , ˜u(sn, θ), and
using these to estimate the solution, u(sn+1, θ), at the subsequent grid point. Although this
sequential loss of precision may seem alarming, standard assumptions about the smoothness

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
10
of the vector ﬁeld typically result in an upper bound on such error propagation.
Collocation approaches are based on a solution approximation via truncated basis ex-
pansion at each grid knot:
˜u(sn+1, θ) =
J
∑
j=1
cj ψj(sn+1),
1 ≤n ≤N,
with continuously diﬀerentiable basis functions, ψj ∈
(
C1(
[a, b]
))P , and unknown coeﬃ-
cients, cj ∈RP . The coeﬃcients are then obtained by solving the system of N nonlinear
algebraic equations:







∑J
j=1 cj ˙ψj(sn+1)
=
fθ
(
sn+1, ˜u(sn+1, θ)
)
,
1 ≤n ≤N,
u(s1, θ)
=
ua,
usually, by means of a sequential optimization procedure.
When bases have a bounded
support on the scale of the discretization size, we have local collocation schemes. A well-
known global collocation scheme (Shu et al., 2003) uses radially symmetric basis functions,
ψj(tk) = ψj
(
|tk −tj|
)
. The formalism presented in this thesis has some similarities with
collocation methods constructed using a basis system of eigenfunctions of a covariance op-
erator.
Understanding the performance of numerical methods is crucial for practitioners and
modellers. This is called error analysis, and includes statements about the convergence of
a particular method to a true unknown ODE solution under some assumptions, as well as
asymptotic rates of convergence. The theorems apply to a sequence of partitions of the
range [a, b] into N intervals, (sn−1, sn], 2 < n ≤N. The dependence on N of the knots is
supressed in what follows.
Deﬁnition 3. A numerical approximation, ˜un(t, θ), of the true unknown ODE solution,
u(t, θ), is convergent if it satisﬁes,
sup
t∈[a,b]
|˜un(t, θ) −u(t, θ)| →0,
as
max
n=2,...,N
(
sn −sn−1
)
→0,
where | · | is the Euclidian vector norm.

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
11
Proving that this global error tends to zero requires some assumptions about the smooth-
ness of the true solution, typically that it is continuously diﬀerentiable on each interval
(sn−1, sn).
For linear multistep methods, convergence arguments hold the step number
ﬁxed, but consider the step length (related to the size of the discretization grid) in the limit.
Chaotic systems
The deterministic nature and apparent simplicity of many nonlinear dynamical systems can
nevertheless lead to unpredictable behaviour in the solution. This phenomenon, known as
deterministic chaos, lacks a formal deﬁnition but has many characteristic features. Chaotic
systems have time-evolution trajectories that diverge exponentially fast with time following
any small perturbation in the phase space. This feature was ﬁrst noted in Poincar´e (1913,
p. 397), who suggested that prediction for such systems appears to be impossible. The
detailed study of chaos is a recent undertaking, which became possible with computational
advances that allowed for numerical simulation of chaotic systems, starting with the work
of Lorenz (1963) on atmospheric convection using an early computer.
When systems of nonlinear diﬀerential equations describe chaotic trajectories, numeri-
cal solutions inherit their extreme sensitivity to perturbations. Hence, numerical methods
produce solutions that can vary immensely with small changes in the discretization grid or
in the choice of numerical solver. As a result, the deterministic solution approximation does
not capture, even on average, the true long-range behaviour of the system. Crucially, this
uncertainty in solving the forward problem leads to challenges for the inverse problem.
Presently, the study of chaos is extremely relevant for modelling a large variety of physical
systems, including laser cavities, chemical reactions, ﬂuid motion, crystal growth, weather
prediction, earthquake dynamics (Baker and Gollub, 1996, chapter 7). In this thesis, we
provide a probabilistic framework for addressing existing issues for such systems. We will
provide examples ranging from the canonical Lorenz system to a model of reaction-diﬀusion
dynamics, and the classical Navier-Stokes model of turbulent ﬂuid ﬂow.
Lorenz chaotic system
The Lorenz system (Lorenz, 1963) is a canonical example of a chaotic ODE model of greatly
simpliﬁed, three-state ﬂuid convection between two moving surfaces of diﬀerent tempera-
tures. States u, v, and w describe, respectively, the streamﬂow (a feature of the ﬂuid ﬂow),

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
12
the temperature diﬀerence between rising and descending currents, and the nonlinearity
in the temperature diﬀerence between surfaces. The model is formulated as a ﬁrst-order
nonlinear IVP,

























˙u(t, θ)
=
−θ1 u(t, θ) + θ1 v(t, θ),
t ∈[a, b],
˙v(t, θ)
=
−θ2 u(t, θ) −v(t, θ) −u(t, θ) w(t, θ),
t ∈[a, b],
˙w(t, θ)
=
u(t, θ) v(t, θ) −θ3 w(t, θ),
t ∈[a, b],
(u(a, θ), v(a, θ), w(a, θ))
=
(ua, va, wa),
(2.4)
with dimensionless parameters, θ = (θ1, θ2, θ3) ∈R3, that represent ﬂuid properties and
the experimental and temperature conﬁguration. We consider the standard choice of pa-
rameters θ = (10, 8/3, 28) in the chaotic regime. In such cases, the ﬂow (state trajectory)
becomes restricted around a three-dimensional bounded region called a strange attractor
(this was shown independently by Afraimovich et al., 1977; Guckenheimer and Williams,
1979; Williams, 1979). Trajectories travel around one of two unstable ﬁxed points, at times
traversing the attractor to the other ﬁxed point. Moreover, neighbouring trajectories diverge
exponentially fast from one another.
The Lorenz IVP has a unique solution whose trajectory lies on a bounded region of
the phase space (e.g., Robinson, 2001, pp. 271-272). However, the solution is not known
in closed form and must be approximated numerically. Although numerical solutions are
by their nature deterministic, qualitative dynamics of the system are nevertheless studied
statistically by introducing artiﬁcial perturbations on the numerical solution. For example,
the rate of exponential growth (the Lyapunov exponent) between nearby trajectories can be
estimated statistically using numerical techniques while introducing small changes to the
trajectory over a grid deﬁned along the domain.
As with all chaotic systems, numerical methods fail to capture the long-range eﬀect of
error propagation on the system dynamics due to the severe ampliﬁcation of truncation
error (e.g., Foias and Temam (2001); Sauer et al. (1997); and, as illustrated in Figure 2.2).
Nevertheless, the importance of computing numerical solutions to such systems lies in the,
often qualitative, study of the dynamics at larger scales of resolution.
Despite the extreme eﬀects of truncation error, numerical solutions of the Lorenz system

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
13
Figure 2.1: Two numerical solutions to the Lorenz IVP (2.4), for each state in the system (top, middle,
bottom rows), computed under almost identical initial conditions. The trajectories correspond to the initial
function (−10, −5, 36) (red), and the initial function perturbed by adding 10−3 (blue). Numerical solutions
were obtained via the ode45 (MATLAB) numerical solver with an error tolerance of 10−4.
have played a central role in the mathematical study of deterministic chaos (e.g., Mischaikow
and Mrozek, 1995). In particular, the existence of a strange attractor for this system was
shown by Tucker (1999) using a computer-assisted proof. Roughly speaking, the proof re-
quired constructing a specialized numerical solver for the Lorenz system that would provide
exact upper bounds on the numerical error at each solver iteration, particularly within
phase space regions where the trajectories are most sensitive to perturbations. However,
computation of error bounds for general chaotic systems is still an active area of research
(see review in Mrozek and Srzednicki, 2010).
2.1.2
Multi-point boundary value problems
The boundary value problem (BVP) imposes constraints on the states at one or more lo-
cations, c = [c1, · · · , cD] ∈[a, b]D, on the time domain. The method of solution for these
systems varies depending on the form of the constraints. Here we examine two categories of
BVP, depending on whether constraints are speciﬁed at the same time locations for some
subset of the system states. It is important to note that BVP constraints often have the form

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
14
of algebraic equations, which we limit in our exposition to the case where each boundary
value is given an explicit value.
Deﬁnition 4. The P-dimensional, ﬁrst-order, multi-point boundary value problem (MP-
BVP) in explicit form is given by:







˙u(t, θ)
=
fθ
(
t, u(t, θ)
)
,
t ∈[a, b]
(
u(α)(c1, θ), . . . , u(α)(cD, θ)
)
=
(
u(α)
c1 , . . . , u(α)
cD
)
,
α ∈{1, . . . , P} \ ∅,
(2.5)
on the interval [a, b], where u( · , θ) : [a, b] →RP , and fθ : [a, b] × RP →RP . The notation
u(α) represents some non-empty subset of the P states.
Mixed, or separated, boundary value problems do not have all constraints applied at the
same set of time points across states.
Deﬁnition 5. The P-dimensional, ﬁrst-order mixed boundary value problem (MBVP) with
two constraints is given in explicit form by:







˙u(t, θ)
=
fθ
(
t, u(t, θ)
)
,
t ∈[a, b],
(
u(α1)(a, θ), u(α2)(b, θ)
)
=
(
u(α2)
a
, u(α2)
b
)
,
αi ∈{1, . . . , P} \ ∅, α1 ̸= α2,
(2.6)
on the interval [a, b], where u( · , θ) : [a, b] →RP , and fθ : [a, b] × RP →RP .
In contrast to IVPs, which require relatively mild conditions for existence of a unique
solution, the multiple constraints imposed by BVPs can result in no solutions or multiple
solutions under the same conditions (e.g. Keller, 1968).
Approaches for numerically solving multi-point BVPs fall within two categories: shooting
(or initial value) methods, and the ﬁnite diﬀerences method. Both approaches begin by
discretizing the time domain over a ﬁnite grid.
The ﬁnite diﬀerence method attempts
to solve the fully constrained algebraic equations (2.3) simultaneously. The more popular
shooting methods ignore all the constraints in the system except for the known initial states,
propose values for the unknown initial states, and solve the resulting IVP numerically. Each
proposed initial value is then given a weight corresponding to how well the associated IVP
solution satisﬁes the remaining boundary conditions. The resulting objective function in

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
15
the unknown state is then optimized numerically (see, for example Keller, 1968), yielding a
single deterministic approximation of the solution, without any indication that additional
solutions may exist. For this reason, the presence of multiple solutions introduces severe
challenges for parameter estimation methods based on deterministic numerical BVP solvers
if the data is inconsistent with the single solution found in the optimization step.
Lane-Emden mixed boundary value problem
It may be diﬃcult to imagine a dynamic system that imposes constraints on the states at
multiple locations along the domain. Indeed, BVPs do not often directly arise in models
of a speciﬁc phenomenon, but instead occur when higher-order IVPs are translated to ﬁrst-
order, or when PDE boundary value problems are reduced to ODE problems via spectral
projection techniques.
As an example, we consider a special case of the Lane-Emden equation, a second-order
IVP which appears in models of gaseous spherical objects, such as stars (Shampine, 2003),
relating pressure scaled relative to central density to its rate of change over distance from
the centre of the object. The Lane-Emden equation is solved by rewriting it as a ﬁrst order
mixed boundary value problem,















˙u(t, θ)
=
v(t, θ),
t ∈[a, b],
˙v(t, θ)
=
−θ1
v(t,θ)
t
−θ2 u5(t, θ),
t ∈[a, b],
(
u(b, θ), v(a, θ)
)
=
(
ub, va
)
,
(2.7)
where θ = (θ1, θ2) ∈R2. When θ2 is a constant with respect to time, this system has a
unique analytical solution on the interval t ∈[0, 1]. However, multiple solutions may be
possible on a restricted domain of integration. Indeed, numerical approximations of the
solution to the Lane-Emden MBVP on the interval t ∈[0.5, 1], given parameter values
θ = (2, 1), and boundary conditions
(
ub, va
)
=
( √
3
2 , −288
2197
)
, appear to suggest the presence
of two solutions. Each of the numerical solutions shown in Figure 2.2 is obtained using the
bvp4c solver in MATLAB, which employs a shooting technique starting from a user-speciﬁed
initial guess for the optimization routine. In the case that multiple solutions approximately
satisfy the system dynamics, the objective function of the shooting algorithm is multimodal

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
16
Figure 2.2: Numerical solutions to the Lane-Emden MBVP with parameters θ = (2, 1) and boundary
conditions
(
ub, va
)
=
( √
3
2 , −288
2197
)
. The bvp4c (MATLAB) numerical solver was used with two diﬀerent
starting points: ua ∈{1, 2}. Two distinct solutions were obtained (red, blue).
in the unknown initial value. Moreover, modes are extremely highly peaked (converging to
removable discontinuities as the step size tends to zero), and may be located relatively far
apart from one another. The optimizer is thus capable of detecting only one mode, and
corresponding single solution, at a time for a given initial guess.
2.1.3
Delay initial function problems
Incorporating time lags into ODE models allows the description of a large class of systems,
such as mechanisms governing gene transcription (Bernard et al., 2006; Lewis, 2003), sig-
nalling pathways (Swameye et al., 2003), and cell kinetics (Busenberg and Tang, 1994).
ODE delay initial function problems (DIFPs) relate state derivatives to both present and
past states, with ﬁxed delays τj ∈[0, ∞).
Deﬁnition 6. The P-dimensional, ﬁrst-order delay initial function probelm (DIFP) is given
in explicit form by:

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
17







˙u(t, θ)
=
fθ
(
t, u(t, θ), u(t −τ1, θ), . . . , u(t −τd, θ)
)
,
t ∈[a, b],
u(t, θ)
=
ϕ(t),
t ∈[a −max
1≤j≤dτj, a] ,
(2.8)
where u( · , θ) : [a, b] →RP , fθ : [a, b] × RP →RP , and ϕ : [a −max
1≤j≤dτj, a] →RP .
DIFP trajectories depend on an inﬁnite-dimensional input function, or history, ϕ, in-
stead of a ﬁnite-dimensional initial state. For this reason, DIFPs are considered transitional
models between ODEs, which provide vector-valued constraints, and PDEs, which impose
function-valued constraints at the boundaries of the domain. DIFPs are well-suited to de-
scribing biological and physical dynamics that take time to propagate through systems.
Time delayed components are often used to proxy unmodelled or poorly-understood mech-
anisms that introduce a time lag in the dynamics.
Such models can contain additional
functional inputs and yield solutions with periodic discontinuities in the derivative.
We refer the interested reader to Bellen and Zennaro (2003) for an overview of the condi-
tions required for the existence and uniqueness of DIFP solutions, and for a discussion of how
numerical methods approach the variety of challenges associated with such problems. Al-
though numerical DIFP solvers are available, they are sometimes considered unsatisfactory
in applications, as illustrated in Figure 2.3 for a simple system of time-delayed oscillatory
decay. Additional problems arise when the history function is not fully speciﬁed, but only
available at a ﬁnite number of nodes. In this case, numerical methods rely on interpolation
of the initial nodes, and ignore the uncertainty introduced by replacing ϕ with an approx-
imation. This has potential for impacting the system dynamics even over the short term,
changing the structure of the solution. The following simple example illustrates how this
situation aﬀects the numerical approach.
Delayed oscillatory decay model
As an example, let us consider the simple one-dimensional DIFP,

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
18







˙u(t, θ)
=
−θ1 u(t −τ, θ),
t ∈[a, b]
u(t, θ)
=
ϕ(t),
t ∈[a −τ, a],
(2.9)
and consider the model parameters θ = (θ1, τ) = (1, 1). The analytical solution for this
DIFP is obtained by a recursion implemented using the symbolic toolbox in MATLAB.
Let us examine, for comparison, a numerical solution obtained via the dde23 software in
MATLAB when the initial function is only available at a selected number of nodes.
The ﬁrst row of Figure 2.3 shows the analytical solution to problem (2.9) with fully known
initial function, ϕ(t) = 1, t ∈[−1, 0]. In this case, the numerical solution undersmooths
the function starting at t = 1 on the domain, although the oscillatory decay dynamics
of the system are such that the solution eventually settles into a trajectory that matches
the true solution well. In the second row of Figure 2.3, we consider the history function,
ϕ(t) = 1 + sin(4πt)
4
, t ∈[−1, 0], that is only partially speciﬁed, without error, over a set
of six nodes.
An approximation to the initial function is taken to be the mean of an
interpolating Gaussian process with square exponential covariance structure. Uncertainty
associated with the interpolation is discarded, and the dde23 algorithm is applied to solve
the problem numerically. This results in severe deviations from the true solution, even for
this very stable system.
2.2
Partial diﬀerential equation models
Partial diﬀerential equation (PDE) models describe the evolution of a variable implicitly as
a function of its rates of change with respect to multiple indexing variables. PDE models
are ubiquitous in the applied sciences, where they are used to study a variety of phenomena,
from animal movement, to the propagation pattern of a pollutant in the atmosphere.
2.2.1
Initial boundary value problems
PDE boundary value problems (PDE BVPs) relate the state to its rates of change with
respect to multiple variables, under a constraint imposed at the boundary of the domain
D ⊂Rq+1.

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
19
Figure 2.3: Top: analytical (black) and numerical (red) solution to the DIFP (2.9) with fully-speciﬁed
initial function ϕ(t) = 1, t ∈[−1, 0]. Bottom: analytical (black) solution to the DIFP (2.9) with initial
function ϕ(t) = 1 + sin(4πt)
4
, t ∈[−1, 0]. The numerical solution (red) is obtained for the same system with
only a partially speciﬁed initial function, given without error at a set of six knots (circled in red).

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
20
Deﬁnition 7. The P-dimensional PDE boundary value problem (PDE BVP) in implicit
form is given by:







Fθ
(
x, t, Du(x, t, θ), D2u(x, t, θ), . . . , Dku(x, t, θ)
)
=
0,
(x, t) ∈D,
u(x, t, θ)
=
uB(x, t),
(x, t) ∈∂D,
(2.10)
where D ⊂Rq+1 is the domain with boundary ∂D, x ∈Rq is a vector of spatial coordinates,
and u( · , θ) : D →RP is the state. The operator D is deﬁned as:
Dαu(x, t, θ) =
∂|α|u(x, t, θ)
∂tα0 ∂x(1)α1 . . . ∂x(q)αq = ∂tα0 ∂x(1)α1 . . . ∂x(q)αq u(x, t, θ),
using the multi-index notation, |α| = α0, . . . , αq.
Existence of solutions for PDE boundary value problems is a vast area, and we refer
the reader to Jost (2012) for an introduction to this topic. Methods for numerically solving
PDE BVPs of this type typically begin by discretizing the spatial derivatives using algebraic
approximations. Finite diﬀerence approximations for the partial derivatives relate the state
of the PDE at neighbouring spatial discretization nodes to one another. The more precise
spectral methods project the functions deﬁning the PDE onto a ﬁnite-dimensional subspace
of the space of solution trajectories.
More precisely, spectral methods provide a global
approximation in terms of basis functions ψk ∈
(
C1(
[a, b]
))P :
˜um(xm, t, θ) =
Nt
∑
j=1
bj ψj(xm, t),
1 ≤m ≤Nx,
with coeﬃcients bj ∈RP . Substituting this representation into the PDE, and projecting on
the bases, yields a system of Nt × Nx coupled ODEs which, given associated constraints,
can then be solved by one of a number of numerical methods, such as those described in the
previous section.

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
21
Kuramoto-Sivashinsky nonlinear PDE
The Kuramoto-Sivashinsky (KS) PDE is a model of reaction-diﬀusion systems (Kuramoto
and Tsuzuki, 1976; Sivashinsky and Michelson, 1980), originally used to describe phenom-
ena, such as laminar ﬂame fronts, driven far from equilibrium by instabilities. This model
is also of mathematical interest because it exhibits temporal chaotic dynamics.
Just as
with the Lorenz system, we point out that numerical solutions do not provide a sensible
approximation of this system’s long-range behaviour.
The KS system is a one-dimensional, nonlinear PDE BVP describing the time evolution
of the intensity of a ﬂame front, u, by,







∂
∂tu(x, t, θ)
=
−u(x, t, θ) ∂
∂xu(x, t, θ) −∂2
∂x2 u(x, t, θ) −∂4
∂x4 u(x, t, θ),
(x, t) ∈D
u(x, a, θ)
=
uB(x)
(x, t) ∈∂D.
(2.11)
Following Kassam and Trefethen (2005), we consider the spatio-temporal domain D =
[0, 32π] × [0, 150]. Figure 2.4 shows two numerical solutions of the Kuramoto-Sivashinsky
BVP under two nearly identical initial functions: uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
, and
uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
+10−2. Numerical trajectories were obtained by transform-
ing (2.11) to a 128-dimensional system of ODEs via spectral projection on the Fourier space.
The resulting stiﬀODE initial value problem was solved numerically with a fourth-order
Runge-Kutta scheme while using an exponential time-diﬀerencing transformation at each
algorithm iteration (see, for example, Kassam and Trefethen, 2005). Figure 2.4 illustrates
that small diﬀerences in the initial function become ampliﬁed over time, resulting in sub-
stantially diﬀerent dynamics over the second half of the domain. Discretization uncertainty
along the domain likely has a similarly disruptive eﬀect on the solution, which however
cannot be studied using a deterministic numerical solution approximation.
Navier-Stokes model of ﬂuid dynamics
The Navier-Stokes system is a fundamental model of ﬂuid dynamics, incorporating laws of
conservation of mass, energy and linear momentum, as well as physical properties of the
ﬂuid over a domain under constraints imposed along the boundaries. It is used to describe

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
22
Figure 2.4: Top view of a numerical solution of the Kuramoto-Sivashinsky PDE boundary value problem
on the domain D = [0, 32π] × [0, 150]. The numerical solution on the left was obtained under the initial
function uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
, while the numerical solution on the right was obtained under the
initial function uB(x) = cos
( x
16
) {
1 + sin
( x
16
)}
+ 10−2.
a variety of phenomena from the ﬂow of water around a bridge beam to the movement of
air around the wing of an aircraft. Therefore it is an important component of complex
models in oceanography, weather, atmospheric pollution, and glacier movement. Despite
its extensive use, the dynamics of Navier-Stokes models are poorly understood at small
time-scales, where they can give rise to turbulence (chaotic dynamics).
The Navier-Stokes PDEs models the time evolution of P components of the velocity,
u : D →RP , of an incompressible ﬂuid on a spatial domain X. The Navier-Stokes BVP on
the spatio-temporal domain D = X × T is deﬁned by:

























∂
∂tu −θ1 ∆u +
(
u · ∇
)
u
=
f −∇p,
(x, t) ∈D,
∇· u
=
0,
(x, t) ∈D,
∫
u(j) dx
=
0,
(x, t) ∈D, j = 1, 2,
u
=
uB,
(x, t) ∈X × {0}.
(2.12)
The model is parameterized by the viscosity of the ﬂuid, θ1 > 0; the pressure function

CHAPTER 2. DIFFERENTIAL EQUATION MODELS
23
p : D →R, and the external time-homogeneous forcing function f : X →R. We consider
a domain deﬁned by a 2-dimensional torus X = [0, 2π) × [0, 2π), expressed in spherical
coordinates. We further assume periodic boundary conditions, and viscosity θ1 = 1 × 10−3
in the turbulent regime.
Often, the quantity of interest is the local spinning motion of the incompressible ﬂuid,
called vorticity, which we will deﬁne as,
ϖ(x, t, θ) = −∇× u(x, t, θ),
where clockwise rotation corresponds to positive vorticity. This variable will be used to
better visualize the solution of the Navier-Stokes system by reducing the two components
of velocity to a one-dimensional function.

Chapter 3
A Probability Model for the
Unknown Solution and its
Derivative
This chapter introduces a probability model for the functional uncertainty associated with
estimating an unknown DE solution from a ﬁnite number of model evaluations. Such an idea
was ﬁrst considered informally by Skilling (1991), although with little practical success. In
this chapter we translate this insight into a formal framework, and show that the resulting
probability model is well-deﬁned under standard assumptions for general univariate and
multivariate solutions.
Section 3.1 reviews some fundamental notions required for deﬁning measures on general
Hilbert spaces (our focus in this thesis will be the L2(D) Hilbert space of square integrable
functions). In Section 3.2, we introduce the proposed probabilistic model for the uncertainty
in the unknown DE solution. We ﬁrst deﬁne a prior measure on the space of solutions and
then provide an informal derivation of the posterior measure given a set of noisy derivative
realizations. In Section 3.3 we show that this probability measure is indeed well-deﬁned and
absolutely continuous with respect to the prior measure.
24

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
25
3.1
Review of selected measure theoretic concepts
A model of functional uncertainty in the solution of diﬀerential equations requires deﬁning
convenient measures on (inﬁnite-dimensional) spaces of functions of speciﬁed smoothness.
In this section, we provide a selected review of some relevant measure theoretic concepts
and deﬁne the probability measures that we will work with in this thesis. For an in-depth
overview of this subject, we refer the reader to Lifshits (1995), Stuart (2010), and references
therein. This section assumes familiarity with the basics of function space theory, for which
we refer the reader to Megginson (1998).
3.1.1
Radon measures on Hilbert spaces
We restrict1 our attention speciﬁcally to measures deﬁned on Hilbert spaces.
Deﬁnition 8. A Hilbert space (H, || · ||) is a complete vector space, whose norm, || · ||,
is deﬁned by its inner product ⟨·, ·⟩.
Moreover, a Hilbert space that admits a countable
orthonormal basis is called separable.
We will hereafter refer to this space simply as H and omit the vector norm from the
notation. A useful separable Hilbert space that we will work with is the L2(D) space of
square integrable functions on the compact closed set D ⊂Rq+1.
Deﬁnition 9. The space L2(D) includes all functions f : D →R such that
∫
D f2 (x) dx <
∞, and has inner product:
⟨f, g⟩=
∫
D
f(x) g(x) dx,
f, g ∈L2(D).
The above deﬁnition applies to real functions. There are corresponding deﬁnitions of
Hilbert spaces in which the scalar ﬁeld is the set of complex numbers. We will use them
without further comment.
Throughout this thesis we will consider the underlying probability space, (H, A, µ0),
where H is a Hilbert space with associated norm || · ||; A is a σ-algebra of subsets of H; and
µ0 is a probability measure deﬁned on the measure space (H, A). We further restrict our
1Results for more general Banach spaces are sometimes required. We refer the reader to Stuart (2010)
for a more general review in the Banach space setting.

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
26
attention to the class of Radon measures, which have desirable topological properties, and
avoid certain pathologies (e.g., Lifshits, 1995).
Deﬁnition 10. A Radon measure on H is a measure µ such that,
µ(A) = sup
{
µ(B)|B ⊂A, B compact
}
,
A ∈A.
These include Gaussian measures on ﬁnite and inﬁnite-dimensional Hilbert spaces (Billings-
ley, 1968).
We will also use the notions of absolute continuity and the Radon-Nikodym
Theorem:
Deﬁnition 11. Let µ and µ0 be two measures deﬁned on the same measure space, (H, A).
Then µ is absolutely continuous with respect to µ0 if µ0(A) = 0 implies µ(A) = 0 for A ∈A.
Theorem 3.1.1 (Radon-Nikodym Theorem). Let µ and µ0 be two measures deﬁned on
the same measure space, (H, A). If µ is absolutely continuous with respect to µ0 and µ0 is
σ-ﬁnite, then there exists an A-measurable function f : H →[0, ∞] such that,
µ(A) =
∫
A
f(x) dµ0(x),
∀A ∈A.
The function f(x) is denoted by dµ
dµ0 (x) and is called the Radon-Nikodym derivative of
µ with respect to the reference measure µ0, or informally, the density. When H is ﬁnite-
dimensional, the Lebesgue measure is a useful standard reference measure. However, the
Lebesgue measure is not deﬁned when H is inﬁnite-dimensional (see, for example,
Kuo,
1975). In these cases, the Gaussian measure, which exists on inﬁnite-dimensional spaces, is
a convenient choice.
3.1.2
Preliminaries
Appropriate choice of positive deﬁnite covariance functions for our model will require con-
sideration of their properties, including stationarity and isotropy.
Deﬁnition 12. A function K : D × D →R that induces an operator on a Hilbert space H
of functions deﬁned on D via Kg(t) =
∫
K(t, ˜t)g(˜t)d˜t, g ∈H, is called positive semideﬁnite
if ⟨g, Kg⟩≥0. Moreover, if the function also satisﬁes the condition that ⟨g, Kg⟩= 0 iﬀ
g = 0, it is called positive deﬁnite.

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
27
Note for the space of interest L2(D), the (necessary and suﬃcient) condition for positive
semideﬁniteness can be written as,
∫
D
∫
D K(t, ˜t) g(t) g(˜t) dµ(t) dµ(˜t)
≥0,
∀g ∈L2(D),
with respect to the measure µ. For positive deﬁniteness, this integral must also equal zero
iﬀg = 0.
Deﬁnition 13. A function K : D × D →R is called stationary if it can be written as a
function of the diﬀerence between its arguments:
K(t, ˜t) = g(t −˜t).
Furthermore K is isotropic if it can be written as function of the normed diﬀerence between
its arguments,
K(t, ˜t) = g(|t −˜t|).
3.1.3
The Gaussian measure
In this thesis, we will consider continuously-indexed Gaussian processes, where the indexing
variable can be any element of an uncountable set D ⊂Rq+1.
Deﬁnition 14. A Gaussian process {h(d, ω) : d ∈D, ω ∈Ω} is a mapping D ×
(
Ω, A, P
)
→
R such that h(·, ω) ∈H, distributed according to a Gaussian measure on
(
H, B(H)
)
, where
B(H) is the Borel sigma algebra of subsets of H.
Therefore, we will sometimes refer to the Gaussian process {h(d) : d ∈D}, omitting
dependence on ω, as a random function with samples h : D →R ∈H, having Gaussian
measure. The Gaussian measure on an inﬁnite-dimensional Hilbert space is characterized
by the joint distribution of any ﬁnite-dimensional realization.
Deﬁnition 15. A measure µ deﬁned on
(
H, B(H)
)
is Gaussian if for any ﬁnite set of
indices, d = [d1, · · · , dN] ∈DN, the vector h = [h(d1), · · · , h(dN)] ∈
(
H∗)N, where H∗is
the dual space of linear functionals of H, is distributed according to a multivariate Gaussian
measure, NN
(
m, C
)
, where m ∈RN is the mean vector and C ∈MN(R) is a positive deﬁnite
covariance matrix.

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
28
The mean and covariance of a measure on H are deﬁned as follows.
Deﬁnition 16. Let H be a Hilbert space of functions. The mean of a measure µ on (H, A)
is a function m ∈H such that for all h ∈H,
m =
∫
H
h(x)dµ(x),
which we denote by Eh. The covariance of µ is a linear operator C : H →H such that for
all k, ℓ∈H
C = E (k −m) ⊗(ℓ−m),
where ⊗is the tensor product.
Importantly, given a mean and covariance operator, we can uniquely describe a Gaussian
process.
Theorem 3.1.2. A Gaussian process is uniquely deﬁned by its mean and covariance oper-
ator.
Next, we present an important theorem that will allow us to show that the conditional
unknown DE solution and its derivative, as deﬁned in the next section, exist and are dis-
tributed according to a Gaussian measure.
Theorem 3.1.3 (e.g., Stuart, 2010). Let H = H1 ⊕H2 be a separable Hilbert space, and let
(
h1, h2
)
∈H1 ⊕H2 be a Gaussian random variable with mean m =
(
m1, m2
)
and positive
deﬁnite covariance operator C. Then the conditional distribution of h1 given h2 is well-
deﬁned, Gaussian with mean and covariance operator:
m = m1 + C12C−1
22 (h2 −m2) ,
C = C11 −C12C−1
22 C21,
where the cross-covariances are deﬁned as Cij = E (hi −mi) ⊗(hj −mj).

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
29
3.2
Model for the probabilistic solution and its derivative
We wish to construct a probability model describing our uncertainty about an unknown
inﬁnite-dimensional solution of a diﬀerential equation problem reconstructed from a ﬁnite-
dimensional vector of evaluations generated from the model. Such a probabilistic represen-
tation requires modelling solution uncertainty as a measure deﬁned on a suitable probability
space. From a modelling perspective, care must be taken in describing this as a model of our
knowledge about the system given available information, rather than of any stochasticity
inherent in the deterministic DE model. This characteristic is a familiar one in the ﬁeld
of Bayesian statistics, where distributions on unknown quantities are used to describe their
uncertainty relative to the observer.
We begin by considering a univariate ODE solution u ∈H on the domain D ⊂R, and
then extend the result for multivariate PDE solutions. We restrict our attention to solutions
on the space H =
(
L2(D)
)P , noting that the model can be extended to more general Hilbert
spaces. Employing Gaussian distributional assumptions throughout allows us to obtain a
closed form expression for the posterior distribution of the solution and its derivative. It is
certainly possible to make alternative distributional assumptions, which will introduce the
need for an additional layer of Monte Carlo draws from the posterior distribution, as we
will see in Chapter 4.
3.2.1
Univariate solution model
Suppose that there exists an unknown function, or functions, u(·, θ) : D →RP satisfying
a general ODE problem (e.g., 2.1, 2.5, 2.6, or 2.8). The vector ﬁeld fθ and parameters θ
are fully speciﬁed, and any constraints, such as initial or boundary conditions, are provided
without error. From this we can usually obtain full or partial a priori information about
the smoothness of the solution. We can also make a judgement about whether multiple
solutions are possible, such as in the case of mixed boundary value problems, and, in some
cases, the exact number of solutions. This information will be encoded in the prior measures
on the solution and its derivative, which will then be updated based on a set of derivative
realizations generated from the model.
For exposition, we have made the simplifying assumption that the P solution states are
mutually independent, so that cross-covariances between states in our model may be set to
zero and the analysis can proceed sequentially over each dimension. Indeed, an analogous

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
30
assumption commonly underlies classical numerical methods. However, our probabilistic
approach can straightforwardly incorporate dependence between states by modelling the
derivatives via dependent Gaussian process priors (see, for example, Boyle and Frean, 2005).
Derivative realizations
Let us consider a discrete grid, or mesh, deﬁned by the partition s1:n = [s1, · · · , sn]T ∈Dn
of the domain D = [a, b] ⊂R. Suppose also that we are able to obtain n realizations,
f1:n = [f1, · · · , fn]T ∈M n,P (
R
)
, of the random function describing our knowledge of the
derivative. Assume the following Gaussian error model:
f1:n | ˙u(s1, θ), . . . , ˙u(sn, θ) ∼Nn
(
[ ˙u(s1, θ), . . . , ˙u(sn, θ)]T, Λn
)
,
(3.1)
where Λn ∈
(
Mn(R)
)P is a positive deﬁnite covariance matrix.
Solution derivative
Next, deﬁne a Gaussian prior measure for the time derivative of the solution function,
˙u(·, θ) ∼µf
0 = N(mf
0, Cf
0 ),
(3.2)
on the measure space
(
L2(
[a, b]
)P , A
)
with mean function and covariance:
mf
0(t) = ℓ(t),
(3.3)
Cf
0 (t, ˜t) = α−1 ∫
R Rλ(t, z)Rλ(˜t, z)dz ≡RR(t, ˜t).
(3.4)
The prior mean function, ℓ: [a, b] →RP , contains any information about the shape of
the derivative that is known a priori2. The covariance operator, Cf
0 , is deﬁned in terms
of a kernel function, Rλ : D × R →RP , chosen in such a way that the eigenfunctions of
Cf
0 form a basis for the space containing the derivative of the true solution. The function
Rλ is positive deﬁnite and square integrable in each input, and scaled by the length-scale
hyperparameter, λ ∈(0, ∞)P , and the prior precision hyperparameter, α ∈(0, ∞)P . We use
the notation RR = RKR∗to denote the operator obtained by a convolution of R with its
2Practically speaking, if we have some prior knowledge of the derivative dynamics, encoding them in the
prior mean function will, in general, require using fewer model evaluations than would otherwise be necessary
to obtain a reliable estimate of the solution.

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
31
adjoint, R∗weighted by K = α−1. For convenience, we will often omit from the notation
the dependence on auxiliary parameters, Ψn = [α, λ, R, s1:n], associated with the derivative
model.
Let us use the subscript n to indicate an update based on n derivative realizations, f1:n.
Informally, updating the prior derivative measure (3.2) using model derivative realizations
distributed according to (3.1) yields the Gaussian predictive posterior measure,
˙u(·, θ) | f1:n, ua ∼µf
n = N
(
mf
n, Cf
n
)
,
(3.5)
on the probability space
(
L2(
[a, b]
)P , A, µf
0
)
with mean and covariance:
mf
n(t) = mf
0(t) + RR(t, s1:n)
(
Λn + RR(s1:n, s1:n)
)−1(
f1:n −mf
0(s)
)
,
(3.6)
Cf
n(t, ˜t) = RR(t, ˜t) −RR(t, s1:n)
(
Λn + RR(s1:n, s1:n)
)−1RR(s1:n, ˜t).
(3.7)
This notation will be convenient when we consider a sequential algorithm for sampling
derivative realizations in the next chapter.
It is useful to note here that the posterior distribution (3.5) in the derivative space takes
the form of the well-known Gaussian process regression model (see, for example, Rasmussen
and Williams, 2006).
Solution state
We can now obtain a model of the solution, u(·, θ) : D →RP , by taking the corresponding
integrated Gaussian prior measure:
u(·, θ) ∼µ0 = N(m0, C0),
(3.8)
on the measure space
(
L2(
[a, b]
)P , A
)
with mean and covariance:
m0(t) = ua +
∫t
a ℓ(z)dz,
(3.9)
C0(t, ˜t) = α−1 ∫
R Qλ(t, z)Qλ(˜t, z)dz ≡QQ(t, ˜t),
(3.10)
where Qλ(t, z) =
∫t
a Rλ(x, z)dx is the integrated scaled kernel, and ua is the initial condition
of the diﬀerential equation. The mean function m0 can be used to enforce boundary con-
ditions and constraints, as will be shown in Chapter 4. Informally, the resulting posterior

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
32
measure for the solution of the diﬀerential equation follows as,
u(·, θ) | f1:n, ua ∼µn = N
(
mn, Cn
)
,
(3.11)
on the probability space
(
L2(
[a, b]
)P , A, µ0
)
with mean function and covariance:
mn(t) = m0(t) + QR(t, s1:n)
(
Λn + RR(s1:n, s1:n)
)−1(
f1:n −mf
0(s1:n)
)
,
(3.12)
Cn(t, ˜t) = QQ(t, ˜t) −QR(t, s1:n)
(
Λn + RR(s1:n, s1:n)
)−1RQ(s1:n, ˜t).
(3.13)
Hence, under the Gaussian prior speciﬁcation and Gaussian error model, we have a
closed form representation of the posterior distribution of the solution state and of its
derivative. All that remains for conveniently evaluating their realizations is a closed form
representation of RR, RQ, QR, QQ : [a, b] × [a, b] →RP . These are provided in Chapter 6
for three illustrative types of kernel function.
3.2.2
Multivariate solutions
Having deﬁned a probability model for the uncertainty in an unknown ODE solution, we
can now examine the case of PDE solutions of the form u(·, θ) : D →RP on the domain
D ⊂Rq+1, q > 0. We will consider Gaussian processes indexed by a spatio-temporal variable
d = [x, t]T ∈Rq+1 with partition deﬁned by the mesh with vertices s1:n ∈Dn. As before,
we deﬁne a Gaussian prior distribution on the derivative function ˙u(·, θ) and update it
given derivative realizations f1:n. We deﬁne the mean function as before, but assume tensor
product (multiplicative) kernel functions in each dimension,
Rλ(d, ˜d) =
q+1
∏
i=1
Rλ(i)(d(i), ˜d(i)),
where λ ∈Rq+1 is a vector of length-scales, and the notation, d(i), denotes the ith element
of the vector d. Indeed, a multiplicative kernel greatly simpliﬁes the analysis, as we are able
to compute the required convolutions exactly as follows:
RR(d, ˜d) = α−1 ∫
Rq+1 Rλ(d, z)Rλ( ˜d, z)dz
= α−1
∫
Rq+1
q+1
∏
i=1
Rλ(i)(d(i), z(i))
q+1
∏
i=1
Rλ(i)( ˜d(i), z(i)) dz(i)

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
33
= α−1
q+1
∏
i=1
∫
R
Rλ(i)(d(i), z(i)) Rλ(i)( ˜d(i), z(i)) dz
= α−1
q+1
∏
i=1
RRλ(i)(d(i), ˜d(i)),
and, similarly,
RQ(d, ˜d) = α−1
q+1
∏
i=1
RQλ(i)(d(i), ˜d(i)),
QR(d, ˜d) = α−1
q+1
∏
i=1
QRλ(i)(d(i), ˜d(i)),
QQ(d, ˜d) = α−1
q+1
∏
i=1
QQλ(i)(d(i), ˜d(i)).
One important requirement for obtaining the correct covariances is to properly parameterize
the domain D in order to evaluate each of the bivariate convolutions involving Qλ(i) over
the correct intervals D(1), . . . , D(q+1).
Probabilistic Posterior Solution
We will hereafter use the term probabilistic solution to refer to the random function dis-
tributed according to the posterior measure (3.11) deﬁning our belief about the unknown
diﬀerential equation solution given a set of noisy realizations of the derivative.
Deﬁnition 17. The probabilistic solution of a PDE problem, given n realizations of the
derivative, refers to the random function,
u(·, θ) | θ, f1:n, ua, Ψn ∈
(
L2(
D; (Ω, A, P)
))P ,
(3.14)
distributed according to (3.11). For notational convenience, we shall sometimes denote the
probabilistic solution by un(·, θ), and its derivative by ˙un(·, θ).
Therefore, we now have a probability statement describing DE solution uncertainty at

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
34
any ﬁnite set of time points, d = [x, t]T ∈R(q+1)×T , conditioned on an approximate ﬁnite-
dimensional solution estimated using an n-dimensional mesh s:
p
(
u(d, θ)
 θ, f1:n, ua, Ψn
)
= NT
(
mn(d), Cn(d, d)
)
.
(3.15)
The next section provides technical details about the above informal derivation, and
shows that the posterior measures presented above are indeed well-deﬁned.
3.3
The probabilistic solution is well-deﬁned
In this section, we justify the claim that the conditional distribution of the DE solution
given n derivative evaluations is well-deﬁned, and that it has density dµn/dµ0 with respect
to the prior measure µ0.
Theorem 3.3.1. Consider the model presented in Section 3.2 and let ℓ: D →RP and
Rλ : Rq+1 × Rq+1 →RP be deterministic, square integrable functions. The probabilistic
solution un (·, θ) and its derivative ˙un (·, θ) are well-deﬁned and distributed according to
(3.11) and (3.5), respectively.
Proof. Without loss of generality, we restrict our attention to solutions of dimension P = 1
on a univariate domain, D = [a, b] ⊂R. Consider the space F = L2 (R; (H, A, µ0)) and
let F∗denote the dual space of linear functionals of F. Deﬁne, for all u ∈F and v ∈
F∗, the following linear continuous operators. Deﬁne R
: F
→F∗to be the integral
transform, Ru(t) =
∫∞
−∞Rλ(t, z)u(z)dz, and let R∗: F∗→F be its adjoint, R∗v(t) =
∫∞
−∞Rλ(z, t)v(z)dz. Similarly, deﬁne Q : F →F∗to be the integral transform, Qu(t) =
∫∞
−∞Qλ(t, z)u(z)dz, and let Q∗: F∗→F denote its adjoint, Q∗v(t) =
∫∞
−∞Qλ(z, t)v(z)dz.
Consider the white noise process ζ(t) ∈F, distributed as,
ζ ∼N(0, K),
with mean 0 and covariance K(t, ˜t) = δt(˜t) α−1. Next, model the derivative of the solution
by the integral transform:
˙u(t, θ) = mf
0(t) + Rζ(t),
(3.16)

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
35
deﬁned for all ζ ∈F and t ∈[a, b]. Then, the diﬀerential equation solution model is obtained
by integrating ˙u(t, θ) with respect to t:
u(t, θ) = m0(t) + Qζ(t).
(3.17)
We are interested in the conditional distribution of the state u(·, θ) −m0 ∈F∗and
derivative ˙u(·, θ)−mf
0 ∈F∗given a vector of n noisy derivative evaluations, f1:n−mf
0(s1:n) ∈
Rn, on a grid deﬁned by s1:n ∈[a, b]n under the Gaussian error model:
f1:n −mf
0(s1:n) = Rζ(s1:n) + η(s1:n),
where η(s1:n) ∼Nn(0, Λn) is independent of ζ, and Λn ∈Mn(R) is a positive deﬁnite matrix.
Now consider the vector [ ˙u(·, θ)−mf
0, f1:n−mf
0(s1:n)] = [Rζ, Rζ(s1:n)+η(s1:n)] ∈F∗⊕Rn,
where the ﬁrst element is function-valued and the second element is a vector-valued. This
vector is jointly Gaussian with mean M = (0, 0) and covariance operator C with positive
deﬁnite (see Chapter 6) cross-covariance operators:
C11 = RKR∗
C12 = RKR∗
C21 = RKR∗
C22 = RKR∗+ Λn.
(3.18)
Since both F∗and Rn are separable Hilbert spaces, it follows from Theorem 3.1.3 that the
random variable [ ˙u(·, θ) −mf
0|f1:n −mf
0(s1:n)] is well-deﬁned and distributed according to a
Gaussian measure with mean and covariance:
E
(
˙u(·, θ) −mf
0
 f1:n −mf
0(s1:n)
)
= C12C−1
22 (f1:n −mf
0(s1:n)),
Cov
(
˙u(·, θ) −mf
0
 f1:n −mf
0(s1:n)
)
= C11 −C12C−1
22 C21.
Similarly, we consider the vector [u(·, θ)−m0, f1:n−mf
0(s1:n)] = [Qζ, Rζ(s1:n)+η(s1:n)] ∈
F∗⊕Rn, with mean M = (0, 0) and cross-covariances:
C11 = QKQ∗
C12 = QKR∗
C21 = RKQ∗
C22 = RKR∗+ Λn.

CHAPTER 3. PROBABILITY MODEL FOR THE UNKNOWN SOLUTION
36
By Theorem 3.1.3, the conditional distribution of [ ˙u(·, θ) −mf
0|f1:n −mf
0(s1:n)] is a well-
deﬁned Gaussian distribution with mean and covariance:
E
(
u(·, θ) −m0
 f1:n −mf
0(s1:n)
)
= C12C−1
22 (f1:n −mf
0(s1:n)),
Cov
(
u(·, θ) −m0
 f1:n −mf
0(s1:n)
)
= C11 −C12C−1
22 C21.
Furthermore, µn and ˙µn are absolutely continuous with respect to the corresponding
prior Gaussian measures µ0 and µf
0, and therefore the Radon-Nikodym derivatives dµn/dµ0
and dµf
n/dµf
0 exist.
Extension to multivariate solutions follows straightforwardly under tensor product ker-
nels, deﬁned in Section 3.2.2.
Extension to solutions of dimension P > 1 is immediate
for mutually independent states. Dependence between states can be incorporated through
dependent Gaussian priors, and requires more cumbersome notation.
The posterior distribution of the derivative function is a Gaussian process, and the
posterior distribution of the state is its integrated version. Therefore, we can obtain the
distribution of any ﬁnite number T of sample evaluations on d ∈DT by Deﬁnition 15. The
conditional density of the state evaluated at d is jointly Gaussian,
p
(
u(d, θ)
 f1:n −mf
0(s1:n), ua, Ψn
)
= NT
(
mn (t) , Cn (d, d)
)
,
where mn and Cn are deﬁned in (3.13). Similarly, the conditional density of the derivative
computed at d becomes,
p
( ˙u(d, θ)
 f1:n −mf
0(s1:n), ua, Ψn
)
= NT
(
mf
n (d) , Cf
n
(
d, d
))
.
where mf
n and Cf
n are deﬁned in (3.7). As a consequence, estimation only requires a ﬁnite
number of matrix operations if the convolutions RR, QR, RQ, and QQ can be obtained
analytically (see Chapter 6).
Equation (3.15) suggests the probabilistic solution can be utilized as an alternative to
a numerical solution approximation and associated error analysis, with prediction replacing
deterministic interpolation. There just remains the question of how to obtain noisy eval-
uations of the derivative. In the following chapter we provide a detailed description of a
suitable probabilistic algorithm and show that it has desirable properties.

Chapter 4
Probabilistic Solution of
Diﬀerential Equations
In Chapter 3, we presented a Gaussian process model for the DE solution given a ﬁnite-
dimensional set of realizations, f1:n = [f1, · · · , fn]T ∈Mn,P (
R
)
, of the derivative process
over the discrete mesh with vertices s1:n = [s1, · · · , sn]T ∈Dn. In this chapter we show
how to generate realizations, f1:n, from the model when the solution and its derivative
are unknown.
Based on this sequential approach, we develop probabilistic solutions for
P-dimensional ODE problems (2.1), (2.5), (2.6), and (2.8), as well as the P-dimensional
PDE boundary value problem (2.10). For each case, we outline a sampling algorithm for
generating realizations from the joint density of the predicted state, u(t, θ), t ∈DT and
vector ﬁeld evaluations, f1:N,
p
(
u(t, θ), f1:N
θ, ua, ΨN
)
= p
(
u(t, θ)
f1:N, θ, ua, ΨN
)
p
(
f1:N
θ, ua, ΨN
)
,
(4.1)
where the two terms on the right hand side represent the probability density of the proba-
bilistic solution and the auxiliary vector of ﬁeld evaluations, respectively. We also suggest
a recursive formulation of the proposed algorithms, which introduces computational eﬃ-
ciencies, provides intuitive insight about iterative updating, and is used to show that the
probabilistic solution of a Lipschitz-continuous ODE initial value problem is consistent for
the unique solution satisfying the system under standard regularity conditions. Finally, we
illustrate our methodology through a number of challenging forward problems described in
37

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
38
Chapter 2.
4.1
Solving ODE initial value problems
Our goal is to generate a sequence of derivative realizations, f1:N, upon which we may
condition our inference of the solution of a system of diﬀerential equations.
Moreover,
we must generate f1:N in a completely model-based way, without the availability of any
experimental measurements of the states, or any prior knowledge of the true solution or
suitable approximation. As hinted in the introduction, we may consider these realizations
to be auxiliary variables that extract information from the model sequentially.
4.1.1
Generating derivative realizations from the model
Suppose that at time step (n + 1), 1 ≤n < N, we have access to n derivative realizations,
f1:n, corresponding to the ordered knots s1:n. Conditioned on these, and according to the
model presented in Chapter 3, we can obtain a closed form predictive distribution µn for the
solution that admits sampling. We then generate a realization, ˜un
(
sn+1, θ
)
, of the solution
state at the next deﬁned knot sn+1 and map the realization to the derivative space by using
the vector ﬁeld transformation,
fn+1 = fθ
(
sn+1, ˜u(sn+1, θ)
)
.
In a fundamental diﬀerence with numerical solvers, our model accounts for the fact that the
derivative realization fn+1 is measured with error (arising from using a previously estimated
state). Indeed, we are able to incorporate this uncertainty into each subsequent posterior
solution by parameterizing the error model (3.1) for fn+1 using the variance Cf
n(sn+1, sn+1).
We can now update the estimated state given the newly generated derivative observation,
fn+1, and update the covariance of the error model (3.1) as follows:
Λn+1 = diag
(
Cf
0 (s1, s1), . . . , Cf
n(sn+1, sn+1)
)
.
(4.2)
Once all N derivative realizations are obtained, we can make predictions about any of the
P solution states at an arbitrary collection of evaluation points, t ∈DT . This procedure
is described in Algorithm 1 for the ODE initial value problem (2.1), using the notation

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
39
.
ua
.
f1 = fθ
(
a, ua
)
.
˜u1(s2, θ)
.
f1:2 = [f1, fθ
(
s2, ˜u1(s2, θ)
)
]
.
. . .
.
. . .
.
˜uN−1(sN, θ)
.
f1:N = [f1:N−1, fθ
(
sN, ˜uN−1(sN, θ)
)
]
.
uN(t, θ)
Figure 4.1:
Directed acyclic graph diagram for Algorithm 1, producing a sample from the probability
density p
(
uN(t, θ), f1:N
θ, ua, ΨN
)
. The grey nodes represent values that are returned by the algorithm,
and all others are discarded.
mn and Cn deﬁned in (3.12, 3.13). We characterize the error in the derivative realizations
by deﬁning the covariance matrix Λn in terms of the variance of the step-ahead predictive
posterior density for the derivatives as shown in (4.2). This algorithm is also illustrated
graphically in Figure 4.1.
In Section 4.1.4 we will show that the sequential probabilistic solution generated via
Algorithm 1 is consistent for the unique solution satisfying the IVP (2.1). The recursive
formulation used in the proof, which allows eﬃcient implementation of Algorithm 1, is
developed in Section 4.1.3.
Probabilistic solution of the Lorenz system
Figure 4.2 shows 100 realizations from the probabilistic solution of the Lorenz system with
model parameters θ = (10, 8/3, 28) and initial states ua = (−11, −5, 38). The probabilistic
solution is obtained by discretizing the interval t ∈[0, 30] by an equally-spaced grid of size
N = 3000. The squared exponential covariance is chosen for this application based on our
assumption that the solution is inﬁnitely-diﬀerentiable.
We set the length-scale in each
dimension to twice the step size, eﬀectively giving largest weight to the latest generated
derivative realization. The prior precision is set to the low value of α = 10−2 [1, 1, 1]T units,
reﬂecting our prior knowledge that the system exhibits chaotic dynamics.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
40
Algorithm 1 Sample a probabilistic IVP solution from p
(
u(t, θ), f1:N
θ, ua, Ψn
)
Initialize f1 = fθ(a, ua) and C0(s1, s1) = 0 ;
for step number n = 1 : N −1 do
for state p = 1 : P do
Sample step-ahead realization ˜u(p)(sn+1, θ) from the predictive distribution for state
p,
˜u(p)(sn+1) ∼p
(
u(p)(sn+1, θ)
θ, f (p)
1:n, ua, Ψn
)
= N
(
m(p)
n (sn+1), C(p)
n (sn+1, sn+1)
)
;
end for
Compute next derivative observation with mean fn+1 = fθ(sn+1, ˜u(sn+1, θ)) and vari-
ance Cf(p)
n
(sn+1, sn+1);
end for
for state p = 1 : P do
Sample a realization, u(p)(t, θ), of the solution from the conditional distribution for
state p at the chosen points t ∈[a, b]T ,
u(p)(t, θ) ∼p
(
u(p)(t, θ)
θ, f(p)
1:N, ua, Ψn
)
= N
(
m(p)
N (t), C(p)
N (t, t)
)
;
end for
Return u(t, θ), f1:N.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
41
Figure 4.2: Sample of 100 realizations from the probabilistic solution for the Lorenz system under a ﬁxed
initial state.
4.1.2
Relationship with numerical solvers
Interestingly, many classical numerical solvers can be interpreted from this probabilistic
perspective. Instead of providing a probability measure on the a space of functions, they
instead yield a functional point estimate of the DE solution.
This can be achieved by
Rao-Blackwellizing the probabilistic solution at each step of Algorithm 1,
un(sn+1) = E
(
u(·, θ) | θ, f1:n, ua, Ψn
)
,
while assuming noise-free derivative realizations f1:n, by letting Λn = 0 for all 1 ≤n ≤N.
Under these assumptions, the probabilistic solver becomes a k-step numerical radial basis
function quadrature method with bases given by the eigenfunctions of the covariance C0.
The number of steps is determined by the length-scale1. Using covariance functions with
unbounded support results in a more general multi-step solver, which gives positive weight
to all past derivative realizations, with weights decaying towards zero at rates that depend
1for example, under a covariance function with bounded support of length 2λ and ﬁxed step size h, the
number of steps in the algorithm becomes k = ⌈λ/h⌉.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
42
on the choice of covariance.
4.1.3
Recursive formulation of probabilistic solution
In this section we introduce two results. Lemma 4.1.1 provides a recursive formulation for
the probabilistic solution of the IVP (2.1) generated via Algorithm 1. This lemma is an
important part of the proof of consistency in Section 4.1.4. The related result in Lemma
4.1.2 allows us to avoid computationally expensive matrix inversions in the algorithmic
implementation.
We denote the probabilistic solution and its derivative obtained after n solver iterations
by un(t, θ) and ˙un(t, θ) respectively, and derivative realizations by:
f1:n =
[
fθ
(
a, ua
)
, fθ
(
s2, ˜u1(s2, θ)
)
, · · · , fθ
(
sn, ˜un−1(sn, θ)
)]T,
where ˜un(t, θ) denotes a realization of the probabilistic solution un(t, θ) at time t ∈D.
Lemma 4.1.1. The probabilistic IVP solution and its derivative at the nth iteration of
Algorithm 1 can be expressed in terms of the mean and covariance computed in the (n−1)st
iteration. They are related by the expressions:
mn(t) = m0(t) + mn−1(t) + (fn −mf
0(sn) −mf
n−1(sn))
∫t
a Cf
n−1(x, sn)dx/2Cf
n−1(sn, sn)
mf
n(t) = mf
0(t) + mf
n−1(t) + (fn −mf
0(sn) −mf
n−1(sn)) Cf
n−1(t, sn)/2Cf
n−1(sn, sn)
Cn(t, v) = Cn−1(t, v) −Cn−1(t, sn) Cn−1(sn, v)/2Cf
n−1(sn, sn)
Cf
n(t, v) = Cf
n−1(t, v) −Cf
n−1(t, sn) Cf
n−1(sn, v)/2Cf
n−1(sn, sn).
Furthermore, the matrix B−1
n
= (Λn + RR(s1:n, s1:n))−1 can be written in block form in
terms of B−1
n−1 (row and column indices are denoted in brackets),
B−1
n(1:n−1,1:n−1) =
B−1
n−1
(
2Cf
n−1(sn, sn) + RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1
)
/2Cf
n−1(sn, sn)
B−1
n(1:n−1,n) = −B−1
n−1RR(s1:n−1, sn)/2Cf
n−1(sn, sn)
B−1
n(n,1:n−1) = −RR(sn, s1:n−1)B−1
n−1/2Cf
n−1(sn, sn)
B−1
n(n,n) =
1/2Cf
n−1(sn, sn),
starting from B−1
1
= 1/RR(s1, s1) and Cf
0 (s1, s1) = 0.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
43
Proof. We use the fact that Bn is a non-negative symmetric partitioned matrix to write its
inverse in the block form:
B−1
n
≡(Λn + RR(s1:n, s1:n))−1 =


Bn−1
RR(s1:n−1, sn)
RR(sn, s1:n−1)
Cf
n−1(sn, sn) + B1


−1
=
1
2Cf
n−1(sn, sn)


B−1
n−1
(
2Cf
n−1(sn, sn) + RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1
)
−B−1
n−1RR(s1:n−1, sn)
−RR(sn, s1:n−1)B−1
n−1
1

,
(see for example, Rohde, 1965), starting with the base case:
B−1
1
=
(
Cf
0 (s1, s1) + RR(s1, s1)
)−1
= 1/RR(s1, s1).
Using this and the deﬁnition of the mean and covariance of the probabilistic solution and
its derivative obtained in Section 3.2, we obtain the expressions,
mn(t) = m0(t) + QR(t, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
+
{
QR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
−QR(t, sn)RR(sn, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
−QR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)(fn −mf
0(sn))
+ QR(t, sn)(fn −mf
0(sn))
}
/2Cf
n−1(sn, sn)
= mn−1(t) + (fn −mf
0(sn) −mf
n−1(sn))
∫t
a Cf
n−1(x, sn)dx/2Cf
n−1(sn, sn),
mf
n(t) = m0(t) + RR(t, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
+
{
RR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
−RR(t, sn)RR(sn, s1:n−1)B−1
n−1(f1:n−1 −mf
0(s1:n−1))
−RR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)(fn −mf
0(sn))
+ RR(t, sn)(fn −mf
0(sn))
}
/2Cf
n−1(sn, sn)
= mf
n−1(t) + (fn −mf
0(sn) −mf
n−1(sn)) Cf
n−1(t, sn)/2Cf
n−1(sn, sn),

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
44
Cf
n(t, v) = RR(t, v) −RR(t, s1:n−1)B−1
n−1RR(s1:n−1, v)
−RR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1RR(s1:n−1, v)/2Cf
n−1(sn, sn)
+ RR(t, sn)RR(sn, s1:n−1)B−1
n−1RR(s1:n−1, v)/2Cf
n−1(sn, sn)
+ 2Cf
n−1(sn, sn)RR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RR(sn, v)/2Cf
n−1(sn, sn)
−RR(t, sn)RR(sn, v)/2Cf
n−1(sn, sn)
= Cf
n−1(t, v) −Cf
n−1(t, sn) Cf
n−1(sn, v)/2Cf
n−1(sn, sn),
Cn(t, v) = QQ(t, v) −QR(t, s1:n−1)B−1
n−1RQ(s1:n−1, v)
−QR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RR(sn, s1:n−1)B−1
n−1RQ(s1:n−1, v)/2Cf
n−1(sn, sn)
+ QR(t, sn)RR(sn, s1:n−1)B−1
n−1RQ(s1:n−1, v)/2Cf
n−1(sn, sn)
+ QR(t, s1:n−1)B−1
n−1RR(s1:n−1, sn)RQ(sn, v)/2Cf
n−1(sn, sn)
−QR(t, sn)RQ(sn, v)/2Cf
n−1(sn, sn)
= Cn−1(t, v) −Cn−1(t, sn) Cn−1(sn, v)/2Cf
n−1(sn, sn).
Lemma 4.1.2. The step-ahead predictive probabilistic solution at the nth step can be ob-
tained by using the recursion,
mn(sn+1) = m0(sn+1) + mn−1(sn+1)
+ (fn −mf
0(sn) −mf
n−1(sn))
∫sn+1
a
Cf
n−1(x, sn)dx/2Cf
n−1(sn, sn)
Cf
n(sn+1, sn+1) = Cf
n−1(sn+1, sn+1) −(Cf
n−1(sn+1, sn))2/2Cf
n−1(sn, sn),
starting with the base case:
B−1
1
= 1/RR(s1, s1),
Cf
1 (s2, s2) = RR(s2, s2) −RR(s2, s1) B−1
1 RR(s1, s2),
m1(s2) = m0(s2) + QR(s2, s1) B−1
1 (f(s1, ua) −mf
0(s1)).

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
45
Proof. The proof follows by directly applying Lemma 4.1.1 and using the fact that the
Gaussian measure is uniquely deﬁned by its mean and covariance (see Theorem 3.1.2).
Computational considerations
The iterative smoothing step in Algorithm 1 requires inverting the n × n matrix, Bn, for
every 1 ≤n ≤N, which quickly becomes computationally expensive. Therefore we rec-
ommend avoiding costly matrix inversions by instead updating the matrix inverse, B−1
n , at
each iteration via Lemmas 4.1.1 and 4.1.2 using output obtained in the previous algorithm
iteration. Additionally, computational eﬃciency can further be increased by using bounded
support kernel functions, which render the Toeplitz matrix RR(s1:n, s1:n) sparse, reducing
the number of matrix operations required for the inversion of Bn.
For a ﬁxed grid s and given hyperparameters, α and λ, the inverse matrix B−1
n
and the
derivative covariance matrix Cf
n(sn+1, sn+1) may be stored and reused when many evalua-
tions of Algorithm 1 are required2 (e.g., when evaluating the posterior distribution of model
parameters θ from observed data).
4.1.4
Posterior consistency
We have described how to infer the unknown solution to the initial value problem (2.1) with
Lipschitz-continuous vector ﬁeld fθ : [a, b] × RP →RP , based on derivative information
generated sequentially from the model. We now show that the resulting probabilistic solution
converges in L1 to the true solution satisfying (2.1). The result is obtained for the case of
IVPs, but also extends to the probabilistic solutions of ODE and PDE problems that are
based on Algorithm 1, which will be presented in the following sections.
Theorem 4.1.3. The probabilistic solution obtained using Algorithm 1 for the initial value
problem (2.1) on [a, b] with Lipschitz continuous vector ﬁeld fθ : [a, b] × Rp →Rp, converges
in L1 to the unique solution satisfying (2.1) if the solution is continuously diﬀerentiable
and its derivative lies in the function space spanned by the eigenfunctions of the covariance
operator Cf
0 .
2The reader is also referred to the related discussion of ensemble MCMC in Neal (2011) for sampling from
mixture distributions while viewing function evaluations as “fast” parameters.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
46
Proof. For clarity of exposition, we assume that m0(t) = 0 for all t ∈[a, b], and deﬁne
h = maxn=2,...,N
(
sn −sn−1
)
to be the maximum step length. We would like to show that
the integrated Gaussian process {un(t, θ), t ∈[a, b]}, or probabilistic solution, is a consistent
estimator of the solution when the vector f1:n is built up sequentially using Algorithm 1
as h →0 and λ, α−1 =
(
o(h)
)P . Solution updates are constructed using the recurrence
derived in Lemma 4.1.1, where the mean update is the expected diﬀerence between the
forward predicted derivative at the nth knot and the corresponding vector ﬁeld evaluation,
multiplied by the scaled cross-covariance
Cf
n−1(t,sn)
2Cf
n−1(sn,sn).
If we further consider the case of
interpolation, the scaled cross-covariance becomes
Cf
n−1(t,sn)
Cf
n−1(sn,sn).
Assume that the true solution u(·, θ) is continuously diﬀerentiable on [a,b]. Then, if
the probabilistic solution un−1(·, θ) is mean-square diﬀerentiable on [a,b], we can write the
diﬀerence between the expected probabilistic solution and the true solution in terms of their
Taylor expansions around sn (assuming bounded remainders):
E (un(t, θ) −u(t, θ))
= E
[{
un(sn, θ) −u(sn, θ)
}
+ (t −sn) ·
{ ˙un(sn, θ) −fθ
(
sn, u(sn, θ)
)}
+ O(h2)
]
= E
[
un−1(sn, θ) +
{
fθ
(
sn, un−1(sn, θ)
)
−˙un−1(sn, θ)
} Cn−1(sn, sn)
Cf
n−1(sn, sn)
−u(sn, θ)
+ (t −sn) ·
[ ˙un−1(sn, θ) +
{
fθ
(
sn, un−1(sn, θ)
)
−˙un−1(sn, θ)
} Cf
n−1(sn, sn)
Cf
n−1(sn, sn)
−fθ
(
sn, u(sn, θ)
)]
+ O(h2)
]
= E
[
un−1(sn, θ) −u(sn, θ) +
{
fθ
(
sn, un−1(sn, θ)
)
−˙un−1(sn, θ)
} Cn−1(sn, sn)
Cf
n−1(sn, sn)
+ (t −sn) ·
[
fθ
(
sn, un−1(sn, θ)
)
−fθ
(
sn, u(sn, θ)
)]
+ O(h2)
]
,
(4.3)
where we have used the recursion from Lemma 4.1.1 to rewrite un(sn, θ) and ˙u(sn, θ). Since
(un(t, θ) −u(t, θ)) is a normal random variable, its absolute value follows a folded normal
distribution (see, for example, Leone et al., 1961) with mean,
E
un(t, θ) −u(t, θ)

(4.4)

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
47
= E
(
un(t) −u(t, θ)
)
{
1 + 2Φ
(
E(un(t) −u(t, θ))
√
Cn(t, t)
)}
+
√
2
π Cn(t, t) exp
{
−E(un(t, θ) −u(t, θ))2
2Cn(t, t)
}
,
where Φ is the cdf of the standard normal distribution. Then we can bound the expected
absolute diﬀerence as follows:
βn(t) ≡E
un(t, θ) −u(t, θ)

≤|E(un(t, θ) −u(t, θ))| +
√
2C1(t, t)
≤βn−1(sn) + |t −sn| · E
fθ
(
sn, un−1(sn, θ)
)
−fθ
(
sn, u(sn, θ)
)
+ E
fθ
(
sn, un−1(sn, θ)
)
−˙un−1(sn, θ)
Cn−1(sn, sn)
Cf
n−1(sn, sn)
+
√
2C1(t, t),
where we have used equation (4.4), then equation (4.3) together with Jensen’s inequality.
The Lipschitz continuity of f and the fact that the expected value of
fθ
(
sn, un−1(sn, θ)
)
−
˙un−1(sn, θ)
 is bounded then implies:
βn(t) ≤βn−1(sn) + L|t −sn|βn−1(sn) + O
(
Cn−1(sn, sn)
2Cf
n−1(sn, sn)
)
+ O(
√
C1(t, t))
= βn−1(sn) (1 + L|t −sn|) + O(h2),
Now we apply the Gronwall Lemma and the standard transformation used in the proofs of
convergence for one-step methods (see, for example, Butcher, 2008, p.67-68) to obtain the
inequality:
βn(t) ≤







β0(s1) + hB(t −a),
L = 0,
exp{(t −a)L}β0(s1) + exp{(t −a)L −1}hB/L,
L > 0,
where B is the constant upper bound on all the remainders. This expression tends to 0 as
α−1, λ, h →0, since β0(s1) = 0. Then, taking the expectation of βn(t) with respect to the
vector of derivative realizations, we obtain:
Ef
un(t, θ) −u(t, θ)
 = Ef
[
E{|un(t, θ) −u(t, θ)|}
]
→0,
as α−1, λ, h →0,
and the probabilistic solution un(·, θ) is consistent for u(·, θ).

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
48
To those who are familiar with the convergence arguments used in numerical analysis,
the assumption that auxiliary parameters, λ and α−1, associated with the solver should
tend to zero with the step size may seem unclear. However, this assumption is analogous
to maintaining a constant number of steps in a k-step numerical method regardless of the
step size.
4.2
Solving ODE Boundary value problems
We begin this section by describing our probability model for the unknown solution of the
mixed boundary value problem (2.6). We then use a variant of this method to obtain and
sample from the probabilistic solution of the two-point boundary value problem (2.5).
Mixed boundary value problems require a diﬀerent probabilistic solver formulation from
the probabilistic IVP, as well as some care about the sampling algorithm. For the purpose
of exposition, we restrict our attention to the MBVP (2.6) with P = 2 states, and boundary
values
(
u(2)(b, θ), u(1)(a, θ)
)
=
(
u(2)
b , u(1)
a
)
. Estimating the solution for this problem can
be reduced to (i) estimation of the initial value for the second state, u(2)(a, θ), followed by
(ii) estimation of the system solution given ˜u(2)(a, θ). We note that the second stage is
equivalent to solving an IVP with initial condition, u(a, θ) =
(
u(1)
a , ˜u(2)(a, θ)
)
, where we
again make the simplifying assumption that the states, u(i)(·, θ), i = 1, 2, are conditionally
independent given vector ﬁeld evaluations, to write,
u(·, θ)
 f1:N, [u(1)
a , ˜u(2)(a, θ)], Ψn ∼N1
(
mn, Cn
)
,
where mn and Cn are deﬁned in (3.12, 3.13), and the derivative realizations are,
f1:n =
[
fθ
(
a, [u(1)
a , ˜u(2)(a, θ)]T)
, fθ
(
s2, ˜u1(s2, θ)
)
, · · · , fθ
(
sn, ˜un−1(sn, θ)
)]T,
where ˜un(t, θ) denotes a sampled realization of the probabilistic solution, un(t, θ), of the
IVP obtained by setting u(a, θ) = [u(1)
a , ˜u(2)(a, θ)], while ignoring the endpoint constraint,
u(2)(b, θ) = u(2)
b .
We propose to address the ﬁrst part of the problem by treating the known boundary
value, u(2)
b , of the second state, as a data point obtained from the model, with the following

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
49
likelihood:
u(2)
b
| f1:N, [u(1)
a , ˜u(2)(a, θ)], Ψn ∼N1
(
mn(b), Cn(b, b)
)
,
(4.5)
where the predictive variance of the probabilistic solution at the endpoint boundary, Cn(b, b),
is interpreted analogously to an endpoint mismatch tolerance in the numerical setting. The
prior measure on the unknown second initial state, u(2)(a, θ), depends on our knowledge
about the system3.
We can now write the posterior distribution of the states, based on a mesh of size N, as
follows:
[
u(·, θ)
 f1:N, [u(2)
b , u(1)
a ], ΨN
]
∝
[
u(2)
b
| f1:N, [u(1)
a , ˜u(2)(a, θ)], ΨN
] [
u(·, θ)
 f1:N, [u(1)
a , ˜u(2)(a, θ)], ΨN
]
[
f1:N
 [u(1)
a , ˜u(2)(a, θ)], ΨN
] [˜u(2)(a, θ)
]
.
(4.6)
In contrast to the case of IVPs, the MBVP probabilistic solution distributed according
to (4.6) will typically not be available analytically. However, any number of samples from
it may be obtained via Monte Carlo. In fact, we can easily evaluate the likelihood (4.5) of
the endpoint constraint of the ﬁrst state, and the prior distribution on the unknown second
initial state, while the second and third factors of the expression (4.6) can be obtained by
forward-simulation from Algorithm 1.
Algorithm 2 describes the basic Metropolis-Hastings procedure for sampling from the
probabilistic solution (4.6). An important consideration for solving MBVPs probabilistically
is to sample eﬃciently from possibly multimodal posteriors in the initial value. In fact,
MBVPs often have multiple solutions, whose number may not be known a priori. In such
cases, there may be several disjoint regions of the state space where functions obey the DE
model dynamics approximately. This multimodality in (4.5) translates into a multimodal
posterior probabilistic solution, as shown on the left side of Figure 4.3.
Therefore, we
recommend using a population MCMC scheme, such as parallel tempering (Geyer, 1991),
which can quickly identify and explore disjoint regions of high posterior probability.
3For example, if we know that the endpoint is equally likely to lie within a certain interval, we may use
a uniform prior on that interval. Otherwise, a prior distribution with unbounded support should be used.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
50
Algorithm
2
Draw
K
realizations
from
the
probabilistic
mixed
BVP
solution
p
(
u(t, θ), f1:N
 θ,
(
u(2)
b , u(1)
a
)
, ΨN
)
Initialize ˜u(2)(a, θ), u(t, θ), f1:N;
for iteration k = 1 : K do
Propose ˜u(2)⋆(a, θ) ∼q(˜u(2)⋆(a, θ)|˜u(2)(a, θ));
Conditionally simulate solution u⋆(t, θ) and vector ﬁeld realizations f⋆
1:N from the den-
sity p
(
u⋆(t, θ), f⋆
1:N
 θ,
(
u(1)
a , ˜u(2)⋆(a, θ)
)
, ΨN
)
via Algorithm 1;
Compute:
ρ
(˜u(2)(a, θ), u(t, θ), f1:N →˜u(2)⋆(a, θ), u⋆(t, θ), f⋆
1:N
)
= q(˜u(2)⋆(a, θ)|˜u(2)(a, θ))
q(˜u(2)(a, θ)|˜u(2)⋆(a, θ))
p(˜u(2)⋆(a, θ))
p(˜u(2)(a, θ))
p
(
u(2)
b
| f⋆
1:N, [u(1)
a , ˜u(2)⋆(a, θ)], ΨN
)
p
(
u(2)
b
| f1:N, [u(1)
a , ˜u(2)(a, θ)], ΨN
) ;
if min
{
1, ρ
}
> U[0, 1] then
Update ˜u(2)(a, θ) = ˜u(2)⋆(a, θ);
Update u(t, θ) = u⋆(t, θ);
Update f1:N = f⋆
1:N;
end if
Return u(t, θ), f1:N.
end for

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
51
Next, let us consider the related question of solving a two-point boundary value problem
(2.5) on the domain D = [a, b]. For expositional simplicity, we consider a boundary value
problem with one constraint located at each boundary of the domain for each of at most
M < P states,
(
u(i)(a, θ), u(i)(b, θ)
)
=
(
u(i)
a , u(i)
b
)
, i = 1, . . . , M4.
We again treat the known boundary values, u(1)
b , . . . , u(M)
b
, as data points obtained from
the model, assumed independent with likelihood,
u(1)
b
| f1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN ∼N1
(
m(1)
N (b), C(1)
N (b, b)
)
,
...
u(M)
b
| f1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN ∼N1
(
m(M)
N
(b), C(M)
N
(b, b)
)
,
The posterior distribution of the states, based on a mesh of size N, diﬀers from (4.6) and
has the form:
[
u(·, θ)
 f1:N, {u(i)
a , u(i)
b }i=1,...,M, ΨN
]
∝
[
u(2)
b
| f1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN
]
[
u(·, θ)
 f1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN
]
[
f1:N
 [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN
] [˜u(2)(a, θ)
]
.
(4.7)
Realizations from this posterior distribution can be obtained via Algorithm 3.
Probabilistic solution of the Lane-Emden mixed boundary value problem
We illustrate the probabilistic solution of a MBVP by solving the Lane-Emden model de-
scribed in the introduction. Our goal is to obtain a Monte Carlo estimate of the unknown
initial state u(a, θ) and the associated probabilistic solution for the system (2.7). As ex-
plained above, we treat the given boundary value ub as a single data sample, with Gaussian
observation error centered at the mean of probabilistic solution on t = b. The unknown ini-
tial state ˜u(a, θ) is assigned a Gaussian prior with a mean of 1.5 and a standard deviation
of 2
ub −va
. The probabilistic solver in this example uses a squared exponential covariance
4Note that the order of the states can be permuted without loss of generality.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
52
Algorithm
3 Draw K realizations from the probabilistic two-point BVP solution
p
(
u(t, θ), f1:N
 θ, {u(i)
a , u(i)
b }i=1,...,M, ΨN
)
Initialize {˜u(i)(a, θ)}i=M+1,...,P , u(t, θ), f1:N;
for iteration k = 1 : K do
Propose ˜u(i)⋆(a, θ) ∼q(˜u(i)⋆(a, θ)|˜u(i)(a, θ)), i = M + 1, . . . , P;
Conditionally simulate u⋆(t, θ) and vector ﬁeld realizations f⋆
1:N from the density
p
(
u⋆(t, θ), f ⋆
1:N
 θ, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN
)
via Algorithm 1;
Compute:
ρ
(
{˜u(i)(a, θ)}i=M+1,...,P , u(t, θ), f1:N →{˜u(i)⋆(a, θ)}i=M+1,...,P , u⋆(t, θ), f⋆
1:N
)
=
P
∏
i=M+1
q(˜u(i)⋆(a, θ)|˜u(i)(a, θ))
q(˜u(i)(a, θ)|˜u(i)⋆(a, θ))
p(˜u(i)⋆(a, θ))
p(˜u(i)(a, θ))
p
(
u(i)
b | f⋆
1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)⋆(a, θ), . . . , ˜u(P)⋆(a, θ)], ΨN
)
p
(
u(i)
b | f1:N, [u(1)
a , . . . , u(M)
a
, ˜u(M+1)(a, θ), . . . , ˜u(P)(a, θ)], ΨN
) ;
if min
{
1, ρ
}
> U[0, 1] then
Update ˜u(i)(a, θ) = ˜u(i)⋆(a, θ), i = M + 1, . . . , P;
Update u(t, θ) = u⋆(t, θ);
Update f1:N = f⋆
1:N;
end if
Return u(t, θ), f1:N.
end for

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
53
Figure 4.3: Sample of 100 probabilistic solution realizations for states u(·, θ) (left, above) and v(·, θ) (left,
below); the estimated marginal unnormalized log-density of the unknown initial state ua (right).
to model the solution on a grid of N = 100 equally-spaced solver knots. The length-scale
is set to twice the step size and the prior precision is 1 unit. The left side of Figure 4.3
shows a sample from the probabilistic solution and identiﬁes two high-density regions cor-
responding to distinct solutions. The right side of Figure 4.3 shows the estimated marginal
unnormalized log-density of the unknown initial state ua, highlighting two distinct regions
of the state space where solutions may exist.
4.3
Solving ODE delay initial function problems
In this section, we outline a probabilistic solution method for the ﬁxed delay initial function
problem (2.8). For notational clarity, we consider a delay initial function problem with a
single non-zero delay τ. We begin by partitioning the domain of integration, D = [a, b], into
I = ⌈(b −a)/τ⌉intervals of length τ and deﬁne the mesh,
si =
[
si
1, · · · , si
N
]
,
i = 0, . . . , I,

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
54
si
1 = a + τ(i −1), si
N = a + τi,
i = 0, . . . , I,
with s =
[
s1, · · · , sI]
. Again for notational simplicity, we use the same number, N, of mesh
points over each interval (this can be generalized straightforwardly to any number Ni > 2
of mesh points per interval). Any variable pertaining to the ith interval, where 1 ≤i ≤I,
will be represented with the superscript i.
In the case where the initial function ϕ : [a −τ, a] →RP in (2.8) is fully speciﬁed, we
may obtain the initial value, u(a) = ϕ(a), directly from the initial function. If the initial
function is known only at a ﬁnite set of nodes, s0
1:J, we ﬁrst ﬁt an interpolating Gaussian
process to the initial function evaluations,
ϕ | ϕ(s0
1), . . . , ϕ(s0
J), Ψ0
J ∼N
(
m0
J, C0
J
)
using auxiliary parameters Ψ0
J = [α0, λ0, R0, s0
1:J] (see for example, Rasmussen and Williams,
2006). We then take the initial state, u(a) = ˜ϕ(a), to be a realization of the estimated
initial function. The uncertainty in the initial function generates uncertainty in the initial
condition, which we encode in the state and derivative covariances by,
C1
0(a, a) = C0
N(a, a) + C0
N(a −τ, a −τ) + 2C0
N(a, a −τ)
(4.8)
Cf1
0 (a, a) = Cf0
N (a, a) + Cf0
N (a −τ, a −τ) + 2Cf0
N (a, a −τ).
(4.9)
Given n interval i derivative realizations,
f i
1:n =
[
fθ
(
si
1, ˜ui
0(si
1, θ), ˜ui−1
N (si
1 −τ, θ)
)
, · · · , fθ
(
si
n, ˜ui
n−1(si
n, θ), ˜ui−1
N (si
n −τ, θ)
)]T
the predictive distribution also depends on the states evaluated on the preceding interval
through the following probabilistic solution:
ui(·, θ)
 f i
1:n, ϕ, Ψi
n ∼N
(
mi
k, Ci
n
)
where, Ψi
n = [α, λ, R, si
1:n], and,
mi
n(t) = mi−1
N (si
1) + QR(t, si
1:n)
(
Λi
n + RR(si
1:n, si
1:n)
)−1(
f i
1:n −mf
0(si
1:n)
)
,
(4.10)
Ci
n(t, ˜t) = Ci−1
N (si
1, si
1) + QQ(t, ˜t) −QR(t, si
1:n)
(
Λi
n + RR(si
1:n, si
1:n)
)−1RQ(si
1:n, ˜t),
(4.11)

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
55
Cfi
n (t, ˜t) = Cf(i−1)
N
(si
1, si
1) + RR(t, ˜t) −RR(t, si
1:n)
(
Λi
n + RR(si
1:n, si
1:n)
)−1RR(si
1:n, ˜t),
(4.12)
Λi
n = diag
(
Cfi
0 (si
1, si
1), · · · , Cfi
n−1(si
n, si
n)
)
.
(4.13)
This process is repeated for each of the I = ⌈(b −a)/τ⌉intervals and one additional
update is performed over the entire domain of integration,
u(·, θ)
 f1:NI, ϕ, ΨNI ∼N
(
mNI, CNI
)
where ΨNI = [α, λ, R, s], fNI =
[
f1
1:N, . . . , f I
1:N
]T, and mNI, CNI are deﬁned in (3.6), (3.7).
in order to ensure continuity of the solution between intervals.
As before, the smoothness of the solution depends on the choice of the covariance struc-
ture. Therefore, we must account for second or higher derivative discontinuities by selecting
a derivative covariance function whose eigenbases span a space of diﬀerentiable functions
(e.g. the uniform kernel in Chapter 6 has this property).
Probabilistic solution of the oscillatory decay model with uncertain initial func-
tion
We illustrated in Chapter 2 that even in very simple cases, uncertainty in the initial function
can have a substantial eﬀect on the numerical solution. Now we illustrate our probabilistic
approach, which quantiﬁcation of both the uncertainty incurred in the discretization and
any additional uncertainty associated with the input.
The ﬁrst row of Figure 4.4 shows the analytical, numerical, and mean probabilistic
solutions to problem (2.9) with fully known history ϕ(t) = 1, t ∈[−1, 0]. The probabilistic
solution was obtained using a total of N = 500 equally spaced solver knots on I = 10
intervals [(i −1)τ, iτ], i = 1, . . . , 10.
Let us consider a modiﬁed version of problem (2.9) with history ϕ(t) = sin(4πt)/4+1 that
is only partially speciﬁed, without error, over a set of six nodes. The second row of Figure
4.4 illustrates this case. The ﬁrst step to solving such a problem approximately is to infer
the initial function over the entire initial interval. For this we use an interpolating Gaussian
process with square exponential covariance structure.
The uncertainty expressed by its
posterior distribution given the six nodes is accounted for in the probabilistic algorithm.
Although in this case the numerical and probabilistic mean solutions give similar results, the

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
56
Algorithm 4 Sample a probabilistic DDE solution from p
(
u(t, θ), f1:NI
θ, ϕ, ΨNI
)
Initialize C1
0(s1
1, s1
1), Cf1
0 (s1
1, s1
1) from (4.8-4.9), and set,
u0(s1
1:N −τ, θ) = ˜ϕ(s1
1:N −τ)
f 1
1 = fθ
(
s1
1, ˜ϕ(s1
1), ˜ϕ(s1
1 −τ)
)
for interval I = 1 : I do
Conditionally simulate a probabilistic solution realization, ui(si+1 −τ, θ), and deriva-
tive realizations, fi
1:N, from the density p
(
ui(·, θ), fi
1:N
 θ, ϕ, Ψi
N
)
via Algorithm 1 with
means and covariances deﬁned in (4.10 - 4.13);
end for
for state p = 1 : P do
Sample a realization of the solution, u(p)(t), for state p from the conditional distribution
at chosen points t ∈[a, b]T ,
u(p)(t, θ) ∼p
(
u(p)(t, θ)
θ, f(p)
1:NI, ϕ, ΨNI
)
= N
(
mNI(t), CNI(t, t)
)
;
end for
Return u(t, θ), f1:NI.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
57
Figure 4.4: Mean probabilistic solution (blue dashed line) constructed using 500 solver knots compared
with numerical (red dashed line) and analytical (black solid line) solutions for the system (2.9) with initial
function ϕ(t) = 1, t ∈[−1, 0] fully speciﬁed (top), and ϕ(t) = sin(4πt)/4 + 1 (below) fully speciﬁed (black)
and estimated (dotted red line) from six nodal points (red circles). The grey bands show ± 100 standard
deviations around the mean probabilistic solution for exposition.
probabilistic solution provides information about how the additional functional uncertainty
propagates through the estimated solution.
4.4
Solving PDE boundary value problems
In this section, we describe how to formulate probabilistic solutions to general PDE bound-
ary value problems in two ways. The ﬁrst, indirect method, models uncertainty along the
temporal dimension of the domain using the probabilistic solution developed for ODE initial
and boundary value problems, while the spatial discretization is achieved through spectral
projection techniques. The second, direct method, is analogous to the ﬁnite-diﬀerencing ap-
proach and follows from the model developed in Section 3.2.2 for multivariate solutions, to
produce a technique analogous to the numerical ﬁnite-diﬀerencing method, which is however
limited for high dimensional domains.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
58
4.4.1
Indirect probabilistic solution method
In Chapter 2 we introduced two nonlinear PDEs exhibiting temporal chaos, where interest
lay in understanding the impact of temporal discretization on the system. Modeling the
eﬀect of discretization over the time domain can be achieved by using the framework de-
veloped in Section 4.1 for ODE initial value problems. The PDE boundary value problem
is ﬁrst discretized over the spatial domain and mapped to a ﬁnite-dimensional subspace of
the solution space via spectral projection, yielding a system of coupled ODEs with initial
constraints. We can then use Algorithm 1 to solve the system and reconstruct the estimated
PDE solution.
Probabilistic solution of the Kuramoto-Sivashinsky nonlinear PDE
Following Kassam and Trefethen (2005), we considered the spatio-temporal domain D =
[0, 32π] × [0, 150] with initial function uB(x) = cos (x/16)
{
1 + sin (x/16)
}
, and discretized
the spatial domain via a Fourier spectral method, obtaining a high-dimensional (128 dimen-
sions) system of stiﬀODEs. The exponential time-diﬀerencing transformation was used to
solve the linear part of each ODE exactly, while eﬀectively dealing with the transformed
nonlinear part via the probabilistic solver. The inverse transformation was then applied
within each solver iteration. The probabilistic IVP solution was obtained via Algorithm 1
using N = 2000 equally-spaced mesh points. The squared exponential covariance was used
with a length-scale of twice the step size and prior precision of 100 units.
Figure 4.5 shows ﬁfteen realizations of the probabilistic posterior solution for the KS non-
linear PDE boundary value problem (2.11). The divergence between the solutions starting
approximately around the middle of the temporal domain, illustrates the eﬀect of temporal
discretization error propagation on the solution of this stiﬀsystem.
Probabilistic solution of the Navier-Stokes equations on a torus
The Navier-Stokes model on a two-dimensional torus (2.12) was reduced to a set of 64 × 64
coupled ODEs with associated constraints through a pseudo-spectral projection in Fourier
space. Once again, we used exponential time-diﬀerencing to solve the linear part of each
ODE, alleviating problems with stiﬀness. The resulting probabilistic solution for this two-
dimensional PDE in two spatial and one time dimensions is diﬃcult to visualize. Figures

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
59
Figure 4.5:
A sample of 100 realizations of the probabilistic solution of the Kuramoto-
Sivashinsky PDE using a ﬁxed initial function. The spatial and temporal dimensions are
shown on the horizontal and vertical axes respectively. The solution is known to exhibit tem-
poral chaos, as evidenced by the variety of dynamics observed due to uncertainty introduced
in its estimation.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
60
Figure 4.6:
Vorticities for a sample of 6 realizations of the probabilistic solution of the
Navier-Stokes equation on a two-dimensional torus at time t = 30 units, under a ﬁxed
initial ﬁeld. Vorticity of the 6 realizations are very similar at this stage.
4.6 and 4.7, show vorticity, a function of the two components of velocity, on a torus param-
eterized in spherical coordinates by the angle of the inner ring, θ, on the horizontal axis and
the angle of the cross-section of the ring, ρ, on the vertical axis. The probabilistic solution
was computed using an equally spaced temporal discretization grid on the interval [0, 100]
with N = 1000 time steps. The length-scale was chosen to be twice the length of a time
step, and the prior precision was set to the low value α = 0.5 to reﬂect the fact that, under
our choice of the viscosity parameter, the system lies in the turbulent regime.
Figure 4.6 corresponds to six realizations of the probabilistic solution for this system at
time t = 30 units, where no diﬀerence between the samples can be detected. Figure 4.7
shows the same six realizations at time t = 100 units, where diﬀerences in vorticity can now
clearly be seen. These diﬀerences are a result of the accumulation of discretization errors
along the time domain and the chaotic nature of the system dynamics.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
61
Figure 4.7:
Vorticities for a sample of 6 realizations of the probabilistic solution of the
Navier-Stokes equation on a two-dimensional torus at time t = 100 units, under a ﬁxed
initial ﬁeld. Vorticity of the 6 realizations have begun to diverge from one another as a
result of discretization error accumulation along the temporal domain.

CHAPTER 4. PROBABILISTIC SOLUTION OF DIFFERENTIAL EQUATIONS
62
4.4.2
Direct probabilistic solution method
If instead interest lies in quantifying discretization uncertainty along more than one dimen-
sion, we recommend constructing a spatio-temporal mesh and using a variant of Algorithm
1 based on the multivariate solution model presented in Section 3.2.2. An important con-
sideration when solving PDEs in this way is how to enforce the spatio-temporal boundary
condition, which is now a function instead of a single point. This turns out to be relatively
straightforward once the domain is parameterized in a convenient way. In each dimension,
the boundary condition may need to be enforced (i) at the lower bound of the domain, or
(ii) at both endpoints, often within the same problem. In modelling dimensions of the type
(i) any standard kernel function will automatically satisfy the boundary value at the lower
limit of integration. In modelling dimensions of the type (ii) we recommend an asymmet-
rical kernel that takes on both positive and negative value and integrates to zero (such as
the diagonal kernel constructed in Chapter 6), which ensures that the estimated solution
in a particular dimension satisﬁes the model at both endpoints. As shown in Section 3.2.2
we may combine diﬀerent kernels in each input dimension, which will allow us to enforce
mixed boundary constraints or work with functions with diﬀerent smoothness in diﬀerent
dimensions.
Finally, we note that this direct solution approach can be considered analogous to a
ﬁnite-diﬀerence numerical method, and as such incurs the same problems related to dimen-
sionality. Indeed, the number of vertices in the spatio-temporal mesh would need to increase
exponentially in the number of dimensions to yield a reasonable solution as the number of
dimensions increases.

Chapter 5
Incorporating Model Uncertainty
in the Inverse Problem
This chapter examines the eﬀect of model uncertainty on the statistical inverse problem of
recovering information in the form of a probability distribution for variables1 given data the
y. In this chapter we consider the case where the data generating mechanism is governed by
a diﬀerential equation model deﬁned by unknown parameters θ ∈Θ. When the DE solution
is known in closed form, the inverse problem can be approached via nonlinear regression.
If the closed form solution is unavailable, we propose to incorporate model error resulting
from discretization into the inverse problem by augmenting the unknown parameters by the
solution u and related auxiliary variables.
The following sections show how our probabilistic model of discretization uncertainty
for an unknown DE solution naturally ﬁts into the Bayesian framework for solving inverse
problems. We contrast our exact fully probabilistic approach with the conventional approx-
imate inference method based on numerical DE solutions.
Finally, we demonstrate our
methodology on a delay initial function model of protein dynamics.
1Often in applications, the dimension of the unknown is greater than that of the observations y. Such un-
derdetermined problems arise when we try to recover an inﬁnite-dimensional function from ﬁnite-dimensional
observations, as in the problem of inference on intractable diﬀerential equation models. This situation re-
quires restricting attention to a ﬁnite-dimensional subspace, which is accomplished in the Bayesian approach
by the choice of prior measure.
63

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
64
5.1
Statistical inverse problem
Statistical inversion is the problem of inferring ξ from data y ∈Hobs, both elements of
Hilbert spaces. The error model for the data depends on the often nonlinear operator G
and a zero-mean stochastic process ϵ ∈Hobs that is independent of ξ. For simplicity, we
consider the additive error model,
y = G(ξ) + ϵ.
(5.1)
5.1.1
Inference for diﬀerential equation models
Consider the problem of inference on parameters θ ∈Θ deﬁning a DE model given R-
dimensional measurements y(t) ∈MT,R(
R
)
taken at each of T time points, t ∈DT , and
observed indirectly through the possibly nonlinear observation function G : MT,P (
R
)
→
MT,R(
R
)
. Although this framework can be applied to general error models, for simplicity
we shall hereafter consider the additive Gaussian model with error covariance Σ(r) ∈MT (
R
)
and R independent observation states:
y(r)(t) = G(r)(
u(t, θ)
)
+ ϵ(r)(t),
ϵ(r)(t) ∼NT (0, Σ(r)),
1 ≤r ≤R.
For Σ = diag
(
Σ(1), . . . , Σ(R))
, the data likelihood follows as,
Ly(t)
(
G ◦u(t, θ)
)
= NT×R
(
G ◦u(t, θ), Σ
)
.
Therefore in (5.1), ξ typically represents any unknown model parameters, θ, indexing
the DE:
ξ ≡θ ∈Θ,
transformed via the nonlinear observation operator,
G ≡G ◦u(t, ·) : Θ →Hobs
which maps the parameters from Θ to the observation space Hobs. When the true or exact
solution, u : D × Θ →RP , is known in closed form, then G is known and the parameters θ
can typically be inferred via nonlinear regression.

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
65
When the exact solution is unknown, the observation operator G ◦u(t, ·) becomes un-
certain, as only G is available.
Therefore we propose to restate the inverse problem by
considering the posterior probability distribution over the parameters θ as well as the sys-
tem states u, by augmenting the ξ as follows:
ξ ≡[θ, u(·, θ), uB, f1:N, ΨN] ∈Θ × H ≡H,
and rewriting the observation operator in terms of the observation transformation G only,
G ≡G : H →Hobs.
As a result, inference can now proceed via nonlinear regression as before.
5.1.2
Bayesian approach
In this section we will consider the standard Bayesian framework for statistical inverse
problems, which consists of obtaining a posterior measure for the unknown ξ ∈H, where
H is a general Hilbert space, and will be referred to as inference on ξ. Informally, this
approach proceeds by applying Bayes’ rule:
ξ | y ∼dνy
dν0
(ξ) =
Ly
(
G(ξ)
)
∫
H Ly
(
G(ξ)
)
dν0(ξ) ∝Ly
(
G(ξ)
)
,
(5.2)
where νy is the posterior measure of ξ conditioned on the data, and where dνy/dν0 denotes
its Radon-Nikodym derivative with respect to the prior measure ν0.
As in Chapter 3,
such an informal derivation is not typically suﬃcient to verify that the Radon-Nikodym
derivative exists, particularly in the inﬁnite-dimensional setting.
The following theorem
(see, for example, Stuart, 2010) sets out the conditions when this is the case.
Theorem 5.1.1. Consider the error model (5.1). Suppose that Ly is deﬁned on RR, and
that ν0(H) = 1.
If the observation operator, G : H →RR, is continuous, then ξ | y is
distributed according to νy, which is absolutely continuous with respect ν0, and has Radon-
Nikodym derivative (5.2).
From now on, we will work with distributions which are absolutely continuous with
respect to the Lebesgue measure, and will therefore adopt a simpler notation by writing the

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
66
posterior density (5.2) as,
p(ξ | y) =
Ly
(
G(ξ)
)
p(ξ)
∫
H Ly
(
G(ξ)
)
p(ξ)dξ ∝Ly
(
G(ξ)
)
p(ξ).
5.1.3
Approximate inference under an unknown DE solution
When the DE solution is known, the likelihood Ly(t)
(
G ◦u(t, θ)
)
can be evaulated, and thus
functionals of the posterior distribution (5.2) may be obtained through standard techniques
(see, for example, Gelman et al., 2004).
In practice, however, the likelihood cannot be
evaluated due to lack of a closed form solution for most nonlinear DE models (see, for
example, discussion in Ramsay et al., 2007). Therefore, the conventional approach replaces
the unknown analytical solution u(t, θ) by its ﬁnite-dimensional numerical approximation
uN(t, θ) obtained via numerical integration of the system equations over an N-dimensional
spatio-temporal mesh, to obtain the likelihood approximation,
Ly(t)
(
G ◦u(t, θ)
)
≈Ly(t)
(
G ◦uN(t, θ)
)
.
Under the conditions of Theorem 5.1.1, this approach yields the approximate posterior
density,
pN(
θ | y(t)
)
∝
Ly(t)
(
G ◦uN(t, θ)
)
|
{z
}
approximate likelihood of the data
×
p(θ).
| {z }
prior density
(5.3)
The approximate observation operator GN ≡G ◦uN can diﬀer substantially from G ≡
G ◦u in practice, leading to estimation bias, as illustrated in the extreme cases of chaotic
and ill-conditioned systems. Importantly, any systematic uncertainty associated with this
approximation is ignored under the conventional framework. Our goal is to show that an
alternative, exact model is not only feasible, but in many cases necessary.
5.2
A fully probabilistic approach
We propose to incorporate our probability model of discretization uncertainty into the in-
verse problem by introducing one additional layer in the posterior hierarchy, deﬁned by the
joint density p
(
u(t, θ), f1:N | θ, uB, ΨN
)
over solutions u(t, θ) and derivative realizations

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
67
f1:N. This yields a fully probabilistic alternative to the approximate posterior density (5.3),
by characterizing the uncertainty in the unknown system solution, u(t, θ).
5.2.1
Exact posterior density
First, deﬁne a prior measure ν0 for the unknown vector ξ = [u(·, θ), uB, f1:N, ΨN, θ] ∈H
in such a way that ν0(H) = 1. Intuitively this condition guarantees that the prior assigns
positive probability to any function that satisﬁes the regularity conditions on the state space
of possible solutions. If the observation operator G : MT,P (
R
)
→MT,R(
R
)
is continuous,
then ξ | y is distributed according to the measure νy that is absolutely continuous with
respect ν0, by Theorem 5.1.1. Evaluating the solution at time points t ∈DT ensures that
the posterior distribution of the probabilistic solution is continuous with respect to the
Lebesgue measure, with density (3.15). Thus, the posterior density of the solution and the
model and auxiliary parameters is,
p (θ, u(t, θ), uB, f1:N, ΨN | y(t))
∝
Ly(t)
(
G ◦u(t, θ)
)
|
{z
}
likelihood of the data
× p
(
u(t, θ) | θ, uB, f1:N, ΨN
)
|
{z
}
probabilistic solution
× p
(
θ, uB, f1:N, ΨN
)
|
{z
}
prior density
.
(5.4)
Marginalizing over the auxiliary variables, derivative realizations, and the solution, yields
an exact posterior over model parameters, which explicitly takes into account the mismatch
between the true solution and a ﬁnite-dimensional approximation.
5.3
Sampling from the posterior distribution over model pa-
rameters
Posterior functionals of (5.4) will not usually be analytically tractable, but can be obtained
via Monte Carlo (e.g., Gelman et al., 2004). This requires a forward-simulation layer to
address the fact that the density (5.4) is not available analytically for most DE models,
due to the sequential nonlinear vector ﬁeld transformation required to generate derivative
realizations given θ.
We propose a specialized Monte Carlo procedure to generate a sample from the fully
probabilistic posterior (5.4), which allows estimation of the model parameters and solution

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
68
while taking into account solver uncertainty. For each proposed value of θ, Algorithm 5
applies the observation transformation G to each probabilistic solution realization generated
given initial conditions uB, and auxiliary parameters ΨN, avoiding explicitly calculating
the intractable density of the observation states within the Metropolis-Hastings acceptance
ratio.
For clarity of exposition, Algorithm 5 targets the posterior distribution of model
parameters θ given boundary conditions uB, auxiliary parameters ΨN, and error covariances
Σ. However, the algorithm extends simply to accommodate sampling when uB, ΨN, and
Σ are also unknown.
Algorithm 5 Draw K samples from p
(
θ, u(t, θ), f1:N | uB, ΨN, y(t)
)
Initialize θ;
for k = 1 : K do
Propose θ⋆∼q(θ⋆| θ);
Conditionally simulate vector ﬁeld realizations, f ⋆
1:N, and associated solution realization,
u⋆(t, θ⋆), from p
(
u(t, θ⋆), f1:N
θ⋆, uB, Ψ1:N
)
using one of Algorithms 1 through 4;
Compute:
ρ
(
θ, u(t, θ), f1:N →θ⋆, u⋆(t, θ⋆), f⋆
1:N
)
= q(θ⋆| θ, )
q(θ | θ⋆)
p(θ⋆)
p(θ)
p
(
y(t)|G
(
u⋆(t, θ⋆)
)
, Σ
)
p
(
y(t)|G
(
u(t, θ)
)
, Σ
) ;
if min{1, ρ} > U[0, 1] then
Update θ = θ⋆;
Update u(t, θ) = u⋆(t, θ⋆);
Update f1:N = f⋆
1:N;
end if
Return θ, u(t, θ), f1:N.
end for

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
69
5.3.1
Inference for the JAK-STAT protein network model
We demonstrate fully probabilistic state and parameter inference for a 4-state ODE delay
initial function problem describing the dynamics of the JAK-STAT cellular signal trans-
duction pathway (Raue et al., 2009).
Features of this model which pose a challenge to
conventional methods of parameter estimation include the presence of systematic model
uncertainty inherent in numerical DIFP solutions, dependence of the model on an uncer-
tain discretely speciﬁed input function, and possible model misspeciﬁcation (Campbell and
Chkrebtii, 2013).
This mechanism describes a series of reversible biochemical reactions of STAT-5 tran-
scription factors in response to binding of Erythropoietin (Epo) hormone to cell surface
receptors, Pellegrini and Dusanter-Fourt (1997). After gene activation occurs within the
nucleus, the transcription factors revert to their initial state, returning to the cytoplasm to
be used in the next activation cycle. This last stage is not well understood and is proxied
in the model by the time delay τ. The model for this mechanism describes changes in 4
reaction states of STAT-5 through the DIFP,

























˙u(1)(t, θ)
= −θ1 u(1)(t, θ) EpoRA(t) + 2 θ4 u(4)(t −τ, θ),
˙u(2)(t, θ)
=
θ1 u(1)(t, θ) EpoRA(t) −θ2 u(2)2(t, θ),
˙u(3)(t, θ)
= −θ3 u(3)(t, θ) + 0.5 θ2 u(2)2(t, θ),
˙u(4)(t, θ)
=
θ3 u(3)(t, θ) −θ4 u(4)(t −τ, θ),
(5.5)
on t ∈[0, 60] time units, with known initial functions u(2)(t, θ) = u(3)(t, θ) = u(4)(t, θ) = 0
on t ∈[−τ, 0]. Constant initial function u(1)(t, θ) on t ∈[−τ, 0], the time delay τ, and the
reaction rates θ1, . . . , θ4 are unknown variables parameterizing this model.
The states cannot be measured directly, but are observed through the nonlinear trans-
formation G : R3 × Θ →R4
G1(u, θ) = θ5
(
u(2) + 2u(3))
G2(u, θ) = θ6
(
u(1) + u(2) + 2u(3))

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
70
Figure 5.1:
Experimental data (red circles) and sample paths (lines) of the observation pro-
cesses. They are obtained by transforming a sample from the marginal posterior distribution
of the states by the observation function (5.6).
G3(u, θ) = u(1)
G4(u, θ) = u(3)/
(
u(2) + u(3))
,
(5.6)
which is expressed in terms of unknown scaling factors θ5 and θ6. As per Raue et al. (2009),
observations y(t) = G
(
u(t, θ), θ
)
+ ϵ(t) are assumed contaminated with additive zero-mean
Gaussian noise with experimentally determined standard deviations.
Our analysis is based on experimental data from Swameye et al. (2003), which consists
of 16 observations (illustrated by red circles in Figure 5.1) for the ﬁrst two states of the
observation process. Raue et al. (2009) further utilize an additional artiﬁcial data point
for each of the third and fourth observation process states to deal with lack of parameter
identiﬁability for this system. This assumption is also adopted in our analysis.
Prior distributions were deﬁned on the unknown parameters as follows: θi ∼Exp (1), i =
1, . . . , 6, τ ∼χ2
6, and u(1)(0) ∼N1
(
y(3)(0), 402)
. We estimated functionals of the posterior
distribution from a Monte Carlo sample of model parameters, θ = [θ1, . . . , θ6, τ, u(1)(0)],
solution states, u(t, θ), and auxiliary variables, ΨN. In order to construct a Markov chain
that would eﬃciently traverse the parameter space of this multimodal posterior distribution,
we chose to use a parallel tempering sampler (Geyer, 1991) with 7 parallel chains along a
uniformly spaced temperature proﬁle over the interval [0.4, 1]. Parameters were sampled in
three blocks. The ﬁrst block consists of the four rate parameters, two scaling factors, the
delay parameter and the initial value of the ﬁrst state. The second and third blocks consist
of the precisions and length-scales respectively, with priors αi + 100 ∼Log-N1 (10, 1) and
λi ∼χ2
1, i = 1, . . . , 4. Each probabilistic DIFP solution was generated using Algorithm 4

CHAPTER 5. MODEL UNCERTAINTY IN THE INVERSE PROBLEM
71
Figure 5.2:
Marginal fully probabilistic posterior distribution in the model parameters
based on a sample of size 100,000 generated by a parallel tempering algorithm utilizing
seven chains, with the ﬁrst 10,000 samples removed. Prior densities are shown in red.
using an equally spaced grid of size N = 300. The input function was estimated using a
linear interpolator of the EpoRA experimental data, following Swameye et al. (2003).
We obtained 100,000 posterior samples and removed the ﬁrst 10,000 samples as burn-
in. Convergence was monitored by examining the marginal empirical distributions visually,
while ensuring that the acceptance rate fell roughly within the accepted range of 18%–28%
for each of the three parameter blocks, and that the total swap rate between any two chains
remained roughly within 5%-15%.
Marginal posterior samples of the observation states G
(
u(·, θ), θ
)
are shown in Figure
5.1, while marginal posterior samples of model parameters are illustrated using correlation
plots in Figure 5.2. All rate and scaling parameters are found to be identiﬁable with the
exception of θ2, as noted also in the identiﬁability analysis of Raue et al. (2009). Scaling
factors are correlated, positively with each other and inversely with the constant initial state
u(1)(0). Auxiliary parameters for the fully probabilistic model are found, for the most part,
to be non-identiﬁable. Indeed the transformed probabilistic solution for this system does
not appear to be overly sensitive to changes in the auxiliary parameters within a reasonable
range.

Chapter 6
Choice of Covariance Structure
The covariance function describes how two elements of a space are related to one another.
In the model for an unknown DE solution presented in Chapter 3, the prior covariance
determines the regularity of the space of possible solutions, and thus its speciﬁcation is
an important consideration. In this chapter we discuss the construction of the covariance
operator for the unknown solution and its derivative, examine its properties, and provide
three examples of covariance structures that are used throughout this thesis.
6.1
Role and properties of prior covariances
In Chapter 3 we deﬁned the following prior measures for the unknown DE solution state
and its derivative:
µ0 = N(m0, C0),
µf
0 = N(mf
0, Cf
0 ).
Covariance operators Cf
0 = RR(t, ˜t) and C0 = QQ(t, ˜t) follow a user-speciﬁed structure
governed by the kernel function Rλ and its integrated version Qλ.
6.1.1
Positive-deﬁniteness
We have seen in Theorem 3.3.1 that a necessary condition for a well-deﬁned probabilistic
solution derivative (3.14) is that the cross-covariance operators, (3.18), between the deriva-
tive and n noisy realizations be positive deﬁnite. Let us show that this condition is satisﬁed
72

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
73
under the above prior covariance speciﬁcation.
Recall that α denotes the prior precision deﬁned in Section 3.2.1. Consider a kernel
Rλ ∈L2(D2), such that the integral transform Rg of any nonzero function g ∈L2(D) is not
everywhere zero (this condition is simple to verify for a given kernel), then,
⟨g, RKR∗g⟩= α−1 ∫∫
Rλ(t, z) g (t) dt
∫
Rλ(˜t, z) g
(˜t
)
d˜tdz
=











α−1 ∫( ∫
Rλ(t, z)g (t) dt
)2dz = 0
if g(t) = 0 ∀t ∈D
α−1 ∫( ∫
Rλ(t, z)g (t) dt
)2dz > 0
otherwise.
Therefore, the cross-covariance operators C11, C12, C21, deﬁned in (3.18), are positive deﬁnite.
Next, we consider the covariance operator C22, deﬁned in (3.18). For any vector g ∈Rn and
positive semideﬁnite matrix ΛN, we have,
⟨g,
(
RKR∗+ Λn
)
g⟩=
n
∑
i=1
n
∑
j=1
(
α−1 ∫
R Rλ(ti, z)Rλ(tj, z)dz + Λn(ti, tj)
)
g (ti) g (tj)
=











⟨g, RKR∗g⟩+ ⟨g, Λn g⟩= 0
if g(t) = 0 ∀t ∈D,
⟨g, RKR∗g⟩+ ⟨g, Λn g⟩> 0
otherwise.
Therefore C22 is also positive deﬁnite. This argument simply extends to the case of the
integrated covariances used for modelling the solution states.
6.1.2
Regularity
Theorem 4.1.3 requires that the mean-centered derivative of the true solution must be an
element of the space H spanned by the eigenfunctions of the covariance operator Cf
0 . We give
an intuitive justiﬁcation for this requirement by showing that the prior Gaussian measure
assigns positive probability to any function that is suitably regular, i.e. µf
0(H) = 1. The
following theorem shows how to obtain realizations of a Gaussian process on a Hilbert space,
and relates the smoothness of the realizations to the covariance structure.
Theorem 6.1.1 (Karhunen–Lo`eve theorem). Let C be a self-adjoint, positive semi-deﬁnite,

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
74
trace class1 operator in a Hilbert space H with an orthonormal set of eigenfunctions and as-
sociated eigenvalues, {ϕk, γk}∞
k=1, ordered so that γ1 ≥γ2 ≥· · · . Let ℓ∈H and take {zk}∞
k=1
to be a sequence of iid standard normal random variables. Then, the random function,
u = ℓ+
∞
∑
k=1
zk
√γkϕk
has distribution µ = N(ℓ, C).
From this theorem, follows a result (e.g. Stuart, 2010, pp. 539-540) which implies that
samples from µf
0 are almost surely in L2(D) under some relatively mild conditions on the
covariance.
6.1.3
Role of kernel functions
Covariances for the unknown solution are constructed using the kernel Rλ and its integrated
version Qλ.
Therefore, the choice of the kernel function should reﬂect our assumptions
about the true unknown DE solution, such as smoothness and any ﬁxed zero points. In
general, imposing unrealistically strict smoothness assumptions on H by choice of covariance
operator may introduce estimation bias if the true solution derivative is not an element of
this space. Therefore, in cases where the solution smoothness is not known with certainty,
we recommend to err on the side of less regular kernels which alleviate possible bias by
allowing for the possibility of functions less smooth than the solution, at the cost of slower
rate of convergence of the estimated solution to the true unknown solution.
6.2
Some useful covariances
The illustrative examples in this thesis employ one of three types of covariance structure con-
structed by the choice of kernel. In the following section, we provide closed form expressions
of pairwise convolutions corresponding to each type of kernel.
1compact operator with ﬁnite trace that is independent of the choice of basis.

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
75
6.2.1
Squared exponential covariance
The squared exponential isotropic covariance function,
Cf
0 (t, ˜t) = √π α λ exp
(
−1
4
(
t−˜t
λ
)2 )
,
(6.1)
is popular in the Gaussian process literature (Rasmussen and Williams, 2006) for modelling
analytic functions. Here, we will focus on the case that D = [a, b]. We will show that this
covariance is obtained by pairwise convolution of the Gaussian kernel,
Rλ(t, ˜t) = exp
(
−1
2
(
t−˜t
λ
)2 )
,
and the integrated covariance is obtained by the pairwise convolution of the integrated
kernel,
Qλ(t, ˜t) = 1
2 erf
( 1
√
2
(
t−˜t
λ
)2 )
−1
2 erf
( 1
√
2
(
a−˜t
λ
)2 )
.
The integrated kernel produces a non-stationary state covariance operator, C0, which allows
us to constrain the solution state to zero at the initial boundary a of the domain.
Let us derive closed form expressions for the pairwise convolutions of the Gaussian
kernel and its integrated version, which are required for updating the probabilistic solution
as described in Chapter 4.
αRR(t, ˜t)
= √π λ exp
(
−1
4
(
t−˜t
λ
)2 )
,
t, ˜t ∈[a, b]
αQR(t, ˜t)
= π λ2 erf
( 1
2λ(t −˜t)
)
+ π λ2 erf
( 1
2λ(˜t −a)
)
,
t, ˜t ∈[a, b]
αQQ(t, ˜t)
= π λ2(t −a) erf
( 1
2
( t−a
λ
) )
+ 2√π λ3 exp
(
−1
4
( t−a
λ
)2 )
−πλ2(˜t −t) erf
( 1
2
(
˜t−t
λ
) )
−2√π λ3 exp
(
−1
4
(
˜t−t
λ
)2 )
+π λ2(˜t −a) erf
( 1
2
(
˜t−a
λ
) )
+ 2√π λ3 exp
(
−1
4
(
˜t−a
λ
)2 )
−2√π λ3,
t, ˜t ∈[a, b]

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
76
Importantly, the result RR(t, t), QQ(t, t) < ∞for all t ∈R implies that the kernel and its
integrated version are square integrable. Also note that RR(t, ˜t) has the form of the squared
exponential covariance (6.1). Contour plots of the kernel, its integrated version, and the
state and derivative prior covariances are shown in Figure 6.1 on the domain D = [0, 1] with
prior precision α = 1 and length-scale λ = 0.1.
The covariance operator, Cf
0 , is inﬁnitely diﬀerentiable, so that the Gaussian process
model for the derivative is mean-square diﬀerentiable. As such, this covariance is suited to
modelling solution derivatives that are known a priori to be very smooth. In this thesis,
we have used the squared exponential covariance for modelling the probabilistic solution
of the Lorenz system, the Kuramoto-Sivashinsky initial boundary function problem, the
Lane-Emden mixed boundary value problem, and the Navier-Stokes equations.
6.2.2
Uniform covariance
In some cases, one may wish to model the derivative by a function with ﬁrst or higher
derivative discontinuities. For this purpose we propose the following uniform kernel function
and its integrated version:
Rλ(t, ˜t) = I
(˜t ∈(t −λ, t + λ)
)
,
Qλ(t, ˜t) = I
(˜t ∈(a + λ, t −λ)
)
+
1
2
( t−a
λ
)
I
(˜t ∈(t −λ, a + λ)
)
+ 1
2
( ˜t+λ−a
λ
)
I
(˜t ∈(a −λ, min(a + λ, t −λ))
)
+ 1
2
( t−˜t+λ
λ
)
I
(˜t ∈(max(a + λ, t −λ), t + λ)
)
.
Next, we derive the closed form expressions for the pairwise convolutions for the uniform
kernel.
αRR(t, ˜t) =
(
min(t, ˜t) −max(t, ˜t) + 2λ
)
I
(
min(t, ˜t) −max(t, ˜t) > −2λ
)
,
t, ˜t ∈[a, b],
αQR(t, ˜t) = 2λx I
(
min(t −λ, ˜t + λ) > max(a + λ, ˜t −λ)
)min(t−λ,˜t+λ)
max(a+λ,˜t−λ)
+
( 1
2x2 + (λ −a)x
)
I
(
min(a + λ, t −λ, ˜t + λ) > ˜t −λ
)min(a+λ,t−λ,˜t+λ)
˜t−λ
+
(
(t + λ)x −1
2x2)
I
(
min(t, ˜t) + λ > max(a + λ, t −λ, ˜t −λ)
)min(t,˜t)+λ
max(a+λ,t−λ,˜t−λ)
+(t −a)
(
max(t, ˜t) −a −2λ
)
I
(
a −max(t, ˜t) > −2λ
)
,
t, ˜t ∈[a, b],

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
77
Figure 6.1:
Top row: contour plots of Gaussian kernel (left), and its integrated version
(right). Bottom row: contour plots of squared exponential derivative covariance (left) and
associated state covariance (right). Functions are evaluated over t, ˜t ∈[0, 1], with (α, λ) =
(1, 0.1).

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
78
αQQ(t, ˜t) = 4λ2 (
min(t, ˜t) −a −2λ
)
I
(
min(t, ˜t) > a + 2λ
)
+2λ
(
(˜t + λ)x −1
2x2)
I
(
min(t −λ, ˜t + λ) > max(a + λ, ˜t −λ)
)min(t−λ,˜t+λ)
max(a+λ,˜t−λ)
+
( 1
3x3 + (λ −a)x2 + (λ −a)2x
)
I
(
min(a + λ, t −λ, ˜t −λ) > a −λ
)min(a+λ,t−λ,˜t−λ)
a−λ
+(˜t −a)
( 1
2x2 + (λ −a)x
)
I
(
min(a + λ, t −λ) > ˜t −λ
)min(a+λ,t−λ)
˜t−λ
+2λ
(
(t + λ)x −1
2x2)
I
(
min(t + λ, ˜t −λ) > max(a + λ, t −λ)
)min(t+λ,˜t−λ)
max(a+λ,t−λ)
+
(
(t + λ)(˜t + λ)x −1
2(t + ˜t + 2λ)x2 + 1
3x3)
I
(
min(t, ˜t) > max(a, t −2λ, ˜t −2λ)
)min(t,˜t)+λ
max(a+λ,t−λ,˜t−λ)
+(t −a)
( 1
2x2 + (λ −a)x
)
I
(
min(a + λ, ˜t −λ) > t −λ
)min(a+λ,˜t−λ)
t−λ
+(t −a)(˜t −a)
(
a + 2λ −max(t, ˜t)
)
I
(
a + 2λ > max(t, ˜t)
)
,
t, ˜t ∈[a, b].
The result RR(t, t), QQ(t, t) < ∞implies that Rλ and Qλ are square integrable. Contour
plots of the resulting covariance functions Cf
0 (t, ˜t) = RR(t, ˜t) and C0(t, ˜t) = QQ(t, ˜t), as well
as kernels Rλ, Qλ are shown in Figure 6.2 on the domain D = [0, 1] with prior precision
α = 1 and length-scale λ = 0.1.
The uniform covariance Cf
0 is not everywhere diﬀerentiable, allowing discontinuities in
the ﬁrst derivative of the solution. First derivative-discontinuous solutions typically arise
for delay initial function problems, where derivative discontinuities are often present at the
lag locations. Examples include the JAK-STAT system considered in Chapter 5.
6.2.3
Diagonal boundary covariance
Smoothness is not the only property of the solution that may be modelled by appropriate
choice of covariance. Indeed, we have seen that the use of integrated covariances in the
previous sections guarantees that the solution state always takes a zero or baseline value,
ℓ(a), at the initial boundary a.
This is a very useful property for the ODE problems
examined in this thesis, but is not always suitable for modelling multivariate PDE BVP
solutions directly, where additional constraints may be required at the boundaries along
some of the spatio-temporal dimensions.
In order to constrain solutions at both boundaries of a component of the domain, we
propose a diagonal kernel, which takes on both positive and negative values in such a way
that its integral over the full domain, [a, b], is zero, and that still satisﬁes the necessary

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
79
Figure 6.2:
Top row: contour plots of uniform kernel (left), and its integrated version
(right).
Bottom row: contour plots of derivative covariance (left) and state covariance
(right). Functions are evaluated over t, ˜t ∈[0, 1], with (α, λ) = (1, 0.1).

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
80
condition for the resulting covariance to be positive deﬁnite (i.e. that Rg is not everywhere
zero for all non-zero functions g ∈L2(D)). This has the eﬀect of setting the solution state
to zero at both ends of a one-dimensional domain. The proposed diagonal kernel function
and its integrated version are given by,
Rλ(t, ˜t) =
(˜t −t
)
I
(
t ∈(˜t −λ, ˜t + λ), ˜t ∈[a + λ, b −λ]
)
+1
2
(
a + ˜t + λ −2t
)
I
(
t ∈(a, ˜t + λ), ˜t ∈[a, min(a + λ, b −λ))
)
+1
2
(
b + ˜t −λ −2t
)
I
(
t ∈(˜t −λ, b), ˜t ∈(max(a + λ, b −λ), b]
)
+1
2 (a + b −2t) I
(
t ∈(a, b), ˜t ∈[b −λ, a + λ]
)
,
Qλ(t, ˜t) = 1
2
(
2˜tt −t2 −˜t2 + λ2)
I
(˜t ∈[max(t −λ, a + λ), min(t + λ, b −λ)]
)
+ 1
2
(
(a + ˜t + λ)t −t2 −(˜t + λ)a
)
I
(˜t ∈[max(t −λ, a), min(a + λ, b −λ))
)
+1
2
(
(b + ˜t −λ)t −t2 −(˜t −λ)b
)
I
(˜t ∈(max(a + λ, b −λ), min(t + λ, b)]
)
+1
2
(
(a + b)t −t2 −ab
)
I
(˜t ∈[b −λ, a + λ]
)
.
Below we derive the closed form convolutions required for function estimation under a
diagonal kernel.
αRR(t, ˜t) = 1
6x
(
6t˜t −3(t + ˜t)x + 2x2) min(t+λ,˜t+λ,b−λ)
max(t−λ,˜t−λ,a+λ)
+ 1
4
(
(a −2t + λ)(a −2˜t + λ)x + (a −t −˜t + λ)x2 + 1
3x3) min(a+λ,b−λ)
max(t−λ,˜t−λ,a)
+ 1
4
(
(b −2t −λ)(b −2˜t −λ)x + (b −t −˜t −λ)x2 + 1
3x3) min(t+λ,˜t+λ,b)
max(a+λ,b−λ)
+ 1
4(a + b −2t)(a + b −2˜t)x
a+λ
b−λ,
t, ˜t ∈[a, b],
αRQ(t, ˜t) =
1
24x
(
(−6˜t2 + 6λ2 + 8˜tx −3x2)x + 4t(3˜t2 −3λ2 −3˜tx + x2)
) min{t+λ,˜t+λ,b−λ}
max{t−λ,˜t−λ,a+λ}
+ 1
4(a −˜t)
(
(˜t −λ)(a −2t + λ)x + 1
2(−a + 2t + ˜t −2λ)x2 −1
3x3) min(a+λ,b−λ)
max(t−λ,˜t−λ,a)
+ 1
4(b −˜t)
(
(b −2t −λ)(˜t + λ)x + 1
2(−b + 2t + ˜t + 2λ)x2 −1
3x3) min(t+λ,˜t+λ,b)
max(a+λ,b−λ)
−1
4(a + b −2t)(a −˜t)(b −˜t)x
a+λ
b−λ,
t, ˜t ∈[a, b],
αQQ(t, ˜t) = 1
4
(
−(t2 −λ2)(−˜t2 + λ2)x −(t + ˜t)(t˜t −λ2)x2 + 1
3(t2 + 4t˜t + ˜t2 −2λ2)x3

CHAPTER 6. CHOICE OF COVARIANCE STRUCTURE
81
Figure 6.3:
Top row: contour plots of diagonal kernel (left), and its integrated version
(right).
Bottom row: contour plots of derivative covariance (left) and state covariance
(right). Functions are evaluated over t, ˜t ∈[0, 1], with (α, λ) = (1, 0.1).
−1
2(t + ˜t)x4 + 1
5x5)
I
(
t, ˜t ∈(x −λ, x + λ)
)b−λ
a+λ
+ 1
24x(a −t)(a −˜t)(−6(t −λ)(−˜t + λ) −3(t + ˜t −2λ)x + 2x2) I
(
t, ˜t ∈(a, x + λ)
)min(a+λ,b−λ)
a−λ
+ 1
24(b −t)(b −˜t)(6(t + λ)(˜t + λ) −3(t + ˜t + 2λ)x + 2x2) I
(
t, ˜t ∈(x −λ, b)
)b+λ
max(a+λ,b−λ)
+ 1
4x(a −t)(b −t)(a −˜t)(b −˜t)
a+λ
b−λ,
t, ˜t ∈[a, b].
Here again, we have RR(t, t), QQ(t, t) < ∞, so that Rλ and Qλ are square integrable.
Contour plots of the resulting diagonal covariance function Cf
0 (t, ˜t) = RR(t, ˜t) and its inte-
grated version C0(t, ˜t) = QQ(t, ˜t), as well as kernels Rλ, Qλ are shown in Figure 6.3 on the
domain D = [0, 1] with prior precision α = 1 and length-scale λ = 0.1.

Chapter 7
Sequential Design for Probabilistic
Mesh Selection
For many DE systems, such as those with fast-changing derivatives, small local errors in
the solution approximation can propagate into large deviations from the true solution along
the domain. We have seen that for a numerical or probabilistic DE solution approximation
to be acceptable, it must converge in some sense to the true solution deﬁned by the DE.
Rates of convergence depend on many factors, such as the smoothness of the true function,
and given these, are typically directly related to the size, N, of the discrete grid subdividing
the domain. As N cannot be made arbitrarily large due to computational limitations, its
choice presents a trade-oﬀbetween accuracy of the estimated solution and computational
resources. Therefore, given N, a natural question is how we can arrange the grid on the
domain in such a way that the resulting approximation is as good as possible?
Most commercially available numerical DE solvers select the step length of the dis-
cretization grid sequentially. Below we discuss how this is done, and relate this method to
a sequential design problem in the probabilistic setting.
7.1
Preliminaries
We propose to adaptively select the step length in each dimension for use in the probabilis-
tic solution by optimizing a measure theoretic criterion. We then examine the modelling
requirements for incorporating a variable step length into the probabilistic solvers presented
82

CHAPTER 7. PROBABILISTIC MESH SELECTION
83
in Chapter 4.
7.2
Numerical step length selection
Local truncation error bounds for numerical methods at a given step n are usually pro-
portional to the distance between the estimated solutions evaluated at subsequent knot
locations, |˜u(sn) −˜u(sn−1)|. This quantity is related to the linearized ﬁrst derivative so
that, when the derivative changes quickly, local truncation error increases proportionally.
Controlling the local error by choice of the step length sn −sn−1 is called adaptive step size
selection in the numerical analysis literature. In the simplest cases, this is accomplished by
evaluating the local truncation error at each step and halving the step size if this exceeds an
error tolerance that is pre-speciﬁed by the user (the process may be repeated several times
per step until an appropriate local truncation error is achieved).
7.2.1
Kullback-Leibler divergence criterion
The relative entropy or Kullback-Leibler (KL) entropy (Kullback and Leibler, 1951) is a
non-symmetric metric for the divergence between two probability distributions.
Deﬁnition 18 (Kullback and Leibler (1951)). Let µi, i = 1, 2 be two probability measures
deﬁned on the same measure space (H, A), and absolutely continuous with respect to µ0.
Also let fi = dµi/dµ0, i = 1, 2 represent the corresponding densities. The Kullback-Leibler
divergence is given by,
D
(
f1||f2
)
=
∫
log
(f1(u)
f2(u)
)
dµ1(u) =
∫
f1(u) log
(f1(u)
f2(u)
)
dµ0(u).
For example, the KL divergence between two K-variate Gaussian probability distribu-
tions, µi = NK(mi, Ci), i = 1, 2, deﬁned on (RK, B) is given by,
D
(
f1||f2
)
=
∫1
2
(
log
(|C2|
|C1|
)
+ (u −m2)TC−1
2 (u −m2) −(u −m1)TC−1
1 (u −m1)
)
dµ1
= 1
2
(
log
(|C2|
|C1|
)
+
∫
(u −m2)TC−1
2 (u −m2)dµ1 −
∫
(u −m1)TC−1
1 (u −m1)dµ1
)
= 1
2
(
log
(|C2|
|C1|
)
+ (m1 −m2)TC−1
2 (m1 −m2) + tr
(
C−1
2 C1
)
−K
)
,
(7.1)

CHAPTER 7. PROBABILISTIC MESH SELECTION
84
where we have used properties of expectations with respect to a multivariate Gaussian
measure. Note that the divergence between the two distributions is directly proportional to
the scaled distance between the means, inversely proportional to the number of dimensions,
K, and depends nonlinearly on C−1
2 C1.
In particular, the direct dependence on scaled
distance between the means is reminiscent of the form of the local truncation error objective
function used for numerical step size selection.
7.3
Probabilistic sequential step length selection
Consider an ordered partition s = [s1, · · · , sn] ∈[a, sn]N, s1 = a, sn < b, of the one-
dimensional interval [a, sn]. In this section we derive the KL divergence criterion between
the latest estimated probabilistic solution state un(t, θ) constructed using derivative real-
izations at s1, . . . , sn, and the corresponding solution un+1(t, θ) obtained by placing one
additional knot at a new location s∗∈(sn, min{sn + ε, b}], ε > 0.
7.3.1
KL divergence between current and step-ahead estimated solution
Finite-dimensional evaluations of both un(·, θ) and un+1(·, θ) are jointly Gaussian with
mean vector and covariance matrix deﬁned in (3.13) and (3.15). Therefore, we can obtain
the integrated KL divergence from (7.1):
∫b
a
D (un+1(t) || un(t)) dλ(t)
=
∫b
a
1
2
[Cn+1(t, t)
Cn(t, t)
+ {mn(t) −mn+1(t)}2
Cn(t, t)
−1 −log Cn+1(t, t)
Cn(t, t)
]
dt.
Calculating this criterion becomes eﬀectively computationally infeasible, due to the inversion
of the matrix RR([s1:n; s∗], [s1:n; s∗])+Λ required to evaluate mn+1(t) and Cn+1(t, t) for every
proposed value of s∗∈(sn, min{sn+ε, b}], ε > 0 at each step. However, this inversion can be
easily avoided by using the recursive formulation from Lemma 4.1.1 to rewrite the criterion
as,
∫b
a
D (un+1(t) || un(t)) dλ(t)
=
∫b
a
1
2
[
Cn(t, t) −C2
n(t, s∗)/2Cf
n(s∗, s∗)
Cn(t, t)
+ {(mf
n(s∗) −fθ(s∗))
∫t
a Cf
n(x, s∗)dx/2Cf
n(s∗, s∗)}2
Cn(t, t)

CHAPTER 7. PROBABILISTIC MESH SELECTION
85
−1 −log Cn(t, t) −C2
n(t, s∗)/2Cf
n(s∗, s∗)
Cn(t, t)
]
dt
=
∫b
a
1
2


{fθ(s∗) −mf
n(s∗)}2 (∫t
a Cf
n(x, s∗)dx
)2
4Cf2
n (s∗, s∗)Cn(t, t)
−
C2
n(t, s∗)
2Cf
n(s∗, s∗)Cn(t, t)
−log
{
1 −
C2
n(t, s∗)
2Cf
n(s∗, s∗)Cn(t, t)
}]
dt,
(7.2)
where now the expression is written solely in terms of the nth estimated derivative mean
mf
n, state covariance Cn, and cross-covariance
∫t
a Cf
n(x, ·)dx, which were already obtained
at the nth iteration. It is important to note the integrand of criterion (7.2) is always non-
negative and equal to zero iﬀun+1(t) = un(t) almost everywhere (this holds in general for
the KL divergence, see for example Cover and Thomas, 2006).
To our knowledge, the integral (7.2) cannot be evaluated in closed form. Thus, somewhat
ironically, we must resort to numerical integration to evaluate this criterion. However, we
note that this is a rather straightforward one-dimensional integration problem relative to
the one of solving the DE itself, and is not sensitive to properties of the solution trajectory.
Therefore, in practice we approximate the integral numerically with respect to t using the
trapezoidal rule.
7.3.2
Implementation
Using the KL divergence to inform the step size requires a number of practical considerations.
We suggest ﬁrst specifying the maximum computationally feasible number of equidistant
mesh points N∗for solving the DE, and allocating these to a temporary equidistant ﬁxed
design over [a, b] with step size h∗= (b −a)/N∗. We may now deﬁne solver step sizes in
terms of these smallest unit steps. In particular, we ﬁx an integer G > 1 so that h∗G is
the maximum allowable step length for the problem. The length-scale is typically deﬁned
in terms of step sizes, so we must be careful to specify a length-scale that is at least twice
the maximum step size h∗G. In this way, information from nearby derivative realizations is
included even when taking the largest allowed steps1.
The algorithm must begin by iterating through the ﬁrst G unit steps of length h∗.
At iteration n > G we may begin to adjust the step size.
Deﬁne the candidate nodes
1for this purpose, adaptive selection of length-scales would be a useful extension.

CHAPTER 7. PROBABILISTIC MESH SELECTION
86
s∗
g = sn + gh∗for g = 1 . . . , G. We will choose the node s∗
g that maximizes the discrep-
ancy between the posterior solution derivatives un =
[
u(·, θ) | θ, f1:n, ua, Ψn
]
, and un+1 =
[
u(·, θ) | θ, [f1:n; fθ(s∗
g)], ua, Ψn
]
, the latter being computed using one additional derivative
realization, i.e. f1:n+1 = [f1:n; fθ(s∗
g)]. Thus the next node is selected as,
sn+1 = arg max
s∗g
max
p
∫b
a
D
(
u(p)
n+1(t) || u(p)
n (t)
)
dλ(t).
Due to the recursion employed in the derivation of the criterion, the computational cost
of step size selection is negligible relative to the rest of the algorithm. Thus, we recommend
to adaptively select step sizes whenever possible. A computational issue arises when dealing
with chaotic systems. After the onset of chaotic dynamics, the sample paths begin to diﬀer
substantially from one another, and have diﬀerent associated optimal mesh designs.
In
theory, optimizing the design for each draw is certainly feasible. In reality this requirement
introduces large computational costs associated with matrix operations, since covariances
can no longer be recycled as suggested in Section 4.1.3. In comparison multiple draws from
the probabilistic solution can be obtained almost instantaneously when the mesh is identical
for each draw, as was shown.
Figure 7.1 shows a single realization of the probabilistic solution for the Lorenz system
before the onset of chaos, obtained using adaptive step sizes with a maximum possible mesh
size of N∗= 2000 and selection region deﬁned by G = 4. The actual grid size obtained
adaptively was only N = 740, as in practice the smallest step size was rarely selected by
the algorithm.

CHAPTER 7. PROBABILISTIC MESH SELECTION
87
Figure 7.1: Realizations of the states (top, solid lines) and derivatives (middle, solid lines)
of a single draw from the probabilistic solution of the Lorenz system on the interval [0, 5].
State and derivative realizations obtained at each step are shown as dots. The mesh was
selected adaptively, and the resulting step lengths are shown in the lower panel in dark blue.
Light blue dotted lines represent the possible step lengths {gh∗}g=1 ...,4.

Chapter 8
Conclusion
This thesis describes a new probabilistic formalism for studying the structure of solution
uncertainty for general systems of diﬀerential equations. Rather than providing a single
deterministic solution that approximately satisﬁes the model dynamics, our approach pro-
vides a probability statement over the space of suitably regular functions. This allows the
explicit study of the error structure and propagation of functional uncertainty through the
statistical inference process, which had been until now an open problem.
Speciﬁcally, we develop a Bayesian probability model ﬁrst suggested in Skilling (1991)
for the unknown deterministic solution of general intractable diﬀerential equation problems
given a ﬁnite discretization mesh deﬁned on the domain of integration. We show how such
a model can (i) provide a probability statement about the dynamics of the unknown DE
solution given a ﬁnite number of derivative evaluations, or (ii) deﬁne a level of uncertainty
in a hierarchical model for any unknown model parameters. This allows us to characterize
the systematic model uncertainty introduced into the inference problem by discretization
and to further distinguish it from other sources of uncertainty.
8.1
Impact and Recommendations
The framework described in this thesis is a ﬁrst step in a promising new avenue for research.
Broadly, our work has potential for impact in three areas: uncertainty quantiﬁcation for
inverse problems, analysis of computer experiments, and the study of diﬀerential equation
dynamics.
Uncertainty quantiﬁcation is the study of the impact and propagation of variability
88

CHAPTER 8. CONCLUSION
89
through complex dynamical systems. However, until now a coherent probabilistic frame-
work for characterizing uncertainty resulting from discretization of inﬁnite-dimensional DE
solutions has been unavailable (DeVolder et al., 2002). This thesis demonstrates that such
an approach is feasible and can be incorporated naturally into the inverse problem.
Large-scale models are often encoded in computer simulators, consisting of complex
systems of diﬀerential equations and numerical algorithms to solve them approximately
given unknown parameters or initial values of interest. Currently numerical uncertainty
in the model is largely ignored, although it is incorporated heuristically through a covari-
ance nugget when constructing the emulator (see, for example, Gramacy and Lee, 2012).
Adopting our probabilistic approach on a large scale will have practical implications in
this research area by allowing relaxation of the error-free assumption used for computer
codes, resulting in more realistic and ﬂexible emulators. We particularly recommend such
an approach for models of turbulent systems, such as those found in local weather models
or hydrodynamics. The inherent sensitivity of such models to perturbations highlights the
need to characterize the impact of numerical uncertainty.
Further potential applications of the probabilistic approach include the extension to
Stochastic Diﬀerential Equation (SDE) systems, for which closed form solutions are typically
unavailable, and existing inference methods are largely simulation-based. In this context,
probabilistic solutions could be useful in determining a set of probable sample paths. We
demonstrated that our approach provides a functional probabilistic alternative to numerical
solution and error analysis, which is particularly useful for systems that are sensitive to
perturbations. This approach can be further extended to integration on complex domains,
via ﬁnite element methods (FEMs). Indeed, we believe that a probabilistic approach has
the potential to provide a general framework for error analysis of FEMs.
Other useful
extensions of our methodology include the area of numerical bifurcation analysis, used to
study behaviours of analytically intractable systems over diﬀerent regimes. Not only would
fully probabilistic solutions yield credible intervals on estimated bifurcation boundaries, but
they would also detect possible multiplicity of solutions when systems are ill-conditioned.

Bibliography
Afraimovich, V. S., V. V. Bykov, and L. P. Shil’nikov (1977).
On the appearance and
structure of the Lorenz attractor. Proceedings of the USSR Academy of Sciences 234,
336–339. 12
Baker, G. and J. Gollub (1996). Chaotic Dynamics: an Introduction. Cambridge University
Press. 11
Bellen, A. and M. Zennaro (2003). Numerical Methods for Delay Diﬀerential Equations.
Clarendon Press. 17
Berliner, L. M. and S. N. MacEachern (1993). Examples of inconsistent Bayes procedures
based on observations on dynamical systems. Statistics & Probability Letters 17(5), 355–
360.
Bernard, S., B. ˇCajavec, L. Pujo-Menjouet, M. C. Mackey, and H. Herzel (2006). Modelling
transcriptional feedback loops: the role of Gro/TLE1 in Hes1 oscillations. Philosophical
Transactions: Mathematical, Physical and Engineering Sciences 364(1842), 1155–1170.
16
Billingsley, P. (1968). Convergence of Probability Measures. Wiley. 26
Boyle, P. and M. Frean (2005). Dependent Gaussian processes. Advances in Neural Infor-
mation Processing Systems 17, 217–224. 30
Busenberg, S. and B. Tang (1994). Mathematical models of the early embryonic cell cycle:
the role of MPF activation and cyclin degradation. Journal of Mathematical Biology 32,
573–596. 16
Butcher, J. (2008). Numerical Methods for Ordinary Diﬀerential Equations. John Wiley
and Sons Ltd. 2, 8, 47
Campbell, D. A. and O. Chkrebtii (2013). Maximum proﬁle likelihood estimation of diﬀer-
ential equation parameters through model based smoothing state estimates. Mathematical
Biosciences (in press). 69
Cover, T. M. and J. A. Thomas (2006). Elements of Information Theory (Wiley Series in
Telecommunications and Signal Processing). Wiley-Interscience. 85
90

BIBLIOGRAPHY
91
DeVolder, B., J. Glimm, J. Grove, Y. Kang, Y. Lee, K. Pao, D. H. Sharp, and K. Ye (2002).
Uncertainty quantiﬁcation for multiscale simulations. Journal of Fluids Engineering 124,
29–41. 89
Diaconis, P. (1988). Bayesian Numerical Analysis. Springer-Verlag. 3
Doucet, A., A. M. Johansen, and V. B. Tadic (2010). On solving integral equations using
Markov chain Monte Carlo methods. Applied Mathematics and Computation 216, 2869–
2880. 4
Foias, C., R. R. M. O. and R. Temam (2001).
Navier-Stokes equations and turbulence.
Encyclopedia of Mathematics and Its Applications, Cambridge University Press. 12
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (2004). Bayesian Data Analysis.
Chapman and Hall/CRC. 66, 67
Geyer, C. (1991).
Markov chain Monte Carlo maximum likelihood.
In Computing Sci-
ence and Statistics, Proceedings of the 23rd Symposium on the Interface, 156. American
Statistical Association. 49, 70
Gramacy, R. and H. Lee (2012). Cases for the nugget in modeling computer experiments.
Statistics and Computing 22, 713–722. 89
Guckenheimer, J. and R. F. Williams (1979).
Structural stability of Lorenz attractors.
Publications math´ematiques de l’IH´ES 50, 59–72. 12
Hastings, W. (1970). Monte Carlo sampling methods using Markov chains and their appli-
cation. Biometrika 57, 97–109. 4
Jost, J. (2012). Partial Diﬀerential Equations. Graduate texts in mathematics. Springer.
20
Kassam, A. and L. Trefethen (2005). Fourth-order time-stepping for stiﬀPDEs. SIAM
Journal on Scientiﬁc Computing 26(4), 1214–1233. 21, 58
Keller, H. B. (1968). Numerical Methods for Two-point Boundary-value Problems. Blaisdell
Publishing Company. 14, 15
Kullback, S. and R. A. Leibler (1951). On information and suﬃciency. Annals of Mathe-
matical Statistics 22, 49–86. 83
Kuo, H. (1975). Gaussian Measures in Banach Spaces.
Lecture Notes in Mathematics.
Springer-Verlag. 26
Kuramoto, Y. and T. Tsuzuki (1976). Persistent propagation of concentration waves in
dissipative media far from thermal equilibrium. Progress of Theoretical Physics 55, 356–
369. 21

BIBLIOGRAPHY
92
Lange, K. (1999).
Numerical Analysis for Statisticians.
Statistics and computing. New
York, NY: Springer. 3
Leone, F. C., L. S. Nelson, and R. B. Nottingham (1961). The folded normal distribution.
Technometrics 3, 543–550. 46
Lewis, J. (2003). Autoinhibition with transcriptional delay: a simple mechanism for the
zebraﬁsh somitogenesis oscillator. Current Biology 13(16), 1398 – 1408. 16
Lifshits, A. (1995). Gaussian Random Functions. Mathematics and Its Applications. Kluwer
Academic Publishers. 25, 26
Lorenz, E. N. (1963).
Deterministic nonperiodic ﬂow.
Journal of the Atmospheric Sci-
ences 20, 130–141. 11
Megginson, R. (1998). An Introduction to Banach Space Theory. Graduate texts in mathe-
matics. Plenum Press. 25
Metropolis, N. and S. Ulam (1949). The Monte Carlo method. Journal of the American
Statistical Association 44, 335–341. 4
Mischaikow, K. and M. Mrozek (1995). Chaos in the Lorenz equations: a computer-assisted
proof. Bulletin of the American Mathematical Society 32(1), 66–72. 13
Mrozek, M. and R. Srzednicki (2010). Topological approach to rigorous numerics of chaotic
dynamical systems with strong expansion of error bounds. Foundations of Computational
Mathematics 10(2), 191–220. 13
Neal, R. M. (2011).
MCMC using ensembles of states for problems with fast and slow
variables such as Gaussian process regression. 45
O’Hagan, A. O. (1992). Some Bayesian numerical analysis. Bayesian Statistics 4, 345–363.
4
Pellegrini, S. and I. Dusanter-Fourt (1997). The structure, regulation and function of the
Janus kinases (JAKs) and the signal transducers and activators of transcription (STATs).
European Journal of Biochemistry 248(3), 615–633. 69
Poincar´e, H. (1896). Calcul des Probabilities. Georges Carr´e. 3
Poincar´e, H. (1913). The Foundation of Science: Science and Method, English translation.
Lancaster, PA: The Science Press. 11
Ramsay, J., G. Hooker, and J. Cao (2007). Parameter estimation for diﬀerential equations:
a generalized smoothing approach. Journal of the Royal Statistical Society B 69, 741–796.
66

BIBLIOGRAPHY
93
Rasmussen, C. E. and C. K. I. Williams (2006). Gaussian Processes for Machine Learning.
Cambridge, Massachusetts: MIT Press. 4, 31, 54, 75
Raue, A., C. Kreutz, T. Maiwald, J. Bachmann, M. Schilling, U. Klingmller, and J. Timmer
(2009). Structural and practical identiﬁability analysis of partially observed dynamical
models by exploiting the proﬁle likelihood. Bioinformatics 25, 1923–1929. 69, 70, 71
Robinson, J. (2001). Inﬁnite-Dimensional Dynamical Systems: An Introduction to Dissipa-
tive Parabolic PDEs and the Theory of Global Attractors. Cambridge Texts in Applied
Mathematics. Cambridge University Press. 12
Rohde, C. (1965). Generlized inverses of partitioned matrices. Journal of the Society for
Industrial and Applied Mathematics 13(4), 1033–1035. 43
Sauer, T., C. Grebogi, and J. A. Yorke (1997). How long do numerical chaotic solutions
remain valid? Physical Review Letters 79, 59–62. 12
Shampine, L. (2003). Singular boundary value problems for ODEs. Applied Mathematics
and Computation 138, 99 – 112. 15
Shu, C., H. Ding, and K. Yeo (2003). Local radial basis function-based diﬀerential quadra-
ture method and its application to solve two-dimensional incompressible Navier-Stokes
equations. Computer Methods in Applied Mechanics and Engineering 192, 941 – 954. 10
Sivashinsky, G. I. and D. M. Michelson (1980). On irregular wavy ﬂow of a liquid ﬁlm down
a vertical plane. Progress of Theoretical Physics 63(6), 2112–2114. 21
Skilling, J. (1991). Bayesian Solution of Ordinary Diﬀerential Equations, pp. 23–37. Seattle:
Kluwer Academic Publishers. 4, 24, 88
Stuart, A. M. (2010). Inverse problems: a Bayesian perspective. Acta Numerica 19, 451–559.
25, 28, 65, 74
Swameye, I., T. Muller, J. Timmer, O. Sandra, and U. Klingmuller (2003). Identiﬁcation of
nucleocytoplasmic cycling as a remote sensor in cellular signaling by databased modeling.
Proceedings of the National Academy of Sciences 100, 1028–1033. 16, 70, 71
Tucker, W. (1999). The Lorenz attractor exists. Comptes Rendus de l’Acade ´mie des Sciences
- Series I - Mathematics 328(12), 1197 – 1202. 13
Williams, R. F. (1979). The structure of the Lorenz attractor. Publications math´ematiques
de l’IH´ES 50, 73–99. 12

