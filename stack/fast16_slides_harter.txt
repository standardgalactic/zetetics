Slacker: Fast Distribution 
with Lazy Docker Containers
Tyler Harter, Brandon Salmon†, Rose Liu†, 
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau
†

Container Popularity
spoon.net

Theory and Practice
Theory: containers are lightweight 
•
just like starting a process! 

Theory and Practice
[1] Large-scale cluster management at Google with Borg. 
      http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf
Theory: containers are lightweight 
•
just like starting a process! 
Practice: container startup is slow 
•
25 second startup time [1]
task startup latency (the time from job submission to a task running) is an 
area that has received and continues to receive signiﬁcant attention. It is 
highly variable, with the median typically about 25 s. Package installation 
takes about 80% of the total: one of the known bottlenecks is 
contention for the local disk where packages are written. ”
“

Theory and Practice
[1] Large-scale cluster management at Google with Borg. 
      http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf
Theory: containers are lightweight 
•
just like starting a process! 
Practice: container startup is slow 
•
25 second startup time [1]
task startup latency (the time from job submission to a task running) is an 
area that has received and continues to receive signiﬁcant attention. It is 
highly variable, with the median typically about 25 s. Package installation 
takes about 80% of the total: one of the known bottlenecks is 
contention for the local disk where packages are written. ”
“

Theory and Practice
[1] Large-scale cluster management at Google with Borg. 
      http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43438.pdf
Theory: containers are lightweight 
•
just like starting a process! 
Practice: container startup is slow 
•
25 second startup time [1] 
Startup time matters 
•
ﬂash crowds 
•
load balance 
•
interactive development

Contributions
HelloBench 
•
Docker benchmark for stressing startup 
•
based on 57 container workloads 
Startup analysis 
•
76% of startup time spent copying/installing images 
•
startup requires only 6% of that image data 
Slacker: Docker storage driver 
•
lazily pull only needed data 
•
leverage extensions to Linux kernel and NFS server 
•
5-20x startup speedups

Slacker Outline
Background 
•
Containers: lightweight isolation 
•
Docker: ﬁle-system provisioning 
Container Workloads 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Why use containers?

Why use containers?
(it’s trendy)

Why use containers?
(it’s trendy) 
(efﬁcient solution to classic problem)

Big Goal: Sharing and Isolation
App A
App B
want: multitenancy
Physical Machine

Big Goal: Sharing and Isolation
App A
App B
don’t want: crashes
Physical Machine

Big Goal: Sharing and Isolation
App A
App B
don’t want: crashes
Physical Machine

Big Goal: Sharing and Isolation
App A
App B
don’t want: unfairness
Physical Machine

Big Goal: Sharing and Isolation
App A
App B
don’t want: leaks
Physical Machine
sensitive 
data

Solution: Virtualization
namespaces and scheduling provide illusion of private resources

Evolution of Virtualization
1st generation: process virtualization 
•
isolate within OS (e.g., virtual memory) 
•
fast, but incomplete (missing ports, ﬁle system, etc.)
App A
process
App B
process
Operating System
process virtualization

Evolution of Virtualization
1st generation: process virtualization 
•
isolate within OS (e.g., virtual memory) 
•
fast, but incomplete (missing ports, ﬁle system, etc.) 
2nd generation: machine virtualization 
•
isolate around OS 
•
complete, but slow (redundancy, emulation)
App A
process
App B
process
VM
OS
OS
VM
App A
App B
Operating System
process virtualization
machine virtualization

Evolution of Virtualization
1st generation: process virtualization 
•
isolate within OS (e.g., virtual memory) 
•
fast, but incomplete (missing ports, ﬁle system, etc.) 
2nd generation: machine virtualization 
•
isolate around OS 
•
complete, but slow (redundancy, emulation)
App A
process
App B
process
VM
OS
OS
VM
App A
App B
Operating System
process virtualization
machine virtualization

Evolution of Virtualization
1st generation: process virtualization 
•
isolate within OS (e.g., virtual memory) 
•
fast, but incomplete (missing ports, ﬁle system, etc.) 
2nd generation: machine virtualization 
•
isolate around OS 
•
complete, but slow (redundancy, emulation) 
3rd generation: container virtualization 
•
extend process virtualization: ports, ﬁle system, etc. 
•
fast and complete

Evolution of Virtualization
1st generation: process virtualization 
•
isolate within OS (e.g., virtual memory) 
•
fast, but incomplete (missing ports, ﬁle system, etc.) 
2nd generation: machine virtualization 
•
isolate around OS 
•
complete, but slow (redundancy, emulation) 
3rd generation: container virtualization 
•
extend process virtualization: ports, ﬁle system, etc. 
•
fast and complete???
many storage 
challenges

New Storage Challenges
Crash isolation 
Physical Disentanglement in a Container-Based File System. 
Lanyue Lu, Yupu Zhang, Thanh Do, Samer Al-Kiswany, 
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau.  OSDI ‘14. 
Performance isolation 
Split-level I/O Scheduling For Virtualized Environments. 
Suli Yang, Tyler Harter, Nishant Agrawal, Salini Selvaraj Kowsalya, Anand 
Krishnamurthy, Samer Al-Kiswany, Andrea C. Arpaci-Dusseau, 
Remzi H. Arpaci-Dusseau.  SOSP ‘15. 
File-system provisioning 
Slacker: Fast Distribution with Lazy Docker Containers. 
Tyler Harter, Brandon Salmon, Rose Liu, 
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau.  FAST ‘16.
today

Slacker Outline
Background 
•
Containers: lightweight isolation 
•
Docker: ﬁle-system provisioning 
Container Workloads 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Docker Background
Deployment tool built on containers 
An application is deﬁned by a ﬁle-system image 
•
application binary 
•
shared libraries 
•
etc. 
Version-control model 
•
extend images by committing additional ﬁles 
•
deploy applications by pushing/pulling images

Containers as Repos 
LAMP stack example 
•
commit 1: Linux packages (e.g., Ubuntu) 
•
commit 2: Apache 
•
commit 3: MySQL 
•
commit 4: PHP 
Central registries 
•
Docker HUB 
•
private registries 
Docker “layer” 
•
commit 
•
container scratch space

Push, Pull, Run
registry
worker
worker
worker

registry
worker
worker
worker
push
Push, Pull, Run

registry
worker
worker
worker
Push, Pull, Run

registry
worker
worker
worker
pull
pull
Push, Pull, Run

registry
worker
worker
worker
Push, Pull, Run

registry
worker
worker
worker
Push, Pull, Run
C
C
C
run
run
run

registry
worker
worker
worker
Push, Pull, Run
C
C
C
need a new benchmark 
to measure Docker push, 
pull, and run operations.
run
run
run

Slacker Outline
Background 
Container Workloads 
•
HelloBench 
•
Analysis 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

HelloBench
Goal: stress container startup 
•
including push/pull 
•
57 container images from Docker HUB 
•
run simple “hello world”-like task 
•
wait until it’s done/ready
push
pull
run

HelloBench
Goal: stress container startup 
•
including push/pull 
•
57 container images from Docker HUB 
•
run simple “hello world”-like task 
•
wait until it’s done/ready
push
pull
run

HelloBench
Goal: stress container startup 
•
including push/pull 
•
57 container images from Docker HUB 
•
run simple “hello world”-like task 
•
wait until it’s done/ready
push
pull
run
ready

HelloBench
Goal: stress container startup 
•
including push/pull 
•
57 container images from Docker HUB 
•
run simple “hello world”-like task 
•
wait until it’s done/ready 
Development cycle 
•
distributed programming/testing 
push
pull
run
ready
development cycle

HelloBench
Goal: stress container startup 
•
including push/pull 
•
57 container images from Docker HUB 
•
run simple “hello world”-like task 
•
wait until it’s done/ready 
Development cycle 
•
distributed programming/testing 
Deployment cycle 
•
ﬂash crowds, rebalance
push
pull
run
ready
deployment cycle

Workload Categories
Linux Distro 
alpine    
busybox              
centos               
cirros               
crux                 
debian               
fedora               
mageia               
opensuse             
oraclelinux          
ubuntu               
ubuntu-
debootstrap   
ubuntu-upstart      
Database 
cassandra            
crate                
elas6csearch        
mariadb              
mongo                
mysql                
percona              
postgres             
redis                
rethinkdb            
!
Web Framework 
django               
iojs                 
node                 
rails               
Language 
clojure              
gcc                  
golang               
haskell              
hylang               
java                 
jruby                
julia                
mono                 
perl                 
php                  
pypy                 
python               
r-base               
rakudo-star          
ruby                 
thri<              
Web Server 
glassﬁsh            
h>pd                
je>y                
nginx                
php-zendserver       
tomcat       
!
Other 
drupal               
ghost                
hello-world          
jenkins              
rabbitmq             
registry             
sonarqube                  

Slacker Outline
Background 
Container Workloads 
•
HelloBench 
•
Analysis 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Questions
How is data distributed across Docker layers? 
!
How much image data is needed for container startup? 
!
How similar are reads between runs? 

Questions
How is data distributed across Docker layers? 
!
How much image data is needed for container startup? 
!
How similar are reads between runs? 

HelloBench images 
•
circle: commit 
•
red: image

Image Data Depth

Image Data Depth
half of data is at depth 9+

Questions
How is data distributed across Docker layers? 
•
half of data is at depth 9+ 
•
design implication: ﬂatten layers at runtime 
How much image data is needed for container startup? 
How similar are reads between runs? 

Questions
How is data distributed across Docker layers? 
•
half of data is at depth 9+ 
•
design implication: ﬂatten layers at runtime 
How much image data is needed for container startup? 
How similar are reads between runs? 

Container Ampliﬁcation

Container Ampliﬁcation

Container Ampliﬁcation
only 6.4% of data needed during startup

Questions
How is data distributed across Docker layers? 
•
half of data is at depth 9+ 
•
design implication: ﬂatten layers at runtime 
How much image data is needed for container startup? 
•
6.4% of data is needed 
•
design implication: lazily fetch data 
How similar are reads between runs? 

Questions
How is data distributed across Docker layers? 
•
half of data is at depth 9+ 
•
design implication: ﬂatten layers at runtime 
How much image data is needed for container startup? 
•
6.4% of data is needed 
•
design implication: lazily fetch data 
How similar are reads between runs? 

Repeat Runs
measure hits/misses for second of two runs

Repeat Runs
up to 99% of reads could be serviced by a cache
measure hits/misses for second of two runs

Questions
How is data distributed across Docker layers? 
•
half of data is at depth 9+ 
•
design implication: ﬂatten layers at runtime 
How much image data is needed for container startup? 
•
6.4% of data is needed 
•
design implication: lazily fetch data 
How similar are reads between runs? 
•
containers from same image have similar read patterns 
•
design implication: share cache state between containers

Slacker Outline
Background 
Container Workloads 
Default Driver: AUFS 
•
Design 
•
Performance 
Our Driver: Slacker 
Evaluation 
Conclusion

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories
Operations 
•
push 
•
pull 
•
run

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
layers:
…
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
directories:
…
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
…
PUSH
directories:
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
tar.gz
PUSH
…
directories:
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
tar.gz
PUSH
…
directories:
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
PULL
…
directories:
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
tar.gz
PULL
…
directories:
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
tar.gz
PULL
directories:
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
PULL
directories:
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
directories:
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
scratch dir:
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
root FS
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
read B
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
read B
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
read X
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
read X
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
append Z
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
append Z
Z
copy
A
B
C
A
B
C
X
Y
Z
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
append Z
Z
X
Y
Z
A
B
C
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
append Z
Z’
X
Y
Z
A
B
C
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

AUFS Storage Driver
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
each Docker layer is a directory in underlying FS 
•
union these directories to create complete view of FS
AUFS Driver
A
B
C
X
Y
Z
RUN
AUFS
Z’
X
Y
Z
A
B
C
A
B
C
A
B
C
Uses AUFS ﬁle system (Another Union FS) 
•
stores data in an underlying FS (e.g., ext4) 
•
layer ⇒ directory in underlying FS 
•
root FS ⇒ union of layer directories

Slacker Outline
Background 
Container Workloads 
Default Driver: AUFS 
•
Design 
•
Performance 
Our Driver: Slacker 
Evaluation 
Conclusion

AUFS File System

AUFS File System
Deep data is slow

AUFS Storage Driver

AUFS Storage Driver
76% of deployment cycle spent on pull

Slacker Outline
Background 
Container Workloads 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

images and 
containers
Prefetch vs. Lazy Fetch
registry
images
worker
containers
registry
worker
AUFS
Slacker

images and 
containers
Prefetch vs. Lazy Fetch
registry
images
worker
containers
registry
worker
AUFS
Slacker
signiﬁcant copying 
•
over network 
•
to/from disk
centralized storage 
•
easy sharing

Prefetch vs. Lazy Fetch
registry
images
worker
containers
AUFS
images and 
containers
registry
worker
Slacker

Prefetch vs. Lazy Fetch
images and 
containers
registry
Slacker
loopback
ext4
container
NFS File

loopback
ext4
container
Prefetch vs. Lazy Fetch
Slacker
registry
NFS File

loopback
ext4
container
Prefetch vs. Lazy Fetch
Slacker
registry
NFS File
VMstore abstractions…

VMstore Abstractions
Copy-on-Write 
•
VMstore provides snapshot() and clone() 
•
block granularity avoids AUFS’s problems with ﬁle granularity 
snapshot(nfs_path)!
•
create read-only copy of NFS ﬁle 
•
return snapshot ID 
clone(snapshot_id)!
•
create r/w NFS ﬁle from snapshot 
Slacker Usage 
•
NFS ﬁles ⇒ container storage 
•
snapshots ⇒ image storage 
•
clone() ⇒ provision container from image 
•
snapshot() ⇒ create image from container

Snapshot and Clone
Tintri VMstore
worker A
container
NFS ﬁle

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
snapshot
Worker A: push

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
snapshot
snap N
COW
Worker A: push

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
Worker A: push

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker A: push

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker A: push
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Note: registry is only a name server. 
Maps layer metadata ⇒ snapshot ID
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker B: pull and run
worker B
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker B: pull and run
worker B
N
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker B: pull and run
worker B
clone N
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker B: pull and run
worker B
clone N
COW
NFS ﬁle
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
N
snap N
registry
Worker B: pull and run
worker B
NFS ﬁle
N
snap N
N
img

Snapshot and Clone
Tintri VMstore
worker A
NFS ﬁle
snap N
registry
Worker B: pull and run
worker B
NFS ﬁle
container
snap N
img

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

Slacker Flattening
File Namespace Level 
•
ﬂatten layers 
•
if B is child of A, then “copy” A to B to start.  Don’t make B empty 
Block Level 
•
do COW+dedup beneath NFS ﬁles, inside VMstore
ext4
dir
dir
dir
dir
copy-on-write
ext4
NFS NFS NFS NFS
copy-on-write
ext4 ext4 ext4
AUFS
Slacker
namespace
block

Slacker Flattening
File Namespace Level 
•
ﬂatten layers 
•
if B is child of A, then “copy” A to B to start.  Don’t make B empty 
Block Level 
•
do COW+dedup beneath NFS ﬁles, inside VMstore
ext4
A
B
C
D
copy-on-write
ext4
A
AB
ABC ABCD
copy-on-write
ext4 ext4 ext4
AUFS
Slacker
namespace
block

Challenge: Framework Assumptions
Assumed Layout
Actual Layout
       D
     C  
   B
 A
Layers
 A B C D
 A B C
 A B
 A
Layers
      
      
runnable
runnable

Challenge: Framework Assumptions
       D
     C  
   B
 A
Layers
 A B C D
 A B C
 A B
 A
Layers
      
      
pull
pull
Assumed Layout
Actual Layout

Challenge: Framework Assumptions
       D
     C  
   B
 A
Layers
 A B C D
 A B C
 A B
 A
Layers
      
optimize
Strategy: lazy cloning.  Don’t clone non-top 
layers until Docker tries to mount them.
Assumed Layout
Actual Layout

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

Slacker Driver
Goals 
•
make push+pull very fast 
•
utilize powerful primitives of a modern storage server (Tintri VMstore) 
•
create drop-in replacement; don’t change Docker framework itself 
Design 
•
lazy pull 
•
layer ﬂattening 
•
cache sharing

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
storage for 2 containers 
started from same image
ABC
image

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY

NFS Client:
Challenge: Cache Sharing
cache:
read
ABX
ABY

NFS Client:
Challenge: Cache Sharing
cache:
AABX
ABY

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A
read

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A
A

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A
A

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A
A
Challenge: how to avoid 
space and I/O waste?

NFS Client:
Challenge: Cache Sharing
cache:
ABX
ABY
A
A
Strategy: track differences and 
deduplicate I/O (more in paper)
001
001

Slacker Outline
Background 
Container Workloads 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Questions
What are deployment and development speedups? 
How is long-term performance? 
!

Questions
What are deployment and development speedups? 
How is long-term performance? 

HelloBench Performance
deployment: pull+run 
development: push+pull+run

Questions
What are deployment and development speedups? 
•
5x and 20x faster respectively (median speedup) 
How is long-term performance? 

Questions
What are deployment and development speedups? 
•
5x and 20x faster respectively (median speedup) 
How is long-term performance? 

Server Benchmarks
Databases and Web Servers 
•
PostgreSQL 
•
Redis 
•
Apache web server (static) 
•
io.js Javascript server (dynamic) 
Experiment 
•
measure throughput (after startup) 
•
run 5 minutes 

Server Benchmarks
Databases and Web Servers 
•
PostgreSQL 
•
Redis 
•
Apache web server (static) 
•
io.js Javascript server (dynamic) 
Experiment 
•
measure throughput (after startup) 
•
run 5 minutes 
Result: Slacker is always at least as fast as AUFS

Questions
What are deployment and development speedups? 
•
5x and 20x faster respectively (median speedup) 
How is long-term performance? 
•
there is no long-term penalty for being lazy 

Slacker Outline
Background 
Container Workloads 
Default Driver: AUFS 
Our Driver: Slacker 
Evaluation 
Conclusion

Conclusion
Containers are inherently lightweight 
•
but existing frameworks are not 
COW between workers is necessary for fast startup 
•
use shared storage 
•
utilize VMstore snapshot and clone 
Slacker driver 
•
5x deployment speedup 
•
20x development speedup 
HelloBench: https://github.com/Tintri/hello-bench

