What makes unlearning hard and what to do about it
Kairan Zhao
University of Warwick
Kairan.Zhao@warwick.ac.uk
Meghdad Kurmanji
University of Warwick
Meghdad.Kurmanji@warwick.ac.uk
George-Octavian Barbulescu
University of Warwick
George-Octavian.Barbulescu@warwick.ac.uk
Eleni Triantafillou∗
Google DeepMind
etriantafillou@google.com
Peter Triantafillou∗
University of Warwick
P.Triantafillou@warwick.ac.uk
Abstract
Machine unlearning is the problem of removing the effect of a subset of training
data (the “forget set”) from a trained model e.g. to comply with users’ requests to
delete their data, or remove mislabeled, poisoned or otherwise problematic data.
With unlearning research still being at its infancy, many fundamental open ques-
tions exist: Are there interpretable characteristics of forget sets that substantially
affect the difficulty of the problem? How do these characteristics affect different
state-of-the-art algorithms? We present the first investigation into these questions.
We identify two key factors affecting unlearning difficulty and the performance of
unlearning algorithms. Our evaluation on forget sets that isolate these identified
factors reveals previously-unknown behaviours of state-of-the-art algorithms that
don’t materialize on random forget sets. Based on our insights, we develop a
framework coined Refined-Unlearning Meta-algorithm (RUM) that encompasses:
(i) refining the forget set into homogenized subsets, according to different char-
acteristics; and (ii) a meta-algorithm that employs existing algorithms to unlearn
each subset and finally delivers a model that has unlearned the overall forget set.
RUM substantially improves top-performing unlearning algorithms. Overall, we
view our work as an important step in deepening our scientific understanding of
unlearning and revealing new pathways to improving the state-of-the-art.
1
Introduction
Deep learning models have generated impressive success stories recently by leveraging increasingly
large and data-hungry neural networks that are also increasingly expensive to train. This trend has
led to reusing previously-trained models for a wide range of tasks more than ever before. However,
the heavy reliance of deep models on training data, together with the difficulty of removing data
from trained models after-the-fact, has exacerbated concerns on perpetuating harmful or outdated
information, violating user privacy and other issues. Specifically, deep networks are highly non-
convex, making it difficult to trace (and thus attempt to remove) the effect of a given subset of
training data on the model weights. We are therefore faced with important technical challenges when
it comes to building machine learning pipelines that are performant while efficiently supporting
*Equal senior contribution
Preprint.
arXiv:2406.01257v1  [cs.LG]  3 Jun 2024

deletion requests. Machine unlearning [Nguyen et al., 2022] is a growing field that aims to address
this important issue.
While unlearning is receiving increasing attention [Triantafillou et al., 2023], it is still a young area
of research and the factors affecting the success of different approaches remain poorly-understood.
Understanding what makes an unlearning problem easy or hard is crucial for several reasons. First,
knowledge of behaviours of unlearning algorithms on different types of forgetting requests may
inform which unlearning method to choose for a given request. In fact, for some requests it may
be that all current methods are inadequate, suggesting that one should pay the cost of retraining
from scratch rather than opting for “approximate unlearning” that imperfectly removes information
after-the-fact. Further, deepening our understanding of unlearning can illuminate pathways for
improving both unlearning algorithms as well as evaluation protocols by focusing on relevant factors
that affect difficulty.
To this end, we present the first investigation into different factors that characterize the difficulty
of an unlearning problem. We find that the unlearning problem becomes harder i) the more entan-
gled the retain and forget sets are and ii) the more memorized the forget set examples are. Our
investigation reveals that different unlearning algorithms suffer disproportionately as the difficulty
level increases and surfaces previously-unknown behaviours and failure modes of state-of-the-art
unlearning algorithms. Inspired by our findings, we propose a Refined-Unlearning Meta-algorithm
(RUM) for improving unlearning pipelines. RUM contains two steps: i) a refinement procedure that
divides the given forget set into subsets that are homogeneous with respect to relevant factors that
influence algorithms’ behaviours, and ii) a meta-algorithm that dictates how to unlearn each of those
subsets and compose the resulting models to arrive at one that has unlearned the entire forget set. Our
thorough investigation shows that RUM boosts unlearning performance of several state-of-the-art
algorithms and addresses issues that our investigation of unlearning difficulty has uncovered.
2
Preliminaries
2.1
Unlearning problem formulation
Let θo = A(Dtrain) be the weights obtained by applying a training algorithm A on a training dataset
Dtrain. We will refer to θo as the “original model”. Further, let S ⊆Dtrain denote a subset of the
training data referred to as the “forget set”. For convenience, we will refer to its complement as the
“retain set” R = Dtrain \ S. Informally, the goal of an unlearning algorithm U is to utilize θo, S and
R to produce an unlearned model θu = U(θo, S, R) from which the influence of S is removed.
This idea has been formalized by considering the distributional similarity between the model θu
produced by U and the model θr produced by the optimal unlearning approach: retraining from
scratch on an adjusted training dataset that excludes the forget set: θr = A(Dtrain \ S). Note that
we refer to distributions here since rerunning U and A with different random seeds (that control e.g.
the initialization and the order of mini-batches) will lead to slightly different weights. The ideal
unlearning algorithm then, according to this viewpoint, is one that yields the same distribution of
weights as retraining from scratch. Of course, for an unlearning algorithm to be practical, we would
additionally desire it to be significantly more computationally efficient than retraining the model.
The following definition, borrowed from [Triantafillou et al., 2023] (and similar to [Ginart et al.,
2019]) formalizes this idea in a framework inspired by differential privacy [Dwork, 2006].
Definition 2.1. Unlearning. An unlearning algorithm U is an (ϵ, δ)-unlearner (for A, Dtrain and S)
if the distributions of A(Dtrain \ S) and U(θo, S, Dtrain \ S) are (ϵ, δ)-close.
where we say two distributions µ, ν are (ϵ, δ)-close if µ(B) ≤eϵν(B) + δ and ν(B) ≤eϵµ(B) + δ
for all measurable events B.
According to the above definition, an unlearning algorithm is said to be exact unlearning if it satisfies
the above definition for ϵ = δ = 0, i.e., it yields a distribution of models identical to that of
retraining from scratch. For neural networks, the only known exact solutions involve retraining,
either naively, or in the context of mixtures where one can retrain only a subset of models affected by
the deletion request [Bourtoule et al., 2021]. These approaches unfortunately are inefficient; in the
worst-case, even clever schemes suffer inefficiency similar to naive retraining, and may also yield
poorer performance. To address this, a plethora of approximate unlearning algorithms have been
2

recently proposed, whose ϵ and δ values aren’t known in general, but are substantially more efficient
and may have higher utility.
Evaluating approximate unlearning.
Since the success of (most) approximate unlearning algo-
rithms cannot be proved within tight (ϵ, δ) bounds, the community has considered various empirical
measurements of success, guided by three desiderata: i) good forgetting quality, 2) high utility, and
3) efficiency. An unlearning algorithm thus is faced with a complex balancing act, as there are
well-known trade-offs both between forgetting quality and utility, as well as forgetting quality and
efficiency, and a good unlearning metric should capture these nuances.
Utility and efficiency are straightforward to measure, and, in the context of classifiers, can be
represented by the accuracy on the retain and test sets, and time in seconds, respectively. Measuring
forgetting quality, on the other hand, is more complex and several proxies have been proposed. The
simplest one is to inspect the accuracy on the forget set, with the goal of matching the accuracy
on the forget set that would have been obtained by retraining from scratch. Alternatively, inspired
from privacy literature Carlini et al. [2022a], Mattern et al. [2023], Membership Inference Attacks
(MIAs) have been adopted by the unlearning community Kurmanji et al. [2024], Liu et al. [2024],
Hayes et al. [2024] to measure forgetting quality. In essence, an MIA is designed to infer from the
model’s characteristics (e.g. loss, confidence) whether a data point has been used in training, and then
unlearned, versus was never trained on in the first place. Intuitively, the failure of an attacker to tell
apart unlearned examples from never-seen examples marks a success for the unlearning algorithm in
terms of this metric. We will consider both of these proxies in our experimental investigation.
To holistically evaluate an unlearning algorithm, we desire a single metric that captures both forgetting
quality and utility. We will later introduce a “tug-of-war” metric for this purpose, inspired by
Triantafillou et al. [2023].
2.2
Memorization
Deep neural networks are known to “memorize” (a subset of) their training data, with a recent theory
showing that label memorization is in fact necessary for achieving close-to-optimal generalization
error in classifiers [Feldman, 2020] when the data distribution is long-tailed.
Definition 2.2. Memorization score [Feldman, 2020]. The memorization score for an example
i ∈D, with respect to a training dataset D and training algorithm A is
mem(A, D, i) =
Pr
f∼A(D)[f(xi) = yi] −
Pr
f∼A(D\i)[f(xi) = yi]
(1)
where xi and yi are the feature and label, respectively, of example i.
The first term in the above equation considers models trained on all of D whereas the second term
considers models trained on D excluding example i. Intuitively, the memorization score for an
example i is high if including it in training yields a different distributon of predictions on that example
than excluding it from training would have. Recent works [Feldman, 2020, Feldman and Zhang,
2020, Jiang et al., 2020] identify atypical examples or outliers of the data distribution as examples
that are more highly memorized: if an example has a noisy or incorrect label, the model is required to
memorize it in order to predict it correctly.
3
Related Work
Approximate unlearning algorithms.
A plethora of algorithms have been proposed that aim to
identify effective data scrubbing procedures post-training. We now describe representative methods.
Fine-tune Warnecke et al. [2021], Golatkar et al. [2020] relies on catastrophic forgetting to diminish
the confidence of the original model θo on S. Catastrophic forgetting is induced by simply fine-tuning
on the retain set Dtrain \ S. On the other hand, NegGrad Golatkar et al. [2020], Graves et al.
[2021], Thudi et al. [2022] instead directly maximizes the loss on S. This approach has been found
empirically to cause a large drop in the utility of the model. To address this, NegGrad+ Kurmanji
et al. [2024] combines fine-tuning and gradient ascent, by jointly minimizing the loss function on
the retain set, and maximizing the loss function with respect to the forget set. SCRUB, proposed by
the same authors as NegGrad+, extends the contrastive learning behind NegGrad+ by framing it as
3

a student-teacher problem. Concretely, SCRUB is a bi-optimization algorithm, where the student
aims to mimic the teacher’s behaviour on R and to disobey the teacher’s output with respect to S.
L1-sparse Liu et al. [2024] infuses weight “sparsity” into the unlearning algorithm by fine-tuning on
the retain set with an L1-penalty, drawing inspiration from the model pruning literature Frankle and
Carbin [2018], Ma et al. [2021]. Influence Unlearning Izzo et al. [2021], Koh and Liang [2017]
arrives at the important model’s weights by estimating how removing a data point affects θo via
influence functions Cook and Weisberg [1982], and draws connections to (ϵ, δ)-forgetting Wang et al.
[2022], Guo et al. [2019].
A different line of work is “relabelling-based” methods that trick the model to learn new labels for S.
This can be achieved by finetuning the model with respect to a dataset Drelabel = (XS, Y ), where
XS are the features and labels Y are sampled from a prior distribution of the label space. Saliency
Unlearning (SalUn) Fan et al. [2023] learns Drelabel by optimising only the salient parameters of
the model. Concretely, the authors argue that the model’s weights θo can be decomposed into salient
weights and “intact” model weights, by investigating the weight space with respect to the forget set S
ala Smilkov et al. [2017], Adebayo et al. [2018].
Difficulty of Unlearning.
The closest research to ours is the contemporaneous work of Fan et al.
[2024], where the authors study adversarial unlearning cases, i.e. “worst-case” forget sets. Fan
et al. [2024] arrives at difficult forget sets by solving a bi-level optimization based on fine-tuning (i.e.
catastrophic forgetting). Instead, we arrive at difficult partitions through the lens of interpretable
factors: the degree of entanglement between the retain and forget set and memorization. While the
primary aim of Fan et al. [2024] is to construct more pessimistic evaluation benchmarks, our primary
aim is to deepen our understanding of unlearning problems and of the behaviour of state-of-the-art
algorithms when operating on forget set of different identified characteristics, ultimately improving
unlearning pipelines.
Catastrophic forgetting, atypical examples and privacy.
[Jagielski et al., 2022] and [Toneva et al.,
2018] study catastrophic forgetting during training. [Jagielski et al., 2022] finds that, when training
on large datasets, examples that were only seen early in training may enjoy better privacy, in terms of
MIAs and extraction attacks, compared to examples seen recently. [Toneva et al., 2018] investigate
“forgetting events”, where an example that was previously correctly predicted becomes incorrectly
predicted later in training. They find that examples with noisy labels witness a larger number of these
forgetting events. Further, Carlini et al. [2019] find that models trained with Differential Privacy (DP)
find it primarily hard to correctly predict atypical examples. We build on this literature by studying
the difficulty of unlearning after-the-fact, rather than (passive) forgetting during training and draw
connections to memorization, a notion closely related to atypicality in the data distribution.
4
What Makes Unlearning Hard?
In this section, we identify and empirically examine two factors that affect the difficulty of unlearning.
Before diving in, we first define a simple proxy for unlearning difficulty that we will use in this
section. Our goal is to capture the difficulty of performing the “balancing act” of forgetting S while
retaining the ability to perform well on R and generalize well to the test set. We propose a metric to
capture this “tug-of-war” (ToW) using the relative difference between the accuracies of the unlearned
and the retrained model on the forget, retain and test sets, in a manner inspired by [Triantafillou et al.,
2023].
ToW(θu, θr, S, R, Dtest) = (1 −da(θu, θr, S)) · (1 −da(θu, θr, R)) · (1 −da(θu, θr, Dtest))
where a(θ, D) =
1
|D|
P
(x,y)∈D[f(x; θ) = y] is the accuracy on D of a model f parameterized by
θ and da(θu, θr, D) = |a(θu, D) −a(θr, D)| is the absolute difference between the accuracy of
models θu and θr on D. Therefore, ToW rewards unlearned models that match the accuracy of the
retrained-from-scratch model, on each of the forget, retain, and test sets. ToW ranges from 0 to 1,
with higher values associated with better unlearning.
4

(a) Entanglement Score (ES) vs ToW
(b) Memorization vs ToW
Figure 1: Uncovering two factors that affect unlearning difficulty according to ToW (where higher is
better). Left: the more entangled the retain and forget sets are in the embedding space, the harder it
is to unlearn. Right: the less memorized a forget set is (thus having influenced the model less), the
easier it is to unlearn (for most algorithms). Error bars correspond to 95% confidence intervals from
running each algorithm 3 times (6 times for relabelling-based that had higher variance).
4.1
The more entangled the forget and retain sets are, the harder unlearning becomes
Prior research (e.g., by Feldman et al Feldman [2020] and Carlini et al Carlini et al. [2019]) has tried
to identify prototypical (or atypical) examples and their impact on learning. Primarily, this depended
on the position of examples within the overall data space distribution. In contrast, here we focus
on the embedding space, as unlearning depends heavily on how the model has learned to represent
training data. Furthermore, instead of looking at isolated examples, we delve into how “entangled”
the retain and forget sets are in embedding space. We hypothesize that higher “entanglement” leads to
harder unlearning: if the two sets are highly entangled, attempting to erase S will cause accidentally
erasing R too.
We propose to measure entanglement between the retain and forget sets via the below Entanglement
Score (ES), inspired by a measure previously introduced in [Goldblum et al., 2020] to study learned
representations.
ES(R, S; θo) =
1
|R|
P
i∈R(ϕi −µR)2 +
1
|S|
P
j∈S(ϕj −µS)2
1
2
 (µR −µ)2 + (µS −µ)2
(2)
where ϕi = g(xi; θo) is the embedding of example xi according to the “original model” f, parameter-
ized by θo; where g denotes the forward pass through f up till the penultimate layer, i.e. excluding the
classifier layer. Further, µR =
1
|R|
P
i∈R ϕi is the mean embedding of the retain set, and analogously,
µS the mean embedding of the forget set, while µ is the mean embedding over all of Dtrain = R ∪S.
Intuitively, ES measures entanglement between S and R in the embedding space of the original model
(before unlearning begins). The numerator measures intra-class variance, capturing the tightness of
each of those two sets, independently, while the denominator measures inter-class variance between
those two sets. Higher ES score corresponds to higher entanglement in the embedding space.
Our investigation hinges on generating three different forget/retain partitions with different degrees
of entanglement: low, medium, and high. But, Equation (2) does not directly suggest a procedure
for generating retain/forget partitions with a desired ES score. Hence, we achieved this indirectly
using a proxy. Specifically, let d(i, µ; θo) = ||ϕi −µ||2 denote the l2-distance in the original model’s
embedding space between example i and centroid µ, as defined above. We compute this distance
for each example in Dtrain and sort those examples according to their d-values. We then form each
forget set to contain a contiguous subset of examples from different ranges of that sorted list. We
find that this procedure allows us to construct retain/forget partitions of varying ES values. The ES
values for our low, medium and high partitions are 309.94±98.56, 1076.99±78.64, 1612.21±110.82
for CIFAR-10, and 963.82±113.53, 2831.24±558.63, and 3876.90±426.92 for CIFAR-100. We
include details of this procedure in the Section A.3, and visualizations to confirm the degree of
retain/forget entanglement is in line with the scores we compute. We experiment on two datasets
and two architectures, CIFAR-10/CIFAR-100 and ResNet-18/ResNet-50 with |S| = 3000. Refer to
Section A.2 for implementation details and Section A.6 for more detailed results for both datasets.
We observe from Figure 1a that it is harder to unlearn when the retain and forget sets are more
entangled: all unlearning algorithms have poorer performance for highly-entangled vs lower-entangled
5

settings. Further, we notice that different unlearning algorithms suffer disproportionately as the
entanglement increases. Notably, methods based on relabelling (SalUn and Random-label) perform
very poorly when the entanglement is high. We hypothesize that this is because, if two examples i
and j are close neighbours in embedding space, with i in the forget set and j in the retain set, forcing
example i to be confidently predicted as an incorrect class (as relabelling algorithms do) will also
cause j to be predicted as that incorrect class, too, thus causing a drop in retain accuracy, which is
captured by ToW. This effect will be less pronounced if i and j are far from each other.
4.2
The more memorized the forget examples are, the harder unlearning becomes
Feldman et al Feldman [2020] have already established that models must memorize some atypical
examples in order to perform well. Further, prior literature has also established that noisy examples
(that are more likely to be memorized) witness more “forgetting events” during training (their
predicted label flips to an incorrect one) Toneva et al. [2018] and that models trained with Differential
Privacy (DP), a procedure where noise is added to the gradients (making it harder to memorize), find
it primarily hard to correctly predict atypical examples Carlini et al. [2019]. In this section, we build
upon these prior insights by investigating the connection between the degree of memorization of the
forget set and difficulty of unlearning.
Let’s begin by inspecting Definition 2.2: if an example is not really memorized, the predictions of the
model on that example will not change much whether the example was included in training or not.
This implies that even the original model (no unlearning) is similar to retrain-from-scratch in terms
of predictions on those examples, making unlearning unnecessary or trivial. On the other hand, for
highly-memorized examples, the predictions between the original and retrained models will differ
significantly, implying that an unlearning algorithm has “more work” to do to turn the original model
into one that resembles the retrained one. We now investigate how the level of memorization of the
forget set affects the behaviour of state-of-the-art unlearning algorithms. We hypothesize, based on
our above intuition, that unlearning is easier when the forget set contains less-memorized examples.
To investigate this, we first compute the memorization score mem(A, Dtrain, i) of each example
i ∈Dtrain and we sort all examples according to their scores. We then use that sorted list to
create three different forget sets, corresponding to the lowest N scores (“low-mem”), the highest
N (“high-mem”), and the N that are nearest to 0.5, i.e. the midpoint of the range of memorization
scores (“medium-mem”), where N = 3000. We then apply different unlearning algorithms on each
of these forget sets and compute ToW. We perform this experiment on CIFAR-10 using ResNet-18
and on CIFAR-100 using ResNet-50. Refer to Section A.2 for implementation details.
We first emphasize two key sets of conclusions. First, in terms of ToW, Figure 1b shows that, indeed,
for most algorithms, the lower the memorization level of the forget set, the easier the problem is. In
line with our prior discussion, even the original model performs well on “low-mem”, but performs
very poorly on “high-mem”. Interestingly though, the two relabelling-based algorithms (SalUn and
Random-label) follow an inverse trend: they perform better for higher-memorized forget sets. Second,
breaking down ToW into its parts, we find interesting trends in terms of the forget set accuracy, in
Figure 8. Specifically, for several unlearning algorithms, the forget accuracy for “low-mem” is still
very high after unlearning them, as the model can infer the correct labels for such examples even
when they weren’t included in training; this follows directly by Definition 2.2 if unlearning is done by
retraining, and is shown here for the first time for approximate unlearning algorithms. On the other
hand, we find that, for “high-mem”, different unlearning algorithms can (to varying degrees) cause
the forget set accuracy to drop substantially; this is consistent with both Toneva et al. [2018] and
Carlini et al. [2019], but shown here for the first time for approximate unlearning algorithms. Notably,
we find that relabelling-based algorithms cause a larger drop in the accuracy of the forget set, relative
to other approaches. This benefits ToW in the case of “high-mem” forget sets, where retraining has
poor accuracy on this set (so they get rewarded by matching it), but it hurts on “low-mem”, since
it causes a large discrepancy from retraining, which has high accuracy on this set (since it makes
similar predictions to the original model on this set, by definition, and the original model has high
accuracy on all of Dtrain).
Overall, we have presented the first investigation into the behaviour of unlearning algorithms applied
on forget sets of different degrees of memorization. A key finding is that different algorithms outshine
others for different forget sets. Most notable is the failure of relabelling-based algorithms on the
“low-mem” forget set, which is easy for other algorithms and, in fact, even no unlearning in that case
6

might be an acceptable solution. We intuit that this is due to their aggressive unlearning strategy
yielding “overforgetting” (producing a forget set accuracy that is lower than that of retraining from
scratch) as discussed above. Furthermore, we observe that different unlearning algorithms work best
for the “low-mem”, “medium-mem” and “high-mem” forget sets. Concretely, from Figure 1b we note
that Finetune is best for “medium-mem”, SalUn is best for “high-mem”, and a number of algorithms
are top-performers for “low-mem” (including no unlearning). This reveals a possible pathway for
improving unlearning based on using different algorithms for different forget sets. So, how can one
build on these insights to further improve unlearning algorithms performance?
5
Refined-Unlearning Meta-algorithm (RUM) for Improved Unlearning
Previously, we observed that unlearning algorithms have different behaviours on forget sets with
different properties. For example, while “low mem” forget sets are almost trivial to unlearn (and even
doing nothing may be acceptable), SalUn and Random-label perform poorly on them. On the other
hand, SalUn and Random-label evidently outperform other unlearning algorithms on “high mem”.
These observations suggest that the optimal unlearning algorithm to use is dependent on the properties
of the forget set. One could therefore pick the best unlearning algorithm for each unlearning request,
based on these factors. However, in practical scenarios, forget sets may be distributed differently than
in our preliminary experiments, that were designed to cleanly separate different factors of interest.
Indeed, real-world forget sets would likely contain a mixture of examples from different modes of the
data distribution, some rare or highly-memorized while others common and not memorized at all. So,
what can be done about these expected heterogeneous forget sets? How can our insights above be
leveraged to improve unlearning for such cases?
Figure 2: Overview of RUM.
To address this, we first propose a refinement
procedure that divides forget sets into homoge-
neous subsets (with respect to the factors that we
have found to affect the difficulty of unlearning
and behaviours of existing algorithms). Second,
we propose to utilize a pool of state-of-the-art
algorithms to unlearn different subsets. Put to-
gether, we propose a Refined-Unlearning Meta-
algorithm (RUM), comprised of two steps: 1)
Refinement and 2) Meta-unlearning. Figure 2
overviews RUM.
Step 1: Refinement.
We introduce a function
F that partitions the forget set S into K subsets:
{Si}K
i=1 = F(S) such that each forget set ex-
ample appears in exactly one such subset. The
intention of F is to generate homogeneous subsets w.r.t factor(s) that affect difficulty / algorithm’s
behaviours.
Step 2: Meta-Unlearning.
Having obtained the subsets {Si}K
i=1 of S, we now require a “meta-
algorithm” M that dictates how to perform the individual unlearning requests and how to compose
the resulting unlearned models to arrive to a model that has unlearned all of S. In this work, we focus
on meta-algorithms that tackle unlearning of subsets in a sequence, leaving other designs for future
work. It therefore remains for our “meta-algorithm” to decide: i) what unlearning algorithm to apply
for each subset, ii) what order should the unlearning requests be executed in.
More concretely, we assume access to a pool of existing unlearning algorithms U1 . . . UN, like the
ones described in related work, for instance. Let MU(Si) denote a procedure that takes as input a
subset Si and returns an unlearning algorithm U ∈{U1 . . . UN} that will be used for that subset. This
selection can be done by leveraging insights such as those in Section 4. Further, let MO denote a
procedure that takes as input the K subsets of S and returns a sorted list S′ = MO(F(S)) containing
the K subsets in the desired order of execution.
Given the above ingredients, RUM proceeds by executing K unlearning requests in a sequence, with
step i of that sequence corresponding to unlearning subset S′[i] by applying Ui(θo, S′[i], Ri) = θu
i ,
where θu
i denotes the unlearned model up to step i and Ui = MU(S′[i]) and Ri = R ∪{S′[i +
7

(a) CIFAR-10 with ResNet-18
(b) CIFAR-10 with ResNet-18
(c) CIFAR-100 with ResNet-50
Figure 3: From subplots a and c, we observe that RUMF improves each unlearning algorithm.
Vanilla corresponds to unlearning S in one go, whereas Shuffle and RUMF operate sequentially on 3
subsets of S. In the case of RUMF, the 3 subsets are the result of applying F and the order is low →
medium →high, whereas Shuffle uses equal-sized random subsets, serving as a control experiment.
Further, from subplot b, we observe that full RUM, equipped with the best algorithm for each
subset (do nothing →Fine-tune →SalUn), yields the overall best results (note: in CIFAR-100,
NegGrad+ is the best algorithm, so full RUM corresponds to the RUMF variant of NegGrad+).
1], . . . S′[K]} is the retain set for step i, containing R as well as all other subsets of S that have not
yet been unlearned in the sequence so far. We finally return the unlearned model of the last step θu
K.
Our RUM framework is meant as an analysis framework, surfacing new problems to be solved and
offering new pathways into future state-of-the-art algorithms. Nonetheless, we contribute below
specific top-performing RUM instantiations, with specific choices for F and for M.
6
Experimenting with RUM flavours
We now present RUM instantiations using a refinement strategy based on memorization scores and
experimental evaluations answering the following questions: Q1: How useful is refinement alone?
That is, for a given unlearning algorithm U, does applying U sequentially on the K homogeneous
subsets of S outperform applying U once on all of S? Q2: Can we obtain further gains by additionally
selecting the best-performing unlearning algorithm for each forget set subset? Q3: Are there
interpetable factors behind the boost obtained by sequential unlearning of homogeneous subsets?
CIFAR-10
CIFAR-100
ToW (↑)
MIA gap (↓)
ToW (↑)
MIA gap (↓)
Retrain
1.000±0.000
0.000
1.000±0.000
0.000
Fine-tune vanilla
0.849±0.030
0.120
0.734±0.025
0.139
Fine-tune shuffle
0.712±0.040
0.098
0.589±0.036
0.345
Fine-tune RUMF
0.937±0.052
0.099
0.784±0.040
0.093
L1-sparse vanilla
0.794±0.035
0.175
0.824±0.011
0.089
L1-sparse shuffle
0.716±0.023
0.257
0.604±0.023
0.353
L1-sparse RUMF
0.900±0.020
0.072
0.883±0.046
0.033
NegGrad+ vanilla
0.802±0.028
0.230
0.861±0.069
0.159
NegGrad+ shuffle
0.632±0.022
0.520
0.613±0.054
0.417
NegGrad+ RUMF
0.879±0.068
0.134
0.921±0.034
0.059
SalUn vanilla
0.731±0.070
0.374
0.545±0.061
0.372
SalUn shuffle
0.727±0.030
0.234
0.538±0.019
0.237
SalUn RUMF
0.887±0.069
0.031
0.614±0.037
0.181
RUM
0.965±0.014
0.034
0.921±0.034
0.059
Table 1: Each algorithm U is applied in three
ways: i) in one-go (“vanilla”), ii) on a random
partition of S into 3 equal-sized subsets, se-
quentially (“shuffle”), and iii) on three equal-
sized subsets obtained by F in low →med →
high order (“RUMF”). In last row, RUM ad-
ditionally chooses the best algorithm for each
subset: none →Fine-tune →SalUn in CIFAR-
10 and NegGrad+ RUMF in CIFAR-100.
Experimental setup
We experiment with a re-
finement strategy based on memorization scores
where K = 3. Specifically, we study unlearning
a forget set S that is the union of the three sets
containing the N lowest, the N closest to 0.5, and
the N highest memorized examples in the dataset,
where N = 1000, making the overall size of the
forget set 3000. We conduct experiments in two dif-
ferent datasets: CIFAR-10 and CIFAR-100, using
ResNet-18 and ResNet-50, respectively. We evalu-
ate unlearning algorithms both in terms of ToW, as
before, and using a commonly-used MIA [Fan et al.,
2023, Liu et al., 2024] that is a binary classifier
trained to separate R from Dtest and then queried
on examples from S. Following previous work, we
report as “MIA” the fraction of examples from S
that were predicted to be held-out. The goal is to
match the “MIA” score of retrain-from-scratch. For
convenience, we report the “MIA gap”: the absolute
difference of the MIA score of unlearning from the
MIA score of retrain-from-scratch (lower is better).
See Section A.4 for details. We also consider and
analyze the effect of two different orderings: i) low
→medium →high and i) high →medium →low.
8

(a) low →med →high (MIA gap: 0.031)
(b) high →med →low (MIA gap: 0.221)
Figure 4: Sequence dynamics for SalUn RUMF on CIFAR-10. We report the accuracy on overall S,
R and Dtest, and subsets of S after each step. Both orderings yield similar Tow (see Table 11a).
How useful is refinement alone?
To investigate this, we selected a subset of highest-performing
algorithms and apply each algorithm U in three ways: i) “vanilla”, i.e. applying U(θo, S, R) as
usual, ii) “shuffle” where U is applied sequentially on three equal-sized subsets of S that were
determined randomly, and iii) RUMF, where U is applied sequentially on the subsets obtained by
F(S), i.e. utilizing only the refinement step of RUM and applying the same U on each subset.
We include “shuffle” as a control experiment, so that any gain of RUMF over shuffle is due to
homogenization rather than simply reducing the size of the forget set or other effects of sequential
unlearning. We observe from Figure 3 and Tables 11a and 11b that, for four different unlearning
algorithms and on two different datasets, RUMF significantly outperforms “vanilla” and “shuffle”,
indicating that operating sequentially on homogenized subsets according to memorization can boost
the performance of unlearning algorithms. Interestingly, in most cases, “shuffle” actually performs
worse than “vanilla”, indicating that the difficulty of the problem may increase rather than decrease
given a poor refinement strategy.
Can we further boost performance by per-subset algorithm selection?
To answer this, we
leverage our findings from Section 4 to identify the best unlearning algorithm for each of the “low-
mem”, “medium-mem” and “high-mem” scenarios. On CIFAR-10, this corresponds to doing nothing
for low-mem (i.e. using the original model directly), using Fine-tune for medium-mem and SalUn for
high-mem (based on Figure 1b). From Figure 3b (and Table 11a), we observe that we get the overall
best results by far, both in terms of ToW and MIA, by applying RUM with “nothing →Fine-tune →
SalUn”, demonstrating the value of incorporating our insights into unlearning pipelines. In fact, lets
revisit our previous observation that SalUn and Random-Label perform uncharacteristically poorly
on the “low-mem” forget set. In line with our hypothesis, we notice that “nothing →SalUn →SalUn”
outperforms applying SalUn on all three subsets (a.k.a. SalUn RUMF). Note that, for CIFAR-100,
the best algorithm for all subsets is NegGrad+, so full RUM corresponds to NegGrad+ RUMF.
Analysis of sequence dynamics
We report ToW and MIA with different orderings in Tables
11a and 11b. We find that, while ToW is similar for different orderings, MIA can vary. To better
understand these dynamics, we inspect the accuracies on S and its subsets after each step in Figure
4 for SalUnF on CIFAR-10 and in Figure 9 for NegGrad+F CIFAR-100. The former reveals why
SalUnF with low →med →high order greatly outperforms vanilla SalUn. Recall that we identified
in Section 4.2 that SalUn “overforgets” low-mem examples (its forget accuracy is lower than that of
retraining). We observe from Figure 4 that future steps of the sequence neutralize that overforgetting
effect on low-mem, leading to better ToW (see Figure 3). Interestingly, in line with our previous
insights (Section 4.2), we find (from both Figures 4 and 9) that it is hard to cause the “low-mem”
accuracy to become low and stay low. NegGrad+ does not drop it for any order of execution; SalUn
drops it in the first ordering, but that drop is later reversed. We leave it to future work to further
study these sequential dynamics and their helpful or harmful effects on Tow and MIA. The fact that
MIA results differ based on the sequence may tie in with the recently-identified “privacy onion effect”
Carlini et al. [2022b].
7
Discussion and conclusion
We presented the first investigation into interpretable factors that affect the difficulty of unlearning.
We found that unlearning gets harder i) the more entangled the forget and retain sets are in embedding
space and ii) the more memorized the forget set is. Our investigation led into uncovering previously-
9

unknown behaviours of state-of-the-art algorithms that were not surfaced when considering random
forget sets: when do they fail, when do they excel and when do they exhibit different trends from
each other. Notably, we discovered that relabelling-based methods suffer disproportionately as the
embedding-space entanglement increases, and exhibit a reverse trend compared to other methods
in their behaviour for different memorization levels. Armed with these insights, we then proposed
the RUM framework surfacing new (sub)problems within unlearning, whose solution may lead
to greater performance. Finally, we derived specific instantiations of RUM and analyzed how its
different components can improve performance. We found that sequential unlearning of homogenized
forget set subsets improves all considered state-of-the-art unlearning algorithms and investigated the
dynamics of sequential unlearning to glean insights as to why that is. We also found that we can
further boost performance by selecting the best unlearning algorithm per subset.
Efficiency
How is this important aspect affected in our sequential framework? We remark that it
depends on the unlearning algorithm. For instance, applying Fine-tune three times is much more
expensive than applying it once, because Fine-tune performs (at least) one epoch over the entire
retain set. But for other algorithms the overall cost does not increase significantly. Further, a key
observation from our results is that we can do well by actually doing nothing on a subset of the forget
set, which can really boost efficiency (especially since the vast majority of examples are “low mem”).
Data-space vs embedding-space outliers
How does the embedding space entanglement interact
with the level of memorization of the forget set? We analyzed this and found in Table 3 that all of our
memorization buckets have relatively high ES, indicating that separating out (data-space) outliers in
the forget set doesn’t lead to lower entanglement between the two sets in the embedding space. We
leave it to future work to study the interaction of these factors.
Limitations and future work
We hope future work explores other refinement strategies (e.g. for
a notion that captures embedding-space entanglement), designing refinement and meta-unlearning
strategies that are practical, e.g. leveraging cheap proxies for quantities like memorization scores,
investigating privacy implications of sequential RUM, e.g. in terms of the “privacy onion effect”
Carlini et al. [2022b]. We also hope to see how our RUM framework can be adopted and adapted for
unlearning in LLMs, especially given the findings from the contemporaneous paper Barbulescu and
Triantafillou [2024] where unlearning is performed only on the highest-memorized examples in the
forget set (albeit, memorization is defined differently for LLMs). We hope our framework continues
to enable progress in understanding and improving unlearning and that our identified factors of
difficulty and associated behaviours of existing algorithms continue to improve the state-of-the-art
and inform the development of strong evaluation metrics that consider forget sets that vary in terms
of relevant identified characteristics.
8
Acknowledgements
We thank Vincent Dumoulin and Fabian Pedregosa for valuable conversations and feedback at various
stages of the project.
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity
checks for saliency maps. Advances in neural information processing systems, 31, 2018.
George-Octavian Barbulescu and Peter Triantafillou. To each (textual sequence) its own: Improving
memorized-data unlearning in large language models. arXiv preprint arXiv:2405.03097 (to appear
in ICML 2024), 2024.
Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers,
Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE Symposium
on Security and Privacy (SP), pages 141–159. IEEE, 2021.
Nicholas Carlini, Ulfar Erlingsson, and Nicolas Papernot. Distribution density, tails, and outliers in
machine learning: Metrics and applications. arXiv preprint arXiv:1910.13427, 2019.
10

Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer.
Membership inference attacks from first principles. In 2022 IEEE Symposium on Security and
Privacy (SP), pages 1897–1914. IEEE, 2022a.
Nicholas Carlini, Matthew Jagielski, Chiyuan Zhang, Nicolas Papernot, Andreas Terzis, and Florian
Tramer. The privacy onion effect: Memorization is relative. Advances in Neural Information
Processing Systems, 35:13263–13276, 2022b.
R Dennis Cook and Sanford Weisberg. Criticism and influence analysis in regression. Sociological
methodology, 13:313–361, 1982.
Cynthia Dwork. Differential privacy. In International colloquium on automata, languages, and
programming, pages 1–12. Springer, 2006.
Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Em-
powering machine unlearning via gradient-based weight saliency in both image classification and
generation. arXiv preprint arXiv:2310.12508, 2023.
Chongyu Fan, Jiancheng Liu, Alfred Hero, and Sijia Liu. Challenging forgets: Unveiling the
worst-case forget sets in machine unlearning. arXiv preprint arXiv:2403.07362, 2024.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Proceedings
of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, pages 954–959, 2020.
Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long
tail via influence estimation. Advances in Neural Information Processing Systems, 33:2881–2891,
2020.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data
deletion in machine learning. Advances in neural information processing systems, 32, 2019.
Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:
Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 9304–9312, 2020.
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Gold-
stein. Unraveling meta-learning: Understanding feature representations for few-shot tasks. In
International Conference on Machine Learning, pages 3607–3616. PMLR, 2020.
Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, volume 35, pages 11516–11524, 2021.
Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal
from machine learning models. arXiv preprint arXiv:1911.03030, 2019.
Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inexact
unlearning needs more careful evaluations to avoid a false sense of privacy. arXiv preprint
arXiv:2403.01218, 2024.
Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion
from machine learning models. In International Conference on Artificial Intelligence and Statistics,
pages 2008–2016. PMLR, 2021.
Matthew Jagielski, Om Thakkar, Florian Tramer, Daphne Ippolito, Katherine Lee, Nicholas Carlini,
Eric Wallace, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, et al. Measuring forgetting of
memorized training examples. arXiv preprint arXiv:2207.00099, 2022.
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural
regularities of labeled data in overparameterized models. arXiv preprint arXiv:2002.03206, 2020.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
International conference on machine learning, pages 1885–1894. PMLR, 2017.
11

Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded
machine unlearning. Advances in Neural Information Processing Systems, 36, 2024.
Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA, Sijia Liu,
et al. Model sparsity can simplify machine unlearning. Advances in Neural Information Processing
Systems, 36, 2024.
Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai
Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning ticket
really win the jackpot? Advances in Neural Information Processing Systems, 34:12749–12760,
2021.
Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schölkopf, Mrinmaya Sachan,
and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neigh-
bourhood comparison. arXiv preprint arXiv:2305.18462, 2023.
Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin,
and Quoc Viet Hung Nguyen. A survey of machine unlearning. arXiv preprint arXiv:2209.02299,
2022.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Under-
standing factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on
Security and Privacy (EuroS&P), pages 303–319. IEEE, 2022.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and
Geoffrey J Gordon. An empirical study of example forgetting during deep neural network learning.
arXiv preprint arXiv:1812.05159, 2018.
Eleni Triantafillou, Fabian Pedregosa, Jamie Hayes, Peter Kairouz, Isabelle Guyon, Meghdad Kur-
manji, Gintare Karolina Dziugaite, Peter Triantafillou, Kairan Zhao, Lisheng Sun Hosoya, Julio
C. S. Jacques Junior, Vincent Dumoulin, Ioannis Mitliagkas, Sergio Escalera, Jun Wan, Sohier
Dane, Maggie Demkin, and Walter Reade. Neurips 2023 - machine unlearning, 2023. URL
https://kaggle.com/competitions/neurips-2023-machine-unlearning.
Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative
pruning. In Proceedings of the ACM Web Conference 2022, pages 622–632, 2022.
Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning
of features and labels. arXiv preprint arXiv:2108.11577, 2021.
A
Appendix / supplemental material
A.1
Broader impact
Unlearning research can have profound broader impact in allowing users to request their data to be
deleted from models or making models safer or more accurate by removing harmful or outdated
information. Our work is exploratory: we identify factors affecting difficulty that are interpretable
and can drive improvements to unlearning pipelines. As such, we don’t see any direct harmful impact
of our work.
A.2
Implementation details
Datasets and models
We use CIFAR-10 and CIFAR-100 datasets for our evaluation. The CIFAR-
10 dataset consists of 60,000 32x32 color images, including 10 classes of 6000 images each. The
CIFAR-100 dataset has 100 classes, each containing 600 32x32 color images. In our experiments, the
sizes of train, validation, and test sets are 45000, 5000, and 10000 respectively. We use ResNet-18
and ResNet-50 as model architectures. Specifically, ResNet-18 is used for CIFAR-10 and ResNet-50
is used for CIFAR-100, respectively.
12

Training details for original models
We trained a ResNet-18 model on the CIFAR-10 dataset and
a ResNet-50 model on the CIFAR-100 dataset. For CIFAR-10, the ResNet-18 model was trained
for 30 epochs using the SGD optimizer. The learning rate was initialized at 0.1 and scheduled
with a cosine decay. We used a weight decay of 0.0005, a momentum of 0.9, and a batch size of
256. For CIFAR-100, the ResNet-50 model was trained for 150 epochs using the SGD optimizer.
The learning rate was initialized at 0.1 and decayed by a factor of 0.2 at 60 and 120 epochs. We
applied a weight decay of 0.0005, a momentum of 0.9, and a batch size of 256. Additionally, data
augmentations, including random cropping and random horizontal flipping, were employed during
training on CIFAR-100. All models were trained on Nvidia RTX A5000 GPUs.
Training details for machine unlearning
To ensure optimal performance, we carefully set the
hyperparameters for each unlearning method across different datasets and architectures. For retrain-
from-scratch, we followed the exact same training procedure as the original model, but only trained
on the retain set, excluding the forget set. For Fine-tune, we trained the model for 10 epochs with
a learning rate in the range [0.01, 0.1]. L1-sparse was run for 10 epochs with a learning rate in the
range [0.005, 0.1] and a sparsity-promoting regularization parameter γ in the range [10−6, 10−4].
NegGrad was also trained for 10 epochs, with a learning rate in the range [10−4, 0.1]. For NegGrad+,
we used a β value in the range [0.85, 0.99] as a weighting factor that balances two components of the
loss, training for 5 epochs with a learning rate in the range [0.01, 0.05]. Influence unlearning involved
tuning the parameter α for the woodfisher Hessian Inverse approximation within the range [1, 100].
SalUn was trained for 5-10 epochs with a learning rate in the range [0.005, 0.1] and sparsity ratios in
the range [0.3, 0.7]. Random-label was trained for 10 epochs with a learning rate in the range [0.01,
0.1].
In our experiments with forget / retain set partitions at varied levels of memorization or ES, we tuned
the hyperparameters to achieve the best ToW performance for each unlearning algorithm. The results
are reported as averages with 95% confidence intervals over 3 runs, except for relabeling-based
methods, which had higher variance and were therefore run 6 times. For the RUM experiment, we
adjusted the hyperparameters at each step to ensure that the accuracy after each step closely matched
the accuracy obtained by retraining from scratch. This procedure was repeated for all algorithms,
with results reported as averages over 3 runs with 95% confidence intervals.
A.3
Procedure for creating retain / forget partitions with varying ES
To create retain / forget partitions with varied levels of ES, we followed a systematic procedure.
Initially, we trained the original model θo on the entire training dataset Dtrain. Using θo, we then
extracted embeddings for each data point in Dtrain. The global centroid for Dtrain, denoted as µ,
was determined by calculating the mean of all example embeddings. For each example i in Dtrain,
we then computed its l2-distance from the global centroid µ in the original model’s embedding space
as follows:
d(i, µ; θo) = ||ϕi −µ||2
(3)
We ranked these distances for all data examples in Dtrain and selected the 3000 examples with the
highest distances to form the low ES bucket. Subsequently, we moved further down the ranked list,
selecting 3000 examples with progressively lower distances to form the medium and high ES buckets,
until we achieved the desired levels of ES variation. This approach allowed us to form forget sets,
each with a size of 3000, categorized into low, medium, and high ES levels. The rationale behind
this selection is that examples with high distances from the global centroid are considered “distant"
from the overall data distribution in the embedding space and are therefore less entangled with the
rest of the dataset, i.e., the retain set. Various unlearning algorithms were then deployed on θo across
different forget / retain partitions. Their performance was measured using ToW along with forget,
retain, and test accuracy, as well as MIA.
This procedure enabled us to create retain / forget partitions with varying ES values. The ES values
for our low, medium, and high ES partitions are shown in Table 2. As observed from the table, the
ES values increase from low to high ES partitions for both CIFAR-10 and CIFAR-100, confirming
the effectiveness of our procedure.
13

Additionally, Figure 5 presents the data representation of low, medium, and high ES partitions,
confirming that the degree of entanglement between the retain and forget sets aligns with the
computed ES values. As we move from low to high ES partitions, the forget set (yellow) and the
retain set (blue) become increasingly entangled. This indicates that higher ES partitions reflect greater
complexity in distinguishing between the two sets.
Low ES
Medium ES
High ES
ES value
309.94±98.56
1076.99±78.64
1612.210±110.82
(a) CIFAR-10
Low ES
Medium ES
High ES
ES value
963.82±113.53
2831.24±558.63
3876.90±426.92
(b) CIFAR-100
Table 2: ES values for the low, medium and high forget / retain partitions for CIFAR-10 and CIFAR-
100.
(a) low ES
(b) medium ES
(c) high ES
Figure 5: Data representation visualization for forget / retain partitions with low, medium, and high
ES. We used PCA to reduce the dimensionality for visualization. In each figure, the data examples
from the forget set are shown in yellow, while those from the retain set are in blue. The global
centroid is marked in red at the center of the figures.
A.4
Description of MIA
We adopted a MIA based on prediction confidence, following the procedure described by Liu et al.
[2024]. To conduct this attack, we first sampled equal-sized data from the retain set and the test
set, using these to train a binary classifier as the MIA model. This model is designed to distinguish
whether a data example was involved in the training stage or not.
Next, we applied this attack model to the forget set to evaluate unlearning performance during the
testing phase, after an unlearning method was implemented. Intuitively, for successful unlearning,
we want forget set examples to be classified as “non-training” data. We define “training" data as
the positive class and “non-training" data as the negative class, and measured the performance of
MIA by calculating the ratio of true negatives (i.e., the number of the forgetting samples predicted as
non-training examples) predicted by the MIA model to the total size of the forget set, as shown in (4),
following the same procedure as prior work Liu et al. [2024].
MIA Performance = TNS
|S| ,
(4)
In this context, S ⊆Dtrain represents the forget set and |S| is the total size of this set. The term TNS
denotes the number of true negatives predicted by the MIA model, indicating examples identified as
“non-training” data. This metric ranges from 0 to 1, with higher values signifying a larger portion
14

of the forget set predicted as “non-training” data. The ideal MIA score, for an unlearing algorithm,
is one that matches the MIA score of retraining-from-scratch. Note that, even if applying this MIA
on retrain-from-scratch, some portion of the forget set will be classified as “training data” due to
heavily resembling the retain set (e.g. examples that are not really memorized may have similar
confidences regardless on whether or not they were trained on). And we want the model confidences
of the unlearned model to resemble as closely as possible those of retrain-from-scratch. For this
reason, we additionally report the “MIA gap” (the absolute differentce between the MIA score for
unlearning compared to that of retrain-from-scratch) as a easier-to-interpret metric, where lower is
better, and the ideal score there is 0.
A.5
Data-space vs embedding-space outliers
Building on our discussion in Section 4, we identify two factors that affect the difficulty of unlearning:
the entanglement between the forget and retain sets, and the memorization level of the forget examples.
This raises the question: how do these two factors interact with each other? Are the outliers in data
space the same as those in embedding space? Our findings in this section suggest that these factors
are indeed distinct.
Memorization within ES-Based Partitions
We analyzed the memorization scores within each for-
get set categorized by different ES values. Figure 6 shows the distribution and average memorization
scores in the forget sets of low, medium, and high ES categories. It can be seen that each ES category’s
forget set contains a mix of memorization levels. Specifically, while the mean memorization score
of forget examples increases from low to high ES categories, the memorization scores still span the
entire range from 0 to 1.
(a) low ES
(b) medium ES
(c) high ES
CIFAR-10 with ResNet-18
(d) low ES
(e) medium ES
(f) high ES
CIFAR-100 with ResNet-50
Figure 6: Memorization distribution for low, medium, high ES forget / retain partitions. The mean
memorization score for low, medium, high ES partitions are 0.084±0.203, 0.134±0.235, 0.390±0.326
for CIFAR-10, and 0.159±0.283, 0.222±0.329, and 0.317±0.364 for CIFAR-100.
Entanglement within Memorization-Based Partitions
We examined the ES values for each mem-
orization category to understand the entanglement between the forget and retain sets in the embedding
space. Table 3 presents the ES values corresponding to low, medium, and high memorization buckets.
The findings reveal that all the memorization bucket exhibits relatively high ES, suggesting that
separating outliers in the data space does not reduce the entanglement between the forget and retain
sets in the embedding space.
15

Low memorization
Medium memorization
High memorization
ES value
21134.127
32785.711
14736.591
(a) CIFAR-10
Low memorization
Medium memorization
High mamorization
ES value
30028.924
58683.180
20528.561
(b) CIFAR-100
Table 3: ES values for forget / retain partitions across varied memorization levels for CIFAR-10 and
CIFAR-100.
These findings demonstrate that embedding space entanglement and the memorization level of the
forget set are distinct concepts, not merely different aspects of the same phenomenon.
A.6
Detailed results
In this section, we show the results that were used to construct all figures in the paper, as well as
additional results and analyses.
Entanglement of forget and retain sets affects unlearning difficulty
The first factor affecting
unlearning difficulty, as discussed in Section 4.1, is the degree of entanglement between the forget
and retain sets in the embedding space. Specifically, the more entangled these sets are, the harder
it becomes to unlearn. We present detailed results on how this factor affects unlearning difficulty
in Table 4, 5, 6, and Figure 7. Table 4 displays the ToW results for different ES partitions for both
CIFAR-10 and CIFAR-100. Comprehensive data, including forget, retain, test accuracy, and MIA
performance, are provided in Figure 7, as well as in Table 5 for CIFAR-10 and Table 6 for CIFAR-100.
All the results are averaged over 3 runs for each algorithm (6 runs for relabelling-based algorithms
due to their higher variance), along with 95% confidence intervals.
(a) CIFAR-10 with ResNet-18
(b) CIFAR-100 with ResNet-50
Figure 7: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varied
ES, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the ES increases, there is a
notable decline in forget accuracy and a significant rise in MIA performance. This trend indicates
that as the forget and retain sets become more entangled, more information from the forget set is
effectively removed after unlearning.
16

Low ES
Medium ES
High ES
Retrain
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
Original
0.944 ± 0.014
0.928 ± 0.022
0.759 ± 0.048
Fine-tune
0.952 ± 0.024
0.923 ± 0.019
0.908 ± 0.018
L1-sparse
0.945 ± 0.013
0.926 ± 0.019
0.836 ± 0.031
NegGrad
0.929 ± 0.030
0.887 ± 0.047
0.766 ± 0.042
NegGrad+
0.941 ± 0.029
0.920 ± 0.038
0.800 ± 0.029
Influence Unlearning
0.928 ± 0.023
0.890 ± 0.012
0.744 ± 0.029
SalUn
0.783 ± 0.031
0.656 ± 0.046
0.618 ± 0.056
Random-label
0.767 ± 0.107
0.663 ± 0.055
0.443 ± 0.087
(a) CIFAR-10 with ResNet-18.
Low ES
Medium ES
High ES
Retrain
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
Original
0.836 ± 0.043
0.768 ± 0.036
0.690 ± 0.061
Fine-tune
0.878 ± 0.014
0.852 ± 0.006
0.773 ± 0.029
L1-sparse
0.867 ± 0.039
0.803 ± 0.025
0.709 ± 0.057
NegGrad
0.755 ± 0.027
0.707 ± 0.039
0.666 ± 0.060
NegGrad+
0.922 ± 0.055
0.844 ± 0.014
0.766 ± 0.022
Influence Unlearning
0.806 ± 0.027
0.756 ± 0.041
0.668 ± 0.037
SalUn
0.835 ± 0.037
0.704 ± 0.028
0.678 ± 0.042
Random-label
0.675 ± 0.029
0.629 ± 0.031
0.585 ± 0.014
(b) CIFAR-100 with ResNet-50.
Table 4: ToW for different unlearning algorithms applied to forget / retain sets with varying ES,
evaluated on CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. Across all algorithms,
including the baseline without any unlearning performed (denoted as “Original"), we observe that
ToW decreases from low to high ES partitions, indicating that unlearning becomes harder as the
forget and retain sets become more entangled.
Memorization of forget examples affects unlearning difficulty
In Section 4.2, we discussed
another critical factor affecting unlearning difficulty: the memorization level of the forget examples.
Specifically, when a forget set consists of examples that are more highly memorized by the model,
it becomes more difficult to unlearn (for most algorithms). The detailed results that support this
observation are presented in Table 7, Table 8, Table 9, and Figure 8.
Table 7 illustrates the ToW for forget sets with varying memorization levels. Additionally, Figure 8
and the tables for CIFAR-10 (Table 8) and CIFAR-100 (Table 9) provide extensive data on forget,
retain, and test accuracy, as well as MIA performance. For all the experiments, the average results are
reported with 95% confidence intervals over 6 runs for relabelling-based algorithms and 3 runs for
others.
RUM experiments
We present the details of our RUM experiment in this section. Table 10 presents
the distribution of examples in the forget set for each class in the RUM experiments for both CIFAR-
10 and CIFAR-100 datasets. The size of the forget set is 3000 in all the experiments. For CIFAR-10,
the table lists the number of forget set examples for each class. For CIFAR-100, the table arranges
the number of forget set examples in a 20x5 format for better readability. As shown in Table 10, the
forget set covers examples from all classes in both CIFAR-10 and CIFAR-100 experiments.
Tables 11 provide detailed results when applying RUM to different unlearning algorithms for CIFAR-
10 using ResNet-18 and CIFAR-100 using ResNet-50. We selected the top-performing algorithms
from previous experiments (i.e., Fine-tune, L1-sparse, NegGrad+, SalUn) and applied the refinement
strategy to each algorithm in three ways: i) “vanilla": unlearning S in one go, ii)“shuffle": sequentially
applying the algorithm on 3 equal-sized random subsets, serving as a control experiment, iii)“RUMF":
sequentially applying the algorithm on 3 subsets of S in low →med →high memorization order.
17

Forget Acc
Retain Acc
Test Acc
MIA
Retrain
95.078 ± 0.995
100.000 ± 0.003
84.040 ± 1.387
0.100 ± 0.010
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.000 ± 0.000
Fine-tune
98.522 ± 3.296
99.675 ± 1.385
83.107 ± 3.862
0.035 ± 0.074
L1-sparse
100.000 ± 0.000
99.994 ± 0.022
83.787 ± 3.632
0.001 ± 0.001
NegGrad
99.800 ± 0.461
99.853 ± 0.429
81.740 ± 5.270
0.006 ± 0.007
NegGrad+
99.500 ± 0.647
99.988 ± 0.051
82.743 ± 6.193
0.007 ± 0.007
Influence unlearning
100.000 ± 0.000
99.964 ± 0.154
81.667 ± 2.547
0.000 ± 0.001
SalUn
88.400 ± 6.843
91.442 ± 2.438
75.833 ± 0.725
0.696 ± 0.172
Random-label
81.100 ± 10.427
95.396 ± 0.682
77.470 ± 4.529
0.908 ± 0.026
(a) Low ES
Forget Acc
Retain Acc
Test Acc
MIA
Retrain
94.067 ± 0.722
100.000 ± 0.000
84.167 ± 1.243
0.147 ± 0.009
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.000 ± 0.000
Fine-tune
96.678 ± 3.525
98.655 ± 2.575
80.210 ± 3.004
0.078 ± 0.019
L1-sparse
99.978 ± 0.096
99.990 ± 0.044
83.580 ± 3.270
0.005 ± 0.014
NegGrad
96.300 ± 4.774
96.527 ± 4.294
78.163 ± 4.326
0.074 ± 0.057
NegGrad+
93.200 ± 4.241
98.907 ± 1.839
78.580 ± 4.404
0.087 ± 0.041
Influence unlearning
99.889 ± 0.172
99.372 ± 0.774
79.260 ± 0.774
0.007 ± 0.005
SalUn
68.044 ± 7.008
96.222 ± 6.649
76.380 ± 6.843
0.739 ± 0.270
Random-label
73.922 ± 3.527
92.244 ± 7.643
74.157 ± 10.427
0.607 ± 0.316
(b) Medium ES
Forget Acc
Retain Acc
Test Acc
MIA
Retrain
77.300 ± 3.130
100.000 ± 0.003
83.653 ± 2.280
0.562 ± 0.026
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.000 ± 0.001
Fine-tune
81.289 ± 2.298
98.371 ± 1.395
79.763 ± 1.587
0.375 ± 0.066
L1-sparse
83.022 ± 14.615
95.164 ± 10.943
76.943 ± 6.159
0.357 ± 0.075
NegGrad
96.756 ± 3.705
97.925 ± 3.977
80.927 ± 6.576
0.066 ± 0.052
NegGrad+
95.222 ± 2.993
99.966 ± 0.065
81.447 ± 3.720
0.126 ± 0.055
Influence unlearning
96.111 ± 6.259
98.161 ± 3.205
77.093 ± 5.087
0.236 ± 0.044
SalUn
40.722 ± 11.043
99.967 ± 0.080
81.250 ± 3.335
0.951 ± 0.077
Random-label
31.656 ± 5.551
91.871 ± 0.275
72.273 ± 1.895
0.879 ± 0.055
(c) High ES
Table 5: Accuracy and MIA performance for different unlearning algorithms applied to forget / retain
sets with varying ES for CIFAR-10 using ResNet-18.
18

Forget Acc
Retain Acc
Test Acc
MIA
Retrain
84.222 ± 3.734
99.968 ± 0.012
75.493 ± 0.756
0.296 ± 0.041
Original
100.000 ± 0.000
99.959 ± 0.015
75.003 ± 2.809
0.000 ± 0.000
Fine-tune
90.044 ± 8.075
99.148 ± 1.639
69.577 ± 5.366
0.231 ± 0.067
L1-sparse
84.611 ± 6.139
96.807 ± 2.881
65.840 ± 3.228
0.246 ± 0.040
NegGrad
94.200 ± 2.319
91.843 ± 1.718
66.793 ± 3.820
0.118 ± 0.022
NegGrad+
88.111 ± 1.532
99.841 ± 0.162
71.493 ± 2.158
0.149 ± 0.028
Influence unlearning
99.722 ± 0.981
99.202 ± 2.531
71.610 ± 5.558
0.007 ± 0.020
SalUn
100.000 ± 0.000
99.952 ± 0.016
74.707 ± 2.064
0.006 ± 0.002
Random-label
89.267 ± 1.159
85.992 ± 0.557
58.190 ± 1.640
0.130 ± 0.023
(a) Low ES
Forget Acc
Retain Acc
Test Acc
MIA
Retrain
78.133 ± 1.616
99.960 ± 0.009
73.273 ± 2.131
0.407 ± 0.007
Original
100.000 ± 0.000
99.959 ± 0.015
75.003 ± 2.809
0.001 ± 0.001
Fine-tune
86.078 ± 8.732
98.069 ± 4.782
67.680 ± 4.230
0.287 ± 0.040
L1-sparse
89.511 ± 4.761
96.656 ± 2.561
66.983 ± 3.893
0.195 ± 0.025
NegGrad
88.289 ± 1.800
87.242 ± 2.187
63.380 ± 3.313
0.189 ± 0.018
NegGrad+
68.900 ± 2.582
98.251 ± 1.412
67.913 ± 1.354
0.408 ± 0.035
Influence unlearning
98.978 ± 1.962
98.613 ± 2.268
70.093 ± 0.981
0.035 ± 0.042
SalUn
92.744 ± 1.706
91.400 ± 1.657
63.460 ± 1.319
0.120 ± 0.011
Random-label
80.622 ± 0.751
76.779 ± 0.664
57.213 ± 1.410
0.204 ± 0.004
(b) Medium ES
Forget Acc
Retain Acc
Test Acc
MIA
Retrain
69.789 ± 6.038
99.963 ± 0.012
73.827 ± 3.116
0.536 ± 0.073
Original
99.989 ± 0.048
99.960 ± 0.018
75.003 ± 2.809
0.009 ± 0.007
Fine-tune
63.622 ± 6.326
93.054 ± 2.972
62.320 ± 3.726
0.470 ± 0.060
L1-sparse
97.856 ± 1.519
99.706 ± 0.286
72.620 ± 1.985
0.225 ± 0.057
NegGrad
83.489 ± 23.803
87.013 ± 19.000
63.343 ± 12.854
0.260 ± 0.245
NegGrad+
52.944 ± 5.967
98.491 ± 0.732
67.257 ± 2.996
0.649 ± 0.091
Influence unlearning
84.133 ± 8.032
88.398 ± 5.947
62.170 ± 1.267
0.242 ± 0.046
SalUn
99.167 ± 1.444
99.289 ± 1.222
70.567 ± 2.274
0.080 ± 0.058
Random-label
76.756 ± 4.633
78.256 ± 4.378
54.160 ± 1.832
0.250 ± 0.041
(c) High ES
Table 6: Accuracy and MIA performance of different unlearning algorithms on forget / retain sets
with varying ES for CIFAR-100 using ResNet-50.
19

Low memorization
Medium memorization
High memorization
Retrain
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
Original
0.988 ± 0.007
0.723 ± 0.053
0.231 ± 0.058
Fine-tune
0.933 ± 0.052
0.884 ± 0.019
0.760 ± 0.065
L1-sparse
0.914 ± 0.061
0.816 ± 0.011
0.629 ± 0.087
NegGrad
0.938 ± 0.028
0.738 ± 0.005
0.325 ± 0.098
NegGrad+
0.965 ± 0.020
0.831 ± 0.032
0.661 ± 0.082
Influence unlearning
0.986 ± 0.031
0.738 ± 0.051
0.381 ± 0.037
Salun
0.774 ± 0.043
0.758 ± 0.053
0.886 ± 0.082
Random-label
0.709 ± 0.029
0.548 ± 0.093
0.877 ± 0.064
(a) CIFAR-10 with ResNet-18
Low memorization
Medium memorization
High memorization
Retrain
1.000 ± 0.000
1.000 ± 0.000
1.000 ± 0.000
Original
0.983 ± 0.014
0.516 ± 0.041
0.026 ± 0.005
Fine-tune
0.974 ± 0.064
0.830 ± 0.052
0.768 ± 0.025
L1-sparse
0.857 ± 0.035
0.828 ± 0.021
0.754 ± 0.036
NegGrad
0.680 ± 0.130
0.564 ± 0.044
0.270 ± 0.039
NegGrad+
0.986 ± 0.024
0.917 ± 0.040
0.889 ± 0.012
Influence unlearning
0.960 ± 0.112
0.556 ± 0.044
0.154 ± 0.081
SalUn
0.964 ± 0.091
0.564 ± 0.047
0.538 ± 0.061
Random-label
0.770 ± 0.105
0.548 ± 0.022
0.409 ± 0.044
(b) CIFAR-100 with ResNet-50
Table 7: ToW for different unlearning algorithms applied to forget sets with varying levels of memo-
rization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the memorization level
of the forget examples increases, the ToW significantly decreases for most algorithms, indicating that
unlearning becomes harder when the forget examples are more memorized by the model.
Each experiment was conducted 3 times, with average values and 95% confidence intervals reported.
Our results indicate that RUMF improves the performance of each unlearning algorithm, and the full
RUM approach, which uses the best algorithm for each subset, achieves the overall best results.
Furthermore, to gain a deeper understanding of the dynamics involved in applying RUM, we plotted
the sequence dynamics for SalUn RUMF on CIFAR-10 (Figure 4) and NegGrad+ RUMF on CIFAR-
100 (Figure 9). These plots show the accuracy of the entire forget set, the retain set, the test set, and
subsets of the forget set after each step. They demonstrate that while both orderings (low →med →
high and high →med →low memorization) yield similar ToW according to Table 11, their sequence
dynamics during the unlearning phase are different. This phenomenon is discussed in Section 6 in
the main paper, specifically in the Analysis of Sequence Dynamics paragraph.
20

Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
99.711 ± 0.253
100.000 ± 0.000
84.280 ± 1.184
0.010 ± 0.006
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.000 ± 0.000
Fine-tune
99.156 ± 1.634
98.390 ± 1.731
79.587 ± 3.173
0.020 ± 0.024
L1-sparse
98.678 ± 2.046
97.736 ± 0.449
78.750 ± 4.659
0.039 ± 0.047
NegGrad
99.178 ± 1.574
99.241 ± 1.512
79.390 ± 1.904
0.030 ± 0.033
NegGrad+
98.300 ± 1.490
99.944 ± 0.142
82.257 ± 3.363
0.020 ± 0.019
Influence unlearning
100.000 ± 0.000
100.000 ± 0.000
83.133 ± 4.128
0.002 ± 0.004
SalUn
81.278 ± 5.906
99.996 ± 0.003
79.223 ± 5.991
0.960 ± 0.053
Random-label
86.667 ± 6.902
90.604 ± 7.740
74.350 ± 1.287
0.611 ± 0.089
(a) Low memorization
Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
73.611 ± 3.478
100.000 ± 0.000
82.977 ± 2.060
0.654 ± 0.067
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.000 ± 0.000
Fine-tune
82.856 ± 1.706
99.422 ± 0.721
80.957 ± 2.376
0.413 ± 0.117
L1-sparse
82.922 ± 5.971
95.660 ± 5.166
77.130 ± 4.536
0.368 ± 0.013
NegGrad
92.900 ± 10.384
96.755 ± 6.091
77.593 ± 5.946
0.164 ± 0.130
NegGrad+
88.122 ± 5.826
99.962 ± 0.072
80.303 ± 5.202
0.193 ± 0.060
Influence unlearning
91.800 ± 13.743
96.267 ± 6.256
76.843 ± 5.060
0.217 ± 0.099
SalUn
51.100 ± 7.729
100.000 ± 0.000
80.847 ± 3.332
0.965 ± 0.012
Random-label
36.233 ± 6.904
94.675 ± 2.471
75.373 ± 2.546
0.829 ± 0.076
(b) Medium memorization
Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
23.444 ± 5.753
100.000 ± 0.000
82.967 ± 1.910
0.961 ± 0.018
Original
100.000 ± 0.000
100.000 ± 0.000
84.353 ± 3.455
0.002 ± 0.001
Fine-tune
45.822 ± 10.494
98.987 ± 4.112
82.003 ± 4.700
0.820 ± 0.135
L1-sparse
55.644 ± 16.226
97.001 ± 1.941
78.730 ± 5.902
0.714 ± 0.075
NegGrad
85.133 ± 15.675
93.289 ± 10.807
74.827 ± 7.390
0.266 ± 0.179
NegGrad+
48.400 ± 5.596
94.876 ± 2.829
75.783 ± 4.310
0.641 ± 0.107
Influence unlearning
74.567 ± 3.934
88.536 ± 2.176
71.143 ± 3.499
0.417 ± 0.067
SalUn
32.311 ± 5.235
99.560 ± 0.597
80.647 ± 3.328
0.951 ± 0.008
Random-label
22.700 ± 3.360
94.728 ± 4.453
76.323 ± 2.830
0.936 ± 0.025
(c) High memorization
Table 8: Accuracy and MIA results for various unlearning algorithms applied on forget / retain sets
with different memorization levels for CIFAR10 using ResNet-18.
21

Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
99.878 ± 0.096
99.960 ± 0.003
74.077 ± 2.112
0.016 ± 0.005
Original
100.000 ± 0.000
99.959 ± 0.015
75.003 ± 2.809
0.001 ± 0.001
Fine-tune
99.878 ± 0.266
99.806 ± 0.273
71.693 ± 4.515
0.018 ± 0.015
L1-sparse
98.100 ± 0.299
95.385 ± 2.147
65.550 ± 2.179
0.040 ± 0.007
NegGrad
90.589 ± 2.987
85.454 ± 6.778
61.637 ± 6.310
0.140 ± 0.010
NegGrad+
99.556 ± 0.345
99.951 ± 0.018
73.823 ± 2.673
0.014 ± 0.010
Influence unlearning
99.867 ± 0.430
99.077 ± 2.993
71.843 ± 9.367
0.006 ± 0.017
Salun
99.922 ± 0.191
99.225 ± 1.600
71.227 ± 6.693
0.008 ± 0.008
Random-label
98.144 ± 1.559
88.833 ± 5.531
62.207 ± 4.559
0.030 ± 0.011
(a) Low memorization
Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
52.622 ± 4.204
99.976 ± 0.006
72.767 ± 2.651
0.906 ± 0.015
Original
99.800 ± 0.000
99.973 ± 0.015
75.003 ± 2.809
0.020 ± 0.007
Fine-tune
60.178 ± 14.403
96.944 ± 5.329
65.433 ± 7.219
0.663 ± 0.102
L1-sparse
56.567 ± 3.067
94.525 ± 3.186
64.013 ± 1.340
0.615 ± 0.028
NegGrad
82.000 ± 4.362
87.858 ± 4.205
63.673 ± 5.832
0.294 ± 0.062
NegGrad+
54.267 ± 14.532
99.566 ± 0.704
69.440 ± 4.251
0.649 ± 0.161
Influence unlearning
84.667 ± 15.692
90.643 ± 12.409
63.330 ± 11.045
0.220 ± 0.095
Salun
79.889 ± 8.212
85.875 ± 4.268
63.147 ± 1.900
0.646 ± 0.740
Random-label
68.022 ± 15.101
77.440 ± 12.416
56.787 ± 6.257
0.354 ± 0.133
(b) Medium memorization
Forget accuracy
Retain accuracy
Test accuracy
MIA
Retrain
2.556 ± 0.669
99.972 ± 0.009
73.580 ± 0.661
1.000 ± 0.001
Original
99.900 ± 0.166
99.966 ± 0.003
75.003 ± 2.809
0.045 ± 0.020
Fine-tune
12.300 ± 2.010
94.729 ± 1.488
63.413 ± 3.273
0.947 ± 0.003
L1-sparse
15.244 ± 2.825
94.814 ± 4.718
64.707 ± 2.808
0.940 ± 0.023
NegGrad
62.578 ± 14.734
80.621 ± 9.071
58.027 ± 9.041
0.493 ± 0.096
NegGrad+
5.567 ± 2.654
97.193 ± 2.041
67.820 ± 0.927
0.978 ± 0.005
Influence unlearning
85.389 ± 9.154
95.745 ± 2.685
67.240 ± 1.869
0.331 ± 0.170
Salun
33.556 ± 6.167
88.021 ± 2.044
62.127 ± 2.447
0.997 ± 0.004
Random-label
39.778 ± 13.921
78.972 ± 8.172
56.333 ± 3.225
0.645 ± 0.101
(c) High memorization
Table 9: Accuracy and MIA performance for different unlearning algorithms applied on forget / retain
sets of varying levels of memorization for CIFAR100 using ResNet-50.
22

(a) CIFAR-10 with ResNet-18
(b) CIFAR-100 with ResNet-50
Figure 8: Forget, retain, test accuracy and MIA performance for forget / retain partitions with varying
levels of memorization, for CIFAR-10 using ResNet-18 and CIFAR-100 using ResNet-50. As the
memorization level of the forget sets increases, forget accuracy significantly decreases while MIA
performance increases. This trend indicates that as the forget examples become more memorized by
the model, unlearning becomes more effective in removing the effect of these examples.
(a) low →med →high (Tow: 0.92, MIA gap: 0.059) (b) high →med →low (ToW: 0.93, MIA gap: 0.022)
Figure 9: Analysis of sequence dynamics for two different orderings. We apply NegGrad+ RUM
on CIFAR-100 dataset using ResNet-50 and show the accuracy on overall forget set (and each of its
subsets), the retain set and test set after each step in the RUM sequence.
23

Class
0
1
2
3
4
5
6
7
8
9
Count
291
259
363
314
277
316
298
309
306
267
(a) CIFAR-10
Class
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Count
23
24
34
29
34
25
25
40
17
43
34
30
27
21
25
30
25
32
37
26
Class
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
Count
31
22
29
30
39
37
48
30
33
30
25
27
45
40
31
23
28
30
32
31
Class
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
Count
32
32
36
27
28
26
28
32
34
22
38
32
19
34
20
41
29
40
20
34
Class
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
Count
28
26
21
33
31
32
25
35
26
28
19
19
37
26
53
33
20
23
34
37
Class
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
Count
24
22
27
24
28
21
27
34
32
27
22
38
32
39
43
30
41
28
24
25
(b) CIFAR-100
Table 10: Class distribution of the forget set in the RUM experiment for CIFAR-10 and CIFAR-100,
with 3000 examples in the forget set for each dataset. The forget set includes examples from all
classes in both datasets.
24

ToW (↑)
Forget accuracy
Retain accracy
Test accracy
MIA
MIA gap (↓)
Retrain
1.000±0.000
64.156±2.632
100.000±0.000
83.917±2.040
0.549±0.020
0.000
Fine-tune
0.849±0.030
73.067±14.064
97.923±4.649
79.277±7.487
0.429±0.086
0.120
Fine-tune shuffle
0.712±0.040
88.389±9.089
96.578±8.717
82.037±8.500
0.451±0.037
0.098
Fine-tune RUMF
0.937±0.052
69.133±6.583
99.664±0.175
83.230±2.221
0.450±0.038
0.099
L1-sparse
0.794±0.035
79.300±11.467
98.187±2.695
79.373±6.872
0.374±0.097
0.175
L1-sparse shuffle
0.716±0.023
88.244±5.217
97.152±1.462
81.047±3.378
0.211±0.077
0.338
L1-sparse RUMF
0.900±0.020
69.967±6.014
98.458±1.399
81.047±3.378
0.477±0.039
0.072
NegGrad+
0.802±0.028
76.867±7.382
98.415±0.783
77.337±4.664
0.319±0.093
0.230
NegGrad+ shuffle
0.632±0.022
99.600±0.933
99.929±0.141
83.700±5.336
0.029±0.029
0.520
NegGrad+ RUMF
0.879±0.068
61.744±2.097
96.655±2.274
77.063±1.991
0.415±0.027
0.134
SalUn
0.731±0.070
40.056±0.391
99.796±0.429
80.370±4.159
0.923±0.067
0.374
SalUn shuffle
0.727±0.030
81.889±1.528
94.693±2.866
77.270±0.390
0.315±0.019
0.234
SalUn RUMF
0.887±0.069
62.878±3.726
96.457±2.482
77.857±3.280
0.518±0.060
0.031
nothing →Fine-tune →SalUn
0.965±0.014
66.011±5.139
99.205±1.962
83.007±3.941
0.515±0.019
0.034
nothing →SalUn →Fine-tune
0.911±0.010
69.322±6.915
99.070±2.281
81.457±6.968
0.440±0.047
0.109
nothing →SalUn →SalUn
0.919±0.059
70.733±4.310
100.000±0.000
84.190±3.328
0.644±0.010
0.095
Fine-tune RUM (low →med →high)
0.937±0.052
69.133±6.583
99.664±0.175
83.230±2.221
0.450±0.038
0.099
Fine-tune RUM (high →med →low)
0.942±0.032
68.222±1.184
99.515±2.020
83.587±2.236
0.478±0.120
0.071
SalUn RUM (low →med →high)
0.887±0.069
62.878±3.726
96.457±2.482
77.857±3.280
0.518±0.060
0.031
SalUn RUM (high →med →low)
0.881±0.024
74.644±1.615
99.921±0.124
82.393±3.018
0.328±0.011
0.221
(a) RUM results on CIFAR-10 using ResNet-18
ToW (↑)
Forget accuracy
Retain accracy
Test accracy
MIA
avg. MIA gap (↓)
Retrain
1.000±0.000
52.044±2.160
99.966±0.018
73.260±2.150
0.635±0.005
0.000
NegGrad+
0.861±0.069
63.644±4.508
99.641±0.316
70.970±3.014
0.477±0.019
0.159
NegGrad+ shuffle
0.613±0.054
88.011±4.628
97.994±1.394
70.867±2.836
0.218±0.074
0.417
NegGrad+ RUMF
0.921±0.034
50.789±7.713
98.413±0.855
68.820±3.633
0.576±0.075
0.059
L1-sparse
0.824±0.011
54.078±3.275
93.367±1.376
63.287±2.381
0.546±0.028
0.089
L1-sparse shuffle
0.604±0.023
82.111±2.285
95.310±0.879
63.880±2.323
0.282±0.014
0.353
L1-sparse RUMF
0.883±0.046
52.444±1.628
97.269±1.393
64.317±4.990
0.602±0.007
0.033
Fine-tune
0.734±0.025
76.400±4.445
99.575±0.761
70.740±3.421
0.496±0.156
0.139
Fine-tune shuffle
0.589±0.036
81.689±5.860
93.616±2.316
62.653±3.413
0.290±0.049
0.345
Fine-tune RUMF
0.784±0.040
61.767±10.301
96.706±3.397
63.090±3.575
0.542±0.030
0.093
SalUn
0.545±0.061
88.967±28.001
94.207±19.407
66.327±10.914
0.259±0.052
0.372
SalUn shuffle
0.538±0.019
63.389±3.817
75.479±3.208
53.713±2.518
0.398±0.022
0.237
SalUn RUMF
0.614±0.037
58.489±1.581
79.719±1.225
55.607±3.857
0.454±0.008
0.181
NegGrad+ RUM (low →med →high)
0.921±0.034
50.789±7.713
98.413±0.855
68.820±3.633
0.576±0.075
0.059
NegGrad+ RUM (high →med →low)
0.929±0.058
49.900±12.333
99.703±0.340
71.080±3.141
0.657±0.040
0.022
L1-sparse RUM (low →med →high)
0.883±0.046
52.444±1.628
97.269±1.393
64.317±4.990
0.602±0.007
0.033
L1-sparse RUM (high →med →low)
0.908±0.013
53.967±2.587
98.732±1.376
66.963±2.814
0.591±0.020
0.044
(b) RUM results on CIFAR-100 using ResNet-50
Table 11: RUM results on CIFAR-10 and CIFAR-100. Results obtained by applying RUM with
different algorithms, according to ToW (higher is better), its constituent ingredients, and MIA (lower
is better for MIA gap). The top section compares applying an unlearning algorithm U in three ways:
i) in one-go, as usual (e.g. Fine-tune), ii) on three randomly-determined equal-sized subsets of S,
sequentially (e.g. Fine-tune shuffle), and iii) on three equal-sized buckets obtained by refinement
F(S) according to memorization scores, in low →med →high order (e.g. Fine-tune RUMF). The
middle section of Table 11a further experiments with picking a different unlearning algorithm for
each subset of F(S). Here A →B →C denotes applying algorithm A on the first subset, B on the
second subset, and C on the third subset, where the subsets appear in low →medium →high order.
The bottom section shows different orderings.
25

