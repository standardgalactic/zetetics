Technical Report
AMUSED: AN OPEN MUSE REPRODUCTION
Suraj Patil1, William Berman1, Robin Rombach2, Patrick von Platen1
1Hugging Face, 2Stability AI
{suraj, patrick}@huggingface.co
WLBberman@gmail.com
robin@stability.ai
ABSTRACT
We present aMUSEd, an open-source, lightweight masked image model (MIM)
for text-to-image generation based on MUSE (Chang et al. (2023)). With 10%
of MUSE’s parameters, aMUSEd is focused on fast image generation. We be-
lieve MIM is underexplored compared to latent diffusion (Rombach et al. (2022)),
the prevailing approach for text-to-image generation. Compared to latent diffu-
sion, MIM requires fewer inference steps (Chang et al. (2023)) and is more inter-
pretable. Additionally, MIM can be fine-tuned to learn additional styles with only
a single image (Sohn et al. (2023)). We hope to encourage further exploration of
MIM by demonstrating its effectiveness on large-scale text-to-image generation
and releasing reproducible training code. We also release checkpoints for two
models which directly produce images at 256x256 and 512x512 resolutions.
1
INTRODUCTION
In recent years, diffusion based text-to-image generative models have achieved unprecedented qual-
ity (Rombach et al. (2022); Podell et al. (2023); DeepFloyd (2023); Saharia et al. (2022); Betker
et al. (2023); Ramesh et al. (2022; 2021); Peebles & Xie (2023)). Improvements can be mainly
attributed to large open-source pre-training datasets (Schuhmann et al. (2022a)), pre-trained text
encoders (Radford et al. (2021); Raffel et al. (2023)), latent image encoding methods (Kingma &
Welling (2019), Esser et al. (2021)) and improved sampling algorithms (Song et al. (2020); Zhang
& Chen (2023); Lu et al. (2022a;b); Dockhorn et al. (2022); Song et al. (2021); Karras et al. (2022)).
MIM has proven to be a promising alternative to diffusion models for image generation (Chang
et al. (2023; 2022)). MIM’s repeated parallel prediction of all tokens is particularly efficient for
high-resolution data like images. While diffusion models usually require 20 or more sampling steps
during inference, MIM allows for image generation in as few as 10 steps.
MIM brings the modeling approach closer to the well-researched field of language modeling (LM).
Consequently, MIM can directly benefit from findings of the LM research community, including
quantization schemes (Dettmers et al. (2022; 2023)), token sampling methods (Fan et al. (2018),
Holtzman et al. (2020)), and token-based uncertainty estimation Guo et al. (2017).
As MIM’s default prediction objective mirrors in-painting, MIM demonstrates impressive zero-shot
in-painting performance, whereas diffusion models generally require additional fine-tuning (Run-
wayML (2022)). Moreover, recent style-transfer (Sohn et al. (2023)) research has shown effective
single image style transfer for MIM, but diffusion models have not exhibited the same success.
Despite MIM’s numerous benefits over diffusion-based image generation methods, its adoption has
been limited. Proposed architectures require significant computational resources, e.g. MUSE uses a
4.6b parameter text-encoder, a 3b parameter base transformer, and a 1b parameter super-resolution
transformer. Additionally, previous models have not released training code and modeling weights.
We believe an open-source, lightweight model will support the community to further develop MIM.
In this work, we introduce aMUSEd, an efficient, open-source 800M million parameter model1
based on MUSE. aMUSEd utilizes a CLIP-L/14 text encoder (Radford et al. (2021)), SDXL-style
1Including all parameters from the U-ViT, CLIP-L/14 text encoder, and VQ-GAN.
1
arXiv:2401.01808v1  [cs.CV]  3 Jan 2024

Technical Report
micro-conditioning (Podell et al. (2023)), and a U-ViT backbone (Hoogeboom et al. (2023)). The
U-ViT backbone eliminates the need for a super-resolution model, allowing us to successfully train
a single-stage 512x512 resolution model. The design is focused on reduced complexity and re-
duced computational requirements to facilitate broader use and experimentation within the scientific
community.
We demonstrate many advantages such as 4bit and 8bit quantization, zero-shot in-painting, and
single image style transfer with styledrop (Sohn et al. (2023)). We release all relevant model weights
and source code.
2
RELATED WORK
2.1
TOKEN-BASED IMAGE GENERATION
Esser et al. (2021) demonstrated the effectiveness of VQ-GAN generated image token embeddings
for auto-regressive transformer based image modeling. With large-scale text-to-image datasets,
auto-regressive image generation can yield state-of-the-art results in image quality (Yu et al. (2022;
2023)). Additionally, auto-regressive token prediction allows framing image and text-generation as
the same task, opening an exciting research direction for grounded multimodal generative models
(Huang et al. (2023); Aghajanyan et al. (2022)). While effective, auto-regressive image generation
is computationally expensive. Generating a single image can require hundreds to thousands of token
predictions.
As images are not inherently sequential, Chang et al. (2022) proposed MIM. MIM predicts all
masked image tokens in parallel for a fixed number of inference steps. On each step, a predeter-
mined percentage of the most confident predictions are fixed, and all other tokens are re-masked.
MIM’s training objective mirrors BERT’s training objective (Devlin et al. (2018)). However, MIM
uses a varied masking ratio to support iterative sampling starting from only masked tokens.
Consequently, MUSE successfully applied MIM to large-scale text-to-image generation (Chang
et al. (2023)). MUSE uses a VQ-GAN (Esser et al. (2021)) with a fine-tuned decoder, a 3 billion pa-
rameter transformer, and a 1 billion parameter super-resolution transformer. Additionally, MUSE is
conditioned on text embeddings from the pre-trained T5-XXL text encoder (Raffel et al. (2023)). To
improve image quality when predicting 512x512 resolution images, MUSE uses a super-resolution
model conditioned on predicted tokens from a 256x256 resolution model. As MIM’s default pre-
diction objective mirrors in-painting, MUSE demonstrates impressive zero-shot in-painting results.
In contrast, diffusion models generally require additional fine-tuning for in-painting (RunwayML
(2022)).
MIM has not been adopted by the research community to the same degree as diffusion models.
We believe this is mainly due to a lack of lightweight, open-sourced models, e.g. MUSE is closed
source and has a 4.5 billion parameter text encoder, a 3 billion parameter base model, and a 1 billion
parameter super-resolution model.
2.2
FEW-STEP DIFFUSION MODELS
Diffusion models are currently the prevailing modeling approach for text-to-image generation. Dif-
fusion models are trained to remove noise from a target image at incrementally decreasing levels
of noise. Models are frequently trained on 1000 noise levels (Rombach et al. (2022); Podell et al.
(2023); Saharia et al. (2022); Chen et al. (2023)), but noise levels can be skipped or approximated
without suffering a significant loss in image quality (Song et al. (2021); Karras et al. (2022); Song
et al. (2020); Zhang & Chen (2023); Lu et al. (2022a;b); Dockhorn et al. (2022)). As of writing
this report, effective denoising strategies (Lu et al. (2022b); Zhao et al. (2023); Zheng et al. (2023))
require as few as 20 steps to generate images with little to indistinguishable quality degradation
compared to denoising at each trained noise level.
20 sampling steps is still prohibitively expensive for real-time image generation. Diffusion models
can be further distilled to sample in as few as 1 to 4 sampling steps. Salimans & Ho (2022) shows
how a pre-trained diffusion model can be distilled to sample in half the number of sampling steps.
This distillation can be repeated multiple times to produce a model that requires as few as 2 to
2

Technical Report
4 sampling steps. Additionally, framing the denoising process as a deterministic ODE integration,
consistency models can learn to directly predict the same fully denoised image from any intermediate
noisy image on the ODE trajectory (Song et al. (2021)). Luo et al. (2023a) and Luo et al. (2023b)
were the first to successfully apply consistency distillation to large-scale text-to-image datasets,
generating high-quality images in as few as 4 inference steps. Sauer et al. (2023) demonstrated that
an adversarial loss objective and a score distillation sampling (Poole et al. (2022)) objective can be
combined to distill few step sampling.
Distilled diffusion models are faster than the current fastest MIM models. However, distilled diffu-
sion models require a powerful teacher model. A teacher model requires additional training com-
plexity, additional training memory, and limits the image quality of the distilled model. MIM’s
training objective does not require a teacher model or approximate inference algorithm and is fun-
damentally designed to require fewer sampling steps.
2.3
INTERPRETABILITY OF TEXT-TO-IMAGE MODELS
Auto-regressive image modeling and MIM output explicit token probabilities, which naturally mea-
sure prediction confidence (Guo et al. (2017)). Token probability-based language models have been
used to research model interpretability (Jiang et al. (2021)). We do not extensively explore the in-
terpretability of token prediction-based image models, but we believe this is an interesting future
research direction.
3
METHOD
VQ-GAN
We trained a 146M parameter VQ-GAN (Esser et al. (2021)) with no self-attention
layers, a vocab size of 8192, and a latent dimension of 64. Our VQ-GAN downsamples resolutions
by 16x, e.g. a 256x256 (512x512) resolution image is reduced to 16x16 (32x32) latent codes. We
trained our VQ-GAN for 2.5M steps.
Text Conditioning
Due to our focus on inference speed, we decided to condition our model on
text embeddings from a smaller CLIP model (Radford et al. (2021)) instead of T5-XXL (Raffel
et al. (2023)). We experimented with both the original CLIP-l/14 (Radford et al. (2021)) and the
equivalently sized CLIP model released with DataComp (Gadre et al. (2023)). Even with the re-
ported improvements in Gadre et al. (2023), we found that the original CLIP-l/14 resulted in qual-
itatively better images. The penultimate text encoder hidden states are injected via the standard
cross-attention mechanism. Additionally, the final pooled text encoder hidden states are injected via
adaptive normalization layers (Perez et al. (2017)).
U-ViT
For the base model, we used a variant of the U-ViT (Hoogeboom et al. (2023)), a trans-
former (Vaswani et al. (2023)) inspired scalable U-Net (Ronneberger et al. (2015)). Hoogeboom
et al. (2023) finds that U-Nets can be effectively scaled by increasing the number of low-resolution
blocks as the increased parameters are more than compensated for by the small feature maps. Addi-
tionally, Hoogeboom et al. (2023) turns the lowest resolution blocks into a transformer by replacing
convolution blocks with MLPs. For our 256x256 resolution model, we used no downsampling or
upsampling in the convolutional residual blocks. For our 512x512 resolution model, we used a sin-
gle 2x downsampling and corresponding 2x upsampling in the convolutional residual blocks. As a
result, the lower resolution U-ViT of the 256x256 and 512x512 models receive an input vector se-
quence of 256 (16x16) with a feature dimension of 1024. The 256x256 resolution model has 603M
parameters, and the 512x512 resolution model has 608M parameters. The 5M additional parameters
in the 512x512 resolution model are due to the additional down and upsampling layers.
Masking Schedule
Following MUSE (Chang et al. (2023)) and MaskGIT (Chang et al. (2022)),
we use a cosine based masking schedule. After each step t, of predicted tokens, those with the
most confident predictions are permanently unmasked such that the proportion of tokens masked is
cos( t
T · π
2 ), with T being the total number of sampling steps. We use T = 12 sampling steps in all
of our evaluation experiments. Through ablations, Chang et al. (2022) shows that concave masking
schedules like cosine outperform convex masking schedules. Chang et al. (2022) hypothesizes that
3

Technical Report
Figure 1: The diagram shows the training and inference pipelines for aMUSEd. aMUSEd consists
of three separately trained components: a pre-trained CLIP-L/14 text encoder, a VQ-GAN, and a
U-ViT. During training, the VQ-GAN encoder maps images to a 16x smaller latent resolution. The
proportion of masked latent tokens is sampled from a cosine masking schedule, e.g. cos(r · π
2 )
with r ∼Uniform(0, 1). The model is trained via cross-entropy loss to predict the masked tokens.
After the model is trained on 256x256 images, downsampling and upsampling layers are added, and
training is continued on 512x512 images. During inference, the U-ViT is conditioned on the text
encoder’s hidden states and iteratively predicts values for all masked tokens. The cosine masking
schedule determines a percentage of the most confident token predictions to be fixed after every
iteration. After 12 iterations, all tokens have been predicted and are decoded by the VQ-GAN into
image pixels.
concave masking schedules benefit from fewer fixed predictions earlier in the denoising process and
more fixed predictions later in the denoising process.
Micro-conditioning
Just as Podell et al. (2023), we micro-condition on the original image resolu-
tion, crop coordinates, and LAION aesthetic score (Schuhmann (2022)). The micro-conditioning
values are projected to sinusoidal embeddings and appended as additional channels to the final
pooled text encoder hidden states.
4
EXPERIMENTAL SETUP
4.1
PRE-TRAINING
Data Preparation
We pre-trained on deduplicated LAION-2B (Schuhmann et al. (2022a)) with
images above a 4.5 aesthetic score (Schuhmann (2022)). We filtered out images above a 50% water-
mark probability or above a 45% NSFW probability. The deduplicated LAION dataset was provided
by Laurenc¸on et al. (2023) using the strategy presented in Webster et al. (2023).
Training Details
For pre-training, the VQ-GAN and text encoder weights were frozen, and only
the U-ViTs of the respective models were trained. The 256x256 resolution model2 was trained on 2
2https://huggingface.co/amused/amused-256
4

Technical Report
8xA100 servers for 1,000,000 steps and used a per GPU batch size of 128 for a total batch size of
2,048. The 512x512 resolution model3 was initialized from step 84,000 of the 256x256 resolution
model and continued to train for 554,000 steps on 2 8xA100 servers. The 512x512 resolution model
used a per GPU batch size 64 for a total batch size of 1024.
Masking Rate Sampling
Following Chang et al. (2022) and Chang et al. (2023), the percent-
age of masked latent tokens was sampled from a cosine masking schedule, e.g. cos(r · π
2 ) with
r ∼Uniform(0, 1). Chang et al. (2022) ablates different choices of masking schedules, finding that
concave functions outperform convex functions. They hypothesize that this is due to more challeng-
ing masking ratios during training.
4.2
FINE-TUNING
We further fine-tuned the 256x256 resolution model for 80,000 steps on journeydb (Sun et al.
(2023)). We also further fine-tuned the 512x512 model for 2,000 steps on journeydb, synthetic
images generated by SDXL (Podell et al. (2023)) from LAION-COCO captions (Schuhmann et al.
(2022b)), unsplash lite, and LAION-2B above a 6 aesthetic score (Schuhmann et al. (2022a); Schuh-
mann (2022)). We found that the synthetic image generated by SDXL (Podell et al. (2023)) from
LAION-COCO captions (Schuhmann et al. (2022b)) qualitatively improved text-image alignment.
The 512x512 resolution model was fine-tuned for much fewer steps than the 256x256 model because
it began to overfit on the fine-tuning data.
To improve the reconstruction of high-resolution images, we further fine-tuned the VQ-GAN de-
coder on a dataset of images greater than 1024x1024 resolution. The VQ-GAN decoder was fine-
tuned on 2 8xA100 servers for 200,000 steps and used a per GPU batch size of 16 for a total batch
size of 256.
5
RESULTS
Figure 2: A100 40GB end to end image generation time. Full A100 and 4090 benchmarks can be
found in appendix A.
5.1
INFERENCE SPEED
aMUSEd’s inference speed is superior to non-distilled diffusion models and competitive with few-
step distilled diffusion models. Compared to many popular diffusion models, aMUSEd scales par-
3https://huggingface.co/amused/amused-512
5

Technical Report
ticularly well with batch size, making it a good choice for text-to-image applications that require
high throughput4.
For batch size 1, single-step distilled diffusion models such as sd-turbo and sdxl-turbo (Sauer et al.
(2023)) outperform both of our 256x256 and 512x512 resolution models. Notably, sd-turbo gener-
ates higher resolution images than our 256x256 resolution model while being 3.5x faster.
Compared to batch size 1, the end to end generation time for batch size 8 of sd-turbo (sdxl-turbo)
is reduced by 3.6x (3.38x). However, aMUSEd’s 256x256 (512x12) resolution model’s inference
time only decreases by 1.28x (1.8x). At batch size 8, sd-turbo is still the fastest image generation
model, but it is only 1.3x faster than our 256x256 resolution model. At batch size 8, aMUSEd’s
256x256 (512x512) resolution model outperforms the 4-step latent consistency model by a factor of
3x (1.8x).
Both aMUSEd models are significantly faster than non-distilled diffusion models. Compared to
stable diffusion 1.55 (Rombach et al. (2022)), the 512x512 resolution aMUSEd model is 1.6x (3x)
faster at batch size 1 (batch size 8). At batch size 8, the state of the art SDXL (Podell et al. (2023))
is orders of magnitude slower than both aMUSEd models.
5.2
MODEL QUALITY
We benchmarked both aMUSEd models on zero-shot FID (Heusel et al. (2017)), CLIP (Radford
et al. (2021)), and inception score (Salimans et al. (2016)) on the MSCOCO (Lin et al. (2015)) 2017
validation set with 2 samples per caption for a total of 10k samples. Due to either a lack of re-
ported metrics or ambiguities in measurement methodologies, we manually ran quality benchmarks
for all models we compared against. Our 512x512 resolution model has competitive CLIP scores.
However, both our 256x256 and 512x512 resolution models lag behind in FID and Inception scores.
Subjectively, both models perform well at low detail images with few subjects, such as landscapes.
Both models may perform well for highly detailed images such as faces or those with many subjects
but require prompting and cherry-picking. See 3.
(a) CLIP vs. FID tradeoff curve
(b) CLIP score vs. classifier free guidance (cfg) scale
Figure 4: See appendix B for additional FID, CLIP, and inception score measurements.
4batch size × latency
5Stable diffusion 1.5 outputs images at the same 512x512 resolution as the 512x512 resolution aMUSEd
model.
6

Technical Report
Figure 3: Cherry-picked images from 512x512 and 256x256 resolution models. Images are slightly
degraded for file size considerations
5.3
STYLEDROP
Styledrop (Sohn et al. (2023)) is an efficient fine-tuning method for learning a new style from a
small number of images. It has an optional first stage to generate additional training samples, which
can be used to augment the training dataset. Styledrop demonstrates effective single example image
style adoption on MUSE and aMUSEd. Sohn et al. (2023) shows that similar fine-tuning proce-
dures such as LoRa Dreambooth (Ruiz et al. (2023)) on Stable Diffusion (Rombach et al. (2022))
and Dreambooth on Imagen (Saharia et al. (2022)) do not show the same degree of style adherence.
Figure 5 compares a LoRa Dreambooth Stable Diffusion training run6 with a styledrop training run
on aMUSEd. Using the same reference training image and example prompts, styledrop on aMUSEd
demonstrates much stronger style adherence. In our experiments with aMUSEd, we achieved good
results with fine-tuning on a single image and not generating any additional training samples. Style-
drop can cheaply fine-tune aMUSEd in as few as 1500-2000 training steps.
6Using the same training parameters as Sohn et al. (2023) - 400 training steps, UNet LR 2e-4, CLIP LR
5e-6
7

Technical Report
Figure 5
Model
Learning Rate
Batch Size
Memory Required
Steps
LoRa Alpha
LoRa Rank
amused-256
4e-4
1
6.5 GB
1500-2000
32
16
amused-512
1e-3
1
5.6 GB
1500-2000
1
16
Table 1: Styledrop configs. LoRa applied to all QKV projections.
5.4
8BIT QUANTIZATION
Token based modeling allows for the use of techniques from the language modeling literature, such
as 8-bit quantization for transformer feed-forward and attention projection layers (Dettmers et al.
(2022)). Using 8-bit quantization, we can load the whole model with as little as 800 MB of VRAM,
making mobile and CPU applications more feasible.
(a) a horse in the wild
(b) the mountains
(c) a house on a hill
Figure 6: aMUSEd 256x256 images with 8-bit quantization
5.5
TASK TRANSFER
Image variation and in-painting
Similar to Chang et al. (2023), aMUSEd performs zero-shot
image editing tasks such as image variation and in-painting. For masked token based image model-
ing, both image variation and in-painting are close to the default training objective, so both tasks use
8

Technical Report
the regular decoding procedure. For image variation, some number of latent tokens are masked with
more masked latent tokens corresponding to more variation from the original image. For in-painting,
the in-painting mask directly determines which tokens are initially masked.
(a) Original image
(b) apple watercolor
Figure 7: aMUSEd 256x256 image variation
(a) Original image
(b) winter mountains
Figure 8: aMUSEd 512x512 image variation
(a) Original Image
(b) Mask
(c) fall mountains
Figure 9: aMUSEd 512x512 in-painting
Video generation
We further extended aMUSEd to zero-shot video generation by modifying
text2video-zero (Khachatryan et al. (2023)). Text2video-zero operates on stable diffusion’s (Rom-
bach et al. (2022)) continuous latents. Noised latents are warped by varying amounts to produce
latents for successive frames. Additional noise is then added to the frame latents. During the stan-
dard denoising process, self attention is replaced with cross-attention over the first frame to maintain
temporal consistency. Because aMUSEd operates on quantized latents, we must first de-quantize the
latents before they are warped. We can then re-quantize the warped latents. Because the aMUSEd
latent space is discrete, we completely re-mask the boundary of the image warp, which creates con-
sistent image backgrounds from frame to frame. We found that the between-frame cross-attention
degraded quality for frames warped too far away from the initial frame, so we did not use the modi-
fied self attention and instead performed the warp much later in the denoising process.
9

Technical Report
Figure 10: Video generation examples. Full videos
.
6
ETHICS AND SAFETY
We filtered out images in the training data above a 50% watermark probability or above a 45%
NSFW probability. We manually checked that both models do not accurately follow NSFW prompts
and therefore concluded that our NSFW filtering helps prevent possible harmful use cases.
7
CONCLUSION
We introduced aMUSEd, a lightweight and open-source reproduction of MUSE. Our primary goal
was to achieve fast sampling and provide an efficient alternative to diffusion models. In our re-
production, aMUSEd demonstrated competitive zero-shot image variation and in-painting without
requiring task specific training. We made several modifications for efficiency, including the use
of the smaller CLIP-l/14 text encoder (Radford et al. (2021)) and an efficient U-ViT (Hoogeboom
et al. (2023)) backbone. Our results show that aMUSEd’s inference speed is competitive with dis-
tilled diffusion-based text-to-image models, particularly when scaling with batch size. Additionally,
aMUSEd demonstrates efficient fine-tuning capabilities, providing flexibility for various applica-
tions. We hope that by open-sourcing all model weights and code, future research into masked
image modeling for text-to-image generation is made more accessible.
8
CONTRIBUTION & ACKNOWLEDGEMENT
Suraj led training. William led data and supported training. Patrick supported both training and
data and provided general guidance. Robin trained the VQ-GAN and provided general guidance.
10

Technical Report
Also, immense thanks to community contributor Isamu Isozaki for helpful discussions and code
contributions.
11

Technical Report
REFERENCES
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,
Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-
modal model of the internet. arXiv preprint arXiv:2201.07520, 2022.
James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang
Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,
and Aditya Ramesh. Improving image generation with better captions. 2023.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative
image transformer, 2022.
Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan
Yang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan.
Muse: Text-to-image generation via masked generative transformers, 2023.
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James
Kwok, Ping Luo, Huchuan Lu, et al. Pixart-α: Fast training of diffusion transformer for photore-
alistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.
DeepFloyd.
Stability ai releases deepfloyd if, a powerful text-to-image model that can smartly
integrate text into images. https://stability.ai/news/deepfloyd-if-text-to-image-model, 2023.
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multi-
plication for transformers at scale, 2022.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning
of quantized llms, 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers,
2022.
Patrick Esser, Robin Rombach, and Bj¨orn Ommer. Taming transformers for high-resolution image
synthesis, 2021.
Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.
Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao
Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim En-
tezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen
Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexan-
der Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,
Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In
search of the next generation of multimodal datasets, 2023.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks, 2017.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in
neural information processing systems, 30, 2017.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration, 2020.
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for
high resolution images, 2023.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
12

Technical Report
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,
Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning
perception with language models. arXiv preprint arXiv:2302.14045, 2023.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language
models know? on the calibration of language models for question answering. Transactions of the
Association for Computational Linguistics, 9:962–977, 2021. doi: 10.1162/tacl a 00407. URL
https://aclanthology.org/2021.tacl-1.57.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-
based generative models. Advances in Neural Information Processing Systems, 35:26565–26577,
2022.
Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang
Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models
are zero-shot video generators, 2023.
Diederik P. Kingma and Max Welling.
An introduction to variational autoencoders.
CoRR,
abs/1906.02691, 2019. URL http://arxiv.org/abs/1906.02691.
Hugo Laurenc¸on, Lucile Saulnier, L´eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,
Thomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and
Victor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents,
2023.
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro
Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft coco: Common objects
in context, 2015.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.
Dpm-solver: A
fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint
arXiv:2206.00927, 2022a.
Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast
solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095,
2022b.
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Syn-
thesizing high-resolution images with few-step inference, 2023a.
Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin´ario Passos, Longbo
Huang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module,
2023b.
William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023.
Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoning with a general conditioning layer, 2017.
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M¨uller, Joe
Penna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image
synthesis. arXiv preprint arXiv:2307.01952, 2023.
Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d
diffusion, 2022.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pp.
8748–8763. PMLR, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer, 2023.
13

Technical Report
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,
and Ilya Sutskever. Zero-shot text-to-image generation, 2021.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 10684–10695, 2022.
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-
cal image segmentation, 2015.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023.
RunwayML.
Stable diffusion inpainting.
https://huggingface.co/runwayml/stable-diffusion-
inpainting, 2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-
yar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-
imans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffu-
sion models with deep language understanding, 2022.
Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv
preprint arXiv:2202.00512, 2022.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans, 2016.
Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion dis-
tillation, 2023.
Christoph Schuhmann. Laion-aesthetics. https://laion.ai/blog/laion-aesthetics/, 2022.
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi
Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.
Laion-5b:
An open large-scale dataset for training next generation image-text models.
arXiv preprint
arXiv:2210.08402, 2022a.
Christoph Schuhmann, Andreas K¨opf, Richard Vencu, Theo Coombes, and Romain Beaumont.
Laion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/laion-coco/, 2022b.
Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred
Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip
Krishnan. Styledrop: Text-to-image generation in any style, 2023.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben
Poole. Score-based generative modeling through stochastic differential equations, 2021.
Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun
Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb:
A benchmark for generative image understanding, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,
Mishig Davaadorj, and Thomas Wolf.
Diffusers: State-of-the-art diffusion models.
URL
https://github.com/huggingface/diffusers.
14

Technical Report
Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b,
2023.
Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin
Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich
text-to-image generation, 2022.
Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun
Babu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes,
Vasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan
Zhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz,
Luke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretrain-
ing and instruction tuning, 2023.
Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator,
2023.
Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-
corrector framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867,
2023.
Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode
solver with empirical model statistics, 2023.
15

Technical Report
A
INFERENCE SPEED
All measurements were taken in fp16 with the diffusers library (von Platen et al.) model implemen-
tations. All measurements are of end to end image generation.
Model
inference
time
timesteps
resolution
sd-turbo
0.13 s
1
512
sdxl-turbo
0.21 s
1
1024
latent consistency models
0.32 s
4
512
amused-256
0.47 s
12
256
amused-512
0.54 s
12
512
stable diffusion 1.5
0.85 s
20
512
SSD-1B
1.75 s
20
1024
w¨urstchen
1.96 s
41
1024
sdxl
2.7 s
20
1024
(a) batch size 1
Model
inference
time
timesteps
resolution
sd-turbo
0.47 s
1
512
amused-256
0.6 s
12
256
sdxl-turbo
0.71 s
1
1024
amused-512
1.0 s
12
512
latent consistency models
1.77 s
4
512
stable diffusion 1.5
2.96 s
20
512
w¨urstchen
10.9 s
41
1024
SSD-1B
12.62 s
20
1024
sdxl
18.47 s
20
1024
(b) batch size 8
Figure 11: A100 inference time measurements
Model
inference
time
timesteps
resolution
sd-turbo
0.07 s
1
512
sdxl-turbo
0.11 s
1
1024
amused-256
0.2 s
12
256
latent consistency models
0.24 s
4
512
amused-512
0.24 s
12
512
stable diffusion 1.5
0.55 s
20
512
w¨urstchen
1.34 s
41
1024
SSD-1B
1.88 s
20
1024
sdxl
2.84 s
20
1024
(a) batch size 1
Model
inference
time
timesteps
resolution
sd-turbo
0.44 s
1
512
amused-256
0.45 s
12
256
sdxl-turbo
0.75 s
1
1024
amused-512
0.76 s
12
512
latent consistency models
1.61 s
4
512
stable diffusion 1.5
3.16 s
20
512
w¨urstchen
11.15 s
41
1024
SSD-1B
11.73 s
20
1024
sdxl
18.38 s
20
1024
(b) batch size 8
Figure 12: 4090 inference time measurements
16

Technical Report
B
MODEL QUALITY
Figure 13
Figure 14
17

Technical Report
Model
CLIP
guidance scale
timesteps
resolution
ssd-1b
27.38
10.0
20
1024
sdxl
27.03
10.0
20
1024
stable diffusion 1.5
26.54
10.0
20
512
amused-256
25.97
5.0
12
256
latent consistency models
25.91
7.0
4
512
w¨urstchen
25.82
4.0
41
1024x1536
amused-512
24.78
8.0
12
512
(a) CLIP
Model
FID
guidance scale
timesteps
resolution
stable diffusion 1.5
15.97
3.0
20
512
sdxl
20.21
5.0
20
1024
ssd-1b
23.43
4.0
20
1024
w¨urstchen
28.28
4.0
41
1024x1536
latent consistency models
29.91
7.0
4
512
amused-512
34.87
7.0
12
512
amused-256
38.7
3.0
12
256
(b) FID
Model
ISC
guidance scale
timesteps
resolution
sdxl
38.88
7.0
20
1024
w¨urstchen
38.82
4.0
41
1024x1536
stable diffusion 1.5
36.94
6.0
20
512
ssd-1b
36.79
6.0
20
1024
latent consistency models
34.93
7.0
4
512
amused-512
26.16
6.0
12
512
amused-256
23.82
3.0
12
256
(c) Inception Score
Figure 15: Model Quality Tables
18

Technical Report
C
FINETUNING
aMUSEd can be finetuned on simple datasets relatively cheaply and quickly. Using LoRa (Hu et al.
(2021)), and gradient accumulation, aMUSEd can be finetuned with as little as 5.6 GB VRAM.
(a) a pixel art character
(b)
square red glasses on a pixel
art character with a baseball-shaped
head
(c) a pixel art character with square
blue glasses, a microwave-shaped
head and a purple-colored body on
a sunny background
Figure 16: Example outputs of finetuning 256x256 model on dataset
(a) minecraft pig
(b) minecraft character
(c) minecraft Avatar
Figure 17: Example outputs of fine-tuning 512x512 model on dataset
8bit Adam
LoRa
Single Step Batch Size
Grad. Accum. Steps
Learning Rate
Memory
Steps
No
No
8
1
1e-4
19.7 GB
750-1000
No
No
4
2
1e-4
18.3 GB
750-1000
No
No
1
8
1e-4
17.9 GB
750-1000
Yes
No
16
1
2e-5
20.1 GB
∼750
Yes
No
8
2
2e-5
15.6 GB
∼750
Yes
No
1
16
2e-5
10.7 GB
∼750
No
Yes
16
1
8e-4
14.1 GB
1000-1250
No
Yes
8
2
8e-4
10.1 GB
1000-1250
No
Yes
1
16
8e-4
6.5 GB
1000-1250
Table 2: amused-256 fine-tuning configs. All LoRa trainings used rank 16 and alpha 32. LoRa
applied to all QKV projections. dataset
19

Technical Report
8bit Adam
LoRa
Single Step Batch Size
Grad. Accum. Steps
Learning Rate
Memory
Steps
No
No
8
1
8e-5
24.2 GB
500-1000
No
No
4
2
8e-5
19.7 GB
500-1000
No
No
1
8
8e-5
16.99 GB
500-1000
Yes
No
8
1
5e-6
21.2 GB
500-1000
Yes
No
4
2
5e-6
13.3 GB
500-1000
Yes
No
1
8
5e-6
9.9 GB
500-1000
No
Yes
8
1
1e-4
12.7 GB
500-1000
No
Yes
4
2
1e-4
9.0 GB
500-1000
No
Yes
1
8
1e-4
5.6 GB
500-1000
Table 3: amused 5-12 fine-tuning configs. All LoRa trainings used rank 16 and alpha 32. LoRa
applied to all QKV projections. dataset
20

Technical Report
D
STYLEDROP EXAMPLES
(a)
Reference image: A
mushroom in [V] style
(b) A tabby cat walking in
the forest in [V] style
(c) A church on the street
in [V] style
(d) A chihuahua walking
on the street in [V] style
Figure 18: Styledrop amused-256
(a)
Reference image: A
mushroom in [V] style
(b) A tabby cat walking in
the forest in [V] style
(c) A church on the street
in [V] style
(d) A chihuahua walking
on the street in [V] style
Figure 19: LoRa Dreambooth Stable Diffusion
(a)
Reference image: A
bear in [V] style.png
(b) A tabby cat walking in
the forest in [V] style
(c) A church on the street
in [V] style
(d) A chihuahua walking
on the street in [V] style
Figure 20: Styledrop amused-512
Model
Learning Rate
Batch Size
Memory Required
Steps
LoRa Alpha
LoRa Rank
amused-256
4e-4
1
6.5 GB
1500-2000
32
16
amused-512
1e-3
1
5.6 GB
1500-2000
1
16
Table 4: Styledrop configs. LoRa applied to all QKV projections.
21

