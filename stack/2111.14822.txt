Vector Quantized Diffusion Model for Text-to-Image Synthesis
Shuyang Gu1
Dong Chen2
Jianmin Bao2
Fang Wen2
Bo Zhang2
Dongdong Chen3
Lu Yuan3
Baining Guo2
1University of Science and Technology of China
2Microsoft Research
3Microsoft Cloud+AI
gsy777@mail.ustc.edu.cn
{doch,jianbao,fangwen,zhanbo,dochen,luyuan,bainguo}@microsoft.com
Abstract
We present the vector quantized diffusion (VQ-Diffusion)
model for text-to-image generation. This method is based
on a vector quantized variational autoencoder (VQ-VAE)
whose latent space is modeled by a conditional variant
of the recently developed Denoising Diffusion Probabilis-
tic Model (DDPM). We ﬁnd that this latent-space method
is well-suited for text-to-image generation tasks because
it not only eliminates the unidirectional bias with exist-
ing methods but also allows us to incorporate a mask-and-
replace diffusion strategy to avoid the accumulation of er-
rors, which is a serious problem with existing methods. Our
experiments show that the VQ-Diffusion produces signif-
icantly better text-to-image generation results when com-
pared with conventional autoregressive (AR) models with
similar numbers of parameters. Compared with previous
GAN-based text-to-image methods, our VQ-Diffusion can
handle more complex scenes and improve the synthesized
image quality by a large margin. Finally, we show that the
image generation computation in our method can be made
highly efﬁcient by reparameterization. With traditional AR
methods, the text-to-image generation time increases lin-
early with the output image resolution and hence is quite
time consuming even for normal size images.
The VQ-
Diffusion allows us to achieve a better trade-off between
quality and speed. Our experiments indicate that the VQ-
Diffusion model with the reparameterization is ﬁfteen times
faster than traditional AR methods while achieving a bet-
ter image quality. The code and models are available at
https://github.com/cientgu/VQ-Diffusion.
1. Introduction
Recent success of Transformer [11, 65] in neural lan-
guage processing (NLP) has raised tremendous interest
in using successful language models for computer vision
tasks. Autoregressive (AR) model [4, 46, 47] is one of the
most natural and popular approach to transfer from text-to-
text generation (i.e., machine translation) to text-to-image
generation. Based on the AR model, recent work DALL-
E [48] has achieved impressive results for text-to-image
generation.
Despite their success, existing text-to-image generation
methods still have weaknesses that need to be improved.
One issue is the unidirectional bias. Existing methods pre-
dict pixels or tokens in the reading order, from top-left
to bottom-right, based on the attention to all preﬁx pix-
els/tokens and the text description. This ﬁxed order intro-
duces unnatural bias in the synthesized images because im-
portant contextual information may come from any part of
the image, not just from left or above. Another issue is
the accumulated prediction errors. Each step of the infer-
ence stage is performed based on previously sampled to-
kens – this is different from that of the training stage, which
relies on the so-called “teacher-forcing” practice [15] and
provides the ground truth for each step. This difference is
important and its consequence merits careful examination.
In particular, a token in the inference stage, once predicted,
cannot be corrected and its errors will propagate to the sub-
sequent tokens.
We
present
the
vector
quantized
diffusion
(VQ-
Diffusion) model for text-to-image generation, a model
that eliminates the unidirectional bias and avoids accu-
mulated prediction errors.
We start with a vector quan-
tized variational autoencoder (VQ-VAE) and model its la-
tent space by learning a parametric model using a con-
ditional variant of the Denoising Diffusion Probabilistic
Model (DDPM) [23, 59], which has been applied to im-
age synthesis with compelling results [12]. We show that
the latent-space model is well-suited for the task of text-
to-image generation. Roughly speaking, the VQ-Diffusion
model samples the data distribution by reversing a forward
diffusion process that gradually corrupts the input via a
ﬁxed Markov chain. The forward process yields a sequence
of increasingly noisy latent variables of the same dimen-
sionality as the input, producing pure noise after a ﬁxed
number of timesteps. Starting from this noise result, the
reverse process gradually denoises the latent variables to-
wards the desired data distribution by learning the condi-
arXiv:2111.14822v3  [cs.CV]  3 Mar 2022

tional transit distribution.
The VQ-Diffusion model eliminates the unidirectional
bias. It consists of an independent text encoder and a dif-
fusion image decoder, which performs denoising diffusion
on discrete image tokens. At the beginning of the infer-
ence stage, all image tokens are either masked or random.
Here the masked token serves the same function as those in
mask-based generative models [11]. The denoising diffu-
sion process gradually estimates the probability density of
image tokens step-by-step based on the input text. In each
step, the diffusion image decoder leverages the contextual
information of all tokens of the entire image predicted in
the previous step to estimate a new probability density dis-
tribution and use this distribution to predict the tokens in
the current step. This bidirectional attention provides global
context for each token prediction and eliminates the unidi-
rectional bias.
The VQ-Diffusion model, with its mask-and-replace dif-
fusion strategy, also avoids the accumulation of errors. In
the training stage, we do not use the “teacher-forcing” strat-
egy.
Instead, we deliberately introduce both masked to-
kens and random tokens and let the network learn to pre-
dict the masked token and modify incorrect tokens. In the
inference stage, we update the density distribution of all to-
kens in each step and resample all tokens according to the
new distribution. Thus we can modify the wrong tokens
and prevent error accumulation. Comparing to the conven-
tional replace-only diffusion strategy for unconditional im-
age generation [1], the masked tokens effectively direct the
network’s attention to the masked areas and thus greatly re-
duce the number of token combinations to be examined by
the network. This mask-and-replace diffusion strategy sig-
niﬁcantly accelerates the convergence of the network.
To assess the performance of the VQ-Diffusion method,
we conduct text-to-image generation experiments with a
wide variety of datasets, including CUB-200 [66], Oxford-
102 [40], and MSCOCO [36]. Compared with AR model
with similar numbers of model parameters, our method
achieves signiﬁcantly better results, as measured by both
image quality metrics and visual examination, and is much
faster. Compared with previous GAN-based text-to-image
methods [67,70,71,73], our method can handle more com-
plex scenes and the synthesized image quality is improved
by a large margin. Compared with extremely large models
(models with ten times more parameters than ours), includ-
ing DALL-E [48] and CogView [13], our model achieves
comparable or better results for speciﬁc types of images,
i.e., the types of images that our model has seen during
the training stage. Furthermore, our method is general and
produces strong results in our experiments on both uncon-
ditional and conditional image generation with FFHQ [28]
and ImageNet [10] datasets.
The VQ-Diffusion model also provides important bene-
ﬁts for the inference speed. With traditional AR methods,
the inference time increases linearly with the output image
resolution and the image generation is quite time consuming
even for normal-size images (e.g., images larger than small
thumbnail images of 64 × 64 pixels). The VQ-Diffusion
provides the global context for each token prediction and
makes it independent of the image resolution. This allows
us to provide an effective way to achieve a better tradeoff
between the inference speed and the image quality by a
simple reparameterization of the diffusion image decoder.
Speciﬁcally, in each step, we ask the decoder to predict
the original noise-free image instead of the noise-reduced
image in the next denoising diffusion step. Through ex-
periments we have found that the VQ-Diffusion method
with reparameterization can be ﬁfteen times faster than AR
methods while achieving a better image quality.
2. Related Work
GAN-based Text-to-image generation. In the past few
years, Generative Adversarial Networks (GANs) [18] have
shown promising results on many tasks [19–21],, especially
text-to-image generation [5, 8, 9, 14, 17, 25, 27, 31–34, 38,
43, 44, 50, 51, 58, 60–63, 67–73]. GAN-INT-CLS [50] was
the ﬁrst to use a conditional GAN formulation for text-to-
image generation.
Based on this formulation, some ap-
proaches [34, 44, 67–71, 73] were proposed to further im-
prove the generation quality. These models generate high
ﬁdelity images on single domain datasets, e.g., birds [66]
and ﬂowers [40]. However, due to the inductive bias on the
locality of convolutional neural networks, they struggle on
complex scenes with multiple objects, such as those in the
MS-COCO dataset [36].
Other works [25,33] adopt a two-step process which ﬁrst
infer the semantic layout then generate different objects, but
this kind of method requires ﬁne-grained object labels, e.g.,
object bounding boxes or segmentation maps.
Autoregressive Models. AR models [4,46,47] have shown
powerful capability of density estimation and have been
applied for image generation [7, 16, 41, 42, 49, 53, 64] re-
cently. PixelRNN [53,64], Image Transformer [42] and Im-
ageGPT [7] factorized the probability density on an image
over raw pixels. Thus, they only generate low-resolution
images, like 64 × 64, due to the unaffordable amount of
computation for large images.
VQ-VAE [41, 49], VQGAN [16] and ImageBART [15]
train an encoder to compress the image into a low-
dimensional discrete latent space and ﬁt the density of the
hidden variables. It greatly improves the performance of
image generation.
DALL-E [48], CogView [13] and M6 [35] propose AR-
based text-to-image frameworks. They model the joint dis-
tribution of text and image tokens.
With powerful large
transformer structure and massive text-image pairs, they

greatly advance the quality of text-to-image generation, but
still have weaknesses of unidirectional bias and accumu-
lated prediction errors due to the limitation of AR models.
Denoising Diffusion Probabilistic Models. Diffusion gen-
erative models were ﬁrst proposed in [59] and achieved
strong results on image generation [12,23,24,39] and image
super super-resolution [52] recently. However, most previ-
ous works only considered continuous diffusion models on
the raw image pixels. Discrete diffusion models were also
ﬁrst described in [59], and then applied to text generation
in Argmax Flow [26]. D3PMs [1] applies discrete diffusion
to image generation. However, it also estimates the density
of raw image pixels and can only generate low-resolution
(e.g.,32 × 32) images.
3. Background:
Learning Discrete Latent
Space of Images Via VQ-VAE
Transformer architectures have shown great promise in
image synthesis due to their outstanding expressivity [7,16,
48]. In this work, we aim to leverage the transformer to
learn the mapping from text to image. Since the compu-
tation cost is quadratic to the sequence length, it is com-
putationally prohibitive to directly model raw pixels using
transformers. To address this issue, recent works [16, 41]
propose to represent an image by discrete image tokens with
reduced sequence length. Hereafter a transformer can be ef-
fectively trained upon this reduced context length and learn
the translation from the text to image tokens.
Formally, a vector quantized variational autoencoder
(VQ-VAE) [41] is employed. The model consists of an en-
coder E, a decoder G and a codebook Z = {zk}K
k=1 ∈
RK×d containing a ﬁnite number of embedding vectors ,
where K is the size of the codebook and d is the dimen-
sion of codes. Given an image x ∈RH×W ×3, we obtain
a spatial collection of image tokens zq with the encoder
z = E(x) ∈Rh×w×d and a subsequent spatial-wise quan-
tizer Q(·) which maps each spatial feature zij into its clos-
est codebook entry zk:
zq = Q(z) =

argmin
zk∈Z
∥zij −zk∥2
2

∈Rh×w×d
(1)
Where h × w represents the encoded sequence length and
is usually much smaller than H × W. Then the image can
be faithfully reconstructed via the decoder, i.e., ˜x = G(zq).
Hence, image synthesis is equivalent to sampling image to-
kens from the latent distribution. Note that the image to-
kens are quantized latent variables in the sense that they
take discrete values. The encoder E, the decoder G and
the codebook Z can be trained end-to-end via the following
loss function:
LVQVAE = ∥x −˜x∥1+∥sg[E(x)] −zq∥2
2
+β∥sg[zq] −E(x)∥2
2.
(2)
Where, sg[·] stands for the stop-gradient operation. In prac-
tice, we replace the second term of Equation 2 with expo-
nential moving averages (EMA) [41] to update the code-
book entries which is proven to work better than directly
using the loss function.
4. Vector Quantized Diffusion Model
Given the text-image pairs, we obtain the discrete image
tokens x ∈ZN with a pretrained VQ-VAE, where N = hw
represents the sequence length of tokens. Suppose the size
of the VQ-VAE codebook is K, the image token xi at loca-
tion i takes the index that speciﬁes the entries in the code-
book, i.e., xi ∈{1, 2, ..., K}. On the other hand, the text to-
kens y ∈ZM can be obtained through BPE-encoding [56].
The overall text-to-image framework can be viewed as max-
imizing the conditional transition distribution q(x|y).
Previous autoregressive models, e.g., DALL-E [48] and
CogView [13], sequentially predict each image token de-
pends on the text tokens as well as the previously predicted
image tokens, i.e., q(x|y) = QN
i=1 q(xi|x1, · · · , xi−1, y).
While achieving remarkable quality in text-to-image syn-
thesis, there exist several limitations of autoregressive mod-
eling. First, image tokens are predicted in a unidirectional
ordering, e.g., raster scan, which neglects the structure of
2D data and restricts the expressivity for image modeling
since the prediction of a speciﬁc location should not merely
attend to the context on the left or the above. Second, there
is a train-test discrepancy as the training employs ground
truth whereas the inference relies on the prediction as pre-
vious tokens. The so-called “teacher-forcing” practice [15]
or exposure bias [54] leads to error accumulation due to the
mistakes in the earlier sampling. Moreover, it requires a
forward pass of the network to predict each token, which
consumes an inordinate amount of time even for the sam-
pling in the latent space of low resolution (i.e., 32 × 32),
making the AR model impractical for real usage.
We aim to model the VQ-VAE latent space in a
non-autoregressive manner.
The proposed VQ-Diffusion
method maximizes the probability q(x|y) with the diffu-
sion model [23, 59], an emerging approach that produces
compelling quality on image synthesis [12]. While the ma-
jority of recent works focus on continuous diffusion mod-
els, using them for categorical distribution is much less re-
searched [1, 26]. In this work, we propose to use its con-
ditional variant discrete diffusion process for text-to-image
generation. We will subsequently introduce the discrete dif-
fusion process inspired by the masked language modeling
(MLM) [11], and then discuss how to train a neural network
to reverse this process.
4.1. Discrete diffusion process
On a high level, the forward diffusion process gradu-
ally corrupts the image data x0 via a ﬁxed Markov chain

𝑧𝑧1 𝑧𝑧2
𝑧𝑧𝐾𝐾
Encoder
Decoder
𝑞𝑞𝑥𝑥𝑡𝑡𝑥𝑥𝑡𝑡−1
𝑡𝑡
Step1: VQ-VAE
Step2: VQ-Diffusion
2 4 … 1 3
𝑝𝑝𝜃𝜃𝑥𝑥𝑡𝑡−1|𝑥𝑥𝑡𝑡, 𝑦𝑦
A single giraffe 
grassing leave 
outside
Text
Encoder
Transformer
Block
AdaLN
𝑦𝑦
Codebook
𝑧𝑧3 𝑧𝑧4
𝑄𝑄ȉ
2
5
2
1
3
4
1
𝑥𝑥𝑇𝑇
𝑥𝑥𝑡𝑡
𝑥𝑥𝑡𝑡−1
𝑥𝑥0
…
…
Diffusion Image Decoder
BPE
Transformer
Block
AdaLN
M 1 … M M
2 1 … M M
2 4 … M 3
Figure 1. Overall framework of our method. It starts with the VQ-
VAE. Then, the VQ-Diffusion models the discrete latent space by
reversing a forward diffusion process that gradually corrupts the
input via a ﬁxed Markov chain.
q(xt|xt−1), e.g., random replace some tokens of xt−1.
After a ﬁxed number of T timesteps, the forward pro-
cess yields a sequence of increasingly noisy latent variables
z1, ..., zT of the same dimensionality as z0, and zT be-
comes pure noise tokens. Starting from the noise zT , the
reverse process gradually denoises the latent variables and
restore the real data x0 by sampling from the reverse dis-
tribution q(xt−1|xt, x0) sequentially. However, since x0
is unknown in the inference stage, we train a transformer
network to approximate the conditional transit distribution
pθ(xt−1|xt, y) depends on the entire data distribution.
To be more speciﬁc, consider a single image token xi
0
of x0 at location i, which takes the index that speciﬁes the
entries in the codebook, i.e., xi
0 ∈{1, 2, ..., K}. Without in-
troducing confusion, we omit superscripts i in the following
description. We deﬁne the probabilities that xt−1 transits to
xt using the matrices [Qt]mn = q(xt = m|xt−1 = n) ∈
RK×K. Then the forward Markov diffusion process for the
whole token sequence can be written as,
q(xt|xt−1) = v⊤(xt)Qtv(xt−1)
(3)
where v(x) is a one-hot column vector which length is K
and only the entry x is 1. The categorical distribution over
xt is given by the vector Qtv(xt−1).
Importantly, due to the property of Markov chain, one
can marginalize out the intermediate steps and derive the
probability of xt at arbitrary timestep directly from x0 as,
q(xt|x0) = v⊤(xt)Qtv(x0), with Qt = Qt · · · Q1. (4)
Besides, another notable characteristic is that by con-
ditioning on z0, the posterior of this diffusion process is
tractable, i.e.,
q(xt−1|xt, x0) = q(xt|xt−1, x0)q(xt−1|x0)
q(xt|x0)
=
 v⊤(xt)Qtv(xt−1)
  v⊤(xt−1)Qt−1v(x0)

v⊤(xt)Qtv(x0)
.
(5)
The transition matrix Qt is crucial to the discrete diffu-
sion model and should be carefully designed such that it is
not too difﬁcult for the reverse network to recover the signal
from noises.
Previous works [1, 26] propose to introduce a small
amount of uniform noises to the categorical distribution and
the transition matrix can be formulated as,
Qt =


αt + βt
βt
· · ·
βt
βt
αt + βt
· · ·
βt
...
...
...
...
βt
βt
· · ·
αt + βt


(6)
with αt ∈[0, 1] and βt = (1 −αt)/K. Each token has a
probability of (αt + βt) to remain the previous value at the
current step while with a probability of Kβt to be resampled
uniformly over all the K categories.
Nonetheless, the data corruption using uniform diffusion
is a somewhat aggressive process that may pose challenge
for the reverse estimation. First, as opposed to the Gaussian
diffusion process for ordinal data, an image token may be
replaced to an utterly uncorrelated category, which leads to
an abrupt semantic change for that token. Second, the net-
work has to take extra efforts to ﬁgure out the tokens that
have been replaced prior to ﬁxing them. In fact, due to the
semantic conﬂict within the local context, the reverse esti-
mation for different image tokens may form a competition
and run into the dilemma of identifying the reliable tokens.
Mask-and-replace diffusion strategy. To solve the above
issues of uniform diffusion, we draw inspiration from mask
language modeling [11] and propose to corrupt the tokens
by stochastically masking some of them so that the cor-
rupted locations can be explicitly known by the reverse net-
work. Speciﬁcally, we introduce an additional special to-
ken, [MASK] token, so each token now has (K + 1) discrete
states. We deﬁne the mask diffusion as follows: each or-
dinary token has a probability of γt to be replaced by the
[MASK] token and has a chance of Kβt to be uniformly dif-
fused, leaving the probability of αt = 1 −Kβt −γt to
be unchanged, whereas the [MASK] token always keeps its
own state. Hence, we can formulate the transition matrix
Qt ∈R(K+1)×(K+1) as,
Qt =


αt + βt
βt
βt
· · ·
0
βt
αt + βt
βt
· · ·
0
βt
βt
αt + βt
· · ·
0
...
...
...
...
...
γt
γt
γt
· · ·
1


.
(7)

Algorithm 1 Training of the VQ-Diffusion, given transition
matrix {Qt}, initial network parameters θ, loss weight λ,
learning rate η.
1: repeat
2:
(I, s) ←sample training image-text pair
3:
x0 ←VQVAE-Encoder(I), y ←BPE(s)
4:
t ∼Uniform({1, · · · , T})
5:
xt ←sample from q(xt|x0)
▷Eqn. 4 and 8
6:
L ←
(
L0,
if t = 1
Lt−1 + λLx0,
otherwise ▷Eqn. 9 and 12
7:
θ ←θ −η∇θL
▷Update network parameters
8: until converged
The beneﬁt of this mask-and-replace transition is that:
1) the corrupted tokens are distinguishable to the network,
which eases the reverse process. 2) Comparing to the mask
only approach in [1], we theoretically prove that it is nec-
essary to include a small amount of uniform noises besides
the token masking, otherwise we get a trivial posterior when
xt ̸= x0. 3) The random token replacement forces the net-
work to understand the context rather than only focusing on
the [MASK] tokens. 4) The cumulative transition matrix Qt
and the probability q(xt|x0) in Equation 4 can be computed
in closed form with:
Qtv(x0) = αtv(x0) + (γt −βt)v(K + 1) + βt
(8)
Where αt = Qt
i=1 αi, γt = 1 −Qt
i=1(1 −γi), and
βt = (1 −αt −γt)/K can be calculated and stored in
advance. Thus, the computation cost of q(xt|x0) is reduced
from O(tK2) to O(K). The proof is given in the supple-
mental material.
4.2. Learning the reverse process
To reverse the diffusion process, we train a denoising
network pθ(xt−1|xt, y) to estimate the posterior transition
distribution q(xt−1|xt, x0). The network is trained to min-
imize the variational lower bound (VLB) [59]:
Lvlb = L0 + L1 + · · · + LT −1 + LT ,
L0 = −log pθ(x0|x1, y),
Lt−1 = DKL(q(xt−1|xt, x0) || pθ(xt−1|xt, y)),
LT = DKL(q(xT |x0) || p(xT )).
(9)
Where p(xT ) is the prior distribution of timestep T. For the
proposed mask-and-replace diffusion, the prior is:
p(xT ) =

βT , βT , · · · , βT , γT
⊤
(10)
Note that since the transition matrix Qt is ﬁxed in the train-
ing, the LT is a constant number which measures the gap
between the training and inference and can be ignored in
the training.
Algorithm 2 Inference of the VQ-Diffusion, given fast in-
ference time stride ∆t, input text s.
1: t ←T, y ←BPE(s)
2: xt ←sample from p(xT )
▷Eqn. 10
3: while t > 0 do
4:
xt ←sample from pθ(xt−∆t|xt, y)
▷Eqn. 13
5:
t ←t −∆t
6: end while
7: return VQVAE-Decoder(xt)
Reparameterization trick on discrete stage. The network
parameterization affects the synthesis quality signiﬁcantly.
Instead of directly predicting the posterior q(xt−1|xt, x0),
recent works [1, 23, 26] ﬁnd that approximating some sur-
rogate variables, e.g., the noiseless target data q(x0) gives
better quality. In the discrete setting, we let the network
predict the noiseless token distribution pθ(˜x0|xt, y) at each
reverse step. We can thus compute the reverse transition
distribution according to:
pθ(xt−1|xt, y) =
K
X
˜x0=1
q(xt−1|xt, ˜x0)pθ(˜x0|xt, y). (11)
Based on the reparameterization trick, we can introduce an
auxiliary denoising objective, which encourages the net-
work to predict noiseless token x0:
Lx0 = −log pθ(x0|xt, y)
(12)
We ﬁnd that combining this loss with Lvlb improves the im-
age quality.
Model architecture.
We propose an encoder-decoder
transformer to estimate the distribution pθ(˜x0|xt, y). As
shown in Figure 1, the framework contains two parts: a
text encoder and a diffusion image decoder. Our text en-
coder takes the text tokens y and yields a conditional fea-
ture sequence. The diffusion image decoder takes the im-
age token xt and timestep t and outputs the noiseless to-
ken distribution pθ(˜x0|xt, y). The decoder contains several
transformer blocks and a softmax layer. Each transformer
block contains a full attention, a cross attention to com-
bine text information and a feed forward network block.
The current timestep t is injected into the network with
Adaptive Layer Normalization [2](AdaLN) operator, i.e.,
AdaLN(h, t) = atLayerNorm(h) + bt, where h is the in-
termediate activations, at and bt are obtained from a linear
projection of the timestep embedding.
Fast inference strategy In the inference stage, by leverag-
ing the reparameterization trick, we can skip some steps in
diffusion model to achieve a faster inference.
Speciﬁcally, assuming the time stride is ∆t, instead
of sampling images in the chain of xT , xT −1, xT −2...x0,
we sample images in the chain of xT , xT −∆t, xT −2∆t...x0

A small gray bird 
with white and 
dark gray wingbars
and white breast
A small sized blue 
bird that has a 
short pointed bill
This beautiful little 
bird has a white 
breast and very 
intriguing red eyes
The long wings 
spreaded showing 
the breast and the 
belly of the large bird
An airplane that is 
parked at airport
A giraffe is 
standing in 
a green field
Some children are 
playing soccer 
on the field
A white and blue 
bus driving down a 
road next to trees
Ours
DF-GAN
DM-GAN
Figure 2. Comparison with GAN-based method on CUB-200 and MSCOCO datasets.
with the reverse transition distribution:
pθ(xt−∆t|xt, y) =
K
X
˜x0=1
q(xt−∆t|xt, ˜x0)pθ(˜x0|xt, y).
(13)
We found it makes the sampling more efﬁcient which
only causes little harm to quality. The whole training and
inference algorithm is shown in Algorithm 1 and 2.
5. Experiments
In this section, we ﬁrst introduce the overall experiment
setups and then present extensive results to demonstrate the
superiority of our approach in text-to-image synthesis. Fi-
nally, we point out that our method is a general image syn-
thesis framework that achieves great performance on other
generation tasks, including unconditional and class condi-
tional image synthesis.
Datasets.
To demonstrate the capability of our pro-
posed method for text-to-image synthesis, we conduct
experiments on CUB-200 [66], Oxford-102 [40], and
MSCOCO [36] datasets. The CUB-200 dataset contains
8855 training images and 2933 test images belonging to
200 bird species. Oxford-102 dataset contains 8189 im-
ages of ﬂowers of 102 categories. Each image in CUB-
200 and Oxford-102 dataset contains 10 text descriptions.
MSCOCO dataset contains 82k images for training and 40k
images for testing. Each image in this dataset has ﬁve text
descriptions.
To further demonstrate the scalability of our method,
we also train our model on large scale datasets, including
Conceptual Captions [6, 57] and LAION-400M [55]. The
Conceptual Caption dataset, including both CC3M [57] and
CC12M [6] datasets, contains 15M images. To balance the
text and image distribution, we ﬁlter a 7M subset according
to the word frequency. The LAION-400M dataset contains
400M image-text pairs. We train our model on three subsets
from LAION, i.e., cartoon, icon, and human, each of them
contains 0.9M, 1.3M, 42M images, respectively. For each
subset, we ﬁlter the data according to the text.
Traning Details.
Our VQ-VAE’s encoder and decoder
follow the setting of VQGAN [16] which leverages the
GAN loss to get a more realistic image. We directly adopt
the publicly available VQGAN model trained on Open-
Images [30] dataset for all text-to-image synthesis experi-
ments. It converts 256×256 images into 32×32 tokens. The
codebook size K = 2886 after removing useless codes. We
adopt a publicly available tokenizer of the CLIP model [45]
as text encoder, yielding a conditional sequence of length
77. We ﬁx both image and text encoders in our training.
For fair comparison with previous text-to-image meth-
ods under similar parameters, we build two different diffu-
sion image decoder settings: 1) VQ-Diffusion-S (Small),
it contains 18 transformer blocks with dimension of 192.
The model contains 34M parameters. 2) VQ-Diffusion-B
(Base), it contains 19 transformer blocks with dimension of
1024. The model contains 370M parameters.
In order to show the scalability of our method, we also
train our base model on a larger database Conceptual Cap-
tions, and then ﬁne-tune it on each database. This model is
denoted as VQ-Diffusion-F.
For the default setting, we set timesteps T = 100 and
loss weight λ = 0.0005. For the transition matrix, we lin-
early increase γt and βt from 0 to 0.9 and 0.1, respectively.
We optimize our network using AdamW [37] with β1 = 0.9
and β2 = 0.96. The learning rate is set to 0.00045 after
5000 iterations of warmup. More training details are pro-
vided in the appendix.

5.1. Comparison with state-of-the-art methods
We qualitatively compare the proposed method with sev-
eral state-of-the-art methods, including some GAN-based
methods [60, 61, 63, 67, 70, 71, 73], DALL-E [48] and
CogView [13], on MSCOCO, CUB-200 and Oxford-102
datasets. We calculate the FID [22] between 30k gener-
ated images and 30k real images, and show the results in
Table 1.
We can see that our small model, VQ-Diffusion-S, which
has the similar parameter number with previous GAN-based
models, has strong performance on CUB-200 and Oxford-
102 datasets. Our base model, VQ-Diffusion-B, further im-
proves the performance. And our VQ-Diffusion-F model
achieves the best results and surpasses all previous meth-
ods by a large margin, even surpassing DALL-E [48] and
CogView [13], which have ten times more parameters than
ours, on MSCOCO dataset.
Some visualized comparison results with DM-GAN [73]
and DF-GAN [63] are shown in Figure 2. Obviously, our
synthesized images have better realistic ﬁne-grained details
and are more consistent with the input text.
5.2. In the wild text-to-image synthesis
To demonstrate the capability of generating in-the-wild
images, we train our model on three subsets from LAION-
400M dataset, e.g., cartoon, icon and human. We provide
our results here in Figure 3. Though our base model is much
smaller than previous works like DALL-E and CogView, we
also achieved a strong performance.
Compared with the AR method which generates images
from top-left to down-right, our method generates images
in a global manner. It makes our method can be applied to
many vision tasks, e.g., irregular mask inpainting. For this
task, we do not need to re-train a new model. We simply
set the tokens in the irregular region as [MASK] token, and
send them to our model. This strategy supports both uncon-
ditional mask inpainting and text conditional mask inpaint-
ing. We show these results in the appendix.
5.3. Ablations
Number of timesteps.
We investigate the timesteps in
training and inference. As shown in Table 2, we perform
the experiment on the CUB-200 dataset. We ﬁnd when the
training steps increase from 10 to 100, the result improves,
when it further increase to 200, it seems saturated. So we
set the default timesteps number to 100 in our experiments.
To demonstrate the fast inference strategy, we evaluate the
generated images from 10, 25, 50, 100 inference steps on
ﬁve models with different training steps. We ﬁnd it still
maintains a good performance when dropping 3/4 inference
steps, which may save about 3/4 inference times.
Mask-and-replace diffusion strategy. We explore how the
mask-and-replace strategy beneﬁts our performance on the
MSCOCO
CUB-200
Oxford-102
StackGAN [70]
74.05
51.89
55.28
StackGAN++ [71]
81.59
15.30
48.68
EFF-T2I [60]
-
11.17
16.47
SEGAN [61]
32.28
18.17
-
AttnGAN [67]
35.49
23.98
-
DM-GAN [73]
32.64
16.09
-
DF-GAN [63]
21.42
14.81
-
DAE-GAN [51]
28.12
15.19
-
DALLE [48]
27.50
56.10
-
Cogview [13]
27.10
-
-
VQ-Diffusion-S
30.17
12.97
14.95
VQ-Diffusion-B
19.75
11.94
14.88
VQ-Diffusion-F
13.86
10.32
14.10
Table 1.
FID comparison of different text-to-image synthesis
method on MSCOCO, CUB-200, and Oxford-102 datasets.
A handsome man 
with thick eyebrows
and moustache
The man with 
sunglasses has
a big beard
The sunset on the 
beach is wonderful
Snow mountain 
and tree reflection 
in the lake
Black and white
icon of man 
and woman 
Two smiling 
beautiful ladies are 
standing together
A cartoon boy
is smiling
A movie poster of 
mountain and sea
A red bus 
is driving 
on the road
A picture of a 
very tall stop sign
A bare kitchen has 
wood cabinets and
white appliances
A green heart
with shadow
Figure 3. In the wild text-to-image synthesis results.
Oxford-102 dataset. We set different ﬁnal mask rate (γT )
to investigate the effect. Both mask only strategy (γT = 1)
and replace only strategy (γT = 0) are special cases of our
mask-and-replace strategy. From Figure 4, we ﬁnd it get
the best performance when M = 0.9. When M > 0.9,
it may suffer from the error accumulation problem, when
M < 0.9, the network may be difﬁcult to ﬁnd which region
needs to pay more attention.
Truncation. We also demonstrate that the truncation sam-
pling strategy is extremely important for our discrete dif-
fusion based method. It may avoid the network sampling
from low probability tokens. Speciﬁcally, we only keep top
r tokens of pθ(˜x0|xt, y) in the inference stage. We evalu-
ate the results with different truncation rates r on CUB-200

inference steps
training steps
10
25
50
100
200
10
32.35
27.62
23.47
19.84
20.96
25
-
18.53
15.25
14.03
16.13
50
-
-
13.82
12.45
13.67
100
-
-
-
11.94
12.27
200
-
-
-
-
11.80
Table 2. Ablation study on training steps and inference steps. Each
column shares the same training steps while each row shares the
same inference steps.
Figure 4. Ablation study on the mask rate and the truncation rate.
Model
steps
FID
throughput(imgs/s)
VQ-AR-S
18.12
0.08
VQ-Diffusion-S
25
15.46
1.25
VQ-Diffusion-S
50
13.62
0.67
VQ-Diffusion-S
100
12.97
0.37
VQ-AR-B
17.76
0.03
VQ-Diffusion-B
25
14.03
0.47
VQ-Diffusion-B
50
12.45
0.24
VQ-Diffusion-B
100
11.94
0.13
Table 3. Comparison between VQ-Diffusion and VQ-AR mod-
els. By changing the inference steps, the VQ-Diffusion model is
15 times faster than the VQ-AR model while maintaining better
performance.
dataset. As shown in Figure 4, we ﬁnd that it achieves the
best performance when the truncation rate equals 0.86.
VQ-Diffusion vs VQ-AR. For a fair comparison, we re-
place our diffusion image decoder with an autoregressive
decoder with the same network structure and keep other set-
tings the same, including both image and text encoders. The
autoregressive model is denoted as VQ-AR-S and VQ-AR-
B, corresponding to VQ-Diffusion-S and VQ-Diffusion-B.
The experiment is performed on the CUB-200 dataset. As
shown in Table 3 , on both -S and -B settings the VQ-
Diffusion model surpasses the VQ-AR model by a large
margin. Meanwhile, we evaluate the throughput of both
methods on a V100 GPU with a batch size of 32. The VQ-
Diffusion with the fast inference strategy is 15 times faster
than the VQ-AR model with a better FID score.
5.4. Uniﬁed generation model
Our method is general, which can also be applied to
other image synthesis tasks, e.g., unconditional image syn-
Model
ImageNet
FFHQ
StyleGAN2 [29]
-
3.8
BigGAN [3]
7.53
12.4
BigGAN-deep [3]
6.84
-
IDDPM [39]
12.3
-
ADM-G [12]
10.94
-
VQGAN [16]
15.78
9.6
ImageBART [15]
21.19
9.57
Ours
11.89
6.33
ADM-G (1.0guid) [12]
4.59
-
VQGAN (acc0.05) [16]
5.88
-
ImageBART (acc0.05) [15]
7.44
-
Ours (acc0.05)
5.32
-
Table 4. FID score comparison for class-conditional synthesis on
ImageNet, and unconditional synthesis on FFHQ dataset. ’guid’
denotes using classiﬁer guidance [12], ’acc’ denotes adopting ac-
ceptance rate [16].
thesis and image synthesis conditioned on labels. To gen-
erate images from a given class label, we ﬁrst remove the
text encoder network and cross attention part in transformer
blocks, and inject the class label through the AdaLN oper-
ator. Our network contains 24 transformer blocks with di-
mension 512. We train our model on the ImageNet dataset.
For VQ-VAE, we adopt the publicly available model from
VQ-GAN [16] trained on ImageNet dataset, which down-
samples images from 256 × 256 to 16 × 16.
For un-
conditional image synthesis, we trained our model on the
FFHQ256 dataset, which contains 70k high quality face
images. The image encoder also downsamples images to
16 × 16 tokens.
We assess the performance of our model in terms of FID
and compare with a variety of previously established mod-
els [3, 12, 15, 16, 39]. For a fair comparison, we calculate
FID between 50k generated images and all real images. Fol-
lowing [16] we can further increase the quality by only ac-
cepting images with a top 5% classiﬁcation score, denoted
as acc0.05. We show the quantitative results in Table 4.
While some task-specialized GAN models report better FID
scores, our approach provides a uniﬁed model that works
well across a wide range of tasks.
6. Conclusion
In this paper, we present a novel text-to-image architec-
ture named VQ-Diffusion. The core design is to model the
VQ-VAE latent space in a non-autoregressive manner. The
proposed mask-and-replace diffusion strategy avoids the ac-
cumulation of errors of the AR model. Our model has the
capacity to generate more complex scenes, which surpasses
previous GAN-based text-to-image methods. Our method
is also general and produces strong results on unconditional
and conditional image generation.

Acknowledgement
We thank Qiankun Liu from University of Science and
Technology of China for his help, he provided the initial
code and datasets.
References
[1] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tar-
low, and Rianne van den Berg.
Structured denoising dif-
fusion models in discrete state-spaces.
arXiv preprint
arXiv:2107.03006, 2021. 2, 3, 4, 5
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-
ton. Layer normalization. arXiv preprint arXiv:1607.06450,
2016. 5
[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large
scale gan training for high ﬁdelity natural image synthesis.
arXiv preprint arXiv:1809.11096, 2018. 8
[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners.
arXiv preprint
arXiv:2005.14165, 2020. 1, 2
[5] Miriam Cha, Youngjune L Gwon, and HT Kung. Adversar-
ial learning of semantic relevance in text to image synthesis.
In Proceedings of the AAAI conference on artiﬁcial intelli-
gence, volume 33, pages 3272–3279, 2019. 2
[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12m: Pushing web-scale image-text pre-
training to recognize long-tail visual concepts. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 3558–3568, 2021. 6
[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-
woo Jun, David Luan, and Ilya Sutskever. Generative pre-
training from pixels. In International Conference on Ma-
chine Learning, pages 1691–1703. PMLR, 2020. 2, 3
[8] Jun Cheng, Fuxiang Wu, Yanling Tian, Lei Wang, and
Dapeng Tao. Rifegan: Rich feature generation for text-to-
image synthesis from prior knowledge. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 10911–10920, 2020. 2
[9] Ayushman Dash, John Cristian Borges Gamboa, Sheraz
Ahmed, Marcus Liwicki, and Muhammad Zeshan Afzal.
Tac-gan-text conditioned auxiliary classiﬁer generative ad-
versarial network. arXiv preprint arXiv:1703.06412, 2017.
2
[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, pages 248–255. Ieee, 2009. 2
[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert:
Pre-training of deep bidirectional
transformers for language understanding.
arXiv preprint
arXiv:1810.04805, 2018. 1, 2, 3, 4
[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat
gans on image synthesis. arXiv preprint arXiv:2105.05233,
2021. 1, 3, 8
[13] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. arXiv preprint arXiv:2105.13290,
2021. 2, 3, 7
[14] Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, De-
von Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua
Bengio, and Graham W Taylor. Tell, draw, and repeat: Gen-
erating and modifying images based on continual linguistic
instruction. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 10304–10312, 2019.
2
[15] Patrick Esser, Robin Rombach, Andreas Blattmann, and
Bj¨orn Ommer. Imagebart: Bidirectional context with multi-
nomial diffusion for autoregressive image synthesis. arXiv
preprint arXiv:2108.08827, 2021. 1, 2, 3, 8
[16] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming
transformers for high-resolution image synthesis.
In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pages 12873–12883, 2021. 2, 3, 6,
8, 12
[17] Lianli Gao, Daiyuan Chen, Jingkuan Song, Xing Xu, Dongx-
iang Zhang, and Heng Tao Shen. Perceptual pyramid adver-
sarial networks for text-to-image synthesis. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 33,
pages 8312–8319, 2019. 2
[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. Generative adversarial nets. In Advances in
Neural Information Processing Systems, pages 2672–2680,
2014. 2
[19] Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Giqa:
Generated image quality assessment. In European Confer-
ence on Computer Vision, pages 369–385. Springer, 2020.
2
[20] Shuyang Gu, Jianmin Bao, Dong Chen, and Fang Wen. Pri-
organ: Real data prior for generative adversarial nets. arXiv
preprint arXiv:2006.16990, 2020. 2
[21] Shuyang Gu, Jianmin Bao, Hao Yang, Dong Chen, Fang
Wen, and Lu Yuan. Mask-guided portrait editing with con-
ditional gans. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 3436–
3445, 2019. 2
[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,
Bernhard Nessler, and Sepp Hochreiter. Gans trained by a
two time-scale update rule converge to a local nash equilib-
rium. Advances in neural information processing systems,
30, 2017. 7
[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. arXiv preprint arXiv:2006.11239,
2020. 1, 3, 5
[24] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,
Mohammad Norouzi, and Tim Salimans. Cascaded diffusion
models for high ﬁdelity image generation. arXiv preprint
arXiv:2106.15282, 2021. 3
[25] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and
Honglak Lee. Inferring semantic layout for hierarchical text-
to-image synthesis. In Proceedings of the IEEE Conference

on Computer Vision and Pattern Recognition, pages 7986–
7994, 2018. 2
[26] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick
Forr´e, and Max Welling. Argmax ﬂows and multinomial dif-
fusion: Towards non-autoregressive language models. arXiv
preprint arXiv:2102.05379, 2021. 3, 4, 5
[27] Yupan Huang, Hongwei Xue, Bei Liu, and Yutong Lu. Uni-
fying multimodal transformer for bi-directional image and
text generation. In Proceedings of the 29th ACM Interna-
tional Conference on Multimedia, pages 1138–1147, 2021.
2
[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based
generator architecture for generative adversarial networks.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4401–4410, 2019. 2
[29] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,
Jaakko Lehtinen, and Timo Aila.
Analyzing and improv-
ing the image quality of stylegan.
In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 8110–8119, 2020. 8
[30] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami
Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Ui-
jlings, Stefan Popov, Andreas Veit, et al. Openimages: A
public dataset for large-scale multi-label and multi-class im-
age classiﬁcation.
Dataset available from https://github.
com/openimages, 2(3):18, 2017. 6, 12
[31] Qicheng Lao, Mohammad Havaei, Ahmad Pesaranghader,
Francis Dutil, Lisa Di Jorio, and Thomas Fevens. Dual ad-
versarial inference for text-to-image synthesis. In Proceed-
ings of the IEEE/CVF International Conference on Com-
puter Vision, pages 7567–7576, 2019. 2
[32] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip HS
Torr. Controllable text-to-image generation. arXiv preprint
arXiv:1909.07083, 2019. 2
[33] Wenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang,
Xiaodong He, Siwei Lyu, and Jianfeng Gao. Object-driven
text-to-image synthesis via adversarial training. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 12174–12182, 2019. 2
[34] Jiadong Liang, Wenjie Pei, and Feng Lu. Cpgan: Content-
parsing generative adversarial networks for text-to-image
synthesis.
In European Conference on Computer Vision,
pages 491–508. Springer, 2020. 2
[35] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding,
Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan
Jia, et al.
M6: A chinese multimodal pretrainer.
arXiv
preprint arXiv:2103.00823, 2021. 2
[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
European conference on computer vision, pages 740–755.
Springer, 2014. 2, 6
[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 6
[38] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovit-
skiy, and Jason Yosinski. Plug & play generative networks:
Conditional iterative generation of images in latent space.
In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 4467–4477, 2017. 2
[39] Alex Nichol and Prafulla Dhariwal.
Improved de-
noising diffusion probabilistic models.
arXiv preprint
arXiv:2102.09672, 2021. 3, 8
[40] Maria-Elena Nilsback and Andrew Zisserman. Automated
ﬂower classiﬁcation over a large number of classes. In 2008
Sixth Indian Conference on Computer Vision, Graphics &
Image Processing, pages 722–729. IEEE, 2008. 2, 6
[41] Aaron
van
den
Oord,
Oriol
Vinyals,
and
Koray
Kavukcuoglu.
Neural discrete representation learning.
arXiv preprint arXiv:1711.00937, 2017. 2, 3, 12
[42] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz
Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-
age transformer. In International Conference on Machine
Learning, pages 4055–4064. PMLR, 2018. 2
[43] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.
Learn, imagine and create: Text-to-image generation from
prior knowledge. Advances in Neural Information Process-
ing Systems, 32:887–897, 2019. 2
[44] Tingting Qiao, Jing Zhang, Duanqing Xu, and Dacheng Tao.
Mirrorgan: Learning text-to-image generation by redescrip-
tion. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 1505–1514,
2019. 2
[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. arXiv preprint arXiv:2103.00020, 2021. 6, 12
[46] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
Sutskever. Improving language understanding by generative
pre-training. 2018. 1, 2
[47] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
pervised multitask learners. OpenAI blog, 1(8):9, 2019. 1,
2
[48] Aditya Ramesh,
Mikhail Pavlov,
Gabriel Goh,
Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever.
Zero-shot text-to-image generation.
arXiv
preprint arXiv:2102.12092, 2021. 1, 2, 3, 7
[49] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-
ing diverse high-ﬁdelity images with vq-vae-2. In Advances
in neural information processing systems, pages 14866–
14876, 2019. 2
[50] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-
geswaran, Bernt Schiele, and Honglak Lee. Generative ad-
versarial text to image synthesis. In International Conference
on Machine Learning, pages 1060–1069. PMLR, 2016. 2
[51] Shulan Ruan, Yong Zhang, Kun Zhang, Yanbo Fan, Fan
Tang, Qi Liu, and Enhong Chen. Dae-gan: Dynamic aspect-
aware gan for text-to-image synthesis. In Proceedings of the
IEEE/CVF International Conference on Computer Vision,
pages 13960–13969, 2021. 2, 7
[52] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-
imans, David J Fleet, and Mohammad Norouzi.
Image
super-resolution via iterative reﬁnement.
arXiv preprint
arXiv:2104.07636, 2021. 3

[53] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P
Kingma.
Pixelcnn++: Improving the pixelcnn with dis-
cretized logistic mixture likelihood and other modiﬁcations.
arXiv preprint arXiv:1701.05517, 2017. 2
[54] Florian Schmidt. Generalization in generation: A closer look
at exposure bias. arXiv preprint arXiv:1910.00292, 2019. 3
[55] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-ﬁltered 400 million image-text pairs.
arXiv preprint arXiv:2111.02114, 2021. 6
[56] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural
machine translation of rare words with subword units. arXiv
preprint arXiv:1508.07909, 2015. 3
[57] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Soricut. Conceptual captions: A cleaned, hypernymed, im-
age alt-text dataset for automatic image captioning. In Pro-
ceedings of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages
2556–2565, 2018. 6
[58] Shikhar Sharma,
Dendi Suhubdy,
Vincent Michalski,
Samira Ebrahimi Kahou, and Yoshua Bengio. Chatpainter:
Improving text to image generation using dialogue. arXiv
preprint arXiv:1802.08216, 2018. 2
[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics. In International Confer-
ence on Machine Learning, pages 2256–2265. PMLR, 2015.
1, 3, 5
[60] Douglas M Souza, Jˆonatas Wehrmann, and Duncan D Ruiz.
Efﬁcient neural architecture for text-to-image synthesis. In
2020 International Joint Conference on Neural Networks
(IJCNN), pages 1–8. IEEE, 2020. 2, 7
[61] Hongchen Tan, Xiuping Liu, Xin Li, Yi Zhang, and Baocai
Yin. Semantics-enhanced adversarial nets for text-to-image
synthesis.
In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 10501–10510, 2019.
2, 7
[62] Hongchen Tan, Xiuping Liu, Meng Liu, Baocai Yin, and Xin
Li. Kt-gan: knowledge-transfer generative adversarial net-
work for text-to-image synthesis. IEEE Transactions on Im-
age Processing, 30:1275–1290, 2020. 2
[63] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan
Jing, Fei Wu, and Bingkun Bao. Df-gan: Deep fusion gener-
ative adversarial networks for text-to-image synthesis. arXiv
preprint arXiv:2008.05865, 2020. 2, 7
[64] Aaron
Van
Oord,
Nal
Kalchbrenner,
and
Koray
Kavukcuoglu.
Pixel recurrent neural networks.
In In-
ternational
Conference
on
Machine
Learning,
pages
1747–1756. PMLR, 2016. 2
[65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998–6008, 2017. 1
[66] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-
ona, and Serge Belongie. The caltech-ucsd birds-200-2011
dataset. 2011. 2, 6
[67] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,
Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-
grained text to image generation with attentional generative
adversarial networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pages 1316–
1324, 2018. 2, 7
[68] Guojun Yin, Bin Liu, Lu Sheng, Nenghai Yu, Xiaogang
Wang, and Jing Shao. Semantics disentangling for text-to-
image generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
2327–2336, 2019. 2
[69] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and
Yinfei Yang. Cross-modal contrastive learning for text-to-
image generation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
833–842, 2021. 2
[70] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan: Text to photo-realistic image synthesis with stacked
generative adversarial networks. In Proceedings of the IEEE
international conference on computer vision, pages 5907–
5915, 2017. 2, 7
[71] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-
gang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-
gan++: Realistic image synthesis with stacked generative ad-
versarial networks. IEEE transactions on pattern analysis
and machine intelligence, 41(8):1947–1962, 2018. 2, 7
[72] Zizhao Zhang, Yuanpu Xie, and Lin Yang.
Photographic
text-to-image synthesis with a hierarchically-nested adver-
sarial network.
In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pages 6199–
6208, 2018. 2
[73] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan:
Dynamic memory generative adversarial networks for text-
to-image synthesis. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
5802–5810, 2019. 2, 7

A. Implementation details
In our experiments on text-to-image synthesis, we adopt
the public VQ-VAE [41] model provided by VQGAN [16]
trained on the OpenImages [30] dataset, which downsam-
ples images from 256 × 256 to 32 × 32.
We use the
CLIP [45] pretrained model (ViT-B) as our text encoder,
which encodes a sentence to 77 tokens. Our diffusion im-
age decoder consists of several transformer blocks, each
block contains full attention, cross attention, and feed for-
ward network(FFN). Our base model contains 19 trans-
former blocks, the channel of each block is 1024. The FFN
contains two linear layer, which expand the dimension to
4096 in the middle layer. The model contains 370M pa-
rameters. For our small model, it contains 18 transformer
blocks while the channel is 192, the FFN contains two con-
volution layers with kernel size 3, the channel expand rate
is 2. The model contains 34M parameters.
For our class conditional generation model on ImageNet,
we adopt the public VQ-VAE model provided by VQGAN
trained on ImageNet, which downsamples images from
256 × 256 to 16 × 16. Our model contains 24 transformer
blocks, each block contains a full attention layer and a FFN.
The base channel number is 512. Besides, the FFN also uses
convolution instead of linear layer, and the channel expand
rate is 4.
A small pizza
in the plate
A green bird 
with dark head
A man wears
striped tie
A woman wears
white tshirt with 
a rectangular 
pattern on it
Input Image
Mask
Edited Image
Figure 5. Text guided image editing by VQ-Diffusion.
B. Proof of Equation 8
Mathematical induction can be used to prove the Equa-
tion 8 in the paper.
When t = 1, we have
Q1v(x0) =





α1 + β1,
x = x0
β1,
x ̸= x0 and x ̸= K + 1
γ1,
x = K + 1
(14)
which is clearly hold. Suppose the Equation 8 is hold at step
t, then for t = t + 1:
Qt+1v(x0) = Qt+1Qtv(x0)
When x = x0,
Qt+1v(x0)(x) = βtβt+1(K −1) + (αt+1 + βt+1)(αt + βt)
= βt(Kβt+1 + αt+1) + αt(αt+1 + βt+1)
= 1
K (βt(1 −γt+1) + αtβt+1 −βt+1) ∗K + \
αt+1 + βt+1
= 1
K [(1 −αt −γt)(1 −γt+1) + Kαtβt+1 −\
(1 −αt+1 −γt+1)] + αt+1 + βt+1
= 1
K [(1 −γt+1) −αt(1 −γt+1 −Kβt+1) −\
(1 −γt+1) + αt+1] + αt+1 + βt+1
= αt+1 + βt+1
When x = K + 1,
Qt+1v(x0)(x) = γt + (1 −γt)γt+1
= 1 −(1 −γt+1)
= γt+1
When x ̸= x0 and x ̸= K + 1,
Qt+1v(x0)(x) = βt(αt+1 + βt+1) + βtβt+1(K −1) + αtβt+1
= βt(αt+1 + βt+1 ∗K) + αtβt+1
= 1 −αt −γt
K
∗(1 −γt+1) + αtβt+1
= 1
K (1 −γt+1) + αt(βt+1 −1 −γt+1
K
)
= βt+1
So proof done.
C. Results
In this part, we provide more visualization results. First,
we compare our results with XMC-GAN in Figure 7. We
got their results directly from their paper.
The irregular
mask inpainting results are shown in Figure 5. we show
our more in the wild text-to-image results in Figure 6. And
we provide our results on ImageNet and FFHQ in Figure 9
and Figure 8.

A group of 
people gather 
for a photo
A man riding a 
motorcycle 
on the beach
A giraffe walking 
through a green 
grass covered field
A cool cartoon boy
A woman is talking 
in an interview
A woman with 
long straight hair 
wears glasses
A man with 
beard in 1920s
A green train 
is coming 
down the tracks
A woman with 
curly hairs and 
brown skin
Icon of a 
red heart
A handsome man
with beard and
moustache
A red bus is
driving on
the road
Sunset on 
the beach
A living area with a 
television and a table
A group of skiers 
are preparing to ski 
down a mountain
A small galley kitchen 
with wooden cabinets 
and white appliances
A red hydrant
on the grass
Very pretty makeup
with a red lip
Two girls in 
cartoon style
A mountain 
near the lake
A man wears
black suit 
and a tie
A cartoon house
with red roof
A black and white 
landscape photograph
of a black tree 
A woman with
white hair
A hydrant 
in the park
A woman with 
delicate makeup
A Chinese girl 
with long hair
Two beautiful ladies
standing together
Some children 
are sitting in 
the classroom
The face of messi
Face of an 
orange frog in
cartoon style
A cartoon tiger face
A boy and a girl
in cartoon style
A movie poster 
of haunted house
A man wears 
black suit 
and a tie
A picture of 
some food 
in the plate
Two beautiful ladies 
standing together
A woman riding a
red motorcycle 
wearing a helmet
Sketch of a 
woman face
Sunset over the sea
A vector illustration of a tree
A small house in the wilderness
Sunset over the skyline of a city
A handsome man with beard with sunglasses
chocolate
water
wood
cookie
purple
A heart made of ________
A ________ bus is driving on the road
blue
yellow
green
Figure 6. In the wild text-to-image synthesis by VQ-Diffusion.

A group of skiers 
are preparing to 
ski down a mountain
A living area 
with a television
and a table
A group of 
elephants walking 
in muddy water
An image of 
people outside 
playing frisbee
The bus is pulling
off to the side
of the road
A picture of a
very tall stop sign
A boat in the
middle
of the ocean
A picture of some
food on the plate
A long boat is 
sitting on the
clear water
XMC-GAN
Ours
Figure 7. Comparison our results with XMC-GAN, their results come from their paper.
Figure 8. VQ-Diffusion results on FFHQ1024 and FFHQ256 datasets.
c207
c7
c407
c849
c971
golden retriever
cock
ambulance
teapot
bubble
daisy
ostrich
matchstick
c985
c9
c644
Figure 9. VQ-Diffusion results of class conditional synthesis on ImageNet dataset.

