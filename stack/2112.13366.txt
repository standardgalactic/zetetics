AIDA: AN ACTIVE INFERENCE-BASED DESIGN AGENT FOR
AUDIO PROCESSING ALGORITHMS
A PREPRINT
Albert Podusenko∗†,1, Bart van Erp†,1, Magnus Koudahl†,1,2, Bert de Vries1,3
1BIASlab, Department of Electrical Engineering, Eindhoven University of Technology, Eindhoven, The Netherlands
2Nested Minds Solutions, Liverpool, England
3GN Hearing, Eindhoven, The Netherlands
∗a.podusenko@tue.nl
January 11, 2022
ABSTRACT
In this paper we present AIDA, which is an active inference-based agent that iteratively designs
a personalized audio processing algorithm through situated interactions with a human client. The
target application of AIDA is to propose on-the-spot the most interesting alternative values for the
tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisﬁed with their
HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of
optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as
an active inference-based agent with an Expected Free Energy criterion for trial design. This type of
architecture is inspired by neuro-economic models on efﬁcient (Bayesian) trial design in brains and
implies that AIDA comprises generative probabilistic models for acoustic signals and user responses.
We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive
ﬁlters and a user response model based on a Gaussian Process Classiﬁer. The full AIDA agent
has been implemented in a factor graph for the generative model and all tasks (parameter learning,
acoustic context classiﬁcation, trial design, etc.) are realized by variational message passing on the
factor graph. All veriﬁcation and validation experiments and demonstrations are freely accessible at
our GitHub repository.
Keywords Active inference, Bayesian trial design, Hearing aids, Noise reduction, Probabilistic modeling, Source
separation, Speech enhancement, Variational Message passing
1
Introduction
Hearing aids (HA) are often equipped with specialized noise reduction algorithms. These algorithms are developed by
teams of engineers who aim to create a single optimal algorithm that suits any user in any situation. Taking a one-size-
ﬁts-all approach to HA algorithm design leads to two problems that are prevalent throughout today’s hearing aid industry.
First, modeling all possible acoustic environments is simply infeasible. The daily lives of HA users are varied and the
different environments they traverse even more so. Given differing acoustic environments, a single static HA algorithm
cannot possibly account for all eventualities - even without taking into account the particular constraints imposed by the
HA itself, such as limited computational power and allowed processing delays [1]. Secondly, hearing loss is highly
personal and can differ signiﬁcantly between users. Each HA user consequently requires their own, individually tuned
HA algorithm that compensates for their unique hearing loss proﬁle [2, 3, 4] and satisﬁes their personal preferences for
parameter settings [5]. Considering that HAs nowadays often consist of multiple interconnected digital signal processing
units with many integrated parameters, the task of personalizing the algorithm requires exploring a high-dimensional
search space of parameters, which often do not yield a clear physical interpretation. The current most widespread
approach to personalization requires the HA user to physically travel to an audiologist who manually tunes a subset of
all HA parameters. This is a burdensome activity that is not guaranteed to yield an improved listening experience for
the HA user.
arXiv:2112.13366v2  [eess.AS]  10 Jan 2022

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
From these two problems, it becomes clear that we need to move towards a new approach for hearing aid algorithm
design that empowers the user. Ideally, users should be in control of their own HA algorithms and should be able to
change and update them at will instead of having to rely on teams of engineers that operate with long design cycles,
separated from the users’ living experiences.
The question then becomes, how do we move the HA algorithm design away from engineers and into the hands of the
user? While a naive implementation that allows for tuning HA parameters with sliders on, for example, a smartphone is
trivial to develop, even a small number of adjustable parameters gives rise to a large, high-dimensional search space
that the HA user needs to learn to navigate. This puts a large burden on the user, essentially asking them to be their own
trained audiologist. Clearly, this is not a trivial task and this approach is only feasible for a small set of parameters,
which carry a clear physical interpretation. Instead, we wish to support the user with an agent that intelligently proposes
new parameter trials. In this setting, the user is only tasked to cast (positive or negative) appraisals of the current HA
settings . Based on these appraisal, the agent will autonomously traverse the search space with the goal of proposing
satisfying parameter values for that user under the current environmental conditions in as few trials as possible.
Designing an intelligent agent that learns to efﬁciently navigate a parameter space is not trivial. In the solution approach
in this paper, we rely on a probabilistic modeling approach inspired by the free energy principle (FEP) [6]. The FEP is a
framework originally designed to explain the kinds of computations that biological, intelligent agents (such as the human
brain) might be performing. Recent years have seen the FEP applied to the design of synthetic agents as well [7, 8, 9, 10].
A hallmark feature of FEP-based agents is that they exhibit a dynamic trade-off between exploration and exploitation
[11, 12, 13], which is a highly desirable property when learning to navigate an HA parameter space. Concretely, the
FEP proposes that intelligent agents should be modeled as probabilistic models. These types of models do not only
yield point estimates of variables, but also capture uncertainty through modeling full posterior probability distributions.
Furthermore, user appraisals and actions can be naturally incorporated by simply extending the probabilistic model.
Taking a model-based approach also allows for fewer parameters than alternative data-driven solutions, as we can
incorporate ﬁeld-speciﬁc knowledge, making it more suitable for computationally constrained hearing aid devices. The
novelty of our approach is rooted in the fact that the entire proposed systems is framed as a probabilistic generative
model in which we can perform (active) inference through (expected) free energy minimization.
In this paper we present AIDA1, an active inference-based design agent for the situated development of context-
dependent audio processing algorithms, which provides the user with her own controllable audio processing algorithm.
This approach embodies an FEP-based agent that operates in conjunction with an acoustic model and actively learns
optimal context-dependent tuning parameter settings. After formally specifying the problem and solution approach in
Section 2 we make the following contributions:
1. We develop a modular probabilistic model that embodies situated, (acoustic) scene-dependent, and personalized
design of its corresponding hearing aid algorithm in Section 3.1.
2. We develop an expected free energy-based agent (AIDA) in Section 3.2, whose proposals for tuning parameter
settings are well-balanced in terms of seeking more information about the user’s preferences (explorative agent
behavior) versus seeking to optimize the user’s satisfaction levels by taking advantage of previously learned
preferences (exploitative agent behavior).
3. Inference in the acoustic model and AIDA is elaborated upon in Section 4 and their operations are individually
veriﬁed through representative experiments in Section 5. Furthermore, all elements are jointly validated
through a demonstrator application in Section 5.4.
We have intentionally postponed a more thorough review of related work to Section 7 as we deem it more relevant after
the introduction of our solution approach. Finally, Section 6 discusses the novelty and limitations of our approach and
Section 8 concludes this paper.
2
Problem statement and proposed solution approach
2.1
Automated hearing aid tuning by optimization
In this paper we consider the problem of choosing values for the tuning parameters u of a hearing aid algorithm that
processes an acoustic input signal x to output signal y. In Figure 1, we sketch an automated optimization-based
approach to this problem. Assume that we have access to a generic “signal quality” model which rates the quality of a
1Aida is a girl’s name of Arabic origin, meaning “happy”. We use this name as an abbreviation for an "Active Inference-based
Design Agent" that aims to make an end user “happy”.
2

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
optimizer
data base
group model
Figure 1: A schematic overview of the conventional approach to hearing aid algorithm tuning. Here the parameters of
the hearing aid u are optimized with respect to some generic user rating model r(y) for a large data base X of input
data x.
HA output signal y = f(x, u), as a function of the HA input x and parameters u, by a rating r(x, u) ≜r(y). If we
run this system on a representative set of input signals x ∈X, then the tuning problem reduces to the optimization task
u∗= arg max
u
X
x∈X
r(x, u) .
(1)
Unfortunately, in commercial practice, this optimization approach does not always result in satisfactory HA performance,
because of two reasons. First, the signal quality models in the literature have been trained on large databases of
preference ratings from many users and therefore only model the average HA client rather than any speciﬁc client
[14, 15, 16, 17, 18, 19]. Secondly, the optimization approach averages over a large set of different input signals, so it
will not deal with acoustic context-dependent client preferences. By acoustic context, we consider signal properties that
depend on environmental conditions such as being inside, outside, in a car or at the mall. Generally, client preferences
for HA tuning parameters are both highly personal and context-dependent. Therefore, there is a need to develop a
personalized, context-sensitive controller for tuning HA parameters u.
2.2
Situated hearing aid tuning with the user in-the-loop
In this paper, we will develop a personalized, context-aware design agent, based on the architecture shown in Figure 2.
In contrast to Figure 1, the outside world (rather than a database) produces an input signal x under situated conditions
that is processed by a hearing aid algorithm to produce an output signal y. A particular human hearing aid client listens
to the signal y and is invited to cast at any time binary appraisals r ∈{0, 1} about the current performance of the
hearing aid algorithm, where 1 and 0 correspond to the user being satisﬁed and unsatisﬁed, respectively. Context-aware
trials for HA tuning parameters are provided by AIDA. Rather than an ofﬂine design procedure, the whole system
designs continually under situated conditions. The HA device itself houses a custom hearing aid algorithm, based on
state inference in a generative acoustic model. The acoustic model contains two sub-models: 1) a source dynamics
model and 2) a context dynamics model.
Inference in the acoustic model is based on the observed signal x and yields the output y and context c. Based on this
context signal c and previous user appraisals r, AIDA will actively propose new parameters trials u with the goal of
making the user happy. Technically, the objective is that AIDA expects to receive fewer negative appraisals in the future,
relative to not making parameter adaptations, see Section 3.2 for details.
The design of AIDA is non-trivial. For instance, since there is a priori no personalized model of HA ratings for any
particular user, AIDA will have to build such a model on-the-ﬂy from the context c and user appraisals r. Since the
system operates under situated conditions, we want to impose as little burden on the end user as possible. As a result,
most users will only once in a while cast an appraisal and this complicates the learning of a personalized HA rating
model.
To make this desire for very light-weight interactions concrete, we now sketch how we envision a typical interaction
between AIDA and a HA client. Assume that the HA client is in a conversation with a friend at a restaurant. The signal
of interest, in this case, is the friend’s speech signal while the interfering signal is an environmental babble noise signal.
The HA algorithm tries to separate the input signal x into its constituent speech and noise source components, then
applies gains u to each source component and sums these weighted source signals to produce output y. If the HA
3

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
acoustic model
source dynamics
context
dynamics
AIDA
user response
model
Figure 2: A schematic overview of the proposed situated HA design loop containing AIDA. An incoming signal x
enters the hearing aid and is used to infer the context of the user c. Based on this context and previous user appraisals,
AIDA proposes a new set of parameters u for the hearing aid algorithm. Based on the input signal, the proposed
parameters and the current context, the output y of the hearing aid is determined, which are used together with the
context in the hearing aid algorithm. The parameters u are actively optimized by AIDA, based on the inferred context
c from the input signal x and appraisals r from the user in the loop. All individual subsystems represent parts of a
probabilistic generative model as described in Section 3, where the corresponding algorithms follows from performing
probabilistic inference in these models as described in Section 4.
client is happy with the performance of her HA, she will not cast any appraisals. After all, she is in the middle of a
conversation and has no imperative to change the HA behavior. However, if she cannot understand her conversation
partner, the client may covertly tap her watch or make another gesture to indicate that she is not happy with her current
HA settings. In response, AIDA, which may be implemented as a smartwatch application, will reply instantaneously by
sending a tuning parameter update u to the hearing aid algorithm in an effort to ﬁx the client’s current hearing problem.
Since the client’s preferences are context-dependent, AIDA needs to incorporate information about the acoustic context
from HA input x. As an example, the HA user might leave the restaurant for a walk outside. Walking outside presents a
different type of background noise and consequently requires different parameter settings.
Crucially, we would like HA clients to be able to tune their hearing aids without interruption of any ongoing activities.
Therefore, we will not demand that the client has to focus visual attention on interacting with a smartphone app. At
most, we want the client to apply a tap or make a simple gesture that does not draw any attention away from the
ongoing conversation. A second criterion is that we do not want the conversation partner to notice that the client is
interacting with the agent. The client may actually be in a situation (e.g., a business meeting) where it is not appropriate
to demonstrate that her priorities have shifted to tuning her hearing aids. In other words, the interactions must be very
light-weight and covert. A third criterion is that we want the agent to learn from as few appraisals as possible. Note that,
if the HA has 10 tuning parameters and 5 interesting values (very low, low, middle, high, very high) per parameter,
then there are 510 (about 10 million) parameter settings. We do not want the client to get engaged in an endless loop of
disapproving new HA proposals as this will lead to frustration and distraction from the ongoing conversation. Clearly,
this means that each update of the HA parameters cannot be selected randomly: we want the agent to propose the most
interesting values for the tuning parameters, based on all observed past information and certain goal criteria for future
HA behavior. In Section 4.2, we will quantify what most interesting means in this context.
In short, the goal of this paper is to design an intelligent agent that supports user-driven situated design of a personalized
audio processing algorithm through a very light-weight interaction protocol.
4

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
In order to accomplish this task, we will draw inspiration from the way how human brains design algorithms (e.g., for
speech and object recognition, riding a bike, etc.) solely through environmental interactions. Speciﬁcally, we base the
design of AIDA on the Active Inference (AIF) framework. Originating from the ﬁeld of computational neuroscience,
AIF proposes to view the brain as a prediction engine that models sensory inputs. Formally, AIF accomplishes this
through specifying a probabilistic generative model of incoming data. Performing approximate Bayesian inference
in this model by minimizing free energy then constitutes a uniﬁed procedure for both data processing and learning.
To select tuning parameter trials, an AIF agent predicts the expected free energy in the near future, given a particular
choice of parameter settings. AIF provides a single, uniﬁed method for designing all components of AIDA. The design
of a HA system that is controlled by an AIF-based design agent involves solving the following tasks:
1. Classiﬁcation of acoustic context
2. Selecting acoustic context-dependent trials for the HA tuning parameters.
3. Execution of the HA signal processing algorithm (that is controlled by the trial parameters).
Task 1 (context classiﬁcation) involves determining the most probable current acoustic environment. Based on a
dynamic context model (described in Section 3.1.2), we infer the most probable acoustic environment as described in
Section 4.1.
Task 2 (trial design) encompasses proposing alternative settings for the HA tuning parameters. Sections 3.2 and 4.2
describe the user response model and execution of AIDA’s trial selection procedure based on expected free energy
minimization, respectively.
Finally, task 3 (hearing aid algorithm execution) concerns performing variational free energy minimization with respect
to the state variables in a generative probabilistic model for the acoustic signal. In Section 3.1 we describe the generative
acoustic model underlying the HA algorithm and Section 4.3 describes the inferred HA algorithm itself.
Crucially, in the AIF framework, all three tasks can be accomplished by variational free energy minimization in a
generative probabilistic model for observations. Since we can automate variational free energy minimization by a
probabilistic programming language, the only remaining task for the human designer is to specify the generative models.
The next section describes the model speciﬁcation.
3
Model speciﬁcation
In this section, we present the generative model of the AIDA controlled HA system, as illustrated in Figure 2. In
Section 3.1, we describe a generative model for the HA input and output signals x and y respectively. In this model, the
hearing aid algorithm follows through performing probabilistic inference, as will be discussed in Section 4. Part of the
hearing aid algorithm is a mechanism for inferring the current acoustic context. In Section 3.2 we introduce a model for
agent AIDA that is used to infer new parameter trials. A concise summary of the generative model is also presented in
Appendix B and an overview of the corresponding symbols is given in Table 2.
Throughout this section, we will make use of factor graphs for visualization of probabilistic models. In this paper we
focus on Forney-style factor graphs (FFG), as introduced in [20] with notational conventions adopted from [21]. FFGs
represent factorized functions by undirected graphs, whose nodes represent the individual factors of the global function.
The nodes are connected by edges representing the mutual arguments of the factors. In an FFG, a node can be connected
to an arbitrary number of edges, but edges are constrained to have a maximum degree of two. A more detailed review
of probabilistic modeling and factor graphs has been provided in Appendix A.
3.1
Acoustic model
Our acoustic model of the observed signal and hearing aid output consists of a model of the source dynamics of the
underlying signals and a model for the context dynamics.
3.1.1
Model of source dynamics
We assume that the observed acoustic signal x consists of a speech signal (or more generally, a target signal that the HA
client wants to focus on) and an additive noise signal (that the HA client is not interested in), as
xt = st + nt
(2)
where xt ∈R represents the observed signal at time t, i.e. the input to the HA. The speech and noise signals are
represented by st ∈R and nt ∈R, respectively. At this point, the source dynamics of sn and nt need to be further
5

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
speciﬁed. Here we choose to model the speech signal by a time-varying auto-regressive model and the noise signal by a
context-dependent auto-regressive model. The remainder of this subsection will elaborate on both these source models
and will further specify how the hearing aid output is generated. An FFG visualization of the described acoustic model
is depicted in Figure 3.
Historically, Auto-Regressive (AR) models have been widely used to represent speech signals [22, 23]. As the dynamics
of the vocal tract exhibit non-stationary behavior, speech is usually segmented into individual frames that are assumed
to be quasi-stationary. Unfortunately, the signal is often segmented without any prior information about the phonetic
structure of the speech signal. Therefore the quasi-stationarity assumption is likely to be violated and time-varying
dynamics are more likely to occur in the segmented frames [24]. To address this issue, we can use a time-varying prior
for the coefﬁcients of the AR model, leading to a time-varying AR (TVAR) model [25]
θt ∼N (θt−1, ωIM)
(3a)
st ∼N (A(θt)st−1, V (γ))
(3b)
where θt = [θ1t, θ2t, ..., θMt]⊺∈RM, st = [st, st−1, ..., st−M+1]⊺∈RM are the coefﬁcients and states of an M-th
order TVAR model for speech signal st = e⊺
1st. We use N(µ, Σ) to denote a Gaussian distribution with mean µ and
covariance matrix Σ. In this model, the AR coefﬁcients θt are represented by a Gaussian random walk with process
noise covariance ωIM, with IM denoting the identity matrix of size (M × M), scaled by ω ∈R>0. γ ∈R>0 represents
the process noise precision matrix of the AR process. Here, we have adopted the state-space formulation of TVAR
models as in [26], where V (γ) = (1/γ)e1e⊺
1 creates a covariance matrix with a single non-zero entry. We use ei to
denote an appropriately sized Cartesian standard unit vector that represents a column vector of zeros where only the ith
entry is 1. A(θ) denotes the companion matrix of size (M × M), deﬁned as
A(θ) =

θ⊺
IM−1
0

.
(4)
Multiplication of a state vector by this companion matrix, such as A(θt)st−1, basically performs two operations: an
inner product θ⊺
t st−1 and a shift of st−1 by one time step to the past.
The acoustic model also encompasses a model for background noise, such as the sounds at a bar or train station. Many
of these background sounds can be well represented by colored noise [27], which in turn can be modeled by a low-order
AR model [28, 29]
nt ∼N (A(ζk)nt−1, V (τk)) ,
for t = t−, t−+ 1, . . . , t+
(5)
where ζk = [ζ1k, ζ2k, ..., ζNk]⊺∈RN, nt = [nt, nt−1, ..., nt−N+1]⊺∈RN are the coefﬁcients and states of an AR
model of order N ∈N+ for noise signal nt = e⊺
1nt. τk ∈R>0 denotes the process noise precision of the AR process.
In contrast to the speech model, we assume the processes ζk and τk to be stationary when the user is in a speciﬁc
acoustic environment or context. To make clear that contextual states change much slower that raw acoustic data signals,
we index the slower parameters at time index k, which is related to index t by
k =
 t
W

.
(6)
Here, ⌈·⌉denotes the ceiling function that returns the largest integer smaller or equal than its argument, while W is the
window length. The above equation makes sure that k is intuitively aligned with segments of length W, i.e. t ∈[1, W]
corresponds to k = 1. To denote the start and end indices of the time segment corresponding to context index k, we
deﬁne t−= (k −1)W + 1 and t+ = kW as an implicit function of k, respectively. The context can be assumed to be
stationary within a longer period of time compared to the speech signal. However, abrupt changes in the dynamics of
background noise may occasionally occur. For example, if the user moves from a train station to a bar, the parameters
of the AR model that are attributed to the train station will now inadequately describe the background noise of the
new environment. To deal with these changing acoustic environments, we introduce context-dependent priors for the
background noise, using a Gaussian and Gamma mixture model:
ζk ∼
L
Y
l=1
N (µl, Σl)clk
(7a)
τk ∼
L
Y
l=1
Γ (αl, βl)clk
(7b)
The context at time index k, denoted by ck, comprises a 1-of-L binary vector with elements clk ∈{0, 1}, which
are constrained by P
l clk = 1. Γ(α, β) represents a Gamma distribution with shape and rate parameters α and
6

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
AR
st−1
=
st
=
N
ωI
θt
θt−1
=
γ
AR
nt−1
=
nt
e⊤
1
e⊤
1
+
xt
GMM
ΓMM
=
=
Cat
ck−1
ck
=
T
ζk
τk
context
A
×
N
V
A(θ)
←
−
ν (θ)
−
→
ν (θ)
↑↓
−
→
ν (y)
←
−
ν (y)
←
→
←
−
ν (x)
−
→
ν (x)
←
→
V (γ)
←
−
ν (γ)
−
→
ν (γ)
↑↓
AR
Figure 3: (left) A Forney-style factor graph representation of the acoustic source signals model as speciﬁed by (3)-(2)
at time index t. The observation xt is speciﬁed as the sum of a latent speech signal st and a latent noise signal nt.
The speech signal is modeled by a time-varying auto-regressive process, where its coefﬁcients θt are modeled by a
Gaussian random walk. The noise signal is a context-dependent auto-regressive process, modeled by Gaussian (GMM)
and Gamma mixture models (ΓMM) for the parameters ζk and τk, respectively. The selection variable of these mixture
models represents the context ck. The model for the context dynamics is enclosed by the dashed box. The composite
AR factor node represents the auto-regressive transition dynamics speciﬁed by (3b). (right) The composite AR node
that conceals its internal operations from the rest of the graph [30]. The arrows show the direction of incoming and
outgoing messages.
β, respectively. The hyperparameters µl, Σl, αl and βl deﬁne the characteristics of the different background noise
environments.
Now that an acoustic model of the environment has been formally speciﬁed, we will extend this model with the goal
of obtaining a HA algorithm. The principal goal of a HA algorithm is to improve audibility and intelligibility of
acoustic signals. Audibility can be improved by amplifying the received input signal. Intelligibility can be improved by
increasing the Signal-to-Noise Ratio (SNR) of the received signal. Assuming that we can infer the constituent source
signals st and nt from received signal xt, the desired HA output signal can be modeled by
yt = uskst + unknt,
for t = t−, t−+ 1, . . . , t+
(8)
where uk = [usk, unk]⊺∈[0, 1]2 represents a vector of 2 tuning parameters or source-speciﬁc gains for the speech
and background noise signal, respectively. In this expression the output of the hearing aid is modeled by a weighted
sum of the constituent source signals. The gains control the ampliﬁcation of the extracted speech and noise signals
individually and thus allows the user to perform source-speciﬁc ﬁltering, also known as soundscaping [31]. Because
of imperfections during inference of the source signals (see Section 4), the gains simultaneously reﬂect a trade-off
between residual noise and speech distortion. We show the FFG representation of the HA output in Figure 4. Finding
good values for the gains u can be a difﬁcult task because the preferred parameter settings may depend on the speciﬁc
listener and on the acoustic context.
Next, we describe the acoustic context model that will allow AIDA to make context-dependent parameter proposals.
3.1.2
Model of context dynamics
As HA clients move through different acoustic background settings, such as being in a car, doing groceries, watching TV
at home, etc.) the preferred parameter settings for HA algorithms tend to vary. The context signal allows to distinguish
between these different acoustic environments.
7

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
st
=
st
×
usk
nt
=
nt
×
unk
+
yt
Figure 4: A Forney-style factor graph representation of the hearing aid output model of (8). The output of the hearing
aid is modeled by reweighing the separated speech and background noise signals.
The hidden context state variable ck at time index k is a 1-of-L encoded binary vector with elements clk ∈{0, 1},
which are constrained by P
l clk = 1. This context is responsible for the operations of the noise model in (7). Context
transitions are supported by a dynamic model
ck ∼Cat(Tck−1),
(9)
where the elements of transition matrix T, are deﬁned as Tij = p(cik = 1 | cj,k−1 = 1), which are constrained by
Tij ∈[0, 1] and PL
j=1 Tij = 1. We model the individual columns of T by a Dirichlet distribution as
T1:L,j ∼Dir(αj),
(10)
where αj denotes the vector of concentration parameters corresponding to the jth column of T. The context state is
initialized by a categorical distribution as
c0 ∼Cat(π) =
L
Y
l=1
πcl0
l
such that
L
X
l=1
πl = 1,
(11)
where the vector π = [π1, π2, . . . , πL]⊺contains the event probabilities, whose elements can be chosen as πl = 1/L if
the initial context is unknown. An FFG representation of the context dynamics model is shown in the dashed box in
Figure 3.
3.2
AIDA’s user response model
The goal of AIDA is to continually provide the most “interesting” settings for the HA tuning parameters uk, where
interesting has been quantitatively interpreted by minimization of Expected Free Energy. But how does AIDA know
what the client wants? In order to learn the client’s preferences, she is invited to cast at any time her appraisal
rk ∈{∅, 0, 1} of current HA performance. To keep the user interface very light, we will assume that appraisals are
binary, encoded by rk = 0 for disapproval and rk = 1 indicating a positive experience. If a user does not cast an
appraisal, we will just record a missing value, i.e., rk = ∅. The subscript k for rk indicates that we record appraisals at
the same rate as the context dynamics.
If a client submits a negative appraisal rk = 0, AIDA interprets this as an expression that the client is not happy with
the current HA settings uk in the current acoustic context ck (and vice versa for positive appraisals). To learn client
preferences from these appraisals, AIDA holds a context-dependent generative model to predict user appraisals and
updates this model after observing actual appraisals. In this paper, we opt for a Gaussian Process Classiﬁer (GPC)
model as the generative model for binary user appraisals. A Gaussian Process (GP) is a very ﬂexible probabilistic model
and GPCs have successfully been applied to preference learning in a variety of tasks before [32, 33, 34]. For an in-depth
discussion on GPs, we refer the reader to [35]. Speciﬁcally, the context-dependent user response model is deﬁned as
vk(·) ∼
L
Y
l=1
GP(ml(·), Kl(·, ·))clk
(12a)
rk ∼Ber(Φ(vk(uk))) .
if rk ∈{0, 1}
(12b)
8

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
uk
GPM
ck
Φ
Ber
rk
vk
Figure 5: A Forney-style factor graph representation of the user response model speciﬁed by (12). The context state ck
is passed to the GP Mixture (GPM) node as a selector variable for its arguments.
In (12a), vk(·) is a latent function drawn from a mixture of GPs with mean functions ml(·) and kernels Kl(·, ·).
Evaluating vk(·) at the point uk provides an estimate of user preferences. Without loss of generality, we can set
ml(·) = 0. Since ck is one-hot encoded, raising to the power clk serves to select the GP that corresponds to the active
context. Φ(·) denotes the Gaussian cumulative distribution function, deﬁned as Φ(x) =
1
√
2π
R x
−∞exp
 −t2/2

dt.
This map in (12b) casts vk(uk) to a Bernoulli-distributed variable rk. An FFG representation of the user response
model is shown in Figure 5.
4
Solving tasks by probabilistic inference
This section elaborates on solving the three tasks of Section 2.2: 1) context classiﬁcation, 2) trial design and 3) hearing
aid algorithm execution. All tasks can be solved through probabilistic inference in the generative model speciﬁed by
(2)-(12b) in Section 3. In this section, the inference goals are formally speciﬁed based on the previously proposed
generative model.
For the realization of the inference tasks we will use variational message passing in a factor graph representation of the
generative model. Message passing-based inference is highly efﬁcient, modular and scales well to large inference tasks
[36, 37]. With message passing, inference tasks in the generative model reduce to automatable procedures revolving
around local computations on the factor graphs.
A thorough discussion on message passing and related topics is omitted here for readability, but made available in
Appendix A to serve as reference.
4.1
Inference for context classiﬁcation
The acoustic context ck describes the dynamics of the background noise model through (5) and (7). For determining the
current environment of the user, the goal is to infer the current context based on the preceding observations. Technically
we are interested in determining the marginal distribution p(ck | x1:t+), where the index range over t of x takes
into account the relation between t and k as deﬁned in (6). In our online setting, we wish to calculate this marginal
distribution iteratively by solving
p(ck | x1:t+)
|
{z
}
posterior
∝
Z
p(zt−:t+, Ψk, xt−:t+ | zt−−1, ck)
|
{z
}
observation model
p(ck, T | ck−1)
|
{z
}
context dynamics
· p(ck−1, zt−−1 | x1:t−−1)
|
{z
}
prior
dzt−−1:t+ dΨk dck−1 dT.
(13)
The observation model is fully speciﬁed by the model speciﬁcation in Section 3, similarly as the context dynamics.
The prior distribution is a joint result of the iterative execution of both (13) and (18), where the latter refers to the HA
algorithm execution from Section 4.3. The calculation of this marginal distribution renders intractable and therefore
exact inference of the context is not possible. This is a result of 1) the intractability resulting from the autoregressive
model as described in the previous subsection and of 2) the intractability that is a result of performing message
passing with mixture models. In (7) the model structure contains a Normal and Gamma mixture model for the AR-
coefﬁcients and process noise precision parameter, respectively. Exact inference with these mixture models quickly
leads to intractable inference through message passing, especially when multiple background noise models are involved.
Therefore, we need to resort to a variational approximation where the output messages of these mixture models are
constrained to be within the exponential family.
Although variational inference with the mixture models is feasible [30, 38, 39], it is prone to converge to local minima
of the Bethe free energy (BFE) for more complicated models. The variational messages originating from the mixture
9

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
models are constrained to either Normal or Gamma distributions, possibly losing important multi-modal information,
and as a result they can lead to suboptimal inference of the context variable. Because the context is vital for the above
underdetermined source separation stage, we wish to limit the amount of (variational) approximations during context
inference. At the cost of an increased computational complexity, we will remove the variational approximation around
the mixture models and instead expand the mixture components into distinct models. As a result, each distinct model
now contains one of the mixture components for a given context and now results in exact messages originating from the
priors of ξk and τk. Therefore we only need to resort to a variational approximation for the auto-regressive node. By
expanding the mixture models into distinct models to reduce the number of variational approximations, calculation
of the posterior distribution of the context p(ck | x1:t+) reduces to an approximate Bayesian model comparison
problem, similarly as described in [31]. Appendix C.1 gives a more in-depth description on how we use Bayesian model
comparison for solving the inference task in (13).
4.2
Inference for trial design of HA tuning parameters
The goal of proposing alternative HA tuning parameter settings (task 3) is to receive positive user responses in the future.
Free energy minimization over desired future user responses can be achieved through a procedure called Expected Free
Energy (EFE) minimization [11, 40].
EFE as a trial selection criterion induces a natural trade-off between explorative (information seeking) and exploitative
(reward seeking) behaviour. In the context of situated HA personalization, this is desirable because soliciting user
feedback can be burdensome and invasive, as described in Section 2.2. From the agent’s point of view, this means that
striking a balance between gathering information about user preferences and satisfying learned preferences is vital. The
EFE provides a way to tackle this trade-off, inspired by neuro-scientiﬁc evidence that brains operate under a similar
protocol [11, 41]. The EFE is deﬁned as [11]
Gu[q] = Eq(r,v|u)

ln q(v | u)
p(r, v | u)

,
(14)
where the subscript indicates that the EFE is a function of a trial u. The EFE can be decomposed into [11]
Gu[q] ≈−Eq(r|u)

ln p(r)

|
{z
}
Utility drive
−Eq(r,v|u)

ln q(v | u, r)
q(v | u)

|
{z
}
Information gain
,
(15)
which contains an information gain term and a utility-driven term. Minimization of the EFE reduces to maximization of
both these terms. Maximization of the utility drive pushes the agent towards matching predicted user responses q(r | u)
with a goal prior over desired user responses p(r). This goal prior allows encoding of beliefs about future observations
that we wish to observe. Setting the goal prior to match positive user responses then drives the agent towards parameter
settings that it believes make the user happy in the future. The information gain term in (15) drives agents that optimize
the EFE to seek out responses that are maximally informative about latent states v.
To select the next set of gains u to propose to the user, we need to ﬁnd
u∗= arg min
u

min
q
Gu[q]

.
(16)
Intuitively, one can think of (16) as a two step procedure with an inner and an outer loop. The inner loop ﬁnds the
approximate posterior q using (approximate) Bayesian inference, conditioned on a particular action parameter u. The
outer loop evaluates the resulting EFE as a function of u and proposes a new set of gains to bring the EFE down. For
our experiments we consider a candidate grid of possible gains. For each candidate we compute the resulting EFE and
then select the lowest scoring proposal as the next set of gains to be presented to the user.
The probabilistic model used for AIDA is a mixture GPC. For simplicity we will restrict inference to the GP corre-
sponding to the MAP estimate of ck. Between trials, the corresponding GP needs to be updated to adapt to the new data
gathered from the user. Speciﬁcally, we are interested in ﬁnding the posterior over the latent user preference function
p(v∗| u1:k, r1:k−1) =
Z
p(v∗| u1:k−1, uk, v)p(v | u1:k−1, r1:k−1)dv .
(17)
where we assume AIDA has access to a dataset consisting of previous queries u1:k−1 and appraisals r1:k−1 and we are
querying the model at uk. While this inference task in the GPC is intractable, there exist a number of techniques for
approximate inference, such as variational Bayesian methods, Expectation Propagation, and the Laplace approximation
[35]. Appendix C.2 describes the exact details of the inference realization of the inference tasks of AIDA.
10

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
4.3
Inference for executing the hearing aid algorithm
The main goal of the proposed hearing aid algorithm is to improve audibility and intelligibility by re-weighing inferred
source signals in the HA output signal. In our model of the observed signal in (2)-(7) we are interested in iteratively
inferring the marginal distribution over the latent speech and noise signals p(st, nt | x1:t). This inference task is in
literature sometimes referred to as informed source separation [42]. Inferring the latent speech and noise signals tries to
optimally disentangle these signals from the observed signal based on the sub-models of the speech and noise source.This
requires us to compute the posterior distributions associated with the speech and noise signals. To do so, we perform
probabilistic inference by means of message passing in the acoustic model of (2)-(7) . The posterior distributions can be
calculated in an online manner using sequential Bayesian updating by solving the Chapman-Kolmogorov equation [43]
p(zt, Ψk | x1:t)
|
{z
}
posterior
∝p(xt | zt)
|
{z
}
observation
Z
p(zt | zt−1, Ψk)
|
{z
}
state dynamics
p(zt−1, Ψk | x1:t−1)
|
{z
}
prior
dzt−1,
for t = t−, t−+ 1, . . . , t+
(18)
where zt and Ψk denote the sets of dynamic states and static parameters zt = {θt, st, nt} and Ψk = {γ, τk, ζk},
respectively. Here, the states and parameters correspond to the latent AR and TVAR models of (3) and (5). Furthermore,
we assume that the context does not change, i.e. k is ﬁxed. When the context does change (18) will need to be extended
by integrating over the varying parameters. Unfortunately, the solution of (18) is not analytically tractable. This
happens because of 1) the integration over large state spaces, 2) the non-conjugate prior-posterior pairing, and 3) the
absence of a closed-form solution for the evidence factor [44]. To circumvent this issue, we resort to a hybrid message
passing algorithm that combines structured variational message passing (SVMP) and loopy belief propagation for the
minimization of Bethe free energy [45]. Appendix A describes these concepts in more detail.
For the details of the SVMP and BP algorithms, we refer the reader to Appendix A and [45, 46]. Owing to the
modularity of the factor graphs, the message passing update rules can be tabulated and only need to be derived once
for each of the included factor nodes. The derivations of the sum-product update rules for elementary factor nodes
can be found in [36] and the derived structured variational rules for the composite AR node can be found in [44]. The
variational updates in the mixture models can be found in [30, 39]. The required approximate marginal distribution
of some variable z can be computed by multiplying the incoming and outgoing variational messages on the edges
corresponding to the variables of our interest as q(z) ∝⃗ν(z) ·
⃗
ν(z).
Based on the inferred posterior distributions of st and nt, these signals can be used for inferring the hearing aid output
through (8) to produce a personalized output which compromises between residual noise and speech distortion.
5
Experimental veriﬁcation & validation
In this section, we ﬁrst verify our approach for the three design tasks of Section 2.2. Speciﬁcally, in Section 5.1 we
evaluate the context inference approach by reporting the classiﬁcation performance of correctly classifying the context
corresponding to a signal segment. In Section 5.2 we evaluate the performance of our intelligent agent that actively
proposes hearing aid settings and learns user preferences. The execution of the hearing aid algorithm is veriﬁed in
Section 5.3 by evaluating the source separation performance. To conclude this section, we present a demonstrator for
the entire system in Section 5.4.
All algorithms have been implemented in the scientiﬁc programming language Julia [47]. Probabilistic inference in
our model is automated using the open source Julia package ReactiveMP2 [48]. All of the experiments presented in
this section can be found at our AIDA GitHub repository3.
5.1
Context classiﬁcation veriﬁcation
To verify that the context is appropriately inferred through Bayesian model selection, we generated synthetic data from
the following generative model:
ck ∼Cat(Tck−1)
(19a)
2ReactiveMP [48] is available at https://github.com/biaslab/ReactiveMP.jl.
3The AIDA GitHub repository with all experiments is available at https://github.com/biaslab/AIDA.
11

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
Table 1: The parameters of autoregressive processes that are used for generating a time series with simulated context
dynamics.
AR order
ζ
τ −1
1
-0.308
1.0
2
0.722
-0.673
2.0
3
-0.081
0.079
-0.362
0.5
4
-1.433
-0.174
0.757
0.466
1.0
200
210
220
230
240
250
260
270
280
290
300
AR-0
AR-1
AR-2
AR-3
AR-4
AR-5
frame
models
active context
selected context
Figure 6: True and inferred evolution of contexts from frames 200 to 300. Each frame consists of 100 data points.
Circles denote the active contexts that were used to generate the frame. Crosses denote the model that achieves the
lowest Bethe free energy for a speciﬁc frame.
with priors
c0 ∼Cat(π)
(20a)
T1:L,j ∼Dir(αj),
(20b)
where co is chosen to have length L = 4. The event probabilities π and concentration parameters αj are deﬁned as
π = [0.25, 0.25, 0.25, 0.25]⊺and αj = [1.0, 1.0, 1.0, 1.0]⊺, respectively. We generated a sequence of 1000 frames,
each containing 100 samples, such that we have 100 x 1000 data points. Each frame is associated with one of the 4
different contexts. Each context corresponds to an AR model with the parameters presented in Table 1.
For veriﬁcation of the context classiﬁcation procedure, we wish to identify which model best approximates the observed
data. To do that, 4 models with the same speciﬁcations as were used to generate the dataset were employed. We used
informative priors for the coefﬁcients and precision of AR models. Additionally, we extended our set of models with
an AR(5) model with weakly informative priors and a Gaussian i.i.d. model that can be viewed as an AR model of
zeroth order, i.e. AR(0). The individual frames containing 100 samples each were processed individually and we
computed the Bethe free energy for each of the different models. The Bethe free energy is introduced in Appendix A.4.
By approximating the true model evidence using the Bethe free energy as described in Appendix C.1, we performed
approximate Bayesian model selection by selecting the model with the lowest Bethe free energy. This model then
corresponds to the most likely context hat we are in. We highlight the obtained inference result in Figure 6.
We evaluate the performance of the context classiﬁcation procedure using approximate Bayesian model selection by
computing the categorical accuracy metric deﬁned as
acc = tp + tn
N
(21)
12

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
User preference function
0
0.2
0.4
0.6
0.8
1
 Figure 7: Simulated user preference function p(rk = 1 | uk). The coloring corresponds to the probability of the user
giving a positive appraisal for the search space of gains uk = [usk, unk]⊺.
where tp, tn are the number of true positive and true negative values, respectively. N corresponds to the number of total
observations, which in this experiment is set to N = 1000. In this context classiﬁcation experiment, we have achieved
a categorical accuracy of acc = 0.94.
5.2
Trial design veriﬁcation
Evaluating the performance of the intelligent agent is not trivial. Because the agent adaptively trades off exploration
and exploitation, accuracy is not an adequate metric. There are reasons for the agent to veer away from what it believes
is the optimum to obtain more information. As a veriﬁcation experiment we can investigate how the agent interacts with
a simulated user. Our simulated user samples binary appraisals rk based on the HA parameters uk as
rk ∼Ber

2
1 + exp
 (uk −u∗)T Λuser(uk −u∗)


,
(22)
where u∗denotes the optimal parameter setting, uk is the set of parameters proposed by AIDA at time k, Λuser is a
diagonal weighing matrix that controls how quickly the probability of positive appraisals decays with the squared
distance to u∗. The constant 2 ensures that when uk = u∗, the probability of positive appraisals is 1 instead of 0.5. For
our experiments, we set u∗= [0.8, 0.2]⊺and the diagonal elements of Λuser to 0.004. This results in the user preference
function p(rk = 1 | uk) as shown in Figure 7. The kernel used for AIDA is a squared exponential kernel, given by
K(u, u′) = σ2 exp

−∥u −u′∥2
2
2l2

,
(23)
where l and σ are the hyperparameters of this kernel. Intuitively, σ is a static noise parameter and l encodes the
smoothness of the kernel function. Both hyperparameters were initialized to σ = l = 0.5, which is uninformative on the
scale of the experiment. We let the agent search for 80 trials and update hyperparameters every 5th trial using conjugate
gradient descent as implemented in Optim.jl [49]. We constrain both hyperparameters to the domain [0.1, 1] to ensure
stability of the optimization. As we will see, for large parts of each experiment AIDA only receives negative appraisals.
The generative model of AIDA is fundamentally a classiﬁer and unconstrained optimization can therefore lead to
degenerate results when the data set only contains examples of a single class. For all experiments, the ﬁrst proposal
of AIDA was a randomly sampled parameter from the admissible set of parameters, because the AIDA has no prior
knowledge about the user preference function. This random initial proposal, lead to distinct behaviour for all simulated
agents.
We provide two veriﬁcation experiments for AIDA. First, we will thoroughly examine a single run in order to investigate
how AIDA switches between exploratory and exploitative behaviour. Secondly, we examine the aggregate performance
of an ensemble of agents to test the average performance. To assess the performance for a single run, we can examine
13

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
the evolution of the distinct terms in the EFE decomposition of (15) over time. We expect that when AIDA is primarily
exploring, the utility drive is relatively low while the information gain is relatively high. When AIDA is primarily
engaged in exploitation, we expect the opposite pattern. We show these terms separately in Figure 8.
0
10
20
30
40
50
60
70
80
−30
−25
−20
−15
−10
−5
0
k
value
Utility drive
0
10
20
30
40
50
60
70
80
−1
−0.8
−0.6
−0.4
−0.2
0
k
Information gain
Figure 8: Evolution of the utility drive and negative information gain after throughout a single experiment.
Figure 8 shows that there are distinct phases to the experiment. In the beginning (k < 5) AIDA sees a sharp decrease
in utility drive and information gain terms. This indicates a saturation of the search space such that no points present
good options. This happens early due to uninformative hyperparameter settings in the GPC. After trial 5, these
hyperparameters are optimized and the agent no longer thinks it has saturated the search space, which can be explained
by the jumps in Figure 8 from trial 5 to 6. From trial 6 throughout 15 we observe a relatively high information gain and
relatively low utility drive, meaning that the agent is still exploring the search space for parameter settings which yield
a positive user appraisal. The agent obtains its ﬁrst positive appraisal at k = 16, as denoted by the jump in utility drive
and drop in information gain. This ﬁrst positive appraisal is followed by a period of oscillations in both terms, where
the agent is reﬁning its parameters. Finally AIDA settles down to predominantly exploitative behaviour starting from
41st trial. To examine the ﬁrst transition, we can visualize the EFE landscape at k = 5 and k = 6, the upper row of
Figure 9.
Recall that AIDA is minimizing EFE. Therefore, it is looking for the lowest values corresponding to blue regions and
avoiding the high values corresponding to red regions. Between k = 5 and k = 6 we perform the ﬁrst hyperparameter
update, which drastically changes the EFE landscape. This indicates that initial parameter settings were not informative,
as we did not cover the majority of the search space within the ﬁrst 5 iterations. The yellow regions at k = 6 indicates
regions corresponding to previous proposals of AIDA that resulted into negative appraisals. We can visualize snapshots
of the exploration phase starting from k = 6 in a similar manner. The second row of Figure 9 displays the EFE
landscape at two different time instances during the exploration phase. It shows that over the course of the experiment,
AIDA gradually builds a representation over the search space. In trial 16 this takes the form of patterns of connected
regions that denote areas that AIDA believes are unlikely to results in positive appraisals.
Once AIDA receives its ﬁrst positive appraisal at k = 16, it switches from exploring the search space to focusing only
on the local region. If we examine Figure 8, we see that at this time the information gain term is still reasonably high.
This indicates a subtle point: once AIDA receives a positive appraisal, it starts with local exploration around where
the optimum might be located. However, the agent was located near the boundary of the optimum and next receives a
negative appraisal. Therefore in trials 18 to 22 AIDA queries points which it deems most informative. At time 23 the
position of AIDA in the search space (black dot in the third row of Figure 9) returns to the edge of the user preference
function in Figure 7. This causes AIDA to receive a mixture of positive and negative appraisals in the following trials,
leading to the oscillations seen in Figure 8. Finally, we can examine the landscape after AIDA has conﬁdently located
the optimum and switched to purely exploitative behavior. This happens at k = 42 where the utility drive goes to 0 and
the information gain concentrates around −1.
The last row of Figure 9 shows that once u∗is conﬁdently located, AIDA disregards the remainder of the search space
in favour of providing good parameter settings. Finally, if the user continues to supply data to AIDA, it will gradually
extend the potential region of samples around the optimum. This indicates that if a user keeps requesting updated
parameters, AIDA will once again perform local exploration around the optimum. This further indicates that AIDA
accommodates gradual retraining as user’s hearing loss proﬁle changes over time.
Having thoroughly examined an example run and investigated the types of behavior produced by AIDA, we can now
turn our attention to aggregate performance over an ensemble of agents. To that end we repeat the experiment 80 times
with identical hyperparameters, but with different initial proposals. The metric we are most interested in is how quickly
AIDA is able to locate the optimum and produce a positive appraisal.
14

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 5
Target
Current
32
33
34
35
36
37
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 6
Target
Current
20
25
30
35
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 15
Target
Current
20
25
30
35
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 16
Target
Current
10
15
20
25
30
35
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 22
Target
Current
20
25
30
35
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 23
Target
Current
10
15
20
25
30
35
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 42
Target
Current
10
20
30
EFE
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
us
un
EFE landscape at time 80
Target
Current
10
20
30
EFE
Figure 9: Snapshot of EFE landscape at different time points as a function of gains us and un. The black dot denotes
the current parameter settings and the green dot denotes u∗.
Figure 10 shows a heatmap of when each agent obtains positive responses. Positive responses are indicated by yellow
squares and negative responses by black squares. Each row contains results for a single AIDA-agent and each column
15

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
20
40
60
80
20
40
60
80
Time Index
Agent number
Positive Appraisals
10
27.5
45
62.5
80
1
3
5
7
10
12
14
Time Index
Number of Postive responses
Ensemble Performance
Figure 10: (Left) Heatmap showing ensemble performance over 80 agents. Positive and negative responses are indicated
with yellow and black squares, respectively. (Right) Histogram showing time indices where the agents receive their
ﬁrst positive response. The right most column indicates agents that failed to obtain a positive appraisal. In total, 66/80
agents solve the task, corresponding to a success rate of 82.5%.
indicates a time step of the experiment. Consistent with the results for a single agent, we see that each experiment starts
with a period of exploration. A large number of rows also show a yellow square within the ﬁrst 35 trials, indicating that
the optimum was found. Interestingly, no agents receive only positive responses, even after locating the optimum. This
follows from AIDA actively trading off exploration and exploitation. When exploring, AIDA can select parameters
that are suboptimal with respect to eliciting positive user responses, to gather more information. Figure 10 also shows
a histogram indicating when each agent obtains its ﬁrst positive appraisal. The very right column shows agents that
failed to locate the optimum within the designated number of trials. In total, 66/80 agents correctly solve the task,
corresponding to a success rate of 82.5%. Disregarding unsuccessful runs, on average, AIDA obtains a positive response
in 37.8 trials with a median of 29.5 trials.
5.3
Hearing aid algorithm execution veriﬁcation
To verify the proposed inference methodology for the hearing aid algorithm execution, we synthesized data by sampling
from the following generative model:
θt ∼N (θt−1, ωIM)
(24a)
st ∼N (A(θt)st−1, V (γ))
(24b)
nt ∼N (A(ζ)nt−1, V (τ))
(24c)
xt = st + nt,
(24d)
with priors
θ0 ∼N(0, ωIM)
(25a)
ζ ∼N(0, IN)
(25b)
γ ∼Γ(1.0, 1e −4)
(25c)
τ ∼Γ(1.0, 1.0)
(25d)
ω = 1e −4
(25e)
where M and N are the orders of TVAR and AR models, respectively, and where M ≥N holds, as we assume that the
noise signal can be modeled by a lower AR order in comparison to the speech signal. We use an uninformative prior for
the output of the hearing aid yt as in Figure 3 to prevent interactions from that part of the graph. We generated 1000
distinct time series of length 100. For each generated time series, the (TV)AR orders M and N were sampled from the
discrete domains [4, 8] and [1, 4], respectively. We resampled the priors that initially resulted into unstable TVAR and
AR processes.
16

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
The generated time series were used in the following experiment. We ﬁrst created a probabilistic model with the same
speciﬁcations as the generative model in (24). However, we used non-informative priors for the states and parameters
of the model that corresponds to the TVAR process in (24b). To ensure the identiﬁability of the separated sources, we
used weakly informative priors for the parameters of the AR process in (24c). Speciﬁcally, the mean of the prior for ζ
was centered around the real AR coefﬁcients that were used in the data generation process. The goals of the experiment
are 1) to verify that the proposed inference procedure recovers the hidden states θt, st and nt for each generated dataset
and 2) to verify convergence of the BFE as convergence is not guaranteed, because our graph contains loops [50]. For a
typical case, the inference results for the hidden states st and nt are shown in the top row of Figure 11. The bottom row
0
20
40
60
80
100
−5
0
5
t
value
Dataset 999
observations
TVAR(5)
AR(4)
0
20
40
60
80
100
−6
−4
−2
0
2
4
6
t
Dataset 999
TVAR(5)
inferred
0
20
40
60
80
100
−2
0
2
4
t
Dataset 999
AR(4)
inferred
0
20
40
60
80
100
−0.5
0
0.5
t
value
Coefficient 1
hidden
inferred
0
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
t
value
Coefficient 2
hidden
inferred
0
20
40
60
80
100
−0.8
−0.6
−0.4
−0.2
0
t
value
Coefficient 3
hidden
inferred
0
20
40
60
80
100
−0.8
−0.6
−0.4
−0.2
0
t
value
Coefficient 4
hidden
inferred
0
20
40
60
80
100
−0.2
0
0.2
0.4
0.6
t
value
Coefficient 5
hidden
inferred
h
Figure 11: (Top) Inference results for the hidden states st and nt of coupled (TV)AR process on dataset 999. (left)
The generated observed signal xt with underlying generated signals st and nt. (center) The latent signal st and its
corresponding posterior approximation. (right) The latent signal nt and its corresponding posterior approximation. The
dashed lines corresponds to the mean of the posterior estimates. The transparent regions represent the corresponding
remaining uncertainty as plus-minus one standard deviation from the mean. (Bottom) Inference results for the
coefﬁcients θt of dataset 999. The solid lines correspond to the true latent AR coefﬁcients. The dashed lines correspond
to the mean of the posterior estimates of the coefﬁcients and the transparent regions correspond to plus-minus one
standard deviation from the mean of the estimated coefﬁcients.
of Figure 11 shows the tracking of the time-varying coefﬁcients θt. This plot does not show the correlation between
the inferred coefﬁcients, whereas this actually contains vital information for modeling an acoustic signal. Namely, the
coefﬁcients together specify a set of poles, which inﬂuence the characteristics of the frequency spectrum of the signal.
An interesting example is depicted in Figure 12. We can see that the inference results for the latent states st and nt are
swapped with respect to the true underlying signals. This behaviour is undesirable in standard algorithms when the
output of the HA is produced based on hard-coded gains. However, the presence of our intelligent agent can still ﬁnd
the optimal gains for this situation. The automation of the hearing aid algorithm and intelligent agent will relieve this
burden on HA clients. As can be seen from Figure 13, the Bethe free energy averaged over all generated time series
monotonically decreases. Note that even though the proposed hybrid message passing algorithm results in a stationary
solution, it does not provide convergence guarantees.
5.4
Validation experiments
For the validation of the proposed HA algorithm and AIDA, we created an interactive web application4 to demonstrate
the the joint system. Figure 14 shows the interface of the demonstrator.
The user listens to the output of the hearing aid algorithm by pressing the "output" button. The buttons "speech" and
"noise" correspond to the beliefs of AIDA about the constituent signals of the HA input. Note that in reality the user
does not have access to this information and can only listen to HA output. After listening to the output signal, the user
4A web application of AIDA is available at https://github.com/biaslab/AIDA-app/.
17

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
0
20
40
60
80
100
−6
−4
−2
0
2
4
6
t
value
Dataset 42
observations
TVAR(6)
AR(3)
0
20
40
60
80
100
−4
−2
0
2
4
6
t
Dataset 42
TVAR(6)
inferred
0
20
40
60
80
100
−5
0
5
t
Dataset 42
AR(3)
inferred
Figure 12: Inference results for the hidden states st and nt of coupled (TV)AR process on dataset 42. In this particular
case it can be noted that the inferred states are swapped with respect to the true underlying signals. However, the
accompanying intelligent agent is able to cope with these kinds of situations, such that the HA clients do not experience
any problems as a result.
−20
0
20
40
60
80
100
120
140
160
180
200
220
240
260
280
300
320
340
320
330
340
350
360
370
iteration
Bethe free energy [nats]
BFE
Figure 13: Evolution of the Bethe free energy for the coupled autoregressive model averaged over all generated time
series. The iteration index speciﬁes the number of marginal updates for all edges in the graph.
is invited to assess the performance of the current HA setting. The user can send positive and negative appraisals by
pressing the thumb up or thumb down buttons respectively. Once the appraisal is sent, AIDA updates its beliefs about
the parameters’ space and provides new settings for the HA algorithm to make the user happy. As AIDA models user
appraisals using a GPC, we provide an additional button that forces AIDA to optimize the parameters of GPC. This
could be useful when AIDA has already collected some feedback from the user that contains both positive and negative
appraisals.
The demonstrator works in two environments: synthetic and real. The synthetic environment allows the user to listen
to a spoken sentence with two artiﬁcial noise sources, i.e. either interference from a sinusoidal wave or a drilling
machine. In the synthetic environment the hearing aid algorithm exploits the knowledge about acoustic contexts,
i.e, it uses informative priors for the AR model that corresponds to noise. The real environment uses the data from
NOIZEUS speech corpus5. In particular, the real environment consists of 30 sentences pronounced in two different
noise environments. Here the user is either experiencing surrounding noise at a train station or babble noise. In the
real environment, the HA algorithm uses weakly informative priors for the background noise which inﬂuences the
performance of the HA algorithm. Both the HA algorithm and AIDA determine the acoustic context based on the
Bethe free energy score, which is also shown in the demonstrator. The context with the lower Bethe free energy score
corresponds to the selected acoustic context.
5The NOIZEUS database is available at https://ecs.utdallas.edu/loizou/speech/noizeus/.
18

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
Active Inference Design Agent
Environment
 
Hearing Aid
 
 
 
 
 
EFE Agent
Classifier
SYNTHETIC
REAL
RESET
0
5k
10k
15k
20k
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
input
speech
noise
output
amplitude
NEXT
INPUT
SPEECH
NOISE
OUTPUT
0
0.5
1
1.5
2
0
0.5
1
1.5
2
20
21
22
23
24
25
speech gain
noise gain
0
1
2
3
4
−210
−200
−190
−180
−170
−160
Babble
Train
BFE [nats]
Figure 14: Screenshot of the interactive web application of AIDA. The dashboard consists of four distinct cells. The top
cell Environment allows the user to change the interfering noise signal from a generated noise signal (synthetic) to a
real noise signal. Furthermore it contains a reset button for resetting the application. The Hearing Aid cell provides an
interactive plot of the input, separated speech, separated noise, and generated output waveform signals. Each waveform
can be played when the corresponding button is pressed. The NEXT button loads a new audio ﬁle for evaluation. The
thumbs-up and thumbs-down buttons correspond to providing AIDA with positive and negative appraisals, respectively.
The brain button starts optimization of the parameters of GPC. The EFE Agent cell reﬂects the agent’s beliefs about
optimal parameters for the user as an EFE heatmap. The Classiﬁer cell shows the Bethe free energy (BFE) score for
the different models, corresponding to the different contexts. For the real noise signal, the algorithm automatically
determines whether we are surrounded by babble noise, or by noise from a train station.
19

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
6
Discussion
We have introduced a design agent that is capable of tuning the context-dependent parameters of a hearing aid algorithm
by incorporating user feedback. Throughout the paper, we have made several design choices whose implications we
shortly review in this section.
The audio model introduced in Section 3.1 describes the dynamics of the speech signal perturbed by colored noise.
Despite the fact that the proposed inference algorithm allows for the decomposition of such signals into speech and
noise components, there are a few limitations that must be highlighted. First, the identiﬁability of the coupled AR
model depends on the selected priors. Non-informative priors can lead to poor source estimation [51, 52]. To tackle the
identiﬁability issue, we use informative context-dependent priors. In other words, for each context, we use a different set
of priors that better describe the dynamics of the acoustic signal in that context. Secondly, throughout our experiments
we used ﬁxed orders of TVAR and AR models. In reality, we do not have prior information about the actual order of
the underlying signals. Therefore, to continuously update our models of the underlying sources we need to perform
active order selection, which can be realized using Bayesian model reduction [53, 54]. Thirdly, our model assumes
that the hearing aid device only has access to a monaural input, which means that the observed signal originates from
single microphone. As a result we do not use any spatial information about an acoustic signal that could have been
obtained using multiple microphones. This assumption is mostly inﬂuenced by our desire to focus on the concept of
designing a novel class of hearing aid algorithms rather than building real-world HA engine. Fortunately, the proposed
framework allows for the easy substitution of source models with more versatile models that might be better suited for
speech. For instance, one can use several microphones, as commonly done in beamforming [55], or use a frequency
decomposition for improving the source separation performance [56, 57, 58]. Inevitably, a more complex model will
also likely result in a higher computational burden. Hence, the implementation of this algorithm on an embedded device
remains a challenge.
The power of the agent comes from the choice of the objective function. Since the objective is independent of the
generative model, a straightforward approach to improving the agent is to adapt the generative model. In particular,
a GPC is a nonparametric model with very few assumptions on the underlying function. Placing constraints on the
preference function, such as was done in [59, 60], is likely to improve data efﬁciency of the agent. Arguably a core
move of [59, 60] is to acknowledge that user preferences are likely to be peaked around one or a few optima. Even
if the true preference function has multiple modes, assuming a single peak for the agent is safe since it only needs to
locate one of the modes to provide good parameter settings. Making this assumption allows the authors to work with a
parametric model over user preferences. Working with a less ﬂexible model predictably leads to higher data efﬁciency,
which can aid performance of the agent. Given that the target demographic for AIDA consists of HA users, it is of
paramount importance that the agent is able to learn an adequate representation of user preferences in as few trials as
possible to avoid inconveniencing the user.
During model speciﬁcation in Section 3.2, we make some assumptions on the control variable uk and user appraisals rk.
First, we set the domain of the elements of control variable uk to [0, 1]. Note that this is an arbitrary constraint which
we use for illustrative purposes. The domain can be easily rescaled without loss of generality. For example, in our
demonstrator, we use the default domain of uk ∈[0, 2]2. Secondly, we opt for binary user appraisals, i.e. rk ∈{∅, 0, 1}.
This design choice follows from the requirement of allowing users to communicate covertly to AIDA. Binary user
appraisal can more easily be linked to for example covert wrist movements when wearing a smartwatch to update the
control variables. With continuous user appraisals, e.g. rk ∈[0, 1], or pairwise comparison tests the convergence of
AIDA can be greatly improved as these appraisals yield more information per appraisal. However, providing AIDA with
these appraisals requires more attention, which is undesirable in certain circumstances, for example during business
meetings.
Real-world testing of AIDA has not been included in our work as performance evaluation with human HA clients is
not straightforward. The performance of AIDA should be evaluated by means of a randomized controlled trial (RCT)
where HA clients should be randomly assigned to either an experimental group or a control group. Unfortunately, our
implementation is currently not able to achieve real-time performance and hence cannot be tested in the proper RCT
setting. Nonetheless, we provide a demo that simulates AIDA and can be tested freely.
7
Related work
The problem of hearing aid personalization has been explored in various works. In [3] the HA parameters are
tuned according to a pairwise user assessment tests, during which the user’s perception is encoded using Gaussian
processes. The intractable posterior distribution corresponding to the user’s perception is then computed using a Laplace
approximation with Expected Improvement as the acquisition function used to select the next set of gains. Our agent
20

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
improves upon [3] in two concrete ways. Firstly, AIDA places a lower cognitive load on the user by not requiring
pairwise comparisons. This means the user does not need to keep in her memory what the HA sounded like at the
previous trial but only needs to consider the current HA output. AIDA accomplishes this without requiring more trials
for training. In fact, since AIDA does not require pre-training but can be trained fully online under in-situ conditions,
AIDA requires less data to locate optimal gains. Secondly, AIDA can be trained and retrained in a continual learning
fashion. In case the users preferences change over time, for instance by a change in the hearing loss proﬁle, AIDA can
smoothly accommodate the user as long as she continues to provide the agent with feedback. Using EFE as acquisition
function means the agent will engage in local exploration once the optimum is located, leading the agent to naturally
learn shifts in the users preferences by balancing exploration and exploitation. In [4], personalization of the hearing
aid compression algorithm is framed in terms of deep reinforcement learning. On the contrary, in our work we take
inspiration from the active inference framework where agents act to maximize model evidence of their underlying
generative model. Importantly, this does not require us to explicitly specify a loss function that drives exploitative and
epistemic behaviour. In the recent work of [59], the hearing aid preference learning algorithm is implemented through
sequential Bayesian optimization with pairwise comparisons. Their hearing aid system comprises two subsystems
representing a user with their preferences and the agent that guides the learning process. However, [59] focus only on
exploration through maximising information gain with a parametric model. The EFE additionally adds a goal directed
term that ensures the agent will stay near the optimum once located, even if other parameter settings provide more
information. Extending the model of [59] to employ the full EFE is an exciting potential direction for future work.
Finally neither [3] nor [59] takes context dependence into account.
[61] introduces Active Listening (AL), which performs speech recognition based on the principles of active inference.
In [61], they regard listening as an active process that is largely inﬂuenced by lexical, speaker and prosodic information.
[61] distinguishes itself from conventional audio processing algorithms, because it explicitly includes the process of
word boundary selection before word classiﬁcation and recognition, and that they regard this as an active process. Word
boundaries are selected from a group of candidate word boundaries, based on Bayesian model selection, by choosing
the word boundary that optimizes the VFE during classiﬁcation. In the future, we see the potential of incorporating
the AL approach into AIDA. Active inference is successfully applied in the work [62] that studies to model selective
attention in a cocktail party listening setup.
The audio processing components of AIDA essentially perform informed source separation [42], where sources are
separated based on prior knowledge. Even though blind source separation approaches [63, 64] always use some degree
of prior information, we do not focus on this direction and instead we actively try to model the underlying sources
based on variations of auto-regressive processes. For audio processing applications source separation has often been
performed in the log-power domain [56, 57, 58]. However, the interaction of the signals in this domain is no longer
linear. The intractability that results from performing exact inference in this model is often resolved by simplifying the
interaction function [65, 66]. Although this approach has shown to be successful in the past, its performance is limited
because of the negligence of phase information.
8
Conclusions
This paper has presented AIDA, an active inference design agent for novel situation-aware personalized hearing aid
algorithms. AIDA and the corresponding hearing aid algorithm are based on probabilistic generative models that model
the user and the underlying speech and context-dependent background noise signals of the observed acoustic signal,
respectively. Through probabilistic inference by means of message passing, we perform informed source separation in
this model and use the separated signals to perform source-speciﬁc ﬁltering. AIDA then learns personalized source-
speciﬁc gains through user interaction, depending on the environment that the user is in. Users can give a binary
appraisal after which the agent will make an improved proposal, based on expected free energy minimization for
encouraging both exploitative and epistemic behaviour. AIDA’s operations are context-dependent and uses the context
from the hearing aid algorithm, which is based on Bayesian model selection. Experimental results show that hybrid
message passing is capable of ﬁnding the hidden states of the coupled AR model that are associated with the speech
and noise components. Moreover, Bayesian model selection has been tested for the context inference problem where
each source is modelled by AR process. The experiments on preference learning showed the potential of applying
expected free energy minization for ﬁnding the optimal settings of the hearing aid algorithm. Although real-world
implementations still present challenges, this novel class of audio processing algorithms has the potential to change
the leading approach to hearing aid algorithm design. Future plans encompass developing AIDA towards real-time
applications.
21

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
Acknowledgments
This work was partly ﬁnanced by GN Advanced Science, which is the research department of GN Hearing A/S, and by
research programs ZERO and EDL with project numbers P15-06 and P16-25, respectively, which are (partly) ﬁnanced
by the Netherlands Organisation for Scientiﬁc Research (NWO). The authors would also like to thank the BIASlab
team members for insightful discussions on various topics related to this work.
References
[1] James Kates and Kathryn Arehart. Multichannel Dynamic-Range Compression Using Digital Frequency Warping.
EURASIP Journal on Applied Signal Processing, 18:3003–3014, 2005.
[2] Thijs van de Laar and Bert de Vries. A Probabilistic Modeling Approach to Hearing Loss Compensation.
IEEE/ACM Transactions on Audio, Speech, and Language Processing, 24(11):2200–2213, November 2016.
[3] J.B.B. Nielsen, J. Nielsen, and J. Larsen. Perception-Based Personalization of Hearing Aids Using Gaussian
Processes and Active Learning. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 23(1):162–
173, January 2015.
[4] Nasim Alamdari, Edward Lobarinas, and Nasser Kehtarnavaz. Personalization of Hearing Aid Compression by
Human-in-the-Loop Deep Reinforcement Learning. IEEE Access, 8:203503–203515, 2020.
[5] C. Karadagur Ananda Reddy, N. Shankar, G. Shreedhar Bhat, R. Charan, and I. Panahi. An Individualized
Super-Gaussian Single Microphone Speech Enhancement for Hearing Aid Users With Smartphone as an Assistive
Device. IEEE Signal Processing Letters, 24(11):1601–1605, November 2017.
[6] Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of Physiology, Paris,
100(1-3):70–87, September 2006.
[7] Thijs van de Laar and Bert de Vries. Simulating Active Inference Processes by Message Passing. Frontiers in
Robotics and AI, 6:20, 2019.
[8] Thijs van de Laar, Ayça Özçelikkale, and Henk Wymeersch. Application of the Free Energy Principle to Estimation
and Control. arXiv preprint arXiv:1910.09823, 2019.
[9] Beren Millidge. Deep Active Inference as Variational Policy Gradients. arXiv:1907.03876 [cs], July 2019. arXiv:
1907.03876.
[10] Alexander Tschantz, Manuel Baltieri, Anil K. Seth, and Christopher L. Buckley. Scaling active inference. In 2020
International Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2020.
[11] Karl Friston, Francesco Rigoli, Dimitri Ognibene, Christoph Mathys, Thomas Fitzgerald, and Giovanni Pezzulo.
Active inference and epistemic value. Cognitive Neuroscience, 6(4):187–214, March 2015.
[12] Lancelot Da Costa, Thomas Parr, Noor Sajid, Sebastijan Veselic, Victorita Neacsu, and Karl Friston. Active
inference on discrete state-spaces: a synthesis. arXiv:2001.07203 [q-bio], January 2020. arXiv: 2001.07203.
[13] Karl Friston, Lancelot Da Costa, Danijar Hafner, Casper Hesp, and Thomas Parr. Sophisticated Inference. Neural
Computation, 33(3):713–763, March 2021.
[14] Antony W. Rix, John G. Beerends, Michael P. Hollier, and Andries P. Hekstra. Perceptual evaluation of speech
quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In 2001 IEEE
International Conference on Acoustics, Speech, and Signal Processing, 2001. Proceedings., volume 2, pages
749–752. IEEE, 2001.
[15] James M. Kates and Kathryn H. Arehart. The hearing-aid speech quality index (HASQI). Journal of the Audio
Engineering Society, 58(5):363–381, 2010.
[16] Cees H. Taal, Richard C. Hendriks, Richard Heusdens, and Jesper Jensen. An Algorithm for Intelligibility
Prediction of Time–Frequency Weighted Noisy Speech. IEEE Transactions on Audio, Speech, and Language
Processing, 19(7):2125–2136, September 2011.
[17] John G. Beerends, Christian Schmidmer, Jens Berger, Matthias Obermann, Raphael Ullmann, Joachim Pomy,
and Michael Keyhl. Perceptual Objective Listening Quality Assessment (POLQA), The Third Generation ITU-T
Standard for End-to-End Speech Quality Measurement Part I—Temporal Alignment. Journal of the Audio
Engineering Society, 61(6):366–384, July 2013. Publisher: Audio Engineering Society.
[18] Andrew Hines, Jan Skoglund, Anil C Kokaram, and Naomi Harte. ViSQOL: an objective speech quality model.
EURASIP Journal on Audio, Speech, and Music Processing, 2015(1):13, December 2015.
22

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
[19] Michael Chinen, Felicia S. C. Lim, Jan Skoglund, Nikita Gureev, Feargus O’Gorman, and Andrew Hines. ViSQOL
v3: An Open Source Production Ready Objective Speech and Audio Metric. arXiv:2004.09584 [cs, eess], April
2020.
[20] G.David Forney. Codes on graphs: normal realizations. IEEE Transactions on Information Theory, 47(2):520–548,
February 2001.
[21] Hans-Andrea Loeliger. An introduction to factor graphs. Signal Processing Magazine, IEEE, 21(1):28–41, January
2004.
[22] O. Kakusho and M. Yanagida. Hierarchical AR model for time varying speech signals. In ICASSP ’82. IEEE
International Conference on Acoustics, Speech, and Signal Processing, volume 7, pages 1295–1298, Paris, France,
May 1982.
[23] K. Paliwal and A. Basu. A speech enhancement method based on Kalman ﬁltering. In ICASSP ’87. IEEE
International Conference on Acoustics, Speech, and Signal Processing, volume 12, pages 177–180, Dallas, TX,
USA, April 1987.
[24] J. Vermaak, C. Andrieu, A. Doucet, and S.J. Godsill. Particle methods for Bayesian modeling and enhancement of
speech signals. IEEE Transactions on Speech and Audio Processing, 10(3):173–185, March 2002.
[25] Daniel Rudoy, Thomas F. Quatieri, and Patrick J. Wolfe. Time-Varying Autoregressions in Speech: Detection
Theory and Applications. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):977–989, May
2011.
[26] Albert Podusenko, Wouter M. Kouw, and Bert de Vries. Online Variational Message Passing in Hierarchical
Autoregressive Models. In 2020 IEEE International Symposium on Information Theory (ISIT), pages 1337–1342,
Los Angeles, CA, USA, June 2020. ISSN: 2157-8117.
[27] D.C. Popescu and I. Zeljkovic. Kalman ﬁltering of colored noise for speech enhancement. In Proceedings of the
1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP ’98, volume 2, pages
997–1000, Seattle, WA, USA, May 1998. ISSN: 1520-6149.
[28] S. Gannot, D. Burshtein, and E. Weinstein. Iterative and sequential Kalman ﬁlter-based speech enhancement
algorithms. IEEE Transactions on Speech and Audio Processing, 6(4):373–385, July 1998.
[29] J.D. Gibson, B. Koo, and S.D. Gray. Filtering of colored noise for speech enhancement and coding. IEEE
Transactions on Signal Processing, 39(8):1732–1742, August 1991.
[30] Albert Podusenko, Bart van Erp, Dmitry Bagaev, Ismail Senoz, and Bert de Vries. Message Passing-Based
Inference in the Gamma Mixture Model. In 2021 IEEE 31st International Workshop on Machine Learning for
Signal Processing (MLSP), pages 1–6, Gold Coast, Australia, October 2021. IEEE.
[31] Bart van Erp, Albert Podusenko, Tanya Ignatenko, and Bert de Vries. A Bayesian Modeling Approach to Situated
Design of Personalized Soundscaping Algorithms. Applied Sciences, 11(20):9535, October 2021. Number: 20
Publisher: Multidisciplinary Digital Publishing Institute.
[32] Neil Houlsby, Ferenc Huszár, Zoubin Ghahramani, and Máté Lengyel. Bayesian Active Learning for Classiﬁcation
and Preference Learning. arXiv:1112.5745 [cs, stat], December 2011.
[33] Wei Chu and Zoubin Ghahramani. Preference learning with Gaussian processes. In Proceedings of the 22nd
international conference on Machine learning, ICML ’05, pages 137–144, New York, NY, USA, August 2005.
Association for Computing Machinery.
[34] Ferenc Huszar. A GP classiﬁcation approach to preference learning. In NIPS Workshop on Choice Models and
Preference Learning, page 4, Sierra Nevada, Spain, 2011.
[35] Carl Edward Rasmussen and Christopher K. I Williams. Gaussian Processes for Machine Learning. MIT Press,
2006.
[36] Hans-Andrea Loeliger, Justin Dauwels, Junli Hu, Sascha Korl, Li Ping, and Frank R. Kschischang. The Factor
Graph Approach to Model-Based Signal Processing. Proceedings of the IEEE, 95(6):1295–1322, June 2007.
[37] Marco Cox, Thijs van de Laar, and Bert de Vries. A factor graph approach to automated design of Bayesian signal
processing algorithms. International Journal of Approximate Reasoning, 104:185–204, January 2019.
[38] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.
[39] Thijs van de Laar. Automated Design of Bayesian Signal Processing Algorithms. PhD thesis, Eindhoven University
of Technology, Eindhoven, The Netherlands, 2019.
[40] Noor Sajid, Philip J. Ball, Thomas Parr, and Karl J. Friston. Active Inference: Demystiﬁed and Compared. Neural
Computation, 33(3):674–712, March 2021.
23

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
[41] Thomas Parr and Karl J. Friston. Uncertainty, epistemics and active inference. Journal of The Royal Society
Interface, 14(136):20170376, November 2017.
[42] Kevin H. Knuth. Informed Source Separation: A Bayesian Tutorial. arXiv:1311.3001 [cs, stat], November 2013.
arXiv: 1311.3001.
[43] Simo Särkkä. Bayesian Filtering and Smoothing. Cambridge University Press, London ; New York, October 2013.
[44] Albert Podusenko, Wouter M. Kouw, and Bert de Vries. Message Passing-Based Inference for Time-Varying
Autoregressive Models. Entropy, 23(6):683, June 2021. Number: 6 Publisher: Multidisciplinary Digital Publishing
Institute.
[45] ˙Ismail ¸Senöz, Thijs van de Laar, Dmitry Bagaev, and Bert de Vries. Variational Message Passing and Local
Constraint Manipulation in Factor Graphs. Entropy, 23(7):807, 2021. Publisher: Multidisciplinary Digital
Publishing Institute.
[46] Justin Dauwels. On Variational Message Passing on Factor Graphs. In IEEE International Symposium on
Information Theory, pages 2546–2550, Nice, France, June 2007.
[47] J. Bezanson, A. Edelman, S. Karpinski, and V. Shah. Julia: A Fresh Approach to Numerical Computing. SIAM
Review, 59(1):65–98, January 2017.
[48] Dmitry Bagaev and Bert de Vries. Reactive Message Passing for Scalable Bayesian Inference. 2022. Submitted
to the Journal of Machine Learning Research.
[49] Patrick K Mogensen and Asbjørn N Riseth. Optim: A mathematical optimization package for Julia. Journal of
Open Source Software, 3(24):615, April 2018.
[50] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate inference: An
empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artiﬁcial intelligence, pages 467–475.
Morgan Kaufmann Publishers Inc., 1999.
[51] Tesheng Hsiao. Identiﬁcation of Time-Varying Autoregressive Systems Using Maximum a Posteriori Estimation.
IEEE Transactions on Signal Processing, 56(8):3497–3509, August 2008.
[52] Frank Kleibergen and Henk Hoek. Bayesian Analysis of ARMA models using Noninformative Priors. CentER
Discussion Paper, 1995-116:24, 1995.
[53] Karl Friston and Will Penny. Post hoc Bayesian model selection. Neuroimage, 56(4-2):2089–2099, June 2011.
[54] Karl Friston, Thomas Parr, and Peter Zeidman. Bayesian model reduction. arXiv:1805.07092 [stat], May 2018.
arXiv: 1805.07092.
[55] A. Ozerov and C. Fevotte. Multichannel Nonnegative Matrix Factorization in Convolutive Mixtures for Audio
Source Separation. IEEE Transactions on Audio, Speech, and Language Processing, 18(3):550–563, March 2010.
[56] Steven Rennie, Trausti Kristjansson, Peder Olsen, and Ramesh Gopinath. Dynamic noise adaptation. In 2006
IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, volume 1, pages 1–4,
Toulouse, France, 2006. IEEE.
[57] S.J. Rennie, J.R. Hershey, and P.A. Olsen. Single-channel speech separation and recognition using loopy belief
propagation. In IEEE International Conference on Acoustics, Speech and Signal Processing, 2009. ICASSP 2009,
pages 3845–3848, Taipei, Taiwan, April 2009.
[58] Brendan J Frey, Li Deng, Alex Acero, and Trausti Kristjansson. ALGONQUIN: Iterating Laplace’s Method to
Remove Multiple Types of Acoustic Distortion for Robust Speech Recognition. In Proceedings of the Eurospeech
Conference, pages 901–904, Aalborg, Denmark, September 2001.
[59] Tanya Ignatenko, Kirill Kondrashov, Marco Cox, and Bert de Vries. On Sequential Bayesian Optimization with
Pairwise Comparison. arXiv:2103.13192 [cs, math, stat], March 2021. arXiv: 2103.13192.
[60] Marco Cox and Bert de Vries. A parametric approach to Bayesian optimization with pairwise comparisons. In
NIPS Workshop on Bayesian Optimization (BayesOpt 2017), pages 1–5, Long Beach, USA, December 2017.
[61] Karl J. Friston, Noor Sajid, David Ricardo Quiroga-Martinez, Thomas Parr, Cathy J. Price, and Emma Holmes.
Active listening. Hearing Research, 399(Stimulus-speciﬁc adaptation, MMN and predicting coding):107998,
January 2021.
[62] Emma Holmes, Thomas Parr, Timothy D. Grifﬁths, and Karl J. Friston. Active inference, selective attention, and
the cocktail party problem. Neuroscience and Biobehavioral Reviews, 131:1288–1304, October 2021.
[63] Y. Laufer and S. Gannot. A Bayesian Hierarchical Model for Blind Audio Source Separation. In 2020 28th
European Signal Processing Conference (EUSIPCO), pages 276–280, January 2021. ISSN: 2076-1465.
24

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
[64] S. Xie, L. Yang, J. Yang, G. Zhou, and Y. Xiang. Time-Frequency Approach to Underdetermined Blind Source
Separation. IEEE Transactions on Neural Networks and Learning Systems, 23(2):306–316, February 2012.
[65] John R Hershey, Peder Olsen, and Steven J Rennie. Signal Interaction and the Devil Function. In Proceedings of
the Interspeech 2010, pages 334–337, Makuhari, Chiba, Japan, 2010.
[66] M.H. Radfar, A.H. Banihashemi, R.M. Dansereau, and A. Sayadiyan. Nonlinear minimum mean square error
estimator for mixture-maximisation approximation. Electronics Letters, 42(12):724–725, June 2006.
[67] Judea Pearl. Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach. In Proceedings of the
Second AAAI Conference on Artiﬁcial Intelligence, AAAI’82, pages 133–136, Pittsburgh, Pennsylvania, 1982.
AAAI Press.
[68] Frank R. Kschischang, Brendan J. Frey, and H.-A. Loeliger. Factor graphs and the sum-product algorithm. IEEE
Transactions on information theory, 47(2):498–519, 2001.
[69] Jonathan S Yedidia, William T Freeman, and Yair Weiss. Bethe free energy, Kikuchi approximations, and belief
propagation algorithms. Advances in neural information processing systems, 13:24, 2001.
[70] Thomas Minka. Divergence Measures and Message Passing. Technical report, Microsoft Research, 2005.
25

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
A
Factor graphs, free energy and message passing-based inference
This appendix introduces the basics of the probabilistic modeling approach, which underlies all computations in this
paper. First, in Section A.1 we describe factor graphs as a useful tool for visualizing probabilistic models. In Section A.2
we describe how exact probabilistic inference can be performed through message passing. We then introduce the
variational free energy in Section A.3 and the Bethe free energy in Section A.4 for quantifying the performance of
the probabilistic model when exact inference is not possible. By adding constraints to the approximate posterior
distributions, we end up with hybrid message passing algorithms, such as variational message passing, as will be
described in Section A.5.
A.1
Forney-style factor graphs
For the remainder of this section consider the factorized function
f(z) =
Y
a∈V
fa(za),
(26)
where {fa | a ∈V} denotes the set of factors fa(za) indexed by a ∈V from the set of vertices V and z denotes the set
of variables z = {zi | i ∈E}, which are indexed by i ∈E from the set of edges E. The variable za represents the set of
all neighbouring variables of factor fa.
Factor graphs are a speciﬁc type of probabilistic graphical models. In this paper we will focus on Forney-style factor
graphs (FFG) as introduced in [20] with notational conventions adopted from [21]. FFGs visualize factorized functions
as undirected graphs, whose nodes represent the individual factors of the global function. The nodes are interconnected
by edges representing the mutual arguments of the factors. In FFGs, a node can be connected to an arbitrary number of
edges, but edges are constrained to have a maximum degree of two. As an example, consider the factorized function
f(z1, z2, z3, z4, z5) = fa(z1)fb(z1, z2, z3)fc(z3, z4, z5)fd(z4).
(27)
The FFG representation of this function is visualized in Figure 15.
When a variable occurs in more than three factors, this constraint can be satisﬁed by introducing equality factors,
deﬁned as f=(z, z′, z′′) = δ(z −z′)δ(z −z′′). Here z′ and z′′ are variable copies of z, whose posterior distributions
are constrained to be identical as a result of the equality factor. For a more extensive overview of factor graphs, we refer
the interested reader to [21, 36].
A.2
Sum-product message passing
Probabilistic inference concerns the calculation of the posterior distribution in our model. The posterior distribution of
the factorized function in (26) is deﬁned as
p(z) = f(z)
Z
,
(28)
where Z =
R
f(z) dz is the normalization constant. Here and throughout the rest of this section, we assume to be
dealing with continuous variables for generality. For discrete variables, this integration simply reduces to a summation.
Furthermore we implicitly assume that the factors fa are appropriate (possibly unnormalized) probability density
functions, meaning that their mapping is speciﬁed as fa : R|za| →R≥0, where |za| denotes the cardinality of set za.
Computing the marginal distributions of this posterior requires integration over all nuisance variables. Because of the
conditional (in)dependencies in a factorized model, this computationally complex global integration can be performed
through a set of smaller local computations. This approach is better known as the sum-product algorithm or belief
propagation [67, 68]. The results of the local computations are termed messages µ and they propagate over the edges of
the corresponding FFG. In order to distinguish between the messages in the forward and backward direction, the edges
in the corresponding FFG are made arbitrarily directed as shown in Figure 15. Now the messages ⃗µ and
⃗
µ are speciﬁed
to propagate in and against the direction of the edge, respectively. The sum-product message ⃗µ(zi) [68] ﬂowing out of
some node fa with zi ∈za is deﬁned as
⃗µ(zi) ∝
Z
fa(za)
Y
zj∈za\i
⃗µ(zj)dza\i,
(29)
where the notation za\i refers to the set za excluding the element zi, formally deﬁned as za\i = {z ∈za | z /∈{zi}}.
The messages ⃗µ(zj) are the incoming messages to the factor node. Propagating these messages throughout the graph
26

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
fa
fb
fc
fd
z1
⃗µ(z1)
→
z3
⃗µ(z3)
→
⃗
µ(z3)
←
z4
⃗
µ(z4)
↑
z2
z5
Figure 15: A Forney-style factor graph representation of the factorized function in (27). The edges are arbitrarily
directed to distinguish between forward and backward messages on the graph. The drawn messages can be regarded as
summaries of the dashed boxes, used for solving (31).
allows us to determine the marginal distributions of some variable zi as the product of the messages propagating on that
respective edge as
p(zi) ∝⃗µ(zi) ·
⃗
µ(zi).
(30)
To clarify the above points, suppose that we are interested in calculating the marginal distribution of s3 from the model
in (27). This distribution can be calculated (up to a scaling constant) as
p(z3) =
Z
p(z) dz\3
∝
ZZ
fa(z1)fb(z1, z2, z3) dz1 dz2
|
{z
}
⃗µ(z3)
·
ZZ
fd(z4)fc(z3, z4, z5) dz4 dz5
|
{z
}
⃗
µ(z3)
.
(31)
From this derivation the marginal distribution of z3 can be calculated as the product of two terms that each summarize a
different part of the model. Figure 15 visualizes the example model of (27), now with directed edges, and visualizes
the corresponding messages as summaries of the dashed parts of the graph. In the example the prior distributions over
z1 and z4 can be regarded as messages themselves, meaning that ⃗µ(z1) = fa(z1) and
⃗
µ(z4) = fd(z4). Furthermore,
the edges corresponding to z2 and z5 are dangling, meaning that they only receive information from one side. The
messages in the reverse direction are deﬁned to be uninformative as
⃗
µ(z2) =
⃗
µ(z5) = 1, because then the posterior
distribution is fully derived by the messages in the forward direction as e.g. p(z2) ∝⃗µ(z2)
⃗
µ(z2) = ⃗µ(z2).
A.3
Variational free energy
The calculation of the normalization constant Z in (28) is often difﬁcult or even intractable. Similarly, also the marginal
distributions might be unobtainable in closed form. To resolve this, we will approximate the posterior distribution
p(z) of (28) with an approximate distribution q(z). Using this approximation we can deﬁne the variational free energy
(VFE) function as
F[q, f] = Eq(z)

ln q(z)
f(z)

= KL [q(z) ∥p(z)] −ln Z ≥−ln Z,
(32)
which provides an upper-bound on the negative log-normalization constant and is used for approximating the model
performance in intractable models. This bound is attained at the minimum of the VFE when the approximate posterior
distribution equals the exact posterior distributions q(z) = p(z). However, to allow for tractable inference, q(z) often
has to be constrained to some family of distributions Q as q(z) ∈Q under some factorization.
A.4
Bethe free energy
The Bethe assumption
q(z) =
Y
a∈V
qa(za)
Y
i∈E
qi(zi)−1.
(33)
is a useful constraint on the approximate posterior q(z), [69].
Here we made use of the fact that all edges in the FFG have a maximum degree of two, which can be strictly enforced
by adding uninformative priors p(zi) = 1 to dangling edges. Under the Bethe assumption, the VFE reduces to the
27

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
Bethe free energy (BFE)
FB[q, f] = −
X
a∈V
Eq(za) [ln fa(za)] −
X
a∈V
H[qa(za)] +
X
i∈E
H[qi(zi)],
(34)
which equals the VFE for acyclic graphs (i.e. trees). The BFE decomposes the VFE into a sum of node-local free
energies contributions and edge-speciﬁc entropies H.
A.5
Variational and hybrid message passing
Under the variational approximation we can employ variational inference in the model, which iteratively ﬁnds stationary
points on the BFE by ﬁxing all approximate posterior distributions besides the one that is being optimized. This
inference procedure can be cast to a message passing paradigm and is called variational message passing [46]. Here the
exact message update rule of (29) then reduces to the variational message update rule [46]
⃗ν(zi) ∝exp
n
Eq(za\i) [ln fa(za)]
o
(35)
where ⃗ν(zi) denotes the outgoing variational message on edge zi. The approximate marginal distributions are then
iteratively updated as
qi(zi) ∝⃗ν(zi) ·
⃗
ν(zi).
(36)
The calculations of variational messages and approximate marginal distributions are then iteratively repeated until
convergence of the VFE is reached.
In addition to the structure imposed by the Bethe approximation, additional constraints can be enforced. Depending
on these local constraints different inference algorithms naturally follow [45]. [45] shows that amongst others the
sum-product algorithm [67, 68], variational message passing [46] and expectation propagation [70] can be recovered.
By combining different local constraints we can achieve hybrid message passing-based inference in the probabilistic
model. We highly recommend the interested reader the work of [45] for an extensive overview of hybrid message
passing schemes.
28

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
B
Probabilistic model overview
This appendix gives a concise overview of the generative model of the acoustic model and AIDA. The prior distributions
are uninformative unless stated otherwise in Section 5.
B.1
Acoustic model
The observed signal xt is the sum of a speech and noise signal as
xt = st + nt
The speech signal st = e⊺
1st is modeled by a time-varying auto-regressive process as
st ∼N (A(θt)st−1, V (γ))
The auto-regressive coefﬁcients of the speech signal are time-varying as
θt ∼N (θt−1, ωIM)
The noise signal nt = e⊺
1nt is also modeled by an auto-regressive process
nt ∼N (A(ζk)nt−1, V (τk))
for t = t−, t−+ 1, . . . , t+
The parameters of the noise model depend on the context
ζk ∼
L
Y
l=1
N (µl, Σl)clk
τk ∼
L
Y
l=1
Γ (αl, βl)clk
The context ck evolves over a different time scale indexed by k as
ck ∼Cat(Tck−1)
The transition matrix of the context is modeled as
T1:L,j ∼Dir(αj)
Finally, the output of the hearing aid algorithm yt is formed as the weighted sum of the speech and noise signals as
yt = uskst + unknt
for t = t−, t−+ 1, . . . , t+
B.2
AIDA’s user response model
The user responses are modeled by a Bernoulli distribution containing a Gaussian cumulative probability distribution
that enforces the output vk(uk) to the allowed domain for the argument of the Bernoulli distribution
rk ∼Ber(Φ(vk(uk)))
if rk ∈{0, 1}
vk(uk) encodes our beliefs about the user response function (evaluated at uk), modeled by a mixture of Gaussian
processes as
vk ∼
L
Y
l=1
GP(ml(·), Kl(·, ·))clk
whose kernel function is deﬁned as
K(u, u′) = σ2 exp

−∥u −u′∥2
2
2l2

where σ denotes noise and l the length scale of the kernel.
29

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
Table 2: Summary of the notational conventions used throughout Section 3 in this paper.
Notation
Deﬁnition
Explanation
xt
(2)
Observed signal at time index t.
yt
(8)
Output of the hearing aid at time index t.
st
Latent speech signal at time index t.
nt
Latent noise signal at time index t.
st
(3b)
Vector with hidden states of the auto regressive model of the speech signal at time index t.
nt
(5)
Vector with hidden states of the auto regressive model of the noise signal at time index t.
θt
(3a)
Vector of auto regressive coefﬁcients of the speech signal at time index t.
ω
Covariance matrix scaling of the dynamics of the auto regressive coefﬁcients of the speech
signal.
IM
Identity matrix of size (M × M).
M
Auto regressive order of the speech signal.
N
Auto regressive order of the noise signal.
W
Sampling ratio between t and k.
A(θ)
(4)
State space transition matrix of an auto regressive process with coefﬁcients θ.
γ
Process noise precision of the auto regressive model for the speech signal.
V (γ)
Covariance matrix of the auto regressive model for the speech signal, containing only zeros
except for the ﬁrst element, which is 1/γ.
ei
Appropriately sized cartesian standard unit vector containing only zeros, except for the ith
entry, which is 1.
0
Appropriately sized vector of zeros.
N(µ, Σ)
Normal distribution with mean µ and covariance matrix Σ.
Γ(α, β)
Gamma distribution with shape and rate parameters α and β, respectively.
ζk
(7a)
Vector of auto-regressive coefﬁcients of the noise signal at context time scale index k.
τk
(7b)
Process noise precision of the auto regressive model of the noise signal at context time scale
index k.
L
Number of contexts.
ck
(9)
The context at context time scale k, containing a 1-of-L binary vector with elements clk ∈
{0, 1}.
π
Vector of event probabilities.
T
(10)
Transition matrix representing the discrete context dynamics.
αj
Concentration parameters for the prior over the jth column of T.
usk
Gain of the speech signal at context time index k.
unk
Gain of the noise signal at context time index k.
uk
Vector of tuning parameters at context time index k deﬁned as [usk, unk]⊺.
vk(·)
(12a)
Latent function drawn from a mixture of GPs.
rk
(12b)
Binary user response at time index k.
Φ(·)
Standard Gaussian cumulative distribution function.
ml(·)
Mean function of the lth GP.
Kl(·, ·)
Kernel function of the lth GP.
30

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms
C
Inference realization
This appendix describes in detail how the inference tasks of Sections 4.1 and 4.2 are realized. The inference task of
Section 4.3 is performed by automated message passing using the update rules of [44].
C.1
Realization of inference for context classiﬁcation
The inference task for context classiﬁcation of (13) renders intractable as discussed in Section (4.1). To circumvent this
problem, we will solve this task as a Bayesian model comparison task.
In a Bayesian model comparison task, we are interested in calculating the posterior probability p(ml | x) of some
model ml after observing data x. The posterior model probability p(ml | x) can be calculated using Bayes’ rule as
p(ml | x) =
p(x | ml)p(ml)
P
j p(x | mj)p(mj),
(39)
where the denominator represents the weighted model evidence p(x), i.e. the model evidence obtained for the individual
models p(x | ml), weighted by their priors p(ml).
To formulate our inference task as a Bayesian model comparison task, the distinct models ml ﬁrst have to be speciﬁed.
In order to do so, we ﬁrst note that we obtain the priors of ck−1 and zt−−1 in (13) separately, and therefore we implicitly
assume a factorization of our prior p(ck−1, zt−−1 | x1:t−−1) as
p(ck−1, zt−−1 | x1:t−−1) = p(ck−1 | xt−−1) p(zt−−1 | x1:t−−1).
(40)
As a result (13) can be rewritten as
p(ck | x1:t+) ∝
Z
p(ck, T | ck−1) p(ck−1 | x1:t−−1) dT dck−1
|
{z
}
⃗µ(ck)
·
Z
p(zt−:t+, Ψk, xt−:t+ | zt−−1, c) p(zt−−1 | x1:t−−1) dzt−−1:t+ dΨk
|
{z
}
p(xt−:t+|x1:t−−1,ck)
.
(41)
The ﬁrst term ⃗µ(ck) can be regarded as the forward message towards the context ck originating from the previous
context. It gives us an estimate of the new context solely based on the context dynamics as stipulated by the transition
matrix T. The second term p(xt−:t+ | x1:t−−1, ck) can be regarded as the incremental model evidence under some
given context ck. Comparison of (41) and (39) allows us to formulate our inference problem in (13) into a Bayesian
model comparison problem by deﬁning
p(ml) = ⃗µ(ck = el),
(42a)
p(x | ml) = p(xt−:t+ | x1:t−−1, ck = el).
(42b)
We can therefore deﬁne a model ml by clamping the context variable in generative model as ck = el. This means that
each model only has one active component for both the Gaussian and Gamma mixture nodes and therefore the messages
originating from these nodes are exact and do not require a variational approximation.
Despite the expansion of the mixture models, the incremental model evidence p(xt−:t+ | x1:t−−1, ck = el) cannot be
computed exactly as the auto-regressive source models lead to intractable inference. As a result we approximate the
model evidence in (42b) using the Bethe free energy, as deﬁned in (34) in Appendix A, as
p(x | ml) ≈exp{−FB[q, ml]},
(43)
where FB[q, ml] denotes the Bethe free energy observed after convergence of the inference algorithm for model ml.
Similarly the calculation of (42a) is intractable. Therefore we will approximate the model prior with the variational
message towards ck instead as
p(ml) ≈⃗ν(ck = el).
(44)
C.2
Realization of inference for trial design
Probabilistic inference in AIDA encompasses 2 tasks: 1) optimal proposal selection and 2) updating of the Gaussian
process classiﬁer (GPC). Here we specify how these inference tasks are executed in more detail.
31

AIDA: An Active Inference-based Design Agent for Audio Processing Algorithms Podusenko et al.
Optimal proposal selection
A closed-form expression of the EFE decomposition in (15) can be obtained for the GPC as shown in [32].
The ﬁrst term in the decomposition, the negative utility drive, resembles the cross-entropy loss between our goal prior
and posterior marginal. Since user responses are binary, we can evaluate this binary cross-entropy term as [32]
−Eq(r|u) [ln p(r)] = Φ


µu,D
q
σ2
u,D + 1

ln Ep(r)[r] +

1 −Φ


µu,D
q
σ2
u,D + 1



ln
 1 −Ep(r)[r]

,
(45)
where µu,D and σ2
u,D denote the posterior mean and variance returned by the GPC when queried at the point u given
some data set D = {u1:k−1, r1:k−1}, respectively. More concretely, the GPC returns a Gaussian distribution from
which the posterior mean and variance are extracted as v(u) = N(µu,D, σ2
u,D). Φ(·) denotes the standard Gaussian
cumulative distribution function. p(r) denotes the Bernoulli goal prior over desired user feedback. h is the binary
entropy function and C =
q
π ln 2
2
. For brevity, we denote the data set of parameters and matching user responses
collected so far as D.
The second term in the decomposition, the (negative) information gain, describes how much information we gain by
observing a new user appraisal. This information gain term (IG) can be expressed in a GPC as [32]
IG[r, v | D, u] ≈h

Φ


µu,D
q
σ2
u,D + 1



−
C
q
σ2
u,D + C2 exp

−
µ2
u,D
2

σ2
u,D + C2


,
(46)
where the constant C is deﬁned as C =
q
π ln 2
2
and where h(·) is deﬁned as h(p) = −p ln(p) −(1 −p) ln(1 −p).
Inference in the Gaussian process classiﬁer
For our experiments we use Laplace approximation as described in [35, Chapter 3.4] for performing inference in
the GPC. The Laplace approximation is a two-step procedure, where we approximate the posterior distribution by a
Gaussian distribution. We ﬁrst ﬁnd the mode of the exact posterior, which resembles the mean of the approximated
Gaussian distribution. Then we approximate the corresponding precision as the negative Hessian around the mode.
Finding the exact posterior p(v | D) amounts to calculating
p(v | D) = p(r1:k−1 | v)p(v | u1:k−1)
p(r1:k−1 | u1:k−1)
(47a)
∝p(r1:k−1 | v)p(v | u1:k−1) .
(47b)
Taking the logarithm of (47b) and differentiating twice with respect to v gives
∇v ln p(v | D) = ∇v ln p(r1:k−1 | v) −K−1v
(48a)
∇v∇v ln p(v | D) = ∇v∇v ln p(r1:k−1 | v) −K−1 = −W −K−1
(48b)
where ∇v denotes the gradient with respect to v, K = K(u1:k−1, u1:k−1) is the kernel matrix over the queries u1:k−1
and W = −∇v∇v ln p(r1:k−1 | v) is a diagonal matrix since the likelihood factorizes over independent observations.
At the mode ˆv (48a) equals zero which implies
ˆv = K∇v ln p(r1:k−1|ˆv) .
(49)
Directly solving (49) is intractable, because of the recursive non-linear relationship. Instead we can estimate ˆv
using Newton’s method, where we perform iterations with an adaptive step size. We omit the computational and
implementation details here and instead refer to [35, Algorithm 3.1]. We determine the step size using a line search as
implemented in Optim.jl [49]. Having found the mode ˆv, we can construct our posterior approximation as
p(v | D) ≈N

ˆv,
 K−1 + W
−1
,
(50)
where W is evaluated at v = ˆv. If we now recall that evaluating a GP at any ﬁnite number of points results in a Gaussian,
we see that under the Laplace approximation the solution can be obtained using standard results for marginalization
of jointly Gaussian variables. We deﬁne the shorthand K(uk, u1:k−1) = K1:k and K(uk, uk) = Kk and ﬁnd the
posterior mean µu as [35, p. 44]
µu,D = K⊺
1:kK−1ˆv = K⊺
1:k∇ln p(r1:k−1 | ˆv) .
(51)
The posterior covariance σ2
u,D is given by [35, p. 44]
σ2
u,D = Kk −K⊺
1:k
 K + W −1−1 K1:k .
(52)
32

