Counterfactual Invariance to Spurious Correlations:
Why and How to Pass Stress Tests
Victor Veitch1,2, Alexander D’Amour1, Steve Yadlowsky1, and Jacob
Eisenstein1
1Google Research
2University of Chicago
Abstract
Informally, a ‘spurious correlation’ is the dependence of a model on some aspect
of the input data that an analyst thinks shouldn’t matter. In machine learning, these
have a know-it-when-you-see-it character; e.g., changing the gender of a sentence’s
subject changes a sentiment predictor’s output. To check for spurious correlations, we
can ‘stress test’ models by perturbing irrelevant parts of input data and seeing if model
predictions change. In this paper, we study stress testing using the tools of causal infer-
ence. We introduce counterfactual invariance as a formalization of the requirement that
changing irrelevant parts of the input shouldn’t change model predictions. We connect
counterfactual invariance to out-of-domain model performance, and provide practical
schemes for learning (approximately) counterfactual invariant predictors (without ac-
cess to counterfactual examples). It turns out that both the means and implications of
counterfactual invariance depend fundamentally on the true underlying causal structure
of the data. Distinct causal structures require distinct regularization schemes to induce
counterfactual invariance. Similarly, counterfactual invariance implies different domain
shift guarantees depending on the underlying causal structure. This theory is supported
by empirical results on text classiﬁcation.
1
Introduction
Our focus in this paper is the sort of spurious correlations revealed by “poke it and see what
happens” testing procedures for machine-learning models. For example, we might test a
sentiment analysis tool by changing one proper noun for another (“tasty Mexican food” to
“tasty Indian food”), with the expectation that the predicted sentiment should not change.
This kind of perturbative stress testing is increasingly popular: it is straightforward to
understand and offers a natural way to test the behavior of models against the expectations
of practitioners [Rib+20; Wu+19; Nai+18].
Intuitively, models that pass such stress tests are preferable to those that do not. However,
fundamental questions about the use and meaning of perturbative stress tests remain open.
For instance, what is the connection between passing stress tests and model performance on
prediction? Eliminating predictor dependence on a spurious correlation should help with
domain shifts that affect the spurious correlation—but how do we make this precise? And,
how should we develop models that pass stress tests when our ability to generate perturbed
examples is limited? For example, automatically perturbing the sentiment of a document in
a general fashion is difﬁcult.
The ad hoc nature of stress testing makes it difﬁcult to give general answers to such questions.
In this paper, we will use the tools of causal inference to formalize what it means for models
to pass stress tests, and use this formalization to answer the questions above. We will
1
arXiv:2106.00545v2  [cs.LG]  2 Jun 2021

formalize passing stress tests as counterfactual invariance, a condition on how a predictor
should behave when given certain (unobserved) counterfactual input data. We will then
derive implications of counterfactual invariance that can be measured in the observed
data. Regularizing predictors to satisfy these observable implications provides a means
for achieving (partial) counterfactual invariance. Then, we will connect counterfactual
invariance to robust prediction under certain domain shifts, with the aim of clarifying what
counterfactual invariance buys and when it is desirable.
An important insight that emerges from the formalization is that the true underlying causal
structure of the data has fundamental implications for both model training and guarantees.
Methods for handing ‘spurious correlations’ in data with a given causal structure need not
perform well when blindly translated to another causal structure.
Counterfactual Invariance
Consider the problem of learning a predictor f that predicts a
label Y from covariates X. In this paper, we’re interested in constructing predictors whose
predictions are invariant to certain perturbations on X. Our ﬁrst task is to formalize the
invariance requirement.
To that end, assume that there is an additional variable Z that captures information that
should not inﬂuence predictions. However, Z may causally inﬂuence the covariates X. Using
the potential outcomes notation, let X(z) to denote the counterfactual X we would have
seen had Z been set to z, leaving all else ﬁxed. Informally, we can understand perturbative
stress tests as a way of producing particular realizations of counterfactual pairs X(z), X(z′)
that differ by an intervention on z. Then, we formalize the requirement that an arbitrary
change to z does not change predictions:
Deﬁnition 1.1. A predictor f is counterfactually invariant to Z if f (X(z)) = f (X(z′)) almost
everywhere, for all z,z′ in the sample space of Z. When Z is clear from context, we’ll just
say the predictor is counterfactually invariant.
2
Causal Structure
Counterfactual invariance is a condition on how the predicted label behaves under inter-
ventions on parts of the input data. However, intuitions about stress testing are based on
how the true label behaves under interventions on parts of the input data. We will see
that the true causal structure fundamentally affects both the implications of counterfactual
invariance, and the techniques we use to achieve it. To study this phenomenon, we’ll use
two causal structures that are commonly encountered in applications; see Figure 1.
2.1
Prediction in the Causal Direction
We begin with the case where X is a cause of Y .
Example 2.1. We want to automatically classify the quality of product reviews. Each review
has a number of “helpful” votes Y (from site users). We predict Y using the text of the
product review X. However, we ﬁnd interventions on the sentiment Z of the text change
our prediction; changing “Great shoes!” to “Bad shoes!” changes the prediction.
In the examples in this paper, the covariate X is text data. Usually, the causal relationship
between the text and Y and Z will be complex—e.g., the relationships may depend on
abstract, unlabeled, parts of the text such as topic, writing quality, or tone. In principle, we
could enumerate all such latent variables, construct a causal graph capturing the relation-
ships between these variables and Y, Z, and use this causal structure to study counterfactual
invariance. For instance, if we think that topic causally inﬂuences the helpfulness Y , but is
not inﬂuenced by sentiment Z, then we could build a counterfactually invariant predictor by
2

Z
XY ∧Z
X ⊥
Z
X ⊥
Y
Y
(a) Causal direction
Y
Z
X ⊥
Z
XY ∧Z
X ⊥
Y
(b) Anticausal direction
Figure 1: Causal models for the data generating process. We decompose the observed covariate X into
latent parts deﬁned by their causal relationships with Z and Y . Solid arrows denote causal relationships,
while dashed lines denote non-causal associations. The differences between these causal structures
will turn out to be key for understanding counterfactual invariance.
extracting the topic and predicting Y using topic alone. However, exhaustively articulating
all possible such variables is a herculean task.
Instead, notice that the only thing that’s relevant about these latent variables is their causal
relationship with Y and Z. Accordingly, we’ll decompose the observed variable X into parts
deﬁned by their causal relationships with Y and Z. We remain agnostic to the semantic
interpretation of these parts. Namely, we deﬁne X ⊥
Z as the part of X that is not causally
inﬂuenced by Z (but may inﬂuence Y ), X ⊥
Y as the part that does not causally inﬂuence Y
(but may be inﬂuenced by Z), and XY ∧Z is the remaining part that is both inﬂuenced by Z
and that inﬂuences Y . The causal structure is shown in Figure 1a.
We see there are two paths that lead to Y and Z being associated. The ﬁrst is when Z
affects XY ∧Z which, in turn, affects Y . For example, a very enthusiastic reviewer might
write a longer, more detailed review, which will in turn be more helpful. The second
is when a common cause or selection effect in the data generating process induces an
association between Z and Y , which we denote with a dashed arrow. For example, if books
tend to get more positive reviews, and also people who buy books are more likely to ﬂag
reviews as helpful, then the product type would be a common cause of sentiment and
helpfulness.
2.2
Prediction in the Anti-Causal Direction
We also consider the case where Y causes X.
Example 2.2. We want to predict the star rating Y of movie reviews from the text X.
However, we ﬁnd that predictions are inﬂuenced by the movie genre Z; e.g., changing
“Adam Sandler” (a comedy actor) to “Hugh Grant” (a romance actor) changes the predictions.
Figure 1b shows the causal structure. Here, the observed X is inﬂuenced by both Y and Z.
Again, we decompose X into parts deﬁned by their causal relationship with Z and Y . Here,
Z (and thus X ⊥
Y ) can be associated with Y through two paths. First, if XY ∧Z is non-trivial,
then conditioning on it causes a dependence between Z and Y (because XY ∧Z is a collider).
For example, if Adam Sandler tends to appear in good comedy movies but bad movies of
other genres then seeing “Sandler” in the text induces a dependency between sentiment
and genre. Second, Z and Y may be associated due to a common cause, or due to selection
effects in the data collection protocol—this is represented by the dashed line between Z
and Y . For example, fans of romantic comedies may tend to give higher reviews (to all
ﬁlms) than fans of horror movies.
3

2.3
Non-Causal Associations
Frequently, a predictor trained to predict Y from X will rely on X ⊥
Y , even though there is no
causal connection between Y and X ⊥
Y , and therefore will fail counterfactual invariance. The
reason is that X ⊥
Y serves as a proxy for Z, and Z is predictive of Y due to the non-causal
(dashed line) association.
There are two mechanisms that can induce such associations. First, Y and Z may be
confounded: they are both inﬂuenced by an unobserved common cause U. For example,
people who review books may be more upbeat than people who review clothing. This leads
to positive sentiments and high helpfulness votes for books, creating an association between
sentiment and helpfulness. Second, Y and Z may be subject to selection: there is some
condition (event) S that depends on Y and Z, such that a data point from the population
is included in the sample only if S = 1 holds. For example, our training data might only
include movies with at least 100 reviews. If only excellent horror movies have so many
reviews (but most rom-coms get that many), then this selection would induce an association
between genre and score. Formally, the dashed-line causal graphs mean our sample is
distributed according to P(X, Y, Z) =
R
P(X, Y, Z,u | S = 1)dP(u) where Y, Z are caused by
U and are causes of S, and (X, Y, Z) are causally related according to the graph.
In addition to the non-causal dashed-line relationship, there is also dependency induced by
between Y and Z by XY ∧Z. Whether or not each of these dependencies is “spurious” is a
problem-speciﬁc judgement that must be made by each analyst based on their particular
use case. E.g., using genre to predict sentiment may or may not be reasonable, depending
on the actual application in mind. However, there is a special case that captures a common
intuition for purely spurious association.
Deﬁnition 2.3. We say that the association between Y and Z is purely spurious if Y ⊥⊥
X | X ⊥
Z , Z.
That is, if the dashed-line association did not exist (removed by conditioning on Z) then
the part of X that is not inﬂuenced by Z would sufﬁce to estimate Y .
3
Observable Signatures of Counterfactually Invariant Pre-
dictors
We now consider the question of how to achieve counterfactual invariance in practice. The
challenge is that counterfactual invariance is deﬁned by the behavior of the predictor on
counterfactual data that is never actually observed. This makes checking counterfactual
invariance impossible. Instead, we’ll derive a signature of counterfactual invariance that
actually can be measured—and enforced—using ordinary datasets where Z (or a proxy) is
measured. For example, the star rating of a review as a proxy for sentiment, or genre labels
in the movie review case.
Intuitively, a predictor f is counterfactually invariant if it depends only on X ⊥
Z , the part of X
that is not affected by Z. To formalize this, we need to show that such a X ⊥
Z is well deﬁned:
Lemma 3.1. Let X ⊥
Z be a X-measurable random variable such that, for all measurable functions
f , we have that f is counterfactually invariant if and only if f (X) is X ⊥
Z -measurable. If Z is
discrete then such a X ⊥
Z exists.
Accordingly, we’d like to construct a predictor that is a function of X ⊥
Z only (i.e., is X ⊥
Z
measurable). The key insight is that we can use the causal graphs to read off a set of
conditional independence relationships that are satisﬁed by Z, X ⊥
Z , Y . Critically, these
4

conditional independence relationships are testable from the observed data. Thus, they
provide a signature of counterfactual invariance:
Theorem 3.2. If f is a counterfactually invariant predictor:
1. Under the anti-causal graph, f (X) ⊥⊥Z | Y .
2. Under the causal-direction graph, if Y and Z are not subject to selection (but possibly
confounded), f (X) ⊥⊥Z.
3. Under the causal-direction graph, if the association is purely spurious, Y ⊥⊥X | X ⊥
Z , Z,
and Y and Z are not confounded (but possibly selected), f (X) ⊥⊥Z | Y .
Causal Regularization
Without access to counterfactual examples, we cannot directly
enforce counterfactual invariance. However, we can require a trained model to satisfy the
counterfactual invariance signature of Theorem 3.2. The hope is that enforcing the signature
will lead the model to be counterfactually invariant. To do this, we regularize the model to
satisfy the appropriate conditional independence condition. For simplicity of exposition, we
restrict to binary Y and Z. The (inﬁnite data) regularization terms are
marginal regularization = MMD(P(f (X | Z = 0),P(f (X | Z = 1))
(3.1)
conditional regularization = MMD(P(f (X | Z = 0, Y = 0),P(f (X | Z = 1, Y = 0))
+ MMD(P(f (X | Z = 0, Y = 1),P(f (X | Z = 1, Y = 1)).
(3.2)
Maximum mean discrepancy (MMD) is a metric on probability measures.1 The marginal
independence condition is equivalent to (3.1) equal 0, and the conditional independence is
equivalent to (3.2) equal 0. In practice, we can estimate the MMD with ﬁnite data samples
[Gre+12]. When training with stochastic gradient descent, we compute the penalty on each
minibatch.
The procedure is then: if the data has causal-direction structure and the Y ↔Z association
is due to confounding, add the marginal regularization term to the the training objective. If
the data has anti-causal structure, or the association is due to selection, add the conditional
regularization term instead. In this way, we regularize towards models that satisfy the
counterfactual invariance signature.
A key point is that the regularizer we must use depends on the true causal structure. The
conditional and marginal independence conditions are generally incompatible. Enforc-
ing the condition that is mismatched to the true underlying causal structure will not in
general enforce counterfactual invariance, or may throw away more information than is
required.
Gap to Counterfactual Invariance
The conditional independence signature of Theo-
rem 3.2 is necessary but not sufﬁcient for counterfactual invariance. This is for two reasons.
First, counterfactual invariance applies to individual datapoint realizations, but the signature
is distributional. In particular, the invariance P(f (X) | do(Z = z)) = P(f (X) | do(Z = z′))
for all z,z′ would also imply the conditional independence signature. But, this invariance
is weaker than counterfactual invariance, since it doesn’t require access to counterfactual
realizations. Second, f (X) ⊥⊥Z does not imply, in general, that Z is not a cause of f (X).
This (unusual) behavior can happen if, e.g., there are levels of Z that we do not observe in
the training data, or there are variables omitted from the causal graph that are a common
cause of Z and X.
Unfortunately, the gap between the signature and counterfactual invariance is a fundamental
restriction of using observational data. The conditional independence signature is in some
1The choice of MMD is for concreteness, any distance on probability spaces would do.
5

sense the closest proxy for counterfactual invariance we can hope for. In section 5, we’ll
see that enforcing the signature does a good job of enforcing counterfactual invariance in
practice.
4
Performance Out of Domain
Counterfactual invariance is an intuitively desirable property for a predictor to have. How-
ever, it’s not immediately clear how it relates to model performance as measured by, e.g.,
accuracy. Intuitively, eliminating predictor dependence on a spurious Z may help with
domain shift, where the data distribution in the target domain differs from the distribution
of the training data. We now turn to formalizing this idea.
First, we must articulate the set of domain shifts to be considered. In our setting, the natural
thing is to hold the causal relationships ﬁxed across domains, but to allow the non-causal
(“spurious”) dependence between Y and Z to vary. Demanding that the causal relationships
stay ﬁxed reﬂects the requirement that the causal structure describes the dynamics of an
underlying real-world process—e.g., the author’s sentiment is always a cause (not an effect)
of the text in all domains. On the other hand, the dependency between Y and Z induced by
either confounding or selection can vary without changing the underlying causal structure.
For confounding, the distribution of the confounder may differ between domains—e.g.,
books are rare in training, but common in deployment. For selection, the selection criterion
may differ between domains—e.g., we include only frequently reviewed movies in training,
but make predictions for all movies in deployment.
We want to capture spurious domain shifts by considering domain shifts induced by selec-
tion or confounding. However, there is an additional nuance. Changes to the marginal
distribution of Y will affect the risk of a predictor, even in the absence of any spurious
association between Y and Z. Therefore, we restrict to shifts that preserve the marginal
distribution of Y .
Deﬁnition 4.1. We say that distributions P,Q are causally compatible if both obey the same
causal graph, P(Y ) = Q(Y ), and there is a confounder U and/or selection conditions S, ˜S
such that P =
R
P(X, Y, Z | U,S =1)d˜P(U) and Q =
R
P(X, Y, Z | U, ˜S =1)d˜Q(U) for some
˜P(U), ˜Q(U).
We can now connect counterfactual invariance and robustness to domain shift.
Theorem 4.2. Let F invar be the set of all counterfactually invariant predictors. Let L be
either square error or cross entropy loss. And, let f ∗:= argminf ∈F invar EP[L(Y, f (X))] be the
counterfactually invariant risk minimizer. Suppose that the target distribution Q is causally
compatible with the training distribution P. Suppose that any of the following conditions hold:
1. the data obeys the anti-causal graph
2. the data obeys the causal-direction graph, there is no confounding (but possibly selection),
and the association is purely spurious, Y ⊥⊥X | X ⊥
Z , Z, or
3. the data obeys the causal-direction graph, there is no selection (but possibly confounding),
the association is purely spurious and the causal effect of X ⊥
Z on Y is additive, i.e., the
true data generating process is
Y ←g(X ⊥
Z ) + ˜g(U) + ξ where E[ξ | X ⊥
Z ] = 0,
(4.1)
for some functions g, ˜g.
Then, the training domain counterfactually invariant risk minimizer is also the target domain
counterfactually invariant risk minimizer, f ∗= argminf ∈F invar EQ[L(Y, f (X))].
6

Remark 4.3. The causal case with confounding requires an additional assumption (additive
structure) because, e.g., an interaction between confounder and X ⊥
Z can yield a case where
X ⊥
Z and Y have a different relationship in each domain (whence, out-of-domain learning is
impossible).
This result gives a recipe for ﬁnding a good predictor in the target domain even without
access to any target domain examples at training time. Namely, ﬁnd the counterfactually
invariant risk minimizer in the training domain. In practice, we can use the regularization
scheme of section 3 to (approximately) achieve this. We’ll see in section 5 that this works
well in practice.
Optimality
Theorem 4.2 begs the question: if the only thing we know about the target
setting is that it’s causally compatible with the training data, is the best predictor the coun-
terfactually invariant predictor with lowest training risk? A natural way to formalize this
question is to study the predictor with the best performance in the worst case target distri-
bution. We deﬁne Q = {Q : Q causally compatible with P} and the Q-minimax predictor
f ∗
minimax = argminf ∈F maxQ∈Q EQ[L(Y, f (X)]. The question is then: what’s the relationship
between the counterfactually invariant risk minimizer and the minimax predictor?
Theorem 4.4. The counterfactually invariant risk minimizer is not Q-minimax in general.
However, under the conditions of Theorem 4.2, if the association is purely spurious, XY ∧Z ⊥⊥
Y | X ⊥
Z , Z, and P(Z, Y ) satisﬁes overlap, then the two predictors are the same. By overlap we
mean that P(Z, Y ) is a discrete distribution such that for all (z, y), if P(z, y) > 0 then there is
some y′ ̸= y such that also P(z, y′) > 0.
Conceptually, Theorem 4.4 just says that the counterfactually invariant predictor excludes
XY ∧Z, even when this information is useful in every domain. In the purely spurious case,
XY ∧Z carries no useful information, so counterfactual invariance is optimal.
5
Experiments
The main claims of the paper are:
1. Stress test violations can be reduced by suitable conditional independence regulariza-
tion.
2. This reduction will improve out-of-domain prediction performance.
3. To get the full effect, the imposed penalty must match the causal structure of the data.
Setup
To assess these claims, we’ll examine the behavior of predictors trained with the
marginal or conditinal regularization on multiple text datasets that have either causal or
anti-causal structure. We expect to see that marginal regularization improves stress test
and out-of-domain performance on data with causal-confounded structure, and conditional
regularization improves these on data with anti-causal structure.
For each experiment, we use BERT [Dev+19] ﬁnetuned to predict a label Y from the text as
our base model. We train multiple causally-regularized models on the each dataset. The
training varies by whether we use the conditional or marginal penalty, and by the strength
of the regularization term. That is, we train identical architectures using CrossEntropy + λ ·
Regularizer as the objective function, where we vary λ and take Regularizer as either the
marginal penalty, (3.1), or conditional penalty, (3.2). We compare these models’ predictions
on data with causal and anti-causal structure.
See supplement for experimental details.
7

0.002
0.003
0.004
0.005
conditional MMD
0.00
0.05
0.10
|P(1|x)
P(1|x)|
test accuracy
0.56
0.64
0.72
0.80
penalty type
conditional MMD
marginal MMD
0.84
0.85
0.86
0.87
test accuracy
0.000
0.025
0.050
0.075
0.100
Pr(Y(X)
Y(X))
penalty coeff
1
10
100
1000
penalty type
conditional MMD
marginal MMD
Figure 2: Regularizing conditional MMD improves counterfactual invariance on synthetic anti-
causal data. Sufﬁciently high regularization of marginal MMD also improves invariance, but impairs
accuracy. Dashed lines show baseline performance of an unregularized predictor. Left: lower conditional
MMD implies that predictive probabilities are invariant to perturbation. Although marginal MMD
penalization can result in low conditional MMD and good stress test performance, this comes at the cost
of very low in-domain accuracy. Right: MMD regularization reduces the rate of predicted label ﬂips
on perturbed data, with little affect on in-domain accuracy. Conditional MMD regularization reduces
predicted label ﬂips to 1.4%, while the best result for marginal MMD is 2.8%.
5.1
Robustness to Stress Tests
First, we examine whether enforcing the causal regularization actually helps to enforce
counterfactual invariance. We create counterfactual (stress test) examples by perturbing the
input data and compare the prediction on these. We build the experimental datasets using
Amazon reviews from the product category “Clothing, Shoes, and Jewelry” [NLM19].
Synthetic
To study the relationship between counterfactual invariance and the distribu-
tional signature of Theorem 3.2, we construct a synthetic confound. For each review, we
draw a Bernoulli random Z, and then perturb the text X so that the common words “the”
and “a” carry information about Z: for example, we replace “the” with the token “thexxxxx”
when Z = 1. We take Y to be the review score, and subsample so Y is balanced. This data
has anti-causal structure: the text X is written to explain the score Y . Further, we expect
that the Y, Z association is purely spurious, because “the” and “a” carry little information
about the label.
We train the models on data where P(Y = Z) = 0.3. We then create perturbed stress-test
datasets by changing each example Xi(z) to the counterfactual Xi(1−z) (using the synthetic
model). By measuring the performance of each model on the perturbed data, we can test
whether the distributional properties enforced by the regularizers result in counterfactual
invariance at the instance level. Figure 2 shows that conditional regularization (matching
the anti-causal structure) reduces checklist failures, as measured by the frequency that the
predicted label changes due to perturbation as well as the mean absolute difference in
predictive probabilities that is induced by perturbation.
Natural
To study the relationship in real data, we use the review data in a different way.
We now take Z to be the score, binarized as Z ∈{1 or 2 stars,4 or 5 stars}. We use this Z as
a proxy for sentiment, and consider problems where sentiment should (plausibly) not have
a causal effect on Y . For the causal prediction problem, we take Y to be the helpfulness
score of the review (binarized as described below). This is causal because readers decide
whether the review is helpful based on the text. For the anti-causal prediction problem, we
take Y to be whether “Clothing” is included as a category tag for the product under review
(e.g., boots typically do not have this tag). This is anti-causal because the product category
affects the text.
We control the strength of the spurious association between Y and Z. In the anti-causal case,
this is done by selection: we randomly subset the data to enforce a target level of dependence
between Y and Z. The causal-direction case with confounding is more complicated. To
manipulate confounding strength, we binarize the number of helpfulness votes V in a
manner determined by the target level of association. We take Y = 1[V > TZ] where TZ is
8

1
2.6 7.0
18
49 128
penalty coeff
0.030
0.035
0.040
|P(1|x)
P(1|x)|
Dataset = anticausal
1
2.6 7.0
18
49 128
penalty coeff
0.025
0.050
0.075
0.100
Dataset = causal
conditional
marginal
Figure 3: Penalizing the MMD matching the causal structure improves stress test performance
on natural product review data. Note that penalizing the wrong MMD may not help: the marginal
MMD hurts on the anticausal dataset. Perturbations are generated by swapping positive and negative
sentiment adjectives in examples.
a Z-dependent threshold, chosen to induce a target association. We choose P(Y = 0 | Z =
0) = P(Y = 1 | Z = 1) = 0.3. We balance Z by subsampling, which also balances Y .
Now, we create stress test perturbations of these datasets by randomly changing adjectives in
the examples. Using predeﬁned lists of postive sentiment adjectives and negative sentiment
adjectives, we swap any adjective that shows up on a list with a randomly sampled adjective
from the other list. This preserves basic sentence structure, and thus creates a limited set of
counterfactual pairs that differ on sentiment.
Results for differences in predicted probabilities between original and perturbed data are
shown in Figure 3. Each point is a trained model, which vary in measured MMD on the test
data and on sensitivity to perturbations. Recall that the conditional independence signature
of Theorem 3.2 are necessary but not sufﬁcient for counterfactual invariance, so it’s not
certain that regularizing to reduce the MMD will reduce perturbation sensitivity. Happily,
we see that regularizing to reduce the MMD that matches the causal structure does indeed
reduce sensitivity to perturbations.
Notice that regularizing the causally mismatched MMD can have strange effects. Regular-
izing marginal MMD in the anti-causal case actually makes the model more sensitive to
perturbations!
5.2
Domain Shift
Next, we study the effect of causal regularization on model performance under domain
shift.
Natural Product Review
We again use the natural Amazon review data described above.
For both the causal and anti-causal data, we create multiple test sets with variable spurious
correlation strength. This is done in the manner described above, varying P(Y = 0 | Z =
0) = P(Y = 1 | Z = 1) = γ. Here, γ is the strength of spurious association. The test sets
are out-of-domain samples. By design, Y is balanced in each dataset, so these samples are
causally compatible with the training data. For both the causal and anti-causal datasets,
the training data has P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = 0.3. We train a classiﬁer for
each regularization type and regularization strength, and measure the accuracy on each
test domain. The results are shown in Figure 4.
First, the unregularized predictors do indeed learn to rely on the spurious association
between sentiment and the label. The accuracy of these predictors decays dramatically
as the spurious assocation moves from negative (0.3) to positive—in the causal case, the
unregularized predictor is worse than chance in the 0.8 domain.
Following section 3, the regularization that matches the underlying causal structure should
9

0.2
0.4
0.6
0.8
Ptest(Y = 1|Z = 1) = Ptest(Y = 0|Z = 0)
0.80
0.83
0.85
0.88
0.90
Accuracy
0.800 0.825 0.850 0.875 0.900
Test Accuracy
0.80
0.83
0.85
0.88
0.90
Worst Domain Accuracy
conditional MMD
0.004
0.008
0.012
0.016
0.020
penalty type
conditional
marginal
none
Anti-Causal Data: conditional regularization improves domain-shift robustness.
0.2
0.4
0.6
0.8
Ptest(Y = 1|Z = 1) = Ptest(Y = 0|Z = 0)
0.50
0.60
0.70
0.80
Accuracy
0.5
0.6
0.7
Test Accuracy
0.50
0.60
0.70
Worst Domain Accuracy
marginal MMD
0.004
0.008
0.012
0.016
0.020
penalty type
conditional
marginal
none
Causal-Direction Data: marginal regularization improves domain-shift robustness.
Figure 4: The best domain-shift robustness is obtained by using the regularizer that matches the
underlying causal structure of the data. The plots show out-of-domain accuracy for models trained
on the (natural) review data. In each row, the left ﬁgure shows out-of-domain accuracies (lines are
models), with the X-axis showing the level of spurious correlation in the test data (0.3 is the training
condition); the right ﬁgure shows worst out-of-domain accuracy versus in-domain test accuracy (dots
are models).
0.0
0.5
2.0
8.0
32.0
128.0
penalty coeff
0.0
0.2
0.4
0.6
worst group accuracy
0.75
0.76
0.77
0.78
0.79
0.80
0.81
0.82
overall accuracy
0.50
0.55
0.60
0.65
0.70
0.75
worst group accuracy
conditional MMD
marginal MMD
Figure 5: Conditional MMD penalization improves robustness in anti-causal MNLI data. Marginal
regularization does not improve over the baseline unregularized model, shown with dashed lines. Left:
Conditional regularization improves minimum accuracy across (Y, Z) groups. When overregularized,
the predictor returns the same ˆY for all inputs, yielding a worst-group accuracy of 0. Right: Conditional
MMD regularization signiﬁcantly improves worst (Y, Z) group accuracy (y-axis) while only mildly
reducing overall accuracy (x-axis).
yield a predictor that is (approximately) counterfactually invariant. Following Theorem 4.2,
we expect that good performance of a counterfactually-invariant predictor in the training
domain should imply good performance in each of the other domains. Indeed, we see
that this is so. Models that are regularized to have small values of the appropriate MMD
do indeed have better out-of-domain performance. Such models have somewhat worse
in-domain performance, because they no longer exploit the spurious correlation.
MNLI Data
For an additional test on naturally-occurring confounds, we use the multi-
genre natural language inference (MNLI) dataset [WNB18]. Instances are concatena-
tions of two sentences, and the label describes the semantic relationship between them,
Y ∈{contradiction, entailment, neutral}. There is a well-known confound in this dataset:
examples where the second sentence contain a negation word (e.g., “not”) are much more
likely to be labeled as contradictions [Gur+18]. Following Sagawa et al. [Sag+20], we set
Z to indicate whether one of a small set of negation words is present. Although Z is derived
from the text X, it can be viewed as a proxy for a latent variable indicating whether the
author intended to use negation in the text. This is an anti-causal prediction problem: the
10

annotators were instructed to write text to reﬂect the desired label [WNB18].
Following Sagawa et al. [Sag+20], we divide the MNLI data into groups by (Y, Z) and
compute the “worst group accuracy” across all such groups. Because this is an anti-causal
problem, we predict that the conditional MMD is a more appropriate penalty than the
marginal MMD. As shown in Figure 5, this prediction holds: conditional MMD regularization
dramatically improves performance on the worst group, while only lightly impacting the
overall accuracy across groups.
6
Related work
Several papers draw a connection between causality and domain shifts [SS18; SCS19;
Arj+20; Mei18; PBM16; RC+18; Zha+13]. Typically, this work considers a prediction
setting where the covariates X include both causes and effects of Y , and it is unknown
which is which. The goal is to learn to predict Y using only its causal parents. Zhang et al.
[Zha+13] considers anti-causal domain shift induced by changing P(Y ) and proposes a
data reweighting scheme. Counterfactual invariance is not generally the same as invariance
to the domain shifts previously considered.
A related body of work focuses on “causal representation learning” [Bes+19; Loc+20;
Sch+21; Arj+20]. Our approach follows this tradition, but focuses on splitting X into
components deﬁned by their causal relationships with the label Y and an additional covariate
Z. Rather than attempting to infer the causal relationship between X and Y , we show that
domain knowledge of this relationship is essential for obtaining counterfactually-invariant
predictors. The role of causal vs anti-causal data generation in semi-supervised learning
has also been studied [Sch+21; Sch+12]. In this paper we focus on a different implication
of the causal vs anti-causal distinction.
Another line of work considers the case where the counterfactuals X(z), X(z′) are ob-
served for at least some data points [Wu+21; Gar+19; Mit+20; WZ19; KCC20; KHL20;
TAH20]. Kusner et al. [Kus+17] and Garg et al. [Gar+19] in particular examine a notion
of counterfactual fairness that can be seen as equivalent to counterfactual invariance. In
these papers, approximate counterfactuals are produced by direct manipulation of the text
(change male to female names), generative language models, or crowdsourcing. Then,
these counterfactuals can either be used as additional training data or the predictor can be
regularized such that it cannot distinguish between Xi(z) and Xi(z′). This strategy can be
viewed as enforcing counterfactual invariance directly; an advantage is that it avoids the
necessary-but-not-sufﬁcient nuance of Theorem 3.2. However, counterfactual examples can
be difﬁcult to obtain for language data in many realistic problem domains, and it may be
difﬁcult to learn to generalize from such examples [HLB20].
Finally, the marginal and conditional independencies of Theorem 3.2 have appeared in
other contexts. If we think of Z as a protected attribute and f as a ‘fair’ classiﬁer, then
the marginal independence is demographic parity, and the conditional independence is
equalized odds [Meh+19]. We can now understand these conditions as consequences of a
single desideratum: the prediction should not change under intervention on a protected
attribute. Similarly, an approach to domain adaptation is to seek representations φ such that
either φ(X) [e.g., MBS13; Bak+13; Gan+16; Tze+14] or φ(X) | Y [e.g., MLM19; Yan+17]
are distributionally invariant over domains. If we take Z to be a domain label, these are the
marginal and conditional independencies, and can be understood as consequences of the
desideratum that the prediction shouldn’t change under domain shift.
11

7
Discussion
We used the tools of causal inference to formalize and study perturbative stress tests. A main
insight of the paper is that counterfactual desiderata can be linked to observationally-testable
conditional independence criteria. This requires consideration of the true underlying causal
structure of the data. Done correctly, the link yields a simple procedure for enforcing the
counterfactual desiderata, and mitigating the effects of domain shift.
The main limitation of the paper is the restrictive causal structures we consider. In particular,
we require that X ⊥
Z , the part of X not causally affected by Z, is also statistically independent
of Z in the observed data. However, in practice these may be dependent due to a common
cause. In this case, the procedure here will be overly conservative, throwing away more
information than required. Additionally, it is not obvious how to apply the ideas described
here to more complicated causal situations, which can occur in structured prediction (e.g.,
question answering). Extending the ideas to handle richer causal structures is an important
direction for future work. The work described here can provide a template for this research
program.
References
[Arj+20]
M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant Risk Minimiza-
tion. 2020. arXiv: 1907.02893 [stat.ML].
[Bak+13]
M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann. “Unsuper-
vised domain adaptation by domain invariant projection”. In: Proceedings of
the 2013 IEEE International Conference on Computer Vision. 2013.
[Bes+19]
M. Besserve, A. Mehrjou, R. Sun, and B. Schölkopf. Counterfactuals uncover the
modular structure of deep generative models. 2019. arXiv: 1812.03253 [cs.LG].
[Dev+19]
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. “Bert: pre-training of deep
bidirectional transformers for language understanding”. In: Proceedings of the
2019 Conference of the North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies, Volume 1 (Long and Short
Papers). 2019.
[Gan+16]
Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M.
Marchand, and V. Lempitsky. “Domain-adversarial training of neural networks”.
In: J. Mach. Learn. Res. 1 (2016).
[Gar+19]
S. Garg, V. Perot, N. Limtiaco, A. Taly, E. H. Chi, and A. Beutel. “Counterfactual
fairness in text classiﬁcation through robustness”. In: Proceedings of the 2019
AAAI/ACM Conference on AI, Ethics, and Society. 2019.
[Gre+12]
A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Schölkopf, and A. Smola. “A kernel
two-sample test”. In: The Journal of Machine Learning Research 1 (2012).
[Gur+18]
S. Gururangan, S. Swayamdipta, O. Levy, R. Schwartz, S. Bowman, and N. A.
Smith. “Annotation artifacts in natural language inference data”. In: Proceedings
of the 2018 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, Volume 2 (Short
Papers). 2018.
[HLB20]
W. Huang, H. Liu, and S. R. Bowman. Counterfactually-Augmented SNLI Training
Data Does Not Yield Better Generalization Than Unaugmented Data. 2020. arXiv:
2010.04762 [cs.CL].
[KHL20]
D. Kaushik, E. Hovy, and Z. C. Lipton. Learning the Difference that Makes a
Difference with Counterfactually-Augmented Data. 2020. arXiv: 1909.12434
[cs.CL].
[KCC20]
V. Kumar, A. Choudhary, and E. Cho. “Data augmentation using pre-trained
transformer models”. In: Proceedings of the 2nd Workshop on Life-long Learning
for Spoken Language Systems. 2020.
12

[Kus+17]
M. J. Kusner, J. Loftus, C. Russell, and R. Silva. “Counterfactual fairness”. In:
Advances in Neural Information Processing Systems. 2017.
[Loc+20]
F. Locatello, B. Poole, G. Raetsch, B. Schölkopf, O. Bachem, and M. Tschannen.
“Weakly-supervised disentanglement without compromises”. In: Proceedings of
the 37th International Conference on Machine Learning. 2020.
[MLM19]
J. Manders, T. van Laarhoven, and E. Marchiori. Adversarial Alignment of
Class Prediction Uncertainties for Domain Adaptation. 2019. arXiv: 1804.04448
[stat.ML].
[Meh+19]
N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan. A Survey on
Bias and Fairness in Machine Learning. 2019. arXiv: 1908.09635 [cs.LG].
[Mei18]
N. Meinshausen. “Causality from a distributional robustness point of view”. In:
2018 IEEE Data Science Workshop (DSW). 2018.
[Mit+20]
J. Mitrovic, B. McWilliams, J. Walker, L. Buesing, and C. Blundell. Represen-
tation Learning via Invariant Causal Mechanisms. 2020. arXiv: 2010.07922
[cs.LG].
[MBS13]
K. Muandet, D. Balduzzi, and B. Schölkopf. “Domain generalization via invari-
ant feature representation”. In: Proceedings of the 30th International Conference
on Machine Learning. 2013.
[Nai+18]
A. Naik, A. Ravichander, N. Sadeh, C. Rose, and G. Neubig. “Stress test evalua-
tion for natural language inference”. In: Proceedings of the 27th International
Conference on Computational Linguistics. 2018.
[NLM19]
J. Ni, J. Li, and J. McAuley. “Justifying recommendations using distantly-labeled
reviews and ﬁned-grained aspects”. In: Empirical Methods in Natural Language
Processing (EMNLP) (2019).
[PBM16]
J. Peters, P. Bühlmann, and N. Meinshausen. “Causal inference by using invari-
ant prediction: identiﬁcation and conﬁdence intervals”. In: Journal of the Royal
Statistical Society. Series B (Statistical Methodology) 5 (2016).
[Rib+20]
M. T. Ribeiro, T. Wu, C. Guestrin, and S. Singh. “Beyond accuracy: behavioral
testing of nlp models with checklist”. In: Proceedings of the 58th Annual Meeting
of the Association for Computational Linguistics. 2020.
[RC+18]
M. Rojas-Carulla, B. Schölkopf, R. Turner, and J. Peters. “Invariant models for
causal transfer learning”. In: Journal of Machine Learning Research 36 (2018).
[Sag+20]
S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang. “Distributionally robust
neural networks for group shifts: on the importance of regularization for worst-
case generalization”. In: International Conference on Learning Representations.
2020.
[Sch+12]
B. Schölkopf, D. Janzing, J. Peters, E. Sgouritsa, K. Zhang, and J. Mooij. “On
causal and anticausal learning”. In: Proceedings of the 29th International Cofer-
ence on International Conference on Machine Learning. 2012.
[Sch+21]
B. Schölkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and
Y. Bengio. “Toward causal representation learning”. In: Proceedings of the IEEE
5 (2021).
[SCS19]
A. Subbaswamy, B. Chen, and S. Saria. A Universal Hierarchy of Shift-Stable
Distributions and the Tradeoff Between Stability and Performance. 2019. arXiv:
1905.11374 [stat.ML].
[SS18]
A. Subbaswamy and S. Saria. “Counterfactual normalization: proactively ad-
dressing dataset shift and improving reliability using causal mechanisms”. In:
Proceedings of the 34th Conference on Uncertainty in Artiﬁcial Intelligence (UAI),
2018. 2018.
[TAH20]
D. Teney, E. Abbasnejad, and A. van den Hengel. “Learning what makes a
difference from counterfactual examples and gradient supervision”. In: CoRR
(2020). arXiv: 2004.09034.
[Tze+14]
E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep Domain
Confusion: Maximizing for Domain Invariance. 2014. arXiv: 1412.3474 [cs.CV].
13

[WZ19]
J. Wei and K. Zou. “EDA: easy data augmentation techniques for boosting
performance on text classiﬁcation tasks”. In: Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2019.
[WNB18]
A. Williams, N. Nangia, and S. Bowman. “A broad-coverage challenge corpus
for sentence understanding through inference”. In: Proceedings of the 2018
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018.
[Wu+19]
T. Wu, M. T. Ribeiro, J. Heer, and D. Weld. “Errudite: scalable, reproducible,
and testable error analysis”. In: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. 2019.
[Wu+21]
T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld. “Polyjuice: generating counter-
factuals for explaining, evaluating, and improving models”. In: Proceedings of
the 59th Annual Meeting of the Association for Computational Linguistics. 2021.
[Yan+17]
H. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the Class Weight Bias:
Weighted Maximum Mean Discrepancy for Unsupervised Domain Adaptation.
2017. arXiv: 1705.00609 [cs.CV].
[Zha+13]
K. Zhang, B. Schölkopf, K. Muandet, and Z. Wang. “Domain adaptation under
target and conditional shift”. In: Proceedings of the 30th International Conference
on Machine Learning. 2013.
14

A
Proofs
Lemma 3.1. Let X ⊥
Z be a X-measurable random variable such that, for all measurable functions
f , we have that f is counterfactually invariant if and only if f (X) is X ⊥
Z -measurable. If Z is
discrete then such a X ⊥
Z exists.
Proof. Write {X(z)}z for the potential outcomes.
First notice that if f (X) is {X(z)}z-
measurable then f (X) is counterfactually invariant. This is essentially by deﬁnition—
intervention on Z doesn’t change the potential outcomes, so it doesn’t change the value
of f (X). Conversely, if f is counterfactually invariant, then f (X) is {X(z)}z-measurable.
To see this, notice that X =
P
z 1[Z = z]X(z) is determined by Z and {X(z)}z, so f (X) =
˜f (Z,{X(z)}z) for ˜f (z,{x(z)}z) = f (
P′
z 1[z′ = z]x(z)). Now, if ˜f depends only on {X(z)}z
we’re done. So suppose that there is z,z′ such that ˜f (z,{X(z)}z) ̸= ˜f (z′,{X(z)}z) (almost
everywhere). But then f (X(z)) ̸= f (X(z′)), contradicting counterfactual invariance.
Now, deﬁne FX ⊥
Z = σ(X) ∧σ({X(z)}z) as the intersection of sigma algebra of X and the
sigma algebra of the potential outcomes {X(z)}z. Because FX ⊥
Z is the intersection of sigma
algebras, it is itself a sigma algebra. Because every FX ⊥
Z -measurable random variable is
{X(z)}z-measurable, we have that Z is not a cause of any FX ⊥
Z -measurable random variable
(i.e., there is no arrow from Z to X ⊥
Z ). Because, for f counterfactually invariant, f (X) is
both X-measurable and {X(z)}z-measurable, it is also FX ⊥
Z -measurable. FX ⊥
Z is countably
generated, as {X(z)}z and X are both Borel measurable. Therefore, we can take X ⊥
Z to be
any random variable such that σ(X ⊥
Z ) = FX ⊥
Z .
Theorem 3.2. If f is a counterfactually invariant predictor:
1. Under the anti-causal graph, f (X) ⊥⊥Z | Y .
2. Under the causal-direction graph, if Y and Z are not subject to selection (but possibly
confounded), f (X) ⊥⊥Z.
3. Under the causal-direction graph, if the association is purely spurious, Y ⊥⊥X | X ⊥
Z , Z,
and Y and Z are not confounded (but possibly selected), f (X) ⊥⊥Z | Y .
Proof. Reading d-separation from the causal graphs, we have X ⊥
Z ⊥⊥Z in the causal-direction
graph when Y and Z are not selected on, and X ⊥
Z ⊥⊥Z | Y for the other cases. By assumption,
f is a counterfactually-invariant predictor, which means that f is X ⊥
Z -measurable.
To see that interventional invariance sufﬁces for the conditional independencies, notice
that they only the distribution of (Y, Z, X). It is not possible to distinguish interventional
and counterfactual invariance based only on the distribution, so the condition must also
hold
Theorem 4.2. Let F invar be the set of all counterfactually invariant predictors. Let L be
either square error or cross entropy loss. And, let f ∗:= argminf ∈F invar EP[L(Y, f (X))] be the
counterfactually invariant risk minimizer. Suppose that the target distribution Q is causally
compatible with the training distribution P. Suppose that any of the following conditions hold:
1. the data obeys the anti-causal graph
2. the data obeys the causal-direction graph, there is no confounding (but possibly selection),
and the association is purely spurious, Y ⊥⊥X | X ⊥
Z , Z, or
3. the data obeys the causal-direction graph, there is no selection (but possibly confounding),
the association is purely spurious and the causal effect of X ⊥
Z on Y is additive, i.e., the
15

true data generating process is
Y ←g(X ⊥
Z ) + ˜g(U) + ξ where E[ξ | X ⊥
Z ] = 0,
(4.1)
for some functions g, ˜g.
Then, the training domain counterfactually invariant risk minimizer is also the target domain
counterfactually invariant risk minimizer, f ∗= argminf ∈F invar EQ[L(Y, f (X))].
Proof. First, since counterfactual invariance implies X ⊥
Z -measurable,
argmin
f ∈F invar EP[L(Y, f (X)] = argmin
f
EP[L(Y, f (X ⊥
Z )].
(A.1)
It is well-known that under squared error or cross entropy loss the minimizer is f ∗(x⊥
Z ) =
EP[Y | x⊥
Z ]. By the same argument, the counterfactually invariant risk minimizer in the
target domain is EQ[Y | x⊥
Z ]. Thus, our task is to show EP[Y | x⊥
Z ] = EQ[Y | x⊥
Z ].
We begin with the anti-causal case. We have that P(Y | X ⊥
Z ) = P(X ⊥
Z | Y )P(Y )/
R
P(X ⊥
Z | Y )dP(Y ).
By assumption, P(Y ) = Q(Y ). So, it sufﬁces to show that P(X ⊥
Z | Y ) = Q(X ⊥
Z | Y ). To that
end, from the anti-causal direction graph we have that X ⊥
Z ⊥⊥S, U | Y . Then,
P(X ⊥
Z | Y ) =
Z
P(X ⊥
Z | Y, U,S = 1)d˜P(U)
(A.2)
=
Z
P(X ⊥
Z | Y, U, ˜S = 1)d˜Q(U)
(A.3)
= Q(X ⊥
Z | Y ),
(A.4)
where the ﬁrst and third lines are causal compatibility, and the second line is from X ⊥
Z ⊥⊥
S, ˜S, U | Y .
The causal-direction case with no confounding follows essentially the same argument.
For the causal-direction case without selection,
EP[Y | X ⊥
Z ] = g(X ⊥
Z ) + EP[˜g(U) | X ⊥
Z ] + EP[ξ | X ⊥
Z ]
(A.5)
= g(X ⊥
Z ) + EP[˜g(U)] + 0.
(A.6)
The ﬁrst line is the assumed additivity. The second line follows because EP[ξ | X ⊥
Z ] = 0
for all causally compatible distributions (P(ξ, X ⊥
Z ) doesn’t change), and U ⊥⊥X ⊥
Z . Taking
an expectation over X ⊥
Z , we have EP[Y ] = EP[g(X ⊥
Z )] + EP[˜g(U)]. By the same token,
EQ[Y ] = EQ[g(X ⊥
Z )] + EQ[˜g(U)]. But, EP[g(X ⊥
Z )] = EQ[g(X ⊥
Z )], since changes to the
confounder don’t change the distribution of X ⊥
Z (that is, X ⊥
Z ⊥⊥U). And, by assumption,
EQ[Y ] = EP[Y ]. Together, these imply that EP[˜g(U)] = EQ[˜g(U)]. Whence, from (A.6),
we have EP[Y | X ⊥
Z ] = EQ[Y | X ⊥
Z ], as required.
Theorem 4.4. The counterfactually invariant risk minimizer is not Q-minimax in general.
However, under the conditions of Theorem 4.2, if the association is purely spurious, XY ∧Z ⊥⊥
Y | X ⊥
Z , Z, and P(Z, Y ) satisﬁes overlap, then the two predictors are the same. By overlap we
mean that P(Z, Y ) is a discrete distribution such that for all (z, y), if P(z, y) > 0 then there is
some y′ ̸= y such that also P(z, y′) > 0.
Proof. The reason that the predictors are not the same in general is that the counterfactually
invariant predictor will always exclude information in XY ∧Z, even when this information is
helpful for predicting Y in all target settings. For example, consider the case where Y, Z are
16

binary, X = XY ∧Z and, in the anti-causal direction, XY ∧Z = AND(Y, Z). With cross-entropy
loss, the counterfactually invariant predictor is just the constant E[Y ], but the decision rule
that uses f (X) = 1 if X = 1 is always better. In the causal case, consider XY ∧Z = Z and
Y = XY ∧Z.
Informally, the second claim follows because—in the absence of XY ∧Z information—any
predictor f that’s better than the counterfactually invariant predictor when Y and Z are
positively correlated will be worse when Y and Z are negatively correlated.
To formalize this, we begin by considering the case where Y is binary and X = X ⊥
Y . So, in
particular, the counterfactually invariant predictor is just some constant c. Let f be any
predictor that uses the information in X ⊥
Y . Our goal is to show that EQ[L(f (X ⊥
Y ), Y )] >
EQ[L(c, Y )] for at least one test distribution (so that f is not minimax). To that end, let P be
any distribution where f (X ⊥
Y ) has lower risk than c (this must exist, or we’re done). Then,
deﬁne A = {(z, y) : EP[L(f (X ⊥
Y ), y) | z] < L(c, y)}. In words: A is the collection of z, y
points where f did better than the constant predictor. Since f is better than the constant
predictor overall, we must have P(A) > 0. Now, deﬁne Ac = {(z,1 −y) : (z, y) ∈A}. That
is, the set constructed by ﬂipping the label for every instance where f did better. By the
overlap assumption, P(Ac) > 0. By construction, f is worse than c on Ac. Further, S = 1A is
a random variable that has the causal structure required by a selection variable (it’s a child
of Y and Z and nothing else). So, the distribution Q deﬁned by selection on S is causally
compatible with P and satisﬁes EQ[L(f (X ⊥
Y ), Y )] > EQ[L(c, Y )], as required.
To relax the requirement that X = X ⊥
Y , just repeat the same argument conditional on each
value of X ⊥
Z . To relax the condition that Y is binary, swap the ﬂipped label 1 −y for any
label y′ with worse risk.
B
Experimental Details
B.1
Model
All experiments use BERT as the base predictor. We use bert_en_uncased_L-12_H-
768_A-12 from TensorFlow Hub and do not modify any parameters. Following standard
practice, predictions are made using a linear map from the representation layer. We use
CrossEntropy loss as the training objective. We train with vanilla stochastic gradient descent,
batch size 1024, and learning rate 1e −5 × 1024. We use patience 10 early stopping on
validation risk. Each model was trained using 2 Tensor Processing Units.
For the MMD regularizer, we use the estimator of Gretton et al. [Gre+12] with the Gaussian
RBF kernel. We set kernel bandwidth to 10.0. We compute the MMD on (log f0(x),...,log fk(x)),
where f j(x) is the model estimate of P(Y = k | x). (Note: this is log, not logit—the later
has an extra, irrelevant, degree of freedom). We use log-spaced regularization coefﬁcients
between 0 and 128.
B.2
Data
We don’t do any pre-processing on the MNLI data.
The Amazon review data is from [NLM19].
B.2.1
Inducing Dependence Between Y and Z in Amazon Product Reviews
To produce the causal data with P(Y = 1 | Z = 1) = P(Y = 0 | Z = 0) = γ
1. Randomly drop reviews with 0 helpful votes V, until both P(V > 0 | Z = 1) > γ and
P(V > 0 | Z = 0) > 1 −γ.
17

2. Find the smallest Tz such that P(V > T1 | Z = 1) < γ and P(V > T0 | Z = 0) < 1 −γ.
3. Set Y = 1[V > T0] for each Z = 0 example and Y = 1[V > T1] for each Z = 1
example.
4. Randomly ﬂip Y = 0 to Y = 1 in examples where (Z = 0, V = T0 + 1) or (Z = 1, V =
T1 + 1), until P(Y = 1 | Z = 1) > γ and P(Y = 1 | Z = 0) > 1 −γ.
After data splitting, we have 58393 training examples, 16221 test examples, and 6489
validation examples.
To produce the anti-causal data with P(Y = 1 | Z = 1) = P(Y = 0 | Z = 0) = γ, choose a
random subset with the target association. After data splitting, we have 157616 training
examples, 43783 test examples, and 17513 validation examples.
B.2.2
Synthetic Counterfactuals in Product Review Data
We select 105 product reviews from the Amazon “clothing, shoes, and jewelery” dataset,
and assign Y = 1 if the review is 4 or 5 stars, and Y = 0 otherwise. For each review, we use
only the ﬁrst twenty tokens of text. We then assign Z as a Bernoulli random variable with
P(Z = 1) = 1
2. When Z = 1, we replace the tokens “and” and “the” with “andxxxxx” and
“thexxxxx” respectively; for Z = 0 we use the sufﬁx “yyyyy” instead. Counterfactuals can
then be produced by swapping the sufﬁxes. To induce a dependency between Y and Z, we
randomly resample so as to achieve γ = 0.3 and P(Y = 1) = 1
2, using the same procedure
that was used on the anti-causal model of “natural” product reviews. After selection there
are 13,315 training instances and 3,699 test instances.
18

