ATHENA: Advanced Techniques for High Dimensional
Parameter Spaces to Enhance Numerical Analysis
Francesco Romor∗, Marco Tezzele†, and Gianluigi Rozza‡
Mathematics Area, mathLab, SISSA, via Bonomea 265, I-34136 Trieste, Italy
May 17, 2021
Abstract
ATHENA is an open source Python package for reduction in parameter space. It imple-
ments several advanced numerical analysis techniques such as Active Subspaces (AS), Kernel-
based Active Subspaces (KAS), and Nonlinear Level-set Learning (NLL) method. It is in-
tended as a tool for regression, sensitivity analysis, and in general to enhance existing numer-
ical simulations’ pipelines tackling the curse of dimensionality. Source code, documentation,
and several tutorials are available on GitHub at https://github.com/mathLab/ATHENA under
the MIT license.
Keywords: parameter space reduction, active subspaces, kernel-based active subspaces,
nonlinear level-set learning, python
1
Introduction
Parameter space reduction techniques are particularly suited to tackle the curse of dimensionality
aﬀecting high dimensional problems for many-queries and outer-loop applications such as optimiza-
tion, uncertainty propagation, and statistical inference. The increased capabilities in simulating
every day more and more complex phenomena with many input design parameters need to be com-
pensated by a proper handling of the dimension of such parameters. This is due to the fact that
the amount of samples needed scales superlinearly with respect to the input dimension. Possible
strategies to mitigate this issue could be sparse grid sampling, adaptive sampling, and model order
reduction to cite a few.
In this package we focus on techniques which use linear and nonlinear transformations to align
the input space along the directions of maximum variation of the function of interest. In particular
in the ATHENA package are implemented the interfaces to easily employ Active Subspaces ([4]),
Kernel-based Active Subspaces ([9]), and the Nonlinear Level-set Learning ([13]) method. Figure 1
depicts a simple application of two of these methods.
These techniques are purely data driven, since they necessitate only input-output couples. We
emphasize that even if the methods are gradient-based, it is possible to approximate the gradients
of the target function with respect to the parameters using only input-output data. Thus they
are particularly suited to enhance existing numerical simulations’ pipelines such as optimization
procedures, possibly including commercial softwares. Moreover it is possible to couple parameter
space reduction with reduced order methods, enabling higher speedup computations with controlled
accuracy, especially for the solution of parametric PDEs, both in academia and in industry.
The package is multi-purpose and intended for practitioners, engineers, data scientists, and
researchers.
∗francesco.romor@sissa.it
†marco.tezzele@sissa.it
‡gianluigi.rozza@sissa.it
1
arXiv:2105.06713v1  [math.NA]  14 May 2021

ACTIVE SUBSPACES
NONLINEAR LEVEL-SET LEARNING
Figure 1: Illustration of the application of active subspaces (top panel) and nonlinear level-set
learning (bottom panel).
2
Project Management
We ensure a well organized project management exploiting all the instruments GitHub is providing,
such as issue tracker, and automatic online documentation deployment, and external tools. The
contributions to the package are welcomed and we set up a thorough description on how to open a
pull request to review the code. All the dependencies are open source python packages, ensuring
an easy installation of the whole framework. In the following we highlight the single services used.
Continuous integration:
we adopt Travis as continuous integration tool. It runs all the unit
and functional tests on the supported platforms, which are linux and osx, using Python 3.6.
Tests coverage:
we use Coveralls to check the tests coverage and to keep track of the historical
trends.
Static analysis:
we exploit Codacy as static analysis service to ensure code quality. Each PR
has to pass such analysis which includes checks on code duplications, on PyLint standards, and
several code patterns.
Documentation:
through a GitHub action the online documentation is automatically updated
whenever a new tag is pushed.
Dependencies:
we make use of only open source Python dependencies. They are:
• NumPy [6] for its numerical computing tools;
• Matplotlib [7] for the visualization of diagnostic procedures and regression plots;
• SciPy library [12] mainly for hyperparameters optimization;
• PyTorch [2] for the implementation of the Nonlinear Level-set Learning class;
• GPy [5] for the design of response surfaces with Gaussian process regression;
• GPyOpt [10] for Bayesian optimization,
• scikit-learn [8] for randomized singular value decomposition.
2

PyPI distribution:
through a GitHub action the PyPI distribution page1 is automatically
updated and the latest version published whenever a new tag is pushed.
3
Implementation Description and Capabilities
We structured the package following the well known scikit-learn APIs [8] for the Active Sub-
spaces and Kernel-based Active Subspaces classes, while for the Nonlinear Level-set Learning class
we follow the PyTorch conventions. In the following we report code snippets to emphasize the
APIs implementation. We also provide extensive tutorials to show not only the basic usage of
the package, but also how to exploit it to solve inverse problems and stochastic partial diﬀerential
equations in high dimensions2.
1
from
athena
import
ActiveSubspaces
2
3
ss = ActiveSubspaces (dim=1, method=’ exact ’ ,
n boot =100)
4
ss . f i t ( inputs ,
gradients )
5
# transform
inputs
to
a c t i v e
and
i n a c t i v e
v a r i a b l e s
6
a c t i v e v a rs ,
i n a c t i v e v a r s = ss . transform ( inputs )
7
# map an
input
in
the
a c t i v e
subspace
to
one
of
the
corresponding
8
# points
in
the
f u l l
parameter
space
9
f u l l i n p u t = ss . in ve rse tr an sf orm ( a c t i v e v a r s [ 4 ] ) [ 0 ]
Listing 1: Code snippet illustrating the API for the active subspaces example.
1
from
athena
import
KernelActiveSubspaces ,
FeatureMap ,
CrossValidation ,
average rrmse
2
3
# d e f i n e
the
f e a t u r e map
4
fm = FeatureMap ( d is t r ,
bias ,
input dim ,
n f e a t u r e s =1000, params ,
variance )
5
# i n s t a n t i a t e
a
KernelActiveSubspaces
object
with
a s s o c i a t e d
f e a t u r e map
6
kss = KernelActiveSubspaces ( feature map=fm ,
dim=1,
n f e a t u r e s =1000)
7
# tuning
of
the
f e a t u r e map with a
c r o s s
v a l i d a t i o n
procedure
8
csv = CrossValidation ( inputs ,
outputs ,
gradients ,
folds ,
kss )
9
best = fm . tune pr matrix ( average rrmse ,
bounds ,
f n a r g s={ ’ csv ’ : csv , ’ verbose ’ :
verbose } , method=’ bso ’ ,
maxiter =20)
10
# compute
the
kernel −based
a c t i v e
subspace
11
kss . f i t ( inputs ,
gradients )
12
# transform
inputs
to
a c t i v e
and
i n a c t i v e
v a r i a b l e s
13
a c t i v e v a rs ,
i n a c t i v e v a r s = kss . transform ( inputs )
Listing 2: Code snippet illustrating the API for the kernel-based active subspaces example.
1
from
athena
import
NonlinearLevelSet
2
3
n l l = NonlinearLevelSet ( n l a y e r s =8,
active dim =1,
l r =0.02 ,
epochs =10000)
4
n l l . t r a i n ( inputs torch ,
g r a d i e n t s t o r c h )
5
# transform
inputs
to
reduced
v a r i a b l e s
by
taking
the
f i r s t
component
6
reduced inputs = n l l . forward ( inputs ) [ : ,
0]
Listing 3: Code snippet illustrating the API for the nonlinear level-set learning example.
4
Related Software
The Python Active-subspaces Utility Library ([3]) is the main open source existing package
for parameter space reduction using active subspaces. Unfortunately, its development has stopped
4 years ago and it supports only Python 2.7. Moreover it is able to deal only with scalar functions.
Other minor packages exist, mainly to replicate the results of published works, such as pyaspg3
employed for the study of gradient-free active subspaces in [11], or the Active Manifold method ([1])
implementation4.
1Available at https://github.com/mathLab/ATHENA/tree/master/tutorials.
2A comprehensive presentation of the tutorials can be found at https://github.com/mathLab/ATHENA/tree/
master/tutorials.
3The package pyaspg is available at https://github.com/PredictiveScienceLab/py-aspgp.
4The
package
active-manifold-icml2019-code
is
available
at
https://github.com/bridgesra/
active-manifold-icml2019-code.
3

5
Conclusions and Future Perspectives
The future development of the ATHENA package includes the integration of new parameter space
reduction techniques, as for example the aforementioned Active Manifold method. Moreover ex-
isting classes will be extended with new functionalities following the theoretical advances. Some
future directions could be a back mapping for the Kernel-based Active Subspaces, gradient-free
implementation of Active Subspaces, and improved uncertainty quantiﬁcation capabilities.
Acknowledgements
This work was partially supported by an industrial Ph.D. grant sponsored by Fincantieri S.p.A.
(IRONTH Project), by MIUR (Italian ministry for university and research) through FARE-X-
AROMA-CFD project, and partially funded by European Union Funding for Research and In-
novation — Horizon 2020 Program — in the framework of European Research Council Executive
Agency: H2020 ERC CoG 2015 AROMA-CFD project 681447 “Advanced Reduced Order Methods
with Applications in Computational Fluid Dynamics” P.I. Professor Gianluigi Rozza.
References
[1] R. A. Bridges, A. D. Gruber, C. Felder, M. Verma, and C. Hoﬀ. Active Manifolds: A non-linear
analogue to Active Subspaces. arXiv preprint arXiv:1904.13386, 2019.
[2] R. Collobert, K. Kavukcuoglu, and C. Farabet.
Torch7: A Matlab-like Environment for
Machine Learning. In BigLearn, NIPS Workshop, 2011.
[3] P. Constantine, R. Howard, A. Glaws, Z. Grey, P. Diaz, and L. Fletcher. Python Active-
subspaces Utility Library. Journal of Open Source Software, 1(5):79, 2016.
[4] P. G. Constantine. Active subspaces: Emerging ideas for dimension reduction in parameter
studies, volume 2 of SIAM Spotlights. SIAM, 2015.
[5] GPy. GPy: A Gaussian process framework in python. http://github.com/SheffieldML/
GPy, since 2012.
[6] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau,
E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk,
M. Brett, A. Haldane, J. F. del R’ıo, M. Wiebe, P. Peterson, P. G’erard-Marchant, K. Shep-
pard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming
with NumPy. Nature, 585(7825):357–362, Sept. 2020.
[7] J. D. Hunter. Matplotlib: A 2D graphics environment. Computing in Science & Engineering,
9(3):90–95, 2007.
[8] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel,
P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,
M. Perrot, and E. Duchesnay. Scikit-learn: Machine Learning in Python. Journal of Machine
Learning Research, 12:2825–2830, 2011.
[9] F. Romor, M. Tezzele, A. Lario, and G. Rozza. Kernel-based Active Subspaces with appli-
cation to CFD parametric problems using Discontinuous Galerkin method. arXiv preprint
arXiv:2008.12083, 2020.
[10] The GPyOpt authors.
GPyOpt: A Bayesian Optimization framework in python.
http:
//github.com/SheffieldML/GPyOpt, 2016.
[11] R. Tripathy, I. Bilionis, and M. Gonzalez. Gaussian processes with built-in dimensionality re-
duction: Applications to high-dimensional uncertainty propagation. Journal of Computational
Physics, 321:191–223, 2016.
4

[12] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau,
E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wil-
son, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey,
˙I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Hen-
riksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van
Mulbregt, and SciPy 1.0 Contributors.
SciPy 1.0: Fundamental Algorithms for Scientiﬁc
Computing in Python. Nature Methods, 17:261–272, 2020.
[13] G. Zhang, J. Zhang, and J. Hinkle. Learning nonlinear level sets for dimensionality reduction
in function approximation.
In Advances in Neural Information Processing Systems, pages
13199–13208, 2019.
5

