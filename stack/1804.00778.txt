High-Dimensional Joint Estimation
of Multiple Directed Gaussian Graphical
Models
Yuhao Wang∗
Statistical Laboratory, University of Cambridge, Cambridge, UK
e-mail: yw505@cam.ac.uk
Santiago Segarra∗
Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA
e-mail: segarra@rice.edu
Caroline Uhler
Laboratory for Information & Decision Systems and Institute for Data, Systems, and Society
Massachusetts Institute of Technology, Cambridge, MA, USA
e-mail: cuhler@mit.edu
Abstract: We consider the problem of jointly estimating multiple related directed
acyclic graph (DAG) models based on high-dimensional data from each graph. This
problem is motivated by the task of learning gene regulatory networks based on gene
expression data from different tissues, developmental stages or disease states. We prove
that under certain regularity conditions, the proposed ℓ0-penalized maximum likeli-
hood estimator converges in Frobenius norm to the adjacency matrices consistent with
the data-generating distributions and has the correct sparsity. In particular, we show
that this joint estimation procedure leads to a faster convergence rate than estimating
each DAG model separately. As a corollary, we also obtain high-dimensional consis-
tency results for causal inference from a mix of observational and interventional data.
For practical purposes, we propose jointGES consisting of Greedy Equivalence Search
(GES) to estimate the union of all DAG models followed by variable selection using
lasso to obtain the different DAGs, and we analyze its consistency guarantees. The pro-
posed method is illustrated through an analysis of simulated data as well as epithelial
ovarian cancer gene expression data.
MSC 2010 subject classiﬁcations: Primary 62F12; secondary 62F30.
Keywords and phrases: Causal inference, linear structural equation model, high-
dimensional statistics, graphical model.
Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
2
Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2.1
Directed acyclic graphs and linear structural equation models . . . . .
4
2.2
ℓ0-penalized maximum likelihood estimation for a single DAG model
6
∗At the time this research was completed, Yuhao Wang and Santiago Segarra were at the Massachusetts
Institute of Technology.
1
arXiv:1804.00778v3  [math.ST]  27 Jun 2020

Y. Wang et al./Joint Estimation of Multiple DAGs
2
2.3
Collection of DAGs . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3
Joint estimation of multiple DAGs . . . . . . . . . . . . . . . . . . . . . .
7
3.1
Joint ℓ0-penalized maximum likelihood estimator . . . . . . . . . . .
7
3.2
JointGES: Joint greedy equivalence search . . . . . . . . . . . . . . .
8
4
Consistency guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
4.1
Statistical consistency of the joint ℓ0-penalized MLE
. . . . . . . . .
9
4.2
Conditions for Theorem 4.9 . . . . . . . . . . . . . . . . . . . . . . .
11
4.3
Consistency under milder conditions . . . . . . . . . . . . . . . . . .
13
5
Extension to interventions . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
6
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
6.1
Performance evaluation of joint causal inference . . . . . . . . . . . .
18
6.2
Simulation analysis of Condition 4.7 . . . . . . . . . . . . . . . . . .
20
6.3
Gene regulatory networks of ovarian cancer subtypes . . . . . . . . .
21
7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
A Theoretical analysis of statistical consistency results . . . . . . . . . . . . .
23
A.1
Random events . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
A.1.1
Random event E1 . . . . . . . . . . . . . . . . . . . . . . . .
23
A.1.2
Random event E2 . . . . . . . . . . . . . . . . . . . . . . . .
27
A.1.3
Random event E3 . . . . . . . . . . . . . . . . . . . . . . . .
31
A.2
Proof of Theorem 4.9 . . . . . . . . . . . . . . . . . . . . . . . . . .
32
A.2.1
Bounds on new probability space
. . . . . . . . . . . . . . .
32
A.2.2
Proof of Theorem 4.9 . . . . . . . . . . . . . . . . . . . . . .
37
A.3
Proof of Theorem 4.11 . . . . . . . . . . . . . . . . . . . . . . . . .
38
A.4
Proof of Corollary 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . .
39
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
1. Introduction
Directed acyclic graph (DAG) models, also known as Bayesian networks, are widely
used to model causal relationships in complex systems across various ﬁelds such as
computational biology, epidemiology, sociology, and environmental management [1,
12, 31, 36, 40]. In these applications we often encounter high-dimensional datasets
where the number of variables or nodes greatly exceeds the number of observations.
While the problem of structure identiﬁcation for undirected graphical models in the
high-dimensional setting is quite well understood [35, 26, 11, 5, 46], such results are
just starting to become available for directed graphical models. The difﬁculty in identi-
fying DAG models can be attributed to the fact that searching over the space of DAGs
is NP-complete in general [6].
Methods for structure identiﬁcation in directed graphical models can be divided into
two categories and hybrids of these categories. Constraint-based methods, such as the
prominent PC algorithm, ﬁrst learn an undirected graph from conditional independence
relations and in a second step orient some of the edges [15, 40]. Score-based methods,
on the other hand, posit a scoring criterion for each DAG model, usually a penalized

Y. Wang et al./Joint Estimation of Multiple DAGs
3
likelihood score, and then search for the network with the highest score given the ob-
servations. An example is the celebrated Greedy Equivalence Search (GES) algorithm,
which can be used to greedily optimize the ℓ0-penalized likelihood such as the Bayesian
Information Criterion (BIC) [7]. High-dimensional consistency guarantees were re-
cently obtained for the PC algorithm [20] and for score-based methods [24, 29, 45].
Existing methods have focused on estimating a single directed graphical model.
However, in many applications we have access to data from related classes, such as
gene expression data from different tissues, cell types or states [25, 38], different de-
velopmental stages [3], different disease states [42], or from different perturbations
such as knock-out experiments [9]. In all these applications, one would expect that the
underlying regulatory networks are similar to each other, since they stem from the same
species, individual or cell type, but also have important differences that drive differen-
tiation, development or a certain disease. This raises an important statistical question,
namely how to jointly estimate related directed graphical models in order to effectively
make use of the available data.
Various methods have been proposed for jointly estimating undirected Gaussian
graphical models. To preserve the common structure, Guo et al. [16] suggested to use
a hierarchical penalty and Danaher et al. [8] suggested the use of a generalized fused
lasso or group lasso penalty. While both approaches achieve the same convergence
rate as the individual estimators, Cai et al. [4] were able to improve the asymptotic
convergence rate of joint estimation using a weighted constrained ℓ∞/ℓ1 minimization
approach. Bayesian methods have been proposed for this problem as well [33]. Related
works also include [28], where it is assumed that the networks differ only locally in
a few nodes and [22, 39], where the assumption is that the networks are ordered and
related by continuously changing edge weights.
In this paper, we propose a framework based on ℓ0-penalized maximum likelihood
estimation for jointly estimating related directed Gaussian graphical models. We show
that the joint ℓ0-penalized maximum likelihood estimator (MLE) achieves a faster
asymptotic convergence rate as compared to the individual estimators. In addition, by
viewing interventional data as data coming from a related network, we show that the
interventional BIC scoring function proposed in [17] can be obtained as a special case
of the joint ℓ0-penalized maximum likelihood approach presented here. Our theoretical
consistency guarantees also explain the empirical ﬁndings of [17], namely that esti-
mating a DAG model from interventional data usually leads to better recovery rates
as compared to estimating a DAG model from the same amount of purely observa-
tional data. These theoretical results are based on the global optimum of ℓ0-penalized
maximum likelihood estimation. To overcome the computational bottleneck of this op-
timization problem we propose a greedy approach (jointGES) for solving this problem
by extending GES [7] to the joint estimation setting. We analyze its properties from a
theoretical point of view and test its performance on synthetic data and gene expression
data from epithelial ovarian cancer.
The remainder of this paper is structured as follows. In Section 2, we review some
relevant background related to DAG models and introduce notation for the joint DAG
estimation problem studied in this paper. In Section 3, we present the joint ℓ0-penalized
maximum likelihood estimator and jointGES, an adaptation of GES for solving this
optimization problem. Section 4 establishes results regarding the statistical consistency

Y. Wang et al./Joint Estimation of Multiple DAGs
4
of the ℓ0-penalized MLE and jointGES. Section 5 presents the implications for learning
DAG models from a mix of observational and interventional data. In Section 6, we
illustrate the performance of our proposal in a simulation study and an application to
the analysis of gene expression data. We conclude with a short discussion in Section 7.
The proofs of supporting results are contained in the Appendix.
2. Preliminaries
In Section 2.1 we introduce DAG models, in particular linear structural equation mod-
els, and discuss statistical features enjoyed by random vectors following these models.
In Section 2.2 we brieﬂy review existing approaches for estimating a single directed
graphical model from observational data. Finally, Section 2.3 describes a setting where
multiple related directed graphical models exist.
2.1. Directed acyclic graphs and linear structural equation models
Let G = (V, E) denote a DAG with vertices V = [p] = {1, · · · , p} and directed edges
E ⊆V × V , where |G| denotes the cardinality of E. Let A ∈Rp×p be the adjacency
matrix specifying the edge weights of the underlying DAG G, i.e., Aij ̸= 0 if and
only if (i, j) ∈E. Also, let ϵ ∼N(0, Ω) denote a p-dimensional multivariate Gaus-
sian random variable with zero mean and diagonal covariance matrix Ω. In this work,
we assume that the observed random vector X = (X1, · · · , Xp) ∈Rp is generated
according to the following linear structural equation model (SEM).
X = AT X + ϵ.
(1)
Hence X follows a multivariate Gaussian distribution with zero mean and covariance
matrix Σ, where the inverse covariance (or precision) matrix Θ = Σ−1 is given by
Θ = (I −A)Ω−1(I −A)T .
(2)
Let Paj(G) denote the parents of node j in G;then it follows from (1) that the distribu-
tion of X factorizes as
P(X) =
p
Y
j=1
P(Xj|XPaj(G)).
Such a factorization of P according to G is equivalent to the Markov assumption with
respect to G [23, Theorem 3.27]. Formally, given j, k ∈V and an arbitrary subset of
nodes S ⊂V \ {j, k}, then
j is d-separated from k | S in G
⇒
Xj ⊥⊥Xk|XS in P.
(3)
If the implication (3) holds bidirectionally, then P is said to be faithful [40] with respect
to G. Note that there exist DAGs G1 and G2 that encode the same d-separations and
hence the same conditional independence relations. Such DAGs are said to belong to
the same Markov equivalence class.

Y. Wang et al./Joint Estimation of Multiple DAGs
5
A consequence of the acyclicity of G is that there exists at least one permutation
π of [p] such that Aij = 0 for all π(i) ≥π(j). Putting it differently, if the rows and
columns of A are reordered according to π, then the resulting matrix is strictly upper
triangular. Hence, if such a permutation π is known a priori, one can obtain the SEM
parameters (A, Ω) from Θ according to the following steps [cf. (2)]. First, we reorder Θ
according to π. Then, we perform on the reordered Θ an upper-triangular-plus-diagonal
Cholesky decomposition to obtain (A′, Ω′). Finally, we revert the ordering by permut-
ing the rows and columns of A′ and Ω′ according to π−1 and obtain the sought (A, Ω).
For an arbitrary permutation π and a given Θ, we denote by (Aπ, Ωπ) the Cholesky de-
composition parameters obtained from the procedure just described. Alternatively, one
can obtain (Aπ, Ωπ) by solving p linear regressions [cf. (1)]. More precisely, we can
obtain each column of Aπ by regressing Xj only on those Xi such that π(i) < π(j)
for all j. Once Aπ is obtained, one can estimate the variance of ϵ in (1) to get Ωπ. In the
remainder of the paper, we denote by (A0, Ω0) and Θ0 the true parameters of the data-
generating SEM and the associated precision matrix, respectively. Moreover, we denote
by (A0π, Ω0π) the SEM parameters obtained from the described procedure when the
true precision matrix Θ0 is used. Notice that (A0, Ω0) = (A0π, Ω0π) if π is any per-
mutation consistent with the true underlying DAG G0. The DAG Gπ corresponding to
the non-zero entries of Aπ is known as the minimal I-MAP (independence map) with
respect to π. The minimal I-MAP with the fewest number of edges is called minimal-
edge I-MAP [45]. If P is faithful with respect to a DAG G, then G is a minimal-edge
I-MAP of P [34, 45].
Furthermore, it has been shown in [32] that all DAGs in a Markov equivalence class
share the same skeleton – i.e., the set of edges when directions are ignored – and v-
structures. A v-structure is a triplet (j, k, ℓ) ⊆V such that (j, k), (ℓ, k) ∈E but j and
ℓare not connected in either direction. This motivates the representation of a Markov
equivalence class as a completely partially directed acyclic graph (CPDAG), which is a
graph containing both directed and undirected edges [2]. A directed edge means that all
DAGs in the Markov equivalence class share the same direction for this edge whereas
an undirected edge means that both directions for that speciﬁc edge are present within
the class. In the same way, one can represent a subset of a Markov equivalence class via
a partially directed acyclic graph (PDAG), where the directions of the edges are only
determined by the graphs within the subset. In particular, some undirected edges in a
CPDAG would become directed edges in a PDAG representing a subset of the class.
Notice that both DAGs and CPDAGs are special cases of PDAGs, where the former
represents a single graph and the latter represents the whole equivalence class.
To consistently estimate causal DAG models in high dimensions, the ℓ0-penalized
maximum likelihood estimation approach [45], the high-dimensional PC method [20]
and the ARGES method [29] have been proposed. These methods have high-dimensional
guarantees under different conditions, and are thus not directly comparable. In particu-
lar, the theoretical guarantees of ℓ0-penalized maximum likelihood estimation requires
the so-called “beta-min” condition [45]; the high-dimensional PC algorithm requires
the “strong faithfulness” condition [20]; and ARGES requires the “strong faithfulness”
condition as well as additional conditions. For further discussions on the strength of
different conditions, especially the “beta-min” and “strong faithfulness” conditions,
we refer the readers to Remark 4.10 and [45, Section 4.3.2].

Y. Wang et al./Joint Estimation of Multiple DAGs
6
2.2. ℓ0-penalized maximum likelihood estimation for a single DAG model
We denote by ˆX ∈Rn×p the observed data, where each row of ˆX represents a re-
alization of the random vector X. We say that we are in the low-dimensional setting
if asymptotically p remains a constant as n →∞. By contrast, whenever p →∞as
n →∞, we say that we are in the high-dimensional setting. Assuming faithfulness,
Chickering [7] shows that GES outputs a consistent estimator in the low-dimensional
setting by optimizing the following objective – also known as the Bayesian information
criterion (BIC) –
( ˆA, ˆΩ) :=
argmax
A∈A, Ω∈D+
ℓn( ˆX; A, Ω) −λ2∥A∥0,
(4)
where λ2 = 1
2
log n
n , A denotes the set of all valid adjacency matrices associated with
DAGs, D+ is the set of non-negative diagonal matrices, and ℓn is the likelihood func-
tion
ℓn( ˆX; A, Ω) := −trace
 ˆXT ˆX
n
· (I −A)Ω−1(I −A)T
!
+ log det
 (I −A)Ω−1(I −A)T 
.
(5)
In the high-dimensional setting, van de Geer and B¨uhlmann [45] give consistency guar-
antees for the global optimum of (4) when the collection A is further constrained
to contain only adjacency matrices with at most d incoming edges for each node,
where d = O(n/ log p). More precisely, they show that there exists some parame-
ter λ2 ≍
log p
n
such that the optimum ( ˆA, ˆΩ) in (4) converges in Frobenius norm to
(A0ˆπ, Ω0ˆπ) for increasing n and p, where ˆπ is a permutation consistent with ˆA, i.e.,
∥ˆA −A0ˆπ∥2
F + ∥ˆΩ−Ω0ˆπ∥2
F = O
 λ2|G0|

.
(6)
Notice, however, that (6) does not guarantee statistical consistency since ˆπ need not be
a permutation consistent with the true underlying DAG. Moreover, (6) does not hold
for every permutation ˆπ consistent with ˆA, but [45] shows the existence of at least one
such permutation. In addition, it is shown in [45] that the number of non-zero elements
in ˆA, A0ˆπ, and A0 are all of the same order of magnitude, i.e., | ˆG| ≍|G0ˆπ| ≍|G0|.
2.3. Collection of DAGs
Consider the setting where not all the observed data comes from the same DAG, but
rather from a collection of DAGs {G(k) = (V, E(k))}K
k=1 that share the same node set
V = [p]. In addition, we assume that all DAGs in a collection are consistent with some
permutation π. This precludes a scenario where (i, j) ∈E(k) and (j, i) ∈E(k′) for
some k ̸= k′. This is a reasonable assumption in, e.g., the analysis of gene expression
data, where regulatory links may appear or disappear, but they in general do not change
direction.
Denote by {(A(k), Ω(k))}K
k=1 a set of SEMs on the K DAGs {G(k)}K
k=1 and by
{ ˆX(k)}K
k=1 the data generated from each SEM, where we observe nk realizations for

Y. Wang et al./Joint Estimation of Multiple DAGs
7
each DAG G(k). In this way, each row of the data matrix ˆX(k) ∈Rnk×p corresponds
to a realization of the random vector X(k) deﬁned as
X(k) = A(k)T X(k) + ϵ(k)
with
ϵ(k) ∼N(0, Ω(k)).
Collections of DAGs arise for example naturally when considering data from perfect
(also known as hard) interventions [10]. Consider a non-intervened DAG G with SEM
parameters (A, Ω) [cf. (1)]. Then a perfect intervention on a subset of nodes Ik ⊂V
gives rise to the interventional distribution
XIk = AIk T XIk + ϵIk
with
ϵIk ∼N(0, ΩIk),
where AIk
ij = 0 if j ∈Ik and AIk
ij = Aij otherwise, and the diagonal matrix ΩIk
satisﬁes ΩIk
ii = Ωii if i ̸∈Ik [17, 18]. We denote the DAG given by the non-zero
entries of AIk by GIk.
In accordance with the notation introduced in Section 2.1, we denote by G(k)
0
and
(A(k)
0 , Ω(k)
0 ) the true data-generating DAG and SEM parameters for class k, respec-
tively, and by π0 a permutation that is consistent with A(k)
0
for all classes k ∈[K].
Moreover, we denote by G(k)
0π and (A(k)
0π , Ω(k)
0π ) the DAG and SEM parameters obtained
from the Cholesky decomposition of the true precision matrix Θ(k)
0
when permuted by
π. We denote by Σ(k)
0
the true covariance matrix of the SEM for class k, i.e., the inverse
of Θ(k)
0 . Finally, we deﬁne Gunion
0
as the union of all G(k)
0
– i.e., an edge appears in Gunion
0
if it appears in any G(k)
0
– and Gunion
0π
as the union of G(k)
0π . For interventional data, we
use (AIk
0 , ΩIk
0 ) to denote the true SEM parameters after intervening on targets Ik.
3. Joint estimation of multiple DAGs
We ﬁrst present a penalized maximum likelihood estimator that is the natural extension
of (4) for the case where a collection of DAGs is being estimated. Since this involves
minimizing ∥·∥0, we then discuss a greedy approach that alleviates the computational
complexity of this estimator.
3.1. Joint ℓ0-penalized maximum likelihood estimator
With d denoting a pre-speciﬁed sparsity level and wk = nk/n indicating the proportion
of observed data from DAG k, we propose the following estimator:
n
ˆπ,{( ˆA(k), ˆΩ(k))}K
k=1
o
:=
argmax
π,{(A(k),Ω(k))}K
k=1
K
X
k=1
wkℓnk( ˆX(k); A(k), Ω(k)) −λ2

K
X
k=1
|A(k)|

0
(7)
subject to
A(k) ∈Aπ, ∥A(k)∥∞,0 ≤d, Ω(k) ∈D+ ∀k,

Y. Wang et al./Joint Estimation of Multiple DAGs
8
Algorithm 1 JointGES for joint ℓ0-penalized maximum likelihood estimation of mul-
tiple DAGs.
Input:
Collection of observations ˆ
X(1) ∈Rn1×p, · · · , ˆ
X(K) ∈RnK×p, sparsity bound d, penalization
parameters λ1 and λ2
Output: Collection of weighted adjacency matrices ˆ
A(1), · · · , ˆ
A(K)
1: Apply GES to ﬁnd ˆGunion, an approximate solution to the following optimization problem
argmin
G
p
X
j=1
 K
X
k=1
wk
"
min
a∈R|Paj (G)| log

∥ˆ
X(k)
j
−ˆ
X(k)
Paj(G) a∥2
2
#
+ λ2
1|Paj(G)|
!
subject to
max
j
|Paj(G)| ≤d
(8)
2: Estimate the weighted adjacency matrices { ˆ
A(k)}K
k=1 consistent with ˆGunion by solving Kp sparse
regressions of the form
ˆa(k)
j
=
argmin
a | supp(a)⊆Paj( ˆ
Gunion)
1
nk
∥ˆ
X(k)
j
−ˆ
X(k)a∥2
2 + λ2
2∥a∥1.
where Aπ is the set of all adjacency matrices consistent with permutation π and the
matrix norm ∥· ∥∞,0 computes the maximum ℓ0-norm across the rows of the argument
matrix. The optimization problem in (7) seeks to maximize a weighted log-likelihood
of the observations (where more weight is given to SEMs with more realizations) pe-
nalized by the support of the union of all estimated DAGs. To see why this is true,
notice that ∥PK
k=1 |A(k)|∥0 counts the number of (i, j) entries for which A(k)
ij
̸= 0
for at least one graph k. This penalization on the union of estimated DAGs promotes
overlap in the supports of the different A(k). Regarding the constraints in (7), the ﬁrst
constraint imposes that all estimated DAGs are consistent with the same permutation
π, which is itself an optimization variable. This constraint is in accordance with our as-
sumption in Section 2.3 and drastically reduces the search space of DAGs. The second
constraint ensures that the maximum in-degree in all graphs is at most d, and the last
constraint imposes the natural requirement that all noise covariances are diagonal and
non-negative.
Notice that (7) is a natural extension of (4). Indeed, for the case K = 1 the objective
in (7) immediately boils down to that in (4). Moreover, when there is only one graph and
π can be selected freely, the constraint A(1) ∈Aπ is effectively identical to A(1) ∈A,
i.e., the constraint in (4). Finally, observe that in (7) we have included the additional
maximum in-degree constraint required in the high-dimensional setting [cf. discussion
after (5)].
3.2. JointGES: Joint greedy equivalence search
The ℓ0 norm as well as the optimization over all permutations π render the problem
of (7) non-convex, thus, hard to solve efﬁciently. In this section, we present a greedy
approach to ﬁnd a computationally tractable approximation to a solution to (7). The al-
gorithm, which we term JointGES, is succinctly presented in Algorithm 1 and consists
of two steps.

Y. Wang et al./Joint Estimation of Multiple DAGs
9
In the ﬁrst step of Algorithm 1 we recover ˆGunion, our estimate of the union of all the
DAGs to be inferred. We do this by ﬁnding an approximate solution to (8) via the im-
plementation of GES [7]. The objective (scoring function) in (8) consists of two terms.
The ﬁrst term is given by the sum of the log-likelihoods of the achievable residues
when regressing the jth column of X(k), denominated as X(k)
j
, on X(k)
Paj(G) for each
node j and DAG k. In [45], van de Geer and B¨uhlmann show that if we keep the un-
derlying DAG G ﬁxed, the maximum likelihood estimator proposed in (4) is equivalent
to optimizing Pp
j=1

mina∈R|Paj (G)| log

∥ˆXj −ˆXPaj(G) a∥2
2

. Thus, the ﬁrst term
in (8) corresponds to the ﬁrst term in the objective of (7). The second term penalizes
the size of the parent set of each node in the graph to be recovered, effectively penaliz-
ing the number of edges in the graph. In this way, the scoring function in (8) promotes
a sparse G in the same way that the objective of (7) promotes the union of all K re-
covered graphs to have a sparse support. Additionally, it is immediate to see that the
scoring function in (8) is decomposable [7], a key feature that enables the implemen-
tation of GES to ﬁnd an approximate solution. Once we have obtained the union of all
sought DAGs ˆGunion from step 1, in the second step of our algorithm we estimate the
DAGs ˆG(1), · · · , ˆG(K) by searching over the subDAGs of ˆGunion. More precisely, for
each node j we estimate its parents in ˆG(k) by regressing X(k)
j
on X(k)
Paj( ˆGunion) using
lasso, where the support of ˆa(k)
j
corresponds to the set of parents of j in ˆG(k).
To summarize, Algorithm 1 recovers K DAGs by ﬁrst estimating the union of all
these DAGs ˆGunion using GES and then inferring the speciﬁc weight adjacency matri-
ces ˆA(k) via a lasso regression, while ensuring consistency with the previously esti-
mated ˆGunion.
4. Consistency guarantees
The main goal of this section is to provide theoretical guarantees on the consistency of
the solution to Problem (7) in the high-dimensional setting. Our main result is presented
in Theorem 4.9; in Section 4.3 we present a laxer statement of consistency based on
milder conditions.
4.1. Statistical consistency of the joint ℓ0-penalized MLE
A series of conditions must hold for our main result to be valid. We begin by stating
these conditions followed by the formal consistency result in Theorem 4.9. The ratio-
nale behind these conditions and their implications are discussed after the theorem in
Section 4.2.
Condition 4.1. All DAGs G(1)
0 , · · · , G(K)
0
are minimal-edge I-MAPs.
Condition 4.2. There exists a constant σ2
0 that bounds the variance of all the observed
processes, i.e., maxk,i[Σ(k)
0 ]ii ≤σ2
0.
Condition 4.3. The smallest eigenvalues of all Σ(k)
0
are non-zero, i.e. mink Λmin(Σ(k)
0 ) =
Λmin > 0.

Y. Wang et al./Joint Estimation of Multiple DAGs
10
Condition 4.4. There exists some constant α such that, for all k, the maximum allow-
able in-degree d in the objective function (7) is bounded as d ≤αnk/ log p.
Condition 4.5. For all π and j there exist some constants ˜α and cs > 2 such that
|Paj(Gunion
0π
)| + cs ≤˜α
 
min
(
n
K7(log p)3
 1
3
,
n
K7(log n)2 log p
)!
.
Condition 4.6. The number of DAGs K satisﬁes K = o(log p) and the amount of data
associated with each DAG is comparable in the sense that n1 ≍n2 ≍· · · ≍nK.
Condition 4.7. There exists some constant ct > 0 such that |Gunion
0π
| ≤ct
PK
k=1 wk|G(k)
0π |
for any permutation π.
Condition 4.8. There exist constants η0 and η1 such that 0 ≤η1 < 1, 0 < η2
0 <
(1 −η1)/ct, and
X
i,j
1
([A(k)
0π ]i,j
 >
p
log p/n
η0
q
p/|Gunion
0
| ∨1
)
≥(1 −η1)|G(k)
0π |,
(9)
for all permutations π and graphs k ∈[K], where 1{·} denotes the indicator function
and ct is as in Condition 4.7.
With the above conditions in place, the following result can be shown.
Theorem 4.9. If Conditions 4.1-4.8 hold and λ is chosen such that
λ2 ≍log p
n

p
|Gunion
0
| ∨1

,
then there exists a constant c > 0 , that depends on cs, such that with probability
1 −exp(−cp) the solution to (7) satisﬁes
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F +
K
X
k=1
wk∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F = O
 λ2|Gunion
0
|

.
(10)
Furthermore, denoting by ˆG the union of the graphs ˆG(k) associated with the K recov-
ered adjacency matrices ˆA(k), we have that
| ˆG| ≍|Gunion
0ˆπ
| ≍|Gunion
0
|.
(11)
The proof of Theorem 4.9 is given in Appendix A.2. To intuitively grasp the result
in the above theorem, assume that the number of edges in Gunion
0
is proportional to the
number of nodes p so that λ2|Gunion
0
| →0 for increasing n as long as n > p log p.
Hence, under these conditions, (10) guarantees that for the recovered permutation ˆπ,
the estimated adjacency matrix ˆA(k) converges to A(k)
0ˆπ in Frobenius norm for all k. This
not only implies that both adjacency matrices have similar structure, but also that the
edge weights are similar. Moreover, from (11) it follows that the number of edges in the

Y. Wang et al./Joint Estimation of Multiple DAGs
11
estimated graph ˆG, i.e., | ˆG| is similar to the number of edges in the union of all minimal
I-MAPs with permutation ˆπ, i.e., |Gunion
0ˆπ
|. More importantly, | ˆG| is also similar to the
number of edges in the true union graph |Gunion
0
|. Despite these guarantees, it should be
noted that similar to the results in [45], the permutation ˆπ need not coincide with the
permutation π of the true graphs to be recovered.
We now assess the beneﬁts of performing joint estimation of the K DAGs as op-
posed to estimating them separately. To do so, we compare the guarantees in Theo-
rem 4.9 to those developed in [45] for separate estimation. The application of the con-
sistency bound reviewed in (6) yields that for the separate estimation of K DAGs, when
we are in the setting where all K DAGs are highly overlapping (cf. Condition 4.7), by
choosing λ such that λ2 ≍log p
n

p
|Gunion
0
| ∨1

, one can guarantee that
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ(k)∥F +
K
X
k=1
wk∥ˆΩ(k) −Ω(k)
0ˆπ(k)∥2
F = O

Kλ2 max
k∈[K] |G(k)
0 |

,
(12)
where it should be noted that in the separate estimation the recovered permutation
ˆπ(k) can vary with k. A direct comparison of (10) and (12) reveals that performing
joint estimation improves the accuracy by a factor of K from Ω(K log p
n ) to Ω( log p
n ).
Hence, for joint estimation the accuracy scales with the total number of samples n,
showing that our procedure yields maximal gain from each observation, even if the
data is generated from K different DAGs. Moreover, the result in (10) holds under
slightly milder conditions than those needed for (12) to hold since Condition 4.8 is a
relaxed version of the beta-min condition in [45]. A more detailed discussion about the
conditions of Theorem 4.9 is given next.
4.2. Conditions for Theorem 4.9
It has been shown in [34] that if a data-generating distribution is faithful with respect to
G, then G must be a minimal-edge I-MAP. By enforcing the latter for every true graph,
Condition 4.1 imposes a milder requirement compared to the well-established faithful-
ness assumption [40]. Conditions 4.2-4.4 ensure that we avoid overﬁtting and provide
bounds for the noise variances. These are direct adaptations from Conditions 3.1-3.3 in
[45]. Condition 4.5 is required to bound the difference between the sample variances
of our observations and the true variances, and is related to Condition 3.4 in [45] but
adapted to our joint inference setting. Notice that Condition 4.5 is trivially satisﬁed
when p = O

n1/3
K7/3 log n

. Condition 4.6 follows from the bounds for sample vari-
ances shown in [4]. Intuitively, we are imposing the natural restriction that the number
of DAGs is small compared to the number of nodes p in each DAG and the total number
of observations n. Moreover, given that our objective is to draw estimation power from
the joint inference of multiple graphs, we require that each DAG is associated with a
non-vanishing fraction of the total observations.
Condition 4.7 enforces that, for every permutation π, the number of edges in the
union of all recovered graphs is proportional to the weighted sum of the edges in every

Y. Wang et al./Joint Estimation of Multiple DAGs
12
graph as K →∞. In particular, this requires the individual graphs G(k)
0π to be highly
overlapping. To see why this is the case, notice that PK
k=1 wk|G(k)
0π | is upper bounded
by the maximum number of edges across graphs G(k)
0π . Consequently, Condition 4.7
enforces the number of edges in the union of graphs to be proportional to the number
of edges in the single graph with most edges, thus requiring a high level of overlap. Im-
posing high overlap for all permutations π might seem too restrictive in some settings.
Nonetheless, Condition 4.7 can sometimes be derived from apparently less restrictive
conditions as the following example illustrates.
Consider the more relaxed bound |Gunion
0
| ≤ct
PK
k=1 wk|G(k)
0 |, which is equivalent
to requiring Condition 4.7 to hold but only for permutations consistent with the true
graph Gunion
0
. In the following example, we show that this might be sufﬁcient for Con-
dition 4.7 to hold. Suppose that Gunion
0
consists of two connected components G′union
0
and G′′union
0
respectively deﬁned on the subsets of nodes V1 and V2. Moreover, assume
that the subDAGs of G(k)
0
over V1 (denoted by G′(k)
0 ) are identical for all k. Putting it
differently, the differences between the DAGs G(k)
0
are limited to the second connected
component. In addition, assume that for all possible permutations π2 of nodes V2 we
have that |G′′union
0π2 | ≤|G′union
0
|. Then, for any permutation π, where we denote by π1
(respectively π2) the restriction of π to the node set V1 (respectively V2), we have
|Gunion
0π
| = |G′union
0π1 | + |G′′union
0π2 | ≤
K
X
k=1
wk|G′(k)
0π1| +
K
X
k=1
wk|G′(k)
0 | ≤2
K
X
k=1
wk|G(k)
0π |,
which shows that Condition 4.7 is satisﬁed for ct = 2. This example shows that learn-
ing the structure of large components that are common across the different DAGs is
not affected by the changes in the smaller components of these DAGs. Beyond this
example, in Section 6.2, we also provide simulation results to study the strength of
Condition 4.7 for sparse DAG models. Our simulation analysis shows that, when the
G(k)
0 ’s are highly overlapping (recall that this corresponds to a more relaxed scenario
than Condition 4.7 that requires high overlap across G(k)
0π ’s for all π’s), Condition 4.7 is
naturally satisﬁed with a reasonably small ct. Despite the above example as well as the
empirical analysis, Condition 4.7 might still be too restrictive for some applications;
we discuss a relaxed requirement and its implications on the consistency guarantees in
Section 4.3.
Condition 4.8 requires that, for every permutation π and every graph k, the value of
at least a ﬁxed proportion (1−η1) of the edges in G(k)
0π is above the ‘noise level’, i.e., the
lower bound within the indicator function in (9). Intuitively, if the true weight of many
edges is close to zero then correct inference of the graphs would be impossible since the
true edges would be mistaken with spurious ones. Thus, it is expected that the weights
of a sufﬁciently large fraction of the edges have to be sufﬁciently large. Condition 4.8 is
the right formalization of this intuition. Moreover, notice that a straightforward replica-
tion of the beta-min condition introduced in [45] would have required the ‘noise level’
to scale with
p
log p/nk, instead of the smaller scaling of
p
log p/n required in (9).
In this sense, Condition 4.8 (together with Condition 4.1) is a relaxed version of the
extension of the beta-min condition to the setting of joint graph estimation.

Y. Wang et al./Joint Estimation of Multiple DAGs
13
Remark 4.10 (Strength of assumptions). Requiring strong assumptions for consistent
estimation is a common theme in existing methods for causal inference. For exam-
ple, the PC algorithm requires the strong faithfulness assumption [20], which has been
shown to be a very restrictive assumption for high-dimensional causal graphical mod-
els [44]. For a discussion on the comparison between the strong faithfulness assumption
and the beta-min condition for estimating a single DAG model, see [45, Section 4.3.2].
In this context, the assumptions presented here are in line with or slight relaxations
(Conditions 4.1 and 4.8) of those in state-of-the-art approaches. While it would be
interesting in future work to formally compare Conditions 4.1 and 4.8 to strong faith-
fulness, our goal here is not to relax existing assumptions for the estimation of DAG
models, but to show that joint estimation can result in faster rates than separate esti-
mation of multiple DAGs under comparable assumptions.
4.3. Consistency under milder conditions
As previously discussed, in some settings Condition 4.7 might be too restrictive. Hence,
in this section we present a consistency statement akin to Theorem 4.9 that holds for a
milder version of Condition 4.7:
Condition 4.7’. Let ct(π) be some function of π that scales as a constant for permu-
tations consistent with Gunion
0
and scales as o(K) for all other permutations such that
|Gunion
0π
| ≤ct(π) PK
k=1 wk|G(k)
0π | for all π.
Observe that for permutations π consistent with the true union graph Gunion
0
, Condi-
tion 4.7’ boils down to the previously discussed Condition 4.7. However, for all other
permutations, ct(π) need not be a constant and is allowed to grow with K. Intuitively,
for all permutations not consistent with Gunion
0
we are not requiring a high level of over-
lap among all the graphs G(k)
0π . Nonetheless, since ct(π) = o(K) we do require Gunion
0π
to be ‘sparser’ than the extreme case in which all graphs G(k)
0π are disjoint.
In order to account for the fact that ct depends on the permutation π in Condi-
tion 4.7’, we have to modify Condition 4.8 accordingly, resulting in the following al-
ternative statement.
Condition 4.8’. Let Cmax := max
π
ct(π), then there exist constants η0 and η1 such that
0 ≤η1 < 1, 0 < η2
0 < (1 −η1), and
X
i,j
1
([A(k)
0π ]i,j
 >
p
Cmax log p/n
η0
q
p/|Gunion
0
| ∨1
)
≥(1 −η1)|G(k)
0π |,
for all permutations π and graphs k, where 1{·} denotes the indicator function.
The following consistency result holds for the alternative set of conditions.
Theorem 4.11. Under Conditions 4.1-4.6, 4.7’ and 4.8’ and with λ such that λ2 ≍
Cmax
log p
n

p
|Gunion
0
| ∨1

, then there exists a constant c > 0 that depends on cs such
that with probability 1 −exp(−cp), the solution to (7) satisﬁes that, at least for one

Y. Wang et al./Joint Estimation of Multiple DAGs
14
k ∈[K],
∥ˆA(k) −A(k)
0ˆπ ∥2
F + ∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F = O

λ2|G(k)
0 |

.
(13)
Furthermore, denoting by ˆG(k) the graph associated with ˆA(k) for the k ∈[K] satisfy-
ing (13), we have that
| ˆG(k)| ≍|G(k)
0ˆπ | ≍|G(k)
0 |.
(14)
The proof is given in Appendix A.3. Condition 4.7’ is milder than Condition 4.7 and
this relaxation entails a corresponding loss in the guarantees of recovery: Comparing
(13) and (14) with (10) and (11) immediately reveals that what could be guaranteed for
the ensemble of graphs in Theorem 4.9 can only be guaranteed for a single graph in
Theorem 4.11, thereby explaining the trade-off in relaxing the conditions.
However, the result in Theorem 4.11 still draws inference power from the joint
estimation of multiple graphs since neither (13) nor (14) can be shown using exist-
ing results for separate estimation. To be more precise, as discussed in Section 4.2,
when performing separate estimation, theoretical guarantees are based on the assump-
tion that at least a ﬁxed proportion of the edge weights are above the ‘noise level’,
which scales as
p
log p/nk. However, Condition 4.8’ requires the noise level to scale
with
p
Cmax log p/n which, given the fact that Cmax = o(K), is not large enough
to achieve the guarantee needed for separate estimation. In addition, the convergence
rate of Ω(Cmax
log p
n ) in (13) is still faster than the corresponding convergence rate
of Ω(K log p
n ) associated with separate estimation [cf. discussion after (12)]. A poten-
tial limitation of Theorem 4.11 is that, since one cannot know which of the K DAGs
achieves such
p
Cmax log p/n rate and which remains at
p
log p/nk, the result may be
of limited utility for practitioners. However, note that the much weaker Condition 4.7’
helps to illustrate that, even in such scenario, joint estimation can be helpful compared
with separate estimation. In addition, it also clariﬁes which guarantees are lost with re-
spect to the more stringent scenario when Condition 4.7 holds. In this sense, although
not fully interpretable, this intermediate case provides an idea of how the guarantees
degrade as we start to soften the assumptions.
We end this section with the following remark discussing the consistency guarantees
of jointGES.
Remark 4.12 (Consistency of jointGES). In the low-dimensional setting, by choosing
λ2
1 = PK
k=1 wk
log nk
2nk , assuming faithfulness and assuming that GES ﬁnds the global
optimum of (8), it can be inferred from [7] that, in the limit of large data, the ﬁrst step
in Algorithm 1 is guaranteed to produce a Markov equivalence class (MEC) ˆ
M that is
within the following set of MECs:
M∗:=

M : there exists a π ∈Π such that M = M(Gunion
0π
)
	
where
Π :=

π : ∀k ∈[K], G(k)
0π ∈M(G(k)
0 )
	
.

Y. Wang et al./Joint Estimation of Multiple DAGs
15
This allows us to recover {M(G(k)
0 )}K
k=1 by successively considering all DAGs in ˆ
M
as inputs to the second step of Algorithm 1, and selecting the DAG ˆGunion ∈ˆ
M whose
output { ˆG(k)}K
k=1 from step 2 is the sparsest. Then the MECs {M( ˆG(k))}K
k=1 produced
from step 2 asymptotically coincide with {M(G(k)
0 )}K
k=1. Note that no matter which
MEC is chosen from the set M∗, by doing edge reductions in step 2, the ﬁnal result is
always guaranteed to asymptotically converge to {M(G(k)
0 )}K
k=1. In Example 4.13, we
show how Algorithm 1 works for a particular 3-node instance. In the high-dimensional
setting, where even the global optimum of (7) is not guaranteed to recover the true
Gunion
0
(cf. Theorems 4.9 and 4.11), jointGES is in general not consistent. Recently,
Maathuis et al. [29] showed consistency of GES for single DAG estimation in the high-
dimensional setting under more restrictive assumptions than the ones considered here.
Although of potential interest, further strengthening the presented conditions to guar-
antee consistency of jointGES also in the high-dimensional setting is not pursued in
the current paper.
Example 4.13. We present an example to illustrate how the output from Algorithm 1
works in the low-dimensional regime. Consider the setting where we have data col-
lected from two causal DAG models on 3 nodes, namely 1 →2 and 2 ←3. In the ﬁrst
step, Algorithm 1 will produce either the PDAG 1 →2 ←3 or 1 −2 −3. Then, no
matter which of the two PDAGs is learned in the ﬁrst step, by taking it to Step 2, it is
guaranteed to asymptotically converge to the desired output 1 →2 (or 1 ←2) as well
as 2 →3 (or 2 ←3), of which the MECs are 1 −2 and 2 −3, respectively.
5. Extension to interventions
In this section, we show how our proposed method for joint estimation can be extended
to learn DAGs from interventional data. It is natural to consider learning from inter-
ventional data as a special case of joint estimation since the DAGs associated with
interventions are different but closely related. In this section, we mimic some of the
developments of Sections 3 and 4 but specialized for the case of interventional data.
More precisely, we ﬁrst propose an optimization problem akin to (7) and then state
the consistency guarantees in the high-dimensional setting of the associated optimal
solution.
Recall from Section 2.3 that the true adjacency matrix AIk
0 of the SEM associated
with an intervention on the nodes Ik is identical to the true adjacency matrix A0 of the
non-intervened model except that [AIk
0 ]ij = 0 for all j ∈Ik. In this way, our assump-
tion that there exists a common permutation π consistent with all DAGs under consid-
eration (cf. Section 2.3) is automatically satisﬁed for interventional data. Additionally,
assuming that we observe samples ˆXIk from K different models corresponding to the
respective intervention on the nodes in {Ik}K
k=1, the knowledge of the intervened nodes

Y. Wang et al./Joint Estimation of Multiple DAGs
16
can be incorporated into our optimization problem as follows [cf. (7)].
n
ˆπ, ˆA, ˆΩ, {( ˆAIk, ˆΩIk)}K
k=1
o
=
argmax
π,A,Ω,{(AIk ,ΩIk )}K
k=1
K
X
k=1
wkℓnk( ˆXIk; AIk, ΩIk) −λ2 ∥A∥0
(15a)
subject to
A ∈Aπ,
∥A∥∞,0 ≤d,
Ω∈D+,
(15b)
AIk
ij = Aij ∀j ̸∈Ik,
AIk
ij = 0 ∀j ∈Ik,
(15c)
ΩIk
jj = Ωjj ∀j ̸∈Ik,
ΩIk ∈D+.
(15d)
From the solution of (15) we obtain an estimate for the non-intervened SEM ( ˆA, ˆΩ) as
well as K estimates for the corresponding intervened models ( ˆAIk, ˆΩIk). The objective
in (15a) is equivalent to that in (7) where we leverage the fact that the union of all in-
tervened graphs results in the non-intervened one under the implicit assumption that no
single node has been intervened in every experiment. Alternatively, if some nodes were
intervened in all experiments, objective (15a) would still be valid since enforcing zeros
in the unobservable portions of A does not affect the recovery of the intervened adja-
cency matrices AIk. The constraints in (15b) impose that A has to be consistent with
permutation π and with bounded in-degree, and Ωhas to be a valid covariance ma-
trix for uncorrelated noise. Putting it differently, (15b) enforces for the non-intervened
SEM what we impose separately for all SEMs in (7). The constraints in (15c) impose
the known relations between the intervened and the non-intervened adjacency matrices.
Finally, (15d) constrains the matrices ΩIk to be consistent with the base model on the
non-intervened nodes while still being a valid covariance on the intervened ones.
Even though it might seem that in (15) we are estimating K + 1 SEMs (the base
case plus the K intervened ones), from the previous reasoning it follows that the effec-
tive number of optimization variables is signiﬁcantly smaller. To be more speciﬁc, for a
given π, once A is ﬁxed then all the adjacency matrices AIk are completely determined.
Moreover, for a ﬁxed Ω, the only freedom in ΩIk corresponds to the diagonal entries as-
sociated with intervened nodes in Ik. In this way, it is expected that for a given number
of samples, the joint estimation of K SEMs obtained from interventional data [cf. (15)]
outperforms the corresponding estimation from purely observational data [cf. (7)].
Recalling that we denote by (AIk
0ˆπ, ΩIk
0ˆπ) the parameters recovered from the Cholesky
decomposition of the true precision matrix ΘIk
0 under the assumption of consistency
with permutation ˆπ, the following result holds.
Corollary 5.1. If Conditions 4.1-4.8 hold and λ is chosen as λ2 ≍log p
n

p
|G0| ∨1

,
then there exist constants c1, c2 > 0 such that with probability 1 −c1 exp(−c2p), the
solution to (15) satisﬁes
K
X
k=1
wk∥ˆAIk −AIk
0ˆπ∥2
F = O(λ2|G0|).
(16)
Furthermore, denoting by ˆG the graph associated with the recovered adjacency matrix

Y. Wang et al./Joint Estimation of Multiple DAGs
17
ˆA for the non-intervened model, we have that
| ˆG| ≍|G0ˆπ| ≍|G0|.
(17)
The proof is given in Appendix A.4. A quick comparison of Theorem 4.9 and Corol-
lary 5.1 seems to indicate that the consistency guarantees of observational and inter-
ventional data are very similar. However, recovery from interventional data is strictly
better as we argue next. As discussed after Theorem 4.9, the results presented do not
guarantee that the permutation ˆπ recovered coincides with the true permutation of the
nodes. In principle, one could recover a spurious permutation ˆπ (different from the true
permutation π) that correctly explains the observed data [cf. (10) and (16)] and leads to
sparse graphs [cf. (11) and (17)]. However, the more interventions we have, the smaller
the set of spurious permutations ˆπ that can be recovered, as we illustrate in the follow-
ing example. Figure 1 portrays the existence of a spurious permutation that could be
recovered from observational data but cannot be recovered from interventional data.
More precisely, Figure 1(a) presents the two true DAGs that we aim to recover, where
the second one is obtained by intervening on node 2. By contrast, Figure 1(b) shows
the DAGs that are obtained when performing Cholesky decompositions on the true
precision matrices under the spurious permutation π1. Notice that the sparsity levels of
the DAGs in both ﬁgures are the same. In general, one could recover π1 instead of π0
from observational data, but one would never recover π1 from interventional data. To
see this, simply notice from the ﬁgure that [AI2
0π1]32 ̸= 0 whereas for the interventional
estimate [ ˆAI2]32 = 0 [cf. (15c)], thus, the error terms in (16) cannot vanish for ˆπ = π1.
This example also indicates that it is preferable to intervene on multiple targets in the
same experiment instead of doing interventions one at a time. This observation is in
accordance with new genetic perturbation techniques, such as Perturb-seq [9].
From a practical perspective, the objective in (15) corresponds to the same scoring
function as GIES [17]. Therefore, GIES can be used to obtain an approximate solution
to (15). A simulation study using GIES was performed in [17, Section 5.2] showing
that in line with the theoretical results obtained in this section, not only identiﬁability
increases, but also the estimates obtained using interventional data are better than with
the same amount of purely observational data.
1
2
3
AI1
0 :
I1 = ;
1
2
3
AI2
0 :
I2 = {2}
π0 = (1,2,3)
(a)
3
2
1
AI1
0π1 :
I1 = ;
3
2
1
AI2
0π1 :
I2 = {2}
π1 = (3,2,1)
(b)
FIG 1. Interventional data can avoid the recovery of spurious permutations. (a) True DAGs to be recovered.
(b) DAGs obtained from the Cholesky decomposition consistent with π1. The spurious permutation π1 does
not satisfy (16) for cases where node 2 is intervened.

Y. Wang et al./Joint Estimation of Multiple DAGs
18
6. Experiments
In this section, we present numerical experiments on both synthetic (Section 6.1) and
real (Section 6.3) data that support our theoretical ﬁndings. We also provide an em-
pirical analysis to study the strength of Condition 4.7 for sparse DAG models in Sec-
tion 6.2.
6.1. Performance evaluation of joint causal inference
We analyze the performance of the joint recovery of K different DAGs where we vary
K ∈{3, 5, 8} and n ∈{600, 900, 1200}. For all experiments, we set the number of
nodes p = 100. In addition, we selected the number of samples from each DAG to be
the same, i.e., n1 = . . . = nK = n/K. For each experiment, the true DAGs were con-
structed in two steps. First, we generated a core graph that is shared among the K DAGs
under consideration. We did this by generating a random graph from an Erd˝os-R´enyi
model with 100 edges in expectation, and then oriented the edges according to a ran-
dom permutation of the nodes. Then we sampled eprivate ∈{30, 60} additional private
edges uniformly at random. Each such edge was assigned uniformly at random to one
of the K DAGs, thereby keeping the total number of private edges across all K DAGs
(a) Average SHD, n = 600
(b) Average SHD, n = 900
(c) Average SHD, n = 1200
(d) Average ROC, n = 600
(e) Average ROC, n = 900
(f) Average ROC, n = 1200
FIG 2. Simulation results when we set the private-to-core edge ratio to 0.3. (a) - (c) Average SHD as a
function of the scaling constant c for joint and separate GES with n = 600, 900, 1200 respectively; (d)
- (f) Average ROC curve obtained by varying the tuning parameters with n = 600, 900, 1200. JointGES
consistently achieves a better performance across all settings.

Y. Wang et al./Joint Estimation of Multiple DAGs
19
(a) Average SHD, n = 600
(b) Average SHD, n = 900
(c) Average SHD, n = 1200
(d) Average ROC, n = 600
(e) Average ROC, n = 900
(f) Average ROC, n = 1200
FIG 3. Simulation results when we set the private-to-core edge ratio to 0.6. (a) - (c) Average SHD as a
function of the scaling constant c for joint and separate GES with n = 600, 900, 1200 respectively; (d)
- (f) Average ROC curve obtained by varying the tuning parameters with n = 600, 900, 1200. JointGES
consistently achieves a better performance across all settings.
to be eprivate. This procedure results in the generation of a collection of true underlying
DAGs G(1)
0 , . . . , G(K)
0
with a private-to-core edge ratio of 0.3 and 0.6 respectively. As-
sociated with each DAG, we generated a true adjacency matrix A(k)
0
and a true diagonal
error covariance matrix Ω(k)
0 . For the latter, we drew each error variance independently
and uniformly from the interval [1, 2.25]. Regarding the adjacency matrices, we drew
the edge weights independently and uniformly from [−1, −0.1]∪[0.1, 1] to ensure that
they are bounded away from zero. Notice that we did not put any constraints on the
edge weights that are in the shared core structure for different DAGs: the same edge
can change its weight in different DAGs, or even ﬂip sign.
We randomly generated 100 collections of DAGs and data associated with them. We
then estimated the DAGs from the data via two different methods: a joint estimation
procedure using jointGES presented in Algorithm 1 and a separate estimation proce-
dure using the well-established GES method [7].
To assess performance of the two algorithms, we considered two standard mea-
sures, namely the structural Hamming distance (SHD) [43] and the receiver operating
characteristic (ROC) curve. SHD is a commonly used metric based on the number of
operations needed to transform the estimated DAG into the true one [20, 43]. Hence,
a smaller SHD value indicates better performance. The ROC curve plots the true pos-
itive rate against the false positive rate for different choices of tuning parameters. The

Y. Wang et al./Joint Estimation of Multiple DAGs
20
results are shown in Figures 2 and 3. Notice that for plotting SHD, we selected the
ℓ0-penalization parameter λ2
1 = c log p
n
with scaling constant c ∈{1, 2, 3, 4, 5} in both
joint and separate estimation and then plotted average SHD as a function of the scal-
ing constant c averaged over the K DAGs to be recovered and the 100 realizations.
The penalization parameter λ2 in the second step of the joint estimation procedure was
chosen based on 10-fold cross validation. We plotted the average ROC curve where for
each choice of tuning parameter, we computed the true positive and false positive rates
by averaging over the K DAGs to be recovered and the 100 realizations. It is clear
from the two ﬁgures that in general joint inference achieves better performance, which
matches our theoretical results in Section 4.
However, Figures 2 (a)-(c) and 3 (a)-(c) show also that jointGES performs worse
than separate estimation for small scaling constants (c = 1). Note that this is in line
with our theoretical ﬁndings in Theorem 4.11, which imply that whenever Condi-
tion 4.7 – which sometimes is a restrictive assumption – is violated, we need to choose
a larger penalization parameter.
6.2. Simulation analysis of Condition 4.7
In this section we provide simulation results to empirically study the strength of Con-
dition 4.7. We follow the same procedure as in Section 6.1 to generate a collection of
DAGs, except that we set p ∈{10, 30, 50} and K ∈{3, 5, 8, 10, 13, 15}. Then for each
randomly generated {G(k)
0 }K
k=1, we randomly select 10000 permutations and estimate
the corresponding ct by ct := maxπ∈Π |Gunion
0π
|/
  1
K
PK
k=1 |G(k)
0π |

.
In Figure 4 we present the estimated value of ct as a function of K for p = 10, 30, 50.
Note that for each setting, we plotted the estimated ct by averaging over 100 realiza-
tions. The ﬁgure shows that if the private-to-core edge ratio is below 0.6, Condition 4.7
(a) private-to-core edge ratio 0.3
(b) private-to-core edge ratio 0.6
FIG 4. Averaged ct across 100 realizations. (a) is the curve when the private-to-core edge ratio is chosen as
0.3; (b) corresponds to the curve with private-to-core edge ratio of 0.6.

Y. Wang et al./Joint Estimation of Multiple DAGs
21
holds with ct ≤3, even for very large K = 15. This implies that when the true DAGs
{G(k)
0 }K
k=1 are highly overlapping, Condition 4.7 can usually be satisﬁed with a rea-
sonably small constant ct. This is in line with the results from Section 6.1, where we
showed that jointGES signiﬁcantly outperforms the separate estimation approaches.
6.3. Gene regulatory networks of ovarian cancer subtypes
To assess the performance of the proposed joint ℓ0-penalized maximum likelihood
method on real data, we analyzed gene expression microarray data of patients with
ovarian cancer [42]. Patients were divided into six subtypes of ovarian cancer, labeled
as C1-C6, where C1 is characterized by signiﬁcant differential expression of genes
associated with stromal and immune cell types and with a lower survival rate as com-
pared to the other 5 subtypes. We hence grouped the subtypes C2-C6 together and
our goal was to infer the differences in terms of gene regulatory networks in ovarian
cancer that could explain the different survival rates. The gene expression data in [42]
includes the expression proﬁle of n = 83 patients with C1 subtype and n = 168 pa-
tients with other subtypes. We implemented our jointGES algorithm (Algorithm 1) to
jointly learn two gene regulatory networks: one corresponding to the C1 subtype GC1
and another corresponding to the other ﬁve subtypes together GC2−6. In addition, as
in [4], we focused on a particular pathway, namely the apoptosis pathway. Using the
KEGG database [21, 30] we selected the genes in this pathway that were associated
with at most two microarray probes, resulting in a total of p = 76 genes.
Table 1 lists the number of edges discovered by jointGES as well as for two separate
estimation methods, namely using the GES [7] and PC [40] algorithms. All three meth-
ods were combined with stability selection [27] in order to increase robustness of the
output and provide a fair comparison. As expected, the two graphs inferred using joint-
GES share a signiﬁcant proportion of edges, whereas the overlap is markedly smaller
for the two separate estimation methods. Interestingly, under all estimation methods
the network for the C1 subtype contains fewer edges than the network of the other sub-
types, thereby suggesting that GC1 could lack some important links that are associated
with patient survival.
To obtain more insights into the relevance of the obtained networks, we analyzed
the inferred hub nodes in the three networks. For our analysis we deﬁned as hub nodes
those nodes for which the sum of the in- and out-degree was larger than some threshold
T in the union of the two DAGs, where T was chosen such that there are at most 5 hub
nodes discovered by each method. For jointGES, this union is given by ˆGunion, the graph
identiﬁed in the ﬁrst step of Algorithm 1. The hub nodes identiﬁed by jointGES are
CAPN1, CTSD, LMNB1, CSF2RB, BIRC3. Among these, CAPN1 [13], CTSD [41],
LMNB1 [37], and BIRC3 [19] have been reported as being central to ovarian cancer
in the existing literature. In addition, CSF2RB was also discovered by joint estimation
of undirected graphical models on this data set [4]. The hub nodes discovered by GES
are ATF4, BIRC2, CSF2RB, TUBA1C, MAPK3, while PC only discovered the hub
node CSF2RB. While we were not able to validate the relevance of any of these genes
for ovarian cancer in the literature, interestingly, CSF2RB has been identiﬁed as a hub
node by all methods, thereby suggesting this gene as an interesting candidate for future

Y. Wang et al./Joint Estimation of Multiple DAGs
22
Method
|GC1|
|GC2−C6|
|GC1 ∩GC2−C6|
JointGES
50
73
48
GES
68
101
32
PC
14
30
9
TABLE 1
Number of edges in the DAGs estimated by different methods for the gene regulatory network of subtype C1
(|GC1|) and all other subtypes (|GC2−6|). The last column shows the number of edges shared between both
inferred graphs.
genetic intervention experiments.
7. Discussion
In this paper we presented jointGES, an algorithm for the joint estimation of multiple
related DAG models from independent realizations. Joint estimation is of particular
interest in applications where data is collected not from a single DAG, but rather mul-
tiple related DAGs, such as gene expression data from different tissues, cell types or
from different interventional experiments. JointGES ﬁrst estimates the union of DAGs
ˆGunion by applying a greedy search to approximate the joint ℓ0-penalized maximum
likelihood estimator and then it uses variable selection to discover each DAG as a sub-
DAG of ˆGunion. From an algorithmic perspective, jointGES is to the best of our knowl-
edge the ﬁrst method to jointly estimate related DAG models in the high-dimensional
setting. From a theoretical perspective, we presented consistency guarantees on the
joint ℓ0-penalized maximum likelihood estimator, and showed that the accuracy bound
scales with the total number of samples, rather than the number of samples from each
DAG that would be achieved by separately estimating each DAG. As a corollary to
this result, we obtained consistency guarantees for ℓ0-penalized maximum likelihood
estimation of a causal graph from a mix of observational and interventional data. Fi-
nally, we validated our results via numerical experiments on simulated and real-world
data, showing that the proposed jointGES algorithm for joint inference outperforms
separate-inference approaches using well-established algorithms such as PC and GES.
The present work serves as a platform for the potential development of multiple
future directions. These directions include: i) relaxing the assumption that all DAGs
must be consistent with the same underlying permutation; ii) extending jointGES to
the setting where the samples come from K related DAGs but it is unknown a priori
which particular DAG each sample comes from; this is for example of interest in the
analysis of gene expression data from tumors or tissues that consist of a mix of cell
types; iii) extending jointGES to allow for latent confounders.
Acknowledgments
The authors thank two anonymous reviewers for their thoughtful comments, which
helped improve our paper. Santiago Segarra was supported by an MIT IDSS seed grant.
Caroline Uhler was partially supported by NSF (DMS-1651995), ONR (N00014-17-
1-2147 and N00014-18-1-2765), IBM, a Sloan Fellowship and a Simons Investigator
Award.

Y. Wang et al./Joint Estimation of Multiple DAGs
23
Appendix A: Theoretical analysis of statistical consistency results
In the following, we develop the proofs of Theorems 4.9 and 4.11. To facilitate under-
standing, we ﬁrst provide a high-level explanation of the rationale behind the proof. If
we have data generated from a single DAG and we are given a permutation π consistent
with the true DAG a priori, then we can estimate ( ˆA, ˆΩ) by performing p regressions as
explained in Section 2.1. By contrast, when the permutation is unknown and we need to
consider all the possible permutations, the total number of regressions to run increases
to p · p!. However, these regressions are not independent and, intuitively, by bounding
the noise level of a subset of these regressions, we can derive bounds for the noise on
the other ones. We characterize the ‘noise level’ of these regressions by analyzing the
asymptotic properties of three random events. More precisely, whenever these events
hold – and we show that they hold with high probability –, the noise is small enough so
that the error of the ℓ0-penalized maximum likelihood estimator can also be bounded.
Finally, we use this upper bound in the error to show that the recovered graph converges
to a minimal I-MAP that is as sparse as the true DAG.
The remainder of the appendix is organized as follows. In Section A.1 we deﬁne
the three random events previously mentioned and show that each of them holds with
high probability. Section A.2 then leverages the deﬁnition of these events to prove
Theorem 4.9, our main result. In Section A.3 we prove Theorem 4.11, which relaxes
some of the conditions of Theorem 4.9, but uses similar proof techniques. Finally,
Section A.4 ﬂeshes out the proof of Corollary 5.1, our result applicable to the setting
for interventional data.
Throughout the appendix, we use the following notation. Let ˆaj denote the j-th
column of ˆA and ˆωj denote the j-th diagonal entry of ˆΩ. Also, denote by a0jπ and
ω0jπ the j-th column of A0π and the j-th diagonal entry of Ω0π, respectively.
A.1. Random events
As in [45], our proofs of Theorems 4.9 and 4.11 are based on a set of random events.
However, the events considered here differ from those in [45] since, as explained in
Section 4.1, a naive application of the guarantees in [45] to the joint estimation scenario
does not achieve the desired learning rates [cf. discussion after (12)]. Intuitively, the
rate gain achieved here comes from the assumption that all DAGs are consistent with a
permutation, allowing us to effectively reduce the size of the search space.
In our proofs we consider three random events E1, E2, and E3 that are respectively
stated – along with proofs showing that they hold with high probability – in Sections
A.1.1, A.1.2, and A.1.3.
A.1.1. Random event E1
Let ϵ(k)
jπ ∈Rn denote the residual when regressing X(k)
j
on X(k)
S
with S = {i | π(i) <
π(j)}, i.e., ϵ(k)
jπ := X(k)
j
−X(k)T a(k)
0jπ. Similarly, let ˆϵ(k)
jπ ∈Rn denote the regression
residual from the sampled data, i.e., ˆϵ(k)
jπ := ˆX(k)
j
−ˆX(k)a(k)
0jπ. Consider a generic

Y. Wang et al./Joint Estimation of Multiple DAGs
24
set {A(k)}K
k=1 of adjacency matrices consistent with a given permutation π, where the
columns of A(k) are denoted by A(k) := (a(k)
1 , . . . , a(k)
p ), and let Gunion denote the
union of the support of A(1), . . . , A(K). Then, event E1 is deﬁned as
E1 :=
(
2
p
X
j=1
K
X
k=1
wk
nk
ˆϵ(k)T
jπ
ˆX(k)(a(k)
j
−a(k)
0jπ)

≤δ1
p
X
j=1
K
X
k=1
wk
nk
 ˆX(k)(a(k)
j
−a(k)
0jπ)

2
2 + λ2
1
 |Gunion| + |Gunion
0π
|

/δ1,
∀permutations π , and ∀{A(k)}K
k=1 consistent with π
o
,
(18)
for some constant δ1 > 0 and some λ1 ≍
p
(log p)/n. On random event E1 a uniform
inequality holds across all K DAGs for the sample correlation between the regression
residual ϵ(k)
jπ and any random variable spanned by the random vector X(k)
S , i.e., X(k)T v
for any v ∈Rp with vi = 0 for all i ̸∈S. Notice that for convenience for further steps
of the analysis, this generic vector v is written as a(k)
j
−a(k)
0jπ in (18). Furthermore, for
simplicity in the rest of this appendix, we denominate the space spanned by X(k)
S
as the
projection space of ϵ(k)
jπ . Intuitively, one could foresee E1 holding since the expected
correlation between the regression residual ϵ(k)
jπ and XS is equal to zero, and therefore
the sample correlation can be upper bounded by a sum of terms that converge to zero
as n →∞as in (18).
We now show that random event E1 holds with high probability, a result stated in
Theorem A.2. Essential towards proving this result is the observation that, since the
random variable X(k)T (a(k)
j
−a(k)
0jπ) lies within the projection space of ϵ(k)
jπ , these two
random variables are independent. We can therefore deal with the randomness in ˆϵ(k)
jπ
and ˆX(k)
S
separately, one at a time. To formally leverage this observation, we rely on
Lemma 7.4 of [45], stated next for completeness.
Lemma A.1. [45, Lemma 7.4] Let Z be a ﬁxed n × m matrix and e1, · · · , en be
independent N(0, σ2
e)-distributed random variables. Then for all t > 0
P
 
sup
∥Za∥2
2/n≤1
|eT Za|/n ≥σe(
p
2m/n +
p
2t/n)
!
≤exp(−t).
Based on the above lemma and recalling that Aπ denotes the set of adjacency ma-
trices consistent with a given permutation π, we can show the following result.
Theorem A.2. Assume that Conditions 4.2 and 4.6 hold, then for all t > 0 and all

Y. Wang et al./Joint Estimation of Multiple DAGs
25
δ1 > 0,
P
 
max
π
sup
{A(k)}K
k=1∈Aπ
2
p
X
j=1
K
X
k=1
wk
nk
ˆϵ(k)T
jπ
ˆX(k)(a(k)
j
−a(k)
0jπ)

−δ1
p
X
j=1
K
X
k=1
wk
nk
 ˆX(k)(a(k)
j
−a(k)
0jπ)

2
2
≥16σ2
0(t + 2 log p)(|Gunion| + |Gunion
0π
|)
nδ1

≤exp(−t).
Proof. Let ˆϵjπ and a0jπ be the concatenated vectors ˆϵjπ := (ˆϵ(1)T
jπ
, . . . , ˆϵ(K)T
jπ
)T and
a0jπ := (a(1)T
0jπ , . . . , a(K)T
0jπ )T . Also, deﬁne the block diagonal matrix
ˆX := diag( ˆX(1), · · · , ˆX(K)).
We denote by Ajπ ⊂RpK the set containing all vectors that can be formed by vertically
concatenating the jth columns a(k)
j
for all k and satisfy
Ajπ :=
n
aj ∈RpK | ∀i, if ∃k such that a(k)
i,j ̸= 0, then X(k)
i
⊥⊥ϵ(k)
jπ for all k
o
.
Based on this, and recalling that Paj(·) denotes the set of parent nodes of j in the
argument graph, we deﬁne the random event Bjπ as
Bjπ :=
(
∃aj ∈Ajπ :
sup
∥ˆ
X(aj−a0jπ)∥2
2/n≤1
ˆϵT
jπ ˆX(aj −a0jπ)
 /n
(19)
≥σ0
 r
2K(|Paj(Gunion)| + |Paj(Gunion
0π
)|)
n
+
r
2(t + |Paj(Gunion
0π
)| log p + 2 log p)
n
!)
.
Combining the facts that: i) aj−a0jπ may have at most K(|Paj(Gunion)|+|Paj(Gunion
0π
)|)
non-zero entries, and ii) the variance of each element of ˆϵjπ is upper bounded by σ2
0
(cf. Condition 4.2), we may apply Lemma A.1 to show that
P(Bjπ) ≤exp
 −t −|Paj(Gunion
0π
)| log p −2 log p

.
As can be seen from (19), event Bjπ depends exclusively on the set of parents of node
j in Gunion
0π
. Putting it differently, if for two permutations π1 and π2 node j has the same
set of parents in Gunion
0π1
and Gunion
0π2 , then the random events Bjπ1 and Bjπ2 coincide,
since Ajπ, a0jπ and ˆϵjπ would all be the same for π ∈{π1, π2}. If we denote by
Πj(m) the set of permutations where node j has exactly m parents in Gunion
0π
, then there
are at most
  p
m

unique events Bjπ for all π ∈Πj(m). We therefore obtain that
P


[
π∈Πj(m)
Bjπ

≤
 p
m

exp (−t −m log p −2 log p) ≤exp (−t −2 log p) .

Y. Wang et al./Joint Estimation of Multiple DAGs
26
Applying a union bound on the events Bjπ across all nodes j and permutations π yields
that
P

[
j
[
π
Bjπ

≤
p
X
j=1
p−1
X
m=1
P


[
π∈Πj(m)
Bjπ

≤p2 exp (−t −2 log p) = exp(−t).
(20)
Combining (20) and (19) it follows that with probability at least 1 −exp(−t), for all j,
π and all aj ∈Ajπ,
|ˆϵT
jπ ˆX(aj −a0jπ)|
∥ˆX(aj −a0jπ)∥2
≤σ0
q
2K(|Paj(Gunion)| + |Paj(Gunion
0π
)|)
+
q
2(t + |Paj(Gunion
0π
)| log p + 2 log p)

.
Based on the collection of adjacency matrices {A(k)}K
k=1 ∈Aπ we deﬁne another
collection {A′(k)}K
k=1 where each column a′(k)
j
is given by
a′(k)
j
=
(
a(k)
j
if ˆϵ(k)T
jπ
ˆX(k)(a(k)
j
−a(k)
0jπ) ≥0,
2a(k)
0jπ −a(k)
j
otherwise.
Notice that the positions of the non-zero entries in a′(k)
j
−a(k)
0jπ coincide with those in
a(k)
j
−a(k)
0jπ. By also using the fact that
∥ˆX(aj −a0jπ)∥2
2 =
K
X
k=1
∥ˆX(k)(a(k)
j
−a(k)
0jπ)∥2
2
=
K
X
k=1
∥ˆX(k)(a′(k)
j
−a(k)
0jπ)∥2
2 = ∥ˆX(a′
j −a0jπ)∥2
2,
we have that for all j and π with probability at least 1 −exp(−t), it holds that
PK
k=1 |ˆϵ(k)T
jπ
ˆX(k)(a(k)
j
−a(k)
0jπ)|
∥ˆX(aj −a0jπ)∥2
=
PK
k=1 |ˆϵ(k)T
jπ
X(k)(a′(k)
j
−a(k)
0jπ)|
∥ˆX(a′
j −a0jπ)∥2
= |ˆϵT
jπ ˆX(a′
j −a0jπ)|
∥ˆX(a′
j −a0jπ)∥2
≤σ0
q
2K(|Paj(Gunion)| + |Paj(Gunion
0π
)|)
+
q
2(t + |Paj(Gunion
0π
)| log p + 2 log p)

.
In the above expression we may use that ab ≤δ1a2 +b2/δ1 for all δ1, a, b > 0 in order

Y. Wang et al./Joint Estimation of Multiple DAGs
27
to obtain
2|ˆϵT
jπ ˆX(a′
j −a0jπ)| ≤δ1∥ˆX(a′
j −a0jπ)∥2
2 +
(21)
4σ2
0
q
2K(|Paj(Gunion)| + |Paj(Gunion
0π
)|)
+
q
2(t + |Paj(Gunion
0π
)| log p + 2 log p)
2
/δ1.
By combining this with the fact that ∀a, b > 0, (a + b)2 ≤2(a2 + b2) and the fact that
K = o(log p) from Condition 4.6, we get that
q
2K(|Paj(Gunion)| + |Paj(Gunion
0π
)|) +
q
2(t + |Paj(Gunion
0π
)| log p + 2 log p)
2
≤4(t + 2 log p)(|Paj(Gunion)| + |Paj(Gunion
0π
)|).
(22)
By replacing (22) back into (21), we obtain that
2|ˆϵT
jπ ˆX(a′
j −a0jπ)|/n ≤δ1∥ˆX(a′
j −a0jπ)∥2
2/n
+ 16σ2
0(t + 2 log p)(|Paj(Gunion)| + |Paj(Gunion
0π
)|)
nδ1
.
Rewriting the absolute value in the left-hand side as the sum of the corresponding K
absolute values and adding the above inequality for j = 1, . . . , p we get that, with
probability at least 1 −exp(−t),
2
p
X
j=1
K
X
k=1
wk|ˆϵ(k)T
jπ
ˆX(k)(a(k)
j
−a0
(k)
jπ )|/nk
≤
p
X
j=1

δ1∥ˆX(aj −a0jπ)∥2
2/n+ 16σ2
0(t+2 log p)(|Paj(Gunion)|+|Paj(Gunion
0π
)|)
nδ1

.
Finally, by noticing that ∥ˆX(aj −a0jπ)∥2
2/n = PK
k=1 wk∥ˆX(k)(a(k)
j
−a(k)
0jπ)∥2
2/nk
we recover the statement of the theorem, thereby concluding the proof.
A.1.2. Random event E2
Let ω(k)
0jπ denote the j-th diagonal entry of Ω(k)
0π , then E2 holds whenever the empirical
variances of all ϵ(k)
jπ , i.e., ∥ˆϵ(k)
jπ ∥2
2/nk are close to the true variances ω(k)
0jπ, where
E2 :=



p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω(k)
0jπ
ω(k)
0jπ
!2
≤4 λ2
2
 p + |Gunion
0π
|



,
(23)
for some λ2 ≍
p
(log p)/n. Mimicking the development for event E1, we now show
that E2 also holds with high probability. This result is stated in Theorem A.4. Similar

Y. Wang et al./Joint Estimation of Multiple DAGs
28
to the proof of Theorem A.2, we ﬁrst consider the asymptotic property for a particular
node j and permutation π, and then leverage this to get a uniform bound across all
permutations and nodes. For this proof, we use the following lemma stated in [4],
which also follows from [47]. After the lemma, we formally state our result.
Lemma A.3. [4, Lemma 2] Suppose X1, · · · , Xn are K-dimensional random vectors
satisfying EXi = 0 and ∥Xi∥2 ≤M for 1 ≤i ≤n. We have for any β > 0 and x > β
P(∥Pn
i=1 Xi∥2 ≥x) ≤P

∥N∥2 ≥(x −β)/λ1/2
max

+ c1K5/2 exp(−c2K−5/2β/M),
where λmax is the largest eigenvalue of Cov(Pn
i=1 Xi), N is a K-dimensional stan-
dard normal random vector and c1, c2 are positive constants.
Theorem A.4. Assume Conditions 4.5 and 4.6 hold, then there exist constants c1, c2, c3 >
0 such that
P
 
∃π :
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω(k)
0jπ
ω(k)
0jπ
!2
≥c1
csp log p + |Gunion
0π
| log p
n
!
≤c2 exp(−c3 log p),
where cs is the constant deﬁned in Condition 4.5.
Proof. We begin by analyzing the asymptotic properties of ϵ(k)
jπ for all k given a ﬁxed
permutation π and node j. More speciﬁcally, consider the following random event
Cjπ :=



K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω(k)
0jπ
ω(k)
0jπ
!2
≥c2
1
log p (|Paj(Gunion
0π
)| + cs)
n


,
(24)
for some positive constant c1. Following the proof of Theorem 1 in [4], we deﬁne u(k)
t
as follows:
u(k)
t
:=



√wk
nk

[ˆϵ(k)
jπ ]
2
t −ω(k)
0jπ
ω(k)
0jπ

if t ≤nk,
0
otherwise.
Denoting by ut = (u(1)
t , · · · , u(K)
t
)T the random vector collecting all u(k)
t
, by deﬁni-
tion it follows that
K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω(k)
0jπ
ω(k)
0jπ
!2
=

n
X
t=1
ut

2
2
.
A straightforward substitution in (24) allows us to rewrite the event Cjπ as
Cjπ :=
(
n
X
t=1
ut

2
≥c1λn
)
with
λ2
n := log p (|Paj(Gunion
0π
)| + cs)
n
.
(25)

Y. Wang et al./Joint Estimation of Multiple DAGs
29
Notice that in order to apply Lemma A.3 to bound the probability of occurrence of Cjπ,
we would need ∥ut∥2 to be smaller than a constant M, which is not true in general.
Hence, we split Cjπ into two subevents, enabling the utilization of Lemma A.3. More
precisely, whenever the following random event holds
Fa :=
n
|u(k)
t
| ≤λ−1
n K1/2−a/n,
∀t, k
o
,
then the ℓ2 norm of ut is bounded as follows:
∥ut∥2 ≤Ma := λ−1
n K1−a/n,
where a is a free parameter that will be ﬁxed later in the proof. In detail, we bound the
probability of Cjπ according to
P(Cjπ) ≤P(Cjπ|Fa)P(Fa) + P(¬Fa),
(26)
where we use Lemma A.3 to bound P(Cjπ|Fa) and where P(¬Fa) can be estimated
from the chi-squared tail bound [14].
We ﬁrst focus on bounding P(Cjπ|Fa). For this, we introduce a new variable ˜ut
obtained by truncating the tail of ut. Formally, recalling that 1{·} denotes the indicator
function, we have that ˜ut := (˜u(1)
t , · · · , ˜u(K)
t
) where
˜u(k)
t
:= u(k)
t
1
n
|u(k)
t
| ≤λ−1
n K1/2−a/n
o
−E
h
u(k)
t
1
n
|u(k)
t
| ≤λ−1
n K1/2−a/n
oi
.
Notice that whenever the random event Fa holds, then ut and ˜ut follow the same
distribution except for a shift v(k)
t
:= E
h
u(k)
t
1
n
|u(k)
t
| ≤λ−1
n K1/2−a/n
oi
. Putting it
differently, the distribution of u(k)
t
−˜u(k)
t
is a constant v(k)
t
whenever we are on the
random event Fa. This implies that ∥Pn
t=1 ut∥2 ≤n maxt,k |v(k)
t
| + ∥Pn
t=1 ˜ut∥2.
Therefore, if we can guarantee that n|v(k)
t
| = o(1)λn for all t and k, then there must
exist a constant 0 < δ < 1 such that if we are on the random event Fa, ∥Pn
t=1 ut∥2 ≥
c1λn implies that ∥Pn
t=1 ˜ut∥2 ≥(1 −δ)c1λn. Equivalently, we may write
P(Cjπ|Fa)P(Fa) ≤P
 
n
X
t=1
˜ut

2
≥(1 −δ)c1λn
Fa
!
P(Fa)
(27)
≤P
 
n
X
t=1
˜ut

2
≥(1 −δ)c1λn
!
,
where the second inequality follows from Bayes’ theorem, and we can bound the last
term by applying Lemma A.3 since ˜ut is bounded by deﬁnition. We now show that,
indeed, n|v(k)
t
| = o(1)λn for all t and k. From the cumulative tail bound of a chi-
squared random variable with one degree of freedom we have that P(u(k)
t
≥l) ≤
exp(−ηnl/
√
K) for some constant η > 0. Based on this, we can estimate the scale of
v(k)
t
with respect to p and n as
|v(k)
t
|=
E
h
u(k)
t
1
n
|u(k)
t
| ≤λ−1
n K1/2−a/n
oi
=
E
h
u(k)
t
1
n
|u(k)
t
| > λ−1
n K1/2−a/n
oi ≤exp
 −η′λ−1
n K−a

Y. Wang et al./Joint Estimation of Multiple DAGs
30
for some 0 < η′ < η, where the second equality follows from the fact that u(k)
t
has zero mean. Notice that in the last inequality, u(k)
t
has been absorbed into the
exponential term. As |v(k)
t
| decays exponentially with respect to λ−1
n , we have that
n|v(k)
t
| = o(1)λn. Having justiﬁed this, we may now apply Lemma A.3 to the right-
most term in (27) in order to bound P(Cjπ|Fa)P(Fa). From the deﬁnition of ˜ut it
follows that λmax{Cov(Pn
t=1 ˜ut)} ≤λmax{Cov(Pn
t=1 ut)}. Furthermore, since the
variables u(k)
t
are independently distributed for all t, we have that
var
 n
X
t=1
u(k)
t
!
= wk
nk
var

[ˆϵ(k)
jπ ]
2
t −ω(k)
0jπ
ω(k)
0jπ

≤c2/n
for some constant c2 > 0. This also implies that λmax{Cov(Pn
t=1 ˜ut)} ≤c2/n. Ap-
plying Lemma A.3, where we select x = (1 −δ)c1λn, β = δ′x for some arbitrary
positive constant 0 < δ′ < 1 and, M = λ−1
n K1−a/n, it follows that
P
 
n
X
t=1
˜ut

2
≥(1 −δ)c1λn
!
≤exp(−c3(nλ2
n −K))
+ exp
2
5 log K −c4Ka−7
2 nλ2
n

,
for some constants c3, c4 > 0 that increase if constant c1 is increased. In addition, by
choosing a = 7/2, there must exist a large enough c1 such that c3, c4 > 1 and therefore
P
 
n
X
t=1
˜ut

2
≥(1 −δ)c1λn
!
(28)
≤exp(−nλ2
n) = exp(−|Paj(Gunion)| log p −cs log p).
Replacing (28) into (27) gives us the sought exponential bound for the ﬁrst summand
in (26).
We are now left with the task of ﬁnding a bound for P(¬Fa). By relying on the fact
that n(1) ≍· · · ≍n(K) (cf. Condition 4.6), we get that (maxk nk) K ≤c5n for some
constant c5 and therefore
P(¬Fa) ≤c5n
max
1≤k≤K,1≤l≤nP
n
|u(k)
t
| ≥λ−1
n K1/2−a/n
o
.
Following this, in order to bound the probability that ¬Fa holds we further rely on the
tail bound of the chi-squared random variable u(k)
t
to obtain
P(¬Fa) ≤c5n exp
 −ηλ−1
n K−a
= c5 exp
 log n −ηλ−1
n K−a
.
Recalling the deﬁnition of λn from (25), Condition 4.5 implies that
λ−1
n
K7/2 ≥log p(|Paj(Gunion)| + cs)/˜α
3
2
and
√
˜α λ−1
n
K7/2 ≥log n.
(29)

Y. Wang et al./Joint Estimation of Multiple DAGs
31
By recalling that we have ﬁxed a = 7
2, it follows that there exists a constant η′ such
that
P(¬Fa) ≤c5 exp
 log n −ηλ−1
n K−a
≤c5 exp
√
˜αλ−1
n K−a −ηλ−1
n K−a
≤c5 exp
 −η′λ−1
n K−a
,
where we have used the second inequality in (29). Furthermore, by leveraging the ﬁrst
inequality in (29) we obtain that
P(¬Fa) ≤c5 exp
 −η′λ−1
n K−a
≤c5 exp

−η′ log p (|Paj(Gunion)| + cs)/˜α
3
2

,
thus obtaining an exponential bound for the second summand in (26).
Having found exponential bounds for both summands in (26), it follows that for
c1 > 0 sufﬁciently large and ˜α sufﬁciently small we have that
P(Cjπ) ≤(1 + c5) exp(−|Paj(Gunion)| log p −cs log p).
Following an argument based on union bounds similar to the one presented in the proof
of Theorem A.2, we have that
P


K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω(k)
0jπ
ω(k)
0jπ
!2
≤c1
log p (|Paj(Gunion)| + cs)
n
,
∀j, π


≥1 −P


p[
j=1
p[
m=1
[
π∈Πj(m)
Cjπ

≥1 −(1 + c5) exp(−(cs −2) log p)
for some constant c1 > 0. It is immediately implied from the previous expression that
P

∃π :
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jπ ∥2
2/nk −ω0
(k)
jπ
ω0
(k)
jπ
!2
≥c1
csp log p + |Gunion
0π
| log p
n


≤(1 + c5) exp(−(cs −2) log p),
thus recovering the statement of the theorem (since cs > 2 by Assumption 4.5) after
accordingly renaming the constants on the right-hand side.
A.1.3. Random event E3
Event E3 is deﬁned as the intersection of 2K events that we denote by {E(k)
3a }K
k=1 and
{E(k)
3b }K
k=1, where events E(k)
3a and E(k)
3b are speciﬁc to the kth SEM. More speciﬁcally,
event E(k)
3a ensures that all the estimated noise variances ˆω(k)
j
associated with the kth
SEM are ﬁnite and bounded away from zero. Formally, we deﬁne the following events
for k = 1, . . . , K:
E(k)
3a :=
n
min

ˆω(k)
j
, 1/ˆω(k)
j

≥1/β2,
for j = 1, . . . , p
o
,
(30)

Y. Wang et al./Joint Estimation of Multiple DAGs
32
for some β > 0. Event E(k)
3b imposes a universal lower bound on the norm achievable
by any linear combinations of the data associated with the k-th DAG. Mathematically,
we consider the ensuing events for k = 1, . . . , K:
E(k)
3b :=
n
∥ˆX(k)v∥2/√nk ≥

δ3 −λ(k)
3
p
∥v∥0

∥v∥2,
∀v ∈Rpo
,
(31)
for some δ3 > 0 and λ(k)
3
≍
p
(log p)/nk. Based on (30) and (31) we deﬁne events
E(k)
3
:= E(k)
3a ∩E(k)
3b , and
E3 :=
K
\
k=1
E(k)
3
.
(32)
Leveraging the fact that Condition 4.4 enforces the maximum in-degree of each G(k)
0π
to be at most αnk/ log p for some positive constant α, we can generalize Lemmas 7.5
and 7.7 from [45] into the following lemma.
Lemma A.5. [45, Lemmas 7.5 and 7.7] Assume Conditions 4.2, 4.3, 4.4 and 4.5 hold
and that
3
p
Λmin/4 −
r
2(t + log p)
n
−3σ0
√
α + ˜α ≥1/β > 0,
for some t > 0. Based on this, deﬁne
λ(k)
3
:= 3σ0
r
log p
nk
,
and
δ3 := 3
p
Λmin/4 −
s
2(t + log p)
nk
.
Then P(E(k)
3
) ≥1 −5 exp(−t) and on E(k)
3
it holds that
∥ˆX(k)(a(k)
j
−a(k)
0jˆπ)∥2/√nk ≥∥a(k)
j
−a(k)
0jˆπ∥2/β2.
(33)
Lemma A.5 shows that under certain conditions the events E(k)
3
hold with high prob-
ability, thus playing a role analogous to that of Theorem A.2 for event E1 and Theo-
rem A.4 for event E2.
With the events E1, E2, and E3 deﬁned and having shown under which conditions
these hold with high probability, in the next section we leverage these events to prove
Theorem 4.9.
A.2. Proof of Theorem 4.9
A.2.1. Bounds on new probability space
Through direct manipulation of the likelihood function, in Lemma A.6 we show that
the global optimum ( ˆA(k), ˆΩ(k))K
k=1 converges to the SEMs (A(k)
0ˆπ , Ω(k)
0ˆπ )K
k=1, where ˆπ
is some permutation consistent with the estimated adjacency matrices ˆA(1), · · · , ˆA(K).

Y. Wang et al./Joint Estimation of Multiple DAGs
33
Lemma A.6. Assume we are on E1 ∩E2 ∩E3 and Condition 4.2 holds. Consider a
regularizer in (7) satisfying λ2 > λ2
1/δ1 + λ2
2/δ2 with 0 < δ1 < 1/β2 and 0 < δ2 <
1/(2β2σ2
0). Then,
n
ˆπ, {( ˆA(k), ˆΩ(k))}K
k=1
o
the global optimum of (7), satisﬁes
 1
β2 −δ1

p
X
j=1
K
X
k=1
wk∥X(k)(ˆa(k)
j
−a(k)
0jˆπ)∥2
2/nk
(34)
+

1
2β4σ4
0
−δ2

p
X
j=1
K
X
k=1
wk
 
ˆω(k)
j
−ω0
(k)
j
ˆω(k)
j
!2
+

λ2 −λ2
1
δ1
−λ2
2
δ2

| ˆG| ≤λ2|Gunion
0
| + λ2
2(p + |Gunion
0ˆπ
|)
δ2
+ λ2
1|Gunion
0ˆπ
|
δ1
.
Proof. By deﬁnition, the global optimum must satisfy
K
X
k=1
wkℓnk( ˆX(k); ˆA(k), ˆΩ(k)) −λ2| ˆG| ≥
K
X
k=1
wkℓnk( ˆX(k); A(k)
0 , Ω(k)
0 ) −λ2|Gunion
0
|.
(35)
Let ˆπ denote any permutation consistent with all ˆA(k). Since the value of the likelihood
ℓnk( ˆX(k); A(k)
0 , Ω(k)
0 ) is completely determined by the precision matrices {Θ(k)
0 }K
k=1,
it then follows that the likelihood function ℓnk( ˆX(k); A(k)
0 , Ω(k)
0 ) and the function
ℓnk( ˆX(k); A(k)
0ˆπ , Ω(k)
0ˆπ ) achieve the same value. We therefore replace the former by the
latter in (35) and expand the deﬁnition of the likelihood function in (5) to obtain
p +
p
X
j=1
K
X
k=1
wk log ˆω(k)
j
+ λ2| ˆG|
≤
p
X
j=1
K
X
k=1
wk
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
+
p
X
j=1
K
X
k=1
wk log ω(k)
0jˆπ + λ2|Gunion
0
|.
Basic manipulations transform the above expression into the following inequality
p
X
j=1
K
X
k=1
wk log
 
ˆω(k)
j
ω(k)
0jˆπ
!
+ λ2| ˆG| ≤
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
+ λ2|Gunion
0
|.
(36)
Since we are on E3, we have that 1/ˆω(k)
j
≤β2 [cf. (30)]. By combining this with Con-
dition 4.2 we can further bound ω(k)
0jˆπ/ˆω(k)
j
≤β2σ2
0. Then, using the Taylor expansion
log(1 + x) ≤x −x2/(2(1 + t)2), for −1 < x ≤t, we can further replace log

ˆω(k)
j
ω(k)
0j ˆπ


Y. Wang et al./Joint Estimation of Multiple DAGs
34
in (36) to obtain
p
X
j=1
K
X
k=1
wk
 
ˆω(k)
j
−ω(k)
0jˆπ
ˆω(k)
j
!
+
1
2β4σ4
0
 
ω(k)
0jˆπ
ˆω(k)
j
−1
!2
+ λ2| ˆG|
≤
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
+ λ2|Gunion
0
|.
(37)
Finally, using the fact that ˆX(k)
j
= ˆϵ(k)
jˆπ + ˆX(k)a(k)
0jˆπ, we also rewrite ˆω(k)
j
as
ˆω(k)
j
= ∥ˆX(k)
j
−ˆX(k)ˆa(k)
j ∥2
2/nk
= ∥ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)∥2
2/nk −2ˆϵ(k)T
jˆπ
ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)/nk + ∥ˆϵ(k)
j ∥2
2/nk.
By replacing the above into (37), we get that
p
X
j=1
K
X
k=1
wk
∥ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)∥2
2/nk
ˆω(k)
j
+
1
2β4σ4
0
p
X
j=1
K
X
k=1
wk
 
ˆω(k)
j
−ω(k)
0jˆπ
ˆω(k)
j
!2
+λ2| ˆG|
≤2
p
X
j=1
K
X
k=1
wk
ˆϵ(k)T
jˆπ
ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)/nk
ˆω(k)
j
+
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
−
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ˆω(k)
j
!
+ λ2|Gunion
0
|.
(38)
In order to further bound the expression in (38), notice that the ﬁrst summand in the
right hand side of the inequality corresponds to the sum of all empirical correlation
coefﬁcients. Leveraging that we are under the assumption that E1 holds [cf. (18)], we
have that
2
p
X
j=1
K
X
k=1
wk
ˆϵ(k)T
jˆπ
ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)/nk
ˆω(k)
j
≤δ1
p
X
j=1
K
X
k=1
wk∥ˆX(k)(ˆa(k)
j
−a(k)
0jˆπ)∥2
2/nk + λ2
1
δ1
| ˆG|.
(39)
In order to bound the second and third terms, we ﬁrst restate their difference as follows
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ˆω(k)
j
!
−wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
=
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ω(k)
0jˆπ
! 
ω(k)
0jˆπ −ˆω(k)
j
ˆω(k)
j
!
.
(40)

Y. Wang et al./Joint Estimation of Multiple DAGs
35
Next, by using Cauchy-Schwarz inequality, i.e.
|
n
X
i=1
uivi|2 ≤
n
X
j=1
|uj|2
n
X
k=1
|vk|2,
we further bound (40) as

p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ˆω(k)
j
!
−
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
≤


p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ω(k)
0jˆπ
!2

1/2 

p
X
j=1
K
X
k=1
wk
 
ω(k)
0jˆπ −ˆω(k)
j
ˆω(k)
j
!2

1/2
.
(41)
From the fact that event E2 holds [cf. (23)], we can upper bound the ﬁrst of the two
factors in the right-hand side of (41) by 2
p
λ2
2(p + |Gunion
0
(ˆπ)|). Further, relying on the
inequality 2ab ≤a2/δ2 + δ2b2 for any δ2 > 0, it follows that

p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk −ω(k)
0jˆπ
ˆω(k)
j
!
−
p
X
j=1
K
X
k=1
wk
 
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
−1
!
(42)
≤λ2
2(p + |Gunion
0
(ˆπ)|)
δ2
+ δ2
p
X
j=1
K
X
k=1
wk
 
ω(k)
0jˆπ −ˆω(k)
j
ˆω(k)
j
!2
.
By replacing (39) and (42) into (38), we recover (34), as we wanted to show.
From Lemma A.6 it follows that the global optimum of (7) corresponds to a min-
imal I-MAP, but no claim is made about the sparsity level of this I-MAP. In order to
show that the solution is indeed sparse, we must rely on Conditions 4.7 and 4.8. In
Lemma A.7 we show that |Gunion
0ˆπ
| cannot be much larger than | ˆG|. Then, in Thm. A.8
we further show how to cancel out |Gunion
0ˆπ
| with | ˆG| in (34) to obtain our main result.
Lemma A.7. Assume Condition 4.7 holds and let ˜λ > 0 be such that
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F ≤˜λ2|Gunion
0ˆπ
|.
(43)
Consider constants η1, η2 with 0 ≤η1 < 1 and 0 < η2
2ct < 1 −η1 such that
P
i,j 1
n[A(k)
0ˆπ ]i,j
 ≥˜λ/η2
o
≥(1 −η1)|G(k)
0ˆπ |. Then, it follows that
| ˆG| ≥1 −η1 −η2
2ct
ct
|Gunion
0ˆπ
|.
Proof. Let N (k) and M(k) be the sets of entries satisfying
N (k) := {(i, j) : |[A(k)
0ˆπ ]i,j| ≥˜λ/η2}

Y. Wang et al./Joint Estimation of Multiple DAGs
36
and
M(k) := {(i, j) : |[ ˆA(k)]i,j −[A(k)
0ˆπ ]i,j| ≥˜λ/η2}.
From these deﬁnitions it follows that
K
X
k=1
wk|N (k) ∩M(k)|
˜λ2
η2
2
≤
K
X
k=1
wk
X
(i,j) ∈N (k)∩M(k)
|[ ˆA(k)]i,j −[A(k)
0ˆπ ]i,j|2
≤
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F .
Leveraging inequality (43) and Condition 4.7 we further have that
K
X
k=1
wk|N (k) ∩M(k)| ≤η2
2|Gunion
0ˆπ
| ≤η2
2ct
K
X
k=1
wk|G(k)
0ˆπ |.
(44)
Notice that for all (i, j)-th entries in the set N (k)∩M(k)C it must be that |[ ˆA(k)]i,j| > 0.
Hence, N (k) ∩M(k)C corresponds to a subset of non-zero entries of ˆA(k), which in
turn corresponds to a subset of edges in ˆG. From this we can infer that
| ˆG|=
K
X
k=1
wk| ˆG|≥
K
X
k=1
wk|N (k)∩M(k)C|=
K
X
k=1
wk(|N (k)| −|N (k) ∩M(k)|)
≥(1 −η1 −η2
2ct)
K
X
k=1
wk|G(k)
0ˆπ |,
where the last inequality follows by combining (44) with the deﬁnition of η1 in the
statement of the lemma. The proof concludes by replacing Condition 4.7 in the above
inequality.
Theorem A.8. Assume Conditions 4.1, 4.7 and 4.8 hold, and suppose that there ex-
ist constants δB and 0 < δs < 1 as well as λ and λ0 that scale as λ2 ≍λ2
0 ≍
log p
n (p/|Gunion
0
| ∨1) such that
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F + λ2δs| ˆG| ≤λ2|Gunion
0
| + λ2
0|Gunion
0ˆπ
|.
(45)
If the constant η0 in Condition 4.8 is sufﬁciently small, then there exist constants
δ′
s, cg, c′
g > 0 such that
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F + (λ2δs −λ2
0δ′
s)| ˆG| ≤λ2|Gunion
0
|
(46)
and
| ˆG| ≥cg|Gunion
0ˆπ
| ≥c′
g|Gunion
0
|.
(47)

Y. Wang et al./Joint Estimation of Multiple DAGs
37
Proof. Using Conditions 4.1 and 4.7, we have that |Gunion
0
| ≤ct max
k
|G(k)
0 | ≤ct |Gunion
0ˆπ
|.
Suppose that for some ˜λ > 0 one has that ˜λ2 ≍λ2 ≍λ2
0 and ˜λ2δB ≥λ2ct + λ2
0, then
it follows from (45) that
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F ≤λ2|Gunion
0
| + λ2
0|Gunion
0ˆπ
| ≤˜λ2δB|Gunion
0ˆπ
|.
Let η2 be a constant deﬁned as η2 := η0˜λ/
q
log p
n (p/|Gunion
0
| ∨1), then we can rewrite
Condition 4.8 as
X
i,j
1
n[A(k)
0ˆπ ]i,j
 ≥˜λ/η2
o
≥(1 −η1)|G(k)
0ˆπ |.
Moreover, for η0 sufﬁciently small, η2 is also guaranteed to satisfy 0 < η2
2ct < 1 −η1.
We could therefore apply Lemma A.7 and get that | ˆG| ≥
1−η1−η2
2ct
ct
|Gunion
0ˆπ
|, which
completes the proof of (47) by choosing cg = 1−η1−η2
2ct
ct
and c′
g = cg · ct. Notice that
in order to apply Lemma A.7, it is required that η2
2ct < 1 −η1, which is guaranteed by
the assumption that η0 is sufﬁciently small. Leveraging the ﬁrst inequality in (47), we
can replace λ2
0|Gunion
0ˆπ
| in (45) by
λ2
0ct
1−η1−η2
2ct | ˆG| in order to obtain
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F +

λ2δs −
λ2
0ct
1 −η1 −η2
2ct

| ˆG| ≤λ2|Gunion
0
|.
(48)
Notice that (48) coincides with the sought expression (46) upon substituting δ′
s =
ct/(1 −η1 −ctη2
2).
A.2.2. Proof of Theorem 4.9
It follows from Theorem A.2, Theorem A.4, and Lemma A.5 that there exist constants
λ1, λ2 with λ2
1 ≍λ2
2 ≍log p
n
as well as some λ(k)
3
2 ≍log p
nk
for all k such that with
probability 1 −exp(−c log p) for some constant c > 0, the random event E1 ∩E2 ∩E3
occurs.
We may then apply Lemma A.6 to show that there exist constants δB, δW such that
with high probability, for any λ > 0 satisfying λ2 > λ2
1/δ1 + λ2
2/δ2, it holds that
δBβ2
p
X
j=1
K
X
k=1
wk∥X(k)(ˆa(k)
j
−a(k)
0jˆπ)∥2
2/nk+δW
K
X
k=1
wk∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F + λ2δs| ˆG|
≤λ2|Gunion
0
| + λ2
2(p + |Gunion
0ˆπ
|)
δ2
+ λ2
1|Gunion
0ˆπ
|
δ1
.
(49)
Note that compared with Lemma A.6, we have replaced Pp
j=1

ˆω(k)
j
−ω0
(k)
j
ˆω(k)
j
2
by
∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F . This follows from combining the facts that Pp
j=1(ˆω(k)
j
−ω0
(k)
j )2

Y. Wang et al./Joint Estimation of Multiple DAGs
38
is equal to ∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F and that
1
ˆω(k)
j
is bigger than 1/β2 on the random event E3.
In addition, the constants β, σ0, δ1 and δ2 in Lemma A.6 have been absorbed into the
new constants δB and δW . We also replaced λ2−λ2
1
δ1 −λ2
2
δ2 by λ2δs for some 0 < δs < 1.
By applying Lemma A.5 [cf. (33)] we may bound the ﬁrst summand on the left-
hand side of (49) by δB
PK
k=1 wk∥ˆA(k) −A(k)
0ˆπ ∥2
F . Furthermore, replacing λ1 and λ2
by some λ0 that scales as λ2
0 ≍log p
n (p/|Gunion
0
| ∨1), we obtain that
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F +δW
K
X
k=1
wk∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F + λ2δs| ˆG|
≤λ2|Gunion
0
| + λ2
0|Gunion
0ˆπ
|.
(50)
By applying (50), we have that for a constant η0 small enough, (46) and (47) in The-
orem A.8 hold by choosing λ such that λ2 ≍log p
n (p/|Gunion
0
| ∨1) and λ2δs > λ2
0δ′
s.
Moreover, from (46) we further infer that
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F + λ2δ′′
s | ˆG| ≤λ2|Gunion
0
|,
(51)
where the constant δ′′
s is chosen such that λ2δ′′
s = λ2δs −λ2
0δ′
s in (46). From (51)
it can thus be inferred that | ˆG| ≤|Gunion
0
|/δ′′
s . Combining this with (47) and the fact
that |Gunion
0ˆπ
| ≥c′
g/cg|Gunion
0
|, we recover the ﬁrst part of (11) in the statement of the
theorem, i.e., | ˆG| ≍|Gunion
0ˆπ
|. For the relation between |Gunion
0ˆπ
| and |Gunion
0
|, we use that
|Gunion
0ˆπ
| ≤| ˆG|/cg ≤|Gunion
0
|/(δ′′
s · cg) and |Gunion
0ˆπ
| ≥c′
g/cg|Gunion
0
|. Finally, to recover
(10) we combine (50) with (11), which concludes the proof.
A.3. Proof of Theorem 4.11
We ﬁrst introduce a lemma that will be instrumental in proving Theorem 4.11 and that
can be obtained directly from Lemmas 7.2 and 7.3 in [45].
Lemma A.9. [45, Lemmas 7.2 and 7.3] Suppose for some δB, δs, λ0, λ > 0 one has
that δB∥ˆA(k) −A(k)
0ˆπ ∥2
F + λ2δs| ˆG(k)| ≤λ2|G(k)
0 | + λ2
0|G(k)
0ˆπ |. Let ˜λ2δB ≥λ2 + λ2
0 and
assume that P
i,j 1
n[A(k)
0ˆπ ]i,j
 ≥˜λ/η2
o
≥(1 −η1)|G(k)
0ˆπ |. Then
δB∥ˆA(k) −A(k)
0ˆπ ∥2
F +

λ2δs −
λ2
0
1 −η1 −η2
2

| ˆG(k)| ≤λ2|G(k)
0 |
and
| ˆG(k)| ≥(1 −η1 −η2
2)|G(k)
0ˆπ | ≥(1 −η1 −η2
2)|G(k)
0 |.
In order to show Theorem 4.11, we begin the proof just like for Theorem 4.9 in
Section A.2.2 until we get to expression (50). It then follows from Condition 4.7’ and

Y. Wang et al./Joint Estimation of Multiple DAGs
39
| ˆG| ≥PK
k=1 wk| ˆG(k)| that there exists some λ′2
0 ≍Cmax
log p
n (p/|Gunion
0
| ∨1), where
Cmax is deﬁned in Condition 4.8, such that for any λ > 0,
δB
K
X
k=1
wk∥ˆA(k) −A(k)
0ˆπ ∥2
F + δW
K
X
k=1
wk∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F + λ2δs
K
X
k=1
wk| ˆG(k)|
≤λ2ct(π0)
K
X
k=1
wk|G(k)
0 | + λ′2
0
K
X
k=1
wk|G(k)
0ˆπ |.
Let λ′2 := λ2 · ct(π0) and δ′
s := δs/ct(π0), it then follows that there must exist at least
one k such that for any λ′ > 0,
δB∥ˆA(k) −A(k)
0ˆπ ∥2
F + δW ∥ˆΩ(k) −Ω(k)
0ˆπ ∥2
F + λ′2δ′
s| ˆG(k)| ≤λ′2|G(k)
0 | + λ′2
0 |G(k)
0ˆπ |
Since according to Condition 4.7’, ct(π) scales as a constant for permutations consis-
tent with Gunion
0
, we have that δ′
s is still a constant and λ′ ≍λ. In this case, it follows
from Lemma A.9 and Condition 4.8’ that there exists some constant 0 < δs < 1
and δ′
s > 0 such that by choosing λ′ such that λ′ ≍Cmax
log p
n (p/|Gunion
0
| ∨1) and
λ′2δs > λ′2
0 δ′
s, it holds that
δB∥ˆA(k) −A(k)
0ˆπ ∥2
F +
 λ′2δs −λ′2
0 δ′
s

| ˆG(k)| ≤λ′2|G(k)
0 |.
It also follows from Lemma A.9 that | ˆG(k)| ≥cg|G(k)
0ˆπ | ≥cg|G(k)
0 | for some positive
constant cg. Mimicking the arguments employed in the proof of Theorem 4.9 from (51)
until the end of the proof, one can show that expressions (13) and (14) in the statement
of Theorem 4.11 hold true, which completes the proof.
A.4. Proof of Corollary 5.1
The following lemma is instrumental in proving the corollary.
Lemma A.10. [18, Lemma 4] Given ﬁxed G, the maximum likelihood estimator in (15)
can be written as
p +
p
X
j=1
 
min
a∈R|Paj (G)|
n−j
n
log
 X
k:j̸∈Ik
nk
n−j
∥ˆX(k)
j
−ˆX(k)
Paj(G) · a∥2
2/nk
!
+
X
k:j∈Ik
wk log

∥ˆX(k)
j
∥2
2/nk
 !
where n−j is the total number of samples where node j is not intervened on, i.e.,
n−j = P
k:j̸∈Ik nk.
Recall that in the interventional setting Gunion
0
is given by the true graph G0 of the
non-intervened model, and that the K models (A(k)
0 , Ω(k)
0 ) to be inferred correspond
to the interventional models (AIk
0 , ΩIk
0 ). Denoting by (ˆπ, ˆA, ˆΩ) the (non-intervened)

Y. Wang et al./Joint Estimation of Multiple DAGs
40
global optimum of (15), let ˆω(k)
j
denote the empirical variance of the random variable
X(k)
j
−X(k)ˆaj if j ∈Ik and the empirical variance of X(k)
j
otherwise. It follows from
Lemma A.10 that the global optimum satisﬁes
p +
p
X
j=1

n−j
n
log

X
k:j̸∈Ik
nk
n−j
ˆω(k)
j

+
X
k:j∈Ik
wk log ˆω(k)
j

+ λ2| ˆG|
≤
p
X
j=1
K
X
k=1
wk
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
+
p
X
j=1
K
X
k=1
wk log ω(k)
0jˆπ + λ2|Gunion
0
|.
Then, applying the inequality log(PK
k=1 wkak) ≥PK
k=1 wk log ak for any choices
of a1, . . . , aK > 0 and w1, . . . , wK > 0 with PK
k=1 wk = 1, we obtain
p +
p
X
j=1
K
X
k=1
wk log ˆω(k)
j
+ λ2| ˆG| ≤
p
X
j=1
K
X
k=1
wk
∥ˆϵ(k)
jˆπ ∥2
2/nk
ω(k)
0jˆπ
+
p
X
j=1
K
X
k=1
wk log ω(k)
0jˆπ + λ2|Gunion
0
|.
Hence Corollary 5.1 directly follows from the proof of Theorem 4.9.
References
[1] P. A. Aguilera, A. Fern´andez, R. Fern´andez, R. Rum´ı, and A. Salmer´on. Bayesian
networks in environmental modelling.
Environmental Modelling & Software,
26(12):1376–1388, 2011.
[2] S. A. Andersson, D. Madigan, and M. D. Perlman. A characterization of Markov
equivalence classes for acyclic digraphs. The Annals of Statistics, 25(2):505–541,
1997.
[3] M. N. Arbeitman, E. EM. Furlong, F. Imam, E. Johnson, B. H. Null, B. S. Baker,
M. A. Krasnow, M. P. Scott, R. W. Davis, and K. P. White. Gene expression during
the life cycle of Drosophila melanogaster. Science, 297(5590):2270–2275, 2002.
[4] T. T. Cai, H. Li, W. Liu, and J. Xie. Joint estimation of multiple high-dimensional
precision matrices. Statistica Sinica, 26(2):445, 2016.
[5] T. T. Cai, W. Liu, and X. Luo. A constrained ℓ1 minimization approach to sparse
precision matrix estimation.
Journal of the American Statistical Association,
106(494):594–607, 2011.
[6] D. M. Chickering. Learning Bayesian networks is NP-complete. In Proceedings
of the Fifth International Workshop on Artiﬁcial Intelligence and Statistics, 1995.
[7] D. M. Chickering. Optimal structure identiﬁcation with greedy search. Journal
of Machine Learning Research, 3(Nov):507–554, 2002.
[8] P. Danaher, P. Wang, and D. M. Witten. The joint graphical lasso for inverse
covariance estimation across multiple classes. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 76(2):373–397, 2014.

Y. Wang et al./Joint Estimation of Multiple DAGs
41
[9] A. Dixit, O. Parnas, B. Li, J. Chen, C. P. Fulco, L. Jerby-Arnon, N. D. Mar-
janovic, D. Dionne, T. Burks, R. Raychowdhury, B. Adamson, T. M. Norman,
E. S. Lander, J. S. Weissman, N. Friedman, and A. Regev. Perturb-seq: Dissect-
ing molecular circuits with scalable single-cell RNA proﬁling of pooled genetic
screens. Cell, 167(7):1853–1866, 2016.
[10] F. Eberhardt, C. Glymour, and R. Scheines. On the number of experiments suf-
ﬁcient and in the worst case necessary to identify all causal relations among n
variables. In Proceedings of the Twenty-First Conference on Uncertainty in Arti-
ﬁcial Intelligence, pages 178–184. AUAI Press, 2005.
[11] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation
with the graphical lasso. Biostatistics, 9(3):432–441, 2008.
[12] N. Friedman, M. Linial, I. Nachman, and D. Peter. Using Bayesian networks
to analyze expression data. Journal of Computational Biology, 7(3-4):601–620,
2000.
[13] D. M. Gau, J. L. Lesnock, B. L. Hood, R. Bhargava, M. Sun, K. Darcy, S. Luthra,
U. Chandran, T. P. Conrads, R. P. Edwards, J. L. Kelley, T. C. Krivak, and
P. Roy. BRCA1 deﬁciency in ovarian cancer is associated with alteration in ex-
pression of several key regulators of cell motility–a proteomics study. Cell Cycle,
14(12):1884–1892, 2015.
[14] B. George. Probability inequalities for the sum of independent random variables.
Journal of the American Statistical Association, 57(297):33–45, 1962.
[15] C. Glymour, R. Scheines, P. Spirtes, and K. Kelly. Discovering Causal Strucure.
Academic Press, 1987.
[16] J. Guo, E. Levina, G. Michailidis, and J. Zhu. Joint estimation of multiple graph-
ical models. Biometrika, 98(1):1–15, 2011.
[17] A. Hauser and P. B¨uhlmann. Characterization and greedy learning of interven-
tional Markov equivalence classes of directed acyclic graphs. Journal of Machine
Learning Research, 13(Aug):2409–2464, 2012.
[18] A. Hauser and P. B¨uhlmann. Jointly interventional and observational data: Esti-
mation of interventional Markov equivalence classes of directed acyclic graphs.
Journal of the Royal Statistical Society: Series B (Statistical Methodology),
77(1):291–318, 2015.
[19] J. J¨onsson, K. Bartuma, M. Dominguez-Valentin, K. Harbst, Z. Ketabi, S. Malan-
der, M. J¨onsson, A. Carneiro, A. M˚asb¨ack, G. J¨onsson, and M. Nilbert. Distinct
gene expression proﬁles in ovarian cancer linked to Lynch syndrome. Familial
Cancer, 13:537–545, 2014.
[20] M. Kalisch and P. B¨uhlmann.
Estimating high-dimensional directed acyclic
graphs with the PC-algorithm.
Journal of Machine Learning Research,
8(Mar):613–636, 2007.
[21] M. Kanehisa, S. Goto, Y. Sato, M. Furumichi, and T. Mao. KEGG for integration
and interpretation of large-scale molecular data sets. Nucleic Acids Research,
40(D1):D109–D114, 2011.
[22] M. Kolar, L. Song, A. Ahmed, and E. P. Xing. Estimating time-varying networks.
The Annals of Applied Statistics, 4(1):94–123, 2010.
[23] S. L. Lauritzen. Graphical Models, volume 17. Clarendon Press, 1996.
[24] P. Loh and P. B¨uhlmann. High-dimensional learning of linear causal networks

Y. Wang et al./Joint Estimation of Multiple DAGs
42
via inverse covariance estimation.
Journal of Machine Learning Research,
15(1):3065–3105, 2014.
[25] E. Z. Macosko, A. Basu, R. Satija, J. Nemesh, K. Shekhar, M. Goldman, I. Tirosh,
A. R. Bialas, N. Kamitaki, E. M. Martersteck, J. J. Trombetta, D. A. Weitz,
J. R. Sanes, A. K. Shalek, A. Regev, and S. A. McCarroll.
Highly parallel
genome-wide expression proﬁling of individual cells using nanoliter droplets.
Cell, 161(5):1202–1214, 2015.
[26] N. Meinshausen and P. B¨uhlmann. High-dimensional graphs and variable selec-
tion with the lasso. The Annals of Statistics, 34(3):1436–1462, 2006.
[27] N. Meinshausen and P. B¨uhlmann. Stability selection. Journal of the Royal Sta-
tistical Society: Series B (Statistical Methodology), 72(4):417–473, 2010.
[28] K. Mohan, P. London, M. Fazel, D. Witten, and S. Lee. Node-based learning of
multiple Gaussian graphical models. The Journal of Machine Learning Research,
15(1):445–488, 2014.
[29] P. Nandy, A. Hauser, and M. H. Maathuis. High-dimensional consistency in score-
based and hybrid structure learning. The Annals of Statistics, 46(6A):3151–3183,
2018.
[30] H. Ogata, S. Goto, K. Sato, W. Fujibuchi, H. Bono, and M. Kanehisa. KEGG:
Kyoto encyclopedia of genes and genomes. Nucleic Acids Research, 27(1):29–
34, 1999.
[31] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge University
Press, 2000.
[32] J. Pearl and T. S. Verma. Equivalence and synthesis of causal models. In Proceed-
ings of Sixth Conference on Uncertainty in Artijicial Intelligence, pages 220–227,
1991.
[33] C. Peterson, F. C. Stingo, and M. Vannucci.
Bayesian inference of multiple
Gaussian graphical models.
Journal of the American Statistical Association,
110(509):159–174, 2015.
[34] G. Raskutti and C. Uhler. Learning directed acyclic graphs based on sparsest
permutations. Stat, 7:e183, 2018.
[35] P. Ravikumar, M. J. Wainwright, G. Raskutti, and B. Yu.
High-dimensional
covariance estimation by minimizing ℓ1-penalized log-determinant divergence.
Electronic Journal of Statistics, 5:935–980, 2011.
[36] J. M. Robins, M. A. Hernan, and B. Brumback. Marginal structural models and
causal inference in epidemiology. Epidemiology, 11(5):550–560, 2000.
[37] A. D. Santin, F. Zhan, S. Bellone, M. Palmieri, S. Cane, E. Bignotti, S. An-
fossi, M. Gokden, D. Dunn, J. J. Roman, T. J. O’Brien, E. Tian, M. J. Cannon,
J. Shaughnessy, and S. Pecorelli. Gene expression proﬁles in primary ovarian
serous papillary tumors and normal ovarian epithelium: identiﬁcation of candi-
date molecular markers for ovarian cancer diagnosis and therapy. International
Journal of Cancer, 112(1):14–25, 2004.
[38] A. K. Shalek, R. Satija, J. Shuga, J. J. Trombetta, D. Gennert, D. Lu, P. Chen, R. S.
Gertner, J. T. Gaublomme, N. Yosef, S. Schwartz, B. Fowler, S. Weaver, J. Wang,
X. Wang, R. Ding, R. Raychowdhury, N. Friedman, N. Hacohen, H. Park, A. P.
May, and A. Regev. Single cell RNA Seq reveals dynamic paracrine control of
cellular variation. Nature, 510(7505):363, 2014.

Y. Wang et al./Joint Estimation of Multiple DAGs
43
[39] L. Song, M. Kolar, and E. P. Xing. Keller: estimating time-varying interactions
between genes. Bioinformatics, 25(12):i128–i136, 2009.
[40] P. Spirtes, C. Glymour, and R. Scheines. Causation, Prediction and Search. MIT
Press, 2000.
[41] E. A. Stronach, G. C. Sellar, C. Blenkiron, G. J. Rabiasz, K. J. Taylor, E. P. Miller,
C. E. Massie, A. Al-Nafussi, J. F. Smyth, D. J. Porteous, and H. Gabra. Identi-
ﬁcation of clinically relevant genes on chromosome 11 in a functional model of
ovarian cancer tumor suppression. Cancer Research, 63(24):8648–8655, 2003.
[42] R. W. Tothill, A. V. Tinker, J. George, R. Brown, S. B. Fox, S. Lade, D. S. John-
son, M. K. Trivett, D. Etemadmoghadam, B. Locandro, N. Traﬁcante, S. Fereday,
J. A. Hung, Y. Chiew, I. Haviv, Australian Ovarian Cancer Study Group, D. Ger-
tig, A. deFazio, and D. D.L. Bowtell. Novel molecular subtypes of serous and en-
dometrioid ovarian cancer linked to clinical outcome. Clinical Cancer Research,
14(16):5198–5208, 2008.
[43] I. Tsamardinos, L. E. Brown, and C. F. Aliferis.
The max-min hill-climbing
Bayesian network structure learning algorithm. Machine learning, 65(1):31–78,
2006.
[44] C. Uhler, G. Raskutti, P. B¨uhlmann, and B. Yu. Geometry of the faithfulness
assumption in causal inference. The Annals of Statistics, 41(2):436–463, 2013.
[45] S. van de Geer and P. B¨uhlmann. ℓ0-penalized maximum likelihood for sparse
directed acyclic graphs. The Annals of Statistics, 41(2):536–567, 04 2013.
[46] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 68(1):49–67, 2006.
[47] A. Y. Zaitsev. On the Gaussian approximation of convolutions under multidimen-
sional analogues of S.N. Bernstein’s inequality conditions. Probability Theory
and Related Fields, 74(4):535–566, 1987.

