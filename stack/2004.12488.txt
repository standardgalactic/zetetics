arXiv:2004.12488v2  [cs.LG]  17 Nov 2020
Order preserving
hierarchical agglomerative clustering
Daniel Bakkelund
daniel.bakkelund@ifi.uio.no
Abstract
We present a method for hierarchical clustering of directed acyclic graphs and other
strictly partially ordered data that preserves the data structure. In particular, if we have
a < b in the original data and denote their respective clusters by [a] and [b], we get [a] < [b]
in the produced clustering. The clustering uses standard linkage functions, such as single-
and complete linkage, and is a generalisation of hierarchical clustering of non-ordered sets.
To achieve this, we deﬁne the output from running hierarchical clustering algorithms
on strictly ordered data to be partial dendrograms; sub-trees of classical dendrograms with
several connected components. We then construct an embedding of partial dendrograms
over a set into the family of ultrametrics over the same set. An optimal hierarchical clus-
tering is now deﬁned as follows: Given a collection of partial dendrograms, the optimal
clustering is the partial dendrogram corresponding to the ultrametric closest to the ori-
ginal dissimilarity measure, measured in the p-norm. Thus, the method is a combination
of classical hierarchical clustering and ultrametric ﬁtting.
Keywords
Hierarchical clustering
·
Order preserving
clustering
·
Partial
dendrogram · Unsupervised classiﬁcation · Ultrametric ﬁtting · Acyclic partition
1
Introduction
Clustering is one of the oldest and most frequently used techniques for exploratory data analysis
and unsupervised classiﬁcation. The toolbox contains a large variety of methods and algorithms,
spanning from the initial, but still popular ideas of k-means (Macqueen, 1967) and hierarchical
clustering (Johnson, 1967), to more recent methods, such as density- and model based cluster-
ing (Kriegel et al., 2011; Fraley and Raftery, 2002), and semi-supervised methods (Basu et al.,
2008), plus a large list of variants. All these methods have one thing in common: they try
to extract hidden structure from the data, and make it visible to the analyst. But they also
share another feature: if the analysed data is already endowed with some form of structure, the
structure is lost in the clustering process; the clustering does not try to retain the structure.
This in spite of the fact that strictly partially ordered data, such as directed acyclic graphs,
rooted trees and linear orders, are types of data that is more and more commonly analysed by
practitioners.
In this paper, we show how to extend hierarchical clustering to relational data in a way that
preserves the relations. In particular, if the input is a set X equipped with a strict order relation
<, and if a, b ∈X, we ensure that if a < b then we will have [a] <′ [b] after clustering, where [a]
and [b] are the respective clusters of a and b, and <′ is an order relation on the clusters naturally
induced by <.
1

1.1
Motivating real-world use case
The motivation for our method comes from an industry database of machine parts that are
arranged in part-of relations: parts are registered as sub-parts of other parts. For historical
reasons, there have been incidents of copy-paste of machine designs, and the copies have been
given entirely new identiﬁers with no links to the original design. In hindsight, there is a wish
to identify these equivalent machine parts, but telling them apart is hard. Also, the metadata
that is available has a tendency of displaying high similarity between a part and its sub-parts,
leading to “vertical clustering” in the data.
Since the motivation is to identify equivalent machinery with the aim of replacing one piece
of machinery with an equivalent part, and since a part and its sub-parts by no means can be
interchanged, it is essential to maintain this parent-child relationship. Moreover, since a part
and its sub-part are never equivalent, this is a strict order relation. The set of all machine parts
thus makes up a strictly partially ordered set.
By preserving these relations in the clustering process, we can eliminate the errors due to
close resemblance between the part and the sub-part, resulting in improved over all quality of
the clustering.
This is but one concrete example of a real world problem where the method we present
performs signiﬁcantly better than standard methods that disregard the structure, but it is
possible to imagine several other cases for which we have not yet had the opportunity to test
our methodology. We will only mention two here; citation networks and time series:
Citation networks are partial orders, where the order is deﬁned by the citations.
If we
perform order preserving clustering in the above sense on citation networks, the clusters will
contain related research, and the clusters will be ordered according to appearance relative other
related research. This diﬀers from clustering with regards to time: when clustering with time
as a parameter, you have to choose, implicitly or explicitly, a time interval for each cluster.
When the citation graph is used for ordering, the clusters will contain research that occurred
in parallel, citing similar sources, and being cited by similar sources, regardless to whether they
occurred in some particular time interval.
Time series are totally ordered sets of events, making a family of time series a partially
ordered set of events. If you want to do correlations across the time series, but the time stamps
cannot be used for this purpose (diﬀerent time zones, errors in time stamps, drift in time stamps,
etc.), the method we present in this article allows you to produce slices across all or some of the
time series while keeping the order intact, suggesting which time stamps in one series correlate
to time stamps in the other series.
1.2
Problem overview
This section presents the research problem on a high level. All terms and concepts used in this
section will be properly deﬁned in the main text.
1.2.1
Hierarchical clustering at a glance
From a bird’s-eye view, one may describe hierarchical agglomerative clustering as follows: A
clustering of a set X is a partitioning of X into disjoint subsets called clusters. Given a set X
together with a notion of (dis-)similarity between the elements of X, a hierarchical agglomerative
clustering can be obtained as follows:
1. Start by placing each element of X in a separate cluster.
2

2. Pick the two clusters that are most similar according to the (dis-)similarity measure, and
combine them into one cluster by taking their union.
3. If all elements of X are in the same cluster, we are done. Otherwise, go to Step 2 and
continue.
The result from this process is a dendrogram; a tree structure showing the sequence of the
clustering process. Figure 1 shows a dendrogram over the set X = {a, b, c, d, e}. The elements
of X are the leaf nodes of the dendrogram, and, starting at the bottom, the horizontal bars
indicate which elements are joined at which step in the process. The numbers on the y-axis
indicate at which dissimilarity level the diﬀerent clusters were formed.
2.0
4.5
8.0
10.0
a
b
c
d
e
Figure 1: A dendrogram over the set X = {a, b, c, d, e}.
The set of dendrograms over a ﬁnite set is in a bijective correspondence with the set of
ultrametrics over the same set (Carlsson and M´emoli, 2010).
An ultrametric is a particular
type of metric. From Figure 1, we deﬁne the ultrametric distance between two elements to
be the minimal height you have to ascend to in order to traverse from one element to the
other. For example, the ultrametric distance between elements c and e is 8.0. Dendrograms and
ultrametrics play a central role in our theory.
1.2.2
Introducing a strict order relation on X
Given a set X = {a, b, c, d} where a < b and c < d, we can use arrows to denote the order
relation, thinking of X as a directed acyclic graph with two connected components. If we want
to produce a hierarchical clustering of X as described above, while at the same time maintaining
the order relation, our options are depicted in the Hasse digram in Figure 2.
ac
bd
a
bc
d
c
ad
b
ac
b
d
a
c
bd
a
b
c
d
Figure 2: Possible hierarchical clusterings over the set X = {a, b, c, d} with a < b and c < d.
Adjacent elements indicate clusters.
Each path in this diagram, starting at the bottom and advancing upwards, represents a
hierarchical clustering. But, since we are required to preserve the order relation, we cannot merge
any more elements than what we see here. This means that we will never obtain dendrograms
like the one in Figure 1, that joins at the top when all elements are placed in a single cluster.
3

Rather, the output of hierarchical agglomerative clustering would take the form of the partial
dendrograms of Figure 3.
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 3: Partial dendrograms over the set X = {a, b, c, d} with a < b and c < d. Each partial
dendrogram corresponds to a path in Figure 2 starting at the bottom and advancing upwards
to the ordered set depicted below the dendrogram.
Moreover, consider the situation where both (a, d) and (a, c) are pairs of minimal dissimilar-
ity. Being mutual exclusive merges, choosing to merge one over the other leads to very diﬀerent
solutions, so we need a method that tells us which of the candidates is the better.
1.3
Outline of our method and contributions
In this section, we provide a high level view of our method, together with our main contributions
and results.
1.3.1
An embedding of partial dendrograms into ultrametrics
When we apply hierarchical agglomerative clustering to strictly partially ordered data, the
output is partial dendrograms. One of the contributions of this paper is an embedding of partial
dendrograms over a set into the family of ultrametrics over the same set. Due to the bijective
correspondence between ultrametrics and dendrograms, this also provides an embedding from
partial dendrograms into dendrograms.
This embedding is of obvious theoretical interest, since it parallels the bijective correspond-
ence between dendrograms over a set and ultrametrics over the same set. The embedding lifts
to the bijection, tying the theories for hierarchical agglomerative clustering for ordered and
non-ordered sets together.
But the embedding has value beyond the purely theoretical. The ﬁrst is that ultrametrics
(or metrics) are more suitable as tools for mathematics than are partial dendrograms. Secondly,
substantial work has been conducted since Jardine and Sibson (1971) studying hierarchical clus-
tering in the light of the correspondence between dendrograms and ultrametrics. Also, several
optimisation based methods for hierarchical clustering use the clustering structure, in terms of
dendrograms, as input to the objective function. Our embedding makes it possible to beneﬁt
from and participate in all of this. We have therefore laid down extra eﬀort, making sure that
this mapping is indeed an embedding (that is, an injective map), and not just any map.
But more importantly, we deﬁne our objective function as follows: from a selected list of
candidate partial dendrograms, the partial dendrogram representing the best hierarchical clus-
tering is the one that corresponds to the ultrametric that is closest to the original dissimilarity
measure when measured in the p-norm. This makes our model a variant of ultrametric ﬁtting.
4

1.3.2
Optimised hierarchical agglomerative clustering
Consider the case of a non ordered set X equipped with a dissimilarity measure. A family of
candidate hierarchical clusterings are deﬁned as follows: By running the algorithm for classical
hierarchical clustering from Section 1.2.1, we consider every output a candidate solution. In
particular, if the algorithm faces two or more equally valid pairs for merging, we simply merge
those pairs in every possible order. This generates one candidate solution for each permutation
of those connections.
Given the collection of candidate solutions, we deﬁne the optimised hierarchical agglomerative
clustering to be the dendrogram among the candidate solutions corresponding to the ultrametric
closest to the original dissimilarity measure when measured in the p-norm.
Due to the way the candidates are generated, this deﬁnition is permutation invariant; it
is not inﬂuenced by the order in which the elements of the set are enumerated.
The main
characteristics of optimised hierarchical clustering may be summarised as follows:
1. Optimised hierarchical clustering is permutation invariant
2. Classical hierarchical clustering with single linkage is identical to optimised hierarchical
clustering with single linkage
3. Optimised hierarchical clustering with complete linkage is NP-hard
As we demonstrate in relation to the proof of Item 3, optimised hierarchical clustering with
complete linkage is able to detect maximal cliques in graphs. This is a powerful property, and
we take this as part of the evidence that our choice of objective function is reasonable. In some
sense, we may say that this is a fulﬁllment of the idea behind complete linkage.
1.3.3
Order preserving optimised hierarchical agglomerative clustering
Our main result: order preserving hierarchical agglomerative clustering for strictly partially
ordered sets extends the above model for non-ordered clustering.
We copy the process for
candidate generation, with the modiﬁcation that we ensure that every merge leads to an order
preserving clustering. Our set of candidates now consists of partial dendrograms. The optimal
partial dendrogram is deﬁned as the partial dendrogram corresponding to the ultrametric being
closest to the original dissimilarity measure when measured in the p-norm. This comparison is
possible due to our embedding of partial dendrograms into ultrametrics.
In this way, we end up with order preserving hierarchical clustering that is permutation
invariant. For hierarchical clustering of strictly ordered sets, permutation invariance is key: As
described in Section 1.2.2, diﬀerent choices of merges may lead to very diﬀerent results. Being
dependent on the enumeration order of the underlying set could be disastrous.
The complexity class of order preserving hierarchical agglomerative clustering remains an
open question. At the time of writing, the authors have neither managed to ﬁnd a proof of
NP-completeness, nor found any eﬃcient algorithm for solving the clustering problem.
1.3.4
Polynomial time approximation
Since no eﬃcient algorithm currently exists, we present a method of approximation that can be
computed in polynomial time. The approximation method is based on random sampling from
candidate solutions, and therefore allows for parallel processing. We demonstrate the approx-
imation method on synthetic data generated as random directed acyclic graphs and random
dissimilarity measures.
5

We report the eﬃcacy of the approximation in terms of convergence plots, and when we have
a reference clustering available, we also compute the correlation between the approximation and
the reference clustering in terms of the adjusted Rand index. We also present a method for
computing the Rand index for the induced order relation relative to the reference order relation,
which be believe is a ﬁrst of its kind.
The approximation converges very rapidly on the demonstration datasets, indicating that
just a few samples are required to obtain close to optimal solutions.
1.3.5
Contributions
Our main contribution is the theory and algorithms for order preserving hierarchical agglomer-
ative clustering for strict posets. Further contributions we wish to highlight are:
• Optimised hierarchical agglomerative clustering for non-ordered sets; a hierarchical clus-
tering methodology very close to classical hierarchical clustering, but that is permutation
invariant.
• A general framework for order preserving ultrametric ﬁtting of strict partial orders and
directed acyclic graphs equipped with a dissimilarity measure.
• A polynomial time approximation scheme for order preserving hierarchical agglomerative
clustering
• A novel method for comparison of induced order relations over a set based on the adjusted
Rand index.
1.4
Related work
Hierarchical agglomerative clustering is described in a plethora of books and articles, and we
shall not try to give a comprehensive account of the material here. For an introduction to the
subject, refer to (Jain and Dubes, 1988, §3.2).
1.4.1
Clustering of ordered data
There are quite a few articles presenting clustering of orders, placing themselves in one of two
categories.
The ﬁrst is clustering of sets where the (dis)similarity measure is replaced by information
about whether one pair of elements is more similar than another pair of elements, for example
based on user preferences. This is sometimes referred to as comparison based clustering. See the
recent article by Ghoshdastidar et al. (2019) for an example and references. In this category, we
also ﬁnd the works of Janowitz (2010), providing a wholly order theoretic description of hier-
archical clustering, including the case where the dissimilarity measure is replaced by a partially
ordered set.
The second variant is to partition a family of ordered sets so that similarly ordered sets are
associated with each other. Examples include the paper by Kamishima and Fujiki (2003), where
they develop a variation of k-means, called k-o′means, for clustering preference data, each list
of preferences being a totally ordered set. Other examples in this category include clustering of
times series, identifying which times series are alike ( Luczak, 2016).
Our method diﬀers from all of these examples in that we cluster elements inside one ordered
set through the use of a (dis)similarity measure, while maintaining the original orders of ele-
ments.
6

1.4.2
Clustering to detect order
Another variant is the detection of order relations in data through clustering: In (Carlsson et al.,
2014), it is demonstrated how hierarchical agglomerative quasi-clustering can be used to deduce
a partial order of “net ﬂow” from an asymmetric network.
In this category, it is also worth mentioning dynamic time warping. This is a method for
aligning time series, and can be considered as clustering across two time series that is indeed
order preserving. See ( Luczak, 2016) for further references on this.
1.4.3
Acyclic graph partitioning problems
The problem of order preserving hierarchical agglomerative clustering can be said to belong to
the family of acyclic graph partitioning problems (Herrmann et al., 2017). If we consider the
strict partial order to be a directed acyclic graph (DAG), the task is to partition the vertices
into groups so that the groups together with the arrows still makes up a DAG.
Graph partitioning has received a substantial attention from researchers, especially within
computer science, over the last 50 years. Two important ﬁelds of application of this theory are
VLSI and parallel execution.
For VLSI, short for Very Large Scale Integration, the problem can be formulated as follows:
Given a set of micro processors, the wires that connect them, and a set of circuit boards, how
do you best place the processors on the circuit boards in order to optimise a given objective
function? Typically, a part of the objective function is to minimise the wire length. But other
features may also be part of the optimisation, such as the amount or volume of traﬃc between
certain processors etc. (Markov et al., 2015)
For parallel processing, the input data is a set of tasks to be executed.
The tasks are
organised as a DAG, where predecessors must be executed before descendants. Given a ﬁnite
number of processors, the problem is to group the tasks so that they can be run group-wise
on a processor, or running groups in parallel on diﬀerent processors, in order to execute all
tasks as quickly as possible. Typically additional information available is memory requirements,
expected execution times for the tasks, etc. (Bulu¸c et al., 2016)
It is not diﬃcult to understand why both areas have received attention, being essential
in the development of modern computers. The development of theory and methods has been
both successful and abundant, and a large array of techniques are available, both academic and
commercially.
Although both problems do indeed perform clustering of strict partial orders, their solutions
are not directly transferable to exploratory data analysis. Mostly because they have very speciﬁc
constraints and objectives originating from their respective problem domains.
The method we propose in this paper has as input a strict partial order (equivalently; a DAG)
together with an arbitrary dissimilarity measure. We then use the classical linkage functions
single-, average-, and complete linkage to suggest clusterings of the vertices from the input
dataset, while preserving the original order relation.
Our method therefore places itself ﬁrmly in the family of acyclic graph partitioning meth-
odologies, but with diﬀerent motivation, objective and solution, compared to existing methods.
1.4.4
Hierarchical clustering as an optimisation problem
Several publications aim at solving hierarchical clustering in terms of optimisation. However,
due to the procedural nature of classical hierarchical clustering, combined with the linkage
functions, pinning down an objective function may be an impossible task.
Especially since
classical hierarchical clustering is not even well deﬁned for complete linkage in the presence
7

of tied connections. This leads to a general abandonment of linkage functions in optimisation
based hierarchical clustering.
Quite commonly, optimisation based hierarchical clustering is done in terms of ultrametric ﬁt-
ting. That is, it aims to ﬁnd an ultrametric that is as close to the original dissimilarity measure as
possible, perhaps adding some additional constraints (Gilpin et al., 2013; Chierchia and Perret,
2019). It is well known that solving single linkage hierarchical clustering is equivalent to ﬁnd-
ing the so called maximal sub-dominant ultrametric. That is; the ultrametric that is pointwise
maximal among all ultrametrics not exceeding the original dissimilarity (Rammal et al., 1986).
But for the other linkage functions, there is no equivalent result.
Optimisation based hierarchical clustering therefore generally present alternative deﬁnitions
of hierarchical clustering. Quite often based on objective functions that originate from some par-
ticular domain. Exceptions from this are, for example, Ward’s method (Ward, 1963), where the
topology of the clusters are the focus of the objective, and also the recent addition by Dasgupta
(2016), where the optimisation aims towards topological properties of the generated dendrogram.
Although our method is, eventually, based on ultrametric ﬁtting, we optimise over a very
particular set of dendrograms. Namely the dendrograms that can be generated through classical
hierarchical clustering with linkage functions. It is therefore reasonable to claim that our method
places itself between classical hierarchical clustering and optimised models.
1.4.5
Clustering with constraints
A signiﬁcant amount of research has been devoted to the topic of clustering with constraints
in the form of pairwise must-link or cannot-link constraints, often in addition to other con-
straints, such as minimal- and maximal distance constraints, and so on. Some work as also
been done on hierarchical agglomerative clustering with constraints, starting with the works
of Davidson and Ravi (2005). For a thorough treatment of constrained clustering, see (Basu et al.,
2008).
Order preserving clustering (as well as acyclic partitioning) can be seen as a particular version
of constrained clustering, where the constraint is a directed, transitive cannot-link constraint. A
type of constraint that is not found in the constrained clustering literature.
1.5
Organisation of the remainder of this paper
Section 2 provides the necessary background material. We start by recalling strict and non-strict
partial order relations and equivalence relations. Thereafter, we revisit classical hierarchical
agglomerative clustering, recalling central concepts such as dissimilarity measures, ultrametrics
and dendrograms.
In Section 3, we develop optimised hierarchical agglomerative clustering for non-ordered
sets. This is a permutation invariant clustering model that is tailored especially to ﬁt into our
framework for agglomerative clustering of ordered sets.
In Section 4, we tackle the problem of order preservation during clustering: We deﬁne what
we mean by order preservation, and classify exactly the clusterings that are order preserving.
We also provide concise necessary and suﬃcient conditions for an hierarchical agglomerative
clustering algorithm to be order preserving.
In Section 5, we deﬁne partial dendrograms, and develop the embedding of partial dendro-
grams over an ordered set into the family of ultrametrics over the same set.
Our main results are given in Section 6, where we generalise the clustering model provided
in Section 3 to order preserving hierarchical agglomerative clustering for strict partial orders.
Section 7 provides a polynomial time approximation scheme for the clustering method, and
demonstrate the eﬃcacy of the approximation on synthetic data. Section 8 closes the article
8

with some concluding remarks, and a list of future work topics.
2
Background
In this section we recall basic background material.
We start by recollecting the required
order-theoretical tools together with equivalence relations, before recalling classical hierarchical
clustering.
2.1
Relations
Deﬁnition 1. A relation R on a set X is a subset R ⊆X × X, and we say that x and y are
related if (x, y) ∈R. The short hand notation aRb is equivalent to writing (a, b) ∈R.
2.1.1
Strict and non-strict partial orders
A strict partial order on a set X is a relation S on X that is irreﬂexive and transitive. Recall
that, an irreﬂexive and transitive relation is also anti-symmetric. A strictly partially ordered
set, or a strict poset, is a pair (X, S), where X is a set and S is a strict partial order on X.
We commonly denote a strict partial order by the symbol <.
On the other hand a partial order on X is a relation P on X that is reﬂexive, asymmetric
and transitive, and the pair (X, P) is called a partially ordered set, or a poset. The usual
notation for a partial order is ≤.
We shall just refer to strict and non-strict partial orders as orders, unless there is any need
for disambiguation: If R is an order on X, we say that a, b ∈X are comparable if either
(a, b) ∈R or (b, a) ∈R. And, if every pair of elements in X are comparable, we call X totally
ordered. A totally ordered subset of an ordered set is called a chain, and a subset where no
two elements are comparable is called an antichain. We denote non-comparability by a⊥b.
That is, for any elements a, b in an antichain, we have a⊥b.
A cycle in a relation E is a sequence in E on the form (a, b1), (b1, b2), . . . , (bn, a).
The
transitive closure of E is the minimal set E for which the following holds: If there is a
sequence of pairs (a1, a2), (a2, a3), . . . , (an−1, an) in E, then (a1, an) ∈E.
Let (X, E) be an ordered set. An element x0 ∈X is a minimal element if there is no
element y ∈X −{x0} for which (y, x0) ∈E. Dually, y0 is a maximal element if there is no
x ∈X −{y0} for which (y0, x) ∈E. If (X, E) has a unique minimal element, then this is called
the bottom element or the least element, and a unique maximal element is called the top
element or the greatest element.
Finally, a map f : (X, <X) →(Y, <Y ) is order preserving if a <X b ⇒f(a) <Y f(b), and
if f is a set isomorphism (that is, a bijection) for which f −1 is also order preserving, we say that
f is an order isomorphism, and that the sets (X, <X) and (Y, <Y ) are order isomorphic,
writing (X, <X) ≈(Y, <Y ).
2.1.2
Partitions and equivalence relations
A partition of X is a collection of disjoint subsets of X, the union of which is X. That is;
a clustering of X is a partition of X. The family of all partitions of X, denoted P(X), has a
natural partial order deﬁned by partition-reﬁnement: If A = {Ai}i and B = {Bj}j are partitions
of X, we say that A is a reﬁnement of B, writing A ⋐B, if, for every Ai ∈A there exists a
Bj ∈B such that Ai ⊆Bj. The sets of a partition are referred to as blocks.
Partitions are intimately related to the concept of equivalence relations: An equivalence
relation is a relation R on X that is reﬂexive, symmetric and transitive. Let the family of all
9

equivalence relations over a set X be denoted by R(X). If R ∈R(X) and (x, y) ∈R, we say
that x and y are equivalent, writing x ∼y. The maximal set of elements equivalent to x ∈X
is called the equivalence class of x, and is denoted [x]. R(X) is also partially ordered, but
by subset inclusion: that is, for R, S ∈R(X), we say that R is less than or equal to S if and
only if R ⊆S .
The quotient of X modulo R, denoted X/R, is the set of equivalence classes of X under R.
Notice that [x] is an element of X/R, but a subset of X.
Since the equivalence classes are subsets of X that together cover X, X/R is a partition of X.
Indeed, the family of partitions of X is in a one-to-one correspondence with the equivalence
relations of X: If A ∈P(X), then there exists a unique R ∈R(X) for which A = X/R.
Moreover, the correspondence is order preserving; for A = X/R and B = X/S , we have
A ⋐B ⇔R ⊆S .
Both P(X) and R(X) have top- and bottom elements: The least element of P(X) is the
singleton partition S(X), where each element is in an equivalence class by itself: S(X) =
{{x} | x ∈X}. The singleton partition corresponds to the diagonal equivalence relation, given
by ∆(X) = {(x, x) | x ∈X}, which is the least element of R(X). The greatest element of P(X)
is the trivial partition, {X}, corresponding to the equivalence relation X ×X, where all element
are equivalent. That is
S(X) = X/∆(X)
and
{X} = X/(X × X).
If A and B are partitions of X with A being a reﬁnement of B, we say that A is ﬁner than
B, and that B is coarser than A. We use the exact same terminology for the corresponding
equivalence relations A , B ∈R(X), having A ⊆B.
For a subset A ⊆X, let the notation X/A denote the partition of X where all of A is one
equivalence class, and the rest of X remains as singletons. Formally, this corresponds to the
equivalence relation RA = ∆(X) ∪(A × A). And ﬁnally, the quotient map corresponding to
an equivalence relation R ∈R(X) is the unique map qR : X →X/R deﬁned as qR(x) = [x].
That is, qR sends each element to its equivalence class.
Deﬁnition 2. A clustering of a set X is a partition of X, and a hierarchical clustering is
a chain in P(X) containing both the bottom and top elements. A cluster in a clustering is a
block in the partition.
Alternatively, a clustering of X is an equivalence relation R ∈R(X), and a hierarchical
clustering is a chain in R(X) containing both the bottom- and top elements of R(X). A cluster
is, then, an equivalence class in X/R.
We will refer to clusters as equivalence classes, clusters or blocks depending on the context,
all terms being frequently used in clustering literature.
Example 1. For the three-element space X = {a, b, c}, the lattice of partitions takes the form
of the below Hasse diagram.
{{a, b, c}}
{{a, b}, {c}}
{{a, c}, {b}}
{{a}, {b, c}}
{{a}, {b}, {c}}
P(X) :
The elements in bold make up a chain in P(X) that contains both the bottom- and top elements,
and therefore also constitutes a hierarchical clustering of the elements in X.
10

2.2
Classical hierarchical clustering
In this section, we recall classical hierarchical clustering in terms of Jardine and Sibson (1971).
Our theory builds directly on the theory for classical hierarchical clustering, so we provide a
fair bit of detail: We start by recalling the formal deﬁnition of a dendrogram, before recalling
dissimilarity measures and ultrametrics.
Thereafter, we recall linkage functions, and at the
end of the section, we tie all the concepts together and provide a description of the classical
hierarchical agglomerative clustering algorithm.
2.2.1
Dendrograms
Hierarchical clustering outputs dendrograms. But the graphical tree structure depicted in Fig-
ure 1 is not well-suited for formal deduction. In this section, we recall the deﬁnition of dendro-
gram due to Jardine and Sibson (1971) for this purpose. For the remainder of the text, let R+
denote the non-negative reals.
Deﬁnition 3. Let R+ be equipped with the usual total order ≤, and let P(X) be partially
ordered by partition reﬁnement. A dendrogram over a ﬁnite set X is an order preserving map
θ : R+ →P(X) for which
D1. θ(0) = S(X), the least element of P(X).
D2. ∃t0 > 0 s.t. θ(t0) = {X}, the greatest element of P(X),
D3. ∀t ∈R+ ∃ε > 0 s.t. θ(t) = θ(t + ε).
Axiom D3 ensures that the dendrogram is piecewise constant on intervals on the form [t, t′),
as illustrated in the following example.
Example 2. Below, we see a graphical dendrogram over the set X = {a, b, c, d, e} on the left
hand side, and the corresponding deﬁnition of θ : R+ →P(X) on the right.
2.0
4.5
8.0
10.0
a
b
c
d
e
θ(x) =
















{a}{b}{c}{d}{e}
	
for x ∈[0.0, 2.0)

{a}{b}{c, d}{e}
	
for x ∈[2.0, 4.5)

{a, b}{c, d}{e}
	
for x ∈[4.5, 8.0)

{a, b}{c, d, e}
	
for x ∈[8.0, 10.0)

{a, b, c, d, e}
	
for x ∈[10.0, ∞)
We will use the term dendrogram to denote both the graphical and the functional repres-
entation. If im(θ) = {Bi}n
i=0, we assume that the enumeration is compatible with the order
relation on P(X); in other words, that {Bi}n
i=0 is a chain in P(X). We denote the family of
all dendrograms over X by D(X).
2.2.2
Dissimilarity measures and ultrametrics
As pointed out in Section 1.2.1, in order to produce a hierarchical agglomerative clustering, we
need a notion of (dis-)similarity, or “distance” between elements: A dissimilarity measure on
a set X is a function d : X × X →R+, satisfying
d1. ∀x ∈X : d(x, x) = 0,
d2. ∀x, y ∈X : d(x, y) = d(y, x).
11

If d additionally satisﬁes
d3. ∀x, y, z ∈X : d(x, z) ≤max{d(x, y), d(y, z)},
we call d an ultrametric. The pair (X, d) is correspondingly called a dissimilarity space or
an ultrametric space. The family of all dissimilarity measures over X is denoted by M(X),
and the family of all ultrametrics by U(X).
Example 3 (Ultrametric). Property d3 is referred to as the ultrametric inequality, and is
a strengthening of the usual triangle inequality. In an ultrametric space (X, u), every triple of
points is arranged in an isosceles triangle: Let a, b, c ∈X, and let the pair a, b be of minimal
distance such that u(a, b) ≤min{u(a, c), u(b, c)}. The ultrametric inequality gives us
u(a, c) ≤max{u(a, b), u(b, c)} = u(b, c)
u(b, c) ≤max{u(b, a), u(a, c)} = u(a, c)

⇔u(a, c) = u(b, c).
Ultrametrics show up in many diﬀerent contexts, such as p-Adic number theory (Holly, 2001),
inﬁnite trees (Hughes, 2004), numerical taxonomy (Sneath and Sokal, 1973) and also within
physics (Rammal et al., 1986), just to cite a few. For hierarchical clustering, ultrametrics are
relevant because the dendrograms over a set are in a bijective relation to the ultrametrics over
the same set (Carlsson and M´emoli, 2010).
We shall also need the following terms, which apply to any dissimilarity space: The diameter
of (X, d) is given by the maximal inter-point distance:
diam(X, d) = max{ d(x, y) | x, y ∈X }.
And the separation of (X, d) is the minimal inter point distance:
sep(X, d) = min{ d(x, y) | x, y ∈X ∧x ̸= y }.
2.2.3
Classical hierarchical clustering
Before we deﬁne classical hierarchical clustering, we need to recall linkage functions. Our deﬁn-
ition follows the lines of the deﬁnition found in (Carlsson and M´emoli, 2010):
Deﬁnition 4. Given a set X, a family L of linkage functions on X is a set of maps
ℓQ : Q × Q × M(X) −→R+
for Q ∈P(X)
so that for each partition Q ∈P(X) and dissimilarity measure d ∈M(X), the map ℓQ(−, −, d) :
Q × Q →R+ is a dissimilarity measure on Q.
Let (X, d) be a dissimilarity space with Q ∈P(X), and let p, q ∈Q. We will commit to the
following abuse of notation: for a family of linkage functions L, we will write L(p, q, d) for the
dissimilarity between p and q assigned by the unique dissimilarity measure ℓQ(−, −, d). We will,
also somewhat misleading, refer to the family L as a linkage function.
Deﬁnition 5. For a dissimilarity space (X, d), let Q ∈P(X) and recall that p, q ∈Q are subsets
of X, since they are blocks of Q. The classical linkage functions are deﬁned as follows:
Single linkage
:
SL(p, q, d) = minx∈p miny∈q d(x, y).
Complete linkage
:
CL(p, q, d) = maxx∈p maxy∈q d(x, y).
Average linkage
:
AL(p, q, d) =
P
x∈p
P
y∈q d(x, y)
|p| · |q|
.
12

Deﬁnition 6 (Classical HC). Given a dissimilarity space (X, d) and a linkage function L, if
we follow the procedure outlined in Section 1.2.1, using L as the “notion of dissimilarity”, the
result is a chain of partitions {Qi}|X|−1
i=1
together with the dissimilarities {ρi}|X|−1
i=1
at which the
partitions were formed. The sequence of pairs Q = {(Qi, ρi)}|X|−1
i=1
maps to a dendrogram θQ
as follows:
θQ(x) = Qmax{i∈N | ρi≤x}.
(1)
We deﬁne a classical hierarchical clustering of (X, d) using L to be a dendrogram
HCL(X, d) = θQ
obtained through this procedure.
Remark 7. Notice that (1) maps {(Qi, ρi)}|X|−1
i=1
to a dendrogram if and only if
sep(Qi, L) ≤sep(Qi+1, L)
for 0 ≤i < |X| −1.
(2)
Otherwise, the ρi will not make up a monotone sequence, and the resulting function θQ will not
be an order preserving map. Although all of SL, AL and CL satisfy (2), it is fully possible to
deﬁne linkage functions that do not.
At any point during the clustering process, if we encounter a partition Q ∈P(X) with two
distinct pairs of elements (p1, q1), (p2, q2) ∈Q × Q for which
L(p1, q1, d) = L(p2, q2, d) = sep(Q, L),
we say that the two connections are tied, since they are both eligible candidates for the next
merge.
It is well known that HCSL is invariant with respect to the order of resolution of
ties (Jardine and Sibson, 1971), a property referred to as being permutation invariant, since
the order of enumeration of elements will not aﬀect the output of the clustering process. On
the other hand, both HCAL and HCCL are sensitive to enumeration order.
2.2.4
Dendrograms and ultrametrics
It has been long known that dendrograms map to ultrametrics (Jardine and Sibson, 1971):
ΨX : D(X) −→U(X).
In (Carlsson and M´emoli, 2010) the map ΨX is shown to be a bijection. If θ ∈D(X), the map
is deﬁned as
ΨX(θ)(x, y) = min{ t ∈R+ | ∃B ∈θ(t) : x, y ∈B }.
(3)
That is, the ultrametric distance is the least real number t for which θ maps to a partition
where x and y are in the same block.
The minimisation is well deﬁned due to Axiom D3.
The ultrametric can be read from the diagrammatic representation of the dendrogram as the
minimum height you have to ascend to in order to traverse from one element to the other
following the paths in the tree.
3
Optimised hierarchical clustering
In this section we devise a permutation invariant version of hierarchical clustering based on the
classical deﬁnition. We do this in two steps: First, we produce a family of candidate dendrograms
13

based on occurrences of tied connections. Then we pick the dendrogram that closest resembles
the initial dissimilarity measure as the best solution.
To tackle the problem of tied connections, consider the procedure outlined in Section 1.2.1.
If we resolve tied connections by picking a random minimal dissimilarity pair, the way the
procedure is speciﬁed, HCL becomes a non-deterministic algorithm; it may produce diﬀerent
dendrograms for the same input in the presence of ties, depending on which tied pair is selected.
And, moreover, it is capable of producing any dendrogram than can be produced by any tie
resolution order:
Deﬁnition 8. Given a dissimilarity space (X, d) and a linkage function L, let DL(X, d) be the
set of all possible outputs from HCL(X, d).
Recall that a dissimilarity measure d over a ﬁnite set X can be described as an |X| × |X|
real matrix [di,j]. Hence, we can compute the p-norm of a dissimilarity measure, and for an
ultrametric u ∈U(X), we can compute the pointwise diﬀerence
∥u −d∥p =
p
s X
x,y∈X
|u(x, y) −d(x, y)|p.
(4)
We suggest the following deﬁnition, recalling the deﬁnition of ΨX (3):
Deﬁnition 9. Given a dissimilarity space (X, d) and a linkage function L, the optimised
hierarchical agglomerative clustering over (X, d) using L is given by
HCL
opt(X, d) =
arg min
θ∈DL(X,d)
∥ΨX(θ) −d∥p.
(5)
That is; among all dendrograms that can be generated by HCL(X, d), optimised hierarch-
ical agglomerative clustering picks the dendrogram that is closest to the original dissimilarity
measure. In the tradition of ultrametric ﬁtting, this is the right choice of candidate.
Since DL(X, d) contains all dendrograms generated over all possible permutations of enu-
merations of X, the below theorem follows directly from Deﬁnition 9:
Theorem 10. HCL
opt is permutation invariant. That is, the order of enumeration of the ele-
ments of the set X does not aﬀect the output from HCL
opt(X, d).
Also, since SL is permutation invariant, we always have
DSL(X, d)
 = 1, yielding
Theorem 11. HCSL
opt(X, d) = HCSL(X, d).
Since HCAL and HCCL are not permutation invariant, we have no corresponding result in
these cases. For complete linkage, however, we have the following theorem. First, notice that
due to the deﬁnition of complete linkage (Deﬁnition 5), if θ is a solution to HCCL
opt(X, d) and
u = ΨX(θ) is the corresponding ultrametric, then
u(x, y) ≥d(x, y)
∀x, y ∈X.
Hence, in the case of complete linkage we can reformulate (5) as follows:
HCCL
opt(X, d) =
arg min
θ∈DCL(X,d)
∥ΨX(θ)∥p.
(6)
To see why this is the case, notice that if u, u′ ∈M(X) and both d ≤u and d ≤u′ pointwise,
then we can produce two non-negative functions δ, δ′ on X ×X so that u = d+δ and u′ = d+δ′.
In particular, we have u −d = δ, from which we deduce
∥u −d∥p ≤∥u′ −d∥⇔∥δ∥p ≤∥δ′∥p ⇔∥d + δ∥p ≤∥d + δ′∥p ⇔∥u∥p ≤∥u′∥p.
14

Theorem 12. Solving HCCL
opt(X, d) is NP-hard.
Proof. Let G = (V, E) be an undirected graph with vertices V and edges E ⊆V × V . Recall
the clique problem: Given a positive integer K < |V |, is there a clique in G of size at least K?
Equivalently: is there a set V ′ ⊆V with |V ′| ≥K for which V ′ × V ′ ⊆E? This is a known
NP-hard problem (Karp, 1972).
To reduce clique to HCCL
opt, deﬁne a dissimilarity measure on V as follows:
d(v, v′) =
(
1
if (v, v′) ∈E,
2
otherwise.
(7)
Then (V, d) is a dissimilarity space. Let θ be a solution of HCCL
opt(V, d), and set d = ΨV (θ).
An intrinsic property of CL is that if two blocks p, q ∈Qi are merged, then
∀v, v′ ∈p ∪q : d(v, v′) ≤CL(p, q, d).
And since we have d(v, v′) = 1 ⇔(v, v′) ∈E, it means that for a subset V ′ ⊆V , we have that
∀v, v′∈V ′ : d(v, v′) = 1 ⇔V ′ is a clique in G.
(8)
It follows that a largest possible cluster at proximity level 1 is a maximal clique in G.
We claim that minimising the norm is equivalent to producing a maximal cluster at proximity
level 1: Let d be the |V | × |V | distance matrix [di,j]. Due to the deﬁnition of CL, we have
d(v, v′) ∈{0, 1, 2}. If θ(1) = {Vi}s
i=1, then these are exactly the blocks that are subsets of
cliques, so each Vi contributes with |Vi|(|Vi| −1) ones in [di,j].
Having more ones reduces the norm of d.
Let Vj be of maximal cardinality in {Vi}s
i=1.
Assume ﬁrst that Vj has at least two elements more than the next to largest block, and let
|Vj| = P.
Removing one element from Vj reduces the number of ones in the dissimilarity matrix by
P(P −1)−(P −1)(P −2) = 2(P −1). Let the next to largest block have Q elements. Transferring
the element to this block then increases the number of ones by (Q+1)Q−Q(Q−1) = 2Q. Since
Q < P −1, this means that the total number of ones is reduced by moving an element from the
largest block to any of the smaller blocks. Hence, achieving the largest possible number of ones
implies maximising the size of the largest block.
If now, Vj only has one element more than the next to largest block, moving an element as
above corresponds to keeping the number of ones. Since each Vi for 1 ≤i ≤s is a subset of
a clique in G, the maximal number of ones is achieved by producing a block Vj that contains
exactly a maximal clique of G.
Therefore, if I{1}(x) is the indicator function for the set {1}, the size of a maximal clique in
G can be computed as
max
1≤i≤|V |
n |V |
X
j=1
I{1}
 di,j
o
,
counting the maximal number of row-wise ones in [di,j] in O(N 2) time. We therefore conclude
that HCCL
opt is NP-hard.
The computational hardness of HCCL
opt is directly connected to the presence of
tied connections: every encounter of n tied connections leads to n! new can-
didate solutions.
Since neither HCAL
opt is permutation invariant, the authors strongly believe that this is also
NP-hard, although that remains to be proven.
15

Remark 13. We cannot, in general, expect the mapping θ 7→∥ΨX(θ) −d∥p to be injective,
meaning that the answer to (5) may not be unique. Now, HCL, by construction, and HCSL
opt, by
Theorem 11, have unique solutions for every input (X, d). But both HCAL
opt and HCCL
opt may have
more than one solution, each solution being optimal.
Given a set X, denote the power set of X by P(X). We shall consider HCL
opt(X, −) to be
the function
HCL
opt(X, −) : M(X) −→P(D(X)),
mapping a dissimilarity measure over X to a set of dendrograms over X.
3.1
Other permutation invariant solutions
Carlsson and M´emoli (2010) oﬀer an alternative approach to permutation invariant hierarchical
agglomerative clustering. In their solution, when they face a set of tied connections, they merge
all tied the pairs in one operation, resulting in permutation invariance.
In the case of order preserving clustering, a family of tied connections can contain several
mutually exclusive merges due to the order relation. Using the method of Carlsson and M´emoli
leads to a problem of ﬁguring which blocks of tied connections to merge together, and in which
combinations and order. This leads to a combinatorial explosion of alternatives. The method we
have suggested is utterly simple, no doubt, but it is designed to circumvent this very problem.
4
Order preserving clustering
In this section, we determine what it means for an equivalence relation to be order preserving
with regards to a strict partial order. Some of the material presented here is already known,
and can be found in articles on acyclic graph partitioning, for example (Herrmann et al., 2017).
In most of these works, one is usually content by stating that the resultant graph shall be
acyclic. We proceed further to establish precise conditions that are necessary and suﬃcient for
a hierarchical agglomerative clustering algorithm to be order preserving.
4.1
Order preserving equivalence relations
Having established what we mean by a clustering (Deﬁnition 2), we can start the discussion of
what constitutes an order preserving clustering of a strict poset (X, <). If R is an equivalence
relation on X with quotient map q : X →X/R, we have already established, in Section 1.1,
that we require
∀x, y∈X : x < y ⇒q(x) <′ q(y).
That is, we are looking for a particular class of equivalence relations; namely
those that preserve the structure of the strict partial order—in other words,
the equivalence relations for which the quotient map is order preserving.
Given an ordered set (X, E), there is a particular relation induced on the quotient set X/R
for any equivalence relation R ∈R(X) (Blyth, 2005, §3.1):
Deﬁnition 14. Given an ordered set (X, E) and an equivalence relation R ∈R(X), ﬁrst deﬁne
the relation S0 on X by
([a], [b]) ∈S0 ⇔∃x, y ∈X : a ∼x ∧b ∼y ∧(x, y) ∈E.
(9)
The transitive closure of S0 is called the relation on X/R induced by E. We denote this
relation by S.
16

Example 4. An instructive illustration of what the relation S0 looks like for an ordered set
(X, <) under the equivalence relation R is that of an R-fence (Blyth, 2005), or just fence, for
short:
b1
b2
bn−1
bn
· · ·
a1
a2
an−1
an
Triple lines represent equivalences under R, and the arrows represent the order on (X, <). The
fence illustrates visually how one can traverse from a1 to bn along arrows and through equivalence
classes in X/R, and in that case we say that the fence links b1 to an. The induced relation
S has the property that (a, b) ∈S if there exists an R-fence in X linking a to b.
Recall that a cycle in a relation R is a sequence of pairs starting and ending with the same
element: (a, b1), (b1, b2), . . . , (bn, a).
Theorem 15. Let (X, E) be a strict poset, R ∈R(X), and let S be the relation on X/R induced
by E. Then the following statements are equivalent:
1. S is a strict partial order on X/R;
2. There are no cycles in S0;
3. qR : (X, E) −→(X/R, S) is order preserving.
Proof. From the deﬁnition of strict posets, they contain no cycles, so 1 ⇒2. Since a non-cyclic
set is irreﬂexive, and since S is transitive by construction, 2 ⇒1.
Let qR be order preserving.
Notice that if S0 is the set deﬁned in (9), we have S0 =
qR × qR(E). In particular, for all x, y ∈X for which (x, y) ∈E, we have ([x], [y]) ∈S0. Assume
that S is not a strict order. Then there is a cycle in S0; that is there are x, y ∈X for which
(x, y) ∈E, but ([y], [x]) ∈S0 also. This yields
∃a′, b′ ∈X : a′ ∼x ∧b′ ∼y ∧(b′, a′) ∈E.
But, since ([x], [y]) ∈S0, we also have
∃a, b ∈X : a ∼x ∧b ∼y ∧(a, b) ∈E.
This yields a ∼a′ and b ∼b′, so we have
 qR(a), qR(b)

∈S0 ∧qR(b) = qR(b′) ∧
 qR(b′), qR(a′)

∈S0.
But, since we have both qR(a) = qR(a′) and (a, b) ∈E, this contradicts the fact that qR is
order preserving, so our assumption that both ([x], [y]) and ([y], [x]) are elements of S0 must be
wrong. Hence, if qR is order preserving, there are no cycles in S0, and S is a strict partial order
on X/R. This shows that 3 ⇒1.
Finally, let S be a strict partial order, and assume that qR is not order preserving. Then,
there exists x, y ∈X where (x, y) ∈E and for which at least one of ([x], [y]) ̸∈S or ([y], [x]) ∈S
holds. Now, ([x], [y]) ∈S by Deﬁnition 14. Therefore, ([y], [x]) ∈S implies that S has a cycle,
contradicting the fact that S is a strict partial order.
17

Deﬁnition 16. Let (X, E) be an ordered set. An equivalence relation R ∈R(X) is regular if
there exists an order on X/R for which the quotient map is order preserving. We denote the
set of all regular equivalence relations over an ordered set (X, <) by R(X, <). Likewise,
the family of all regular partitions of (X, <) is denoted P(X, <).
In general, for an ordered set (X, <) and a regular equivalence relation R ∈R(X, <), we
will denote the induced order relation by <′.
4.2
The structure of regular equivalence relations
In this section, we establish a suﬃcient and necessary condition for an agglomerative clustering
algorithm to be order preserving. To help in the proof, we employ the concept of crowns (Blyth,
2005):
If (X, <) is a strict poset and the induced order on X/R contains a cycle, then this corres-
ponds to the existence of an R-crown:
b1
b2
bn−1
bn
· · ·
a1
a2
an−1
an.
That is; the R-crown is a “circular” R-fence.
Recall that, if A ⊆X, X/A denotes the quotient for which the quotient map qA : X →X/A
sends all of A to a point, and is the identity otherwise. That is, for every x, y ∈X, we have
qA(x) = qA(y) ⇔x, y ∈A.
Theorem 17. If A ⊆X for a strictly ordered set (X, <), the quotient map qA : X →X/A is
order preserving if and only if A is an antichain in (X, <).
Proof. If A is not an antichain, then X/A places comparable elements in the same equivalence
class, so qA is not order preserving.
Assume A is an antichain. If qA is not order preserving, then there is a cycle in (X/A, <′),
and since we have only one non-singleton equivalence class, there must exist a crown on the
form
b
A
c
But this means we have a, a′ ∈A for which b < a and a′ < c, but since c < b, this implies
a′ < a, contradicting the fact that A is an antichain.
Since a composition of order preserving maps is order preserving, this also applies to a
composition of quotient maps for a chain of regular equivalence relations R1 ⊆· · · ⊆Rn.
Combining this with Theorem 17, we have the following:
A clustering of a strictly ordered set will be order preserving if it can be pro-
duced as a sequence of pairwise merges of non-comparable elements.
We close the section with an observation about the family of all hierarchical clusterings over
a strict poset:
18

Theorem 18. For a strictly ordered set (X, <), the set P(X, <) of regular partitions over (X, <)
has S(X) as its least element. Unless < is the empty order, there is no greatest element.
Proof. S(X) is always a regular partition, so S(X) ∈P(X, <). And since S(X) is a reﬁnement
of every partition of X, S(X) is the least element of P(X, <).
If the order relation is not empty, then there are at least two elements that are comparable,
and, according to Theorem 17, they cannot be in the same equivalence class. Hence, there is no
greatest element.
The situation of Theorem 18 is depicted in Figure 2, and has already been discussed in
Section 1.2.2: In the case of tied connections that represent mutually exclusive merges, choosing
to merge one connection over the other may lead to very diﬀerent results. We therefore need a
strategy to select one of these solutions over the others. This will be the main focus of Sections 5
and 6.
5
Partial dendrograms
In this section, we construct the embedding of partial dendrograms into ultrametrics. Let an
ordered dissimilarity space be denoted by (X, <, d). We generally assume that the order
relation is non-empty, meaning that there are comparable elements in (X, <).
Recall that
P(X, <) is the set of all regular partitions over (X, <); that is, the partitions for which the
quotient map is order preserving.
Example 5. Recall the Hasse diagram of regular partitions of the set X = {a, b, c, d} equipped
with the order relation a < b, c < d depicted in Figure 2. Assume that we equip X with the
dissimilarity measure
dX :
b
c
d
a
2.0
1.0
1.3
b
1.0
1.5
c
2.0
.
(10)
For the usual agglomerative hierarchical clustering algorithms, the maximal chains in P(X, <)
will correspond to the partial dendrograms in Figure 4.
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 4: The partial dendrograms corresponding to the maximal chains in the Hasse diagram
of Figure 2.
The levels in the dendrograms are given by the dissimilarity dX in (10). The
corresponding maximal partitions are displayed below each dendrogram.
A signiﬁcant diﬀerence from non-ordered sets is that there is no greatest element {X} in
P(X, <).
As a consequence, there are no dendrograms θ : R+ →P(X, <), so we
have to provide an alternative:
19

Deﬁnition 19. Let R+ be equipped with the usual total order ≤, and let P(X, <) be partially
ordered by partition reﬁnement. A partial dendrogram over (X, <) is an order preserving
map θ : R+ →P(X, <) satisfying
P1. θ(0) = S(X), the least element of P(X, <).
P2. ∀t ∈R+ ∃ε > 0 s.t. θ(t) = θ(t + ε).
We will let θ(∞) denote the maximal partition in the image of θ, and denote the family of
all partial dendrograms over (X, <) by PD(X, <).
Remark 20. If the order relation is non-empty, we have PD(X, <) ∩D(X) = ∅.
The only diﬀerence between a partial dendrogram, and the deﬁnition of dendrogram given
in Deﬁnition 3, is that we do not any longer require a greatest element to be in the image of θ.
However, since P(X, <) is ﬁnite, a partial dendrogram θ ∈PD(X, <) is eventually constant;
that is, there exists a positive real number t0 for which
t ≥t0 ⇒θ(t) = θ(∞).
Partial dendrograms are clearly a generalisation of dendrograms. To distinguish between
the two, we will occasionally refer to the non-partial dendrograms as complete dendrograms.
As before, we will use the term partial dendrogram to address both the diagrammatic- and
functional representations.
Example 6. The partial dendrogram θ : R+ →P(X, <) corresponding to the right hand side
diagram in Figure 4 is deﬁned as follows:
1
2
a
c
b
d
ac
bd
θ(x) =






{a}, {b}, {c}, {d}
	
for x ∈[0.0, 1.0)

{a, c}, {b}, {d}
	
for x ∈[1.0, 1.5)

{a, c}, {b, d}
	
for x ∈[1.5, ∞)
5.1
Mapping partial dendrograms to dendrograms
We will now demonstrate, on a high level, how we can construct an ultrametric from a partial
dendrogram in a well deﬁned manner. Looking at the partial dendrograms of Example 5, each
connected component in the partial dendrograms is a complete dendrogram over its leaf nodes.
Since complete dendrograms map to ultrametrics, each connected component gives rise to an
ultrametric on the subset of X constituted by its leaf nodes:
Example 7. Recalling that any singleton {x} is a trivial ultrametric space with ultrametric
d{x} : (x, x) 7→0, the center diagram of Figure 4 provides the following ultrametrics for the
subsets {a, d}, {b} and {c} of X:
d{a,d}(x, y) =
(
0
if x = y,
1.3
otherwise
d{b} = 0
d{c} = 0.
For a disjoint family of ultrametric spaces we have the following result:
20

Lemma 21. Given a family of bounded, disjoint ultrametric spaces {(Xj, dj)}n
j=1 together with
a positive real number K ≥maxj {diam(Xj, dj)}, the map
d∪:
[
Xj ×
[
Xj −→R+
given by
d∪(x, y) =
(
dj(x, y)
if ∃j : x, y∈Xj,
K
otherwise
is an ultrametric on S
j Xj.
Proof. To prove that the ultrametric inequality holds, we start by showing that d∪1,2 is an
ultrametric on the restriction to the disjoint union X1 ∪X2: Let x, y ∈X1 and z ∈X2, and
choose a positive K ≥max{diam(X1, d1), diam(X2, d2)}. We now have
d∪1,2(x, z) = K
d∪1,2(x, y) = d1(x, y)
d∪1,2(y, z) = K.
This means that every triple of points are either already contained in an ultrametric space, or
they make up an isosceles triangle. In both cases, the ultrametric inequality holds, according to
the observation in Example 3.
By induction, we can now prove that
 (X1 ∪X2) ∪X3), d∪1,2,3

is an ultrametric space, and
so on, until all the (Xj, dj) are included.
Restricting K to be strictly positive makes the above construction work even when each
(Xj, dj) is a trivial ultrametric space, in which case d∪becomes the discrete metric on S
j X.
Turning back to partial dendrograms, assume θ ∈PD(X, <), and that the partition θ(∞) is
given by B = {Bj}m
j=1. That is; the number of connected components in the partial dendrogram
of θ is m: one connected component for each block in the coarsest partition.
Let dX|Bj be the restriction of dX to Bj × Bj, and, likewise, let <|Bj be the order relation
induced on Bj by <.
Each space (Bj, < |Bj, dX|Bj) is an ordered dissimilarity subspace of
(X, <, dX), and each Bj has a complete dendrogram over all of its points.
Since complete
dendrograms correspond to ultrametrics, each connected component in the partial dendrogram
of X corresponds to an ultrametric space (Bj, du
j ). Furthermore, since the {(Bj, du
j )}m
i=1 make up
a disjoint family of ultrametric spaces covering X, we can use Lemma 21 to deﬁne an ultrametric
on all of X as follows:
Pick a K ≥maxj{diam(Bj, du
j )}, and deﬁne uθ : X × X →R+ by
uθ(x, y) =
(
du
j (x, y)
if ∃j : x, y∈Bj,
K
otherwise.
(11)
Example 8. An illustration of how this construction turns out in the case of the partial dendro-
grams of Example 5 is provided in Figure 5:
1
2
b
c
a
d
a
bc
d
1
2
a
d
b
c
c
ad
b
1
2
a
c
b
d
ac
bd
Figure 5: “Completed” dendrograms corresponding to the partial dendrograms of Example 5,
using K = 2.0. The completions are marked by the dashed lines.
21

5.2
Ultrametric completions
We will now formalise the above construction in terms of a function from partial dendrograms to
complete dendrograms. We will also present necessary and suﬃcient conditions for this function
to be injective. Injectivity is not strictly required for the theory to work, but it signiﬁcantly
increases the discriminative power of the theory. Without injectivity, the families of optimal
solutions may include clusterings of rather appalling quality. An example is provided towards
the end of the section.
We deﬁne the diameter of a partial dendrogram θ to be the number
diam(θ) = sup{x ∈R+ | θ(x) ̸= θ(∞)}.
Deﬁnition 22. Given an ordered space (X, <) and a positive real number ε, we deﬁne the
ultrametric completion on ε to be the map Uε : PD(X, <) −→U(X) for which
Uε : θ 7→uθ,
where uθ is deﬁned as in (11), setting K = diam(θ) + ε.
From the discussion in Section 5.1, we know that this is well-deﬁned, but we want a concrete
function describing the mapping. We already have the map ΨX : D(X) −→U(X) from (3),
mapping dendrograms to ultrametrics. We therefore seek a map
κε : PD(X, <) −→D(X)
making the following diagram commute:
D(X)
U(X)
PD(X, <)
ΨX
κε
Uε
.
(12)
Seeing that κε must map partial dendrograms to complete dendrograms, a quick glance at
Figure 5 suggests the following deﬁnition:
κε(θ)(x) =
(
θ(x)
for x < diam(θ) + ε
{X}
otherwise.
It is straightforward to check that κε(θ) is a complete dendrogram.
Theorem 23. ΨX ◦κε = Uε. That is; the diagram in (12) commutes.
Proof. Assume ﬁrst that θ ∈PD(X, <) is a proper partial dendrogram, and that im(θ) =
{Bi}n
i=0. Let the coarsest partition in the image of θ be given by Bn = {Bj}m
j=1. That is, each
block Bj corresponds to a connected component in the partial dendrogram. Pick a block B ∈Bn
and assume x, y ∈B.
If
k = min{ i ∈N | ∃B′ ∈Bi : B = B′ },
then Bk is the ﬁnest partition containing all of B in one block. Since B ⊆X, the partitions
BB
i
= { B ∩B′ | B′ ∈Bi }
for 1 ≤i ≤k
22

constitute a chain in P(B) containing both S(B) and {B}. Hence, we can construct a complete
dendrogram over B by deﬁning
θB(x) = { B ∩B′ | B′ ∈θ(x) }.
(13)
This is exactly the complete dendrogram corresponding to the connected component of the tree
over X having the elements of B as leaf nodes. By Deﬁnition 22,
x, y ∈B ⇒Uε(θ)(x, y) = ΨB(θB)(x, y).
(14)
Due to (13), we have
x, y ∈B ⇒(∃B ∈θB(x) : x, y ∈B ⇔∃B′ ∈θ(x) : x, y ∈B′)
⇒min{ t ∈R+ | ∃B ∈θB(t) : x, y ∈B }
= min{ t ∈R+ | ∃B′ ∈θ(t) : x, y ∈B′ }.
Hence, by the deﬁnition of ΨX in (3) we conclude that
x, y ∈B ⇒ΨB(θB)(x, y) = (ΨX ◦κε)(θ)(x, y).
Combining this with (14), we get that whenever x, y ∈B, we have ΨX ◦κε = Uε.
On the other side, let x ∈Bi and y ∈Bj with i ̸= j. By deﬁnition, we have Uε(θ)(x, y) =
diam(θ) + ε. And, since there is no block in θ(∞) containing both x and y, we ﬁnd that the
minimal partition in im(κε(θ)) containing x and y in one block is {X}. But this means that
ΨX(κε(θ))(x, y) = diam(θ) + ε, so ΨX ◦κε = Uε holds in this case too.
Finally, if θ is a complete dendrogram, we have κε(θ) = θ, so ΨX ◦κε(θ) = ΨX(θ). But
since θ(∞) = {X}, it follows that Uε(θ) maps exactly to the ultrametric over X deﬁned by
ΨX(θ).
Theorem 24. Given a strict poset (X, <) with a non-empty order relation and a positive real
number ε, the map
Uε : PD(X, <) −→U(X)
is injective.
Proof. Since Uε = ΨK ◦κε and ΨX is a bijection, injectivity follows if κε is injective. Assume
that κε(θ) = κε(θ′). Then, for every x < diam(θ) + ε, we have
κε(θ)(x) = κε(θ′)(x) ⇔θ(x) = θ′(x).
Example 9. If ε is not chosen to be strictly positive, the map Uε will not necessarily be
injective. Let ε = 0 and consider the two partial dendrograms
1
a
b
c
d
1
a
b
c
d
Both are mapped to the following dendrogram via κ0:
23

1
a
b
c
d
This demonstrates what we mean by reduced discriminative power in the case of a non-injective
completion. Since the partial dendrograms exhibit distinctively diﬀerent information, it is de-
sirable that the methodology distinguishes between them.
6
Hierarchical clustering of ordered sets
We are now ready to embark on the speciﬁcation of order preserving hierarchical clustering
of ordered sets. We do this by extending our notion of optimised hierarchical clustering from
Section 3.
For an ordered set (X, <), recall that non-comparability of a, b ∈X is denoted a⊥b. We
introduce the non-comparable separation of (X, <, d), deﬁned as
sep⊥(X, <, d) =
min
x,y∈X{ d(x, y) | x ̸= y ∧x⊥y }.
Consider the following modiﬁcation of classical hierarchical clustering. The only diﬀerence is
that for each iteration, we check that there are elements that actually can be merged while
preserving the order relation; according to Theorem 17, merging a pair of non-comparable
elements produces a regular quotient. Recall that S(X) denotes the singleton partition of X.
Let (X, <, d) be given together with a linkage function L.
1. Start by setting Q0 = S(X), and endow Q0 with the induced order relation <0.
2. Among the pairs of non-comparable clusters, pick a pair of minimal dissimilarity according
to L, and combine them into one cluster by taking their union.
3. Endow the new clustering with the induced order relation.
4. If all elements of X are in the same cluster, or if all clusters are comparable, we are done.
Otherwise, go to Step 2 and continue.
The procedure results in a chain of ordered partitions {(Qi, <i)}m
i=0 together with the dis-
similarities {ρi}m
i=0 at which the partitions where formed.
And the sequence of pairs Q =
{(Qi, ρi)}m
i=0 maps to a partial dendrogram through (1) if the following lemma is satisﬁed:
Lemma 25. The sequence of pairs {(Qi, ρi)}m
i=0 maps to a partial dendrogram through applic-
ation of (1) if and only if
sep⊥(Qi, <i, L) ≤sep⊥(Qi+1, <i+1, L).
Since the singleton partition Q0 maps to a partial dendrogram, the algorithm will produce a
partial dendrogram for any ordered dissimilarity space, and since there can be at most |X| −1
merges, the procedure always terminates.
As for classical hierarchical clustering, the procedure is non-deterministic in the sense that
given a set of tied pairs, we select a random pair for the next merge. Hence, the procedure is
able to produce all possible partial dendrograms for all possible tie resolution strategies:
24

Deﬁnition 26. Given an ordered dissimilarity space (X, <, d) and a linkage function L, let the
set of all possible outputs from the above procedure be denoted by DL(X, <, d).
The set DL(X, <, d) diﬀers from DL(X, d) in two important ways:
• DL(X, <, d) contains partial dendrograms, not dendrograms
• The cardinality of DL(X, <, d) is at least that of DL(X, d), and often higher, due to
mutually exclusive merges and the “dead ends” in P(X, <) (see Figure 2).
Even for single linkage we have
DSL(X, <, d)
 > 1 if there are mutually exclusive tied connec-
tions.
In the spirit of optimised hierarchical clustering, we suggest the following deﬁnition, employ-
ing the ultrametric completion Uε from Deﬁnition 22:
Deﬁnition 27. Given an ordered dissimilarity space (X, <, d), together with a linkage func-
tion L, let ε > 0. An order preserving hierarchical agglomerative clustering using L
and ε is given by
HC<L
opt,ε(X, <, d) =
arg min
θ∈DL(X,<,d)
∥Uε(θ) −d∥p.
(15)
Our next result shows that if we remove the order relation, then optimised clustering and
order preserving clustering coincide.
Keep in mind that a dissimilarity space is an ordered
dissimilarity space with an empty order relation; that is, (X, d) = (X, ∅, d).
Theorem 28. If the order relation is empty, then order preserving optimised hierarchical clus-
tering and optimised hierarchical clustering coincide:
HC<L
opt,ε(X, ∅, d) = HCL
opt(X, d).
Proof. First, notice that
∀(Q, <Q) ∈P(X, ∅) : sep⊥(Q, <Q, L) = sep(Q, L),
where <Q denotes the (trivial) induced order. Hence, we have DL(X, ∅, d) = DL(X, d). Since
Uε|D(X) = ΨX, the result follows.
6.1
On the choice of ε
In HC<L
opt,ε(X, <, d) we identify the elements from DL(X, <, d) that are closest to the dissimilarity
measure d when measured in the p-norm. Since Uε : PD(X, <) →U(X) is injective, Uε induces
a relation ⪯d,ε on PD(X, <) deﬁned by
θ ⪯d,ε θ′ ⇔∥Uε(θ) −d∥p ≤∥Uε(θ′) −d∥p,
and the optimisation ﬁnds the minimal elements under this order.
The choice of ε may aﬀect the ordering of dendrograms under ⪯d,ε. We will show this by
providing an alternative formula for ∥u −d∥p that better expresses the eﬀect of the choice of ε
in the ultrametric completion: Assume that θ is a partial dendrogram over (X, <), and let
θ(∞) = {Bi}m
i=1. Furthermore, let ni = |Bi| for 1 ≤i ≤m be the cardinalities of the blocks,
and let the corresponding ultrametric be given by u = Uε(θ). The sum in the standard formula
25

for ∥u −d∥p given by (4) can be split in two: the intra-block diﬀerences and the inter-block
diﬀerences. The intra-block diﬀerences are independent of ε, and are given by
α =
m
X
i=1
X
x,y∈Bi
|u(x, y) −d(x, y)|p.
(16)
On the other hand, for every inter-block pair (x, y), we have u(x, y) = diam(θ) + ε, so the
inter-block diﬀerences, that are dependent on ε, can be computed as
βε =
X
(x,y)∈Bi×Bj
i̸=j
|diam(θ) + ε −d(x, y)|p.
(17)
We can now write ∥u −d∥p =
p√α + βε. If we think of u as an approximation of d, and saying
that |X| = N, the mean p-th error of this approximation can be expressed as a function of ε:
Ed(ε|θ, p) =
1
N ∥u −d∥p
p =
α
N +
1
N
X
(x,y)∈Bi ×Bj
i̸=j
|diam(θ) + ε −d(x, y)|p.
Notice that the minimisation in (15) exactly identiﬁes the partial dendrograms
of minimal mean p-th error.
Moreover, X being ﬁnite means that d is bounded, so Ed(ε|θ, p) →∞when ε →∞. Hence,
Ed(ε|θ, p) has at least one global minimum on [0, ∞).
Theorem 29. Diﬀerent choices of ε may result in diﬀerent orders on the dendrograms.
Proof. Let θ1 be a partial dendrogram over (X, <, d), and let {ε1}n
i=1 be the set of strictly
positive global minima of Ed(ε|θ1, p), assuming n ≥1.
Then θ1 is a minimal element in
(PD(X, <), ⪯εi,p) for 1 ≤i ≤n. Assume that there exists a partial dendrogram θ2 ∈PD(X, <)
that is not minimal in (PD(X, <), ⪯εi,p) for any of the εi, so that θ1 ≺εi,p θ2. Assume fur-
ther that there is an ε′ > 0 that is a global minimum of Ed(ε|θ2, p) so that θ2 is minimal in
(PD(X, <), ⪯ε′,p). Since ε′ ̸= εi for 1 ≤i ≤n, we get θ2 ≺ε′,p θ1.
The question now is which value to pick for ε.
From the formula for Ed(ε|θ, p), we see
that when ε becomes large, the inter-block diﬀerences dominate the approximation
error. For increasing ε, having low error eventually equals having few inter-block
pairs. Alternatively: the intra-block diﬀerences have insigniﬁcant inﬂuence on the
approximation error for large ε.
The eﬀect of this is that, when ε increases, the partial dendrograms close to d will be
those that have a low number of inter-block pairs, regardless of the quality of the intra-block
approximations of d. From the standpoint of ultrametric ﬁtting, this is intuitively wrong. Also,
it will lead to clusterings where as many elements as possible are placed in one large cluster,
since this is the most eﬀective method for reducing the number of inter-block pairs.
On the other side, a low value of ε will move the weight towards improving the approximation
of the intra-block distances. Again from the standpoint of ultrametric ﬁtting, this is the right
thing to do. Also, since the inter-block distances are between non-mergeable pairs, one may
claim that these diﬀerences should be given less attention in the ultrametric ﬁtting.
This points towards selecting a low value for ε. In the process of choosing, we have the
following result at our aid:
26

Theorem 30. For any ﬁnite ordered dissimilarity space (X, <, d) and linkage function L, there
exists an ε0 > 0 for which
ε, ε′ ∈(0, ε0) ⇒
 DL(X, <, d), ⪯d,ε) ≈
 DL(X, <, d), ⪯d,ε′).
That is; all ε ∈(0, ε0) induce the same order on the partial dendrograms.
Proof. Since X is ﬁnite, DL(X, <, d) is also ﬁnite. And according to Ed(ε|θ, p), if the cardinality
of DL(X, <, d) is n, there are at most pn positive values of ε that are distinct global minima
of partial dendrograms in DL(X, <, d). But this means there is a ﬁnite set of ε for which the
order on (DL(X, <), ⪯ε,p) changes. And since all these values are strictly positive, they have a
strictly positive lower bound.
It is, of course, possible to play with diﬀerent values of ε to obtain diﬀerent results. But,
since ε+diam(θ) is an upper bound of the partial dendrogram to begin with, we generally advise
as follows:
We suggest to use a value of ε that is as small as possible.
6.2
Idempotency of HC<L
opt,ε
A detailed axiomatic analysis along the lines of for example Ackerman and Ben-David (2016) is
beyond the scope of this paper, and is considered for future work. We still include a proof of
idempotency of HC<L
opt,ε, since this is an essential property of classical hierarchical clustering.
A function f is idempotent if f ◦f = f. For classical hierarchical clustering, the set of
ultrametrics U(X) ⊆M(X) over a set X are ﬁxed points under the map
(ΨX ◦HCL(X, −)) : M(X) →U(X).
In particular, if u = ΨX ◦HCL(X, d) for some d ∈M(X), since u ∈U(X), this yields u =
ΨX ◦HCL(X, u). This property is what Jardine and Sibson (1971) refers to as appropriateness,
being the ﬁrst of a set of conditions expected fulﬁlled by clustering methods.
This property does necessarily depend on the linkage function. We say that L is a convex
linkage function if we always have
SL(p, q, d) ≤L(p, q, d) ≤CL(p, q, d).
Now, if u is an ultrametric on X, the ultrametric inequality yields
u(a, b) = sep(X, u) ⇒∀c ∈X : u(a, c) = u(b, c).
So if L is a convex linkage function and u(a, b) = sep(X, u), we have
L({a, b}, {c}) = L({a}, {c}) = L({b}, {c})
∀c ̸= a, b.
This is to say that a convex linkage function preserves the structure of the original ultrametric
when minimal dissimilarity elements are merged. As a result, we get that DL(X, u) contains
exactly one element, namely the dendrogram that corresponds to the ultrametric.
Theorem 31. For a convex linkage function L, an ultrametric u ∈U(X) and θ = Ψ−1
X (u), we
have HCL
opt(X, u) = {θ}.
27

Hence, all of U(X) are ﬁxed points under ΨX ◦HCL
opt(X, −) whenever L is convex.
For ordered spaces, the case is diﬀerent. It is easy to construct an ordered ultrametric space
(X, <, u) for which u(a, b) = sep(X, u) and a < b, in which case the ultrametric cannot be
reproduced. Hence, all of U(X) cannot be ﬁxed points under Uε ◦HC<L
opt,ε(X, <, −), but the
mapping is still idempotent:
Theorem 32 (Idempotency). For an ordered dissimilarity space (X, <, d) and a convex linkage
function L, if θ ∈HC<L
opt,ε(X, <, d) and Uε(θ) = u, then HC<L
opt,ε(X, <, u) = {θ}.
Proof. Let θ(∞) = {Bi}m
i=1. Then each Bi is an antichain in (X, <), so we have
∀x, y ∈Bi : sep(Bi, u|Bi) = sep⊥(Bi, u|Bi)
for 1 ≤i ≤m.
Since ε > 0, we also have
x, y ∈Bi ⇒u(x, y) < diam(X, u)
for 1 ≤i ≤m.
And, lastly, since every pair of comparable elements are in pairwise diﬀerent blocks, we have
x < y ∨y < x ⇒u(x, y) = diam(X, u).
Now, since L is convex, based on the discussion before Theorem 31, the intra-block structure
of every block will be preserved. And, since every inter-block dissimilarity is accompanied by
comparability across blocks, the procedure for generation of DL(X, <, d) will exactly reproduce
the intra block structure of all blocks and then halt. Hence, DL(X, <, d) = {θ}.
6.3
On the computational complexity of HC<L
opt,ε
Due to Theorem 28, HC<CL
opt,ε is NP-complete. The complexity classes of HC<SL
opt,ε and HC<AL
opt,ε are
not known. The problems share similarity with several known NP-complete problems, but no
viable reduction has been found so far.
Typical similar problems, to mention just a few, are acyclic partition, and precedence con-
strained scheduling (Garey and Johnson, 1979), and also oriented circuit free graph coloring (Culus and Demange,
2006). The similarity lies in the fact that HC<L
opt,ε produces an acyclic partition of the type that
is asked for, but the mentioned problems put constraints on the number of blocks, or the sizes
of the blocks, in the partition. As of now, it is not obvious to us how to do this for HC<L
opt,ε.
The main tool for manipulation of the algorithm is the dissimilarity measure, and that does not
allow us to constrain this aspect of the partition directly.
7
Polynomial time approximation
This section describes a polynomial time approximation scheme for HC<L
opt,ε, and provides a
demonstration of the eﬃcacy of the approximation on synthetic random data. We start by
describing the approximation scheme, before introducing the random data model and how we
can measure the quality of the approximation. We then proceed by presenting the performance
of the approximation scheme on families of random datasets in terms of plots, displaying the
convergence towards the optimum as a function of the number of random samples.
28

7.1
The approximation model
Recall the set DL(X, <, d) of partial dendrograms over (X, <, d) from Deﬁnition 26. The al-
gorithm for producing a random element of DL(X, <, d) is described at the beginning of Sec-
tion 6; the key is to pick a random pair for merging whenever we encounter a set of tied
connections.
Our approximation model now becomes very simple: we generate a family of random partial
dendrograms over (X, <, d), and choose the one with the best ultrametric ﬁt:
Deﬁnition 33. Let (X, <, d) be given, and let N be a positive integer.
For any random
selection of N partial dendrograms {θi}i from DL(X, <, d), an N-fold approximation of
HC<L
opt,ε(X, <, d) is a partial dendrogram θ ∈{θi}i minimising ∥Uε(θ) −d∥p.
7.1.1
Running time complexity
As mentioned above, the algorithm for producing one random partial dendrogram is described
at the beginning of Section 6.
Assume that |X| = n. In the worst case, we may have to check
 n
2

pairs to ﬁnd one that is
not comparable, and the test for a⊥b has complexity O(n2), leading to a complexity of O(n4)
of ﬁnding a mergeable pair. Since there are up to n −1 merges, the worst case estimate of the
running time complexity for producing one element in DL(X, <, d) is O(n5).
As a function of the order density.
A part of this estimate is the number of comparability
tests we have to perform in order to ﬁnd a mergeable pair. For a sparse order relation, we
may have to test signiﬁcantly less than
 n
2

pairs before ﬁnding a mergeable pair: if K is the
expected number of test we have to do, the resulting complexity of ﬁnding a mergeable pair
becomes O(Kn2). This yields a total expected algorithmic complexity of O(Kn3). Notice that
limK→1 O
 Kn3
= O(n3), which is the running time complexity of classical HC. So, if the
order relation is sparse, we can generally expect the algorithm to execute signiﬁcantly faster
than the worst case estimate.
Parallel execution.
When producing an N-fold approximation, the N random partial dendro-
grams can be generated in up to N parallel processes, reducing the computational time of the
approximation. For the required number of dendrograms to obtain a good approximation, please
see the below demonstration.
7.2
Random ordered dissimilarity spaces
This section gives a concise description of the random model used to generate ordered dissimil-
arity spaces. The model consists of two parts: the random order and the random dissimilarity
measure.
7.2.1
Random partial order
A partial order is equivalent to a transitively closed directed acyclic graph, and since any acyclic
graph has a unique transitive closure, we can use any random model for directed acyclic graphs
to generate random partial orders. We choose to use the classical Erd˝os-R´enyi random graph
model (Bollob´as, 2001). Recall that a directed acyclic graph on n vertices is a binary n × n
adjacency matrix that is permutation similar a strictly upper triangular matrix; that is, there
exists a permutation that, when applied on both rows and columns of one matrix, transforms it
29

into the other. Let this family of n×n matrices be denoted by A(n). For a number p ∈[0, 1], the
sub-family A(n, p) ⊆A(n) is deﬁned as follows: for A ∈A(n), let A′ be strictly upper triangular
and permutation similar to A. Then each entry above the diagonal of A′ is 1 with probability
p. The sought partial order is the transitive closure of this graph; we denote the corresponding
set of transitively closed directed acyclic graphs by A(n, p).
7.2.2
Random dissimilarity measure
If |X| = n, a dissimilarity measure over X with no tied connections consists of
 n
2

distinct
values. Hence, any permutation of the sequence {1, . . . ,
 n
2

} is a non-tied random dissimilarity
measure over X.
To generate tied connections, let t ≥1 be the expected number of ties per level. That is,
for each unique value in the dissimilarity measure, that value is expected to have multiplicity t.
In the case where t does not divide
 n
2

, we resolve this by setting the multiplicity of the largest
dissimilarity to
  n
2

mod t

.
We write D(n, t) to denote the family of random dissimilarity measures over sets of n elements
with an expected number of t ties per level, in the above sense.
Deﬁnition 34. Given positive integers n and t, together with p ∈[0, 1], the family of random
ordered dissimilarity spaces generated by (n, p, t) is given by
O(n, p, t) = A(n, p) × D(n, t).
7.3
A measure of cluster quality
A large body of literature exists on the topic of comparing clusterings (see for instance (Vinh et al.,
2010) for a brief review). In our demonstration, we use the rather popular adjusted Rand in-
dex (Hubert and Arabie, 1985) to measure the ability of the approximation in ﬁnding a decent
partition, comparing against the optimal result.
Less work is done on this type of comparison for partial orders and directed acyclic graphs.
We suggest to use a modiﬁed version of the adjusted Rand index for this purpose too, based
on an adaptation of the Rand index used for network analysis (Hoﬀman et al., 2015). For an
introduction to the Rand index, and also to some of the versions of the adjusted Rand index,
see (Rand, 1971; Hubert and Arabie, 1985; Gates and Ahn, 2017).
7.3.1
Adjusted Rand index for partition quality
We use the adjusted Rand index (ARI) to compute the eﬃcacy of the approximation in ﬁnding
a partition close to the optimum, when we have an optimal solution to compare against. This
corresponds to what Gates and Ahn (2017) refers to as a one sided Rand index, since one of
the partitions are given as a reference, whereas the other is drawn from some distribution. In
the below demonstration, we assume that the approximating partition is drawn from the set of
all partitions over X under the uniform distribution.
7.3.2
Adjusted Rand index for induced order relations
When comparing induced orders on partitions over a set, unless the partitions coincide, it is not
obvious which blocks in one partition correspond to which blocks in the other. To overcome this
problem, we base our measurements on the base space projection:
30

Deﬁnition 35. For an ordered set (X, E) and a partition Q of X with induced order E′, the
base space projection of (Q, E′) onto X is the order relation EQ on X deﬁned as
(x, y) ∈EQ ⇔([x], [y]) ∈E′.
This allows us to compare the induced orders in terms of diﬀerent orders on X. Notice that if
the induced order E′ is a [strict] partial order on Q, then EQ is a [strict] partial order on X.
Hoﬀman et al. (2015) demonstrate that the adjusted Rand index can be used to detect
missing links in networks. The concept relies on the fact that a network link and a link in
an equivalence relation are not that diﬀerent: Both networks and equivalence relations are
special classes of relations, and the Rand index simply counts the number of coincidences and
mismatches between the two. A necessary criterion for this way of thinking about the Rand
index requires that the compared networks have known and ﬁxed labels, so that we know which
vertices map to which vertices (Warrens, 2008).
We can extend on this method, to develop an order relation similarity measure. Let A and
B be the adjacency matrices of two base space projections, and let Ai denote the i-th row of A,
and likewise for Bi. If ⟨a, b⟩is the inner product of a and b, we deﬁne
ai = ⟨Ai, Bi⟩
ci = ⟨Ai, 1 −Bi⟩
bi = ⟨1 −Ai, Bi⟩
di = ⟨1 −Ai, 1 −Bi⟩.
Here, ai is the number of common direct descendants of i in both relations, bi is the number of
descendants of i found in A but not in B, ci is the number of descendants of i in B but not in
A, while di counts the common non-descendants of i in the two relations.
Notice that we compare the i-th row in A only to the i-row in B since these rows corres-
pond to the same element in the set X. This diﬀers from the computation of the Rand index
in (Hoﬀman et al., 2015), where aij, bij, cij and dij are computed for 1 ≤i, j ≤n. We may now
compute the adjusted order Rand index (¯oARI) for the pair of relations as
¯oARI(A, B) = 1
n
n
X
i=1
2(aidi −bici)
(ai + bi)(bi + di) + (ai + ci)(ci + di).
(18)
7.3.3
Normalised ultrametric ﬁt
A natural choice of quality measure is to report the ultrametric ﬁt ∥Uε(θ) −d∥p of the obtained
partial dendrogram θ, especially if we can compare it to the ultrametric ﬁt of the optimal
solution. The scale of the ultrametric ﬁt depends heavily on both the size of the space and the
order of the norm, so we choose to normalise. Also, we invert the normalised value, so that the
optimal ﬁt has a value of 1, and a worst possible ﬁt has value 0. This makes it easy to compare
the convergence of the ultrametric ﬁt to the convergence of the ARI and ¯oARI.
Deﬁnition 36. Given a set of partial dendrograms {θi} over (X, <, d), let their respective
ultrametric ﬁts be given by δi = ∥Uε(θi) −d∥p. The normalised ultrametric ﬁt are the
corresponding values
ˆδi = 1 −
δi −mini{δi}
maxi{δi} −mini{δi}.
In the presence of a reference solution, we substitute mini{δi} with the ultrametric ﬁt of the
reference.
31

7.3.4
ARI vs ultrametric ﬁt
We could measure the quality of the clustering in terms of the ultrametric ﬁt alone. However,
we do not know the distribution of the ultrametric ﬁt, so as a stand-alone value, this provides
little information. Using ARI and ¯oARI to measure the quality of the clustering gives us a
measure that is, at least, adjusted for chance.
On the other hand, the ARI and ¯oARI only compare the ﬁnal partition and order relative
the reference, and not the full hierarchy. The reference partition can be reached through diﬀerent
sequences of merges, and neither AL nor CL are invariant in this respect. The hierarchy and
the merge threshold values are reﬂected in the ultrametric, and deviations in the hierarchy will
therefore be reported by the ultrametric ﬁt.
7.4
Demonstration
This section demonstrates the eﬃcacy of the polynomial time approximation of HC<L
opt,ε. The
demonstration is split in two: In the ﬁrst part, we demonstrate the eﬃcacy of the approximation
relative a known optimal solution.
In the second part, we demonstrate the convergence of
ultrametric ﬁt for larger spaces with much larger numbers of tied connections; spaces for which
the optimal algorithm does not terminate within any reasonable time.
For each parameter combination, a set of 30 random ordered dissimilarity spaces are gen-
erated. For each space, the optimal solution is found, and 100 approximations are generated
according to the prescribed procedure.
We then bootstrap the approximations to generate
N-fold approximations for diﬀerent N.
7.4.1
Results
We present the results in terms of convergence plots, showing the eﬃcacy of the approximation
as a function of the sample size.
For the results where a reference solution is available, the plots contain three curves:
E(ARI)
- The expected adjusted Rand index of the approximated partition.
E(¯oARI) - The expected adjusted Rand index of the approximated induced order.
norm.fit - The mean of the normalised ﬁt.
For the results where no reference solution is available, we present the distribution of the
normalised ﬁt.
The results are presented in Figures 6, 7, 8 and 10 on pages 33, 34, 34 and 35, respectively.
The parameter settings corresponding to the ﬁgures are given in Table 1 for easy reference, and
are also repeated in the ﬁgure text.
The parameters have been chosen to illustrate how the algorithm behaviour changes with
changing expected number of ties, changing link probability in the random partial order, and
choice of linkage function.
In general, a large expected number of tied connections requres
larger sample size for the approximation, while a more dense order relation (higher value of p
in O(n, p, t)) seems to require a smaller sample compared to a more sparse relation. Regarding
choice of linkage function, the approximation is very eﬃcient for both SL and AL, while CL
shows a signiﬁcant degeneration in approximation for larger numbers of tied connections.
32

n
L
p
t
has reference
Figure 6
200
SL, AL, CL
0.01, 0.02, 0.05
5
yes
Figure 7
200
SL, AL, CL
0.05
3, 7
yes
Figure 8
500
SL, AL, CL
0.01
10, 50, 100
no
Figure 9
500
SL, AL, CL
0.05
50, 100
no
Figure 10
500
SL, AL, CL
0.10
100
no
Table 1: Parameter settings for the demonstrations. The right-most column indicates whether
the reference clustering is available or not.
SL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.01
2
4
6
8
10
0.6
0.8
1
1
SL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.02
2
4
6
8
10
0.6
0.8
1
1
SL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
AL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
CL, p = 0.05
2
4
6
8
10
0.6
0.8
1
1
E(ARI);
E(¯oARI);
norm.fit
Figure 6: Eﬃcacy for n = 200 and t = 5 with p ∈{0.01, 0.02, 0.05}. The ﬁrst axis is the size of
the drawn sample.
33

SL, t = 3
2
4
6
8
10
0.6
0.8
1
1
AL, t = 3
2
4
6
8
10
0.6
0.8
1
1
CL, t = 3
2
4
6
8
10
0.6
0.8
1
1
SL, t = 7
2
4
6
8
10
0.6
0.8
1
1
AL, t = 7
2
4
6
8
10
0.6
0.8
1
1
CL, t = 7
2
4
6
8
10
0.6
0.8
1
1
E(ARI);
E(¯oARI);
norm.fit
Figure 7: Eﬃcacy for n = 200 and p = 0.05 with t ∈{3, 7}. The ﬁrst axis is the size of the
drawn sample. The plots for t = 5 can be found in the bottom row of Figure 6.
SL, t = 10
2
4
6
8
10
0.6
0.8
1
1
AL, t = 10
2
4
6
8
10
0.6
0.8
1
1
CL, t = 10
2
4
6
8
10
0.6
0.8
1
1
SL, t = 50
2
4
6
8
10
0.6
0.8
1
1
AL, t = 50
10
20
0.6
0.8
1
1
CL, t = 50
20
40
60
80
0.6
0.8
1
1
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
10
20
0.6
0.8
1
1
CL, t = 100
20
40
60
80
0.6
0.8
1
1
[Q1 −1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 8: Polynomial approximation rate for n = 500, P = 0.01 and t ∈{10, 20, 40}. The ﬁrst
axis is the size of the drawn sample.
34

SL, t = 50
2
4
6
8
10
0.6
0.8
1
1
AL, t = 50
2
4
6
8
10
0.6
0.8
1
1
CL, t = 50
2
4
6
8
10
0.6
0.8
1
1
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
2
4
6
8
10
0.6
0.8
1
1
CL, t = 100
2
4
6
8
10
0.6
0.8
1
1
[Q1 −1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 9: Polynomial approximation rate for n = 500, p = 0.05 and t ∈{50, 100}. The ﬁrst axis
is the size of the drawn sample.
SL, t = 100
2
4
6
8
10
0.6
0.8
1
1
AL, t = 100
2
4
6
8
10
0.6
0.8
1
1
CL, t = 100
2
4
6
8
10
0.6
0.8
1
1
[Q1 −1.5IQR, Q3 + 1.5IQR];
[Q1, Q3];
median
Figure 10: Polynomial approximation rate for n = 500, p = 0.10 and t = 100. The ﬁrst axis is
the size of the drawn sample.
35

7.4.2
First conclusions
The ﬁrst thing that strikes the eye is that the approximations converge very rapidly. Even for
moderately sized spaces (∼500 elements), it appears to be suﬃcient with 20 samples for SL and
AL, and for smaller spaces (∼200 elements), even fewer samples are required. We also notice
that there is a strong correlation between the ARI, ¯oARI and normalised ﬁt.
For the part of the demonstration where we have no reference clustering, we cannot know for
sure whether the best reported ﬁt is also optimal. However, from the convergent behaviour of
the data, and the strong correlation between optimality and normalised ﬁt in Figures 6 and 7,
this points in the direction of convergence to the true optimum.
Only CL displays convergence issues, indicating that if one wishes to use CL for large spaces
or large numbers of tied connections, it may be wise to do so in conjunction with convergence
tests.
On the other hand, since SL is independent on tie resolution order, every sequence of merges
ending in the same maximal partition will produce the same partial dendrogram. This explains
why the convergence rate of SL is less aﬀected by the expected number of tied connections than,
say, CL.
On the very rapid convergence for high link probability
The convergence rate is very
high in some of the plots of Figures 9 and 10. The authors believe this is due the high probability
of two random elements being comparable (high p in O(n, p, t)), since a dense relation leads to
fewer candidate solutions. This in contrast to the larger set of candidates for a more sparse
relation, such as in Figure 8.
On the other hand, as we can see in Figures 8 and 9, keeping p ﬁxed and increasing the
number of tied connections, and thereby the number of possible branch points, causes a slower
convergence rate.
Comparing against classical HC.
For each reference clustering, we also computed the ARI
of the solution produced by classical HC1. To do this, we ran classical HC on the dissimilarity
measure, extracted the partition having the same number of blocks as the reference clustering,
and computed the ARI of this partition relative the reference clustering.
However, most ARI values for classical HC are too low to even show up in the above plots,
with 75% of the samples having an ARI of less than 0.31, and no observations of an ARI of
more than 0.58. This is presumably due to the fact that classical HC is unable to take into
account the constraints provided by the order relation, or said diﬀerently: the family of possible
solutions for HC is a proper superset of DL(X, <, d).
7.5
Reference implementation
The implementation that was used to generate the above data is available as open source 2.
8
Summing up
We have established order preserving hierarchical agglomerative clustering for strictly partially
ordered sets. The clustering uses classical linkage functions such as single-, average-, and com-
plete linkage. We have shown that the clustering is idempotent and permutation invariant.
1python 3.7.7 and scipy.cluster.hierarchy version 1.3.2
2https://bitbucket.org/Bakkelund/ophac/wiki/Home
36

The output of order preserving hierarchical agglomerative clustering are partial dendrograms;
sub-trees of classical dendrograms, the diﬀerence being that partial dendrograms have several
connected components. We have shown that the family of partial dendrograms over a set embed
into the family of dendrograms over the set.
When applying the theory to non-ordered sets, we see that we have a new theory for hier-
archical agglomerative clustering that is very close to the classical theory.
But, diﬀerently
from classical hierarchical clustering, our theory is permutation invariant. We have shown that
for single linkage, our theory coincide with classical hierarchical clustering, while for complete
linkage, the clustering problem becomes NP-hard. However, the computational complexity is
directly linked to the number of tied connections, and in the absence of tied connections, the
theories coincide again.
We also present a polynomial approximation scheme for the clustering theory, and demon-
strate its convergence properties and eﬃcacy on randomly generated ordered sets and randomly
generated dissimilarity measures. The approximation model demonstrates fast convergence to-
wards the optimal solution for both single- and average linkage in all tests.
8.1
Future work topics
We suggest the following future work topics:
8.1.1
Complexity
The complexity classes of order preserving hierarchical agglomerative clustering for SL and AL
remain to be established (see Section 6.3).
8.1.2
Order versus dissimilarity
The order relation has a signiﬁcant eﬀect on the output from the clustering process: If the
dissimilarity measure starts out by associating “wrong” elements, the induced order may exclude
future merges of elements correctly belonging together. Also, if the order relation erroneously
identiﬁes elements as comparable, this may prevent elements that belong together from be
merged, regardless of the quality of the dissimilarity measure.
Together, these observations call for a need to “loosen up” the stringent nature of the or-
der relation, or to allow to balance the merge-aﬃnity of the dissimilarity measure against the
prohibitions of the order relation.
Acknowledgments.
I would like to thank Henrik Forssell, Department of Informatics (Uni-
versity of Oslo), and Gudmund Hermansen, Department of Mathematics (University of Oslo),
for their comments, questions and discussions greatly improving the exposition of this work.
References
M.
Ackerman
and
S.
Ben-David.
A
characterization
of
linkage-based
hierarchical
clustering.
Journal
of
Machine
Learning
Research,
17(231):1–17,
2016.
URL
http://jmlr.org/papers/v17/11-198.html.
S. Basu, I. Davidson, and K. Wagstaﬀ. Constrained Clustering: Advances in Algorithms, Theory,
and Applications. Chapman & Hall/CRC, 1 edition, 2008. ISBN 1584889969, 9781584889960.
37

T. Blyth. Lattices and Ordered Algebraic Structures. Universitext. Springer London, 2005. ISBN
9781852339050. URL https://www.springer.com/gp/book/9781852339050.
B. Bollob´as. Random Graphs. Cambridge Studies in Advanced Mathematics. Cambridge Uni-
versity Press, 2 edition, 2001. doi: 10.1017/CBO9780511814068.
A. Bulu¸c, H. Meyerhenke, I. Safro, P. Sanders, and C. Schulz.
Recent Advances in Graph
Partitioning, pages 117–158. Springer International Publishing, Cham, 2016. ISBN 978-3-319-
49487-6. URL https://link.springer.com/chapter/10.1007/978-3-319-49487-6_4.
G. Carlsson and F. M´emoli. Characterization, stability and convergence of hierarchical clus-
tering methods.
J. Mach. Learn. Res., 11:1425–1470, Aug. 2010. ISSN 1532-4435. URL
http://www.jmlr.org/papers/v11/carlsson10a.html.
G. Carlsson, F. M´emoli, A. Ribeiro, and S. Segarra.
Hierarchical quasi-clustering meth-
ods for asymmetric networks.
In E. P. Xing and T. Jebara, editors, Proceedings of the
31st International Conference on Machine Learning, volume 32 of Proceedings of Ma-
chine Learning Research, pages 352–360, Bejing, China, 22–24 Jun 2014. PMLR.
URL
http://proceedings.mlr.press/v32/carlsson14.html.
G. Chierchia and B. Perret. Ultrametric ﬁtting by gradient descent. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Inform-
ation Processing Systems, volume 32, pages 3181–3192. Curran Associates, Inc., 2019. URL
https://proceedings.neurips.cc/paper/2019/file/b865367fc4c0845c0682bd466e6ebf4c-Paper.pdf.
J.-F. Culus and M. Demange. Oriented coloring: Complexity and approximation. In J. Wie-
dermann, G. Tel, J. Pokorn´y, M. Bielikov´a, and J. ˇStuller, editors, SOFSEM 2006: Theory
and Practice of Computer Science, pages 226–236, Berlin, Heidelberg, 2006. Springer Berlin
Heidelberg.
S. Dasgupta. A cost function for similarity-based hierarchical clustering. In Proceedings of the
Forty-Eighth Annual ACM Symposium on Theory of Computing, STOC ’16, pages 118–127,
New York, NY, USA, 2016. Association for Computing Machinery. ISBN 9781450341325. doi:
10.1145/2897518.2897527. URL https://dl.acm.org/doi/10.1145/2897518.2897527.
I. Davidson and S. S. Ravi. Agglomerative hierarchical clustering with constraints: Theoretical
and empirical results.
In A. M. Jorge, L. Torgo, P. Brazdil, R. Camacho, and J. Gama,
editors, Knowledge Discovery in Databases: PKDD 2005, pages 59–70, Berlin, Heidelberg,
2005. Springer Berlin Heidelberg.
C. Fraley and A. E. Raftery. Model-based clustering, discriminant analysis, and density es-
timation.
Journal of the American Statistical Association, 97(458):611–631, 2002.
doi:
10.1198/016214502760047131. URL https://doi.org/10.1198/016214502760047131.
M. R. Garey and D. S. Johnson.
Computers and Intractability:
A Guide to the The-
ory of NP-Completeness.
W. H. Freeman & Co., USA, 1979.
ISBN 0716710447.
URL
https://dl.acm.org/doi/book/10.5555/578533.
A.
J.
Gates
and
Y.-Y.
Ahn.
The
impact
of
random
models
on
clustering
similarity.
Journal
of
Machine
Learning
Research,
18(87):1–28,
2017.
URL
http://jmlr.org/papers/v18/17-039.html.
38

D.
Ghoshdastidar,
M.
Perrot,
and
U.
von
Luxburg.
Foundations
of
comparison-
based
hierarchical
clustering.
In
H.
Wallach,
H.
Larochelle,
A.
Beygelzimer,
F.
d'Alch´e-Buc,
E.
Fox,
and
R.
Garnett,
editors,
Advances
in
Neural
Informa-
tion Processing Systems 32, pages 7456–7466. Curran Associates, Inc., 2019.
URL
http://papers.nips.cc/paper/8964-foundations-of-comparison-based-hierarchical-clustering.pdf.
S.
Gilpin,
S.
Nijssen,
and
I.
Davidson.
Formalizing
hierarchical
clustering
as
in-
teger
linear
programming.
In
Proceedings
of
the
Twenty-Seventh
AAAI
Confer-
ence on Artiﬁcial Intelligence,
AAAI’13,
pages 372–378. AAAI Press,
2013.
URL
https://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/view/6440.
J. Herrmann, J. Kho, B. U¸car, K. Kaya, and ¨U. V. C¸ataly¨urek. Acyclic partitioning of large
directed acyclic graphs. In 2017 17th IEEE/ACM International Symposium on Cluster, Cloud
and Grid Computing (CCGRID), pages 371–380, May 2017. doi: 10.1109/CCGRID.2017.101.
URL https://hal.inria.fr/hal-01744603.
M.
Hoﬀman,
D.
Steinley,
and
M.
J.
Brusco.
A
note
on
using
the
adjus-
ted
rand
index
for
link
prediction
in
networks.
Social
Networks,
42:72
–
79,
2015.
ISSN
0378-8733.
doi:
https://doi.org/10.1016/j.socnet.2015.03.002.
URL
http://www.sciencedirect.com/science/article/pii/S0378873315000210.
J. E. Holly.
Pictures of ultrametric spaces, the p-adic numbers, and valued ﬁelds.
The
American Mathematical Monthly, 108(8):721–728, 2001.
ISSN 00029890, 19300972. URL
http://www.jstor.org/stable/2695615.
L. Hubert and P. Arabie. Comparing partitions. Journal of Classiﬁcation, pages 193–218, 1985.
B. Hughes. Trees and ultrametric spaces: a categorical equivalence. Advances in Mathematics,
189(1):148 – 191, 2004. URL https://doi.org/10.1016/j.aim.2003.11.008.
A. K. Jain and R. C. Dubes. Algorithms for Clustering Data. Prentice-Hall, Inc., Upper Saddle
River, NJ, USA, 1988. ISBN 0-13-022278-X.
M. F. Janowitz. Ordinal and Relational Clustering. WORLD SCIENTIFIC, 2010. doi: 10.1142/
7449. URL https://www.worldscientific.com/doi/abs/10.1142/7449.
N. Jardine and R. Sibson. Mathematical Taxonomy. Wiley series in probability and mathematical
statistics. Wiley, 1971. ISBN 9780471440505.
S. C. Johnson. Hierarchical clustering schemes.
Psychometrika, 32(3):241–254, 1967. URL
https://link.springer.com/article/10.1007/BF02289588.
T. Kamishima and J. Fujiki. Clustering orders. In G. Grieser, Y. Tanaka, and A. Yamamoto, ed-
itors, Discovery Science, pages 194–207, Berlin, Heidelberg, 2003. Springer Berlin Heidelberg.
ISBN 978-3-540-39644-4. URL https://doi.org/10.1007/978-3-540-39644-4_17.
R. M. Karp.
Reducibility among Combinatorial Problems, pages 85–103.
Springer US,
Boston, MA, 1972.
ISBN 978-1-4684-2001-2.
doi:
10.1007/978-1-4684-2001-2 9.
URL
https://doi.org/10.1007/978-1-4684-2001-2_9.
H.-P. Kriegel, P. Kr¨oger, J. Sander, and A. Zimek.
Density-based clustering.
WIREs
Data Mining and Knowledge Discovery, 1(3):231–240, 2011. doi: 10.1002/widm.30. URL
https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.30.
39

M.
 Luczak.
Hierarchical
clustering
of
time
series
data
with
parametric
derivat-
ive
dynamic
time
warping.
Expert
Systems
with
Applications,
62:116
–
130,
2016.
ISSN
0957-4174.
doi:
https://doi.org/10.1016/j.eswa.2016.06.012.
URL
http://www.sciencedirect.com/science/article/pii/S0957417416302937.
J. Macqueen. Some methods for classiﬁcation and analysis of multivariate observations. In In
5-th Berkeley Symposium on Mathematical Statistics and Probability, pages 281–297, 1967.
URL https://projecteuclid.org/euclid.bsmsp/1200512992.
I. L. Markov,
J. Hu,
and M. Kim.
Progress and challenges in vlsi placement re-
search.
Proceedings of the IEEE, 103(11):1985–2003, Nov 2015.
ISSN 1558-2256.
URL
https://ieeexplore.ieee.org/document/7295553.
R. Rammal, G. Toulouse, and M. A. Virasoro.
Ultrametricity for physicists.
Rev.
Mod.
Phys.,
58:765–788,
Jul
1986.
doi:
10.1103/RevModPhys.58.765.
URL
https://link.aps.org/doi/10.1103/RevModPhys.58.765.
W. M. Rand.
Objective criteria for the evaluation of clustering methods.
Journal of
the American Statistical Association, 66(336):846–850, 1971.
ISSN 01621459.
URL
http://www.jstor.org/stable/2284239.
P. Sneath and R. Sokal.
Numerical Taxonomy: The Principles and Practice of Numerical
Classiﬁcation. A Series of books in biology. W. H. Freeman, 1973. ISBN 9780716706977.
N. X. Vinh, J. Epps, and J. Bailey. Information theoretic measures for clusterings comparison:
Variants, properties, normalization and correction for chance. Journal of Machine Learning
Research, 11(95):2837–2854, 2010. URL http://jmlr.org/papers/v11/vinh10a.html.
J. H. Ward.
Hierarchical grouping to optimize an objective function.
Journal of
the American Statistical Association, 58(301):236–244, 1963.
ISSN 01621459.
URL
http://www.jstor.org/stable/2282967.
M. J. Warrens. On the equivalence of cohen’s kappa and the hubert-arabie adjusted rand index.
Journal of Classiﬁcation, pages 177–183, 2008.
40

