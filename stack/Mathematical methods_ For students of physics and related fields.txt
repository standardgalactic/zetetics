
Mathematical Methods

Sadri Hassani
Mathematical Methods
For Students of Physics and Related Fields
123

Sadri Hassani
IIlinois State University
Normal, IL
USA
hassani@entropy.phy.ilstu.edu
ISBN: 978-0-387-09503-5
e-ISBN: 978-0-387-09504-2
Library of Congress Control Number: 2008935523
c⃝Springer Science+Business Media, LLC 2009
All rights reserved. This work may not be translated or copied in whole or in part without the written permission of the publisher
(Springer Science+Business Media, LLC, 233 Spring Street, New York, NY 10013, USA), except for brief excerpts in connection
with reviews or scholarly analysis. Use in connection with any form of information storage and retrieval, electronic adaptation,
computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identiﬁed as such, is
not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.
Printed on acid-free paper
springer.com

To my wife, Sarah,
and to my children,
Dane Arash and Daisy Bita

Preface to the Second
Edition
In this new edition, which is a substantially revised version of the old one,
I have added ﬁve new chapters: Vectors in Relativity (Chapter 8), Tensor
Analysis (Chapter 17), Integral Transforms (Chapter 29), Calculus of Varia-
tions (Chapter 30), and Probability Theory (Chapter 32). The discussion of
vectors in Part II, especially the introduction of the inner product, oﬀered the
opportunity to present the special theory of relativity, which unfortunately,
in most undergraduate physics curricula receives little attention. While the
main motivation for this chapter was vectors, I grabbed the opportunity to
develop the Lorentz transformation and Minkowski distance, the bedrocks of
the special theory of relativity, from ﬁrst principles.
The short section, Vectors and Indices, at the end of Chapter 8 of the ﬁrst
edition, was too short to demonstrate the importance of what the indices are
really used for, tensors. So, I expanded that short section into a somewhat
comprehensive discussion of tensors. Chapter 17, Tensor Analysis, takes
a fresh look at vector transformations introduced in the earlier discussion of
vectors, and shows the necessity of classifying them into the covariant and
contravariant categories. It then introduces tensors based on—and as a gen-
eralization of—the transformation properties of covariant and contravariant
vectors. In light of these transformation properties, the Kronecker delta, in-
troduced earlier in the book, takes on a new look, and a natural and extremely
useful generalization of it is introduced leading to the Levi-Civita symbol. A
discussion of connections and metrics motivates a four-dimensional treatment
of Maxwell’s equations and a manifest uniﬁcation of electric and magnetic
ﬁelds. The chapter ends with Riemann curvature tensor and its place in Ein-
stein’s general relativity.
The Fourier series treatment alone does not do justice to the many appli-
cations in which aperiodic functions are to be represented. Fourier transform
is a powerful tool to represent functions in such a way that the solution to
many (partial) diﬀerential equations can be obtained elegantly and succinctly.
Chapter 29, Integral Transforms, shows the power of Fourier transform in
many illustrations including the calculation of Green’s functions for Laplace,
heat, and wave diﬀerential operators. Laplace transforms, which are useful in
solving initial-value problems, are also included.

viii
Preface to Second Edition
The Dirac delta function, about which there is a comprehensive discussion
in the book, allows a very smooth transition from multivariable calculus to
the Calculus of Variations, the subject of Chapter 30. This chapter takes
an intuitive approach to the subject: replace the sum by an integral and the
Kronecker delta by the Dirac delta function, and you get from multivariable
calculus to the calculus of variations!
Well, the transition may not be as
simple as this, but the heart of the intuitive approach is. Once the transition
is made and the master Euler-Lagrange equation is derived, many examples,
including some with constraint (which use the Lagrange multiplier technique),
and some from electromagnetism and mechanics are presented.
Probability Theory is essential for quantum mechanics and thermody-
namics. This is the subject of Chapter 32. Starting with the basic notion of
the probability space, whose prerequisite is an understanding of elementary
set theory, which is also included, the notion of random variables and its con-
nection to probability is introduced, average and variance are deﬁned, and
binomial, Poisson, and normal distributions are discussed in some detail.
Aside from the above major changes, I have also incorporated some other
important changes including the rearrangement of some chapters, adding new
sections and subsections to some existing chapters (for instance, the dynamics
of ﬂuids in Chapter 15), correcting all the mistakes, both typographic and
conceptual, to which I have been directed by many readers of the ﬁrst edition,
and adding more problems at the end of each chapter. Stylistically, I thought
splitting the sometimes very long chapters into smaller ones and collecting
the related chapters into Parts make the reading of the text smoother. I hope
I was not wrong!
I would like to thank the many instructors, students, and general readers
who communicated to me comments, suggestions, and errors they found in the
book. Among those, I especially thank Dan Holland for the many discussions
we have had about the book, Rafael Benguria and Gebhard Gr¨ubl for pointing
out some important historical and conceptual mistakes, and Ali Erdem and
Thomas Ferguson for reading multiple chapters of the book, catching many
mistakes, and suggesting ways to improve the presentation of the material.
Jerome Brozek meticulously and diligently read most of the book and found
numerous errors. Although a lawyer by profession, Mr. Brozek, as a hobby,
has a keen interest in mathematical physics. I thank him for this interest and
for putting it to use on my book. Last but not least, I want to thank my
family, especially my wife Sarah for her unwavering support.
S.H.
Normal, IL
January, 2008

Preface
Innocent light-minded men, who think that astronomy can
be learnt by looking at the stars without knowledge of math-
ematics will, in the next life, be birds.
—Plato, Timaeos
This book is intended to help bridge the wide gap separating the level of math-
ematical sophistication expected of students of introductory physics from that
expected of students of advanced courses of undergraduate physics and engi-
neering. While nothing beyond simple calculus is required for introductory
physics courses taken by physics, engineering, and chemistry majors, the next
level of courses—both in physics and engineering—already demands a readi-
ness for such intricate and sophisticated concepts as divergence, curl, and
Stokes’ theorem. It is the aim of this book to make the transition between
these two levels of exposure as smooth as possible.
Level and Pedagogy
I believe that the best pedagogy to teach mathematics to beginning students
of physics and engineering (even mathematics, although some of my mathe-
matical colleagues may disagree with me) is to introduce and use the concepts
in a multitude of applied settings. This method is not unlike teaching a lan-
guage to a child: it is by repeated usage—by the parents or the teacher—of
the same word in diﬀerent circumstances that a child learns the meaning of
the word, and by repeated active (and sometimes wrong) usage of words that
the child learns to use them in a sentence.
And what better place to use the language of mathematics than in Nature
itself in the context of physics.
I start with the familiar notion of, say, a
derivative or an integral, but interpret it entirely in terms of physical ideas.
Thus, a derivative is a means by which one obtains velocity from position
vectors or acceleration from velocity vectors, and integral is a means by
which one obtains the gravitational or electric ﬁeld of a large number of
charged or massive particles. If concepts (e.g., inﬁnite series) do not succumb
easily to physical interpretation, then I immediately subjugate the physical

x
Preface
situation to the mathematical concepts (e.g., multipole expansion of electric
potential).
Because of my belief in this pedagogy, I have kept formalism to a bare
minimum. After all, a child needs no knowledge of the formalism of his or her
language (i.e., grammar) to be able to read and write. Similarly, a novice in
physics or engineering needs to see a lot of examples in which mathematics
is used to be able to “speak the language.” And I have spared no eﬀort to
provide these examples throughout the book. Of course, formalism, at some
stage, becomes important. Just as grammar is taught at a higher stage of a
child’s education (say, in high school), mathematical formalism is to be taught
at a higher stage of education of physics and engineering students (possibly
in advanced undergraduate or graduate classes).
Features
The unique features of this book, which set it apart from the existing text-
books, are
• the inseparable treatments of physical and mathematical concepts,
• the large number of original illustrative examples,
• the accessibility of the book to sophomores and juniors in physics and
engineering programs, and
• the large number of historical notes on people and ideas.
All mathematical concepts in the book are either introduced as a natural tool
for expressing some physical concept or, upon their introduction, immediately
used in a physical setting. Thus, for example, diﬀerential equations are not
treated as some mathematical equalities seeking solutions, but rather as a
statement about the laws of Nature (e.g., the second law of motion) whose
solutions describe the behavior of a physical system.
Almost all examples and problems in this book come directly from physi-
cal situations in mechanics, electromagnetism, and, to a lesser extent, quan-
tum mechanics and thermodynamics. Although the examples are drawn from
physics, they are conceptually at such an introductory level that students of
engineering and chemistry will have no diﬃculty beneﬁting from the mathe-
matical discussion involved in them.
Most mathematical-methods books are written for readers with a higher
level of sophistication than a sophomore or junior physics or engineering stu-
dent. This book is directly and precisely targeted at sophomores and juniors,
and seven years of teaching it to such an audience have proved both the need
for such a book and the adequacy of its level.
My experience with sophomores and juniors has shown that peppering the
mathematical topics with a bit of history makes the subject more enticing. It
also gives a little boost to the motivation of many students, which at times can

Preface
xi
run very low. The history of ideas removes the myth that all mathematical
concepts are clear cut, and come into being as a ﬁnished and polished prod-
uct. It reveals to the students that ideas, just like artistic masterpieces, are
molded into perfection in the hands of many generations of mathematicians
and physicists.
Use of Computer Algebra
As soon as one applies the mathematical concepts to real-world situations,
one encounters the impossibility of ﬁnding a solution in “closed form.” One
is thus forced to use approximations and numerical methods of calculation.
Computer algebra is especially suited for many of the examples and problems
in this book.
Because of the variety of the computer algebra softwares available on the
market, and the diversity in the preference of one software over another among
instructors, I have left any discussion of computers out of this book. Instead,
all computer and numerical chapters, examples, and problems are collected in
Mathematical Methods Using Mathematica⃝
R, a relatively self-contained com-
panion volume that uses Mathematica⃝
R.
By separating the computer-intensive topics from the text, I have made it
possible for the instructor to use his or her judgment in deciding how much
and in what format the use of computers should enter his or her pedagogy.
The usage of Mathematica⃝
R in the accompanying companion volume is only a
reﬂection of my limited familiarity with the broader ﬁeld of symbolic manipu-
lations on the computers. Instructors using other symbolic algebra programs
such as Maple⃝
R and Macsyma⃝
R may generate their own examples or trans-
late the Mathematica⃝
R commands of the companion volume into their favorite
language.
Acknowledgments
I would like to thank all my PHY 217 students at Illinois State University
who gave me a considerable amount of feedback. I am grateful to Thomas
von Foerster, Executive Editor of Mathematics, Physics and Engineering at
Springer-Verlag New York, Inc., for being very patient and supportive of the
project as soon as he took over its editorship.
Finally, I thank my wife,
Sarah, my son, Dane, and my daughter, Daisy, for their understanding and
support.
Unless otherwise indicated, all biographical sketches have been taken from
the following sources:
Kline, M. Mathematical Thought: From Ancient to Modern Times, Vols. 1–3,
Oxford University Press, New York, 1972.

xii
Preface
History of Mathematics archive at www-groups.dcs.st-and.ac.uk:80.
Simmons, G. Calculus Gems, McGraw-Hill, New York, 1992.
Gamow, G. The Great Physicists: From Galileo to Einstein, Dover, New York,
1961.
Although extreme care was taken to correct all the misprints, it is very
unlikely that I have been able to catch all of them. I shall be most grateful to
those readers kind enough to bring to my attention any remaining mistakes,
typographical or otherwise. Please feel free to contact me.
Sadri Hassani
Department of Physics, Illinois State University, Normal, Illinois

Note to the Reader
“Why,” said the Dodo, “the best way to ex-
plain it is to do it.”
—Lewis Carroll
Probably the best advice I can give you is, if you want to learn mathematics
and physics, “Just do it!”
As a ﬁrst step, read the material in a chapter
carefully, tracing the logical steps leading to important results. As a (very
important) second step, make sure you can reproduce these logical steps, as
well as all the relevant examples in the chapter, with the book closed.
No
amount of following other people’s logic—whether in a book or in a lecture—
can help you learn as much as a single logical step that you have taken yourself.
Finally, do as many problems at the end of each chapter as your devotion and
dedication to this subject allows!
Whether you are a physics or an engineering student, almost all the ma-
terial you learn in this book will become handy in the rest of your academic
training.
Eventually, you are going to take courses in mechanics, electro-
magnetic theory, strength of materials, heat and thermodynamics, quantum
mechanics, etc. A solid background of the mathematical methods at the level
of presentation of this book will go a long way toward your deeper under-
standing of these subjects.
As you strive to grasp the (sometimes) diﬃcult concepts, glance at the his-
torical notes to appreciate the eﬀorts of the past mathematicians and physi-
cists as they struggled through a maze of uncharted territories in search of
the correct “path,” a path that demands courage, perseverance, self-sacriﬁce,
and devotion.
At the end of most chapters, you will ﬁnd a short list of references that you
may want to consult for further reading. In addition to these speciﬁc refer-
ences, as a general companion, I frequently refer to my more advanced book,
Mathematical Physics: A Modern Introduction to Its Foundations, Springer-
Verlag, 1999, which is abbreviated as [Has 99]. There are many other excellent
books on the market; however, my own ignorance of their content and the par-
allelism in the pedagogy of my two books are the only reasons for singling out
[Has 99].

Contents
Preface to Second Edition
vii
Preface
ix
Note to the Reader
xiii
I
Coordinates and Calculus
1
1
Coordinate Systems and Vectors
3
1.1
Vectors in a Plane and in Space . . . . . . . . . . . . . . . . . .
3
1.1.1
Dot Product
. . . . . . . . . . . . . . . . . . . . . . . .
5
1.1.2
Vector or Cross Product . . . . . . . . . . . . . . . . . .
7
1.2
Coordinate Systems
. . . . . . . . . . . . . . . . . . . . . . . .
11
1.3
Vectors in Diﬀerent Coordinate Systems . . . . . . . . . . . . .
16
1.3.1
Fields and Potentials . . . . . . . . . . . . . . . . . . . .
21
1.3.2
Cross Product
. . . . . . . . . . . . . . . . . . . . . . .
28
1.4
Relations Among Unit Vectors
. . . . . . . . . . . . . . . . . .
31
1.5
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2
Diﬀerentiation
43
2.1
The Derivative
. . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.2
Partial Derivatives . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.2.1
Deﬁnition, Notation, and Basic Properties . . . . . . . .
47
2.2.2
Diﬀerentials . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.2.3
Chain Rule . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.2.4
Homogeneous Functions . . . . . . . . . . . . . . . . . .
57
2.3
Elements of Length, Area, and Volume . . . . . . . . . . . . . .
59
2.3.1
Elements in a Cartesian Coordinate System . . . . . . .
60
2.3.2
Elements in a Spherical Coordinate System . . . . . . .
62
2.3.3
Elements in a Cylindrical Coordinate System . . . . . .
65
2.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68

xvi
CONTENTS
3
Integration: Formalism
77
3.1
“ ” Means “ um” . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.2
Properties of Integral . . . . . . . . . . . . . . . . . . . . . . . .
81
3.2.1
Change of Dummy Variable . . . . . . . . . . . . . . . .
82
3.2.2
Linearity
. . . . . . . . . . . . . . . . . . . . . . . . . .
82
3.2.3
Interchange of Limits
. . . . . . . . . . . . . . . . . . .
82
3.2.4
Partition of Range of Integration . . . . . . . . . . . . .
82
3.2.5
Transformation of Integration Variable . . . . . . . . . .
83
3.2.6
Small Region of Integration . . . . . . . . . . . . . . . .
83
3.2.7
Integral and Absolute Value . . . . . . . . . . . . . . . .
84
3.2.8
Symmetric Range of Integration
. . . . . . . . . . . . .
84
3.2.9
Diﬀerentiating an Integral . . . . . . . . . . . . . . . . .
85
3.2.10 Fundamental Theorem of Calculus . . . . . . . . . . . .
87
3.3
Guidelines for Calculating Integrals . . . . . . . . . . . . . . . .
91
3.3.1
Reduction to Single Integrals . . . . . . . . . . . . . . .
92
3.3.2
Components of Integrals of Vector Functions
. . . . . .
95
3.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4
Integration: Applications
101
4.1
Single Integrals . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
4.1.1
An Example from Mechanics . . . . . . . . . . . . . . . 101
4.1.2
Examples from Electrostatics and Gravity . . . . . . . . 104
4.1.3
Examples from Magnetostatics . . . . . . . . . . . . . . 109
4.2
Applications: Double Integrals
. . . . . . . . . . . . . . . . . . 115
4.2.1
Cartesian Coordinates . . . . . . . . . . . . . . . . . . . 115
4.2.2
Cylindrical Coordinates . . . . . . . . . . . . . . . . . . 118
4.2.3
Spherical Coordinates . . . . . . . . . . . . . . . . . . . 120
4.3
Applications: Triple Integrals . . . . . . . . . . . . . . . . . . . 122
4.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5
Dirac Delta Function
139
5.1
One-Variable Case . . . . . . . . . . . . . . . . . . . . . . . . . 139
5.1.1
Linear Densities of Points . . . . . . . . . . . . . . . . . 143
5.1.2
Properties of the Delta Function . . . . . . . . . . . . . 145
5.1.3
The Step Function . . . . . . . . . . . . . . . . . . . . . 152
5.2
Two-Variable Case . . . . . . . . . . . . . . . . . . . . . . . . . 154
5.3
Three-Variable Case . . . . . . . . . . . . . . . . . . . . . . . . 159
5.4
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
II
Algebra of Vectors
171
6
Planar and Spatial Vectors
173
6.1
Vectors in a Plane Revisited . . . . . . . . . . . . . . . . . . . . 174
6.1.1
Transformation of Components . . . . . . . . . . . . . . 176
6.1.2
Inner Product . . . . . . . . . . . . . . . . . . . . . . . . 182

CONTENTS
xvii
6.1.3
Orthogonal Transformation . . . . . . . . . . . . . . . . 190
6.2
Vectors in Space
. . . . . . . . . . . . . . . . . . . . . . . . . . 192
6.2.1
Transformation of Vectors . . . . . . . . . . . . . . . . . 194
6.2.2
Inner Product . . . . . . . . . . . . . . . . . . . . . . . . 198
6.3
Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
6.4
The Jacobian . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
6.5
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
7
Finite-Dimensional Vector Spaces
215
7.1
Linear Transformations
. . . . . . . . . . . . . . . . . . . . . . 216
7.2
Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
7.3
The Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . 222
7.4
Eigenvectors and Eigenvalues . . . . . . . . . . . . . . . . . . . 224
7.5
Orthogonal Polynomials . . . . . . . . . . . . . . . . . . . . . . 227
7.6
Systems of Linear Equations . . . . . . . . . . . . . . . . . . . . 230
7.7
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
8
Vectors in Relativity
237
8.1
Proper and Coordinate Time
. . . . . . . . . . . . . . . . . . . 239
8.2
Spacetime Distance . . . . . . . . . . . . . . . . . . . . . . . . . 240
8.3
Lorentz Transformation
. . . . . . . . . . . . . . . . . . . . . . 243
8.4
Four-Velocity and Four-Momentum . . . . . . . . . . . . . . . . 247
8.4.1
Relativistic Collisions
. . . . . . . . . . . . . . . . . . . 250
8.4.2
Second Law of Motion . . . . . . . . . . . . . . . . . . . 253
8.5
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
III
Inﬁnite Series
257
9
Inﬁnite Series
259
9.1
Inﬁnite Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . 259
9.2
Summations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
9.2.1
Mathematical Induction . . . . . . . . . . . . . . . . . . 265
9.3
Inﬁnite Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
9.3.1
Tests for Convergence . . . . . . . . . . . . . . . . . . . 267
9.3.2
Operations on Series . . . . . . . . . . . . . . . . . . . . 273
9.4
Sequences and Series of Functions
. . . . . . . . . . . . . . . . 274
9.4.1
Properties of Uniformly Convergent Series . . . . . . . . 277
9.5
Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
10 Application of Common Series
283
10.1 Power Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
10.1.1 Taylor Series
. . . . . . . . . . . . . . . . . . . . . . . . 286
10.2 Series for Some Familiar Functions . . . . . . . . . . . . . . . . 287
10.3 Helmholtz Coil . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
10.4 Indeterminate Forms and L’Hˆopital’s Rule . . . . . . . . . . . . 294

xviii
CONTENTS
10.5 Multipole Expansion . . . . . . . . . . . . . . . . . . . . . . . . 297
10.6 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
10.7 Multivariable Taylor Series
. . . . . . . . . . . . . . . . . . . . 305
10.8 Application to Diﬀerential Equations . . . . . . . . . . . . . . . 307
10.9 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
11 Integrals and Series as Functions
317
11.1 Integrals as Functions
. . . . . . . . . . . . . . . . . . . . . . . 317
11.1.1 Gamma Function . . . . . . . . . . . . . . . . . . . . . . 318
11.1.2 The Beta Function . . . . . . . . . . . . . . . . . . . . . 320
11.1.3 The Error Function
. . . . . . . . . . . . . . . . . . . . 322
11.1.4 Elliptic Functions . . . . . . . . . . . . . . . . . . . . . . 322
11.2 Power Series as Functions . . . . . . . . . . . . . . . . . . . . . 327
11.2.1 Hypergeometric Functions . . . . . . . . . . . . . . . . . 328
11.2.2 Conﬂuent Hypergeometric Functions . . . . . . . . . . . 332
11.2.3 Bessel Functions . . . . . . . . . . . . . . . . . . . . . . 333
11.3 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336
IV
Analysis of Vectors
341
12 Vectors and Derivatives
343
12.1 Solid Angle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
12.1.1 Ordinary Angle Revisited . . . . . . . . . . . . . . . . . 344
12.1.2 Solid Angle . . . . . . . . . . . . . . . . . . . . . . . . . 347
12.2 Time Derivative of Vectors
. . . . . . . . . . . . . . . . . . . . 350
12.2.1 Equations of Motion in a Central Force Field . . . . . . 352
12.3 The Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
12.3.1 Gradient and Extremum Problems . . . . . . . . . . . . 359
12.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
13 Flux and Divergence
365
13.1 Flux of a Vector Field . . . . . . . . . . . . . . . . . . . . . . . 365
13.1.1 Flux Through an Arbitrary Surface
. . . . . . . . . . . 370
13.2 Flux Density = Divergence
. . . . . . . . . . . . . . . . . . . . 371
13.2.1 Flux Density . . . . . . . . . . . . . . . . . . . . . . . . 371
13.2.2 Divergence Theorem . . . . . . . . . . . . . . . . . . . . 374
13.2.3 Continuity Equation . . . . . . . . . . . . . . . . . . . . 378
13.3 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
14 Line Integral and Curl
387
14.1 The Line Integral . . . . . . . . . . . . . . . . . . . . . . . . . . 387
14.2 Curl of a Vector Field and Stokes’ Theorem . . . . . . . . . . . 391
14.3 Conservative Vector Fields . . . . . . . . . . . . . . . . . . . . . 398
14.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404

CONTENTS
xix
15 Applied Vector Analysis
407
15.1 Double Del Operations . . . . . . . . . . . . . . . . . . . . . . . 407
15.2 Magnetic Multipoles . . . . . . . . . . . . . . . . . . . . . . . . 409
15.3 Laplacian . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
15.3.1 A Primer of Fluid Dynamics
. . . . . . . . . . . . . . . 413
15.4 Maxwell’s Equations . . . . . . . . . . . . . . . . . . . . . . . . 415
15.4.1 Maxwell’s Contribution
. . . . . . . . . . . . . . . . . . 416
15.4.2 Electromagnetic Waves in Empty Space . . . . . . . . . 417
15.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
16 Curvilinear Vector Analysis
423
16.1 Elements of Length . . . . . . . . . . . . . . . . . . . . . . . . . 423
16.2 The Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425
16.3 The Divergence . . . . . . . . . . . . . . . . . . . . . . . . . . . 427
16.4 The Curl
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
16.4.1 The Laplacian
. . . . . . . . . . . . . . . . . . . . . . . 435
16.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 436
17 Tensor Analysis
439
17.1 Vectors and Indices . . . . . . . . . . . . . . . . . . . . . . . . . 439
17.1.1 Transformation Properties of Vectors . . . . . . . . . . . 441
17.1.2 Covariant and Contravariant Vectors . . . . . . . . . . . 445
17.2 From Vectors to Tensors . . . . . . . . . . . . . . . . . . . . . . 447
17.2.1 Algebraic Properties of Tensors . . . . . . . . . . . . . . 450
17.2.2 Numerical Tensors . . . . . . . . . . . . . . . . . . . . . 452
17.3 Metric Tensor . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
17.3.1 Index Raising and Lowering . . . . . . . . . . . . . . . . 457
17.3.2 Tensors and Electrodynamics . . . . . . . . . . . . . . . 459
17.4 Diﬀerentiation of Tensors
. . . . . . . . . . . . . . . . . . . . . 462
17.4.1 Covariant Diﬀerential and Aﬃne Connection
. . . . . . 462
17.4.2 Covariant Derivative . . . . . . . . . . . . . . . . . . . . 464
17.4.3 Metric Connection . . . . . . . . . . . . . . . . . . . . . 465
17.5 Riemann Curvature Tensor
. . . . . . . . . . . . . . . . . . . . 468
17.6 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 471
V
Complex Analysis
475
18 Complex Arithmetic
477
18.1 Cartesian Form of Complex Numbers . . . . . . . . . . . . . . . 477
18.2 Polar Form of Complex Numbers . . . . . . . . . . . . . . . . . 482
18.3 Fourier Series Revisited
. . . . . . . . . . . . . . . . . . . . . . 488
18.4 A Representation of Delta Function
. . . . . . . . . . . . . . . 491
18.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 493

xx
CONTENTS
19 Complex Derivative and Integral
497
19.1 Complex Functions . . . . . . . . . . . . . . . . . . . . . . . . . 497
19.1.1 Derivatives of Complex Functions . . . . . . . . . . . . . 499
19.1.2 Integration of Complex Functions . . . . . . . . . . . . . 503
19.1.3 Cauchy Integral Formula
. . . . . . . . . . . . . . . . . 508
19.1.4 Derivatives as Integrals
. . . . . . . . . . . . . . . . . . 509
19.2 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 511
20 Complex Series
515
20.1 Power Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516
20.2 Taylor and Laurent Series . . . . . . . . . . . . . . . . . . . . . 518
20.3 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
21 Calculus of Residues
525
21.1 The Residue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 525
21.2 Integrals of Rational Functions . . . . . . . . . . . . . . . . . . 529
21.3 Products of Rational and Trigonometric Functions . . . . . . . 532
21.4 Functions of Trigonometric Functions
. . . . . . . . . . . . . . 534
21.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
VI
Diﬀerential Equations
539
22 From PDEs to ODEs
541
22.1 Separation of Variables . . . . . . . . . . . . . . . . . . . . . . . 542
22.2 Separation in Cartesian Coordinates . . . . . . . . . . . . . . . 544
22.3 Separation in Cylindrical Coordinates
. . . . . . . . . . . . . . 547
22.4 Separation in Spherical Coordinates
. . . . . . . . . . . . . . . 548
22.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550
23 First-Order Diﬀerential Equations
551
23.1 Normal Form of a FODE
. . . . . . . . . . . . . . . . . . . . . 551
23.2 Integrating Factors . . . . . . . . . . . . . . . . . . . . . . . . . 553
23.3 First-Order Linear Diﬀerential Equations
. . . . . . . . . . . . 556
23.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561
24 Second-Order Linear Diﬀerential Equations
563
24.1 Linearity, Superposition, and Uniqueness . . . . . . . . . . . . . 564
24.2 The Wronskian . . . . . . . . . . . . . . . . . . . . . . . . . . . 566
24.3 A Second Solution to the HSOLDE . . . . . . . . . . . . . . . . 567
24.4 The General Solution to an ISOLDE . . . . . . . . . . . . . . . 569
24.5 Sturm–Liouville Theory . . . . . . . . . . . . . . . . . . . . . . 570
24.5.1 Adjoint Diﬀerential Operators
. . . . . . . . . . . . . . 571
24.5.2 Sturm–Liouville System . . . . . . . . . . . . . . . . . . 574
24.6 SOLDEs with Constant Coeﬃcients
. . . . . . . . . . . . . . . 575
24.6.1 The Homogeneous Case . . . . . . . . . . . . . . . . . . 576
24.6.2 Central Force Problem . . . . . . . . . . . . . . . . . . . 579

CONTENTS
xxi
24.6.3 The Inhomogeneous Case . . . . . . . . . . . . . . . . . 583
24.7 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
25 Laplace’s Equation: Cartesian Coordinates
591
25.1 Uniqueness of Solutions
. . . . . . . . . . . . . . . . . . . . . . 592
25.2 Cartesian Coordinates . . . . . . . . . . . . . . . . . . . . . . . 594
25.3 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
26 Laplace’s Equation: Spherical Coordinates
607
26.1 Frobenius Method
. . . . . . . . . . . . . . . . . . . . . . . . . 608
26.2 Legendre Polynomials
. . . . . . . . . . . . . . . . . . . . . . . 610
26.3 Second Solution of the Legendre DE . . . . . . . . . . . . . . . 617
26.4 Complete Solution . . . . . . . . . . . . . . . . . . . . . . . . . 619
26.5 Properties of Legendre Polynomials . . . . . . . . . . . . . . . . 622
26.5.1 Parity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
26.5.2 Recurrence Relation . . . . . . . . . . . . . . . . . . . . 622
26.5.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . 624
26.5.4 Rodrigues Formula . . . . . . . . . . . . . . . . . . . . . 626
26.6 Expansions in Legendre Polynomials . . . . . . . . . . . . . . . 628
26.7 Physical Examples . . . . . . . . . . . . . . . . . . . . . . . . . 631
26.8 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 635
27 Laplace’s Equation: Cylindrical Coordinates
639
27.1 The ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639
27.2 Solutions of the Bessel DE . . . . . . . . . . . . . . . . . . . . . 642
27.3 Second Solution of the Bessel DE . . . . . . . . . . . . . . . . . 645
27.4 Properties of the Bessel Functions
. . . . . . . . . . . . . . . . 646
27.4.1 Negative Integer Order . . . . . . . . . . . . . . . . . . . 646
27.4.2 Recurrence Relations . . . . . . . . . . . . . . . . . . . . 646
27.4.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . 647
27.4.4 Generating Function . . . . . . . . . . . . . . . . . . . . 649
27.5 Expansions in Bessel Functions . . . . . . . . . . . . . . . . . . 653
27.6 Physical Examples . . . . . . . . . . . . . . . . . . . . . . . . . 654
27.7 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 657
28 Other PDEs of Mathematical Physics
661
28.1 The Heat Equation . . . . . . . . . . . . . . . . . . . . . . . . . 661
28.1.1 Heat-Conducting Rod . . . . . . . . . . . . . . . . . . . 662
28.1.2 Heat Conduction in a Rectangular Plate . . . . . . . . . 663
28.1.3 Heat Conduction in a Circular Plate . . . . . . . . . . . 664
28.2 The Schr¨odinger Equation . . . . . . . . . . . . . . . . . . . . . 666
28.2.1 Quantum Harmonic Oscillator
. . . . . . . . . . . . . . 667
28.2.2 Quantum Particle in a Box . . . . . . . . . . . . . . . . 675
28.2.3 Hydrogen Atom
. . . . . . . . . . . . . . . . . . . . . . 677
28.3 The Wave Equation
. . . . . . . . . . . . . . . . . . . . . . . . 680
28.3.1 Guided Waves
. . . . . . . . . . . . . . . . . . . . . . . 682

xxii
CONTENTS
28.3.2 Vibrating Membrane . . . . . . . . . . . . . . . . . . . . 686
28.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 687
VII
Special Topics
691
29 Integral Transforms
693
29.1 The Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . 693
29.1.1 Properties of Fourier Transform . . . . . . . . . . . . . . 696
29.1.2 Sine and Cosine Transforms . . . . . . . . . . . . . . . . 697
29.1.3 Examples of Fourier Transform . . . . . . . . . . . . . . 698
29.1.4 Application to Diﬀerential Equations . . . . . . . . . . . 702
29.2 Fourier Transform and Green’s Functions
. . . . . . . . . . . . 705
29.2.1 Green’s Function for the Laplacian . . . . . . . . . . . . 708
29.2.2 Green’s Function for the Heat Equation . . . . . . . . . 709
29.2.3 Green’s Function for the Wave Equation . . . . . . . . . 711
29.3 The Laplace Transform
. . . . . . . . . . . . . . . . . . . . . . 712
29.3.1 Properties of Laplace Transform
. . . . . . . . . . . . . 713
29.3.2 Derivative and Integral of the Laplace Transform . . . . 717
29.3.3 Laplace Transform and Diﬀerential Equations . . . . . . 718
29.3.4 Inverse of Laplace Transform . . . . . . . . . . . . . . . 721
29.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 723
30 Calculus of Variations
727
30.1 Variational Problem
. . . . . . . . . . . . . . . . . . . . . . . . 728
30.1.1 Euler-Lagrange Equation
. . . . . . . . . . . . . . . . . 729
30.1.2 Beltrami identity . . . . . . . . . . . . . . . . . . . . . . 731
30.1.3 Several Dependent Variables
. . . . . . . . . . . . . . . 734
30.1.4 Several Independent Variables . . . . . . . . . . . . . . . 734
30.1.5 Second Variation . . . . . . . . . . . . . . . . . . . . . . 735
30.1.6 Variational Problems with Constraints . . . . . . . . . . 738
30.2 Lagrangian Dynamics
. . . . . . . . . . . . . . . . . . . . . . . 740
30.2.1 From Newton to Lagrange . . . . . . . . . . . . . . . . . 740
30.2.2 Lagrangian Densities . . . . . . . . . . . . . . . . . . . . 744
30.3 Hamiltonian Dynamics . . . . . . . . . . . . . . . . . . . . . . . 747
30.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 750
31 Nonlinear Dynamics and Chaos
753
31.1 Systems Obeying Iterated Maps . . . . . . . . . . . . . . . . . . 754
31.1.1 Stable and Unstable Fixed Points . . . . . . . . . . . . . 755
31.1.2 Bifurcation . . . . . . . . . . . . . . . . . . . . . . . . . 757
31.1.3 Onset of Chaos . . . . . . . . . . . . . . . . . . . . . . . 761
31.2 Systems Obeying DEs . . . . . . . . . . . . . . . . . . . . . . . 763
31.2.1 The Phase Space . . . . . . . . . . . . . . . . . . . . . . 764
31.2.2 Autonomous Systems
. . . . . . . . . . . . . . . . . . . 766
31.2.3 Onset of Chaos . . . . . . . . . . . . . . . . . . . . . . . 770

CONTENTS
xxiii
31.3 Universality of Chaos . . . . . . . . . . . . . . . . . . . . . . . . 773
31.3.1 Feigenbaum Numbers
. . . . . . . . . . . . . . . . . . . 773
31.3.2 Fractal Dimension . . . . . . . . . . . . . . . . . . . . . 775
31.4 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 778
32 Probability Theory
781
32.1 Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . 781
32.1.1 A Set Theory Primer . . . . . . . . . . . . . . . . . . . . 782
32.1.2 Sample Space and Probability . . . . . . . . . . . . . . . 784
32.1.3 Conditional and Marginal Probabilities
. . . . . . . . . 786
32.1.4 Average and Standard Deviation . . . . . . . . . . . . . 789
32.1.5 Counting: Permutations and Combinations . . . . . . . 791
32.2 Binomial Probability Distribution . . . . . . . . . . . . . . . . . 792
32.3 Poisson Distribution . . . . . . . . . . . . . . . . . . . . . . . . 797
32.4 Continuous Random Variable . . . . . . . . . . . . . . . . . . . 801
32.4.1 Transformation of Variables . . . . . . . . . . . . . . . . 804
32.4.2 Normal Distribution . . . . . . . . . . . . . . . . . . . . 806
32.5 Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 809
Bibliography
815
Index
817

Part I
Coordinates and Calculus

Chapter 1
Coordinate Systems
and Vectors
Coordinates and vectors—in one form or another—are two of the most
fundamental concepts in any discussion of mathematics as applied to physi-
cal problems. So, it is beneﬁcial to start our study with these two concepts.
Both vectors and coordinates have generalizations that cover a wide vari-
ety of physical situations including not only ordinary three-dimensional space
with its ordinary vectors, but also the four-dimensional spacetime of relativity
with its so-called four vectors, and even the inﬁnite-dimensional spaces used
in quantum physics with their vectors of inﬁnite components. Our aim in this
chapter is to review the ordinary space and how it is used to describe physical
phenomena. To facilitate this discussion, we ﬁrst give an outline of some of
the properties of vectors.
1.1
Vectors in a Plane and in Space
We start with the most common deﬁnition of a vector as a directed line
segment without regard to where the vector is located. In other words, a vector
is a directed line segment whose only important attributes are its direction
and its length. As long as we do not change these two attributes, the vector is
general properties
of vectors
not aﬀected. Thus, we are allowed to move a vector parallel to itself without
changing the vector. Examples of vectors1 are position r, displacement Δr,
velocity v, momentum p, electric ﬁeld E, and magnetic ﬁeld B. The vector
that has no length is called the zero vector and is denoted by 0.
Vectors would be useless unless we could perform some kind of operation
on them. The most basic operation is changing the length of a vector. This
is accomplished by multiplying the vector by a real positive number.
For
example, 3.2r is a vector in the same direction as r but 3.2 times longer. We
1Vectors will be denoted by Roman letters printed in boldface type.

4
Coordinate Systems and Vectors
a
b
a
a
b
b
b + a
a + b
Δr1
Δr2
ΔR = Δr1 + Δr2
(a)
(b)
A
C
B
Figure 1.1: Illustration of the commutative law of addition of vectors.
can ﬂip the direction of a vector by multiplying it by −1. That is, (−1) × r =
−r is a vector having the same length as r but pointing in the opposite
direction. We can combine these two operations and think of multiplying a
vector by any real (positive or negative) number. The result is another vector
operations on
vectors
lying along the same line as the original vector. Thus, −0.732r is a vector
that is 0.732 times as long as r and points in the opposite direction. The zero
vector is obtained every time one multiplies any vector by the number zero.
Another operation is the addition of two vectors. This operation, with
which we assume the reader to have some familiarity, is inspired by the obvious
addition law for displacements. In Figure 1.1(a), a displacement, Δr1 from
A to B is added to the displacement Δr2 from B to C to give ΔR their
resultant, or their sum, i.e., the displacement from A to C: Δr1 +Δr2 = ΔR.
Figure 1.1(b) shows that addition of vectors is commutative: a + b = b + a.
It is also associative, a + (b + c) = (a + b) + c, i.e., the order in which you
add vectors is irrelevant. It is clear that a + 0 = 0 + a = a for any vector a.
Example 1.1.1. The parametric equation of a line through two given points
can be obtained in vector form by noting that any point in space deﬁnes a vector
whose components are the coordinates of the given point.2 If the components of
the points P and Q in Figure 1.2 are, respectively, (px, py, pz) and (qx, qy, qz), then
we can deﬁne vectors p and q with those components. An arbitrary point X with
components (x, y, z) will lie on the line PQ if and only if the vector x = (x, y, z)
has its tip on that line. This will happen if and only if the vector joining P and X,
namely x −p, is proportional to the vector joining P and Q, namely q −p. Thus,
for some real number t, we must have
vector form of the
parametric
equation of a line
x −p = t(q −p)
or
x = t(q −p) + p.
This is the vector form of the equation of a line. We can write it in component
form by noting that the equality of vectors implies the equality of corresponding
components. Thus,
x = (qx −px)t + px,
y = (qy −py)t + py,
z = (qz −pz)t + pz,
which is the usual parametric equation for a line.
■
2We shall discuss components and coordinates in greater detail later in this chapter. For
now, the knowledge gained in calculus is suﬃcient for our discussion.

1.1 Vectors in a Plane and in Space
5
O
P
Q
X
x
y
z
p
q
x
Figure 1.2: The parametric equation of a line in space can be obtained easily using
vectors.
There are some special vectors that are extremely useful in describing
physical quantities. These are the unit vectors.
If one divides a vector
use of unit vectors
by its length, one gets a unit vector in the direction of the original vector.
Unit vectors are generally denoted by the symbol ˆe with a subscript which
designates its direction. Thus, if we divided the vector a by its length |a| we
get the unit vector ˆea in the direction of a. Turning this deﬁnition around,
we have
Box 1.1.1. If we know the magnitude |a| of a vector quantity as well as
its direction ˆea, we can construct the vector: a = |a|ˆea.
This construction will be used often in the sequel.
The most commonly used unit vectors are those in the direction of coor-
unit vectors along
the x-, y-, and
z-axes
dinate axes. Thus ˆex, ˆey, and ˆez are the unit vectors pointing in the positive
directions of the x-, y-, and z-axes, respectively.3
We shall introduce unit
vectors in other coordinate systems when we discuss those coordinate systems
later in this chapter.
1.1.1
Dot Product
The reader is no doubt familiar with the concept of dot product whereby
two vectors are “multiplied” and the result is a number. The dot product of
a and b is deﬁned by
dot product
deﬁned
a · b ≡|a| |b| cos θ,
(1.1)
where |a| is the length of a, |b| is the length of b, and θ is the angle between
the two vectors. This deﬁnition is motivated by many physical situations.
3These unit vectors are usually denoted by i, j, and k, a notation that can be confusing
when other non-Cartesian coordinates are used. We shall not use this notation, but adhere
to the more suggestive notation introduced above.

6
Coordinate Systems and Vectors
N
v
F
Figure 1.3: No work is done by a force orthogonal to displacement. If such a work
were not zero, it would have to be positive or negative; but no consistent rule exists to
assign a sign to the work.
The prime example is work which is deﬁned as the scalar product of force and
displacement. The presence of cos θ ensures the requirement that the work
done by a force perpendicular to the displacement is zero. If this requirement
were not met, we would have the precarious situation of Figure 1.3 in which
the two vertical forces add up to zero but the total work done by them is
not zero! This is because it would be impossible to assign a “sign” to the
work done by forces being displaced perpendicular to themselves, and make
the rule of such an assignment in such a way that the work of F in the ﬁgure
cancels that of N. (The reader is urged to try to come up with a rule—e.g.,
assigning a positive sign to the work if the velocity points to the right of the
observer and a negative sign if it points to the observer’s left—and see that it
will not work, no matter how elaborate it may be!) The only logical deﬁnition
of work is that which includes a cos θ factor.
The dot product is clearly commutative, a · b = b · a. Moreover, it dis-
properties of dot
product
tributes over vector addition
(a + b) · c = a · c + b · c.
To see this, note that Equation (1.1) can be interpreted as the product of the
length of a with the projection of b along a. Now Figure 1.4 demonstrates4
that the projection of a + b along c is the sum of the projections of a and b
along c (see Problem 1.2 for details). The third property of the inner product
is that a · a is always a positive number unless a is the zero vector in which
case a · a = 0. In mathematics, the collection of these three properties—
properties deﬁning
the dot (inner)
product
commutativity, positivity, and distribution over addition—deﬁnes a dot (or
inner) product on a vector space.
The deﬁnition of the dot product leads directly to a · a = |a|2 or
|a| = √a · a,
(1.2)
which is useful in calculating the length of sums or diﬀerences of vectors.
4Figure 1.4 appears to prove the distributive property only for vectors lying in the same
plane.
However, the argument will be valid even if the three vectors are not coplanar.
Instead of dropping perpendicular lines from the tips of a and b, one drops perpendicular
planes.

1.1 Vectors in a Plane and in Space
7
A
B
O
a
b
a+b
c
Proj. of a
Proj. of b
Figure 1.4: The distributive property of the dot product is clearly demonstrated if we
interpret the dot product as the length of one vector times the projection of the other
vector on the ﬁrst.
One can use the distributive property of the dot product to show that
if (ax, ay, az) and (bx, by, bz) represent the components of a and b along the
axes x, y, and z, then
dot product in
terms of
components
a · b = axbx + ayby + azbz.
(1.3)
From the deﬁnition of the dot product, we can draw an important conclu-
sion. If we divide both sides of a · b = |a| |b| cos θ by |a|, we get
a · b
|a|
= |b| cosθ
or
 a
|a|

· b = |b| cos θ ⇒ˆea · b = |b| cos θ.
Noting that |b| cos θ is simply the projection of b along a, we conclude
a useful relation to
be used frequently
in the sequel
Box 1.1.2. To ﬁnd the perpendicular projection of a vector b along
another vector a, take the dot product of b with ˆea, the unit vector along a.
Sometimes “component” is used for perpendicular projection.
This is not
entirely correct. For any set of three mutually perpendicular unit vectors in
space, Box 1.1.2 can be used to ﬁnd the components of a vector along the
three unit vectors. Only if the unit vectors are mutually perpendicular do
components and projections coincide.
1.1.2
Vector or Cross Product
Given two space vectors, a and b, we can ﬁnd a third space vector c, called
the cross product of a and b, and denoted by c = a × b. The magnitude
cross product of
two space vectors
of c is deﬁned by |c| = |a| |b| sin θ where θ is the angle between a and b.
The direction of c is given by the right-hand rule: If a is turned to b (note
right-hand rule
explained
the order in which a and b appear here) through the angle between a and b,

8
Coordinate Systems and Vectors
a (right-handed) screw that is perpendicular to a and b will advance in the
direction of a × b. This deﬁnition implies that
a × b = −b × a.
This property is described by saying that the cross product is antisymmet-
cross product is
antisymmetric
ric. The deﬁnition also implies that
a · (a × b) = b · (a × b) = 0.
That is, a × b is perpendicular to both a and b.5
The vector product has the following properties:
a × (αb) = (αa) × b = α(a × b),
a × b = −b × a,
a × (b + c) = a × b + a × c,
a × a = 0.
(1.4)
Using these properties, we can write the vector product of two vectors in terms
of their components. We are interested in a more general result valid in other
coordinate systems as well. So, rather than using x, y, and z as subscripts for
unit vectors, we use the numbers 1, 2, and 3. In that case, our results can
cross product in
terms of
components
also be used for spherical and cylindrical coordinates which we shall discuss
shortly.
a × b = (α1ˆe1 + α2ˆe2 + α3ˆe3) × (β1ˆe1 + β2ˆe2 + β3ˆe3)
= α1β1ˆe1 × ˆe1 + α1β2ˆe1 × ˆe2 + α1β3ˆe1 × ˆe3
+ α2β1ˆe2 × ˆe1 + α2β2ˆe2 × ˆe2 + α2β3ˆe2 × ˆe3
+ α3β1ˆe3 × ˆe1 + α3β2ˆe3 × ˆe2 + α3β3ˆe3 × ˆe3.
But, by the last property of Equation (1.4), we have
ˆe1 × ˆe1 = ˆe2 × ˆe2 = ˆe3 × ˆe3 = 0.
Also, if we assume that ˆe1, ˆe2, and ˆe3 form a so-called right-handed set,
i.e., if
right-handed set
of unit vectors
ˆe1 × ˆe2 = −ˆe2 × ˆe1 = ˆe3,
ˆe1 × ˆe3 = −ˆe3 × ˆe1 = −ˆe2,
(1.5)
ˆe2 × ˆe3 = −ˆe3 × ˆe2 = ˆe1,
then we obtain
a × b = (α2β3 −α3β2)ˆe1 + (α3β1 −α1β3)ˆe2 + (α1β2 −α2β1)ˆe3
5This fact makes it clear why a × b is not deﬁned in the plane. Although it is possible
to deﬁne a × b for vectors a and b lying in a plane, a × b will not lie in that plane (it
will be perpendicular to that plane). For the vector product, a and b (although lying in a
plane) must be considered as space vectors.

1.1 Vectors in a Plane and in Space
9
e1
e2
e3
α1
α2
α3
β1
β2
β3
det
e1
e2
e3
α1
α2
α3
β1
β2
β3
e1
e2
e3
α1
α2
α3
β1
β2
β3
=
Figure 1.5: A 3 × 3 determinant is obtained by writing the entries twice as shown,
multiplying all terms on each slanted line and adding the results. The lines from upper
left to lower right bear a positive sign, and those from upper right to lower left a negative
sign.
which can be nicely written in a determinant form6
cross product in
terms of the
determinant of
components
a × b = det
⎛
⎝
ˆe1
ˆe2
ˆe3
α1
α2
α3
β1
β2
β3
⎞
⎠.
(1.6)
Figure 1.5 explains the rule for “expanding” a determinant.
Example 1.1.2. From the deﬁnition of the vector product and Figure 1.6(a),
we note that
area of a
parallelogram in
terms of cross
product of its two
sides
|a × b| = area of the parallelogram deﬁned by a and b.
So we can use Equation (1.6) to ﬁnd the area of a parallelogram deﬁned by two
vectors directly in terms of their components. For instance, the area deﬁned by
a = (1, 1, −2) and b = (2, 0, 3) can be found by calculating their vector product
a × b = det
⎛
⎝
ˆe1
ˆe2
ˆe3
1
1
−2
2
0
3
⎞
⎠= 3ˆe1 −7ˆe2 −2ˆe3,
and then computing its length
|a × b| =
	
32 + (−7)2 + (−2)2 =
√
62.
■
a
b
c
θ θ |a| cos θ
a
b
|a| sin θ
θ
b × c
(a)
(b)
Figure 1.6: (a) The area of a parallelogram is the absolute value of the cross product of
the two vectors describing its sides. (b) The volume of a parallelepiped can be obtained
by mixing the dot and the cross products.
6No knowledge of determinants is necessary at this point. The reader may consider (1.6)
to be a mnemonic device useful for remembering the components of a × b.

10
Coordinate Systems and Vectors
Example 1.1.3. The volume of a parallelepiped deﬁned by three non-coplanar
vectors, a, b, and c, is given by |a · (b × c)|. This can be seen from Figure 1.6(b),
where it is clear that
volume of a
parallelepiped as a
combination of
dot and cross
products
volume = (area of base)(altitude) = |b × c|(|a| cos θ) = |(b × c) · a|.
The absolute value is taken to ensure the positivity of the area. In terms of compo-
nents we have
volume = |(b × c)1α1 + (b × c)2α2 + (b × c)3α3|
= |(β2γ3 −β3γ2)α1 + (β3γ1 −β1γ3)α2 + (β1γ2 −β2γ1)α3|,
which can be written in determinant form as
volume of a
parallelepiped as
the determinant of
the components of
its side vectors
volume = |a · (b × c)| =






det
⎛
⎝
α1
α2
α3
β1
β2
β3
γ1
γ2
γ3
⎞
⎠






.
Note how we have put the absolute value sign around the determinant of the matrix,
so that the area comes out positive.
■
Historical Notes
The concept of vectors as directed line segments that could represent velocities,
forces, or accelerations has a very long history. Aristotle knew that the eﬀect of two
forces acting on an object could be described by a single force using what is now
called the parallelogram law. However, the real development of the concept took an
unexpected turn in the nineteenth century.
With the advent of complex numbers and the realization by Gauss, Wessel, and
especially Argand, that they could be represented by points in a plane, mathemati-
cians discovered that complex numbers could be used to study vectors in a plane.
A complex number is represented by a pair7 of real numbers—called the real and
imaginary parts of the complex number—which could be considered as the two
components of a planar vector.
This connection between vectors in a plane and complex numbers was well es-
tablished by 1830. Vectors are, however, useful only if they are treated as objects
in space. After all, velocities, forces, and accelerations are mostly three-dimensional
objects. So, the two-dimensional complex numbers had to be generalized to three
dimensions. This meant inventing ways of adding, subtracting, multiplying, and
dividing objects such as (x, y, z).
The invention of a spatial analogue of the planar complex numbers is due to
William R. Hamilton. Next to Newton, Hamilton is the greatest of all English
William R.
Hamilton
1805–1865
mathematicians, and like Newton he was even greater as a physicist than as a
mathematician. At the age of ﬁve Hamilton could read Latin, Greek, and Hebrew.
At eight he added Italian and French; at ten he could read Arabic and Sanskrit,
and at fourteen, Persian.
A contact with a lightning calculator inspired him to
study mathematics. In 1822 at the age of seventeen and a year before he entered
Trinity College in Dublin, he prepared a paper on caustics which was read before the
Royal Irish Academy in 1824 but not published. Hamilton was advised to rework
and expand it. In 1827 he submitted to the Academy a revision which initiated the
science of geometrical optics and introduced new techniques in analytical mechanics.
7See Chapter 18.

1.2 Coordinate Systems
11
In 1827, while still an undergraduate, he was appointed Professor of Astronomy
at Trinity College in which capacity he had to manage the astronomical observations
and teach science. He did not do much of the former, but he was a ﬁne lecturer.
Hamilton had very good intuition, and knew how to use analogy to reason from
the known to the unknown. Although he lacked great ﬂashes of insight, he worked
very hard and very long on special problems to see what generalizations they would
lead to. He was patient and systematic in working on speciﬁc problems and was
willing to go through detailed and laborious calculations to check or prove a point.
After mastering and clarifying the concept of complex numbers and their relation
to planar vectors (see Problem 18.11 for the connection between complex multiplica-
tion on the one hand, and dot and cross products on the other), Hamilton was able
to think more clearly about the three-dimensional generalization.
His eﬀorts led
unfortunately to frustration because the vectors (a) required four components, and
(b) deﬁed commutativity! Both features were revolutionary and set the standard
for algebra. He called these new numbers quaternions.
In retrospect, one can see that the new three-dimensional complex numbers had
to contain four components. Each “number,” when acting on a vector, rotates the
latter about an axis and stretches (or contracts) it.
Two angles are required to
specify the axis of rotation, one angle to specify the amount of rotation, and a
fourth number to specify the amount of stretch (or contraction).
Hamilton announced the invention of quaternions in 1843 at a meeting of the
Royal Irish Academy, and spent the rest of his life developing the subject.
1.2
Coordinate Systems
Coordinates are “functions” that specify points of a space.
The smallest
number of these functions necessary to specify a point is called the dimension
of that space. For instance, a point of a plane is speciﬁed by two numbers, and
as the point moves in the plane the two numbers change, i.e., the coordinates
are functions of the position of the point. If we designate the point as P, we
may write the coordinate functions of P as (f(P), g(P)).8 Each pair of such
coordinate
systems as
functions.
functions is called a coordinate system.
There are two coordinate systems used for a plane, Cartesian, denoted
by (x(P), y(P)), and polar, denoted by (r(P), θ(P)). As shown in Figure 1.7,
P
y(P)
x(P)
O
P
O
θ(P)
r(P)
Figure 1.7: Cartesian and polar coordinates of a point P in two dimensions.
8Think of f (or g) as a rule by which a unique number is assigned to each point P .

12
Coordinate Systems and Vectors
the “function” x is deﬁned as giving the distance from P to the vertical axis,
while θ is the function which gives the angle that the line OP makes with a
given ﬁducial (usually horizontal) line. The origin O and the ﬁducial line are
completely arbitrary. Similarly, the functions r and y give distances from the
origin and to the horizontal axis, respectively.
Box 1.2.1. In practice, one drops the argument P and writes (x, y) and
(r, θ).
We can generalize the above concepts to three dimensions. There are three
coordinate functions now. So for a point P in space we write
the three common
coordinate
systems:
Cartesian,
cylindrical and
spherical
(f(P), g(P), h(P)),
where f, g, and h are functions on the three-dimensional space. There are
three widely used coordinate systems, Cartesian (x(P), y(P), z(P)), cylin-
drical (ρ(P), ϕ(P), z(P)), and spherical (r(P), θ(P), ϕ(P)). ϕ(P) is called
the azimuth or the azimuthal angle of P, while θ(P) is called its polar
angle. To ﬁnd the spherical coordinates of P, one chooses an arbitrary point
as the origin O and an arbitrary line through O called the polar axis. One
measures OP and calls it r(P); θ(P) is the angle between OP and the polar
axis. To ﬁnd the third coordinate, we construct the plane through O and per-
pendicular to the polar axis, drop a projection from P to the plane meeting
the latter at H, draw an arbitrary ﬁducial line through O in this plane, and
measure the angle between this line and OH. This angle is ϕ(P). Cartesian
and cylindrical coordinate systems can be described similarly. The three co-
ordinate systems are shown in Figure 1.8. As indicated in the ﬁgure, the polar
axis is usually taken to be the z-axis, and the ﬁducial line from which ϕ(P)
is measured is chosen to be the x-axis. Although there are other coordinate
systems, the three mentioned above are by far the most widely used.
x
y
z
x(P)
y(P)
z(P)
P
(a)
(b)
x
y
z
P
z(P)
(P)
H
ρ (P)
ϕ
(c)
x
y
z
P
H
(P)
r (P)
θ (P)
ϕ
Figure 1.8: (a) Cartesian, (b) cylindrical, and (c) spherical coordinates of a point P in
three dimensions.

1.2 Coordinate Systems
13
Which one of the three systems of coordinates to use in a given physi-
cal problem is dictated mainly by the geometry of that problem. As a rule,
spherical coordinates are best suited for spheres and spherically symmetric
problems. Spherical symmetry describes situations in which quantities of in-
terest are functions only of the distance from a ﬁxed point and not on the
orientation of that distance. Similarly, cylindrical coordinates ease calcula-
tions when cylinders or cylindrical symmetries are involved. Finally, Cartesian
coordinates are used in rectangular geometries.
Of the three coordinate systems, Cartesian is the most complete in the
following sense: A point in space can have only one triplet as its coordinates.
This property is not shared by the other two systems. For example, a point
limitations of
non-Cartesian
coordinates
P located on the z-axis of a cylindrical coordinate system does not have a
well-deﬁned ϕ(P). In practice, such imperfections are not of dire consequence
and we shall ignore them.
Once we have three coordinate systems to work with, we need to know
how to translate from one to another. First we give the transformation rule
from spherical to cylindrical. It is clear from Figure 1.9 that
transformation
from spherical to
cylindrical
coordinates
ρ = r sin θ,
ϕcyl = ϕsph,
z = r cos θ.
(1.7)
Thus, given (r, θ, ϕ) of a point P, we can obtain (ρ, ϕ, z) of the same point by
substituting in the RHS.
Next we give the transformation rule from cylindrical to Cartesian. Again
transformation
from cylindrical to
Cartesian
coordinates
Figure 1.9 gives the result:
x = ρ cosϕ,
y = ρ sin ϕ,
zcar = zcyl.
(1.8)
We can combine (1.7) and (1.8) to connect Cartesian and spherical coordi-
transformation
from spherical to
Cartesian
coordinates
nates:
x = r sin θ cos ϕ,
y = r sin θ sin ϕ,
z = r cos θ.
(1.9)
x
y
z
P
θ
r
ρ
ρ
ϕ
Figure 1.9: The relation between the cylindrical and spherical coordinates of a point
P can be obtained using this diagram.

14
Coordinate Systems and Vectors
Box 1.2.2. Equations (1.7)–(1.9) are extremely important and worth be-
ing committed to memory.
The reader is advised to study Figure 1.9
carefully and learn to reproduce (1.7)–(1.9) from the ﬁgure!
The transformations given are in their standard form. We can turn them
around and give the inverse transformations. For instance, squaring the ﬁrst
and third equations of (1.7) and adding gives ρ2 + z2 = r2 or r =
	
ρ2 + z2.
Similarly, dividing the ﬁrst and third equation yields tan θ = ρ/z, which
implies that θ = tan−1(ρ/z), or equivalently,
z
r = cos θ ⇒θ = cos−1 z
r

= cos−1

z
	
ρ2 + z2

.
Thus, the inverse of (1.7) is
transformation
from cylindrical to
spherical
coordinates
r =
	
ρ2 + z2,
θ = tan−1 ρ
z

= cos−1

z
	
ρ2 + z2

,
ϕsph = ϕcyl.
(1.10)
Similarly, the inverse of (1.8) is
ρ =
	
x2 + y2,
ϕ = tan−1 y
x

= cos−1

x
	
x2 + y2

= sin−1

y
	
x2 + y2

,
(1.11)
zcyl = zcar,
and that of (1.9) is
transformation
from Cartesian to
spherical
coordinates
r =
	
x2 + y2 + z2,
θ = tan−1
	
x2 + y2
z

= cos−1

z
	
x2 + y2 + z2

= sin−1

	
x2 + y2
	
x2 + y2 + z2

,
(1.12)
ϕ = tan−1 y
x

= cos−1
x
	
x2 + y2

= sin−1
y
	
x2 + y2

.
An important question concerns the range of these quantities. In other
words: In what range should we allow these quantities to vary in order to cover
the whole space? For Cartesian coordinates all three variables vary between
−∞and +∞. Thus,
range of
coordinate
variables
−∞< x < +∞,
−∞< y < +∞,
−∞< z < +∞.
The ranges of cylindrical coordinates are
0 ≤ρ < ∞,
0 ≤ϕ ≤2π,
−∞< z < ∞.

1.2 Coordinate Systems
15
Note that ρ, being a distance, cannot have negative values.9 Similarly, the
ranges of spherical coordinates are
0 ≤r < ∞,
0 ≤θ ≤π,
0 ≤ϕ ≤2π.
Again, r is never negative for similar reasons as above. Also note that the
range of θ excludes values larger than π. This is because the range of ϕ takes
care of points where θ “appears” to have been increased by π.
Historical Notes
One of the greatest achievements in the development of mathematics since Euclid
was the introduction of coordinates. Two men take credit for this development: Fer-
mat and Descartes. These two great French mathematicians were interested in the
uniﬁcation of geometry and algebra, which resulted in the creation of a most fruitful
branch of mathematics now called analytic geometry. Fermat and Descartes who
were heavily involved in physics, were keenly aware of both the need for quantitative
methods and the capacity of algebra to deliver that method.
Fermat’s interest in the uniﬁcation of geometry and algebra arose because of his
involvement in optics. His interest in the attainment of maxima and minima—thus
Pierre de Fermat
1601–1665
his contribution to calculus—stemmed from the investigation of the passage of light
rays through media of diﬀerent indices of refraction, which resulted in Fermat’s
principle in optics and the law of refraction. With the introduction of coordinates,
Fermat was able to quantify the study of optics and set a trend to which all physicists
of posterity would adhere.
It is safe to say that without analytic geometry the
progress of science, and in particular physics, would have been next to impossible.
Born into a family of tradespeople, Pierre de Fermat was trained as a lawyer
and made his living in this profession becoming a councillor of the parliament of
the city of Toulouse. Although mathematics was but a hobby for him and he could
devote only spare time to it, he made great contributions to number theory, to
calculus, and, together with Pascal, initiated work on probability theory.
The coordinate system introduced by Fermat was not a convenient one. For one
thing, the coordinate axes were not at right angles to one another. Furthermore,
the use of negative coordinates was not considered. Nevertheless, he was able to
translate geometric curves into algebraic equations.
Ren´e Descartes was a great philosopher, a founder of modern biology, and a
superb physicist and mathematician. His interest in mathematics stemmed from his
desire to understand nature. He wrote:
. . . I have resolved to quit only abstract geometry, that is to say, the
consideration of questions which serve only to exercise the mind, and
this, in order to study another kind of geometry, which has for its object
the explanation of the phenomena of nature.
His father, a relatively wealthy lawyer, sent him to a Jesuit school at the age
Ren´e Descartes
1596–1650
of eight where, due to his delicate health, he was allowed to spend the mornings in
bed, during which time he worked. He followed this habit during his entire life. At
twenty he graduated from the University of Poitier as a lawyer and went to Paris
where he studied mathematics with a Jesuit priest. After one year he decided to
9In some calculus books ρ is allowed to have negative values to account for points on the
opposite side of the origin. However, in physics literature ρ is assumed to be positive.To go
to “the other side” of the origin along ρ, we change ϕ by π, keeping ρ positive at all times.

16
Coordinate Systems and Vectors
join the army of Prince Maurice of Orange in 1617. During the next nine years he
vacillated between various armies while studying mathematics.
He eventually returned to Paris, where he devoted his eﬀorts to the study of
optical instruments motivated by the newly discovered power of the telescope. In
1628 he moved to Holland to a quieter and freer intellectual environment. There he
lived for the next twenty years and wrote his famous works. In 1649 Queen Christina
of Sweden persuaded Descartes to go to Stockholm as her private tutor. However
the Queen had an uncompromising desire to draw curves and tangents at 5 a.m.,
causing Descartes to break the lifelong habit of getting up at 11 o’clock! After only
a few months in the cold northern climate, walking to the palace for the 5 o’clock
appointment with the queen, he died of pneumonia in 1650.
Descartes described his algebraic approach to geometry in his monumental work
La G´eom´etrie. It is in this work that he solves geometrical problems using algebra
by introducing coordinates. These coordinates, as in Fermat’s case, were not lengths
along perpendicular axes. Nevertheless they paved the way for the later generations
of scientists such as Newton to build on Descartes’ and Fermat’s ideas and improve
on them.
Throughout the seventeenth century, mathematicians used one axis with the y
values drawn at an oblique or right angle onto that axis. Newton, however, in a book
Newton uses polar
coordinates for the
ﬁrst time
called The Method of Fluxions and Inﬁnite Series written in 1671, and translated
much later into English in 1736, describes a coordinate system in which points are
located in reference to a ﬁxed point and a ﬁxed line through that point. This was
the ﬁrst introduction of essentially the polar coordinates we use today.
1.3
Vectors in Diﬀerent Coordinate Systems
Many physical situations require the study of vectors in diﬀerent coordinate
systems. For example, the study of the solar system is best done in spherical
coordinates because of the nature of the gravitational force. Similarly calcu-
lation of electromagnetic ﬁelds in a cylindrical cavity will be easier if we use
cylindrical coordinates. This requires not only writing functions in terms of
these coordinate variables, but also expressing vectors in terms of unit vectors
suitable for these coordinate systems. It turns out that, for the three coordi-
nate systems described above, the most natural construction of such vectors
renders them mutually perpendicular.
Any set of three (two) mutually perpendicular unit vectors in space (in the
plane) is called an orthonormal basis.10 Basis vectors have the property
orthonormal basis
that any vector can be written in terms of them.
Let us start with the plane in which the coordinate system could be Carte-
sian or polar. In general, we construct an orthonormal basis at a point and
note that
10The word “orthonormal” comes from orthogonal meaning “perpendicular,” and normal
meaning “of unit length.”

1.3 Vectors in Diﬀerent Coordinate Systems
17
P
Q
ex^
ey^
ex^
ey^
(a)
P
Q
er^
er^
eθ^
(b)
eθ
^
Figure 1.10: The unit vectors in (a) Cartesian coordinates and (b) polar coordinates.
The unit vectors at P and Q are the same for Cartesian coordinates, but diﬀerent in
polar coordinates.
Box 1.3.1. The orthonormal basis, generally speaking, depends on the
point at which it is constructed.
The vectors of a basis are constructed as follows. To ﬁnd the unit vector
corresponding to a coordinate at a point P, hold the other coordinate ﬁxed
and increase the coordinate in question. The initial direction of motion of P
is the direction of the unit vector sought. Thus, we obtain the Cartesian unit
vectors at point P of Figure 1.10(a): ˆex is obtained by holding y ﬁxed and
letting x vary in the increasing direction; and ˆey is obtained by holding x ﬁxed
at P and letting y increase. In each case, the unit vectors show the initial
direction of the motion of P. It should be clear that one obtains the same set
general rule for
constructing a
basis at a point
of unit vectors regardless of the location of P. However, the reader should
take note that this is true only for coordinates that are deﬁned in terms of
axes whose directions are ﬁxed, such as Cartesian coordinates.
If we use polar coordinates for P, then holding θ ﬁxed at P gives the
direction of ˆer as shown in Figure 1.10(b), because for ﬁxed θ, that is the
direction of increase for r. Similarly, if r is ﬁxed at P, the initial direction
of motion of P when θ is increased is that of ˆeθ shown in the ﬁgure. If we
choose another point such as Q shown in the ﬁgure, then a new set of unit
vectors will be obtained which are diﬀerent form those of P. This is because
polar coordinates are not deﬁned in terms of any ﬁxed axes.
Since {ˆex, ˆey} and {ˆer, ˆeθ} form a basis in the plane, any vector a in the
plane can be expressed in terms of either basis as shown in Figure 1.11. Thus,
we can write
a = axP ˆexP + ayP ˆeyP = arP ˆerP + aθP ˆeθP = arQˆerQ + aθQˆeθQ,
(1.13)
where the coordinates are subscripted to emphasize their dependence on the
points at which the unit vectors are erected. In the case of Cartesian coor-
dinates, this, of course, is not necessary because the unit vectors happen to
be independent of the point. In the case of polar coordinates, although this

18
Coordinate Systems and Vectors
P
Q
ex^
ey^
ex^
ey^
a
a
P
Q
er^
er^
eθ
^
eθ
^
a
a
(a)
(b)
Figure 1.11: (a) The vector a has the same components along unit vectors at P and Q
in Cartesian coordinates. (b) The vector a has diﬀerent components along unit vectors
at diﬀerent points for a polar coordinate system.
dependence exists, we normally do not write the points as subscripts, being
aware of this dependence every time we use polar coordinates.
So far we have used parentheses to designate the (components of) a vector.
angle brackets
denote vector
components
Since, parentheses—as a universal notation—are used for coordinates of points,
we shall write components of a vector in angle brackets. So Equation (1.13)
can also be written as
a = ⟨ax, ay⟩P = ⟨ar, aθ⟩P = ⟨ar, aθ⟩Q,
where again the subscript indicating the point at which the unit vectors are
deﬁned is normally deleted. However, we need to keep in mind that although
⟨ax, ay⟩is independent of the point in question, ⟨ar, aθ⟩is very much point-
dependent. Caution should be exercised when using this notation as to the
location of the unit vectors.
The unit vectors in the coordinate systems of space are deﬁned the same
way. We follow the rule given before:
Box 1.3.2. (Rule for Finding Coordinate Unit Vectors). To ﬁnd
the unit vector corresponding to a coordinate at a point P, hold the other
coordinates ﬁxed and increase the coordinate in question. The initial di-
rection of motion of P is the direction of the unit vector sought.
It should be clear that the Cartesian basis {ˆex, ˆey, ˆez} is the same for all
points, and usually they are drawn at the origin along the three axes. An
arbitrary vector a can be written as
a = axˆex + ayˆey + azˆez
or
a = ⟨ax, ay, az⟩,
(1.14)
where we used angle brackets to denote components of the vector, reserving
the parentheses for coordinates of points in space.

1.3 Vectors in Diﬀerent Coordinate Systems
19
x
y
z
O
P(ρ, ϕ, z)
ϕ
eϕ
^
eρ
^
ez^
z
ρ
Figure 1.12: Unit vectors of cylindrical coordinates.
The unit vectors at a point P in the other coordinate systems are obtained
similarly. In cylindrical coordinates, ˆeρ lies along and points in the direction
of increasing ρ at P; ˆeϕ is perpendicular to the plane formed by P and the
z-axis and points in the direction of increasing ϕ; ˆez points in the direction of
positive z (see Figure 1.12). We note that only ˆez is independent of the point
at which the unit vectors are deﬁned because z is a ﬁxed axis in cylindrical
coordinates. Given any vector a, we can write it as
a = aρˆeρ + aϕˆeϕ + azˆez
or
a = ⟨aρ, aϕ, az⟩.
(1.15)
The unit vectors in spherical coordinates are deﬁned similarly: ˆer is taken
along r and points in the direction of increasing r; this direction is called
radial direction
radial; ˆeθ is taken to lie in the plane formed by P and the z-axis, is per-
pendicular to r, and points in the direction of increasing θ; ˆeϕ is as in the
cylindrical case (Figure 1.13). An arbitrary vector in space can be expressed
in terms of the spherical unit vectors at P:
a = arˆer + aθˆeθ + aϕˆeϕ
or
a = ⟨ar, aθ, aϕ⟩.
(1.16)
It should be emphasized that
Box 1.3.3. The cylindrical and spherical unit vectors ˆeρ, ˆer, ˆeθ, and ˆeϕ
are dependent on the position of P.
Once an origin O is designated, every point P in space will deﬁne a vector,
called a position vector and denoted by r. This is simply the vector drawn
position vector
from O to P. In Cartesian coordinates this vector has components ⟨x, y, z⟩,
thus one can write
r = xˆex + yˆey + zˆez.
(1.17)

20
Coordinate Systems and Vectors
er^
eϕ
^
eθ
^
x
y
z
O
r
θ
P(r, θ, ϕ)
ϕ
Figure 1.13: Unit vectors of spherical coordinates. Note that the intersection of the
shaded plane with the xy-plane is a line along the cylindrical coordinate ρ.
But (x, y, z) are also the coordinates of the point P. This can be a source of
diﬀerence between
coordinates and
components
explained
confusion when other coordinate systems are used. For example, in spherical
coordinates, the components of the vector r at P are ⟨r, 0, 0⟩because r has
only a component along ˆer and none along ˆeθ or ˆeϕ. One writes11
r = rˆer.
(1.18)
However, the coordinates of P are still (r, θ, ϕ)! Similarly, the coordinates of
P are (ρ, ϕ, z) in a cylindrical system, while
r = ρ ˆeρ + zˆez,
(1.19)
because r lies in the ρz-plane and has no component along ˆeϕ. Therefore,
Box 1.3.4. Make a clear distinction between the components of the
vector r and the coordinates of the point P.
A common symptom of confusing components with coordinates is as fol-
lows.
Point P1 has position vector r1 with spherical components ⟨r1, 0, 0⟩
at P1. The position vector of a second point P2 is r2 with spherical compo-
nents ⟨r2, 0, 0⟩at P2. It is easy to fall into the trap of thinking that r1 −r2
has spherical components ⟨r1 −r2, 0, 0⟩! This is, of course, not true, because
the spherical unit vectors at P1 are completely diﬀerent from those at P2,
and, therefore, contrary to the Cartesian case, we cannot simply subtract
components.
11We should really label everything with P . But, as usual, we assume this labeling to be
implied.

1.3 Vectors in Diﬀerent Coordinate Systems
21
One of the great advantages of vectors is their ability to express results
Physical laws
ought to be
coordinate
independent!
independent of any speciﬁc coordinate systems.
Physical laws are always
coordinate-independent. For example, when we write F = ma both F and a
could be expressed in terms of Cartesian, spherical, cylindrical, or any other
convenient coordinate system. This independence allows us the freedom to
choose the coordinate systems most convenient for the problem at hand. For
example, it is extremely diﬃcult to solve the planetary motions in Cartesian
coordinates, while the use of spherical coordinates facilitates the solution of
the problem tremendously.
Example 1.3.1. We can express the coordinates of the center of mass (CM) of
center of mass
a collection of particles in terms of their position vectors.12 Thus, if r denotes the
position vector of the CM of the collection of N mass points, m1, m2, . . . , mN with
respective position vectors r1, r2, . . . , rN relative to an origin O, then13
r = m1r1 + m2r2 + · · · + mNrN
m1 + m2 + · · · + mN
=
N
k=1 mkrk
M
,
(1.20)
where M = N
k=1 mk is the total mass of the system. One can also think of Equation
(1.20) as a vector equation. To ﬁnd the component equations in a coordinate system,
one needs to pick a ﬁxed point (say the origin), a set of unit vectors at that point
(usually the unit vectors along the axes of some coordinate system), and substitute
the components of rk along those unit vectors to ﬁnd the components of r along the
unit vectors.
■
1.3.1
Fields and Potentials
The distributive property of the dot product and the fact that the unit vectors
of the bases in all coordinate systems are mutually perpendicular can be used
to derive the following:
dot product in
terms of
components in the
three coordinate
systems
a · b = axbx + ayby + azbz
(Cartesian),
a · b = aρbρ + aϕbϕ + azbz
(cylindrical),
(1.21)
a · b = arbr + aθbθ + aϕbϕ
(spherical).
The ﬁrst of these equations is the same as (1.3 ).
It is important to keep in mind that the components are to be expressed
in the same set of unit vectors. This typically means setting up mutually per-
pendicular unit vectors (an orthonormal basis) at a single point and resolving
all vectors along those unit vectors.
The dot product, in various forms and guises, has many applications in
physics. As pointed out earlier, it was introduced in the deﬁnition of work,
but soon spread to many other concepts of physics. One of the simplest—and
most important—applications is its use in writing the laws of physics in a
coordinate-independent way.
12This implies that the equation is most useful only when Cartesian coordinates are
used, because only for these coordinates do the components of the position vector of a
point coincide with the coordinates of that point.
13We assume that the reader is familiar with the symbol  simply as a summation
symbol. We shall discuss its properties and ways of manipulating it in Chapter 9.

22
Coordinate Systems and Vectors
q
q'
er^
r
(x, y, z)
x
y
z
Figure 1.14: The diagram illustrating the electrical force when one charge is at the
origin.
Example 1.3.2. A point charge q is situated at the origin. A second charge q′ is
located at (x, y, z) as shown in Figure 1.14. We want to express the electric force
on q′ in Cartesian, spherical, and cylindrical coordinate systems.
We know that the electric force, as given by Coulomb’s law, lies along the line
joining the two charges and is either attractive or repulsive according to the signs
of q and q′. All of this information can be summarized in the formula
Coulomb’s law
Fq′ = keqq′
r2
ˆer
(1.22)
where ke = 1/(4πϵ0) ≈9 × 109 in SI units. Note that if q and q′ are unlike, qq′ < 0
and Fq′ is opposite to ˆer, i.e., it is attractive. On the other hand, if q and q′ are of
the same sign, qq′ > 0 and Fq′ is in the same direction as ˆer, i.e., repulsive.
Equation (1.22) expresses Fq′ in spherical coordinates. Thus, its components in
terms of unit vectors at q′ are

keqq′/r2, 0, 0

. To get the components in the other
coordinate systems, we rewrite (1.22). Noting that ˆer = r/r, we write
Fq′ = keqq′
r2
r
r = keqq′
r3
r.
(1.23)
For Cartesian coordinates we use (1.12) to obtain r3 = (x2+y2+z2)3/2. Substituting
this and (1.17) in (1.23) yields
Fq′ =
keqq′
(x2 + y2 + z2)3/2 (xˆex + yˆey + zˆez).
Therefore, the components of Fq′ in Cartesian coordinates are

keqq′x
(x2 + y2 + z2)3/2 ,
keqq′y
(x2 + y2 + z2)3/2 ,
keqq′z
(x2 + y2 + z2)3/2

.
Finally, using (1.10) and (1.19) in (1.23), we obtain
Fq′ =
keqq′
(ρ2 + z2)3/2 (ρ ˆeρ + zˆez).

1.3 Vectors in Diﬀerent Coordinate Systems
23
x
y
z
P1
P2
r1
r2 − r1
r2
Figure 1.15: The displacement vector between P1 and P2 is the diﬀerence between
their position vectors.
Thus the components of Fq′ along the cylindrical unit vectors constructed at the
location of q′ are

keqq′ρ
(ρ2 + z2)3/2 , 0,
keqq′z
(ρ2 + z2)3/2

.
■
Since r gives the position of a point in space, one can use it to write
the distance between two points P1 and P2 with position vectors r1 and r2.
Figure 1.15 shows that r2 −r1 is the displacement vector from P1 to P2. The
importance of this vector stems from the fact that many physical quantities
are functions of distances between point particles, and r2 −r1 is a concise way
of expressing this distance. The following example illustrates this.
Historical Notes
During the second half of the eighteenth century many physicists were engaged in a
quantitative study of electricity and magnetism. Charles Augustin de Coulomb,
who developed the so-called torsion balance for measuring weak forces, is credited
with the discovery of the law governing the force between electrical charges.
Coulomb was an army engineer in the West Indies. After spending nine years
there, due to his poor health, he returned to France about the same time that the
French Revolution began, at which time he retired to the country to do scientiﬁc
Charles Coulomb
1736–1806
research.
Beside his experiments on electricity, Coulomb worked on applied mechanics,
structural analysis, the fracture of beams and columns, the thrust of arches, and the
thrust of the soil.
At about the same time that Coulomb discovered the law of electricity, there
lived in England a very reclusive character named Henry Cavendish.
He was
born into the nobility, had no close friends, was afraid of women, and disinterested
in music or arts of any kind. His life revolved around experiments in physics and
chemistry that he carried out in a private laboratory located in his large mansion.
During his long life he published only a handful of relatively unimportant pa-
pers. But after his death about one million pounds sterling were found in his bank
Henry Cavendish
1731–1810
account and twenty bundles of notes in his laboratory. These notes remained in
the possession of his relatives for a long time, but when they were published one

24
Coordinate Systems and Vectors
hundred years later, it became clear that Henry Cavendish was one of the greatest
experimental physicists ever. He discovered all the laws of electric and magnetic
interactions at the same time as Coulomb, and his work in chemistry matches that
of Lavoisier. Furthermore, he used a torsion balance to measure the universal grav-
itational constant for the ﬁrst time, and as a result was able to arrive at the exact
mass of the Earth.
Example 1.3.3. Coulomb’s law for two arbitrary charges
Suppose there are point charges q1 at P1 and q2 at P2. Let us write the force exerted
on q2 by q1. The magnitude of the force is
F21 = keq1q2
d2
,
where d = P1P2 is the distance between the two charges. We use d because the
usual notation r has special meaning for us: it is one of the coordinates in spherical
systems. If we multiply this magnitude by the unit vector describing the direction
of the force, we obtain the full force vector (see Box 1.1.1). But, assuming repulsion
for the moment, this unit vector is
r2 −r1
|r2 −r1| ≡ˆe21.
Also, since d = |r2 −r1|, we have
F21 = keq1q2
d2
ˆe21 =
keq1q2
|r2 −r1|2
r2 −r1
|r2 −r1|
or
Coulomb’s law
when charges are
arbitrarily located
F21 =
keq1q2
|r2 −r1|3 (r2 −r1).
(1.24)
Although we assumed repulsion, we see that (1.24) includes attraction as well. In-
deed, if q1q2 < 0, F21 is opposite to r2 −r1, i.e., F21 is directed from P2 to P1. Since
F21 is the force on q2 by q1, this is an attraction. We also note that Newton’s third
law is included in (1.24):
F12 =
keq2q1
|r1 −r2|3 (r1 −r2) = −F21
because r2 −r1 = −(r1 −r2) and |r2 −r1| = |r1 −r2|.
We can also write the gravitational force immediately
vector form of
gravitational force
F21 = −Gm1m2
|r2 −r1|3 (r2 −r1),
(1.25)
where m1 and m2 are point masses and the minus sign is introduced to ensure
attraction.
■
Now that we have expressions for electric and gravitational forces, we can
obtain the electric ﬁeld of a point charge and the gravitational ﬁeld of a point
mass. First recall that the electric ﬁeld at a point P is deﬁned to be the
force on a test charge q located at P divided by q. Thus if we have a charge
q1, at P1 with position vector r1 and we are interested in its ﬁelds at P with

1.3 Vectors in Diﬀerent Coordinate Systems
25
position vector r, we introduce a charge q at r and calculate the force on q
from Equation (1.24):
Fq =
keq1q
|r −r1|3 (r −r1).
Dividing by q gives
electric ﬁeld of a
point charge
E1 =
keq1
|r −r1|3 (r −r1),
(1.26)
where we have given the ﬁeld the same index as the charge producing it.
The calculation of the gravitational ﬁeld follows similarly. The result is
g1 = −
Gm1
|r −r1|3 (r −r1).
(1.27)
In (1.26) and (1.27), P is called the ﬁeld point and P1 the source point.
ﬁeld point and
source point
Note that in both expressions, the ﬁeld position vector comes ﬁrst.
If there are several point charges (or masses) producing an electric (gravita-
tional) ﬁeld, we simply add the contributions from each source. The principle
superposition
principle explained
behind this procedure is called the superposition principle. It is a princi-
ple that “seems” intuitively obvious, but upon further reﬂection its validity
becomes surprising. Suppose a charge q1 produces a ﬁeld E1 around itself.
Now we introduce a second charge q2 which, far away and isolated from any
other charges, produced a ﬁeld E2 around itself. It is not at all obvious that
once we move these charges together, the individual ﬁelds should not change.
After all, this is not what happens to human beings! We act completely dif-
ferently when we are alone than when we are in the company of others. The
presence of others drastically changes our individual behaviors. Nevertheless,
charges and masses, unfettered by any social chains, retain their individuality
and produce ﬁelds as if no other charges were present.
It is important to keep in mind that the superposition principle applies
only to point sources.
For example, a charged conducting sphere will not
produce the same ﬁeld when another charge is introduced nearby, because the
presence of the new charge alters the charge distribution of the sphere and
indeed does change the sphere’s ﬁeld. However each individual point charge
(electron) on the sphere, whatever location on the sphere it happens to end
up in, will retain its individual electric ﬁeld.14
Going back to the electric ﬁeld, we can write
E = E1 + E2 + · · · + En
for n point charges q1, q2, . . . , qn (see Figure 1.16). Substituting from (1.26),
with appropriate indices, we obtain
E =
keq1
|r −r1|3 (r −r1) +
keq2
|r −r2|3 (r −r2) + · · · +
keqn
|r −rn|3 (r −rn)
or, using the summation symbol, we obtain
14The superposition principle, which in the case of electrostatics and gravity is needed
to calculate the ﬁelds of large sources consisting of many point sources, becomes a vital
pillar upon which quantum theory is built and by which many of the strange phenomena
of quantum physics are explained.

26
Coordinate Systems and Vectors
x
y
z
r
P
ri
r – ri
Figure 1.16: The electrostatic ﬁeld of N point charges is the sum of the electric ﬁelds
of the individual charges.
Box 1.3.5. The electric ﬁeld of n point charges q1, q2, . . . , qn, lo-
cated at position vectors r1, r2, . . . , rn is E = n
i=1
keqi
|r−ri|3 (r −ri), and
the analogous expression for the gravitational ﬁeld of n point masses
m1, m2, . . . , mn is g = −n
i=1
Gmi
|r−ri|3 (r −ri).
Historical Notes
The concept of force has a fascinating history which started in the works of Galileo
around the beginning of the seventeenth century, mathematically formulated and
precisely deﬁned by Sir Isaac Newton in the second half of the seventeenth century,
revised and redeﬁned in the form of ﬁelds by Michael Faraday and James Maxwell
in the mid nineteenth century, and ﬁnally brought to its modern quantum ﬁeld
theoretical form by Dirac, Heisenberg, Feynman, Schwinger, and others by the mid
twentieth century.
Newton, in his theory of gravity, thought of gravitational force as
“action-
notion of ﬁeld
elaborated
at-a-distance,” an agent which aﬀects something that is “there” because of the
inﬂuence of something that is “here.” This kind of interpretation of force had both
philosophical and physical drawbacks.
It is hard to accept a ghostlike inﬂuence
on a distant object. Is there an agent that “carries” this inﬂuence? What is this
agent, if any? Does the inﬂuence travel inﬁnitely fast? If we remove the Sun from
the Solar System would the Earth and other planets “feel” the absence of the Sun
immediately?
These questions, plus others, prompted physicists to come up with the idea of a
ﬁeld. According to this interpretation, the Sun, by its mere presence, creates around
itself an invisible three dimensional “sheet” such that, if any object is placed in this
sheet, it feels the gravitational force. The reason that planets feel the force of gravity
of the Sun is because they happen to be located in the gravitational ﬁeld of the Sun.
The reason that an apple falls to the Earth is because it is in the gravitational ﬁeld
of the Earth and not due to some kind of action-at-a-distance ghost.

1.3 Vectors in Diﬀerent Coordinate Systems
27
Therefore, according to this concept, the force acts on an object here, because
there exists a ﬁeld right here. And force becomes a local concept. The ﬁeld con-
cept removes the diﬃculties associated with action-at-a-distance. The “agent” that
transmits the inﬂuence from the source to the object, is the ﬁeld. If the Sun is stolen
from the solar system, the Earth will not feel the absence of the Sun immediately.
It will receive the information of such cosmic burglary after a certain time-lapse
corresponding to the time required for the disturbance to travel from the Sun to the
Earth. We can liken such a disturbance (disappearance of the Sun) to a disturbance
in the smooth water of a quiet pond (by dropping a stone into it). Clearly, the dis-
turbance travels from the source (where the stone was dropped) to any other point
with a ﬁnite speed, the speed of the water waves.
The concept of a ﬁeld was actually introduced ﬁrst in the context of electricity
and magnetism by Michael Faraday as a means of “visualizing” electromagnetic
eﬀects to replace certain mathematical ideas for which he had little talent. However,
in the hands of James Maxwell, ﬁelds were molded into a physical entity having an
existence of their own in the form of electromagnetic waves to be produced in 1887
by Hertz and used in 1901 by Marconi in the development of radio.
A concept related to that of ﬁelds is potential which is closely tied to the
potential
work done by the ﬁelds on a charge (in the case of electrostatics) or a mass
(in the case of gravity). It can be shown15 that the gravitational potential
Φ(r) at r, of n point masses, is given by
Φ(r) = −
n

i=1
Gmi
|r −ri|
(1.28)
and that of n point charges by
Φ(r) =
n

i=1
keqi
|r −ri|.
(1.29)
Note that in both cases, the potential goes to zero as r goes to inﬁnity. This
has to do with the choice of the location of the zero of potential, which we
have chosen to be the point at inﬁnity in Equations (1.28) and (1.29).
Example 1.3.4. The electric charges q1, q2, q3, and q4 are located at Cartesian
(a, 0, 0), (0, a, 0), (−a, 0, 0), and (0, −a, 0), respectively. We want to ﬁnd the electric
ﬁeld and the electrostatic potential at an arbitrary point on the z-axis. We note
that
r1 = aˆex,
r2 = aˆey,
r3 = −aˆex,
r4 = −aˆey,
r = zˆez,
so that
r −r1 = −aˆex + zˆez,
r −r2 = −aˆey + zˆez,
r −r3 = aˆex + zˆez,
r −r4 = aˆey + zˆez,
15See Chapter 14 for details.

28
Coordinate Systems and Vectors
and |r −ri|3 = (a2 + z2)3/2 for all i. The electric ﬁeld can now be calculated using
Box 1.3.5:
E =
keq1
(a2 + z2)3/2 (−aˆex + zˆez) +
keq2
(a2 + z2)3/2 (−aˆey + zˆez)
+
keq3
(a2 + z2)3/2 (aˆex + zˆez) +
keq4
(a2 + z2)3/2 (aˆey + zˆez)
=
ke
(a2 + z2)3/2 [(−aq1 + aq3)ˆex + (−aq2 + aq4)ˆey + (q1 + q2 + q3 + q4)zˆez] .
It is interesting to note that if the sum of all charges is zero, the z-component of
the electric ﬁeld vanishes at all points on the z-axis. Furthermore, if, in addition,
q1 = q3 and q2 = q4, there will be no electric ﬁeld at any point on the z-axis.
The potential is obtained similarly:
Φ =
keq1
(a2 + z2)1/2 +
keq2
(a2 + z2)1/2 +
keq3
(a2 + z2)1/2 +
keq4
(a2 + z2)1/2
= ke(q1 + q2 + q3 + q4)
√
a2 + z2
.
So, the potential is zero at all points of the z-axis, as long as the total charge
is zero.
■
1.3.2
Cross Product
The unit vectors in the three coordinate systems are not only mutually perpen-
dicular, but in the order in which they are given, they also form a right-handed
set [see Equation (1.5)]. Therefore, we can use Equation (1.6) and write
a × b = det
⎛
⎝
ˆex
ˆey
ˆez
ax
ay
az
bx
by
bz
⎞
⎠



in Cartesian CS
= det
⎛
⎝
ˆeρ
ˆeϕ
ˆez
aρ
aϕ
az
bρ
bϕ
bz
⎞
⎠



in cylindrical CS
= det
⎛
⎝
ˆer
ˆeθ
ˆeϕ
ar
aθ
aϕ
br
bθ
bϕ
⎞
⎠



in spherical CS
(1.30)
Two important prototypes of the concept of cross product are angular
momentum and torque. A particle moving with instantaneous linear mo-
mentum p relative to an origin O has instantaneous angular momentum
L = r × p if its instantaneous position vector with respect to O is r.
In
Figure 1.17 we have shown r, p, and r × p. Similarly, if the instantaneous
force on the above particle is F, then the instantaneous torque acting on it is
T = r × F.
If there are more than one particle we simply add the contribution of
individual particles. Thus, the total angular momentum L of N particles and
angular
momentum and
torque as
examples of cross
products
the total torque T acting on them are
L =
N

k=1
rk × pk
and
T =
N

k=1
rk × Fk,
(1.31)
where rk is the position of the kth particle, pk its instantaneous momentum,
and Fk the instantaneous force acting on it.

1.3 Vectors in Diﬀerent Coordinate Systems
29
r
p
O
r × p
Figure 1.17: Angular momentum of a moving particle with respect to the origin O.
The circle with a dot in its middle represents a vector pointing out of the page. It is
assumed that r and p lie in the page.
Example 1.3.5. In this example, we show that the torque on a collection of three
particles is caused by external forces only. The torques due to the internal forces
add up to zero. The generalization to an arbitrary number of particles will be done
in Example 9.2.1 when we learn how to manipulate summation symbols.
For N = 3, the second formula in Equation (1.31) reduces to
T = r1 × F1 + r2 × F2 + r3 × F3.
Each force can be divided into an external part and an internal part, the latter being
the force caused by the presence of the other particles. So, we have
F1 = F(ext)
1
+ F12 + F13,
F2 = F(ext)
2
+ F21 + F23,
F3 = F(ext)
3
+ F31 + F32,
where F12 is the force on particle 1 exerted by particle 2, etc. Substituting in the
above expression for the torque, we get
T = r1 × F(ext)
1
+ r2 × F(ext)
2
+ r3 × F(ext)
3
+ r1 × F12 + r1 × F13 + r2 × F21 + r2 × F23 + r3 × F31 + r3 × F32
= T(ext) + (r1 −r2) × F12 + (r1 −r3) × F13 + (r2 −r3) × F23,
where we used the third law of motion: F12 = −F21, etc. Now we note that the
internal force between two particles, 1 and 2 say, is along the line joining them, i.e.,
along r1 −r2. It follows that all the cross products in the last line of the equation
above vanish and we get T = T(ext).
■
We have already seen that multiplying a vector by a number gives another
vector. A physical example of this is electric force which is obtained by multi-
plying electric ﬁeld by electric charge. In fact we divided the electric force by
charge to get the electric ﬁeld. Historically, it was the law of the force which
from electric ﬁeld
to electric force
was discovered ﬁrst and then the concept of electric ﬁeld was deﬁned. We have
also seen that one can get a new vector by cross-multiplying two vectors. The
rule of this kind of multiplication is, however, more complicated. It turns out

30
Coordinate Systems and Vectors
that the magnetic force is related to the magnetic ﬁeld via such a cross multi-
plication. What is worse is that the magnetic ﬁeld is also related to its source
(electric charges in motion) via such a product. Little wonder that magnetic
phenomena are mathematically so much more complicated than their electric
counterparts. That is why in the study of magnetism, one ﬁrst introduces
the concept of magnetic ﬁeld and how it is related to the motion of charges
producing it, and then the force of this ﬁeld on moving charges.
Example 1.3.6. A charge q, located instantaneously at the origin, is moving
with velocity v relative to P [see Figure 1.18(a)]. Assuming that |v| is much smaller
than the speed of light, the instantaneous magnetic ﬁeld at P due to q is given by
B = kmq v × ˆer
r2
, or, using ˆer = r/r, by B = kmq v × r
r3
. This is a simple version of
magnetic ﬁeld of a
moving charge or
Biot–Savart law
a more general formula known as the Biot–Savart law. In the above relations, km
is the analog of ke in the electric case.
If we are interested in the magnetic ﬁeld when q is located at a point other
than the origin, we replace r with the vector from the instantaneous location of the
moving charge to P. This is shown in Figure 1.18(b), where the vector from q1 to
P is to replace r in the above equation. More speciﬁcally, we have
B1 = kmq1v1 × (r −r1)
|r −r1|3
.
(1.32)
If there are N charges, the total magnetic ﬁeld will be
B =
N

k=1
kmqkvk × (r −rk)
|r −rk|3
,
(1.33)
where we have used the superposition principle.
■
When a charge q moves with velocity v in a magnetic ﬁeld B, it experiences
a force given by
magnetic force on
a moving charge.
F = qv × B.
(1.34)
It is instructive to write the magnetic force exerted by a charge q1 moving
with velocity v1 on a second charge q2 moving with velocity v2. We leave this
as an exercise for the reader.
(b)
O
r
q1
P
q
P
v
r
(a)
r1
r − r1
v1
Figure 1.18: The (instantaneous) magnetic ﬁeld at P of a moving point charge (a)
when P is at the origin, and (b) when P is diﬀerent from the origin. The ﬁeld points
out of the page for the conﬁguration shown.

1.4 Relations Among Unit Vectors
31
Example 1.3.7. A charge q moves with constant speed v (assumed to be small
compared to the speed of light) on a straight line. We want to calculate the magnetic
ﬁeld produced by the charge at a point P located at a distance ρ from the line as a
function of time. Cylindrical coordinates are most suitable for this problem because
of the existence of a natural axis. Choose the path of the charge to be the z-axis.
Also assume that P lies in the xy-plane, and that q was at the origin at t = 0. Then
v = vˆez, r = ρˆeρ, r1 = vtˆez, r −r1 = ρˆeρ −vtˆez. So
|r −r1| =
	
(ρˆeρ −vtˆez) · (ρˆeρ −vtˆez) =
	
ρ2 + v2t2
and v × (r −r1) = vˆez × (ρˆeρ −vtˆez) = ρvˆeϕ. Therefore, the magnetic ﬁeld is
B = kmqv × (r −r1)
|r −r1|3
=
kmqρv
(ρ2 + v2t2)3/2 ˆeϕ.
Readers familiar with the relation between magnetic ﬁelds and currents in long wires
will note that the magnetic ﬁeld above obeys the right-hand rule.
■
1.4
Relations Among Unit Vectors
We have seen that, depending on the nature of problems encountered in
physics, one coordinate system may be more useful than others. We have
also seen that the coordinates can be transformed back and forth using func-
tional relations that connect them. Since many physical quantities are vectors,
transformation and expression of components in bases of various coordinate
systems also become important. The key to this transformation is writing one
set of unit vectors in terms of others. In the derivation of these relations, we
shall make heavy use of Box 1.1.2.
First we write the cylindrical unit vectors in terms of Cartesian unit vec-
tors. Since {ˆex, ˆey, ˆez} form a basis, any vector can be written in terms of
them. In particular, ˆeρ can be expressed as
ˆeρ = a1ˆex + b1ˆey + c1ˆez
(1.35)
with a1, b1, and c1 to be determined. Next we recall that
Box 1.4.1. The dot product of two unit vectors is the cosine of the angle
between them.
Furthermore, Figure 1.12 shows that the angle between ˆeρ and ˆex is ϕ, and
that between ˆeρ and ˆey is π/2 −ϕ. So, by dotting both sides of Equation
(1.35) by ˆex, ˆey, and ˆez in succession, we obtain
ˆex · ˆeρ
  
=cos ϕ
= a1 + 0 + 0 = a1 ⇒a1 = cos ϕ,
ˆey · ˆeρ
  
=sin ϕ
= 0 + b1 + 0 = b1 ⇒b1 = sin ϕ,
ˆez · ˆeρ
  
=0
= 0 + 0 + c1 = c1 ⇒c1 = 0.

32
Coordinate Systems and Vectors
Therefore,
ˆeρ = ˆex cos ϕ + ˆey sin ϕ.
With the ﬁrst and third cylindrical unit vectors ˆeρ and ˆez at our disposal,16
we can determine the second, using Equation (1.5):
ˆeϕ = ˆez × ˆeρ = det
⎛
⎝
ˆex
ˆey
ˆez
0
0
1
cos ϕ
sin ϕ
0
⎞
⎠= −ˆex sin ϕ + ˆey cos ϕ.
Thus,
ˆeρ = ˆex cos ϕ + ˆey sin ϕ,
ˆeϕ = −ˆex sin ϕ + ˆey cos ϕ,
(1.36)
ˆez = ˆez.
cylindrical unit
vectors in terms of
Cartesian unit
vectors
This equation can easily be inverted to ﬁnd the Cartesian unit vectors in
terms of the cylindrical unit vectors. For example, the coeﬃcients in
ˆex = a2ˆeρ + b2ˆeϕ + c2ˆez
can be obtained by dotting both sides of it with ˆeρ, ˆeϕ, and ˆez, respectively,
ˆeρ · ˆex = a2 + 0 + 0 ⇒cos ϕ = a2,
ˆeϕ · ˆex = 0 + b2 + 0 ⇒−sin ϕ = b2,
ˆez · ˆex = 0 + 0 + c2 ⇒0 = c2,
where we have used ˆeρ · ˆex = cos ϕ, and ˆeϕ · ˆex = −sin ϕ—obtained by
dotting the ﬁrst and second equations of (1.36) with ˆex—as well as ˆez ·ˆex = 0.
Similarly, one can obtain ˆey in terms of the cylindrical unit vectors. The entire
result is
ˆex = ˆeρ cos ϕ −ˆeϕ sin ϕ
ˆey = ˆeρ sin ϕ + ˆeϕ cos ϕ
(1.37)
ˆez = ˆez
Cartesian unit
vectors in terms of
cylindrical unit
vectors
Now we express the spherical unit vectors in terms of the cylindrical ones.
This is easily done for ˆer, because it has only ˆeρ and ˆez components (why?).
Thus, with
ˆer = a3ˆeρ + b3ˆez,
we obtain
ˆeρ · ˆer = a3 + 0 ⇒a3 = sin θ,
ˆez · ˆer = 0 + b3 ⇒b3 = cos θ,
16Remember that ˆez is a unit vector in both coordinate systems. So, one can say that
the cylindrical ˆez has components ⟨0, 0, 1⟩in the Cartesian basis {ˆex, ˆey, ˆez}.

1.4 Relations Among Unit Vectors
33
where in the last step of each line, we used the fact that the angle between ˆer
and ˆez is θ and that between ˆer and ˆeρ is π/2 −θ (see Figure 1.13). With a3
and b3 so determined, we can write
ˆer = ˆeρ sin θ + ˆez cos θ.
Having two spherical unit vectors ˆer and ˆeϕ at our disposal,17 we can
determine the third one, using (1.5) and (1.30):
ˆeθ = ˆeϕ × ˆer = det
⎛
⎝
ˆeρ
ˆeϕ
ˆez
0
1
0
sin θ
0
cos θ
⎞
⎠= ˆeρ cos θ −ˆez sin θ.
Thus,
ˆer = ˆeρ sin θ + ˆez cos θ,
ˆeθ = ˆeρ cos θ −ˆez sin θ,
(1.38)
ˆeϕ = ˆeϕ.
spherical unit
vectors in terms of
cylindrical unit
vectors
The inverse relations can be obtained as before. We leave the details of
the calculation as an exercise for the reader.
Combining Equations (1.36) and (1.38), we can express spherical unit vec-
tors in terms of the Cartesian unit vectors:
spherical unit
vectors in terms of
Cartesian unit
vectors
ˆer = ˆex sin θ cos ϕ + ˆey sin θ sin ϕ + ˆez cos θ,
ˆeθ = ˆex cos θ cos ϕ + ˆey cos θ sin ϕ −ˆez sin θ,
(1.39)
ˆeϕ = −ˆex sin ϕ + ˆey cos ϕ.
Equations (1.39) and (1.36) are very useful when calculating vector quan-
tities in spherical and cylindrical coordinates as we shall see in many examples
to follow. These equations also allow us to express a unit vector in one of the
three coordinate systems in terms of the unit vectors of any other coordinate
system.
Example 1.4.1. P1 and P2 have Cartesian coordinates (1, 1, 1) and (−1, 2, −1),
respectively. A vector a has spherical components ⟨0, 2, 0⟩at P1. We want to ﬁnd
the spherical components of a at P2. These are given by a · ˆer2, a · ˆeθ2, and a · ˆeϕ2.
In order to calculate these dot products, it is most convenient to express all vectors
in Cartesian form. So, using Equation (1.39), we have
a = 2ˆeθ1 = 2 (ˆex cos θ1 cos ϕ1 + ˆey cos θ1 sin ϕ1 −ˆez sin θ1) ,
where (r1, θ1, ϕ1) are coordinates of P1. We can calculate these from the Cartesian
coordinates of P1:
r1 =
	
12 + 12 + 12 =
√
3,
cos θ1 = z1
r1 =
1
√
3
,
tan ϕ1 = y1
x1 = 1 ⇒ϕ1 = π
4 .
17Recall that ˆeϕ is both a cylindrical and a spherical unit vector.

34
Coordinate Systems and Vectors
Therefore,
a = 2

ˆex 1
√
3
1
√
2
+ ˆey 1
√
3
1
√
2
−ˆez

2
3

=
2
√
6
ˆex + 2
√
6
ˆey −4
√
6
ˆez.
Now we need to express ˆer2, ˆeθ2, and ˆeϕ2 in terms of Cartesian unit vectors.
Once again we use Equation (1.39) for which we need the spherical coordinates of
P2:
r2 =
	
(−1)2 + 22 + (−1)2 =
√
6,
cos θ2 = z2
r2 = −1
√
6
,
tan ϕ2 = y2
x2 = −2.
Similarly, Equations (1.11) and (1.12) yield
sin θ2 = +

5
6,
cos ϕ2 = −1
√
5
,
sin ϕ2 = + 2
√
5
.
Then
ˆer2 = ˆex sin θ2 cos ϕ2 + ˆey sin θ2 sin ϕ2 + ˆez cos θ2
= ˆex

5
6

−1
√
5

+ ˆey

5
6
2
√
5
−ˆez 1
√
6
= −1
√
6
ˆex + 2
√
6
ˆey −1
√
6
ˆez,
ˆeθ2 = ˆex cos θ2 cos ϕ2 + ˆey cos θ2 sin ϕ2 −ˆez sin θ2
= ˆex

−1
√
6
 
−1
√
5

+ ˆey

−1
√
6
 2
√
5
−ˆez

5
6
=
1
√
30
ˆex −
2
√
30
ˆey −
5
√
30
ˆez,
ˆeϕ2 = −ˆex sin ϕ2 + ˆey cos ϕ2 = −2
√
5
ˆex −1
√
5
ˆey.
We can now take the dot products required for the components:
r comp = a · ˆer2 =
 2
√
6
ˆex + 2
√
6
ˆey −4
√
6
ˆez

·

−1
√
6
ˆex + 2
√
6
ˆey −1
√
6
ˆez

= −2
6 + 4
6 + 4
6 = 1,
θ comp = a · ˆeθ2 =
 2
√
6
ˆex + 2
√
6
ˆey −4
√
6
ˆez

·

1
√
30
ˆex −
2
√
30
ˆey −
5
√
30
ˆez

=
2
6
√
5
−
4
6
√
5
+ 20
6
√
5
=
3
√
5
,
ϕ comp = a · ˆeϕ2 =
 2
√
6
ˆex + 2
√
6
ˆey −4
√
6
ˆez

·

−2
√
5
ˆex −1
√
5
ˆey

= −
4
√
30
−
2
√
30
= −
6
√
30
= −

6
5.
It now follows that
a = ˆer2 + 3
√
5
ˆeθ2 −

6
5 ˆeϕ2.

1.4 Relations Among Unit Vectors
35
As a check, we note that
|a| =



12 +
 3
√
5
2
+

−

6
5
2
=

5 + 9 + 6
5
=
√
4 = 2,
which agrees with the length of a.
■
Example 1.4.2. Points P1 and P2 have spherical coordinates (r1, θ1, ϕ1) and
(r2, θ2, ϕ2), respectively.
We want to ﬁnd: (a) the angle between their position
vectors r1 and r2 in terms of their coordinates; (b) the spherical components of r2
at P1; and (c) the spherical components of r1 at P2. Once again, we shall express
all vectors in terms of Cartesian unit vectors when evaluating dot products.
(a) The cosine of the angle—call it γ12—between the position vectors is simply
ˆer1 · ˆer2. We can readily ﬁnd this by using Equation (1.39):
cos γ12 = ˆer1 · ˆer2 = (ˆex sin θ1 cos ϕ1 + ˆey sin θ1 sin ϕ1 + ˆez cos θ1)
· (ˆex sin θ2 cos ϕ2 + ˆey sin θ2 sin ϕ2 + ˆez cos θ2)
= sin θ1 cos ϕ1 sin θ2 cos ϕ2 + sin θ1 sin ϕ1 sin θ2 sin ϕ2 + cos θ1 cos θ2
= sin θ1 sin θ2(cos ϕ1 cos ϕ2 + sin ϕ1 sin ϕ2) + cos θ1 cos θ2
= sin θ1 sin θ2 cos(ϕ1 −ϕ2) + cos θ1 cos θ2.
(b) To ﬁnd the spherical components of r2 at P1, we need to take the dot product
of r2 with the spherical unit vectors at P1:
r comp = r2 · ˆer1 = r2ˆer2 · ˆer1
= r2 [sin θ1 sin θ2 cos(ϕ1 −ϕ2) + cos θ1 cos θ2] ,
θ comp = r2 · ˆeθ1 = r2ˆer2 · ˆeθ1
= r2 (ˆex sin θ2 cos ϕ2 + ˆey sin θ2 sin ϕ2 + ˆez cos θ2)
· (ˆex cos θ1 cos ϕ1 + ˆey cos θ1 sin ϕ1 −ˆez sin θ1)
= r2(sin θ2 cos ϕ2 cos θ1 cos ϕ1 + sin θ2 sin ϕ2 cos θ1 sin ϕ1 −cos θ2 sin θ1)
= r2[sin θ2 cos θ1 cos(ϕ1 −ϕ2) −cos θ2 sin θ1],
ϕ comp = r2 · ˆeϕ1 = r2ˆer2 · ˆeϕ1
= r2 (ˆex sin θ2 cos ϕ2 + ˆey sin θ2 sin ϕ2 + ˆez cos θ2) · (−ˆex sin ϕ1 + ˆey cos ϕ1)
= r2 (−sin θ2 cos ϕ2 sin ϕ1 + sin θ2 sin ϕ2 cos ϕ1) = r2 sin θ2 sin(ϕ2 −ϕ1).
(c) The spherical components of r1 at P2 can be found similarly.
In fact,
switching the indices “1” and “2” in the expressions of part (b) gives the desired
formulas.
■
Example 1.4.3. To illustrate further the conversion of vectors from one coordinate
system to another, consider a charge q that is located at the cylindrical coordinates
(a, π/3, −a). We want to ﬁnd the spherical components of the electrostatic ﬁeld E
of this charge at a point P with Cartesian coordinates (a, a, a).
The most straightforward way of doing this is to convert all coordinates to
Cartesian, ﬁnd the ﬁeld, and then take the dot products with appropriate unit
vectors. The Cartesian coordinates of the charge are
xq = ρq cos ϕq = a cos
 π
3

= 1
2a,
yq = ρq sin ϕq = a sin
 π
3

=
√
3a
2
= 0.866a,
zq = −a.

36
Coordinate Systems and Vectors
Thus,
r −rq = (a −1
2a)ˆex + (a −0.866a)ˆey + (a −(−a))ˆez = 0.5aˆex + 0.134aˆey + 2aˆez
and
|r −rq| =
	
(0.5a)2 + (0.134a)2 + (2a)2 = 2.066a,
and the electric ﬁeld at P can be written in terms of Cartesian unit vectors at P:
E =
keq
|r −rq|3 (r −rq) = keq 0.5aˆex + 0.134aˆey + 2aˆez
(2.066a)3
= keq 0.5ˆex + 0.134ˆey + 2ˆez
8.818a2
= keq
a2 (0.0567ˆex + 0.0152ˆey + 0.2268ˆez).
To ﬁnd the spherical components of the ﬁeld at P, we ﬁrst express the spherical
unit vectors at P in terms of Cartesian unit vectors. For this, we need the spherical
coordinates of P:
r =
	
a2 + a2 + a2 =
√
3 a = 1.732a,
cos θ = z
r =
a
√
3 a
=
1
√
3
= 0.577 ⇒θ = 0.955,
tan ϕ = y
x = a
a = 1 ⇒ϕ = π
4 = 0.785.
It now follows that
ˆer = ˆex sin θ cos ϕ + ˆey sin θ sin ϕ + ˆez cos θ = 0.577ˆex + 0.577ˆey + 0.577ˆez,
ˆeθ = ˆex cos θ cos ϕ + ˆey cos θ sin ϕ −ˆez sin θ = 0.408ˆex + 0.408ˆey −0.816ˆez,
ˆeϕ = −ˆex sin ϕ + ˆey cos ϕ = −0.707ˆex + 0.707ˆey.
Now we take the dot product of E with these unit vectors to ﬁnd its spherical
components at P. The reader may ﬁrst easily check that
ˆer · ˆex = 0.577,
ˆer · ˆey = 0.577,
ˆer · ˆez = 0.577,
ˆeθ · ˆex = 0.408,
ˆeθ · ˆey = 0.408,
ˆeθ · ˆez = −0.816,
ˆeϕ · ˆex = −0.707,
ˆeϕ · ˆey = 0.707,
ˆeϕ · ˆez = 0.
We can now ﬁnally calculate the ﬁeld components:
Er = E · ˆer = keq
a2 (0.0567ˆer · ˆex + 0.0152ˆer · ˆey + 0.2268ˆer · ˆez)
= keq
a2 (0.0567 × 0.577 + 0.0152 × 0.577 + 0.2268 × 0.577) = 0.1724keq
a2 ,
Eθ = E · ˆeθ = keq
a2 (0.0567ˆeθ · ˆex + 0.0152ˆeθ · ˆey + 0.2268ˆeθ · ˆez)
= keq
a2 (0.0567 × 0.408 + 0.0152 × 0.408 −0.2268 × 0.816) = −0.1558keq
a2 ,
Eϕ = E · ˆeϕ = keq
a2 (0.0567ˆeϕ · ˆex + 0.0152ˆeϕ · ˆey + 0.2268ˆeϕ · ˆez)
= keq
a2 (−0.0567 × 0.707 + 0.0152 × 0.707) = −0.0294keq
a2 .
The choice of Cartesian coordinates was the most straightforward one, but one
can choose any other coordinate system to calculate the ﬁeld and ﬁnd the com-
ponents in any other set of unit vectors.
The reader is urged to try the other
choices.
■

1.5 Problems
37
1.5
Problems
1.1. Find the equation of a line that passes through the following pairs of
points:
(a) (1, 0, 1) and (−1, 1, 0).
(b) (2, 2, −1) and (−2, −1, 1).
(c) (1, 1, 1) and (−1, 1, −1).
(d) (1, 1, 1) and (−2, 2, 0).
(e) (0, 2, −1) and (3, −1, 1).
(f) (0, 1, 0) and (−1, 0, −1).
1.2. Use Figure 1.4 and the interpretation of the a · b as the product of the
length of a with the projection of b along a to show that
(a + b) · c = a · c + b · c.
1.3. Take the dot product of a = b−c with itself and prove the law of cosines
by interpreting the result geometrically. Note that the three vectors form a
triangle.
1.4. Find the angle between a = 2ˆex + 3ˆey + ˆez and b = ˆex −6ˆey + 2ˆez.
1.5. Find the angle between a = 9ˆex + ˆey −6ˆez and b = 4ˆex −6ˆey + 5ˆez.
1.6. Show that a = ˆex cos α + ˆey sin α and b = ˆex cos β + ˆey sin β are unit
vectors in the xy-plane making angles α and β with the x-axis. Then take their
dot product and obtain a formula for cos(α−β). Now use sin x = cos(π/2−x)
to ﬁnd the formula for sin(α −β).
1.7. Vectors a and b are the sides of a parallelogram, c and d are its diagonals,
and θ is the angle between a and b. Show that
|c|2 + |d|2 = 2(|a|2 + |b|2)
and that
|c|2 −|d|2 = 4|a| |b| cos θ.
1.8. Given a, b, and c—vectors from the origin to the points A, B, and C—
show that the vector (a × b) + (b × c) + (c × a) is perpendicular to the plane
ABC.
1.9. Show that the vectors a = 2ˆex −ˆey + ˆez, b = ˆex −3ˆey −5ˆez, and
c = 3ˆex −4ˆey −4ˆez form the sides of a right triangle.
1.10. (a) Find the vector form of the equation of the plane deﬁned by the three
points P, Q, and R with coordinates (p1, p2, p3), (q1, q2, q3), and (r1, r2, r3),
respectively. Hint: The position vector of a point X = (x, y, z) in the plane
is perpendicular to the cross product of −−→
PQ and −→
PR.
(b) Determine an equation for the plane passing through the points (2, −1, 1),
(3, 2, −1), and (−1, 3, 2).
1.11. Derive the law of sines for a triangle using vectors.

38
Coordinate Systems and Vectors
1.12. Using vectors, show that the diagonals of a rhombus are orthogonal.
1.13. Show that a necessary and suﬃcient condition for three vectors to be
in the same plane is that the dot product of one with the cross product of the
other two be zero.
1.14. Show that two nonzero vectors have the same direction if and only if
their cross product vanishes.
1.15. Show the following vector identities by writing each vector in terms of
Cartesian unit vectors and showing that each component of the LHS is equal
to the corresponding component of the RHS.
(a) a · (b × c) = c · (a × b) = b · (c × a).
(b) a × (b × c) = b(a · c) −c(a · b),
this is called the bac cab rule.
(c) (a × b) · (c × d) = (a · c)(b · d) −(a · d)(b · c).
(d) (a × b) × (c × d) = b[a · (c × d)] −a[b · (c × d)].
(e) (a × b) × (c × d) = c[a · (b × d)] −d[a · (b × c)].
(f) (a × b) · (a × b) = |a|2|b|2 −(a · b)2.
1.16. Convert the following triplets from the given coordinate system to the
other two. All angles are in radians.
Cartesian:
(1, 2, 1), (0, 0, 1), (1, −1, 0), (0, 1, 0), (1, 1, 1), (2, 2, 2), (0, 0, 5),
(1, 1, 0), (1, 0, 0).
Spherical:
(2, π/3, π/4),
(5, 0, π/3),
(3, π/3, 3π/4),
(1, 1, 0),
(1, 0, 0),
(5, 0, ♣), (3, π, ♥), (0, ♠, ♦).
Cylindrical: (0, ♣, 4), (2, π, 0), (0, 217, −18), (1, 3π/4, −2), (1, 2, 3), (1, 0, 0).
1.17. Derive the second and third relations in Equation (1.21).
1.18. Points P and P ′ have spherical coordinates (r, θ, ϕ) and (r′, θ′, ϕ′),
cylindrical coordinates (ρ, ϕ, z) and (ρ′, ϕ′, z′), and Cartesian coordinates
(x, y, z) and (x′, y′, z′), respectively.
Write |r −r′| in all three coordinate
systems. Hint: Use Equation (1.2) with a = r −r′ and r and r′ written in
terms of appropriate unit vectors.
1.19. Show that Equation (1.24) is independent of where we choose the origin
to be. Hint: Pick a diﬀerent origin O′ whose position vector relative to O is
R and write the equation in terms of position vectors relative to O′ and show
that the ﬁnal result is the same as in Equation (1.24).
1.20. Three point charges are located at the corners of an equilateral triangle
of sides a with the origin at the center of the triangle as shown in Figure 1.19.
(a) Find the general expression for the electric ﬁeld and electric potential at
(0, 0, z).
(b) Find a relation between q and Q such that the z-component of the ﬁeld
vanishes for all values of z. What are E and Φ for such charges?
(c) Calculate E and Φ for z = a.

1.5 Problems
39
x
y
q
q
Q
Figure 1.19:
1.21. A point charge Q and two point charges q are located in the xy-plane
at the corners of an equilateral triangle of side a as shown in Figure 1.20.
(a) Find the potential and the Cartesian components of the electrostatic ﬁeld
at (0, 0, z).
(b) Show that it is impossible for E to be along the z-axis.
(c) Calculate E for z = a and ﬁnd Q in terms of q such that Ez vanishes for
this value of z.
(d) What is the value of Φ at z = a for the charges found in (c)?
1.22. Three point charges each of magnitude Q and one point charge q are
located at the corners of a square of side 2a. Using an appropriate coordinate
system.
(a) Find the electric ﬁeld and potential at point P located on the diagonal
from Q to q (and beyond) a distance 2
√
2 a from the center.
(b) Find a relation, if it exists, between q and Q such that the ﬁeld vanishes
at P.
1.23. A charge q is located at the spherical coordinates (a, π/4, π/3). Find
the electrostatic potential and the Cartesian components of the electrostatic
ﬁeld of this charge at a point P with spherical coordinates (a, π/6, π/4). Write
the ﬁeld components as numerical multiples of keq/a2, and the potential as a
x
y
q
q
Q
Figure 1.20:

40
Coordinate Systems and Vectors
numerical multiple of keq/a.
1.24. A charge q is located at the cylindrical coordinates (a, π/4, 2a). Find
the Cartesian components of the electrostatic ﬁeld of this charge at a point P
with cylindrical coordinates (2a, π/6, a). Write your answers as a numerical
multiple of keq/a2. Find the electrostatic potential at P and express it as a
numerical multiple of keq/a.
1.25. A charge q is located at the cylindrical coordinates (a, π/3, −a).
(a) Find the Cartesian components of the electrostatic ﬁeld E of this charge
at a point P with cylindrical coordinates (a, π/4, 2a). Write your answers as
a numerical multiple of keq/a2.
(b) Write E in terms of the cylindrical unit vectors at P.
(c) Find the electrostatic potential at P as a numerical multiple of keq/a.
1.26. Two charges q and −2q are located at the cylindrical coordinates
(a, π/4, a) and (a, 2π/3, −a), respectively.
(a) Find the Cartesian components of the electrostatic ﬁeld at a point P with
spherical coordinates (3a, π/6, π/4). Write your answers as a numerical mul-
tiple of keq/a2.
(b) Find the electrostatic potential at P. Write your answer as a numerical
multiple of keq/a.
1.27. Two charges 3q and −q are located at the spherical coordinates
(a, π/3, π/6) and (2a, π/6, π/4), respectively.
(a) Find the cylindrical components of the electrostatic ﬁeld at a point P
with spherical coordinates (3a, π/4, π/4). Write your answers as a numerical
multiple of keq/a2.
(b) Find the electrostatic potential at P. Write your answer as a numerical
multiple of keq/a.
1.28. A charge q is located at the spherical coordinates (a, π/3, π/6). Find
the Cartesian components of the electrostatic ﬁeld of this charge at a point P
with cylindrical coordinates (a, 2π/3, 2a). Write your answers as a numerical
multiple of keq/a2. Also ﬁnd the electrostatic potential at P.
1.29. Four charges are located at Cartesian coordinates as follows: q at
(2a, 0, 0), −2q at (0, 2a, 0), 4
√
2
5
√
5
q at (−a, 0, 0), and −2
√
2
5
√
5
q at (0, −a, 0). Find
the Cartesian components of the electrostatic ﬁeld at (0, 0, a).
1.30. Charge q is moving at constant speed v along the positive x-axis. Two
other charges −q and 2q are moving at constant speeds v and 2v along positive
y and negative z axes, respectively. Assume that at t = 0, q is at the origin,
−q is at (0, a, 0), and 2q at (0, 0, −a).
(a) Find the Cartesian components of the magnetic ﬁeld at a point (x, y, z)
for t > 0.
(a) Find the cylindrical components of the magnetic ﬁeld at a point (ρ, ϕ, z)
for t > 0.

1.5 Problems
41
(a) Find the spherical components of the magnetic ﬁeld at a point (r, θ, ϕ) for
t > 0.
1.31. A charge q is moving at constant speed v along a curve parametrized
by
x′ = 6as,
y′ = 3as2, z′ = −2as3
(a) Find the Cartesian components of the magnetic ﬁeld at a point (x, y, z)
as a function of s.
(a) Find the cylindrical components of the magnetic ﬁeld at a point (ρ, ϕ, z)
as a function of s.
(a) Find the spherical components of the magnetic ﬁeld at a point (r, θ, ϕ) as
a function of s.
1.32. Points P1 and P2 have Cartesian coordinates (1, 1, 1) and (1, 1, 0), re-
spectively.
(a) Find the spherical coordinates of P1 and P2.
(b) Write down the components of r1, the position vector of P1, in terms of
spherical unit vectors at P1.
(c) Write down the components of r2, the position vector of P2, in terms of
spherical unit vectors at P1.
1.33. Points P1 and P2 have Cartesian coordinates (2, 2, 0) and (1, 0, 1), re-
spectively.
(a) Find the spherical coordinates of P1.
(b) Express ˆer1, ˆeθ1, and ˆeϕ1, the spherical unit vectors at P1, in terms of the
Cartesian unit vectors.
(c) Find the components of the position vector of P2 along the spherical unit
vectors at P1.
(d) From its components in (c) ﬁnd the length of r2, and show that it agrees
with the length as calculated from its Cartesian components.
1.34. Points P1 and P2 have spherical coordinates
P1 : (a, π/4, π/3)
and
P2 : (a, π/3, π/4).
(a) Find the angle between their position vectors r1 and r2.
(b) Find the spherical components of r2 −r1 at P1.
(c) Find the spherical components of r2 −r1 at P2.
1.35. Point P1 has Cartesian coordinates (1, 1, 0), point P2 has cylindrical
coordinates (1, 1, 0), and point P3 has spherical coordinates (1, 1, 0) where all
angles are in radians. Express r3 −r1 in terms of the spherical unit vectors
at P2.
1.36. Points P1 and P2 have Cartesian coordinates (1, 1, 1) and (1, 2, 1), and
position vectors r1 and r2, respectively.
(a) Find the spherical coordinates of P1 and P2.
(b) Find the components of r1, in terms of spherical unit vectors at P1.

42
Coordinate Systems and Vectors
(c) Find the components of r2, in terms of spherical unit vectors at P2.
(d) Find the components of r1, in terms of spherical unit vectors at P2.
(e) Find the components of r2, in terms of spherical unit vectors at P1.
1.37. Points P1 and P2 have Cartesian coordinates
(x1, y1, z1)
and
(x2, y2, z2).
(a) Find the angle between their position vectors r1 and r2 in terms of their
coordinates.
(b) Find the Cartesian components of r2 −r1 at P1.
(c) Find the Cartesian components of r2 −r1 at P2.
1.38. Points P1 and P2 have cylindrical coordinates
(ρ1, ϕ1, z1)
and
(ρ2, ϕ2, z2)
(a) Find the angle between their position vectors r1 and r2 in terms of their
coordinates.
(b) Find the cylindrical components of r2 −r1 at P1.
(c) Find the cylindrical components of r2 −r1 at P2.
1.39. Write the Cartesian unit vectors in terms of spherical unit vectors with
coeﬃcients written in spherical coordinates.
1.40. Write the spherical unit vectors in terms of Cartesian unit vectors with
coeﬃcients written in Cartesian coordinates.
1.41. In Example 1.4.3, calculate the electric ﬁeld using cylindrical coordi-
nates, then ﬁnd the components in terms of (a) Cartesian and (b) spherical
unit vectors.
1.42. In Example 1.4.3, calculate the electric ﬁeld using spherical coordinates,
then ﬁnd the components in terms of (a) Cartesian and (b) cylindrical unit
vectors.

Chapter 2
Diﬀerentiation
Physics deals with both the large and the small. Its domain of study includes
the interior of the nucleus of an atom as well as the exterior of a galaxy. It is,
therefore, natural for the scope of physical theories to switch between global,
or large-scale, and local, or small-scale regimes. Such an interplay between
the local and the global has existed ever since Newton and others discovered
the mathematical translation of this interplay: Derivatives are deﬁned as local
objects while integrals encompass global properties. This chapter is devoted
to the concept of diﬀerentiation, which we shall consider as a natural tool with
which many physical concepts are expressed most concisely and conveniently.
All physical quantities reside in space and change with time. Even a static
quantity—once scrutinized—will reveal noticeable attributes of change, vali-
dating the old adage “The only thing that doesn’t change is the change itself.”
Thus, static, or time-independent, quantities are so only as approximations
to the true physical quantity which is dynamic.
Take the temperature of the surface of the Earth. As we move about on the
globe, we notice the variation of this quantity with location—poles as opposed
to the equator—and with time—winter versus summer.
A speciﬁcation of
temperature requires that of location and time. We thus speak of local and
instantaneous temperature. This is an example of the fact that, generally
speaking, all physical quantities are functions of space and time.
Locality and instantaneity have both a mathematical and a physical (or
operational) interpretation. Mathematically, they correspond to a point in
space and an instant of time with no extension or spread whatsoever. Physi-
cally, or operationally, many quantities require an extension in space and an
interval in time to be deﬁned. Thus, a local weatherman’s morning statement
“Today’s high will be 45” limits the location to the size of a city, and the time
to at most a.m. or p.m. This is admittedly a rough localization, suitable for
a weatherman’s forecast. Nevertheless, even the most precise statements in
physics embody a space extension as well as a time interval whose “sizes” are
determined by the physical system under investigation. If we are studying
heat conduction by a metal bar several inches long, then “local” temperature

44
Diﬀerentiation
takes a completely diﬀerent meaning from the weatherman’s “local” temper-
ature. In the latter case, a city is as local as one gets, while in the former,
variations over a centimeter are signiﬁcant.
2.1
The Derivative
A prime example of an instantaneously deﬁned quantity is velocity. To ﬁnd
velocity as an
example of an
instantaneously
deﬁned quantity
the velocity of a moving particle at time t0, determine its position r0 at time
t0, determine also its position r at time t with t close to t0, divide r −r0 by
t −t0, and make t −t0 as small as possible. This deﬁnes the derivative of r
with respect to t which we call velocity v:
derivative
v(t0) = lim
t→t0
r −r0
t −t0
≡dr
dt




t=t0
≡˙r(t0).
Acceleration is deﬁned similarly:
a(t0) = lim
t→t0
v −v0
t −t0
≡dv
dt




t=t0
≡d2r
dt2




t=t0
≡¨r(t0).
Velocity and acceleration are examples of derivatives which are generally
derivative as rate
of change:
independent
variable is time or
a coordinate
called rate of change. In the rate of change, one is interested in the way
a quantity (dependent variable) changes as another quantity (independent
variable) is allowed to vary. In the majority of rates of change, the independent
variable is either time or one of the space coordinates.
The second type of derivative is simply the ratio of two inﬁnitesimal phys-
ical quantities. In general, whenever a physical quantity Q is deﬁned as the
derivative as the
ratio of two
inﬁnitesimal
physical quantities
ratio of two other physical quantities R and S, one must deﬁne Q in a small
neighborhood (small volume, area, length, or time interval). One, therefore,
writes
Q = lim
ΔS→0
ΔR
ΔS ≡dR
dS ,
(2.1)
where ΔR and ΔS are both local small quantities. Being physical quantities,
both R and S, and therefore ΔR and ΔS are, in general, functions of position
and time. Hence, their ratio, Q, is also a function of position and time. The
last sentence requires further elaboration.
In physics, we deal with two completely diﬀerent, yet subtly related, ob-
jects: particles and ﬁelds. The former is no doubt familiar to the reader.
particles and ﬁelds
Examples of the latter are the gravitational, electric, and magnetic ﬁelds, as
well as the less familiar velocity ﬁeld of a ﬂuid such as water in a river or air
in the atmosphere. Suppose we want to specify the “state” of the two types
of objects at a particular time t. For a particle, this means determining its
position and momentum or velocity1 at t. Imagine the particle carrying with
1It is a fundamental result of classical mechanics that such a speciﬁcation completely
determines the subsequent motion of the particle and, therefore, any other property of the
particle will be speciﬁed by the initial position and momentum.

2.1 The Derivative
45
it a vector representing its velocity. Then a snapshot of the particle at time t
depicts its location as well as its velocity, and thus, a complete speciﬁcation
of the particle. A large collection of such snapshots speciﬁes the motion of the
particle. Since each snapshot represents an instant of time, and since the col-
lection of snapshots speciﬁes the motion, we conclude that, for particles, the
only independent variable is time.2 A problem involving a classical particle is
solved once we ﬁnd its position as a function of time alone.
How do we specify the “state” of a ﬂuid? A ﬂuid is an extended object,
diﬀerent parts of which behave diﬀerently. Attaching a vector to diﬀerent
points of the ﬂuid to represent the velocity at that point, and taking snapshots
at diﬀerent times, we can get an idea of how the ﬂuid behaves.
This is
done constantly (without the arrows, of course) by weather satellites whose
snapshots are sometimes shown on our TV screens and reveal, for example,
the turbulence developed by a hurricane. A complete determination of the
ﬂuid, therefore, entails a speciﬁcation of the velocity vector at diﬀerent points
of the ﬂuid for diﬀerent times. A vector which varies from point to point is
called a vector ﬁeld. A problem involving a classical ﬂuid is, therefore, solved
vector ﬁeld
once we ﬁnd its velocity ﬁeld as a function of position and time. The concept
of a ﬁeld can be abstracted from the physical reality of the ﬂuid.3 It then
becomes a legitimate physical entity whose speciﬁcation requires a position,
a time, and a direction (if the ﬁeld happens to be a vector ﬁeld), just like the
speciﬁcation of the velocity ﬁeld of a ﬂuid.
The reason for going into so much detail in the last two paragraphs is to
prevent a possible confusion. In the case of velocity and acceleration, one
divides two quantities and the limit of the ratio turns out to be a function
of the denominator, and one might get the impression that in (2.1), Q is a
function of S. This is not the case, as, in general, all three quantities, R, S,
and Q are functions of other (independent) variables, for instance, the three
coordinates specifying position and time.
Velocity and acceleration are examples of the ﬁrst interpretation of deriva-
tive, the rate of change. There are many situations in which the second inter-
pretation of derivative is applicable. One important example is the density
of a physical quantity R:
density: an
example of the
second
interpretation of
derivative
ρR =
lim
ΔV →0
ΔR
ΔV ≡dR
dV ,
(2.2)
where ΔR is the amount of the quantity R in the small volume ΔV . Examples
of densities are mass density ρm, electric charge density ρq, number density
2This is true only in a classical picture of particles.
A quantum mechanical picture
disallows a complete determination of the position and momentum of a particle.
3Historically, this abstraction was very hard to achieve in the case of electromagnetism,
where, for a long time a hypothetical “ﬂuid” called æther was assumed to support the
electromagnetic ﬁeld. It was Einstein who suggested getting rid of the ﬂuid altogether, and
attaching physical reality and signiﬁcance to the ﬁeld itself.

46
Diﬀerentiation
ρn, energy density ρE, and momentum density ρp. Sometimes it is convenient
to deﬁne surface and linear densities:
σR = lim
Δa→0
ΔR
Δa ≡dR
da ,
λR = lim
Δl→0
ΔR
Δl ≡dR
dl ,
(2.3)
where ΔR is the amount of R on the small area Δa or along the small length
Δl. The most frequently encountered surface density is that of electric charge
which is commonly found on the surface of a conductor.
Another example of Equation (2.1) is pressure deﬁned as
pressure: another
example of the
second
interpretation of
derivative
P = lim
Δa→0
ΔF⊥
Δa ≡dF⊥
da ,
(2.4)
where ΔF⊥is the force perpendicular to the surface Δa.
This discussion
makes it clear that The most natural setting for the concept of derivative is
the ratio of two physical quantities which are deﬁned locally. Equations (2.2)
and (2.3) are hardly interpreted as the rate of change of density with respect
to volume, area, or length!
Historical Notes
Descartes said that he “neither admits nor hopes for any principles in Physics other
than those which are in Geometry or in abstract mathematics.” And Nature couldn’t
agree more! The start of modern physics coincides with the start of modern mathe-
matics. Calculus was, in large parts, motivated by the need for a quantitative anal-
ysis of physical problems. Calculation of instantaneous velocities and accelerations,
determination of tangents to lens surfaces, evaluation of the angle corresponding to
the maximum range of a projectile, and calculation of the lengths of curves such
as the orbits of planets around the Sun were only a few of the physical motiva-
tions that instigated the intense activities of the seventeenth-century physicists and
mathematicians alike.
The problems mentioned above were tackled by at least a dozen of the greatest
mathematicians of the seventeenth century and many other minor ones. All of these
eﬀorts climaxed in the monumental achievements of Newton and Leibniz. Newton,
in particular, noted the generality of the concept of rate of change—a concept he
used for calculating instantaneous velocities—and bestowed a universal character
upon the notion of derivative.
Of the several methods advanced to ﬁnd the tangent to a curve, Fermat’s is the
closest to the modern treatment. He approximates the increment of the tangent line
with the increment of the function describing the curve and takes the ratio of the
two increments to ﬁnd the angle of the tangent line. Fermat, however, ignores the
question of limits as the increments go to zero, a procedure necessary for ﬁnding
the slope of tangents. Descartes method, on the other hand, is purely algebraic and
is not plagued by the question of the limits. However, his method worked only for
polynomials.
Another great name associated with the development of calculus is Isaac Barrow
who used elaborate geometrical methods to ﬁnd tangents. He was the ﬁrst to point
out the connection between integration and diﬀerentiation. Barrow was a professor

2.2 Partial Derivatives
47
of mathematics at Cambridge University. Well versed in both Greek and Arabic (he
was once nominated for a chair of Greek at Cambridge in 1655 but was denied the
chair due to his loyalist views), he was able to translate some of Euclid’s works and
to improve the translations of other works of Euclid as well as Archimedes.
After spending some time in eastern Europe, he returned to England and ac-
Isaac Barrow
1630–1677
cepted the Greek chair denied him before. To supplement his income, he taught
geometry at Gresham College, London. However, he soon gave up his geometry
chair to serve as the ﬁrst Lucasian professor of mathematics at Cambridge from
1663 to 1669, at which time Barrow resigned his chair of mathematics in favor of
his student Isaac Newton and turned to theological studies.
His chief work Lectiones Geometricae is one of the great contributions to cal-
culus. In it he used geometrical methods, “freed from the loathsome burdens of
calculations,” as he put it.
2.2
Partial Derivatives
All physical quantities are real functions of space and time. This means that
given the three coordinates of a point in space, and an instant of time, we
can associate a real number with them which happens to be the value of the
physical quantity at that point and time.4 Thus, Q(x, y, z, t) is the value of
the physical quantity Q at time t at a point whose Cartesian coordinates are
(x, y, z).
Similarly, we write Q(r, θ, ϕ, t) and Q(ρ, ϕ, z, t) for spherical and
cylindrical coordinates, respectively. Thus, ultimately, the physical quantities
are functions of four real variables. However, there are many circumstances in
which the quantity may be a function of less or more variables. An example of
the former is all static phenomena in which the quantity is assumed—really
approximated—to be independent of time. Then the quantity is a function
of only three variables.5 Physical quantities that depend on more than four
variables are numerous in physics: In the mechanics of many particles, all
quantities of interest depend, in general, on the coordinates of all particles,
and in thermodynamics one encounters a multitude of thermodynamical vari-
ables upon which many quantities of interest depend.
2.2.1
Deﬁnition, Notation, and Basic Properties
We consider real functions f(x1, x2, . . . , xn) of many variables.
General-
izing the notation that denotes the set of real numbers by R, the set of
points in a plane by R2, and those in space by R3, we consider the n-tuples
(x1, x2, . . . , xn) as points in a (hyper)space Rn. Similarly, just as the triplet
(x, y, z) can be identiﬁed with the position vector r, we abbreviate the n-tuple
(x1, x2, . . . , xn) by r. Constant n-tuples will be denoted by the same letter
4This statement is not strictly true. There are many physical quantities which require
more than one real number for their speciﬁcation.
A vector is a prime example which
requires three real numbers to be speciﬁed. Thus, a vector ﬁeld, which we discussed earlier,
is really a collection of three real functions.
5If the natural setting of the problem is a surface or a line, then the number of variables
is further reduced to two or one.

48
Diﬀerentiation
used for components but in boldface type. For example (a1, a2, . . . , an) ≡a
and (b1, b2, . . . , bn) ≡b. This suggests using x in place of r, and we shall do
so once in a while.
Being independent, we can vary any one of the variables of a function
at will while keeping the others constant. The concept of derivative is now
applied to such a variation. The result is partial derivative. To be more
precise, the partial derivative of f(r) with respect to the independent variable
xk at (a1, a2, . . . , an) is denoted6 by ∂f
∂xk (a) and is deﬁned as follows:
partial derivative
deﬁned
∂f
∂xk
(a) ≡lim
ϵ→0
f(a1, . . . , ak + ϵ, . . . , an) −f(a1, . . . , ak, . . . , an)
ϵ
.
(2.5)
One usually leaves out the a’s and simply writes ∂f
∂xk , keeping in mind that
the result has to be evaluated at some speciﬁc “point” of Rn. As the deﬁnition
suggests, the partial derivative with respect to xk is obtained by the usual rules
of diﬀerentiation with the proviso that all the other variables are assumed to
be constants.
A useful strategy is to turn Equation (2.5) around and write the incre-
ment in f in terms of the partial derivative. This possibility is the result of
the meaning of the limit: The closer ϵ gets to zero the better the ratio approx-
imates the partial derivative. Thus we can leave out limϵ→0 and approximate
the two sides. After multiplying both sides by ϵ, we obtain
Δkf ≡f(a1, . . . , ak + ϵ, . . . , an) −f(a1, . . . , ak, . . . , an) ≈ϵ ∂f
∂xk
,
where the subscript k on the LHS indicates the independent variable being var-
ied. Sometimes we use the notation Δkf(a) to emphasize the point at which
the increment of the function—due to an increment in the kth argument—is
being evaluated. Most of the time, however, for notational convenience, we
shall leave out the arguments, it being understood that all quantities are to
be evaluated at some speciﬁc “point.” Since ϵ is an increment in xk, it is
natural to denote it as Δxk, and write the above equation as
Δkf ≡f(a1, . . . , ak + Δxk, . . . , an) −f(a1, . . . , ak, . . . , an) ≈∂f
∂xk
Δxk.
If two independent variables, say xk and xj, are varied we still can ﬁnd
the increment in f:
Δk,jf ≡f(a1, . . . , ak + Δxk, . . . , aj + Δxj, . . . , an)
−f(a1, . . . , ak, . . . , aj, . . . , an)
= f(a1, . . . , ak + Δxk, . . . , aj + Δxj, . . . , an)
−f(a1, . . . , ak, . . . , aj + Δxj, . . . , an)
+ f(a1, . . . , ak, . . . , aj + Δxj, . . . , an)
−f(a1, . . . , ak, . . . , aj, . . . , an),
6This notation may be confusing because of the a’s and the x’s. A better notation will
be introduced shortly.

2.2 Partial Derivatives
49
where we have added and subtracted the same term on the RHS of this equa-
tion. Now we use the deﬁnition of the change in a function at a point to
write
Δk,jf = Δkf(a1, . . . , ak, . . . , aj + Δxj, . . . , an)
+ Δjf(a1, . . . , ak, . . . , aj, . . . , an)
≈Δxk
∂f
∂xk
(a1, . . . , ak, . . . , aj + Δxj, . . . , an)
+ Δxj
∂f
∂xj
(a1, . . . , ak, . . . , aj, . . . , an).
The ﬁrst term on the RHS expresses the change in the function due to a
change in xk, and the second expresses the change in the function due to a
change in xj. As their arguments show, the derivatives in the last two lines
are not evaluated at the same point. However, the diﬀerence between these
arguments is small—of order Δxj—which, when multiplied by the small Δx’s
in front of them, will be even smaller. In the limit that Δxj and Δxk go to
zero, we can ignore this subtle diﬀerence and write
Δk,jf ≈∂f
∂xk
Δxk + ∂f
∂xj
Δxj.
(2.6)
This shows that the total change is simply the sum of the change due to xj
and xk.
Box 2.2.1. In general, the change in f due to a change in all the inde-
pendent variables is Δf ≈n
i=1
∂f
∂xi Δxi.
Some of the Δx’s may be zero of course. For example, if all of the Δx’s are
zero except Δxj and Δxk, then the equation in the Box above reduces to
(2.6). The following example describes a situation which occurs frequently in
thermodynamics.
Example 2.2.1. Suppose a physical quantity Q is a function of other physical
quantities U, V , and W . We write this as Q = f(U, V, W ) with the intention that
U, V , and W are the independent variables. It is possible, however, to solve for one of
an example that is
useful for
thermodynamics
the independent variables in terms of Q and the rest of the independent variables.7 It
is therefore legitimate to seek the partial derivative of any one of the four quantities
with respect to any other one. Because of the multitude of thermodynamic variables,
it may become confusing as to which variables are kept constant. Therefore, it is
common in thermodynamics to use the variables held constant as subscripts of the
partial derivative. Thus,
 ∂Q
∂V

U,W
,
 ∂V
∂Q

U,W
,
 ∂U
∂V

Q,W
,
(2.7)
7That this can be done under very mild assumptions regarding the function f is the
content of the celebrated implicit function theorem proved in higher analysis.

50
Diﬀerentiation
are typical examples of partial derivatives, and in priciple, one can solve for V in
terms of Q, U, and W and diﬀerentiate the resulting funtion with respect to Q to
ﬁnd the second term in Equation (2.7). Similarly, one can solve for U in terms of Q,
V , and W and diﬀerentiate the resulting funtion with respect to V to ﬁnd the last
term. However, Box 2.2.1 allows us to bypass this (sometimes impossible) task and
evaluate derivatives by directly diﬀerentiating the given function. Let’s see how.
The ﬁrst term is obvious:
 ∂Q
∂V

U,W
=
 ∂f
∂V

U,W
.
The key to the evaluation of the other two is Box 2.2.1 as applied to Q. We thus
write
ΔQ ≈
 ∂f
∂U

V,W
ΔU +
 ∂f
∂V

U,W
ΔV +
 ∂f
∂W

U,V
ΔW.
(2.8)
If U and W are kept constant, then ΔU = 0 = ΔW , and we have
ΔQ ≈
 ∂f
∂V

U,W
ΔV
⇒1 ≈
 ∂f
∂V

U,W
ΔV
ΔQ.
In the limit that ΔQ goes to zero, the ratio of the Δ’s becomes the corresponding
partial derivative and the approximation becomes equality, leading to the relation
1 =
 ∂f
∂V

U,W
 ∂V
∂Q

U,W
.
Changing f to Q,8 and solving for the partial derivative, we obtain
 ∂V
∂Q

U,W
=
1
 ∂Q
∂V

U,W
(2.9)
which is a result we should have expected. This equation shows that we don’t have
to solve for V in terms of the other three variables to ﬁnd its derivative with respect
to Q. Just diﬀerentiate f(U, V, W ) with respect to V and take its reciprocal!
The last partial derivative is obtained by setting ΔQ and ΔW equal to zero in
(2.8). The result is
0 ≈
 ∂f
∂U

V,W
ΔU +
 ∂f
∂V

U,W
ΔV
⇒ΔU
ΔV ≈−
 ∂f
∂V

U,W
 ∂f
∂U

V,W
.
Once again, taking the limit as ΔV →0, noting that the LHS becomes a partial
derivative, subscripting this partial with the variables held constant, and substitut-
ing Q for f,9 we obtain
 ∂U
∂V

Q,W
= −
 ∂Q
∂V

U,W
 ∂Q
∂U

V,W
.
(2.10)
8Recall that if y = f(x), then dy/dx and df/dx represent the same quantity.
9This is an abuse of notation because Q is held constant and the derivative of any
constant is always zero, while the derivative of f is well deﬁned. This abuse of notation is
so common in thermodynamics that we shall adopt it here as well.

2.2 Partial Derivatives
51
Thus, by diﬀerentiating f(U, V, W ) with respect to V and U and taking their ratios,
we obtain the derivative of U with respect to V ; no need to solve for U in terms of
the other three variables!
Equation (2.10) is ususlly written in a more symmetric way. The numerator of
the fraction on the RHS can be replaced using Equation (2.9). Then, the result can
be written as
an important
relation used often
in
thermodynamics
 ∂U
∂V

Q,W
 ∂V
∂Q

U,W
∂Q
∂U

V,W
= −1.
(2.11)
A simpler version of this result, in which the fourth variable W is absent, is com-
monly used in thermodynamics.
■
A word of caution about notation is in order. We chose the set of vari-
ables (x1, x2, . . . , xn) as arguments of the function f, and then denoted the
derivative by ∂f/∂xk. We could have chosen any other set of symbols such
as (y1, y2, . . . , yn), or (t1, t2, . . . , tn) as the arguments. Then we would have
had to write ∂f/∂yk, or ∂f/∂tk for partial derivatives. This freedom of choice
confusion
surrounding the
expression
(∂f /∂x)(y, x)
and a notation
that resolves the
confusion
can become confusing because, little eﬀort is made in the literature to distin-
guish between the “free” general arguments and the speciﬁc point at which
the derivative is to be evaluated. For example, the symbol (∂f/∂x)(y, x) can
be interpreted in two ways: It can be the derivative of a function of two vari-
ables with respect to its ﬁrst argument, subsequently evaluated at the point
with coordinates (y, x), or it could be the derivative with respect to the sec-
ond argument, in a seemingly strange world in which y is used as the ﬁrst
argument! The longstanding usage of x as the ﬁrst partner of a doublet by no
means reserves the ﬁrst slot for x at all times. Therefore, the confusion above
is indeed a legitimate one.
We started the discussion by distinguishing between the free arguments
(x1, x2, . . . , xn) and the speciﬁc point (a1, a2, . . . , an). However, making this
distinction every time we write down a partial derivative can become very
clumsy. Nevertheless, the reader should always keep in mind this distinction
and write it down explicitly whenever necessary. To minimize the confusion,
we leave out all symbols but keep only the position of the variable in the array.
Speciﬁcally,
Box 2.2.2. We write ∂kf for the derivative of f with respect to its
kth argument.
This derivative is a function:
We can evaluate it at
(a1, a2, . . . , an), for which we write ∂kf(a1, a2, . . . , an) ≡∂kf(a).
This notation avoids any reference to the “free” arguments. One can choose
any symbol for the free arguments; the ﬁnal answer is independent of this
choice:
∂kf(a1, a2, . . . , an) = ∂f(t1, t2, . . . , tn)
∂tk




t=a
= ∂f(y1, y2, . . . , yn)
∂yk




y=a
= ∂f(♥1, ♥2, . . . , ♥n)
∂♥k




(♥1=a1,...,♥n=an)

52
Diﬀerentiation
because the only thing that matters is the index k which tells us with respect
to what variable we are diﬀerentiating.
Example 2.2.2. Consider the function f(x, y, z) = exy/z. We write it ﬁrst as
f(x1, x2, x3) = ex1x2/x3. Then
∂1f(x1, x2, x3) = (x2/x3)ex1x2/x3,
∂2f(x1, x2, x3) = (x1/x3)ex1x2/x3,
∂3f(x1, x2, x3) = −(x1x2/x2
3)ex1x2/x3.
Now that the functional form of all partial derivatives are derived, we can evaluate
them at any point we want. For example,
∂2f(1, 2, 3) = 1
3e2/3,
∂3f(1, 1, 1) = −e,
∂1f(t, u, v) = (u/v)etu/v,
∂3f(z, x, y) = −(zx/y2)exz/y.
■
Higher-order derivatives are deﬁned just as in the single-variable case,
except that now mixed derivatives are also possible. Thus,
∂1(∂1f) ≡∂2
1f ≡∂2f
∂x1
2 ,
∂1(∂5f) ≡
∂2f
∂x1∂x5
,
∂j(∂kf) ≡
∂2f
∂xj∂xk
,
are all legitimate derivatives. An important property of mixed derivatives is
order of
diﬀerentiation in a
mixed derivative is
immaterial
that—for well-behaved functions—the order of diﬀerentiation is immaterial.
Example 2.2.3. Functions which can be written as the product of single-variable
functions are important in the solution of partial diﬀerential equations. Suppose
that F(x, y, z) = f(x)g(y)h(z). Then ∂1F(x, y, z) = f ′(x)g(y)h(z) and the function
∂1F
F (x, y, z) = f ′(x)g(y)h(z)
f(x)g(y)h(z) = f ′(x)
f(x)
is seen to be independent of y and z. One can show similarly that
∂2F
F (x, y, z) = g′(y)
g(y) ,
∂3F
F (x, y, z) = h′(z)
h(z) ,
each one depending on only one variable.
■
Example 2.2.4. It is sometimes necessary to ﬁnd the most general function, one of
whose partial derivatives is given. This can be done by antidiﬀerentiating (indeﬁnite
integral) with respect to the variable of the partial derivative, treating the rest of the
variables constant. The usual “constant” of integration is replaced by a function of
the undiﬀerentiating variables. For example, suppose ∂3f(z, x, y) = yex2y2/z. Since
the third variable is y, and the partial derivative is with respect to the third variable,
we need to integrate with respect to y, keeping x and z constant. This gives
f(z, x, y) =
z
2x2 ex2y2/z + g(x, z) ⇒f(x, y, z) =
x
2y2 ey2z2/x + g(y, x),
where g, the “constant” of integration, is an arbitrary function of the ﬁrst two
variables.
■

2.2 Partial Derivatives
53
Δ f
+
f(x0)
+ df
f(x0)
f(x0)
x0
x0 + Δ x
Figure 2.1: The tangent line at x0 approximates the curve in a small neighborhood of
x0. If conﬁned in this neighborhood, i.e., if Δx—which is equal to dx—is small, Δf
and df are approximately equal. However, df is deﬁned regardless of the size of Δx.
2.2.2
Diﬀerentials
We now introduce the notion of diﬀerentials. Recall from calculus that, in
diﬀerentials
the case of one variable, the diﬀerential of a function is related to a linear
approximation of that function (see Figure 2.1). Basically, the tangent line at
a point x0 is considered as the linear approximation to the curve representing
the function f in the neighborhood of x0. The increment in the value of the
function representing the tangent line—denoted by df(x0)—when x0 changes
to x0 + Δx, is given by
df(x0) ≡
 df
dx

x0
Δx ≡
 df
dx

x0
dx,
where, as a matter of notation, Δx has been replaced by dx, because by deﬁ-
nition, the diﬀerential of an independent variable is nothing but its increment.
The above equation is not an approximation: dx can be any number, large or
small, and df(x0) will be correspondingly large or small. The approximation
starts when we try to replace Δf with df: The smaller the Δx = dx, the
better the approximation Δf(x0) ≈df(x0). The generalization of this idea
to two variables involves approximating the surface representing the function
f(x, y) by its tangent plane. For more variables, no visualizable geometric
interpretation is possible, but the basic idea is to replace the Δ’s with d’s and
the approximation with equality in Box 2.2.1. The result is
df = ∂f
∂x1
dx1 + ∂f
∂x2
dx2 + · · · + ∂f
∂xn
dxn =
n

i=1
∂f
∂xi
dxi.
(2.12)
We note that dxi’s in Equation (2.12) determine the independent variables
on which f depends, and the coeﬃcient of dxi is ∂f/∂xi. This observation is
the basis of transforming functions in such a way that the resulting functions
depend on variables which are physically more useful. To be speciﬁc, suppose a
function f exists which depends on (x, y, z), but from a physical perspective,
a function which depends on the derivative of f with respect to its second

54
Diﬀerentiation
argument, and not on the second argument itself, is more valuable.
This
function can be obtained by a Legendre transformation on f, obtained
Legendre
transformation
by subtracting from f the product of the second argument and the derivative
of f with respect to that argument. So, deﬁne a new function g by
g ≡f −y∂2f ≡f −yh
where
h = ∂2f.
Then, we get
dg = df −h dy −y dh = ∂1f dx + ∂2f dy + ∂3f dz −h dy −y dh
= ∂1f dx + ∂3f dz −y dh.
The diﬀerentials on the RHS of the last line indicate that the “natural” inde-
pendent variables for g are x, z, and h, and that
∂g
∂x = ∂1f,
∂g
∂z = ∂3f,
∂g
∂h = −y.
Legendre transformation is used frequently in thermodynamics and mechanics.
Example 2.2.5. The internal energy U of a thermodynamical system is a function
of entropy S, volume V , and number of moles N. These variables are called the
natural variables of U, and we write U(S, V, N). Temperature T, pressure P, and
natural variables
of thermodynamic
functions
chemical potential μ, are deﬁned as follows:
T =
∂U
∂S

V,N
,
P = −
 ∂U
∂V

S,N
,
μ =
 ∂U
∂N

S,V
,
where, as is common in thermodynamics, we have indicated the variables that are
held constant as subscripts. Entropy is a hard quantity to measure: If we were to
measure ∂U/∂S, we would have to ﬁnd the ratio of the change of U to that of S;
not an easy task! On the other hand, T is easy to measure, and thus it is desirable
to Legendre transform U to obtain a function which has T as a natural variable.
The Helmholtz free energy F is deﬁned as F = U −ST. We note that since
Helmholtz free
energy
dU = ∂U
∂S dS + ∂U
∂V dV + ∂U
∂N dN = T dS −P dV + μ dN,
we have
dF = dU −SdT −TdS = TdS −PdV + μdN



=dU
−SdT −TdS
= −SdT −PdV + μdN
and, therefore
 ∂F
∂T

V,N
= −S,
 ∂F
∂V

T,N
= −P,
 ∂F
∂N

T,V
= μ.
Helmholtz free energy is by far the most frequently used thermodynamic function,
because all its “natural” variables, namely, T, V , and N, are easily measurable
quantities.
■

2.2 Partial Derivatives
55
2.2.3
Chain Rule
In many cases of physical interest, the “independent” variables xi may in
turn depend on one or more variables. Let us denote these new independent
variables by (t1, t2, . . . , tm) and the functional dependence of xi by gi, so that
xi = gi(t1, t2, . . . , tm) ≡gi(t),
i = 1, 2, . . ., n.
(2.13)
As the t’s vary, so will the x’s and consequently the function f. Therefore, f
becomes dependent on the t’s and we can talk about partial derivatives of f
with respect to one of the t’s. To ﬁnd such a partial derivative, we go back to
Box 2.2.1 and substitute for Δxi in terms of Δt’s. From (2.13), we have
Δxi ≈∂gi
∂t1
Δt1 + ∂gi
∂t2
Δt2 + · · · + ∂gi
∂tm
Δtm =
m

j=1
∂gi
∂tj
Δtj, i = 1, 2, . . . , n.
Substituting this in the equation of Box 2.2.1 yields
Δf ≈∂f
∂x1
m

j=1
∂g1
∂tj
Δtj + ∂f
∂x2
m

j=1
∂g2
∂tj
Δtj + · · · + ∂f
∂xn
m

j=1
∂gn
∂tj
Δtj
=
n

i=1
∂f
∂xi
m

j=1
∂gi
∂tj
Δtj.
(2.14)
Now suppose that we keep all of the t’s constant except for one, say t7. Then
Δtj = 0 for all j except j = 7 and the sum over j will have only one nonzero
term, i.e., the seventh term. In such a case, Equation (2.14) becomes
Δf ≈∂f
∂x1
∂g1
∂t7
Δt7 + ∂f
∂x2
∂g2
∂t7
Δt7 + · · · + ∂f
∂xn
∂gn
∂t7
Δt7 =
n

i=1
∂f
∂xi
∂gi
∂t7
Δt7.
Dividing both sides by Δt7, taking limit, and replacing the approximation by
equality, we obtain
∂f
∂t7
= ∂f
∂x1
∂g1
∂t7
+ ∂f
∂x2
∂g2
∂t7
+ · · · + ∂f
∂xn
∂gn
∂t7
=
n

i=1
∂f
∂xi
∂gi
∂t7
.
Instead of t7, we could have used any other one of the t’s, say t19, or t217.
the chain rule
Theorem 2.2.6. (The Chain Rule). Let f(x) be a function of the xi and
xi = gi(t). Let h(t) = f(g1(t), g2(t), . . . , gn(t)) be a function of the tk, called
the composite of f and the gi. If tp is any one of these t’s, then
∂ph(t) =
n

i=1
∂if(g(t))∂pgi(t),
(2.15)
where g = (g1, g2, . . . , gn), and g(t) ≡(g1(t), g2(t), . . . , gn(t)).

56
Diﬀerentiation
In words, the chain rule states that to evaluate the partial derivative of h
with respect to its pth argument (of which there are m) at t, multiply the ith
partial of f evaluated at g(t) by the pth partial of gi evaluated at t and sum
over i.
Sometimes the chain rule is written in the following less precise form:
∂f
∂tp
=
n

i=1
∂f
∂xi
∂gi
∂tp
=
n

i=1
∂f
∂xi
∂xi
∂tp
,
(2.16)
where in the last line we have substituted xi for gi.
Example 2.2.7. Suppose F is a function of three variables given by
F(x, y, z) = f
 x2y
az2

,
where f is some given function, and a is a constant. Let us calculate all partial
derivatives of F at (a, 2a, a) assuming that f ′(2) = a. Denote the single variable of
f by u, so that F is obtained by substituting x2y/(az2) for u in f(u). The chain
rule gives
∂1F(x, y, z) = f ′(u)∂1u = f ′(u)2xy
az2 ,
∂2F(x, y, z) = f ′(u)∂2u = f ′(u) x2
az2 ,
∂3F(x, y, z) = f ′(u)∂3u = −2f ′(u)x2y
az3 .
If x = a, y = 2a, and z = a, then u = a2(2a)/a3 = 2, and
∂1F(a, 2a, a) = f ′(2)2a(2a)
a3
= 4.
Similarly, ∂2F(a, 2a, a) = 1 and ∂3F(a, 2a, a) = −4.
In the notation of Theorem 2.2.6, there are three t’s: t1 = x, t2 = y, t3 = z, and
only one g: g(t1, t2, t3) = t2
1t2/(at2
3). Then F becomes the composite function of f
and g.
■
Example 2.2.8. One of the important occasions of the use of the chain rule is in
the transformation of derivatives from Cartesian to spherical coordinates. A good
example of such a transformation occurs in quantum mechanics where an expression
such as x∂f/∂y −y∂f/∂x turns out to be related to angular momentum, and it is
most conveniently expressed in spherical coordinates. In this example we go through
the detailed exercise of converting that expression into spherical coordinates.
We start with the transformations
x = r sin θ cos ϕ,
y = r sin θ sin ϕ,
z = r cos θ,
and their inverse
r =
	
x2 + y2 + z2,
cos θ =
z
	
x2 + y2 + z2 ,
tan ϕ = y
x.
(2.17)

2.2 Partial Derivatives
57
We shall need the derivatives of spherical coordinates with respect to x and y written
how derivatives
are transformed
under a coordinate
transformation
in terms of spherical coordinates. We easily ﬁnd these by diﬀerentiating both sides
of the equations in (2.17):
∂r
∂x = ∂
∂x
	
x2 + y2 + z2

= 1
2
1
	
x2 + y2 + z2 (2x) = x
r = sin θ cos ϕ,
−sin θ ∂θ
∂x = z

−1
2
1
r3 2x

= −xz
r3 = −sin θ cos ϕ cos θ
r
⇒
∂θ
∂x = cos θ cos ϕ
r
,
sec2 ϕ∂ϕ
∂x = −y
x2 = −
sin ϕ
r sin θ cos2 ϕ ⇒∂ϕ
∂x = −sin ϕ
r sin θ .
Similarly,
∂r
∂y = sin θ sin ϕ,
∂θ
∂y = cos θ sin ϕ
r
,
∂ϕ
∂y = cos ϕ
r sin θ .
Therefore, using the chain rule as given in Equation (2.16), we get
∂f
∂x = ∂f
∂r
∂r
∂x + ∂f
∂θ
∂θ
∂x + ∂f
∂ϕ
∂ϕ
∂x
= sin θ cos ϕ∂f
∂r + cos θ cos ϕ
r
∂f
∂θ −sin ϕ
r sin θ
∂f
∂ϕ,
∂f
∂y = ∂f
∂r
∂r
∂y + ∂f
∂θ
∂θ
∂y + ∂f
∂ϕ
∂ϕ
∂y
= sin θ sin ϕ∂f
∂r + cos θ sin ϕ
r
∂f
∂θ + cos ϕ
r sin θ
∂f
∂ϕ.
If we multiply the ﬁrst of the last two equations by y = r sin θ sin ϕ and subtract
it from the second equation multiplied by x = r sin θ cos ϕ, the terms involving
derivatives with respect to r and θ cancel while the terms with ϕ derivatives add to
give
x∂f
∂y −y ∂f
∂x = ∂f
∂ϕ.
Details are left as an exercise for the reader.
■
There is a multitude of examples in thermodynamics, for which a mastery
of the techniques of partial diﬀerentiation is essential. A property that is used
often in thermodynamics is homogeneity of functions which we derive below.
2.2.4
Homogeneous Functions
A function is called homogeneous of degree q if multiplying all of its arguments
by a parameter λ results in the multiplication of the function itself by λq. More
precisely,
Box 2.2.3. We say that f(x1, x2, . . . , xn) is homogeneous of degree q
if f(λx1, λx2, . . . , λxn) = λqf(x1, x2, . . . , xn).

58
Diﬀerentiation
Two cases merit special consideration. When q = 1, the function changes
at exactly the same rate as its arguments: Doubling all its arguments doubles
extensive and
intensive functions
the function and so on. Such a function is called extensive. When q = 0,
the function is called intensive, and it will not change if all its arguments
are changed by exactly the same factor.
In many cases, we want a relation between f and its partial derivatives.
We shall ﬁnd this relation by diﬀerentiating both sides of Box 2.2.3 with
respect to λ. To avoid any confusion, let us evaluate both sides at the point
(b1, b2, . . . , bn) after diﬀerentiation. Diﬀerentiation of the RHS is easy:
RHS = qλq−1f(b1, b2, . . . , bn).
For the LHS, we ﬁrst let yi = λxi for all i = 1, 2, . . . , n—so that we have a
single variable (one symbol) in the ith place—and note that
LHS = ∂f
∂λ =
n

i=1
∂f
∂yi
∂yi
∂λ =
n

i=1
[∂if(y1, y2, . . . , yn)]xi,
where we have used the fact that ∂yi
∂λ = xi—by the deﬁnition of yi. Evaluating
the result at xi = bi, we obtain
LHS =
n

i=1
bi∂if(λb1, λb2, . . . , λbn).
Equating the LHS and the RHS, we obtain the important result
qλq−1f(b1, b2, . . . , bn) =
n

i=1
bi∂if(λb1, λb2, . . . , λbn)
This relation holds for all values of λ, in particular we can substitute λ = 1 to
obtain qf(b1, b2, . . . , bn) = n
i=1 bi∂if(b1, b2, . . . , bn). Keep in mind that the
b’s, although ﬁxed, are completely arbitrary. In particular, one can substitute
x’s for them and arrive at the functional relation
relation between a
homogeneous
function and the
sum of its partial
derivatives
qf(x1, x2, . . . , xn) =
n

i=1
xi∂if(x1, x2, . . . , xn).
(2.18)
This is the relation we were looking for.
Another important result, which the reader is asked to derive in Problem
2.17, is
Box 2.2.4. If f is homogeneous of degree q, then ∂if is homogeneous of
degree q −1.

2.3 Elements of Length, Area, and Volume
59
Example 2.2.9. We have already seen that the natural variables of the internal
extensive and
intensive variables
of
thermodynamics
and their relation
to homogeneous
functions
energy U of a thermodynamical system are entropy S, volume V , and number of
moles N. Based on physical intuition, we expect the total internal energy, entropy,
volume and number of moles of the combined system to be doubled when two iden-
tical systems are brought together. We conclude that the internal energy function
increases by the same factor as its arguments. A thermodynamic quantity that has
this property is called an extensive variable. It follows that U is an extensive
variable and a homogeneous function of degree one.
Now consider temperature T, pressure P, and chemical potential μ, which are
all partial derivatives of U with respect to its natural variables.
From Problem
2.17, we conclude that these quantities are homogeneous of degree zero. It follows
that, if we bring two identical systems together, temperature, pressure, and the
chemical potential will not change, a result expected on physical grounds. Such a
thermodynamic quantity is called an intensive variable.
■
2.3
Elements of Length, Area, and Volume
We mentioned earlier the signiﬁcance of the second interpretation of the
derivative in conjunction with density. This interpretation is often used in
reverse order, i.e., in writing the inﬁnitesimal (element) of the physical quan-
tity as a product of density and the element of volume (or area, or length).
These elements appear inside integrals and will be integrated over (see the
next chapter). As a concrete example, let us consider the mass element which
can be expressed as
volume distribution:
dm(r′) = ρ(r′) dV (r′)
surface distribution:
dm(r′) = σ(r′) da(r′)
linear distribution:
dm(r′) = λ(r′) dl(r′)
where r′ denotes the coordinates of the location of the element of mass.
The relations above reduce the problem to that of writing the elements
of volume, area, and length. Most of the time, the evaluation of the integral
simpliﬁes considerably if we choose the correct coordinate system. Therefore,
we need these elements in all three coordinate systems.
Basic to the calculation of all elements are elements of length in the direc-
tion of unit vectors in any of the three coordinate systems. First we deﬁne
Box 2.3.1. The primary curve along any given coordinate is the curve
obtained when that coordinate is allowed to vary while the other two coor-
dinates are held ﬁxed.
The primary length elements are inﬁnitesimal lengths along the primary
primary length
elements
curves. By construction, they are also inﬁnitesimal lengths along unit vectors.
To ﬁnd a primary length element at point P ′ with position vector r′ along

60
Diﬀerentiation
a given unit vector, one keeps the other two coordinates ﬁxed and allows
the given coordinate to change by an inﬁnitesimal amount.10 This procedure
displaces P ′ an inﬁnitesimal distance. The length of this displacement, written
in terms of the coordinates of P ′, is the primary length element along the
given unit vector.
Once the three primary length elements are found, we
can calculate area and volume elements by multiplying appropriate length
elements.
A notion related to the primary length is
Box 2.3.2. A primary surface perpendicular to a primary length is
obtained when the coordinate determining the primary length is held ﬁxed
and the other two coordinates are allowed to vary arbitrarily.
The primary element of area at a point on a primary surface is, by deﬁni-
primary area
elements
tion, the product of the two primary length elements whose coordinates deﬁne
that surface.
Integrating over a primary surface of a coordinate system is facilitated if
all boundaries of the surface can be described by qi = ci where qi is either of
the two coordinates that vary on the surface and ci is a constant. For example,
the third primary surface in Cartesian coordinates is a plane parallel to the
xy-plane. A problem involving integration on this plane becomes simplest if
the boundaries of the region of integration are of the form, x = c1 and y = c2,
i.e., if the region of integration is a rectangle.
Finally, by taking the product of all three primary length elements, we
obtain the volume element in the given coordinate system.
2.3.1
Elements in a Cartesian Coordinate System
Consider the point P ′ with coordinates (x′, y′, z′) as shown in Figure 2.2. To
ﬁnd the primary length along ˆex′ = ˆex,11 keep y′ and z′ ﬁxed and let x′
change to x′ + dx′. Then P ′ will be displaced by dx′ along ˆex. Thus, the
ﬁrst primary length element—denoted by dl1—is simply dx′. Similarly, we
have dl2 = dy′, and dl3 = dz′. A general inﬁnitesimal displacement, which is
a vector, can be written as
general Cartesian
inﬁnitesimal
displacement
d⃗l = ˆex dl1 + ˆey dl2 + ˆez dl3 = ˆex dx′ + ˆey dy′ + ˆez dz′ ≡dr′.
(2.19)
Figure 2.2 shows that d⃗l represents the displacement vector from P ′, with
position vector r′, to a neighboring point P ′′, with position vector r′′. But
this displacement is simply the increment in the position vector of P ′. That is
why dr′ is also used for d⃗l. Note that this vectorial inﬁnitesimal displacement
10Usually an inﬁnitesimal amount is expressed by a diﬀerential. Thus, an increment in x
is simply dx.
11Recall that this equality holds in Cartesian—and only in Cartesian—coordinates, where
the unit vectors are independent of the coordinates of P ′.

2.3 Elements of Length, Area, and Volume
61
x
y
z
'
x
'
y
e^x
dl1 = dx
dl3 = dz
dl2 = dy
'
'
'
z
'
'
P
''
P
y
e^
ze^
Figure 2.2: Elements of length, area, and volume in Cartesian coordinates.
includes the primary length elements as special cases: When a coordinate is
held ﬁxed, the corresponding diﬀerential will be zero. Thus, setting dy′ = 0 =
dz′, i.e., holding y′ and z′ ﬁxed, we recover the ﬁrst primary length element.
The length of d⃗l is also of interest:
dl ≡|d⃗l| =

dl2
1 + dl2
2 + dl2
3
=
	
(dx′)2 + (dy′)2 + (dz′)2 ≡
	
dx′2 + dy′2 + dz′2.
(2.20)
In one-dimensional problems involving curves, one is either given, or has
to ﬁnd, the parametric equation of a curve γ whereby the coordinates
parametric
equation of a
curve
(x′, y′, z′) of a point on γ are expressed as functions of a parameter, usually
denoted by t. This is concisely written as
γ(t) = (x′, y′, z′) ≡
 
f(t), g(t), h(t)
!
,
so that the “curve function” γ takes a real number t and gives three real
numbers f(t), g(t), and h(t) which are the coordinates x′, y′, and z′ of a
point on the curve in space. Usually one considers an interval12 (a, b) for the
real variable t. Then
 
f(a), g(a), h(a)
!
is the initial point of the curve and
 
f(b), g(b), h(b)
!
its ﬁnal point. The parameter t and the functions f, g, and
h are not unique. For example, the three functions
f1(t) = a cost,
g1(t) = a sin t,
h1(t) = 0,
0 ≤t ≤π,
describe a semicircle in the xy-plane. However,
f2(t) = a cos
 
t3!
,
g2(t) = a sin
 
t3!
,
h2(t) = 0,
0 ≤t ≤π1/3,
also describe the same semicircle. This arbitrariness is useful, because it allows
us to choose f, g, and h so that calculations become simple.
12Do not confuse this with the coordinates of a point in the plane. The notation (a, b)
here means all the real numbers between a and b excluding a and b themselves.

62
Diﬀerentiation
For “ﬂat” curves [lying in the xy-plane and given by an equation y = f(x)],
one obvious parameterization—which may not be the most convenient one—is
x = t, y = f(t).
Let us assume that we have chosen the three functions and they are of the
form
x′ = f(t),
y′ = g(t),
z′ = h(t).
Then the primary lengths can be written as
dx′ = f ′(t) dt,
dy′ = g′(t) dt,
dz′ = h′(t) dt,
and the element of displacement along the curve becomes
the inﬁnitesimal
element of
displacement
along a curve
dr′(t) = d⃗l(t) = ˆex f ′(t) dt + ˆey g′(t) dt + ˆez h′(t) dt,
|dr′(t)| = dl(t) =
	
[f ′(t) dt]2 + [g′(t) dt]2 + [h′(t) dt]2
=
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2 dt,
(2.21)
where a prime on a function denotes its derivative with respect to its argu-
ment.13
The ﬁrst primary surface at P ′ is obtained by holding x′ constant and
letting the other two coordinates vary arbitrarily. It is clear that the resulting
primary surfaces
of Cartesian
coordinates are
planes
surface is a plane passing through P ′ and parallel to the yz-plane. It is also
clear that the ﬁrst primary length element, dx′ is perpendicular to the ﬁrst
primary surface. The ﬁrst primary element of area, denoted by da1, is simply
dy′ dz′. The second and third primary surfaces are the xz-plane and the xy-
plane, respectively. These planes are perpendicular to their corresponding
length elements. The primary elements of area are obtained similarly. We
thus have
primary elements
of area in
Cartesian
coordinates
da1 = dy′ dz′,
da2 = dx′ dz′,
da3 = dx′ dy′.
(2.22)
Finally, the volume element is
element of volume
in Cartesian
coordinates
dV = dl1 dl2 dl3 = dx′dy′dz′.
(2.23)
2.3.2
Elements in a Spherical Coordinate System
The point P ′ in Figure 2.3 now has coordinates (r′, θ′, ϕ′). To ﬁnd the primary
length along ˆer′, keep θ′ and ϕ′ ﬁxed and let r′ change to r′ + dr′. Then P ′
will be displaced by dr′ along ˆer′. Thus, the ﬁrst primary length element, dl1,
is simply dr′. To ﬁnd the primary length along ˆeθ′, keep r′ and ϕ′ ﬁxed, i.e.,
13The use of primes to represent both the derivative and the coordinates of the element of
the source (such as dm) is unfortunately confusing. However, this practice is so widespread
that any alteration to it would result in more confusion. The context of any given problem
is usually clear enough to resolve such confusion.

2.3 Elements of Length, Area, and Volume
63
′
P
′′
P
Δ
′
ϕ
Δ
′
θ
Δ ′r
′r Δ
′
θ
′r sin
′
θ Δ
′
ϕ
x
y
z
Figure 2.3: Elements of length, area, and volume in spherical coordinates. We have
used “Δ” instead of “d.”
conﬁne yourself to the plane passing through P ′ and the polar—or z—axis,
and let θ′ change to θ′ + dθ′. Then P ′ will be displaced by14 r′ dθ′ along
ˆeθ′. The primary length along ˆeϕ′ is obtained by keeping r′ and θ′ ﬁxed,
i.e., conﬁning oneself to a plane passing through P ′ and perpendicular to the
z-axis,15 and letting ϕ′ change to ϕ′ + dϕ′. Then P ′ will be displaced along
a circle of radius r′ sin θ′ by an angle dϕ′. This can be seen by noting that P ′
lies in the xy-plane and that its distance from the z-axis is given by
x′2 + y′2 = (r′ sin θ′ cos ϕ′)2 + (r′ sin θ′ sin ϕ′)2 = r′2 sin2 θ′
and that the RHS, which is the square of the radius of the circle, is a con-
stant. The displacement of P ′ is therefore r′ sin θ′ dϕ′ along ˆeϕ′. A general
inﬁnitesimal (vector) displacement can, therefore, be written as
general spherical
inﬁnitesimal
displacement
dr′ = d⃗l = ˆer′ dl1 + ˆeθ′ dl2 + ˆeϕ′ dl3
= ˆer′ dr′ + ˆeθ′ r′ dθ′ + ˆeϕ′ r′ sin θ′ dϕ′.
(2.24)
Note again that this vectorial inﬁnitesimal displacement includes the primary
length elements as special cases. Thus, setting dθ′ = 0 = dϕ′, i.e., holding θ′
and ϕ′ ﬁxed, we recover the ﬁrst primary length element. The length of dr′
(or d⃗l) is
do not confuse
|dr′| with dr′, they
are not equal.
|dr′| = dl =
	
(dr′)2 + (r′ dθ′)2 + (r′ sin θ′ dϕ′)2
=

dr′2 + r′2 dθ′2 + r′2 sin2 θ′ dϕ′2.
(2.25)
14Since r′ is held ﬁxed, P ′ is conﬁned to move on a circle of radius r′, describing an
inﬁnitesimal arc subtended by the angle d θ′.
15Fixing r′ and θ′ ﬁxes z′ = r′ cos θ′ which describes a plane parallel to the xy-plane, i.e.,
a plane perpendicular to the z-axis.

64
Diﬀerentiation
If we know the parametric equation of a curve in spherical coordinates,
i.e., if the coordinates r′, θ′, and ϕ′ of a point on the curve can be expressed as
functions of the parameter t, then we can ﬁnd the diﬀerentials in terms of dt
and substitute in Equation (2.25) to ﬁnd an expression analogous to Equation
(2.21). We leave this as an exercise for the reader.
The ﬁrst primary surface at P ′ is obtained by holding r′ constant and
letting the other two coordinates vary arbitrarily. It is clear that the resulting
primary surfaces
of spherical
coordinates
consist of a
sphere, a cone,
and a plane.
surface is a sphere of radius r′ passing through P ′. It is also clear that the
ﬁrst primary length element dr′ is perpendicular to the ﬁrst primary surface.
It is not hard to convince oneself that the second and third primary surfaces
are, respectively, a cone of (half) angle θ′, and a plane containing the z-axis
and making an angle of ϕ′ with the x-axis. These surfaces are perpendicular
to their corresponding length elements.
The primary elements of area are
obtained easily. We simply quote the results:
primary elements
of area in spherical
coordinates
da1 = (r′ dθ′)(r′ sin θ′ dϕ′) = r′2 sin θ′ dθ′ dϕ′,
da2 = (dr′)(r′ sin θ′ dϕ′) = r′ sin θ′ dr′ dϕ′,
(2.26)
da3 = (dr′)(r′ dθ′) = r′ dr′ dθ′.
Finally, the volume element is
element of volume
in spherical
coordinates
dV = (dr′)(r′ dθ′)(r′ sin θ′ dϕ′) = r′2 sin θ′ dr′dθ′dϕ′.
(2.27)
Table 2.1 gathers together all the primary curves and surfaces for the
three coordinate systems used frequently in this book. The reader is advised
to remember that
Box 2.3.3. All the diﬀerentials of Table 2.1 carry a prime to emphasize
that they are evaluated at P ′, the location of inﬁnitesimal elements.
Coordinate
Primary
Primary
system
curves
surfaces
1st: Straight line (x-axis)
yz-plane
Cartesian
2nd: Straight line (y-axis)
xz-plane
3rd: Straight line (z-axis)
xy-plane
1st: Rays perp. to z-axis
Cylinder with axis z
Cylindrical
2nd: Circle centered on z-axis
Half-plane from z-axis
3rd: Straight line (z-axis)
Plane perp. z-axis
1st: Rays from origin
Sphere
Spherical
2nd: Half-circle
Cone of half angle θ
3rd: Circle centered on polar axis
Half-plane from z-axis
Table 2.1: Primary curves and surfaces of the three common coordinate systems.

2.3 Elements of Length, Area, and Volume
65
2.3.3
Elements in a Cylindrical Coordinate System
The coordinates of P ′ are now (ρ′, ϕ′, z′) as shown in Figure 2.4. To ﬁnd the
primary length along ˆeρ′, keep ϕ′ and z′ ﬁxed and let ρ′ change to ρ′ + dρ′.
Then P ′ will be displaced by dρ′ along ˆeρ′. Thus, the ﬁrst primary length
element dl1 is simply dρ′. To ﬁnd the primary length along ˆeϕ′, keep ρ′ and z′
ﬁxed, i.e., conﬁne yourself to a circle of radius ρ′ in the plane passing through
P ′ and perpendicular to the z-axis, and let ϕ′ change to ϕ′ + dϕ′. Then P ′
will be displaced by ρ′ dϕ′ along ˆeϕ′. The primary length along ˆez′ = ˆez is16
obtained by keeping ρ′ and ϕ′ ﬁxed, and letting z′ change to z′ + dz′. Then
general cylindrical
inﬁnitesimal
displacement
P ′ will be displaced by dz′. A general inﬁnitesimal (vector) displacement can,
therefore, be written as
dr′ = d⃗l = ˆeρ′ dl1 + ˆeϕ′ dl2 + ˆez′ dl3
= ˆeρ′ dρ′ + ˆeϕ′ ρ′ dϕ′ + ˆez dz′.
(2.28)
Note again that this inﬁnitesimal displacement includes the primary length
elements as special cases. The length of this vector is
|dr′| = dl =
	
(dρ′)2 + (ρ′ dϕ′)2 + (dz′)2
=
	
dρ′2 + ρ′2dϕ′2 + dz′2.
(2.29)
If we know the parametric equation of a curve in cylindrical coordinates,
i.e., if the coordinates ρ′, ϕ′, and z′ of a point on the curve can be expressed as
x
y
z
Δ ′ 
ϕ 
′ 
P 
′ ′ 
P 
′ 
ρ Δ ′ 
ϕ 
Δ ′ 
ρ 
′ 
Δz 
Figure 2.4: Elements of length, area, and volume in cylindrical coordinates. We have
used “Δ” instead of “d.”
16This is the only unit vector in “curvilinear coordinates” which is independent of the
position of P ′.

66
Diﬀerentiation
functions of the parameter t, then we can ﬁnd the diﬀerentials in terms of dt
and substitute in Equation (2.29) to ﬁnd an expression analogous to Equation
(2.21). We leave this as an exercise for the reader.
The ﬁrst primary surface at P ′ is obtained by holding ρ′ constant and
letting the other two coordinates vary arbitrarily. It is clear that the resulting
surface is a cylinder of radius ρ′ passing through P ′. It is also clear that the
ﬁrst primary length element dρ′ is perpendicular to the ﬁrst primary surface.
The second and third primary surfaces are, respectively, a plane containing
the z-axis and making an angle of ϕ′ with the x-axis, and a plane perpen-
dicular to the z-axis and cutting it at z′. These surfaces are perpendicular to
primary surfaces
of cylindrical
coordinates
consist of a
cylinder and two
planes.
their corresponding length elements. The primary elements of area are again
obtained easily, and we merely quote the results
primary elements
of area in
cylindrical
coordinates
da1 = (ρ′ dϕ′)(dz′) = ρ′ dϕ′ dz′,
da2 = dρ′dz′,
(2.30)
da3 = (dρ′)(ρ′ dϕ′) = ρ′ dρ′ dϕ′.
Finally, the volume element is
element of volume
in cylindrical
coordinates
dV = (dρ′)(ρ′ dϕ′)(dz′) = ρ′ dρ′ dϕ′ dz′.
(2.31)
Table 2.2 gathers together all the elements of primary length, surface, and
volume for the three commonly used coordinate systems.
Example 2.3.1. Examples of elements in various coordinate systems
(a) The element of length in the ϕ direction at a point with spherical coordinates
(a, γ, ϕ) is a sin γ dϕ. Note that this element is independent of ϕ, and for a ﬁxed a,
it has the largest value when γ = π/2, corresponding to the equatorial plane.
(b) The element of area for a cone of half-angle α is r sin α dr dϕ, because for a cone,
θ is a constant (in this case, α).
Coordinate
Primary
Primary
Volume
system
length
area
element
elements
elements
1st: dx
dy dz
Cartesian
2nd: dy
dx dz
dx dy dz
(x, y, z)
3rd: dz
dx dy
1st: dρ
ρ dϕ dz
Cylindrical
2nd: ρ dϕ
dρ dz
ρ dρ dϕ dz
(ρ, ϕ, z)
3rd: dz
ρ dρ dϕ
1st: dr
r2 sin θ dθ dϕ
Spherical
2nd: r dθ
r sin θ dr dϕ
r2 sin θ dr dθ dϕ
(r, θ, ϕ)
3rd: r sin θ dϕ
r dr dθ
Table 2.2: Primary length and area as well as volume elements in the three common
coordinate systems. In almost all applications of the next chapter each of these variables
carries a prime.

2.3 Elements of Length, Area, and Volume
67
(c) The element of area of a cylinder of radius a is a dϕ dz.
(d) The element of area of a sphere of radius a is a2 sin θ dθ dϕ. Note that the largest
element of area (for given dθ and dϕ) is at the equator and the smallest (zero) at
the two poles.
(e) The element of area of a half-plane containing the z-axis and making an angle
α with the x-axis is dρ dz, independent of the angle α.
■
Example 2.3.2. Suppose Cartesian coordinates of the plane are related to two
ﬁnding unit
vectors without
use of geometry!
other variables u and v via the formulas
x = f(u, v),
y = g(u, v).
We want to consider u and v as coordinates and ﬁnd the unit vectors corresponding
to them using our knowledge of diﬀerentiation gained in this chapter without any
resort to geometric arguments.
In general, the unit vector in the direction of any coordinate variable at a point P
is obtained by increasing the coordinate slightly (keeping other coordinate variables
constant), calculating the displacement vector described by the motion of P, and
dividing this vector by its length. So, consider changing u while v is kept constant.
Call the displacement obtained Δ⃗l1. Then
Δ⃗l1 = ˆexΔx + ˆeyΔy = ˆex ∂f
∂uΔu + ˆey ∂g
∂uΔu
and
|Δ⃗l1| =
	
(Δx)2 + (Δy)2 =
"∂f
∂u
2
+
 ∂g
∂u
2
Δu.
Therefore,
ˆeu = lim
Δu→0
Δ⃗l1
|Δ⃗l1|
=
ˆex ∂f
∂u + ˆey ∂g
∂u
 ∂f
∂u
2
+
 ∂g
∂u
2 =
ˆex ∂x
∂u + ˆey ∂y
∂u
 ∂x
∂u
2
+
 ∂y
∂u
2 .
For ˆev, we keep u ﬁxed and vary v. Calling the resulting displacement Δ⃗l2, we
easily obtain
ˆev = lim
Δv→0
Δ⃗l2
|Δ⃗l2|
=
ˆex ∂f
∂v + ˆey ∂g
∂v
 ∂f
∂v
2
+
 ∂g
∂v
2 ==
ˆex ∂x
∂v + ˆey ∂y
∂v
 ∂x
∂v
2
+
 ∂y
∂v
2 .
Note that for general f and g, ˆeu and ˆev are not perpendicular.
The result can easily be generalized to three variables. In fact, if
x = f(u, v, w),
y = g(u, v, w),
z = h(u, v, w),

68
Diﬀerentiation
then, a similar calculation as above will yield
ˆeu =
ˆex ∂f
∂u + ˆey ∂g
∂u + ˆez ∂h
∂u
 ∂f
∂u
2
+
 ∂g
∂u
2
+
 ∂h
∂u
2 =
ˆex ∂x
∂u + ˆey ∂y
∂u + ˆez ∂z
∂u
 ∂x
∂u
2
+
 ∂y
∂u
2
+
 ∂z
∂u
2 ,
ˆev =
ˆex ∂f
∂v + ˆey ∂g
∂v + ˆez ∂h
∂v
 ∂f
∂v
2
+
 ∂g
∂v
2
+
 ∂h
∂v
2 =
ˆex ∂x
∂v + ˆey ∂y
∂v + ˆez ∂z
∂v
∂x
∂v
2
+
 ∂y
∂v
2
+
 ∂z
∂v
2 ,
ˆew =
ˆex ∂f
∂w + ˆey ∂g
∂w + ˆez ∂h
∂w
 ∂f
∂w
2
+
 ∂g
∂w
2
+
 ∂h
∂w
2 =
ˆex ∂x
∂w + ˆey ∂y
∂w + ˆez ∂z
∂w
 ∂x
∂w
2
+
 ∂y
∂w
2
+
 ∂z
∂w
2 .
■
2.4
Problems
2.1. Find the partial derivatives of the following functions at the given points
with respect to the given variables. In the following r = (x, y, z) and r′ =
(x′, y′, z′):
exyz
with respect to x
at
(1, 0, −1),
cos(xy/z)
with respect to z
at
(π, 1, 1),
x2y + y2z + z2x
with respect to y
at
(1, −1, 2),
ln
ax + by + cz
x2 + y2 + z2

with respect to x
at
(a, b, c),
r ≡
	
x2 + y2 + z2
with respect to x
at
(x, y, z),
|r −r′|
with respect to y
at
(x, y, z, x′, y′, z′),
1
|r −r′|
with respect to z′
at
(x, y, z, x′, y′, z′).
2.2. The Earth has a radius of 6400 km. The thickness of the atmosphere
is about 50 km. Starting with the volume of a sphere and using diﬀerentials,
estimate the volume of the atmosphere. Hint: Find the change in the volume
of a sphere when its radius changes by a “small” amount.
2.3. The gravitational potential (potential energy per unit mass) at a distance
r from the center of the Earth (assumed to be the origin of a Cartesian
coordinate system) is given by
Φ = −GM
r
,
r =
	
x2 + y2 + z2,
where G = 6.67 × 10−11 N·m2/kg2 and M = 6 × 1024 kg. Using diﬀerentials,
ﬁnd the energy needed to raise a 10-kg object from the point with coordinates
(4000 km, 4000 km, 3000 km) to a point with coordinates (4020 km, 4050 km,
3010 km).

2.4 Problems
69
2.4. Show that the function f(x ± ct) satisﬁes the one-dimensional wave
equation:
one-dimensional
wave equation
∂2f
∂x2 −1
c2
∂2f
∂t2 = 0.
Hint: Let y = x ± ct and use the chain rule.
2.5. Assume that f ′′+kf = 0 and g′′−kg = 0. Show that F(x, y) ≡f(x)g(y)
two-dimensional
Laplace’s equation
satisﬁes the two-dimensional Laplace’s equation:
∂2F
∂x2 + ∂2F
∂y2 = 0.
2.6. Suppose that f ′′ −αf = 0, g′′ −βg = 0, and h′′ −γh = 0. Write an
equation relating α, β, and γ such that the function
F(x, y, z) ≡f(x)g(y)h(z)
satisﬁes the three-dimensional Laplace’s equation:
three-dimensional
Laplace’s equation
∂2F
∂x2 + ∂2F
∂y2 + ∂2F
∂z2 = 0.
2.7. Suppose that f ′′ −αf = 0, g′′ −βg = 0, h′′ −γh = 0, and u′ −ωu = 0.
Write an equation relating α, β, γ, and ω such that the function
F(x, y, z, t) ≡f(x)g(y)h(z)u(t)
satisﬁes the heat equation:
heat equation
∂2F
∂x2 + ∂2F
∂y2 + ∂2F
∂z2 = a∂F
∂t .
where a is a constant.
2.8. Suppose that f ′′+k2
xf = 0, g′′+k2
yg = 0, h′′+k2
zh = 0, and u′′+ω2 u = 0.
(a) Write an equation relating kx, ky, kz, and ω such that the function
F(x, y, z, t) ≡f(x)g(y)h(z)u(t)
satisﬁes the three-dimensional wave equation:
three-dimensional
wave equation
∂2F
∂x2 + ∂2F
∂y2 + ∂2F
∂z2 −1
c2
∂2F
∂t2 = 0.
(b) If ω is considered as angular frequency, and c as the speed of the wave,
what is the magnitude of the vector k ≡⟨kx, ky, kz⟩?
2.9. Consider the function F(x, y, z) ≡f
x2y + y2x
a2z

in which a is a con-
stant. Assuming that f ′(2) = a, ﬁnd the unit vector ˆev in the direction of
v = ˆex∂1F(a, a, a) + ˆey∂2F(a, a, a) + ˆez∂3F(a, a, a).

70
Diﬀerentiation
2.10. Consider the function F(x, y, z) ≡f
x3y −y3z + 2z3x
a4

in which a is
a constant. Assuming that f ′(17) = a, ﬁnd the unit vector ˆev in the direction
of
v = ˆex∂1F(a, −a, 2a) + ˆey∂2F(a, −a, 2a) + ˆez∂3F(a, −a, 2a).
2.11. Given that f(x, y, z) = e−k√
x2+y2+z2/
	
x2 + y2 + z2, where k is a
constant, ﬁnd the radial component (component along ˆer) of the vector
V = ˆex∂1f(x, y, z) + ˆey∂2f(x, y, z) + ˆez∂3f(x, y, z).
2.12. Given that
∂1f(x, y, z) = ∂2f(z, x, y) = ∂3f(y, z, x) = 2kx
y
−kz2
x2 ,
where k is a constant, ﬁnd the function f(x, y, z).
Note the order of the
variables in each pair of parentheses.
2.13. Given that f(x, y, z) = x2y sin (yz/x), ﬁnd
∂2f(1, 1, π/2), ∂1f(2, π, 1), ∂3f(4, π, 1), ∂1f(y, z, x), ∂1f(t, u, v).
2.14. Derive the analogue of Equation (2.11) assuming this time that Q is
held constant in all derivatives instead of W.
2.15. Which of the following functions are homogeneous?
exy/z2,
xyz sin xy
az ,
x2y2
z
cos xz
y2 ,
x2 + y2 −z2,
ax + y(z −x),
where a is a constant. For those functions that are homogeneous, ﬁnd their
degree and verify that they satisfy Equation (2.18).
2.16. Suppose f and g are homogeneous functions of degrees q and p, respec-
tively. What can you say about the homogeneity of f ± g, fg, and f/g. If
they are homogeneous, ﬁnd their degree, and verify that they satisfy Equation
(2.18).
2.17. If f is homogeneous of degree q, show that ∂if is homogeneous of degree
q −1. Hint: Use the deﬁnition of homogeneity and diﬀerentiate with respect
to xi.
2.18. A function f(x, y, z) of Cartesian coordinates can also be thought of as
a function of cylindrical coordinates ρ, ϕ, z, because the latter are functions
of the former via the relations ρ =
	
x2 + y2 and tan ϕ = y/x.
(a) Using the chain rule for diﬀerentiation, ﬁnd ∂f/∂x and ∂f/∂y in terms
of ∂f/∂ρ and ∂f/∂ϕ. Express your answers entirely in terms of cylindrical
coordinates.

2.4 Problems
71
(b) Show that the vector ˆex
∂f
∂x +ˆey
∂f
∂y +ˆez
∂f
∂z , when written entirely in terms
of cylindrical coordinates and cylindrical unit vectors, becomes
ˆeρ
∂f
∂ρ + ˆeϕ
1
ρ
∂f
∂ϕ + ˆez
∂f
∂z .
2.19. In each of the following, the partial derivative of a function is given.
Find the most general function with such a derivative.
(a) ∂2f(x, y, z) = xy2z.
(b) ∂1f(x, y, z) = xy2z.
(c) ∂1h(z, x, y) = ex sin z
x
.
(d) ∂1g(z, x, y) = exy2.
(e) ∂2g(z, x, y) = exy2.
(f) ∂2h(x, y, z) = ex sin z
xy
.
(g) ∂3f(x, y, z) = xy2z.
(h) ∂3g(z, x, y) = exy2.
(i) ∂3h(y, x, z) = ex sin z
x
2.20. Finish the calculation of Example 2.2.8.
2.21. Find y∂f/∂z −z∂f/∂y and z∂f/∂x −x∂f/∂z in terms of spherical
coordinates. Warning! These will not be as nice-looking as the expression
calculated in Example 2.2.8.
2.22. Given that f ′(1) = 2, ﬁnd
∂f
∂x ˆex + ∂f
∂y ˆey + ∂f
∂z ˆez
for f(xyz) at the Cartesian point (−1, 2, −1/2).
2.23. Given that f ′(3) = −1, ﬁnd the radial component of the vector
∂f
∂x ˆex + ∂f
∂y ˆey + ∂f
∂z ˆez
for f(
	
x2 + y2 + z2) at the Cartesian point (2, 1, −2).
2.24. Show that the function F(k·r−ωt) satisﬁes the three-dimensional wave
equation:
∂2F
∂x2 + ∂2F
∂y2 + ∂2F
∂z2 −1
c2
∂2F
∂t2 = 0
if k ≡⟨kx, ky, kz⟩is a constant vector, ω is a constant, and a certain relation
exists between k = |k| and ω. Find this relation.
2.25. In electromagnetic radiation theory one encounters an equation of the
form
t =
1
	
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]2

72
Diﬀerentiation
and one is interested in the partial derivative of t with respect to x, y, and z.
Note the hybrid role that t plays here as both a dependent and an independent
variable. Show that
∂t
∂x =
x −f(t)
[x −f(t)]f ′(t) + [y −g(t)]g′(t) + [z −h(t)]h′(t) −F 3/2 ,
where F(x, y, z, t) ≡[x−f(t)]2+[y−g(t)]2+[z−h(t)]2. Find similar expressions
for partial derivatives of t with respect to y and z.
2.26. Consider the function f(|r −r′|) with r = xˆex + yˆey + zˆez and r′ =
x′ˆex + y′ˆey + z′ˆez being the position vectors of P and P ′.
(a) Find a general expression for the vector
V = ∂f
∂x ˆex + ∂f
∂y ˆey + ∂f
∂z ˆez
in terms of r and r′.
(b) If f ′(3) = 3 and the coordinates of P and P ′ are (1, −1, 0), and (0, 1, 2),
respectively, ﬁnd the numerical value of V.
2.27. Find an expression in cylindrical and spherical coordinates analogous
to Equation (2.21).
2.28. A function f(x, y) of Cartesian coordinates can also be thought of as a
function of some other coordinates u and v deﬁned by
x = u sin v,
y = u cosv.
(a) Applying the procedure of Example 2.3.2, ﬁnd the unit vectors ˆeu and ˆev.
(b) Find u and v as functions of x and y.
(c) Calculate ˆex and ˆey in terms of ˆeu and ˆev.
(d) Write the vector
A = ˆex
∂f
∂x + ˆey
∂f
∂y
entirely in the (u, v) coordinate system.
2.29. Find the cylindrical unit vectors in terms of Cartesian unit vectors
using the procedure of Example 2.3.2.
2.30. Find the spherical unit vectors in terms of Cartesian unit vectors using
the procedure of Example 2.3.2.
2.31. In the ﬁrst part of Example 2.3.2, assume that f(u, v) = uf1(v) and
g(u, v) = ug1(v) where f1 and g1 are functions of only one variable.
(a) Find a relation between f1 and g1 to make ˆeu and ˆev perpendicular.
(b) Can you recover the polar coordinates as a special case of (a)?

2.4 Problems
73
2.32. The elliptic coordinates (u, θ) are given by
x = a cosh u cosθ,
y = a sinh u sin θ,
where a is a constant.
(a) What are the curves of constant u?
(b) What are the curves of constant θ?
(c) Find ˆeu and ˆeθ in terms of the Cartesian unit vectors, and examine their
orthogonality.
2.33. The parabolic coordinates (u, v) are given by
x = a(u2 −v2),
y = 2auv,
where a is a constant.
(a) What are the curves of constant u?
(b) What are the curves of constant v?
(c) Find ˆeu and ˆev in terms of the Cartesian unit vectors, and examine their
orthogonality.
2.34. The two-dimensional bipolar coordinates (u, v) are given by
x =
a sinh u
cosh u + cos v ,
y =
a sin v
cosh u + cos v ,
where a is a constant.
(a) What are the curves of constant u?
(b) What are the curves of constant v?
(c) Find ˆeu and ˆev in terms of the Cartesian unit vectors, and examine their
orthogonality.
2.35. The elliptic cylindrical coordinates (u, θ, z)are given by
x = a cosh u cosθ,
y = a sinh u sin θ,
z = z,
where a is a constant.
(a) What are the surfaces of constant u?
(b) What are the surfaces of constant θ?
(c) Find ˆeu, ˆeθ, and ˆez in terms of the Cartesian unit vectors and examine
their orthogonality.

74
Diﬀerentiation
2.36. The prolate spheroidal coordinates (u, θ, ϕ) are given by
x = a sinh u sin θ cos ϕ,
y = a sinh u sin θ sin ϕ,
z = a cosh u cos θ,
where a is a constant.
(a) What are the surfaces of constant u?
(b) What are the surfaces of constant θ?
(c) Find ˆeu, ˆeθ, and ˆeϕ in terms of the Cartesian unit vectors, and examine
their mutual orthogonality.
2.37. The toroidal coordinates (u, θ, ϕ) are given by
x = a sinh u cos ϕ
cosh u −cos θ,
y = a sinh u sin ϕ
cosh θ −cos θ,
z =
a sin u
cosh u −cos θ,
where a is a constant.
(a) What are the surfaces of constant u?
(b) What are the surfaces of constant θ?
(c) Find ˆeu, ˆeθ, and ˆeϕ in terms of the Cartesian unit vectors, and examine
their mutual orthogonality.
2.38. The paraboloidal coordinates (u, v, ϕ) are given by
x = 2auv cos ϕ,
y = 2auv sin ϕ,
z = a(u2 −v2),
where a is a constant.
(a) What are the surfaces of constant u?
(b) What are the surfaces of constant v?
(c) Find ˆeu, ˆev, and ˆeϕ in terms of the Cartesian unit vectors, and examine
their mutual orthogonality.
2.39. The three-dimensional bipolar coordinates (u, θ, ϕ) are given by
x =
a sin θ cos ϕ
cosh u −cos θ,
y =
a sin θ sin ϕ
cosh u −cos θ,
z =
a sinh u
cosh u −cos θ,

2.4 Problems
75
where a is a constant.
(a) What are the surfaces of constant u?
(b) What are the surfaces of constant θ?
(c) Find ˆeu, ˆeθ, and ˆeϕ in terms of the Cartesian unit vectors, and examine
their mutual orthogonality.
2.40. A coordinate system (R, Θ, φ) in space is deﬁned by
x = R cos Θ cos φ + b cosφ,
y = R cos Θ sin φ + b sin φ,
z = R sin Θ,
where b is a constant, and 0 < R < b.
1. Express the unit vectors ˆeR, ˆeΘ, and ˆeφ in terms of Cartesian unit
vectors with coeﬃcients being functions of (R, Θ, φ).
2. Are unit vectors mutually perpendicular?

Chapter 3
Integration: Formalism
It is not an exaggeration to say that the most important concept, whose mas-
tery ensures a much greater understanding of all undergraduate physics, is the
concept of integral. Generally speaking, physical laws are given in local form
Physical laws are
given for
mathematical
points but applied
to extended
objects.
while their application to the real world requires a departure from locality.
For instance, Coulomb’s law in electrostatics and the universal law of gravity
are both given in terms of point particles. These are mathematical points and
the laws assume that. In real physical situations, however, we never deal with
a mathematical point. Usually, we approximate the objects under considera-
tion as points, as in the case of the gravitational force between the Earth and
the Sun. Whether such an approximation is good depends on the properties
of the objects and the parameters of the law. In the example of gravity, on
the sizes of the Earth and the Sun as compared to the distance between them.
On the other hand, the precise motion of a satellite circling the earth requires
more than approximating the Earth as a point; all the bumps and grooves of
the Earth’s surface will aﬀect the satellite’s motion.
This chapter is devoted to a thorough discussion of integrals from a phys-
ical standpoint, i.e., the meaning and the use of the concept of integration
rather than the technique and the art of evaluating integrals.
3.1
“

” Means “

um”
One of the ﬁrst diﬃculties we have to overcome is the preconception instilled
in all of us from calculus that integral is “area under a curve.” This pre-
conception is so strong that in some introductory physics books the authors
translate physical concepts, in which integral plays a natural role, into the
unphysical and unnatural notion of area under a curve. It is true that calcula-
Integral is not just
area under a
curve!
tion of the area under a curve employs the concept of integration, but it does
so only because the calculation happens to be the limit of a sum, and such
limits ﬁnd their natural habitat in many physical situations.

78
Integration: Formalism
Take the gravitational force, for example. As a fundamental physical law,
it is given for point masses, but when we want to calculate the force between
the Earth and the Moon, we cannot apply the law directly because the Earth
and the Moon cannot be considered as points with the Moon being only 60
Earth radii away. This problem was recognized by Newton who found its solu-
tion in integration. Inherent in the concept of integration is the superposition
principle whereby, as mentioned in Chapter 1, diﬀerent parts of a system are
assumed to act independently.
Thus a natural procedure is to divide the
big Earth and the big Moon into small pieces, write down the gravitational
force between these small pieces, invoke the superposition principle, and add
the contribution of these pieces to get the total force. Now, nothing is more
natural than this process, and no example is a more illustrative example of
integration than such a calculation.
In order to deﬁne and elucidate the concept of integration,1 let us recon-
calculation of
ﬁelds of
continuous
distribution of
sources as the
natural setting for
the concept of
integral
sider the gravitational ﬁeld of Box 1.3.5. Instead of a known collection of point
masses, let us calculate the gravitational ﬁeld at a point P of a continuous
distribution of mass such as that distributed in the volume of the Earth.
The point P is called the ﬁeld point.2
We divide the large mass into N
pieces, denoting the mass of the ith piece, located around the point Pi, by
Δmi as shown in Figure 3.1. To be able to even write the ﬁeld equation for
the ith piece of mass, we have to make sure that the size of Δmi is small
enough. We thus write
gi ≈−GΔmi
|r −ri|3 (r −ri).
z
x
y
r
x
y
y
z
P
ri
Δm i
ri
r −
Pi
i
zi
x i
(a)
z
x
y
r
x
y
y
z
z
P
dm (r )
'
'
r
r −
'
r
'
x
'
P
'
'
(b)
Figure 3.1: The mass distribution giving rise to a gravitational ﬁeld. (a) The mass is
divided into discrete pieces labeled 1 through N with the ith piece singled out. (b) The
mass is divided into inﬁnitesimal pieces with the piece located at r′ singled out.
1The discussion that follows may seem speciﬁc to one example, but in reality, it is much
more general. Instead of the gravitational law one can substitute any other local law, and
instead of mass, the appropriate physical quantity must be used. The examples that follow
throughout this chapter will clarify any vague points.
2The same terminology applies to electrostatic ﬁelds as well.

3.1 “

” Means “

um”
79
The smaller the size, the better this expression approximates the ﬁeld due to
Δmi. Invoking the superposition principle, we write
g(r) ≈
N

i=1
gi = −
N

i=1
GΔmi
|r −ri|3 (r −ri) = −
N

i=1
GΔm(ri)
|r −ri|3 (r −ri),
(3.1)
where in the last equality we have replaced Δmi with Δm(ri). Aside from a
change in notation, this replacement emphasizes the dependence of the small
piece of mass on its “location.” The quotation marks around the last word
need some elaboration. In any practical slicing of the gravitating object, such
as the Earth, each piece still has some nonzero size. This makes it impossible
to deﬁne the distance between Δmi and the point P. We can deﬁne this
distance to be that of the “center” of Δmi from P, but then the diﬃculty
shifts to deﬁning the center of the piece. Fortunately, it turns out that, as long
as we ultimately make the size of all Δmi’s indeﬁnitely small, any point—such
as Pi shown in the ﬁgure—in Δmi can be chosen to deﬁne its distance from
P. We are thus led to taking the limit of Equation (3.1) as the size of all
pieces tends to zero, and, necessarily, the number of pieces tends to inﬁnity.
If such a
limit exists, we call it the integral of the gravitational ﬁeld and
integral as the
limit of a sum
denote it as follows:3
g(r) = −lim
Δm→0
N→∞
N

i=1
GΔm(ri)
|r −ri|3 (r −ri) ≡−
##
Ω
G dm(r′)
|r −r′|3 (r −r′).
(3.2)
An identical procedure leads to a similar formula for the electrostatic ﬁeld
and potential:
E =
##
Ω
kedq(r′)
|r −r′|3 (r −r′),
Φ =
##
Ω
kedq(r′)
|r −r′| .
(3.3)
Equations (3.2) and (3.3) will be used frequently in the sequel as we try
to illustrate their use in various physical situations.
Note that Equations
(3.2) and (3.3) are independent of any coordinate systems as all physical laws
should be.
In the symbolic representation of integral on the RHS, Ω, called the region
region of
integration
of integration,4 is the region—for example, the volume of the Earth—in
which the mass distribution resides, and dm(r′) is called an element of mass
located5 at point P ′ whose position vector is r′.
P ′ is called the source
point because it is the location of the source of the gravitational ﬁeld, i.e.,
the mass element at that point. We also call it the integration point. The
integration point,
integration
variables, and
integrand deﬁned
3We shall use the symbol

Ω (or simply

Ω) to indicate general integration without
regard to the dimensionality (single, double, or triple) of the integral.
4When the region of integration is one dimensional, such as an interval (a, b) on the real
line, one uses  b
a instead of 
(a,b).
5Whenever r′ is used as an argument of a quantity, it will refer to the coordinates of a
point not the components of its position vector.

80
Integration: Formalism
coordinates of r′ upon which the mass element depends—and in terms of
which it will eventually be expressed—are called the integration variables,
and whatever multiplies the products of the diﬀerentials of these variables is
called the integrand.
It is not hard to abstract the concept of integration from the speciﬁc
example of gravity. Instead of the speciﬁc form of the integral in Equations
(3.2) and (3.3), we use f(r, r′), and instead of the element of mass, we use the
element of some other quantity which we generically designate as dQ(r′). We
thus write
h(r) =
lim
ΔQ→0
N→∞
N

i=1
f(r, ri)ΔQ(ri) ≡
##
Ω
f(r, r′) dQ(r′),
(3.4)
where h(r), the result of integration, will be a function of r, the position
vector of P whose coordinates are called the parameters of integration.
integration
parameters
Although we have used r and r′, the concept of integration does not require
the parameters and integration variables to be position vectors. They could be
any collection of parameters and variables. Nevertheless, we continue to use
the terminology of position vectors and call such collections the coordinates
of points.
An immediate—and important—consequence of the deﬁnition of integral
is that if the region of integration Ω is small, then, for practical calculations,
we do not need to subdivide it into many pieces. In fact, if Ω is small enough,
only one piece may be a good approximation to the integral. We thus write
##
ΔΩ
f(r, r′) dQ(r′) ≈f(r, rM) ΔQ,
(3.5)
where it is understood that ΔΩ is a small region around point M whose
“position vector” is rM.
Another immediate and important consequence of the deﬁnition of integral
is that if Ω is divided into two regions Ω1 and Ω2, then
##
Ω
f(r, r′) dQ(r′) =
##
Ω1
f(r, r′) dQ(r′) +
##
Ω2
f(r, r′) dQ(r′)
(3.6)
In order to be able to evaluate integrals, one has to express both dQ(r′)
and f(r, r′) in terms of a suitable set of coordinates. f(r, r′) poses no problem,
and in most cases it involves a mere substitution. The element of Q, on the
other hand, is often related, via density, to the element of volume (or area,
or length) whose expression is more involved.
Section 2.3 dealt with the
construction of elements of length, area, and volume in the three coordinate
systems.
Historical Notes
Integral calculus, in its geometric form, was known to the ancient Greeks.
For
example, Euclid, by adding pieces to the area of a square inscribed in a circle,

3.2 Properties of Integral
81
constructing newer polygons of larger numbers of sides, and continuing the process
until the circle is “exhausted” by regular polygons, proved the theorem: Circles
are to one another as the squares on the diameters. In essence, Euclid thinks of a
circle as the limiting case of a regular polygon and proves the above theorem for
polygons. Then he uses the argument of “exhaustion” to get to the result. Although
mathematicians of antiquity made frequent use of the method of exhaustion, no one
did it with the mastery of Archimedes.
Archimedes is arguably believed to be the greatest mathematician of antiquity.
The son of an astronomer, he was born in Syracuse, a Greek settlement in Sicily.
As a young man he went to Alexandria to study mathematics, and although he
went back to Syracuse to spend the rest of his life there, he never lost contact with
Alexandria.
Archimedes possessed a lofty intellect, great breadth of interest—both theoret-
Archimedes
287–212 B.C.
ical and practical—and excellent mechanical skills. He is credited with ﬁnding the
areas and volumes of many geometric ﬁgures using the method of exhaustion, the
calculation of π, a new scheme of presenting large numbers in verbal language, ﬁnd-
ing the centers of gravity of many solids and plane ﬁgures, and founding the science
of hydrostatics.
His great achievements in mathematics—he is ranked with Newton and Gauss
as one of the three greatest mathematicians of all time—did not overshadow his
practical inventions. He invented the ﬁrst planetarium and a pump (Archimedean
screw). He showed how to use levers to move great weights, and used compound
pulleys to launch a galley of the king of Syracuse. Taking advantage of the focusing
power of a parabolic mirror, so the story goes, he concentrated the Sun’s rays on
the Roman ships besieging Syracuse and burned them!
Perhaps the most famous story about Archimedes is his discovery of the method
of testing the debasement of a crown of gold. The king of Syracuse had ordered
the crown.
Upon delivery, he suspected that it was ﬁlled with baser metal and
sent it to Archimedes to test it for purity. Archimedes pondered about the problem
for some time, until one day, as he was taking a bath, he observed that his body
was partly buoyed up by the water and suddenly grasped the principle—now called
Archimedes’ principle—by which he could solve the problem.
He was so ex-
cited about the discovery that he forgetfully ran out into the street naked shouting
“Eureka!” (“I have found it!”).
3.2
Properties of Integral
Now that we have developed the formalism of integration, we should look
at some applications in which integrals are evaluated. As we shall see, all
integral evaluations eventually reduce to integrals involving only one variable.
Thus, it is important to have a thorough understanding of the properties
of such integrals. Some of these properties are familiar, others may be less
familiar or completely new. We gather all these properties here for the sake
of completeness.

82
Integration: Formalism
3.2.1
Change of Dummy Variable
The symbol used as the variable of integration in the integral is completely
irrelevant. Thus, we have
Feel free to use
any symbol you
like for the
integration
variable!
# t2
t1
g(t) dt =
# t2
t1
g(x) dx =
# t2
t1
g(s) ds
=
# t2
t1
g(t′) dt′ =
# t2
t1
g(⋆) d⋆.
Note how the limits of integration remain the same in all integrals. The fact
that these limits use the same symbol as the ﬁrst dummy variable should not
confuse the reader. What is important is that they are ﬁxed real numbers.
3.2.2
Linearity
For arbitrary constant real numbers a and b, we have
# c2
c1
[af(t) + bg(t)] dt = a
# c2
c1
f(t) dt + b
# c2
c1
g(t) dt.
3.2.3
Interchange of Limits
Interchanging the limits of integration introduces a minus sign:
# d
c
f(t) dt = −
# c
d
f(t) dt.
(3.7)
This relation implies that
 s
s f(t) dt = 0. (Show this implication!)
3.2.4
Partition of Range of Integration
If q is a real number between the two limits, i.e., if p < q < r, then
# r
p
f(t) dt =
# q
p
f(t) dt +
# r
q
f(t) dt.
(3.8)
which is a special case of Equation (3.6).
This property is used to evaluate
piecewise
continuous
functions
piecewise continuous functions, i.e., functions that have a ﬁnite number
of discontinuities in the interval of integration. For instance, suppose f(t) is
deﬁned to be
f(t) =
⎧
⎪
⎨
⎪
⎩
f1(t)
if
p < t < q1,
f2(t)
if
q1 < t < q2,
f3(t)
if
q2 < t < r,
where f1(t), f2(t), and f3(t) are, in general, totally unrelated (continuous)
functions.
Then one divides the interval of integration into three natural
parts and writes
# r
p
f(t) dt =
# q1
p
f1(t) dt +
# q2
q1
f2(t) dt +
# r
q2
f3(t) dt.

3.2 Properties of Integral
83
p
r
t
f1
f2
q2
f3
q1
Figure 3.2: The integral is deﬁned as long as there is only a ﬁnite number of disconti-
nuities (jumps) in the function.
This is illustrated in Figure 3.2.
3.2.5
Transformation of Integration Variable
When evaluating an integral it is sometimes convenient to use a new variable
of integration of which the old one is a function. Call the new integration
variable y and assume that t = h(y). Then we have
# b
a
f(t) dt =
# q
p
f(h(y)) h′(y) dy,
(3.9)
where p and q are the solutions to the two equations
Transformation of
integration
variable
accompanies a
change in the
limits of
integration.
a = h(p), b = h(q).
Each of these two equations must have a unique solution, otherwise, the trans-
formation of the integration variable will not be a valid procedure. This con-
dition puts restrictions on the type of function h can be. Note that we have
essentially substituted h(y) for t in the original integral including the dif-
ferential h′(y) dy for dt. It is vital to remember to change the limits of
integration when transforming variables.
3.2.6
Small Region of Integration
When the region of integration is small, in the sense that the integrand does
When is the
region of
integration small?
not change much over the range of integration, then the integral can be ap-
proximated by the product of integrand and the size of the range.6 We thus
can write
# b
a
f(t) dt ≈(b −a)f(t0),
(3.10)
where t0 is a number between a and b, mostly taken to be the midpoint of
the interval (a, b).
6This is simply a restatement of Equation (3.5) for the case of one variable.

84
Integration: Formalism
3.2.7
Integral and Absolute Value
A useful property of integrals that we shall be using sometimes is





# b
a
f(t) dt





 ≤
# b
a
|f(t)| dt.
(3.11)
This should be clear once we realize that an integral is the limit of a sum and
the absolute value of a sum is always less than or equal to the sum of the
absolute values.
3.2.8
Symmetric Range of Integration
By a symmetric range of integration, we mean a range that has 0—the origin—
as its midpoint. For certain functions, partitioning such a range into two equal
pieces can simplify the evaluation of the integral considerably. So, let us write
# +T
−T
f(t) dt =
# 0
−T
f(t) dt +
# +T
0
f(t) dt.
For the ﬁrst integral, make a change of variable t = −y to obtain
h(y) = −y ⇒h′(y) dy = (−1) dy = −dy.
The limits of integration in y are determined by
h(−T ) = ylower, h(0) = yupper ⇒ylower = +T, yupper = 0.
We therefore have
# 0
−T
f(t) dt =
# 0
+T
f(−y)(−dy) =
# +T
0
f(−y) dy =
# +T
0
f(−t) dt,
where we have used the properties in Subsections 3.2.3 and 3.2.1. Combining
our results and using the second property, we get
# +T
−T
f(t) dt =
# +T
0
f(−t) dt +
# +T
0
f(t) dt
=
# +T
0
[f(t) + f(−t)] dt.
(3.12)
A real-valued function f is called even if f(−x) = f(x), and odd if
even and odd
functions deﬁned
f(−x) = −f(x). Thus, from Equation (3.12), we obtain
# +T
−T
f(t) dt =
# +T
0
[f(t) + f(−t)] dt = 2
# +T
0
f(t) dt
(3.13)
when f is even, and
# +T
−T
f(t) dt =
# +T
0
[f(t) + f(−t)] dt = 0
(3.14)
when it is odd.

3.2 Properties of Integral
85
3.2.9
Diﬀerentiating an Integral
We have seen that an integral can have an integrand which depends on a set of
parameters, and that the result of integration will depend on these parameters.
Thus, we can think of the integral as a function of those parameters, and in
particular, we may want to know its derivative with respect to one of the
parameters. Using the deﬁnition of integral as the limit of a sum, and the
fact that the derivative of a sum is the sum of derivatives, it is easy to show
that
∂
∂xi
# b
a
f(x1, x2, . . . , xn, t) dt =
# b
a
∂
∂xi
f(x1, x2, . . . , xn, t) dt,
(3.15)
where we have represented the list of parameters as (x1, x2, . . . , xn). We can
write exactly the same relation for the integral of Equation (3.4). Assuming
that r = (x1, x2, . . . , xn), we have
∂
∂xi
h( r) =
∂
∂xi
##
Ω
f(r, r′) dQ( r′) =
##
Ω
∂
∂xi
f(r, r′) dQ( r′).
(3.16)
In both cases the region of integration is assumed to be independent of xi.
Restricting ourselves to single integrals,7 we now consider the case where
the limits of integration depend on some parameters. First, consider an inte-
gral of the form
# v
u
f(t) dt
and treat the result as a function of the limits. So, let us write
F(u, v) ≡
# v
u
f(t) dt ⇒F(v, u) = −F(u, v)
and evaluate the partial derivative of F with respect to its arguments:
∂F
∂u ≡∂1F(u, v) = lim
ϵ→0
F(u + ϵ, v) −F(u, v)
ϵ
= lim
ϵ→0
 v
u+ϵ f(t) dt −
 v
u f(t) dt
ϵ
= −lim
ϵ→0
 v
u f(t) dt +
 u+ϵ
v
f(t) dt
ϵ
= −lim
ϵ→0
 u+ϵ
u
f(t) dt
ϵ
= −lim
ϵ→0
ϵf(u0)
ϵ
= −lim
ϵ→0 f(u0) = −f(u).
The last equality follows from the fact that as ϵ →0, u0, lying between u and
u + ϵ, will be squeezed to u. Note that the derivative above is independent of
the second variable. To ﬁnd the other derivative, we use the result obtained
above and simply note that
∂F(u, v)
∂v
= −∂F(v, u)
∂v
= −∂1F(v, u) = −
 
−f(v)
!
= f(v).
7Since all multiple integrals are reducible to single integrals, this restriction is not severe.

86
Integration: Formalism
Putting these two results together, we can write
∂
∂v
# v
u
f(t) dt = f(v),
∂
∂u
# v
u
f(t) dt = −f(u).
(3.17)
In words,
Box 3.2.1. The derivative of an integral with respect to its upper (lower)
limit equals the integrand (minus the integrand) evaluated at the upper
(lower) limit.
By evaluation, we mean replacing the variable of integration. If the integrand
has parameters, they are to be left alone.
By combining Equations (3.15) and (3.17) we can derive the most general
equation. So, assume that both u and v are functions of (x1, x2, . . . , xn), and
write
G(x1, x2, . . . , xn, u, v) ≡
# v(x1,x2,...,xn)
u(x1,x2,...,xn)
f(x1, x2, . . . , xn, t) dt.
Then, using the chain rule. we get
DiG ≡∂G
∂u
∂u
∂xi
+ ∂G
∂v
∂v
∂xi
+ ∂iG,
where DiG stands for the “total” derivative with respect to xi. This means
total derivative
that the dependence of u and v on xi is taken into account. In contrast, ∂iG
is evaluated assuming that u and v are constants. We note that
∂G
∂u = ∂
∂u
# v
u
f(x1, x2, . . . , xn, t) dt = −f(x1, x2, . . . , xn, u),
∂G
∂v = ∂
∂v
# v
u
f(x1, x2, . . . , xn, t) dt = f(x1, x2, . . . , xn, v),
∂G
∂xi
=
∂
∂xi
# v
u
f(x1, x2, . . . , xn, t) dt =
# v
u
∂
∂xi
f(x1, x2, . . . , xn, t) dt,
where the partial derivative in the last equation treats u and v as constants.
It follows that
Box 3.2.2. The most general formula for the derivative of an integral is
∂
∂xi
# v(r)
u(r)
f(r, t) dt = ∂v
∂xi
f(r, v) −∂u
∂xi
f(r, u) +
# v(r)
u(r)
∂
∂xi
f(r, t) dt,
where r = (x1, x2, . . . , xn).
As indicated in Equation (2.16), it is common to ignore the diﬀerence between
Di and ∂i; and the formula in Box 3.2.2 reﬂects this.

3.2 Properties of Integral
87
3.2.10
Fundamental Theorem of Calculus
A special case of Box 3.2.2 is extremely useful.
Consider a function g of
a single variable x.
We want to ﬁnd a function called the primitive, or
primitive
(antiderivative) of
a function
antiderivative, or indeﬁnite integral8 whose derivative is g.
This can be
easily done using integrals. In fact using Box 3.2.2, we have
G(x) ≡
# x
a
g(s) ds ⇒dG
dx = d
dx
# x
a
g(s) ds = g(x),
(3.18)
where a is an arbitrary constant. We can add an arbitrary constant to the
RHS of the above equation and still get a primitive. Adding such a constant,
evaluating both sides at x = a, and noting that the integral vanishes, we ﬁnd
that the constant must be G(a). We, therefore, obtain
G(x) −G(a) =
# x
a
g(s) ds.
(3.19)
Now suppose that F(x) is any function whose derivative is g(x). Then,
from Equation (3.18), we see that
d
dx[G(x) −F(x)] = dG
dx −dF
dx = g(x) −g(x) = 0.
Therefore, G(x)−F(x) must be a constant C. It now follows from (3.19) that
F(x) −F(a) = G(x) −C −[G(a) −C] = G(x) −G(a) =
# x
a
g(s) ds,
and we have
fundamental
theorem of
calculus
Box 3.2.3. (Fundamental Theorem of Calculus). Let F(x) be any
primitive of g(x) deﬁned in the interval (a, b), i.e., any function whose
derivative is g(x) in that interval. Then,
F(b) −F(a) =
# b
a
g(s) ds.
(3.20)
The founders of calculus such as Barrow, Newton, and Leibniz thought of
an integral as a sum. At the beginning no connection between integration and
Connection
between integrals
and antiderivatives
was not apparent
at the time
integration was
introduced. It was
discovered later.
diﬀerentiation was established, and to obtain the result of an integral one
had to go through the painstaking process of adding the terms of a (inﬁnite)
sum. It was later, that the founders of calculus realized (but did not prove)
that the process of summation and taking limits was intimately connected
8We would like to emphasize the concept of integral as the limit of a sum. Therefore,
we think it is better to reserve the word “integral” for such sums and will avoid using the
phrase “indeﬁnite integral.”

88
Integration: Formalism
to the process of (anti) diﬀerentiation.
In this respect, Equation (3.20) is
indeed a fundamental result, because it eliminates the cumbersome labor of
summation.
Another useful result is
G(x) −G(a) =
# x
a
g(s) ds =
# x
a
dG
ds (s) ds =
# x
a
dG.
(3.21)
In words, the integral of the diﬀerential of a physical quantity is equal to the
quantity evaluated at the upper limit minus the quantity evaluated at the lower
limit.
Example 3.2.1. The properties mentioned above can be very useful in evaluating
some integrals. Consider the integral
 ∞
−∞e−t2dt whose value is known to be √π (see
Example 3.3.1). We want to use this information to obtain the integral
 ∞
−∞t2e−t2dt.
First, we note that
# ∞
−∞
e−xt2dt =

π
x .
This can be shown readily by changing the variable of integration to u = √x t and
using the result of Example 3.3.1. Next, we diﬀerentiate both sides with respect to
x and use Box 3.2.2 with u = −∞and v = ∞. We then get
LHS = ∂
∂x
# ∞
−∞
e−xt2dt =
# ∞
−∞
∂
∂xe−xt2dt =
# ∞
−∞
(−t2)e−xt2dt
for the LHS, and ∂
∂x
 π
x = −1
2
√π
x3/2 for the RHS. So
using derivative of
integral to obtain
new integral
formulas from
known integral
formulas
# ∞
−∞
t2e−xt2dt = √π 1
2x−3/2
(3.22)
or, setting x = 1,
 ∞
−∞t2e−t2dt =
√π
2 .
We can obtain more general results.
Diﬀerentiating both sides of Equation
(3.22), we obtain
# ∞
−∞
t4e−xt2dt = √π 1
2
3
2x−5/2 = √π 1 · 3
22 x−5/2.
Continuing the process n times, we obtain
# ∞
−∞
t2ne−xt2dt = √π 1 · 3 · 5 · · · (2n −1)
2n
x−(2n+1)/2.
(3.23)
In particular, if x = 1, we have
# ∞
−∞
t2ne−t2dt = √π 1 · 3 · 5 · · · (2n −1)
2n
.
■
Example 3.2.2. Integrals involving only trigonometric functions are easy to
evaluate:
# b
a
sin t dt = −cos t



b
a = cos a −cos b,
# b
a
cos t dt = sin t



b
a = sin b −sin a.

3.2 Properties of Integral
89
However, integrals of the form I ≡
 b
a tn sin t dt, in which n is a positive integer,
are not as easy to evaluate although they occur frequently in applications. One can
of course evaluate these integrals using integration by parts. But that is a tedious
process. A more direct method of evaluation is to use the ideas developed above.
A pair of slightly more complicated trigonometric integrals which will be useful
for our purposes is
# b
a
sin st dt = −1
s cos st




b
a
= cos sa −cos sb
s
,
# b
a
cos st dt = 1
s sin st




b
a
= sin sb −sin sa
s
.
(3.24)
If we diﬀerentiate both sides with respect to s, we can obtain the integrals we are
after.9 This is because each diﬀerentiation introduces one power of t in the integrand.
For example if we are interested in I with n = 1, then we can diﬀerentiate the second
equation in (3.24):
LHS = d
ds
# b
a
cos st dt =
# b
a
∂
∂s(cos st) dt = −
# b
a
t sin st dt.
On the other hand,
RHS = ∂
∂s
 sin sb −sin sa
s

= −sin sb −sin sa
s2
+ b cos sb −a cos sa
s
.
Setting s = 1 in these equations yields
# b
a
t sin t dt = sin b −sin a −b cos b + a cos a.
(3.25)
We can also ﬁnd the primitive of functions of the form xn sin x. All we need to
do is change b to x as suggested by Equation (3.18). For example, the primitive
(indeﬁnite integral) of x sin x is obtained by substituting x for b in Equation (3.25):
#
x sin x dx = sin x −sin a −x cos x + a cos a = sin x −x cos x + C
because −sin a + a cos a is simply a constant.
■
Historical Notes
After a lull of almost two millennia, the subject of “exhaustion,” like any other form
of human intellectual activity, was picked up after the Renaissance. Johannes Kepler
is reportedly the ﬁrst one to begin work on ﬁnding areas, volumes, and centers of
gravity. He is said to have been attracted to such problems because he noted the
inaccuracy of methods used by wine dealers to ﬁnd the volumes of their kegs.
Some of the results he obtained were the relations between areas and perimeters.
For example, by considering the area of a circle to be covered by an inﬁnite number
of triangles, each with a vertex at the center, he shows that the area of a circle is 1
2
9We can set s = 1 at the end if need be.

90
Integration: Formalism
its radius times its circumference. Similarly, he regarded the volume of a sphere as
the sum of a large number of small cones with vertices at the center. Since he knew
the volume of each cone to be 1
3 its height times the area of its base, he concluded
that the volume of a sphere should be 1
3 its radius times the surface area.
Galileo used the same technique as Kepler to treat the uniformly accelerated
motion and essentially arrived at the formula x =
1
2at2. They both regarded an
area as the sum of inﬁnitely many lines, and a volume as the sum of inﬁnitely many
planes, without questioning the validity of manipulating inﬁnities. Galileo regarded
a line as an indivisible element of area, and a plane as an indivisible element of
volume.
Inﬂuenced by the idea of “indivisibles,” Bonaventura Cavalieri, a pupil of
Galileo and professor in a lyceum in Bologna, took up the study of calculus upon
Galileo’s recommendation. He developed the ideas of Galileo and others on indivis-
ibles into a geometrical method and in 1635 published a book on the subject called
Geometry Advanced by a thus far Unknown Method, Indivisible of Continua.
Cavalieri joined the religious order Jesuati in Milan in 1615 while he was still
a boy. In 1616 he transferred to the Jesuati monastery in Pisa.
His interest in
mathematics was stimulated by Euclid’s works and after meeting Galileo, considered
himself a disciple of the astronomer. The meeting with Galileo was set up by Car-
dinal Federico Borromeo who saw clearly the genius in Cavalieri while he was at the
monastery in Milan.
Bonaventura
Cavalieri
1598–1647
Cavalieri was largely responsible for introducing logarithms as a computational
tool in Italy. The tables of logarithms which he published included logarithms of
trigonometric functions for use by astronomers. Cavalieri also wrote on conic sec-
tions, trigonometry, optics, and astronomy. He showed by his methods of indivisibles
that, in the modern notation,
# a
0
xn dx = an+1
n + 1
for positive integral values of n up to 9.
The next important step in the development of integral calculus began when
the seventeenth-century mathematicians generalized the Greek method of exhaus-
tion. Whereas this method requires diﬀerent rectilinear approximation for diﬀerent
geometrical ﬁgures, the new generation of mathematicians approximated the area
under any curve by a large number of rectangles of equal width (much like it is
done today), summed up the areas, and neglected the “small corrections” in the
sum. Using essentially this kind of summation technique, Fermat showed the above
integral formula for all rational n except −1 before 1636.
Before Newton and Leibniz, the man who did most to replace the geometrical
techniques with analytical ones in calculus was John Wallis.
Although he did
not begin to learn mathematics until he was about twenty, he became professor
of geometry at Oxford and the ablest British mathematician of the century, next
to Newton. One of Wallis’s notable results, obtained while he was trying to ﬁnd
the area of a circle analytically, was a new formula for π. He calculated the area
bounded by the axes and the curves y = (1 −x2)n for n = 0, 1, 2, . . .. Then by
interpolation and further complicated reasoning he related the area of a unit circle
John Wallis
1616–1703
y = (1 −x2)1/2 to the previous areas and showed that
π
2 = 2.2.4.4.6.6.8.8 . . .
1.3.3.5.5.7.7.9 . . .

3.3 Guidelines for Calculating Integrals
91
3.3
Guidelines for Calculating Integrals
The number of situations in which integrals are used is unlimited, and we shall
see many examples of such usage in this chapter and throughout the book. Be-
fore embarking on speciﬁc examples, let us summarize some guidelines which
will be helpful in applying integrals in physical problems:
• Make sure you understand what physical quantity you are trying to
Let the problem
determine
formulas!
calculate. Instead of searching randomly for formulas, think about the
problem and let it determine the formulas.
• Determine which coordinate system is most suited for the problem.
Then place the origin and orient the axes in such a way that the prob-
lem takes the simplest form. Usually spherical coordinates are suited
Choose
coordinates,
origin, and
orientation of axes
wisely!
for regions of integration which are symmetric about a single point. If
there is a natural “axis” associated with the problem, then cylindrical
coordinates are useful, and if the region of integration is in the shape of
a rectangular box, Cartesian coordinates may be most suitable. If there
is no obvious symmetry, then any one of the systems is just as good (or
just as bad).
• Write down the local formula ﬁrst, i.e., conﬁne the problem to a small
Write the local
formula, then put
it inside the
integral.
region and write the formula, for instance, in terms of dQ(r′), dm(r′),
etc., then put the formula inside the integral. Do this in a coordinate-
independent way ﬁrst. All physical laws are written with no reference
to a particular coordinate system, anyway.
• Now express the formula in terms of the coordinates you have chosen.
When dealing with vector quantities, pay particular attention to unit
vectors whose directions depend on the integration point. They cannot
in general be taken out of the integral sign (see Section 3.3.2 for details).
• Determine the limits of integration. In a typical situation, if you have
chosen a good coordinate system, placed the origin properly, and ori-
ented the axes nicely, then the limits of integration should be easy to
write.
• Never take anything out of the integral unless you are absolutely sure
Never take
anything out of
the integral
unless. . . .
that it is independent of the integration variables. This is easily said,
but most often also easily forgotten.
• Once you have evaluated the integrals and found the physical quantity
you are after, try to express your result in a coordinate-free language.
This is not, in general, easy, but in special circumstances you can im-
mediately guess the coordinate-free form of the result.
• As a general rule—valid in all physical calculations—check your ﬁnal
Always check the
dimension of your
ﬁnal result!
answer for correct dimensions. The dimension of the LHS must match
that of the RHS.

92
Integration: Formalism
3.3.1
Reduction to Single Integrals
Most integrals encountered in physics are multidimensional. Thus, it is impor-
tant to know how to evaluate multiple integrals. Let us concentrate on triple
integration, and for deﬁniteness, let us assume that the integration variables
are actual coordinates in a Cartesian coordinate system.10 The most general
integral, namely Equation (3.4), will then be rewritten as
##
Ω
f(r, r′) dQ(r′) ≡
##
Ω
f(r, x′, y′, z′) dQ(x′, y′, z′)
=
# #
V
#
f(r, x′, y′, z′)ρQ(x′, y′, z′) dx′ dy′ dz′,
where we have reexpressed dQ in terms of some density. The region of integra-
tion V may have to be divided into a number of other more easily integrable
regions. However, in most applications, by a good choice of the order of inte-
gration, one can avoid such division. Let us assume that by integrating the
z′ variable ﬁrst, we will not need to divide the region. The z′ integration is a
single integral and is done by keeping x′ and y′ constant. To ﬁnd the upper
limit of this integral, we pick an arbitrary point11 in the region, ﬁx its ﬁrst
two coordinates, move “up” until we hit the boundary of V at a point. The
third coordinate of this boundary point, when expressed in terms of x′ and
y′, will be the upper limit of the z′ integration. The lower limit is obtained
similarly. In most cases, V is bounded by a given upper surface of the form
z = g(x, y), and a lower surface of the form z = h(x, y) as shown in Figure 3.3.
Figure 3.3: The limits of the ﬁrst integration of a triple integral are deﬁned by two
surfaces.
10Recall that the integration variables, although considered as “coordinates of a point,”
need not be an actual geometric point in space.
They could, for instance, be a set of
thermodynamical variables describing a thermodynamical system.
11A common mistake at this stage is to pick a special point. To make sure that you have
picked an arbitrary point, go through the following process using the point chosen, then
pick a diﬀerent point, go through the process and see if you obtain the same result for the
upper and lower limits of the integral.

3.3 Guidelines for Calculating Integrals
93
Thus, since the ﬁrst two coordinates of the boundary points are x′ and y′,
the upper limit will be g(x′, y′) and the lower limit will be h(x′, y′). We thus
write the integral as
##
Ω
f(r, r′) dQ(r′) =
# #
S
dx′ dy′
# g(x′,y′)
h(x′,y′)
f(r, x′, y′, z′)ρQ(x′, y′, z′) dz′,
where S is the projection of V on the xy-plane. For S to be useful, it must
have the following property: Every point of the upper and lower boundaries of
V has one and only one image in S, and no two points of the upper (or lower)
boundary project onto the same point in S. If this property is not fulﬁlled,
then we must choose another coordinate as our ﬁrst integration variable, or,
if this does not work, divide the region of integration into pieces for each one
of which this property holds.
Let us assume that the property holds for S, and that we can do the
integral in z′. The result of this integration is a complete elimination of the
z′-coordinate and the reduction of the triple integral down to a double integral.
To be more speciﬁc, assume that the primitive of the integrand, as a function
of z′, is F(r, x′, y′, z′), i.e., that
∂F
∂z′ = f(r, x′, y′, z′)ρQ(x′, y′, z′).
Then, the z′ integration yields
# g(x′,y′)
h(x′,y′)
f(r, x′, y′, z′)ρQ(x′, y′, z′) dz′
= F(r, x′, y′, g(x′, y′)) −F(r, x′, y′, h(x′, y′)) ≡G(r, x′, y′),
where the last line deﬁnes G. The triple integration has now been reduced to
a double integral, and we have
##
Ω
f(r, r′) dQ(r′) =
# #
S
dx′ dy′ G(r, x′, y′).
We follow the same procedure as above to do the double integral. Once
again, the region of integration S may have to be divided into a number of
other more easily integrable regions. However, let us assume that by inte-
grating the x′ variable ﬁrst, we will not need to divide the region. The x′
integration is again a single integral and is done by keeping y′ constant. To
ﬁnd the upper limit of this integral, we pick an arbitrary point in S, ﬁx its
second coordinate, and move “to the right” until we hit the boundary of S at
a point. The ﬁrst coordinate of this boundary point, when expressed in terms
of y′, will be the upper limit of the x′ integration. The lower limit is obtained
by “moving to the left.” Once again, in most cases, S is bounded by a given
upper curve of the form x = v(y), and a lower curve of the form x = u(y) (see
Figure 3.4). Thus, since the second coordinate of both boundary points is y′,

94
Integration: Formalism
x
y
v( y )
u( y )
a
b
I
'
'
(x , y  )
'
'
Figure 3.4: The limits of the second integration of a triple integral are deﬁned by two
curves.
the upper limit will be v(y′) and the lower limit will be u(y′). We thus write
the integral as
##
Ω
f(r, r′) dQ(r′) =
#
I
dy′
# v(y′)
u(y′)
dx′ G(r, x′, y′),
where I is the projection of S on the y-axis. For I to be useful, it must have
the same property as S, namely: Every point of the right and left boundaries
of S has one and only one image in I, and no two points of the right (or left)
boundary project onto the same point in I. If this property is not fulﬁlled,
then we must choose y′ as our ﬁrst integration variable, or, if this does not
work, divide the region of integration into pieces for each one of which this
property holds. Assuming that I satisﬁes this property, and that the primitive
of the integrand, as a function of x′, is W(r, x′, y′), i.e, that
∂W
∂x′ = G(r, x′, y′),
we get
# v(y′)
u(y′)
G(r, x′, y′) dx′ = W(r, v(y′), y′) −W(r, u(y′), y′) ≡H(r, y′),
where the last line deﬁnes H. The triple integration has now been reduced to
a single integral, and we have
##
Ω
f(r, r′) dQ(r′) =
#
I
H(r, y′) dy′ =
# b
a
H(r, y′) dy′,
where a and b are the end points of the interval I.
Sometimes the inverse of the foregoing operation is useful whereby a single
integral is turned into a multiple integral. This happens when the integrand
is given in terms of an integral. To be speciﬁc, suppose in the integral

3.3 Guidelines for Calculating Integrals
95
I ≡
# b
a
g(x) dx,
g(x) is given by
g(x) =
# v
u
h(x, t) dt,
where u and v could be functions of x. Then, the original integral can be
written as
I =
# b
a
(# v
u
h(x, t) dt
)
dx =
# b
a
# v
u
h(x, t) dt dx.
Example 3.3.1. A historical example of this inverse operation is the evaluation
of the integral
integral of e−x2
over the positive
real line
I ≡
# ∞
0
e−x2dx.
As the reader attempting to solve this integral will soon ﬁnd out, it is impossible to
ﬁnd a primitive of the integrand. However, with
I2 =
# ∞
0
e−x2dx



=I
# ∞
0
e−y2dy



=I
=
# ∞
0
# ∞
0
e−x2e−y2



=e−(x2+y2)
dx dy
we end up with an integration over the ﬁrst quadrant of the xy-plane which opens up
the possibility of using other coordinate systems. In polar coordinates, the integrand
becomes e−r2 and the Cartesian element of area dx dy becomes the element of area
in polar coordinates, namely r dr dθ. The limits of integration correspond to the
ﬁrst quadrant, with the range of θ being from 0 to π/2 and that of r being from 0
to inﬁnity. This leads to
I2 =
# π/2
0
# ∞
0
e−r2r dr dθ =
# π/2
0
dθ



=π/2
# ∞
0
e−r2r dr.



=−1
2 e−r2

∞
0
This shows that I2 = π/4 and, therefore, I = √π/2. The reader may verify that
# ∞
−∞
e−x2dx = √π
(3.26)
by either invoking the evenness of the integrand or starting from scratch as done
above.
■
3.3.2
Components of Integrals of Vector Functions
Many calculations involve an integrand which is a vector and whose integra-
tion also leads to a vector. Let us write this as
ﬁnding the
components of the
vector resulting
from integration
of another vector
F(r) =
##
Ω
A(r, r′) dQ(r′)
=
##
Ω
[A1(r, r′)ˆe1(r′) + A2(r, r′)ˆe2(r′) + A3(r, r′)ˆe3(r′)] dQ(r′),

96
Integration: Formalism
where A1, A2, and A3 are the components of the vector A along the mutually
perpendicular unit vectors ˆe1, ˆe2, and ˆe3, respectively.12 Note that these unit
vectors are, in general, functions of the variables of integration, and that
Box 3.3.1. The geometry of the distribution of the source determines the
most convenient variables of integration (coordinate variables).
To ﬁnd the component of F(r) along any unit vector ˆea, one simply takes
the dot product of F(r) with ˆea. Thus,
Fa(r) ≡ˆea · F(r) = ˆea ·
##
Ω
A(r, r′) dQ(r′) =
##
Ω
[ˆea · A(r, r′)] dQ(r′)
≡
##
Ω
[A1(r, r′)f1(r′) + A2(r, r′)f2(r′) + A3(r, r′)f3(r′)] dQ(r′),
(3.27)
where f1(r′) ≡ˆea · ˆe1, f2(r′) ≡ˆea · ˆe2, and f3(r′) ≡ˆea · ˆe3. Once these dot
products are expressed in terms of the variables of integration, the integral
becomes an ordinary integral which, in principle, can be performed using the
guidelines above.
Box 3.3.2. In practice, ˆea is one of the unit vectors of some convenient
coordinate system which need not be the same as the coordinate system
used for integration.
For example, one may be interested in the Cartesian components of the grav-
itational ﬁeld of a spherical distribution of mass. In that case, one uses spher-
ical coordinates for integration and the unit vectors inside the integral, and
ˆex, ˆey, or ˆez for ˆea. We shall illustrate this point extensively with numerous
examples scattered throughout this chapter.
Historical Notes
By the time Newton entered the scene, an immense amount of knowledge of calculus
had accumulated. In his book Lectiones Geometricae, Barrow, for example, shows
a method of ﬁnding tangents, theorems on the diﬀerentiation of products and quo-
tients of functions, change of variables in a deﬁnite integral, and even diﬀerentiation
of implicit functions. So, why, one may wonder, is the word “calculus” so much
attached to Newton and Leibniz? The answer is in these two men’s recognition of
the generality of the methods of calculus, and, more importantly, their emphasis on
the newly discovered analytic geometry.
Isaac Newton was born in the hamlet of Woolsthorpe, England, two months
after his father’s death. His mother, in need of help for the management of the fam-
ily farm, wanted Isaac to pursue a farming career. However, Isaac’s uncle persuaded
him to enter Trinity College, Cambridge University. Newton took the entrance exam
12These unit vectors are usually those of a convenient coordinate system.

3.3 Guidelines for Calculating Integrals
97
and was accepted to the College in 1661 with a deﬁciency in Euclidean geometry.
Apparently receiving very little stimulation from his teachers, except possibly Bar-
row, he studied Descartes’s G´eom´etrie, as well as the works of Copernicus, Kepler,
Galileo, Wallis, and Barrow, by himself.
Isaac Newton
1642–1727
Upon his graduation, Newton had to leave Cambridge due to the widespread
plague in the London area to spend the next eighteen months, during 1665 and
1666, in the quiet of his family farm at Woolsthorpe. These eighteen months were
the most productive of his (as well as any other scientist’s) life. In his own words:
In the beginning of 1665 I found the . . . rule for reducing any dignity
[power] of binomial to a series.13 The same year, in May, I found the method
of tangents . . . and in November the direct method of Fluxions [the elements
of what is now called diﬀerential calculus], and the next year in January had
the theory of Colours, and in May following I had entrance into the inverse
method of Fluxions [integral calculus], and in the same year I began to think
of gravity extending to the orb of the Moon . . . and . . . compared the force
requisite to keep the Moon in her orb with the force of gravity at the surface
of the Earth.
Newton spent the rest of his scientiﬁc life developing and reﬁning the ideas
conceived at his family farm.
At the age of 26 he became the second Lucasian
professor of mathematics at Cambridge replacing Isaac Barrow who stepped aside
in favor of Newton. At 30 he was elected a Fellow of the Royal Society, the highest
scientiﬁc honor in England.
Newton often worked until early morning, kept forgetting to eat his meals, and
when he appeared, once in a while, in the dining hall of the college, his shoes
were down at the heels, stockings untied, and his hair scarcely combed.
Being
always absorbed in his thoughts, he was very naive and impractical concerning
daily routines. It is said that once he made a hole in the door of his house for his
cat to come in and out. When the cat had kittens, he added some smaller holes in
the door!
Newton did not have a pleasant personality, and was often involved in contro-
versy with his colleagues. He quarreled bitterly with Robert Hooke (founder of the
theory of elasticity and the discoverer of Hooke’s law) concerning his theory of color
as well as priority in the discovery of the universal law of gravitation. He was also
involved in another priority squabble with the German mathematician Gottfried Leib-
niz over the development of calculus. With Christian Huygens, the Dutch physicist,
he got into an argument over the theory of light. Astronomer John Flamsteed, who
was hardly on speaking terms with Newton, described him as “insidious, ambitious,
excessively covetous of praise, and impatient of contradictions . . . a good man at the
bottom but, through his nature, suspicious.”
De Morgan says that “a morbid fear of opposition from others ruled his whole
life.”
Because of this fear of criticism, Newton hesitated to publish his works.
When in 1672 he did publish his theory of light and his philosophy of science, he
was criticized by his contemporaries. Newton decided not to publish in the future,
a decision that had to be abandoned frequently.
His theory of gravity, although germinated in 1665 under the inﬂuence of works
by Hooke and Huygens, was not published until much later, partly because of his
fear of criticism. Another reason for this hesitance in publishing this result was his
13Newton is talking about the binomial theorem here.

98
Integration: Formalism
lack of proof that the gravitational attraction of a solid sphere acts as if the sphere’s
map were concentrated at the center. So, when his friend Edmund Halley urged
him in 1684 to publish his results, he refused. However, in 1685 he showed that
a sphere whose density varies only with distance to the center does in fact attract
particles as though its mass were concentrated at the center, and agreed to write up
his work. Halley then assisted Newton editorially and paid for the publication. The
ﬁrst edition of Philosophiae Naturalis Principia Mathematica appeared in 1687, and
the Newtonian age began.
3.4
Problems
3.1. Use Equation (3.7) to show that
 a
a f(t) dt = 0.
3.2. In Equation (3.8), it was assumed that p < q < r.
Show that the
equation holds even if q is not between p and r.
3.3. For each of the following integrals make the given change of variables:
(a)
 8
0 t dt,
t = y3.
(b)
 1
0
dt
1+t2 ,
t = tan y, 0 ≤y ≤π/2.
(c)
 1
0
dt
1+t,
t = ln y.
(d)
 ∞
1
t dt
1+t3 ,
t = 1
y.
3.4. By a suitable change of variables, show the following integral identities:
(a)
 ∞
−∞
dt
(a2+t2)3/2 =
2
a2
 π/2
0
cos t dt.
(b)
 ∞
0
dt
(1+t)2 =
 1
0 dt.
3.5. If
g(x) =
# sin(πx)
x2−1
{cos[π(t + x)]} e−t4 sin2[(π/2) ln(tx2+1)] dt,
ﬁnd g′(1).
3.6. Suppose that F(x) =
 cos x
0
ext2dt, G(x) =
 cos x
0
t2ext2dt, and H(x) =
G(x) −F ′(x).
Find H(x) in terms of elementary functions.
Show that
H(π/4) = eπ/8/
√
2.
3.7. Suppose that F(x) =
 sin x
0
ln(cos2 x + t2 + 1) dt, G(x) =
 sin x
0
(cos2 x +
t2 + 1)−1dt, and H(x) = F ′(x) + 2 sin x cos xG(x). Find H(x) in terms of
elementary functions. Show that H(π/3) = ln 2/2.
3.8. Evaluate the derivative of the following integrals with respect to x at
the given values of x:
(a)
 x
0 e−t2dt
at x = 1.
(b)
 x
−3 cos t dt
at x = π.
(c)
 √
cos(x/3)
−∞
e−t2dt
at x = π.
(d)
 x2
0
cos (√s) ds
at x = π.

3.4 Problems
99
3.9. Find the numerical value of the derivative of the following two integrals
at x = 1:
(a)
 ln x
0
e−x(t2−2)dt.
(b)
 x2+a−1
a
sin
*
πxe−t2
2e−(x2+a−1)2
+
dt.
3.10. Write the derivatives with respect to x of the following integrals in
terms of other integrals. Do not try to evaluate the integrals.
(a)
 b
a ln(1 + sx) ds.
(b)
 b
a
dt
t2+x2 .
(c)
 1
0
√
x2 + a2 −2ax cos t dt.
3.11. Diﬀerentiate
 ∞
−∞dt/(z + t2) = π/√z with respect to z to show that
(a)
 ∞
−∞
dt
(1+t2)2 = π
2 .
(b)
 ∞
−∞
dt
(1+t2)3 = 3π
8 .
3.12. Using the method of Example 3.2.2, ﬁnd the following integrals:
(a)
 b
a t2 sin t dt.
(b)
 b
a t3 sin t dt.
(c)
 b
a t4 sin t dt.
(d)
 b
a t2 cos t dt.
(e)
 b
a t3 cos t dt.
(f)
 b
a t4 cos t dt.
In each case calculate the primitive of the integrand and verify your answer
by diﬀerentiating the primitive.
3.13. Find the integral
Γ(n + 1) =
# ∞
0
tne−tdt
by ﬁrst evaluating the integral
# ∞
0
e−xtdt
and then diﬀerentiating the result n times, and setting x = 1 at the end. Can
you see why Γ(n + 1) is called the factorial function?
3.14. Sketch each of the following integrands to decide whether the approxi-
mation to the integral is good or not.
(a)
 0.1
−0.1
dt
10+t2 ≈0.02.
(b)
 0.1
−0.1
dt
0.001+t2 ≈200.
(c)  0.1
−0.1 cos(5πx) dx ≈0.2.
(d)  0.1
−0.1 cos πx
10 dx ≈0.2.
(e)
 0.1
−0.1 e−100t2dt ≈0.2.
(f)
 0.1
−0.1 e−t2/100dt ≈0.2.
3.15. Show that if a function is even (odd), then its derivative is odd (even).
3.16. Use the result of Example 3.3.1 to show that
# ∞
−∞
e−xt2dt =
π
x.

100
Integration: Formalism
3.17. By diﬀerentiating the electrostatic potential
Φ(r) =
##
Ω
ke dq(r′)
|r −r′|
with respect to x, y, and z, and assuming that Ω is independent of x, y, and
z, show that the electric ﬁeld
E(r) =
##
Ω
ke dq(r′)
|r −r′|3 (r −r′)
can be written as
E = −∂Φ
∂x ˆex −∂Φ
∂y ˆey −∂Φ
∂z ˆez.

Chapter 4
Integration: Applications
The preceding chapter introduced integration and dealt with its formal as-
pects. It also gave some general guidelines concerning the calculation and
manipulation of integrals, in particular how to reduce the process of multiple
integration to a number of single integrations. In this chapter, we apply the
formalism of the previous chapter to concrete examples.
4.1
Single Integrals
This section is devoted to the simple but important case of single integrals
with examples from mechanics, electrostatics and gravity, and magnetostatics.
Generally, we encounter problems which are deﬁned and set up in a single
dimension leading to integrals that have a single variable to be integrated.
4.1.1
An Example from Mechanics
In our discussion of primitive, Equation (3.18) clearly shows that integration
can be interpreted as the inverse of diﬀerentiation.
Thus, if we know the
functional form of the derivative of a quantity, we should be able to express
the quantity in terms of an integral.
Velocity is the derivative of displacement. So, we seek to write displace-
ment in terms of an integral of velocity. This is easily done as follows:1
dr
dt = v(t) ⇒dr
ds = v(s) ⇒dr = v(s) ds.
Integrating both sides from 0 to t, we get
# t
0
dr =
# t
0
v(s) ds ⇒r(t) −r0 =
# t
0
v(s) ds,
(4.1)
where r0 = r(0), and we used Equation (3.21).
1As cautioned below, we change t to s because we anticipate using t as the upper limit
of the integral.

102
Integration: Applications
There is an alternative derivation of the last formula which relies directly
on the deﬁnition of integral. Since the velocity of the particle is changing,
we cannot ﬁnd the displacement by simple multiplication with time. How-
ever, if we divide the time interval (from 0 to t) into N small subintervals,
and concentrate on the motion of the particle in each subinterval, then each
displacement can be approximated by the product of velocity and the small
time-interval, and the total displacement r(t) −r0 will be simply the sum of
all such displacements. This is summarized as
r(t) −r0 ≈
N

i=1
v(si) Δsi
which, in the limit of larger and larger N, gives
r(t) −r0 =
# t
0
v(s) ds.
Notice how careful we have been to avoid using the same variable for
integration as well as the limit of integration. This is a practice the reader
should constantly keep in mind. As a rule
important caution!
Box 4.1.1. (Caution!). Never use the same symbol for the variable of
an integral and its limits, or of an integral and of another integral of which
the ﬁrst integral is the integrand.
The following example is a good illustration of the signiﬁcance of the concept
of an integral and the rule in the Box above.
Example 4.1.1. In mechanics, Newton’s second law places special importance
on acceleration,2 and a knowledge of acceleration is normally suﬃcient to solve a
mechanical problem, i.e., ﬁnd displacement as a function of time.
A particular
example of this situation is when acceleration is known as a function of time, in
which case we can immediately ﬁnd the velocity in exact analogy with Equation
(4.1). We thus have
v(t) −v0 =
# t
0
a(s) ds ⇒v(t) = v0 +
# t
0
a(s) ds.
Notice how the argument of v is the same as the upper limit of integration. Now that
we have velocity, we can substitute it in Equation (4.1) to ﬁnd the displacement.
This gives
r(t) −r0 =
# t
0
(
v0 +
# s
0
a(u) du
)
ds
or
r(t) = r0 + v0t +
# t
0
ds
# s
0
a(u) du.
2Because the second law of motion connects acceleration and the cause of motion, force.

4.1 Single Integrals
103
u
s
u
line s = 
s = t
u = t
Figure 4.1: The region of integration for calculating position as a double integral.
In the double integral, it is understood that the u-integration is to be done ﬁrst,
followed by the s-integration. As the last double integral suggests, the region of
integration, in the us-plane, is a right triangle bounded by the vertical axis (the s-
axis, or u = 0), the line u = s, and the horizontal line s = t as shown in Figure 4.1.
It is convenient, in this case, to change the order of integration. The lower limit of
given a deﬁnite
double integral,
one can
reconstruct the
region of
integration in a
plane.
the s-integral—the ﬁrst integration—is u and the upper limit is t. Once this integral
is done, the u-integral goes from 0 to t, as can easily be veriﬁed. We, therefore, have
r(t) = r0 + v0t +
# t
0
du
# t
u
a(u) ds = r0 + v0t +
# t
0
a(u) du
# t
u
ds
(4.2)
= r0 + v0t +
# t
0
a(u)(t −u) du = r0 + v0t + t
# t
0
a(u) du −
# t
0
u a(u) du.
It is instructive for the reader to show that the ﬁrst derivative of this expression
gives the velocity and the second derivative the acceleration.
■
Historical Notes
Two men are credited with the invention of calculus, Newton and Leibniz. Of course,
as we have seen, the “invention” of calculus was a long process involving many gen-
erations of mathematicians. Nevertheless, Newton and Leibniz made great contri-
butions to the subject and gave it a prominent role in the subsequent evolution of
mathematical thought.
Gottfried Wilhelm Leibniz studied law and, after defending a thesis in logic,
received a Bachelor of Philosophy degree. He wrote a second thesis on a universal
method of reasoning in 1666 which completed his work for a doctorate in philoso-
phy at the University of Altdorf and qualiﬁed him for a professorship. During the
years 1670 and 1671, Leibniz wrote his ﬁrst papers on mechanics and produced his
calculating machine.
Leibniz was also involved in the politics of his time. In March, 1672, he went to
Paris on a political mission as an ambassador of the Elector of Mainz. While in Paris,
he made contact with notable mathematicians and scientists including Huygens. This
stirred up his interest in mathematics, a subject that he knew nothing about prior
to 1672. In 1673 he went to London and met other scientists and mathematicians
including the secretary of the Royal Society of London.
Gottfried Wilhelm
Leibniz 1646–1716
While making his living as a diplomat, he delved further into mathematics and
read Descartes and Pascal. In 1676 Leibniz was appointed librarian and councilor to
the Elector of Hanover. Twenty-four years later the Elector of Brandenburg invited

104
Integration: Applications
Leibniz to work for him in Berlin.
While involved in many political maneuvers,
including the succession of George Ludwig of Hanover to the English throne, Leibniz
worked in many ﬁelds and his side activities encompassed an enormous range. He
died in 1716, undeservedly neglected.
In addition to being a diplomat, Leibniz was a philosopher, lawyer, historian,
and pioneer geologist. He did important work in logic, mathematics, optics, me-
chanics, hydrostatics, nautical science, and calculating machines. Although law was
his profession, his contributions to mathematics and philosophy are among the best.
He tried endlessly to reconcile the Catholic and Protestant faiths. He founded the
Berlin Academy in 1700.
He criticized the universities for being “monkish” and
charged that they possessed learning but no judgment and were absorbed in triﬂes.
Instead he urged that true knowledge—mathematics, physics, chemistry, anatomy,
botany, zoology, history, and geography be pursued. He favored the German lan-
guage over Latin because Latin was tied to the older, useless thought. Men mask
their ignorance, he said, by using the Latin language to impress people.
His numerous mathematical notes on diﬀerentiation and integration is full of
novel ideas. His notations were quite ingenious: He introduced the notation dy/dx
for the derivative and

for the integral. He recognized the operations of integration
and diﬀerentiation as the inverse of one another.
4.1.2
Examples from Electrostatics and Gravity
In electrostatics or magnetostatics, one is sometimes interested in calculating
the electric or magnetic ﬁeld of a linear charge or current distribution. In
electrostatics, one can imagine sprinkling electric charges on a thin piece of
string and asking for the electric ﬁeld of the charge distribution. In magne-
tostatics, one ﬂows an electric current through a thin wire and asks for the
resulting magnetic ﬁeld. In general, the string or the wire, being a curve in
space, has a parametric equation given, in Cartesian coordinates say, by
 
f(t), g(t), h(t)
!
, where f, g, and h are known functions of the parameter t.
The problems of gravity are entirely analogous to those of electrostatics. The
master equation of electrostatocs is Equation (3.3) which we reproduce here
for convenience:
E =
##
Ω
kedq(r′)
|r −r′|3 (r −r′),
Φ =
##
Ω
kedq(r′)
|r −r′| .
(4.3)
Cartesian Coordinates
Let us assume that Cartesian coordinates are suitable for the problem, and we
want to calculate the electrostatic ﬁeld at a point P with coordinates (x, y, z)
as shown in Figure 4.2. We reduce the integrals in Equation (4.3) to single
integrals by calculating their various parts entirely in terms of t. First we
note that the source point P ′ lies on the curve, and therefore, its coordinates
(x′, y′, z′) are functions of t. Since we are using Cartesian coordinates, the
components of the position vector of P ′ are the same as the source point’s
coordinates. Therefore, r′ = x′ˆex + y′ˆey + z′ˆez = ⟨x′, y′, z′⟩.

4.1 Single Integrals
105
P
dl
r
r
'
x
y
z
Figure 4.2: Electrostatic ﬁeld of a general linear charge distribution.
The element of charge
dq(r′) = λ(r′) dl(r′) = λ(r′)
	
(dx′)2 + (dy′)2 + (dz′)2
(4.4)
turns into a function of t (times dt) after the substitutions:
x′ = f(t),
y′ = g(t),
z′ = h(t),
dx′ = f ′(t)dt,
dy′ = g′(t)dt,
dz′ = h′(t)dt.
Similarly,
r −r′ = xˆex + yˆey + zˆez −x′ˆex −y′ˆey −z′ˆez
= (x −x′) ˆex + (y −y′) ˆey + (z −z′) ˆez
(4.5)
and
|r −r′| =

(x −x′)2 + (y −y′)2 + (z −z′)2,
|r −r′|3 =
*
(x −x′)2 + (y −y′)2 + (y −y′)2+3/2
.
(4.6)
Substituting all the above in Equation (4.3) yields an integral in t for E and
another integral in t for Φ. The limits of these integrals are determined from
the parametric equation of the curve describing the linear charge distribution.
As a general rule, in order to ﬁnd the components of the ﬁeld along a unit
vector, we use Box 1.1.2, i.e., we take the dot product of the ﬁeld with that
unit vector. This involves taking the dot product of the integrand with the
unit vector. In the case of Cartesian unit vectors, this procedure simply picks
out the integral multiplying one of the unit vectors. For other coordinate
systems, this is not the case, as we shall see shortly.
Box 4.1.2. Although the geometry of the source (charge distribution) may
dictate a particular coordinate system, the components of the ﬁeld can be
calculated in any coordinate system desired.

106
Integration: Applications
Thus, by multiplying the integrand by ˆeρ, ˆeϕ, and ˆez and expressing the dot
products ˆeρ ·ˆex, ˆeϕ ·ˆex, etc., in terms of Cartesian coordinates, we can obtain
Eρ, Eϕ, and Ez as integrals over t. A similar derivation gives the electric
potential Φ as an integral over t. Although a formula can be obtained for the
components of the electric ﬁeld for a general curve (see Problem 4.3), it is
best to learn the formalism by an example.
Example 4.1.2. The simplest example of the general discussion above is a thin
rod of length L that is uniformly charged with constant linear density λ. We want
to ﬁnd the electric ﬁeld and the electrostatic potential at an arbitrary point P in
space, as shown in Figure 4.3(a).
As discussed at the beginning of this section, it pays to choose one’s coordinates
wisely. Clearly, the rod deﬁnes an axis naturally. So, let us choose our z-axis to lie
along the rod. Once this is done, we are free to move the origin up and down, and
orient the x- and y-axes. Let us use this freedom to put the ﬁeld (or observation)
point P on the x-axis. We then have a situation depicted in Figure 4.3(b).
To continue, we need the parametric equation of the rod. Clearly, the x′ and
y′ parts have the (unique) “parameterization” x′ = 0 and y′ = 0. There are many
ways to parameterize the z′ part of the curve. However, in situations involving only
one coordinate, it is most natural to set that coordinate equal to the parameter t.
So, we choose the following simple parameterization:
x′ = 0, y′ = 0, z′ = t, a ≤t ≤a + L ≡b.
Substituting this and r = xˆex in Equations (4.5) and (4.6) yields
r −r′ = xˆex −tˆez,
as well as |r −r′| =
√
x2 + t2 and |r −r′|3 = (x2 + t2)3/2.
Putting all this in Equation (4.3) yields
E(x, y, z) =
# b
a
keλdt
(x2 + t2)3/2 (xˆex −tˆez) dt
(4.7)
P
P
Ez
Ex
E
x
y
z
a
L
(a)
(b)
Figure 4.3: Electrostatic ﬁeld of a uniformly charged rod of length L. (a) The point
P and the rod, and (b) a convenient Cartesian coordinate system for the calculation of
the ﬁeld. The ﬁgure assumes a negative λ.

4.1 Single Integrals
107
To ﬁnd the components of the ﬁeld in any coordinate system, dot-multiply Equation
(4.7) by the unit vectors of that coordinate system.
For Cartesian components,
Ex = E · ˆex, which picks the term multiplying ˆex in (4.7); Ey = E · ˆey, which is
zero; Ez = E · ˆez, which picks the term multiplying ˆez in (4.7). Thus,
Ex = keλx
# b
a
dt
(x2 + t2)3/2 = keλ
x

b
√
x2 + b2 −
a
√
x2 + a2

,
Ey = 0,
(4.8)
Ez = −keλ
# b
a
t dt
(x2 + t2)3/2 = −keλ

1
√
x2 + a2 −
1
√
x2 + b2

.
It is instructive to consider special cases of these formulas, such as when a = −L/2
and b = +L/2 (especially when L is large compared to x), which may be more
familiar to the reader. We leave such considerations as exercises.
The electrostatic potential can be obtained similarly. From Equation (4.3), we
get
Φ(x, y, z) =
# b
a
keλ
(x2 + t2)1/2 dt = ke λ ln(t +
	
x2 + t2)



b
a
= keλ ln
 b +
√
x2 + b2
a +
√
x2 + a2

.
■
Cylindrical Coordinates
For cylindrical coordinates the components of the position vector of P ′ are
caution!
coordinates and
components are
not the same.
not the same as the coordinates of P ′. In fact, r′ = ρ′ˆeρ′ + z′ˆez.
Various parts of the “master” equation (4.3) [or (3.3)] can be calculated
as before—this time, of course, in cylindrical coordinates—and the results
substituted in it to arrive at the expression for E entirely in terms of t. Thus
dq(r′) = λ(r′) dl(r′) = λ(r′)
	
(dρ′)2 + ρ′2(dϕ′)2 + (dz′)2,
(4.9)
where use has been made of Equation (2.29). Similarly, we have
r −r′ = ρˆeρ + zˆez −ρ′ˆeρ′ −z′ˆez = ρˆeρ −ρ′ˆeρ′ + (z −z′)ˆez
(4.10)
which leads to the absolute value
|r −r′| =
	
(r −r′) · (r −r′)
=

[ρˆeρ −ρ′ˆeρ′ + (z −z′)ˆez] · [ρˆeρ −ρ′ˆeρ′ + (z −z′)ˆez].
Carrying out the dot product and keeping in mind that ˆeρ and ˆeρ′ are neither
the same nor perpendicular to each other, but make the two diﬀerent angles
ϕ and ϕ′ with the x-axis, we obtain
|r −r′| =

ρ2 + ρ′2 −2ρρ′ cos
 
ϕ −ϕ′!
+ (z −z′)2,
|r −r′|3 =
,
ρ2 + ρ′2 −2ρρ′ cos
 
ϕ −ϕ′!
+ (z −z′)2-3/2 .

108
Integration: Applications
Putting everything together, we obtain
E =
##
Ω
keλ(r′)
	
(dρ′)2 + ρ′2(dϕ′)2 + (dz′)2
,
ρ2 + ρ′2 −2ρρ′ cos
 
ϕ −ϕ′!
+ (z −z′)2-3/2
×
 
ρˆeρ −ρ′ˆeρ′ + (z −z′)ˆez
!
.
(4.11)
To ﬁnd components in any coordinate system, use Box 1.1.2 and take the dot
product of Equation (4.11) with the appropriate unit vectors. The electro-
static potential is derived in a similar way.
Example 4.1.3. Let us reconsider the example of a rod. Obviously we should
choose our z-axis along the rod. We further move the origin so that P ends up in the
xy-plane (see Figure 4.4). This will reduce r to ρ ˆeρ. The simplest parameterization
of the rod is
ρ′ = 0, z′ = t, a ≤t ≤a + L ≡b.
We note that ϕ′ is undeﬁned. This poses no problem because, as will be seen below,
it will drop out of the equations. Putting these in Equation (4.11) we obtain
E = keλ
##
Ω
	
(0)2 + (0)(dϕ′)2 + (dz′)2
,
ρ2 + (0)2 −2ρ(0) cos
 
ϕ −ϕ′!
+ (0 −z′)2-3/2
×
.
ρˆeρ + (0)ˆeρ + (0 −z′)ˆez
/
(4.12)
= keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρˆeρ −tˆez
!
To ﬁnd the components of the electric ﬁeld, take the dot product of one of the
unit vectors of a coordinate system and Equation (4.12). For the ρ component, we
have
Eρ = E · ˆeρ = keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρˆeρ −tˆez
!
· ˆeρ
= keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρ ˆeρ · ˆeρ
  
=1
−t ˆez · ˆeρ
  
=0
!
(4.13)
= keλρ
# b
a
dt
(ρ2 + t2)3/2 = keλ
ρ
0
b
	
ρ2 + b2 −
a
	
ρ2 + a2
1
;
x
y
z
P
ρ
Ez
Eρ
E
a
L
Figure 4.4: Electrostatic ﬁeld of a uniformly charged rod of length L in cylindrical
coordinates. The ﬁgure assumes a negative λ.

4.1 Single Integrals
109
for the ϕ component, we obtin
Eϕ = E · ˆeϕ = keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρˆeρ −tˆez
!
· ˆeϕ
= keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρ ˆeρ · ˆeϕ
  
=0
−t ˆez · ˆeϕ
  
=0
!
= 0.
(4.14)
Note how the dependence on ϕ has completely disappeared because of the azimuthal
symmetry of the rod. Finally the z component is
Ez = E · ˆez = keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρˆeρ −tˆez
!
· ˆez
= keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρ ˆeρ · ˆez
  
=0
−t ˆez · ˆez
  
=1
!
(4.15)
= −keλ
# b
a
t dt
(ρ2 + t2)3/2 = keλ
0
1
	
ρ2 + b2 −
1
	
ρ2 + a2
1
The electrostatic potential Φ can be calculated similarly.
We can also ﬁnd the components in Cartesian coordinates by dot-multiplying
Equation (4.12) with Cartesian unit vectors. For example,
Ex = E · ˆex = keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρˆeρ −tˆez
!
· ˆex
= keλ
# b
a
dt
(ρ2 + t2)3/2
 
ρ ˆeρ · ˆex
  
=cos ϕ
−t ˆez · ˆex
  
=0
!
,
= keλρ cos ϕ
# b
a
dt
(ρ2 + t2)3/2 = keλ cos ϕ
ρ
0
b
	
ρ2 + b2 −
a
	
ρ2 + a2
1
Ey will be the same except that instead of cos ϕ it will have sin ϕ, and Ez will
be identical to the Ez of Equation (4.15). When ϕ = 0, we recover the result of
Example 4.1.2, because ρ = x when ϕ = 0.
■
All the foregoing derivations in electrostatics can be applied almost ver-
batim to the theory of gravitation. The only diﬀerence is the appearance of
G instead of ke and the interpretation of λ as linear mass density.
4.1.3
Examples from Magnetostatics
Probably the most realistic physical application of single integrals appears in
the calculation of magnetic ﬁelds of currents in (thin) wires. Before looking
at examples, let us brieﬂy review magnetism.
We already mentioned in Chapter 1 that the magnetic ﬁeld of N (slowly)
moving point charges is given by3
B =
N

k=1
kmqkvk × (r −rk)
| r −rk|3
.
(4.16)
3“Slow” compared to the speed of light which is 3 × 108 m/s.

110
Integration: Applications
vj
Δ qj
(a)
(b)
Figure 4.5: Magnetic ﬁeld of a moving charge distribution. (a) All charges in motion
with a “sample” singled out. The vectors show the velocities of some of the charges in
the sample. (b) The sample is described by a charge Δqj and an average velocity vj.
In a typical situation, N is of the order of 1025 or more.
So, instead of
adding all the terms individually, we lump together those that are close to
one another, i.e., in a small region, and subsequently describe the situation
by a current density (see Figure 4.5). This boils down to writing Equation
(4.16) as
B ≈
M

j=1
kmΔqjvj × (r −rj)
|r −rj|3
,
where Δqj is the amount of charge in the jth region, vj is the average velocity
of all charges in the jth region, and rj is the position vector of the “center”
of the jth region. We can rewrite the equation above as
B ≈
M

j=1
km [Δq(rj)v(rj)] × (r −rj)
|r −rj|3
.
In the limit that M →∞and Δq →0, we obtain
Biot–Savart law
Box 4.1.3. The magnetic ﬁeld of a moving charge distribution is given
by
B(r) = km
##
Ω
dq(r′)v(r′) × (r −r′)
|r −r′|3
.
(4.17)
This is the most general form of the Biot–Savart law.
The product of the element of charge and velocity appearing in the equa-
tion is related to the various forms of current we may encounter. These are
described below:
volume current density:
dq(r′)v(r′) = ρ(r′)v(r′)



≡J(r′)
dV (r′) ≡J(r′) dV (r′),

4.1 Single Integrals
111
surface current density:
dq(r′)v(r′) = σ(r′)v(r′)



≡j(r′)
da(r′) ≡j(r′) da(r′),
linear current density:
dq(r′)v(r′) = λ(r′)v(r′)



≡I(r′)
dl(r′) ≡I(r′) dl(r′).
The volume current density J(r′) describes a situation in which charges are
free to move in all directions. The surface current density j(r′) is used when
charges are conﬁned to a surface. The most familiar current density is the
linear current density which is usually rewritten as
I(r′) dl(r′) = Id⃗l(r′) = Idr′.
This follows from the fact that I(r′) is in the same direction as the velocity
(at r′) which, since charges are conﬁned to a curve (the wire), has the same
direction as the (inﬁnitesimal) tangent displacement along the wire, namely
dr′.
We are particularly interested in the linear case as shown in Figure 4.6.
Thus, assuming that the current I is constant—it has to be due to charge
Biot–Savart law
for circuits
conservation—we obtain
Box 4.1.4. The general expression for the magnetic ﬁeld of a circuit is
given by
B(r) = kmI
2 dr′ × (r −r′)
| r −r′|3
,
(4.18)
where the circle on the integral sign implies a closed loop.
This equation is independent of any coordinate systems. We now specialize
to Cartesian and cylindrical systems.
P
d l
r
x
y
z
dB
′r
Figure 4.6: A general current ﬁlament described parametrically and used to calculate
the magnetic ﬁeld in Cartesian coordinates.

112
Integration: Applications
Cartesian Coordinates
To obtain the magnetic ﬁeld we substitute
r = xˆex + yˆey + zˆez,
r′ = x′ˆex + y′ˆey + z′ˆez,
dr′ = ˆexdx′ + ˆeydy′ + ˆezdz′,
r −r′ =
 
x −x′!ˆex +
 
y −y′!ˆey +
 
z −z′!ˆez,
|r −r′|3 =
* 
x −x′!2 +
 
y −y′!2 +
 
z −z′!2+3/2
in Equation (4.18). For the cross product, we need to expand the determinant
dr′ × (r −r′) = det
⎛
⎝
ˆex
ˆey
ˆez
dx′
dy′
dz′
x −x′
y −y′
z −z′
⎞
⎠,
using Figure 1.5.
Cylindrical Coordinates
The cylindrical coordinates can be handled in exact analogy with the Carte-
sian case. Using Equations (1.19) and (2.28), we have
r = ρ ˆeρ + zˆez,
r′ = ρ′ˆeρ′ + z′ˆez,
r −r′ = ρ ˆeρ −ρ′ˆeρ′ + (z −z′)ˆez,
(4.19)
dr′ = ˆeρ′dρ′ + ˆeϕ′ρ′dϕ′ + ˆezdz′,
|r −r′|3 =
,
ρ2 + ρ′2 −2ρρ′ cos(ϕ −ϕ′) + (z −z′)2-3/2.
The cross product cannot be done using determinants because not everything
is written in terms of the three mutually perpendicular unit vectors: ˆeρ is
diﬀerent from ˆeρ′ but not perpendicular to it. In fact, this diﬀerence is the
cause for the appearance of the cosine term in the last equation of (4.19). To
ﬁnd the cross product, we simply multiply the two terms and use the following
relations, most of which should be familiar, and the unfamiliar ones can be
obtained using Figure 4.7:
ˆeρ′ × ˆeρ = ˆez sin(ϕ −ϕ′),
ˆeρ′ × ˆez = −ˆeϕ′,
ˆeϕ′ × ˆeρ = −ˆez cos(ϕ −ϕ′),
ˆeϕ′ × ˆeρ′ = −ˆez,
ˆeϕ′ × ˆez = ˆeρ′,
ˆez × ˆeρ = ˆeϕ,
(4.20)
ˆez × ˆeρ′ = ˆeϕ′.
The cross product can be written as
dr′ × (r −r′) = ˆez
.
ρ′2dϕ′ + ρ sin(ϕ −ϕ′) dρ′ −ρρ′ cos(ϕ −ϕ′) dϕ′/
−ˆeϕ′.
(z −z′) dρ′ + ρ′dz′/
(4.21)
+ ˆeϕρ dz′ + ˆeρ′ρ′(z −z′) dϕ′.

4.1 Single Integrals
113
z
x
y
e^
ρ
e^
e^
ϕ
e^
e^
ρ
'
ϕ
'
ϕ
'
ϕ − ϕ
'
Figure 4.7:
The orientation of some of the cylindrical unit vectors drawn for the
calculation of cross products.
To ﬁnd the components of the magnetic ﬁeld, we substitute this in
Equation (4.18), take the dot product of cylindrical unit vectors with the
integrand, and use
ˆeρ · ˆeρ′ = cos(ϕ′ −ϕ),
ˆeρ′ · ˆeϕ = sin(ϕ′ −ϕ),
ˆeρ · ˆeϕ′ = −sin(ϕ′ −ϕ),
ˆeϕ · ˆeϕ′ = cos(ϕ′ −ϕ),
(4.22)
as well as the other more obvious dot products of unit vectors.
We can derive a general expression for the components of the electric ﬁeld
in terms of the parametric functions of a general curve (see Problem 4.6).
However, a simple example will also illustrate the general procedure without
entangling the formulas with complicated expressions.
Example 4.1.4. A simple application of the foregoing general formalism is to
calculate the magnetic ﬁeld of a circular loop of radius a. The choice of the axes
and origin of Figure 4.8 yields the following parameterization of the loop:
ρ′ = a, dρ′ = 0;
ϕ′ = t, dϕ′ = dt;
z′ = 0, dz′ = 0,
0 ≤t ≤2π.
Furthermore, because of the azimuthal symmetry of the current distribution, the
ﬁnal answer will be independent of ϕ. Thus, we can set that equal to zero. Inserting
this information in Equations (4.19) and (4.21) gives
x
y
z
I
r
ρ
P
a
z
+
−
Figure 4.8: The geometry of the circular loop of current.

114
Integration: Applications
|r −r′|3 =
.
ρ2 + a2 −2ρa cos(t) + z2/3/2
dr′ × (r −r′) = ˆez
.
a2dt −ρa cos(t) dt
/
+ ˆeρ′az dt.
The magnetic ﬁeld of Equation (4.18) can now be written as
B = kmI
# 2π
0
ˆez
.
a2 −ρa cos(t)
/
+ ˆeρ′az
.
ρ2 + a2 −2ρa cos(t) + z2/3/2 dt
(4.23)
Finally, to ﬁnd the cylindrical components, dot-multiply (4.23) with the cylin-
drical unit vectors and use Equation (4.22) with ϕ = 0 (and ϕ′ = t):
Bρ = B · ˆeρ = kmIa
# 2π
0
*
ˆez
 
a −ρ cos t
!
+ ˆeρ′z
+
· ˆeρ
 
ρ2 + a2 −2ρa cos t + z2!3/2 dt
= kmIa
# 2π
0
=0
  
ˆez · ˆeρ
 
a −ρ cos t
!
+
=cos t
  
ˆeρ′ · ˆeρ z
 
ρ2 + a2 −2ρa cos t + z2!3/2 dt
(4.24)
= kmIaz
# 2π
0
cos t dt
(ρ2 + a2 −2ρa cos t + z2)3/2 ,
Similarly,
Bϕ = B · ˆeϕ = kmIa
# 2π
0
=0
  
ˆez · ˆeϕ
 
a −ρ cos t
!
+
=sin t
  
ˆeρ′ · ˆeϕ z
 
ρ2 + a2 −2ρa cos t + z2!3/2 dt
= kmIaz
# 2π
0
sin t dt
(ρ2 + a2 −2ρa cos t + z2)3/2
(4.25)
= −kmIz
ρ
(ρ2 + a2 −2aρ cos t + z2)−1/2 


2π
0 = 0,
and
Bz = B · ˆez = kmIa
# 2π
0
=1
  
ˆez · ˆez
 
a −ρ cos t
!
+
=0
  
ˆeρ′ · ˆez z
 
ρ2 + a2 −2ρa cos t + z2!3/2 dt
= kmIa
# 2π
0
(a −ρ cos t) dt
(ρ2 + a2 −2ρa cos t + z2)3/2 .
(4.26)
Once again the azimuthal symmetry prohibits a ϕ-component for the ﬁeld. These
integrals cannot be evaluated analytically, but if we specialize to the case where P
is on the z-axis (i.e., when ρ = 0), the integrals become trivial. In fact, we have
Bρ = kmIaz
# 2π
0
cos t dt
(a2 + z2)3/2 = 0,
Bϕ = 0,
Bz = −kmIa
# 2π
0
−a dt
(a2 + z2)3/2 =
2πkmIa2
(a2 + z2)3/2 .
■

4.2 Applications: Double Integrals
115
Historical Notes
After graduating from the college of Louis-le-Grand in Paris and subsequently spend-
ing some time in the army, Jean-Baptiste Biot entered the Ecole Polytechnique
in Paris where Monge (a noted mathematician of the time and an expert in dif-
ferential geometry) realized his potential.
Because of his political views and his
participation in an attempted insurrection by the royalists against the Convention,
Biot was captured by government forces and taken prisoner. Had it not been for
Jean-Baptiste Biot
1774–1862
Monge’s intervention and plead for his release, Biot’s promising career might have
ended.
Biot became Professor of Mathematics at the Ecole Centrale at Beauvais in
1797, and three years later joined the faculty of the Coll`ege de France as Professor
of Mathematical Physics an appointment which was due to the inﬂuence of Laplace.
Biot studied a wide range of mathematical topics, mostly on the applied math-
ematics side. He made advances in astronomy, elasticity, heat, and optics while, in
pure mathematics, he also did important work in geometry. He collaborated with
Arago on the refractive properties of gases.
Biot’s most notable contribution was done in collaboration with Felix Savart
(1791–1841), who was an acoustics expert and developed the Savart disk, a device
which produced a sound wave of known frequency, using a rotating cog wheel as a
measuring device.
Biot and Savart jointly discovered that the intensity of the magnetic ﬁeld set up
by a current ﬂowing through a wire varies inversely with the distance from the wire.
This is a special case of what is now known as Biot–Savart’s Law and is fundamental
to modern electromagnetic theory.
For his work on the polarization of light passing through chemical solutions Biot
was awarded the Rumford Medal of the Royal Society. He tried twice for the post
of Secretary to the Acad´emie des Sciences but lost out in 1822 to Fourier for this
post. When Fourier died he applied again only to lose to Arago.
4.2
Applications: Double Integrals
Whenever areas are sources of physical quantities such as ﬁelds, or interactions
take place on areas, such as pressure applied on a surface, double integrals
are used. We can be as general as in the previous section and consider a gen-
eral surface given by a parametric equation in two variables (instead of one
used for curves). However, since the geometry of surfaces is much more com-
plicated, and much less illuminating, we shall conﬁne our discussion to very
simple geometries which require trivial and obvious parameterization. More
speciﬁcally, we restrict ourselves to primary surfaces of the three coordinate
systems.
4.2.1
Cartesian Coordinates
Since we are restricting ourselves to primary surfaces, our choice for Cartesian
coordinates is narrowed down to planes, and if we want the boundaries of the
plane to be simple in Cartesian coordinates, we are limited to just a rectangle.

116
Integration: Applications
Example 4.2.1. We start with an example from electrostatics. A rectangular ﬂat
surface of sides a and b is charged uniformly with surface charge density σ, and we
are interested in the electric ﬁeld at a general point P in space. This is given by
E =
##
Ω
kedq(r′)
|r −r′|3 (r −r′)
with r = xˆex +yˆey +zˆez = ⟨x, y, z⟩and r′ = x′ˆex +y′ˆey = ⟨x′, y′, 0⟩, where we have
chosen the plane of the rectangle to be the xy-plane. If we choose the center of the
rectangle to be our origin, our z-axis perpendicular to the plane of the rectangle,
and our x-and y-axes parallel to the sides as shown in Figure 4.9, then the element
of area coincides with the third primary element, and we can write
dq(r′) = σ(r′) da(r′) = σ dx′ dy′.
We also have
r −r′ = (x −x′)ˆex + (y −y′)ˆey + zˆez = ⟨x −x′, y −y′, z⟩,
|r −r′| =
	
(x −x′)2 + (y −y′)2 + z2,
|r −r′|3 =
,
(x −x′)2 + (y −y′)2 + z2-3/2 .
Inserting all these relations in the expression for E, we obtain
E =
##
Ω
keσ dx′ dy′
{(x −x′)2 + (y −y′)2 + z2}3/2
.
(x −x′)ˆex + (y −y′)ˆey + zˆez
/
with components
electric ﬁeld of a
uniformly charged
rectangle
Ex = keσ
##
Ω
(x −x′) dx′ dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
Ey = keσ
##
Ω
(y −y′) dx′ dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
Ez = keσz
##
Ω
dx′ dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
where everything independent of the variables of integration, x′ and y′, is taken out
of the integrals.
We have already discussed a general procedure for evaluating multiple integrals
by reducing them to lower-dimensional integrals.
We follow the same procedure
here: The y′ integration has the lower limit −b/2 and the upper limit +b/2, both
independent of x′.4
Similarly, the x′ integration has −a/2 and a/2 as its limits.
This means that the components can be written as
Ex = keσ
# a/2
−a/2
(x −x′) dx′
# b/2
−b/2
dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
Ey = keσ
# a/2
−a/2
dx′
# b/2
−b/2
(y −y′) dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
4The independence of the limits is one reason that Cartesian coordinates are useful for
rectangular regions of integration. If we had chosen cylindrical coordinates, then the limits
of integration, the lines y′ = −b/2 and y′ = b/2, would have had to be written in cylindrical
coordinates, giving, for the upper limit, for example, ρ′ sin ϕ′ = b/2 or ρ′ = b/(2 sin ϕ′).
Thus a ρ′ integration with limits dependent on ϕ′ would have been involved.

4.2 Applications: Double Integrals
117
x
y
z
P
a
b
r
'
r
Figure 4.9: Electrostatic ﬁeld of a ﬂat rectangular charge distribution.
Ez = keσz
# a/2
−a/2
dx′
# b/2
−b/2
dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 .
Note that the x′ integration cannot be done until after the y′ integration, because
the latter has an x′-dependent integrand.
■
Having exhausted the (simple) possibilities for electrostatics (and gravity,
since the two are almost identical), we now turn to magnetostatics.
Example 4.2.2. Approximate the belt of a Van de Graﬀmachine with an isolated
moving rectangle having sides a and b, and velocity v along the side b as shown in
Figure 4.10. Furthermore, assume that the charges are uniformly distributed on the
belt with surface charge density σ. We want to ﬁnd the magnetic ﬁeld of the belt
at a general point P in space. Let us choose the positive y-direction to be that of
the velocity. Then, Equation (4.17) becomes
B(r) = km
##
Ω
σdav × (r −r′)
|r −r′|3
.
The geometry of this example is identical to that of Example 4.2.1. Therefore, we
can immediately write the integral for B:
magnetic ﬁeld of a
charged
rectangular
moving belt
B(r) = km
##
Ω
σdx′dy′vˆey × [(x −x′)ˆex + (y −y′)ˆey + zˆez]
{(x −x′)2 + (y −y′)2 + z2}3/2
,
from which the components of the magnetic ﬁeld are easily calculated:
Bx = kmσvz
# a/2
−a/2
dx′
# b/2
−b/2
dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 ,
By = 0,
(4.27)
Bz = −kmσv
# a/2
−a/2
(x −x′) dx′
# b/2
−b/2
dy′
{(x −x′)2 + (y −y′)2 + z2}3/2 .
■
P
y
z
x
r 
'
Figure 4.10: A rectangular distribution of moving charges whose magnetic ﬁeld can
be calculated using Cartesian coordinates.

118
Integration: Applications
4.2.2
Cylindrical Coordinates
The cylindrical system has two types of primary surface: planes and cylinders.
Although we considered planes in the previous subsection, we shall reconsider
them here because the third primary surface, that perpendicular to the z-axis,
gives us the possibility of solving planar problems with nonrectangular regions
of integration. Let us start with such a problem.
Example 4.2.3. In this example we want to calculate the gravitational ﬁeld of
a uniform surface mass distribution of density σm which is a segment of a planar
annular region with inner radius a and outer radius b, and whose sides make an
angle of α as shown in Figure 4.11(a). Let us choose our origin to coincide with
the center of the annular region, our x-axis to be along one of the sides, and the
xy-plane to be the plane of the mass distribution [see Figure 4.11(b)].
Recall that in cylindrical coordinates, the components of the position vector of
P ′ are not the same as the source point’s coordinates. In fact, we have
r′ = ρ′ˆeρ′,
r = ρˆeρ + zˆez,
r −r′ = ρˆeρ + zˆez −ρ′ˆeρ′,
|r −r′|3 = (ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2,
where in the last line we have made the simpliﬁcation that the ﬁeld point is in the
xz-plane, so that ϕ = 0; otherwise, we would have cos(ϕ−ϕ′) instead of cos ϕ′. The
element of mass is given by
dm(r′) = σm da(r′) = σm(dρ′)(ρ′dϕ′) = σmρ′dρ′dϕ′.
Thus, the gravitational ﬁeld is
g = −
##
Ω
Gdm(r′)
|r −r′|3 (r −r′),
= −Gσm
# b
a
ρ′ dρ′
# α
0
dϕ′(ρˆeρ + zˆez −ρ′ˆeρ′)
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 .
(4.28)
To ﬁnd the components, we take the dot product of this integral with the cylindrical
x
y
z
(a)
(b)
α
′ρ 
′
ϕ
d
d ′ρ
′
ϕ
a
b
′r
Figure 4.11: The annular region whose gravitational ﬁeld is being calculated. The
position vector of the source point and the lengths of the sides of the element of area
are also shown.

4.2 Applications: Double Integrals
119
unit vectors. The result will then be
gρ = −Gσm
# b
a
ρ′ dρ′
# α
0
(ρ −ρ′ cos ϕ′) dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 ,
gϕ = Gσm
# b
a
ρ′ dρ′
# α
0
ρ′ sin ϕ′ dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 ,
(4.29)
gz = −Gσmz
# b
a
ρ′ dρ′
# α
0
dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 .
Let us look at some special cases of this. For a complete annular region, we
simply replace α with 2π:
gρ = −Gσm
# b
a
ρ′ dρ′
# 2π
0
(ρ −ρ′ cos ϕ′) dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 ,
gϕ = Gσm
# b
a
ρ′ dρ′
# 2π
0
ρ′ sin ϕ′ dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 = 0,
(4.30)
gz = −Gσmz
# b
a
ρ′ dρ′
# 2π
0
dϕ′
(ρ2 + ρ′2 −2ρρ′ cos ϕ′ + z2)3/2 .
As expected, the ϕ-component has disappeared.
We can further simplify the geometry by locating the ﬁeld point on the z-axis.
Then, ρ = 0 and we have
gρ = Gσm
# b
a
ρ′2 dρ′
(ρ′2 + z2)3/2
# 2π
0
cos ϕ′ dϕ′ = 0,
gϕ = 0,
gz = −Gσmz
# b
a
ρ′ dρ′
(ρ′2 + z2)3/2
# 2π
0
dϕ′ = −2πGσmz
# b
a
ρ′ dρ′
(ρ′2 + z2)3/2
= −2πGσmz
(
1
√
a2 + z2 −
1
√
b2 + z2
)
.
If we take the limit a →0 and b →∞, we obtain
g = −2πGσm
z
√
z2 ˆez = −2πGσm z
|z| ˆez,
where we have used Box 4.2.1 (see below). Now note that z/|z| = ±1 depending
on the sign of z. When z > 0, we get z/|z|ˆez = ˆez which is the unit normal to the
surface. When z < 0, we get z/|z|ˆez = −ˆez which is again the unit normal to (the
other side of) the surface. Denoting the unit normal by ˆen, we can write
g = −2πGσmˆen.
The electrostatic analogue of this is obtained by substituting −ke = −1/4πϵ0
for G. This yields
E = σq
2ϵ0 ˆen
which is the ﬁeld of an inﬁnite sheet of charge with which the reader is familiar.
Note that while g always points toward the sheet (opposite to ˆen, because σm is
always positive), the direction of E is determined by the sign of σq.
■

120
Integration: Applications
4.2.3
Spherical Coordinates
One of the primary surfaces of a spherical coordinate system is a sphere, and
since there are a lot of spherical objects around, it is useful to gain experience
in calculations involving spheres.
In the following, we shall be taking square roots of functions. Care needs
to be taken when doing so:
Box 4.2.1. For any real-valued quantity A,
√
A2 ≡|A|, i.e., the square
root of the square of a quantity is the absolute value of that quantity.
Failure to keep this in mind will result in incorrect conclusions, as we shall
see below.
Example 4.2.4. In this example we are interested in the gravitational ﬁeld at a
general point P of a spherical cap, i.e., a segment of a spherical shell of radius a and
uniform surface density σ such that the cone deﬁned by the segment and the center
of the sphere has a half-angle α (see Figure 4.12). It is clear that the choice of axes
and origin resulting in the greatest simpliﬁcation is as shown in Figure 4.12. Notice
that P is taken to lie in the xz-plane, so that ϕ = 0. We can immediately write
g = −G
##
Ω
dm(r′)
|r −r′|3 (r −r′)
(4.31)
with
r′ = aˆer′,
r = rˆer,
r −r′ = rˆer −aˆer′,
|r −r′|3 =
3
r2 + a2 −2ra
ˆer·ˆer′



(sin θ sin θ′ cos ϕ′ + cos θ cos θ′)
43/2
,
(4.32)
dm(r′) = σda1 = σa2 sin θ′ dθ′ dϕ′.
By inserting these relations in (4.31) and dotting the result with unit vectors, we
obtain the three components of g in various coordinate systems. In spherical coor-
dinates these are
P
2α
x
y
z
r
Figure 4.12: A spherical cap whose gravitational ﬁeld can be calculated using spherical
coordinates.

4.2 Applications: Double Integrals
121
gr = −Gσa2
##
Ω
sin θ′ {r −a(sin θ sin θ′ cos ϕ′ + cos θ cos θ′)} dθ′ dϕ′
{r2 + a2 −2ra(sin θ sin θ′ cos ϕ′ + cos θ cos θ′)}3/2 ,
gθ = Gσa3
##
Ω
sin θ′(cos θ sin θ′ cos ϕ′ −sin θ cos θ′) dθ′ dϕ′
{r2 + a2 −2ra(sin θ sin θ′ cos ϕ′ + cos θ cos θ′)}3/2 ,
(4.33)
gϕ = Gσa3
##
Ω
sin2 θ′ sin ϕ′ dθ′ dϕ′
{r2 + a2 −2ra(sin θ sin θ′ cos ϕ′ + cos θ cos θ′)}3/2 = 0.
The region of integration Ω is one in which θ′ varies from 0 to α, and ϕ′ from 0 to
2π. The last integral vanishes because of the ϕ′ integration. The vanishing of the
ϕ-component is simply the result of the azimuthal symmetry.
The result above is not interesting, but if we move P to the polar axis, so that
θ = 0, then the equations simplify considerably, and we get
gr = −Gσa2
# α
0
sin θ′(r −a cos θ′) dθ′
(r2 + a2 −2ra cos θ′)3/2
# 2π
0
dϕ′
= −2πGσa2
# α
0
sin θ′(r −a cos θ′) dθ′
(r2 + a2 −2ra cos θ′)3/2 ,
gθ = Gσa3
# α
0
sin2 θ′dθ′
(r2 + a2 −2ra cos θ′)3/2
# 2π
0
cos ϕ′ dϕ′ = 0,
gϕ = 0.
The most interesting result is obtained when α = π, i.e., when we have a com-
plete spherical shell. Then using
# π
0
sin θ′(r −a cos θ′) dθ′
(r2 + a2 −2ra cos θ′)3/2 = 1
r2

1 −
	
(a −r)2
a −r

≡1
r2

1 −|a −r|
a −r

,
which can be looked up in a good integral table, we obtain
gr = −2πGσa2
r2

1 −|a −r|
a −r

,
gθ = 0,
gϕ = 0.
For points inside the shell, r < a; therefore |a −r|
a −r
= a −r
a −r = 1, and the ﬁeld
vanishes. Thus, the gravitational ﬁeld inside a spherical shell is zero. On the other
gravitational ﬁeld
inside a spherical
shell is zero.
hand, for points outside, r > a, and |a −r|
a −r = r −a
a −r = −1, leading to
gr = −4πGσa2
r2
= −G M
r2 ,
where M = 4πa2σ is the total mass of the shell. This is identical to the gravitational
ﬁeld of a point charge of mass M located at the center of the shell. Now, if we have
concept of
spherical mass
distribution
elaborated
a number of concentric shells, then, at a point outside the outermost one, the ﬁeld
must be that of a point charge at the common center having a mass equal to the total
mass of all the shells. Note that each shell can have a diﬀerent uniform density than
others. In particular, if we have a solid sphere, with a density which is a function of
r alone, the same result holds. A density which is a function of r alone is called a
spherical mass distribution. We thus have the famous result:

122
Integration: Applications
Box 4.2.2. When gravitationally attracting objects outside it, a spherical mass
distribution acts as if all its mass were concentrated into a point at its center.
Newton spent approximately twenty years convincing himself of this result.
Because of the similarity between gravity and electrostatics, the conclusion above
can be applied to the electrostatic ﬁeld as well. Thus, in particular, the electrostatic
ﬁeld inside any uniformly charged shell is zero.
■
We take the ﬁnal example of this section from mechanics and calculate the
moment of inertia of the foregoing shell about the polar axis. Recall that
moment of inertia
the moment of inertia of a mass distribution about an axis is deﬁned as
I =
##
Ω
R2 dm,
(4.34)
where R is the distance from the integration point—location of dm—to the
reference axis.
Example 4.2.5. The moment of inertia of the spherical shell segment is obtained
easily. All we need to note is that R = a sin θ′. Then Equation (4.34) gives
I =
##
Ω
(a sin θ′)2σa2 sin θ′ dθ′ dϕ′ = a4σ
# α
0
sin3 θ′ dθ′
# 2π
0
dϕ′
= 2πa4σ
 1
3 cos3 θ′ −cos θ′!

α
0 = 2πa4σ
3
(cos3 α −3 cos α + 2).
We can express this in terms of total mass if we note that the area is given by
A =
##
Ω
a2 sin θ′ dθ′ dϕ′ = 2πa2
# α
0
sin θ′ dθ′ = 2πa2(1 −cos α)
so that
σ = M
A =
M
2πa2(1 −cos α).
Therefore,
I = 1
3Ma2 cos3 α −3 cos α + 2
1 −cos α
,
which reduces to I = 2
3Ma2 for a complete spherical shell (with α = π).
■
4.3
Applications: Triple Integrals
To illustrate the diﬃculty of calculations when appropriate coordinate systems
are not chosen, in the following example we calculate the gravitational ﬁeld
of a uniform hemisphere at a point P on its axis in Cartesian coordinates.

4.3 Applications: Triple Integrals
123
z
x
y
dV(r ) = dx  dy  dz
P
'
'
'
'
= (x , y , z  )
'
'
'
r 
'
r
Figure 4.13: Calculating the gravitational ﬁeld of a hemisphere in the “unnatural”
Cartesian coordinates.
Example 4.3.1. The geometry of the problem is shown in Figure 4.13.
The
location of P and the choice of axes indicate that
r = zˆez,
r′ = x′ˆex + y′ˆey + z′ˆez,
|r −r′|3 =
,
x′2 + y′2 + (z −z′)2-3/2 ,
dm(r′) = ρmdV (r′) = ρmdx′dy′dz′,
where ρm is the uniform mass density. Thus,
g = −
##
Ω
G dm(r′)
|r −r′|3 (r −r′) = Gρm
##
Ω
dx′ dy′ dz′ {x′ˆex + y′ˆey + (z′ −z)ˆez}
{x′2 + y′2 + (z −z′)2}3/2
with components
gx = Gρm
##
Ω
x′ dx′ dy′ dz′
{x′2 + y′2 + (z −z′)2}3/2 ,
gy = Gρm
##
Ω
y′ dx′ dy′ dz′
{x′2 + y′2 + (z −z′)2}3/2 ,
gz = Gρm
##
Ω
(z′ −z) dx′ dy′ dz′
{x′2 + y′2 + (z −z′)2}3/2 .
The limits of integrals associated with Ω can be done as discussed in Section 3.3.
In Figure 4.13, we have chosen the ﬁrst integral to be along the z-axis. Then the
lower limit will be the xy-plane, or z′ = 0, and the upper limit, the surface of
the hemisphere. A general point P ′ in Ω with coordinates (x′, y′, z′) will hit the
hemisphere at z′ =
	
a2 −x′2 −y′2.
So, this will be the upper limit of the z′
integration. Concentrating on the x-component for a moment, we thus write
gx = Gρm
# #
S
x′ dx′ dy′
# √
a2−x′2−y′2
0
dz′
{x′2 + y′2 + (z −z′)2}3/2 ,
where S is the projection of the hemispherical surface on the xy-plane.
To do
the remaining integrations, we refer to Figure 4.14, where the projections of the
hemisphere and the point P ′ are shown.
It is clear that the y′ integration has

124
Integration: Applications
x
y
−a
+a
dx 
y 
x 
y 
lower
y 
upper
dy 
'
'
'
'
'
'
Figure 4.14: The projection of Ω, a hemisphere, in the xy-plane.
the lower semicircle as the lower limit and the upper semicircle as the upper limit.
Finally the x′ integration has lower and upper limits of −a and +a, respectively.
We, therefore, have
gx = Gρm
# +a
−a
x′ dx′
# +√
a2−x′2
−√
a2−x′2 dy′
# √
a2−x′2−y′2
0
dz′
{x′2 + y′2 + (z −z′)2}3/2 .
Instead of looking up the integrals in an integral table, we note that the integrand
of the x′ integration is an odd function. This is because it is the product of x′, which
is odd, and another function, in the form of a double integral whose integrand and
limits are even functions of x′. Since the interval of integration is symmetric, the
x′ integration vanishes. A similar argument shows that the y′ integration vanishes
as well. This is as expected intuitively: We expect the ﬁeld to be along the z-axis.
Therefore, gx = 0, gy = 0, and
gz = Gρm
# +a
−a
dx′
# +√
a2−x′2
−√
a2−x′2 dy′
# √
a2−x′2−y′2
0
(z′ −z) dz′
{x′2 + y′2 + (z −z′)2}3/2
= Gρm
# +a
−a
dx′
# +√
a2−x′2
−√
a2−x′2
dy′
	
x′2 + y′2 + z2
−Gρm
# +a
−a
dx′
# +√
a2−x′2
−√
a2−x′2
dy′

a2 + z2 + x′2 + y′2 −2z
	
a2 −x′2 −y′2
.
The y′ integration in the ﬁrst integral can be done, but the remaining x′ integration
will be complicated. The second y′ integral cannot even be performed in closed form.
This diﬃculty is a result of our poor choice of coordinates whereby the boundary of
the region of integration does not turn out to be a “natural” surface.
■
The example of the hemisphere in Cartesian coordinates indicates the
diﬃculty encountered when the boundaries of the integration region do not
match the primary surfaces of the coordinate system. In the next example,
we calculate the gravitational ﬁeld of the hemisphere in spherical coordinates.
Example 4.3.2. The spherical coordinate system makes the problem so man-
ageable that we can consider a more general mass distribution. We will calculate

4.3 Applications: Triple Integrals
125
P
g
2α
Figure 4.15: The gravitational ﬁeld of a solid cone with a spherically curved top.
the gravitational ﬁeld of a cone-shaped segment of a solid sphere of half-angle α as
shown in Figure 4.15. We are interested in the ﬁeld at a point P on the axis of the
cone as shown. Since ˆeθ and ˆeϕ cannot be deﬁned at P (why?), we expect, from
physical intuition, that the only surviving component of the gravitational ﬁeld is
radial. This component is obtained by dotting the vector ﬁeld with ˆer:
gr = ˆer · g = ˆer ·
(
−
##
Ω
G dm(r′)
|r −r′|3 (r −r′)
)
= −G
##
Ω
dm(r′)
|r −r′|3/2 (r −r′ cos θ′)
= −Gρm
##
Ω
r′2 sin θ′ dr′ dθ′ dϕ′
(r2 + r′2 −2rr′ cos θ′)3/2 (r −r′ cos θ′)
= −Gρm
# 2π
0
dϕ′
# a
0
r′2 dr′
# α
0
sin θ′ dθ′
(r2 + r′2 −2rr′ cos θ′)3/2 (r −r′ cos θ′).
To do the integrations, we use the technique of diﬀerentiating inside the integral
and note that
r −r′ cos θ′
(r2 + r′2 −2rr′ cos θ′)3/2 = −∂
∂r
1
√
r2 + r′2 −2rr′ cos θ′ .
Therefore, the integral becomes
gr = 2πGρm
# a
0
r′2 dr′
# α
0
sin θ′ dθ′ ∂
∂r
1
√
r2 + r′2 −2rr′ cos θ′
= 2πGρm ∂
∂r
# a
0
r′2 dr′
# α
0
sin θ′ dθ′
√
r2 + r′2 −2rr′ cos θ′
= 2πGρm ∂
∂r
# a
0
r′2 dr′
 1
rr′
	
r2 + r′2 −2rr′ cos θ′



α
0

= 2πGρm ∂
∂r
(1
r
# a
0
r′ dr′ 	
r2 + r′2 −2rr′ cos α −
	
(r −r′)2
)
= 2πGρm ∂
∂r
(1
r
# a
0
r′ dr′ 	
r2 + r′2 −2rr′ cos α −|r −r′|
)
.
(4.35)
The integral involving the absolute value can be done easily. However, we have
to be careful about the relative size of r, a, and r′. We therefore consider two cases:
r ≥a and r ≤a. Keeping in mind that r′ ≤a, the ﬁrst case yields

126
Integration: Applications
# a
0
r′|r −r′| dr′ =
# a
0
r′(r −r′) dr′ = ra2
2
−a3
3 ,
r ≥a.
For the second case, we have to split the interval of integration in two, and write
the absolute value accordingly:
# a
0
r′|r −r′| dr′ =
# r
0
r′(r −r′) dr′ +
# a
r
r′(r′ −r) dr′
= r3
3 + a3
3 −ra2
2 ,
r ≤a.
Substituting these in Equation (4.35), we get
gr = 2πGρm ∂
∂r
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
a3
3r −a2
2 + 1
r
# a
0
r′√
r2 + r′2 −2rr′ cos α dr′
if
r ≥a,
a2
2 −r2
3 −a3
3r + 1
r
# a
0
r′√
r2 + r′2 −2rr′ cos α dr′
if
r ≤a.
The remaining integral can also be performed with the result
1
r
# a
0
r′	
r2 + r′2 −2rr′ cos α dr′ = −r2
12(1 −3 cos 2α)
+
	
r2 + a2 −2ra cos α
 a2
3r + r
12 −a cos α
6
−r cos 2α
4

+ r2 cos α sin2 α
2
ln
 a −r cos α +
√
r2 + a2 −2ra cos α
r −r cos α

.
The special case of α = π, i.e., a full sphere, is very important, because his-
torically it motivated the rapid development of integral calculus. For this case, we
have
1
r
# a
0
r′	
r2 + r′2 −2rr′ cos α dr′
α=π
−→r2
6 + (a + r)
 a2
3r −r
6 + a
6

,
whereby the radial component of the ﬁeld becomes
gr = 2πGρm ∂
∂r
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
a3
3r −a2
2 + r2
6 + (a + r)
a2
3r −r
6 + a
6

if
r ≥a,
a2
2 −r2
6 −a3
3r + (a + r)
a2
3r −r
6 + a
6

if
r ≤a,
= 2πGρm
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
−2a3
3r2
if
r ≥a
−2r
3
if
r ≤a
=
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
−GM
r2
if
r ≥a,
−4πGρmr
3
if
r ≤a.

4.3 Applications: Triple Integrals
127
The ﬁrst result is the well-known fact that the ﬁeld outside a uniform sphere is the
same as the ﬁeld of a point charge with the same mass concentrated at the center
of the original sphere. The second result, usually obtained in electrostatics by using
Gauss’s law, would not have been obtained if we had not used absolute values when
extracting a square root.
■
Example 4.3.3. A uniformly charged hollow cylinder of length L and volume
charge density ρq has an inner radius a and an outer radius b (see Figure 4.16).
The cylinder is rotating with constant angular speed ω about its axis. We want
to ﬁnd the magnetic ﬁeld produced by this motion of charges. We note that the
problem has an azimuthal symmetry, so we do not lose generality if we choose our
coordinates so that our ﬁeld point P lies in the xz-plane. This is equivalent to setting
ϕ = 0.
We use cylindrical coordinates in Equation (4.17) to ﬁnd the magnetic ﬁeld. For
a general ﬁeld point, we have
r = ρ ˆeρ + zˆez,
r′ = ρ′ˆeρ′ + z′ˆez,
r −r′ = ρ ˆeρ −ρ′ˆeρ′ + (z −z′)ˆez,
|r −r′|3 =
,
ρ2 + ρ′2 −2ρρ′ cos ϕ′ + (z −z′)2-3/2,
dq(r′) = ρq dV (r′) = ρqρ′dρ′dϕ′dz′,
v(r′) = ρ′ωˆeϕ′,
so that
v(r′) × (r −r′) = ωρρ′ ˆeϕ′ × ˆeρ



−ˆez cos ϕ′
−ωρ′2 ˆeϕ′ × ˆeρ′



−ˆez
+ωρ′(z −z′) ˆeϕ′ × ˆez



ˆeρ′
= ωρ′(z −z′)ˆeρ′ + ω(ρ′2 −ρρ′ cos ϕ′)ˆez.
Substituting all these results in Equation (4.17), we obtain
B =
##
Ω
ωkm(ρqρ′dρ′dϕ′dz′)
.
ρ′(z −z′)ˆeρ′ + (ρ′2 −ρρ′ cos ϕ′)ˆez
/
,
ρ2 + ρ′2 −2ρρ′ cos ϕ′ + (z −z′)2-3/2
.
x
y
z
Figure 4.16: The charged rotating hollow cylinder produces a magnetic ﬁeld due to
the motion of charges.

128
Integration: Applications
The cylindrical components are obtained by dotting this equation with the cylindri-
cal unit vectors at P:
Bρ = B · ˆeρ = ωkmρq
# 2π
0
# b
a
# L/2
−L/2
ρ′2(z −z′) cos ϕ′dρ′dϕ′dz′
,
ρ2 + ρ′2 −2ρρ′ cos ϕ′ + (z −z′)2-3/2 ,
Bϕ = B · ˆeϕ = ωkmρq
# 2π
0
# b
a
# L/2
−L/2
ρ′2(z −z′) sin ϕ′dρ′dϕ′dz′
,
ρ2 + ρ′2 −2ρρ′ cos ϕ′ + (z −z′)2-3/2 = 0,
Bz = B · ˆez = ωkmρq
# 2π
0
# b
a
# L/2
−L/2
 
ρ′3 −ρρ′2 cos ϕ′!
dρ′dϕ′dz′
,
ρ2 + ρ′2 −2ρρ′ cos ϕ′ + (z −z′)2-3/2 .
The middle equation gives zero as a result of the ϕ′ integration.
It turns out
that the z′ and ρ′ integrations of the remaining integrals can be performed in
closed form.
However, the results are very complicated and will not be repro-
duced here. Furthermore, the ϕ′ integration has no closed form and must be done
numerically.
We can also obtain the components of B in other coordinate systems by dotting
B into the corresponding unit vectors. The reader may check, for example, that in
Cartesian coordinates, Bx = B · ˆex is the same as Bρ above and By is the same
as Bϕ, i.e., By = 0. This is due to the particular choice of our coordinate system
(ϕ = 0).
■
4.4
Problems
4.1. Diﬀerentiate Equation (4.2) to ﬁnd the velocity and acceleration and
compare with the expected results.
4.2. By choosing a coordinate system properly, write down the simplest para-
metric equation for the following curves. In each case specify the range of the
parameter you use:
(a) a rectangle of sides a and b, lying in the xy-plane with center at the origin
and sides parallel to the axes;
(b) an ellipse with semi-major and semi-minor axes a and b;
(c) a helix wrapped around a cylinder with an elliptical cross section of the
type described in (b); and
(d) a helix wrapped around a cone.
4.3. Assume that the parametric equations of a linear charge density are
x′ = f(t), y′ = g(t), z′ = h(t). By writing everything in Equation (4.3) in
Cartesian coordinates, show that
E =
# b
a
keΛ(t)
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2
3
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]243/2
×
 
[x −f(t)] ˆex + [y −g(t)] ˆey + [z −h(t)] ˆez
!
dt.
(4.36)

4.4 Problems
129
and that
Ex =
# b
a
keΛ(t)
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2
3
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]243/2 [x −f(t)] dt
Ey =
# b
a
keΛ(t)
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2
3
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]243/2 [y −g(t)] dt
(4.37)
Ez =
# b
a
keΛ(t)
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2
3
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]243/2 [z −h(t)] dt
and
Φ(x, y, z) =
# b
a
keΛ(t)
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2
3
[x −f(t)]2 + [y −g(t)]2 + [z −h(t)]241/2 dt
How is Λ(t) related to λ(r′)?
4.4. (a) Show that
ˆeρ · ˆex =
x
	
x2 + y2 ,
ˆeρ · ˆey =
y
	
x2 + y2 .
(b) Similarly, express ˆeϕ · ˆex and ˆeϕ · ˆey in Cartesian coordinates.
(c) Use (a), (b), and Equation (4.36) to ﬁnd the general expressions for Eρ and
Eϕ as integrals in Cartesian coordinates similar to the integrals of Equation
(4.37).
4.5. (a) Find the nine dot products of all Cartesian and spherical unit vectors
and express the results in terms of Cartesian coordinates.
(b) Use (a) and Equation (4.36) to ﬁnd general expressions for Er, Eθ, and
Eϕ as integrals in Cartesian coordinates similar to the integrals of Equation
(4.37).
4.6. Assume that the parametric equations of a linear charge density are
ρ′ = f(t), ϕ′ = g(t), z′ = h(t). By writing everything in Equation (4.3) in
cylindrical coordinates, show that Equation (4.11) holds and that
Eρ =
# b
a
keΛ(t)
	
[f ′(t)]2 + [f(t)]2[g′(t)]2 + [h′(t)]2
3
ρ2 + [f(t)]2 −2ρf(t) cos
 
ϕ −g(t)
!
+ [z −h(t)]243/2
×
.
ρ −f(t) cos
 
g(t) −ϕ
!/
dt
(4.38)
Eϕ = −
# b
a
keΛ(t)
	
[f ′(t)]2 + [f(t)]2[g′(t)]2 + [h′(t)]2
3
ρ2 + [f(t)]2 −2ρf(t) cos
 
ϕ −g(t)
!
+ [z −h(t)]243/2
(4.39)
× f(t) sin
 
g(t) −ϕ
!
dt

130
Integration: Applications
Ez =
# b
a
keΛ(t)
	
[f ′(t)]2 + [f(t)]2[g′(t)]2 + [h′(t)]2
3
ρ2 + [f(t)]2 −2ρf(t) cos
 
ϕ −g(t)
!
+ [z −h(t)]243/2
(4.40)
×
.
z −h(t)
/
dt
and
Φ =
# b
a
keΛ(t)
	
[f ′(t)]2 + f 2(t)[g′(t)]2 + [h′(t)]2
3
ρ2 + f 2(t) −2ρf(t) cos
 
ϕ −g(t)
!
+ [z −h(t)]241/2 dt
How is Λ(t) related to λ(r′)?
4.7. Use (4.11) to calculate Cartesian and spherical components of the electric
ﬁeld in terms of integrals in cylindrical variables similar to (4.38).
4.8. Use the cylindrical coordinates for the integration variables of Example
4.1.3, but calculate the Cartesian components of E.
4.9. A uniformly charged inﬁnitely thin circular ring of radius a has total
charge Q. Place the ring in the xy-plane with its center at the origin. Use
cylindrical coordinates.
(a) Find the electrostatic potential at P with cylindrical coordinates (ρ, ϕ, z)
in terms of a single integral.
(b) Find the analytic form of the potential if P is on the z-axis (evaluate the
integral).
(c) Find the potential at a point in the xy-plane a distance 2a from the origin.
Give your answer as a number times keQ/a.
4.10. Write a general formula for Φ(r) of a charged curve in spherical coor-
dinates.
4.11. A straight-line segment of length 2L is placed on the z-axis with its
midpoint at the origin. The segment has a linear charge density given by
λ(x, y, z) =
Q
|z| + a,
where Q and a are constants with a > 0. Find the electrostatic potential of
this charge distribution at a point on the x-axis in Cartesian coordinates.
4.12. Same as the previous problem, except that
λ(x, y, z) =
aQ
z2 + a2 .
Look up the integral in an integral table.
(a) Does anything peculiar happen at x = ±a?
Based on the integration
result? Based on physical intuition? Look at the result carefully and reconcile
any conﬂict.
(b) What is the potential when L →∞?

4.4 Problems
131
4.13. A segment of the parabola y = x2/a—with a a constant—extending
from x = 0 to x = L has a linear charge density given by
λ(x, y, z) =
λ0
	
1 + (2x/a)2 ,
where λ0 is a constant. Find the potential and the electric ﬁeld at the point
(0, 0, z). What are Φ and E at (0, 0, a/2)? Simplify your results as much as
possible.
4.14. A circular ring of radius a is uniformly charged with linear density λ.
(a) Find an expression for each of the three components of the electric ﬁeld
at an arbitrary point in space in terms of an integral in an appropriate coor-
dinate system. Evaluate the integrals whenever possible.
(b) Find the components of the ﬁeld at the point P shown in Figure 4.17.
Express your answers as a numerical multiple of keλ/a.
(c) Find the electrostatic potential at the point P shown in Figure 4.17. Ex-
press your answer as a numerical multiple of keλ.
For (b) and (c) you will need to evaluate certain integrals numerically.
4.15. Consider a uniform linear charge distribution in the form of an ellipse
with linear charge density λ. The semi-major and semi-minor axes of the el-
lipse are a and b, respectively. Use Cartesian coordinates and the parametric
equation of the ellipse.
(a) Write down the integrals that give the electric ﬁeld and the electric po-
tential at an arbitrary point P in space.
(b) Specialize to the case where P lies on the axis that is perpendicular to the
plane of the ellipse and passes through its center.
(c) Specialize (a) to the case where P lies on the line containing the minor
axis.
4.16. Consider a uniform linear charge distribution in the form of an ellipse
with linear charge density λ located in the xy-plane. The semi-major and
semi-minor axes of the ellipse are 2a and a, respectively.
(a) Write the Cartesian parameterization of the ellipse in terms of trigono-
metric functions.
2a
a
P
Figure 4.17: The ﬁgure for Problems 4.14 and 4.21.

132
Integration: Applications
(b) Write the integral that gives the Cartesian components of the electric ﬁeld
at an arbitrary point (x, y, z) in space.
(c) Specialize to the point (a, 2a, 2a), and write your answer as a numerical
multiple of keλ/a.
4.17. Consider a uniform linear charge distribution, with linear charge density
λ, in the form of an elliptical helix whose parametric equation is given by
x′ = a cos t,
y′ = b sin t,
z′ = ct
Use Cartesian coordinates.
(a) Write down the integrals that give the electric ﬁeld and the electric po-
tential at an arbitrary point P in space.
(b) Verify that when c = 0, you get the ﬁeld and potential of an ellipse (see
Problem 4.15).
(c) Verify that when c = 0 = b, you get the ﬁeld and potential of a straight
line segment.
(d) Verify that when c = 0 = b and a →∞, you get the ﬁeld of an inﬁnite
straight line.
4.18. Find the three components of the electric ﬁeld and the potential of Ex-
ample 4.1.2 when a = −L/2 and b = L/2. Approximate the three components
of the electric ﬁeld for the case where L >> x.
4.19. Derive all relations in Equations (4.20) and (4.21).
4.20. Figure 4.18 shows a hyperbola y =
√
x2 + a2. Only the segment be-
tween x = 0 and x = a is charged uniformly with linear density λ.
(a) Write the expression for E as an integral in Cartesian coordinates.
(b) Find the three components of E as integrals over x′.
(c) Making the substitution x′ = au, write each component as a numerical
multiple of keλ/a.
4.21. A circular ring of radius a is uniformly charged with linear density λ.
The ring rotates with angular speed ω about the axis perpendicular to the
plane of the ring, passing through its center.
a
a
x
Figure 4.18: The segment of the hypebola that is charged.

4.4 Problems
133
(a) Find an expression for each of the three components of the magnetic
ﬁeld at an arbitrary point in space in terms of an integral in an appropriate
coordinate system. Evaluate the integrals whenever possible.
(b) Find the components of the ﬁeld at the point P shown in Figure 4.17.
Express your answers as a numerical multiple of kmλω. (You will need to
evaluate some integrals numerically!)
4.22. An elliptical conducting ring of semi-major axis a and semi-minor axis
b carries a current I.
(a) Find an expression for each of the three Cartesian components of the
magnetic ﬁeld at an arbitrary point in space in terms of an integral in the
Cartesian coordinate system.
(b) Find an integral expression for the components of the ﬁeld at a point on
the line perpendicular to the ellipse that passes through its center.
4.23. Perform the integrals for Ex, Ey, and Ez of Example 4.2.1 when the
ﬁeld point is on the z-axis. Hint: You can get Ex and Ey without doing the
integrals.
4.24. Assume that the parametric equations of a current loop are x′ =
f(t), y′ = g(t), z′ = h(t). By writing everything in Equation (4.18) in Carte-
sian coordinates, show that
Bx(r) = kmI
# b
a
g′(t)
.
z −h(t)
/
−h′(t)
.
y −g(t)
/
3.
x −f(t)
/2 +
.
y −g(t)
/2 +
.
z −h(t)
/243/2 dt,
By(r) = kmI
# b
a
h′(t)
.
x −f(t)
/
−f ′(t)
.
z −h(t)
/
3.
x −f(t)
/2 +
.
y −g(t)
/2 +
.
z −h(t)
/243/2 dt,
Bz(r) = kmI
# b
a
f ′(t)
.
y −g(t)
/
−g′(t)
.
x −f(t)
/
3.
x −f(t)
/2 +
.
y −g(t)
/2 +
.
z −h(t)
/243/2 dt,
where a and b are the initial and ﬁnal values of the parameter t.
4.25. By writing everything in Equation (4.18) in cylindrical coordinates,
show that
Bρ = kmI
2
N1 dρ′ + ρ′N2 dϕ′ + ρ′ sin(ϕ′ −ϕ) dz′
,
ρ2 + ρ′2 −2ρρ′ cos(ϕ −ϕ′) + (z −z′)2-3/2
Bϕ = kmI
2 ρ′N1 dϕ′ −N2 dρ′ +
.
ρ −ρ′ cos(ϕ′ −ϕ)
/
dz′
,
ρ2 + ρ′2 −2ρρ′ cos(ϕ −ϕ′) + (z −z′)2-3/2
Bz = −kmI
2 ρ sin(ϕ′ −ϕ) dρ′ +
.
ρρ′ cos(ϕ′ −ϕ) −ρ′2/
dϕ′
,
ρ2 + ρ′2 −2ρρ′ cos(ϕ −ϕ′) + (z −z′)2-3/2
where
N1 ≡(z −z′) sin(ϕ′ −ϕ), N2 ≡(z −z′) cos(ϕ′ −ϕ)

134
Integration: Applications
2a
2a
P
a
Figure 4.19: The ﬁgure for Problem 4.28.
4.26. Derive Equation (4.27).
4.27. Derive Equation (4.29) from Equation (4.28).
4.28. A square of side 2a is uniformly charged with surface density σ.
(a) Find the electrostatic potential at an arbitrary point in space. Do one
of the integrals and express your answer in terms of a single integral in an
appropriate coordinate system.
(b) Find the potential at a point a distance a directly above the midpoint of
one of the sides as shown in Figure 4.19. Express your answer as a numerical
multiple of keσa.
4.29. The area in the xy-plane shown in Figure 4.21 is uniformly charged
with surface charge density σ. The equation of the parabolic boundary is
y = x2/a. Assume that the observation point (ﬁeld point) P is on the z-axis
at z = a.
(a) Derive the Cartesian components of the electric ﬁeld at P as double inte-
grals.
(b) Do the y′ integration ﬁrst and then the x′ integration to ﬁnd the compo-
nents of the electric ﬁeld. Write your answers as a numerical multiples of keσ.
You will need to evaluate certain integral(s) numerically.
4.30. Using cylindrical coordinates, ﬁnd the electrostatic ﬁeld of a uniformly
charged circular disk of charge density σ and radius a:
(a) at an arbitrary point in space;
(b) at an arbitrary point on the perpendicular axis of the disk; and
x
y
Figure 4.20: The region of the xy-plane that is charged.

4.4 Problems
135
− a
a
x
y
Figure 4.21: The shaded region is uniformly charged.
(c) at an arbitrary point in the plane of the disk.
(d) For (b), consider the case of inﬁnite radius and compare your result with
the inﬁnite rectangle discussed in introductory physics books and Example
4.2.3.
4.31. Figure 4.20 shows a region of the xy-plane that is uniformly charged
with surface charge density σ.
The boundary of the region is given in a
polar/cylindrical coordinate system by ρ = a cos(2ϕ) with −π/4 ≤ϕ ≤π/4.
We are interested in the electrostatic potential at a point P on the z-axis with
z = a.
(a) Write the position vector of P and P ′ (a typical source point) in cylindrical
coordinates. Now evaluate |r −r′|.
(b) Write the expression for dq(r′) in cylindrical coordinates.
(c) Write the expression for the potential Φ as a double integral in cylindrical
coordinates.
(d) Perform one of the integrations, and wrtie your ﬁnal answer as a single
integral.
(e) Find the value of the potential as a numerical multiple of keσa.
4.32. A cylindrical shell of radius a and length L is uniformly charged with
surface charge density σ. Using an appropriate coordinate system and axis
orientation:
(a) Find the electric ﬁeld at an arbitrary point in space.
(b) Now let the length go to inﬁnity and ﬁnd a closed-form expression for the
ﬁeld in (a). You will have to look up the integral in an integral table.
(c) Find the expression of the ﬁeld for a point outside and a point inside the
cylinder.
4.33. A uniformly charged disk of radius a and surface charge density σ is
inthe xy-plane with its center at the origin and is rotating about its perpen-
dicular axis with angular frequency ω.
(a) Find the cylindrical components of the magnetic ﬁeld produced at a point
P = (ρ, 0, z) as double integrals in cylindrical coordinates.
(b) Now assume that P is on the z-axis and ﬁnd the components of B by
performing all the integrals involved.

136
Integration: Applications
4.34. An electrically charged disk of radius a is rotating about its perpen-
dicular axis with angular frequency ω. Its surface charge density is given in
cylindrical coordinates by σ = (σ0/a2)ρ2, where σ0 is a constant.
(a) Find the Cartesian components of the magnetic ﬁeld produced at an ar-
bitrary point P = (ρ, 0, z) as double integrals in cylindrical coordinates.
(b) Now assume that P is on the z-axis and ﬁnd the components of B by
performing all the integrals involved.
4.35. Express the components of g of Example 4.2.4 in Cartesian and cylin-
drical coordinates in terms of integrals similar to Equation (4.33).
4.36. A conic surface of (maximum) radius a and half-angle α is uniformly
charged with surface density σ.
(a) Find the three components of the electric ﬁeld at a point on the cone’s axis
a distance r from its vertex. Express your answers in terms of single integrals
in an appropriate coordinate system.
(b) Find the components of the ﬁeld at r = a/
√
3 when α = π/6. By eval-
uating integrals numerically if necessary, express your answer as a numerical
multiple of keσ.
4.37. A cone with half-angle α, the distance of whose vertex from its circular
rim is L, is rotating with angular speed ω about its axis. Electric charge
is distributed uniformly on the cone with surface charge density σ. Use the
coordinate system appropriate for this geometry.
(a) Express the components of the magnetic ﬁeld produced at an arbitrary
point in space in terms of double integrals. Evaluate those components whose
integrals are easily done.
(b) Move the ﬁeld point to the axis of the cone, and write the components
of the ﬁeld in terms of single integrals. Evaluate the remaining components
whose integrals are easily done.
(c) Now assume that α = π/3, and express the magnitude of the ﬁeld on the
axis at a distance L from the vertex of the cone as a number times kmωσL.
4.38. A uniformly charged solid cylinder of length L, radius a, and total
charge q is rotated about its axis with angular speed ω. Find the magnetic
ﬁeld at a point on this axis.
4.39. Use cylindrical coordinates to calculate the gravitational ﬁeld of the
hemisphere of Example 4.3.1 at a point on the z-axis.
(a) Show that
gz = 2πGρm
(	
z2 + a2 −|z| −(a2 + z2)3/2 −|a −z|(a2 + z2 + az)
3z2
)
with the other components being zero.
(b) Simplify this expression for points outside (z < 0 and z > a), and inside
(0 < z < a).
(c) Using the result of (b), ﬁnd the gravitational ﬁeld of a hemisphere whose
ﬂat side points up.
(d) Add the results of (b) and (c) to ﬁnd the ﬁeld of a full sphere.

4.4 Problems
137
a
b
L
α
x
y
z
Figure 4.22: The segment of a cylinder with uniform charge density used in Problem
4.41.
4.40. Find the moment of inertia of a uniform solid cone of mass M and
half-angle α cut out of a solid sphere of radius a. What is the moment of
inertia of a whole solid sphere?
4.41. A solid cylinder of length L has a cross section which is in the shape
of a segment of an annular ring with outer radius b and inner radius a. It
is subtended by an angle α and is uniformly charged with total charge q
(Figure 4.22). Find the electric ﬁeld at:
(a) an arbitrary point in space; and
(b) a point on the axis of the ring.
(c) What is the answer to (b) if we have a complete ring?
(d) What is the answer to (a) if we have a complete ring that is inﬁnitely
long? Consider the three regions: ρ ≤a, a ≤ρ ≤b, and ρ ≥b.
4.42. Find the moment of inertia of the (incomplete) cylinder of the previous
problem about the perpendicular axis passing through the common center of
the inner and outer radii. Assume that the total mass is M. From this result
obtain the moment of inertia of a hollow as well as a solid cylinder.

Chapter 5
Dirac Delta Function
Paul Adrian Maurice Dirac, one of the most inventive mathematical physicists
of all time, co-founder of quantum theory, inventor of relativistic quantum
mechanics in the form of an equation which bears his name, predictor of the
existence of anti-matter, clariﬁer of the concept of spin, and contributor to the
unraveling of the mathematical diﬃculties associated with the quantization
of the general theory of relativity, came across the subject matter of this
chapter in his study of quantum mechanical scattering. In order to appreciate
the usefulness of this function, we shall start with an intuitive approach drawn
from electrostatics.
5.1
One-Variable Case
Consider a straight linear charge distribution of length L with uniform charge
density as shown in Figure 5.1(a). If the total charge of the line segment is q,
then the linear charge density will be λ = q/L. We are interested in the graph
of the function describing the linear density in the interval (−∞, +∞). As-
suming that the midpoint of the segment is x0 and its length L, we can easily
draw the graph of the function. This is shown in Figure 5.1(b). The graph
L
q/L
0
(a)
(b)
x
L
x0
Figure 5.1: (a) The charged line segment and (b) its linear density function.

140
Dirac Delta Function
is that of a function that is zero for values less than x0 −L/2, q/L for val-
ues between x0 −L/2 and x0 + L/2, and zero again for values greater than
x0 + L/2. Let us call this function λ(x). Then, we can write
λ(x) =
⎧
⎪
⎨
⎪
⎩
0
if
x < x0 −L/2,
q/L
if
x0 −L/2 < x < x0 + L/2,
0
if
x > x0 + L/2.
Now suppose that we squeeze the segment on both sides so that the length
shrinks to L/2 without changing the position of the midpoint and the amount
of charge. The new function describing the linear charge density will now be
λ(x, x0) = q
⎧
⎪
⎨
⎪
⎩
0
if
x < x0 −L/4,
2/L
if
x0 −L/4 < x < x0 + L/4,
0
if
x > x0 + L/4.
We have “factored out” q for later convenience. We have also introduced a
second argument to emphasize the dependence of the function on the mid-
point. Instead of one-half, we can shrink the segment to any fraction, still
keeping both the amount of charge and the midpoint unchanged. Shrinking
the size to L/n and renaming the function λn(x, x0) to reﬂect its dependence
on n, gives
λn(x, x0) = q
⎧
⎪
⎨
⎪
⎩
0
if
x < x0 −L/2n,
n/L
if
x0 −L/2n < x < x0 + L/2n,
0
if
x > x0 + L/2n,
which is depicted in Figure 5.2 for n = 10 as well as some smaller values of n.
The important property of λn(x, x0) is that its height increases at the same
time that its width decreases.
Instead of a charge distribution that abruptly changes from zero to some
ﬁnite value and just as abruptly drops to zero, let us consider a distribution
x0
x0
x0
x0
x0
Figure 5.2: The linear density function as the length shrinks.

5.1 One-Variable Case
141
that smoothly rises to a maximum value and just as smoothly falls to zero.
There are many functions describing such a distribution. For example,
λn(x, x0) = q
n
π e−n(x−x0)2
has a peak of q
	
n/π at x = x0 and drops to smaller and smaller values as
we get farther and farther away from x0 in either direction. This function is
plotted for various values of n in Figure 5.3. It is clear from the ﬁgure that
the “width” of the graph of λn(x, x0) gets smaller as n →∞.
In both cases λn(x, x0) is a true linear (charge) density in the sense that
its integral gives the total charge. This is evident in the ﬁrst case because
of the way the function was deﬁned. In the second case, once we integrate
λn(x, x0) from −∞to +∞, we also obtain the total charge q. The region of
integration extends over all real numbers in the second case because at every
point of the real line we have some nonzero charge. Furthermore, we can
extend the interval of integration over all the real numbers even for the ﬁrst
case, because the function vanishes outside the interval (x0−L/2n, x0+L/2n)
and no extra contribution to the integral arises. We thus write
# +∞
−∞
λn(x, x0) dx = q
for all such functions. It is convenient to divide by q and deﬁne new functions
δn(x, x0) by
δn(x, x0) ≡λn(x, x0)
q
–1
1
2
3
Figure 5.3: The Gaussian bell-shaped curve approaches the Dirac delta function as the
width of the curve approaches zero. The value of n is 1 for the dashed curve, 4 for the
heavy curve, and 20 for the light curve.

142
Dirac Delta Function
so that
δn(x, x0) =
⎧
⎪
⎨
⎪
⎩
0
if
x < x0 −L/2n,
n/L
if
x0 −L/2n < x < x0 + L/2n,
0
if
x > x0 + L/2n,
in the ﬁrst case, and
δn(x, x0) =
n
π e−n(x−x0)2
(5.1)
in the second case. Both these functions have the property that
# +∞
−∞
δn(x, x0) dx = 1,
(5.2)
i.e., their integral over all the real numbers is one, and, in particular, inde-
pendent of n.
Dirac delta
function deﬁned
Box 5.1.1. The Dirac delta function δ(x, x0) is deﬁned as
δ(x, x0) ≡lim
n→∞δn(x, x0)
(5.3)
and has the following property:
# +∞
−∞
δ(x, x0) dx = 1.
(5.4)
Equation (5.4) follows from the fact that the integral in (5.2) is independent
of n. The Dirac delta function has inﬁnite height and zero width at x0, but
these two undeﬁned quantities compensate for one another to give a ﬁnite area
under the “graph” of the function. The Dirac delta function is not a well-
behaved mathematical function as deﬁned in elementary textbooks because at
the only point that it is nonzero, it is inﬁnite! Nevertheless, this function has
been investigated rigorously in higher mathematics. For us, the Dirac delta
function is a convenient way of describing densities.
Although we have separated the arguments of the Dirac delta function
by a comma, the function depends only on the diﬀerence between the two
arguments. This becomes clear if we think of the Dirac delta function as the
limit of the exponential because the latter is a function of x−x0. We therefore
have the important relation
δ(x, x0) = δ(x −x0).
(5.5)
In particular, since the delta function becomes inﬁnite at x = x0, we have
δ(x, x0)



x=x0= δ(x −x0)



x=x0= δ(0) = ∞.
(5.6)

5.1 One-Variable Case
143
One can think of the last equality as an identity satisﬁed by the Dirac delta
function:
Box 5.1.2. The Dirac delta function is zero everywhere except at the point
which makes its argument zero, in which case the Dirac delta function is
inﬁnite.
Since the Dirac delta function is zero almost everywhere, we can shrink
the region of integration to a smaller interval. In fact,
# b
a
δ(x −x0) dx = 1
as long as x0 lies in the interval (a, b). If x0 is outside the interval, then the
integral will be zero because the delta function would always be zero in the
region of integration. We summarize these results:
Box 5.1.3. The Dirac delta function satisﬁes the following relation
# b
a
δ(x −x0) dx =
0
1
if a < x0 < b,
0
otherwise.
(5.7)
Equation (5.4) is a special case of this, because −∞< x0 < +∞for any
value of x0.
5.1.1
Linear Densities of Points
Any function λ(x) whose integral over all real numbers is one is called a linear
density function. The δn’s deﬁned above are such functions. If we multiply
linear density
function
a linear density function by a physical quantity Q, the result will be a linear
density for Q. In fact, this was how we arrived at δn. Thus, Q0λ(x) is a
Q-linear density with total magnitude Q0. Similarly, if M represents a mass,
then Mλ(x) is a linear mass density with total mass M. Conversely, if f(x)
describes the linear density of a physical quantity with total magnitude Q,
then λ(x) ≡f(x)/Q is a linear density function.
Because of Equation (5.4) the Dirac delta function is a linear density func-
tion. What kind of a distribution does it describe? To be speciﬁc, consider
δ function and
densities of point
charges and point
masses
mδ(x, x0) with m designating mass. This function is zero everywhere except
at x0. Thus, if it is to be a mass distribution, it has to be a point mass located
at x0. Keep in mind that mδ(x, x0) is a linear mass density, so that its integral
is the total mass m. The linear “density” of a point mass is inﬁnite because

144
Dirac Delta Function
its length is zero, and this is precisely what mδ(x, x0) describes. In fact, the
linear density of a point physical quantity of magnitude Q located at x0 can
be written as Qδ(x, x0) = Qδ(x −x0), or generalizing,
Box 5.1.4. The linear density λ(x) of N point physical quantities
Q1, Q2, . . . , QN located at x1, x2, . . . , xN, respectively, can be written as
λ(x) =
N

k=1
Qkδ(x −xk).
(5.8)
We see that with the help of the Dirac delta function we can express discrete
charge distributions (collection of point charges) in terms of functions. This
is the most useful property of the Dirac delta function.
Example 5.1.1. Three charges −q, 2q, and −q are located along the x-axis at
−a, the origin, and +a, respectively. How do we write the linear charge density for
such a charge distribution? We use Equation (5.8) with Q replaced by q:
λ(x) =
3

k=1
qkδ(x −xk) = −qδ(x −(−a)) + 2qδ(x −0) −qδ(x −a)
= −qδ(x + a) + 2qδ(x) −qδ(x −a).
Note that the Dirac delta functions ensure that no electric charge is present any
where except at x = a, x = −a, and x = 0.
■
Example 5.1.2. A more interesting example of a linear charge distribution using
the Dirac delta function is that of an inﬁnite array of point charges equally spaced
on a straight line having equal magnitudes and alternating in sign. This is a one-
dimensional model of ionic crystals.
Let us assume that the magnitude of each charge is ±q, the spacing between it
and the neighboring charge is a, and that the charges start at −∞and extend to
+∞with one positive charge at the origin as shown in Figure 5.4. Then it is easy
to write the density of this distribution. It is
density of
one-dimensional
ionic crystal
λ(x) =
+∞

k=−∞
(−1)kqδ(x −ka).
a
O
Figure 5.4: A one-dimensional ionic crystal. The black circles represent positive charges
and the white circles the negative charges.

5.1 One-Variable Case
145
Note that for odd k the charge is negative and for even k it is positive. This is
because we placed a positive charge at the origin.
Had we chosen the origin to
be the site of a negative charge, the above arrangement would have shifted by one
spacing.
■
5.1.2
Properties of the Delta Function
From a mathematical point of view, the most important property, which is
sometimes used to deﬁne the Dirac delta function, occurs when it multiplies a
“smooth”1 function in an integrand. First look at an integral with a δn(x−x0)
inside. If the function f(x) multiplying δn(x −x0) is smooth and n is large
enough, the product f(x)δn(x −x0) practically vanishes outside a narrow
interval in which δn(x −x0) is appreciably diﬀerent from zero. For example,
if n = 107, x = x0 + 0.001, and we use the exponential function of Equation
(5.1), then δn(x −x0) = 0.08, so that f(x)δn(x −x0) drops to about 8% of
the value it has at x0, assuming that f does not change appreciably in the
small interval of width 0.002 around x0. For larger values of n this drop is
even sharper. In fact, no matter what function we choose, there is always a
large enough n such that the product f(x)δn(x −x0) will drop to as small
a value as we please in as short an interval as we please. Therefore, we can
approximate the integral over all real numbers to an integral over that small
interval. Let the interval be (x0 −ϵ, x0 + ϵ). Then, we have
# +∞
−∞
f(x)δn(x −x0) dx ≈
# x0+ϵ
x0−ϵ
f(x)δn(x −x0) dx
≈f(x0)
# x0+ϵ
x0−ϵ
δn(x −x0) dx
≈f(x0)
# +∞
−∞
δn(x −x0) = f(x0).
The approximation in the second line follows from the fact that f(x) is almost
constant in the small interval (x0 −ϵ, x0 + ϵ). The third approximation is a
result of the smallness of δn outside the interval, and the equality follows
because δn is a linear density function.
The approximation above reaches
equality once the limit of n →∞is taken in which case δn becomes the Dirac
delta function. Thus, we have the important relation
# +∞
−∞
f(x)δ(x −x0) dx = f(x0).
(5.9)
1In the present context, a smooth function is one that does not change abruptly when
its argument changes by a small amount.

146
Dirac Delta Function
This is equivalent to the following statement:
integral of product
of δ(x −x0) and
f(x) is simply
f(x0)
Box 5.1.5. The Dirac delta function satisﬁes
# b
a
f(x)δ(x −x0) dx =
0
f(x0)
if
a < x0 < b,
0
otherwise.
(5.10)
In words, the result of integration is the value of f at the root of the
argument of the delta function, provided this root is inside the range
of integration.
Example 5.1.3. In this example we illustrate some of the properties of the
Dirac delta function.
For instance
 ∞
1
f(t)δ(t) dt = 0 because the root of the
argument of the Dirac delta function (the point that makes the argument of the
Dirac delta function zero)—namely t = 0—is outside the range of integration. The
integral
 +∞
−∞xδ(x) dx is zero because the function x vanishes at the point x = 0 (the
root of the argument of the delta function). Also,
# +3
−∞
cos yδ(y −π) dy = 0
because π—which makes the argument of the delta function vanish—lies outside the
range of integration. However,
# +3.2
−∞
cos yδ(y −π) dy = cos π = −1
because now π lies inside the range of integration.
The reader is urged to check the following results:
# +∞
−∞
cos yδ(y −π) dy = −1,
# +∞
−∞
sin zδ(z) dz = 0,
# +∞
0
cos yδ(y + π) dy = 0,
# +∞
−∞
cos y
2δ(y −π) dy = 0,
# 1
−1
etδ(t) dt = 1,
# +∞
−∞
xf(x)δ(x)dx = 0,
# 2.7
−∞
ln t δ(t −e) dt = 0,
# 2.8
−∞
ln t δ(t −e) dt = 1.
■
As noted earlier, the Dirac delta function is not an ordinary over-the-
counter function. Nevertheless, it is possible to study it, along with many
other “weird” functions called distributions, in a mathematically rigorous
distributions
and systematic way. It turns out that, in all physical applications, distribu-
tions occur inside an integral, and once they do, Equation (5.10) tells us how
to manipulate such integrals. The result of integration is always well deﬁned
because it is simply the value of a “good” function at a point, say x0. In fact,

5.1 One-Variable Case
147
the result of integration is so nice that one can even deﬁne the derivative of
the Dirac delta function by diﬀerentiating (5.10) with respect to x0. We leave
the details as an exercise and simply quote the result:
# +∞
−∞
f(x)δ′(x −x0) dx = −f ′(x0)
(5.11)
Higher order derivatives of the Dirac delta function can be obtained similarly.
derivatives of
Dirac delta
function
In fact, we have
Box 5.1.6. The nth derivative of the Dirac delta function satisﬁes
# b
a
f(x)δ(n)(x−x0)(x−x0) dx =
0
(−1)nf (n)(x0)
if a < x0 < b,
0
otherwise,
(5.12)
where the superscript (n) indicates the nth derivatives.
In many applications the argument of the Dirac delta function is not of
the simple form (x −x0), but may itself be a function g(x) whose deriva-
tive is assumed to be continuous in (a, b). Since by Equation (5.6) the delta
what happens
when the
argument of δ is
itself a function?
function vanishes except when its argument is zero, in such a case, one has
to concentrate on the roots of g(x), i.e., values c for which g(c) = 0. For
simplicity, ﬁrst assume that there is only one root c of g in the interval (a, b)
and that g′(c) > 0. Then, since the Dirac delta function is zero everywhere
in the interval (a, b), except at x = c, we can shrink the region of integration
to (c −ϵ, c + ϵ), and write
# b
a
δ (g(x)) dx =
# c+ϵ
c−ϵ
δ (g(x)) dx.
Now make the change of variable y = g(x), dy = g′(x) dx with the appropriate
transformation of limits of integration to get
# b
a
δ (g(x)) dx =
# g(c+ϵ)
g(c−ϵ)
δ(y) dy
g′(x).
With g(c) = 0 and g′(c) > 0, we conclude that g is increasing in the interval
(c −ϵ, c + ϵ), that g(c −ϵ) < 0, and that g(c + ϵ) > 0. We can therefore write
# b
a
δ (g(x)) dx =
# g(c+ϵ)
g(c−ϵ)
δ(y) dy
g′(x) =
1
g′(x)




y=0
=
1
g′(c) > 0,
because zero is in the region of integration and y = 0 is equivalent to x = c
there.

148
Dirac Delta Function
When g′(c) < 0, g will be decreasing in the interval (c −ϵ, c + ϵ), and
g(c −ϵ) > 0 and g(c + ϵ) < 0. Thus, ﬂipping the limits of integration so that
the smaller number corresponds to the lower limit, we obtain
# b
a
δ (g(x)) dx = −
# g(c−ϵ)
g(c+ϵ)
δ(y) dy
g′(x) = −
1
g′(x)




y=0
= −
1
g′(c) > 0.
We summarize the two results as
# b
a
δ (g(x)) dx =
1
|g′(c)|.
If there are two roots of g in the interval, say c1 and c2 with c2 > c1, we
break up (a, b):
# b
a
δ (g(x)) dx =
0



# c1−ϵ
a
δ (g(x)) dx +
# c1+ϵ
c1−ϵ
δ (g(x)) dx
+
0



# c2−ϵ
c1+ϵ
δ (g(x)) dx +
# c2+ϵ
c2−ϵ
δ (g(x)) dx
+
0



# b
c2+ϵ
δ (g(x)) dx =
1
|g′(c1)| +
1
|g′(c2)|,
where in the last line we used the result obtained in the previous paragraph.
It should be clear that if g has n roots c1, c2, . . . , cn in (a, b), there will be a
summation of n terms in the last line of the above equation. In fact, we can
summarize the result of the foregoing discussion as
Box 5.1.7. If g(x) has the roots c1, c2, . . . , cn, and g′(ck) ̸= 0 for all k
between 1 and n, then
# b
a
δ(g(x)) dx =
0n
k=1 1/|g′(ck)|
if a < ck < b,
0
otherwise.
(5.13)
When the delta function is multiplied by a smooth function f(x), a similar
argument as above—which is left to the reader—can be used to show that
# b
a
f(x)δ(g(x)) dx =
0n
k=1 f(ck)/|g′(ck)|
if a < ck < b,
0
otherwise,
(5.14)

5.1 One-Variable Case
149
provided g′(ck) ̸= 0. These results are sometimes written as an identity among
the delta functions.
a very important
relation
Box 5.1.8. The Dirac delta function satisﬁes the following relation:
δ(g(x)) =
n

k=1
δ(x −ck)
|g′(ck)| ,
g′(ck) ̸= 0,
(5.15)
where {ck}n
k=1 are all the roots of the equation g(x) = 0.
The formula analogous to Equation (5.14) involving the derivative of the Dirac
delta function is
# b
a
f(x)δ′(g(x)) dx =
0
−n
k=1 f ′(ck)/|g′(ck)|
if a < ck < b,
0
otherwise.
(5.16)
Example 5.1.4. As a concrete example, let us evaluate the integral
I ≡
# +∞
−∞
f(t)δ(t2 −a2) dt,
where f is a smooth function and a is a real constant. We can identify g(t) as t2 −a2
with roots c1 = −a, c2 = a and derivative g′(t) = 2t. Therefore, Equation (5.15)
reduces to
δ(t2 −a2) = δ(t −c1)
|g′(c1)| + δ(t −c2)
|g′(c2)| = δ(t −(−a))
| −2a|
+ δ(t −a)
|2a|
=
1
|2a| {δ(t + a) + δ(t −a)} .
Substituting in the integral, we obtain
I =
1
2|a|
# +∞
−∞
f(t) {δ(t + a) + δ(t −a)}
=
1
2|a|
(# +∞
−∞
f(t)δ(t + a) +
# +∞
−∞
f(t)δ(t −a)
)
=
1
2|a| {f(−a) + f(a)} .
Note that the integral vanishes—as expected—if f is odd.
■
Example 5.1.5. We illustrate further the foregoing general discussions with some
more concrete examples. To evaluate the integral
# ∞
1
sin t δ(t2 −π2/4) dt,

150
Dirac Delta Function
we note that g(t) = t2 −π2/4 which has two roots c1 = π/2 and c2 = −π/2 with
only the positive root lying in the range of integration. Moreover, g′(t) = 2t. Thus,
# ∞
1
sin t δ(t2 −π2/4) dt =
f(c1)
|g′(c1)| = sin(c1)
|2c1|
= sin(π/2)
π
= 1
π .
On the other hand,
# ∞
−∞
sin t δ(t2 −π2/4) dt = 0
because the second root c2 is also included in the range of integration and its con-
tribution cancels that of c1.
To evaluate the integral
# ∞
0
ln z δ(z2 −4) dz
we note that g(z) = z2 −4 which has two roots c1 = 2 and c2 = −2 with only the
positive root lying in the range of integration. Thus, with g′(z) = 2z, we have
# ∞
0
ln z δ(z2 −4) dz =
f(c1)
|g′(c1)| = ln(c1)
|2c1| = ln 2
4
= 0.1733.
The integral
# +∞
−∞
f(y) δ(y2 + a2) dy
is zero because there is no point in the range of integration at which the argument
of the Dirac delta function vanishes. In other words, g(y) = y2 + a2 has no real
roots at all.
To evaluate the integral
# +π/2
−π/2
(t + 1)2 δ(sin πt) dt
we note that g(t) = sin πt which has three roots c1 = −1, c2 = 0, and c3 = +1 in
the range of integration. Thus, with g′(t) = π cos πt, we have
# +π/2
−π/2
(t + 1)2 δ(sin πt) dt =
3

k=1
f(ck)
|g′(ck)| =
3

k=1
(ck + 1)2
|π cos(ckπ)|
= (−1 + 1)2
|π cos(−π)| + (0 + 1)2
|π cos(0)| + (1 + 1)2
|π cos(π)| = 5
π .
Some other concrete examples are:
# +∞
−∞
sin |t| δ(t2 −π2/4) dt = 2/π,
# +∞
−∞
cos x δ(x2 −π2) dx = −1/π,
# ∞
0
ln z δ(z2 −1) dz = 0,
# +3
−∞
cos y δ(y2 + π2) dy = 0,
# +π
−π
(t + 1)2 δ(sin πt) dt = 35/π,
# +∞
−∞
f(t) δ(et −1) dt = f(0),
# ∞
0
ln x δ(10x2 + 3x −1) dx = −0.23,
# +∞
−∞
f(t) δ(et) dt = 0.
The reader is urged to derive all the above relations.
■

5.1 One-Variable Case
151
Historical Notes
“Physical laws should have mathematical beauty.” This statement was Dirac’s re-
sponse to the question of his philosophy of physics, posed to him in Moscow in 1955.
He wrote it on a blackboard that is still preserved today.
Paul Adrien Maurice Dirac (1902–1984), was born in 1902 in Bristol, Eng-
land, of a Swiss, French-speaking father and an English mother.
His father, a
taciturn man who refused to receive friends at home, enforced young Paul’s silence
by requiring that only French be spoken at the dinner table. Perhaps this explains
Dirac’s later disinclination toward collaboration and his general tendency to be a
loner in most aspects of his life.
The fundamental nature of his work made the
involvement of students diﬃcult, so perhaps Dirac’s personality was well-suited to
his extraordinary accomplishments.
Dirac went to Merchant Venturer’s School, the public school where his father
taught French, and while there displayed great mathematical abilities. Upon grad-
uation, he followed in his older brother’s footsteps and went to Bristol University to
study electrical engineering. He was 19 when he graduated from Bristol University
in 1921. Unable to ﬁnd a suitable engineering position due to the economic reces-
“The amount of
theoretical ground
one has to cover
before being able
to solve problems
of real practical
value is rather
large, but this
circumstance is an
inevitable
consequence of
the fundamental
part played by
transformation
theory and is likely
to become more
pronounced in the
theoretical physics
of the future.”
P.A.M. Dirac
(1930)
sion that gripped post-World War I England, Dirac accepted a fellowship to study
mathematics at Bristol University. This fellowship, together with a grant from the
Department of Scientiﬁc and Industrial Research, made it possible for Dirac to go
to Cambridge as a research student in 1923. At Cambridge Dirac was exposed to
the experimental activities of the Cavendish Laboratory, and he became a member
of the intellectual circle over which Rutherford and Fowler presided. He took his
PhD in 1926 and was elected in 1927 as a fellow. His appointment as university
lecturer came in 1929.
He assumed the Lucasian professorship following Joseph
Larmor in 1932 and retired from it in 1969. Two years later he accepted a position
at Florida State University where he lived out his remaining years. The FSU library
now carries his name.
In the late 1920s the relentless march of ideas and discoveries had carried physics
to a generally accepted relativistic theory of the electron. Dirac, however, was dis-
satisﬁed with the prevailing ideas and, somewhat in isolation, sought for a better
formulation. By 1928 he succeeded in ﬁnding an equation, the Dirac equation, that
accorded with his own ideas and also ﬁtted most of the established principles of the
time. Ultimately, this equation, and the physical theory behind it, proved to be
one of the great intellectual achievements of the period. It was particularly remark-
able for the internal beauty of its mathematical structure, which not only clariﬁed
previously mysterious phenomena such as spin and the Fermi–Dirac statistics
associated with it, but also predicted the existence of an electron-like particle of
negative energy, the antielectron, or positron, and, more recently, it has come to
play a role of great importance in modern mathematics, particularly in the inter-
relations between topology, geometry, and analysis. Heisenberg characterized the
discovery of antimatter by Dirac as “the most decisive discovery in connection with
the properties or the nature of elementary particles . . . . This discovery of particles
and antiparticles by Dirac . . . changed our whole outlook on atomic physics com-
pletely.” One of the interesting implications of his work that predicted the positron
Paul Adrien
Maurice Dirac
1902–1984
was the prediction of a magnetic monopole. Dirac won the Nobel Prize in 1933 for
this work.
Dirac is not only one of the chief authors of quantum mechanics, but he is
also the creator of quantum electrodynamics and one of the principal architects of

152
Dirac Delta Function
quantum ﬁeld theory. While studying the scattering theory of quantum particles, he
invented the (Dirac) delta function; in his attempt at quantizing the general theory
of relativity, he founded constrained Hamiltonian dynamics, which is one of the most
active areas of theoretical physics research today. One of his greatest contributions
is the invention of the bra ⟨| and ket | ⟩notation used in quantum theory.
While at Cambridge, Dirac did not accept many research students. Those who
worked with him generally thought that he was a good supervisor, but one who
did not spend much time with his students.
A student needed to be extremely
independent to work under Dirac. One such student was Dennis Sciama, who later
became the supervisor of Stephen Hawking, the current holder of the Lucasian chair.
Salam and Wigner in their Preface to the Festschrift that honors Dirac on his
seventieth birthday and commemorates his contributions to quantum mechanics
succinctly assessed the man:
Dirac is one of the chief creators of quantum mechanics.... Posterity
will rate Dirac as one of the greatest physicists of all time. The present
generation values him as one of its greatest teachers.... On those privi-
leged to know him, Dirac has left his mark . . . by his human greatness.
He is modest, aﬀectionate, and sets the highest possible standards of
personal and scientiﬁc integrity. He is a legend in his own lifetime and
rightly so.
(Taken from Schweber, S. S. “Some chapters for a history of quantum ﬁeld theory:
1938–1952,” in Relativity, Groups, and Topology II, vol. 2, B. S. DeWitt and R.
Stora, eds., North-Holland, Amsterdam, 1984.)
5.1.3
The Step Function
The step function θ is deﬁned as
θ(x) =
0
1
if x > 0
0
if x < 0
(5.17)
The θ function (as it is often called) is useful in writing functions that have
discontinuities or cusps. For instance, absolute values can be written in terms
of the step function:
|x| = xθ(x) −xθ(−x)
or
|x −y| = (x −y)[θ(x −y) −θ(y −x)]
A piecewise continuous function such as
g(x) =
0
g1(x)
if 0 < x < 1
g2(x)
if x > 1
(5.18)
can be written as
g(x) = g1(x)θ(x)θ(1 −x) + g2(x)θ(x −1)

5.1 One-Variable Case
153
Because θ is constant everywhere except at 0, its derivative is zero ev-
erywhere except at 0. The discontinuity at 0 makes the derivative inﬁnite
there:
step function and
its relation to
delta function
θ′(0) = lim
ϵ→0
θ(ϵ) −θ(−ϵ)
2ϵ
= lim
ϵ→0
1 −0
2ϵ
→∞
This strongly suggests the identiﬁcation of the derivative of the step function
as the Dirac delta function. In fact, noting that
θ(x −x0) =
0
1
if x > x0
0
if x < x0,
(5.19)
and the fact that θ′(x −x0) is zero everywhere except at x0, for any well-
behaved function f(x) we obtain
# ∞
−∞
f(x)θ′(x −x0) dx =
# x0+ϵ
x0−ϵ
f(x)θ′(x −x0) dx ≈f(x0)
# x0+ϵ
x0−ϵ
θ′(x −x0) dx
= f(x0) θ(x −x0)|x0+ϵ
x0−ϵ = f(x0)[θ(ϵ)

=1
−θ(−ϵ)
  
=0
] = f(x0)
We thus have another important representation of the Dirac delta function:
δ(x −x0) = θ′(x −x0)
(5.20)
Example 5.1.6. For positive a, tanh(ax) goes to 1 as x →∞and to −1 as
x →−∞and it makes a smooth transition from one of these asymptotic values to
the other. This transition gets steeper and steeper for larger and larger values of a.
This suggests the following relation:
θ(x −x0) = 1
2 lim
a→∞{1 + tanh[a(x −x0)]}
Let θa(x −x0) stand for the function on the right-hand side for any ﬁnite positive
a. Then
θ′
a(x −x0) = 1
2
d
dx {1 + tanh[a(x −x0)]} = a sech2[a(x −x0)]
2
and
# ∞
−∞
θ′
a(x −x0) dx = θa(x −x0)|∞
−∞= 1
2 {1 + tanh[a(x −x0)]}|∞
−∞= 1
for any value of a > 0, in particular for a →∞. Thus, we get yet another represen-
tation of the Dirac delta function:
δ(x −x0) = lim
a→∞θ′
a(x −x0) = lim
a→∞
a sech2[a(x −x0)]
2
■

154
Dirac Delta Function
5.2
Two-Variable Case
We can generalize the discussion of the previous section to the case of many
variables. For example, in two dimensions using Cartesian coordinates, we
can deﬁne the functions δn as
δn(x −x0, y −y0) = Ce−n
.
(x−x0)2+(y−y0)2/
= Ce−n(x−x0)2e−n(y−y0)2, (5.21)
where C is a constant to be determined in such a way as to make the integral
of δn over the entire xy-plane equal to one. A simple calculation will show
that C = n/π. This constant is simply the product of two “one-dimensional
constants”: one for the exponential in x and the other for the exponential in y.
This is as expected, because δn(x −x0, y −y0) is deﬁned to be the product of
two one-dimensional δn’s. Such a simplicity is the result of the coordinate sys-
tems we have used and does not prevail in other—non-Cartesian—coordinate
systems, for which the constant C must be evaluated separately.
It should be clear from (5.21) that as n increases, the height of δn at
(x0, y0) increases while its width decreases (see Figure 5.5). What may not be
clear is that this reciprocal behavior takes place in such a way as to keep the
volume under the surface equal to one. We can deﬁne—as we did in the one
dimensional case—a surface density function as a function whose integral
surface density
function
over the entire plane is one. For any n, then, δn will be a surface density
function.
The passage to the two-dimensional Dirac delta function is now clear:
two-dimensional
Dirac delta
function
δ(x −x0, y −y0) ≡lim
n→∞δn(x −x0, y −y0).
(5.22)
The two-dimensional Dirac delta function above is zero everywhere except at
(x0, y0) where it is inﬁnite. Thus for the Dirac delta function not to be zero
both of its arguments must be zero. It is convenient to deﬁne points P and P0
Figure 5.5: As n gets larger and larger, the two-dimensional Gaussian exponential
approaches the two-dimensional Dirac delta function. For the left bump, n = 400; for
the middle, n = 1000; and for the right spike n = 4000.

5.2 Two-Variable Case
155
with respective Cartesian coordinates (x, y) and (x0, y0), and position vectors
r = ⟨x, y⟩, r0 = ⟨x0, y0⟩, and write
δ(x −x0, y −y0) ≡δ(r −r0) =
0
δ(⃗0) ≡δ(0, 0) = ∞
if
r = r0,
0
otherwise.
(5.23)
This means
Box 5.2.1. The two-dimensional Dirac delta function is zero everywhere
except at the point which makes both of its arguments zero, in which case
the two-dimensional Dirac delta function is inﬁnite.
We noted above that in Cartesian coordinates—and only in Cartesian
coordinates—the product of two one-dimensional δn’s gave rise to a two-
dimensional δn which subsequently yielded the two-dimensional Dirac delta
function. Thus only in Cartesian coordinates can we conclude that
δ(r −r0) = δ(x −x0, y −y0) = δ(x −x0) δ(y −y0).
(5.24)
We shall see that in polar coordinates, the two-dimensional delta function
is not merely the product of two one-dimensional delta functions, but some
other factor is also present.
The density property of the two-dimensional Dirac delta function survives
the n →∞process because the integral of δn is independent of n. On the
other hand, the delta function is zero everywhere except at the point which
makes both of its arguments zero. Therefore, for any two-dimensional region
Ω, we have
##
Ω
δ(r −r0) da(r) =
0
1
if
P0 is in Ω,
0
otherwise.
(5.25)
Equation (5.25) is written independently of coordinates, and as such, the
vector arguments are to be interpreted as coordinates not components. We
can use this equation in polar coordinates to write the two-dimensional Dirac
delta function as a product of two one-dimensional delta functions.
First
write2
δ(r −r0) = Cδ(ρ −ρ0)δ(ϕ −ϕ0).
2We use ρ and ϕ instead of the more common r and θ because we have reserved the
latter for the three-dimensional spherical coordinates. There is no danger of confusing the
pair (ρ, ϕ) with the corresponding pair in cylindrical coordinates because the two pairs are
identical.

156
Dirac Delta Function
Now substitute this in Equation (5.25) with Ω being the entire plane, and
note that da = ρ dρ dϕ:
1 =
##
Ω
Cδ(ρ −ρ0)δ(ϕ −ϕ0) ρ dρ dϕ
= C
# ∞
0
δ(ρ −ρ0)ρ dρ
# 2π
0
δ(ϕ −ϕ0) dϕ



=1
= Cρ0 ⇒C = 1
ρ0
.
In the above derivation, we have used properties of the one-dimensional delta
function as applied to δ(ρ −ρ0) and δ(ϕ −ϕ0).
Box 5.2.2. The two-dimensional Dirac delta function can be written in
polar coordinates as
δ(r −r0) = 1
ρ0
δ(ρ −ρ0)δ(ϕ −ϕ0) = 1
ρδ(ρ −ρ0)δ(ϕ −ϕ0).
(5.26)
The last equality follows because the Dirac delta function in ρ forces ρ and
ρ0 to be equal.
A collection of point physical quantities Q1, Q2, . . . , Qn located on a sur-
face can be described by a surface density σQ(r) using the two-dimensional
Dirac delta function:
surface density
and
two-dimensional
delta function
σQ(r) =
n

k=1
Qk δ(r −rk),
(5.27)
where rk is the position vector of Qk. This equation can be rewritten as
σQ(x, y) =
n

k=1
Qk δ(x −xk)δ(y −yk)
in Cartesian coordinates, and as
σQ(ρ, ϕ) =
n

k=1
Qk
ρk
δ(ρ −ρk)δ(ϕ −ϕk) = 1
ρ
n

k=1
Qk δ(ρ −ρk)δ(ϕ −ϕk)
in polar coordinates.
Example 5.2.1. With an appropriate choice of the origin and the axes of a Carte-
sian coordinate system, the surface charge density for four charges q1, q2, q3, q4 lo-
cated at the four corners of a square of sides 2a can be written as
σq(x, y) =
4

k=1
qkδ(x −xk)δ(y −yk)
= q1δ(x −a)δ(y −a) + q2δ(x + a)δ(y −a)
+ q3δ(x + a)δ(y + a) + q4δ(x −a)δ(y + a).

5.2 Two-Variable Case
157
If polar coordinates are used, the surface charge density becomes
σq(ρ, ϕ) =
4

k=1
qk
ρk δ(ρ −ρk)δ(ϕ −ϕk)
= δ(ρ −
√
2 a)
√
2 a
3
q1δ(ϕ −π/4) + q2δ(ϕ −3π/4)
+ q3δ(ϕ −5π/4) + q4δ(ϕ −7π/4)
4
.
The reader is urged to study these two equations carefully and make sure to under-
stand the details of their derivation.
■
A more interesting example is the two-dimensional ionic crystal.
Example 5.2.2. Suppose positive and negative charges ±q are arranged on an
inﬁnite square grid in such a way that the nearest neighbors of each charge have
charges of opposite sign, i.e., charges alternate both horizontally and vertically (see
Figure 5.6). Assume that the distance between each charge and its nearest neighbor
is a, and that we place our Cartesian origin at the location of a positive charge.
Then the surface charge density can be written as
two-dimensional
ionic crystal
σq(x, y) = q
∞

i=−∞
∞

j=−∞
(−1)i+jδ(x −ia)δ(y −ja).
For a ﬁnite 2M × 2N grid one substitutes the ﬁrst inﬁnity with M and the second
one with N. Similarly, one can consider rectangular units of sides a and b for the
grid. Then one should change the second argument of the delta function (or the
argument of the delta function corresponding to y) to y −jb.
■
With an extra dimension at our disposal, we can invent many new vari-
eties of distribution of point physical quantities that were not possible in one
dimension. For example, we can put the points on a curve in the xy-plane.
It is instructive to ﬁnd the surface density of such a collection of points. The
following example examines this problem.
Figure 5.6: A two-dimensional ionic crystal.

158
Dirac Delta Function
x
y
Figure 5.7: Point charges located on a curve in the xy-plane.
Example 5.2.3. For concreteness, we consider n point charges located at n points
{Pk}n
k=1 with Pk having Cartesian coordinates (xk, yk). These points are assumed
to be on a curve with the Cartesian equation y = f(x) as shown in Figure 5.7. The
surface charge density in Cartesian coordinates becomes
σq(x, y) =
n

k=1
qkδ(x −xk)δ(y −yk) =
n

k=1
qkδ(x −xk)δ

y −f(xk)

.
If the curve is given as ρ = g(ϕ), then polar coordinates are more appropriate,
and the surface charge density will be3
σq(ρ, ϕ) =
n

k=1
qk
ρk
δ(ρ −ρk)δ(ϕ −ϕk) =
n

k=1
qk
g(ϕk) δ(ρ −g(ϕk)) δ(ϕ −ϕk).
For instance, if the charges are located on a circle of radius a each separated from
its nearest neighbor by an angle α, with the ﬁrst charge on the x-axis, then
σq(ρ, ϕ) = δ(ρ −a)
a
n

k=1
qkδ(ϕ −(k −1)α),
where we have used the fact that g(ϕ) = a for a circle of radius a.
■
All the properties of the delta function can be generalized to two dimen-
sions. One important property is given in Equation (5.10).
Box 5.2.3. Let Ω be a region in the xy-plane and P0 a point there; then
##
Ω
f(r)δ(r −r0) da =
0
f(r0) ≡f(x0, y0)
if P0 is in Ω,
0
otherwise,
(5.28)
where (x0, y0) are the Cartesian coordinates of P0.
3Because of the two delta functions, one can substitute ρ for ρk and ϕ for ϕk in the
denominators.

5.3 Three-Variable Case
159
Diﬀerentiating both sides with respect to the ﬁrst argument x0, we easily
obtain the analog of Equation (5.12):
##
Ω
f(r)∂1δ(r −r0) da =
0
−∂1f(r0) ≡−∂1f(x0, y0)
if P0 is in Ω,
0
otherwise,
with a similar relation for diﬀerentiation with respect to the second argument.
We can combine the two relations into a single relation:
Box 5.2.4. The derivative of the Dirac delta function in two dimensions
satisﬁes
##
Ω
f(r)∂iδ(r −r0) da =
0
−∂if(r0) ≡−∂if(x0, y0)
if P0 is in Ω,
0
otherwise,
where i can be 1 or 2, ∂1 = ∂x and ∂2 = ∂y.
5.3
Three-Variable Case
Once the generalization to two variables is realized, the three—and more—
variable cases become trivial. In fact, we had such generalizations in mind
when we wrote most of the formulas in the last section: All that is needed
is to change da to dV and keep in mind that the vectors r and r0 have
three components, and points in space have three coordinates. Nevertheless,
we shall summarize the most important properties of the three-dimensional
Dirac delta function.
First we note that
δ(r −r0) =
0
δ(⃗0) ≡δ(0, 0, 0) = ∞
if r = r0,
0
otherwise.
(5.29)
This means
Box 5.3.1. The three-dimensional Dirac delta function is zero everywhere
except at the point which makes all three of its arguments zero in which
case it is inﬁnite.
In Cartesian coordinates, we have
3D Dirac delta
function in
Cartesian
coordinates
δ(r −r0) = δ(x −x0, y −y0, z −z0)
= δ(x −x0) δ(y −y0) δ(z −z0).
(5.30)

160
Dirac Delta Function
An argument similar to the two-dimensional case can be used to show that
3D Dirac delta
function in
cylindrical and
spherical
coordinates
Box 5.3.2. In cylindrical coordinates
δ(r −r0) = 1
ρ0
δ(ρ −ρ0)δ(ϕ −ϕ0)δ(z −z0),
(5.31)
where r and r0 on the LHS are to be understood as cylindrical coordi-
nates, not cylindrical position vectors. The corresponding formula for the
spherical coordinate system is
δ(r −r0) =
1
r2
0 sin θ0
δ(r −r0)δ(θ −θ0)δ(ϕ −ϕ0),
(5.32)
with r and r0 representing the coordinates (r, θ, ϕ) and (r0, θ0, ϕ0), re-
spectively.
The density property of the three-dimensional Dirac delta function is given
by
##
Ω
δ(r −r0) dV (r) =
0
1
if P0 is in Ω,
0
otherwise,
(5.33)
where Ω is a region of space and P0 is the point with Cartesian coordi-
nates (x0, y0, z0), spherical coordinates (r0, θ0, ϕ0), and cylindrical coordinates
(ρ0, ϕ0, z0). Similarly,
Box 5.3.3. If Ω is a region of space, then for a “good” function f(r),
##
Ω
f(r)δ(r −r0) dV (r) =
0
f(r0)
if P0 is in Ω,
0
otherwise.
Thus integration reduces to the evaluation of the function f at the coordi-
nates of P0.
The density property allows us to write the distribution of discrete physical
quantities in terms of the three-dimensional Dirac delta function. In general,
volume density
and 3D delta
function
ρQ(r) =
n

k=1
Qkδ(r −rk)
(5.34)
which can be rewritten as
ρQ(x, y, z) =
n

k=1
Qkδ(x −xk)δ(y −yk)δ(z −zk)

5.3 Three-Variable Case
161
in Cartesian coordinates, as
ρQ(ρ, ϕ, z) =
n

k=1
Qk
ρk
δ(ρ −ρk)δ(ϕ −ϕk)δ(z −zk)
in cylindrical coordinates, and as
ρQ(r, θ, ϕ) =
n

k=1
Qk
r2
k sin θk
δ(r −rk)δ(θ −θk)δ(ϕ −ϕk)
in the spherical coordinate system. In fact, the linear and surface distributions
of a physical quantity involving the Dirac delta function are special cases of
the volume distribution. For instance, a collection of point quantities in the
xy-plane can be described by the volume density
ρQ(x, y, z) =
n

k=1
Qkδ(x −xk)δ(y −yk)δ(z)
= δ(z)
n

k=1
Qkδ(x −xk)δ(y −yk).
The delta function outside the sum restricts the z-coordinates of point quan-
tities to zero, and thus their location, to the xy-plane. Similarly,
ρQ(r, θ, ϕ) = δ(r −a)
a2
n

k=1
Qk
sin θk
δ(θ −θk)δ(ϕ −ϕk)
describes a distribution of n point quantities on a sphere of radius a.
Example 5.3.1. Let us calculate the electrostatic ﬁeld of the one-dimensional
inﬁnite ionic crystal in Cartesian coordinates. Assume that the charges are located
on the z-axis (Figure 5.8). We treat this as a three-dimensional charge distribution
with density
ρq(x, y, z) = q
∞

k=−∞
(−1)k δ(x)δ(y)δ(z −ka).
(5.35)
x
y
z
P
Figure 5.8: The geometry for the calculation of the electrostatic ﬁeld of the one-
dimensional ionic crystal.

162
Dirac Delta Function
The ﬁrst two delta functions restrict the charges to the z-axis and the third locates
them.
This density is to be substituted in the equation for the electric ﬁeld in
Cartesian coordinates. Let us concentrate on the x-component
Ex(x, y, z) = ke
##
Ω
ρq(x′, y′, z′)(x −x′) dx′ dy′ dz′
{(x −x′)2 + (y −y′)2 + (z −z′)2}3/2 .
We can always take Ω to be the entire space because the delta function will restrict
the integration to the region of charges automatically.
We can also choose our
coordinate system so that the ﬁeld point lies in the xz-plane, i.e., y = 0. Note that
we have to prime all the arguments of ρq before we substitute it in the integral.
Having done this, we obtain
Ex(x, y, z) = keq
∞

k=−∞
(−1)k
##
Ω
(x −x′)δ(x′)δ(y′)δ(z′ −ka) dx′ dy′ dz′
{(x −x′)2 + y′2 + (z −z′)2}3/2
.
Using Box 5.3.3, noting that
f(x′, y′, z′) =
(x −x′)
{(x −x′)2 + y′2 + (z −z′)2}3/2 ,
and that the result of integration is the evaluation of f at x′ = 0 = y′, z′ = ka, we
obtain
Ex(x, y, z) = keq
∞

k=−∞
(−1)k
x
{x2 + (z −ka)2}3/2
= keq
−1

k=−∞
(−1)k
x
{x2 + (z −ka)2}3/2
+ keq
x
(x2 + z2)3/2 + keq
∞

k=+1
(−1)k
x
{x2 + (z −ka)2}3/2 ,
where we have broken up the summation into three pieces, a permissible act as long
as the series converges. We can combine the ﬁrst and third terms by changing k to
−k in the ﬁrst and noting that
(−1)−k =
1
(−1)k = (−1)k.
Doing so, we get
Ex(x, 0, z) = keq
x
(x2 + z2)3/2
+ keq
∞

k=1
(−1)k

x
{x2 + (z + ka)2}3/2 +
x
{x2 + (z −ka)2}3/2

.
The other components of the ﬁeld can be found similarly:
Ey(x, 0, z) = 0,
Ez(x, 0, z) = keq
z
(x2 + z2)3/2
(5.36)
+ keq
∞

k=1
(−1)k

z + ka
{x2 + (z + ka)2}3/2 +
z −ka
{x2 + (z −ka)2}3/2

.

5.3 Three-Variable Case
163
Let us further simplify the problem by positioning the ﬁeld point on the x-axis,
i.e., setting z = 0. This reduces the above expressions to
Ex(x, 0, 0) = keq x
|x|3 + 2keq
∞

k=1
(−1)k
x
(x2 + k2a2)3/2 ,
Ey(x, 0, 0) = 0,
Ez(x, 0, 0) = keq
∞

k=1
(−1)k

ka
{x2 + (ka)2}3/2 +
−ka
{x2 + (−ka)2}3/2

= 0.
At a distance a from the origin on the x-axis, the ﬁeld strength is
Ex(x, 0, 0) = keq
a2
0
1 + 2
∞

k=1
(−1)k
(1 + k2)3/2



=−0.286269
1
= 0.42746keq
a2 ,
Ey(x, 0, 0) = 0,
Ez(x, 0, 0) = 0,
where the numerical value for the sum—accurate to six decimal places—is obtained
by adding its ﬁrst 150 terms.
Another useful quantity is the electrostatic potential which for an arbitrary
charge distribution is given by
Φ(r) = ke
##
Ω
dq(r′)
|r −r′|.
(5.37)
For the one-dimensional crystal, with the volume charge density of Equation (5.35),
the electrostatic potential at an arbitrary point (x, y, z) in space becomes
Φ(x, y, z) = ke
##
Ω
ρq(x′, y′, z′) dx′ dy′ dz′
	
(x −x′)2 + (y −y′)2 + (z −z′)2
= keq
∞

k=−∞
(−1)k
##
Ω
δ(x′)δ(y′)δ(z′ −ka) dx′ dy′ dz′
	
(x −x′)2 + (y −y′)2 + (z −z′)2
= keq
∞

k=−∞
(−1)k
	
x2 + y2 + (z −ka)2 .
If we are interested in the potential at a speciﬁc point such as (x, 0, 0), the
expression simpliﬁes to
Φ(x, 0, 0) = keq
∞

k=−∞
(−1)k
√
x2 + k2a2
= keq
1
√
x2 + 2keq
∞

k=1
(−1)k
√
x2 + k2a2
= keq
|x| + 2keq
∞

k=1
(−1)k
√
x2 + k2a2 .

164
Dirac Delta Function
For x = a, this further simpliﬁes to
Φ(a, 0, 0) = keq
a
0
1 + 2
∞

k=1
(−1)k
√
1 + k2



=−0.4409
1
= 0.1182keq
a .
We note that the potential is positive, because the ﬁeld point is closest to the positive
charge at the origin. To obtain the numerical value of the sum accurate to only four
decimal places, we have to add at least 40,000 terms! This sum is, therefore, much
less convergent than the sum encountered in the evaluation of Ex above.
■
An important physical quantity for real crystals is the potential energy U
of the crystal. Physically, this is the amount of energy required to assemble
the charges in their ﬁnal conﬁguration. A positive potential energy corre-
sponds to positive energy stored in the system, i.e., a tendency for the system
to provide energy to the outside, once disrupted slightly from its equilibrium
position. A negative potential energy is a sign of the stability of the system,
i.e., the tendency for the system to restore its original conﬁguration if dis-
rupted slightly from its equilibrium position.4 It is shown in electrostatics
that the potential energy of a system located within the region Ω is
U = 1
2
##
Ω
dq(r)Φ(r).
(5.38)
Example 5.3.2. Let us calculate the electrostatic potential energy of the one-
electrostatic
potential energy of
a one-dimensional
crystal
dimensional crystal. Let us assume that there are a total of 2N +1 charges stretching
from z = −Na to z = +Na with a positive charge at the origin. Eventually we
shall let N go to inﬁnity, but, in order not to deal explicitly with inﬁnities, we
assume that N is ﬁnite but large. Substituting in (5.38) the element of charge in
terms of volume density, and electrostatic potential found in the previous example,
we ﬁnd
U = 1
2
##
Ω
ρq(x, y, z)Φ(x, y, z) dx dy dz
= 1
2
##
Ω
0
q
N

j=−N
(−1)j δ(x)δ(y)δ(z −ja)
1
×
0
keq
N

k=−N
(−1)k
	
x2 + y2 + (z −ka)2
1
dx dy dz
= keq2
2
N

j=−N
N

k=−N
k̸=j
(−1)j+k
	
(ja −ka)2 .
4A system that has negative potential energy requires some positive energy (such as
kinetic energy of a projectile) to reach a state of zero potential energy corresponding to
dissociation of its parts and their removal to inﬁnity (where potential energy is zero).

5.3 Three-Variable Case
165
The restriction k ̸= j is necessary, because the k = j terms correspond to the
interaction energy of each charge with itself, and should be excluded. Continuing
with the calculation, we write
U = keq2
2a
N

j=−N
N

k=−N
k̸=j
(−1)j+k
|j −k|
= keq2
2a
N

j=−N
⎧
⎨
⎩
j−1

k=−N
(−1)j+k
j −k
+
N

k=j+1
(−1)j+k
k −j
⎫
⎬
⎭.
In the ﬁrst inner sum, let j −k = m, and in the second let k −j = m. These
substitutions change the limits of the sums, and we get
U = keq2
2a
N

j=−N
0
1

m=N+j
(−1)2j−m
m
+
N−j

m=1
(−1)2j+m
m
1
= keq2
2a
N

j=−N
0N+j

m=1
(−1)m
m
+
N−j

m=1
(−1)m
m
1
.
To evaluate the inner sums, denoted by S, we now assume that N is very large—
compared to j—so that N −j ≈N ≈N + j. Then the inner sum yields5
S =
N+j

m=1
(−1)m
m
+
N−j

m=1
(−1)m
m
≈
N

m=1
(−1)m
m
+
N

m=1
(−1)m
m
= 2
N

m=1
(−1)m
m
≈−2
∞

m=1
(−1)m+1
m
= −2 ln 2.
Substituting S in the expression for U, we get
U ≈keq2
2a
N

j=−N
(−2 ln 2) = −keq2
a
ln 2
N

j=−N
1 = −(2N + 1)keq2
a
ln 2.
The negative sign indicates that the one-dimensional salt crystal is stable. A useful
quantity used in solid-state physics is ionization energy per molecule which is deﬁned
to be the potential energy divided by the number of molecules. Noting that the
number of molecules is half the number of particles, we obtain
u ≡U/N = −2N + 1
N
keq2
a
ln 2 ≈−keq2
a
2 ln 2 ≡−αkeq2
a
.
A real three-dimensional salt crystal has exactly the same expression. However,
Madelung
constant
the constant α, called the Madelung constant has the value of 1.747565 instead
of 2 ln 2 = 1.386294. (See Problem 5.17 for an alternative way of calculating the
potential energy of the one-dimensional ionic crystal.)
■
5We are really cheating here! The sum over j indicates that j can assume values close to
N, and therefore, the approximation is not valid for such j’s. However, a careful analysis,
in which one breaks up the sum over j and separates large and small values of j, shows that
the original approximation is valid as long as N is large enough.

166
Dirac Delta Function
5.4
Problems
5.1. Plot the distribution on the real line of each of the following electric
linear charge densities:
(a) λ(x) = δ(x −2).
(b) λ(x) = −δ(x + 1).
(c) λ(x) = 5δ(x) −3δ(x + 3).
(d) λ(x) = δ(x + 1) + 3δ(x −1).
5.2. Evaluate the following integrals:
(a)
# ∞
0
ex sin πx
2 δ(x2 −1) dx.
(b)
# 2
−2
ex sin πx
2 δ(x2 −1) dx.
(c)
# ∞
0
ex sin πx
2 δ(x3 + 1) dx.
(d)
# ∞
−∞
sin
πex
2

δ(x4 + 1) dx.
(e)
# ∞
0
sin−1(1/x)δ(x4 −1) dx.
(f)
# ∞
−∞
cos(πx)δ(6x2 −x −1) dx.
(g)
# ∞
−0.1
sin
πex
2

δ(x2 + x) dx.
(h)
# ∞
−∞
ex sin πx
2 δ(ex sin πx
2 ) dx.
(i)
# 5
0
esin xδ(cos x) dx.
(j)
# ∞
0
sin−1
 1
x

δ(x4 −4) dx.
(k)
# ∞
−∞
ex sin πx
3 δ(4x2 −1) dx.
(l)
# ∞
−∞
ln(1 + x) sin πx
2 δ(x3 −1) dx.
(m)
# ∞
−∞
sin πex
2 δ(x3 + 1) dx.
5.3. Show that
# +∞
−∞
f(x)δ′(x −x0) dx = −f ′(x0)
and
# +∞
−∞
f(x)δ′(g(x)) dx = −
# +∞
−∞
f ′(x)δ(g(x)) dx.
5.4. Evaluate the following integrals:
(a)
# ∞
0
ex sin πx
2 δ′(x2 −1) dx.
(b)
# 2
−2
ex sin πx
2 δ′(x2 −1) dx.
(c)
# ∞
0
ex sin πx
2 δ′(x3 + 1) dx.
(d)
# ∞
−∞
sin
πex
2

δ′(x4 + 1) dx.
(e)
# ∞
0
sin−1(1/x)δ′(x4 −1) dx.
(f)
# ∞
−∞
cos(πx)δ′(6x2 −x −1) dx.

5.4 Problems
167
(g)
# ∞
−0.1
sin
πex
2

δ′(x2 + x) dx.
(h)
# ∞
−∞
ex sin πx
2 δ′(ex sin πx
2 ) dx.
(i)
# 5
0
esin xδ′(cos x) dx.
(j)
# ∞
0
sin−1
 1
x

δ′(x4 −4) dx.
(k)
# ∞
−∞
ex sin πx
3 δ′(4x2 −1) dx.
(l)
# ∞
−∞
ln(1 + x) sin πx
2 δ′(x3 −1) dx.
(m)
# ∞
−∞
sin πex
2 δ′(x3 + 1) dx.
5.5. Use integration by parts (or diﬀerentiation with respect to x0) to show
that
# +∞
−∞
f(x)δ′′(x −x0) dx = f ′′(x0)
and
# +∞
−∞
f(x)δ′′′(x −x0) dx = −f ′′′(x0)
and, in general,
# +∞
−∞
f(x)δ(n)(x −x0) dx = (−1)nf (n)(x0)
where δ(n) and f (n) represent the nth derivatives.
5.6. Derive Equation (5.16). Hint: Use the result of Problem 5.3.
5.7. Six point charges of equal strength q are equally spaced on a circle of
radius a. What is the volume charge density describing such a distribution in
cylindrical coordinates?
5.8. Convince yourself that
σq(x, y) = q
∞

i=−∞
∞

j=−∞
(−1)i+jδ(x −ia)δ(y −ja)
indeed describes a two-dimensional ionic crystal. Pay particular attention to
the power of (−1).
5.9. Derive Equations (5.31) and (5.32).
5.10. Plot (or describe) the distribution in space of each of the following
volume charge densities:
ρq(x, y, z) = δ(x)δ(y) {2δ(z) −3δ(z + 3)} ,
ρq(x, y, z) = 5δ(x + 1)δ(y −1) {δ(z −1) −δ(z + 1)} ,

168
Dirac Delta Function
ρq(ρ, ϕ, z) = −2δ(ρ −3)δ(ϕ −π)δ(z),
ρq(ρ, ϕ, z) = 2δ(ϕ −π/4)δ(z)
0 10

k=1
(−1)k+1δ(ρ −0.5k)
1
,
ρq(r, θ, ϕ) = 2δ(ϕ −π/4)δ(r −2)
0 10

k=1
(−1)k+1δ

θ −π
20k
1
,
ρq(r, θ, ϕ) = 2δ(θ −π/4)δ(r −2)
0 20

k=1
(−1)k+1δ

ϕ −π
10k
1
.
5.11. Derive Equation (5.36).
5.12. Plot θ(t)θ(1 −t), θ(t) −θ(−t), and θ(t2 + 1) for −∞< t < +∞.
5.13. Write θ(t2 −1) as a product of two step functions.
5.14. For the two-dimensional ionic crystal shown in Figure 5.6:
(a) write the volume charge density describing the distribution (charges are
in the xy-plane);
(b) calculate the electrostatic ﬁeld at (0, 0, a); and
(c) calculate the electrostatic potential at an arbitrary point in space with
coordinates (x, y, z).
(d) Show that the ionization energy is of the form −αkeq2/a with α given in
terms of a sum.
(e) Numerically evaluate α.
5.15. For the three-dimensional ionic crystal:
(a) write the volume charge density describing the distribution; and
(b) calculate the electrostatic potential at an arbitrary point in space with
coordinates (x, y, z).
(c) Show that the ionization energy is of the form −αkeq2/a with α given in
terms of a sum.
(d) Numerically evaluate α.
5.16. Two electric charges +q and −q are located at P1 and P2 with position
vectors r1 and r2.
(a) Write the volume charge density describing these charges.
(b) Use (a) to ﬁnd their dipole moment deﬁned by

r′dq(r′).
5.17. The electric charge density of the one-dimensional ionic crystal can be
written as ρ(r) = N
i=−N qiδ(r −ri).
(a) Substitute this in Equation (5.38) and get
U = 1
2
N

i=−N
qiΦ(ri)

5.4 Problems
169
(b) Assuming that N is very large (inﬁnite), convince yourself that all products
qiΦ(ri) in the sum are equal (in particular the sign of the charge does not
matter). Therefore, U = 1
2(2N + 1)q0Φ(r0), where the subscript denotes the
zeroth charge.
(c) Show that Φ(r0) = N
j=−N keqj/|rj −r0|.
(d) Place the origin at the location of the zeroth charge, and assume that the
this charge is positive. Then, r0 = 0, rj = jaˆez, and qj = −(−1)jq. Now
show that
U = −(N + 1
2)q2ke
N

j=−N
(−1)j
|aj|
(e) By breaking up the sum into two parts show that
U = −(2N + 1)q2ke
a
N

j=1
(−1)j
j
5.18. 2N charges of equal sign and magnitude q are arranged equally spaced
on a circle of radius a located in the xy-plane. Assume that the charge num-
bered 2N is at (a, 0, 0).
(a) Write the volume charge density of such a distribution in cylindrical co-
ordinates.
(b) Starting with an integral expression for the electric ﬁeld, ﬁnd the cylin-
drical components of the ﬁeld at an arbitrary point P in space in terms of
a sum. The coordinates of P are (ρ, ϕ, z). Simplify your answer as much as
possible.
(c) Now let P have coordinates (2a, 0, 0). Show that all components of the
ﬁeld are of the form (keq/a2)α. Express the α for each component in terms
of a sum. What do you expect the value of α to be? Can you ﬁnd that value?
(d) For N = 3, i.e., six charges, calculate the numerical value of α in part (c)
for all components.
5.19. 2N + 1 charges of equal sign and magnitude q are arranged on the x-
axis of a Cartesian coordinate system as shown in Figure 5.9, with the zeroth
charge at the origin. The numbers below the axis are labels of the charges.
(a) From the pattern of the ﬁgure, determine the location of the kth charge
for −N ≤k ≤N.
x
0
1
2
3
−1
−2
−3
4a
a
9a
a
4a
9a
Figure 5.9: The charges and their distances on the x-axis.

170
Dirac Delta Function
(b) Write a volume charge density in terms of the Dirac delta function de-
scribing such a charge distribution.
(c) Calculate the components of the electric ﬁeld at a general point P with
coordinates (x, y, z).
(d) Now let P have coordinates (a, a, 0). Show that all components of the
ﬁeld are of the form (keq/a2)α where α is a numerical factor. Find this factor
for each component.
5.20. 2N positive and negative charges of equal magnitude are arranged
equally spaced and alternating in sign on a circle of radius a.
(a) Write the expression of the volume charge density describing this charge
distribution.
(b) Find the ionization energy in the form −αkeq2/a with α given in terms
of a sum. Simplify this sum as much as possible.

Part II
Algebra of Vectors

Chapter 6
Planar and Spatial Vectors
The preceding chapters made heavy use of vectors in the plane and in space.
The enormous utility of the concept of vectors has prompted mathematicians
and physicists to generalize this concept to include other objects that at ﬁrst
glance have no resemblance whatsoever with the planar and spatial vectors.
In this chapter, we shall study this generalization in its limited form, i.e.,
only in an algebraic context. Although the analysis of vectors is discussed
in Chapters 12 through 17, it is conﬁned to vectors in space. The analysis
of generalized vectors is the subject of diﬀerential geometry and functional
analysis that are beyond the scope of this book.1
There are many mathematical objects used in physics that allow for the
two operations of addition and multiplication by a number. The collection
of such objects is called a vector space. Thus, a vector space is a bunch
vector spaces
deﬁned
of “things” having the property that when you add two “things” you get a
third one, and if you multiply a “thing” by a number you get another one of
those “things.” Furthermore, the operation of multiplication by a number and
addition of “things” is distributive, and a vector space always has a “thing”
that we call the zero vector.
Using the two operations of multiplication by a number and addition, we
can form a sum,
α1a1 + α2a2 + · · · + αnan,
(6.1)
where α1, α2, . . . , αn, are real numbers and a1, a2, . . . , an are vectors. The
sum in Equation (6.1) is called a linear combination of the n vectors and
linear combination
α1, α2, . . . , αn are called the coeﬃcients of the linear combinations.
coeﬃcients
1Hassani,
S. Mathematical Physics:
A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, discusses diﬀerential geometry and functional analysis in some detail.

174
Planar and Spatial Vectors
Box 6.0.1. If we can ﬁnd some set of real numbers, α1, α2, . . . , αn (not
all of which are zero), such that the sum in (6.1) is zero, we say that the
vectors are linearly dependent. If no such set of real numbers can be
found, then the vectors are called linearly independent.
6.1
Vectors in a Plane Revisited
Before elaborating further on the generalization of vectors and their spaces,
it is instructive to revisit the familiar vectors in a plane from a point of view
suitable for generalization. We ﬁrst discuss the notion of linear independence
as applied to vectors in the plane.
The two vectors ˆex and ˆey (sometimes denoted as i and j) are linearly
independent because αˆex + βˆey = 0 can be satisﬁed only if both α and β are
zero. If one of them, say α, were diﬀerent from zero, one could divide the
equation by α and get
ˆex = −β
αˆey
which is impossible because ˆex and ˆey cannot lie along the same line.
Example 6.1.1. The arrows in the plane are not the only kinds of vectors dealt
with in physics. For instance, consider the set of all linear functions, or polynomials
of degree one (or less), i.e., functions of the form α0 + α1t where α0 and α1 are real
numbers and t is an arbitrary variable. Let us call this set P1[t], where P stands for
polynomials as
vectors?
“polynomial,” 1 signiﬁes the degree of these polynomials, and t is just the variable
used. We can add two such polynomials and get a third one of the same form. We
can multiply any such polynomial by a real number and get another polynomial. In
fact, P1[t] has all the properties of the vectors in a plane. We say that P1[t] and
the vectors in a plane are isomorphic which literally means they have the “same
shape.”
It is important to emphasize that two polynomials are equal if and only if all
their coeﬃcients are equal. In particular, a polynomial is equal to zero only if it is so
for all values of t, i.e., only if its coeﬃcients vanish. This immediately leads to the
fact that the two polynomials 1 and t are linearly independent because if α+ βt = 0
(for all values of t), then α = β = 0 (try t = 0 and t = 1).
■
It is easy to show that any three vectors in the plane are linearly dependent.
proof of the fact
that any three
vectors in the
plane are linearly
dependent
Figure 6.1 shows three arbitrary vectors drawn in a plane. From the tip of one
of the vectors (a3 in the ﬁgure), a line is drawn parallel to one of the other two
vectors such that it meets the third vector (or its extension) at point D. The
vectors −−→
OD and −−→
DC are proportional to a1 and a2, respectively, and their
sum is equal to a3. So we can write
a3 = −−→
OD + −−→
DC = αa1 + βa2 ⇒αa1 + βa2 −a3 = 0
and a1, a2, and a3 are linearly dependent. Clearly we cannot do the same
with two arbitrary vectors. Thus

6.1 Vectors in a Plane Revisited
175
A
B
C
D
O
a1
a2
β a2
a3
α a1
Figure 6.1: Any three vectors a1, a2, and a3 in the plane are linearly dependent.
Box 6.1.1. The maximum number of linearly independent vectors in a
plane is two. Any vector in a plane can be written as a linear combination
of only two non-collinear (not lying along the same line) vectors.
We also say that any two non-collinear vectors span the plane.
Suppose that we can write a vector a as a linear combination of n vectors
a = α1a1 + α2a2 + · · · + αnan.
We want to see under what conditions the coeﬃcients are unique. Suppose
that we can also write
a = β1a1 + β2a2 + · · · + βnan,
where the β’s are diﬀerent from the α’s. Then, subtracting these two linear
combinations, we get
0 = (α1 −β1)a1 + (α2 −β2)a2 + · · · + (αn −βn)an.
This is possible only if the vectors are linearly dependent. Therefore, if we
want the coeﬃcients to be unique, the vectors have to be linearly independent.
In particular, we can have at most two such vectors in the plane.
Thus,
choosing any two linearly independent vectors a1 and a2 in the plane, we can
expand any other vector uniquely as a linear combination of a1 and a2. This
brings us to the notion of a basis.
basis deﬁned
Box 6.1.2. Vectors that span the plane and are linearly independent are
called a basis for the plane.
The foregoing argument showed that any two non-collinear vectors form a
basis for the plane.
With the notion of a basis comes the concept of components of a vector.
Given a basis, there is a unique way in which a particular vector can be written

176
Planar and Spatial Vectors
in terms of the vectors in the basis. The unique coeﬃcients of the basis vectors
are called the components of the particular vector in that basis. Another
components and
dimension
concept associated with the basis is dimension which is deﬁned to be the
number of vectors in a basis. It follows that the plane has two dimensions.
Example 6.1.2. The components of a3 in the basis {a1, a2} of Figure 6.1 are
(α, β).2 Given any basis {a1, a2} of the plane, it is readily seen that the components
of a1 are (1, 0) and those of a2 are (0, 1).
■
Example 6.1.3. The polynomials {1, t} form a basis for P1[t], because they are
linearly independent and they span P1[t].
Therefore P1[t] is a two-dimensional
vector space. The components of f = α0 + α1t are (α0, α1) in this basis. How do
we determine the components of f in another basis {a1, a2} with a1 = 1 + t and
a2 = 1 −t? Since {a1, a2} is a basis, we can write
f = x1a1 + x2a2 = x1(1 + t) + x2(1 −t) = (x1 + x2) + (x1 −x2)t
or
α0 + α1t = (x1 + x2) + (x1 −x2)t ⇒(α0 −x1 −x2) · 1 + (α1 −x1 + x2)t = 0.
The linear independence of 1 and t now tells us that the coeﬃcients of 1 and t should
vanish. This leads to two equations in two unknowns:
x1 + x2 = α0,
x1 −x2 = α1.
The solution of these equations are easily found to be
x1 = 1
2(α0 + α1),
x2 = 1
2(α0 −α1).
Thus, the components of f are ( 1
2(α0 + α1), 1
2(α0 −α1)) in the new basis.
■
6.1.1
Transformation of Components
There are inﬁnitely many bases in a plane, because there are inﬁnitely many
pairs of vectors that are linearly independent. Therefore, there are inﬁnitely
many sets of components for any given vector, and it is desirable to be able
to ﬁnd a relation between any two such sets. Such a relation employs the
machinery of matrices.
Consider a vector a with components (α1, α2) in the basis {a1, a2} and
components (α′
1, α′
2) in the basis {a′
1, a′
2}. We can write
a = α1a1 + α2a2
and
a = α′
1a′
1 + α′
2a′
2.
(6.2)
Since {a′
1, a′
2} form a basis, any vector, in particular, a1 or a2, can be written
in terms of them:
a1 = a11a′
1 + a21a′
2,
a2 = a12a′
1 + a22a′
2,
(6.3)
2Since in this chapter we are dealing primarily with components (and not coordinates),
we shall use parentheses—instead of angle brackets—to list the components.

6.1 Vectors in a Plane Revisited
177
where (a11, a21) and (a12, a22) are, respectively, components of a1 and a2 in
the basis {a′
1, a′
2}. Combining Equations (6.2) and (6.3), we obtain
α1(a11a′
1 + a21a′
2) + α2(a12a′
1 + a22a′
2) = α′
1a′
1 + α′
2a′
2
or
(α′
1 −a11α1 −a12α2)a′
1 + (α′
2 −a21α1 −a22α2)a′
2 = 0.
The linear independence of a′
1 and a′
2 gives
α′
1 = a11α1 + a12α2,
α′
2 = a21α1 + a22α2.
(6.4)
These equations can be written concisely as3

α′
1
α′
2

=

a11
a12
a21
a22
 
α1
α2

or
a′ = Aa,
(6.5)
where we have introduced the matrices
matrix and
column vector
a ≡
α1
α2

,
a′ ≡
α′
1
α′
2

,
A ≡
a11
a12
a21
a22

.
(6.6)
The matrices a and a′ are called column vectors or 2 × 1 matrices because
they each have two rows and one column. Similarly, A is called a 2×2 matrix.
Let us now choose a third basis, {a′′
1, a′′
2}, and write a = α′′
1a′′
1 + α′′
2a′′
2. If
(a′
11, a′
21) and (a′
12, a′
22) are, respectively, the components of a′
1 and a′
2 in this
third basis, then
a′
1 = a′
11a′′
1 + a′
21a′′
2,
a′
2 = a′
12a′′
1 + a′
22a′′
2.
Substituting these in the second equation of (6.2) and equating the result to
a = α′′
1a′′
1 + α′′
2a′′
2 yields
α′′
1 = a′
11α′
1 + a′
12α′
2,
α′′
2 = a′
21α′
1 + a′
22α′
2.
(6.7)
We can write Equation (6.7) in matrix form:
α′′
1
α′′
2

=
a′
11
a′
12
a′
21
a′
22
 α′
1
α′
2

or
a′′ = A′a′,
(6.8)
where
a′′ ≡
α′′
1
α′′
2

,
A′ ≡
a′
11
a′
12
a′
21
a′
22

,
(6.9)
and a′ is as deﬁned before.
3At this point, think of Equation (6.5) as a short-hand way of writing Equation (6.4).
Further signiﬁcance of this notation will become clear after Box 6.1.3.

178
Planar and Spatial Vectors
We can also discover how a′′ and a are related by substituting (6.4) in
(6.7). This leads to the equation
two
transformations in
a row suggest the
rule of matrix
multiplication.
α′′
1 = (a′
11a11 + a′
12a21)α1 + (a′
11a12 + a′
12a22)α2,
α′′
2 = (a′
21a11 + a′
22a21)α1 + (a′
21a12 + a′
22a22)α2,
which, in matrix form, becomes
a′′ = A′′a
where
A′′ ≡
a′
11a11 + a′
12a21
a′
11a12 + a′
12a22
a′
21a11 + a′
22a21
a′
21a12 + a′
22a22

.
(6.10)
On the other hand, the matrix equations (6.8) and (6.5) yield a′′ = A′(Aa),
which is consistent with Equation (6.10) only if matrix multiplication is
deﬁned so that A′′ = A′A, i.e.,
a′
11
a′
12
a′
21
a′
22
 a11
a12
a21
a22

=
a′
11a11 + a′
12a21
a′
11a12 + a′
12a22
a′
21a11 + a′
22a21
a′
21a12 + a′
22a22

.
(6.11)
All discussions and all the equations obtained so far are based on ﬁxing
a vector and looking at its components in diﬀerent bases. However, there is
another, more physical, way of interpreting these equations. Consider (6.5).
active and passive
transformations
distinguished
Here the column vector on the RHS represents the components of a vec-
tor a in the basis {a1, a2}.
Applying the matrix A to this column vector
yields a new column vector given on the LHS, which can be interpreted as
the components of a new vector a′ in the same basis. So, in essence we have
changed the vector a into a new vector a′ via the transformation A. The ﬁrst
interpretation mentioned above is called a passive transformation (a is
“passively” unchanged as basis vectors are altered); the second interpretation
is called active transformation (a is actively changed into a′). We shall
have occasion to employ both interpretations. However, the active transfor-
mation is more direct and we shall use that more often.
The reader may
convince himself or herself that passive transformation in one “direction” is
completely equivalent to active transformation in the “opposite” direction.
A good example to keep in mind is the rotation of axes (passive rotation)
versus the rotation of a vector (active rotation) in the plane as shown in
Figure 6.2.
Equation (6.11) deﬁnes the “product” of two matrices in a prescribed man-
ner. To ﬁnd the entry in the ﬁrst row and ﬁrst column of the product, multiply
the entries of the ﬁrst row of the ﬁrst matrix by the corresponding entries of
the ﬁrst column of the second matrix and add the terms thus obtained. To
ﬁnd the entry in the ﬁrst row and second column of the product, multiply
the entries of the ﬁrst row of the ﬁrst matrix by the corresponding entries of
the second column of the second matrix and add the terms. Other entries
are found similarly. This leads us to the following rule which applies to all
matrices, not just those that are 2 × 2:

6.1 Vectors in a Plane Revisited
179
O
a
'
(b)
x
a
O
(a)
y
x
y
a
O
'
(c)
x
y
y
'
x
'
Figure 6.2: (a) A vector a in a coordinate system Oxy can be (b) actively transformed
to a new vector a′ in the same coordinate system, or (c) passively transformed to a
new coordinate system O′x′y′. Note that the relation of a′ to Oxy is identical to the
relation of a to O′x′y′.
Box 6.1.3. (Matrix Multiplication Rule). To obtain the entry in the
ith row and jth column of the product of two matrices, multiply the entries
of the ith row of the matrix on the left by the corresponding entries of the
jth column of the matrix on the right and add the products thus obtained.
For this rule to make sense, the number of entries in a row of the matrix on
for the product
rule to make
sense, number of
columns of the left
matrix must equal
number of rows of
the right matrix.
the left must equal the number of entries in a column of the matrix on the
right.
We identiﬁed a column vector as a 2 × 1 matrix. With this identiﬁcation,
the RHS of Equation (6.5) can be interpreted as the product of two matrices,
a 2 × 2 matrix and a 2 × 1 matrix, resulting in a 2 × 1 matrix, the column
vector on the LHS.
Matrices were obtained in a natural way in the discussion of basis changes,
matrices forming a
mathematical
structure with
operations other
than
multiplication
and the natural operation ensued was that of multiplication. Once a mathe-
matical entity is created in this manner, a full mathematical structure becomes
irresistibly enticing. For example, such operations as addition, subtraction,
division, inversion, etc., also demand our attention. We now consider such
operations.
First, we need to deﬁne the equality of matrices: Two matrices are equal
if they have the same number of rows and columns, and their corresponding
elements are equal. Addition of two matrices is deﬁned if they have the same
number of rows and columns in which case the sum is deﬁned to be the sum
of corresponding elements. A 2 × 2 matrix can be added to another 2 × 2
matrix, but a column vector cannot. Thus if
A =

a11
a12
a21
a22

and
B =

b11
b12
b21
b22

,

180
Planar and Spatial Vectors
then
A + B =
a11 + b11
a12 + b12
a21 + b21
a22 + b22

.
From the deﬁnition of the sum and the product of matrices, it is clear that
addition is always commutative but product need not be:
A + B = B + A
but
AB ̸= BA.
(6.12)
We can turn the set of 2 × 2 matrices into a vector space by deﬁning the
2 × 2 matrices
form a vector
space
product of a number and a matrix as a new matrix whose elements are the old
elements times the number. The zero “vector” is simply the zero matrix—
the 2 × 2 matrix all of whose elements are zero. The reader may verify that
zero matrix
all the usual operations of vectors apply to this set.4 If you multiply a matrix
by the number 0, you get the zero matrix.
Example 6.1.4. Suppose
A =
1
−1
2
3

and
B =
−1
0
1
2

.
Then
A + B =
1 −1
−1 + 0
2 + 1
3 + 2

=
0
−1
3
5

= B + A
and
AB =
1
−1
2
3
 −1
0
1
2

=
1 · (−1) + (−1) · 1
1 · 0 + (−1) · 2
2 · (−1) + 3 · 1
2 · 0 + 3 · 2

=
−2
−2
1
6

,
while
BA =

−1
0
1
2
 
1
−1
2
3

=

(−1) · 1 + 0 · 2
(−1) · (−1) + 0 · 3
1 · 1 + 2 · 2
1 · (−1) + 2 · 3

=

−1
1
5
5

.
Clearly, AB ̸= BA.
■
The 2 × 2 matrix
1 ≡

1
0
0
1

is called the 2 × 2 identity matrix or unit matrix, and has the property
identity matrix or
unit matrix
that when it multiplies any other matrix (on the right or on the left), the
latter does not get aﬀected. The unit matrix is used to deﬁne the inverse of
a matrix A as a matrix B that multiplies A on either side and gives the unit
inverse of a matrix
matrix. The inversion of a matrix is a much more complicated process than
that of ordinary numbers, and we shall discuss it in greater length later. At
this point, suﬃce it to say that, contrary to numbers, not all nonzero matrices
have an inverse. For example, the reader can easily verify that the nonzero
matrix
 1 0
0 0
!
cannot have an inverse.
4Note that the extra operation of multiplication of a matrix by another matrix is not
part of the requirement for the set to be a vector space.

6.1 Vectors in a Plane Revisited
181
We have introduced 2 × 2 and column (or 2 × 1) matrices. To complete
the picture, we also introduce a row vector, or a 1 × 2 matrix. The rule of
row vector
matrix multiplication allows the multiplication of a 2×2 matrix and a column
vector, as long as the latter is to the right of the former: You cannot multiply
a 2 × 1 matrix situated to the left of a 2 × 2 matrix. Similarly, you cannot
multiply two 2 × 1 matrices. However, the product of a row vector (a 1 × 2
matrix) and a column vector (a 2 × 1 matrix) is deﬁned—as long as the latter
is to the right of the former—and the result is a 1 × 1 matrix, i.e., a number.
This is because we have only one row to the left of a single column. What
about the product of a row vector and a 2 × 2 matrix? As long as the matrix
is to the right of the row vector, the product is deﬁned and the result is a row
vector.
Example 6.1.5. With A and B as deﬁned in Example 6.1.4 and
x =
 1
−1

,
y =
 
−1
2
!
,
we have
Ax =
1
−1
2
3
  1
−1

=
 2
−1

,
yB =
 
−1
2
! −1
0
1
2

=
 
3
4
!
,
yx =
 
−1
2
!  1
−1

= (−3) = −3,
yAx =
 
−1
2
! 1
−1
2
3
  1
−1

=
 
−1
2
!  2
−1

= −4,
yBAx =
 
3
4
!  2
−1




=2
=
 
−1
2
! −1
1
5
5




=BA
 1
−1

=
 
−1
2
! −2
0

= 2,
yABx =
 
−1
2
! 
−2
−2
1
6
 
1
−1

=
 
4
14
! 
1
−1

= −10.
In the manipulations above, we have used the associativity of matrix multiplication
and multiplied matrices in diﬀerent orders without, of course, commuting them.
Products such as Ay, By, yy, and xx are not deﬁned; therefore, we have not considered
them here.
■
There is a new operation on matrices which does not exist for ordinary
numbers. This is called transposition and is deﬁned as follows:
transpose of a
matrix
Box 6.1.4. The transpose of a matrix is a new matrix whose rows are
the columns of the old matrix and whose columns are the rows of the old
matrix. The transpose of A is denoted by At or 8A.

182
Planar and Spatial Vectors
Therefore
A =

a11
a12
a21
a22

⇒At = 8A =

a11
a21
a12
a22

.
If At = A, we say that A is symmetric.
symmetric matrix
Example 6.1.6. With A, B, x, and y as deﬁned in Example 6.1.5, we have
At =
 1
2
−1
3

,
8B =
−1
1
0
2

,
xt =
 
1
−1
!
,
8y =
−1
2

.
Note that although xx and yy are not deﬁned, all the combinations 8xx, y8y, 8yy, and
x8x are deﬁned: In the ﬁrst two cases one gets a number, and in the last two cases a
2 × 2 matrix.
■
It should be clear from the deﬁnition of the transpose that
properties of
transposition
(A + B)t = At + Bt,
(AB)t = BtAt,
(At)t = A.
(6.13)
Of the three relations, the middle one is the least obvious, but the reader
can verify it directly by choosing appropriate general matrices and carrying
through the multiplications on both sides of the relation.
6.1.2
Inner Product
From our discussion of Chapter 1, we know that if a and b are vectors in the
plane having components (ax, ay) and (bx, by) along the x- and y-axes, then
their dot product is
a · b = axbx + ayby.
(6.14)
We want to generalize this dot product so that it applies to arbitrary bases.
This generalization is called the inner product.
Recall that any two non-collinear vectors {a1, a2} in the plane form a
basis and any vector can be written as a linear combination of them with
the unique coeﬃcients being the components of the vector in the basis. In
particular, the components of a1 are (1, 0) and those of a2 are (0, 1). If we were
to deﬁne the dot product in terms of components, we would have to modify
Equation (6.14) because that equation would give zero for a1 ·a2 which would
equation (6.14)
will not work for
arbitrary bases!
be inconsistent with (1.1). How should we modify (6.14)? Since we want to
deal with components, a natural setting would be the language of matrices.
If a and b are the column vectors
 ax
ay
!
and
 bx
by
!
, respectively, then we can
rewrite Equation (6.14) as
inner (dot)
product in terms
of row and column
vectors
a · b = atb =
 ax
ay
! bx
by

= axbx + ayby.
(6.15)
It is this matrix relation that we want to generalize so that the result is the
true dot product of vectors no matter what basis we choose in which to express
our vectors.

6.1 Vectors in a Plane Revisited
183
Besides the failure of Equation (6.15) for general bases, the demand for
generalization stems from another source: There are other kinds of “vectors”
that are not just arrows in the plane. For instance, the polynomials P1[t] of
degree one that we introduced in Example 6.1.1 are such vectors. How do we
deﬁne inner products for these vectors? We cannot use Equation (1.1) because
neither the length of a polynomial nor the angle between two polynomials is
deﬁned.
In fact, both the length and the angle are deﬁned only after an
inner product has been introduced. Furthermore, there is no guarantee that
Equation (6.15) will make sense.
Let’s see how far we can go using the general properties of the inner prod-
uct discussed at the beginning of Section 1.1.1. Write a and b as a linear
combination of the basis vectors {a1, a2}:
a = α1a1 + α2a2,
b = β1a1 + β2a2
Take the dot-product of these vectors and write it in terms of the dot-products
of the basis vectors:
a · b =
 
α1a1 + α2a2
!
·
 
β1a1 + β2a2
!
= α1β1a1 · a1 + α1β2a1 · a2
+ α2β1a2 · a1 + α2β2a2 · a2
Deﬁne a matrix with elements
g11 = a1 · a1,
g12 = a1 · a2 = a2 · a1 = g21,
g22 = a2 · a2
Then, representing a and b as column vectors a =
 α1
α2
!
and b =
 β1
β2
!
, the
dot product can be generalized to
a symmetric
matrix G is needed
to generalize the
inner product.
a · b = atGb =
 α1
α2
! g11
g12
g21
g22
 β1
β2

,
(6.16)
where G is a symmetric matrix.
Example 6.1.7. In this example, we shall deﬁne an inner product for the vectors
inner (dot)
product of two
polynomials
in P1[t] that happens to be useful in physical applications. The idea is to ﬁnd a rule
that takes two “vectors” in P1[t] and gives a real number. Since the vectors in P1[t]
are functions (albeit a very special kind), one natural way of getting numbers out
of functions is by integrating them. It turns out that this is indeed the most useful
way of deﬁning the inner product for such polynomials. So, let (a, b) be an interval
on the real line and let f = α0 + α1t and g = β0 + β1t be two “vectors” in P1[t] .
We deﬁne
f · g ≡
# b
a
f(t)g(t) dt.
(6.17)
One can show that Equation (6.17) exhibits all the properties expected of an inner
product (as outlined in Section 1.1.1). For instance, f · f is always positive because
the integrand [f(t)]2 is always positive. Furthermore, f · g = g · f, and, as the reader
may check,
f · (g + h) = f · g + f · h.

184
Planar and Spatial Vectors
These all indicate that we are on the right track.
We also note that the inner product depends on the interval chosen on the real
line. For diﬀerent (a, b), we get a diﬀerent inner product. The choice is usually
dictated by the physical application. We shall choose a = 0, b = 1, although this
may not be a physically suitable choice. With such a choice and with {f1 = 1, f2 = t}
as a basis, we obtain
g11 = f1 · f1 =
# 1
0
f1(t)f1(t) dt =
# 1
0
dt = 1,
g12 = f1 · f2 =
# 1
0
f1(t)f2(t) dt =
# 1
0
t dt = 1
2 = g21,
g22 = f2 · f2 =
# 1
0
f2(t)f2(t) dt =
# 1
0
t2dt = 1
3.
So the inner product matrix is
G =

1
1
2
1
2
1
3

.
■
We started with Equation (1.1) as the deﬁnition of the inner product. This
deﬁnition assumed a knowledge of lengths and angles. These are notions with
which we become intuitively familiar very early in our mental development.
the notion of
length comes after
that of the inner
product!
However, such notions are not intuitively obvious for two polynomials. That
is why the concepts of lengths and angles for objects such as polynomials
come after introducing the notion of inner product. Of course, we want these
notions to agree with the intuitive notions of lengths and angles, i.e., we want
them to be related to the inner product in precisely the same manner as given
in Equation (1.1). If we let b = a in that equation, we get a · a = |a|2. This
becomes our deﬁnition for length:
Box 6.1.5. Given any inner product on a set of objects that we can call
“vectors,” we deﬁne the length of a vector a as |a| ≡+√a · a.
Once the notion of length is established for a general set of vectors, we
can deﬁne the angle between two vectors a and b as
cos θ ≡a · b
|a| |b| =
a · b
√a · a
√
b · b
.
(6.18)
This equation and the one in Box 6.1.5 clearly show that lengths and angles
are given entirely in terms of inner products. For these concepts to be valid,
we must ensure that however we deﬁne the inner product, it will have the
property that a · a > 0 for a nonzero vector. It turns out that most inner
products encountered in applications have this property. Nevertheless, there

6.1 Vectors in a Plane Revisited
185
are cases (very important ones) for which a·a ≤0. In such cases, the concepts
in some important
physical situations
the “length” of a
nonzero vector
can be zero—even
negative!
of length and angles, as we know them, break down, and we have to be content
with “dot products” that may produce nonpositive numbers when a nonzero
vector is “dotted” with itself.
Even if a · a > 0, there is no a priori guarantee that the cosine obtained in
Equation (6.18) will lie between −1 and +1, as it should. However, there is
a famous inequality in mathematics called the Schwarz inequality, which
establishes this fact for those inner products which satisfy a · a > 0. We shall
come back to this later in this chapter.
Example 6.1.8. The lengths of the basis vectors {f1 = 1, f2 = t} of P1[t] can be
found easily using the results of Example 6.1.7:
|f1| =
√
f1 · f1 = +
√
1 = 1
|f2| =
√
f2 · f2 = +

1
3.
We can also ﬁnd the “angle” between the two polynomials
cos θ = f1 · f2
|f1| |f2| =
1
2
1 · (1/
√
3)
=
√
3
2
⇒θ = π
6 .
■
The matrix G, called the inner product matrix or metric matrix,
G is the inner
product matrix or
the metric matrix.
completely determines the inner product of vectors when they are written
as linear combinations of a1 and a2. For example, consider a vector a with
components (α1, α2) in the basis {a1, a2}. Figure 6.3 shows a as the sum of
−→
OA (which is the same as α1a1) and −−→
OA′ (which is the same as α2a2). Using
the law of cosines for the triangle OAP, we get
|a|2 = OP
2 = OA
2 + AP
2 −2OA AP cos ϕ
= α2
1|a1|2 + α2
2|a2|2 + 2α1α2|a1||a2| cos θ12.
θ12
θ12
θ
a
b
a1
a2
a 1
a 2
O
A
B
P
ϕ
'
A
Figure 6.3: The length of a is the same whether we use the law of cosine or the inner
product matrix G.

186
Planar and Spatial Vectors
On the other hand, using Equation (6.16), we obtain
|a|2 = a · a =
 α1
α2
! g11
g12
g21
g22
 α1
α2

=
 α1
α2
! 
g11α1 + g12α2
g21α1 + g22α2

= g11α2
1 + 2g12α1α2 + g22α2
2
= a1 · a1α2
1 + 2α1α2a1 · a2 + a2 · a2α2
2
= |a1|2α2
1 + 2α1α2|a1| |a2| cos θ12 + |a2|2α2
2
and the two expressions agree.
In fact, we can show this agreement very
generally:
a · b = (α1a1 + α2a2) · (β1a1 + β2a2)
= α1β1a1 · a1 + α1β2a1 · a2 + α2β1a2 · a1 + α2β2a2 · a2
= α1β1g11 + α1β2g12 + α2β1g21 + α2β2g22
=
 α1
α2
! g11
g12
g21
g22
 β1
β2

= 8aGb,
where we used the distributive property of the inner product.
It should now be clear to the reader that the matrix G contains all the
information needed to evaluate the inner product of any pair of vectors. Sup-
pose now that instead of {a1, a2} we choose {ˆe1, ˆe2} where ˆe1 and ˆe2 are
unit vectors and perpendicular to one another. Then, the matrix G will have
elements
g11 = ˆe1 · ˆe1 = 1,
g12 = g21 = ˆe1 · ˆe2 = 0,
g22 = ˆe2 · ˆe2 = 1,
i.e., G is the unit matrix. In that case, we obtain
8aGb =
 α1
α2
! 
1
0
0
1
 
β1
β2

= α1β1 + α2β2
which is the usual expression of the dot product of two vectors in terms of
their components. A basis whose vectors have unit length and are mutually
perpendicular to one another is called an orthonormal basis. Thus,
orthonormal basis
Box 6.1.6. Only in an orthonormal basis is the dot (inner) product of two
vectors equal to the sum of the products of their corresponding components.
In such a basis the inner product matrix G is the unit matrix.
The matrix G was introduced to ensure the validity of the inner product
in an arbitrary basis.
This poses some restriction on G; for example, we
saw that it had to be symmetric, i.e., g12 = g21 because of the symmetry of
the dot product. Another restriction—if we want thedot product of a basis

6.1 Vectors in a Plane Revisited
187
vector with itself to be positive—is that g11 > 0 and g22 > 0, in which case
the inner product is called positive deﬁnite (or Riemannian). It turns
positive deﬁnite,
or Riemannian
inner product
out, however, that such a restriction constrains G too much to be useful in
physical applications.
Although, in most of this book, we shall adhere to
the usual positive deﬁnite or Euclidean inner product, the reader should be
aware that non-Euclidean inner products also have important applications in
physics.
Box 6.1.7. Regardless of the nature of G, we call two vectors a and b
G-orthogonal if a · b ≡8aGb = 0.
Every point in the plane can be thought of as the tip of a vector whose
tail is the origin. With this interpretation, we can express the (G-dependent)
distance between two points in terms of vectors. Let r1 be the vector to point
P1 and r2 the vector to point P2. Then the “length” of the displacement
vector Δr ≡r1 −r2 is the “distance” between P1 and P2:
P1P2
2 = Δr · Δr = (r1 −r2) · (r1 −r2) = (9
Δr)G(Δr).
(6.19)
Keep in mind that only in the positive deﬁnite (Euclidean) case is P1P2
2
nonnegative. There are physical situations in which the square of the length
of the displacement vectors can be zero or even negative. We shall encounter
one such example when we discuss the special theory of relativity.
The simplicity of G in orthonormal bases makes them very much in de-
mand. So, it is important to know whether it is always possible to construct
orthonormal vectors out of general basis vectors. The construction should
involve linear combinations only. In other words, given a basis {a1, a2}, we
want to know if there are linear combinations of a1 and a2 which are orthonor-
mal. We assume that the inner product is positive deﬁnite, so that the inner
product of every nonzero vector with itself is positive. First we divide a1 by
its length to get
ˆe1 ≡a1
|a1| =
a1
√a1 · a1
.
To obtain the second orthonormal vector, we refer to Figure 6.4 which shows
that if we take away from a2 its projection on a1, the remaining vector will
be orthogonal to a1. So consider
a′
2 = a2 −(a2 · ˆe1)ˆe1



projection of
a2 on a1
and note that
Gram–Schmidt
process for the
plane
ˆe1 · a′
2 = ˆe1 · a2 −(a2 · ˆe1) ˆe1 · ˆe1
  
=1
= 0.

188
Planar and Spatial Vectors
(b)
(a)
a 1
a 2
e 1
^
a 2
a 2
'
(c)
e 1
^
a 2
'
(d)
e 2
^
e 1
^
Figure 6.4: The illustration of the Gram–Schmidt process for two linearly independent
vectors in the plane.
This suggests deﬁning ˆe2 as
ˆe2 ≡a′
2
|a′
2| =
a′
2
	
a′
2 · a′
2
.
The reader should note that in the construction of {ˆe1, ˆe2}, we have added
vectors and multiplied them by numbers, i.e., we have taken a linear combi-
nation of a1 and a2. This process, and its generalization to arbitrary number
of vectors, is called the Gram–Schmidt process, and shows that by appro-
priately taking linear combinations, it is always possible to ﬁnd orthonormal
vectors out of any linearly independent set of vectors.
Example 6.1.9. The basis {1, t} introduced for P1[t] is not orthonormal when
the inner product is integration over the interval (0, 1) as in Example 6.1.7. Let us
use the Gram–Schmidt process to ﬁnd an orthonormal basis. We note that the ﬁrst
basis vector already has a unit length; so we let ˆe1 = f1 = 1. To ﬁnd the second
vector, we ﬁrst construct
f ′
2 = f2 −(f2 · ˆe1)ˆe1 = t −( 1
2)1 = t −1
2
with
|f ′
2|2 = f ′
2 · f ′
2 =
# 1
0
(t −1
2)2dt =
1
12.
Then the second vector will be
ˆe2 = f ′
2
|f ′
2| = t −1
2

1
12
=
√
12(t −1
2) =
√
3(2t −1).
The reader may verify directly that {ˆe1, ˆe2} is an orthonormal basis.
■
Example 6.1.10. Consider the vectors
a1 = ˆex + ˆey
and
a2 = 2ˆex + ˆey.

6.1 Vectors in a Plane Revisited
189
The inner product matrix elements in the basis {a1, a2} are
g11 = a1 · a1 = (ˆex + ˆey) · (ˆex + ˆey) = 2,
g12 = (ˆex + ˆey) · (2ˆex + ˆey) = 3,
g21 = a2 · a1 = g12 = 3,
g22 = (2ˆex + ˆey) · (2ˆex + ˆey) = 5.
or, in matrix form, G =
 2 3
3 5
!
.
Now consider vectors b and c, whose components in {a1, a2} are, respectively,
(1, 1) and (−3, 2). We can compute the scalar product of b and c in terms of these
components using Equation (6.16):
b · c = 8bGc =
 
1
1
! 2
3
3
5
 −3
2

=
 
1
1
! 0
1

= 1.
We can also write b and c in terms of ˆex and ˆey and use the usual deﬁnition of
the inner product (in terms of components) to ﬁnd b·c. Since b has the components
(1, 1) in {a1, a2}, it can be written as
b = a1 + a2 = (ˆex + ˆey) + (2ˆex + ˆey) = 3ˆex + 2ˆey.
Similarly,
c = −3a1 + 2a2 = −3(ˆex + ˆey) + 2(2ˆex + ˆey) = ˆex −ˆey.
Thus, in {ˆex, ˆey}, b has components (3, 2), and c has components (1, −1). Then
b · c = bxcx + bycy = 3 · 1 + 2 · (−1) = 1
which agrees with the previous result obtained above.
■
Example 6.1.11. Consider two vectors f and g in P1[t] with
f ≡f(t) = α0 + α1t,
g ≡g(t) = β0 + β1t.
We want to ﬁnd the inner product of these two vectors. First, we use the basis {1, t}
and its corresponding G matrix found in Example 6.1.7:
f · g = ftGg =
 
α0
α1
!

1
1
2
1
2
1
3
 
β0
β1

= α0β0 + 1
2(α0β1 + α1β0) + 1
3α1β1.
Next, we use the orthonormal basis found in Example 6.1.9. In this basis G is the
identity matrix and the inner product is the usual one in terms of components.
However, the components of f and g need to be found in {ˆe1, ˆe2}. The reader may
check that
f = α0 + α1t = α0ˆe1 + α1

1
2
√
3
ˆe2 + 1
2ˆe1

= (α0 + 1
2α1)ˆe1 + α1
2
√
3
ˆe2,
g = β0 + β1t = (β0 + 1
2β1)ˆe1 + β1
2
√
3
ˆe2.
It then follows that
f · g = (α0 + 1
2α1)(β0 + 1
2β1) +
 α1
2
√
3
  β1
2
√
3

= α0β0 + 1
2(α0β1 + α1β0) + 1
3α1β1.

190
Planar and Spatial Vectors
Finally, we take the dot product of the two vectors using the deﬁnition of this dot
product:
f · g =
# 1
0
(α0 + α1t)(β0 + β1t) dt
= α0β0
# 1
0
dt + (α0β1 + α1β0)
# 1
0
t dt + α1β1
# 1
0
t2 dt
= α0β0 + 1
2(α0β1 + α1β0) + 1
3α1β1.
All three ways of calculating the inner product agree, as they should.
■
6.1.3
Orthogonal Transformation
Now that we have deﬁned inner products, we may combine it with the concept
of transformation. More speciﬁcally, we seek transformations that leave the
inner product—which we shall assume to be positive deﬁnite (Euclidean)—
unchanged. Under such transformations, the length of a vector and the angle
between two vectors will not change. That is why such transformations are
rigid
transformations
called rigid transformations.
We choose an orthonormal basis, so that
G = 1, and denote the transformed vectors by a prime: a′ = Aa, b′ = Ab.
Then the invariance of the inner product yields
8a′b′ = 8ab ⇒
:
(Aa)Ab = 8a8AAb = 8ab.
This will hold for arbitrary a and b only if
8AA = 1.
(6.20)
Matrices that satisfy this relation are called orthogonal. We now investigate
orthogonal
matrices
conditions under which Equation (6.20) holds by writing out the matrices:

a11
a21
a12
a22
 
a11
a12
a21
a22

=

a2
11 + a2
21
a11a12 + a21a22
a12a11 + a22a21
a2
12 + a2
22

=

1
0
0
1

which is equivalent to the following three equations:
a2
11 + a2
21 = 1,
a11a12 + a21a22 = 0,
a2
12 + a2
22 = 1.
(6.21)
Squaring the second equation and substituting from the ﬁrst and third, we
get
a2
11a2
12 = a2
21a2
22 ⇒(1 −a2
21)a2
12 = a2
21(1 −a2
12) ⇒a2
21 = a2
12.
The ﬁrst and third equations of (6.21) now yield
a2
22 = a2
11
and
a2
12 = a2
21 = 1 −a2
11.

6.1 Vectors in a Plane Revisited
191
Therefore, all parameters are given in terms of a11. Now the ﬁrst equation
2 × 2 orthogonal
matrices are
described in terms
of a single
parameter.
of (6.21) indicates that −1 ≤a11 ≤1. It follows that a11 can be thought
of as a sine or a cosine of some angle, say θ. Let us choose cosine. Then
a22 = ± cosθ. If we choose the plus sign for cosine, then the middle equation
of (6.21) shows that a12 = −a21 = ± sin θ, and if we choose the minus sign,
a12 = a21 = ± sin θ. Let us choose the plus sign for cosine. Then, we obtain
two possibilities for A:
A =
cos θ
−sin θ
sin θ
cos θ

or
A =
 cos θ
sin θ
−sin θ
cos θ

.
The diﬀerence is in the sign of the angle θ.
Writing (x, y) for the components of a vector in the plane [instead of
(α1, α2)], and (x′, y′) for the transformed vector, and using the ﬁrst choice for
A, we have
x′
y′

=
cos θ
−sin θ
sin θ
cos θ
 x
y

or
x′ = x cos θ −y sin θ,
(6.22)
y′ = x sin θ + y cos θ.
(6.23)
This is how the coordinates of a point in the plane transform under a counter-
clockwise rotation of angle θ. Had we chosen the second form of A, we would
have obtained a clockwise rotation of the coordinates. Notice how we chose
the signs of sines and cosines to ensure that when θ = 0, the rotation is the
unit matrix, i.e., no rotation at all. Although rotations are part of orthogonal
transformations, the converse is not true: There are orthogonal transforma-
tions that do not correspond to a rotation. For example, the matrix
A =

cos θ
sin θ
sin θ
−cosθ

(6.24)
is orthogonal (as the reader can verify), but it does not correspond to a rota-
tion because at θ = 0 it does not give the identity matrix.
In general, the inner product of the transformed (primed) vectors will be
8a′Gb′ = :
(Aa)GAb = 8a8AGAb.
For A to preserve the inner product, i.e., for 8a′Gb′ to be equal to 8aGb, we need
to have
G-orthogonal
matrices
8AGA = G.
(6.25)
A matrix that satisﬁes Equation (6.25) is called G-orthogonal.
Historical Notes
Matrices entered mathematics slowly and somewhat reluctantly. The related notion
of determinant, which is a number associated with an array of numbers, was intro-
duced as early as the middle of the eighteenth century in the study of a system of

192
Planar and Spatial Vectors
linear equations. However, the recognition that the array itself could be treated as
a mathematical object, obeying certain rules of manipulation, came much later.
Logically, the idea of a matrix precedes that of a determinant as Arthur Cayley
has pointed out; however, the order was reversed historically. In fact, many of the
properties of matrices were known as a result of their connection to determinants.
Because the uses of matrices were well established, it occurred to Cayley to introduce
them as distinct entities. He says, “I certainly did not get the notion of a matrix in
any way through quaternions; it was either directly from that of a determinant or
as a convenient way of expression of” a system of two equations in two unknowns.
Because Cayley was the ﬁrst to single out the matrix itself and was the ﬁrst to
publish a series of articles on them, he is generally credited with being the creator
of the theory of matrices.
Arthur Cayley
1821–1895
Arthur Cayley’s father, Henry Cayley, although from a family who had lived
for many generations in Yorkshire, England, lived in St. Petersburg, Russia. It was
in St. Petersburg that Arthur spent the ﬁrst eight years of his childhood before his
parents returned to England and settled near London. Arthur showed great skill
in numerical calculations at school and, after he moved to King’s College in 1835,
his aptitude for advanced mathematics became apparent. His mathematics teacher
advised that Arthur be encouraged to pursue his studies in this area rather than
follow his father’s wishes to enter the family business as a merchant.
In 1838 Arthur began his studies at Trinity College, Cambridge, from where he
graduated in 1842. While still an undergraduate he had three papers published in
the newly founded Cambridge Mathematical Journal. For four years he taught at
Cambridge having won a Fellowship and, during this period, he published 28 papers.
A Cambridge Fellowship had a limited tenure so Cayley had to ﬁnd a profession.
He chose law and was admitted to the bar in 1849. He spent 14 years as a lawyer but
Cayley, although very skilled in conveyancing (his legal speciality), always considered
it as a means to make money so that he could pursue mathematics. During this
period he met Sylvester who was also in the legal profession. Both worked at the
courts of Lincoln’s Inn in London and discussed deep mathematical questions during
their working day. During these 14 years as a lawyer Cayley published about 250
mathematical papers!
In 1863 Cayley was appointed to the newly created Sadleirian professorship of
mathematics at Cambridge. Except for the year 1882, spent at the Johns Hopkins
University at the invitation of Sylvester, he remained at Cambridge until his death
in 1895.
6.2
Vectors in Space
The ideas developed so far can be easily generalized to vectors in space. For
example, a linear combination of vectors in space is again a vector in space.
We can also ﬁnd a basis for space. In fact, any three non-coplanar (not lying
in the same plane) vectors constitute a basis. To see this, let {a1, a2, a3} be
three such vectors drawn from a common point5 and assume that b is any
fourth vector in space. If b is along any of the a’s, we are done, because then
b is a multiple of that vector, i.e., a linear combination of the three vectors
5If the vectors are not originally drawn from the same point, we can transport them
parallel to themselves to a common point.

6.2 Vectors in Space
193
b
a1
a2
O
B
P
a3
Figure 6.5: Any vector in space can be written as a linear combination of three non-
coplanar vectors.
(with two coeﬃcients being zero). So assume that b is not along any of the
a’s. The plane formed by b and a3 intersects the plane of a1 and a2 along
a certain line common to both (see Figure 6.5). Draw a line from the tip of
b parallel to a3. This line will resolve b into a vector −−→
OB in the plane of
a1 and a2 and a vector −−→
BP parallel to a3. So, we write b = −−→
OB + α3a3.
Furthermore, since −−→
OB is in the plane of a1 and a2, it can be written as a
linear combination of these two vectors: −−→
OB = α1a1 + α2a2. Putting all of
this together, we get
b = α1a1 + α2a2 + α3a3.
This shows that
Box 6.2.1. The maximum number of linearly independent vectors in space
is three. Any three non-coplanar vectors form a basis for the space.
It follows that the space is a three-dimensional vector space.
In the previous section we introduced P1[t], the set of polynomials of ﬁrst
polynomials of
degree 2 or less
form a
3-dimensional
vector space.
degree, and showed that they could be treated as vectors. We even deﬁned
an inner product for these vectors, and from that, we calculated the length of
a vector and the angle between two vectors. This process can be generalized
to three dimensions. Let P2[t] be the set of polynomials of degree 2 (or less)
in the variable t. One can easily show that such a set, a typical element of
which looks like α0 +α1t+α2t2, has all the properties of arrows in space. We
shall use P2[t] as a prototype of vectors that are not directed line segments.
Clearly, {1, t, t2} form a basis for P2[t]; therefore, P2[t] is a three-dimensional
vector space.

194
Planar and Spatial Vectors
6.2.1
Transformation of Vectors
In the case of the plane, the machinery of matrices connected the components
of a vector in diﬀerent bases.
In the same context, we contrasted active
versus passive transformation. From now on, we want to concentrate on active
transformations, i.e., we consider transformations that alter the vectors rather
transformation of
vectors in space
lead to 3 × 3
matrices.
that the axes.
Consider a vector a with components(α1, α2, α3) in the basis B ≡{a1, a2,
a3}. If we transform this vector, it will acquire new components, (α′
1, α′
2, α′
3),
in the same basis B. We can therefore write
a = α1a1 + α2a2 + α3a3
and
a′ = α′
1a1 + α′
2a2 + α′
3a3,
(6.26)
where a′ is the transform of a. Now suppose that we transform both a and
the basis vectors in exactly the same manner. Then the components of the
transformed a will be the same in the new basis as the original a was in the
old basis:
a′ = α1a′
1 + α2a′
2 + α3a′
3.
(6.27)
Since B is a basis, any vector, in particular, the transformed basis vectors
can be written in terms of them:
a′
1 = a11a1 + a21a2 + a31a3,
a′
2 = a12a1 + a22a2 + a32a3,
(6.28)
a′
3 = a13a1 + a23a2 + a33a3.
Now substitute Equation (6.28) in the RHS of (6.27), and the second equation
of (6.26) in the LHS of (6.27) and rearrange terms to obtain
(α′
1 −a11α1 −a12α2 −a13α3)a1 + (α′
2 −a21α1 −a22α2 −a23α3)a2
+ (α′
3 −a31α1 −a32α2 −a33α3)a3 = 0.
The linear independence of a1, a2, and a3 gives
α′
1 = a11α1 + a12α2 + a13α3,
α′
2 = a21α1 + a22α2 + a23α3,
(6.29)
α′
3 = a31α1 + a32α2 + a33α3,
which, with the introduction of 3 × 1 (column), and 3 × 3 matrices, can be
written concisely as
⎛
⎝
α′
1
α′
2
α′
3
⎞
⎠=
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠
⎛
⎝
α1
α2
α3
⎞
⎠
or
a′ = Aa.
(6.30)
To know how a general vector transforms, we only need the transformation
matrix, namely the 3 × 3 matrix in Equation (6.30). This, in turn, is ob-
tained completely from the transformation of basis vectors as given in Equa-
tion (6.28). The reader should note, however, that the coeﬃcients in each line
of (6.28) appear as a column in the transformation matrix. Thus,

6.2 Vectors in Space
195
Box 6.2.2. To ﬁnd the transformation matrix, apply the transforma-
tion to the basis vectors, and write the transformed basis vectors in terms
of the old basis vectors. The “horizontal” coeﬃcients become the columns
of the transformation matrix.
Let us apply a transformation to a′ and to {a′
1, a′
2, a′
3}. We could denote
the new vectors by a second prime; but, then it would give the impression
that it is the same transformation as the earlier one. This is not the case.
Therefore, we use a new symbol “˘” to emphasize that the second transfor-
mations is of a completely diﬀerent nature, and denote the new transformed
vectors by ˘a′ and {˘a′
1, ˘a′
2, ˘a′
3}. In the basis {a1, a2, a3}, ˘a′ can be written as
˘a′ = α′′
1a1 + α′′
2a2 + α′′
3a3,
(6.31)
while the application of the new transformation to the second equation of
(6.26) gives
˘a′ = α′
1˘a1 + α′
2˘a2 + α′
3˘a3.
The vectors on the RHS can be written as a linear combination of {a1, a2, a3}:
˘a1 = a′
11a1 + a′
21a2 + a′
31a3,
˘a2 = a′
12a1 + a′
22a2 + a′
32a3,
(6.32)
˘a3 = a′
13a1 + a′
23a2 + a′
33a3.
Using the by-now-familiar procedure, we can relate the coeﬃcients as follows:
⎛
⎝
α′′
1
α′′
2
α′′
3
⎞
⎠=
⎛
⎝
a′
11
a′
12
a′
13
a′
21
a′
22
a′
23
a′
31
a′
32
a′
33
⎞
⎠
⎛
⎝
α′
1
α′
2
α′
3
⎞
⎠
or
a′′ = A′a′.
(6.33)
We can also ﬁnd how a′′ and a are related in two ways. The ﬁrst way
applies “˘” to both sides of Equations (6.27) and (6.28), substitutes (6.32)
in the transformed (6.28), and the result of this substitution in (6.27). This
will give ˘a′ as a linear combination of a1, a2, and a3. Equating this with
Equation (6.31) will give us a matrix relation between the a′′ and a. Second,
we can substitute the matrix relation of Equation (6.30) in that of (6.33)
and obtain a relation between the a′′ and a via the product of two matrices.
Comparison of these two relations will give us the rules of multiplication for
3 × 3 matrices which, except for the number of elements involved, is identical
to the multiplication rule for the 2 × 2 matrices. Similarly, the multiplication
by a row or a column vector, etc., is exactly as before.
There is a new kind of matrix associated with the space that we could not
consider in our discussion of the plane. Let B = {a1, a2, a3} be a basis for
the space, and take any two of the vectors in B, say a1 and a2. These two

196
Planar and Spatial Vectors
vectors form a plane any vector of which has only two components: If a is in
this plane, it can be written as
a = α1a1 + α2a2.
Now suppose we apply the same transformation to both a and {a1, a2}. Then,
on the one hand, a′ = α1a′
1+α2a′
2, and on the other hand, a′ = α′
1a1+α′
2a2+
α′
3a3, because the transformed a, in general, comes out of the plane of a1 and
a2. Therefore,
α1a′
1 + α2a′
2 = α′
1a1 + α′
2a2 + α′
3a3.
(6.34)
But we also have
a′
1 = a11a1 + a21a2 + a31a3,
a′
2 = a12a1 + a22a2 + a32a3.
Substituting these in Equation (6.34) yields
(α′
1 −a11α1 −a12α2)a1 +(α′
2 −a21α1 −a22α2)a2 +(α′
3 −a31α1 −a32α2)a3 = 0.
Linear independence of the vectors in B now gives
α′
1 = a11α1 + a12α2,
α′
2 = a21α1 + a22α2,
(6.35)
α′
3 = a31α1 + a32α2,
which can be written in matrix form as
⎛
⎝
α′
1
α′
2
α′
3
⎞
⎠=
⎛
⎝
a11
a12
a21
a22
a31
a32
⎞
⎠
α1
α2

or
a′ = Aa.
(6.36)
The matrix A is now a 3×2 matrix. It relates two-component column vectors
to three-component column vectors.
Example 6.2.1. Another way to illustrate the preceding discussion is to use ﬁrst
degree polynomials. Let us multiply all polynomials of P1[t] by a ﬁxed ﬁrst degree
polynomial, say 1 + t.
This will transform vectors of P1[t] into vectors of P2[t].
In particular, it will transform the basis {1, t} into vectors in P2[t] which can be
expressed as a linear combination of the basis vectors {1, t, t2} of P2[t]. Let f1 = 1,
f2 = t, and f3 = t2, and note that
f ′
1 = 1 · (1 + t) = 1 + t = 1 · f1 + 1 · f2 + 0 · f3,
f ′
2 = t(1 + t) = t + t2 = 0 · f1 + 1 · f2 + 1 · f3.
According to Box 6.2.2, the transformation matrix is
⎛
⎝
1
0
1
1
0
1
⎞
⎠

6.2 Vectors in Space
197
from which we can ﬁnd the transform of a general vector f = α0 + α1t in P1[t]. If
the transformed vector is written as f ′ = α′
0 + α′
1t + α′
2t2, then
⎛
⎝
α′
0
α′
1
α′
2
⎞
⎠=
⎛
⎝
1
0
1
1
0
1
⎞
⎠
α0
α1

.
This can be veriﬁed directly by multiplying f = α0 + α1t by 1 + t.
■
In the discussion above, we started with the plane (with two dimensions)
and transformed to space (with three dimensions). Example 6.2.1 illustrated
this transformation for P1[t] and P2[t]. We can also start with three dimen-
sions and end up in two dimensions. The result will be a matrix relation of
the form
α1
α2

=
b11
b12
b13
b21
b22
b23
 ⎛
⎝
α′
1
α′
2
α′
3
⎞
⎠
or
a = Ba′
(6.37)
with B a 2 × 3 matrix. The following example illustrates this point.
Example 6.2.2. Let us start with P2[t] and as transformation, consider diﬀeren-
tiation which acts on the basis {1, t, t2}. It is clear that the resulting vectors will
diﬀerentiation is a
(linear)
transformation.
belong to P1[t], because they will be linear combinations of 1 and t. With f1 = 1,
f2 = t, and f3 = t2, and using a prime to denote the transformed vector, we can
write
f ′
1 = d
dt(1) = 0 = 0 · f1 + 0 · f2,
f ′
2 = d
dt(t) = 1 = 1 · f1 + 0 · f2,
f ′
3 = d
dt(t2) = 2t = 0 · f1 + 2 · f2,
giving rise to the transformation matrix
0
1
0
0
0
2

.
The reader may verify that the coeﬃcients (α′
0, α′
1) in P1[t] of the derivative of an
arbitrary polynomial f(t) = α0 + α1t + α2t2 are given by
α′
0
α′
1

=
0
1
0
0
0
2
 ⎛
⎝
α0
α1
α2
⎞
⎠
which can also be obtained directly by diﬀerentiating f(t).
■
The point of this discussion is that if you have a collection of vectors with
various numbers of components, then it is possible to construct matrices that
relate the two sets of vectors. These matrices have diﬀerent numbers of rows
and columns. The mathematics of these new matrices, their notion of equality,
their addition, subtraction, multiplication, transposition, etc., is exactly the
same as before

198
Planar and Spatial Vectors
Example 6.2.3. Suppose
A =
⎛
⎝
1
−1
−1
2
0
1
⎞
⎠
and
B =
−1
0
1
1
2
−2

.
Then A + B is not deﬁned, but
At + B =
 1
−1
0
−1
2
1

+
−1
0
1
1
2
−2

=
0
−1
1
0
4
−1

and
A + Bt =
⎛
⎝
1
−1
−1
2
0
1
⎞
⎠+
⎛
⎝
−1
1
0
2
1
−2
⎞
⎠=
⎛
⎝
0
0
−1
4
1
−1
⎞
⎠= (At + B)t.
As for multiplication, we have
AB =
⎛
⎝
1
−1
−1
2
0
1
⎞
⎠
−1
0
1
1
2
−2

=
⎛
⎝
−2
−2
3
3
4
−5
1
2
−2
⎞
⎠
and
BA =
−1
0
1
1
2
−2
 ⎛
⎝
1
−1
−1
2
0
1
⎞
⎠=
−1
2
−1
1

,
where the element in the ith row and jth column of the product is obtained by
multiplying the ith row of the left factor by the jth row of the right factor term-by-
term and adding the products (see Box 6.1.3).
■
The 3 × 3 matrix
1 ≡
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠
is the 3×3 identity matrix (or unit matrix), and has the property that when it
multiplies any other 3×3 matrix on either side, the latter does not get aﬀected.
Similarly, when this identity matrix multiplies a three-column vector on the
left or a three-row vector on the right, it does not aﬀect them. As in the case
of the plane, the unit matrix is used to deﬁne the inverse of a matrix A as a
matrix B that multiplies A on either side and gives the unit matrix.
6.2.2
Inner Product
As in the case of two dimensions, the usual rule of the dot product of space
vectors in terms of their components along ˆex, ˆey, and ˆez does not apply in
the general case. For that, we need an inner product matrix G. As in the
plane, this is a matrix whose elements are dot products of the basis vectors.
If B = {a1, a2, a3} is a basis for space, then G is a 3 × 3 symmetric matrix
G =
⎛
⎝
g11
g12
g13
g21
g22
g23
g31
g32
g33
⎞
⎠,
gij = gji = ai · aj,
i, j = 1, 2, 3.
(6.38)

6.2 Vectors in Space
199
Example 6.2.4. Let us ﬁnd the inner product matrix for the basis {1, t, t2} of
P2[t] when the inner product integration is from 0 to 1. Because of the symmetry of
the matrix and the fact that we have already calculated the 2×2 submatrix of G, we
need to ﬁnd g13, g23, and g33. Let f1 = f1(t) = 1, f2 = f2(t) = t, and f3 = f3(t) = t2;
then
g13 = f1 · f3 =
# 1
0
f1(t)f3(t) dt =
# 1
0
t2dt = 1
3,
g23 = f2 · f3 =
# 1
0
f2(t)f3(t) dt =
# 1
0
t3dt = 1
4,
g33 = f3 · f3 =
# 1
0
f3(t)f3(t) dt =
# 1
0
t4dt = 1
5.
It follows that
G =
⎛
⎜
⎝
1
1
2
1
3
1
2
1
3
1
4
1
3
1
4
1
5
⎞
⎟
⎠.
This matrix can be used to ﬁnd the dot product of any two vectors in terms of their
components in the basis {1, t, t2} of P2[t].
■
If a and b have components (α1, α2, α3) and (β1, β2, β3) in B, then their
inner product is given by
8aGb =
 
α1
α2
α3
!
⎛
⎝
g11
g12
g13
g21
g22
g23
g31
g32
g33
⎞
⎠
⎛
⎝
β1
β2
β3
⎞
⎠.
(6.39)
If this expression is zero, we say that a and b are G-orthogonal. For an or-
G-orthogonal
vectors in space
thonormal basis, the inner product matrix G becomes the unit matrix6 and
we recover the usual inner product of space vectors in terms of components.
As discussed in the case of the plane, every point in space can be thought
of as the tip of a vector whose tail is the origin. Then, we can express the
(G-dependent) distance between two points in terms of vectors. Let r1 be
the vector to point P1 and r2 the vector to point P2. Then the length of the
displacement vector is the “distance” between P1 and P2:
Δr · Δr = (r1 −r2) · (r1 −r2) = (9
Δr)G(Δr).
(6.40)
Recall that only in the positive deﬁnite case is P1P2
2 nonnegative.
As in the case of the plane, it is convenient to construct orthonormal basis
vectors in space. This can be done by the Gram–Schmidt process. Suppose
Gram–Schmidt
process for vectors
in space
B = {a1, a2, a3} is a basis for space as shown in Figure 6.6. Again, to avoid
complications, we assume that the inner product is positive deﬁnite, so that
the inner product of every nonzero vector with itself is positive. We know
how to construct two orthonormal vectors out of {a1, a2}; we did that in
6Only if the inner product is positive deﬁnite.

200
Planar and Spatial Vectors
(a)
(c)
(b)
a 3
e 2
^
e 1
^
e 2
^
e 1
^
a 3
′
e 3
^
e 2
^
e 1
^
a 3
′
Figure 6.6: The Gram–Schmidt process for three linearly independent vectors in space.
our discussion of the plane. Call these new orthonormal vectors {ˆe1, ˆe2} and
construct the vector a′
3,
a′
3 = a3 −(a3 · ˆe1)ˆe1 −(a3 · ˆe2)ˆe2
which is obtained from a3 by taking away its projections along ˆe1 and ˆe2.
Now note that
ˆe1 · a′
3 = ˆe1 · a3 −(a3 · ˆe1) ˆe1 · ˆe1
  
=1
−(a3 · ˆe2) ˆe2 · ˆe1
  
=0
= 0,
ˆe2 · a′
3 = ˆe2 · a3 −(a3 · ˆe1) ˆe2 · ˆe1
  
=0
−(a3 · ˆe2) ˆe2 · ˆe2
  
=1
= 0,
i.e., a′
3 is orthogonal to both ˆe1 and ˆe2. This suggests deﬁning ˆe3 as
ˆe3 ≡a′
3
|a′
3| =
a′
3
	
a′
3 · a′
3
.
The reader should note that in the construction of {ˆe1, ˆe2, ˆe3}, we have simply
taken the linear combination of a1, a2, and a3.
Transformations that leave the inner products unchanged can be obtained
in exactly the same way as for the plane. For A to preserve the inner product,
we need to have
G-orthogonal
matrices
8AGA = G,
(6.41)
i.e., it has to be G-orthogonal. If G is the identity matrix, then A can be
thought of as a rigid rotation and is simply called orthogonal; it satisﬁes
8AA = 1.
(6.42)
If we write A as
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠,

6.2 Vectors in Space
201
then Equation (6.42) can be written as
⎛
⎝
a11
a21
a31
a12
a22
a32
a13
a23
a33
⎞
⎠
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠=
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠.
(6.43)
It is clear from Equation (6.43) that the columns of the matrix A, considered
as vectors, have unit length and are orthogonal to other columns in the usual
positive deﬁnite inner product.7 This is why A is called orthogonal.
The product on the LHS of Equation (6.43) is a 3×3 matrix whose elements
must equal the corresponding elements of the unit matrix on the RHS. For
example,
a2
11 + a2
21 + a2
31 = 1.
(6.44)
Similarly, the equality of the elements located in the ﬁrst row and second
column on both sides gives
a11a12 + a21a22 + a31a32 = 0
and so on. Thus we obtain nine equations. However, simple inspection of these
equations reveals that only six of them are independent. Therefore, we can
orthogonal
matrices in space
are determined by
three parameters
such as the Euler
angles.
only solve for the nine unknowns in terms of three of them (see Section 7.6).
It does not matter which three matrix elements we choose. If we choose a11,
a21, and a31, for example, then Equation (6.44) reveals that these parameters
can be sines and cosines. What this means physically is that Three parameters
are required to specify a rigid rotation of the axes.
There are many ways to specify these three parameters.
One of the
most useful and convenient ways is by using Euler angles ψ, ϕ, and θ (see
Euler angles
Figure 6.7). Example 6.2.5 below shows that in terms of these angles, the
matrix A can be written as
A =
⎛
⎝
cos ψ cos ϕ−sin ψ cos θ sin ϕ
−cos ψ sin ϕ−sin ψ cos θ cos ϕ
sin ψ sin θ
sin ψ cos ϕ+cos ψ cos θ sin ϕ
−sin ψ sin ϕ+cos ψ cos θ cos ϕ
−cos ψ sin θ
sin θ sin ϕ
sin θ cos ϕ
cos θ
⎞
⎠.
It is straightforward to verify that AtA = 1. Euler angles are useful in de-
scribing the rotational motion of a rigid body in mechanics.
Example 6.2.5. From Figure 6.7 it should be clear that the primed basis is ob-
a general
orthogonal matrix
in space can be
written as the
product of three
successive
rotations.
tained from the basis {ˆe1, ˆe2, ˆe3} by the following three operations.
(a) Rotate the coordinate system about the ˆe3-axis through angle ϕ. This corre-
sponds to a rotation in the ˆe1ˆe2-plane, leaving the ˆe3-axis unchanged. We saw in
the previous section how the 2 × 2 part of the matrix looked like. The complete
3 × 3 matrix corresponding to such a rotation is
A1 =
⎛
⎝
cos ϕ
−sin ϕ
0
sin ϕ
cos ϕ
0
0
0
1
⎞
⎠.
(6.45)
7This holds for 2 × 2 orthogonal matrices as well.

202
Planar and Spatial Vectors
ˆe 1
ˆe 2
ˆe 3
ˆ′ 
e 3
ˆ′ 
e 2
ˆ′ 
e 1
ϕ
ψ
θ
ξ
Figure 6.7: The Euler angles and the rotations about three axes making up a general
rotation in space.
It is clear that this matrix leaves the third (z) component of a column vector un-
changed while rotating the ﬁrst two (x and y) components by ϕ.
(b) Rotate the new coordinate system around the new ˆe1-axis (the ξ-axis in the
ﬁgure) through an angle θ. The corresponding matrix is
A2 =
⎛
⎝
1
0
0
0
cos θ
−sin θ
0
sin θ
cos θ
⎞
⎠.
(6.46)
(c) Rotate the system about the new ˆe3-axis (the ˆe′
3-axis in the ﬁgure) through an
angle ψ. The corresponding matrix is
A3 =
⎛
⎝
cos ψ
−sin ψ
0
sin ψ
cos ψ
0
0
0
1
⎞
⎠.
(6.47)
It is easily veriﬁed that A = A3A2A1, i.e., the rotation A has the same eﬀect as that
of A1, A2, and A3 performed in succession.
■
6.3
Determinant
Matrices have found application in many diverse ﬁelds of pure and applied
from matrices to
systems of linear
equations to
determinants
mathematics. One such application is in the solution of linear equations. Con-
sider
the
ﬁrst
set
of
equations
in
which
we
introduced
matrices,
Equations (6.4) and (6.5). The ﬁrst of these equations associates a pair of
numbers (α′
1, α′
2) to a given pair (α1, α2), i.e., if we know the latter pair,
Equation (6.4) gives the former. What if we treat (α1, α2) as unknown? Un-
der what conditions can we ﬁnd these unknowns in terms of the known pair
(α′
1, α′
2)? Let us use a more suggestive notation and write Equation (6.4) as
a11x + a12y = b1,
a21x + a22y = b2.
(6.48)

6.3 Determinant
203
We want to investigate conditions under which a pair (x, y) exists which sat-
isﬁes Equation (6.48). Let us assume that none of the aij’s is zero. The case
in which one of them is zero is included in the ﬁnal conclusion we are about
to draw. Multiply the ﬁrst equation of (6.48) by a22 and the second by a12
and subtract the resulting two equations. This yields (a11a22 −a12a21)x =
a22b1 −a12b2, which has a solution for x of the form
x =
a22b1 −a12b2
a11a22 −a12a21
≡a22b1 −a12b2
det A
(6.49)
if a11a22 −a12a21 ̸= 0. In the last equality we have deﬁned the determinant
determinant of a
2 × 2 matrix
of A:
A =
a11
a12
a21
a22

⇒det A ≡a11a22 −a12a21.
(6.50)
We can also ﬁnd y. Multiply the ﬁrst equation of (6.48) by a21 and the second
by a11 and subtract the resulting two equations. This yields
(a11a22 −a12a21)y = a11b2 −a21b1
which has a solution for y of the form
y = a11b2 −a21b1
det A
.
(6.51)
We can combine Equations (6.49) and (6.51) into a single matrix equation:
x
y

=
1
det A
 a22
−a12
−a21
a11
 b1
b2

.
(6.52)
This is the inverse of the matrix form of Equation (6.48). Indeed if we had
written that equation in the form Ax = b, and if A had an inverse, say B,
then we could have multiplied both sides of the equation by B and obtained
BA

=1
x = Bb ⇒x = Bb.
This is precisely what we have in Equation (6.52)! Is the matrix multiplying
the column vector b the inverse of A? Let us ﬁnd out
1
det A

a22
−a12
−a21
a11
 
a11
a12
a21
a22

=
1
det A
a22a11 −a12a21
0
0
−a21a12 + a11a22

=
1
0
0
1

.
So, it is indeed the inverse of A. We denote this inverse by A−1.
Theorem 6.3.1. A matrix A =
a11
a12
a21
a22

has an inverse if and only if
its determinant, deﬁned by det A ≡a11a22 −a12a21, is not zero, in which case
A−1 =
1
det A
 a22
−a12
−a21
a11

.

204
Planar and Spatial Vectors
The reader may verify that, not only A−1A = 1, but also AA−1 = 1.
Equation (6.48) gives the components b1 and b2 of a new vector obtained
from an old vector with components x and y when the matrix A acts on the
latter. We want to see what conditions A must satisfy for it to transform
vectors in a basis into vectors of a new basis. Let B = {a1, a2} be the old
basis. The components of a1 in B are x = 1 and y = 0; so by (6.48), a′
1,
the vector obtained from a1 by the action of A, has components b1 = a11 and
b2 = a21. The components of a2 in B are x = 0 and y = 1; so a′
2, the vector
obtained from a2 by the action of A, has components c1 = a12 and c2 = a22.
The vectors (b1, b2) and (c1, c2) form a basis if and only if they are linearly
independent, i.e.,
(b1, b2) = k(c1, c2) = (kc1, kc2) ⇒b1 = kc1,
b2 = kc2,
does not hold for any constant k. This is equivalent to saying that
b1
c1
̸= b2
c2
or
b1c2 −b2c1 ̸= 0.
Expressing the b’s and c’s in terms of aij’s, we recognize the last relation as
a condition on the determinant of A. Using Theorem 6.3.1, we thus have
Box 6.3.1. A transformation (or a matrix) transforms a basis into an-
other basis if and only if it is invertible.
Let us now consider three equations in three unknowns:
a11x + a12y + a13z = b1,
a21x + a22y + a23z = b2,
(6.53)
a31x + a32y + a33z = b3,
which can also be written in matrix form as
⎛
⎝
a11
a12
a13
a21
a22
a23
a31
a32
a33
⎞
⎠
⎛
⎝
x
y
z
⎞
⎠=
⎛
⎝
b1
b2
b3
⎞
⎠⇒Ax = b.
(6.54)
We eliminate z from the set of equations by multiplying the ﬁrst equation
of (6.53) by a23 and the second by a13 and subtracting. This will give one
from three
equations in three
unknowns to two
equations in two
unknowns, and
from the
determinant of a
2 × 2 matrix to
that of a 3 × 3
matrix
equation in x and y. Similarly, multiplying the ﬁrst equation by a33 and the
third by a13 and subtracting gives another equation in x and y. These two
equations are
(a11a23 −a21a13)



≡a11
x + (a12a23 −a22a13)



≡a12
y = a23b1 −a13b2



≡b1
,
(a11a33 −a31a13)



≡a21
x + (a12a33 −a32a13)



≡a22
y = a33b1 −a13b3



≡b2
.
(6.55)

6.3 Determinant
205
Thus, we have reduced the three equations in three unknowns to two equa-
tions in two unknowns.
We know how to ﬁnd the solution for this set of
equations. These solutions are given in Equations (6.49) and (6.51). In order
for this equation to have a solution, the determinant of the coeﬃcients must
not vanish. Let us calculate this determinant:
a11a22 −a12a21 = (a11a23 −a21a13)(a12a33 −a32a13)
−(a12a23 −a22a13)(a11a33 −a31a13)
= a11a23a12a33 −a11a23a32a13 −a21a13a12a33 + a21a13a32a13
−a12a23a11a33 + a12a23a31a13 + a22a13a11a33 −a22a13a31a13
= a13[a11(a22a33 −a23a32) −a12(a21a33 −a31a23)
+ a13(a21a32 −a22a31)]
= a13

a11 det
a22
a23
a32
a33

−a12 det
a21
a23
a31
a33

+ a13 det
a21
a22
a31
a32

.
If the original set of equations is to have a solution, the expression in the
square brackets must not vanish.
We call this expression the determinant
of the 3 × 3 matrix A. We can give a cookbook recipe for calculating the
determinant; but ﬁrst we need the following deﬁnition:
Box 6.3.2. The cofactor Aij of an element aij of a matrix A is deﬁned
as the product of (−1)i+j (i.e., +1 if i + j is even and −1 if i + j is odd)
and the determinant of the smaller matrix (2 × 2, if A is a 3 × 3 matrix)
obtained from A when its ith row and jth column are deleted.
The following recipe applies to any (square) matrix, not just to 3 × 3
matrices:
Box 6.3.3. The determinant of A is obtained by multiplying each ele-
ment of a row (or a column) by its cofactor and adding the products.
If det A ̸= 0, then Equation (6.49) gives
x = a22b1 −a12b2
a13 det A
.
The numerator is
a22b1 −a12b2 = (a12a33 −a32a13)(a23b1 −a13b2)
−(a12a23 −a22a13)(a33b1 −a13b3)
= a13
.
(a22a33 −a32a23)



≡C11
b1 + (a32a13 −a12a33)



≡C12
b2 + (a12a23 −a22a13)



≡C13
b3
/
= a13(C11b1 + C12b2 + C13b3).

206
Planar and Spatial Vectors
Therefore,
x = C11b1 + C12b2 + C13b3
det A
.
(6.56)
Similarly, using Equation (6.51), we ﬁnd
y = a11b2 −a21b1
a13 det A
with
a11b2 −a21b1 = (a11a23 −a21a13)(a33b1 −a13b3)
−(a11a33 −a31a13)(a23b1 −a13b2)
= a13
.
(a31a23 −a21a33)



≡C21
b1 + (a11a33 −a31a13)



≡C22
b2 + (a21a13 −a11a23)



≡C23
b3
/
= a13(C21b1 + C22b2 + C23b3),
so that
y = C21b1 + C22b2 + C23b3
det A
.
(6.57)
With x and y thus determined, we can substitute them in any of the three
original equations and ﬁnd z. Let us use the ﬁrst equation; then
z = b1 −a11x −a12y
a13
=
b1 −a11 C11b1 + C12b2 + C13b3
det A
−a12 C21b1 + C22b2 + C23b3
det A
a13
= b1(det A −a11C11 −a12C21) −b2(a11C12 + a12C22) −b3(a11C13 + a12C23)
a13 det A
.
The numerator N can be calculated:
N = b1[a11(a22a33 −a23a32) −a12(a21a33 −a31a23) + a13(a21a32 −a22a31)
−a11(a22a33 −a32a23) −a12(a31a23 −a21a33)]
−b2[a11(a32a13 −a12a33) + a12(a11a33 −a31a13)]
−b3[a11(a12a23 −a22a13) + a12(a21a13 −a11a23)]
= a13
.
(a21a32 −a22a31)



≡C31
b1 + (a12a31 −a11a32)



≡C32
b2 + (a11a22 −a12a21)



≡C33
b3
/
= a13(C31b1 + C32b2 + C33b3).
It now follows that
z = C31b1 + C32b2 + C33b3
det A
.
(6.58)
We can put Equations (6.56), (6.57), and (6.58) in matrix form:
⎛
⎝
x
y
z
⎞
⎠=
1
det A
⎛
⎝
C11
C12
C13
C21
C22
C23
C31
C32
C33
⎞
⎠
⎛
⎝
b1
b2
b3
⎞
⎠⇒x =
1
det ACb.
(6.59)

6.4 The Jacobian
207
This is the inverse of Equation (6.54). The reader may verify that multiplying
A on either side of C/ det A yields the identity matrix, so that C/ det A is indeed
the inverse of A. The rule for calculating this inverse is as follows. Construct
a matrix out of the cofactors and denote it by A:
A ≡
⎛
⎝
A11
A12
A13
A21
A22
A23
A31
A32
A33
⎞
⎠
(6.60)
and note that
⎛
⎝
C11
C12
C13
C21
C22
C23
C31
C32
C33
⎞
⎠= 8A
so, we obtain the important result
inverse of a 3 × 3
matrix
A−1 =
1
det A
8A =
1
det A
⎛
⎝
A11
A21
A31
A12
A22
A32
A13
A23
A33
⎞
⎠.
(6.61)
Equation (6.61), although derived for a 3 × 3 matrix, applies to all matrices,
including a 2 × 2 one whose inverse was given in Theorem 6.3.1, as the reader
is asked to verify.
As in the case of 2 × 2 matrices, a transformation in space that takes a
basis onto another basis is invertible.
6.4
The Jacobian
With the machinery of determinants at our disposal, we can formalize the
geometric construction of area and volume elements in Chapter 2 to a pro-
cedure which can be used for all coordinate transformations. We start with
two dimensions and consider the coordinate transformation
x = f(u, v),
y = g(u, v).
(6.62)
Our goal is to write the element of area in the (u, v) coordinate system. This
is the area formed by inﬁnitesimal elements in the direction of u and v, i.e.,
elements in the direction of the primary curves of the (u, v) coordinate system.
For an arbitrary change du and dv in u and v, the Cartesian coordinates
change as follows:
dx = ∂f
∂u du + ∂f
∂v dv,
dy = ∂g
∂u du + ∂g
∂v dv.

208
Planar and Spatial Vectors
The element in the direction of the ﬁrst primary curve is obtained by holding
v constant and letting u vary. This corresponds to setting dv = 0 in the above
equations. It follows that the ﬁrst primary (vector) length element is
d⃗l1 = ˆex dx1 + ˆey dy1 = ˆex
∂f
∂u du + ˆey
∂g
∂u du.
(6.63)
Similarly, the second primary (vector) length element, obtained by ﬁxing u
and letting v vary, is
d⃗l2 = ˆex dx2 + ˆey dy2 = ˆex
∂f
∂v dv + ˆey
∂g
∂v dv.
(6.64)
When we derived the elements of area and volume in the three coordinate
systems in Chapter 2, we used the fact that the set of unit vectors in each
system were mutually perpendicular. Therefore, the area and volume elements
were obtained by mere multiplication of length elements. We are not assuming
that ˆeu and ˆev are perpendicular. Thus, we cannot simply multiply the lengths
to get the area. However, we can use the result of Example 1.1.2 which gives
the area of a parallelogram formed by two non-collinear vectors. Writing the
cross product in terms of the determinant, we have
d⃗l1 × d⃗l2 = det
⎛
⎜
⎜
⎜
⎜
⎝
ˆex
ˆey
ˆez
∂f
∂u du
∂g
∂u du
0
∂f
∂v dv
∂g
∂v dv
0
⎞
⎟
⎟
⎟
⎟
⎠
= ˆez det
⎛
⎝
∂f
∂u
∂g
∂u
∂f
∂v
∂g
∂v
⎞
⎠du dv
and the area is simply the absolute value of this cross product:
Jacobian matrix
and Jacobian
da =






det
⎛
⎝
∂f
∂u
∂g
∂u
∂f
∂v
∂g
∂v
⎞
⎠






du dv ≡






∂x
∂u
∂y
∂u
∂x
∂v
∂y
∂v






du dv,
(6.65)
where we substituted x and y for f and g and introduced a new notation
for the (absolute value of the) determinant. The matrix whose determinant
multiplies du dv is called the Jacobian matrix, and the absolute value of its
determinant, the Jacobian.
Example 6.4.1. Let us apply Equation (6.65) to polar coordinates. The trans-
formation is
x = f(r, θ) = r cos θ,
y = g(r, θ) = r sin θ.
This gives
∂x
∂r = ∂f
∂r = cos θ,
∂x
∂θ = ∂f
∂θ = −r sin θ,
∂y
∂r = ∂g
∂r = sin θ,
∂y
∂θ = ∂g
∂θ = r cos θ,

6.4 The Jacobian
209
and
da =







∂x
∂r
∂y
∂r
∂x
∂θ
∂y
∂θ







dr dθ =




cos θ
sin θ
−r sin θ
r cos θ




 dr dθ
= (r cos2 θ + r sin2 θ) dr dθ = r dr dθ,
which is the familiar element of area in polar coordinates.
■
The procedure discussed above for two dimensions can be generalized to
three dimensions using the result of Example 1.1.3 which gives the volume of a
parallelepiped formed by three non-coplanar vectors. Suppose the coordinate
transformations are of the form
x = f(u, v, w),
y = g(u, v, w),
z = h(u, v, w).
Then
dx = ∂f
∂u du + ∂f
∂v dv + ∂f
∂w dw,
dy = ∂g
∂u du + ∂g
∂v dv + ∂g
∂w dw,
dz = ∂h
∂u du + ∂h
∂v dv + ∂h
∂w dw.
The ﬁrst primary element of length is obtained by ﬁxing v and w and
allowing u to vary; similarly for the second and third primary elements of
length. We therefore have
d⃗l1 = ˆex dx1 + ˆey dy1 + ˆez dz1 = ˆex
∂f
∂u du + ˆey
∂g
∂u du + ˆez
∂h
∂u du,
d⃗l2 = ˆex dx2 + ˆey dy2 + ˆez dz2 = ˆex
∂f
∂v dv + ˆey
∂g
∂v dv + ˆez
∂h
∂v dv,
d⃗l3 = ˆex dx3 + ˆey dy3 + ˆez dz3 = ˆex
∂f
∂w dw + ˆey
∂g
∂w dw + ˆez
∂h
∂w dw.
Example 1.1.3 now yields
dV =



d⃗l1 · (d⃗l2 × d⃗l3)



 =











det
⎛
⎜
⎜
⎜
⎜
⎜
⎝
∂f
∂u du
∂g
∂u du
∂h
∂u du
∂f
∂v dv
∂g
∂v dv
∂h
∂v dv
∂f
∂w dw
∂g
∂w dw
∂h
∂w dw
⎞
⎟
⎟
⎟
⎟
⎟
⎠











.
We summarize the foregoing argument in
Theorem 6.4.2. For the coordinates u, v, and w, related to the Cartesian
coordinates by x = f(u, v, w), y = g(u, v, w), and z = h(u, v, w), the volume
element is given by

210
Planar and Spatial Vectors
dV =










∂x
∂u
∂y
∂u
∂z
∂u
∂x
∂v
∂y
∂v
∂z
∂v
∂x
∂w
∂y
∂w
∂z
∂w










du dv dw.
(6.66)
The (absolute value of the) determinant multiplying du dv dw is called the
Jacobian deﬁned
Jacobian of the coordinate transformation.
Historical Notes
Determinants were mathematical objects created in the process of solving a system
of linear equations. As early as 1693 Leibniz used a systematic set of indices for the
coeﬃcients of a system of three equations in two unknowns. By eliminating the two
unknowns from the set of three equations, he obtained an expression involving the
coeﬃcients that “determined” whether a solution existed for the set of equations.
The solution of simultaneous linear equations in two, three, and four unknowns
by the method of determinants was created by Maclaurin around 1729. Though
not as good in notation, his rule is the one we use today and which Cramer used
in connection with his study of the conic sections. In 1764, Bezout systematized
the process of determining the signs of the terms of a determinant for n equations
in n unknowns and showed that the vanishing of the determinant is a necessary
condition for nonzero solutions to exist.
Vandermonde was the ﬁrst to give a connected and logical exposition of the
theory of determinants detached from any system of linear equations, although he
used his theory mostly as applied to such systems. He also gave a rule for expanding
a determinant by using second-order minors and their complementary minors. In
the sense that he concentrated on determinants, he is aptly considered the founder
of the theory.
One of the consistent workers in determinant theory over a period of over ﬁfty
years was James Joseph Sylvester.
In 1833 he became a student at St. John’s College, Cambridge, and took the
diﬃcult tripos examination in the same year along with two other famous math-
ematicians, Gregory and Green (the creator of the important Green’s functions).
Sylvester came second, Green who was 20 years older than the other two came fourth
with Duncan Gregory ﬁfth. (The ﬁrst-place winner did little work of importance
after graduating.)
James Joseph
Sylvester
1814–1897
At this time it was necessary for a student to sign a religious oath to the Church
of England before graduating and Sylvester, being Jewish, refused to take the oath,
so could not graduate. For the same reason he was not eligible for a Smith’s prize
nor for a Fellowship.
From 1838 Sylvester started to teach physics at the University of London, one
of the few places which did not bar him because of his religion. Three years later
he was appointed to a chair in the University of Virginia but he resigned after a few
months. A student who had been reading a newspaper in one of Sylvester’s lectures
insulted him and Sylvester struck him with a sword stick. The student collapsed in
shock and Sylvester believed (wrongly) that he had killed him. He ﬂed to New York
boarding the ﬁrst available ship back to England.
On his return, Sylvester worked as an actuary and lawyer but gave private
mathematics lessons. His pupils included Florence Nightingale. By good fortune

6.5 Problems
211
Cayley was also a lawyer, and both worked at the courts of Lincoln’s Inn in London.
Cayley and Sylvester discussed mathematics as they walked around the courts and,
although very diﬀerent in temperament, they became life-long friends.
Sylvester tried hard to return to mathematics as a profession, and he applied
unsuccessfully for a lectureship in geometry at Gresham College, London, in 1854.
Another failed application was for the chair in mathematics at the Royal Military
Academy at Woolwich, but, after the successful applicant died within a few months
of being appointed, Sylvester became professor of mathematics at Woolwich. Being
at a military academy, Sylvester had to retire at age 55. At ﬁrst it looked as though
he might give up mathematics since he had published his only book at this time,
and it was on poetry. Apparently Sylvester was proud of this work, entitled The
Laws of Verse, since after this he sometimes signed himself “J. J. Sylvester, author
of The Laws of Verse.”
In 1877 Sylvester accepted a chair at the Johns Hopkins University and founded
in 1878 the American Journal of Mathematics, the ﬁrst mathematical journal in the
USA.
In 1883 Sylvester, although 68 years old at this time, was appointed to the
Savilian chair of geometry at Oxford. However he only liked to lecture on his own
research and this was not well liked at Oxford where students wanted only to do well
in examinations. In 1892, at the age of 78, Oxford appointed a deputy professor
in his place and Sylvester, by this time partially blind and suﬀering from loss of
memory, returned to London where he spent his last years at the Athenaeum Club.
Sylvester did important work on matrix and determinant theory, a topic in which
he became interested during the walks with Cayley while they were at the courts
of Lincoln’s Inn. In particular he used matrix theory to study higher-dimensional
geometry. He also devised an improved method of determining conditions under
which a system of polynomial equations has a solution.
The formula for the derivative of a determinant when the elements are functions
of a variable was ﬁrst given in 1841 by Jacobi who had earlier used them in the
change of variables in a multiple integral. In this context the determinant is called
the Jacobian of the transformation (as discussed in the current section of this book).
6.5
Problems
6.1. What vector is obtained when the vector a2 of a basis {a1, a2} is actively
transformed with the matrix
 0 1
0 0
!
.
6.2. Show that the nonzero matrix A =
 1 0
0 0
!
cannot have an inverse. Hint:
Suppose that B =
 a b
c d
!
is the inverse of A. Calculate AB and BA, set them
equal to the unit matrix and show that no solution exists for a, b, c, and d.
6.3. Let A =
 a1 b1
c1 d1
!
and B =
 a2 b2
c2 d2
!
be arbitrary matrices. Find AB, At,
and Bt and show that (AB)t = BtAt.
6.4. Find the angle between 1 + t and 1 −t when the inner product is inte-
gration over the interval (0, 1).
6.5. Instead of (0, 1), choose (−1, 1) as the interval of integration for P1[t].
From the basis {1, t}, construct an orthonormal basis using the Gram–Schmidt
process.

212
Planar and Spatial Vectors
6.6. Take the interval of the integration to be (−1, +1), and ﬁnd the inner
product matrix for the basis {1, t} of P1[t].
6.7. Find the angle between two vectors a and b, whose components in an
orthonormal basis are, respectively, (1, 2) and (2, −3). Use the Gram–Schmidt
process to ﬁnd the orthonormal vectors obtained from a and b.
6.8. Use the Gram–Schmidt process to ﬁnd an orthonormal basis in three
dimensions from each of the following:
(a) (−1, 1, 1), (1, −1, 1), (1, 1, −1)
(b) (1, 2, 2), (0, 0, 1), (0, 1, 0)
6.9. (a) Find the inner product matrix associated with the basis vectors
a1 = ˆex + ˆey, a2 = ˆex + ˆez, and a3 = ˆey + ˆez.
(b) Calculate the inner product of two vectors a and b, whose components in
the basis above are, respectively, (1, −1, 2) and (0, 2, 3).
(c) Use the Gram–Schmidt process to ﬁnd three orthonormal vectors out of
the basis of (a).
6.10. Use Gram–Schmidt process to ﬁnd orthonormal vectors out of the three
vectors (2, −1, 3), (−1, 1, −2), and (3, 1, 2).
What do you get as the last
vector?
What can you say about the linear independence of the original
vectors?
6.11. What is the angle between the second and fourth vectors in the standard
basis of P3[t] when the interval of integration of the inner product is (0, 1)?
Between the ﬁrst and fourth vectors?
6.12. Calculate the inner product matrix for the standard basis of P3[t] when
the interval of integration of the inner product is (−1, +1). Now ﬁnd the angle
between all vectors in that basis.
6.13. The inner product matrix in a basis {a1, a2} is given by
G =
 2
−1
−1
3

.
(a) Calculate the cosine of the angle between a1 and a2.
(b) Suppose that a = −a1 + a2 and b = 2a1 −a2. Calculate |a|, |b|, a · b,
and the cosine of the angle between a and b.
6.14. Let a1 = 1 + t and a2 = 1 −t be a basis of P1[t]. Deﬁne the inner
product as the integral of products of polynomials over the interval (0, a) with
a > 0.
(a) Determine a such that a1 and a2 are orthogonal.
(b) Given this value of a, calculate |a1| and |a2|.
(c) Find two orthogonal polynomials {ˆe1, ˆe2} of unit length that form a basis
for P1[t].
(d) Write the polynomial b = 3 −2t as a linear combination of ˆe1 and ˆe2.
(e) Calculate b · b using the deﬁnition of the inner product.
(f) Calculate b · b by squaring (and then adding) the components in {ˆe1, ˆe2}.

6.5 Problems
213
6.15. Show that the matrix C deﬁned in Equations (6.56)–(6.59) is indeed
the transpose of the matrix A of cofactors of A.
6.16. Show directly that the matrix given in Equation (6.61) is indeed the
inverse of the matrix A.
6.17. From the transformation rules (1.8) and (1.9) giving the Cartesian
coordinates as functions of cylindrical and spherical coordinates, and using
the Jacobian (6.66), ﬁnd the volume elements in cylindrical and spherical
coordinates
6.18. The elliptic coordinates are given by
x = a cosh u cosθ
y = a sinh u sin θ.
Using the Jacobian for two variables (6.65), ﬁnd the element of area for the
elliptic coordinate system.
6.19. The elliptic cylindrical coordinates are given by
x = a coshu cos θ
y = a sinh u sin θ
z = z
Using the Jacobian for three variables (6.66), ﬁnd the element of volume for
the elliptic cylindrical coordinate system.
6.20. The prolate spheroidal coordinates are given by
x = a sinh u sin θ cos ϕ
y = a sinh u sin θ sin ϕ
z = a cosh u cosθ
Using the Jacobian for three variables (6.66), ﬁnd the element of volume for
the prolate spheroidal coordinate system.
6.21. The toroidal coordinates are given by
x = a sinh θ cos ϕ
cosh θ −cos u
y = a sinh θ sin ϕ
cosh θ −cos u
z =
a sin u
cosh θ −cos u
Using the Jacobian for three variables (6.66), ﬁnd the element of volume for
the toroidal coordinate system.

214
Planar and Spatial Vectors
6.22. A coordinate system (R, Θ, φ) in space is deﬁned by
x = R cos Θ cosφ + b cosφ
y = R cos Θ sin φ + b sin φ
z = R sin Θ
where b is a constant, and 0 < R < b. Using the Jacobian for three variables
(6.66), ﬁnd the element of volume for this coordinate system.

Chapter 7
Finite-Dimensional Vector
Spaces
Human visual perception of dimension is limited to two and three, the plane
and space. However, his mental perception, and his ability to abstract, rec-
ognizes no bounds. If this abstraction were a mere useless mental exercise,
we would not bother to add this chapter to the book.
It is an intriguing
coincidence that Nature plays along with the tune of human mental abstrac-
tion in the most harmonious way. This harmony was revealed to Hermann
Minkowski in 1908 when he convinced physicists and mathematicians alike,
that the most natural setting for the newly discovered special theory of rel-
ativity was a four-dimensional space.
Eight years later, Einstein used this
concept to formulate his general theory of relativity which is the only viable
theory of gravity for the large-scale structure of space and time. In 1921,
Kaluza, in a most beautiful idea, uniﬁed the electromagnetic interaction with
gravity using a ﬁve-dimensional spacetime. Today string theory, one of the
most promising candidates for the uniﬁcation of all forces of nature, uses
11-dimensional spacetime; and the language of quantum mechanics—a the-
ory that describes atomic, molecular, and solid-state physics, as well as all
of chemistry—is best spoken in an inﬁnite-dimensional space, called Hilbert
space.
The key to this multidimensional abstraction is Descartes’ ingenious idea
of translating Euclid’s geometry into the language of coordinates whereby the
abstract Euclidean point in a plane is given the two coordinates (x, y), and
that in space, the three coordinates (x, y, z), where x, y, and z are real num-
bers. Once this crucial step is taken, the generalization to multidimensional
spaces becomes a matter of adding more and more coordinates to the list:
(x, y, z, w) is a point in a four-dimensional space, and (x, y, z, w, u) describes
a point in a ﬁve-dimensional space. In the spirit of this chapter, we want to
identify points with vectors as in the plane and space, in which we drew a

216
Finite-Dimensional Vector Spaces
directed line segment from the origin to the point in question. In general, an
n-dimensional Cartesian vector x is
n-dimensional
Cartesian vector
x = (x1, x2, . . . , xn)
(7.1)
in which xj is called the jth component of the vector. These have all the
properties expected of vectors: You can add them
x + y = (x1, x2, . . . , xn) + (y1, y2, . . . , yn) ≡(x1 + y1, x2 + y2, . . . , xn + yn),
you can multiply a vector by a number
αx = α(x1, x2, . . . , xn) ≡(αx1, αx2, . . . , αxn),
and the zero vector is 0 = (0, 0, . . ., 0). Two vectors are equal if and only if
their corresponding components are equal. Sometimes, it will be convenient
to denote these vectors as columns rather than rows.
The set of real numbers, or the set of points on a line, is denoted by R.
It is common to denote the set of points in a plane—or, in the language of
Cartesian coordinates, the set of pairs of real numbers (x, y)—by R2, and
the set of points in space by R3. Generalizing this notation, we denote the
set of points in the n-dimensional Cartesian space by Rn. We now have an
inﬁnite collection of “spaces” of various dimensions, starting with the one-
dimensional real line R1 = R, moving on to the two-dimensional plane R2,
and the three-dimensional space R3, and continuing to all the abstract spaces
Rn with n ≥4. The concepts of linear combination, linear independence,and
standard basis of
Rn
basis are exactly the same as before. The vectors
ˆe1 ≡(1, 0, . . ., 0),
ˆe2 ≡(0, 1, . . . , 0),
. . .
ˆen ≡(0, 0, . . ., 1)
(7.2)
form a basis for Rn, called the standard basis.
7.1
Linear Transformations
A linear transformation or a linear operator is a correspondence that
formal deﬁnition
of a linear
transformation or
a linear operator
takes a vector in one space and produces a vector in another space in such a
way that the operation of summation of vectors and multiplication of vectors
by numbers is preserved. If we denote the linear transformation by T, then
in mathematical symbolism, the above statement becomes
T(αx + βy) = αT(x) + βT(y).
(7.3)
Matrices are prototypes of linear transformations. In fact, we saw earlier
that it was possible to transform vectors in the plane to vectors in space
and vice versa via 3 × 2 or 2 × 3 matrices. We did not attempt to verify
Equation (7.3) for those transformations, but the reader can easily do so.

7.1 Linear Transformations
217
In fact, denoting vectors of Rn and Rm by column vectors, we can immediately
generalize Equations (6.36) and (6.37) to
⎛
⎜
⎜
⎜
⎝
α′
1
α′
2
...
α′
m
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
am1
am2
. . .
amn
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
α1
α2
...
αn
⎞
⎟
⎟
⎟
⎠
or
a′ = Aa,
(7.4)
where A is an m × n matrix—i.e., it has m rows and n columns—whose
elements aij are real numbers. The reader may verify that Equation (7.4) is
a linear transformation that maps vectors of Rn to those of Rm.
Other linear operators of importance are various diﬀerential operators,
derivative is a
linear operator
i.e., derivatives of various order. For example, it is easily veriﬁed that d/dx
is a linear operator acting on the space of diﬀerentiable functions.1 This is
because
d
dx(αf + βg) = α df
dx + β dg
dx
for α and β real constants. Similarly d2/dx2 and derivative of higher orders, as
well as partial derivatives of various kinds and orders, are all linear operators.
In fact, even when these derivatives are multiplied by functions (on the left),
they are still linear. In particular, the second-order linear diﬀerential operator
second-order linear
diﬀerential
operator
L ≡p2(x) d2
dx2 + p1(x) d
dx + p0(x)
is indeed a linear operator.
If a linear transformation T maps vectors of Rn to vectors of Rm, and S
maps vectors of Rm to vectors of Rk, then we can “compose” or “multiply”
the two transformations to obtain a linear transformation ST which maps
vectors of Rn to vectors of Rk. In terms of matrices, T is represented by an
m × n matrix T, S is represented by a k × m matrix S, and ST is represented
by an k × n matrix which is the product of S and T with S to the left of T.
The product of matrices is as outlined in Box 6.1.3.
Box 7.1.1. If A is a k × m matrix, and B is an m × n matrix, then AB
is a k × n matrix whose entries are given by Box 6.1.3.
The product BA is not deﬁned unless k = n, in which case BA will be an
m × m matrix.
Using polynomials, we can generate multidimensional vector spaces by
polynomials can
generate
multidimensional
vector spaces!
adding increasing powers of t. Then, the collection Pn[t] of polynomials of
degree n and less becomes an (n + 1)-dimensional vector space. A convenient
basis for this vector space is {1, t, t2, . . . , tn} which we call the standard
1The reader may want to check that the collection of diﬀerentiable functions is indeed a
vector space with the “zero function” being the zero vector.

218
Finite-Dimensional Vector Spaces
basis of Pn[t]. The reader may verify that the operation of diﬀerentiation (of
any order) is a linear transformation on Pn[t] which can be represented by
matrices as done in Example 6.2.2.
Example 7.1.1. Let us ﬁnd the matrix that represents the operation of second
diﬀerentiation on P3[t] using the standard basis of P3[t]. Recall that we only need to
apply the second derivative to the basis vectors f1 = 1, f2 = t, f3 = t2, and f4 = t3.
We use a prime to denote the transformed vector:
f ′
1 = d2
dt2 (1) = 0 = 0 · f1 + 0 · f2,
f ′
2 = d2
dt2 (t) = 0 = 0 · f1 + 0 · f2,
f ′
3 = d2
dt2 (t2) = 2 = 2 · f1 + 0 · f2,
f ′
4 = d2
dt2 (t3) = 6t = 0 · f1 + 6 · f2,
where we have anticipated the fact that double diﬀerentiation of P3[t] results in
P1[t]. Following the rule of Box 6.2.2, we can write the transformation matrix as
0
0
2
0
0
0
0
6

.
We may verify that the coeﬃcients in P1[t] of the second derivative of an arbi-
trary polynomial f(t) = α0 + α1t + α2t2 + α3t3 can be obtained by the product of
the matrix of second derivative and the 4 × 1 column vector representing f(t). In
fact,
0
0
2
0
0
0
0
6

⎛
⎜
⎜
⎝
α0
α1
α2
α3
⎞
⎟
⎟
⎠=
2α2
6α3

.
These are the two coeﬃcients of the resulting polynomial in P1[t]. The polynomial
itself is 2α2 + 6α3t which is indeed the derivative of the third degree polynomial
f(t).
■
7.2
Inner Product
Since the concepts of length and angle are not familiar for Rn, we need to
deﬁne the inner product ﬁrst and then deduce those concepts. We can gener-
alize the usual inner product of R2 and R3 in terms of components of vectors.
Let
a = (a1, a2, . . . , an)
and
b = (b1, b2, . . . , bn).
Then
inner product in
Rn deﬁned in
terms of
components in the
standard basis
a · b = a1b1 + a2b2 + · · · + anbn
(7.5)
is the immediate generalization of the dot product to Rn.

7.2 Inner Product
219
This, of course, is not the most general inner product. For that, we need
an inner product matrix G. As in the case of the plane and space, this is
simply a symmetric n × n matrix whose elements determine the dot products
of the vectors of the basis in which we are working.
G =
⎛
⎜
⎜
⎜
⎝
g11
g12
. . .
g1n
g21
g22
. . .
g2n
...
...
...
gn1
gn2
. . .
gnn
⎞
⎟
⎟
⎟
⎠,
gij = gji,
i, j = 1, 2, . . ., n.
(7.6)
Example 7.2.1. Let us ﬁnd the inner product matrix for the basis {1, t, t2, t3} of
P3[t]. As usual, we assume that the interval of integration for the inner product is
(0, 1). Because of the symmetry of the matrix and the fact that we have already
calculated the 3 × 3 submatrix of G, we need to ﬁnd g14, g24, g34, and g44. Once
again, let f1 = f1(t) = 1, f2 = f2(t) = t, f3 = f3(t) = t2, and f4 = f4(t) = t3;
then
g14 = f1 · f4 =
# 1
0
f1(t)f4(t) dt =
# 1
0
t3dt = 1
4,
g24 = f2 · f4 =
# 1
0
f2(t)f4(t) dt =
# 1
0
t4dt = 1
5.
Similarly, g34 = 1
6 and g44 = 1
7. It follows that
G =
⎛
⎜
⎜
⎜
⎝
1
1
2
1
3
1
4
1
2
1
3
1
4
1
5
1
3
1
4
1
5
1
6
1
4
1
5
1
6
1
7
⎞
⎟
⎟
⎟
⎠.
This matrix can be used to ﬁnd the dot product of any two vectors in terms of their
components in the basis {1, t, t2, t3} of P3[t].
■
If a and b have components (a1, a2, . . . , an) and (b1, b2, . . . , bn), then their
inner product is given by
inner product in
Rn deﬁned in
terms of the
metric matrix and
components in a
general basis
8aGb =
 a1
a2
. . .
an
!
⎛
⎜
⎜
⎜
⎝
g11
g12
. . .
g1n
g21
g22
. . .
g2n
...
...
...
gn1
gn2
. . .
gnn
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
b1
b2
...
bn
⎞
⎟
⎟
⎟
⎠.
(7.7)
As usual, if this expression is zero, we say that a and b are G-orthogonal. For
an orthonormal basis, the inner product matrix G becomes the unit matrix2
and we recover the usual inner product of vectors in terms of components.
With a positive deﬁnite inner product at hand, we can deﬁne the length of
length of a vector
deﬁned in terms of
inner product
a vector as the (positive) square root of the inner product of the vector with
itself. Can we deﬁne the angle as well? We can always deﬁne
2Only if the inner product is positive deﬁnite.

220
Finite-Dimensional Vector Spaces
angle deﬁned in
terms of inner
product
cos θ ≡a · b
|a| |b| =
a · b
√a · a
√
b · b
.
But how do we know that the ratio on the RHS is less than one? After all,
a true cosine must have this property! It is an amazing fact of nature that
any positive deﬁnite inner product has precisely this property. To show this,
let a and b be two vectors in any vector space on which an inner product is
deﬁned. Denote the unit vector in the a direction by ˆea, and construct the
vector
b′ = b −(b · ˆea)
  
a number
ˆea
(7.8)
which is easily seen to be perpendicular to ˆea (and therefore to a). If the
inner product is positive deﬁnite, then
derivation of the
Schwarz inequality
b′ · b′ ≥0 ⇒[b −(b · ˆea)ˆea] · [b −(b · ˆea)ˆea] ≥0
or
b · b

=|b|2
−2 b · [(b · ˆea)ˆea]



=(b·ˆea)2
+(b · ˆea)2 ˆea · ˆea
  
=1
≥0.
It follows that
|b|2 −(b · ˆea)2 ≥0 ⇒|b|2 ≥

b ·
 a
|a|
2
and
|b|2 ≥
b · a
|a|
2
⇒|b|2|a|2 ≥(b · a)2.
This is the desired inequality.
Box 7.2.1. (Schwarz Inequality). If a and b are two nonzero vectors
of a vector space for which a positive deﬁnite inner product is deﬁned,
then
|a| |b| ≥|a · b|.
The equality holds only if b is a multiple of a.
The last statement follows from the fact that b′ · b′ = 0 only if b′ = 0 when
the inner product is positive deﬁnite [see Equation (7.8)].
The Schwarz inequality holds not only for ﬁnite-dimensional vector spaces
Schwarz inequality
holds in all inner
product spaces
regardless of their
dimensionality.
such as Rn or Pn[t], but also for inﬁnite-dimensional vector spaces.
It is
one of the most important inequalities in mathematical physics. One of its
consequences is that we can actually deﬁne the angle between two nonzero
vectors in Rn or Pn[t] (or any other vector space, ﬁnite or inﬁnite, for which
a positive deﬁnite inner product exists).

7.2 Inner Product
221
Example 7.2.2. What is the angle between the third and fourth vectors in the
standard basis of P3[t] when the interval of integration of the inner product is (0, 1)?
All the inner products are calculated in Example 7.2.1. Therefore,
cos θ =
f3 · f4
√
f3 · f3
√
f4 · f4
=
g34
√g33 √g44 =
1/6
	
1/5
	
1/7
=
√
35
6
or θ = 9.594◦.
■
As in the case of the plane and space, it is convenient to construct or-
thonormal basis vectors in Rn. This is done by the Gram–Schmidt process
Gram–Schmidt
process
which can easily be generalized. Suppose B = {a1, a2, . . . , an} is a basis for
Rn. Again, to avoid complications, we assume that the inner product is Eu-
clidean so that the inner product of every nonzero vector with itself is positive.
We know how to construct three orthonormal vectors out of {a1, a2, a3}, we
did that in our discussion of the space vectors. Call these new orthonormal
vectors {ˆe1, ˆe2, ˆe3}. Now construct the vector a′
4,
a′
4 = a4 −(a4 · ˆe1)ˆe1 −(a4 · ˆe2)ˆe2 −(a4 · ˆe3)ˆe3
which is obtained from a4 by taking away its projections along ˆe1, ˆe2, and ˆe3.
Now note that
ˆe1 · a′
4 = ˆe1 · a4 −(a4 · ˆe1) ˆe1 · ˆe1
  
=1
−(a4 · ˆe2) ˆe2 · ˆe1
  
=0
−(a4 · ˆe3) ˆe3 · ˆe1
  
=0
= 0.
Similarly, ˆe2 · a′
4 = 0 and ˆe3 · a′
4 = 0; i.e., a′
4 is orthogonal to ˆe1, ˆe2, and ˆe3.
This suggests deﬁning ˆe4 as
ˆe4 ≡a′
4
|a′
4| =
a′
4
	
a′
4 · a′
4
.
This process can continue until we come up with n orthonormal vectors. This
will happen only if the n vectors with which we started are linearly indepen-
dent.
Box 7.2.2. If {a1, a2, . . . , an} are linearly independent vectors of Rn,
then we can construct a set of n orthonormal vectors out of them by the
Gram–Schmidt process.
An orthonormal basis will be denoted by {ˆe1, ˆe2, . . . , ˆen}, where, as usual,
the symbol ˆe stands for unit vectors. We can abbreviate the orthonormal
property of these vectors by writing
ˆei · ˆej =
0
1
if i = j,
0
if i ̸= j.

222
Finite-Dimensional Vector Spaces
There is a symbol that shortens the above statement even further. It is called
the Kronecker delta and denoted by δij. It is deﬁned by
Kronecker delta
and its use in
discussing
orthonormal
vectors
δij =
0
1
if i = j,
0
if i ̸= j.
(7.9)
Therefore, the orthonormality condition can be expressed as
ˆei · ˆej = δij.
(7.10)
We shall see many examples of the use of the Kronecker delta in the sequel.
Transformations that leave the inner products unchanged can be obtained
in exactly the same way as for the plane and the space. For A to preserve the
inner product, we need to have
G-orthogonal
matrix
8AGA = G,
(7.11)
i.e., it has to be G-orthogonal. If G is the identity matrix, then A can be
thought of as an n-dimensional rigid rotation and is simply called orthogonal;
it satisﬁes
8AA = 1
(7.12)
or
⎛
⎜
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
an1
an2
. . .
ann
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
a11
a21
. . .
an1
a12
a22
. . .
an2
...
...
...
a1n
a2n
. . .
ann
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
1
0
. . .
0
0
1
. . .
0
...
...
...
0
0
. . .
1
⎞
⎟
⎟
⎟
⎠.
It should be clear from this that the columns of the matrix A, considered as
vectors, have unit length and are orthogonal to other columns in the usual
Euclidean inner product.
7.3
The Determinant
The determinant of an n×n matrix is obtained in terms of cofactors in exactly
the same way as in the case of 3 × 3 matrices. The cofactors are themselves
determinants of (n −1) × (n −1) matrices which can be expanded in terms
of cofactors of their elements which are determinants of (n −2) × (n −2)
matrices, etc. Continuing this process, we ﬁnally end up with determinants
of 2 × 2 matrices. The determinant is also related to the inverse of a matrix
[see Equations (6.60) and (6.61)]:
Theorem 7.3.1. The matrix A has an inverse if and only if det A ̸= 0 in
which case
A−1 =
1
det A
8A =
1
det A
⎛
⎜
⎜
⎜
⎝
A11
A21
. . .
An1
A12
A22
. . .
An2
...
...
...
A1n
A2n
. . .
Ann
⎞
⎟
⎟
⎟
⎠,
(7.13)
where Aij is the cofactor of aij as deﬁned in Box 6.3.2.

7.3 The Determinant
223
Calculation of the determinant becomes extremely cumbersome when the
dimension of the matrix increases beyond 4 or 5. However, there are certain
properties of the determinant which may sometimes facilitate its calculation.
The determinant has the following properties:
some properties of
the determinant
1. To obtain the determinant of an n × n matrix, multiply each element of
one row (or one column) by its cofactor and then add the results.
2. The determinant of the unit matrix is 1.
3. The determinant of a matrix is equal to the determinant of its transpose:
det A = det At.
4. If two rows (or two columns) of a matrix are proportional (in particular,
equal), the determinant of the matrix is zero.
5. If a row or column—treated as a vector in Rn—of a matrix is multiplied
by a constant, the determinant of the matrix will be multiplied by the
same constant.
6. If two rows (or two columns) of a matrix are interchanged, the determi-
nant changes sign.
7. The determinant will not change if we add to one row (or one column)
a multiple of another row (or another column). The addition of rows or
columns and their multiplication by numbers are to be understood as
operations in Rn.
An important relation, which we state without proof,3 is
the determinant of
a product is the
product of
determinants.
det(AB) = det A det B.
(7.14)
This, in combination with det 1 = 1 and AA−1 = 1, gives
det(AA−1) = det 1 ⇒det A det(A−1) = 1 ⇒det(A−1) =
1
det A.
(7.15)
In words, the determinant of the inverse of a matrix is the inverse of its
determinant of
inverse is inverse
determinant
determinant.
Recall that an orthogonal matrix A satisﬁes AAt = 1. The third property
of the determinant given above and (7.14) can be used to obtain
det(AAt) = det 1 ⇒(det A)2 = 1 ⇒det A = ±1.
(7.16)
So
Box 7.3.1. The determinant of an orthogonal matrix is either +1 or −1.
3See Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, Chapters 3 and 25.

224
Finite-Dimensional Vector Spaces
7.4
Eigenvectors and Eigenvalues
One of the most important applications of the determinant is in ﬁnding cer-
tain vectors that are not aﬀected by transformations. As an example, consider
rotation which is a linear transformation of space onto itself (or a transfor-
mation from R3 to R3). A general rotation in space is very complicated (see
Example 6.2.5 and the discussion immediately preceding it), but if we can
ﬁnd an axis which is unaﬀected by the operation, then the process becomes a
simple rotation about this axis.
When we say that a vector is unaﬀected, we mean that its direction (and
not necessarily its magnitude) is unchanged. We use n × n matrices to repre-
sent transformations of Rn. If x is a (column) vector in Rn whose direction is
not aﬀected by the transformation T, then we can write
T x = λx
or
(T −λ1)x = 0,
(7.17)
where λ is a real number and we introduced the unit matrix to give meaning to
the subtraction of λ from T. In Equation (7.17), x is called the eigenvector
eigenvector,
eigenvalue, and
eigenvalue
equation
and λ the eigenvalue of the linear transformation. Since the zero vector triv-
ially satisﬁes (7.17), we demand that eigenvectors always be nonzero. Equation
(7.17) itself is called an eigenvalue equation; its solution involves calculat-
ing both the eigenvalues and the eigenvectors. It is clear from (7.17) that a
multiple of an eigenvector is also an eigenvector (see Problem 7.6). There-
fore, an eigenvalue equation (7.17) has no unique solution. By convention, we
normalize eigenvectors so that their length is unity.
To ﬁnd the solution to (7.17), we note that the matrix (T −λ1) must have
no inverse, because if it did, then we could multiply both sides of the equation
by (T −λ1)−1 and obtain
(T −λ1)−1(T −λ1)



=1
x = (T −λ1)−10



=0
⇒x = 0
which is not an acceptable solution. So, we must demand that the matrix
a necessary
condition for an
eigenvalue
equation to have
nontrivial
solutions is that
the determinant of
T −λ1 be zero.
(T −λ1) have no inverse. This will happen only if the determinant of this
matrix vanishes. So, the problem is reduced to ﬁnding those λ’s which make
the determinant of the matrix vanish. In other words, the eigenvalues are the
solutions of the equation
det(T −λ1) = 0.
(7.18)
Once the eigenvalues are determined, we substitute them one by one in the
matrix equation (7.17) and ﬁnd the corresponding eigenvectors by solving the
resulting n linear equations in n unknowns. The best way to explain this is
through an example.
Example 7.4.1. Let T be a linear transformation of space (or R3) represented by
the matrix
T =
⎛
⎝
1
0
0
0
1
2
0
2
1
⎞
⎠.

7.4 Eigenvectors and Eigenvalues
225
The eigenvalue equation is
(T −λ1)x = 0
or
⎡
⎣
⎛
⎝
1
0
0
0
1
2
0
2
1
⎞
⎠−λ
⎛
⎝
1
0
0
0
1
0
0
0
1
⎞
⎠
⎤
⎦
⎛
⎝
x1
x2
x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
This can also be written as
⎛
⎝
1 −λ
0
0
0
1 −λ
2
0
2
1 −λ
⎞
⎠
⎛
⎝
x1
x2
x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠
(7.19)
whose nontrivial solution is obtained by setting the determinant of the matrix equal
to zero:
det
⎛
⎝
1 −λ
0
0
0
1 −λ
2
0
2
1 −λ
⎞
⎠= 0
or
(1 −λ) det
1 −λ
2
2
1 −λ

= (1 −λ)
.
(1 −λ)2 −4
/
= 0.
This equation has the solutions
1 −λ = 0
or
(1 −λ)2 = 4 ⇒1 −λ = ±2.
It follows that there are three eigenvalues: λ1 = 1, λ2 = −1, and λ3 = 3. We now
ﬁnd the eigenvectors corresponding to each eigenvalue.
Substituting λ1 = 1 for λ in Equation (7.19) yields
⎛
⎝
0
0
0
0
0
2
0
2
0
⎞
⎠
⎛
⎝
x1
x2
x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠
or
⎛
⎝
0
2x3
2x2
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.
It follows that x2 = 0 = x3. Therefore, the ﬁrst eigenvector is
a1 =
⎛
⎝
x1
0
0
⎞
⎠= x1
⎛
⎝
1
0
0
⎞
⎠
with x1 an arbitrary real number. This arbitrariness comes from the fact that a
multiple of an eigenvector is also an eigenvector. We choose x1 = 1 to normalize the
eigenvector to unit length. Denoting this eigenvector by e1, we have
e1 =
⎛
⎝
1
0
0
⎞
⎠.
To ﬁnd the second eigenvector, we substitute λ2 = −1 for λ in Equation (7.19).
This gives
⎛
⎝
2
0
0
0
2
2
0
2
2
⎞
⎠
⎛
⎝
x1
x2
x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠
or
⎛
⎝
2x1
2x2 + 2x3
2x2 + 2x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠.

226
Finite-Dimensional Vector Spaces
It follows that x1 = 0, and 2x2 + 2x3 = 0 or x3 = −x2. Therefore, the second
eigenvector is
a2 =
⎛
⎝
0
x2
−x2
⎞
⎠= x2
⎛
⎝
0
1
−1
⎞
⎠
with x2 arbitrary. To normalize the eigenvector, we divide it by its length.4 This
amounts to choosing x2 = 1/
√
2 (see Problem 7.7). We thus have
e2 =
1
√
2
⎛
⎝
0
1
−1
⎞
⎠.
For the third eigenvector, we substitute λ3 = 3 in Equation (7.19) to obtain
⎛
⎝
−2
0
0
0
−2
2
0
2
−2
⎞
⎠
⎛
⎝
x1
x2
x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠
or
⎛
⎝
−2x1
−2x2 + 2x3
2x2 −2x3
⎞
⎠=
⎛
⎝
0
0
0
⎞
⎠
or x1 = 0, and x3 = x2. Therefore, the third eigenvector is
a3 =
⎛
⎝
0
x2
x2
⎞
⎠= x2
⎛
⎝
0
1
1
⎞
⎠
with x2 arbitrary. To normalize the eigenvector, we divide it by its length and get
e3 =
1
√
2
⎛
⎝
0
1
1
⎞
⎠.
■
The unit eigenvectors ˆe1, ˆe2, and ˆe3 of the preceding example are mutually
perpendicular as the reader may easily verify. This is no accident! The matrix
of that example happens to be symmetric, and for such matrices, we have the
following general property:
Box 7.4.1. Eigenvectors of a symmetric matrix corresponding to diﬀerent
eigenvalues are orthogonal.
To show this, let x and y be eigenvectors of a symmetric matrix T correspond-
ing to eigenvalues λ and λ′, respectively:
Tx = λx,
Ty = λ′y.
Multiply both sides of the ﬁrst equation by 8y and the second by 8x to get
8y Tx = λ8y x,
8x Ty = λ′ 8xy.
(7.20)
4Here we are assuming that the inner product for the calculation of length is the usual
Euclidean one.

7.5 Orthogonal Polynomials
227
Now take the transpose of both sides of the ﬁrst equation in (7.20). This
gives5
(8y Tx)t = λ(8y x)t ⇒8x 8T88y = λ8x88y.
But double transposing y gives back y. Furthermore, 8T = T, because T is
symmetric. So,
8x Ty = λ8xy.
Subtracting both sides of this equation from those of the second equation in
(7.20), we obtain
0 = (λ −λ′)8xy.
By assumption, λ ̸= λ′; so, we must have 8xy = 0, i.e., that x and y are
orthogonal.
7.5
Orthogonal Polynomials
The last section generalized the two- and three-dimensional “arrows” and
polynomials to higher dimensions in which many of the original properties of
vectors—such as the inner product—were retained. In this section, we want to
make two more generalizations which are necessary for many physical applica-
tions. The ﬁrst is the introduction of a weight function in the deﬁnition of
weight function
inner product. A weight function is a function that is positive deﬁnite6 in the
interval (a, b) of integration of the inner product. More speciﬁcally, let p =
p(t) and q = q(t) be polynomials in Pn[t]. We deﬁne their inner product as
p · q =
# b
a
p(t)q(t)w(t) dt,
(7.21)
where w(t) is a function that is never zero or negative for a < t < b, and its
form is usually dictated by the physical application. The reader may verify
that Equation (7.21) deﬁnes a positive deﬁnite inner product.
The second generalization is to consider the collection of all polynomials
of arbitrary degree. In other words, instead of conﬁning ourselves to Pn[t] for
some ﬁxed n, we shall allow all polynomials without any restriction on their
degree. Clearly, such a collection is indeed a vector space; however it does
not have a ﬁnite basis. We denote this inﬁnite-dimensional space by Pw
(a,b)[t],
in which notation both the weight function and the interval of integration are
included.
Given any basis for Pw
(a,b)[t], we can apply the Gram–Schmidt process on it
to turn it into an orthonormal basis. Due to historical reasons, the normality
is not a desirable property for the basis vectors. So, one seeks polynomials
that are orthogonal, but not necessarily of unit length. Instead of normalizing
tradition and
history
“standardize”
polynomials
instead of “nor-
malizing” them.
the vectors, one standardizes them. Standardization is a rule—dictated by
tradition—that ﬁxes some of the coeﬃcients of the polynomials. The proce-
dure for ﬁnding these orthogonal polynomials is to start from the constant
5Recall that8and t mean the same thing.
6This just means that the function is positive and never zero.

228
Finite-Dimensional Vector Spaces
polynomial (of degree zero) and standardize it to get the ﬁrst polynomial.
Next apply the standardization to the polynomial of degree one (with two
unknown coeﬃcients), and make sure that it is perpendicular to the ﬁrst
polynomial, where the inner product is deﬁned by (7.21). These two require-
ments (standardization and perpendicularity) provide two equations and two
unknowns which can be solved to ﬁnd the coeﬃcients of the second polyno-
mial. The next polynomial has degree two with three unknown coeﬃcients.
Standardization and orthogonality to the ﬁrst two polynomials provide three
equations in three unknowns, the solution of which equations determines the
third polynomial. This process can be continued indeﬁnitely determining the
coeﬃcients of orthogonal polynomials up to any desired degree.
Example 7.5.1. The procedure above is best illustrated by a concrete example.
The Legendre polynomial of degree n, denoted by Pn(t), is characterized by
Legendre
polynomial
the standardization Pn(1) = 1. We denote the collection of these polynomials by
P1
(−1,1)[t], indicating that the interval of integration for them is from −1 to +1
and that the weight function is unity. Because of standardization, we must choose
P0(t) = 1. The ﬁrst degree polynomial is generally written as P1(t) = α0 + α1t.
Standardization gives α0 + α1 = 1. Orthogonality to P0(t) gives
0 =
# 1
−1
P0(t)P1(t)w(t) dt =
# 1
−1
1 · (α0 + α1t) · 1 dt = 2α0.
So, α0 = 0 and α1 = 1. Therefore, P1(t) = t.
For P2(t) = α0 + α1t + α2t2 we have (reader please verify!)
α0 + α1 + α2 = 1
(by standardization),
2α0 + 0 · α1 + 2
3α2 = 0
(by orthogonality to P0),
0 · α0 + 2
3 · α1 + 0 · α2 = 0
(by orthogonality to P1).
The solution to these equations is α0 = −1
2, α1 = 0, and α2 = −3
2, so that P2(t) =
1
2(3t2 −1). Other Legendre polynomials can be found analogously.
■
By their very construction, orthogonal polynomials, which are denoted by
Fn(t), satisfy the following orthogonality condition:
orthogonal
polynomials
deﬁned
# b
a
Fn(t)Fm(t)w(t) dt =
0
0
if
m ̸= n,
hn
if
m = n,
(7.22)
where hn is just a positive number (depending on n, of course) which is
diﬀerent for diﬀerent types of Fn.7 As before, let us treat these polynomials
as vectors and write Fn for Fn(t). Then using the Kronecker delta of (7.9),
Equation (7.22) can be written as
Fn · Fm =
0
0
if
m ̸= n
hn
if
m = n = hnδmn.
7There are many diﬀerent types of orthogonal polynomials, distinguished from each other
by diﬀerent intervals, and diﬀerent w(t). Diﬀerent symbols—such as Pn(t), Hn(t), Tn(t),
etc., are used for diﬀerent types. We have used Fn(t) to represent any one of these types
in our general discussion.

7.5 Orthogonal Polynomials
229
In particular, Fn · Fn = hn or |Fn|2 = hn. So, the “length” of Fn is √hn.
Now consider the set of all functions deﬁned in the interval (a, b) any two
the set of all
functions (not just
polynomials) is
also a vector
space.
of which give a ﬁnite result when integrated as in Equation (7.22). The reader
may easily verify that this set is indeed a vector space. If f = f(t) and g = g(t)
are two vectors in this space, then we deﬁne their inner product as
f · g =
# b
a
f(t)g(t)w(t) dt.
(7.23)
It is clear that the Fn belong to this space. Furthermore, it can be shown
that they form a convenient basis for the vector space. In fact, any function of
the space can be written as a (inﬁnite) linear combination of the orthogonal
polynomials
f =
∞

n=0
anFn,
whose coeﬃcients can be determined by taking the inner product of both sides
with Fm:
f · Fm =
 ∞

n=0
anFn

· Fm =
∞

n=0
anFn · Fm = amFm · Fm = hmam
because in the last inﬁnite sum all the terms are zero except one. We can
solve this equation for am to obtain am = f · Fm/hm. Thus,
f =
∞

n=0
anFn
where
an = f · Fn
hn
.
(7.24)
In terms of functions and polynomials, we have the important result:
expansion of
functions in terms
of orthogonal
polynomials
Theorem 7.5.2. A function f(t), deﬁned in the interval (a, b), can be repre-
sented as an inﬁnite sum in orthogonal polynomials given by
f(t) =
∞

n=0
anFn(t),
where
an = 1
hn
# b
a
f(t)Fn(t)w(t) dt.
(7.25)
There are a number of so-called classical orthogonal polynomials used
classical
orthogonal
polynomials
in mathematical physics a number of whose properties we simply cite here.
We have already mentioned Legendre polynomials for which the interval is
(−1, +1) and w(x) = 1.8 For Legendre polynomials, hn = 2/(2n + 2), i.e.,
# 1
−1
Pn(t)Pm(t) dt =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0
if
m ̸= n
2
2n + 1
if
m = n
=
2
2n + 1δmn.
(7.26)
If the interval is (−∞, ∞) and w(t) = e−t2, then the resulting polynomials,
denoted by Hn(t), are called Hermite polynomials. For Hermite polynomi-
Hermite
polynomials
als, we have
8A detailed discussion of Legendre polynomials and their origin can be found in Chapter
26.

230
Finite-Dimensional Vector Spaces
# ∞
−∞
Hn(t)Hm(t)e−t2 dt =
⎧
⎪
⎨
⎪
⎩
0
if
m ̸= n
√π 2nn!
if
m = n
= √π 2nn! δmn.
(7.27)
If the interval is (0, ∞) and w(t) = tme−t with m a positive integer,9
then the resulting polynomials, denoted by Lm
n (t), are called Laguerre poly-
nomials. For Laguerre polynomials, we have
Laguerre
polynomials
# ∞
0
Lm
n (t)Lm
k (t)tme−t dt =
0
0
if
k ̸= n
√π (n + m)!/n!
if
k = n
= √π (n + m)!
n!
δkn.
(7.28)
There are other (classical) orthogonal polynomials which we shall not inves-
tigate here.10
7.6
Systems of Linear Equations
Our discussion of determinants in Section 6.3 started with a system of two
linear equations in two unknowns and led to the result that if the determinant
of the matrix of coeﬃcients is nonzero, then the inverse of this matrix exists,
and the unknowns can be found conveniently using this inverse [see Equation
(6.52) and Theorem 6.3.1]. This was further generalized to the case of three
linear equations in three unknowns and stated in Equation (6.59). A system
of n linear equations in n unknowns can be handled in the same way. We
write such a system as
⎛
⎜
⎜
⎜
⎝
x1
x2
...
xn
⎞
⎟
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎝
a11
a12
. . .
a1n
a21
a22
. . .
a2n
...
...
...
an1
an2
. . .
ann
⎞
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎝
b1
b2
...
bn
⎞
⎟
⎟
⎟
⎠⇒x = Ab
(7.29)
and note that, if det A ̸= 0, we can calculate A−1 according to Box 7.3.1,
and multiply both sides of (7.29) by this inverse and obtain x = A−1b. The
case of the vanishing determinant is best treated in the context of a system
of equations for which the number of unknowns is not equal to the number of
equations.
The process that led to Equations (6.49) and (6.51) is called elimination,
and can be extended to m linear equations in n unknowns of the form
m linear equations
in n unknowns
9Actually m need not be an integer. However, the space and scope of this book does
not permit us to consider the general case.
10The interested reader may ﬁnd Hassani, S. Mathematical Physics: A Modern Intro-
duction to Its Foundations, Springer-Verlag, 1999, Chapter 7, a useful reference for all
orthogonal polynomials including many derivations and proofs that we have skipped here.

7.6 Systems of Linear Equations
231
a11x1 + a12x2+ · · · + a1nxn = b1,
a21x1 + a22x2+ · · · + a2nxn = b2,
...
(7.30)
am1x1 + am2x2+ · · · + amnxn = bm.
We will now describe a general process known as Gauss elimination,
Gauss elimination
for ﬁnding all solutions of the given system of linear equations.
The idea
is to replace the given system by a simpler system, which is equivalent to
the original system in the sense that it has precisely the same solutions. For
example, the degenerate equation
0 · x1 + 0 · x2 + · · · + 0 · xn = bj
is equivalent to 0 = bj, which cannot be satisﬁed unless bj is zero.
In a more compact notation, we write only the ith equation, indicating
its form by a sample term aijxj and the statement that the equation is to be
summed over j from 1 to n by writing11
n

j=1
aijxj = bi
for
i = 1, 2, . . ., m.
(7.31)
We distinguish two cases:
1. Every ai1 = 0, i.e., all coeﬃcients of the unknown x1 vanish. Then, triv-
ially, the system (7.31) is equivalent to a smaller system of m equations
in the n −1 unknowns x2, . . . , xn with x1 arbitrary for any solution of
the smaller system.
2. Some ai1 ̸= 0. By interchanging the ﬁrst equation with another if nec-
essary, we get an equivalent system with a11 ̸= 0. Dividing the ﬁrst
equation by a11, we then get an equivalent system in which a11 = 1.
Then subtracting ai1 times the new ﬁrst equation from each ith equa-
tion for i = 2, . . . , m, we get an equivalent system of the form
x1 + a′
12x2 + a′
13x3+ · · · + a′
1nxn = b′
1,
a′
22x2 + a′
23x3+ · · · + a′
2nxn = b′
2,
...
(7.32)
a′
m2x2 + a′
m3x3+ · · · + a′
mnxn = b′
m.
Now we apply the same procedure to the system of equations in (7.32) in-
volving only x2 through xn so that x2 will appear only in the ﬁrst of these
equations. If case 2 always arises, the given system is said to be compatible.
compatible and
incompatible
systems of linear
equations
If case 1 arises once in a while, then we may get degenerate equations of the
form 0 = dk. If all dk turn out to be zero, these can be ignored; if one dk ̸= 0,
the original system (7.30) is incompatible (has no solutions). We summarize
these ﬁndings as
11The reader may ﬁnd an adequate discussion of summations and “dummy” indices in
Section 9.2.

232
Finite-Dimensional Vector Spaces
Theorem 7.6.1. Any system (7.30) of m linear equations in n unknowns can
be reduced to an equivalent system of r linear equations whose ith equation has
the form
xi + ci,i+1xi+1 + ci,i+2xi+2 + · · · + cinxn = di
(7.33)
plus m −r equations of the form 0 = dk.
Written out in full, Equation (7.33) looks like
x1 + c12x2 + c13x3 + c14x4+ · · · + c1nxn = d1,
x2 + c23x3 + c24x4 · · · + c2nxn = d2,
x3 + c34x4 · · · + c3nxn = d3,
(7.34)
...
xr+ · · · + crnxn = dr
(r ≤m),
which is said to be in echelon form.
echelon form of a
system of linear
equations
Solutions of any system of the echelon form (7.34) are easily described.
Consider the succession of the unknowns starting with xn and going down to
x1. If a given xi appears as the ﬁrst variable in an equation of (7.34), then it
can be written in terms of all preceding unknowns:12
xi = di −ci,i+1xi+1 −ci,i+2xi+2 −· · · −cinxn.
(7.35)
If xi does not appear as the ﬁrst variable in an equation of (7.34), then it can
be chosen arbitrarily. We thus have
Box 7.6.1. In the compatible case of Theorem 7.6.1, the set of all solu-
tions of Equation (7.30) are determined as follows. The m −r unknowns
xk not occurring in (7.34) can be chosen arbitrarily (they are free param-
eters). For any choice of these xk’s, the remaining xi can be computed by
substituting in (7.35).
Example 7.6.2. Consider the following four linear equations in three unknowns
(so m = 4 and n = 3):
−x2 + 2x3 = 1,
x1 + x2 −3x3 = 0,
−x1 + x2 + x3 = −2,
(7.36)
x1 + 2x2 −x3 = −1.
The coeﬃcient of x1 in the ﬁrst equation is zero. So, we switch this equation with
one of the other equations, say the second. Then we multiply the new ﬁrst equation
by the negative of the coeﬃcient of x1 in each remaining equation and add the result
12If r = n, then the last equation of (7.34) will be xn = dn, and (if the set of equations
is compatible) all unknowns will be determined.

7.6 Systems of Linear Equations
233
to that equation to eliminate x1. Thus, we add the new ﬁrst equation to the third
equation of (7.36), and subtract the new ﬁrst equation from the last equation of
(7.36). The result is
x1 + x2 −3x3 = 0,
−x2 + 2x3 = 1,
2x2 −2x3 = −2,
(7.37)
x2 + 2x3 = −1.
To eliminate x2 from the last two equations, multiply the second equation of (7.37)
by 2 (or 1 for the last) and add it to the third (or last) equation. This will yield
x1 + x2 −3x3 = 0,
−x2 + 2x3 = 1,
4x3 = 0,
(7.38)
4x3 = 0.
Multiply the second equation by −1, divide the third equation in (7.38) by 4, and
ﬁnally subtract the result from the last equation. The ﬁnal result is the following
echelon form:
x1 + x2 −3x3 = 0,
x2 −2x3 = −1,
x3 = 0,
(7.39)
0 = 0,
which corresponds to Equation (7.34) with r = n = 3. Thus, we have one equation
of the form 0 = dk for which dk is zero. So, the system has a solution. To ﬁnd
this solution, start with the third equation of (7.39) which gives x3 = 0. Substitute
in the equation above it to get x2 = −1, and these values in the ﬁrst equation to
obtain x1 = 1.
■
Example 7.6.3. As another example, consider the following:
x1 + x2 + x3 = 0,
2x1 −x2 + x3 = −2,
−x1 + 2x2 + x3 = −1,
(7.40)
x1 −2x2 + x3 = 2.
Multiply the ﬁrst equation successively by −2, 1, and −1 and add it to the second,
third and fourth equations. The result will be
x1 + x2 + x3 = 0,
−3x2 −x3 = −2,
3x2 + 2x3 = −1,
−3x2 + 0 · x3 = 2.

234
Finite-Dimensional Vector Spaces
Now divide the second equation by −3,
x1 + x2 + x3 = 0,
x2 + 1
3x3 = 2
3,
3x2 + 2x3 = −1,
(7.41)
−3x2 + 0 · x3 = 2.
Multiply the second equation of (7.41) successively by −3 and +3 and add it to the
third and last equations. This will yield
x1 + x2 + x3 = 0,
x2 + 1
3x3 = 2
3,
x3 = −3,
(7.42)
x3 = 4.
Subtract the third equation from the last to get
x1 + x2 + x3 = 0,
x2 + 1
3x3 = 2
3,
x3 = −3,
(7.43)
0 = 7.
In this case, we have an equation of the form 0 = dk for which dk = 7. So, the
system is incompatible, i.e., it has no solution.
■
A system of linear equations (7.30) is homogeneous if the constants bi
homogeneous
system of linear
equations
on the RHS are all zero. Such a system always has a trivial solution with
all the unknowns equal to zero. There may be no further solutions, but if
the number of variables exceeds the number of equations, the last equation
of (7.32) will always contain more than one variable at least one of which can
be chosen at will. Furthermore, the inconsistent equations 0 = dk can never
arise for such homogeneous equations. Hence,
Box 7.6.2. A system of m homogeneous linear equations in n unknowns,
with n > m, always has a solution in which not all the unknowns are zero.
7.7
Problems
7.1. Show that Equation (7.4) is a linear transformation.
7.2. Verify that the operation of diﬀerentiation of any order is a linear trans-
formation on Pn[t].
7.3. Show that
L ≡p2(x) d2
dx2 + p1(x) d
dx + p0(x)
is a linear operator on the space of diﬀerentiable functions.

7.7 Problems
235
7.4. Show that the coeﬃcients in P1[t] of the second derivative of an arbitrary
polynomial f(t) = α0 + α1t + α2t2 + α3t3 can be obtained by the product of
the matrix of the second derivative obtained in Example 7.1.1, and the 4 × 1
column vector representing f(t).
7.5. Express the element in the ith row and jth column of a unit matrix in
terms of the Kronecker delta.
7.6. Suppose x is an eigenvector of T with eigenvalue λ. Show that, for any
constant α, αx is also an eigenvector of T with the same eigenvalue.
7.7. Find the length of a2 of Example 7.4.1 in terms of x2. Now show that
a2/|a2| = ˆe2.
7.8. Show that the rotation of the plane aﬀects all vectors in the plane. Hint:
Try to ﬁnd an eigenvector of the 2 × 2 rotation matrix (6.24).
7.9. Find the eigenvalues and normalized (unit length) eigenvectors of the
following matrices. In cases where the matrix is symmetric, verify directly
that its eigenvectors corresponding to diﬀerent eigenvalues are orthogonal.
(a)
1
2
2
−2

.
(b)
2
4
5
3

.
(c)
3
2
2
3

.
(d)
⎛
⎝
1
1
0
1
0
1
0
1
1
⎞
⎠.
(e)
⎛
⎝
2
0
0
0
1
1
0
1
1
⎞
⎠.
(f)
⎛
⎝
1
1
1
1
1
1
1
1
1
⎞
⎠.
7.10. Show that Box 7.4.1 is not necessarily true for a general inner product
with matrix G. However, if G and T commute (i.e., if GT = TG), then Box
7.4.1 holds. Hint: Follow the argument after Box 7.4.1 and see how far you
can proceed.
7.11. Show that the inner product deﬁned in Equation (7.21) is indeed a
positive deﬁnite inner product.
7.12. Find the fourth Legendre polynomial using the results of Example 7.5.1.
7.13. Find the ﬁrst three Hermite polynomials using the standardization (or
normalization) Equation (7.27).
7.14. The volume element of a four-dimensional Euclidean space with Carte-
sian coordinates x, y, z, and w is dxdydzdw. In any other coordinate system,
it is given by a 4-dimensional generalization of the Jacobian (6.66)
(a) Write this Jacobian for a general transformation to coordinates s, t, u,
and v where x, y, z, and w are functions of these new coordinates.
(b) Now consider the 4-dimensional spherical coordinates:
x = r sin μ sin θ cos ϕ
y = r sin μ sin θ sin ϕ
z = r sin μ cosθ
w = r cos μ

236
Finite-Dimensional Vector Spaces
and calculate the 4-dimensional Jacobian to ﬁnd the volume element of a 4-
dimensional sphere.
(c) With 0 ≤ϕ ≤2π, 0 ≤θ ≤π, 0 ≤μ ≤π, ﬁnd the volume of a 4-sphere of
radius a.
7.15. Determine the r of Equation (7.34) for each of the following systems of
linear equations and whether or not the system is compatible. If the system
is compatible, ﬁnd a solution for it.
(a)
2x −y −4z = 1,
x + 2y + 2z = 0,
−x −y + 6z = 3.
(b)
x + y + z = −1,
2x −y + 2z = −5,
3x + 3y + z = 1.
(c)
x + y + z = 2,
2x −y + 2z = −2,
3x + y −z = 4.
(d)
2x + y −2z = 2,
3x −y −4z = −1,
3x + 4y −2z = 7.
(e)
3x + 2y = 7,
x + y + z = 6,
5x + 4y + 2z = 19,
x −2y = −5.
(f)
x + 5y −z = 2,
2x + y + 3z = −1,
−x + 3y + 2z = −3,
3x + 2y −z = 4.

Chapter 8
Vectors in Relativity
One of the most rewarding applications of vectors is to relativity. The special
theory of relativity (STR) was a direct consequence of Maxwell’s equations,
which summarize the entire theory of electromagnetism (see Section 15.4).
These equations predict mathematically that there must exist electromagnetic
(EM) waves which travel at the speed of light in empty space. This speed c is
found in terms of purely electric and magnetic measurements:
c =
1
√μ0ϵ0
=
1
	
(4π × 10−7) (8.854 × 10−12)
= 2.998 × 108 m/s,
where ϵ0 = 1/4πke and μ0 = 4πkm, with ke and km the electric and magnetic
constants introduced in Chapter 1.
Imagine two laboratories on two spaceships, S1 and S2, with S1 behind
(and moving towards) S2 at 0.9c relative to S2. The physicists on S1 perform
electric and magnetic experiments, measure ϵ0 and μ0, and conclude that
EM waves travel at 300,000 km/s in empty space.
The physicists on S2
also perform electric and magnetic experiments, measure ϵ0 and μ0, and also
conclude that EM waves travel at 300,000 km/s in empty space.
Now a
physicist on S1 takes a ﬂashlight and sends a beam of light in the forward
direction in empty space. The consequence of Maxwell’s equations is that the
physicists on S2, although seeing S1 moving towards them at 0.9c and the
light beam moving away from S1 at c, conclude that the speed of the light
beam is c and not 1.9c, as expected from the Newtonian law of addition of
velocities.
To appreciate the strange consequence of Maxwell’s equations, consider
the following example: A train moving at 30 m/s and a passenger throwing
law of addition
of velocities
a ball in the forward direction with a speed of 20 m/s. A ground observer
measures the speed of the ball to be 30+20 = 50 m/s: velocities add. Here is
another familiar example: A car moves at 75 mph on a highway on which your
car is moving at 50 mph. The speed of the fast car relative to you is 25 mph.
You speed up to 70 mph. Then the other car appears to have “slowed down,”
because, now you measure its speed relative to you to be only 5 mph. Go to

238
Vectors in Relativity
outer space, let someone in your spaceship ﬁre a bullet moving at 500 mph.
Increase your speed to 450 mph, the bullet appears to be moving at 50 mph
away from you. Increase your speed by another 100 mph. You catch up with
the bullet, and if you decrease your speed by 50 mph, the bullet appears
stationary relative to you.
Now shoot a beam of light forward, and once the beam leaves your ﬂash-
light, accelerate your spaceship to a speed of 299,000 km/s.
Measure the
speed of the light beam. It is still 300,000 km/s, and not 1000 km/s, as in-
tuitively expected! Maxwell’s equations defy intuition, and the (STR), which
is entirely based on these equations is extremely counter-intuitive. Let us
summarize these observations:
Box 8.0.1. (Principle of Relativity) Every time you detect an electro-
magnetic wave, it moves at the rate of 300,000 km per second in vacuum,
regardless of the motion of its source or its detector. Speed of light in
vacuum is a universal constant.
An immediate consequence of the principle of relativity is the fact that time
is observer-dependent. As Einstein said “Time is something that is measured
by clocks.” So, let us look at the eﬀect of motion on clocks. The clock best
suited for this investigation is the “arm” of the Michelson–Morley apparatus
shown in Figure 8.1. It consists of a source S of light, or electromagnetic
waves, and a mirror M. The distance between S and M is L. Therefore, it
takes light Δτtick ≡2L/c to go from S to M and back. If we place a light
the
Michelson–Morley
clock
sensitive “ticker” at S, the clock will tick every Δτtick second. We call such
a clock a Michelson–Morley clock, or an MM clock, and Δτtick the proper
tick of the MM clock. Δτtick is the tick measured by an observer for whom
the clock is at rest, or for whom the beginning and the end of a tick occur at
the same location.
S
M
L
Figure 8.1: A Michelson–Morley clock. A “tick” of this clock occurs when the light
signal makes a round trip along the length L.

8.1 Proper and Coordinate Time
239
8.1
Proper and Coordinate Time
An MM clock is placed on a train and observed by two observers, O (on the
ground) and O′ (on the train) moving to the right of O. Consider three events:
The emission of a light beam at S, its reﬂection at M, and its reception at
S. These three events constitute one tick. Let us denote them by E1, E2,
and E3, respectively. How does O′ see the ticking of the clock? The clock
is sitting right beside her, and she observes the whole process of ticking as
the light going straight up and coming straight down. She concludes that her
clock’s ticks are Δτtick long.
Now, let us see how O perceives the succession of these three events. Since
the clock is moving to the right, the light signal that leaves S will reach M
only after M has moved to the right. Thus, to O, the events E1 and E2 are
separated not only by a vertical distance, but also by a horizontal distance (see
Figure 8.2). Since the speed of light is the same for all observers, O concludes
that it takes light more than 2L/c to travel E1E2 and E2E3. Therefore, he
concludes that the clock on the train must tick slower!
moving clocks
slow down.
We can quantify the above statement by referring to the triangle E1AE2
of Figure 8.2. Pythagoras’ theorem implies
 
E1E2
!2 =
 
E1A
!2 +
 
AE2
!2 .
Let the speed of the train be v and the light beam’s travel time from S to M
be δt according to O. Then E1A = vδt and E1E2 = cδt with c the (universal)
speed of light. Putting all of this in the above equation gives
(cδt)2 = (vδt)2 + L2 ⇒c2(δt)2 = v2(δt)2 + L2,
(8.1)
or
(δt)2 −v2
c2 (δt)2 = L2
c2 , ⇒(δt)2

1 −v2
c2

= L2
c2 .
E1
E2
E3
M
A
Figure 8.2: A moving Michelson–Morley clock. The path of light (represented by a
black dot) is not a vertical line but a slanted one due to the motion of M.

240
Vectors in Relativity
This yields
(δt)2 =
L2/c2
1 −v2/c2 ⇒δt =
L/c
	
1 −(v/c)2 .
Let us denote by Δttick the duration of the light’s round trip as seen by O.
Then
Δttick =
2L/c
	
1 −(v/c)2 =
Δτtick
	
1 −(v/c)2 .
(8.2)
In deriving this equation, we have tacitly assumed that motion does not aﬀect
transverse lengths. Thus the length of the MM clock does not change because it
motion does not
aﬀect transverse
lengths.
is perpendicular to the direction of motion. To see this, consider the distance
between two wheels of a train, and suppose that this distance shrinks1 due to
its motion as seen by a ground observer. This means that the wheels will fall
between the rails. On the other hand, the engineer of the train sees the rail
moving and concludes that the distance between the rails shrink; i.e., that the
wheels fall outside the rails. This contradicts the previous conclusion. Thus,
the length perpendicular to the direction of motion must not change.
Although Equation (8.2) is derived for a single tick, it really applies to all
time intervals, because any such interval is a multiple of a single tick. We now
rewrite Equation (8.2) without the subscript “tick,” realizing that Δτ is the
proper time between any two events, i.e., the time interval between the two
events measured by a clock that is present at both events:
relation between
proper time and
coordinate time
Δt =
Δτ
	
1 −(v/c)2 .
(8.3)
Δτ can also be deﬁned as the time measured by an observer for whom the two
events occur at the same spatial point. Δt, called the coordinate time, is
the time measured by another observer, moving relative to the ﬁrst one with
speed v, for whom the two events occur at two diﬀerent spatial points.
8.2
Spacetime Distance
The most elegant way of relating an event’s space and time properties as
described by two observers is to use geometry. We start with the description
of the event itself. An event has a position and an instant of time. Therefore,
it can be represented by a set of four coordinates: three for position and
one for time. It is common to multiply the time t by c (to make a distance
out of it) and put it as the ﬁrst coordinate. Thus in Cartesian coordinate
system, an event is described by (ct, x, y, z). Geometrically, we have added
spacetime
introduced
the extra “dimension” of time to the three-dimensional space to create the
four-dimensional spacetime.
At the heart of any geometry is the distance between two nearby points,
and how it is written in terms of the coordinates of the points. Euclidean
1The same argument applies to the case where the distance expands.

8.2 Spacetime Distance
241
geometry started without coordinates, with the notion of the distance be-
tween two points being “evident.” In fact, we use the properties of Euclidean
distance (such as the Pythagoras’ theorem involving three distances corre-
sponding to the three sides of a right triangle) to show that the distance
geometry and
distance formula
between two points whose Cartesian coordinates diﬀer by (Δx, Δy, Δz) is
	
(Δx)2 + (Δy)2 + (Δz)2.
In the case of the spacetime geometry, we have started with coordinates.
Now we have to ﬁnd a distance formula in terms of the diﬀerence between
coordinates of two events.
We get some clues from Euclidean distance as
expressed in terms of coordinates. The ﬁrst clue is that distance is observer-
independent: If observer O uses his Cartesian coordinate system to label point
P1 by (x1, y1, z1) and P2 by (x2, y2, z2), and ﬁnds
(P1P2)O ≡Δr =
	
(x2 −x1)2 + (y2 −y1)2 + (z2 −z1)2,
and if observer O′ uses her Cartesian coordinate system to label point P1 by
(x′
1, y′
1, z′
1) and P2 by (x′
2, y′
2, z′
2), and ﬁnds
(P1P2)O′ ≡Δr′ =

(x′
2 −x′
1)2 + (y′
2 −y′
1)2 + (z′
2 −z′
1)2,
then Δr′ = Δr. The second clue is that if P1 and P2 lie along a single axis of
an observer, then the distance is the (absolute value of the) diﬀerence between
the coordinates of P1 and P2.
Now consider two events E1 and E2, which occur at the same spatial
location according to O′, with E2 happening after E1. This means that O′
(his clock) is present at both events, i.e., that E1 and E2 lie along the time
axis of O′, and that O′ is measuring the proper time interval between the
two events: Δτ = t′
2 −t′
1. By the second clue above, cΔτ = c(t′
2 −t′
1) is
the distance we are looking for (again we multiply by c to make a distance
out of it). We introduce the notation Δs ≡cΔτ and call Δs the spacetime
distance or the invariant interval between the two events.
Another observer O assigns spacetime coordinates (ct1, x1, y1, z1) to E1
and (ct2, x2, y2, z2) to E2. Now the spatial separation between E1 and E2
according to O is
	
(x2 −x1)2 + (y2 −y1)2 + (z2 −z1)2,
and since O′ is at E1 when it happens and at E2 when it happens, this
equation is precisely the distance that O′ travels in time t2 −t1 with respect
to O. Therefore, the speed of O′ relative to O is
v =
	
(x2 −x1)2 + (y2 −y1)2 + (z2 −z1)2
t2 −t1
,
or
v2 = (x2 −x1)2 + (y2 −y1)2 + (z2 −z1)2
(t2 −t1)2
.

242
Vectors in Relativity
Up to this point, we have not used any physics (except for the deﬁnition
of speed). Now comes the crucial ﬁnal step. Equation (8.3) (which is a direct
result of Box 8.0.1) can now be used to ﬁnd the expression of Δs in terms of
coordinate diﬀerences. Equation (8.3) implies that
cΔt =
cΔτ
	
1 −(v/c)2 =
Δs
	
1 −(v/c)2 ,
or
Δs = cΔt
	
1 −(v/c)2 = c(t2 −t1)
	
1 −v2/c2
=
	
c2(t2 −t1)2 −v2(t2 −t1)2.
Substituting the expression for v2 above, we get
spacetime distance
Δs =
	
c2(t2 −t1)2 −(x2 −x1)2 −(y2 −y1)2 −(z2 −z1)2.
We rewrite this important formula as
(Δs)2 = (cΔτ)2 = c2(Δt)2 −(Δx)2 −(Δy)2 −(Δz)2.
(8.4)
Let’s emphasize the signiﬁcance of this equation: If observer O uses his
Cartesian coordinate system to label event E1 by (ct1, x1, y1, z1) and E2 by
(ct2, x2, y2, z2), and ﬁnds
(Δs)2 = c2(t2 −t1)2 −(x2 −x1)2 −(y2 −y1)2 −(z2 −z1)2,
and if observer O′ uses her Cartesian coordinate system to label event E1 by
(ct′
1, x′
1, y′
1, z′
1) and E2 by (ct′
2, x′
2, y′
2,′ z2), and ﬁnds
(Δs′)2 = c2(t′
2 −t′
1)2 −(x′
2 −x′
1)2 −(y′
2 −y′
1)2 −(z′
2 −z′
1)2,
then (Δs′)2 = (Δs)2. Thus, although events are coordinatized diﬀerently by
diﬀerent observers, the spacetime distance between two events is universal.
In contrast to Newtonian physics, neither the time interval nor the spatial
distance between two events is universal in relativity.
Example 8.2.1. Observer O spots a light beam (event E1) at (x1, y1, z1) at time
t1. A little later he ﬁnds the beam (event E2) at (x2, y2, z2) at time t2. What is the
zero spacetime
distance for two
diﬀerent events
spacetime interval for this light beam (i.e., for the two events E1 and E2)?
Since light travels from (x1, y1, z1) to (x2, y2, z2) with speed c, we have
	
(x2 −x1)2 + (y2 −y1)2 + (z2 −z1)2 = c(t2 −t1).
Therefore,
(Δs)2 = c2(t2 −t1)2 −(x2 −x1)2 −(y2 −y1)2 −(z2 −z1)2 = 0,
which holds for any light signal, as the two events above are quite general. Thus the
spacetime distance between two diﬀerent events which can be connected by a light
signal is zero. This is in contrast to the Euclidean case where two diﬀerent points
always have a nonzero distance between them.
■

8.3 Lorentz Transformation
243
8.3
Lorentz Transformation
Because of the intuitiveness of the concept of distance in Euclidean geometry,
it is not essential to know how the coordinates of a point in one coordinate
system (CS) are related to the coordinates of that same point in another
CS. This transformation was found long after the maturity of the Euclidean
geometry [see Section 6.1.3 and especially Equation (6.22) for a discussion of
the two-dimensional version of coordinate transformation], and it was based
entirely on the expression for the distance between two points in terms of the
coordinates of those points.
In spacetime geometry such a transformation is indispensable due to the
counter-intuitive properties of the invariant interval (see Example 8.2.1 above).
And while in Euclidean geometry, one can picture diﬀerent coordinate systems
and how they relate to one another (see Figure 6.7, for example), spacetime
geometry does not readily allow such a direct pictorial representation without
some preliminary algebraic discussion.
Let r1 = (ct1, x1, y1, z1) and r2 = (ct2, x2, y2, z2) be the spacetime “po-
sition vectors” of two events E1 and E2 relative to a coordinate system O.
Construct the diﬀerence
Δr = r2 −r1 = (ct2 −ct1, x2 −x1, y2 −y1, z2 −z1),
and deﬁne the square of the “length” of this vector to be (Δs)2.
In fact,
this is generalized for any four-dimensional vector. But ﬁrst, let’s introduce a
notation.
A spacetime vector has the form a = (a0, a1, a2, a3), which is usually
four-vectors
introduced
called a four-vector or a 4-vector.2
It is also denoted by (a0,⃗a) where
⃗a ≡(a1, a2, a3) is the space part (or the 3-vector part) of the 4-vector. A pri-
mary example of a four-vector is r = (ct, x, y, z) ≡(ct,⃗r). The generalization
mentioned above deﬁnes the square of the length of a (or the inner product
of a with itself) as
a · a ≡a2
0 −a2
1 −a2
2 −a2
3 ≡a2
0 −⃗a · ⃗a = a2
0 −|⃗a|2.
(8.5)
Then it is easy (see Problem 8.1) to show that the inner product of any two
vectors must be given by
a · b ≡a0b0 −a1b1 −a2b2 −a3b3 = a0b0 −⃗a ·⃗b.
(8.6)
In matrix form this can be written as
a · b =
 a0
a1
a2
a3
!
⎛
⎜
⎜
⎝
1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
b0
b1
b2
b3
⎞
⎟
⎟
⎠,
(8.7)
2Note that the ﬁrst component of a has zero as an index, and is called the time compo-
nent. This is common in relativity.

244
Vectors in Relativity
or
a · b = 8aηb
where
η =
⎛
⎜
⎜
⎝
1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1
⎞
⎟
⎟
⎠,
(8.8)
and 8a and b are the row and column vectors in Equation (8.7).
A linear transformation that leaves the inner product of Equation (8.8)—
general Lorentz
transformation
and therefore the spacetime length Δs—invariant is called a Lorentz trans-
formation. By Equation (7.11), such a transformation Λ—which is a 4 × 4
matrix—satisﬁes
8ΛηΛ = η.
(8.9)
The study of the general structure of Lorentz transformations is beyond
the scope of this book. Here we shall conﬁne ourselves to the Lorentz trans-
formations in two dimensions, in which the third and fourth components of
vectors are ignored. This means that vectors are of the form a = (a0, a1),
b = (b0, b1), the inner product is of the form a · b ≡a0b0 −a1b1, and the
matrix η reduces to
η =
1
0
0
−1

.
In addition, the Lorentz transformations become 2 × 2 matrices.
Let Λ =
a11
a12
a21
a22

be a two-dimensional Lorentz transformation that
acts on 2-vectors in O to give the corresponding 2-vectors in O′. Then Λ must
satisfy Equation (8.9) or

a11
a21
a12
a22
 
1
0
0
−1
 
a11
a12
a21
a22

=

1
0
0
−1

,
(8.10)
which is equivalent to the following three equations [see (6.21) for a guide]:
a2
11 −a2
21 = 1,
a11a12 −a21a22 = 0,
a2
12 −a2
22 = −1.
(8.11)
As in the case of rotations (see Section 6.1.3), we can conclude that
a2
22 = a2
11,
a2
12 = a2
21,
a2
12 = a2
11 −1.
(8.12)
So, all parameters are once again given in terms of a11.
To determine a11, consider the 2-vector (cΔt, Δx), the diﬀerence between
the time and position of two events in O. This 2-vector is represented by
(cΔt′, Δx′) in O′, and, by the deﬁnition of the Lorentz transformations,

cΔt′
Δx′

=

a11
a12
a21
a22
 
cΔt
Δx

.
(8.13)
Now suppose that Δx = 0, i.e., that the two events occur at the same location.
Then O is measuring the proper time, so that Δt = Δτ. From Equation (8.13),

8.3 Lorentz Transformation
245
we also have cΔt′ = a11cΔt or Δt′ = a11Δτ. Comparison with Equation (8.3)
yields
a11 =
1
	
1 −(v/c)2 .
Introducing the two symbols β ≡v/c and γ = 1/
	
1 −(v/c)2, we obtain
a11 =
1
	
1 −β2 ≡γ.
(8.14)
The rest of the matrix elements can now be found. The ﬁrst equation in
(8.12) gives a22 = ±γ. To choose the correct sign for a22, note that if O and
O′ are not moving relative to one another, the coordinates do not change.
Therefore Λ must be the unit matrix. So, a22 = 1 when v = 0. This can
happen only if a22 = +γ. The second equation in (8.12) now gives a12 = a21;
and the third equation yields
a2
12 = γ2 −1 =
1
1 −β2 −1 =
β2
1 −β2 = β2γ2 ⇒a12 = ±βγ.
The ambiguity in the sign comes from the choice we have for the direction of
motion. We absorb this choice of sign in β, and write
Λ =

γ
γβ
γβ
γ

.
(8.15)
For the important case of spacetime “position” vector (ct, x), this yields
Lorentz
transformation in
two spacetime
dimensions
ct′ = γ (ct + βx) ,
x′ = γ(x + βct).
(8.16)
β is positive (negative) when observer O—who uses (ct, x) for events—travels
in the positive (negative) direction of O′—who uses primed coordinates. Equa-
tion (8.16) displays the celebrated Lorentz transformations in two spacetime
dimensions.
Example 8.3.1. Emmy (observer O) is riding a train and she is standing in the
middle of one of the cars of length L at the two ends of which are two ﬁrecrackers that
explode simultaneously. Karl (observer O′) is standing on the platform watching
Emmy go by with speed β. Time zero for both coincides with the moment that Emmy
passes by Karl. Suppose that the simultaneous explosion of the two forecrackers
(according to Emmy) also takes place at t = 0. We want to see how all this appears
to Karl.
Assume that Emmy and Karl are located at their respective origins. Let the front
ﬁrecracker be labeled as 1 and the back as 2. Then the front and back ﬁrecrackers
have coordinates (0, L/2) and (0, −L/2), respectively, in Emmy’s RF. Karl, on the
other hand, measures the coordinates of the ﬁrecrackers as
ct′
1 = γβL/2,
x′
1 = γL/2
ct′
2 = γ(−βL/2),
x′
2 = γ(−L/2)

246
Vectors in Relativity
from Equation (8.16). This shows that, for Karl, the back ﬁrecracker occurs ﬁrst.
In fact, it occurs before Emmy reaches him (at time t′ = 0). The time diﬀerence
between the two events is
Δt′ = t′
1 −t′
2 = γβL/c.
Take L to be 30 m. Then, for the time diﬀerence to be a mere one second, we must
have
30γβ = 3 × 108
or
β
	
1 −β2 = 107,
giving β = 0.999999999999995, awfully close to the speed of light!
On the other hand, if L is a typical interstellar distance of say 10 light years,
then
γβ = Δt′
10
with Δt′ measured in years.
For a time diﬀerence of one hour, we have γβ =
1.14 × 10−5, yielding β = 1.14 × 10−5, or v = 3425 m/s, an easily attainable
speed.
■
Example 8.3.2. Observer O moves in the positive space direction of observer O′
at speed v (or β = v/c). A particle moves at speed βp in the positive space direction
of O. What is β′
p, the speed of the particle relative to O′?
The deﬁnition of speed is distance between two events divided by time interval
between those events: spotting of the particle at a point in space and an instant in
time (ﬁrst event), and spotting the particle at a nearby point a little later (second
event). For example, observer O assigns the coordinates (ct, x) to the ﬁrst event
and (ct + cΔt, x + Δx) to the second event, and concludes that the (dimensionless)
speed of the particle is βp = Δx/(cΔt).
Similarly, observer O′ assigns the coordinates (ct′, x′) to the ﬁrst event and
(ct′ + cΔt′, x′ + Δx′) to the second event, and concludes that the speed of the
particle is β′
p = Δx′/(cΔt′), where Δx′ and cΔt′ are related to Δx and cΔt via the
Lorentz transformation. Using Equation (8.16), we ﬁnd
β′
p = Δx′
cΔt′ = γ(Δx + βcΔt)
γ (cΔt + βΔx),
dividing the numerator and denominator by cΔt, we get
relativistic law of
addition of
velocities
β′
p = βp + β
1 + ββp ,
(8.17)
which is called the relativistic law of addition of velocities.
One can show that if 0 < βp < 1 and 0 < β < 1, then 0 < β′
p < 1. So, it is
impossible to add two velocities close to light speed and get a velocity larger than
light speed. Furthermore, if the particle happens to be a photon (or a light beam),
then βp = 1 and
β′
p = 1 + β
1 + β = 1,
verifying the universality of the speed of light, the starting point of relativity
theory!
■

8.4 Four-Velocity and Four-Momentum
247
In many situations, an observer in three dimensions moves along the x-
axis. Then, the y and z coordinates of events—being perpendicular to the
direction of motion—do not change. This suggests a slightly more general
Lorentz transformation than (8.16):
ct′ = γ (ct + βx) ,
x′ = γ(x + βct),
y′ = y,
(8.18)
z′ = z.
If an object moves in the xy-plane of an observer O with a velocity whose
components are (vx, vy), then the same object moves in the x′y′-plane of
another observer O′ with a velocity whose components are
vx′ = dx′
dt′ = γ(dx + βcdt)
γ (dt + βdx/c) =
vx + βc
1 + βvx/c,
vy′ = dy′
dt′ =
dy
γ (dt + βdx/c) =
vy
γ(1 + βvx/c),
(8.19)
where β is the velocity of O relative to O′. In particular, if the object is light
and the angle it makes with the x-axis is α, then vx = c cos α, vy = c sin α,
vx′ = c cos α′ and vy′ = c sin α′, and the equations above yield
cos α′ = cos α + β
1 + β cos α,
sin α′ =
sin α
γ(1 + β cos α).
(8.20)
Now suppose that an observer O carries an EM radiation source which
an ultrarelativistic
source radiates
only in the
forward direction.
radiates uniformly in all directions. If β is very close to 1, then (8.20) im-
plies that cos α′ →1 (and of course, sin α′ →0), regardless of α.
Thus,
an ultrarelativistic source of EM wave radiates (almost) only in the forward
direction.
8.4
Four-Velocity and Four-Momentum
In Newtonian mechanics velocity is deﬁned as the derivative of the position
vector with respect to time. In terms of (Cartesian) coordinates, an observer
O locates the object in motion by assigning it the coordinates (x, y, z), and
diﬀerentiates these coordinates with respect to (the universal) time t to get
the velocity of the object: ⃗v = ( ˙x, ˙y, ˙z).
In relativity, the “position vector” is r = (ct, x, y, z) ≡(ct,⃗r), and there is
no universal time. However, each moving object has a proper time (measured
by a clock carried by the object), which is universal in the sense that all
observers measure it to be the same [see Equation (8.4) and the comments

248
Vectors in Relativity
after it]. Therefore, it is natural to deﬁne the dimensionless four-velocity as
u ≡dr
ds = 1
c
dr
dτ =
 dt
dτ , 1
c
dx
dτ , 1
c
dy
dτ , 1
c
dz
dτ

= γ

1, ˙x
c , ˙y
c , ˙z
c

= γ (1,⃗v/c) ,
(8.21)
where a dot represents diﬀerentiation with respect to the coordinate time t,
and we used dt = γdτ [see Equation (8.3)].
An interesting property of the four-velocity is that its spacetime length is
one:
4-velocity has
constant length
u·u = u2
0 −u2
1 −u2
2 −u2
3 = γ2 [1 −(⃗v/c) · (⃗v/c)] = γ2  
1 −v2/c2!
= 1, (8.22)
from the deﬁnition of γ in (8.14). The four-velocity of an object in the object’s
rest frame is (1, 0, 0, 0), i.e., it is a unit vector in the time direction. If we deﬁne
the four-acceleration as the rate of change of the four-velocity with respect
to proper time, then the inner product of the 4-velocity and the 4-acceleration
of any object is zero, i.e., because of (8.22), the 4-acceleration is η-orthogonal
to the 4-velocity. Summarizing these two properties of the 4-velocity, we get
4-velocity is
perpendicular to
4-acceleration
u · u = 1,
u · a = 0.
(8.23)
Example 8.4.1. A particle is moving in the two-dimentional spacetime of an
inertial frame on a path given parametrically as
t(σ) = b sinh(σ),
x(σ) = cb cosh(σ),
where σ is a dimensionless parameter. The diﬀerential of the particle’s proper time
is
(cdτ)2 = (cdt)2 −(dx)2 = (cb)2 cosh2(σ) (dσ)2 −(cb)2 sinh2(σ) (dσ)2
= (cb)2 (dσ)2 ⇒dσ = 1
b dτ,
and σ = τ/b. Thus, as a function of the proper time, the path becomes
t(τ) = b sinh(τ/b),
x(τ) = cb cosh(τ/b).
The components of the (dimensionless) 4-velocity are
u0 = dt
dτ = cosh(τ/b),
u1 = dx
cdτ = sinh(τ/b),
which satisfy u2
0 −u2
1 = 1 as they should.
The acceleration of the particle has components
a0 = du0
dτ = 1
b sinh(τ/b),
a1 = du1
dτ = 1
b cosh(τ/b).
It is easily veriﬁed that a · u = 0 and that
a · a = a2
0 −a2
1 = −
1
b
2
.
So, the particle has a uniform acceleration of 1/b. The negative sign in the last
equation is due to the fact that the magnitude of the acceleration has to be deﬁned
as −a · a = ⃗a2 −a2
0, with the space part appearing as positive (so that when a0 is
absent, we get back the Newtonian acceleration).
■

8.4 Four-Velocity and Four-Momentum
249
The (kinematic) 4-velocity leads to the (dynamic) 4-momentum: just mul-
4-momentum
deﬁned
tiply u by mc—the c is to give dimension to the 4-velocity. In a reference
frame in which an object of mass m moves with velocity ⃗v, the 4-momentum
p is given by
p ≡(p0, p1, p2, p3) ≡(p0, ⃗p) = mcu = γmc (1,⃗v/c) = (γmc, γm⃗v) .
(8.24)
The space part of the 4-momentum is
relativistic
momentum
⃗p = γm⃗v =
m⃗v
	
1 −(v/c)2 ,
(8.25)
and gives ordinary Newtonian momentum when |⃗v| << c, because in that
limit, γ ≈1. Therefore, we call ⃗p the relativistic momentum.
What about p0? How are we to interpret that? If we set γ ≈1, we get
p0 ≈mc which does not correspond to any Newtonian quantity.3 However, if
we make the next best approximation to γ (see Example 10.2.1 and Problem
10.8), i.e.,
γ =
1
	
1 −(v/c)2 ≈1 + 1
2(v/c)2,
then
p0 = mcγ ≈mc
 
1 + 1
2v2/c2!
⇒p0c ≈mc2 + 1
2mv2.
The second term gives us the clue that p0c must be the relativistic energy
E. So we write
relativistic energy
p ≡(p0, ⃗p) = (E/c, ⃗p) = (γmc, γm⃗v),
E = γmc2 =
mc2
	
1 −(v/c)2 .
(8.26)
An important special case of this is the 4-momentum p of a particle in its rest
frame:
p = (mc,⃗0) = (mc, 0, 0, 0).
(8.27)
The deﬁnition of the relativistic energy allows objects to have rest energy:
when v = 0, we get
the most famous
equation in
physics!
E = mc2,
(8.28)
which states the equivalence of mass and energy and allows their conversion
into one another.
The invariance of the length of a 4-vector tells us that p · p is a quantity
that is independent of observers. From Equation (8.26), we get
p · p = (E/c)2 −|⃗p|2 = γ2m2c2 −γ2m2v2 = γ2m2c2(1 −v2/c2) = m2c2,
which we rewrite for future reference
p · p = m2c2
or
E2 −|⃗p|2c2 = m2c4.
(8.29)
3One may interpret mc as the momentum of an object moving at the speed of light.
However, while objects moving at light speed are possible in Newtonian physics, relativity
does not allow a massive object to go at the speed of light [see (8.25)].

250
Vectors in Relativity
Thus, although diﬀerent observers measure diﬀerent values for the energy
and 3-momentum of an object, when they subtract the square of their value
of momentum (times c) from their corresponding value of energy squared, all
get the same numerical value, namely the square of the mass of the object
(time c4).
Equation (8.29) allows particles with zero mass to have energy and mo-
mentum. For such particles,
E2 −|⃗p|2c2 = 0
or
E = |⃗p|c.
(8.30)
Since ⃗p/E = ⃗v/c2 [see Equation (8.26)], we conclude from (8.29) and (8.30)
that
Box 8.4.1. A particle is massless if and only if it moves at light speed.
The particle (quantum) of electromagnetic waves is photon. It travels at the
photon is
massless!
speed of light (obviously!). Therefore, it must be massless.
Example 8.4.2. A particle has 4-momentum p relative to an observer O′ whose 4-
velocity is u′. In the rest frame of this observer u′ = (1, 0, 0, 0), and if p = (E′/c, ⃗p ′)
in this frame, then
p · u′ = E′/c.
Now consider another observer O with respect to whom the 4-momentum of the
particle is p = (E/c, ⃗p ) and the 4-velocity of O′ is u′ = (γ, γ⃗v/c). In the frame of O,
p · u′ = γE/c −γ⃗p · ⃗v/c.
The invariance of the inner product now gives
E′ = γ(E −⃗p · ⃗v).
(8.31)
In the special case in which the particle is at rest with respect to O, ⃗p = 0 and
E = mc2. This leads to
E′ = γmc2 =
mc2
	
1 −(v/c)2 ,
which is the expected expression for the relativistic energy of a particle moving with
velocity ⃗v relative to O′.
■
8.4.1
Relativistic Collisions
Conservation of energy and momentum in relativistic collisions is stated suc-
cinctly in terms of the total four-momenta before and after: pbef
tot = paft
tot,
where in each case, ptot is the sum of the 4-momenta of all particles involved.
As a ﬁrst example, consider two particles that collide and form a single
third particle. Let the masses of the ﬁrst two particles be m1 and m2. We can
immediately ﬁnd the mass M of the third particle. Before doing so, we set

8.4 Four-Velocity and Four-Momentum
251
c = 1 to avoid the cluttering of calculations. This is common in high energy
physics, in which energy, momentum, and mass are all measured in the same
unit (usually electron volt, eV). If desired, we can easily restore the factors of
c at the end by a simple dimensional analysis. With this convention, Equation
(8.29) becomes p · p = m2.
The conservation of 4-momentum in the present situation is p1 + p2 = P,
where P is the four-momentum of the ﬁnal particle. Since this is a vector
equation, all components must equal. In particulare, separating the time and
the space parts, we get
p01 + p02 = P0,
or
E1 + E2 = E,
⃗p1 + ⃗p2 = ⃗P,
(8.32)
which are the conservation of energy and momentum.
Squaring both sides of p1 + p2 = P gives
(p1 + p2) · (p1 + p2) = P · P,
or
p1 · p1 + p2 · p2 + 2p1 · p2 = P · P,
or
m2
1 + m2
2 + 2p1 · p2 = M 2.
(8.33)
Because of the invariance of the dot product, this equation holds in any in-
ertial frame.
Let us evaluate (8.33) in the rest frame of the second particle, where
p2 = (m2,⃗0) by (8.27), and the energy of the ﬁrst particle is assumed to be
E1. Then
p1 · p2 = (E1, ⃗p1) · (m2,⃗0) = E1m2,
and Equation (8.33) immediately gives the mass of the ﬁnal particle:
M 2 = m2
1 + m2
2 + 2m2E1,
or
M 2 = m2
1 + m2
2 + 2m2E1/c2,
(8.34)
where the second equation restores the necessary powers of c. Note how the
initial energy E1 on the right-hand side has turned into (part of) the ﬁnal mass
M on the left-hand side. This is how large accelerators create new particles
out of the energy of collision.
We can also ﬁnd the momentum of the ﬁnal particle from the second
equation in (8.32). This easily gives ⃗P = ⃗p1, indicating that, in the rest frame
of particle 2, the ﬁnal particle moves in the initial direction of particle 1. The
magnitude of ⃗P can be calculated in terms of energies and masses:
|⃗P| = |⃗p1| =

E2
1 −m2
1.
(8.35)
The ﬁrst equation in (8.32) gives the energy of the ﬁnal particle
E = E1 + m2.
(8.36)

252
Vectors in Relativity
Combining Equations (8.35) and (8.36), we can obtain the speed of the ﬁnal
particle:
V = |⃗P|
E =
	
E2
1 −m2
1
E1 + m2
.
(8.37)
A more common collision has two particles initially and two ﬁnally. So
the conservation of 4-momentum becomes p1 + p2 = p3 + p4. Separating the
time and the space parts yields the conservation of energy and momentum:
E1 + E2 = E3 + E4,
⃗p1 + ⃗p2 = ⃗p3 + ⃗p4.
(8.38)
Squaring both sides of p1 + p2 = p3 + p4 gives
m2
1 + m2
2 + 2p1 · p2 = m2
3 + m2
4 + 2p3 · p4,
(8.39)
which holds in any inertial frame. Evaluating this equation in the rest frame
of the second particle, yields
m2
1 + m2
2 + 2m2E1 = m2
3 + m2
4 + 2(E3E4 −⃗p3 · ⃗p4).
(8.40)
In this frame, Equation (8.38) becomes E1 + m2 = E3 + E4 and ⃗p1 = ⃗p3 + ⃗p4.
Solving for E4 and ⃗p4 from these equations and substituting the results in
(8.40) yields (after some algebra and using E2
3 −|⃗p3|2 = m2
3)
m2
1 + m2
2 + 2m2E1 = m2
4 −m2
3 + 2E3(E1 + m2) −2⃗p1 · ⃗p3,
or
m2
1 + m2
2 + 2m2E1 = m2
4 −m2
3 + 2E3(E1 + m2) −2|⃗p1||⃗p3| cos θ13,
(8.41)
where θ13 is the scattering angle of the third particle. Once the energy E1
of the initial incident particle is known, Equation (8.41) gives the scattering
angle as a function of the energy of the third particle (|⃗p1| and |⃗p3| are related
to E1 and E3, respectively).
Example 8.4.3. The particle nature of light, which had been proposed by Einstein
Compton
scattering
in his explanation of the photoelectric eﬀect, was demonstrated by Compton in what
is now called the Compton scattering. In this scattering, a photon of energy E
is scattered oﬀa stationary electron of mass me. The scattered photon is detected
at an angle θ from the direction of the incident photon. What is the change in the
wavelength of the photon as a function of θ?
In (8.41), let 1 denote the incident photon, 2 the stationary electron, 3 the
scattered photon, and 4 the scattered electron. Let E′ denote the energy of the
scatterd photon, then, with m1 = m3 = 0, Equation (8.41) becomes
m2
e + 2meE = m2
e + 2E′(E + me) −2EE′ cos θ,
or
meE = E′(E + me) −EE′ cos θ ⇒me(E −E′) = EE′(1 −cos θ).

8.4 Four-Velocity and Four-Momentum
253
Restoring the factors of c and noting that E = hc/λ, we obtain
mec2
hc
λ −hc
λ′

=
hc
λ
  hc
λ′

(1 −cos θ),
which can be simpliﬁed to
Δλ ≡λ′ −λ =
h
mec(1 −cos θ) ≡λc(1 −cos θ),
(8.42)
where λc = h/mec is called the Compton wavelength of the electron. By mea-
suring the diﬀerence between the wavelengths of scattered and incident photons,
Compton could verify Equation (8.42) and demonstrate that light had particle
property.
■
8.4.2
Second Law of Motion
The Newtonian mechanics deﬁnes force as the rate of change of momentum.
We generalize this to relativity and deﬁne
f = dp
dτ = mdu
dτ = ma,
(8.43)
where τ is the proper time of the moving object with mass m, four-velocity u,
and four-momentum p. Let us explore the meaning of the components of f.
In a particular inertial frame, we assume that Newton’s second law holds:
d⃗p
dt = ⃗F,
(8.44)
where ⃗p is the space part of the 4-momentum. The space part of f can now
be written as
⃗f = d⃗p
dτ = d⃗p
dt
dt
dτ = γ ⃗F.
(8.45)
The time part of f is a little trickier. First note that
f0 = dp0
dτ = 1
c
dE
dτ .
Next diﬀerentiate (8.29) with respect to τ to obtain E(dE/dτ) = c2⃗p·(d⃗p/dτ).
Finally use ⃗p/E = ⃗v/c2 to arrive at
f0 = 1
c
dE
dτ = 1
c
c2⃗p
E · d⃗p
dτ = 1
c γ⃗v · ⃗F = γ⃗β · ⃗F,
where ⃗β ≡⃗v/c. Thus,
f ≡(f0, ⃗f) = (γ⃗β · ⃗F, γ ⃗F).
(8.46)
The fact that f0 = γ⃗β · ⃗F could also be obtained by using f · u = 0, which
is a result of Equation (8.43) and the orthogonality of the 4-velocity and
4-acceleration (see Problem 8.13).

254
Vectors in Relativity
Example 8.4.4. Let a constant force act on a particle of mass m in some inertial
frame. What is the speed of the particle at time t if it starts from rest?
Equation (8.44) can be trivially integrated to give ⃗p = ⃗Ft. Since the force is
constant, the motion takes place in one dimension. So, we can ignore the vector
sign and (remembering that β = v/c) write
mγv = Ft,
or
mγβ = Ft
c ,
or
β
	
1 −β2 = Ft
mc.
Squaring both sides and solving for β gives
β =
Ft/mc
	
1 + (Ft/mc)2 ,
or
v =
Ft/m
	
1 + (Ft/mc)2 .
(8.47)
Note that for large t (i.e., when Ft >> mc), β ≈1 or v ≈c. However, the particle
can never attain the speed of light no matter how long we wait. On the other hand,
if Ft << mc, then v = (F/m)t, which is the Newtonian speed of a particle moving
with constant acceleration.
It is interesting to consider a particle having a constant acceleration of 10 m/s2
(approximately Earth’s gravitational acceleration). How long does it take to attain a
speed of 0.999c? Over 21 years! (See Problem 8.14). On the other hand, Newtonian
mechanics requires under one year to achieve the same speed!
■
8.5
Problems
8.1. Show that Equation (8.6) follows from Equation (8.5). Hint: Consider
the three vectors a, b, and c = a + c.
8.2. Multiply the matrices in Equation (8.10) to obtain the three equations
of (8.11). Solve these equations to ﬁnd all matrix elements in terms of a11.
8.3. In Example 8.3.1, Emmy receives the two signals from the explosions at
the same time.
(a) Show that this time is L/(2c) according to Emmy, and γL/(2c) according
to Karl.
(b) Let T ′
1 and T ′
2 denote the times that Karl receives the signal from the
front and back ﬁrecrackers, respectively. Show that
T ′
1 = L
2c
"
1 + β
1 −β ,
T ′
2 = L
2c
"
1 −β
1 + β .
(c) How is ΔT ′ ≡T ′
1 −T ′
2 related to Δt′ calculated in Example 8.3.1? Discuss
your answer.
8.4. Show that the relativistic law of addition of velocities (8.17) prohibits
the sum of two large velocities to be larger than the speed of light. Hint:
Multiply both sides of βp < 1 by 1 −β.
8.5. Show that the 4-acceleration is η-orthogonal to the 4-velocity.

8.5 Problems
255
8.6. Provide the details of the proof of the statement: a particle is massless
if and only if it moves at light speed.
8.7. Apply (8.31) to a photon moving in the x-direction and use |⃗p| = E/c
to show that
E′ =
"
1 −β
1 + β E.
Now use E = hc/λ to ﬁnd a formula for the relativistic Doppler shift.
8.8. Two identical particles of mass m approach each other along a straight
line with speed v = βc as measured in the lab frame. Show that the energy
of one particle as measured in the rest frame of the other is
1 + β2
1 −β2 mc2.
8.9. A particle of mass m and relativistic energy 4mc2 collides with another
stationary particle of mass 2m and sticks to it.
What is the mass of the
resulting composite particle.
8.10. An electron of kinetic energy 1 GeV (109 eV) strikes a positron (anti-
electron) at rest and the two particles annihilate each other and produce two
photons, one moving in the forward direction (the direction that electron had
before collision) and the other in the backward direction. What are the ener-
gies of the two photons. The mass (times c2) of electron and positron are the
same and equal to 0.511 MeV (106 eV).
8.11. A particle of mass m and energy E collides with an identical particle
at rest. The collision results in the formation of a single particle. Show that
the mass and the speed of the formed particle are, respectively,
	
2m(E + m)
and
	
(E −m)/(E + m), assuming that c = 1.
8.12. A photon of energy E is absorbed by a stationary nucleus of mass m.
The collision results in an excitation of the nucleus. Show that the mass and
the speed of the excited nucleus are, respectively,
	
m(2E + m) and E/(E +
m), assuming that c = 1.
8.13. Use Equations (8.21), (8.43), (8.45), and the orthogonality of the 4-
velocity and 4-acceleration to show that f0 = γ⃗β · ⃗F.
8.14. How long does it take a particle to attain a speed of 0.999c, if its
acceleration is 10 m/s2? What is the answer based on Newtonian mechanics?
How do the answers change if the ultimate speed of the particle is 0.99999c?

Part III
Inﬁnite Series

Chapter 9
Inﬁnite Series
Physics is an exact science of approximation. Although this statement sounds
like an oximoron, it does summarize the nature of physics. All the laws we deal
with in physics are mathematical laws, and as such, they are exact. However,
once we try to apply them to Nature, they become only approximations.
Therefore, methods of approximation play a central role in physics. One such
method is inﬁnite series which we study in this chapter.
9.1
Inﬁnite Sequences
An inﬁnite sequence is an association between the set of natural numbers
inﬁnite sequence
(often zero is also included) and the real numbers, so that for every natural
number k there is a real number sk. Instead of the association, one calls the
collection of real numbers the inﬁnite sequence. Two common notations for
a sequence are an indicated list, and enclosure in a pair of braces, as given
below:
{s1, s2, . . . , sk, . . .} ≡{sk}∞
k=1 .
Instead of k, one can use any other symbol usually used for natural numbers
such as i, j, n, m, etc. We call sn the nth term of the sequence.
In practice, elements of a sequence are given by a rule or formula. The
following are examples of sequences:
(1
2, 1
4, 1
8, . . .
)
=
( 1
2n
)∞
n=1
,
0
2,
3
2
2
,
4
3
3
, . . .
1
=
0k + 1
k
k1∞
k=1
,
(
1, 1
23 , 1
33 , . . .
)
=
( 1
j3
)∞
j=1
,
(1
2, −2
3, 3
4, . . .
)
=
(
(−1)m+1
m
m + 1
)∞
m=1
.
(9.1)
An important sequence is the sequence of partial sums in which each
term is a sum. Examples of such sequences are the following:
sequence of partial
sums

260
Inﬁnite Series
(
1, 1 + 1
2, 1 + 1
2 + 1
4, . . .
)
,
(
1, 1 + 1
2, 1 + 1
2 + 1
3, . . .
)
,
(
1, 1 + 1
23 , 1 + 1
23 + 1
33 , . . .
)
,
(
1, 1 + 1, 1 + 1 + 1
2!, . . .
)
.
The nth term of the sequences above are, respectively,
sn = 1 + 1
2 + 1
4 + · · · + 1
2n =
n

k=0
1
2k ,
sn = 1 + 1
2 + 1
3 + · · · + 1
n =
n

i=1
1
i ,
sn = 1 + 1
23 + 1
33 + · · · + 1
n3 =
n

j=1
1
j3 ,
sn = 1 + 1 + 1
2! + · · · + 1
n! =
n

k=0
1
k!,
so that the sequences can be written, respectively, as
convention:
0! = 1.
0 n

k=0
1
2k
1∞
n=0
,
0 n

i=1
1
i
1∞
n=1
,
⎧
⎨
⎩
n

j=1
1
j3
⎫
⎬
⎭
∞
n=1
,
0 n

k=0
1
k!
1∞
n=0
.
(9.2)
In the last sequence, we have used the usual deﬁnition, 0! ≡1. A sequence is
convergence and
limit of a sequence
said to converge to the number s or to have limit s if for every positive (usu-
ally very small) real number ϵ there exists a (usually large) natural number
N such that |sn −s| < ϵ whenever n > N. We then write
lim
n→∞sn = lim
ν→∞sν = lim
♣→∞s♣= lim
♥→∞s♥= s.
(9.3)
Note the freedom of choice in using the symbol of the limit. A sequence that
does not converge is said to diverge. The ﬁrst three sequences in Equation
(9.1) are convergent and their limits are
lim
n→∞
 1
2n

= 0,
lim
n→∞
n + 1
n
n
= e,
lim
n→∞
 1
n3

= 0.
The last sequence diverges because there is no single number to which the
terms get closer and closer.
There are many ways that a sequence can converge to its limit. For in-
stance, the terms sn may steadily increase toward s after some large integer

9.1 Inﬁnite Sequences
261
(a)
(b)
(c)
(d)
(e)
(f)
s5
s
s
s
A
A
A
A
B
B
B
B
s4
s3
s2
s1
s5
s4
s3
s2
s1
s5 s4
s3
s2
s1
s5
s4
s3
s2
s1
s5
s4
s3
s2
s1
s5 s3
s8
s1
s4
s7
s2
s6
Figure 9.1: Types of sequences and modes of their convergence: (a) convergent, (b)
convergent monotone increasing, (c) convergent monotone decreasing, (d) divergent
monotone increasing, (e) divergent bounded, (f) divergent unbounded.
N, so that for all n ≥N, sn ≤sn+1 ≤sn+2 ≤sn+3 ≤· · · .
1 In this case
we say that the sequence is monotone increasing. If the terms sn steadily
monotone
increasing,
monotone
decreasing, and
bounded
sequences
decrease toward s after some large integer N, the sequence is called mono-
tone decreasing. A sequence may bounce back and forth on either side of
its limit, getting closer and closer to it. A sequence is called bounded if there
exist two numbers A and B such that
A ≤sn ≤B
for all n.
A sequence may be bounded but divergent. Various forms of convergence and
divergence are depicted in Figure 9.1.
A sequence may have an upper and/or a lower limit. The upper limit is
a number s such that there are inﬁnitely many n’s with the property that sn
is very close to s if n is large enough, and there is no other number larger
than s with the same property. Similarly, the lower limit is a number s such
that there are inﬁnitely many n’s with the property that sn is very close to
s if n is large enough, and there is no other number smaller than s with the
same property. The last sequence of Equation (9.1) has an upper limit of 1
and a lower limit of −1. It is intuitively obvious that a sequence converges if
and only if its upper and lower limits are ﬁnite and equal. For instance, the
sequence {(−1)n/n}∞
n=1 converges to the single limit 0 after bouncing left and
right of it inﬁnitely many times.
One can decide whether a sequence converges or not without knowing its
limit:
Cauchy criterion
1We often use the loose phrase: “For large enough n, . . . .” The precise statement would
be: There exists an N such that for all n ≥N, . . . .

262
Inﬁnite Series
Box 9.1.1. (Cauchy Criterion). The sequence {sn}∞
n=1 converges if
the diﬀerence sn −sm approaches zero as both m and n approach inﬁnity.
We can add, subtract, multiply, and divide two convergent sequences term
by term and obtain a new sequence. The limit of the new sequence is obtained
by the corresponding operation of the limits. Thus, if
lim
n→∞xn = x,
lim
n→∞yn = y,
then
lim
n→∞(xn ± yn) = x ± y,
lim
n→∞(xn · yn) = x · y,
lim
n→∞
xn
yn
= x
y ,
provided, of course, that y ̸= 0 when it is in the denominator.
9.2
Summations
We have been using summation signs on a number of occasions, and we shall
be making heavy use of them in this chapter as well. It is appropriate at
this point to study some of the properties associated with such sums. Every
summation has a dummy index which has a lower limit, usually written
dummy
summation index
can be any symbol
you want it to be!
under the summation symbol , and an upper limit, usually written
above it.
The limits are always ﬁxed, but the dummy index can be any
symbol one wishes to use except the symbols used in the expression being
summed. Therefore, all the following sums are identical:
N

i=1
aixi,
N

k=1
akxk,
N

α=1
aαxα,
N

♣=1
a♣x♣,
N

ℵ=1
aℵxℵ.
(9.4)
It is not a good idea, however, to use a or x as the dummy index for the
summation above!
When adding or subtracting sums of equal length, it is better to use the
same symbol for the dummy index of the sum:
N

i=1
ai +
N

♥=1
b♥=
N

i=1
(ai + bi) =
N

♥=1
(a♥+ b♥) =
N

k=1
(ak + bk).
However,
Box 9.2.1. When multiplying two sums (not necessarily of equal length),
it is essential to choose two diﬀerent dummy indices for the two sums.

9.2 Summations
263
Thus, to multiply N
i=1 ai by M
i=1 bi, one writes
N

i=1
ai
M

j=1
bj =
N

i=1
M

j=1
aibj.
Failure to obey this simple rule can lead to catastrophe. For example, one
may end up with N
i=1 ai
M
i=1 bi = N
i=1
M
i=1 aibi, which is a sum of terms
of the form a1b1 + a2b2 + · · · , excluding terms such as a1b2 or a3b5, etc.
The freedom of choice for the symbol of dummy index can be used to ma-
nipulate sums and get results very quickly. As an example, suppose that {aij}
is a set of (doubly indexed) numbers which are symmetric under interchange
of their indices, i.e., aij = aji. Similarly, suppose that bij are antisymmet-
ric under interchange of their indices, i.e., bij = −bji. Furthermore, assume
that i and j have the lower limit of 1 and the upper limit of n. What is
n
i=1
n
j=1 aijbij? Call this sum S. Since the choice of the dummy symbol
is irrelevant, we have
S =
n

i=1
n

j=1
aijbij =
n

α=1
n

β=1
aαβbαβ = −
n

α=1
n

β=1
aβαbβα,
(9.5)
where we used the symmetry of aij and the antisymmetry of bij. Since the
order of summation is irrelevant, we can write S as S = −n
β=1
n
α=1 aβαbβα.
Once again, change the dummy symbols: Choose i for β and j for α. Then
Equation (9.5) becomes
S = −
n

i=1
n

j=1
aijbij = −S ⇒2S = 0 ⇒S = 0.
As another illustration, suppose we want to multiply M
i=0 aiti and N
i=0 biti,
and express the coeﬃcient of a typical power of t in the product in terms of
ai and bi. Call the product P. Then
P =
M

i=0
aiti
N

j=0
bjtj =
M

i=0
N

j=0
aibjti+j.
We need to use a single symbol for the power of t in the double sum. So, let
α = i + j. Our goal is to write P =  cαtα, ﬁnd cα in terms of ai and bi,
and determine the lower and upper limits of the summation on α. The latter
is easy: α has a lower limit of 0 (when both i and j are zero), and an upper
limit of M + N.
For the second dummy index we choose one of the original indices, say i.
The limits of i cannot be the original limits, because i is now mixed up with
α and j through j = α−i. Because of the original bounds of i and j, we have
0 ≤i ≤M as well as
0 ≤α −i ≤N
or
−α ≤−i ≤N −α
or
α ≥i ≥α −N.

264
Inﬁnite Series
Since i is greater than both 0 and α−N, it must be greater than the maximum
of the two: i ≥max(0, α −N). This means that the lower limit of the i-
summation is max(0, α −N). Similarly, since i is smaller than both M and
α, it must be smaller than the minimum of the two: i ≤min(M, α), making
the upper limit of the i-summation min(M, α). We therefore have
P =
M+N

α=0
min(M,α)

i=max(0,α−N)
aibα−itα =
M+N

α=0
⎛
⎝
min(M,α)

i=max(0,α−N)
aibα−i
⎞
⎠



≡cα
tα.
(9.6)
Example 9.2.1. As further practice in working with the summation symbol, we
show that the torque on a collection of particles is caused by external forces only.
The torques due to the internal forces add up to zero. We have already illustrated
this for three particles in Example 1.3.5.
Here, we generalize the result to any
number of particles.
We use the second formula in Equation (1.31) and separate the forces
T =
N

k=1
rk × Fk =
N

k=1
rk ×
⎛
⎝F(ext)
k
+

i̸=k
Fki
⎞
⎠
=
T(ext)



N

k=1
rk × F(ext)
k
+
T(int)



N

k=1

i̸=k
rk × Fki .
We need to show that the double sum is zero. To do so, we break the inner sum
into two parts, i > k and i < k. This yields
T(int) ≡
N

i,k=1
i̸=k
rk × Fki =
N

i,k=1
i>k
rk × Fki +
N

i,k=1
i<k
rk × Fki
=
N

i,k=1
i>k
rk × Fki −
N

i,k=1
i<k
rk × Fik,
because, by the third law of motion, Fik = −Fki. Now, in the second sum, change
the dummy indices twice:
T(int) =
N

i,k=1
i>k
rk × Fki −
N

α,β=1
α>β
rα × Fβα
=
N

i,k=1
i>k
rk × Fki −
N

i,k=1
i>k
ri × Fki =
N

i,k=1
i>k
(rk −ri) × Fki.
As in Example 1.3.5, we assume that Fki and rk −ri lie along the same line in which
case the cross products in the sum are all zero.
■

9.2 Summations
265
In the sequel, we shall have many occasions to use summations and ma-
nipulate them in ways similar to above. The reader is urged to go through
such manipulations with great care and diligence. The skill of summation
techniques is acquired only through such diligent pursuit.
9.2.1
Mathematical Induction
Many a time it is desirable to make a mathematical statement that is true
for all natural numbers. For example, we may want to establish a formula
involving an integer parameter that will hold for all positive integers. One
encounters this situation when, after experimenting with the ﬁrst few positive
integers, one recognizes a pattern and discovers a formula, and wants to make
sure that the formula holds for all natural numbers. For this purpose, one
uses mathematical induction. The essence of mathematical induction is
stated in
induction principle
Box 9.2.2. (Mathematical Induction). Suppose that there is asso-
ciated with a natural number (positive integer) n a statement Sn. Then
Sn is true for every positive integer provided the following two conditions
hold:
1. S1 is true.
2. If Sm is true for some given positive integer m, then Sm+1 is also
true.
We illustrate the use of mathematical induction by proving the binomial
theorem:
binomial theorem
(a + b)m =
m

k=0
m
k

am−kbk =
m

k=0
m!
k!(m −k)!am−kbk
= am + mam−1b + m(m −1)
2!
am−2b2 + · · · + mabm−1 + bm, (9.7)
where we have used the shorthand notation
m
k

≡
m!
k!(m −k)!.
(9.8)
The mathematical statement Sm is Equation (9.7). We note that S1 is trivially
true: (a + b)1 = a + b. Now we assume that Sm is true and show that Sm+1
is also true. This means starting with Equation (9.7) and showing that
(a + b)m+1 =
m+1

k=0
m + 1
k

am+1−kbk.

266
Inﬁnite Series
Then the induction principle ensures that the statement (equation) holds for
all positive integers.
Multiply both sides of Equation (9.7) by a + b to obtain
(a + b)m+1 =
m

k=0
m
k

am−k+1bk +
m

k=0
m
k

am−kbk+1.
Now separate the k = 0 term from the ﬁrst sum and the k = m term from
the second sum:
(a + b)m+1 = am+1 +
m

k=1
m
k

am−k+1bk +
m−1

k=0
m
k

am−kbk+1



let k = j −1 in this sum
+bm+1
= am+1 +
m

k=1
m
k

am−k+1bk +
m

j=1
 m
j −1

am−j+1bj + bm+1.
The second sum in the last line involves j. Since this is a dummy index, we
can substitute any symbol we please. The choice k is especially useful because
then we can unite the two summations. This gives
(a + b)m+1 = am+1 +
m

k=1
(m
k

+
 m
k −1
)
am−k+1bk + bm+1.
If we now use
m + 1
k

=
m
k

+
 m
k −1

which the reader can easily verify, we ﬁnally obtain
(a + b)m+1 = am+1 +
m

k=1
m + 1
k

am−k+1bk + bm+1
=
m+1

k=0
m + 1
k

am−k+1bk.
Mathematical induction is also used in deﬁning quantities involving inte-
gers. Such deﬁnitions are called inductive deﬁnitions. For example, induc-
inductive
deﬁnitions
tive deﬁnition is used in deﬁning powers: a1 = a and am = am−1a.
9.3
Inﬁnite Series
An inﬁnite series is an indicated sum of the members of a sequence {ak}∞
k=1.
This sum is written as
a1 + a2 + a3 + · · · ≡
∞

k=1
ak ≡
∞

j=1
aj ≡
∞

n=1
an ≡
∞

♣=1
a♣,

9.3 Inﬁnite Series
267
where we have exploited the freedom of choice in using the dummy index as
emphasized in the previous section.
Box 9.3.1. Associated with an inﬁnite series is the sequence of partial
sums {Sn}∞
n=1 with Sn = a1 + a2 + · · · + an = n
k=1 ak. A series is con-
vergent (divergent) if its associated sequence of partial sums converges
(diverges).
For a convergent series the nth member of the sequence of partial sums will
be a good approximation to the series if n is large enough. This is a simple
but important property of the series that is very useful in practice. It should
be clear that the convergence property of a series is not aﬀected by changing
a ﬁnite number of terms in the series. Convergent series can be added or
multiplied by a constant to obtain new convergent series. In other words, if
∞
n=1 an = A and ∞
n=1 bn = B, then
∞

n=1
(an ± bn) = A ± B,
r
∞

n=1
an = rA,
for any real number r.
9.3.1
Tests for Convergence
When adding, subtracting, or multiplying ﬁnite sums, no problem occurs
because these operations are all well deﬁned for a ﬁnite number of terms.
However, when adding an inﬁnite number of terms, no operation on the inﬁnite
sum will be deﬁned unless the series converges. It is therefore important to
have criteria to test whether a series converges or not. We list various tests
which are helpful in determining whether an inﬁnite series is convergent or
not.
The nth Term Test
If limn→∞an ̸= 0, then ∞
n=1 an diverges. This is easily shown by looking at
if the inﬁnite
series is to
converge, its nth
term must
approach zero.
But that by itself
is not enough for
convergence!
the diﬀerence Sn −Sn−1 and noting that it is simply an, and that if the series
converges, then this diﬀerence must approach zero by the Cauchy criterion.
Thus none of the following series converges:
∞

n=1
n
n + 1,
∞

k=1
(−1)k k −1
5k −1,
∞

j=1
(−1)j,
∞

m=1
m2 −10
8m2 + 1.
On the other hand, the series
∞

n=1
n
n2 + 1,
∞

k=1
(−1)k 1
k ,
∞

j=1
1
j ,
∞

m=1
1
m2 ,

268
Inﬁnite Series
may or may not converge: The approach of an to zero does not guarantee
the convergence of the series. In fact, the ﬁrst and third of the series above
diverge while the second and last converge.
Box 9.3.2. Do not confuse the convergence of an inﬁnite series with the
convergence of its nth term. If the nth term converges to anything but
zero, the series will not converge!
Absolute Convergence
If ∞
n=1 |an| converges, so does ∞
n=1 an. The series is then said to be abso-
absolute
convergence
lutely convergent. For example, the series ∞
k=1(−1)k/2k converges because
∞
k=1 1/2k converges. However, although the series ∞
k=1 1/k can be shown
to diverge, ∞
k=1(−1)k/k is known to converge.
Comparison Test
If |an| ≤bn for large enough values of n and ∞
n=1 bn converges, then ∞
n=1 an
is absolutely convergent and ∞
n=1 an ≤∞
n=1 bn.
On the other hand, if
an ≥bn ≥0 for large values of n and ∞
n=1 bn diverges, then so does ∞
n=1 an.
Integral Test
This is probably the most powerful test of convergence for inﬁnite series.
Assume that limn→∞an = 0, so that the series is at least a candidate for
convergence. Now ﬁnd a function f which expresses an, i.e., such that f(n) =
an, and assume that f(n) decreases monotonically for large values of n. Then
Theorem 9.3.1. The series ∞
n=1 an converges if and only if the integral
 ∞
c
f(t) dt exists and is ﬁnite for some real number c > 1.
To see this, refer to Figure 9.2 and suppose that c lies between two con-
secutive positive integers m and m + 1. Since the convergence or divergence
of a series is not aﬀected by the removal of a ﬁnite number of terms of the
series, we are allowed to consider either the series ∞
k=m ak or ∞
k=m+1 ak.
Figure 9.2(a) compares the area under the curve f(t) with the shaded area
which is the sum of the areas of an inﬁnite number of rectangles each of height
f(k) = ak for some positive integer k larger than (or equal to) m + 1. The
width of all rectangles is unity. The shaded area A is therefore
A =
∞

k=m+1
f(k)

=ak
Δt =
∞

k=m+1
ak · 1 =
∞

k=m+1
ak.
It is clear from Figure 9.2(a) that
A <
# ∞
c
f(t) dt ⇒
∞

k=m+1
ak <
# ∞
c
f(t) dt.

9.3 Inﬁnite Series
269
t = m
t = c
t = m + 1
t = m
t = c t = m + 1
)b
(
)a(
f (t)
f (t)
t
t
Figure 9.2: The area under the curve (a) bounds, and (b) is bounded by, the inﬁnite
sum obtained from the series by removing a ﬁnite number of terms. This ﬁnite number
of terms is the ﬁrst m terms for (a) and the ﬁrst m −1 terms for (b).
Similarly, Figure 9.2(b) shows that ∞
k=m ak is larger than the area under the
curve. We thus can write
∞

k=m+1
ak <
# ∞
c
f(t) dt <
∞

k=m
ak.
Hence, if the integral is ﬁnite ∞
k=m+1 ak (being smaller than the integral)
is also ﬁnite and the series converges. If, on the other hand, the integral is
inﬁnite then ∞
k=m ak (being larger than the integral) diverges.
The integral test leads directly to the observation that the Riemann zeta
function, also called the harmonic series of order p deﬁned by
Riemann zeta
function or
harmonic series of
order p
ζ(p) ≡
∞

k=1
1
kp = 1 + 1
2p + 1
3p + · · ·
(9.9)
converges for p > 1 and diverges for p ≤1. In particular,
ζ(1) =
∞

k=1
1
k = 1 + 1
2 + 1
3 + · · · ,
called simply the harmonic series, diverges.
harmonic series
Ratio Test
Consider the series ∞
n=1 an. If an ̸= 0 for large enough n and
lim
n→∞




an+1
an




 = R,
then the series is absolutely convergent if R < 1 and is divergent if R > 1.

270
Inﬁnite Series
The terms that we choose for the ratio test need not be consecutive. To
see this, note that
lim
n→∞




an+2
an




 = lim
n→∞




an+2
an+1




 · lim
n→∞




an+1
an




 =

lim
n→∞




an+1
an




2
.
In going to the last equality, we have used the following:
lim
n→∞




an+2
an+1




 =
lim
(m−1)→∞




am+1
am




 = lim
m→∞




am+1
am




 = lim
n→∞




an+1
an




 ,
where we have substituted m = n + 1 and used Equation (9.3) and the fact
that m →∞if and only if (m −1) →∞. It now follows that
lim
n→∞




an+1
an




 =
"
lim
n→∞




an+2
an




and the LHS will be less than or greater than one if the term inside the square
root sign is. In fact, one can generalize the above argument and state that
the series is convergent (divergent) if
lim
n→∞




an+j
an




 =

lim
n→∞




an+1
an




j
(9.10)
is less than (greater than) one for any ﬁnite j.
The Riemann zeta function can sharpen the ratio test of convergence to
allow for certain cases in which the ratio is one. Instead of taking the complete
limit, we approximate the ratio of consecutive terms for the Riemann zeta
function to ﬁrst order in 1/n. This yields
an+1
an
=

n
n + 1
p
=
n + 1
n
−p
=

1 + 1
n
−p
≈1 −p
n,
where we used the binomial expansion formula, to which we shall come back
[see Equation (10.15)]. We know that such a ratio leads to a convergent series
if p > 1 and to a divergent series if p ≤1. Therefore, we obtain
Theorem 9.3.2. (Generalized Ratio Test).
If the ratio of consecutive
terms of a series satisﬁes




an+1
an




 →1 −p
n, then the series converges if p > 1
and diverges if p ≤1.
Alternating Series Test
An alternating series
a1 −a2 + a3 −a4 + · · · =
∞

j=1
(−1)j+1aj,
aj > 0,
converges if limj→∞aj = 0, and if there exists a positive integer N such that
ak > ak+1 for all k > N.

9.3 Inﬁnite Series
271
Example 9.3.3. A useful series is the geometric series:
geometric series
b + bu + bu2 + bu3 + · · · =
∞

k=0
buk.
We claim that this series converges to b/(1 −u) if |u| < 1, and diverges if |u| ≥1.
To show this, let Sn represent the sum of the ﬁrst n terms, so that {Sn}∞
n=0 is the
sequence of partial sums. We calculate Sn as follows. First note that
Sn =
n

k=0
buk ⇒uSn =
n

k=0
buk+1.
Next separate the zeroth term from the rest of Sn and rewrite it as
Sn = b +
n

k=1
buk = b +
n−1

m=0
bum+1 = b +
n−1

k=0
buk+1,
where in the second equality, we changed k to m = k −1 and in the last equality
we changed the dummy index back to k. Subtracting uSn from Sn, we obtain
Sn −uSn = (1 −u)Sn = b +
n−1

k=0
buk+1 −
n

k=0
buk+1
= b +
n−1

k=0
buk+1 −
n−1

k=0
buk+1 + bun+1

= b −bun+1
or
Sn = b −bun+1
1 −u
.
It is now clear that un+1 →0 for n →∞only if |u| < 1. For |u| > 1, the series
clearly diverges. For |u| = 1 the partial sum is either Sn = nb (when u = 1), which
diverges for any nonzero b, or Sn = b ∞
n=0(−1)n, which bounces back and forth
between +b and −b, and never converges. So the series diverges for |u| ≥1
For example, if b = 0.3 and u = 0.1, then the series gives
0.3 + 0.3 × 0.1 + 0.3 × 0.01 + · · · = 0.33333 · · · =
0.3
1 −0.1 = 1
3.
For b = 1 the series gives
1 + u + u2 + · · · =
1
1 −u = (1 −u)−1,
(9.11)
which can be thought of as the binomial expansion when the power is −1. As we
shall see in Section 10.1, there is a generalization of binomial expansion for any real
power.
■
The result of Example 9.3.3 is important enough to be summarized:
Box 9.3.3. The series b + bu + bu2 + bu3 + · · · = ∞
k=0 buk is called the
geometric series. It converges to b/(1 −u) if |u| < 1, and diverges if
|u| ≥1.

272
Inﬁnite Series
Example 9.3.4. Another example of a series used often is
1 + 1 + 1
2! + 1
3! + · · · =
∞

k=0
1
k!.
The ratio test shows only that the series converges, but the comparison test gives
us more information. In fact, since 1/n! ≤1/2n−1 for n ≥1, we conclude that
1 + 1
2! + 1
3! + · · · ≤1 + 1
2 + 1
22 + 1
23 + · · · .
But the RHS is the geometric series with u = 1/2 which is known to converge to 2.
We thus obtain the upper bound to our series:
2 ≤
∞

k=0
1
k! ≤3.
It is well known that the series converges to e = 2.718281828 · · · .
■
Example 9.3.5. If one alternates the sign of the terms in the harmonic series,
one obtains the series
1 −1
2 + 1
3 −1
4 + · · ·
which is convergent by the alternating series test. In fact, we shall show in Example
conditional
convergence
9.4.4 that the series converges to ln 2. Note that the series is not absolutely conver-
gent. A convergent series that does not converge absolutely is called conditionally
convergent.
■
Historical Notes
The invention of calculus motivated several other areas of investigation in math-
ematics.
One of these areas was inﬁnite series.
For example, it was not always
possible to ﬁnd a closed formula for the integral of a function. So, it was common to
expand the integrand in powers of the variable and integrate the resulting inﬁnite
series.
No question was asked as to the legitimacy of the operations performed.
In fact, Newton, Leibniz, and Euler regarded inﬁnite series as an extension of the
algebra of polynomials, and they did not realize that new problems would arise if
a ﬁnite sum were extended to an inﬁnite series. However the apparent diﬃculties
that did arise caused them occasionally to bring up the question of convergence and
divergence.
Some mathematicians of the seventeenth century had observed the diﬀerence
between convergence and divergence. In 1668 Lord Brouncker, while studying the
relation between ln x and the area under y = 1/x, demonstrated the convergence
of the series for ln 2 and ln( 5
4) by comparison with a geometric series. Newton and
James Gregory, who made much use of the numerical values of series to calculate
logarithmic and other function tables and to evaluate integrals, were aware that the
sum of a series can be ﬁnite or inﬁnite. The terms “convergent” and “divergent”
were actually used by Gregory in 1668, but he did not develop the ideas.
Leibniz, too, felt some concern about convergence and noted in a letter of Oc-
tober 25, 1713 to John Bernoulli what is now a theorem that we call the alternating
series test. Maclaurin used series as a regular method for integration. He recognized

9.3 Inﬁnite Series
273
that the terms of a convergent series must continually decrease and become less than
any given quantity no matter how small.
D’Alembert also distinguished convergent from divergent series. In his article
“S´erie” in the Encyclop´edie he describes a convergent series as that which approaches
a ﬁnite value and consequently has terms that keep diminishing.
In this same
volume, d’Alembert gave a test for the absolute convergence of the series ∞
k=1 ak,
namely, if for all k > N, the ratio |ak+1/ak| < r where r is a positive number
independent of k and less than 1, the series converges absolutely.
Edward Waring (1734–1798), Lucasian professor of mathematics at Cambridge
University, held advanced views on convergence. He showed that the harmonic series
of order p converges if p > 1 and diverges if p < 1. He also gave the well-known test
for convergence and divergence, now known as the ratio test.
9.3.2
Operations on Series
It has already been mentioned that convergent series can be added, subtracted,
and multiplied by a constant. There are other important operations one can
perform on convergent series. These operations may be “obvious” for ﬁnite
sums, but they have to be justiﬁed for inﬁnite series. In fact, performing such
obvious operations on divergent series leads to contradictory results.
One such operation is grouping:
grouping of
convergent series
Box 9.3.4. One can group the terms of a ﬁnite sum or a convergent
inﬁnite series in any way one desires, and the sum will not change.
The operation of grouping is essentially putting parentheses around a collec-
tion of terms of the series (or the sum), adding the terms inside each parenthe-
ses ﬁrst, and then adding the results. This is simply the associative property
of addition. It turns out that this associative property of addition does not
apply to divergent inﬁnite series.2 For example, ∞
m=0(−1)m gives an inﬁnite
number of zeros if every +1 is grouped with one −1. On the other hand, the
same series can be grouped such that the ﬁrst +1 is set aside and the rest of
the terms are paired. The result would then be a +1 with an inﬁnite number
of zeros.If a series is divergent and not bounded, so that the sum is inﬁnite,
warning!
rearranging terms
is not, in general,
allowed!
then any grouping of terms gives inﬁnity.
Another operation is the rearrangement of terms of a series. This is the
commutative property of addition:
Box 9.3.5. If a series is absolutely convergent then the rearrangement
of terms does not change either the nature of convergence or the limit of
the series. A conditionally convergent series does not share this property.
2Caution is to be exercised not to move the terms around, as this will, in general, aﬀect
the sum as explained in the property of rearrangement described below.

274
Inﬁnite Series
To see the importance of absolute convergence, consider the alternating series
∞
k=1(−1)k+1/k—which converges conditionally to ln 2—and rearrange terms
as follows:
∞

k=1
(−1)k+1
k
= 1 + 1
3 + 1
5 + · · · −1
2
 
1 + 1
2 + 1
3 + · · ·
!
= 1 + 1
2 + 1
3 + 1
4 + 1
5 + · · · −1
2 −1
4 −1
6 −· · ·
−1
2
 
1 + 1
2 + 1
3 + · · ·
!
= 1 + 1
2 + 1
3 + 1
4 + 1
5 + · · · −
 
1 + 1
2 + 1
3 + · · ·
!
= 0,
where in the second line, terms with even denominators have been added
and subtracted with the positive ones interspersed among terms with odd
multiplication of
two series
denominators.
The third operation is multiplication of two series. As for rearrange-
ment,
Box 9.3.6. Multiplication is deﬁned only for absolutely convergent series:
If the two series ∞
k=1 ak and ∞
j=1 bj are absolutely convergent, then
their product (∞
k=1 ak) · (∞
j=1 bj) ≡∞
k=1
∞
j=1 akbj ≡∞
i=1 ci is also
absolutely convergent.
The last series is a rearrangement of the terms akbj into a single term ci.
This rearrangement makes it necessary for the original series to be absolutely
convergent.
9.4
Sequences and Series of Functions
The inﬁnite series of the last section are useful when we want to approximate
a number, such as e or ln 2 by a (large) sum of other (rational, decimal) num-
bers. Physics, however, deals with functions as well as numbers. It is therefore
useful to know how to approximate functions in terms of “elementary” func-
tions. In this section we shall investigate the possibility of expressing a given
function in terms of a series of functions. Since functions give numbers once
their arguments are assigned a value, many of the ideas developed in the
preceding two sections will be employed.
Suppose for each natural number n there is a function fn(x). Then, the
set {fn(x)}∞
n=1 is called a sequence of functions. Just as in the case of
sequence of
functions
sequences of numbers, we need to address the question of the convergence
of the sequence of functions. This reduces to the question of convergence of
ordinary numbers once we substitute values for x. Variation of fn(x) with x
opens up the possibility of convergence for some values of x and divergence
for others. For instance, the sequence {xn}∞
n=1 converges for −1 < x < 1 and
diverges for all other values of x.

9.4 Sequences and Series of Functions
275
More interesting than sequences of functions are series of functions:
series of functions
f1(x) + f2(x) + f3(x) + · · · =
∞

k=1
fk(x).
The nth partial sum of such a series is
Sn(x) = f1(x) + f2(x) + · · · + fn(x) =
n

j=1
fj(x).
The convergence of a series of functions ∞
k=1 fk(x) depends on x. For ex-
ample, the series may converge for x = 0.35. This means that the series of
numbers ∞
k=1 fk(0.35) converges, i.e., there exists a real number s such that
for every ϵ there exists an N with the property that | n
k=1 fk(0.35) −s| < ϵ
whenever n > N. It should be clear that an N that works for one value of
x—here 0.35—and ϵ, may not work for other values of x and ϵ. Thus, N
depends on x and ϵ, and this dependence is denoted by N(x, ϵ).
We can imagine making a table with one column consisting of the values
of x and a second column consisting of the corresponding limits of the series
of numbers whose terms are fn evaluated at the value of x. The table then
deﬁnes a real-valued function, say S(x), which is called the limit of the series
of functions, and one writes
S(x) = lim
n→∞Sn(x) ≡
∞

k=1
fk(x).
(9.12)
We have already seen examples of series of functions: the geometric series
∞
n=0 un —convergent for |u| < 1—in which the terms are functions of u
with fn(u) = un, and the Riemann zeta function (or harmonic series of degree
p)—convergent for |p| > 1—in which the terms were functions of p with
fn(p) = 1/np.
In general, the sum in Equation (9.12) may converge only for a limited
range of values of x. To ﬁnd this range, we impose the ratio test on the terms
of the series. This yields
r(x) ≡lim
k→∞




fk+1(x)
fk(x)




 < 1,
(9.13)
which is an inequality in x that can be solved to ﬁnd the values of x for which
the series converges.
Example 9.4.1. As an example of the application of Equation (9.13), let us ﬁnd
the values of x for which the series
∞

k=1
[ln(x + 1)]k
k

276
Inﬁnite Series
converges. The ratio in (9.13) is
r(x) = lim
k→∞




[ln(x + 1)]k+1/(k + 1)
[ln(x + 1)]k/k




 = lim
k→∞




[ln(x + 1)]k+1
[ln(x + 1)]k
·
k
k + 1




= |ln(x + 1)| lim
k→∞




k
k + 1




 = |ln(x + 1)| .
So, the condition for convergence is
|ln(x + 1)| < 1 ⇒−1 < ln(x + 1) < 1
or
e−1 < x + 1 < e ⇒e−1 −1 < x < e −1
and the series converges for −0.632 < x < 1.718.
Let us now check the convergence of the series for the two end points. The left
end point corresponds to ln(x+1) = −1 for which the series becomes ∞
n=1(−1)n/n
which is convergent (see Example 9.3.5). On the other hand, for the right end point,
ln(x + 1) = 1, and the series becomes ∞
n=1 1/n which is the divergent harmonic
series. Thus, the interval of convergence is −0.632 ≤x < 1.718.
■
An important notion is uniform convergence:
uniform
convergence
Box 9.4.1. If, for a given ϵ, it is possible to ﬁnd an N such that |Sn(x)−
S(x)| < ϵ whenever n > N for all values of x in some interval (a, b)—so
that N is independent of x—then the series is said to converge uniformly
on (a, b).
Clearly, for uniform convergence to have any meaning, there must exist a range
of values of x for which the series converges uniformly because a series may
converge for all values of x on the real line without converging uniformly for
any interval of the real line. A pictorial representation of uniform convergence
is shown in Figure 9.3. Basically, we say that a series is uniformly convergent
if the graphs of partial sums Sn(x), after a certain large N, all lie within a
(narrow) strip of width ϵ containing the graph of the limit function f(x).
There is a useful test for the uniform convergence which works for a large
test for uniform
convergence
number of familiar series and goes by the name of the Weierstrass M-test:
Let ∞
k=1 fk(x) be a series of functions all deﬁned in an interval3 (a, b). If
there is a convergent series of positive numbers ∞
k=1 Mk, such that |fk(x)| ≤
Mk for all x in (a, b), then ∞
k=1 fk(x) converges absolutely for each such x,
and is uniformly convergent in (a, b).
Example 9.4.2. Consider the series ∞
n=1 xn/np, which is a generalization of the
geometric series (for which p = 0). We want to see for what values of p and in what
3Instead of an interval, one may use the union of many intervals. In fact, the statement
is true even when the interval (a, b) is replaced with a general subset of the real line.

9.4 Sequences and Series of Functions
277
f(x) + ε
f(x) − ε
f(x)
Sn(x)
Sn+1(x)
Figure 9.3: Uniform convergence.
interval of x is the series convergent. One way to get the answer is to apply the
ratio test:
lim
n→∞




an+1
an




 = lim
n→∞




xn+1
(n + 1)p · np
xn




 = |x| lim
n→∞

n
n + 1
p
= |x|.
It follows that, regardless of the value of p, the series converges for |x| < 1, and
diverges for |x| > 1. For x = 1, the series becomes ∞
n=1 1/np which converges for
p > 1 and diverges for p ≤1 as pointed out in the integral test of convergence.
Finally if x = −1, the alternating series test of convergence tells us that the series
converges for all p > 0. What about the uniformity of convergence? We note that
for Mn = 1/np, and for |x| ≤1, we have




xn
np




 ≤1
np ≡Mn
and the series of Mn converges as long as p > 1.
Thus, for p > 1, the series
∞
n=1 xn/np is uniformly convergent.
■
9.4.1
Properties of Uniformly Convergent Series
The importance of uniformly convergent series lies in the nice properties such
series possess. For instance, if ui(x) is continuous in the interval a ≤x ≤b,
and if the series ∞
i=1 ui(x) is uniformly convergent in that interval, then the
function deﬁned by f(x) = ∞
i=1 ui(x) is also continuous in the interval. This
statement is equivalent to saying that for x and x0 in the interval (a, b), one
has
lim
x→x0
*
lim
n→∞Sn(x)
+
= lim
n→∞

lim
x→x0 Sn(x)

.
Accordingly, uniform convergence permits the interchange of the two limit
processes.

278
Inﬁnite Series
Another property, which is extremely useful in physical applications, is the
fact that
you can integrate
a uniformly
convergent series
term by term.
Theorem 9.4.3. If f(x) = ∞
i=1 ui(x) is uniformly convergent, and each
ui(x) is continuous for a ≤x ≤b, then the series can be integrated term by
term, i.e.,
# b
a
f(x) dx =
# b
a
 ∞

i=1
ui(x)

dx =
∞

i=1
# b
a
ui(x) dx,
i.e., integration and summation can be interchanged.
Example 9.4.4. Consider the geometric series
1
1−t = ∞
i=0 ti, which, by Example
9.4.2, converges uniformly for −1 < t < 1. Changing t to −t does not change either
the interval or the nature of convergence of the series. We thus have
1
1 + t =
∞

i=0
(−t)i =
∞

i=0
(−1)iti.
(9.14)
Because of the uniform convergence of the series, we can integrate both sides from
0 to x with −1 < x < 1 to obtain
# x
0
dt
1 + t = ln(1 + x) =
∞

i=0
(−1)i
# x
0
ti dt =
∞

i=0
(−1)i xi+1
i + 1.
With x = 1, we obtain the result alluded to in Example 9.3.5.
Note that the integral of a series may be convergent for a bigger range of values
of its argument than the original series. Here, the original series was divergent (for
t = 1) while its integral converges (for x = 1).
■
The property stated in Theorem 9.4.3 is a useful tool for the expansion
of physical quantities in terms of some more “elementary” quantities. For
example, one can expand the electric potential—usually given in terms of an
integral—as a sum of the potentials of a single charge, a dipole, a quadrupole,
etc. (see Section 10.5). In many physical situations only the ﬁrst few terms
of the series expansion will be of importance. Thus, for instance, in atomic
transitions, it is only the dipole term that participates signiﬁcantly.
One can also diﬀerentiate a uniformly convergent series. To be speciﬁc,
you can
diﬀerentiate a
uniformly
convergent series
term by term.
Theorem 9.4.5. Suppose that u′
n(x) = dun/dx is continuous for a ≤x ≤b,
that the series ∞
n=1 un(x) converges to f(x) for a ≤x ≤b, and that the
series ∞
n=1 u′
n(x) converges uniformly for a ≤x ≤b. Then
f ′(x) = d
dx
∞

n=1
un(x) =
∞

n=1
u′
n(x),
a ≤x ≤b,
i.e., one can change the order of diﬀerentiation and summation.

9.5 Problems
279
Other operations deﬁned on uniformly convergent series are addition, sub-
traction, and multiplication by a continuous function:
If ∞
i=1 ui(x) and
∞
i=1 vi(x) are uniformly convergent for a ≤x ≤b and h(x) is continuous
in the same interval, then the series
∞

i=1
[ui(x) ± vi(x)] ,
∞

i=1
h(x)ui(x),
are also uniformly convergent for a ≤x ≤b.
Historical Notes
The mathematicians of the seventeenth and eighteenth centuries used series indis-
criminately. By the beginning of the nineteenth century some absurd results from
manipulating inﬁnite series stirred up some interest in questioning the validity of
operations performed on them. Around 1810 a number of mathematicians began
the exact handling of inﬁnite series.
In his 1811 paper and his Analytical Theory of Heat, Fourier gave a satisfactory
deﬁnition of convergence, though in general he worked freely with divergent series.
His deﬁnition of convergence was essentially in terms of the sequence of partial sums.
Moreover, he recognized that the convergence of a series of functions of the variable
x may be achieved only in an interval of x values. Although Fourier stressed that
a necessary condition for convergence is that the terms of the series approach zero,
he was fooled by the series ∞
k=0(−1)k, and thought that its sum was 1
2 [substitute
t = 1 on both sides of (9.14)].
The ﬁrst important and strictly rigorous investigation of convergence was made
by Gauss in his 1812 paper Disquisitiones Generales Circa Seriem Inﬁnitam wherein
he studied the hypergeometric series (see Section 11.2.1). Though Gauss is often
mentioned as one of the ﬁrst to recognize the need for restricting the series to their
interval of convergence, he avoided any decisive position. He was so much concerned
to solve concrete problems by numerical calculations that he used a divergent ex-
pansion of the gamma function. When he did investigate the convergence of the
hypergeometric series, he remarked that he did so to please those who favored the
rigor of the ancient geometers.
Cauchy’s work on the convergence of series is the ﬁrst extensive treatment of
the subject. In his Cours d´Analyse Cauchy clearly deﬁnes the sequence of partial
sums and gives a rigorous deﬁnition of the convergence and divergence of the series
in terms of this sequence. It is also in this work that he gives what is now called
the Cauchy criterion for convergence of a sequence (see Box 9.1.1). He proves this
to be a necessary condition, but merely remarks that if the condition holds, the
convergence of the series is assured. He lacked the knowledge of the properties of
real numbers to provide a proof. Cauchy then goes on to state and prove many of
the results that we have outlined in our discussion of the tests for convergence.
9.5
Problems
9.1. Show that
(a) n
k=1 kzk−1 = n−1
k=0(k + 1)zk.
(b) x2 n
k=0 akxk = n+2
k=2 ak−2xk.

280
Inﬁnite Series
9.2. Use some small values of M and N (say M = 2, N = 3) and verify the
validity of Equation (9.6).
9.3. Use Equation (9.8) to show that
m + 1
k

=
m
k

+
 m
k −1

.
9.4. Use mathematical induction to prove the following relations:
(a)
d
dx(xn) = nxn−1.
(b) n
k=0 xk = xn+1−1
x−1 .
9.5. Use the integral test to show that the harmonic series of order p is
convergent for p > 1 and divergent for p ≤1.
9.6. Test the following series for convergence or divergence:
(a) ∞
n=1
(−1)nn
n2+1 .
(b) ∞
n=1
(−1)n sin2 nα
n+1
.
(c) ∞
n=1
ln n
np .
(d) ∞
n=1
n+1
3n2+3n.
(e) ∞
n=1
n+1
3n2+5n−10.
(f) ∞
n=2
1
n ln n.
where α is some real number. For (c), consider the three cases p > 1, p < 1,
and p = 1.
9.7. Prove convergence or divergence by the comparison test:
∞

n=1
sin n
n2 ,
∞

n=2
1
n3 −1,
∞

n=1
n + 5
n2 −3n −5,
∞

n=2
1
√n ln n.
9.8. Prove convergence or divergence by the integral test:
∞

n=1
1
n2 + 1,
∞

n=1
n
n2 + 1,
∞

n=2
1
n ln2 n,
∞

n=2
1
n ln n ln ln n.
9.9. Prove convergence or divergence by the ratio test:
∞

n=1
2n + 1
3n + n,
∞

n=1
(−1)n
n!
,
∞

n=1
5n
n! .
9.10. Use the ratio test to ﬁnd the range of values of x for which the following
series converge. Make sure to investigate the end points.
(a) ∞
n=1
(ln x)n
n+1 .
(b) ∞
n=1
4n sinn x
(n+1)5n.
(c) ∞
n=1
xn
√n.
(d) ∞
n=1
(ln x)n
n!
.
(e) ∞
n=1
xn
3nn!.
(f) ∞
n=3
n2
(x−2)n .
(g) ∞
n=1 nxn.
(h) ∞
n=1 n!xn.
(i) ∞
n=1
n3
(ln x)n .
(j) ∞
n=0
nxn
n2+1.
(k) ∞
n=1
(x2+1)n
n3
.
(l) ∞
n=1
n2
(x+1)n .
(m) ∞
n=0

x2+1
3
n
.
(n) ∞
n=1

x2
√n
n
.
(o) ∞
n=0
(x−2)n
n2+1 .
(p) ∞
n=0
 x
2
!n .
(q) ∞
n=1
 x
n
!n .
(r) ∞
n=0
xn
n2+1.[6bp]

9.5 Problems
281
9.11. Write the ﬁrst four terms of the following series:
∞

n=1
n!
2 · 4 · · · 2n,
∞

n=1
(−1)n
ln(n + 1),
∞

n=1
1
10√
n9 ,
∞

n=1
1
9√
n10 .
Test for convergence or divergence of these series.

Chapter 10
Application of Common
Series
The preceding chapter concerned itself with the formal properties of inﬁnite
sequences and series, especially the sequences and series of functions. One
of the useful properties of the inﬁnite series of functions is that they can be
approximated by ﬁnite sums. In this approximation, two important features
of the series play crucial roles: the simplicity of the functions used in the series
and the convergence of the series. This chapter deals with some of the series
of functions most commonly used in mathematical physics.
10.1
Power Series
One of the most common series of functions is the power series where the
nth term of the series is cn(x −a)n with cn a real number. To be speciﬁc, a
power series in powers of (x −a) is of the form
∞

n=0
cn(x −a)n = c0 + c1(x −a) + c2(x −a)2 + · · · .
(10.1)
An important special case is when a = 0, so that we have
∞

n=0
cnxn = c0 + c1x + c2x2 + · · · .
(10.2)
Sometimes negative powers are also included, but by power series we usually
mean Equation (10.1).
We note that Equation (10.1) converges for x = a. The question is whether
radius of
convergence of a
power series
it converges for any other values of x, and if so, what these values are. It turns
out that:

284
Application of Common Series
Theorem 10.1.1. Every power series ∞
n=0 cn(x −a)n has a radius of
convergence r∗such that the series converges absolutely and uniformly when
|x −a| < r∗and diverges for |x −a| > r∗. If r∗̸= 0 and r1 is a number such
that 0 < r1 < r∗, then the series converges absolutely and uniformly for
|x −a| ≤r1.
The number r∗can be 0 (in which case the series converges only for x = a),
a ﬁnite positive number, or ∞(in which case the series converges for all x).
The radius of convergence can be evaluated by using the ratio test. Con-
sider the ratio
r(x) = lim
n→∞




cn+1(x −a)n+1
cn(x −a)n




 = |x −a| lim
n→∞




cn+1
cn




and note that the series converges if r(x) < 1, or
|x −a| < lim
n→∞




cn
cn+1




 .
The RHS is naturally deﬁned to be the radius of convergence
r∗= lim
n→∞




cn
cn+1




if the limit exists.
(10.3)
It can be shown that the radius of convergence can also be found from the
following formula:
r∗= lim
n→∞
1
n	
|cn|
if the limit exists.
(10.4)
Example 10.1.2. Consider the exponential function ex which, as we shall see,
has a power series expansion
ex ≡
∞

n=0
cnxn =
∞

n=0
xn
n! .
By the ratio test, we have
r(x) = lim
n→∞




xn+1/(n + 1)!
xn/n!




 = lim
n→∞|x|




n!
(n + 1)!




 = |x| lim
n→∞




1
n + 1




 = 0
for all values of x. So, regardless of x, the series representation of ex converges, i.e.,
the radius of convergence is inﬁnite. We can also use Equation (10.3) to calculate
the radius of convergence
r∗= lim
n→∞




cn
cn+1




 = lim
n→∞




1/n!
1/(n + 1)!




 = lim
n→∞|n + 1| = ∞.
■
Example 10.1.3. Let us ﬁnd the interval of convergence of
∞

k=0
(−1)kxk
4k(k + 1).

10.1 Power Series
285
The ratio test gives
r(x) = lim
k→∞




fk+1(x)
fk(x)




 = lim
k→∞




(−1)k+1xk+1/[4k+1(k + 2)]
(−1)kxk/[4k(k + 1)]




= lim
k→∞




x(k + 1)
4(k + 2)




 =



x
4



 lim
k→∞




k + 1
k + 2







=1
= |x|
4 .
So, the series converges if r(x) < 1, i.e., if |x| < 4, or −4 < x < 4.
What about the end points? For x = 4, the series becomes
∞

k=0
(−1)k
k + 1
which is the alternating series and it converges. On the other hand, if x = −4, the
series becomes
∞

k=0
(−1)k(−4)k
4k(k + 1)
=
∞

k=0
(−1)k(−1)k
k + 1
=
∞

k=0
1
k + 1,
which is the divergent harmonic series. So, the interval of convergence of the series
is −4 < x ≤4, and its radius of convergence is r∗= 4.
■
Because of the uniform convergence of power series, we can perform all
the common operations used for ordinary functions on the power series. We
list all these properties in the following:
▶Continuity. A power series represents a continuous function within its
a convergent
power series
represents a
continuous
function
radius of convergence; i.e., if r∗is the radius of convergence, then the
series
f(x) =
∞

n=0
cn(x −a)n
for
a −r∗< x < a + r∗
(10.5)
is continuous.
▶Integration. The power series (10.5) can be integrated term by term
a convergent
power series can
be integrated term
by term
within its radius of convergence; i.e., for a −r∗< p < q < a + r∗,
# q
p
f(t) dt =
∞

n=0
cn
# q
p
(t −a)ndt =
∞

n=0
cn
(q −a)n+1 −(p −a)n+1
n + 1
.
(10.6)
▶Diﬀerentiation. The power series (10.5) can be diﬀerentiated term by
a convergent
power series can
be diﬀerentiated
term by term
term within its radius of convergence; that is,
f ′(x) =
∞

n=1
ncn(x −a)n−1, a −r∗< x < a + r∗.
(10.7)
▶Zero Power Series. If a power series has nonzero radius of convergence
if two power series
are equal, so are
their correspond-
ing coeﬃcients
and has a sum which is identically zero, then every coeﬃcient of the
series must be zero. This leads to the following

286
Application of Common Series
Theorem 10.1.4. If two power series ∞
n=0 cn(x −a)n and ∞
n=0 bn
(x −a)n have nonzero convergence radii and have equal sums whenever
both series converge, then the two series are identical, i.e.,
cn = bn, n = 0, 1, 2, . . ..
This property is very eﬀectively used to ﬁnd solutions of diﬀerential
equations in terms of inﬁnite power series.
10.1.1
Taylor Series
A power series whose coeﬃcients are derivatives of the function representing
the sum is called Taylor series. More precisely, let
f(x) =
∞

n=0
cn(x −a)n,
a −r∗< x < a + r∗.
(10.8)
This series is called the Taylor series of f(x) at x = a if the coeﬃcients cn are
given by the rule:
c0 = f(a),
c1 = f ′(a)
1!
,
c2 = f ′′(a)
2!
, . . . ,
ck = f (k)(a)
k!
,
so that
Taylor series
f(x) = f(a) + f ′(a)
1!
(x −a) + · · · + f (k)(a)
k!
(x −a)k + · · ·
=
∞

k=0
f (k)(a)
k!
(x −a)k
where
f (0)(a) ≡f(a), 0! ≡1.
(10.9)
From Theorem 10.1.4 and the equality of (10.8) and (10.9), we conclude that
every power series with nonzero convergence radius is the Taylor series of the
function denoting its sum, and conversely every inﬁnitely diﬀerentiable func-
tion can be represented by a Taylor series within the interval of convergence
of the series.
An alternative way of writing the Taylor series which suggests approxima-
tion is to let Δx = x −a. Then Equation (10.9) becomes
Taylor series and
approximating
functions
f(a + Δx) = f(a) + f ′(a)
1!
Δx + · · · =
∞

k=0
f (k)(a)
k!
(Δx)k.
Since a is an arbitrary real number, we can replace it with x which is more
suggestive of the generality of this formula:
f(x + Δx) = f(x) + f ′(x)
1!
Δx + · · · =
∞

k=0
f (k)(x)
k!
(Δx)k.
(10.10)
With Δx interpreted as the increment in x, Equation (10.10) states that the
function at the incremented value of x is f(x) plus a “correction” involving

10.2 Series for Some Familiar Functions
287
all powers of Δx. The smaller the increment, the smaller the number of terms
of the correction we need to keep to achieve a given accuracy.
A convenient value for a is 0, in which case the series is called Maclaurin
series:
f(x) = f(0) + f ′(0)
1!
x + · · · =
∞

k=0
f (k)(0)
k!
xk.
(10.11)
10.2
Series for Some Familiar Functions
In this subsection, we give the Maclaurin series representation of a few familiar
functions.
These representations are so useful that the reader is urged to
commit them to memory.
The Exponential Function
For ex, the derivatives of all orders are ex implying that f (n)(0) = 1 for all n.
Therefore,
Maclaurin series of
exponential
function
ex = 1 + x
1! + x2
2! + · · · =
∞

n=0
xn
n! .
(10.12)
This series converges uniformly for all x as we saw in Example 10.1.2.
The Trigonometric Functions
The sine function has the following derivatives:
Maclaurin series of
trigonometric
function
f ′(x) = cos x, f ′′(x) = −sin x, f ′′′(x) = −cos x, f (iv)(x) = sin x, . . . .
This can be summarized as
f (n)(x) =
0
(−1)n/2 sin x
if n is even,
(−1)(n−1)/2 cos x
if n is odd.
Evaluating at x = 0 for the Maclaurin series yields
f (n)(0) =
0
0
if n is even,
(−1)(n−1)/2
if n is odd,
so that
sin x = 0 + x −0 −x3
3! + 0 + x5
5! −· · · =
∞

k=0
(−1)k
x2k+1
(2k + 1)!.
(10.13)
The combination 2k+1 ensures that only odd terms are included even though
there is no restriction on the sum over k. The radius of convergence is
r∗= lim
k→∞




(−1)k/(2k + 1)!
(−1)k+1/(2k + 3)!




 = lim
k→∞
(2k + 3)!
(2k + 1)! = ∞.

288
Application of Common Series
Thus the Taylor series representation of the sine function is convergent for
all x.
The Maclaurin series representation of the cosine function can be obtained
similarly. We leave the details to the reader, and simply quote the result:
cos x =
∞

k=0
(−1)k x2k
(2k)!,
−∞< x < ∞.
(10.14)
The Binomial Function
Another useful function which is used extensively in physics is the binomial
function with arbitrary exponent, i.e., (1 + x)α with α an arbitrary real num-
ber. It is easy to ﬁnd the nth derivative of this function:
f (n)(x) = α(α −1)(α −2) · · · (α −n + 1)(1 + x)α−n,
n ≥1.
Evaluating this at x = 0 gives
cn = f (n)(0)
n!
= α(α −1) · · · (α −n + 1)
n!
,
n ≥1.
From this, we can immediately ﬁnd the radius of convergence:
r∗= lim
n→∞




cn
cn+1




 = lim
n→∞




α(α −1) · · · (α −n + 1)
n!
·
(n + 1)!
α(α −1) · · · (α −n)




= lim
n→∞




n + 1
α −n




 = 1.
Thus, the series is convergent for −1 < x < 1, and we can write
Maclaurin series of
binomial function
(1 + x)α = 1 +
∞

n=1
α(α −1) · · · (α −n + 1)
n!
xn,
−1 < x < 1.
(10.15)
Example 10.2.1. Because of the frequent occurrence of the square root, we work
through the calculation of (10.15) for α = ± 1
2. For α = + 1
2, we have
√
1 + x = (1 + x)1/2 = 1 +
∞

n=1
1
2( 1
2 −1) · · · ( 1
2 −n + 1)
n!
xn
= 1 + 1
2x +
∞

n=2
(−1)n−1 1 · 3 · 5 · · · (2n −3)
2nn!
xn.
Now let n = m + 1 and rewrite the sum as
√
1 + x = 1 + 1
2x +
∞

m=1
(−1)m 1 · 3 · 5 · · · (2m −1)
2m+1(m + 1)!
xm+1
= 1 + 1
2x −1
8x2 +
3
48x3 −· · · .
(10.16)

10.2 Series for Some Familiar Functions
289
The case of α = −1
2 can be handled in exactly the same way. We simply quote
the result
1
√1 + x = 1 +
∞

m=1
(−1)m 1 · 3 · 5 · · · (2m −1)
2m m!
xm
= 1 −1
2x + 3
8x2 −15
48x3 · · · ,
(10.17)
and urge the reader to ﬁll in the details.
■
It is important to note the limitations of the power series representation
of a function: Although (1 + x)α is deﬁned for all positive1 values of x, the
power series representation of it is good only for a limited region of the real
line.
In many applications, the binomial function appears in the form (u + v)α
where |v| < |u| and one is interested in the power series expansion in v/u.
This is easily done:
(u + v)α =
3
u

1 + v
u
4α
= uα 
1 + v
u
α
= uα + uα
∞

n=1
α(α −1) · · · (α −n + 1)
n!
v
u
n
(10.18)
= uα +
∞

n=1
α(α −1) · · · (α −n + 1)
n!
uα−nvn,
−|u| < v < |u|.
In practice, v is usually much smaller than u, and the requirement of conver-
gence is overwhelmingly met.
The Hyperbolic Functions
The exponential function and the trigonometric functions have very similar
power series: Except for (the crucial) coeﬃcient (−1)k, sin x appears to be
the odd part of the expansion of ex and cos x its even part. The (−1)k factor
makes the trigonometric functions periodic. What if we take this factor away,
and simply collect the even powers of ex together and do the same to the
odd powers? The resulting series will of course be (absolutely and uniformly)
convergent because the exponential is so. So, let us introduce the following
functions:
Maclaurin series of
hyperbolic
functions
sinh x ≡
∞

k=0
x2k+1
(2k + 1)! = x + x3
3! + x5
5! + · · · ,
cosh x ≡
∞

k=0
x2k
(2k)! = 1 + x2
2! + x4
4! + · · · ,
(10.19)
1It is really deﬁned for more than just positive values. For instance, if α is an integer,
the function is deﬁned for all values of x. For fractional powers such as α = 1/2, 1 + x
cannot be negative, so that we must restrict the values of x to x > −1.

290
Application of Common Series
sinh x (pronounced “sinch”) is called the hyperbolic sine function. Similarly,
cosh x (pronounced “kahsh”) is called the hyperbolic cosine function. By
their very deﬁnition, we have
ex = cosh x + sinh x.
If we change x to −x, and note that sinh x is odd and cosh x is even, we can
also write
e−x = cosh(−x) + sinh(−x) = cosh x −sinh x.
Adding and subtracting the last two equations yields
cosh x = ex + e−x
2
,
sinh x = ex −e−x
2
.
(10.20)
This is how the hyperbolic functions are usually deﬁned. From these deﬁni-
tions, one can obtain a host of relations for the sinh and cosh that look similar
to the relations satisﬁed by sine and cosine. For example, it is easy to show
that
cosh2 x −sinh2 x = 1,
d
dx cosh x = sinh x,
d
dx sinh x = cosh x,
cosh(x ± y) = cosh x cosh y ± sinh x sinh y,
(10.21)
sinh(x ± y) = sinh x cosh y ± cosh x sinh y,
cosh(2x) = cosh2 x + sinh2 x,
sinh(2x) = 2 sinh x cosh x.
We give the derivation for the hyperbolic cosine of the sum, leaving the rest
of them as problems for the reader. We start with the RHS:
cosh x cosh y + sinh x sinh y
=
ex + e−x
2
 ey + e−y
2

+
ex −e−x
2
 ey −e−y
2

= (ex + e−x)(ey + e−y) + (ex −e−x)(ey −e−y)
4
= ex+y + ex−y + e−x+y + e−x−y + ex+y −ex−y −e−x+y + e−x−y
4
= 2ex+y + 2e−x−y
4
= ex+y + e−x−y
2
= cosh(x + y).
We can also deﬁne the analogs of other trigonometric functions:
tanh x ≡sinh x
cosh x = ex −e−x
ex + e−x ,
coth x ≡cosh x
sinh x = ex + e−x
ex −e−x ,
(10.22)
sech x ≡
1
cosh x =
2
ex + e−x ,
cosech x ≡
1
sinh x =
2
ex −e−x .
These functions have such properties as
sech2 x = 1 −tanh2 x,
cosech2 x = coth2 x −1,
and
d
dx tanh x = sech2 x,
d
dx coth x = −cosech2 x.

10.3 Helmholtz Coil
291
The Logarithmic Function
Finally, we state the Maclaurin series for ln(1 + x), which occurs frequently
in physics, and which the reader can verify:
Maclaurin series of
logarithmic
function
ln(1 + x) =
∞

n=1
(−1)n+1 xn
n ,
−1 < x < 1.
(10.23)
10.3
Helmholtz Coil
Power series are very useful tools for approximating functions, and the closer
one gets to the point of expansion, the better the approximation. The essence
of this approximation is replacing the inﬁnite series with a ﬁnite sum, i.e.,
approximating the function with a polynomial.
In general, to get a very good approximation, one has to retain very large
powers of the power series. So, the approximating polynomial will have to
be of a high degree. However, suppose that a function f(x) has the following
expansion
f(x) = c0 + c1(x −a) + · · · + cm(x −a)m + cm(x −a)m+k + · · · ,
where k is a fairly large number. Then the polynomial
p(x) = c0 + c1(x −a) + · · · + cm(x −a)m
approximates the function very accurately because, as long as we are “close”
enough to the point of expansion a, the next term in the series will not aﬀect
the polynomial much. In particular, if the series looks like
f(x) = c0 + ck(x −a)k + · · · ,
(10.24)
then the constant “polynomial” c0 is an extremely good approximation to the
function for values of x close to a.
The argument above can be used to design devices to produce physical
quantities that are constant for a fairly large values of the variable on which the
outcome of the device depends. A case in point is the Helmholtz coil, which
is used frequently in laboratory situations in which homogeneous magnetic
ﬁelds are desirable.
Figure 10.1 shows two loops of current-carrying wires of radii a and b
separated by a distance L.
We are interested in the z-component of the
magnetic ﬁeld midway between the two loops, which, to simplify expressions,
we have chosen to be the origin. Example 4.1.4 gives the expression for the
magnetic ﬁeld of a loop at a point on its axis at a distance z from its center.

292
Application of Common Series
x
y
z
I1
I2
a
b
L
Figure 10.1: Two circular loops with diﬀerent radii producing a magnetic ﬁeld.
Let us denote the magnetic ﬁeld of the loop of radius a by B1 and that of the
loop of radius b by B2. Then Example 4.1.4 gives
B(z) ≡B1(z) + B2(z) =
2πkmI1a2
[a2 + (z + L/2)2]3/2 +
2πkmI2b2
[b2 + (z −L/2)2]3/2
=
16πkmI1a2
[4a2 + (2z + L)2]3/2 +
16πkmI2b2
[4b2 + (2z −L)2]3/2 .
(10.25)
We want to adjust the parameters of the two loops in such a way that the
magnetic ﬁeld at the origin is maximally homogeneous. This can be accom-
plished by setting as many derivatives of B(z) equal to zero at the origin as
possible, so that the Maclaurin expansion of B(z) will have a maximum num-
ber of consecutive terms equal to zero, i.e., we will have an expression of the
form (10.24).
The ﬁrst derivative of B(z) is
dB
dz = −96πkmI1a2(2z + L)
[4a2 + (2z + L)2]5/2 −96πkmI2b2(2z −L)
[4b2 + (2z −L)2]5/2 .
Setting this equal to zero at z = 0 gives
I1a2
(4a2 + L2)5/2 =
I2b2
(4b2 + L2)5/2 .
(10.26)
The second derivative of B(z) is
d2B
dz2 = −768πkmI1a2[a2 −(2z + L)2]
[4a2 + (2z + L)2]7/2
−768πkmI2b2[b2 −(2z −L)2]
[4b2 + (2z −L)2]7/2
.
Setting this equal to zero at z = 0 gives
I1a2(a2 −L2)
(4a2 + L2)7/2 + I2b2(b2 −L2)
(4b2 + L2)7/2 = 0.
(10.27)
Since both terms are positive, the only way that we can get zero in (10.27)
is if each term on the LHS vanishes. It follows that a = L = b. Substituting

10.3 Helmholtz Coil
293
this in Equation (10.26) gives I1 = I2 which we denote by I. Therefore, we
can now write the magnetic ﬁeld as
B(z) = 16πkmIa2
(
1
[4a2 + (2z + a)2]3/2 +
1
[4a2 + (2z −a)2]3/2
)
.
(10.28)
The reader may verify that not only are the ﬁrst and the second derivatives
of B(z) of Equation (10.28) zero, but also its third derivative. In fact, we have
B(z) = 32πkmI
5
√
5a
−4608πkmI
625
√
5a5 z4 + · · · .
(10.29)
That only even powers appear in the expansion (10.29) could have been antic-
ipated, because (10.28) is even in z as the reader may easily verify. It follows
from Equation (10.29) that B(z) should be fairly insensitive to the variation
of z at points close to the origin. Physically, this means that the magnetic
ﬁeld is fairly homogeneous at the midpoint between the two loops as long as
the loops are equal and separated by a distance equal to their common radius,
and as long as they carry the same current. Figure 10.2 shows the plot of the
magnetic ﬁeld as a function of z. Note how ﬂat the function is for even fairly
large values of z.
–1
–0.5
0.5
1
Figure 10.2: Magnetic ﬁeld of a Helmholtz coil as a function of z. The horizontal axis
is z in units of a.
Historical Notes
One of the problems faced by mathematicians of the late seventeenth and early eigh-
teenth centuries was interpolation (the word was coined by Wallis) of tables of values.
Greater accuracy of the interpolated values of the trigonometric, logarithmic, and
nautical tables was necessary to keep pace with progress in navigation, astronomy,
and geography. The common method of interpolation, whereby one takes the aver-
age of the two consecutive entries of a table, is called linear interpolation because
it gives the exact result for a linear function. This gives a crude approximation for

294
Application of Common Series
functions that are not linear, and mathematicians realized that a better method of
interpolation was needed.
The general method which can give interpolations that are more and more accu-
rate was given by Gregory and independently by Newton. Suppose f(x) is a function
whose values are given at a, a + h, a + 2h, . . . , and we are interested in the value
of the function at an x that lies between two table entries. The Gregory–Newton
formula states that
f(a + r) = f(a) + r
hΔf(a) +
r
h
 r
h −1
!
2!
Δ2f(a) +
r
h
 r
h −1
!  r
h −2
!
3!
Δ3f(a) + · · · ,
where
Δf(a) = f(a + h) −f(a),
Δ2f(a) = Δf(a + h) −Δf(a),
Δ3f(a) = Δ2f(a + h) −Δ2f(a),
Δ4f(a) = Δ3f(a + h) −Δ3f(a), . . .
To calculate f at any value y between the known values, one simply substitutes y−a
for r.
Brook Taylor’s Methodus incrementorum directa et inversa, published in 1715,
added to mathematics a new branch now called the calculus of ﬁnite diﬀerences, and
he invented integration by parts. It also contained the celebrated formula known
as Taylor’s expansion, the importance of which remained unrecognized until 1772
when Lagrange proclaimed it the basic principle of the diﬀerential calculus.
Brook Taylor
1685–1731
To arrive at the series that bears his name, Taylor let h in the Gregory–Newton
formula be Δx and took the limit of smaller and smaller Δx. Thus, the third term,
for example, gave
r(r −Δx)
2!
Δ2f(a)
Δx2
→r2
2! f ′′(a)
which is the familiar third term in the Taylor series.
In 1708 Taylor produced a solution to the problem of the center of oscillation
which, since it went unpublished until 1714, resulted in a priority dispute with
Johann Bernoulli.
Taylor also devised the basic principles of perspective in Linear Perspective
(1715). Together with New Principles of Linear Perspective the ﬁrst general treat-
ment of the vanishing points are given.
Taylor gives an account of an experiment to discover the law of magnetic attrac-
tion (1715) and an improved method for approximating the roots of an equation by
giving a new method for computing logarithms (1717).
Taylor was elected a Fellow of the Royal Society in 1712 and was appointed in
that year to the committee for adjudicating the claims of Newton and of Leibniz to
have invented the calculus.
10.4
Indeterminate Forms and L’Hˆopital’s Rule
It is good practice to approximate functions with their power series repre-
sentations, keeping as many terms as is necessary for a given accuracy. This
practice is especially useful when encountering indeterminate expressions of
the form 0
0. Although L’Hˆopital’s rule (discussed below) can be used to ﬁnd
the ratio, on many occasions the substitution of the series leads directly to
the answer, saving us the labor of multiple diﬀerentiation.

10.4 Indeterminate Forms and L’Hˆopital’s Rule
295
Example 10.4.1. Let us look at some examples of the ratios mentioned above.
In all cases treated in this example, the substitution x = 0 gives 0
0, which is inde-
terminate. Using the Maclaurin series (10.12) and (10.13), we get
lim
x→0
2ex −2 −2x −x2
sin x −x
= lim
x→0
2(1 + x + x2/2 + x3/6 + x4/24 + · · · ) −2 −2x −x2
x −x3/6 + x5/120 + · · · −x
= lim
x→0
x3/3 + x4/12 + · · ·
−x3/6 + x5/120 −· · · = lim
x→0
1/3 + x/12 + · · ·
−1/6 + x2/120 −· · · = −2.
The series (10.14) and (10.23) can be used to evaluate the following limit:
lim
x→0
ln(1 + x) −x
cos x −1
= lim
x→0
x −x2/2 + x3/3 −· · · −x
1 −x2/2 + x4/24 −· · · −1
= lim
x→0
−x2/2 + x3/3 −· · ·
−x2/2 + x4/24 −· · · = lim
x→0
−1/2 + x/3 −· · ·
−1/2 + x2/24 −· · · = 1.
With (10.12) and (10.15), we have
lim
x→0
√1 + 2x −x −1
ex2 −1
= lim
x→0
1 + 1
2(2x) +
1
2 ( 1
2 −1)
2!
(2x)2 +
1
2 ( 1
2 −1)( 1
2 −2)
3!
(2x)3 + · · · −x −1
1 + x2 + (x2)2/2! + · · · −1
= lim
x→0
−x2/2 + x3/2 + · · ·
x2 + x4/2 + · · ·
= lim
x→0
−1/2 + x/2 + · · ·
1 + x2/2 + · · ·
= −1
2.
■
The method of expanding the numerator and denominator of a ratio as
a Taylor series is extremely useful in applications in which mere substitution
results in the indeterminate expression2 of the form 0
0. However, there are
many other indeterminate forms that occur in applications. For example, a
mere substitution of x = 0 in (1+x)1/x yields 1∞which is also indeterminate.
Other examples of indeterminate expressions are 0×∞, ∞
∞, 00, and ∞0. Most
of these expressions can be reduced to indeterminate ratios for which one can
use l’Hˆopital’s rule:
l’Hˆopital’s rule
Box 10.4.1. (L’Hˆopital’s Rule). If f(a)/g(a) is indeterminate, then
lim
x→a
f(x)
g(x) = lim
x→a
f ′(x)
g′(x) ,
(10.30)
where f ′ and g′ are derivatives of f and g, respectively.
2An expression is indeterminate if it involves two parts each of which gives a result that
is contradictory to the other. Thus the numerator of the ratio 0
0 says that the ratio should
be zero, while the denominator says that the ratio should be inﬁnite.

296
Application of Common Series
In practice, one converts the indeterminate form into a ratio and diﬀeren-
tiates the numerator and denominator as many times as necessary until one
obtains a deﬁnite result or inﬁnity. The following general rules can be of help:
• If f(a) = 0 and g(a) = ∞, then to ﬁnd limx→a f(x)g(x), rewrite the
limit as
lim
x→a f(x)g(x) = lim
x→a
f(x)
 1
g(x)

or
lim
x→a f(x)g(x) = lim
x→a
g(x)

1
f(x)
,
the ﬁrst of which gives 0
0 and the second ∞
∞. In either case, one can
apply L’Hˆopital’s rule.
• If f(a) = 1 and g(a) = ∞, ﬁrst deﬁne h(x) ≡[f(x)]g(x). Then to ﬁnd
lim
x→a h(x) = lim
x→a[f(x)]g(x),
take the natural logarithm of h(x) and convert the result into the ratio
lim
x→a ln[h(x)] = lim
x→a g(x) ln[f(x)] = lim
x→a
ln[f(x)]
 1
g(x)
 .
Then use Equation (10.30).
• If f(a) = ∞(or f(a) = 0) and g(a) = 0, then to ﬁnd
lim
x→a h(x) ≡lim
x→a[f(x)]g(x),
take the natural logarithm of h(x) and convert the result into the ratio
lim
x→a ln[h(x)] = lim
x→a g(x) ln[f(x)] = lim
x→a
ln[f(x)]
 1
g(x)
 .
Then use Equation (10.30).
Example 10.4.2. To ﬁnd the limx→0(1+2x)1/x, we write h(x) ≡(1+2x)1/x and
note that
lim
x→0 ln[h(x)] = lim
x→0(1/x) ln(1 + 2x) = lim
x→0
ln(1 + 2x)
x
is indeterminate. Using Equation (10.30) yields
lim
x→0 ln[h(x)] = lim
x→0
ln(1 + 2x)
x
= lim
x→0
2
1 + 2x
1
= lim
x→0
2
1 + 2x = 2.
Therefore, limx→0 h(x) = e2.
To ﬁnd the limx→0 xx, we write h(x) ≡xx and note that
lim
x→0 ln[h(x)] = lim
x→0 x ln x = lim
x→0
ln x
1/x

10.5 Multipole Expansion
297
is indeterminate. Using Equation (10.30) yields
lim
x→0 ln[h(x)] = lim
x→0
1/x
−1/x2 = lim
x→0(−x) = 0.
Therefore, limx→0 h(x) = e0 = 1. So, we have the interesting result limx→0 xx = 1.
The limit of x2/(1 −cos x) as x goes to zero is obtained as follows:
lim
x→0
x2
1 −cos x = lim
x→0
2x
sin x = lim
x→0
2
cos x = 2.
Here we had to diﬀerentiate twice because the ratio of the ﬁrst derivatives was also
indeterminate.
■
It is instructive for the reader to verify all limits in Example 10.4.1 using
L’Hˆopital’s rule to appreciate the ease of the Taylor expansion method.
10.5
Multipole Expansion
One extremely useful application of the power series representation of func-
tions is in potential theory. The electrostatic or gravitational potential can
be written as
Φ(r) = K
##
Ω
dQ(r′)
|r −r′|,
(10.31)
where K is ke for electrostatics and −G for gravity. Similarly, Q represents
either electric charge or mass. In some applications, especially for electrostatic
potential, the distance of the ﬁeld point P from the origin is much larger than
the distance of the source point P ′ from the origin. This means that r >> r′
and we can expand in the powers of the ratio r′/r which we denote by ϵ. The
key to this expansion is a power series expansion of 1/|r −r′|. First write
1
|r −r′| =
1
√
r2 + r′2 −2r · r′ =
1
r√1 + ϵ2 −2ϵˆer · ˆer′
= 1
r
 
1 + ϵ2 −2ϵˆer · ˆer′!−1/2 .
Next use the binomial expansion (10.15) with x = ϵ2 −2ϵˆer · ˆer′ and α = −1
2.
Up to second order in ϵ, this yields
1
|r −r′| = 1
r
3
1 −1
2
 
ϵ2 −2ϵˆer · ˆer′!
+ 3
8
 
ϵ2 −2ϵˆer · ˆer′!2 + · · ·
4
= 1
r
,
1 + ϵˆer · ˆer′ + ϵ2 .
−1
2 + 3
2(ˆer · ˆer′)2/
+ · · ·
-
= 1
r + ˆer · r′
r2
+ r′2
r3
.
−1
2 + 3
2(ˆer · ˆer′)2/
+ · · · .
(10.32)

298
Application of Common Series
Substituting this in Equation (10.31), we obtain
Φ(r) = K
r
##
Ω
dQ(r′) + K
r2 ˆer ·
##
Ω
r′ dQ(r′)
+ K
r3
##
Ω
r′2 .
−1
2 + 3
2(ˆer · ˆer′)2/
dQ(r′) + · · ·
(10.33)
= KQ
r
+ K
r2 ˆer · pQ + K
r3
##
Ω
r′2 .
−1
2 + 3
2(ˆer · ˆer′)2/
dQ(r′) + · · · ,
where
Q ≡
##
Ω
dQ(r′)
is the total Q (charge, or mass)—also called the zeroth Q moment—and
electric dipole
moment deﬁned
pQ ≡
##
Ω
r′ dQ(r′)
(10.34)
is the ﬁrst Q moment, which in the case of charge is also called the electric
dipole moment. One can also deﬁne higher moments.
If the source of the potential is discrete, the integral in Equation (10.31)
becomes a sum. The steps leading to (10.33) will not change except for switch-
ing all the integrals to summations. In particular, the dipole moment of N
point sources {Qk}N
k=1, located at {rk}N
k=1, turns out to be
pQ =
N

k=1
Qkrk.
(10.35)
For the special case of two electric charges q1 = +q and q2 = −q, we obtain3
p = qr1 −qr2 = q(r1 −r2).
(10.36)
Thus, the dipole moment of a pair of equal charges of opposite sign is the
magnitude of the charge times the displacement vector from the negative to
the positive charge.
Example 10.5.1. Electric dipoles are fairly abundant in Nature. For example,
an antenna is approximated as a dipole at distances far away from it; and in atomic
transitions one uses the so-called dipole approximation to calculate the rate of
dipole
approximation
transition and the lifetime of a state.
Let us write the explicit form of the potential of a dipole, i.e., the second term on
the RHS of Equation (10.33). In Cartesian coordinates, in which the dipole moment
is in the z-direction (so that p = pˆez), the potential can be written as
3It is customary to denote the electric dipole moment by p with no subscript.

10.6 Fourier Series
299
Φdip(x, y, z) = ke
r2 ˆer · p = ke
r3 r · p =
kepz
(x2 + y2 + z2)3/2 .
More important is the expression for potential in spherical coordinates:
electric potential
of a dipole
Φdip(r, θ, ϕ) = kep
r2
cos θ
  
ˆer · ˆez = kep
r2 cos θ.
(10.37)
The azimuthal symmetry (independence of ϕ) comes about because we chose p to
lie along the z-axis.
■
10.6
Fourier Series
Power series are special cases of the series of functions in which the nth func-
tion is (x −a)n—or simply xn—multiplied by a constant. These functions,
simple and powerful as they are, cannot be used in all physical applica-
tions. More general functions are needed for many problems in theoretical
physics.
The most widely used series of functions in applications are Fourier series
in which the functions are sines and cosines. These are especially suitable for
periodic functions which repeat themselves with a certain period. Suppose
periodic functions
that a function f(x) is deﬁned in the interval (a, b). Can we write it as a
series in sines and cosines, as we did in terms of orthogonal polynomials [see
Theorem 7.5.2]? Let L = b −a denote the length of the interval, and consider
the functions
sin 2nπx
L
, cos 2nπx
L
.
Let us try the series expansion
Fourier series
expansion
f(x) = a0 +
∞

n=1

an cos 2nπx
L
+ bn sin 2nπx
L

,
(10.38)
where we have separated the n = 0 term. Now the sine and cosine terms have
the following easily obtainable useful properties:
# b
a
sin 2nπx
L
dx =
# b
a
cos 2nπx
L
dx =
# b
a
sin 2nπx
L
cos 2mπx
L
dx = 0,
# b
a
sin 2nπx
L
sin 2mπx
L
dx =
0
0
if
m ̸= n,
L/2
if
m = n ̸= 0,
(10.39)
# b
a
cos 2nπx
L
cos 2mπx
L
dx =
0
0
if
m ̸= n,
L/2
if
m = n ̸= 0.

300
Application of Common Series
These properties suggest a way of determining the coeﬃcients of the series
expansion of
periodic functions
in terms of Fourier
series
for a given function as in the case of orthogonal polynomials. If we integrate
both sides of Equation (10.38) from a to b, we get4
# b
a
f(x) dx = a0
# b
a
dx +
# b
a
∞

n=1

an cos 2nπx
L
+ bn sin 2nπx
L

dx
= (b −a)a0 +
∞

n=1
an
# b
a
cos 2nπx
L
dx



=0
+
∞

n=1
bn
# b
a
sin 2nπx
L
dx



=0
or
 b
a f(x) dx = a0L. This yields
a0 = 1
L
# b
a
f(x) dx.
(10.40)
Multiplying Equation (10.38) by cos(2mπx/L) and integrating both sides from
a to b, we obtain
# b
a
f(x) cos 2mπx
L
dx
= a0
# b
a
cos 2mπx
L
dx +
# b
a
∞

n=1

an cos 2nπx
L
+ bn sin 2nπx
L

cos 2mπx
L
dx
= 0 +
∞

n=1
an
# b
a
cos 2nπx
L
cos 2mπx
L
dx +
∞

n=1
bn
# b
a
sin 2nπx
L
cos 2mπx
L
dx
= amL/2,
where we used Equation (10.39). This yields
an = 2
L
# b
a
f(x) cos 2nπx
L
dx.
(10.41)
Similarly, multiplying both sides of Equation (10.38) by sin(2mπx/L) and
integrating from a to b, yields
bn = 2
L
# b
a
f(x) sin 2nπx
L
dx.
(10.42)
Equations (10.38), (10.40), (10.41), and (10.42) provide a procedure for
representing a function f(x) as a Fourier series. However, the RHS of Equation
(10.38) is periodic. This means that for values of x outside the interval (a, b),
f(x) is also periodic. In fact, from Equation (10.38), we have
Fourier series
always represents
a periodic
function.
4Here we are assuming that the series converges uniformly so that we can switch the
order of integration and summation. This assumption turns out to be correct, but we shall
forego its (diﬃcult) proof.

10.6 Fourier Series
301
f(x + L) = a0 +
∞

n=1
(
an cos 2nπ(x + L)
L
+ bn sin 2nπ(x + L)
L
)
= a0 +
∞

n=1
(
an cos
2nπx
L
+ 2nπ

+ bn sin
2nπx
L
+ 2nπ
)
= a0 +
∞

n=1

an cos 2nπx
L
+ bn sin 2nπx
L

= f(x).
Thus, f(x) repeats itself at the end of each interval of length L, i.e., it is pe-
riodic with period L. Fourier series is especially suited for representing such
functions. In fact, any periodic function has a Fourier series expansion, and
the simplicity of sine and cosine functions makes this expansion particularly
useful in applications such as electrical engineering and acoustics where peri-
odic functions in the form of waves and voltages are daily occurrences. Let
us look at some examples.5
Example 10.6.1. In the study of electrical circuitry, periodic voltage signals of
diﬀerent shapes are encountered. An example is the so-called square wave of height
V0, and duration and “rest duration” T [see Figure 10.3(top)]. The potential as a
function of time, V (t), can be expanded as a Fourier series. The interval is (0, 2T),
square wave
potential
because that is one whole cycle of potential variation. We thus write
V (t) = a0 +
∞

n=1

an cos 2nπt
2T
+ bn sin 2nπt
2T

(10.43)
1
2
3
4
5
6
–0.2
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
–0.2
0.2
0.4
0.6
0.8
1
Figure 10.3:
Top: The periodic square-wave potential with V0 = 1 and T = 2.
Bottom: Various approximations to the Fourier series of the square-wave potential. The
dashed plot is that of the ﬁrst term of the series, the thick gray plot keeps 3 terms, and
the solid plot 15 terms.
5While Taylor series expansion demands that the function be (inﬁnitely) diﬀerentiable,
the orthogonal polynomial and Fourier series expansion require only piecewise continuity.
This means that the function can have any (ﬁnite) number of discontinuities in the interval
(a, b).
Thus, the expanded function can not only be nondiﬀerentiable, it can even be
discontinuous.

302
Application of Common Series
with
a0 = 1
2T
# 2T
0
V (t) dt,
an = 2
2T
# 2T
0
V (t) cos 2nπt
2T
dt = 1
T
# 2T
0
V (t) cos nπt
T
dt,
(10.44)
bn = 1
T
# 2T
0
V (t) sin nπt
T
dt.
Substituting
V (t) =
0
V0
if
0 ≤t ≤T,
0
if
T < t ≤2T,
in Equation (10.44), we obtain
a0 = 1
2T
# T
0
V0 dt = 1
2V0,
an = 1
T
# T
0
V0 cos nπt
T
dt = 0,
and
bn = 1
T
# T
0
V0 sin nπt
T
dt = −V0
T
T
nπ cos nπt
T




T
0
= V0
nπ (1 −cos nπ) = V0
nπ [1 −(−1)n] .
Thus, there is no contribution from the cosine sum, and in the sine sum only the
odd terms contribute (bn = 0 if n is even). Therefore, let n = 2k + 1, where k now
takes all values even and odd, and substitute all the above information in Equation
(10.43), to obtain
V (t) = 1
2V0 +
∞

k=0
V0
(2k + 1)π
*
1 −(−1)2k+1+
sin (2k + 1)πt
T
= V0
2
0
1 + 4
π
∞

k=0
sin[(2k + 1)πt/T]
2k + 1
1
.
The plots of the sum truncated at the ﬁrst, third, and ﬁfteenth terms are shown
in Figure 10.3(bottom). Note how the Fourier approximation overshoots the value
of the function at discontinuities.
This is a general feature of all discontinuous
functions and is called the Gibb’s phenomenon.6
Gibb’s
phenomenon
■
Example 10.6.2. Another frequently used potential is the sawtooth potential.
sawtooth potential
The interval is (0, T ) and the equation for the potential is
V (t) = V0 t
T
for
0 ≤t < T.
6A discussion of Gibb’s phenomenon can be found in Hassani, S. Mathematical Physics:
A Modern Introduction to Its Foundations, Springer-Verlag, 1999, Chapter 8.

10.6 Fourier Series
303
The coeﬃcients of expansion can be obtained as usual:
a0 = 1
T
# T
0
V0 t
T dt = 1
2V0,
an = 2
T
# T
0
V0 t
T cos 2nπt
T
dt = 2V0
T 2
# T
0
t cos 2nπt
T
dt
= 2V0
T 2
0
T
2nπ t sin 2nπt
T




T
0
−
T
2nπ
# T
0
sin 2nπt
T
dt
1
= 0,
and
bn = 2
T
# T
0
V0 t
T sin 2nπt
T
dt = 2V0
T 2
# T
0
t sin 2nπt
T
dt
= 2V0
T 2
0
−T
2nπ t cos 2nπt
T




T
0
+
T
2nπ
# T
0
cos 2nπt
T
dt
1
= −V0
nπ .
Substituting the coeﬃcients in the sum, we get
V (t) = 1
2V0 −
∞

n=1
V0
nπ sin 2nπt
T
= V0
2
0
1 −2
π
∞

n=1
sin(2nπt/T )
n
1
.
The plot of the sawtooth wave as well as those of the sum truncated at the ﬁrst,
third, and ﬁfteenth term are shown in Figure 10.4.
■
1
2
3
4
5
6
0.2
0.4
0.6
0.8
1
1
2
3
4
5
6
0.2
0.4
0.6
0.8
1
Figure 10.4: Top: The periodic sawtooth potential with V0 = 1 and T = 2. Bottom:
Various approximations to the Fourier series of the sawtooth potential. The dashed plot
is that of the ﬁrst term of the series, the thick gray plot keeps 3 terms, and the solid
plot 15 terms.
Historical Notes
Although Euler made use of the trigonometric series as early as 1729, and d’Alembert
considered the problem of the expansion of the reciprocal of the distance between

304
Application of Common Series
two planets in a series of cosines of the multiples of the angle between the rays from
the origin to the two planets, it was Fourier who gave a systematic account of the
trigonometric series.
Joseph Fourier did very well as a young student of mathematics but had set
his heart on becoming an army oﬃcer. Denied a commission because he was the son
“The profound
study of nature is
the most fruitful
source of
mathematical
discoveries.”
Joseph Fourier
of a tailor, he went to a Benedictine school with the hope that he could continue
studying mathematics at its seminary in Paris.
The French Revolution changed
those plans and set the stage for many of the personal circumstances of Fourier’s
later years, due in part to his courageous defense of some of its victims, an action
that led to his arrest in 1794.
He was released later that year, and he enrolled
as a student in the Ecole Normale, which opened and closed within a year. His
performance there, however, was enough to earn him a position as assistant lec-
turer (under Lagrange and Monge) in the Ecole Polytechnique. He was an excellent
mathematical physicist, was a friend of Napoleon, and accompanied him in 1798 to
Egypt, where Fourier held various diplomatic and administrative posts while also
conducting research. Napoleon took note of his accomplishments and, on Fourier’s
return to France in 1801, appointed him prefect of the district of Is`ere, in south-
eastern France, and in this capacity built the ﬁrst real road from Grenoble to Turin.
He also befriended the boy Champollion, who later deciphered the Rosetta stone
as the ﬁrst long step toward understanding the hieroglyphic writing of the ancient
Egyptians.
Joseph Fourier
1768–1830
Like other scientists of his time, Fourier took up the ﬂow of heat. The ﬂow was
of interest as a practical problem in the handling of metals in industry and as a
scientiﬁc problem in attempts to determine the temperature at the interior of the
Earth, the variation of that temperature with time, and other such questions. He
submitted a basic paper on heat conduction to the Academy of Sciences of Paris
in 1807. The paper was judged by Lagrange, Laplace, and Legendre, and was not
published, mainly due to the objections of Lagrange, who had earlier rejected the
use of trigonometric series.
But the Academy did wish to encourage Fourier to
develop his ideas, and so made the problem of the propagation of heat the subject
of a grand prize to be awarded in 1812. Fourier submitted a revised paper in 1811,
which was judged by the men already mentioned, and others. It won the prize but
was criticized for its lack of rigor and so was not published at that time in the
M´emoires of the Academy.
He developed a mastery of clear notation, some of which is still in use today. (The
placement of the limits of integration near the top and bottom of the integral sign was
introduced by Fourier.) It was also his habit to maintain close association between
mathematical relations and physically measurable quantities, especially in limiting
or asymptotic cases, even performing some of the experiments himself.
He was
one of the ﬁrst to begin full incorporation of physical constants into his equations,
and made considerable strides toward the modern ideas of units and dimensional
analysis.
Fourier continued to work on the subject of heat and, in 1822, published one of
the classics of mathematics, Th´eorie Analytique de la Chaleur, in which he made
extensive use of the series that now bears his name and incorporated the ﬁrst part
of his 1811 paper practically without change. Two years later he became secretary
of the Academy and was able to have his 1811 paper published in its original form
in the M´emoires.

10.7 Multivariable Taylor Series
305
10.7
Multivariable Taylor Series
The approximation to which we alluded at the beginning of this chapter is
just as important when we are dealing with functions depending on several
variables as those depending on a single variable. After all, most functions
encountered in physics depend on space coordinates and time. We begin with
two variables because the generalization to several variables will be trivial
once we understand the two-variable case.
A direct—and obvious—generalization of the power series to the case of a
function f(u, v) of two variables about the point (u0, v0) gives
f(u, v) = a00 + a10(u −u0) + a01(v −v0) + a20(u −u0)2
+ a02(v −v0)2 + a11(u −u0)(v −v0) + a30(u −u0)3
+ a21(u −u0)2(v −v0) + a12(u −u0)(v −v0)2
+ a03(v −v0)3 + · · · .
(10.45)
The notation used above needs some explanation. All the a’s are constants
with two indices such that the ﬁrst index indicates the power of (u −u0) and
the second that of (v −v0). To obtain a Taylor series, we need to relate a’s
to derivatives of f. This is straightforward: To ﬁnd akj, diﬀerentiate both
sides of Equation (10.45) k times with respect to u and j times with respect
to v and evaluate the result at (u0, v0). Thus, to evaluate a00, we diﬀerentiate
zero times with respect to u and zero times with respect to v and substitute
u0 for u and v0 for v on both sides. We then obtain
f(u0, v0) = a00 + 0 + 0 + · · · + 0 + · · · = a00.
By diﬀerentiating with respect to u and evaluating both sides at (u0, v0), we
obtain
∂1f(u0, v0) = 0 + a10 + 0 + · · · + 0 + · · · = a10.
Similarly,
∂2f(u0, v0) = 0 + 0 + a01 + 0 + · · · + 0 + · · · = a01,
∂1∂1f(u0, v0) = ∂2
1f(u0, v0) = 2a20,
∂2∂2f(u0, v0) = ∂2
2f(u0, v0) = 2a02,
∂2∂1f(u0, v0) = a11.
We want to write Equation (10.45) in a succinct form to be able to extract
a general formula for the coeﬃcients. An inspection of that equation suggests
that
f(u, v) =
∞

j=0
∞

k=0
ajk(u −u0)j(v −v0)k.

306
Application of Common Series
It is more useful to collect terms of equal total power together. Thus, writing
m = k + j, and noting that j cannot be larger than m, we rewrite the above
equation as
f(u, v) =
∞

m=0
m

j=0
aj,m−j(u −u0)j(v −v0)m−j.
Let us introduce the notation ∂k,n−k for k diﬀerentiations with respect to the
ﬁrst variable, and n −k diﬀerentiations with respect to the second:7
∂k,n−kf ≡
∂nf
∂uk∂vn−k
and apply it to both sides of the sum above. Evaluating the result at (u0, v0),
we obtain
∂k,n−kf(u0, v0) =
∞

m=0
m

j=0
aj,m−j∂k,n−k
,
(u −u0)j(v −v0)m−j- 


(u0,v0) .
If j < k or m −j < n −k then the corresponding terms diﬀerentiate to zero.
On the other hand, if j > k or m −j > n −k then some powers of u −u0 or
v −v0 will survive and evaluation at (u0, v0) will also give zero. So, the only
term in the sum that survives the diﬀerentiation is the term with j = k and
m −j = n −k which gives k!(n −k)!. We thus have
Taylor series of a
function of two
variables
∂k,n−kf(u0, v0) = k!(n −k)!ak,n−k ⇒ak,n−k = ∂k,n−kf(u0, v0)
k!(n −k)!
,
and the Taylor series can ﬁnally be written as
f(u, v) =
∞

n=0
n

k=0
∂k,n−kf(u0, v0)
k!(n −k)!
(u −u0)k(v −v0)n−k.
(10.46)
Sometimes this is written in terms of increments to suggest approximation as
in the single-variable case:
f(u + Δu, v + Δv) =
∞

n=0
n

k=0
∂k,n−kf(u, v)
k!(n −k)!
(Δu)k(Δv)n−k,
(10.47)
where we used (u, v) instead of (u0, v0). Once again, the ﬁrst term in the
expansion is f(u, v) and the rest is a correction.
The three-dimensional formula should now be easy to construct. We write
Taylor series of a
function of three
variables
this as8
f(u, v, w) =
∞

n=0

i+j+k=n
∂n
ijkf(u0, v0, w0)
i!j!k!
(u−u0)i(v−v0)j(w−w0)k. (10.48)
7This notation is not universal.
Sometimes ∂n
kj is used with the understanding that
k + j = n.
8The symbol ∂n
ijk represents the nth derivative with i diﬀerentiations with respect to the
ﬁrst variable, j diﬀerentiations with respect to the second variable, and k diﬀerentiations
with respect to the third variable, such that i + j + k = n.

10.8 Application to Diﬀerential Equations
307
For a given value of n, suggested by the outer sum, the inner sum describes a
procedure whereby all terms whose i, j, and k indices add up to n are grouped
together. As a comparison, we also write Equation (10.46) in this notation:
f(u, v) =
∞

n=0

j+k=n
∂n
jkf(u0, v0)
j!k!
(u −u0)j(v −v0)k.
(10.49)
The three-dimensional Taylor series in terms of increments becomes
f(u + Δu, v + Δv, w + Δw)
=
∞

n=0

i+j+k=n
∂n
ijkf(u, v, w)
i!j!k!
(Δu)i(Δv)j(Δw)k,
(10.50)
where again (u0, v0, w0) has been replaced by (u, v, w).
Example 10.7.1. As an example we expand ex sin y about the origin.9 Using the
notation in Equation (10.49), the coeﬃcients, within a factor of j!k!, can be written
as
∂n
jk (ex sin y)



(0,0) =
∂n
∂xj∂yk (ex sin y)



(0,0)= ∂j
∂xj (ex)



x=0



=1
∂k
∂yk (sin y)



y=0
= ∂k
∂yk (sin y)



y=0 .
The ﬁrst few terms of the Taylor expansion of this function can now be written
down:
ex sin y = y + xy + x2y
2
−y3
6 −xy3
6
+ x3y
6
+ · · · .
One could also obtain this result by multiplying the Taylor expansions of ex and
sin y term by term.
■
10.8
Application to Diﬀerential Equations
One of the most powerful methods of solving an ordinary diﬀerential equation
(ODE) is the power series method, and we shall use this method to solve some
of the most recurring diﬀerential equations of mathematical physics in Chap-
ters 25 through 27. Power series are uniformly and absolutely convergent, and
can be diﬀerentiated term by term. This makes them a good candidate for
representing the (unknown) solutions of diﬀerential equations. The relation
among the derivatives, expressed in a diﬀerential equation, becomes a relation
among coeﬃcients of the power series, the so-called recursion relation, which
is enough to determine all the relevant coeﬃcients of the series, leaving only
those coeﬃcients which require initial conditions for their determination. The
best way to understand the method is to look at an example.
9The use of x and y in place of u and v should not cause any confusion.

308
Application of Common Series
Example 10.8.1. The diﬀerential equation
dx
dt = bx
can be assumed to have a power series solution of the form
x(t) =
∞

n=0
cntn.
This power series will be uniformly and absolutely convergent for some interval
on the real line, and as such, can be diﬀerentiated. Diﬀerentiating the foregoing
equation and substituting the result in the diﬀerential equation, we get
∞

n=1
ncntn−1 = b
∞

n=0
cntn.
The essential property of power series is the equality of the corresponding coeﬃcients
when two such series are equal (see Theorem 10.1.4). Before using this property in
the above equation, however, we need to reexpress the LHS so that the power of t
is the same on both sides. We thus change the dummy index from n to m = n −1,
so that all n’s are replaced by m + 1. We then get
LHS =
∞

m+1=1
(m + 1)cm+1tm =
∞

m=0
(m + 1)cm+1tm.
Since we are free to use any dummy index we please, let us change m to n so that
we can compare the two sides of the equation. This gives
∞

n=0
(n + 1)cn+1tn =
∞

n=0
bcntn ⇒(n + 1)cn+1 = bcn.
(10.51)
We can immediately test for the convergence of the series using the ratio test:
lim
n→∞




cn+1tn+1
cntn




 = lim
n→∞




tbcn/(n + 1)
cn




 = lim
n→∞




bt
n + 1




 = 0
for all b and t. Thus, regardless of the value of b and t, the series converges.
We have established the convergence of the series representation of the solution
of our diﬀerential equation. We now have to ﬁnd the coeﬃcients. This is done by
rewriting Equation (10.51) as
cn+1 =
b
n + 1cn
(10.52)
which is called the recursion relation of the series. By iterating this relation we
recursion relation
can obtain all the coeﬃcients in terms of the ﬁrst one as follows:
cn+1 =
b
n + 1cn =
b
n + 1
 b
ncn−1

=
b2
(n + 1)ncn−1
=
b2
(n + 1)n

b
n −1cn−2

=
b3
(n + 1)n(n −1)cn−2.

10.8 Application to Diﬀerential Equations
309
Since we are interested in ﬁnding cn, we can rewrite this equation as
cn =
b3
n(n −1)(n −2)cn−3,
where we have lowered all n’s on both sides by one unit. This relation can easily be
generalized to an arbitrary positive integer j:
cn =
bj
n(n −1) · · · (n −j + 1)cn−j.
In particular, if we set j = n, we obtain
cn =
bn
n(n −1) · · · 2 · 1c0 = bn
n! c0
(10.53)
which upon substitution in the original series, yields
x(t) =
∞

n=0
c0 bn
n! tn = c0
∞

n=0
(bt)n
n!
= c0ebt,
where we have used Equation (10.12). The unknown c0 is determined by the value
of x(t) at a given t, usually t = 0.
■
There are of course much easier ways of solving the simple diﬀerential
equation above, and the method used may appear to “kill a ﬂy with a sledge-
hammer.” Nevertheless, it illustrates the almost mechanical way of obtaining
the solution without resorting to any “tricks” used so often in arriving at the
closed-form solutions of diﬀerential equations.
Example 10.8.2. Let us look at another familiar example. The motion of a mass
m driven by a spring with spring constant k is governed by the diﬀerential equation
md2x
dt2 = −kx ⇒d2x
dt2 + k
mx = 0.
Once again we assume a solution of the form
x(t) =
∞

n=0
antn = a0 + a1t + a2t2 + · · · + antn + · · ·
and diﬀerentiate it twice to get
dx
dt =
∞

n=1
nantn−1 = a1 + 2a2t + · · · + nantn−1 + · · · ,
d2x
dt2 =
∞

n=2
n(n −1)antn−2 = 2a2 + 3 · 2a3t + · · · + n(n −1)antn−2 + · · · .
Substitute j = n −2 to bring the power of t into a form that can be compared with
the RHS. This amounts to substituting j + 2 for all n’s:
d2x
dt2 =
∞

j=0
(j + 2)(j + 1)aj+2tj =
∞

n=0
(n + 2)(n + 1)an+2tn.

310
Application of Common Series
In the last step we simply changed the dummy index. Substituting this and the
series for x(t) in the diﬀerential equation, we obtain
∞

n=0
(n + 2)(n + 1)an+2tn + k
m
∞

n=0
antn = 0
which gives the recursion relation
(n + 2)(n + 1)an+2 + k
man = 0 ⇒an+2 = −
k/m
(n + 2)(n + 1)an.
(10.54)
Application of the ratio test [as given by Equation (9.10) with j = 2] immediately
yields that the series is convergent for all values of k/m and all values of t. If we
lower the value of n by two units on both sides, we get
an = −
k/m
n(n −1)an−2 = −
k/m
n(n −1)
(
−
k/m
(n −2)(n −3)an−4
)
=
(−k/m)2
n(n −1)(n −2)(n −3)an−4
=
(−k/m)2
n(n −1)(n −2)(n −3)
(
−
k/m
(n −4)(n −5)an−6
)
=
(−k/m)3
n(n −1)(n −2)(n −3)(n −4)(n −5)an−6
...
=
(−k/m)i
n(n −1) · · · (n −2i + 1)an−2i,
where i is some positive integer. Because of the form of this equation, we should
consider two cases: For even n, we let i = n/2 or n = 2i to obtain
a2i =
(−k/m)i
2i(2i −1) · · · 2 · 1a0 = (−k/m)i
(2i)!
a0
and for odd n we let i = (n −1)/2 or n = 2i + 1 to get
a2i+1 =
(−k/m)i
(2i + 1)2i · · · 2 · 1a1 = (−k/m)i
(2i + 1)! a1.
Thus all even coeﬃcients are given in terms of a0, and all odd ones in terms of a1.
Absolute convergence of the series now allows us to rearrange terms and separate
even and odd terms to write
x(t) =
∞

n=even
antn +
∞

n=odd
antn =
∞

j=0
a2jt2j +
∞

j=0
a2j+1t2j+1
=
∞

j=0
(−k/m)j
(2j)!
a0t2j +
∞

j=0
(−k/m)j
(2j + 1)! a1t2j+1
= a0
∞

j=0
(−1)j
(2j)!
	
k/m t
2j
+
a1
	
k/m
∞

j=0
(−1)j
(2j + 1)!
	
k/m t
2j+1
= A cos
	
k/m t

+ B sin
	
k/m t

,

10.9 Problems
311
where A = a0 and B = a1/
	
k/m are arbitrary constants to be determined by the
initial conditions of the problem.
The Maclaurin series for sine and cosine used
above are given in Equations (10.13) and (10.14).
■
The examples above, although illustrating the utility of the power series
method of solving diﬀerential equations, should not give the impression that
one needs no other methods. The closed-form solutions are sometimes essen-
tial for interpreting the physical properties of the system under consideration.
For example, if the mass of the preceding example is in a ﬂuid, so that a
damping force retards the motion, the closed-form solution will turn out to
be
x(t) = Ae−γt cos(ωt + α),
ω ≡

k
m,
where γ is the damping factor and α is an arbitrary phase. Deciphering this
damping factor
closed form from its power series expansion, obtained by solving the diﬀer-
ential equation by the series method, is next to impossible. The closed-form
solution shows clearly, for instance, how the amplitude of the oscillation de-
creases with time, an information that may not be evident from the series
solution of the problem. Nevertheless, on many occasions, a closed-form so-
lution may not be available, in which case the power series solution will be
the only alternative. In fact, many of the functions of mathematical physics
were invented in the last century as the power series solutions of diﬀerential
equations.
10.9
Problems
10.1. Write the ﬁrst ﬁve terms of the expansion of the binomial function
(10.15) for (a) α = 3
2, (b) α = 1
3, and (c) α = 3
4.
10.2. Find the rational number of which each of the following decimal num-
bers is a representation:
(a) 0.5555 . . ..
(b) 0.676767 . . ..
(c) 0.123123 . . ..
(d) 1.1111 . . ..
(e) 2.727272 . . ..
(f) 1.108108 . . ..
10.3. Find the interval of convergence of the Maclaurin series for each of the
familiar functions discussed in Section 10.2.
10.4. Using the series representation of the familiar functions evaluate the
following series:
(a) ∞
k=1
(−1)kx2k+1
2k
.
(b) ∞
k=0
x2k+1
(2k)! .
(c) ∞
k=0
xk+1
(k+1)!.
(d) ∞
n=1
(−1)n−1x3n−2
n3n
.
(e) ∞
n=0
(−1)n+2x3n+1
33n+1(2n)! .
(f) ∞
m=0
xm+1
(2m+1)!.
10.5. Derive Equation (10.17).

312
Application of Common Series
10.6. Use the Maclaurin series to ﬁnd the limits of the following ratios as
x →0:
2
√
1 −x2 + x2 −2
2 cos x −2 + x2
,
sin x −ln(1 + x)
ex −x −cos x .
10.7. (a) Use the Maclaurin series expansion up to x3 to ﬁnd the following
limit:
lim
x→0
2
3√1 −6x −2 cosx + 4 sin x + 7x2
ln(1 −x) + ex −1
.
(b) Use the Maclaurin series expansion up to x4 to ﬁnd the following limit:
lim
x→0
ex −ln(1 + x2) −cos x + sin x −2x
2
√
4 + x2 + cos x −5
.
10.8. In the special theory of relativity the energy E of a particle of mass m
and speed v is given by
E =
mc2
	
1 −(v/c)2 ,
where c is the speed of light. Show that for ordinary speeds (v << c), one
obtains the classical expression for the kinetic energy, deﬁned to be E minus
the rest energy.
10.9. The gravitational potential energy for a particle of mass m at a distance
r from the center of a planet of radius R and mass M is given by
Φ(r) = −GMm
r
+ C,
r > R.
(a) Find C so that the potential at the surface of the planet is zero.
(b) Show that at a height h << R above the surface of the planet, the potential
energy can be written as mgh. Find g in terms of M and R and calculate
the numerical value of g for the Earth, the Moon, and Jupiter. Look up the
data you need in a table usually found in introductory physics or astronomy
books.
10.10. Prove the hyperbolic identities of Equation (10.21).
10.11. Show that
sech2 x = 1 −tanh2 x,
cosech2 x = coth2 x −1,
and
d
dx tanh x = sech2 x,
d
dx coth x = −cosech2 x.
10.12. Derive Equation (10.23).

10.9 Problems
313
10.13. Use L’Hˆopital’s rule to obtain the following limits:
(a) limx→0
(2+x) ln(1−x)
(1−ex) cos x .
(b) limx→∞x ln

x+1
x−1

.
(c) limx→a
3√x−3√a
x−a
.
(d) limx→0
xex
1−ex .
(e) limx→1
2 π(tan x)cos x.
(f) limx→0(ln x) tan x.
10.14. Use L’Hˆopital’s rule to obtain the limits of Example 10.4.1.
10.15. Show that the following sequences converge and ﬁnd their limits:
ln n
np , n2
2n , n ln

1 + 1
n

, P(n) e−n,
where p is a positive number and P(n) is a polynomial in n.
10.16. The Yukawa potential of a charge distribution is given by
Φ(r) =
##
Ω
kee−κ|r−r′| dq(r′)
|r −r′|
,
where κ is a constant. By expanding |r−r′| up to the ﬁrst order in r′/r, show
that
Φ(r) ≈keQe−κr
r
+ ke(κr + 1)e−κr
r2
ˆer · p,
where p is the dipole moment of the charge distribution.
10.17. A conic surface has an opening angle of 2α and a lateral length a as
shown in Figure 10.5. It carries a uniform charge density σ.
(a) Show that the electrostatic potential Φ at a distance r from the vertex on
the axis of the cone is
Φ(r) = 2πkeσ sin α
	
r2 + a2 −2ar cos α −r

+ (2πkeσ sin α cos α)r ln





a −r cos α +
√
r2 + a2 −2ar cos α
r −r cos α





 .
α
a
Figure 10.5: The cone of Problem 10.17.

314
Application of Common Series
(b) Now suppose that r ≫a, expand the square roots and the log up to the
second power of the ratio a/r, and show that
	
r2 + a2 −2ar cos α ≈r −a cosα + a2
2r sin2 α
and
ln



a −r cos α +
	
r2 + a2 −2ar cos α



 ≈ln |r −r cos α| + a
r + a2
2r2 (1 + cos α).
(c) Put (a) and (b) together to show that the potential can be approximated
by
Φ(r) ≈πkeσa2 sin α
r
.
Write this expression in terms of the total charge in the cone. Do you get
what you expect?
10.18. Recall from your introductory physics courses that the electric ﬁeld at
a distance ρ from a long uniformly charged rod has only a radial component
which is given by E = λ/2πϵ0ρ, where λ is the linear charge density. Show
this result by setting a = −L/2 (why?) and taking the limit of inﬁnite L in
Equation (4.13).
10.19. After calculating the potentials of Problems 4.11 and 4.12 for ﬁnite
L, ﬁnd their limits when L →∞.
10.20. The potential of a certain charge distribution with total charge Q is
given by
Φ = ke
a0
#
[ln |r −r′| −ln b] dq(r′),
where ke, a0, and b are constants.
(a) Show that for r′ ≪r, one can use the approximation
ln |r −r′| ≈ln r −r′
r ˆer · ˆer′.
(b) Use (a) to show that the multipole expansion of Φ only up to the dipole
moment is
Φ ≈keQ
a0
ln r
b −ke
a0
p · r
r2 .
10.21. Find the dipole moment of a uniformly charged sphere about its center.
10.22. A voltage is given by the graph shown in Figure 10.6.
(a) Write the function V (t) describing the voltage for 0 ≤t ≤2T .
(b) If this voltage repeats itself periodically, ﬁnd the Fourier series expansion
of V (t).

10.9 Problems
315
T
2T
V0
t
V(t)
Figure 10.6: The voltage of Problem 10.22.
10.23. A periodic voltage with period 2T is given by
V (t) =
0
V0 cos(πt/T )
if
−T/2 ≤t ≤T/2,
0
if
T/2 ≤|t| ≤T.
(a) Sketch this function for the interval −3T ≤t ≤3T .
(b) Find a0 and a1, the ﬁrst two cosine coeﬃcients of the Fourier series ex-
pansion of V (t).
(c) Find an and all bn, the sine coeﬃcients.
(d) Write down the Fourier series of V (t). Evaluate both sides at t = 0 to
show that
π
2 = 1 −2
∞

n=1
(−1)n
4n2 −1.
This is one of the many series representations of π.
10.24. An electric voltage V (t) is given by
V (t) = V0 sin
 πt
2T

,
0 ≤t ≤T
and repeats itself with period T .
(a) Sketch V (t) for values of t from t = 0 to t = 3T .
(b) Find the Fourier series expansion of V (t).
10.25. A periodic voltage is given by the formula
V (t) =
0
V0 sin(πt/2T )
if
0 ≤t ≤T,
0
if
T ≤t ≤2T.
(a) Sketch the voltage for the interval (−4T, 4T ).
(b) Find the Fourier series representation of this voltage.
10.26. A periodic voltage with period 4T is given by
V (t) =
⎧
⎨
⎩
V0

1 −t2
T 2

if
−T ≤t ≤T
0
if
T ≤|t| ≤2T.

316
Application of Common Series
(a) Sketch this function for the interval −6T ≤t ≤6T .
(b) Find a0, an, and bn, the coeﬃcients of the Fourier series expansion of
V (t).
(c) Write down the Fourier series of V (t).
(d) Evaluate both sides at t = T . Do you obtain an identity? If not, what
sort of relationship is obtained if we demand the equality of both sides?
10.27. Write out Equation (10.50) up to the second power in the Δ’s.
10.28. Find the Taylor series expansion of ex ln(1 + y) about (0, 0).
10.29. (a) Find the multivariable Taylor series expansion of exy about (0, 0).
(b) Now let z = xy, expand the function ez, and substitute xy for z in the
expansion. Show that the results of (a) and (b) agree.
10.30. Determine all the solutions of the diﬀerential equation
dx
dt + 2tx = 0
using inﬁnite power series. From the power series solution guess the closed-
form solution. Now suppose that x(0) = 1. What is the speciﬁc solution with
this property?
10.31. Consider the diﬀerential equation
dx
dt + 3t2x = 0.
(a) Use a solution of the form ∞
n=0 antn and ﬁnd a1 and a2.
(b) Find a recursion relation relating coeﬃcients.
(c) From the recursion relation determine the radius of convergence of the
inﬁnite series.
(d) Find all coeﬃcients in terms of only one.
(e) Guess the closed-form solution from the series. Now suppose that x(0) = 2.
What is the speciﬁc solution with this property? What is the numerical value
of x(−2)?

Chapter 11
Integrals and Series as
Functions
The notion of a function as a mathematical entity has a long history as rich as
the history of mathematics itself. With the invention of the coordinate plane in
the seventeenth century, functions started to acquire graphical representations
which, in turn, facilitated the connection between algebra and geometry. It
was really calculus that triggered an explosion in function theory, and indeed,
in all mathematics. With calculus came not only the concept of diﬀerentiation
and integration, but also—in the hands of Newton and his contemporaries,
as they were studying no smaller an object than the universe itself—that
of diﬀerential equations.
All these concepts, in particular integration and
diﬀerential equation, had a dramatic inﬂuence on the notion of functions. The
aim of this chapter is to give the reader a ﬂavor of the variety of functions
made possible by integration and diﬀerential equations.1
11.1
Integrals as Functions
Integrals are one of the most convenient media in which new functions can be
deﬁned. As we saw in Chapter 3, if the integrand or the limits of integration
include parameters, those parameters can be treated as variables and the
integral itself as a function of those parameters. In this section, we list some
of the most important functions that are normally deﬁned in terms of integrals.
1We shall not solve any diﬀerential equations in this chapter, but simply quote solutions
to some of them in the form of power series. We shall come back to diﬀerential equations
later in the book.

318
Integrals and Series as Functions
11.1.1
Gamma Function
Consider the integral
Γ(x) ≡
# ∞
0
tx−1e−t dt,
(11.1)
where x is a real number.2 Integrate Equation (11.1) by parts with u = tx−1
equation (11.1)
deﬁnes the
gamma function
evaluated at x.
and dv = e−t dt to obtain
Γ(x) =
≡uv



tx−1[−e−t]



∞
0



=0
+ (x −1)
# ∞
0
≡−vdu



tx−2e−t dt



=Γ(x−1)
or
Γ(x) = (x −1)Γ(x −1).
(11.2)
In particular, if x is a positive integer n, then repeated use of Equation (11.2)
gives
Γ(n) = (n −1)Γ(n −1) = (n −1)(n −2)Γ(n −2)
= (n −1)(n −2) · · · 1 · Γ(1) = (n −1)!,
where we used the fact that Γ(1) = 1 as the reader may easily verify using
Equation (11.1). This equation is written as
for integers, the
gamma function
becomes a
factorial.
Γ(n + 1) = n!
for positive integer n.
(11.3)
Let us rewrite (11.2) as Γ(x −1) = Γ(x)/(x −1). Then,
lim
x→1 Γ(x −1) = lim
x→1
Γ(x)
x −1 →∞
because Γ(1) = 1. Thus, Γ(0) = ∞. Similarly,
lim
x→0 Γ(x −1) = lim
x→0
Γ(x)
x −1 →Γ(0)
−1 →∞,
i.e., Γ(−1) = ∞. It is clear that Γ(n) = ∞for any negative integer n or zero.
It turns out that these are the only points at which Γ(x) is not deﬁned.
Deﬁnition 11.1.1. The function deﬁned by Equation (11.1) is called the
gamma function, which, because it satisﬁes Equation (11.3), is the gener-
alization of the factorials to noninteger values. We sometimes write
Γ(x + 1) = x!
for any real x
(11.4)
and call Γ the factorial function. The gamma function is deﬁned for all
values of its argument except zero and negative integers, for which the gamma
function becomes inﬁnite.
2The most complete analytic discussion of Γ(z) allows z to be complex and uses the
full machinery of complex calculus. Here, we shall avoid such completeness and refer the
reader to Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, where a full discussion of Γ(z) can be found in Section 11.4.

11.1 Integrals as Functions
319
It follows from Equation (11.2) that by repeatedly subtracting 1 from the
the values of Γ(x)
for 0 < x ≤1
determine Γ(x)
for all x.
argument of the gamma function, we can reduce the evaluation of Γ(x) to the
case where x lies between 0 and 1. Such an evaluation can be done numerically
and the results tabulated.
Example 11.1.1. In this example, we evaluate Γ( 1
2). Equation (11.1) gives
Γ( 1
2) =
# ∞
0
t−1/2e−t dt.
Change the variable of integration to u =
√
t with du = (1/2
√
t) dt. Then
Γ( 1
2) = 2
# ∞
0
e−u2 du = 2
 1
2
√π
!
= √π,
where we used the result of Example 3.3.1.
With Γ( 1
2) at our disposal, we can evaluate the gamma function at any half-
integer value by the remarks above. For example,
Γ( 7
2) = 5
2Γ( 5
2) = ( 5
2)( 3
2)Γ( 3
2) = ( 5
2)( 3
2)( 1
2)Γ( 1
2) = 15√π
8
.
Similarly, with Γ( 1
2) = −1
2Γ(−1
2), we obtain
Γ(−1
2) = −2Γ( 1
2) = −2√π.
■
It is instructive to generalize the result of the example above and ﬁnd a
general formula for the gamma function of any half-integer. Such a formula
is related to the notion of the double factorial:
double factorial
Deﬁnition 11.1.2. The double factorial (2n)!! [or (2n −1)!!] is deﬁned as
the product of all even (or odd) integers up to 2n (or 2n −1).
Problem 11.1 gives the detail of the derivation of the following formulas:
(2n)!! = 2nn! = 2nΓ(n + 1),
(2n −1)!! = Γ(n + 1
2)2nπ−1/2.
(11.5)
An extremely useful approximation to the gamma function is the so-called
Stirling approximation which is valid for large arguments of the gamma
Stirling
approximation
function and which we present without derivation3
x! ≡Γ(x + 1) ≈
√
2πe−xxx+1/2.
(11.6)
The Stirling formula works best when x is large. However, even for x = 10,
it gives
√
2πe−101010.5 = 3598696, which is surprisingly close to the exact
value of 10! = 3628800. For x = 20, the Stirling formula yields 2.42 × 1018
to three signiﬁcant ﬁgures as opposed to the calculator result, which to the
same number of signiﬁcant ﬁgures is 2.43 × 1018. For larger and larger values
of x, the two results get closer and closer.
3For a derivation, see Hassani, S. Mathematical Physics: A Modern Introduction to Its
Foundations, Springer-Verlag, 1999, Chapter 11.

320
Integrals and Series as Functions
11.1.2
The Beta Function
A function that sometimes shows up in applications is the beta function.
Consider
Γ(x)Γ(y) =
# ∞
0
tx−1e−t dt
# ∞
0
sy−1e−s ds =
# ∞
0
# ∞
0
tx−1sy−1e−(t+s) dt ds.
Introduce the new variable u = t+s and use it to rewrite the s integral. Since
the lower limits of both s and t are 0, the lower limit of the u integral will
also be 0. Similarly, the upper limit of u will be inﬁnity. However, since s
and t are positive and their sum is u, the upper limit of t cannot exceed u.
Therefore,
Γ(x)Γ(y) =
# ∞
0
du
# u
0
dt tx−1(u −t)y−1e−u.
Now introduce another variable w by t = uw. Since in the t integration, u is
held constant, we have dt = u dw, and the limits of integration for w are 0
and 1. This will allow us to write
Γ(x)Γ(y) =
# ∞
0
du e−uux+y−1



≡Γ(x+y)
# 1
0
dw wx−1(1 −w)y−1.
The last integral deﬁnes the beta function. So,
beta function
deﬁned
B(x, y) ≡Γ(x)Γ(y)
Γ(x + y) =
# 1
0
dt tx−1(1 −t)y−1,
(11.7)
where we changed the (dummy) variable of integration from w to t.
We can ﬁnd another representation of the beta function by substituting
t = sin2 θ. Then
dt = 2 sin θ cos θ,
1 −t = 1 −sin2 θ = cos2 θ,
and the limits of integration become 0 and π/2. So,
B(x, y) = 2
# π/2
0
(sin θ)2x−1(cos θ)2y−1dθ.
(11.8)
Historical Notes
Integration and diﬀerentiation and the whole machinery of calculus opened up en-
tirely new ways of deﬁning functions. Of these, one of the most important is the
gamma function, which arose from work on two problems, interpolation theory and
antidiﬀerentiation. The problem of interpolation had been considered by James Stir-
ling (1692–1770), Daniel Bernoulli (1700–1782), and Christian Goldbach. It was posed
to Euler and he announced his solution in a letter of October 13, 1729, to Goldbach.
A second letter, of January 8, 1730, brought in the integration problem.

11.1 Integrals as Functions
321
The interpolation problem had to do with giving meaning to n! for nonintegral
values of n, and the integration problem was the evaluation of an integral already
considered by Wallis, namely
# 1
0
tx(1 −t)y dt.
Euler showed that this integral led to our integral (11.1).
Leonhard Euler was Switzerland’s foremost scientist and one of the three
greatest mathematicians of modern times (Gauss and Riemann being the other two).
He was perhaps the most proliﬁc author of all time in any ﬁeld. From 1727 to 1783
his writings poured out in a seemingly endless ﬂood, constantly adding knowledge
to every known branch of pure and applied mathematics, and also to many that
were not known until he created them.
He averaged about 800 printed pages a
year throughout his long life, and yet he almost always had something worthwhile
to say. The publication of his complete works was started in 1911, and the end
is not in sight. This edition was planned to include 887 titles in 72 volumes, but
since that time extensive new deposits of previously unknown manuscripts have been
unearthed, and it is now estimated that more than 100 large volumes will be required
for completion of the project. Euler evidently wrote mathematics with the ease and
ﬂuency of a skilled speaker discoursing on subjects with which he is intimately
familiar. His writings are models of relaxed clarity. He never condensed, and he
reveled in the rich abundance of his ideas and the vast scope of his interests. The
French physicist Arago, in speaking of Euler’s incomparable mathematical facility,
remarked that “He calculated without apparent eﬀort, as men breathe, or as eagles
sustain themselves in the wind.” He suﬀered total blindness during the last 17 years
of his life, but with the aid of his powerful memory and fertile imagination, and
with assistants to write his books and scientiﬁc papers from dictation, he actually
increased his already prodigious output of work.
Leonhard Euler
1707–1783
Euler was a native of Basel and a student of Johann Bernoulli at the University,
but he soon outstripped his teacher.
He was also a man of broad culture, well
versed in the classical languages and literatures (he knew the Aeneid by heart),
many modern languages, physiology, medicine, botany, geography, and the entire
body of physical science as it was known in his time. His personal life was as placid
and uneventful as is possible for a man with 13 children.
Though he was not himself a teacher, Euler has had a deeper inﬂuence on the
teaching of mathematics than any other person. This came about chieﬂy through
his three great treatises: Introductio in Analysin Inﬁnitorum (1748); Institutiones
Calculi Diﬀerentialis (1755); and lnstitutiones Calculi Integralis (1768–1794). There
is considerable truth in the old saying that all elementary and advanced calculus
textbooks since 1748 are essentially copies of Euler or copies of copies of Euler.
These works summed up and codiﬁed the discoveries of his predecessors, and are
full of Euler’s own ideas. He extended and perfected plane and solid analytic geom-
etry, introduced the analytic approach to trigonometry, and was responsible for the
modern treatment of the functions ln x and ex. He created a consistent theory of
logarithms of negative and imaginary numbers, and discovered that ln x has an inﬁ-
nite number of values. It was through his work that the symbols e, π, and i = √−1
became common currency for all mathematicians, and it was he who linked them
together in the astonishing relation eiπ = −1. Among his other contributions to
standard mathematical notation were sin x, cos x, the use of f(x) for an unspeciﬁed
function, and the use of  for summation.

322
Integrals and Series as Functions
His work in all departments of analysis strongly inﬂuenced the further develop-
ment of this subject through the next two centuries. He contributed many important
ideas to diﬀerential equations, including substantial parts of the theory of second-
order linear equations and the method of solution by power series. He gave the ﬁrst
systematic discussion of the calculus of variations, which he founded on his basic
diﬀerential equation for a minimizing curve. He discovered the integral deﬁning the
gamma function and developed many of its applications and special properties. He
also worked with Fourier series, encountered the Bessel functions in his study of the
vibrations of a stretched circular membrane, and applied Laplace transforms to solve
diﬀerential equations—all before Fourier, Bessel, and Laplace were born.
E. T. Bell, the well-known historian of mathematics, observed that “One of the
most remarkable features of Euler’s universal genius was its equal strength in both
of the main currents of mathematics, the continuous and the discrete.” In the realm
of the discrete, he was one of the originators of number theory and made many far-
reaching contributions to this subject throughout his life. In addition, the origins
of topology—one of the dominant forces in modern mathematics—lie in his solution
of the K¨onigsberg bridge problem and his formula V −E + F = 2 connecting the
numbers of vertices, edges, and faces of a simple polyhedron.
The distinction between pure and applied mathematics did not exist in Euler’s
day, and for him the entire physical universe was a convenient object whose diverse
phenomena oﬀered scope for his methods of analysis. The foundations of classical
mechanics had been laid down by Newton, but Euler was the principal architect. In
his treatise of 1736 he was the ﬁrst to explicitly introduce the concept of a mass-
point, or particle, and he was also the ﬁrst to study the acceleration of a particle
moving along any curve and to use the notion of a vector in connection with velocity
and acceleration. His continued successes in mathematical physics were so numerous,
and his inﬂuence was so pervasive, that most of his discoveries are not credited to
him at all and are taken for granted in the physics community as part of the natural
order of things. However, we do have Euler’s angles for the rotation of a rigid body,
and the all-important Euler–Lagrange equation of variational dynamics.
11.1.3
The Error Function
The error function, used extensively in statistics, is deﬁned as
erf(x) =
1
√π
# x
−x
e−t2dt =
2
√π
# x
0
e−t2dt
(11.9)
and has the property that erf(∞) = 1. The error function erf(x) gives the
area under the bell-shaped (normal) probability distribution located between
−x and +x.
11.1.4
Elliptic Functions
Recall from calculus4 that the element of length of a curve parameterized by
x = f(t),
y = g(t),
z = h(t),
t1 ≤t ≤t2,
4Or from our discussion of the parametric equation of curves in Chapter 4.

11.1 Integrals as Functions
323
in Cartesian coordinates is
dl =
	
dx2 + dy2 + dz2 =
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2 dt,
where prime indicates the derivative. So, the length L of the curve connecting
the initial point (f(t1), g(t1), h(t1)) to the ﬁnal point (f(t2), g(t2), h(t2)) is
L =
# t2
t1
	
[f ′(t)]2 + [g′(t)]2 + [h′(t)]2 dt.
(11.10)
The length of many curves, some very complicated-looking, can be found
there is no formula
in closed form for
the circumference
of an ellipse!
analytically using Equation (11.10). However, that of a simple curve such
as an ellipse turns out to be impossible! Let us see what we get when we
try to calculate the circumference of an ellipse. The parametric equation of
an ellipse of respective semi-major and semi-minor axes a and b lying in the
xy-plane is conveniently written as
x = a sin t,
y = b cost,
z = 0,
0 ≤t ≤2π.
(11.11)
Substitution of these equations in (11.10) yields
L =
# 2π
0
	
[a cos t]2 + [−b sin t]2 + [0]2 dt =
# 2π
0
	
a2 cos2 t + b2 sin2 t dt
=
# 2π
0

a2(1 −sin2 t) + b2 sin2 t dt = a
# 2π
0
	
1 −k2 sin2 t dt,
(11.12)
where k2 = (a2 −b2)/a2. This innocent-looking integral does not succumb
to any technique of integration. It was this resistance to analytical solution
that prompted the nineteenth century mathematicians to study this and other
related integrals as functions in their own right.
The elliptic integral of the ﬁrst kind is deﬁned as
elliptic integral of
the ﬁrst kind
F(ϕ, k) ≡
# ϕ
0
dt
	
1 −k2 sin2 t
(11.13)
with F a function of two variables because the integral involves two parame-
ters, one appearing in the integrand and the other appearing as a limit.
The elliptic integral of the second kind is deﬁned as
elliptic integral of
the second kind
E(ϕ, k) ≡
# ϕ
0
	
1 −k2 sin2 t dt.
(11.14)
The elliptic integral of the second kind can be interpreted as the length of
partial arcs of an ellipse. The circumference L of an ellipse with respective
semi-major and semi-minor axes a and b is simply
L = aE(2π, k)
where
k =
√
a2 −b2
a
.

324
Integrals and Series as Functions
It is common to deﬁne the complete elliptic integral of the ﬁrst and
second kinds:
complete elliptic
integrals
K(k) ≡F
π
2 , k

=
# π/2
0
dt
	
1 −k2 sin2 t
,
E(k) ≡E
π
2 , k

=
# π/2
0
	
1 −k2 sin2 t dt.
(11.15)
The reader may easily verify (Problem 11.10) that the total circumference of
an ellipse can be given in terms of complete elliptic integrals.
The parameterization given in Equation (11.11) is that of a horizontal
ellipse (a > b). However, one may wish to start with a vertical ellipse (a < b).
Then, as the reader may verify, one ends up with an integral similar to (11.14),
except that the coeﬃcient of sin2 t is +k2.
Would this be a new elliptic
integral? Problem 11.9 shows that the new integral can be written as a sum
of the existing elliptic integrals.
Example 11.1.2. Elliptic integrals show up in areas of physics totally unrelated
to the circumference of an ellipse. Consider a pendulum of mass m and length l
large-angle
pendulum and
elliptic integrals
displaced by an angle θ from its equilibrium position as shown in Figure 11.1. When
the angle is θ, the velocity of the pendulum is l ˙θ and its height is h. Conservation
of energy leads to
E = KE + PE = 1
2m(l ˙θ)2 + mgh = 1
2ml2 ˙θ2 + mg(l −l cos θ),
where E is the total mechanical energy of the pendulum. If θm is the maximum
angular displacement, then the total energy at this angle will be just the potential
energy.5 It then follows that
1
2m(l ˙θ)2 + mgh = 1
2ml2 ˙θ2 + mg(l −l cos θ) = mg(l −l cos θm),
or, after dividing both sides by ml,
1
2l ˙θ2 −g cos θ = −g cos θm.
(11.16)
l
h
θ
Figure 11.1: The pendulum displaced by an arbitrary angle θ.
5The KE is zero at θm because the pendulum comes to a momentary stop there.

11.1 Integrals as Functions
325
The elementary treatment of the pendulum problem diﬀerentiates Equation
(11.16) with respect to time, assumes that the maximum angle—and therefore any
angle—is small, and approximates sin θ with θ in radians. This leads to
l2 ˙θ¨θ + gl ˙θ sin θ = 0
or
¨θ + g
l sin θ = 0
θ→0
−→
¨θ + g
l θ = 0,
which is the equation of a simple harmonic oscillator6 with ω2 = g/l or T = 2π
	
l/g.
This is the famous result—known even to Galileo—that, for small angles, the period
of oscillation is independent of the angle.
A more advanced treatment makes no approximation for the angle and simply
integrates (11.16). Assuming that ˙θ > 0, Equation (11.16) gives
dθ
dt =

2g
l
√
cos θ −cos θm = 2

g
l
"
sin2
 θm
2

−sin2
 θ
2

,
(11.17)
where we used the trigonometric identity cos θ = 1 −2 sin2(θ/2). Introducing a new
variable s given by
sin
θ
2

≡sin
 θm
2

sin s,
diﬀerentiating this equation with respect to t, and using Equation (11.17) yields
ds
dt =

g
l
"
1 −sin2
 θm
2

sin2 s.
(11.18)
This leads to

g
l dt =
ds
	
1 −sin2(θm/2) sin2 s
which can be integrated to yield
t =
"
l
g
# s
0
du
	
1 −sin2(θm/2) sin2 u
≡
"
l
g F

s(θ), sin θm
2

,
(11.19)
where s = sin−1[sin(θ/2)/ sin(θm/2)], and we have assumed that at t = 0, the angle
θ is zero and therefore s = 0 as well.
Of particular interest is the period of the oscillation which is four times the time
period of a
pendulum depends
on the amplitude
of oscillation.
it takes the pendulum to go from θ = 0 to θ = θm. These values correspond to s = 0
and s = π/2. It follows that
T = 4
"
l
g
# π/2
0
du
	
1 −sin2(θm/2) sin2 u
≡4
"
l
g F
 π
2 , sin θm
2

= 4
"
l
g K

sin θm
2

.
(11.20)
6Recall that the equation of a simple harmonic oscillator (SHO)—such as a spring–mass
system with mass m and spring constant k—is m¨x + kx = 0 or ¨x + (k/m)x = 0. It is shown
in elementary physics that the angular frequency of this SHO is ω =
	
k/m. Thus, in
any SHO equation in which the second derivative appears with no coeﬃcient, the coeﬃcient
of the undiﬀerentiated quantity is the square of the angular frequency.

326
Integrals and Series as Functions
This shows clearly that for large maximum angles, the period does depend on the
amplitude. By expanding the integrand in a power series as developed in Chapter
10, one can obtain the deviation from constant period as powers of sin2(θm/2). We
quote the result of such an expansion
T = 2π
"
l
g

1 + 1
4 sin2 θm
2 + 9
64 sin4 θm
2 + · · ·

.
(11.21)
The reader is urged to verify this result (see Problems 11.11 and 11.12).
■
Historical Notes
The study of elliptical integrals can be said to have started in 1655 when Wallis
began to study the arc length of an ellipse. In fact he considered the arc lengths of
various cycloids and related these arc lengths to that of the ellipse. Both Wallis and
Newton published an inﬁnite series expansion for the arc length of the ellipse.
In 1679 Jacob Bernoulli attempted to ﬁnd the arc length of a spiral and encoun-
tered an example of an elliptic integral. He made an important step in the theory
of elliptic integrals in 1694. He examined the shape that an elastic rod will take if
compressed at the ends. He showed that the curve could be expressed in terms of
an integral, which was very similar to the one obtained by Wallis.
There is no doubt that Gauss obtained a number of key results in the theory
of elliptic functions, because many of these were found after his death in papers he
had never published. However, the acknowledged founders of the theory of elliptic
functions were Abel and Jacobi.
Niels Henrik Abel was the son of a poor pastor. As a student in Christiania
(Oslo), Norway, he had the luck to have Berndt Holmb¨oe (1795–1850) as a teacher.
Holmb¨oe recognized Abel’s genius and predicted when Abel was seventeen that he
would become the greatest mathematician in the world. After studying at Christia-
nia and at Copenhagen, Abel received a scholarship that permitted him to travel.
In Paris, he was presented to Legendre, Laplace, and Cauchy, but they ignored him.
Having exhausted his funds, he departed for Berlin and spent the years 1825–1827
with Crelle.
He returned to Christiania so exhausted that he found it necessary, he wrote, to
hold on to the gates of a church. To earn money he gave lessons to young students.
Niels Henrik Abel
1802–1829
He began to receive attention through his published works, and Crelle thought he
might be able to secure him a professorship at the University of Berlin. But Abel
became ill with tuberculosis and died in 1829 when he was only twenty-seven years
old.
Abel knew of the work of Euler, Lagrange, and Legendre on elliptic integrals, and
may have gotten ideas for his own work from the work of Gauss. Abel started to
write papers in 1825. He presented his major paper to the Academy of Sciences in
Paris in 1826. The paper was given to Cauchy to review it. But partly because of the
length and the diﬃculty of the paper and partly to favor his own work, Cauchy laid
it aside. After Abel’s death, when his fame was established, the academy searched
for the paper, found it, and published it in 1841.
The other discoverer of elliptic functions was Carl Gustav Jacob Jacobi.
Unlike Abel, he lived a quiet life. Born in Potsdam to a Jewish family, he studied at

11.2 Power Series as Functions
327
the University of Berlin and in 1827 became a professor at K¨onigsberg. In 1842 he
had to give up his post because of ill health. He was given a pension by the Prussian
government and retired to Berlin, where he died in 1851. His fame was great even
during his lifetime, and his students spread his ideas to many centers.
Jacobi taught the subject of elliptic functions for many years. His approach be-
came the model according to which the theory of functions itself was developed. He
also worked in functional determinants (Jacobians), ordinary and partial diﬀerential
equations, dynamics, celestial mechanics, and ﬂuid dynamics.
Carl Gustav Jacobi
1804–1851
Jacobi’s work on elliptic functions started in 1827 when he submitted a paper for
publication without proof. Almost simultaneously, Abel wrote his research paper on
elliptic functions. Both had arrived at the key idea of working with inverse functions
of the elliptic integrals, an idea that Abel had had since 1823. Thereafter, they both
published on the subject. But whereas Abel died in 1829, Jacobi lived to publish
much more. In particular, his Fundamenta Nova Theoriae Functionum Ellipticarum
of 1829 became a leading work on the subject.
11.2
Power Series as Functions
Diﬀerential equations have found their way into all areas of physics from the
motion of planets around the Sun to standing waves on a rope or a drum,
to electrical properties of conductors, and the behavior of electromagnetic
ﬁelds and beyond. As is always the case, no mathematics can draw more
attention than that which deals directly with Nature. The urgency of ﬁnding
solutions to these diﬀerential equations prompted many mathematicians of the
latter part of the eighteenth and the beginning of the nineteenth centuries to
concentrate heavily on certain speciﬁc diﬀerential equations. It appeared that
every diﬀerential equation dictated by Nature gave rise to a new function. The
most common scheme for solving these diﬀerential equations was to assume
a power series solution, substitute the assumed solution in the diﬀerential
equation, and determine the (unknown) coeﬃcients from the resulting equality
of power series. We shall come back to this powerful method in Chapters 24
and 25 through 27. At this point, we want to simply give examples of solutions
(functions) of certain diﬀerential equations that were discovered in the form
of a power series.
Chapter 10 showed how known functions (such as trigonometric and log-
arithmic functions) can be represented as power series. These functions had
been known prior to the popularity of inﬁnite series, and the origin of their
discovery lay in areas of mathematics outside calculus. One does not need a
power series to calculate sin(35◦); an appropriate right triangle and careful
measurement of its sides and hypotenuse will do the job. The functions we are
discussing here are deﬁned in terms of power series and do not have indepen-
dent existence. With some mathematical manipulation they may be written
as a deﬁnite integral—which cannot be evaluated analytically. But that is
just as abstract as an inﬁnite series because in the latter case, the integrals
become their deﬁnition.

328
Integrals and Series as Functions
11.2.1
Hypergeometric Functions
In their studies of second-order diﬀerential equations (DE), mathematicians,
always in search of generalities, came up with the most general form of a
second-order linear DE which appeared to encompass all known DEs of phys-
ical interest. This DE, called the hypergeometric diﬀerential equation,
hypergeometric
diﬀerential
equation
turned out to be7
x(1 −x)y′′ + [γ −(α + β + 1)x]y′ −αβy = 0,
(11.22)
where α, β, and γ are constants.8
The series solution of this DE, called
the hypergeometric function can be written in terms of the gamma func-
hypergeometric
function
tion as9
F(α, β; γ; x) ≡
Γ(γ)
Γ(α)Γ(β)
∞

n=0
Γ(α + n)Γ(β + n)
Γ(γ + n)Γ(n + 1) xn.
(11.23)
From this series representation, we immediately note that the hyperge-
ometric function is symmetric under interchange of α and β. Furthermore,
if either α or β is a negative integer, say −m, then the denominator of the
constant outside becomes inﬁnite by Deﬁnition 11.1.1. However, the gamma
function in the numerator of the ﬁrst m terms of the sum will also be inﬁnite.
The cancellation of these inﬁnities [see Problem 11.4(c)] gives a nonzero sum
up to m, but the rest of the series will be zero. Therefore,
Box 11.2.1. The hypergeometric function is symmetric under interchange
of α and β: F(α, β; γ; x) = F(β, α; γ; x). Furthermore, F(−m, β; γ; x)
[and therefore F(α, −m; γ; x)] is a polynomial if m is a positive integer.
As mentioned before, many a time, the inﬁnite series can be “integrated”
and the resulting function written in terms of an integral. In this case, we
start by multiplying and dividing the series of Equation (11.23) by Γ(γ −β)
to obtain
F(α, β; γ; x) =
Γ(γ)
Γ(α)Γ(β)Γ(γ −β)
∞

n=0
Γ(α + n)
Γ(n + 1)
Γ(γ −β)Γ(β + n)
Γ(γ + n)



≡B(γ−β,β+n) by (11.7)
xn.
7For a comprehensive treatment of this diﬀerential equation, see Hassani, S. Mathemati-
cal Physics: A Modern Introduction to Its Foundations, Springer-Verlag, 1999, Chapter 14.
8Some authors use a, b, and c instead of α, β, and γ.
9Some authors use 2F1 instead of F . Our use of F to represent both the elliptic integral
of the ﬁrst kind and the hypergeometric function should not cause any confusion because
the two functions have diﬀerent numbers of arguments (independent variables).

11.2 Power Series as Functions
329
Now use Γ(n + 1) = n! and the integral representation of the beta function to
get
F(α, β; γ; x) =
Γ(γ)
Γ(α)Γ(β)Γ(γ−β)
∞

n=0
# 1
0
dt(1−t)γ−β−1tβ+n−1Γ(α+n)xn
n!
=
Γ(γ)
Γ(β)Γ(γ −β)
# 1
0
dt(1 −t)γ−β−1tβ−1
∞

n=0
Γ(α + n)
Γ(α)
(tx)n
n!
.
Using the result of Problem 11.4, we can now write
integral
representation of
the
hypergeometric
function
F(α, β; γ; x) =
Γ(γ)
Γ(β)Γ(γ −β)
# 1
0
dt(1 −t)γ−β−1tβ−1(1 −tx)−α.
(11.24)
This is the integral representation of the hypergeometric function.
The generality of the hypergeometric DE results in the ability to express
many functions—both elementary and the so-called special functions of math-
ematical physics—in terms of the hypergeometric function. For example, con-
sider the complete elliptic integral of the second kind E(k). The two factors of
double factorials in both the numerator and denominator of its series expan-
sion (see Problem 11.13), together with Equation (11.5) and the hypergeomet-
ric series (11.23), hint at the possibility of writing E(k) as a hypergeometric
function. This is indeed the case. Substituting (11.5) in the expansion of
E(k) as given in Problem 11.13 yields
E(k) = π
2
0
1 −
∞

n=1
Γ(n + 1
2)Γ(n + 1
2)π−1
Γ(n + 1)Γ(n + 1)
k2n
2(n −1
2)
1
= π
2 −1
4
∞

n=1
Γ(n + 1
2)Γ(n −1
2)
Γ(n + 1)Γ(n + 1) (k2)n,
where we used Γ(n + 1
2) = (n −1
2)Γ(n −1
2). The sum starts with n = 1. To
make it look like a hypergeometric series, we need to include the zero term as
well. Adding and subtracting this term gives
E(k) = π
2 −1
4
∞

n=0
Γ(n + 1
2)Γ(n −1
2)
Γ(n + 1)Γ(n + 1) (k2)n + 1
4
Γ( 1
2)Γ(−1
2)
Γ(1)Γ(1) (k2)0

= −1
4
∞

n=0
Γ(n + 1
2)Γ(n −1
2)
Γ(n + 1)Γ(n + 1) (k2)n
because Γ(−1
2) = −2Γ( 1
2) = −2√π by Example 11.1.1. We now note that
except for a multiplicative constant, the sum is that of the hypergeometric
function with α = 1
2 = −β and γ = 1. Inserting the multiplicative constant
Γ(1)
Γ( 1
2)Γ(−1
2) =
1
(−2π)

330
Integrals and Series as Functions
we obtain
E(k) = π
4 F
 1
2, −1
2; 1; k2!
.
(11.25)
The reader may verify that
K(k) = π
4 F
 1
2, 1
2; 1; k2!
.
(11.26)
Historical Notes
Johann Carl Friedrich Gauss was the greatest of all mathematicians and perhaps
the most richly gifted genius of whom there is any record. He was born in the city of
Brunswick in northern Germany. His exceptional skill with numbers was clear at a
very early age, and in later life he joked that he knew how to count before he could
talk. It is said that Goethe wrote and directed little plays for a puppet theater when
he was six and that Mozart composed his ﬁrst childish minuets when he was ﬁve,
but Gauss corrected an error in his father’s payroll accounts at the age of three. At
the age of seven, when he started elementary school, his teacher was amazed when
Gauss summed the integers from 1 to 100 instantly by spotting that the sum was
50 pairs of numbers each pair summing to 101.
His long professional life is so ﬁlled with accomplishments that it is impossible
to give a full account of them in the short space available here. All we can do is
simply give a chronology of his almost uncountable discoveries.
1792–1794: Gauss reads the works of Newton, Euler, and Lagrange; discovers the
prime number theorem (at the age of 14 or 15); invents the method of least squares;
conceives the Gaussian law of distribution in the theory of probability.
1795: (only 18 years old!) Proves that a regular polygon with n sides is constructible
(by ruler and compass) if and only if n is the product of a power of 2 and distinct
prime numbers of the form pk = 22k + 1, and completely solves the 2000-year old
problem of ruler-and-compass construction of regular polygons. He also discovers
the law of quadratic reciprocity.
Carl Friedrich
Gauss 1777–1855
1799: Proves the fundamental theorem of algebra in his doctoral dissertation
using the then-mysterious complex numbers with complete conﬁdence.
1801: Gauss publishes his Disquisitiones Arithmeticae in which he creates the mod-
ern rigorous approach to mathematics; predicts the exact location of the asteroid
Ceres.
1807: Becomes professor of astronomy and the director of the new observatory at
G¨ottingen.
1809: Publishes his second book, Theoria motus corporum coelestium, a major
two-volume treatise on the motion of celestial bodies and the bible of planetary as-
tronomers for the next 100 years.
1812: Publishes Disquisitiones generales circa seriem inﬁnitam, a rigorous treat-
ment of inﬁnite series, and introduces the hypergeometric function for the ﬁrst
time, for which he uses the notation F(α, β; γ; z); an essay on approximate integra-
tion.
1820–1830: Publishes over 70 papers, including Disquisitiones generales circa su-
perﬁcies curvas, in which he creates the intrinsic diﬀerential geometry of general
curved surfaces, the forerunner of Riemannian geometry and the general theory of
relativity.

11.2 Power Series as Functions
331
From the 1830s on, Gauss was increasingly occupied with physics, and he en-
riched every branch of the subject he touched. In the theory of surface tension,
he developed the fundamental idea of conservation of energy and solved the earli-
est problem in the calculus of variations. In optics, he introduced the concept
of the focal length of a system of lenses. He virtually created the science of geo-
magnetism, and in collaboration with his friend and colleague Wilhelm Weber he
invented the electromagnetic telegraph. In 1839 Gauss published his fundamental
paper on the general theory of inverse square forces, which established potential
theory as a coherent branch of mathematics and in which he established the di-
vergence theorem.
Gauss had many opportunities to leave G¨ottingen, but he refused all oﬀers and
remained there for the rest of his life, living quietly and simply, traveling rarely, and
working with immense energy on a wide variety of problems in mathematics and
its applications. Apart from science and his family—he married twice and had six
children, two of whom emigrated to America—his main interests were history and
world literature, international politics, and public ﬁnance. He owned a large library
of about 6000 volumes in many languages, including Greek, Latin, English, French,
Russian, Danish, and of course German. His acuteness in handling his own ﬁnancial
aﬀairs is shown by the fact that although he started with virtually nothing, he left
an estate over a hundred times as great as his average annual income during the last
half of his life.
The foregoing list is the published portion of Gauss’s total achievement; the un-
published and private part is almost equally impressive. His scientiﬁc diary, a little
booklet of 19 pages, discovered in 1898, extends from 1796 to 1814 and consists of 146
very concise statements of the results of his investigations, which often occupied him
for weeks or months. These ideas were so abundant and so frequent that he physi-
cally did not have time to publish them. Some of the ideas recorded in this diary:
Cauchy Integral Formula: Gauss discovers it in 1811, 16 years before Cauchy.
Non-Euclidean Geometry: After failing to prove Euclid’s ﬁfth postulate at the
age of 15, Gauss came to the conclusion that the Euclidean form of geometry cannot
be the only one possible.
Elliptic Functions: Gauss had found many of the results of Abel and Jacobi (the
two main contributors to the subject) before these men were born. The facts became
known partly through Jacobi himself. His attention was caught by a cryptic passage
in the Disquisitiones, whose meaning can only be understood if one knows some-
thing about elliptic functions. He visited Gauss on several occasions to verify his
suspicions and tell him about his own most recent discoveries, and each time Gauss
pulled 30-year-old manuscripts out of his desk and showed Jacobi what Jacobi had
just shown him. After a week’s visit with Gauss in 1840, Jacobi wrote to his brother,
“Mathematics would be in a very diﬀerent position if practical astronomy had not
diverted this colossal genius from his glorious career.”
A possible explanation for not publishing such important ideas is suggested by
his comments in a letter to Bolyai: “It is not knowledge but the act of learning, not
possession but the act of getting there, which grants the greatest enjoyment. When
I have clariﬁed and exhausted a subject, then I turn away from it in order to go into
darkness again.” His was the temperament of an explorer who is reluctant to take the
time to write an account of his last expedition when he could be starting another. As
it was, Gauss wrote a great deal, but to have published every fundamental discovery
he made in a form satisfactory to himself would have required several long lifetimes.

332
Integrals and Series as Functions
11.2.2
Conﬂuent Hypergeometric Functions
The parameters α, β, and γ determine the behavior of the hypergeometric
function completely. A great number of diﬀerential equations in mathematical
physics correspond to the case where only two parameters are involved. The
most eﬀective way of accommodating this arises from the conﬂuence β →∞.
Let us see how this works.
Substitute x = u/β in the hypergeometric DE using the—very simple—
chain rule to transform the x-derivatives to the u-derivatives. This leads to
the DE
u
β

1 −u
β

β2 d2y
du2 +

γ −(α + β + 1)u
β

β dy
du −αβy = 0.
Dividing the entire equation by β, taking the limit β →∞—thus neglecting
u/β and 1/β—yields the so-called conﬂuent hypergeometric diﬀerential
equation:
conﬂuent
hypergeometric
diﬀerential
equation
xy′′ + (γ −x)y′ −αy = 0,
(11.27)
where we restored x as the independent variable.
The inﬁnite series solution of this DE is called the conﬂuent hypergeo-
metric function. This solution, as well as its integral representation, can be
conﬂuent
hypergeometric
function
obtained by taking the appropriate limit of the corresponding expression for
the hypergeometric function. The limit of Equation (11.23) yields
Φ(α; γ; x) ≡lim
β→∞F(α, β; γ; x/β) = Γ(γ)
Γ(α)
∞

n=0
Γ(α + n)
Γ(γ + n)Γ(n + 1)xn,
(11.28)
where we used
Γ(β + n)
βnΓ(β) = (β + n −1)(β + n −2) · · · βΓ(β)
βnΓ(β)
=
β + n −1
β
 β + n −2
β

· · ·
β
β

β→∞
−−−−→1.
Similarly, we have
Φ(α; γ; x) = lim
β→∞F(β, α; γ; x/β)
= lim
β→∞
Γ(γ)
Γ(α)Γ(γ −α)
# 1
0
dt(1 −t)γ−α−1tα−1

1 −tx
β
−β



→etx (Prob. 11.3)
,
where we have used the symmetry of the hypergeometric function under inter-
change of its ﬁrst two parameters. It follows that the integral representation
integral
representation of
the conﬂuent
hypergeometric
function
of the conﬂuent hypergeometric function is
Φ(α; γ; x) =
Γ(γ)
Γ(α)Γ(γ −α)
# 1
0
dt(1 −t)γ−α−1tα−1etx.
(11.29)

11.2 Power Series as Functions
333
We note that
Φ(α; α; x) =
∞

n=0
1
Γ(n + 1)xn =
∞

n=0
xn
n! = ex
and Problem 11.20 shows that
erf(x) = 2x
√π Φ( 1
2; 3
2; −x2).
Many other functions encountered in mathematical physics can also be ex-
pressed in terms of conﬂuent hypergeometric functions, and we shall point
this out as we come across these functions in the sequel. We note in passing
that, as in the case of hypergeometric function,
Box 11.2.2. If α happens to be a negative integer, then Φ(α; γ; x) becomes
a polynomial, i.e., the inﬁnite series truncates.
11.2.3
Bessel Functions
Bessel functions are arguably among the most utilized functions of mathe-
matical physics. We shall come back to them when we consider solutions of
Laplace’s equation in cylindrical coordinates and discover their connection
with other functions treated in this chapter. At this point, we simply intro-
duce them as power series. The Bessel function Jν(x) of order ν is a solution
of the Bessel diﬀerential equation:
Bessel diﬀerential
equation
xd2y
dx2 + dy
dx +

x −ν2
x

y = 0.
(11.30)
Chapter 27 shows how to obtain the power series expansion of Jν(x):
Jν(x) =
x
2
ν
∞

k=0
(−1)k
k!Γ(ν + k + 1)
x
2
2k
.
(11.31)
The point to emphasize is that
Box 11.2.3. Bessel functions are always given in terms of their expan-
sion in power series (or as an integral involving parameters). It is gener-
ally impossible to reduce Bessel functions to any functional combination
of more elementary functions such as polynomials, or trigonometric and
exponential functions.

334
Integrals and Series as Functions
Properties and applications of Bessel functions are treated in some detail in
Chapter 27.10 However, some relations are elementary enough to be included
here, as they also illustrate the use of summation symbols. First note that if
ν is an integer −m, then
J−m(x) =
x
2
−m ∞

k=0
(−1)k
k!Γ(−m + k + 1)
x
2
2k
=
x
2
−m
∞

k=m
(−1)k
k!Γ(−m + k + 1)
x
2
2k
because the ﬁrst m terms of the ﬁrst series have gamma functions in the
denominator with negative integer (or zero) arguments. Now in the second
series, replace k by n = k −m. This yields
J−m(x) =
x
2
−m
∞

n=0
(−1)m+n
(m + n)!Γ(n + 1)
x
2
2m+2n
(11.32)
= (−1)m x
2
m
∞

n=0
(−1)n
Γ(m + n + 1)n!
x
2
2n
= (−1)mJm(x),
where we used Γ(j + 1) = j! for positive integer j.
Example 11.2.1. Bessel functions of half-integer order are related to trigonomet-
ric functions. To see this, note that
J1/2 =
 x
2
1/2
∞

k=0
(−1)k
k!Γ(k + 3
2)
 x
2
2k
=
 x
2
−1/2
∞

k=0
(−1)k
k!Γ(k + 3
2)22k+1 x2k+1.
Now substitute for Γ(k + 3
2) in terms of factorials as given in Problem 11.1 to obtain
J1/2 =
 x
2
−1/2
1
√π
∞

k=0
(−1)k
(2k + 1)!x2k+1



=sin x
=
 2
πx
1/2
sin x.
Similarly,
J−1/2 =
 2
πx
1/2
cos x
as the reader may verify.
■
10See also Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, Section 14.5.

11.2 Power Series as Functions
335
Another formula of interest is a recursion relation connecting Bessel func-
tions of diﬀerent integer orders. Write Jm−1(x) as
Jm−1(x) =
x
2
m−1 ∞

k=0
(−1)k
k!Γ(m + k)
x
2
2k
(11.33)
=
x
2
m−1
A
1
Γ(m) +
∞

k=1
(−1)k
k!Γ(m + k)
x
2
2k
B
,
where we separated the k = 0 term from the rest of the sum. Similarly, write
Jm+1(x) as
Jm+1(x) =
x
2
m+1 ∞

k=0
(−1)k
k!Γ(m + k + 2)
x
2
2k
=
x
2
m+1 ∞

j=1
(−1)j−1
(j −1)!Γ(m + j + 1)
x
2
2j−2
(11.34)
= −
x
2
m−1 ∞

k=1
(−1)k
(k −1)!Γ(m + k + 1)
x
2
2k
,
where in the second line, we substituted j = k + 1 for k, and in the last line,
we used (−1)−1 = −1, factored (x/2)−2 out of the summation, and changed
the dummy index back to k. Now add Equations (11.33) and (11.34) and use
1
k!Γ(m + k) −
1
(k −1)!Γ(m + k + 1) =
m
k!Γ(m + k + 1),
and 1/Γ(m) = m/Γ(m + 1) to obtain
Jm−1(x) + Jm+1(x) =
x
2
m−1
A
m
Γ(m + 1) +
∞

k=1
(−1)km
k!Γ(m + k + 1)
x
2
2k
B
= m
x
2
−1
Ax
2
m ∞

k=0
(−1)k
k!Γ(m + k + 1)
x
2
2k
B



=Jm(x)
or, ﬁnally,
Jm−1(x) + Jm+1(x) = 2m
x Jm(x).
(11.35)
The straightforward details are left as Problem 11.22. One can also show that
Jm−1(x) −Jm+1(x) = 2J′
m(x),
(11.36)
where prime indicates diﬀerentiation. Equations (11.35) and (11.36) lead to
Jm−1(x) = m
x Jm(x) + J′
m(x),
Jm+1(x) = m
x Jm(x) −J′
m(x).
(11.37)
These plus the results of Example 11.2.1 give all Bessel functions of half-
integer order.

336
Integrals and Series as Functions
11.3
Problems
11.1. (a) Show that (see Deﬁnition 11.1.2 for the deﬁnition of the following
notation):
(2n)!! = 2nn!
and
(2n −1)!! = (2n)!
2nn! .
Hint: For the second relation, supply the missing even factors in the “numer-
ator” and the “denominator” of (2n −1)!!
(b) Using (a) and Example 11.1.1, show that
Γ(n + 1
2) = (2n −1)!!
2n
√π = (2n)!
22nn!
√π.
(c) Now use (b) to obtain the following result:
Γ(n + 3
2) = (2n + 1)!
22n+1n!
√π.
11.2. Using the result of Problem 11.1, show that
(2n + m)! =
1
√π 22n+mΓ

n + m
2 + 1

Γ

n + m + 1
2

.
Hint: Consider the two cases of even m (with m = 2k) and odd m (with
m = 2k + 1) separately, and show at the end that both can be written as a
single formula.
11.3. Using the result
lim
n→∞

1 + 1
n
n
= e
show that
lim
n→∞

1 −t
n
n
= e−t.
Hint: Let n = −tm.
11.4. (a) By using Equation (11.2) repeatedly, show that
Γ(a + n) = (a + n −1)(a + n −2) · · · (a + n −k)Γ(a + n −k).
(b) Let k = n in the above equation to show that
a(a + 1) · · · (a + n −1) = Γ(a + n)
Γ(a)
.
(c) Using (b) show that
α(α −1) · · · (α −n + 1) = (−1)n Γ(n −α)
Γ(−α) .

11.3 Problems
337
11.5. Show that
Γ(x) = 2
# ∞
0
e−t2t2x−1 dt
and
Γ(x) =
# 1
0
(
ln
1
t
)x−1
dt.
11.6. Find the following integrals in terms of the gamma function:
(a)  ∞
0
t2x+1e−at2 dt.
(b)  ∞
0
t2xe−at2 dt.
11.7. Using only its integral representation, show that beta function is sym-
metric under interchange of its arguments.
11.8. Using the deﬁnition of the gamma function, show the justiﬁcation for
the frequently used equality 0! = 1.
11.9. Show that
# ϕ
0
	
1 + k2 sin2 t dt =
	
1 + k2
*
E(k′) −E
π
2 −ϕ, k′+
,
where k′ = k/
√
1 + k2. Hint: Change t to s = π/2 −t and break up the
interval of integration of the resulting integral into two.
11.10. Show that the circumference of an ellipse of respective semi-major and
semi-minor axes a and b is 4aE(k) where k =
√
a2 −b2/a. Verify that you
get the expected result when a = b.
11.11. (a) Expand the square roots in the deﬁnition of the elliptic integrals
of the ﬁrst and second kinds in powers of k2 sin2 t, and keep the ﬁrst three
terms.
(b) Now integrate those terms to ﬁnd an approximation to elliptic integrals
for small k.
(c) Substitute π/2 for ϕ to obtain approximation for the complete elliptic
integrals.
11.12. Use the result of Problem 11.11 to obtain Equation (11.21).
11.13. Use the integral
# π/2
0
sin2n t dt = (2n −1)!!
(2n)!!
π
2
to show that
E(k) = π
2
0
1 −
∞

n=1
(2n −1)!!
(2n)!!
2
k2n
2n −1
1
,
K(k) = π
2
0
1 +
∞

n=1
(2n −1)!!
(2n)!!
2
k2n
1
.

338
Integrals and Series as Functions
11.14. Show that E(0) = K(0) = π/2, and that E(1) = 1, K(1) = ∞.
11.15. Use the ratio test on the hypergeometric series to determine its radius
of convergence.
11.16. Verify that the complete elliptic integral of the ﬁrst kind is related to
the hypergeometric function as follows:
K(k) = π
4 F
 1
2, 1
2; 1; k2!
.
11.17. Show that ln(1 + x) = xF(1, 1; 2; −x).
11.18. Use the result of Problem 11.4 to express Equation (10.15) of Chapter
10 in terms of the gamma function; then show that
(1 + x)α =
∞

n=0
Γ(n −α)
Γ(−α)Γ(n + 1)(−x)n = F(−α, β; β; −x)
for arbitrary β.
11.19. By using integral representations:
(a) Show that
B(a, b) = Γ(a)Γ(b + r)
Γ(a + b + r) F(a, r; a + b + r; 1),
where B is the beta function and r is any real number. Choose r appropriately
and show that
B(a, b) = 1
aF(a, 1 −b; a + 1; 1).
(b) Also prove that
F(α, β; γ; 1) = Γ(γ)Γ(γ −α −β)
Γ(γ −α)Γ(γ −β).
11.20. Expand the integrand of erf(x) in its Maclaurin series and use
2n + 1 = 2(n + 1
2) = Γ( 3
2)
Γ( 1
2)
to show that
erf(x) = 2x
√π Φ( 1
2; 3
2; −x2).
11.21. Using the same procedure as in Example 11.2.1, show that
J−1/2 =
 2
πx
1/2
cos x.

11.3 Problems
339
11.22. Show that
1
k!Γ(m + k) −
1
(k −1)!Γ(m + k + 1) =
m
k!Γ(m + k + 1)
and use it to derive Equation (11.35).
11.23. Derive Equation (11.36).
11.24. Find J3/2(x) and J−3/2(x). Hint: Use Equation (11.37).

Part IV
Analysis of Vectors

Chapter 12
Vectors and Derivatives
One of the basic tools of physics is the calculus of vectors. A great variety
of physical quantities are vectors which are functions of several variables such
as space coordinates and time, and, as such, are good candidates for mathe-
matical analysis. We have already encountered examples of such analyses in
our treatment of the integration of vectors as in calculating electric, magnetic,
and gravitational ﬁelds. However, vector analysis goes beyond simple vector
integration. Vectors have a far richer structure than ordinary numbers, and,
therefore, allow a much broader range of concepts.
Fundamental to the study of vector analysis is the notion of ﬁeld, with
which we have some familiarity based on our study of Chapters 1 and 4.
Fields play a key role in many areas of physics: In the motion of ﬂuids, in the
conduction of heat, in electromagnetic theory, in gravitation, and so forth. All
these situations involve a physical quantity that varies from point to point as
well as from time to time,1 i.e., it is a function of space coordinates and time.
This physical quantity can be either a scalar, in which case we speak of a
scalar ﬁeld, or a vector, in which case we speak of a vector ﬁeld. There are
scalar and vector
ﬁelds
also tensor ﬁelds, which we shall discuss brieﬂy in Chapter 17, and spinor
ﬁelds, which are beyond the scope of this book.
The temperature of the atmosphere is a scalar ﬁeld because it is a function
of space coordinates—equator versus the poles—and time (summer versus
winter), and because temperature has no direction associated with it. On the
other hand, wind velocity is a vector ﬁeld because (a) it is a vector and (b) its
magnitude and direction depend on space coordinates and time. In general,
when we talk of a vector ﬁeld, we are dealing with three functions of space
and time, corresponding to the three components of the vector.
1In many instances ﬁelds are independent of time in which case we call them static
ﬁelds.

344
Vectors and Derivatives
12.1
Solid Angle
Before discussing the calculus of vectors, we want to introduce the concept of
a solid angle which is an important and recurrent concept in mathematical
physics, especially in the discussion of vector calculus.
12.1.1
Ordinary Angle Revisited
We start with the concept of angle from a new perspective which easily gener-
alizes to solid angle. Consider a curve and a point P in a plane. The point P
concept of angle
reexamined
is taken to lie oﬀthe curve [Figure 12.1(a)]. An arbitrary segment of the curve
deﬁnes an angle which is obtained by joining the two ends of the segment to
P. In particular, an element of length along the curve deﬁnes an inﬁnitesimal
angle. We want to relate the length of this element to the size of its angle
measured in radians.
Connect P to the midpoint of the inﬁnitesimal line element of length Δl,
and call the resulting vector R with the corresponding unit vector ˆeR as shown
in Figure 12.1(a).2 Let the angle between ˆeR and the unit normal3 to the
length element ˆen be α. As shown in the magniﬁed diagram of Figure 12.1(b),
α is also the angle between the line element QQ′ and the line segment obtained
by dropping a perpendicular QH onto the ray PQ′.
It is clear from the
diagram that
QH = QQ′ cos α ⇒QH = Δl cos α = Δl ˆeR · ˆen.
Now recall that the measure of an angle in radians is given by the ratio of
the length of the arc of a circle subtended by the angle to the radius of the
circle, and this measure is independent of the size of the circle chosen. To
ﬁnd the measure of Δθ in radians, let us choose a circle of radius R = |R|,
(a)
(b)
P
eR
^
R
Δθ
Q 
Q
en
^
'
en
^
α
Q
C
C 
H
α
to  P
to  P
Q 
eR
^
'
'
'
dl cos α
dl
O
r
r
Figure 12.1: Deﬁning angles as ratios of lengths.
2In actual calculations, it is convenient to denote the position vector of P by r, say, and
that of the midpoint by r′. Then R = r′ −r.
3There are two possible directions for this unit normal: one as shown in Figure 12.1,
and the other in the opposite direction. As long as we deal with open curves (no loops)
this arbitrariness persists.

12.1 Solid Angle
345
the distance from P to the midpoint of the line element. The arc of this circle
subtended by Δθ is CC′, and the ﬁgure shows that the length of this arc is
very nearly equal to QH. One can think of CC′ as the projection of the line
element onto the circle. Thus,
Δθ ≈QH
R
= Δl ˆeR · ˆen
R
.
If we denote the location of P by r and that of Δl by r′, then
R = r′ −r,
ˆeR = r′ −r
|r′ −r|,
and we obtain
Δθ = Δl(r′) ˆen · (r′ −r)
|r′ −r|2
,
(12.1)
where we have emphasized the dependence of Δl on r′.
For a ﬁnite segment of the curve, we integrate to obtain the angle. This
yields
angle as integral
θ =
# b
a
dl ˆeR · ˆen
R
=
# b
a
dl(r′) ˆen · (r′ −r)
|r′ −r|2
,
(12.2)
where a and b are the beginning and the end of the ﬁnite segment. There is a
way of calculating this ﬁnite angle which, although extremely simple-minded,
is useful when we generalize to solid angle. Since the size of the circle used to
measure the angle is irrelevant, let us choose a single ﬁducial circle of radius a
centered at P (see Figure 12.2). Then, as we project elements of length from
the curve, we obtain inﬁnitesimal arcs of this circle with the property that
dθ = dl ˆeR · ˆen
R
= dlc
a ,
where dlc is the element of arc of the ﬁducial circle. From this equation, we
obtain
θ =
# b′
a′
dlc
a = 1
a
# b′
a′ dlc = s
a,
(12.3)
where a′ and b′ are projections of a and b on the circle, and s is the length of
the arc from a′ to b′. This last relation is, of course, our starting point where
we deﬁned the measure of an angle in radians!
Of special interest is the case where the curve loops back on itself. For
such a case, the direction of ˆen is predetermined by
Box 12.1.1. (Convention). We agree that for angle calculations, the
unit normal shall always point out of a closed loop.

346
Vectors and Derivatives
P
a
b
a 
b 
′
′
P
(a)
(b)
en^
eR
^
eR
^
en
^
Figure 12.2: Total angle subtended by a closed curve about a point (a) inside and (b)
outside.
If P happens to be inside the loop [Figure 12.2(a)], the total angle, corre-
sponding to a complete traversal of the loop, is
θ = s
a = 2πa
a
= 2π.
When P is outside, we get θ = 0. This can be seen in Figure 12.2(b) where the
projection of the closed curve covers only a portion of the ﬁducial circle and it
does so twice, once with a positive sign—when ˆeR and ˆen are separated by an
acute angle—and once with a negative sign—when ˆeR and ˆen are separated
by an obtuse angle. Let us denote by θC
P the total angle subtended by the
closed curve C about a point P and by U the region enclosed by C. Then,
we have
total angle at a
point subtended
by a closed curve
θC
P =
0
2π
if P is in U,
0
if P is not in U.
(12.4)
Example 12.1.1. Point P is located outside a rectangle of sides 2a and 2b as
shown in Figure 12.3. We want to verify Equation (12.4). The integration is nat-
urally divided into four regions: right, top, left, and bottom.
We shall do the
2a
2b
y0
P
y
x
O
′r
r
α
β
A
B
C
D
θr
Figure 12.3: Total angle subtended by a rectangle about a point outside.

12.1 Solid Angle
347
right-hand-side integration in detail, leaving the rest for the reader to verify. For
the right side we have r = y0ˆey, r′ = aˆex + y′ˆey, and
dl = +dy′, R = r′ −r = ⟨a, y′ −y0⟩, ˆen = ˆex.
Therefore,
dθr = dl ˆeR · ˆen
R
= dy′ R · ˆex
R2
=
a dy′
a2 + (y′ −y0)2 ,
and the total integrated angle for the right side is
θr = a
# b
−b
dy′
a2 + (y′ −y0)2 =
=∠CBP



tan−1
 y0 + b
a

−
=∠DAP



tan−1
y0 −b
a

= π
2 −α −
 π
2 −β

= β −α.
Similarly, one can easily show that θt = −2β, θl = β −α, and θb = 2α, where t
stands for “top,” l for “left,” and b for “bottom.” The total subtended angle is,
therefore zero, as expected. Note that only for the top side is the angle between ˆen
and ˆeR obtuse, and this fact results in the negative value for θt.
■
The purpose of the whole discussion of the ordinary angle in such a high-
brow fashion and detail has been to lay the ground work for the introduction
of the solid angle. As we shall see shortly, a good understanding of the new
properties of the ordinary angle discussed above makes the transition to the
solid angle almost trivial.
12.1.2
Solid Angle
We are now ready to generalize the notion of the angle to one dimension
higher. Instead of a curve we have a surface, instead of a line element we have
an area element, and instead of dividing by R we need to divide by R2. This
last requirement is necessary to render the “angle” dimensionless. Referring
to Figure 12.4,
solid angle deﬁned
R
eR
^
P
en^
O
r
r
'
Figure 12.4: Solid angle as the ratio of area to distance squared.

348
Vectors and Derivatives
Box 12.1.2. We deﬁne the solid angle subtended by the element of area
Δa as
ΔΩ ≈Δa ˆen · ˆeR
R2
= ˆeR · Δa
R2
= (r′ −r) · Δa
|r′ −r|3
,
where ˆen is the unit normal to the surface and Δa ≡ˆenΔa(r′).
The numerator is simply the projection of Δa onto a sphere of radius R
as Figure 12.5 shows. This projection is obtained by the intersection of the
ﬁducial sphere and the rays drawn from P to the boundary of Δa. As in the
case of the angle, the choice of ﬁducial sphere is arbitrary. The integral form
of the above equation is
Ω =
# #
S
ˆeR · da
R2
=
# #
S
R · da
R3
=
# #
S
(r′ −r) · da(r′)
|r′ −r|3
,
(12.5)
where S is the surface subtended by the solid angle Ω.
Box 12.1.3. (Convention). For any closed surface S, we take ˆen to be
pointing outward.
If we use a single ﬁducial sphere of radius b for all points of S, we obtain
Ω =
# #
Sb
da
b2 = 1
b2
# #
Sb
da = A
b2 ,
(12.6)
where Sb is the projection of S onto the ﬁducial sphere and A its area. This
equation is the analog of Equation (12.3) and can be used to deﬁne the measure
R
eR
^
P
en
^
Figure 12.5: The relation between the ˆeR · Δa and its projection on a ﬁducial sphere.

12.1 Solid Angle
349
of solid angles. In particular, if the surface S is closed and P is inside, then
A will be the total area of the ﬁducial sphere and we get Ω = 4πb2/b2 = 4π.
When P is outside, we get equal amounts of positive and negative contribu-
tions with the net result of zero.
total solid angle at
a point subtended
by a closed surface
Theorem 12.1.2. Denote by ΩS
P the total solid angle subtended by the closed
surface S about a point P and by V the region enclosed by S. Then,
ΩS
P =
0
4π
if P is in V,
0
if P is not in V.
(12.7)
Example 12.1.3. As an example of the calculation of the solid angle, consider a
square of side 2a with the point P located a distance z0 from its center as shown in
Figure 12.6. With r = ⟨0, 0, z0⟩and r′ = ⟨x′, y′, 0⟩, we have R = r′−r = ⟨x′, y′, −z0⟩,
and assuming that ˆen points in the negative z-direction,4 we have
dΩ = da ˆen · ˆeR
R2
= dx′ dy′ (−ˆez) · R
R3
=
z0 dx′ dy′
(x′2 + y′2 + z2
0)3/2 .
The solid angle is obtained by integrating this:
Ω = z0
# a
−a
dx′
# a
−a
dy′
(x′2 + y′2 + z2
0)3/2
= 2az0
# a
−a
dx′
	
x′2 + a2 + z2
0 (x′2 + z2
0)
= 4 tan−1

a2
z0
	
2a2 + z2
0

.
An interesting special case is when z0 = a. Then
Ω = 4 tan−1

a2
a
√
3a2

= 4 tan−1
 1
√
3

= 4(π/6) = 2π/3.
The last result can also be derived in a simpler way. When z0 = a, the point P will
be at the center of a cube of side 2a. Since the total solid angle subtended about P
is 4π, and all six sides contribute equally, the solid angle subtended by one side is
4π/6.
■
r
P
2a
r
'
Figure 12.6: The solid angle subtended by a square of side 2a.
4This assumption is not forced by any convention. It is chosen to make the ﬁnal result
positive.

350
Vectors and Derivatives
Example 12.1.4. Let us replace the square of the last example with a circle of
radius a. We can proceed along the same lines as before. However, in this particular
case, we note that the solid angle is in the shape of a cone which is one of the
primary surfaces of the spherical coordinate system. Placing the origin at P and
projecting the area on a ﬁducial sphere, of radius b say, we may write
Ω = Ab
b2 = 2πb2(1 −cos α)
b2
= 2π(1 −cos α),
where Ab ≡2πb2(1 −cos α) is the area of the projection of the circle on the ﬁducial
sphere. The half-angle of the cone is denoted by α with
tan α = a
z0
⇒cos α =
z0
	
a2 + z2
0
.
The ﬁnal result is
Ω = 2π

1 −
z0
	
a2 + z2
0

.
(12.8)
It is instructive to obtain this result directly as in the previous example.
■
12.2
Time Derivative of Vectors
Scalar and vector ﬁelds can be subjected to such analytic operations as diﬀer-
entiation and integration to obtain new scalar and vector ﬁelds. The deriva-
tive of a vector with respect to a variable (say time) in Cartesian coordinates
amounts to diﬀerentiating each component:
∂A
∂t = ∂Ax
∂t ˆex + ∂Ay
∂t ˆey + ∂Az
∂t ˆez.
(12.9)
In other coordinate systems, one needs to diﬀerentiate the unit vectors as well.
In general, the derivative of a vector is deﬁned in exactly the same manner
as for ordinary functions. We have to keep in mind that a vector physical
quantity, such as an electric ﬁeld, is a function of space and time, i.e., its
components are real-valued functions of space and time. So, consider a vector
A which is a function of a number of independent variables (t1, t2, . . . , tm).
Then, we deﬁne the partial derivative as before:
∂A
∂tk
(a1, a2, . . . , an)
≡lim
ϵ→0
A(a1, . . . , ak + ϵ, . . . , an) −A(a1, . . . , ak, . . . , an)
ϵ
.
(12.10)
As immediate consequences of this deﬁnition, we list the following useful
relations:
∂
∂tk
(fA) = ∂f
∂tk
A + f ∂A
∂tk
,
∂
∂tk
(A · B) = ∂A
∂tk
· B + A · ∂B
∂tk
,
(12.11)
∂
∂tk
(A × B) = ∂A
∂tk
× B + A × ∂B
∂tk
.

12.2 Time Derivative of Vectors
351
These relations can be used to calculate the derivatives of vectors when written
in terms of unit vectors, keeping in mind that the derivative of a unit vector
is not necessarily zero! Only Cartesian unit vectors are constant vectors, and
only Cartesian
unit vectors are
constant.
for purposes of diﬀerentiation, it is convenient to write vectors in terms of
these unit vectors, perform the derivative operation, and then substitute for
ˆex, ˆey, and ˆez in terms of other—spherical or cylindrical—unit vectors.
Example 12.2.1. A vector whose magnitude is constant is always perpendicular
to its derivative. This can be easily proved as follows:
A · A = const. ⇒
∂
∂tk
(A · A) =
∂
∂tk
(const.) = 0.
On the other hand, the LHS can be evaluated using the second relation in Equation
(12.11). This gives
∂
∂tk (A · A) = ∂A
∂tk · A + A · ∂A
∂tk = 2A · ∂A
∂tk .
These two equations together imply that A and (∂/∂tk)(A) are perpendicular to
one another.
■
An important consequence of the example above is that
Box 12.2.1. A unit vector is always perpendicular to its derivative.
Example 12.2.2. Newton’s second law for a collection of particles leads directly
to the corresponding law for rotational motion. Diﬀerentiating the total angular
momentum
L =
N

k=1
rk × pk,
with respect to time and using the second law, Fk = dpk/dt, for the kth particle,
we get
dL
dt =
N

k=1
d
dt(rk × pk) =
N

k=1
(˙rk × pk + rk × ˙pk) =
N

k=1
(0 + rk × Fk) ≡T,
where an overdot indicates the derivative with respect to time and in the last line
we used the deﬁnition of torque and the fact that velocity ˙rk and momentum pk
have the same direction.
■
As a special case of the example above, we obtain the law of angular
momentum conservation:
angular
momentum
conservation
Box 12.2.2. When the total torque on a system of particles vanishes, the
total angular momentum will be a constant of motion. This means that
its components in a Cartesian coordinate system are constant.
Since the unit vectors in other coordinate systems are not, in general, constant,
a constant vector has variable components in these systems.

352
Vectors and Derivatives
12.2.1
Equations of Motion in a Central Force Field
When one discusses the central-force problems in mechanics, for instance in
the study of planetary motion, one uses spherical coordinates to locate the
moving object. Thus, the position vector of the object, say a planet, is given
in terms of spherical unit vectors. Newton’s second law, on the other hand,
requires a knowledge of the second time-derivative of the position vector.
In this subsection we ﬁnd the second derivative of the position vector of
a moving point particle P with respect to time in spherical coordinates. The
coordinates (r, θ, ϕ) of P are clearly functions of time.
First we calculate
velocity and write it in terms of the spherical unit vectors
v = dr
dt = d
dt(r) = d
dt(rˆer) = ˆer
dr
dt + rdˆer
dt .
We thus have to ﬁnd the time-derivative of the unit vector ˆer. The most
straightforward way of taking such a derivative is to use the chain rule:
dˆer
dt = ∂ˆer
∂r
dr
dt + ∂ˆer
∂θ
dθ
dt + ∂ˆer
∂ϕ
dϕ
dt = ˙θ∂ˆer
∂θ + ˙ϕ∂ˆer
∂ϕ ,
where we have used the fact that the spherical unit vectors are independent
of r [see Equation (1.39)]. We now evaluate the partial derivatives using (1.39)
and noting that the Cartesian unit vectors are constant:
∂ˆer
∂θ = ˆex
∂
∂θ (sin θ cos ϕ) + ˆey
∂
∂θ (sin θ sin ϕ) + ˆez
∂
∂θ (cos θ)
= ˆex cos θ cos ϕ + ˆey cos θ sin ϕ −ˆez sin θ.
(12.12)
We are interested in writing all vectors in terms of spherical coordinates. A
straightforward way is to substitute for the above Cartesian unit vectors, their
expressions in terms of spherical unit vectors. We can easily calculate such
expressions using the method introduced at the end of Chapter 1. We leave
the details for the reader and merely state the results:
ˆex = ˆer sin θ cos ϕ + ˆeθ cos θ cos ϕ −ˆeϕ sin ϕ,
ˆey = ˆer sin θ sin ϕ + ˆeθ cos θ sin ϕ + ˆeϕ cos ϕ,
(12.13)
ˆez = ˆer cos θ −ˆeθ sin θ.
Substituting these expressions in the previous equation, we get
∂ˆer
∂θ = (ˆer sin θ cos ϕ + ˆeθ cos θ cos ϕ −ˆeϕ sin ϕ) cos θ cos ϕ
+ (ˆer sin θ sin ϕ + ˆeθ cos θ sin ϕ + ˆeϕ cos ϕ) cos θ sin ϕ
−(ˆer cos θ −ˆeθ sin θ) sin θ,
which simpliﬁes to
∂ˆer
∂θ = ˆeθ.
(12.14)

12.2 Time Derivative of Vectors
353
We could have immediately obtained this result by comparing Equation (12.12)
with the expression for ˆeθ in Equation (1.39). The other partial derivative is
obtained the same way:
∂ˆer
∂ϕ = ˆex
∂
∂ϕ (sin θ cos ϕ) + ˆey
∂
∂ϕ (sin θ sin ϕ) + ˆez
∂
∂ϕ (cos θ)
= −ˆex sin θ sin ϕ + ˆey sin θ cos ϕ
= −(ˆer sin θ cos ϕ + ˆeθ cos θ cos ϕ −ˆeϕ sin ϕ) sin θ sin ϕ
+ (ˆer sin θ sin ϕ + ˆeθ cos θ sin ϕ + ˆeϕ cos ϕ) sin θ cos ϕ
= ˆeϕ sin θ.
(12.15)
Substituting this and Equation (12.14) in the expression for velocity, we obtain
components of
velocity in
spherical
coordinates
v = ˆer ˙r + r

˙θ ∂ˆer
∂θ + ˙ϕ∂ˆer
∂ϕ

= ˆer ˙r + ˆeθr ˙θ + ˆeϕr ˙ϕ sin θ.
(12.16)
To write the equations of motion, we need to calculate the acceleration
which involves the diﬀerentiation of other unit vectors. The procedure out-
lined for ˆer can be used to obtain the partial derivatives of the other unit vec-
tors. We collect the result of such calculations, including Equations (12.14)
and (12.15) in the following:
∂ˆer
∂r = 0,
∂ˆer
∂θ = ˆeθ,
∂ˆer
∂ϕ = ˆeϕ sin θ,
∂ˆeθ
∂r = 0,
∂ˆeθ
∂θ = −ˆer,
∂ˆeθ
∂ϕ = ˆeϕ cos θ,
(12.17)
∂ˆeϕ
∂r = 0,
∂ˆeϕ
∂θ = 0,
∂ˆeϕ
∂ϕ = −ˆer sin θ −ˆeθ cos θ.
Similarly the time-derivatives of the unit vectors are given as follows:
dˆer
dt = ˙θˆeθ + ˙ϕ sin θˆeϕ,
dˆeθ
dt = −˙θˆer + ˙ϕ cos θˆeϕ,
(12.18)
dˆeϕ
dt = −˙ϕ sin θˆer −˙ϕ cos θˆeθ.
Diﬀerentiating Equation (12.16) with respect to t, inserting (12.18) in the
result, and collecting the components, we get
components of
acceleration in
spherical
coordinates
d2r
dt2 = dv
dt = ˆer

¨r −r ˙θ2 −r ˙ϕ2 sin2 θ

+ ˆeθ

˙r ˙θ + d
dt(r ˙θ) −r ˙ϕ2 sin θ cos θ

(12.19)
+ ˆeϕ

˙r ˙ϕ sin θ + r ˙θ ˙ϕ cos θ + d
dt(r ˙ϕ sin θ)

.

354
Vectors and Derivatives
One can use these expressions to write Newton’s second law in spherical
coordinates.
Now suppose that a particle (a planet) is under the inﬂuence of a central
force, i.e., a force that always points toward, or away from, an origin (the
Sun), and has a magnitude that is a function of the distance between the
particle and the origin. This means that, in spherical coordinates, the force
is of the form F = ˆerF(r). The second law of motion now yields
md2r
dt2 = ˆerF(r) ⇒d2r
dt2 = ˆer
F(r)
m
≡ˆerf(r)
which, together with Equation (12.19), gives
central-force
problem in
spherical
coordinates
¨r −r ˙θ2 −r ˙ϕ2 sin2 θ = f(r),
˙r ˙θ + d
dt(r ˙θ) −r ˙ϕ2 sin θ cos θ = 0,
(12.20)
˙r ˙ϕ sin θ + r ˙θ ˙ϕ cos θ + d
dt(r ˙ϕ sin θ) = 0.
These equations are the starting point of the study of planetary motion.
We shall not pursue their solution at this point, but consider some of their
general properties, using angular momentum conservation. Since the force
has only an ˆer component, its torque vanishes:
angular
momentum is
conserved in
motions caused by
central forces.
T = r × F = r ˆer × (F(r)ˆer) = rF(r)ˆer × ˆer = 0.
Therefore, by Box 12.2.2, the angular momentum of the particle relative to
the origin is a constant vector. Equation (12.16) now yields
L = r × (mv) = mr ˆer ×

ˆer ˙r + ˆeθr ˙θ + ˆeϕr ˙ϕ sin θ

= mr2 ˆer × (ˆeθ ˙θ + ˆeϕ ˙ϕ sin θ) = mr2(ˆeϕ ˙θ −ˆeθ ˙ϕ sin θ)
= mr2 ˙θ(−ˆex sin ϕ + ˆey cos ϕ)
−mr2 ˙ϕ sin θ(ˆex cos θ cos ϕ + ˆey cos θ sin ϕ −ˆez sin θ)
= Lxˆex + Lyˆey + Lzˆez,
where Lx, Ly, and Lz are the constant Cartesian components of angular
momentum and m is the mass of the particle. Equating the components of
this vectorial relation gives
Lx = −mr2( ˙θ sin ϕ + ˙ϕ sin θ cos θ cos ϕ),
Ly = mr2( ˙θ cos ϕ −˙ϕ sin θ cos θ sin ϕ),
(12.21)
Lz = mr2 ˙ϕ sin2 θ.
The last equation gives
˙ϕ =
Lz
mr2 sin2 θ.
(12.22)

12.3 The Gradient
355
From all of these relations, we obtain
L2 = L2
x + L2
y + L2
z = m2r4 ˙θ2 +
L2
z
sin2 θ .
(12.23)
Now suppose that we choose our coordinate axes so that initially, i.e., at
t = 0, both the position and the velocity vectors of the particle lie in the xy-
plane. Since L is perpendicular to both r and v, it must be initially entirely
in the z-direction. Conservation of angular momentum implies that L will
always be in the z-direction. In particular, L2 = L2
z. Substituting this in
Equation (12.23) yields
L2 = m2r4 ˙θ2 +
L2
sin2 θ ⇒0 = m2r4 ˙θ2 +
L2
sin2 θ −L2
or 0 = m2r4 ˙θ2 + L2 cot2 θ.
Neither of the two terms on the RHS of this
equation is negative. Thus, for their sum to be zero, each term must be zero.
It follows that
m2r4 ˙θ2 = 0 ⇒˙θ = 0 ⇒θ = const.,
L2 cot2 θ = 0 ⇒cot2 θ = 0 ⇒θ = π/2,
assuming that r ̸= 0 and L ̸= 0. These relations hold for all times. Thus,
proof that planets
move in a plane
the particle is conﬁned to a plane, our xy-plane, for eternity! This is why the
planets do not wobble “up and down” out of their orbital planes.5
If we substitute π/2 for θ and use (12.22) for ˙ϕ in Equation (12.20), then
the second and third relations are satisﬁed identically, and the ﬁrst relation
becomes
¨r −
L2
m2r3 = f(r)
(12.24)
which is a single diﬀerential equation in one variable. The general problem
of a particle’s motion in three dimensions has reduced to a one-dimensional
problem.
12.3
The Gradient
Analysis of vectors deals with the derivatives and integrals of vector ﬁelds.
Because of its simplicity, we shall work in a Cartesian coordinate system at
the beginning, and later generalize to other coordinates.
In many situations arising in physics, rates of change of certain scalar
functions with distance are of importance. For instance, the way potential
energy changes as we move in space is directly related to the force producing
the potential energy. Similarly, the rate of change—derivative—of the elec-
trostatic potential with respect to distance gives the electrostatic ﬁeld. The
concept of gradient makes precise the vague notion of a derivative with respect
to distance.
5Actually, the planets, due to the inﬂuence of other planets, do wobble out of their
orbits. But this is a very small eﬀect.

356
Vectors and Derivatives
y
x
f (x)
Δx
Δf
x0
x0 + Δx
Figure 12.7: “Gradient” or diﬀerentiation with respect to distance in one dimension.
Let us analyze the notion of diﬀerentiation with respect to distance, start-
ing with one variable. In Figure 12.7, a function f(x) has an increment, Δf,
notion of ordinary
derivative
reexamined
corresponding to a change Δx in x. If Δx is small enough, we can write
Δf ≈
 df
dx

x=x0
Δx.
This shows that (df/dx)x=x0 is a measure of how fast the function f is chang-
ing at the point x0.
With one variable, there is no ambiguity in deﬁning the derivative, because
there is only one line along which we can change x, the only coordinate. With
two or more variables, the situation is completely diﬀerent, as illustrated
in Figure 12.8.
A point P0 = (x0, y0) in the xy-plane is shown with the
corresponding value of the function, f(x0, y0) = z0.
Out of the inﬁnitude of
notion of gradient
analyzed
points that are close to P0 and cause a change in the function, only three are
shown. These indicate how the change in f(x, y) depends on the direction
in which the neighboring point is located in relation to P0. For example, if
we move in the direction P0P1, there is very little change in f(x, y), but if
we move in the direction P0P2, we notice more change in the function, and
if we move in the direction of P0P3, the change seems to be maximum. This
maximum change, and the direction associated with it, is called the gradient.
O
x
y
z
P0
P1
P2
P3
z0
z1
z2
z3
Figure 12.8: Gradient or diﬀerentiation with respect to distance is shown in two di-
mensions. The gradient is a vector in the xy-plane. Do not think of the surface as a
variation in height! It could represent, for instance, the temperature at various points
of the xy-plane.

12.3 The Gradient
357
Let us use dr to denote the inﬁnitesimal displacement vector6 connecting
P0 to a neighboring point in the xy-plane. If f(x, y) is diﬀerentiable, Equation
(2.12) gives
df =
∂f
∂x

P0
dx +
∂f
∂y

P0
dy,
where dx and dy are the components of the displacement from P0 and df is
(approximately) the change in f corresponding to the increments dx and dy.
We can rewrite this equation as
df = (∇f)P0 · dr = |∇f||dr| cos θ,
(12.25)
where, by deﬁnition,
gradient in two
dimensions
(∇f)P0 ≡
∂f
∂x, ∂f
∂y

P0
(12.26)
is a vector in the xy-plane and θ is the angle between this vector and dr. It
is clear that df will be maximum when cos θ = 1, that is, when dr is in the
direction of ∇f. We conclude, therefore, that ∇f gives the direction along
which f changes most rapidly. The vector in Equation (12.26) is the gradient
of f at P0.
The notion of gradient can be generalized to three variables although it is
harder to visualize than the two-variable case. In three dimensions we deal
with a function f(x, y, z)—which cannot be plotted as in Figure 12.8—and
ask which dr = ⟨dx, dy, dz⟩maximizes the change in f.
Once again, the
three-dimensional version of Equation (12.25) shows that dr and
gradient in three
dimensions
∇f ≡
∂f
∂x, ∂f
∂y , ∂f
∂z

(12.27)
should be in the same direction for df to have a maximum.
Deﬁnition 12.3.1. The gradient of a function f(x, y, z) is deﬁned as
∇f ≡ˆex
∂f
∂x + ˆey
∂f
∂y + ˆez
∂f
∂z .
For the same small displacement |Δr|, the change in f is maximum when Δr
is in the direction of ∇f.
Example 12.3.1. As an example, let us ﬁnd the gradient of the function
V (x, y, z) = f(r) = f
	
x2 + y2 + z2

(which depends on r alone) at a point P with Cartesian coordinates (x, y, z). Using
the chain rule, we have
6A better notation is Δr.
However, since there is no diﬀerence between diﬀerential
and increment of an independent variable, and since eventually we will be interested in
diﬀerentials, we use the latter notation.

358
Vectors and Derivatives
∇V = ˆex ∂V
∂x + ˆey ∂V
∂y + ˆez ∂V
∂z =
∂V
∂x , ∂V
∂y , ∂V
∂z

=

f ′(r) ∂r
∂x, f ′(r) ∂r
∂y , f ′(r)∂r
∂z

= f ′(r)
C x
r , y
r , z
r
D
= f ′(r)
r
⟨x, y, z⟩= f ′(r)r
r .
The last equality shows that, for functions that depend on r alone, the gradient is
proportional to the position vector of the point P, i.e., it is radial.
■
Given a scalar function f(x, y, z), we can consider surfaces on which this
function maintains a constant value. If that constant value is C, the surface
will be described by f(x, y, z) = C. One can, in principle, solve for z as a
function of x and y to ﬁnd the explicit dependence of the function. However,
we are interested in the implicit dependence given above. Now consider two
points P1 and P2 on the surface with coordinates (x, y, z) and (x + Δx, y +
Δy, z + Δz), respectively. We have
f(x, y, z) = f(x + Δx, y + Δy, z + Δz) ⇒f(x, y, z) = f(x, y, z) + Δf
or 0 = Δf ≈∂f
∂xΔx + ∂f
∂y Δy + ∂f
∂z Δz, if the increments of coordinates are
small.
This relation shows that ∇f is perpendicular to the displacement
from P1 to P2. The same argument applies to a curve g(x, y) = C; i.e., the
two-dimensional gradient is perpendicular to the displacement from P1 to P2,
both being points on the curve. Since P1 and P2 are completely arbitrary, we
conclude that
Theorem 12.3.2. The gradient ∇f is perpendicular to all surfaces f(x, y, z) =
C for diﬀerent C’s. Similarly, ∇g is perpendicular to all curves g(x, y) = C.
For example, as we shall see later, the electrostatic ﬁeld is the gradient of
electrostatic ﬁeld
is perpendicular to
surfaces of
conductors
the electrostatic potential. Therefore, the electrostatic ﬁeld is perpendicular
to surfaces of constant potential such as conductors.
Example 12.3.3. The perpendicularity property of the gradient can be used to
ﬁnd the equation of the tangent plane to a surface z = g(x, y) at a point P with
coordinates (x0, y0, z0). This surface can be written as
f(x, y, z) ≡z −g(x, y) = 0.
Then, the normal to the surface at P—which is the same as the normal to the
tangent plane at P—is the gradient of f at P:
 
∇f
!
P =
 ∂f
∂x, ∂f
∂y , ∂f
∂z

P
=

−∂g
∂x, −∂g
∂y , 1

P
.
A point of the tangent plane at P is completely determined by the property that
derivation of the
equation of a
plane tangent to a
surface
its displacement vector Δr from P should be perpendicular to the gradient at P (see
Figure 12.9). If we denote the position vector of P by r0 and that of the point on
the plane by r = ⟨x, y, z⟩, then the equation of the tangent plane is given by
(r −r0) · (∇f)P = 0 ⇒−(x −x0)
 ∂g
∂x

P
−(y −y0)
 ∂g
∂y

P
+ (z −z0) = 0

12.3 The Gradient
359
P
(x, y, z)
r
r0
O
Figure 12.9: The plane tangent to the surface z = g(x, y) at P.
or
z −z0 = (x −x0)
 ∂g
∂x

P
+ (y −y0)
 ∂g
∂y

P
■
It is convenient to introduce a diﬀerentiation operator which we shall use
later.
the del operator
Deﬁnition 12.3.2. The symbol ∇can be thought of as a vector operator,
called del or nabla, whose components are ∂/∂x, ∂/∂y, and ∂/∂z. Thus, we
can write
∇= ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z .
(12.28)
This vector operator ∇operates on diﬀerentiable functions and produces vec-
tor ﬁelds.
12.3.1
Gradient and Extremum Problems
The gradient is very nicely used to ﬁnd the maxima and minima of functions
of several variables. A function f(x) of n variables x = (x1, x2, . . . , xn) has a
local extremum (maximum or minimum) at a point a if its diﬀerential vanishes
at that point for arbitrary dx:
df = ∂f
∂x1




a
dx1 + ∂f
∂x2




a
dx2 + · · · + ∂f
∂xn




a
dxn ≡
 
∇f(a)
!
· dx = 0
where
∇f ≡
 ∂f
∂x1
, ∂f
∂x2
, . . . , ∂f
∂xn

and
dx ≡⟨dx1, dx2, . . . , dxn⟩.
If the dot product of ∇f(a) and dx is to vanish for arbitrary dx, then ∇f(a)
must be zero. Thus for f to have an extremum at a, we must have
∇f(a) = 0
or
∂f
∂xi




a
= 0,
i = 1, 2, . . ., n.
(12.29)

360
Vectors and Derivatives
This is the generalization to n variables the familiar condition known from
calculus.
In many situations, there are auxiliary conditions or constraints imposed
on the independent variables. For example, let P1, Q, and P2 be three points
in space, with P1 and P2 ﬁxed but Q being allowed to move. Consider the
path P1QP2 consisting of straight line segments P1Q and QP2. What choice
of Q gives the shortest path? If we denote the coordinates of Q by (x, y, z)
and those of P1 and P2 with obvious subscripts, then we have to ﬁnd the
extremum of
f(x, y, z) =
	
(x −x1)2 + (y −y1)2 + (z −z1)2
+
	
(x −x2)2 + (y −y2)2 + (z −z2)2.
So we set partial derivatives equal to zero and solve for (x, y, z). The answer,
as expected, turns out to be the path for which Q lies on the line segment
P1P2 between P1 and P2.
Now suppose we demand that Q lie on a sphere of radius a centered at the
origin. Then the problem becomes extremizing f(x, y, z) with the constraint
condition that
g(x, y, z) ≡x2 + y2 + z2 −a2 = 0.
To solve this problem, we could solve for one of the variables of the constraint
equation in terms of the other two, substitute in f(x, y, z), and solve the re-
sulting two-variable problem. But there is a much more elegant way involving
gradients, which we discuss now.
Suppose that we want to ﬁnd the extremum of a function f(x) of n vari-
ables x = (x1, x2, . . . , xn) subject to the condition that x must lie on the
hypersurface g(x) = 0. We cannot set ∇f equal to zero because dx is no
longer arbitrary.
With constraint, dx is conﬁned to the surface g(x) = 0. Now, the only
n-dimensional vector which has a vanishing dot product with any dx on the
constrained surface is (a multiple of) the normal to the surface. Therefore, if
(∇f)·dx is to be zero for dx lying on the surface, then ∇f must be a multiple
of the normal to the surface g(x) = 0. But this normal is nothing but ∇g.
Therefore, if f is to have an extremum subject to the constraint g(x) = 0,
then it must obey the following equation
∇f = −λ∇g
or
∇f + λ∇g = 0,
where λ is an arbitrary constant called the Lagrange multiplier.
This
equation shows that to ﬁnd the extremum of the function f with constraint
g(x) = 0, one can deﬁne the function F of n + 1 variables
Lagrange
multipliers
F(x1, x2, . . . , xn; λ) ≡f(x1, x2, . . . , xn) + λg(x1, x2, . . . , xn),

12.3 The Gradient
361
and extremize it without constraint. Then we have
∂F
∂xi
= ∂f
∂xi
+ λ ∂g
∂xi
= 0,
i = 1, 2, . . ., n,
∂F
∂λ = g(x1, x2, . . . , xn) = 0.
(12.30)
The last equation is just the constraint condition, but it comes out conve-
niently as one of the extremal equations of F.
Example 12.3.4. A rectangular box is to be made out of a given amount A of
material to have the largest volume. What dimensions should the box have? Here
f(x, y, z) = xyz, the volume, and g(x, y, z) = 2xy + 2xz + 2yz −A. Setting the
components of the gradient of
F(x, y, z; λ) = xyz + 2λ(xy + xz + yz −A/2)
equal to zero yields four equations
yz + 2λ(y + z) = 0,
xz + 2λ(x + z) = 0,
xy + 2λ(x + y) = 0,
2(xy + xz + yz) −A = 0.
Multiplying the ﬁrst equation by x and the second equation by y and subtracting
yields
2λx(y + z) −2λy(x + z) = 0,
or
x = y.
Similarly, from the second and third equations we get y = z. So, the box should be
a cube. The last equation then gives
6x2 −A = 0,
or
x = y = z =

A
6 .
Substituting this in any of the above equations involving λ yields λ = −1
4
	
A/6. ■
The extremal problems may have several constraint equations such as
gj(x1, x2, . . . , xn) ≡gj(x) = 0,
j = 1, 2, . . ., m.
(12.31)
We can “eliminate” the ﬁrst constraint by replacing f(x1, x2, . . . , xn) with
F1(x; λ1) ≡f(x) + λ1g1(x),
where F1 has only m −1 constraint equations. Now eliminate the second
constraint by deﬁning
F2(x; λ1, λ2) ≡F1(x; λ1) + λ2g2(x) = f(x) + λ1g1(x) + λ2g2(x).
Continuing, we can eliminate all constraints by deﬁning
F(x; λ1, λ2, . . . , λm) ≡f(x) +
m

j=1
λjgj(x),
(12.32)
whose unconstrained extremization yields the extremal equations.

362
Vectors and Derivatives
12.4
Problems
12.1. Find directly the solid angle subtended by a disk of radius a at a point
P on its perpendicular axis located a distance b from the center.
12.2. A closed curve ρ = 3a + a cos ϕ in cylindrical coordinates bounds a
region in the xy-plane. Find the solid angle subtended by this region at a
point P on the z-axis a distance 2a above the xy-plane.
12.3. Derive Equation (12.11).
12.4. Show that when a moving particle is conﬁned to a circle, its velocity is
always perpendicular to its radius. If, furthermore, the speed of the particle
is constant, then its acceleration is radial.
12.5. Derive Equations (12.17) and (12.18).
12.6. The vectors a and b are given by
a = uˆex + vˆey,
b = vˆex −uˆey.
(a) Write ˆea and ˆeb in terms of Cartesian unit vectors.
(b) Find the four vectors ∂ˆea/∂u, ∂ˆea/∂v, ∂ˆeb/∂u, and ∂ˆeb/∂v in terms of
Cartesian unit vectors.
(c) Express ˆex and ˆey in terms of ˆea and ˆeb.
(d) Express the four vectors ∂ˆea/∂u, ∂ˆea/∂v, ∂ˆeb/∂u, and ∂ˆeb/∂v in terms
of ˆea and ˆeb.
(e) If u and v are functions of time, ﬁnd dˆea/dt and dˆeb/dt in terms of ˆea
and ˆeb.
12.7. Derive Equation (12.19).
12.8. Derive Equation (12.23).
12.9. Show that (12.22) and the assumption θ = π/2 solve the last two
equations of (12.20) and reduce the ﬁrst one to (12.24).
12.10. (a) Obtain the time derivatives of the cylindrical unit vectors:
dˆeρ
dt = ˙ϕˆeϕ,
dˆeϕ
dt = −˙ϕˆeρ,
dˆez
dt = 0.
(b) Use the result of (a) to show that if A is a vector written in terms of
cylindrical unit vectors, then
dA
dt =
dAρ
dt −Aϕ ˙ϕ

ˆeρ +

Aρ ˙ϕ + dAϕ
dt

ˆeϕ + dAz
dt ˆez.
12.11. A surface is given by x2
a2 + y2
4a2 + z2
2a2 = 1. Find the unit normal to
the surface and the equation of the tangent plane at (a/2, a, a).

12.4 Problems
363
12.12. The potential of a certain charge distribution is given by
Φ(x, y, z) = z2 + y2
4 + x2
9 .
(a) Find the electric ﬁeld E = −∇Φ at (3/
√
2, 1, 1/2) and show that it is
normal to the surface
z2 + y2
4 + x2
9 = 1.
(b) Show that the electric ﬁeld is normal at every point of this surface.
(c) Show that the electric ﬁeld is normal at every point of the surface obtained
by replacing 1 on the RHS of the last equation by any arbitrary constant.
12.13. Show that ∇(fg) = (∇f)g + f(∇g) for any two (diﬀerentiable) func-
tions f and g of (x, y, z).
12.14. Consider the plane ax + by + cz = d and a point P = (x0, y0, z0)
not lying in the plane. Use Lagrange multipliers to show that the parametric
equation of the line passing through P that gives the minimum distance to
the plane is
r = r0 + tn,
where
r = ⟨x, y, z⟩,
r0 = ⟨x0, y0, z0⟩,
n = ⟨a, b, c⟩.
(12.33)
From this deduce that the distance from P to the plane is
|d −ax0 −by0 −cz0|
√
a2 + b2 + c2
.
Hint: Take the dot product of (12.33) with n and use the fact that n · r = d
when the tip of r is in the plane.
12.15. Consider the sphere (x −a)2 + (y −b)2 + (z −c)2 = d2 and a point
P = (x0, y0, z0) not lying on the sphere. Use Lagrange multipliers to show that
the shortest line segment connecting P to the sphere is that which extends
through the center of the sphere.
12.16. For a vector A(r, t) that is a function of position and time, show that
dA = (dr · ∇)A + ∂A
∂t dt.
12.17. Find the gradient of
u(x, y, z, x′, y′, z′) ≡u(r −r′) = |r −r′|m,
ﬁrst with respect to the components of r and then with respect to the com-
ponents of r′, and write the answer completely in terms of r and r′. What is
the answer when m = −1?

Chapter 13
Flux and Divergence
A vector ﬁeld is a function with direction, and because of this directional
property, many new kinds of diﬀerentiation and integration can be performed
on it. For instance, a vector ﬁeld can be made to pierce a surface or an element
thereof, and as it pierces that surface its variation from point to point can be
monitored. This leads to one kind of diﬀerentiation and integration which we
discuss next. The integration leads to the concept of the ﬂux of a vector ﬁeld,
and the associated diﬀerentiation to the notion of divergence.
13.1
Flux of a Vector Field
The paradigm of the concept of ﬂux is that of the velocity ﬁeld of a ﬂuid (see
Figure 13.1). A small ring of area Δa is situated in the ﬂow. How much ﬂuid
is passing through the ring per unit time? It is clear that the answer depends
on the density of the ﬂuid,1 the speed of the ﬂuid, the size of the area Δa, and
also on the relative orientation of the direction of the ﬂow and the unit normal
to the area, denoted by ˆen. A little contemplation reveals that the amount of
ﬂuid of constant unit density passing through Δa is proportional to2
ﬂux of ﬂow
velocity through a
small area
Δφ = v · ˆenΔa ≡v · Δa,
(13.1)
where Δφ is called the ﬂux of v through Δa, and Δa is deﬁned to be ˆenΔa.
If the ring is replaced by a large surface S then we have to divide the surface
into small areas—not necessarily in the shape of a ring—and sum up the con-
tribution of each area to the ﬂux. In the limit of smaller and smaller areas
and larger and larger numbers of such areas, we obtain an integral:
total ﬂux of ﬂow
velocity through a
large area
φ = lim
Δa→0
N→∞
N

i=1
vi · ˆeniΔai ≡lim
Δa→0
N→∞
N

i=1
vi · Δai =
# #
S
v · da,
(13.2)
where φ is the total ﬂux through S.
1For simplicity we assume that density is constant and we take it to be 1.
2We shall come back to a rigorous derivation of the ﬂow of a substance through a small
loop later (see the discussion after Theorem 13.2.2).

366
Flux and Divergence
ˆ e n
v
da
Figure 13.1: Flux of velocity vector through a small area Δa.
There is an arbitrariness in the direction of the unit vector normal to an
element of area, because for any unit normal, there is another which points in
the opposite direction. The ﬂux for these two unit normals will have opposite
signs. This may appear as if one could arbitrarily choose every one of the
unit normals ˆeni in the sum (13.2) to have either one of the two opposite
orientations, leading to an arbitrary result for the integral. This is not the case,
the total ﬂux can
be determined
only up to a sign.
because the direction of the unit normal to an element of area is determined
by the neighboring unit normals and the requirement of continuity. So, once
the choice is made between the two possibilities of the unit normal for one
element of area of the surface S, say the ﬁrst one ˆen1, the second one can
diﬀer only slightly from ˆen1—in particular, it cannot be of opposite sign. The
third one should point in almost the same direction as the second one, and
so on. This requirement of continuity will uniquely determine the remaining
unit normals. However, the initial choice remains arbitrary, and since the
two orientations of the initial choice diﬀer by a sign, the two total ﬂuxes
corresponding to these two orientations will also diﬀer by a sign. We shall see
shortly, however, that for closed surfaces, such an arbitrariness in sign can be
overcome by convention.
The discussion above works for orientable surfaces. This means that on
orientable surface
any closed loop entirely on the surface, the direction of a normal vector will
not change when one displaces it on the loop continuously one complete orbit.
It is clear that the lateral surface of a cylinder is orientable.
A cylinder is obtained by glueing the two edges of a rectangle. Now take
the same rectangle and twist one of the (smaller) edges before glueing it to the
opposite edge. The result—which the reader may want to construct—is a very
famous mathematical surface called the M¨obius band. A M¨obius band is not
M¨obius band
orientable, because if one starts at the midpoint of the glued edges and moves
perpendicular to it along the large circle (length of the original rectangle),
then a unit normal displaced continuously and completely along the circle
will be ﬂipped.3 In this book we shall never encounter nonorientable surfaces.
3The reader is urged to perform this surprising experiment using a (portion of a) tooth-
pick as a unit normal.

13.1 Flux of a Vector Field
367
Example 13.1.1. Consider the ﬂow of a river and assume that the velocity of the
water is given by
v = v0

1 −4x2
w2

ˆez,
where x is the distance from the midpoint of the river and w is the width of the
river. Let us ﬁnd the ﬂux of the velocity, assuming that the cross section of the river
is a rectangle with depth equal to h, as shown in Figure 13.2.
The normal to the area da is perpendicular to the xy-plane and is in the same
direction as the velocity. Thus, we have v · da = v da = v dx dy, and
φ =
# #
S
v dx dy =
# h/2
−h/2
dy
# w/2
−w/2
v0

1 −4x2
w2

dx
= hv0
# w/2
−w/2

1 −4x2
w2

dx = hv0 (w −1
3 w) = 2
3Av0,
where S is the cross section of the river and A is its area.
■
The concept of ﬂux, although indicative of a ﬂow, is not limited to the
velocity vector ﬁeld. We can deﬁne the ﬂux of any vector ﬁeld A in exactly
ﬂux can be deﬁned
not only for
velocity, but for
any vector ﬁeld.
the same way:
φ =
# #
S
A · da.
(13.3)
Whether such a deﬁnition is useful or not should be determined by experi-
ment. It turns out that the ﬂux of every physically relevant vector ﬁeld is
not only useful, but essential for the theoretical—as well as experimental—
investigation of that ﬁeld.
For example, the ﬂux of a gravitational ﬁeld
through a closed surface is related to the amount of mass in the volume
enclosed in the surface. Similarly, the rate of change of the ﬂux of a magnetic
ﬁeld through a surface gives the electric ﬁeld produced at the boundary of the
surface.
ˆ e n
O
x
y
da = dx dy 
Figure 13.2: The river with its cross section.

368
Flux and Divergence
θ
θ
E
ˆe n
dϕ
ϕ
a
x
y
d
r
q
ρ
Figure 13.3: The ﬂux of the electric ﬁeld through a circle. The normal unit vector ˆen
could be chosen to be either up or down. We choose (quite arbitrarily) the up direction
to make the ﬂux positive for positive q.
Example 13.1.2. Consider the ﬂux of the electric ﬁeld of a point charge located
at a distance d from the center of a circle of radius a as shown in Figure 13.3. The
element of ﬂux is given by
E · da = |E| cos θda = |E| cos θρdρdϕ = kq
r2
d
r ρdρdϕ =
kqd
(d2 + ρ2)3/2 ρdρdϕ,
where ˆen is chosen to point up. The polar coordinates (ρ, ϕ) are used to specify a
point in the plane of the circle at which point the element of area is ρ dρ dϕ. To ﬁnd
the total ﬂux, we integrate the last expression above:
φ =
# #
S
kqd
(d2 + ρ2)3/2 ρ dρ dϕ = kqd
# 2π
0
dϕ
# a
0
ρ dρ
(d2 + ρ2)3/2
= 2πkqd
3
−(d2 + ρ2)−1/2 


a
0
4
= 2πkq

1 −
d
√
d2 + a2

.
Note that since d represents a distance, as opposed to a coordinate, it is always
positive and d =
√
d2 = |d|.
■
It is often necessary to calculate the ﬂux of a vector ﬁeld through a closed
surface bounding a volume. Intuitively, such a ﬂux gives a measure of the
for a closed
surface, one can
uniquely
determine the
direction of
normal at each
point of the
surface.
strength of the source of the vector ﬁeld in the volume. For instance, the ﬂux
of the velocity ﬁeld of water through a closed surface bounding a fountain
measures the rate of the water output of the fountain. If the surface does
not enclose the fountain, the net ﬂux will be zero because the ﬂux through
one “side” of the closed surface will be positive and that of the other “side”
will be negative with the total ﬂux vanishing. In the case of an electrostatic
ﬁeld, the ﬂux through a closed surface measures the amount of charge in the
volume bounded by that surface. The sign of the ﬂux requires an orientation
of the bounding surface which is equivalent to the assignment of a positive
direction to the unit normal to the surface at each of its points. We agree to
out is positive!
adhere to the convention of Box 12.1.3.4
4Only orientable surfaces can have a well deﬁned orientation. Since we are excluding
nonorientable surfaces from this book, all our surfaces respect Box 12.1.3.

13.1 Flux of a Vector Field
369
Example 13.1.3. Let us consider the ﬂux through a sphere of radius a centered
at the origin of a vector ﬁeld A given by A = kQrmˆer with k a proportionality
constant and Q the strength of the source. Assuming that the outward normal is
considered positive (see Box 12.1.3) the total ﬂux through the sphere is calculated
as
φQ =
# #
S
A · da =
# #
S
kQamˆer · (ˆera2 sin θ dθ dϕ)
= kQ
# 2π
0
dϕ
# π
0
ama2 sin θ dθ = 2πkQam+2
# π
0
sin θ dθ = 4πkQam+2.
It is important to keep in mind that when calculating the ﬂux of a vector ﬁeld, one
remember to
evaluate the
vector ﬁeld at the
surface when
calculating its
ﬂux!
has to evaluate the ﬁeld at the surface. That is why a appears in the integral rather
than r. Notice how the ﬂux depends on the radius of the sphere. If m + 2 > 0, then
the farther away one moves from the origin, the more total ﬂux passes through the
sphere. On the other hand, if m + 2 < 0, although the size of the sphere increases,
and therefore, more area is available for the ﬁeld to cross, the ﬁeld decreases too
rapidly to give enough ﬂux to the large sphere, so the ﬂux decreases. The important
case of m = −2 eliminates the dependence on a: The total ﬂux through spheres of
diﬀerent sizes is constant. This last statement is a special case of the content of the
celebrated Gauss’s law.
■
Historical Notes
Space vectors were conceived as three-dimensional generalizations of complex num-
bers. The primary candidates for such a generalization however turned out to be
quaternions—discovered by Hamilton—which had four components. One could nat-
urally divide a quaternion into its “scalar” component and its vector component,
the latter itself consisting of three components. The product of two quaternions,
being itself a quaternion, can also be divided into scalar and vector parts. It turns
out that the scalar part of the product contains the dot product of the vector parts,
and the vector part of the product contains the cross product of the vector parts.
However, the full product contains some extra terms.
Physicists, on the other hand, were seeking a concept that was more closely
associated with Cartesian coordinates than quaternions were. The ﬁrst step in this
direction was taken by James Clerk Maxwell. Maxwell singled out the scalar and the
vector parts of Hamilton’s quaternion and put the emphasis on these separate parts.
In his celebrated A Treatise on Electricity and Magnetism (1873) he does speak of
quaternions but treats the scalar and the vector parts separately.
Hamilton also developed a calculus of quaternions. In fact, the gradient operator
introduced in Deﬁnition 12.3.2 and its name “nabla” were both Hamilton’s inven-
tion.5 Hamilton showed that if ∇acts on the vector part v of a quaternion, the
result will be a quaternion. Maxwell recognized the scalar part of this quaternion
to be the divergence (to be discussed in the next section) of the vector v, and the
vector part to be the curl (to be discussed in the Section 14.2) of v.
Maxwell often used quaternions as the basic mathematical entity or he at least
made frequent reference to quaternions, perhaps to help his readers. Nevertheless,
his work made it clear that vectors were the real tool for physical thinking and not
just an abbreviated scheme of writing, as some mathematicians maintained. Thus
5He used the word “nabla” because ∇looks like an ancient Hebrew instrument of that
name.

370
Flux and Divergence
by Maxwell’s time a great deal of vector analysis was created by treating the scalar
and vector parts of quaternions separately.
The formal break with quaternions and the inauguration of a new independent
subject, vector analysis, was made independently by Josiah Willard Gibbs and Oliver
Heaviside in the early 1880s.
13.1.1
Flux Through an Arbitrary Surface
It may be useful to have a general formula for calculating the ﬂux through
an arbitrary surface whose equation is given in parametric form in Cartesian
coordinates. Let
x = f(u, v),
y = g(u, v),
z = h(u, v),
(13.4)
be the parametric equation of a surface. When v is held ﬁxed and u is allowed
to vary, a curve is traced on the surface whose inﬁnitesimal displacement can
be written as [see Equation (6.63)]
d⃗l1 = ˆex
∂f
∂u du + ˆey
∂g
∂u du + ˆez
∂h
∂u du.
Similarly inﬁnitesimal displacement along curves of constant u is
d⃗l2 = ˆex
∂f
∂v dv + ˆey
∂g
∂v dv + ˆez
∂h
∂v dv.
The cross product of these two displacements is the element of area of the
surface:
da = d⃗l1 × d⃗l2 = det
⎛
⎜
⎜
⎜
⎜
⎝
ˆex
ˆey
ˆez
∂f
∂u
∂g
∂u
∂h
∂u
∂f
∂v
∂g
∂v
∂h
∂v
⎞
⎟
⎟
⎟
⎟
⎠
dudv ≡det
⎛
⎜
⎜
⎜
⎜
⎝
ˆex
ˆey
ˆez
∂x
∂u
∂y
∂u
∂z
∂u
∂x
∂v
∂y
∂v
∂z
∂v
⎞
⎟
⎟
⎟
⎟
⎠
dudv.
Using this in (13.3) we get
φ =
# #
R
det
⎛
⎜
⎜
⎜
⎜
⎝
Ax
Ay
Az
∂x
∂u
∂y
∂u
∂z
∂u
∂x
∂v
∂y
∂v
∂z
∂v
⎞
⎟
⎟
⎟
⎟
⎠
du dv,
(13.5)
where Ax, Ay, and Az are considered functions of u and v obtained by substi-
tuting (13.4) for their arguments. Equation (13.5) is an integral over a region
R in the uv-plane determined by the range of the variables u and v suﬃcient
to describe the surface S.
The special, but important case, of a surface given by z = f(x, y) deserves
special attention. In this case the parametrization is
x = u,
, y = v,
z = f(u, v)

13.2 Flux Density = Divergence
371
and (13.5) yields
φ =
# #
R
det
⎛
⎜
⎜
⎜
⎜
⎝
Ax
Ay
Az
1
0
∂z
∂u
0
1
∂z
∂v ,
⎞
⎟
⎟
⎟
⎟
⎠
du dv
or, writing (x, y) for (u, v)
φ =
# #
R

−Ax
∂z
∂x −Ay
∂z
∂y + Az

dx dy,
(13.6)
where R is the projection of the surface S onto the xy-plane.
13.2
Flux Density = Divergence
The connection between ﬂux and the strength of the source of a vector ﬁeld
was mentioned above. We now analyze this connection further. The variation
in the strength of the source of a vector ﬁeld is measured by the density of
the source.
For example, the variation in the strength—concentration—of
the source of electrostatic (gravitational) ﬁeld is measured by charge (mass)
density. We expect this variation to inﬂuence the intensity of ﬂux at various
points in space.
13.2.1
Flux Density
Densities are physical quantities treated locally. A local consideration of ﬂux,
therefore, requires the introduction of the notion of ﬂux density:
notion of ﬂux
density and
divergence of a
vector ﬁeld
introduced
Box 13.2.1. Take a small volume around a point P, evaluate the total ﬂux
of a vector ﬁeld through the bounding surface of the volume, and divide
the result by the volume to get the ﬂux density or divergence of the
vector ﬁeld at P.
We denote the ﬂux density by ρφ for the moment. Later we shall introduce
another notation which is more commonly used.
Let us quantify the discussion above for a vector ﬁeld A. Consider a small
rectangular 6 volume ΔV centered at P with coordinates (x, y, z). Let the
sides of the box be Δx, Δy, and Δz as in Figure 13.4. We are interested in
6The rectangular shape of the volume is not a restriction because it will be made smaller
and smaller at the end. In such a limit, any volume can be built from—a large number of—
these small rectangular boxes. Compare this with the rectangular strips used in calculating
the area under a curve.

372
Flux and Divergence
ˆe z
Az
ˆe y
Ay
ˆe x
Ax
(x, y, z)
Δ x
Δy
Δ z
A
Figure 13.4: The ﬂux of the vector ﬁeld A through a closed inﬁnitesimal rectangular
surface.
the net outward7 ﬂux of the vector ﬁeld, A(x, y, z). The six faces of the box
are assumed to be so small that the angle between the normal to each face
and the vector ﬁeld A is constant over the area of the face. Since we are
calculating the outward ﬂux, we must assume that ˆen is always pointing out
of the volume.
The total ﬂux Δφ through the surface can be written as
Δφ = (Δφ1 + Δφ2) + (Δφ3 + Δφ4) + (Δφ5 + Δφ6),
where each pair of parentheses indicates one coordinate axis. For instance,
Δφ1 is the ﬂux through the face having a normal component along the positive
x-axis, Δφ2 is the ﬂux through the face having a normal component along the
negative x-axis, and so on. Let us ﬁrst look at Δφ1, which can be written as
Δφ1 = A1 · ˆen1Δa1
or, since ˆen1 is the same as ˆex,
Δφ1 = A1 · ˆexΔa1 = A1xΔa1.
This requires some explanation. The subscript 1 in A1x indicates the evalu-
ation of the vector ﬁeld at the midpoint8 of the ﬁrst face. The subscript x
in A1x, of course, means the x-component. So, A1x means the x-component
of A evaluated at the midpoint of the ﬁrst face; Δa1 is the area of face 1
which is simply ΔyΔz (see Figure 13.4). The center of the box—point P—
has coordinates (x, y, z) by assumption. Thus, the midpoint of face 1 will
have coordinates (x + Δx/2, y, z). Therefore,
Δφ1 = Ax

x + Δx
2 , y, z

ΔyΔz.
(13.7)
7The choice of outward direction is dictated by Box 12.1.3.
8The restriction to midpoint is only for convenience. Since the area is small, any other
point of the face can be used.

13.2 Flux Density = Divergence
373
The ﬂux density that we are evaluating will be the density at P. Thus,
as a function of the three coordinates, the result will have to be given at the
coordinates of P, namely at (x, y, z). This means that in Equation (13.7),
all quantities must have (x, y, z) as their arguments. This suggests expanding
the function on the RHS of Equation (13.7) as a Taylor series about the point
(x, y, z). Recall from Chapter 10 that
f(x + Δx, y + Δy, z + Δz) =
∞

n=0

i+j+k=n
∂n
ijkf(x, y, z)
i!j!k!
(Δx)i(Δy)j(Δz)k.
We are interested only in the ﬁrst power because the size of the box will
eventually tend to zero. Therefore, we write this in the following abbreviated
form:
f(x + Δx, y + Δy, z + Δz)
= f(x, y, z) + Δx∂f
∂x + Δy ∂f
∂y + Δz ∂f
∂z + · · · ,
(13.8)
where it is understood that all derivatives are evaluated at (x, y, z). Applying
this result to the function on the RHS of Equation (13.7), for which Δy and
Δz are zero, yields
Ax

x + Δx
2 , y, z

= Ax(x, y, z) + Δx
2
∂Ax
∂x + 0 + 0 + · · ·
and
Δφ1 =
(
Ax(x, y, z) + Δx
2
∂Ax
∂x
)
ΔyΔz + · · · .
Similarly, for the second face we obtain
Δφ2 = A2 · ˆen2Δa2 = A2 · (−ˆex) Δa2 = −A2xΔyΔz
= −Ax

x −Δx
2 , y, z

ΔyΔz
= −
(
Ax(x, y, z) −Δx
2
∂Ax
∂x + · · ·
)
ΔyΔz.
Adding the expressions for Δφ1 and Δφ2, we obtain
Δφ1 + Δφ2
=
(
Ax(x, y, z) + Δx
2
∂Ax
∂x −Ax(x, y, z) + Δx
2
∂Ax
∂x + · · ·
)
ΔyΔz
or
Δφ1 + Δφ2 = ∂Ax
∂x ΔxΔyΔz + · · · = ∂Ax
∂x ΔV + · · · .

374
Flux and Divergence
The reader may check that
Δφ3 + Δφ4 = ∂Ay
∂y ΔV + · · · ,
Δφ5 + Δφ6 = ∂Az
∂z ΔV + · · · ,
(13.9)
so that the total ﬂux through the small box is
Δφ =
∂Ax
∂x + ∂Ay
∂y + ∂Az
∂z

ΔV + · · · .
The ﬂux density, or divergence as it is more often called, can now be obtained
by dividing both sides by ΔV and taking the limit as ΔV →0. Since all the
terms represented by dots are of at least the fourth order, they vanish in the
limit and we obtain
Theorem 13.2.1. The relation between the ﬂux density of a vector ﬁeld and
the derivatives of its components is
ρφ ≡div A ≡∇· A =
lim
ΔV →0
Δφ
ΔV = ∂Ax
∂x + ∂Ay
∂y + ∂Az
∂z .
The term “divergence,” whose abbreviation is used as a symbol of ﬂux
origin of the term
“divergence”
density, is reminiscent of water ﬂowing away from its source, a fountain. In
this context, the ﬂux density measures how quickly or intensely water “di-
verges” away from the fountain.
The third notation ∇· A combines the
dot product in terms of components with the deﬁnition of ∇as given in
Equation (12.28).9
13.2.2
Divergence Theorem
The use of the word (volume) density for divergence suggests that the total
ﬂux through a (large) surface should be the (volume) integral of divergence.
However, any calculation of ﬂux—even locally—requires a surface, as we saw
in the derivation of ﬂux density. What are the “small” surfaces used in the
calculation of ﬂux density, and how is the large surface related to them? The
answer to this question will come out of a treatment of an important theorem
in vector calculus which we investigate now.
First consider two boxes with one face in common (Figure 13.5) and index
quantities related to the volume on the left by a and those related to the one
on the right by b. The total ﬂux is, of course, the sum of the ﬂuxes through
all six faces of the composite box:
Δφ = (Δφ1 + Δφ2) + (Δφ3 + Δφ4) + (Δφ5 + Δφ6),
9This notation is misleading because, as we shall see later, in non-Cartesian coordinate
systems, the expression of divergence in terms of derivatives will not be equal to simply
the dot product of ∇with the vector ﬁeld. One should really think of ∇· A as a symbol,
equivalent to ρφ or div A and not as an operation involving two vectors.

13.2 Flux Density = Divergence
375
(xa , ya , za)
y
x
z
(xb , yb , zb)
Figure 13.5: The common boundaries contribute no net ﬂux.
where, as before, Δφ1 is the total ﬂux through the face having a normal in
the positive x-direction, and Δφ2 that through the face having a normal in
the negative x-direction, and so on. It is evident from Figure 13.5 that
Δφ1 = Δφa1 + Δφb1,
where Δφa1 is the ﬂux through the positive x face of box a and Δφb1 is the
ﬂux through the positive x face of box b. Using a similar notation, we can
write
Δφ2 = Δφa2 + Δφb2,
Δφ5 + Δφ6 = Δφa5 + Δφb5 + Δφa6 + Δφb6.
However, for the y faces we have Δφ3 = Δφb3 and Δφ4 = Δφa4, because the
face of the composite box in the positive y-direction belongs to box b and that
in the negative y-direction to box a. Now note that the outward ﬂux through
the left face of box b is the negative of the outward ﬂux through the right face
of box a; that is,
Δφb4 = −Δφa3 ⇒Δφb4 + Δφa3 = 0.
Thus, we obtain
Δφ3 + Δφ4 = Δφb3 + Δφa4 = Δφa3 + Δφb3 + Δφa4 + Δφb4.
Using all the above relations yields
Δφ = (Δφa1 + Δφa2) + (Δφa3 + Δφa4) + (Δφa5 + Δφa6)
+ (Δφb1 + Δφb2) + (Δφb3 + Δφb4) + (Δφb5 + Δφb6)
or Δφ = Δφa + Δφb, or Δφ = (∇· A)aΔVa + (∇· A)bΔVb. These equations
say that
Box 13.2.2. The total ﬂux through the outer surface of a composite box
consisting of two adjacent boxes is equal to the sum of the total ﬂuxes
through the bounding surfaces of the two boxes, including the common
boundary. Stated diﬀerently, in summing the total outward ﬂux of adjacent
boxes, the contributions of the common boundary cancel.

376
Flux and Divergence
It is now clear how to generalize to a large surface bounding a volume: Di-
vide up the volume into N rectangular boxes and write φ ≈N
i=1(∇·A)iΔVi.
The LHS of this equation is the outward ﬂux through the bounding surface
only.
Contributions from the sides of all inner boxes cancel out because
each face of a typical inner box is shared by another box whose outward
ﬂux through that face is the negative of the outward ﬂux of the original box.
However, boxes at the boundary cannot ﬁnd enough boxes to cancel all their
ﬂux contributions, leaving precisely the ﬂux through the original surface. The
use of the approximation sign here reﬂects the fact that N, although large, is
not inﬁnite, and that the boxes are not small enough. To attain equality we
must make the boxes smaller and smaller and their number larger and larger,
in which case we approach the integral:
φ =
# #
V
#
∇· A dV.
(13.10)
Then, using Equation (13.2), we can state the important
the very
important
divergence
theorem
Theorem 13.2.2. (Divergence Theorem). The surface integral (ﬂux) of
any vector ﬁeld A through a closed surface S bounding a volume V is equal
to the volume integral of the divergence (or ﬂux density) of A:
# #
S
A · da =
# #
V
#
∇· A dV.
(13.11)
Let A = cf where c is an arbitrary constant vector and f a function.
Applying the divergence theorem to this A and using the readily veriﬁable
identity ∇· (cf) = c · ∇f, we get
# #
S
fc·da =
# #
V
#
c·(∇f)dV
or
c·
⎛
⎝
# #
S
fda
⎞
⎠= c·
⎛
⎝
# #
V
#
(∇f)dV
⎞
⎠
Since this holds for any c, we must have
# #
S
fda =
# #
V
#
∇fdV
(13.12)
Example 13.2.3. In this example we derive Gauss’s law for ﬁelds which vary
as the inverse of distance squared, speciﬁcally, gravitational and electrostatic ﬁelds.
Let Q be a source point (a point charge or a point mass) located at P0 with position
vector r0 and S a closed surface bounding a volume V . Let A(r) denote the ﬁeld
produced by Q at the ﬁeld point P with position vector r as shown in Figure 13.6(a).
We know that
A(r) =
KQ
|r −r0|3 (r −r0).
(13.13)

13.2 Flux Density = Divergence
377
S
O
P0
r
r0
(a)
(b)
Figure 13.6: Derivation of Gauss’s law for (a) a single point source, and (b) a number
of point sources.
The ﬂux of A through S can be written immediately:
# #
S
A · da =
# #
S
KQ(r −r0) · da
|r −r0|3
.
But the RHS is—apart from a constant—the solid angle subtended by S about P0.
Using Equation (12.7), we have
# #
S
A · da =
0
4πKQ
if P0 is in V,
0
if P0 is not in V.
(13.14)
If there are N point sources Q1, Q2, . . . , QN, then A will be the sum of individual
contributions, and we have
# #
S
A · da =
# #
S
N

k=1
Ak · da =
N

k=1
# #
S
KQk(rk −r0) · da
|rk −r0|3
= K
N

k=1
Qk
# #
S
(rk −r0) · da
|rk −r0|3
= K
N

k=1
QkΩk,
where Ωk is zero if Qk is outside V , and 4π if it is inside [see Figure 13.6(b)]. Thus,
only the sources enclosed in the volume will contribute to the sum and we have
# #
S
A · da = 4πKQenc,
(13.15)
where Qenc is the amount of source enclosed in S.
global (integral)
form of Gauss’s
law
For electrostatics, K = ke = 1/4πϵ0, Q = q, and A = E, so that
# #
S
E · da = qenc/ϵ0.
(13.16)
For gravitation, K = −G, Q = M, and A = g, so that
# #
S
g · da = −4πGMenc.
(13.17)

378
Flux and Divergence
The minus sign appears in the gravitational case because of the permanent attraction
of gravity. Gauss’s law is very useful in calculating the ﬁelds of very symmetric source
distributions, and it is put to good use in introductory electromagnetic discussions.
The derivation above shows that it is just as useful in gravitational calculations. ■
Equation (13.15) is the integral or global form of Gauss’s law. We can also
derive the diﬀerential or local form of Gauss’s law by invoking the divergence
theorem and assigning a volume density ρQ to Qenc:
LHS =
# #
V
#
∇· A dV,
RHS = 4πK
# #
V
#
ρQ dV.
Since these relations are true for arbitrary V , we obtain
local (diﬀerential)
form of Gauss’s
law
Theorem 13.2.4. (Diﬀerential Form of Gauss’s Law). If a point source
produces a vector ﬁeld A that obeys Equation (13.13), then for any volume
distribution ρQ of the source we have ∇· A = 4πKρQ.
This can easily be specialized to the two cases of interest, electrostatics
and gravity.
13.2.3
Continuity Equation
To improve our physical intuition of divergence, let us consider the ﬂow of a
ﬂuid of density ρ(x, y, z, t) and velocity v(x, y, z, t). The arguments to follow
are more general. They can be applied to the ﬂow (bulk motion) of many
physical quantities such as charge, mass, energy, momentum, etc. All that
needs to be done is to replace ρ—which is the mass density for the ﬂuid
ﬂow—with the density of the physical quantity.
We are interested in the amount of matter crossing a surface area Δa
per unit time. We denote this quantity momentarily by ΔM, and because
of its importance and wide use in various areas of physics, we shall derive
it in some detail. Take a small volume ΔV of the ﬂuid in the shape of a
slanted cylinder. The lateral side of this volume is chosen to be instantaneously
in the same direction as the velocity v of the particles in the volume. For
large volumes this may not be possible, because the macroscopic motion of
particles is, in general, not smooth, with diﬀerent parts having completely
diﬀerent velocities. However, if the volume ΔV (as well as the time interval
of observation) is taken small enough, the variation in the velocity of the
enclosed particles will be negligible. This situation is shown in Figure 13.7.
The lateral length of the cylinder is vΔt where Δt is the time it takes the
particles inside to go from the base to the top, so that all particles inside will
have crossed the top of the cylinder in this time interval. Thus, we have
amount crossing top = amount in ΔV = ρΔV.
But ΔV = (vΔt) · Δa = v · Δa Δt, where the dot product has been used
because the base and the top are not perpendicular to the lateral surface.

13.2 Flux Density = Divergence
379
vΔt
n
e^
Δa
Figure 13.7: The ﬂux through a small area is related to the current density.
Therefore,
ΔM = amount crossing top
Δt
= ρv · Δa Δt
Δt
= (ρv) · Δa.
The RHS of this equation is the ﬂux of the vector ﬁeld ρv which is called the
current density
mass current density, and usually denoted as J.
As indicated earlier, this result is general and applies to any physical
quantity in motion. We can therefore rewrite the equation in its most general
form as
ΔφQ = (ρQv) · Δa ≡JQ · Δa.
(13.18)
This is so important that we state it in words:
Box 13.2.3. The amount of a ﬂowing physical quantity Q crossing an
area Δa per unit time is the ﬂux JQ · Δa. The current density JQ at each
point is simply the product of volume density and velocity vector at that
point.
relation between
ﬂux and current
density
For a (large) surface S we need to integrate the above relation:
φQ =
# #
S
(ρQv) · da ≡
# #
S
JQ · da
(13.19)
and if S is closed, the divergence theorem gives
φQ =
# #
S
JQ · da =
# #
V
#
∇· JQ dV.
(13.20)
Let Q, which may change with time, denote the total amount of physical
quantity in the volume V . Then, clearly
Q(t) =
# #
V
#
ρQ dV =
# #
V
#
ρQ(r, t) dV (r),

380
Flux and Divergence
where in the last integral we have emphasized the dependence of various quan-
tities on location and time. Now, if Q is a conserved quantity such as energy,
momentum, charge, or mass,10 the amount of Q that crosses S outward (i.e.,
the ﬂux through S) must precisely equal the rate of depletion of Q in the
volume V .
global or integral
form of continuity
equation
Theorem 13.2.5. In mathematical symbols, the conservation of a conserved
physical quantity Q is written as
dQ
dt = −
# #
S
JQ · da,
(13.21)
which is the global or integral form of the continuity equation.
The minus sign ensures that positive ﬂux gives rise to a depletion, and
vice versa. The local or diﬀerential form of the continuity equation can be
obtained as follows: The LHS of Equation (13.21) can be written as
dQ
dt = d
dt
# #
V
#
ρQ(r, t) dV (r) =
# #
V
# ∂ρQ
∂t (r, t) dV (r),
while the RHS, with the help of the divergence theorem, becomes
−
# #
S
JQ · da = −
# #
V
#
∇· JQ dV.
Together they give
# #
V
# ∂ρQ
∂t dV = −
# #
V
#
∇· JQ dV
or
# #
V
# (∂ρQ
∂t + ∇· JQ
)
dV = 0.
This relation is true for all volumes V . In particular, we can make the volume
as small as we please. Then, the integral will be approximately the integrand
times the volume. Since the volume is nonzero (but small), the only way that
the product can be zero is for the integrand to vanish.
Box 13.2.4. The diﬀerential form of the continuity equation is
∂ρQ
∂t + ∇· JQ = 0.
(13.22)
local (diﬀerential)
form of continuity
equation
10In the theory of relativity mass by itself is not a conserved quantity, but mass in
combination with energy is.

13.2 Flux Density = Divergence
381
Both integral and diﬀerential forms of the continuity equation have a wide
range of applications in many areas of physics.
Equation (13.22) is sometimes written in terms of ρQ and the velocity.
This is achieved by substituting ρQv for JQ:
∂ρQ
∂t + ∇· (ρQv) = 0
or
∂ρQ
∂t + (∇ρQ) · v + ρQ∇· v = 0.
However, using Cartesian coordinates, we write the sum of the ﬁrst two terms
as a total derivative:
∂ρQ
∂t + (∇ρQ) · v = ∂ρQ
∂t +
∂ρQ
∂x , ∂ρQ
∂y , ∂ρQ
∂z

·
dx
dt , dy
dt , dz
dt

= ∂ρQ
∂t + ∂ρQ
∂x
dx
dt + ∂ρQ
∂y
dy
dt + ∂ρQ
∂z
dz
dt



=total derivative=dρQ/dt
= dρQ
dt .
Thus the continuity equation can also be written as
dρQ
dt + ρQ∇· v = 0.
(13.23)
Historical Notes
Aside from Maxwell, two names are associated with vector analysis (completely
detached from their quaternionic ancestors): Willard Gibbs and Oliver Heaviside.
Josiah Willard Gibbs’s father, also called Josiah Willard Gibbs, was profes-
sor of sacred literature at Yale University. In fact the Gibbs family originated in
Warwickshire, England, and moved from there to Boston in 1658.
Gibbs was educated at the local Hopkins Grammar School where he was de-
scribed as friendly but withdrawn. His total commitment to academic work together
with rather delicate health meant that he was little involved with the social life of
the school. In 1854 he entered Yale College where he won prizes for excellence in
Latin and mathematics.
Josiah Willard
Gibbs 1839–1903
Remaining at Yale, Gibbs began to undertake research in engineering, writing a
thesis in which he used geometrical methods to study the design of gears. When he
was awarded a doctorate from Yale in 1863 it was the ﬁrst doctorate of engineering
to be conferred in the United States. After this he served as a tutor at Yale for
three years, teaching Latin for the ﬁrst two years and then Natural Philosophy in
the third year. He was not short of money however since his father had died in
1861 and, since his mother had also died, Gibbs and his two sisters inherited a fair
amount of money.
From 1866 to 1869 Gibbs studied in Europe. He went with his sisters and spent
the winter of 1866–67 in Paris, followed by a year in Berlin and, ﬁnally spending
1868–69 in Heidelberg. In Heidelberg he was inﬂuenced by Kirchhoﬀand Helmholtz.
Gibbs returned to Yale in June 1869, where two years later he was appointed
professor of mathematical physics. Rather surprisingly his appointment to the pro-
fessorship at Yale came before he had published any work.
Gibbs was actually

382
Flux and Divergence
a physical chemist and his major publications were in chemical equilibrium and
thermodynamics. From 1873 to 1878, he wrote several important papers on ther-
modynamics including the notion of what is now called the Gibbs potential.
Gibbs’s work on vector analysis was in the form of printed notes for the use of
his own students written in 1881 and 1884. It was not until 1901 that a properly
published version appeared, prepared for publication by one of his students. Using
ideas of Grassmann, a high school teacher who also worked on the generalization of
complex numbers to three dimensions and invented what is now called Grassmann
algebra, Gibbs produced a system much more easily applied to physics than that of
Hamilton.
His work on statistical mechanics was also important, providing a mathematical
framework for the earlier work of Maxwell on the same subject. In fact his last
publication was Elementary Principles in Statistical Mechanics, which is a beautiful
account putting statistical mechanics on a ﬁrm mathematical foundation.
Except for his early years and the three years in Europe, Gibbs spent his whole
life living in the same house which his father had built only a short distance from the
school Gibbs had attended, the college at which he had studied, and the university
where he worked all his life.
Oliver Heaviside caught scarlet fever when he was a young child and this
aﬀected his hearing. This was to have a major eﬀect on his life making his childhood
unhappy, and his relations with other children diﬃcult. However his school results
were rather good and in 1865 he was placed ﬁfth from 500 pupils.
Academic subjects seemed to hold little attraction for Heaviside, however, and
at age 16 he left school. Perhaps he was more disillusioned with school than with
learning since he continued to study after leaving school, in particular he learnt the
Morse code, and studied electricity and foreign languages, in particular Danish and
German. He was aiming at a career as a telegrapher and in this he was advised
and helped by his uncle Charles Wheatstone (the piece of electrical apparatus the
Wheatstone bridge is named after him).
Oliver Heaviside
1850–1925
In 1868 Heaviside went to Denmark and became a telegrapher. He progressed
quickly in his profession and returned to England in 1871 to take up a post in
Newcastle upon Tyne in the oﬃce of the Great Northern Telegraph Company which
dealt with overseas traﬃc.
Heaviside became increasingly deaf but he worked on his own researches into
electricity. While still working as chief operator in Newcastle he began to publish
papers on electricity. One of these was of suﬃcient interest to Maxwell that he men-
tioned the results in the second edition of his Treatise on Electricity and Magnetism.
Maxwell’s treatise fascinated Heaviside and he gave up his job as a telegrapher and
devoted his time to the study of the work. Although his interest and understanding
of this work was deep, Heaviside was not interested in rigor. Nevertheless, he was
able to develop important methods in vector analysis in his investigations.
His operational calculus, developed between 1880 and 1887, caused much con-
troversy. Burnside rejected one of Heaviside’s papers on the operational calculus,
which he had submitted to the Proceedings of the Royal Society, on the grounds that
it “contained errors of substance and had irredeemable inadequacies in proof.” Tait
championed quaternions against the vector methods of Heaviside and Gibbs and
sent frequent letters to Nature attacking Heaviside’s methods. Eventually, however,
his work was recognized, and in 1891 he was elected a Fellow of the Royal Society.
Whittaker rated Heaviside’s operational calculus as one of the three most important
discoveries of the late nineteenth Century.

13.3 Problems
383
Heaviside seemed to become more and more bitter as the years went by. In 1908
he moved to Torquay where he showed increasing evidence of a persecution complex.
His neighbors related stories of Heaviside as a strange and embittered hermit who
replaced his furniture with granite blocks which stood about in the bare rooms like
the furnishings of some Neolithic giant. Through those fantastic rooms he wandered,
growing dirtier and dirtier, with one exception: His nails were always exquisitely
manicured, and painted a glistening cherry pink.
13.3
Problems
13.1. Using (13.6) ﬁnd the ﬂux of the vector ﬁeld A = kx2ˆez through the
portion of the sphere of radius a centered at the origin lying in the ﬁrst octant
of a Cartesian coordinate system.
13.2. Using (13.6) ﬁnd the ﬂux of the vector ﬁeld A = yˆex + 3zˆey −2xˆez
through the portion of the plane x + 2y −3z = 5 lying in the ﬁrst octant of a
Cartesian coordinate system.
13.3. A vector ﬁeld is given by A = r. Using (13.6) ﬁnd the ﬂux of this
vector ﬁeld through the upper hemisphere centered at the origin. Verify your
answer by calculating the ﬂux using (the much easier) spherical coordinates.
13.4. Find the ﬂux of the vector ﬁeld A = x2ˆex + y2ˆey + z2ˆez through the
portion of the plane x + y + z = 1 lying in the ﬁrst octant of a Cartesian
coordinate system.
13.5. Using (13.6), ﬁnd the ﬂux of the vector ﬁeld A = kr/r3 through the
upper hemisphere centered at the origin. Verify your answer by calculating
the ﬂux using spherical coordinates.
13.6. Find the ﬂux of the vector ﬁeld A = yˆey + aˆez through the portion of
the paraboloid z = b2 −x2 −y2 above the xy-plane.
13.7. Derive Equation (13.9).
13.8. Find the ﬂux of the vector
A =
6ka2y
	
x2 + y2 + a2 ˆex +
3ka2z
	
y2 + z2 + 4a2 ˆey +
2ka2x
√
x2 + z2 + 9a2 ˆez
through the surface of the box shown in Figure 13.8:
(a) by integrating over the surface of the box; and
(b) by using the divergence theorem and integrating over the volume of the
box.
13.9. The gravitational ﬁeld of a certain mass distribution is given by
g(x, y, z) = −kG
,
(x3y2z2)ˆex + (x2y3z2)ˆey + (x2y2z3)ˆez
-
,
where k is a constant and G is the universal gravitational constant:
(a) Find the mass density of the source of this ﬁeld.
(b) What is the total mass in a cube of side 2a centered about the origin?

384
Flux and Divergence
x
y
z
2a
3a
a
Figure 13.8: The box of Problem 13.8.
13.10. The gravitational ﬁeld of a certain mass distribution in the ﬁrst octant
of a Cartesian coordinate system is given by
g(x, y, z) = −GM
a3 re−(x+y+z)/a,
where r is the position vector, M and a are constants, and G is the universal
gravitational constant.
(a) Find the mass density of the source of this ﬁeld.
(b) What is the total mass in a cube of side a with one corner at the origin
and sides parallel to the axes?
13.11. The electrostatic potential of a certain charge distribution in Cartesian
coordinates is given by
Φ(x, y, z) = V0
a3 xyze−(x+y+z)/a,
where V0 and a are constants.
(a) Find the electric ﬁeld E = −∇Φ of this potential.
(b) Calculate the charge density of the source of this ﬁeld.
(c) What is the total charge in a cube of side a with one corner at the origin
and sides parallel to the axes? Write your answer as a numerical multiple of
ϵ0V0a.
13.12. The electric ﬁeld of a charge distribution is given by
E = E0
a4 xyze−(x+y+z)/ar.
(a) Write the Cartesian components of this electric ﬁeld completely in Carte-
sian coordinates.
(b) Calculate the volume charge density giving rise to this ﬁeld.
(c) Find the total charge in a cube of side a whose sides are parallel to the axes
and one of whose corners is at the origin. Write your answer as a numerical
multiple of ϵ0E0a2.

13.3 Problems
385
13.13. The velocity of a physical quantity Q is radial and given by v = kr
where k is a constant. Show that if the density ρQ is independent of position,
then it is given by
ρQ(t) = ρ0Qe−3kt
where ρ0Q is the initial density of Q.

Chapter 14
Line Integral and Curl
Last chapter introduced the concept of ﬂux and the surface integral associated
with it. Flux uses the directional property of a vector ﬁeld to have it pierce an
element of area. The directional property can also naturally assign a varying
direction along a line. One can consider how a vector ﬁeld changes direction
as it moves along a curve in space. This change can also lead to a new kind of
integration and diﬀerentiation of vector ﬁelds. The integration leads to the no-
tion of a line integral and the associated diﬀerentiation to the concept of curl.
14.1
The Line Integral
The prime example of a line integral is the work done by a force. Consider
the force ﬁeld F(r) acting on an object and imagine the object being moved
by a small displacement Δr. Then the work done by the force in eﬀecting this
displacement is deﬁned as
ΔW = F(r) · Δr,
where it is assumed that F(r) is (approximately) constant during the displace-
ment.
To calculate the work for a ﬁnite displacement, such as the one shown
in Figure 14.1, we break up the displacement into N small segments, cal-
culate the work for each segment, and add all contributions to obtain W ≈
N
i=1 F(ri) · Δri. The approximation sign can be removed by taking Δri as
small as possible and N as large as possible. Then we have
line integral
deﬁned
W =
# P2
P1
F(r) · dr ≡
#
C
F · dr,
(14.1)
where C stands for the particular curve on which the force is displaced. This
equation is, by deﬁnition, the line integral of the force ﬁeld F. In this partic-
ular case it is the work done by F in moving from P1 to P2. Of course, we can
apply the line integral to any vector ﬁeld, not just force. In electromagnetic

388
Line Integral and Curl
P1
Δ r1
Δ r3
Δ r2
Δ rN
Δ ri
P2
F(xi, yi, zi)
Figure 14.1: The line integral of a vector ﬁeld F from P1 to P2.
theory, for example, the line integrals of the electric and magnetic ﬁelds play
a central role.
The most general way to calculate a line integral is through parametric
equation of the curve. Thus, if the Cartesian set of parametric equations of
the curve is
x = f(t),
y = g(t),
z = h(t),
then the components of the vector ﬁeld A will be functions of a single variable
t obtained by substitution:
Ax(x, y, z) = Ax
 
f(t), g(t), h(t)
!
≡F(t),
Ay(x, y, z) = Ay
 
f(t), g(t), h(t)
!
≡G(t),
Az(x, y, z) = Az
 
f(t), g(t), h(t)
!
≡H(t),
and the components of dr are
dx = f ′(t) dt,
dy = g′(t) dt,
dz = h′(t) dt.
Then the line integral of A can be written as
line integral in
terms of the
parametric
equations of the
curve
#
C
A · dr =
#
C
(Ax dx + Ay dy + Az dz)
=
# b
a
,
F(t)f ′(t) + G(t)g′(t) + H(t)h′(t)
-
dt,
(14.2)
where t = a and t = b designate the initial and ﬁnal points of the curve,
respectively. Other coordinate systems can be handled similarly. Instead of
giving a general formula for these coordinate systems, we present an example
using cylindrical coordinates.
Example 14.1.1. Consider the vector ﬁeld given by
A = c1zϕˆeρ + c2ρzˆeϕ + c3ρϕˆez,
where c1, c2, and c3 are constants. We want to calculate the line integral of this
ﬁeld, starting at z = 0, along one turn of a uniformly wound helix of radius a whose

14.1 The Line Integral
389
x
y
z
b
Figure 14.2: The helical path for calculating the line integral.
windings are separated by a constant value b (see Figure 14.2 ). The parametric
equation of this helix in cylindrical coordinates is
ρ ≡f(t) = a, ϕ ≡g(t) = t, z ≡h(t) = b
2π t.
Notice that as ϕ = t changes by 2π, the height (i.e., z) changes by b as required.
Substituting for the three coordinates in terms of t in the expression for A, we obtain
A ≡⟨F(t), G(t), H(t)⟩=

c1 b
2π t2, c2a b
2π t, c3at

.
Similarly,
dr = ⟨dρ, ρ dϕ, dz⟩= ⟨f ′(t), f(t)g′(t), h′(t)⟩dt =

0, a, b
2π

dt,
so that
#
C
A · dr =
# b
a
,
F(t)f ′(t) + G(t)g′(t) + H(t)h′(t)
-
dt
=
# 2π
0
(
0 + c2a2 b
2π t + c3 b
2π at
)
dt = πab(c2a + c3).
■
Example 14.1.2. Consider the vector ﬁeld A = K(xy2ˆex + x2yˆey). We want
to evaluate the line integral of this ﬁeld from the origin to the point (a, a) in the
xy-plane along three diﬀerent paths (i), (ii), and (iii), as shown in Figure 14.3. Since
the vector ﬁeld is independent of z and the paths are all in the xy-plane, we ignore
z completely.
The ﬁrst path is the straight line y = x. A convenient parameterization is x = at,
y = at with 0 ≤t ≤1. Along this path the components of A become
Ax = Kxy2 = K(at)(at)2 = Ka3t3,
Ay = Kx2y = K(at)2(at) = Ka3t3.
Furthermore, taking the diﬀerentials of x and y, we obtain dx = a dt and dy = a dt.
Thus,
#
C
A · dr =
# (a,a)
(0,0)
(Axdx + Aydy) = K
# 1
0
. 
a3t3!
a dt +
 
a3t3!
a dt
/
= 2Ka4
# 1
0
t3 dt = Ka4
2
.

390
Line Integral and Curl
x
y
(i)
(ii)
(iii)
(a, a)
(iv)
Figure 14.3: The three paths joining the origin to the point (a, a). Path (iv) is to
illustrate the importance of parameterization.
Although parameterization is very useful, systematic, and highly recommended,
it is not always necessary. We calculate the line integral along path (ii)—given by
y = x2/a—without using parameterization. All we have to notice is that all the y’s
are to be replaced by x2/a [and therefore, dy by (2x/a) dx]. Thus,
Ax = Kxy2 = Kx
 x2
a
2
= K x5
a2 ,
Ay = Kx2y = Kx2
 x2
a

= K x4
a .
The line integral can now be evaluated easily:
# (a,a)
(0,0)
(Axdx + Aydy) = K
# a
0
x5
a2

dx +
 x4
a
  2x
a dx

= 3K
# a
0
x5
a2 dx = Ka4
2
.
Finally, we calculate the line integral along the quarter of a circle. For this calcu-
lation, we return to the parameterization technique, because it eases the integration.
A simple parameterization is
x = a −a cos t,
y = a sin t,
0 ≤t ≤π
2 ,
with dx = a sin t dt and dy = a cos t dt. This yields
Axdx + Aydy = K[(a −a cos t)a2 sin2 t]a sin t dt + K[(a −a cos t)2a sin t]a cos t dt
= Ka4[(1 −cos t)(1 −cos2 t) + (1 −cos t)2 cos t] sin t dt
= Ka4(1 −3 cos2 t + 2 cos3 t) sin t dt.
This is now integrated to give the line integral:
# (a,a)
(0,0)
(Axdx + Aydy) = Ka4
# π/2
0
(1 −3 cos2 t + 2 cos3 t) sin t dt
= Ka4

−cos t



π/2
0
+ cos3 t



π/2
0
−1
2 cos4 t



π/2
0

= Ka4
2
.
The fact that the three line integrals yield the same result may seem surprising.
However, as we shall see shortly, it is a property shared by a special group of vector
ﬁelds of which A is a member.
■

14.2 Curl of a Vector Field and Stokes’ Theorem
391
Many a time parameterization makes life a lot easier! Suppose we want
parameterization
is essential for
obtaining the
correct sign for
some line
integrals!
to calculate the line integral of a vector ﬁeld along path (iv) of Figure 14.3.
First let us attempt to calculate the line integral using the coordinates. Along
path (iv) dr = −ˆex dx; so A · dr = −Ax dx. Then
# (0,a)
(a,a)
A · dr = −
# 0
a
Ax dx =
# a
0
Ax dx.
Thus, if Ax > 0 (try Ax = x2), the integral will be positive. But this is wrong:
A positive Ax should yield a negative A · dr because the two vectors are in
opposite directions!
With parameterization, this problem is alleviated.
A parameterization
that represents path (iv) is
x = a(1 −t),
y = a,
0 ≤t ≤1.
Clearly, t = 0 corresponds to the beginning of path (iv) and t = 1 to its
endpoint. The parameterization automatically gives dx = −a dt and dy = 0.
For instance, the vector ﬁeld of Example 14.1.2 yields
# (0,a)
(a,a)
A · dr =
# 1
0
a(1 −t)a2(−a dt) = −a4
# 1
0
(1 −t) dt = −1
2a4.
This has the correct sign because Ax is positive and the direction of integration
negative. The other method would have given a positive result!
14.2
Curl of a Vector Field and Stokes’
Theorem
Line integrals around a closed path are of special interest. For example, if
the velocity vector of a ﬂuid has a nonzero integral around a closed path, the
ﬂuid must be turning around that path and a whirlpool must reside inside
the closed path. It is remarkable that such a mundanely concrete idea can be
applied verbatim to much more abstract and sophisticated concepts such as
electromagnetic ﬁelds with proven success and relevance. Thus, for a vector
ﬁeld, A, and a closed path, C, we denote the line integral as
2
C
A · dr
where the circle on the integral sign indicates that the path is closed and C
denotes the particular path taken.
In our discussion of divergence and ﬂux, we encountered Equation (13.11)
where an integral (over volume V ) was related to an integral over its boundary
(surface S). This remarkable property has an analog in one lower dimension:
Any closed curve bounds a surface inside it.
Is it possible to connect the

392
Line Integral and Curl
S1
C
S2
Figure 14.4: There is no “the” surface having C as its boundary. Both S1 and S2—as
well as a multitude of others—are such surfaces.
line integral over the closed curve to a surface integral over the surface? The
answer is yes, but we have to be careful here. What do we mean by “the” sur-
face? A given closed curve may bound many diﬀerent surfaces, as Figure 14.4
shows. It turns out that this freedom, which was absent in the divergence
case,1 is irrelevant and the relation holds for any surface whose boundary is
the given curve.
Let us now develop the analog of the divergence theorem for closed line
integrals. To begin, we consider a small closed rectangular path with a unit
normal ˆen, which is related to the direction of traversing the path by the
right-hand rule (RHR):
Right-hand rule
(RHR) rules here!
Box 14.2.1. (The Right-Hand Rule). Curl the ﬁngers of your right
hand in the direction of integration along the curve, your thumb should
then point in the direction of ˆen.
Without loss of generality we assume that the rectangle is parallel to the xy-
plane with sides parallel to the x-axis and the y-axis and that ˆen is parallel
to the z-axis (see Figure 14.5). The line integral can be written as
2
C
A · dr =
# b
a
A · dr +
# c
b
A · dr +
# d
c
A · dr +
# a
d
A · dr.
We do the ﬁrst integral in detail; the rest are similar. Along ab the element
of displacement dr is always in the positive x-direction and has magnitude dx,
1It should be clear that we cannot change the shape of the volume enclosed in S without
changing S itself. This rigidity is due to the maximality of the dimension of the enclosed
region: A volume is a three-dimensional object, and three is the maximum dimension we
have. Theories with higher dimension than three will allow a deformability similar to the
one discussed above.

14.2 Curl of a Vector Field and Stokes’ Theorem
393
a
b
c
d
Δx
Δ y
x
y
z
O
A
x
y
z
ˆen
Figure 14.5: A closed rectangular path parallel to the xy-plane with center at (x, y, z).
so it can be written as dr = ˆex dx. Thus, the ﬁrst integral on the RHS above
becomes
# b
a
A · dr ≡
# b
a
A1 · dr1 =
# b
a
A1 · (ˆex dx) =
# b
a
A1x dx,
where, as before, the subscript 1 indicates that we have to evaluate A at the
midpoint of ab and the subscript x denotes the x-component. Now, since ab
is small and the angle between A and dr does not change appreciably on ab,2
we can approximate the integral with A1xab and write
# b
a
A · dr ≈A1xab = A1x Δx = Ax

x, y −Δy
2 , z




coordinates of
midpoint of ab
Δx
≈
(
Ax(x, y, z) −Δy
2
∂Ax
∂y
)
Δx,
where in the last line we used the Taylor expansion of Ax. Similarly, we can
write
# d
c
A · dr =
# d
c
A2 · dr2 =
# d
c
A2 · (−ˆex dx) = −
# d
c
A2x dx
≈−A2xcd = −A2x Δx = −Ax

x, y + Δy
2 , z




coordinates of
midpoint of cd
Δx
≈−
(
Ax(x, y, z) + Δy
2
∂Ax
∂y
)
Δx.
Adding the contributions from sides ab and cd yields
# b
a
A · d r +
# d
c
A · dr ≈−∂Ax
∂y Δx Δy.
2This condition is essential, because a rapidly changing angle implies a rapidly changing
component A1x which is not suitable for the approximation to follow.

394
Line Integral and Curl
The contributions from the other two sides of the rectangle can also be
calculated:
# c
b
A · dr +
# a
d
A · dr ≈A3y Δy −A4y Δy
= Ay

x + Δx
2 , y, z

Δy −Ay

x −Δx
2 , y, z

Δy
≈
(
Ay(x, y, z) + Δx
2
∂Ay
∂x
)
Δy −
(
Ay(x, y, z) −Δx
2
∂Ay
∂x
)
Δy
= ∂Ay
∂x Δx Δy.
The sum of these two equations gives the total contribution:
2
C
A · d r ≈
∂Ay
∂x −∂Ax
∂y

Δx Δy.
(14.3)
Let us look at Equation (14.3) more closely. The expression in parentheses
can be interpreted as the z-component of the cross product of the gradient
operator ∇with A. In fact, using the mnemonic determinant form of the
vector product, we can write
∇× A = det
⎛
⎜
⎜
⎜
⎜
⎜
⎝
ˆex
ˆey
ˆez
∂
∂x
∂
∂y
∂
∂z
Ax
Ay
Az
⎞
⎟
⎟
⎟
⎟
⎟
⎠
=
∂Az
∂y −∂Ay
∂z

ˆex +
∂Ax
∂z −∂Az
∂x

ˆey +
∂Ay
∂x −∂Ax
∂y

ˆez.
This cross product is called the curl of A and is an important quantity in
curl of a vector
ﬁeld deﬁned
vector analysis. We will look more closely at it later. At this point, however,
we are interested only in its deﬁnition as applied in Equation (14.3). The
RHS of that equation can be written as
∂Ay
∂x −∂Ax
∂y

Δx Δy = (∇× A)z Δx Δy = (∇× A) · ˆezΔa,
where Δa = Δx Δy is the area of the rectangle. Noting that ˆez is in the
direction normal to the area, we can replace it with ˆen. Therefore, we can
write Equation (14.3) as
2
C
A · dr ≈(∇× A) · ˆenΔa = (∇× A) · Δa.
(14.4)
Equation (14.4) states that for a small rectangular path C the closed line
integral is equal to the normal component of the curl of A evaluated at the
center of the rectangle times the area of the rectangle. This statement does

14.2 Curl of a Vector Field and Stokes’ Theorem
395
not depend on the choice of coordinate system. In fact, any rectangle (or any
closed planar loop) deﬁnes a plane and we are at liberty to designate that
plane the xy-plane. Thus, we can deﬁne the curl of a vector ﬁeld this way:
coordinate
independent
deﬁnition of curl
Deﬁnition 14.2.1. Given a small closed curve C, calculate the line integral
of A around it and divide the result by the area enclosed by C. The component
of the curl of A along the unit normal to the area is given by
Curl A · ˆen ≡∇× A · ˆen = lim
Δa→0
E
C A · dr
Δa
.
(14.5)
The direction of ˆen is related to the sense of integration via the right-hand
rule.
In Equation (14.5) we are assuming that the area is ﬂat. This is always
possible by taking the curve small enough. Deﬁnition 14.2.1 is completely
independent of the coordinate system and we shall use it to derive expressions
for the curl of vector ﬁelds in spherical and cylindrical coordinates as well.
The reader should be aware that the notation ∇× A is just that, a notation,
and—except in Cartesian coordinates—should not be considered as a cross
product.
What happens with a large closed path? Figure 14.6 shows a closed path C
from small
rectangles to large
loops
with an arbitrary surface S, whose boundary is the given curve. We divide S
into small rectangular areas and assign a direction to their contours dictated
by the direction of integration around C.3 If we sum all the contributions
from the small rectangular paths, we will be left with the integration around
C because the contributions from the common sides of adjacent rectangles
cancel.4 This is because the sense of integration along their common side is
ˆe nΔa
C
S
Figure 14.6: An arbitrary surface with the curve C as its boundary. The sum of the
line integrals around the rectangular paths shown is equal to the line integral around C.
3The direction of the contour with one side on the curve C is determined by the direction
of the integration of C. The direction of a distant contour is determined by working one’s
way to it one (small) rectangle at a time.
4This situation is completely analogous to the calculation of the total ﬂux in the deriva-
tion of the divergence theorem.

396
Line Integral and Curl
opposite for two adjacent rectangles (see Figure 14.6). Thus, the macroscopic
version of Equation (14.4) is
2
C
A · dr ≈
N

i=1
(∇× A)i · ˆeniΔai =
N

i=1
(∇× A)i · Δai,
where (∇× A)i is the curl of A evaluated at the center of the ith rectangle,
which has area Δai and normal ˆeni, and N is the number of rectangles on
the surface S. If the areas become smaller and smaller as N gets larger and
larger, we can replace the summation by an integral and obtain
the most
important Stokes’
theorem
Theorem 14.2.1. (Stokes’ Theorem). The line integral of a vector ﬁeld
A around a closed path C is equal to the surface integral of the curl of A on
any surface whose only edge is C. In mathematical symbols, we have
2
C
A · dr =
# #
S
∇× A · da.
(14.6)
The direction of the normal to the inﬁnitesimal area da of the surface S is
related to the direction of integration around C by the right-hand rule.
Example 14.2.2. In this example we apply the concepts of closed line integral
and the Stokes’ theorem to a concrete vector ﬁeld. Consider the vector ﬁeld
A = K(x2yˆex + xy2ˆey)
obtained from the vector ﬁeld of Example 14.1.2 by switching the x- and y-components.
We want to calculate the line integral around the two closed loops (the circle and
the rectangle) of Figure 14.7 and verify the Stokes’ theorem.
A convenient parameterization for the circle is
x = a cos t,
y = a sin t,
0 ≤t ≤2π,
with dx = −a sin t dt and dy = a cos t dt. Thus,
A · dr = K(a cos t)2(a sin t)(−a sin t dt) + K(a cos t)(a sin t)2(a cos t dt) = 0,
x
y
b
2b
a
Figure 14.7: Two loops around which the vector ﬁeld of Example 14.2.2 is calculated.

14.2 Curl of a Vector Field and Stokes’ Theorem
397
and the LHS of the Stokes’ theorem is zero. For the RHS, we need the curl of the
vector.
∇× A = K











ˆex
ˆey
ˆez
∂
∂x
∂
∂y
∂
∂z
x2y
xy2
0











= K(y2 −x2)ˆez.
It is convenient to use cylindrical coordinates for integration over the area of the
circle. Moreover, the right-hand rule determines the unit normal to the area of the
circle to be ˆez. Thus,
# #
S
∇× A · da = K
# a
0
# 2π
0
(ρ2 sin2 ϕ −ρ2 cos2 ϕ)ρ dρ dϕ = 0
by the ϕ integration. Thus the two sides of the Stokes’ theorem agree.
The two sides of the rectangular loop sitting on the axes will give zero because
A = 0 there. The contribution of the side parallel to the y-axis can be obtained by
noting that x = 2b and dx = 0, so that
A · dr = Ax dx + Ay dy = 0 + 2bKy2 dy
and
# (2b,b)
(2b,0)
A · dr = 2bK
# b
0
y2 dy = 2
3Kb4.
To avoid ambiguity,5 we employ parameterization for the last line integral. A con-
venient parametric equation would be
x = 2b(1 −t),
y = b,
0 ≤t ≤1,
which gives dx = −2b dt, dy = 0, and for which the line integral yields
# (2b,0)
(2b,b)
A · dr = K
# 1
0
[2b(1 −t)]2(b)(−2b dt) = −8b4K
# 1
0
(1 −t)2 dt = −8
3Kb4.
So, the line integral for the entire loop (the LHS of the Stokes’ theorem) is
2
C
A · dr = 2
3Kb4 −8
3Kb4 = −2Kb4.
We have already calculated the curl of A. Thus, the RHS of the Stokes’ theorem
becomes
# #
S
∇× A · da = K
# #
S
(y2 −x2) dx dy
= K
# 2b
0
dx
# b
0
y2dy



=2b(b3/3)
−K
# 2b
0
x2dx
# b
0
dy



(8b3/3)b
= −2Kb4
and the two sides agree.
■
5See the discussion following Example 14.1.2.

398
Line Integral and Curl
Historical Notes
George Gabriel Stokes published papers on the motion of incompressible ﬂuids in
1842–43 and on the friction of ﬂuids in motion, and on the equilibrium and motion
of elastic solids in 1845.
In 1849 Stokes was appointed Lucasian Professor of Mathematics at Cambridge,
and in 1851 he was elected to the Royal Society and was secretary of the society
from 1854 to 1884 when he was elected president.
He investigated the wave theory of light, named and explained the phenomenon
of ﬂuorescence in 1852, and in 1854 theorized an explanation of the Fraunhofer lines
in the solar spectrum. He suggested these were caused by atoms in the outer layers
of the Sun absorbing certain wavelengths. However, when Kirchhoﬀlater published
this explanation, Stokes disclaimed any prior discovery.
George Gabriel
Stokes 1819–1903
Stokes developed mathematical techniques for application to physical problems
including the most important theorem which bears his name. He founded the science
of geodesy, and greatly advanced the study of mathematical physics in England. His
mathematical and physical papers were published in ﬁve volumes, the ﬁrst three of
which Stokes edited himself in 1880, 1883, and 1891. The last two were edited by
Sir Joseph Larmor in 1887 and 1891.
14.3
Conservative Vector Fields
Of great importance are conservative vector ﬁelds, which are those vec-
tor ﬁelds that have vanishing line integrals around every closed path.
An
immediate result of this property is that
conservative
vector ﬁelds
deﬁned
Box 14.3.1. The line integral of a conservative vector ﬁeld between two
arbitrary points in space is independent of the path taken.
To see this, take any two points P1 and P2 connected by two diﬀerent directed
paths C1 and C2 as shown in Figure 14.8(a). The combination of C1 and the
negative of C2 forms a closed loop [Figure 14.8(b)] for which we can write
#
C1
A · dr +
#
−C2
A · dr = 0
because A is conservative by assumption. The second integral is the negative
of the integral along C2. Thus, the above equation is equivalent to
#
C1
A · dr −
#
C2
A · dr = 0 ⇒
#
C1
A · dr =
#
C2
A · dr
which proves the above claim.
Now take an arbitrary reference point P0 and connect it via arbitrary paths
to all points in space. At each point P with Cartesian coordinates (x, y, z),
deﬁne the function Φ(x, y, z) by
Φ(x, y, z) = −
# P
P0
A · dr ≡−
#
C
A · dr,
(14.7)

14.3 Conservative Vector Fields
399
− C2
 P2
 P1
 C1
 P2
 P1
 C1
 C2
)b(
)a(
Figure 14.8: (a) Two paths from P1 to P2, and (b) the loop formed by them.
where C is any path from P0 to P and the minus sign is introduced for
the function Φ, so
deﬁned, has the
mathematical
property expected
of a function,
namely, that for
every point P, the
function has only
one value that we
may denote as
Φ(P).
historical reasons only. Φ is a well-deﬁned function because its value does not
depend on C and is called the potential associated with the vector ﬁeld A.
We note that the potential at P0 is zero. That is why P0 is called the potential
reference point.
Now consider two arbitrary points P1 and P2, with Cartesian coordinates
(x1, y1, z1) and (x2, y2, z2), connected by some path C. We can also connect
these two points by a path that goes from P1 to P0 and then to P2 (see
Figure 14.9). Since A is conservative, we have
# P2
P1
A · dr =
# P0
P1
A · dr +
# P2
P0
A · dr = Φ(x1, y1, z1) −Φ(x2, y2, z2)
or
potential of a
conservative
vector ﬁeld
Φ(x2, y2, z2) −Φ(x1, y1, z1) = −
# P2
P1
A · dr,
(14.8)
which expresses the potential diﬀerence between the two points.
If P1 and P2 are displaced inﬁnitesimally by dr, then their inﬁnitesimal
potential diﬀerence will be
dΦ = −A · dr.
On the other hand, Φ, being a scalar diﬀerentiable function of x, y, and z, has
inﬁnitesimal increment
dΦ = ∂Φ
∂x dx + ∂Φ
∂y dy + ∂Φ
∂z dz = (∇Φ) · dr,
C
P2
P0
P1
Figure 14.9: Any path C from P1 to P2 is equivalent to the path P1 →P0 →P2.

400
Line Integral and Curl
so we have
−A · dr = (∇Φ) · dr.
But this is true for an arbitrary dr. Taking dr to be ˆex dx, ˆey dy, and ˆez dz
in turn, we obtain the equality of the three components of ∇Φ and −A.
Therefore, we have
A = −∇Φ,
(14.9)
which states that
Theorem 14.3.1. A conservative vector ﬁeld can be written as the negative
gradient of a potential function deﬁned as
Φ(x, y, z) = −
# P
P0
A · dr,
where (x, y, z) are the coordinates of P, and the integral is taken along any
path connecting P0 and P.
Another property of a conservative vector ﬁeld can be obtained by rewrit-
ing Equation (14.4), which is true for an arbitrary inﬁnitesimal closed path:
2
C
A · dr ≈(∇× A) · ˆenΔa.
However, the LHS is zero because A is conservative. Thus we have
the curl of a
conservative
vector ﬁeld is zero.
(∇× A) · ˆenΔa = 0.
This is true for arbitrary Δa and ˆen.
Therefore, we have the important
conclusion that ∇× A = 0 for a conservative vector ﬁeld. It is important to
note that although
E
C A · dr is zero and C is small, we cannot deduce that
A · dr = 0 and, therefore, A = 0. (Why?)
A conservative vector ﬁeld demands the vanishing of the curl.
But is
∇× A = 0 does
not necessarily
imply that A is
conservative!
∇× A = 0 suﬃcient for A to be conservative? The answer, in general, is
no! (See Example 14.3.3 below.) If the vector ﬁeld is well deﬁned and well
behaved (smoothly varying, diﬀerentiable, etc.) in a region of space U, then
∇×A = 0 in U implies that E
C A·dr = 0 for all closed curves C lying entirely
in U. In modern mathematical jargon such a region is said to be contractible
to zero, which means that any closed curve in U can be contracted to a point
(or “zero” closed curve) without encountering any singular point of the vector
ﬁeld (where it is not deﬁned or well behaved). We state this result as follows:
Box 14.3.2. Let the region U in space be contractible to zero for the vector
ﬁeld A. Then for any closed curve C in U, the two relations ∇× A = 0
and
E
C A · dr = 0 are equivalent.

14.3 Conservative Vector Fields
401
Example 14.3.2. The line integral of the vector ﬁeld of Example 14.1.2 was
independent of the three paths examined there. Could it be that the vector ﬁeld is
conservative? The vector ﬁeld is clearly well behaved everywhere. Therefore, the
vanishing of its curl proves that it is conservative. But
∇× A = K











ˆex
ˆey
ˆez
∂
∂x
∂
∂y
∂
∂z
xy2
x2y
0











= (0)ˆex + (0)ˆey + (2xy −2xy)ˆez = 0.
So, A is indeed conservative.
Next we ﬁnd the potential of A at a point (x0, y0) in the xy-plane.6 Let the
reference point be the origin. Since it does not matter what path we take, we choose
a straight line joining the origin and (x0, y0). A convenient parametric equation is
x = x0t,
y = y0t,
0 ≤t ≤1,
which gives dx = x0 dt and dy = y0 dt. We now have
Φ(x0, y0) = −
# (x0,y0)
(0,0)
A · dr
= −K
# 1
0
[(x0t)(y0t)2(x0 dt) + (x0t)2(y0t)(y0 dt)]
= −2Kx2
0y2
0
# 1
0
t3dt = −1
2Kx2
0y2
0.
We can now substitute (x, y) for (x0, y0) to obtain
Φ(x, y) = −1
2Kx2y2.
The reader may verify that A = −∇Φ.
■
It should be clear that ∇×A ̸= 0 always implies that A is not conservative.
However, ∇× A = 0 implies that A is conservative only if the region in
question is contractible to zero.
Example 14.3.3. Consider the vector ﬁeld
A =
ky
x2 + y2 ˆex −
kx
x2 + y2 ˆey,
where k is a constant. Since the components of this vector are independent of z, the
curl of the vector can have only a z-component:
∇× A =











ˆex
ˆey
ˆez
∂
∂x
∂
∂y
∂
∂z
Ax
Ay
0











=
 ∂Ay
∂x −∂Ax
∂y

ˆez.
6We completely ignore the z-coordinate because A has no component in that direction.

402
Line Integral and Curl
The reader may easily verify that
∂Ay
∂x = −
k
x2 + y2 + k
2x2
(x2 + y2)2 ,
∂Ax
∂y
=
k
x2 + y2 + k
2y2
(x2 + y2)2 ,
so that
∂Ay
∂x −∂Ax
∂y
= −
2k
x2 + y2 + k 2(x2 + y2)
(x2 + y2)2 = 0
and ∇× A = 0.
Now take a circle of radius a about the origin and calculate the line integral of
A on this circle. For integration, use the parameterization
x = a cos t,
y = a sin t,
0 ≤t ≤2π,
with dx = −a sin t dt and dy = a cos t dt. Then
A · dr = Axdx + Aydy = k(a sin t)(−a sin t dt)
(a cos t)2 + (a sin t)2 −k(a cos t)(a cos t dt)
(a cos t)2 + (a sin t)2 = −k dt
and, therefore
2
circ
A · dr = −k
# 2π
0
dt = −2πk.
This is an example of a vector ﬁeld whose curl vanishes but yields a nonzero
result for a closed line integral. The reason is, of course, that the region inside the
circle is not contractable to zero: At the origin the vector is inﬁnite.
■
If the vector ﬁeld is conservative, in principle we can determine its potential
either by direct antidiﬀerentiation or by integration. The following example
illustrates the former procedure.
Example 14.3.4. Consider the vector ﬁeld
A = (2xy + 3z2)ˆex + (x2 + 4yz)ˆey + (2y2 + 6xz)ˆez.
The reader may check that ∇× A = 0. Thus, since A is well deﬁned everywhere,
it is conservative. To ﬁnd its potential Φ, we note that
∂Φ
∂x = −Ax = −2xy −3z2 ⇒Φ = −x2y −3z2x + g(y, z),
where we have simply antidiﬀerentiated Ax with respect to x—assuming that y
and z are merely constants—and added a “constant” of integration: As far as x
diﬀerentiation is concerned, any function of y and z is a constant. Now diﬀerentiate
Φ obtained this way with respect to y and set it equal to −Ay:
−Ay = −(x2 + 4zy) = ∂Φ
∂y = ∂
∂y
 
−x2y −3z2x + g(y, z)
!
= −x2 + ∂g
∂y .
This gives
∂g
∂y = −4yz ⇒g(y, z) = −2y2z + h(z)
Note that our second “constant” of integration has no x-dependence because g(y,z)
does not depend on x. Substituting this back in the expression for Φ, we obtain
Φ = −x2y −3z2x + g(y,z) = −x2y −3z2x −2y2z + h(z).

14.3 Conservative Vector Fields
403
Finally, diﬀerentiating this with respect to z and setting it equal to −Az, we obtain
−Az = −(2y2 + 6xz) = ∂Φ
∂z = ∂
∂z
 
−x2y −3z2x −2y2z + h(z)
!
= −6xz −2y2 + dh
dz .
This gives
dh
dz = 0 ⇒h(z) = const. ≡C.
The ﬁnal answer is therefore
Φ(x, y, z) = −x2y −3z2x −2y2z + C.
The arbitrary constant depends on the potential reference point, and is zero if we
choose the origin as that point. It is easy to verify that −∇Φ is indeed the vector
ﬁeld we started with.
■
There are various vector identities which connect gradient, divergence,
and curl. Most of these identities can be obtained by direct substitution. For
example, by substituting the Cartesian components of A×B in the Cartesian
expression for divergence, one can show that
∇· (A × B) = B · ∇× A −A · ∇× B.
(14.10)
Similarly, one can show that
∇· (fA) = A · ∇f + f∇· A,
∇× (fA) = f∇× A + (∇f) × A
(14.11)
A × (∇× A) = 1
2∇|A|2 −(A · ∇)A
We can use Equation (14.10) to derive an important vector integral relation
akin to the divergence theorem. Let B be a constant vector. Then the second
term on the RHS vanishes. Now apply the divergence theorem to the vector
ﬁeld A × B:
# #
S
A × B · da =
# #
V
#
∇· (A × B) dV.
Using Equation (14.10), the RHS can be written as
RHS =
# #
V
#
B · ∇× A dV = B ·
# #
V
#
∇× A dV.
Moreover, the use of the cyclic property of the mixed triple product (see
Problem 1.15) will enable us to write the LHS as
LHS =
# #
S
(da × A) · B =
# #
S
B · (da × A) = B ·
# #
S
da × A.

404
Line Integral and Curl
Equating the new versions of the two sides, we obtain
B ·
# #
V
#
∇× A dV = B ·
# #
S
da × A
or
B ·
⎛
⎝
# #
V
#
∇× A dV −
# #
S
da × A
⎞
⎠= 0.
Since the last relation is true of arbitrary B, the vector inside the parentheses
must be zero. This gives the result we are after:
# #
V
#
∇× A dV =
# #
S
da × A.
(14.12)
14.4
Problems
14.1. Evaluate the line integral of
A(x, y, z) = x2ˆex + y2ˆey −z2ˆez
along the path given parametrically by
x = at2,
y = bt,
z = c sin (πt/2)
from the origin to (a, b, c).
14.2. Evaluate the line integral of
A(x, y, z) = xˆex + y2
b ˆey −z2
c ˆez
along the path given parametrically by
x = a cos(πt/2),
y = b sin(πt/2),
z = ct
from (a, 0, 0) to (0, b, c).
14.3. Evaluate the line integral of
A(x, y) = xˆex + y2
b ˆey
along the closed ellipse given parametrically by
x = a cos t,
y = b sin t.
14.4. Show that ∇× (A × r) = 2A.

14.4 Problems
405
14.5. Let
A(x, y) = Ax(x, y)ˆex + Ay(x, y)ˆey
B(x, y) = Bx(x, y)ˆex + By(x, y)ˆey
be vectors in two-dimensions.
(a) Apply the divergence theorem to A using a volume V enclosed by a cylin-
der whose bottom base is an arbitrary closed curve C in the xy-plane and
whose top base is the same curve in a plane parallel to the xy-plane, and
whose lateral side is parallel to the z-axis. Now conclude that
2
C
(Axdy −Aydx) =
##
R
∂Ax
∂x + ∂Ay
∂y

dx dy
where R is the region enclosed by C in the xy-plane. This is the divergence
theorem in two dimensions.
(b) Apply Stokes’ theorem to B with C as above and S the region R deﬁned
above. Show that
2
C
(Bxdx + Bydy) =
##
R
∂By
∂x −∂Bx
∂y

dx dy
This is the Stokes’ theorem in two dimensions.
(c) Show that in two dimensions the Stokes’ theorem and divergence theorem
are the same.
14.6. Evaluate the line integral of
A(x, y) =
 
x2 + 3y
!
ˆex +
 
y2 + 2x
!
ˆey
from the origin to the point (1, 2):
(a) along the straight line joining the two points; and
(b) along the parabola passing through the two points as well as the point
(−1, 2).
(c) Is A conservative?
14.7. Is the vector ﬁeld A(x, y) = xex2 cos y ˆex −1
2ex2 sin y ˆey conservative?
If so, ﬁnd its potential.
14.8. A vector ﬁeld is given by
A = Φ0
b2
*
y

1 + x
b

ˆex + xˆey + xy
b ˆez
+
e(x+z)/b,
where Φ0 and b are constants.
(a) Determine whether or not A is conservative.
(b) Find the potential of A if it is conservative.
14.9. The components of a vector ﬁeld are given by
Ax = V0k3yzek2xy,
Ay = V0k3xzek2xy + V0k sin ky,
Az = V0kek2xy.
(a) Determine whether A is conservative or not.
(b) If it is conservative, ﬁnd its potential.

406
Line Integral and Curl
14.10. The Cartesian components of a vector are given by
Ax = 2axekz,
Ay = 2ayekz,
Az = ka(x2 + y2)ekz,
where a and k are constants.
(a) Test whether A is conservative or not.
(b) If A is conservative, ﬁnd its potential.
14.11. Prove Equations (14.10) and (14.11).
14.12. Show that
∇(A · B) = (B · ∇)A + (A · ∇)B + B × (∇× A) + A × (∇× B)
and that
A × (∇× B) = ∇(A · B) −(A · ∇)B
14.13. Verify the vector identity
∇× (A × B) = (B · ∇)A −(A · ∇)B −B(∇· A) + A(∇· B)
14.14. Verify that for constant A and B
∇[A · (B × r)] = A × B

Chapter 15
Applied Vector Analysis
In the last three chapters, we introduced the operator ∇and used it to make
vectors out of scalars (gradient), scalars out of vectors (divergence), and new
vector out of old vectors (curl).
It is obvious that all these processes can
be combined to form new scalars and vectors. For instance one can create
a vector out of a scalar by the operation of gradient and use the resulting
vector as an input for the operation of divergence. Since almost all equations
of physics involve derivatives of at most second order, we shall conﬁne our
treatment to “double del operations” in this chapter.
15.1
Double Del Operations
We can make diﬀerent combinations of the vector operator ∇with itself. By
direct diﬀerentiation we can easily verify that
∇× (∇f) = 0.
(15.1)
Equation (14.9) states that a conservative vector ﬁeld is the gradient of its
potential.
Equation (15.1) says, on the other hand, that if a ﬁeld is the
gradient of a function then it is conservative.1 We can combine these two
statements into one by saying that
Box 15.1.1. A vector ﬁeld is conservative (i.e., its curl vanishes) if and
only if it can be written as the gradient of a scalar function, in which case
the scalar function is the ﬁeld’s potential.
Example 15.1.1. The electrostatic and gravitational ﬁelds, which we denote
generically by A, are given by an equation of the form
A(r) = K
##
Ω
dQ(r′)
|r −r′|3 (r −r′).
1Assuming that the region in which the gradient of the function is deﬁned is contractable
to zero, i.e., the region has no point at which the gradient is inﬁnite.

408
Applied Vector Analysis
Furthermore, the reader may show that (see Problem 12.17)
r −r′
|r −r′|3 = −∇

1
|r −r′|

.
(15.2)
Substitution in the above integral then yields
A(r) = −K
##
Ω
dQ(r′)∇

1
|r −r′|

= −∇

K
##
Ω
dQ(r′)
|r −r′|

= −∇Φ(r),
(15.3)
where Φ, the potential of A, is given by
Φ(r) ≡K
##
Ω
dQ(r′)
|r −r′|.
(15.4)
Equation (15.3), in conjunction with Equation (15.1), automatically implies that
both the electrostatic and gravitational ﬁelds are conservative.
■
In a similar fashion, we can directly verify the following identity:
∇· (∇× A) = 0.
(15.5)
Example 15.1.2. Magnetic ﬁelds can also be written in terms of the so-called
vector potentials.
To ﬁnd the expression for the vector potential, we substitute
Equation (15.2) in the magnetic ﬁeld integral:
B =
##
Ω
kmdq(r′)v(r′) × ( r −r′)
|r −r′|3
= km
##
Ω
dq(r′)v( r′) ×
0
−∇

1
|r −r′|
1
.
We want to take the ∇out of the integral. However, the cross product prevents a
direct “pull out.” So, we need to get around this by manipulating the integrand.
Using the second relation in Equation (14.11), we can write
∇×
 v(r′)
|r −r′|

=
1
|r −r′|
=0
  
∇× v −v × ∇

1
|r −r′|

= −v(r′) × ∇

1
|r −r′|

.
We note that ∇× v = 0 because ∇diﬀerentiates with respect to (x, y, z) of which
v(r′) is independent.
Substituting this last relation in the expression for B, we
obtain
B = km
##
Ω
dq(r′)∇×
 v( r′)
|r −r′|

= ∇×

km
##
Ω
dq(r′)v( r′)
|r −r′|

≡∇× A,
(15.6)
where we have taken ∇× out of the integral since it diﬀerentiates with respect to
the parameters of integration and Ω is assumed independent of (x, y, z). The vector
potential A is deﬁned by the last line, which we rewrite as
vector potential
deﬁned
A = km
##
Ω
dq(r′)v(r′)
|r −r′|
.
(15.7)

15.2 Magnetic Multipoles
409
If the charges are conﬁned to one dimension, so that we have a current loop, then
dq(r′)v(r′) = I dr′ and Equation (15.7) reduces to
A = kmI
2
dr′
|r −r′|.
(15.8)
An important consequence of Equations (15.6) and (15.5) is
Vanishing of
divergence of
magnetic ﬁeld
implies absence of
magnetic charges.
∇· B = 0.
(15.9)
Since the divergence of a vector ﬁeld is related to the density of its source, we
conclude that there are no magnetic charges.
This statement is within the context of classical electromagnetic theory. Re-
cently, with the advent of the uniﬁcation of electromagnetic and weak nuclear inter-
actions, there have been theoretical arguments for the existence of magnetic charges
(or monopoles).
However, although the theory predicts—very rare—occurrences
of such monopoles, no experimental conﬁrmation of their existence has been
made.
■
15.2
Magnetic Multipoles
The similarity between the vector potential [Equation (15.8)] and the electro-
static potential motivates the expansion of the former in terms of multipoles
as was done in (10.33). We carry this expansion only up to the dipole term.
Substituting Equation (10.32) in Equation (15.8), we obtain
A = kmI
2 1
r + ˆer · r′
r2
+ · · ·

dr′ = kmI
r
2
dr′
  
=0
+kmI
r2
2
ˆer · r′dr′.
The reader can easily show that the ﬁrst integral vanishes (Problem 15.5).
To facilitate calculating the second integral, choose Cartesian coordinates
and orient your axes so that ˆer is in the x-direction. Denote the integral by
V. Then
V =
2
ˆer · r′dr′ =
2
ˆex · r′dr′ =
2
x′dr′ =
2
x′(ˆex dx′ + ˆey dy′ + ˆez dz′).
We evaluate each component of V separately.
Vx =
2
x′dx′ = 1
2
2
d
 
x′2!
= 1
2x′2


end
beginning = 0
because the beginning and end points of a loop coincide.
Now consider the identity
2
(x′dy′ + y′dx′) =
2
d(x′y′) = (x′y′)



end
beginning = 0
(15.10)

410
Applied Vector Analysis
with an analogous identity involving x′ and z′. For the y-component of V,
we have
Vy =
2
x′dy′ = 1
2
2
x′dy′ + 1
2
2
x′dy′ + 1
2
2
y′dx′ −1
2
2
y′dx′



These add up to nothing!
= 1
2
2
x′dy′ +
2
y′dx′




=0 by Equation (15.10)
+ 1
2
2
x′dy′ −
2
y′dx′

= 1
2
2
(x′dy′ −y′dx′) = 1
2
2
(r′ × dr′)z = 1
2
2
r′ × dr′

· ˆez.
It follows that
Ay = kmI
r2 Vy = kmI
2r2
2
r′ × dr′

· ˆez ≡km
r2 μ · ˆez,
where we have deﬁned the magnetic dipole moment μ as
magnetic dipole
moment
μ ≡I
2
2
r′ × dr′.
(15.11)
A similar calculation will yield
Az = kmI
r2 Vz = −kmI
2r2
2
r′ × dr′

· ˆey ≡−km
r2 μ · ˆey.
Therefore,
A = Axˆex + Ayˆey + Azˆez = km
r2 ( ˆey μ · ˆez −ˆez μ · ˆey )



=μ×(ˆey×ˆez) by bac cab rule
.
Recalling that ˆey × ˆez = ˆex, and that by our choice of orientation of the axes
ˆer = ˆex, we ﬁnally obtain
A = kmμ × ˆer
r2
= kmμ × r
r3
.
(15.12)
There is a striking resemblance between the vector potential of a magnetic
dipole [Equation (15.12)] and the scalar potential of an electric dipole [the
second term in the last line of Equation (10.33)]: The scalar potential is
given in terms of the scalar (dot) product of the electric dipole moment and
the position vector, the vector potential is given in terms of the vector product
of the magnetic dipole moment and the position vector.
Example 15.2.1. Let us calculate the magnetic dipole moment of a circular
magnetic dipole
moment of a
circular current
loop
current of radius a. Placing the circle in the xy-plane with its center at the origin,
we have
μ = I
2
2
r′ × dr′ = I
2
2
(aˆeρ′) × (a dϕ′ˆeϕ′) = Ia2
2
# 2π
0
dϕ′ˆez = Iπa2ˆez.
So, the magnitude of the magnetic dipole moment of a circular loop of current is
the product of the current and the area of the loop. Its direction is related to the
direction of the current by the right-hand rule.
■

15.3 Laplacian
411
15.3
Laplacian
The divergence of the gradient is an important and frequently occurring op-
erator called the Laplacian:
Laplacian of a
function
∇· (∇f) ≡∇2f = ∂2f
∂x2 + ∂2f
∂y2 + ∂2f
∂z2 .
(15.13)
Laplacian occurs throughout physics, in situations ranging from the waves on
Laplacian is found
everywhere!
a drum to the diﬀusion of matter in space, the propagation of electromagnetic
waves, and even the most basic behavior of matter on a subatomic scale, as
governed by the Schr¨odinger equation of quantum mechanics.
We discuss one situation in which the Laplacian occurs naturally. The
result of the example above and Theorem 13.2.4 can be combined to obtain
an important equation in electrostatics and gravity called the Poisson equa-
tion: ∇· (−∇Φ) = 4πKρQ, or
Poisson equation
∇2Φ(r) = −4πKρQ(r).
(15.14)
This is a partial diﬀerential equation whose solution determines the potential
at various points in space.2 In many situations the density in the region of
interest is zero. Then the RHS vanishes and we obtain an important special
case of the above equation called Laplace’s equation:
Laplace’s equation
∇2Φ(r) = 0.
(15.15)
Consider a ﬁxed point P in space with Cartesian coordinates (x0, y0, z0)
and position vector r0. Take another (variable) point with Cartesian coordi-
nates (x, y, z) and position vector r. By direct diﬀerentiation, one can verify
that
∇·
 r −r0
|r −r0|3

= 0
at all points of space except at r = r0 for which the vector is not deﬁned.
Moreover, if S is any closed surface bounding a volume V , we have
# #
S
 r −r0
|r −r0|3

· da ≡ΩS
P =
0
4π
if P is in V,
0
if P is not in V,
by Theorem 12.1.2. On the other hand, the divergence theorem relates the
LHS of this equation with the volume integral of divergence. Thus,
# #
V
#
∇·

r −r0
|r −r0|3

dV =
0
4π
if P is in V,
0
if P is not in V.
(15.16)
2The reader should consider this, and any other diﬀerential equation, as a local equation,
meaning that the derivatives on the LHS and the quantities on the RHS are to be evaluated
at the same point.

412
Applied Vector Analysis
This shows that ∇· [(r −r0)/|r −r0|3] has the property that it is zero every-
where except at P, but whose volume integral is not zero. This is reminiscent
of the three-dimensional Dirac delta function. In fact, it follows from Equation
(15.16) that
∇·

r −r0
|r −r0|3

= 4πδ(r −r0).
(15.17)
Using Equation (15.2) and the deﬁnition of Laplacian, we also get
relation between
Laplacian and
Dirac delta
function
∇2

1
|r −r0|

= −4πδ(r −r0).
(15.18)
The last double-del operation we consider is
∇× (∇× A) = ∇(∇· A) −∇2A
(15.19)
which holds only in Cartesian coordinates and can be veriﬁed component by
component.
Example 15.3.1. Angular Momentum Operator
In quantum mechanics,
the angular momentum L = r × p becomes the diﬀerential operator L = −iℏr × ∇,
where ℏis the reduced Planck constant, which we set equal to 1 in the following
discussion. The quantity L2 ≡|L|2 appears frequently in applications of quantum
mechanics. It is therefore instructive to compute this quantity.
Since L2 is a diﬀerential operator, we let it act on some function f and carry
out the diﬀerentiation until we get a simple result. Since
L2 = L2
x + L2
y + L2
z,
we let each component act on f separately. First note that
Lxf = −i (r × ∇f)x = −i

y ∂f
∂z −z ∂f
∂y

Lyf = −i (r × ∇f)y = −i

z ∂f
∂x −x∂f
∂z

(15.20)
Lzf = −i (r × ∇f)z = −i

x∂f
∂y −y ∂f
∂x

Therefore,
−L2
xf =

y ∂
∂z −z ∂
∂y
 
y ∂f
∂z −z ∂f
∂y

= y2 ∂2f
∂z2 + z2 ∂2f
∂y2 −y ∂f
∂y −z ∂f
∂z −2yz ∂2f
∂y∂z
. Similarly,
−L2
yf = x2 ∂2f
∂z2 + z2 ∂2f
∂x2 −x∂f
∂x −z ∂f
∂z −2xz ∂2f
∂x∂z ,
and
−L2
zf = x2 ∂2f
∂y2 + y2 ∂2f
∂x2 −x∂f
∂x −y ∂f
∂y −2xy ∂2f
∂x∂y .

15.3 Laplacian
413
Adding the three components and using a little algebra, we get
−L2f = r2∇2f −

x2 ∂2f
∂x2 + y2 ∂2f
∂y2 + z2 ∂2f
∂z2

−2r · (∇f) −2

yz ∂2f
∂y∂z + xz ∂2f
∂x∂z + xy ∂2f
∂x∂y

.
(15.21)
Let A denote the sum of the two expressions in the large parentheses. We can write
A in a compact form by expanding (r · ∇)(r · ∇f):
(r · ∇)2f ≡(r · ∇)(r · ∇f) =

x ∂
∂x + y ∂
∂y + z ∂
∂z
 
x∂f
∂x + y ∂f
∂y + z ∂f
∂z

= x∂f
∂x + x2 ∂2f
∂x2 + xy ∂2f
∂x∂y + xz ∂2f
∂x∂z



comes from x diﬀerentiation
+terms from y and z diﬀerentiation.
Adding the terms from x, y, and z diﬀerentiations we obtain
(r · ∇)2f = r · (∇f) + A
or
A = (r · ∇)2f −r · (∇f).
Substituting this in (15.21) yields
L2f = −r2∇2f + r · (∇f) + (r · ∇)2f.
(15.22)
As a diﬀerential operator, L2 is written as
L2 = −r2∇2 + r · ∇+ (r · ∇)2.
(15.23)
We shall come back to this discussion in Chapter 17 to show how index manipulation
eases the calculation (see Example 17.3.3).
■
15.3.1
A Primer of Fluid Dynamics
We have already talked about the ﬂow of a ﬂuid in Section 13.2.3, where
we derived the continuity equation, which states the conservation of mass in
mathematical terms. We now want to take up the dynamics of a ﬂuid, i.e.,
the motion of various parts of the ﬂuid due to the forces acting on them.
Consider a volume V of the ﬂuid bounded by a surface S. The pressure p
exerted from outside at any point of S in the element of area da is normal to
S at that point and pointing into the volume V . Thus, the element of force
due to pressure is −pda. If pressure is the only source of force on the volume
V of the ﬂuid, then the total force on V is
F = −
# #
S
p da.
Using Equation (13.12), we rewrite this as
F = −
# #
S
p da = −
# #
V
#
∇p dV.

414
Applied Vector Analysis
This shows that ∇p is a force density, whose volume integral gives the force.
If the density of the ﬂuid is ρ and the mass element dm in V has velocity v,
then the “mass time acceleration” is dm dv/dt = ρ dV (dv/dt), and the total
“mass time acceleration” is the volume integral of this quantity. If there are
other forces acting on the ﬂuid described by a force density f, we can add it
to the right-hand side. Thus, Newton’s second law of motion gives
# #
V
#
ρ(dv/dt) dV = −
# #
V
#
∇p dV +
# #
V
#
f dV,
and this holds for any volume V , in particular for an inﬁnitesimal volume for
which the integrals become the integrand. Hence, the second law of motion
for the ﬂuid is
ρ(dv/dt) = −∇p + f.
(15.24)
The total time derivative of velocity is
dv
dt = ∂v
∂t + ∂v
∂x
dx
dt + ∂v
∂y
dy
dt + ∂v
∂z
dz
dt = ∂v
∂t + (v · ∇)v.
Substituting this in (15.24) and dividing by ρ yields
Euler’s equation of
ﬂuid dynamics
∂v
∂t + (v · ∇)v = −∇p + f
ρ
.
(15.25)
This is Euler’s equation and is one of the fundamental equations of ﬂuid
dynamics.
The force density f in Euler’s equation is usually that of the gravitational
force. Since the gravitational force on an element ρ dV is gρ dV , where g is
the gravitational acceleration (or ﬁeld), the gravitational force density is ρg
and (15.25) becomes
∂v
∂t + (v · ∇)v = −∇p
ρ
+ g.
(15.26)
Example 15.3.2. In hydrostatic situations with a uniform gravitational ﬁeld the
ﬂuid is not moving and Equation (15.26) becomes
∇p = ρg,
and if g is in the negative z-direction, then
∂p
∂x = ∂p
∂y = 0,
∂p
∂z = −ρg.
Thus the pressure is independent of x and y, and depends only on height z. We
assume that the ﬂuid (really the liquid) is incompressible, meaning that its density
does not depend on the pressure. Then, integrating the z equation gives
p = −ρgz + C.
If the liquid has a free surface at z = h where the pressure is p0, then C = p0 + ρgh,
and
p = p0 + ρg(h −z).
■

15.4 Maxwell’s Equations
415
Example 15.3.3. Stellar equilibrium
A star is a large mass of ﬂuid held
together by gravitational attraction. If the star is in equilibrium, its ﬂuid has no
motion and (15.26) becomes
∇p = ρg
or
∇p = −ρ∇Φ
where Φ is the gravitational potential. Dividing this equation by ρ, and taking the
divergence of both sides, we obtain
∇·
 ∇p
ρ

= −∇2Φ
or
∇·
 ∇p
ρ

= 4πGρ
where we used the Poisson equation (15.14). For a spherically symmetric star, only
equation for stellar
equilibrium
the radial coordinate enters in the equation above, and borrowing from the next
chapter the expressions (16.7) for gradient and (16.12) for divergence in spherical
coordinates, the equation above takes the form
1
r2
d
dr
 r2
ρ
dp
dr

= 4πGρ
This is one of the fundamental equations of astrophysics.
■
15.4
Maxwell’s Equations
No treatment of vector analysis is complete without a discussion of Maxwell’s
equations.
Electromagnetism was both the producer and the consumer of
vector analysis. It started with the accidental discovery by ¨Orsted in 1820
that an electric current produced a magnetic ﬁeld. Subsequently, an intense
search was undertaken by many physicists such as Amp`ere and Faraday to
ﬁnd a connection between electric and magnetic phenomena. By the mid-
1800s, a fairly good theory of electromagnetism was attained which, in the
contemporary language of vectors is translated in the following four equations:
the four equations
that Maxwell
inherited in
integral form
(1)
# #
S
E · da = Q
ϵ0
;
(2)
# #
S
B · da = 0;
(3)
2
C
E · dr = −dφm
dt ;
(4)
2
C
B · dr = μ0I.
(15.27)
The ﬁrst integral, Gauss’s law (or Coulomb’s law in disguise), states that the
electric ﬂux through the closed surface S is essentially the total charge Q in
the volume surrounded by S. The second integral says that the correspond-
ing ﬂux for a magnetic ﬁeld is zero. The fact that this holds for an arbitrary
surface implies that there are no magnetic charges. The third equation, Fara-
day’s law, connects the electric ﬁeld to the rate of change of magnetic ﬂux
φm. Finally, the last equation, Amp`ere’s law, states that the source of the
magnetic ﬁeld is the electric current I. The constant ϵ0 and μ0 arise from a
particular set of units used for charges and currents.

416
Applied Vector Analysis
15.4.1
Maxwell’s Contribution
Equations (15.27) can be cast in diﬀerential form as well. The diﬀerential
form of the equations is important because it places particular emphasis on
the ﬁelds which are the primary objects. The diﬀerential form of the equations
above are:
the four equations
that Maxwell
inherited in
diﬀerential form
(1) ∇· E = ρ
ϵ0
;
(2) ∇· B = 0;
(3) ∇× E = −∂B
∂t ;
(4) ∇× B = μ0J.
(15.28)
We have already derived the ﬁrst two equations in Theorem 13.2.4 and Equa-
tion (15.9). Here we derive the third equation and leave the derivation of
the last equation—which is very similar to that of the third—to the reader.
Stokes’ theorem turns the LHS of the third equation of (15.27) into
LHS =
# #
S
∇× E · da.
The RHS is
−dφm
dt
= −d
dt
# #
S
B · da =
# #
S

−∂B
∂t

· da,
where we have assumed that the change in the ﬂux comes about solely due
to a change in the magnetic ﬁeld. This makes it possible to push the time
diﬀerentiation inside the integral, upon which it becomes a partial derivative
because B is a function of position as well. Since the last two equations hold
for arbitrary S, the integrands must be equal. This proves the third equation
in (15.28).
Maxwell inherited the four equations in (15.28), and started pondering
about them in the 1860s. He noticed that while the second and third are
Maxwell discovers
the inconsistency
of Equation
(15.28) with the
conservation of
electric charge,
and modiﬁes the
last equation to
resolve the
inconsistency.
consistent with other aspects of electromagnetism, the other two equations
lead to a contradiction. Let us retrace his argument. By Equation (15.5),
the divergence of the LHS of the last equation of (15.28) vanishes. Therefore,
taking the divergence of both sides, we get ∇· J = 0. This contradicts the
diﬀerential form of the continuity equation (13.22) for charges which expresses
the conservation of electric charge. Because of the ﬁrm establishment of the
charge conservation, Maxwell decided to try altering the four equations to
make them compatible with charge conservation.
The clue is in the ﬁrst
equation. If we diﬀerentiate that equation with respect to time, we obtain
∂
∂t∇· E = 1
ϵ0
∂ρ
∂t
⇒∇·
∂E
∂t

= 1
ϵ0
∂ρ
∂t
⇒∇·

ϵ0
∂E
∂t

= ∂ρ
∂t
This suggested to Maxwell that, if the four equations are to be consistent
with charge conservation, the fourth equation had to be modiﬁed to include
ϵ0∂E/∂t. With this modiﬁcation, the four equations in (15.28) become
the four Maxwell
equations

15.4 Maxwell’s Equations
417
(1) ∇· E = ρ
ϵ0
;
(2) ∇· B = 0;
(3) ∇× E = −∂B
∂t ;
(4) ∇× B = μ0J + μ0ϵ0
∂E
∂t .
(15.29)
It was a great moment in the history of physics and mathematics when
Maxwell, prompted solely by the forces of logic and pure deduction, intro-
duced the second term in the last equation. Such moments were rare prior
mathematics and
the force of logic
and human
reasoning unravel
one of the greatest
secrets of Nature!
to Maxwell, and with the exception of Copernicus’s introduction of the he-
liocentric theory of the solar system and Descartes’s introduction of analytic
geometry, deductive reasoning was the exception rather than the rule. The-
ories and laws were empirical (or inductive); they were introduced to ﬁt the
data and summarize, more or less directly, the numerous observations made.
Maxwell broke this tradition and set the stage for deductive reasoning which,
after a great deal of struggle to abandon the inductive tradition, became the
norm for modern physics.
Today, we aptly call all four equations in (15.29) Maxwell’s equations,
although his contribution to those equations was a “mere” introduction of
the second term on the RHS of the last equation. However, no other “small”
contribution has ever aﬀected humankind so enormously. This very “small”
contribution was responsible for Maxwell’s prediction of the electromagnetic
waves which were subsequently produced in the laboratory in 1887—only eight
years after Maxwell’s premature death—and put to technological use in 1901
in the form of the ﬁrst radio. Today, Maxwell’s equations are at the heart of
every electronic device. Without them, our entire civilization, as we know it,
would be nonexistent.
15.4.2
Electromagnetic Waves in Empty Space
Let us look at some of the implications of Maxwell equations. Taking the curl
from Maxwell’s
equations to wave
equation
of the third Maxwell’s equation and using (15.19) and the ﬁrst and fourth
equations of (15.29), we obtain for the LHS
LHS = ∇× (∇× E) = ∇(∇· E) −∇2E = 1
ϵ0
∇ρ −∇2E,
and for the RHS
RHS = −∇×
∂B
∂t

= −∂
∂t(∇× B) = −∂
∂t

μ0J + μ0ϵ0
∂E
∂t

.
In particular, in free space, where ρ = 0 = J, these equations give
∇2E −μ0ϵ0
∂2E
∂t2 = 0.
(15.30)

418
Applied Vector Analysis
This is a three-dimensional wave equation.3 Recall that the inverse of the co-
eﬃcient of the second time derivative is the square of the speed of propagation
of the wave. It follows that
v =
1
√μ0ϵ0
=
1
	
(4π × 10−7) (8.854 × 10−12)
= 2.998 × 108 m/s,
i.e., that the electric ﬁeld propagates in empty space with the speed of light,
c. The reader may check that the magnetic ﬁeld also satisﬁes the same wave
equation, and that it too propagates with the same speed. In fact, it can be
shown that the so-called plane wave solutions of Maxwell’s equations consist
of an electric and a magnetic component which are coupled to one another
electromagnetic
waves propagate
at the speed of
light.
and, therefore do not propagate independently (see Problem 15.9).
Sometimes it is more convenient to work with potentials than the ﬁelds
themselves. The vanishing of the divergence of magnetic ﬁelds suggests that
B = ∇× A where A is the vector potential [see also Equation (15.6)]. The
vector potential, as its scalar counterpart, has some degree of arbitrariness,
because adding the gradient of an arbitrary function does not change its curl.
This is an example of gauge transformation whereby a measurable physical
gauge
transformation
quantity—the magnetic ﬁeld, here—does not change when another (nonmea-
surable) physical quantity is changed. Using this expression for B in the third
Maxwell equation, we obtain
∇× E = −∂
∂t(∇× A) ⇒∇×

E + ∂A
∂t

= 0 ⇒E + ∂A
∂t = −∇Φ,
where we switched the order of diﬀerentiation with respect to position and
time, and used the fact that if the curl of a vector vanishes, that vector is the
gradient of a function (Box 15.1.1). We therefore write
E = −∂A
∂t −∇Φ
and
B = ∇× A.
(15.31)
Substituting these two expressions in the fourth Maxwell equation, we obtain
∇× (∇× A) = μ0J + 1
c2
∂
∂t

−∂A
∂t −∇Φ

.
Expanding the LHS using the double curl identity of Equation (15.19), and
switching time and space partial derivatives yields
∇

∇· A + 1
c2
∂Φ
∂t

−∇2A + 1
c2
∂2A
∂t2 = μ0J.
Because of the gauge freedom, we can choose A and Φ to satisfy
∇· A + 1
c2
∂Φ
∂t = 0.
(15.32)
3The reader may be familiar with the one-dimensional wave equation in which only one
second partial derivative with respect to a single space coordinate appears.

15.4 Maxwell’s Equations
419
This choice is called the Lorentz gauge, from which it follows that
Lorentz gauge
∇2A −1
c2
∂2A
∂t2 = −μ0J.
(15.33)
Similarly, by taking the divergence of the ﬁrst equation in (15.31) and using
the ﬁrst Maxwell equation and the Lorentz gauge, we obtain
∇2Φ −1
c2
∂2Φ
∂t2 = −ρ
ϵ0
.
(15.34)
Equations (15.32), (15.33), and (15.34) are the fundamental equations of elec-
tromagnetic theory. They not only give the solutions in empty space, where
ρ = J = 0, but also when the sources are not zero, i.e., when the mechanism
of wave production becomes of interest, as in radiation and antenna theory.
Historical Notes
James Clerk Maxwell attended Edinburgh Academy where he had the nickname
“Dafty.” While still at school he had two papers published by the Royal Society of
Edinburgh. Maxwell then went to Peterhouse, Cambridge, but moved to Trinity,
where it was easier to obtain a fellowship.
Maxwell graduated with a degree in
mathematics from Trinity College in 1854.
He held chairs at Marischal College in Aberdeen (1856) and married the daughter
of the Principal. However in 1860 Marischal College and King’s College combined
and Maxwell, as the junior of the department, had to seek another post. After failing
to gain an appointment to a vacant chair at Edinburgh he was appointed to King’s
College in London (1860) and became the ﬁrst Cavendish Professor of Physics at
Cambridge in 1871.
James Clerk
Maxwell 1831–1879
Maxwell’s ﬁrst major contribution to science was a study of the planet Sat-
urn’s rings, and won him the Adams Prize at Cambridge. He showed that stability
could be achieved only if the rings consisted of numerous small solid particles, an
explanation now conﬁrmed by the Voyager spacecraft.
Maxwell next considered the kinetic theory of gases. By treating gases statis-
tically in 1866 he formulated, independently of Ludwig Boltzmann, the Maxwell–
Boltzmann kinetic theory of gases. This theory showed that temperatures and heat
involved only molecular movement.
This theory meant a change from a concept of certainty, heat viewed as ﬂowing
from hot to cold, to one of statistics, molecules at high temperature have only a
high probability of moving toward those at low temperature. Maxwell’s approach
did not reject the earlier studies of thermodynamics but used a better theory of the
basis to explain the observations and experiments.
Maxwell’s most important achievement was his extension and mathematical for-
mulation of Michael Faraday’s theories of electricity and magnetic lines of force. His
paper On Faraday’s lines of force was read to the Cambridge Philosophical Society
in two parts, 1855 and 1856. Maxwell showed that a few relatively simple math-
ematical equations could express the behavior of electric and magnetic ﬁelds and
their interrelation.
The four partial diﬀerential equations, now known as Maxwell’s equations,
ﬁrst appeared in fully developed form in Treatise on Electricity and Magnetism
(1873). They are one of the great achievements of nineteenth-century mathematical

420
Applied Vector Analysis
physics. Solving these equations Maxwell predicted the existence of electromagnetic
waves and the fact that these waves propagate at the speed of light (1862).
He
proposed that the phenomenon of light is therefore an electromagnetic phenomenon.
Maxwell left King’s College, London, in the spring of 1865 and returned to
his Scottish estate. He made periodic trips to Cambridge and, rather reluctantly,
accepted an oﬀer from Cambridge to be the ﬁrst Cavendish Professor of Physics in
1871. He designed the Cavendish laboratory and helped set it up.
15.5
Problems
15.1. Show that the curl of the gradient of a function is always zero.
15.2. Show that the divergence of the curl of a vector is always zero.
15.3. Verify Equation (15.19) component by component.
15.4. Provide the details of Example 15.3.1:
(a) Compute the three components of L and verify Equation (15.20).
(b) Calculate L2
xf, L2
yf, L2
zf and show that you obtain the expressions given
in the example.
(c) Verify that L2f is as given in Equation (15.21).
(d) Show that A = (r · ∇)2f −r · (∇f) and obtain (15.22). Here A is deﬁned
by the sum of the expressions in the two pairs of parentheses in Equation
(15.21)
15.5. By taking each component of dr′ separately in a convenient coordinate
system show that its integral round any closed loop vanishes.
15.6. Recall that the total magnetic force on a current loop is given by
total magnetic
force on a current
loop in a constant
magnetic ﬁeld is
zero.
F = I
2
dr × B.
Show that the total force on a current loop located in a homogeneous magnetic
ﬁeld is zero.
15.7. Derive the diﬀerential form of Maxwell’s last equation from the corre-
sponding integral form.
15.8. Starting with Maxwell’s equations, show that the magnetic ﬁeld satis-
ﬁes the same wave equation as the electric ﬁeld. In particular, that it, too,
propagates with the same speed.
15.9. Consider E = E0ei(ωt−k·r) and B = B0ei(ωt−k·r), where i = √−1, E0,
B0, k, and ω are constants. The E and the B so deﬁned represent plane waves
moving in the direction of the vector k.
(a) Show that they satisfy Maxwell’s equations in free space if:
(1) k · E0 = 0;
(2) k · B0 = 0;
(3) k × E0 = ωB0;
(4) k × B0 = −ω
c2 E0.

15.5 Problems
421
(b) In particular, show that k, the propagation direction, and E and B form
a mutually perpendicular set of vectors.
(c) By taking the cross product of k with an appropriate equation, show that
|k| = ω/c.
15.10. Derive Equation (15.34).

Chapter 16
Curvilinear Vector
Analysis
All the vector analytical quantities discussed in the previous chapters can
also be calculated in other coordinate systems. The general procedure is to
start with deﬁnitions of quantities in a coordinate-free way and substitute the
known quantities in terms of the particular coordinates we are interested in
and “read oﬀ” the vector analytic quantity. Instead of treating cylindrical and
spherical coordinate systems separately, we lump them together and derive re-
lations that hold not only in the three familiar coordinate systems, but also in
all coordinate systems whose unit vectors form a set of right-handed mutually
perpendicular vectors. Since the geometric deﬁnitions of all vector-analytic
quantities involve elements of length, we start with the length elements.
16.1
Elements of Length
Consider curvilinear coordinates1 (q1, q2, q3) in which the primary line
curvilinear
coordinates
elements are given by
dl1 = h1(q1, q2, q3) dq1, dl2 = h2(q1, q2, q3) dq2, dl3 = h3(q1, q2, q3) dq3,
where h1, h2, and h3 are some functions of coordinates. By examining the
primary line elements in Cartesian, spherical, and cylindrical coordinates, we
can come up with Table 16.1.
Denoting the unit vectors in curvilinear coordinate systems by ˆe1, ˆe2, and
ˆe3, we can combine all the equations for the elements of length and write
them as a single vector equation:
dr = d⃗l = ˆe1dl1 + ˆe2dl2 + ˆe3dl3 = ˆe1h1dq1 + ˆe2h2dq2 + ˆe3h3dq3.
(16.1)
1As will be seen shortly, Cartesian coordinates are also included in such curvilinear
coordinates. The former have lines (and planes) as their primary lengths and surfaces, thus
the word “linear” in the name of the latter.

424
Curvilinear Vector Analysis
Curvilinear
Cartesian
Spherical
Cylindrical
q1
x
r
ρ
q2
y
θ
ϕ
q3
z
ϕ
z
h1
1
1
1
h2
1
r
ρ
h3
1
r sin θ
1
Table 16.1: The speciﬁcations of the three coordinate systems in terms of curvilinear
coordinates.
This equation is useful in its own right. For example, we can obtain the curvi-
linear unit vectors as follows. Rewrite Equation (16.1) in terms of increments:
Δr ≈ˆe1h1Δq1 + ˆe2h2Δq2 + ˆe3h3Δq3.
Keeping q2 and q3 constant (so that Δq2 = 0 = Δq3), divide both sides by
Δq1 to obtain
Δr
Δq1
≈ˆe1h1.
In the limit, the LHS becomes a partial derivative and we get
ˆe1 = 1
h1
∂r
∂q1
.
(16.2)
The other two unit vectors can be obtained similarly. We thus have
Box 16.1.1. The ith unit vector of a curvilinear coordinate system is
given by
ˆei = 1
hi
∂r
∂qi
,
i = 1, 2, 3.
(16.3)
This is a useful formula for obtaining the Cartesian components of curvilinear
unit vectors, when the Cartesian components of the position vector are given
in terms of curvilinear coordinates.
Example 16.1.1. As an illustration of the above procedure, we calculate the unit
vectors in spherical coordinates. First we write
r = xˆex + yˆey + zˆez = ˆexr sin θ cos ϕ + ˆeyr sin θ sin ϕ + ˆezr cos θ.
Now we diﬀerentiate with respect to r to get
ˆe1 ≡ˆer = ∂r
∂r = ˆex sin θ cos ϕ + ˆey sin θ sin ϕ + ˆez cos θ.

16.2 The Gradient
425
Similarly,
ˆe2 ≡ˆeθ = 1
r
∂r
∂θ = ˆex cos θ cos ϕ + ˆey cos θ sin ϕ −ˆez sin θ,
ˆe3 ≡ˆeϕ =
1
r sin θ
∂r
∂ϕ = −ˆex sin ϕ + ˆey cos ϕ,
where we have used Table 16.1. These are the results we obtained in Chapter 1 from
purely geometric arguments.
■
We are now in a position to ﬁnd the gradient, divergence, and curl of
a vector ﬁeld in general curvilinear coordinates. Once these are obtained,
ﬁnding their speciﬁc forms in cylindrical and spherical coordinates entails
simply substituting the appropriate expressions for q1, q2, and q3 and h1, h2,
and h3.
16.2
The Gradient
The gradient is found by equating
df = ∂f
∂q1
dq1 + ∂f
∂q2
dq2 + ∂f
∂q3
dq3
to the diﬀerential of f in terms of the gradient:
df = ∇f · dr = (∇f)1h1 dq1 + (∇f)2h2 dq2 + (∇f)3h3 dq3.
The last two equations yield
(∇f)1h1 = ∂f
∂q1
,
(∇f)2h2 = ∂f
∂q2
,
(∇f)3h3 = ∂f
∂q3
,
which gives
gradient in
curvilinear
coordinates
Box 16.2.1. The gradient of a function f in a curvilinear coordinate
system is given by
∇f = ˆe1
1
h1
∂f
∂q1
+ ˆe2
1
h2
∂f
∂q2
+ ˆe3
1
h3
∂f
∂q3
.
(16.4)
This result, in conjunction with Table 16.1, agrees with the expression ob-
tained for the gradient in the Cartesian coordinate system.
In cylindrical
coordinates, we obtain
∇f = ˆeρ
∂f
∂ρ + ˆeϕ
1
ρ
∂f
∂ϕ + ˆez
∂f
∂z ,
(16.5)

426
Curvilinear Vector Analysis
so that the operator ∇in cylindrical coordinates is given by
∇= ˆeρ
∂
∂ρ + ˆeϕ
1
ρ
∂
∂ϕ + ˆez
∂
∂z .
(16.6)
Similarly, in spherical coordinates, we get
gradient of a
function in
spherical
coordinates
∇f = ˆer
∂f
∂r + ˆeθ
1
r
∂f
∂θ + ˆeϕ
1
r sin θ
∂f
∂ϕ
(16.7)
with the operator ∇given by
∇= ˆer
∂
∂r + ˆeθ
1
r
∂
∂θ + ˆeϕ
1
r sin θ
∂
∂ϕ.
(16.8)
Example 16.2.1. The electrostatic potential of an electric dipole was given in
Example 10.5.1 in spherical coordinates. With the expression for the gradient given
above, we can ﬁnd the electric ﬁeld E = −∇Φ of a dipole in spherical coordinates:
Er = −∂Φdip
∂r
= −∂
∂r
kep cos θ
r2

= 2kep cos θ
r3
,
Eθ = −1
r
∂Φdip
∂θ
= −1
r
∂
∂θ
kep cos θ
r2

= kep sin θ
r3
,
Eϕ = −
1
r sin θ
∂Φdip
∂ϕ
= −
1
r sin θ
∂
∂ϕ
 kep cos θ
r2

= 0.
Summarizing, we have
electric ﬁeld of an
electric dipole
Edip = kep
r3 (2ˆer cos θ + ˆeθ sin θ).
(16.9)
This is the characteristic ﬁeld of a dipole.
■
Example 16.2.2. Just as electric charges can produce electric dipoles, electric
currents can produce magnetic dipoles. We saw this in Subsection 15.2. In this
example, we will calculate the magnetic ﬁeld of a dipole directly.
Consider the
magnetic ﬁeld of a circular loop of current as given in Equations (4.24) and (4.26).
We change the coordinates of the ﬁeld point P to spherical and assume that P is far
away from the loop, i.e., that a is very small compared to r. Writing r2 for ρ2 + z2
and r sin θ for ρ, we expand the integrands of (4.24) and (4.26) in powers of a/r
keeping only the ﬁrst nonzero power. Thus,
1
(r2+a2−2ra sin θ cos t)3/2 = 1
r3

1 +
a
r
2
−2
 a
r

sin θ cos t
−3/2
= 1
r3
*
1 + 3
a
r

sin θ cos t
+
+ · · · ,
r sin θ cos t−a
(r2+a2−2ra sin θ cos t)3/2 = 1
r2

sin θ cos t −a
r
 
1 +
 a
r
2
−2
 a
r

sin θ cos t
−3/2
= 1
r2

sin θ cos t −a
r
 *
1 + 3
 a
r

sin θ cos t
+
+ · · ·
= 1
r2

sin θ cos t −a
r + 3a
r sin2 θ cos2 t

.

16.3 The Divergence
427
Substituting these in the integrals of (4.24) and (4.26) yields
Bρ = kmIaz
r3
# 2π
0
cos t

1 + 3a
r sin θ cos t

dt = 3kmIπa2 cos θ sin θ
r3
,
where we substituted r cos θ for z. In an analogous way, we also obtain
Bz = −kmIa
r2
# 2π
0

sin θ cos t −a
r + 3a
r sin2 θ cos2 t

dt
= −kmIa
r2

−2πa
r
+ 3aπ
r
sin2 θ

.
We are interested in the spherical components of the magnetic ﬁeld. To ﬁnd
these components, we ﬁrst write
B = Bρˆeρ + Bzˆez
and take the dot product with appropriate unit vectors:
Br = B · ˆer = Bρˆeρ · ˆer + Bzˆez · ˆer = Bρ sin θ + Bz cos θ
= 3kmIπa2 cos θ sin θ
r3
sin θ + kmIa
r2
2πa
r
−3aπ
r
sin2 θ

cos θ
= 2kmIπa2
r3
cos θ.
Similarly,
Bθ = B · ˆeθ = Bρˆeρ · ˆeθ + Bzˆez · ˆeθ = Bρ cos θ −Bz sin θ
= 3kmIπa2 cos θ sin θ
r3
cos θ −kmIa
r2
 2πa
r
−3aπ
r
sin2 θ

sin θ
= kmIπa2
r3
sin θ.
Summarizing, we write
magnetic ﬁeld of a
magnetic dipole
B = kmIπa2
r3
(2ˆer cos θ + ˆeθ sin θ).
(16.10)
This has a striking resemblance to Equation (16.9). In fact once we identify Iπa2
as the magnetic dipole of the loop, and change all magnetic labels to electric ones,
we recover Equation (16.9).
■
16.3
The Divergence
To ﬁnd the divergence of a vector A, we consider the volume element of
Figure 16.1 and ﬁnd the outward ﬂux through the sides of the volume. For
the front face we have
Δφf = Af · ˆe1Δaf,

428
Curvilinear Vector Analysis
A
A1 ˆ e 1
A2ˆe 2
A3ˆe 3
P(q1, q2, q3)
Δ
Δ
Δ
l1
l3
l2
Figure 16.1: Point P and the surrounding volume element in curvilinear coordinates.
Note that the midpoints of the front and back faces are Δq1/2 away from P in the
positive and negative ˆe1 directions, respectively. Similarly for the other four faces.
where Af means the value of A at the center of the front face and Δaf is the
area of the front face. Following the arguments presented for the Cartesian
case, we write
Δφf ≈Af · ˆe1Δaf = A1fΔl2fΔl3f
= A1f(h2Δq2)f(h3Δq3)f = A1fh2fh3fΔq2Δq3
The subscript 1 in A1f, for example, means component of A in the direction
of the ﬁrst coordinate. The subscript f implies evaluation—at the midpoint—
on the front side whose second and third coordinates are the same as P, and
whose ﬁrst coordinate is q1 + Δq1/2. Thus, we have
Δφf ≈A1

q1 + Δq1
2 , q2, q3

h2

q1 + Δq1
2 , q2, q3

× h3

q1 + Δq1
2 , q2, q3

Δq2Δq3
because, unlike the Cartesian case, h1, h2, and h3 are functions of the co-
ordinates. Using Taylor series expansion for the functions A1, h2, and h3
yields
Δφf ≈
0
A1(q1, q2, q3) + Δq1
2
∂A1
∂q1
10
h2(q1, q2, q3) + Δq1
2
∂h2
∂q1
1
×
0
h3(q1, q2, q3) + Δq1
2
∂h3
∂q1
1
Δq2Δq3.

16.3 The Divergence
429
Multiplying out and keeping terms up to the third order (corresponding to
the order of a volume element by which we shall divide shortly), we obtain
Δφf ≈
0
A1h2h3 + A1h2
∂h3
∂q1
+ A1h3
∂h2
∂q1
+ h2h3
∂A1
∂q1
1
Δq1
2 Δq2Δq3
=
0
A1h2h3 + ∂
∂q1
(h2h3A1)
1
Δq1
2 Δq2Δq3,
where we left out the explicit dependence of the functions on their independent
coordinate variables. For the back face we have
Δφb ≈Ab · (−ˆe1Δab) = −A1bΔl2bΔl3b = −A1b(h2Δq2)b(h3Δq3)b
= −A1

q1 −Δq1
2 , q2, q3

h2

q1 −Δq1
2 , q2, q3

× h3

q1 −Δq1
2 , q2, q3

Δq2Δq3.
Taylor expanding the three functions A1, h2, and h3 as above, and multiplying
out yields
Δφb ≈−
0
A1h2h3 −∂
∂q1
(h2h3A1)
1
Δq1
2 Δq2Δq3.
Adding the front and back contributions, we obtain
Δφ1 ≡Δφf + Δφb ≈
∂
∂q1
(h2h3A1) Δq1Δq2Δq3.
Similarly, the ﬂuxes through the faces perpendicular to ˆe2 and ˆe3 are
Δφ2 ≈
∂
∂q2
(h1h3A2) Δq1Δq2Δq3,
Δφ3 ≈
∂
∂q3
(h1h2A3) Δq1Δq2Δq3.
(16.11)
Adding the three contributions and dividing by the volume
ΔV = Δl1Δl2Δl3 = h1h2h3Δq1Δq2Δq3
and ﬁnally taking the limit of smaller and smaller volumes—which turns all
approximations into equalities—we get
divergence in
curvilinear
coordinates
Theorem 16.3.1. The divergence of a vector ﬁeld A in a curvilinear coordi-
nate system is given by
∇· A =
1
h1h2h3
0
∂
∂q1
(h2h3A1) + ∂
∂q2
(h1h3A2) + ∂
∂q3
(h1h2A3)
1
.

430
Curvilinear Vector Analysis
Now that we have a general formula for the divergence, we can use Table
16.1 to write the divergence in a speciﬁc coordinate system. For instance,
substituting the entries of the second column gives the formula in Theorem
13.2.1, and the third column yields
divergence of a
vector ﬁeld in
spherical
coordinates
∇· A =
1
r2 sin θ
( ∂
∂r
 
r2 sin θAr
!
+ ∂
∂θ (r sin θAθ) + ∂
∂ϕ (rAϕ)
)
= 1
r2
∂
∂r
 
r2Ar
!
+
1
r sin θ
0
∂
∂θ (sin θAθ) + ∂Aϕ
∂ϕ
1
.
(16.12)
To obtain the divergence in cylindrical coordinates, we use the last column
and obtain
∇· A = 1
ρ
0
∂
∂ρ (ρAρ) + ∂
∂ϕ (Aϕ) + ∂
∂z (ρAz)
1
= 1
ρ
∂
∂ρ (ρAρ) + 1
ρ
∂Aϕ
∂ϕ + ∂Az
∂z .
(16.13)
Example 16.3.2. Consider the vector ﬁeld deﬁned by
A = krαˆer,
where k and α are constants. Let us verify the divergence theorem for a spherical
surface of radius R (see Figure 16.2). The total ﬂux is obtained by integrating over
the surface of the sphere:
φ =
# #
S
A · da =
# #
S
kRαˆer · ˆenR2 sin θ dθ dϕ
= kRα+2
# #
S
sin θ dθ dϕ = 4πkRα+2.
dθ
dϕ
ˆe n = ˆe r
R
x
y
z
Figure 16.2: The element of area and its unit normal for a sphere.

16.4 The Curl
431
On the other hand, using the expression for divergence in the spherical coordinate
system and noting that Aθ = 0 = Aϕ, we obtain
∇· A = 1
r2
∂
∂r (r2Ar) = 1
r2
d
dr
 
krα+2!
= (α + 2)krα−1,
where we have assumed that α ̸= −2. Therefore,
# #
V
#
∇· A dV =
# R
0
(α + 2)krα−1r2 dr
# π
0
sin θ dθ
# 2π
0
dϕ = 4πkRα+2
which agrees with the surface integration.
For α = −2 the divergence appears to vanish everywhere. However, a closer
examination reveals that the statement is true only if r ̸= 0. In fact, as we discussed
before, the divergence of A is proportional to the Dirac delta function, δ(r) in this
case [see Equation (15.2)].
■
16.4
The Curl
To calculate the curl, we choose a closed path perpendicular to one of the unit
vectors, say ˆe1 and calculate the line integral of A around it. The situation is
depicted in Figure 16.3. We calculate the contribution to the line integral from
path (1) in detail and leave calculation of contributions from the remaining
three paths to the reader. In all calculations, terms of higher order than the
second will be omitted
#
(1)
A · dr ≈Al · Δrl = Al · (−ˆe3Δll) = −A3lΔll = −A3lh3lΔq3
= −A3

q1, q2 −Δq2
2 , q3

h3

q1, q2 −Δq2
2 , q3

Δq3
= −
(
A3 −Δq2
2
∂A3
∂q2
)(
h3 −Δq2
2
∂h3
∂q2
)
Δq3
≈−A3h3Δq3 + ∂
∂q2
(h3A3) Δq2
2 Δq3.
P(q1, q2, q3)
(1)
(2)
(3)
(4)
h2 b dq2
h2 t dq2
h3l dq3
h3r dq3
ˆe2
ˆe3
ˆe1
Figure 16.3: Path of integration for the ﬁrst component of the curl of A in curvilinear
coordinates.

432
Curvilinear Vector Analysis
Following similar steps, the reader may check that
#
(2)
A · dr ≈A3h3Δq3 + ∂
∂q2
(h3A3) Δq2
2 Δq3,
#
(3)
A · dr ≈A2h2Δq2 −∂
∂q3
(h2A2) Δq3
2 Δq2,
(16.14)
#
(4)
A · dr ≈−A2h2Δq2 −∂
∂q3
(h2A2) Δq3
2 Δq2.
Summing up all these contributions, we obtain
2
A · dr ≈
( ∂
∂q2
(h3A3) −∂
∂q3
(h2A2)
)
Δq2Δq3.
Dividing this by the area enclosed by the path
Δa = Δl2Δl3 = h2h3Δq2Δq3
we obtain the ﬁrst component, the component along the unit normal to the
area:
(∇× A)1 =
1
h2h3
( ∂
∂q2
(h3A3) −∂
∂q3
(h2A2)
)
.
Corresponding expressions for the other two components of the curl can
be found by proceeding as above. We can put all of the components together
in a mnemonic determinant form:
curl in curvilinear
coordinates
Theorem 16.4.1. The curl of a vector ﬁeld A in a curvilinear coordinate
system is given by
∇× A =
1
h1h2h3











ˆe1h1
ˆe2h2
ˆe3h3
∂
∂q1
∂
∂q2
∂
∂q3
h1A1
h2A2
h3A3











.
(16.15)
Note that ∇×A is not a cross product (except in Cartesian coordinates),
warning! ∇× A is
not a cross
product in general
curvilinear
coordinates!
but a vector deﬁned by the determinant on the RHS of (16.15).
If we substitute the appropriate values for h’s and q’s in spherical coordi-
nates, we obtain
∇× A =
1
r2 sin θ











ˆer
ˆeθr
ˆeϕr sin θ
∂
∂r
∂
∂θ
∂
∂ϕ
Ar
rAθ
r sin θAϕ











.
(16.16)

16.4 The Curl
433
In cylindrical coordinates we get
∇× A = 1
ρ











ˆeρ
ˆeϕρ
ˆez
∂
∂ρ
∂
∂ϕ
∂
∂z
Aρ
ρAϕ
Az











.
(16.17)
Example 16.4.2. We have already calculated the magnetic ﬁeld of a dipole in
Example 16.2.2. Here we want to obtain the same result using the vector potential
of a dipole given in Equation (15.12). We take μ to be along the z-axis. Then
μ = μˆez = μ(ˆer cos θ −ˆeθ sin θ)
and
μ × ˆer = μ(−sin θˆeθ × ˆer) = μ sin θˆeϕ.
Therefore,
B = ∇× A = ∇×
kmμ × ˆer
r2

= ∇×
kmμ sin θˆeϕ
r2

=
kmμ
r2 sin θ












ˆer
ˆeθr
ˆeϕr sin θ
∂
∂r
∂
∂θ
∂
∂ϕ
0
0
r sin θ sin θ
r2












=
kmμ
r2 sin θ












ˆer
ˆeθr
ˆeϕr sin θ
∂
∂r
∂
∂θ
∂
∂ϕ
0
0
sin2 θ
r












=
kmμ
r2 sin θ

ˆer
 2 sin θ cos θ
r

−rˆeθ

−sin2 θ
r2

= kmμ
r3 (2 cos θˆer + sin θˆeθ),
which is the expression obtained in Example 16.2.2.
■
Example 16.4.3. Consider the vector ﬁeld B described in cylindrical coordinates
as
B = k
ρ ˆeϕ,
where k is a constant. The curl of B is easily found to be zero:
∇× B = 1
ρ











ˆeρ
ˆeϕρ
ˆez
∂
∂ρ
∂
∂ϕ
∂
∂z
0
ρ(k/ρ)
0











= 0.
However, for any circle (of radius a, for example) centered at the origin and located
in the xy-plane, we get2
2
C
B · dr =
# 2π
0
k
a ˆeϕ · (ˆeϕa dϕ) = 2πk ̸= 0.
2See also Example 14.3.3 which discusses this same vector ﬁeld in Cartesian coordinates.

434
Curvilinear Vector Analysis
The reason for this result is that the circle is not contractible to zero: At the
origin—which is inside the circle and at which ρ = 0—B is not deﬁned.
This vector ﬁeld should look familiar.
It is the magnetic ﬁeld due to a long
straight wire carrying a current along the z-axis. According to Amp`ere’s circuital
law, the line integral of B along any closed curve encircling the wire, such as the
above circle, gives, up to a multiplicative constant, the current in the wire, and this
current is not zero.
■
Example 16.4.4. A vector ﬁeld that can be written as
central force ﬁelds
are conservative
F = f(r)r,
where r is the displacement vector from the origin, is conservative. It is instructive
to show this using both Cartesian and spherical coordinate systems.
First, in Cartesian coordinates
F = xf(r)ˆex + yf(r)ˆey + zf(r)ˆez
and the curl is
∇× F =











ˆex
ˆey
ˆez
∂
∂x
∂
∂y
∂
∂z
xf
yf
zf











= ˆex
( ∂
∂y (zf) −∂
∂z (yf)
)
+ ˆey
( ∂
∂z (xf) −∂
∂x(zf)
)
+ ˆez
( ∂
∂x(yf) −∂
∂y (xf)
)
.
Concentrating on the x-component ﬁrst and using the chain rule, we have
∂
∂y (zf) = z ∂f
∂y = z df
dr
∂r
∂y = zf ′ ∂r
∂y .
But
∂r
∂y = ∂
∂y
	
x2 + y2 + z2 = y
r .
Thus,
∂
∂y (zf) = yzf ′.
Similarly,
∂
∂z (yf) = yzf ′.
Therefore, the x-component of ∇× F is zero. The y- and z-components can also be
shown to be zero, and we get ∇× F = 0.
On the other hand, using spherical coordinates, we easily obtain
∇× F =
1
r2 sin θ











ˆer
ˆeθr
ˆeϕr sin θ
∂
∂r
∂
∂θ
∂
∂ϕ
rf(r)
0
0











= 0.
Obviously,
the use of spherical coordinates simpliﬁes the calculation consi-
derably.
■

16.4 The Curl
435
The preceding example shows that
Box 16.4.1. Any well-behaved vector ﬁeld whose magnitude is only a
function of radial distance, r, and whose direction is along r is conserva-
tive. Such vector ﬁelds are generally known as central vector ﬁelds.
16.4.1
The Laplacian
Combining divergence and the gradient gives the Laplacian. Using Equation
(16.4) in Theorem 16.3.1, we get
Laplacian in
curvilinear
coordinates
Theorem 16.4.5. The Laplacian of a function f is the divergence of gradient
of f and—in a curvilinear coordinate system—is given by
∇2f =
1
h1h2h3
( ∂
∂q1
h2h3
h1
∂f
∂q1

+
∂
∂q2
h1h3
h2
∂f
∂q2

+
∂
∂q3
 h1h2
h3
∂f
∂q3
)
.
For cylindrical coordinates the Laplacian is
∇2f = 1
ρ
∂
∂ρ

ρ∂f
∂ρ

+ 1
ρ2
∂2f
∂ϕ2 + ∂2f
∂z2
(16.18)
and for spherical coordinates it is
∇2f = 1
r2
∂
∂r

r2 ∂f
∂r

+
1
r2 sin θ
( ∂
∂θ

sin θ∂f
∂θ

+
1
sin θ
∂2f
∂ϕ2
)
.
(16.19)
Equations (16.7) and (16.19) allow us to write the angular momentum
diﬀerential operator derived in Example 15.3.1 in spherical coordinates, which
is the most common way of writing it. We note that
∂
∂r

r2 ∂f
∂r

= 2r∂f
∂r + r2 ∂2f
∂r2 ,
and
r · (∇f) = r∂f
∂r ,
and
(r · ∇)2f = r ∂
∂r
∂f
∂r

= r∂f
∂r + r2 ∂2f
∂r2 .
Substituting these plus (16.19) in (15.22) yields
L2f = −
1
sin θ
( ∂
∂θ

sin θ∂f
∂θ

+
1
sin θ
∂2f
∂ϕ2
)
.
(16.20)
Therefore, the angular momentum operator depends only on angles in spher-
ical coordinates.

436
Curvilinear Vector Analysis
16.5
Problems
16.1. The divergence of a vector can be obtained in any coordinate system
by brute force calculation. In this problem you are asked to ﬁnd ∇· A in
cylindrical coordinates.
(a) Express Ax in terms of cylindrical coordinates and components. Hint:
Write A in cylindrical ccordinates and take the dot product with ˆex expressing
everything in terms of cylindrical ccordinates.
(b) Use the chain rule
∂Ax
∂x = ∂Ax
∂ρ
∂ρ
∂x + ∂Ax
∂ϕ
∂ϕ
∂x + ∂Ax
∂z
∂z
∂x
where Ax is what you found in (a).
(c) Do the same with Ay and Az, and add the three terms to obtain the
divergence in cylindrical coordinates.
16.2. Find the divergence of a vector in spherical coordinates following the
procedure outlined in Problem 16.1.
16.3. Find the gradient of a function in cylindrical and spherical coordinates
following a procedure similar to the one outlined in Problem 16.1.
16.4. Find the curl of a vector in cylindrical and spherical coordinates fol-
lowing a procedure similar to the one outlined in Problem 16.1.
16.5. Start with the Laplacian in Cartesian coordinates.
(a) By using the chain rule and expressing the second derivatives in cylindrical
coordinates, ﬁnd the Laplacian in cylindrical coordinates.
(b) Do the same for spherical coordinates.
16.6. The elliptic cylindrical coordinates (u, θ, z)are given by
x = a cosh u cosθ
y = a sinh u sin θ
z = z
where a is a constant.
(a) What is the expression for the gradient of a function f in elliptic cylindri-
cal coordinates?
(b) What is the expression for the divergence of a vector A in elliptic cylin-
drical coordinates?
(c) What is the expression for the curl of a vector A in elliptic cylindrical
coordinates?
(d) What is the expression for the Laplacian of a function f in elliptic cylin-
drical coordinates?

16.5 Problems
437
16.7. The prolate spheroidal coordinates (u, θ, ϕ) are given by
x = a sinh u sin θ cos ϕ
y = a sinh u sin θ sin ϕ
z = a cosh u cosθ
where a is a constant.
(a) What is the expression for the gradient of a function f in prolate spheroidal
coordinates?
(b) What is the expression for the divergence of a vector A in prolate spheroidal
coordinates?
(c) What is the expression for the curl of a vector A in prolate spheroidal
coordinates?
(d) What is the expression for the Laplacian of a function f in prolate
spheroidal coordinates?
16.8. The toroidal coordinates (u, θ, ϕ) are given by
x = a sinh u cos ϕ
cosh u −cos θ
y = a sinh u sin ϕ
cosh θ −cos θ
z =
a sin u
cosh u −cos θ
(a) What is the expression for the gradient of a function f in toroidal coordi-
nates?
(b) What is the expression for the divergence of a vector A in toroidal coor-
dinates?
(c) What is the expression for the curl of a vector A in toroidal coordinates?
(d) What is the expression for the Laplacian of a function f in toroidal coor-
dinates?
16.9. The paraboloidal coordinates (u, v, ϕ) are given by
x = 2auv cos ϕ
y = 2auv sin ϕ
z = a(u2 −v2)
where a is a constant.
(a) What is the expression for the gradient of a function f in paraboloidal
coordinates?
(b) What is the expression for the divergence of a vector A in paraboloidal
coordinates?
(c) What is the expression for the curl of a vector A in paraboloidal coordi-
nates?
(d) What is the expression for the Laplacian of a function f in paraboloidal
coordinates?

438
Curvilinear Vector Analysis
16.10. The three-dimensional bipolar coordinates (u, θ, ϕ) are given by
x =
a sin θ cos ϕ
cosh u −cos θ
y =
a sin θ sin ϕ
cosh u −cos θ
z =
a sinh u
cosh u −cos θ
(a) What is the expression for the gradient of a function f in three-dimensional
bipolar coordinates?
(b) What is the expression for the divergence of a vector A in three-dimensional
bipolar coordinates?
(c) What is the expression for the curl of a vector A in three-dimensional
bipolar coordinates?
(d) What is the expression for the Laplacian of a function f in three-dimensional
bipolar coordinates?

Chapter 17
Tensor Analysis
Our study of vectors in this part of the book has been limited to their anal-
ysis in speciﬁc coordinate systems, and although we touched on the general
curvilinear coordinate system, our treatment aimed at orthogonal coordinates,
and speciﬁcally at only three-dimensional spherical and cylindrical coordinate
systems. Many situations in physics demand a three-fold generalization: non-
orthogonal coordinate systems, higher-dimensional spaces, and objects, called
tensors, whose components have more subscripts than one. This chapter is
devoted to an analysis of tensors.
17.1
Vectors and Indices
Vector manipulations will be greatly simpliﬁed if equations are written in
terms of a general component. How do we accomplish this? Start with a
generic vector equation, which can be written as
U = V,
where U and V are, in general, vector expressions.
Examples of such an
equation are
B = ∇× A,
E = −∇Φ,
A =
# b
a
f(r)ˆer dr.
You can also write each of these vector equations as three equations involving
components. Thus, the foregoing generic equation becomes
Ux = Vx,
Uy = Vy,
Uz = Vz.
It is very helpful to convert letter indices into number indices. Let x →1,
y →2, and z →3, and write1
U1 = V1,
U2 = V2,
U3 = V3.
1Note that the replacements here refer to indices not the Cartesian coordinates. The
latter will have somewhat diﬀerent symbols in the sequel.

440
Tensor Analysis
These equations are abbreviated as
Ui = Vi,
i = 1, 2, 3.
(17.1)
This is what we mean by an equation in terms of a general component: The
index i refers to any one of the components of the vectors on either side of
the equation. It is called a free index because it is free to take any one of the
free index deﬁned
values between 1 and 3. An important property of a free index is that
Box 17.1.1. A free index appears once and only once on both sides of
a vector equation.
One can use any symbol to represent a free index, although the most common
symbols used are i, j, k, l, m, and n. Thus, Equation (17.1) can be written in
any one of the following alternative ways:
Uj = Vj,
j = 1, 2, 3,
Up = Vp,
p = 1, 2, 3,
U♥= V♥,
♥= 1, 2, 3.
Of special interest are the components of the position vector r. These are
denoted by xi rather than ri. Thus, the vector relation R = r −r′ is written
indexed Cartesian
coordinates
as
Xj = xj −x′
j,
j = 1, 2, 3.
An abbreviation used for derivatives with respect to Cartesian coordinates
(which coincide with the components of the position vector) is given as follows.
First ∂/∂x is replaced by ∂/∂x1, and the latter by the much shorter notation,
∂1. Similarly, ∂/∂y becomes ∂2, and ∂/∂z becomes ∂3. In particular, the
general component of the gradient of a function f will be written as ∂kf, k =
1, 2, 3.
components of
gradient
All operations on vectors can be translated into the language of indexed
relations. For example, A+B = C is equivalent to Ak +Bk = Ck, k = 1, 2, 3,
and A = αB becomes Ak = αBk, k = 1, 2, 3, etc. The two operations of
vector multiplication are a little more involved and we treat them separately
in the following.
First let us consider the dot product. In terms of components, the dot
product of A and B can be written as
A · B = AxBx + AyBy + AzBz.
Converting to number indices, we get
A · B = A1B1 + A2B2 + A3B3 =
3

i=1
AiBi.

17.1 Vectors and Indices
441
We now introduce a further simpliﬁcation in notation due to Einstein, which
gets rid of the clumsy summation sign:
Einstein
summation
convention
Box 17.1.2. (Einstein Summation Convention). Whenever an in-
dex is repeated, it is a dummy index and is summed from 1 to 3.
Using this convention we write the dot product as
dot product
A · B = AiBi.
(17.2)
No summation sign is needed as long as we remember that the repeated index
i is summed over. Since the repeated index is a dummy index, we can change
it to any other symbol. Thus,
A · B = AkBk = AjBj = AnBn = A♥B♥= · · · .
Example 17.1.1. In this example, we write some of the familiar vector relations
in both vector form and component form:
E = −∇Φ
⇐⇒
Ek = −∂kΦ,
∇· A
⇐⇒
∂jAj,
# #
S
A · da =
# #
V
#
∇· A dV
⇐⇒
# #
S
Ak dak =
# #
V
#
∂jAj dV,
∇2Φ
⇐⇒
∂m∂mΦ,
∇· (fA) = A · ∇f + f∇· A
⇐⇒
∂i(fAi) = Ai∂if + f∂iAi.
The reader is urged to verify all these relations, remembering the Einstein summa-
tion convention.
■
17.1.1
Transformation Properties of Vectors
Section 6.2.1 discussed the transformation of vectors, i.e., the way the compo-
nents of a vector change when they are expressed in term of a new basis. To
coordinates with
superscripts
initiate the transformations relevant to the present chapter, let us begin with
the position vector r, which in one Cartesian coordinate system (with basis
{ˆe1, ˆe2, ˆe3}) is represented by (x1, x2, x3), and in another by (¯x1, ¯x2, ¯x3). Here
we are beginning to introduce new notation and terminology : instead of “vec-
tor space,” we use “Cartesian coordinate system,” and instead of subscripts,
we use superscripts to label the coodinates.
Since both (x1, x2, x3), and (¯x1, ¯x2, ¯x3) are components of the same posi-
tion vector, they are related via Equation (6.29):
¯x1 = a11x1 + a12x2 + a13x3,
¯x2 = a21x1 + a22x2 + a23x3,
(17.3)
¯x3 = a31x1 + a32x2 + a33x3.

442
Tensor Analysis
In terms of a free index, we can rewrite this as
¯xi = ai1x1 + ai2x2 + ai3x3,
i = 1, 2, 3,
and using the summation notation
¯xi =
3

j=1
aijxj,
i = 1, 2, 3.
Finally, using the Einstein summation convention and always keeping in mind
that the free index i takes the values 1, 2, or 3, we come up with the following
very succinct replacement for 17.3
¯xi = aijxj.
(17.4)
Equations (17.3) and (17.4) are identical despite the enormous brevity of the
latter.
As an application of the use of indices and summation convention, we
conveniently express the rule of matrix multiplication, which we shall use
frequently. Box 6.1.3 gives this rule. Let C = AB be the product of A and B.
Then the rule in Box 6.1.3 can be written as
cij = aikbkj.
(17.5)
Notice that here we have two free indices i and j.
The index k is being
summed over on the right.
Of particular importance are transformations that leave the dot product
intact. We called these transformations orthogonal (see Section 6.1.3). These
orthogonal transformations satisfy Equation (6.20), which could be written in
terms of indices. Noting that the ij-th element of the unit matrix is δij, the
familiar Kronecker delta, which as the reader may recall, is deﬁned as
δij ≡
0
1
if i = j,
0
if i ̸= j,
(17.6)
we rewrite (6.20) as

8A

ik (A)kj = (1)ij
or
akiakj = δij.
(17.7)
Now multiply both sides of (17.4) by aik and sum over i to get
Kronecker delta in
a sum
aik¯xi = aikaij
  
=δkj
xj = xk,
where in the last step we used the most important property of the Kronecker
delta:

17.1 Vectors and Indices
443
Box 17.1.3. When an indexed quantity shares a common repeated index
with the Kronecker delta (thus a sum over that index understood), the
result is an expression in which both the sum and the Kronecker delta are
removed and the repeated index of the indexed quantity is replaced by the
other index of the Kronecker delta.
Thus the inverse of Equation (17.4) is
xj = aij ¯xi.
(17.8)
Note the diﬀerence in the position of the dummy index between this equation
and (17.4).
Equations (17.4) and (17.8) give the transformation rules for the compo-
nents of the position vector when one goes from one Cartesian coordinate
system to another.
It should be clear that the same transformation rules
apply to the components of any vector, as long as one adheres to Cartesian
coordinate systems. Thus if Vi and ¯Vi represent the components of a vector
V in two Cartesian coordinate systems, then
¯Vi = aijVj
and
Vj = aij ¯Vi.
(17.9)
In fact, it is customary to deﬁne vectors in terms of their transformation
properties:
vectors deﬁned in
terms of their
transformation
property
Box 17.1.4. A set of quantities Vi is said to be the components of a
Cartesian vector V if, under the orthogonal transformation (17.4), the
transformed quantities ¯Vi and the original quantities are related by (17.9).
Section 1.3 introduced the idea of expressing vectors in diﬀerent coordinate
systems, mainly Cartesian, cylindrical, and spherical. In all cases, care was
taken to use orthogonal unit vectors. In fact, this has been the sole practice
throughout the book so far, and for good reason: the dot product of two
vectors—and hence length of a vector, deﬁned as the square root of the dot
product of the vector with itself—does not change when their components in
one set of orthogonal unit vectors are written in terms of their components in
another set of orthogonal unit vectors. This actually deﬁnes the orthogonal
transformation of Section 6.1.3, and Equation (6.20) or (17.7) guarantees the
invariance of the length of a vector.
Orthogonal transformations are not always the most suitable. As an exam-
ple, consider a curve in space parametrized in a Cartesian coordinate system
by xi = fi(t), where f1(t), f2(t), and f3(t) are some smooth functions. The

444
Tensor Analysis
tangent to this curve—a vector—has components ˙xi ≡dxi/dt = f ′
i(t). Now
consider a new coordinate system, not necessarily Cartesian, given by
¯xi = gi(x1, x2, x3).
(17.10)
The curve can be written in terms of the new coordinates by substituting fi(t)
for each xi:
¯xi = gi(f1(t), f2(t), f2(t)) ≡hi(t),
where the last identity deﬁnes the function hi(t).
The components of the
tangent to the curve in the new coordinate system are given by the chain
rule:
˙¯xi = h′
i(t) = ∂1gi
df1
dt +∂2gi
df2
dt +∂3gi
df3
dt = ∂1gi ˙x1 +∂2gi ˙x2 +∂3gi ˙x3 = ∂jgi ˙xj.
Recalling that ∂jgi = ∂gi/∂xj and that gi = ¯xi, this is usually written as
˙¯xi = ∂¯xi
∂xj ˙xj.
(17.11)
It is instructive to see what happens if ¯xi is given by (17.4). In that case,
we have
∂¯xi
∂xj =
∂
∂xj
 
aikxk!
= aik
∂xk
∂xj

=δkj
= aij,
(17.12)
where we have used an obvious property of partial derivative which is so useful
that it is worth boxing it:
Box 17.1.5. If {y1, y2, . . . , ym} are independent variables, then ∂yi/∂yj =
δij.
Equation (17.12) shows that, when applied to Cartesian coordinate transfor-
mations, (17.11) is consistent with the deﬁnition of a vector as given in Box
17.1.4.
What about the inverse of (17.11)? Equation (17.10) can be treated as
three equations in the three unknowns {x1, x2, x3}. One can then solve these
unknowns as functions of the independent variables
,
¯x1, ¯x2, ¯x3-
. Whether
or not one can actually solve (17.10) for
,
¯xi-
depends on the form of the
functions {g1, g2, g3}. If these functions satisfy certain (mild) mathematical
properties, then Equation (17.10) is said to be invertible and each xj can
be written as a function of the independent variables
,
¯xi-
. We assume that
(17.10) is indeed invertible.
Treating xj as dependent and
,
¯xi-
as independent variables, using the
chain rule, and employing obvious notation, we can write
˙xj = dxj
dt = ∂xj
∂¯x1
d¯x1
dt + ∂xj
∂¯x2
d¯x2
dt + ∂xj
∂¯x3
d¯x3
dt = ∂xj
∂¯xk
d¯xk
dt = ∂xj
∂¯xk ˙¯xk.
(17.13)

17.1 Vectors and Indices
445
Is this consistent with Equation (17.11)? In other words, if we substitute ˙xj
from this equation into the right-hand side of (17.11), do we get ˙¯xi? Let’s
try it!
RHS of (17.11) = ∂¯xi
∂xj ˙xj = ∂¯xi
∂xj
∂xj
∂¯xk ˙¯xk = ∂¯xi
∂¯xk ˙¯xk = δik ˙¯xk = ˙¯xi,
where in the third equality we used the chain rule (2.16), in the fourth equality
we used Box 17.1.5 as applied to the independent variables ¯xi, and in the last
equality we used Box 17.1.3. Thus, Equation (17.13) is indeed consistent with
(17.11). It is tempting to call objects which transform according to (17.11)
components of a vector.
But before jumping to conclusions, let’s look at
another vector with which we are familiar.
17.1.2
Covariant and Contravariant Vectors
The gradient of a function was ﬁrst deﬁned in Section 12.3. It is a vector
whose components are essentially derivatives of the function with respect to
the coordinates. Because we are interested in the transformation properties of
objects, we ﬁrst have to clarify the notion of a function. A scalar function is
a physical quantity, such as temperature, which takes on a single value at each
point of space. Now, a point has an existence independent of any coordinate
scalar function
systems. Nevertheless, coordinates are useful for calculations. And if the point
is described by (x1, x2, x3) in a coordinate system, and φ denotes the scalar
function, then we write φ(x1, x2, x3) for the value of the scalar function at
that point. The same point is described by (¯x1, ¯x2, ¯x3) in another coordinate
system, and the value of the scalar function in terms of the new coordinates is
¯φ(¯x1, ¯x2, ¯x3). It should be obvious that the form of the scalar function changes
when one changes the coordinates. Thus the notation ¯φ instead of φ. Clearly,
¯φ(¯x1, ¯x2, ¯x3) = φ(x1, x2, x3).
(17.14)
Now diﬀerentiate both sides with respect to ¯xi. The left side gives the ith
component of the gradient of ¯φ; and using the chain rule on the right side, we
get
∂φ
∂¯xi = ∂φ
∂x1
∂x1
∂¯xi + ∂φ
∂x2
∂x2
∂¯xi + ∂φ
∂x3
∂x3
∂¯xi = ∂φ
∂xj
∂xj
∂¯xi .
We thus obtain
∂¯φ
∂¯xi = ∂xj
∂¯xi
∂φ
∂xj ,
(17.15)
which is a diﬀerent transformation than (17.11).
It appears that we have two kinds of vectors: those whose components
transform according to (17.11) and those transforming according to (17.15).
To further elucidate the discussion, let’s look at the dot product. Let A and
B be vectors which transform according to (17.11):
¯Ai = ∂¯xi
∂xj Aj,
¯Bi = ∂¯xi
∂xk Bk.

446
Tensor Analysis
The dot product in the ¯x coordinate system is ¯Ai ¯Bi (sum over repeated indices
understood!). Write this in terms of the x coordinates:
¯Ai ¯Bi = ∂¯xi
∂xj Aj
∂¯xi
∂xk Bk = ∂¯xi
∂xj
∂¯xi
∂xk AjBk.
The right-hand side does not reduce to a dot product.
Now consider one vector U that transforms according to (17.11) and an-
other V that transforms according to (17.15)
¯Ui = ∂¯xi
∂xj Uj,
¯Vi = ∂xk
∂¯xi Vk,
and take the dot product of these two vectors:
¯Ui ¯Vi = ∂¯xi
∂xj Uj
∂xk
∂¯xi Vk = ∂xk
∂¯xi
∂¯xi
∂xj UjVk = ∂xk
∂xj UjVk



by the chain rule
= δkjUjVk = UjVj



by Box 17.1.3
.
(17.16)
This is the magic of a general coordinate transformation! Although the func-
tions {g1, g2, g3} of (17.10) are completely arbitrary (except for invertibility),
they respect the dot product, as long as one vector transforms according to
(17.11) and the other according to (17.15).
So far we have been considering coordinates in a three-dimensional space.
However, as this section’s discussion easily points out, nothing prevents us
from generalizing to n-dimensions: the only change we have to make is that
the sums (and the repeated indices that imply them) should go from 1 to n.
For example, (17.10) becomes
¯xi = gi(x1, x2, . . . , xn),
i = 1, 2, . . ., n.
(17.17)
And this generalization is not purely academic, because, as we saw in Chapter
covariant and
contravariant
vectors
8, relativity demands a four-dimensional spacetime. Having this generaliza-
tion in mind, we make the following deﬁnition of the two kinds of vector
discussed above:
Box 17.1.6. The quantities {A1, A2, . . . An} and {B1, B2, . . . Bn} are said
to constitute the components of a contravariant and a covariant vector,
respectively, if, under a coordinate transformation (17.17) they transform
according to
¯Ai = ∂¯xi
∂xj Aj
and
¯Bi = ∂xj
∂¯xi Bj.
(17.18)
Note the placement of the indices on the two types of vector. Only when
an “upper” index appears with a “lower” index in a sum is the result (the dot

17.2 From Vectors to Tensors
447
product) independent of the coordinate system used. Now the question arises:
If one needs an upper and a lower index in the sum to get a quantity that is
invariant, how does one deﬁne the length of a contravariant vector (which has
only an upper index) or a covariant vector (which has only a lower index)?
For this, we need to wait until we have introduced tensors and, in particular,
the metric tensor.
17.2
From Vectors to Tensors
We have already discussed one kind of multiplication of vectors, the dot prod-
uct [see Equation (17.2)]. Now we consider the cross product as a prototype
of objects that have more than one index.
The cross product of two vec-
tors involves diﬀerent components of those vectors (as opposed to the same
components involved in the inner product). In terms of the index labels intro-
duced above, this means that the cross product carries two indices. In fact,
consider two (covariant) vectors Ai and Bj. The components of their cross
product are of the form AiBj −AjBi. In another coordinate system related
to the ﬁrst by (17.10), the components are ¯Ai ¯Bj −¯Aj ¯Bi. Using (17.18) in
Box 17.1.6 for A and B, we get
¯Ai ¯Bj = ∂xk
∂¯xi Ak
∂xh
∂¯xj Bh = ∂xk
∂¯xi
∂xh
∂¯xj AkBh,
and
¯Aj ¯Bi = ∂xk
∂¯xj Ak
∂xh
∂¯xi Bh = ∂xk
∂¯xj
∂xh
∂¯xi AkBh = ∂xh
∂¯xj
∂xk
∂¯xi AhBk,
where in the last step we just changed the dummy indices [see Equation (9.4)].
Subtracting the last two equations, we get
¯Ai ¯Bj −¯Aj ¯Bi = ∂xk
∂¯xi
∂xh
∂¯xj (AkBh −AhBk).
Thus, if we deﬁne Ckh ≡AkBh −AhBk as the components of A × B, the last
cross product as a
two-indexed
quantity
equation gives their transformation property:
¯Cij = ∂xk
∂¯xi
∂xh
∂¯xj Ckh.
(17.19)
Cross products are special cases of a more general category of mathemat-
ical objects called tensors which carry multiple indices. Some of the indices
may be upper, some lower. The most general tensor carries multiple upper
and multiple lower indices.

448
Tensor Analysis
Box 17.2.1. A set of nr+s quantities T i1...ir
j1...js is said to constitute the com-
ponents of a tensor T of type (r, s) if, under a coordinate transformation
(17.17) they transform according to
T
i1...ir
j1...js = ∂¯xi1
∂xh1 · · · ∂¯xir
∂xhr
∂xk1
∂¯xj1 · · · ∂xks
∂¯xjs T h1...hr
k1...ks
(17.20)
{i1 . . . ir} and {j1 . . . js} are called the contravariant and covariant
indices, respectively. The rank of the tensor is deﬁned as r + s.
Note that for every index on the left there is an identical index on the right,
and that only an upper index and its lower partner are repeated on the right.
Here we are using the obvious convention that in the partial derivatives of the
form ∂xk/∂¯xj or ∂¯xk/∂xj, k is considered an upper index and j a lower one.
Example 17.2.1. When we introduced multipoles in Chapter 10, we were able to
write the potential of a source distribution as an inﬁnite sum of moments of source of
higher and higher order. Although Cartesian coordinates are extremely clumsy for
higher moments, the third moment can be handled neatly in Cartesian coordinates
once we use the machinery of indices developed in this section.
Recall that the integrand of the third term in the expansion of potential is [see
Equation (10.33)]
Integrand ≡r′2

−1
2 + 3
2(ˆer · ˆer′)2

= −r′2
2 + 3
2r′2
r · r′
rr′
2
.
Writing the position vectors in terms of their Cartesian components and rearranging
terms yields
Integrand = 3
2
(xx′ + yy′ + zz′)2
r2
−r′2
2
=
1
2r2
,
x2(3x′2 −r′2) + y2(3y′2 −r′2) + z2(3z′2 −r′2)
+6xyx′y′ + 6xzx′z′ + 6yzy′z′-
.
(17.21)
We want to express (17.21) in terms of indices. First let us concentrate on the terms
involving x2, y2, and z2. Since these diagonal terms involve x2 = x1x1, etc., it is
natural to deﬁne a two-indexed quantity, say V ′
ij, such that
x2(3x′2 −r′2) ≡x1x1V ′
11,
y2(3y′2 −r′2) ≡x2x2V ′
22,
z2(3z′2 −r′2) ≡x3x3V ′
33,
with
V ′
11 = 3x′
1x′
1 −r′2,
V ′
22 = 3x′
2x′
2 −r′2,
V ′
33 = 3x′
3x′
3 −r′2.
Next, we note that the oﬀ-diagonal terms such as 6xyx′y′ can be written as 6xixjx′
ix′
j
(no summation!). It appears as if we can write all terms in the last line of Equation
(17.21) as 3
i,j=1 xixjV ′
ij if we can deﬁne V ′
ij properly. The oﬀ-diagonal sum sug-
gests deﬁning V ′
ij as V ′
ij ≡3x′
ix′
j. The reader may wonder why we did not include

17.2 From Vectors to Tensors
449
the factor of 6 in the deﬁnition. The reason is that when summed over indices, the
symmetry of V ′
ij under interchange of its indices automatically introduces a factor
of 2. The problem with this deﬁnition is that when i = j, i.e., when evaluating the
diagonal terms, the r′2 term is absent. To remedy this, we change the deﬁnition to
V ′
ij ≡3x′
ix′
j −r′2δij.
(17.22)
Then, the Kronecker delta contributes only to the diagonal terms as it should. The
reader is urged to show that
Integrand =
1
2r2
3

i,j=1
xixjV ′
ij =
1
2r2 xixjV ′
ij,
(17.23)
where in the last equality the summation convention is implied.
Now we substitute this in Equation (10.33) and denote the third term as Φ3(r).
This yields
Φ3(r) = K
r5 xixj
1
2
#
Ω
V ′
ij dQ(r′)

≡K
r5 xixjQij.
(17.24)
The last equation deﬁnes the components of the quadrupole moment:
quadrupole
moment deﬁned
Qij = 1
2
#
Ω
V ′
ij dQ(r′) = 1
2
#
Ω
(3x′
ix′
j −r′2δij) dQ(r′).
(17.25)
One can use (17.25) to calculate the quadrupole moment of any source distribution.
The quadrupole moment of electric charge distributions plays a signiﬁcant role in
nuclear physics.
■
A scalar (function) is a tensor of type (0, 0); a contravariant vector is a
tensor of type (1, 0); a covariant vector is a tensor of type (0, 1). Similarly, the
cross product, the transformation of whose components is given in (17.19), is
a tensor of type (0, 2). Of special interest is the zero tensor, which can be of
any type. Box 17.2.1 shows clearly that
Box 17.2.2. If a tensor has zero components in one coordinate system,
it has zero components in all coordinate systems.
We have also encountered another two-indexed quantity, the Kronecker
delta. Is it a tensor? If so, what type? We may think—since we have chosen
Kronecker delta
reindexed!
both of its indices to be covariant—that it is of type (0, 2). However, that
is not the case, for the following reason. Equation (17.6), which deﬁnes the
Kronecker delta, must hold in all coordinate systems. If Kronecker delta were
of type (0, 2), then it would transform according to
¯δij = ∂xk
∂¯xi
∂xh
∂¯xj δkh = ∂xk
∂¯xi
∂xk
∂¯xj ,
and the right-hand side does not satisfy Equation (17.6). For the same reason
the Kronecker delta cannot be a tensor of type (2, 0). What if we deﬁne it to
be a tensor of type (1, 1)? Then
¯δi
j = ∂¯xi
∂xk
∂xh
∂¯xj δk
h = ∂¯xi
∂xk
∂xk
∂¯xj = ∂¯xi
∂¯xj = δi
j.

450
Tensor Analysis
This shows that the proper way of indexing the Kronecker delta is to give it
one covariant and one contravariant index, i.e., to treat it as a tensor of type
(1, 1).
Example 17.2.2. Chapter 8 introduced the idea of a four-vector, which is a
vector with four components labeled 0, 1, 2, 3, with 0 being the time component and
the rest the space components. It is common to label 4-vectors by Greek indices.
the dot product in
relativity
For example, xα represents the coordinates, uα = dxα/dτ represents the 4-velocity,
pα = muα represents the 4-momentum, etc. The matrix η can be naturally assumed
to be a tensor ηαβ, and the inner product of two 4-vectors aα and bβ can be written
as ηαβaαbβ, with the summation over 0, 1, 2, 3 of a repeated index (one up, one
down) understood. Because we have used i, j, k, etc., for the space part, we shall
stick to this and write, for example uα = (u0, ui), and
aαbα ≡
3

i=0
aαbα = a0b0 + aibi ≡a0b0 +
3

i=1
aibi.
■
The notation of the example above is very commonly used in relativity
theory:
Box 17.2.3. Greek indices, representing the four-dimensional spacetime,
run from 0 to 4, while Roman indices, representing the space part, run
from 1 to 3.
17.2.1
Algebraic Properties of Tensors
In our treatment of vectors, we saw that there were some formal operations
which they obeyed. For instance, we could multiply a vector by a number,
we could add two vectors, and we could multiply two vectors to get a third
vector. Tensors also have some important properties which we summarize in
the following.
Addition
If T and S are tensors of type (r, s), then their sum U = T + S, deﬁned
componentwise as
U i1...ir
j1...js = T i1...ir
j1...js + S i1...ir
j1...js ,
is also a tensor of type (r, s). To show this, one simply has to demonstrate
that U i1...ir
j1...js transform according to (17.20) in Box 17.2.1.
Moreover, if we deﬁne V = αT componentwise as
V i1...ir
j1...js = αT i1...ir
j1...js ,
where α is a real number, then V is also a tensor of type (r, s). The combi-
nation of these two operations makes the collection of tensors of type (r, s) a
vector space.

17.2 From Vectors to Tensors
451
Multiplication
If T is a tensor of type (r1, s1) and S is a tensor of type (r2, s2), then their
tensor product U = T ⊗S, deﬁned componentwise as
U
i1...ir1+r2
j1...js1+s2 = T
i1...ir1
j1...js1 S
ir1+1...ir1+r2
js1+1...js1+s2
(17.26)
is a tensor of type (r1 + r2, s1 + s2). For example, if T is a tensor of type
(2, 1) with components T ij
k and S is a tensor of type (0, 2) with components
Slm, then the components of their tensor product U are U ij
klm ≡T ij
k Slm, and
they transform according to the following rule:
U
ij
klm = T
ij
k Slm = ∂¯xi
∂xh
∂¯xj
∂xp
∂xq
∂¯xk T hp
q
∂xr
∂¯xl
∂xs
∂¯xm Srs
= ∂¯xi
∂xh
∂¯xj
∂xp
∂xq
∂¯xk
∂xr
∂¯xl
∂xs
∂¯xm T hp
q Srs = ∂¯xi
∂xh
∂¯xj
∂xp
∂xq
∂¯xk
∂xr
∂¯xl
∂xs
∂¯xm U hp
qrs,
which shows that U is a tensor of rank (2, 3).
Example 17.2.3. One can obtain a tensor of any type by multiplying contravari-
ant and covariant vectors: take r contavariant vectors and s covariant vectors and
multiply them to get a tensor of type (r, s). For example, if A is a contravariant
vector with components Ai and B a covariant vector with components Bj, then
T ij ≡AiAj is a tensor of type (2, 0), Sijk ≡BiBjBk is a tensor of type (0, 3), and
U ij
k ≡AiAjBk is a tensor of type (2, 1).
■
Contraction
Given a tensor of type (r, s), take a covariant index and set it equal to a
contravariant index, i.e., sum over those two indices. The process is called
contraction and the end result is a tensor of type (r−1, s−1). For example,
take the tensor of type (2, 1) whose components are T ij
k and set k = j. How
do the components T ij
j
transform?
T
ij
j = ∂¯xi
∂xh
∂¯xj
∂xp
∂xq
∂¯xj T hp
q
= ∂¯xi
∂xh
∂xq
∂¯xj
∂¯xj
∂xp



=δq
p
T hp
q
= ∂¯xi
∂xh T hq
q .
This shows that T ij
j
transform as components of a contravariant vector [see
Equation (17.18)], i.e., a tensor of type (1, 0).
Of special interest is a tensor of type (1, 1).
When you contract this
tensor, you get a tensor of type (0, 0), i.e., a scalar. For example, let A be
a contravariant vector with components Ai and B a covariant vector with
components Bj.
Then T i
j ≡AiBj is a tensor of type (1, 1).
When you
contract it, you get T i
i ≡AiBi, which is the dot product of the two vectors,
i.e., a scalar [see Equation (17.16)].

452
Tensor Analysis
Symmetrization
Some important tensors in physics have the property that when two of its
symmetric and
antisymmetric
tensors
indices are interchanged the tensor does not change or it changes sign. In the
ﬁrst case, we say that the tensor is symmetric, in the second case, antisym-
metric. For example, if T is a tensor of type (2, 0) and U of type (0, 2), and if
T ij = T ji
and
Uij = −Uji,
then T is symmetric and U is antisymmetric.
Given any tensor, one can always construct from it a tensor which is
symmetric or antisymmetric in the interchange of any pair of its indices. In
particular, if T is any tensor of type (2, 0), then the tensors S and A with
components
Sij = 1
2(T ij + T ji)
and
Aij = 1
2(T ij −T ji)
are called the symmetric and antisymmetric parts of T, and
T ij = 1
2(T ij + T ji) + 1
2(T ij −T ji) ≡Sij + Aij.
(17.27)
The symmetric part Sij is sometimes denoted by T (ij) and the antisymmetric
part Aij by T [ij].
17.2.2
Numerical Tensors
There are certain “constant” tensors which play important roles in tensor
analysis. We have seen one such tensor already: the (1, 1)-type Kronecker
delta. In fact, all the so-called numerical tensors are built form this funda-
mental tensor. The generalized Kronecker delta δi1···ir
j1···jr is deﬁned as
generalized
Kronecker delta
δi1···ir
j1···jr = det
⎛
⎜
⎜
⎜
⎝
δi1
j1
δi1
j2
· · ·
δi1
jr
δi2
j1
δi2
j2
· · ·
δi2
jr
...
...
...
...
δir
j1
δir
j2
· · ·
δir
jr
⎞
⎟
⎟
⎟
⎠.
(17.28)
The determinant of an r × r matrix is a sum of terms each consisting
of the product of r matrix elements. In (17.28), each term is a product of
r Kronecker deltas. Since the Kronecker delta is a (1, 1)-type tensor, each
term, thus the determinant, and thus the generalized Kronecker delta, is an
(r, r)-type tensor.
It is clear from (17.28) that the upper indices label the rows and the lower
indices the columns of the matrix. Thus interchanging any two of the upper
indices is equivalent to interchanging two rows of the matrix. This changes
the sign of the determinant. Similarly for the interchange of two columns.

17.2 From Vectors to Tensors
453
Box 17.2.4. The generalized Kronecker delta is a completely antisym-
metric tensor in its upper and lower indices: interchanging any two of its
upper indices or any two of its lower indices changes its sign.
Example 17.2.4. In this example, we demonstrate a useful property of the gen-
eralized Kronecker delta. We illustrate the property for r = 3 and n = 3,2 but the
result can easily be generalized. Expand the determinant of δijk
lmp about the last row
starting from the right:
δijk
lmp = det
⎛
⎝
δi
l
δi
m
δi
p
δj
l
δj
m
δj
p
δk
l
δk
m
δk
p
⎞
⎠= δk
p det
δi
l
δi
m
δj
l
δj
m

−δk
m det
δi
l
δi
p
δj
l
δj
p

+ δk
l det
δi
m
δi
p
δj
m
δj
p

= δk
pδij
lm −δk
mδij
lp + δk
l δij
mp.
Now contract over the indices k and p to obtain
δijk
lmk = δk
kδij
lm −δk
mδij
lk +δk
l δij
mk = 3δij
lm −δij
lm +δij
ml = 2δij
lm +δij
ml = δij
lm = δi
lδj
m −δi
mδj
l ,
where in the next to the last step we used the antisymmetry of the generalized Kro-
necker delta. Note that because of the antisymmetry of the generalized Kronecker
delta in both upper and lower indices, we can move both the upper and the lower
last indices to the beginning:
δi1···ir
j1···jr = δ
iri1···ir−1
jrj1···jr−1.
In particular,
δkij
klm = δijk
lmk = δi
lδj
m −δi
mδj
l .
■
The procedure of the example above can be generalized to arbitrary r and
n. Furthermore, one can contract over more than one pair of indices. The
result is the following useful identity:
δi1···isis+1···ir
j1···jsis+1···ir = (n −s)!
(n −r)!δi1···is
j1···js.
(17.29)
From the generalized Kronecker delta two other important numerical ten-
sors are built. These are called the Levi-Civita symbols. They are deﬁned
Levi-Civita
symbols
as follows:
ϵj1···jn = δ12···n
j1···jn
and
ϵi1···in = δi1···in
12···n .
(17.30)
Note that both Levi-Civita symbols are antisymmetric in all their indices and
will thus vanish if any two of their indices are equal. Moreover,
ϵ12···n = δ12···n
12···n = 1
and
ϵ12···n = δ12···n
12···n = 1,
(17.31)
so that we have
ϵi1···in = ϵi1···in =
⎧
⎪
⎨
⎪
⎩
+1
if i1 · · · in is an even permutation of 1,2,. . . n,
−1
if i1 · · · in is an odd permutation of 1,2,. . . n,
0
otherwise.
(17.32)
2Recall that n is the dimension of the space.

454
Tensor Analysis
Now consider the quantity
Ai1···in
j1···jn = ϵi1···inϵj1···jn −δi1···in
j1···jn,
which is clearly antisymmetric in all its upper as well as lower indices. This
means that the only nonzero elements of Ai1···in
j1···jn are those obtained from
A12···n
12···n. But this is zero by (17.31) and the deﬁnition of Ai1···in
j1···jn. We have just
shown the following important result
ϵi1···inϵj1···jn = δi1···in
j1···jn.
(17.33)
17.3
Metric Tensor
Let {x′i} denote a set of Cartesian coordinates, and {xj} some other coordi-
nates of which {x′i} are functions. We then have
dx′i = ∂x′i
∂xj dxj
(sum over j implied as usual).
The element of length (squared)—which is customarily denoted by ds2—in
the Cartesian coordinate system is
ds2 = (dx′1)2 + (dx′2)2 + · · · + (dx′n)2 =
n

i=1
(dx′i)2.
In terms of the other coordinates, this can be written as
ds2 =
n

i=1
(dx′i)2 =
n

i=1
dx′idx′i
=
n

i=1
∂x′i
∂xj dxj
 ∂x′i
∂xk dxk

=
 n

i=1
∂x′i
∂xj
∂x′i
∂xk

dxjdxk.
The expression in parentheses on the last line, denoted by gjk(x), is a sym-
metric tensor of type (0, 2), which as indicated, is a function of the {xj}:
gjk(x) =
n

i=1
∂x′i
∂xj
∂x′i
∂xk .
(17.34)
That gjk(x) is symmetric should be obvious. To show that it is a tensor
of type (0, 2), let {¯xk} be some new set of coordinates of which {x′i} are

17.3 Metric Tensor
455
functions. We assume that all functional dependences are invertible. This
means that {¯xk} can be thought of as functions of {x′i}, and through {x′i},
as functions of {xj}. In terms of the ¯x variables,
¯gjk(¯x) ≡
n

i=1
∂x′i
∂¯xj
∂x′i
∂¯xk .
Using the chain rule, this can be written as
¯gjk(¯x) =
n

i=1
∂x′i
∂xp
∂xp
∂¯xj
∂x′i
∂xq
∂xq
∂¯xk =
 n

i=1
∂x′i
∂xp
∂x′i
∂xq




=gpq(x)
∂xp
∂¯xj
∂xq
∂¯xk = ∂xp
∂¯xj
∂xq
∂¯xk gpq(x),
which shows that gpq transforms as a (0, 2)-type tensor.
In terms of this
tensor, ds2 is written as
ds2 =
n

i=1
(dx′i)2 = gjk(x)dxjdxk.
(17.35)
The matrix whose elements are gpq is invertible. In fact, consider
hkm(x) ≡
n

p=1
∂xk
∂x′p
∂xm
∂x′p ,
which the reader can show to be a tensor of type (2, 0). Then
gjk(x)hkm(x) =
 n

i=1
∂x′i
∂xj
∂x′i
∂xk
  n

p=1
∂xk
∂x′p
∂xm
∂x′p

=
n

i,p=1
∂x′i
∂xj
∂x′i
∂xk
∂xk
∂x′p



= ∂x′i
∂x′p =δip
∂xm
∂x′p =
n

i=1
∂xm
∂x′i
∂x′i
∂xj



= ∂xm
∂xj
= δm
j ,
where on the second line use was made of the chain rule and Box 17.1.5.
This equation shows that the matrix whose elements are hkm(x) is inverse
to the matrix whose elements are gjk(x).
It is common to use the same
symbol for the inverse as for the original tensor. Thus, instead of hkm(x), we
use gkm(x).
The (0, 2)-tensor gjk(x) was deﬁned in terms of the transformation rule
between a Cartesian and a second coordinate system. It turns out that one
can abstract the properties of gjk(x) and deﬁne the metric tensor:

456
Tensor Analysis
Box 17.3.1. A metric tensor g with components gij is a symmetric
type-(0, 2) tensor whose matrix has an inverse g−1 with components gkm.
Every metric tensor deﬁnes a geometry in which the (square of the)
element of length ds2 is given by
ds2 = gij(x)dxidxj,
where {xi} are some appropriate coordinates in that geometry.
The word “geometry” in this Box is used rather loosely. A precise deﬁni-
geometry,
manifold, and
metric tensor
tion of “geometry” is beyond the scope of this book. Nevertheless, we mention
that the notion of geometry starts with the concept of a manifold, which is a
“space” that locally looks like a Euclidean space. For example, the surface of
a sphere is a two-dimensional manifold, because a very small area of a sphere
looks like a two-dimensional Euclidean space, i.e., a ﬂat plane. Mathemati-
cians study manifolds that have no metric tensors deﬁned on them. However,
in physics, almost all manifolds have a metric, and this metric deﬁnes the
geometry of that manifold.
In our discussion of the inner product in Section 6.1.2, we also encountered
the metric tensor, although we called it the metric matrix. There, we deﬁned
the notion of positive deﬁniteness. In the context of the discussion here, this
Riemannian
manifold
property becomes the cornerstone of a special kind of geometry: if ds2 of
Box 17.3.1 is always strictly greater than zero for nonzero dxi and dxj, then
the manifold on which gij is deﬁned is a called a Riemannian manifold.
Relativity requires manifolds that are not Riemannian, i.e., for which ds2 can
be zero or negative.
Geometry is an intrinsic property of a space, while gij(x) depends on the
coordinates used. This is evident in Equation (17.35) where ds2 is given in
terms of Cartesian coordinates as well as the other general coordinates. De-
spite this coordinate dependence, the metric tensor does deﬁne the geometry
of a manifold. In fact, there are some quantities obtained from the metric
which characterize the intrinsic geometry of the manifold. We shall return to
this discussion later.
Example 17.3.1. Let us ﬁnd the metric tensor in spherical coordinates.
Use
spherical coordinate symbols as indices with r, θ, and ϕ as ﬁrst, second, and third
coordinates, respectively. Recalling that x′1 = x, x′2 = y, and x′3 = z, with
x = r sin θ cos ϕ,
y = r sin θ sin ϕ,
z = r cos θ,
and using Equation (17.34), we get
grr(r, θ, ϕ) =
∂x
∂r
2
+
 ∂y
∂r
2
+
∂z
∂r
2
=(sin θ cos ϕ)2 + (sin θ sin ϕ)2 + (cos θ)2 = 1
grθ(r, θ, ϕ) = ∂x
∂r
∂x
∂θ + ∂y
∂r
∂y
∂θ + ∂z
∂r
∂z
∂θ
= (sin θ cos ϕ)(r cos θ cos ϕ) + (sin θ sin ϕ)(r cos θ sin ϕ) + (cos θ)(−r sin θ)
= r sin θ cos θ cos2 ϕ + r sin θ cos θ sin2 ϕ −r cos θ sin θ = 0.

17.3 Metric Tensor
457
Similarly, the reader can show that grϕ = 0, and in fact all the oﬀ-diagonal elements
vanish. On the other hand,
gθθ(r, θ, ϕ) =
 ∂x
∂θ
2
+
∂y
∂θ
2
+
∂z
∂θ
2
= (r cos θ cos ϕ)2 + (r cos θ sin ϕ)2 + (−r sin θ)2 = r2
gϕϕ(r, θ, ϕ) =
 ∂x
∂ϕ
2
+
 ∂y
∂ϕ
2
+
 ∂z
∂ϕ
2
= (−r sin θ sin ϕ)2 + (r sin θ cos ϕ)2 = r2 sin2 θ.
Therefore,
ds2 = (dr)2 + r2(dθ)2 + r2 sin2 θ(dϕ)2 = dr2 + r2dθ2 + r2 sin2 θdϕ2,
which agrees with Equation (2.25). Note how the parentheses have been removed
from around the diﬀerentials.
This is a very common (albeit inaccurate)
practice.
■
17.3.1
Index Raising and Lowering
After Box 17.1.6, we mentioned that the length of a covariant or contravari-
ant vector cannot be deﬁned without a metric tensor. Now that we have a
metric tensor, we deﬁne them. In fact, we can do better! We can deﬁne the
dot product of any two vectors. If one vector is covariant and the other con-
travariant, their dot product is the usual one: the sum of the product of their
components as shown in (17.16). If both vectors A and B are contravariant,
deﬁne the dot product as
A · B = gijAiBj,
(17.36)
and if both vectors are covariant, deﬁne the dot product as
A · B = gijAiBj.
(17.37)
The reader can routinely show that ¯A · ¯B = A · B in both cases.
Equations (17.36) and (17.37) have an interesting interpretation. Take the
ﬁrst equation and recall from Equation (17.26) that the product gijAk is a
tensor of type (1, 2). Contracting the indices i and k turns that into a tensor
of type (0, 1), i.e., a covariant vector, say C with components Cj. But now
note that
C · A = CjAj = gijAiAj = A · A.
It is therefore natural to denote gijAi—which is equal to gjiAi because of the
symmetry of the metric tensor—by Aj. Thus, the metric tensor gij provides
us with a way of changing contravariant vectors to covariant vectors, i.e.,
lowering their indices. Similar arguments show that the inverse of the metric
tensor gij can be used to raise indices; and these two processes are consistent,
in the sense that if we lower the index of a contravariant vector with gij and
then raise the index of the resulting covariant vector with gij, we get the

458
Tensor Analysis
original contravariant vector. Here is a proof! Let Ck = gkjAj, where Aj is
the covariant vector obtained from Ai. Then,
Ck = gkjAj = gkjgijAi = gkjgjiAi = δk
i Ai = Ak,
and the original contravariant vector is restored. The process of raising and
lowering of indices works for arbitrary tensors:
Box 17.3.2. Any contravariant index i of a general tensor can be made
into a covariant index j by multiplying the component that includes i by
gij. Any covariant index i of a general tensor can be made into a con-
travariant index j by multiplying the component that includes i by gij.
In Cartesian coordinates the (Euclidean) metric tensor is just the Kronecker
delta. Therefore
Aj = gijAi = δi
jAi = Aj,
in Cartesian coordinates with Euclidean metric,
(17.38)
and the distinction between covariant and contravariant vectors (and indices)
disappears.
In special relativity and in Cartesian coordinates, the metric tensor is ηαβ,
whose matrix is given in Equation (8.8). This tensor has components
η00 = 1, η11 = η22 = η33 = −1, ηαβ = 0 if α ̸= β
in special relativity.
The inverse of ηαβ is itself: ηαβ = ηαβ. In raising and lowering of an index,
the time component does not change, while the space components change sign
(see Box 17.2.3 for the meaning of Greek and Roman indices in relativity):
Aα = ηαβAβ ⇒A0 = A0, Ai = −Ai
(17.39)
Example 17.3.2. The Levi-Civita symbols are conveniently used to express the
components of the cross product of two vectors in Cartesian coordinate systems.
components of
cross product
Since there is no diﬀerence between covariant and contravariant indices in Cartesian
coordinate system, we use only covariant indices.
(A × B)i = ϵijkAjBk,
i = 1, 2, 3,
(17.40)
where a sum over j and k is understood. As a practice in index manipulation, the
reader is urged to verify the above relation. The order of the two vectors on both
sides of the equation is important!
Using Equation (17.40) and some properties of the Levi-Civita symbol, we can
derive the bac cab rule:
A × (B × C) = B(A · C) −C(A · B).

17.3 Metric Tensor
459
Start with a general component of the LHS and work through index manipulations
until you reach the corresponding component of the RHS:
{A × (B × C)}i = ϵijkAj(B × C)k = ϵijkAjϵkmnBmCn
= ϵkijϵkmnAjBmCn = (δimδjn −δinδjm) AjBmCn
= δimδjnAjBmCn −δinδjmAjBmCn = AjBiCj −AjBjCi
= Bi(AjCj) −Ci(AjBj) = Bi(A · C) −Ci(A · B).
On the second line we used (17.33) and the result obtained in Example 17.2.4. The
last expression above is the ith component of the RHS of the bac cab rule.
■
Example 17.3.3. Example 15.3.1 calculated the angular momentum diﬀerential
operator using Cartesian coordinates. To illustrate the power of indices and the
ease with which they allow some complex manipulations, we redo the calculation of
Example 15.3.1 using indices.
We have −L2f = (r × ∇) · (r × ∇)f. Letting ∂j stand for the partial derivative
with respect to xj, using Einstein summation convention, and recalling that no
raising or lowering of indices is necessary for Euclidean space, we write
−L2f = (r × ∇)i(r × ∇)if = (ϵijkxj∂k) (ϵilmxl∂m) f = ϵijkϵilmxj∂k (xl∂mf) ,
where we used (17.40). Continuing, refer to (17.33) and write the above equation as
−L2f = (δjlδkm −δjmδkl) xj∂k (xl∂mf) = xj∂k (xj∂kf) −xj∂k (xk∂jf)
= xjδkj∂kf + xjxj∂k∂kf −xjδkk∂jf −xjxk∂k∂jf
(17.41)
= xj∂jf + r2∇2f −3xj∂jf −xjxk∂k∂jf = r2∇2f −2(r · ∇)f −xjxk∂k∂jf,
because ∂kxj = δkj, xjxj = r2, δkk = 3, xj∂j = r · ∇, and ∂k∂k = ∇2. The last
term in (17.41) above can be found from the following relation:
xk∂k(xj∂jf) = xkδkj∂jf + xkxj∂k∂jf = xi∂jf + xkxj∂k∂jf,
or
xkxj∂k∂jf = (r · ∇)2f −(r · ∇)f.
Substituting in (17.41) yields Equation (15.22). Compare this derivation with the
laborious calculation of Example 15.3.1!
■
17.3.2
Tensors and Electrodynamics
Relativity was a logical outcome of the electromagnetic theory.
It should
therefore come as no surprise if the equations of electromagnetism found their
most natural form in the language of relativity and tensors associated with
it. In the discussion that follows, it is convenient and common practice to set
the speed of light equal to 1; then since c = 1/√ϵ0μ0, we have
c = 1,
1
ϵ0
= μ0.
Consider the Lorentz force law
f = q(E + v × B)
or
fi = q (Ei + ϵijkvjBk) ,
(17.42)

460
Tensor Analysis
where as in Example 17.3.2, we used covariant indices for all tensors in the
second equation. Since this is the fundamental force of electromagnetism, we
expect it to have a natural expression in relativity.
As a starting point, we note that the magnetic part is of the form vjFij,
where Fij = ϵijkBk is an antisymmetric tensor of rank two.
The obvious
generalization that might lead to a connection with relativity is to consider
an expression of the form uβFαβ, where uβ is the velocity 4-vector and Fαβ is
an antisymmetric tensor of rank two which reduces to Fij when both α and
β are nonzero. Let us look at uβFαβ when α is i:
uβFiβ = u0Fi0 + ujFij,
where we used the convention of Example 17.2.2. Equation (8.21) now gives
u0 = γ, and ui = γvi. Then the equation above gives
electromagnetic
ﬁeld tensor
uβFiβ = γFi0 + γvjFij = γ
 
Fi0 + vjFij
!
= γ (Fi0 + vjϵijkBk) ,
where in the last step, we disregarded the diﬀerence between covariant and
contravariant indices.
Comparison with Equation (17.42) shows that it is
natural to set Fi0 = Ei. The second rank antisymmetric tensor Fαβ is called
the electromagnetic ﬁeld tensor.
Maxwell’s equations (15.29) take a specially simple form when written in
terms of the electromagnetic ﬁeld tensor. The ﬁrst equation can be written
as
∂iFi0 = ∂Fi0
∂xi = ρ
ϵ0
= μ0ρ.
(17.43)
The obvious generalization of the left-hand side to relativity is ∂Fαβ/∂xα. But
there is something wrong with this! Both α’s are lower indices—recall that the
superscript of a coordinate in the denominator leads to a subscript—and you
cannot sum over them. In the Euclidean case, this causes no problem because
by (17.38), there is no diﬀerence between lower and upper indices and we can
simply raise one of the i’s. In relativity, however, there is a diﬀerence. So, we
have to introduce the (inverse) η tensor. The left-hand side now becomes
ηαν ∂Fαβ
∂xν .
Since β is a free index, we expect the right-hand side to have a free index as
well. So, we write the generalization of Maxwell’s ﬁrst equation as
ηαν ∂Fαβ
∂xν
= μ0Vβ,
(17.44)
with Vβ to be determined. For β = 0, we get
ηαν ∂Fα0
∂xν = μ0V0,
or
ηiν ∂Fi0
∂xν = μ0V0,
or
−∂Fi0
∂xi = μ0V0,
where we used the fact that Fαβ is antisymmetric, so all its “diagonal” com-
ponents are zero. We also used the fact that η is diagonal with the space
elements being −1. Comparing with (17.43), we see that V0 = −ρ.

17.3 Metric Tensor
461
Now let β = i in (17.44). Then
ηαν ∂Fαi
∂xν = μ0Vi,
or
η0ν ∂F0i
∂xν + ηjν ∂Fji
∂xν = μ0Vi,
or
∂F0i
∂x0 −∂Fji
∂xj = μ0Vi,
or
−∂Ei
∂t + ϵijk∂jBk = μ0Vi.
This is the ith component of the vector equation
−∂E
∂t + ∇× B = μ0V.
Comparing this with the fourth Maxwell’s equation, we identify V as J. Thus,
Maxwell’s 1st and
4th equations and
four-current
the ﬁrst and fourth equations, the inhomogeneous Maxwell’s equations
are combined into
ηαν ∂Fαβ
∂xν
= μ0Jβ,
(17.45)
where Jβ = (−ρ, J) is the 4-current. We leave it to the reader to verify that
Maxwell’s 2nd and
3rd equations
∂Fαβ
∂xν + ∂Fνα
∂xβ + ∂Fβν
∂xα = 0
(17.46)
combines the second and third equations, the homogeneous Maxwell’s
equations.
Equation (17.46) is satisﬁed if Fαβ = ∂αAβ −∂βAα for any 4-vector Aα,
as the reader can easily verify. For α = i and β = 0, this gives
Fi0 = ∂iA0 −∂0Ai,
or
Ei = ∂iA0 −∂0Ai,
or
E = ∇A0 −∂A
. ∂t
Comparing this with (15.31) identiﬁes A0 with the negative of the scalar
potential Φ and A with the vector potential. We can thus write
Fαβ = ∂αAβ −∂βAα,
Aα = (−Φ, A).
(17.47)
Now that we have solved the homogeneous Maxwell’s equations by in-
troducing the 4-potential, we can insert the result in (17.45) to write the
inhomogeneous Maxwell’s equations in terms of the 4-potential as well. We
then have
ηαν∂ν (∂αAβ −∂βAα) = μ0Jβ,
or
ηαν∂ν∂αAβ −∂β (ηαν∂νAα) = μ0Jβ.
(17.48)
The expression in parentheses—when set equal to zero—gives the Lorentz
gauge condition [see Equation (15.32)]. The remaining part of the equation
gives the wave equation for A and Φ.

462
Tensor Analysis
17.4
Diﬀerentiation of Tensors
Tensors represent many quantities, whose variation with coordinates (points
in space) has physical signiﬁcance. Therefore, the notion of a derivative of a
tensor becomes important. Although we can always diﬀerentiate components
of a tensor (they are just functions), the resulting derivative is not necessarily
a tensor.
To obtain a tensor, one needs to generalize the concept of the
derivative, as we do in this section.
17.4.1
Covariant Diﬀerential and Aﬃne Connection
Let us begin by noting that the diﬀerentials of coordinates form the com-
ponents of a contravariant vector. In fact, when the new coordinates ¯xi are
written as functions of the old coordinates xj and one takes the diﬀerential
of the new coordinates, one obtains
d¯xi = ∂¯xi
∂xj dxj,
(17.49)
which is precisely the way a contravariant vector transforms. In fact, this
is the archetypal example of a contravariant vector, and can be a guide in
helping the reader remember the rule of transformation of the contravariant
components of a tensor.
The diﬀerential of a scalar—a tensor of type (0, 0)—is again a scalar,
because
dφ = ∂φ
∂xi dxi,
and the ﬁrst term is the components of a covariant vector [see Equation
(17.15)], and the second term the components of a covariant vector (as shown
above).
Next take the diﬀerential of a contravariant vector Ai. How does it trans-
form? By taking the diﬀerential of the transformation rule
¯Ai = ∂¯xi
∂xj Aj,
(17.50)
one obtains
d ¯Ai = ∂¯xi
∂xj dAj + d
 ∂¯xi
∂xj

Aj = ∂¯xi
∂xj dAj +
∂2¯xi
∂xk∂xj dxkAj.
(17.51)
If the second term on the right were absent, dAj would transform as a con-
travariant vector. It turns out that one can add something to dAj whose eﬀect
is to cancel the unwanted term.
Consider quantities Γj
mp, which transform according to
components of
aﬃne connection
Γ
j
mp = ∂¯xj
∂xl
∂xh
∂¯xm
∂xk
∂¯xp Γl
hk −
∂2¯xj
∂xh∂xk
∂xh
∂¯xm
∂xk
∂¯xp .
(17.52)

17.4 Diﬀerentiation of Tensors
463
Any set of three-indexed symbols Γj
mp which transform according to this equa-
tion is said to constitute the components of an aﬃne connection. An aﬃne
connection is not a tensor because of the second term on the right-hand side
of (17.52). Since this term is the same for all aﬃne connections, the diﬀerence
between two aﬃne connections is a tensor of type (1, 2). If Γj
mp and Λj
mp are
any two aﬃne connections then
Γ
j
mp −Λ
j
mp = ∂¯xj
∂xl
∂xh
∂¯xm
∂xk
∂¯xp
 
Γl
hk −Λl
hk
!
,
(17.53)
showing that Γl
hk −Λl
hk transform as components of a tensor of type (1, 2).
In particular, if Λl
hk = Γl
kh, then the diﬀerence Γl
hk −Γl
kh is essentially the
antisymmetric part of the aﬃne connection Γ:
Γl
hk = 1
2
 
Γl
hk + Γl
kh
!



symmetric part
+
1
2
 
Γl
hk −Γl
kh
!



antisymmetric part
.
The antisymmetric part of an aﬃne connection is called its torsion ten-
torsion tensor
sor. Clearly if it vanishes in one coordinate system then it vanishes in all
coordinates (the zero tensor is zero in all coordinate systems).
Thus, the
torsion tensor of an aﬃne connection is zero, if an only if the connection is
symmetric.
Lack of tensorial character of the aﬃne connection is precisely what is
needed to make dAj, as well as dAj a tensor:
Box 17.4.1. For any aﬃne connection Γj
kl, the quantities DAj and DAj
deﬁned by
DAj = dAj + Γj
klAkdxl
and
DAj = dAj −Γk
jlAkdxl
are, respectively, the components of a contravariant and a covariant vec-
tor. They are called the covariant or absolute diﬀerential of the vectors.
We show that DAj is a contravariant vector, leaving the proof of the second
claim to the reader. In the bar coordinates, we have
D ¯Aj = d ¯Aj + Γ
j
kl ¯Akd¯xl.

464
Tensor Analysis
Using Equations (17.49), (17.50), (17.51), and (17.52), we obtain
D ¯Aj = ∂¯xj
∂xk dAk +
∂2¯xj
∂xk∂xl dxkAl
+
∂¯xj
∂xp
∂xq
∂¯xk
∂xr
∂¯xl Γp
qr −
∂2¯xj
∂xq∂xr
∂xq
∂¯xk
∂xr
∂¯xl
  ∂¯xk
∂xm Am ∂¯xl
∂xs dxs

= ∂¯xj
∂xk dAk +
∂2¯xj
∂xk∂xl dxkAl
+ ∂¯xj
∂xp
∂xq
∂¯xk
∂¯xk
∂xm



=δq
m
∂xr
∂¯xl
∂¯xl
∂xs



=δr
s
Γp
qrAmdxs −
∂2¯xj
∂xq∂xr
∂xq
∂¯xk
∂¯xk
∂xm



=δq
m
∂xr
∂¯xl
∂¯xl
∂xs



=δr
s
Amdxs
= ∂¯xj
∂xk dAk +
∂2¯xj
∂xk∂xl dxkAl + ∂¯xj
∂xp Γp
qrAqdxr −
∂2¯xj
∂xq∂xr Aqdxr.
The second term cancels the last term (remember that you can use any symbol
for the dummy indices that are summed over). Therefore,
D ¯Aj = ∂¯xj
∂xk dAk + ∂¯xj
∂xp Γp
qrAqdxr = ∂¯xj
∂xk dAk + ∂¯xj
∂xk Γk
qrAqdxr
= ∂¯xj
∂xk
 
dAk + Γk
qrAqdxr!
= ∂¯xj
∂xk DAk,
which is the transformation rule of a contravariant vector.
Absolute diﬀerential can be deﬁned for any tensor. For a scalar φ, Dφ =
dφ. In the case of other tensors, for each contravariant index an aﬃne con-
nection term with a positive sign, and for each covariant index an aﬃne con-
nection term with a negative sign is introduced. For example, the covariant
diﬀerential of T ij
k is a tensor of type (2, 1) given by
DT ij
k = dT ij
k +

Γi
pqT pj
k + Γj
pqT ip
k −Γp
kqT ij
p

dxq.
Covariant diﬀerential has all the properties of ordinary diﬀerential when ap-
plied to tensors. For example, the covariant diﬀerential of the sum of two
tensors of type (r, s) is a tensor of type (r, s), and D(αT) = αDT for any
constant α and any tensor T. Covariant diﬀerential also obeys the Leibniz
rule:
D(T ⊗S) = DT ⊗S + T ⊗DS.
(17.54)
17.4.2
Covariant Derivative
In the ﬁrst equation of Box 17.4.1, write dAj in terms of partial derivatives.
Then, the equation becomes
DAj = ∂Aj
∂xl dxl + Γj
klAkdxl =
∂Aj
∂xl + Γj
klAk.

dxl

17.4 Diﬀerentiation of Tensors
465
Since the left-hand side and dxl are contravariant vectors, we suspect that the
expression in parentheses is a tensor of type (1, 1). This can in fact be shown
directly. It is called the covariant derivative of Aj with respect to xl and
denoted by Aj
;l. Thus,
covariant
derivative
Aj
;l ≡∂Aj
∂xl + Γj
klAk.
(17.55)
This is the generalization of ordinary derivative to situations in which the
aﬃne connection is nonzero. Covariant derivative can similarly be deﬁned
for covariant vectors as well as arbitrary tensors. For example, the covariant
derivative of T ij
k is a tensor of type (2, 2) given by
T ij
k;q = ∂T ij
k
∂xq + Γi
pqT pj
k + Γj
pqT ip
k −Γp
kqT ij
p .
Consider a curve in Euclidean space parametrized by t. Let Ai(t) be the
parallel translation
along a curve
value of a vector ﬁeld at a point on the curve. If dAi/dt = 0, then the vector is
constant along the curve, and we say that the vector is parallel translated
along the curve. When the aﬃne connection is nonzero, we divide both
sides of the ﬁrst equation in Box 17.4.1 by dt (which on the left we denote by
Dt for aesthetic reasons), and say that a contravariant vector ﬁeld is parallel
translated along a curve if
DAj
Dt
= 0
or
dAj
dt + Γj
klAk dxl
dt = 0,
(17.56)
with a similar deﬁnition for a covariant vector ﬁeld. Since Aj depends on t only
through the coordinates, we use the chain rule dAj/dt = (∂Aj/∂xl)dxl/dt to
rewrite the equation above as
DAj
Dt =
∂Aj
∂xl + Γi
kjAk
 dxl
dt ≡Aj
;l
dxl
dt ≡Aj
;l ˙xl = 0.
(17.57)
A curve whose tangent vector is parallel translated along that curve is
geodesic and
geodesic equation
called a geodesic.
The components of the vector tangent to a curve is
dxi/dt ≡˙xi.
If we substitute this in (17.56) we obtain the following sec-
ond order diﬀerential equation called the geodesic equation:
D ˙xj
Dt = 0,
or
d2xj
dt2 + Γj
kl
dxk
dt
dxl
dt = 0,
or
¨xj + Γj
kl ˙xk ˙xl = 0, (17.58)
where each super dot represents a diﬀerentiation with respect to t. Solving
this diﬀerential equation yields the parametric equation of a geodesic.
17.4.3
Metric Connection
The aﬃne connection, which is deﬁned by its transformation property of
(17.52) is completely arbitrary.
One can deﬁne covariant diﬀerentials and

466
Tensor Analysis
covariant derivatives in terms of any set of quantities that transform accord-
ing to Equation (17.52). With a metric tensor, however, one can deﬁne a
unique symmetric (therefore, torsion-free) aﬃne connection called metric
connection given by
Γj
kl = Γj
lk = 1
2gjm
∂gmk
∂xl
+ ∂gml
∂xk −∂gkl
∂xm

≡gjmΓmkl,
(17.59)
where
Γmkl = 1
2
∂gmk
∂xl + ∂gml
∂xk −∂gkl
∂xm

,
(17.60)
with all lower indices, is easier to remember. Note that it is the ﬁrst index
of Γmkl that is raised to give the components of the metric connection, and
for this reason the metric connection is sometimes denoted by Γj
kl.
The
veriﬁcation that (17.59) is indeed an aﬃne connection—i.e., that it transforms
according to (17.52)—is straightforward but tedious.
Example 17.4.1. If all components of a metric tensor are constant in some coor-
dinate system, then all the components of the metric connection vanish. Note that
this is true only in that particular coordinate system. Changing coordinates changes
the aﬃne connection, and in general, the components of a metric connection will
not be zero even if they are zero in some coordinate system. If we use Cartesian
coordinates, then the Euclidean metric is just the Kronecker delta. Therefore, all
components of the metric connection are zero. Similarly, the metric of special rel-
ativity in Cartesian coordinates in ηαβ, whose components are either 0 or 1 or −1.
Hence, all components of the metric connection of special relativity in Cartesian
coordinates vanish.
■
The metric connection has some special properties which are of physical
importance. The ﬁrst property which could be easily veriﬁed is that
gij;k = 0
or
∂gij
∂xk −Γp
jkgip −Γp
ikgpj = 0.
(17.61)
The second property is that between any two points passes a single geodesic
of the metric connection, and this geodesic extremizes the distance between
the two points. If the geometry is Riemannian (i.e., if the metric is positive
deﬁnite) then the geodesic gives the shortest distance. In relativity, where the
metric is not Riemannian, the geodesics give the longest distance.
Example 17.4.2. In this example, we ﬁnd the geodesics of a sphere. The spherical
angular coordinates θ and ϕ can be used on the surface of a sphere of radius a. From
the element of length ds2 = a2dθ2 + a2 sin2 θdϕ2 on this sphere, and using θ and ϕ
to label components, we deduce that
g11 ≡gθθ = a2, g22 ≡gϕϕ = a2 sin2 θ, g12 ≡gθϕ = g21 ≡gϕθ = 0,
and similarly,
g11 ≡gθθ = 1
a2 , g22 ≡gϕϕ =
1
a2 sin2 θ , g12 ≡gθϕ = g21 ≡gϕθ = 0.

17.4 Diﬀerentiation of Tensors
467
Substituting these in (17.59), we can calculate the components of the aﬃne connec-
tion. The nonzero components turn out to be
Γθ
ϕϕ = −sin θ cos θ, Γϕ
θϕ = Γϕ
ϕθ = cot θ.
Using these in the geodesic equation (17.58), we obtain the following two diﬀerential
equations:
d2θ
dt2 −sin θ cos θ
 dϕ
dt
2
= 0,
d2ϕ
dt2 + 2 cot θ dϕ
dt
dθ
dt = 0.
(17.62)
The second equation can be solved to give
dϕ
dt =
C
sin2 θ ⇒dϕ =
C
sin2 θ dt,
(17.63)
where C is a constant of integration. Substituting this in the ﬁrst equation of (17.62)
gives
d2θ
dt2 −C2 cos θ
sin3 θ
= 0.
(17.64)
To ﬁnd the geodesic, it is more convenient to express θ as a function of ϕ. This
means changing the independent variable in Equation (17.64) from t to ϕ. This is
done formally by using the second equation of (17.63) to substitute for dt in (17.62).
Thus, the ﬁrst tem of (17.62) can be written as
d
dt
dθ
dt

=
Cd
sin2 θdϕ

Cdθ
sin2 θdϕ

=
C2
sin2 θ
d
dϕ

1
sin2 θ
dθ
dϕ

.
Substituting this in (17.64) yields
d
dϕ

1
sin2 θ
dθ
dϕ

−cot θ = 0.
Diﬀerentiating the ﬁrst term, we get
−2 cos θ
sin3 θ
 dθ
dϕ
2
+
1
sin2 θ
d2θ
dϕ2 −cot θ = 0,
which can be simpliﬁed to the following diﬀerential equation:
sin θ d2θ
dϕ2 −2 cos θ
 dθ
dϕ
2
−sin2 θ cos θ = 0.
(17.65)
If we could solve this equation, we would ﬁnd θ as a function of ϕ, and this
should be the equation of a geodesic on a sphere. Instead, let us use our knowledge
of the geodesics (curves giving the shortest distance) on a sphere, write it with θ as
a function of ϕ and see if it satisﬁes (17.65). Our sphere is parametrized as
x = a sin θ cos ϕ, y = a sin θ sin ϕ, z = a cos θ.
The great circles—curves of shortest distance—are the intersection of a plane passing
through the origin and the sphere. Such a plane has an equation of the form Ax +

468
Tensor Analysis
By + Cz = 0. The intersection with the sphere is obtained by substituting for x, y,
and z from the above equations:
Aa sin θ cos ϕ + Ba sin θ sin ϕ + Ca cos θ = 0.
Dividing by Ca sin θ and redeﬁning A to be −A/C and B to be −B/C, we get
cot θ = A cos ϕ + B sin ϕ,
as the equation of geodesic on a sphere.
It is straightforward to show that this
equation indeed satisﬁes (17.65).
■
17.5
Riemann Curvature Tensor
Consider a closed loop, such as a rectangle, on a ﬂat surface. Start a vector
at one point of the rectangle (the lower left corner) and carry it parallel to
itself to the point diagonally opposite the initial point [Figure 17.1(a)]. In one
case carry the vector to the right and then up. In the second case carry the
vector up and then to the right. Compare the vector at the end of the two
cases. They are equal. Do the same on a curved space such as the surface of a
sphere. The two vectors at the end do not coincide [see Figure 17.1(b)]! The
degree to which they are diﬀerent is a measure of the curvature of the space.
Let us quantify the notion of the curvature. Suppose that the lower and
upper curves of the “rectangle” are parametrized by t and the right and the
left curves by s. Moving along a curve parametrized by t does not change s,
and vice versa. Using a Taylor expansion, in which derivatives are replaced
by covariant derivatives, parallel translate a contravariant vector Aj ﬁrst to
the right and then upward [see Figure 17.1(b) for clariﬁcation]. Assume that
the lower left corner has (t, s) as the parameter values. As you move along
the lower curve, the parameters change from (t, s) to (t + Δt, s). So, to ﬁrst
order in Δt, we have
Aj(t + Δt, s) = Aj(t, s) + DAj
Dt Δt = Aj(t, s) + Aj
;l(t, s)dxl
dt Δt.
(a)
(b)
Figure 17.1: (a) In a ﬂat space, the direction of the vector does not change when
carried along two diﬀerent paths. (b) In a curved space, the two vectors are diﬀerent.

17.5 Riemann Curvature Tensor
469
Now parallel translate this vector upward, the direction in which t is constant
but s changes:
Aj(t + Δt, s + Δs) = Aj(t + Δt, s) + D
Ds(Aj(t + Δt, s))Δs
= Aj(t, s) + DAj
Dt Δt + D
Ds

Aj(t, s) + Aj
;l(t, s)dxl
dt Δt

Δs
= Aj(t, s) + DAj
Dt Δt + DAj
Ds Δs + D
Ds

Aj
;l(t, s)dxl
dt Δt

Δs
= Aj(t, s) + DAj
Dt Δt + DAj
Ds Δs + Aj
;l;m(t, s)dxm
ds
dxl
dt ΔtΔs.
Since Aj is assumed to be parallel translated on both curves, DAj/Dt = 0 =
DAj/Ds, and
Aj(t + Δt, s + Δs)1 = Aj(t, s) + Aj
;l;m(t, s)ΔxlΔxm,
where we used Δxl ≈(dxl/dt)Δt and Δxm ≈(dxm/ds)Δs. The subscript 1
on the left hand side stands for the “ﬁrst route.” The “second route” is going
up ﬁrst and then to the right. It should be clear that the only diﬀerence in
the ﬁnal result is the interchange of l and m. We therefore have
Aj(t + Δt, s + Δs)2 = Aj(t, s) + Aj
;m;l(t, s)ΔxlΔxm.
Thus, using Aj
;lm for the second covariant derivative, we have
Aj(t+Δt, s+Δs)1 −Aj(t+Δt, s+Δs)2 =

Aj
;lm −Aj
;ml

ΔxlΔxm. (17.66)
The diﬀerence in parentheses should be related to the curvature of the space
(manifold) under consideration.
Finding this diﬀerence is straightforward. Using the rule of covariant dif-
ferentiation for general tensors, we get
Aj
;lm =
∂Aj
;l
∂xm + Γj
kmAk
;l −Γp
lmAj
;p
=
∂
∂xm
∂Aj
∂xl + Γj
klAk

+ Γj
km
∂Ak
∂xl + Γk
rlAr

−Γp
lmAj
;p
=
∂2Aj
∂xm∂xl + ∂Γj
kl
∂xm Ak + Γj
kl
∂Ak
∂xm + Γj
km
∂Ak
∂xl + Γj
kmΓk
rlAr −Γp
lmAj
;p.
In the last line switch l and m to get Aj
;ml:
Aj
;ml =
∂2Aj
∂xl∂xm + ∂Γj
km
∂xl Ak + Γj
km
∂Ak
∂xl + Γj
kl
∂Ak
∂xm + Γj
klΓk
rmAr −Γp
mlAj
;p.
Subtracting, and changing the dummy indices when necessary, we obtain
Aj
;lm −Aj
;ml =

∂Γj
kl
∂xm −∂Γj
km
∂xl
+ Γj
rmΓr
kl −Γj
rlΓr
km

Ak −(Γp
lm −Γp
ml) Aj
;p.
(17.67)

470
Tensor Analysis
It is straightforward but tedious to show that the expression in the ﬁrst
pair of parentheses transforms as a component of a tensor of type (1, 3). This
tensor is denoted by Rj
klm and is called Riemann curvature tensor:
Riemann
curvature tensor
Rj
klm = ∂Γj
kl
∂xm −∂Γj
km
∂xl
+ Γj
rmΓr
kl −Γj
rlΓr
km.
(17.68)
The expression in the second pair of parentheses in (17.67) is the torsion tensor
introduced earlier [see Equation (17.53) and the discussion after it].
Example 17.5.1. Example 17.4.1 showed that the metric connection of Euclidean
space and special relativistic spacetime in Cartesian coordinates are both zero.
Equation (17.68) shows that for these spaces, the Riemannian curvature tensor
expressed in Cartesian coordinates is zero. Since Riemannian curvature tensor is a
ﬂat spaces (or
manifolds)
tensor, it must be zero in all coordinates, as expressed in Box 17.2.2. Spaces that
have zero Riemannian curvature tensor are called ﬂat. We thus see that ﬂatness
is an intrinsic property of a space, independent of any coordinates used in that
space.
■
The curvature tensor has some important properties which we state with-
out proof. One property that is evident from (17.68) is
.Rj
klm = −Rj
kml
(17.69)
The second property, which is true only if the torsion tensor vanishes, i.e.,
when the aﬃne connection is symmetric, is
Rj
klm + Rj
lmk + Rj
mkl = 0.
(17.70)
The third property, which involves the covariant derivative of the curvature
Bianchi identity
tensor and is true only for torsion-free connections, is
Rj
klm;i + Rj
kmi;l + Rj
kil;m = 0.
(17.71)
This is also called the Bianchi identity. The last property, which holds for
Riemannian tensor of the metric connection, is that Rj
klm has n2(n2 −1)/12
components.
Various other tensors can be obtained from the Riemann curvature tensor
by contraction. For example, by contracting the contravariant index with the
last covariant index one obtains the so-called Ricci tensor:
Ricci tensor and
scalar curvature
Rkl = Rj
klj = Rj
klj = ∂Γj
kl
∂xj −
∂Γj
kj
∂xl + Γj
rjΓr
kl −Γj
rlΓr
kj,
(17.72)
and by raising one of the Ricci tensor’s indices and contracting, we obtain the
scalar curvature:
R = Rl
l = gklRkl.
(17.73)
Einstein’s general theory of relativity explains gravity as a manifestation
of the curvature of spacetime. Since gravity is caused by mass, and since

17.6 Problems
471
mass and energy are equivalent, the source of curvature is energy. Pursuing
Einstein curvature
tensor and
Einstein equation
this idea, Einstein came up with an equation, the Einstein equation, that
describes all (large scale) gravitational interactions. Deﬁning the Einstein
curvature tensor as
Gij ≡Rij −1
2gijR,
(17.74)
the Einstein equation is written as
Gij = 8πGTij,
(17.75)
where G is the universal gravitational constant and Tij is the energy momen-
tum tensor.
Example 17.5.2. For the sphere of Example 17.4.2, the Ricci curvature tensor
can be written as
Rkl = ∂Γθ
kl
∂θ
−
∂Γϕ
kϕ
∂xl
+ Γϕ
θϕΓθ
kl −Γθ
ϕlΓϕ
kθ −Γϕ
θlΓθ
kϕ −Γϕ
ϕlΓϕ
kϕ
Using this, it is easy to show that Rθϕ = 0 = Rϕθ, while
Rθθ = 1,
Rϕϕ = sin2 θ
Furthermore, since gθθ = 1/a2 and gϕϕ = 1/(a2 sin2 θ), the scalar curvature becomes
R = gijRij = gθθRθθ + gϕϕRϕϕ = 2
a2
showing that a sphere is a space of constant (and positive) curvature, as we
expect.
■
17.6
Problems
17.1. Write ∂ixj in a form that includes the Kronecker delta. Now show that
∇· r = 3.
17.2. Recall that a homogeneous function f of n variable of degree q satisﬁes
qf(x1, x2, . . . , xn) =
n

i=1
xi∂if.
(a) Diﬀerentiate both sides with respect to xj and show that
(q −1)∂jf(x1, x2, . . . , xn) =
n

i=1
xi∂i∂jf.
(b) Multiply this equation by xj and sum over j to obtain
q(q −1)f(x1, x2, . . . , xn) =
n

i,j=1
xixj∂i∂jf.

472
Tensor Analysis
17.3. Verify Equation (17.23).
17.4. Let the scalar function φ be given by φ(x, y, z) = x2 + y3 + z and
x = sin ¯x + cos ¯y + ¯z, y = ¯x¯y + ¯z, z = ¯x2.
What is the functional form of ¯φ?
17.5. Show that the sum of two tensors of type (r, s) is a tensor of the same
type.
17.6. Derive Equation (17.29). Show that δ12···n
12···n = 1.
17.7. Show that the inverse of a metric tensor given by
gkm(x) ≡
n

p=1
∂xk
∂x′p
∂xm
∂x′p
is a tensor of type (2, 0). Here {x′i} are as deﬁned in the beginning of Section
17.3.
17.8. Following Example 17.3.1, ﬁnd the metric tensor for cylindrical coordi-
nates.
17.9. Show that the dot products of Equations (17.36) and (17.37) do not
change in a general coordinate transformation.
17.10. Verify Equation (17.40) component by component.
17.11. Using indices, show that the divergence of a curl and the curl of a
gradient are both zero.
17.12. Using indices, prove the following “derivative” identities:
∇· (fA) = (∇f) · A + f∇· A,
∇× (fA) = (∇f) × A + f∇× A,
∇(fg) = g∇f + f∇g.
17.13. Using indices, prove the Green’s identity:
∇· (g∇f −f∇g) = g∇2f −f∇2g.
17.14. Prove the following vector identities using index notation for vectors:
∇· (A × B) = B · ∇× A −A · ∇× B,
∇× (∇× A) = ∇(∇· A) −∇2A.
17.15. Show that the diﬀerence between any two aﬃne connections is a tensor
of type (1, 2).

17.6 Problems
473
17.16. Verify that Equation (17.46) combines the second and third Maxwell’s
equations.
17.17. Verify that Fαβ = ∂βAα −∂αAβ satisﬁes Equation (17.46).
17.18. Diﬀerentiate both sides of Equation (17.45) with respect to xβ and
raise the index β to be able to sum over it; use the symmetry of second
derivative and the antisymmetry of Fαβ to show that the left-hand side is
zero. On the right-hand side, you should have something like μ0ηβσ∂σJβ.
Show that ηβσ∂σJβ = 0 expresses charge conservation or continuity equation
of Box 13.2.4.
17.19. With c = 1 and μ0 = 1/ϵ0, show that ηαν∂νAα = 0 is the Lorentz
gauge condition [Equation (15.32)]
∂Φ
∂t + ∇· A = 0,
and that ηαν∂ν∂αAβ = μ0Jβ combines the two wave equations [Equations
(15.33) and (15.34)]
∂2A
∂t2 −∇2A = μ0J,
∂2Φ
∂t2 −∇2Φ = μ0ρ.
17.20. Show that DAj of Box 17.4.1 is a covariant vector.
17.21. Show that
∂Aj
∂xl + Γj
klAk
is a tensor of type (1, 1).
17.22. Show that Γj
lk given in Equation (17.59) is an aﬃne connection, i.e.,
that it transforms according to Equation (17.52).
17.23. Show that the metric connection satisﬁes Equation (17.61).
17.24. (a) Find all the components of the aﬃne metric connection on the
surface of the sphere of Example 17.4.2.
(b) Derive Equation (17.62) from Equation (17.58).
(c) Show that (17.63) satisﬁes the second equation of (17.62).
(d) Show that cot θ = A cos ϕ + B sin ϕ is a solution of (17.65).
17.25. Show that the Riemann curvature tensor of Equation (17.68) is a
tensor of type (1, 3).
17.26. Example 17.5.1 showed that the Riemannian curvature tensor of the
Euclidean space, when expressed in Cartesian coordinates is zero. Since Rie-
mannian curvature tensor is a tensor it should be zero when expressed in

474
Tensor Analysis
any coordinate system. Starting with the spherical components of the Eu-
clidean metric obtained in Example 17.3.1, ﬁnd the components of the metric
connection in spherical coordinates. From these calculate the components of
Riemannian curvature tensor and show that they all vanish.
17.27. Derive the expression for the Ricci curvature tensor of Example 17.5.2
and show that
Rθϕ = 0 = Rϕθ,
Rθθ = 1,
Rϕϕ = sin2 θ.

Part V
Complex Analysis

Chapter 18
Complex Arithmetic
Complex numbers were developed because there was a need to expand the
notion of numbers to include solutions of algebraic equations whose proto-
type is x2 + 1 = 0. Such developments are not atypical in the history of
mathematics. The invention of irrational numbers occurred because of a need
for a number that could solve an equation of the form x2 −2 = 0. Similarly,
rational numbers were the oﬀspring of the operations of multiplication and
division and the quest for a number that gives, for example, 4 when multiplied
by 3, or, equivalently, a number that solves the equation 3x −4 = 0.
There is a crucial diﬀerence between complex numbers and all the num-
bers mentioned above: All rational, irrational, and, in general, real numbers
correspond to measurable physical quantities.
However, there is no single
measurable physical quantity that can be described by a complex number.
A natural question then is this: What need is there for complex numbers
if no physical quantity can be measured in terms of them? The answer is that
although no single physical quantity can be expressed in terms of complex
numbers, a pair of physical quantities can be neatly described by a single
complex number. For example, a wave with a given amplitude and phase can
be concisely described by a complex number. Another, more fundamental,
reason is that equations that describe the behavior of subatomic particles are
inherently complex.
18.1
Cartesian Form of Complex Numbers
We demand a number system broad enough to include solutions to the
equation
x2 + 1 = 0
or
x2 = −1.
Clearly the solution(s) cannot be real because a real number raised to the
second power gives a positive real number, and we want x2 to be negative.

478
Complex Arithmetic
So we broaden the concept of numbers by considering complex numbers.
Such numbers are of the form
Cartesian form of
a complex number
z = x + iy
with
i ≡
√
−1
and
i2 = −1.
(18.1)
It turns out that we don’t need to introduce any other numbers to solve all
algebraic equations—equations of the form p(x) = 0 with p(x) a polynomial.
In fact, the fundamental theorem of algebra, to which we shall return,
states that all roots of any algebraic equation
anxn + an1−xn−1 + · · · + a1x + a0 = 0
with arbitrary real or complex coeﬃcients a0, a1, . . . , an, are in the complex
number system. In this sense, then, the complex number system is the most
complete system.
A complex number can be conveniently represented as a point (or equiv-
alently, as a vector) in the xy-plane, called the complex plane, as shown
complex plane,
real and imaginary
parts
in Figure 18.1. In Equation (18.1), x is called the real part of z, written
Re(z), and y is called the imaginary part of z, written Im(z). Similarly, the
horizontal axis in Figure 18.1 is named the real axis, and the vertical axis is
named the imaginary axis. The set of all complex numbers—or the set of
points in the complex plane—is denoted by C.
We can deﬁne various operations on C that are extensions of similar oper-
ations on the real number system, R. The only proviso is that i2 = −1, and
that the ﬁnal form of an equation must be written as Equation (18.1)—with
real and imaginary parts. For instance, the sum of two complex numbers,
z1 = x1 + iy1 and z2 = x2 + iy2, is
z1 + z2 = (x1 + x2) + i(y1 + y2).
This sum can be represented in the complex plane as the vector sum of z1 and
z2, as shown in Figure 18.2. The product of z1 and z2 can also be obtained:
z1z2 = (x1 + iy1)(x2 + iy2) = x1x2 + x1(iy2) + iy1x2 + iy1(iy2)
= x1x2 + i(x1y2 + y1x2) −y1y2 = x1x2 −y1y2 + i(x1y2 + y1x2).
Thus,
Re(z1z2) = x1x2 −y1y2,
Im(z1z2) = x1y2 + x2y1.
(18.2)
Im
Re
x
y
z
Figure 18.1: Complex numbers as points or vectors in a plane.

18.1 Cartesian Form of Complex Numbers
479
Re
Im
z1
z2
z1 + z2
Figure 18.2: Addition of complex numbers as addition of vectors.
To obtain this equation, we have implicitly used the fact that two complex
numbers are equal if and only if their real parts are equal and their imaginary
parts are equal.
The factor i in z allows new operations for complex numbers that do not
exist for real numbers. One such operation is complex conjugation. The
complex
conjugation
complex conjugate, z∗or ¯z, of z is deﬁned as
z∗≡¯z = (x + iy)∗= x −iy
(18.3)
which is obtained from z by replacing i with −i. We note immediately that
zz∗= (x + iy)(x −iy) = x2 + y2 = z∗z
which is a positive real number. The positive square root of zz∗is called the
absolute value of z and denoted by |z|. It is simply the length of the vector
absolute value
representing z in the xy-plane. Thus, we have
|z| =
√
zz∗=
√
z∗z =
	
x2 + y2 =

(Re(z))2 + (Im(z))2.
(18.4)
We can also deﬁne the division of two complex numbers using complex
conjugation.
Box 18.1.1. To ﬁnd the real and imaginary parts of a quotient, multiply
the numerator and denominator by the complex conjugate of the denomi-
nator.
So, for the ratio of z1/z2, we get
z1
z2
= z1z∗
2
z2z∗
2
= (x1 + iy1)(x2 −iy2)
|z2|2
= x1x2 + y1y2 + i(y1x2 −x1y2)
|z2|2
= x1x2 + y1y2
|z2|2
+ iy1x2 −x1y2
|z2|2
.
Thus,
Re
z1
z2

= x1x2 + y1y2
x2
2 + y2
2
and
Im
z1
z2

= y1x2 −x1y2
x2
2 + y2
2
.
(18.5)

480
Complex Arithmetic
In particular,
1
z = z∗
|z|2 = x −iy
x2 + y2
and
1
i = −i.
Some useful properties of absolute values are as follows:
properties of
absolute value of
complex numbers
|z1z2| = |z1| |z2|,




z1
z2




 = |z1|
|z2|,



 |z1| −|z2|



 ≤|z1 + z2| ≤|z1| + |z2|.
(18.6)
This last inequality is called the triangle inequality and it comes directly
from the vector property of complex numbers. The right half of it can be
generalized to more than two complex numbers:





n

k=1
zk





 ≤
n

k=1
|zk| .
(18.7)
Example 18.1.1. Here we present some sample manipulations with complex num-
bers:
(1 + i)2 = (1)2 + (i)2 + 2i = 1 −1 + 2i = 2i,
1
1 −i −
1
1 + i = 1 + i −(1 −i)
(1 −i)(1 + i) =
2i
|1 + i|2 = 2i
2 = i,
(1 + i)−4 =
1
(1 + i)2(1 + i)2 =
1
(2i)(2i) =
1
−4 = −1
4,
2 + i
3 −i = (2 + i)(3 + i)
|3 −i|2
=
5 + i5
32 + (−1)2 = 1
2 + i1
2,




2i −1
i −2




 = | −1 + i2|
| −2 + i| =
	
(−1)2 + 22
	
(−2)2 + 12 = 1.
The equation |z −a| = b, where a is a ﬁxed complex number and b is real and
positive, describes a circle of radius b with center at a ≡ax + iay. This is easily
seen because
b2 = |z −a|2 = |(x + iy) −(ax + iay)|2
= |(x −ax) + i(y −ay)|2 = (x −ax)2 + (y −ay)2.
We note that |z −a| is the distance between the two complex numbers z and a.
Therefore, |z −a| = b—with a a constant and z a variable—is the collection of all
points z that are at a distance b from a.
■
Complex conjugation satisﬁes some nice properties that we list below:
properties of
complex
conjugation of
complex numbers
(z1 + z2)∗= z∗
1 + z∗
2,
(z1z2)∗= z∗
1z∗
2,
z1
z2
∗
= z∗
1
z∗
2
,
Re(z) = 1
2(z + z∗),
Im(z) = 1
2i(z −z∗),
(18.8)
(z∗)∗= z,
(zn)∗= (z∗)n.

18.1 Cartesian Form of Complex Numbers
481
The complex conjugate of a function of z is easily obtained by substituting
to ﬁnd the
complex conjugate
of a function,
change all its i’s
to −i.
z∗for z in that function.1 This can be summarized as
(f(z))∗= f(z∗)
(18.9)
which is equivalent to replacing every i with −i in the expression for f(z).
Historical Notes
In the ﬁrst half of the sixteenth century there was hardly any change from the
attitude or spirit of Arabs, whose work had put practical arithmetical calculations
in the forefront of mathematics, but merely an increase in the kind of activity
Europeans had learned from Arabs. Moreover, the technological advances spurred by
the Renaissance demanded further reﬁnement in magnitudes such as trigonometric
tables and astronomical observations.
By 1500 or so, zero was accepted as a number and irrational numbers were used
more freely in calculations. However, the problem of whether irrationals were really
numbers still troubled people. Michael Stifel (1486?–1567), the German mathemati-
cian, argued that
Since, in proving geometrical ﬁgures, when rational numbers fail us irrational
numbers . . . prove exactly those things which rational numbers could not prove
. . . we are compelled to assert that they truly are numbers . . . . On the other
hand, . . . that cannot be called a true number which is of such a nature that
it lacks precision [decimal representation].
He then argues that only whole numbers or fractions can be called true numbers, and
since irrationals are neither, they are not real numbers. Even a century later, Pascal,
Barrow, and Newton thought of irrational numbers as being understood in terms of
geometric magnitude; they were mere symbols that had no existence independent
of continuous geometrical magnitude.
Negative numbers were treated with equal suspicion by the sixteenth- and
seventeenth-century mathematicians.
They were considered “absurd.”
Jerome
Cardan (1501–1576), the great Italian mathematician of the Renaissance, was will-
ing to accept the negative numbers as roots of equations, but considered them as
“ﬁctitious,” while he called the positive roots real. Fran¸cois Vieta (1540–1603), a
lawyer by profession but recognized far more as the foremost mathematician of the
sixteenth century, discarded negative numbers entirely. Descartes accepted them in
part, but called negative roots of equations false, on the grounds that they repre-
sented numbers less than nothing.
An interesting argument against negative numbers was given by Antoine Arnauld
(1612–1694), a theologian and mathematician who was a close friend of Pascal.
Arnauld questioned the equality −1 : 1 = 1 : (−1) because, he said, −1 is less than
+1; hence, How could a smaller number be to a greater as a greater is to a smaller?
Without having fully overcome their diﬃculties with irrational and negative
numbers, the Europeans were hit by another problem: the complex numbers! They
obtained these new numbers by extending the arithmetic operation of square root
1This statement is not strictly true for all functions. However, only a mild restriction
is to be imposed on them for the statement to be true. We shall not go into details of
such restrictions because they require certain complex analytic tools which go beyond the
scope of this book. See Hassani, S. Mathematical Physics: A Modern Introduction to Its
Foundations, Springer-Verlag, 1999, Chapter 11 for details.

482
Complex Arithmetic
to whatever numbers appeared in solving quadratic equations. Thus Cardan sets
up and solves the problem of dividing 10 into two parts whose product is 40. The
equation is x(10−x) = 40, for which he obtains the roots 5±√−15 and then he says
“Putting aside the mental torture involved,” multiply these two roots and note that
the product is 25 −(−15) or 40. He then states, “So progresses arithmetic subtlety
the end of which, as is said, is as reﬁned as it is useless.”
Descartes also rejected complex roots and coined them “imaginary.” Even New-
ton did not regard complex roots as signiﬁcant, most likely because in his day they
lacked physical meaning. The confusion surrounding complex numbers is illustrated
by the oft-quoted statement by Leibniz, “The Divine Spirit found a sublime outlet
in that wonder of analysis, that portent of the ideal world, that amphibian between
being and not being, which we call the imaginary root of negative unity.”
18.2
Polar Form of Complex Numbers
The introduction of polar coordinates in the complex plane makes available
a powerful tool with which to facilitate complex manipulations. Figure 18.3
shows a complex number and its polar coordinates. In terms of these polar
coordinates, z can be written as
polar
representation of a
complex number
z = x + iy = r cos θ + ir sin θ = r(cos θ + i sin θ).
(18.10)
Assuming that series of complex numbers can be manipulated as those of real
numbers,2 we obtain the useful relation between imaginary exponentials and
trigonometric functions.
In Chapter 10 we presented the Maclaurin series for the exponential and
trigonometric functions.
Let us assume that those functions are valid for
complex numbers as well. Then, we have
a very important
relation
eiθ =
∞

n=0
(iθ)n
n!
=
∞

n=even
(iθ)n
n!
+
∞

n=odd
(iθ)n
n!
=
∞

k=0
(iθ)2k
(2k)! +
∞

k=0
(iθ)2k+1
(2k + 1)!
=
∞

k=0
(−1)k θ2k
(2k)! + i
∞

k=0
(−1)k
θ2k+1
(2k + 1)! = cos θ + i sin θ
(18.11)
Im
Re
z
r sin θ
r cos θ
θ
r = |z|
Figure 18.3: Complex numbers in polar coordinates.
2This assumption turns out to be correct. In particular, the power series expansion used
in the following example plays a central role in complex analysis.

18.2 Polar Form of Complex Numbers
483
because i2k = (i2)k = (−1)k. This is probably the most important relation in
complex number theory.
Box 18.2.1. The trigonometric and imaginary exponential functions are
related by the Euler equation: eiθ = cos θ + i sin θ.
The use of Equation (18.11) in (18.10) leads to another way of representing
complex numbers:
z = reiθ,
r =
	
x2 + y2,
θ = tan−1 y
x

.
(18.12)
Note that
Box 18.2.2. The angle θ is not uniquely determined: Any multiple of 2π
can be added to it without aﬀecting z.
We can use Equation (18.12) together with x = r cos θ, y = r sin θ to con-
vert from Cartesian coordinates to polar coordinates, and vice versa. The
coordinate θ is called the argument of z and written θ = arg(z).
argument of a
complex number
Example 18.2.1. Let us look at some numerical examples of polar-Cartesian
conversion.
In many cases, a diagram can be very helpful.
For instance, take i
whose real part is obviously zero and whose imaginary part is 1. If we were to use
the formula, we would have tan θ = 1/0 which is not deﬁned. However, Figure 18.4
shows that z = i lies on the positive imaginary axis, and, thus, θ = π/2. Since we
can always add a multiple of 2π to the angle, we have
i = eiπ/2+i2nπ,
n = 0, ±1, ±2, . . . .
Similarly, the same ﬁgure makes it clear that
−i = e−iπ/2+i2nπ = ei3π/2+i2nπ,
n = 0, ±1, ±2, . . . .
θ  =  π/2
r
1
=
i
Re
Im
θ  =  − π/2
Re
Im
−i
r
1
=
Figure 18.4: Cartesian and polar coordinates for i and −i.

484
Complex Arithmetic
Re
θ  =  π
r = 1
Im
−1
θ  = π/4
Re
Im
1+ i
1− i
θ  = − π/4
θ
Re
Im
2 + i3
θ
Re
Im
−1 + i2
Figure 18.5: Cartesian and polar coordinates for some other complex numbers.
Referring to Figure 18.5, the reader may verify the following polar representa-
tions of complex numbers:
−1 = eiπ+i2nπ,
1 + i =
√
2 eiπ/4+i2nπ,
1 −i =
√
2 e−iπ/4+i2nπ =
√
2 ei7π/4+i2nπ,
2 + i3 =
√
13 ei tan−1(3/2)+i2nπ =
√
13 ei0.983+i2nπ,
−1 + i2 =
√
5 ei tan−1(−2)+i2nπ =
√
5 ei2.03+i2nπ.
In all cases, n is an integer and angles are in radians.
■
The complex conjugate of z in polar coordinates is
z∗= x −iy = r cos θ −ir sin θ = r cos(−θ) + ir sin(−θ) = re−iθ.
This equation conﬁrms the earlier statement that complex conjugation is
equivalent to replacing i with −i.
Generally speaking, polar coordinates are useful for operations of multipli-
cation, division, and exponentiation, and Cartesian coordinates for addition
and subtraction.
Example 18.2.2. We can use the polar representation of complex numbers to
ﬁnd some trigonometric identities. In all of the following, we set r = 1:
1 = eiθe−iθ = (cos θ + i sin θ)(cos θ −i sin θ) = cos2 θ + sin2 θ.
Now consider the identity
ei(θ1+θ2) = cos(θ1 + θ2) + i sin(θ1 + θ2)
which can also be written as
ei(θ1+θ2) = eiθ1eiθ2 = (cos θ1 + i sin θ1)(cos θ2 + i sin θ2)
= cos θ1 cos θ2 −sin θ1 sin θ2 + i(sin θ1 cos θ2 + sin θ2 cos θ1).
Equating the real and imaginary parts of the last two equations, we obtain
cos(θ1 + θ2) = cos θ1 cos θ2 −sin θ1 sin θ2,
sin(θ1 + θ2) = sin θ1 cos θ2 + sin θ2 cos θ1.

18.2 Polar Form of Complex Numbers
485
Similarly, equating the real and imaginary parts of
ei3θ = cos 3θ + i sin 3θ
and
ei3θ =

eiθ3
= (cos θ + i sin θ)3 = cos3 θ + 3i cos2 θ sin θ −3 sin2 θ cos θ −i sin3 θ
gives the following trigonometric identity:
cos 3θ = 4 cos3 θ −3 cos θ,
sin 3θ = 3 sin θ −4 sin3 θ.
■
From
einθ = cos nθ + i sin nθ
and
einθ = (eiθ)n = (cos θ + i sin θ)n
we obtain the so-called de Moivre theorem:
de Moivre theorem
(cos θ + i sin θ)n = cos nθ + i sin nθ.
(18.13)
Equation (18.11) and its complex conjugate lead to the following useful
results:
two important
relations
cos θ = 1
2
 
eiθ + e−iθ!
,
sin θ = 1
2i
 
eiθ −e−iθ!
.
(18.14)
As mentioned earlier, the exponential nature of polar coordinates makes
them especially useful in multiplication, division, and exponentiation. For
instance,
z1
z2
= r1eiθ1
r2eiθ2 = r1
r2
ei(θ1−θ2),
z1z2 =
 
r1eiθ1!  
r2eiθ2!
= r1r2ei(θ1+θ2),
(18.15)
√z =
√
reiθ =
 
reiθ!1/2 = r1/2  
eiθ!1/2 = √reiθ/2,
and so forth.
All of these relations have interesting geometric interpretations. For ex-
ample, the second equation says that when you multiply a complex number z1
by another complex number z2, you dilate the magnitude of z1 by a factor r2
and increase its angle by θ2. That is, multiplication involves both a dilation
and a rotation. In particular, if we multiply a complex number by eiωt where
t is time, we get a vector of constant length in the xy-plane that is rotating
with angular velocity ω.
Example 18.2.3. A plane wave is represented by a periodic function such as
A cos(kx −ωt)
or
B sin(kx −ωt).

486
Complex Arithmetic
On the other hand, sine and cosine are related by
sin(kx −ωt) = −cos

kx −ωt + π
2

.
Therefore, one can concentrate solely on the cosine function with a phase an-
gle added to its argument. Thus a typical periodic plane wave is represented as
A cos (kx −ωt + α). To make connection with the material of this section, we note
that
A cos (kx −ωt + α) = A Re

ei(kx−ωt+α)
= Re

Aei(kx−ωt+α)
= Re

Aeiαei(kx−ωt)
= Re

Zei(kx−ωt)
,
where Z is a complex number—called complex amplitude—of magnitude A and
complex amplitude
argument α. It is therefore convenient to represent plane waves by the complex
function Zei(kx−ωt) which includes the phase of the wave as the argument of Z. ■
Another interesting application of these ideas is ﬁnding roots of complex
numbers. Suppose we are interested in all the nth roots of Z; i.e., all z’s
roots of complex
numbers
satisfying zn = Z. To ﬁnd the roots of a complex number Z, write it in polar
form in the most general way:
Z = ReiΘ+i2πk,
k = 0, ±1, ±2, . . . ,
Thus,
zn = ReiΘ+i2πk
with
k = 0, ±1, ±2, . . . .
Taking the nth root of both sides, we obtain
z = Z1/n = R1/neiΘ/n+i2πk/n,
k = 0, ±1, ±2, . . .,
and
Box 18.2.3. The distinct nth roots {zk} of Z = ReiΘ are
zk = R1/neiΘ/n+i2πk/n,
k = 0, 1, 2, . . . , n −1.
(18.16)
We see that the number of nth roots of a complex number is exactly n.
It is clear that zk of Equation (18.16) repeats itself for k ≥n.
Example 18.2.4. Let us ﬁnd the three cube roots of unity. With n = 3 and
Z = ei2πk, we have
zk = ei2πk/3,
k = 0, 1, 2,
or
z0 = e0 = 1,
z1 = ei2π/3 = cos 2π
3 + i sin 2π
3 = −1
2 + i
√
3
2 ,
z2 = ei4π/3 = cos 4π
3 + i sin 4π
3 = −1
2 −i
√
3
2 .

18.2 Polar Form of Complex Numbers
487
It is instructive to show directly that

−1
2 + i
√
3
2
3
= 1
and

−1
2 −i
√
3
2
3
= 1.
Here are some more examples of ﬁnding roots:
√
1 + i =
√
2 eiπ/4+i2nπ1/2
= 21/4eiπ/8+inπ
n = 0, 1,
z0 = 21/4eiπ/8 = 21/4 3
cos
 π
8

+ i sin
π
8
4
= 1.1 + i0.456,
z1 = 21/4eiπ/8+iπ = −21/4eiπ/8 = −1.1 −i0.456.
The equation z3 = i has the roots
3√
i =

eiπ/2+i2nπ1/3
= eiπ/6+i2nπ/3,
n = 0, 1, 2,
or
z0 = eiπ/6 = cos
 π
6

+ i sin
π
6

=
√
3
2 + i1
2,
z1 = eiπ/6+i2π/3 = cos
5π
6

+ i sin
5π
6

= −
√
3
2 + i1
2,
z2 = eiπ/6+i4π/3 = cos
3π
2

+ i sin
3π
2

= −i.
The reader is urged to show that z3
k = i for k = 0, 1, 2.
Note how careful we were to include the factor of ei2nπ when taking roots of
complex numbers.
■
All nth roots of Z = ReiΘ are equally spaced on a circle of radius R1/n in
the complex plane. Figure 18.6 shows two circles on which the sixth and the
eighth roots of unity are located.
π/4
Re
π/3
Re
Im
Im
(a)
(b)
Figure 18.6: The (a) sixth and (b) eighth roots of unity.
Example 18.2.5. In certain applications of electromagnetic wave propagation (as
Cartesian form of
the square root of
a complex number
in conductors) it becomes necessary to ﬁnd an analytic expression for the Cartesian
representation of the square root of a complex number. In this example, we derive
such an expression.

488
Complex Arithmetic
We are trying to calculate the Cartesian representation of the square root of
z = x + iy. First we express z in polar form; next we take its square root, and
ﬁnally reexpress the result in Cartesian form. Thus,
z = rei(θ+2nπ)
where
r =
	
x2 + y2,
tan θ = y
x,
n = 0, ±1, ±2, . . . .
Taking the square root of both sides yields
√z = z1/2 = r1/2ei(θ+2nπ)/2 = (x2 + y2)1/4eiθ/2+inπ
= ±(x2 + y2)1/4eiθ/2 = ±(x2 + y2)1/4

cos θ
2 + i sin θ
2

because einπ = 1 if n is even and einπ = −1 if n is odd. All that is left now is to
express the trigonometric functions in terms of x and y:
cos θ
2 =
. 1
2(1 + cos θ)
/1/2 =
1
√
2

1 +
1
√
1 + tan2 θ
1/2
=
1
√
2

1 +
1
	
1 + (y/x)2
1/2
=
1
√
2

1 +
|x|
	
x2 + y2
1/2
.
Similarly,
sin θ
2 =
1
√
2

1 −
|x|
	
x2 + y2
1/2
.
Collecting all these formulas together and simplifying, we obtain
	
x + iy = ± 1
√
2
	
x2 + y2 + |x|
1/2
+ i
	
x2 + y2 −|x|
1/2
.
(18.17)
The complexity of the expression for the square root rests on our insistence on
an analytic form. The process of converting the Cartesian form of a complex number
to polar, taking the square root, and converting the result back to Cartesian form
is a far easier process than the one leading to Equation (18.17).
■
18.3
Fourier Series Revisited
The connection between the trigonometric and exponential functions can be
utilized to write the Fourier series expansion of periodic functions more suc-
cinctly. If we substitute
cos 2nπx
L
= e2inπx/L + e−2inπx/L
2
,
sin 2nπx
L
= e2inπx/L −e−2inπx/L
2i
,
in Equation (10.38) and collect the similar exponential terms, we obtain
f(x) = a0 + 1
2
∞

n=1
*
(an −ibn) e2inπx/L + (an + ibn) e−2inπx/L+
= a0 + 1
2
∞

n=1
(an −ibn) e2inπx/L + 1
2
∞

n=1
(an + ibn) e−2inπx/L.
(18.18)

18.3 Fourier Series Revisited
489
In the second sum, let n = −m to obtain
2nd sum = 1
2
−∞

m=−1
(a−m + ib−m) e2imπx/L = 1
2
−∞

n=−1
(a−n + ib−n) e2inπx/L,
(18.19)
where in the last step, we switched the dummy index back to n. If we now
introduce new coeﬃcients An deﬁned as
An =
⎧
⎪
⎨
⎪
⎩
1
2 (an −ibn)
if
1 ≤n ≤∞,
1
2 (a−n + ib−n)
if
−∞≤n ≤−1,
a0
if
n = 0,
and use Equation (18.19) in (18.18), we obtain
Fourier series in
terms of complex
exponentials
f(x) =
+∞

n=−∞
Ane2inπx/L
where
L = b −a,
(18.20)
which is the equation we are after. To ﬁnd An directly from this equation,
multiply both sides by e−2ikπx/L, integrate from a to b, and use the readily
obtainable relation
# b
a
e2i(n−k)πx/L =
0
0
if
n ̸= k
L
if
n = k = Lδnk,
(18.21)
where δnk is the Kronecker delta. It follows that
Ak = 1
L
# b
a
f(x)e−2ikπx/Ldx
or
An = 1
L
# b
a
f(x)e−2inπx/Ldx.
(18.22)
It is customary to redeﬁne the coeﬃcients in the summation of Equation
(18.20) in such a way that the summation giving f(x) and the integral giving
An are more symmetric, i.e., have the same constant in front of them. To this
end, deﬁne fn ≡
√
LAn. Then (18.20) and (18.22) become
f(x) =
1
√
L
+∞

n=−∞
fne2inπx/L,
fn =
1
√
L
# b
a
f(x)e−2inπx/Ldx
(18.23)
Note that the coeﬃcients fn are complex; however, when f(x) is a real
function, the exponentials and their complex coeﬃcients add up in such a
way that the ﬁnal result can be expressed as an inﬁnite sum of trigonometric
functions with real coeﬃcients.
In fact, we can show this generally using
Equations (18.23). First, we note that, for real f(x),
f ∗
n =
1
√
L
# b
a
f(x)e+2inπx/Ldx =
1
√
L
# b
a
f(x)e−2i(−n)πx/Ldx = f−n.
(18.24)

490
Complex Arithmetic
Next, we split the sum in (18.23) into positive integers, negative integers, and
zero:
f(x) =
1
√
L
−1

n=−∞
fne2inπx/L + f0
√
L
+
1
√
L
∞

n=1
fne2inπx/L.
(18.25)
Changing the dummy index n to −m, the ﬁrst sum can be rewritten as
1st sum =
1
√
L
−1

−m=−∞
f−me−2imπx/L =
1
√
L
∞

m=1
f−me−2imπx/L
=
1
√
L
∞

m=1
f ∗
me−2imπx/L =
1
√
L
∞

n=1
f ∗
ne−2inπx/L,
where we used Equation (18.24) and changed m back to n at the end. Sub-
stituting the last equation in (18.25) yields
f(x) = f0
√
L
+
1
√
L
∞

n=1

f ∗
ne−2inπx/L + fne2inπx/L
= f0
√
L
+
2
√
L
∞

n=1
Re

fne2inπx/L
showing that f(x) is indeed real. Equation (18.23) implies that f0 is also real
when f(x) is. It is not hard to show that the expression in the parentheses of
the ﬁrst line is the sum of a sine and a cosine with real coeﬃcients.
Example 18.3.1. Let us redo the square potential—whose Fourier series was
calculated in Example 10.6.1—using exponentials. From Equation (18.23), for n ̸= 0,
we obtain
fn =
1
√
2T
# 2T
0
V (t)e−2inπt/(2T )dt =
1
√
2T
# T
0
V0e−inπt/T dt
=
V0
√
2T
T
−inπ e−inπt/T




T
0
=

T
2
V0
inπ [1 −(−1)n]
because einπ = (eiπ)n = (−1)n. Similarly, f0 = V0
	
T/2. We now substitute these
in the Fourier series expansion
V (t) =
1
√
2T
+∞

n=−∞
fne2inπt/2T
to get
V (t) = V0
2 +
−1

n=−∞
V0
2inπ [1 −(−1)n]e2inπt/2T +
∞

n=1
V0
2inπ [1 −(−1)n]e2inπt/2T .

18.4 A Representation of Delta Function
491
If we change the dummy index of the ﬁrst sum from n to −m, and back to n again,
and put the two sums together, we obtain
V (t) = V0
2 +
∞

n=1
V0
2inπ [1 −(−1)n]

einπt/T −e−inπt/T 
= V0
2 + 2V0
π
∞

n=odd
1
2in

einπt/T −e−inπt/T 



=2i sin(nπt/T )
= V0
2 + 2V0
π
∞

k=0
sin[(2k + 1)πt/T]
2k + 1
,
which is the expansion we obtained in Example 10.6.1 using trigonometric
functions.
■
18.4
A Representation of Delta Function
Consider the function DT (x −x0) deﬁned as
DT (x −x0) ≡1
2π
# T
−T
ei(x−x0)tdt.
(18.26)
The integral is easily evaluated, with the result
DT (x −x0) = 1
2π
ei(x−x0)t
i(x −x0)




T
−T
= 1
π
sin T (x −x0)
x −x0
.
The graph of DT (x) as a function of x for various values of T is shown in
Figure 18.7. Note that the width of the curve decreases as T increases. The
area under the curve can be calculated:
# ∞
−∞
DT (x −x0) dx = 1
π
# ∞
−∞
sin T (x −x0)
x −x0
dx = 1
π
# ∞
−∞
sin y
y
dy



=π
= 1.
Figure 18.7 shows that DT (x −x0) becomes more and more like the Dirac
delta function as T gets larger and larger. In fact, we have
δ(x −x0) = lim
T →∞DT (x −x0) = lim
T →∞
1
π
sin T (x −x0)
x −x0
.
(18.27)
To see this, we note that for any ﬁnite T we can write
DT (x −x0) = T
π
sin T (x −x0)
T (x −x0)
.
Furthermore, for values of x that are very close to x0,
T (x −x0) →0
and
sin T (x −x0)
T (x −x0)
→1.

492
Complex Arithmetic
–5
5
0
Figure 18.7: The function sin T x/x also approaches the Dirac delta function
as the width of the curve approaches zero. The value of T is 0.5 for the dashed
curve, 2 for the heavy curve, and 15 for the light curve.
Thus, for such values of x and x0, we have DT (x −x0) ≈(T/π), which is
large when T is large. This is as expected of a delta function: δ(0) = ∞. On
the other hand, the width of DT (x −x0) around x0 is given, roughly, by the
distance between the points at which DT (x −x0) drops to zero: T (x −x0) =
±π, or x −x0 = ±π/T . This width is roughly Δx = 2π/T , which goes to
zero as T grows. Again, this is as expected of the delta function. Therefore,
from (18.26) and (18.27), we have the following important representation of
the Dirac delta function:
delta function as
integral of
imaginary
exponential
δ(x −x0) = 1
2π
# ∞
−∞
ei(x−x0)tdt.
(18.28)
Equation (18.28) can be generalized to higher dimensions, because (at least
in Cartesian coordinates) the multi-dimensional Dirac delta function is just
the product of the one-dimenstional delta functions. Using the more common
k instead of t as the variable of integration, the two-dimensional Dirac delta
function can be represented as
δ(r −r0) =
1
(2π)2
# ∞
−∞
# ∞
−∞
eik·(r−r0)dkxdky ≡
1
(2π)2
##
Ω∞
eik·(r−r0)d2k,
(18.29)
where Ω∞means over all kxky-plane and in the last integral we substituted
d2k for dkxdky.
Similarly, the three dimensional Dirac delta function has the following
representation:
δ(r −r0) =
1
(2π)3
##
Ω∞
eik·(r−r0)d3k,
(18.30)
where d3k means a triple integral over k and Ω∞means over all k-space.

18.5 Problems
493
18.5
Problems
18.1. Find the real and imaginary parts of the following complex numbers:
(a) (2 −i)(3 + 2i).
(b) (2 −3i)(1 + i).
(c) (a −ib)(2a + 2ib).
(d)
i
1 + i.
(e) 1 + i
2 −i.
(f) 1 + 3i
1 −2i.
(g) 1 + 2i
2 −3i.
(h)
2
1 −3i.
(i) 1 −i
1 + i.
(j)
5
(1 −i)(2 −i)(3 −i).
(k) 1 + 2i
3 −4i + 2 −i
5i .
18.2. Convert the following complex numbers to polar form and ﬁnd all cube
roots of each:
(a) 2 −i.
(b) 2 −3i.
(c) 3 −2i.
(d) i.
(e) −i.
(f)
i
1 + i.
(g) 1 + i
2 −i.
(h)1 + 3i
1 −2i.
(i) 1 + i
√
3.
(j) 2 + 3i
3 −4i.
(k) 27i.
(l) −64.
(m) 2 −5i.
(n) 1 + i.
(o) 1 −i.
(p) 5 + 2i.
18.3. Using polar coordinates, show that:
(a) (−1 + i)7 = −8(1 + i).
(b) (1 + i
√
3)−10 = 2−11(−1 + i
√
3).
18.4. Find the real and imaginary parts of the following:
(a) (1 + i
√
3)3.
(b) (2 + i)53.
(c)
4√
i.
(d)
3
1 + i
√
3.
(e) (1 + i
√
3)63.
(f)
1 −i
1 + i
81
.
(g)
6√
−i.
(h)
4√
−1.
(i)

1 + i
√
3
√
3 + i
217
.
(j) (1 + i)22.
(k)
6√
1 −i.
(l) (1 −i)4.
18.5. Find the Cartesian form of all complex numbers z which satisfy (a)
z3 + 1 = 0, and (b) z4 −16i = 0.
18.6. Find the absolute value of 3 + 4i
3 −4i and a + ib
a −ib.
18.7. Derive the following trigonometric identities:
cos 3θ = 4 cos3 θ −3 cosθ,
sin 3θ = 3 sin θ −4 sin3 θ.
18.8. Show that Equation (18.11) leads to Equation (18.14).

494
Complex Arithmetic
18.9. Show that z is real if and only if z = z∗.
18.10. Show that | Re(z)| + | Im(z)| ≥|z| ≥(| Re(z)| + | Im(z)|)/
√
2.
18.11. Let z1 = x1 + iy1 and z2 = x2 + iy2 represent two planar vectors z1
and z2. Show that
z1z∗
2 = z1 · z2 −iˆez · z1 × z2.
18.12. Sketch the set of points determined by each of the following conditions:
(a) |z −2 + i| = 2.
(b) |z + 2i| ≤4.
(c) |z + i| = |z −i|.
(d) Im(z∗+ i) = 2.
(e) 2z + 3z∗= 1.
(f) z2 + (z∗)2 = 2.
Hint: Find a relation between x and y.
18.13. Show that the equation of a circle of radius r centered at z0 can be
written as |z|2 −2 Re(zz∗
0) = r2 −|z0|2.
18.14. Given that z1z2 ̸= 0, show that
(a) Re(z1z∗
2) = |z1||z2|, and |z1 + z2| = |z1| + |z2|, if and only if arg(z1) −
arg(z2) = 2nπ, for n = 0, ±1, ±2, . . ..
(b) What does the second equality mean geometrically?
18.15. Assume that z ̸= 1 and zn = 1. Show that 1 +z +z2 +· · ·+zn−1 = 0.
18.16. Substitute x + iy for z in z2 + z + 1 = 0 and solve the resulting
equations for x and y. Compare these with the roots obtained by solving the
equation in z directly.
18.17. Find the roots of z4 + 4 = 0 and use them to factor z4 + 4 into a
product of quadratic polynomials with real coeﬃcients. Hint: First factor
z4 + 4 into linear terms.
18.18. Evaluate the following roots and plot them on the complex plane:
(a)
5√
1 + i.
(b)
4√
−1.
(c)
8√
1.
(d)
5√
−32.
(e)
√
3 + 4i.
(f)
3√
−1.
(g)
4√
−16i.
(h)
6√
−1.
18.19. Use binomial expansion to show directly that

−1
2 + i
√
3
2
3
= 1
and

−1
2 −i
√
3
2
3
= 1.
18.20. Use

eax = eax/a to ﬁnd the indeﬁnite integral of sin2 x. Verify that
the derivative of your answer is indeed sin2 x.
18.21. Use

eax = eax/a and eibx = cos(bx)+i sin(bx) to verify the following
relations by integrating a certain complex exponential:
#
eax cos(bx) dx =
eax
a2 + b2 [a cos(bx) + b sin(bx)],
#
eax sin(bx) dx =
eax
a2 + b2 [a sin(bx) −b cos(bx)],
where a and b are assumed to be real constants.

18.5 Problems
495
18.22. (a) Using N
k=1 rk = (rN+1−r)/(r−1), evaluate the sum N
k=1 e−iβk.
In particular, show that
N

k=1
ei(α−βk) = ei(α−β) e−iβN −1
e−iβ −1 .
(b) Now show that if β = 2π/N, then
N

k=1
cos(α −βk) = 0 =
N

k=1
sin(α −βk).
18.23. Express cos 4θ and sin 4θ in terms of powers of cos θ and sin θ.
18.24. Use mathematical induction to show the de Moivre theorem.
18.25. Using binomial expansion and the de Moivre theorem, show that
cos nθ =
[n/2]

m=0
(−1)m
 n
2m

sin2m θ cosn−2m θ,
sin nθ =
[n/2]

m=0
(−1)m

n
2m + 1

sin2m+1 θ cosn−2m−1 θ,
where [x] stands for the greatest integer less than or equal to x.
18.26. Derive Equation (18.17) from the equations preceding it.
18.27. Find the following sums, where α and β are real:
(a) cos α + cos(α + β) + cos(α + 2β) + · · · + cos(α + nβ).
(b) sin α + sin(α + β) + sin(α + 2β) + · · · + sin(α + nβ).
Hint: Use the result of Problem 18.22.
18.28. Show that
# b
a
e2i(n−k)πx/L =
0
0
if
n ̸= k,
L
if
n = k,
where b = a + L.
18.29. Use Equations (18.20) and (18.21) to obtain Equation (18.22)
18.30. Find the Fourier series expansion of Problem 10.22 using complex
exponentials.
18.31. An electric voltage V (t) is given by
V (t) = V0 sin
 πt
2T

,
0 ≤t ≤T
and repeats itself with period T . Find the Fourier series expansion of V (t)
using complex exponential functions.

496
Complex Arithmetic
18.32. A periodic voltage is given by the formula
V (t) =
0
V0 sin(πt/2T )
if
0 ≤t ≤T,
0
if
T ≤t ≤2T,
in the interval (0, 2T ). Find the Fourier series representation of this voltage
using complex exponential functions.
18.33. A periodic voltage with period 4T is given by
V (t) =
⎧
⎨
⎩
V0

1 −t2
T 2

if
−T ≤t ≤T,
0
if
T ≤|t| ≤2T.
Write the Fourier series of V (t) using complex exponential functions.
18.34. The function f(x) is given by the integral
f(x) =
# ∞
−∞
g(y)eixydy.
Find g(y) as an integral over f(x). Hint: Multiply both sides of the equation
by e−ixz and integrate over x, changing the order of integration on the right-
hand side and using (18.28).

Chapter 19
Complex Derivative
and Integral
So far we have concerned ourselves with the algebra of the complex numbers.
The subject of complex analysis is extremely rich and important. The scope
and the level of this book does not allow a comprehensive treatment of complex
analysis.
Therefore, we shall brieﬂy review some of the more elementary
topics and encourage the reader to refer to more advanced books for a more
comprehensive treatment. We start here, as is done in real analysis, with the
notion of a function.
19.1
Complex Functions
A complex function f(z) is a rule that associates one complex number to
another. We write f(z) = w where both z and w are complex numbers. The
graph of a
complex function
is impossible to
visualize because
it lives in a four
dimensional space.
function f can be geometrically thought of as a correspondence between two
complex planes, the z-plane and the w-plane. In the real case, this correspon-
dence can be represented by a graph. It could also be represented by arrows
from one real line (the x-axis) to another real line (the y-axis) joining a point
of the ﬁrst real line to the image point of the second real line. When the
possibility of graph is available, the second representation of real functions
appears prohibitively clumsy! For complex functions, no graph is available,
because one cannot draw pictures in four dimensions!1 Therefore, the second
alternative is our only choice. The w-plane has a real axis and an imaginary
axis, which we can call u and v, respectively. Both u and v are real functions
of the coordinates of z, i.e., x and y. Therefore, we may write
f(z) = u(x, y) + iv(x, y).
(19.1)
1The “graph” of a complex function would be a collection of pairs (z, f(z)) just as the
graph of a real function is a collection of pairs (x, f(x)). While in the latter case the graph
can be drawn in the (x, y) plane, the former needs four dimensions because both z and f(z)
have two components each.

498
Complex Derivative and Integral
z
w
f
(x, y)
(u, v)
Figure 19.1: A map from the z-plane to the w-plane.
This equation gives a unique point (u, v) in the w-plane for each point
(x, y) in the z-plane (see Figure 19.1). Under f, regions of the z-plane are
mapped onto regions of the w-plane. For instance, a curve in the z-plane may
be mapped into a curve in the w-plane.
Example 19.1.1. Let us investigate the behavior of some elementary complex
functions. In particular, we shall look at the way a line y = mx in the z-plane is
mapped to lines and curves in the w-plane by the action of these functions.
(a) Let us start with the simple function w = f(z) = z2. We have
w = (x + iy)2 = x2 −y2 + 2ixy
with u(x, y) = x2 −y2 and v(x, y) = 2xy.
For y = mx, these equations yield
u = (1 −m2)x2 and v = 2mx2. Eliminating x in these equations, we ﬁnd v =
[2m/(1 −m2)]u. This is a line passing through the origin of the w-plane [see Fig-
ure 19.2(a)]. Note that the angle the line in the w-plane makes with its real axis is
twice the angle the line in the z-plane makes with the x-axis.
(b) Now let us consider w = f(z) = ez = ex+iy, which gives u(x, y) = ex cos y
and v(x, y) = ex sin y. Substituting y = mx, we obtain u = ex cos mx and v =
ex sin mx.
Unlike part (a), we cannot eliminate x to ﬁnd v as an explicit func-
tion of u. Nevertheless, the last pair of equations are the parametric equations of
a curve (with x as the parameter) which we can plot in a uv-plane as shown in
Figure 19.2(b).
■
(a)
α
y
x
2α
u
v
(b)
α
y
x
u
v
Figure 19.2: (a) The map z2 takes a line with slope angle α and maps it onto a line
with twice the angle in the w-plane. (b) The map ez takes the same line and maps it
onto a spiral in the w-plane.

19.1 Complex Functions
499
19.1.1
Derivatives of Complex Functions
Limits of complex functions are deﬁned in terms of absolute values. Thus,
limz→a f(z) = w0 means that, given any real number ϵ > 0, we can ﬁnd a
corresponding real number δ > 0 such that |f(z)−w0| < ϵ whenever |z−a| < δ.
Similarly, we say that a function f is continuous at z = a if limz→a f(z) =
f(a), or if there exist ϵ > 0 and δ > 0 such that |f(z) −f(a)| < ϵ whenever
|z −a| < δ.
The derivative of a complex function is deﬁned as usual:
Deﬁnition 19.1.1. Let f(z) be a complex function. The derivative of f at
z0 is
df
dz




z0
= lim
Δz→0
f(z0 + Δz) −f(z0)
Δz
provided the limit exists and is independent of Δz.
In this deﬁnition “independent of Δz” means independent of Δx and Δy
(the components of Δz) and, therefore, independent of the direction of ap-
proach to z0. The restrictions of this deﬁnition apply to the real case as well.
For instance, the derivative of f(x) = |x| at x = 0 does not exist because it
approaches +1 from the right and −1 from the left.
It can easily be shown that all the formal rules of diﬀerentiation that
apply to the real case also apply to the complex case.
For example, if f
and g are diﬀerentiable, then f ± g, fg, and—as long as g is not zero—f/g
are also diﬀerentiable, and their derivatives are given by the usual rules of
diﬀerentiation.
Box 19.1.1. A function f(z) is called analytic at z0 if it is diﬀerentiable
at z0 and at all other points in some neighborhood of z0. A point at which
f is analytic is called a regular point of f. A point at which f is not
analytic is called a singular point or a singularity of f. A function for
which all points in C are regular is called an entire function.
Example 19.1.2. Let us examine the derivative of f(z) = x + 2iy at z = 0:
example
illustrating
path-dependence
of derivative
df
dz




z=0
= lim
Δz→0
f(Δz) −f(0)
Δz
= lim
Δx→0
Δy→0
Δx + 2iΔy
Δx + iΔy .
In general, along a line that goes through the origin, y = mx, the limit yields
df
dz




z=0
= lim
Δx→0
Δx + 2imΔx
Δx + imΔx = 1 + 2im
1 + im .
This indicates that we get inﬁnitely many values for the derivative depending on
the value we assign to m—corresponding to diﬀerent directions of approach to the
origin. Thus, the derivative does not exist at z = 0.
■

500
Complex Derivative and Integral
A question arises naturally at this point: Under what conditions does
the limit in the deﬁnition of derivative exist?
We will ﬁnd the necessary
and suﬃcient conditions for the existence of that limit. It is clear from the
deﬁnition that diﬀerentiability puts a severe restriction on f(z) because it
requires the limit to be the same for all paths going through z0, the point
at which the derivative is being calculated. Another important point to keep
in mind is that diﬀerentiability is a local property. To test whether or not a
function f(z) is diﬀerentiable at z0, we move away from z0 by a small amount
Δz and check the existence of the limit in Deﬁnition 19.1.1.
For f(z) = u(x, y) + iv(x, y), Deﬁnition 19.1.1 yields
df
dz




z0
= lim
Δx→0
Δy→0
(u(x0 + Δx, y0 + Δy) −u(x0, y0)
Δx + iΔy
+ i v(x0 + Δx, y0 + Δy) −v(x0, y0)
Δx + iΔy
)
.
If this limit is to exist for all paths, it must exist for the two particular paths
on which Δy = 0 (parallel to the x-axis) and Δx = 0 (parallel to the y-axis).
For the ﬁrst path we get
df
dz




z0
= lim
Δx→0
u(x0 + Δx, y0) −u(x0, y0)
Δx
+ i lim
Δx→0
v(x0 + Δx, y0) −v(x0, y0)
Δx
= ∂u
∂x




(x0,y0)
+ i ∂v
∂x




(x0,y0)
.
For the second path (Δx = 0), we obtain
df
dz




z0
= lim
Δy→0
u(x0, y0 + Δy) −u(x0, y0)
iΔy
+ i lim
Δy→0
v(x0, y0 + Δy) −v(x0, y0)
iΔy
= −i ∂u
∂y




(x0,y0)
+ ∂v
∂y




(x0,y0)
.
If f is to be diﬀerentiable at z0, the derivatives along the two paths must be
equal. Equating the real and imaginary parts of both sides of this equation
and ignoring the subscript z0 (x0, y0, or z0 is arbitrary), we obtain
∂u
∂x = ∂v
∂y
and
∂u
∂y = −∂v
∂x.
(19.2)
These two conditions, which are necessary for the diﬀerentiability of f, are
called the Cauchy–Riemann (C–R) conditions.
Cauchy–Riemann
conditions
The arguments leading to Equation (19.2) imply that the derivative, if it
exists, can be expressed as
df
dz = ∂u
∂x + i∂v
∂x = ∂v
∂y −i∂u
∂y .
(19.3)
The C–R conditions assure us that these two equations are equivalent.

19.1 Complex Functions
501
Example 19.1.3. Let us examine the diﬀerentiability of some complex functions.
(a) We have already established that f(z) = x + 2iy is not diﬀerentiable at z = 0.
We can now show that it is has no derivative at any point in the complex plane. This
is easily seen by noting that u = x and v = 2y, and that ∂u/∂x = 1 ̸= ∂v/∂y = 2,
and the ﬁrst C–R condition is not satisﬁed. The second C–R condition is satisﬁed,
but that is not enough.
(b) Now consider f(z) = x2−y2+2ixy for which u = x2−y2 and v = 2xy. The C–R
conditions become ∂u/∂x = 2x = ∂v/∂y and ∂u/∂y = −2y = −∂v/∂x. Thus, f(z)
may be diﬀerentiable. Recall that C–R conditions are only necessary conditions; we
do not know as yet if they are also suﬃcient.
(c) Let u(x, y) = ex cos y and v(x, y) = ex sin y. Then ∂u/∂x = ex cos y = ∂v/∂y
and ∂u/∂y = −ex sin y = −∂v/∂x and the C–R conditions are satisﬁed.
■
The requirement of diﬀerentiability is very restrictive: the derivative must
exist along inﬁnitely many paths. On the other hand, the C–R conditions
seem deceptively mild: they are derived for only two paths. Nevertheless,
the two paths are, in fact, true representatives of all paths; that is , the C–R
conditions are not only necessary, but also suﬃcient. This is the content of
the Cauchy–Riemann theorem which we state without proof:2
Theorem 19.1.4. (Cauchy–Riemann Theorem). The function f(z) =
u(x, y) + iv(x, y) is diﬀerentiable in a region of the complex plane if and only
if the Cauchy–Riemann conditions
∂u
∂x = ∂v
∂y
and
∂u
∂y = −∂v
∂x
are satisﬁed and all ﬁrst partial derivatives of u and v are continuous in that
region. In that case
df
dz = ∂u
∂x + i∂v
∂x = ∂v
∂y −i∂u
∂y .
The C–R conditions readily lead to
∂2u
∂x2 + ∂2u
∂y2 = 0,
∂2v
∂x2 + ∂2v
∂y2 = 0,
(19.4)
i.e., both real and imaginary parts of an analytic function satisfy the two-
harmonic
functions deﬁned
dimensional Laplace equation [Equations (15.13) and (15.15)]. Such functions
are called harmonic functions.
Example 19.1.5. Let us consider some examples of derivatives of complex func-
tions.
(a)
f(z) = z.
Here u = x and v = y; the C–R conditions are easily shown to hold, and for
2For a simple proof, see Hassani, S. Mathematical Physics: A Modern Introduction to
Its Foundations, Springer-Verlag, 1999, Chapter 9.

502
Complex Derivative and Integral
any z, we have df/dz = ∂u/∂x + i∂v/∂x = 1. Therefore, the derivative exists at all
points of the complex plane, i.e., f(z) = z is entire.
(b)
f(z) = z2.
Here u = x2 −y2 and v = 2xy; the C–R conditions hold, and for all points z of the
complex plane, we have df/dz = ∂u/∂x + i∂v/∂x = 2x + i2y = 2z. Therefore, f(z)
is diﬀerentiable at all points. So, f(z) = z2 is also entire.
(c)
f(z) = zn
for n ≥1.
We can use mathematical induction and the fact that the product of two entire
functions is an entire function to show that d
dz (zn) = nzn−1.
(d)
f(z) = a0 + a1z + · · · + an−1zn−1 + anzn,
where ai are arbitrary constants. That f(z) is entire follows directly from (c) and
the fact that the sum of two entire functions is entire.
(e)
f(z) = ez.
Here u(x, y) = ex cos y and v(x, y) = ex sin y. Thus, ∂u/∂x = ex cos y = ∂v/∂y and
∂u/∂y = −ex sin y = −∂v/∂x and the C–R conditions are satisﬁed at every point
(x, y) of the xy-plane. Furthermore,
df
dz = ∂u
∂x + i ∂v
∂x = ex cos y + iex sin y = ex(cos y + i sin y) = exeiy = ex+iy = ez
and ez is entire as well.
(f)
f(z) = 1/z.
The derivative can be found to be f ′(z) = −1/z2 which does not exist for z = 0.
Thus, z = 0 is a singularity of f(z). However, any other point is a regular point of f.
(g)
f(z) = 1/ sin z.
This gives df/dz = −cos z/ sin2 z. Thus, f has (inﬁnitely many) singular points at
z = ±nπ for n = 0, 1, 2, . . . .
■
Example 19.1.5 shows that any polynomial in z, as well as the exponential
function ez is entire. Therefore, any product and/or sum of polynomials and
ez will also be entire.
We can build other entire functions.
For instance,
eiz and e−iz are entire functions; therefore, the complex trigonometric
functions, deﬁned by
complex
trigonometric
functions
sin z = eiz −e−iz
2i
and
cos z = eiz + e−iz
2
(19.5)
are also entire functions. Problem 19.7 shows that sin z and cos z have only
real zeros.
The complex hyperbolic functions can be deﬁned similarly:
complex
hyperbolic
functions
sinh z = ez −e−z
2
and
cosh z = ez + e−z
2
.
(19.6)
Although the sum and product of entire functions are entire, the ratio
is not. For instance, if f(z) and g(z) are polynomials of degrees m and n,
respectively, then for n > 0, the ratio f(z)/g(z) is not entire, because at the
zeros of g(z)—which always exist—the derivative is not deﬁned.

19.1 Complex Functions
503
The functions u(x, y) and v(x, y) of an analytic function have an interesting
property which the following example investigates.
Example 19.1.6. The family of curves u(x, y) = constant is perpendicular to
curves of constant
u and v are
perpendicular.
the family of curves v(x, y) = constant at each point of the complex plane where
f(z) = u + iv is analytic.
This can easily be seen by looking at the normal to the curves. The normal to
the curve u(x, y) = constant is simply ∇u = ⟨∂u/∂x, ∂u/∂y⟩(see Theorem 12.3.2).
Similarly, the normal to the curve v(x, y) = constant is ∇v = ⟨∂v/∂x, ∂v/∂y⟩.
Taking the dot product of these two normals, we obtain
(∇u) · (∇v) = ∂u
∂x
∂v
∂x + ∂u
∂y
∂v
∂y = ∂u
∂x

−∂u
∂y

+ ∂u
∂y
 ∂u
∂x

= 0
by the C–R conditions.
■
Historical Notes
One can safely say that rigorous complex analysis was founded by a single man:
Cauchy. Augustin-Louis Cauchy was one of the most inﬂuential French mathe-
maticians of the nineteenth century. He began his career as a military engineer, but
when his health broke down in 1813 he followed his natural inclination and devoted
himself wholly to mathematics.
In mathematical productivity Cauchy was surpassed only by Euler, and his col-
lected works ﬁll 27 fat volumes. He made substantial contributions to number theory
and determinants; is considered to be the originator of the theory of ﬁnite groups;
and did extensive work in astronomy, mechanics, optics, and the theory of elasticity.
Augustin-Louis
Cauchy 1789–1857
His greatest achievements, however, lay in the ﬁeld of analysis. Together with his
contemporaries Gauss and Abel, he was a pioneer in the rigorous treatment of limits,
continuous functions, derivatives, integrals, and inﬁnite series. Several of the basic
tests for the convergence of series are associated with his name. He also provided the
ﬁrst existence proof for solutions of diﬀerential equations, gave the ﬁrst proof of the
convergence of a Taylor series, and was the ﬁrst to feel the need for a careful study
of the convergence behavior of Fourier series. However, his most important work
was in the theory of functions of a complex variable, which in essence he created and
which has continued to be one of the dominant branches of both pure and applied
mathematics. In this ﬁeld, Cauchy’s integral theorem and Cauchy’s integral formula
are fundamental tools without which modern analysis could hardly exist.
Unfortunately, his personality did not harmonize with the fruitful power of his
mind. He was an arrogant royalist in politics and a self-righteous, preaching, pious
believer in religion—all this in an age of republican skepticism—and most of his
fellow scientists disliked him and considered him a smug hypocrite. It might be fairer
to put ﬁrst things ﬁrst and describe him as a great mathematician who happened
also to be a sincere but narrow-minded bigot.
19.1.2
Integration of Complex Functions
We have thus far discussed the derivative of a complex function. The concept
of integration is even more important because, as we shall see later, derivatives
can be written in terms of integrals.

504
Complex Derivative and Integral
The deﬁnite integral of a complex function is naively deﬁned in analogy
to that of a real function. However, a crucial diﬀerence exists: While in the
complex integrals
are
path-dependent.
real case, the limits of integration are real numbers and there is only one way
to connect these two limits (along the real line), the limits of integration of
a complex function are points in the complex plane and there are inﬁnitely
many ways to connect these two points. Thus, we speak of a deﬁnite integral
of a complex function along a path. It follows that complex integrals are, in
general, path-dependent.
# α2
α1
f(z) dz =
lim
N→∞
Δzi→0
N

i=1
f(zi)Δzi,
(19.7)
where Δzi is a small segment—situated at zi—of the curve that connects the
complex number α1 to the complex number α2 in the z-plane (see Figure 19.3).
An immediate consequence of this equation is




# α2
α1
f(z) dz




 =
lim
N→∞
Δzi→0





N

i=1
f(zi)Δzi





 ≤
lim
N→∞
Δzi→0
N

i=1
|f(zi)Δzi|
=
lim
N→∞
Δzi→0
N

i=1
|f(zi)| |Δzi| =
# α2
α1
|f(z)| |dz|,
(19.8)
where we have used the triangle inequality as expressed in Equation (18.7).
Since there are inﬁnitely many ways of connecting α1 to α2, there is no
guarantee that Equation (19.7) has a unique value: It is possible to obtain
diﬀerent values for the integral of some functions for diﬀerent paths. It may
seem that we should avoid such functions and that they will have no use in
physical applications. Quite to the contrary, most functions encountered, will
not, in general, give the same result if we choose two completely arbitrary
paths in the complex plane.
In fact, it turns out that the only complex
function that gives the same integral for any two arbitrary points connected
by any two arbitrary paths is the constant function. Because of the importance
of paths in complex integration, we need the following deﬁnition:
α1
α2
Δ zi
z i
Figure 19.3: One of the inﬁnitely many paths connecting two complex points α1 and
α2.

19.1 Complex Functions
505
Box 19.1.2. A contour is a collection of connected smooth arcs. When
the beginning point of the ﬁrst arc coincides with the end point of the last
one, the contour is said to be a simple closed contour (or just closed
contour).
We encountered path-dependent integrals when we tried to evaluate the
line integral of a vector ﬁeld in Chapter 14. The same argument for path-
independence can be used to prove (see Problem 19.21)
Theorem 19.1.7. (Cauchy–Goursat Theorem). Let f(z) be analytic on
a simple closed contour C and at all points inside C. Then
2
C
f(z) dz = 0
Equivalently,  α2
α1 f(z) dz is independent of the smooth path connecting α1 and
α2 as long as the path lies entirely in the region of analyticity of f(z).
Example 19.1.8. We consider a few examples of deﬁnite integrals.
(a) Let us evaluate the integral I1 =

γ1 z dz where γ1 is the straight line drawn
from the origin to the point (1, 2) (see Figure 19.4). Along such a line y = 2x and
thus γ1(t) = t + 2it where 0 ≤t ≤1, and3
I1 =
#
γ1
z dz =
# 1
0
(t + 2it)(dt + 2idt) =
# 1
0
(−3tdt + 4itdt) = −3
2 + 2i.
For a diﬀerent path γ2, along which y = 2x2, we get γ2(t) = t+2it2 where 0 ≤t ≤1,
and
I′
1 =
#
γ2
z dz =
# 1
0
(t + 2it2)(dt + 4itdt) = −3
2 + 2i.
Therefore, I1 = I′
1. This is what is expected from the Cauchy–Goursat theorem
because the function f(z) = z is analytic on the two paths and in the region bounded
by them.
γ1
γ 2
γ 3
x
y
(1, 2)
Figure 19.4: The three diﬀerent paths of integration corresponding to the integrals I1,
I′
1, I2, and I′
2.
3We are using the parameterization x = t, y = 2x = 2t for the curve.

506
Complex Derivative and Integral
(b) Now let us consider I2 =

γ1 z2dz with γ1 as in part (a). Substituting for z in
terms of t, we obtain
I2 =
#
γ1
(t + 2it)2(dt + 2idt) = (1 + 2i)3
# 1
0
t2dt = −11
3 −2
3i.
Next we compare I2 with I′
2 =

γ3 z2dz where γ3 is as shown in Figure 19.4. This
path can be described by
γ3(t) =
0
t
for
0 ≤t ≤1,
1 + i(t −1)
for
1 ≤t ≤3.
Therefore,
I′
2 =
# 1
0
t2dt +
# 3
1
[1 + i(t −1)]2(idt) = 1
3 −4 −2
3i = −11
3 −2
3i,
which is identical to I2, once again because the function is analytic on γ1 and γ3 as
well as in the region bounded by them.
(c) An example of the case where equality for diﬀerent paths is not attained is
I3 =

γ4 dz/z where γ4 is the upper semicircle of unit radius, as shown in Figure 19.5.
A parametric equation for γ4 can be given in terms of θ:
γ4(θ) = cos θ + i sin θ = eiθ ⇒dz = ieiθdθ,
0 ≤θ ≤π.
Thus, we obtain
I3 =
# π
0
1
eiθ ieiθdθ = iπ.
On the other hand,
I′
3 =
#
γ′
4
1
z dz =
# π
2π
. 1
eiθ ieiθdθ = −iπ.
Here the two integrals are not equal. From γ4 and γ′
4 we can construct a counter-
clockwise simple closed contour C, along which the integral of f(z) = 1/z becomes
E
C dz/z = I3 −I′
3 = 2iπ. That the integral is not zero is a consequence of the
fact that 1/z is not analytic at all points of the region bounded by the closed
contour C.
■
γ4
′ 
γ 4
r = 1
θ
Figure 19.5: The two semicircular paths for calculating I3 and I′
3.

19.1 Complex Functions
507
C
C
Γ
L
(a)
(b)
Figure 19.6: A contour of integration can be deformed into another contour. The
second contour is usually taken to be a circle because of the ease of its corresponding
integration. (a) shows the original contour, and (b) shows the two contours as well as
the (shaded) region between them in which the function is analytic.
The Cauchy–Goursat theorem applies to more complicated regions. When
a region contains points at which f(z) is not analytic, those points can be
avoided by redeﬁning the region and the contour (Figure 19.6). Such a pro-
cedure requires a convention regarding the direction of “motion” along the
contour. This convention is important enough to be stated separately.
convention for
positive sense of
integration around
a closed contour
Box 19.1.3. (Convention). When integrating along a closed contour,
we agree to traverse the contour in such a way that the region enclosed
by the contour lies to our left. An integration that follows this convention
is called integration in the positive sense. Integration performed in the
opposite direction acquires a minus sign.
Suppose that we want to evaluate the integral
E
C f(z) dz where C is some
contour in the complex plane [see Figure 19.6(a)]. Let Γ be another—usually
simpler, say a circle—contour which is either entirely inside or entirely out-
side C.
Figure 19.6 illustrates the case where Γ is entirely inside C.
We
assume that Γ is such that f(z) does not have any singularity in the region
between C and Γ. By connecting the two contours with a line as shown in
Figure 19.6(b), we construct a composite closed contour consisting of C, Γ,
and twice the line segment L, once in the positive directions and once in
the negative. Within this composite contour, the function f(z) is analytic.
Therefore, by the Cauchy–Goursat theorem, we have
−
2
C
f(z) dz +
2
Γ
f(z) dz +
#
L
f(z) dz −
#
L
f(z) dz = 0.
The negative sign for C is due to the convention above. It follows from this
equation that the integral along C is the same as that along the circle Γ. This
result can be interpreted by saying that

508
Complex Derivative and Integral
Box 19.1.4. We can always deform the contour of an integral in the com-
plex plane into a simpler contour, as long as in the process of deformation
we encounter no singularity of the function.
19.1.3
Cauchy Integral Formula
One extremely important consequence of the Cauchy–Goursat theorem, the
centerpiece of complex analysis, is the Cauchy integral formula which we state
without proof.4
Theorem 19.1.9. Let f(z) be analytic on and within a simple closed contour
C integrated in the positive sense. Let z0 be any interior point of C. Then
f(z0) =
1
2πi
2
C
f(z)
z −z0
dz.
This is called the Cauchy integral formula (CIF).
Example 19.1.10. We can use the CIF to evaluate the following integrals:
I1 =
2
C1
z2 dz
(z2 + 3)2(z −i),
I2 =
2
C2
(z2 −1) dz
(z −1
2)(z2 −4)3 ,
I3 =
2
C3
ez/2 dz
(z −iπ)(z2 −20)4 ,
where C1, C2, and C3 are circles centered at the origin with radii r1 = 3
2, r2 = 1,
and r3 = 4, respectively.
For I1 we note that f(z) = z2/(z2 + 3)2 is analytic within and on C1, and z0 = i
lies in the interior of C1. Thus,
I1 =
2
C1
f(z)dz
z −i
= 2πif(i) = 2πi
i2
(i2 + 3)2 = −iπ
2 .
Similarly, f(z) = (z2 −1)/(z2 −4)3 for I2 is analytic on and within C2, and z0 = 1
2
is an interior point of C2. Thus, the CIF gives
I2 =
2
C2
f(z)dz
z −1
2
= 2πif( 1
2) = 2πi
1
4 −1
( 1
4 −4)3 = 32π
1125i.
For the last integral, f(z) = ez/2/(z2 −20)4, and the interior point is z0 = iπ:
I3 =
2
C3
f(z)dz
z −iπ = 2πif(iπ) = 2πi
eiπ/2
(−π2 −20)4 = −
2π
(π2 + 20)4 .
■
The CIF gives the value of an analytic function at every point inside a
simple closed contour when it is given the value of the function only at points
on the contour. It seems as though analytic functions have no freedom within
why analytic
functions “remote
sense” their values
at distant points
a contour: They are not free to change inside a region once their value is
ﬁxed on the contour enclosing that region. There is an analogous situation in
4For a proof, see Hassani, S. Mathematical Physics: A Modern Introduction to Its Foun-
dations, Springer-Verlag, 1999, Chapter 9.

19.1 Complex Functions
509
certain areas of physics, for example, electrostatics: The speciﬁcation of the
potential at the boundaries, such as conductors, automatically determines it
at any other point in the region of space bounded by the conductors. This is
the content of the uniqueness theorem (to be discussed later in this book) used
in electrostatic boundary-value problems. However, the electrostatic potential
Φ is bound by another condition, Laplace’s equation; and the combination of
Laplace’s equation and the boundary conditions furnishes the uniqueness of Φ.
It seems, on the other hand, as though the mere speciﬁcation of an analytic
function on a contour, without any other condition, is suﬃcient to determine
the function’s value at all points enclosed within that contour. This is not
the case. An analytic function, by its very deﬁnition, satisﬁes another re-
strictive condition: Its real and imaginary parts separately satisfy Laplace’s
equation in two dimensions! [see Equation (19.4)]. Thus, it should come as
no surprise that the value of an analytic function at a boundary (contour)
determines the function at all points inside the boundary.
19.1.4
Derivatives as Integrals
The CIF is a very powerful tool for working with analytic functions. One
of the applications of this formula is in evaluating the derivatives of such
functions. It is convenient to change the dummy integration variable to ξ and
write the CIF as
f(z) =
1
2πi
2
C
f(ξ) dξ
ξ −z ,
(19.9)
where C is a simple closed contour in the ξ-plane and z is a point within C.
By carrying the derivative inside the integral, we get
df
dz =
1
2πi
d
dz
2
C
f(ξ) dξ
ξ −z
=
1
2πi
2
C
d
dz
f(ξ) dξ
ξ −z

=
1
2πi
2
C
f(ξ) dξ
(ξ −z)2 .
By repeated diﬀerentiation, we can generalize this formula to the nth deriva-
tive, and obtain
Theorem 19.1.11. The derivatives of all orders of an analytic function f(z)
exist in the domain of analyticity of the function and are themselves analytic
in that domain. The nth derivative of f(z) is given by
f (n)(z) = dnf
dzn = n!
2πi
2
C
f(ξ) dξ
(ξ −z)n+1 .
(19.10)
Example 19.1.12. Let us apply Equation (19.10) directly to some simple func-
tions. In all cases, we will assume that the contour is a circle of radius r centered
at z.
(a) Let f(z) = K, a constant. Then, for n = 1 we have
df
dz =
1
2πi
2
C
K dξ
(ξ −z)2 .

510
Complex Derivative and Integral
Since ξ is always on the circle C centered at z, ξ −z = reiθ and dξ = rieiθdθ. So
we have
df
dz =
1
2πi
# 2π
0
Kireiθdθ
(reiθ)2
= K
2πr
# 2π
0
e−iθdθ = 0.
That is, the derivative of a constant is zero.
(b) Given f(z) = z, its ﬁrst derivative will be
df
dz =
1
2πi
2
C
ξ dξ
(ξ −z)2 =
1
2πi
# 2π
0
(z + reiθ)ireiθdθ
(reiθ)2
= 1
2π
 z
r
# 2π
0
e−iθdθ +
# 2π
0
dθ

= 1
2π (0 + 2π) = 1.
(c) Given f(z) = z2, for the ﬁrst derivative Equation (19.10) yields
df
dz =
1
2πi
2
C
ξ2 dξ
(ξ −z)2 =
1
2πi
# 2π
0
(z + reiθ)2ireiθdθ
(reiθ)2
= 1
2π
# 2π
0
*
z2 + (reiθ)2 + 2zreiθ+
(reiθ)−1dθ
= 1
2π
z2
r
# 2π
0
e−iθdθ + r
# 2π
0
eiθdθ + 2z
# 2π
0
dθ

= 2z.
It can be shown that, in general, (d/dz)zm = mzm−1. The proof is left as Problem
19.24.
■
The CIF is a central formula in complex analysis. However, due to space
limitations, we cannot explore its full capability here. Nevertheless, one of its
applications is worth discussing at this point. Suppose that f is a bounded
entire function and consider
df
dz =
1
2πi
2
C
f(ξ) dξ
(ξ −z)2 .
Since f is analytic everywhere in the complex plane, the closed contour C can
be chosen to be a very large circle of radius R with center at z. Taking the
absolute value of both sides yields




df
dz




 = 1
2π




# 2π
0
f(z + Reiθ)
(Reiθ)2
iReiθdθ




≤1
2π
# 2π
0
|f(z + Reiθ)|
R
dθ ≤1
2π
# 2π
0
M
R dθ = M
R ,
where we used Equation (19.8) and |eiθ| = 1.
M is the maximum of the
function in the complex plane.5 Now as R →∞, the derivative goes to zero.
The only function whose derivative is zero is the constant function. Thus
Box 19.1.5. A bounded entire function is necessarily a constant.
5M exists because f is assumed to be bounded.

19.2 Problems
511
There are many interesting and nontrivial real functions that are bounded and
have derivatives (of all orders) on the entire real line. For instance, e−x2 is such
any nontrivial
function is either
unbounded or not
entire.
a function. No such freedom exists for complex analytic functions according
to Box 19.1.5! Any nontrivial analytic function is either not bounded (goes
to inﬁnity somewhere on the complex plane) or not entire [it is not analytic
at some point(s) of the complex plane].
A consequence of Proposition 19.1.5 is the fundamental theorem of
algebra which states that any polynomial of degree n ≥1 has n roots (some
fundamental
theorem of algebra
proved
of which may be repeated). In other words, the polynomial
p(x) = a0 + a1x + · · · + anxn
for n ≥1
can be factored completely as p(x) = c(x −z1)(x −z2) . . . (x −zn) where c is
a constant and the zi are, in general, complex numbers.
To see how Proposition 19.1.5 implies the fundamental theorem of algebra,
we let f(z) = 1/p(z) and assume the contrary, i.e., that p(z) is never zero for
any (ﬁnite) z. Then f(z) is bounded and analytic for all z, and Proposition
19.1.5 says that f(z) is a constant. This is obviously wrong. Thus, there must
be at least one z, say z = z1, for which p(z) is zero. So, we can factor out
(z −z1) from p(z) and write p(z) = (z −z1)q(z) where q(z) is of degree n −1.
Applying the above argument to q(z), we have p(z) = (z −z1)(z −z2)r(z)
where r(z) is of degree n −2. Continuing in this way, we can factor p(z) into
linear factors. The last polynomial will be a constant (a polynomial of degree
zero) which we have denoted as c.
19.2
Problems
19.1. Show that f(z) = z2 maps a line that makes an angle α with the
real axis of the z-plane onto a line in the w-plane which makes an angle
2α with the real axis of the w-plane. Hint: Use the trigonometric identity
tan 2α = 2 tan α/(1 −tan2 α).
19.2. Show that the function w = 1/z maps the straight line y = 1
2 in the
z-plane onto a circle in the w-plane.
19.3. (a) Using the chain rule, ﬁnd ∂f/∂z∗and ∂f/∂z in terms of partial
derivatives with respect to x and y.
(b) Evaluate ∂f/∂z∗and ∂f/∂z assuming that the C–R conditions hold.
19.4. (a) Show that, when z is represented by polar coordinates, the C–R
conditions on a function f(z) are
∂U
∂r = 1
r
∂V
∂θ ,
∂U
∂θ = −r∂V
∂r ,
where U and V are the real and imaginary parts of f(z) written in polar
coordinates.

512
Complex Derivative and Integral
(b) Show that the derivative of f can be written as
df
dz = e−iθ
∂U
∂r + i∂V
∂r

.
Hint: Start with the C–R conditions in Cartesian coordinates and apply the
chain rule to them using x = r cos θ and y = r sin θ.
19.5. Prove the following identities for diﬀerentiation by ﬁnding the real
and imaginary parts of the function—u(x, y) and v(x, y)—and diﬀerentiat-
ing them:
(a) d
dz (f + g) = df
dz + dg
dz .
(b) d
dz (fg) = df
dz g + f dg
dz .
(c) d
dz
f
g

= f ′(z)g(z) −g′(z)f(z)
[g(z)]2
,
where g(z) ̸= 0.
19.6. Show that d/dz(ln z) = 1/z. Hint: Find u(x, y) and v(x, y) for ln z
using the exponential representation of z, then diﬀerentiate them.
19.7. Show that sin z and cos z have only real roots. Hint: Use deﬁnition of
sine and cosine in terms of exponentials.
19.8. Use mathematical induction and the product rule for diﬀerentiation to
show that d
dz (zn) = nzn−1.
19.9. Use Equations (19.5) and (19.6), to establish the following identities:
(a) Re(sin z) = sin x cosh y,
Im(sin z) = cos x sinh y.
(b) Re(cos z) = cos x cosh y,
Im(cos z) = −sin x sinh y.
(c) Re(sinh z) = sinh x cos y,
Im(sinh z) = cosh x sin y.
(d) Re(cosh z) = cosh x cos y,
Im(cosh z) = sinh x sin y.
(e) | sin z|2 = sin2 x + sinh2 y,
| cos z|2 = cos2 x + sinh2 y.
(f) | sinh z|2 = sinh2 x + sin2 y,
| cosh z|2 = sinh2 x + cos2 y.
19.10. Find all the zeros of sinh z and cosh z.
19.11. Verify the following trigonometric identities:
(a) cos2 z + sin2 z = 1.
(b) cos(z1 + z2) = cos z1 cos z2 −sin z1 sin z2.
(c) sin(z1 + z2) = sin z1 cosz2 + cos z1 sin z2.
(d) sin
π
2 −z

= cos z,
cos
π
2 −z

= sin z.
(e) cos 2z = cos2 z −sin2 z,
sin 2z = 2 sin z cos z.
(f) tan(z1 + z2) = tan z1 + tan z2
1 −tan z1 tan z2
.

19.2 Problems
513
19.12. Verify the following hyperbolic identities:
(a) cosh2 z −sinh2 z = 1.
(b) cosh(z1 + z2) = cosh z1 cosh z2 + sinh z1 sinh z2.
(c) sinh(z1 + z2) = sin z1 cosh z2 + cosh z1 sinh z2.
(d) cosh 2z = cosh2 z + sinh2 z,
sinh 2z = 2 sinh z cosh z.
(e) tanh(z1 + z2) = tanh z1 + tanh z2
1 + tanh z1 tanh z2
.
19.13. Show that
(a) tanh
z
2

= sinh x + i sin y
cosh x + cosy .
(b) coth
z
2

= sinh x −i sin y
cosh x −cos y .
19.14. Prove the following identities:
(a) cos−1 z = −i ln(z ±
	
z2 −1).
(b) sin−1 z = −i ln[iz ±
	
1 −z2)].
(c) tan−1 z = 1
2i ln
i −z
i + z

.
(d) cosh−1 z = ln(z ±
	
z2 −1).
(e) sinh−1 z = ln(z ±
	
z2 + 1).
(f) tanh−1 z = 1
2 ln
1 + z
1 −z

.
19.15. Prove that exp(z∗) is not analytic anywhere.
19.16. Show that eiz = cos z + i sin z for any z.
19.17. Show that both the real and imaginary parts of an analytic function
are harmonic.
19.18. Show that each of the following functions—call each one u(x, y)—
is harmonic, and ﬁnd the function’s harmonic partner, v(x, y), such that
u(x, y) + iv(x, y) is analytic. Hint: Use C–R conditions.
(a) x3 −3xy2.
(b) ex cos y.
(c)
x
x2 + y2
where x2 + y2 ̸= 0.
(d) e−2y cos 2x.
(e) ey2−x2 cos 2xy.
(f) ex(x cos y −y sin y) + 2 sinh y sin x + x3 −3xy2 + y.
19.19. Describe the curve deﬁned by each of the following equations:
(a) z = 1 −it,
0 ≤t ≤2.
(b) z = t + it2,
−∞< t < ∞.
(c) z = a(cos t + i sin t)
π
2 ≤t ≤3π
2 .
(d) z = t + i
t
−∞< t < 0.
19.20. Let f(z) = w = u + iv. Suppose that ∂2Φ
∂x2 + ∂2Φ
∂y2 = 0. Show that if f
is analytic, then ∂2Φ
∂u2 + ∂2Φ
∂v2 = 0. That is, analytic functions map harmonic
functions in the z-plane to harmonic functions in the w-plane.

514
Complex Derivative and Integral
19.21. (a) Show that

f(z) dz can be written as
#
A · dr + i
#
B · dr,
where A = ⟨u, −v, 0⟩, B = ⟨v, u, 0⟩, and dr = ⟨dx, dy, 0⟩.
(b) Show that both A and B have vanishing curls when f is analytic.
(c) Now use the Stokes’ theorem to prove the Cauchy–Goursat theorem.
19.22. Find the value of the integral

C[(z + 2)/z] dz, where C is: (a) the
semicircle z = 2eiθ, for 0 ≤θ ≤π; (b) the semicircle z = 2eiθ, for π ≤θ ≤2π;
and (c) the circle z = 2eiθ, for −π ≤θ ≤π.
19.23. Evaluate the integral

γ dz/(z −1 −i) where γ is: (a) the line joining
z1 = 2i and z2 = 3; and (b) the path from z1 to the origin and from there to
z2.
19.24. Use Equation (19.10) to show that d
dz (zm) = mzm−1. Hint: Use the
binomial theorem.
19.25. Let C be the boundary of a square whose sides lie along the lines
x = ±3 and y = ±3. For the positive sense of integration, evaluate each of
the following integrals by using CIF or the derivative formula (19.10):
(a)
2
C
e−z
z −iπ/2 dz.
(b)
2
C
ez
z(z2 + 10) dz. (c)
2
C
cos z
(z −π
4 )(z2 −10) dz.
(d)
2
C
sinh z
z4
dz.
(e)
2
C
cosh z
z4
dz.
(f)
2
C
cos z
z3
dz.
(g)
2
C
cos z
(z −iπ/2)2 dz.
(h)
2
C
ez
(z −iπ)2 dz.
(i)
2
C
cos z
z + iπ dz.
(j)
2
C
ez
z2 −5z + 4 dz.
(k)
2
C
sinh z
(z −iπ/2)2 dz. (l)
2
C
cosh z
(z −π/2)2 dz.
(m)
2
C
z2
(z −2)(z2 −10) dz.

Chapter 20
Complex Series
As in the real case, representation of functions by inﬁnite series of “simpler”
functions is an endeavor worthy of our serious consideration. We start with
an examination of the properties of sequences and series of complex numbers
and derive series representations of some complex functions.
Most of the
discussion is a direct generalization of the results of the real series.
A sequence {zk}∞
k=1 of complex numbers is said to converge to a limit z if
sequence,
convergence to a
limit, partial sums,
and series
limk→∞|z −zk| = 0. In other words, for each positive number ε there must
exist an integer N such that |z −zk| < ε whenever k > N. The reader may
show that the real (imaginary) part of the limit of a sequence of complex
numbers is the limit of the real (imaginary) part of the sequence. Series can
be converted into sequences by partial summation. For instance, to study
the inﬁnite series ∞
k=1 zk, we form the partial sums Zn ≡n
k=1 zk and
investigate the sequence {Zn}∞
n=1. We thus say that the inﬁnite series ∞
k=1 zk
converges to Z if limn→∞Zn = Z.
Example 20.0.1. A series that is used often in analysis is the geometric series
Z = ∞
k=0 zk. Let us show that this series converges to 1/(1 −z) for |z| < 1. For a
partial sum of n terms, we have
Zn ≡
n

k=0
zk = 1 + z + z2 + · · · + zn.
Multiply this by z and subtract the result from the Zn sum to get (see also Example
9.3.3)
Zn −zZn = 1 −zn+1 ⇒Zn = 1 −zn+1
1 −z
.
We now show that Zn converges to Z = 1/(1 −z). We have
|Z −Zn| =




1
1 −z −1 −zn+1
1 −z




 =




zn+1
1 −z




 = |z|n+1
|1 −z|
and
lim
n→∞|Z −Zn| = lim
n→∞
|z|n+1
|1 −z| =
1
|1 −z| lim
n→∞|z|n+1 = 0
for |z| < 1. Thus, ∞
k=0 zk = 1/(1 −z) for |z| < 1.
■

516
Complex Series
If the series ∞
k=0 zk converges, both the real part, ∞
k=0 xk, and the
imaginary part, ∞
k=0 yk, of the series also converge. From Chapter 9, we
know that a necessary condition for the convergence of the real series ∞
k=0 xk
and ∞
k=0 yk is that xk →0 and yk →0. Thus, a necessary condition for
the convergence of the complex series is limk→∞zk = 0. The terms of such a
series are, therefore, bounded. Thus, there exists a positive number M such
that |zk| < M for all k.
A complex series is said to converge absolutely, if the real series
absolute
convergence
∞

k=0
|zk| =
∞

k=0

x2
k + y2
k
converges. Clearly, absolute convergence implies convergence.
20.1
Power Series
We now concentrate on the power series which, as in the real case, are inﬁnite
sums of powers of (z −z0). It turns out—as we shall see shortly—that for
complex functions, the inclusion of negative powers is crucial.
power series
Theorem 20.1.1. If the power series ∞
k=0 ak(z −z0)k converges for z1
(assumed to be diﬀerent from z0), then it converges absolutely for every value
of z such that |z −z0| < |z1 −z0|. Similarly if the power series ∞
k=0 bk/(z −
z0)k converges for z2 ̸= z0, then it converges absolutely for every value of z
such that |z −z0| > |z2 −z0|.
Proof. We prove the ﬁrst part of the proposition; the second part is done
similarly. Since the series converges for z = z1, all the terms |ak(z1 −z0)k|
are smaller than a positive number M. We, therefore have
∞

k=0
|ak(z −z0)k| =
∞

k=0




ak(z1 −z0)k (z −z0)k
(z1 −z0)k




=
∞

k=0
|ak(z1 −z0)k|




z −z0
z1 −z0




k
≤
∞

k=0
MBk
= M
∞

k=0
Bk =
M
1 −B ,
where B ≡|(z −z0)/(z1 −z0)| is a positive real number less than 1. Since
the RHS is a ﬁnite (positive) number, the series of absolute values converges,
and the proof is complete.
The essence of Theorem 20.1.1 is that if a power series—with positive
powers—converges for a point at a distance r1 from z0, then it converges for
all interior points of a circle of radius r1 centered at z0. Similarly, if a power
series—with negative powers—converges for a point at a distance r2 from z0,
then it converges for all exterior points of a circle of radius r2 centered at z0
(see Figure 20.1).

20.1 Power Series
517
z0
r1
(a)
(b)
z0
r2
Figure 20.1: (a) Power series with positive exponents converge for the interior points
of a circle. (b) Power series with negative exponents converge for the exterior points of
a circle.
Box 20.1.1. When constructing power series, positive powers are used
for points inside a circle and negative powers for points outside it.
The largest circle about z0 such that the ﬁrst power series of Theorem
20.1.1 converges is called the circle of convergence of the power series. It
circle of
convergence
follows from Theorem 20.1.1 that the series cannot converge at any point
outside the circle of convergence. (Why?)
Let us consider the power series
S(z) ≡
∞

k=0
ak(z −z0)k
(20.1)
which we assume to be convergent at all points interior to a circle for which
|z −z0| = r.
This implies that the sequence of partial sums {Sn(z)}∞
n=0
converges. Therefore, for any ε > 0, there exists an integer Nε such that
|S(z) −Sn(z)| < ε
whenever
n > Nε.
In general, the integer Nε may be dependent on z; that is, for diﬀerent values
uniform
convergence
explained
of z, we may be forced to pick diﬀerent Nε’s. When Nε is independent of z, we
say that the convergence is uniform. We state the following result without
proof:
a power series is
uniformly
convergent and
analytic; it can be
diﬀerentiated and
integrated term by
term.
Theorem 20.1.2. The power series S(z) = ∞
n=0 an(z −z0)n is uniformly
convergent for all points within its circle of convergence, and S(z) is an ana-
lytic function of z there. Furthermore, such a series can be diﬀerentiated and
integrated term by term:
dS(z)
dz
=
∞

n=1
nan(z −z0)n−1,
#
γ
S(z) dz =
∞

n=0
an
#
γ
(z −z0)ndz,

518
Complex Series
at each point z and each path γ located inside the circle of convergence of the
power series.
By substituting the reciprocal of (z −z0) in the power series, we can show
that if ∞
k=0 bk/(z −z0)k is convergent in the annulus r2 < |z −z0| < r1, then
it is uniformly convergent for all z in that annulus, and the series represents
a continuous function of z there.
20.2
Taylor and Laurent Series
Complex series, just as their real counterparts, ﬁnd their most frequent utility
in representing well-behaved functions. The following theorem, which we state
without proof,1 is essential in the application of complex analysis.
Theorem 20.2.1. Let C1 and C2 be circles of radii r1 and r2, both centered
at z0 in the z-plane with r1 > r2. Let f(z) be analytic on C1 and C2 and
throughout S, the annular region between the two circles. Then, at each point
z of S, f(z) is given uniquely by the Laurent series
f(z) =
∞

n=−∞
an(z −z0)n,
where
an =
1
2πi
2
C
f(ξ)
(ξ −z0)n+1 dξ,
and C is any contour within S that encircles z0. When r2 = 0, the series is
called Taylor series. In that case an = 0 for negative n and an = f (n)(z0)/n!
for n ≥0.
We can see the reduction of the Laurent series to Taylor series as follows.
The Laurent expansion is convergent as long as r2 < |z −z0| < r1. In partic-
ular, if r2 = 0, and if the function is analytic throughout the interior of the
larger circle, then f(ξ)/(ξ −z0)n+1 will be analytic for negative integer n, and
the integral will be zero by the Cauchy–Goursat theorem. Therefore, an will
be zero for n = −1, −2, . . .. Thus, only positive powers of (z −z0) will be
present in the series, and we obtain the Taylor series.
For z0 = 0, the Taylor series reduces to the Maclaurin series:
Maclaurin series
f(z) = f(0) + f ′(0)z + · · · =
∞

n=0
f (n)(0)
n!
zn.
Box 19.1.4 tells us that we can enlarge C1 and shrink C2 until we encounter
a point at which f is no longer analytic. Thus, we can include all the possible
analytic points by enlarging C1 and shrinking C2.
Example 20.2.2. Let us expand some functions in terms of series. For entire
functions there is no point in the entire complex plane at which they are not analytic.
1For a proof, see Hassani, S. Mathematical Physics: A Modern Introduction to Its Foun-
dations, Springer-Verlag, 1999, Section 9.6.

20.2 Taylor and Laurent Series
519
Thus, only positive powers of (z −z0) will be present, and we will have a Taylor
expansion that is valid for all values of z.
(a) We expand ez around z0 = 0. The nth derivative of ez is ez. Thus, f (n)(0) = 1,
and the Taylor (Maclaurin) expansion gives
ez =
∞

n=0
f (n)(0)
n!
zn =
∞

n=0
zn
n! .
(b) The Maclaurin series for sin z is obtained by noting that
dn
dzn sin z




z=0
=
0
0
if n is even,
(−1)(n−1)/2
if n is odd,
and substituting this in the Maclaurin expansion:
sin z =

n odd
(−1)(n−1)/2 zn
n! =
∞

k=0
(−1)k
z2k+1
(2k + 1)!.
Similarly, we can obtain
cos z =
∞

k=0
(−1)k z2k
(2k)!,
sinh z =
∞

k=0
z2k+1
(2k + 1)!,
cosh z =
∞

k=0
z2k
(2k)!.
It is seen that the series representation of all these functions is obtained by replacing
the real variable x in their real series representation with a complex variable z.
(c) The function 1/(1+z) is not entire, so the region of its convergence is limited. Let
us ﬁnd the Maclaurin expansion of this function. Starting from the origin (z0 = 0),
the function is analytic within all circles of radii r < 1. At r = 1 we encounter a
singularity, the point z = −1. Thus, the series converges for all points z for which
|z| < 1.2 For such points we have
f (n)(0) =
dn
dzn [(1 + z)−1]




z=0
= (−1)nn!.
Thus,
1
1 + z =
∞

n=0
f (n)(0)
n!
zn =
∞

n=0
(−1)nzn.
■
The Taylor and Laurent series allow us to express an analytic function as
a power series. For a Taylor series of f(z) the expansion is routine because
the coeﬃcient of its nth term is simply f (n)(z0)/n!, where z0 is the center of
the circle of convergence. However, when a Laurent series is applicable in a
there is only one
Laurent series for
a given function
deﬁned in a given
region.
given region of the complex plane, the nth coeﬃcient is not, in general, easy to
evaluate. Usually it can be found by inspection and certain manipulations of
other known series. Then the uniqueness of Laurent series expansion assures
us that the series so obtained is the unique Laurent series for the function in
that region.3
2As remarked before, the series diverges for all points outside the circle |z| = 1. This
does not mean that the function cannot be represented by a series for points outside the
circle. On the contrary, we shall see shortly that the Laurent series, with negative powers
is designed precisely for such a purpose.
3See Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, p. 258.

520
Complex Series
As in the case of real series,
we can add,
subtract, and
multiply
convergent power
series.
Box 20.2.1. We can add, subtract, and multiply convergent power series.
Furthermore, if the denominator does not vanish in a neighborhood of a
point z0, then we can obtain the Laurent series of the ratio of two power
series about z0 by long division.
Thus converging power series can be manipulated as though they were
ﬁnite sums (polynomials).
Such manipulations are extremely useful when
dealing with Taylor and Laurent expansions in which the straightforward cal-
culation of coeﬃcients may be tedious. The following examples illustrate the
power of inﬁnite-series arithmetic. In these examples, the following equations
are very useful:
1
1 −z =
∞

n=0
zn,
1
1 + z =
∞

n=0
(−1)nzn,
|z| < 1.
(20.2)
Example 20.2.3. To expand the function f(z) =
2 + 3z
z2 + z3 in a Laurent series
about z = 0, rewrite it as
f(z) = 1
z2
 2 + 3z
1 + z

= 1
z2

3 −
1
1 + z

= 1
z2

3 −
∞

n=0
(−1)nzn

= 1
z2 (3 −1 + z −z2 + z3 −· · · ) = 2
z2 + 1
z −1 + z −z2 + · · · .
This series converges for 0 < |z| < 1. We note that negative powers of z are also
present. This is a reﬂection of the fact that the function is not analytic inside the
entire circle |z| = 1; it diverges at z = 0.
■
Example 20.2.4. The function f(z) = z/[(z −1)(z −2)] has a Taylor expansion
around the origin for |z| < 1. To ﬁnd this expansion, we write4
f(z) = −
1
z −1 +
2
z −2 =
1
1 −z −
1
1 −z/2.
Expanding both fractions in geometric series (both |z| and |z/2| are less than 1), we
obtain f(z) = ∞
n=0 zn −∞
n=0(z/2)n. Adding the two series yields
f(z) =
∞

n=0
(1 −2−n)zn
for
|z| < 1.
This is the unique Taylor expansion of f(z) within the circle |z| = 1.
4We could, of course, evaluate the derivatives of all orders of the function at z = 0 and
use the Maclaurin formula. However, the present method gives the same result much more
quickly.

20.2 Taylor and Laurent Series
521
For the annular region 1 < |z| < 2 we have a Laurent series. This can be seen
by noting that
f(z) =
1/z
1/z −1 −
1
1 −z/2 = −1
z

1
1 −1/z

−
1
1 −z/2.
Since both fractions on the RHS are analytic in the annular region (|1/z| < 1,
|z/2| < 1), we get
f(z) = −1
z
∞

n=0
 1
z
n
−
∞

n=0
 z
2
n
= −
∞

n=0
z−n−1 −
∞

n=0
2−nzn
= −
−∞

n=−1
zn −
∞

n=0
2−nzn = −
∞

n=−∞
anzn,
where an = −1 for n < 0 and an = −2−n for n ≥0. This is the unique Laurent
expansion of f(z) in the given region.
Finally, for |z| > 2 we have only negative powers of z. We obtain the expansion
in this region by rewriting f(z) as follows:
f(z) = −
1/z
1 −1/z +
2/z
1 −2/z .
Expanding the fractions yields
f(z) = −
∞

n=0
z−n−1 +
∞

n=0
2n+1z−n−1 =
∞

n=0
(2n+1 −1)z−n−1.
This is again the unique expansion of f(z) in the region |z| > 2.
■
The example above shows that a single function may have diﬀerent series
representations in diﬀerent regions of the complex plane, each series having
its own region of convergence.
Example 20.2.5. Deﬁne f(z) as
f(z) =
0
(1 −cos z)/z2
for z ̸= 0,
1
2
for z = 0.
We can show that f(z) is an entire function.
Since 1 −cos z and z2 are entire functions, their ratio is analytic everywhere
except at the zeros of its denominator. The only such zero is z = 0. Thus, f(z) is
analytic everywhere except possibly at z = 0. To see the behavior of f(z) at z = 0,
we look at its Maclaurin series:
1 −cos z = 1 −
∞

n=0
(−1)n z2n
(2n)!
which implies that
1 −cos z
z2
=
∞

n=1
(−1)n+1 z2n−2
(2n)! = 1
2 −z2
4! + z4
6! −· · · .
The expansion on the RHS shows that the value of the series is 1
2, which, by deﬁni-
tion, is f(0). Thus, the series converges for all z, and Box 20.1.2 says that f(z) is
entire.
■

522
Complex Series
A Laurent series can give information about the integral of a function
around a closed contour in whose interior the function may not be analytic.
In fact, the coeﬃcient of the ﬁrst negative power in a Laurent series is given by
a−1 =
1
2πi
2
C
f(ξ) dξ.
(20.3)
Thus,
Box 20.2.2. To ﬁnd the integral of a (nonanalytic) function around a
closed contour surrounding z0, write the Laurent series for the function
and read oﬀa−1, the coeﬃcient of the 1/(z −z0) term. The integral is
2πia−1.
Example 20.2.6. As an illustration of this idea, let us evaluate the integral I =
E
C dz/[z2(z −2)], where C is a circle of radius 1 centered at the origin. The function
is analytic in the annular region 0 < |z| < 2. We can, therefore, expand it as a
Laurent series about z = 0 in that region:
1
z2(z −2) = −1
2z2

1
1 −z/2

= −1
2z2
∞

n=0
 z
2
n
= −1
2
 1
z2

−1
4
 1
z

−1
8 −· · · .
Thus, a−1 = −1
4, and
E
C dz/[z2(z −2)] = 2πia−1 = −iπ/2.
Any other way of
evaluating the integral is nontrivial.
■
20.3
Problems
20.1. Expand sinh z in a Taylor series about the point z = iπ.
20.2. Let C be the circle |z −i| = 3 integrated in the positive sense. Find
the value of each of the following integrals using the CIF or the derivative
formula (19.10):
(a)
2
C
ez
z2 + π2 dz.
(b)
2
C
sinh z
(z2 + π2)2 dz.
(c)
2
C
dz
z2 + 9.
(d)
2
C
dz
(z2 + 9)2 .
(e)
2
C
cosh z
(z2 + π2)3 dz.
(f)
2
C
z2 −3z + 4
z2 −4z + 3 dz.
20.3. For 0 < r < 1, show that
∞

k=0
rk cos kθ =
1 −r cos θ
1 + r2 −2r cos θ
and
∞

k=0
rk sin kθ =
r sin θ
1 + r2 −2r cos θ .

20.3 Problems
523
20.4. Find the Taylor expansion of 1/z2 for points inside the circle |z−2| < 2.
20.5. Use mathematical induction to show that
dn
dzn (1 + z)−1




z=0
= (−1)nn!.
20.6. Find the (unique) Laurent expansion of each of the following functions
in each of its regions of analyticity:
(a)
1
(z −2)(z −3).
(b) z cos(z2).
(c)
1
z2(1 −z).
(d) sinh z −z
z4
.
(e)
1
(1 −z)3 .
(f)
1
z2 −1.
(g) z2 −4
z2 −9.
(h)
1
(z2 −1)2 .
(i)
z
z −1.
20.7. Show that the following functions are entire:
(a) f(z) =
⎧
⎨
⎩
e2z −1
z2
−2z
for z ̸= 0,
2
for z = 0.
(b) f(z) =
0sin z
z
for z ̸= 0,
1
for z = 0.
(c) f(z) =
⎧
⎨
⎩
cos z
z2 −π2/4
for z ̸= ±π/2,
−1/π
for z = ±π/2.
20.8. Obtain the ﬁrst few nonzero terms of the Laurent-series expansion of
each of the following functions about the origin by approximating the denomi-
nator by a polynomial and using the technique of long division of polynomials.
Also ﬁnd the integral of the function along a small simple closed contour en-
circling the origin.
(a)
1
sin z .
(b)
1
1 −cos z .
(c)
z
1 −cosh z .
(d)
z2
z −sin z .
(e)
1
ez −1.
(f)
1
z2 sin z .
(g)
z4
6z + z3 −6 sinh z .
20.9. Obtain the Laurent-series expansion of f(z) = sinh z/z3 about the
origin.

Chapter 21
Calculus of Residues
One of the most powerful tools made available by complex analysis is the
theory of residues, which makes possible the routine evaluation of certain real
deﬁnite integrals that are impossible to calculate otherwise. Example 20.2.6
showed a situation in which an integral was related to expansion coeﬃcients
of Laurent series. Here we will develop a systematic way of evaluating both
real and complex integrals using the same idea.
Recall that a singular point z0 of f(z) is a point at which f fails to be
analytic.
If, in addition, there is some neighborhood of z0 in which f is
analytic at every point (except, of course, at z0 itself), then z0 is called an
isolated singularity of f. All singularities we have encountered so far have
isolated singularity
been isolated singularities. Although singularities that are not isolated also
exist, we shall not discuss them in this book.
21.1
The Residue
Let z0 be an isolated singularity of f. Then there exists an r > 0 such that,
within the “annular” region 0 < |z −z0| < r, the function f has the Laurent
expansion1
f(z) =
∞

n=−∞
an(z −z0)n ≡
∞

n=0
an(z −z0)n +
b1
z −z0
+
b2
(z −z0)2 + · · · ,
where
an =
1
2πi
2
C
f(ξ) dξ
(ξ −z0)n+1
and
bn =
1
2πi
2
C
f(ξ)(ξ −z0)n−1 dξ.
In particular,
b1 =
1
2πi
2
C
f(ξ) dξ,
(21.1)
1We are using bn for a−n.

526
Calculus of Residues
where C is any simple closed contour around z0, traversed in the positive
sense, on and interior to which f is analytic except at the point z0 itself.
residue deﬁned
Box 21.1.1. The complex number b1, which is
1
2πi times the integral of
f(z) along the contour, is called the residue of f at the isolated singular
point z0.
It is important to note that the residue is independent of the contour C as
long as z0 is the only isolated singular point within C.
Example 21.1.1. We want to evaluate the integral
E
C sin z dz/(z −π/2)3 where
C is any simple closed contour having z = π/2 as an interior point.
To evaluate the integral we expand around z = π/2 and use Equation (21.1).
We note that
sin z = cos

z −π
2

=
∞

n=0
(−1)n (z −π/2)2n
(2n)!
= 1 −(z −π/2)2
2
+ · · ·
so
sin z
(z −π/2)3 =
1
(z −π/2)3 −1
2

1
z −π/2

+ · · · .
It follows that b1 = −1
2; therefore,
E
C sin z dz/(z −π/2)3 = 2πib1 = −iπ.
■
Example 21.1.2. The integral
E
C cos z dz/z2, where C is the circle |z| = 1, is
zero because
cos z
z2
= 1
z2
∞

n=0
(−1)n z2n
(2n)! = 1
z2 −1
2 + z2
4! + · · ·
yields b1 = 0 (no 1/z term in the Laurent expansion). Therefore, by Equation (21.1)
the integral must vanish.
When C is the circle |z| = 2,
E
C ez dz/(z −1)3 = iπe because
ez = eez−1 = e
∞

n=0
(z −1)n
n!
= e

1 + (z −1) + (z −1)2
2!
+ · · ·

and
ez
(z −1)3 = e

1
(z −1)3 +
1
(z −1)2 + 1
2

1
z −1

+ · · ·

.
Thus, b1 = e/2, and the integral is 2πib1 = iπe.
■
We use the notation Res[f(z0)] to denote the residue of f at the isolated
singular point z0. Equation (21.1) can then be written as
2
C
f(z) dz = 2πi Res[f(z0)].
What if there are several isolated singular points within the simple closed
contour C?
Let Ck be the positively traversed circle around zk shown in
Figure 21.1. Then the Cauchy–Goursat theorem yields
0 =
2
C′ f(z) dz =
2
circles
f(z) dz +
2
parallel
lines
f(z) dz +
2
C
f(z) dz,

21.1 The Residue
527
z1
z2
zm
C1
C2
Cm
Figure 21.1: Singularities are avoided by going around them.
where C′ is the union of all contours inside which union there are no singu-
larities. The contributions of the parallel lines cancel out, and we obtain
2
C
f(z) dz = −
m

k=1
2
Ck
f(z) dz =
m

k=1
2πi Res[f(zk)],
where in the last step the deﬁnition of residue at zk has been used. The minus
sign disappears in the ﬁnal result because the sense of Ck, while positive for
the shaded region of Figure 21.1, is negative for the interior of Ck because
this interior is to our right as we traverse Ck in the direction indicated. We
thus have
Theorem 21.1.3. (The Residue Theorem). Let C be a positively inte-
grated simple closed contour within and on which a function f is analytic
except at a ﬁnite number of isolated singular points z1, z2, . . . , zm interior to
C. Then
2
C
f(z) dz = 2πi
m

k=1
Res[f(zk)].
(21.2)
Example 21.1.4. Let us evaluate the integral
E
C(2z −3) dz/[z(z −1)] where C
is the circle |z| = 2. There are two isolated singularities in C, z1 = 0 and z2 = 1.
To ﬁnd Res[f(z1)], we expand around the origin using Equation (20.2):
2z −3
z(z −1) = 3
z −
1
z −1 = 3
z +
1
1 −z = 3
z + 1 + z + · · ·
for
|z| < 1.
This gives Res[f(z1)] = 3. Similarly, expanding around z = 1 gives
2z −3
z(z −1) =
3
(z −1) + 1 −
1
z −1 = −
1
z −1 + 3
∞

n=0
(−1)n(z −1)n
which yields Res[f(z2)] = −1. Thus,
2
C
2z −3
z(z −1) dz = 2πi{Res[f(z1)] + Res[f(z2)]} = 2πi(3 −1) = 4πi.
■

528
Calculus of Residues
Let f(z) have an isolated singularity at z0. Then there exist a real number
r > 0 and an annular region 0 < |z −z0| < r such that f can be represented
by the Laurent series
f(z) =
∞

n=0
an(z −z0)n +
∞

n=1
bn
(z −z0)n .
(21.3)
The second sum in Equation (21.3), involving negative powers of (z −z0), is
called the principal part of f at z0. The principal part is used to classify
principal part of a
function
isolated singularities. We consider two cases:
(a) If bn = 0 for all n ≥1, z0 is called a removable singular point of f.
removable singular
point
In this case, the Laurent series contains only nonnegative powers of (z −z0),
and setting f(z0) = a0 makes the function analytic at z0. For example, the
function f(z) = (ez −1 −z)/z2, which is indeterminate at z = 0, becomes
entire if we set f(0) = 1/2, because its Laurent series
f(z) = 1
2 + z
3! + z2
4! + · · ·
has no negative power.
(b) If bn = 0 for all n > m and bm ̸= 0, z0 is called a pole of order m. In
poles deﬁned
this case, the expansion takes the form
f(z) =
∞

n=0
an(z −z0)n +
b1
z −z0
+ · · · +
bm
(z −z0)m
for 0 < |z −z0| < r. In particular, if m = 1, z0 is called a simple pole.
simple pole
Example 21.1.5. Let us consider some examples of poles of various orders.
(a)
The function (z2 −3z +5)/(z −1) has a Laurent series around z = 1 containing
only three terms: (z2 −3z + 5)/(z −1) = −1 + (z −1) + 3/(z −1). Thus, it has a
simple pole at z = 1, with a residue of 3.
(b)
The function sin z/z6 has a Laurent series
sin z
z6
= 1
z6
∞

n=0
(−1)n
z2n+1
(2n + 1)! = 1
z5 −
1
6z3 +
1
(5!)z −z
7! + · · ·
about z = 0. The principal part has three terms. The pole, at z = 0, is of order 5,
and the function has a residue of 1/120 at z = 0.
(c)
The function (z2 −5z +6)/(z −2) has a removable singularity at z = 2, because
z2 −5z + 6
z −2
= (z −2)(z −3)
z −2
= z −3 = −1 + (z −2)
and bn = 0 for all n.
■
The type of isolated singularity that is most important in applications is
of the second type—poles. For a function that has a pole of order m at z0,
the calculation of residues is routine. Such a calculation, in turn, enables us

21.2 Integrals of Rational Functions
529
to evaluate many integrals eﬀortlessly. How do we calculate the residue of a
function f having a pole of order m at z0?
It is clear that if f has a pole of order m, then g(z) deﬁned by g(z) ≡
(z −z0)mf(z) is analytic at z0. Thus, for any simple closed contour C that
contains z0 but no other singular point of f, we have
Res[f(z0)] =
1
2πi
2
C
f(z) dz =
1
2πi
2
C
g(z) dz
(z −z0)m = g(m−1)(z0)
(m −1)! ,
where we used Equation (19.10). In terms of f this yields2
Res[f(z0)] =
1
(m −1)! lim
z→z0
dm−1
dzm−1 [(z −z0)mf(z)].
(21.4)
For the special, but important, case of a simple pole, we obtain
Res[f(z0)] = lim
z→z0[(z −z0)f(z)].
(21.5)
The most widespread application of residues occurs in the evaluation of
application of the
residue theorem in
evaluating deﬁnite
integrals
real deﬁnite integrals. It is possible to “complexify” certain real deﬁnite in-
tegrals and relate them to contour integrations in the complex plane. What
is typically involved is the addition of a number of semicircles to the real
integral such that it becomes a closed contour integral whose value can be
determined by the residue theorem. One then takes the limit of the contour
integral when the radii of the semicircles go to inﬁnity or zero. In this limit
the contributions from the semicircles should vanish for the method to work.
In that case, one recovers the real integral. There are three types of integrals
most commonly encountered. We discuss these separately below. In all cases
we assume that the contribution of the semicircles will vanish in the limit.
21.2
Integrals of Rational Functions
The ﬁrst type of integral we can evaluate using the residue theorem is of the
form
I1 =
# ∞
−∞
p(x)
q(x) dx,
where p(x) and q(x) are real polynomials, and q(x) ̸= 0 for any real x. We
can then write
I1 = lim
R→∞
# R
−R
p(x)
q(x) dx = lim
R→∞
#
Cx
p(z)
q(z) dz,
where Cx is the (open) contour lying on the real axis from −R to +R. We
now close that contour by adding to it the semicircle of radius R [see Fig-
ure 21.2(a)]. This will not aﬀect the value of the integral because, by our
2The limit is taken because in many cases the mere substitution of z0 may result in an
indeterminate form.

530
Calculus of Residues
3i
i
−R
R
(a)
−i
−3i
−R
R
(b)
Figure 21.2: (a) The large semicircle is chosen in the UHP. (b) Note how the direction
of contour integration is forced to be clockwise when the semicircle is chosen in the
LHP.
assumption, the contribution of the integral of the semicircle tends to zero in
the limit R →∞. We close the contour in the upper half-plane (UHP) if q(z)
has a zero there. We then get
I1 = lim
R→∞
2
C
p(z)
q(z) dz = 2πi
k

j=1
Res
p(zj)
q(zj)

,
where C is the closed contour composed of the interval (−R, R) and the
semicircle CR, and {zj}k
j=1 are the zeros of q(z) in the UHP. We may instead
close the contour in the lower half-plane (LHP), in which case
I1 = −2πi
m

j=1
Res
p(zj)
q(zj)

,
where {zj}m
j=1 are the zeros of q(z) in the LHP. The minus sign indicates that
in the LHP we (are forced to) integrate in the negative sense.
Example 21.2.1. Let us evaluate the integral I =
 ∞
0
x2 dx/[(x2 + 1)(x2 + 9)].
Since the integrand is even, we can extend the interval of integration to all real
numbers (and divide the result by 2). It is shown below that in the limit that the
radius of the semicircle goes to inﬁnity, the integral of that semicircle goes to zero.
Therefore, we write the contour integral corresponding to I:
I = 1
2
2
C
z2 dz
(z2 + 1)(z2 + 9),
where C is as shown in Figure 21.2(a). Note that the contour is integrated in the
positive sense. This is always true for the UHP. The singularities of the function
in the UHP are the simple poles i and 3i corresponding to the simple zeros of the
denominator. By (21.5), the residues at these poles are

21.2 Integrals of Rational Functions
531
Res[f(i)] = lim
z→i

(z −i)
z2
(z −i)(z + i)(z2 + 9)

= −1
16i,
Res[f(3i)] = lim
z→3i

(z −3i)
z2
(z2 + 1)(z −3i)(z + 3i)

=
3
16i.
Thus, we obtain
I =
# ∞
0
x2 dx
(x2 + 1)(x2 + 9) = 1
2
2
C
z2 dz
(z2 + 1)(z2 + 9) = πi

−1
16i + 3
16i

= π
8 .
It is instructive to obtain the same results using the LHP. In this case the contour
is as shown in Figure 21.2(b). It is clear that the interior is to our right as we traverse
the contour. So we have to introduce a minus sign for its integration. The singular
points are at z = −i and z = −3i. These are simple poles at which the residues of
the function are
Res[f(−i)] = lim
z→−i

(z + i)
z2
(z −i)(z + i)(z2 + 9)

=
1
16i,
Res[f(−3i)] =
lim
z→−3i

(z + 3i)
z2
(z2 + 1)(z −3i)(z + 3i)

= −3
16i.
Therefore,
I =
# ∞
0
x2 dx
(x2 + 1)(x2 + 9) = 1
2
2
C
z2 dz
(z2 + 1)(z2 + 9) = −πi
 1
16i −3
16i

= π
8 .
We now show that the integral of the large circle Γ tends to zero. On such a
circle, z = Reiθ; therefore
#
Γ
z2 dz
(z2 + 1)(z2 + 9) =
#
Γ
R2e2iθReiθdθ
(R2e2iθ + 1)(R2e2iθ + 9).
In the limit that R →∞, we can ignore the small numbers 1 and 9 in the denom-
inator.
Then the overall integral becomes 1/R times a ﬁnite integral over θ.
It
follows that as R tends to inﬁnity, the contribution of the large circle indeed goes to
zero.
■
Example 21.2.2. Let us now consider a more complicated integral:
# ∞
−∞
x2 dx
(x2 + 1)(x2 + 4)2
which turns into
E
C z2 dz/[(z2 + 1)(z2 + 4)2]. The poles in the UHP are at z = i and
z = 2i. The former is a simple pole, and the latter is a pole of order 2. Thus,
Res[f(i)] = lim
z→i

(z −i)
z2
(z −i)(z + i)(z2 + 4)2

= −1
18i,
Res[f(2i)] =
1
(2 −1)! lim
z→2i
d
dz

(z −2i)2
z2
(z2 + 1)(z + 2i)2(z −2i)2

= lim
z→2i
d
dz

z2
(z2 + 1)(z + 2i)2

=
5
72i,
and
# ∞
−∞
x2 dx
(x2 + 1)(x2 + 4)2 = 2πi

−1
18i + 5
72i

= π
36.
Closing the contour in the LHP would yield the same result as the reader is urged
to verify.
■

532
Calculus of Residues
21.3
Products of Rational and Trigonometric
Functions
The second type of integral we can evaluate using the residue theorem is of
the form
# ∞
−∞
p(x)
q(x) cos ax dx
or
# ∞
−∞
p(x)
q(x) sin ax dx,
where a is a real number, p(x) and q(x) are real polynomials in x, and q(x)
has no real zeros. These integrals are the real and imaginary parts of
I2 =
# ∞
−∞
p(x)
q(x)eiax dx.
The presence of eiax dictates the choice of the half-plane: If a ≥0, we choose
the UHP because
eiaz = eia(x+iy) = eiaxe−ay
where
y > 0,
and the negative exponent ensures convergence for large R and y. For the same
reason, we choose the LHP when a ≤0. The following examples illustrate the
procedure.
Example 21.3.1. Let us evaluate
 ∞
−∞cos ax dx/(x2 + 1)2 where a ̸= 0. This
integral is the real part of the integral I2 =
 ∞
−∞eiax dx/(x2 + 1)2. When a > 0, we
close in the UHP. Then we proceed as for integrals of rational functions. Thus, we
have
I2 =
2
C
eiaz
(z2 + 1)2 dz = 2πi Res[f(i)]
for
a > 0,
because there is only one singularity in the UHP at z = i which is a pole of order 2.
We next calculate the residue:
Res[f(i)] = lim
z→i
d
dz

(z −i)2
eiaz
(z −i)2(z + i)2

= lim
z→i
d
dz

eiaz
(z + i)2

= lim
z→i
 (z + i)iaeiaz −2eiaz
(z + i)3

= e−a
4i (1 + a).
Substituting this in the expression for I2, we obtain I2 = (π/2)e−a(1 + a) for a > 0.
When a < 0, we have to close the contour in the LHP, where the pole of order
2 is at z = −i and the contour is taken clockwise. Thus, we get
I2 =
2
C
eiaz
(z2 + 1)2 dz = −2πi Res[f(−i)]
for a < 0.
For the residue we obtain
Res[f(−i)] = lim
z→−i
d
dz

(z + i)2
eiaz
(z −i)2(z + i)2

= −ea
4i (1 −a)
and the expression for I2 becomes I2 = (π/2)ea(1 −a) for a < 0. We can combine
the two results and write
# ∞
−∞
cos ax
(x2 + 1)2 dx = Re(I2) = I2 = π
2 (1 + |a|)e−|a|.
■

21.3 Products of Rational and Trigonometric Functions
533
Example 21.3.2. As another example, let us evaluate
# ∞
−∞
x sin ax
x4 + 4 dx
where a ̸= 0.
This is the imaginary part of the integral I2 =
 ∞
−∞xeiax dx/(x4+4) which, in terms
of z and for the closed contour in the UHP (when a > 0), becomes
I2 =
2
C
zeiaz
z4 + 4 dz = 2πi
m

j=1
Res[f(zj)]
for
a > 0,
(21.6)
where C is the large semicircle in the UHP. The singularities are determined by the
zeros of the denominator: z4 + 4 = 0 or z = 1 ± i, −1 ± i. Of these four simple poles
only two, 1 + i and −1 + i, are in the UHP. We now calculate the residues:
Res[f(1 + i)] =
lim
z→1+i(z −1 −i)
zeiaz
(z −1 −i)(z −1 + i)(z + 1 −i)(z + 1 + i)
= (1 + i)eia(1+i)
(2i)(2)(2 + 2i) = eiae−a
8i
,
Res[f(−1 + i)] =
lim
z→−1+i(z + 1 −i)
zeiaz
(z + 1 −i)(z + 1 + i)(z −1 −i)(z −1 + i)
= (−1 + i)eia(−1+i)
(2i)(−2)(−2 + 2i) = −e−iae−a
8i
.
Substituting in Equation (21.6), we obtain
I2 = 2πie−a
8i (eia −e−ia) = iπ
2 e−a sin a.
Thus,
# ∞
−∞
x sin ax
x4 + 4 dx = Im(I2) = π
2 e−a sin a
for
a > 0.
(21.7)
For a < 0, we could close the contour in the LHP. But there is an easier way of
getting to the answer. We note that −a > 0, and Equation (21.7) yields
# ∞
−∞
x sin ax
x4 + 4 dx = −
# ∞
−∞
x sin[(−a)x]
x4 + 4
dx = −π
2 e−(−a) sin(−a) = π
2 ea sin a.
We can collect the two cases in
# ∞
−∞
x sin ax
x4 + 4 dx = π
2 e−|a| sin a.
■
Example 21.3.3. The integral
 ∞
0
sin ax
x
dx occurs frequently in physics. To eval-
uate it, ﬁrst we assume that a > 0 and note that since the integrand is even, we can
extend the lower limit of integration to −∞and write
# ∞
0
sin ax
x
dx = 1
2
# ∞
−∞
sin ax
x
dx.
As in the previous examples, we are inclined to choose the contour C in the UHP.
However, since C passes through the origin, this will not work because the origin is
the pole of the integrand. So, let’s avoid the origin by going around it on a small

534
Calculus of Residues
Figure 21.3: To avoid the origin move on an inﬁnitesimal semicircle γϵ of radius ϵ.
circle of radius ϵ as shown in Figure 21.3. This contour does not surround a pole.
Therefore, we can write
0 =
2
C
eiaz
z
dz =
# −ϵ
−∞
eiax
x dx +
#
γϵ
eiaz
z
dz +
# ∞
ϵ
eiax
x dx
As ϵ →0, the two integrals in x become a single integral over all real numbers.
Thus, we get
# ∞
−∞
eiax
x dx = −lim
ϵ→0
#
γϵ
eiaz
z
dz
But on γϵ, z = ϵeiθ. Thus
lim
ϵ→0
#
γϵ
eiaz
z
dz = lim
ϵ→0
# 0
π
eiaϵeiθ
ϵeiθ
iϵeiθdθ = i lim
ϵ→0
# 0
π
eiaϵeiθ dθ = −iπ
and
# ∞
−∞
eiax
x dx = iπ.
Putting everything together, we obtain
# ∞
0
sin ax
x
dx = 1
2
# ∞
−∞
sin ax
x
dx = 1
2 Im
# ∞
−∞
eiax
x dx = 1
2 Im(iπ) = π
2
If a < 0, then sin ax = −sin |a|x and we get the negative of the answer above.
■
21.4
Functions of Trigonometric Functions
The third type of integral we can evaluate using the residue theorem involves
only trigonometric functions and is typically of the form
# 2π
0
F(sin θ, cos θ) dθ,
where F is some (typically rational) function3 of its arguments. Since θ varies
from 0 to 2π, we can consider it as the angle of a point z on the unit circle
3Recall that a rational function is, by deﬁnition, the ratio of two polynomials.

21.4 Functions of Trigonometric Functions
535
centered at the origin. Then z = eiθ and e−iθ = 1/z, and we can substitute
cos θ = (z + 1/z)/2, sin θ = (z −1/z)/(2i), and dθ = dz/(iz) in the original
integral to obtain
2
C
F
z −1/z
2i
, z + 1/z
2
 dz
iz .
This integral can often be evaluated using the method of residues.
Example 21.4.1. Let us evaluate the integral
 2π
0
dθ/(1 + a cos θ) where |a| < 1.
Substituting for cos θ and dθ in terms of z, we obtain
2
C
dz/iz
1 + a[(z2 + 1)/2z] = 2
i
2
C
dz
2z + az2 + a,
where C is the unit circle centered at the origin. The singularities of the integrand
are the zeros of its denominator 2z + az2 + a ≡a(z −z1)(z −z2) with
z1 = −1 +
√
1 −a2
a
and
z2 = −1 −
√
1 −a2
a
.
For |a| < 1 it is clear that z2 will lie outside the unit circle C; therefore, it does not
contribute to the integral. But z1 lies inside, and we obtain
2
C
dz
2z + az2 + a = 2πi Res[f(z1)].
The residue of the simple pole at z1 can be calculated:
Res[f(z1)] = lim
z→z1(z −z1)
1
a(z −z1)(z −z2) = 1
a

1
z1 −z2

= 1
a

a
2
√
1 −a2

=
1
2
√
1 −a2 .
It follows that
# 2π
0
dθ
1 + a cos θ = 2
i
2
C
dz
2z + az2 + a = 2
i 2πi

1
2
√
1 −a2

=
2π
√
1 −a2 .
■
Example 21.4.2. As another example, let us consider the integral
I =
# π
0
dθ
(a + cos θ)2
where
a > 1.
Since cos θ is an even function of θ, we may write
I = 1
2
# π
−π
dθ
(a + cos θ)2
where
a > 1.
This integration is over a complete cycle around the origin, and we can make the
usual substitution:
I = 1
2
2
C
dz/iz
[a + (z2 + 1)/2z]2 = 2
i
2
C
z dz
(z2 + 2az + 1)2 .
The denominator has the roots z1 = −a +
√
a2 −1 and z2 = −a −
√
a2 −1 which
are both of order 2. The second root is outside the unit circle because a > 1. The
reader may verify that for all a > 1, z1 is inside the unit circle. Since z1 is a pole of
order 2, we have

536
Calculus of Residues
Res[f(z1)] = lim
z→z1
d
dz

(z −z1)2
z
(z −z1)2(z −z2)2

= lim
z→z1
d
dz

z
(z −z2)2

=
1
(z1 −z2)2 −
2z1
(z1 −z2)3 =
a
4(a2 −1)3/2 .
We thus obtain
I = 2
i 2πi Res[f(z1)] =
πa
(a2 −1)3/2 .
■
21.5
Problems
21.1. Evaluate each of the following integrals, for all of which C is the circle
|z| = 3:
(a)
2
C
4z −3
z(z −2) dz.
(b)
2
C
ez
z(z −iπ) dz.
(c)
2
C
cos z
z(z −π) dz.
(d)
2
C
z2 + 1
z(z −1) dz.
(e)
2
C
cosh z
z2 + π2 dz.
(f)
2
C
1 −cos z
z2
dz.
(g)
2
C
sinh z
z4
dz.
(h)
2
C
z cos
1
z

dz.
(i)
2
C
dz
z3(z + 5) dz.
(j)
2
C
tan z dz.
(k)
2
C
dz
sinh 2z dz.
(l)
2
C
ez
z2 dz.
(m)
2
C
dz
z2 sin z dz.
(n)
2
C
ez dz
(z −1)(z −2).
21.2. Find the residue of f(z) = 1/ cosz at all its poles.
21.3. Evaluate the integral
 ∞
0
dx/[(x2 + 1)(x2 + 4)] by closing the contour
(a) in the UHP and (b) in the LHP.
21.4. Evaluate the following integrals in which a and b are nonzero real con-
stants:
(a)
# ∞
0
2x2 + 1
x4 + 5x2 + 6 dx.
(b)
# ∞
0
dx
6x4 + 5x2 + 1.
(c)
# ∞
0
dx
x4 + 1.
(d)
# ∞
0
cos x dx
(x2 + a2)2(x2 + b2).
(e)
# ∞
0
cos ax
(x2 + b2)2 dx.
(f)
# ∞
0
dx
(x2 + 1)2 .
(g)
# ∞
0
dx
(x2 + 1)2(x2 + 2).
(h)
# ∞
0
2x2 −1
x6 + 1 dx.
(i)
# ∞
0
x2dx
(x2 + a2)2 .
(j)
# ∞
−∞
x dx
(x2 + 4x + 13)2 .
(k)
# ∞
0
x3 sin ax
x6 + 1 dx.
(l)
# ∞
0
x2 + 1
x2 + 4 dx.
(m)
# ∞
−∞
x cos x dx
x2 −2x + 10.
(n)
# ∞
−∞
x sin x dx
x2 −2x + 10.
(o)
# ∞
0
dx
x2 + 1.
(p)
# ∞
0
x2dx
(x2 + 4)2(x2 + 25).
(q)
# ∞
0
cos ax
x2 + b2 dx.
(r)
# ∞
0
dx
(x2 + 4)2 .
21.5. Evaluate each of the following integrals by turning them into contour
integrals around the unit circle.

21.5 Problems
537
(a)
# 2π
0
dθ
5 + 4 sin θ.
(b)
# 2π
0
dθ
a + cos θ
where a > 1.
(c)
# 2π
0
dθ
1 + sin2 θ.
(d)
# 2π
0
dθ
(a + b cos2 θ)2
where a, b > 0.
(e)
# 2π
0
cos2 3θ
5 −4 cos 2θ dθ.
(f)
# π
0
dφ
1 −2a cosφ + a2
where a ̸= ±1.
(g)
# π
0
cos2 3φ dφ
1 −2a cosφ + a2
where a ̸= ±1.
(h)
# π
0
cos 2φ dφ
1 −2a cosφ + a2
where a ̸= ±1.
21.6. Use the method of residues to show that
# π
0
cos2n θ dθ = π
(2n)!
22n(n!)2
21.7. Use the contour in Figure 21.4(a) to show that
# ∞
−∞
sin x
x
dx = π
by letting X →∞, Y →∞, and ϵ →0.
21.8. Use the contour in Figure 21.4(b) to show that
# ∞
0
1
1 + xn dx =
π/n
sin(π/n)
by letting R →∞.
21.9. Use the contour in Figure 21.4(c) to show that
# ∞
0
sin(x2) dx =
# ∞
0
cos(x2) dx =
π
8
by letting R →∞.
X
−X
−ε
ε
X + iY
−X + iY
R
R
π /4
2π /n
R
R
(a)
(c)
(b)
Figure 21.4: (a) The contour used for sin x/x. (b) The contour used for 1/(1 + xn).
(c) The contour used for sin(x2).

Part VI
Diﬀerential Equations

Chapter 22
From PDEs to ODEs
Physics, as the most exact science, is characterized by its ability to make
mathematical predictions. Predictions are based on two factors: the initial
information (data), and the law governing the physical process.
Knowing
what the situation is here and now (initial data, initial conditions, boundary
conditions) enables physics to predict what the situation will be there and
then.
This ability to predict is based on the intuitive belief that physical
quantities, dependent on continuous parameters such as position and time,
must be continuous functions of those parameters. Thus, knowledge of the
initial conditions
are needed to
predict the
evolution of a
physical system.
values of those functions at one (initial) point and of how the functions change
from one point to a neighboring point (given by the laws of physics) allows
the values of the functions at the neighboring point to be predicted. Once
the values of the functions are determined at the new point, their values can
be predicted for its neighboring points, and the process can continue until a
distant point is reached.
In mechanics, for example, knowledge of the force acting on a particle of
mass m, located at r0 and moving with momentum p0 at time t0, allows its
momentum and position at a later time t0 + Δt to be predicted as follows.
Because dp/dt = F by Newton’s second law of motion, we have
Δp ≈F(r0, p0, t0)Δt
and
p(t0 + Δt) = p0 + Δp ≈p0 + F(r0, p0, t0)Δt.
Similarly,
r(t0 + Δt) ≈r0 + v0t ≈r0 + p0
m Δt.
The smaller Δt is, the better the prediction will be.
Newton’s second law of motion,
d
dt

mdr
dt

= F(r, dr/dt, t)

542
From PDEs to ODEs
is an example of an ordinary diﬀerential equation (ODE). A dependent
ordinary
diﬀerential
equation (ODE)
variable r is determined from an equation involving a single independent vari-
able t, the dependent variable r, and its various derivatives.
In (point) particle mechanics there is only one independent variable, lead-
ing to ODEs. In other areas of physics, however, in which extended objects
such as ﬁelds are studied, variations with respect to position are also present.
Partial derivatives with respect to coordinate variables show up in the diﬀer-
ential equations, which are therefore called partial diﬀerential equations
partial diﬀerential
equations (PDEs)
(PDEs). For instance, in electrostatics, where time-independent scalar ﬁelds
such as potentials, and vector ﬁelds such as electrostatic ﬁelds, are studied,
the law is described by Poisson’s equation, ∇2Φ(r) = −4πρ(r), where Φ is
the electrostatic potential and ρ is the volume charge density. Other PDEs
occurring in mathematical physics include the heat equation, describing the
transfer of heat, the wave equation, describing the propagation of various
kinds of wave, and the Schr¨odinger equation, describing nonrelativistic quan-
tum mechanical phenomena.
In fact, except for the laws of particle mechanics and electrical circuits,
in which the only independent variable is time, almost all laws of physics are
described by PDEs. We shall not study PDEs in their full generalities, but
concentrate on the simplest ones encountered most frequently in ideal physical
applications. The method of solution that works for all these equations is the
separation of variables, whereby a PDE is turned into a number of ODEs.
Before embarking on the separation of variables, we need to formalize the
discussion above. An ordinary or a partial DE will provide a unique solution
to a physical problem only if the initial or the starting value of the solution
is known. We refer to this as the boundary conditions, or BCs for short.
the meaning of
boundary
conditions (or
BCs) elaborated
For ODEs, boundary conditions amount to the speciﬁcation of one or more
properties of the solution at an initial time; that is why for ODEs, one speaks
of initial conditions. BCs for PDEs involve speciﬁcation of the solution on
a surface (or a curve, if the PDE has only two variables).
22.1
Separation of Variables
We list here the PDEs encountered in undergraduate courses and initiate
their transformation into ODEs. Let us start with the simplest PDE arising
in electrostatic problems, the Poisson equation, derived in Chapter 15,
Poisson equation
Laplace’s equation
∇2Φ(r) = −4πρ(r).
(22.1)
In vacuum, where ρ(r) = 0, Equation (22.1) reduces to Laplace’s equation,
∇2Φ(r) = 0.
(22.2)
Many electrostatic problems involve conductors held at constant potentials
and situated in a vacuum. In the space between such conducting surfaces, the
electrostatic potential obeys Equation (22.2).

22.1 Separation of Variables
543
Next in complexity is the heat equation, whose most simpliﬁed version—
heat equation
the one studied here—is
∂T
∂t = k2∇2T (r, t),
(22.3)
where T is the temperature and k is a real constant characterizing the medium
in which heat is ﬂowing.
Probably one of the most recurring PDEs encountered in mathematical
physics is the wave equation,
wave equation
∇2Ψ −1
c2
∂2Ψ
∂t2 = 0.
(22.4)
This equation (or its simpliﬁcation to lower dimensions) is applied to the
vibration of strings and drums, the propagation of sound in gases, solids, and
liquids, the propagation of disturbances in plasmas, and the propagation of
electromagnetic waves.
The Schr¨odinger equation, describing the nonrelativistic quantum phe-
Schr¨odinger
equation
nomena, is
−ℏ2
2m∇2Ψ + V (r)Ψ = −iℏ∂Ψ
∂t ,
(22.5)
where m is the mass of a subatomic particle, ℏis Planck’s constant (divided by
2π), V is the potential energy of the particle, and |Ψ(r, t)|2 is the probability
density of ﬁnding the particle at r at time t.
Equations (22.3)–(22.5) have partial derivatives with respect to time. As
a ﬁrst step toward solving these PDEs, let us separate the time variable. We
will denote the functions in all four equations by the generic symbol Ψ(r, t).
The separation of variables starts with separating the r and t dependence
into factors:1
Ψ(r, t) ≡R(r)T (t).
This factorization permits us to separate the two operations of space diﬀer-
time is separated
from space
entiation and time diﬀerentiation. As an illustration, we separate the time
and space dependence for the Schr¨odinger equation. The other equations are
done similarly. Substituting for Ψ, we get
−ℏ2
2m∇2(RT ) + V (r)(RT ) = −iℏ∂
∂t(RT ),
or
−T ℏ2
2m∇2R + V (r)(RT ) = −iRℏdT
dt ,
where we have used ordinary derivatives for T because, by assumption, it is
a function of a single variable. Dividing both sides by RT yields
1Note that there is no a priori reason why the basic assumption underlying the separation
of variables is legitimate. After all, we cannot write sin(xt) as a product, f(x)g(t). However,
in all cases of physical interest the separation of variables works.

544
From PDEs to ODEs
−1
R
ℏ2
2m∇2R + V (r) = −i 1
T ℏdT
dt .
(22.6)
Now comes the crucial step in the process of the separation of variables.
central argument
in separation of
variables
The LHS of Equation (22.6) is a function of position alone, and the RHS is a
function of time alone. Since r and t are independent variables, the only way
that (22.6) can hold is for both sides to be constant, say α:
−1
R
ℏ2
2m∇2R + V (r) = α ⇒−ℏ2
2m∇2R + V (r)R = αR
and
−iℏ1
T
dT
dt = α ⇒dT
dt = iα
ℏT.
(22.7)
We have reduced the original time-dependent Schr¨odinger equation, a
PDE, to an ODE involving only time, and a PDE involving only the posi-
tion variables. Most problems of elementary mathematical physics have the
same property, i.e., they are completely equivalent to Equation (22.7) plus
the equation before it, which we write generically as
∇2R + f(r)R = 0,
(22.8)
where we have simpliﬁed the notation by including α in the function f.
The foregoing discussion is summarized in this statement:
Box 22.1.1. The time-dependent PDEs of mathematical physics can be
reduced to an ODE in the time variable and the PDE given in Equation
(22.8).
For those PDEs involving second time derivatives, such as the
wave equation, (22.7) will be a second-order ODE.
With the exception of Poisson’s equation, in all the foregoing equations
the term on the RHS is zero.
We will restrict ourselves to this so-called
homogeneous case2 and rewrite (22.8) as
∇2Ψ(r) + f(r)Ψ(r) = 0.
(22.9)
The rest of this section is devoted to the study of this equation in various
coordinate systems.
22.2
Separation in Cartesian Coordinates
In Cartesian coordinates, Equation (22.9) becomes
∂2Ψ
∂x2 + ∂2Ψ
∂y2 + ∂2Ψ
∂z2 + f(x, y, z)Ψ = 0.
2The most elegant way of solving inhomogeneous PDEs is the method of Green’s func-
tions, of which we shall have a brief discussion in Chapter 29. For a thorough discussion
of Green’s functions, see Hassani, S. Mathematical Physics: A Modern Introduction to Its
Foundations, Springer-Verlag, 1999, Part VI.

22.2 Separation in Cartesian Coordinates
545
As in the case of the separation of the time variable, we assume that we can
separate the dependence on various coordinates and write
Ψ(x, y, z) = X(x)Y (y)Z(z).
Then the above PDE yields
Y Z d2X
dx2 + XZ d2Y
dy2 + XY d2Z
dz2 + f(x, y, z)XY Z = 0.
Dividing by XY Z gives
1
X
d2X
dx2 + 1
Y
d2Y
dy2 + 1
Z
d2Z
dz2 + f(x, y, z) = 0.
(22.10)
This equation is almost separated. The ﬁrst term is a function of x alone, the
second of y alone, and the third of z alone. However, the last term, in general,
mixes the coordinates. The only way the separation can become complete is
for the last term to be separated as well, that is, expressed as a sum of three
functions, each depending on a single coordinate.3 In such a special case we
obtain
1
X
d2X
dx2 + 1
Y
d2Y
dy2 + 1
Z
d2Z
dz2 + f1(x) + f2(y) + f3(z) = 0
or
 1
X
d2X
dx2 + f1(x)

+
 1
Y
d2Y
dy2 + f2(y)

+
 1
Z
d2Z
dz2 + f3(z)

= 0.
The ﬁrst term on the LHS depends on x alone, the second on y alone, and
the third on z alone. Since the sum of these three terms is a constant (zero),
independent of all variables, each term must be a constant. Denoting the
constant corresponding to the ith term by −αi, we obtain
1
X
d2X
dx2 + f1(x) = −α1,
1
Y
d2Y
dy2 + f2(y) = −α2,
1
Z
d2Z
dz2 + f3(z) = −α3,
which can be reexpressed as
d2X
dx2 + [f1(x) + α1] X = 0,
d2Y
dy2 + [f2(y) + α2] Y = 0,
d2Z
dz2 + [f3(z) + α3] Z = 0,
α1 + α2 + α3 = 0.
(22.11)
If f(x, y, z) happens to be a constant C, then the ﬁrst three terms of Equation
(22.10) can be taken to be respectively −α1, −α2, and −α3, leading to
d2X
dx2 + α1X = 0,
d2Y
dy2 + α2Y = 0,
d2Z
dz2 + α3Z = 0,
α1 + α2 + α3 = C.
(22.12)
3This is where the limitation of the method of the separation of variables becomes
evident. However, surprisingly, all physical applications, at our level of treatment, involve
functions that are indeed separated.

546
From PDEs to ODEs
These equations constitute the most general set of ODEs resulting from the
separation of the PDE of Equation (22.9) in Cartesian coordinates.
Example 22.2.1. Let us consider a few cases for which (22.11) or (22.12) is
applicable.
(a) In electrostatics, separation of Laplace’s equation, for which f(r) = 0, leads to
Laplace’s equation
these ODEs:
d2X
dx2 + α1X = 0,
d2Y
dy2 + α2Y = 0,
d2Z
dz2 −(α1 + α2)Z = 0.
The solutions to these equations are trigonometric or hyperbolic (exponential) func-
tions, determined from the boundary conditions (conducting surfaces). The unsym-
metrical treatment of the three coordinates—the plus sign in front of the ﬁrst two
constants and a minus sign in front of the third—is not dictated by the above equa-
tions. There is a freedom in the choice of sign in these equations. However, the
boundary conditions will force the constants to adapt to values appropriate to the
physical situation at hand.
(b) In quantum mechanics the time-independent Schr¨odinger equation for a free
particle in three dimensions is
∇2Ψ + 2mE
ℏ2 Ψ = 0.
Separation of variables yields the ODEs of Equation (22.12) with
α1 + α2 + α3 = 2mE
ℏ2 .
After time is separated, the heat and wave equations also yield equations similar to
(22.12).
(c) In quantum mechanics the time-independent Schr¨odinger equation for a three
dimensional isotropic harmonic oscillator is
∇2Ψ −
m2ω2
ℏ2
r2 −2mE
ℏ2

Ψ = 0.
Thus,
f(r) = −m2ω2
ℏ2
r2 + 2mE
ℏ2
= −m2ω2
ℏ2
(x2 + y2 + z2) + 2mE
ℏ2 .
Equation (22.11) then yields
d2X
dx2 −m2ω2
ℏ2
x2X + α1X = 0,
d2Y
dy2 −m2ω2
ℏ2
y2Y + α2Y = 0,
d2Z
dz2 −m2ω2
ℏ2
z2Z + α3Z = 0,
with α1 + α2 + α3 = 2mE/ℏ2.
■

22.3 Separation in Cylindrical Coordinates
547
22.3
Separation in Cylindrical Coordinates
Equation (22.9) takes the following form in cylindrical coordinates:4
1
ρ
∂
∂ρ

ρ∂Ψ
∂ρ

+ 1
ρ2
∂2Ψ
∂ϕ2 + ∂2Ψ
∂z2 + f(ρ, ϕ, z)Ψ = 0.
To separate the variables, we write Ψ(ρ, ϕ, z) = R(ρ)S(ϕ)Z(z), substitute in
the general equation, and divide both sides by RSZ to obtain
1
R
1
ρ
d
dρ

ρdR
dρ

+ 1
S
1
ρ2
d2S
dϕ2 + 1
Z
d2Z
dz2 + f(ρ, ϕ, z) = 0.
We shall consider only the special (but important) case in which f(ρ, ϕ, z)
is a constant λ. In that case, the equation becomes
 1
R
1
ρ
d
dρ

ρdR
dρ

+ 1
ρ2
 1
S
d2S
dϕ2




function of ρ and ϕ only
+
 1
Z
d2Z
dz2




fn. of z
+λ = 0.
The sum of the ﬁrst two terms is independent of z, so the third term must be
as well. We thus get
1
Z
d2Z
dz2 = λ1
and
 1
R
1
ρ
d
dρ

ρdR
dρ

+ 1
ρ2
 1
S
d2S
dϕ2

+ λ1 + λ = 0.
Multiplying this equation by ρ2 yields
 ρ
R
d
dρ

ρdR
dρ

+ (λ1 + λ)ρ2




function of ρ only
+
 1
S
d2S
dϕ2




fn. of ϕ
= 0.
Since the ﬁrst term is a function of ρ only and the second a function of ϕ only,
both terms must be constants whose sum vanishes. Thus,
1
S
d2S
dϕ2 = μ,
ρ
R
d
dρ

ρdR
dρ

+ (λ1 + λ)ρ2 + μ = 0.
(22.13)
Putting together all of the above, we conclude that when Equation (22.9)
is separable in cylindrical coordinates and f(r) = λ, it will separate into the
following three ODEs:
d2Z
dz2 −λ1Z = 0,
d2S
dϕ2 −μS = 0,
d
dρ

ρdR
dρ

+
(
(λ1 + λ)ρ +
μ
ρ
)
R = 0,
(22.14)
4See Chapter 16 for the expression of ∇2 in spherical and cylindrical coordinate systems.

548
From PDEs to ODEs
where in rewriting the second equation in (22.13), we multiplied both sides of
the equation by R and divided it by ρ. The last equation of (22.14) is called the
Bessel diﬀerential equation. This equation shows up in electrostatic and
Bessel diﬀerential
equation
heat-transfer problems with cylindrical geometry and in problems involving
two-dimensional wave propagation, as in drumheads.
Historical Notes
Jean Le Rond d’Alembert was the illegitimate son of a famous salon host-
ess of eighteenth-century Paris and a cavalry oﬃcer.
Abandoned by his mother,
d’Alembert was raised by a foster family and later educated by the arrangement of
his father at a nearby church-sponsored school, in which he received instruction in
the classics and above-average instruction in mathematics. After studying law and
medicine, he ﬁnally chose to pursue a career in mathematics. In the 1740s he joined
the ranks of the philosophes, a growing group of deistic and materialistic thinkers
and writers who actively questioned the social and intellectual standards of the day.
He traveled little (he left France only once, to visit the court of Frederick the Great),
preferring instead the company of his friends in the salons, among whom he was well
known for his wit and laughter.
Jean Le Rond
d’Alembert
1717–1783
d’Alembert turned his mathematical and philosophical talents to many of the
outstanding scientiﬁc problems of the day, with mixed success. Perhaps his most
famous scientiﬁc work, entitled Traite de dynamique, shows his appreciation that
a revolution was taking place in the science of mechanics—the formalization of
the principles stated by Newton into a rigorous mathematical framework. Later,
d’Alembert produced a treatise on ﬂuid mechanics, a paper dealing with vibrating
strings, and a skillful treatment of celestial mechanics. d’Alembert is also credited
with the use of the ﬁrst partial diﬀerential equation as well as the ﬁrst solution to
such an equation using separation of variables.
Much of the work for which d’Alembert is remembered occurred outside math-
ematical physics. He was chosen as the science editor of the Encyclopedie, and his
lengthy Discours Preliminaire in that volume is considered one of the deﬁning doc-
uments of the Enlightenment. Other works included writings on law, religion, and
music.
22.4
Separation in Spherical Coordinates
By far the most commonly used coordinate system in mathematical physics
is the spherical coordinate system. This is because forces, potential energies,
and most geometries encountered in Nature have a spherical symmetry. One
of the consequences of this spherical symmetry is that the function f(r) is
a function of r and not of angles. We shall assume this to be true in this
subsection.
In spherical coordinates, Equation (22.9) becomes [see Equation (16.19)]
1
r2
∂
∂r

r2 ∂Ψ
∂r

+
1
r2 sin θ
( ∂
∂θ

sin θ∂Ψ
∂θ

+
1
sin θ
∂2Ψ
∂ϕ2
)
+f(r)Ψ=0. (22.15)

22.4 Separation in Spherical Coordinates
549
To separate this equation means to write Ψ(r, θ, ϕ) = R(r)Θ(θ)Φ(ϕ). If we
substitute this in Equation (22.15) and note that each diﬀerentiation acts on
only one of the three functions, we get
ΘΦ 1
r2
d
dr

r2 dR
dr

+ R
r2
 Φ
sin θ
d
dθ

sin θdΘ
dθ

+
Θ
sin2 θ
d2Φ
dϕ2

+ f(r)RΘΦ = 0.
Now divide both sides by RΘΦ and multiply by r2 to obtain
1
R
d
dr

r2 dR
dr

+ r2f(r)



function of r alone
+

1
Θ sin θ
d
dθ

sin θdΘ
dθ

+
1
Φ sin2 θ
d2Φ
dϕ2




function of θ and ϕ only
= 0.
Since each one of the two terms is a function of diﬀerent variables, each must
be a constant; and the two constants must add up to zero. Therefore, we have
1
R
d
dr

r2 dR
dr

+ r2f(r) = α,
1
Θ sin θ
d
dθ

sin θdΘ
dθ

+
1
Φ sin2 θ
d2Φ
dϕ2 = −α.
The second equation can be further separated. We add α to both sides and
multiply the resulting equation by sin2 θ to obtain
sin θ
Θ
d
dθ

sin θdΘ
dθ

+ α sin2 θ



function of θ alone; set = β
+ 1
Φ
d2Φ
dϕ2
  
=−β
= 0.
We have thus obtained three ODEs in three variables. We rewrite these ODEs
in the following equations:
1
r2
d
dr

r2 dR
dr

+
*
f(r) −α
r2
+
R = 0,
1
sin θ
d
dθ

sin θdΘ
dθ

+

α −
β
sin2 θ

Θ = 0,
(22.16)
d2Φ
dϕ2 + βΦ = 0.
radial, polar, and
azimuthal
equations
The ﬁrst equation is called the radial equation, the second the polar
equation, and the third the azimuthal equation. The radial equation can
be further simpliﬁed by making the substitution R = u/r. This gives
d2u
dr2 +
*
f(r) −α
r2
+
u = 0.
(22.17)
Our task in this chapter was to separate the PDEs most frequently encoun-
tered in undergraduate mathematical physics into ODEs; and we have done

550
From PDEs to ODEs
this in the three coordinate systems regularly used in applications. We shall
return to a thorough treatment of the ODEs so obtained later in the book,
and in the process we shall be introduced to the so-called special functions
that came into being in the nineteenth century as a result of the then newly
discovered technique of the separation of variables.
22.5
Problems
22.1. Assume that two functions Φ1 and Φ2 satisfy the Poisson equation.
Show that
(a) Φ deﬁned by Φ = Φ1 −Φ2 satisﬁes the Laplace’s equation;
(b) ∇· (Φ∇Φ) = |∇Φ|2
22.2. Separate the solution of the heat equation (22.3): T (r, t) ≡R(r)τ(t),
and show that
(a) the solution to the time equation is
τ(t) = Ae−αk2t,
(b) in which case, the space part must satisfy the following PDE:
∇2R + αR = 0
22.3. Show that any function of the form f(k · r ± ωt) satisﬁes the wave
equation (22.4) if ω = c|k|.
22.4. Separate the solution of the wave equation (22.4): Ψ(r, t) ≡R(r)T (t),
and show that
(a) the solution to the time equation is
T (t) = A cos ωt + B sin ωt,
(b) and the space part must satisfy the following PDE:
∇2R + k2R = 0
where k = ω/c.
22.5. Provide the details of the derivation of Equation (22.16).
22.6. By substituting R = u/r in the radial DE of spherical coordinates,
show that it reduces to Equation (22.17).

Chapter 23
First-Order Diﬀerential
Equations
The last chapter showed that all PDEs discussed there resulted in ODEs of
second order, i.e., diﬀerential equations involving second derivatives. Thus,
treating the ﬁrst-order DEs (FODEs) may seem irrelevant. However, some-
times a second-order DE (SODE) may be expressed in terms of ﬁrst deriva-
tives.
For example, take Newton’s second law of motion along a straight
line (free fall, say): m d2x/dt2 = F. If we write this in terms of velocity,
we obtain m dv/dt = F, and if F is a function of v alone—as in a fall with
air resistance—then we have a FODE. FODEs arise in other areas of physics
beside mechanics. Therefore, it is worthwhile to study them here.
23.1
Normal Form of a FODE
The most general FODE is of the form G(x, y, y′) = 0, where G is some
function of three variables. We can ﬁnd y′ (the derivative of y) as a function
of x and y if the function G(x1, x2, x3) is suﬃciently well behaved. In that
case, we have
the most general
FODE in normal
form
y′ ≡dy
dx = F(x, y)
(23.1)
which is said to be a normal FODE.
Example 23.1.1. There are three special cases of Equation (23.1) that lead im-
mediately to a solution.
(a) If F(x, y) is independent of y, then y′ = g(x), and the most general solution can
be written as y = f(x) = C +
 x
a g(t)dt where C = f(a).
(b) If F(x, y) is independent of x, then dy/dx = h(y), and
dy
h(y) = dx ⇒
# y
C
dt
h(t)



≡H(y)
−x + a = 0 ⇒H(y) −x + a = 0

552
First-Order Diﬀerential Equations
embodies a solution. That is, H(y) = x −a can be solved for y in terms of x, say
y = f(x), and this y will be a solution of the DE. Note that y|x=a ≡f(a) = C.
(c) The third special case is really a generalization of the ﬁrst two. If F(x, y) =
g(x)h(y), then y′ = g(x)h(y) or dy/h(y) = g(x)dx and
# y
C
dt
h(t) =
# x
a
g(t)dt
(23.2)
is an implicit solution.
■
The example above contains an information which is important enough to
be “boxed.”
Box 23.1.1. A diﬀerential equation is considered to be solved if its solu-
tion can be obtained by solving an algebraic equation involving integrals of
known functions. Whether these integrals can be done in closed form or
not is irrelevant.
So, although we may not be able to actually perform the integration of (23.2),
we consider the DE solved because, in principle, Equation (23.2) gives y as a
(implicit) function of x.
As Example 23.1.1 shows, the solutions to a FODE are usually obtained
in an implicit form, as a function u of two variables such that the solution y
can be found by solving u(x, y) = 0 for y. Included in u(x, y) is an arbitrary
constant related to the initial conditions. The equation u(x, y) = 0 deﬁnes
a curve in the xy-plane, which depends on the (hidden) constant in u(x, y).
Since diﬀerent constants give rise to diﬀerent curves, it is convenient to sep-
arate the constant and write u(x, y) = C. This leads to the concept of an
integral of a diﬀerential equation.
integral of a
normal FODE
Deﬁnition 23.1.1. An integral of a normal FODE [Equation (23.1)] is
a function of two variables u(x, y) such that u(x, f(x)) is a constant for all
possible values of x whenever y = f(x) is a solution of the diﬀerential equation.
The integrals of diﬀerential equations are encountered often in physics. If
an integral of a
FODE is also
called a constant
of motion.
x is replaced by t (time), then the diﬀerential equation describes the motion
of a physical system, and a solution, y = f(t), can be written implicitly as
u(t, y) = C, where u is an integral of the diﬀerential equation. The equation
u(t, y) = C describes a curve in the ty-plane on which the value of the function
u(t, y) remains unchanged for all t. Thus, u(t, y), the integral of the FODE,
is also called a constant of motion.
Example 23.1.2. Consider a point particle moving under the inﬂuence of a force
depending on position only. Denoting the position1 by x and the velocity by v,
we have, by Newton’s second law, m dv/dt = F(x). Using the chain rule, dv/dt =
(dv/dx)(dx/dt) = v dv/dx, we obtain
mv dv
dx = F(x) ⇒mv dv = F(x) dx,
(23.3)
1Here we are restricting the motion to one dimension.

23.2 Integrating Factors
553
which is easily integrated to
1
2mv2 =
#
F(x) dx + C ≡−V (x) + C.
(23.4)
The potential energy V (x) = −

F(x) dx has been introduced as an indeﬁnite
potential energy
integral. We can write Equation (23.4) as
1
2mv2 + V (x) = C.
(23.5)
Thus, the integral of Equation (23.3) is u(x, v) =
1
2mv2 + V (x) which is the ex-
pression for the energy of the one-dimensional motion of a particle experiencing the
potential V (x). If v is a solution of Equation (23.3), then u(x, v) = constant. Since
a solution of Equation (23.3) describes a possible motion of the particle, Equation
(23.5) implies that the energy of a particle does not change in the course of its
motion. This statement is the conservation of (mechanical) energy.
■
23.2
Integrating Factors
Let D be a region in the xy-plane, and let M(x, y) and N(x, y) be continuous
functions of x and y deﬁned on D. The diﬀerential Mdx + Ndy is exact if,
for arbitrary points P1 and P2 of D, the line integral
exact diﬀerential
# P2
P1
[M(x, y) dx + N(x, y) dy]
is independent of the path joining the two points. This condition is equivalent
to saying that the line integral of the integrand around any closed loop in
D vanishes. A necessary and suﬃcient condition for exactness is, therefore,
that the curl of the vector A = ⟨M, N, 0⟩be zero.2 The vector A is then
conservative, and we can deﬁne a (potential) function v such that A = ∇v =
⟨∂v/∂x, ∂v/∂y, 0⟩, or
dv = ∂v
∂x dx + ∂v
∂y dy = M dx + N dy.
(23.6)
Thus, M dx + N dy is exact if and only if there exists a function v(x, y)
satisfying (23.6), in which case, M = ∂v/∂x and N = ∂v/∂y.
Now consider all y’s that satisfy v(x, y) = C for some constant C. Then
since dC = 0, we have
0 = dv = M dx + N dy.
It follows that v(x, y) = C is an implicit solution of the diﬀerential equation.
We therefore have
2The statement is true only if the region D does not contain any singularities of M or
N. The region is then called contractable to a point (see Section 14.3).

554
First-Order Diﬀerential Equations
Theorem 23.2.1. If M(x, y) dx + N(x, y) dy is an exact diﬀerential dv in a
domain D of the xy-plane, then v(x, y) is an integral of the DE
M(x, y) dx + N(x, y) dy = 0
whose solutions are of the form v(x, y) = C.
We saw above that, for an exact diﬀerential, M = ∂v/∂x and N = ∂v/∂y.
A necessary consequence of this result is ∂M/∂y = ∂N/∂x. Could this relation
be a suﬃcient condition as well? Consider the function v(x, y) deﬁned by
v(x, y) ≡
# x
a
M(t, y) dt +
# y
b
N(a, t) dt,
and note that
dv = ∂v
∂x dx + ∂v
∂y dy
= ∂
∂x
# x
a
M(t, y) dt

dx +
# x
a
∂M
∂y (t, y)



∂N/∂t
dt + ∂
∂y
# y
b
N(a, t) dt

dy
= M(x, y) dx +

N(t, y)



t=x
t=a + N(a, y)




=N(x,y)
dy,
and v(x, y) indeed satisﬁes dv = M dx + N dy. It follows that (see Problem
23.1)
Theorem 23.2.2. A necessary and suﬃcient condition for M dx + N dy to
be exact is ∂M/∂y = ∂N/∂x, in which case
v(x, y) ≡
# x
a
M(t, y) dt +
# y
b
N(a, t) dt
is the function such that dv = M dx + N dy.
Not very many FODEs are exact. However, there are many that can be
turned into exact FODEs by multiplication by a suitable function. Such a
function, if it exists, is called an integrating factor. Thus, if the diﬀerential
integrating factor
M(x, y) dx + N(x, y) dy is not exact, but
μ(x, y)M(x, y) dx + μ(x, y)N(x, y) dy = dv,
then μ(x, y) is an integrating factor for the diﬀerential equation
M(x, y) dx + N(x, y) dy = 0
whose solution is then v(x, y) = C. Integrating factors are not unique, as the
following example illustrates.

23.2 Integrating Factors
555
Example 23.2.3. The diﬀerential x dy −y dx is not exact. Let us see if we can
illustration of
nonuniqueness of
integrating factor
ﬁnd a function μ(x, y) such that dv = μx dy −μy dx, for some v(x, y). We assume
that the domain D of the xy-plane in which v is deﬁned is contractable to a point.
Then a necessary and suﬃcient condition for the equation above to hold is
∂
∂x(μx) = ∂
∂y (−μy) ⇒x∂μ
∂x + y ∂μ
∂y + 2μ = 0.
(23.7)
(a) Let us assume that μ is a function of x only. Then Equation (23.7) reduces to
x dμ/dx = 2μ or μ = C/x2 where x ̸= 0. In this case we get
dv = C
 1
x dy −y
x2 dx

= C d
 y
x

where
x ̸= 0.
Thus, as long as x ̸= 0, any function C/x2, with arbitrary C, is an integrating factor
for x dy −y dx = 0. This integrating factor leads to the solution
v = Cy
x
= constant.
(23.8)
In order to determine the constant, suppose that y = m when x = 1. Then (23.8)
determines the constant in terms of m:
Cm
1
= constant ⇒constant = Cm.
So, (23.8) becomes
Cy
x = Cm ⇒y = mx.
(b) Now let us assume that μ is a function of y only. This leads to the integrating
factor μ = C/y2 where y ̸= 0. In this case v = Cx/y is the integral of the DE, and a
general solution is of the form Cx/y = constant. If we further impose the condition
y(1) = m, we get C/m = constant. Equation (23.8) then yields
Cx
y
= C
m ⇒y = mx
as in (a).
(c) The reader may verify that
μ =
C
x2 + y2
where
(x, y) ̸= (0, 0)
is also an integrating factor leading to the integral
v = tan−1  y
x

= constant
⇒
y
x = tan(constant) ≡C′.
Imposing y(1) = m gives C′ = m, so that y = mx as before.
■
The example above is a special case of the general fact that if a diﬀerential
has one integrating factor, then it has an inﬁnite number of them. Suppose
proof of
nonuniqueness of
integrating factor
that ν(x, y) is an integrating factor of M dx + N dy, i.e., νM dx + νN dy is
an exact diﬀerential, say du. Take any diﬀerentiable function F(u). Then
μ(x, y) ≡ν(x, y)F ′(u) is also an integrating factor. In fact,
μ(M dx + N dy) = νF ′(M dx + N dy) = dF
du (νM dx + νN dy)



=du
= dF.

556
First-Order Diﬀerential Equations
23.3
First-Order Linear Diﬀerential Equations
A linear DE is a sum of terms each of which is the product of a derivative
of the dependent variable (say y) and a function of the independent variable
(say x). The highest order of the derivative is called the order of the linear
order of a linear
DE
DE. The most general ﬁrst-order linear diﬀerential equation (FOLDE) is
p1(x)y′ + p0(x)y = q(x)
⇔
p1dy + (p0y −q) dx = 0.
(23.9)
If this equation is to have a solution, then by the argument at the end of the
last subsection, it must have at least one integrating factor. Let μ(x, y) be an
integrating factor. Then there exists v(x, y) such that
dv = μ(p0y −q) dx + μp1dy = 0
The necessary and suﬃcient condition for this to hold is
∂
∂y[μ(p0y −q)] = ∂
∂x(μp1).
To simplify the problem, let us assume that μ is a function of x only (we are
looking for any integrating factor, not the most general one). Then the above
condition leads to the diﬀerential equation
μp0 = d
dx(μp1) = p1
dμ
dx + μdp1
dx
(23.10)
or
p1
dμ
dx = μ

p0 −dp1
dx

⇒dμ
μ = p0
p1
dx −dp1
p1
.
Integrating both sides gives
ln μ =
# p0
p1
dx −ln p1 + ln C ⇒ln
μp1
C

=
# p0
p1
dx
or
μp1
C
= e

p0dx/p1 ⇒μ = Ce

p0dx/p1
p1
.
Neglecting the unimportant constant of integration, we have found the in-
tegrating factor μ = exp[(

p0 dx/p1)]/p1. Now multiply both sides of the
original equation by μ to obtain
μp1y′ + μp0y = μq.
(23.11)
With the identity μp1y′ ≡(μp1y)′ −(μp1)′y and the fact that (μp1)′ = μp0
[the ﬁrst equality of Equation (23.10)], Equation (23.11) becomes
d
dx(μp1y) = μq ⇒μp1y =
#
μ(x)q(x) dx + C.

23.3 First-Order Linear Diﬀerential Equations
557
Therefore,
explicit solution of
a general
ﬁrst-order linear
diﬀerential
equation
Theorem 23.3.1. Any FOLDE of the form p1(x)y′+p0(x)y = q(x), in which
p0, p1, and q are continuous functions in some interval (a, b), has a general
solution
y = f(x) =
1
μ(x)p1(x)

C +
#
μ(x)q(x) dx

,
(23.12)
where C is an arbitrary constant, and
μ(x) =
1
p1(x) exp
# p0(x)
p1(x) dx

.
(23.13)
Example 23.3.2. In an electric circuit with a resistance R and a capacitance C,
detailed treatment
of an RC circuit
Kirchhoﬀ’s law gives rise to the equation R dQ/dt + Q/C = V (t), where V (t) is the
time-dependent voltage and Q is the (instantaneous) charge on the capacitor. This
is a simple FOLDE with p1 = R, p0 = 1/C, and q = V . The integrating factor is
μ(t) = 1
R exp
#
1
RC dt

= 1
Ret/RC,
which yields
Q(t) =
1
μ(t)p1(t)

B + 1
R
#
et/RCV (t) dt

= Be−t/RC + e−t/RC
R
#
et/RCV (t) dt.
Recall that an indeﬁnite integral can be written as a deﬁnite integral whose upper
limit is the independent variable—in which case we need to use a diﬀerent symbol
for the integration variable. For the arbitrary lower limit, choose zero. We then
have
Q(t) = Be−t/RC + e−t/RC
R
# t
0
es/RCV (s) ds.
(23.14)
Let Q(0) ≡Q0 be the initial charge. Then, substituting t = 0 in (23.14), we get
Q0 = B and the charge at time t will be given by
Q(t) = Q0e−t/RC + e−t/RC
R
# t
0
es/RCV (s) ds.
(23.15)
As a speciﬁc example, assume that the voltage is a constant V0, as in the case
of a battery. Then the charge on the capacitor as a function of time will be
Q(t) = Q0e−t/RC + V0C(1 −e−t/RC).
It is interesting to note that the ﬁnal charge Q(∞) is V0C, independent of the initial
charge. Intuitively, this is what we expect, of course, as the “capacity” of a capacitor
to hold electric charge should not depend on its initial charge.
■

558
First-Order Diﬀerential Equations
Example 23.3.3. As a concrete illustration of the general formula derived in the
previous example, we ﬁnd the charge on a capacitor in an RC circuit when a voltage,
V (t) = V0 cos ωt, is applied to it for a period T and then removed. V (t) can thus
be written as
V (t) =
0
V0 cos ωt
if
t < T,
0
if
t > T.
The general solution is given as Equation (23.15). We have to distinguish between
two regions in time, t < T and t > T.
(a) For t < T, we have (using a table of integrals)
Q(t) = Q0e−t/RC + e−t/RC
R
# t
0
es/RCV0 cos ωs ds
= Q0e−t/RC + V0
R
1
(1/RC)2 + ω2

−1
RC e−t/RC + cos ωt
RC
+ ω sin ωt

If T ≫RC, and we wait long enough,3 i.e., t ≫RC, then only the oscillatory part
survives due to the large negative exponents of the exponentials. Thus,
Q(t) ≈V0
R
1
(1/RC)2 + ω2
 cos ωt
RC
+ ω sin ωt

.
The charge Q(t) oscillates with the same frequency as the driving voltage.
(b) For t > T, the integral goes up to T beyond which V (t) is zero. Hence, we have
Q(t) = Q0e−t/RC+ e−t/RC
R
# T
0
es/RCV0 cos ωs ds
= Q0e−t/RC+
V0/R
(1/RC)2 + ω2

−e−t/RC
RC
+ e(T −t)/RC
cos ωT
RC
+ ω sin ωT

.
We note that the oscillation has stopped (sine and cosine terms are merely constants
now), and for t −T ≫RC, the charge on the capacitor becomes negligibly small: If
there is no applied voltage, the capacitor will discharge.
■
Although ﬁrst-order linear DEs can always be solved—yielding solutions as
given in Equation (23.12)—no general rule can be applied to solve a general
FODE. Nevertheless, it can be shown that a solution of such a DE always
exist, and, under some mild conditions, this solution is unique. Some special
nonlinear FODEs can be solved using certain techniques some of which are
described in the following examples as well as the problems at the end of the
chapter.
Example 23.3.4. In Problem 23.11 you are asked to ﬁnd the velocity of a falling
falling object with
air resistance
object when the air drag is proportional to velocity. This is a good approximation
at low velocities for small objects; at higher speeds, and for larger objects, the drag
force becomes proportional to higher powers of speed. Let us consider the case when
the drag force is proportional to v2. Then the second law of motion becomes
mdv
dt = mg −bv2 ⇒dv
dt = g −γv2,
γ ≡b
m.
3Of course, we still assume that t < T.

23.3 First-Order Linear Diﬀerential Equations
559
This equation can be written as
dv
g −γv2 = dt ⇒
dv
A2 −v2 = γ dt,
A2 = g
γ .
(23.16)
Now we rewrite
1
A2 −v2 =
1
2A

1
v + A −
1
v −A

,
multiply both sides of Equation (23.16) by 2A and integrate to obtain
ln |v + A| −ln |v −A| = 2Aγt + ln C,
where we have written the constant of integration as ln C for convenience. This
equation can be rewritten as




v + A
v −A




 = Ce2Aγt.
Suppose that at t = 0, the velocity of the falling object is v0, then




v0 + A
v0 −A




 = C
and




v + A
v −A




 =




v0 + A
v0 −A




 e2Aγt.
Now note that A > 0, and v > 0 (if we take “down” to be the positive direction).
Therefore, the last equation becomes
v + A
|v −A| = v0 + A
|v0 −A|e2Aγt.
Suppose that v0 > A; then we can remove the absolute value sign from the RHS,
and since the two sides must agree at t = 0, we can remove the absolute value sign
on the LHS as well. Similarly, if v0 < A, then v < A as well. It follows that
v + A
v −A = v0 + A
v0 −Ae2Aγt ⇒(v + A)(v0 −A) = (v −A)(v0 + A)e2Aγt.
Solving for v gives
v = A (v0 + A)e2Aγt + v0 −A
(v0 + A)e2Aγt −(v0 −A)
= Av0(e2Aγt + 1) + A(e2Aγt −1)
v0(e2Aγt −1) + A(e2Aγt + 1)
(23.17)
= Av0 cosh(Aγt) + A sinh(Aγt)
v0 sinh(Aγt) + A cosh(Aγt).
It follows from Equation (23.17) that at t = 0, the velocity is v0, as we expect. It
also shows that, when t →∞, the velocity approaches A =
	
g/γ, the so-called
terminal velocity. This is the velocity at which the gravitational force and the
terminal velocity
drag force become equal, causing the acceleration of the object to be zero. The
terminal velocity can thus be obtained directly from the second law without solving
the diﬀerential equation.

560
First-Order Diﬀerential Equations
Figure 23.1: The achievement of terminal velocity for a drag force that is proportional
to the square of speed (the heavy curve) is considerably faster than for a drag force that
is linear in speed (the light curve) if γ has the same numerical value for both cases.
Figure 23.1 shows the plot of speed as a function of time for the two cases of the
drag force being proportional to v and v2 with the same proportionality constant.
Because of the higher power of speed, the terminal velocity is achieved considerably
more quickly for v2 force than for v force. Furthermore, as the ﬁgure shows clearly,
the terminal speed itself is much smaller in the former case. Since larger surfaces
provide a v2 drag force, parachutes that have very large surface are desirable.
■
Example 23.3.5. We consider here some other examples of (nonlinear) FODEs
whose solutions are available:
(a) Bernoulli’s FODE: This equation is of the form y′ + p(x)y + q(x)yn = 0 where
Bernoulli’s FODE
n ̸= 1. This DE can be simpliﬁed if we substitute y = ur and choose r appropriately.
In terms of u, the DE becomes
u′ + p(x)
r
u + q(x)
r
unr−r+1 = 0.
The simplest DE—whose solution could be found by a simple integration—would
be obtained if the exponent of the last term could be set equal to 1. But this would
require r to be zero, which is not acceptable. The next simplest DE results if we set
the exponent equal to zero, i.e., if r = 1/(1 −n). Then the DE becomes
u′ + (1 −n)p(x)u + (1 −n)q(x) = 0
which is a ﬁrst-order linear DE whose solution we have already found.
(b) Homogeneous FODE: This DE is of the form
homogeneous
FODE
dy
dx = w
 y
x

.
To ﬁnd the solution, make the obvious substitution u = y/x, to obtain y′ = u + xu′
and
u + xu′ = w(u) ⇒u′ = w(u) −u
x
⇒
du
w(u) −u = dx
x

23.4 Problems
561
with the solution
ln x =
# u
c
dt
w(t) −t
or
x = exp
A# y/x
c
dt
w(t) −t
B
,
where c is an arbitrary constant to be determined by the initial conditions.
■
23.4
Problems
23.1. Suppose that region D is contractible to zero. Using the equivalence of
the vanishing of curl and vanishing of closed line integrals, show that ∂M/∂y =
∂N/∂x is both necessary and suﬃcient condition for M dx+N dy to be exact.
23.2. Verify that μ = C/(x2 +y2) is an integrating factor of x dy−y dx which
gives rise to
v = tan−1 y
x

= constant
⇒
y
x = C′
for a solution of x dy −y dx.
23.3. Find the general solution of Bernoulli’s FODE
y′ + p(x)y + q(x)yn = 0
where
n ̸= 1.
Hint: See Example 23.3.5.
23.4. Find a solution to the linear fractional DE
dy
dx = a1x + a2y
b1x + b2y
where
a1b2 ̸= a2b1.
Hint: Divide the numerator and denominator by x to obtain a homogeneous
FODE.
23.5. Lagrange’s FODE is y −xp(y′) −q(y′) = 0.
Lagrange’s FODE
(a) Let y′ = t and consider x as a function of t. Using the chain rule, ﬁnd
dx/dt in terms of dy/dt.
(b) Diﬀerentiate Lagrange’s DE with respect to t.
Use the result of this
diﬀerentiation and that of (a) to arrive at [t −p(t)] ˙x −˙px = ˙q, where the dot
indicates diﬀerentiation with respect to t.
(c) Find the (parametric) solution of the DE, considering two separate cases:
t = p(t) and t ̸= p(t).
23.6. Let u(x, y) = C be a solution of the DE M dx + N dy = 0. Show that:
(a) (∂u/∂x)/M = (∂u/∂y)/N; and
(b) μ(x, y) ≡(∂u/∂x)/M is an integrating factor for the DE.
23.7. Use direct diﬀerentiation to show that the function given in Equation
(23.12) solves the FOLDE of Equation (23.9).

562
First-Order Diﬀerential Equations
23.8. Analyze the capacitor’s charge in an RC circuit in which a constant
potential V0 is applied for a time T > 0 and then disconnected. Consider the
cases where t < T and t > T .
23.9. Find all functions f(x) whose deﬁnite integral from 0 to x equals the
square of their reciprocal.
23.10. (a) Let p1u′+p0u = 0 be a homogeneous FOLDE in u. Solve it. (Note
that it can easily be integrated.)
(b) Consider p1y′ + p0y = q. Let y = uv, where u is as in (a), and obtain
an equation for v.
Solve this equation, and obtain a general solution for
p1y′ +p0y = q. This is the method of variation of parameters, which can
also be used for second-order diﬀerential equations.
23.11. A falling body in air has a motion approximately described by the DE
m dv/dt = mg −bv, where v = dx/dt is the velocity of the body. Find this
velocity as a function of time assuming that the object starts from rest.
23.12. Suppose that both the linear (av) and the quadratic (bv2) terms are
present in the fall of an object with air drag.
(a) Solve the DE and ﬁnd the most general solution for the velocity as a
function of time. Hint: Make the substitution u = v + a/2b.
(b) From this general solution, extract the solutions to the cases where only
the linear and only the quadratic terms are present by taking the limits b →0
and a →0.
23.13. Take the limit of Equation (23.17) as t →∞and show that it is equal
to
	
g/γ.

Chapter 24
Second-Order Linear
Diﬀerential Equations
The majority of problems encountered in physics lead to second order linear
diﬀerential equations (SOLDEs) when the so-called nonlinear terms are ap-
proximated out. Thus, a general treatment of the properties and methods
of obtaining solutions to SOLDEs is essential. In this section, we investigate
their general properties, and leave methods of obtaining their solutions for
the next section and later chapters.
The most general SOLDE is
p2(x)d2y
dx2 + p1(x)dy
dx + p0(x)y = p3(x).
(24.1)
Dividing by p2(x), and writing p for p1/p2, q for p0/p2, and r for p3/p2,
reduces Equation (24.1) to the normal form,
normal form of a
SOLDE
d2y
dx2 + p(x)dy
dx + q(x)y = r(x).
(24.2)
Equation (24.2) is equivalent to (24.1) if p2(x) ̸= 0. The points at which p2(x)
vanishes are called the singular points of the DE.
diﬀerence between
singular points of
linear and
nonlinear
diﬀerential
equations
There is a crucial diﬀerence between the singular points of linear DEs and
those of nonlinear DEs. For a nonlinear DE such as (x2 −y)y′ = x2 + y2,
the curve y = x2 is the collection of singular points. This makes it impossible
to construct solutions y = f(x) that are deﬁned on an interval I = [a, b] of
the x-axis because for any a < x < b, there is a y = x2 for which the DE is
undeﬁned. On the other hand, linear DEs do not have this problem because
the coeﬃcients of the derivatives are functions of x only. Therefore, all the
singular “curves” are vertical, and we can ﬁnd intervals on the x-axis in which
the DE is well behaved.

564
Second-Order Linear Diﬀerential Equations
24.1
Linearity, Superposition, and Uniqueness
The FOLDE has only one solution; and we found this solution in closed form
in Equation (23.12). The SOLDE may have (in fact, it does) more than one
solution. Therefore, it is important to know how many solutions to expect for
a SOLDE and what relation (if any) exists between these solutions.
We write Equation (24.1) as
L[y] = p3
where
L ≡p2
d2
dx2 + p1
d
dx + p0.
(24.3)
It is clear that L is a linear operator1 by which we mean that for constants
α and β, L[αy1 + βy2] = αL[y1] + βL[y2]. In particular, if y1 and y2 are two
solutions of Equation (24.3), then
L[y1 −y2] = L[y1] −L[y2] = p3 −p3 = 0.
That is, the diﬀerence between any two solutions of a SOLDE is a solution2
of the homogeneous equation obtained by setting p3 = 0. An immediate
homogeneous
SOLDE
consequence of the linearity of L is that any linear combination of solutions
of the homogeneous SOLDE (HSOLDE) is also a solution. This is called the
superposition principle.
superposition
principle
We saw in the introduction to Chapter 22 that, based on physical intu-
ition, we expect to be able to predict the behavior of a physical system if we
know the DE obeyed by that system and equally importantly, the initial data.
Physical intuition also tells us that if the initial conditions are changed by an
inﬁnitesimal amount, then the solutions will be changed inﬁnitesimally. Thus,
the solutions of linear DEs are said to be continuous functions of the initial
conditions. Nonlinear DEs can have completely diﬀerent solutions for two
initial conditions that are inﬁnitesimally close. Since initial conditions cannot
be speciﬁed with mathematical precision in practice, nonlinear DEs lead to
unpredictable solutions, or chaos. This subject has received much attention
in recent years, and we shall present a brief discussion of chaos in Chapter 31.
By its very nature, a prediction is expected to be unique. This expectation
for linear equations becomes—in the language of mathematics—an existence
and a uniqueness theorem. First, we need the following3
Theorem 24.1.1. The only solution g(x) of the homogeneous equation y′′ +
py′ + qy = 0, deﬁned on the interval [a, b], which satisﬁes g(a) = 0 = g′(a), is
the trivial solution, g = 0.
Let f1 and f2 be two solutions of (24.2) satisfying the same initial condi-
tions on the interval [a, b]. This means that f1(a) = f2(a) = c and f ′
1(a) =
1Recall from Chapter 7 that an operator is a correspondence on a vector space that
takes one vector and gives another. A linear operator is an operator that satisﬁes Equation
(7.3). The vector space on which L acts is the vector space of diﬀerentiable functions.
2This conclusion is not limited to the SOLDE; it holds for all linear DEs.
3For a proof, see Hassani, S. Mathematical Physics: A Modern Introduction to Its Foun-
dations, Springer-Verlag, 1999, p. 354.

24.1 Linearity, Superposition, and Uniqueness
565
f ′
2(a) = c′ for some given constants c and c′. Then it is readily seen that their
diﬀerence, g ≡f1 −f2, satisﬁes the homogeneous equation [with r(x) = 0].
The initial condition that g(x) satisﬁes is clearly g(a) = 0 = g′(a). By Theo-
rem 24.1.1, g = 0 or f1 = f2. We have just shown
uniqueness of
solutions to
SOLDE
Theorem 24.1.2. (Uniqueness Theorem). If p and q are continuous on
[a, b], then at most one solution of Equation (24.2) can satisfy a given set of
initial conditions.
The uniqueness theorem can be applied to any homogeneous SOLDE to
ﬁnd the latter’s most general solution. In particular, let f1(x) and f2(x) be
any two solutions of
y′′ + p(x)y′ + q(x)y = 0
(24.4)
deﬁned on the interval [a, b]. Assume that the two vectors v1 = (f1(a), f ′
1(a))
and v2 = (f2(a), f ′
2(a)) are linearly independent.4 Let g(x) be another so-
lution. The vector (g(a), g′(a)) can be written as a linear combination of v1
and v2, giving the two equations
g(a) = c1f1(a) + c2f2(a),
g′(a) = c1f ′
1(a) + c2f ′
2(a).
The function u(x) ≡g(x) −c1f1(x) −c2f2(x) satisﬁes the DE (24.4) and
the initial conditions u(a) = u′(a) = 0. It follows from Theorem 24.1.1 that
u(x) = 0 or g(x) = c1f1(x) + c2f2(x). We have proved
basis of solutions
Theorem 24.1.3. Let f1 and f2 be two solutions of the HSOLDE
y′′ + py′ + qy = 0,
where p and q are continuous functions deﬁned on the interval [a, b].
If
(f1(a), f ′
1(a)) and (f2(a), f ′
2(a)) are linearly independent vectors, then every
solution g(x) of this HSOLDE is equal to some linear combination
g(x) = c1f1(x) + c2f2(x),
with constant coeﬃcients c1 and c2. The functions f1 and f2 are called a
basis of solutions of the HSOLDE.
The uniqueness theorem states that only one solution can exist for a
there is also an
existence theorem!
SOLDE which satisﬁes a given set of initial conditions.
Whether such a
solution does exist is beyond the scope of the theorem.
Under some mild
assumptions, however, it can be shown that a solution does indeed exist. We
shall not prove this existence theorem for a general SOLDE, but shall examine
various techniques of obtaining solutions for speciﬁc SOLDEs in this and the
next two chapters.
4If they are not, then one must choose a diﬀerent initial point for the interval.

566
Second-Order Linear Diﬀerential Equations
24.2
The Wronskian
To form a basis of solutions, f1 and f2 must be linearly independent. It is
important to note that the linear dependence or independence of a number of
functions, deﬁned on the interval [a, b], is a concept that must hold for all x
in [a, b]. Thus, if
α1f1(x0) + α2f2(x0) + · · · + αnfn(x0) = 0
for some x0 ∈[a, b], it does not mean that the f’s are linearly dependent.
Linear dependence requires that the equality holds for all x in [a, b].
The nature of the linear relation between f1 and f2 can be determined by
their Wronskian.
Wronskian deﬁned
Deﬁnition 24.2.1. The Wronskian of any two diﬀerentiable functions f1(x)
and f2(x) is deﬁned to be
W(f1, f2; x) = f1(x)f ′
2(x) −f2(x)f ′
1(x) = det
⎛
⎝
f1(x)
f ′
1(x)
f2(x)
f ′
2(x)
⎞
⎠.
If we diﬀerentiate both sides of the deﬁnition of Wronskian and substitute
from Equation (24.4), we obtain
d
dxW(f1, f2; x) = f ′
1f ′
2 + f1f ′′
2 −f ′
2f ′
1 −f2f ′′
1
= f1(−pf ′
2 −qf2) −f2(−pf ′
1 −qf1)
= pf ′
1f2 −pf1f ′
2 = −p(x)W(f1, f2; x).
We can easily ﬁnd a solution to this DE:
dW
dx = −pW ⇒dW
W = −p dx ⇒ln W = −
# x
c
p(t) dt + ln C,
where c is an arbitrary point in the interval [a, b] and C is the constant of
integration. In fact, it is readily seen that C = W(c). We therefore have
W(f1, f2; x) = W(f1, f2; c)e−
 x
c p(t) dt.
(24.5)
Note that W(f1, f2; x) = 0 if and only if W(f1, f2; c) = 0, and that [because
the exponential in (24.5) is positive] W(f1, f2; x) and W(f1, f2; c) have the
same sign if they are not zero. This observation leads to
Box 24.2.1. The Wronskian of any two solutions of Equation (24.4)
does not change sign in the interval [a, b]. In particular, if the Wronskian
vanishes at one point in [a, b], it vanishes at all points in [a, b].

24.3 A Second Solution to the HSOLDE
567
Let f1 and f2 be any two diﬀerentiable functions that are not necessarily
solutions of any DE. If f1 and f2 are linearly dependent, then one is a multiple
of the other, and the Wronskian is readily seen to vanish. Conversely, assume
that the Wronskian is zero. Then f1(x)f ′
2(x) −f2(x)f ′
1(x) = 0. This gives
f1df2 = f2df1 ⇒df2
f2
= df1
f1
⇒ln f2 = ln f1 + ln C ⇒f2 = Cf1
and the two functions are linearly dependent. We have just shown that
diﬀerentiability is
important in the
statement of Box
24.2.2.
Box 24.2.2. Two diﬀerentiable functions, which are nonzero in the inter-
val [a, b], are linearly dependent if and only if their Wronskian vanishes.
Example 24.2.1. Let f1(x) = x and f2(x) = |x| for −1 ≤x ≤1. These two
functions are linearly independent in the given interval, because α1x + α2|x| = 0 for
all x if and only if α1 = α2 = 0. The Wronskian, on the other hand, vanishes for
all −1 ≤x ≤1:
W (f1, f2; x) = xd|x|
dx −|x|dx
dx = xd|x|
dx −|x|
= x d
dx
0
x
if x ≥0
−x
if x ≤0 −
0
x
if x ≥0
−x
if x ≤0
=
0
x −x = 0
if x > 0,
−x −(−x) = 0
if x < 0.
This seems to be in contradiction to Box 24.2.2. It is not! Box 24.2.2 assumes that
both functions are diﬀerentiable in their common interval of deﬁnition. However,
|x| is not diﬀerentiable at x = 0.
■
24.3
A Second Solution to the HSOLDE
If we know one solution to Equation (24.4), we can use the Wronskian to
obtain a second linearly independent solution. Let W(x) ≡W(f1, f2; x) be the
Wronskian of the two solutions f1 and f2. Then, by deﬁnition and Equation
(24.5), we have
f1(x)f ′
2(x) −f2(x)f ′
1(x) = W(x) = W(c)e−
 x
c p(t) dt,
where c is an arbitrary point in the interval of interest. Given f1(x), this is a
FOLDE in f2(x), which can be solved by the method of Subsection 23.3. In
fact, 1/f 2
1(x) is an integrating factor, and dividing both sides by f 2
1 (x) gives
d
dx
f2(x)
f1(x)

= W(x)
f 2
1(x)

568
Second-Order Linear Diﬀerential Equations
or
a second linearly
independent
solution can be
found from a
given solution
f2(x)
f1(x) = C +
# x
α
W(s)
f 2
1(s) ds = C +
# x
α
1
f 2
1 (s)W(c) exp

−
# s
c
p(t) dt

ds,
where C is an arbitrary constant of integration and α is a convenient point in
the interval [a, b]. Thus,
second solution of
the HSOLDE
obtained from the
ﬁrst
f2(x) = f1(x)
(
C + K
# x
α
1
f 2
1 (s) exp

−
# s
c
p(t) dt

ds
)
,
(24.6)
where we substituted K for W(c).
We do not have to know W(x) (this
would require knowledge of f2, which we are trying to calculate!) to obtain
K = W(c). In fact, it is a good exercise for the reader to show that f2, as given
by (24.6), indeed satisﬁes Equation (24.4) no matter what K is. Note also
that f2(α) = Cf1(α). Whenever possible—and convenient—it is customary
to set C = 0 because its presence simply gives a term that is proportional to
the known solution f1(x).
Example 24.3.1. (a) A solution to the SOLDE y′′ −k2y = 0 is ekx. To ﬁnd a
second solution, we let C = 0 and K = 1 in Equation (24.6). Since p(x) = 0, we
have
f2(x) = ekx

0 +
# x
α
ds
e2ks

= −1
2k e−kx + e−2kα
2k
ekx
which, ignoring the second term which is proportional to the ﬁrst solution, leads
directly to the choice of e−kx as a second solution.
(b) The diﬀerential equation y′′ + k2y = 0, which arises in mechanics in the study
of the motion of a mass attached to the end of a spring, has sin kx as a solution.
With C = 0, c = α = π/2k, and K = 1, we get
f2(x) = sin kx

0 +
# x
π/2k
ds
sin2 ks

= −sin kx cot ks|x
π/2k = −cos kx.
Thus, sin kx and cos kx form a basis of solution, and a general solution is of the
form
y(x) = A cos kx + B sin kx,
a result that should be familiar to the reader from introductory physics.
(c) For the solutions in part (a),
W (x) = det

ekx
kekx
e−kx
−ke−kx

= −2k
and for those in part (b),
W (x) = det
sin kx
k cos kx
cos kx
−k sin kx

= −k.
Both Wronskians are constant. This is a special case of a result that holds for all
DEs of the form y′′ + q(x)y = 0.
■

24.4 The General Solution to an ISOLDE
569
Most special functions used in mathematical physics are solutions of SOL-
DEs. The behavior of these functions at certain special points is determined by
the physics of the particular problem. In most situations physical expectation
leads to a preference for one particular solution over the other. For example,
although there are two linearly independent solutions to the Legendre DE,
Legendre
diﬀerential
equation
d
dx

(1 −x2)dy
dx

+ n(n + 1)y = 0,
the solution that is most frequently encountered is a Legendre polynomial
Pn(x) discussed in Chapter 26. The other solution can be obtained by using
Equation (24.6).
24.4
The General Solution to an ISOLDE
We now determine the most general solution of an inhomogeneous SOLDE
(ISOLDE). Let g(x) be a particular solution of
L[y] = y′′ + py′ + qy = r(x)
(24.7)
and let h(x) be any other solution of this equation. Then h(x)−g(x) satisﬁes
Equation (24.4) and, by Theorem 24.1.3, can be written as a linear combina-
tion of a basis of solutions f1(x) and f2(x). It follows that
h(x) = c1f1(x) + c2f2(x) + g(x).
(24.8)
Box 24.4.1. If we have a particular solution of the ISOLDE of Equation
(24.7) and two basis solutions of the HSOLDE, then the most general
solution of (24.7) can be expressed as the sum of a linear combination of
the two basis solutions and the particular solution.
We know how to ﬁnd a second solution to the HSOLDE once we know one
solution. We now show that knowing one such solution will also allow us to
ﬁnd a particular solution to the ISOLDE. The method we use is called the
method of variation of constants.
method of
variation of
constants
Let f1 and f2 be the two (known) solutions of the HSOLDE and g(x) the
sought-after solution to Equation (24.7). Write g as g(x) = f1(x)v(x) with
v a function to be determined. Substitute this in (24.7) to get a SOLDE for
v(x):
with a solution of
HSOLDE at our
disposal, we can
ﬁnd a particular
solution of an
ISOLDE.
v′′ +

p + 2f ′
1
f1

v′ = r
f1
.
This is a ﬁrst-order linear DE in v′ which has a solution of the form (see
Problem 24.6)
v′ = W(x)
f 2
1(x)

C +
# x
a
f1(t)r(t)
W(t)
dt

,

570
Second-Order Linear Diﬀerential Equations
where W(x) is the (known) Wronskian of Equation (24.7). Substituting
W(x)
f 2
1(x) = f1(x)f ′
2(x) −f2(x)f ′
1(x)
f 2
1(x)
= d
dx
f2
f1

in the above expression for v′ and setting C = 0 (we are interested in a
particular solution), we get
dv
dx = d
dx
f2
f1
 # x
a
f1(t)r(t)
W(t)
dt
= d
dx
f2(x)
f1(x)
# x
a
f1(t)r(t)
W(t)
dt

−f2(x)
f1(x)
d
dx
# x
a
f1(t)r(t)
W(t)
dt



=f1(x)r(x)/W(x)
and, by integration,
v(x) = f2(x)
f1(x)
# x
a
f1(t)r(t)
W(t)
dt −
# x
a
f2(t)r(t)
W(t)
dt,
where in the last integral, we used t as the variable of integration. This leads
to the particular solution
g(x) = f1(x)v(x) = f2(x)
# x
a
f1(t)r(t)
W(t)
dt −f1(x)
# x
a
f2(t)r(t)
W(t)
dt.
(24.9)
Note how symmetric f1 and f2 appear in the ﬁnal result.
It thus follows that
Box 24.4.2. Given a single solution f1(x) of the homogeneous equation
corresponding to an ISOLDE, one can use Equation (24.6) to ﬁnd a second
solution f2(x) of the homogeneous equation and Equation (24.9) to ﬁnd a
particular solution g(x). The most general solution h, will then be
h(x) = c1f1(x) + c2f2(x) + g(x).
24.5
Sturm–Liouville Theory
We saw in Chapter 22 that the separation of PDEs normally results in ex-
pressions of the form
L[u] + λu = 0,
or
p2(x)d2u
dx2 + p1(x)du
dx + p0(x)u + λu = 0,
(24.10)
where u is a function of a single variable and λ is, a priori, an arbitrary
constant. This is an eigenvalue equation for the operator L just as Equation
(7.17) was an eigenvalue equation for the matrix T. In this section, we try
to learn some properties of this eigenvalue problem, but ﬁrst we need to
understand the concept of the adjoint of a diﬀerential operator.

24.5 Sturm–Liouville Theory
571
24.5.1
Adjoint Diﬀerential Operators
In our discussion of the eigenvalues and eigenvectors of matrices in Section 7.4,
symmetric matrices seemed to be special (see Theorem 7.4.1). The analog of a
symmetric matrix in the case of diﬀerential operators (DO) is a self-adjoint
diﬀerential operator.
The HSOLDE
L[y] ≡p2(x)y′′ + p1(x)y′ + p0(x)y = 0
(24.11)
is said to be exact if it can be written as
exact SOLDE
L[y] = d
dx[A(x)y′ + B(x)y].
(24.12)
An integrating factor for L[y] is a function μ(x) such that μ(x)L[y] is exact.
integrating factor
for SOLDE
If an integrating factor exists, then Equation (24.11) reduces to
d
dx[A(x)y′ + B(x)y] = 0 ⇒A(x)y′ + B(x)y = C,
a FOLDE with a constant inhomogeneous term whose solution is given in
Theorem 23.3.1. Even the ISOLDE corresponding to Equation (24.11) can be
solved, because
μ(x)L[y] = μ(x)r(x) ⇒
d
dx[A(x)y′ + B(x)y] = μ(x)r(x)
⇒A(x)y′ + B(x)y =
# x
α
μ(t)r(t) dt,
which is a general FOLDE. Thus, the existence of an integrating factor com-
pletely solves a SOLDE. It is therefore important to know whether or not a
SOLDE admits an integrating factor.
If the SOLDE is exact, then (24.12) must equal (24.11), implying that
p2 = A, p1 = A′ + B, and p0 = B′. It follows that p′′
2 = A′′, p′
1 = A′′ + B′,
and p0 = B′, which in turn give p′′
2 −p′
1+p0 = 0. Conversely if p′′
2 −p′
1+p0 = 0,
then, substituting p0 = −p′′
2 + p′
1 in the LHS of Equation (24.11), we obtain
p2y′′ + p1y′ + p0y = p2y′′ + p1y′ + (−p′′
2 + p′
1)y
= p2y′′ −p′′
2y + (p1y)′ = (p2y′ −p′
2y)′ + (p1y)′
= d
dx(p2y′ −p′
2y + p1y),
and the DE is exact. Therefore,
Box 24.5.1. The SOLDE of Equation (24.11) is exact if and only if
p′′
2 −p′
1 + p0 = 0.

572
Second-Order Linear Diﬀerential Equations
A general SOLDE is clearly not exact. Can we make it exact by multi-
plying it by an integrating factor as we did with a FOLDE? An immediate
consequence of Box 24.5.1 is
Box 24.5.2. A function μ is an integrating factor of the SOLDE of Equa-
tion (24.11) if and only if it is a solution of the HSOLDE
M[μ] ≡(p2μ)′′ −(p1μ)′ + p0μ = 0.
(24.13)
We can expand Equation (24.13) to obtain the equivalent equation
p2μ′′ + (2p′
2 −p1)μ′ + (p′′
2 −p′
1 + p0)μ = 0.
(24.14)
The operator M given by
adjoint of a
second-order linear
diﬀerential
operator
M ≡p2
d2
dx2 + (2p′
2 −p1) d
dx + (p′′
2 −p′
1 + p0)
(24.15)
is called the adjoint of the operator L and denoted by M ≡L†. This is the
equivalent of the transpose of a matrix Tt.
Box 24.5.2 conﬁrms the existence of an integrating factor. However, the
latter can be obtained only by solving Equation (24.14), which is at least as
diﬃcult as solving the original diﬀerential equation! In contrast, the integrat-
ing factor for a FOLDE can be obtained by a mere integration [see Equation
(23.13)].
Although integrating factors for SOLDEs are not as useful as their coun-
terparts for FOLDEs, they can facilitate the study of SOLDEs.
Let us
ﬁrst note that the adjoint of the adjoint of a diﬀerential operator is the
original operator: (L†)† = L (see Problem 24.10).
This suggests that if
v is an integrating factor of L[u], then u will be an integrating factor of
M[v] ≡L†[v]. In particular, multiplying the ﬁrst one by v and the second
one by u and subtracting the results, we obtain [see Equations (24.11) and
(24.13)] vL[u] −uM[v] = (vp2)u′′ −u(p2v)′′ + (vp1)u′ + u(p1v)′, which can be
simpliﬁed to
vL[u] −uM[v] = d
dx[p2vu′ −(p2v)′u + p1uv].
(24.16)
Integrating this from a to b yields
Lagrange
identities
# b
a
 
vL[u] −uM[v]
!
dx =
.
p2vu′ −(p2v)′u + p1uv
/

b
a.
(24.17)
Equations (24.16) and (24.17) are called the Lagrange identities.
As in the case of matrices, a self-adjoint diﬀerential operator (correspond-
ing to a symmetric matrix for which T = Tt) merits special consideration.

24.5 Sturm–Liouville Theory
573
For M[v] ≡L†[v] to be equal to L[v], we must have [see Equations (24.11) and
(24.14)] 2p′
2 −p1 = p1 and p′′
2 −p′
1 +p0 = p0. The ﬁrst equation gives p′
2 = p1,
which also solves the second equation. If this condition holds, then we can
write Equation (24.11) as L[y] = p2y′′ + p′
2y′ + p0y, or
L[y] = d
dx

p2(x)dy
dx

+ p0(x)y = 0.
Can we make all SOLDEs self-adjoint?
Let us multiply both sides of
Equation (24.11) by a function w(x), to be determined later. We get the new
DE
w(x)p2(x)y′′ + w(x)p1(x)y′ + w(x)p0(x)y = 0,
which we desire to be self-adjoint. This will be accomplished if we choose
w(x) such that wp1 = (wp2)′, or p2w′ + w(p′
2 −p1) = 0, which can be readily
integrated to give
w(x) = 1
p2
exp
# x p1(t)
p2(t) dt

.
We have just proved the following:
Theorem 24.5.1. The SOLDE of Equation (24.11) is self-adjoint if and only
all SOLDEs can
be made
self-adjoint
if p′
2 = p1, in which case the DE has the form
d
dx

p2(x)dy
dx

+ p0(x)y = 0.
If it is not self-adjoint, it can be made so by multiplying it through by
w(x) = 1
p2
exp
# x p1(t)
p2(t) dt

.
Example 24.5.2. (a) The Legendre equation in normal form,
y′′ −
2x
1 −x2 y′ +
λ
1 −x2 y = 0,
is not self-adjoint. However, we get a self-adjoint version if we multiply through by
w(x) = 1 −x2:
(1 −x2)y′′ −2xy′ + λy = 0,
or
[(1 −x2)y′]′ + λy = 0
(b) Similarly, the normal form of the Bessel equation
y′′ + 1
xy′ +

1 −n2
x2

y = 0
is not self-adjoint, but multiplying through by h(x) = x yields
d
dx

x dy
dx

+

x −n2
x

y = 0,
which is clearly self-adjoint.
■

574
Second-Order Linear Diﬀerential Equations
24.5.2
Sturm–Liouville System
Now that we know that every SOLDE can be made self-adjoint, let’s apply
the procedure to our starting DE (24.10). If we multiply that equation by the
w(x) of Theorem 24.5.1 it becomes self-adjoint, and can be written as
d
dx

p(x)du
dx

+ [λw(x) −q(x)]u = 0
or
L[u] = d
dx

p(x)du
dx

−q(x)u = −λw(x)u
(24.18)
with p(x) = w(x)p2(x) and q(x) = −p0(x)w(x).
Equation (24.18) is the
standard form of the Sturm-Liouville (S-L) equation.
The appearance of w is the result of our desire to render the diﬀerential
operator self-adjoint. It also appears in another context. Write the Lagrange
identity (24.16) for a self-adjoint diﬀerential operator L:
uL[v] −vL[u] = d
dx{p(x)[u(x)v′(x) −v(x)u′(x)]}.
(24.19)
If we specialize this identity to the S-L equation of (24.18) with u = u1
corresponding to the eigenvalue λ1 and v = u2 corresponding to the eigenvalue
λ2, we obtain for the LHS
u1L[u2] −u2L[u1] = u1(−λ2wu2) + u2(λ1wu1) = (λ1 −λ2)wu1u2.
Integrating both sides of (24.19) then yields
(λ1 −λ2)
# b
a
wu1u2dx = {p(x)[u1(x)u′
2(x) −u2(x)u′
1(x)]}b
a.
(24.20)
A desired property of the solutions of a self-adjoint DE is their orthogonality
when they belong to diﬀerent eigenvalues. This property will be satisﬁed if
we assume an inner product integral with weight function w(x), and if the
RHS of Equation (24.20) vanishes. There are various boundary conditions
(BC) that fulﬁll the latter requirement. One such boundary conditions are
separated boundary conditions:
α1u(a) + β1u′(a) = 0,
α2u(b) + β2u′(b) = 0,
(24.21)
where α1, α2, β1, and β2 are real constants. Another set of appropriate bound-
Sturm–Liouville
systems
ary conditions is the periodic BC given by
u(a) = u(b)
and
u′(a) = u′(b).
(24.22)
The collection of the DO and the boundary conditions is called a Sturm–
Liouville (S-L) system.

24.6 SOLDEs with Constant Coeﬃcients
575
Example 24.5.3. For ﬁxed ν the DE
d2u
dr2 + 1
r
du
dr +

k2 −ν2
r2

u = 0,
0 ≤r ≤b
(24.23)
transforms into the Bessel equation u′′ + u′/x + (1 −ν2/x2)u = 0 if we make
the substitution kr = x.
Thus, the solution of the S-L equation (24.23) that is
analytic at r = 0 and corresponds to the eigenvalue k2 is uk(r) = Jν(kr)—because
Bessel functions Jν(x) are entire functions. For two diﬀerent eigenvalues, k2
1 and k2
2,
the eigenfunctions are orthogonal if the boundary term of (24.20) corresponding to
Equation (24.23) vanishes, that is, if
{r[Jν(k1r)J′
ν(k2r) −Jν(k2r)J′
ν(k1r)]}b
0
vanishes, which will occur if and only if Jν(k1b)J′
ν(k2b)−Jν(k2b)J′
ν(k1b) = 0. A com-
mon choice is to take Jν(k1b) = 0 = Jν(k2b), that is, to take both k1b and k2b as (dif-
ferent) roots of the Bessel function of order ν. We thus have
 b
0 rJν(kir)Jν(kjr) dr =
0 if ki and kj are diﬀerent roots of Jν(kb) = 0.
The Legendre equation
d
dx

(1 −x2)du
dx

+ λu = 0,
where −1 < x < 1,
is already self-adjoint. Thus, w(x) = 1, and p(x) = 1 −x2. Solutions of this DE
corresponding to λ = n(n + 1) are the Legendre polynomials Pn(x). The bound-
ary term of (24.20) clearly vanishes at a = −1 and b = +1, and we obtain the
orthogonality relation:
 +1
−1 Pn(x)Pm(x) dx = 0 if m ̸= n.
The Hermite equation is
u′′ −2xu′ + λu = 0.
(24.24)
It is transformed into an S-L system if we multiply it by w(x) = e−x2. The resulting
S-L equation is
d
dx

e−x2 du
dx

+ λe−x2u = 0.
(24.25)
The function u is an eigenfunction of (24.25) corresponding to the eigenvalue λ if
and only if it is a solution of (24.24). Solutions of this DE corresponding to λ = 2n
are the Hermite polynomials Hn(x). The boundary term corresponding to the two
eigenfunctions u1(x) and u2(x) having the respective eigenvalues λ1 and λ2 ̸= λ1 is
{e−x2[u1(x)u′
2(x) −u2(x)u′
1(x)]}b
a.
This vanishes for arbitrary u1 and u2 if a = −∞and b = +∞. We can therefore
write
 +∞
−∞e−x2Hn(x)Hm(x) dx = 0 if m ̸= n.
■
24.6
SOLDEs with Constant Coeﬃcients
The SOLDEs with constant coeﬃcients occur frequently and their solutions
are easily accessible. In fact, we need not conﬁne ourselves to the second order
equations. The most general nth-order linear diﬀerential equation (NOLDE)
with constant coeﬃcients can be written as
L[y] ≡y(n) + an−1y(n−1) + · · · + a1y′ + a0y = r(x).
(24.26)
The corresponding homogeneous NOLDE (HNOLDE) is obtained by setting
r(x) = 0.

576
Second-Order Linear Diﬀerential Equations
24.6.1
The Homogeneous Case
The solution to the HNOLDE
L[y] ≡y(n) + an−1y(n−1) + · · · + a1y′ + a0y = 0
(24.27)
can be found by making the exponential substitution y = eλx, which results in
characteristic
polynomial of a
HNOLDE
the equation L[eλx] = (λn + an−1λn−1 + · · · + a1λ + a0)eλx = 0.This equation
will hold only if λ is a root of the characteristic polynomial
p(λ) ≡λn + an−1λn−1 + · · · + a1λ + a0
which, by the fundamental theorem of algebra, can be written as
p(λ) = (λ −λ1)k1(λ −λ2)k2 . . . (λ −λm)km.
(24.28)
The λi are the distinct roots of p(λ) with λj having multiplicity kj.
It is convenient to introduce D ≡d/dx and deﬁne the diﬀerential
operator
L = p(D) = Dn + an−1Dn−1 + · · · + a1D + a0.
Since D −μ and D −λ commute for arbitrary constants μ and λ, we can
unambiguously factor out the above and obtain
L = p(D) = (D −λ1)k1(D −λ2)k2 . . . (D −λm)km.
(24.29)
In preparation for ﬁnding the most general solution for Equation (24.27),
we ﬁrst note that
(D −λ)eλx = d
dxeλx −λeλx = 0
(24.30)
and
(D −λ)(xreλx) = d
dx(xreλx) −λxreλx = rxr−1eλx.
If we apply D −λ twice, we get
(D −λ)2(xreλx) = (D −λ)(rxr−1eλx) = r(r −1)xr−2eλx
and in general,
(D −λ)k(xreλx) = r(r −1) . . . (r −k + 1)xr−keλx
which, for k = r, gives
(D −λ)r(xreλx) = r!eλx.
If we apply D −λ one more time, we get zero by (24.30). Therefore,
(D −λ)k(xreλx) = 0
if
k > r.
(24.31)
The set of functions
{xr1eλ1x}k1−1
r1=0, {xr2eλ2x}k2−1
r2=0, . . . , {xrmeλmx}km−1
rm=0,

24.6 SOLDEs with Constant Coeﬃcients
577
are all solutions of Equation (24.27). For example, an element of the ﬁrst set
yields
L[xr1eλ1x] = (D −λ1)k1(D −λ2)k2 . . . (D −λm)km(xr1eλ1x)
= (D −λ2)k2 . . . (D −λm)km (D −λ1)k1(xr1eλ1x)



=0 because k1 > r1
= 0.
If the root λ is complex and the coeﬃcients of the DE are real, then
the complex conjugate λ∗is also a root (see Problem 24.14). It follows that
whenever xrjeλjx is a solution of the DE for complex λj, so is xrjeλ∗
j x. Thus,
writing λj = αj + iβj and using the linearity of L, we conclude that
xrjeαjx cos βjx
and
xrjeαjx sin βjx,
where
rj = 0, 1, . . ., kj −1,
are all solutions of (24.27).
It is easily proved that the functions xrjeλjx are linearly independent (see
Problem 24.13). Furthermore, m
j=1 kj = n by Equation (24.28). Therefore,
the set
,
xrjeλjx-
,
where
rj = 0, 1 . . . , kj −1 and j = 1, 2, . . . , m,
contains exactly n elements. We have thus shown that there are at least n
linearly independent solutions for the HNOLDE of Equation (24.27). In fact,
it can be shown that there are exactly n linearly independent solutions.
Box 24.6.1. Let λ1, λ2, . . . , λm be the roots of the characteristic poly-
nomial of the real HNOLDE of Equation (24.27), and let the respective
roots have multiplicities k1, k2, . . . , km. Then the functions xrjeλjx, where
rj = 0, 1 . . . , kj −1, are a basis of solutions of Equation (24.27).
Example 24.6.1. An equation that is used in both mechanics and circuit theory is
d2y
dt2 + ady
dt + by = 0
with
a, b > 0.
(24.32)
Its characteristic polynomial is p(λ) = λ2 + aλ + b which has the roots
λ1 = 1
2(−a +
	
a2 −4b)
and
λ2 = 1
2(−a −
	
a2 −4b).
We can distinguish three diﬀerent possible motions depending on the relative sizes
of a and b.
(a) a2 > 4b (overdamped): Here we have two distinct simple roots. The multi-
overdamped
plicities are both one: k1 = k2 = 1 (see Box 24.6.1). Therefore, the power of t for
both solutions is zero (r1 = r2 = 0). Let γ ≡1
2
√
a2 −4b. Then the most general
solution is
y(t) = e−at/2(c1eγt + c2e−γt).

578
Second-Order Linear Diﬀerential Equations
Since a > 2γ, this solution starts at y = c1 + c2 at t = 0 and continuously
decreases; so, as t →∞, y(t) →0.
(b) a2 = 4b (critically damped): In this case we have one multiple root of order
critically damped
2 (k1 = 2); therefore, the power of x can be zero or 1 (r1 = 0, 1). Thus, the general
solution is
y(t) = c1te−at/2 + c0e−at/2.
This solution starts at y(0) = c0 at t = 0, reaches a maximum (or minimum) at
t = 2/a −c0/c1, and subsequently approaches zero asymptotically (see Problem
24.23).
(c) a2 < 4b (underdamped): Once more, we have two distinct simple roots. The
underdamped
multiplicities are both one (k1 = k2 = 1); therefore, the power of x for both solutions
is zero (r1 = r2 = 0). Let ω ≡1
2
√
4b −a2. Then λ1 = −a/2 + iω and λ2 = λ∗
1. The
roots are complex, and the most general solution is thus of the form
y(t) = e−at/2(c1 cos ωt + c2 sin ωt) = Ae−at/2 cos(ωt + α).
The solution is a harmonic variation with a decaying amplitude A exp(−at/2). Note
that if a = 0, the amplitude does not decay. That is why a is called the damping
factor (or the damping constant). All three cases are shown in Figure 24.1.
damping factor
These equations describe either a mechanical system oscillating (with no external
driving force) in a viscous (dissipative) ﬂuid, or an electrical circuit consisting of a
resistance R, an inductance L, and a capacitance C. For mechanical oscillators,
a = β/m and b = k/m, where β is the dissipative constant related to the drag force
fdrag and the velocity v by fdrag = βv, and k is the spring constant (a measure of
the stiﬀness of the spring).
For RLC circuits, a = R/L and b = 1/LC. Thus, the damping factor depends
on the relative magnitudes of R and L. On the other hand, the frequency
ω ≡

b −
 a
2
2
=

1
LC −R2
4L2
depends on all three elements. In particular, for R ≥2
	
L/C, the circuit does not
oscillate.
■
Figure 24.1: The solid thin curve shows the behavior of an overdamped oscillator. The
critically damped case is the dashed curve, and the underdamped oscillator is the thick
curve.

24.6 SOLDEs with Constant Coeﬃcients
579
24.6.2
Central Force Problem
One of the nicest applications of the theory of DEs, and the one that initiated
central force
problem
the modern mathematical analysis, is the study of motion of a particle under
the inﬂuence of a central gravitational force. Surprisingly, such a motion can
be reduced to a one-dimensional problem, and eventually to a SOLDE with
constant coeﬃcients as follows.
Subsection 12.2.1 treated the equations of motion of a particle under the
inﬂuence of a central force. Conservation of angular momentum and the right
choice of the initial position and velocity (what amounted to setting L =
Lzˆez = Lˆez) eliminated the polar angle θ by assigning it the value π/2. Thus
the particle is conﬁned to the plane perpendicular to the angular momentum
vector, i.e., essentially the vector r × v. The set of three complicated DEs
(12.20) reduces to a much simpler set consisting of (12.22) and (12.24) which
we rewrite here as
m¨r −L2
mr3 = F(r),
˙ϕ =
L
mr2 .
(24.33)
In principle, we can solve the ﬁrst equation and ﬁnd r as a function of t,
then substitute it in the second equation and integrate the result to ﬁnd ϕ as
a function of time. However, it is more desirable to ﬁnd r as a function of ϕ,
i.e., ﬁnd the shape of the orbit of the moving particle.
In that spirit, we deﬁne a new dependent variable u = 1/r, and making
multiple use of the chain rule, we write the DEs with u as the dependent
variable and ϕ as the independent variable. We thus have
r = 1
u ⇒˙r = −˙u
u2 = −1
u2 ˙ϕ du
dϕ = −r2 ˙ϕ du
dϕ = −L
m
du
dϕ,
¨r = −L
m
d
dt
 du
dϕ

= −L
m
d2u
dϕ2 ˙ϕ = −L
m
d2u
dϕ2
L
mr2 = −L2
m2 u2 d2u
dϕ2 .
Substituting for ¨r and r in terms of u and its derivative, Equation (24.33)
yields
d2u
dϕ2 + u = −m
L2u2 F
 1
u

.
(24.34)
Historical Notes
Johannes Kepler (1571–1630) was a premature baby and a very delicate child
who was brought up by his grandparents. After elementary and secondary schooling,
Kepler entered T¨ubingen University to become a Protestant minister. At T¨ubingen
Kepler was taught astronomy by one of the leading astronomers of the day, Michael
Maestlin (1550–1631).
The astronomy of the curriculum was, of course, geocen-
tric astronomy. At the end of his ﬁrst year Kepler got ’A’s for everything except
mathematics. Probably Maestlin was trying to tell him he could do better, because
Johannes Kepler
1571–1629
Kepler was in fact one of the select pupils to whom he chose to teach more ad-
vanced astronomy by introducing them to the new, heliocentric cosmological system

580
Second-Order Linear Diﬀerential Equations
of Copernicus. It was from Maestlin that Kepler learned that the preface to Coper-
nicus’s book, explaining that this was ’only mathematics’, was not by Copernicus.
Kepler seems to have accepted almost instantly that the Copernican system was
physically true, and from then on, astronomy and mathematics became his passion.
Kepler also worked and wrote a book in optics, in which he used the idea of a ‘ray
of light’ for the ﬁrst time.
For the Kepler problem this equation is easy to solve because5
Kepler problem
F(r) = −K
r2 ⇒F
 1
u

= −Ku2
and we have
d2u
dϕ2 + u = Km
L2 .
(24.35)
Let v = u −Km/L2. Then Equation (24.35) becomes
d2v
dϕ2 + v = 0.
The characteristic polynomial of this equation is λ2 + 1, whose roots are
λ = ±i. These simple roots give rise to the linearly independent solutions
v = sin ϕ and v = cos ϕ. The general solution can therefore be expressed
as v = C1 cos ϕ + C2 sin ϕ which, using Problem 24.22, can be rewritten as
v = A cos(ϕ −ϕ0). Therefore,
v = u −Km/L2 = A cos(ϕ −ϕ0) ⇒u = Km/L2 + A cos(ϕ −ϕ0)
or
equation of the
orbits in the
Kepler problem
r =
1
(Km/L2) + A cos(ϕ −ϕ0).
(24.36)
This is the equation of a conic section in plane polar coordinates (see Problem
24.15).
We now investigate the details of Equation (24.36). First we note that
when ϕ = ϕ0, r is either a maximum or a minimum depending on the sign
of A. With an ellipse in mind, this corresponds to the (major) axis of the
ellipse making an angle ϕ0 with the x-axis. Thus setting ϕ0 = 0 corresponds
to choosing the axis of the conic section to be our x-axis. We adhere to this
choice and write
r =
1
(Km/L2) + A cos ϕ.
(24.37)
Next we want to determine the constant A in terms of the energy of the
particle. The potential energy (PE) is clearly −K/r. So, let us concentrate
5Although the Kepler problem usually refers to the gravitational central force, we want
to keep the discussion general enough so that electrostatic force is also included. Thus, K
introduced below can be either GMm or −keq1q2.

24.6 SOLDEs with Constant Coeﬃcients
581
on the kinetic energy (KE). The velocity of the particle is given in Equation
(12.16) with θ = π/2 and ˙θ = 0. Thus
KE = 1
2mv2 = 1
2m( ˙r2 + r2 ˙ϕ2) = 1
2m ˙r2 +
L2
2mr2 ,
(24.38)
where we used the second equation in (24.33). The second term in (24.38) is
sometimes called the centrifugal potential because (like a potential energy)
centrifugal
potential
it is a position-dependent energy that (like a centrifugal force) has resulted
from a velocity-dependent term. Diﬀerentiating Equation (24.37) with respect
to time gives
˙r =
A ˙ϕ sin ϕ
[(Km/L2) + A cos ϕ]2 = Ar2 ˙ϕ sin ϕ.
Squaring and using the second equation in (24.33), we obtain
˙r2 = A2r4 ˙ϕ2 sin2 ϕ = L2
m2 A2 sin2 ϕ.
We can eliminate the sine term in favor of terms involving r by solving for
A cos ϕ in (24.37):
A cos ϕ = 1
r −Km
L2
⇒A2 sin2 ϕ = A2 −
1
r −Km
L2
2
.
It follows that
KE = 1
2m
A
L2A2
m2
−L2
m2
1
r −Km
L2
2B
+
L2
2mr2
= L2A2
2m
−K2m
2L2 + K
r
and
E = KE + PE = L2A2
2m
−K2m
2L2 + K
r −K
r = L2A2
2m
−K2m
2L2 ,
so that
A = ±

2mE
L2
+ K2m2
L4
.
To avoid negative signs at later stages, we choose the negative sign now and
ﬁnally write
r =
L2/(Km)
1 −
	
2EL2/(K2m) + 1 cos ϕ
= L2/(Km)
1 −e cos ϕ,
(24.39)
where
e ≡

2EL2
K2m + 1
(24.40)
is called the eccentricity of the conic section.
eccentricity of
orbits

582
Second-Order Linear Diﬀerential Equations
The eccentricity, which by its very deﬁnition is always positive, determines
the shape of the orbit. Let us concentrate on the interesting case of elliptic
orbits corresponding to 0 < e < 1 indicating that the total energy of the
particle is negative. Inspection of Problem 24.15 reveals that the semi-major
and semi-minor axes of the ellipse are, respectively,
a2 =
L4
(1 −e2)2K2m2
and
b2 =
L4
(1 −e2)K2m2 .
Substituting for e from Equation (24.40) and noting that E < 0, we obtain
a = −K
2E ⇒E = −K
2a
and
b =
L
√
−2mE ⇒L =

mK
a
b.
(24.41)
The negativity of energy in an elliptic orbit is an indication of the stability
of the orbit. The potential energy is negative and larger in absolute value than
the kinetic energy. If the total energy is negative (and, of course, constant),
the particle cannot move too far away from the center of attraction, because
the magnitude of the PE may become too small to oﬀset the positive KE. The
absolute value of this total negative energy is called the binding energy. For
binding energy
an ellipse this binding energy is K/2a.
Kepler’s Laws
In 1609 Johannes Kepler, the German astronomer, after painstakingly an-
alyzing the motion of Mars for many years announced what is now called
Kepler’s ﬁrst law of planetary motion: The orbit of Mars is not a circle
Kepler’s ﬁrst law
but an ellipse. In the context of a very resilient tradition—dating back to
Pythagoras himself—in which circular orbits were given almost a divine sta-
tus, this announcement was truly monumental. Kepler had a hunch that all
planets obey this same law, but could not prove it. Equation (24.37) is the
mathematical statement of Kepler’s ﬁrst law.
Kepler’s second law of planetary motion states that equal areas are
Kepler’s second
law
swept out in equal times by the line joining the planet to the center of attraction
(the Sun). In other words the rate of change of the area is a constant. This
can be seen by referring to Figure 24.2 and noting that
ΔA ≈1
2rAB ≈1
2r(rΔϕ) ⇒ΔA
Δt ≈1
2r2 Δϕ
Δt →dA
dt = 1
2r2 ˙ϕ.
So, by the second equation in (24.33), dA/dt = L/2m which is a constant.
After the ﬁrst two laws, Kepler spent another 12 years searching for a
“harmony” in the motion of planets.
The imperfection he injected in the
planetary motions by the assumption of elliptical orbits prompted him to
seek for some sort of compensation. His third law was precisely that. He felt
that this law, with its precise mathematical structure, gave suﬃcient harmony
to the waltz of planets around the Sun to oﬀset the imperfection of elliptical

24.6 SOLDEs with Constant Coeﬃcients
583
S
A
B
r
Figure 24.2: The shaded area is almost equal to the area of the triangle SAB.
orbits. Kepler’s third law of planetary motion relates the period of each
planet to the length of its major axis. To derive it, we use Kepler’s second
Kepler’s third law
law:
T =
πab
dA/dt =
πab
(L/2m) =
2πabm
	
mK/ab
= 2πa3/2m1/2
√
K
,
where we used Equation (24.41). For gravity, K = GMm, and squaring both
sides of the above equation gives
T 2 = 4π2a3
GM .
This is the mathematical statement of Kepler’s third law.
24.6.3
The Inhomogeneous Case
When a driving force acts on a physical system, it will appear as the inho-
mogeneous term of the NOLDE. For the particular, but important, case in
which the inhomogeneous term is a product of polynomials and exponentials,
the solution can be found in closed form. This subsection shows how this is
done.
We assume that the inhomogeneous term in Equation (24.26) is of the
form r(x) = 
k pk(x)eλkx where pk(x) are polynomials and λk are (complex)
constants. The most general solution of Equation (24.26) is a linear combi-
nation of a basis of solutions (as given in Box 24.6.1) of the homogeneous
NOLDE and a particular solution of the NOLDE. We need to ﬁnd the latter.
Because L is a linear operator, it is clear that if y1 is a particular solution
of L[y] = r1(x) and y2 that of L[y] = r2(x), then y1 + y2 is a solution of
L[y] = r1(x) + r2(x). This suggests breaking up the inhomogeneous term into
smaller pieces. Thus, no generality is lost if we restrict r(x) to be p(x)eλx
where p(x) is a polynomial.
The reader may verify that, for any diﬀerentiable function f, we have
(D −λ)[eλxf(x)] = eλxf ′(x),
(D −λ)2[eλxf(x)] = eλxf ′′(x),

584
Second-Order Linear Diﬀerential Equations
and, in general,
(D −λ)k[eλxf(x)] = eλx dkf
dxk .
In particular, if p(x) is a polynomial of degree n, then
(D −λ)ku = eλxp(x)
has a solution of the form u = eλxq(x), where q(x) is a polynomial of degree
n + k that is the primitive (indeﬁnite integral) of p(x) of order k [so that the
kth derivative of q(x) is p(x)].
If ν ̸= λ, then the reader may check that
(D −ν)[eλxf(x)] = eλx[(λ −ν)f(x) + f ′(x)]
and, therefore, (D −ν)u = eλxp(x) has a solution of the form u = eλxq(x),
where q(x) is a polynomial of degree k.
Applying the last two equations
repeatedly leads to
particular solution
of nth order linear
DE
Box 24.6.2. The NOLDE L[y] = eλxS(x), where S(x) is a polynomial,
has the particular solution eλxq(x), where q(x) is also a polynomial. The
degree of q(x) equals that of S(x) unless λ = λj, a root of the characteristic
polynomial of L, in which case the degree of q(x) exceeds that of S(x) by
kj, the multiplicity of λj.
Once we know the form of the particular solution of the NOLDE, we can
ﬁnd the coeﬃcients in the polynomial of the solution by substituting in the
NOLDE and matching the powers on both sides.
Example 24.6.2. We ﬁnd the most general solutions of two diﬀerential equations
subject to the boundary conditions y(0) = 0 and y′(0) = 1.
(a) The ﬁrst DE we want to consider is
y′′ + y = xex.
(24.42)
The characteristic polynomial is λ2 + 1 whose roots are λ1 = i and λ2 = −i. Thus,
a basis of solutions is {cos x, sin x}. To ﬁnd the particular solution we note that λ
(the coeﬃcient of x in the exponential part of the inhomogeneous term) is 1, which
is neither of the roots λ1 and λ2. Thus, the particular solution is of the form q(x)ex,
where q(x) = Ax + B is of degree 1 [same degree as that of S(x) = x]. We now
substitute u = (Ax + B)ex in Equation (24.42) to obtain the relation
Axex + (2A + B)ex + (Ax + B)ex = xex.
Matching the coeﬃcients, we have
2A = 1
and
2A + 2B = 0 ⇒A = 1
2 = −B.
Thus, the most general solution is
y = c1 cos x + c2 sin x + 1
2(x −1)ex.

24.6 SOLDEs with Constant Coeﬃcients
585
Imposing the given boundary conditions yields 0 = y(0) = c1−1
2 and 1 = y′(0) = c2.
Thus,
y = 1
2 cos x + sin x + 1
2(x −1)ex
is the unique solution.
(b) The next DE we want to consider is
y′′ −y = xex.
(24.43)
Here p(λ) = λ2 −1, and the roots are λ1 = 1 and λ2 = −1. A basis of solutions
is {ex, e−x}. To ﬁnd a particular solution, we note that S(x) = x and λ = 1 = λ1.
Box 24.6.2 then implies that q(x) must be of degree 2 because λ1 is a simple root,
i.e., k1 = 1. We, therefore, try
q(x) = Ax2 + Bx + C ⇒u = (Ax2 + Bx + C)ex.
Taking the derivatives and substituting in Equation (24.43) yields two equations,
4A = 1
and
A + B = 0,
whose solution is A = −B = 1/4. Note that C is not determined, because Cex is
a solution of the homogeneous DE corresponding to Equation (24.43), so when L is
applied to u, it eliminates the term Cex. Another way of looking at the situation is
to note that the most general solution to (24.43) is of the form
y = c1ex + c2e−x + ( 1
4x2 −1
4x + C)ex.
The term Cex could be absorbed in c1ex.
We, therefore, set C = 0, apply the
boundary conditions, and ﬁnd the unique solution
y = 5
4 sinh x + 1
4(x2 −x)ex.
■
The inhomogeneous DE (IDE) L[y] = r(x) can be thought of as a machine
(or a black box) that produces a function y(x) when a function r(x) is fed
into it. Such an interpretation is common in the study of electrical or acoustic
ﬁlters. A signal, the function r(x), is sent into the ﬁlter, and a second function,
y(x), is received as an output. In such a context, by far the most important
input signal is a sinusoidal function of the general form r(t) = A cos(ωt + α),
which, with B = Aeiα, can be written in complex notation as (see Example
18.2.3)
r(t) = Re(R(t))
where
R(t) ≡Beiωt = Aei(ωt+α),
where A, B, α, and ω, the angular frequency, are all constants, and t represents
time (the independent variable).
Assuming that iω is not a root of p(λ),
the characteristic polynomial of L, Box 24.6.2 suggests a particular (complex)
solution, U = C(ω)eiωt where C(ω) is a (ω-dependent) constant. To determine
it, we substitute U in L[U] = Beiωt:
L[U] = L[C(ω)eiωt] = C(ω)L[eiωt] = C(ω)p(iω)eiωt,
so that
L[U] = Beiωt ⇒C(ω)p(iω)eiωt = Beiωt ⇒C(ω) =
B
p(iω).

586
Second-Order Linear Diﬀerential Equations
Writing the complex numbers in polar form
C(ω) ≡ρ(ω)eiγ(ω),
B = Aeiα,
p(iω) ≡P(ω)eiθ(ω),
we obtain
ρ(ω) =
A
P(ω)
and
γ(ω) = α −θ(ω).
The real solution, u(t) = Re[U(t)], will then be
u(t) = Re[C(ω)eiωt] = ρ(ω) cos[ωt + γ(ω)]
=
A
P(ω) cos[ωt + α −θ(ω)].
(24.44)
The function C(ω) is called the transfer function associated with the lin-
transfer function
ear operator L. Equation (24.44) shows that the output u(t) has the same fre-
quency as the input. It also indicates that the amplitude of u(t) is frequency-
dependent, making it possible to obtain large output amplitudes by varying
the frequency until P(ω) is minimum. This is the phenomenon of resonance
in AC circuits.
Example 24.6.3. Let us apply the analysis above to Example 24.6.1 and, for
deﬁniteness, take the underdamped case.
In this case, 4b > a2; and ω0 ≡
√
b
is called the natural frequency of the system. The characteristic polynomial is
natural frequency
p(λ) = λ2 + aλ + b. Thus,
p(iω) = −ω2 + iωa + b = (ω2
0 −ω2) + iωa
and
P(ω) =

(ω2
0 −ω2)2 + ω2a2,
θ(ω) = tan−1

ωa
ω2
0 −ω2

.
The amplitude of the output signal, sometimes called the gain function, is
gain function
ρ(ω) =
A
P(ω) =
A
	
(ω2
0 −ω2)2 + ω2a2 .
The minimum of the denominator occurs at ω = ω0, that is, when the driving
frequency equals the natural frequency. In such a situation we have ρ(ω) = A/(ω0a),
showing that the output signal will have a large amplitude when a, the damping
coeﬃcient, is small.
We have considered only the particular solution, u(t), because the most general
solution
y(t) = Ke−at/2 cos(ω1t + β) + u(t)
in which K and β are constants, eventually reduces to u(t). The ﬁrst term on the
RHS, the transient term, decays to zero.
The rate of this decay is determined
transient term
by the time constant 2/a, the time interval during which the amplitude of the
time constant
transient term drops to 1/e of its initial value.
■
The importance of the sinusoidal signal becomes clear when we recall that
any periodic signal can be expanded in a Fourier series, R(t) = ∞
n=−∞bneinωt

24.7 Problems
587
where ω is the fundamental frequency. The linearity of L suggests the solution
u(t) = Re[U(t)], where
U(t) =
∞

n=−∞
Cn(ω)einωt.
Substituting in L[U] = R(t) gives
∞

n=−∞
Cn(ω)p(inω)einωt =
∞

n=−∞
bneinωt.
Since the einωt are orthonormal, we get Cn(ω) = bn/[p(inω)], and
u(t) = Re
A
∞

n=−∞
bneinωt
p(inω)
B
.
Thus, u(t) is also periodic and has the same fundamental frequency as r(t).
24.7
Problems
24.1. Let f and g be two diﬀerentiable functions that are linearly dependent.
Show that their Wronskian vanishes. (Note that f and g need not be solutions
of a homogeneous SOLDE.)
24.2. Show that if (f1, f ′
1) and (f2, f ′
2) are linearly dependent at one point,
then f1 and f2 are linearly dependent at all a ≤x ≤b. Here f1 and f2 are
solutions of the DE of (24.4). Hint: Derive the identity
W(f1, f2; x2) = W(f1, f2; x1) exp
(
−
# x2
x1
p(t) dt
)
.
24.3. Show by direct substitution that f2 of Equation (24.6) indeed satisﬁes
(24.4) no matter what K is.
24.4. Show that the solutions to the SOLDE y′′ + q(x)y = 0 have a constant
Wronskian.
24.5. Find a general integral formula for Gn(x), the linearly independent
“partner” of the Hermite polynomial Hn(x) which satisﬁes the Hermite DE
y′′ −2xy′ + 2ny = 0.
Specialize this to n = 0, 1. Is it possible to ﬁnd G0(x) and G1(x) in terms of
elementary functions?

588
Second-Order Linear Diﬀerential Equations
24.6. Use Theorem 23.3.1 to construct
y = W(x)
f 2
1(x)

C +
# x
a
f1(t)r(t)
W(t)
dt

,
a solution of
y′ +

p + 2f ′
1
f1

y = r
f1
.
24.7. Show that each pair of the following functions satisfy the DE next to it.
Calculate the Wronskian, and give a solution satisfying the initial conditions
y(0) = 2 and y′(0) = 1.
(a) cos x and sin x; y′′ + y = 0.
(b) ex and e3x; y′′ + 4y′ + 3y = 0.
(c) x and ex; y′′ +
x
1 −xy′ −
1
1 −xy = 0.
24.8. For the HSOLDE y′′ + py′ + qy = 0, show that
p = −f1f ′′
2 −f2f ′′
1
W(f1, f2)
and
q = f ′
1f ′′
2 −f ′
2f ′′
1
W(f1, f2) .
Thus, knowing two solutions of an HSOLDE allows us to reconstruct the DE.
24.9. Show that the HSOLDE y′′ + py′ + qy = 0 can be cast in the form
u′′ + S(x)u = 0. Hint: Deﬁne w(x) by y = wu, substitute in the DE, and
demand that the coeﬃcient of u′ be zero to obtain
w(x) = C exp

−1
2
# x
α
p(t) dt

.
Now show that the original DE can be written as u′′ + S(x)u = 0 with
S(x) = q + pw′
w + w′′
w = q −1
4p2 −1
2p′.
24.10. Show that the adjoint of M given in Equation (24.14) is the original L.
24.11. Show that S-L equation (24.18) can be transformed into
d2v
dt2 + [λ −Q(t)]v = 0,
by the so-called Liouville substitution, which changes both independent
and dependent variables:
u(x) = v(t)[p(x)w(x)]−1/4,
t =
# x
a
"
w(s)
p(s) ds.
Then
Q(t) = q(x(t))
w(x(t)) + [p(x(t))w(x(t))]−1/4 d2
dt2 [(pw)1/4].

24.7 Problems
589
24.12. Show that
(a) the Liouville substitution (see Problem 24.11) transforms the Bessel DE
(xu′)′ + (k2x −ν2/x)u = 0 into
d2v
dt2 +

k2 −ν2 −1/4
t2

v = 0.
(b) Specialize to ν = 1
2 and show that
J1/2(kt) = Asin kt
√
t
+ B cos kt
√
t .
(c) Use the fact that Jν(x) is an analytic function of x to show that
J1/2(kt) = Asin kt
√
t .
24.13. Show that the functions xreλx, where r = 0, 1, 2, . . ., k, are linearly
independent. Hint: Starting with (D −λ)k, apply powers of D −λ to a linear
combination of xreλx for all possible r’s.
24.14. Suppose λ is a root of the polynomial
p(x) ≡xn + an−1xn−1 + · · · + a1x + a0,
where all coeﬃcients are real.
Show that λ∗is also a root of p(x).
Hint:
Complex conjugate p(λ) = 0. Does the same result hold if the coeﬃcients
were complex?
24.15. Write Equation (24.39) in the more familiar Cartesian coordinates
and show that e = 0 gives a circle, 0 < e < 1 gives an ellipse, e = 1 gives a
parabola, and e > 1 gives a hyperbola. Show that except for the case of a
parabola, the Cartesian equation of the conic section is
(1 −e2)2K2m2
L4

x −
L2e
Km(1 −e2)
2
+ (1 −e2)K2m2
L4
y2 = 1.
24.16. Derive all the formulas in Equation (24.41).
24.17. Find a basis of real solutions for each DE:
(a) y′′ + 5y′ + 6 = 0.
(b) y′′′ + 6y′′ + 12y′ + 8y = 0.
(c) y(4) = y.
(d) y(4) = −y.
24.18. Solve the following DEs subject to the given initial conditions.
(a)
y(4) = y, y(0) = y′(0) = y′′′(0) = 0, y′′(0) = 1.
(b)
y(4) + y′′ = 0, y(0) = y′′(0) = y′′′(0) = 0, y′(0) = 1.
(c)
y(4) = 0, y(0) = y′(0) = y′′(0) = 0, y′′′(0) = 2.

590
Second-Order Linear Diﬀerential Equations
24.19. Solve y′′ −2y′ + y = xex subject to the initial conditions y(0) =
0, y′(0) = 1.
24.20. Find the general solution of each equation:
(a) y′′ = xex.
(b) y′′ −4y′ + 4y = x2.
(c) y′′ + y = sin x sin 2x.
(d) y′′ −y = (1 + e−x)2.
(e) y′′ −y = ex sin 2x.
(f) y(6) −y(4) = x2.
(g) y′′ −4y′ + 4 = ex + xe2x.
(h) y′′ + y = e2x.
24.21. Consider the Euler equation
xny(n) + an−1xn−1y(n−1) + · · · + a1xy′ + a0y = r(x).
Substitute x = et and show that such a substitution reduces this to a DE
with constant coeﬃcients. In particular, solve x2y′′ −4xy′ + 6y = x.
24.22. Show that v = C1 cos θ + C2 sin θ can be written as v = A cos(θ −θ0).
Find A and θ0 in terms of C1 and C2.
24.23. (a) Show that the extremum (maximum or minimum) of the function
y(t) = c1te−at/2 + c0e−at/2
occurs at t = 2/a −c0/c1.
(b) Prove that if c1 > 0, the extremum is maximum and if c1 < 0, it is
minimum.
24.24. Verify that, for any diﬀerentiable function f, we have
(D −λ)[eλxf(x)] = eλxf ′(x)
and if ν ̸= λ, then
(D −ν)[eλxf(x)] = eλx[(λ −ν)f(x) + f ′(x)].
24.25. Derive Equation (24.44).

Chapter 25
Laplace’s Equation:
Cartesian Coordinates
In Chapter 22 we discussed the technique of the separation of variables for the
most important PDEs encountered in introductory physics and engineering
courses. One such PDE deserving special attention is the Laplace equation
∇2Φ = 0
(25.1)
which shows up extensively in problems in electrostatics and steady-state heat
conduction. The latter arises in situations in which the temperature does not
change with time, so that the LHS of Equation (22.3) vanishes.
Aside from its signiﬁcance in applications, Laplace’s equation is important
because its solution leads naturally to some of the most famous functions
of mathematical physics. In fact, when separating this equation in various
coordinate systems, one obtains not only such elementary functions as sines
and cosines, but also the more advanced “special functions” such as Legendre
polynomials and the Bessel functions. At the heart of such functions is the
linearity of Laplace’s equation which allows summing a (inﬁnite) number of
solutions to get a new solution. This leads naturally to solutions of Laplace’s
equation in terms of inﬁnite series.
In a typical situation, Φ is given on some surfaces bounding a volume in
space and its value is sought for all points in the volume. When the bounding
surfaces are arbitrarily shaped, the solution can be found only by numerical
techniques; but when they are primary surfaces of a coordinate system, then
we can generally solve the problem by separating Laplace’s equation in the
appropriate coordinate system.

592
Laplace’s Equation: Cartesian Coordinates
25.1
Uniqueness of Solutions
We shall see many examples of solutions to Laplace’s equation in various
coordinate systems in this and the following chapters. All of these solutions
will be obtained in the form of inﬁnite series. So, we know that solutions to
Laplace’s equation indeed exist. What we want to do in this section is to
show that the solution which satisﬁes all the boundary conditions is unique.
In other words, no matter how we ﬁnd the solution, as long as it satisﬁes the
boundary condition, it is the solution of Laplace’s equation. In fact, we can
be more general and prove the uniqueness for the Poisson equation ∇2Φ = ρ.
Consider the volume V with some surfaces bounding it. Figure 25.1 shows
two such volumes. Assume that two functions Φ1 and Φ2 satisfy the Poisson
equation at every point of the volume, and that they both satisfy some other
conditions related to the surfaces which we shall look into shortly. Let Φ =
Φ1 −Φ2 and note that Φ satisﬁes Laplace’s equation because
∇2Φ = ∇2(Φ1 −Φ2) = ∇2Φ1 −∇2Φ2 = ρ −ρ = 0.
For any function f, we have [see Equation (14.11)]
∇· (f∇f) = ∇f · ∇f + f∇2f = |∇f|2 + f∇2f.
For Φ—since it satisﬁes Laplace’s equation—we get
∇· (Φ∇Φ) = ∇Φ · ∇Φ + Φ ∇2Φ

=0
= |∇Φ|2.
Integrating both sides of the last equation over the volume V and using the
divergence theorem on the LHS yields
# #
S
(Φ∇Φ) · ˆen da =
# #
S
(Φ1 −Φ2)(ˆen · ∇Φ1 −ˆen · ∇Φ2) da
=
# #
V
#
|∇Φ|2dV.
(25.2)
(a)
(b)
V
V
Figure 25.1: A volume (shaded region) with its bounding surface. (a) The volume is
“inside” the bounding surface. (b) The volume is “outside” the bounding surface(s).

25.1 Uniqueness of Solutions
593
Now suppose:
• Dirichlet boundary condition: Φ1 and Φ2 take on the same value at
Dirichlet boundary
condition
every point of the bounding surface(s), i.e., Φ1 −Φ2 = 0 on S; or
• Neumann boundary condition: the so-called normal derivatives
Neumann
boundary
condition; normal
derivatives
ˆen · ∇Φ1 and ˆen · ∇Φ2 take on the same value at every point of the
bounding surface(s), i.e., ˆen · ∇Φ1 −ˆen · ∇Φ2 = 0 on S.
Then, in either case, the ﬁrst line of Equation (25.2) yields zero. Since the
integrand of the RHS is never negative, the integrand must vanish.1 It follows
that
|∇Φ|2 = 0 ⇒∇Φ = 0 ⇒Φ = constant ⇒Φ1 −Φ2 = constant
for all points in the volume V . Since Φ = 0 on the bounding surface, the
constant must be zero, i.e., Φ1 = Φ2 for all points in the volume V . We thus
have
Box 25.1.1. Let V be a volume bounded by a (possibly disconnected) sur-
face S. Then there exists a unique function which satisﬁes both Laplace’s
equation (or the Poisson equation) at every point of V and either Dirichlet
or Neumann boundary conditions on S.
Historical Notes
Pierre Simon de Laplace was a French mathematician and theoretical astronomer
who was so famous in his own time that he was known as the Newton of France. His
main interests throughout his life were celestial mechanics, the theory of probability,
and personal advancement.
At the age of 24 he was already deeply engaged in the detailed application of
Newton’s law of gravitation to the solar system as a whole, in which the planets and
their satellites are not governed by the Sun alone, but interact with one another
in a bewildering variety of ways. Even Newton had been of the opinion that di-
vine intervention would occasionally be needed to prevent this complex mechanism
from degenerating into chaos. Laplace decided to seek reassurance elsewhere, and
succeeded in proving that the ideal solar system of mathematics is a stable dynam-
ical system that will endure unchanged for all time. This achievement was only
one of the long series of triumphs recorded in his monumental treatise M´ecanique
C´eleste (published in ﬁve volumes from 1799 to 1825), which summed up the work
on gravitation of several generations of illustrious mathematicians. Unfortunately
for his later reputation, he omitted all reference to the discoveries of his predecessors
and contemporaries, and left it to be inferred that the ideas were entirely his own.
Many anecdotes are associated with this work. One of the best known describes
the occasion on which Napoleon tried to get a rise out of Laplace by protesting that
1The integral is the limit of a sum. If no term of this sum is negative, and the sum
equals zero, then each term of the sum must be zero.

594
Laplace’s Equation: Cartesian Coordinates
he had written a huge book on the system of the world without once mentioning
God as the author of the universe. Laplace is supposed to have replied, “Sire, I
had no need of that hypothesis.” The principal legacy of the M´ecanique C´eleste to
later generations lay in Laplace’s wholesale development of potential theory, with its
far-reaching implications for a dozen diﬀerent branches of physical science ranging
from gravitation and ﬂuid mechanics to electromagnetism and atomic physics. Even
though he lifted the idea of the potential from Lagrange without acknowledgment,
he exploited it so extensively that ever since his time the fundamental equation of
potential theory has been known as Laplace’s equation.
Pierre Simon de
Laplace 1749–1827
After the French Revolution, Laplace’s political talents and greed for position
came to full ﬂower. His compatriots speak ironically of his “suppleness” and “versa-
tility” as a politician. What this really means is that each time there was a change
of regime (and there were many), Laplace smoothly adapted himself by changing his
principles—back and forth between fervent republicanism and fawning royalism—
and each time he emerged with a better job and grander titles. He has been aptly
compared with the apocryphal Vicar of Bray in English literature, who was twice a
Catholic and twice a Protestant. The Vicar is said to have replied as follows to the
charge of being a turncoat: “Not so, neither, for if I changed my religion, I am sure
I kept true to my principle, which is to live and die the Vicar of Bray.”
To balance his faults, Laplace was always generous in giving assistance and
encouragement to younger scientists. From time to time he helped forward in their
careers such men as the chemist Gay–Lussac, the traveler and naturalist Humboldt,
the physicist Poisson, and—appropriately—the young Cauchy, who was destined to
become one of the chief architects of nineteenth-century mathematics.
25.2
Cartesian Coordinates
The separation of Laplace’s equation in Cartesian coordinates is obtained
from Equation (22.12) by setting the constant C equal to zero.2 This leads
to the following three equations3
d2X
dx2 −α1X = 0,
d2Y
dy2 −α2Y = 0,
d2Z
dz2 + (α1 + α2)Z = 0,
(25.3)
where the α’s could be any number (including zero and complex). The speciﬁc
value that each α takes on depends on the boundary conditions (BCs). We
consider bounding surfaces parallel to the planes of the Cartesian coordinates.
The most eﬀective way of learning how to solve Laplace’s equation is to
go into the details of the solution of a number of speciﬁc examples. We do
so in the following, hoping that the reader will examine these examples very
carefully, taking note of steps taken with an eye on how each step would
change in a diﬀerent situation (diﬀerent BCs, etc).
Example 25.2.1. Two semi-inﬁnite conducting plates starting on the y-axis and
semi-inﬁnite
electrically
conducting plates
parallel to the x-axis are grounded (the potential Φ is zero on them) and separated by
2Recall from Subsection 22.2 that Φ(x, y, z) = X(x)Y (y)Z(z).
3We have changed the sign of the α’s to illustrate how the boundary conditions force on
us the correct functional form of X, Y , and Z.

25.2 Cartesian Coordinates
595
y
x
Φ = 0
Φ = 0
b
Φ = V
(b)
(a)
y
x
z
Figure 25.2:
(a) The semi-inﬁnite plates, and (b) the cross section of the two
(grounded) plates and the strip maintained at potential V .
a distance b [Figure 25.2(a)]. Both plates extend from −∞to ∞in the z-direction. A
conducting strip of width b—also inﬁnite in both directions of the z-axis—is located
between the two plates and separated from them by an inﬁnitesimal gap, so that
the strip can be maintained at a diﬀerent potential of Φ = V . Figure 25.2(b) shows
the cross section of the geometry of the problem. We want to ﬁnd the potential in
the region enclosed by the conductors.
The potential is independent of z because, as a small observer moves along the
Symmetry tells us
that the potential
is independent
of z.
z-axis keeping the other two coordinates ﬁxed, his detectors and instruments will not
detect any change in the physics of the problem, because the physical environment
of the detectors remains unchanged. So, Z(z) is a constant which we absorb in X(x)
or Y (y). Furthermore, substituting Z = const. in the third equation of (25.3) yields
α1 + α2 = 0.
Thus the problem is reduced to ﬁnding X(x) and Y (y) which satisfy the diﬀer-
ential equations of (25.3). First let us consider the Y equation. If α2 = 0, then the
solution will be of the form
Y (y) = Ay + B.
The case of α2 ̸= 0 is a SOLDE with constant coeﬃcients whose most general
solution can be written as
Y (y) = Ae
√α2 y + Be−√α2 y.
(25.4)
The vanishing of Φ at y = 0 and y = b means that
Φ(x, 0) = X(x)Y (0) = 0
for all x ⇒Y (0) = 0,
Φ(x, b) = X(x)Y (b) = 0
for all x ⇒Y (b) = 0.
Therefore, for the case of α2 = 0, this implies
Y (0) = A × 0 + B = 0 ⇒B = 0,
Y (b) = Ab + B = Ab + 0 = 0 ⇒A = 0.
Thus, if α2 = 0, we get Y (y) = 0 and Φ(x, y) = X(x)Y (y) = 0 which is the trivial
solution.
It follows that if we are interested in nontrivial solutions, we had better assume
that α2 ̸= 0. Then, Equation (25.4) gives
Y (0) = A + B = 0
and
Y (b) = Ae
√α2 b + Be−√α2 b = 0.

596
Laplace’s Equation: Cartesian Coordinates
Multiplying the second equation by e
√α2 b and using B = −A, we obtain
A
*
e2√α2 b −1
+
= 0 ⇒A = 0 or e2√α2 b = 1.
The ﬁrst choice (A = 0) and A = −B yields a trivial solution again. Therefore, we
Boundary
conditions force
α2 to be
imaginary.
have to assume that the second choice holds. However, even with the second choice,
if we restrict ourselves to the real numbers, the only solution of e2√α2 b = 1 would
be α2 = 0 which is a contradiction because we are dealing precisely with the case
of α2 ̸= 0. It follows that √α2 must be a complex number. In fact, recalling that
e2inπ = 1 for any integer n, we immediately get
2√α2 b = 2inπ ⇒√α2 b = inπ ⇒α2 = −
 nπ
b
2
,
n = ±1, ±2, . . . .
Note that n = 0 is excluded because this choice would make α2 = 0.
We now turn to the X equation. Since α1 + α2 = 0, we obtain
α1 = −α2 =
 nπ
b
2
,
n = ±1, ±2, . . . ,
and
d2X
dx2 −
 nπ
b
2
X = 0 ⇒X(x) = Cenπx/b + De−nπx/b.
To be physically meaningful, the potential must remain ﬁnite as x →+∞.
It
follows from the last equation that either n is negative and D = 0, or n is positive
and C = 0. Either choice will lead to the same ﬁnal result as the reader may verify.
Choosing positive values of n with C = 0, the potential can be written as
Φn(x, y) = ADe−nπx/b *
einπy/b −e−inπy/b+
= Ane−nπx/b sin
 nπy
b

,
where we used A = −B and introduced a new constant An. We also subscripted the
potential because for every n, we get a diﬀerent function for Φ. All such functions
are solutions of Laplace’s equation and therefore, so is their sum. In fact, it is only
the sum that is general enough to result in the ﬁnal solution. We thus write
Φ(x, y) =
∞

n=1
Φn(x, y) =
∞

n=1
Ane−nπx/b sin
 nπy
b

.
(25.5)
This is a Fourier series in y with x dependent coeﬃcients. The potential will be
completely determined if the constants An can be determined. This is where the
last unused information comes in: The potential at x = 0 is V . Substituting this
information in Equation (25.5) yields
V = Φ(0, y) =
∞

n=1
An sin
 nπy
b

from which An can be determined using the Fourier series techniques. We leave it
for the reader to show that An = 2V [1 −(−1)n]/(nπ) (see Problem 25.1), or
An =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
4V
nπ
if
n is odd,
0
if
n is even.

25.2 Cartesian Coordinates
597
By writing n = 2k + 1 with k = 0, 1, 2, . . ., the potential in the region of interest
becomes
Φ(x, y) = 4V
π
∞

k=0
e−(2k+1)πx/b
2k + 1
sin
 (2k + 1)πy
b

(25.6)
= 4V
π

e−πx/b sin πy
b + e−3πx/b
3
sin 3πy
b
+ e−5πx/b
5
sin 5πy
b
+ · · ·

.
Because of the exponential factor, the series converges very rapidly, and for large
values of x the potential very quickly drops to zero. Figure 25.3 shows the potential
function (in arbitrary units) as a function of x and y.
■
Example 25.2.1 illustrates the general feature of solving Laplace’s equation
by the separation of variables in Cartesian coordinates. This feature works in
other coordinate systems as well. The separation of variables results in some
ODEs which involve parameters (in the case above, the α’s) to be determined
by some of the BCs. All values of these parameters—which in all cases of
interest to us will turn out to be integers—consistent with the used boundary
conditions are allowed and must be taken into account, i.e., an inﬁnite sum
(with as yet undetermined coeﬃcients) over such parameters is to be formed
as the most general solution of Laplace’s equation. By applying the remaining
BCs, the undetermined coeﬃcients can be evaluated, resulting in the unique
solution appropriate for the geometry of the problem. If the geometry extends
to inﬁnity in a certain direction, then such an inﬁnity is to be considered as a
BC. It is extremely useful to take into account any symmetry of the problem
as such symmetries will simplify the solution considerably. The symmetry in
the z-direction of Example 25.2.1 saved us the trouble of solving one (out of
three) complete ODE.
Figure 25.3: The potential function inside the semi-inﬁnite box of Figure 25.2 when
only 20 terms of the inﬁnite series are kept. Note how quickly the potential drops to
zero along the x-axis due to the exponential factor.

598
Laplace’s Equation: Cartesian Coordinates
Example 25.2.2. Steady-state heat conduction problems also obey Laplace’s
equation. So, let us consider a rectangular medium inﬁnite in the z direction en-
inﬁnitely long
rectangular heat
conductor
closed by two pairs of parallel slabs of width a and b as shown in Figure 25.4. The
temperatures of the slabs of width a—assumed parallel to the x-axis—are zero. The
temperatures of the other two slabs are T1 and T2. We want to ﬁnd the temperature
at all points in the region enclosed after the equilibrium is reached.
As in Example 25.2.1, we can ignore the z-dependence and write T(x, y) =
X(x)Y (y) where X and Y satisfy Equation (25.3) with α1 = −α2. For exactly the
same reason as in Example 25.2.1, α2 cannot be zero and Y can only be of the form
Y (y) = An sin nπy
b
,
n = 1, 2, . . . ,
where the subscript on An reminds us that diﬀerent constants can be chosen to
multiply diﬀerent sine functions. The solution for X will, however, be diﬀerent. We
still have
X(x) = Cnenπx/b + Dne−nπx/b,
n = 1, 2, . . . ,
but neither Cn nor Dn is zero this time. Multiplying the two functions and redeﬁning
the constants, we can write
Tn(x, y) =

Anenπx/b + Bne−nπx/b
sin nπy
b
and the most general inﬁnite series solution becomes
T(x, y) =
∞

n=1

Anenπx/b + Bne−nπx/b
sin nπy
b
.
(25.7)
So far, we have used only two of the four BCs.
The remaining two will deter-
mine the unknowns An and Bn. Substituting these BCs yields the following two
equations:
T1 = T(0, y) =
∞

n=1
(An + Bn)



≡En
sin nπy
b
=
∞

n=1
En sin nπy
b
,
T2 = T(a, y) =
∞

n=1

Anenπa/b + Bne−nπa/b



≡Fn
sin nπy
b
=
∞

n=1
Fn sin nπy
b
,
y
x
Τ = 0
Τ = 0
b
Τ = T1
Τ = T2
a
Figure 25.4: The cross section of the two pairs of parallel slabs maintained at diﬀerent
temperatures.

25.2 Cartesian Coordinates
599
where we have redeﬁned the constants multiplying the sine functions. As in Example
25.2.1 and Problem 25.1, we have
En = 2T1
nπ [1 −(−1)n],
Fn = 2T2
nπ [1 −(−1)n].
These relations show that only odd terms of the inﬁnite series are of relevance, and
they are given by
E2k+1 ≡A2k+1 + B2k+1 =
4T1
π(2k + 1),
(25.8)
F2k+1 ≡A2k+1e(2k+1)πa/b + B2k+1e−(2k+1)πa/b =
4T2
π(2k + 1).
These are two equations in two unknowns which can be solved to get
A2k+1 =
2(T2 −T1e−(2k+1)πa/b)
π(2k + 1) sinh[(2k + 1)πa/b],
B2k+1 =
2(T1e(2k+1)πa/b −T2)
π(2k + 1) sinh[(2k + 1)πa/b].
Substituting in Equation (25.7)—with n replaced by 2k + 1—and rearranging terms
yields
T(x, y) = 4
π
∞

k=0
T1 sinh
*
(2k+1)π(a−x)
b
+
+ T2 sinh
*
(2k+1)πx
b
+
(2k + 1) sinh
*
(2k+1)πa
b
+
sin (2k + 1)πy
b
. (25.9)
The reader is urged to verify that when T2 = 0 and a →∞, we recover the result
of Example 25.2.1—with V replaced by T1—as we should. Figure 25.5 shows the
potential function (in arbitrary units) as a function of x and y.
■
The examples treated so far may give the impression that α1 or α2 is
never zero. This has to do with the speciﬁc BCs imposed on Φ (or T ). In
Figure 25.5: The potential function inside the box of Figure 25.4 for the special case
of a = b and T1 = T2 when only 20 terms of the inﬁnite series are kept.

600
Laplace’s Equation: Cartesian Coordinates
both examples, Y vanishes at both y = 0 and y = b. Such a BC excludes
α2 = 0 because the corresponding Y , namely Y = Ay + B, cannot satisfy
those conditions unless Y = 0 identically.
Example 25.2.3. To see how the α1 = −α2 = 0 terms can enter in the game, let
us modify the temperatures of the plates and strips of Example 25.2.2 so that the
bottom plate and the left strip are held at T = 0 while the top plate is held at T1
and the right strip at T2.
Let us write the most general solution of Laplace’s equation obtained from sep-
arating variables including the α1 = 0 = −α2 term.
Since the nonzero α1 and
α2 are of opposite signs, one of them will be positive and will have real square
roots and the other pure imaginary roots.
Let us assume that α1 is positive.
Then X will be of exponential type and Y of imaginary exponential or trigono-
metric type. It follows that the most general solution of Laplace’s equation can be
written as
T(x, y) = (A0x + B0) (C0y + D0)
(25.10)
+
∞

α

Aαe
√α x + Bαe−√α x .
Cα sin(√α y) + Dα cos(√α y)
/
,
where we have used α for α1 = −α2. It is convenient to impose the y BCs ﬁrst. So,
since T(x, 0) = 0, we have
0 = (A0x + B0) D0 +
∞

α

Aαe
√α x + Bαe−√α x
Dα
which should hold for arbitrary values of x. This can happen only if D0 = Dα = 0.
So, absorbing the multiplicative constant C0 and Cα into the A’s and B’s, we get a
new expression for the temperature:
T(x, y) = (A0x + B0) y +
∞

α

Aαe
√α x + Bαe−√α x
sin(√α y).
The other y BC gives
T1 = (A0x + B0) b +
∞

α

Aαe
√α x + Bαe−√α x
sin(√α b).
For this to hold for arbitrary x, we need to have
The importance of
α1 = 0 term is
displayed here by
the relation
between B0 and
T1.
A0 = 0,
B0b = T1 ⇒B0 = T1
b ,
sin(√α b) = 0 ⇒α =
 nπ
b
2
.
The temperature function reduces to
T(x, y) = T1
b y +
∞

n=1

Anenπx/b + Bne−nπx/b
sin nπy
b
.
We now impose the other two BCs. These will give us the following two equations:
0 = T(0, y) = T1
b y +
∞

n=1
(An + Bn) sin nπy
b
,
T2 = T(a, y) = T1
b y +
∞

n=1

Anenπa/b + Bne−nπa/b
sin nπy
b
.

25.2 Cartesian Coordinates
601
Multiplying both sides of these equations by sin(mπy/b) and integrating from 0 to
b yields the following two equations for Am and Bm:
Am + Bm = −2T1
b2
# b
0
y sin mπy
b
dy = 2T1
mπ (−1)m,
Amemπa/b + Bme−mπa/b = 2
b
# b
0

T2 −2T1
b y

sin mπy
b
dy
(25.11)
=
2
mπ [T2 + (−1)m(T1 −T2)] .
These two equations can be solved to obtain the remaining unknown coeﬃcients Am
and Bm.
■
A couple of remarks are in order. The preceding example illustrated clearly
the importance of the α1 = 0 term: Had we not included it in the expansion
of T , we would not have obtained the answer. This is overlooked in most
elementary treatments of Laplace’s equation. It is worthwhile to emphasize
this point.
Box 25.2.1. Always start with the most general solution of Laplace’s
equation, including the term corresponding to the case in which the con-
stants of the separation of variables are zero, as given in Equation (25.10).
Then apply the BCs, keeping in mind that there may be a preferred order
for such an application.
In Example 25.2.3, the order in which the y BCs were applied ﬁrst was the
preferred choice.
The second remark has to do with the choice of the functional form of
X and Y . In Example 25.2.3, we chose X to be exponential and Y to be
trigonometric. We could just as well have chosen Y to be exponential and X
to be trigonometric. The appearance of the series would have changed, but
the value of T at any point in the region of interest would have been the same
for both series. This is due to the uniqueness of the solution of Laplace’s
equation.4
Example 25.2.4. The examples treated so far have been exclusively in two di-
mensions. We now consider a three-dimensional problem. Although this particular
a three-
dimensional
example of the
application of
Laplace’s equation
problem can be solved more quickly by relying on our intuition (as we did in Exam-
ple 25.2.1, for example), we shall start from the most general solution, as prescribed
by Box 25.2.1.
Suppose that the four lateral sides of widths a and b of a semi-inﬁnite rectangular
conducting tube are grounded and the closed base is held at potential V . The cross
section of this tube is shown in Figure 25.4 where it is assumed that the tube starts
at z = 0 and extends to inﬁnity in the positive z-direction. We are interested in
ﬁnding the potential inside this tube.
4The representation of the same function by diﬀerent series should be familiar to the
reader from calculus where f(x) can be written as a Taylor expansion about any point in
its domain of deﬁnition. Although such expansions look diﬀerent, they all represent the
same function.

602
Laplace’s Equation: Cartesian Coordinates
We start with Equation (25.3) which holds for all solutions of Laplace’s equation
the four
alternatives for
constants α1 and
α2
in Cartesian coordinates. There are four diﬀerent cases to consider:
1. α1 = 0 = α2: In this case, X(x) is of the generic form Ax + B, and with y or
z replacing x, this is also the generic form of Y and Z. Let us denote these
solutions as X0, Y0, and Z0.
2. α1 = 0, α2 ̸= 0: In this case, X(x) is of the generic form Ax + B. But Y and
Z are either exponential or trigonometric. Let us denote these solutions as
X0, Yα2, and Zα2.
3. α1 ̸= 0, α2 = 0: In this case, Y (y) is of the generic form Ay + B. But X and
Z are either exponential or trigonometric. Let us denote these solutions as
Y0, Xα1, and Zα1.
4. α1 ̸= 0, α2 ̸= 0: In this case X, Y , and Z are either exponential or trigono-
metric. Let us denote these solutions as Xα1, Yα2, and Zα1+α2.
The most general solution for the potential, encompassing all values of α1 and α2, is
Φ(x, y, z) = X0(x)Y0(y)Z0(z) + X0(x)

α2
Yα2(y)Zα2(z)
(25.12)
+ Y0(y)

α1
Xα1(x)Zα1(z) +

α1̸=0

α2̸=0
Xα1(x)Yα2(y)Zα1+α2(z).
We now apply the BCs. Since Φ(0, y, z) = Φ(a, y, z) = 0 for arbitrary y and z,
Boundary
conditions severely
restrict the terms
of the inﬁnite
sums in (25.12).
and since each term in Equation (25.12) is independent of all others, we conclude
that X0(0) = 0 = X0(a) and X0(0) = 0 = X0(a). It follows that A and B are both
zero for X0 and X0. So, X0(x) = 0 = X0(x). Similarly, Y0(y) = 0, and Φ is reduced
to the last term (the double sum) of (25.12). Furthermore, since both Xα1 and Yα2
vanish at the two ends of their respective ranges, we expect them to be periodic,
i.e., of trigonometric type. So, the most general solution is now
Φ(x, y, z) =

α1,α2
[Aα1 cos(√α1 x) + Bα1 sin(√α1 x)]
· [Cα2 cos(√α2 y) + Dα2 sin(√α2 y)] Zα1+α2(z).
If this is to vanish at x = 0 for arbitrary y and z, then Aα1 must be zero; and if
Φ(a, y, z) = 0 for all y and z, then all coeﬃcients of the product of the y and z
functions in the sum must be zero. These coeﬃcients—after setting Aα1 equal to
zero—are of the form sin(√α1 a). It follows that
√α1 a = mπ ⇒α1 =
 mπ
a
2
,
m = 1, 2, . . . ,
where we have excluded the negative values of m as in the previous examples. An
entirely analogous reasoning leads to Cα2 = 0 and
√α2 b = nπ ⇒α2 =
nπ
b
2
,
n = 1, 2, . . . .
The z-dependence is exponential, and since the potential cannot diverge at large
values of z, the positive exponent will be absent. Absorbing all multiplicative con-
stants into (a single doubly indexed) one, we can now write
Φ(x, y, z) =
∞

m,n=1
Amn sin mπx
a
sin nπy
b
e−π√
m2/a2+n2/b2 z.
(25.13)

25.3 Problems
603
The unknown constants Amn are determined by using the last BC:
a two-dimensional
Fourier series
V = Φ(x, y, 0) =
∞

m,n=1
Amn sin mπx
a
sin nπy
b
.
(25.14)
This is a double Fourier series.
Theorem 25.2.5. The coeﬃcients of the double Fourier series (25.14) can be
calculated by multiplying both sides by sin jπx
a
sin kπy
b
and integrating the result
from 0 to a in the x variable and from 0 to b in y:
Ajk = 4
ab
# a
0
# b
0
Φ(x, y, 0) sin jπx
a
sin kπy
b dx dy
It now follows that
Ajk = 4V
ab
# a
0
# b
0
sin jπx
a
sin kπy
b dx dy = 4V
ab
# a
0
sin jπx
a dx
# b
0
sin kπy
b dy
= 4V
ab
 a
πj [1 −(−1)j]
  b
πk [1 −(−1)k]

or
Ajk = 4V
π2
1 −(−1)j
j
1 −(−1)k
k
.
It is clear that only the odd terms of the double sum will contribute. Thus, the ﬁnal
answer for the potential inside [Equation (25.13)] is
Φ(x, y, z) = 16V
π2
∞

m,n=1
sin[(2m + 1)πx/a]
2m + 1
sin[(2n + 1)πy/b]
2n + 1
· e−π√
(2m+1)2/a2+(2n+1)2/b2 z.
By its very construction, this function satisﬁes Laplace’s equation as well as all the
BCs. Therefore, by the uniqueness theorem it must represent the unique potential
for the region of interest.
■
25.3
Problems
25.1. Given that V = ∞
n=1 An sin(nπy/b) where V is a constant in the
interval (0, b), show that An = 2V [1 −(−1)n]/(nπ).
25.2. A long hollow cylinder with square cross section of side a has three sides
grounded and the fourth side maintained at potential V0 (see Figure 25.6).
Find the potential at all points inside.
25.3. Example 25.2.1 treated the case in which the plate at x = 0 was held at
the constant potential V . Now suppose that it is held at a potential that varies
with y. Use Equation (25.5) to ﬁnd the potential as a function of x and y when

604
Laplace’s Equation: Cartesian Coordinates
y
x
Φ = 0
Φ = 0
a
Φ = 0
Φ = V0
Figure 25.6: The cross section of the conducting cylinder extended along the z-axis.
(a) Φ(0, y) = V0
b2 y(y −b)
(b) Φ(0, y) = V0
b y
(c) Φ(0, y) = V0 sin πy
b .
25.4. In Example 25.2.2, we assumed constant temperatures for the left and
right plates. Now suppose that the top and bottom plates are as before, but
the left plate is held at a varying temperature given by
T (0, y) = T0
b2 y(y −b).
Use Equation (25.7) to ﬁnd the temperature as a function of x and y when
(a) T (a, y) = 0;
(b) T (a, y) = T0
b2 y(y −b);
(c) T (a, y) = T0;
(d) T (a, y) = T0
b y;
(e) T (a, y) = T0 sin πy
b .
25.5. Suppose that the top and bottom plates of Example 25.2.2 are as before,
but the left plate is held at a varying temperature given by
T (0, y) = T0 sin 2πy
b .
Use Equation (25.7) to ﬁnd the temperature as a function of x and y when
(a) T (a, y) = 0;
(b) T (a, y) = T0
b2 y(y −b);
(c) T (a, y) = T0;

25.3 Problems
605
(d) T (a, y) = T0
b y;
(e) T (a, y) = T0 sin πy
b .
25.6. Solve Equation (25.8) for A2k+1 and B2k+1 and substitute in (25.7) to
obtain (25.9).
25.7. Verify that when T2 = 0 and a →∞, Equation (25.9) approaches the
result of Example 25.2.1—with V replaced by T1.
25.8. Derive Equation (25.11). Assume that T1 = T2 = T0 and solve for Am
and Bm.
25.9. Obtain the expression for Ajk in Theorem 25.2.5.
25.10. Find the potential inside a cube with sides of length a when the top
side is held at a constant potential V0 with all other sides grounded (zero
potential).
25.11. Find the electrostatic potential inside a cube with sides of length a if
all faces are grounded except the top, which is held at a potential given by:
(a) V0
a x,
0 ≤x ≤a.
(b) V0
a y,
0 ≤y ≤a.
(c) V0
a2 xy,
0 ≤x, y ≤a.
(d) V0 sin
π
a x

,
0 ≤x ≤a.

Chapter 26
Laplace’s Equation:
Spherical Coordinates
The separation of Laplace’s equation in spherical coordinates is obtained from
Equation (22.16) by substituting f(r) = 0. This will yield1
1
r2
d
dr

r2 dR
dr

−α
r2 R = 0,
1
sin θ
d
dθ

sin θdΘ
dθ

+

α −
β
sin2 θ

Θ = 0,
(26.1)
d2S
dϕ2 + βS = 0.
We consider the case where S is the constant function.2
This corresponds
to problems with an azimuthal symmetry, i.e., problems for which it is a
azimuthal
symmetry means
independence
from ϕ
priori clear that the potential is independent of the azimuthal angle ϕ. For
such situations, the third equation in (26.1) implies that β = 0 because S is a
(nonzero) constant. The independent variables are reduced to two and, with
Φ(r, θ) = R(r)Θ(θ), the remaining ODEs simplify to
1
r2
d
dr

r2 dR
dr

−α
r2 R = 0,
1
sin θ
d
dθ

sin θdΘ
dθ

+ αΘ = 0.
(26.2)
We shall now concentrate on the second equation and come back to the ﬁrst
after we have found solutions to the second.
1Here we have changed the symbol of the azimuthal function to S so that Φ(r, θ, ϕ) =
R(r)Θ(θ)S(ϕ).
2The case in which S is not constant—so that Φ depends on the azimuthal angle—is
more complicated and will not be pursued here. Instead, the interested reader is referred
to Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations, Springer-
Verlag, 1999, Chapter 12 for details.

608
Laplace’s Equation: Spherical Coordinates
The appearance of sin θ dθ (the diﬀerential of cos θ) in the denominator
suggests changing the independent variable from θ to u ≡cos θ.
For any
function f(θ), the chain rule gives3
df
du = df
dθ
dθ
du = df
dθ
1
du/dθ = −
1
sin θ
df
dθ
or
df
dθ = −sin θ df
du,
(26.3)
which allows us to convert the derivative of a function with respect to u to
the derivative with respect to θ and vice versa.
Introduce a new function P(u) such that P(u) ≡Θ(θ). Using the chain
rule, substituting in the second equation of (26.2), and writing sin2 θ = 1−u2,
the DE becomes
−
1
sin θ
d
dθ

(1 −u2)dP
du

+ αP = 0.
The term in the square brackets is a function of u. So, by Equation (26.3),
we can convert the θ-derivative to a u-derivative and obtain
d
du

(1 −u2)dP
du

+ αP = 0,
(26.4)
which can also be written as
(1 −u2)d2P
du2 −2udP
du + αP = 0
(26.5)
or
Legendre
diﬀerential
equation
d2P
du2 −
2u
1 −u2
dP
du +
α
1 −u2 P = 0.
(26.6)
Equation (26.4), or (26.5), or (26.6) is called the Legendre equation. We
shall solve this DE using the so-called Frobenius method or the method of
undetermined coeﬃcients.
26.1
Frobenius Method
The basic assumption of the Frobenius method is that the solution of the DE
can be represented by a power series. This is not a restrictive assumption
because all functions encountered in physical applications can be written as
power series as long as we are interested in their values lying in their interval
of convergence. This interval may be very small or it may cover the entire
real line.
A second order homogeneous linear DE can be written as
p2(x)d2y
dx2 + p1(x)dy
dx + p0(x)y = 0.
(26.7)
3Note that f can be considered a function of u as well as θ.

26.1 Frobenius Method
609
For almost all applications encountered in physics (certainly in this book),
p0, p1, and p2 are polynomials.4 The ﬁrst step in the implementation of the
Frobenius method is to assume an inﬁnite power series for y. It is common
to choose the point of expansion to be x = 0. If p2(0) ̸= 0, only nonnegative
powers of x need be considered.5 If p2(0) = 0, the DE loses its character of
being “second order” and the solutions we are seeking may not be deﬁned
there. In such a case, we have two choices:
1. choose a diﬀerent point of expansion x0 ̸= 0 so that p2(x0) ̸= 0; or
2. allow nonpositive powers of x in the expansion of y.
The ﬁrst choice is rarely used. It turns out that the most economic—but
general—way of incorporating the second choice is to write the solution as
y = xr
∞

n=0
anxn =
∞

n=0
anxn+r = a0xr + a1xr+1 + a2xr+2 + a3xr+3 + · · · ,
(26.8)
where r is a real number (not necessarily a positive integer) to be determined
by the DE.6 It is customary to choose a0 = 1 because any constant multiple
of a solution is also a solution; so, if a0 ̸= 1, we simply multiply the series by
1/a0 to make it so.7 Since a power series is uniformly convergent—within its
radius of convergence—it can be diﬀerentiated term by term. So, we have
dy
dx =
∞

n=0
an(n + r)xn+r−1 = ra0xr−1 + (r + 1)a1xr + · · · ,
d2y
dx2 =
∞

n=0
an(n + r)(n + r −1)xn+r−1
(26.9)
= r(r −1)a0xr−2 + (r + 1)ra1xr−1 + (r + 2)(r + 1)a2xr + · · · .
We now substitute Equations (26.8) and (26.9) in the DE (26.7), multiply
out the polynomials into the series, collect all distinct powers of x together,
and set the coeﬃcient of each term equal to zero. We thus obtain a set of
equations whose solution determines r and the an’s. The equation arising form
the lowest power of x involves only r and is called the indicial equation.
indicial equation
This is usually a quadratic equation in r which can be solved to obtain the
4The DE may not emerge in the form given here out of, say, the separation of variables,
but can be cast in that form. The most complicated form of the coeﬃcients of the derivatives
in a DE are typically rational functions (ratios of two polynomials). Therefore, multiplying
the DE by the product of all three denominators will cast the DE in the form given in
(26.7).
5For a thorough discussion of the Frobenius method, including motivation and proofs for
the claims cited here, consult Hassani, S. Mathematical Physics: A Modern Introduction
to Its Foundations, Springer-Verlag, 1999, Chapter 14.
6As Problem 26.2 indicates, one can start with a solution of the form (26.8) even when
p2(0) ̸= 0. The diﬀerential equation will then force r to be zero.
7The choice a0 = 1 is convenient only when p2(0) = 0.
If p2(0) ̸= 0, we need not
restrict a0.

610
Laplace’s Equation: Spherical Coordinates
(two) possible values of r, each leading generally to a diﬀerent solution. The
other equations coming from higher powers of x give recursion relations,
recursion relations
i.e., equations which give an in terms of an−1 and an−2. By iterating this
relation, one can obtain all an’s in terms of only two which can be determined
by the BCs. Let us summarize the procedure outlined above:
Theorem 26.1.1. (Frobenius method). To solve the DE (26.7), assume
a solution of the form (26.8). If p2(0) ̸= 0, choose r = 0, substitute y and
its derivatives (26.9) in the DE, multiply out, collect all powers of x, and set
their coeﬃcients equal to zero. If p2(0) = 0, let a0 = 1 and solve the indicial
equation to obtain r. Set the coeﬃcients of all other powers of x equal to zero
to ﬁnd the recursion relation giving an in terms of an−1 and an−2. Use this
relation and the values of r obtained above to ﬁnd all an’s in terms of only
two.
26.2
Legendre Polynomials
We now apply the Frobenius method to the Legendre DE for which—using u
as the independent variable—p2(u) = 1 −u2, p1(u) = −2u, and p0(u) = α.
Since p2(0) ̸= 0, we need not introduce an extra power of r for the series.
Therefore, we may write
P(u) =
∞

n=0
anun = a0 + a1u + a2u2 + a3u3 + · · · ,
dP
du =
∞

n=1
nanun−1 = a1 + 2a2u + 3a3u2 + · · · =
∞

n=0
(n + 1)an+1un,
d2P
du2 =
∞

n=2
n(n −1)anun−2 = 2a2 + 6a3u + 12a4u2 + · · ·
=
∞

n=0
(n + 1)(n + 2)an+2un.
Multiplying each of the expressions above by its corresponding polynomial,
we obtain
αP(u) = αa0 + αa1u + αa2u2 + αa3u3 + · · · =
∞

n=0
αanun,
−2udP
du = −2a1u −4a2u2 −6a3u3 + · · · = −
∞

n=0
2(n + 1)an+1un+1,
(1 −u2)d2P
du2 = 2a2 + 6a3u + 12a4u2 + 20a5u3 + · · ·
−2a2u2 −6a3u3 −12a4u4 −20a5u5 + · · ·
= 2a2 + 6a3u + (12a4 −2a2)u2 + (20a5 −6a3)u3 + · · · .

26.2 Legendre Polynomials
611
We add these three series, noting that their sum must equal zero
0 = αa0 + αa1u + αa2u2 + αa3u3 −2a1u −4a2u2 −6a3u3 + 2a2 + 6a3u
+ (12a4 −2a2)u2 + (20a5 −6a3)u3 + · · · ,
= (αa0 + +2a2) + [(α −2)a1 + 6a3]u
+ [(α −6)a2 + 12a4]u2 + [(α −12)a3 + 20a5]u3 + · · · .
The reader may note the pattern emerging in the expression for the coeﬃ-
cients. In fact, the coeﬃcient of un can be written as [α −n(n + 1)]an + (n +
1)(n + 2)an+2. Setting this coeﬃcient equal to zero, we obtain the recursion
relation
recursion relation
for the Legendre
equation
an+2 = n(n + 1) −α
(n + 1)(n + 2)an,
n = 0, 1, 2, . . .,
(26.10)
which gives all an’s in terms of a0 and a1.
Although writing out the series term by term is a sure way of arriving at
each individual coeﬃcient, and—by the discovery of a pattern—the recursion
relation, manipulation with the summation symbols can also lead to the recur-
sion relations without any expectation of pattern recognition. We go through
How to get to the
recursion relation
(26.10) by
manipulating
summations.
the details of such a manipulation as a noteworthy exercise in working with
the summation signs. The general procedure is to write all sums in such a
way that the exponent of u agrees in all of them. To be speciﬁc, we write
all sums over n so that the power of u is n. This may require redeﬁning the
summation index. So, the last term of the DE can be expressed as
αP(u) =
∞

n=0
αanun.
(26.11)
The term involving the ﬁrst derivative is
−2udP
du = −2u
∞

n=1
nanun−1 = −
∞

n=1
2nanun
(26.12)
and the term involving the second derivative becomes
(1 −u2)d2P
du2 = (1 −u2)
∞

n=2
n(n −1)anun−2
=
∞

n=2
n(n −1)anun−2 −
∞

n=2
n(n −1)anun
(26.13)
=
∞

n=0
(n + 2)(n + 1)an+2un −
∞

n=2
n(n −1)anun,
where in the ﬁrst sum we replaced n with n+2 to change the power of u from
n −2 to n.8 The power of u in all the sums is now n.
8The phrase “in the ﬁrst sum, we replaced n with n+2” is an abbreviation for a procedure
whereby ﬁrst a new dummy index m is deﬁned by m = n −2 (or n = m + 2), and then it
is changed back to n.

612
Laplace’s Equation: Spherical Coordinates
The next step is to separate a suﬃcient number of terms of the “longer
sums” so that all sums start with the same n corresponding to the shortest
sum. In the case at hand, the shortest sum is the one that starts with n = 2.
So, rewrite the sums in Equations (26.11), (26.12), and (26.13) as
αP(u) = αa0 + αa1u +
∞

n=2
αanun,
−2udP
du = −2a1u −
∞

n=2
2nanun,
(1 −u2)d2P
du2 = 2a2 + 6a3u +
∞

n=2
(n + 2)(n + 1)an+2un −
∞

n=2
n(n −1)anun.
Adding these sums and noting that the LHS is zero gives
0 = αa0 + 2a2 + (αa1 −2a1 + 6a3)u
+
∞

n=2
[αan −2nan + (n + 2)(n + 1)an+2 −n(n −1)an]



=[α−n(n+1)]an+(n+2)(n+1)an+2
un.
By setting the coeﬃcients of all powers of u equal to zero, we obtain
αa0 + 2a2 = 0,
(α −2)a1 + 6a3 = 0,
[α −n(n + 1)]an + (n + 2)(n + 1)an+2 = 0,
with the ﬁrst two being special cases of the last one, which in turn happens
to be the recursion relation (26.10).
Equation (26.10) is at the heart of the solution to the Legendre DE. It
generates all the an’s with even n from a0, and all the odd an’s from a1. We
derive a general formula for even an’s, and leave the odd case to the reader.
For n = 0, Equation (26.10) gives a2 = −(α/2)a0, and for n = 2 we obtain
a4 = 2 · 3 −α
4 · 3
a2 = 2 · 3 −α
4 · 3

−α
2

= α(α −2 · 3)
4!
a0.
Similarly, for n = 4, we get
a6 = 4 · 5 −α
6 · 5
a4 = −α(α −2 · 3)(α −4 · 5)
6!
a0.
The reader may easily check that
a8 = α(α −2 · 3)(α −4 · 5)(α −6 · 7)
8!
a0.
All these equations show a pattern that can be generalized to
a2n = (−1)n α(α −2 · 3)(α −4 · 5) · · · [α −(2n −2)(2n −1)]
(2n)!
a0.
(26.14)

26.2 Legendre Polynomials
613
Similarly, the odd terms can be calculated with the result
a2n+1 = (−1)n (α −2)(α −3 · 4)(α −5 · 6) · · · [α −(2n −1)2n]
(2n + 1)!
a1.
(26.15)
Inserting these coeﬃcients in the series expansion of P(u), we obtain
P(u) = a0
∞

n=0
(−1)n α(α −2 · 3)(α −4 · 5) · · · [α −(2n −2)(2n −1)]
(2n)!
u2n
+ a1
∞

n=0
(−1)n (α −2)(α −3 · 4)(α −5 · 6) · · · [α −(2n −1)2n]
(2n + 1)!
u2n+1.
(26.16)
If either of the series in Equation (26.16) is to have a physical utility, it
must be convergent. The appearance of (−1)n may lead us to believe that
the series is alternating. This is not true, because the terms involving α could
The generalized
ratio test shows
that either of the
series in Equation
(26.16) diverges.
be positive as well as negative. So, we cannot use the alternating series test.
Let us use the ratio test. We apply this ratio test to the even series, the odd
series calculation is identical. Calling the entire nth term of the series cn, we
have
lim
n→∞




cn+1
cn




 = lim
n→∞




a2n+2u2n+2
a2nu2n




 = lim
n→∞




2n(2n + 1) −α
(2n + 1)(2n + 2)




 u2 = u2.
So, when u2 < 1, the series converges. Recall that u = cos θ, and θ = 0, π
are points of physical interest corresponding to u = ±1. Therefore, the series
ought to converge there. In this case we cannot decide about the convergence
of the series based on the ratio test. Let us apply the generalized ratio test.
Then, for very large n, we have




cn+1
cn




u2=1
=




2n(2n + 1) −α
(2n + 1)(2n + 2)




 =




n
n + 1 −
α
(2n + 1)(2n + 2)




≈1 −
1
n + 1 ≈1 −1
n,
and the generalized ratio test implies divergence for the series! This conclusion
holds for both the even and odd series of (26.16).
There is a way of making the series convergent. Recall that the parameter
α is completely arbitrary. In particular, we can—if it is helpful—put restric-
To make the series
convergent,
truncate it into a
ﬁnite sum!
tions on it. Can we choose α in such a way that the series converges? We
note that as long as the series is inﬁnite, we have no luck because we get back
to the generalized ratio test and divergence. However, if we choose α so that
all an’s after a certain ﬁnite number of terms vanish, then the series turns
into a ﬁnite sum, and P(u) becomes a polynomial for which the question of
convergence is irrelevant. So, let us assume that a0, a2, and all the other a’s
up to a2k can have nonzero values, but all the remaining coeﬃcients are to

614
Laplace’s Equation: Spherical Coordinates
be zero. All we need to do is to choose α so that a2k+2 vanishes; then the
recursion relation guarantees the vanishing of a2k+4, a2k+6, etc. Since
a2k+2 = 2k(2k + 1) −α
(2k + 1)(2k + 2)a2k,
we must choose α = 2k(2k + 1). A similar argument yields α = (2k −1)2k
for the odd series.
Choosing α to turn one of the inﬁnite sums into a ﬁnite polynomial is only
a partial solution to the problem. Is it possible to choose α so that both the
odd and the even series are truncated after a ﬁnite number of terms? Suppose
we have chosen α to be 2k(2k +1), so that the even series has no term beyond
the (2k)th term. The recursion relation for the odd series can be written as
a2n+1 = (2n −1)2n −2k(2k + 1)
(2n + 1)2n
a2n−1.
Setting the numerator equal to zero gives a quadratic equation which can be
solved for n to obtain
n = −k
or
n = k + 1
2.
Neither of these is a positive integer! Thus, the value of α chosen to truncate
the even series does not allow the truncation of the odd series.
To avoid
this dilemma, we resort to a choice of another arbitrary constant, a1. By
setting a1 equal to zero, we completely avoid the odd series. Conversely, if
α = (2k −1)2k—chosen to truncate the odd series—then a0 ought to be set
equal to zero. By convention, a0 and a1 are determined so that P(1) = 1. Let
us summarize our ﬁndings:
Theorem 26.2.1. A solution to the Legendre DE (26.5) exists only if α =
k(k + 1) where k is a nonnegative integer.
The corresponding solution is
denoted by Pk(u) and is a polynomial of degree k, called the kth Legendre
polynomial, which has only even powers of u if k is even and odd powers of
u if k is odd. By convention Pk(1) = 1 for all k.
Thus, for each k we have a diﬀerent solution, and a diﬀerent a0 or a1 to
evaluate. That is why it is more appropriate to write Ck for either a0 or a1.
We can use either (26.14) and (26.15), or the recursion relation (26.10) to ﬁnd
the coeﬃcients of each polynomial.
Example 26.2.2. We calculate the Legendre polynomials up to order 4 using the
calculation of the
ﬁrst ﬁve Legendre
polynomials using
the recursion
relation
recursion relation. P0 is of degree zero, so it is a constant and Pk(1) = 1 forces it
to be 1. So, P0(u) = 1. Since, P1(u) is of degree 1 with no even “powers” of u, it
can be only of the form C1u where C1 is a constant. But P1(1) = 1; so C1 = 1 and
P1(u) = u. For P2, α = 2 · 3 = 6, and the recursion relation gives
a2 = −α
2 a0 = −3C2 ⇒P2(u) = C2 −3C2u2

26.2 Legendre Polynomials
615
because P2(u) has no u term. For P2(1) to be equal to 1, we must have C2 = −1
2,
so that P2(u) = 1
2(3u2 −1). For P3, α = 3 · 4 = 12, and the recursion relation gives
a3 = 2 −α
6
a1 = 2 −12
6
C3 = −5
3C3 ⇒P3(u) = C3u −5
3C3u3
because P3(u) has no constant or u2 term. For P3(1) to be equal to 1, we must
have C3 = −3
2, so that P3(u) =
1
2(5u3 −3u). Finally, we calculate P4 for which
α = 4 · 5 = 20, and the recursion relations give
a2 = −20
2 a0 = −10C4
and
a4 = 6 −20
12
a2 = 35
3 C4
and P4(u) = C4 −10C4u2 −35
3 C4u4. The condition P4(1) = 1 gives C4 = 3/8.
Therefore,
P4(u) = 1
8(35u4 −30u2 + 3).
Other Legendre polynomials can be obtained similarly. However, as we shall see
shortly, there is a much easier way of calculating Legendre polynomials.
■
With α determined to be of the form k(k + 1), we can now calculate all
coeﬃcients of the Legendre polynomials. We start by rewriting the recursion
relation (26.10) as
an = (n −2)(n −1) −k(k + 1)
n(n −1)
an−2
= −(k −n + 2)(k + n −1)
n(n −1)
an−2,
n = 2, 3, . . . , k.
(26.17)
Iterating this once, we obtain
an = (−1)2 (k −n + 2)(k + n −1)
n(n −1)
(k −n + 4)(k + n −3)
(n −2)(n −3)
an−4
= (−1)2 [(k −n + 2)(k −n + 4)][(k + n −1)(k + n −3)]
n(n −1)(n −2)(n −3)
an−4.
By iterating a few times, the reader may check that
an = (−1)m[(k −n + 2)(k −n + 4) · · · (k −n + 2m)]
· [(k + n −1)(k + n −3) · · · (k + n −2m + 1)]
n(n −1) · · · (n −2m + 1)
an−2m.
(26.18)
To proceed, we need to take the two cases of even and odd n separately.
We treat the even case and leave the odd case as an exercise for the reader.
Let us assume that n = 2m, then k must also be even9 and the last equation
above yields
a2m = (−1)m [(2j) · · · (2j −2m + 2)][(2j + 2m −1) · · · (2j + 1)]
2m(2m −1) · · · 1
a0
= (−1)m [(2j)!!/(2j −2m)!!][(2j + 2m −1)!!/(2j −1)!!]
(2m)!
a0,
9Recall from our discussion above that even Legendre polynomials correspond to even
α = 2j(2j + 1).

616
Laplace’s Equation: Spherical Coordinates
where we set k = 2j. Using the relations (see Problem 11.1)
(2l −1)!! = (2l)!
2ll! ,
(2l)!! = 2ll!,
we ﬁnally obtain
a2m = (−1)mj!
2j
(2j + 2m)!
(j + m)!(j −m)!
1
(2m)!a0
≡Aj(−1)m
(2j + 2m)!
(j + m)!(j −m)!
1
(2m)!,
(26.19)
where Aj = a0(j!/2j). The reader may check that
a2m+1 = Bj(−1)m
(2j + 2m + 2)!
(j + m + 1)!(j −m)!
1
(2m + 1)!
(26.20)
for some constant Bj. Therefore, the even Legendre polynomials will be given
even Legendre
polynomial
by
P2j(x) = Aj
j

m=0
(−1)m
(2j + 2m)!
(j + m)!(j −m)!
x2m
(2m)!
(26.21)
and the odd polynomials by
odd Legendre
polynomial
P2j+1(x) = Bj
j

m=0
(−1)m
(2j + 2m + 2)!
(j + m + 1)!(j −m)!
x2m+1
(2m + 1)!.
(26.22)
We now introduce a new summation index r = j −m in either sum and let
n = 2j in the even sum and n = 2j + 1 in the odd sum. Then both sums can
be written simply as
Pn(x) = Kn
[n/2]

r=0
(−1)r (2n −2r)!
(n −r)!r!
xn−2r
(n −2r)!,
(26.23)
where [a]—for any real number a—denotes the largest integer less than or
equal to a, and Kn is an arbitrary constant which, by convention, is taken to
be 1/2n so that Pn(1) = 1. This leads to
Pn(x) = 1
2n
[n/2]

r=0
(−1)r (2n −2r)!
(n −r)!r!
xn−2r
(n −2r)!.
(26.24)
Referring to the deﬁnition of the hypergeometric function (11.23) and
Equation (26.24), the reader may verify that
P2n(x) = (−1)n
(2n)!
22n(n!)2 F(−n, n + 1
2; 1
2; x2)
(26.25)

26.3 Second Solution of the Legendre DE
617
and
P2n+1(x) = (−1)n (2n + 1)!
22n(n!)2 xF(−n, n + 3
2; 3
2; x2).
(26.26)
Historical Notes
Adrien-Marie Legendre came from a well-to-do Parisian family and received an
excellent education in science and mathematics. His university work was advanced
enough that his mentor used many of Legendre’s essays in a treatise on mechanics.
A man of modest fortune until the revolution, Legendre was able to devote himself
to study and research without recourse to an academic position. In 1782 he won the
prize of the Berlin Academy for calculating the trajectories of cannonballs taking
air resistance into account. This essay brought him to the attention of Lagrange
and helped pave the way to acceptance in French scientiﬁc circles, notably the
Academy of Sciences, to which Legendre submitted numerous papers. In July 1784
he submitted a paper on planetary orbits that contained the now-famous Legendre
polynomials, mentioning that Lagrange had been able to “present a more complete
theory” in a recent paper by using Legendre’s results. In the years that followed,
Legendre concentrated his eﬀorts on number theory, celestial mechanics, and the
theory of elliptic functions.
In addition, he was a proliﬁc calculator, producing
large tables of the values of special functions, and he also authored an elementary
textbook that remained in use for many decades. In 1824 Legendre refused to vote
for the government’s candidate for the Institut National. Because of this, his pension
was stopped and he died in poverty and in pain at the age of 80 after several years
of failing health.
Adrien-Marie
Legendre
1752–1833
Legendre produced a large number of useful ideas but did not always develop
them in the most rigorous manner, claiming to hold the priority for an idea if
he had presented merely a reasonable argument for it. Gauss, with whom he had
several quarrels over priority, considered rigorous proof the standard of ownership.
To Legendre’s credit, however, he was an enthusiastic supporter of his young rivals
Abel and Jacobi and gave their work considerable attention in his writings.
Legendre also contributed to practical eﬀorts in science and mathematics. He
and two of his contemporaries were assigned in 1787 to a panel conducting geodetic
work in cooperation with the observatories at Paris and Greenwich.
Four years
later the same panel members were appointed as the Academy’s commissioners to
undertake the measurements and calculations necessary to determine the length of
the standard meter. Legendre’s seemingly tireless skill at calculating produced large
tables of the values of trigonometric and elliptic functions, logarithms, and solutions
to various special equations.
26.3
Second Solution of the Legendre DE
Recall that any second order linear DE has two bases of solutions. We have
so far found one solution of Legendre DE in the form of the Legendre poly-
nomials. Once we have these solutions, we can obtain a second solution using
Equation (24.6). To conform with Equation (24.6), we need to reexpress the
Legendre DE as
d2y
dx2 −
2x
1 −x2
dy
dx + n(n + 1)
1 −x2 y = 0.

618
Laplace’s Equation: Spherical Coordinates
This is an homogeneous second order linear DE with
p(x) = −
2x
1 −x2
and
q(x) = n(n + 1)
1 −x2 .
Using Pn(x) as our input, we can generate another set of solutions. Let Qn(x)
stand for the linearly independent “partner” of Pn(x). Then, setting C = 0
in Equation (24.6) yields10
Qn(x) = KPn(x)
# x
α
1
P 2n(s) exp
# s
c
2t
1 −t2 dt

ds.
But
# s
c
2t
1 −t2 dt = −ln |1 −t2|


s
c = −ln




1 −s2
1 −c2




 = ln




1 −c2
1 −s2




so that
exp
# s
c
2t
1 −t2 dt

ds = exp

ln




1 −c2
1 −s2





=




1 −c2
1 −s2




 = |1 −c2|
1 −s2 ,
because s, being the argument of a Legendre polynomial, is the cosine of an
angle and therefore cannot exceed 1 so that 1 −s2 ≥0. It now follows that
Qn(x) = AnPn(x)
# x
α
ds
(1 −s2)P 2n(s),
(26.27)
where An ≡K|1 −c2| is an arbitrary constant determined by convention, and
α is an arbitrary point in the interval [−1, +1]. The subscript for An indicates
that the constant may be diﬀerent for diﬀerent n. These new solutions are
called Legendre functions of the second kind. Note that, contrary to
Legendre
functions of the
second kind
Pn(x), Qn(x) is not well behaved at x = ±1 due to the presence of 1 −s2
in the denominator of the integrand of Equation (26.27). For this reason, we
shall not use these second solutions in this book.
Example 26.3.1. Example 26.2.2 gives P0(x) = 1. Therefore,
Q0(x) = A0
# x
α
ds
1 −s2 = A0
2
# x
α

1
1 + s +
1
1 −s

ds
= A0
1
2 ln




1 + x
1 −x




 −1
2 ln




1 + α
1 −α





.
The standard form of Q0(x) is obtained by setting A0 = 1 and α = 0:
Q0(x) = 1
2 ln




1 + x
1 −x




for
|x| < 1.
Similarly, since P1(x) = x, we obtain
Q1(x) = A1x
# x
α
ds
(1 −s2)s2 = Ax + Bx ln




1 + x
1 −x




 + C
for
|x| < 1,
10Since we are interested in a diﬀerent second solution, we can ignore any constant
multiple of the ﬁrst solution that is added to the sought-after second solution.

26.4 Complete Solution
619
where A, B, and C are constants, and to perform the integration, we used
1
(1 −s2)s2 = 1
s2 +
1
2(1 −s) +
1
2(1 + s),
which renders the integral elementary. In the case of Q1(x), convention demands
that A = 0, B = 1
2, and C = −1. Thus,
Q1(x) = 1
2x ln




1 + x
1 −x




 −1.
■
26.4
Complete Solution
Having found the angular solution of Laplace’s equation, we now tackle the
radial part. With α = k(k + 1), we can write the ﬁrst equation in (26.2) as
r2 d2R
dr2 + 2rdR
dr −k(k + 1)R = 0.
(26.28)
Since p2(0) = 0, we have to consider a solution of the form R(r) = rs ∞
n=0 bnrn.
Diﬀerentiating this series and substituting it in Equation (26.28) gives
∞

n=0
[(n + s)(n + s + 1) −k(k + 1)]bnrn+s = 0
or
[(n + s)(n + s + 1) −k(k + 1)]bn = 0
for
n = 0, 1, 2, . . ..
In particular, for n = 0, and assuming that b0 ̸= 0, we obtain the indicial
equation
an example of the
indicial equation
s(s + 1) −k(k + 1) = 0 ⇒s = k or s = −k −1.
For s = k, the equation for general nonzero n gives
[(n + k)(n + k + 1) −k(k + 1)]bn = 0 ⇒n(n + 2k + 1)bn = 0.
Since neither n nor n + 2k + 1 is zero, we have to conclude that bn = 0 for all
n ≥1. Thus, for s = k, we obtain the solution R(r) = Akrk where Ak is an
arbitrary constant (we called it b0 before).
For s = −k −1, we have
[(n −k −1)(n −k) −k(k + 1)]bn = 0 ⇒n(n −2k −1)bn = 0
for which we can have either n = 2k + 1 or bn = 0. If n = 2k + 1, then
R(r) = r−k−1b2k+1r2k+1 = b2k+1rk,
which is (a constant times) what we already have. So assume that n ̸= 2k +1.
Then bn = 0 for all n ≥1, and we obtain the solution R(r) = Bkr−k−1 where

620
Laplace’s Equation: Spherical Coordinates
Bk is another arbitrary constant. It follows that the most general solution of
the radial DE is
the most general
solution of the
spherical radial DE
Rk(r) ≡Akrk + Bk
rk+1 ,
k = 0, 1, 2, . . ..
We can now put the radial and the angular parts together:
Theorem 26.4.1. To ﬁnd the most general azimuthally symmetric solution
of Laplace’s equation in spherical coordinates, we multiply the radial solution
and the angular solution (Legendre polynomial) for each k and sum over all
possible values of k:
Φ(r, θ) =
∞

k=0

Akrk + Bk
rk+1

Pk(cos θ),
(26.29)
where we have substituted cos θ for u.
Equation (26.29) gives the general solution of Laplace’s equation, and we
From a known
solution of
Laplace’s
equation, we ﬁnd
a formula that
generates all
Legendre
polynomials.
shall consider examples of how to use it to solve some representative problems,
but ﬁrst we will go backward: From a particular known solution of Laplace’s
equation, we want to ﬁnd an important property of Legendre polynomials.
Equation (15.18) shows that 1/|r −r0| is a solution of Laplace’s equation at
all points of space except r0. In general, |r−r0| is not azimuthally symmetric.
However, if we place r0 along the z-axis, the ϕ-dependence will disappear. In
fact, with r0 = aˆez, we have
|r −r0| = |r −aˆez| =
	
r2 + a2 −2ar cos θ.
According to (26.29) the solution 1/|r −aˆez| can be written as a series:
1
√
r2 + a2 −2ar cos θ
=
∞

k=0

Akrk + Bk
rk+1

Pk(cos θ).
We are interested in the region of space inside the sphere of radius a. Since
the origin is included in this region, no negative powers of r are allowed.
Therefore, all coeﬃcients of such powers must be zero, i.e., Bk = 0.
To
determine the other set of coeﬃcients, evaluate both sides at θ = 0 and use
Pk(1) = 1. This gives
1
√
r2 + a2 −2ar
=
1
|r −a| =
1
a −r =
∞

k=0
Akrk.
Using the result of Example 9.3.3 and the fact that r/a < 1, the LHS can be
expanded in powers of r/a:
1
a −r =
1
a(1 −r/a) = 1
a
∞

k=0
r
a
k
=
∞

k=0
rk
ak+1 .

26.4 Complete Solution
621
Comparison of the last two equations gives Ak = 1/ak+1. It follows that
1
√
r2 + a2 −2ar cos θ
= 1
a
∞

k=0
r
a
k
Pk(cos θ),
r < a.
Introducing t ≡r/a and u ≡cos θ on both sides, we ﬁnally obtain the impor-
tant relation
g(t, u) ≡
1
√
1 + t2 −2tu
=
∞

k=0
tkPk(u).
(26.30)
The RHS can be considered as a Taylor (or Maclaurin) series in t for the
function on the LHS.
Theorem 26.4.2. The kth coeﬃcient of the Maclaurin series expansion of
g(t, u) ≡1/
√
1 + t2 −2tu about t = 0 is Pk(u). Speciﬁcally,
Pk(u) = 1
k!
∂k
∂tk
1
√
1 + t2 −2tu




t=0
.
(26.31)
The function g(t, u) is called the generating function of the Legendre poly-
nomials.
Example 26.4.3. As an immediate application of the generating function to
potential theory, consider the electrostatic or gravitational potential which can be
written as
Φ(r) = K
##
Ω
dQ(r′)
|r −r′|,
(26.32)
where K is ke for electrostatics and −G for gravity, and Q represents either electric
charge or mass. Assuming that r ≫r′, we can expand in powers of the ratio r′/r
which we denote by t. The key to this expansion is the following power series of
1/|r −r′|:
Legendre
polynomial and
multipole
expansion
1
|r −r′| =
1
√
r2 + r′2 −2r · r′ = 1
r
1
	
1 + t2 −2t cos γ
= 1
r
∞

k=0
tkPk(cos γ),
where γ is the angle between r and r′ and we used Equation (26.30). Substituting
this expansion for 1/|r −r′| in (26.32), we obtain
Φ(r) = K
##
Ω
∞

k=0
r′k
rk+1 Pk(cos γ) dQ(r′) = K
∞

k=0
Qk
rk+1 ,
(26.33)
where we replaced t with r′/r and introduced Qk, the so-called k-th moment of
source (charge or mass), by11
Qk ≡
##
Ω
r′kPk(cos γ) dQ(r′).
(26.34)
11Do not confuse this Qk with the second solution of Legendre DE introduced in Equation
(26.27).

622
Laplace’s Equation: Spherical Coordinates
Recall that cos γ depends on θ and ϕ. Thus, once the integral over Ω is done, the
result will depend on θ and ϕ as it should, because Φ(r) is, in general, dependent
on these angles.
The moments Qk are supposed to describe the intrinsic properties of charge (or
mass) distributions and should not depend on the observation point—described, in
part, by θ and ϕ. This is the reason that Cartesian coordinates are more useful—at
this level of presentation—than spherical coordinates. In Cartesian coordinates, we
can separate the primed from the unprimed coordinates (as we did in the deﬁnition
of dipole in Chapter 10 and of quadrupole in Chapter 17), and deﬁne multipole
moments entirely in terms of the density function of the distribution of the source.
This does not mean, however, that a complete separation is impossible in spherical
coordinates. In fact, there are techniques of performing such a separation—in terms
of the so-called “spherical harmonics”—but they are much more complicated and
beyond the scope of this book.12
■
26.5
Properties of Legendre Polynomials
From the Legendre DE, the generating function, and other formulas derived
earlier, one can obtain a variety of relations connecting Legendre polynomials.
26.5.1
Parity
The easiest property to obtain is parity which is the content of the following
formula:
Pk(−u) = (−1)kPk(u).
(26.35)
This is a direct consequence of the fact that Pk(u) has only even powers of u
if k is even, and odd powers if k is odd.
26.5.2
Recurrence Relation
Diﬀerentiate both sides of Equation (26.30) with respect to t to obtain
u −t
(1 + t2 −2tu)3/2 =
∞

k=1
ktk−1Pk(u).
(26.36)
Rewrite the LHS as
u −t
1 + t2 −2tu
1
√
1 + t2 −2tu
=
u −t
1 + t2 −2tu
∞

k=0
tkPk(u),
(26.37)
where we used (26.30) for the term with the square root. Equating the RHS
of (26.37) with the RHS of (26.36) and multiplying the result by 1 + t2 −2tu
yields
(t −u)
∞

k=0
tkPk(u) + (1 + t2 −2tu)
∞

k=1
ktk−1Pk(u) = 0
12See Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, Chapter 12 for a discussion of spherical harmonics.

26.5 Properties of Legendre Polynomials
623
or
∞

k=0
tk+1Pk(u) −u
∞

k=0
tkPk(u) +
∞

k=1
ktk−1Pk(u)
+
∞

k=1
ktk+1Pk(u) −2u
∞

k=1
ktkPk(u) = 0.
All the coeﬃcients of powers of t must vanish.
To ﬁnd these coeﬃcients,
change the dummy index in each sum so that all sums will have the same
power of t. So, let k = n −1 in the ﬁrst and fourth sums, k = n in the second
and the last sums, and k = n + 1 in the third sum. Then the above equation
can be written as
∞

n
[Pn−1(u) −uPn(u) + (n + 1)Pn+1(u)
+(n −1)Pn−1(u) −2unPn(u)]tn = 0,
where we have purposefully left out the lower limit of summation because
diﬀerent sums start at diﬀerent initial values of n. Since a power series is zero
only if all its coeﬃcients are zero, we set the coeﬃcients of the series above
equal to zero to obtain
recurrence relation
for Legendre
polynomial
(2n + 1)uPn(u) = (n + 1)Pn+1(u) + nPn−1(u),
n = 1, 2, 3, . . ..
(26.38)
Using P0(u) = 1 and P1(u) = u, one can generate all Legendre polynomials
from Equation (26.38).
Example 26.5.1. For n = 1, Equation (26.38) gives
3uP1(u) = 2P2(u) + P0(u) ⇒3u2 = 2P2(u) + 1 ⇒P2(u) = 1
2(3u2 −1)
For n = 2, Equation (26.38) gives
5uP2(u) = 3P3(u) + 2P1(u) ⇒
5
2u(3u2 −1) = 3P3(u) + 2u ⇒P3(u) = 1
2(5u3 −3u),
and so on.
■
The recurrence relation can be used to obtain Pn(0) which is a useful
quantity. We quote the result and leave the details as an exercise for the
reader. For odd n, we have Pn(0) = 0. The result for the even case is
P2n(0) = (−1)n (2n −1)!!
(2n)!!
= (−1)n
(2n)!
22n(n!)2 .
(26.39)
Example 26.5.2. We can also obtain Pn(0) by letting u = 0 in Equation (26.30):
(1 + t2)−1/2 =
∞

k=0
tkPk(0).

624
Laplace’s Equation: Spherical Coordinates
The binomial expansion of the LHS gives [see Equation (10.15)]
(1 + t2)−1/2 = 1 +
∞

n=1
(−1
2)(−1
2 −1) · · · (−1
2 −n + 1)
n!
 
t2!n
= 1 +
∞

n=1
(−1)n
1
2( 1
2 + 1) · · · (n −1
2)
n!
t2n
= 1 +
∞

n=1
(−1)n 1 · 3 · · · (2n −1)
2nn!
t2n.
Comparing this with the RHS of the ﬁrst equation, we see that Pn(0) = 0 when n
is odd and that
P2n(0) = (−1)n 1 · 3 · · · (2n −1)
2nn!
= (−1)n (2n −1)!!
2nn!
which is the same as (26.39) because 2nn! = (2n)!! by Problem 11.1.
■
26.5.3
Orthogonality
The most useful property of the Legendre polynomials is their orthogonality.
We have already seen in Chapters 6 and 7 how dot products can be deﬁned
for polynomials. We now show that Legendre polynomials of diﬀerent orders
are necessarily orthogonal once the dot product is deﬁned in terms of suitable
integrals (also see Example 24.5.3). Write the Legendre DE for Pn and Pm as
d
du[(1 −u2)P ′
n(u)] + n(n + 1)Pn(u) = 0,
d
du[(1 −u2)P ′
m(u)] + m(m + 1)Pm(u) = 0,
where the prime indicates derivative. Multiply both sides of the ﬁrst equation
by Pm(u) and the second equation by Pn(u) and integrate from −1 to +1:
# 1
−1
d
du[(1−u2)P ′
n(u)]Pm(u) du + n(n+1)
# 1
−1
Pn(u)Pm(u) du=0,
# 1
−1
d
du[(1−u2)P ′
m(u)]Pn(u) du + m(m+1)
# 1
−1
Pm(u)Pn(u) du=0.
(26.40)
Use integration by parts to write the ﬁrst integral as
# 1
−1
d
du[(1 −u2)P ′
n(u)]Pm(u) du = (1 −u2)P ′
n(u)Pm(u)


1
−1



=0 because of (1−u2)
−
# 1
−1
[(1 −u2)P ′
n(u)]P ′
m(u) du.

26.5 Properties of Legendre Polynomials
625
The ﬁrst integral of the second line of Equation (26.40) gives exactly the same
result. Therefore, if we subtract the two equations of (26.40), we obtain
[n(n + 1) −m(m + 1)]
# 1
−1
Pn(u)Pm(u) du = 0.
It now follows that
Theorem 26.5.3. If m ̸= n, then
 1
−1 Pn(u)Pm(u) du = 0, i.e., if the inner
product is deﬁned as an integral from −1 to +1, then Legendre polynomials of
diﬀerent orders are orthogonal.
We put this orthogonality relation to immediate use. Square both sides
of Equation (26.30) keeping in mind to introduce a new dummy index when
multiplying the sums, and integrate the result from −1 to +1:
# 1
−1
du
1 + t2 −2tu =
# 1
−1
 ∞

k=0
tkPk(u)
  ∞

m=0
tmPm(u)

du.
(26.41)
On the RHS, we switch the order of summation and integration:
RHS =
∞

k=0
∞

m=0
tm+k
# 1
−1
Pk(u)Pm(u) du



=0 unless m=k
.
As we perform the inner sum, by Theorem 26.5.3, all terms will vanish except
one, i.e., only when m = k. So, the double sum reduces to a single sum
RHS =
∞

k=0
t2k
# 1
−1
P 2
k (u) du.
The integral on the LHS of (26.41) can be done by substituting y = 1+t2−2tu
and dy = −2t du:
LHS = −1
2t
# (1−t)2
(1+t)2
dy
y = 1
t [ln(1 + t) −ln(1 −t)].
The two natural log terms can be expanded using Equation (10.23).
The
reader may check that
1
t [ln(1 + t) −ln(1 −t)] = 2
∞

k=0
t2k
2k + 1.
(26.42)
The fact that only even powers of t are present could have been anticipated
because the function on the LHS of Equation (26.42) is even in t. Equating
the RHS and the LHS of (26.41), we obtain
2
∞

k=0
t2k
2k + 1 =
∞

k=0
t2k
# 1
−1
P 2
k (u) du.

626
Laplace’s Equation: Spherical Coordinates
For these two power series in t to be equal, their coeﬃcients must equal:
# 1
−1
P 2
k (u) du =
2
2k + 1.
Combining this with the orthogonality relation of Theorem 26.5.3 and using
the Kronecker delta introduced in Equation (7.9), we have
orthogonality
relation of
Legendre
polynomials
# 1
−1
Pm(u)Pn(u) du =
2
2n + 1δmn.
(26.43)
26.5.4
Rodrigues Formula
We started our discussion of Legendre polynomials by representing them as
inﬁnite series and then truncating the series due to physical restrictions. We
noted that the recursion relation obtained by the Frobenius method gave all
the coeﬃcients of the polynomials in terms of a0 and a1. Later, we found
a “closed” expression for all Legendre polynomials in terms of derivatives of
the generating functions which is a very useful function as the derivation of
(26.43) demonstrated.
There is another “closed” expression of Legendre polynomials which we
shall discuss now. This expression is called the Rodrigues formula and is
given by13
Rodrigues formula
Pn(x) =
1
2nn!
dn
dxn
.
(x2 −1)n/
.
(26.44)
To see that the RHS indeed gives the nth Legendre polynomial, we show
that it satisﬁes the corresponding Legendre DE. The most elegant way to
show this is to resort to complex analysis where derivatives are represented
as integrals [see Equation (19.10)]. Thus, for f(z) = (z2 −1)n, the Cauchy
integral formula gives
(z2 −1)n =
1
2πi
2
C
(ξ2 −1)n
(ξ −z) dξ,
and Equations (19.10) and (26.44) yield
Pn(z) =
1
2nn!
dn
dzn
.
(z2 −1)n/
=
1
2n(2πi)
2
C
(ξ2 −1)n
(ξ −z)n+1 dξ.
To ﬁnd P ′
n(z) and P ′′
n (z), we diﬀerentiate the integral, carrying the derivative
inside and letting it diﬀerentiate the denominator:
dP
dz =
1
2n(2πi)
2
C
d
dz
 (ξ2 −1)n
(ξ −z)n+1

dξ =
n + 1
2n(2πi)
2
C
(ξ2 −1)n
(ξ −z)n+2 dξ,
d2P
dz2 = d
dz
dP
dz

=
n + 1
2n(2πi)
2
C
d
dz
 (ξ2 −1)n
(ξ −z)n+2

dξ
= (n + 1)(n + 2)
2n(2πi)
2
C
(ξ2 −1)n
(ξ −z)n+3 dξ.
13The fact that we are using x, rather than u, as the argument of the Legendre polynomial
should not cause any confusion.

26.5 Properties of Legendre Polynomials
627
Substituting these expressions in the DE, the reader may check that
(1 −z2)d2P
dz2 −2z dP
dz + n(n + 1)P
=
n + 1
2n(2πi)
2
C
(ξ2 −1)n[nξ2 −2(n + 1)ξz + n + 2]
(ξ −z)n+3
dξ.
The reader may also verify that
(ξ2 −1)n[nξ2 −2(n + 1)ξz + n + 2]
(ξ −z)n+3
= d
dξ
(ξ2 −1)n+1
(ξ −z)n+2

,
so that the integrand is the derivative of a function. Since the contour of
integration is closed, the lower and upper limits of integration coincide and
the integral vanishes. So, the Rodrigues formula indeed yields Legendre poly-
nomials.
Example 26.5.4. As an illustration of the use of the Rodrigues formula, let us
evaluate the integral
I =
# 1
−1
xkPn(x) dx
for
k ≤n.
The procedure is to replace Pn(x) by the RHS of Equation (26.44) and integrate by
parts repeatedly. After one integration by parts, we get
I =
1
2nn!
# 1
−1
xk

u
dn
dxn
.
(x2 −1)n/
dx



dv
=
1
2nn!
0
xk dn−1
dxn−1
.
(x2 −1)n/



1
−1
−
# 1
−1
kxk−1 dn−1
dxn−1
.
(x2 −1)n/
dx
1
.
The ﬁrst term on the RHS of the second line is zero because each diﬀerentiation
reduces the power of (x2 −1)n by at most one unit. So after n −1 diﬀerentiations,
we get a sum of terms each having (x2 −1) raised to various powers, with the lowest
power being one. All these terms vanish at x = 1 as well as at x = −1. Continuing
the integration by parts, we get
I = −kxk−1
2nn!
dn−2
dxn−2
.
(x2 −1)n/



1
−1



=0 for same reason as above
+(−1)2
2nn!
# 1
−1
k(k −1)xk−2 dn−2
dxn−2
.
(x2 −1)n/
dx.
After k integrations by part, we obtain
I = (−1)k
2nn! k!
# 1
−1
dn−k
dxn−k
.
(x2 −1)n/
dx
because after k diﬀerentiations xk yields k!. Now, if k < n, the integral vanishes for
the same reason as above. If n = k, no diﬀerentiation will be left, and we have
I = (−1)n
2nn! n!
# 1
−1
(x2 −1)n dx.

628
Laplace’s Equation: Spherical Coordinates
Problem 26.14 shows how to evaluate the ﬁnal integral and obtain
# 1
−1
(x2 −1)n dx = (−1)n22n+1
(n!)2
(2n + 1)!.
Therefore,
I = (−1)n
2n
# 1
−1
(x2 −1)n dx = 2n+1(n!)2
(2n + 1)! .
We summarize the above derivation
# 1
−1
xkPn(x) dx =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
0
if
k < n,
2n+1(n!)2
(2n + 1)!
if
k = n.
(26.45)
If instead of xk we have a general polynomial of order k in x with n > k, the
integral will still vanish.
■
The result of the preceding example is summarized as
Box 26.5.1. Any polynomial of degree less than n is orthogonal to Pn.
26.6
Expansions in Legendre Polynomials
The orthogonality of Legendre polynomials—as the orthogonality of the Fourier
trigonometric functions—makes them very useful for expansion of functions
deﬁned in the interval (−1, +1). Let f(x) be such a function. Then we write
f(x) =
∞

n=0
cnPn(x)
(26.46)
and seek to ﬁnd cn. But cn can be obtained by multiplying both sides of
the series by Pm(x) and integrating from −1 to +1. On the LHS, we get
 1
−1 f(x)Pm(x) dx, and on the RHS
# 1
−1
 ∞

n=0
cnPn(x)

Pm(x) dx =
∞

n=0
cn
# 1
−1
Pn(x)Pm(x) dx



[2/(2n+1)]δmn by (26.43)
= cm
2
2m + 1.
Equating the RHS and the LHS, we obtain
cm = 2m + 1
2
# 1
−1
f(x)Pm(x) dx
or
cn = 2n + 1
2
# 1
−1
f(x)Pn(x).
(26.47)
Equations (26.46) and (26.47) give a procedure for expanding an arbitrary
function deﬁned in the interval (−1, +1) in terms of Legendre polynomials.

26.6 Expansions in Legendre Polynomials
629
If f(x) happens to be a polynomial of degree k, then it can be written
as a ﬁnite sum of Legendre polynomials of degree k and less. In fact, for
f(x) = xk, we have
cn = 2n + 1
2
# 1
−1
xkPn(x)dx = 0
for
n > k
by Box 26.5.1. Thus the coeﬃcients in the sum (26.46) beyond k are all zero.
Example 26.6.1. We want to ﬁnd the Legendre expansion of a function f(x)
deﬁned as
f(x) =
⎧
⎨
⎩
V0
if
0 < x ≤1,
−V0
if
−1 ≤x < 0.
To ﬁnd the coeﬃcients of expansion, we use Equation (26.47):
cn = 2n + 1
2
# 1
−1
f(x)Pn(x) dx
= 2n + 1
2
# 0
−1
f(x)

=−V0
Pn(x) dx + 2n + 1
2
# 1
0
f(x)

=+V0
Pn(x) dx
(26.48)
= 2n + 1
2
V0

−
# 0
−1
Pn(x) dx +
# 1
0
Pn(x) dx

.
In the ﬁrst integral of the last line, we make the substitution x = −y so that
# 0
−1
Pn(x) dx =
# 0
+1
Pn(−y)(−dy) =
# 1
0
Pn(−y) dy = (−1)n
# 1
0
Pn(x) dx,
where we used (26.35) and, in the last equality, we changed the dummy variable of
integration from y to x (Section 3.2). Inserting this in (26.48), we obtain
cn = 2n + 1
2
V0[1 −(−1)n]
# 1
0
Pn(x) dx,
= 2n + 1
2
V0
⎧
⎨
⎩
0
if
n is even
2
 1
0 P2k+1(x) dx
if
n = 2k + 1
where we have written the odd n as 2k + 1 for k = 0, 1, . . . .
It remains to evaluate the integral of a Legendre polynomial of odd order in the
interval (0, 1). To this end, we use the Rodrigues formula:
# 1
0
P2k+1(x) dx =
1
22k+1(2k + 1)!
# 1
0
d2k+1
dx2k+1
*
(x2 −1)2k+1+
dx
=
1
22k+1(2k + 1)!
d2k
dx2k
*
(x2 −1)2k+1+



1
0
=
1
22k+1(2k + 1)!
( d2k
dx2k
*
(x2 −1)2k+1+



x=1
−d2k
dx2k
*
(x2 −1)2k+1+



x=0
)
.

630
Laplace’s Equation: Spherical Coordinates
The ﬁrst term gives zero because there is no suﬃcient number of diﬀerentiations to
get rid of all factors of (x2 −1). For the second term, we note that (x2 −1)2k+1 is
a polynomial in x whose derivatives of various orders consist of powers of x. These
powers will give zero at x = 0 except for the constant term (of zeroth power). So,
let us use binomial expansion for (x2 −1)2k+1 which is equal to −(1 −x2)2k+1:
d2k
dx2k
*
(x2 −1)2k+1+



x=0
= −d2k
dx2k
A2k+1

j=0
(2k + 1)!
j!(2k + 1 −j)!(−x2)j
B




x=0
= −
2k+1

j=0
(2k + 1)!
j!(2k + 1 −j)!(−1)j d2k
dx2k

x2j




x=0
,
whose constant term is obtained when k = j, all the other terms of the sum will
vanish either because of too many diﬀerentiations (when j < k, we end up diﬀeren-
tiating constants) or too few diﬀerentiations (when j > k, a power of x will remain
which evaluates to zero at x = 0). Therefore,
d2k
dx2k
*
(x2 −1)2k+1+



x=0
= −(2k + 1)!
k!(k + 1)!(−1)k d2k
dx2k

x2k



x=0
= (2k + 1)!
k!(k + 1)!(−1)k+1(2k)!
and
# 1
0
P2k+1(x) dx = −
1
22k+1(2k + 1)!
 (2k + 1)!
k!(k + 1)!(−1)k+1(2k)!

=
(−1)k(2k)!
22k+1k!(k + 1)!.
(26.49)
Finally, we can write the coeﬃcient c2k+1 as
c2k+1 = 22(2k + 1) + 1
2
V0
# 1
0
P2k+1(x) dx = (−1)k(4k + 3)(2k)!
22k+1k!(k + 1)!
V0
with cn = 0 for even n. The ﬁnal expansion series can now be given:
f(x) =
⎧
⎨
⎩
V0
if
0 < x ≤1
−V0
if
−1 ≤x < 0
= V0
∞

k=0
(−1)k(4k + 3)(2k)!
22k+1k!(k + 1)!
P2k+1(x)
= V0
. 3
2P1(x) −7
8P3(x) + 11
16P5(x) −· · ·
/
.
■
Example 26.6.2. We can easily obtain the Legendre expansion of the Dirac delta
function. The expansion coeﬃcients are given by
cn = 2n + 1
2
# 1
−1
f(x)Pn(x) dx = 2n + 1
2
# 1
−1
δ(x)Pn(x) dx = 2n + 1
2
Pn(0).
From Equation (26.39) and the discussion preceding it we can ﬁnd all values of
Pn(0). Substituting these values in the above equation, we conclude that cn = 0 if
n is odd, and
c2k = 4k + 1
2

(−1)k
(2k)!
22k(k!)2

.
It now follows that
Legendre
expansion of the
Dirac delta
function
δ(x) =
∞

k=0
(−1)k (4k + 1)(2k)!
22k+1(k!)2
P2k(x).
■

26.7 Physical Examples
631
26.7
Physical Examples
The most common physical problems involving Laplace’s equation are those
from electrostatics in empty space, and steady-state heat transfer. In each
case, a surface is held at some (not necessarily uniform) potential or tem-
perature and the potential or temperature is sought in regions away from
the surface. In the present context, these surfaces are typically (portions of)
spheres.
Example 26.7.1. Two solid heat-conducting hemispheres of radius a, separated
by a very small insulating gap, form a sphere. The two halves of the sphere are
two solid
heat-conducting
hemispheres held
at temperatures
T0 and −T0
in contact—on the outside—with two (inﬁnite) heat baths at temperatures T0 and
−T0 [Figure 26.1(a)]. We want to ﬁnd the temperature distribution T(r, θ, ϕ) inside
the sphere. We choose a spherical coordinate system in which the origin coincides
with the center of the sphere and the polar axis is perpendicular to the equatorial
plane. The hemisphere with temperature T0 is assumed to constitute the northern
hemisphere.
Since the problem has azimuthal symmetry, T is independent of ϕ, and we can
immediately write the general solution from Equation (26.29). However, since the
origin is in the region of interest, we need to exclude all negative powers of r. This
is accomplished by setting all the B coeﬃcients equal to zero. Thus, we have
T(r, θ) =
∞

n=0
AnrnPn(cos θ).
(26.50)
It remains to calculate the constants An. This is done by noting that
T(a, θ) =
⎧
⎪
⎨
⎪
⎩
T0
if
0 ≤θ < π
2 ,
−T0
if
π
2 < θ ≤π.
In terms of u = cos θ, this is written as
T(a, u) =
⎧
⎨
⎩
−T0
if
−1 ≤u < 0,
T0
if
0 < u ≤1.
Substituting this in Equation (26.50), we obtain
T(a, θ) =
⎧
⎨
⎩
−T0
if
−1 ≤u < 0
T0
if
0 < u ≤1
=
∞

n=0
Anan
  
≡cn
Pn(u),
(26.51)
which—except for using u instead of x—is entirely equivalent to the expansion of
Example 26.6.1, where we found that even coeﬃcients are absent and
c2k+1 ≡A2k+1a2k+1 = (−1)k(4k + 3)(2k)!
22k+1k!(k + 1)!
T0.
Finding A2k+1 from this equation and inserting the result in (26.50) yields
T(r, θ) = T0
∞

k=0
(−1)k(4k + 3)(2k)!
22k+1k!(k + 1)!
 r
a
2k+1
P2k+1(cos θ),
(26.52)
where we have substituted cos θ for u.
■

632
Laplace’s Equation: Spherical Coordinates
(b)
(a)
T0
−T0
V0
−V0
Figure 26.1: (a) Two heat-conducting hemispheres held at two diﬀerent temperatures.
(b) Two electrically conducting hemispheres held at two diﬀerent potentials. The upper
hemispheres have the polar angle range 0 ≤θ < π/2 or 0 < cos θ ≤1, and the lower
hemispheres have the range π/2 < θ ≤π or −1 ≤cos θ < 0.
Example 26.7.2. Consider two electrically conducting hemispheres of radius a
two electrically
conducting
hemispheres held
at potentials V0
and −V0
separated by a small insulating gap at the equator. The upper hemisphere is held
at potential V0 and the lower one at −V0 as shown in Figure 26.1(b). We want to
ﬁnd the potential at points outside the resulting sphere. Since the potential must
vanish at inﬁnity, we expect the ﬁrst term in Equation (26.29) to be absent, i.e.,
Ak = 0. To ﬁnd Bk, substitute a for r in (26.29), and let cos θ ≡u. Then,
Φ(a, u) =
∞

k=0
Bk
ak+1
  
≡ck
Pk(u),
where
Φ(a, u) =
0
−V0
if
−1 < u < 0,
+V0
if
0 < u < 1.
The calculation of the coeﬃcients is identical to that of Example 26.6.1. Thus,
ck = 0 for even k and
c2m+1 = B2m+1
a2m+2 = (−1)m (4m + 3)(2m)!
22m+1(m + 1)!m!V0
or
B2m+1 = (−1)m(4m + 3)(2m)!
22m+1m!(m + 1)!
a2m+2V0.
Having found the coeﬃcients, we can write the potential:
Φ(r, θ) = V0
∞

m=0
(−1)m (4m + 3)(2m)!
22m+1m!(m + 1)!
 a
r
2m+2
P2m+1(cos θ),
(26.53)
where cos θ has been restored. Equation (26.53) is the multipole expansion of the
potential of the two hemispheres. It is interesting to note that the monopole term

26.7 Physical Examples
633
(the term with a single power of r in the denominator) is absent. It follows from
Equation (10.33) that the total charge on the two spheres must be zero. This is
consistent with the symmetry of the problem from which we expect equal surface
charge densities of opposite signs on the two hemispheres.
■
Example 26.7.3. As yet another example of the solution of Laplace’s equation
in spherical coordinates, consider a grounded neutral conducting sphere of radius
conducting sphere
in an originally
uniform electric
ﬁeld
a placed in an originally uniform electric ﬁeld E0 which is assumed to be inﬁnite
in extent (see Figure 26.2). We want to ﬁnd the electrostatic potential everywhere
outside the sphere. Choosing the ﬁeld to be in the positive z-direction and placing
the center of the sphere at the origin, we will have a problem that is azimuthally
symmetric. The general solution is therefore given by Equation (26.29). The bound-
aries outside the sphere consist of the sphere itself as well as inﬁnity. The electric
ﬁeld at inﬁnity is the original uniform ﬁeld, because the ﬁeld due to the charges
induced on the sphere vanishes at inﬁnity. The potential of this ﬁeld (at inﬁnity)
can be deduced from14
E = E0ˆez = −∇Φ ⇒E0 = −∂Φ
∂z ,
∂Φ
∂x = 0 = ∂Φ
∂y .
Thus, the potential at inﬁnity is independent of x and y, and can be written as
Φ(r, θ) = −E0z = −E0r cos θ = −E0rP1(cos θ)
for
r →∞.
As r →∞, the B terms in Equation (26.29) will go to zero, and we must have the
“limiting” equality
∞

k=0
AkrkPk(u) →−E0rP1(u).
x
y
z
Figure 26.2: The electric ﬁeld in the vicinity of a sphere placed in an external uniform
ﬁeld will change, but the ﬁeld far away from the sphere will remain almost uniform.
14We could express the gradient in terms of spherical coordinates, but, as the reader will
note, the initial manipulation is noticeably easier in the Cartesian coordinates.

634
Laplace’s Equation: Spherical Coordinates
The orthogonality of the Legendre polynomials requires the coeﬃcients on both sides
to be equal. This gives
Ak = 0
for k = 0 and k ≥2,
A1 = −E0.
The B’s are obtained by applying the boundary condition of the sphere itself,
namely the fact that it is grounded. This means that Φ(a, θ) = 0, or
0 = A1aP1(u) +
∞

k=0
Bk
ak+1 Pk(u) = B0
a +
B1
a2 −E0a

P1(u) +
∞

k=2
Bk
ak+1 Pk(u).
Again orthogonality of the Legendre polynomials requires the coeﬃcient of each
polynomial to vanish. This yields
B0 = 0,
B1 = E0a3,
and
Bk = 0
for k ≥2.
Inserting all these coeﬃcients in Equation (26.29), we obtain
Φ(r, θ) = −E0

r −a3
r2

P1(cos θ).
(26.54)
Because of the simplicity of the expression for potential, we can evaluate some
other physical quantities of interest. For example, the electric ﬁeld at all points in
space is
E = −∇Φ = −ˆer ∂Φ
∂r −ˆeθ 1
r
∂Φ
∂θ
or
Er = −∂Φ
∂r = E0

1 + 2a3
r3

cos θ,
Eθ = −1
r
∂Φ
∂r = −E0

1 −a3
r3

sin θ,
Eϕ = 0.
This is the sum of the original uniform ﬁeld
E0(cos θˆer −sin θˆeθ) = E0ˆez
and the ﬁeld due to the charges induced on the sphere
Esph = a3E0
r3
(2 cos θˆer + sin θˆeθ),
which (see Example 16.2.1) is the ﬁeld of an electric dipole with dipole moment
p = a3E0
ke
= 4πϵ0a3E0.
It is interesting to note that at r = a, the only nonvanishing component of
the ﬁeld is Er.
This is consistent with the known fact that electrostatic ﬁelds
are perpendicular to conducting surfaces. Furthermore, this perpendicular ﬁeld is
related to the surface charge density by E = σ/ϵ0. Therefore,
σ = ϵ0Er


r=a = 3ϵ0E0 cos θ,
indicating an accumulation of positive charge on the “upper” (right in the ﬁgure)
hemisphere and an identical distribution of negative charge on the “lower” (left in
the ﬁgure) hemisphere.
■

26.8 Problems
635
26.8
Problems
26.1. Show that by writing P(u) ≡Θ(θ)—with u = cos θ—and using the
chain rule, the second equation of (26.2) becomes
−
1
sin θ
d
dθ

(1 −u2)dP
du

+ αP = 0.
26.2. Choose a solution of the form ur ∞
n=0 anun for the Legendre DE,
assume that a0 and a1 are both nonzero, and show that the only solution for
r is r = 0.
26.3. Derive Equation (26.15).
26.4. Derive Equations (26.18), (26.19), and (26.20).
26.5. Derive Equations (26.21) and (26.22) and show that they can both be
written as (26.23).
26.6. Show by mathematical induction (or otherwise) that Equation (26.24)
satisﬁes Pn(1) = 1.
26.7. Show that Legendre polynomials and the hypergeometric function are
related via (26.25) and (26.26).
26.8. Suppose that Q represents electric charge. Show that in (26.34) Q0
is the total charge and Q1 is the dot product of ˆer and the electric dipole
moment.
26.9. (a) Change t to −t and u to −u, and show that the generating function
g(t, u) of Legendre polynomials does not change.
(b) Now substitute −t for t and −u for u in Equation (26.30) and compare the
resulting equation with (26.30) to derive the parity of Legendre polynomials.
26.10. (a) Show that (1/t)[ln(1 + t) −ln(1 −t)] is an even function of t.
(b) Use the Maclaurin expansion of ln(1 ± t) to derive the following series:
1
t [ln(1 + t) −ln(1 −t)] = 2
∞

k=0
t2k
2k + 1.
26.11. (a) Show that Pn(0) = 0 if n is odd.
(b) Show that for u = 0, Equation (26.38) yields
P2n(0) = −2n −1
2n
P2n−2(0).
(c) Iterate this relation and obtain
P2n(0) = (−1)n (2n −1)!!
(2n)!!
P0(0) = (−1)n (2n −1)!!
(2n)!!
.
Now use the result of Problem 11.1 to obtain the ﬁnal form of (26.39).

636
Laplace’s Equation: Spherical Coordinates
26.12. Suppose f(x) = ∞
k=0 ckPk(x). Show that
# 1
−1
[f(x)]2 dx =
∞

m=0
2c2
m
2m + 1.
26.13. Show the following two equalities:
(1 −z2)d2P
dz2 −2z dP
dz + n(n + 1)
=
n + 1
2n(2πi)
2
C
(ξ2 −1)n[nξ2 −2(n + 1)ξz + n + 2]
(ξ −z)n+3
dξ
=
n + 1
2n(2πi)
2
C
d
dξ
(ξ2 −1)n+1
(ξ −z)n+2

dξ.
26.14. In the integral
 1
−1(x2 −1)n dx, let u = (x2 −1)n and dv = dx and
integrate by parts to show that
# 1
−1
(x2 −1)n dx = −2n
# 1
−1
x2(x2 −1)n dx.
Integrate by parts a few more times and show that
# 1
−1
(x2 −1)n dx = (−2)m n(n −1) . . . (n −m + 1)
(2m −1)!!
# 1
−1
x2m(x2 −1)n−m dx.
Set m = n and, using the result of Problem 11.1, obtain the following ﬁnal
result:
# 1
−1
(x2 −1)n dx = (−1)n22n+1
(n!)2
(2n + 1)!.
26.15. Use the procedures of Example 26.5.4 and the previous problem to
show that for m ≥n:
# 1
−1
xmPn(x) dx =
0
0
if m and n have opposite parities,
2(m+n)/2+1m!( m+n
2
)!
(m−n)!(m+n+1)!
if m and n have the same parities,
where having the same parity means being both even or both odd.
26.16. Show that
 1
0 P2k(x) dx = 0 if k ≥1. Hint: Extend the interval of
integration to (−1, 1) and use the orthogonality of Legendre polynomials.
26.17. Find the Legendre expansion for the function f(x) = |x| in the interval
(−1, +1). Hint: Break up the integrals into two pieces, employ the recurrence
relation to express xPn(x) in terms of Pn−1(x) and Pn+1(x), and use the
result of Example 26.6.1.
26.18. (a) Find the total charge on the upper and lower hemispheres and on
the entire sphere of Example 26.7.3.
(b) Using p =

r′dq(r′), calculate the (induced) dipole moment of the sphere.

26.8 Problems
637
26.19. Suppose that the sphere of Example 26.7.3 is held at potential V0.
(a) Find the potential Φ(r, θ) and the electrostatic ﬁeld at all points in space.
(b) Calculate the surface charge density on the sphere.
(c) Find the total charge on the upper and lower hemispheres and on the
entire sphere.
26.20. Using the inﬁnite series expansion, ﬁnd the electrostatic potential
both inside and outside a conducting sphere of radius a held at the constant
potential V0.
26.21. Find the electrostatic potential inside a sphere of radius a with an
insulating small gap at the equator if the bottom hemisphere is grounded and
the top hemisphere is maintained at a constant potential V0.
26.22. A sphere of radius a is maintained at a temperature T0. The sphere is
inside a large heat-conducting mass. Find the expressions for the steady-state
temperature distribution both inside and outside the sphere.
26.23. A ring of total charge q and radius a in the xy-plane with its center
at the origin constitutes an azimuthally symmetric charge distribution whose
potential is also azimuthally symmetric.
(a) Write the most general potential function valid for r > a.
(b) By direct integration show that
Φ(r, θ = 0) =
1
4πϵ0
#
dq(r′)
|r −r′|




θ=0
=
q
4πϵ0
1
√
r2 + a2 .
(c) Expand this expression in powers of (a/r) and compare the result with
the series in (a) to ﬁnd the coeﬃcients of Legendre expansion and show that
Φ(r, θ) =
q
4πϵ0r
∞

k=0
(−1)k(2k)!
22k(k!)2
a
r
2k
P2k(cos θ).
(d) Find a similar expression for Φ(r, θ) for r < a.
26.24. A conducting sphere of radius a is inside another conducting sphere
of radius b. The inner sphere is held at potential V1; the outer sphere at
V2. Find the potential inside the inner sphere, between the two spheres, and
outside the outer sphere.
26.25. A conducting sphere of radius a is inside another conducting sphere
of radius b which is composed of two hemispheres with an inﬁnitesimal gap
between them. The inner sphere is held at potential V1. The upper half of the
outer sphere is at potential +V2 and its lower half at −V2. Find the potential
inside the inner sphere, between the two spheres, and outside the outer sphere.
26.26. A heat conducting sphere of radius a is composed of two hemispheres
with an inﬁnitesimal gap between them. The upper and lower halves of the
sphere are in contact with heat baths of temperatures +T1 and −T1, respec-
tively. The sphere is inside a second heat conducting sphere of radius b held
at temperature T2. Find the temperature inside the inner sphere, between
the two spheres, and outside the outer sphere.

Chapter 27
Laplace’s Equation:
Cylindrical Coordinates
Before working speciﬁc examples of cylindrical geometry, let us consider a
question that has more general implications.
We saw in Chapter 22 that
separation of variables led to ODEs in which certain constants appeared,
and that diﬀerent choices of signs for these constants can lead to a diﬀerent
functional form of the general solution. For example, an equation such as
d2x/dt2 −kx = 0 can have exponential solutions if k > 0 and trigonometric
solutions if k < 0. One cannot a priori assign a speciﬁc sign to k. Thus, the
general form of the solution is indeterminate. However, once the boundary
conditions are imposed, the unique solutions will emerge regardless of the
initial functional form of the solutions. The following argument illustrates this
point on the angular DE resulting from the separation of Laplace’s equation
in cylindrical coordinates.
27.1
The ODEs
The separation of variables Φ(ρ, ϕ, z) = R(ρ)S(ϕ)Z(z) for Laplace’s equation
∇2Φ = 0 yields the following three ODEs [see Equation (22.14) noting that
λ = 0]. In what follows, we shall use λ for λ1:
d
dρ

ρdR
dρ

+

λρ + μ
ρ

R = 0,
d2S
dϕ2 −μS = 0,
d2Z
dz2 −λZ = 0.
(27.1)

640
Laplace’s Equation: Cylindrical Coordinates
Let us concentrate on the second equation whose most general solution we
can write as
The dependence
of the solution on
ϕ is dictated by
physical
conditions.
S(ϕ) =
0
Ae
√μ ϕ + Be−√μ ϕ
if
μ ̸= 0,
Cϕ + D
if
μ = 0.
(27.2)
No matter what type of boundary conditions are imposed on the potential Φ,
it must give the same value at ϕ and at ϕ + 2π while keeping the other two
variables ﬁxed.1 This is because (ρ, ϕ, z) and (ρ, ϕ+2π, z) represent the same
physical point in space. It follows that
R(ρ)S(ϕ)Z(z) = R(ρ)S(ϕ + 2π)Z(z) ⇒S(ϕ + 2π) = S(ϕ)
because the identity holds for all values of ρ and z. If the last relation is to
be true for the case of μ = 0, we must have C = 0 and S(ϕ) = D. For μ ̸= 0,
Equation (27.2) yields
Ae
√μ (ϕ+2π) + Be−√μ (ϕ+2π) = Ae
√μ ϕ + Be−√μ ϕ
or
Ae
√μ ϕ(e
√μ 2π −1) + Be−√μ ϕ(e−√μ 2π −1) = 0.
This must hold for all ϕ. The only way that can happen (we want to keep A
and B nonzero) is to have
e
√μ 2π −1 = 0
and
e−√μ 2π −1 = 0
both of which are equivalent to e
√μ 2π = 1.2 If we conﬁne ourselves to real
μ, we get only trivial solutions. To avoid this, we have to have √μ = im for
m = 0, ±1, ±2, . . . or μ = −m2 for m = 0, ±1, ±2, . . .. With this choice of μ,
the DE for S(ϕ) becomes S′′ + m2S = 0 whose general solution is a sum of
trigonometric functions. We summarize this ﬁnding:
Theorem 27.1.1. For all physical problems for which the azimuthal angle
varies between 0 and 2π, one is forced to restrict the value of μ to the negative
of the square of an integer. The solution for the angular part then becomes
S(ϕ) = Am cos mϕ + Bm sin mϕ,
m = 0, 1, 2, . . .,
(27.3)
where Am and Bm are constants that may diﬀer for diﬀerent m’s.
The negative values of m will not give rise to any new solutions, so they
are not included in the range of m. The case of μ = 0 need not be treated
separately, because the acceptable solution for this case is S = D = const.,
which is what is obtained in (27.3) when m = 0.
1This argument is valid only for physical situations deﬁned for the entire range of ϕ.
If the region of interest restricts ϕ to a subset of the interval [0, 2π], the argument breaks
down.
2The second equation can be obtained by multiplying the ﬁrst equation by e−√μ 2π.

27.1 The ODEs
641
The DE for Z(z) is independent of m and has an exponential solution
if λ > 0 and a trigonometric solution if λ < 0. Assuming the former, and
writing λ ≡l2, we have
Z(z) = Aelz + Be−lz.
(27.4)
Least familiar is the radial DE which, in terms of l =
√
λ, can be rewritten
as
d2R
dρ2 + 1
ρ
dR
dρ +

l2 −m2
ρ2

R = 0.
(27.5)
Furthermore, if we deﬁne the variable v = lρ, we can cast (27.5) in the form
Bessel diﬀerential
equation
d2R
dv2 + 1
v
dR
dv +

1 −m2
v2

R = 0.
(27.6)
Equation (27.5), or (27.6), is one of the most famous DEs of mathematical
physics called the Bessel diﬀerential equation. Our task for the remainder
of this chapter is to ﬁnd solutions of this DE and list some of their properties
and examples of their usage.
Historical Notes
Friedrich Wilhelm Bessel showed no signs of unusual academic ability in school,
although he did show a liking for mathematics and physics. He left school intending
to become a merchant’s apprentice, a desire that soon materialized with a seven-year
unpaid apprenticeship with a large mercantile ﬁrm in Bremen. The young Bessel
proved so adept at accounting and calculation that he was granted a small salary,
with raises, after only the ﬁrst year. An interest in foreign trade led Bessel to study
geography and languages at night, astonishingly learning to read and write English
in only three months. He also studied navigation in order to qualify as a cargo oﬃcer
aboard ship, but his innate curiosity soon compelled him to investigate astronomy
at a more fundamental level.
Still serving his apprenticeship, Bessel learned to
observe the positions of stars with suﬃcient accuracy to determine the longitude
of Bremen, checking his results against professional astronomical journals. He then
tackled the more formidable problem of determining the orbit of Halley’s comet
from published observations.
After seeing the close agreement between Bessel’s
calculations and those of Halley, the German astronomer Olbers encouraged Bessel
to improve his already impressive work with more observations.
The improved
calculations, an achievement tantamount to a modern doctoral dissertation, were
published with Olbers’s recommendation. Bessel later received appointments with
increasing authority at observatories near Bremen and in K¨onigsberg, the latter
position being accompanied by a professorship. (The title of doctor, required for the
professorship, was granted by the University of G¨ottingen on the recommendation
of Gauss.)
Friedrich Wilhelm
Bessel 1784–1846
Bessel proved himself an excellent observational astronomer. His careful mea-
surements coupled with his mathematical aptitude allowed him to produce accurate
positions for a number of previously mapped stars, taking account of instrumental
eﬀects, atmospheric refraction, and the position and motion of the observation site.
In 1820 he determined the position of the vernal equinox accurate to 0.01 second, in
agreement with modern values. His observation of the variation of the proper motion

642
Laplace’s Equation: Cylindrical Coordinates
of the stars Sirius and Procyon led him to posit the existence of nearby, large, low-
luminosity stars called dark companions. Between 1821 and 1833 he catalogued the
positions of about 75,000 stars, publishing his measurements in detail. One of his
most important contributions to astronomy was the determination of the distance
to a star using parallax. This method uses triangulation, or the determination of
the apparent positions of a distant object viewed from two points a known distance
apart, in this case two diametrically opposed points of the Earth’s orbit. The angle
subtended by the baseline of the Earth’s orbit, viewed from the star’s perspective,
is known as the star’s parallax. Before Bessel’s measurement, stars were assumed
to be so distant that their parallaxes were too small to measure, and it was further
assumed that bright stars (thought to be nearer) would have the largest parallax.
Bessel correctly reasoned that stars with large proper motions were more likely to
be nearby ones and selected such a star, 61 Cygni, for his historic measurement. His
measured parallax for that star diﬀers by less than 8% from the currently accepted
value.
Given such an impressive record in astronomy, it seems only ﬁtting that the
famous functions that bear Bessel’s name grew out of his investigations of pertur-
bations in planetary systems. He showed that such perturbations could be divided
into two eﬀects and treated separately: the obvious direct attraction due to the
perturbing planet and an indirect eﬀect caused by the Sun’s response to the per-
turber’s force. The so-called Bessel functions then appear as coeﬃcients in the series
treatment of the indirect perturbation. Although special cases of Bessel functions
were discovered by Bernoulli, Euler, and Lagrange the systematic treatment by Bessel
clearly established his preeminence, a ﬁtting tribute to the creator of the most fa-
mous functions in mathematical physics.
27.2
Solutions of the Bessel DE
The Frobenius method is an eﬀective way of ﬁnding solutions for ODEs. We
rewrite (27.6) by multiplying it by v2 to turn all its coeﬃcients into polyno-
mials as suggested by Equation (26.7). This yields
v2 d2R
dv2 + v dR
dv +
 
v2 −m2!
R = 0.
(27.7)
Since v2 vanishes at v = 0, we must assume a solution of the form
R(v) = vs
∞

k=0
ckvk =
∞

k=0
ckvk+s
from which we obtain
v dR
dv =
∞

k=0
ck(k + s)vk+s,
v2 d2R
dv2 =
∞

k=0
ck(k + s)(k + s −1)vk+s.

27.2 Solutions of the Bessel DE
643
Substituting these as well as (v2 −m2) ∞
k=0 ckvk+s in the DE yields
∞

k=0
ck[k + s + (k + s)(k + s −1)



=(k+s)2
−m2]vk+s +
∞

k=0
ckvk+s+2 = 0.
To ﬁnd the recursion relation, we need to have the same power of v in the
sum. We do this by rewriting the ﬁrst sum as
c0(s2 −m2)vs + c1[(s + 1)2 −m2]vs+1 +
∞

k=2
ck[(k + s)2 −m2]vk+s
= c0(s2 −m2)vs + c1[(s + 1)2 −m2]vs+1
+
∞

n=0
cn+2[(n + 2 + s)2 −m2]vn+2+s,
where in the second line, we introduced n = k −2. Since n is a dummy index,
we can change it back to k. It then follows that
c0(s2 −m2)vs + c1[(s + 1)2 −m2]vs+1
+
∞

k=0
,
ck+2[(k + 2 + s)2 −m2] + ck
-
vk+2+s = 0.
Assuming that c0 ̸= 0 and setting the coeﬃcients of all powers of v equal to
zero, we get
s2 = m2,
c1[(s + 1)2 −m2] = 0,
ck+2[(k + 2 + s)2 −m2] + ck = 0.
The ﬁrst equation gives m = ±s. Inserting this in the second equation gives
c1(2s + 1) = 0 ⇒c1 = 0
or
s = −1
2.
The choice s = −1
2 gives m = ∓1
2 which is not acceptable,3 as we decided
that m is to be a positive integer. We therefore conclude that s = ±m and
c1 = 0. It follows from the recursion relation that all odd c’s are zero. The
Frobenius series will therefore look like
R(v) = v±m
∞

k=0
c2kv2k,
c2k+2
c2k
= −
1
(2k + 2 + s)2 −m2 .
(27.8)
The ratio test for the convergence of series yields
lim
k→∞




c2k+2v2k+2
c2kv2k




 = lim
k→∞




1
(2k + 2 + s)2 −m2




 v2 = 0,
3Actually, problems arising from other areas of physics beyond electrostatics and steady-
state heat transfer allow noninteger values of m.
However, we shall not deal with such
problems here.

644
Laplace’s Equation: Cylindrical Coordinates
which indicates that
Box 27.2.1. The series of Equation (27.8) is convergent for all values
of v.
We now use the recursion relation to obtain the coeﬃcients of expansion.
Rewrite the recursion relation as
recursion relation
for Bessel
equation
ck+2 = −
1
(k + 2 + s)2 −s2 ck = −
1
(k + 2)(2s + k + 2)ck,
where we substituted s2 for m2. This gives
c2 = −
1
2(2s + 2)c0,
c4 = −
1
4(2s + 4)c2 = (−1)2
1
4(2s + 4)
1
2(2s + 2)c0,
c6 = −
1
6(2s + 6)c4 = (−1)3
1
6(2s + 6)
1
4(2s + 4)
1
2(2s + 2)c0,
and, in general,
c2k = (−1)k
1
2k · (2k −2) . . . 2



=2kk!
(2s + 2k)[2s + (2k −2)] . . . (2s + 2)



=2k(s+k)(s+k−1)...(s+1)
c0.
Multiplying the numerator and denominator by s!, we obtain
c2k = (−1)k
s!
22kk!(s + k)!c0.
(27.9)
Substituting (27.9) in (27.8) yields
R(v) = c0s!vs
∞

k=0
(−1)k
22kk!(s + k)!v2k = c0s!2s v
2
s ∞

k=0
(−1)k
k!(s + k)!
v
2
2k
,
where we substituted s for ±m in the exponent of v outside the summation.
We also absorbed the powers of 2 in the denominator of the sum into the
powers of v, and outside the sum, we multiplied and divided by 2s.
It is
customary to choose the arbitrary constant c0 to be equal to 1/(s!2s). This
leads to
Box 27.2.2. The Bessel function of order s is denoted by Js and is
given by the series
Js(x) =
x
2
s ∞

k=0
(−1)k
k!(s + k)!
x
2
2k
(27.10)
which is convergent for all values of x.

27.3 Second Solution of the Bessel DE
645
Although Equation (27.10) was derived assuming that m—and therefore
Equation (27.10)
is valid not only
for integer s, but
also for real and
even complex s.
s—was an integer, lifting this restriction will still yield a series which is con-
vergent everywhere, and one can deﬁne Bessel functions whose orders are
real or even complex numbers. The only diﬃculty is to correctly interpret
(s + n)! for non-integer s. But this is precisely what the gamma function was
invented for (see Deﬁnition 11.1.1). Thus, we let Equation (27.10) stand for
Bessel functions of all orders.
27.3
Second Solution of the Bessel DE
As in the case of Legendre polynomials, we can obtain a second solution of
the Bessel DE using Equation (24.6). For the Bessel DE, we have p(x) = 1/x.
Using Jm(x) as our input, we can generate another solution. With C = 0 in
Equation (24.6), we obtain
Zm(x) = KJm(x)
# x
α
1
J2m(u) exp

−
# u
c
dt
t

du = AmJm(x)
# x
α
du
uJ2m(u),
where Am ≡Kc and α are arbitrary constants determined by convention.
Note that, contrary to Jm(x), Zm(x) is not well behaved at x = 0 due to the
presence of u in the denominator of the integrand.
Although the above procedure manufactures a second solution for the
Bessel DE, it is not the customary procedure.
It turns out that for non-
integer s, the Bessel function J−s(x) is independent of Js(x) and can be used
as a second solution.4 However, a more common second solution is the linear
combination
Ys(x) = Js(x) cos sπ −J−s(x)
sin sπ
(27.11)
called the Bessel function of the second kind, or the Neumann function.
For integer s the function is indeterminate because of Equation (11.32) and
the identity cos nπ = (−1)n. Therefore, we use l’Hˆopital’s rule and deﬁne
Bessel function of
the second kind or
Neumann function
Yn(x) ≡lim
s→n Ys(x) = lim
s→n
∂
∂s[Js(x) cos sπ −J−s(x)]
π cos nπ
= 1
π lim
s→n
∂Js
∂s −(−1)n ∂J−s
∂s

,
From (27.10) we obtain
∂Js
∂s = Js(x) ln
x
2

−
x
2
s ∞

k=0
(−1)k Ψ(s + k + 1)
k!Γ(s + k + 1)
x
2
2k
,
where
Ψ(x) ≡d
dx ln[(x −1)!] = d
dx ln Γ(x) = dΓ(x)/dx
Γ(x)
.
4See Hassani, S. Mathematical Physics: A Modern Introduction to Its Foundations,
Springer-Verlag, 1999, Chapter 14 for details.

646
Laplace’s Equation: Cylindrical Coordinates
Similarly,
∂J−s
∂s
= −J−s(x) ln
x
2

+
x
2
−s ∞

k=0
(−1)k Ψ(−s + k + 1)
k!Γ(−s + k + 1)
x
2
2k
.
Substituting these expressions in the deﬁnition of Yn(x) and using J−n(x) =
(−1)nJn(x) [Equation (11.32)], we obtain
Yn(x) = 2
π Jn(x) ln
x
2

−1
π
x
2
n ∞

k=0
(−1)k Ψ(n + k + 1)
k!Γ(n + k + 1)
x
2
2k
−1
π (−1)n x
2
−n ∞

k=0
(−1)k Ψ(k −n + 1)
k!Γ(k −n + 1)
x
2
2k
.
(27.12)
It should be clear from (27.12) that the Neumann function Ys(x) is ill deﬁned
at x = 0, as expected of the second solution of the Bessel DE such as Zm(x)
discussed above.
Since Ys(x) is linearly independent of Js(x) for any s, integer or noninteger,
it is convenient to consider {Js(x), Ys(x)} as a basis of solutions for the Bessel
DE. In particular, the solution of the radial equation in cylindrical coordinates,
i.e., the ﬁrst equation in (27.1), becomes
R(ρ) = AJm(v) + BYm(v) = AJm(lρ) + BYm(lρ).
(27.13)
27.4
Properties of the Bessel Functions
We have already considered some properties of the Bessel functions in Chap-
ter 11.
In this subsection, we quote those results and obtain other useful
properties of the Bessel functions.
27.4.1
Negative Integer Order
Equation (11.32) gives a relation between a Bessel function of integer order
and the Bessel function whose order is negative of the ﬁrst one
J−m(x) = (−1)mJm(x).
(27.14)
27.4.2
Recurrence Relations
A number of recurrence relations involving Bessel functions of integer orders
and their derivatives were derived in Chapter 11 which we reproduce here.
The ﬁrst one, involving no derivatives is
Jm−1(x) + Jm+1(x) = 2m
x Jm(x).
(27.15)

27.4 Properties of the Bessel Functions
647
The second one, which includes derivatives of Bessel functions, is
Jm−1(x) −Jm+1(x) = 2J′
m(x).
(27.16)
Combining these two equations, one obtains
recurrence
relations involving
derivatives
Jm−1(x) = m
x Jm(x) + J′
m(x),
Jm+1(x) = m
x Jm(x) −J′
m(x).
(27.17)
We can use these equations to obtain new—and more useful—relations.
For example, by diﬀerentiating xmJm(x), we get
[xmJm(x)]′ = mxm−1Jm(x) + xmJ′
m(x)
= xm *m
x Jm(x) + J′
m(x)
+



=Jm−1(x) by (27.17)
= xmJm−1(x).
Integrating (really, antidiﬀerentiating) this equation yields
#
xmJm−1(x) dx = xmJm(x).
(27.18)
Similarly, the reader may check that
#
x−mJm+1(x) dx = −x−mJm(x).
(27.19)
27.4.3
Orthogonality
Bessel functions satisfy an orthogonality relation similar to that of the Leg-
endre polynomials. However, unlike Legendre polynomials, the quantity that
determines the orthogonality of diﬀerent Bessel functions is not the order but
a parameter in their argument (also see Example 24.5.3).
Consider two solutions of the Bessel DE corresponding to the same az-
imuthal parameter, but with diﬀerent radial parameter. More speciﬁcally, let
f(ρ) = Jm(kρ) and g(ρ) = Jm(lρ). Then
d2f
dρ2 + 1
ρ
df
dρ +

k2 −m2
ρ2

f = 0,
d2g
dρ2 + 1
ρ
dg
dρ +

l2 −m2
ρ2

g = 0.
The reader may check that by multiplying the ﬁrst equation by ρg and the
second equation by ρf and subtracting, one gets
d
dρ[ρ(fg′ −gf ′)] = (k2 −l2)ρfg,

648
Laplace’s Equation: Cylindrical Coordinates
where the prime indicates diﬀerentiation with respect to ρ. Now integrate
this equation with respect to ρ from some initial value (say a) to some ﬁnal
value (say b) to obtain
[ρ(fg′ −gf ′)]b
a = (k2 −l2)
# b
a
ρf(ρ)g(ρ) dρ.
In all physical applications a and b can be chosen to make the LHS vanish.
Then, substituting for f and g in terms of Bessel functions, we get
(k2 −l2)
# b
a
ρJm(kρ)Jm(lρ) dρ = 0.
It follows that if k ̸= l, then the integral vanishes, i.e.,
# b
a
ρJm(kρ)Jm(lρ) dρ = 0
if
k ̸= l.
(27.20)
This is the orthogonality relation for Bessel functions also derived in Example
24.5.3.
To complete the orthogonality relation, we must also address the case when
k = l. This involves the evaluation of the integral

ρJ2
m(kρ) dρ, which, upon
the change of variable x ≡kρ, reduces to (

xJ2
m(x) dx)/k2. By integration
by parts, we have
I ≡
#
J2
m(x)
  
u
x dx

dv
= 1
2x2J2
m(x) −
#
Jm(x)J′
m(x)x2 dx.
In the last integral, substitute for x2Jm(x) from the Bessel DE (27.6)—using
x instead of v:
x2Jm(x) = m2Jm(x) −xJ′
m(x) −x2J′′
m(x).
Therefore,
I = 1
2x2J2
m(x) −
#
J′
m(x)[m2Jm(x)
=−( 1
2 x2[J′
m(x)]2)′



−xJ′
m(x) −x2J′′
m(x)] dx
= 1
2x2J2
m(x) −m2
#
= 1
2 [J2
m(x)]′



Jm(x)J′
m(x) dx + 1
2
#
d
dx
 
x2[J′
m(x)]2!
dx
= 1
2x2J2
m(x) −1
2m2J2
m(x) + 1
2x2[J′
m(x)]2.
Returning back to ρ, we obtain the indeﬁnite integral
#
ρJ2
m(kρ) dρ = I
k2 = 1
2

ρ2 −m2
k2

J2
m(kρ) + 1
2ρ2[J′
m(kρ)]2.
(27.21)

27.4 Properties of the Bessel Functions
649
In most applications, the lower limit of integration is zero and the upper limit
is a positive number a. The RHS of (27.21) vanishes at the lower limit because
of the following reason. The ﬁrst term vanishes at ρ = 0 because Jm(0) = 0
for all m > 0 as is evident from the series expansion (27.10). For m = 0 (and
ρ = 0), the parentheses in the ﬁrst term of (27.21) vanishes. So, the ﬁrst
term is zero for all m ≥0 at the lower limit of integration. The second term
vanishes due to the presence of ρ2. Thus, we obtain
# a
0
ρJ2
m(kρ) dρ = 1
2

a2 −m2
k2

J2
m(ka) + 1
2a2[J′
m(ka)]2
(27.22)
for all m ≥0 and, by (27.14), also for all negative integers. As mentioned
earlier, we shall conﬁne our discussion to Bessel functions of integer orders.
It is customary to simplify the RHS of (27.22) by choosing k in such a way
that Jm(ka) = 0, i.e., that ka is a root of the Bessel function of order m. In
general, there are inﬁnitely many roots. So, let xmn denote the nth root of
Jm(x). Then,
ka = xmn ⇒k = xmn
a ,
n = 1, 2, . . . ,
and if we use Equation (27.17), we obtain
# a
0
ρJ2
m(xmnρ/a) dρ = 1
2a2[Jm+1(xmn)]2.
(27.23)
Equations (27.20) and (27.23) can be combined into a single equation using
the Kronecker delta:
orthogonality
relations involving
Bessel functions
Box 27.4.1. The Bessel functions of integer order satisfy the orthogonal-
ity relations
# a
0
Jm(xmnρ/a)Jm(xmkρ/a)ρ dρ = 1
2a2J2
m+1(xmn)δkn,
(27.24)
where a > 0 and xmn is the nth root of Jm(x).
27.4.4
Generating Function
Just as in the case of Legendre polynomials, Bessel functions of integer order
have a generating function, i.e., there exists a function g(x, t) such that
g(x, t) =
∞

n=−∞
tnJn(x).
(27.25)
To ﬁnd g, start with the recurrence relation
Jm−1(x) + Jm+1(x) = 2m
x Jm(x),

650
Laplace’s Equation: Cylindrical Coordinates
multiply it by tm, and sum over all m to obtain
∞

m=−∞
tmJm−1(x) +
∞

m=−∞
tmJm+1(x) = 2
x
∞

m=−∞
mtmJm(x).
(27.26)
The ﬁrst sum can be written as
∞

m=−∞
tmJm−1(x) = t
∞

m=−∞
tm−1Jm−1(x) = t
∞

n=−∞
tnJn(x) = tg(x, t),
where we substituted the dummy index n = m −1 for m. Similarly
∞

m=−∞
tmJm+1(x) = 1
t
∞

m=−∞
tm+1Jm+1(x) = 1
t g(x, t)
and
2
x
∞

m=−∞
mtmJm(x) = 2t
x
∞

m=−∞
mtm−1Jm(x) = 2t
x
∂g
∂t .
It follows from Equation (27.26) that

t + 1
t

g(x, t) = 2t
x
∂g
∂t ,
or
x
2

1 + 1
t2

dt = dg
g ,
where x is assumed to be a constant because we have been diﬀerentiating with
respect to t. Integrating both sides gives
# x
2

1 + 1
t2

dt



= x
2 (t−1
t )
= ln g + ln φ(x),
where the last term is the “constant” of integration. Thus,
g(x, t) = φ(x) exp
x
2

t −1
t

.
To determine φ(x), we note that
g(x, t) = φ(x)ext/2e−x/2t = φ(x)
∞

n=0
(xt/2)n
n!
∞

m=0
(−x/2t)m
m!
= φ(x)
∞

n,m=0
(−1)m
n!m!
x
2
n+m
tn−m.

27.4 Properties of the Bessel Functions
651
In the last double sum, collect all terms whose power of t is zero, and call the
sum S0. This is obtained by setting n = m. Then,
S0 = φ(x)
∞

n=0
(−1)n
n!n!
x
2
2n
= φ(x)J0(x),
where we used Equation (27.10) with s = 0.
But (27.25) shows that the
collection of all terms whose power of t is zero is simply J0(x). Thus, S0 =
J0(x), and φ(x) = 1. This leads to the ﬁnal form of the Bessel generating
function:
generating
function for Bessel
functions
g(x, t) = exp
x
2

t −1
t

=
∞

n=−∞
tnJn(x).
(27.27)
Example 27.4.1. The generating function for Bessel functions can be used to
obtain a useful identity. First we note that
g(x + y, t) = g(x, t)g(y, t)
as the reader may easily verify. Expanding each side gives
∞

n=−∞
tnJn(x + y) =
∞

k=−∞
tkJk(x)
∞

m=−∞
tmJm(y) =
∞

k=−∞
∞

m=−∞
tk+mJk(x)Jm(y).
In the last double sum, let n = k+m, so that k = n−m. Since there is no limitation
on the value of either of the dummy indices, the limits of the new indices n and m
are still −∞and ∞. Therefore,
∞

n=−∞
tnJn(x + y) =
∞

n=−∞
∞

m=−∞
tnJn−m(x)Jm(y)
=
∞

n=−∞
tn

∞

m=−∞
Jn−m(x)Jm(y)

.
Since each power of t should have the same coeﬃcient on both sides, we obtain the
so-called addition theorem for Bessel functions:
addition theorem
for Bessel
functions
Jn(x + y) =
∞

m=−∞
Jn−m(x)Jm(y) =
∞

m=−∞
Jm(x)Jn−m(y),
(27.28)
where the last equality follows from the symmetry of Jn(x + y) under the exchange
of x and y.
■
The Bessel generating function can also lead to some very important iden-
tities. In Equation (27.27), let t = eiθ and use (18.14) to obtain
eix sin θ =
∞

n=−∞
einθJn(x).
(27.29)

652
Laplace’s Equation: Cylindrical Coordinates
This is a Fourier series expansion in θ—as given in (18.20)—whose coeﬃcients
are Bessel functions. To ﬁnd these coeﬃcients, we multiply both sides by
e−imθ and integrate from −π to π [see also Equation (18.22)].
The LHS
gives
LHS =
# π
−π
eix sin θe−imθ dθ =
# π
−π
ei(x sin θ−mθ) dθ.
For the RHS, we obtain
∞

n=−∞
# π
−π
ei(n−m)θdθ

Jn(x) = 2πJm(x),
where we used the easily veriﬁable result [also see Equation (18.21)]:
# π
−π
ei(n−m)θdθ =
0
0
if
n ̸= m
2π
if
n = m = 2πδmn.
Equating the RHS and the LHS, we obtain
integral
representation of
the Bessel
function
Jm(x) = 1
2π
# π
−π
ei(x sin θ−mθ) dθ.
(27.30)
The reader may check that this can be reduced to
Jm(x) = 1
π
# π
0
cos(x sin θ −mθ) dθ
(27.31)
which is called Bessel’s integral.
Bessel’s integral
Bessel functions can be written in terms of the conﬂuent hypergeometric
function. To see this, substitute R(v) = vμe−ηvf(v)—with μ and η to be
determined—in Equation (27.6) to obtain
d2f
dv2 +
2μ + 1
v
−2η
 df
dv +
μ2 −m2
v2
−η(2μ + 1)
v
+ η2 + 1

f = 0
which, if we set μ = m and η = i, reduces to
f ′′ +
2m + 1
v
−2i

f ′ −(2m + 1)i
v
f = 0.
(27.32)
Making the further substitution 2iv = t, and multiplying out by t, we obtain
td2f
dt2 + (2m + 1 −t)df
dt −(m + 1
2)f = 0
which is in the form of (11.27) with α = m + 1
2 and γ = 2m + 1. Thus, Bessel
functions Jm(x) can be written as constant multiples of xme−ixΦ(m+ 1
2, 2m+
relation between
Bessel functions
and conﬂuent
hypergeometric
function
1; 2ix). In fact,
Jm(x) =
1
Γ(m + 1)
x
2
m
e−ixΦ(m + 1
2, 2m + 1; 2ix).
(27.33)

27.5 Expansions in Bessel Functions
653
27.5
Expansions in Bessel Functions
The orthogonality of Bessel functions can be useful in expanding other func-
tions in terms of them. The basic idea is similar to the expansion in Fourier
series and Legendre polynomials. If a function f(ρ) is deﬁned in the interval
(0, a), then we may write
f(ρ) =
∞

n=1
cnJm(xmnρ/a).
(27.34)
The coeﬃcients can be found by multiplying both sides by ρJm(xmkρ/a) and
integrating from zero to a. The reader may verify that this yields
cn =
2
a2J2
m+1(xmn)
# a
0
f(ρ)Jm(xmnρ/a)ρ dρ.
(27.35)
Equations (27.34) and (27.35) are the analogues of Equations (10.38),
(10.40), (10.41), and (10.42) for Fourier series, and Equations (26.46) and
(26.47) for Legendre polynomials. Like those sets of equations, they can be
used to expand functions in terms of Bessel functions of a speciﬁc order.
Example 27.5.1. The trigonometric functions can be expanded in Bessel func-
tions with very little eﬀort. In fact, Equation (27.29) leads immediately to
eix =
∞

n=−∞
inJn(x)
or
expansion of sine
and cosine in
Bessel functions
cos x + i sin x =
∞

k=−∞
i2kJ2k(x) +
∞

k=−∞
i2k+1J2k+1(x),
where we have separated the even and odd sums. The ﬁrst sum is real and the
second sum pure imaginary. Therefore,
cos x =
∞

k=−∞
(−1)kJ2k(x) =
−1

k=−∞
(−1)kJ2k(x) + J0(x) +
∞

k=1
(−1)kJ2k(x).
The ﬁrst sum can be written as
−1

k=−∞
(−1)kJ2k(x) =
∞

k=1
(−1)−kJ−2k(x) =
∞

k=1
(−1)k(−1)2kJ2k(x)
=
∞

k=1
(−1)kJ2k(x)
which is identical to the last sum. It follows that
cos x = J0(x) + 2
∞

k=1
(−1)kJ2k(x).
(27.36)
Similarly,
sin x = 2
∞

k=0
(−1)kJ2k+1(x)
(27.37)
as the reader is urged to verify.
■

654
Laplace’s Equation: Cylindrical Coordinates
If we square Equation (27.34), multiply by ρ, and integrate from zero to
a, we obtain
# a
0
f 2(ρ)ρ dρ =
∞

n=1
∞

k=1
cnck
# a
0
Jm(xmnρ/a)Jm(xmkρ/a)ρ dρ



= 1
2 a2J2
m+1(xmn)δkn by (27.24)
.
This leads to the so-called Parseval relation:
Parseval relation
# a
0
f 2(ρ)ρ dρ = 1
2a2
∞

n=1
c2
nJ2
m+1(xmn)
(27.38)
for some m. This m can be chosen to make the integrations as simple as
possible.
Example 27.5.2. Let us ﬁnd the expansion of ρk in terms of Bessel functions.
expansion of ρk in
terms of Bessel
functions
Equations (27.35) and (27.18) suggest expanding in terms of Jk(x) because the
integrals can be performed. Therefore, we write
ρk =
∞

n=1
cnJk(xknρ/a),
where
cn =
2
a2J2
k+1(xkn)
# a
0
ρkJk(xknρ/a)ρ dρ =
2
a2J2
k+1(xkn)
# a
0
ρk+1Jk(xknρ/a) dρ.
Introducing y = xknρ/a in the integral gives
cn =
2ak
xk+2
kn J2
k+1(xkn)
# xkn
0
yk+1Jk(y) dy =
2ak
xknJk+1(xkn),
where we used (27.18) with m replaced by k + 1. Thus, we have
ρk = 2ak
∞

n=1
Jk(xknρ/a)
xknJk+1(xkn).
■
27.6
Physical Examples
Our discussion of Laplace’s equation has led us to believe that trigonometric
functions and Legendre polynomials are, respectively, the “natural” functions
of Cartesian and spherical geometries. It is of no surprise now to learn that
Bessel functions are the natural functions of cylindrical geometry.
As in the case of Cartesian and spherical coordinates, unless the symmetry
of the problem simpliﬁes the situation, the separation of Laplace’s equation
results in two parameters leading to a double sum as in Example 25.2.4. The
reason that we did not obtain double sums in spherical coordinates is that
from the very beginning we assumed azimuthal symmetry. Thus, we expect

27.6 Physical Examples
655
h
x
y
z
a
Φ = V(ρ, φ)
Figure 27.1: A conducting cylindrical can whose top has a potential given by V (ρ, θ)
with the rest of the surface grounded.
a double summation in the most general solution of Laplace’s equation in
cylindrical geometries. One of these sums is over m which, as Equation (27.3)
shows, appears in the argument of the sine and cosine functions.
It also
designates the order of the Bessel (or Neumann) function.
To understand the origin of the second summation, consider a cylindrical
conducting can of radius a and height h (see Figure 27.1). Suppose that the
potential at the top face varies as V (ρ, ϕ) while the lateral surface and the
bottom face are grounded. Let us ﬁnd the electrostatic potential Φ at all
points inside the can.
The general solution is a product of (27.3), (27.4), and (27.13):
Φ(ρ, ϕ, z) = R(ρ)S(ϕ)Z(z).
Since Φ(ρ, ϕ, 0) = 0 for arbitrary ρ and ϕ, we must have Z(0) = 0 yielding—to
within a constant—Z(z) = sinh(lz).
Since Φ(0, ϕ, z) is ﬁnite, no Neumann function is allowed in the expan-
sion, and, to within a constant, we have R(ρ) = Jm(lρ). Furthermore, since
Φ(a, ϕ, z) = 0 for arbitrary ϕ and z, we must have
R(a) = Jm(la) = 0 ⇒la = xmn ⇒l = xmn
a ,
n = 1, 2, . . .,
where, as before, xmn is the nth root of Jm.
We can now multiply R, S, and Z and sum over all possible values of m
and n, keeping in mind that negative values of m give terms that are linearly
dependent on the corresponding positive values. The result is the so-called
Fourier–Bessel series:
Fourier–Bessel
series
Φ(ρ, ϕ, z) =
∞

m=0
∞

n=1
Jm
xmn
a ρ

sinh
xmn
a z

(Amn cos mϕ + Bmn sin mϕ)
(27.39)

656
Laplace’s Equation: Cylindrical Coordinates
where Amn and Bmn are constants to be determined by the remaining bound-
ary condition which states that Φ(ρ, ϕ, h) = V (ρ, ϕ) or
V (ρ, ϕ) =
∞

m=0
∞

n=1
Jm
xmn
a ρ

sinh
xmn
a h

(Amn cos mϕ + Bmn sin mϕ).
(27.40)
Multiplying both sides by ρJm(xmka/ρ) cos jϕ and integrating from zero to
2π in ϕ, and from zero to a in ρ gives Ajk. Changing cosine to sine and
following the same steps yields Bjk. Switching back to m and n, the reader
may verify that
Amn =
2
# 2π
0
dϕ
# a
0
dρ ρV (ρ, ϕ)Jm
xmn
a ρ

cos mϕ
πa2J2
m+1(xmn) sinh(xmnh/a)
,
Bmn =
2
# 2π
0
dϕ
# a
0
dρ ρV (ρ, ϕ)Jm
xmn
a ρ

sin mϕ
πa2J2
m+1(xmn) sinh(xmnh/a)
,
(27.41)
where we have used Equation (27.24).
The important case of azimuthal symmetry requires special consideration.
In such a case, the potential of the top surface V (ρ, ϕ) must be independent
of ϕ. Furthermore, since S(ϕ) is constant,5 its derivative must vanish. Hence,
the second equation in (27.1) yields μ = −m2 = 0. This zero value for m
reduces the double summation of (27.39) to a single sum, and we get
Φ(ρ, z) =
∞

n=1
AnJ0
x0n
a ρ

sinh
x0n
a z

.
(27.42)
The coeﬃcients An can be obtained by setting m = 0 in the ﬁrst equation of
(27.41):
An =
4
a2J2
1(x0n) sinh(x0nh/a)
# a
0
ρV (ρ)J0
x0n
a ρ

dρ,
(27.43)
where V (ρ) is the ϕ-independent potential of the top surface.
Example 27.6.1. Suppose that the top face of a conducting cylindrical can is
held at the constant potential V0 while the lateral surface and the bottom face are
grounded. We want to ﬁnd the electrostatic potential Φ at all points inside the can.
Since the potential of the top is independent of ϕ, azimuthal symmetry prevails,
and Equation (27.43) gives
An =
4V0
a2J2
1 (x0n) sinh(x0nh/a)
# a
0
ρJ0
 x0n
a ρ

dρ =
4V0
x0nJ1(x0n) sinh(x0nh/a),
where we used Equation (27.18). The detail of calculating the integral is left as
Problem 27.15 for the reader. Therefore,
Φ(ρ, z) = 4V0
∞

n=1
J0(x0nρ/a) sinh(x0nz/a)
x0nJ1(x0n) sinh(x0nh/a).
■
5S(ϕ) must be a constant. Otherwise, the potential would depend on ϕ.

27.7 Problems
657
27.7
Problems
27.1. Derive (27.6) from the ﬁrst equation of (27.1).
27.2. Show that both equations in (27.17) give
J′
0(x) = −J1(x).
27.3. Show that
[x−mJm(x)]′ = −x−mJm+1(x)
and derive Equation (27.19).
27.4. Obtain the following equation from the Bessel DE:
d
dρ[ρ(fg′ −gf ′)] = (k2 −l2)ρfg,
where f and g are solutions of two Bessel DEs for which the “constants” of
the DEs are k2 and l2, respectively.
27.5. (a) Show that for the Bessel generating function,
g(x + y, t) = g(x, t)g(y, t)
and
g(x, −t) =
1
g(x, t).
(b) Use the second relation to show that
∞

m=−∞
Jm−k(x)Jm(x) = δ0k ≡
0
1
if
k = 0,
0
if
k ̸= 0.
Hint: Set the powers of t equal on both sides of 1 = g(x, t)g(x, −t).
(c) In particular,
1 =
∞

m=−∞
J2
m(x) = J2
0(x) + 2
∞

m=1
J2
m(x),
showing that |J0(x)| ≤1 and |Jm(x)| ≤1/
√
2 for m > 0.
27.6. Derive Equation (27.31) from (27.30).
27.7. Use Equation (27.31) to show that J−m = (−1)mJm.
27.8. Show that the substitution R(v) = vme−ivf(v) turns Equation (27.6)
into (27.32).
27.9. Using the orthogonality of Bessel functions derive Equation (27.35)
from (27.34).
27.10. Prove that
eix cos θ =
∞

n=−∞
ineinθJn(x).

658
Laplace’s Equation: Cylindrical Coordinates
27.11. Derive the expansion of the sine function in terms of Bessel functions.
Hint: See Example 27.5.1.
27.12. The integral
 ∞
0
e−axJ0(bx) dx may look intimidating, but leads to a
very simple expression. To see this:
(a) Substitute for J0(bx) its series representation, and express the result of
the integration in terms of the gamma function (a factorial, in this case).
(b) Use one of the results of Problem 11.1 to show that
# ∞
0
e−axJ0(bx) dx =
1
a√π
∞

n=0
Γ(n + 1
2)
n!

−b2
a2
n
.
(c) Show that this result can also be expressed in terms of the hypergeometric
function:
# ∞
0
e−axJ0(bx) dx = 1
aF

1
2, 1; 1; −b2
a2

.
(d) Now use the result of Problem 11.4 to express the integral in a very simple
form.
27.13. By writing the series representation of the Bessel function as in the
previous problem, and using the result of Problem 11.2, show that for integer
m
# ∞
0
e−axJm(bx) dx =
1
a√π
 b
a
m Γ(m/2 + 1)Γ((m + 1)/2)
Γ(m + 1)
· F
m
2 + 1, m + 1
2
; m + 1; −b2
a2

.
27.14. Multiply both sides of Equation (27.40) by ρJm(xmka/ρ) cos jϕ and
integrate appropriately to obtain Ajk. Switch cosine to sine and do the same
to ﬁnd Bjk.
27.15. Use Equation (27.18) to show that
# a
0
ρJ0
x0n
a ρ

dρ = a2
x0n
J1(x0n).
27.16. Use the Parseval relation (27.38) for f(ρ) = ρk to obtain
∞

n=1
1
x2mn
=
1
4(m + 1)
for any m. Hint: See Example 27.5.2.
27.17. A long heat conducting cylinder of radius a is composed of two halves
(with semicircular cross sections) with an inﬁnitesimal gap between them.
The upper and lower halves of the cylinder are in contact with heat baths of
temperatures +T0 and −T0, respectively. Find the temperature both inside
and outside the cylinder.

27.7 Problems
659
27.18. A long heat conducting cylinder of radius a is composed of two halves
(with semicircular cross sections) with an inﬁnitesimal gap between them.
The upper and lower halves of the cylinder are in contact with heat baths
of temperatures +T1 and −T1, respectively. The cylinder is inside a larger
cylinder (and coaxial with it) held at temperature T2. Find the temperature
inside the inner cylinder, between the two cylinders, and outside the outer
cylinder.
27.19. A long conducting cylinder of radius a is kept at potential V1. The
cylinder is inside a larger cylinder (and coaxial with it) held at potential V2.
Find the potential inside the inner cylinder, between the two cylinders, and
outside the outer cylinder.

Chapter 28
Other PDEs
of Mathematical Physics
Chapters 25, 26, and 27 discussed one of the most important PDEs of mathe-
matical physics, Laplace’s equation. The techniques used in solving Laplace’s
equation apply to all PDEs encountered in introductory physics. Since we
have already spent a considerable amount of time on these techniques, we
shall simply provide some illustrative examples of solving other PDEs.
28.1
The Heat Equation
The heat equation, sometimes also called the diﬀusion equation, was in-
diﬀusion equation
troduced in Chapter 22 [see Equation (22.3)]. The separation of variables
T (t, r) = g(t)R(r) yields
∂
∂t [g(t)R(r)] = k2∇2[g(t)R(r)] ⇒R(r)dg
dt = k2g(t)∇2R.
Dividing both sides by g(t)R(r), we obtain
1
g
dg
dt = k2 1
R∇2R
  
≡−λ
.
The LHS is a function of t, and the RHS a function of r. The independence of
these variables forces each side to be a constant. Calling this constant −k2λ
for later convenience, we obtain an ODE in time and a PDE in the remaining
variables:
dg
dt + k2λg = 0
and
∇2R + λR = 0.
(28.1)
The general solution of the ﬁrst equation is
g(t) = Ae−k2λt
(28.2)

662
Other PDEs of Mathematical Physics
and that of the second equation can be obtained precisely by the methods of
the last chapter. We illustrate this by some examples, but ﬁrst we need to
keep in mind that λ is to be assumed positive, otherwise the exponential in
λ of the heat
equation is always
positive
Equation (28.2) will cause a growth of g(t) (and, therefore, the temperature)
beyond bounds.
28.1.1
Heat-Conducting Rod
Let us consider a one-dimensional conducting rod with one end at the origin
x = 0 and the other at x = b. The two ends are held at T = 0. Initially,
heat-conducting
rod
at t = 0, we assume a temperature distribution on the rod given by some
function f(x). We want to calculate the temperature at time t at any point
x on the rod.
Due to the one-dimensionality of the rod, the y- and z-dependence can be
ignored, and the Laplacian is reduced to a second derivative in x. Thus, the
second equation in (28.1) becomes
d2X
dx2 + λX = 0,
(28.3)
where X is a function of x alone. The general solution of this equation is1
X(x) = B cos(
√
λ x) + C sin(
√
λ x).
Since the two ends of the rod are held at T = 0, we have the boundary
conditions T (t, 0) = 0 = T (t, b), which imply that X(0) = 0 = X(b). These
give B = 0 and2
sin(
√
λ b) = 0 ⇒
√
λ b = nπ
for
n = 1, 2, . . ..
With a label n attached to λ, the solution, and the constant multiplying it,
we can now write
λn =
nπ
b
2
and
Xn(x) = Cn sin
nπ
b x

for
n = 1, 2, . . . .
The (subscripted) solution of the time equation is also simply obtained:
gn(t) = Ane−k2(nπ/b)2t.
This leads to a general solution of the form
T (t, x) =
∞

n=1
Bne−(nπk/b)2t sin
nπ
b x

,
(28.4)
where Bn ≡AnCn. The initial condition f(x) = T (0, x) yields
f(x) =
∞

n=1
Bn sin(nπx/b)
1The reader may check that the only solution for λ = 0 is the trivial solution.
2Consult Section 25.2.

28.1 The Heat Equation
663
which is a Fourier series from which we can calculate the coeﬃcients
Bn = 2
b
# b
0
sin
nπ
b x

f(x) dx.
Thus if we know the initial temperature distribution on the rod [the function
f(x)], we can determine the temperature of the rod for all time. For instance,
if the initial temperature distribution of the rod is uniform, say T0, then
Bn = 2T0
b
# b
0
sin
nπ
b x

dx = 2T0
b

−b
nπ cos
nπ
b x
b
0
= 2T0
nπ [1 −(−1)n].
It follows that the odd n’s survive, and if we set n = 2m + 1, we obtain
B2m+1 =
4T0
π(2m + 1)
and
T (t, x) = 4T0
π
∞

m=0
e−[(2m+1)πk/b]2t
2m + 1
sin
(2m + 1)π
b
x

.
This distribution of temperature for all time can be obtained numerically for
any heat conductor whose k is known. Note that the exponential in the sum
causes the temperature to drop to zero (the ﬁxed temperature of its two end
points) eventually. This conclusion is independent of the initial temperature
distribution of the rod as Equation (28.4) indicates.
28.1.2
Heat Conduction in a Rectangular Plate
As a more complicated example involving a second spatial variable, consider
a rectangular heat-conducting plate with sides of length a and b all held at
T = 0. Assume that at time t = 0 the temperature has a distribution function
conduction of heat
in a rectangular
plate
f(x, y). Let us ﬁnd the variation of temperature for all points (x, y) at all
times t > 0.
The spatial part of the heat equation for this problem is
∂2R
∂x2 + ∂2R
∂y2 + λR = 0.
A separation of variables, R(x, y) = X(x)Y (y), and its usual procedure leads
to the following equation:
1
X
d2X
dx2
  
≡−μ
+ 1
Y
d2Y
dy2
  
≡−ν
+ λ = 0.
This leads to the following two ODEs:
d2X
dx2 + μX = 0,
d2Y
dy2 + νY = 0,
λ = μ + ν.

664
Other PDEs of Mathematical Physics
Due to the periodicity of the BCs, the general solutions of these equations are
trigonometric functions. The four boundary conditions
T (0, y, t) = T (a, y, t) = T (x, 0, t) = T (x, b, t) = 0
determine the speciﬁc form of the solutions as well as the indexed constants
of separation:
μn =
nπ
a
2
and
Xn(x) = An sin
nπ
a x

for
n = 1, 2, . . .,
νm =
mπ
b
2
and
Ym(y) = Bm sin
mπ
b y

for
m = 1, 2, . . . .
So, λ becomes a double indexed quantity:
λ ≡λmn = μn + νm =
nπ
a
2
+
mπ
b
2
.
The solution to the g equation can be expressed as g(t) = Cmne−k2λmnt.
Putting everything together, we obtain
T (x, y, t) =
∞

n=1
∞

m=1
Amne−k2λmnt sin
nπ
a x

sin
mπ
b y

,
where Amn = AnBmCmn is an arbitrary constant. To determine it, we impose
the initial condition T (x, y, 0) = f(x, y). This yields
f(x, y) =
∞

n=1
∞

m=1
Amn sin
nπ
a x

sin
mπ
b y

from which we ﬁnd the coeﬃcients Amn (see Theorem 25.2.5):
Amn = 4
ab
# a
0
dx
# b
0
dyf(x, y) sin
nπ
a x

sin
mπ
b y

.
28.1.3
Heat Conduction in a Circular Plate
In this example, we consider a circular plate of radius a whose rim is held
at T = 0 and whose initial surface temperature is characterized by a func-
tion f(ρ, ϕ). We are seeking the temperature distribution on the plate for
circular plate
all time. The spatial part of the heat equation in z-independent cylindrical
coordinates,3 appropriate for a circular plate, is
1
ρ
∂
∂ρ

ρ∂R
∂ρ

+ 1
ρ2
∂2R
∂ϕ2 + λR = 0
3See the discussion of Subsection 22.3.

28.1 The Heat Equation
665
which, after the separation of variables, R(ρ, ϕ) = R(ρ)S(ϕ), reduces to
S(ϕ) = A cos mϕ + B sin mϕ
for
m = 0, 1, 2, . . .,
d2R
dρ2 + 1
ρ
dR
dρ +

λ −m2
ρ2

R = 0.
The solution of the last (Bessel) equation, which is well deﬁned for ρ = 0 and
vanishes at ρ = a, is
R(ρ) = CJm
xmn
a ρ

with
√
λ = xmn
a
and n = 1, 2, . . .,
where, as usual, xmn is the nth root of Jm. We see that λ is a double-indexed
quantity. The time equation (28.2) has a solution of the form
g(t) = Dmne−k2λmnt = Dmne−k2(x2
mn/a2)t.
Multiplying the three solutions and summing over the two indices yields the
most general solution
T (ρ, ϕ, t) =
∞

m=0
∞

n=1
Jm
xmn
a ρ

e−(kxmn/a)2t(Amn cos mϕ + Bmn sin mϕ).
The coeﬃcients are determined from the initial condition
f(ρ, ϕ) = T (ρ, ϕ, 0) =
∞

m=0
∞

n=1
Jm
xmn
a ρ

(Amn cos mϕ + Bmn sin mϕ).
Except for the hyperbolic sine term, this equation is identical to (27.40).
Therefore, the coeﬃcients are given by expressions similar to Equation (27.41).
In the case at hand, we get
Amn =
2
πa2J2
m+1(xmn)
# 2π
0
dϕ
# a
0
dρ ρf(ρ, ϕ)Jm
xmn
a ρ

cos mϕ,
Bmn =
2
πa2J2
m+1(xmn)
# 2π
0
dϕ
# a
0
dρ ρf(ρ, ϕ)Jm
xmn
a ρ

sin mϕ.
In particular, if the initial temperature distribution is independent of ϕ,
then only the term with m = 0 contributes,4 and we get
T (ρ, t) =
∞

n=1
AnJ0
x0n
a ρ

e−(kx0n/a)2t.
With f(ρ) representing the ϕ-independent initial temperature distribution,
the coeﬃcient An is found to be
An =
4
a2J2
1 (x0n)
# a
0
dρ ρf(ρ)J0
x0n
a ρ

.
Note that the temperature distribution does not develop any ϕ dependence
at later times.
4See the discussion after Equation (27.41).

666
Other PDEs of Mathematical Physics
28.2
The Schr¨odinger Equation
Chapter 22 separated the time part of the Schr¨odinger equation from its space
part, and resulted in the following two equations:
∇2ψ + 2m
ℏ2 [E −V (r)]ψ = 0
and
dT
dt = iE
ℏT,
(28.5)
where E, the energy of the quantum particle, is the constant of separation.5
We have also used ψ instead of R, because the latter is usually reserved to
denote a function of the radial variable r (or ρ) when separating the variables
of the Laplacian in spherical (or cylindrical) coordinates.
The solution of the time part is easily obtained: It is simply
T (t) = AeiEt/ℏ= Aeiωt
where
ω ≡E
ℏ.
(28.6)
It is the solution of the ﬁrst equation in (28.5), the time-independent
time-independent
Schr¨odinger
equation
Schr¨odinger equation that will take up most of our time in this section.
Historical Notes
Erwin Schr¨odinger was a student at Vienna from 1906 and taught there for ten
years from 1910 to 1920 with a break for military service in World War I. While
at Vienna he worked on radioactivity, proving the statistical nature of radioactive
decay. He also made important contributions to the kinetic theory of solids, studying
the dynamics of crystal lattices.
After leaving Vienna in 1920 he was appointed to a professorship in Jena, where
he stayed for a short time. He then moved to Stuttgart, and later to Breslau before
accepting the chair of theoretical physics at Zurich in late 1921. During these years
of changing from one place to another, Schr¨odinger studied physiological optics, in
particular the theory of color vision.
Zurich was to be the place where Schr¨odinger made his most important contribu-
tions. From 1921 he studied atomic structure. In 1924 he began to study quantum
statistics soon after reading de Broglie’s thesis which was to have a major inﬂuence
on his thinking.
Schr¨odinger published very important work relating to wave mechanics and the
general theory of relativity in a series of papers in 1926. Wave mechanics, proposed
by Schr¨odinger in these papers, was the second formulation of quantum theory, the
ﬁrst being matrix mechanics due to Heisenberg.
For this work Schr¨odinger was
awarded the Nobel prize in 1933.
Erwin Schr¨odinger
1887–1961
Schr¨odinger went to Berlin in 1927 where he succeeded Planck as the chair of
theoretical physics and he became a colleague of Einstein’s.
Although he was a Catholic, Schr¨odinger decided in 1933 that he couldn’t live
in a country in which the persecution of Jews had become a national policy. He left,
spending time in Britain where he was at the University of Oxford from 1933 until
1936. In 1936 he went to Austria and spent the years 1936–1938 in Graz. However,
the advancing Nazi threat caught up with him again in Austria and he ﬂed again,
this time settling in Dublin, Ireland, in 1939.
5We used α in place of E in Chapter 22.

28.2 The Schr¨odinger Equation
667
His study of Greek science and philosophy is summarized in Nature and the
Greeks (1954) which he wrote while in Dublin. Another important book written
during this period was What Is Life (1944) which led to progress in biology. He
remained in Dublin until he retired in 1956 when he returned to Vienna.
During his last few years Schr¨odinger remained interested in mathematical physics
and continued to work on general relativity, uniﬁed ﬁeld theory, and meson
physics.
28.2.1
Quantum Harmonic Oscillator
As an important example of the Schr¨odinger equation, we consider a particle
quantum harmonic
oscillator
in a one-dimensional harmonic oscillator potential.
The one-dimensional time-independent Schr¨odinger equation for a particle
of mass μ in a potential V (x) is
d2ψ
dx2 + 2μ
ℏ2 [E −V (x)]ψ = 0,
where E is the total energy of the particle.
For a harmonic oscillator (with the “spring” constant k),
V (x) = 1
2kx2 ≡1
2μω2x2
and
ψ′′ −μ2ω2
ℏ2 x2ψ + 2μ
ℏ2 Eψ = 0,
ω ≡
"
k
μ.
To simplify the equation, we make the change of variables x = (
	
ℏ/μω)y.
The equation then becomes
ψ′′ −y2ψ + 2E
ℏω ψ = 0,
(28.7)
where the primes indicate diﬀerentiation with respect to y.
We could solve this DE by the Frobenius power series method. However,
tradition suggests that we ﬁrst look at the behavior of the solution at y →∞.
In this limit, we can ignore the last term in (28.7), and the DE becomes
ψ′′ −y2ψ ≈0
which can easily be shown to have (an approximate) solution of the form
e±y2/2. Since the positive exponent diverges at inﬁnity, we have to retain
only the solution with negative exponent.
Following the traditional steps,
we consider a solution of the form ψ(y) ≡H(y) exp(−y2/2) in which the
asymptotic function has been separated. Substitution of this separated form
of ψ in (28.7) results in
H′′ −2yH′ + λH = 0
where
λ = 2E
ℏω −1.
(28.8)

668
Other PDEs of Mathematical Physics
This is the Hermite diﬀerential equation.
Hermite
diﬀerential
equation
To solve the Hermite DE by the Frobenius method, we assume an expan-
sion of the form H(y) = ∞
n=0 cnyn with
H′(y) =
∞

n=1
ncnyn−1 =
∞

n=0
(n + 1)cn+1yn,
H′′(y) =
∞

n=1
n(n + 1)cn+1yn−1 =
∞

n=0
(n + 1)(n + 2)cn+2yn,
where in the last step of each equation, we changed the dummy index to
m = n −1, and in the end, replaced m with n. Substituting in Equation
(28.8) gives
∞

n=0
[(n + 1)(n + 2)cn+2 + λcn]yn



≡S1
−2
∞

n=0
(n + 1)cn+1yn+1 = 0.
(28.9)
Now separate the zeroth term of the ﬁrst sum to obtain
S1 = 2c2 + λc0 +
∞

n=1
[(n + 1)(n + 2)cn+2 + λcn]yn.
Changing the dummy index to m = n −1 yields
S1 = 2c2 + λc0 +
∞

m=0
[(m + 2)(m + 3)cm+3 + λcm+1]ym+1
whose dummy index can be switched back to n. Substitution of this last result
in (28.9) now yields
2c2 + λc0 +
∞

n=0
[(n + 2)(n + 3)cn+3 + λcn+1 −2(n + 1)cn+1]yn+1 = 0.
Setting the coeﬃcients of powers of y equal to zero, we obtain
c2 = −λ
2 c0,
cn+3 = 2(n + 1) −λ
(n + 2)(n + 3)cn+1
for n ≥0,
or, replacing n with n −1 and noting that the resulting recursion relation is
true for n = 0 as well, we obtain
recursion relation
for Hermite DE
cn+2 =
2n −λ
(n + 1)(n + 2)cn,
n ≥0.
(28.10)
The ratio test yields easily that the series is convergent for all values of y.
Physics dictates
mathematics!
However, on physical grounds, i.e., the demand that limx→∞ψ(x) = 0, the
series must be truncated. Let us see why.

28.2 The Schr¨odinger Equation
669
Construction of Hermite Polynomials
The fact that we are interested in the behavior of ψ (and therefore, H) as x
(or y) goes to inﬁnity permits us to concentrate on the very large powers of y
in the series for H(y). Hence, separating the even and odd parts of the series,
we may write
H(y) =
∞

k=0
c2ky2k +
∞

k=0
c2k+1y2k+1
= P2M+1(y) +
∞

k=M+1
c2ky2k +
∞

k=M+1
c2k+1y2k+1
(28.11)
= P2M+1(y) +
∞

k=0
c2k+2M+2y2k+2M+2 +
∞

k=0
c2k+2M+3y2k+2M+3,
where P2M+1(y) is a generic polynomial obtained by adding all the “small”
powers of the series, and M is a very large number.6 Now note that for very
large n, the recursion relation yields
cn+2 ≈
2n
(n + 1)(n + 2)cn ≈
2n
(n)(n)cn ≈2
ncn ⇒cn ≈
2
n −2cn−2.
A few iterations give
cn ≈
2k
(n −2)(n −4) · · · (n −2k)cn−2k.
In particular,
c2k+N ≈
2k
(2k + N −2)(2k + N −4) · · · (N)cN.
(28.12)
To ﬁnd the coeﬃcients in Equation (28.11), ﬁrst let N = 2M + 2 and obtain
c2k+2M+2 ≈
2k
(2k + 2M)(2k + 2M −2) · · · (2M + 2)c2M+2
=
2k
[2(k + M)][2(k + M −1)] · · · [2(M + 1)]c2M+2
(28.13)
=
1
(k + M)(k + M −1) · · · (M + 1)c2M+2 =
M!
(k + M)!c2M+2.
Similarly
c2k+2M+3 ≈
2k(2M + 1)!!
(2k + 2M + 1)!!c2M+3
=
2k[2(M + 1)]!/[2M+1(M + 1)!]
[2(k + M + 1)]!/[2k+M+1(k + M + 1)!]c2M+3
= 22k [2(M + 1)]!(k + M + 1)!
(M + 1)![2(k + M + 1)]!c2M+3,
(28.14)
6In particular, M is very large compared to λ of Equation (28.10).

670
Other PDEs of Mathematical Physics
where we used the result of Problem 11.1. By using the Stirling approximation
(11.6), the reader may verify that
c2k+2M+3 ≈
(M + 1)!
(k + M + 1)!c2M+3.
(28.15)
With the coeﬃcients given in terms of two constants (c2M+2 and c2M+3),
Equation (28.11) becomes
H(y) = P2M+1(y) + c2M+2M!y2
∞

k=0
y2k+2M
(k + M)! + c2M+3(M + 1)!y
∞

k=0
y2k+2M+2
(k + M + 1)!
= P2M+1(y) + c2M+2M!y2
∞

j=M
y2j
j! + c2M+3(M + 1)!y
∞

j=M+1
y2j
j! .
(28.16)
The ﬁrst sum over j can be reexpressed as follows:
∞

j=M
y2j
j! =
∞

j=0
(y2)j
j!
−
M−1

j=0
y2j
j! = ey2 −Q2M−2(y),
where Q2M−2(y) is a polynomial of degree 2M −2 in y. The second sum in
(28.16) can be expressed similarly. Adding all the polynomials together, we
ﬁnally get
H(y) ≈P2M+1(y) + c2M+2M!



≡βM
y2ey2 + c2M+3(M + 1)!



≡αM
yey2
= P2M+1(y) + (αMy + βMy2)ey2.
(28.17)
Let us now go back to ψ(y) and note that
ψ(y) = H(y)e−y2/2 ≈P2M+1(y)e−y2/2



→0 as y →∞
+ (αMy + βMy2)ey2/2



→∞as y →∞
because any exponential decay outweighs any polynomial growth. It follows
that, if H(y) is an inﬁnite series, ψ(y) will diverge at inﬁnity. From a physical
standpoint this means that the quantum particle inside the harmonic oscillator
potential well has an inﬁnite probability of being found at inﬁnity!7
To avoid this unrealistic conclusion, we have to reexamine H(y). The case
Truncation of the
inﬁnite series gives
the quantization
of harmonic
oscillator energy
levels.
of Legendre polynomials tells us that the inﬁnite series needs to be truncated.
This will take place only if the numerator of the recursion relation vanishes for
some n, i.e., if λ = 2m for some integer m. An immediate consequence of such
a truncation is the famous quantization of the harmonic oscillator energy:
2m = λ = 2E
ℏω −1 ⇒E = (m + 1
2)ℏω.
The polynomials obtained by truncating the inﬁnite series are called the
Hermite polynomials. We now construct them. With λ = 2m, the recursion
Hermite
polynomials
relation (28.10) can be written as
7The Copenhagen interpretation, the only valid interpretation of quantum mechan-
ics, states that |ψ(x)|2 is the probability density for ﬁnding the particle at x.

28.2 The Schr¨odinger Equation
671
cn = 2(n −m −2)
n(n −1)
cn−2 = −2(m + 2 −n)
n(n −1)
cn−2,
m ≥n ≥2.
The upper limit for n is due to the truncation mentioned above. After a few
iteration, the pattern will emerge and the reader may verify that
cn = (−1)k 2k(m + 2 −n)(m + 4 −n) · · · (m + 2k −n)
n(n −1) · · · (n −2k + 1)
cn−2k.
(28.18)
We need to consider the two cases of even and odd n separately. For n = 2k,
we get
c2k = (−1)k 2km(m −2) · · · (m + 2 −2k)
(2k)!
c0.
(28.19)
Now, since the numerator of (28.10) must vanish beyond some integer, and
since 2n = 4k, we must have λ = 4j or m = λ/2 = 2j for some integer j.
Then, the reader may check that
c2k = (−1)k
22kj!
(2k)!(j −k)!c0
(28.20)
and
H2j(y) = c(j)
0
j

k=0
(−1)k22kj!
(2k)!(j −k)!y2k,
(28.21)
where we have given the constant a superscript to distinguish among the c0’s
of diﬀerent j’s. The odd polynomials can be obtained similarly:
H2j+1(y) = c(j)
1
j

k=0
(−1)k22k+1j!
(2k + 1)!(j −k)!y2k+1.
(28.22)
The constants are determined by convention. To adhere to this convention,
we deﬁne
c(j)
0
= (−1)j(2j)!
j!
,
c(j)
1
= (−1)j(2j + 1)!
j!
.
The reader may check that, with these constants, the Hermite polynomials of
all degrees (even or odd) can be concisely written as
Hn(y) =
[n/2]

r=0
(−1)rn!
(n −2r)!r!(2y)n−2r,
(28.23)
where [a], for any real a, stands for the largest integer less than or equal to a.
Orthogonality of Hermite Polynomials
The Hermite polynomials satisfy an orthogonality relation resembling that of
the Legendre polynomials. We can obtain this relation by multiplying the DE

672
Other PDEs of Mathematical Physics
for Hm(x) by Hn(x)e−x2, and the DE for Hn(x) by Hm(x)e−x2 and subtract-
ing. The factor e−x2, the so-called weight function, may appear artiﬁcial in
weight function
for Hermite
polynomials
this derivation, but an in-depth analysis of the classical orthogonal polyno-
mials, of which Hermite and Legendre polynomials are examples, reveals that
such weight functions are necessary. The reason we did not see such a factor
in Legendre polynomials was that for them, the weight function is unity.8 At
any rate, the result of the above suggested calculation will be
(H′′
mHn −2xH′
mHn −H′′
nHm + 2xH′
nHm)e−x2 + (2m −2n)HmHne−x2 = 0.
The reader may easily verify that the ﬁrst term is the derivative of
(H′
mHn −H′
nHm)e−x2,
so that
d
dx
*
(H′
mHn −H′
nHm)e−x2+
+ (2m −2n)HmHne−x2 = 0
and if we integrate this over the entire real line, we obtain
(H′
mHn −H′
nHm)e−x2


∞
−∞+ (2m −2n)
# ∞
−∞
Hm(x)Hn(x)e−x2 dx = 0.
The ﬁrst term vanishes because of the exponential factor. It now follows that
if m ̸= n, then
orthogonality of
Hermite
polynomials
# ∞
−∞
Hm(x)Hn(x)e−x2 dx = 0.
(28.24)
Generating Function for Hermite Polynomials
We constructed the generating function for Legendre polynomials in Chapter
26. Here we want to do the same thing for Hermite polynomials. By deﬁnition,
the generating function must have an expansion of the form
g(t, x) =
∞

n=0
antnHn(x),
where an is a constant to be determined. Diﬀerentiate both sides with respect
to x assuming that t is a constant:
dg
dx =
∞

n=1
antnH′
n(x).
The sum starts at 1 because H′
0(x) = 0. Use the result of Problem 28.7 to
obtain
dg
dx =
∞

n=1
antn(2n)Hn−1(x) = 2t
∞

n=1
nantn−1Hn−1(x).
8We have no space to go into the details of the theory of classical orthogonal polynomials,
but the interested reader can ﬁnd a uniﬁed discussion of them in Hassani, S. Mathematical
Physics: A Modern Introduction to Its Foundations, Springer-Verlag, 1999, Chapter 7.

28.2 The Schr¨odinger Equation
673
Now choose the constant an in such a way that it satisﬁes the recursion relation
nan = an−1. It then follows that
dg
dx = 2t
∞

n=1
an−1tn−1Hn−1(x) = 2t
∞

m=0
amtmHm(x) = 2tg.
Thus
dg
g = 2t dx ⇒ln g = 2tx + ln C(t) ⇒g(t, x) = C(t)e2tx,
where the “constant” of integration has been given the possibility of depending
on the other variable, t. To ﬁnd this constant of integration, we ﬁrst determine
an:
an = an−1
n
=
an−2
n(n −1) = · · · =
an−k
n(n −1) · · · (n −k + 1) · · · = a0
n! .
Using our results obtained so far, we get
C(t)e2tx = a0
∞

n=0
tn
n!Hn(x) →
∞

n=0
tn
n!Hn(x),
where in the last step we absorbed a0 (really 1/a0) in the “constant” C(t).
To determine C(t), evaluate both sides of the equation at x = 0 and use the
result of Problem 28.8. This yields
C(t) =
∞

n=0
tn
n!Hn(0) =
∞

k=0
t2k
(2k)!
(−1)k(2k)!
k!
=
∞

k=0
(−t2)k
k!
= e−t2.
It follows that
e2tx−t2 =
∞

n=0
tn
n!Hn(x).
(28.25)
We now summarize what we have done:
generating
function for
Hermite
polynomials
Box 28.2.1. The nth coeﬃcient of the Maclaurin series expansion of the
generating function g(t, x) ≡e2tx−t2 about t = 0 is Hn(x). Speciﬁcally,
Hn(x) = ∂n
∂tn e2tx−t2



t=0
.
(28.26)
We can put the result above to immediate good use. Let us square both
sides of Equation (28.25), multiply by e−x2, and integrate the result from −∞

674
Other PDEs of Mathematical Physics
to +∞. For the LHS, we have
LHS =
# +∞
−∞
e2(2tx−t2)e−x2dx = e−2t2 # +∞
−∞
e−x2+4txdx
= e−2t2 # +∞
−∞
e−(x−2t)2+4t2dx = e2t2 # +∞
−∞
e−(x−2t)2dx
= e2t2 # +∞
−∞
e−u2du = √πe2t2 = √π
∞

n=0
(2t2)n
n!
= √π
∞

n=0
2nt2n
n!
,
where we introduced u = x −2t for the integration variable, and used the
result of Example 3.3.1.
To square the RHS, we need to write it as the product of two inﬁnite sums
using two diﬀerent dummy indices! Therefore,
RHS =
# +∞
−∞
 ∞

n=0
tn
n!Hn(x)
  ∞

m=0
tm
m!Hm(x)

e−x2dx
=
∞

m,n=0
tm+n
m!n!
# +∞
−∞
Hm(x)Hn(x)e−x2dx



=0 unless m = n by (28.24)
=
∞

n=0
t2n
(n!)2
# +∞
−∞
[Hn(x)]2e−x2dx.
Comparing the LHS and the RHS, we conclude that
# +∞
−∞
[Hn(x)]2e−x2dx = √π 2nn!.
We can combine this result and Equation (28.24) and write
# +∞
−∞
Hm(x)Hn(x)e−x2dx = √π 2nn!δmn,
(28.27)
where δmn is the Kronecker delta which is 1 if m = n and 0 if m ̸= n.
Historical Notes
Charles Hermite, one of the most eminent French mathematicians of the nine-
teenth century, was particularly distinguished for the clean elegance and high artistic
quality of his work. As a student, he courted disaster by neglecting his routine as-
signed work to study the classic masters of mathematics; and though he nearly
failed his examinations, he became a ﬁrst-rate creative mathematician while still in
his early twenties. In 1870 he was appointed to a professorship at the Sorbonne,
where he trained a whole generation of well-known French mathematicians, includ-
ing Picard, Borel, and Poincar´e.
Charles Hermite
1822–1901
The character of his mind is suggested by a remark of Poincar´e: “Talk with
M. Hermite.
He never evokes a concrete image, yet you soon perceive that the

28.2 The Schr¨odinger Equation
675
most abstract entities are to him like living creatures.” He disliked geometry, but
was strongly attracted to number theory and analysis, and his favorite subject was
elliptic functions, where these two ﬁelds touch in many remarkable ways. Earlier in
the century the Norwegian genius Abel had proved that the general equation of the
ﬁfth degree cannot be solved by functions involving only rational operations and
root extractions. One of Hermite’s most surprising achievements (in 1858) was to
show that this equation can be solved by elliptic functions.
His 1873 proof of the transcendence of e was another high point of his career.9
If he had been willing to dig even deeper into this vein, he could probably have
disposed of π as well, but apparently he had had enough of a good thing. As he
wrote to a friend, “I shall risk nothing on an attempt to prove the transcendence
of the number π. If others undertake this enterprise, no one will be happier than I
at their success, but believe me, my dear friend, it will not fail to cost them some
eﬀorts.” As it turned out, Lindemann’s proof nine years later rested on extending
Hermite’s method.
Several of his purely mathematical discoveries had unexpected applications many
years later to mathematical physics. For example, the Hermitian forms and matrices
that he invented in connection with certain problems of number theory turned out
to be crucial for Heisenberg’s 1925 formulation of quantum mechanics, and Hermite
polynomials are useful in solving Schr¨odinger’s wave equation.
28.2.2
Quantum Particle in a Box
The behavior of an atomic particle of mass μ conﬁned in a rectangular box
with sides a, b, and c (an inﬁnite three-dimensional potential well) is gov-
quantum particle
in a box
erned by the Schr¨odinger equation for a free particle, i.e., V = 0. With this
assumption, the ﬁrst equation of (28.5) becomes
∇2ψ + 2μE
ℏ2 ψ = 0.
A separation of variables, ψ(x, y, z) = X(x)Y (y)Z(z), yields the ODEs:
d2X
dx2 + λX = 0,
d2Y
dy2 + σY = 0,
d2Z
dz2 + νX = 0,
with λ + σ + ν = 2μE/ℏ2 (see Example 22.2.1).
These equations, together with the boundary conditions
ψ(0, y, z) = ψ(a, y, z) = 0
⇒
X(0) = 0 = X(a),
ψ(x, 0, z) = ψ(x, b, z) = 0
⇒
Y (0) = 0 = Y (b),
(28.28)
ψ(x, y, 0) = ψ(x, y, c) = 0
⇒
Z(0) = 0 = Z(c),
9Transcendental numbers are those that are not roots of polynomials with integer
coeﬃcients.

676
Other PDEs of Mathematical Physics
lead to the following solutions:
Xn(x) = sin
nπ
a x

,
λn =
nπ
a
2
for
n = 1, 2, . . . ,
Ym(y) = sin
mπ
b y

,
σm =
mπ
b
2
for
m = 1, 2, . . . ,
Zl(z) = sin
lπ
c z

,
νl =
lπ
c
2
for
l = 1, 2, . . .,
where the multiplicative constants have been suppressed.
The BCs in Equation (28.28) arise from the demand that the probability
quantum
tunneling
of ﬁnding the particle be continuous and that it be zero outside the box. This
is not true for a particle inside a ﬁnite potential well, in which case the particle
has a nonzero probability of “tunneling” out of the well.
The time equation has a solution of the form
T (t) = e−iωlmnt
where
ωlmn = ℏ
2μ
Anπ
a
2
+
mπ
b
2
+
lπ
c
2B
.
The solution of the Schr¨odinger equation that is consistent with the bound-
ary conditions is, therefore,
ψ(x, y, z, t) =
∞

l,m,n=1
Almne−iωlmnt sin
nπ
a x

sin
mπ
b y

sin
lπ
c z

.
The constants Almn are determined by the initial shape ψ(x, y, z, 0) of the
wave function. In fact, setting t = 0, multiplying by the product of the three
sine functions in the three variables, and integrating over appropriate intervals
for each coordinate, we obtain
Almn =
8
abc
# a
0
dx
# b
0
dy
# c
0
dzψ(x, y, z, 0) sin
nπ
a x

sin
mπ
b y

sin
lπ
c z

.
The energy of the particle is
E = ℏωlmn = ℏ2π2
2μ
n2
a2 + m2
b2 + l2
c2

.
Each set of three positive integers (n, m, l) represents a quantum state of
the particle. For a cube, a = b = c ≡L, and the energy of the particle is
E = ℏ2π2
2μL2 (n2 + m2 + l2) =
ℏ2π2
2μV 2/3 (n2 + m2 + l2),
(28.29)
where V = L3 is the volume of the box. The ground state is (1, 1, 1), has
energy E = 3ℏ2π2/2μV 2/3, and is nondegenerate (only one state corresponds
to this energy). However, the higher-level states are degenerate. For instance,

28.2 The Schr¨odinger Equation
677
the three distinct states (1, 1, 2), (1, 2, 1), and (2, 1, 1) all correspond to the
same energy, E = 6ℏ2π2/2μV 2/3.
The degeneracy increases rapidly with
larger values of n, m, and l.
Equation (28.29) can be written as
n2 + m2 + l2 = R2
where
R2 = 2μEV 2/3
ℏ2π2
.
This looks like the equation of a sphere in the nml-space. If R is large, the
number of states contained within the sphere of radius R (the number of states
with energy less than or equal to E) is simply the volume of the ﬁrst octant10
of the sphere. If N is the number of such states, we have
N = 1
8
4π
3

R3 = π
6
2μEV 2/3
ℏ2π2
3/2
= π
6
 2μE
ℏ2π2
3/2
V.
Thus the density of states (the number of states per unit volume) is then
density of states
n = N
V = π
6
 2μ
ℏ2π2
3/2
E3/2.
(28.30)
This is an important formula in solid-state physics, because the energy E is
(with minor modiﬁcations required by spin) the Fermi energy. If the Fermi
Fermi energy
energy is denoted by Ef, Equation (28.30) gives Ef = αn2/3 where α is some
constant.
28.2.3
Hydrogen Atom
When an electron moves around a nucleus containing Z protons, the potential
energy of the system is V (r) = −Ze2/r. In units in which ℏand the mass of
the electron are set equal to unity, the time-independent Schr¨odinger equation
of (28.5) gives
∇2Ψ +

2E + 2Ze2
r

Ψ = 0.
The radial part of this equation is given by the ﬁrst equation in (22.16) with
f(r) = 2E + 2Ze2/r. Deﬁning u = rR(r), we may write
d2u
dr2 +

λ + a
r −α
r2

u = 0,
(28.31)
where λ = 2E and a = 2Ze2. This equation can be further simpliﬁed by
deﬁning r ≡kz (k is an arbitrary constant to be determined later):
d2u
dz2 +

λk2 + ak
z −α
z2

u = 0.
10This is because n, m, and l are all positive.

678
Other PDEs of Mathematical Physics
Choosing λk2 = −1
4 and introducing β ≡a/(2
√
−λ) yields
d2u
dz2 +

−1
4 + β
z −α
z2

u = 0.
(28.32)
Let us examine the two limiting cases of z →∞and z →0. For the ﬁrst
case, Equation (28.32) reduces to
d2u
dz2 −1
4u = 0 ⇒u = e−z/2.
For the second case the dominant term will be α/z2 and the DE becomes
d2u
dz2 −α
z2 u = 0
for which we try a solution of the form zm with m to be determined by
substitution:
d2u
dz2 = m(m −1)zm−2 ⇒m(m −1)zm−2 −α
z2 zm = 0 ⇒α = m(m −1).
Recalling from Theorem 26.2.1 that α = l(l + 1), we determine m to be l + 1.
Factoring out these two limits, we seek a solution for (28.32) of the form
u(z) = zl+1e−z/2f(z).
Substitution of this function in (28.32) gives a new DE:
f ′′ +
2(l + 1)
z
−1

f ′ −l + 1 −β
z
f = 0.
(28.33)
Multiplying by z gives
zf ′′ + [2(l + 1) −z]f ′ −(l + 1 −β)f = 0
which is a conﬂuent hypergeometric DE [see Equation (11.27)]. Therefore, as
the reader may verify, f is proportional to Φ(l + 1 −β, 2l + 2; z). Thus, the
solution of (28.31) can be written as
u(z) = Czl+1e−z/2Φ(l + 1 −β, 2l + 2; z).
Laguerre Polynomials
An argument similar to that used in the discussion of a quantum harmonic
oscillator will reveal that the product e−z/2Φ(l+1−β, 2l+2; z) will be inﬁnite
unless the power series representing Φ terminates (becomes a polynomial).
This takes place only if (see Box 11.2.2)
l + 1 −β = −N
(28.34)

28.2 The Schr¨odinger Equation
679
for some integer N ≥0. In that case we obtain the Laguerre polynomials
Laguerre
polynomials
Lj
N ≡
Γ(N + j + 1)
Γ(N + 1)Γ(j + 1)Φ(−N, j + 1; z)
where
j = 2l + 1,
(28.35)
where the factor in front of Φ is a standardization factor.
Condition (28.34) is the quantization rule for the energy levels of a hydrogen- Truncation of
inﬁnite series gives
the quantization
rule for the energy
levels of the
hydrogen atom.
like atom. Writing everything in terms of the original parameters, and re-
deﬁning β as β = N + l + 1 ≡n to reﬂect its integer character, yields—after
restoring all the μ’s and the ℏ’s—the energy levels of a hydrogen-like atom:
E = −Z2μe4
2ℏ2n2 = −Z2
μc2
2

α2 1
n2 ,
where α = e2/ℏc = 1/137 is the ﬁne structure constant.
ﬁne structure
constant
The radial wave functions can now be written as
Rn,l(r) = un,l(r)
r
= Crle−Zr/na0Φ

−n + l + 1, 2l + 2; 2Zr
na0

,
where a0 = ℏ2/me2 = 0.529 × 10−8 cm is the Bohr radius.
The explicit form of Laguerre polynomials can be obtained by substitut-
ing the truncated conﬂuent hypergeometric series [see Equation (11.28)] in
(28.35):
Lj
N(x) =
Γ(N + j + 1)
Γ(N + 1)Γ(j + 1)
Γ(j + 1)
Γ(−N)
N

k=0
Γ(−N + k)
Γ(j + 1 + k)Γ(k + 1)xk.
We now use the result of Problem 11.4 and write
Γ(k −N)
Γ(−N)
= (−1)kN(N −1) · · · (N −k + 1) = (−1)kN!
(N −k)!.
It follows that
Lj
N(x) = Γ(N + j + 1)
Γ(N + 1)
N

k=0
(−1)kN!
(N −k)!
1
Γ(j + 1 + k)Γ(k + 1)xk.
Simplifying and writing all gamma functions in terms of factorials, we obtain
the ﬁnal form of the Laguerre polynomials:
Lj
N(x) =
N

k=0
(N + j)!(−1)k
(N −k)!k!(k + j)!xk.
(28.36)
The generating function of the Laguerre polynomials can be calculated
using a procedure similar to the one used in the case of Hermite polynomials.
We ﬁrst write
gj(t, x) =
∞

n=0
antnLj
n(x),

680
Other PDEs of Mathematical Physics
diﬀerentiate it with respect to x, and use the result of Problem 28.10 to obtain
dgj
dx = −
∞

n=0
antnLj+1
n−1(x) = −t
∞

n=0
antn−1Lj+1
n−1(x) = −tgj+1,
(28.37)
where we have taken an = an−1 as a natural choice whereby the last sum
could be written in closed form. To ﬁnd a solution of (28.37), we look at
g(t, 0). The recursion relation an = an−1 implies that all an are equal, and
we set all of them equal to 1. Then
gj(t, 0) =
∞

n=0
tnLj
n(0) =
∞

n=0
(n + j)!
n!j!
tn = (1 −t)−j−1,
where we used the fact that the only contribution to Lj
n(0) comes from the con-
stant term in the polynomial [corresponding to k = 0 in (28.36)]. Furthermore,
the last sum is the binomial series (10.15) with x →−t and α →(−j −1).
This suggests deﬁning a new function g(t, x) via
gj(t, x) = (1 −t)−j−1g(t, x).
Substitution of this in (28.37) gives
(1 −t)−j−1 dg
dx = −t(1 −t)−j−2g ⇒dg
g =
−t
1 −t dx ⇒g = C(t)e−tx/(1−t)
and
gj(t, x) = (1 −t)−j−1C(t)e−tx/(1−t) = C(t)e−tx/(1−t)
(1 −t)j+1
.
With the value of gj(t, 0) given, we determine C(t) to be one.
Box 28.2.2. The nth coeﬃcient of the Maclaurin series expansion of the
generating function gj(t, x) ≡(1 −t)−j−1e−tx/(1−t) about t = 0 is Lj
n(x).
Speciﬁcally,
Lj
n(x) = 1
n!
∂n
∂tn
e−tx/(1−t)
(1 −t)j+1




t=0
.
(28.38)
28.3
The Wave Equation
In the preceding sections the time variation has been given by a ﬁrst derivative.
Thus, as far as time is concerned, we have a FODE. It follows that the initial
speciﬁcation of the physical quantity of interest (temperature T or Schr¨odinger
wave function ψ) is suﬃcient to determine the solution uniquely.
The wave equation
∇2ψ = 1
c2
∂2ψ
∂t2
(28.39)

28.3 The Wave Equation
681
contains time derivatives of the second order, and, therefore, requires two ar-
bitrary parameters in its general solution. To determine these, we expect two
initial conditions. For example, if the wave is standing, as in a rope clamped at
both ends, the initial shape of the rope is not suﬃcient to determine the wave
function uniquely. One also needs to specify the initial (transverse) velocity
of each point of the rope, i.e., the velocity proﬁle on the rope.
Example 28.3.1. one-dimensional wave
one-dimensional
wave
The simplest kind of wave equation is that in one dimension, for example, a wave
propagating on a rope. Such a wave equation can be written as
∂2ψ
∂x2 = 1
c2
∂2ψ
∂t2 ,
where c is the speed of wave propagation. For a rope, this speed is related to the
tension τ and the linear mass density ρ by c =
	
τ/ρ.
Let us assume that the rope has length a and is fastened at both ends (located
at x = 0 and x = a). This means that the “displacement” ψ is zero at x = 0 and at
x = a.
A separation of variables, ψ(x, t) = X(x)T(t), leads to two ODEs:
d2X
dx2 + λX = 0,
d2T
dt2 + c2λT = 0.
(28.40)
The ﬁrst equation and the spatial boundary conditions give rise to the solutions
Xn(x) = sin
 nπ
a x

,
λn =
 nπ
a
2
for
n = 1, 2, . . . .
The second equation in (28.40) has a general solution of the form
T(t) = An cos ωnt + Bn sin ωnt,
where ωn = cnπ/a and An and Bn are arbitrary constants. The general solution is
thus
ψ(x, t) =
∞

n=1
(An cos ωnt + Bn sin ωnt) sin
nπx
a

.
(28.41)
Speciﬁcation of the initial shape of the rope as ψ(x, 0) = f(x) gives a Fourier
series,
f(x) =
∞

n=1
An sin
 nπx
a

from which we can determine An:
An = 2
a
# a
0
f(x) sin
 nπx
a

dx.
What about Bn?
Physically, the shape of the wave is not enough to deﬁne the
problem uniquely.
It is possible that the rope, while having the required initial
shape, may be in an unspeciﬁed motion of some sort. Thus, we must also know the
“velocity proﬁle,” which means specifying the function ∂ψ/∂t at t = 0. If it is given

682
Other PDEs of Mathematical Physics
that ∂ψ/∂t|t=0 = g(x), then diﬀerentiating (28.41) with respect to t and evaluating
both sides at t = 0 yields
g(x) =
∞

n=1
ωnBn sin
 nπx
a

and Bn is also determined:
Bn =
2
aωn
# a
0
g(x)sin
 nπx
a

dx.
The frequency ωn is referred to as that of the nth mode of oscillation. Thus
mode of
oscillation
a general solution is a linear superposition of inﬁnitely many modes. In practice,
it is possible to “excite” one mode or, with appropriate initial conditions, a ﬁnite
number of modes.
■
28.3.1
Guided Waves
Waveguides are hollow tubes (or tubes ﬁlled with some dielectric material) in
which electromagnetic waves can propagate along an axis which we take to
be the z-axis of either Cartesian or cylindrical coordinates. We assume that
the dependence of the electric and magnetic ﬁelds on z and t is of the form
ei(ωt−kz) where ω and k are constants to be determined. We therefore write
E = E0(x, y)ei(ωt−kz),
B = B0(x, y)ei(ωt−kz),
(28.42)
for Cartesian coordinates. If cylindrical geometry is appropriate, then E0 and
B0 will be functions of ρ and ϕ. Note that, in general, E0 and B0 have three
components.
The electric and magnetic ﬁelds of (28.42) ought to satisfy the four Maxwell’s
equations. Let us assume that the waveguide is free of any charges or cur-
rents, so that Maxwell’s equations for empty space are appropriate. Because
of the nature of the dependence on z, it is useful to separate the longitudi-
nal geometry—the geometry along z—from the transverse geometry—the
longitudinal and
transverse parts of
guided waves.
geometry perpendicular to z. So, we write
E = Et + ˆezEz = (E0t + ˆezE0z)ei(ωt−kz),
B = Bt + ˆezBz = (B0t + ˆezB0z)ei(ωt−kz),
(28.43)
∇= ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z ≡∇t + ˆez
∂
∂z ,
where the subscript t stands for transverse. With these assumptions, Maxwell’s
ﬁrst equation becomes
0 = ∇· E =

∇t + ˆez
∂
∂z

·
*
(E0t + ˆezE0z)ei(ωt−kz)+
= (∇t · E0t) ei(ωt−kz) + (−ikE0z) ei(ωt−kz),

28.3 The Wave Equation
683
because ∇t · (ˆezE0z) = 0 and ˆez · E0t = 0. It follows that
∇t · E0t = ikE0z.
An analogous calculation gives a similar result for Maxwell’s second equation.
Putting these two equations together, we get
∇t · E0t = ikE0z,
∇t · B0t = ikB0z.
(28.44)
The LHS of Maxwell’s third equation gives
LHS = ∇× E =

∇t + ˆez
∂
∂z

×
*
E0ei(ωt−kz)+
= ∇t ×
*
E0ei(ωt−kz)+
+ ˆez ×
*
−ikE0ei(ωt−kz)+
= ei(ωt−kz) (∇t × E0 −ikˆez × E0t) .
The RHS of the third equation is
−∂B
∂t = −iωB0ei(ωt−kz).
Equating the two sides yields
−iωB0 = ∇t × E0 −ikˆez × E0t.
(28.45)
The ﬁrst term on the RHS can be written as
∇t × E0 =











ˆex
ˆey
ˆez
∂
∂x
∂
∂y
0
E0x
E0y
E0z











= ∂E0z
∂y ˆex −∂E0z
∂x ˆey



This is transverse.
+ˆez
∂E0y
∂x
−∂E0x
∂y

= ∇× (E0zˆez) + ˆez
∂E0y
∂x
−∂E0x
∂y

.
The reader may easily check that the second line follows from the ﬁrst. Equat-
ing the transverse parts of the two sides of Equation (28.45), we get
−iωB0t = ∇× (E0zˆez) −ikˆez × E0t.
(28.46)
A similar calculation turns the fourth Maxwell equation into
i ω
c2 E0t = ∇× (B0zˆez) −ikˆez × B0t.
(28.47)
We now want to express the transverse components in terms of the lon-
gitudinal components. Multiply both sides of (28.47) by −iω and substitute
for −iωB0t from (28.46):
ω2
c2 E0t = −iω∇× (B0zˆez) −ikˆez × [∇× (E0zˆez) −ikˆez × E0t]
= −iω∇× (B0zˆez) −ikˆez × [∇× (E0zˆez)] −k2ˆez × (ˆez × E0t).

684
Other PDEs of Mathematical Physics
Using the bac cab rule, the last term gives
ˆez × (ˆez × E0t) = ˆez (ˆez · E0t)



=0
−E0t(ˆez · ˆez) = −E0t.
It now follows that
ω2
c2 −k2

E0t = −iω∇× (B0zˆez) −ikˆez × [∇× (E0zˆez)] .
(28.48)
The ﬁrst term on the RHS can be simpliﬁed by using the second equation in
(14.11):
∇× (B0zˆez) = B0z ∇× ˆez
  
=0
+(∇B0z) × ˆez = −ˆez × (∇tB0z)
because ˆez is a constant vector (both magnitude and direction), and B0z is
independent of z. The second term on the RHS of (28.48) can be simpliﬁed
as follows:
ˆez × [∇× (E0zˆez)] = ˆez ×
∂E0z
∂y ˆex −∂E0z
∂x ˆey

= ∂E0z
∂y ˆey + ∂E0z
∂x ˆex ≡∇tE0z.
Substituting these results in Equation (28.48) yields
γ2E0t = i [−k∇tE0z + ωˆez × (∇tB0z)]
where
γ2 ≡ω2
c2 −k2.
A similar calculation gives an analogous result for the magnetic ﬁeld. We
assemble these two equations in
γ2E0t = i [−k∇tE0z + ωˆez × (∇tB0z)] ,
(28.49)
γ2B0t = i [−k∇tB0z −ωˆez × (∇tE0z)] ,
γ2 ≡ω2
c2 −k2.
Although we derived (28.44) and (28.49) using Cartesian coordinates, the
fact that the ﬁnal result is written explicitly in terms of transverse and lon-
gitudinal parts—without reference to any coordinate system—implies that
these equations are valid in cylindrical coordinates as well.
Three types of guided waves are usually studied.
1. Transverse magnetic (TM) waves have Bz = 0 everywhere. The bound-
transverse
magnetic (TM)
waves
ary condition on E demands that Ez vanish at the conducting walls of
the guide.
2. Transverse electric (TE) waves have Ez = 0 everywhere. The boundary
transverse electric
(TE) waves
condition on B requires that the normal directional derivative
∂Bz
∂n ≡ˆen · (∇Bz)
vanish at the walls.

28.3 The Wave Equation
685
3. Transverse electromagnetic (TEM) waves have Bz = 0 = Ez. For a
transverse
electromagnetic
(TEM) waves
nontrivial solution, Equation (28.49) demands that γ2 = 0. This form
resembles a free wave with no boundaries.
In the following, we consider some examples of the TM mode (see any
book on electromagnetic theory for further details). The basic equations in
this mode are
B0z = 0,
γ2E0t = −ik∇tE0z,
γ2B0t = −iωˆez × (∇tE0z).
Taking the dot product of ∇t with the middle equation and using the ﬁrst
equation in (28.44) yields
Basic equation for
TM waves.
∇2
t E0z + γ2E0z = 0.
(28.50)
This is the basic equation for TM waves propagating in a waveguide.
Example 28.3.2. rectangular wave guides
rectangular wave
guides
For a wave guide with a rectangular cross section of sides a and b in the x and the
y direction, respectively, (28.50) gives
∂2E0z
∂x2
+ ∂2E0z
∂y2
+ γ2E0z = 0.
A separation of variables, E0z(x, y) = X(x)Y (y), leads to
d2X
dx2 + λX = 0,
X(0) = 0 = X(a),
d2Y
dy2 + μY = 0,
Y (0) = 0 = Y (b),
where γ2 = λ + μ. These equations have the solutions
Xn(x) = sin
 nπ
a x

,
λn =
 nπ
a
2
for
n = 1, 2, . . . ,
Ym(y) = sin
mπ
b y

,
μm =
 mπ
b
2
for
m = 1, 2, . . . .
The wave number is given by k =
	
(ω/c)2 −γ2, or, introducing indexes for k,
kmn =

ω2
c2 −
 nπ
a
2
−
 mπ
b
2
,
which has to be real if the wave is to propagate [an imaginary k leads to exponential
decay or growth along the z-axis because of the exponential factor in (28.49)]. Thus,
there is a cut-oﬀfrequency,
ωmn = c
 nπ
a
2
+
 mπ
b
2
for
m, n ≥1,
below which the wave cannot propagate through the wave guide. It follows that,
for a TM wave, the lowest frequency that can propagate along a rectangular wave
guide is ω11 = πc
√
a2 + b2/ab.

686
Other PDEs of Mathematical Physics
The most general solution for Ez is, therefore,
Ez =
∞

m,n=1
Amn sin
 nπ
a x

sin
 mπ
b y

ei(ωt±kmnz).
The constants Amn are arbitrary and can be determined from the initial shape of
the wave, but that is not commonly done. Once Ez is found, the other components
can be calculated using Equation (28.50).
■
Example 28.3.3. cylindrical wave guide
cylindrical wave
guide
For a TM wave propagating along the z-axis in a hollow circular conductor, we have
[see Equation (28.50)]
1
ρ
∂
∂ρ

ρ∂E0z
∂ρ

+ 1
ρ2
∂2E0z
∂ϕ2



≡∇2
t E0z
+ γ2E0z = 0.
The separation E0z = R(ρ)S(ϕ) yields S(ϕ) = A cos mϕ + B sin mϕ and the Bessel
DE
d2R
dρ2 + 1
ρ
dR
dρ +

γ2 −m2
ρ2

R = 0.
The solution to this equation, which is regular at ρ = 0 and vanishes at ρ = a, is
R(ρ) = CJm
 xmn
a
ρ

and
γ = xmn
a
.
Recalling the deﬁnition of γ, we obtain
ω2
c2 −k2 = γ2 = x2
mn
a2
⇒k =

ω2
c2 −x2mn
a2 .
This gives the cut-oﬀfrequency ωmn = cxmn/a.
The solution for the azimuthally symmetric case (m = 0) is
Ez(ρ, ϕ, t) =
∞

n=1
AnJ0
 x0n
a ρ

ei(ωt±knz)
and
Bz = 0,
where kn =
	
ω2/c2 −x2
0n/a2.
■
28.3.2
Vibrating Membrane
Waves on a circular drumhead are historically important because their inves-
tigation was one of the ﬁrst instances in which Bessel functions appeared. The
following example considers such waves.
For a circular membrane over a cylinder, the wave equation (28.39) in
cylindrical coordinates becomes11
1
ρ
∂
∂ρ

ρ∂ψ
∂ρ

+ 1
ρ2
∂2ψ
∂ϕ2 = 1
c2
∂2ψ
∂t2
11Assuming that the membrane is perpendicular to the z-axis, the wave amplitude will
depend only on ρ and ϕ.

28.4 Problems
687
which, after separation of variables, reduces to
S(ϕ) = A cos mϕ + B sin mϕ
for
m = 0, 1, 2, . . .,
T (t) = C cos ωt + D sin ωt,
d2R
dρ2 + 1
ρ
dR
dρ +
ω2
c2 −m2
ρ2

R = 0.
The solution of the last (Bessel) equation, which is deﬁned for ρ = 0 and
vanishes at ρ = a, is
R(ρ) = EJm
xmn
a ρ

where
ω
c = xmn
a
and
n = 1, 2, . . ..
This shows that only the frequencies ωmn = (c/a)xmn are excited.
If we assume an initial shape for the membrane, given by f(ρ, ϕ), and an
initial velocity of zero, then D = 0, and the general solution will be
ψ(ρ, ϕ, t) =
∞

m=0
∞

n=1
Jm
xmn
a ρ

cos
cxmn
a
t

(Amn cos mϕ + Bmn sin mϕ),
where
Amn =
2
πa2J2
m+1(xmn)
# 2π
0
dϕ
# a
0
dρ ρf(ρ, ϕ)Jm
xmn
a ρ

cos mϕ,
Bmn =
2
πa2J2
m+1(xmn)
# 2π
0
dϕ
# a
0
dρ ρf(ρ, ϕ)Jm
xmn
a ρ

sin mϕ,
and the orthogonality of Bessel functions (27.24) has been used. In particular,
if the initial displacement of the membrane is independent of ϕ, then only the
term with m = 0 contributes, and we get
ψ(ρ, t) =
∞

n=1
AnJ0
x0n
a ρ

cos
cx0n
a
t

,
where
An =
4
a2J2
1 (x0n)
# a
0
dρ ρf(ρ)J0
x0n
a ρ

.
Note that the wave does not develop any ϕ-dependence at later times.
28.4
Problems
28.1. Suppose λ = 0 in Equation (28.3). Show that X(x) = 0.
28.2. The two ends of a thin heat-conducting bar are held at T = 0. Initially,
the ﬁrst half of the bar is held at T = T0, and the second half is held at
T = 0. The lateral surface of the bar is then thermally insulated. Find the
temperature distribution for all time.

688
Other PDEs of Mathematical Physics
28.3. The two ends of a thin heat-conducting bar of length b are held at
T = 0. The bar is along the x-axis with one end at x = 0 and the other
at x = b. The lateral surface of the bar is thermally insulated. Find the
temperature distribution at all times if initially it is given by:
(a) T (0, x) =
0
T0 for the middle third of the bar,
0 for the other two-thirds.
(b) T (0, x) =
⎧
⎪
⎨
⎪
⎩
0
if 0 ≤x ≤b
3
or 2b
3 ≤x ≤b,
T0 sin
3π
b x −π

if b
3 ≤x ≤2b
3 .
(c) T (0, x) = T0




x
b −1
2




 −T0
2 .
(d) T (0, x) = T0 sin
π
b x

.
28.4. Derive Equation (28.7) from the equation before it by changing variables.
28.5. Using the Stirling approximation (11.6), write all four factorials of
Equation (28.14) as exponentials. Then simplify to arrive at Equation (28.15).
28.6. Derive Equations (28.20)–(28.23).
28.7. By diﬀerentiating both sides of Equation (28.23) with respect to y, and
(slightly) manipulating the resulting sum, show that H′
n(y) = 2nHn−1(y).
28.8. Evaluate Equation (28.23) at y = 0 and note that only the last term
survives. Now show that
Hn(0) =
⎧
⎨
⎩
0
if n is odd,
(−1)k(2k)!
k!
if n = 2k.
28.9. Use the substitution u(z) = zl+1e−z/2f(z) in (28.32) to derive Equation
(28.33).
28.10. Diﬀerentiate both sides of Equation (28.36) with respect to x and
show that
d
dxLj
N(x) = −Lj+1
N−1(x).
28.11. The two ends of a rope of length a are ﬁxed. The midpoint of the
rope is raised a distance a/2, measured perpendicular to the tense rope, and
released from rest. What is the subsequent wave function?
28.12. A string of length a fastened at both ends has an initial velocity of
zero and is given an initial displacement as shown in Figure 28.1. Find ψ(x, t)
in each case.

28.4 Problems
689
(c)
a/2
a
a/4
x
ψ (x, 0)
(b)
a/2
a
a/4
x
ψ (x, 0)
(a)
a/2
a
a/4
x
ψ (x, 0)
− a/4
Figure 28.1: The initial shape of the waves.
28.13. Repeat Problem 28.12 assuming that the initial displacement is zero
and the initial velocity distribution is given by each ﬁgure.
28.14. Repeat Problem 28.13 assuming that the initial velocity distribution
is given by:
(a) g(x) =
⎧
⎨
⎩
v0
if 0 ≤x ≤a
2,
0
if a
2 < x ≤a.
(b) g(x) =
⎧
⎨
⎩
v0 sin 2πx
a
if 0 ≤x ≤a
2,
0
if a
2 < x ≤a.
28.15. A wave guide consists of two coaxial cylinders of radii a and b (b > a).
Find the electric ﬁeld for a TM mode propagating along the two cylinders in
the region between them. Hint: Both linearly independent solutions of the
Bessel DE are needed for the radial function.

Part VII
Special Topics

Chapter 29
Integral Transforms
Chapters 26 and 27 illustrated the Frobenius method of solving diﬀerential
equations using power series, which gives a solution that converges within an
interval of the real line. This chapter introduces another method of solving
DEs, which uses integral transforms. The integral transform of a function
v is another function u given by
u(x) =
# b
a
K(x, t)v(t) dt,
(29.1)
where (a, b) is a convenient interval, and K(x, t), called the kernel of the
kernel of integral
transforms
integral transform, is an appropriate function of two variables.
The idea behind using integral transform is to write the solution u(x)
of a DE in x in terms of an integral such as Equation (29.1) and choose v,
the kernel, and the interval (a, b) in such a way as to render the DE more
Strategy for
solving DEs using
integral transforms
manageable. There are many kernels appropriate for speciﬁc DEs. However,
two kernels are most widely used in physics, which lead to two important
integral transforms, the Fourier transform and the Laplace transform.
29.1
The Fourier Transform
Fourier transform has a kernel of the form K(x, t) = eitx and an interval
(−∞, +∞). Let us see how this comes about.
The Fourier series representation of a function F(x) is valid for the entire
real line as long as F(x) is periodic. However, most functions encountered
in physical applications are deﬁned in some interval (a, b) without repetition
beyond that interval. It would be useful if we could also expand such functions
in some form of Fourier “series.”
One way to do this is to start with the periodic series and then let the
period go to inﬁnity while extending the domain of the deﬁnition of the func-
tion. As a speciﬁc case, suppose we are interested in representing a function
f(x) that is deﬁned only for the interval (a, b) and is assigned the value zero

694
Integral Transforms
f (x)
x
L
a
b = a+L
x
a
a+L
+
a
a–L
2L
(a)
(b)
Figure 29.1: (a) The function we want to represent. (b) The Fourier series represen-
tation of the function.
everywhere else [see Figure 29.1(a)]. To begin with, we might try the Fourier
series representation, but this will produce a repetition of our function. This
situation is depicted in Figure 29.1(b).
Next we may try a function fΛ(x) deﬁned in the interval (a−Λ/2, b+Λ/2),
where Λ is an arbitrary positive number:
fΛ(x) =
⎧
⎪
⎨
⎪
⎩
0
if
a −Λ/2 < x < a,
f(x)
if
a < x < b,
0
if
b < x < b + Λ/2.
This function, which is depicted in Figure 29.2, has the Fourier series repre-
sentation [see Equation (18.23)]
fΛ(x) =
1
√
L + Λ
∞

n=−∞
fΛ,ne2iπnx/(L+Λ),
(29.2)
where
fΛ,n =
1
√
L + Λ
# b+Λ/2
a−Λ/2
e−2iπnx/(L+Λ)fΛ(x) dx.
(29.3)
We have managed to separate various copies of the original periodic func-
tion by Λ. It should be clear that if Λ →∞, we can completely isolate the
function and stop the repetition. Let us investigate the behavior of Equations
(29.2) and (29.3) as Λ grows without bound. First, we notice that the quan-
tity kn deﬁned by kn ≡2nπ/(L + Λ) and appearing in the exponent becomes
almost continuous. In other words, as n changes by one unit, kn changes only
slightly. This suggests that the terms in the sum in Equation (29.2) can be
lumped together in j intervals of width Δnj, giving
fΛ(x) ≈
∞

j=−∞
fΛ(kj)
√
L + ΛeikjxΔnj,

29.1 The Fourier Transform
695
x
a–Λ/2
b+Λ/2
Figure 29.2: By introducing the parameter Λ, we have managed to separate the copies
of the function.
where kj ≡2njπ/(L + Λ), and fΛ(kj) ≡fΛ,nj. Substituting Δnj = [(L +
Λ)/2π]Δkj in the above sum, we obtain
fΛ(x) ≈
∞

j=−∞
fΛ(kj)
√L + Λeikjx L + Λ
2π
Δkj =
1
√
2π
∞

j=−∞
˜fΛ(kj)eikjxΔkj,
where we introduced ˜fΛ(kj) deﬁned by ˜fΛ(kj) ≡
	
(L + Λ)/2π fΛ(kj). It is
now clear that the preceding sum approaches an integral in the limit that
Λ →∞. In the same limit, fΛ(x) →f(x), and we have
f(x) =
1
√
2π
# ∞
−∞
˜f(k)eikxdk,
(29.4)
where
Fourier and
inverse Fourier
transforms
˜f(k) ≡lim
Λ→∞
˜fΛ(kj) = lim
Λ→∞

L + Λ
2π
fΛ(kj)
= lim
Λ→∞

L + Λ
2π
1
√
L + Λ
# b+Λ/2
a−Λ/2
e−ikjxfΛ(x) dx,
or
˜f(k) =
1
√
2π
# ∞
−∞
f(x)e−ikxdx.
(29.5)
The function f in (29.4) is called the Fourier transform of ˜f and ˜f in (29.5)
is called the inverse Fourier transform of f.
Note that the diﬀerence
between the two transforms is the sign of the exponential in the integrand.
Another notation that is commonly used for Fourier transform of a func-
tion f is F[f]. The inverse Fourier transform of a function g is then denoted
by F−1[g]. This means that F[f] is a function whose value at x is given by
F[f](x) =
1
√
2π
# ∞
−∞
f(k)eikxdk,
(29.6)
Similarly, F−1[g] is a function whose value at k is given by
F−1[g](k) =
1
√
2π
# ∞
−∞
g(x)e−ikxdx,
(29.7)

696
Integral Transforms
Note that the use of k and x in these two equations is completely arbitrary.
The only requirement is that the function and the variable in its argument
on the left appear, respectively, in the integrand and in the exponent on the
right. For example, (29.6) could be written as
F[f](k) =
1
√
2π
# ∞
−∞
f(x)eikxdx,
or
F[h](t) =
1
√
2π
# ∞
−∞
h(ω)eiωtdω,
and (29.7) as
F−1[g](x) =
1
√
2π
# ∞
−∞
g(k)e−ikxdk or F−1[f](y) =
1
√
2π
# ∞
−∞
f(x)e−ixydx.
29.1.1
Properties of Fourier Transform
Equations (29.4) and (29.5) are reciprocals of one another. However, it is not
obvious that they are consistent. In other words, if we substitute (29.4) in
the RHS of (29.5), do we get an identity? Let’s try this:
˜f(k) =
1
√
2π
# ∞
−∞
dx e−ikx

1
√
2π
# ∞
−∞
˜f(k′)eik′xdk′

= 1
2π
# ∞
−∞
dx
# ∞
−∞
˜f(k′)ei(k′−k)xdk′.
We now change the order of the two integrations:
˜f(k) =
# ∞
−∞
dk′ ˜f(k′)
 1
2π
# ∞
−∞
dx ei(k′−k)x

.
But the expression in the square brackets is the Dirac delta function given by
Equation (18.28). Thus, we have ˜f(k) =
 ∞
−∞dk′ ˜f(k′)δ(k′ −k), which is an
identity. In the F notation, this result can be written as
F−1F[f] = FF−1[f] = f,
(29.8)
for any function f. The second identity can be shown similarly. Another
property enjoyed by the Fourier transform and its inverse in linearity. If a
and b are constants and f and g functions, then
F[af + bg] = aF[f] + bF[g],
and
F−1[af + bg] = aF−1[f] + bF−1[g].
(29.9)
It is useful to generalize Fourier transform equations to more than one
dimension. The generalization is straightforward:
F[ ˜f](r) ≡f(r) =
1
(2π)n/2
##
Ωk∞
dnkeik·r ˜f(k),
F−1[f](k) ≡˜f(k) =
1
(2π)n/2
##
Ωx∞
dnxf(r)e−ik·r.
(29.10)
where n is usually 2 or 3, Ωk
∞is the entire k-space, and Ωx
∞is the entire
x-space.

29.1 The Fourier Transform
697
29.1.2
Sine and Cosine Transforms
The complex exponential in the deﬁnition of Fourier transform or its inverse
can be broken down into its trigonometric parts. Then for an even function,
the cosine part contributes and for an odd function, the sine part contributes.
In either case, the integration
 ∞
−∞can be equated to 2
 ∞
0 . This leads to
the sine transform and cosine transform denoted by Fs[f] and Fc[f],
respectively, for any function:
Sine and cosine
transforms
Fs[f](x) =

2
π
# ∞
0
f(k) sin kxdk,
Fc[f](x) =

2
π
# ∞
0
f(k) coskxdk.
(29.11)
What is the inverse of a cosine transform? To ﬁnd out, let F(x) denote
the left-hand side of the second equation in (29.11). Multiply both sides of
the equation by cos k′x—with k′ > 0—and integrate over all positive values
of x to get
# ∞
0
F(x) cos k′xdx =

2
π
# ∞
0
f(k)dk
# ∞
0
cos kx cos k′xdx.
(29.12)
Writing the cosines in terms of exponential, the x integration on the right
gives
# ∞
0
cos kx cos k′xdx = 1
4
# ∞
0
 
eikx + e−ikx! 
eik′x + e−ik′x
dx
= 1
4
# ∞
0
*
eix(k+k′) + e−ix(k+k′) + eix(k−k′) + e−ix(k−k′)+
dx
= 1
4
# ∞
−∞
eix(k+k′)dx + 1
4
# ∞
−∞
eix(k−k′)dx
= π
2 [δ(k + k′) + δ(k −k′)] .
To go from the second to third line, we used
 ∞
0
e−iaxdx =
 0
−∞eiaxdx, which
Inverses of sine
and cosine
transforms
the reader can easily verify; and to go from the third to the last line, we used
Equation (18.28). Substituting the last result in (29.12), we obtain
# ∞
0
F(x) cos k′xdx =
π
2
# ∞
0
f(k)δ(k + k′)dk



=0 (Reader, why?)
+
π
2
# ∞
0
f(k)δ(k −k′)dk
=
π
2 f(k′),
or
f(k′) =

2
π
# ∞
0
F(x) cos k′xdx.

698
Integral Transforms
This shows that the inverse of a cosine transform is another cosine trans-
form. Similarly, one can show that the inverse of a sine transform is another
sine transform. We shall not use sine or cosine transforms, as the Fourier
transform, with the exponential in the integrand, is much more convenient.
29.1.3
Examples of Fourier Transform
Example 29.1.1. Let us evaluate the inverse Fourier transform of the function
deﬁned by
f(x) =
0
b
if |x| < a,
0
if |x| > a
(see Figure 29.3). From (29.5) and (29.7) we have
F−1[f](k) ≡˜f(k) =
1
√
2π
# ∞
−∞
f(x)e−ikxdx =
b
√
2π
# a
−a
e−ikxdx = 2ab
√
2π
sin ka
ka

,
which is the function encountered on page 491 and depicted in Figure 18.7.
This result deserves some detailed discussion. First, note that if a →∞, the
function f(x) becomes a constant function over the entire real line, and we get
˜f(k) =
2b
√
2π
lim
a→∞
sin ka
k
=
2b
√
2π
πδ(k)
by Equation (18.27). This is the Fourier transform of an everywhere-constant func-
tion (see Problem 29.1). Next, let b →∞and a →0 in such a way that 2ab, which
is the area under f(x), is 1. Then f(x) will approach the delta function, and ˜f(k)
becomes
˜f(k) = lim
b→∞
a→0
2ab
√
2π
sin ka
ka
=
1
√
2π
lim
a→0
sin ka
ka
=
1
√
2π
.
So the Fourier transform of the delta function is the constant 1/
√
2π as implied by
(29.5).
Finally, we note that the width of f(x) is Δx = 2a, and the width of ˜f(k) is
roughly the distance, on the k-axis, between its ﬁrst two roots, k+ and k−, on either
side of k = 0: Δk = k+ −k−= 2π/a. Thus increasing the width of f(x) results
in a decrease in the width of ˜f(k). In other words, when the function is wide, its
Fourier transform is narrow. In the limit of inﬁnite width (a constant function), we
get inﬁnite sharpness (the delta function). The last two statements are very general.
In fact, it can be shown that ΔxΔk ≥1 for any function f(x). When both sides
x
a
b
–a
f (x)
Δx
Figure 29.3: The square “bump” function.

29.1 The Fourier Transform
699
of this inequality are multiplied by the (reduced) Planck constant ℏ≡h/(2π), the
result is the celebrated Heisenberg uncertainty relation:1
Heisenberg
uncertainty
relation
ΔxΔp ≥ℏ,
where p = ℏk is the momentum of the particle.
Having obtained the transform of f(x), we can write
f(x) =
1
√
2π
# ∞
−∞
2b
√
2π
sin ka
k
eikxdk = b
π
# ∞
−∞
sin ka
k
eikxdk.
Figure 29.4 shows the integral
b
π
# K
−K
sin ka
k
eikxdk
when K = 10, K = 20, and K = 100.
It is seen that by making the limits
of integration larger and larger, the graph approximates Figure 29.3 better and
better.
■
Example 29.1.2. Let us evaluate the Fourier transform of a Gaussian g(x) =
ae−bx2 with a, b > 0:
˜g(k) =
a
√
2π
# ∞
−∞
e−b(x2+ikx/b)dx = ae−k2/4b
√
2π
# ∞
−∞
e−b(x+ik/2b)2dx.
To evaluate this integral rigorously, we would have to use the calculus of residues
developed in Chapter 21.
However, we can ignore the fact that the exponent is
complex, substitute y = x + ik/(2b), and write
# ∞
−∞
e−b[x+ik/(2b)]2dx =
# ∞
−∞
e−by2dy =

π
b .
Figure 29.4: The thinnest plot represents K = 10; the next thinnest plot represents
K = 20; and the thickest plot represents K = 100.
1In the context of the uncertainty relation, the width of the function—the so-called
wave packet—measures the uncertainty in the position x of a quantum mechanical particle.
Similarly, the width of the Fourier transform measures the uncertainty in k, which is related
to momentum p via p = ℏk.

700
Integral Transforms
Thus, we have ˜g(k) =
a
√
2b
e−k2/(4b), which is also a Gaussian.
We note again that the width of g(x), which is proportional to 1/
√
b, is in inverse
relation to the width of ˜g(k), which is proportional to
√
b.
We thus have ΔxΔ
k ∼1.
■
Example 29.1.3. In this example we evaluate the inverse Fourier transform of the
Coulomb potential V (r) of a point charge q at the origin: V (r) = keq/r. The inverse
Fourier transform is important in scattering experiments with atoms, molecules, and
solids. As we shall see in the following, the inverse Fourier transform of V (r) is not
deﬁned. However, if we work with the Yukawa potential,
Yukawa potential
Vα(r) = keqe−αr
r
,
α > 0,
the inverse Fourier transform will be well-deﬁned, and we can take the limit α →0
to recover the Coulomb potential. Thus, we seek the inverse Fourier transform of
Vα(r).
We are working in three dimensions and therefore may write
F−1[Vα](k) ≡˜Vα(k) =
1
(2π)3/2
##
Ωx∞
d3xe−ik·r keqe−αr
r
.
It is clear from the presence of r that spherical coordinates are appropriate. We
are free to pick any direction as the z-axis. A simplifying choice in this case is the
direction of k. So, we let k = |k|ˆez = kˆez, or k · r = kr cos θ, where θ is the polar
angle in spherical coordinates. Now we have
˜Vα(k) =
keq
(2π)3/2
# ∞
0
r2 dr
# π
0
sin θ dθ
# 2π
0
dϕe−ikr cos θ e−αr
r
.
The ϕ integration is trivial and gives 2π. The θ integration simpliﬁes if we make
the substitution u = cos θ:
# π
0
sin θe−ikr cos θ dθ =
# 1
−1
e−ikrudu =
1
ikr (eikr −e−ikr).
We thus have
˜Vα(k) = keq(2π)
(2π)3/2
# ∞
0
dr r2 e−αr
r
1
ikr (eikr −e−ikr)
=
keq
(2π)1/2
1
ik
# ∞
0
dr
*
e(−α+ik)r −e−(α+ik)r+
=
keq
(2π)1/2
1
ik

e(−α+ik)r
−α + ik




∞
0
+ e−(α+ik)r
α + ik




∞
0

.
Note how the factor e−αr has tamed the divergent behavior of the exponential at
r →∞. This was the reason for introducing it in the ﬁrst place. Simplifying the last
expression yields ˜Vα(k) = (2keq/
√
2π)(k2 +α2)−1. The parameter α is a measure of
the range of the potential. It is clear that the larger α is, the smaller the range. In
fact, it was in response to the short range of nuclear forces that Yukawa introduced
α. For electromagnetism, where the range is inﬁnite, α becomes zero and Vα(r)
reduces to V (r). Thus, the inverse Fourier transform of the Coulomb potential is
˜VCoul(k) = 2keq
√
2π
1
k2 .

29.1 The Fourier Transform
701
If a charge distribution is involved, the inverse Fourier transform will be interestingly
diﬀerent as the following example shows.
■
Example 29.1.4. The example above deals with the electrostatic potential of a
point charge. Let us now consider the case where the charge is distributed over a
ﬁnite volume. Then the potential is
V (r) =
### keqρ(r′)
|r′ −r| d3x′ ≡keq
#
ρ(r′)
|r′ −r|d3x′,
where qρ(r′) is the charge density at r′, and we have used a single integral because
d3x′ already indicates the number of integrations to be performed. Note that we
have normalized ρ(r′) so that its integral over the volume is 1. Figure 29.5 shows
the geometry of the situation.
Making a change of variables, R ≡r′ −r, or r′ = R + r, and d3x′ = d3X, with
R ≡(X, Y, Z), we get
F−1[V ](k) ≡˜V (k) =
1
(2π)3/2
#
d3xe−ik·rkeq
# ρ(R + r)
R
d3X.
(29.13)
To evaluate Equation (29.13), we substitute for ρ(R + r) in terms of its Fourier
transform,
ρ(R + r) =
1
(2π)3/2
#
d3k′ ˜ρ(k′)eik′·(R+r).
(29.14)
Combining (29.13) and (29.14), we obtain
˜V (k) =
keq
(2π)3
#
d3x d3X d3k′ eik′·R
R
˜ρ(k′)eir·(k′−k)
= keq
#
d3X d3k′ eik′·R
R
˜ρ(k′)

1
(2π)3
#
d3x eir·(k′−k)




δ(k′−k)by Equation (18.30)
= keq˜ρ(k)
#
d3X eik·R
R
.
(29.15)
dq
r
r ′
r − r ′
P
Figure 29.5: The inverse Fourier transform of the potential of a continuous charge
distribution at P is calculated using this geometry.

702
Integral Transforms
What is nice about this result is that the contribution of the charge distribution,
˜ρ(k), has been completely factored out. The integral, aside from a constant and a
change in the sign of k, is simply the inverse Fourier transform of the Coulomb
potential of a point charge obtained in the previous example.
We can therefore
write Equation (29.15) as
˜V (k) = (2π)3/2 ˜ρ(k) ˜VCoul(−k) = 4πkeq˜ρ(k)
|k|2
.
This equation is important in analyzing the structure of atomic particles. The
inverse Fourier transform ˜V (k) is directly measurable in scattering experiments.
In a typical experiment a (charged) target is probed with a charged point particle
(electron). If the analysis of the scattering data shows a deviation from 1/k2 in the
behavior of ˜V (k), then it can be concluded that the target particle has a charge
distribution.
More speciﬁcally, a plot of k2 ˜V (k) versus k gives the variation of
˜ρ(k), the form factor, with k. If the resulting graph is a constant, then ˜ρ(k) is a
form factor
constant, and the target is a point particle [˜ρ(k) is a constant for point particles,
where ˜ρ(r′) ∝δ(r −r′)]. If there is any deviation from a constant function, ˜ρ(k)
must have a dependence on k, and correspondingly, the target particle must have a
charge distribution.
The above discussion, when generalized to four-dimensional relativistic space-
Fourier transform
and the discovery
of quarks
time, was the basis for a strong argument in favor of the existence of point-like
particles—quarks—inside a proton in 1968, when the results of the scattering of
high-energy electrons oﬀprotons at the Stanford Linear Accelerator Center revealed
deviation from a constant for the proton form factor.
■
29.1.4
Application to Diﬀerential Equations
The Fourier transform is very useful for solving diﬀerential equations. This is
because the derivative operator in r space turns into ordinary multiplication
in k space. For example, if we diﬀerentiate f(r) in Equation (29.10) with
respect to xj, we obtain
∂
∂xj
f(r) =
1
(2π)n/2
##
Ωk∞
dnk ∂
∂xj
ei(k1x1+···+kjxj+···+knxn) ˜f(k)
=
1
(2π)n/2
##
Ωk∞
dnk(ikj)eik·r ˜f(k).
(29.16)
That is, every time we diﬀerentiate with respect to any component of r, the
corresponding component of k “comes down.” Thus, the n-dimensional gra-
dient and Laplacian can be written as
∇
∇
∇f(r) = (2π)−n/2
#
dnk(ik)eik·r ˜f(k)
∇2f(r) = (2π)−n/2
#
dnk(−k2)eik·r ˜f(k).
(29.17)

29.1 The Fourier Transform
703
Let us illustrate the above points with a simple example. Consider the
ordinary second-order diﬀerential equation
C2
d2x
dt2 + C1
dx
dt + C0x = f(t),
(29.18)
where C0, C1, and C2 are constants. We can “solve” this equation by simply
substituting the following in it:
x(t) =
1
√
2π
# ∞
−∞
dω˜x(ω)eiωt,
dx
dt =
1
√
2π
# ∞
−∞
dω˜x(ω)(iω)eiωt,
d2x
dt2 = −
1
√
2π
# ∞
−∞
dω˜x(ω)ω2eiωt,
f(t) =
1
√
2π
# ∞
−∞
dω ˜f(ω)eiωt.
This gives
1
√
2π
# ∞
−∞
dω˜x(ω)(−C2ω2 + iC1ω + C0)eiωt =
1
√
2π
# ∞
−∞
dω ˜f(ω)eiωt.
Equating the coeﬃcients of eiωt on both sides, we obtain
˜x(ω) =
˜f(ω)
−C2ω2 + iC1ω + C0
.
(29.19)
If we know ˜f(ω) [which can be obtained from f(t)], we can calculate x(t)
by Fourier-transforming ˜x(ω). The resulting integrals are not generally easy
to evaluate. In some cases the methods of complex analysis may be helpful; in
others numerical integration may be the last resort. However, the real power
of the Fourier transform lies in the formal analysis of diﬀerential equations.
Example 29.1.5. A harmonically driven circuit consisting of an inductor L, a
resistor R, and a capacitor C, obeys the following diﬀerential equation:
Ld2Q
dt2 + RdQ
dt + Q
C = E cos ω0t,
where Q is the charge on the capacitor. Except for the constants, this is identical
to (29.18). The Fourier transform of cosine is a sum of two Dirac delta functions
(see Problem 29.6). Substituting in Equation (29.19), we obtain
˜Q(ω) = E

π
2
δ(ω −ω0) + δ(ω + ω0)
−Lω2 + iRω + (1/C) .
Therefore,
Q(t) =
1
√
2π
# ∞
−∞
dω ˜Q(ω)eiωt = E
2
# ∞
−∞
dω δ(ω −ω0) + δ(ω + ω0)
−Lω2 + iRω + (1/C) eiωt
= E
2

eiω0t
−Lω2
0 + iRω0 + (1/C) +
e−iω0t
−Lω2
0 −iRω0 + (1/C)

.

704
Integral Transforms
Noting that the second term in the outer parentheses is the complex conjugate of
the ﬁrst term, we obtain
Q(t) = E
2 2 Re

eiω0t
−Lω2
0 + iRω0 + (1/C)

and using Re(z1/z∗
2) = (x1x2+y1y2)/|z2|2, where x and y are the real and imaginary
parts of a complex number z, we ﬁnally obtain
Q(t) = E [(1/C) −Lω2
0] cos ω0t + Rω0 sin ω0t
[−Lω2
0 + (1/C)]2 + R2ω2
0
for the charge on the capacitor and
I(t) = dQ
dt = E −[(1/C) −Lω2
0]ω0 sin ω0t + Rω2
0 cos ω0t
[−Lω2
0 + (1/C)]2 + R2ω2
0
for the current in the circuit. Note the occurrence of a resonance (large current)
at the voltage source frequency of ω0 = 1/
√
LC. Note also that the Q(t) obtained
above is a particular, not the most general, solution of the diﬀerential equation (see
Box 24.4.1).
■
Example 29.1.6. The one-dimensional heat equation, the PDE governing the
behavior of the temperature T(x, t) along a rod, is
∂T
∂t = κ2 ∂2T
∂x2 ,
(29.20)
where we have used κ [see Equation (22.3)] to leave k exclusively for Fourier trans-
forms. Write T(x, t) as a Fourier transform in the x variable
T(x, t) =
1
√
2π
# ∞
−∞
˜T(k, t)eikxdk,
(29.21)
and substitute in (29.20) to obtain
1
√
2π
# ∞
−∞
∂˜T
∂t eikxdk =
1
√
2π
# ∞
−∞
(−κ2k2) ˜T(k, t)eikxdk
or
∂˜T
∂t = −κ2k2 ˜T(k, t)
This is a ﬁrst order ordinary diﬀerential equation which can be easily solved
˜T(k, t) = C(k)e−κ2k2t,
(29.22)
where C(k) is the constant of integration, which could depend on k. Now suppose
that initially the temperature distribution on the rod is T(x, 0) = f(x), where f(x)
is a given known function. Then the last equation gives ˜T(k, 0) = C(k), and (29.21)
yields
f(x) = T(x, 0) =
1
√
2π
# ∞
−∞
˜T(k, 0)eikxdk =
1
√
2π
# ∞
−∞
C(k)eikxdk,
showing that C(k) is the inverse Fourier transform of f(x):
C(k) =
1
√
2π
# ∞
−∞
f(x)e−ikxdx.

29.2 Fourier Transform and Green’s Functions
705
Substituting this in (29.22) and the result in (29.21) yields
T(x, t) = 1
2π
# ∞
−∞
# ∞
−∞
f(y)e−ikydy

e−κ2k2teikxdk
= 1
2π
# ∞
−∞
f(y) dy
# ∞
−∞
e−κ2k2t+ik(x−y)dk.
(29.23)
The inner integral can be done by completing the square in the exponent as in
Example 29.1.2. The result is
# ∞
−∞
e−κ2k2t+ik(x−y)dk = √π e−(x−y)2
4κ2t
κ
√
t
.
Putting this in (29.23) and noting that f(y) = T(y, 0), we ﬁnally obtain
T(x, t) =
1
√
4πκ2t
# ∞
−∞
T(y, 0)e−(x−y)2
4κ2t
dy.
(29.24)
If we know the initial shape of the temperature distribution T(y, 0) on the rod,
we can calculate the temperature at every point of the rod for any time. A simple
example is if the temperature is inﬁnitely hot at one point, say x0 of the rod and
zero every where else. Then
T(y,0) = T0δ(y −x0),
and (29.24) yields
T(x, t) =
1
√
4πκ2t
# ∞
−∞
T0δ(y −x0)e−(x−y)2
4κ2t
dy = T0e−(x−x0)2
4κ2t
√
4πκ2t
.
■
29.2
Fourier Transform and Green’s Functions
Suppose you are given a system of n linear equations in n unknowns and asked
to slove them. An elegant approach would be to use matrices. So, let L be
the matrix of coeﬃcients, y the column vector of the n unknowns, and f the
column vector of the constants appearing on the right-hand side of the system
of equations. The matrix equation and the corresponding system of equations
would look like the following:
Ly = f,
n

j=1
Lijyj = fi,
i = 1, 2, . . . , n.
(29.25)
If L has an inverse G, i.e., if there is a matrix G such that LG = 1, then
the solution to the above equation can formally be written as y = Gf, or in
component form as
yi =
n

j=1
Gijfj,
i = 1, 2, . . ., n.
(29.26)

706
Integral Transforms
Thus, the problem of solving the system of linear equations turns into the
problem of ﬁnding the inverse of the coeﬃcient matrix; and this is independent
of what f is! Once I know the inverse of L, I can solve any system of linear
equations, regardless of the constants on the right-hand side. Recalling that
the elements of the unit matrix are just the correctly labeled Kronecker delta,
the equation that G has to satisfy becomes
LG = 1,
n

j=1
LijGjk = δik,
i, k = 1, 2, . . ., n.
(29.27)
Now think of a column vector v as a “machine:” feed the machine an
From discrete
matrices to
continuous
diﬀerential
operators
integer between 1 and n, and it will give you a real number, i.e., the element
of the column vector carrying the integer as an index. Similarly, think of a
matrix M as another “machine” which gives you a real number if you feed it
a pair of integers between 1 and n. Write this as
v(i) = vi,
and
M(i, j) = Mij,
i, j = 1, 2, . . . , n.
(29.28)
Would it be beneﬁcial to generalize the action of the machine to include
all real numbers? A vector machine that feeds on real numbers is a function:
feed a function a real number and it will spit out a real number. Replacing
i with x, we have v(x) = vx ≡v(x), because vx is not a common notation.
Similarly, M(x, x′) = Mxx′ ≡M(x, x′). Furthermore, all summations have
to be replaced by integrals. For example, the system of equations (29.25)
becomes
Ly = f
# b
a
L(x, x′)y(x′) dx′ = f(x),
where (a, b) is a convenient interval of the real line usually taken to be
(−∞, ∞).
What is the meaning of L(x, x′)?
It can be merely a function
of two variables.
But more interestingly, it can be a diﬀerential operator.
However, a diﬀerential operator is a local operator, i.e., it is a linear combi-
nation of derivatives of various orders at a single point, say x. This requires
the last integral above to collapse to x. The only way that can happen is if
L(x, x′) = δ(x −x′)L(x) ≡δ(x −x′)Lx,
(29.29)
where Lx is by deﬁnition a diﬀerential operator in the variable x.
Now that we have a diﬀerential operator which is the generalization of a
matrix, how do we ﬁnd its inverse? In other words, how do we generalize
Equation (29.27)? We suspect that the Kronecker delta turns into a Dirac
delta function. With this suspicion, we generalize (29.27) to
LG = 1,
# b
a
L(x, x′)G(x′, x0) = δ(x −x0).
Substituting (29.29) in the second equation yields
Diﬀerential
equation for
Green’s function
# b
a
δ(x −x′)LxG(x′, x0) = δ(x −x0),

29.2 Fourier Transform and Green’s Functions
707
or
LxG(x, x0) = δ(x −x0).
(29.30)
A function which satisﬁes this equation is called the Green’s function for
the diﬀerential operator Lx. If we can ﬁnd the Green’s function for Lx, then
the solution to the diﬀerential equation Lxy(x) = f(x) can be written as the
generalization of (29.26):
y(x) =
# b
a
G(x, x′)f(x′) dx′.
(29.31)
To show this, note that
Lxy(x) = Lx
# b
a
G(x, x′)f(x′) dx′ =
# b
a
LxG(x, x′)f(x′) dx′
=
# b
a
δ(x −x′)f(x′) dx′ = f(x).
Green’s functions are powerful tools for solving diﬀerential equations. Or-
dinary diﬀerential equations have ordinary derivatives and the diﬀerential
operator involves a single variable. Partial diﬀerential equations correspond
to diﬀerential operators involving several variables. If x denotes the collec-
tion of all these variables, then the diﬀerential operator can be denoted by Lx
and the Green’s function by G(x, x′), which satisﬁes the partial diﬀerential
equation
LxG(x, x′) = δ(x −x′).
(29.32)
Since Fourier transform turns diﬀerentiation into multiplication, and the
Dirac delta function has a very simple inverse Fourier transform, Green’s
function are very elegantly calculated via Fourier transform techniques. For
example, if Lx is a second order partial diﬀerential operator with constant
coeﬃcients in n variables, then Fourier transforming only the x variables and
writing
G(x, x′) =
1
(2π)n/2
#
dnk ˜G(k, x′)eik·x,
δ(x −x′) =
1
(2π)n
#
dnkeik·(x−x′),
(29.33)
the diﬀerential equation (29.32) becomes
#
dnk ˜G(k, x′)Lxeik·x =
1
(2π)n/2
#
dnkeik·(x−x′) =
1
(2π)n/2
#
dnkeik·xe−ik·x′.
When Lx acts on the exponential, it produces a polynomial p(kj) of second
Green’s function
in n dimensions
degree in components kj of k. Therefore, equating the coeﬃcient of eik·x on
both sides, we obtain
˜G(k, x′)p(kj) =
1
(2π)n/2 e−ik·x′
or
˜G(k, x′) =
1
(2π)n/2
e−ik·x′
p(kj) .

708
Integral Transforms
Substituting this in the ﬁrst equation of (29.33), we get
G(x, x′) =
1
(2π)n
#
dnk eik·(x−x′)
p(kj)
,
which shows that the Green’s function is a function of the diﬀerence between
its arguments. We therefore have
G(x −x′) =
1
(2π)n
#
dnk eik·(x−x′)
p(kj)
.
(29.34)
29.2.1
Green’s Function for the Laplacian
Equation (29.17) tells us that p(kj) = −k2 for the Laplacian. Thus, with
n = 3, (29.34) becomes
G(r −r′) = −
1
(2π)3
#
d3k eik·(r−r′)
k2
.
(29.35)
To evaluate this integral, use spherical coordinates in the k-space, and choose
the polar axis to be along the vector r −r′. Then, d3k = k2 sin θdkdθdϕ and
(29.35) becomes
G(r −r′) = −
1
(2π)3
# ∞
0
k2dk
# π
0
sin θdθ
# 2π
0
dϕeik|r−r′| cos θ
k2
.
The ϕ integration gives 2π. For the θ integration, let u = cos θ. Then the
integral becomes
G(r −r′) =
1
(2π)2
# ∞
0
dk
# −1
1
dueik|r−r′|u =
1
(2π)2
# ∞
0
dk eik|r−r′|u
ik|r −r′|





−1
1
=
1
(2π)2|r −r′|
# ∞
0
dk e−ik|r−r′| −eik|r−r′|
ik
= −
2
(2π)2|r −r′|
# ∞
0
dk sin (k|r −r′|)
k
.
Example 21.3.3 calculated the last integral and yielded π/2 for it. We thus
obtain the important result that for the Laplacian, the Green’s function is
G(r −r′) = −
1
4π|r −r′|.
(29.36)
From this and ∇2G(r −r′) = δ(r −r′), we obtain another important result:
∇2

1
|r −r′|

= −4πδ(r −r′).
(29.37)

29.2 Fourier Transform and Green’s Functions
709
With the Green’s function of the Laplacian at our disposal, we can solve
the Poisson equation ∇2Φ(r) = −4πkeρ(r) in electrostatics, using the three-
dimensional version of Equation (29.31):
Green’s function
solves Poisson
equation
Φ(r) = −4πke
#
d3x′G(r −r′)ρ(r′) = ke
#
d3x′ ρ(r′)
|r −r′|,
which is the electrostatic potential of a charge distribution described by the
volume charge density ρ(r).
29.2.2
Green’s Function for the Heat Equation
The heat equation was given in (22.3), which, due to the special signiﬁcance
attached to the letter k in this chapter, we write as
∂T
∂t = κ2∇2T (r),
or
∂T
∂t −κ2∇2T (r) = 0.
(29.38)
This is a PDE in four variables. We let t be the “zeroth” coordinate, and r
the remaining three. Similarly, the 4-dimensional k space consists of k0 and
k. The polynomial p(kj) of Equation (29.34) is
p(kj) = ik0 + κ2  
k2
1 + k2
2 + k2
3
!
≡ik0 + κ2k2.
Hence, with n = 4, (29.34) gives
G(x −x′) =
1
(2π)4
#
d4k eik0(x0−x′
0)+ik·(r−r′)
ik0 + κ2k2
=
1
(2π)4
#
d3keik·(r−r′)
# ∞
−∞
dk0
eik0(x0−x′
0)
ik0 + κ2k2 .
(29.39)
Let’s do the k0 integration ﬁrst. Multiply the numerator and denominator by
−i to change the denominator to k0 −iκ2k2; then use the calculus of residues
and choose the contour in the UHP (assuming that x0 −x′
0 > 0). The only
pole of the integrand is at k0 = iκ2k2. Thus,
# ∞
−∞
dk0
eik0(x0−x′
0)
ik0 + κ2k2 = −i
# ∞
−∞
dk0
eik0(x0−x′
0)
k0 −iκ2k2 = −i

2πie−κ2k2(x0−x′
0)
.
Substituting this in (29.39) and using spherical coordinates in the 3-dimensional
k-space with the polar axis along r −r′ yields
G(x −x′) =
1
(2π)3
#
d3keik·(r−r′)e−κ2k2(x0−x′
0)
=
1
(2π)3
# ∞
0
k2e−κ2k2(x0−x′
0)dk
# π
0
sin θdθ
# 2π
0
dϕeik|r−r′| cos θ.
The ϕ integration yields 2π, and as in the Laplacian case, the θ integration
gives 2 sin(k|r −r′|)/(k|r −r′|); and since the resulting integrand of the k

710
Integral Transforms
integral is even, we can extend the lower limit of integration to −∞and
introducing a factor of half. Thus, the equation above becomes
G(x −x′) =
2
(2π)2|r −r′|
# ∞
0
ke−κ2k2(x0−x′
0) sin(k|r −r′|)dk
=
1
(2π)2|r −r′|
# ∞
−∞
ke−κ2k2(x0−x′
0) sin(k|r −r′|)dk,
or, since sine is the imaginary part of complex exponential,
G(x −x′) =
1
(2π)2|r −r′| Im
# ∞
−∞
ke−κ2k2(x0−x′
0)eik|r−r′|dk
=
1
(2π)2|r −r′| Im
# ∞
−∞
ke−κ2k2(x0−x′
0)+ik|r−r′|dk.
(29.40)
Completing the square in the exponent, we have
−κ2k2(x0−x′
0)+ik|r−r′| = −κ2(x0−x′
0)

k −
i|r −r′|
2κ2(x0 −x′
0)
2
−
|r −r′|2
4κ2(x0 −x′
0).
Call the imaginary number in the large parentheses iα and substitute the
result in (29.40) to obtain
G(x −x′) = e
−
|r−r′|2
4κ2(x0−x′
0)
(2π)2|r −r′| Im
# ∞
−∞
ke−κ2(x0−x′
0)(k−iα)2dk.
(29.41)
Change the variable of integration to u = k −iα. Then the integral becomes
# ∞
−∞
(u + iα)e−κ2(x0−x′
0)u2du = iα
# ∞
−∞
e−κ2(x0−x′
0)u2du = iα

π
κ2(x0 −x′
0).
The integral involving the u of (u+iα) vanishes because the integrand is odd.
The Gaussian integral was evaluated in Example 3.3.1. Substituting this and
the value of α in (29.41), we obtain
G(x −x′) = e
−
|r−r′|2
4κ2(x0−x′
0)
(2π)2|r −r′|
|r −r′|
2κ2(x0 −x′
0)

π
κ2(x0 −x′
0),
or, recalling that x0 = t and assuming that x′
0 = t′ = 0, yields the ﬁnal form
of the Green’s function for the heat equation:
Green’s function
for heat equation
G(r −r′; t) =
e−|r−r′|2
4κ2t
(4πκ2t)3/2 .
(29.42)

29.2 Fourier Transform and Green’s Functions
711
29.2.3
Green’s Function for the Wave Equation
The wave equation, which we write as
1
c2
∂2Ψ
∂t2 −∇2Ψ = 0,
(29.43)
with c the speed of the wave, is a PDE in 4 variables. As in the case of the
heat equation, we let the fourth variable have 0 as subscript. Then
p(kj) = −k2
0
c2 + k2
1 + k2
2 + k2
3 ≡−k2
0
c2 + k2,
and the Green’s function can be written as
G(x −x′) = −
1
(2π)4
#
d4k eik0(x0−x′
0)+ik·(r−r′)
k2
0/c2 −k2
= −
c2
(2π)4
#
d3keik·(r−r′)
# ∞
−∞
dk0
eik0t
k2
0 −c2k2 ,
(29.44)
where we substituted t for x0 and assumed x′
0 = t′ = 0.
Let us concentrate on the k0 integration and use the calculus of residues
to calculate it. The integrand has two poles k0 = ±ck on the real axis, and
depending on how these poles are handled, diﬀerent Green’s functions are
obtained.
One way to handle the poles is to move them up slightly, i.e.,
give them an inﬁnitesimal positive imaginary part. If t > 0, the contour of
integration should be in the UHP with zero contribution from the large circle
there. If t < 0, the contour of integration should be in the LHP for which
the integral vanishes because there are no poles inside the contour. Thus,
denoting the integrand by f, we have
# ∞
−∞
dk0
eik0t
k2
0 −c2k2 = 2πi [Res(f(ck)) + Res(f(−ck))] .
But
Res(f(ck)) = lim
k0→ck
(
(k0 −ck)
eik0t
k2
0 −c2k2
)
= lim
k0→ck
( eik0t
k0 + ck
)
= eickt
2ck .
Similarly, Res(f(ck)) = −e−ickt/2ck, and the k0 integral gives
# ∞
−∞
dk0
eik0t
k2
0 −c2k2 = 2πi
eickt
2ck −e−ickt
2ck

= −2π sin ckt
ck
.
Substituting this in (29.44) yields
G(x −x′) =
c
(2π)3
#
d3keik·(r−r′) sin ckt
k
,

712
Integral Transforms
which, through a by-now-familiar routine in k-space spherical integration
yields
G(r −r′; t) =
2c
(2π)2|r −r′|
# ∞
0
dk sin(k|r −r′|) sin ckt
=
c
(2π)2|r −r′|
# ∞
−∞
dk eik|r−r′| −e−ik|r−r′|
2i
eickt −e−ickt
2i
.
Multiply the exponentials and note that
 ∞
−∞e−ixdx =
 ∞
−∞eixdx to obtain
G(r −r′; t) = −
c
4(2π)2|r −r′|2
# ∞
−∞
dk
*
eick(t+|r−r′|/c) −eick(t−|r−r′|/c)+
= −
1
2(2π)2|r −r′| [2πδ(t + |r −r′|/c) −2πδ(t −|r −r′|/c)] .
(29.45)
The ﬁrst delta function vanishes because t > 0. Therefore, the ﬁnal form of
Retarded Green’s
function for wave
equation
the Green’s function for the wave equation is
Gret(r −r′; t) = δ(t −|r −r′|/c)
4π|r −r′|
.
(29.46)
The subscript “ret” on the Green’s function stands for retarded. As the
argument of the delta function implies, Gret(r −r′; t) is zero unless t = |r −
r′|/c, i.e., unless the wave has had time to move from the source point r′ to the
observation point r. The signal is “retarded” by this amount of time. Had we
given the poles of the k0 integral of (29.44) an inﬁnitesimal negative imaginary
part and chosen t to be negative, the ﬁrst delta function of (29.45) would have
survived and we would have obtained the advanced Green’s function:
Advanced Green’s
function for wave
equation
Gadv(r −r′; t) = δ(t + |r −r′|/c)
4π|r −r′|
.
(29.47)
29.3
The Laplace Transform
In the previous section, the power of the Fourier transform was illustrated by
formalism and application. Fourier transform is by far the most important of
all the transforms used in mathematical analysis. Another transform which is
widely used in solving ordinary diﬀerential equations is the Laplace transform,
the subject of this section.
Let f(t) be a suﬃciently well-behaved function. The Laplace transform
of f is another function L[f] whose value at s is given by
Laplace transform
deﬁned
L[f](s) =
# ∞
0
e−stf(t) dt.
(29.48)

29.3 The Laplace Transform
713
s could be complex, although it is usually taken to be real. To assure the
convergence of the integral, s must have a positive real part. The left-hand
side of (29.48) is usually denoted by F(s). It is also common to write it (less
precisely) as L[f(t)] with the letter s understood!
Example 29.3.1. The Laplace transform of the unit function—the function whose
value everywhere is 1—evaluated at s can easily be shown to be 1/s. The Laplace
transform of eiωt can be readily calculated as well:
F(s) =
# ∞
0
e−steiωt dt =
# ∞
0
e(−s+iω)t dt = e(−s+iω)t
−s + iω




∞
0
=
1
s −iω .
The Laplace transforms of sin ωt and cos ωt can now be evaluated:
L[cos ωt] = Re

L[eiωt]

= Re

1
s −iω

= Re
 s + iω
s2 + ω2

=
s
s2 + ω2 ,
(29.49)
and
L[sin ωt] = Im

L[eiωt]

= Im

1
s −iω

= Im
 s + iω
s2 + ω2

=
ω
s2 + ω2 .
(29.50)
The Laplace transform of the step function θ(t−a) is very useful in applications
[see Section 5.1.3 for the deﬁnition of the step function].
L[θ(t −a)] =
# ∞
0
e−stθ(t −a) dt =
# ∞
a
e−st dt = e−as
s
.
The lower limit of integration was changed because θ(t −a) is zero for t < a (and it
is equal to 1 for t > a).
Knowing L[1], we can ﬁnd the Laplace transform of any power of t because
L[tn] =
# ∞
0
tne−st dt = (−1)n dn
dsn
# ∞
0
e−st dt.
Since L[1](s) = 1/s, we have
L[tn] = (−1)n dn
dsn
1
s

=
n!
sn+1 .
(29.51)
What if n in the above equation is not an integer? Let’s evaluate L[tν] directly.
L[tν] =
# ∞
0
tνe−st dt =
# ∞
0
u
s
ν
e−u 1
s du =
1
sν+1
# ∞
0
uνe−u du = Γ(ν + 1)
sν+1
,
(29.52)
where Γ is the gamma function introduced in Section 11.1.1. Note that if ν = n, we
regain (29.51) because Γ(n + 1) = n!.
■
29.3.1
Properties of Laplace Transform
In a typical application, one obtains the Laplace transform of a function from,
say a diﬀerential equation, and inverts it to ﬁnd the actual function. This is
what was done in the case of the Fourier transform, and indeed in any other
transform used.
While the formula for inverting a Fourier transform [see

714
Integral Transforms
Equation (29.7)] is nice and symmetric, that for the Laplace transform is not
as nice. Furthermore, Fourier transform adapts itself very naturally to partial
diﬀerential equations as demonstrated in the previous section. However, the
adaptation of Laplace transform to PDEs is not so natural. That is why the
Fourier transform techniques are much more powerful—both formally and for
calculations—than the Laplace transform.
Because of this drawback, one has to rely on some formal properties of the
Laplace transform and its inverse—as well as a lot of examples—to be able to
reconstruct the original function.
Linearity
One such property is the linearity of the Laplace transform and it inverse:
L[af + bg] = aL[f] + bL[g],
L−1[af + bg] = aL−1[f] + bL−1[g].
(29.53)
First shift property
Another is the ﬁrst shift property. If F(s) is the Laplace transform of f(t),
then F(s −a) is the Laplace transform of eatf(t). This can easily be veriﬁed:
F(s−a) =
# ∞
0
e−(s−a)tf(t) dt =
# ∞
0
e−st  
eatf(t)
!
dt = L[eatf(t)]. (29.54)
A more useful way of writing this equation is
L−1[F(s −a)] = eatL−1[F(s)].
(29.55)
Second shift property
The second shift property involves the step function:
L[θ(t −a)f(t −a)] = e−asL[f](s).
(29.56)
This is because
L[θ(t−a)f(t−a)] =
# ∞
a
e−stf(t−a) dt =
# ∞
0
e−s(τ+a)f(τ) dτ = e−asL[f](s),
where in the second equality we changed the variable of integration to τ = t−a.
Denoting by F(s) the Laplace transform of f(t), we write (29.56) as
L−1[e−asF(s)] = θ(t −a)f(t −a) =
0
f(t −a)
if a > 0
0
if a < 0.
(29.57)
Example 29.3.2. Since L[tn] = n!/sn+1, using the ﬁrst shift property, we get
L[tneat] =
n!
(s −a)n+1 .
(29.58)

29.3 The Laplace Transform
715
In particular, if n = 0, we have L[eat] = 1/(s −a). From this, and the linearity
property, we can ﬁnd the Laplace transforms of the hyperbolic sine:
L[sinh γt] = L[ 1
2
 
eγt −e−γt!
] = 1
2
 
L[eγt] −L[e−γt]
!
= 1
2

1
s −γ −
1
s + γ

=
γ
s2 −γ2
(29.59)
and hyperbolic cosine:
L[cosh γt] = L[ 1
2
 
eγt + e−γt!
] = 1
2
 
L[eγt] + L[e−γt]
!
= 1
2

1
s −γ +
1
s + γ

=
s
s2 −γ2 .
(29.60)
With our accumulated knowledge of the Laplace transform, let’s see if we can ﬁnd
the inverse transform of 1/(s2 + 2as + b2). Complete the square in the denominator
and consider three cases: b > a, b < a, and b = 0. First assume b > a and deﬁne
ω2 = b2 −a2. Then
L−1

1
(s + a)2 + b2 −a2

= 1
ω L−1

ω
(s + a)2 + ω2

= e−at
ω
L−1

ω
s2 + ω2

= e−at
ω
sin ωt
where we used (29.55) and (29.50). Substituting for ω, we get
L−1

1
s2 + 2as + b2

=
e−at
√
b2 −a2 sin(
	
b2 −a2 t),
a < b.
For b < a deﬁne γ2 = a2 −b2. Then
L−1

1
(s + a)2 + b2 −a2

= 1
γ L−1

γ
(s + a)2 −γ2

= e−at
γ
L−1

γ
s2 −γ2

= e−at
γ
sinh γt
where we used (29.55) and (29.59). Substituting for γ, we get
L−1

1
s2 + 2as + b2

=
e−at
√
a2 −b2 sinh(
	
a2 −b2 t),
a > b.
If b = a, then the denominator is a complete square and
L−1

1
(s + a)2

= e−att
by (29.58).
Similarly, we can show that
L−1

s
s2 + 2as + b2

=e−at cos(
	
b2 −a2 t)
−
a
√
b2 −a2 e−at sin(
	
b2 −a2 t), b > a
L−1

s
s2 + 2as + b2

=e−at cosh(
	
a2 −b2 t)
−
a
√
a2 −b2 e−at sinh(
	
a2 −b2 t), b < a
We
shall
use
the
formulas
derived
in
this
example
in
solving
diﬀerential
equations.
■

716
Integral Transforms
Periodic functions
Although Fourier series are better suited for periodic functions, Laplace trans-
form of periodic functions is also of interest. If f(t) is periodic of period T ,
i.e., if f(t + T ) = f(t), then
L[f(t)] =
# T
0
e−stf(t) dt



call this F1(s)
+
# ∞
T
e−stf(t) dt = F1(s) +
# ∞
0
e−s(u+T )f(u + T ) du
= F1(s) + e−sT
# ∞
0
e−suf(u) du = F1(s) + e−sT
=L[f(t)]



# ∞
0
e−stf(t) dt .
We thus have L[f(t)] = F1(s) + e−sT L[f(t)], which upon solving for L[f(t)]
yields
L[f(t)] =
1
1 −e−sT F1(s).
(29.61)
Example 29.3.3. The Laplace transform of the square wave function of Example
10.6.1 deﬁned by
V (t) =
0
V0
if
0 ≤t ≤T,
0
if
T < t ≤2T,
can be readily found. We simply note that the period is 2T and F1(s) is
F1(s) =
# 2T
0
e−stV (t) dt = V0
# 2T
T
e−st dt = V0
s (e−sT −e−2sT ).
Substituting this in (29.61), we obtain
L[V (t)] =
1
1 −e−2sT
V0
s (e−sT −e−2sT )

=
V0e−sT (1 −e−sT )
s(1 −e−sT )(1 + e−sT ) =
V0e−sT
s(1 + e−sT ) =
V0
s(1 + esT ).
■
Convolution
The convolution of two functions is deﬁned as
(f ∗g)(t) =
# t
0
f(u)g(t −u)du.
Let v = t −u and change the variable of integration to v. Then
(f ∗g)(t) =
# 0
t
f(t −v)g(v)(−dv) =
# t
0
g(v)f(t −v)dv = (g ∗f)(t),
showing that convolution is commutative. Commutativity is only one of the
following properties of convolution:

29.3 The Laplace Transform
717
1. c(f ∗g) = cf ∗g = f ∗cg, c a constant;
2. f ∗g = g ∗f
(commutative property);
3. f ∗(g ∗h) = (f ∗g) ∗h
(associative property);
4. f ∗(g + h) = f ∗g + f ∗h
(distributive property).
The notion of convolution is useful for Laplace transform because one can
show the following:
Box 29.3.1. The Laplace transform of the convolution of two functions
is the product of the Laplace transforms of the two functions:
L[f ∗g](s) = L[f](s) · L[g](s)
or
(f ∗g)(t) = L−1.
L[f](s) · L[g](s)
/
Example 29.3.4. Suppose we want to ﬁnd the inverse transform of s/(s2 + a2)2.
We can write this as [see Equations (29.49) and (29.50)]
s
(s2 + a2)2 = 1
a
s
s2 + a2 ·
a
s2 + a2 = 1
aL[cos at] · L[sin at].
Therefore,
L−1

s
(s2 + a2)2

= 1
a cos at ∗sin at = 1
a
# t
0
cos au sin a(t −u)du = 1
2a t sin at.
Similarly
L−1

s2
(s2 + a2)2

= L−1.
L[cos at] · L[cos at]
/
= cos at ∗cos at
=
# t
0
cos au cos a(t −u)du = 1
2t cos at + 1
2a sin at.
■
29.3.2
Derivative and Integral of the Laplace Transform
By diﬀerentiating the integral of a Laplace transform, one can easily obtain
the formulas
dn
dsn F(s) = L[(−1)ntnf(t)]
or
L−1 *
F (n)(s)
+
= (−1)ntnL−1[F(s)],
(29.62)
where F (n) denotes the nth derivative of F. For n = 1, this formula leads to
the following useful relation:
L−1 [F(s)] = −1
t L−1 [F ′(s)] ,
(29.63)
because sometimes it is easier to ﬁnd the inverse Laplace transform of the
derivative of a function than the function itself.

718
Integral Transforms
Example 29.3.5. It is not easy to ﬁnd the inverse transform of F(s) = ln[(s +
a)/(s + b)] directly. But the inverse transform of
F ′(s) = d
ds[ln(s + a) −ln(s + b)] =
1
s + a −
1
s + b
is much easier to ﬁnd. In fact,
L−1 .
F ′(s)
/
= L−1

1
s + a

−L−1

1
s + b

= e−at −e−bt,
by (29.58) with n = 0. Therefore, according to (29.63)
L−1

ln s + a
s + b

= e−bt −e−at
t
.
■
One can also ﬁnd the primitive (antiderivative, indeﬁnite integral) of F(s).
Recall that the indeﬁnite integral of a function can be written as a deﬁnite
integral with one of its limits being a variable [see Equation (3.18)]. Therefore,
let’s write the indeﬁnite integral of F(s) as −
 ∞
s
F(u) du. This integral can
be easily evaluated:
# ∞
s
F(u) du =
# ∞
s
du
# ∞
0
e−utf(t) dt =
# ∞
0
f(t) dt
# ∞
s
e−utdu
=
# ∞
0
f(t) dt

−e−ut
t




u=∞
u=s

=
# ∞
0
e−st f(t)
t
dt.
This can be written as
# ∞
s
L[f](u) du = L
f(t)
t

.
(29.64)
Example 29.3.6. Let’s use (29.64) to ﬁnd the Laplace transform of sin ωt/t. From
(29.50), we have
L
 sin ωt
t

=
# ∞
s
ω
u2 + ω2 du = tan−1  u
ω



∞
s = π
2 −tan−1  s
ω

= tan−1  ω
s

(see Problem 29.17 for the last equality). Similarly,
L
sinh γt
t

=
# ∞
s
γ
u2 −γ2 du = 1
2 lim
x→∞
# x
s

1
u −γ −
1
u + γ

du
= 1
2 lim
x→∞

ln x −γ
x + γ −ln s −γ
s + γ

= 1
2 ln s + γ
s −γ .
■
29.3.3
Laplace Transform and Diﬀerential Equations
Certain diﬀerential equations with appropriate boundary conditions or initial
values can be nicely solved by Laplace transform techniques. For the appli-
cation of Laplace transform to diﬀerential equations, we need to know the
transform of the derivative of a function. Using integration by parts, we have
# ∞
0
e−stf ′(t) dt = e−stf(t)


∞
0 + s
# ∞
0
e−stf(t) dt = −f(0) + sL[f](s).

29.3 The Laplace Transform
719
Therefore,
L[f ′](s) = sL[f](s) −f(0).
(29.65)
This can be iterated to give
L[f ′′](s) = sL[f ′](s) −f ′(0) = s[sL[f](s) −f(0)] −f ′(0),
or
L[f ′′](s) = s2L[f](s) −sf(0) −f ′(0).
(29.66)
We can continue iterating the formula, but since most diﬀerential equations
encountered in applications are of second order, we stop at the second deriva-
tive.
To solve a diﬀerential equation, take the Laplace transform of both sides
and use (29.65) and (29.66). Solve for L[f](s) and take the inverse transform
to ﬁnd the solution. Let’s look at a speciﬁc example. Consider a mass m
attached to a spring of spring constant k. The diﬀerential equation of motion
of this system is
m¨x + kx = 0
or
¨x + ω2
0x = 0,
ω0 =

k
m.
Taking the Laplace transform of both sides gives
L[¨x](s) + ω2
0L[x](s) = 0.
Using (29.66), this becomes
s2L[x](s) −sx(0) −˙x(0) + ω2
0L[x](s) = 0,
or, letting x0 = x(0) and ˙x0 = ˙x(0), we get
(s2 + ω2
0)L[x](s) = sx0 + ˙x0
or
L[x](s) = x0s + ˙x0
s2 + ω2
0
,
and from (29.49) and (29.50) we obtain
x(t) = x0L−1

s
s2 + ω2
0

+ ˙x0
ω0
L−1

ω0
s2 + ω2
0

= x0 cos ω0t + ˙x0
ω0
sin ω0t.
Note how the initial values are automatically included in the solution.
A more general problem has a damping term as well as a driving force.
This leads to a diﬀerential equation of the form
¨x + γ ˙x + ω2
0x = f(t),
ω0 =

k
m.
(29.67)
To solve this, once again take the Laplace transform of both sides:
L[¨x](s) + γL[ ˙x](s) + ω2
0L[x](s) = L[f](s),

720
Integral Transforms
and use (29.65) and (29.66) to get
s2L[x](s) −x0s −˙x0 + γ{sL[x](s) −x0} + ω2
0L[x](s) = L[f](s),
where x0 = x(0) and ˙x0 = ˙x(0). Therefore,
(s2 + γs + ω2
0)L[x](s) = L[f](s) + x0s + ˙x0 + γx0,
which yields
L[x](s) = L[f](s) + x0s + ˙x0 + γx0
s2 + γs + ω2
0
.
The solution can be obtained by inversion once we know L[f](s). Symbolically,
we write
x(t) = L−1

L[f](s)
s2 + γs + ω2
0

+ x0L−1

s
s2 + γs + ω2
0

+ ( ˙x0 + γx0)L−1

1
s2 + γs + ω2
0

.
(29.68)
We consider only the case of a damped harmonic oscillator, i.e., that
ω0 > γ/2.
The second and third inversions are given in Example 29.3.2
with a = γ/2 and b = ω0. Then, with Ω ≡
	
ω2
0 −(γ/2)2, we have
L−1

s
s2 + γs + ω2
0

= e−γt/2 cos Ωt −γ
2Ωe−γt/2 sin Ωt,
L−1

1
s2 + γs + ω2
0

= e−γt/2
Ω
sin Ωt.
(29.69)
Substituting these in (29.68), we obtain
x(t) = e−γt/2

x0 cos Ωt + ˙x0 + x0γ/2
Ω
sin Ωt

+ L−1

L[f](s)
s2 + γs + ω2
0

.
(29.70)
Let us denote the last term of this equation by Φ(t) and evaluate the equation
at t = 0 to obtain x(0) = x0 + Φ(0) implying that Φ(0) = 0.
Similarly,
diﬀerentiating the equation and evaluating the result at t = 0 yields ˙x(0) =
˙x0 + ˙Φ(0) implying that ˙Φ(0) = 0. This is an interesting result since f(t)
is quite arbitrary! The following example looks at a speciﬁc instance of this
result.
Example 29.3.7. As an example of the general formula (29.70), let’s consider a
damped harmonic oscillator driven by a sinusoidal source f(t) = A sin ω0t operating
at the natural frequency of the oscillator as given in (29.67). Then by (29.50)
L[f](s) = L[A sin ω0t] =
Aω0
s2 + ω2
0
,
and the last term of (29.70) becomes
L−1

Aω0
(s2 + ω2
0)(s2 + γs + ω2
0)

.

29.3 The Laplace Transform
721
Using partial fraction techniques, we can write this as
Aω0
(s2 + ω2
0)(s2 + γs + ω2
0) =
A
γω0
s
s2 + γs + ω2
0
+ A
ω0
1
s2 + γs + ω2
0
−
A
γω0
s
s2 + ω2
0
.
Each term can now be inverted using the results we have obtained in several exam-
ples. Denoting the ﬁnal result by Φ(t), we get
Φ(t) ≡L−1

L[f](s)
s2 + γs + ω2
0

= −A
γω0 cos ω0t +
A
γω0 e−γt/2 
cos Ωt + γ
2Ω sin Ωt

.
Note that Φ(0) = 0 as expected from the discussion above.
Diﬀerentiating, we
obtain
˙Φ(t) = A
γ sin ω0t −
A
2ω0
e−γt/2 
cos Ωt + γ
2Ω sin Ωt

+
A
γω0
e−γt/2 
−Ω sin Ωt + γ
2 cos Ωt

.
It is readily veriﬁed that ˙Φ(0) = 0 as explained above. Substituting Φ(t) for the
last term of (29.70) yields
x(t) = e−γt/2

x0 cos Ωt + ˙x0 + x0γ/2
Ω
sin Ωt

−
A
γω0 cos ω0t +
A
γω0 e−γt/2 
cos Ωt + γ
2Ω sin Ωt

.
(29.71)
After a long time (i.e., as t →∞), the terms containing an exponential—the so-
called transient terms—will be negligible and x(t) →−
A
γω0 cos ω0t as expected from
the elementary analysis of the same problem.
■
We can understand this interesting behavior of Φ(t) in terms of the prop-
erties of convolution. Let g(t) be the inverse transform of 1/(s2 + γs + ω2
0).
Then invoking Box 29.3.1, the last term of (29.70) can be written as
Φ(t) = L−1.
L[f](s) · L[g](s)
/
= (f ∗g)(t) =
# t
0
f(u)g(t −u)du,
whose derivative is (see Box 3.2.2)
˙Φ(t) = f(t)g(0) +
# t
0
f(u)˙g(t −u)du.
It is now clear why Φ(0) = 0.
As for the derivative, we see that ˙Φ(0) =
f(0)g(0). But g(t) is given by (29.69) which is clearly 0 at t = 0.
29.3.4
Inverse of Laplace Transform
As mentioned earlier, the procedure for inverting a Laplace transform is im-
portant in solving diﬀerential equations, as the technique—like any other
transform—yields the transform of the solution, and to get the solution, one
has to invert that transform. So far, we have used various tricks and prop-
erties of the Laplace transform to get from F(s) ≡L[f](s) to f(t). Now, we

722
Integral Transforms
provide a general formula that can always be used to yield the function. The
procedure is the Mellin inversion integral:
Mellin inversion
integral
f(t) =
1
2πi
# γ+i∞
γ−i∞
F(s)estds.
(29.72)
The integration is along a line, called the Bromwich contour, parallel to
Bromwich contour
the imaginary axis of the complex s plane. The real number γ is arbitrary as
long as the integration line is to the right of all the singularities of F(s). To
ﬁnd the actual value of the integral, one closes the contour with an inﬁnite
semicircle to the left of the line and uses the residue theorem.
To prove that the right-hand side of (29.72) is indeed f(t), substitute the
deﬁnition of F(s),
F(s) =
# ∞
0
f(τ)e−sτdτ,
in the integral and switch the order of integrations to get
RHS =
1
2πi
# ∞
0
f(τ)dτ
# γ+i∞
γ−i∞
es(t−τ)ds



Denote this by J
.
(29.73)
Introduce a new variable of integration σ by s = γ + iσ in the inner integral
to get
J =
# ∞
−∞
e(γ+iσ)(t−τ)idσ = ieγ(t−τ)
# ∞
−∞
eiσ(t−τ)dσ



=2πδ(t−τ) by (18.28)
= 2πiδ(t −τ).
The last step follows because δ(t −τ) = 0 unless t = τ in which case the
exponent of the exponential is zero. Substituting this in (29.73) and noting
that τ > 0, we get RHS = f(t).
To see why the integration line must lie to the right of all singularities,
take the Laplace transform of both sides of (29.72):
L[f(t)] =
1
2πi
# ∞
0
e−st
# γ+i∞
γ−i∞
F(σ)eσtdσ

dt
=
1
2πi
# γ+i∞
γ−i∞
F(σ)dσ
# ∞
0
e(σ−s)tdt = −1
2πi
# γ+i∞
γ−i∞
F(σ)
σ −sdσ,
assuming that Re(s) > Re(σ) = γ. If F(σ) is analytic to the right of the
Bromwich contour, then closing the inﬁnite semicircle on the right, there will
be a single pole at σ = s inside the closed contour, and the residue theorem
gives the value of the integral as −2πiF(s), with the negative sign coming
from the clockwise integration. If any of the poles of F were on the right of
the Bromwich contour we would not obtain −2πiF(s) for the integration.

29.4 Problems
723
Example 29.3.8. Let us ﬁnd the inverse Laplace transform of F(s) = 1/(s2+ω2).
This is given by
f(t) =
1
2πi
# γ+i∞
γ−i∞
est
s2 + ω2 ds
where the contour of integration includes the inﬁnite semicircle to the left. The
poles of the integrand are at ±iω, so as long as γ > 0, the contour encloses both
poles. The residue theorem then yields
f(t) =
1
2πi
0
2πi
A
Res

est
(s −iω)(s + iω)




s=iω
+ Res

est
(s −iω)(s + iω)




s=−iω
B1
= eiωt
2iω + e−iωt
−2iω = 1
ω sin ωt
which is the expected result (see Example 29.3.1).
■
We can similarly ﬁnd the inverse Laplace transform of F(s) = s/(s2 +ω2):
f(t) =
1
2πi
# γ+i∞
γ−i∞
sest
s2 + ω2 ds
The contour of integration again includes the inﬁnite semicircle to the left,
and the poles of the integrand are at ±iω, as above. The residue theorem now
yields
f(t) =
1
2πi
(
2πi

Res

sest
(s −iω)(s + iω)




s=iω
+ Res

sest
(s −iω)(s + iω)




s=−iω
B1
= iωeiωt
2iω
+ −iωe−iωt
−2iω
= cos ωt
which is also treated in Example 29.3.1.
29.4
Problems
29.1. Find directly the Fourier transform of
(a) the constant function f(x) = C, and
(b) the Dirac delta function δ(x).
29.2. Show the second identity in (29.8).
29.3. Show that the inverse of a sine transform is another sine transform.
29.4. Show (29.9), the linearity property of Fourier transform and its inverse.
29.5. Suppose that ˜f(k) is the inverse Fourier transform of f(x). Show that
the inverse Fourier transform of f(x + a) is eiak ˜f(k).

724
Integral Transforms
29.6. Show that if f(t) = cos ω0t, then
˜f(ω) =
π
2 [δ(ω −ω0) + δ(ω + ω0)] .
29.7. Show that
(a) g(x) is real if and only if ˜g∗(k) = ˜g(−k),
(a) g(x) is imaginary if and only if ˜g∗(k) = −˜g(−k), and
(c) if g(x) is even (odd), then ˜g(k) is also even (odd).
29.8. Evaluate the Fourier transform of
g(x) =
0
b −b|x|/a
if |x| < a,
0
if |x| > a.
29.9. Let
f(t) =
0
sin ω0t
if |t| < T,
0
if |t| > T.
Show that
˜f(ω) =
1
√
2π
(sin[(ω −ω0)T ]
ω −ω0
−sin[(ω + ω0)T ]
ω + ω0
)
.
Verify the uncertainty relation ΔωΔt ≈4π.
29.10. If f(x) = g(x + a), show that ˜f(k) = e−iak˜g(k).
29.11. For a > 0 ﬁnd the Fourier transform of f(x) = e−a|x|. Is ˜f(k) sym-
metric? Is it real? Verify the uncertainty relations.
29.12. The displacement of a damped harmonic oscillator is given by
f(t) =
0
Ae−αteiω0t
if t > 0,
0
if t < 0.
Find ˜f(ω) and show that the frequency distribution | ˜f(ω)|2 is given by
| ˜f(ω)|2 = A2
2π
1
(ω −ω0)2 + α2 .
29.13. Prove the convolution theorem for Fourier transform:
convolution
theorem for
Fourier transform
# ∞
−∞
f(x)g(y −x) dx =
# ∞
−∞
˜f(k)˜g(k)eiky dk.
What will this give when y = 0?
29.14. Prove Parseval’s relation for Fourier transforms:
Parseval’s relation
# ∞
−∞
f(x)g∗(x) dx =
# ∞
−∞
˜f(k)˜g∗(k) dk.

29.4 Problems
725
29.15. Find the sine and cosine transform of e−ax.
29.16. Following Example 29.1.6, substitute the Fourier Transform of the
wave function Ψ(x, t) in the one-dimensional wave equation
1
c2
∂2Ψ
∂t2 = ∂2Ψ
∂x2 ,
and solve the diﬀerential equation in t to get
˜Ψ(k, t) = C(k)e±ickt.
Assuming that the initial shape of the wave Ψ(x, 0) is given by a function
f(x), show that the solution Ψ(x, t) can be written as
Ψ(x, t) = f(x ± ct).
29.17. Show the relation used in Example 29.3.6:
π
2 −tan−1  s
ω

= tan−1 ω
s

.
Hint: Let x denote the left-hand side and α = tan−1(s/ω). Take the tan of
both sides of the deﬁnition of x and use cot α = tan(π/2 −α) = 1/ tan α.
29.18. Let f(t) = sin ωt be the periodic function of (29.61) and verify that
the equation holds (for T = 2π/ω). Do the same for f(t) = cos ωt.
29.19. Find the Laplace transform of the periodic sawtooth function with
period T deﬁned by
V (t) = V0
t
T
for
0 ≤t < T.
29.20. Find the Laplace transform of 2t + 4e2t −3 cos3t.
29.21. Compute L[cosh2 γt] and L[sinh2 γt].
29.22. Compute L[cos2 ωt] and L[sin2 ωt] directly from the deﬁnition of Laplace
transform. Now show that
L[cos2 ωt] = L[1] −L[sin2 ωt].
29.23. A function N(t) is called a null function if
# t
0
N(u) du = 0
for all t > 0. Show that L[N(t)] = 0.
29.24. Compute L[e2t sin 3t], L[t2e−γt], L−1[e−2s/s3], and
L−1
a
s −
s
s2 + 1e−bs

.

726
Integral Transforms
29.25. Find L[e3t/
√
t], L[
√
t], and L−1[e−2s/√s].
29.26. (a) Show that
∂
∂ν tν−1 = tν−1 ln t.
(b) Now use (29.52) to prove that
L[tν−1 ln t] = Γ′(ν) −Γ(ν) ln s
sν
.
29.27. Using Laplace transform, solve the following initial-value problems
(a) d2x
dt2 + 4x = sin t,
x(0) = 1, ˙x(0) = 0
(b) d2x
dt2 −2dx
dt −3y = tet,
x(0) = 2, ˙x(0) = 1
(c) d2x
dt2 + dx
dt = θ(1 −t),
x(0) = 1, ˙x(0) = −1, where θ is the step
function.
(d) d2x
dt2 + x = θ(π −t) cos t,
x(0) = 0, ˙x(0) = 0, where θ is the step
function.
29.28. Using Laplace transform, solve the following boundary-value problems
(a) d2x
dt2 + ω2x = sin ωt,
x(0) = 1, x( π
2ω) = π.
(b) d2x
dt2 + ω2x = t,
x(0) = 1, ˙x( π
ω) = −1.
29.29. Find L−1[
1
2s2+2s+5] and L−1[
1
s2−a2 ] using Mellin inversion integral
(29.72).

Chapter 30
Calculus of Variations
In a typical multivariable extremum problem, you are given a function of n
variables f(x1, x2, . . . , xn) and asked to ﬁnd those n values of the variables
that maximize or minimize the function. The procedure is, of course, to set
the partial derivative of the function with respect to each variable equal to
zero and solve the resulting equations.
Geometrically, f is a function in an n-dimensional space, and the problem
is to ﬁnd the point in that space at which f has the highest (or lowest)
value compared to the neighboring points. There is another geometric way
of looking at the extremum problem. Think of (x1, x2, . . . , xn) as a piecewise
New way of
looking at the
multivariable
extremum problem
linear path in a two-dimensional coordinate system. The horizontal axis is
restricted to the values 1, 2, . . . , n, and for each of these values i the value of
the corresponding variable xi determines one point with coordinates (i, xi).
Connecting the neighboring points by a straight line segment produces the
path. Figure 30.1 shows a couple of such paths.
Figure 30.1: For each integer i between 1 and n, pick the real number xi and draw
a point with coordinates (i, xi). Connect these points to form a path. Two such paths
are shown for n = 5.

728
Calculus of Variations
The extremum problem can now be stated in terms of paths: Find the path
for which f has either the largest or the smallest value compared with its value
at the neighboring paths. And to do so, we diﬀerentiate with respect to a point
of the path. But let’s be more general in anticipation of the problems typical
of this chapter. Let xα be a variable where α is not necessarily an integer
between 1 and n. Diﬀerentiate the function with respect to xα and set the
result equal to zero:
∂f
∂xα
=
n

i=1
∂f
∂xi
∂xi
∂xα
=
n

i=1
∂f
∂xi
δαi = 0.
(30.1)
If α is not equal to one of the integers between 1 and n, the sum vanishes
identically, i.e., the left-hand side is identically zero because f is not a function
of xα. However, if α is one of the integers between 1 and n, (30.1) gives one
of the equations to be solved for determining the extremizing path.
30.1
Variational Problem
Our treatment of the extremum problem above in terms of paths was mo-
tivated by situations in which variations of smooth paths are to be consid-
ered. A typical variational problem has a function whose value depends
on the path, i.e., it takes a path and puts out a number. We say that it is a
functional, because its argument is a function rather than a set of numbers.
Functional deﬁned
If L is a functional and x(t) represents a path in the tx-plane, then the value
of the functional for this path is represented by L[x]. The most common func-
tional integrates a certain function of x(t) and ˙x(t) over some interval (a, b).
If L(x, ˙x, t) is such a function, then
L[x] =
# b
a
L
 
x(t), ˙x(t), t
!
dt.
(30.2)
For every path, the integrand becomes a function of t which can be integrated
to give a single number, and the variational problem asks for the path that
yields either the largest or the smallest such number.
Example 30.1.1. Before delving into formalism, let’s look at a very simple con-
crete example.
Take two points Pa = (a, ya) and Pb = (b, yb) in the xy-plane.
Consider points PY = ( a+b
2 , Y ) lying on the perpendicular bisector of the interval
(a, b), and the path consisting of the line segments PaPY and PY Pb as shown in
Figure 30.2. For what value of Y is the length of this path minimum?
The length L of the path is given by
L =
# b
a
	
dx2 + dy2 =
# b
a
"
1 +
 dy
dx
2
dx.
(30.3)

30.1 Variational Problem
729
Pa
b
a
y
x
PY
Pb
Figure 30.2: Here, a path consists of only two line segments. The middle point PY
is constrained to move on the vertical line on which it is located to produce diﬀerent
paths.
The equation of the path can be shown to be
y(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
2(Y −ya)
b −a
x + (a + b)ya −2aY
b −a
if a < x < (a + b)/2,
2(yb −Y )
b −a
x + 2bY −(a + b)ya
b −a
if (a + b)/2 < x < b.
Substituting this in the integral gives
L =
# (a+b)/2
a
"
1 + 4(Y −ya)2
(b −a)2
dx +
# b
(a+b)/2
"
1 + 4(yb −Y )2
(b −a)2
dx
= 1
2
*	
(b −a)2 + 4(Y −ya)2 +
	
(b −a)2 + 4(yb −Y )2
+
.
Diﬀerentiating with respect to Y and setting the result equal to zero leads to the
following equation:
Y −ya
	
(b −a)2 + 4(Y −ya)2 =
yb −Y
	
(b −a)2 + 4(yb −Y )2 .
Square both sides and simplify to get Y −ya = yb −Y , whose solution is Y =
(ya + yb)/2, placing PY on the line joining Pa and Pb. Thus among all the paths
PaPY Pb the shortest is the straight line joining Pa and Pb.
■
30.1.1
Euler-Lagrange Equation
The preceding example showed that from among paths consisting of two spe-
ciﬁc straight line segments, the one whose middle point lies on the straight
line joining the two end points gives the shortest length. What if the point
PY is not on the perpendicular bisector of (a, b), or if the path has more than
three points? There is a procedure which picks the minimizing path from
among all possible paths. Let’s discuss this procedure.

730
Calculus of Variations
Going back to Equation (30.2), we ask if there is a process whereby one
can take the derivative of L[x], set it equal to zero, and solve for the desired
path. Is there a derivative with respect to a path? To ﬁnd out, let’s see if
Functional
derivative
explained
we can generalize (30.1) from the discrete case of a path consisting of only n
points to a continuous path. The derivative with respect to a path is called
a functional derivative and δ is used instead of ∂to symbolize it. So, let’s
write
δL[x]
δx(τ) =
δ
δx(τ)
# b
a
L
 
x(t), ˙x(t), t
!
dt =
# b
a
δ
δx(τ)L
 
x(t), ˙x(t), t
!
dt.
(30.4)
In analogy with (30.1), and noting that L is to be considered as an ordinary
function (not functional) of x, ˙x, and t, we have
δ
δx(τ)L
 
x(t), ˙x(t), t
!
= ∂L
∂x
δx(t)
δx(τ) + ∂L
∂˙x
δ ˙x(t)
δx(τ),
(30.5)
because t is independent of x(τ). In the discrete case, we had
∂xi
∂xα = δαi.
What is the generalization of the Kronecker delta to the continuous case?
The Dirac delta function! This can be shown more rigorously, but the proof
is outside the scope of this book. So, let’s write the fundamental functional
derivative:
A fundamental
functional
derivative
δx(t)
δx(τ) = δ(t −τ).
(30.6)
What about the functional derivative in the second term of (30.5)? Using
the deﬁnition of the derivative and (30.6), we have
Another
fundamental
functional
derivative
δ ˙x(t)
δx(τ) =
δ
δx(τ) lim
ϵ→0
x(t + ϵ) −x(t)
ϵ
= lim
ϵ→0
1
ϵ
δx(t + ϵ)
δx(τ)
−δx(t)
δx(τ)

= lim
ϵ→0
1
ϵ

δ(t + ϵ −τ) −δ(t −τ)

= d
dtδ(t −τ).
(30.7)
Putting (30.6) and (30.7) in (30.5) and the result in (30.4), we obtain
δL[x]
δx(τ) =
# b
a
∂L
∂x δ(t −τ) + ∂L
∂˙x
d
dtδ(t −τ)

dt = ∂L
∂x (τ) −d
dτ
∂L
∂˙x (τ), (30.8)
where in the last step we used the properties of the Dirac delta function and
its derivative as given in (5.10) and (5.11). We have assumed that τ lies in
the interval (a, b).
Having found the functional derivative, we now equate it to zero and ﬁnd
the equation that determines the path—the function x(t)—which extremizes
the functional. The equation is
Euler-Lagrange
equation
∂L
∂x −d
dt
∂L
∂˙x = 0,
(30.9)
and is called the Euler-Lagrange equation. It is at the heart of all varia-
tional problems. If we know the function L, we can diﬀerentiate it, substitute

30.1 Variational Problem
731
the derivatives in (30.9) and solve the resulting diﬀerential equation.
We
should emphasize that a path could be written as y(x) or any other form,
depending on the variables used in a particular problem.
Example 30.1.2. Shortest Length
Example 30.1.1 looked at very speciﬁc
paths connecting two points and found that the straight-line path minimizes the
length. Is this true for all paths?
For any path y(x), the length between (a, ya) and (b, yb) is given by the (30.3),
where the independent variable is x and dependent variable is y. Thus, L =
	
1 + y′2
and the Euler-Lagrange equation becomes
∂L
∂y −d
dx
∂L
∂y′ = 0
or
d
dx

y′
	
1 + y′2

= 0.
(30.10)
Diﬀerentiating the expression inside the parentheses yields
y′′
(1 + y′2)3/2 = 0,
or
y′′ = 0,
or
y = cx + d,
where c and d are the constants of integration. This is the equation of a straight line.
Thus out of all the possible paths between (a, ya) and (b, yb), the straight line gives
the smallest length. Actually, we don’t know if the straight line is the shortest or the
longest distance. Euler-Lagrange equation, being the ﬁrst derivative, is necessary,
but not suﬃcient. As in calculus, to show minimality one has to look at the second
derivatives. We shall do this later.
■
30.1.2
Beltrami identity
Most variational problems have an L which is independent of t. In such a
case, the Euler-Lagrange equation simpliﬁes considerably. Consider the total
derivative of L with respect to t:
dL
dt = ∂L
∂x ˙x + ∂L
∂˙x
d ˙x
dt .
Substitute for ∂L/∂x from Euler-Lagrange equation to obtain
dL
dt = ˙x d
dt
∂L
∂˙x + ∂L
∂˙x
d ˙x
dt = d
dt

˙x∂L
∂˙x

,
or
d
dt

L −˙x∂L
∂˙x

= 0.
This gives the Beltrami identity:
L −˙x∂L
∂˙x = C.
(30.11)
Example 30.1.3. The Brachistochrone Problem
A bead slides on friction-
less bars of various shapes due to gravity.
What shape gives the shortest time?
This is the famous brachistochrone problem which started the calculus of variations.
Speciﬁcally, consider various paths connecting Pa = (xa, ya) and Pb = (xb, yb) with
yb < ya. A mass m starts from rest at Pa and moves on a frictionless path from Pa
to Pb. Find the equation of the path that yields the shortest time.

732
Calculus of Variations
For each element ds of the path, the time of travel is dt = ds/v, where v is the
speed at ds. If ds is located at height y above the ground, then conservation of
energy gives
mgya = 1
2mv2 + mgy
or
v =
	
2g(ya −y).
Therefore,
L[y] =
# Pb
Pa
ds
v =
# Pb
Pa
	
dx2 + dy2
	
2g(ya −y)
=
# xb
xa
"
1 + y′2
2g(ya −y) dx,
and L(y, y′) =
	
(1 + y′2)/[2g(ya −y)]. Since L is independent of x, we can use the
Beltrami identity:
"
1 + y′2
2g(ya −y) −y′ ∂
∂y′
"
1 + y′2
2g(ya −y) = C,
or
	
1 + y′2 −y′ ∂
∂y′
	
1 + y′2 = C
	
2g(ya −y).
Diﬀerentiating and simplifying the left-hand side gives
1
	
1 + y′2 = C
	
2g(ya −y).
Square both sides, introduce a new constant, and solve for y′ to get
dy
dx =
"
k
ya −y −1.
The substitution u = k/(ya −y) give dy = (k/u2)du and changes the diﬀerential
equation to
k
u2
du
dx =
√
u −1
or
du
u2√u −1 = 1
k dx.
Integrating both sides—and using an integral table—yields
x
k =
√u −1
u
+ tan−1  √
u −1
!
+ C.
As y →ya, u →∞and x →xa. Therefore, C = xa/k −π/2, and the solution
becomes
x −xa
k
=
√u −1
u
+ tan−1  √
u −1
!
−π
2 .
(30.12)
Let tan−1  √u −1
!
= ϕ. Then √u −1 = tan ϕ and
u = 1 + tan2 ϕ = sec2 ϕ
or
y = ya −k cos2 ϕ.
(30.13)
Substituting u in terms of ϕ in (30.12) yields
x −xa
k
= sin ϕ cos ϕ + ϕ −π
2 .
Finally deﬁning θ = 2ϕ −π, this equation and (30.13) give x and y in terms of the
parameter θ:
x −xa = k
2 (θ −sin θ) ,
y −ya = −k
2 (1 −cos θ) .
This is the parametric equation of a cycloid.
■

30.1 Variational Problem
733
Example 30.1.4. The Soap Film Problem
When a ﬁlm of soap is stretched
across a frame, the surface tension causes the area to be a minimum. The ﬁlm of
Figure 30.3 is an area of revolution with an element of area shown. This element of
area is 2πy
	
dx2 + dy2. Therefore, we have to extremize the functional
L[y] = 2π
# h
0
y
	
1 + y′2 dx,
y(0) = a, y(h) = b.
Since L(x, y, y′) = y
	
1 + y′2 is independent of x, we can use the Beltrami identity
and get
y
	
1 + y′2 −y′ ∂
∂y′

y
	
1 + y′2

= C1.
This yields
y = C1
	
1 + y′2
or
y′ =
	
(y/C1)2 −1.
Let u = y/C1 to simplify this equation to
C1u′ =
	
(u2 −1
or
C1
du
	
(u2 −1
= dx,
which can be easily integrated to give
x = C1 ln

u +
	
u2 −1

+ C2
or
u +
	
u2 −1 = e
x−C2
C1
≡ev,
where v is the exponent of the exponential. From this, we get
	
u2 −1 = ev −u
or
u2 −1 = e2v −2uev + u2
or
u = ev + e−v
2
= cosh v.
Returning to y and x, we obtain
y
C1 = cosh
 x −C2
C1

or
y = C1 cosh
 x −C2
C1

.
The constants C1 and C2 can be found by the conditions y(0) = a, y(h) = b.
■
y
x
b
a
h
Figure 30.3: The soap ﬁlm attaches itself to the two rings in such a way that the area
obtained is minimum.

734
Calculus of Variations
30.1.3
Several Dependent Variables
The path of (30.2) had only one dependent variable. One can consider paths in
an m-dimensional space where L depends on
,
xα(t)
-m
α=1 and their derivatives.
Such a generalization is straightforward: In (30.4) instead of x(τ), we have
xα(τ), which changes (30.5) to
δ
δxα(τ)L
 
x(t), ˙x(t)
!
=
m

β=1
 ∂L
∂xβ
δxβ(t)
δxα(τ) + ∂L
∂˙xβ
δ ˙xβ(t)
δxα(τ)

,
where x = (x1, x2, . . . , xm). For this we need the equivalent of (30.6) and
(30.7) which are easily shown to be
δxβ(t)
δxα(τ) = δαβδ(t −τ),
δ ˙xβ(t)
δxα(τ) = δαβ
d
dtδ(t −τ).
(30.14)
Substituting this in the above sum yields
δ
δxα(τ)L
 
x(t), ˙x(t)
!
= ∂L
∂xα
δ(t −τ) + ∂L
∂˙xα
d
dtδ(t −τ),
which replaces the x and ˙x of (30.8) with xα and ˙xα. We thus obtain the
multivariable version of the Euler-Lagrange equation:
∂L
∂xα
−d
dt
∂L
∂˙xα
= 0,
α = 1, 2, . . ., m.
(30.15)
30.1.4
Several Independent Variables
Equation (30.15) is one generalization of the Euler-Lagrange equation. It still
corresponds to a path, a (generally) curved line, albeit in a multi-dimensional
space. There is another generalization which is also important: going from a
path to a surface. In this case, our dependent variable is a function of several
independent variables.
So, consider a function φ of m variables which we
collectively denote by x, and instead of (30.2) consider the functional
L[φ] =
##
Ω
dmxL(φ; φ,1, φ,2, . . . , φ,m; x),
(30.16)
where φ,α denotes the derivative of φ with respect to xα, and Ω is some region
in the m-dimensional space. Note the change in notation: we use L instead
of L when integration is over a multidimensional “volume.” The variational
derivative (30.5) now becomes
δ
δφ(y)L
 
φ; φ,1, φ,2, . . . , φ,m; x
!
= ∂L
∂φ
δφ(x)
δφ(y) +
m

α=1
∂L
∂φ,α
δφ,α
δφ(y).
(30.17)

30.1 Variational Problem
735
Furthermore, (30.6) and (30.7) generalize to
More fundamental
functional
derivatives
δφ(x)
δφ(y) = δ(x −y),
δφ,α(x)
δφ(y) =
∂
∂xα
δ(x −y).
(30.18)
Substituting these in the functional derivative of the integral (30.16) and
setting the result equal to zero yields another Euler-Lagrange equation:
∂L
∂φ −
m

α=1
∂
∂xα
∂L
∂φ,α
= 0.
(30.19)
Finally if we have several dependent variables
,
φi-N
i=1, collectively repre-
sented by Φ, and several independent variables {xα}m
α=1, collectively repre-
sented by x, then the variational functional becomes
L[Φ] =
##
Ω
dmxL(Φ; Φ,1, Φ,2, . . . , Φ,m; x),
(30.20)
with the variational derivatives
δ
δφi(y)L
 
Φ; Φ,1, Φ,2, . . . , Φ,m; x
!
=
N

j=1
∂L
∂φj
δφj(x)
δφi(y) +
N

j=1
m

α=1
∂L
∂φj
,α
δφj
,α
δφi(y),
(30.21)
and
... and more
fundamental
functional
derivatives
δφj(x)
δφi(y) = δijδ(x −y),
δφj
,α(x)
δφi(y) = δij
∂
∂xα
δ(x −y).
(30.22)
Substitution of these in (30.20) leads to the Euler-Lagrange equations
∂L
∂φi −
m

α=1
∂
∂xα
∂L
∂φi,α
= 0,
i = 1, 2, . . . , N.
(30.23)
In many situations, the variational problem consists of various parts each
having one or several dependent or independent variables.
30.1.5
Second Variation
Euler-Lagrange equation was obtained by setting the ﬁrst variational deriva-
tive (30.8) equal to zero. As in the multivariable calculus, this only ﬁnds
the extremum. And just as in the multivariable calculus, to see if we have a
minimum or a maximum, we have to run the second derivative test.
The easiest way to apply the second derivative test in calculus is to consider
the Taylor expansion of the function. And since we are interested in local
minima and maxima, we ignore the third and higher orders in the Taylor
expansion. Now recall from Section 10.7 that the Taylor series of a function

736
Calculus of Variations
f of N independent variables
,
xi-N
i=1 ≡x up to the second order around x0
is
f(x) = f(x0)+
N

i=1
(xi−x0i)f,i(x0)+1
2
N

i,j=1
(xi−x0i)(xj−x0j)f,ij(x0), (30.24)
where
f,i ≡∂f
∂xi
and
f,ij ≡
∂2f
∂xi∂xj
.
If x0 is an extremum of f, then f,i(x0) = 0 and the above equation becomes
f(x) −f(x0) = 1
2
N

i,j=1
(xi −x0i)(xj −x0j)f,ij(x0) ≡δ2f(x0),
(30.25)
where we introduced the abbreviation δ2f(x0)—the second variation of f at
x0—for the sum. The test for maximum or minimum of f can now be stated:
If for any x that is close enough to x0, the second variation δ2f(x0) is positive,
then x0 is a minimum point, and if δ2f(x0) is negative, then x0 is a maximum
point.
The generalization to the variational problem follows from our usual pas-
sage from the discrete to the continuous. For the most general integral (30.20),
the second variation is
δ2L[Φ0] =1
2
N

i,j=1
##
Ω
dmx
##
Ω
dmy

φi(x) −φi
0(x)

φi(y) −φi
0(y)

δ2L
δφi(x)δφj(y)[Φ0],
(30.26)
where the last term means “ﬁnd the second variational derivative and evaluate
the result at the solution Φ0 of the Euler-Lagrange equation.” For a single
dependent variable and several independent variables this becomes
δ2L[φ0] = 1
2
##
Ω
dmx
##
Ω
dmy

φ(x) −φ0(x)

φ(y) −φ0(y)

δ2L
δφ(x)δφ(y)[φ0],
(30.27)
and for a single independent variable and several dependent variables we get
δ2L[x0] = 1
2
N

i,j=1
# b
a
dt
# b
a
dτ

xi(t)−xi0(t)

xj(τ)−xj0(τ)

δ2L
δxi(t)δxj(τ)[x0],
(30.28)
and for the simplest case of a single independent variable and a single depen-
dent variable (30.26) reduces to
δ2L[x0] = 1
2
# b
a
dt
# b
a
dτ

x(t)−x0(t)

x(τ)−x0(τ)

δ2L
δx(t)δx(τ) [x0]. (30.29)

30.1 Variational Problem
737
In the calculation of the second variation, we need to ﬁnd the variational
derivatives of second derivatives of dependent variables. It is not hard to show
that
Fundamental
functional
derivatives
involving second
partial derivatives
δφj
,αβ(x)
δφi(y)
= δij
∂2
∂xβ∂xα
δ(x −y).
(30.30)
Example 30.1.5. The necessary condition for the straight line to be the shortest
distance between two given points is that it satisﬁes the Euler-Lagrange equation
(30.10). Example 30.1.2 showed that y0(x) = cx+d solves the Euler-Lagrange equa-
tion. To see if this is minimum or not, calculate the second variation (30.29). The
ﬁrst derivative is given by (30.8), which with the current symbols for independent
and dependent variables, becomes
δL[y]
δy(x) = ∂L
∂y (x) −d
dx
∂L
∂y′ (x) = −d
dx

y′
	
1 + y′2

= −
y′′
(1 + y′2)3/2 ,
and
δ2L[y]
δy(x′)δy(x) = −
δ
δy(x′)
0
y′′
(1 + y′2)3/2
1
= −
δ
δy(x′)
3
y′′  
1 + y′2!−3/24
,
or
δ2L[y]
δy(x′)δy(x) = −δy′′(x)
δy(x′)
 
1 + y′2!−3/2 −y′′
δ
δy(x′)
 
1 + y′2!−3/2 .
Using (30.30), this yields
δ2L[y]
δy(x′)δy(x) = −δ′′(x −x′)
(1 + y′2)3/2 + y′′ 3
2(2y′)
 
1 + y′2!−5/2 δy′(x)
δy(x′)
= −δ′′(x −x′)
(1 + y′2)3/2 + 3y′y′′δ′(x −x′)
(1 + y′2)5/2
.
Now we have to evaluate this at the solution y0(x) of the Euler-Lagrange equation
for which y′
0 = c and y′′
0 = 0. Thus,
δ2L[y]
δy(x′)δy(x)[y0] = −δ′′(x −x′)
(1 + c2)3/2 .
Substituting this in (30.29) and using the derivative property (5.12) of the Dirac
delta function yields
δ2L[y0] = −
1
2(1 + c2)3/2
# b
a
dx
# b
a
dx′
y(x) −y0(x)

y(x′) −y0(x′)

δ′′(x −x′)
= −
1
2(1 + c2)3/2
# b
a
dx

y(x) −y0(x)
 d2
dx2

y(x) −y0(x)

.
The last integral can be integrated by parts to give

y(x) −y0(x)
 d
dx

y(x) −y0(x)




b
a



=0 because y(a) = y0(a), y(b) = y0(b)
−
# b
a
dx
( d
dx

y(x) −y0(x)
)2
.

738
Calculus of Variations
Therefore,
δ2L[y0] =
1
2(1 + c2)3/2
# b
a
dx
( d
dx

y(x) −y0(x)
)2
,
which is a manifestly positive quantity for any y(x). Hence, y0(x) = cx + d does
indeed minimize the distance between any two given points.
■
We should emphasize that although the calculation of the second varia-
tional derivative is rather straightforward, showing that the second variation
δ2L—the integral of the second variational derivative as given in Equations
(30.26) to (30.29)—is positive or negative is by no means trivial. Example
30.1.5 is one of those rare cases where the calculation of δ2L is manageable.
30.1.6
Variational Problems with Constraints
The variational problems treated so far have been problems with boundary
conditions, namely that all “paths,” or extremal candidates, must go through
the same boundary. In many applications, there are other auxiliary conditions
or constraints that the extremal candidates must obey. A typical example is
the problem of ﬁnding the closed curve of the largest area when the perimeter
is a given ﬁxed length.
The most elegant way of treating the constrained
variational problems is via Lagrange multipliers discussed in Section 12.3.1.
Suppose that we are looking for a curve that not only extremizes L[x] of
Isoperimetric
problem
(30.2), but also is such that another functional,
K[x] =
# b
a
G
 
x(t), ˙x(t), t
!
dt,
(30.31)
takes a ﬁxed value l.
Such a problem is called isoperimetric.
In exact
analogy with the multivariable calculus, we form a new function L + λG and
extremize that function. This means that we have to solve the Euler-Lagrange
equation
∂L
∂x −d
dt
∂L
∂˙x + λ
∂G
∂x −d
dt
∂G
∂˙x

= 0.
(30.32)
Example 30.1.6. As an example of the isoperimetric variational problem, con-
sider all curves of length l in the upper half plane passing through the points (−a, 0)
and (a, 0). What is the equation of the curve that together with the interval [−a, a]
encloses the largest area? The sought-after function y(x) must extremize
L[y] =
# a
−a
ydx,
subject to the condition that
y(−a) = 0 = y(a),
K[y] =
# a
−a
	
1 + y′2 dx = l.
Equation (30.32) with L = y and G =
	
1 + y′2 gives
1 + λ d
dx
y′
	
1 + y′2 = 0.

30.1 Variational Problem
739
Integrating this yields
x + λ
y′
	
1 + y′2 = C1,
which can be solved for y′ to give
y′ = ±
C1 −x
	
λ2 −(C1 −x)2 ,
whose solution is
y = ±
	
λ2 −(C1 −x)2 + C2,
or
(x −C1)2 + (y −C2)2 = λ2.
This is a circle of radius λ. The values of the three unknowns C1, C2, and λ are
determined from the conditions
y(−a) = 0 = y(a),
K[y] = l.
■
There is another type of variational problem with constraint applicable
Finite constraint
problem
to the case of one independent and several dependent variables, in which the
constraint is given by an equation of the form
g
 
x(t), ˙x(t), t
!
= 0
This is called the ﬁnite constraint problem and is similar to Equation
(12.31) where the discrete index j has been replaced with the continuous index
t. Thus, the Lagrange multipliers λj should be replaced with λ(t) and the
sum in (12.32) replaced with an integral over t, which is already present in the
extremal problem. Therefore, the problem changes to ﬁnding the extremum
of
# b
a
,
L
 
x(t), ˙x(t), t
!
+ λ(t)g
 
x(t), ˙x(t), t
!-
dt,
(30.33)
and the Euler-Lagrange equation becomes
∂L
∂xi
−d
dt
∂L
∂˙xi
+ λ
 ∂g
∂xi
−d
dt
∂g
∂˙xi

−dλ
dt
∂g
∂˙xi
= 0,
i = 1, 2, . . ., N. (30.34)
If there are multiple constraint equations,
gα
 
x(t), ˙x(t), t
!
= 0,
α = 1, 2, . . . , m,
then there will be m Lagrange multipliers and a sum over α in (30.33),
# b
a
0
L
 
x(t), ˙x(t), t
!
+
m

α=1
λα(t)gα
 
x(t), ˙x(t), t
!
1
dt,
(30.35)
leading to the following Euler-Lagrange equation:
∂L
∂xi
−d
dt
∂L
∂˙xi
+
m

α=1
(
λα
∂gα
∂xi
−d
dt
∂gα
∂˙xi

−dλα
dt
∂g
∂˙xi
)
= 0,
i = 1, 2, . . ., N.
(30.36)

740
Calculus of Variations
Example 30.1.7. Among all curves lying on the sphere centered at the origin
and of radius a and passing through two points (x1, y1, z1) and (x2, y2, z2), ﬁnd the
shortest one. This is a ﬁnite constraint problem with
L[y, z] =
# x2
x1
	
1 + y′2 + z′2 dx
and
g(x,y, z) = x2 + y2 + z2 −a2.
The solution is the set of functions {y(x), z(x)} which extremize the integral
# x2
x1
3	
1 + y′2 + z′2 + λ(x)(x2 + y2 + z2 −a2)
4
dx,
i.e., functions that satisfy the Euler-Lagrange equations
2yλ(x) −d
dx
y′
	
1 + y′2 + z′2 = 0,
2zλ(x) −d
dx
z′
	
1 + y′2 + z′2 = 0.
Solving these equations, we get the solutions in terms of four constants which can
be determined from the boundary conditions
y(x1) = y1,
y(x2) = y2,
z(x1) = z1,
z(x2) = z2.
■
30.2
Lagrangian Dynamics
Variational calculus has become an indispensable tool in physics. Almost all
(partial) diﬀerential equations of physics can be derived from some variational
problem. Furthermore, symmetry considerations, which are the cornerstones
of modern fundamental physics, ﬁnd their natural settings in functionals.
And a very elegant and powerful formulation of quantum mechanics done by
Richard Feynman uses the variational techniques.
30.2.1
From Newton to Lagrange
For most conservative systems one can deﬁne functionals whose extremization
leads to diﬀerential equations of motion of those systems. The second law of
motion for a particle acted on by a conservative force can be written as
−∇Φ = mdv
dt
or
−∂Φ
∂xi
= md ˙xi
dt
or
∂
∂xi
(−Φ) −d
dt(m ˙xi) = 0.
(30.37)
This looks very much like (30.15)! Let’s see if we can construct an L that leads
to the equations of mechanics. Use x, y, and z for the moment with n = 3.

30.2 Lagrangian Dynamics
741
By equating the ﬁrst term of (30.15) to the ﬁrst term of the last equation in
(30.37), we get
∂L
∂x = ∂
∂x(−Φ).
Antidiﬀerentiation yields L = −Φ(x, y, z) + f(y, z, ˙x, ˙y, ˙z), where f is the
“constant” of integration. If the partials of L with respect to y and z are to
be equal to the corresponding partials of −Φ, then f cannot depend on y and
z. So, f is a function of velocity components. If the second term of (30.37) is
to equal the second term of (30.15), then
m ˙x = ∂L
∂˙x = ∂f
∂˙x
or
f( ˙x, ˙y, ˙z) = 1
2m ˙x2 + g( ˙y, ˙z),
where g( ˙y, ˙z) is the “constant” of this new integration. Applying the same
argument to y and z, we conclude that f is just the kinetic energy. Therefore,
we arrive at the important conclusion that for a single particle with position
vector r, the extremization of
L(r, ˙r, t) = −Φ(r) + 1
2m |˙r|2 = −Φ(x, y, z) + 1
2m
 
˙x2 + ˙y2 + ˙z2!
(30.38)
gives the equation of motion of the particle. L(r, ˙r, t) is called the Lagrangian
of a single particle moving in potential Φ.
For N non-interacting particles in an external potential, the Lagrangian
is the sum of the single-particle Lagrangians:
L =
N

i=1
Li =
N

i=1

−Φi + 1
2mi | ˙ri|2
=
N

i=1
.
−Φi + 1
2mi
 
˙x2
i + ˙y2
i + ˙z2
i
!/
,
where Φi = Φ(xi, yi, zi). Note that this can be written as
L = KE −Φ, where KE =
N

i=1
1
2mi
 
˙x2
i + ˙y2
i + ˙z2
i
!
, and Φ =
N

i=1
Φi.
(30.39)
If the particles are interacting, then Φ is no longer the sum of individual
potentials, but a general function of all coordinates. It is therefore common to
collect all the N triple coordinates into one big 3N-component vector q and
call it the generalized coordinates vector. Then the Lagrangian is written
as
L (q, ˙q, t) = KE −Φ =
3N

i=1
1
2μi ˙q2
i −Φ(q1, q2, . . . , q3N).
(30.40)
We changed the mass to μi to avoid confusion with the mi of the previous
equation. For example, for three particles interacting gravitationally,
Φ(q1, q2, . . . , q9) = Φ(r1, r2, r3) = −Gm1m2
|r1 −r2| −Gm1m3
|r1 −r3| −Gm2m3
|r2 −r3|,
which can be written in terms of the q’s, once the latter are deﬁned in terms
of the position vectors. Note that many of the μi’s in (30.40) are equal. For
instance, if q1 = x1, q2 = y1, and q3 = z1, then μ1 = μ2 = μ3 = m1, etc.

742
Calculus of Variations
r
X
θ
Figure 30.4: The inclined plane moves as m moves on it.
Example 30.2.1. A block of mass m slides on a frictionless inclined plane, which
has mass M and moves on a frictionless horizontal surface as shown in Figure 30.4.
The position of the incline is denoted by X and that of the block by r, or (x, y) with
x = X + r cos θ,
y = (l −r) sin θ,
where l is the length of the inclined plane. The kinetic energy of the system is
KE = 1
2M ˙X2 + 1
2m
 
˙x2 + ˙y2!
= 1
2M ˙X2 + 1
2m

˙X + ˙r cos θ
2
+ ˙r2 sin2 θ

= 1
2M ˙X2 + 1
2m

˙X2 + ˙r2 + 2 ˙X ˙r cos θ

,
and the potential energy
Φ = mgy = mg(l −r) sin θ,
giving rise to the Lagrangian
L = 1
2M ˙X2 + 1
2m

˙X2 + ˙r2 + 2 ˙X ˙r cos θ

−mg(l −r) sin θ.
The equations of motion
∂L
∂X −d
dt
 ∂L
∂˙X

= 0,
∂L
∂r −d
dt
 ∂L
∂˙r

= 0,
can now be calculated:
−M ¨
X −m

¨
X + ¨r cos θ

= 0,
mg sin θ −m

¨r + ¨
X cos θ

= 0.
Solving for the two accelerations, we get
¨X = −mg sin θ cos θ
M + m sin2 θ ,
¨r = (m + M)g sin θ
M + m sin2 θ .
Note that for an inﬁnitely heavy inclined plane,
¨
X = 0 and ¨r = g sin θ, as
expected.
■
Historical Notes
was born Giuseppe Luigi Lagrangia but adopted the French version of his name. He
was the eldest of eleven children, most of whom did not reach adulthood. His father
destined him for the law—a profession that one of his brothers later pursued—
and Lagrange oﬀered no objections. But having begun the study of physics and
geometry, he quickly became aware of his talents and henceforth devoted himself to

30.2 Lagrangian Dynamics
743
the exact sciences. Attracted ﬁrst by geometry, at the age of seventeen he turned
to analysis, then a rapidly developing ﬁeld.
In 1755, in a letter to the geometer Giulio da Fagnano, Lagrange speaks of
one of Euler’s papers published at Lausanne and Geneva in 1744. The same letter
shows that as early as the end of 1754 Lagrange had found interesting results in
this area, which was to become the calculus of variations (a term coined by Euler
Joseph Louis
Lagrange
1736–1813
in 1766). In the same year, Lagrange sent Euler a summary, written in Latin, of
the purely analytical method that he used for this type of problem. Euler replied
to Lagrange that he was very interested in the technique. Lagrange’s merit was
likewise recognized in Turin; and he was named, by a royal decree, professor at the
Royal Artillery School with an annual salary of 250 crowns—a sum never increased
in all the years he remained in his native country. Many years later, in a letter
to d´Alembert, Lagrange conﬁrmed that this method of maxima and minima was
the ﬁrst fruit of his studies—he was only nineteen when he devised it—and that he
regarded it as his best work in mathematics.
In 1756, in a letter to Euler that has been lost, Lagrange, applying the calculus of
variations to mechanics, generalized Euler’s earlier work on the trajectory described
by a material point subject to the inﬂuence of central forces to an arbitrary system
of bodies, and derived from it a procedure for solving all the problems of dynamics.
In 1757 some young Turin scientists, among them Lagrange, founded a scientiﬁc
society that was the origin of the Royal Academy of Sciences of Turin. One of the
main goals of this society was the publication of a miscellany in French and Latin,
Miscellanea Taurinensia ou M´elanges de Turin, to which Lagrange contributed fun-
damentally. These contributions included works on the calculus of variations, prob-
ability, vibrating strings, and the principle of least action.
To enter a competition for a prize, in 1763 Lagrange sent to the Paris Academy
of Sciences a memoir in which he provided a satisfactory explanation of the trans-
lational motion of the moon. In the meantime, the Marquis Caraccioli, ambassador
from the kingdom of Naples to the court of Turin, was transferred by his government
to London. He took along the young Lagrange, who until then seems never to have
left the immediate vicinity of Turin. Lagrange was warmly received in Paris, where
he had been preceded by his memoir on lunar libration. He may perhaps have been
treated too well in the Paris scientiﬁc community, where austerity was not a leading
virtue. Being of a delicate constitution, Lagrange fell ill and had to interrupt his
trip. In the spring of 1765 Lagrange returned to Turin by way of Geneva.
In the autumn of 1765 d´Alembert, who was on excellent terms with Frederick II
of Prussia, and familiar with Lagrange’s work through M´elanges de Turin, suggested
to Lagrange that he accept the vacant position in Berlin created by Euler’s departure
for St. Petersburg. It seems quite likely that Lagrange would gladly have remained
in Turin had the court of Turin been willing to improve his material and scientiﬁc
situation. On 26 April, d´Alembert transmitted to Lagrange the very precise and
advantageous propositions of the king of Prussia. Lagrange accepted the proposals
of the Prussian king and, not without diﬃculties, obtained his leave through the
intercession of Frederick II with the king of Sardinia. Eleven months after his arrival
in Berlin, Lagrange married his cousin Vittoria Conti who died in 1783 after a long
illness. With the death of Frederick II in August 1786 he also lost his strongest
support in Berlin. Advised of the situation, the princes of Italy zealously competed
in attracting him to their courts. In the meantime the French government decided
to bring Lagrange to Paris through an advantageous oﬀer. Of all the candidates,
Paris was victorious.

744
Calculus of Variations
Lagrange left Berlin on 18 May 1787 to become pensionnaire v´et´eran of the Paris
Academy of Sciences, of which he had been a foreign associate member since 1772.
Warmly welcomed in Paris, he experienced a certain lassitude and did not imme-
diately resume his research. Yet he astonished those around him by his extensive
knowledge of metaphysics, history, religion, linguistics, medicine, and botany.
In 1792 Lagrange married the daughter of his colleague at the Academy, the
astronomer Pierre Charles Le Monnier. This was a troubled period, about a year
after the ﬂight of the king and his arrest at Varennes. Nevertheless, on 3 June the
royal family signed the marriage contract “as a sign of its agreement to the union.”
Lagrange had no children from this second marriage, which, like the ﬁrst, was a
happy one.
When the academy was suppressed in 1793, many noted scientists, including
Lavoisier, Laplace, and Coulomb were purged from its membership; but Lagrange
remained as its chairman. For the next ten years, Lagrange survived the turmoil of
the aftermath of the French Revolution, but by March of 1813, he became seriously
ill. He died on the morning of 11 April 1813, and three days later his body was
carried to the Panth´eon. The funeral oration was given by Laplace in the name of
the Senate.
30.2.2
Lagrangian Densities
Particles are localized objects (indeed mathematical points), whose trajecto-
ries, determined by ordinary diﬀerential equations, describe curves in space.
A Lagrangian of the form (30.40), with one independent variable (time), is
therefore appropriate for particles.
Most of physical quantities, however, are not particles, but ﬁelds, which
are not localized. In order to apply the variational techniques to ﬁelds, one has
to consider a Lagrangian density L, whose integral over some volume gives
the Lagrangian, which can now be integrated over time as in (30.2). Thus,
in ﬁeld theories, the integration is over the 4-dimensional spacetime, a nat-
ural setting for relativity—which is very relevant because most ﬁeld theories
are relativistic—to operate. A physical ﬁeld usually has several components,
making Equation (30.23) relevant to the situation.
Electrodynamics Lagrangian
Section 17.3.2 derived the electromagnetic ﬁeld tensor Fαβ and wrote the four
Maxwell’s equations in terms of it. Since Fαβ seems to be so fundamental,
and the variational techniques seem to yield the (partial) diﬀerential equations
of physics, there may be a chance that electrodynamics can be described by
a Lagrangian density. In the language of tensors, a Lagrangian density is a
scalar. Thus, we have to construct a scalar out of Fαβ. The simplest such
scalar is F αβFαβ. Equation (17.47) showed that the ﬁeld tensor can be written
as derivatives of the 4-potential Aα, which is therefore more “fundamental”
than Fαβ. There is another 4-vector appearing in Maxwell’s equations, namely
the 4-current Jα. Thus, by taking the dot product JαAα, we form another

30.2 Lagrangian Dynamics
745
scalar. We therefore write
L = aF αβFαβ + bJαAα = aηαμηβνFμνFαβ + bJαAα,
where a and b are to be determined later. Writing the ﬁeld tensor in terms of
the 4-potential, we get
L = aηαμηβν(∂μAν −∂νAμ)(∂αAβ −∂βAα) + bJαAα
≡aηαμηβν(Aν,μ −Aμ,ν)(Aβ,α −Aα,β) + bJαAα.
(30.41)
The Euler-Lagrange equation for Aα can be written as
∂L
∂Aσ
−
∂
∂xρ
∂L
∂Aσ,ρ
= 0.
(30.42)
The ﬁrst term is easy to calculate:
∂L
∂Aσ
= bJα ∂Aα
∂Aσ
= bJαδσ
α = bJσ.
The second term is only slightly more complicated once we realize that
∂Aβ,α
∂Aσ,ρ
= δσ
βδρ
α.
With this in mind, the second term of (30.42) can be shown to be
∂
∂xρ
∂L
∂Aσ,ρ
= 4a∂ρ (∂ρAσ −∂σAρ) = 4a∂ρF ρσ,
and (30.42) becomes
4a∂ρF ρσ = bJσ
or
4a∂ρFρσ = bJσ.
This becomes Maxwell’s ﬁrst and fourth equations combined [see Equation
(17.45)] if a = 1
4 and b = μ0. Thus the Lagrangian density for electrodynamics
is
L = 1
4ηαμηβν(Aν,μ −Aμ,ν)(Aβ,α −Aα,β) + μ0JαAα.
(30.43)
This, like any other Lagrangian, can be multiplied by a constant without
aﬀecting the Euler-Lagrange equations.
Example 30.2.2. Charged Particle in EM Field
Problem 30.18 shows that
the Lagrangian density (30.43) can be written as
L = 1
2
 
|B|2 −|E|2!
+ μ0 (ρΦ −J · A) ,
with the variational problem
L =
# b
a
##
Ω
L d3x′

dt.

746
Calculus of Variations
Now consider a single particle of charge q interacting with an electromagnetic ﬁeld.
For such a particle,
ρ = qδ(r −r′)
and
J = ρv = qvδ(r −r′),
and L becomes
L = 1
2
# b
a
dt
##
Ω
 
|B|2 −|E|2!
d3x′ + μ0
# b
a
dt
##
Ω
 
qΦδ(r −r′) −qv · Aδ(r −r′)
!
d3x′
= 1
2
# b
a
dt
##
Ω
 
|B|2 −|E|2!
d3x′ + μ0q
# b
a
dt {Φ(r, t) −v · A(r, t)} .
The particle also has kinetic energy, which needs to be added to this Lagrangian.
When adding Lagrangians, one has to incorporate the freedom in multiplying La-
grangians by constants. In the case at hand, the kinetic energy of the particle should
be added to the negative of the scalar potential energy (recall that L = KE −Φ).
To assure this, we have to divide the entire EM Lagrangian by −1/μ0 and add it to
the kinetic energy of the particle. Hence the total Lagrangian becomes
L = −1
2μ0
# b
a
dt
##
Ω
 
|B|2 −|E|2!
d3x′ +
# b
a
dt
, 1
2m|v|2 −qΦ(r, t) + qv · A(r, t)
-
.
Notice how the ﬁrst integral is four-dimensional while the second integral is over a
single variable.
We are interested in the motion of the particle.
Therefore, the ﬁrst integral
is just a constant (independent of the coordinates and velocity components of the
particle) and can be dropped. Thus, substituting ˙r for v, we have
Lpart =
# b
a
dt
, 1
2m|˙r|2 −qΦ(r, t) + q˙r · A(r, t)
-
,
with the Lagrangian
Lagrangian of a
charged particle in
EM ﬁeld
L(r, ˙r, t) = 1
2m|˙r|2 −qΦ(r, t) + q˙r · A(r, t).
(30.44)
Let’s look at the x-component of the motion:
∂L
∂x −d
dt
∂L
∂˙x = 0
or
−q ∂Φ
∂x + q˙r · ∂A
∂x −d
dt (m ˙x + qAx) = 0,
or
m¨x + q ∂Φ
∂x + q
dAx
dt
−˙r · ∂A
∂x

= 0.
(30.45)
Now note that
dAx
dt
= ∂Ax
∂t
+ ∂Ax
∂x ˙x + ∂Ax
∂y ˙y + ∂Ax
∂z ˙z,
and
˙r · ∂A
∂x = ˙x∂Ax
∂x + ˙y ∂Ay
∂x + ˙z ∂Az
∂x .
Putting these two equations in (30.45) and rearranging, we obtain
m¨x + q
 ∂Φ
∂x + ∂Ax
∂t




=−Ex by (15.31)
+q ˙y
 ∂Ax
∂y −∂Ay
∂x




=−Bz by (15.31)
+q ˙z
∂Ax
∂z −∂Az
∂x




=By by (15.31)
= 0,

30.3 Hamiltonian Dynamics
747
or
m¨x −qEx −q ( ˙yBz −˙zBy) = 0.
(30.46)
The expression in parentheses is just the x-component of v×B. Thus, (30.46) is the
x-component of the Lorentz force law, governing the motion of a charged particle in
an electromagnetic ﬁeld.
■
Klein-Gordon Lagrangian
One of the ﬁrst attempts at combining the special theory of relativity with
quantum mechanics was made by Oskar Klein and Walter Gordon. In fact,
Schr¨dinger himself started with the relativistic version of his equation, but
abandoned it because of some diﬃculty he encountered when applying it to
hydrogen atom. By the usual substitution
E →iℏ∂
∂t,
p →−iℏ∇
in the relativistic equation E2/c2 −p · p = m2c2, Klein and Gordon derived
the equation that now bears their names:
1
c2
∂2φ
∂t2 −∇2φ + m2c2
ℏ2 φ = 0,
which, in units ℏ= 1 = c, becomes
∂2φ
∂t2 −∇2φ + m2φ = 0.
This equation can also be obtained from the Lagrangian density
L = ηαβ (∂αφ) (∂βφ) −m2
2 φ2,
(30.47)
as the reader can verify.
30.3
Hamiltonian Dynamics
The Lagrangian formulation of mechanics treated in the previous section is a
powerful tool for studying many diﬀerent dynamical systems and ﬁelds. Fur-
thermore, considerations of symmetry, an indispensable technique in the inves-
tigation of fundamental forces, is most adequately handled in the Lagrangian
language. Once the Lagrangian is known, the Euler-Lagrange equations pro-
vide second-order diﬀerential equations to be solved under given boundary (or
initial) conditions.
There is another formulation of mechanics, which instead of second-order
diﬀerential equations, yields twice as many ﬁrst-order DEs. It is called the
Hamiltonian formulation. We describe only the case of several dependent
and one independent variables, the other cases being very similar. Let us

748
Calculus of Variations
assume that our dynamical system has n generalized coordinates {qi}n
i=1 and
a Lagrangian L (q, ˙q, t). In the simplest case (30.40), L = KE −Φ where KE
is a quadratic term in velocities alone and Φ dependent on the coordinates
alone. In such a case,
∂L
∂˙qj
= ∂KE
∂˙qj
= μj ˙qj,
which is the momentum associated with the jth generalized coordinate. It is
therefore natural to generalize the concept of momentum as well, write
pj ≡∂L (q, ˙q, t)
∂˙qj
,
(30.48)
and call pj so deﬁned the generalized momentum of the dynamical system.
The transition from Lagrangian to Hamiltonian formulation, from a strictly
mathematical standpoint, is to go from the set of variables (q, ˙q, t) to (q, p, t).
The procedure for making this transition is the Legendre transformation dis-
cussed in Section 2.2.2. To ﬁnd the variables involved, consider the diﬀerential
of the Lagrangian:
dL =
n

i=1
 ∂L
∂qi
dqi + ∂L
∂˙qi
d ˙qi

+ ∂L
∂t dt,
and use (30.48) and the Euler-Laggrange equation to rewrite the above as
dL =
n

i=1
( ˙pi dqi + pi d ˙qi) + ∂L
∂t dt.
If we want to switch the independent variable from ˙qi to pi, then we have to
deﬁne the Hamiltonian as
H (q, p, t) =
n

i=1
pi ˙qi −L (q, ˙q, t) .
(30.49)
To verify this, we note that
dH =
n

i=1
( ˙qidpi + pid ˙qi)−dL=
n

i=1
( ˙qidpi + pid ˙qi)−
n

i=1
( ˙pi dqi + pi d ˙qi)−∂L
∂t dt.
Note that pid ˙qi terms cancel and we are left with
dH =
n

i=1
( ˙qidpi −˙pi dqi) −∂L
∂t dt.
On the other hand,
dH =
n

i=1
∂H
∂qi
dqi + ∂H
∂pi
dpi

+ ∂H
∂t dt.

30.3 Hamiltonian Dynamics
749
Comparison of the last two equations gives
˙qi = ∂H
∂pi
,
˙pi = −∂H
∂qi
,
−∂L
∂t = ∂H
∂t ,
i = 1, 2, . . ., n,
(30.50)
which are called Hamilton or canonical equations. Note that instead of n
second-order DEs, we now have 2n ﬁrst order DEs.
To discover the physical signiﬁcance of the Hamiltonian, consider the fa-
miliar simple Lagrangian L = KE −Φ, where KE is the usual kinetic energy
term and Φ is the potential energy which is independent of velocities. Then,
(30.48) yields pi = μi ˙qi and
Hamiltonian is the
total energy.
H =
n

i=1
pi ˙qi −L =
n

i=1
μi ˙q2
i



=2KE
−KE + Φ = KE + Φ.
So H is the sum of the kinetic and potential energies, i.e., the total energy.
Example 30.3.1. Hamiltonian of a Charged Particle in EM Field
The
Lagrangian of a charged particle in an electromagnetic ﬁeld is given by (30.44).
Let’s ﬁnd the Hamiltonian of this system. First we need the generalized momentum
(30.48):
pi = ∂L
∂˙xi = m ˙xi + qAi
or
p = m˙r + qA.
(30.51)
This is an important equation in its own right. It says that the momentum of the
system is not just that of the particle, but that it also includes a contribution from
the EM ﬁeld. In particular, that EM ﬁeld has momentum.1
To ﬁnd the Hamiltonian, compute ˙r from (30.51):
˙r = p −qA
m
,
and substitute in the deﬁnition of the Hamiltonian (30.49), where, in this case the
sum is just the dot product:
H(r, p, t) = p ·
 p −qA
m

−1
2m




p −qA
m




2
+ qΦ −q
 p −qA
m

· A
= (p −qA) ·
p −qA
m

−1
2
|p −qA|2
m
+ qΦ,
or
H(r, p, t) = |p −qA(r, t)|2
2m
+ qΦ(r, t).
(30.52)
Thus, in the presence of an electromagnetic ﬁeld, the Hamiltonian of a particle takes
the same form as the total energy of a particle in a potential qΦ, except that in the
expression for the KE part, p −qA replaces p. Such a replacement is called the
minimal coupling and plays a key role in the quantum mechanical treatment of
charged particles interacting with EM ﬁelds.
■
1This momentum is the source of radiation pressure.

750
Calculus of Variations
30.4
Problems
30.1. Show that, in Example 30.1.6, C1 = 0, λ = λ0, and C2 =
	
λ2
0 −a2,
where λ0 is the solution of the equation
λ sin
 l
2λ

= a.
30.2. Find the extremal of the functional
L[x, y] =
# π/2
0
( ˙x2 + ˙y2 + 2xy) dt
subject to the boundary conditions
x(0) = 0,
x(π/2) = 1,
y(0) = 0,
y(π/2) = 1.
30.3. Find the extremals of the following functionals:
(a) L[x, y] =
# b
a
( ˙x2 + ˙y2 + ˙x ˙y) dt,
(b) L[x, y] =
# b
a
(2xy −2x2 + ˙x2 −˙y2) dt.
30.4. Find the extremal of a functional of the form
L[x, y] =
# b
a
L( ˙x, ˙y) dt,
given that
∂2L
∂˙x2
∂2L
∂˙y2 −
 ∂2L
∂˙x∂˙y
2
̸= 0 for a ≤x ≤b.
30.5. Find the extremal of the functional
L[x] =
# 1
0
( ˙x2 + t2) dt,
subject to the boundary conditions
x(0) = 0,
x(1) = 0,
# 1
0
x2 dt = 2.
30.6. Show that the extremization of (30.33) leads to the Euler-Lagrange
equations (30.34).
30.7. Among all triangles with a given base line and a ﬁxed perimeter, show
that the isosceles triangle has the largest area.
30.8. An airplane with ﬁxed air speed v0 ﬂies for a time T on a closed curve.
The wind velocity u is constant in magnitude and direction and |u| < v0.
What closed curve encloses the largest area?

30.4 Problems
751
30.9. Among all curves joining a given point (0, b) on the y-axis to a point
on the x-axis and enclosing a given area S together with the x-axis, ﬁnd the
curve which generates the least area when rotated about the x-axis.
30.10. An Atwood machine consists of two masses m1 and m2 connected by
a light inextensible cord of length l which passes over a pulley whose radius
is a and whose moment of inertia is I. Let x denote the distance of m1 from
the top of the pulley. Using Lagrangian methos, show that the acceleration
of m1 is
¨x =
g(m1 −m2)
m1 + m2 + I/a2 .
30.11. Using polar coordinates, write the Lagrangian of a particle of mass m
moving in a central force ﬁeld with potential Φ(r). Show that the equations
of motion are
m¨r = mr ˙θ2 −dΦ
dr ,
d
dt(mr2 ˙θ) = 0.
30.12. Using Lagrangian method, ﬁnd the acceleration of a solid sphere
rolling without sliding down an inclined plane having an angle θ with the
horizontal.
30.13. Using Lagrangian method, ﬁnd the acceleration of a solid sphere
rolling without sliding down a movable wedge of mass M having an angle
θ. The wedge moves on a frictionless horzontal surface.
30.14. Two blocks of equal mass m are connected by an inextensible cord
whose linear mass density is μ. One block is placed on a smooth horizontal
table, the other hangs over the edge of the table. What is the acceleration of
the system? Use the Lagrangian method.
30.15. A simple pendulum of length l and mass m oscillates about its point of
support which is attached to a block of mass M moving without friction along
a horizontal line lying in the plane of the pendulum. Write the Lagrangian in
terms of x, the position of M on the horizontal line, and θ, the angle l makes
with the vertical. Find the equations of motion of m and M.
30.16. Find the equation of a curve describing the equilibrium position of
a uniformly dense heavy ﬂexible inextensible cord of length l fastened at its
ends. Hint: The Lagrangian is just the potential energy written as an integral.
30.17. Show that the Lagrangian density (30.43) can be written as
L = 1
2

|B|2 −|E|2
+ μ0 (ρΦ −J · A) .
Hint: See Sections 17.3.1 and 17.3.2 and be careful about possible change of
sign when raising or lowering indices.
30.18. Show that the Lagrangian density (30.47) leads to the Klein-Gordon
equation.

Chapter 31
Nonlinear Dynamics
and Chaos
A variety of techniques including the Frobenius method of inﬁnite power series
could solve almost all linear DEs of physical interest. However, some very fun-
damental questions such as the stability of the solar system led to DEs that
were not linear, and for such DEs no analytic (including series representation)
solution existed. In the 1890s, Henri Poincar´e, the great French mathemati-
cian, took upon himself the task of gleaning as much information from the
DEs describing the whole solar system as was possible. The result was the
invention of one of the most powerful branches of mathematics (topology) and
the realization that the qualitative analysis of (nonlinear) DEs could be very
useful.
One of the discoveries made by Poincar´e, which much later became the
cornerstone of many developments, was that
Box 31.0.1. Unlike the linear DEs, nonlinear DEs may be very sensitive
to the initial conditions.
In other words, if a nonlinear system starts from some initial conditions and
develops into a certain ﬁnal conﬁguration, then starting it with slightly dif-
ferent initial conditions may cause the system to develop into a ﬁnal conﬁg-
uration completely diﬀerent from the ﬁrst one. This is in complete contrast
to the linear DEs where two nearby initial conditions lead to nearby ﬁnal
conﬁgurations.
In general, the initial conditions are not known with inﬁnite accuracy.
Therefore, the ﬁnal states of a nonlinear dynamical system may exhibit an
indeterministic behavior resulting from the initial (small) uncertainties. This
is what has come to be known as chaos. The reader should note that the inde-
chaos due to
uncertainty in
initial conditions
terminism discussed here has nothing to do with the quantum indeterminism.

754
Nonlinear Dynamics and Chaos
All equations here are completely deterministic. It is the divergence of the
initially nearby—and completely deterministic—trajectories that results in
unpredictable ﬁnal states.
There are two general categories exhibiting chaotic behavior: systems
obeying iterated maps and systems obeying DEs. We shall study the ﬁrst
category in some detail, and only outline some of the general features of the
much more complicated category of systems obeying DEs.
31.1
Systems Obeying Iterated Maps
Consider the population of a species in consecutive years if the population is
initially N0. The simplest relation connecting N1, the population after one
year, to N0 is
N1 = αN0,
where α is a positive number depending on the environment in which the
species lives. Under the most favorable conditions, α is a large number, indi-
cating rapid growth of population. Under less favorable conditions, α will be
small. And if the environment happens to be hostile, then α will be smaller
than one, indicating a decline in population.
The above equation is unrealistic because we know that if α > 1 and the
population grows excessively, there will not be enough food to support the
species. So, there must be a mechanism to suppress the growth. A more
realistic equation should have a suppressive term which is small for small N0
and grows for larger values of N0. A possible term having such properties is
one proportional to N 2
0 . This leads to
N1 = αN0 −βN 2
0
where
0 < β ≪α.
The minus sign causes the second term to decrease the population. Iterating
this equation, we can ﬁnd the population in the second, third, and subsequent
years:
N2 = αN1 −βN 2
1 ,
N3 = αN2 −βN 2
2 , . . . ,
and, in general,
Nk+1 = αNk −βN 2
k.
(31.1)
It is customary to rewrite (31.1) in a slightly diﬀerent form. First we note
that since population cannot be negative, there exists a maximum number
beyond which the population cannot grow. In order for Nk+1 to be positive,
we must have
αNk −βN 2
k > 0 ⇒Nk < α
β
for all k. It follows that Nmax = α/β. Dividing (31.1) by Nmax yields
xk+1 = αxk(1 −xk),
(31.2)

31.1 Systems Obeying Iterated Maps
755
where xk is the fraction of the maximum population of the species after k
years, and therefore, its value must lie between zero and one. Any equation
of the form
xk+1 = fα(xk),
(31.3)
where α is—as in the case of the logistic map—a control parameter, and in
which a value of some (discrete) quantity at k +1 is given in terms of its value
at k, is called an iterated map, and the function fα is called the iterated
iterated map,
iterated map
function, and
logistic map
function
map function. The particular function in (31.2) is called the logistic map
function.
Starting from an initial value x0, one can generate a sequence of x values
by consecutively substituting in the RHS of (31.3). This sequence is called a
trajectory or orbit of the iterated map.
31.1.1
Stable and Unstable Fixed Points
It is clear that the ﬁrst few points of an orbit depend on the starting point.
What may not be so clear is that, for a given α, the eventual behavior of
the orbit is fairly insensitive to the starting point. There are, however, some
starting points which are manifestly diﬀerent from others. For example, in the
logistic map, if x0 = 0, no other point will be produced by iteration because
fα(0) = 0 or fα(x0) = x0, and further application of fα will not produce any
new values of x. In general, a point xα which has the property that
fα(xα) = xα
(31.4)
is called a ﬁxed point of the iterated map associated with α. For the logistic
ﬁxed point of an
iterated map
map we have
xα = αxα(1 −xα) ⇒xα(1 −α + αxα) = 0 ⇒xα = 0, 1 −1
α.
(31.5)
Since 0 ≤xα ≤1, there is only one ﬁxed point (i.e., x = 0) for α ≤1, and
two ﬁxed points (i.e., x = 0 and x = 1 −1/α) for α > 1.
What is the signiﬁcance of ﬁxed points? When α < 1, Equation (31.2)
shows—since both xk and 1 −xk are at most one—that the population keeps
decreasing until it vanishes completely. And this is independent of the initial
graphical way of
approaching a
ﬁxed point
value of x. It is instructive to show this pictorially. Figure 31.1(a) shows the
logistic map function with α = 0.5. Start at any point x0 on the horizontal
axis; draw a vertical line to intersect the logistic map function at f(x0) ≡x1;
from the intersection draw a horizontal line to intersect the line y = x at y1 =
x1; draw a vertical line to intersect the logistic map function at f(x1) ≡x2;
continue to ﬁnd x3 and the rest of x’s. The diagram shows that the x’s are
getting smaller and smaller.
What happens when α > 1? Figure 31.1(b) shows the logistic map function
with α = 2. We note that the orbit is attracted to the ﬁxed point at x =
0.5.
We also note that the ﬁxed point at x = 0 has now turned into a

756
Nonlinear Dynamics and Chaos
α = 2
y = f (x)
1
1
0
x
y
y = x
1
α = 0.5
y = f (x)
1
0
x3
x0
x2 x1
y
y = x
f (x0)
(a)
(b)
x0
x3
x2
x1
Figure 31.1: (a) Regardless of the value of x0, the orbit always ends up at the origin
when α < 1. (b) Even for α > 1, it appears that the orbit always ends up at some
attractor regardless of the value of x0. Note that now the origin has become a “repellor.”
“repellor.” We can treat the behavior of the logistic map at general ﬁxed
points analytically.
First let us consider a general (one-dimensional) iterated map as given by
Equation (31.3). We are seeking the ﬁxed points of (31.3). These points—
commonly labeled by an asterisk—satisfy
x∗= fα(x∗),
i.e., they are intersections of the curves y = x and y = fα(x) in the xy-plane.
An important property of ﬁxed points is their stability—or whether they are
attractors or repellors. To test this property, we Taylor-expand the iterated
stability of ﬁxed
points
map function around x∗, keeping the ﬁrst two terms:
xk+1 = fα(xk) = fα(x∗) + dfα
dx




x∗(xk −x∗) = x∗+ dfα
dx




x∗(xk −x∗)
or
|xk+1 −x∗|
|xk −x∗|
=




dfα
dx




x∗
= |f ′
α(x∗)|.
So, xk+1 will be farther away from (or closer to) x∗than xk if the absolute
value of the derivative of the function is greater than one (or less than one).
analytic criterion
for stability of a
ﬁxed point of an
iterated map
Box 31.1.1. A ﬁxed point x∗of an iterated map (31.3) is stable if
|f ′
α(x∗)| < 1 and unstable if |f ′
α(x∗)| > 1.
Example 31.1.1. For the logistic map, fα(x) = αx(1−x) so that f ′
α(x) = α−2αx.
The ﬁxed points are x∗
1 = 0 and x∗
2 = 1 −1/α. Therefore,
f ′
α(x∗
1) = f ′
α(0) = α
and
f ′
α(x∗
2) = f ′
α(1 −1/α) = 2 −α.
(31.6)

31.1 Systems Obeying Iterated Maps
757
It follows that the ﬁxed point at x = 0 is stable (attractive) if α < 1, while for
this same value of α the ﬁxed point x∗
2 is unstable (repulsive). Thus, for α < 1, all
trajectories are attracted to the ﬁxed point at x = 0.
Equation (31.6) also shows that for 1 < α < 3, the other ﬁxed point becomes
stable while the ﬁxed point at the origin becomes unstable. This is also consistent
with the behavior of the logistic map depicted in Figure 31.1(b).
■
The criterion of Box 31.1.1 can also be stated graphically. Since 1 is the
slope of the line y = x, and since a ﬁxed point is an intersection of the two
curves y = x and y = fα(x), the criterion of Box 31.1.1 is a comparison of the
slope of the tangent to y = fα(x) with the slope of y = x: A ﬁxed point x∗
of an iterated map (31.3) is stable, if the acute angle that the tangent line at
(x∗, fα(x∗)) makes with the x-axis is smaller than the corresponding angle of
the line y = x. If this angle is larger, then the ﬁxed point is unstable. This is
equivalent to the simpler statement:
graphical criterion
for stability of a
ﬁxed point of an
iterated map
Box 31.1.2. A ﬁxed point x∗of an iterated map fα(x) is stable (unstable)
if immediately to the right of x∗, the curve y = fα(x) lies below (above)
the line y = x.
31.1.2
Bifurcation
Although the logistic map has no stable ﬁxed points beyond xα = 1−1/α, we
may ask whether there are points at which the iterated map is “semi-stable.”
What does this mean? Instead of demanding strict stability or instability, let
us consider a case in which the map may oscillate between two values. This
situation is neither completely stable nor completely unstable: Although the
system moves away from the point in question, it does not leave it forever.
Suppose that just above the largest value of a stable α, the system starts to
oscillate between two values of x. This is an example of bifurcation:
bifurcation and
period doubling
Box 31.1.3. When the development of a system splits into two regions as
a parameter of the equations of motion of the system increases slightly, we
say that a bifurcation has occurred and call the splitting of the trajectory
a period-doubling bifurcation.
Suppose that there are two “ﬁxed” points x∗
1 and x∗
2 between which the
function oscillates such as the two points illustrated in Figure 31.2(a). These
ﬁxed points must satisfy
x∗
2 = fα(x∗
1),
x∗
1 = fα(x∗
2).
(31.7)
To gain further insight into the behavior of the logistic map, we introduce the
so-called second iterate of fα denoted by f [2]
α
and deﬁned by
second iterate

758
Nonlinear Dynamics and Chaos
x0
0
0.2
1
0.558
0.7646
(a)
(b)
Figure 31.2: (a) For α = 3.1, there are clearly two attractors located at x = 0.5580
and x = 0.7646. (b) For α = 3.99, no attractor seems to exist because the iterations
do not seem to converge in the diagram.
f [2]
α (x) ≡fα(fα(x)).
(31.8)
From this deﬁnition, it is clear that every ﬁxed point of fα is also a ﬁxed
point of f [2]
α . However, the converse statement is not true. In fact, x∗
1 and x∗
2
deﬁned in Equation (31.7) are ﬁxed points of f [2]
α :
f [2]
α (x∗
1) = fα(fα(x∗
1)) = fα(x∗
2) = x∗
1,
f [2]
α (x∗
2) = fα(fα(x∗
2)) = fα(x∗
1) = x∗
2,
but not of fα. It now follows that ﬁxed points of f [2]
α
give information about
period-doubling bifurcation.
For the logistic map, f [2]
α
can be found easily:
f [2]
α (x) = fα(fα(x)) = αfα(x)[1 −fα(x)]
= α[αx(1 −x)][1 −αx(1 −x)] = α2x(1 −x)(1 −αx + αx2)
= −α3x4 + 2α3x3 −(α2 + α3)x2 + α2x.
(31.9)
The ﬁxed points of f [2]
α (x) are, therefore, determined by the equation
x = −α3x4 + 2α3x3 −(α2 + α3)x2 + α2x
which shows that there are, in general, four ﬁxed points, one at x = 0, and
three others satisfying the cubic equation
α3x3 −2α3x2 + (α3 + α2)x −α2 + 1 = 0.
(31.10)
We can actually solve this equation because we know that one of its roots is
x1(α) = 1 −1/α, a ﬁxed point of fα(x). The cubic polynomial in Equation

31.1 Systems Obeying Iterated Maps
759
(31.10) can thus be factored out as α3[x−x1(α)] times a quadratic polynomial
whose roots give the remaining solutions to (31.10). The reader may verify
that these roots are
x2(α) = 1 + α +
√
α2 −2α −3
2α
,
x3(α) = 1 + α −
√
α2 −2α −3
2α
.
(31.11)
These two functions start out at the common value of 2
3 when α = 3. Then, as
a function of α, x2(α) monotonically increases and asymptotically approaches
1; x3(α) monotonically decreases and asymptotically approaches 0.
We are interested in those values of α for which the ﬁxed points are not
completely unstable. In the present case, this means that the value of the
iterated map must oscillate between only two values. This will happen only
if the two points are stable ﬁxed points of f [2]
α . Since by Box 31.1.1 stability
imposes a condition on the derivative of the function, we need to look at the
derivative of f [2]
α .
Using the chain rule, which in its most general form is
d
dx[g(h(x))] = g′(h(x))h′(x)
we obtain
d
dxf [2]
α (x) = [f ′
α(fα(x))]f ′
α(x).
(31.12)
In particular, if x happens to be a ﬁxed point x∗of fα, then
df [2]
α
dx





x∗
= f ′
α(fα(x∗)
  
=x∗
)f ′
α(x∗) = [f ′
α(x∗)]2.
(31.13)
This shows that if x∗is a stable ﬁxed point of fα, then
|f ′
α(x∗)| < 1 ⇒[f ′
α(x∗)]2 < 1 ⇒




d
dxf [2]
α (x∗)




 < 1
and x∗is a stable ﬁxed point of f [2]
α
as well. Furthermore, at the two ﬁxed
points of f [2]
α
discussed above, Equation (31.12) yields
df [2]
α
dx





x∗
1
= f ′
α(fα(x∗
1)
  
x∗
2
)f ′
α(x∗
1) = f ′
α(x∗
2)f ′
α(x∗
1),
df [2]
α
dx





x∗
2
= f ′
α(fα(x∗
2)
  
x∗
1
)f ′
α(x∗
2) = f ′
α(x∗
1)f ′
α(x∗
2).
(31.14)
It follows that f [2]
α
has the same derivatives at these two points.

760
Nonlinear Dynamics and Chaos
The concept of iteration of fα can be readily generalized. The nth iterate
nth iterate
of fα is
f [n]
α
≡f(f(. . . f



n times
(x) . . .))
and as in the case of f [2]
α , the ﬁxed points of fα are also ﬁxed points of f [n]
α
and
the stable ﬁxed points of fα are also stable ﬁxed points of f [n]
α . The converse
of neither of these statements is, in general, true.
The utility of the concept of the nth iterate comes in the analysis of the
location of bifurcation points. To be speciﬁc, let us go back to the logistic map
and Equation (31.6). The stable points, being characterized by the absolute
value of the derivative of the map function, occur at x = 0 when 0 < α < 1
and at x = 1 −1/α when 1 < α < 3. Within the α-range of stability, the
derivative of fα ranges between1 −1 and +1, starting with +1 at α = 1 and
ending with −1 at α = 3 [see the second equation in (31.6)]. Beyond this
value of α—which is the parameter at which the 2-cycle ﬁxed point occurs
and which we now denote by α1—fα has no stable points. Equation (31.13),
however, shows that the derivative of f [2]
α
is +1 there. This means that the
derivative of f [2]
α
can decrease down to −1 as α increases beyond α1. In fact,
what happens as α increases past α1 is precisely a repetition of what happened
to fα between α = 1 and α = 3: The derivative of f [2]
α
keeps decreasing until
at a certain value of α denoted by α2 a period-doubling bifurcation occurs
for f [2]
α . This corresponds to a 4-cycle ﬁxed point. Thus a 4-cycle ﬁxed point
4-cycle ﬁxed point
x∗, as well as the corresponding value of α, is obtained by imposing the two
requirements
f [2]
α2 (x∗) = x∗
and
df [2]
α2
dx





x∗
= −1.
(31.15)
This equation entails an important result. By Equation (31.14), the derivative
of f [2]
α2 at its two stable points are equal. Therefore, both stable points give
rise to the same pair of equations (31.15). In particular,
Box 31.1.4. Any value of α2 that gives a solution for the ﬁrst ﬁxed point
must also give a solution for the second ﬁxed point. In fact, we should ex-
pect two values of x∗for every α2 that solves the pair of equations (31.15).
For the logistic map, Equation (31.15) becomes [see (31.9)]
x∗= α2
2x∗(1 −x∗)(1 −α2x∗+ α2x∗2),
−1 = −4α3
2x∗3 + 6α3
2x∗2 −2(α2
2 + α3
2)x∗+ α2
2.
1The x = 0 is an exception because we are assuming that α is a positive quantity,
therefore, f′(0) = α cannot be negative.

31.1 Systems Obeying Iterated Maps
761
One can solve these equations and obtain eight possible pairs (x∗, α2). The
only two acceptable real pairs which have a value of α2 larger than three are

4 +
√
6 ±
	
14 −4
√
6
10
, 1 +
√
6

= (0.644949 ± 0.204989, 3.44949). (31.16)
In particular, α2 = 1 +
√
6 = 3.44949 is the 4-cycle ﬁxed point depicted in
Figure 31.3 corresponding to the two x values of approximately 0.85 and 0.44.
The generalization to 2n-cycles is now clear. One simply constructs the
2nth iterate of fα and solves the two equations
f [2n]
α
(x∗) = x∗
and
df [2n]
α
dx





x∗
= −1.
In practice, these are too complicated to solve analytically, but numerical
methods are available for their solution.
Each solution consists of a pair
(x∗
n, αn) where x∗
n is the 2n-cycle ﬁxed point and αn is the corresponding
control parameter. As in the case of f [2]
α , for each acceptable αn, there are 2n
ﬁxed points.
31.1.3
Onset of Chaos
Suppose we keep increasing α slowly. It may happen that at a certain value of
α no ﬁnite set of “stable” points exists. A graphical analysis of this situation
is depicted in Figure 31.2(b) showing that the behavior of the logistic map
is chaotic. What is the relation between the value of α at which chaos sets
in (which we denote by αc) and αn?
Considering the chaotic behavior as
Figure 31.3: The bifurcation diagram for the logistic map. The behavior of the function
is analytically very simple for α < 3. For α > 3, the behavior is more complicated. The
4-cycle ﬁxed points are clearly shown to occur at α ≈3.45. From approximately 3.57
onward, chaotic behavior sets in.

762
Nonlinear Dynamics and Chaos
one corresponding to an inﬁnite-cycle ﬁxed point, we conclude that, on a
bifurcation diagram, chaos will occur at
αc = α∞≡lim
n→∞αn.
(31.17)
The limit in (31.17) is one way of characterizing chaos. A more direct way
is to look at the trajectories. Two nearby points starting at x0 and x0 +ϵ will
be separated after n iterations by a distance
the n-th iterative
Lyapunov
exponent
dn ≡



f [n]
α (x0 + ϵ) −f [n]
α (x0)



 .
If this separation grows exponentially, we have a chaotic behavior. We deﬁne
λ[n]
x0 , the nth iterative Lyapunov exponent at x0, by
dn ≡d0eλ[n]
x0 n ≡ϵeλ[n]
x0 n.
Then
λ[n]
x0 = 1
n ln
dn
ϵ

= 1
n ln
⎧
⎨
⎩



f [n]
α (x0 + ϵ) −f [n]
α (x0)



ϵ
⎫
⎬
⎭.
As ϵ →0, the RHS becomes the absolute value of the derivative of the n-th
iterate at x0. But by the chain rule
df [n]
α
dx





x0
=
d
dx
*
fα(f [n−1]
α
(x))
+



x0
= f ′
α(f [n−1]
α
(x0)



=xn−1
) df [n−1]
α
dx





x0
= f ′
α(xn−1) df [n−1]
α
dx





x0
.
Using this relation repeatedly, we obtain
df [n]
α
dx





x0
= f ′
α(xn−1)f ′
α(xn−2) . . . f ′
α(xn−k) df [n−k]
α
dx





x0
= f ′
α(xn−1)f ′
α(xn−2) . . . f ′
α(x0),
where in the last step we set k = n and noted that f [0]
α = fα. It now follows
that
λ[n]
x0 = 1
n ln [|f ′
α(x0)| |f ′
α(x1)| · · · |f ′
α(xn−1)|]
which can also be written as
λ[n]
x0 = 1
n
n−1

k=0
ln |f ′
α(xk)| .
(31.18)

31.2 Systems Obeying DEs
763
It is common to deﬁne the local Lyapunov exponent as follows:
local Lyapunov
exponent
λx0 ≡lim
n→∞λ[n]
x0 = lim
n→∞
1
n
n−1

k=0
ln |f ′
α(xk)| .
(31.19)
To characterize the chaotic behavior of systems obeying iterated maps,
one has to calculate λx0 for a sample of trajectory points and then take their
average. The result is called the Lyapunov exponent for the system. It
Lyapunov
exponent
turns out that
Box 31.1.5. A necessary condition for a system obeying an iterated map
function fα(x) to be chaotic for α is for its Lyapunov exponent to be
positive at α.
31.2
Systems Obeying DEs
As a paradigm of a nonlinear dynamical system, we shall study the motion
of a harmonically driven dissipative pendulum whose angle of oscillation is
not necessarily small. The equation of motion of such a pendulum, coming
directly from the second law of motion, is
md2x
dt2 = F0 cos(Ωt) −bdx
dt −mg sin θ,
(31.20)
where x is the length (as measured from the equilibrium position) of the arc
of the circle on which mass m moves (see Figure 31.4).
The ﬁrst term on the RHS of Equation (31.20) is the harmonic driving
force with angular frequency Ω, the second is the dissipative (friction, drag,
etc.) force, and the last is the gravitational force in the direction of motion.
l
h
θ
x
mg
θ
Figure 31.4: The displacement x and the gravitational force acting on the pendulum.

764
Nonlinear Dynamics and Chaos
The minus signs appear because the corresponding forces oppose the motion.
Since the pendulum is conﬁned to a circle, x and θ are related via x = lθ, and
we obtain
mld2θ
dt2 = F0 cos(Ωt) −bldθ
dt −mg sin θ.
Let us change t to t = τ
	
l/g where τ is a dimensionless parameter measuring
time in units of T/(2π) with T being the period of the small-angle pendulum.2
Then, with the dimensionless constants
γ ≡b
m
"
l
g ,
φ0 ≡F0
mg ,
ωD ≡Ω
"
l
g ,
the DE of motion becomes
d2θ
dτ 2 + γ dθ
dτ + sin θ = φ0 cos(ωDτ).
It is customary to write this as
¨θ + γ ˙θ + sin θ = φ0 cos(ωDt),
(31.21)
where now t is the “dimensionless” time, and the dot indicates diﬀerentiation
with respect to this t.
31.2.1
The Phase Space
The study of dynamical systems—i.e., systems obeying DEs—is considerably
more complicated than systems obeying iterated maps. While in the latter
case we were able to use a fair amount of analytical tools, the discussion of
the former requires an enormous amount of numerical computation.
One of the devices that facilitates our understanding of dynamical systems
is the phase space diagram. The phase space of a dynamical system is a
phase space
diagram
Cartesian multidimensional space whose axes consist of positions and mo-
menta of the particles in the system. Instead of momenta the velocities of
particles are mostly used. Thus a single particle conﬁned to one dimension
(such as a particle in free fall, a mass attached to a spring, or a pendulum)
has a two-dimensional phase space corresponding to the particle’s position and
speed. Two particles moving in a single dimension have a four-dimensional
phase space corresponding to two positions and two speeds. A single par-
ticle moving in a plane also has a four-dimensional phase space because two
coordinates are needed to determine the position of the particle, and two com-
ponents to determine its velocity, and a system of N particles in space has a
6N-dimensional phase space.
A trajectory of a dynamical system is a curve in its phase space corre-
phase space
trajectory
sponding to a possible motion of the system. If we can solve the equations
2Recall that T = 2π
	
l/g. So τ = t/(T/2π) is indeed dimensionless.

31.2 Systems Obeying DEs
765
of motion of a dynamical system, we can express all its position and velocity
variables as a function of time, constituting a parametric equation of a curve
in phase space. This curve is the trajectory of the dynamical system.
Let us go back to our pendulum, and consider the simplest situation in
which there is no driving force, the dissipative eﬀects are turned oﬀ, and the
angle of oscillation is small. Then (31.21) reduces to ¨θ + θ = 0, whose most
general solution is θ = A cos(t + α) so that
x1 ≡θ = A cos(t + α),
x2 ≡ω ≡˙θ ≡dθ
dt = −A sin(t + α).
(31.22)
This is a one-dimensional system (there is only one coordinate, θ) with a two-
dimensional phase space. Equation (31.22) is the parametric equation of a
circle of radius A in the x1x2-plane. Because A is arbitrary (it is, however,
determined by initial conditions), there are (inﬁnitely) many trajectories for
this system, some of which are shown in Figure 31.5.
Let us now make the system a little more complicated by introducing a
dissipative force, still keeping the angle small. The DE is now
¨θ + γ ˙θ + θ = 0
and the general solution for the damped oscillatory case is
x1 = θ(t) = Ae−γt/2 cos(ω0t + α)
where
ω0 ≡
	
4 −γ2
2
with
x2 = ω = ˙θ = −Ae−γt/2 3γ
2 cos(ω0t + α) + ω0 sin(ω0t + α)
4
.
x1
x2
Figure 31.5:
The phase space trajectories of a pendulum undergoing small-angle
oscillations with no driving or dissipative forces. Diﬀerent circles correspond to diﬀerent
initial conditions.

766
Nonlinear Dynamics and Chaos
θ
ω
Figure 31.6: The phase space trajectories of a damped pendulum undergoing small-
angle oscillations with no driving force. Diﬀerent spirals correspond to diﬀerent initial
conditions. The larger shaded region, in time, shrinks to the smaller one.
The trajectories of this system are not as easily obtainable as the un-
damped linear oscillator discussed above. However, since the two coordinates
of the phase space are given in terms of the parameter t, we can plot the
trajectories. Two such trajectories for two diﬀerent A’s (but the same γ) are
shown in Figure 31.6.
A new feature of this system is that regardless of where the trajectory
starts at t = 0, it will terminate at the origin. The analytic reason for this is
of course the exponential factor in front of both coordinates which will cause
their decay to zero after a long time.3 It seems that the origin “attracts” all
trajectories, and for this reason is called an attractor.4
attractor
There are other kinds of attractors in nonlinear dynamics theory.
For
example, if trajectories approach an arc of a curve, or an area of a surface,
then the curve or the area becomes the attractor. Furthermore, for a given
value of the parameter, there may be more than one attractor for a given
dynamical system (just as there were more than one ﬁxed point for iterated
maps); and it may happen that the trajectories approach these attractors only
for certain initial values of the dynamical variables. The set of initial values
corresponding to trajectories that are attracted to an attractor is called the
basin of attraction for that attractor. The set of initial values that lie on
basin of attraction
the border between the basin of attraction of two diﬀerent attractors is called
a separatrix.
separatrix
31.2.2
Autonomous Systems
Now we want to consider motion with large angles. The DE is then no longer
linear. The discussion of (nonlinear) DEs of higher orders is facilitated by
3“Long” compared to 1/γ.
4This is what we called a ﬁxed point in our discussion of iterated maps.
However,
because of the existence of a variety of “ﬁxed objects” for dynamical systems, it is more
common to call these attractors.

31.2 Systems Obeying DEs
767
treating derivatives as independent variables. The deﬁning relations for these
derivatives as well as the DE itself give a set of ﬁrst-order DEs. For example,
the third-order DE
d3x
dt3 = x4 d2x
dt2 + sin(ωt −kx)dx
dt + ex cos(ωt)
can be turned into three ﬁrst-order DEs by setting ˙x ≡x1 and ¨x ≡x2. Then
the DE splits into the following three ﬁrst-order DEs:
˙x = x1,
˙x1 = x2,
˙x2 = x4x2 + sin(ωt −kx)x1 + ex cos(ωt).
This is a set of three equations in the three unknowns x, x1, and x2, which,
in principle, can be solved.
It is desirable to have a so-called autonomous system of ﬁrst-order DEs.
autonomous and
nonautonomous
dynamical systems
These are systems which have no explicit dependence on the independent
variable (in our case, t). Our equations above clearly form a set of nonau-
tonomous DEs. The nonautonomous systems can be reduced to autonomous
ones by a straightforward trick: One simply calls t a new variable. More specif-
ically, in the equation above, let x3 ≡ωt. Then the nonautonomous equations
above turn into the following autonomous system:
˙x = x1,
˙x1 = x2,
˙x2 = x4x2 + sin(x3 −kx)x1 + ex cos x3,
˙x3 = ω.
We have had to increase the dimension of our phase space by one, but in
return, we have obtained an autonomous system of DEs.
Based on the prescription above, we turn the second-order DE of the driven
pendulum into a set of ﬁrst-order DEs. First we rewrite the DE describing a
general pendulum [see Equation (31.21)] as
¨θ + γ ˙θ + sin θ = φ0 cos α,
where α is simply ωDt. Then turn this equation into the following entirely
equivalent set of three ﬁrst-order DEs:
˙θ = ω,
˙ω = −γω −sin θ + φ0 cos α,
˙α = ωD.
(31.23)
The two-dimensional (θ, ω) phase space has turned into a three-dimensional
(θ, ω, α) phase space. But the resulting system is autonomous.
Just as in the linear case, it is instructive to ignore the damping and driving
forces ﬁrst. We set γ and φ0 equal to zero in Equation (31.23) and solve the set
of DEs numerically.5 For small angles, we expect a simple harmonic motion
5The solution can be given in terms of elliptic functions as discussed in Chapter 11.

768
Nonlinear Dynamics and Chaos
(SHM). So, with θ(0) = π/10 and ω(0) = 0,6 we obtain the plot on the left of
Figure 31.7. This plot shows a simple trigonometric dependence of angle on
time.
The initial angular displacement of the plot on the right of Figure 31.7 is
approximately π radians corresponding to raising the mass of the pendulum
all the way to the top.7 The ﬂattening of curves at the maxima and minima
of the plot indicates that the pendulum almost stops once it reaches the top
and momentarily remains motionless there. This is expected physically as
θ(0) = π is a location of (unstable) equilibrium, i.e., with ω(0) = 0, the
pendulum can stay at the top forever. So, for θ(0) ≈π, the pendulum is
expected to stay at the top, not forever, but for a “long” time.
The phase space diagram of the pendulum can give us much information
about its behavior. With the initial angular velocity set at zero, the pendulum
will exhibit a periodic behavior represented by closed loops in the phase space.
Figure 31.8 shows four such closed loops corresponding—from small to large
Figure 31.7: The undamped undriven pendulum shows an SHM for small initial angles
(the plot on the left has a maximum angle of π/10). For large angles, the motion is
periodic but not an SHM. The maximum angle of the plot on the right is slightly less
than π.
–3
–2
–1
0
1
2
3
–2
–1
0
1
2
Figure 31.8: Phase space diagrams for a pendulum corresponding to diﬀerent values
of maximum displacement angles (horizontal axis). The inner diagrams correspond to
smaller values; the outermost plot has a maximum angle of 179.98 degrees at which the
angular speed is 1.
6It is important to keep ω(0) small, because a large initial angular velocity (even at a
small initial angle) can cause the pendulum to reach very large angles!
7For this to be possible, clearly the mass should be attached to a rigid rod (not a string)!

31.2 Systems Obeying DEs
769
loops—to the initial angular displacement of π/5, π/2, 2π/3, and (almost) π.
These loops represent oscillations only: the angular displacement is bounded
between a minimum and a maximum value determined by θ(0). The closed
loops are characterized by the fact that the angular speed vanishes at maxi-
mum (or minimum) θ, allowing the pendulum to start moving in the opposite
direction.
The outermost curves result from θ(0) = −π, ω(0) = 1 (the upper curve),
and θ(0) = π, ω(0) = −1 (the lower curve), and represent rotations. The an-
gular displacement is unbounded: it keeps increasing for all times. Physically,
this corresponds to forcing the pendulum to “go over the hill” at the top by
providing it an initial angular velocity. If the pendulum is pushed over this
hill once, it will continue doing it forever because there is no damping force.
The rotations are characterized by a nonzero angular velocity at θ = ±π. This
is clearly shown in Figure 31.8.
What happens when the damping force it turned on? We expect the tra-
jectories to spiral into the origin of the phase space as in the case of the linear
(small-angle) pendulum. Figure 31.9 shows two such trajectories correspond-
ing to an initial displacement of just below π (on the right), and just above
−π (on the left). For both trajectories, the initial angular velocity is zero. It
is intuitively obvious that regardless of the initial conditions, the pendulum
will eventually come to a stop at θ = 0 if there are no driving forces acting
on it. So, Figure 31.9 is really representative of all dissipative motions of the
pendulum.
The origin of the phase space is a ﬁxed point of the pendulum dynamics.
But it is not the only one. In general, any point in the phase space for which
the time derivative of all coordinates of the trajectory are zero is a ﬁxed point
(see Problem 31.6). If we set all the functions on the RHS of Equation (31.23)
equal to zero,8 we obtain
ω = 0
and
sin θ = 0
corresponding to inﬁnitely many ﬁxed points at (nπ, 0) with n an integer.
Points in the neighborhood of the origin, i.e., those lying in the basin of
–3
-2
-1
0
1
2
3
–1.5
-1
–0.5
0
0.5
1
1.5
Figure 31.9:
Phase space diagrams for a dissipative pendulum.
Two trajectories
starting at θ ≈−π, ω = 0 and θ ≈π, ω = 0 eventually end up at the origin.
8Still assuming no driving force.

770
Nonlinear Dynamics and Chaos
attraction of the origin are attracted to the origin; the rest of the ﬁxed points
are repellors (or unstable) for such points.
The interesting motion of a pendulum begins when we turn on a driving
force regardless of whether the dissipative eﬀect is present or not. Neverthe-
less, let us place the pendulum in an environment in which γ = 0.3. Now
drive this pendulum with a (harmonic) force of amplitude φ0 = 0.5 and angu-
lar frequency ωD = 1. A numerical solution of (31.23) will then give a result
which has a transient motion lasting until t ≈32. From t = 32 onward, the
system traverses a closed orbit in the phase diagram as shown in Figure 31.10.
This orbit is an attractor in the same sense as a point is an attractor for the
logistic map and a dissipative nondriven pendulum. An attractor such as the
one exhibited in Figure 31.10 is called a limit cycle.
limit cycle
31.2.3
Onset of Chaos
As we increase the control parameter φ0, the phase space trajectories go
through a series of periodic limit cycles until they ﬁnally become completely
aperiodic: chaos sets in. Figure 31.11 shows four trajectories whose common
initial angular displacement θ0, initial angular velocity ω0, damping factor γ,
and drive frequency ωD are, respectively, π, 0, 0.5, and 2/3. The only (con-
trol) parameter that is changing is the amplitude of the driving force φ0. This
changes from 0.97 for the upper left to 1.2 for the lower right diagram.
A closer scrutiny of Figure 31.11—which we shall forego—will reveal that
the chaotic behavior of the diagram at the lower right takes place after the
pendulum goes through a bifurcation process as in the case of the logistic
map. However, unlike the logistic map whose bifurcation stages were divided
by ﬁxed “points,” the stages for the pendulum are characterized by limit cy-
cles.
In fact, the diagram at the upper left, corresponding to φ0 = 0.97,
consists of two (very closely spaced) limit cycles. Bifurcations involving limit
cycles are called Hopf bifurcation after the mathematician E. Hopf who
Hopf bifurcation
generalized the earlier results of Poincar´e on such bifurcations to higher di-
mensions. The logistic map and the nonlinear pendulum have the following
property in common: their “route to chaos” is via bifurcation. This is not
4
5
6
7
–1.5
–1
–0.5
0
0.5
1
1.5
Figure 31.10: The moderately driven dissipative pendulum with γ = 0.3 and φ0 = 0.5.
After a transient motion, the pendulum settles down into a closed trajectory.

31.2 Systems Obeying DEs
771
Figure 31.11: Four trajectories in the phase space of the damped driven pendulum.
The only diﬀerence in the plots is the value of φ0 which is 0.97 for the upper left, 1.1
for the upper right, 1.15 for the lower left, and 1.2 for the lower right diagrams.
true for all chaotic systems; there are other “routes to chaos,” but we shall
not investigate them here.
The main characteristic of chaos is the exponential divergence of neigh-
boring trajectories. We have seen this behavior for the logistic map. A very
nice illustration of this phenomenon for the nonlinear pendulum is depicted
in Figure 31.12 where two nearby trajectories in the neighborhood of point
(−2, −2) are seen to diverge dramatically (in eight units of time).
The divergence of trajectories and the ensuing chaos has been termed the
butterﬂy eﬀect by Lorenz who, in the title of one of his talks, asked the
butterﬂy eﬀect
question: “Does the ﬂap of a butterﬂy’s wings in Brazil set oﬀa tornado
in Texas?” The point Lorenz is making in this statement is that if the at-
mosphere displays chaotic behavior (as a simple model proposed by Lorenz
predicts), then a very small disturbance, such as the ﬂapping of a butterﬂy’s
–8
–6
–4
–2
2
–3
–2
–1
1
2
3
Figure 31.12: The projection onto the θω-plane of two trajectories starting at approxi-
mately the same point near (−2, −2) diverge considerably after eight units of time. The
loop does not contradict the DE uniqueness theorem!

772
Nonlinear Dynamics and Chaos
wings, would make it impossible to predict the long-term behavior of the
weather.
In general, for a dynamical system obeying an autonomous set of ﬁrst-
order DEs to be chaotic three requirements are to be met:
1. The trajectories must not intersect.
2. The trajectories must be bounded.
3. Nearby trajectories ought to diverge exponentially.
The ﬁrst requirement is a direct consequence of the uniqueness theorem9 for
the solution of DEs: if two trajectories cross, the system will have a “choice”
for its further development starting at the intersection point, and this is not
allowed. The very notion of ﬁxed point as well as the crisscrosses of Figures
31.11 and 31.12 may appear to violate the ﬁrst property above. However,
we have to remind ourselves that ﬁxed points are (asymptotically) achieved
after an inﬁnite amount of time. As for the two ﬁgures, the reader recalls
that all plots in those ﬁgures are projections of the three-dimensional trajec-
tories onto the yz-plane. The three-dimensional trajectories never cross (see
Figure 31.13).
The second requirement is important because unbounded regions of phase
space correspond to inﬁnities which are to be avoided. The third requirement
is simply what deﬁnes chaos.
It turns out that one- and two-dimensional
phase spaces cannot accommodate all of these requirements.
However, in
three dimensions, one can “stretch” out the trajectories that want to loop in
two dimensions as shown in Figure 31.13 where the loop of Figure 31.12 is
seen to have been only a two-dimensional shadow! Thus,
–2
0
2
0
2
4
6
8
–5
0
Figure 31.13: The two trajectories of Figure 31.12 shown in the full three-dimensional
phase space.
9For our purposes this theorem states that if the dynamical variables and their ﬁrst
derivatives of a system are speciﬁed at some (initial) time, then the evolution of the system
in time is uniquely determined. In the context of phase space this means that from any
point in phase space only one trajectory can pass.

31.3 Universality of Chaos
773
Box 31.2.1. A necessary condition for a system obeying autonomous DEs
of ﬁrst degree to be chaotic is to have a phase space that has at least three
dimensions.
The reader may wonder how a (one-dimensional) pendulum can satisfy
the condition of Box 31.2.1. After all, the phase space of such a pendulum
has only two dimensions. The answer lies in the fact that although a driven
pendulum—with only θ and ω = ˙θ regarded as independent variables—obeys
DEs that are not autonomous, when time is turned into the third dimension
Only a driven
pendulum (of
large-angle
oscillation)
exhibits chaotic
behavior.
of the phase space, a set of three autonomous DEs will result which allows
chaotic behavior. This is in fact obvious from Equation (31.23) where α—the
third dimension of the phase space—is seen to be essentially time in units of
ωD. A pendulum that is not driven does not exhibit chaotic behavior.
31.3
Universality of Chaos
In the preceding sections, we examined two completely diﬀerent systems dis-
playing chaotic behavior. Although there are diﬀerent “routes” to chaos, we
shall concentrate only on the period-doubling route because it has been the-
oretically developed further than the other routes, and because it displays a
universal character common to all such chaotic systems as discovered by one
of the founders of the theory of chaos, Mitchell Feigenbaum.
31.3.1
Feigenbaum Numbers
In our theoretical investigation of the logistic map, we introduced the control
parameters αn at which the nth bifurcation takes place and for which there
are a number of 2n-cycle ﬁxed points. It turns out that the ratio
δn ≡αn −αn−1
αn+1 −αn
(31.24)
is almost the same for all large n, and that, in the limit as n →∞, it ap-
proaches a number δ∗, now called the Feigenbaum delta:
Feigenbaum delta
δ∗≡lim
n→∞δn = lim
n→∞
αn −αn−1
αn+1 −αn
= 4.66920 . . ..
(31.25)
Feigenbaum looked at the same ratio for the so-called iterated sine function
xn+1 = β sin(πxn)
and found that exactly the same number was obtained in the limit. Later, he
showed that δ∗is the same for all iterated map functions!10
10This is not entirely true. The map functions should have a parabolic “shape” at their
maximum. The logistic map and the sine function—as well as many other functions—have
this property.

774
Nonlinear Dynamics and Chaos
We can use δ∗to calculate approximations to αn for large values of n, and,
in particular, to ﬁnd an approximate value for α∞. First we note that, if we
approximate δn with δ∗, then (31.24) yields
αn+1 = αn −αn−1
δn
+ αn ≈αn −αn−1
δ∗
+ αn.
For example,
α3 ≈α2 −α1
δ∗
+ α2,
α4 ≈α3 −α2
δ∗
+ α3,
or
α4 ≈(α2 −α1)/δ∗
δ∗
+ α3 = (α2 −α1)
 1
δ∗+ 1
δ∗2

+ α2.
We can easily generalize this to
αN ≈(α2 −α1)
 1
δ∗+ 1
δ∗2 + · · · +
1
δ∗(N−1)

+ α2.
(31.26)
In the limit that N →∞, the sum becomes a geometric series which adds up
to 1/(δ∗−1). So,
α∞≈α2 −α1
δ∗−1 + α2.
(31.27)
With α1 = 3 and α2 = 1 +
√
6, we obtain
α∞≈
√
6 −2
3.66920 + 1 +
√
6 = 3.572.
The actual value—obtained by more elaborate calculations—is 3.5699 . . ..
Another quantity that seems to be universal is the ratio of the consecutive
“bifurcation sizes.” We mentioned earlier that there are several ﬁxed points
associated with the 2n-cycle parameter αn. At each stage of bifurcation, these
ﬁxed points come in pairs. For example, at α2 ≡1+
√
6, Equation (31.16) gives
the two ﬁxed points at x = 0.849938 and x = 0.43996. We deﬁne the “size”
d1 of the 4-cycle bifurcation as the (absolute value of the) diﬀerence between
these x-values. In general, we deﬁne dn, the size of the bifurcation pattern
of period 2n as the largest (in absolute value) of the diﬀerences between the
two x’s of each of the 2n pairs of ﬁxed points. On a bifurcation diagram, one
would measure the vertical distance between the points where each curve of the
diagram starts to branch out. If there are several such distances, one chooses
the largest one. The second Feigenbaum number, the so-called Feigenbaum
alpha, is then deﬁned as
Feigenbaum alpha
α∗≡lim
n→∞
dn
dn+1
= 2.5029 . . ..
(31.28)
Feigenbaum found that this number is obtained for the bifurcation pattern of
all chaotic systems which reach chaos via bifurcation.

31.3 Universality of Chaos
775
Aside from its universality as applied to diﬀerent chaotic systems, this
number suggests a general “size” scaling within the bifurcation pattern of
a single system: For large enough values of n, the ratio of the size of each
bifurcation is the same as the previous one. If we “blow up” the small bifur-
cations taking place for large values of n, they look almost identical to the
ones occurring before them. This property is also called self-similarity.
self-similarity
31.3.2
Fractal Dimension
An elegant way of quantifying chaos is by examining the geometric properties
of the trajectory of the chaotic system under study. Suppose we let the system
run for a long time and suppose that it gravitates toward an attractor and
remains there.11 What is the “dimension” of the trajectory? The clariﬁcation
of this question and the logic (as well as the application) of its answer is the
subject of this subsection.
Intuitively, one assigns the dimension of 0 to points, 1 to curves, 2 to
surfaces, 3 to volumes, and n to “solid” objects residing in spaces requiring
n coordinates to describe their points. How can we go beyond intuition? We
use the so-called Hausdorﬀdimension, whose calculation goes as follows.
Hausdorﬀ
dimension
Try to cover the geometrical object by appropriate “boxes” of side length r.
Now count the number N(r) of boxes required to contain all points of the
geometric object. The Hausdorﬀdimension D is deﬁned by
N(r) = lim
r→0
 
kr−D!
,
(31.29)
where k is an inessential proportionality constant which describes the shape of
the “box.” For example, as a box, we could use a “sphere” of radius r. Then
the “volume” would be 2r for a line, πr2 for a circle, and 4
3πr3 for a sphere.
Thus, k is 2, or π or 4
3π. If we choose “cubes,” k will always be 1. Furthermore,
by changing the unit of length, one can change k. Fortunately, as we shall see
shortly, k will not enter the ﬁnal deﬁnition of Hausdorﬀdimension.
Equation (31.29) can be solved for D,
D = lim
r→0

−ln N(r)
ln r
+ ln k
ln r

.
Now, we can see why k is not essential: As r →0 the denominator of the
second term grows beyond bound. So,
D = −lim
r→0
ln N(r)
ln r
(31.30)
Let us test (31.30) on some familiar geometric objects. If the object is a
single point on a line, then only one “box” is needed to cover it regardless of
the size of the box. So, N(r) = 1 for all r, and Equation (31.30) gives D = 0.
11By an attractor, we mean any geometrical object on which the trajectory hovers. It can
be a ﬁxed point, a limit cycle, or some multidimensional object in the phase (hyper-)space.

776
Nonlinear Dynamics and Chaos
In fact, the Hausdorﬀdimension of any ﬁnite number of points on a line is
found to be zero. Similarly, the dimension of a ﬁnite number of points on a
surface or in a volume is also zero.
If the object is a surface of area A, then we require A/r2 boxes (squares)
to cover the entire area. Thus,
D = −lim
r→0
ln(A/r2)
ln r
= −lim
r→0
ln A −2 ln r
ln r

= 2.
Similarly, the reader may check that the Hausdorﬀdimension for a curve is
1, and for a volume it is 3. So, the formula seems to be working for familiar
geometric objects.
Example 31.3.1. A not-so-familiar geometric object is the Cantor set: Take the
Cantor set
closed interval [0, 1]; remove its middle third; do the same with the remaining two
segments; continue the process ad inﬁnitum (Figure 31.14). What is left of the line
segment is the Cantor set, named after the German mathematician whose work on
set theory, controversial at the time, laid the foundation of modern formal mathe-
matics. Figure 31.14 should convince the reader that after n steps, 2n segments are
left and that the length of each segment is (1/3)n. Thus, denoting the size of the
box after n steps by rn, we have
rn = (1/3)n,
N(rn) = 2n.
Therefore,
D = −lim
r→0
ln N(r)
ln r
= −lim
n→∞
ln N(rn)
ln rn
= −lim
n→∞
ln(2n)
ln[(1/3)n]
= −lim
n→∞
n ln 2
n ln(1/3) = −
ln 2
ln(1/3) = ln 2
ln 3 = 0.6309 . . . .
(31.31)
So, the Cantor set is more than just a set of points (dimension zero) and less than a
line segment (dimension one). It is amusing to note—as the reader may verify—that
the length of the Cantor set is zero!
■
The Cantor set is only one example of geometrical objects whose dimen-
sions are nonintegers:
fractal object or
fractal
0
1
2
3
4
Figure 31.14: The Cantor set after one, two, three, and four “dissections.”

31.3 Universality of Chaos
777
Box 31.3.1. A geometrical object, whose Hausdorﬀdimension in not an
integer, is called a fractal object or simply a fractal.
Example 31.3.2. Another example of a fractal object is the so-called Koch
snowﬂake. Start with an equilateral triangle of side L [Figure 31.15(a)]; remove
Koch snowﬂake
the middle third of each side and replace it with two identical segments (a “wedge”)
to form a star [Figure 31.15(b)]. Do the same to the small segments so obtained
[Figure 31.15(c)], and continue ad inﬁnitum. The result is the Koch snowﬂake.
Let us ﬁnd the Hausdorﬀdimension of the Koch snowﬂake.12 It should be clear
A ﬁnite area
bounded by an
inﬁnite closed
curve!
that the number of line segments on each side of the triangle at step n is 4n so that
N(rn) = 3 × 4n, and the length rn of each line segment is L/3n. Therefore, the
Hausdorﬀdimension of the Koch snowﬂake is
D = −lim
n→∞
ln N(rn)
ln rn
= −lim
n→∞
ln(3 × 4n)
ln(L/3n)
= −lim
n→∞
ln 3 + n ln 4
ln L −n ln 3 = ln 4
ln 3 = 1.2618595 . . . .
(31.32)
The length of the perimeter of the snowﬂake is
lim
n→∞N(rn)rn = lim
n→∞(3 × 4n) (L/3n) = 3L lim
n→∞
 4
3
!n →∞.
It is interesting to note that the area enclosed by the Koch snowﬂake is ﬁnite while
its perimeter is inﬁnite!
■
The fractals discussed so far have the property which we called self-similarity.
The present case is, however, a true (or regular) self-similarity because, as we
scale the object, we obtain the exact replica of the original. In contrast, for
(a)
(b)
(c)
Figure 31.15: (a) Begin with an equilateral triangle. (b) Remove the middle third of
each side and replace it with a “wedge” to form a star. (c) Remove the middle third of
each new segment and replace them with “wedges.” Continue ad inﬁnitum to obtain
the Koch snowﬂake.
12This is the dimension of the perimeter, not the area of the snowﬂake.

778
Nonlinear Dynamics and Chaos
the logistic map, we obtained bifurcations which contained diﬀerent scaling
ratios: The ratio was α∗only for the largest bifurcation size at each stage.
Comparison of the largest size with smaller sizes would not have yielded α∗.
These (irregular) self-similarities occur frequently in chaotic systems and the
determination of their Hausdorﬀdimension can give information about the
long-term behavior of the dynamics of the system.
In the case of the logistic map, the Hausdorﬀdimension of the set of verti-
cal (ﬁxed) points on the bifurcation diagram—which is zero at all ﬁnite stages
of bifurcation—will not be zero at α∞. It has, in fact, been calculated to be
0.5388 . . .. This is an example of attractors that have noninteger dimensions,
i.e., they are neither points nor lines. If the attractor of a dissipative dy-
namical system has a fractal dimension, then we say that the system has a
strange attractor
strange attractor. Strange attractors play a fundamental role in the theory
of chaos.
31.4
Problems
31.1. Show that (31.1) leads to (31.2).
31.2. For the logistic map, assume that 1 < α < 3. Show that if xk > 1−1/α,
then xk+1 < xk, and if xk < 1 −1/α, then xk+1 > xk. Therefore, conclude
that x∗= 1 −1/α is a stable ﬁxed point.
31.3. Write the cubic polynomial in Equation (31.10) as
α3

x −1 + 1
α

(x2 + ax + b)
and determine a and b by expanding and comparing the result with (31.10).
Now solve x2 + ax + b = 0 to obtain x2(α) and x3(α) of (31.11).
31.4. Derive Equation (31.21) from the equation that precedes it.
31.5. Convince yourself that a system of N particles in space has a 6N-
dimensional phase space.
31.6. Consider a set of autonomous ﬁrst-order DEs. Suppose that a point P
of the phase space is a root of all functions on the RHS. By expanding each
coordinate of a trajectory in a Maclaurin series in t and keeping only the ﬁrst
two terms, show that the trajectory does not move away from P. So, ﬁxed
points are determined by setting all functions on the RHS of an autonomous
system equal to zero and solving for the coordinates.
31.7. Derive Equation (31.27) from (31.26).
31.8. Show that the dimension of a ﬁnite number of points on a surface or
in a volume is zero.

31.4 Problems
779
31.9. Show that the Hausdorﬀdimension of any ﬁnite number of points is
zero, of a curve is 1, and of a volume is 3.
31.10. Show that the Hausdorﬀdimension of the Cantor set is independent
of the length of the original line segment.
31.11. Verify that the length of the Cantor set is zero.

Chapter 32
Probability Theory
Although probability theory did not ﬂourish until after the Renaissance, and
in particular in the 17th and 18th centuries, its roots go back to ancient
history. Archaeological excavations reveal the presence of knuckle-bones (or
astragali) in numbers far larger than any other kind of bones, indicating the
possibility of the use of these bones in games. There is strong evidence that as-
tragali were in use for board games at the time of The First Dynasty in Egypt
(c.3500 B.C.). Other archaeological excavations, unearthing more recent pe-
riods, e.g.
1300 B.C. in Turkey, also reveal a deﬁnite connection between
astragalus and recreation.
It seems that games of chance, such as the board game mentioned above,
are, like counting, as old as civilization itself. Yet the science of counting,
arithmetic, was already in an advanced stage of evolution when probability
started to take root as a mathematical science in the 17th century. Why?
Perhaps the reason is the crudeness with which “randomizers” such as dice—
the artiﬁcial substitutes of astragali—were made for a long time. Abstraction
requires perfection. Although the abstraction of counting from what was being
counted took place naturally, the corresponding abstraction of randomness
from what is random demanded an ideal device capable of producing random
events, and a large number of experimental data for analysis, and this did not
happen until well into the 17th century.
32.1
Basic Concepts
The reader no doubt has some familiarity with the notion of a random event.
Random event:
basis for
probability
Any occurrence or experiment, whose outcome is uncertain is such an event.
Flipping a coin, pulling a card out of a deck of cards, and throwing a die
are all examples of experiments whose outcome are uncertain (if the coin, the
deck of cards, and the die are all “unbiased”). The reader may also know
intuitively that the chance of getting a head in the toss of a coin is 50% (or
1 out of 2, or 0.5); that the chance of getting a 3 in the throw of a die is 1

782
Probability Theory
out of 6; and that the chance of getting a club in drawing a card is 1 out of 4.
The aim of the theory of probability is to make precise these intuitive notions
and to develop a mathematical procedure for answering questions related to
random events. First we need to review some simple concepts from set theory.
32.1.1
A Set Theory Primer
The most fundamental entity in any branch of mathematics is a universal
set. It is the collection of all objects under consideration. For example, the
Universal set
universal set of plane geometry is a ﬂat surface, and of solid geometry is
the three-dimensional space. The universal set of calculus is the set of real
numbers (or the real line), and the complex plane is the universal set of
complex analysis. The generic universal set is denoted by S, but each speciﬁc
universal set has its own symbol: R is the set of real numbers, C is the set of
complex numbers, Z is the set of integers, and N is the set of natural numbers
(nonnegative integers).
The simplest relation in set theory is that of belonging. We write a ∈S
(and say “a belongs to S” or “a is in S” to express the fact that a is one of
the objects in S. An object in S is called an element of S. A collection A
Element and
subset of a set
of elements of S is called a subset of S, and we write A ⊂S. In particular,
S ⊂S. Any subset can be considered as a set with its elements and subsets.
Thus, a ∈A means that a is one of the elements of the subset A, a ̸∈A
means that a is not one of the elements A, and B ⊂A means that B consists
of elements, all of which belong to A. Subsets are often speciﬁed either by
enumeration or by some statement enclosed between a pair of curly brackets.
For example,
{0, 1, 2, 3, . . .},
{2, 4, 6, . . .},
{2n + 1|n ∈N},
{(x, x)|x ∈R},
(
−13.6
n2



n ∈N, n ̸= 0
)
.
The ﬁrst describes N; the second, the set of even numbers; the third, the set of
odd numbers; the fourth, the line y = x; and the ﬁfth, the energy levels of the
hydrogen atom in electron volt. Two subsets are equal if each is a subset of
the other. In other words, if A ⊂B and B ⊂A, then A = B. It is convenient
to introduce the empty set, a subset ∅of S, which has no element.
The subsets of a universal set have a rich mathematics which we can only
brieﬂy outline here. Given two sets1 A and B, we can form another set, called
the union of A and B and denoted by A ∪B, which consists of all elements
belonging to either A or B or both. Thus,
A ∪B = {x ∈S|x ∈A or x ∈B}.
1It is very common to delete the preﬁx ‘sub’ and refer to subsets of a universal set as
simply sets.

32.1 Basic Concepts
783
The intersection of A and B, denoted by A∩B, consists of all elements that
Union,
intersection, and
complement
belong to both A and B:
A ∩B = {x ∈S|x ∈A and x ∈B}.
The complement of a set A is the subset of S which contains all the elements
of S which are not in A. Denoting this set by Ac, we have
Ac = {x ∈S|x ̸∈A}.
The reader may easily verify that S = A∪Ac and ∅= A∩Ac. When A∩B = ∅,
we say that A and B are disjoint.
Disjoint sets
The operations of union and intersection are commutative and associative:
A ∪B = B ∪A,
A ∩B = B ∩A,
(A ∪B) ∪C = A ∪(B ∪C),
(A ∩B) ∩C = A ∩(B ∩C).
Thus one can take the union and intersection of a number of sets without wor-
rying about the order of the sets or where to put the parentheses. This makes
it possible to introduce the following notations for the union and intersection
of a family of sets:
nF
i=1
Ai ≡A1 ∪A2 ∪· · · ∪An,
nG
i=1
Ai ≡A1 ∩A2 ∩· · · ∩An.
(32.1)
We deﬁne the diﬀerence between two sets A−B ≡A∩Bc as the collection
of elements in A that are not in B. It is not hard to show that A −B, B −A,
and A ∩B are mutually disjoint. Furthermore,
A = (A −B) ∪(A ∩B),
B = (B −A) ∪(A ∩B),
(32.2)
A ∪B = (A −B) ∪(A ∩B) ∪(B −A).
Note that all sets on the right-hand side of each equation are mutually disjoint.
A useful way of picturing sets and operations on them is a Venn diagram.
Venn diagrams
The universal set is depicted as a rectangle, and its subsets as circles in the
rectangle. Figure 32.1 shows some examples of the use of Venn diagrams.
Venn diagrams are intuitive representations of relations among sets. For ex-
ample, the diagram on the right of Figure 32.1 shows clearly the equalities of
Equation (32.2).
Using Venn diagrams, one can show that the operation of union distributes
over intersection and vice versa:
A ∩(B ∪C) = (A ∩B) ∪(A ∩C),
A ∪(B ∩C) = (A ∪B) ∩(A ∪C),
(32.3)

784
Probability Theory
A
Ac
A∩B
A∪B
Figure 32.1: Venn diagrams of some sets. The grey region represents the set labeled
at the bottom.
and more generally,
A ∩
 nF
i=1
Bi

=
n
F
i=1
(A ∩Bi),
A ∪
 nG
i=1
Bi

=
n
G
i=1
(A ∪Bi).
(32.4)
32.1.2
Sample Space and Probability
The underlying concept in probability theory is the sample space, which is
the same as the universal set of the set theory and is also denoted by S. It is
the collection of all possible outcomes of an experiment. For example, for the
toss of a coin, S = {H, T }; for the toss of two coins, S = {HH, HT, TH, TT };
and for a die, S = {1, 2, 3, 4, 5, 6}. An event E is simply a subset of the
sample space.
Thus, the event {HT, T H} is described as the outcome in
the toss of two coins, in which one of the coins is head; and {2, 4, 6} is the
Event: elementary
and compound
event that the roll of a die produces an even number. An event, therefore,
can be elementary or compound, with the latter being a collection of the
former.
We are now ready to deﬁne probability. Since the sample space—which
Probability space
is sample space.
is now also called the probability space—includes all possible events, its
probability should be one, corresponding to absolute certainty. The probabil-
ity of any event (any subset of the probability space) has to be a nonnegative
number less than one. We may be tempted to say that the probability of
the union of two events is the sum of their probabilities, but that would be
wrong. For example, let S = {1, 2, 3, 4, 5, 6} be the universal set of a die,
and consider E1 = {1, 3, 5}, the odd outcomes, and E2 = {4, 5, 6}, all out-
comes greater than 3. Intuitively, we know that the probability for each of
these two events is 1
2. But E1 ∪E2 = {1, 3, 4, 5, 6}, and if we were to add
probabilities for the union, we would get that the probability of {1, 3, 4, 5, 6}
is one, which is clearly wrong.
The reason for this is that we have actu-
ally double-counted {5}, the intersection of the two sets.
Only if the two
sets are disjoint, can we add the probabilities for the union. Now we deﬁne
probability:

32.1 Basic Concepts
785
Box 32.1.1. S is called a probability space if for each event E ⊂S
there is a number P(E) satisfying the following conditions.
1. 0 ≤P(E).
2. P(S) = 1.
3. If E1 and E2 are disjoint events, then P(E1∪E2) = P(E1)+P(E2).
Example 32.1.1. In this example we derive some relations involving probabilities.
(1) If E1 ⊂E2, then P(E1) ≤P(E2). To show this, use the ﬁrst equation in (32.2)
and the fact that E1 ∩E2 = E1 to write
E2 = (E2 −E1) ∪E1
and
P(E2) = P(E2 −E1) + P(E1).
Since P(E2 −E1) is nonnegative, we get P(E2) ≥P(E1).
(2) P(E) ≤1 for every event E. This is a consequence of (1), because E ⊂S and
P(S) = 1.
(3) For any two events E1 and E2,
P(E1 ∪E2) = P(E1) + P(E2) −P(E1 ∩E2).
(32.5)
Use Equation (32.2) to write
P(E1) = P(E1 −E2) + P(E1 ∩E2),
P(E2) = P(E2 −E1) + P(E1 ∩E2),
(32.6)
P(E1 ∪E2) = P(E1 −E2) + P(E1 ∩E2) + P(E2 −E1).
Substituting P(E1 −E2) and P(E2 −E1) of the ﬁrst two equations in the last
equation, we obtain the desired result.
Using E as E1 and Ec as E2, and noting that E −Ec = E and E ∩Ec = ∅, the
ﬁrst (or second) equation in (32.6) gives P(E) = P(E) + P(∅), implying that
(4) P(∅) = 0.
Using E as E1 and Ec as E2 again, and noting that E −Ec = E and E ∪Ec = S,
the third equation in (32.6) and (4) give P(S) = P(E) + P(Ec), implying that
(5) P(Ec) = 1 −P(E).
■
Condition 3 of Box 32.1.1 can be generalized to the case of a collection of
mutually disjoint sets E1, E2, . . . , Em:
P(E1 ∪E2 ∪... ∪Em) = P(E1) + P(E2) + · · · + P(Em) =
m

i=1
P(Ei). (32.7)
A collection of mutually disjoint sets E1, E2, ..., Em with S = E1∪E2∪...∪Em
is called a partition of S. Such a collection has the property that
partition of
universal set
m

i=1
P(Ei) = 1.
(32.8)

786
Probability Theory
Up to now, we have not assigned any value to P(E) for a given set E, and
it cannot be done without some further assumptions concerning the physical
properties of the probability space and the events that make it up. In fact,
if E1, E2, ..., Em partition S, any set of nonnegative numbers p1, p2, ..., pm
adding up to 1 with P(Ei) = pi will satisfy the conditions of Box 32.1.1 and
will turn S into a probability space. Physically, however, certain choices will
not make sense. For instance, for S = {H, T }, the sample space of a single
coin, one can set P(H) = 0.75 and P(T ) = 0.25. However, this assignment
is not very useful for ordinary coins, and in practice gives false results. For a
probability space composed of elementary events, it is often natural to assign
equal probability values to the elementary events. Thus if the Ei of Equation
(32.8) are all elementary, then the natural assignment would be P(Ei) = 1/m
for i = 1, 2, . . . , m. For a coin, m = 2 and P(H) = P(T ) = 0.5 is a natural
choice, while for a die P(i) = 1/6, and for a deck of cards, P(Ei) = 1/52.
32.1.3
Conditional and Marginal Probabilities
In many situations, the sample space is partitioned in two diﬀerent ways.
For example, a deck of cards can be partitioned either by 4 suits or by 13
values; the employees of a company can be partitioned by gender or by de-
partments in which they work. Suppose E1, E2, . . . , Em and F1, F2, . . . , Fn
are two collections of events that partition S. It should be clear that Ei ∩Fj,
i = 1, 2, . . ., m; j = 1, 2, . . ., n is also a partition of S, and that
nF
j=1
(Ei ∩Fj) = Ei
and
m
F
i=1
(Ei ∩Fj) = Fj.
(32.9)
Since Ei, Fj, and Ei ∩Fj are all partitions of S, we can deﬁne the proba-
bilities P(Ei), P(Fj), and P(Ei ∩Fj). Then, Equation (32.9) implies that
P(Ei) =
n

j=1
P(Ei ∩Fj)
and
P(Fj) =
m

i=1
P(Ei ∩Fj).
(32.10)
P(Ei) and P(Fj) are called marginal probabilities.
Marginal and
conditional
probabilities
Associated with the marginal probability is the conditional probability.
Suppose we know that Ei has occurred. What is the probability of Fj? For
example, we draw a card from a deck of cards and somebody tells us that it is
a heart. What is the probability that it is a jack? This conditional probability
is denoted by P(Fj|Ei) and is the probability of Fj given that Ei has occurred.
Example 32.1.2. The best way to understand marginal and conditional proba-
bilities is to look at an example. Suppose that in a container, we have 100 marbles
coming in three diﬀerent sizes: small, medium, and large; and ﬁve diﬀerent colors:
white, black, red, green, and blue. Table 32.1 shows the distribution of the marbles
according to color and size.
First note that from the very deﬁnition of probability, the chance of getting a
medium red marble in a random drawing is 0.07, that of a large green marble is 0.03,

32.1 Basic Concepts
787
White
Black
Red
Green
Blue
Total
Small
5
7
6
8
4
30
Medium
8
10
7
12
8
45
Large
9
5
4
3
4
25
Total
22
22
17
23
16
100
Table 32.1: The distribution of marbles according to size and color.
and there is a likelihood of 0.05 for drawing a small white marble. Similarly the
probability that on a random drawing from the container, the ball is black is 0.22,
and for the ball to be medium it is 0.45. This suggests the construction of another
table, Table 32.2, which shows the distribution of the probabilities according color
and size.
Each entry of the last row and last column of Table 32.2 is what we have called a
marginal probability. The conditional probability that the marble is small given that
its color is white is 5/22. This is because, by restricting the color to white, we limit
the number of marbles to 22 rather than 100. Similarly, the probability that the
marble is green given that its size is medium is 12/45; this also is a conditional prob-
ability. Conditional probabilities can be rewritten as ratios of probabilities. Thus,
the probability that the marble is small given that its color is white is 0.05/0.22,
and the second probability is 0.12/0.45.
■
The results of the foregoing example can be easily generalized. Let pij =
P(Ei ∩Fj), construct a table with m rows and n columns, and ﬁll the cells
with the numbers pij. Add one more row for the totals with entries P(F1),
P(F2), all the way to P(Fn). Add one more column for the totals with entries
P(E1), P(E2), all the way to P(Em). It should now be clear that P(Fj|Ei),
the probability of Fj given that Ei has occurred, is
P(Fj|Ei) = P(Ei ∩Fj)
P(Ei)
.
(32.11)
Since any event and its complement partition the universal set, we can let
F1 = A and F2 = Ac (only two F’s), and write the equation above as
P(A|Ei) = P(Ei ∩A)
P(Ei)
,
(32.12)
White
Black
Red
Green
Blue
Total
Small
0.05
0.07
0.06
0.08
0.04
0.3
Medium
0.08
0.1
0.07
0.12
0.08
0.45
Large
0.09
0.05
0.04
0.03
0.04
0.25
Total
0.22
0.22
0.17
0.23
0.16
1
Table 32.2: The distribution of probabilities according to size and color.

788
Probability Theory
or if we have two sets A and B and their complements as two partitions of S,
then
P(A|B) = P(B ∩A)
P(B)
,
(32.13)
and this is true for any two sets.
If the probability P(A|B) does not depend on the event B in any way, i.e.,
if P(A|B) = P(A), then we say that the two events A and B are statistically
independent. Equation (32.13) now yields
Statistically
independent
events
P(A) = P(B ∩A)
P(B)
or
P(A ∩B) = P(A)P(B),
(32.14)
and the second equation becomes the deﬁnition for two events to be statisti-
cally independent.
It is important to diﬀerentiate between statistical independence and mu-
Diﬀerence
between statistical
independence and
mutual exclusion
tual exclusion.
If two events are mutually exclusive then they have to be
statistically dependent since the occurrence of one precludes the occurrence
of the other. Similarly, Equation (32.14) shows that if P(A) > 0, P(B) > 0,
and A and B are statistically independent, then P(A ∩B) ̸= 0, implying that
A ∩B ̸= ∅and, therefore, that A and B cannot be mutually exclusive.
Equation (32.12) could be rewritten as
P(A ∩Ei) = P(Ei)P(A|Ei),
and since A∩Ei are mutually exclusive and their union is A, we have [see the
second equation in (32.10) with A = Fj]
P(A) =
m

i=1
P(Ei)P(A|Ei).
(32.15)
This is called Bayes’ theorem.
Bayes’ theorem
Example 32.1.3. A selective four-year college admits mostly students whose ACT
scores are 32 and higher, with a small number of admitted students whose scores are
below 32. The college has a graduation rate of 97%. Of those who graduate, 98%
have an ACT score of 32 and higher. Of those who drop out, 85% have an ACT
score below 32. We want to calculate the probability of graduation for a student
who has an ACT score below 32.
Let E1 and E2 denote the events corresponding, respectively to graduating and
dropping out. Let F1 and F2 denote the events corresponding to an ACT score of
32 or higher and lower than 32, respectively. We are after P(E1|F2).
Consider the following table, in which the most obvious probabilities are entered:
F1
F2
Total
E1
0.97
E2
0.03
Total
1

32.1 Basic Concepts
789
Since we are given that P(E1) = 0.97 and P(F1|E1) = 0.98, we can use Equation
(32.11) to ﬁnd the entry, p11, in the ﬁrst row and ﬁrst column:
p11 = P(F1 ∩E1) = P(E1 ∩F1) = P(F1|E1)P(E1) = 0.98 × 0.97 = 0.9506.
The entry, p12, in the ﬁrst row and second column can now be calculated because
the total is given as 0.97:
p12 = P(F2 ∩E1) = 0.97 −0.9506 = 0.0194.
The table now looks like
F1
F2
Total
E1
0.9506
0.0194
0.97
E2
0.03
Total
1
We are also given that P(F2|E2) = 0.85. So, using Equation (32.11) again, we
can ﬁnd p22:
p22 = P(F2 ∩E2) = P(F2|E2)P(E2) = 0.85 × 0.03 = 0.0255.
The remaining entries are now trivial to calculate:
p21 = P(F1 ∩E2) = 0.03 −0.0255 = 0.0045,
P(F1) = 0.9506 + 0.0045 = 0.9551,
P(F2) = 0.0194 + 0.0255 = 0.0449,
and the complete table becomes
F1
F2
Total
E1
0.9506
0.0194
0.97
E2
0.0045
0.0255
0.03
Total
0.9551
0.0449
1
The desired probability is therefore,
P(E1|F2) = P(E1 ∩F2)
P(F2)
= 0.0194
0.0449 = 0.432.
So, there is almost a 43% chance for the graduation of a student whose ACT score
is below 32.
■
32.1.4
Average and Standard Deviation
When we are given a set of values—say the scores of students in a class—and
asked to ﬁnd the average, we add the values and divide by the total number
of values. If {xi}N
i=1 is the set of values, then the average ¯x is given by
¯x =
N
i=1 xi
N
=
N

i=1

xi
1
N

.

790
Probability Theory
This equation tacitly assumes that the probability is the same for all values
and equal to 1/N. If the probability depends on i, the deﬁnition of the average
has to take this into account. Let pi denote the probability for the occurrence
of xi, and change the notation for the average to the more common notation
whereby capital letters are used inside angle brackets. Then the average or
mean or expectation value of {xi}N
i=1 is deﬁned as
Average,
expectation value,
mean
⟨X⟩=
N

i=1
xipi.
(32.16)
Another quantity of interest is the standard deviation, which is a mea-
Standard deviation
sure of how the values are spread from the mean. It is the average “distance”
between ¯x and xi. The obvious choice xi −¯x will have a zero average, because
it is both positive and negative and the deﬁnition of ¯x makes the positive and
negative values cancel. To avoid this cancellation, one takes the square of
these diﬀerences and then averages them. The variance σ2 is deﬁned by
Variance
σ2 =
N
i=1(xi −¯x)2
N
,
and the standard deviation by
σ =
"N
i=1(xi −¯x)2
N
.
(32.17)
When probability varies with xi, the deﬁnition of the variance changes to
σ2 =
N

i=1
(xi −⟨X⟩)2pi.
(32.18)
In many situations one may be interested in the average of a quantity that
depends on the random variable xi. Thus, if g(xi) is such a quantity, one
writes
⟨g(X)⟩=
N

i=1
g(xi)pi.
(32.19)
In terms of such averages, one can show that
σ2 = ⟨X2⟩−⟨X⟩2.
(32.20)
Related to averages is the moment generating function deﬁned by
Moment
generating
function
⟨etX⟩=
N

i=1
etxipi.
(32.21)
The name comes from the fact that
dk
dtk ⟨etX⟩




t=0
= ⟨Xk⟩.
(32.22)

32.1 Basic Concepts
791
32.1.5
Counting: Permutations and Combinations
The probability space of many situations is discrete. In fact, one can say
that all probability spaces are discrete, and only in the limit of large samples
(atoms and molecules in thermodynamics, for example) can one approximate
the random variable as a continuous variable. Therefore, it is important to
ﬁnd formulas that give the number of particular events of a universal set.
Suppose you have N distinguishable particles and you want to place
them in M bins. There are two cases that are used in practice: each bin
can hold as many particles as you place in it; or each bin can hold only one
particle. For each case, we are interested in ﬁnding the number of distinct
arrangements, or the number of conﬁgurations. Let this number be denoted
by Ω(N, M).
If there is no restriction on the occupancy number, then you have M
choices for the ﬁrst particle, M choices for the second particle, etc. Therefore,
ΩMB(N, M) = M N.
(32.23)
In statistical mechanics, this is called the Maxwell-Boltzmann statistics.
Maxwell-
Boltzmann
statistics
If the occupancy number is one, then you have M choices for the ﬁrst
particle, M −1 choices for the second particle, etc. Therefore,
Ωp(N, M) = M(M −1)(M −2) · · · (M −N + 1) =
M!
(M −N)!,
M > N.
(32.24)
This is called the permutation of M objects taken N at a time. If M = N,
Permutation
then Ω(N, N) = N! is simply called the permutation of N objects.
The elementary constituents of nature are indistinguishable or iden-
tical. How does this aﬀect the formulas above? Let’s consider the single-
occupancy case ﬁrst because it is easier. Equation (32.24) is overcounting the
arrangement by N! because a permutation of the particles does not give any
new arrangement. Therefore,
Ωb(N, M) =
M!
N!(M −N)! ≡

M
N

,
M > N.
(32.25)
In statistical mechanics, this is called the Fermi-Dirac statistics. It is also
Fermi-Dirac
statistics
called the combination of M objects taken N at a time.
The multiple-occupancy case for indistinguishable particles is harder, but
there is a trick that can make it easier to derive the formula. Figure 32.2(a)
shows some bins with particles inside them. We can represent the arrangement
by placing the particles outside and to the right of the bins and represent the
bins by vertical lines as in Figure 32.2(b). Each vertical line has some particles
to its right and left except the bin on the extreme left, which has particles only
to its right. Since there is no limitation on the number of occupancy, the
number of arrangements can be calculated by permuting both the lines and
dots except the line on the extreme left.
Since the dots are identical (as
are the lines), the problem reduces to ﬁnding the number of permutations of

792
Probability Theory
(b)
(a)
Figure 32.2: (a) The bins with particles inside them.
(b) Bins are represented by
vertical lines with the occupying particles to their right.
N + M −1 objects N of which are identical and M −1 of which are also
identical, but diﬀerent from the other N. Therefore,
ΩBE(N, M) = (N + M −1)!
N!(M −1)!
=
N + M −1
N

.
(32.26)
In statistical mechanics, this is called the Bose-Einstein statistics.
Bose-Einstein
statistics
32.2
Binomial Probability Distribution
The Fermi-Dirac statistics is closely related to the so-called binomial distri-
bution. Each of the M bins has two states: either it is occupied or empty.
There are many situations where the binomial distribution applies. For exam-
ple, in tossing n coins, each coin can be a head or a tail; a quantum mechanical
spin-half particle can have its spin “up” or “down;” in a binary alloy system
each site of the alloy can be occupied by an atom A or B.
In fact, the binomial distribution is more general than this. In any trial,
Universality of
binomial
distribution
one can talk about success and failure, where success refers to one particular
outcome (out of the many possible outcomes), and failure to the rest of the
possible outcomes. Thus, if we are after a 6 in a toss of a die, then getting a
6 is a success, and getting 1, 2, 3, 4, or 5 is a failure.
Let p be the success probability, then the failure probability is q = 1 −p.
What is the probability P(m, n) that in n trials we have m successes? Be-
cause the events are statistically independent (what happens in each trial is
independent of what has happened and what will happen), by (32.14), the
probabilities multiply. Thus the probability of m successes and n−m failures
is pmqn−m; and since there are
n
m

ways that this can happen in n trials,
the probability P(m, n) of m successes in n trials is
P(m, n) =

n
m

pmqn−m =
n!
m!(n −m)!pmqn−m.
(32.27)
Using the Stirling approximation x! ≈
√
2π e−xxx+1/2 of Equation (11.6), the
reader can show that
P(m, n) ≈2n

2
nπ e−(n−2m)2/2n pmqn−m,
(32.28)
assuming that both m and n −m are large, which is true in almost all cases
of large systems.

32.2 Binomial Probability Distribution
793
The special case of p = q = 1
2 is of importance:
P(m, n) =
n!
m!(n −m)!2n ≈

2
nπ e−(n−2m)2/2n.
(32.29)
Sometimes (32.28) is written in terms of the diﬀerence s between the number
of successes and failures. This is conveniently equal to 2m −n which is the
exponent of the exponential. We call s the success excess. Thus, (32.28)
becomes
P(s, n) ≈

2
nπ e−s2/2n,
Ωb(s, n) = 2nP(s, n) = 2n

2
nπ e−s2/2n,
(32.30)
where Ωb is the number of conﬁgurations now written in terms of s.
For the binomial distribution we can easily ﬁnd the moment generating
function. From its deﬁnition, we have
⟨etX⟩=
n

x=0
etxP(x, n) =
n

x=0
etx
n
x

pxqn−x
=
n

x=0

n
x

(pet)xqn−x = (pet + q)n,
(32.31)
the last equality following from the binomial theorem. Equation (32.31) allows
us to easily calculate the average and variance for the binomial distribution.
First note that
d
dt⟨etX⟩= npet(pet + q)n−1,
d2
dt2 ⟨etX⟩= npet(q + npet)(pet + q)n−2.
Now evaluate these at t = 0—and note that p + q = 1—to obtain
⟨X⟩= np,
⟨X2⟩= n2p2 + npq,
σ2 = ⟨X2⟩−⟨X⟩2 = npq.
(32.32)
Example 32.2.1. Assume that the probability at birth that the newborn is male
(or female) is 1
2. What is the probability that in a household of six, three are male?
Blind intuition tells us that the probability is 1
2; but that is wrong! Rephrasing the
question to “What is the probability that in six trials we get three successes?” leads
us to the binomial distribution and the following answer:
P(3, 6) =
6!
3!3!
1
2
3 1
2
6−3
=
6!
3!3!
1
2
6
= 0.3125.
This result may be surprising, but even more surprising is the result we obtain
if we ask the same question about a (small) school: “What is the probability that
in a school with 200 pupils, 100 are male?”
P(100, 200) =
200!
100!100!
 1
2
100 1
2
200−100
=
200!
100!100!
 1
2
200
= 0.056.
■

794
Probability Theory
The surprise encountered in the preceding example is due to the confusion
caused by mixing the expected value with its probability. In a binomial distri-
bution, the expected value (or the mean or average) ⟨X⟩= np, or ⟨X⟩= n/2
when p = 0.5, which is the answer we intuitively gave to the two questions in
the example above. Since our surprise increases with n, let us investigate the
behavior of the binomial distribution for values of m close to the mean for
very large n.
For large n and m, we can use (32.29), from which we obtain
P(n/2, n) ≈

2
nπ .
This shows that P(n/2, n) →0 as n →∞. Thus the probability of having
n/2 successes in n trials becomes negligible as the number of trials increases.
But this is the maximum probability! Therefore, any other probability goes
to zero even faster. Where have all the probabilities gone?
Consider the graph of (32.29) for large n and plot it to a scale such that
the peak of the maximum, although small, is conspicuous. Figure 32.3 shows
such a graph for n = 1000. Note that the maximum probability has a value of
only 0.025, and that the graph drops to a value that is indistinguishable from
zero at about m = 560 on the right and m = 440 on the left. We can actually
calculate the ratio r of the small probability at m = 560 to the maximum at
m = 500 using (32.29):
r ≡P(560, 1000)
P(500, 1000) =

2
1000π e−(−120)2/2000

2
1000π
= e−(−120)2/2000 = 0.00075.
460
480
500
520
540
560
0.005
0.010
0.015
0.020
0.025
Figure 32.3: The plot of the binomial probability distribution for n = 1000.

32.2 Binomial Probability Distribution
795
This same ratio is obtained if we use m = 440, and we can therefore conclude
that the nonzero probabilities are essentially concentrated between m−= 440
and m+ = 560 for n = 1000.
Now let’s turn to a general n and ﬁnd the corresponding values of m−and
m+. These are the values of m at which the probability drops to 0.00075 of
its maximum value. To ﬁnd m±, we have to solve the equation
r ≡P(m, n)
P(n/2, n) =

2
nπe−(n−2m)2/2n

2
nπ
= e−(n−2m)2/2n = 0.00075.
The answer is
m± = 1
2(n ± 3.8√n),
(32.33)
as the reader may verify. So for a general n, a large fraction of the total
probability is concentrated between 1
2(n −3.8√n) and 1
2(n + 3.8√n). But
how much? What is the probability that the number of successes lies between
m−and m+?
To answer this question, we have to add all probabilities between m−and
m+. For large numbers, one can replace the sum with an integral:
P(m−≤m ≤m+, n) =
# m+
m−

2
nπ e−(n−2m)2/2ndm.
Deﬁne a new variable of integration x so that the exponent of the integrand
becomes −x2. This means
n −2m
√
2n
= x,
or
m = n −
√
2n x
2
,
dm = −
√
2n
2
dx,
and the integral in terms of x becomes
1
√π
# 3.8/
√
2
−3.8/
√
2
e−x2dx = 0.999855,
with the last result obtained by numerical integration. Therefore,
P(m−≤m ≤m+, n) = 0.999855,
(32.34)
which is interestingly independent of n.
Let us investigate the meaning of this result. It says that for very large n,
99.99% of the time the successes lie between m−and m+, and the probability
of not getting a success between m−and m+ is only 0.0145%. Note also
that when n gets large, m−and m+ become very nearly equal to n/2. For
example, if n = 109, then
n
2 = 5 × 108
and
m+ = 1
2(109 + 3.8
√
109) = 5.001 × 108.

796
Probability Theory
So the probabilities are concentrated in a very narrow interval; i.e., the prob-
ability curve is extremely sharp.
Going back to the probability of the male gender, we note that in a very
populous country such as China or India with approximately 109 inhabitants,
although the probability that exactly half the population is male is extremely
small, and of the order of

2
nπ =

2
109π = 0.000025,
the probability of the male population deviating too far from half is also
small. So although the exact success (half male) is highly unlikely, a number
of successes very close to exact is almost certain.
Example 32.2.2. An isolated spin- 1
2 particle has equal probability of being in
either spin-up or spin-down states. If there are n such particles, then the probability
of m of them being in the up state is given by (32.29) or (32.30).
When a spin- 1
2 particle with magnetic moment μ is placed in a magnetic ﬁeld
B, it has two possible states: in the direction of the ﬁeld (called up) and opposite
to it (called down). In the ﬁrst case the energy of the particle is −μB and in the
second case +μB. The energy of the system is therefore determined by the success
excess s, which in the present context is called the spin excess.
Now suppose that you have two systems that can exchange energy between
themselves with the combined system isolated. This means that the total energy
of the system is conserved. This energy is determined by the total spin excess s.
Let Ω1b(s1, n1) be the conﬁguration number of the ﬁrst system and Ω2b(s2, n2) for
the second. Let Ωb(s, n) be the number of conﬁgurations for the combined system,
where n = n1 + n2 and s = s1 + s2 is a constant. Since the total conﬁguration
number is the product of the conﬁguration number of the components, we have
Ωb(s, n) = Ω1b(s1, n1)Ω2b(s −s1, n2) = C exp

−s2
1
2n1 −(s −s1)2
2n2

,
(32.35)
where C is independent of s1.
What is the equilibrium state of the system? This corresponds to the most prob-
able state of the combined system, i.e., the state that maximizes Ωb(s, n). Instead
of maximizing Ωb, let’s maximize its logarithm, which is
ln Ωb = ln C −s2
1
2n1 −(s −s1)2
2n2
.
Diﬀerentiating with respect to s1, we get
∂ln Ωb
∂s1
= −s1
n1 + s −s1
n2
.
(32.36)
Note that the second derivative is
−
 1
n1 + 1
n2

,

32.3 Poisson Distribution
797
which is negative, so the extremum is a maximum. Setting (32.36) equal to zero
yields the most probable conﬁguration:
Condition for
thermal
equilibrium
ˆs1
n1 = s −ˆs1
n2
= ˆs2
n2 = s
n,
where the last equality follows from the previous ones (see Problem 32.13) and a
caret on a quantity indicates its value at maximum. If we substitute these in (32.35),
we get the maximum number of conﬁgurations:
Ωmax
b
(s, n) = C exp

−ˆs2
1
2n1 −ˆs2
2
2n2

= Ce−s2/2n.
(32.37)
The veriﬁcation of the last equality is the subject of Problem 32.14.
Once in equilibrium, how likely is it for the system to move away from it? To
investigate this, let s1 and s2 be slightly diﬀerent from their equilibrium values
s1 = ˆs1 + δ,
s2 = s −s1 = s −ˆs1 −δ = ˆs2 −δ.
Substituting these in (32.35) yields
C exp

−ˆs2
1 + 2ˆs1δ + δ2
2n1
−ˆs2
2 −2ˆs2δ + δ2
2n2

= Ωmax
b
(s, n) exp

−2ˆs1δ + δ2
2n1
+ 2ˆs2δ −δ2
2n2

.
But ˆs1/n1 = ˆs2/n2. Therefore,
Ω1b(ˆs1 + δ, n1)Ω2b(ˆs2 −δ, n2)
Ωmax
b
(s, n)
= exp

−δ2
2n1 −δ2
2n2

.
(32.38)
As a realistic numerical example, let n1 = n2 = 1023 and δ = 1013 so that the
fractional deviation δ/n1 = 10−10, a very small number. Then, the ratio in (32.38)
is e−1000 = 5 × 10−435. The probability for fractional deviations larger that 10−10
is smaller than e−1000. Assuming equal probability and adding the terms (about n1
How likely is it for
the system to
abandon its
equilibrium state?
of them), the upper bound for the total probability becomes n1e−1000 or 5 × 10−412
times the probability of ﬁnding the system in equilibrium.
What is the meaning behind the statement “the probability to ﬁnd the sys-
tem with a fractional deviation larger than 10−10 is 5 × 10−412 of the probabil-
ity of ﬁnding the system in equilibrium?” To have a reasonable chance of ﬁnding
the system in such a deviated state, we have to sample 5 × 10412 similar systems.
Even if we could sample at the rate of 1012 systems per second, we would have
to sample for 5 × 10400 seconds, or over 10393 years, or 10383 times the age of the
universe! Therefore, it is safe to say that deviations described above will never be
observed.
■
32.3
Poisson Distribution
Poisson processes are famous results in the probability theory.
A Poisson
process occurs in circumstances under which an event is repeated at a constant
rate of probability. Suppose that dt is so small that the probability of the
occurrence of two or more successes is negligible. Then the probability P1(dt)
of one success in dt is νdt, where ν is a constant.

798
Probability Theory
We are interested in Pn(t), the probability of n successes in a time interval
t. We can obtain a recursive diﬀerential equation involving Pn(t) and Pn−1(t),
which we hope to solve to get Pn(t). Consider Pn(t+dt), the probability that
n successes occur in time t+dt. This can be written as the sum of two disjoint
probabilities, each consisting of the product of two independent probabilities:
(a) n successes occur in time t and none in time dt, (b) n −1 successes occur
in time t and one in time dt. In symbols,
Pn(t + dt) = Pn(t)P0(dt) + Pn−1(t)P1(dt).
But P1(dt) = νdt and P0(dt) = 1 −P1(dt) = 1 −νdt. Therefore,
Pn(t + dt) = Pn(t)(1 −νdt) + Pn−1(t)νdt.
Expanding the left-hand side as Pn(t + dt) = Pn(t) + dPn
dt dt and dividing
both sides by dt, we obtain the desired recursive DE:
dPn(t)
dt
+ νPn(t) = νPn−1(t).
(32.39)
For n = 0 the right-hand side is zero and the DE
dP0(t)
dt
+ νP0(t) = 0
has the solution P0(t) = Ae−νt. The fact that the probability of no success
in zero time interval is 1 yields A = 1.
Equation (32.39) is a ﬁrst order DE which can be solved. In fact, the
solution is given in Theorem 23.3.1, where in the case at hand
μ(t) = exp
#
νdt

= eνt
and
Pn(t) = 1
eνt

C + ν
# t
0
eνt1Pn−1(t1) dt1

.
We must have Pn(0) = 0 because there is no chance that n successes can
be achieved in zero time interval. This sets C = 0, and we get the integral
recursion relation:
Pn(t) = νe−νt
# t
0
eνt1Pn−1(t1) dt1.
(32.40)
Substitute for Pn−1(t1) as an integral of Pn−2 to get
Pn(t) = νe−νt
# t
0
eνt1
(
νe−νt1
# t1
0
eνt2Pn−2(t2) dt2
)
dt1,
or
Pn(t) = ν2e−νt
# t
0
# t1
0
eνt2Pn−2(t2) dt2 dt1.

32.3 Poisson Distribution
799
It should now be clear that if we repeat this k times, we get
Pn(t) = νke−νt
# t
0
# t1
0
· · ·
# tk−2
0
# tk−1
0
eνtkPn−k(tk) dtkdtk−1 · · · dt1.
In particular when k = n,
Pn(t) = νne−νt
# t
0
# t1
0
· · ·
# tn−2
0
# tn−1
0
eνtnP0(tn) dtndtn−1 · · · dt1.
But P0(t) = e−νt so P0(tn) = e−νtn, and the above equation becomes
Pn(t) = νne−νt
# t
0
# t1
0
· · ·
# tn−2
0
# tn−1
0
dtndtn−1 · · · dt1.
(32.41)
Starting with the innermost integral over tn and integrating all the t’s, the
reader can show that the result will be tn/n!.
We thus ﬁnally obtain the
Poisson process
Pn(t) = (νt)n
n!
e−νt.
(32.42)
Poisson process is naturally a time-dependnent process and ν is the rate or
the frequency of that process.
The discrete Poisson distribution p(n) is deﬁned by setting νt = λ to
obtain
Poisson probability
distribution
p(n) = λn
n! e−λ,
n = 0, 1, 2, . . ., ∞.
(32.43)
The moment generating function is
⟨etX⟩=
∞

x=0
etx λx
x! e−λ = e−λ
∞

x=0
(λet)x
x!
= eλ(et−1).
(32.44)
This gives
d
dt⟨etX⟩= λeteλ(et−1),
d2
dt2 ⟨etX⟩= λeteλ(et−1)(λet + 1).
Evaluating these at t = 0 yields
⟨X⟩= λ,
⟨X2⟩= λ(λ + 1),
σ2 = ⟨X2⟩−⟨X⟩2 = λ.
(32.45)
Example 32.3.1. A city had 24 major ﬁre accidents in a year.
What is the
probability that there will be (a) one major ﬁre next month, (b) at least 5 major
ﬁres in the next 6 months?
Here ν, the frequency of ﬁre is 24 per year or 2 per month. So for (a) we have
λ = 2 × 1 = 2 and
p(1) = λe−λ = 2e−2 = 0.27.
For (b), λ = 2 × 6 = 12 and
p(n ≥5) = 1 −p(0) −p(1) −p(2) −p(3) −p(4)
= 1 −e−12 −12e−12 −122
2! e−12 −123
3! e−12 −124
4! e−12 = 0.992.
■

800
Probability Theory
Example 32.3.2. In a department store, 39 light bulbs burn out per year. The
light bulbs are replaced from the store stock, which is replenished every week. What
is the minimum number of light bulbs the stock should hold so that the store will
have all its light bulbs working with a probability of at least 99%?
The frequency is ν = 39/52 = 0.75 per week. Thus, λ = 0.75 × 1 = 0.75. Let
n stand for the number of bulbs burnt per week and m the number of bulbs in the
stock. Then the store will have all its lights on as long as n ≤m. Therefore, we
want p(n ≤m) ≥0.99. This gives
p(n ≤m) =
m

n=0
λn
n! e−λ ≥0.99,
or
p(n ≤m) =
m

n=0
(0.75)n
n!
≥0.99e0.75 = 2.096,
or

1 + 0.75 + 0.752
2!
+ · · · + 0.75m
m!

≥2.096.
By trial and error, the reader can verify that m = 3.
■
Poisson distribution is the limiting case of the binomial distribution when
n →∞, p →0, and λ = np is constant. To see this, expand n!/(n−m)! using
the Stirling approximation:
n!
(n −m)! ≈
√
2π e−nnn+1/2
√
2π e−n+m(n −m)n−m+1/2 = e−m

n
n −m
n+1/2
(n −m)m.
Now note that

n
n −m
n+1/2
≈

n
n −m
n
=
1
(1 −m/n)n →
1
e−m = em,
and (n −m)m ≈nm. Furthermore,
qn−m = (1 −p)n−m =

1 −λ
n
n−m
≈

1 −λ
n
n
→e−λ.
Substituting all this in (32.27) yields
P(m, n) →nm 1
m!pme−λ = (np)m
m!
e−λ = λm
m! e−λ,
which is the Poisson distribution p(m).
Example 32.3.3. A 3000-letter long message has been transmitted electronically
with an error probability of 10−3. What is the probability that there are at least
two errors in the message?
This is a binomial distribution (error is success!) with small probability and large
n. Therefore, we can use Poisson distribution (32.43) with λ = np = 3000×10−3 = 3.
Then
p(n ≥2) = 1 −p(0) −p(1) = 1 −e−3 −3e−3 = 0.8.
The probability that there is exactly one error in the message is
p(1) = 3e−3 = 0.149.
■

32.4 Continuous Random Variable
801
32.4
Continuous Random Variable
Most probability sample spaces are so large that approximating the discrete
events with continuous variables becomes very useful and accurate. Take the
case of the binomial distribution discussed above. We started with discrete
counting, but when our sample grows to 1023, not only does the discrete sum
become unmanageable, it becomes unnecessary as well. This is also reﬂected
in the replacement of the strictly discrete factorial with the more adaptable
exponential function through the use of the Stirling approximation.
When continuous variables are used, probability is described by proba-
bility density. In the case of a single random variable x, the probability
Probability density
density f(x) is used to give the probability that x lies in an interval of length
dx:
P(x −dx/2 < x < x + dx/2) = f(x) dx,
# b
a
f(x) dx = 1,
where (a, b) is the interval for which x is deﬁned. This interval can be taken
to be (−∞, ∞) by assigning zero probability density to points on the left of
a and on the right of b. The integral describes the total probability, which is
1 as in the case of the discrete variable.
For more variables, the generalization is clear. If x = (x1, x2, . . . xm) and
the probability density function is f(x), then the probability that x is in an
inﬁnitesimal m-dimensional volume dmx is
P(x ∈dmx) = f(x) dmx,
##
Ω
f(x) dmx = 1,
(32.46)
where Ω is the region for which f(x) is deﬁned. If V is a subset of Ω, then
P(x ∈V ) =
##
V
f(x) dmx
gives the probability that x lies in V .
For example, a quantum mechanical wave function Ψ(r) gives rise to a
density f(r) = |Ψ(r)|2, and all wave functions are normalized so that
##
Ω
|Ψ(r)|2 d3x = 1
and
P(r ∈V ) =
##
V
|Ψ(r)|2 d3x,
(32.47)
where r is a set of convenient coordinates.
The average and variance is deﬁned in exactly the same way. For example,
the average of the ith component of x, denoted by ⟨Xi⟩, is given by
⟨Xi⟩=
##
Ω
xif(x) dmx.
(32.48)
Similarly
σ2(Xi) =
##
Ω
(xi −⟨Xi⟩)2f(x) dmx,
(32.49)

802
Probability Theory
and
⟨g(X)⟩=
##
Ω
g(x)f(x) dmx.
(32.50)
Example 32.4.1. The ground state of the hydrogen atom is described in spherical
coordinates by the wave function Ae−r/a0, where a0 is the Bohr radius, A is a
constant to be determined by the normalization (32.47), and r is the distance of the
electron to the nucleus, which is placed at the origin. Using the volume element in
spherical coordinates, we have
1 = A2
# ∞
0
# π
0
# 2π
0
e−2r/a0r2 sin θ dr dθdϕ = 4πA2
# ∞
0
e−2r/a0r2 dr



a3
0/4
= πA2a3
0,
giving A =
	
1/πa3
0. Thus the normalized wave function for the ground state of the
hydrogen atom is
Ψ(r, θ, ϕ) =
"
1
πa3
0
e−r/a0,
with |Ψ(r, θ, ϕ)|2 being the probability density of ﬁnding the electron at (r, θ, ϕ).
From this, we can calculate, for instance, the probability that the electron ap-
proaches the nucleus to within 10% of the Bohr radius. The second equation in
(32.47) gives the answer where V is the volume of a sphere with radius 0.1a0.
Therefore,
P(r ∈V ) =
1
πa3
0
# 0.1a0
0
# π
0
# 2π
0
e−2r/a0r2 sin θdr dθ dϕ
= 4
a3
0
# 0.1a0
0
e−2r/a0r2dr = 1
2
# 0.2
0
e−xx2dx ≈0.0013,
where we used the change of variables r = a0x/2 in the last integral to turn it into
a numerical factor.
We can also calculate some averages. For instance, the average for the x coor-
dinate of the electron is (remember that x = r sin θ cos ϕ)
⟨X⟩=
1
πa3
0
# ∞
0
# π
0
# 2π
0
r sin θ cos ϕ e−2r/a0r2 sin θdr dθ dϕ = 0.
The result of zero being due to the ϕ integration. Similarly, ⟨Y ⟩and ⟨Z⟩also vanish.
This null result should be expected because it is just as likely for the electron to
have a positive x value as it is to have a negative value. On the other hand, r is
always positive, and we expect its average value to be nonzero. In fact,
⟨R⟩=
1
πa3
0
# ∞
0
# π
0
# 2π
0
r e−2r/a0r2 sin θdr dθ dϕ = 4
a3
0
# ∞
0
e−2r/a0r3 dr



=3a4
0/8
= 3
2a0.
■
A random variable xα is said to be independent of the rest of the variables
Independent
random variable
if the probability density f(x) factors out into
f(x) = g(xα)h(x1, x2, . . . , xα−1, xα+1, . . . , xm) ≡g(xα)hα(x),

32.4 Continuous Random Variable
803
where hα(x) is a function of all x’s except xα. By multiplying it by a constant,
we can always choose g(xα) in such a way that
# ∞
−∞
g(xα) dxα = 1.
(32.51)
Then since
1 =
##
Ω
f(x) dmx =
# ∞
−∞
g(xα) dxα
##
Ω′ hα(x) dm−1x,
where Ω′ is the region of integration of the remaining variables, we also have
##
Ω′ hα(x) dm−1x = 1.
From this we conclude that the average of any function depending on xα alone
can be calculated using not the whole density f(x), but g(xα). In particular,
⟨Xα⟩=
# ∞
−∞
xαg(xα) dxα,
σ2(Xα) =
# ∞
−∞
(xα −⟨Xα⟩)2g(xα) dxα.
(32.52)
Deﬁne cov(Xα, Xβ), the covariance of xα and xβ, for a general density
Covariance deﬁned
function f(x), by
cov(Xα, Xβ) ≡
##
Ω
(xα −⟨Xα⟩)(xβ −⟨Xβ⟩)f(x) dmx
≡⟨(Xα −⟨Xα⟩)(Xβ −⟨Xβ⟩)⟩,
(32.53)
and note that by (32.49),
cov(Xα, Xα) = σ2(Xα),
cov(Xβ, Xβ) = σ2(Xβ).
(32.54)
Now suppose that xα is independent of the rest of the variables and β ̸= α.
Then
cov(Xα, Xβ) =
# ∞
−∞
(xα −⟨Xα⟩)g(xα) dxα
##
Ω′(xβ −⟨Xβ⟩)hα(x) dm−1x = 0.
The result follows from the fact that the integration over Ω′ is a constant and
the integral over xα can be done independently:
# ∞
−∞
(xα −⟨Xα⟩)g(xα) dxα =
# ∞
−∞
xαg(xα) dxα



=⟨Xα⟩by (32.52)
−⟨Xα⟩
# ∞
−∞
g(xα) dxα



=1 by (32.51)
= 0.
The preceding discussion shows that cov(Xα, Xβ) measures how much xα
is independent of the rest of the variables. If it is, then cov(Xα, Xβ) = 0; if
it is not, then cov(Xα, Xβ) ̸= 0. A quantity related to cov(Xα, Xβ), called
Correlation
deﬁned
correlation, is

804
Probability Theory
cor(Xα, Xβ) = cov(Xα, Xβ)
σ(Xα)σ(Xβ).
(32.55)
The “strongest” correlation occurs when α = β, in which case
cor(Xα, Xα) = cov(Xα, Xα)
σ2(Xα)
= 1
by (32.54).
The “weakest” correlation occurs when α ̸= β and xα is independent of the
rest of the variables, in which case cor(Xα, Xβ) = 0.
Thus, cor(Xα, Xβ)
indeed measures how much xα and xβ are correlated. Problem (32.22) shows
that |cor(Xα, Xβ| ≤1.
32.4.1
Transformation of Variables
Sometimes it is necessary or convenient to change a given set of random
variables to another set. Suppose that x = {xi}m
i=1 is a set of variables, and
u = {ui}m
i=1 are new variables of which the xi are functions. Given a density
f(x), the probability of ﬁnding x in an inﬁnitesimal volume dmx is f(x)dmx.
What is the corresponding probability in terms of the u variables? What is
the probability density g(u) so that g(u)dmu is the probability that u lies in
the inﬁnitesimal volume dmu? The answer is
g(u) = f
 
x1(u), x2(u), . . . , xm(u)
!
J(x, u),
(32.56)
where J(x, u) is the Jacobian of the x-to-u transformation, whose special cases
in two and three dimensions were given in (6.65) and (6.66). Equation (32.56)
is obtained from f(x)dmx by writing x’s in terms of u’s, keeping in mind that
dmx = J(x, u)dmu.
In most cases, there are only two variables x and y, which are transformed
into u and v. Then (32.56) yields
g(u, v) = f
 
x(u, v), y(u, v)
!






∂x
∂u
∂y
∂u
∂x
∂v
∂y
∂v






.
(32.57)
Example 32.4.2. The random variables x and y have the density function
f(x, y) =
0
c(x + y)e−x
if 0 < x, 0 < y < 1;
0
otherwise,
(32.58)
where c is a positive constant.
What is the density function h(u) for the sum
u = x + y?
As will become clear below, it is convenient to write f(x, y) in terms of the θ
function introduced in Section 5.1.3 Equation (5.18):
f(x, y) = cθ(x)θ(y)θ(1 −y)(x + y)e−x.
(32.59)

32.4 Continuous Random Variable
805
The reader is urged to verify that this is identical to (32.58). Let x = v. Then
u = x + y gives y = u −v, and the Jacobian for the transformation is







∂x
∂u
∂y
∂u
∂x
∂v
∂y
∂v







=





0
1
1
−1





 = 1.
(32.60)
Therefore, all is needed is to replace x and y in f(x, y):
g(u, v) = cθ(v)θ(u −v)θ(1 −u + v)ue−v.
This is the convenience we mentioned above: we don’t have to worry about diﬀerent
cases corresponding to diﬀerent limits of u and v; the θ function automatically takes
care of that!
To ﬁnd h(u), we need—by deﬁnition—to integrate over all values of v. Because
of the last θ factor in g(u, v), we need to consider two cases: 0 < u < 1 and u > 1. In
the ﬁrst case, θ(1 −u + v) = 1 because the ﬁrst θ function requires v to be positive.
Then the middle θ function sets the upper limit of v integration to u. Hence,
h(u) ≡
# ∞
−∞
g(u, v) dv =
# u
0
cue−v dv = cu
 
1 −e−u!
,
0 < u < 1.
In the second case, θ(1−u+v) requires v to be grater than u−1, and the middle
θ function still sets the upper limit of v integration to u. Therefore,
h(u) =
# u
u−1
cue−v dv = −cu e−v

u
u−1 = cue−u(e −1),
u > 1.
The two cases can be combined using the θ function:
h(u) = cu
.
θ(u)θ(1 −u)
 
1 −e−u!
+ θ(u −1)e−u(e −1)
/
.
■
Suppose that x and y are independent random variables with the density
function
f(x, y) = f1(x)f2(y).
We want to ﬁnd the density function h(u) of their sum u = x + y.
Let
x = v and y = u −v, so that the sum is indeed x + y. The Jacobian of the
transformation is 1 by (32.60). Therefore, by (32.57),
g(u, v) = f
 
x(u, v), y(u, v)
!
= f1(v)f2(u −v).
The density function of each variable is obtained by integrating over the other
variable. Thus,
h(u) ≡
# ∞
−∞
g(u, v) dv =
# ∞
−∞
f1(v)f2(u −v) dv.
(32.61)
The reader may recall from our discussion of Laplace transform that h is the
convolution of f1 and f2.

806
Probability Theory
Example 32.4.3. Assume that x and y are independent variables with
f1(x) =
1
π(x2 + 1),
−∞< x < ∞;
f2(y) =
1
π(y2 + 1),
−∞< y < ∞.
Then their sum u = x + y has the density function
h(u) = 1
π2
# ∞
−∞
dv
(v2 + 1)[(u −v)2 + 1] =
2
π(u2 + 4),
−∞< u < ∞
leaving the veriﬁcation of the last integration for Problem 32.25.
■
32.4.2
Normal Distribution
One of the most frequently used probability distributions is Gauss’ normal
distribution given by
f(x) =
1
√
2π σ e−(x−m)2/2σ2,
−∞< x < ∞.
(32.62)
It can be easily shown that ⟨X⟩= m and, as the notation suggests, the
variance is σ2.
To ﬁnd the probability that x lies in the interval (a, b), we have to integrate
f(x) from a to b:
p(a < x < b) =
1
√
2π σ
# b
a
e−(x−m)2/2σ2dx.
Let y = (x −m)/
√
2 σ and substitute for x in terms of y. Then
p(a < x < b) =
1
√π
#
b−m
√
2 σ
a−m
√
2 σ
e−y2dy = 1
2

erf
b −m
√
2 σ

−erf
a −m
√
2 σ

,
(32.63)
where erf is the error function introduced in Equation (11.9).
The error
function has been tabulated precisely because of its relation to the normal
distribution.
Suppose a and b are given in terms of their distance from the mean as a
multiple of the standard deviation: a = m + k1σ and b = m + k2σ, then we
have the important relation
p(m + k1σ < x < m + k2σ) = 1
2
*
erf

k2/
√
2

−erf

k1/
√
2
+
.
(32.64)
In particular, if k1 = −k2 ≡−k, then
p(m −kσ < x < m + kσ) = 1
2
*
erf

k/
√
2

−erf

−k/
√
2
+
.
(32.65)

32.4 Continuous Random Variable
807
For k = 1, 2, 3 this yields
p(m −σ < x < m + σ) = 0.6827
p(m −2σ < x < m + 2σ) = 0.9545
p(m −3σ < x < m + 3σ) = 0.9973.
Let x and y be random variables having the same normal distribution with
mean m and variance σ2
f1(x) =
1
√
2π σ e−(x−m)2/2σ2,
f2(y) =
1
√
2π σ e−(y−m)2/2σ2.
We want to ﬁnd the distribution of the sum u = x + y. This is a special case
of (32.61). Therefore, we can immediately write
h(u) =
1
2πσ2
# ∞
−∞
e−(v−m)2/2σ2e−(u−v−m)2/2σ2 dv
=
1
2πσ2
# ∞
−∞
exp

−(v −m)2 + (u −v −m)2
2σ2

dv.
The reader may verify that the exponent can be simpliﬁed to
−(v −u/2)2 + (u −2m)2/4
σ2
.
Substituting back in the integrand gives
h(u) = e−(u−2m)2/4σ2
2πσ2
# ∞
−∞
e−(v−u/2)2/σ2 dv



=
√
πσ2
= e−(u−2m)2/4σ2
√
2π
√
2σ2
.
This shows that
Box 32.4.1. If the random variables x and y have the same normal dis-
tribution, then their sum has a normal distribution with twice the mean
and twice the variance.
Example 32.4.4. Let x and y be independent random variables whose density
functions f1 and f2 are normal distribution with m = 0 and σ = 1. What is the
density function h(u) for the ratio x/y?
Let x = v, then y = v/u makes u the sought-after ratio. The Jacobian of the
transformation is
J =







∂x
∂u
∂y
∂u
∂x
∂v
∂y
∂v







=





0
−v/u2
1
1/u





 = |v|
u2 .

808
Probability Theory
Then
g(u, v) = f1(v)f2
 v
u
 |v|
u2 = 1
2π e−v2/2e−v2/2u2 |v|
u2 .
To ﬁnd h(u) we integrate g(u, v) over all values of v, namely −∞to ∞.
Since
the integrand is even, we can integrate from 0 to ∞and multiply the result by 2.
Therefore,
h(u) =
1
πu2
# ∞
0
e−( 1
2 +
1
2u2 )v2v dv =
1
π(u2 + 1).
The integration is straightforward as the reader can verify.
■
Equation (32.29), when written in the form (replacing m with x)
P(x, n) =

2
nπ e−(x−n/2)2/(n/2)
displays the similarity of the binomial distribution and the normal distribution
for the special case of p = q = 1
2. The equation shows that the mean is n/2
and the variance n/4. We now generalize this to arbitrary p and q.
Using the Stirling approximation x! ≈
√
2π e−xxx+1/2 and replacing m
with x, we write (32.27) as
P(x, n) ≈
√
2π e−nnn+1/2pxqn−x
√
2π e−xxx+1/2√
2π e−n+x(n −x)n−x+1/2
= nn+1/2pxqn−x(n −x)x
√
2π xx+1/2(n −x)n+1/2 ,
or, pulling out the power of 1/2 and collecting all terms with equal powers
together, we obtain
P(x, n) ≈

n
2πx(n −x)
 nq
n −x
n p(n −x)
xq
x
.
In the approximation we are seeking, we assume that x is close to the mean
np and write x = np + δ where δ is small compared to np. Then we get
P(x, n) ≈

n
2π(np + δ)(n −np −δ)

nq
n −np −δ
n p(n −np −δ)
(np + δ)q
np+δ
=

n
2π(np + δ)(nq −δ)

nq
nq −δ
n npq −pδ
npq + qδ
np+δ
,
or
P(x, n) ≈

1
2πnpq

1
1 −δ/nq
n 1 −δ/nq
1 + δ/np
np+δ



≡A
.
(32.66)
To proceed, we take the log of the term we have designated as A:
ln A = −n ln

1 −δ
nq

+ (np + δ)

ln

1 −δ
nq

−ln

1 + δ
np

= (−nq + δ) ln

1 −δ
nq

−(np + δ) ln

1 + δ
np

.

32.5 Problems
809
Expanding the log terms up to the second order yields
ln A ≈(−nq + δ)

−δ
nq −
δ2
2n2q2

−(np + δ)
 δ
np −
δ2
2n2p2

= −δ2
2nq −δ2
2np = −δ2
2n
1
q + 1
p

= −δ2
2npq ⇒A ≈e−
δ2
2npq .
Substituting this in (32.66) with δ = x −np, we obtain
P(x, n) ≈

1
2πnpqe−(x−np)2
2npq ,
which shows that P(x, n) is a normal distribution with mean np and variance
npq.
It can also be shown that the Poisson distribution (32.43) approaches the
normal distribution when n and λ are both large and δ = n −λ is small
compared to both:
p(n) →
1
√
2πλ
e−(n−λ)2/2λ.
We therefore have the law of large numbers:
law of large
numbers
Box 32.4.2. In the limit that the random variable and the mean go to
inﬁnity, both the binomial and Poisson distributions approach the Gauss’
normal distribution.
Normal distribution is a remarkable density function. We just saw that
both binomial and Poisson distributions approach it in the limit of large n.
But it goes beyond these two distributions. In fact it can be shown that a
set of identically distributed random variables with an arbitrary distribution
is approximately normally distributed if the number of components is large
enough. This is the content of the central limit theorem, and the rea-
central limit
theorem
son that normal distribution is the distribution of choice in many statistical
applications.
32.5
Problems
32.1. Using Venn diagrams, show that the operation of union distributes over
intersection and vice versa:
A ∩(B ∪C) = (A ∩B) ∪(A ∩C),
A ∪(B ∩C) = (A ∪B) ∩(A ∪C).
32.2. Using Venn diagrams, show that
A ∩(B −C) = (A ∩B) −(A ∩C),
A −(B ∪C) = (A −B) −C.

810
Probability Theory
32.3. Using Venn diagrams, show that
(A ∪B)c = Ac ∩Bc
and
(A ∩B)c = Ac ∪Bc.
32.4. Fill in the rest of the following table assuming that all probabilities pij
are independent.
F1
F2
Total
E1
0.3
E2
Total
0.4
32.5. Fill in the rest of the following table assuming that all probabilities pij
are independent.
F1
F2
F3
Total
E1
0.3
E2
E3
0.5
Total
0.1
0.7
32.6. Prove Equation (32.22).
32.7. What is the probability of obtaining 400 heads in 800 coin tosses? Of
obtaining more than 500 heads? Of obtaining between 350 and 450 heads?
32.8. A graphic calculator is needed!
Plot the binomial distribution
P(m, n) as a function of m for n = 50 and p = q = 1
2 using the exact formula
(32.27).
(a) From the plot estimate m−, the largest value on the left of maximum at
which the probability is (almost) zero, and m+, the smallest value on the right
of maximum at which the probability is (almost) zero. Compare these values
with (32.33).
(b) Sum the exact formula from m−to m+ to ﬁnd the probability that the
number of successes lies between m−and m+.
(c) Using the Stirling approximation (32.28) estimate the probability found
in (b) and compare the two values.
32.9. Example 32.2.2 used the exponential approximation to the binomial
distribution because the number of spins were assumed very large. Now as-
sume two systems with n1 = 8 and n2 = 12, and the total energy being
exchanged is represented by s = 4.
(a) Find ˆs1 and ˆs2, and show that ˆs1/n1 is (approximately) equal to ˆs2/n2
and s/n.
(b) Find the ratio of the probability that s1 = ˆs1−1 (and therefore, s2 = ˆs2+1)
to the maximum probability. How does this compare with the same ratio
found in Example 32.2.2?

32.5 Problems
811
32.10. Using the Stirling approximation x! ≈
√
2π e−xxx+1/2 of Equation
(11.6), show that
n!
m!(n −m)! ≈2n

2
nπ e−(n−2m)2/2n,
assuming that both m and n −m are large.
32.11. For the binomial distribution,
(a) show that
n

m=0
P(m, n) =
n

m=0

n
m

pmqn−m = 1,
(b) and that
# ∞
−∞
P(s, n) ds =
# ∞
−∞

2
nπ e−s2/2n ds = 1.
32.12. Let r ≡
P (m,n)
P (n/2,n). Solve for m and show that
m = 1
2(n ± √n
√
−2 ln r).
32.13. Show that if a/b = c/d then a/b = (a + c)/(b + d).
32.14. Derive (32.37) from (32.35).
32.15. Using the deﬁnitions of average and variance, show that σ2 =
⟨X2⟩−⟨X⟩2.
32.16. Show directly that (32.43) satisﬁes ∞
n=0 p(n) = 1.
32.17. A city had two earthquakes in a century. Find the probability that in
this city, there will be one earthquake
(a) next year,
(b) in the next 50 years.
(c) What is the probability of three or more independent earthquakes in the
same months?
32.18. The number of α particles emitted from a sample of a radioactive
atom is counted every minute for 50 hours. The total count is 1500.
(a) What is ν for this Poisson distribution?
(b) What is the probability that in the next 6 minutes three α particles will
be emitted?
(b) What is the probability that in the next 3 minutes at least four α particles
will be emitted?
32.19. One of the ﬁrst excited states of the H-atom has the wave function
Ψ(r, θ, ϕ) = Are−r/2a0 cos θ.

812
Probability Theory
(a) Find A so that Ψ(r, θ, ϕ) is normalized to 1.
(b) Evaluate ⟨X⟩, ⟨Y ⟩, and ⟨Z⟩. Are they all zero? Do you expect them to
be?
(c) What is the expectation value of the distance of the electron from the
nucleus for this state?
32.20. Suppose that φ is a function of xα alone and xα is independent of
the rest of the variables of f(x), the density function for a multidimensional
probability space. Show that
⟨φ(Xα)⟩=
##
Ω
φ(xα)f(x) dmx =
# ∞
−∞
φ(xα)g(xα) dxα.
32.21. The uniform probability density function over (a, b) is
f(x) =
0
1/(b −a)
if a < x < b;
0
otherwise.
What is the expectation value ⟨X⟩for this distribution?
32.22. Consider the nonnegative function
x(t) = ⟨[t(xα −⟨Xα⟩) + (xβ −⟨Xβ⟩)]2⟩.
(a) Show that
x(t) = t2σ2(Xα) + 2tcov(Xα, Xβ) + σ2(Xβ),
which is a parabola in the tx-plane.
(b) If the parabola is to be nonnegative, it should have at most one real root.
Show that for this to happen, the following inequality must hold:
cov2(Xα, Xβ) ≤σ2(Xα)σ2(Xβ).
32.23. Show that
# t
0
# t1
0
· · ·
# tn−2
0
# tn−1
0
dtndtn−1 · · · dt1 = tn
n!.
32.24. Let c be a positive constant and
f(x, y) =
0
cx(x + y)
if 0 < x, 0 < y < 1;
0
otherwise.
Let u = x + y. Show that the density h(u) for the variable u is
h(u) =
0
cu3/2
if 0 < u < 1;
cu(u −1
2)
if u > 1.
Show that this can be written as
h(u) = cu
.
θ(u)θ(1 −u)u2/2 + θ(u −1)(u −1
2)
/
.

32.5 Problems
813
32.25. Using partial fractions write the integrand of Example 32.4.3 as
1
(v2 + 1)[(u −v)2 + 1] = av + b
v2 + 1 +
cv + d
(u −v)2 + 1.
Now write the right-hand side as a single fraction with the same denominator
as the left-hand side. Set the coeﬃcients of the powers of v in the numerator
equal to zero, except the constant which must be equal to 1. Find a, b, c, and
d. Show that the integral becomes
(b + d + cu)
# ∞
−∞
dv
v2 + 1 =
2
u2 + 4
# ∞
−∞
dv
v2 + 1.
32.26. Using the Stirling approximation and a procedure similar to the one
used for binomial distribution in the text, show that in the limit of large
n and λ, the Poisson distribution of Equation (32.43) becomes the normal
distribution.
32.27. Certain measurements are assumed to be normally distributed with
25 as the mean 25 and 0.5 as the standard deviation. What is the probability
that a measurement lies between 23 and 27?

Bibliography
[1] Alligood, K.T., Sauer, T.D., and Yorke, J.A. Chaos: An Introduction to
Dynamical Systems, Springer-Verlag, 1996.
[2] Auer, J.W. Linear Algebra with Applications, Prentice-Hall, 1991.
[3] Axler, S. Linear Algebra Done Right, Springer-Verlag, 1996.
[4] Baker, G.L. and Gollub, J.P. Chaotic Dynamics: An Introduction, Cam-
bridge University Press, 1990.
[5] Birkhoﬀ, G. and Rota, G.-C. Ordinary Diﬀerential Equations, 3rd ed.,
Wiley, 1978.
[6] Churchill, R. and Verhey, R. Complex Variables and Applications, 3rd
ed., McGraw-Hill, 1974.
[7] Edwards, C.H., Penney, D.E. Calculus and Analytic Geometry, Prentice
Hall, 1990.
[8] Finney, R.L., Thomas, G.B., Damana, F., and Waits, B.K. Calculus,
Addison-Wesley, 1994.
[9] Friedberg, S., Insel, A., and Spence, L. Linear Algebra, Prentice-Hall,
1997.
[10] Gamow, G., The Great Physicists: From Galileo to Einstein, Dover, 1961.
[11] Gelfand, I. M. and Fomin, S. V., Calculus of Variations, translated and
edited by Silverman, R. A., Dover, 1991.
[12] Goode, S.W. An Introduction to Diﬀerential Equations and Linear Alge-
bra, Prentice-Hall, 1991.
[13] Halmos, P. Finite Dimensional Vector Spaces, 2nd ed., Van Nostrand,
1958.
[14] Hartle, J. Gravity, Addison Wesley, 2003.
[15] Hassani, S. Mathematical Physics: A Modern Introduction to Its Foun-
dations, Springer-Verlag, 1999.

816
BIBLIOGRAPHY
[16] Hilborn, R.C. Chaos and Nonlinear Dynamics: An Introduction for Sci-
entists and Engineers, Oxford University Press, 1994.
[17] Holmgren, R.A. A First Course in Discrete Dynamical Systems, Springer-
Verlag, 1996.
[18] Kaplan, W. Advanced Calculus, Addison-Wesley, 1991.
[19] Kline, M., Mathematical Thought: From Ancient to Modern Times, Vols.
1–3, Oxford University Press, 1972.
[20] Lang, S. Calculus of Several Variables, Springer-Verlag, 1988.
[21] Lang, S. Complex Analysis, 2nd ed., Springer-Verlag, 1985.
[22] Lorrain, P., Corson D., and Lorrain, F. Electromagnetic Fields and
Waves, 3rd ed., W. H. Freeman, 1988.
[23] Lovelock, D. and Rund, H. Tensors, Diﬀerential Forms, and Variational
Principles, Dover, 1989.
[24] Miller, R.K. Introduction to Diﬀerential Equations, Prentice-Hall, 1991.
[25] Pinsky, M.A. Partial Diﬀerential Equations and Boundary-Value Prob-
lems with Applications, McGraw-Hill, 1991.
[26] Rahman, M. Ordinary Diﬀerential Equations and Partial Diﬀerential
Equations, Computational Mechanics Publication, 1991.
[27] Schiﬀ, J. L. The Laplace Transform, Springer, 1991.
[28] Seydel, R. From Equilibrium to Chaos: Practical Bifurcation and Stability
Analysis, Elsevier, 1988.
[29] Simmons, G. Calculus Gems, McGraw-Hill, 1992.
[30] Stewart, J. Calculus, 4th ed., Brooks/Cole, 1999.

Index
Abel, 331, 503, 675
Abel, Niels Henrik
biography, 326
absolute diﬀerential, 463
Acceleration, 44
acceleration
components
spherical coordinates, 353
active transformation, 178
addition of velocities
relativistic law of, 246
adjoint DO, 571–573
advanced Green’s function, 712
aﬃne connection, 462–464, 470
algebra
fundamental theorem, 478, 511
amplitude
complex, 486
angle
as integral, 345
solid, 344–350
total, 349
total, 346
angular momentum, 28
conservation, 351
central force, 354
angular momentum operator, 412
spherical coordinates, 435
antisymmetric tensor, 452
Arago, 321
Archimedes, 47
Archimedes, of Syracuse
biography, 81
area element
primary, 60
astragalus, 781
astrophysics, 415
Poisson equation, 415
attractor, 766
strange, 778
autonomous, 767
average, 790, 801
azimuth, 12
azimuthal
angle, 12
symmetry, 607
azimuthal equation, 549
Barrow, 87, 96, 97, 481
Barrow, Isaac
biography, 47
basin of attraction, 766
basis, 16
for plane, 175
orthonormal, 186
standard, 216
Bayes’ theorem, 788
Beltrami identity, 731
Bernoulli, 272, 294, 320, 321, 326, 642
Bernoulli’s FODE, 560
Bessel, 322
Bessel diﬀerential equation, 548, 641
recursion relation, 644
Bessel equation
Liouville substitution, 589
Bessel function, 333–335, 644
addition theorem, 651
conﬂuent hypergeometric, 652
expansion in, 653–654
physical examples, 654–656
generating function, 651
integral representation, 652
Laplace’s equation, 642–654
order
negative integer, 646
orthogonality relation, 649
properties, 646–652
recurrence relation, 646
second kind, 645
Bessel’s integral, 652

818
INDEX
Bessel, Friedrich Wilhelm
biography, 641
beta function, 320
Bezout, 210
Bianchi identity, 470
bifurcation, 757
Hopf, 770
period-doubling, 757
binding energy, 582
binomial probability distribution,
792–797
binomial theorem, 265
biography
Abel, Niels Henrik, 326
Archimedes, of Syracuse, 81
Barrow, Isaac, 47
Bessel, Friedrich Wilhelm, 641
Biot, Jean-Baptiste, 115
Cauchy, Augustin-Louis, 503
Cavalieri, Bonaventura, 90
Cavendish, Henry, 23
Cayley, Arthur, 192
Coulomb, Charles, 23
d’Alembert, Jean Le Rond, 548
Descartes, Rene, 15
Dirac, Paul Adrien Maurice, 151
Euler, Leonhard, 321
Fermat, Pierre de , 15
Fourier, Joseph, 304
Gauss, Johann Carl Friedrich,
330
Gibbs, Josiah Willard, 381
Hamilton, William R., 10
Heaviside, Oliver, 382
Hermite, Charles, 674
Jacobi, Carl Gustav Jacob, 326
Kepler, Johannes, 579
Laplace, Pierre Simon de, 593
Legendre, Adrien-Marie, 617
Leibniz, Gottfried Wilhelm, 103
Maxwell, James Clerk, 419
Newton, Isaac, 96
Savart, Felix, 115
Stokes, George Gabriel, 398
Sylvester, James Joseph, 210
Taylor, Brook, 294
Wallis, John, 90
Biot, Jean-Baptiste
biography, 115
Biot–Savart law, 30
circuit, 111
general, 110
bipolar coordinates, 73
three-dimensional, 74, 438
Boltzmann, 419
Bose-Einstein statistics, 792
boundary condition, 542
Dirichlet, 593
Neumann, 593
boundary conditions
periodic, 574
separated, 574
brachistochrone, 731
Bromwich contour, 722
butterﬂy eﬀect, 771
calculus
fundamental theorem, 87
calculus of residues, 525–536
canonical equations, 749
Cantor set, 776
Cardan, 481
Cartesian vector, 216
component, 216
Cauchy, 279, 326, 331, 594
Cauchy criterion, 261
Cauchy integral formula, 508–509
Cauchy, Augustin-Louis
biography, 503
Cauchy–Goursat theorem, 505
Cauchy–Riemann conditions, 500
Cauchy–Riemann theorem, 501
Cavalieri, Bonaventura
biography, 90
Cavendish, Henry
biography, 23
Cayley, 192, 211
Cayley, Arthur
biography, 192
center of mass, 21
central force, 354, 579–583
eccentricity, 581
central limit theorem, 809
centrifugal potential, 581
chain rule, 55–57
Champollion, 304
chaos, 753
theory
systems obeying DE, 770–773

INDEX
819
systems obeying iterated map, 763
universality, 773–778
coeﬃcient, 173
cofactor, 205
collision
relativistic, 250–253
column vector, 177
combination, 791
complement, 783
complex
conjugate, 479
function
analytic, 499
continuous, 499
regular point, 499
singular point, 499
singularity, 499
integral
positive sense, 507
number, 478
absolute value, 479
argument, 483
imaginary part, 478
real part, 478
plane, 478
complex amplitude, 486
complex function, 497–511
derivative, 499–503
derivative as integral, 509–511
integration, 503–508
complex number
Cartesian form, 478
Fourier series, 489
polar form, 482
roots, 486
complex numbers, 477–488
Cartesian form, 477–481
Fourier series, 488–491
polar form, 482–488
complex power series, 516
analyticity, 517
convergence circle, 517
diﬀerentiation, 517
integration, 517
uniform convergence, 517
complex series
absolute convergence, 516
component, 176
Compton wavelength, 253
conditional probability, 786–789
conducting cylindrical can, 655
conductor
electrical, 594
heat, 598
conﬂuent hypergeometric function, 332–
333
connection
aﬃne, 462–464, 470
metric, 465–468
constraints, 360, 738
continuity equation, 378–381
diﬀerential form, 380
integral form, 380
contour, 505
Bromwich, 722
simple closed, 505
contractible to zero, 400
contraction, 451
contravariant vector, 445–447
convergence
test, 267–272
convolution, 716
coordinate
generalized, 741
coordinate system, 11–15
bipolar, 73
three-dimensional, 74, 438
Cartesian, 11, 12
cylindrical, 12
elliptic, 73, 213
elliptic cylindrical, 73, 213, 436
parabolic, 73
paraboloidal, 74, 437
polar, 11
prolate spheroidal, 74, 213, 437
spherical, 12
toroidal, 74, 213, 437
unit vector, 31–36
vector, 16–31
coordinate time, 239–240
Copernicus, 97, 417, 580
correlation
probability, 803
cosine transform, 697
Coulomb, 744
Coulomb’s law, 22, 24
Coulomb, Charles
biography, 23
covariance
in probability, 803

820
INDEX
covariant derivative, 464–465
covariant diﬀerential, 462–464
covariant vector, 445–447
Cramer, 210
Crelle, 326
cross product, 7, 28–31
as a tensor, 447
Levi-Civita symbols, 458
parallelepiped volume, 10
parallelogram area, 9
curl
curvilinear coordinates, 431–435
vector ﬁeld, 391–398
current density, 379
and ﬂux, 379
curvature, 468–471
scalar, 470
curve
parametric equation, 61
primary, 59
curvilinear
vector analysis, 423–435
curvilinear coordinates
curl, 431–435
divergence, 427–431
gradient, 425–427
Laplacian, 435
cycloid, 732
d’Alembert, 273, 303
d’Alembert, Jean Le Rond
biography, 548
d´Alembert, 743
damping factor, 311
DE
ﬁrst-order, 551–561
integrating factor, 553–555
linear, 556–561
second-order, 563–570
de Broglie, 666
de Moivre theorem, 485
del operator, 359
delta
Kronecker, 442
delta function
and Laplacian, 412
cylindrical, 160
derivative, 147, 159
Legendre expansion, 630
limit of sequence, 492
one-variable, 139–151
point sources, 144
polar, 156
representation, 491–492
spherical, 160
three-variable, 159–165
two-dimensional, 155
two-variable, 154–159
density, 45
current, 379
ﬂux, 371–381
of states, 677
probability, 801
density function
surface, 154
derivative, 44–46
covariant, 464–465
functional, 730
mixed, 52
normal, 593
partial, 47–59
time
vector, 350–355
total, 86
Descartes, 46, 97, 103, 215, 417, 481, 482
Descartes, Rene
biography, 15
determinant, 202–207, 222–227
parallelepiped volume, 10
diﬀerential, 53–54
absolute, 463
covariant, 462–464
exact, 553
diﬀerential equation
Bessel, 548, 641
recursion relation, 644
second solution, 645–646
solutions, 642–645
conﬂuent hypergeometric, 332
Hermite
recursion relation, 668
hypergeometric, 328
Legendre, 608
second solution, 617–619
order of, 556
ordinary, 542
partial, 542
second-order linear
adjoint, 572
integrating factor, 571

INDEX
821
diﬀerential operator, 217, 576
diﬀusion equation, 661
time-dependent, 663
dimension, 176
fractal, 775–778
dipole
approximation, 298
magnetic, 410
dipole moment, 298
dipole potential, 299
Dirac, 26
biography, 151
Dirac delta function
in variational problems, 730
step function, 153
Dirac, Paul Adrien Maurice
biography, 151
disjoint sets, 783
distance
spacetime, 240–242
distribution, 146
normal, 806–809
sum of two, 807
divergence, 371–381
curvilinear coordinates, 427–431
spherical coordinates, 430
theorem, 374–378
vector ﬁeld, 374
Doppler shift
relativistic, 255
dot product, 5, 21
double del operation, 407–412
double factorial, 319
dummy index, 262
dynamical system
autonomous, 767
nonautonomous, 767
eccentricity, 581
eigenvalue, 224
eigenvalue equation, 224
eigenvector, 224
Einstein, 215, 666
summation convention, 441
Einstein curvature tensor, 471
Einstein equation, 471
electric ﬁeld, 104
point charge, 25
electrical conductor, 594
electrodynamics
Lagrangian density, 745
tensors, 459–461
element
area, 59–68
Cartesian, 60–62
cylindrical, 65–68
length, 59–68
spherical, 62–64
volume, 59–68
elliptic coordinates, 73, 213
elliptic cylindrical coordinates, 73, 213,
436
elliptic functions, 322–326
elliptic integral
complete, 324
ﬁrst kind, 323
second kind, 323
empty set, 782
energy
relativistic, 249
zero mass particle, 250
energy momentum tensor, 471
equation
canonical, 749
Klein-Gordon, 747
error function, 322, 806
Euclid, 47, 80, 90
Euler, 272, 303, 326, 330, 503, 642, 743
Euler angles, 201
Euler equation, 483
Euler’s equation, 414
Euler, Leonhard
biography, 321
Euler-Lagrange equation, 729–731,
734–736, 738, 739
event, 784
compound, 784
elementary, 784
random, 781
exact diﬀerential, 553
expectation value, 790
extremum problem, 727
gradient, 359–361
factorial
double, 319
factorial function, 99, 318
Faraday, 26
Feigenbaum alpha, 774
Feigenbaum delta, 773

822
INDEX
Feigenbaum numbers, 773–775
Fermat, Pierre de
biography, 15
Fermi energy, 677
Fermi-Dirac statistics, 791
Feynman, 26
ﬁeld, 21–28, 343
electric, 104
scalar, 343
spinor, 343
tensor, 343
vector, 343
ﬁeld point, 25, 78
ﬁne structure constant, 679
ﬁnite constraint problem, 739
ﬁxed point
iterated map, 755
stable, 756
ﬂat space, 470
Florence Nightingale, 210
ﬂuid dynamics, 413–415
ﬂux, 365–369
density, 371–381
vector ﬁeld, 365–369
FODE, 551–561
Bernoulli, 560
homogeneous, 560
integrating factor, 553–555
Lagrange, 561
linear, 556–561
normal, 551
integral of, 552
FOLDE, 556–561
explicit solution, 557
force
central, 354
force density, 414
form factor, 702
four-acceleration, 248
four-momentum, 247–250
four-vector, 243
four-velocity, 247–250
Fourier, 115, 279, 322
Fourier series, 299–303
complex numbers, 488–491
to Fourier transform, 693–696
Fourier transform, 693–712
and derivatives, 702–703
and quark model, 702
application to DEs, 702–704
convolution theorem, 724
Coulomb potential
charge distribution, 701
point charge, 700
deﬁnition, 695
examples, 698–702
Gaussian, 699
Green’s functions, 705–712
heat equation
one-dimensional, 704
higher dimensions, 696
inverse, 695
of delta function, 698
properties, 696
Fourier, Joseph
biography, 304
Fourier-Bessel series, 655
fractal, 777
fractal dimension, 775–778
free index, 440
frequency
natural, 586
Frobenius method, 608–610, 693
function
analytic
isolated singularity, 525
principal part, 528
antiderivative, 87
as integral, 317–326
as power series, 327–335
Bessel, 333–335, 644
Laplace’s equation, 642–654
beta, 320
complex, 497–511
derivative, 499–503
residue, 526
complex hyperbolic, 502
complex trigonometric, 502
conﬂuent hypergeometric, 332
delta
point sources, 144
elliptic, 322–326
error, 322
even, 84
factorial, 318
gain, 586
gamma, 318–319
Stirling approximation, 319
harmonic, 501
homogeneous, 57–59

INDEX
823
hypergeometric, 328–330
integral representation, 329
iterated map, 755
linear density, 143
logistic map, 755
odd, 84
periodic, 299
piecewise continuous, 82
primitive, 87
rational
integral, 529–531
sequence, 274–279
series, 274–279
special, 550
transfer, 586
functional, 728
functional derivative, 730
fundamental theorem of algebra, 478
fundamental theorem of calculus, 87
G-orthogonal, 187, 219
matrix, 191, 222
space, 200
vector
in space, 199
gain function, 586
Galileo, 26, 90, 97, 325
gamma function, 318–319
Stirling approximation, 319
gauge transformation, 418
Gauss, 279, 321, 326, 503, 617, 641
Gauss elimination, 231
Gauss’s law, 369
diﬀerential form, 378
integral form, 377
Gauss, Johann Carl Friedrich
biography, 330
Gaussian
Fourier transform of, 699
Gay–Lussac, 594
generalized coordinates, 741
generalized momentum, 748
generating function
Hermite polynomials, 673
geodesic, 465
relativity, 466
sphere, 466
geometric series, 271
geometry
and metric tensor, 456
distance formula, 241
Gibb’s phenomenon, 302
Gibbs, 370
Gibbs, Josiah Willard
biography, 381
Goldbach, 320
gradient, 355–361, 445
components, 440
curvilinear coordinates, 425–427
normal to surface, 358
three dimensions, 357
two dimensions, 357
Gram–Schmidt process, 221
for space, 199
Grassmann, 382
Green, 210
Green’s function
advanced, 712
diﬀerebtial eq. for, 707
heat equation, 709–710
Laplacian, 708–709
Poisson equation, 709
retarded, 712
wave equation, 711–712
Green’s Functions, 705–712
Gregory, 272, 294
guided wave, 682–686
TE, 684
TEM, 685
TM, 684
Halley, 641
Hamilton, 369, 382
Hamilton, William R.
biography, 10
Hamiltonian, 747–749
harmonic oscillator
quantum, 667
Hermite DE, 668
heat-conducting plate
circular, 664–665
rectangular, 663–664
heat-conducting rod, 662–663
heat conductor, 598
heat equation, 543, 661–665
Green’s function, 709–710
one-dimensional, 704
heat transfer
time-dependent, 663

824
INDEX
heat-conducting rod, 662
Heaviside, 370
Heaviside, Oliver
biography, 382
Heisenberg, 26, 151, 675
Heisenberg uncertainty relation, 699
Helmholtz Coil, 291–293
Helmholtz free energy, 54
Hermite DE
recursion relation, 668
Hermite polynomial, 670
orthogonality, 672
Hermite polynomials, 229, 575
generating function, 673
Hermite, Charles
biography, 674
HNOLDE, 575
characteristic polynomial, 576
homogeneous
function, 57
homogeneous function, 57–59
homogeneous SOLDE
exact, 571
Hooke, 97
Hopf bifurcation, 770
HSOLDE, 564
second solution, 568
Huygens, 97, 103
hydrogen atom, 677–680, 802
hyperbolic cosine, 290
hyperbolic sine, 290
hypergeometric function, 328–330
conﬂuent, 332–333
integral representation, 329
identity matrix, 180
indeterminate form, 294–297
index
free, 440
indicial equation, 609
induction
mathematical, 265–266
inductive deﬁnition, 266
inﬁnite series, 266–274
inner product, 218–222
positive deﬁnite, 187
Riemannian, 187
inner product matrix, 185
integral, 79
as function, 317–326
Bessel’s, 652
derivative of, 85–86
function of trigonometric, 534–536
indeﬁnite, 87
line, 387–391
Mellin inversion, 722
rational function, 529–531
rational trigonometric, 532–534
integral transform, 693
kernel, 693
integrand, 80
integrating factor, 553–555
integration, 77–80
application
Cartesian coordinates, 104–107,
112, 115–117
cylindrical coordinates, 107–109,
112–115, 118–119
double integrals, 115–122
electricity, 104–109
general, 91–96
gravity, 104–109
magnetostatics, 109–115
mechanics, 101–103
single integral, 101–115
spherical coordinates, 120–122
triple integrals, 122–128
Cauchy integral formula, 508–509
change of dummy variable, 82
complex function, 503–508
interchange of limits, 82
linearity, 82
parameter, 80
partition of range, 82
point, 79
properties, 81–89
region of, 79
small region, 83
symmetric range, 84
transformation of variable, 83
variable, 80
intersection, 783
inverse
matrix, 203, 207
of a matrix, 180
inverse Fourier transform, 695
ionic crystal
one-dimensional, 145
potential energy, 164
two-dimensional, 157

INDEX
825
ISOLDE, 569
isoperimetric problem, 738
iterated map, 754–763
ﬁxed point, 755
orbit, 755
Jacobi, 211, 331
Jacobi, Carl Gustav Jacob
biography, 326
Jacobian, 207–210
in probability, 804
Jacobian matrix, 208
Kaluza, 215
Kepler, 89, 97
Kepler’s ﬁrst law, 582
Kepler’s second law, 582
Kepler’s third law, 583
Kepler, Johannes
biography, 579
kernel
integral transform, 693
Klein-Gordon equation, 747
Koch snowﬂake, 777
Kronecker delta, 222, 442, 449, 489
Euclidean metric, 466
generalized, 452
Lagrange, 294, 304, 326, 330, 594,
617, 642
biography, 742
Lagrange identity, 572
Lagrange multiplier, 360, 738
Lagrangian, 740–745
interacting particles, 741
Klein-Gordon, 747
particle in EM ﬁeld, 746
single particle, 741
Lagrangian density, 744–745
electrodynamics, 745
Laguerre polynomials, 230, 679
Laplace, 115, 304, 322, 326, 744
Laplace transform, 712–723
and diﬀerential equations, 718–721
Bromwich contour, 722
convolution, 716
cosine, 713
derivative, 717–718
ﬁrst shift, 714
gamma function, 713
imaginary exponential, 713
integral, 717–718
inverse, 721–723
linearity, 714
Mellin inversion integral, 722
periodic functions, 716
properties, 713–717
second shift, 714
sine, 713
step function, 713
unit function, 713
Laplace’s equation, 411, 542, 546
Bessel functions, 642–654
Cartesian coordinates, 594–603
cylindrical coordinates, 639–656
Legendre polynomials, 610–617
radial equation, 619–622
solution
uniqueness, 592
spherical coordinates, 607–634
uniqueness of solution, 592–593
Laplace, Pierre Simon de
biography, 593
Laplacian, 411
and Dirac delta function, 412
curvilinear coordinates, 435
Green’s function, 708–709
Laurent series
complex, 518–522
Lavoisier, 744
law of addition of velocities, 237
law of large numbers, 809
law of motion
relativistic, 253–254
Legendre, 304, 326
Legendre equation, 575
recursion relation, 611
Legendre functions
second kind, 618
Legendre polynomial, 228, 614, 616
expansion in, 628–630
physical examples, 631–634
generating function, 621
Laplace’s equation, 610–617
multipole expansion, 621
orthogonality, 625
parity, 622
properties, 622–628
recurrence relation, 623
Rodrigues formula, 626

826
INDEX
Legendre polynomials, 229, 575
Legendre transformation, 54, 748
Legendre, Adrien-Marie
biography, 617
Leibniz, 46, 87, 90, 97, 210, 272, 482
Leibniz, Gottfried Wilhelm
biography, 103
length element
primary, 59
Levi-Civita symbol, 453
Levi-Civita symbols
cross product, 458
l’Hˆopital’s rule, 294–297
limit cycle, 770
line integral, 387–391
linear combination, 173
linear dependence, 174
linear equation, 230–234
compatible, 231
echelon form, 232
homogeneous, 234
incompatible, 231
linear independence, 174
linear operator, 216
linear transformation, 216–218
Liouville substitution, 588
logistic map, 755
second iterate, 757
Lorentz gauge, 419
Lorentz transformation, 243–247
general, 244
in 2 dimensions, 245
lowering indices, 457–459
Lyapunov exponent, 763
Maclaurin, 210, 272
Maclaurin series, 287
Madelung constant, 165
magnetic charge, 409
magnetic dipole moment, 410
magnetic ﬁeld
moving charge, 30
magnetic force
current loop, 420
moving charge, 30
magnetic monopole, 409
manifold, 456, 469
map
iterated, 754–763
marginal probability, 786–789
mathematical induction, 265–266
matrix, 177
G-orthogonal, 191, 222
space, 200
identity, 180
inner product, 185
inverse, 180, 203, 207
Jacobian, 208
metric, 185
multiplication rule, 442
orthogonal, 190
symmetric, 182
transformation
in space, 195
transpose, 181
unit, 180
zero, 180
Maxwell, 26, 369, 382
Maxwell’s equations, 415–419
derivation of wave equation, 417
relation to relativity, 237
Maxwell, James Clerk
biography, 419
Maxwell-Boltzmann statistics, 791
mean, 790
Mellin inversion integral, 722
membrane, 686–687
metric connection, 465–468
relativity, 466
metric matrix, 185
metric tensor, 454–461
deﬁnition, 456
relativity, 458
minimal coupling, 749
Minkowski, 215
mode
of oscillation, 682
M¨obius band, 366
moment
quadrupole, 449
moment generating function, 790
binomial distribution, 793
Poisson distribution, 799
moment of inertia, 122
momentum
generalized, 748
relativistic, 249
zero mass particle, 250
Monge, 115, 304

INDEX
827
motion
constant of, 552
multipole expansion, 297–299
Napoleon, 304, 593
natural frequency, 586
Neumann function, 645
Newton, 16, 26, 43, 46, 78, 87, 90, 103,
122, 272, 294, 317, 322, 326,
330, 481, 482, 548, 593
Newton, Isaac
biography, 96
NOLDE, 575
nonautonomous, 767
normal distribution, 806–809
sum of two, 807
nth iterate, 760
ODE, 542
ODE and PDEs, 542–550
Olbers, 641
operator
angular momentum, 412
spherical coordinates, 435
del, 359
diﬀerential, 576
linear, 217
linear, 216
orientable surface, 366
orthogonal
matrix, 190
orthogonal polynomial
standardization, 227
orthogonal polynomials, 227–230
orthonormal
basis, 186
parabolic coordinates, 73
paraboloidal coordinates, 74, 437
parallel translation, 465
Parseval relation, 654
Parseval’s relation, 724
partial derivative, 47–59
particle in a box, 675
Pascal, 15, 103, 481
passive transformation, 178
PDE, 542
separation
Cartesian coordinates, 544–546
cylindrical coordinates, 547–548
spherical coordinates, 548–550
PDE and ODE, 542–550
period-doubling, 757
periodic BC, 574
permutation, 791
phase space, 764–766
diagram, 764
trajectory, 764
Planck, 666
plane
basis, 175
Poincar´e, 674
Poisson, 594
Poisson distribution, 797–800
Poisson equation, 411, 542
astrophysics, 415
Green’s function, 709
polar coordinates, 16
polar equation, 549
pole
of order m, 528
simple, 528
polynomial
Hermite, 229, 670
Laguerre, 230, 679
Legendre, 228, 229, 614, 616
Laplace’s equation, 610–617
orthogonal, 227–230
standardization, 227
position vector, 19
potential, 21–28, 399
centrifugal, 581
diﬀerence, 399
of a dipole, 299
potential energy, 553
power series, 283–299
continuity, 285
diﬀerential equations, 307
diﬀerentiation, 285
integration, 285
operations, 520
radius of convergence, 283
zero, 285
pressure, 46
primary curve, 59
primary surface, 60
probability
average, 790
basic concepts, 781–792
binomial distribution, 792–797

828
INDEX
conditional , 786–789
correlation, 803
covariance, 803
density, 801
expectation value, 790
independent random variable, 802
marginal , 786–789
mean, 790
moment generating function, 790
Poisson distribution, 797–800
sample space, 784–786
set theory, 782–784
standard deviation, 790
variance, 790
probability space, 784
prolate spheroidal coordinates, 74, 213,
437
proper time, 239–240
quadrupole moment, 449
quantization
hydrogen atom, 679
quantum harmonic oscillator, 667–674
quantum mechanics
angular momentum operator, 412
spherical coordinates, 435
quantum particle
in a box, 675–677
quantum tunneling, 676
quaternions, 11
radial, 19
radial equation, 549
raising indices, 457–459
random event, 781
random variable
continuous, 801–809
independent, 802
transformation, 804–806
rate of change, 44
ratio test
Waring, 273
recursion relation, 308, 610
relativistic collision, 250–253
relativistic energy, 249
relativistic law of motion, 253–254
relativistic momentum, 249
relativity
geodesic, 466
metric connection, 466
metric tensor, 458
principle, 238
special, 237
residue, 526
calculus, 525–536
residue theorem, 527
deﬁnite integral
rational function, 529
rational trigonometric, 532
trigonometric function, 534
retarded Green’s function, 712
Ricci tensor, 470
Riemann, 321
Riemann curvature tensor, 468–471
Riemann zeta function, 269
Riemannian manifold, 456
right-hand rule, 392
rigid transformation, 190
Rodrigues formula, 626
Rosetta stone, 304
row vector, 181
sample space, 784–786
Savart, Felix
biography, 115
scalar curvature, 470
scalar function, 445
Schr¨odinger, 675
biography, 666
Schr¨odinger equation, 543, 546, 666–680
time-independent, 666
Schwarz inequality, 185, 220
Schwinger, 26
second iterate, 757
second variation, 735–738
self-similarity, 775
separated boundary conditions, 574
separation of time, 543
separatrix, 766
sequence, 259–262
bounded, 261
convergence, 260
Cauchy criterion, 261
divergence, 260
functions, 274–279
limit, 260
monotone decreasing, 261
monotone increasing, 261
partial sum, 259, 267
series, 266–274

INDEX
829
alternating
test, 270
application to DE, 307–311
complex, 518
Laurent, 518–522
Taylor, 518–522
convergence
absolute, 268
comparison test, 268
conditional, 272
generalized ratio test, 270
integral test, 268
n-th term test, 267
ratio test, 269
convergent
grouping, 273
rearranging, 273
familiar functions, 287–291
Fourier, 299–303
complex numbers, 489
Fourier–Bessel, 655
functions, 274–279
uniform convergence, 276
geometric, 271
harmonic
order p, 269
Laurent
complex, 518
Maclaurin, 287
binomial function, 288
complex, 518
exponential function, 287
hyperbolic function, 289
logarithmic function, 291
trigonometric function, 287
operations on, 273–274
power, 283–299
diﬀerential equations, 307
Taylor, 286–287
complex, 518
multivariable, 305–307
uniform convergence
diﬀerentiation, 278
integration, 278
uniformly convergent, 277–279
set theory, 782–784
complement, 783
diﬀerence, 783
disjoint sets, 783
intersection, 783
union, 782
Venn diagrams, 783
sine transform, 697
soap ﬁlm problem, 733
SOLDE, 563–570
basis of solutions, 565
central force, 579
constant coeﬃcient, 575–587
homogeneous, 576–583
inhomogeneous, 583–587
homogeneous, 564
second solution, 567–569
inhomogeneous
general solution, 569–570
Kepler problem, 580
linearity, 564–565
normal form, 563
singular point, 563
superposition, 564–565
superposition principle, 564
uniqueness of solution, 564–565
uniqueness theorem, 565
variation of constants, 569
Wronskian, 566–567
solid angle, 344–350
total, 349
source point, 25, 79
space
dimension, 11
ﬂat, 470
point, 11
probability, 784
spacetime distance, 240–242
being zero, 242
span, 175
special functions, 550
standard basis, 216
standard deviation, 790
statistical independence, 788
statistics
Bose-Einstein, 792
Fermi-Dirac, 791
Maxwell-Boltzmann, 791
stellar equilibrium, 415
step function, 152–153
Dirac delta function, 153
Laplace transform, 713
Stifel, 481
Stirling, 320
Stirling approximation, 319, 792, 808

830
INDEX
Stokes’ theorem, 391–398
Stokes, George Gabriel
biography, 398
strange attractor, 778
Sturm–Liouville
system, 574
Sturm-Liouville equation, 574
subset, 782
success excess, 793
summation, 262–266
superposition principle, 25, 564
surface
primary, 60
Sylvester, and Cayley, 192
Sylvester, James Joseph
biography, 210
symmetric matrix, 182
symmetric tensor, 452
Taylor series, 286–287
complex, 518–522
multivariable, 305–307
Taylor, Brook
biography, 294
tensor, 447–454
addition, 450
algebraic properties, 450–452
contraction, 451
diﬀerentiation, 462–468
Einstein curvature, 471
electrodynamics, 459–461
energy momentum, 471
Levi-Civita symbols, 453
metric, 454–461
deﬁnition, 456
relativity, 458
multiplication, 451
numerical, 452–454
rank of, 448
Ricci, 470
Riemann curvature, 468–471
symmetrization, 452
torsion, 463
terminal velocity, 559
theorem
central limit, 809
time
coordinate, 239–240
proper, 239–240
time constant, 586
toroidal coordinates, 74, 213, 437
torque, 28
torsion tensor, 463
transfer function, 586
transform
cosine, 697
Fourier, 693–712
and quark model, 702
application to DEs, 702–704
convolution theorem, 724
examples, 698–702
Gaussian, 699
Green’s functions, 705–712
heat equation in 1D, 704
inverse, 695
of delta function, 698
properties, 696
integral, 693
Laplace, 712–723
and diﬀerential equations, 718–
721
Bromwich contour, 722
convolution, 716
cosine, 713
derivative, 717–718
ﬁrst shift, 714
gamma function, 713
imaginary exponential, 713
integral, 717–718
inverse, 721–723
linearity, 714
Mellin inversion integral, 722
periodic functions, 716
properties, 713–717
second shift, 714
sine, 713
step function, 713
unit function, 713
sine, 697
transformation
active, 178
coordinate, 13
diﬀerentiation, 197
gauge, 418
Legendre, 54, 748
linear, 216
Lorentz, 243–247
matrix
in space, 195
orthogonal, 442

INDEX
831
passive, 178
rigid, 190
transient term, 586
transpose
of a matrix, 181
transposition, 181
properties, 182
triangle inequality, 480
tunneling, 676
uncertainty relation, 699
uniform convergence
Weierstrass M-test, 276
uniformly convergent series, 277–279
union, 782
unit matrix, 180
unit vectors, 5
universal set, 782
partition, 785
Van de Graﬀ, 117
Vandermonde, 210
variable
random
continuous, 801–809
transformation, 804–806
variance, 790, 801
variational problem, 728–740
constraints, 738–740
several dependent variables, 734
several independent variables, 734
soap ﬁlm, 733
vector
Cartesian
component, 216
n-dimensional, 216
column, 177
component, 176
contravariant, 445–447
coordinate system, 16–31
covariant, 445–447
cross product, 7–10
ﬁeld
conservative, 398–404
curl, 391–398
ﬂux, 365–369
G-orthogonal, 219
in space, 199
indices, 439–471
inner product, 182–191, 198–202
plane, 3–10, 174–191
position, 19
row, 181
space, 3–10, 192–207
time derivative, 350–355
transformation, 194–198
transformation of components, 176–
182
transformation properties, 441–445
unit, 5
vector analysis
curvilinear, 423–435
vector ﬁeld
conservative
curl, 400
curl of, 394
divergence, 374
vector potential, 408
vector space, 173, 215–227
velocity, 44
terminal, 559
Venn diagrams, 783
vibrating membrane, 686–687
Vieta, 481
Wallis, 97, 293, 321, 326
Wallis, John
biography, 90
wave equation, 543, 680–687
advanced Green’s function, 712
from Maxwell’s equations, 417
Green’s function, 711–712
retarded Green’s function, 712
wave guide, 682–686
cylindrical, 686
longitudinal part, 682
rectangular, 685
transverse part, 682
weight function, 227
Wheatstone, 382
Wronskian, 566–567
Yukawa potential, 700
zero mass, 250
zero matrix, 180
zero spacetime distance, 242
zeta function, 269

