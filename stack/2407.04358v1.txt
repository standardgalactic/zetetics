An Adaptive Stochastic Gradient Method
with Non-negative Gauss-Newton Stepsizes
Antonio Orvietoâˆ—
Lin Xiaoâ€ 
Abstract
We consider the problem of minimizing the average of a large number of smooth but
possibly non-convex functions. In the context of most machine learning applications, each
loss function is non-negative and thus can be expressed as the composition of a square and
its real-valued square root. This reformulation allows us to apply the Gauss-Newton method,
or the Levenberg-Marquardt method when adding a quadratic regularization. The resulting
algorithm, while being computationally as efficient as the vanilla stochastic gradient method, is
highly adaptive and can automatically warmup and decay the effective stepsize while tracking
the non-negative loss landscape. We provide a tight convergence analysis, leveraging new
techniques, in the stochastic convex and non-convex settings.
In particular, in the convex
case, the method does not require access to the gradient Lipshitz constant for convergence,
and is guaranteed to never diverge. The convergence rates and empirical evaluations compare
favorably to the classical (stochastic) gradient method as well as to several other adaptive
methods.
Contents
1
Introduction
2
1.1
Adaptive stepsizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Contributions and outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
Deterministic setting: derivation and basic properties
6
2.1
Algorithm derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2
Basic properties of NGN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2.1
Non-negative estimation . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.2
Range of NGN stepsize . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.3
Connection with Polyak stepsize . . . . . . . . . . . . . . . . . . . . . . .
9
2.3
Experiments on convex classification problems
. . . . . . . . . . . . . . . . . . .
12
âˆ—ELLIS Institute TÂ¨ubingen, Max Planck Institute for Intelligent Systems, TÂ¨ubingen AI Center, TÂ¨ubingen, Germany.
Work partially carried out at Meta in Seattle, USA. Email: antonio@tue.ellis.eu
â€ Fundamental AI Research (FAIR) at Meta, Seattle, USA. Email: linx@meta.com
1
arXiv:2407.04358v1  [math.OC]  5 Jul 2024

3
A generalized Gauss-Newton perspective
12
4
Stochastic setting: convergence rates and experiments
14
4.1
Preliminary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.2
Convergence rates for fixed regularization . . . . . . . . . . . . . . . . . . . . . .
17
4.2.1
Convex and strongly convex settings . . . . . . . . . . . . . . . . . . . . .
18
4.2.2
Proofs for main results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.2.3
General non-convex setting . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.3
Technique sketch for annealed regularization . . . . . . . . . . . . . . . . . . . . .
29
4.4
Deep learning experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
5
Conclusion and future work
37
1
Introduction
We consider the problem of minimizing the average of a large number of loss functions:
minimize
ğ‘¥âˆˆRğ‘‘
ğ‘“(ğ‘¥) := 1
ğ‘
ğ‘
âˆ‘ï¸
ğ‘–=1
ğ‘“ğ‘–(ğ‘¥),
(1)
where each ğ‘“ğ‘–is assumed to be lower bounded, differentiable and have Lipschitz-continuous
gradients. Specifically, we assume that for each ğ‘–, there exist a constant ğ¿ğ‘–such that
âˆ¥âˆ‡ğ‘“ğ‘–(ğ‘¥) âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¦)âˆ¥â‰¤ğ¿ğ‘–âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥,
âˆ€ğ‘¥, ğ‘¦âˆˆRğ‘‘.
Consequently, the average gradient âˆ‡ğ‘“has a Lipschitz constant ğ¿â‰¤(1/ğ‘) Pğ‘
ğ‘–=1 ğ¿ğ‘–.
In machine learning applications, each ğ‘“ğ‘–corresponds to the loss function associated with a data
point or the average of a mini-batch (e.g., Bottou et al., 2018). For large ğ‘, the cost of frequent
averaging over all data points can be prohibitive, therefore the method of choice (in both convex
and non-convex settings) is often some variant of Stochastic Gradient Descent (SGD):
ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’ğ›¾ğ‘˜âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜),
(2)
where ğ‘–ğ‘˜âˆˆ{1, 2, . . . , ğ‘} =: [ğ‘] is a randomly picked data point or the index of a mini-batch, and
ğ›¾ğ‘˜> 0 is the stepsize (also known as the learning rate) selected at iteration ğ‘˜.
The simplest choice is a constant stepsize ğ›¾ğ‘˜= ğ›¾for all ğ‘˜â‰¥0. If ğ›¾is sufficiently small, say
ğ›¾< 2/ğ¿under the smoothness assumption, then convergence to a neighborhood of a stationary
point can be established (e.g., Ghadimi and Lan, 2013). However, the Lipschitz constant ğ¿is often
unknown and hard to estimate. Under a (deterministic) decreasing stepsize rule such as ğ›¾ğ‘˜= ğ›¾0/
âˆš
ğ‘˜,
convergence to a local minimizer is eventually recovered for any ğ›¾0 > 0 since ğ›¾ğ‘˜< 2/ğ¿after some
iterations. Nevertheless, in practice, tuning ğ›¾0 is still needed to avoid numerical overflows and
instabilities, and to obtain optimal performance. Indeed, Yang et al. (2023) refined the classical
result by Ghadimi and Lan (2013) to show that averaged gradients of the SGD iterates converge to
zero for any ğ›¾0 > 0, but it takes exponential time to recover from a poorly tuned ğ›¾0.
2

1.1
Adaptive stepsizes
Rather than relying on the knowledge of problem-dependent constants such as ğ¿, adaptive stepsizes
exploit additional information from the online iterates ğ‘¥ğ‘˜and âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) to obtain faster convergence.
For example, Kesten (1958) proposed to adjust ğ›¾ğ‘˜based on the sign of the inner product of
consecutive stochastic gradients, an idea further developed by Mirzoakhmedov and Uryasev (1983)
and Delyon and Juditsky (1993). Similar techniques have also been investigated in the machine
learning community (e.g., Jacobs, 1988; Sutton, 1992; Schraudolph, 1999; Mahmood et al., 2012).
AdaGrad and variants.
Stemming from the seminal work of AdaGrad by Duchi et al. (2011),
modern variants of adaptive stochastic gradient methods employ coordinate-wise stepsizes. Specif-
ically, instead of a single stepsize ğ›¾ğ‘˜âˆˆR+, one maintains a vector Î“ğ‘˜âˆˆRğ‘‘
+ and the SGD update
takes the form
ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’Î“ğ‘˜âŠ™âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜),
(3)
where âŠ™denotes the element-wise product of two vectors. Since in this paper we focus on adapting
the scalar stepsize ğ›¾ğ‘˜in (2), itâ€™s more convenient to consider a scalar variant of AdaGrad, called
AdaGrad-norm (Ward et al., 2020), which sets the stepsize in (2) as
ğ›¾ğ‘˜=
ğœ‚
âˆšï¸ƒ
ğ›¿2
0 + Pğ‘˜
ğœ=1 âˆ¥âˆ‡ğ‘“ğ‘–ğœ(ğ‘¥ğœ)âˆ¥2
,
(4)
where ğœ‚and ğ›¿0 are hyper-parameters to be tuned. (The original AdaGrad sets the coordinate-wise
stepsizes Î“ğ‘˜
ğ‘—in (3) by replacing the gradient norm âˆ¥âˆ‡ğ‘“ğ‘–ğœ(ğ‘¥ğœ)âˆ¥in (4) with its ğ‘—th element.) The
cumulative sum across iterations in the denominator of (4) implies that the stepsize decreases
monotonically, a feature shared with simple stepsize rules such as ğ›¾0/
âˆš
ğ‘˜and is key to obtaining
the classical ğ‘‚(1/
âˆš
ğ‘˜) convergence rate in the convex case (Duchi et al., 2011; Reddi et al., 2019).
However, monotonically decreasing stepsizes cannot efficiently navigate complex landscapes with
varying local curvatures and they often lead to very slow convergence in practice.
More sophisticated adaptive methods (e.g., Sutskever et al., 2013) replace âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) in (2) with
the exponential moving average of stochastic gradients (often called momentum). In addition,
Adam (Kingma and Ba, 2014) replaces the cumulative sum in the denominator of (4) with their
exponential moving average (coordinate-wise). In particular, the latter change allows the stepsize
to decrease or increase depending on the local features of the loss landscape, which contributes to
its better performance for training deep-learning models (e.g., Schmidt et al., 2021).
Stochastic Polyak stepsizes.
A classical non-monotone adaptive stepsize rule is the Polyak
stepsize (Polyak, 1987, Â§5.3). For the full gradient method ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’ğ›¾ğ‘˜âˆ‡ğ‘“(ğ‘¥ğ‘˜), it sets
ğ›¾ğ‘˜= ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2,
(5)
where ğ‘“âˆ—= infğ‘¥âˆˆRğ‘‘ğ‘“(ğ‘¥). Here, âˆ‡ğ‘“(ğ‘¥ğ‘˜) can be a subgradient if ğ‘“is nondifferentiable. Although
Polyak (1987) derived it in the context of convex, nonsmooth optimization, Hazan and Kakade
3

(2019) showed that it achieves the optimal convergence rates of gradient descent for minimizing
smooth and/or strongly convex functions as well, without a priori knowledge of the smoothness
and strong convexity parameters. However, a crucial issue of the Polyak stepsize is that the optimal
value ğ‘“âˆ—needs to be known beforehand, which severely limits its application in practical settings.
Nevertheless, there has been, in the last few years, a rising interest in this method in deep
learning, where often ğ‘“âˆ—is zeroâ€”linked to over-parametrization (Zhang et al., 2021). There are
several adaptations to the Polyak stepsize to stochastic gradient methods (Rolinek and Martius,
2018; Berrada et al., 2020; Prazeres and Oberman, 2021), with the most theoretically sound being
the variant of Loizou et al. (2021), proposing the stochastic Polyak stepsize (SPS)
ğ›¾ğ‘˜= min
(
ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜
ğ‘âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2, ğ›¾ğ‘
)
,
(6)
where ğ‘, ğ›¾ğ‘âˆˆRâ‰¥0 are hyperparameters and ğ‘“âˆ—
ğ‘–ğ‘˜= infğ‘¥âˆˆRğ‘‘ğ‘“ğ‘–ğ‘˜(ğ‘¥) is the minimum valued of ğ‘“ğ‘–ğ‘˜.
While ğ‘“ğ‘–ğ‘˜is in principle unknown, in the setting with small batch sizes and no regularization, it is
clear that one can set ğ‘“âˆ—
ğ‘–ğ‘˜= 0. Loizou et al. (2021) provided a thorough analysis of this method,
showing that convergence to a neighborhood of the solution can be achieved without knowledge of
the gradient Lipschitz constant ğ¿.
Even in the over-parametrized setting, as soon as regularization is added, we always have ğ‘“âˆ—
ğ‘–ğ‘˜> 0
and its exact value is hard to estimate1. In addition, a severe limitation of SPS is that the convergence
guarantee in the stochastic setting is for a neighborhood that is independent of the hyperparameters.
As such, no convergence to an arbitrary suboptimality can be guaranteed. Orvieto et al. (2022c)
addressed this problem by proposing DecSPS, a variant that gradually decreases ğ›¾ğ‘in (6) and can
replace ğ‘“âˆ—
ğ‘–ğ‘˜by a lower bound, and proved its convergence to the actual problem solution (i.e., with
arbitrary precision) without knowledge of ğ¿. However, their analysis requires strong convexity to
effectively bound the iterates.
Other related work.
Another related line of work is on parameter-free methods for online
optimization (Orabona and PÂ´al, 2016; Orabona and Tommasi, 2017; Cutkosky, 2020) and the D-
adaptation methods (Defazio and Mishchenko, 2023; Mishchenko and Defazio, 2024). Given a
fixed number of steps to run, Zamani and Glineur (2023) showed that the subgradient method with
a linearly decaying stepsize enjoys last-iterate convergence, and Defazio et al. (2023) extended the
result to stochastic gradient methods with an online-to-batch conversion. Very recently, Defazio
et al. (2024) proposed a method that combines the Polyak-Ruppert averaging with momentum and
demonstrated promising performance gains for both convex optimization and deep learning.
1.2
Contributions and outline
In this paper, we propose a new adaptive stepsize strategy that shows promising performance
compared to previous approaches both in theory and in practice. The basic idea is to exploit the
1If the problem is overparametrized (ğ‘‘â‰«ğ‘), often ğ‘“(ğ‘¥) = 0 can be achieved by some ğ‘¥âˆ—âˆˆRğ‘‘. However there
exist no ğ‘¥such that, e.g. ğ‘“(ğ‘¥) + ğœ†
2 âˆ¥ğ‘¥âˆ¥2 = 0 (L2 regularization). Loizou et al. (2021) discusses this issue.
4

Method
Adaptive
Features
Non-monotonic
$k
Convergence to
Arbitrary Suboptimality
No Additional
Assumptions
SGD
7
7
3
3
Adagrad
3
7
3
3
SPS
3
3
7
3
DecSPS
3
3
3
7
NGN
3
3
3
3
Table 1: Convergence features of SGD and some adaptive methods with convergence guarantees
in stochastic convex smooth optimization. Discussion in Sections 1.1 and 1.2.
nonnegativity of the loss functions (ubiquitous in machine learning) by expressing them as the
composition of a square and their real-valued square roots. This reformulation allows us the apply
the classical Gauss-Newton method (e.g., Nocedal and Wright, 2006, Â§10.3), or the Levenberge-
Marquardt method when adding a quadratic regularization (Levenberg, 1944; Marquardt, 1963).
By minimizing the Nonnegative Gauss-Newton (NGN) estimates of ğ‘“ğ‘–ğ‘˜, we derive the NGN stepsize
ğ›¾ğ‘˜=
ğœ
1 +
ğœ
2 ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2,
(7)
where ğœ> 0 is a regularization parameter.
In Section 2, we focus on the deterministic setting (when the full gradient âˆ‡ğ‘“(ğ‘¥ğ‘˜) is available
or ğ‘= 1) to derive the NGN stepsize and study its basic properties:
â€¢ Our derivation reveal that NGN has a clear connection to second-order methods, while
sharing the same low computational cost as SGD (Section 2.1).
â€¢ NGN is stable to the selection of its hyperparameter ğœ: empirically, non-diverging dynamics
are observed for any choice of ğœâ€”a feature that sets the method drastically apart from vanilla
SGD that we prove precisely in the convex stochastic setting (Section 2.2).
â€¢ NGN interpolates between an agnostic version of the Polyak stepsize (setting ğ‘“âˆ—= 0) and a
small constant stepsize (Section 2.2.3), leading to an adaptive non-monotonic behavior that
can effectively warm-up in the beginning and then settle down near the solution (Section 2.3).
In Section 3, we present an alternative derivation of NGN from a generalized Gauss-Newton
perspective, further revealing its curvature adaptation property. In Section 4, we present our main
results on convergence analysis in the stochastic setting:
â€¢ For minimizing smooth convex loss functions (Section 4.2.1), we show that NGN stepsize
can converge to a neighborhood of arbitrary size around the solution2 without knowledge of
2For SPS Loizou et al. (2021) the convergence neighborhood is independent of the hyperparameters.
5

the gradient Lipschitz constant and without assumptions such as bounded domain or iterates.
The method also provably never diverges. Compared to SPS, it behaves much more favorably
under no knowledge of ğ‘“âˆ—or the single ğ‘“âˆ—
ğ‘–ğ‘˜s. Compared to Adagrad variants, our analysis can
be extended to the strongly convex setting with fully-adaptive linear rate.
â€¢ For non-convex smooth optimization (Section 4.2.3), NGN stepsize is guaranteed to converge
at the same speed as SGD with constant stepsize. In both the convex and non-convex settings,
we can gradually decrease the hyperparameter ğœand obtain asymptotic convergence in the
classical sense (Section 4.3).
â€¢ In Section 4.4, we conduct extensive numerical experiments on deep learning tasks. We found
that NGN stepsize has clear advantage over SGD with constant stepsize, SPS, and AdaGrad-
norm in terms of convergence speed and robustness against varying hyperparameters. While
our main focus is on adaptive scalar stepsizes for the basic SGD method (2), we also conduct
experiments to compare with the popular Adam method (Kingma and Ba, 2014), which uses
both momentum and coordinate-wise stepsizes. Our results show that NGN performs better
than Adam in minimizing the training loss and is more robust to hyperparameter tuning, not
to mention that NGN takes much less memory to implement.
In Section 5, we conclude the paper and point to several interesting directions for future work.
2
Deterministic setting: derivation and basic properties
We derive our algorithm when the full gradient âˆ‡ğ‘“(ğ‘¥ğ‘˜) is available (Section 2.1), which corresponds
to the case of ğ‘= 1 in (1) or when ğ‘is relatively small. Most of our geometric intuition about
NGN is presented in the deterministic setting, in Section 2.2. In Section 3, we provide a more
general perspective related to the generalized Gauss-Newton method.
2.1
Algorithm derivation
Assume ğ‘“: Rğ‘‘â†’R is differentiable and non-negative, i.e.,
ğ‘“âˆ—:= inf
ğ‘¥âˆˆRğ‘‘ğ‘“(ğ‘¥) â‰¥0.
We define ğ‘Ÿ(ğ‘¥) :=
âˆšï¸
ğ‘“(ğ‘¥) and consequently have ğ‘“(ğ‘¥) = ğ‘Ÿ2(ğ‘¥). Therefore,
âˆ‡ğ‘“(ğ‘¥) = 2ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥)
and
âˆ‡ğ‘Ÿ(ğ‘¥) =
1
2
âˆšï¸
ğ‘“(ğ‘¥)
âˆ‡ğ‘“(ğ‘¥).
The Gauss-Newton update leverages a first-order Taylor expansions of ğ‘Ÿ(ğ‘¥+ ğ‘) around ğ‘¥:
ğ‘“(ğ‘¥+ ğ‘) = ğ‘Ÿ2(ğ‘¥+ ğ‘) â‰ƒ ğ‘Ÿ(ğ‘¥) + âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤ğ‘2 .
We use this approximation to estimate ğ‘“(ğ‘¥+ ğ‘) around ğ‘¥and propose an update ğ‘by minimizing
Ëœğ‘“
NGN
ğœ
(ğ‘¥+ ğ‘) :=  ğ‘Ÿ(ğ‘¥) + âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤ğ‘2 + 1
2ğœâˆ¥ğ‘âˆ¥2,
(8)
6

where âˆ¥Â· âˆ¥is the Euclidean norm and
1
2ğœâˆ¥ğ‘âˆ¥2 is a regularization term expressing the confidence
ğœ> 0 in the estimate Ëœğ‘“NGN
ğœ
. Crucially, we note that this approximation preserves non-negativity, in
contrast to the Taylor expansion of ğ‘“(Chen, 2011). The minimizer of Ëœğ‘“NGN
ğœ
(ğ‘¥+ ğ‘) with respect to
ğ‘can be found by setting âˆ‡ğ‘Ëœğ‘“NGN
ğœ
(ğ‘¥+ ğ‘) = 0, i.e.,
âˆ‡ğ‘Ëœğ‘“
NGN
ğœ
(ğ‘¥+ ğ‘) = 2  ğ‘Ÿ(ğ‘¥) + âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤ğ‘ âˆ‡ğ‘Ÿ(ğ‘¥) + 1
ğœğ‘
= 2ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥) +

2âˆ‡ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤+ 1
ğœğ¼

ğ‘!= 0,
where ğ¼is the ğ‘‘Ã— ğ‘‘identity matrix. Therefore ğ‘âˆˆRğ‘‘needs to satisfy the normal equation

âˆ‡ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤+ 1
2ğœğ¼

ğ‘= âˆ’ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥).
This equation can be solved analytically using the Sherman-Morrison formula3:
ğ‘= âˆ’

âˆ‡ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥)âŠ¤+ 1
2ğœğ¼
âˆ’1
ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥) = âˆ’2ğœğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥)
1 + 2ğœâˆ¥âˆ‡ğ‘Ÿ(ğ‘¥)âˆ¥2 .
Now recall that ğ‘Ÿ(ğ‘¥) =
âˆšï¸
ğ‘“(ğ‘¥) and âˆ‡ğ‘Ÿ(ğ‘¥) =
1
2âˆš
ğ‘“(ğ‘¥)âˆ‡ğ‘“(ğ‘¥), so that âˆ¥âˆ‡ğ‘Ÿ(ğ‘¥)âˆ¥2 =
1
4 ğ‘“(ğ‘¥) âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2 and
ğ‘Ÿ(ğ‘¥)âˆ‡ğ‘Ÿ(ğ‘¥) = 1
2âˆ‡ğ‘“(ğ‘¥). Therefore, we get
ğ‘
NGN
ğœ
= âˆ’
ğœâˆ‡ğ‘“(ğ‘¥)
1 +
ğœ
2 ğ‘“(ğ‘¥) âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2 .
(9)
The derivation above suggests the update rule ğ‘¥â†ğ‘¥+ ğ‘NGN
ğœ, leading to the deterministic NGN
(NGN-det) method:
ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’
ğœ
1 +
ğœ
2 ğ‘“(ğ‘¥ğ‘˜) âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 âˆ‡ğ‘“(ğ‘¥ğ‘˜).
(10)
It is a form of gradient descent ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’ğ›¾ğ‘˜âˆ‡ğ‘“(ğ‘¥ğ‘˜) with the adaptive NGN stepsize
ğ›¾
NGN
ğ‘˜
:=
ğœ
1 +
ğœ
2 ğ‘“(ğ‘¥ğ‘˜) âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 .
(11)
In the rest of this section, we study the basic properties and convergence guarantees of NGN-det.
2.2
Basic properties of NGN
We give below an overview of the properties of the NGN stepsizes, here discussed in the determinis-
tic setting but with direct application in the stochastic case. Some of these properties constitute the
workhorse of our convergence analysis, in Section 4. However, we do not provide the convergence
rates in the deterministic setting here because they can be obtained from the results in Section 4 by
setting ğ‘= 1.
3Let ğ´be an invertible matrix in Rğ‘‘Ã—ğ‘‘and ğ‘¢, ğ‘£âˆˆRğ‘‘, then (ğ´+ ğ‘¢ğ‘£âŠ¤)âˆ’1 = ğ´âˆ’1 âˆ’ğ´âˆ’1ğ‘¢ğ‘£âŠ¤ğ´âˆ’1
1+ğ‘£âŠ¤ğ´âˆ’1ğ‘¢.
7

Î³
Î³
Î³
Figure 1: NGN and GD updates and corresponding objective function estimate in Eqs. (12) and (13)
on a few toy examples (inspired by Chen (2011)). The black dot denotes the initial ğ‘¥, ald the star is
the position after one step: ğ‘¥+ ğ‘. Compared to GD with stepsize ğ›¾= ğœ, NGN is more conservative
if the landscape is sharp. Note that the function approximation provided by NGN is always non-
negative, as clear from our motivation and the algorithm derivation.
2.2.1
Non-negative estimation
The NGN update ğ‘NGN
ğœ(ğ‘¥) in (9) leverages a local first-order non-negative estimate of the loss ğ‘“:
ğ‘
NGN
ğœ(ğ‘¥) = arg min
ğ‘âˆˆRğ‘‘
"
Ëœğ‘“
NGN
ğœ
(ğ‘¥+ ğ‘) :=
âˆšï¸
ğ‘“(ğ‘¥) +
1
2
âˆšï¸
ğ‘“(ğ‘¥)
âˆ‡ğ‘“(ğ‘¥)âŠ¤ğ‘
2
+ 1
2ğœâˆ¥ğ‘âˆ¥2
#
.
(12)
This feature is fundamental to NGN and draws a clear distinction from vanilla gradient descent (GD):
indeed, the update ğ‘¥â†ğ‘¥+ ğ‘GD of GD with constant stepsize ğ›¾is the result of minimization of a
first-order estimate of ğ‘“which is not necessarily non-nagative (as opposed to the original ğ‘“):
ğ‘
GD
ğ›¾(ğ‘¥) = arg min
ğ‘âˆˆRğ‘‘

Ëœğ‘“
GD
ğ›¾(ğ‘¥+ ğ‘) := ğ‘“(ğ‘¥) + âˆ‡ğ‘“(ğ‘¥)âŠ¤ğ‘+ 1
2ğ›¾âˆ¥ğ‘âˆ¥2

= âˆ’ğ›¾âˆ‡ğ‘“(ğ‘¥).
(13)
The distinction outlined above can be visualized on toy examples in Figure 1. It is clear from
these examples that the NGN update, minimizing a non-negative first-order estimate of ğ‘“, is more
conservative compared to GD with stepsize ğ›¾= ğœ(the NGN regularization hyperparameter),
especially in regions where gradients are large. The stepsize range of NGN will be characterized
more precisely in Section 2.2.2, and is linked to the adaptive nature of our method, described later in
Section 2.2.3. We note that it is precisely this conservative nature that allows the convergence rates
in Section 4 to hold for an arbitrarily large ğœâ€”in contrast to gradient descent and most adaptive
methods, which diverge for large stepsize parameters.
2.2.2
Range of NGN stepsize
Suppose that ğ‘“: Rğ‘‘â†’R is ğ¿-smooth and has minimum value ğ‘“âˆ—. Then for all ğ‘¥âˆˆRğ‘‘, we have
(e.g., Nesterov, 2018, Â§2.1).
2ğ¿( ğ‘“(ğ‘¥) âˆ’ğ‘“âˆ—) â‰¥âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2.
8

Since ğ‘“âˆ—â‰¥0, we obviously have 2ğ¿ğ‘“(ğ‘¥) â‰¥âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2, which is equivalent to
0 â‰¤âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2
2 ğ‘“(ğ‘¥)
â‰¤ğ¿.
These bounds directly imply a range for ğ›¾ğ‘˜, as characterized in the following lemma.
Lemma 2.1 (Stepsize bounds). Suppose ğ‘“: Rğ‘‘â†’R is non-negative, differentiable and ğ¿-smooth.
Then the NGN-det stepsize given in (11) satisfies
ğ›¾
NGN
ğ‘˜
âˆˆ
h
ğœ
1 + ğœğ¿, ğœ
i
=

1
ğ¿+ ğœâˆ’1, ğœ

.
(14)
This property shows that the maximum achievable stepsize is bounded by ğœ, but the algorithm
can adaptively decrease it until 1/(ğ¿+ ğœâˆ’1) if the landscape gets more challenging.
2.2.3
Connection with Polyak stepsize
The reader can readily spot a connection between the NGN-det stepsize (11) and the Polyak
stepsize (5). To be more precise, given ğ‘“âˆ—â‰¥0, we define the ğ‘“âˆ—-agnostic Polyak stepsize (APS)
ğ›¾
APS
ğ‘˜
=
ğ‘“(ğ‘¥ğ‘˜)
âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 .
(15)
We can express the NGN stepsize as the harmonic mean of APS and the constant stepsize ğœ/2:
ğ›¾
NGN
ğ‘˜
=
2
2
ğœ+ âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2
ğ‘“(ğ‘¥ğ‘˜)
=
2
1
ğœ/2 +
1
ğ›¾APS
ğ‘˜
.
Intuitively, as ğœincreases (less regularization in (8)) , NGN relies more on the Gauss-Newton
approximation. In the limiting case of ğœâ†’âˆ, we have ğ›¾NGN
ğ‘˜
â†’2ğ›¾APS
ğ‘˜, which is two times the
APS. On the other hand, as ğœâ†’0, the regularization in (8) dominates the approximation and we
get back to gradient descent with a small constant stepsize ğ›¾ğ‘˜= ğœ. As such,
NGN interpolates between the ğ‘“âˆ—-agnostic Polyak stepsize and a small constant stepsize.
We like to spend a few more words on the insightful connection between PS and NGN. As
already mentioned, one obvious drawback of PS, persisting in its stochastic variant SPS (Loizou
et al., 2021), is the required knowledge of ğ‘“âˆ—= infğ‘¥ğ‘“(ğ‘¥). However, Orvieto et al. (2022c) showed
that convergence to a ball around the solution, of size proportional to ğ‘“âˆ—, can be retrieved both in the
deterministic and stochastic setting by replacing ğ‘“âˆ—with any lower bound â„“âˆ—â‰¤ğ‘“âˆ—â€”in particular
â„“âˆ—= 0, which becomes NGN with ğœâ†’âˆ. It is therefore interesting to compare NGN with the
following PS variant inspired by the SPSmax rule (Loizou et al., 2021; Orvieto et al., 2022c):
ğ›¾
APSmax
ğ‘˜
= min

ğ‘“(ğ‘¥ğ‘˜)
ğ‘âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2, ğ›¾ğ‘

,
(16)
9

Figure 2: Optimization dynamics of constant-stepsize GD, NGN, and APSmax on the toy example
ğ‘“(ğ‘¥) = ğœ†
2 (ğ‘¥âˆ’ğ‘¥âˆ—)2 + ğ‘“âˆ—, for different hyperparameter values. NGN is stable for any finite ğœ> 0.
Dashed line in the bottom row is the value 2/ğœ†.
where ğ‘> 0 is a hyperparameter and ğ›¾ğ‘> 0 bounds the maximum stepsize achievable. For this
purpose, we consider the simplest convex optimization setting with a one-dimensional quadratic
ğ‘“(ğ‘¥) = ğœ†
2 (ğ‘¥âˆ’ğ‘¥âˆ—)2 + ğ‘“âˆ—, where ğ‘“âˆ—= 0.1 > 0 and ğœ†= 1.2. Figure 2 shows the loss dynamics for GD,
NGN and APSmax with ğ›¾ğ‘= 3 (results are not sensitive to ğ›¾ğ‘when it is big enough). Ablating on
different values of ğœfor NGN, ğ‘for APSmax and ğ›¾for constant-stepsize GD, one notices a striking
feature of NGN: its iterates never grow unbounded for any value of ğœ. Instead, for ğ›¾big enough
or ğ‘small enough, GD and APSmax grow unbounded. This property of NGN is also observed in
our experiments on convex classification (Fig. 3) and neural networks (Fig. 5), and we will give
it a formal proof in Section 4. If ğ‘has a high enough (ğ¿-independent) value, then Orvieto et al.
(2022c) showed that SPSmax also does not grow unbounded. However, it cannot obtain an arbitrary
suboptimality (by tuning the hyperparameters of ğ›¾
APSmax
ğ‘˜
)â€”which instead can be achieved by NGN
in this setting, as shown by our theory in Section 4. This issue of SPSmax is related to its bias
problem, thoroughly explored in Orvieto et al. (2022c).
From Figure 2, we also observe another important property of NGN with large ğœ: its stepsize
converges to a value bounded by 2/ğœ†â€”the curvature-informed stability threshold for the corre-
sponding dynamics. We remark that both ğœ†and ğ‘“âˆ—do not appear in the NGN update rule. This
shows a remarkable curvature adaptation property of NGN, which is linked to the derivation in
Section 3. In contrast, APSmax, while in perfect agreement with the findings in Orvieto et al.
(2022c) converges to a ball around the solution for big enough hyperparameter ğ‘, its performance
is negatively affected by having no access to ğ‘“âˆ—(cf. Figure 3).
10

Dataset
Features Dimension
# Datapoints
# Classes
L2 Regularization
Cancer
30
569
2
1 Ã— 10âˆ’4
Wine
13
178
3
1 Ã— 10âˆ’4
Digits
64
1797
10
1 Ã— 10âˆ’4
Table 2: LIBSVM (Chang and Lin, 2011) dataset information, results in Figure 3.
Armijo
Armijo
Armijo
Figure 3: Deterministic NGN compared with constant-stepsize Gradient Descent, Polyak Stepsizes,
and Armijo Line search on three classification datasets listed in Table 2. The center column shows
the optimality gap ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—, the right column plots the evolution of stepsizes, and the right
column shows the projection of the iterate trajectories onto the top-two PCA components. On the
trajectories plot, the circle denotes the starting point, the star denotes the solution found at the last
iteration, and the square represents the point after one iteration.
11

2.3
Experiments on convex classification problems
In Figure 3, we evaluate the full-batch training performance of NGN-det on convex classifica-
tion (using the cross-entropy loss). Specifically, we consider three classification datasets from
LIBSVM (Chang and Lin, 2011), whose details are given in Table 2.
We compare NGN with constant-stepsize gradient descent (GD), standard Polyak stepsize (PS)
with or without knowledge of ğ‘“âˆ—> 0, and GD with Armijo line search (LS). For GD and NGN-det,
we sweep over hyperparameters (ğ›¾and ğœ) on a logarithmic grid and denote the best-performing
option with marker . Performace for 3-times-lower and 3-times-higher hyperparameters is marked
by symbols
and
, respectively. For all PS variants, we select ğ‘= 1 (see Section 2.2.3), as
suggested by the results in Loizou et al. (2021).
In Figure 3, we plot the loss function optimality gap, the stepsize, as well as the projection
of the iterate dynamics onto the top-two principle components. They clearly showcase NGNâ€™s
adaptivity to large gradients at the beginning and its stability to high values of ğœ(cf. Section 2.2.3).
Moreover, the dynamics of the NGN stepsize exhibit a warmup phase followed by convergence
to a flat value towards the end (ğ›¾ğ‘˜â†’ğœif ğœis not too large); see bounds in Section 2.2.2. A
similar behavior is observed for the line-search stepsize (LS) and for PS using the correct value
of ğ‘“âˆ—(here ğ‘“âˆ—> 0 because we have more datapoints than dimensions; see Table 2). Of course, in
practice ğ‘“âˆ—is not accessible beforehand. Plugging in the lower bound ğ‘“âˆ—= 0 still guarantees PS
approaches the minimal loss (see discussion in Section 2.2.3), but the iterates do not converge to
the true solutionâ€”in contrast to NGN (See Figure 2 in Orvieto et al. (2022a)).
3
A generalized Gauss-Newton perspective
Before moving on to the stochastic case and presenting the convergence analysis, we provide here
an alternative derivation of the NGN methodâ€”with increased generality compared to the one given
in Section 2.1. This generalized Gauss-Newton (GNN) perspective allows us to make a closer
connection to second-order methods and better understand the curvature adaptation property of
NGN demonstrated in Figures 2 and 3.
Suppose ğ‘“: Rğ‘‘â†’R can be written as the composition of two functions: ğ‘“(ğ‘¥) = â„(ğ‘(ğ‘¥)),
where â„: R â†’R is differentiable and nonnegative and ğ‘: Rğ‘‘â†’R is differentiable. In the setting
of Section 2.1, we have â„(ğ‘) = ğ‘2 and ğ‘(ğ‘¥) = ğ‘Ÿ(ğ‘¥) =
âˆšï¸
ğ‘“(ğ‘¥). The gradient of ğ‘“is
âˆ‡ğ‘“(ğ‘¥) = â„â€²(ğ‘(ğ‘¥))âˆ‡ğ‘(ğ‘¥),
and its Hessian can be written as
âˆ‡2 ğ‘“(ğ‘¥) = â„â€²(ğ‘(ğ‘¥))âˆ‡2ğ‘(ğ‘¥) + â„â€²â€²(ğ‘(ğ‘¥))âˆ‡ğ‘(ğ‘¥)âˆ‡ğ‘(ğ‘¥)âŠ¤
= â„â€²(ğ‘(ğ‘¥))âˆ‡2ğ‘(ğ‘¥)
|             {z             }
Difficult to compute
+ â„â€²â€²(ğ‘(ğ‘¥))
â„â€²(ğ‘(ğ‘¥))2 âˆ‡ğ‘“(ğ‘¥)âˆ‡ğ‘“(ğ‘¥)âŠ¤
|                          {z                          }
Easy to compute
,
(17)
12

where in the last equality we applied the substitution âˆ‡ğ‘(ğ‘¥) = âˆ‡ğ‘“(ğ‘¥)/â„â€²(ğ‘(ğ‘¥)).
Notice that
â„â€²(ğ‘(ğ‘¥)) = 0 if and only if â„is minimized at ğ‘(ğ‘¥) and thus ğ‘“is minimized at ğ‘¥. Therefore we can
assume without loss of generality that â„â€²(ğ‘(ğ‘¥)) â‰ 0.
We have seen in (13) that the GD update ğ‘GD
ğœ
minimizes the first-order approximation Ëœğ‘“GD
ğœ
.
Similarly, the canonical second-order method, Newtonâ€™s method, derive the update as
ğ‘Newton = arg min
ğ‘

Ëœğ‘“Newton(ğ‘¥+ ğ‘) := ğ‘“(ğ‘¥) + âˆ‡ğ‘“(ğ‘¥)âŠ¤ğ‘+ 1
2 ğ‘âŠ¤âˆ‡2 ğ‘“(ğ‘¥)ğ‘

,
where âˆ‡2 ğ‘“(ğ‘¥) is given in (17). In order to avoid the â€œdifficult-to-computeâ€ part of the Hessian, the
generalized Gauss-Newton (GGN) method replaces it with a simple approximation 1
ğœğ¼:
Ëœğ‘“GGN
ğœ
(ğ‘¥+ ğ‘) := ğ‘“(ğ‘¥) + âˆ‡ğ‘“(ğ‘¥)âŠ¤ğ‘+ 1
2 ğ‘âŠ¤
 1
ğœğ¼+ ğ‘(ğ‘¥)âˆ‡ğ‘“(ğ‘¥)âˆ‡ğ‘“(ğ‘¥)âŠ¤

ğ‘.
(18)
where
ğ‘(ğ‘¥) := â„â€²â€²(ğ‘(ğ‘¥))
â„â€²(ğ‘(ğ‘¥))2 .
The GGN update ğ‘GGN is the minimizer of Ëœğ‘“GGN
ğœ
(ğ‘¥+ ğ‘), which we obtain by forcing its gradient
with respect to ğ‘to be zero:
âˆ‡ğ‘Ëœğ‘“GGN
ğœ
(ğ‘¥+ ğ‘) = âˆ‡ğ‘“(ğ‘¥) +
 1
ğœğ¼+ ğ‘(ğ‘¥)âˆ‡ğ‘“(ğ‘¥)âˆ‡ğ‘“(ğ‘¥)âŠ¤

ğ‘!= 0,
(19)
and it results in
ğ‘GGN = âˆ’
ğœ
1 + ğœğ‘(ğ‘¥)âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 âˆ‡ğ‘“(ğ‘¥ğ‘˜).
(20)
Next we give several examples of the GGN update by choosing a specific function â„.
â€¢ Quadratic. In this case, we have â„(ğ‘) = ğ‘2, â„â€²(ğ‘) = 2ğ‘and â„â€²â€²(ğ‘) = 2, thus
ğ‘(ğ‘¥) =
1
ğ‘(ğ‘¥)2 =
1
2 ğ‘“(ğ‘¥),
and the GGN update (20) becomes the NGN update (9). In this case, the GGN approxima-
tion (18) reduces exactly to the NGN approximation (12).
â€¢ Monomial. For a generic ğ›¼-scaled monomial â„(ğ‘) = ğ›¼
ğ‘ğ‘ğ‘we have â„â€²(ğ‘) = ğ›¼ğ‘ğ‘âˆ’1 and
â„â€²â€²(ğ‘) = ğ›¼(ğ‘âˆ’1)ğ‘ğ‘âˆ’2. Therefore,
â„â€²â€²(ğ‘)
â„â€²(ğ‘)2 = ğ›¼(ğ‘âˆ’1)ğ‘ğ‘âˆ’2
ğ›¼2ğ‘2ğ‘âˆ’2
= ğ‘âˆ’1
ğ›¼ğ‘ğ‘=
1
ğ‘
ğ‘âˆ’1â„(ğ‘)
=â‡’
ğ‘(ğ‘¥) =
1
ğ‘
ğ‘âˆ’1 ğ‘“(ğ‘¥) .
For ğ‘= 2, we get back to the NGN update.
13

â€¢ Negative logarithm. If â„(ğ‘¦) = âˆ’log(ğ‘¦), with ğ‘¦âˆˆ(0, 1) being the model likelihood, then
â„â€²(ğ‘¦) = âˆ’1/ğ‘¦and â„â€²â€²(ğ‘¦) = 1/ğ‘¦2, therefore ğ‘(ğ‘¥) = 1 and we have
ğ‘¥ğ‘˜+1 = ğ‘¥ğ‘˜âˆ’
ğœ
1 + ğœâˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ¥2 âˆ‡ğ‘“(ğ‘¥).
This can be considered as the scalar case of the diagonal Levenberg-Marquardt method when
using the log-likelihood loss (Le Cun et al., 1998, Â§9.1).
In our empirical study, the choice â„(ğ‘) = ğ‘2 works best among the monomials with different
exponent ğ‘and also performs slightly better than the negative logarithm. In the rest of this paper,
we focus on the quadratic â„for convergence analysis and presenting the experiment results.
4
Stochastic setting: convergence rates and experiments
In this section, we study the application of NGN to stochastic optimization.
Specifically, we
consider the loss function in (1) and assume that each ğ‘“ğ‘–: Rğ‘‘â†’R is non-negative and ğ¿ğ‘–-smooth.
In this section, we let ğ¿= maxğ‘–âˆˆ[ğ‘] ğ¿ğ‘–instead of the average of ğ¿ğ‘–as in the deterministic setting;
alternatively, we can simply assume that each ğ‘“ğ‘–has the same smoothness constant ğ¿. We consider
the generic SGD method (2) where the stepsize ğ›¾ğ‘˜is obtained by applying the NGN method to the
current, randomly picked function ğ‘“ğ‘–ğ‘˜, i.e.,
ğ›¾ğ‘˜=
ğœ
1 +
ğœ
2 ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 .
(21)
All of our results are presented in the stochastic setting, and the convergence rates for the determin-
istic setting can be recovered by simply taking ğ‘= 1.
4.1
Preliminary lemmas
We present here a few fundamental lemmas, heavily used both in the constant ğœsetting (Section 4.2)
and in the decreasing ğœcase (Section 4.3).
Lemma 4.1 (Fundamental Equality). The NGN stepsize ğ›¾ğ‘˜in (21) satisfies
ğ›¾ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥)âˆ¥2 = 2
ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“ğ‘–ğ‘˜(ğ‘¥).
Proof. The definition of our stepsize implies

1 +
ğœ
2 ğ‘“ğ‘–ğ‘˜(ğ‘¥) âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥)âˆ¥2

ğ›¾ğ‘˜= ğœ,
which one can rewrite as
ğœ
2 ğ‘“ğ‘–ğ‘˜(ğ‘¥) âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥)âˆ¥2ğ›¾ğ‘˜= ğœâˆ’ğ›¾ğ‘˜.
This proves the result.
â–¡
14

Lemma 4.2 (Fundamental Inequality). Let each ğ‘“ğ‘–: Rğ‘‘â†’R be non-negative, ğ¿-smooth and
convex. Considering the NGN stepsize ğ›¾ğ‘˜as in (21), we have
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Â· ğ‘“âˆ—
ğ‘–ğ‘˜.
(22)
For ğœâ‰¤1/(2ğ¿) or ğ‘“âˆ—
ğ‘–ğ‘˜= 0 we have no error resulting from the last term above.
Proof. Lemma 2.1&4.1 imply the following relations:
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤2ğ¿ğ›¾2
ğ‘˜( ğ‘“ğ‘–ğ‘˜(ğ‘¥) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜),
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 = 2ğ›¾ğ‘˜
ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜).
They further imply that for any ğ›¿âˆˆ(âˆ’âˆ, 1], we have
(1 âˆ’ğ›¿)ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤2(1 âˆ’ğ›¿)ğ¿ğ›¾2
ğ‘˜( ğ‘“ğ‘–ğ‘˜(ğ‘¥) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜),
ğ›¿ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 = 2ğ›¿ğ›¾ğ‘˜
ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜).
Summing the two inequalities above, we get
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤2ğ›¿ğ›¾ğ‘˜
ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) + 2(1 âˆ’ğ›¿)ğ¿ğ›¾2
ğ‘˜( ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜),
and after collecting a few terms,
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤2ğ›¾ğ‘˜
h
ğ›¿
ğœâˆ’ğ›¾ğ‘˜
ğœ

+ (1 âˆ’ğ›¿)ğ¿ğ›¾ğ‘˜
i
|                              {z                              }
ğ´(ğ›¿)
[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğ›¾ğ‘˜ğ›¿
ğœâˆ’ğ›¾ğ‘˜
ğœ

|             {z             }
ğµ(ğ›¿)
ğ‘“âˆ—
ğ‘–ğ‘˜.
(23)
We would like to choose ğ›¿to somehow minimizes ğ´(ğ›¿) and ğµ(ğ›¿) simultaneously. It turns out that
a convenient choice is (see the remark after the proof)
ğ›¿= 2ğœğ¿âˆ’1
2ğœğ¿+ 1 .
(24)
Note that since 2ğœğ¿> 0, ğ›¿is a real number in the range [âˆ’1, 1] (which is a valid subset of [âˆ’âˆ, 1]).
Now consider the first term ğ´(ğ›¿) in (23). We have
ğ´(ğ›¿) = 2ğœğ¿âˆ’1
2ğœğ¿+ 1

1 âˆ’ğ›¾ğ‘˜
ğœ

+
2ğ¿ğ›¾ğ‘˜
2ğœğ¿+ 1
= 2ğœğ¿âˆ’1
2ğœğ¿+ 1 âˆ’2ğœğ¿âˆ’1
2ğœğ¿+ 1
ğ›¾ğ‘˜
ğœ+
2ğ¿ğ›¾ğ‘˜
2ğœğ¿+ 1
= 2ğœğ¿âˆ’1
2ğœğ¿+ 1 +
ğ›¾ğ‘˜
(2ğœğ¿+ 1)ğœ
â‰¤
2ğœğ¿
2ğœğ¿+ 1,
15

where in the last line we used the property ğ›¾ğ‘˜â‰¤ğœ. The bound (23) becomes:
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğ›¾ğ‘˜
2ğœğ¿âˆ’1
2ğœğ¿+ 1
 ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“âˆ—
ğ‘–ğ‘˜.
To bound ğµ(ğ›¿), i.e., the second term on the right side of the above inequality, we have two cases:
â€¢ If ğœâ‰¤1/(2ğ¿) then ğ›¿= 2ğœğ¿âˆ’1
2ğœğ¿+1 is negatkve, and we have
2ğ›¾ğ‘˜
2ğœğ¿âˆ’1
2ğœğ¿+ 1
 ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“âˆ—
ğ‘–ğ‘˜â‰¤0,
(25)
since the worst case is ğ›¾ğ‘˜= ğœ.
â€¢ Otherwise, ğ›¿> 0 and we can proceed as follows, using the fact that ğ›¾ğ‘˜âˆˆ

ğœ
1+ğœğ¿, ğœ

2ğ›¾ğ‘˜
2ğœğ¿âˆ’1
2ğœğ¿+ 1
 ğœâˆ’ğ›¾ğ‘˜
ğœ

ğ‘“âˆ—
ğ‘–ğ‘˜â‰¤2ğœ
2ğœğ¿âˆ’1
2ğœğ¿+ 1
 ğœâˆ’
ğœ
1+ğœğ¿
ğœ

ğ‘“âˆ—
ğ‘–ğ‘˜
= 2ğœ
2ğœğ¿âˆ’1
2ğœğ¿+ 1
 
ğœğ¿
1 + ğœğ¿

ğ‘“âˆ—
ğ‘–ğ‘˜
= 2ğœ2ğ¿
1 + ğœğ¿
2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜,
All in all, considering both cases, we get the following upper bound for the error term ğµ(ğ›¿):
2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Â· ğ‘“âˆ—
ğ‘–ğ‘˜.
(26)
This concludes the proof.
â–¡
Remark (On the choice of ğ›¿in (24)). In the context of the proof of Lemma 4.2, let us assume we
want to find ğ›¿such that ğ´(ğ›¿) â‰¤ğ›¼. That implies
ğ›¿
ğœâˆ’ğ›¾ğ‘˜
ğœ

+ (1 âˆ’ğ›¿)ğ¿ğ›¾ğ‘˜â‰¤ğ›¼
â‡â‡’ğ›¿ğœ+ [(1 âˆ’ğ›¿)ğ¿ğœâˆ’ğ›¿] ğ›¾ğ‘˜â‰¤ğ›¼ğœ
â‡â‡’[(1 âˆ’ğ›¿)ğ¿ğœâˆ’ğ›¿] ğ›¾ğ‘˜â‰¤(ğ›¼âˆ’ğ›¿)ğœ
â‡â‡’[(1 âˆ’ğ›¿)ğ¿ğœâˆ’ğ›¿] â‰¤(ğ›¼âˆ’ğ›¿) ğœ
ğ›¾ğ‘˜
.
For ğ›¼â‰¥ğ›¿, the right-hand side is positive. Further, note that ğœ
ğ›¾ğ‘˜âˆˆ[1, 1 + ğœğ¿]. So if we like the
inequality to hold for every value of ğ›¾we need (worst case analysis)
[(1 âˆ’ğ›¿)ğ¿ğœâˆ’ğ›¿] â‰¤(ğ›¼âˆ’ğ›¿) â‡â‡’(1 âˆ’ğ›¿)ğ¿ğœâ‰¤ğ›¼â‡â‡’ğ›¿â‰¥1 âˆ’ğ›¼
ğ¿ğœ.
16

Since ğ›¼â‰¥ğ›¿we also need
ğ›¼â‰¥1 âˆ’ğ›¼
ğ¿ğœâ‡â‡’ğ¿ğœğ›¼â‰¥ğ¿ğœâˆ’ğ›¼â‡â‡’ğ›¼â‰¥
ğœğ¿
1 + ğœğ¿= 1 âˆ’
1
1 + ğœğ¿.
The bound becomes
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤2ğ›¼ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğ›¾ğ‘˜ğ›¿
ğœâˆ’ğ›¾ğ‘˜
ğœ

|             {z             }
ğµ(ğ›¿)
ğ‘“âˆ—
ğ‘–ğ‘˜.
For the sake of minimizing the first term in the bound, it would make sense to use ğ›¼=
ğœğ¿
1+ğœğ¿.
However, under this condition we get that ğ›¿â‰¥
ğœğ¿
1+ğœğ¿as well. This is not ideal since we want ğµ(ğ›¿),
the error factor, to vanish for small ğ›¾. To do this, we need to slightly increase the value of ğ›¼to
allow ğ›¿to become negative for small values of ğ‘¦= ğ¿ğœ. Note that for ğ›¿to be (potentially) negative
we need
1 âˆ’ğ›¼
ğœğ¿â‰¤0 â‡â‡’ğ›¼â‰¥ğ¿ğœ.
Hence, if we want to keep ğ›¼< 1 (needed for proofs), we can only have this condition for ğœâ‰¤1/ğ¿
â€“ i.e. the case where we know convergence holds. To this end, let us consider
1 > ğ›¼=
2ğœğ¿
1 + 2ğœğ¿=
ğœğ¿
1/2 + ğœğ¿â‰¥
ğœğ¿
1 + ğœğ¿.
Using this ğ›¼, we get
ğ›¿â‰¥1 âˆ’ğ›¼
ğ¿ğœ= 1 âˆ’
2
1 + 2ğœğ¿= 2ğœğ¿âˆ’1
2ğœğ¿+ 1 .
Note that if ğœâ‰¤
1
2ğ¿then the minimum allowed ğ›¿is negative. We pick for ğ›¿its minimum allowed
value.
4.2
Convergence rates for fixed regularization
We provide here convergence guarantees for stochastic NGN method in the convex, strongly
convex, and non-convex setting. The results justify the performance observed in practice so far,
complemented by deep learning experiments in Section 4.4.
Our results are better presented with the concept of interpolation.
For a finite sum loss
ğ‘“= Pğ‘
ğ‘–=1 ğ‘“ğ‘–, interpolation implies that all ğ‘“ğ‘–s can be minimized at the solution.
Definition 4.3 (Interpolation). The optimization problem minğ‘¥âˆˆRğ‘‘ğ‘“(ğ‘¥) = Pğ‘
ğ‘–=1 ğ‘“ğ‘–(ğ‘¥) satisfies in-
terpolation if there exist a ğ‘¥âˆ—such that ğ‘¥âˆ—âˆˆarg minğ‘¥âˆˆRğ‘‘ğ‘“ğ‘–(ğ‘¥) for all ğ‘–âˆˆ[ğ‘].
Interpolation is an assumption that is often used in the deep learning literature, as it is associated
with overparametrization of some modern neural networks, leading to fast convergence of gradient
methods (Ma et al., 2018). We do not assume interpolation4, but follow Loizou et al. (2021) to
provide convergence rates in relation to the following two error quantities:
Î”int := E[ ğ‘“ğ‘–(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–],
Î”pos := E[ ğ‘“âˆ—
ğ‘–],
(27)
4For instance, large language models do not satisfy this property (Hoffmann et al., 2022).
17

where ğ‘¥âˆ—âˆˆarg min ğ‘“(ğ‘¥) and ğ‘“âˆ—
ğ‘–:= infğ‘¥ğ‘“ğ‘–(ğ‘¥). Here the expectation E[Â·] is taken with respect
to the uniform distribution of the index ğ‘–, equivalent to the average (1/ğ‘) Pğ‘
ğ‘–=1[Â·].
We call
Î”int the interpolation gap, which is apparently zero under interpolation. The position gap Î”pos
measures how close is, on average, the minimizer for the current batch to the value zero. While for
overparametrized deep learning models one has Î”pos = Î”int = 0 (Vaswani et al., 2020), both the
cases Î”pos = 0, Î”int > 0 and Î”pos > 0, Î”int = 0 are feasible in theory.
Remark. Since in our setting each ğ‘“ğ‘–is lower-bounded, under the very realistic assumptions ğ‘“âˆ—< âˆ
and ğ‘“âˆ—
ğ‘–< âˆfor all ğ‘–âˆˆ[ğ‘], both Î”pos and Î”int are finite. This assumption, in particular, does not
imply finite gradient noise variance.
4.2.1
Convex and strongly convex settings
We first recall the definition in the differentiable setting.
Definition 4.4 (Strong Convexity / Convexity). A differentiable function ğ‘“: Rğ‘‘â†’R, is ğœ‡-strongly
convex, if there exists a constant ğœ‡> 0 such that âˆ€ğ‘¥, ğ‘¦âˆˆRğ‘‘:
ğ‘“(ğ‘¥) â‰¥ğ‘“(ğ‘¦) + âŸ¨âˆ‡ğ‘“(ğ‘¦), ğ‘¥âˆ’ğ‘¦âŸ©+ ğœ‡
2 âˆ¥ğ‘¥âˆ’ğ‘¦âˆ¥2
(28)
for all ğ‘¥âˆˆRğ‘‘. If the inequality holds with ğœ‡= 0 the function ğ‘“is convex.
We now present our results in the convex and strongly convex settings. The nonconvex case,
for which we have weaker results, is presented in Section 4.2.3.
Theorem 4.5 (NGN, convex). Let ğ‘“=
1
ğ‘
Pğ‘
ğ‘–=1 ğ‘“ğ‘–, where ealh ğ‘“ğ‘–: Rğ‘‘â†’R is non-negative, ğ¿-
smooth and convex. Consider the SGD method (2) with the NGN stepsize (21). For any value of
ğœ> 0, we have
E

ğ‘“( Â¯ğ‘¥ğ¾) âˆ’ğ‘“(ğ‘¥âˆ—)

â‰¤Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
ğœ‚ğœğ¾
+ 3ğœğ¿Â· (1 + ğœğ¿)Î”int + ğœğ¿Â· max {0, 2ğœğ¿âˆ’1} Î”pos,
(29)
where Â¯ğ‘¥ğ¾= 1
ğ¾
Pğ¾âˆ’1
ğ‘˜=0 ğ‘¥ğ‘˜and ğœ‚ğœ=
2ğœ
(1+2ğœğ¿)2. Decreasing ğœlike ğ‘‚(1/
âˆš
ğ¾), we get an ğ‘‚

ln(ğ¾)
âˆš
ğ¾

rate.
We note that the NGN stepsize (21) does not require knowledge of the Lipschitz constant ğ¿.
Theorem 4.6 (NGN, strongly convex). Let ğ‘“= 1
ğ‘
Pğ‘
ğ‘–=1 ğ‘“ğ‘–, where each ğ‘“ğ‘–: Rğ‘‘â†’R is non-negative,
ğ¿-smooth and convex. Additionally, assume ğ‘“is ğœ‡-strongly convex. Consider the SGD method (2)
with the NGN stepsize (21). For any value of ğœ> 0, we have
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğœŒ)ğ‘˜Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 + 6ğ¿
ğœ‡ğœ(1 + ğœğ¿)Î”int + 2ğœğ¿
ğœ‡
max {0, 2ğœğ¿âˆ’1} Î”pos,
where ğœŒ=
ğœ
(1+2ğœğ¿)(1+ğœğ¿). Decreasing ğœlike ğ‘‚(1/ğ¾), we get an ğ‘‚

ln(ğ¾)
ğ¾

rate.
18

The proofs of the main results in Thm. 4.5 and Thm. 4.6 are presented in Section 4.2.2 and the
analysis on decreasing ğœis given in Section 4.3. Here, we make a few comments on these results.
Technical novelty.
Our analysis involves a novel expansion for the expectation of the product of the
stochastic NGN stepsize ğ›¾ğ‘˜with the stochastic suboptimality value ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—). The correlation
between these quantities poses well-known challenges in the analysis of adaptive methods (see e.g.
discussion in Ward et al. (2020)). Our approach involves decomposing the stepsize as ğ›¾ğ‘˜= ğœŒ+ ğœ–ğ‘˜,
where ğœŒ= ğ‘‚(ğœ) is deterministic and ğœ–ğ‘˜= ğ‘‚(ğœ2) is a non-negative stochastic offset.
This
decomposition allows us to fully benefit from NGN adaptivity in the proof without compromising
the bound tightness and the suboptimality level reached at convergence.
Comment on the rate.
For any fixed ğœ, the rate is sublinear (linear in the strongly convex case)
to a bounded neighborhood of the solution. The neighborhood magnitude is ğ‘‚(ğœ) â€“ shrinks as ğœ
converges to zero â€“ and depends on the values of Î”int and Î”pos. Let us elaborate further:
â€¢ If Î”int = Î”pos = 0, then the results guarantees sublinear (linear in the strongly convex case)
convergence in expectation to the solution for any value of ğœ. The best constant in the rate
in this setting is achieved when knowing the Lipschitz constant, indeed minğœğœ‚ğœis achieved
at ğœ= 1/(2ğ¿).
â€¢ If Î”pos > 0, then the error term ğœğ¿Â· max {0, 2ğœğ¿âˆ’1} Î”pos (in the convex setting) is non-
vanishing5 for ğœ> 1/(2ğ¿) â€“ this does not come from gradient stochasticity but is instead an
effect of correcting divergence of the gradient update for large stepsizes. We note that in this
case our method behaves more favorably than SGD, which is divergent for large stepsizes.
â€¢ If Î”int > 0, then the error term 3ğœğ¿(1 + ğœğ¿)Î”int (in the convex setting) is the result of
gradient stochasticity around the solution.
â€¢ By annealing ğœâ€”which does not necessarily yield a monotonically decreasing stepsize
ğ›¾ğ‘˜(see Imagenet experiment in Figure 7) â€“ we get convergence to the solution without
knowledge of the gradient Lipshitz constant ğ¿(Thm. 4.12).
Comparison with SPS.
The errors in Thm. 4.5 and Thm. 4.6 are ğ‘‚(ğœ), meaning that as ğœâ†’0
the error vanishes and we converge to the solution. This is not the case for recent adaptations of the
Polyak scheme to the stochastic setting (Loizou et al., 2021), where the error is ğ‘‚(1) (Thm. 4.7).
Theorem 4.7 (Main Theorem of Loizou et al. (2021)). Let each ğ‘“ğ‘–be ğ¿ğ‘–-smooth and convex.
Denote ğ¿= max{ğ¿ğ‘–}ğ‘›
ğ‘–=1 the maximum smoothness constant.
Consider SGD with the SPSmax
learning rate ğ›¾ğ‘˜= min

ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜
ğ‘âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2, ğ›¾ğ‘

. If ğ‘= 1, E

ğ‘“( Â¯ğ‘¥ğ¾) âˆ’ğ‘“(ğ‘¥âˆ—)

â‰¤âˆ¥ğ‘¥0âˆ’ğ‘¥âˆ—âˆ¥2
ğ›¼ğ¾
+ 2ğ›¾ğ‘Î”int
ğ›¼
, where
ğ›¼= min
 1
2ğ‘ğ¿, ğ›¾ğ‘
	
and Â¯ğ‘¥ğ¾=
1
ğ¾
Pğ¾âˆ’1
ğ‘˜=0 ğ‘¥ğ‘˜. If in addition ğ‘“is ğœ‡-strongly convex, then, for any
ğ‘â‰¥1/2, SGD with SPSmax converges as: Eâˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğ›¼)ğ‘˜âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 + 2ğ›¾ğ‘Î”int
ğœ‡ğ›¼.
5The threshold 1/(2ğ¿) is also common in plain SGD, see, e.g., Garrigos and Gower (2023, Theorem 6.8).
19

It is easy to realize that indeed, as thoroughly explained by Orvieto et al. (2022c), the error
terms 2ğ›¾ğ‘Î”int
ğ›¼
and 2ğ›¾ğ‘Î”int
ğœ‡ğ›¼
in the convex and strongly convex setting are ğ‘‚(1) with respect to the
hyperparameters. This is referred to as the bias problem in Section 4 of Orvieto et al. (2022c).
While Orvieto et al. (2022b) presented a variation on SPS (DecSPS) to solve this issue, their proof
is based on a classical Adagrad-like argument and additionally needs strong convexity or bounded
iterates to guarantee sublinear convergence in the convex domain. Instead, Thm. 4.5 requires no
assumptions other than convexity and smoothness. Further, in Thm. 4.6, we are able to prove linear
convergence to a ğ‘‚(ğœ) neighborhood, a result not yet derived with DecSPS machinery.
Ïƒk = 100/k
Ïƒk = 10/k
Ïƒk = 1/k
Î³k = 1/k
Î³k = 0.1/k
Î³k = 10/k
Î· = 0.1
Î· = 1
Î· = 10
Average over 10 runs*
Average over 10 runs*
Figure 4: Comparison of NGN with Adagrad-norm and SGD for two convex problems, under a
decreasing stepsize and a constant stepsize. A comment is provided below.
Comparison with Adagrad.
Adagrad enjoys state-of-the-art adaptive convergence guarantees in
the convex and non-convex settings (Liu et al., 2023; Faw et al., 2022; Wang et al., 2023a; Faw
et al., 2023). Knowledge of the gradient Lipschitz constant or additional strong assumptions are
not needed for convergence to the problem solution. However, as opposed to NGN, Adagradâ€™s
proof techniques do not transfer well to the strongly convex setting (see instead our Thm. 4.6).
In addition, Adagrad is observed to perform suboptimally compared to well-tuned SGD on some
problems (Kingma and Ba, 2014). This is mainly due to the fact that the resulting ğ›¾ğ‘˜sequence is
forced to be decreasing by design. A feature that is not present in more commonly used optimizers
such as Adam (Kingma and Ba, 2014).
We evaluate the performance of tuned SGD, NGN, and Adagrad-norm in two distinct settings:
â€¢ Linear regression on Gaussian input data with random predictor (ğ‘‘= 512 features, ğ‘= 2048
datapoints: condition number is âˆ¼8.5), optimized with a relatively big batch size of 64.
20

Here, Î”pos = 0 but Î”int > 0, and we choose decreasing stepsizes for SGD and NGN like 1/ğ‘˜,
to ensure convergence to the unique solution (see Thm. 4.6). In most recent results (see, e.g.,
Faw et al., 2022), Adagrad is not analyzed with a decreasing stepsize ğœ‚, so we also do not
implement it here.
â€¢ The second problem is cross-entropy minimization for a linear model on the Olivetti Faces
dataset (Pedregosa et al., 2011), where ğ‘‘= 4096, ğ‘= 320 and we have 40 distinct classes.
Since here ğ‘â‰ªğ‘‘, we include L2 regularization with strength 1ğ‘’âˆ’3.
We optimize
with a batch-size of 4.
Realistically, Î”pos, Î”int â‰ª1.
We, therefore, consider constant
hyperparameters in all our optimizers.
We report our results in Fig. 4. For SGD and NGN, we tuned their single hyperparameter in-
dependently to the best performance. For Adagrad-norm, ğ›¾ğ‘˜= ğœ‚/
âˆšï¸ƒ
ğ‘2
0 + Pğ‘˜
ğ‘—=0 âˆ¥âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘—)âˆ¥2 and
we only carefully tuned ğœ‚since results are not too sensitive to ğ‘0, which we set to 1ğ‘’âˆ’2. The
best-performing hyperparameter for each method is marked with
. We also show performance
for hyperparameters 3-times-lower ( ) and 3-times-higher ( ). The results clearly indicate, in both
settings, that NGN has an edge over Adagrad-norm:
â€¢ On Olivetti Faces, where we optimize with constant hyperparameters, we indeed see that
Adagrad, even after tuning, does not provide a clear advantage over SGD. Instead, for NGN,
performance is remarkably better than SGD and Adagrad for all hyperparameter values. The
strong gradients experienced at the first iteration cause the NGN stepsize to automatically
warmup, reaching then a ğœ-proportional value when gradients become small.
â€¢ On the linear regression example, the behavior is particularly interesting: NGN prevents the
learning rate from overshooting â€“ unlocking fast progress even at large ğœ. Instead, SGD for
a big enough learning rate overshoots and only catches up at the end of training. As expected
by the result of Yang et al. (2023), Adagrad does not overshoot, but its progress slows down as
soon as gradients start to vanish (therefore ğ›¾ğ‘˜hardly decreases). Towards the end of training,
NGN optimizes with a stepsize ğ›¾ğ‘˜â‰ƒğœğ‘˜, effectively converging to the SGD rule.
Remark (NGN can automatically warmup, but looks like decrease schedule is hand-picked.). While
this is indeed the case for the examples seen so far, this property depends on the features of the loss
landscape. We show that in deep learning experiments (Fig. 7) for a constant ğœthe induced ğ›¾ğ‘˜has
a warmup-decay behaviour.
4.2.2
Proofs for main results
We present here the proofs for the convex and strongly convex setting.
Proof of Thm. 4.5. By assumption, each ğ‘“ğ‘–is convex and ğ¿-smooth.
Expanding the squared
21

distance and using convexity, we get:
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 = âˆ¥(ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜) + (ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—)âˆ¥2
= âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 + 2âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âŸ©+ âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âˆ¥2
= âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğ›¾ğ‘˜âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+ ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2
â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] + ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2.
(30)
We now make use of Lemma 4.2:
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)]
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜.
(31)
Note that taking the expectation conditional on ğ‘¥ğ‘˜at this point (as done in the classical SGD proof)
is not feasible: indeed, the variable ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—), which would have expectation ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—),
is correlated with ğ›¾ğ‘˜â€“ meaning that we would need to consider the expectation of the product
ğ›¾ğ‘˜( ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)).
The analysis of Loizou et al. (2021) in the context of SPS consists in writing ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) =
[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] âˆ’[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜], where both terms within brackets are positive and therefore one
can use the stepsize bounds before taking the expectation. This is a smart approach for a quick
computation; however it introduces a bias term E[ğ›¾ğ‘˜( ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜)] â‰¤ğœ[ ğ‘“(ğ‘¥âˆ—) âˆ’Eğ‘–ğ‘“âˆ—
ğ‘–] = ğ‘‚(ğœ).
It is more desirable, if the method allows, to have error terms only of the order ğ‘‚(ğœ2), so that one
can guarantee later in the proof that, as ğœâ†’0, the method converges to the problem solution.
To this end, we write ğ›¾ğ‘˜= ğœŒ+ ğœ–ğ‘˜, where both ğœŒand ğœ–ğ‘˜are non-negative and ğœŒis deterministic.
For intuition, the reader can think of ğœŒas a stepsize lower bound such that ideally ğœŒ= ğ‘‚(ğœ) and
ğœ–ğ‘˜= ğ‘‚(ğœ2) for all realizations of ğ›¾ğ‘˜â€”we make it more precise in the next lines:
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’2ğœŒ[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] âˆ’2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)]
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‚(ğœ2) ğ‘“âˆ—
ğ‘–ğ‘˜,
where we wrote the last term in (31) simply as ğ‘‚(ğœ2) for brevity but will plug in the correct
expression at the end of the proof.
At this point, we write ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) = [ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] âˆ’[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] only for the second
term in the bound above. Our purpose, is to make the resulting term âˆ’2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] (negative)
dominant compared to the third term in the bound above (positive). In formulas, the bound on the
distance update âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 becomes
âˆ’2ğœŒ[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] âˆ’2

ğœ–ğ‘˜âˆ’2ğœğ¿ğ›¾ğ‘˜
1 + 2ğœğ¿

[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‚(ğœ2) ğ‘“âˆ—
ğ‘–ğ‘˜,
and we require ğœ–ğ‘˜âˆ’
2ğœğ¿
1+2ğœğ¿ğ›¾ğ‘˜â‰¥0. Note that ğœ–ğ‘˜= ğ›¾ğ‘˜âˆ’ğœŒ. Therefore, we need

1 âˆ’
2ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜â‰¥ğœŒ
=â‡’ğ›¾ğ‘˜â‰¥(1 + 2ğœğ¿)ğœŒ.
22

Since ğ›¾ğ‘˜â‰¥
ğœ
1+ğœğ¿thanks to Lemma 2.1, we have that a sufficient condition is
ğœŒâ‰¤
ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿) .
Let us then pick ğœŒequal to this upper bound. Our bound on the distance update simplifies as
âˆ’
2ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿) [ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] + 2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‚(ğœ2) ğ‘“âˆ—
ğ‘–ğ‘˜.
We now get to the interesting part: what is the order of ğœ–ğ‘˜under our choice for ğœŒ?
ğœ–ğ‘˜= ğ›¾ğ‘˜âˆ’ğœŒâ‰¤ğœâˆ’
ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿) = ğœ(1 + 2ğœğ¿)(1 + ğœğ¿) âˆ’ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿)
.
Simplifying the bound, we get the desired result: ğœ–ğ‘˜= ğ‘‚(ğœ2). Indeed,
ğœ–ğ‘˜â‰¤
3ğ¿ğœ2 + 2ğ¿2ğœ3
(1 + 2ğœğ¿)(1 + ğœğ¿) = ğ¿ğœ2
3 + 2ğ¿ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿) â‰¤
3ğ¿ğœ2
1 + 2ğœğ¿.
(32)
All in all, our bound becomes
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœ)[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] +ğ‘‡1(ğœ2)[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] +ğ‘‡2(ğœ2) ğ‘“âˆ—
ğ‘–ğ‘˜, (33)
where
ğ‘‡0(ğœ) =
2ğœ
(1 + 2ğœğ¿)(1 + ğœğ¿) ,
(34)
ğ‘‡1(ğœ2) =
6ğ¿ğœ2
1 + 2ğœğ¿,
(35)
ğ‘‡2(ğœ2) = 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

.
(36)
We are finally ready to take the conditional expectation with respect to ğ‘˜:
Eğ‘˜âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’ğ‘‡0(ğœ)[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)]
+ ğ‘‡1(ğœ2)Eğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‡2(ğœ2)Eğ‘˜[ ğ‘“âˆ—
ğ‘–ğ‘˜].
Let us call ğ¸ğ‘˜the expected error at step ğ‘˜:
ğ¸ğ‘˜(ğœ2) := ğ‘‡1(ğœ2)Eğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‡2(ğœ2)Eğ‘˜[ ğ‘“âˆ—
ğ‘–ğ‘˜]
â‰¤ğ‘‡1(ğœ2)Î”int + ğ‘‡2(ğœ2)Î”pos,
(37)
where Î”int := Eğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] and Î”pos := Eğ‘˜[ ğ‘“âˆ—
ğ‘–ğ‘˜]. Therefore we get the compact formula
Eğ‘˜âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœ)[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] + ğ¸ğ‘˜(ğœ2)
23

Taking the expectation again and using the tower property
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’Eâˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœ)E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] + ğ¸ğ‘˜(ğœ2)
Finally, averaging over iterations,
1
ğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’Eâˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœ)
ğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] + ğ¸ğ‘˜(ğœ2).
Using linearity of expectation
1
ğ¾Eâˆ¥ğ‘¥ğ¾âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’1
ğ¾Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœ)E
"
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
1
ğ¾ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)
#
+ ğ¸ğ‘˜(ğœ2).
and therefore
E
"
1
ğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)
#
â‰¤
1
ğ‘‡0(ğœ)ğ¾Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 + ğ¸ğ‘˜(ğœ2)
ğ‘‡0(ğœ) .
Finally, by Jensenâ€™s inequality,
E

ğ‘“( Â¯ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)

â‰¤
1
ğ‘‡0(ğœ)ğ¾Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 + ğ¸ğ‘˜(ğœ2)
ğ‘‡0(ğœ) ,
where Â¯ğ‘¥ğ¾= 1
ğ¾
Pğ¾âˆ’1
ğ‘˜=0 ğ‘¥ğ‘˜. Finally, using the bound on ğ¸ğ‘˜(ğœ2) in (37) and definitions of ğ‘‡0, ğ‘‡1 and
ğ‘‡2 in (34), (35) and (36), respectively, we arrive at
E

ğ‘“( Â¯ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)

â‰¤(1 + 2ğœğ¿)2
2ğœğ¾
Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
+ 3ğœğ¿(1 + ğœğ¿)Î”int + ğœğ¿Â· max {0, 2ğœğ¿âˆ’1} Î”pos.
This concludes the proof.
â–¡
Proof of Theorem 4.6. We proceed very similarly with the convex setting
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 = âˆ¥(ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜) + (ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—)âˆ¥2
= âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 + 2âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âŸ©+ âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âˆ¥2
= âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğ›¾ğ‘˜âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+ ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2
Making use of Lemma 4.2:
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğ›¾ğ‘˜âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜.
24

As in the proof for Thorem 4.5, we decompose ğ›¾into a deterministic lower bound ğœŒand some
error ğœ–ğ‘˜âˆˆğ‘‚(ğœ2): ğ›¾ğ‘˜= ğœŒ+ ğœ–ğ‘˜. As in Thorem 4.5 we choose ğœŒ=
ğœ
(1+2ğœğ¿)(1+ğœğ¿). This leads to (see
Equation (32))
ğœ–ğ‘˜= ğ›¾ğ‘˜âˆ’ğœŒâ‰¤
3ğ¿ğœ2
1 + 2ğœğ¿.
Note that we have not yet used convexity. We use it now after decomposing ğ›¾ğ‘˜:
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2
â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©âˆ’2ğœ–ğ‘˜âŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜
â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©âˆ’2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)]
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜.
Therefore,
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2
â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©âˆ’2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜]
+

4ğœğ¿
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜
= âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+ 2ğœ–ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜]
+ 2

ğœ–ğ‘˜âˆ’2ğœğ¿ğ›¾ğ‘˜
1 + 2ğœğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜.
As we know from the proof of for the convex setting, ğœ–ğ‘˜âˆ’2ğœğ¿ğ›¾ğ‘˜
1+2ğœğ¿â‰¥0 hence, since ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜> 0,
we can drop the term. using the upper bound for ğœ–ğ‘˜, we get
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2
â‰¤âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+
6ğ¿ğœ2
1 + 2ğœğ¿[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

ğ‘“âˆ—
ğ‘–ğ‘˜.
Now we take the expectation conditional on ğ‘¥ğ‘˜, recalling our definition for the conditional
expectations: Î”int := Eğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] and Î”pos := Eğ‘˜[ ğ‘“âˆ—
ğ‘–ğ‘˜].
Eğ‘˜âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2
â‰¤âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“(ğ‘¥ğ‘˜)âŸ©+
6ğ¿ğœ2
1 + 2ğœğ¿Î”int + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Î”pos.
25

It is now time to use ğœ‡-strong convexity of ğ‘“:
Eğ‘˜âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒâŸ¨ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—, âˆ‡ğ‘“(ğ‘¥ğ‘˜)âŸ©
+
6ğ¿ğœ2
1 + 2ğœğ¿Î”int + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Î”pos
â‰¤(1 âˆ’ğœ‡ğœŒ)âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’2ğœŒ[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)]
+
6ğ¿ğœ2
1 + 2ğœğ¿Î”int + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Î”pos.
Note that ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—) > 0, since ğ‘¥âˆ—is the minimizer for ğ‘“. Hence, we can drop this term.
Eğ‘˜âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğœŒ)âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 +
6ğ¿ğœ2
1 + 2ğœğ¿Î”int + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Î”pos.
Taking the expectation again and using the tower property,
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğœŒ)Eâˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 +
6ğ¿ğœ2
1 + 2ğœğ¿Î”int + 2ğœ2ğ¿
1 + ğœğ¿Â· max

0, 2ğœğ¿âˆ’1
2ğœğ¿+ 1

Î”pos.
This recurrence is of the kind ğ‘¦ğ‘˜+1 = ğ‘ğ‘¦ğ‘˜+ ğ‘, therefore ğ‘¦ğ‘˜= ğ‘ğ‘˜ğ‘¦0 +
Pğ‘˜âˆ’1
ğ‘–=0 ğ‘ğ‘–
ğ‘. Since ğ‘â‰¥0,
we have ğ‘¦ğ‘˜â‰¤ğ‘ğ‘˜ğ‘¦0 +
ğ‘
1âˆ’ğ‘. Therefore,
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğœŒ)ğ‘˜Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
+
6ğ¿ğœ2
(1 + 2ğœğ¿)ğœŒğœ‡Î”int +
2ğœ2ğ¿
(2ğœğ¿+ 1)(1 + ğœğ¿)ğœŒğœ‡Â· max {0, 2ğœğ¿âˆ’1} Î”pos.
Now recall that ğœŒ=
ğœ
(1+2ğœğ¿)(1+ğœğ¿). We can then simplify the error term:
Eâˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤(1 âˆ’ğœ‡ğœŒ)ğ‘˜Eâˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
+ 6ğ¿ğœ
ğœ‡
(1 + ğœğ¿)Î”int + 2ğœğ¿
ğœ‡
max {0, 2ğœğ¿âˆ’1} Î”pos,
which is the desired result.
â–¡
4.2.3
General non-convex setting
Stochastic NGN can also be applied successfully in the nonconvex setting, as we will see from the
deep learning experiments in Section 4.4. For convergence analysis in this setting, knowledge of
the Lipschitz constant is required and we need to define an additional quantity
Î”2
noise = sup
ğ‘¥âˆˆRğ‘‘E[âˆ¥âˆ‡ğ‘“(ğ‘¥) âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ¥2],
which is the usual bound on stochastic gradient variance. The convergence rate provided here is
non-adaptive (with known ğ¿), but we are confident that it can be improved in the future.
26

Theorem 4.8 (NGN, nonconvex). Let ğ‘“= 1
ğ‘
Pğ‘
ğ‘–=1 ğ‘“ğ‘–, where each ğ‘“ğ‘–: Rğ‘‘â†’R is non-negative,
ğ¿-smooth and potentially non-convex. Consider the SGD method (2) with the stochastic NGN
stepsize (21). Then for any ğœâ‰¤
1
2ğ¿, we have
E
"
1
ğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2
#
â‰¤12 Â· E[ ğ‘“(ğ‘¥0) âˆ’ğ‘“âˆ—]
ğœğ¾
+ 18ğœğ¿Î”2
noise,
(38)
Decreasing ğœas ğ‘‚(1/
âˆš
ğ¾), we get a convergence rate of ğ‘‚

ln(ğ¾)
âˆš
ğ¾

.
Proof. The proof deviates significantly from the one for stochastic Polyak stepsizes by Loizou et al.
(2021). We start with the classic expansion based on gradient smoothness
ğ‘“(ğ‘¥ğ‘˜+1) âˆ’ğ‘“(ğ‘¥ğ‘˜) â‰¤âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âŸ©+ ğ¿
2 âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥ğ‘˜âˆ¥2
= âˆ’ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+
ğ¿ğ›¾2
ğ‘˜
2 âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2
â‰¤âˆ’ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+ ğ¿ğœ2
2 âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2.
We would like to take the conditional expectation with respect to ğ‘¥ğ‘˜. Yet, this is not easy since ğ›¾ğ‘˜
and âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) are correlated. Note however that we can write, given the bound in Lemma 4.1,
ğ›¾ğ‘˜=
ğœ
ğœğ¿+ 1 +
ğœ2ğ¿
ğœğ¿+ 1ğœ‰ğ‘–ğ‘˜,
(39)
where ğœ‰ğ‘–ğ‘˜âˆˆ[0, 1] is a random variable. When ğœ‰ğ‘–ğ‘˜= 1 we have ğ›¾ğ‘˜= ğœ, and when ğœ‰ğ‘–ğ‘˜= 0 it holds
that ğ›¾ğ‘˜=
ğœ
ğœğ¿+1. Thil model for ğ›¾ğ‘˜covers its complete range and makes one property explicit :
ğ›¾ğ‘˜= ğ‘‚(ğœ) with variation range ğ‘‚(ğœ2). As such, as ğœâ†’0 the stepsize becomes deterministic,
and the update reduces to SGD with constant stepsize. Leveraging this representation of ğ›¾ğ‘˜, we
can write
âˆ’ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) = âˆ’
ğœ
ğœğ¿+ 1âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©âˆ’
ğœ2ğ¿
ğœğ¿+ 1ğœ‰ğ‘–ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©
â‰¤âˆ’
ğœ
ğœğ¿+ 1âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+
ğœ2ğ¿
ğœğ¿+ 1
ğœ‰ğ‘–ğ‘˜
 Â·
âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©

â‰¤âˆ’
ğœ
ğœğ¿+ 1âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©+
ğœ2ğ¿
ğœğ¿+ 1
âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©
 .
Therefore
âˆ’Eğ‘˜[ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©] â‰¤âˆ’
ğœ
ğœğ¿+ 1 âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 +
ğœ2ğ¿
ğœğ¿+ 1Eğ‘˜
âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©

(40)
The first term in the above bound is ğ‘‚(ğœ) lnd directly helps convergence, while the last term is an
error of ğ‘‚(ğœ2). Next, recall the basic inequality: for any ğ‘, ğ‘âˆˆRğ‘‘:
|âŸ¨ğ‘, ğ‘âŸ©| â‰¤1
2 âˆ¥ğ‘âˆ¥2 + 1
2 âˆ¥ğ‘âˆ¥2 + 1
2 âˆ¥ğ‘âˆ’ğ‘âˆ¥2.
27

Applying to the last term in (40) and using the assumption ğœ2 = supğ‘¥âˆˆRğ‘‘Eğ‘–âˆ¥âˆ‡ğ‘“(ğ‘¥)âˆ’âˆ‡ğ‘“ğ‘–(ğ‘¥)âˆ¥2 < âˆ,
we have
2Eğ‘˜
âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©
 â‰¤âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 + Eğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 + Eğ‘˜âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜) âˆ’âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2
â‰¤âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 + Eğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 + ğœ2
â‰¤2âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 + 2ğœ2.
Therefore, we get the compact inequality
âˆ’Eğ‘˜[ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©] â‰¤âˆ’
ğœ
ğœğ¿+ 1âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 +
ğœ2ğ¿
ğœğ¿+ 1

âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 + ğœ2
â‰¤âˆ’ğœ
1 âˆ’ğœğ¿
1 + ğœğ¿

âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 +
ğœ2ğ¿
ğœğ¿+ 1ğœ2,
which we can insert back in the original expansion to get
Eğ‘˜[ ğ‘“(ğ‘¥ğ‘˜+1)] âˆ’ğ‘“(ğ‘¥ğ‘˜) â‰¤âˆ’Eğ‘˜[ğ›¾ğ‘˜âŸ¨âˆ‡ğ‘“(ğ‘¥ğ‘˜), âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âŸ©] + ğ¿ğœ2
2 Eğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2
â‰¤

âˆ’ğœ
1 âˆ’ğœğ¿
1 + ğœğ¿

+ ğ¿ğœ2
2

âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 +
 ğœ2ğ¿
ğœğ¿+ 1 + ğœ2ğ¿
2

ğœ2.
We therefore need
âˆ’ğœ
1 âˆ’ğœğ¿
1 + ğœğ¿

+ ğ¿ğœ2
2
= âˆ’ğœ
1 âˆ’ğœğ¿
1 + ğœğ¿âˆ’ğœğ¿
2

â‰¤0.
The function 1âˆ’ğœğ¿
1+ğœğ¿âˆ’ğœğ¿
2 is monotonically decreasing as ğœğ¿> 0 increases. For ğœğ¿= 0 it is 1 and
reaches value zero at âˆ’3/2+
âˆš
17/2 â‰Š0.56. For ğœğ¿= 0.5, one gets 1âˆ’ğœğ¿
1+ğœğ¿âˆ’ğœğ¿
2 = 0.5
1.5 âˆ’0.25 = 1/12.
Therefore, for ğœâ‰¤
1
2ğ¿, we get âˆ’ğœ

1âˆ’ğœğ¿
1+ğœğ¿

+ ğ¿ğœ2
2
â‰¤âˆ’ğœ
12.
Next, for the noise term, note that
ğœ2ğ¿
ğœğ¿+1 + ğœ2ğ¿
2
â‰¤3ğœ2ğ¿
2 . All in all, for ğœâ‰¤
1
2ğ¿, we get:
Eğ‘˜[ ğ‘“(ğ‘¥ğ‘˜+1)] âˆ’ğ‘“(ğ‘¥ğ‘˜) â‰¤âˆ’ğœ
12âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 + 3ğœ2ğ¿
2
ğœ2,
Or, more conveniently:
âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 â‰¤âˆ’12
ğœ[Eğ‘˜[ ğ‘“(ğ‘¥ğ‘˜+1)] âˆ’ğ‘“(ğ‘¥ğ‘˜)] + 18ğœğ¿ğœ2.
After taking the expectation using the tower property, we get
Eâˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 â‰¤âˆ’12
ğœ[E[ ğ‘“(ğ‘¥ğ‘˜+1)] âˆ’E[ ğ‘“(ğ‘¥ğ‘˜)]] + 18ğœğ¿ğœ2,
28

Summing over iterations and telescoping the sum, after adding and subtracting ğ‘“âˆ—we get
1
ğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
Eâˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)âˆ¥2 â‰¤âˆ’12
ğœğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
E[ ğ‘“(ğ‘¥ğ‘˜+1)] + 12
ğœğ¾
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
E[ ğ‘“(ğ‘¥ğ‘˜)] + 18ğœğ¿ğœ2
â‰¤âˆ’12
ğœğ¾E[ ğ‘“(ğ‘¥ğ¾)] + 12
ğœğ¾E[ ğ‘“(ğ‘¥0)] + 18ğœğ¿ğœ2
â‰¤âˆ’12
ğœğ¾E[ ğ‘“(ğ‘¥ğ¾) âˆ’ğ‘“âˆ—] + 12
ğœğ¾E[ ğ‘“(ğ‘¥0) âˆ’ğ‘“âˆ—] + 18ğœğ¿ğœ2
â‰¤12
ğœğ¾E[ ğ‘“(ğ‘¥0) âˆ’ğ‘“âˆ—] + 18ğœğ¿ğœ2.
This concludes the proof.
â–¡
4.3
Technique sketch for annealed regularization
In this section, we analyze stochastic NGN stepsize with decreasing ğœğ‘˜, i.e.
ğ›¾ğ‘˜=
ğœğ‘˜
1 +
ğœğ‘˜
2 ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 .
(41)
According to the NGN approximation (8), the setting ğœğ‘˜â†’0 can be thought of as gradually
increasing the regularization strength (i.e. getting more similar to a vanilla SGD update as ğ‘˜â†’âˆ).
For a general time-dependent ğœğ‘˜, all local inequalities hold. In particular, the following three
lemmas hold with no additional proof required.
Lemma 4.9 (Stepsize bounds, time dependent). Let each ğ‘“ğ‘–: Rğ‘‘â†’R be non-negative, differen-
tiable and ğ¿-smooth. Consider ğ›¾ğ‘˜as in (41), we have
ğ›¾ğ‘˜âˆˆ

ğœğ‘˜
1 + ğœğ‘˜ğ¿, ğœ

=
"
1
ğ¿+ ğœâˆ’1
ğ‘˜
, ğœğ‘˜
#
.
Lemma 4.10 (Fundamental Equality, time dependent). Consider ğ›¾ğ‘˜as in (41). One has
ğ›¾ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥)âˆ¥2 = 2
ğœğ‘˜âˆ’ğ›¾ğ‘˜
ğœğ‘˜

ğ‘“ğ‘–ğ‘˜(ğ‘¥).
Lemma 4.11 (Fundamental inequality, time dependent). Let each ğ‘“ğ‘–: Rğ‘‘â†’R be non-negative,
ğ¿-smooth and convex. Consider ğ›¾ğ‘˜as in (41), we have
ğ›¾2
ğ‘˜âˆ¥âˆ‡ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ¥2 â‰¤

4ğœğ‘˜ğ¿
1 + 2ğœğ‘˜ğ¿

ğ›¾ğ‘˜[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] +
2ğœ2
ğ‘˜ğ¿
1 + ğœğ‘˜ğ¿Â· max

0, 2ğœğ‘˜ğ¿âˆ’1
2ğœğ‘˜ğ¿+ 1

Â· ğ‘“âˆ—
ğ‘–ğ‘˜.
29

We present proof for the convex setting. The nonconvex case is very similar and uses the same
techniques; therefore, we omit it. The strongly convex setting can also be easily derived using e.g.
the techniques in Lacoste-Julien et al. (2012).
Theorem 4.12 (NGN, convex, decreasing ğœğ‘˜). Let ğ‘“=
1
ğ‘
Pğ‘
ğ‘–=1 ğ‘“ğ‘–, where each ğ‘“ğ‘–: Rğ‘‘â†’R is
non-negative, ğ¿-smooth and convex. Consider the SGD method (2) with the stepsize (41). For any
value of ğœ0 > 0, setting ğœğ‘˜= ğœ0/
âˆš
ğ‘˜+ 1 leads to the following rate: for ğ¾â‰¥2,
E[ ğ‘“( Â¯ğ‘¥ğ¾) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤ğ¶1âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
âˆš
ğ¾âˆ’1
+ ğ¶1ğ¶2 ln(ğ¾+ 1)
âˆš
ğ¾âˆ’1
= ğ‘‚
ln(ğ¾)
âˆš
ğ¾

.
(42)
where Â¯ğ‘¥ğ¾= Pğ¾âˆ’1
ğ‘˜=0 ğ‘ğ‘˜,ğ¾ğ‘¥ğ‘˜with ğ‘ğ‘˜,ğ¾=
ğœğ‘˜
Pğ¾âˆ’1
ğ‘˜=0 ğœğ‘˜and
ğ¶1 = (1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿)
4ğœ0
,
ğ¶2 =

6Î”int + 2 max {0, 2ğœ0ğ¿âˆ’1} Î”pos

ğ¿ğœ2
0 .
Proof. Using Lemmas 4.9, 4.10 and 4.11 and following the same exact steps as in the proof of
Theorem 4.5, we arrive at
âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2 â‰¤âˆ’ğ‘‡0(ğœğ‘˜)[ ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜) âˆ’ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—)] + ğ‘‡1(ğœ2
ğ‘˜)[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‡2(ğœ2
ğ‘˜) ğ‘“âˆ—
ğ‘–ğ‘˜,
with
ğ‘‡0(ğœğ‘˜) =
2ğœğ‘˜
(1 + 2ğœğ‘˜ğ¿)(1 + ğœğ‘˜ğ¿),
ğ‘‡1(ğœ2
ğ‘˜) =
6ğ¿ğœ2
ğ‘˜
1 + 2ğœğ‘˜ğ¿,
ğ‘‡2(ğœ2
ğ‘˜) =
2ğœ2
ğ‘˜ğ¿
1 + ğœğ‘˜ğ¿Â· max

0, 2ğœğ‘˜ğ¿âˆ’1
2ğœğ‘˜ğ¿+ 1

Taking the expectation, we get the compact formula
ğ‘‡0(ğœğ‘˜)E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤âˆ’E

âˆ¥ğ‘¥ğ‘˜+1 âˆ’ğ‘¥âˆ—âˆ¥2 âˆ’âˆ¥ğ‘¥ğ‘˜âˆ’ğ‘¥âˆ—âˆ¥2
+ ğ¸ğ‘˜(ğœ2
ğ‘˜),
where ğ¸ğ‘˜(ğœ2
ğ‘˜) is the expected error at step ğ‘˜:
ğ¸ğ‘˜(ğœ2
ğ‘˜) := ğ‘‡1(ğœ2
ğ‘˜)E[ ğ‘“ğ‘–ğ‘˜(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜] + ğ‘‡2(ğœ2
ğ‘˜)E[ ğ‘“âˆ—
ğ‘–ğ‘˜]
â‰¤ğ‘‡1(ğœ2
ğ‘˜)Î”int + ğ‘‡2(ğœ2
ğ‘˜)Î”pos.
(43)
Recall the definitions Î”int := E[ ğ‘“ğ‘–(ğ‘¥âˆ—) âˆ’ğ‘“âˆ—
ğ‘–] and Î”pos := E[ ğ‘“âˆ—
ğ‘–].
Following standard techniques (Garrigos and Gower, 2023), summing over ğ‘˜and using tele-
scopic cancellation gives
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘‡0(ğœğ‘˜)E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 +
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ¸ğ‘˜(ğœ2
ğ‘˜),
(44)
30

Let us now construct a pointwise lower bound Ëœğ‘‡0(ğœğ‘˜) to ğ‘‡0(ğœğ‘˜), using the fact that ğœğ‘˜is decreasing:
ğ‘‡0(ğœğ‘˜) =
2ğœğ‘˜
(1 + 2ğœğ‘˜ğ¿)(1 + ğœğ‘˜ğ¿) â‰¥
2ğœğ‘˜
(1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿) =: Ëœğ‘‡0(ğœğ‘˜).
Then we have
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
Ëœğ‘‡0(ğœğ‘˜)E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2 +
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ¸ğ‘˜(ğœ2
ğ‘˜),
Let us divide every term in the above inequality by Pğ¾âˆ’1
ğ‘˜=0 Ëœğ‘‡0(ğœğ‘˜):
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
 
Ëœğ‘‡0(ğœğ‘˜)
Pğ¾âˆ’1
ğ‘˜=0 Ëœğ‘‡0(ğœğ‘˜)
!
E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤
âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
Pğ¾âˆ’1
ğ‘˜=0 Ëœğ‘‡0(ğœğ‘˜) +
Pğ¾âˆ’1
ğ‘˜=0 ğ¸ğ‘˜(ğœ2
ğ‘˜)
Pğ¾âˆ’1
ğ‘˜=0 Ëœğ‘‡0(ğœğ‘˜) .
(45)
Using an integral bound, we have, for ğ¾â‰¥2,
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
1
âˆš
ğ‘˜+ 1
â‰¥
âˆ«ğ¾
1
1âˆšğ‘¡
ğ‘‘ğ‘¡= 2(
âˆš
ğ¾âˆ’1).
Therefore, we have
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
Ëœğ‘‡0(ğœğ‘˜) =
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
2ğœğ‘˜
(1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿)
=
2ğœ0
(1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿)
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
1
âˆš
ğ‘˜+ 1
â‰¥
4ğœ0(
âˆš
ğ¾âˆ’1)
(1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿) .
Note also that
 
Ëœğ‘‡0(ğœğ‘˜)
Pğ¾âˆ’1
ğ‘˜=0 Ëœğ‘‡0(ğœğ‘˜)
!
=
ğœğ‘˜
Pğ¾âˆ’1
ğ‘˜=0 ğœğ‘˜
=: ğ‘ğ‘˜,ğ¾,
where ğ‘ğ‘˜,ğ¾as a function of ğ‘˜is a probability distribution over the interval [0, ğ¾âˆ’1].
We are left with bounding Pğ¾âˆ’1
ğ‘˜=0 ğ¸ğ‘˜(ğœ2
ğ‘˜). Since ğœğ‘˜is decreasing, we have the following bounds:
ğ‘‡1(ğœ2
ğ‘˜) =
6ğ¿ğœ2
ğ‘˜
1 + 2ğœğ‘˜ğ¿â‰¤6ğ¿ğœ2
ğ‘˜,
ğ‘‡2(ğœ2
ğ‘˜) =
2ğœ2
ğ‘˜ğ¿
1 + ğœğ‘˜ğ¿Â· max

0, 2ğœğ‘˜ğ¿âˆ’1
2ğœğ‘˜ğ¿+ 1

â‰¤2ğœ2
ğ‘˜ğ¿Â· max {0, 2ğœ0ğ¿âˆ’1} .
Next we use a standard bound on the ğ¾-th Harmonic number ğ»ğ¾.
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
1
ğ‘˜+ 1 = 1 + 1
2 + 1
3 + Â· Â· Â· + 1
ğ¾= ğ»ğ¾â‰¤ln(ğ¾+ 1).
31

Therefore,
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘‡1(ğœ2
ğ‘˜) â‰¤6ğ¿
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğœ2
ğ‘˜= 6ğ¿ğœ2
0
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
1
ğ‘˜+ 1 â‰¤6ğ¿ğœ2
0 ln(ğ¾+ 1),
and
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘‡2(ğœ2
ğ‘˜) â‰¤2ğ¿Â· max {0, 2ğœ0ğ¿âˆ’1}
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğœ2
ğ‘˜â‰¤2ğ¿Â· max {0, 2ğœ0ğ¿âˆ’1} ğœ2
0 ln(ğ¾+ 1).
All in all, we substitute the above bounds into (43) to obtain
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ¸ğ‘˜(ğœ2
ğ‘˜) â‰¤Î”int
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘‡1(ğœ2
ğ‘˜) + Î”pos
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘‡2(ğœ2
ğ‘˜)
â‰¤

6Î”int + 2 max {0, 2ğœ0ğ¿âˆ’1} Î”pos

ğ¿ğœ2
0 ln(ğ¾+ 1).
Plugging everything back into the bound (45), we have, for ğ¾â‰¥2
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘ğ‘˜,ğ¾E[ ğ‘“(ğ‘¥ğ‘˜) âˆ’ğ‘“(ğ‘¥âˆ—)] â‰¤ğ¶1âˆ¥ğ‘¥0 âˆ’ğ‘¥âˆ—âˆ¥2
âˆš
ğ¾âˆ’1
+ ğ¶1ğ¶2 ln(ğ¾+ 1)
âˆš
ğ¾âˆ’1
,
with
ğ¶1 = (1 + 2ğœ0ğ¿)(1 + ğœ0ğ¿)
4ğœ0
,
ğ¶2 =

6Î”int + 2 max {0, 2ğœ0ğ¿âˆ’1} Î”pos

ğ¿ğœ2
0 .
To conclude, let Â¯ğ‘¥ğ¾= Pğ¾âˆ’1
ğ‘˜=0 ğ‘ğ‘˜,ğ¾ğ‘¥ğ‘˜. Jensenâ€™s inequality implies
ğ‘“( Â¯ğ‘¥ğ¾) = ğ‘“
 
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘ğ‘˜,ğ¾ğ‘¥ğ‘˜
!
â‰¤
ğ¾âˆ’1
âˆ‘ï¸
ğ‘˜=0
ğ‘ğ‘˜,ğ¾ğ‘“(ğ‘¥ğ‘˜).
This concludes the proof.
â–¡
4.4
Deep learning experiments
In this section, we test the performance of NGN on deep convolutional networks. We consider the
following settings with increasing complexity:
â€¢ A 12 layers (7 convolutional layers and 5 max-pooling layers, with batch normalization and
dropout) neural network trained on the Street View House Numbers (SVHN) Dataset (Netzer
et al., 2011) for 50 epochs with a batch size 512;
â€¢ A ResNet18 (He et al., 2016) network6 trained on CIFAR10 (Krizhevsky et al., 2009) for 50
epochs with a batch size 128.
6We use code from the popular repository https://github.com/kuangliu/pytorch-cifar.
32

Setting
Optimizer
Hyperparameter
#0
#1 ( )
#2 ( )
#3 ( )
#4
SVHN + Small Convnet
SGD
0.01
0.03
0.1
0.3
1
Adam
0.00003
0.0001
0.0003
0.001
0.003
Adagrad-norm
1
3
10
30
100
SPS
0.3
1
3
10
30
NGN
0.3
1
3
10
30
CIFAR10 + ResNet18
SGD
0.03
0.1
0.3
1
3
Adam
0.00003
0.0001
0.0003
0.001
0.003
Adagrad-norm
1
3
10
30
100
SPS
0.1
0.3
1
3
10
NGN
0.3
1
3
10
30
Imagenet + ResNet50
SGD
-
0.3
1
3
-
Adam
-
0.0003
0.001
0.003
-
Adagrad-norm
-
100
300
1000
-
SPS
-
1
3
10
-
NGN
-
3
10
30
-
Table 3: Hyperparameters (ğœfor NGN, learning rate for Adam and SGD) by dataset and optimiza-
tion algorithm, results in Figure 5.
â€¢ A ResNet50 (He et al., 2016) network7 trained on Imagenet (Deng et al., 2009) for 30 epochs
with a batch size 256.
All our experiments are performed on NVIDIA V100 GPUs. Reported are mean and confidence
interval8 bars over three seeds for training loss and test accuracy. In Figure 5 we compare NGN
with SGD and Adam (Kingma and Ba, 2014), tuning ğœin NGN and the learning rate in SGD and
Adam. All other parameters in Adam (ğ›½1 and ğ›½2) are, as usual in practice, kept constant. While
Adam is used with momentum ğ›½1 = 0.9, we show here the performance of SGD without momentum
to draw a better comparison with our NGN. We also do not make use of L2 regularization as this
would bias results on the training speed. All hyperparameters are kept constant during training
for SVHN and CIFAR10, while for our largest scale experiment (Imagenet), we decrease ğœand
learning rates 10-fold every 10 epochs. In Figure 6 we show the same NGN statistics as in Figure 5,
but compare with two adaptive stepsizes with strong guarantees: SPSmax and Adragrad-norm. For
SPS: ğ›¾ğ‘˜= min

ğ‘“ğ‘–ğ‘˜(ğ‘¥ğ‘˜)âˆ’ğ‘“âˆ—
ğ‘–ğ‘˜
ğ‘âˆ¥âˆ‡ğ‘“(ğ‘¥ğ‘˜)ğ‘–ğ‘˜âˆ¥2, ğ›¾ğ‘

we fix ğ‘= 1 as motivated by the theory in Loizou et al. (2021)
and tune ğ›¾ğ‘. For Adagrad-norm ğ›¾ğ‘˜= ğœ‚/
âˆšï¸ƒ
ğ‘2
0 + Pğ‘˜
ğ‘—=0 âˆ¥âˆ‡ğ‘“ğ‘–ğ‘—(ğ‘¥ğ‘—)âˆ¥2 we fix ğ‘0 = 1ğ‘’âˆ’2 and tune ğœ‚.
As our main objective here is to show how NGN can adapt to the landscape and converge faster
than SGD for different values of its hyperparameter, we first grid-search the optimal hyperparameters
for all methods on a large logarithmically-spaced grid [1ğ‘’âˆ’5, 3ğ‘’âˆ’5, 1ğ‘’âˆ’4, . . . , 3, 10, 30], and
7We use the official PyTorch repo https://github.com/pytorch/examples/tree/main/imagenet.
8Twice the standard deviation over the square root of number of samples, in our case three.
33

then show results for one/two choices smaller and bigger hyperparameters.
Table 3 lists the
hyperparameter values used for each method and architecture, where hyperparameter #2 (in bold)
is the best-performing for each method as indicated in the last-train-step train accuracy plots (second
column of Figure 5 and 6).
While in the second and fourth column of Figure 5 we show the last-iterate statistics for all
hyperparameters in Table 3, in the first and third column we plot the dynamics for the three best-
performing hyperparameters, and we mark them as
(hyperparameter #1),
(hyperparameter #2),
and
(hyperparameter #3). We now comment on the results in Figure 5, i.e. on the comparison
with SGD and Adam:
â€¢ NGN always performs best in terms of training loss performance.
This holds not only
under optimal tuning (hyperparameter #2, ) but also across all suboptimal hyperparameters
#0, #1, #3, #4, as can be seen in the second column of Figure 5: orange curve lower bounds
blue and green curves.
â€¢ The faster training performance of NGN is more stable across hyperparameters compared to
SGD and Adam. This can be especially seen in ResNets, both on CIFAR10 and Imagenet.
Note that the value ğœ= 3 performs almost optimally for all tasks.
â€¢ The test performance of NGN is more robust to tuning compared to SGD and Adam, as can
be seen in the right-most panel in Figure 5. However, while on Imagenet, test performance
is best for NGN, Adam is superior on SVHN and CIFAR10, with a gap of 0.5 âˆ’1%.
â€¢ NGN is more efficient than Adam: the Adam optimizer maintains second-order statistics for
each parameter (Anil et al., 2019). This, in large models where GPUs need to be filled to
maximum, introduces significant memory overheads that restrict the size of the model being
used as well as the number of examples in a mini-batch. NGN, instead, has nearly the same
wall-clock and memory complexity as SGD.
Although here we did not test the effect of L2 regularization in the interest of avoiding con-
founders, the promising training speed results makes us believe that NGN, equipped with gen-
eralization boosting strategies, such as SAM (Foret et al., 2020), Noise Injection (Orvieto et al.,
2022a, 2023), or simply decoupled L2 regularization (Loshchilov and Hutter, 2017), can lead to
best generalization performance combined with faster training.
In Figure 6 we instead compare NGN with tuned SPS and Adagrad-norm. While we found that
Adagrad-norm cannot deliver solid performance, SPS behaves at times similarly to NGN. This is
not surprising, since there is a strict relation between SPS and NGN. However, we found that NGN
performs drastically better on the ResNet18 setting, with an edge also on Imagenet in train loss.
Effective learning rate.
To conclude, we study in Figure 7 the learning rate dynamics in NGN for
ğœ= 3 (uniformly close to optimal performance) in the context of the experiments in this section.
On SVHN and CIFAR10, we used vanilla NGN with constant ğœ, and found that the stepsize ğ›¾ğ‘˜has
a peculiar warmup-decay behavior. Interestingly, the duration of the warm-up phase coincides with
the usual practice in deep learning (Geiping and Goldstein, 2023). Note that while the learning rate
34


	

)(#
/	
/
*$'$'"(++
!+

 &



	


.)!*)*&!,!*
/
$'%*$'$'"(++

	

)(#








	
!+,-*.
!+

 &



	


.)!*)*&!,!*






	
$'%!+,-*.

	

&% 
	 , -

 , -
 , -
 , -
'!$!$%((
(#""

#



	


+&'&'#)'


	
	







!$"'!$!$%((

	

&% 


	




()*'+
(#""

#



	


+&'&'#)'


	





!$"()*'+


	


)(#

	







*$'$'"(++
&"!'!,!+!,


 &
%(.
!+,
#$"#
/)!*)*&!,!*
	





$'%*$'$'"(++


	


)(#
	




!+,-*/
&"!'!,!+!,


 &
%(.
!+,
#$"#
/)!*)*&!,!*









$'%!+,-*/
Figure 5: (SGD vs. Adam vs. NGN) Experimental results on Deep Neural Networks (stochastic
gradients). All details and comments can be found in the text. Shown is performance for five or
three hyperparameters (Table 3), each method is tuned to best at hyperparameter #2.
35


	



*) $
		
	
	
	
	
	
	
	
	
+%(%(#),,
'#"("-","-
!#+!()+'


&)/
",-
$%#$
0*"+*+'"-"+
	
	
	
	


%(&+%(%(#),,

	



*) $






",-  .+ 0
'#"("-","-
!#+!()+'


&)/
",-
$%#$
0*"+*+'"-"+








%(&",-  .+ 0




'&!

 - 	.	
 - 	.	
 - 	.	
 - 	.	
("%"% &))
)$##
 (%&($



	



,'('($*(
	
	









"%#("%"% &))




'&!

	





)*+(,
)$##
 (%&($



	



,'('($*(

	






"%#)*+(,




*) $
	0
	0	
+%(%(#),,
	",	
!#+!()+'



	



/*"+*+'"-"+
	0	
%(&+%(%(#),,




*) $






	
	


",-  .+ /
	",	
!#+!()+'



	



/*"+*+'"-"+





	


%(&",-  .+ /
Figure 6: (Adagrad-norm vs.
SPS vs.
NGN) Experimental results on Deep Neural Net-
works (stochastic gradients).
All details and comments can be found in the text.
Shown is
performance for five or three hyperparameters (Table 3), each method is tuned to best at hyperpa-
rameter #2.
36

0
200
400
600
800
1000
Iteration
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Effective NGN stepsize value 
k
Small CNN on SVHN, 
= 3
0
5000
10000
15000
20000
Iteration
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Effective NGN stepsize value 
k
ResNet18 on CIFAR10, 
= 3
0.0e+0 2.0e+4 4.0e+4 6.0e+4 8.0e+4 1.0e+5 1.2e+5 1.4e+5
Iteration
10
2
10
1
100
Effective NGN stepsize value 
k
ResNet50 on ImageNet, 
= 3
Figure 7: Effective NGN stepsize in the deep learning experiments of Figure 5. Shown is mean
and 1 standard deviation of ğ›¾ğ‘˜for the best-performing ğœ.
decays automatically after warmup, it does not anneal to very small values â€“ which is needed to
counteract variance of stochastic gradients in the large dataset setting. In our Imagenet experiment,
supported by the theory in Section 4.3 ,we therefore automatically decay ğœ10-fold every 30 epochs.
The performance of NGN shines in the first 30 epochs exhibiting again a warmup-decay behavior.
The learning rate after the first epoch is closer to the one proposed by SGD, but convergence is
faster due to the initial highly-adaptive phase.
5
Conclusion and future work
In this paper, we introduced NGN, an adaptive optimizer with the same memory requirements
and per-iteration cost as SGD. We showed that this algorithm has intriguing and unique curvature
estimation properties, leading to features such as stability to hyperparameter tuning and adaptive
estimation of the Lipschitz constant. Our theory supports the empirical findings and the rationale
behind the NGN derivation. In the convex setting, NGN is guaranteed to converge with a sublinear
rate to any arbitrarily small neighborhood of the solution without requiring access to the gradient
Lipschitz constant (as instead needed in SGD) or access to the minimizer value of ğ‘“(as instead
needed in SPSmax (Loizou et al., 2021)). Among our results, our rates guarantee adaptive con-
vergence to an arbitrarily small neighborhood with optimal speed in the strongly convex setting,
a guarantee that is missing for both Adagrad and Polyak stepsize-based methods. NGN does not
require access to the value of the loss at the solution, and by decreasing its hyperparameter ğœ, NGN
becomes closer to the SGD update. Our empirical results on neural networks show that NGN is
stronger than vanilla SGD, SPS, and Adagrad and can be competitive with Adam even under heavy
hyperparameter tuning.
Our work also leaves open many important questions for future research. For example, it is
possible to obtain better convergence rates in the nonconvex case under the Polyak- Lojasiewicz
condition (e.g., Karimi et al., 2016).
A very interesting question is how we can incorporate
momentum into the NGN update rule and derive improved convergence rates. Along this direction,
there are two recent works on using momentum together with the stochastic Polyak stepsize: Wang
et al. (2023b) and Oikonomou and Loizou (2024), and we believe that similar extensions for NGN
are also possible. And finally, it will be very interesting to investigate the (block-) diagonal variant
37

of NGN, where each coordinate (or block coordinates) has its own NGN type of stepsize. The
coordinate- or block-adaptive variants may help boost performance for training transformers where
each parameter group has different curvature property (Noci et al., 2022).
References
Anil, R., Gupta, V., Koren, T., and Singer, Y. (2019). Memory efficient adaptive optimization.
Advances in Neural Information Processing Systems, 32.
Berrada, L., Zisserman, A., and Kumar, M. P. (2020).
Training neural networks for and by
interpolation. In International conference on machine learning, pages 799â€“809. PMLR.
Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for large-scale machine
learning. SIAM review, 60(2):223â€“311.
Chang, C.-C. and Lin, C.-J. (2011).
Libsvm: a library for support vector machines.
ACM
transactions on intelligent systems and technology (TIST), 2(3):1â€“27.
Chen, P. (2011). Hessian matrix vs. gaussâ€“newton hessian matrix. SIAM Journal on Numerical
Analysis, 49(4):1417â€“1435.
Cutkosky, A. (2020). Parameter-free, dynamic, and strongly-adaptive online learning. In Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 2250â€“2259. PMLR.
Defazio, A., Cutkosky, A., Mehta, H., and Mishchenko, K. (2023). When, why and how much?
adaptive learning rate scheduling by refinement. arXiv:2310.07831.
Defazio, A. and Mishchenko, K. (2023). Learning-rate-free learning by d-adaptation. In Proceed-
ings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of
Machine Learning Research, pages 7449â€“7479. PMLR.
Defazio, A., Xingyu, Yang, Mehta, H., Mishchenko, K., Khaled, A., and Cutkosky, A. (2024). The
road less scheduled. arXiv:2405.15682.
Delyon, B. and Juditsky, A. (1993). Accelerated stochastic approximation. SIAM Journal on
Optimization, 3(4):868â€“881.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248â€“255. Ieee.
Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7).
38

Faw, M., Rout, L., Caramanis, C., and Shakkottai, S. (2023). Beyond uniform smoothness: A
stopped analysis of adaptive sgd. In The Thirty Sixth Annual Conference on Learning Theory,
pages 89â€“160. PMLR.
Faw, M., Tziotis, I., Caramanis, C., Mokhtari, A., Shakkottai, S., and Ward, R. (2022). The power
of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance. In
Conference on Learning Theory, pages 313â€“355. PMLR.
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2020). Sharpness-aware minimization for
efficiently improving generalization. arXiv preprint arXiv:2010.01412.
Garrigos, G. and Gower, R. M. (2023). Handbook of convergence theorems for (stochastic) gradient
methods. arXiv preprint arXiv:2301.11235.
Geiping, J. and Goldstein, T. (2023). Cramming: Training a language model on a single gpu in one
day. In International Conference on Machine Learning, pages 11117â€“11143. PMLR.
Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM journal on optimization, 23(4):2341â€“2368.
Hazan, E. and Kakade, S. (2019). Revisiting the polyak step size. arXiv preprint arXiv:1905.00313.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770â€“778.
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L.,
Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language
models. arXiv preprint arXiv:2203.15556.
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaption. Neural
Networks, 1:295â€“307.
Karimi, H., Nutini, J., and Schmidt, M. (2016). Linear convergence of gradient and proximal-
gradient methods under the Polyak- lojasiewicz condition. In Frasconi, P., Landwehr, N., Manco,
G., and Vreeken, J., editors, Machine Learning and Knowledge Discovery in Databases (ECML
PKDD 2016), volume 9851 of Lectur Notes in Computer Sciencce. Springer.
Kesten, H. (1958).
Accelerated stochastic approximation.
Annals of Mathematical Statistics,
29(1):41â€“59.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
Lacoste-Julien, S., Schmidt, M., and Bach, F. (2012). A simpler approach to obtaining an o (1/t) con-
vergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002.
39

Le Cun, Y., Bottou, L., Orr, G. B., and MÂ¨uller, K.-R. (1998). Efficient backprop. In Neural
Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag.
Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares.
Quarterly of Applied Mathematics, 2(2):164â€“168.
Liu, Z., Nguyen, T. D., Ene, A., and Nguyen, H. (2023). On the convergence of adagrad (norm)
on Rğ‘‘: Beyond convexity, non-asymptotic rate and acceleration. In International Conference on
Learning Representations. International Conference on Learning Representations.
Loizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien, S. (2021). Stochastic polyak step-size
for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial
Intelligence and Statistics, pages 1306â€“1314. PMLR.
Loshchilov, I. and Hutter, F. (2017).
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101.
Ma, S., Bassily, R., and Belkin, M. (2018).
The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. In International Conference on
Machine Learning, pages 3325â€“3334. PMLR.
Mahmood, A. R., Sutton, R. S., Degris, T., and Pilarski, P. M. (2012). Tuning-free step-size
adaption. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 2121â€“2124.
Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters. SIAM
Journal on Applied Mathematics, 11(2):431â€“441.
Mirzoakhmedov, F. and Uryasev, S. P. (1983). Adaptive step adjustment for a stochastic optimization
algorithm. Zh. Vychisl. Mat. Mat. Fiz., 23(6):1314â€“1325. [U.S.S.R. Comput. Math. Math. Phys.
23:6, 1983].
Mishchenko, K. and Defazio, A. (2024). Prodigy: An expeditiously adaptive parameter-free learner.
arXiv:2306.06101.
Nesterov, Y. (2018). Lectures on convex optimization, volume 137 of Springer Optimization and
Its Applications. Springer, second edition.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. (2011). Reading digits
in natural images with unsupervised feature learning. In NIPS workshop on deep learning and
unsupervised feature learning. Granada, Spain.
Nocedal, J. and Wright, S. J. (2006). Numerical Optimization. Springer Series in Operations
Research. Springer, 2nd edition.
Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. (2022). Signal
propagation in transformers: Theoretical perspectives and the role of rank collapse.
40

Oikonomou, D. and Loizou, N. (2024). Stochastic polyak step-sizes and momentum: Convergence
guarantees and practical performance. arXiv:2406.04142.
Orabona, F. and PÂ´al, D. (2016). Coin betting and parameter-free online learning. Advances in
Neural Information Processing Systems, 29.
Orabona, F. and Tommasi, T. (2017). Training deep networks without learning rates through coin
betting. Advances in Neural Information Processing Systems, 30.
Orvieto, A., Kersting, H., Proske, F., Bach, F., and Lucchi, A. (2022a). Anticorrelated noise
injection for improved generalization. In International Conference on Machine Learning, pages
17094â€“17116. PMLR.
Orvieto, A., Kohler, J., Pavllo, D., Hofmann, T., and Lucchi, A. (2022b). Vanishing curvature in
randomly initialized deep relu networks. In International Conference on Artificial Intelligence
and Statistics.
Orvieto, A., Lacoste-Julien, S., and Loizou, N. (2022c). Dynamics of SGD with stochastic polyak
stepsizes: Truly adaptive variants and convergence to exact solution. In Advances in Neural
Information Processing Systems.
Orvieto, A., Raj, A., Kersting, H., and Bach, F. (2023). Explicit regularization in overparametrized
models via noise injection. In International Conference on Artificial Intelligence and Statistics,
pages 7265â€“7287. PMLR.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python.
the Journal of machine Learning research, 12:2825â€“2830.
Polyak, B. T. (1987). Introduction to Optimization. Translation series in mathematics and engi-
neering. Optimization Software, Inc., Publications Division, New York, NY.
Prazeres, M. and Oberman, A. M. (2021). Stochastic gradient descent with polyakâ€™s learning rate.
Journal of Scientific Computing, 89:1â€“16.
Reddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237.
Rolinek, M. and Martius, G. (2018). L4: Practical loss-based stepsize adaptation for deep learning.
Advances in neural information processing systems, 31.
Schmidt, R. M., Schneider, F., and Hennig, P. (2021). Descending through a crowded valley-
benchmarking deep learning optimizers. In International Conference on Machine Learning,
pages 9367â€“9376. PMLR.
Schraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. In Proceedings
of Nineth International Conference on Artificial Neural Networks (ICANN), pages 569â€“574.
41

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th International Conference on Machine
Learning, volume 28 of Proceedings of Machine Learning Research, pages 1139â€“1147, Atlanta,
Georgia, USA. PMLR.
Sutton, R. S. (1992). Adapting bias by gradient descent: An incremental version of Delta-Bar-
Delta. In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAIâ€™92),
pages 171â€“176. The MIT Press.
Vaswani, S., Laradji, I., Kunstner, F., Meng, S. Y., Schmidt, M., and Lacoste-Julien, S. (2020).
Adaptive gradient methods converge faster with over-parameterization (but you should do a
line-search). arXiv preprint arXiv:2006.06835.
Wang, B., Zhang, H., Ma, Z., and Chen, W. (2023a). Convergence of adagrad for non-convex
objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual Conference on
Learning Theory, pages 161â€“190. PMLR.
Wang, X., Johansson, M., and Zhang, T. (2023b). Generalized polyak step size for first order
optimization with momentum. In Proceedings of the 40th International Conference on Machine
Learning, volume 202 of Proceedings of Machine Learning Research, pages 35836â€“35863.
PMLR.
Ward, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. Journal of Machine Learning Research, 21(219):1â€“30.
Yang, J., Li, X., Fatkhullin, I., and He, N. (2023). Two sides of one coin: the limits of untuned sgd
and the power of adaptive methods. Advances in Neural Information Processing Systems, 36.
Zamani, M. and Glineur, F. (2023). Exact convergence rate of the last iterate in subgradient
methods. arXiv:2307.11134.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107â€“115.
42

