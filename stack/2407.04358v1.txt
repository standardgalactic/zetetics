An Adaptive Stochastic Gradient Method
with Non-negative Gauss-Newton Stepsizes
Antonio Orvieto∗
Lin Xiao†
Abstract
We consider the problem of minimizing the average of a large number of smooth but
possibly non-convex functions. In the context of most machine learning applications, each
loss function is non-negative and thus can be expressed as the composition of a square and
its real-valued square root. This reformulation allows us to apply the Gauss-Newton method,
or the Levenberg-Marquardt method when adding a quadratic regularization. The resulting
algorithm, while being computationally as efficient as the vanilla stochastic gradient method, is
highly adaptive and can automatically warmup and decay the effective stepsize while tracking
the non-negative loss landscape. We provide a tight convergence analysis, leveraging new
techniques, in the stochastic convex and non-convex settings.
In particular, in the convex
case, the method does not require access to the gradient Lipshitz constant for convergence,
and is guaranteed to never diverge. The convergence rates and empirical evaluations compare
favorably to the classical (stochastic) gradient method as well as to several other adaptive
methods.
Contents
1
Introduction
2
1.1
Adaptive stepsizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Contributions and outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
Deterministic setting: derivation and basic properties
6
2.1
Algorithm derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.2
Basic properties of NGN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2.1
Non-negative estimation . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.2
Range of NGN stepsize . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.3
Connection with Polyak stepsize . . . . . . . . . . . . . . . . . . . . . . .
9
2.3
Experiments on convex classification problems
. . . . . . . . . . . . . . . . . . .
12
∗ELLIS Institute T¨ubingen, Max Planck Institute for Intelligent Systems, T¨ubingen AI Center, T¨ubingen, Germany.
Work partially carried out at Meta in Seattle, USA. Email: antonio@tue.ellis.eu
†Fundamental AI Research (FAIR) at Meta, Seattle, USA. Email: linx@meta.com
1
arXiv:2407.04358v1  [math.OC]  5 Jul 2024

3
A generalized Gauss-Newton perspective
12
4
Stochastic setting: convergence rates and experiments
14
4.1
Preliminary lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
4.2
Convergence rates for fixed regularization . . . . . . . . . . . . . . . . . . . . . .
17
4.2.1
Convex and strongly convex settings . . . . . . . . . . . . . . . . . . . . .
18
4.2.2
Proofs for main results . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
4.2.3
General non-convex setting . . . . . . . . . . . . . . . . . . . . . . . . . .
26
4.3
Technique sketch for annealed regularization . . . . . . . . . . . . . . . . . . . . .
29
4.4
Deep learning experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
5
Conclusion and future work
37
1
Introduction
We consider the problem of minimizing the average of a large number of loss functions:
minimize
𝑥∈R𝑑
𝑓(𝑥) := 1
𝑁
𝑁
∑︁
𝑖=1
𝑓𝑖(𝑥),
(1)
where each 𝑓𝑖is assumed to be lower bounded, differentiable and have Lipschitz-continuous
gradients. Specifically, we assume that for each 𝑖, there exist a constant 𝐿𝑖such that
∥∇𝑓𝑖(𝑥) −∇𝑓𝑖(𝑦)∥≤𝐿𝑖∥𝑥−𝑦∥,
∀𝑥, 𝑦∈R𝑑.
Consequently, the average gradient ∇𝑓has a Lipschitz constant 𝐿≤(1/𝑁) P𝑁
𝑖=1 𝐿𝑖.
In machine learning applications, each 𝑓𝑖corresponds to the loss function associated with a data
point or the average of a mini-batch (e.g., Bottou et al., 2018). For large 𝑁, the cost of frequent
averaging over all data points can be prohibitive, therefore the method of choice (in both convex
and non-convex settings) is often some variant of Stochastic Gradient Descent (SGD):
𝑥𝑘+1 = 𝑥𝑘−𝛾𝑘∇𝑓𝑖𝑘(𝑥𝑘),
(2)
where 𝑖𝑘∈{1, 2, . . . , 𝑁} =: [𝑁] is a randomly picked data point or the index of a mini-batch, and
𝛾𝑘> 0 is the stepsize (also known as the learning rate) selected at iteration 𝑘.
The simplest choice is a constant stepsize 𝛾𝑘= 𝛾for all 𝑘≥0. If 𝛾is sufficiently small, say
𝛾< 2/𝐿under the smoothness assumption, then convergence to a neighborhood of a stationary
point can be established (e.g., Ghadimi and Lan, 2013). However, the Lipschitz constant 𝐿is often
unknown and hard to estimate. Under a (deterministic) decreasing stepsize rule such as 𝛾𝑘= 𝛾0/
√
𝑘,
convergence to a local minimizer is eventually recovered for any 𝛾0 > 0 since 𝛾𝑘< 2/𝐿after some
iterations. Nevertheless, in practice, tuning 𝛾0 is still needed to avoid numerical overflows and
instabilities, and to obtain optimal performance. Indeed, Yang et al. (2023) refined the classical
result by Ghadimi and Lan (2013) to show that averaged gradients of the SGD iterates converge to
zero for any 𝛾0 > 0, but it takes exponential time to recover from a poorly tuned 𝛾0.
2

1.1
Adaptive stepsizes
Rather than relying on the knowledge of problem-dependent constants such as 𝐿, adaptive stepsizes
exploit additional information from the online iterates 𝑥𝑘and ∇𝑓𝑖𝑘(𝑥𝑘) to obtain faster convergence.
For example, Kesten (1958) proposed to adjust 𝛾𝑘based on the sign of the inner product of
consecutive stochastic gradients, an idea further developed by Mirzoakhmedov and Uryasev (1983)
and Delyon and Juditsky (1993). Similar techniques have also been investigated in the machine
learning community (e.g., Jacobs, 1988; Sutton, 1992; Schraudolph, 1999; Mahmood et al., 2012).
AdaGrad and variants.
Stemming from the seminal work of AdaGrad by Duchi et al. (2011),
modern variants of adaptive stochastic gradient methods employ coordinate-wise stepsizes. Specif-
ically, instead of a single stepsize 𝛾𝑘∈R+, one maintains a vector Γ𝑘∈R𝑑
+ and the SGD update
takes the form
𝑥𝑘+1 = 𝑥𝑘−Γ𝑘⊙∇𝑓𝑖𝑘(𝑥𝑘),
(3)
where ⊙denotes the element-wise product of two vectors. Since in this paper we focus on adapting
the scalar stepsize 𝛾𝑘in (2), it’s more convenient to consider a scalar variant of AdaGrad, called
AdaGrad-norm (Ward et al., 2020), which sets the stepsize in (2) as
𝛾𝑘=
𝜂
√︃
𝛿2
0 + P𝑘
𝜏=1 ∥∇𝑓𝑖𝜏(𝑥𝜏)∥2
,
(4)
where 𝜂and 𝛿0 are hyper-parameters to be tuned. (The original AdaGrad sets the coordinate-wise
stepsizes Γ𝑘
𝑗in (3) by replacing the gradient norm ∥∇𝑓𝑖𝜏(𝑥𝜏)∥in (4) with its 𝑗th element.) The
cumulative sum across iterations in the denominator of (4) implies that the stepsize decreases
monotonically, a feature shared with simple stepsize rules such as 𝛾0/
√
𝑘and is key to obtaining
the classical 𝑂(1/
√
𝑘) convergence rate in the convex case (Duchi et al., 2011; Reddi et al., 2019).
However, monotonically decreasing stepsizes cannot efficiently navigate complex landscapes with
varying local curvatures and they often lead to very slow convergence in practice.
More sophisticated adaptive methods (e.g., Sutskever et al., 2013) replace ∇𝑓𝑖𝑘(𝑥𝑘) in (2) with
the exponential moving average of stochastic gradients (often called momentum). In addition,
Adam (Kingma and Ba, 2014) replaces the cumulative sum in the denominator of (4) with their
exponential moving average (coordinate-wise). In particular, the latter change allows the stepsize
to decrease or increase depending on the local features of the loss landscape, which contributes to
its better performance for training deep-learning models (e.g., Schmidt et al., 2021).
Stochastic Polyak stepsizes.
A classical non-monotone adaptive stepsize rule is the Polyak
stepsize (Polyak, 1987, §5.3). For the full gradient method 𝑥𝑘+1 = 𝑥𝑘−𝛾𝑘∇𝑓(𝑥𝑘), it sets
𝛾𝑘= 𝑓(𝑥𝑘) −𝑓∗
∥∇𝑓(𝑥𝑘)∥2,
(5)
where 𝑓∗= inf𝑥∈R𝑑𝑓(𝑥). Here, ∇𝑓(𝑥𝑘) can be a subgradient if 𝑓is nondifferentiable. Although
Polyak (1987) derived it in the context of convex, nonsmooth optimization, Hazan and Kakade
3

(2019) showed that it achieves the optimal convergence rates of gradient descent for minimizing
smooth and/or strongly convex functions as well, without a priori knowledge of the smoothness
and strong convexity parameters. However, a crucial issue of the Polyak stepsize is that the optimal
value 𝑓∗needs to be known beforehand, which severely limits its application in practical settings.
Nevertheless, there has been, in the last few years, a rising interest in this method in deep
learning, where often 𝑓∗is zero—linked to over-parametrization (Zhang et al., 2021). There are
several adaptations to the Polyak stepsize to stochastic gradient methods (Rolinek and Martius,
2018; Berrada et al., 2020; Prazeres and Oberman, 2021), with the most theoretically sound being
the variant of Loizou et al. (2021), proposing the stochastic Polyak stepsize (SPS)
𝛾𝑘= min
(
𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘
𝑐∥∇𝑓𝑖𝑘(𝑥𝑘)∥2, 𝛾𝑏
)
,
(6)
where 𝑐, 𝛾𝑏∈R≥0 are hyperparameters and 𝑓∗
𝑖𝑘= inf𝑥∈R𝑑𝑓𝑖𝑘(𝑥) is the minimum valued of 𝑓𝑖𝑘.
While 𝑓𝑖𝑘is in principle unknown, in the setting with small batch sizes and no regularization, it is
clear that one can set 𝑓∗
𝑖𝑘= 0. Loizou et al. (2021) provided a thorough analysis of this method,
showing that convergence to a neighborhood of the solution can be achieved without knowledge of
the gradient Lipschitz constant 𝐿.
Even in the over-parametrized setting, as soon as regularization is added, we always have 𝑓∗
𝑖𝑘> 0
and its exact value is hard to estimate1. In addition, a severe limitation of SPS is that the convergence
guarantee in the stochastic setting is for a neighborhood that is independent of the hyperparameters.
As such, no convergence to an arbitrary suboptimality can be guaranteed. Orvieto et al. (2022c)
addressed this problem by proposing DecSPS, a variant that gradually decreases 𝛾𝑏in (6) and can
replace 𝑓∗
𝑖𝑘by a lower bound, and proved its convergence to the actual problem solution (i.e., with
arbitrary precision) without knowledge of 𝐿. However, their analysis requires strong convexity to
effectively bound the iterates.
Other related work.
Another related line of work is on parameter-free methods for online
optimization (Orabona and P´al, 2016; Orabona and Tommasi, 2017; Cutkosky, 2020) and the D-
adaptation methods (Defazio and Mishchenko, 2023; Mishchenko and Defazio, 2024). Given a
fixed number of steps to run, Zamani and Glineur (2023) showed that the subgradient method with
a linearly decaying stepsize enjoys last-iterate convergence, and Defazio et al. (2023) extended the
result to stochastic gradient methods with an online-to-batch conversion. Very recently, Defazio
et al. (2024) proposed a method that combines the Polyak-Ruppert averaging with momentum and
demonstrated promising performance gains for both convex optimization and deep learning.
1.2
Contributions and outline
In this paper, we propose a new adaptive stepsize strategy that shows promising performance
compared to previous approaches both in theory and in practice. The basic idea is to exploit the
1If the problem is overparametrized (𝑑≫𝑁), often 𝑓(𝑥) = 0 can be achieved by some 𝑥∗∈R𝑑. However there
exist no 𝑥such that, e.g. 𝑓(𝑥) + 𝜆
2 ∥𝑥∥2 = 0 (L2 regularization). Loizou et al. (2021) discusses this issue.
4

Method
Adaptive
Features
Non-monotonic
$k
Convergence to
Arbitrary Suboptimality
No Additional
Assumptions
SGD
7
7
3
3
Adagrad
3
7
3
3
SPS
3
3
7
3
DecSPS
3
3
3
7
NGN
3
3
3
3
Table 1: Convergence features of SGD and some adaptive methods with convergence guarantees
in stochastic convex smooth optimization. Discussion in Sections 1.1 and 1.2.
nonnegativity of the loss functions (ubiquitous in machine learning) by expressing them as the
composition of a square and their real-valued square roots. This reformulation allows us the apply
the classical Gauss-Newton method (e.g., Nocedal and Wright, 2006, §10.3), or the Levenberge-
Marquardt method when adding a quadratic regularization (Levenberg, 1944; Marquardt, 1963).
By minimizing the Nonnegative Gauss-Newton (NGN) estimates of 𝑓𝑖𝑘, we derive the NGN stepsize
𝛾𝑘=
𝜎
1 +
𝜎
2 𝑓𝑖𝑘(𝑥𝑘) ∥∇𝑓𝑖𝑘(𝑥𝑘)∥2,
(7)
where 𝜎> 0 is a regularization parameter.
In Section 2, we focus on the deterministic setting (when the full gradient ∇𝑓(𝑥𝑘) is available
or 𝑁= 1) to derive the NGN stepsize and study its basic properties:
• Our derivation reveal that NGN has a clear connection to second-order methods, while
sharing the same low computational cost as SGD (Section 2.1).
• NGN is stable to the selection of its hyperparameter 𝜎: empirically, non-diverging dynamics
are observed for any choice of 𝜎—a feature that sets the method drastically apart from vanilla
SGD that we prove precisely in the convex stochastic setting (Section 2.2).
• NGN interpolates between an agnostic version of the Polyak stepsize (setting 𝑓∗= 0) and a
small constant stepsize (Section 2.2.3), leading to an adaptive non-monotonic behavior that
can effectively warm-up in the beginning and then settle down near the solution (Section 2.3).
In Section 3, we present an alternative derivation of NGN from a generalized Gauss-Newton
perspective, further revealing its curvature adaptation property. In Section 4, we present our main
results on convergence analysis in the stochastic setting:
• For minimizing smooth convex loss functions (Section 4.2.1), we show that NGN stepsize
can converge to a neighborhood of arbitrary size around the solution2 without knowledge of
2For SPS Loizou et al. (2021) the convergence neighborhood is independent of the hyperparameters.
5

the gradient Lipschitz constant and without assumptions such as bounded domain or iterates.
The method also provably never diverges. Compared to SPS, it behaves much more favorably
under no knowledge of 𝑓∗or the single 𝑓∗
𝑖𝑘s. Compared to Adagrad variants, our analysis can
be extended to the strongly convex setting with fully-adaptive linear rate.
• For non-convex smooth optimization (Section 4.2.3), NGN stepsize is guaranteed to converge
at the same speed as SGD with constant stepsize. In both the convex and non-convex settings,
we can gradually decrease the hyperparameter 𝜎and obtain asymptotic convergence in the
classical sense (Section 4.3).
• In Section 4.4, we conduct extensive numerical experiments on deep learning tasks. We found
that NGN stepsize has clear advantage over SGD with constant stepsize, SPS, and AdaGrad-
norm in terms of convergence speed and robustness against varying hyperparameters. While
our main focus is on adaptive scalar stepsizes for the basic SGD method (2), we also conduct
experiments to compare with the popular Adam method (Kingma and Ba, 2014), which uses
both momentum and coordinate-wise stepsizes. Our results show that NGN performs better
than Adam in minimizing the training loss and is more robust to hyperparameter tuning, not
to mention that NGN takes much less memory to implement.
In Section 5, we conclude the paper and point to several interesting directions for future work.
2
Deterministic setting: derivation and basic properties
We derive our algorithm when the full gradient ∇𝑓(𝑥𝑘) is available (Section 2.1), which corresponds
to the case of 𝑁= 1 in (1) or when 𝑁is relatively small. Most of our geometric intuition about
NGN is presented in the deterministic setting, in Section 2.2. In Section 3, we provide a more
general perspective related to the generalized Gauss-Newton method.
2.1
Algorithm derivation
Assume 𝑓: R𝑑→R is differentiable and non-negative, i.e.,
𝑓∗:= inf
𝑥∈R𝑑𝑓(𝑥) ≥0.
We define 𝑟(𝑥) :=
√︁
𝑓(𝑥) and consequently have 𝑓(𝑥) = 𝑟2(𝑥). Therefore,
∇𝑓(𝑥) = 2𝑟(𝑥)∇𝑟(𝑥)
and
∇𝑟(𝑥) =
1
2
√︁
𝑓(𝑥)
∇𝑓(𝑥).
The Gauss-Newton update leverages a first-order Taylor expansions of 𝑟(𝑥+ 𝑝) around 𝑥:
𝑓(𝑥+ 𝑝) = 𝑟2(𝑥+ 𝑝) ≃ 𝑟(𝑥) + ∇𝑟(𝑥)⊤𝑝2 .
We use this approximation to estimate 𝑓(𝑥+ 𝑝) around 𝑥and propose an update 𝑝by minimizing
˜𝑓
NGN
𝜎
(𝑥+ 𝑝) :=  𝑟(𝑥) + ∇𝑟(𝑥)⊤𝑝2 + 1
2𝜎∥𝑝∥2,
(8)
6

where ∥· ∥is the Euclidean norm and
1
2𝜎∥𝑝∥2 is a regularization term expressing the confidence
𝜎> 0 in the estimate ˜𝑓NGN
𝜎
. Crucially, we note that this approximation preserves non-negativity, in
contrast to the Taylor expansion of 𝑓(Chen, 2011). The minimizer of ˜𝑓NGN
𝜎
(𝑥+ 𝑝) with respect to
𝑝can be found by setting ∇𝑝˜𝑓NGN
𝜎
(𝑥+ 𝑝) = 0, i.e.,
∇𝑝˜𝑓
NGN
𝜎
(𝑥+ 𝑝) = 2  𝑟(𝑥) + ∇𝑟(𝑥)⊤𝑝 ∇𝑟(𝑥) + 1
𝜎𝑝
= 2𝑟(𝑥)∇𝑟(𝑥) +

2∇𝑟(𝑥)∇𝑟(𝑥)⊤+ 1
𝜎𝐼

𝑝!= 0,
where 𝐼is the 𝑑× 𝑑identity matrix. Therefore 𝑝∈R𝑑needs to satisfy the normal equation

∇𝑟(𝑥)∇𝑟(𝑥)⊤+ 1
2𝜎𝐼

𝑝= −𝑟(𝑥)∇𝑟(𝑥).
This equation can be solved analytically using the Sherman-Morrison formula3:
𝑝= −

∇𝑟(𝑥)∇𝑟(𝑥)⊤+ 1
2𝜎𝐼
−1
𝑟(𝑥)∇𝑟(𝑥) = −2𝜎𝑟(𝑥)∇𝑟(𝑥)
1 + 2𝜎∥∇𝑟(𝑥)∥2 .
Now recall that 𝑟(𝑥) =
√︁
𝑓(𝑥) and ∇𝑟(𝑥) =
1
2√
𝑓(𝑥)∇𝑓(𝑥), so that ∥∇𝑟(𝑥)∥2 =
1
4 𝑓(𝑥) ∥∇𝑓(𝑥)∥2 and
𝑟(𝑥)∇𝑟(𝑥) = 1
2∇𝑓(𝑥). Therefore, we get
𝑝
NGN
𝜎
= −
𝜎∇𝑓(𝑥)
1 +
𝜎
2 𝑓(𝑥) ∥∇𝑓(𝑥)∥2 .
(9)
The derivation above suggests the update rule 𝑥←𝑥+ 𝑝NGN
𝜎, leading to the deterministic NGN
(NGN-det) method:
𝑥𝑘+1 = 𝑥𝑘−
𝜎
1 +
𝜎
2 𝑓(𝑥𝑘) ∥∇𝑓(𝑥𝑘)∥2 ∇𝑓(𝑥𝑘).
(10)
It is a form of gradient descent 𝑥𝑘+1 = 𝑥𝑘−𝛾𝑘∇𝑓(𝑥𝑘) with the adaptive NGN stepsize
𝛾
NGN
𝑘
:=
𝜎
1 +
𝜎
2 𝑓(𝑥𝑘) ∥∇𝑓(𝑥𝑘)∥2 .
(11)
In the rest of this section, we study the basic properties and convergence guarantees of NGN-det.
2.2
Basic properties of NGN
We give below an overview of the properties of the NGN stepsizes, here discussed in the determinis-
tic setting but with direct application in the stochastic case. Some of these properties constitute the
workhorse of our convergence analysis, in Section 4. However, we do not provide the convergence
rates in the deterministic setting here because they can be obtained from the results in Section 4 by
setting 𝑁= 1.
3Let 𝐴be an invertible matrix in R𝑑×𝑑and 𝑢, 𝑣∈R𝑑, then (𝐴+ 𝑢𝑣⊤)−1 = 𝐴−1 −𝐴−1𝑢𝑣⊤𝐴−1
1+𝑣⊤𝐴−1𝑢.
7

γ
γ
γ
Figure 1: NGN and GD updates and corresponding objective function estimate in Eqs. (12) and (13)
on a few toy examples (inspired by Chen (2011)). The black dot denotes the initial 𝑥, ald the star is
the position after one step: 𝑥+ 𝑝. Compared to GD with stepsize 𝛾= 𝜎, NGN is more conservative
if the landscape is sharp. Note that the function approximation provided by NGN is always non-
negative, as clear from our motivation and the algorithm derivation.
2.2.1
Non-negative estimation
The NGN update 𝑝NGN
𝜎(𝑥) in (9) leverages a local first-order non-negative estimate of the loss 𝑓:
𝑝
NGN
𝜎(𝑥) = arg min
𝑝∈R𝑑
"
˜𝑓
NGN
𝜎
(𝑥+ 𝑝) :=
√︁
𝑓(𝑥) +
1
2
√︁
𝑓(𝑥)
∇𝑓(𝑥)⊤𝑝
2
+ 1
2𝜎∥𝑝∥2
#
.
(12)
This feature is fundamental to NGN and draws a clear distinction from vanilla gradient descent (GD):
indeed, the update 𝑥←𝑥+ 𝑝GD of GD with constant stepsize 𝛾is the result of minimization of a
first-order estimate of 𝑓which is not necessarily non-nagative (as opposed to the original 𝑓):
𝑝
GD
𝛾(𝑥) = arg min
𝑝∈R𝑑

˜𝑓
GD
𝛾(𝑥+ 𝑝) := 𝑓(𝑥) + ∇𝑓(𝑥)⊤𝑝+ 1
2𝛾∥𝑝∥2

= −𝛾∇𝑓(𝑥).
(13)
The distinction outlined above can be visualized on toy examples in Figure 1. It is clear from
these examples that the NGN update, minimizing a non-negative first-order estimate of 𝑓, is more
conservative compared to GD with stepsize 𝛾= 𝜎(the NGN regularization hyperparameter),
especially in regions where gradients are large. The stepsize range of NGN will be characterized
more precisely in Section 2.2.2, and is linked to the adaptive nature of our method, described later in
Section 2.2.3. We note that it is precisely this conservative nature that allows the convergence rates
in Section 4 to hold for an arbitrarily large 𝜎—in contrast to gradient descent and most adaptive
methods, which diverge for large stepsize parameters.
2.2.2
Range of NGN stepsize
Suppose that 𝑓: R𝑑→R is 𝐿-smooth and has minimum value 𝑓∗. Then for all 𝑥∈R𝑑, we have
(e.g., Nesterov, 2018, §2.1).
2𝐿( 𝑓(𝑥) −𝑓∗) ≥∥∇𝑓(𝑥)∥2.
8

Since 𝑓∗≥0, we obviously have 2𝐿𝑓(𝑥) ≥∥∇𝑓(𝑥)∥2, which is equivalent to
0 ≤∥∇𝑓(𝑥)∥2
2 𝑓(𝑥)
≤𝐿.
These bounds directly imply a range for 𝛾𝑘, as characterized in the following lemma.
Lemma 2.1 (Stepsize bounds). Suppose 𝑓: R𝑑→R is non-negative, differentiable and 𝐿-smooth.
Then the NGN-det stepsize given in (11) satisfies
𝛾
NGN
𝑘
∈
h
𝜎
1 + 𝜎𝐿, 𝜎
i
=

1
𝐿+ 𝜎−1, 𝜎

.
(14)
This property shows that the maximum achievable stepsize is bounded by 𝜎, but the algorithm
can adaptively decrease it until 1/(𝐿+ 𝜎−1) if the landscape gets more challenging.
2.2.3
Connection with Polyak stepsize
The reader can readily spot a connection between the NGN-det stepsize (11) and the Polyak
stepsize (5). To be more precise, given 𝑓∗≥0, we define the 𝑓∗-agnostic Polyak stepsize (APS)
𝛾
APS
𝑘
=
𝑓(𝑥𝑘)
∥∇𝑓(𝑥𝑘)∥2 .
(15)
We can express the NGN stepsize as the harmonic mean of APS and the constant stepsize 𝜎/2:
𝛾
NGN
𝑘
=
2
2
𝜎+ ∥∇𝑓(𝑥𝑘)∥2
𝑓(𝑥𝑘)
=
2
1
𝜎/2 +
1
𝛾APS
𝑘
.
Intuitively, as 𝜎increases (less regularization in (8)) , NGN relies more on the Gauss-Newton
approximation. In the limiting case of 𝜎→∞, we have 𝛾NGN
𝑘
→2𝛾APS
𝑘, which is two times the
APS. On the other hand, as 𝜎→0, the regularization in (8) dominates the approximation and we
get back to gradient descent with a small constant stepsize 𝛾𝑘= 𝜎. As such,
NGN interpolates between the 𝑓∗-agnostic Polyak stepsize and a small constant stepsize.
We like to spend a few more words on the insightful connection between PS and NGN. As
already mentioned, one obvious drawback of PS, persisting in its stochastic variant SPS (Loizou
et al., 2021), is the required knowledge of 𝑓∗= inf𝑥𝑓(𝑥). However, Orvieto et al. (2022c) showed
that convergence to a ball around the solution, of size proportional to 𝑓∗, can be retrieved both in the
deterministic and stochastic setting by replacing 𝑓∗with any lower bound ℓ∗≤𝑓∗—in particular
ℓ∗= 0, which becomes NGN with 𝜎→∞. It is therefore interesting to compare NGN with the
following PS variant inspired by the SPSmax rule (Loizou et al., 2021; Orvieto et al., 2022c):
𝛾
APSmax
𝑘
= min

𝑓(𝑥𝑘)
𝑐∥∇𝑓(𝑥𝑘)∥2, 𝛾𝑏

,
(16)
9

Figure 2: Optimization dynamics of constant-stepsize GD, NGN, and APSmax on the toy example
𝑓(𝑥) = 𝜆
2 (𝑥−𝑥∗)2 + 𝑓∗, for different hyperparameter values. NGN is stable for any finite 𝜎> 0.
Dashed line in the bottom row is the value 2/𝜆.
where 𝑐> 0 is a hyperparameter and 𝛾𝑏> 0 bounds the maximum stepsize achievable. For this
purpose, we consider the simplest convex optimization setting with a one-dimensional quadratic
𝑓(𝑥) = 𝜆
2 (𝑥−𝑥∗)2 + 𝑓∗, where 𝑓∗= 0.1 > 0 and 𝜆= 1.2. Figure 2 shows the loss dynamics for GD,
NGN and APSmax with 𝛾𝑏= 3 (results are not sensitive to 𝛾𝑏when it is big enough). Ablating on
different values of 𝜎for NGN, 𝑐for APSmax and 𝛾for constant-stepsize GD, one notices a striking
feature of NGN: its iterates never grow unbounded for any value of 𝜎. Instead, for 𝛾big enough
or 𝑐small enough, GD and APSmax grow unbounded. This property of NGN is also observed in
our experiments on convex classification (Fig. 3) and neural networks (Fig. 5), and we will give
it a formal proof in Section 4. If 𝑐has a high enough (𝐿-independent) value, then Orvieto et al.
(2022c) showed that SPSmax also does not grow unbounded. However, it cannot obtain an arbitrary
suboptimality (by tuning the hyperparameters of 𝛾
APSmax
𝑘
)—which instead can be achieved by NGN
in this setting, as shown by our theory in Section 4. This issue of SPSmax is related to its bias
problem, thoroughly explored in Orvieto et al. (2022c).
From Figure 2, we also observe another important property of NGN with large 𝜎: its stepsize
converges to a value bounded by 2/𝜆—the curvature-informed stability threshold for the corre-
sponding dynamics. We remark that both 𝜆and 𝑓∗do not appear in the NGN update rule. This
shows a remarkable curvature adaptation property of NGN, which is linked to the derivation in
Section 3. In contrast, APSmax, while in perfect agreement with the findings in Orvieto et al.
(2022c) converges to a ball around the solution for big enough hyperparameter 𝑐, its performance
is negatively affected by having no access to 𝑓∗(cf. Figure 3).
10

Dataset
Features Dimension
# Datapoints
# Classes
L2 Regularization
Cancer
30
569
2
1 × 10−4
Wine
13
178
3
1 × 10−4
Digits
64
1797
10
1 × 10−4
Table 2: LIBSVM (Chang and Lin, 2011) dataset information, results in Figure 3.
Armijo
Armijo
Armijo
Figure 3: Deterministic NGN compared with constant-stepsize Gradient Descent, Polyak Stepsizes,
and Armijo Line search on three classification datasets listed in Table 2. The center column shows
the optimality gap 𝑓(𝑥𝑘) −𝑓∗, the right column plots the evolution of stepsizes, and the right
column shows the projection of the iterate trajectories onto the top-two PCA components. On the
trajectories plot, the circle denotes the starting point, the star denotes the solution found at the last
iteration, and the square represents the point after one iteration.
11

2.3
Experiments on convex classification problems
In Figure 3, we evaluate the full-batch training performance of NGN-det on convex classifica-
tion (using the cross-entropy loss). Specifically, we consider three classification datasets from
LIBSVM (Chang and Lin, 2011), whose details are given in Table 2.
We compare NGN with constant-stepsize gradient descent (GD), standard Polyak stepsize (PS)
with or without knowledge of 𝑓∗> 0, and GD with Armijo line search (LS). For GD and NGN-det,
we sweep over hyperparameters (𝛾and 𝜎) on a logarithmic grid and denote the best-performing
option with marker . Performace for 3-times-lower and 3-times-higher hyperparameters is marked
by symbols
and
, respectively. For all PS variants, we select 𝑐= 1 (see Section 2.2.3), as
suggested by the results in Loizou et al. (2021).
In Figure 3, we plot the loss function optimality gap, the stepsize, as well as the projection
of the iterate dynamics onto the top-two principle components. They clearly showcase NGN’s
adaptivity to large gradients at the beginning and its stability to high values of 𝜎(cf. Section 2.2.3).
Moreover, the dynamics of the NGN stepsize exhibit a warmup phase followed by convergence
to a flat value towards the end (𝛾𝑘→𝜎if 𝜎is not too large); see bounds in Section 2.2.2. A
similar behavior is observed for the line-search stepsize (LS) and for PS using the correct value
of 𝑓∗(here 𝑓∗> 0 because we have more datapoints than dimensions; see Table 2). Of course, in
practice 𝑓∗is not accessible beforehand. Plugging in the lower bound 𝑓∗= 0 still guarantees PS
approaches the minimal loss (see discussion in Section 2.2.3), but the iterates do not converge to
the true solution—in contrast to NGN (See Figure 2 in Orvieto et al. (2022a)).
3
A generalized Gauss-Newton perspective
Before moving on to the stochastic case and presenting the convergence analysis, we provide here
an alternative derivation of the NGN method—with increased generality compared to the one given
in Section 2.1. This generalized Gauss-Newton (GNN) perspective allows us to make a closer
connection to second-order methods and better understand the curvature adaptation property of
NGN demonstrated in Figures 2 and 3.
Suppose 𝑓: R𝑑→R can be written as the composition of two functions: 𝑓(𝑥) = ℎ(𝑐(𝑥)),
where ℎ: R →R is differentiable and nonnegative and 𝑐: R𝑑→R is differentiable. In the setting
of Section 2.1, we have ℎ(𝑐) = 𝑐2 and 𝑐(𝑥) = 𝑟(𝑥) =
√︁
𝑓(𝑥). The gradient of 𝑓is
∇𝑓(𝑥) = ℎ′(𝑐(𝑥))∇𝑐(𝑥),
and its Hessian can be written as
∇2 𝑓(𝑥) = ℎ′(𝑐(𝑥))∇2𝑐(𝑥) + ℎ′′(𝑐(𝑥))∇𝑐(𝑥)∇𝑐(𝑥)⊤
= ℎ′(𝑐(𝑥))∇2𝑐(𝑥)
|             {z             }
Difficult to compute
+ ℎ′′(𝑐(𝑥))
ℎ′(𝑐(𝑥))2 ∇𝑓(𝑥)∇𝑓(𝑥)⊤
|                          {z                          }
Easy to compute
,
(17)
12

where in the last equality we applied the substitution ∇𝑐(𝑥) = ∇𝑓(𝑥)/ℎ′(𝑐(𝑥)).
Notice that
ℎ′(𝑐(𝑥)) = 0 if and only if ℎis minimized at 𝑐(𝑥) and thus 𝑓is minimized at 𝑥. Therefore we can
assume without loss of generality that ℎ′(𝑐(𝑥)) ≠0.
We have seen in (13) that the GD update 𝑝GD
𝜎
minimizes the first-order approximation ˜𝑓GD
𝜎
.
Similarly, the canonical second-order method, Newton’s method, derive the update as
𝑝Newton = arg min
𝑝

˜𝑓Newton(𝑥+ 𝑝) := 𝑓(𝑥) + ∇𝑓(𝑥)⊤𝑝+ 1
2 𝑝⊤∇2 𝑓(𝑥)𝑝

,
where ∇2 𝑓(𝑥) is given in (17). In order to avoid the “difficult-to-compute” part of the Hessian, the
generalized Gauss-Newton (GGN) method replaces it with a simple approximation 1
𝜎𝐼:
˜𝑓GGN
𝜎
(𝑥+ 𝑝) := 𝑓(𝑥) + ∇𝑓(𝑥)⊤𝑝+ 1
2 𝑝⊤
 1
𝜎𝐼+ 𝑞(𝑥)∇𝑓(𝑥)∇𝑓(𝑥)⊤

𝑝.
(18)
where
𝑞(𝑥) := ℎ′′(𝑐(𝑥))
ℎ′(𝑐(𝑥))2 .
The GGN update 𝑝GGN is the minimizer of ˜𝑓GGN
𝜎
(𝑥+ 𝑝), which we obtain by forcing its gradient
with respect to 𝑝to be zero:
∇𝑝˜𝑓GGN
𝜎
(𝑥+ 𝑝) = ∇𝑓(𝑥) +
 1
𝜎𝐼+ 𝑞(𝑥)∇𝑓(𝑥)∇𝑓(𝑥)⊤

𝑝!= 0,
(19)
and it results in
𝑝GGN = −
𝜎
1 + 𝜎𝑞(𝑥)∥∇𝑓(𝑥𝑘)∥2 ∇𝑓(𝑥𝑘).
(20)
Next we give several examples of the GGN update by choosing a specific function ℎ.
• Quadratic. In this case, we have ℎ(𝑐) = 𝑐2, ℎ′(𝑐) = 2𝑐and ℎ′′(𝑐) = 2, thus
𝑞(𝑥) =
1
𝑐(𝑥)2 =
1
2 𝑓(𝑥),
and the GGN update (20) becomes the NGN update (9). In this case, the GGN approxima-
tion (18) reduces exactly to the NGN approximation (12).
• Monomial. For a generic 𝛼-scaled monomial ℎ(𝑐) = 𝛼
𝑝𝑐𝑝we have ℎ′(𝑐) = 𝛼𝑐𝑝−1 and
ℎ′′(𝑐) = 𝛼(𝑝−1)𝑐𝑝−2. Therefore,
ℎ′′(𝑐)
ℎ′(𝑐)2 = 𝛼(𝑝−1)𝑐𝑝−2
𝛼2𝑐2𝑝−2
= 𝑝−1
𝛼𝑐𝑝=
1
𝑝
𝑝−1ℎ(𝑐)
=⇒
𝑞(𝑥) =
1
𝑝
𝑝−1 𝑓(𝑥) .
For 𝑝= 2, we get back to the NGN update.
13

• Negative logarithm. If ℎ(𝑦) = −log(𝑦), with 𝑦∈(0, 1) being the model likelihood, then
ℎ′(𝑦) = −1/𝑦and ℎ′′(𝑦) = 1/𝑦2, therefore 𝑞(𝑥) = 1 and we have
𝑥𝑘+1 = 𝑥𝑘−
𝜎
1 + 𝜎∥∇𝑓(𝑥)∥2 ∇𝑓(𝑥).
This can be considered as the scalar case of the diagonal Levenberg-Marquardt method when
using the log-likelihood loss (Le Cun et al., 1998, §9.1).
In our empirical study, the choice ℎ(𝑐) = 𝑐2 works best among the monomials with different
exponent 𝑝and also performs slightly better than the negative logarithm. In the rest of this paper,
we focus on the quadratic ℎfor convergence analysis and presenting the experiment results.
4
Stochastic setting: convergence rates and experiments
In this section, we study the application of NGN to stochastic optimization.
Specifically, we
consider the loss function in (1) and assume that each 𝑓𝑖: R𝑑→R is non-negative and 𝐿𝑖-smooth.
In this section, we let 𝐿= max𝑖∈[𝑁] 𝐿𝑖instead of the average of 𝐿𝑖as in the deterministic setting;
alternatively, we can simply assume that each 𝑓𝑖has the same smoothness constant 𝐿. We consider
the generic SGD method (2) where the stepsize 𝛾𝑘is obtained by applying the NGN method to the
current, randomly picked function 𝑓𝑖𝑘, i.e.,
𝛾𝑘=
𝜎
1 +
𝜎
2 𝑓𝑖𝑘(𝑥𝑘) ∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 .
(21)
All of our results are presented in the stochastic setting, and the convergence rates for the determin-
istic setting can be recovered by simply taking 𝑁= 1.
4.1
Preliminary lemmas
We present here a few fundamental lemmas, heavily used both in the constant 𝜎setting (Section 4.2)
and in the decreasing 𝜎case (Section 4.3).
Lemma 4.1 (Fundamental Equality). The NGN stepsize 𝛾𝑘in (21) satisfies
𝛾𝑘∥∇𝑓𝑖𝑘(𝑥)∥2 = 2
𝜎−𝛾𝑘
𝜎

𝑓𝑖𝑘(𝑥).
Proof. The definition of our stepsize implies

1 +
𝜎
2 𝑓𝑖𝑘(𝑥) ∥∇𝑓𝑖𝑘(𝑥)∥2

𝛾𝑘= 𝜎,
which one can rewrite as
𝜎
2 𝑓𝑖𝑘(𝑥) ∥∇𝑓𝑖𝑘(𝑥)∥2𝛾𝑘= 𝜎−𝛾𝑘.
This proves the result.
□
14

Lemma 4.2 (Fundamental Inequality). Let each 𝑓𝑖: R𝑑→R be non-negative, 𝐿-smooth and
convex. Considering the NGN stepsize 𝛾𝑘as in (21), we have
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

· 𝑓∗
𝑖𝑘.
(22)
For 𝜎≤1/(2𝐿) or 𝑓∗
𝑖𝑘= 0 we have no error resulting from the last term above.
Proof. Lemma 2.1&4.1 imply the following relations:
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤2𝐿𝛾2
𝑘( 𝑓𝑖𝑘(𝑥) −𝑓∗
𝑖𝑘),
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 = 2𝛾𝑘
𝜎−𝛾𝑘
𝜎

𝑓𝑖𝑘(𝑥𝑘).
They further imply that for any 𝛿∈(−∞, 1], we have
(1 −𝛿)𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤2(1 −𝛿)𝐿𝛾2
𝑘( 𝑓𝑖𝑘(𝑥) −𝑓∗
𝑖𝑘),
𝛿𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 = 2𝛿𝛾𝑘
𝜎−𝛾𝑘
𝜎

𝑓𝑖𝑘(𝑥𝑘).
Summing the two inequalities above, we get
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤2𝛿𝛾𝑘
𝜎−𝛾𝑘
𝜎

𝑓𝑖𝑘(𝑥𝑘) + 2(1 −𝛿)𝐿𝛾2
𝑘( 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘),
and after collecting a few terms,
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤2𝛾𝑘
h
𝛿
𝜎−𝛾𝑘
𝜎

+ (1 −𝛿)𝐿𝛾𝑘
i
|                              {z                              }
𝐴(𝛿)
[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝛾𝑘𝛿
𝜎−𝛾𝑘
𝜎

|             {z             }
𝐵(𝛿)
𝑓∗
𝑖𝑘.
(23)
We would like to choose 𝛿to somehow minimizes 𝐴(𝛿) and 𝐵(𝛿) simultaneously. It turns out that
a convenient choice is (see the remark after the proof)
𝛿= 2𝜎𝐿−1
2𝜎𝐿+ 1 .
(24)
Note that since 2𝜎𝐿> 0, 𝛿is a real number in the range [−1, 1] (which is a valid subset of [−∞, 1]).
Now consider the first term 𝐴(𝛿) in (23). We have
𝐴(𝛿) = 2𝜎𝐿−1
2𝜎𝐿+ 1

1 −𝛾𝑘
𝜎

+
2𝐿𝛾𝑘
2𝜎𝐿+ 1
= 2𝜎𝐿−1
2𝜎𝐿+ 1 −2𝜎𝐿−1
2𝜎𝐿+ 1
𝛾𝑘
𝜎+
2𝐿𝛾𝑘
2𝜎𝐿+ 1
= 2𝜎𝐿−1
2𝜎𝐿+ 1 +
𝛾𝑘
(2𝜎𝐿+ 1)𝜎
≤
2𝜎𝐿
2𝜎𝐿+ 1,
15

where in the last line we used the property 𝛾𝑘≤𝜎. The bound (23) becomes:
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝛾𝑘
2𝜎𝐿−1
2𝜎𝐿+ 1
 𝜎−𝛾𝑘
𝜎

𝑓∗
𝑖𝑘.
To bound 𝐵(𝛿), i.e., the second term on the right side of the above inequality, we have two cases:
• If 𝜎≤1/(2𝐿) then 𝛿= 2𝜎𝐿−1
2𝜎𝐿+1 is negatkve, and we have
2𝛾𝑘
2𝜎𝐿−1
2𝜎𝐿+ 1
 𝜎−𝛾𝑘
𝜎

𝑓∗
𝑖𝑘≤0,
(25)
since the worst case is 𝛾𝑘= 𝜎.
• Otherwise, 𝛿> 0 and we can proceed as follows, using the fact that 𝛾𝑘∈

𝜎
1+𝜎𝐿, 𝜎

2𝛾𝑘
2𝜎𝐿−1
2𝜎𝐿+ 1
 𝜎−𝛾𝑘
𝜎

𝑓∗
𝑖𝑘≤2𝜎
2𝜎𝐿−1
2𝜎𝐿+ 1
 𝜎−
𝜎
1+𝜎𝐿
𝜎

𝑓∗
𝑖𝑘
= 2𝜎
2𝜎𝐿−1
2𝜎𝐿+ 1
 
𝜎𝐿
1 + 𝜎𝐿

𝑓∗
𝑖𝑘
= 2𝜎2𝐿
1 + 𝜎𝐿
2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘,
All in all, considering both cases, we get the following upper bound for the error term 𝐵(𝛿):
2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

· 𝑓∗
𝑖𝑘.
(26)
This concludes the proof.
□
Remark (On the choice of 𝛿in (24)). In the context of the proof of Lemma 4.2, let us assume we
want to find 𝛿such that 𝐴(𝛿) ≤𝛼. That implies
𝛿
𝜎−𝛾𝑘
𝜎

+ (1 −𝛿)𝐿𝛾𝑘≤𝛼
⇐⇒𝛿𝜎+ [(1 −𝛿)𝐿𝜎−𝛿] 𝛾𝑘≤𝛼𝜎
⇐⇒[(1 −𝛿)𝐿𝜎−𝛿] 𝛾𝑘≤(𝛼−𝛿)𝜎
⇐⇒[(1 −𝛿)𝐿𝜎−𝛿] ≤(𝛼−𝛿) 𝜎
𝛾𝑘
.
For 𝛼≥𝛿, the right-hand side is positive. Further, note that 𝜎
𝛾𝑘∈[1, 1 + 𝜎𝐿]. So if we like the
inequality to hold for every value of 𝛾we need (worst case analysis)
[(1 −𝛿)𝐿𝜎−𝛿] ≤(𝛼−𝛿) ⇐⇒(1 −𝛿)𝐿𝜎≤𝛼⇐⇒𝛿≥1 −𝛼
𝐿𝜎.
16

Since 𝛼≥𝛿we also need
𝛼≥1 −𝛼
𝐿𝜎⇐⇒𝐿𝜎𝛼≥𝐿𝜎−𝛼⇐⇒𝛼≥
𝜎𝐿
1 + 𝜎𝐿= 1 −
1
1 + 𝜎𝐿.
The bound becomes
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤2𝛼𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝛾𝑘𝛿
𝜎−𝛾𝑘
𝜎

|             {z             }
𝐵(𝛿)
𝑓∗
𝑖𝑘.
For the sake of minimizing the first term in the bound, it would make sense to use 𝛼=
𝜎𝐿
1+𝜎𝐿.
However, under this condition we get that 𝛿≥
𝜎𝐿
1+𝜎𝐿as well. This is not ideal since we want 𝐵(𝛿),
the error factor, to vanish for small 𝛾. To do this, we need to slightly increase the value of 𝛼to
allow 𝛿to become negative for small values of 𝑦= 𝐿𝜎. Note that for 𝛿to be (potentially) negative
we need
1 −𝛼
𝜎𝐿≤0 ⇐⇒𝛼≥𝐿𝜎.
Hence, if we want to keep 𝛼< 1 (needed for proofs), we can only have this condition for 𝜎≤1/𝐿
– i.e. the case where we know convergence holds. To this end, let us consider
1 > 𝛼=
2𝜎𝐿
1 + 2𝜎𝐿=
𝜎𝐿
1/2 + 𝜎𝐿≥
𝜎𝐿
1 + 𝜎𝐿.
Using this 𝛼, we get
𝛿≥1 −𝛼
𝐿𝜎= 1 −
2
1 + 2𝜎𝐿= 2𝜎𝐿−1
2𝜎𝐿+ 1 .
Note that if 𝜎≤
1
2𝐿then the minimum allowed 𝛿is negative. We pick for 𝛿its minimum allowed
value.
4.2
Convergence rates for fixed regularization
We provide here convergence guarantees for stochastic NGN method in the convex, strongly
convex, and non-convex setting. The results justify the performance observed in practice so far,
complemented by deep learning experiments in Section 4.4.
Our results are better presented with the concept of interpolation.
For a finite sum loss
𝑓= P𝑁
𝑖=1 𝑓𝑖, interpolation implies that all 𝑓𝑖s can be minimized at the solution.
Definition 4.3 (Interpolation). The optimization problem min𝑥∈R𝑑𝑓(𝑥) = P𝑁
𝑖=1 𝑓𝑖(𝑥) satisfies in-
terpolation if there exist a 𝑥∗such that 𝑥∗∈arg min𝑥∈R𝑑𝑓𝑖(𝑥) for all 𝑖∈[𝑁].
Interpolation is an assumption that is often used in the deep learning literature, as it is associated
with overparametrization of some modern neural networks, leading to fast convergence of gradient
methods (Ma et al., 2018). We do not assume interpolation4, but follow Loizou et al. (2021) to
provide convergence rates in relation to the following two error quantities:
Δint := E[ 𝑓𝑖(𝑥∗) −𝑓∗
𝑖],
Δpos := E[ 𝑓∗
𝑖],
(27)
4For instance, large language models do not satisfy this property (Hoffmann et al., 2022).
17

where 𝑥∗∈arg min 𝑓(𝑥) and 𝑓∗
𝑖:= inf𝑥𝑓𝑖(𝑥). Here the expectation E[·] is taken with respect
to the uniform distribution of the index 𝑖, equivalent to the average (1/𝑁) P𝑁
𝑖=1[·].
We call
Δint the interpolation gap, which is apparently zero under interpolation. The position gap Δpos
measures how close is, on average, the minimizer for the current batch to the value zero. While for
overparametrized deep learning models one has Δpos = Δint = 0 (Vaswani et al., 2020), both the
cases Δpos = 0, Δint > 0 and Δpos > 0, Δint = 0 are feasible in theory.
Remark. Since in our setting each 𝑓𝑖is lower-bounded, under the very realistic assumptions 𝑓∗< ∞
and 𝑓∗
𝑖< ∞for all 𝑖∈[𝑁], both Δpos and Δint are finite. This assumption, in particular, does not
imply finite gradient noise variance.
4.2.1
Convex and strongly convex settings
We first recall the definition in the differentiable setting.
Definition 4.4 (Strong Convexity / Convexity). A differentiable function 𝑓: R𝑑→R, is 𝜇-strongly
convex, if there exists a constant 𝜇> 0 such that ∀𝑥, 𝑦∈R𝑑:
𝑓(𝑥) ≥𝑓(𝑦) + ⟨∇𝑓(𝑦), 𝑥−𝑦⟩+ 𝜇
2 ∥𝑥−𝑦∥2
(28)
for all 𝑥∈R𝑑. If the inequality holds with 𝜇= 0 the function 𝑓is convex.
We now present our results in the convex and strongly convex settings. The nonconvex case,
for which we have weaker results, is presented in Section 4.2.3.
Theorem 4.5 (NGN, convex). Let 𝑓=
1
𝑁
P𝑁
𝑖=1 𝑓𝑖, where ealh 𝑓𝑖: R𝑑→R is non-negative, 𝐿-
smooth and convex. Consider the SGD method (2) with the NGN stepsize (21). For any value of
𝜎> 0, we have
E

𝑓( ¯𝑥𝐾) −𝑓(𝑥∗)

≤E∥𝑥0 −𝑥∗∥2
𝜂𝜎𝐾
+ 3𝜎𝐿· (1 + 𝜎𝐿)Δint + 𝜎𝐿· max {0, 2𝜎𝐿−1} Δpos,
(29)
where ¯𝑥𝐾= 1
𝐾
P𝐾−1
𝑘=0 𝑥𝑘and 𝜂𝜎=
2𝜎
(1+2𝜎𝐿)2. Decreasing 𝜎like 𝑂(1/
√
𝐾), we get an 𝑂

ln(𝐾)
√
𝐾

rate.
We note that the NGN stepsize (21) does not require knowledge of the Lipschitz constant 𝐿.
Theorem 4.6 (NGN, strongly convex). Let 𝑓= 1
𝑁
P𝑁
𝑖=1 𝑓𝑖, where each 𝑓𝑖: R𝑑→R is non-negative,
𝐿-smooth and convex. Additionally, assume 𝑓is 𝜇-strongly convex. Consider the SGD method (2)
with the NGN stepsize (21). For any value of 𝜎> 0, we have
E∥𝑥𝑘+1 −𝑥∗∥2 ≤(1 −𝜇𝜌)𝑘E∥𝑥0 −𝑥∗∥2 + 6𝐿
𝜇𝜎(1 + 𝜎𝐿)Δint + 2𝜎𝐿
𝜇
max {0, 2𝜎𝐿−1} Δpos,
where 𝜌=
𝜎
(1+2𝜎𝐿)(1+𝜎𝐿). Decreasing 𝜎like 𝑂(1/𝐾), we get an 𝑂

ln(𝐾)
𝐾

rate.
18

The proofs of the main results in Thm. 4.5 and Thm. 4.6 are presented in Section 4.2.2 and the
analysis on decreasing 𝜎is given in Section 4.3. Here, we make a few comments on these results.
Technical novelty.
Our analysis involves a novel expansion for the expectation of the product of the
stochastic NGN stepsize 𝛾𝑘with the stochastic suboptimality value 𝑓𝑖𝑘(𝑥𝑘)−𝑓𝑖𝑘(𝑥∗). The correlation
between these quantities poses well-known challenges in the analysis of adaptive methods (see e.g.
discussion in Ward et al. (2020)). Our approach involves decomposing the stepsize as 𝛾𝑘= 𝜌+ 𝜖𝑘,
where 𝜌= 𝑂(𝜎) is deterministic and 𝜖𝑘= 𝑂(𝜎2) is a non-negative stochastic offset.
This
decomposition allows us to fully benefit from NGN adaptivity in the proof without compromising
the bound tightness and the suboptimality level reached at convergence.
Comment on the rate.
For any fixed 𝜎, the rate is sublinear (linear in the strongly convex case)
to a bounded neighborhood of the solution. The neighborhood magnitude is 𝑂(𝜎) – shrinks as 𝜎
converges to zero – and depends on the values of Δint and Δpos. Let us elaborate further:
• If Δint = Δpos = 0, then the results guarantees sublinear (linear in the strongly convex case)
convergence in expectation to the solution for any value of 𝜎. The best constant in the rate
in this setting is achieved when knowing the Lipschitz constant, indeed min𝜎𝜂𝜎is achieved
at 𝜎= 1/(2𝐿).
• If Δpos > 0, then the error term 𝜎𝐿· max {0, 2𝜎𝐿−1} Δpos (in the convex setting) is non-
vanishing5 for 𝜎> 1/(2𝐿) – this does not come from gradient stochasticity but is instead an
effect of correcting divergence of the gradient update for large stepsizes. We note that in this
case our method behaves more favorably than SGD, which is divergent for large stepsizes.
• If Δint > 0, then the error term 3𝜎𝐿(1 + 𝜎𝐿)Δint (in the convex setting) is the result of
gradient stochasticity around the solution.
• By annealing 𝜎—which does not necessarily yield a monotonically decreasing stepsize
𝛾𝑘(see Imagenet experiment in Figure 7) – we get convergence to the solution without
knowledge of the gradient Lipshitz constant 𝐿(Thm. 4.12).
Comparison with SPS.
The errors in Thm. 4.5 and Thm. 4.6 are 𝑂(𝜎), meaning that as 𝜎→0
the error vanishes and we converge to the solution. This is not the case for recent adaptations of the
Polyak scheme to the stochastic setting (Loizou et al., 2021), where the error is 𝑂(1) (Thm. 4.7).
Theorem 4.7 (Main Theorem of Loizou et al. (2021)). Let each 𝑓𝑖be 𝐿𝑖-smooth and convex.
Denote 𝐿= max{𝐿𝑖}𝑛
𝑖=1 the maximum smoothness constant.
Consider SGD with the SPSmax
learning rate 𝛾𝑘= min

𝑓𝑖𝑘(𝑥𝑘)−𝑓∗
𝑖𝑘
𝑐∥∇𝑓𝑖𝑘(𝑥𝑘)∥2, 𝛾𝑏

. If 𝑐= 1, E

𝑓( ¯𝑥𝐾) −𝑓(𝑥∗)

≤∥𝑥0−𝑥∗∥2
𝛼𝐾
+ 2𝛾𝑏Δint
𝛼
, where
𝛼= min
 1
2𝑐𝐿, 𝛾𝑏
	
and ¯𝑥𝐾=
1
𝐾
P𝐾−1
𝑘=0 𝑥𝑘. If in addition 𝑓is 𝜇-strongly convex, then, for any
𝑐≥1/2, SGD with SPSmax converges as: E∥𝑥𝑘−𝑥∗∥2 ≤(1 −𝜇𝛼)𝑘∥𝑥0 −𝑥∗∥2 + 2𝛾𝑏Δint
𝜇𝛼.
5The threshold 1/(2𝐿) is also common in plain SGD, see, e.g., Garrigos and Gower (2023, Theorem 6.8).
19

It is easy to realize that indeed, as thoroughly explained by Orvieto et al. (2022c), the error
terms 2𝛾𝑏Δint
𝛼
and 2𝛾𝑏Δint
𝜇𝛼
in the convex and strongly convex setting are 𝑂(1) with respect to the
hyperparameters. This is referred to as the bias problem in Section 4 of Orvieto et al. (2022c).
While Orvieto et al. (2022b) presented a variation on SPS (DecSPS) to solve this issue, their proof
is based on a classical Adagrad-like argument and additionally needs strong convexity or bounded
iterates to guarantee sublinear convergence in the convex domain. Instead, Thm. 4.5 requires no
assumptions other than convexity and smoothness. Further, in Thm. 4.6, we are able to prove linear
convergence to a 𝑂(𝜎) neighborhood, a result not yet derived with DecSPS machinery.
σk = 100/k
σk = 10/k
σk = 1/k
γk = 1/k
γk = 0.1/k
γk = 10/k
η = 0.1
η = 1
η = 10
Average over 10 runs*
Average over 10 runs*
Figure 4: Comparison of NGN with Adagrad-norm and SGD for two convex problems, under a
decreasing stepsize and a constant stepsize. A comment is provided below.
Comparison with Adagrad.
Adagrad enjoys state-of-the-art adaptive convergence guarantees in
the convex and non-convex settings (Liu et al., 2023; Faw et al., 2022; Wang et al., 2023a; Faw
et al., 2023). Knowledge of the gradient Lipschitz constant or additional strong assumptions are
not needed for convergence to the problem solution. However, as opposed to NGN, Adagrad’s
proof techniques do not transfer well to the strongly convex setting (see instead our Thm. 4.6).
In addition, Adagrad is observed to perform suboptimally compared to well-tuned SGD on some
problems (Kingma and Ba, 2014). This is mainly due to the fact that the resulting 𝛾𝑘sequence is
forced to be decreasing by design. A feature that is not present in more commonly used optimizers
such as Adam (Kingma and Ba, 2014).
We evaluate the performance of tuned SGD, NGN, and Adagrad-norm in two distinct settings:
• Linear regression on Gaussian input data with random predictor (𝑑= 512 features, 𝑁= 2048
datapoints: condition number is ∼8.5), optimized with a relatively big batch size of 64.
20

Here, Δpos = 0 but Δint > 0, and we choose decreasing stepsizes for SGD and NGN like 1/𝑘,
to ensure convergence to the unique solution (see Thm. 4.6). In most recent results (see, e.g.,
Faw et al., 2022), Adagrad is not analyzed with a decreasing stepsize 𝜂, so we also do not
implement it here.
• The second problem is cross-entropy minimization for a linear model on the Olivetti Faces
dataset (Pedregosa et al., 2011), where 𝑑= 4096, 𝑁= 320 and we have 40 distinct classes.
Since here 𝑁≪𝑑, we include L2 regularization with strength 1𝑒−3.
We optimize
with a batch-size of 4.
Realistically, Δpos, Δint ≪1.
We, therefore, consider constant
hyperparameters in all our optimizers.
We report our results in Fig. 4. For SGD and NGN, we tuned their single hyperparameter in-
dependently to the best performance. For Adagrad-norm, 𝛾𝑘= 𝜂/
√︃
𝑏2
0 + P𝑘
𝑗=0 ∥∇𝑓𝑖𝑗(𝑥𝑗)∥2 and
we only carefully tuned 𝜂since results are not too sensitive to 𝑏0, which we set to 1𝑒−2. The
best-performing hyperparameter for each method is marked with
. We also show performance
for hyperparameters 3-times-lower ( ) and 3-times-higher ( ). The results clearly indicate, in both
settings, that NGN has an edge over Adagrad-norm:
• On Olivetti Faces, where we optimize with constant hyperparameters, we indeed see that
Adagrad, even after tuning, does not provide a clear advantage over SGD. Instead, for NGN,
performance is remarkably better than SGD and Adagrad for all hyperparameter values. The
strong gradients experienced at the first iteration cause the NGN stepsize to automatically
warmup, reaching then a 𝜎-proportional value when gradients become small.
• On the linear regression example, the behavior is particularly interesting: NGN prevents the
learning rate from overshooting – unlocking fast progress even at large 𝜎. Instead, SGD for
a big enough learning rate overshoots and only catches up at the end of training. As expected
by the result of Yang et al. (2023), Adagrad does not overshoot, but its progress slows down as
soon as gradients start to vanish (therefore 𝛾𝑘hardly decreases). Towards the end of training,
NGN optimizes with a stepsize 𝛾𝑘≃𝜎𝑘, effectively converging to the SGD rule.
Remark (NGN can automatically warmup, but looks like decrease schedule is hand-picked.). While
this is indeed the case for the examples seen so far, this property depends on the features of the loss
landscape. We show that in deep learning experiments (Fig. 7) for a constant 𝜎the induced 𝛾𝑘has
a warmup-decay behaviour.
4.2.2
Proofs for main results
We present here the proofs for the convex and strongly convex setting.
Proof of Thm. 4.5. By assumption, each 𝑓𝑖is convex and 𝐿-smooth.
Expanding the squared
21

distance and using convexity, we get:
∥𝑥𝑘+1 −𝑥∗∥2 = ∥(𝑥𝑘+1 −𝑥𝑘) + (𝑥𝑘−𝑥∗)∥2
= ∥𝑥𝑘−𝑥∗∥2 + 2⟨𝑥𝑘−𝑥∗, 𝑥𝑘+1 −𝑥𝑘⟩+ ∥𝑥𝑘+1 −𝑥𝑘∥2
= ∥𝑥𝑘−𝑥∗∥2 −2𝛾𝑘⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩+ 𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2
≤∥𝑥𝑘−𝑥∗∥2 −2𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] + 𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2.
(30)
We now make use of Lemma 4.2:
∥𝑥𝑘+1 −𝑥∗∥2 ≤∥𝑥𝑘−𝑥∗∥2 −2𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)]
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘.
(31)
Note that taking the expectation conditional on 𝑥𝑘at this point (as done in the classical SGD proof)
is not feasible: indeed, the variable 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗), which would have expectation 𝑓(𝑥𝑘) −𝑓(𝑥∗),
is correlated with 𝛾𝑘– meaning that we would need to consider the expectation of the product
𝛾𝑘( 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)).
The analysis of Loizou et al. (2021) in the context of SPS consists in writing 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗) =
[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] −[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘], where both terms within brackets are positive and therefore one
can use the stepsize bounds before taking the expectation. This is a smart approach for a quick
computation; however it introduces a bias term E[𝛾𝑘( 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘)] ≤𝜎[ 𝑓(𝑥∗) −E𝑖𝑓∗
𝑖] = 𝑂(𝜎).
It is more desirable, if the method allows, to have error terms only of the order 𝑂(𝜎2), so that one
can guarantee later in the proof that, as 𝜎→0, the method converges to the problem solution.
To this end, we write 𝛾𝑘= 𝜌+ 𝜖𝑘, where both 𝜌and 𝜖𝑘are non-negative and 𝜌is deterministic.
For intuition, the reader can think of 𝜌as a stepsize lower bound such that ideally 𝜌= 𝑂(𝜎) and
𝜖𝑘= 𝑂(𝜎2) for all realizations of 𝛾𝑘—we make it more precise in the next lines:
∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2 ≤−2𝜌[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] −2𝜖𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)]
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 𝑂(𝜎2) 𝑓∗
𝑖𝑘,
where we wrote the last term in (31) simply as 𝑂(𝜎2) for brevity but will plug in the correct
expression at the end of the proof.
At this point, we write 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗) = [ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] −[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] only for the second
term in the bound above. Our purpose, is to make the resulting term −2𝜖𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] (negative)
dominant compared to the third term in the bound above (positive). In formulas, the bound on the
distance update ∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2 becomes
−2𝜌[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] −2

𝜖𝑘−2𝜎𝐿𝛾𝑘
1 + 2𝜎𝐿

[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜖𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑂(𝜎2) 𝑓∗
𝑖𝑘,
and we require 𝜖𝑘−
2𝜎𝐿
1+2𝜎𝐿𝛾𝑘≥0. Note that 𝜖𝑘= 𝛾𝑘−𝜌. Therefore, we need

1 −
2𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘≥𝜌
=⇒𝛾𝑘≥(1 + 2𝜎𝐿)𝜌.
22

Since 𝛾𝑘≥
𝜎
1+𝜎𝐿thanks to Lemma 2.1, we have that a sufficient condition is
𝜌≤
𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) .
Let us then pick 𝜌equal to this upper bound. Our bound on the distance update simplifies as
−
2𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) [ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] + 2𝜖𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑂(𝜎2) 𝑓∗
𝑖𝑘.
We now get to the interesting part: what is the order of 𝜖𝑘under our choice for 𝜌?
𝜖𝑘= 𝛾𝑘−𝜌≤𝜎−
𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) = 𝜎(1 + 2𝜎𝐿)(1 + 𝜎𝐿) −𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿)
.
Simplifying the bound, we get the desired result: 𝜖𝑘= 𝑂(𝜎2). Indeed,
𝜖𝑘≤
3𝐿𝜎2 + 2𝐿2𝜎3
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) = 𝐿𝜎2
3 + 2𝐿𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) ≤
3𝐿𝜎2
1 + 2𝜎𝐿.
(32)
All in all, our bound becomes
∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2 ≤−𝑇0(𝜎)[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] +𝑇1(𝜎2)[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] +𝑇2(𝜎2) 𝑓∗
𝑖𝑘, (33)
where
𝑇0(𝜎) =
2𝜎
(1 + 2𝜎𝐿)(1 + 𝜎𝐿) ,
(34)
𝑇1(𝜎2) =
6𝐿𝜎2
1 + 2𝜎𝐿,
(35)
𝑇2(𝜎2) = 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

.
(36)
We are finally ready to take the conditional expectation with respect to 𝑘:
E𝑘∥𝑥𝑘+1 −𝑥∗∥2 ≤∥𝑥𝑘−𝑥∗∥2 −𝑇0(𝜎)[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)]
+ 𝑇1(𝜎2)E𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑇2(𝜎2)E𝑘[ 𝑓∗
𝑖𝑘].
Let us call 𝐸𝑘the expected error at step 𝑘:
𝐸𝑘(𝜎2) := 𝑇1(𝜎2)E𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑇2(𝜎2)E𝑘[ 𝑓∗
𝑖𝑘]
≤𝑇1(𝜎2)Δint + 𝑇2(𝜎2)Δpos,
(37)
where Δint := E𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] and Δpos := E𝑘[ 𝑓∗
𝑖𝑘]. Therefore we get the compact formula
E𝑘∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2 ≤−𝑇0(𝜎)[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] + 𝐸𝑘(𝜎2)
23

Taking the expectation again and using the tower property
E∥𝑥𝑘+1 −𝑥∗∥2 −E∥𝑥𝑘−𝑥∗∥2 ≤−𝑇0(𝜎)E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] + 𝐸𝑘(𝜎2)
Finally, averaging over iterations,
1
𝐾
𝐾−1
∑︁
𝑘=0
E∥𝑥𝑘+1 −𝑥∗∥2 −E∥𝑥𝑘−𝑥∗∥2 ≤−𝑇0(𝜎)
𝐾
𝐾−1
∑︁
𝑘=0
E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] + 𝐸𝑘(𝜎2).
Using linearity of expectation
1
𝐾E∥𝑥𝐾−𝑥∗∥2 −1
𝐾E∥𝑥0 −𝑥∗∥2 ≤−𝑇0(𝜎)E
"
𝐾−1
∑︁
𝑘=0
1
𝐾𝑓(𝑥𝑘) −𝑓(𝑥∗)
#
+ 𝐸𝑘(𝜎2).
and therefore
E
"
1
𝐾
𝐾−1
∑︁
𝑘=0
𝑓(𝑥𝑘) −𝑓(𝑥∗)
#
≤
1
𝑇0(𝜎)𝐾E∥𝑥0 −𝑥∗∥2 + 𝐸𝑘(𝜎2)
𝑇0(𝜎) .
Finally, by Jensen’s inequality,
E

𝑓( ¯𝑥𝑘) −𝑓(𝑥∗)

≤
1
𝑇0(𝜎)𝐾E∥𝑥0 −𝑥∗∥2 + 𝐸𝑘(𝜎2)
𝑇0(𝜎) ,
where ¯𝑥𝐾= 1
𝐾
P𝐾−1
𝑘=0 𝑥𝑘. Finally, using the bound on 𝐸𝑘(𝜎2) in (37) and definitions of 𝑇0, 𝑇1 and
𝑇2 in (34), (35) and (36), respectively, we arrive at
E

𝑓( ¯𝑥𝑘) −𝑓(𝑥∗)

≤(1 + 2𝜎𝐿)2
2𝜎𝐾
E∥𝑥0 −𝑥∗∥2
+ 3𝜎𝐿(1 + 𝜎𝐿)Δint + 𝜎𝐿· max {0, 2𝜎𝐿−1} Δpos.
This concludes the proof.
□
Proof of Theorem 4.6. We proceed very similarly with the convex setting
∥𝑥𝑘+1 −𝑥∗∥2 = ∥(𝑥𝑘+1 −𝑥𝑘) + (𝑥𝑘−𝑥∗)∥2
= ∥𝑥𝑘−𝑥∗∥2 + 2⟨𝑥𝑘−𝑥∗, 𝑥𝑘+1 −𝑥𝑘⟩+ ∥𝑥𝑘+1 −𝑥𝑘∥2
= ∥𝑥𝑘−𝑥∗∥2 −2𝛾𝑘⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩+ 𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2
Making use of Lemma 4.2:
∥𝑥𝑘+1 −𝑥∗∥2 ≤∥𝑥𝑘−𝑥∗∥2 −2𝛾𝑘⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘.
24

As in the proof for Thorem 4.5, we decompose 𝛾into a deterministic lower bound 𝜌and some
error 𝜖𝑘∈𝑂(𝜎2): 𝛾𝑘= 𝜌+ 𝜖𝑘. As in Thorem 4.5 we choose 𝜌=
𝜎
(1+2𝜎𝐿)(1+𝜎𝐿). This leads to (see
Equation (32))
𝜖𝑘= 𝛾𝑘−𝜌≤
3𝐿𝜎2
1 + 2𝜎𝐿.
Note that we have not yet used convexity. We use it now after decomposing 𝛾𝑘:
∥𝑥𝑘+1 −𝑥∗∥2
≤∥𝑥𝑘−𝑥∗∥2 −2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩−2𝜖𝑘⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘
≤∥𝑥𝑘−𝑥∗∥2 −2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩−2𝜖𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)]
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘.
Therefore,
∥𝑥𝑘+1 −𝑥∗∥2
≤∥𝑥𝑘−𝑥∗∥2 −2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩−2𝜖𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜖𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘]
+

4𝜎𝐿
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘
= ∥𝑥𝑘−𝑥∗∥2 −2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩+ 2𝜖𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘]
+ 2

𝜖𝑘−2𝜎𝐿𝛾𝑘
1 + 2𝜎𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘.
As we know from the proof of for the convex setting, 𝜖𝑘−2𝜎𝐿𝛾𝑘
1+2𝜎𝐿≥0 hence, since 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘> 0,
we can drop the term. using the upper bound for 𝜖𝑘, we get
∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2
≤−2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓𝑖𝑘(𝑥𝑘)⟩+
6𝐿𝜎2
1 + 2𝜎𝐿[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

𝑓∗
𝑖𝑘.
Now we take the expectation conditional on 𝑥𝑘, recalling our definition for the conditional
expectations: Δint := E𝑘[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] and Δpos := E𝑘[ 𝑓∗
𝑖𝑘].
E𝑘∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2
≤−2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓(𝑥𝑘)⟩+
6𝐿𝜎2
1 + 2𝜎𝐿Δint + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

Δpos.
25

It is now time to use 𝜇-strong convexity of 𝑓:
E𝑘∥𝑥𝑘+1 −𝑥∗∥2 ≤∥𝑥𝑘−𝑥∗∥2 −2𝜌⟨𝑥𝑘−𝑥∗, ∇𝑓(𝑥𝑘)⟩
+
6𝐿𝜎2
1 + 2𝜎𝐿Δint + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

Δpos
≤(1 −𝜇𝜌)∥𝑥𝑘−𝑥∗∥2 −2𝜌[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)]
+
6𝐿𝜎2
1 + 2𝜎𝐿Δint + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

Δpos.
Note that 𝑓(𝑥𝑘) −𝑓(𝑥∗) > 0, since 𝑥∗is the minimizer for 𝑓. Hence, we can drop this term.
E𝑘∥𝑥𝑘+1 −𝑥∗∥2 ≤(1 −𝜇𝜌)∥𝑥𝑘−𝑥∗∥2 +
6𝐿𝜎2
1 + 2𝜎𝐿Δint + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

Δpos.
Taking the expectation again and using the tower property,
E∥𝑥𝑘+1 −𝑥∗∥2 ≤(1 −𝜇𝜌)E∥𝑥𝑘−𝑥∗∥2 +
6𝐿𝜎2
1 + 2𝜎𝐿Δint + 2𝜎2𝐿
1 + 𝜎𝐿· max

0, 2𝜎𝐿−1
2𝜎𝐿+ 1

Δpos.
This recurrence is of the kind 𝑦𝑘+1 = 𝑎𝑦𝑘+ 𝑏, therefore 𝑦𝑘= 𝑎𝑘𝑦0 +
P𝑘−1
𝑖=0 𝑎𝑖
𝑏. Since 𝑏≥0,
we have 𝑦𝑘≤𝑎𝑘𝑦0 +
𝑏
1−𝑎. Therefore,
E∥𝑥𝑘+1 −𝑥∗∥2 ≤(1 −𝜇𝜌)𝑘E∥𝑥0 −𝑥∗∥2
+
6𝐿𝜎2
(1 + 2𝜎𝐿)𝜌𝜇Δint +
2𝜎2𝐿
(2𝜎𝐿+ 1)(1 + 𝜎𝐿)𝜌𝜇· max {0, 2𝜎𝐿−1} Δpos.
Now recall that 𝜌=
𝜎
(1+2𝜎𝐿)(1+𝜎𝐿). We can then simplify the error term:
E∥𝑥𝑘+1 −𝑥∗∥2 ≤(1 −𝜇𝜌)𝑘E∥𝑥0 −𝑥∗∥2
+ 6𝐿𝜎
𝜇
(1 + 𝜎𝐿)Δint + 2𝜎𝐿
𝜇
max {0, 2𝜎𝐿−1} Δpos,
which is the desired result.
□
4.2.3
General non-convex setting
Stochastic NGN can also be applied successfully in the nonconvex setting, as we will see from the
deep learning experiments in Section 4.4. For convergence analysis in this setting, knowledge of
the Lipschitz constant is required and we need to define an additional quantity
Δ2
noise = sup
𝑥∈R𝑑E[∥∇𝑓(𝑥) −∇𝑓𝑖(𝑥)∥2],
which is the usual bound on stochastic gradient variance. The convergence rate provided here is
non-adaptive (with known 𝐿), but we are confident that it can be improved in the future.
26

Theorem 4.8 (NGN, nonconvex). Let 𝑓= 1
𝑁
P𝑁
𝑖=1 𝑓𝑖, where each 𝑓𝑖: R𝑑→R is non-negative,
𝐿-smooth and potentially non-convex. Consider the SGD method (2) with the stochastic NGN
stepsize (21). Then for any 𝜎≤
1
2𝐿, we have
E
"
1
𝐾
𝐾−1
∑︁
𝑘=0
∥∇𝑓(𝑥𝑘)∥2
#
≤12 · E[ 𝑓(𝑥0) −𝑓∗]
𝜎𝐾
+ 18𝜎𝐿Δ2
noise,
(38)
Decreasing 𝜎as 𝑂(1/
√
𝐾), we get a convergence rate of 𝑂

ln(𝐾)
√
𝐾

.
Proof. The proof deviates significantly from the one for stochastic Polyak stepsizes by Loizou et al.
(2021). We start with the classic expansion based on gradient smoothness
𝑓(𝑥𝑘+1) −𝑓(𝑥𝑘) ≤⟨∇𝑓(𝑥𝑘), 𝑥𝑘+1 −𝑥𝑘⟩+ 𝐿
2 ∥𝑥𝑘+1 −𝑥𝑘∥2
= −𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩+
𝐿𝛾2
𝑘
2 ∥∇𝑓𝑖𝑘(𝑥𝑘)∥2
≤−𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩+ 𝐿𝜎2
2 ∥∇𝑓𝑖𝑘(𝑥𝑘)∥2.
We would like to take the conditional expectation with respect to 𝑥𝑘. Yet, this is not easy since 𝛾𝑘
and ∇𝑓𝑖𝑘(𝑥𝑘) are correlated. Note however that we can write, given the bound in Lemma 4.1,
𝛾𝑘=
𝜎
𝜎𝐿+ 1 +
𝜎2𝐿
𝜎𝐿+ 1𝜉𝑖𝑘,
(39)
where 𝜉𝑖𝑘∈[0, 1] is a random variable. When 𝜉𝑖𝑘= 1 we have 𝛾𝑘= 𝜎, and when 𝜉𝑖𝑘= 0 it holds
that 𝛾𝑘=
𝜎
𝜎𝐿+1. Thil model for 𝛾𝑘covers its complete range and makes one property explicit :
𝛾𝑘= 𝑂(𝜎) with variation range 𝑂(𝜎2). As such, as 𝜎→0 the stepsize becomes deterministic,
and the update reduces to SGD with constant stepsize. Leveraging this representation of 𝛾𝑘, we
can write
−𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘) = −
𝜎
𝜎𝐿+ 1⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩−
𝜎2𝐿
𝜎𝐿+ 1𝜉𝑖𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩
≤−
𝜎
𝜎𝐿+ 1⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩+
𝜎2𝐿
𝜎𝐿+ 1
𝜉𝑖𝑘
 ·
⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩

≤−
𝜎
𝜎𝐿+ 1⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩+
𝜎2𝐿
𝜎𝐿+ 1
⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩
 .
Therefore
−E𝑘[𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩] ≤−
𝜎
𝜎𝐿+ 1 ∥∇𝑓(𝑥𝑘)∥2 +
𝜎2𝐿
𝜎𝐿+ 1E𝑘
⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩

(40)
The first term in the above bound is 𝑂(𝜎) lnd directly helps convergence, while the last term is an
error of 𝑂(𝜎2). Next, recall the basic inequality: for any 𝑎, 𝑏∈R𝑑:
|⟨𝑎, 𝑏⟩| ≤1
2 ∥𝑎∥2 + 1
2 ∥𝑏∥2 + 1
2 ∥𝑎−𝑏∥2.
27

Applying to the last term in (40) and using the assumption 𝜁2 = sup𝑥∈R𝑑E𝑖∥∇𝑓(𝑥)−∇𝑓𝑖(𝑥)∥2 < ∞,
we have
2E𝑘
⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩
 ≤∥∇𝑓(𝑥𝑘)∥2 + E𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 + E𝑘∥∇𝑓(𝑥𝑘) −∇𝑓𝑖𝑘(𝑥𝑘)∥2
≤∥∇𝑓(𝑥𝑘)∥2 + E𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 + 𝜁2
≤2∥∇𝑓(𝑥𝑘)∥2 + 2𝜁2.
Therefore, we get the compact inequality
−E𝑘[𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩] ≤−
𝜎
𝜎𝐿+ 1∥∇𝑓(𝑥𝑘)∥2 +
𝜎2𝐿
𝜎𝐿+ 1

∥∇𝑓(𝑥𝑘)∥2 + 𝜁2
≤−𝜎
1 −𝜎𝐿
1 + 𝜎𝐿

∥∇𝑓(𝑥𝑘)∥2 +
𝜎2𝐿
𝜎𝐿+ 1𝜁2,
which we can insert back in the original expansion to get
E𝑘[ 𝑓(𝑥𝑘+1)] −𝑓(𝑥𝑘) ≤−E𝑘[𝛾𝑘⟨∇𝑓(𝑥𝑘), ∇𝑓𝑖𝑘(𝑥𝑘)⟩] + 𝐿𝜎2
2 E𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2
≤

−𝜎
1 −𝜎𝐿
1 + 𝜎𝐿

+ 𝐿𝜎2
2

∥∇𝑓(𝑥𝑘)∥2 +
 𝜎2𝐿
𝜎𝐿+ 1 + 𝜎2𝐿
2

𝜁2.
We therefore need
−𝜎
1 −𝜎𝐿
1 + 𝜎𝐿

+ 𝐿𝜎2
2
= −𝜎
1 −𝜎𝐿
1 + 𝜎𝐿−𝜎𝐿
2

≤0.
The function 1−𝜎𝐿
1+𝜎𝐿−𝜎𝐿
2 is monotonically decreasing as 𝜎𝐿> 0 increases. For 𝜎𝐿= 0 it is 1 and
reaches value zero at −3/2+
√
17/2 ≊0.56. For 𝜎𝐿= 0.5, one gets 1−𝜎𝐿
1+𝜎𝐿−𝜎𝐿
2 = 0.5
1.5 −0.25 = 1/12.
Therefore, for 𝜎≤
1
2𝐿, we get −𝜎

1−𝜎𝐿
1+𝜎𝐿

+ 𝐿𝜎2
2
≤−𝜎
12.
Next, for the noise term, note that
𝜎2𝐿
𝜎𝐿+1 + 𝜎2𝐿
2
≤3𝜎2𝐿
2 . All in all, for 𝜎≤
1
2𝐿, we get:
E𝑘[ 𝑓(𝑥𝑘+1)] −𝑓(𝑥𝑘) ≤−𝜎
12∥∇𝑓(𝑥𝑘)∥2 + 3𝜎2𝐿
2
𝜁2,
Or, more conveniently:
∥∇𝑓(𝑥𝑘)∥2 ≤−12
𝜎[E𝑘[ 𝑓(𝑥𝑘+1)] −𝑓(𝑥𝑘)] + 18𝜎𝐿𝜁2.
After taking the expectation using the tower property, we get
E∥∇𝑓(𝑥𝑘)∥2 ≤−12
𝜎[E[ 𝑓(𝑥𝑘+1)] −E[ 𝑓(𝑥𝑘)]] + 18𝜎𝐿𝜁2,
28

Summing over iterations and telescoping the sum, after adding and subtracting 𝑓∗we get
1
𝐾
𝐾−1
∑︁
𝑘=0
E∥∇𝑓(𝑥𝑘)∥2 ≤−12
𝜎𝐾
𝐾−1
∑︁
𝑘=0
E[ 𝑓(𝑥𝑘+1)] + 12
𝜎𝐾
𝐾−1
∑︁
𝑘=0
E[ 𝑓(𝑥𝑘)] + 18𝜎𝐿𝜁2
≤−12
𝜎𝐾E[ 𝑓(𝑥𝐾)] + 12
𝜎𝐾E[ 𝑓(𝑥0)] + 18𝜎𝐿𝜁2
≤−12
𝜎𝐾E[ 𝑓(𝑥𝐾) −𝑓∗] + 12
𝜎𝐾E[ 𝑓(𝑥0) −𝑓∗] + 18𝜎𝐿𝜁2
≤12
𝜎𝐾E[ 𝑓(𝑥0) −𝑓∗] + 18𝜎𝐿𝜁2.
This concludes the proof.
□
4.3
Technique sketch for annealed regularization
In this section, we analyze stochastic NGN stepsize with decreasing 𝜎𝑘, i.e.
𝛾𝑘=
𝜎𝑘
1 +
𝜎𝑘
2 𝑓𝑖𝑘(𝑥𝑘) ∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 .
(41)
According to the NGN approximation (8), the setting 𝜎𝑘→0 can be thought of as gradually
increasing the regularization strength (i.e. getting more similar to a vanilla SGD update as 𝑘→∞).
For a general time-dependent 𝜎𝑘, all local inequalities hold. In particular, the following three
lemmas hold with no additional proof required.
Lemma 4.9 (Stepsize bounds, time dependent). Let each 𝑓𝑖: R𝑑→R be non-negative, differen-
tiable and 𝐿-smooth. Consider 𝛾𝑘as in (41), we have
𝛾𝑘∈

𝜎𝑘
1 + 𝜎𝑘𝐿, 𝜎

=
"
1
𝐿+ 𝜎−1
𝑘
, 𝜎𝑘
#
.
Lemma 4.10 (Fundamental Equality, time dependent). Consider 𝛾𝑘as in (41). One has
𝛾𝑘∥∇𝑓𝑖𝑘(𝑥)∥2 = 2
𝜎𝑘−𝛾𝑘
𝜎𝑘

𝑓𝑖𝑘(𝑥).
Lemma 4.11 (Fundamental inequality, time dependent). Let each 𝑓𝑖: R𝑑→R be non-negative,
𝐿-smooth and convex. Consider 𝛾𝑘as in (41), we have
𝛾2
𝑘∥∇𝑓𝑖𝑘(𝑥𝑘)∥2 ≤

4𝜎𝑘𝐿
1 + 2𝜎𝑘𝐿

𝛾𝑘[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓∗
𝑖𝑘] +
2𝜎2
𝑘𝐿
1 + 𝜎𝑘𝐿· max

0, 2𝜎𝑘𝐿−1
2𝜎𝑘𝐿+ 1

· 𝑓∗
𝑖𝑘.
29

We present proof for the convex setting. The nonconvex case is very similar and uses the same
techniques; therefore, we omit it. The strongly convex setting can also be easily derived using e.g.
the techniques in Lacoste-Julien et al. (2012).
Theorem 4.12 (NGN, convex, decreasing 𝜎𝑘). Let 𝑓=
1
𝑁
P𝑁
𝑖=1 𝑓𝑖, where each 𝑓𝑖: R𝑑→R is
non-negative, 𝐿-smooth and convex. Consider the SGD method (2) with the stepsize (41). For any
value of 𝜎0 > 0, setting 𝜎𝑘= 𝜎0/
√
𝑘+ 1 leads to the following rate: for 𝐾≥2,
E[ 𝑓( ¯𝑥𝐾) −𝑓(𝑥∗)] ≤𝐶1∥𝑥0 −𝑥∗∥2
√
𝐾−1
+ 𝐶1𝐶2 ln(𝐾+ 1)
√
𝐾−1
= 𝑂
ln(𝐾)
√
𝐾

.
(42)
where ¯𝑥𝐾= P𝐾−1
𝑘=0 𝑝𝑘,𝐾𝑥𝑘with 𝑝𝑘,𝐾=
𝜎𝑘
P𝐾−1
𝑘=0 𝜎𝑘and
𝐶1 = (1 + 2𝜎0𝐿)(1 + 𝜎0𝐿)
4𝜎0
,
𝐶2 =

6Δint + 2 max {0, 2𝜎0𝐿−1} Δpos

𝐿𝜎2
0 .
Proof. Using Lemmas 4.9, 4.10 and 4.11 and following the same exact steps as in the proof of
Theorem 4.5, we arrive at
∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2 ≤−𝑇0(𝜎𝑘)[ 𝑓𝑖𝑘(𝑥𝑘) −𝑓𝑖𝑘(𝑥∗)] + 𝑇1(𝜎2
𝑘)[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑇2(𝜎2
𝑘) 𝑓∗
𝑖𝑘,
with
𝑇0(𝜎𝑘) =
2𝜎𝑘
(1 + 2𝜎𝑘𝐿)(1 + 𝜎𝑘𝐿),
𝑇1(𝜎2
𝑘) =
6𝐿𝜎2
𝑘
1 + 2𝜎𝑘𝐿,
𝑇2(𝜎2
𝑘) =
2𝜎2
𝑘𝐿
1 + 𝜎𝑘𝐿· max

0, 2𝜎𝑘𝐿−1
2𝜎𝑘𝐿+ 1

Taking the expectation, we get the compact formula
𝑇0(𝜎𝑘)E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] ≤−E

∥𝑥𝑘+1 −𝑥∗∥2 −∥𝑥𝑘−𝑥∗∥2
+ 𝐸𝑘(𝜎2
𝑘),
where 𝐸𝑘(𝜎2
𝑘) is the expected error at step 𝑘:
𝐸𝑘(𝜎2
𝑘) := 𝑇1(𝜎2
𝑘)E[ 𝑓𝑖𝑘(𝑥∗) −𝑓∗
𝑖𝑘] + 𝑇2(𝜎2
𝑘)E[ 𝑓∗
𝑖𝑘]
≤𝑇1(𝜎2
𝑘)Δint + 𝑇2(𝜎2
𝑘)Δpos.
(43)
Recall the definitions Δint := E[ 𝑓𝑖(𝑥∗) −𝑓∗
𝑖] and Δpos := E[ 𝑓∗
𝑖].
Following standard techniques (Garrigos and Gower, 2023), summing over 𝑘and using tele-
scopic cancellation gives
𝐾−1
∑︁
𝑘=0
𝑇0(𝜎𝑘)E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] ≤∥𝑥0 −𝑥∗∥2 +
𝐾−1
∑︁
𝑘=0
𝐸𝑘(𝜎2
𝑘),
(44)
30

Let us now construct a pointwise lower bound ˜𝑇0(𝜎𝑘) to 𝑇0(𝜎𝑘), using the fact that 𝜎𝑘is decreasing:
𝑇0(𝜎𝑘) =
2𝜎𝑘
(1 + 2𝜎𝑘𝐿)(1 + 𝜎𝑘𝐿) ≥
2𝜎𝑘
(1 + 2𝜎0𝐿)(1 + 𝜎0𝐿) =: ˜𝑇0(𝜎𝑘).
Then we have
𝐾−1
∑︁
𝑘=0
˜𝑇0(𝜎𝑘)E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] ≤∥𝑥0 −𝑥∗∥2 +
𝐾−1
∑︁
𝑘=0
𝐸𝑘(𝜎2
𝑘),
Let us divide every term in the above inequality by P𝐾−1
𝑘=0 ˜𝑇0(𝜎𝑘):
𝐾−1
∑︁
𝑘=0
 
˜𝑇0(𝜎𝑘)
P𝐾−1
𝑘=0 ˜𝑇0(𝜎𝑘)
!
E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] ≤
∥𝑥0 −𝑥∗∥2
P𝐾−1
𝑘=0 ˜𝑇0(𝜎𝑘) +
P𝐾−1
𝑘=0 𝐸𝑘(𝜎2
𝑘)
P𝐾−1
𝑘=0 ˜𝑇0(𝜎𝑘) .
(45)
Using an integral bound, we have, for 𝐾≥2,
𝐾−1
∑︁
𝑘=0
1
√
𝑘+ 1
≥
∫𝐾
1
1√𝑡
𝑑𝑡= 2(
√
𝐾−1).
Therefore, we have
𝐾−1
∑︁
𝑘=0
˜𝑇0(𝜎𝑘) =
𝐾−1
∑︁
𝑘=0
2𝜎𝑘
(1 + 2𝜎0𝐿)(1 + 𝜎0𝐿)
=
2𝜎0
(1 + 2𝜎0𝐿)(1 + 𝜎0𝐿)
𝐾−1
∑︁
𝑘=0
1
√
𝑘+ 1
≥
4𝜎0(
√
𝐾−1)
(1 + 2𝜎0𝐿)(1 + 𝜎0𝐿) .
Note also that
 
˜𝑇0(𝜎𝑘)
P𝐾−1
𝑘=0 ˜𝑇0(𝜎𝑘)
!
=
𝜎𝑘
P𝐾−1
𝑘=0 𝜎𝑘
=: 𝑝𝑘,𝐾,
where 𝑝𝑘,𝐾as a function of 𝑘is a probability distribution over the interval [0, 𝐾−1].
We are left with bounding P𝐾−1
𝑘=0 𝐸𝑘(𝜎2
𝑘). Since 𝜎𝑘is decreasing, we have the following bounds:
𝑇1(𝜎2
𝑘) =
6𝐿𝜎2
𝑘
1 + 2𝜎𝑘𝐿≤6𝐿𝜎2
𝑘,
𝑇2(𝜎2
𝑘) =
2𝜎2
𝑘𝐿
1 + 𝜎𝑘𝐿· max

0, 2𝜎𝑘𝐿−1
2𝜎𝑘𝐿+ 1

≤2𝜎2
𝑘𝐿· max {0, 2𝜎0𝐿−1} .
Next we use a standard bound on the 𝐾-th Harmonic number 𝐻𝐾.
𝐾−1
∑︁
𝑘=0
1
𝑘+ 1 = 1 + 1
2 + 1
3 + · · · + 1
𝐾= 𝐻𝐾≤ln(𝐾+ 1).
31

Therefore,
𝐾−1
∑︁
𝑘=0
𝑇1(𝜎2
𝑘) ≤6𝐿
𝐾−1
∑︁
𝑘=0
𝜎2
𝑘= 6𝐿𝜎2
0
𝐾−1
∑︁
𝑘=0
1
𝑘+ 1 ≤6𝐿𝜎2
0 ln(𝐾+ 1),
and
𝐾−1
∑︁
𝑘=0
𝑇2(𝜎2
𝑘) ≤2𝐿· max {0, 2𝜎0𝐿−1}
𝐾−1
∑︁
𝑘=0
𝜎2
𝑘≤2𝐿· max {0, 2𝜎0𝐿−1} 𝜎2
0 ln(𝐾+ 1).
All in all, we substitute the above bounds into (43) to obtain
𝐾−1
∑︁
𝑘=0
𝐸𝑘(𝜎2
𝑘) ≤Δint
𝐾−1
∑︁
𝑘=0
𝑇1(𝜎2
𝑘) + Δpos
𝐾−1
∑︁
𝑘=0
𝑇2(𝜎2
𝑘)
≤

6Δint + 2 max {0, 2𝜎0𝐿−1} Δpos

𝐿𝜎2
0 ln(𝐾+ 1).
Plugging everything back into the bound (45), we have, for 𝐾≥2
𝐾−1
∑︁
𝑘=0
𝑝𝑘,𝐾E[ 𝑓(𝑥𝑘) −𝑓(𝑥∗)] ≤𝐶1∥𝑥0 −𝑥∗∥2
√
𝐾−1
+ 𝐶1𝐶2 ln(𝐾+ 1)
√
𝐾−1
,
with
𝐶1 = (1 + 2𝜎0𝐿)(1 + 𝜎0𝐿)
4𝜎0
,
𝐶2 =

6Δint + 2 max {0, 2𝜎0𝐿−1} Δpos

𝐿𝜎2
0 .
To conclude, let ¯𝑥𝐾= P𝐾−1
𝑘=0 𝑝𝑘,𝐾𝑥𝑘. Jensen’s inequality implies
𝑓( ¯𝑥𝐾) = 𝑓
 
𝐾−1
∑︁
𝑘=0
𝑝𝑘,𝐾𝑥𝑘
!
≤
𝐾−1
∑︁
𝑘=0
𝑝𝑘,𝐾𝑓(𝑥𝑘).
This concludes the proof.
□
4.4
Deep learning experiments
In this section, we test the performance of NGN on deep convolutional networks. We consider the
following settings with increasing complexity:
• A 12 layers (7 convolutional layers and 5 max-pooling layers, with batch normalization and
dropout) neural network trained on the Street View House Numbers (SVHN) Dataset (Netzer
et al., 2011) for 50 epochs with a batch size 512;
• A ResNet18 (He et al., 2016) network6 trained on CIFAR10 (Krizhevsky et al., 2009) for 50
epochs with a batch size 128.
6We use code from the popular repository https://github.com/kuangliu/pytorch-cifar.
32

Setting
Optimizer
Hyperparameter
#0
#1 ( )
#2 ( )
#3 ( )
#4
SVHN + Small Convnet
SGD
0.01
0.03
0.1
0.3
1
Adam
0.00003
0.0001
0.0003
0.001
0.003
Adagrad-norm
1
3
10
30
100
SPS
0.3
1
3
10
30
NGN
0.3
1
3
10
30
CIFAR10 + ResNet18
SGD
0.03
0.1
0.3
1
3
Adam
0.00003
0.0001
0.0003
0.001
0.003
Adagrad-norm
1
3
10
30
100
SPS
0.1
0.3
1
3
10
NGN
0.3
1
3
10
30
Imagenet + ResNet50
SGD
-
0.3
1
3
-
Adam
-
0.0003
0.001
0.003
-
Adagrad-norm
-
100
300
1000
-
SPS
-
1
3
10
-
NGN
-
3
10
30
-
Table 3: Hyperparameters (𝜎for NGN, learning rate for Adam and SGD) by dataset and optimiza-
tion algorithm, results in Figure 5.
• A ResNet50 (He et al., 2016) network7 trained on Imagenet (Deng et al., 2009) for 30 epochs
with a batch size 256.
All our experiments are performed on NVIDIA V100 GPUs. Reported are mean and confidence
interval8 bars over three seeds for training loss and test accuracy. In Figure 5 we compare NGN
with SGD and Adam (Kingma and Ba, 2014), tuning 𝜎in NGN and the learning rate in SGD and
Adam. All other parameters in Adam (𝛽1 and 𝛽2) are, as usual in practice, kept constant. While
Adam is used with momentum 𝛽1 = 0.9, we show here the performance of SGD without momentum
to draw a better comparison with our NGN. We also do not make use of L2 regularization as this
would bias results on the training speed. All hyperparameters are kept constant during training
for SVHN and CIFAR10, while for our largest scale experiment (Imagenet), we decrease 𝜎and
learning rates 10-fold every 10 epochs. In Figure 6 we show the same NGN statistics as in Figure 5,
but compare with two adaptive stepsizes with strong guarantees: SPSmax and Adragrad-norm. For
SPS: 𝛾𝑘= min

𝑓𝑖𝑘(𝑥𝑘)−𝑓∗
𝑖𝑘
𝑐∥∇𝑓(𝑥𝑘)𝑖𝑘∥2, 𝛾𝑏

we fix 𝑐= 1 as motivated by the theory in Loizou et al. (2021)
and tune 𝛾𝑏. For Adagrad-norm 𝛾𝑘= 𝜂/
√︃
𝑏2
0 + P𝑘
𝑗=0 ∥∇𝑓𝑖𝑗(𝑥𝑗)∥2 we fix 𝑏0 = 1𝑒−2 and tune 𝜂.
As our main objective here is to show how NGN can adapt to the landscape and converge faster
than SGD for different values of its hyperparameter, we first grid-search the optimal hyperparameters
for all methods on a large logarithmically-spaced grid [1𝑒−5, 3𝑒−5, 1𝑒−4, . . . , 3, 10, 30], and
7We use the official PyTorch repo https://github.com/pytorch/examples/tree/main/imagenet.
8Twice the standard deviation over the square root of number of samples, in our case three.
33

then show results for one/two choices smaller and bigger hyperparameters.
Table 3 lists the
hyperparameter values used for each method and architecture, where hyperparameter #2 (in bold)
is the best-performing for each method as indicated in the last-train-step train accuracy plots (second
column of Figure 5 and 6).
While in the second and fourth column of Figure 5 we show the last-iterate statistics for all
hyperparameters in Table 3, in the first and third column we plot the dynamics for the three best-
performing hyperparameters, and we mark them as
(hyperparameter #1),
(hyperparameter #2),
and
(hyperparameter #3). We now comment on the results in Figure 5, i.e. on the comparison
with SGD and Adam:
• NGN always performs best in terms of training loss performance.
This holds not only
under optimal tuning (hyperparameter #2, ) but also across all suboptimal hyperparameters
#0, #1, #3, #4, as can be seen in the second column of Figure 5: orange curve lower bounds
blue and green curves.
• The faster training performance of NGN is more stable across hyperparameters compared to
SGD and Adam. This can be especially seen in ResNets, both on CIFAR10 and Imagenet.
Note that the value 𝜎= 3 performs almost optimally for all tasks.
• The test performance of NGN is more robust to tuning compared to SGD and Adam, as can
be seen in the right-most panel in Figure 5. However, while on Imagenet, test performance
is best for NGN, Adam is superior on SVHN and CIFAR10, with a gap of 0.5 −1%.
• NGN is more efficient than Adam: the Adam optimizer maintains second-order statistics for
each parameter (Anil et al., 2019). This, in large models where GPUs need to be filled to
maximum, introduces significant memory overheads that restrict the size of the model being
used as well as the number of examples in a mini-batch. NGN, instead, has nearly the same
wall-clock and memory complexity as SGD.
Although here we did not test the effect of L2 regularization in the interest of avoiding con-
founders, the promising training speed results makes us believe that NGN, equipped with gen-
eralization boosting strategies, such as SAM (Foret et al., 2020), Noise Injection (Orvieto et al.,
2022a, 2023), or simply decoupled L2 regularization (Loshchilov and Hutter, 2017), can lead to
best generalization performance combined with faster training.
In Figure 6 we instead compare NGN with tuned SPS and Adagrad-norm. While we found that
Adagrad-norm cannot deliver solid performance, SPS behaves at times similarly to NGN. This is
not surprising, since there is a strict relation between SPS and NGN. However, we found that NGN
performs drastically better on the ResNet18 setting, with an edge also on Imagenet in train loss.
Effective learning rate.
To conclude, we study in Figure 7 the learning rate dynamics in NGN for
𝜎= 3 (uniformly close to optimal performance) in the context of the experiments in this section.
On SVHN and CIFAR10, we used vanilla NGN with constant 𝜎, and found that the stepsize 𝛾𝑘has
a peculiar warmup-decay behavior. Interestingly, the duration of the warm-up phase coincides with
the usual practice in deep learning (Geiping and Goldstein, 2023). Note that while the learning rate
34


	

)(#
/	
/
*$'$'"(++
!+

 &



	


.)!*)*&!,!*
/
$'%*$'$'"(++

	

)(#








	
!+,-*.
!+

 &



	


.)!*)*&!,!*






	
$'%!+,-*.

	

&% 
	 , -

 , -
 , -
 , -
'!$!$%((
(#""

#



	


+&'&'#)'


	
	







!$"'!$!$%((

	

&% 


	




()*'+
(#""

#



	


+&'&'#)'


	





!$"()*'+


	


)(#

	







*$'$'"(++
&"!'!,!+!,


 &
%(.
!+,
#$"#
/)!*)*&!,!*
	





$'%*$'$'"(++


	


)(#
	




!+,-*/
&"!'!,!+!,


 &
%(.
!+,
#$"#
/)!*)*&!,!*









$'%!+,-*/
Figure 5: (SGD vs. Adam vs. NGN) Experimental results on Deep Neural Networks (stochastic
gradients). All details and comments can be found in the text. Shown is performance for five or
three hyperparameters (Table 3), each method is tuned to best at hyperparameter #2.
35


	



*) $
		
	
	
	
	
	
	
	
	
+%(%(#),,
'#"("-","-
!#+!()+'


&)/
",-
$%#$
0*"+*+'"-"+
	
	
	
	


%(&+%(%(#),,

	



*) $






",-  .+ 0
'#"("-","-
!#+!()+'


&)/
",-
$%#$
0*"+*+'"-"+








%(&",-  .+ 0




'&!

 - 	.	
 - 	.	
 - 	.	
 - 	.	
("%"% &))
)$##
 (%&($



	



,'('($*(
	
	









"%#("%"% &))




'&!

	





)*+(,
)$##
 (%&($



	



,'('($*(

	






"%#)*+(,




*) $
	0
	0	
+%(%(#),,
	",	
!#+!()+'



	



/*"+*+'"-"+
	0	
%(&+%(%(#),,




*) $






	
	


",-  .+ /
	",	
!#+!()+'



	



/*"+*+'"-"+





	


%(&",-  .+ /
Figure 6: (Adagrad-norm vs.
SPS vs.
NGN) Experimental results on Deep Neural Net-
works (stochastic gradients).
All details and comments can be found in the text.
Shown is
performance for five or three hyperparameters (Table 3), each method is tuned to best at hyperpa-
rameter #2.
36

0
200
400
600
800
1000
Iteration
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Effective NGN stepsize value 
k
Small CNN on SVHN, 
= 3
0
5000
10000
15000
20000
Iteration
0.0
0.5
1.0
1.5
2.0
2.5
3.0
Effective NGN stepsize value 
k
ResNet18 on CIFAR10, 
= 3
0.0e+0 2.0e+4 4.0e+4 6.0e+4 8.0e+4 1.0e+5 1.2e+5 1.4e+5
Iteration
10
2
10
1
100
Effective NGN stepsize value 
k
ResNet50 on ImageNet, 
= 3
Figure 7: Effective NGN stepsize in the deep learning experiments of Figure 5. Shown is mean
and 1 standard deviation of 𝛾𝑘for the best-performing 𝜎.
decays automatically after warmup, it does not anneal to very small values – which is needed to
counteract variance of stochastic gradients in the large dataset setting. In our Imagenet experiment,
supported by the theory in Section 4.3 ,we therefore automatically decay 𝜎10-fold every 30 epochs.
The performance of NGN shines in the first 30 epochs exhibiting again a warmup-decay behavior.
The learning rate after the first epoch is closer to the one proposed by SGD, but convergence is
faster due to the initial highly-adaptive phase.
5
Conclusion and future work
In this paper, we introduced NGN, an adaptive optimizer with the same memory requirements
and per-iteration cost as SGD. We showed that this algorithm has intriguing and unique curvature
estimation properties, leading to features such as stability to hyperparameter tuning and adaptive
estimation of the Lipschitz constant. Our theory supports the empirical findings and the rationale
behind the NGN derivation. In the convex setting, NGN is guaranteed to converge with a sublinear
rate to any arbitrarily small neighborhood of the solution without requiring access to the gradient
Lipschitz constant (as instead needed in SGD) or access to the minimizer value of 𝑓(as instead
needed in SPSmax (Loizou et al., 2021)). Among our results, our rates guarantee adaptive con-
vergence to an arbitrarily small neighborhood with optimal speed in the strongly convex setting,
a guarantee that is missing for both Adagrad and Polyak stepsize-based methods. NGN does not
require access to the value of the loss at the solution, and by decreasing its hyperparameter 𝜎, NGN
becomes closer to the SGD update. Our empirical results on neural networks show that NGN is
stronger than vanilla SGD, SPS, and Adagrad and can be competitive with Adam even under heavy
hyperparameter tuning.
Our work also leaves open many important questions for future research. For example, it is
possible to obtain better convergence rates in the nonconvex case under the Polyak- Lojasiewicz
condition (e.g., Karimi et al., 2016).
A very interesting question is how we can incorporate
momentum into the NGN update rule and derive improved convergence rates. Along this direction,
there are two recent works on using momentum together with the stochastic Polyak stepsize: Wang
et al. (2023b) and Oikonomou and Loizou (2024), and we believe that similar extensions for NGN
are also possible. And finally, it will be very interesting to investigate the (block-) diagonal variant
37

of NGN, where each coordinate (or block coordinates) has its own NGN type of stepsize. The
coordinate- or block-adaptive variants may help boost performance for training transformers where
each parameter group has different curvature property (Noci et al., 2022).
References
Anil, R., Gupta, V., Koren, T., and Singer, Y. (2019). Memory efficient adaptive optimization.
Advances in Neural Information Processing Systems, 32.
Berrada, L., Zisserman, A., and Kumar, M. P. (2020).
Training neural networks for and by
interpolation. In International conference on machine learning, pages 799–809. PMLR.
Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for large-scale machine
learning. SIAM review, 60(2):223–311.
Chang, C.-C. and Lin, C.-J. (2011).
Libsvm: a library for support vector machines.
ACM
transactions on intelligent systems and technology (TIST), 2(3):1–27.
Chen, P. (2011). Hessian matrix vs. gauss–newton hessian matrix. SIAM Journal on Numerical
Analysis, 49(4):1417–1435.
Cutkosky, A. (2020). Parameter-free, dynamic, and strongly-adaptive online learning. In Proceed-
ings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of
Machine Learning Research, pages 2250–2259. PMLR.
Defazio, A., Cutkosky, A., Mehta, H., and Mishchenko, K. (2023). When, why and how much?
adaptive learning rate scheduling by refinement. arXiv:2310.07831.
Defazio, A. and Mishchenko, K. (2023). Learning-rate-free learning by d-adaptation. In Proceed-
ings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of
Machine Learning Research, pages 7449–7479. PMLR.
Defazio, A., Xingyu, Yang, Mehta, H., Mishchenko, K., Khaled, A., and Cutkosky, A. (2024). The
road less scheduled. arXiv:2405.15682.
Delyon, B. and Juditsky, A. (1993). Accelerated stochastic approximation. SIAM Journal on
Optimization, 3(4):868–881.
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-
scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern
recognition, pages 248–255. Ieee.
Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7).
38

Faw, M., Rout, L., Caramanis, C., and Shakkottai, S. (2023). Beyond uniform smoothness: A
stopped analysis of adaptive sgd. In The Thirty Sixth Annual Conference on Learning Theory,
pages 89–160. PMLR.
Faw, M., Tziotis, I., Caramanis, C., Mokhtari, A., Shakkottai, S., and Ward, R. (2022). The power
of adaptivity in sgd: Self-tuning step sizes with unbounded gradients and affine variance. In
Conference on Learning Theory, pages 313–355. PMLR.
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. (2020). Sharpness-aware minimization for
efficiently improving generalization. arXiv preprint arXiv:2010.01412.
Garrigos, G. and Gower, R. M. (2023). Handbook of convergence theorems for (stochastic) gradient
methods. arXiv preprint arXiv:2301.11235.
Geiping, J. and Goldstein, T. (2023). Cramming: Training a language model on a single gpu in one
day. In International Conference on Machine Learning, pages 11117–11143. PMLR.
Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic
programming. SIAM journal on optimization, 23(4):2341–2368.
Hazan, E. and Kakade, S. (2019). Revisiting the polyak step size. arXiv preprint arXiv:1905.00313.
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778.
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L.,
Hendricks, L. A., Welbl, J., Clark, A., et al. (2022). Training compute-optimal large language
models. arXiv preprint arXiv:2203.15556.
Jacobs, R. A. (1988). Increased rates of convergence through learning rate adaption. Neural
Networks, 1:295–307.
Karimi, H., Nutini, J., and Schmidt, M. (2016). Linear convergence of gradient and proximal-
gradient methods under the Polyak- lojasiewicz condition. In Frasconi, P., Landwehr, N., Manco,
G., and Vreeken, J., editors, Machine Learning and Knowledge Discovery in Databases (ECML
PKDD 2016), volume 9851 of Lectur Notes in Computer Sciencce. Springer.
Kesten, H. (1958).
Accelerated stochastic approximation.
Annals of Mathematical Statistics,
29(1):41–59.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.
Lacoste-Julien, S., Schmidt, M., and Bach, F. (2012). A simpler approach to obtaining an o (1/t) con-
vergence rate for the projected stochastic subgradient method. arXiv preprint arXiv:1212.2002.
39

Le Cun, Y., Bottou, L., Orr, G. B., and M¨uller, K.-R. (1998). Efficient backprop. In Neural
Networks, Tricks of the Trade, Lecture Notes in Computer Science LNCS 1524. Springer Verlag.
Levenberg, K. (1944). A method for the solution of certain non-linear problems in least squares.
Quarterly of Applied Mathematics, 2(2):164–168.
Liu, Z., Nguyen, T. D., Ene, A., and Nguyen, H. (2023). On the convergence of adagrad (norm)
on R𝑑: Beyond convexity, non-asymptotic rate and acceleration. In International Conference on
Learning Representations. International Conference on Learning Representations.
Loizou, N., Vaswani, S., Laradji, I. H., and Lacoste-Julien, S. (2021). Stochastic polyak step-size
for sgd: An adaptive learning rate for fast convergence. In International Conference on Artificial
Intelligence and Statistics, pages 1306–1314. PMLR.
Loshchilov, I. and Hutter, F. (2017).
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101.
Ma, S., Bassily, R., and Belkin, M. (2018).
The power of interpolation: Understanding the
effectiveness of sgd in modern over-parametrized learning. In International Conference on
Machine Learning, pages 3325–3334. PMLR.
Mahmood, A. R., Sutton, R. S., Degris, T., and Pilarski, P. M. (2012). Tuning-free step-size
adaption. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 2121–2124.
Marquardt, D. (1963). An algorithm for least-squares estimation of nonlinear parameters. SIAM
Journal on Applied Mathematics, 11(2):431–441.
Mirzoakhmedov, F. and Uryasev, S. P. (1983). Adaptive step adjustment for a stochastic optimization
algorithm. Zh. Vychisl. Mat. Mat. Fiz., 23(6):1314–1325. [U.S.S.R. Comput. Math. Math. Phys.
23:6, 1983].
Mishchenko, K. and Defazio, A. (2024). Prodigy: An expeditiously adaptive parameter-free learner.
arXiv:2306.06101.
Nesterov, Y. (2018). Lectures on convex optimization, volume 137 of Springer Optimization and
Its Applications. Springer, second edition.
Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. (2011). Reading digits
in natural images with unsupervised feature learning. In NIPS workshop on deep learning and
unsupervised feature learning. Granada, Spain.
Nocedal, J. and Wright, S. J. (2006). Numerical Optimization. Springer Series in Operations
Research. Springer, 2nd edition.
Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. (2022). Signal
propagation in transformers: Theoretical perspectives and the role of rank collapse.
40

Oikonomou, D. and Loizou, N. (2024). Stochastic polyak step-sizes and momentum: Convergence
guarantees and practical performance. arXiv:2406.04142.
Orabona, F. and P´al, D. (2016). Coin betting and parameter-free online learning. Advances in
Neural Information Processing Systems, 29.
Orabona, F. and Tommasi, T. (2017). Training deep networks without learning rates through coin
betting. Advances in Neural Information Processing Systems, 30.
Orvieto, A., Kersting, H., Proske, F., Bach, F., and Lucchi, A. (2022a). Anticorrelated noise
injection for improved generalization. In International Conference on Machine Learning, pages
17094–17116. PMLR.
Orvieto, A., Kohler, J., Pavllo, D., Hofmann, T., and Lucchi, A. (2022b). Vanishing curvature in
randomly initialized deep relu networks. In International Conference on Artificial Intelligence
and Statistics.
Orvieto, A., Lacoste-Julien, S., and Loizou, N. (2022c). Dynamics of SGD with stochastic polyak
stepsizes: Truly adaptive variants and convergence to exact solution. In Advances in Neural
Information Processing Systems.
Orvieto, A., Raj, A., Kersting, H., and Bach, F. (2023). Explicit regularization in overparametrized
models via noise injection. In International Conference on Artificial Intelligence and Statistics,
pages 7265–7287. PMLR.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., et al. (2011). Scikit-learn: Machine learning in python.
the Journal of machine Learning research, 12:2825–2830.
Polyak, B. T. (1987). Introduction to Optimization. Translation series in mathematics and engi-
neering. Optimization Software, Inc., Publications Division, New York, NY.
Prazeres, M. and Oberman, A. M. (2021). Stochastic gradient descent with polyak’s learning rate.
Journal of Scientific Computing, 89:1–16.
Reddi, S. J., Kale, S., and Kumar, S. (2019). On the convergence of adam and beyond. arXiv
preprint arXiv:1904.09237.
Rolinek, M. and Martius, G. (2018). L4: Practical loss-based stepsize adaptation for deep learning.
Advances in neural information processing systems, 31.
Schmidt, R. M., Schneider, F., and Hennig, P. (2021). Descending through a crowded valley-
benchmarking deep learning optimizers. In International Conference on Machine Learning,
pages 9367–9376. PMLR.
Schraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. In Proceedings
of Nineth International Conference on Artificial Neural Networks (ICANN), pages 569–574.
41

Sutskever, I., Martens, J., Dahl, G., and Hinton, G. (2013). On the importance of initialization and
momentum in deep learning. In Proceedings of the 30th International Conference on Machine
Learning, volume 28 of Proceedings of Machine Learning Research, pages 1139–1147, Atlanta,
Georgia, USA. PMLR.
Sutton, R. S. (1992). Adapting bias by gradient descent: An incremental version of Delta-Bar-
Delta. In Proceedings of the Tenth National Conference on Artificial Intelligence (AAAI’92),
pages 171–176. The MIT Press.
Vaswani, S., Laradji, I., Kunstner, F., Meng, S. Y., Schmidt, M., and Lacoste-Julien, S. (2020).
Adaptive gradient methods converge faster with over-parameterization (but you should do a
line-search). arXiv preprint arXiv:2006.06835.
Wang, B., Zhang, H., Ma, Z., and Chen, W. (2023a). Convergence of adagrad for non-convex
objectives: Simple proofs and relaxed assumptions. In The Thirty Sixth Annual Conference on
Learning Theory, pages 161–190. PMLR.
Wang, X., Johansson, M., and Zhang, T. (2023b). Generalized polyak step size for first order
optimization with momentum. In Proceedings of the 40th International Conference on Machine
Learning, volume 202 of Proceedings of Machine Learning Research, pages 35836–35863.
PMLR.
Ward, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex
landscapes. Journal of Machine Learning Research, 21(219):1–30.
Yang, J., Li, X., Fatkhullin, I., and He, N. (2023). Two sides of one coin: the limits of untuned sgd
and the power of adaptive methods. Advances in Neural Information Processing Systems, 36.
Zamani, M. and Glineur, F. (2023). Exact convergence rate of the last iterate in subgradient
methods. arXiv:2307.11134.
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2021). Understanding deep learning
(still) requires rethinking generalization. Communications of the ACM, 64(3):107–115.
42

