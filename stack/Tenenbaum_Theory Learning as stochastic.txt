Theory learning as stochastic search in the language of thought
The MIT Faculty has made this article openly available. Please share 
how this access benefits you. Your story matters.
Citation
Ullman, Tomer D., Noah D. Goodman, and Joshua B. Tenenbaum.
“Theory Learning as Stochastic Search in the Language of Thought.”
Cognitive Development 27, no. 4 (October 2012): 455–480.
As Published
http://dx.doi.org/10.1016/j.cogdev.2012.07.005
Publisher
Elsevier
Version
Author's final manuscript
Citable link
http://hdl.handle.net/1721.1/102507
Terms of Use
Creative Commons Attribution-NonCommercial-NoDerivs License
Detailed Terms
http://creativecommons.org/licenses/by-nc-nd/4.0/

Theory Learning as Stochastic Search in a Language of
Thought
Tomer D. Ullmana,∗, Noah D. Goodmanb, Joshua B. Tenenbauma
aDepartment of Brain and Cognitive Sciences, Massachusetts Institute of Technology,
Cambridge, USA
bDepartment of Psychology, Stanford University, Stanford, USA
Abstract
We present an algorithmic model for the development of children’s intu-
itive theories within a hierarchical Bayesian framework, where theories are
described as sets of logical laws generated by a probabilistic context-free
grammar.
We contrast our approach with connectionist and other emer-
gentist approaches to modeling cognitive development: while their subsym-
bolic representations provide a smooth error surface that supports eﬃcient
gradient-based learning, our symbolic representations are better suited to
capturing children’s intuitive theories but give rise to a harder learning prob-
lem, which can only be solved by exploratory search. Our algorithm attempts
to discover the theory that best explains a set of observed data by performing
stochastic search at two levels of abstraction: an outer loop in the space of
theories, and an inner loop in the space of explanations or models generated
by each theory given a particular dataset. We show that this stochastic search
is capable of learning appropriate theories in several everyday domains, and
discuss its dynamics in the context of empirical studies of children’s learning.
Keywords:
Bayesian models, MCMC, algorithms, language of thought,
intuitive theory
1. Introduction
If a person should say to you “I have toiled and not found”, don’t believe.
If they say “I have not toiled but found”, don’t believe. If they say “I have
∗Corresponding author. Tel.: +1 617 452 3894
Email address: tomeru@mit.edu (Tomer D. Ullman)
Preprint submitted to Cognitive Development
June 26, 2012

toiled and found”, believe. - Rabbi Itz’hak, Talmud
For the Rabbis of old, learning was toil, exhausting work – a lesson which
many scientists also appreciate. Over recent decades, scientists have toiled
hard trying to understand learning itself: what children know when, and how
they come to know it. How do children go from sparse fragments of observed
data to rich knowledge of the world? From one instance of a rabbit to all
rabbits, from occasional stories and explanations about a few animals to an
understanding of basic biology, from shiny objects that stick together to a
grasp of magnetism – children seem to go far beyond the speciﬁc facts of
experience to structured interpretations of the world.
What some scientists found in their toil is themselves. It has been argued
that children’s learning is much like a kind of science, both in terms of the
knowledge children create, its form, content, and function, and the means by
which they create it. Children organize their knowledge into intuitive theo-
ries, abstract coherent frameworks that guide inference and learning within
particular domains (Carey, 1985, 2009; Wellman & Gelman, 1992; Gopnik
& Meltzoﬀ, 1997; Murphy & Medin, 1985). Such theories allow children to
generalize from given evidence to new examples, make predictions and plan
eﬀective interventions on the world. Children even construct and revise these
intuitive theories using many of the same practices that scientists do (Schulz,
In press): searching for theories that best explain the data observed, trying
to make sense of anomalies, exploring further and even designing new experi-
ments that could produce informative data to resolve theoretical uncertainty,
and then revising their hypotheses in light of the new data.
Consider the following concrete example of theory acquisition which we
will return to frequently below. A child is given a bag of shiny, elongated,
hard objects to play with, and ﬁnds that some pairs seem to exert mysterious
forces on each other, pulling or pushing apart when they are brought near
enough. These are magnets, but she doesn’t know what that would mean.
This is her ﬁrst exposure to the domain. To make matters more interesting,
and more like the situation of early scientists exploring the phenomena of
magnetism in nature, suppose that all of the objects have an identical metal-
lic appearance, but only some of them are magnetic, and only a subset of
those are actually magnets (permanently magnetized). She may initially be
confused trying to ﬁgure out what interacts with what, but like a scientist
developing a ﬁrst theory, after enough exploration and experimentation, she
might start to sort the objects into groups based on similar behaviors or
2

similar functional properties. She might initially distinguish two groups, the
magnetic objects (which can interact with each other) and the nonmagnetic
ones (which do not interact). Perhaps then she will move on to subtler dis-
tinctions, noticing that this very simple theory doesn’t predict everything she
observes. She could distinguish three groups, separating the permanent mag-
nets from the rest of the magnetic objects as well as from the nonmagnetic
objects, and recognizing that there will only be an interaction if at least one
of the two magnetic objects brought together is a permanent magnet. With
more time to think and more careful observation, she might even come to
discover the existence of magnetic poles and the laws by which they attract
or repel when two magnets are brought into contact. These are but three of
a large number of potential theories, varying in complexity and power, that a
child could entertain to explain her observations and make predictions about
unseen interactions in this domain.
Our goal here is to explore computational models for how children might
acquire and revise an intuitive theory such as this, on the basis of domain
experience.
Any model of learning must address two kinds of questions:
what, and how? Which representations can capture the form and content
of what the learner comes to know, and which principles or mechanisms can
explain how the learner comes to know it, moving from one state of knowl-
edge to another in response to observed data? The main new contribution
of this paper addresses the ‘how’ question. We build on much recent work
addressing the ‘what’ question, which proposes to represent the content of
children’s intuitive theories as probabilistic generative models deﬁned over
hierarchies of structured symbolic representations (Tenenbaum et al., 2006,
2011; Kemp et al., 2008b). Previously the ‘how’ question has been addressed
only at a very high level of abstraction, if at all: the principles of Bayesian
inference explain how an ideal learner can successfully identify an appropri-
ate theory, based on maximizing the posterior probability of a theory given
data (as given by Bayes’ rule). But Bayes’ rule says nothing about the pro-
cesses by which a learner could construct such a theory, or revise it in light
of evidence. Here our goal is to address the ‘how’ of theory construction and
revision at a more mechanistic, process level, exploring cognitively realistic
learning algorithms. Put in terms of Marr’s three levels of analysis (Marr,
1982), previous Bayesian accounts of theory acquisition have concentrated
on the level of computational theory, while here we move to the algorith-
mic level of analysis, with the aim of giving a more plausible, practical and
experimentally fertile view of children’s developmental processes within the
3

Bayesian paradigm.
Our work here aims to explain two challenges of theory acquisition in
algorithmic terms. First is the problem of making learning work: getting the
world right, as reliably as children do. As any scientist can tell you, reﬂecting
on their own experiences of toil, the ‘how’ of theory construction and revision
is nontrivial. The process is often slow, painful, a matter of starts and stops,
random ﬁts and bursts, missteps and retreats, punctuated by occasional mo-
ments of great insight, progress and satisfaction – the ﬂashes of ’Aha!’ and
’Eureka!’. And as any parent will tell you, children’s cognitive development
often seems to have much the same character. Diﬀerent children make their
way to adult-like intuitive theories at very diﬀerent paces. Transitions be-
tween candidate theories often appear somewhat random and unpredictable
at a local level, prone to backtracking or “two steps forward, one step back”
behavior (Siegler & Chen, 1998). Yet in core domains of knowledge, and over
long time scales, theory acquisition is remarkably successful and consistent:
diﬀerent children (at least within a common cultural context of shared expe-
rience) tend to converge on the same knowledge structures, knowledge that
is much closer to a veridical account of the world’s causal structure than the
infant’s starting point, and they follow predictable trajectories along the way
(Carey, 2009; Gopnik & Meltzoﬀ, 1997; Wellman et al., 2011).
Our ﬁrst contribution is an existence proof to show how this kind of
learning could work – a model of how a search process with slow, ﬁtful and
often frustrating stochastic dynamics can still reliably get the world right, in
part because of these dynamics, not simply in spite of them. The process may
not look very algorithmic, in the sense of familiar deterministic algorithms
such as those for long division, ﬁnding square roots, or sorting a list, or
what cognitive scientists typically think of as a “learning algorithm”, such
as the backpropagation algorithm for training neural networks. Our model
is based on a Monte Carlo algorithm, which makes a series of randomized
(but not entirely random) choices as part of its execution. These choices
guide how the learner explores the space of theories to ﬁnd those that best
explain the observed data – inﬂuenced by, but not determined by, the data
and the learner’s current knowledge state. We show that such a Monte Carlo
exploratory search yields learning results and dynamics qualitatively similar
to what we see in children’s theory construction, for several illustrative cases.
Our second challenge is to address what could be called the “hard prob-
lem” of theory learning: learning a system of concepts that cannot be simply
expressed as functions of observable sense data or previously available con-
4

cepts – knowledge that is not simply an extension or addition to what was
known before, but that represents a fundamentally new way to think. De-
velopmental psychologists, most notably Carey (2009), have long viewed this
problem of conceptual change or theory change as one of the central explana-
tory challenges in cognitive development. To illustrate, consider the concepts
of “magnet” or “magnetic object” or “magnetic pole” in our scenario above,
for a child ﬁrst learning about them. There is no way to observe an object
on its own and decide if it falls under any of these concepts. There is no way
to deﬁne or describe either “magnet” or “magnetic object” in purely sensory
terms (terms that do not themselves refer to the laws and concepts of mag-
netism), nor to tell the diﬀerence between a “north” and a “south” magnetic
pole from perception alone. How then could these notions arise? They could
be introduced in the context of explanatory laws in a theory of magnetism,
such as “Two objects will interact if both are magnetic and at least one is a
magnet”, or “Magnets have two poles, one of each type, and opposite types
attract while like types repel.” If we could independently identify the mag-
nets and the magnetic objects, or the two poles of each magnetic object and
their types, then these laws would generate predictions that could be tested
on observable data. But only in virtue of these laws’ predictions can magnets,
magnetic objects, or magnetic poles even be identiﬁed or made meaningful.
And how could one even formulate or understand one of these laws without
already having the relevant concepts?
Theory learning thus presents children with a diﬃcult joint inference task
– a “chicken-and-egg” problem – of discovering two kinds of new knowledge,
new concepts and new laws, which can only be made sense of in terms of
each other: the laws are deﬁned over the concepts, but the concepts only get
their meaning from the roles they play in the laws. If learners do not begin
with either the appropriate concepts or the appropriate laws, how can they
end up acquiring both successfully? This is also essentially the challenge that
philosophers have long studied of grounding meaning in conceptual role or in-
ferential role semantics (Block, 1986; Harman, 1975, 1982; Field, 1977; Fodor
& Lepore, 1991). Traditional approaches to concept learning in psychology
do not address this problem, nor do they even attempt to (Bruner et al.,
1956; Smith & Medin, 1981; Rogers & McClelland, 2004). The elusiveness of
a satisfying solution has led some scholars, most famously Jerry Fodor, to a
radical skepticism on the prospects for learning genuinely new concepts, and
a view that most concepts must be innate in some nontrivial way (Fodor,
1975, 1980). Carey (2009) has proposed a set of informal “bootstrapping”
5

mechanisms for how human learners could solve this problem, but no formal
model of bootstrapping exists for theory learning, or concept learning in the
context of acquiring novel theories.
We will argue that the chicken-and-egg problem can be solved by a ratio-
nal learner but must be addressed in algorithmic terms to be truly satisfying:
a purely computational-level analysis will always fail for the Fodorian skeptic,
and will fail to make contact with the crux of the bootstrapping problem as
Carey (2009) frames it, since for the ideal learner the entire space of possible
theories, laws and concepts, is in a sense already available from the start. An
algorithmic implementation of that same ideal learning process can, however,
introduce genuinely new concepts and laws in response to observed data. It
can provide a concrete solution to the problem of how new concepts can be
learned and can acquire meaning in a theory of inferential role semantics.
Speciﬁcally, we show how a Monte Carlo search process deﬁned over a hier-
archically structured Bayesian model can eﬀectively introduce new concepts
as blank placeholders in the context of positing a new candidate explana-
tory law or extending an existing law. The new concept is not expressed in
terms of pre-existing concepts or observable data; rather it is posited as part
of a candidate explanation, together with pre-existing concepts, for observed
data. In testing the candidate law’s explanatory power, the new concepts are
given a concrete interpretation specifying which entities they are most likely
to apply to, assuming the law holds. If the new or modiﬁed law turns out to
be useful – that is, if it leads to an improved account of the learner’s obser-
vations, relative to their current theory – the law will tend to be retained,
and with it, the new concept and its most likely concrete grounding.
The rest of the paper is organized as follows. We ﬁrst present a nontech-
nical overview of the “what” and “how” of our approach to theory learning,
and contrast it with the most well-known alternatives for modeling cognitive
development based on connectionism and other emergentist paradigms. We
then describe our approach more technically, culminating in a Markov Chain
Monte Carlo (MCMC) search algorithm for exploring the space of candidate
theories based on proposing random changes to a theory and accepting prob-
abilistically those changes that tend to improve the theory. We highlight
two features that make the dynamics of learning more eﬃcient and reliable,
as well as more cognitively plausible: a prior that proposes new theoretical
laws drawn from law templates, biasing the search towards laws that express
canonical patterns of explanation useful across many domains, and a pro-
cess of annealing the search that reduces the amount of random exploration
6

over time. We study the algorithm’s behavior on two case studies of theory
learning inspired by everyday cognitive domains: the taxonomic organization
of object categories and properties, and a simpliﬁed version of magnetism.
Finally, we explore the dynamics of learning that arise from the interaction
between computational-level and algorithmic-level considerations: how the-
ories change both as a function of the quantity and quality of the learner’s
observations, and as a function of the time course of the annealing-guided
search process, which suggests promising directions for future experimental
research on children’s learning.
2. A nontechnical overview
A proposal for what children learn and a proposal for how they learn it
may be logically independent in some sense, but the two are mutually con-
straining. Richer, more structured accounts of the form and content of chil-
dren’s knowledge tend to pose harder learning challenges, requiring learning
algorithms that are more sophisticated and more costly to execute. As we ex-
plain below, our focus on explaining the origins of children’s intuitive theories
leads us to adopt relatively rich abstract forms of knowledge representations,
compared to alternative approaches to modeling cognitive development, such
as connectionism. This leaves us with relatively harder learning challenges –
connectionists might argue, prohibitively large. But we see these challenges
as inevitable: Sooner or later, computational models of development must
face them. Perhaps for the ﬁrst time, we can now begin to see what their
solution might look like, by bringing together recent ideas for modeling the
form and content of theories as probabilistic generative models over hier-
archies of symbolic representations (Katz et al., 2008; Kemp et al., 2008a;
Goodman et al., 2011) with tools for modeling the dynamics of learning as
exploratory search based on stochastic Monte Carlo algorithms.
2.1. The ‘What’: Modeling the form and content of children’s theories as
hierarchical probabilistic models over structured representations
As a form of abstract knowledge, an intuitive theory is similar to the
grammar of a language (Tenenbaum et al., 2007): The concepts and laws of
the theory can be used to generate explanations and predictions for an inﬁnite
(though constrained) set of phenomena in the theory’s domain. We follow
a long tradition in cognitive science and artiﬁcial intelligence of represent-
ing such knowledge in terms of compositional symbol systems, speciﬁcally
7

predicate logic that can express a wide range of possible laws and concepts
(Fodor, 1975; Fodor & Pylyshyn, 1988; Russell & Norvig, 2009). Embedding
this symbolic description language in a hierarchical probabilistic generative
model lets us bring to bear the powerful inductive learning machinery of
Bayesian inference, at multiple levels of abstraction (Griﬃths et al., 2010;
Tenenbaum et al., 2011).
Figure 1 illustrates this framework. We assume a domain of cognition is
given, comprised of one or more systems of entities and their relations, each
of which gives rise to some observed data. The learner’s task is to build a
theory of the domain: a set of abstract concepts and explanatory laws that
explain the observed data for each system in that domain. The learner is
assumed to have a hypothesis space of possible theories generated by (and
constrained by) some “Universal Theory”. We formalize this Universal The-
ory as a probabilistic generative grammar, essentially a probabilistic version
of a language of thought (Fodor, 1975). Within this universal language, the
learner constructs a speciﬁc theory that can be thought of as a more speciﬁc
language for explaining the phenomena of the given domain.
In principle, an ideal learner should consider all possible theories express-
ible in the language of thought and weigh them against each other in light of
observed evidence. In practice, there are inﬁnitely many candidate theories
and it will be impossible to explicitly consider even a small fraction of them.
Explaining how a learner proposes speciﬁc candidate theories for evaluation
is a task for our algorithmic-level account (see below under ‘How’).
Candidate theories are evaluated using Bayes’ rule to assess how likely
they are to have generated the observed data. Bayes’ rule scores theories
based on the product of their prior probabilities and their likelihoods. The
prior reﬂects the probability of generating the laws and concepts of a the-
ory a priori from the generative grammar, independent of any data to be
explained.
The likelihood measures the probability of generating the ob-
served data given the theory, independent of the theory’s plausibility. Oc-
cam’s razor-like considerations emerge naturally from a Bayesian analysis:
the prior will be highest for the simplest theories, whose laws can be gener-
ated with the fewest number of a priori stipulations, while the likelihood will
be highest for theories whose laws allow a domain to be described accurately
and compactly, generating the observed data with a spare set of minimal
facts.
The ﬁt of a theory to data cannot be evaluated directly; its laws ex-
press the abstract principles underlying a domain but no speciﬁc expectations
8

about what is true or false. One level below the theory in the hierarchical
framework, the learner posits a logical model of each observed system in the
domain. The logical model, or “model” for short, speciﬁes what is true of
the entities in a particular system in ways consistent with and constrained
by the theory’s abstract laws. Each model can be thought of as one partic-
ular concrete instantiation of the abstract theory. It generates a probability
distribution over possible observations for the corresponding system, and it
can be scored directly in terms of how well those predictions ﬁt the actual
data observed.
As a concrete example of this framework, consider again the child learning
about the domain of magnetism. She might begin by playing with a few
pieces of metal and notice that some of the objects interact, exerting strange
pulling or pushing forces on each other. She could describe the data directly,
as “Object a interacts with object j”, “Object i interacts with object j”, and
so on.
Or she could form a simple theory, in terms of abstract concepts
such as magnet, magnetic object and non-magnetic object, and laws such as
”Magnets interact with other magnets”, “Magnets interact with magnetic
objects”, and “Interactions are symmetric”.
It is important to note that
terms like magnet convey no actual information about the object, and they
are simply labels. Systems in this domain correspond to speciﬁc subsets of
objects, such as the set of objects a, ..., i in Figure 1. A model of a system
speciﬁes the minimal facts needed to apply the abstract theory to the system,
in this case which objects are magnetic, which are magnets, and which are
non-magnetic. From these core facts the laws of the theory determine all
other true facts – in our example, this means all the pairwise interactions
between the objects: e.g., objects i and j, being magnets, should interact,
but i and e should not, because the latter is non-magnetic. Finally, the true
facts generate the actual data observed by the learner via a noisy sampling
process, e.g. observing a random subset of the object pairs that interact, and
occasionally misperceiving an object’s identity or the nature of an interaction.
While the abstract concepts in this simpliﬁed magnetism theory are at-
tributes of objects, more complex relations are possible. Consider for example
a theory of taxonomy, as in Collins and Quillian’s classic model of semantic
memory as an inheritance hierarchy (Collins & Quillian, 1969). Here the
abstract concepts are is a relations between categories and has a relations
between categories and properties. The theory underlying taxonomy has two
basic laws: “The is a relation is transitive” and “The has a relation inherits
down is a relations” (laws 3 and 4 on the “Taxonomy” column of Figure
9

1). A system consists of a speciﬁc set of categories and properties, such as
salmon, eagle, breathes, can ﬂy, and so on. A model speciﬁes the minimal
is a and has a relations, typically corresponding to a tree of is a relations be-
tween categories with properties attached by has a relations at the broadest
category they hold for: e.g., “A canary is a bird”, “A bird is an animal”, “An
animal can breathe”, and so on. The laws then determine that properties
inherit down chains of is a relations to generate many other true facts that
can potentially be observed, e.g., “A canary can breathe”.
The analogy between learning a theory for a domain and learning a gram-
mar for a natural language thus extends down through all levels of the hi-
erarchy of Figure 1. A logical model for a system of observed entities and
relations can be thought of as a parse of that system under the grammar of
the theory, just as the theory itself can be thought of as a parse of a whole do-
main under the grammar of the universal theory. In our hierarchical Bayesian
framework, theory learning is the problem of searching jointly for the the-
ory of a domain and models of each observed system in that domain that
together best parse all the observed data.1
Previous applications of grammar-based hierarchical Bayesian models
have shown how, given suﬃcient evidence and a suitable theory grammar, an
ideal Bayesian learner can identify appropriate theories in domains such as
causality (Griﬃths et al., 2010; Goodman et al., 2011), kinship and other so-
cial structures (Kemp et al., 2008a), and intuitive biology (Tenenbaum et al.,
2007). While our focus in this paper is the algorithmic level – the dynamics
of how learners can search through a space of theories – we have found that
endowing our theory grammars with one innovation greatly improves their
algorithmic tractability. We make the grammar more likely to generate theo-
ries with useful laws by equipping it with law templates, or forms of laws that
capture canonical patterns of coherent explanation arising in many domains.
For example, law templates might suggest explanations for when an observed
relation r(X, Y ) holds between entities X and Y (e.g., X attracts Y , X ac-
tivates Y , X has Y ) in terms of latent attributes of the objects, f(X) and
g(Y ), or in terms of some other relation s(X, Y ) that holds between them,
or some combination thereof: perhaps r(X, Y ) holds if f(X) and s(X, Y ) are
1The idea of hierarchical Bayesian grammar induction, where the prior on grammars
is itself generated by a grammar (or “grammar grammar”), dates back at least to the
seminal work of Feldman and colleagues (Feldman et al., 1969).
10

both true. Explanatory chains introducing novel objects are also included
among the templates: perhaps r(X, Y ) holds if there exists a Z such that
s(X, Z) and s(Z, Y ) hold. As we explain below, making these templates ex-
plicit in the grammar makes learning both more cognitively plausible and
much faster.
The most familiar computational alternative to structured Bayesian ac-
counts of cognitive development are connectionist models, and other emer-
gentist approaches (McClelland et al., 2010). Instead of representing chil-
dren’s abstract knowledge in terms of explicit symbol systems, these ap-
proaches attribute abstract knowledge to children only implicitly as an ‘emer-
gent’ phenomenon that arises in a graded fashion from interactions among
more concrete, lower-level non-symbolic elements – often inspired loosely
by neuroscience. Dynamical systems models view the nervous system as a
complex adaptive system evolving on multiple timescales, with emergent be-
havior in its dynamics. Connectionist models view children’s knowledge as
embedded in the strengths of connections between many neuron-like process-
ing units, and treat development as the tuning of these strengths via some
experience-dependent adjustment rule. Connectionists typically deny that
the basic units of traditional knowledge representation – objects, concepts,
predicates, relations, propositions, rules and other symbolic abstractions –
are appropriate for characterizing children’s understanding of the world, ex-
cept insofar as they emerge as approximate higher-level descriptions for the
behavior dictated by a network’s weights.
While emergentist models have been well-received in some areas of devel-
opment, such as the study of motor and action systems (McClelland et al.,
2010), emergentist models of the structure and origins of abstract knowledge
(Rogers & McClelland, 2004) have not been widely embraced by developmen-
talists studying children’s theories (Gopnik & Meltzoﬀ, 1997; Carey, 2009).
There is every reason to believe that explicit symbolic structure is just as im-
portant for children’s intuitive theories as for scientists’ more formal theories
– that children, like scientists, cannot adequately represent the underlying
structure of a domain such as physics, psychology or biology simply with a
matrix of weights in a network that maps a given set of inputs to a given
set of outputs. Children require explicit representations of abstract concepts
and laws in order to talk about their knowledge in natural language, and
to change and grow their knowledge through talking with others; to reason
causally in order to plan for the future, explain the past, or imagine hypothet-
ical situations; to apply their knowledge in novel settings to solve problems
11

Universal 
Theory
Theory
Model
Data
Probabilistic Horn Clause Grammar
Laws:
 interacts(X,Y)           f(X)    f(Y) 
 interacts(X,Y)           f(X)    g(Y)
 interacts(X,Y)           interacts(Y,X)
Laws: 
 is_a(X,Y)             s(X,Y) 
 has_a(X,Y)          t(X,Y) 
 has_a(X,Y)          is_a(X,Z)    has_a(Z,Y)
 is_a(X,Y)             is_a(X,Z)    is_a(Z,Y)
animal
breathes
ﬁsh
can_swim
salmon
is_pink
shark
can_bite
bird
can_ﬂy
canary
eagle
can_sing
has_claws
s(X,Y): “is_a”
t(X,Y): “has_a”
f(X): “magnets”
g(X): “magnetic objects”
“non-magnetic objects”
“a shark is a ﬁsh”
“a bird can ﬂy”
“a canary can ﬂy”
“a salmon can breathe”
Magnetism
Taxonomy
...
Laws: 
 female(X)             u(X) 
 spouse(X,Y)         v(X,Y) 
 spouse(X,Y)         v(Y,X)  
 child(X,Y)             w(X,Y)
 child(X,Y)             child(X,Z)    spouse(Z,Y)
 father(X,Y)               female(X)    child(X,Y)
Kinship
John
Mary
Richard
Joan
William
Anne
Margaret
Thomas
Judith
Hamnet
“John is William’s father”
“John is Judith’s grandfather”
“Judith is Hamnet’s sister”
“Margaret is Judith’s aunt”
...
=
=
=
Spouse
Parent
=
Laws: 
 goes_to(X,Y,S)          d(X,Z)    location(Z,Y,S)
 
Psychology
...
Agent 1
Agent 2
Core Predicates: u(X), v(X,Y), w(X,Y)
Surface Predicates: female(X), child(X,Y), parent(X,Y)
 
 
          spouse(X,Y), father(X,Y), ...
Core Predicates: d(X,Y)
Surface Predicates: goes_to(X,Y,S) 
Background: location(X,Y,S) 
Core Predicates: s(X,Y), t(X,Y)
Surface Predicates: has_a(X,Y), is_a(X,Y)
Core Predicates: f(X), g(X)
Surface Predicates: interacts(X,Y)
Figure 1: A hierarchical Bayesian framework for theory acquisition. Each level generates the space of possibilities for the
level below, providing constraints for inference. Four examples of possible domain theories are given in separate columns,
while the rows correspond to diﬀerent levels of the hierarchy. A domain theory aims to explain observable values of one or
more surface predicates by positing one or more core predicates and a set of simple laws relating them (perhaps supplemented
by some background knowledge, as with the location predicate in the right-most column). The core predicates represent the
minimal facts necessary to explain the observations; a model of a theory is then a particular extension of the core predicates
to the objects in the domain. The observations are assumed to be a random sample of all the true facts given by the model.
Probabilistic inference on this hierarchical model then supports multiple functions, including learning a theory from observed
data, using a theory to derive the most compact model that explains a set of observations, and using that model to predict
unobserved data.
12

that they have never before encountered; and to compose abstractions re-
cursively, as in forming beliefs about others’ beliefs about the physical world
and how those beliefs might be diﬀerent than one’s own.
Despite these limitations, connectionist models have been appealing to de-
velopmentalists who emphasize the processes and dynamics of learning more
than the nature of children’s knowledge representations (Shultz, 2003; Mc-
Clelland et al., 2010). This appeal may come from the fact that when we turn
from the ‘what’ to the ‘how’ of children’s learning, connectionist models have
a decided advantage: learning in connectionist systems appears much better
suited to practical algorithmic formulation, and much more tractable, rela-
tive to structured probabilistic models or any explicitly symbolic approach.
As we explain below, making the ‘how’ of learning plausible and tractable
may be the biggest challenge facing the structured probabilistic approach.
2.2. The ‘How’: Modeling the dynamics of children’s theory learning as stochas-
tic (Monte Carlo) exploratory search
It is helpful to imagine the problem children face in learning as that of
moving over a “knowledge landscape”, where each point represents a pos-
sible state of knowledge and the height of that point reﬂects the value of
that knowledge-state – how well it allows the child to explain, predict, and
act on their world. Such a picture is useful in showing some of the diﬀer-
ences between our approach to cognitive development and the connectionist
and emergentist alternatives, and it highlights the much more serious ‘how’
challenge that confronts structured probabilistic models.
Viewed in landscape terms (Figure 2), connectionist models typically
posit that children’s knowledge landscape is continuous and smooth, and this
matters greatly for the mechanisms and dynamics of learning. Learning con-
sists of traversing a high-dimensional real-valued “weight space”, where each
dimension corresponds to the strength of one connection in a neural network.
Figure 2 depicts only a two-dimensional slice of the much higher dimensional
landscape corresponding to the three-layer network shown. The height of the
landscape assigned to each point in weight space – each joint setting of all the
network’s weights – measures how well the network explains observed data in
terms of an error or energy function, such as a sum-of-squared-error expres-
sion. The topology of these landscapes is simple and uniform: at any point of
the space, one can always move along any dimension independently of every
other, and changing one parameter has no eﬀect on any other. The geometry
is also straightforward: neighboring states, separated by small changes in
13

the weights or parameters, typically yield networks with very similar input-
output functionality. Thus a small move in any direction typically leads to
only a small rise or fall in the error or energy function.
This geometry directly translates into the dynamics of learning: the Hebb
rule, the Delta Rule, Backpropagation and other standard weight-adjustment
rules (Mcclelland & Rumelhart, 1986) can be seen as implementing gradient
descent – descending the error or energy landscape by taking small steps
along the steepest direction – and it can be proven that this dynamic reli-
ably takes the network to a local minimum of error, or a locally best ﬁtting
state of knowledge. In certain cases, particularly of interest in contempo-
rary machine learning systems (Bishop, 2006), the error landscape can be
designed to have a geometric property known as convexity, ensuring that
any local minimum is also a global minimum and thus that the best pos-
sible learning end-state can be achieved using only local weight-adjustment
rules based on gradient descent. Thus learning becomes essentially a mat-
ter of “rolling downhill”, and is just as simple. Even in cases where there
are multiple distinct local minima, connectionist learning can still draw on
a powerful toolkit of optimization methods that exploit the fact that the
landscape is continuous and smooth to make learning relatively fast, reliable
and automatic.
Weight Space
Higher Energy/Error
Lower Energy/Error
Weight Space
Input
Hidden
Output
1. Current weights 
3. Move along gradient
2. Find gradient
4. New weights 
Input
Hidden
Output
Figure 2: A hypothetical neural network and a weight space spanning the possible values
of two particular connections. Steps 1-4 show the sequence of a learning algorithm in such
a space: the calculation of a gradient and the move to a lower point. This corresponds to
a shift in the network’s connection weights and a smaller error on the output.
Now consider the landscape of theory learning from the perspective of
our structured Bayesian approach, and it should become clear how much
more diﬃcult the problem is (Figure 3). Each point on the landscape now
represents a candidate domain theory expressed in terms of one or more
14

laws in ﬁrst-order logic and one or more abstract concepts indicated by a
blank predicate (e.g., f(X), g(X)). Two possibilities for a simple theory
of magnetism are shown, labeled Theory B and Theory C (these will be
explained in much greater detail below).
The height of the surface at a
given point represents how well the corresponding theory is supported by
the observed data, which we measure as the Bayesian posterior probability.
(Note that in contrast to Figure 2, where “lower is better”, here “higher is
better”, and the goal is to seek out maxima of the landscape, not minima.)
Unlike the weight space shown in Figure 2, this portrait of a “theory space” as
two-dimensional is only metaphorical: it is not simply a lower-dimensional
slice of a higher-dimensional space.
The space of theories in a language
of thought is inﬁnite and combinatorially structured with a neighborhood
structure that is impossible to visualize faithfully on a page.
 interacts(X,Y)           interacts(Y,X)
Theory Space
Theory Space
Higher Probability
Lower Probability
1. Current theory: Theory B 
 interacts(X,Y)           f(X)    f(Y) 
 interacts(X,Y)           f(X)    g(Y)
3. Compare current and
    proposed theories 
4. Probabilistically
    accept proposal 
2. Probabilistically propose an
    alternative theory: Theory C
 interacts(X,Y)           f(X)    f(Y)
 interacts(X,Y)           f(X)    g(Y)
Figure 3: Schematic representation of the learning landscape within the domain of simple
magnetism. Steps 1-4 illustrate the algorithmic process in this framework. The actual
space of of theories is discrete, multidimensional and not necessarily locally connected.
At the level of computational theory, we can imagine an ideal Bayesian
learner who computes the full posterior probability distribution over all pos-
sible theories, that is, who grasps this entire landscape and assesses its height
at all points in parallel, conditioned on any given observed data set. But this
is clearly unrealistic as a starting point for algorithmic accounts of children’s
learning, or any practical learning system with limited processing resources.
Intuition suggests that children may simultaneously consider no more than
a handful of candidate theories in their active thought, and developmental-
ists typically speak of the child’s current theory as if, as in connectionist
models, the learner’s knowledge state corresponds to just a single point on
the landscape rather than the whole surface or posterior distribution. The
ideal Bayesian learner is in a sense similar to a person who has “not toiled
15

but found” from the opening: the entire hypothesis space is already deﬁned
and the learner’s task is merely to reshuﬄe probability over that space in
response to evidence. The actual child must toil and construct her abstract
theory, piece by piece, generalizing from experience.
Considering how a learner could move around on this landscape in search
of the best theory, we see that most of the appealing properties of connec-
tionist knowledge landscapes – the features that support eﬃcient learning
algorithms – are not present here. The geometry of the landscape is far from
smooth: A small change in one of the concepts or laws of a theory will often
lead to a drastic rise or fall in its plausibility, leading to a proliferation of
isolated local maxima. There is typically no local information (such as a
gradient) diagnostic of the most valuable directions in which to move. The
landscape is even more irregular in ways that are not easily visualized. There
is no uniform topology or neighborhood structure: the number and nature
of variants that can be proposed by making local changes to the learner’s
current hypothesis vary greatly over the space, depending on the form of
that hypothesis. Often changing one aspect of a theory requires others to
be changed simultaneously in order to preserve coherence: for instance, if we
posit a new abstract concept in our theory, such as the notion of a magnet,
or if we remove a conceptual distinction (such as the distinction between
magnets and magnetic objects), then one or more laws of the theory will need
to be added, removed or redeﬁned at the same time.
Artiﬁcial intelligence has a long history of treating learning in terms of
search through a discrete space of symbolic descriptions, and a wide variety of
search algorithms have been proposed to solve problems such as rule discov-
ery, concept learning and generalization, scientiﬁc law discovery, and causal
learning (Newell & Simon, 1976; Mitchell, 1982; Bradshaw et al., 1983; Pearl,
2000; Spirtes et al., 2001). For some of these problems, there exist systematic
search algorithms that can be as fast and reliable as the gradient-based opti-
mization methods used in connectionist learning(Mitchell, 1982; Pearl, 2000;
Spirtes et al., 2001). But for problems like scientiﬁc discovery (Bradshaw
et al., 1983), or our formulation of children’s theory learning, the best known
search algorithms are not like this. Much like child learners, we suggest, these
algorithms are slow, unreliable, and unsystematic (indeed often random), but
with enough patience they can be expected to converge on veridical theories.
The speciﬁc search algorithm we describe is based on widely used methods
in statistics and AI for approximating intractable Bayesian inferences, known
as Markov Chain Monte Carlo (MCMC). MCMC algorithms have recently
16

been proposed as models for the short-timescale dynamics of perceptual in-
ferences in the brain (Gershman et al., 2009; Sundareswara & Schrater, 2008;
Moreno-Bote et al., 2011), but they are also well-suited to understanding the
much longer-term dynamics of learning.
The remainder of this section sketches how our MCMC algorithm answers
the two main challenges we set out at the start of this paper: explaining how
children can reliably converge on veridical theories, given their constrained
cognitive resources and a learning dynamic that often appears more random
than systematic, and explaining how children can solve the hard “chicken-
and-egg” inference problem of jointly learning new concepts and new laws
deﬁned in terms of those concepts.
The heart of MCMC theory learning is an iterative loop of several basic
steps, shown in Figure 3. The learner begins at some point in the theory
landscape (e.g. theory B or C in Figure 3). The learner then proposes a
possible move to a diﬀerent theory, based on modifying the current theory’s
form: adding/deleting a law or set of laws, changing parts of a law or in-
troducing a new concept, and so on. The proposed and current theories are
compared based on evaluating (approximately) how well they explain the
observed data (i.e., comparing the relative heights of these two points on the
theory landscape). If the proposed theory scores higher, the learner accepts
it and moves to this new location. If the proposal scores lower, the learner
may still accept it or reject it (staying at the same location), with probability
proportional to the relative scores of the two theories. These steps are then
repeated with a new proposal based on the new current location.
From the standpoint of MCMC, randomness is not a problem but rather
an essential tool for exploring the theory landscape. Because MCMC algo-
rithms consider only one hypothesis at a time and propose local modiﬁcations
to it, and there are no generally available signals (analogous to the error gra-
dient in connectionist learning) for how to choose the best modiﬁcation of the
current hypothesis out of an inﬁnite number of possible variations, the best
learners can do is to propose variant theories to explore chosen in a random-
ized but hopefully intelligent fashion. Our algorithm proposes variants to the
current hypothesis by replacing a randomly chosen part of the theory with
another random draw from the probabilistic generative grammar for theories
(that is, the prior over theories). This process could in principle propose any
theory as a variant on any other, but it is naturally biased towards candi-
dates that are most similar to the current hypothesis, as well as those that
are a priori simpler and more readily generated by the grammar’s templates
17

for coherent laws. The use of law templates is crucial in focusing the random
proposal mechanism on the most promising candidates. Without templates,
all of the laws proposed could still have been generated from a more general
grammar, but they would be much less likely a priori; learners would end
up wasting most of their computational eﬀort considering simple but uesless
candidate laws. The templates make it likely that any random proposal is
at least a plausibly useful explanation, not just a syntactically well-formed
expression in the language of thought.
The decision of whether to accept or reject a proposed theory change is
also made in a randomized but intelligently biased fashion. If a proposed
change improves the theory’s account of the data, it is always accepted, but
sometimes a change that makes the theory worse could also be accepted. This
probabilistic acceptance rule helps keep the leaner from becoming trapped for
too long in poor local maxima of the theory landscape (Gilks & Spiegelhalter,
1996).
Although we use MCMC as a search algorithm, aiming to ﬁnd the best
theory, the algorithm’s proper function is not to ﬁnd a single optimal theory
but rather to visit all theories with probability proportional to their posterior
probability.
We can interpolate between MCMC as a posterior inference
technique and MCMC as a search algorithm by annealing – or starting with
more stochastic (or noisy) search moves and “lowering the temperature”,
making the search more deterministic over time (Kirkpatrick et al., 1983;
Spall, 2003). This greatly improves convergence to the true theory. Such
an algorithm can begin with little or no knowledge of a domain and, given
enough time and suﬃcient data, reliably converge on the correct theory or
at least some approximation thereof, corresponding to a small set of abstract
predicates and laws.
Annealing is also responsible for giving the MCMC search algorithm some
of its psychologically plausible dynamics.
It gives rise to an early high-
temperature exploration period characterized by a large number of proposed
theories, most of which are far from veridical. As we see in young children,
new theories are quick to be adopted and just as quick to be discarded. As the
temperature is decreased, partially correct theories become more entrenched,
it becomes rarer for learners to propose and accept large changes to their the-
ories, and the variance between diﬀerent theory learners goes down. As with
older children, rational learners at the later stages of an annealed MCMC
search tend to mostly agree on what is right, even if their theories are not
perfect. Without annealing, MCMC dynamics at a constant temperature
18

could result in a learner who is either too conservative (at low temperature)
or too aggressive (at high temperature) in pursuing new hypotheses – that
is, a learner who is prone to converge too early on a less-than-ideal theory,
or to never converge at all.
Figures 6a and 7a illustrate these learning dynamics in action. (these are
explained in detail in the next sections). On average, learners are consis-
tently improving. On average, they are improving gradually. But individu-
ally, learners often get worse before they get better. Individually, they adopt
theories in discrete jumps, signifying moments of fortuitous discovery. Such
dynamics on the level of the individual learner are more in line with discov-
ery processes in science and childhood than are the smoother dynamics of
gradient descent on a typical connectionist energy landscape. Critics might
reasonably complain that MCMC methods are slow and unreliable by com-
parison. But theory construction just is a diﬃcult, time-consuming, painful
and frustrating business – in both science and children’s cognition. We can
no more expect the dynamics of children’s learning to follow the much tamer
dynamics of gradient learning algorithms than we could expect to replace
scientists with a gradient-based learning machine and see the discoveries of
new concepts and new scientiﬁc laws emerging automatically. 2 Currently we
have no good alternative to symbolic representational machinery for captur-
ing intuitive theories, and no good alternative to stochastic search algorithms
for ﬁnding good points in the landscape of these symbolic theories.
What of the “hard problem of theory learning”, the challenge of jointly
learning new laws and new concepts deﬁned only in terms of each other?
Our MCMC search unfolds in parallel over two levels of abstraction: an
outer loop in the space of theories, deﬁned by sets of abstract laws; and an
inner loop in the space of explanations or models generated by the theory for
a particular domain of data, deﬁned by groundings of the theory’s concepts
2It is worth noting that not all connectionist architectures and learning procedures
are conﬁned to gradient-based methods operating on ﬁxed parametric architectures. In
particular the constructivist neural networks explored by Tom Shultz and colleagues Shultz
(2003) are motivated by some of the same considerations that we are, aiming to capture the
dynamics of children’s discovery with learning rules that implement a kind of exploratory
search.
These models are still limited in their representational power, however: they
can only express knowledge whose form and content ﬁts into the connections of a neural
network, and not the abstract concepts and laws that constitute an intuitive theory. For
that reason we favor the more explicitly symbolic approach described here.
19

on the speciﬁc entities of the domain. This two-level search lets us address
the “chicken and egg” challenge by ﬁrst proposing new laws or changes to
existing laws of a theory in the outer search loop; these new laws can posit
novel but ‘blank’ concepts of a certain form, whose meaning is then ﬁlled
in the most plausible way on the next inner search loop. For example, the
algorithm may posit a new rule never before considered, that objects of type
f interact with objects type g, without yet specifying what these concepts
mean; they are just represented with blank predicates f(X) and g(X). The
inner loop would then search for a reasonable assignment of objects to these
classes – values for f(X) and g(X), for each object X – grounding them
out as magnets and magnetic objects, for example. If this law proves useful
in explaining the learner’s observations, it is likely to persist in the MCMC
dynamics, and with it, the novel concepts that began as blank symbols f and
g but have now eﬀectively become what we call “magnets” and “magnetic
objects”.
In sum, we see many reasons to think that stochastic search in a lan-
guage of thought, with candidate theories generated by a probabilistic gen-
erative grammar and scored against observations in a hierarchical Bayesian
framework, provides a better account of children’s theory acquisition than
alternative computational paradigms for modeling development such as con-
nectionism. Yet there are also major gaps: scientists and young children
alike are smarter, more active, more deliberate and driven explorers of both
their theories and their experiences and experiments than are our MCMC
algorithms (Schulz, 2012). We now turn to a more technical treatment of
our model but we return to these gaps in the general discussion below.
3. Formal framework
This section gives a more formal treatment of theory learning, beginning
with our hierarchical Bayesian framework for describing ‘what’ is learned
(Figure 1), and then moving to our proposed MCMC algorithm for explaining
‘how’ it could be learned (Figure 3).
Formally, the hierarchical picture of knowledge shown in Figure 1 provides
the backbone for a multilevel probabilistic generative model: conditional
probability distributions that link knowledge at diﬀerent levels of abstraction,
supporting inference at any level(s) conditioned on knowledge or observations
at other levels. For instance, given a domain theory T and a set of noisy,
sparse observations D, a learner can infer the most likely model M and use
20

that knowledge to predict other facts not yet directly observed (Katz et al.,
2008; Kemp et al., 2008a). The theory T sets the hypothesis space and priors
for the model M, while the data D determine the model’s likelihood, and
Bayes’ rule combines these two factors into a model’s posterior probability
score,
P(M|D, T) ∝P(D|M)P(M|T).
(1)
If the theory T is unknown, the learner considers a hypothesis space of candi-
date theories generated by the higher-level universal theory (U) grammar. U
deﬁnes a prior distribution over the space of possible theories, P(T|U), and
again the data D determine a likelihood function, with Bayes’ rule assigning
a posterior probability score to each theory,
P(T|D, U) ∝P(D|T)P(T|U).
(2)
Bayes’ rule here captures the intuition of Occam’s razor, that the theory
which best explains a data set (has highest posterior probability P(T|D, U))
should balance between ﬁtting the data well (as measured by the likelihood
P(D|T)), and being simple or short to describe in our general language of
thought (as measured by the prior P(T|U)). Probabilistic inference can op-
erate in parallel across this hierarchical framework, propagating data-driven
information upward and theory-based constraints downward to make optimal
probabilistic inferences at all levels simultaneously.
Below we explain how each of these probability distributions is deﬁned
and computed. The ﬁrst step is to be more precise about how we represent
theories, which we have described so far using an informal mix of logic and
natural langauge but now formalize using ﬁrst-order predicate logic.
A language for theories. Following (Katz et al., 2008) we choose to
represent the laws in a theory as Horn clauses, logical expressions of the form
r ←(f ∧g∧...∧s∧t), where each term r, f, g, s, t, ... is a predicate expressing
an attribute or relation on entities in the domain, such as f(X) or s(X, Y ).
Horn clauses express logical implications – a set of conjunctive conditions
under which r holds – but can also capture intuitive causal relations (Kemp
et al., 2007) under the assumption that any propositions not generated by
the theory are assumed to be false. The use of implicational clauses as a
language for causal theories was explored extensively in (Feldman, 2006).
While richer logical forms are possible, Horn clauses provide a convenient
and tractable substrate for exploring the ideas of stochastic search over a
space of theories. In our formulation, the Horn clauses contain two kinds of
21

predicates: “core” and “surface”. Core predicates are those that cannot be
reduced further using the theory’s laws. Surface predicates are derived from
other predicates, either surface or core, via the laws. Predicates may or may
not be directly observable in the data. The core predicates can be seen as
compressing the full model into just the minimal bits necessary to specify all
true facts. As we explain in more detail below, a good theory is one that com-
presses a domain well, that explains as much of the observed data as possible
using only the information speciﬁed in the core predicates. In our magnetism
example, the core could be expressed in terms of two predicates f(X) and
g(X). Based on an assignment of truth values to these core predicates, the
learner can use the theory’s laws such as interacts(X, Y ) ←f(X) ∧g(Y ) to
derive values for the observable surface predicate interacts(X, Y ). For n ob-
jects, there are O(n2) interactions that can be observed (between all pairs of
objects) but these can be explained and predicted by specifying only O(n)
core predicate values (for each object, whether or not it is a magnet or is
magnetic).
As another example of how a theory supports compression via its core
predicates and abstract laws, consider the domain of kinship as shown in
Figure 1. A child learning this domain might capture it by core predicates
such as parent, spouse, and gender, and laws such as “Each child has two
parents of opposite gender, and those parents are each others’ spouse”; “A
male parent is a father”; “Two individuals with the same parent are siblings”;
“A female sibling is a sister”; and so on.
Systems in this domain would
correspond to individual families that the child knows about. A system could
then be compressed by specifying only the values of the core predicates, for
example which members of a family are spouses, who is the parent of whom,
and who is male or female. From this minimal set of facts and concepts all
other true facts about a particular family can be derived, predicting new
relationships that were not directly observed.
In constructing a theory, the learner introduces abstract predicates via
new laws, or new roles in existing laws, and thereby essentially creates new
concepts.
Notice that the core predicates in our magnetism theory need
be represented only in purely abstract terms, f(X) and g(X), and initially
they have only this bare abstract meaning. They acquire their meaning as
concepts picking out magnets or magnetic objects respectively in virtue of the
role they play in the theory’s laws and the explanations they come to support
for the observed data.
This is the sense in which our framework allows
the introduction of genuinely new abstract concepts via their inferential or
22

conceptual roles.
Entities may be typed and predicates restricted based on type constraints.
For example, in the taxonomy theory shown in Figure 1, has a(X, Y ) requires
that X be a category and Y be a property, while is a(X, Y ) requires that
X and Y both be categories. Forcing candidate models and theories to re-
spect these type constraints provides the learner with another valuable and
cognitively natural inductive bias.
Although our focus here is on the acquisition of intuitive theories in gen-
eral, across all domains of knowledge and all ages, much research has been
concerned with the form of young children’s theories in a few core domains
and the development of that knowledge over the ﬁrst years of life. Our horn-
clause language is too limited to express the full richness of a two-year-old’s
intuitive physics or intuitive psychology, but it can represent simpliﬁed ver-
sions of them. For example, in Figure 1 we show a fragment of a simple
“desire psychology” theory, one hypothesized early stage in the development
of intuitive psychology around two years of age (Wellman & Woolley, 1990).
This theory aims to explain agents’ goal-directed actions, such as reach-
ing for, moving towards or looking for various object, in terms of basic but
unobservable desires. In our language desires(X,Y) (or simply d(X, Y ) in
Figure 1) is a core predicate relating an agent X to an object Y . Desires
are posited to explain observations of a surface predicate goes to(X, Z, S):
agent X goes to (or reaches for or looks in) location Z in situation S. We
also introduce background information in the form of an additional predicate
location(Y,Z,S) available to the child, specifying that object Y is in location
Z in situation S. Then by positing which agents desire which objects, and
a law that says eﬀectively, “an agent will go to a certain location in a given
situation if that location contains an object that the agent desires”, a child
can predict how agents will act in various situations, and explain why they
do so.
The theory prior P(T|U). We posit U knowledge in the form of a prob-
abilistic context-free Horn clause grammar (PHCG) that generates the hy-
pothesis space of possible Horn-clause theories, and a prior P(T|U) over this
space (Figure 4). This grammar and the Monte Carlo algorithms we use to
sample or search over the theory posterior P(T|D, U) are based heavily on
(Goodman et al., 2008b), who introduced the approach for learning single
23

Top level theory
(S1)
S
⇒
(Law) ∧S
(S2)
S
⇒
(Tem) ∧S
(S3)
S
⇒
Stop
Random law generation
(Law)
Law
⇒
(Fleft ←Fright ∧Add)
(Add1)
A
⇒
F ∧Add
(Add2)
A
⇒
Stop
Predicate generation
(Fleft1)
Fleft
⇒
surface1()
...
(Fleft α)
Fleft
⇒
surfaceα()
(Fright1)
Fright
⇒
surface1()
...
(Fright α)
Fright
⇒
surfaceα()
(Fright(α + 1))
Fright
⇒
core1()
...
(Fright(α + β))
Fright
⇒
coreβ()
Law templates
(Tem1)
Tem
⇒
template1()
...
(Temγ)
Tem
⇒
templateγ()
Figure 4: Production rules of the Probabilistic Horn Clause Grammar. S is the start
symbol and Law, Add, F and Tem are non-terminals. α, β, and γ are the numbers of
surface predicates, core predicates, and law templates, respectively.
24

rule-based concepts rather than the larger law-based theory structures we
consider here. We refer readers to (Goodman et al., 2008b) for many tech-
nical details. Given a set of possible predicates in the domain, the PHCG
draws laws from a random construction process (Law) or from law templates
(Tem; explained in detail below) until the Stop symbol is reached, and then
grounds out these laws as horn clauses. The prior P(T|U) is the product of
the probabilities of choices made at each point in this derivation. Because all
these probabilities are less than one, the prior favors simpler theories with
shorter derivations. The precise probabilities of diﬀerent laws in the gram-
mar are treated as latent variables and integrated out, which favors re-use of
the same predicates and law components within a theory (Goodman et al.,
2008b).
Law templates. We make the grammar more likely to generate useful
laws by equipping it with templates, or canonical forms of laws that capture
structure likely to be shared across many domains. While it is possible for
the PHCG to reach each of these law forms without the use of templates,
their inclusion allows the most useful laws to be invented more readily. They
can also serve as the basis for transfer learning across domains.
For in-
stance, instead of having to re-invent transitivity anew in every domain with
some speciﬁc transitive predicates, a learner could recognize that the same
transitivity template applies in several domains. It may be costly to invent
transitivity for the ﬁrst time, but once found – and appreciated! – its abstract
form can be readily re-used. The speciﬁc law templates used are described in
Figure 5. Each “F(·)” symbol stands for a non-terminal representing a pred-
icate of a certain -arity. This non-terminal is later instantiated by a speciﬁc
predicate. For example, the template F(X, Y ) ←F(X, Z) ∧F(Z, Y ) might
be instantiated as is a(X, Y ) ←is a(X, Z) ∧is a(Z, Y ) (a familiar transi-
tive law) or as has a(X, Y ) ←is a(X, Z) ∧has a(Z, Y ) (the other key law
of taxonomy, which is like saying that “has a is transitive over is a”). This
template could be instantiated diﬀerently in other domains, for example in
kinship as child(X, Y ) ←child(X, Z) ∧spouse(Z, Y ), which states that the
child-parent relationship is transitive over spouse.
The theory likelihood P(D|T).
An abstract theory makes predictions
about the observed data in a domain only indirectly, via the models it gen-
erates. A theory typically generates many possible models: even if a child
25

F(X,Y) ←F(X,Z)∧F(Z,Y)
F(X,Y) ←F(Z,X)∧F(Z,Y)
F(X,Y) ←F(X,Z)∧F(Y,Z)
F(X,Y) ←F(Z,X)∧F(Y,Z)
F(X,Y) ←F(X,Y)∧F(X)
F(X,Y) ←F(Y,X)∧F(X)
F(X,Y) ←F(X,Y)∧F(Y)
F(X,Y) ←F(Y,X)∧F(Y)
F(X,Y) ←F(X)∧F(Y)
F(X,Y) ←F(Y,X)
F(X,Y) ←F(X,Y)
F(X)
←F(X)
F(X)
←F(X,Y)∧F(X)
F(X)
←F(Y,X)∧F(X)
F(X)
←F(X,Y)∧F(Y)
F(X)
←F(Y,X)∧F(Y)
Figure 5: Possible templates for new laws introduced by the grammar. The leftmost F
can be any surface predicate, the right F can be ﬁlled in by any surface or core predicates,
and X and Y follow the type constraints.
has the correct theory and abstract concepts of magnetism, she could catego-
rize a speciﬁc set of metal bars in many diﬀerent ways, each of which would
predict diﬀerent interactions that could be observed as data. Expanding the
theory likelihood,
P(D|T) =
X
M
P(D|M)P(M|T),
(3)
we see that theory T predicts data D well if it assigns high prior P(M|T)
to models M that make the data probable under the observation process
P(D|M).
The model prior P(M|T) reﬂects the intuition that a theory T explains
some data well if it compresses well: if it requires few additional degrees of
freedom beyond its abstract concepts and laws – that is, few speciﬁc and
contingent facts about the system under observation, besides the theory’s
general prescriptions – to make its predictions. This intuition is captured by
a prior that encourages the core predicates to be as sparse as possible, thereby
penalizing theories that can only ﬁt well by “overﬁtting” with many extra
degrees of freedom. This sparseness assumption is reasonable as a starting
point for many domains, given that core predicates are meant to explain and
26

compress the data. Formally, we assume a conjugate beta prior on all binary
facts in M, modeled as Bernoulli random variables which we integrate out
analytically, as in (Katz et al., 2008).
Finally, the model likelihood P(D|M, T) comes from assuming that we
are observing randomly sampled true facts (sampled with replacement, so the
same fact could be observed on multiple occasions), which also encourages the
model extension to be as small as possible. This provides a form of implicit
negative evidence (Tenenbaum & Griﬃths, 2001), useful as an inductive bias
when only positive facts of a domain are observed.
Stochastic search in theory space:
a grammar-based Monte
Carlo algorithm. Following (Goodman et al., 2008b), we use a grammar-
based Metropolis-Hastings (MH) algorithm to sample theories from the pos-
terior distribution over theories conditioned on data, P(T|D, U). This al-
gorithm is applicable to any grammatically structured theory space, such
as the one generated by our PHCG; it is also a version of the Church MH
inference algorithm (Goodman et al., 2008a). The MH algorithm is essen-
tially a Markov chain on the space of potential derivations from the gram-
mar, where each step in the chain – each proposed change to the current
theory – corresponds to regenerating some subtree of the derivation tree
from the PHCG. For example, if our theory of magnetism includes the law
interacts(X, Y ) ←f(X) ∧g(Y ), the MH procedure might propose to add or
delete a predicate (e.g., interacts(X, Y ) ←f(X) ∧g(Y ) ∧h(Y ) or
interacts(X, Y ) ←f(X)), to change one predicate to an alternative of the
same form (e.g., interacts(X, Y ) ←f(X) ∧h(Y )) or a diﬀerent form if avail-
able (e.g., interacts(X, Y ) ←f(X) ∧s(X, Y )); to resample the law from a
template (e.g., interacts(X, Y ) ←t(X, Z) ∧t(Z, Y )); or to add or delete a
whole law.
These proposals are accepted with probability equal to the maximum of
1 and the MH acceptance ratio,
P(T ′|D, U)
P(T|D, U) · Q(T|T ′)
Q(T ′|T),
(4)
where T is the current theory, T ′ is the new proposed theory, and Q(·|·)
is the transition probability from one theory to the other, derived from the
PHCG (Goodman et al., 2008b). To aid convergence we raise these accep-
tance ratios to a power greater than 1, which we increase very slightly after
each MH step in a form of simulated annealing.
Early on in learning, a
learner is thus more likely to try out a new theory that appears worse than
27

the current one, exploring candidate theories relatively freely. However, with
time the learner becomes more conservative – increasingly likely to reject
new theories unless they lead to an improved posterior probability.
While this MH algorithm could be viewed merely as a way to approxi-
mate the calculations necessary for a hierarchical Bayesian analysis, we sug-
gest that it could also capture in a schematic form the dynamic processes
of theory acquisition and change in young children. Stochastic proposals to
add a new law or change a predicate within an existing law are consistent
with some previous characterizations of children’s theory learning dynam-
ics (Siegler & Chen, 1998). These dynamics were previously proposed on
purely descriptive grounds, but here they emerge as a consequence of a ra-
tional learning algorithm. Although the dynamics of an MH search might
appear too random to an omniscient observer who knows the “true” target of
learning, it would not be fair to call the algorithm sub-optimal, because it is
the only known general-purpose approach for eﬀectively searching a complex
space of logical theories. Likewise, the annealing process that leads learning
to look child-like in a certain sense – starting oﬀwith more variable, rapidly
changing and adventurous theories, then becoming more conservative and
less variable over time – also makes very good engineering sense. Annealing
has proven to be useful in stochastic search problems across many scientiﬁc
domains (Kirkpatrick et al., 1983) and is the only known method to ensure
that a stochastic search converges to the globally optimal solution. It does
not seem implausible that some cognitive analog of annealing could be at
work in children’s learning.3
Approximating the theory score: an inner loop of MCMC Com-
puting the theory likelihood P(D|T), necessary to compare alternative theo-
ries in Equation (4), requires a summation over all possible models consistent
with the current theory (Equation (3)). Because this sum is typically very
hard to evaluate exactly, we approximate P(D|T) with P(D|M ∗, T)P(M ∗|T),
where M ∗is an estimate of the maximum a-posteriori (MAP) model in-
3It is worth noting that annealing could be implemented in a learning system with-
out an explicit temperature parameter or cooling schedule, merely based on experience
accumulating over time. Here for simplicity we have kept the learner’s dataset ﬁxed, but
if the learner is exposed to increasing amounts of data over time and treats all data as
independent samples from the model, this also acts to lower the eﬀective temperature by
creating larger ratios between likelihoods (and hence posterior probabilities) for a given
pair of theories.
28

ferred from the data: the most likely values of the core predicates.
The
MAP estimate M* is obtained by running an inner sampling procedure
over the values of the core predicates.
As in (Katz et al., 2008), we use
a specialized form of Metropolis-Hastings sampling known as Gibbs sam-
pling. The Gibbs sampler goes over each core predicate assignment in turn
while keeping all other assignments ﬁxed, and proposes changes to the cur-
rently considered assignment. As a concrete example of how the Gibbs loop
works, consider a learner who is proposing a theory that contains the law
interacts(X, Y ) ←f(X) ∧g(Y ), i.e., objects for which core predicate f is
true interact with objects for which core predicate g is true. The learner
begins by randomly extending the core categories over the domain’s objects:
e.g., f might be posited to hold for objects 1, 4, and 7, while g holds for
objects 2, 4, 6, and 8. (Note how either, both or none of the predicates
may hold for any object, a priori.) The learner then considers the exten-
sion of predicate f and proposes removing object 1, scoring the new model
(with all other assignments as before) on the observed data and accepting
the proposed change probabilistically depending on the relative scores. The
learner then considers objects 2, 3, and so on in turn, considering for each
object whether predicate f should apply, before moving on to predicate g.
(These object-predicate pairs are often best considered in random order on
each sweep through the domain.)
This process continues until a conver-
gence criteria is reached. We anneal slightly on each Gibbs sweep to speed
convergence and lock in the best solution. The Gibbs sampler over models
generated by a given theory is thus an “inner loop” of sampling in our learn-
ing algorithm, operating within each step of an “outer loop” sampling at a
higher level of abstract knowledge, the MH sampler over theories generated
by U knowledge.
4. Case Studies
We now explore the performance of this stochastic approach to theory
learning in two case studies, using simulated data from the domains of tax-
onomy and magnetism introduced above. We examine the learning dynamics
in each domain and make more explicit the possible parallels with human
theory acquisition.
29

4.1. Taxonomy
As we saw earlier, the domain of taxonomy illustrates how a compressive
knowledge representation is useful in capturing semantic data. How can such
a powerful organizing principle itself be learned? Katz et al. (2008) showed
that a Bayesian ideal observer can pick out the best theory of taxonomy given
a small set of eight possible alternatives. Here we show that the theory of
taxonomy can be learned in a more constructive way, via an MCMC search
through our inﬁnite grammar-generated hypothesis space. The theory to be
learned takes the following form:
Two core predicates:
s(X, Y ) and t(X, Y )
Two observable predicates:
is a(X, Y ) and has a(X, Y )
Law 1:
is a(X, Y ) ←s(X, Y )
Law 2:
has a(X, Y ) ←t(X, Y )
Law 3:
is a(X, Y ) ←is a(X, Z) ∧is a(Z, Y )
Law 4:
has a(X, Y ) ←is a(X, Z) ∧has a(Z, Y )
These laws by themselves do not yet capture the complete knowledge rep-
resentation we are after; we also need to instantiate the core predicates in a
particular model. These laws allow many possible models for any given data
sets. One of these models is the compressed tree representation (shown in
Figure 1 in the Model section of the taxonomy domain), which speciﬁes only
the minimal facts needed to derive the observed data from Laws 1-4. A dif-
ferent model could link explicitly all the is a(X,Y) connections, for example
drawing the links between salmon and animal, shark and animal and so on.
Another model could link explicitly all the has a(X,Y) connections. How-
ever, these latter two models would be much less sparse than the compressed
tree representation, and thus would be disfavored relative to the compressed
tree shown in Figure 1, given how we have deﬁned the model prior P(M|T).
In sum, in this framework, the organization of categories and properties into
a tree-structured inheritance hierarchy comes about from a combination of
positing the appropriate abstract laws and core predicates together with a
sparsity preference on the assignments of the core predicates’ values.
Note also that the core predicates s(X, Y ) and t(X, Y ) acquire their mean-
ing in part by their inferred extensions, and in part by how they are related
30

to the observed surface predicates. The surface predicates are assumed to be
verbal labels which the learner observes and needs to account for. The link
between these verbal labels and the core relations are what given by Laws
1 and 2. While these links could in general also be learned, we follow Katz
et al. (2008) in taking Laws 1 and 2 as given for this particular domain and
asking whether a learner can theoretically discover Laws 3 and 4 – but now
at the algorithmic level. We test learning for the same simple model of the
Acquisition 
of rule 4
Acquisition 
of rule 3
Simulation iterations
Log posterior
Simulation iterations
Log posterior
a.
b.
Figure 6: Representative runs of theory learning in Taxonomy. (a) Dashed lines show
diﬀerent runs. Solid line is the average across all runs. (b) Highlighting a particular run,
showing the acquisition of law 4, followed by the acquisition of law 3 and thus achieving
the ﬁnal correct theory.
taxonomy domain studied by Katz et al., using seven categories and seven
properties in a balanced tree structure. We presented all true facts from this
model as observations to the learner, including both property statements
(e.g., “An eagle has claws”) and category membership statements (e.g., “An
eagle is a bird”). The data for this section and the following case study can
be found in the appendix.
We ran 60 simulations, each comprising 1300 iterations of the outer MH
loop (i.e., moves in the space of theories). Four representative runs are shown
in Figure 6, as well as the average across all the runs. Out of 60 simulations,
52 found the correct theory within the given number of iterations, and 8
discovered a partial theory which included only Law 3 or Law 4.
Several points are worth noting beyond these quantitative results. First,
it is striking that abstract structure can be learned eﬀectively from very little
data. Using simple local search, our learning algorithm is able to navigate
an inﬁnite space of potential theories and discover the true laws underlying
31

the domain, even with relatively few observations in the relations between
seven categories and seven properties.
This is a version of the “blessing
of abstraction” described in Goodman et al. (2011) and Tenenbaum et al.
(2011), but one that is realized at the algorithmic level and not just the
computational level of ideal learning.
Second, individual learning trajectories proceed in a characteristic pat-
tern of stochastic leaps. Discovering the right laws gives the learner strong
explanatory power. However, surrounding each “good” theory in the discrete
hypothesis space are many syntactically similar but nonsensical or much less
useless formulations. Moving from a good theory to a better one thus de-
pends on proposing just the right changes to the current hypothesis. Since
these changes are proposed randomly, the learner often stays with a partic-
ular theory for many iterations, rejecting many proposed alternatives which
score worse or not signiﬁcantly better than the current theory, until a new
theory is proposed that is so much better it is almost surely accepted. This
leads to the observed pattern of plateaus in the theory score, punctuated by
sudden jumps upward and occasional jumps downward in probability. While
we do not want to suggest that people learn theories only by making random
changes to their mental structures, the probabilistic nature of proposals in
a stochastic search algorithm could in part explain why individual human
learning curves rarely proceed along a smooth path and can show broad
variation across individuals given the same data.
Third, while individual learning trajectories may be discontinuous, on
average learning appears smooth.
Aggregating performance over all runs
shows a smooth improvement of the theory’s score that belies the underly-
ing discrete nature of learning at an individual level. This emphasizes the
possible danger of studying theory learning and theory change only in the
average behavior of groups of subjects, and the theoretical value of micro-
genetic methods (Siegler & Crowley, 1991) for constraining algorithmic-level
models of children’s’ learning.
4.2. Magnetism
We now turn to the domain of magnetism, where the trajectory of theory
learning reveals not only successful acquisition, but interesting intermedi-
ate stages and transitions corresponding to classic phenomena of conceptual
change in childhood and early science (Carey, 2009). The simpliﬁed theory
of magnetism to be learned takes the following form:
32

Two core predicates:
f(X) and g(X)
One observable predicate:
interacts(X, Y )
Law 1:
interacts(X, Y ) ←f(X) ∧f(Y )
Law 2:
interacts(X, Y ) ←f(X) ∧g(Y )
Law 3:
interacts(X, Y ) ←interacts(X, Y )
The particular model used for learning contained 10 objects: 3 magnets, 5
magnetic objects and 2 non-magnetic objects. The learner was given all true
facts in this model, observing interactions between each magnet and every
other object that was either a magnet or a magnetic object, but no other
interactions. Unlike in the previous taxonomy example, the learner was given
none of the laws or core predicate structure to begin with; the entire theory
had to be constructed by the search algorithm. Assuming the correct laws
(as shown above) can be found, the model prior P(M|T) favoring sparsity
suggests the optimal values for the core predicates should assign one core
predicate (f) to all and only the magnets, and another predicate to all and
only the non-magnet magnetic objects. This leads to the theory and model
depicted jointly in Figure 1.
Simulation iterations
Log posterior
Simulation iterations
Log posterior
a.
b.
Acquisition of rule 1
(confounding of metals 
and magnets)
Acquisition of 
rules 2 and 3 
(splitting of metals
 and magnets)
Discarding unnecessary
rule 
Figure 7: Representative runs of theory learning in Magnetism. (a) Dashed lines show
diﬀerent runs. Solid line is the average across all runs. (b) Highlighting a particular run,
showing the acquisition of law 1 and the confounding of magnets and magnetic (but non-
magnet) objects, the discarding of an unnecessary law which improves the theory prior,
and the acquisition of the ﬁnal correct theory.
33

We ran 70 simulations, each comprising 1600 iterations of the outer MH
loop sampling over candidate theories. In many respects, results in this do-
main were qualitatively similar to what we described above for taxonomy.
Out of 70 simulated learning runs, 50 found the correct theory or a minor
logical variant of it; the rest discovered a partial theory. The correct ﬁnal
theories account for the full observed data and only the observed data, using
three laws. While all the full theories learned included Laws 1 and 2, only
some of them included the exact form of Law 3, expressing the symmetry of
interaction. 4 The dynamics of representative runs are displayed in Figure 7,
as well as the average over all the runs. As in the domain of taxonomy, indi-
vidual learners experienced radical jumps in their theories, while aggregating
across runs learning appears to be much smoother.
The most interesting aspects of learning here were found in the transitions
between distinct stages of learning, when novel core predicates are introduced
and existing core predicates shift their meaning in response. Key transitions
in children’s cognitive development may be marked by restructuring of con-
cepts, as when one core concept diﬀerentiates into two (Carey, 1985). Our
learning algorithm often shows this same dynamic in the magnetism task.
There is no single order of concept acquisition that the algorithm follows
in all or most runs, but the most common trajectory (shown in Figure 7b)
involves learning Law 1 ﬁrst, followed later by the acquisition of Laws 2 and
3. As mentioned earlier, for a learner who knows only Law 1, the optimal
setting of the core predicates is to lump together magnets and magnetic ob-
jects in one core predicate, essentially not diﬀerentiating between them. Only
when Laws 2 and 3 are learned does the learner also acquire a second core
predicate that carves oﬀthe magnetic non-magnets from the magnets. On
a smaller number of runs, a diﬀerent order of acquisition is observed: ﬁrst
Laws 2 and 3 are learned, and then Law 1 is added. This sequence also in-
volves a conceptual restructuring, albeit a less dramatic one. A learner who
4However, the variants discovered were functionally equivalent within this domain to
symmetry.
Such variants include redundant re-statements of symmetry, such as inter-
acts(X,Y) ←interacts(Y,Z) ∧equals(Z,X). Other forms happen to capture the same facts
as symmetry within this particular domain, such as interacts(X,Y) ←interacts(Y,Z) ∧
g(Z). These variants appear more complex than the basic symmetry law, and they do
score slightly worse than theories that recover the original formulation. However, since
they were generated by templates in this case, this extra complexity does not hurt them
signiﬁcantly.
34

possesses only Laws 2 and 3 will optimally assign one predicate to all and
only the magnets, and another core predicate to both magnets and magnetic
non-magnets, again lumping these two classes together. Only once Law 1
is added to Laws 2 and 3 will the learner completely diﬀerentiate the two
core predicates with non-overlapping extensions corresponding to magnets
and magnetic non-magnets.
In both of these cases, the time course of learning appears as a progression
from simpler theories (with fewer core predicates and/or laws) that explain
the data less faithfully or less eﬃciently, to more complex theories (with
more core predicates and/or laws) that explain the data more faithfully or
more eﬃciently. A learner with the simpler theory consisting of only Law
1 (without Laws 2 and 3) will overgeneralize, predicting the existence of
interactions that do not actually occur: interactions between pairs of non-
magnet magnetic objects (which would be treated the same as interactions
between two magnets, or a magnet and a magnetic object). A learner with
the simpler theory consisting of Laws 2 and 3 (but not Law 1) will make
the right predictions about interactions to be observed but would represent
the world less eﬃciently, less sparsely, than they could: they would need to
assign values for both core predicates to represent each magnet, rather than
just using a single core predicate to represent magnets and only magnets. Yet
while being less accurate or less eﬃcient, these earlier, simpler theories are
still reasonable ﬁrst approximations to the optimal theory of this domain.
They are also plausible intermediate points for the learner on the way to
the optimal theory, who can get there merely by adding one or two new laws
and diﬀerentiating the extension of a core predicate into two non-overlapping
subsets of objects, magnets and magnetic non-magnets, which had previously
been merged together in that predicate’s extension.
5. Two Sources of Learning Dynamics
The story of development is in essence one of time and data. In order to
construct adult-level intuitive theories, children require both suﬃcient time
to ponder and exposure to suﬃcient evidence. For a child on the verge of
grasping a new theory, either additional data or additional time to think can
make the diﬀerence (Carey, 2009). Measured as a function of either time
or amount of data experienced, the dynamics of learning typically follows
an arc from simpler theories that only coarsely predict or encode experi-
ence to more complex theories that more faithfully predict and encode it.
35

The above case studies of theory learning in the domains of taxonomy and
magnetism show this dynamic as a function of time elapsed in the search
process, for a ﬁxed data set. Previous Bayesian models of theory learning
(Kemp & Tenenbaum, 2009) have emphasized the complementary perspec-
tive: how increasing amounts of data naturally drive an ideal learner to more
complex but more predictive theories, independent of the dynamics of search
or inference.
These two sources of learning dynamics are most naturally understood
at diﬀerent levels of analysis. Data-driven learning dynamics seems best ex-
plained at the computational level, where the ideal learner shifts probability
mass between candidate theories as a function of the data observed. In con-
trast, time-driven dynamics (independent of the amount of data observed)
seems best approached at the algorithmic level, with models that emphasize
how the learner’s process of searching over a hypothesis space unfolds over
time independent of the pace with which data accumulates.
Our modeling approach is well suited to studying both data-driven and
time-driven dynamics and their interactions, because of its focus on the in-
terface between the computational and algorithmic levels of analysis. In the
rest of this section we return to the domain of simpliﬁed magnetism and ex-
plore the independent eﬀects and interactions of these two diﬀerent drivers
of theory change in our model. How does varying time and data aﬀect our
ideal learner? We provide the learner with several diﬀerent data sets, and
examine how the learning dynamics unfold over time for each one of these
sets. In each data set we provide the learner with diﬀerent observations by
parametrically varying the number of magnetic objects over ﬁve cases, which
can be ordered in the following way: Case 1 had 3 magnets, 1 magnetic
object and 6 non-magnetic objects. Each case then adds one magnetic ob-
ject while removing one non-magnetic object, so that case 2 has 3 magnets,
2 magnetic objects and 5 non-magnetic objects, up to case 5 which has 3
magnets, 5 magnetic objects and 2 non-magnetic objects (the same as the
previous section). We also considered a special case, case X, in which there
is only 1 magnet, 7 magnetic objects and 2 non-magnetic object. In all cases
the theory governing the domain is exactly the same as that described in the
magnetism case study. Given these diﬀerent cases we ﬁnd that at the end of
the simulation the learner almost always settled on one of three theories. We
therefore focus on these three theories, the formal laws of which are given in
Figure 8a. Informally, these theories correspond to:
Theory A: “There is one class of interacting objects in the world, and
36

objects in this class interact with other objects in this class.”
Theory B: “There are two classes of interacting objects in the world,
and objects from one class interact with objects in the other class. These
interactions are symmetric.”
Theory C: “There are two classes of interacting objects in the world, and
objects from one class interact with objects in the other class. Also, objects
in one of the classes interact with other objects in the same class. These
interactions are symmetric.”
It is important to emphasize that theories A, B and C were not given to
the learner as some sort of limited hypothesis space. Rather, the number of
possible theories the learner could consider in each case is potentially inﬁnite,
but practically it settles on one of these three or their logical equivalents.
Many other theories besides A, B and C were considered by the learner, but
they do not ﬁgure signiﬁcantly into the trajectory of learning. These theories
are much less good (i.e., unnecessarily complex or poorly ﬁtting) relative to
neighboring knowledge states, so they tend to be proposed and accepted only
in the early, more random stages of learning, and are quickly discarded. We
could not ﬁnd a way to group these other theories into cohesive or sensibly
interpreted classes, and since they are only transient states of the learner,
we removed them for purposes of analyzing learning curves and studied only
the remaining proportions, renormalized.
In order to see how the dynamics of learning depend on data, consider
speciﬁcally cases in which there are few magnetic objects that are not mag-
nets, perhaps 1 or 2 (as in cases 1 and 2). In this case a partial theory such
as theory A might suﬃce. According to this theory there is only one type
of interacting object, and one law. If there are two magnetic non-magnets
in the domain, the partial theory will classify them as ‘interacting’ objects
based on their behavior with the magnets, conﬂating them with the magnets.
However, it will incorrectly predict the two magnetic non-magnets should in-
teract with each other. Their failure to interact will be treated as an outlier
by the learner who has theory A. The full theory C can correctly predict this
non-interaction, but it does so by positing more laws and types of objects,
which has a lower prior probability. As the number of magnetic non-magnets
increases, the number of ’outliers’ in the data increase as well (see Figure
8b). Theory A now predicts more and more incorrect interactions, and in
a Kuhnian fashion there is a point at which these failures can no longer be
ignored, and a qualitative shift to a new theory is preferred. In a completely
diﬀerent scenario, such as the extreme case of only 1 magnet (case X), we
37

might expect the learner to not come up with magnet interactions laws, and
settle instead on theory B.
For each one of the outlined cases we ran 70 simulations for 1600 itera-
tions. Figure 8c shows the eﬀect of data and time on the learning process, by
displaying the relative proportion of the outlined theories at the end of the
iteration for all simulations. Note the transition from case 1 to case 5: With
a small number of non-magnet magnetic objects, the most frequently repre-
sented theory is theory A, which puts all magnetic objects (magnet or not)
into a single class and treats the lack of interactions between two magnetic
non-magnets as essentially noise. As the number of magnetic non-magnets
increases, the lack of interactions between the diﬀerent non-magnets can no
longer be ignored and the full theory becomes more represented. Case X
presents a special scenario in which there is only 1 magnet, and as expected
theory B is the most represented there.
The source of the diﬀerence be-
tween the proportion of theories learned in these diﬀerent cases is the data
the learner was exposed to. Within each case, the learner undergoes a pro-
cess of learning similar to that described in the case studies – adopting and
discarding theories in a process or time-driven manner.
To summarize, theory acquisition can be both data-driven and process-
driven. Our simulations suggest that, at least in this simpliﬁed domain, both
suﬃcient data and suﬃcient time to think are required. Only when the ob-
served data provide a strong enough signal – as measured here by potential
outliers under a simpler theory – is there suﬃcient inductive pressure for a
Bayesian learner guided by simplicity priors to posit a more complex the-
ory. Yet even with all the data in the world, a practical learning algorithm
still requires suﬃcient time to think, time to search through a challenging
combinatorial space of candidate laws and novel concepts and construct a
sequence of progressively higher scoring theories that will reliably converge
on the highest scoring theory for the domain.5 The fact that both suﬃcient
data and suﬃcient time are needed for proper theory learning ﬁts with the
potentially frustrating experience of many teachers and parents: having laid
out for a child all the data, all the input, that they need to solve a problem,
grasp some explanation, or make a discovery, the child still doesn’t seem to
get it, or takes surprisingly long to get it. Knowing that any realistic learner
5It should however be noted that in some cases, the time-component allows the learner
to ‘weed out’ and abandon overly complex theories.
38

needs both data enough, and time, may at least provide some relief from that
frustration and the patience to watch and wait as learning does its work.
6. Evidence from experiments with children
While our work here has been primarily motivated by theoretical con-
cerns, we also want to consider the empirical evidence that children’s learning
corresponds in some way to the computational picture we have developed.
Our most basic result is that a simple, cognitively plausible, stochastic search
algorithm, guided by an appropriate grammar and language for theories, is
capable of solving the rather sophisticated joint inference problem of learn-
ing both the concepts and the laws of a new theory – what we referred to
as the “hard problem” or the “chicken-and-egg” problem of theory learn-
ing. In the last few years, several lines of experimental work have shown
that children and adults can indeed solve this joint inference problem in the
course of acquiring new theories. Kemp et al. (2010) showed that adults were
able to learn new causal relations, such as objects of type A light up type B,
and to use these relations to categorize objects, for example object 3 is of
type A. In (Lucas et al., 2010) adults performed a task asking about speciﬁc
causal structures leading to evidence (which objects are ‘blickets‘ that cause
a ‘blicket-meter‘ to activate), which required inferring the abstract functional
form of the causal relations (do blickets activate the meter via a noisy-OR
function, a deterministic disjunctive function or a conjunctive function). A
similar experiment (Lucas & Griﬃths, 2010) demonstrated that children are
also able to acquire such abstract knowledge about the functional causal form
while considering the speciﬁc identity of objects.
While in these studies children were explicitly told that only one type
of concept is involved, Schulz and colleagues (Schulz et al., 2008) showed
that young children can solve an even more challenging task: Given sparse
evidence in the form of diﬀerent blocks touching and making diﬀerent noises,
the children correctly posited the existence of three diﬀerent causal kinds
underlying the observed relations. In this case the children had to both infer
the abstract relations governing the behavior, and posit how many concepts
underly these relations. These papers are qualitatively consistent with our
approach’s predictions.
In (Bonawitz et al., in press) we showed a more
quantitative correspondence between our model predictions and children’s
categorization judgements. In that study children were shown interactions in
a domain of simpliﬁed magnetism, where several unlabeled blocks interacted
39

Theory Proportion
Cases
X
1
2
3
4
5
1
Cases
1 metal, 3 magnets, 6 other
4
7 metal, 1 magnets, 2 other
5
5 metal, 3 magnets, 2 other
4 metal, 3 magnets, 3 other
3 metal, 3 magnets, 4 other
2
2 metal, 3 magnets, 5 other
X
3
Object distribution
interacts(X,Y)         f(X)     f(Y)
Rule 1:
interacts(X,Y)         f(X)     g(Y)
Rule 2:
interacts(X,Y)         interacts(Y,X)
Rule 3:
interacts(X,Y)         f(X)     g(Y)
Rule 1:
interacts(X,Y)         interacts(Y,X)
Rule 2:
interacts(X,Y)         f(X)     f(Y)
Rule 1:
Theory A
Theory B
Theory C
c.
a.
Theory A
b.
Objects
Objects
Interaction predicted,
observed
Interaction predicted,
unobserved
Iterations
50
400
800
1200
1600
B
Theory
C
Theory
1
2
3
4
5
Cases
Figure 8: Learning dynamics resulting from two diﬀerent sources: (a) A formal description
of theories A, B and C (b) The predicted and observed interactions given theory A for
the diﬀerent cases, showing the growing number of outliers as the number of magnetic
non-magnet objects grows (c) Proportion of theories accepted by the learner for diﬀerent
cases, during diﬀerent points in the simulation runs. More opaque bars correspond to later
iterations in the simulation. Diﬀerent theories are acquired as a result of varying time and
data.
40

with blue and yellow blocks, either attracting or repelling from them. We
also showed that the Monte Carlo search algorithm given here is capable of
ﬁnding just the theories that children do, or theories that are behaviorally
indistinguishable from them, and revising them appropriately.
Could models from an alternative paradigm such as connectionism also
explain these results?
Connectionist architectures could potentially solve
aspects of the tasks described in (Lucas et al., 2010), for example. There
are certainly networks capable of distinguishing between diﬀerent functional
forms like those in (Lucas et al., 2010), which may be seen as learning govern-
ing laws in a theory. Connectionist networks can also form new concepts - in
the sense of clusters of data that behave similarly - via competitive learning.
However, it has yet to be shown that a connectionist network can learn or
represent the kinds of abstract knowledge that our approach does, and that
children grasp in the other experiments cited above: solving the joint infer-
ence problem of discovering a system of new concepts and laws that together
explain a set of previously unexpected interactions or relations. This prob-
lem poses an intriguing open challenge for connectionist modelers in cognitive
development, one that could stimulate signiﬁcant new research.
Going forward, we would like more ﬁne-grained tests of whether and how
the Monte Carlo search learning mechanism we have posited corresponds to
the mechanisms by which children explore their space of theories. This will be
challenging, as most of the steps of learning are not directly observable. We
are currently working on studies together with Bonawitz and colleagues to
test some general predictions of our model, such as the tradeoﬀbetween data
and time described in the section above. In these experiments we recreate
the domain of simpliﬁed magnetism described in the case studies section,
with three types of objects that interact according to several laws.
The
children will be given diﬀerent amounts of evidence, and crucially diﬀerent
segments of time, after which they will be asked to sort the objects they see
into categories and describe why they do so. The children will not be told
in advance how many object types exist, and we anticipate the number of
types posited by the children will depend on their current domain theory. We
anticipate the same amount of evidence but varying lengths of time will lead
children to transition from one theory to the next, which will be evidenced
in their sorting behavior. This behavior will be matched with running the
stochastic search algorithm for varying amounts of time, as described in the
previous section, though we recognize these are still only indirect tests of the
model’s predictions.
41

More precision could come from microgenetic methods (Siegler & Crow-
ley, 1991), which study developmental change by giving children the same
task several times and inspecting the strategies used to solve the task at
many intervals. Microgenetic studies ﬁnd that often, while the task itself
remains constant, the strategies used to solve it undergo go. This data could
be interpreted as a search process unfolding over time. A fundamental ques-
tion for the microgenetic method remains why and how change occurs. Our
algorithmic approach oﬀers an explanation of how, and can potentially ad-
dress the why. Together with Bonawitz and colleagues we are developing
micro genetic methods to test whether children’s learning can be explained
in terms of Monte Carlo search.
One key challenge in designing a microgenetic study is deﬁning an ex-
ternally measurable sign of the internal cognitive mechanism of hypothesis
testing and discovery. Similar to how microgenetic studies keep a task ﬁxed,
we intend to observe how children play and experiment with a given set of
objects, without introduce new objects or any new data in the form of new
interactions that haven’t been observed before. As in classic microgenetic
studies, we intend to ask the children questions and encourage them to talk
out loud about their hypotheses in, order to probe the state of their search
at more abstract levels of the theory. We can score the theories they un-
cover using computational tools, and observe whether the pattern of theories
abandoned, adopted and uncovered ﬁts with Monte Carlo search.
7. Discussion and Conclusion
We have presented an algorithmic model of theory learning in a hierar-
chical Bayesian framework and explored its dynamics in several case studies.
We ﬁnd encouraging the successful course of acquisition for several exam-
ple theories, and the qualitative parallels with phenomena of human theory
acquisition. These results suggest that previous “ideal learning” analyses
of Bayesian theory acquisition can be approximately realized by a simple
stochastic search algorithm and thus are likely well within the cognitive grasp
of child learners. It is also encouraging to think that state-of-the-art Monte
Carlo methods used in Bayesian statistics and artiﬁcial intelligence to approx-
imate ideal solutions to inductive inference problems might also illuminate
the way that children learn. At this intersection point between the compu-
tational level and the algorithmic level of analysis, we showed that theory
change is expected to be both data-driven and process-driven. This is an
42

important theoretical distinction, but the psychological reality of these two
sources of learning dynamics and their interaction needs to be further studied
in experiments with children and adults.
While the main contributions of this paper are in addressing the algo-
rithmics of theory acquisition, the ’how’, the introduction of law templates
provides some insight regarding ‘what’ the structure of children’s knowledge
might be, and the coupling between how we answer ’what?’ and ’how?’ ques-
tions of learning. On an algorithmic level, we found such templates to be
crucial in allowing learning to converge on a reasonable timescale. On a com-
putational level, these templates can be seen as generalizing useful abstract
knowledge across domains, and providing high-level constraints that apply
across all domain theories. The formal framework section did not directly
treat where such templates come from, but it is possible to imagine that
some of them are built in as overarching constraints on knowledge. More
likely, though, they are themselves learned during the algorithmic acquisi-
tion process. An algorithmic grammar-based model can learn templates by
abstracting successful rules from their particular domain instantiation. That
is, if the model (or child) discovers a particularly useful rule involving a spe-
ciﬁc predicate such as “if is a(X,Y) and is a(Y,Z), then is a(X,Z)”, then the
speciﬁc predicate might be abstracted away to form the transitive template
“if F(X,Y) and F(Y,Z), then F(Y,Z)”. Learning this transitive template then
allows its reuse in subsequent theory, and represents a highly abstract level
of knowledge.
There are many ways in which our modeling work here can and should
be extended in future studies. The algorithm we have explored is only one
particular instance of a more general proposal for how stochastic search oper-
ating over a hierarchically structured hypothesis space can account for theory
acquisition. The speciﬁc theories considered here were only highly simpliﬁed
versions of the knowledge children have about real-world domains. Part of
the reason that actual concepts and theories are richer and more complex
is due to the fact that children have a much richer underlying language for
representations. Horn clauses are expressive and suitable for capturing some
knowledge structures, and in particular certain kinds of causal relations, but
they are not enough. A potentially more suitable theory space would be built
on a functional language, in which the laws are more similar to mathematical
equations. Such a space would be harder to search through, but it would be
much more expressive. A functional language of this sort would allow us
to explore rich theories described in children, such as basic notions about
43

objects and their interactions (Spelke, 1990), and the intuitive physics guide
(Baillargeon, 1994) object behavior. Despite the need for a more expressive
language, we expect the same basic phenomena found in the model domains
considered here to be replicated in more complex models. Moving forward, a
broader range of algorithmic approaches, stochastic as well as deterministic,
need to be evaluated as both as behavioral models and as eﬀective computa-
tional approximations to the theory search problem for larger domains.
Relative to previous Bayesian models of cognitive development that fo-
cused on only the computational level of analysis, this paper has emphasized
algorithmic-level implementations of a hierarchical Bayesian computational
theory, and the interplay between the computational and algorithmic levels.
We have not discussed at all the level of neural implementation, but recent
proposals by a number of authors argue that analogous stochastic-sampling
ideas could plausibly be used to carry out Bayesian learning in the brain
(Fiser et al., 2010). More generally, a “top-down” path to bridging levels
of explanation in the study of mind and brain, starting with higher, more
functional levels and moving down to lower, more mechanistic levels, ap-
pears most natural for Bayesian or other “reverse-engineering” approaches
to cognitive modeling (Griﬃths et al., 2010). Other paradigms for cognitive
modeling adopt diﬀerent ways to navigate the same hierarchy. Connection-
ist approaches, for instance, start from hypothesized constraints on neural
representations (e.g., distributed codes) and learning mechanisms (e.g., error-
driven learning) and move up from there, to see what higher-level phenom-
ena emerge (McClelland et al., 2010). While we agree that actual biological
mechanisms will ultimately be a central feature of any account of children’s
cognitive development, we are skeptical that this is the best place to start
(Griﬃths et al., 2010). The details of how the brain might represent or learn
knowledge such as the abstract theories we consider here remain largely un-
known, making a bottom-up emergent alternative to our approach hard to
contemplate. In contrast, while our top-down approach has yet to make con-
tact with neural phenomena, it has yielded real insights spanning levels. In
moving from computational-level accounts to algorithms that explicitly (if
approximately) implement the computational theory let us see plainly how
the basic representations of children’s theories could be acquired, and sug-
gest explanations for otherwise puzzling features of the dynamics of learning
in young children, as the consequences of eﬃcient and eﬀective algorithms
for approximating the rational computational-level ideal of Bayesian learn-
ing. We hope that as neuroscience learns more about the neural substrates
44

of symbolic representations and mechanisms of exploratory search, our top-
down approach can be meaningfully extended from the algorithmic level to
the level of implementation in the brain’s hardware.
Going back to the puzzle of the “chicken and egg” problem posed at the
beginning of the paper, what do the dynamics explored here tell us about
the coupled challenges of learning the laws of a theory and the invention
of truly novel concepts, and the opposing views represented by Fodor and
Carey? There is a sense in which, at the computational level, the learner al-
ready must begin the learning process with all the laws and concepts needed
to represent a theory already accessible. Otherwise the necessary hypothe-
sis spaces and probability distributions for Bayesian learning could not be
deﬁned. In this sense, Fodor’s skepticism on the prospects for learning or
constructing truly novel concepts is justiﬁed. Learning cannot really involve
the discovery of anything “new”, but merely the changing of one’s degree
of belief in a theory, transporting probability mass from one part of the hy-
pothesis space to another. However, on the algorithmic level explored in this
paper, the level of active processing for any real-world learner, there is in
fact genuine discovery of new concepts and laws. Our learning algorithm
can begin with no explicitly represented knowledge in a given domain – no
laws, no abstract concepts with any non-trivial extensions in the world – and
acquire reasonable theories comprised of novel laws and concepts that are
meaningfully grounded and predictively useful in that domain.
Our speciﬁc algorithm suggests the following account of how new concepts
derive their meanings. Initially, the concepts themselves are only blank pred-
icates. The theory prior induces a non-arbitrary structure on the space of
possible laws relating these predicates, and in that sense can be said to con-
tain a space of proto-meanings. The data are then fused with this structure
in the prior to create a structured posterior: the concepts are naturally ex-
tended over the observed objects in those regions where the posterior has
a high probability, and those are the areas in theory space that the learner
will converge towards. This algorithmic process is, we suggest, an instance
(albeit a very simple one) of Carey’s “bootstrapping” account (Carey, 2004,
2009) of conceptual change, and a concrete computational implementation
of concept learning under an inferential role semantics.
Under Carey’s account of the origins of new concepts, children ﬁrst use
symbols as placeholders for new concepts and learn the relations between
them that will support later inferential roles. Richer meaning is then ﬁlled in
on top of these placeholders and relations, using a “modeling process” involv-
45

ing a range of inductive inferences. The outer loop of our algorithm explains
the ﬁrst stage: why some symbolic structures are used rather than others
and how their relations are created. The second stage of Carey’s account
parallels the inner loop of our algorithm, which attempts to ﬁnd the likeliest
and sparsest assignment of the core predicates, once their interactions have
been ﬁxed by the proposed theory. During our algorithmic learning process,
new concepts may at times have only a vague meaning, especially when they
are ﬁrst proposed. Concepts that are fragmented can be uniﬁed, and con-
cepts that are lumped together may be usefully dissociated, as learners move
around theory space in ways similar to how new concepts are manipulated
in both children’s and scientists’ theory change (Carey, 2009).
Returning to the overarching idea of the child as scientist, it is interest-
ing to recall how from its inception, the study of the cognitive development
of children was heavily inﬂuenced by the philosophy of science. Many re-
searchers have found the metaphor of children as Lilliputian scientists useful
and enlightening, seeing children as testing hypotheses and building struc-
tured causal models of the world, and this idea has found an exact formula-
tion in an ideal Bayesian framework. However, neither children nor scientists
are ideal, and discovering the practical learning algorithms of children may
also lead us back to a better understanding of the process and dynamics of
science itself as a search process.
Despite our optimism, it is important to end by stressing that our models
at best only begin to capture some aspects of how children acquire their theo-
ries of the world. We agree very much with the view of Schulz (2012) that the
hardest aspects of the problem are as yet unaddressed by any computational
account, that there are key senses in which children’s learning is a kind of
exploration much more intelligent and sophisticated than even a smart ran-
domized search such as our grammar-based MCMC. How could our learning
algorithms account for children’s sense of curiosity, knowing when and where
to look for new evidence? How do children come up with the proper interven-
tions to unconfound concepts or properties? How can a learning algorithm
know when it is on the right track, so to speak, or distinguish good bad ideas
from bad bad ideas, which children seem able to do? How do pedagogy and
learning from others interact with interact with internal search dynamics -
are the ideas being taught simply accepted, or do they form the seed of a
new search? How can algorithmic models go beyond the given evidence and
actively explore, in the way children search for new data when appropriate?
There is still much toil left – much rewarding toil, we hope – until we can say
46

reasonably that we have found a model of children’s learning, and believe it.
8. Acknowledgments
We wish to thank Laura Schulz, Liz Bonawitz, Alison Gopnik, Rebecca
Saxe, Henry Wellman and Yarden Katz for helpful discussions. This work
was funded by grants from the McDonnell Causal Learning Collaborative,
ONR (N00014-09-0124), ARO (W911NF-08-1-0242) and an NSF Graduate
Fellowship to the ﬁrst author.
Appendix A. Taxonomy data
For the simulations described in Section 4.1 we used 7 objects (animal,
bird, ﬁsh, canary, eagle, shark, salmon), and 7 properties (breathes, can ﬂy,
can swim, can sing, has claws, can bite, is pink). The core relations were set
up as in (Katz et al., 2008). and used to generate the full set of true facts as
the observable data:
has a(animal,breathes), has a(bird,breathes), has a(ﬁsh,breathes),
has a(canary,breathes), has a(eagle,breathes), has a(shark,breathes),
has a(salmon,breathes), has a(bird,can ﬂy), has a(canary,can ﬂy),
has a(eagle,can ﬂy), has a(ﬁsh,can swim), has a(shark,can swim),
has a(salmon,can swim), has a(canary,can sing), has a(eagle,has claws),
has a(shark,can bite), has a(salmon,is pink),
is a(animal,animal), is a(bird,animal), is a(ﬁsh,animal),
is a(bird,bird), is a(ﬁsh,ﬁsh), is a(canary,bird),
is a(canary,animal), is a(eagle,bird), is a(eagle,animal),
is a(shark,ﬁsh), is a(shark,animal), is a(salmon,ﬁsh),
is a(salmon,animal), is a(canary,canary), is a(eagle,eagle),
is a(shark,shark), is a(salmon,salmon).
Appendix B. Simpliﬁed magnetism data
For the simulations described in Section 4.2 we used 10 objects: 3 magnets
(objects 1-3), 5 magnetic objects (4-8) and two non-magnetic objects (9-10).
The rules described in Section 4.2 were then used to generate an interaction
matrix as the observable data:
Objects 1, 2, 3 each interact with objects 1, 2, 3, 4, 5, 6, 7, 8.
Objects 4, 5, 6, 7, 8 each interact with objects 1,2,3.
Objects 9, 10 do not interact with any other object.
47

References
Baillargeon, R. (1994). How Do Infants Learn About the Physical World?
Current Directions in Psychological Science, 3, 133–140.
Bishop, C. M. (2006). Pattern recognition and machine learning. (1st ed.).
Springer.
Block, N. (1986). Advertisement for a Semantics for Psychology. Midwest
Studies in Philosophy, 10, 615–678.
Bonawitz, E., Gopnik, A., Denison, S., & Griﬃths, T. (in press).
Ra-
tional randomness: The role of sampling in an algorithmic account of
preschooler’s causal learning. In F. Xu, & T. Kushnir (Eds.), Rational
Constructivism. Elsevier.
Bradshaw, G., Langley, P., & Simon, H. (1983). Studying scientiﬁc discovery
by computer simulation. Science, 22, 971 – 975.
Bruner, J. S., Goodnow, J. J., & Austin, G. A. (1956). A study of thinking.
New York: Wiley.
Carey, S. (1985). Conceptual change in childhood. Cambridge, MA: MIT
Press/Bradford Books.
Carey, S. (2004). Bootstrapping and the origin of concepts. Daedalus, 133,
59–68.
Carey, S. (2009). The Origin of Concepts. Oxford University Press.
Collins, A., & Quillian, M. (1969). Retrieval time from semantic memory.
Journal of Verbal Learning and Verbal Behavior, 8, 240–247.
Feldman, J. (2006).
An algebra of human concept learning.
Journal of
Mathematical Psychology, 50, 339–368.
Feldman, J. A., Ps, J. G. I., Horning, J. J., Reder, S., Feldman, J. A.,
Gips, J., Horning, J. J., & Reder, S. (1969). Grammatical complexity and
inference. Technical Report Stanford University.
Field, H. (1977). Logic, Meaning, and Conceptual Role. Journal of Philoso-
phy, 74, 379–409.
48

Fiser, J., Berkes, P., Orbn, G., & Lengye, M. (2010). Statistically optimal
perception and learning: from behavior to neural representations. Trends
in Cognitive Sciences, 14, 119 – 130.
Fodor, J., & Lepore, E. (1991). Why meaning (probably) isn’t conceptual
role. Mind & language, 6, 328–343.
Fodor, J. A. (1975). The language of thought. Harvard University Press:
Cambridge, MA.
Fodor, J. A. (1980). On the impossibility of acquiring ’more powerful’ struc-
tures. In Language and Learning: The Debate Between Jean Piaget and
Noam Chomsky. Harvard University Press.
Fodor, J. A., & Pylyshyn, Z. W. (1988). Connectionism and cognitive archi-
tecture: A critical analysis. Cognition, 28, 3–71.
Gershman, S., Vul, E., & Tenenbaum, J. B. (2009). Perceptual multistability
as Markov Chain Monte Carlo inference. Advances in Neural Information
Processing Systems, 22, 611 – 619.
Gilks, W., & Spiegelhalter, D. (1996). Markov chain Monte Carlo in practice.
Chapman & Hall/CRC.
Goodman, N. D., Mansinghka, V. K., Roy, D. M., Bonawitz, K., & Tenen-
baum, J. B. (2008a). Church: a language for generative models. Uncer-
tainty in Artiﬁcial Intelligence, .
Goodman, N. D., Tenenbaum, J. B., Feldman, J., & Griﬃths, T. L. (2008b).
A rational analysis of rule-based concept learning. Cognitive Science, 32,
108—154.
Goodman, N. D., Ullman, T. D., & Tenenbaum, J. B. (2011). Learning a
theory of causality. Psychological Review, 118, 110—119.
Gopnik, A., & Meltzoﬀ, A. N. (1997).
Words, Thoughts, and Theories.
Cambridge, MA: MIT Press.
Griﬃths, T. L., Chater, N., Kemp, C., Perfors, A., & Tenenbaum, J. B.
(2010). Probabilistic models of cognition: exploring representations and
inductive biases. Trends in Cognitive Sciences, 14, 357 – 364.
49

Harman, G. (1975). Meaning and semantics. In M. Munitz, & P. Unger
(Eds.), Semantics and Philosophy. New York: New York University Press.
Harman, G. (1982).
Conceptual role semantics.
Notre Dame Journal of
Formal Logic, 23, 242–257.
Katz, Y., Goodman, N. D., Kersting, K., Kemp, C., & Tenenbaum, J. B.
(2008). Modeling semantic cognition as logical dimensionality reduction.
Proceedings of the Thirtieth Annual Conference of the Cognitive Science
Society, .
Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2007). Learning causal
schemata. In Proceedings of the Twenty-ninth Annual Meeting of the Cog-
nitive Science Society.
Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2008a). Learning and using
relational theories. Advances in Neural Information Processing Systems,
20, 753–760.
Kemp, C., Goodman, N. D., & Tenenbaum, J. B. (2008b). Theory acquisition
and the language of thought. In Proceedings of Thirtieth Annual Meeting
of the Cognitive Science Society.
Kemp, C., & Tenenbaum, J. B. (2009).
Structured statistical models of
inductive reasoning. Psychological Review, 116, 20 – 58.
Kemp, C., Tenenbaum, J. B., Niyogi, S., & Griﬃths, T. L. (2010). A proba-
bilistic model of theory formation. Cognition, 114, 165 – 196.
Kirkpatrick, S., Gelatt, C. D., & Vecchi, M. P. (1983).
Optimization by
simulated annealing. Science, 220, 671–680.
Lucas, C. G., Gopnik, A., & Griﬃths, T. L. (2010). Learning the form of
causal relationships using hierarchical bayesian models. In Proceedings of
the 32nd Annual Conference of the Cognitive Science Society.
Lucas, C. G., & Griﬃths, T. L. (2010). Developmental diﬀerences in learning
the forms of causal relationships. Cognitive Science, 34, 113–147.
Marr, D. (1982). Vision. Freeman Publishers.
50

McClelland, J. L., Botvinick, M. M., Noelle, D. C., Plaut, D. C., Rogers,
T. T., Seidenberg, M. S., & Smith, L. B. (2010). Letting structure emerge:
connectionist and dynamical systems approaches to cognition. Trends in
Cognitive Sciences, 14, 348 – 356.
Mcclelland, J. L., & Rumelhart, D. E. (Eds.) (1986). Parallel Distributed
Processing: Explorations in the Microstructure of Cognition, Volume 2:
Psychological and Biological Models. Cambridge, MA: MIT Press.
Mitchell, T. (1982). Generalization as search. Artiﬁcial Intelligence, (pp.
203–226).
Moreno-Bote, R., Knill, D., & Pouget, A. (2011).
Bayesian sampling in
visual perception. Proceedings of the National Academy of Sciences, 108,
12491–12496.
Murphy, G. L., & Medin, D. L. (1985). The role of theories in conceptual
coherence. Psychol Rev, 92, 289–316.
Newell, A., & Simon, H. (1976).
Computer science as empirical inquiry:
Symbols and search. Communications of the ACM , 19, 113–126.
Pearl, J. (2000). Causality: models, reasoning, and inference. Cambridge
University Press.
Rogers, T. T., & McClelland, J. L. (2004). Semantic cognition: A parallel
distributed processing approach. Cambridge, MA: MIT Press.
Russell, S., & Norvig, P. (2009). Artiﬁcial Intelligence: a modern approach.
(3rd ed.). Prentice Hall.
Schulz, L. E. (2012). Finding new facts; thinking new thoughts. In Rational
Constructivism, Advances in Child Development and Behavior. Elsevier
volume 42.
Schulz, L. E. (In press).
The origins of inquiry: Inductive inference and
exploration in early childhood. Trends in Cognitive Science, .
Schulz, L. E., Goodman, N. D., Tenenbaum, J. B., & Jenkins, A. C. (2008).
Going beyond the evidence: abstract laws and preschoolers’ responses to
anomalous data. Cognition, 109, 211–223.
51

Shultz, T. R. (2003). Cognitive Developmental Psychology. Cambridge, MA,
USA: MIT Press.
Siegler, R., & Crowley, K. (1991). The micro genetic method. American
Psychologist, 46, 606–620.
Siegler, R. S., & Chen, Z. (1998). Developmental diﬀerences in rule learning:
A microgenetic analysis. Cognitive Psychology, 36, 273–310.
Smith, E., & Medin, D. (1981). Categories and Concepts. Cambridge, MA:
Harvard University Press.
Spall, J. C. (2003). Introduction to stochastic search and optimization: Esti-
mation, simulation, and control. John Wiley and Sons.
Spelke, E. (1990). Principles of object perception. Cognitive Science, 14,
29–56.
Spirtes, P., Glymour, C., & Scheines, R. (2001). Causation, Prediction, and
Search. (2nd ed.). Cambridge, MA, USA: MIT Press.
Sundareswara, R., & Schrater, P. R. (2008). Perceptual multistability pre-
dicted by search model for bayesian decisions. Journal of Vision, 8.
Tenenbaum, J. B., & Griﬃths, T. L. (2001). The rational basis of represen-
tativeness. In Proceedings of the 23rd Annual Conference of the Cognitive
Science Society (pp. 1036–1041).
Tenenbaum, J. B., Griﬃths, T. L., & Kemp, C. (2006).
Theory-based
bayesian models of inductive learning and reasoning. Trends in Cogni-
tive Sciences, 10, 309–318.
Tenenbaum, J. B., Griﬃths, T. L., & Niyogi, S. (2007). Intuitive theories as
grammars for causal inference. In A. Gopnik, & L. Schulz (Eds.), Causal
learning: Psychology, philosophy, and computation. Oxford: Oxford Uni-
versity Press.
Tenenbaum, J. B., Kemp, C., Griﬃths, T. L., & Goodman, N. D. (2011).
How to grow a mind: Statistics, structure, and abstraction. Science, 331,
1279–1285.
52

Wellman, H. M., Fang, F., & Peterson, C. C. (2011). Sequential Progressions
in a Theory-of-Mind Scale: Longitudinal Perspectives. Child Development,
82, 780–792.
Wellman, H. M., & Gelman, S. A. (1992). Cognitive development: Founda-
tional theories of core domains. Annual Review of Psychology, 43, 337–375.
Wellman, H. M., & Woolley, J. D. (1990). From simple desires to ordinary
beliefs: The early development of everyday psychology. Cognition, 35,
245–275.
53

