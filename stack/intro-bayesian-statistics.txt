
Introduction to 
Bayesian Statistics 

m I C I N T C N N I A L  
THE W l L E Y  BICENTENNIAL-KNOWLEDGE FOR GENERATIONS 
ach generation has its unique needs and aspirations. When Charles Wiley first 
opened his small printing shop in lower Manhattan in 1807, it was a generation 
of boundless potential searching for an identity. And we were there, helping to 
define a new American literary tradition. Over half a century later, in the midst 
of the Second Indusrrial Revolution, it was a generation focused on building the 
future. Once again, we were there, supplying the critical scientific, technical, and 
engineering knowledge that helped frame the world. Throughout the 20th 
Century, and into the new millennium. nations began to reach out beyond their 
own bordcrs and a ncw intcrnational community was born. Wiley was there, 
expanding its operations around the world to enable a global exchange of ideas, 
opinions, and know-how. 
G 
For 200 years, Wiley has been an integral part of each generation’s journey, 
enabling the flow of infomiation and understanding necessary to meet their needs 
and fulfill their aspirations. Today, bold ncw technologics are changing the way 
wc live and learn. Wiley will be thcrc, providing you thc must-havc knowlcdgc 
you need to imagine new worlds, new possibilities, and new opportunities. 
Generations come and go, but you can always count on Wiley to provide you the 
knowledge you need, when and where you need it! 
4 
WILLIAM J. PESCE 
PETER BOOTH W l L E Y  
PRESIDENT AND CHIEF EXECMVE OWCER 
CHAIRMAN OF THE BOARD 

Introduction to 
Bayesian Statistics 
Second Edition 
William M. Bolstad 
University of Waihto 
Department of Statistics 
Hamilton, New Zealand 
B I C E N T E N N I A L  
B I C E N T E N N I A L  
WILEY-INTERSCIENCE 
A John Wiley & Sons, Inc., Publication 

Copyright 0 
2007 by John Wiley & Sons, Inc. All rights reserved 
Published by John Wiley & Sons, Inc., Hoboken, New Jersey. 
Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form 
or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 750-4470, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 11 1 River Street, Hoboken, NJ 
07030, (201) 748-601 1, fax (201) 748-6008, or online at http://www.wiley.com/go/permission. 
Limit of LiabilityDisclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representations or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of profit or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages. 
For general information on our other products and services or for technical support, please contact our 
Customer Care Department within the United States at (800) 762-2974, outside the United States at 
(317) 572-3993 or fax (317) 572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may 
not be available in electronic format. For information about Wiley products, visit our web site at 
www.wiley.com. 
Wiley Bicentennial Logo: Richard J. Pacific0 
Library of Congress Cataloging-in-Publication Data: 
Bolstad, William M., 1943- 
Introduction to Bayesian statistics / William M. Bolstad. - 
2nd ed. 
Includes bibliographical references and index. 
ISBN 978-0-470-141 15-1 (cloth) 
1. Bayesian statistical decision theory. I. Title. 
QA279.5.B65 2007 
5 19.5'42-dc22 
p. cm. 
2007013686 
Printed in the United States of America. 
1 0 9 8 7 6 5 4 3 2 1  

This book is dedicated to 
Sy hie, 
Ben, Rachel, 
Mary, and Elizabeth 

This Page Intentionally Left Blank

Contents 
Preface 
Preface to First Edition 
1 
Introduction to Statistical Science 
1. I 
1.2 
I .  3 
1.4 
The ScientiJic Method: A Process for Learning 
The Role of Statistics in the ScientiJic Method 
Main Approaches to Statistics 
Purpose and Organization of This Text 
2 
Scientific Data Gathering 
2. I 
2.2 
Sampling from a Real Population 
Observational Studies and Designed Experiments 
Monte Carlo Exercises 
3 
Displaying and Summarizing Data 
3.1 
3.2 
Graphically Comparing Two Samples 
3.3 
Measures of Location 
Graphically Displaying a Single Variable 
xiii 
XiX 
13 
14 
17 
22 
29 
29 
37 
39 
vii 

viii 
CONTENTS 
3.4 
Measures of Spread 
3.5 
3.6 
Displaying Relationships Between Two or More 
Variables 
Measures of Association for Two or More Variables 
Exercises 
4 
Logic, Probability, and Uncertainty 
4. I 
4.2 
4.3 
4.4 
4.5 
4.6 
4.7 
4.8 
4.9 
Deductive Logic and Plausible Reasoning 
Probability 
Axioms of Probability 
Joint Probability and Independent Events 
Conditional Probability 
Bayes ’ Theorem 
Assigning Probabilities 
Odds Ratios and Bayes Factor 
Beat the Dealer 
Exercises 
5 
Discrete Random Variables 
5.1 
Discrete Random Variables 
5.2 
5.3 
Binomial Distribution 
5.4 
Hypergeometric Distribution 
5.5 
Poisson Distribution 
5.6 
Joint Random Variables 
5.7 
Probability Distribution of a Discrete Random 
Variable 
Conditional Probability for Joint Random Variables 
Exercises 
6 
Bayesian Inference for Discrete Random Variables 
6. I 
6.2 
6.3 
6.4 
Two Equivalent Ways of Using Bayes ’ Theorem 
Bayes ’ Theorem for Binomial with Discrete Prior 
Important Consequences of Bayes ’ Theorem 
Bayes ’ Theorem for Poisson with Discrete Prior 
42 
44 
46 
50 
55 
56 
58 
59 
60 
62 
63 
68 
69 
70 
74 
77 
78 
80 
83 
85 
86 
89 
92 
97 
101 
106 
108 
I I I  
112 

CONTENTS 
iX 
Exercises 
Computer Exercises 
i13 
117 
7 
Continuous Random Variables 
121 
7.1 
Probability Density Function 
i23 
7.2 
Some Continuous Distributions 
126 
7.3 
Joint Continuous Random Variables 
134 
7.4 
Joint Continuous and Discrete Random Variables 
135 
Exercises 
138 
8 Bayesian Inference for Binomial Proportion 
8. I 
8.2 
Using a Beta Prior 
8.3 
Choosing Your Prior 
8.4 
Summarizing the Posterior Distribution 
8.5 
Estimating the Proportion 
8.6 
Bayesian Credible Interval 
Using a Uniform Prior 
Exercises 
Computer Exercises 
9 
Comparing Bayesian and Frequentist Inferences for Proportion 
9. I 
Frequentist Interpretation of Probability and 
Parameters 
9.2 
Point Estimation 
9.3 
Comparing Estimators for Proportion 
9.4 
Interval Estimation 
9.5 
Hypothesis Testing 
9.6 
Testing a One-sided Hypothesis 
9.7 
Testing a Two-sided Hypothesis 
Exercises 
Monte Carlo Exercises 
10 Bayesian Inference for Poisson 
10. I Some Prior Distributions for Poisson 
141 
142 
143 
145 
149 
152 
153 
155 
158 
161 
161 
163 
165 
167 
169 
170 
173 
178 
180 
183 
184 

x 
CONTENTS 
10.2 Inference for Poisson Parameter 
Exercises 
Computer Exercises 
11 Bayesian Inference for Normal Mean 
11. I Bayes ’ Theorem for Normal Mean with a Discrete 
Prior 
11.2 Bayes ’ Theorem for Normal Mean with a Continuous 
Prior 
11.3 Choosing Your Normal Prior 
11.4 Bayesian Credible Interval for Normal Mean 
1 I .  5 Predictive Density for Next Observation 
Exercises 
Computer Exercises 
12 Comparing Bayesian and Frequentist Inferences for Mean 
12.1 Comparing Frequentist and Bayesian Point Estimators 
12.2 Comparing Confidence and Credible Intervals for 
Mean 
12.3 Testing a One-sided Hypothesis about a Normal Mean 
12.4 Testing a Two-sided Hypothesis about a Normal Mean 
Exercises 
13 Bayesian Inference for Difference Between Means 
13.1 Independent Random Samples from Two Normal 
Distributions 
13.2 Case I :  Equal Variances 
13.3 Case 2: Unequal Variances 
13.4 Bayesian Inference for Diference Between Two 
Proportions Using Normal Approximation 
13.5 Normal Random Samples from Paired Experiments 
Exercises 
14 Bayesian Inference for Simple Linear Regression 
14. 1 Least Squares Regression 
190 
i96 
197 
199 
199 
205 
209 
21 I 
214 
21 6 
220 
223 
223 
226 
228 
232 
236 
239 
240 
240 
245 
248 
250 
254 
267 
268 

CONTENTS 
xi 
14.2 Exponential Growth Model 
14.3 Simple Linear Regression Assumptions 
14.4 Bayes ’ Theorem for the Regression Model 
14.5 Predictive Distribution for Future Observation 
Exercises 
Computer Exercises 
15 Bayesian Inference for Standard Deviation 
15.1 Bayes ’ Theorem for Normal Variance with a 
Continuous Prior 
15.2 Some Specific Prior Distributions and the Resulting 
Posteriors 
15.3 Bayesian Inference for Normal Standard Deviation 
Exercises 
Computer Exercises 
16 Robust Bayesian Methods 
16. I Eflect of MisspeciJed Prior 
16.2 Bayes ’ Theorem with Mixture Priors 
Exercises 
Computer Exercises 
A Introduction to Calculus 
B Use of Statistical Tables 
C Using the Included Minitab Macros 
D Using the Included R Functions 
E Answers to Selected Exercises 
References 
2 72 
2 73 
2 76 
281 
286 
294 
29 7 
298 
300 
3 08 
312 
315 
31 7 
318 
319 
329 
330 
333 
353 
3 71 
38 7 
405 
43 1 
Topic Index 
433 

This Page Intentionally Left Blank

Preface 
My original goal for this book was to introduce Bayesian statistics at the earliest 
possible stage to students with a reasonable mathematical background. This entailed 
coverage of a similar range of topics as an introductory statistics text, but from a 
Bayesian perspective. The emphasis is on statistical inference. I wanted to show how 
Bayesian methods can be used for inference and how they compare favorably with 
the frequentist alternatives. This book is meant to be a good place to start the study 
of Bayesian statistics. From the many positive comments I have had from users, I 
think the book succeeded in its goal. 
I also found that many users were taking up the book at a more intermediate level 
instead of the introductory level I had envisaged. The topics covered in Chapters 
2 and 3 would be old-hat for these users, so I would have to include some more 
advanced material to cater for the needs of that group. The second edition is my 
attempt to meet this new goal as well as the original goal. 
Changes in the Second Edition 
The Second Edition contains two new chapters as well as reordering, along with some 
minor rewriting of the existing chapters. The new chapters added include Chapter 
10, Bayesian inference for Poisson observations (added after the chapter comparing 
Bayesian and frequentist inference for binomial), and Chapter 15, Bayesian inference 
for the standard deviation for normal observations with known mean (added after the 
chapter on simple linear regression). Chapters 10-14 in the previous edition have 
been renumbered to become Chapters 1 1-13 and Chapter 16 in the Second Edition. 
xiii 

XiV 
PREFACE 
I have included some new exercises and new computer exercises which use new 
Minitab macros and R-functions. These can be downloaded from the book website: 
www.stats.waikato.ac.nzlpublicationsibolstadi1ntroductionToBayesianStatisticsl. 
My Perspective on Bayesian Statistics 
A book can be characterized as much by what is left out as by what is included. Like 
a sculptor facing a block of marble, the author must see a coherent view of the subject 
and take out the details that would interfere with his or her view. This book is my 
attempt to show a coherent view of Bayesian statistics as a good way to do statistical 
inference. In order to keep the focus on what I wanted, details that are outside the 
scope of the text are included in footnotes. The choice of topics included is mine 
alone. Here are some of my reasons behind my choice. 
I did not mention decision theory or loss functions when discussing Bayesian 
statistics. In many books, Bayesian statistics gets compartmentalized into decision 
theory while inference is presented in the frequentist manner. While decision theory 
is a very interesting topic in its own right, I want to present the case for Bayesian 
statistical inference, and did not want to get side-tracked. 
I think to get full benefit of Bayesian statistics, one really has to consider all priors 
subjective. They are either (1) a summary ofwhat you believe or (2) a summary of all 
you allow yourself to believe initially. I consider the subjective prior as the relative 
weights I give to each possible parameter value, before looking at the data. Even if 
I use a flat prior to give all possible values equal prior weight, it is subjective since I 
chose it. In any case, it gives all values equal weight only in that parameterization, 
so it can be considered ”objective“ only in that parameterization. In my book I do 
not wish to dwell on the problems associated with trying to be objective in Bayesian 
statistics. I explain why universal objectivity is not possible (in a footnote since I 
don’t want to distract the reader). I want to leave h i d e r  with the “relative weight” 
idea of the prior in the parameterization they have the problem in. 
In the first edition I did not mention Jeffreys’ prior explicitly, although the 
beta( 4, 4) prior for binomial and flat prior for normal mean are in fact the Jef- 
freys’ prior for those respective observation distributions. In the second edition I do 
mention Jeffreys’ prior for Poisson and normal standard deviation, so I mentioned it 
for binomial and normal mean as well. I give a brief explanation (in a footnote) and 
references. In particular, I don’t want to get the reader involved with the problems 
about Jeffreys’ prior, such as for mean and variance together, as opposed to indepen- 
dent Jeffreys’ priors, or the Jeffreys’ prior violating the likelihood principal. These 
are beyond the level I wish to go. I just want the reader to note the Jeffreys’ prior 
in these cases as possible priors, the relative weights they give, when they may be 
appropriate, and how to use them. Mathematically, all parameterizations are equally 
valid; however, usually only the main one is very meaningful. I want the reader 
to focus on relative weights for their parameterization as the prior. It should be (a) 
a summary of their prior belief (conjugate prior matching their prior beliefs about 
moments or median), (b) flat (hence objective) for their parameterization, or (c) some 
other form that gives reasonable weight over the whole range of possible values. The 

PREFACE 
XV 
posteriors will be similar for all priors that have reasonable weight over the whole 
range of possible values. 
The Bayesian inference on the standard deviation of the normal was done where 
the mean is considered a known parameter. The conjugate prior for the variance is 
the inverse chi-squared distribution. Our intuition is about the standard deviation, yet 
we are doing Bayes’ theorem on the variance. This required introducing the change 
of variable formula for the prior density. 
Considering the mean as known avoided the mathematically more advanced case 
where both mean and standard deviation are unknown. The Student’s t is presented 
as the required adjustment to credible intervals for the mean when the variance is 
estimated from the data. In fact, this would be the result when the joint posterior 
found, and the variance marginalized out. My other reason was that the Gibbs sampler 
would cycle back and forth, drawing the mean from its distribution given the data 
and the known variance, and then drawing the variance from its distribution given 
the data and the known mean. However, the Gibbs sampler and other Markov chain 
Monte Carlo methods are not mentioned explicitly in this book. They will have to 
wait for a sequel. 
Outline of an Intermediate Level Course Based on This Text 
This is a suggested 36 one-hour lecture course using this text for students who have 
already completed an introductory statistics course. 
0 Chapter 1 (one lecture). This gives an introduction to the course. 
0 Chapter 2 and Chapter 3 (no lectures). Students should read them as a review. 
0 Chapter 4 (two lectures). This introduces rules of probability including Bayes’ 
theorem. 
0 Chapter 5 (two lectures). This introduces discrete random variables and their 
probability functions including binomial, hypergeometric, and Poisson. 
0 Chapter 6 (three lectures). Bayes’ theorem for discrete random variable with 
discrete prior. 
0 Chapter 7 (two lectures). Continuous random variables and their probability 
density functions, including uniform, beta, gamma, and normal. 
0 Chapter 8 (three lectures). Bayesian inference on binomial proportion, using 
either uniform, beta, or general continuous prior. Includes section on how to 
choose your prior. 
0 Chapter 9 (three lectures). Comparing Bayesian inference for proportion with 
the corresponding frequentist inferences. 
0 Chapter 10 (three lectures). Bayesian inference for Poisson using either posi- 
tive uniform, Jeffreys’, or gamma prior. 

xvi 
PREFACE 
0 Chapter 11 (four lectures). Bayesian inference for normal mean with known 
standard deviation for discrete and continuous priors (flat, normal, and general 
continuous). Emphasis on how to choose prior. Student’s t introduced as 
adjustment required when the standard deviation is not known and estimate 
from sample is used. The predictive distribution of next observation is found 
as an example of marginalization. 
0 Chapter 12 (one lecture). Comparing Bayesian inference for normal mean with 
corresponding frequentist inferences. 
0 Chapter 13 (three lectures). Bayesian inference for difference between normal 
means and difference between two binomial proportions using the normal 
approximation. 
0 Chapter 14 (three lectures). Bayesian inference for simple linear regression 
model. Includes section developing the predictive distribution of the next 
observation for a given z. 
0 Chapter 15 (three lectures). Bayesian inference for normal standard deviation, 
when the mean is known. 
0 Chapter 16 (three lectures). Robust Bayesian inference using mixture priors. 
This protects against misspecified priors, which is one of the main reasons 
many people are reluctant to use Bayesian methods. 
An Introductory Level Course Based on This Text. 
My original aim was to have an introductory course, covering the same material as a 
standard introductory statistics course, but from a Bayesian perspective. This remains 
possible following the outline in the Preface for the First Edition. Allowing for the 
renumbering of the chapters, a course aimed at this level would include Chapters 1-9, 
and Chapters 10-1 4. This covers gathering and presenting data, probability including 
Bayes’ theorem, discrete and continuous random variables, statistical inference for 
binomial parameter, statistical inference for normal mean, Bayesian inference for 
difference between normal means and for difference between binomial proportions, 
and Bayesian inference for the simple linear regression model. 
Acknowledgments 
I would like to thank Dr. James Curran, who has written the R macros and Appendix 
D. I would also like to thank Dr. Paul Alper who has bounced ideas to me for the 
new chapters, Jason McClintic who has also helped check the Minitab macros and 
R-functions, and my colleague Dr. Judi McWhirter has gone through the book and 
helped find some inconsistencies. I would like to thank all the readers who have sent 
me comments and pointed out misprints in the first edition. I have corrected all of 
them that have been noted. I would like to thank Cathy Akritas at Minitab for her 

PREFACE 
XVii 
help in improving my Minitab macros. I would like to thank Steve Quigley, Jackie 
Palmieri, Kristen Parrish, and the team at John Wiley & Sons for their support, as 
well as Amy Hendrickson of T'xnology, Inc. for help with LaTex. 
Finally, last but not least, I wish to thank my wife Sylvie for her constant love and 
support. 
WILLIAM 
M. "BILL" BOLSTAD 
Hamilton, New Zealand 

This Page Intentionally Left Blank

Preface to the First Edition 
How This Text Was Developed 
This text grew out of the course notes for an Introduction to Bayesian Statistics 
course that I have been teaching at the University of Waikato for the past few years. 
My goal in developing this course was to introduce Bayesian methods at the earliest 
possible stage and to cover a similar range of topics as a traditional introductory 
statistics course. There is currently an upsurge in using Bayesian methods in applied 
statistical analysis, yet the Introduction to Statistics course most students take is 
almost always taught from a frequentist perspective. In my view, this is not right. 
Students with a reasonable mathematics background should be exposed to Bayesian 
methods from the beginning, because that is the direction applied statistics is moving. 
Mathematical Background Required 
Bayesian statistics uses the rules of probability to make inferences, so students must 
have good algebraic skills for recognizing and manipulating formulas. A general 
knowledge of calculus would be an advantage in reading this book. In particular, 
the student should understand that the area under a curve is found by integration and 
that the location of a maximum or a minimum of a continuous differentiable function 
is found by setting the derivative function equal to zero and solving. The book is 
self-contained with a calculus appendix students can refer to. However, the actual 
calculus used is minimal. 
xix 

xx 
PREFACE TO FlRS J EDlTlON 
Features of the Text 
In this text I have introduced Bayesian methods using a step-by-step development 
from conditional probability. In Chapter 4, the universe of an experiment is set up with 
two dimensions, the horizontal dimension is observable, and the vertical dimension 
is unobservable. Unconditional probabilities are found for each point in the universe 
using the multiplication rule and the prior probabilities of the unobservable events. 
Conditional probability is the probability on that part of the universe that occurred, the 
reduced universe. It is found by dividing the unconditional probability by their sum 
over all the possible unobservable events. Because of way the universe is organized, 
this summing is down the column in the reduced universe. The division scales them up 
so the conditional probabilities sum to one. This result, known as Bayes ’ theorem, 
is the key to this course. In Chapter 6 this pattern is repeated with the Bayesian 
universe. The horizontal dimension is the sample space, the set of all possible values 
of the observable random variable. The vertical dimension is the parameter space, 
the set of all possible values of the unobservable parameter. The reduced universe 
is the vertical slice that we observed. The conditional probabilities, given what 
we observed, are the unconditional probabilities found by using the multiplication 
rule (prior x likelihood) divided by their sum over all possible parameter values. 
Again, this sum is taken down the column. The division rescales the probabilities 
so they sum to one. This gives Bayes’ theorem for a discrete parameter and a 
discrete observation. When the parameter is continuous, the rescaling is done by 
dividing the joint probability-probability density function at the observed value by 
its integral over all possible parameter values so it integrates to one. Again, the joint 
probability-probability density function is found by the multiplication rule and at the 
observed value is (prior x likelihood). This is done for binomial observations and a 
continuous beta prior in Chapter 8. When the observation is also a continuous random 
variable, the conditional probability density is found by rescaling the joint probability 
density at the observed value by dividing by its integral over all possible parameter 
values. Again, the joint probability density is found by the multiplication rule and at 
the observed value is prior x likelihood. This is done for normal observations and 
a continuous normal prior in Chapter 10. All these cases follow the same general 
pattern. 
Bayes’ theorem allows one to revise hisher belief about the parameter, given the 
data that occurred. There must be a prior belief to start from. One’s prior distribution 
gives the relative belief weights he/she has for the possible values of the parameters. 
How to choose one’s prior is discussed in detail. Conjugate priors are found by 
matching first two moments with prior belief on location and spread. When the 
conjugate shape does not give satisfactory representation of prior belief, setting up a 
discrete prior and interpolating is suggested. 
Details that I consider beyond the scope of this course are included as footnotes. 
There are many figures that illustrate the main ideas, and there are many fully 
worked out examples. I have included chapters comparing Bayesian methods with 
the corresponding frequentist methods. There are exercises at the end of each chapter, 
some with short answers. In the exercises, I only ask for the Bayesian methods to be 

PR€FAC€ TO FlRS T EDlTlON 
xxi 
used, because those are the methods I want the students to learn. There are computer 
exercises to be done in Minitab or R using the included macros. Some of these 
are small-scale Monte Carlo studies that demonstrate the efficiency of the Bayesian 
methods evaluated according to frequentist criteria. 
Advantages of the Bayesian Perspective 
Anyone who has taught an Introduction to Statistics class will know that students have 
a hard time coming to grips with statistical inference. The concepts of hypothesis 
testing and confidence intervals are subtle and students struggle with them. Bayesian 
statistics relies on a single tool, Bayes’ theorem, to revise our belief given the data. 
This is more like the kind of plausible reasoning that students use in their everyday 
life, but structured in a formal way. Conceptually, it is a more straightforward method 
for making inferences. The Bayesian perspective offers a number of advantages over 
the conventional frequentist perspective. 
0 The “objectivity“ of frequentist statistics has been obtained by disregarding 
any prior knowledge about the process being measured. Yet in science there 
usually is some prior knowledge about the process being measured. Throwing 
this prior information away is wasteful of information (which often translates 
to money). Bayesian statistics uses both sources of information: the prior 
information we have about the process and the information about the process 
contained in the data. They are combined using Bayes’ theorem. 
0 The Bayesian approach allows direct probability statements about the param- 
eters. This is much more useful to a scientist than the confidence statements 
allowed by frequentist statistics. This is a very compelling reason for using 
Bayesian statistics. Clients will interpret a frequentist confidence interval as a 
probability interval. The statistician knows that this interpretation is not cor- 
rect but also knows that the confidence interpretation relating the probability 
to all possible data sets that could have occurred but didn’t; is of no particular 
use to the scientist. Why not use a perspective that allows them to make the 
interpretation that is useful to them. 
0 Bayesian statistics has a single tool, Bayes’ theorem, which is used in all situ- 
ations. This contrasts to frequentist procedures, which require many different 
tools. 
0 Bayesian methods often outperform frequentist methods, even when judged by 
frequentist criteria. 
0 Bayesian statistics has a straightforward way of dealing with nuisance param- 
eters. They are always marginalized out of the joint posterior distribution. 
0 Bayes’ theorem gives the way to find the predictive distribution of future 
observations. This is not always easily done in a frequentist way. 

xxii 
PREFACE TO FlRST EDl7lON 
These advantages have been well known to statisticians for some time. However, 
there were great difficulties in using Bayesian statistics in actual practice. While it is 
easy to write down the formula for the posterior distribution, 
a closed form existed only in a few simple cases, such as for a normal sample with 
a normal prior. In other cases the integration required had to be done numerically. 
This in itself made it more difficult for beginning students. If there were more than a 
few parameters, it became extremely difficult to perform the numerical integration. 
In the past few years, computer algorithms (e.g., the Gibbs Sampler and the 
Metropolis-Hasting algorithm) have been developed to draw an (approximate) ran- 
dom sample from the posterior distribution, without having to completely evaluate 
it. We can approximate the posterior distribution to any accuracy we wish by taking 
a large enough random sample from it. This removes the disadvantage of Bayesian 
statistics, for now it can be done in practice for problems with many parameters, as 
well as for distributions from general samples and having general prior distributions. 
Of course these methods are beyond the level of an introductory course. Neverthe- 
less, we should be introducing our students to the approach to statistics that gives the 
theoretical advantages from the very start. That is how they will get the maximum 
benefit. 
Outline of a Course Based on This Text 
At the University of Waikato we have a one-semester course based on this text. This 
course consists of 36 one-hour lectures, 12 one-hour tutorial sessions, and several 
computer assignments. In each tutorial session, the students work through a statistical 
activity in a hands-on way. Some of the computer assignments involve Monte Carlo 
studies showing the long-run performance of statistical procedures. 
Chapter 1 (one lecture) gives an introduction to the course. 
Chapter 2 (three lectures) covers scientific data gathering including random 
sampling methods and the need for randomized experiments to make inferences 
on cause-effect relationships. 
Chapter 3 (two lectures) is on data analysis with methods for displaying and 
summarizing data. If students have already covered this material in a previous 
statistics course, this could be covered as a reading assignment only. 
Chapter 4 (three lectures) introduces the rules of probability including joint, 
marginal, and conditional probability and shows that Bayes' theorem is the 
best method for dealing with uncertainty. 
Chapter 5 (two lectures) introduces discrete and random variables. 

PREFACE TO FlRST EDlTlON 
xxiii 
Chapter 6 ((three lectures) shows how Bayesian inference works for an discrete 
random variable with a discrete prior. 
Chapter 7 (two lectures) introduces continuous random variables. 
Chapter 8 (three lectures) shows how inference is done on the population 
proportion from a binomial sample using either a uniform or a beta prior. 
There is discussion on choosing a beta prior that corresponds to your prior 
belief and then graphing it to confirm that it fits your belief. 
Chapter 9 (three lectures) compares the Bayesian inferences for the proportion 
with the corresponding frequentist ones. The Bayesian estimator for the pro- 
portion is compared with the corresponding frequentist estimator in terms of 
mean squared error. The difference between the interpretations of Bayesian 
credible interval and the frequentist confidence interval is discussed. 
Chapter 10 (four lectures) introduces Bayes’ theorem for the mean of a normal 
distribution, using either a “flat” improper prior or a normal prior. There is 
considerable discussion on choosing a normal prior and then graphing it to con- 
firm it fits with your belief. The predictive distribution of the next observation 
is developed. Student’s t distribution is introduced as the adjustment required 
for the credible intervals when the standard deviation is estimated from the 
sample. Section 10.5 is at a higher level, and may be omitted. 
Chapter 11 (one lecture) compares the Bayesian inferences for mean with the 
corresponding frequentist ones. 
Chapter 12 (three lectures) does Bayesian inference for the difference between 
two normal means, and the difference between two binomial proportions using 
the normal approximation. 
Chapter 13 (three lectures) does simple linear regression model in a Bayesian 
manner. Section 13.5 is at a higher level and may be omitted. 
Chapter 14 (three lectures) introduces robust Bayesian methods using mixture 
priors. This chapter shows how to protect against misspecified priors, which 
is one of the main concerns that many people have against using Bayesian 
statistics. It is at a higher level than the previous chapters and could be omitted 
and more lecture time given to the other chapters. 
Acknowledgments 
I would like to acknowledge the help I have had from many people. First, my students 
over the past three years, whose enthusiasm with the early drafts encouraged me to 
continue writing. My colleague, James Curran, for writing the R macros, writing 
Appendix D on how to implement them, and giving me access to the glass data. 

xxiv 
PREFACE TO FlRSJEDlJlON 
Ian Pool, Dharma Dharmalingam, and Sandra Baxendine from the University of 
Waikato Population Studies Centre for giving me access to the NZFEE data. Fiona 
Petchey from the University of Waikato Carbon Dating Unit for giving me access to 
the 14C archeological data. Lance McLeay from the University of Waikato Biology 
Department for giving me access to the slug data. Graham McBride from NIWA 
for giving me access to the New Zealand water quality data. Harold Henderson and 
Neil Cox from AgResearch NZ for giving me access to the I3C enriched Octanoic 
acid breath test data and the endophyte data. Martin Upsdell from AgResearch NZ 
made some usehl suggestions on an early draft. Renate Meyer from the University 
of Auckland gave me usehl comments on the manuscript. My colleagues Lyn Hunt, 
Judi McWhirter, Murray Jorgensen, Ray Littler, Dave Whitaker and Nye John for 
their support and encouragement through this project. Alec Zwart and Stephen Joe 
for help with BTm, and Karen Devoy for her secretarial assistance. John Wilkinson 
for his comments on the R macros which resulted in improved code. 
I would like to also thank my editor Rosalyn Farkas at John Wiley & Sons and 
thank Amy Hendrickson, of Tmnology Inc., for their patience and help through the 
process from rough manuscript to camera-ready copy. 
Finally, last but not least, I wish to thank my wife Sylvie for her constant love and 
support and for her help in producing some of the figures. 
WILL~AM 
M. "BILL" BOLSTAD 
Hamilton, New Zealand 
W. M. B. 

1 
Introduction to 
Statistical Science 
Statistics is the science that relates data to specific questions of interest. This includes 
devising methods to gather data relevant to the question, methods to summarize 
and display the data to shed light on the question, and methods that enable us to 
draw answers to the question that are supported by the data. Data almost always 
contain uncertainty. This uncertainty may arise from selection of the items to be 
measured, or it may arise from variability of the measurement process. Drawing 
general conclusions from data is the basis for increasing knowledge about the world, 
and is the basis for all rational scientific inquiry. Statistical inference gives us 
methods and tools for doing this despite the uncertainty in the data. The methods 
used for analysis depend on the way the data were gathered. It is vitally important 
that there is a probability model explaining how the uncertainty gets into the data. 
Showing a Causal Relationship from Data 
Suppose we have observed two variables X and Y .  Variable X appears to have an 
association with variable Y .  If high values of X occur with high values of variable Y 
and low values of X occur with low values of Y ,  we say the association is positive. On 
the other hand, the association could be negative in which high values of variable X 
occur in with low values of variable Y .  Figure 1.1 shows a schematic diagram where 
the association is indicated by the dotted curve connecting X and Y .  The unshaded 
area indicates that X and Y are observed variables. The shaded area indicates that 
there may be additional variables that have not been observed. 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
7 

2 
/NTRODUCT/ON TO STATlSTlCAL SClENCE 
We would like to determine why the two variables are associated. There are 
several possible explanations. The association might be a causal one. For example, 
X might be the cause of Y .  This is shown in Figure 1.2, where the causal relationship 
is indicated by the arrow from X to Y .  
On the other hand, there could be an unidentified third variable Z that has a causal 
effect on both X and Y .  They are not related in a direct causal relationship. The 
association between them is due to the effect of Z. Z is called a lurking variable, 
since it is hiding in the background and it affects the data. This is shown in Figure 
1.3. 
It is possible that both a causal effect and a lurking variable may both be contribut- 
ing to the association. This is shown in Figure 1.4. We say that the causal effect and 
the effect of the lurking variable are confounded. This means that both effects are 
included in the association. 

THE SClENTlFlC METHOD: A PROCESS FOR LEARNING 
3 
Our first goal is to determine which of the possible reasons for the association 
holds. If we conclude that it is due to a causal effect, then our next goal is to 
determine the size of the effect. If we conclude that the association is due to causal 
effect confounded with the effect of a lurking variable, then our next goal becomes 
determining the sizes of both the effects. 
1.1 THE SCIENTIFIC METHOD: A PROCESS FOR LEARNING 
In the Middle Ages, science was deduced from principles set down many centuries 
earlier by authorities such as Aristotle. The idea that scientific theories should be 
tested against real world data revolutionized thinking. This way of thinking known 
as the scientific method sparked the Renaissance. 
The scientific method rests on the following premises: 
0 A scientific hypothesis can never be shown to be absolutely true. 

4 
INTRODUCTION TO STATISTICAL SCIENCE 
0 However, it must potentially be disprovable. 
0 It is a useful model until it is established that it is not true. 
0 Always go for the simplest hypothesis, unless it can be shown to be false. 
This last principle, elaborated by William of Ockham in the 13th century, is now 
known as "Ockham's razor" and is firmly embedded in science. It keeps science from 
developing fanciful overly elaborate theories. Thus the scientific method directs 
us through an improving sequence of models, as previous ones get falsified. The 
scientific method generally follows the following procedure: 
1. Ask a question or pose a problem in terms of the current scientific hypothesis. 
2. Gather all the relevant information that is currently available. This includes 
the current knowledge about parameters of the model. 
3. Design an investigation or experiment that addresses the question from step 1. 
The predicted outcome of the experiment should be one thing if the current 
hypothesis is true, and something else if the hypothesis is false. 
4. Gather data from the experiment. 
5. Draw conclusions given the experimental results. Revise the knowledge about 
the parameters to take the current results into account. 
The scientific method searches for cause-and-effect relationships between an ex- 
perimental variable and an outcome variable. In other words, how changing the 
experimental variable results in a change to the outcome variable. Scientific mod- 
elling develops mathematical models of these relationships. Both of them need to 
isolate the experiment from outside factors that could affect the experimental results. 
All outside factors that can be identified as possibly affecting the results must be 
controlled. It is no coincidence that the earliest successes for the method were in 
physics and chemistry where the few outside factors could be identified and con- 
trolled. Thus there were no lurking variables. All other relevant variables could 
be identified, and then physically controlled by being held constant. That way they 
would not affect results of the experiment, and the effect of the experimental variable 
on the outcome variable could be determined. In biology, medicine, engineering, 
technology, and the social sciences it isn't that easy to identify the relevant factors 
that must be controlled. In those fields a different way to control outside factors 
is needed, because they can't be identified beforehand and physically controlled. 
1.2 THE ROLE OF STATISTICS IN THE SCIENTIFIC METHOD 
Statistical methods of inference can be used when there is random variability in the 
data. The probability model for the data is justified by the design of the investigation or 

MAIN APPROACHES TO STATISTICS 
5 
experiment. This can extend the scientific method into situations where the relevant 
outside factors cannot even be identified. Since we cannot identify these outside 
factors, we cannot control them directly. The lack of direct control means the outside 
factors will be affecting the data. There is a danger that the wrong conclusions could 
be drawn from the experiment due to these uncontrolled outside factors. 
The important statistical idea of randomization has been developed to deal with 
this possibility. The unidentified outside factors can be "averaged out" by randomly 
assigning each unit to either treatment or control group. This contributes variability 
to the data. Statistical conclusions always have some uncertainty or error due to 
variability in the data. We can develop a probability model of the data variability 
based on the randomization used. Randomization not only reduces this uncertainty 
due to outside factors, it also allows us to measure the amount of uncertainty that 
remains using the probability model. Randomization lets us control the outside 
factors statistically, by averaging out their effects. 
Underlying this is the idea of a statistical population, consisting of all possible 
values of the observations that could be made. The data consists of observations 
taken from a sample of the population. For valid inferences about the population 
parameters from the sample statistics, the sample must be "representative" of the 
population. Amazingly, choosing the sample randomly is the most effective way to 
get representative samples! 
1.3 MAIN APPROACHES TO STATISTICS 
There are two main philosophical approaches to statistics. The first is often referred to 
as thefrequentist approach. Sometimes it is called the classical approach. Procedures 
are developed by looking at how they perform over all possible random samples. The 
probabilities don't relate to the particular random sample that was obtained. In many 
ways this indirect method places the "cart before the horse." 
The alternative approach that we take in this book is the Bayesian approach. It 
applies the laws of probability directly to the problem. This offers many fundamental 
advantages over the more commonly used frequentist approach. We will show these 
advantages over the course of the book. 
Frequentist Approach to Statistics 
Most introductory statistics books take the frequentist approach to statistics, which 
is based on the following ideas: 
0 Parameters, the numerical characteristics of the population, are fixed but un- 
known constants. 
0 Probabilities are always interpreted as long-run relative frequency. 
0 Statistical procedures are judged by how well they perform in the long run over 
an infinite number of hypothetical repetitions of the experiment. 

6 
lNTRODUCTlON TO STATlSTlCAL SClENCE 
Probability statements are only allowed for random quantities. The unknown 
parameters are fixed, not random, so probability statements cannot be made about 
their value. Instead, a sample is drawn from the population, and a sample statistic 
is calculated. The probability distribution of the statistic over all possible random 
samples from the population is determined and is known as the sampling distribution 
of the statistic. The parameter of the population will also be a parameter of the 
sampling distribution. The probability statement that can be made about the statistic 
based on its sampling distribution is converted to a confidence statement about the 
parameter. The confidence is based on the average behavior of the procedure under 
all possible samples. 
Bayesian Approach to Statistics 
The Reverend Thomas Bayes first discovered the theorem that now bears his name. 
It was written up in a paper An Essay Towards Solving a Problem in the Doctrine of 
Chances. This paper was found after his death by his friend Richard Price, who had 
it published posthumously in the Philosophical Transactions of the Royal Society in 
1763. Bayes showed how inverse probability could be used to calculate probability 
of antecedent events from the occurrence of the consequent event. His methods were 
adopted by Laplace and other scientists in the 19th century, but had largely fallen 
from favor by the early 20th century. By the middle of the 20th century, interest in 
Bayesian methods had been renewed by De Finetti, Jeffreys, Savage, and Lindley, 
among others. They developed a complete method of statistical inference based on 
Bayes' theorem. 
This book introduces the Bayesian approach to statistics. The ideas that form the 
basis of the this approach are: 
0 Since we are uncertain about the true value of the parameters, we will consider 
them to be random variables. 
0 The rules of probability are used directly to make inferences about the param- 
eters. 
0 Probability statements about parameters must be interpreted as "degree of 
belief." The prior distribution must be subjective. Each person can have 
hislher own prior, which contains the relative weights that person gives to every 
possible parameter value. It measures how "plausible" the person considers 
each parameter value to be before observing the data. 
We revise our beliefs about parameters after getting the data by using Bayes' 
theorem. This gives our posterior distribution which gives the relative weights 
we give to each parameter value after analyzing the data. The posterior dis- 
tribution comes from two sources: the prior distribution and the observed 
data. 
This has a number of advantages over the conventional frequentist approach. Bayes' 
theorem is the only consistent way to modify our beliefs about the parameters given 

MAIN APPROACHES TO STATISTICS 
7 
the data that actually occurred. ‘This means that the inference is based on the actual 
occurring data, not all possible data sets that might have occurred but didn’t! Allowing 
the parameter to be a random variable lets us make probability statements about it, 
posterior to the data. This contrasts with the conventional approach where inference 
probabilities are based on all possible data sets that could have occurred for the fixed 
parameter value. Given the actual data, there is nothing random left with a fixed 
parameter value, so one can only make conjidence statements, based on what could 
have occurred. Bayesian statistics also has a general way of dealing with a nuisance 
parameter. A nuisance parameter is one which we don’t want to make inference 
about, but we don’t want them to interfere with the inferences we are making about 
the main parameters. Frequentist statistics does not have a general procedure for 
dealing with them. Bayesian statistics is predictive, unlike conventional frequentist 
statistics. This means that we can easily find the conditional probability distribution 
of the next observation given the sample data. 
Monte Carlo Studies 
In frequentist statistics, the parameter is considered a fixed, but unknown, constant. 
A statistical procedure such as a particular estimator for the parameter cannot be 
judged from the value it gives. The parameter is unknown, so we can’t know the 
value the estimator should be giving. If we knew the value of the parameter, we 
wouldn’t be using an estimator. 
Instead, statistical procedures are evaluated by looking how they perform in the 
long run over all possible samples of data, for fixed parameter values over some 
range. For instance, we fix the parameter at some value. The estimator depends 
on the random sample, so it is considered a random variable having a probability 
distribution. This distribution is called the sampling distribution of the estimator, 
since its probability distribution comes from taking all possible random samples. 
Then we look at how the estimator is distributed around the parameter value. This is 
called sample space averaging. Essentially it compares the performance of procedures 
before we take any data. 
Bayesian procedures consider the parameter to be a random variable, and its 
posterior distribution is conditional on the sample data that actually occurred, not all 
those samples that were possible but did not occur. However, before the experiment, 
we might want to know how well the Bayesian procedure works at some specific 
parameter values in the range. 
To evaluate the Bayesian procedure using sample space averaging, we have to 
consider the parameter to be both a random variable and a fixed but unknown value 
at the same time. We can get past the apparent contradiction in the nature of the 
parameter because the probability distribution we put on the parameter measures 
our uncertainty about the true value. It shows the relative belief weights we give to 
the possible values of the unknown parameter! After looking at the data, our belief 
distribution over the parameter values has changed. This way we can think of the 
parameter as a fixed, but unknown, value at the same time as we think of it being 
a random variable. This allows us to evaluate the Bayesian procedure using sample 

8 
INTRODUCTION TO STATISTICAL SCIENCE 
space averaging. This is called pie-posterior analysis because it can be done before 
we obtain the data. 
In Chapter 4, we will find out that the laws of probability are the best way to model 
uncertainty. Because of this, Bayesian procedures will be optimal in the post-data 
setting, given the data that actually occurred. In Chapters 9 and 11, we will see 
that Bayesian procedures perform very well in the pre-data setting when evaluated 
using pie-posterior analysis. In fact, it is often the case that Bayesian procedures 
outperform the usual frequentist procedures even in the pre-data setting. 
Monte Carlo studies are a useful way to perform sample space averaging. We draw 
a large number of samples randomly using the computer and calculate the statistic 
(frequentist or Bayesian) for each sample. The empirical distribution of the statistic 
(over the large number of random samples) approximates its sampling distribution 
(over all possible random samples). We can calculate statistics such as mean and 
standard deviation on this Monte Carlo sample to approximate the mean and standard 
deviation of the sampling distribution. Some small-scale Monte Carlo studies are 
included as exercises. 
1.4 PURPOSE AND ORGANIZATION OF THIS TEXT 
A very large proportion of undergraduates are required to take a service course in 
statistics. Almost all of these courses are based on frequentist ideas. Most of them 
don’t even mention Bayesian ideas. As a statistician, I know that Bayesian methods 
have great theoretical advantages. 1 think we should be introducing our best students 
to Bayesian ideas, from the beginning. There aren’t many introductory statistics text 
books based on the Bayesian ideas. Some other texts include Berry (1996), Press 
(1989), and Lee (1989). 
This book aims to introduce students with a good mathematics background to 
Bayesian statistics. It covers the same topics as a standard introductory statistics 
text, only from a Bayesian perspective. Students need reasonable algebra skills to 
follow this book. Bayesian statistics uses the rules of probability, so competence 
in manipulating mathematical formulas is required. Students will find that general 
knowledge of calculus is helpful in reading this book. Specifically they need to know 
that area under a curve is found by integrating, and that a maximum or minimum of a 
continuous differentiable function is found where the derivative of the function equals 
zero. However, the actual calculus used is minimal. The book is self-contained with 
a calculus appendix that students can refer to. 
Chapter 2 introduces some fundamental principles of scientific data gathering 
to control the effects of unidentified factors. These include the need for drawing 
samples randomly, along with some random sampling techniques. The reason why 
there is a difference between the conclusions we can draw from data arising from an 
observational study and from data arising from a randomized experiment is shown. 
Completely randomized designs and randomized block designs are discussed. 

PURPOSE AND ORGANIZATION OF THIS TEXT 
9 
Chapter 3 covers elementary methods for graphically displaying and summarizing 
data. Often a good data display is all that is necessary. The principles of designing 
displays that are true to the data are emphasized. 
Chapter 4 shows the difference between deduction and induction. Plausible rea- 
soning is shown to be an extension of logic where there is uncertainty. It turns out that 
plausible reasoning must follow the same rules as probability. The axioms of prob- 
ability are introduced and the rules of probability, including conditional probability 
and Bayes’ theorem are developed. 
Chapter 5 covers discrete random variables, including joint and marginal discrete 
random variables. The binomial, hypergeometric, and Poisson distributions are 
introduced, and the situations where they arise are characterized. 
Chapter 6 covers Bayes’ theorem for discrete random variables using a table. We 
see that two important consequences of the method are that multiplying the prior by 
a constant, or that multiplying the likelihood by a constant do not affect the resulting 
posterior distribution. This gives us the ”proportional form” of Bayes’ theorem. 
We show that we get the same results when we analyze the observations sequentially 
using the posterior after the previous observation as the prior for the next observation, 
as when we analyze the observations all at once using the joint likelihood and the 
original prior. We demonstrate Bayes’ theorem for binomial observations with a 
discrete prior and for Poisson observations with a discrete prior. 
Chapter 7 covers continuous random variables, including joint, marginal, and con- 
ditional random variables. The beta, gamma, and normal distributions are introduced 
in this chapter. 
Chapter 8 covers Bayes’ theorem for the population proportion (binomial) with a 
continuous prior. We show how to find the posterior distribution of the population 
proportion using either a uniform prior or a beta prior. We explain how to choose a 
suitable prior. We look at ways of summarizing the posterior distribution. 
Chapter 9 compares the Bayesian inferences with the frequentist inferences. We 
show that the Bayesian estimator (posterior mean using a uniform prior) has better 
performance than the frequentist estimator (sample proportion) in terms of mean 
squared error over most of the range of possible values. This kind of frequentist 
analysis is useful before we perform our Bayesian analysis. We see the Bayesian 
credible interval has a much more useful interpretation than the frequentist confidence 
interval for the population proportion. One-sided and two-sided hypothesis tests using 
Bayesian methods are introduced. 
Chapter 10 covers Bayes’ theorem for the Poisson observations with a continuous 
prior. The prior distributions used include the poisitive uniform, the Jefsreys’ prior, 
and the gamma prior. Bayesian inference for the Poisson parameter using the resulting 
posterior include Bayesian credible intervals and two-sided tests of hypothesis, as 
well as one-sided tests of hypothesis. 
Chapter 11 covers Bayes’ theorem for the mean of a normal distribution with 
known variance. We show how to choose a normal prior. We discuss dealing 
with nuisance parameters by marginalization. The predictive density of the next 
observation is found by considering the population mean a nuisance parameter and 
marginalizing it out. 

70 
INTRODUCTION TO STATISTICAL SCIENCE 
Chapter 12 compares Bayesian inferences with the frequentist inferences for the 
mean of a normal distribution. These comparisons include point and interval estima- 
tion, and hypothesis tests including both the one-sided and the two-sided cases. 
Chapter 13 shows how to perform Bayesian inferences for the difference between 
normal means and how to perform Bayesian inferences for the difference between 
proportions using the normal approximation. 
Chapter 14 introduces the simple linear regression model and shows how to 
perform Bayesian inferences on the slope of the model. The predictive distribution 
of the next observation is found by considering both the slope and intercept to be 
nuisance parameters and marginalizing them out. 
Chapter 15 introduces Bayesian inference for the standard deviation 0, 
when we 
have a random sample of normal observations with known mean p. This chapter 
is at a somewhat higher level than the previous chapters and requires the use of the 
change-of-variable formula for densities. Priors used include positive uniform for 
standard deviation, positive uniform for variance, Jeffreys ’ prior, and the inverse chi- 
squared prior. We discuss how to choose an inverse chi-squared prior that matches 
our prior belief about the median. Bayesian inferences from the resulting posterior 
include point estimates, credible intervals, and hypothesis tests including both the 
one-sided and two-sided cases. 
Chapter 16 shows how we can make Bayesian inference robust against a misspeci- 
fied prior by using a mixture prior and marginalizing out the mixture parameter. This 
chapter is also at a somewhat higher level than the others, but it shows how one of 
the main dangers of Bayesian analysis can be avoided. 
Main Points 
0 An association between two variables does not mean that one causes the other. 
It may be due to a causal relationship, it may be due to the effect of a third 
(lurking) variable on both the other variables, or it may be due to a combination 
of a causal relationship and the effect of a lurking variable. 
Scientific method is a method for searching for cause-and-effect relationships 
and measuring their strength. It uses controlled experiments, where outside 
factors that may affect the measurements are controlled. This isolates the rela- 
tionship between the two variables from the outside factors, so the relationship 
can be determined. 
0 Statistical methods extend the scientific method to cases where the outside 
factors aren’t identified, and hence can’t be controlled. The principle of ran- 
domization is used to statistically control these unidentified outside factors by 
averaging out their effects. This contributes to variability in the data. 
We can use the probability model (based on the randomization method) to 
measure the uncertainty. 

MAIN POINTS 
11 
0 The frequentist approach to statistics considers the parameter to be a fixed but 
unknown constant. The only kind of probability allowed is long-run relative 
frequency. These probabilities are only for observations and sample statistics, 
given the unknown parameters. Statistical procedures are judged by how they 
perform in an infinite number of hypothetical repetitions of the experiment. 
0 The Bayesian approach to statistics allows the parameter to be considered a 
random variable. Probabilities can be calculated for parameters as well as 
observations and sample statistics. Probabilities calculated for parameters 
are interpreted as “degree of belief’ and must be subjective. The rules of 
probability are used to revise our beliefs about the parameters, given the data. 
0 A frequentist estimator is evaluated by looking at its sampling distribution 
for a fixed parameter value and seeing how it is distributed over all possible 
repetitions of the experiment. 
0 If we look at the sampling distribution of a Bayesian estimator for a fixed 
parameter value, it is called pre-posterior analysis since it can be done prior to 
taking the data. 
0 A Monte Car10 study is where we perform the experiment a large number of 
times and calculate the statistic for each experiment. We use the empirical 
distribution of the statistic over all the samples we took in our study instead of 
its sampling distribution over all possible repetitions. 

This Page Intentionally Left Blank

2 
ScientiJic Data Gathering 
Scientists gather data purposefully, in order to find answers to particular questions. 
Statistical science has shown that data should be relevant to the particular questions, 
yet be gathered using randomization. The development of methods to gather data 
purposefully, yet using randomization, is one of the greatest contributions the field 
of statistics has made to the practice of science. 
Variability in data solely due to chance can be averaged out by increasing the 
sample size. Variability due to other causes cannot be. Statistical methods have been 
developed for gathering data randomly, yet relevant to a specific question. These 
methods can be divided into two fields. Sample survey theory is the study of methods 
for sampling from a finite real population. Experimental design is the study of 
methods for designing experiments that focus on the desired factors and that are not 
affected by other possibly unidentified ones. 
Inferences always depend on the probability model which we assume generated 
the observed data being the correct one. When data are not gathered randomly, there 
is a risk that the observed pattern is due to lurking variables that were not observed, 
instead of being a true reflection of the underlying pattern. In a properly designed 
experiment, treatments are assigned to subjects in such a way as to reduce the effects 
of any lurking variables that are present, but unknown to us. 
When we make inferences from data gathered according to a properly designed 
random survey or experiment, the probability model for the observations follows 
from the design of the survey or experiment, and we can be confident that it is 
correct. This puts our inferences on a solid foundation. On the other hand, when we 
Introduction ro Bayesian Statistics, Second Edition. B y  William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
13 

14 
SCIENTIFIC DATA GATHERING 
make inferences from data gathered from a nonrandom design, we don’t have any 
underlying justification for the probability model, we just assume it is true! There is 
the possibility the assumed probability model for the observations is not correct, and 
our inferences will be on shaky ground. 
2.1 SAMPLING FROM A REAL POPULATION 
First, we will define some fundamental terms. 
Population. The entire group of objects or people the investigator wants 
information about. For instance, the population might consist of New Zealand 
residents over the age of eighteen. Usually we want to know some specific 
attribute about the population. Each member of the population has a number 
associated with it, for example, hisher annual income. Then we can consider 
the model population to be the set of numbers for each individual in the 
real population. Our model population would be the set of incomes of all 
New Zealand residents over the age of eighteen. We want to learn about the 
distribution of the population. Specifically, we want information about the 
population parameters, which are numbers associated with the distribution of 
the population, such as the population mean, median, and standard deviation. 
Often it is not feasible to get information about all the units in the population. 
The population may be too big, or spread over too large an area, or it may cost 
too much to obtain data for the complete population. So we don’t know the 
parameters because it is infeasible to calculate them. 
Sample. A subset of the population. The investigator draws one sample from 
the population and gets information from the individuals in that sample. Sample 
statistics are calculated from sample data. They are numerical characteristics 
that summarize the distribution of the sample, such as the sample mean, median, 
and standard deviation. A statistic has a similar relationship to a sample that a 
parameter has to a population. However, the sample is known, so the statistic 
can be calculated. 
Statistical inference. Making a statement about population parameters on basis 
of sample statistics. Good inferences can be made if the sample is representative 
of the population as a whole! The distribution of the sample must be similar 
to the distribution of the population from which it came! Sampling bias, a 
systematic tendency to collect a sample which is not representative of the 
population, must be avoided. It would cause the distribution of the sample to 
be dissimilar to that of the population, and thus lead to very poor inferences. 
Even if we are aware of something about the population and try to represent it in 
the sample, there is probably some other factors in the population that we are unaware 
of, and the sample would end up being nonrepresentative in those factors. 
Example 1 Suppose we are interested in estimating the proportion of Hamilton 
voters who approve the Hamilton City Council’s$nancing a new rugby stadium. We 

SAMPLING FROMA REAL POPULATION 
15 
decide to go downtown one lunch break and draw our sample from people passing 
by. We might decide that our sample should be balanced between males and females 
the same as the voting age population. We might get a sample evenly balanced 
between males and females, but not be aware that the people we interview during the 
day are mainly those on the street during working hours. Ofice workers would be 
overrepresented, while factory workers would be underrepresented. There might be 
other biases inherent in choosing our sample this way, and we might not have a clue 
as to what these biases are. Some groups would be systematically underrepresented, 
and others systematically overrepresented. We can’t make our sample representative 
for classifications we don’t know. 
Surprisingly, random samples give more representative samples than any nonran- 
dom method such as quota samples or judgment samples. They not only minimize 
the amount of error in the inference, they also allow a (probabilistic) measurement 
of the error that remains. 
Simple Random Sampling (without Replacement) 
Simple random sampling requires a sampling frame , which is a list of the population 
numbered from 1 to N. A sequence of n random numbers are drawn from the 
numbers 1 to N .  Each time a number is drawn, it is removed from consideration, so 
it cannot be drawn again. The items on the list corresponding to the chosen numbers 
are included in the sample. Thus, at each draw, each item not yet selected has an 
equal chance of being selected. Every item has equal chance of being in the final 
sample. Furthermore, every possible sample of the required size is equally likely. 
Suppose we are sampling from the population of registered voters in a large city. 
It is likely that the proportion of males in the sample is close to the proportion of 
males in the population. Most samples are near the correct proportions, however, we 
are not certain to get the exact proportion. All possible samples of size n are equally 
likely, including those that are not representative with respect to sex. 
Stratified Random Sampling 
Since we know what the proportions of males and females are from the voters list, 
we should take that information into account in our sampling method. In stratified 
random sampling, the population is divided into subpopulations called strata. In our 
case this would be males and females. The sampling frame would be divided into 
separate sampling frames for the two strata. A simple random sample is taken from 
each stratum where each stratum sample size is proportional to stratum size. Every 
item has equal chance of being selected. And every possible sample that has each 
stratum represented in the correct proportions is equally likely. This method will give 
us samples that are exactly representative with respect to sex. Hence inferences from 
these type samples will be more accurate than those from simple random sampling 
when the variable of interest has different distributions over the strata. If the variable 
of interest is the same for all the strata, stratified random sampling will be no more 

76 
SCl€NT/F/C DATA GATHERlNG 
(and no less) accurate than simple random sampling. Stratification has no potential 
downside as far as accuracy of the inference. However, it is more costly, as the 
sampling frame has to be divided into separate sampling frames for each stratum. 
Cluster Random Sampling 
Sometimes we don’t have a good sampling frame of individuals. In other cases the 
individuals are scattered across a wide area. In cluster random sampling, we divide 
that area into neighborhoods called clusters. Then we make a sampling frame for 
clusters. A random sample of clusters is selected. All items in the chosen clusters 
are included in the sample. This is very cost effective because the interviewer won’t 
have as much travel time between interviews. The drawback is that items in a cluster 
tend to be more similar than items in different clusters. For instance, people living 
in the same neighborhood usually come from the same economic level because the 
houses were built at the same time and in the same price range. This means that each 
observation gives less information about the population parameters. It is less efficient 
in terms of sample size. However, often it is very cost effective, since getting a larger 
sample is usually cheaper by this method. 
Nonsampling Errors in Sample Surveys 
Errors can arise in sample surveys or in a complete population census for reasons 
other than the sampling method used. These nonsampling errors include response 
bias; the people who respond may be somewhat different than those who do not 
respond. They may have different views on the matters surveyed. Since we only 
get observations from those who respond, this difference would bias the results. A 
well-planned survey will have callbacks, where those in the sample who haven’t 
responded will be contacted again, in order to get responses from as many people 
in the original sample as possible. This will entail additional costs, but is important 
as we have no reason to believe that nonrespondents have the same views as the 
respondents. Errors can also arise from poorly worded questions. Survey questions 
should be trialed in a pilot study to determine if there is any ambiguity. 
Randomized Response Methods 
Social science researchers and medical researchers often wish to obtain information 
about the population as a whole, but the information that they wish to obtain is 
sensitive to the individuals who are surveyed. For instance, the distribution of the 
number of sex partners over the whole population would be indicative of the overall 
population risk for sexually transmitted diseases. Individuals surveyed may not wish 
to divulge this sensitive personal information. They might refuse to respond, or even 
worse, they could give an untruthful answer. Either way, this would threaten the 
validity of the survey results. Randomized response methods have been developed 
to get around this problem. There are two questions, the sensitive question and the 
dummy question. Both questions have the same set of answers. The respondent uses 

OBSERVATIONAL STUDIES AND DESIGNED EXPERIMENTS 
17 
a randomization that selects which question he or she answers, and also the answer 
if the dummy question is selected. Some of the answers in the survey data will be to 
the sensitive question and some will be to the dummy question. The interviewer will 
not know which is which. However, the incorrect answers are entering the data from 
known randomization probabilities. This way, information about the population can 
be obtained without actually knowing the personal information of the individuals 
surveyed, since only that individual knows which question he or she answered. 
Bolstad, Hunt, and McWhirter (2001) describe a Sex, Drugs, and Rock & Roll Survey 
that gets sensitive information about a population (Introduction to Statistics class) 
using randomized response methods. 
2.2 OBSERVATIONAL STUDIES AND DESIGNED EXPERIMENTS 
The goal of scientific inquiry is to gain new knowledge about the cause-and-effect 
relationship between a factor and a response variable. We gather data to help us 
determine these relationships and to develop mathematical models to explain them. 
The world is complicated. There are many other factors that may affect the response. 
We may not even know what these other factors are. If we don’t know what they 
are, we cannot control them directly. Unless we can control them, we can’t make 
inferences about cause and effect relationships! Suppose, for example, we want to 
study a herbal medicine for its effect on weight loss. Each person in the study is 
an experimental unit. There is great variability between experimental units, because 
people are all unique individuals with their own hereditary body chemistry and dietary 
and exercise habits. The variation among experimental units makes it more difficult 
to detect the effect of a treatment. Figure 2.1 shows a collection of experimental units. 
The degree of shading shows they are not the same with respect to some unidentified 
variable. The response variable in the experiment may depend on that unidentified 
variable, which could be a lurking variable in the experiment. 
Observational Study 
If we record the data on a group of subjects that decided to take the herbal medicine 
and compared that with data from a control group who did not, that would be an 
observational study. The treatments have not been randomly assigned to treatment 
and control group. Instead they self-select. Even if we observe a substantial difference 
between the two groups, we cannot conclude that there is a causal relationship from an 
observational study. We can’t rule out that the association was due to an unidentified 
lurking variable. In our study, those who took the treatment may have been more 
highly motivated to lose weight than those who did not. Or there may be other factors 
that differed between the two groups. Any inferences we make on an observational 
study are dependent on the assumption that there are no differences between the 
distribution of the units assigned to the treatment groups and the control group. We 
can’t know whether this assumption is actually correct in an observational study. 

78 
SClENTlFlC DATA GATHfRING 
Figure 2.1 Variation among experimental units. 
Designed Experiment 
We need to get our data from a designed experiment if we want to be able to 
make sound inferences about cause-and-effect relationships. The experimenter uses 
randomization to decide which subjects get into the treatment group(s) and control 
group respectively. For instance, he/she uses a table of random numbers, or flips a 
coin. 
We are going to divide the experimental units into four treatment groups (one of 
which may be a control group). We must ensure that each group gets a similar range 
of units. If we don’t, we might end up attributing a difference between treatment 
groups to the different treatments, when in fact it was due to the lurking variable and 
a biased assignment of experimental units to treatment groups. 
Completely randomized design. We will randomly assign experimental units 
to groups so that each experimental unit is equally likely to go to any of the groups. 
Each experimental unit will be assigned (nearly) independently of other experimental 
units. The only dependence between assignments is that having assigned one unit to 
treatment group 1 (for example), the probability of the other unit being assigned to 
group 1 is slightly reduced because there is one less place in group 1. This is known 
as a completely randomized design. Having a large number of (nearly) independent 
randomizations ensures that the comparisons between treatment groups and control 
group are fair since all groups will contain a similar range of experimental units. 
Units having high values and units having low values of the lurking variable will be 

OBSERVATIONAL STUDIES AND DESIGNED EXPERIMENTS 
19 
Figure 2,2 
treatment groups. 
Completely randomized design. Units have been randomly assigned to four 
in all treatment groups in similar proportions. In Figure 2.2 we see the four treatment 
groups have similar range of experimental units with respect to the unidentified 
lurking variable. 
The randomization averages out the differences between experimental units as- 
signed to the groups. The expected value of the lurking variable is the same for all 
groups, because of the randomization. The average value of the lurking variable for 
each group will be close to its mean value in the population because there are a large 
number of independent randomizations. The larger the number of units in the exper- 
iment, the closer the average values of the lurking variable in each group will be to 
its mean value in the population. If we find an association between the treatment and 
the response, it will be unlikely that the association was due to any lurking variable. 
For a large-scale experiment, we can effectively rule out any lurking variable and 
conclude that the association was due to the effect of different treatments. 
Randomized block design. If we identify a variable, we can control for it 
directly. It ceases to be a lurking variable. One might think that using judgment 
about assigning experimental units to the treatment and control groups would lead 
to similar range of units being assigned to them. The experimenter could get similar 
groups according to the criterion (identified variable) he/she was using. However, 
there would be no protection against any other lurking variable that hadn’t been 
considered. We can’t expect it to be averaged out if we haven’t done the assignments 
randomly ! 

20 
SCIENTIFIC DATA GATHERING 
Figure 2.3 Similar units have been put into blocks. 
Any prior knowledge we have about the experimental units should be used before 
the randomization. Units that have similar values of the identified variable should 
be formed into blocks. This is shown in Figure 2.3. The experimental units in each 
block are similar with respect to that variable. Then the randomization is be done 
within blocks. One experimental unit in each block is randomly assigned to each 
treatment group. The blocking controls that particular variable, as we are sure all 
units in the block are similar, and one goes to each treatment group. By selecting 
which one goes to each group randomly, we are protecting against any other lurking 
variable by randomization. It is unlikely that any of the treatment groups was unduly 
favored or disadvantaged by the lurking variable. On the average, all groups are 
treated the same. Figure 2.4 shows the treatment groups found by a randomized 
block design. We see the four treatment groups are even more similar than those 
from the completely randomized design. 
For example, if we wanted to determine which of four varieties of wheat gave 
better yield, we would divide the field into blocks of four adjacent plots because plots 
that are adjacent are more similar in their fertility than plots that are distant from 
each other. Then within each block, one plot would be randomly assigned to each 
variety. This randomized block design ensures that the four varieties each have been 
assigned to similar groups of plots. It protects against any other lurking variable, by 
the within-block randomization. 

MAIN POINTS 
21 
Figure 2.4 
treatment group. Randomizations in different blocks are independent of each other. 
Randomized block design. One unit in each block randomly assigned to each 
When the response variable is related to the trait we are blocking on, the blocking 
will be effective, and the randomized blockdesign will lead to more precise inferences 
about the yields than a completely randomized design with the same number of plots. 
This can be seen by comparing the treatment groups from the completely randomized 
design shown in Figure 2.2 with the treatment groups from the randomized block 
design shown in Figure 2.4. The treatment groups from the randomized block design 
are more similar than those from the completely randomized design. 
Main Points 
0 Population. The entire set of objects or people that the study is about. Each 
member of the population has a number associated with it, so we often consider 
the population as a set of numbers. We want to know about the distribution of 
these numbers. 
0 Sample. The subset of the population from which we obtain the numbers. 
0 Parameter. A number that is a characteristic of the population distribution, 
such as the mean, median, standard deviation, and interquartile range of the 
whole population. 
0 Statistic. A number that is a characteristic of the sample distribution, such as 
the mean, median, standard deviation, and interquartile range of the sample. 

22 
SClENTlFlC DATA GATHERlNG 
Statistical inference. Making a statement about population parameters on the 
basis of sample statistics. 
Simple random sampling. At each draw every item that has not already been 
drawn has an equal chance of being chosen to be included in the sample. 
StratiJed random sampling. The population is partitioned into subpopulations 
called strata, and simple random samples are drawn from each stratum where 
the stratum sample sizes are proportional to the stratum proportions in the 
population. The stratum samples are combined to form the sample from the 
population. 
Cluster random sampling. The area the population lies in is partitioned into 
areas called clusters. A random sample of clusters is drawn, and all members 
of the population in the chosen clusters are included in the sample. 
Randomized response methods. These allow the respondent to randomly de- 
termine whether to answer a sensitive question or the dummy question, which 
both have the same range of answers. Thus the respondents personal informa- 
tion is not divulged by the answer, since the interviewer does not know which 
question it applies to. 
Observational study. The researcher collects data from a set of experimental 
units not chosen randomly, or not allocated to experimental or control group 
by randomization. There may be lurking variables due to the lack of random- 
ization. 
Designed experiment. The researcher allocates experimental units to the treat- 
ment group(s) and control group by some form of randomization. 
Completely randomized design. The researcher randomly assigns the units 
into the treatment groups (nearly) independently. The only dependence is the 
constraint that the treatment groups are the correct size. 
Randomized block design. The researcher first groups the units into blocks 
which contain similar units. Then the units in each block are randomly as- 
signed, one to each group. The randomizations in separate blocks are per- 
formed independent of each other. 
Monte Carlo Exercises 
2.1 Monte Carlo study comparing methods for random sampling. We will 
use a Monte Carlo computer simulation to evaluate the methods of random 
sampling. Now, if we want to evaluate a method, we need to know how it does 
in the long run. In a real-life situation, we can’t judge a method by the sample 
estimate it gives, because if we knew the population parameter, we would not 
be taking a sample and estimating it with a sample statistic. 

MONTE CARL0 EXERClSES 
23 
One way to evaluate a statistical procedure is to evaluate the sampling distri- 
bution which summarizes how the estimate based on that procedure varies in 
the long run (over all possible random samples) for a case when we know the 
population parameters. Then we can see how closely the sampling distribution 
is centered around the true parameter. The closer it is, the better the statistical 
procedure, and the more confidence we will have in it for realistic cases when 
we don’t know the parameter. 
If we use computer simulations to run a large number of hypothetical repetitions 
of the procedure with known parameters, this is known as a Monte Carlo study 
named after the famous casino. Instead of having the theoretical sampling 
distribution, we have the empirical distribution of the sample statistic over 
those simulated repetitions. We judge the statistical procedure by seeing how 
closely the empirical distribution of the estimator is centered around the known 
parameter. 
The population. Suppose there is a population made up of 100 individuals, 
and we want to estimate the mean income of the population from a random 
sample of size 20. The individuals come from three ethnic groups with pop- 
ulation proportions of 40%, 40%, and 20%, respectively. There are twenty 
neighborhoods, and five individuals live in each one. Now, the income dis- 
tribution may be different for the three ethnic groups. Also, individuals in 
the same neighborhood tend to be more similar than individuals in different 
neighborhoods. 
Details about the population are contained in the Minitab worksheet sscsam- 
plemtw. Each row contains the information for an individual. Column 1 
contains the income, column 2 contains the ethnic group, and column 3 con- 
tains the neighborhood. Compute the mean income for the population. That 
will be the true parameter value that we are trying to estimate. 
In the Monte Carlo study we will approximate the sampling distribution of the 
sample means for three types of random sampling, simple random sampling, 
stratified random sampling, and cluster random sampling. We do this by draw- 
ing a large number (in this case 200) random samples from the population using 
each method of sampling, calculating the sample mean as our estimate. The 
empirical distribution of these 200 sample means approximates the sampling 
distribution of the estimate. 
(a) Display the incomes for the three ethnic groups (strata) using boxplots on 
the same scale. Compute the mean income for the three ethnic groups. 
Do you see any difference between the income distributions? 
(b) Draw 200 random samples of size 20 from the population using simple 
random sampling using sscsampkmac and put the output in columns 
c k 9 .  Details of how to use this macro are in Appendix C. Answer the 
following questions from the output: 
i. Does simple random sampling always have the strata represented in 
the correct proportions? 

24 
SClENTlFlC DATA GATHERING 
ii. On the average, does simple random sampling give the strata in their 
correct proportions? 
iii. Does the mean of the sampling distribution of the sample mean for 
simple random sampling appear to be close enough to the population 
mean that we can consider the difference to be due to chance alone? 
(We only took 200 samples, not all possible samples.) 
(c) Draw 200 stratified random samples using the macro and store the output 
i. Does stratified random sampling always have the strata represented 
in the correct proportions? 
ii. On the average, does stratified random sampling give the strata in 
their correct proportions? 
iii. Does the mean of the sampling distribution of the sample mean 
for stratified random sampling appear to be close enough to the 
population mean that we can consider the difference to be due to 
chance alone? (We only took200 samples, not all possible samples.) 
(d) Draw 200 cluster random samples using the macro and put the output in 
columns c l k 1 9 .  Answer the following questions from the output: 
in ~ 1 1 x 1 4 .  
Answer the following questions from the output: 
i. Does cluster random sampling always have the strata represented in 
the correct proportions? 
ii. On the average, does cluster random sampling give the strata in their 
correct proportions? 
iii. Does the mean of the sampling distribution of the sample mean for 
cluster random sampling appear to be close enough to the population 
mean that we can consider the difference to be due to chance alone? 
(We only took 200 samples, not all possible samples.) 
(e) Compare the spreads of the sampling distributions (standard deviation 
and interquartile range). Which method of random sampling seems to be 
more effective in giving sample means more concentrated about the true 
mean? 
(0 
Give reasons for this. 
2.2 Monte Carlo study comparing completely randomized design and ran- 
domized block design. Often we want to set up an experiment to determine 
the magnitude of several treatment effects. We have a set of experimental units 
that we are going to divide into treatment groups. There is variation among 
the experimental units in the underlying response variable that we are going to 
measure. We will assume that we have an additive model where each of the 
treatments has a constant effect. That means the measurement we get for an 
experimental unit i given treatment j will be the underlying value for unit i 

MONTE CARL0 EXERCISES 
25 
plus the effect of the treatment for the treatment it receives: 
where ui is the underlying value for experimental unit i and Tj is the treatment 
effect for treatment j .  The assignment of experimental units to treatment 
groups is crucial. 
There are two things that the assignment of experimental units into treatment 
groups should deal with. First, there may be a "lurking variable" that is 
related to the measurement variable, either positively or negatively. If we 
assign experimental units that have high values of that lurking variable into 
one treatment group, that group will be either advantaged or disadvantaged, 
depending if there is a positive or negative relationship. We would be quite 
likely to conclude that treatment is good or bad relative to the other treatments, 
when in fact the apparent difference would be due to the effect of the lurking 
variable. That is clearly a bad thing to occur. We know that to prevent this, 
the experimental units should be assigned to treatment groups according to 
some randomization method. On the average, we want all treatment groups to 
get a similar range of experimental units with respect to the lurking variable. 
Otherwise, the experimental results may be biased. 
Second, the variation in the underlying values of the experimental units may 
mask the differing effects of the treatments. It certainly makes it harder to 
detect a small difference in treatment effects. The assignment of experimental 
units into treatment groups should make the groups as similar as possible. 
Certainly, we want the group means of the underlying values to be nearly 
equal. 
The completely randomized design randomly divides the set of experimental 
units into treatment groups. Each unit is randomized (almost) independently. 
We want to ensure that each treatment group contains equal numbers of units. 
Every assignment that satisfies this criterion is equally likely. This design does 
not take the values of the other variable into account. It remains a possible 
lurking variable. 
The randomized block design takes the other variable value into account. First 
blocks of experimental units having similar values of the other variable are 
formed. Then one unit in each block is randomly assigned to each of the 
treatment groups. In other words, randomization occurs within blocks. The 
randomizations in different blocks are done independently of each other. This 
design makes use of the other variable. It ceases to be a lurking variable and 
becomes the blocking variable. 
In this assignment we compare the two methods of randomly assigning exper- 
imental units into treatment groups. Each experimental unit has an underlying 
value of the response variable and a value of another variable associated with 
it. (If we don't take the other variable in account, it will be a lurking variable.) 

26 
SCIENTIFIC DATA GATHERING 
We will run a small-scale Monte Carlo study to compare the performance of 
these two designs in two situations. 
(a) First we will do a small-scale Monte Carlo study of 500 random as- 
signments using each of the two designs when the response variable is 
strongly related to the other variable. We let the correlation between them 
be k l  = .8. The details of how to use the Minitab macro Xdesigamac or 
the R function Xdesign are in Appendix 3 and Appendix 4, respectively. 
Look at the boxplots and summary statistics. 
i. Does it appear that, on average, all groups have the same underlying 
mean value for the other (lurking) variable when we use a completely 
randomized design? 
ii. Does it appear that, on average, all groups have the same under- 
lying mean value for the other (blocking) variable when we use a 
randomized block design? 
iii. Does the distribution of the other variable over the treatment groups 
appear to be the same for the two designs? Explain any difference. 
iv. Which design is controlling for the other variable more effectively? 
Explain. 
v. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a completely 
randomized design? 
vi. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a randomized 
block design? 
vii. Does the distribution of the response variable over the treatment 
groups appear to be the same for the two designs? Explain any 
difference. 
viii. Which design will give us a better chance for detecting a small 
difference in treatment effects? Explain. 
ix. Is blocking on the other variable effective when the response variable 
is strongly related to the other variable? 
(b) Next we will do a small-scale Monte Carlo study of 500 random as- 
signments using each of the two designs when the response variable is 
weakly related to the other variable. We let the correlation between them 
be k l  = .4. Look at the boxplots and summary statistics. 
i. Does it appear that, on average, all groups have the same underlying 
mean value for the other (lurking) variable when we use a completely 
randomized design? 
ii. Does it appear that, on average, all groups have the same under- 
lying mean value for the other (blocking) variable when we use a 
randomized block design? 

MONTE CARL0 EXERCISES 
27 
iii. Does the distribution of the other variable over the treatment groups 
appear to be the same for the two designs? Explain any difference. 
iv. Which design is controlling for the other variable more effectively? 
Explain. 
v. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a completely 
randomized design? 
vi. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a randomized 
block design? 
vii. Does the distribution of the response variable over the treatment 
groups appear to be the same for the two designs? Explain any 
difference. 
viii. Which design will give us a better chance for detecting a small 
ix. Is blocking on the other variable effective when the response variable 
difference in treatment effects? Explain. 
is strongly related to the other variable? 
(c) Next we will do a small-scale Monte Carlo study of 500 random assign- 
ments using each of the two designs when the response variable is not 
related to the other variable. We let the correlation between them be 
kl = 0. This will make the response variable independent of the other 
variable. Look at the boxplots for the treatment group means for the other 
variable. 
i. Does it appear that, on average, all groups have the same underlying 
mean value for the other (lurking) variable when we use a completely 
randomized design? 
ii. Does it appear that, on average, all groups have the same under- 
lying mean value for the other (blocking) variable when we use a 
randomized block design? 
iii. Does the distribution of the other variable over the treatment groups 
appear to be the same for the two designs? Explain any difference. 
iv. Which design is controlling for the other variable more effectively? 
Explain. 
v. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a completely 
randomized design? 
vi. Does it appear that, on average, all groups have the same underlying 
mean value for the response variable when we use a randomized 
block design? 

28 
SCIENTIFIC DATA GATHERING 
vii. Does the distribution of the response variable over the treatment 
groups appear to be the same for the two designs? Explain any 
difference. 
viii. Which design will give us a better chance for detecting a small 
difference in treatment effects? Explain. 
ix. Is blocking on the other variable effective when the response variable 
is independent from the other variable? 
x. Can we lose any effectiveness by blocking on a variable that is not 
related to the response? 

3 
Displaying and 
Summarizing Data 
We use statistical methods to extract information from data and gain insight into 
the underlying process that generated the data. Frequently our data set consists of 
measurements on one or more variables over the experimental units in one or more 
samples. The distribution of the numbers in the sample will give us insight into the 
distribution of the numbers for the whole population. 
It is very difficult to gain much understanding by looking at a set of numbers. Our 
brains were not designed for that. We need to find ways to present the data that allow 
us to note the important features of the data. The visual processing system in our brain 
enables us to quickly perceive the overview we want, when the data are represented 
pictorially in a sensible way. They say a picture is worth a thousand words. That 
is true, provided the we have the correct picture. If the picture is incorrect, we can 
mislead ourselves and others very badly! 
3.1 GRAPHICALLY DISPLAYING A SINGLE VARIABLE 
Often our data set consists of a set of measurements on a single variable for a single 
sample of subjects or experimental units. We want to get some insight into the 
distribution of the measurements of the whole population. A visual display of the 
measurements of the sample helps with this. 
Infroduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
29 

30 
DISPLAYING AND SUMMARIZING DATA 
Table 3.7 Earth density measurements by Cavendish 
5.50 5.61 
4.88 
5.07 
5.26 
5.55 
5.36 
5.29 
5.58 
5.65 
5.57 5.53 
5.62 
5.29 
5.44 
5.34 
5.79 
5.10 
5.27 
5.39 
5.42 5.47 
5.63 
5.34 
5.46 
5.30 
5.75 
5.68 
5.85 
. .  
. .  
. . . . , . . . . . . . . . . . . . . . . . . . 
I 
I 
I 
I 
5.0 
5.2 
5.4 
5.6 
5.8 
Figure 3.1 Dotplot of Earth density measurements by Cavendish. 
Example 2 In I798 the English scientist Cavendish performed a series of 29 mea- 
surements on the density of the Earth using a torsion balance. This experiment and 
the data set are described by Stigler (1977). Table 3.1 contains the 29 measurements. 
Dotplot 
A dotplot is the simplest data display for a single variable. Each observation is rep- 
resented by a dot at its value along horizontal axis. This shows the relative positions 
of all the observation values. It is easy to get a general idea of the distribution of the 
values. Figure 3.1 shows the dotplot of Cavendish’s Earth density measurements. 
Boxplot (Box-and-Whisker Plot) 
Another simple graphical method to summarize the distribution of the data is to form 
a boxplot. First we have to sort and summarize the data. 
Originally, the sample values are y1, . . . , yn. The subscript denotes the order (in 
time) the observation was taken, y1 is the first, y2 is the second, and so on up to yn 
which is last. When we order the sample values by size from smallest to largest we 
get the order statistics. They are denoted Y [ ~ I ,  
. . . , yrn], where y[ll is the smallest, y(21 
is the second smallest, on up to the largest Y [ ~ ] .  We divide the ordered observations 
into quarters with the quartiles. Q1, the lower quartile, is the value that 25% of the 
observations are less than or equal to it, and 75% or more of the observations are 
greater than or equal to it. Q2, the middle quartile, is the value that 50% or more of 
the observations are less than or equal to it, and 50% or more of the observations are 
greater than or equal to it. Qz is also known as the sample median. Similarly Q3, the 
upper quartile is the value that 75% of the observations are less than or equal to it, 
and 25% of the observations are greater than or equal to it. We can find these from 

GRAPHICALLY DISPLAYING A SINGLE VARIABLE 
31 
the order statistics: 
Q1 = Y[+] 
3 
Q2 = Y[YI 
7 
4 
I '  
Q3 = yi= 
If the subscripts are not integers, we take the weighted average of the two closest 
order statistics. For example, Cavendish's Earth density data n = 29, 
This is halfway between the 7th and 8th order statistics, so 
QI =+XY[7] fixy[8]. 
The five number summary of a data set is y[11, Q 1 ,  Q2, Q3, yin!. 
This gives the 
minimum, the three quartiles, and the maximum of the observations. The boxplot 
or box-and-whiskerplot is a pictorial way of representing the five number summary. 
The steps are: 
0 Draw and label an axis. 
0 Draw a box with ends at the first and third quartiles. 
0 Draw a line through the box at the second quartile (median). 
0 Draw a line (whisker) from the lower quartile to the lowest observation, and 
draw a line (whisker) from the upper quartile to the highest observation. 
0 Warning: Minitab extends the whiskers only to a maximum length of 1.5 x 
the interquartile range. Any observation further out than that is identified with 
an asterisk (*) to indicate the observation may be an outlier. This can seriously 
distort the picture of the sample, because the criterion does not depend on the 
sample size. A large sample can look very heavy-tailed because the asterisks 
show that there are many possibly outlying values, when the proportion of 
outliers is well within the normal range. In Exercise 6, we show how this 
distortion works and how we can control it by editing the outlier symbol in the 
Minitab boxplot. 
The boxplot divides the observations into quarters. It shows you a lot about the 
shape of the data distribution. Examining the length of the whiskers compared to the 
box length shows whether the data set has light, normal, or heavy tails. Comparing 
the lengths of the whiskers show whether the distribution of the data appears to be 
skewed or symmetric. Figure 3.2 shows the boxplot for Cavendish's Earth density 
measurements. It shows that the data distribution is fairly symmetric but with a 
slightly longer lower tail. 

32 
DISPLAYING AND SUMMARIZING DATA 
I 
I 
I 
5.0 
5.2 
5.4 
5.6 
5.8 
Figure 3.2 Boxplot of Earth density measurements by Cavendish. 
Stem-and-Leaf Diagram 
The stem-and-leaf diagram is a quick and easy way of extracting information about 
the distribution of a sample of numbers. The stem represents the leading digit(s) to a 
certain depth (power of 10) of each data item, and the leaf represents the next digit of 
the data item. A stem-and-leaf diagram can be constructed by hand for a small data 
set. It is often the first technique used on a set of numbers. The steps are 
0 Draw a vertical axis (stem) and scale it for the stem units. Always use a linear 
scale! 
0 Plot leaf for the next digit. We could round off the leaf digit, but usually we 
don’t bother if we are doing it by hand. In any case, we may have lost some 
information by rounding off or by truncating. 
0 Order the leaves with the smallest near stem to the largest farthest away. 
0 State the leaf unit on your diagram. 
The stem-and-leaf plot gives a picture of the distribution of the numbers when we 
turn it on its side. It retains the actual numbers to within the accuracy of the leaf 
unit. We can find the order statistics counting up from the lower end. This helps 
to find the quartiles and the median. Figure 3.3 shows a stem-and-leaf diagram for 
Cavendish’s Earth density measurements. We use a two-digit stem, units and tenths, 
and a one-digit leaf, hundredths. 

GRAPHICALLY DISPLAYING A SINGLE VARIABLE 
33 
48 
49 
50 
51 
52 
53 
54 
55 
56 
57 
58 
leafunit 
.01 
8 
7 
0 
6799 
04469 
2467 
03578 
12358 
59 
5 
Figure 3.3 Stem-and-leaf plot for Cavendish’s Earth density measurements. 
There are 29 measurements. We can count down to the XZ+I = Xi5 to find that 
the median is 5.46. We can count down to XZ+I 
= X7;. Thus the first quartile 
&I = i x X7 + i x X8 which is 5.295 
Frequency Table 
Another main approach to simplify a set of numbers is to put it in a frequency table. 
This is sometimes referred to as binning the data. The steps are: 
0 Partition possible values into nonoverlapping groups (bins). Usually we use 
equal width groups. However, this is not required. 
Put each item into the group it belongs in. 
0 Count the number of items in each group. 
Frequency tables are a useful tool for summarizing data into an understandable form. 
There is a trade-off between the loss of information in our summary, and the ease of 
understanding the information that remains. We have lost information when we put 
a number into a group. We know it lies between the group boundaries, but its exact 
value is no longer known. The fewer groups we use, the more concise the summary, 
but the greater loss of information. If we use more groups we lose less information, 
but our summary is less concise and harder to grasp. Since we no longer have the 
information about exactly where each value lies in a group, it seems logical that the 
best assumption we can then make is that each value in the group is equally possible. 
The Earth density measurements made by Cavendish are shown as a frequency table 
in Table 3.2. 
If there are two many groups, some of them may not contain any observations. 
In that case, it is better to lump two or more adjacent groups into a bigger one to 

34 
DISPLAYING AND SUMMARIZING DATA 
Table 3.2 Frequency table of Earth density measurements by Cavendish 
Boundaries 
Frequency 
4.80 < x 5 5.00 
5.00 < x 5 5.20 
5.20 < x 5 5.40 
5.40 < x 5 5.60 
5.60 < x 5 5.80 
5.80 < x 5 6.00 
1 
2 
9 
9 
7 
1 
get some observations in every group. There are two ways to show the data in a 
frequency table pictorially. They are histograms and cumulativefrequency polygons. 
Histogram 
This is the most common way to show the distribution of data in the frequency table. 
The steps for constructing a histogram are: 
Put group boundaries on horizontal axis drawn on a linear scale. 
Draw a rectangular bar for each group where the area of bar is proportional to 
the frequency of that group. For example, this means that if a group is twice 
as wide as the others, its height is half that group’s frequency. The bar is flat 
across the top to show our assumption that each value in the group is equally 
possible. 
Do not put any gaps between the bars if the data are continuous. 
The scale on the vertical axis is density, which is group frequency divided by 
group width. When the groups have equal width, the scale is proportional to 
frequency, or relative frequency, and they could be used instead of density. 
This is not true if unequal width groups are used. It is not necessary to label 
the vertical axis on the graph. The shape of the graph is the important thing, 
not its vertical scale. 
Warning: If you use unequal group widths in Minitab, you must click on 
dens@ in the options dialog box; otherwise, the histogram will have the wrong 
shape. 
The histogram gives us a picture of how the sample data are distributed. We can see 
the shape of the distribution and relative tail weights. We look at it as representing 
a picture of the underlying population the sample came from. This underlying 

GRAPHICALLY DISPLAYING A SINGLE VARIABLE 
35 
population distribution' would generally be reasonably smooth. There is always a 
trade-off between too many and too few groups. If we use too many groups, the 
histogram has a "saw tooth" appearance and the histogram is not representing the 
population distribution very well. If we use too few groups, we lose details about the 
shape. Figure 3.4 shows histogram of the Earth density measurements by Cavendish 
using 12, 6, and 4 groups, respectively. This illustrates the trade-off between too 
many and too few groups. We see that the histogram with 12 groups has gaps and 
a saw-tooth appearance. The histogram with 6 groups gives a better representation 
of the underlying distribution of Earth density measurements. The histogram with 4 
groups has lost too much detail. The last histogram has unequal width groups. The 
height of the wider bars is shortened to keep the area proportional to frequency. 
Cumulative Frequency Polygon 
The other way for displaying the data from a frequency table is to construct a 
cumulative frequency polygon, sometimes called an ogive. It is particularly useful 
because you can estimate the median and quartiles from the graph. The steps are: 
0 Group boundaries on horizontal axis drawn on a linear scale. 
0 Frequency or percentage shown on vertical axis. 
0 Plot (lower boundary of lowest class, 0). 
0 For each group, plot (upper class boundaq cumulative frequency). We don't 
know the exact value of each observation in the group. However, we do know 
that all the values in a group must be less than or equal to the upper boundary. 
0 Join the plotted points with a straight line. Joining them with a straight line 
shows that we consider each value in the group to be equally possible. 
We can estimate the median and quartiles easily from the graph. To find the median, 
go up to 50% on the vertical scale and then draw a horizontal line across to the 
cumulative frequency polygon, and then a vertical line down to the horizontal axis. 
The value where it hits the axis is the estimate of the median, Similarly, to find 
the quartiles, go up to 25% or 75%, go across to cumulative frequency polygon, 
and go down to horizontal axis to find lower and upper quartile, respectively. The 
underlying assumption behind these estimates is that all values in a group are evenly 
spread across the group. Figure 3.5 shows the cumulative frequency polygon for the 
Earth density measurements by Cavendish. 
'In this case, the population is the set of all possible Earth density measurements that Cavendish could 
have obtained from his experiment. This population is theoretical, as each of its elements was only brought 
into existence by Cavendish performing the experiment. 

36 
DlSfLAYlNG AND SUMMARlZlNG DATA 
I 
- 
1 
I 
4.8 
5.0 
5.2 
5.4 
5.6 
5.8 
6. 
8 
5.1 
5.4 
5.7 
6 
I 
4.8 
5.0 
5.2 5.3 5.4 5.5 5.6 
5.8 
6.0 
I 
I
I
I
,
I
 
I 
I 
rrgufe d.4 
aries. Note that the area is always proportional to frequency. 
niscograms or Oartn aensity measurements by Cavendtsh with dinerent bound- 

GRAPHICALLY COMPARING TWO SAMPLES 
37 
I 
I 
I 
I 
I 
I 
I 
4.8 
5.0 
5.2 
5.4 
5.6 
5.8 
6.0 
F@m? 3.5 Cumulative frequency polygon of Earth density measurements by Cavendish. 
3.2 GRAPHICALLY COMPARING TWO SAMPLES 
Sometimes we have the same variable recorded for two samples. For instance, we 
may have responses for the treatment group and control group from a randomized 
experiment. We want to determine whether or not the treatment has been effective. 
Often a picture can clearly show us this, and there is no need for any sophisticated 
statistical inference. The key to making visual comparisons between two data samples 
is “Don’t compare apples to oranges.” By that, we mean that the pictures for the two 
samples must be lined up, and with the same scale. Stacked dotplots and stacked 
boxplots, when they are lined up on the same axis, give a good comparison of the 
samples. Back-to-back stem-and-leaf diagrams are another good way of comparing 
two small data sets. The two samples use common stem, and the leaves from one 
sample are on one side of the stem, and the leaves from the other sample are on 
the other side of the stem. The leaves of the two sample are ordered, from smallest 
closest to stem to largest farthest away. We can put histograms back-to-back or stack 
them. We can plot the cumulative frequency polygons for the two samples on the 
same axis. If one is always to the left of the other, we can deduce that its distribution 
is shifted relative to the other. 
All of these pictures can show us whether there are any differences between the 
two distributions. For example, do the distributions seem to have the same location 
on the number line, or does one appear to be shifted relative to the other? Do the 
distributions seem to have the same spread, or is one more spread out than the other? 
Are the shapes similar? If we have more than two samples, we can do any of these 
pictures that is stacked. Of course, back-to-back ones only work for two samples. 
Example 3 Between 1879 and 1882, scientists were devising experiments for deter- 
mining the speed of light. Table 3.3 contains measurements collected by Michelson 
in a series of experiments on the speed of light. The first 20 measurements were 

38 
DISPLAYING AND SUMMARIZING DATA 
Michelson (1 879) 
850 
740 
Table 3.3 Michelson's speed-of-light measurements.' 
Michelson (1882) 
883 
816 
900 
930 
950 
980 
1000 
930 
760 
1000 
960 
1070 
850 
980 
880 
980 
650 
810 
1000 
960 
778 
682 
61 1 
105 1 
578 
774 
772 
573 
748 
85 1 
723 
796 
71 1 
599 
781 
796 
820 
696 
748 
797 
809 
a h h e  in table plus 2999000km/s 
Michelson 1882 .. . . 
. . .. . - .." . . 
I 
I 
I 
I 
. .  
. . . .  
Michelson 1879 
. .  . . . . . . . . . 
I 
I 
I 
I 
I 
580 
680 
780 
880 
980 
1080 
Figure 3.6 Dotplots of Michelson's speed-of-light measurements. 
made in 1879, and the next 23 supplementary measurements were made in 1882. The 
experiment and the data are described in Stigler (1977). 
Figure 3.6 shows stacked dotplots for the two data sets. Figure 3.7 shows stacked 
boxplots for the two data sets. The true value of the speed of light in the air is 
2999710. We see from these plots that there was a systematic error (bias) in the first 
series of measurements that was greatly reduced in the second. 
Back-to-back stem-and-leaf diagrams are another good way to show the relation- 
ship between two data sets. The stem goes in the middle. We put the leaves for one 
data set on the right side and put the leaves for the other on the lejL The leaves are 
ascending order moving away from the stem. Back-to-back stem-and-leaf diagrams 
are shown for Michelson's data in Figure 3.8. The stem is hundreds, and the leaf unit 
is 10. 

MEASURES O f  LOCATION 
39 
Michelson 1882 
~ I 
I 
I 
* 
j 
Michelson 1879 
I 
L -
7
,
 
600 
700 
800 
900 
1000 
1100 
Figure 3.7 Boxplot of Michelson’s speed-of-light measurements. 
977 
1 
98 
4412 
9998777 
210 
85 
5 
5 
6 
6 
7 
7 
8 
8 
9 
9 
10 
10 
leaf unit 10 
5 
4 
6 
1 
558 
033 
566888 
OOO 
7 
Figure 3.8 Back-to-back stem-and-leaf plots for Michelson’s data. 
3.3 MEASURES OF LOCATION 
Sometimes we want to summarize our data set with numbers. The most important 
aspect of the data set distribution is determining a value that summarizes its location 
on the number line. The most commonly used measures of location are the mean and 
the median. We will look at each ones’s advantages and disadvantages. 
Both the mean and the median are members of the trimmed mean family, which 
also includes compromise values between them, depending on the amount of trim- 
ming. We do not consider the mode (most common value) to be a suitable measure of 
location for the following reasons. For continuous data values, each value is unique 

40 
DlSPLAYlNG AND SUMMARlZlNG DATA 
if we measure it accurately enough. In many cases, the mode is near one end of the 
distribution, not the central region. The mode may not be unique. 
Mean: Advantages and Disadvantages 
The mean is the most commonly used measure of location, because of its simplicity 
and its good mathematical properties. The mean of a data set y1, . . . , yn is simply 
the arithmetic average of the numbers. 
The mean is simple and very easy to calculate. You just make one pass 
through the numbers and add them up. Then divide the sum by the size of the sample. 
The mean has good mathematical properties. The mean of a sum is the 
sum of the means. For example, if y is total income, u is "earned income" (wages and 
salaries), v is "unearned income" (interest, dividends, rents), and w is "other income" 
(social security benefits and pensions, etc.). Clearly, a persons total income is the 
sum of the incomes he or she receives from each source yi = ui + zli + wi. Then 
g = n + v + s .  
So it doesn't matter if we take the means from each income source and then add them 
together to find the mean total income, or add the each individuals incomes from all 
sources to get hisher total income and then take the mean of that. We get the same 
value either way. 
The mean combines well. The mean of a combined set is the weighted average 
of the means of the constituent sets, where weights are proportions each constituent 
set is to the combined set. For example, the data may come from two sources, males 
and females who had been interviewed separately. The overall mean would be the 
weighted average of the male mean and the female mean where the weights are the 
proportions of males and females in the sample. 
The mean is the first moment or center of gravity of the numbers. We 
can think of the mean as the balance point if an equal weight was placed at each of the 
data points on the (weightless) number line. The mean would be the balance point of 
the line. This leads to the main disadvantage of the mean. It is strongly influenced 
by outliers. A single observation much bigger than the rest of the observations has a 
large effect on the mean. That makes using the mean problematic with highly skewed 
data such as personal income. Figure 3.9 shows how the mean is influenced by an 
outlier. 
Calculating mean for grouped data. When the data have been put in a fre- 
quency table, we only know between which boundaries each observation lies. We 

MEASURES OF LOCATION 
41 
Figure 3.9 The mean as the balance point of the data is affected by moving the outlier. 
no longer have the actual data values. In that case there are two assumptions we can 
make about the actual values. 
1. All values in a group lie at the group midpoint. 
2. All the values in a group are evenly spread across the group. 
Fortunately, both these assumptions lead us to the same calculation of the mean value. 
The total contribution for all the observations in a group is the midpoint times the 
frequency under both assumptions. 
.
J
 
j=1 
where nj is the number of observations in the jth interval, n is the total number of 
observations, and mj is the midpoint of the j t h  interval. 
Median: Advantages and Disadvantages 
The median of a set of numbers is the number such that 50% of the numbers are less 
than or equal to it, and 50% of the numbers are greater than or equal to it. Finding 
the median requires us to sort the numbers. It is the middle number when the sample 
size is odd, or it is the average of the two numbers closest to middle when the sample 
size is even. 
m = y n-tl 
[-TI. 
The median is not influenced by outliers at all. This makes it very suitable 
for highly skewed data like personal income. This is shown in Figure 3.10. However, 
it does not have same good mathematical properties as mean. The median of a sum 
is not necessarily the sum of the medians. Neither does it have good combining 
properties similar to those of the mean. The median of the combined sample is not 
necessarily the weighted average of the medians. For these reasons, the median is 
not used as often as the mean. It is mainly used for very skewed data such as incomes 
where there are outliers which would unduly influence the mean, but don’t affect the 
median. 

42 
DISPLAYING AND SUMMARIZING DATA 
Figure 3.70 
outlier. 
The median as the middle point of the data is not affected by moving the 
Trimmed mean. We find the trimmed mean with degree of trimming equal to k by 
first ordering the observations, then trimming the lower k and upper k order statistics, 
and taking the average of those remaining. 
We see that 30 (where there is no trimming) is the mean. If n is odd and we let 
k = ;, 
then %k is the median. Similarly, if n is even and we let k = y, 
then 3k is 
the median. If k is small, the trimmed mean will have properties similar to the mean. 
If k is large, the trimmed mean has properties similar to the median. 
3.4 MEASURES OF SPREAD 
After we have determined where the data set is located on the number line, the next 
important aspect of the data set distribution is determining how spread out the data 
distribution is. If the data are very variable, the data set will be very spread out. So 
measuring spread gives a measure of the variability. We will look at some of the 
common measures of variability. 
Range: Advantage and Disadvantage 
The range is the largest observation minus the smallest: 
R = Y[n] - Y[1] . 
The range is very easy to find. However, the largest and smallest observation are 
the observations that are most likely to he outliers. Clearly, the range is extremely 
influenced by outliers. 
lnterquartile Range: Advantages and Disadvantages 
The interquartile range measures the spread of the middle 50% of the observations. 
It is the third quartile minus first quartile: 
IQR = Q3 - Q I .  
The quartiles are not outliers, so the interquartile range is not influenced by outliers. 
Nevertheless, it is not used very much in inference because like the median it doesn't 
have good math or combining properties. 

MEASURES OF SPREAD 
43 
Variance: Advantages and Disadvantages 
The variance of a data set is the average squared deviation from the mean.2 
In physical terms, it is the second moment of inertia about the mean. Engineers 
refer to the variance as the MSD, mean squared deviation. It has good mathematical 
properties, although more complicated than those for the mean. The variance of a 
sum (of independent variables) is the sum of the individual variances. 
It has good combining properties, although more complicated than those for the 
mean. The variance of a combined set is the weighted average of the variances 
of the constituent sets, plus the weighted average of the squares of the constituent 
means away from the combined mean, where the weights are the proportions that 
each constituent set is to the combined set. 
Squaring the deviations from the mean emphasizes the observations far from the 
mean. Those observations have large magnitude in a positive or negative direction 
already, and squaring them makes them much larger still, and all positive. Thus the 
variance is very influenced by outliers. The variance is in squared units. Thus its size 
is not comparable to mean. 
Calculating variance for grouped data. The variance is the average squared 
deviation from the mean. When the data have been put in a frequency table, we no 
longer have the actual data values. In that case there are two assumptions we can 
make about the actual values. 
1. All values in a group lie at the group midpoint. 
2. All the values in a group are evenly spread across the group. 
Unfortunately, these two assumptions lead us to different calculation of the variance. 
Under the first assumption we get the approximate formula 
where n3 is the number of observations in the j t h  interval, n is the total number 
of observations, m3 is the midpoint of the j t h  interval. This formula only contains 
between-group variation, and ignores the variation for the observations within the 
2Note that we are defining the variance of a data set using the divisor n. We aren’t making any distinction 
over whether our data set is the whole population or only a sample from the population. Some books 
define the variance of a sample data set using divisor R -- 1. One degree of freedom has been lost because 
for a sample, we are using the sample mean instead of the unknown population mean. When we use the 
divisor n - 1, we are calculating the sample estimate of the variance, not the variance itself. 

44 
DISPLAYING AND SUMMARIZING DATA 
same group. Under the second assumption we add in the variation within each group 
to get the formula 
where Rj is the upper boundary minus the lower boundary for the jth group. 
Standard Deviation: Advantages and Disadvantages 
The standard deviation is the square root of the variance. 
I .  
?l 
Engineers refer to it as the RMS, root mean square. It is not as affected by outliers as 
the variance is, but it is still quite affected. It inherits good mathematical properties 
and good combining properties from the variance. The standard deviation is the most 
widely used measure of spread. It is in the same units as mean, so its size is directly 
comparable to the mean. 
3.5 
DISPLAYING RELATIONSHIPS BETWEEN TWO OR MORE 
VARIABLES 
Sometimes our data are measurements for two variables for each experimental unit. 
This is called bivariate data. We want to investigate the relationship between the two 
variables. 
Scatterplot 
The scatterplot is just a two-dimensional dotplot. Mark off the horizontal axis for 
the first variable, the vertical axis for the second. Each point is plotted on the graph. 
The shape of the "point cloud" gives us an idea as to whether the two variables are 
related, and if so, what is the type of relationship. 
When we have two samples of bivariate data and want to see if the relationship 
between the variables is similar in the two samples, we can plot the points for both 
samples on the same scatterplot using different symbols so we can tell them apart. 
Example 4 The BearsmtwJile stored in Minitab contains 143 measurements on wild 
bears that were anesthetized, measured, tagged, and released. Figure 3.1 I shows 
a scatterplot of head length versus head width for these bears. From this we can 
observe that head length and head width are related. Bears with large width heads 
tend to have heads that are long. We can also see that male bears tend to have larger 
heads than female bears. 

DISPLAYING RELATIONSHIPS BETWEEN TWO OR MORE VARIABLES 
45 
+ 
+ 
+ 
+ 
f
f
l
o
+
+
 
+
+
+
 
+
A
e
f
f
l
*
 
i
t
 
+
o
 
o
m
+
e
e
 
m
e
s
e
e
 
o
o
o
o
m
 
0 
+ 
o
t
f
f
l
+
 
t
o
m
+
 
0 
+
o
-
i
 
f
f
l
+
+
 
+ 
+
+
 
+ 
+
+
 
+ 
+
+
 
+ 
+ 
* - 
+
+
 
+ 
Figure 3.7 7 Head length versus head width in black bears. 
Scatterplot Matrix 
Sometimes our data consists of measurements of several variables on each experi- 
mental unit. This is called multivariate data. To investigate the relationships between 
the variables, form a scatterplot matrix. This means that we construct the scatterplot 
for each pair of variables, and then display them in an array like a matrix. We look 
at each scatterplot in turn to investigate the relationship between that pair of the 
variables. More complicated relationships between three or more of the variables 
may be hard to see on this plot. 
Example 4 (continued) Figure 3.12 shows a scatterplot matrix showing scatter- 
plots of head length, head width, neck girth, length, chest girth, and weight for the 
bear measurement data. We see there are strong positive relationships among the 
variables, and some of them appear to be nonlinear: 

46 
DlSPLAYlNG AND SUMMARIZING DATA 
Figure 3.72 Scatterplot matrix of bear data. 
3.6 MEASURES OF ASSOCIATION FOR TWO OR MORE VARIABLES 
Covariance and Correlation between Two Variables 
The covariance of two variables is the average offirst variable minus its mean times 
second variable minus its mean: 
This measures how the variables vary together. Correlation between two variables is 
the covariance of the two variables divided by product of standard deviations of the 
two variables. This standardizes the correlation to lie between - 1 and + 1. 
Correlation measures the strength of the linear relationship between two variables. 
A correlation of +1 indicates that the points lie on a straight line with positive slope. 
A correlation of - 1 indicates that the points lie on a straight line with negative slope. 
A positive correlation that is less than one indicates that the points are scattered, but 
generally low values of the first variable are associated with low values of the second, 

MAIN POINTS 
47 
Head.L 
Head.W 
Neck.G 
Length 
Chest.G 
Weight 
Table 3.4 Correlation matrix for bear data 
Head.L 
Head.W 
Neck.G 
Length 
Chest.G 
1.000 
.744 
362 
39.5 
.854 
,744 
1 .ooo 
305 
,736 
.756 
362 
305 
1 .ooo 
,873 
.940 
395 
.736 
373 
1.000 
389 
.854 
.756 
.940 
389 
1 .ooo 
,833 
.756 
.943 
275 
.966 
Weight 
333 
,756 
,943 
375 
.966 
1 .ooo 
and high values of the first are associated with high values of the second. The higher 
the correlation, the more closely the points are bunched around a line. A negative 
correlation has low values of the first associated with high values of the second, and 
high values of the first associated with low values of the second. A correlation of 
0 indicates that there is no association of low values or high values of the first with 
either high or low values of the second. It does not mean the variables are not related, 
only that they are not linearly related. 
When we have more than two variables, we put the correlations in a matrix. The 
correlation between x and y equals the correlation between y and x, so the correlation 
matrix is symmetric about the main diagonal. The correlation of any variable with 
itself equals one. 
Example 4 (continued) The correlation matrix for the bear data is given in Table 
3.4. We see that all the variables are correlated with each other. Looking at the 
matrix plot we see that Head.L and Head. W have a correlation of .744, and the 
scatterplot of those two variables is spread out. We see that the Head.L and Length 
have a higher correlation of .895, and on the scatterplot of those variables, we see 
the points lie much closer to a line. We see that Chest.G and Weight are highly 
correlated at ,966. On the scatterplot we see those points lie much closer to a line, 
although we can also see that actually they seem to lie on a curve that is quite close 
to a line. 
Main Points 
0 Data should always be looked at in several ways as the first stage in any 
statistical analysis. Often a good graphical display is enough to show what is 
going on, and no further analysis is needed. Some elementary data analysis 
tools are: 
o Order Statistics. The data when ordered smallest to largest. y[11, . . , y ~ ~ ] .  
o Median. The value that has 50% of the observations above it and 50% of 
the observations below it. This is 

48 
DISPLAYING AND SUMMARIZING DATA 
It is the middle value of the order statistics when n is odd. When n is even, 
the median is the weighted average of the two closest order statistics: 
The median is also known as the second quartile. 
o Lower quartile. The value that 25% of the observations are below it and 
75% of the observations are above it. It is also known as the first quartile. 
It is 
&I = Y[+]. 
If 
is not an integer, we find it by taking the weighted average of the 
two closest order statistics. 
o Upper quartile. The value that 75% of the observations are below it 
and 25% of the observations are above it. It is also known as the upper 
quartile. It is 
Q3 = z [ q 1  
. 
If w
i
s
 
not an integer, the quartile is found by taking the weighted 
average of the two closest order statistics. 
0 When we are comparing samples graphically, it is important that they be on the 
same scale. We have to be able to get the correct visual comparison without 
reading the numbers on the axis. Some elementary graphical data displays are: 
0 Stem-and-leaf diagram. An quick and easy graphic which allows us to 
extract information from a sample. A vertical stem is drawn with a num- 
bers up to stem digit along linear scale. Each number is represented using 
its next digit as a leaf unit at the appropriate place along the stem. The 
leaves should be ordered away from the stem. It is easy to find (approx- 
imately) the quartiles by counting along the graphic. Comparisons are 
done with back-to-back stem-and-leaf diagrams. 
0 Boxplot. A graphic along a linear axis where the central box contains the 
middle 50% of the observation, and a whisker goes out from each end of 
the box to the lowest and highest observation. There is a line through the 
box at the median. So it is a visual representation of the five numbers 
yj11, &I, Q2, Q 3 ,  ylnl that give a quick summary of the data distribution. 
Comparisons are done with stacked boxplots. 
0 Histogram. A graphic where the group boundaries are put on a linear 
scaled horizontal axis. Each group is represented by a vertical bar where 
the area of the bar is proportional to the frequency in the group. 
A graphic where the group 
boundaries are put on a linearly scaled horizontal axis. The point (lower 
boundary of lowest group, 0) and the points (upper group boundary, cu- 
mulative frequency) are plotted and joined by straight lines. The median 
and quartiles can be found easily using the graph. 
0 Cumulative frequency polygon (ogive). 

MAlN POlNTS 
49 
0 It is also useful to summarize the data set using a few numerical summary 
statistics. The most important summary statistic of a variable is a measure of 
location which indicates where the values lie along the number axis. Some 
possible measures of location are: 
o Mean. The average of the numbers. It is easy to use, has good mathemat- 
ical properties, and combines well. It is the most widely used measure of 
location. It is sensitive to outliers, so it is not particularly good for heavy 
tailed distributions. 
o Median. The middle order statistic, or the average of the two closest to 
the middle. This is harder to find as it requires sorting the data. It is 
not affected by outliers. The median doesn’t have the good mathematical 
properties or good combining properties of the mean. Because of this, it 
is not used as often as the mean. Mainly it is used with distributions that 
have heavy tails or outliers, where it is preferred to the mean. 
o Trimmed mean. This is a compromise between the mean and the median. 
Discard the k largest and the k smallest order statistics and take the 
average of the rest. 
0 The second important summary statistic is a measure of spread, which shows 
how spread out are the numbers. Some commonly used measures of spread 
are : 
o Range. This is the largest order statistic minus the smallest order statistic. 
Obviously very sensitive to outliers. 
o Interquartile range (IQR). This is the upper quartile minus the lower 
quartile. It measures the spread of the middle 50% of the observations. 
It is not sensitive to outliers. 
o Variance. The average of the squared deviations from the mean. Strongly 
influenced by outliers. The variance has good mathematical properties, 
and combines well, but it is in squared units and is not directly comparable 
to the mean. 
o Standard deviation. The square root of the variance. This is less sensitive 
to outliers than the variance and is directly comparable to the mean since 
it is in the same units. It inherits good mathematical properties and 
combining properties from the variance. 
0 Graphical display for relationship between two or more variables. 
o Scatterplot. Look for pattern. 
o Scatterplot matrix. An array of scatterplots for all pairs of variables. 
0 Correlation is a numerical measure of the strength of the linear relationship 
between the two variables. It is standardized to always lie between -1 and 
+ 1. If the points lie on a line with negative slope, the correlation is - 1, and if 

50 
DISPLAYING AND SUMMARIZING DATA 
they lie on a line with positive slope, the correlation is +l. A correlation of 0 
doesn’t mean there is no relationship, only that there is no linear relationship. 
Exercises 
3.1 A study on air pollution in a major city measured the concentration of sulphur 
dioxide on 25 summer days. The measurements were: 
3 
9 
16 
23 
3 
11 
17 
25 
5 
13 
18 
26 
7 
13 
19 
27 
9 
14 
23 
28 
29 
35 
43 
44 
46 
(a) Form a stem-and-leaf diagram of the sulphur dioxide measurements. 
(b) Find the median, lower quartile, and upper quartile of the measurements. 
(c) Sketch a boxplot of the measurements. 
3.2 Dutch elm disease is spread by bark beetles that breed in the diseased wood. 
A sample of 100 infected elms was obtained, and the number of bark beetles 
on each tree was counted. The data are summarized in the following table: 
Boundaries 
Frequency 
0 < x <  50 
50 < x 5 100 
100 < x 5 150 
150 < x 5 200 
200 < x < 400 
8 
24 
33 
21 
14 
(a) Graph a histogram for the bark beetle data. 
(b) Graph a cumulative frequency polygon of the bark beetle data. Show the 
median and quartiles on your cumulative frequency polygon. 
3.3 A manufacturer wants to determine whether the distance between two holes 
stamped into a metal part is meeting specifications. A sample of 50 parts was 
taken, and the distance was measured to nearest tenth of a millimeter. The 
results were: 

EXERCISES 
57 
300.6 
300.0 
300.5 
299.9 
300.4 
299.7 
300.5 
300.7 
300.2 
300.3 
299.7 
300.1 
299.6 
300.4 
300.2 
300.1 
300.1 
300.4 
300.3 
299.9 
300.2 
299.9 
300.7 
299.8 
299.4 
299.9 
299.9 
300.0 
300.5 
300.1 
300.0 
300.2 
299.9 
300.4 
300.6 
300.0 
299.8 
300.1 
300.0 
300.2 
300.1 
300.1 
300.2 
300.4 
299.8 
300.0 
300.2 
300.0 
300.1 
299.5 
(a) Form a stem-and-leaf diagram of the measurements. 
(b) Find the median, lower quartile, and upper quartile of the measurements. 
(c) Sketch a boxplot of the measurements. 
(d) Put the measurements in a frequency table with the following classes: 
Boundaries 
Frequency 
299.2 < x 5 299.6 
299.6 < x 5 299.8 
299.8 < x 5 300.0 
300.0 < x 5 300.2 
300.2 < x 5 300.4 
300.4 < x 5 300.8 
(e) Construct a histogram of the measurements. 
(f) Construct a cumulative frequency polygon of the measurements. Show 
the median and quartiles. 
3.4 The manager of a government department is concerned about the efficiency in 
which his department serves the public. Specifically, he is concerned about the 
delay experienced by members of the public waiting to be served. He takes a 
sample of 50 arriving customers, and measures the time each waits until service 
begins. The times (rounded off to the nearest second) are: 

52 
DISPLAYING AND SUMMARIZING DATA 
98 
5 
6 
39 
31 
46 
129 
17 
1 
64 
40 
121 
88 
102 
50 
123 
50 
20 
37 
65 
75 
191 
110 
28 
44 
47 
6 
43 
60 
12 
150 
16 
182 
32 
5 
106 
32 
26 
87 
137 
44 
13 
18 
69 
107 
5 
53 
54 
173 
118 
Form a stem-and-leaf diagram of the measurements. 
Find the median, lower quartile, and upper quartile of the measurements. 
Sketch a boxplot of the measurements. 
Put the measurements in a frequency table with the following classes: 
Boundaries 
Frequency 
o < x <  20 
20 < x 5 40 
40 < x 5 60 
60 < x 5 80 
80 < x 5 100 
100 < x 5 200 
Construct a histogram of the measurements. 
Construct a cumulative frequency polygon of the measurements. Show 
the median and quartiles. 
3.5 A random sample of 50 families reported the dollar amount they had available 
as a liquid cash reserve. The data have been put in the following frequency 
table: 

EXERCISES 
53 
Boundaries 
0 < x 5 
500 
500 < x 5 1000 
1000 < x 5 2000 
2000 < x 5 4000 
4000 < x 5 6000 
6000 < x < 10000 
Frequency 
17 
15 
7 
5 
3 
3 
(a) Construct a histogram of the measurements. 
(b) Construct a cumulative frequency polygon of the measurements. Show 
(c) Calculate the grouped mean for the data. 
the.median and quartiles. 
3.6 In this exercise we see how the default setting in the Minitab boxplot command 
can be misleading, since it doesn’t take the sample size into account. We 
will generate three samples of different sizes from the same distribution, and 
compare their Minitab boxplots. Generate 250 normal (0,l) observations and 
put them in column c l  by pulling down the calc menu to the random data 
command over to normal and filling in the dialog box. Generate lo00 normal 
(0, I) observations the same way and put them in column c2, and generate 4000 
normal (0,l) observations the same way and put them in column c3. Stack 
these three columns by pulling down the manip menu down to stacWunstack 
and over to stack columns and filling in the dialog box to put the stacked column 
into c4, with subscripts into c5. Form stacked boxplots by pulling down graph 
menu to boxplot command and filling in dialog box. Y is c4 and x is c5. 
(a) What do you notice from the resulting boxplot? 
(b) Which sample seems to have a heavier tail? 
(c) Why is this misleading? 
(d) Click on the boxplot. Then pull down the editor menu down to select 
item and over to outlier symbols. Click on custom in the dialog box, and 
select dot. 
(e) Is the graph still as misleading as the original? 
3.7 Barker and McGhie (1984) collected 100 slugs from the species L i m a  maximus 
around Hamilton, New Zealand. They were preserved in a relaxed state, and 
their length in millimeters (mm) and weight in grams (8) were recorded. Thirty 
of the observations are shown below. The full data are in the Minitab worksheet 
slug. mtw. 

54 
DISPLAYING AND SUMMARIZING DATA 
Length 
Weight 
(mm) 
(g) 
73 
3.68 
78 
5.48 
75 
4.94 
69 
3.47 
60 
3.26 
74 
4.36 
85 
6.44 
86 
8.37 
82 
6.40 
85 
8.23 
Length 
Weight 
21 
0.14 
26 
0.35 
26 
0.29 
36 
0.88 
16 
0.12 
35 
0.66 
36 
0.62 
22 
0.17 
24 
0.25 
42 
2.28 
(mm) 
(g) 
Length 
Weight 
(mm) 
(g) 
75 
4.94 
78 
5.48 
22 
0.36 
61 
3.16 
59 
1.91 
78 
8.44 
90 
13.62 
93 
8.70 
71 
4.39 
94 
8.23 
(a) Plot weight on length using Minitab. What do you notice about the shape 
of the relationship? 
(b) Often when we have a nonlinear relationship, we can transform the vari- 
ables by taking logarithms and achieve linearity. In this case, weight is 
related to volume which is related to length times width times height. 
Taking logarithms of weight and length should give a more linear re- 
lationship. Plot log(weight) on log(1ength) using Minitab. Does this 
relationship appear to be linear? 
(c) From the scatterplot of log(weight) on log(1ength) can you identify any 
points that do not appear to fit the pattern? 

4 
Logic, Probability, 
and Uncertainty 
Most situations we deal with in everyday life are not completely predictable. If I 
think about the weather tomorrow at noon, I cannot be certain whether it will or will 
not be raining. I could contact the Meteorological Service and get the most up-to-date 
weather forecast possible, which is based on the latest available data from ground 
stations and satellite images. The forecast could be that it will be a fine day. I decide 
to take that forecast into account and not take my umbrella. Despite the forecast, it 
could rain and I could get soaked going to lunch. There is always uncertainty. 
In this chapter we will see that deductive logic can only deal with certainty. This 
is of very limited use in most real situations. We need to develop inductive logic that 
allows us to deal with uncertainty. 
Since we can’t completely eliminate uncertainty, we need to model it. In real life 
when we are faced with uncertainty, we use plausible reasoning. We adjust our belief 
about something, based on the occurrence or nonoccurrence of something else. We 
will see how plausible reasoning should be based on the rules of probability which 
were originally derived to analyze the outcome of games based on random chance. 
Thus the rules of probability extend logic to include plausible reasoning where there 
is uncertainty. 
Introduction to Bayesian Statistics, Second Edition. B y  William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
55 

56 
LOGIC, PROBAB/L/TY AND UNCERTANTY 
Figure 4.7 
"If A is true then B is true." Deduction is possible. 
4.1 
DEDUCTIVE LOGIC AND PLAUSIBLE REASONING 
Suppose we know "If proposition A is true, then proposition B is true." We are 
then told "proposition A is true." Therefore we know that "B is true." It is the only 
conclusion consistent with the condition. This is a deduction. 
Again suppose we know "If proposition A is true, then proposition B is true." Then 
we are told "B is not true." Therefore we know that "A is not true." This is also a 
deduction. When we determine a proposition is true by deduction using the rules of 
logic, it is certain. Deduction works from the general to the particular. 
We can represent propositions using diagrams. Propositions "A is true" and "B is 
true" are each represented by the interior of a circle. The proposition "$A is true, 
then B is true" is represented by having circle representing A lie completely inside 
B. This is shown in Figure 4.1. The essence of the first deduction is that if we are 
in a circle A that lies completely inside circle B, then we must be inside circle B. 
Similarly, the essence of the second induction is that if we are outside of a circle B 
that completely contains circle A, then we must be outside circle A. 
Other propositions can be seen in the diagram. Proposition "A and B are both 
true" is represented by the intersection, the region in both the circles simultaneously. 
In this instance, the intersection equals A by itself. The proposition "A or B is true" 
is represented by the union, region in either one or the other, or both of the circles. 
In this instance, the union equals B by itself. 
On the other hand, suppose we are told "A is not true." What can we now say about 
B? Traditional logic has nothing to say about this. Both "B is true" and "B is not 
true" are consistent with the conditions given. Some points outside circle A are inside 
circle B, and some are outside circle B. No deduction is possible. Intuitively though, 
we would now believe that it was less plausible that B is true than we previously did 
before we were told "A is not true." This is because one of the ways B could be true, 
namely that A and B are both true is now no longer a possibility. And the ways that 
B could be false have not been affected. 

DEDUCTIVE LOGIC AND PLAUSIBLE REASONING 
57 
Figure 4.2 
false." No deduction is possible here. 
Both "A is true" and " A is false" are consistent with both "B is true" and "B is 
Similarly, when we are told "B is true," traditional logic has nothing to contribute. 
Both "A is true" and "A is not true" are consistent with the conditions given. Never- 
theless, we see that "B is true" increases the plausibility of "A is true" because one of 
the ways A could be false, namely both A and B are false is no longer possible, and 
the ways that A are true have not been affected. 
Often propositions are related in such a way that no deduction is possible. Both 
"A is true" and "A is false" are consistent with both "B is true" and "B is false." Figure 
4.2 shows this by having the two circles intersect, and neither is completely inside 
the other. 
Suppose we try to use numbers to measure plausibility of propositions. When we 
change our plausibility for some proposition on the basis of the occurrence of some 
other proposition, we are making an induction. Induction works from the particular 
to the general. 
Desired Properties of Plausibility Measures 
1. Degrees of plausibility are represented by nonnegative real numbers. 
2. They qualitatively agree with common sense. Larger numbers mean greater 
plausibility. 
3. If a proposition can be represented more than one way, then all representations 
must give the same plausibility. 
4. We must always take all the relevant evidence into account. 
5 .  Equivalent states of knowledge are always given the same plausibility. 
R. T. Cox showed that any set of plausibilities that satisfies the desired properties 
given above must operate according to the same rules as probability. Thus the sensible 
way to revise plausibilities is by using the rules of probability. Bayesian statistics 

58 
LOGIC, PROBABILIP/: AND UNCERTAINTY 
uses the rules of probability to revise our belief given the data. Probability is used 
as an extension of logic to cases where deductions cannot be made. Jaynes (1 995) 
gives an excellent discussion on using probability as logic. 
4.2 PROBABILITY 
We start this section with the idea of a random experiment. In a random experiment, 
though we make the observation under known repeatable conditions, the outcome is 
uncertain. When we repeat the experiment under identical conditions, we may get a 
different outcome. We start with the following definitions: 
Random experiment. An experiment that has an outcome that is not completely 
predictable. We can repeat the experiment under the same conditions and not 
get the same result. Tossing a coin is an example of a random experiment. 
0 Outcome. The result of one single trial of the random experiment. 
0 Sample space. The set of all possible outcomes of one single trial of the 
random experiment. We denote it 0. The sample space contains everything 
we are considering in this analysis of the experiment, so we also can call it the 
universe. In our diagrams we will call it U. 
Event. Any set of possible outcomes of a random experiment. 
Possible events include the universe, U, and the set containing no outcomes, the 
empty set 4. From any two events E and F we can create other events by the 
following operations. 
Union of two events. The union of two events E and F is the set of outcomes 
in either E 
F (inclusive or). Denoted E U F 
Intersection of two events. The intersection of two events E and F is the set 
of outcomes in both E and F simultaneously. Denoted E n F. 
not in E. Denoted E 
Complement of an event. The complement of an event E is the set of outcomes 
We will use Venn diagrams to illustrate the relationship between events. Events 
are denoted as regions in the universe. The relationship between two events depends 
on the outcomes they have in common. If all the outcomes in one event are also in 
the other event, the first event is a subset of the other. This is shown in Figure 4.3. 
If the events have some outcomes in common, but each has some outcomes that 
are not in the other, they are intersecting events. This is shown in Figure 4.4. Neither 
event is contained in the other. 
If the two events have no outcomes in common, they are mutually exclusive events. 
In that case the occurrence of one of the events excludes the occurrence of the other, 
and vice versa. They are also referred to as disjoint events. This is shown in Figure 
4.5. 

AXIOMS OF PROBABILITY 
59 
U 
I 
Figure 4.3 Event F is a subset of event E. 
Figure 4.4 E and F are intersecting events. 
4.3 AXIOMS OF PROBABILITY 
The probability assignment for a random experiment is an assignment of probabilities 
to all possible events the experiment generates. These probabilities are real numbers 
between 0 and 1. The higher the probability of an event, the more likely it is to occur. 
A probability that equals 1 means that the event is certain to occur, and a probability 
of 0 means that the event cannot possibly occur. To be consistent, the assignment of 
probabilities to events must satisfy the following axioms. 
1. P(A) 2 0 for any event A. (Probabilities are nonnegative.) 
2. P( U )  = 1. (Probability of universe = 1. Some outcome occurs every time you 
conduct the experiment.) 
3. If A and B are mutually exclusive events, then P(A U B )  = P ( A )  + P(B). 
(Probability is additive over disjoint events.) 
The other rules of probability can be proved from the axioms. 

60 
LOGIC, PROBABll/73’, AND UNCERTAINN 
Figure 4.5 Event E and event F are mutually exclusive or disjoint events. 
1. P(4) = 0. (The empty set has zero probability.) 
0 U = U U 4 and U n 4 = 4. Therefore by axiom 3 
0 1 = 1 + P(4) . 
9ed 
2. P(A) = 1 - P(A). (The probability of a complement of an event.) 
0 U = A U A and A n 2 = 4 . Therefore by axiom 3 
0 1 = P(A) + P(A). 
9ed 
3. P(A U B )  = P(A) + P(B) - P(A n B). (The addition rule of probability.) 
0 A U B = A U ( A  n B )  and they are disjoint. Therefore by axiom 3 
0 B = ( A  n B )  U ( A  n B )  , and they are disjoint. Therefore by axiom 3 
0 P(B) = P ( A  n B )  + P ( A  n B). Substituting this in previous equation 
0 P ( A u B ) = P ( A ) + P ( B ) - P ( A n B )  . 
0 P(A U B )  = P(A) + &'(An B )  . 
gives 
9ed 
An easy way to remember this rule is to look at the Venn diagram of the events. 
The probability of the part A n B has been included twice, once in P(A) and 
once in P(B), so it has to be subtracted out once. 
4.4 JOINT PROBABILITY AND INDEPENDENT EVENTS 
Figure 4.6 shows the Venn diagram for two events A and Bin the universe U. The joint 
probability of events A and B is the probability that both events occur simultaneously, 

JOINT PROBABILITYAND /NDEP€NDENT EVENTS 
61 
U 
I 
Figure 4.6 Two events A and B in the universe U. 
on the same repetition of the random experiment. This would be the probability of 
the set of outcomes that are in both event A and event B, the intersection A n B. In 
other words the joint probability of events A and B is P(A n B), the probability of 
their intersection. 
If event A and event B are independent, then P ( A  n B )  = P(A) x P(B). The 
joint probability is the product of the individual probabilities. If that does not hold, 
the events are called dependent events. Note that whether or not two events A and B 
are independent or dependent depends on the probabilities assigned. 
Distinction between independent events and mutually exclusive events. 
People often get confused between independent events and mutually exclusive events. 
This semantic confusion arises because the word independent has several meanings. 
The primary meaning of something being independent of something else is that the 
second thing has no affect on the first. This is the meaning of the word independent 
we are using in the definition of independent events. The occurrence of one event 
does not affect the occurrence or nonoccurrence of the other events. 
There is another meaning of the word independent. That is the political meaning 
of independence. When a colony becomes independent of the mother country, it 
becomes a distinct separate country. That meaning is covered by the definition of 
mutually exclusive or disjoint events. 
Independence of two events is not a property of the events themselves, rather it is a 
property that comes from the probabilities of the events and their intersection. This is 
in contrast to mutually exclusive events, which have the property that they contain no 
elements in common. Mutually exclusive events with nonnegative probability cannot 
be independent. Their intersection is the empty set, so it must have probability zero, 
which cannot equal the product of the probabilities of the two events! 

62 
LOGIC, PROBABILIp/: AND UNCERTAINTY 
Figure 4.7 The reduced universe, given event A has occurred. 
Marginal Probability. The probability of one of the events A, in the joint event 
setting is called its marginal probability. It is found by summing P ( A  n B )  and 
P(A n 8) using the axioms of probability. 
0 A = ( A  n B )  u ( A  n 8) , and they are disjoint. Therefore by axiom 3 
0 P(A) = P ( A  n B )  + P ( A  n 8) . The marginal probability of event A is found 
by summing its disjoint parts. 
qed 
4.5 CONDITIONAL PROBABILITY 
If we know that one event has occurred, does that affect the probability that another 
event has occurred? To answer this, we need to look at conditional probability. 
Suppose we are told that the event A has occurred. Everything outside of A is no 
longer possible. We only have to consider outcomes inside event A. The reduced 
universe U, = A. The only part of event B that is now relevant is that part which 
is also in A. This is B n A. Figure 4.7 shows that, given event A has occurred, the 
reduced universe is now the event A, and the only relevant part of event B is B n A. 
Given that event A has occurred, the total probability in the reduced universe must 
equal 1. The probability of B given A is the unconditional probability of that part of 
B that is also in A, multiplied by the scale factor &. 
That gives the conditional 
probability of event B given event A: 
P ( A  n B )  
P(A) . 
P(B1A) = 
(4.1) 
We see that the conditional probability P(B1A) is proportional to the joint probability 
P ( A  n B )  but has been rescaled so the probability of the reduced universe equals 1. 

BAYES THEOREM 
63 
Conditional probability for independent events. Notice that when A and B 
are independent events we have 
P(B1A) = P ( B ) ,  
since P(B n A) = P(B) x P(A) for independent events, and the factor P(A) 
will cancel out. Knowledge about A does not affect the probability of B occurring 
when A and B are independent events! This shows that the definition we used for 
independent events is a reasonable one. 
Multiplication rule. Formally, we could reverse the roles of the two events A and 
B. The conditional probability of A given B would be 
P ( A  n B )  
P(AIB) = -
~
.
 
P(B) 
However, we will not consider the two events the same way. B is an unobservable 
event. That is, the occurrence or nonoccurrence of event B is not observed. A 
is an observable event that can occur either with event B or with its complement 
B. However, the chances of A occurring may depend on which one of B or B has 
occurred. In other words, the probability of event A is conditional on the occurrence or 
nonoccurrence of event B. When we clear the fractions in the conditional probability 
formula we get 
(4.2) 
This is known as the multiplication rule for probability. It restates the conditional 
probability relationship of an observable event given an unobservable event in a way 
that is useful for finding the joint probability P ( A  n B ) .  Similarly, 
P ( A  n B )  = P(B) x P(A1B). 
P ( A  n B) = P ( B )  x P(AIB). 
4.6 BAYES’ THEOREM 
From the definition of conditional probability 
We know that the marginal probability of event A is found by summing the proba- 
bilities of its disjoint parts. Since A = ( A  n B )  U ( A  n B) and clearly ( A  n B )  and 
( A  n B) are disjoint, 
P(A) = P ( A  n B )  + P ( A  n B) . 
We substitute this into the definition of conditional probability to get 
P(A n B )  
P ( A n B )  + P ( A n B )  ’ 
P(BIA) = ____ 

64 
LOGIC, PROBABlLl7Y AND UNCERTAINTY 
Now we use the multiplication rule to find each of these joint probabilities. This 
gives Bayes’ theorem for a single event: 
Summarizing, we see Bayes’ theorem is a restatement of the conditional probability 
P(BIA) where: 
1. The probability of A is found as the sum of the probabilities of its disjoint 
parts, ( A  n B) and ( A  n B), and 
2. Each of the joint probabilities are found using the multiplication rule. 
The two important things to note are that the union of B and B is the whole universe 
U, and that they are disjoint. We say that events B and B partition the universe. 
A set of events partitioning the universe. Often we have a set of more 
than two events that partition the universe. For example, suppose we have n events 
B1, ‘ . . , B, such that: 
The union B1 U B2 U . . . U B, = U ,  the universe, and 
0 Every distinct pair of the events are disjoint, Bi n Bj = 4 for i = 1, . . . , n, 
Then we say the set of events B1,. . ’ , B, partitions the universe. An observable 
event A will be partitioned into parts by the partition. A = ( A  n B1) U ( A  n Bz) U 
. . . ( A n  B,). (An Bi) and ( A n  B j )  are disjoint since Bi and Bj are disjoint. Hence 
P(A) = C P(A n Bj) . 
This is known as the law of total probability. It just says the probability of an event 
A is the sum of the probabilities of its disjoint parts. Using the multiplication rule on 
each joint probability gives 
j =1, ..., n , a n d i # j .  
n 
j=1 
n 
P(A) = 
P(A1Bj) x P(Bj) 
3=1 
The conditional probability P(Bi 1 A) for i = 1, . . . , n is found by dividing each joint 
probability by the probability of the event A. 
Using the multiplication rule to find the joint probability in the numerator, along with 
the law of total probability in the denominator, gives 
(4.4) 

BAYES THEOREM 
65 
Figure 4.8 
A. 
Four events Bi for i = 1, . . . , 4  that partition the universe U, 
along with event 
Figure 4.9 
partitioning the universe. 
The reduced universe given event A has occurred, together with the four events 
This is a result known as Bayes’ theorem published posthumously in 1763 after the 
death of its discoverer, Reverend Thomas Bayes. 
Example 5 Suppose n = 4. Figure 4.8 shows the four unobservable events B1, 
. . . , B4 
that partition the universe U, and an observable event A. Now let us look at the 
conditional probability of B, given A has occurred. Figure 4.9 shows the reduced 
universe, given event A has occurred. The conditionalprobabilities are the probabil- 
ities on the reduced universe, scaled up so they sum to 1. They are given by Equation 
4.4. 
Bayes’ theorem is really just a restatement of the conditional probability formula, 
where the joint probability in the numerator is found by the multiplication rule, and 
the marginal probability found in the denominator is found using the law of total 
probability followed by the multiplication rule. Note how the events A and Bi for 

66 
LOGIC, PROBABlLlm AND UNCERTAINTY 
i = 1,. . . , n are not treated symmetrically. The events B, for i = 1,. . . , n are 
considered unobservable. We never know which one of them occurred. The event 
A is an observable event. The marginal probabilities P(B,) for i = 1, . . . , n are 
assumed known before we start and are called our prior probabilities. 
Bayes’ Theorem: The Key to Bayesian Statistics 
To see how we can use Bayes’ theorem to revise our beliefs on the basis of evidence, 
we need to look at each part. Let B1, . . . , B, be a set of unobservable events which 
partition the universe. We start with P(B,) for i = 1, . . . n, the prior probability 
for the events B,, for i = 1, . . . , n. This distribution gives the weight we attach to 
each of the B, 
from our prior belief. Then we find that A has occurred. 
The likelihood of the unobservable events B1 . . . , B, is the conditional proba- 
bility that A has occurred given B, for i = 1, . . . , n. Thus the likelihood of event 
B, is given by P(AIB,). We see the likelihood is a function defined on the events 
B1, 
. . . , B,. The likelihood is the weight given to each of the B, events given by the 
occurrence of A. 
P(B,lA) for i = 1. . . . , n is theposterior probability of event B,, 
given that event 
A has occurred. This distribution contains the weight we attach to each of the events 
B, 
for i = 1, . . . n after we know event A has occurred. It combines our prior beliefs 
with the evidence given by the occurrence of event A. 
The 6ayeSi8n universe. We can get better insight into Bayes’ theorem if we 
think of the universe as having two dimensions, one observable, and one unob- 
servable. We let the observable dimension be horizontal, and let the unobservable 
dimension be vertical. The unobservable events no longer partition the universe hap- 
hazardly. Instead, they partition the universe as rectangles that cut completely across 
the universe in a horizontal direction. The whole universe consists of these horizon- 
tal rectangles in a vertical stack. Since we don’t ever observe which of these events 
occurred, we never know what vertical position we are in the Bayesian universe. 
Observable events are vertical rectangles, that cut the universe from top to bottom. 
We observe that vertical rectangle A has occurred, so we observe the horizontal 
position in the universe. 
Each event Bi n A is a rectangle at the intersection of Bi 
and A. The probability 
of the event Bi n A is found by multiplying the prior probability of Bi times the 
conditional probability of A given Bi. This is the multiplication rule. 
The event A is the union of the disjoint parts A n Bi for i = 1, . . . n. The 
probability of A is clearly the sum of the probabilities of each of the disjoint parts. 
The probability of A is found by summing the probabilities of each disjoint part down 
the vertical column represented by A. This is the marginal probability of A. 
The posterior probability of any particular Bi given A is the proportion of A that 
is also in Bi. In other words, the probability of that Bi n A divided by the sum of 
Bj n A summed over all j = 1, . . . n. 
In Bayes’ theorem, each of the joint probabilities are found by multiplying the 
prior probability P(Bi) times the likelihood P(AIBi). In Chapter 5 ,  we will see that 

BAYES THEOREM 
67 
Figure 4.10 
The Bayesian universe U with four unobservable events Bi for i = 1,. . . ,4 
which partition it shown in the vertical dimension, and the observable event A shown in the 
horizontal dimension. 
the universe set out with two dimensions for two jointly distributed discrete random 
variables is very similar to that shown in Figures 4.10 and 4.1 1. One random variable 
will be observed, and we will determine the conditional probability distribution of 
the other random variable, given our observed value of the first. In Chapter 6, we will 
develop Bayes’ theorem for two discrete random variables in an analogous manner 
to our development of Bayes’ theorem for events in this chapter. 
Example 5 (continued) Figure 4.10 shows the four unobservable events B, for 
i = 1, . . . ,4 
that partition the Bayesian universe, together with event A which is 
observable. Figure 4.11 shows the reduced universe, given that event A has occurred. 
These figures will give us better insight than Figures 4.8 and 4.9. We know where 
in the Bayesian universe we are in the horizontal direction since we know event A 
occurred. Howevel; we don’t know where we are in the vertical direction since we 
don’t know which one of the Bi occurred. 
Multiplying by constant. The numerator of Bayes’ theorem is the prior probabil- 
ity times the likelihood. The denominator is the sum of the prior probabilities times 
likelihoods over the whole partition. This division of the prior probability times 
likelihood by the sum of prior probabilities times likelihoods makes the posterior 
probability sum to 1. 
Note that if we multiplied each of the likelihoods by a constant, the denominator 
would also be multiplied by the same constant. The constant would cancel out in 
the division, and we would be left with the same posterior probabilities. Because of 
this, we only need to know the likelihood to within a constant of proportionality. The 
relative weights given to each of the possibilities by the likelihood is all we need. 
Similarly, we could multiply each prior probability by a constant. The denominator 
would again be multiplied by the same constant, so we would be left with the same 
posterior probabilities. The only thing we need in the prior is the relative weights we 

68 
LOGIC, PROBABILIV AND UNCERTAINN 
Figure 4.7 7 
unobservable events Bi for i = 1, . . . , 4  that partition it. 
The reduced Bayesian universe, given A has occurred, together with the four 
give to each of the possibilities. We often write Bayes’ theorem in its proportional 
form as 
posterior 0: prior x likelihood 
This gives the relative weights for each of the events Bi for i = 1, . . . , n after we 
know A has occurred. Dividing by the sum of the relative weights rescales the relative 
weights so they sum to 1. This makes it a probability distribution. 
We can summarize the use of Bayes’ theorem for events by the following three 
steps: 
1. Multiply prior times likelihood for each of the Bi. This finds the probability 
of Bi n A by the multiplication rule. 
2. Sum them for i = 1, . . . , n. This finds the probability of A by the law of total 
probability. 
3. Divide each of the prior times likelihood values by their sum. This finds the 
conditional probability of that particular Bi 
given A. 
4.7 ASSIGNING PROBABILITIES 
Any assignment of probabilities to all possible events must satisfy the probability 
axioms. Of course, to be useful the probabilities assigned to events must correspond 
to the real world. There are two methods of probability assignment that we will use: 
1 .  Long-run relative frequency probability assignment : The probability of an 
event is considered to be the proportion of times it would occur if the experiment 
was repeated an infinite number of repetitions. This is the method of assigning 
probabilities used in frequentist statistics. For example, if I was trying to 

ODDS RATIOS AND BAYES FACTOR 
69 
assign the probability of getting a head on a toss of a coin, I would toss it 
a large number of times and use the proportion of heads that occurred as an 
approximation to the probability. 
2. Degree of belief probability assignment: the probability of an event is what I 
believe it is from previous experience. This is subjective. Someone else can 
have a different belief. For example, I could say that I believe the coin is a 
fair one, so for me, the probability of getting a head equals .5. Someone else 
might look at the coin and observing a slight asymmetry heishe might decide 
the probability of getting a head equals .49. 
In Bayesian statistics, we will use long-run relative frequency assignments of 
probabilities for events that are outcomes of the random experiment, given the value 
of the unobservable variable. We call the unobservable variable the parameter. 
Think about repeating the experiment over and over again an infinite number of times 
while holding the parameter (unobservable) at a fixed value. The set of all possible 
observable values of the experiment is called the sample space of the experiment. 
The probability of an event is the long-run relative frequency of the event over all 
these hypothetical repetitions. We see the sample space is the observable (horizontal) 
dimension of the Bayesian universe. 
The set of all possible values of the parameter (unobservable) is called the param- 
eter space. It is the unobservable (vertical) dimension of the Bayesian universe. In 
Bayesian statistics we also consider the parameter value to be random. The proba- 
bility I assign to an event “the parameter has a certain value” can’t be assigned by 
long-run relative frequency. To be consistent with the idea of a fixed but unknown 
parameter value, I must assign probabilities by degree of belief. This shows the 
relative plausibility I give to all the possible parameter values before the experiment. 
Someone else would have different probabilities assigned according to hisher belief. 
I am modelling my uncertainty about the parameter value by a single random 
draw from my prior distribution. I do not consider hypothetical repetitions of this 
draw. I want to make my inference about the parameter value drawn this particular 
time, given this particular data. Earlier in the chapter we saw that using the rules 
of probability is the only consistent way to update our beliefs given the data. So 
probability statements about the parameter value are always subjective, since they 
start with subjective prior belief. 
4.8 ODDS RATIOS AND BAYES FACTOR 
Another way of dealing with uncertain events that we are modelling as random is to 
form the odds ratio of the event. The odds ratio for an event C equals the probability 
of the event occurring divided by the probability of the event not occurring: 

70 
LOGIC, PROBABILIPI: AND UNCERTAlNTY 
Since the probability of the event not occurring equals one minus the probability of 
the event, there is a one-to-one relationship between the odds of an event and its 
probability. 
If we are using prior probabilities, we get the prior odds ratio. In other words, the 
ratio before we have analyzed the data. If we are using posterior probabilities, we 
get the posterior odds ratio. 
Solving the equation for the probability of event C we get 
odds(C) 
(1 + odds(C)) 
P(C) = 
We see that there is a one-to-one correspondence between odds ratios and probabili- 
ties. 
Bayes Factor (B) 
The Bayes factor B contains the evidence in the data D that occurred relevant to the 
question about C occurring. It is the factor by which the prior odds is changed to the 
posterior odds: 
prior odds(C) x B = posterior odds(C) . 
We can solve this relationship for the Bayes factor to get 
posterior odds 
B =  
prior odds 
' 
We can substitute in the ratio of probabilities for both the posterior and prior odds 
ratios to find 
Thus the Bayes factor is the ratio of the probability of getting the data which occurred 
given the event, to the probability of getting the data which occurred given the 
complement of the event. If the Bayes factor is greater than 1, then the data has made 
us believe that the event is more probable than we thought before. If the Bayes factor 
is less than 1, then the data has made us believe that the event is less probable than 
we originally thought. 
4.9 BEAT THE DEALER 
In this section we take a diversion into the gambling world. This story recounts 
the journey of an American mathematician to the Blackjack tables of Las Vegas. 
Armed with an understanding of the laws of probability, along with early access to 

BEAT THE DEALER 
71 
a computer, Edward Thorp, a Professor of Mathematics, developed a strategy that 
could beat the casinos at their own game. It illustrates that observing one event 
changes the probability of another event, and many other statistical ideas introduced 
in this chapter. 
The game of "blackjack" or "twenty-one" has the player and the dealer competing 
to get a score as close as possible to twenty-one, without going bust (over twenty- 
one). Initially both are dealt two cards, one face up and one face down. Each face 
card counts ten, and each number card counts its own value, while an ace can be 
counted either as one or eleven, whichever is advantageous. The player can ask to be 
dealt a card, face up, as long as helshe has not gone bust. If the player holds before 
going bust, then the dealer must be dealt a card, face up when the dealer's total is 
under sixteen, and must hold if the total is seventeen or over. 
The casino had set the payoff assuming the player's probability of winning is 
calculated starting from a full deck that has just been shuffled. That way they had 
thought that they were setting a small advantage to the house. The law of averages 
would ensure that over the long run the house would gain, and the player would lose. 
However, as actually played, the deck was not shuffled after every hand. Rather, 
the cards that had been played (some of which have been observed) were put aside, 
and the next hand was dealt from the remaining cards. They would continue this way 
until almost all of the cards had been played before stopping and shuffling. Thorp 
realized that the real probability the player has of winning a hand depends on the 
cards that remain in the deck. 
0 The conditional probability of winning given the cards that remain in the deck is 
what counts, not the unconditional probability calculated assuming a complete 
shuffled deck. 
Although the long-run odds were against the player, sometimes the actual odds would 
be in favor of the player. If Thorp could identify those times, by making large bets at 
those times and betting the minimum at other times, overall he would be able to win. 
This was in the early days of computing, and he had access to IBM 704 computer. 
He wrote a program that would simulate the playing blackjack with strategies that 
depend on the cards that had been seen, and he ran the program thousands of times. 
0 This is a Monte Curlo study. He determined that a simple strategy that only 
depends on the observed ratio of cards over five to those five or under would 
be effective. This strategy is known as "card counting" and is not illegal. 
He went to Las Vegas and proved his strategy by winning lots of money. Of course, 
the casino was not happy at having to pay out. The reason they were not shuffling the 
deck between each hand was that they considered shuffling to be dead time during 
which the casino was not making money. They did not want to shuffle each time, just 
in case someone was counting cards. One of the first countermeasures they devised 
was to increase the number of decks of cards used. This would make the ratio of 
over fives to five and under less variable. However, Thorp continued his Monte Car10 
study with more decks and found that it still worked, particularly after many hands 
had been played and only a few cards remained. He resumed winning, until the 

72 
LOGIC, PROBABILIW AND UNCERTAINTY 
casinos banned him. Interested readers can read more about this in Thorp (1962). 
Card counting continues to be legal, but casinos try to identify those practicing it and 
ban them from playing. Casinos are private establishments and have the right to ban 
anyone they wish. Of course, the casinos could just shuffle the deck between each 
hand. But they have decided that their overall best strategy is to allow card counting, 
but identify successful practitioners and ban them from further play. 
One may ask "What about the other cards that have been played, but not observed? 
Shouldn't they also be taken into account?" Of course, the actual probability of 
winning depends on all the cards that remain in the deck. The probabilities found by 
Thorp, which depend only on the cards observed, have averaged the cards that have 
been played but not seen over all the possible values. 
Main Points 
0 Deductive logic. A logical process for determining the truth of a statement 
from knowing the truth or falsehood of other statements that the first statement 
is a consequence of. Deduction works from the general to the particular. We 
can make a deduction from a known population distribution to determine the 
sampling distribution of a statistic. 
0 Deductions do not have the possibility of error. 
0 Inductive logic. A process, based on plausible reasoning, for inferring the 
truth of the statement from knowing the truth or falsehood of other statements 
which are consequences of the first statement. It works from the particular to 
the general. Statistical inference is an inductive process for making inferences 
about the parameter, on the basis of the observed statistic from the sampling 
distribution given the parameter. 
0 There is always the possibility of error when making an inference. 
0 Plausible reasoning should be based on the rules of probability to be consistent. 
They are: 
o Probability of an event is a nonnegative number. 
o Probability of the sample space (universe) equals 1. 
o The probability is additive over disjoint events. 
0 A random experiment is an experiment where the outcome is not exactly pre- 
dictable, even when the experiment is repeated under the identical conditions. 
0 The set of all possible outcomes of a random experiment is called the sample 
space R. In frequentist statistics, the sample space is the universe for analyzing 
events based on the experiment. 
0 The union of two events A and B is the set of outcomes in A or B. This is an 
inclusive or. The union is denoted A U B. 

MAIN POINTS 
73 
0 The intersection of two events A and B is the set of outcomes in both A and 
B simultaneously. The intersection is denoted A n B. 
of event A is denoted A. 
0 The complement of event A is the set of outcomes not in A. The complement 
0 Mutually exclusive events have no elements in common. Their intersection 
P(A n B )  equals the empty set, 4. 
0 The conditional probability of event B given event A is given by 
P(A n B) 
P(A) 
P(BIA) = 
0 The event B is unobservable. The event A is observable. We could nominally 
write the conditional probability formula for P(AIB), but the relationship is not 
used in that form. We do not treat the events symmetrically. The multiplication 
rule is the definition of conditional probability cleared of the fraction. 
P ( A  n B )  = P(B) x P(A1B). 
It is used to assign probabilities to compound events. 
0 The law of total probability says that given events B1, . . . , B, that partition 
the sample space (universe), along with another event A, then 
n 
P(A) = 
P(Bj n A) 
j=l 
because probability is additive over the disjoint events, ( A n  B1) . . . ( A n  Bn). 
When we find the probability of each of the intersections A n Bj by the 
multiplication rule, we get 
P (A ) = C P ( B j )  x P(A(Bj) . 
J 
0 Bayes' theorem is the key to Bayesian statistics: 
This comes from the definition of conditional probability. The marginal prob- 
ability of the event A is found by the law of total probability, and each of the 
joint probabilities is found from the multiplication rule. P(Bi) is called the 
prior probability of event B,, and P(BiIA) is called the posterior probability 
of event Bi. 
0 In the Bayesian universe, the unobservable events B1, . . . , B, which partition 
the universe are horizontal slices, and the observable event A is a vertical slice. 

74 
LOGIC, PROBABILITL: AND UNCERTAINTY 
The probability P(A) is found by summing the P ( A  n Bi) down the column. 
Each of the P(A n Bi) is found by multiplying the prior P(Bi) times the 
likelihood P(AIBi). So Bayes’ theorem can be summarized by saying that 
the posterior probability is the prior times likelihood divided by the sum of the 
prior times likelihood. 
The Bayesian universe has two dimensions. The sample space forms the 
observable (horizontal) dimension of the Bayesian universe. The parameter 
space is the unobservable (vertical) dimension. In Bayesian statistics, the 
probabilities are defined on both dimensions of the Bayesian universe. 
The odds ratio of an event A is the ratio of the probability of the event to the 
probability of its complement: 
If it is found before analyzing the data, it is the prior odds ratio. If it is found 
after analyzing the data, it is the posterior odds ratio. 
0 The Bayes factor is the amount of evidence in the data that changes the prior 
odds to the posterior odds: 
B x prior odds = posterior odds 
Exercises 
4.1 There are two events A and B. P(A) = .4 and P(B) = .5. The events A and 
B are independent. 
(a) Find P(A). 
(b) Find P(A n B). 
(c) Find P ( A  u B). 
4.2 There are two events A and B. P(A) = .5 and P(B) = .3. The events A and 
B are independent. 
(a) Find P(A). 
(b) Find P(A n B). 
(c) Find P(A u B). 
4.3 There are two events A and B. P (A ) = .4 and P(B) = .4. P ( A  Ti B )  = .24. 
(a) Are A and B independent events? Explain why or why not. 
(b) Find P ( A  U B). 

EXERCISES 
75 
4.4 There are two events A and B. P(A) = .7 and P(B) = .8. P ( A  n B) = .1. 
(a) Are A and B independent events? Explain why or why not. 
(b) Find P ( A  U B). 
4.5 A single fair die is rolled. Let the event A be "the face showing is even." Let 
the event B be "the face showing is divisible by 3." 
(a) List out the sample space of the experiment. 
(b) List the outcomes in A, and find P(A). 
(c) List the outcomes in B, and find P(B). 
(d) List the outcomes in A n B, and find P ( A  n B). 
(e) Are the events A and B independent? Explain why or why not. 
4.6 Two fair dice, one red and one green, are rolled. Let the event A be "the sum 
of the faces showing is equal to seven." Let the event B be "the faces showing 
on the two dice are equal." 
(a) List out the sample space of the experiment. 
(b) List the outcomes in A, and find P(A). 
(c) List the outcomes in B, and find P(B). 
(d) List the outcomes in A n B, and find P ( A  n B). 
(e) Are the events A and B independent? Explain why or why not. 
(f) How would you describe the relationship between event A and event B? 
4.7 Two fair dice, one red and one green, are rolled. Let the event A be "the sum 
of the faces showing is an even number." Let the event B be "the sum of the 
faces showing is divisible by 3." 
(a) List the outcomes in A, and find P(A). 
(b) List the outcomes in B, and find P(B). 
(c) List the outcomes in A n B, and find P ( A  n B). 
(d) Are the events A and B independent? Explain why or why not. 
4.8 Two dice are rolled. The red die has been loaded. Its probabilities are P( 1) = 
P(2) = P(3) = P(4) = and P(5) = P(6) = $. The green die is fair. Let 
the event A be "the sum of the faces showing is an even number." Let the event 
B be "the sum of the faces showing is divisible by 3." 
(a) List the outcomes in A, and find P(A). 
(b) List the outcomes in B, and find P(B). 
(c) List the outcomes in A n B, and find P ( A  n B). 
(d) Are the events A and B independent? Explain why or why not. 

76 
LOGIC, PROBABILIT): AND UNCERTAINN 
4.9 Suppose there is a medical diagnostic test for a disease. The sensitivity of the 
test is .95. This means that if a person has the disease, the probability that the 
test gives a positive response is .95. The speciJicity of the test is .90. This 
means that if a person does not have the disease, the probability that the test 
gives a negative response is .90, or that the false positive rate of the test is .lo. 
In the population, 1% of the people have the disease. What is the probability 
that a person tested has the disease, given the results of the test is positive? Let 
D be the event "the person has the disease" and let T be the event "the test 
gives a positive result." 
4.10 Suppose there is a medical screening procedure for a specific cancer that 
has sensitivity = .90, and speciJicity = .95. Suppose the underlying rate of the 
cancer in the population is .001. Let B be the event "the person has that specific 
cancer," and let A be the event "the screening procedure gives a positive result." 
(a) What is the probability that a person has the disease given the results of 
(b) Does this show that screening is effective in detecting this cancer? 
4.11 In the game of "blackjack," also known as "twenty-one," the player and the 
dealer are dealt one card face-down and one card face-up. The object is to get 
as close as possible to the score 21, without exceeding that. Aces count either 
1 or 11, face cards count 10, and all other cards count at their face value. The 
player can ask for more cards to be dealt to him, provided that he hasn't gone 
bust (exceeded 21) and lost. Getting 21 on the deal (an ace and a face card or 
10) is called a "blackjack." Suppose 4 decks of cards are shuffled together and 
dealt from. What is the probability the player gets a "blackjack." 
the screening is positive? 
4.12 After the hand, the cards are discarded, and the next hand continues with the 
remaining cards in the deck. The player has had an opportunity to see some of 
the cards in the previous hand, those that were dealt face-up. Suppose he saw a 
total of 4 cards, and none of them were aces, nor were any of them a face card 
or a ten. What is the probability the player gets a "blackjack" on this hand. 

5 
Discrete 
Random Variables 
In the previous chapter, we looked at random experiments in terms of events. We 
also introduced probability defined on events as a tool for understanding random 
experiments. We showed how conditional probability is the logical way to change 
our belief about an unobserved event, given that we observed another related event. 
In this chapter, we introduce discrete random variables and probability distributions. 
A random variable describes the outcome of the experiment in terms of a number. 
If the only possible outcomes of the experiment are distinct numbers separated from 
each other (e.g., counts), we say that the random variable is discrete. There are good 
reasons why we introduce random variables and their notation: 
0 It is quicker to describe an outcome as a random variable having a particular 
value than to describe that outcome in words. Any event can be formed from 
outcomes described by the random variable using union, intersection, and 
complements. 
0 The probability distribution of the discrete random variable is a numerical 
function. It is easier to deal with a numerical function than with probabilities 
being a function defined on sets (events). The probability of any possible event 
can be found from the probability distribution of the random variable using 
the rules of probability. So instead of having to know the probability of every 
possible event, we only have to know the probability distribution of the random 
variable. 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
77 

78 
DISCRETE RANDOM VARIABLES 
Value 
1 
Table 5.7 Typical results of rolling a fair die 
Proportion After 
10 Rolls 
100 Rolls 
1,000 Rolls 
10,000 Rolls 
. . . 
Probability 
.1 
.15 
.198 
.1704 
. . .  
.1666 
2~ 
3 
.3 
.08 
.163 
,1661 
... 
.1 
.20 
.156 
.1670 
... 
.2 
.16 
.153 
.1698 
... 
.o 
.22 
.165 
.1583 
. . .  
.3 
.19 
.165 
.1684 
... 
.1666 
.1666 
.1666 
.1666 
.1666 
0 It becomes much easier to deal with compound events made up from repetitions 
of the experiment. 
5.1 DISCRETE RANDOM VARIABLES 
A number that is determined by the outcome of a random experiment is called a 
random variable. Random variables are denoted with uppercase letters, e.g., Y .  The 
value the random variable takes is denoted by lowercase letters, e.g., y. A discrete 
random variable, Y ,  can only take on separated values yk. There can be a finite 
possible number of values; for example, the random variable defined as "number 
of heads in n tosses of a coin" has possible values 0,1,. . . , n. Or there can be 
a countably infinite number of possible values; for example, the random variable 
defined as "number of tosses until the first head" has possible values 1,2, ' + , 00 . 
The key thing for discrete random variables is that the possible values are separated 
by gaps. 
Thought Experiment 1 : Roll of a die 
Suppose we have a fair six-jaced die. Our random experiment is to roll it, and we let 
the random variable Y be the number on the top face. There are six possible values 
1,2, . . . ,6. Since the die is faic those six values are equally likely. Now, suppose 
we take independent repetitions of the random variable, and record each occurrence 
of Y .  Table 5.1 shows the proportion of times each face has occurred in a typical 
sequence of rolls of the die, after 10, 100, 1,000, and 10,000 rolls. The last column 
shows the true probabilities for a fair die. 
We note that the proportions taking any value are getting closer and closer to the 
true probability of that value as n increases to m. We could draw graphs of the 
proportions having each value. These are shown in Figure 5.1. The graphs are at 
zero for any other y value, and they have a spike at each possible value where the 
spike height equals the proportion of times that value occurred. The sum of spike 
heights equals one. 

DISCRETE RANDOM VARIABLES 
79 
0 2  1 
O 1  
0 0  L=!l!LL 
0
1
2
3
4
5
6
7
 
1 
0 3  - 
0 2  1 
Figure 5.7 
Proportions resulting from 10, 100, 1,OOO, and 10,000 rolls of a fair die. 
Thought Experiment 2 : Random samplingfrom afinite population 
Suppose we have ajnitepopulation of size N. There can be at most ajnite number of 
possible values, and they must be discrete, since there must be a gap between every 
pair of two real numbers. Some members of the population have the same value, so 
there are only K possible values y1, . . . , Y K .  The probability of observing the value 
Y k  is the proportion of population having that value. 
We start by randomly drawing from the population with replacement. Each draw 
is done under identical conditions. If we continue doing the sampling, eventually 
we have seen all possible values. After each draw we update the proportions in 
the accumulated sample that have each value. We sketch a graph with a spike at 
each value in the sample equal to the proportion in the sample having that value. 
The updating of the graph at step n is made by scaling all the existing spikes down 
by the ratio 
and adding 
to the spike at the value observed. The scaling 
changes the proportions after thejrst n - 1 observations to the proportions after the 
first n observations. As the sample size increases, the sample proportions get less 
variable. In the limit as the sample size n approaches infinity, the spike at each value 
approaches its probability. 
Thought Experiment 3 : Number of tails before first head from independent coin 
tosses 
Each toss of a coin results in either a head or a tail. The probability of getting a 
head remains the same on each toss. The outcomes of each toss are independent of 
each other. This is an example of what we call Bernoulli trials. The outcome of a 
trial is either a success (head) or failure (tail), the probability of success remains 

80 
DISCRETE RANDOM VARIABLES 
constant over all trials, and we are taking independent trials. We are counting the 
number of failures before the first success. Every nonnegative integer is a possible 
value, and there are an injinite number of them. They must be discrete, since there is 
a gap between every pair of nonnegative integers. 
We start by tossing the coin and counting the number of tails until the first head 
occurs. Then we repeat the whole process. Eventually we reach a state where most of 
the time we get a value we have gotten before. After each sequence of trials until the 
first head, we update the proportions that have each value. We sketch a graph with 
a spike at each value equal to the proportion having that value. As in the previous 
example, the updating of the graph at step n is made by scaling all the existing 
spikes down by the ratio 
and adding 
to the spike at the value observed. The 
sample proportions get less variable as the sample size increases, and in the limit as 
n approaches injnity, the spike at each value approaches its probability. 
5.2 PROBABILITY DISTRIBUTION OF A DISCRETE RANDOM 
VARIABLE 
The proportion functions that we have seen in the three thought experiments are spike 
functions. They have a spike at each possible value, zero at all other values, and 
the sum of the spike heights equals one. In the limit as the sample size approaches 
infinity, the proportion of times a value occurs approaches the probability of that 
value, and the proportion graphs approach the probability function 
for all possible values y1, . . . , Y k  of the discrete random variable. For any other value 
9, it equals zero. 
Expected Value of a Discrete Random Variable 
The expected value of a discrete random variable Y is defined to be the sum over all 
possible values of each possible value times its probability: 
The expected value of a random variable is often called the mean of the random 
variable and is denoted p. It is like the sample mean of an infinite sample of 
independent repetitions of the random variable. The sample mean of a random 
sample of size n repetitions of the random variable is 
Here yi is the value that occurs on the ith repetition. We are summing over all 

PROBABILITY DlSTRlBUTlON OFA DISCRETE RANDOM VARIABLE 
81 
repetitions. Grouping together all repetitions that have the same possible value, we 
where n k  is the number of observations that have value Y k ,  and we are now summing 
over all possible values. Note that each of the yi (observed values) equals one of the 
Yk (possible values). But in the limit as n approaches M, the relative frequency 2 
approaches the probability f ( Y k ) ,  so the sample mean, y, approaches the expected 
value, E(Y). This shows that the expected value of a random variable is like the 
sample mean of an infinite size random sample of that variable. 
The Variance of a Discrete Random Variable 
The variance of a random variable is the expected value of square of the variable 
minus its mean. 
Var(Y) = E(Y-E(Y))Z 
(5.2) 
k 
This is like the sample variance of an infinite size random sample of that variable. 
We note that if we square the term in brackets, break the sum into three sums, and 
factor the constant terms out of each sum, we get 
Var(Y) = c 
YE x f ( Y k )  - 2p x c 
Y k f ( Y k )  -t p2 c 
f ( Y k )  
k 
k 
k 
= E(Y2)-p? 
Since p = E ( Y ) ,  
this gives another useful formula for computing the variance. 
Var(Y) = E(Y2) - [E(Y)]2. 
(5.3) 
Example 6 Let Y be a discrete random variable with probabilityfunction given in 
the following table. 
0 
.20 
I 
.I5 
2 
.25 
3 
.35 
4 
.05 
Tojnd E ( Y )  we use Equation 5.1, which gives 
E ( Y )  = 
= 1.90. 
0 x .20 + 1 x .15 + 2 x .25 + 3 x .35 + 4 x .05 

82 
DISCRETE RANDOM VARIABLES 
Note that the expected value does not have to be a possible value of the random 
variable Y .  It represents an average. We willJind Var(Y) in two ways and see that 
they give equivalent results. First, we use the definition of variance given in Equation 
5.2. 
Var(Y) = (0 - 1.90)' x .20 + (1 - 1.90)' x .15 + (2 - 1.90)' x .25 
+(3 - 1.90)' x .35 + (4 - 1.90)' x .05 
= 1.49. 
Second, we will use Equation 5.3. We calculate 
E(Y2) = 
= 5.10. 
0' 
x .20 + 1' x .15 + 2' x .25 + 3' x .35 + 4' x .05 
Putting that result in Equation 5.3, we get 
Var(Y) = 5.10- 1.90' 
= 1.49. 
The Mean and Variance of a Linear Function of a Random Variable 
Suppose W = a x Y + b, where Y is a discrete random variable. Clearly, W is 
another number that is the outcome of the same random experiment that Y came 
from. Thus W ,  a linear function of a random variable Y ,  is another random variable. 
We wish to find its mean. 
Similarly, we may wish to know its variance. 

BINOMIAL DISTRIBUTION 
83 
Thus the variance of a linear function is the square of the multiplicative constant a 
times the variance : 
(5.5) 
The additive constant b doesn’t enter into it. 
Example 6 (continued) Suppose W = -2Y + 3. Then from Equation 5.4 we have 
Var(aY + b) = a2Vnr(Y). 
E(W) = - 2 E ( Y ) + 3  
= -2 X 1.90+3 
= -20 
and from Equation 5.5 we have 
V a r ( W )  = (-2)2 x V a r ( Y )  
= 4 x 1.49 
= 5.96. 
5.3 BINOMIAL DISTRIBUTION 
Let us look at three situations and see what characteristics they have in common, 
Coin tossing. Suppose we toss the same coin n times, and count the number of 
heads that occur. We consider that any one toss is not influenced by the outcomes of 
previous tosses, in other words, the outcome of one toss is independent of the out- 
comes of previous tosses. Since we are always tossing the same coin, the probability 
of getting a head on any particular toss remains constant for all tosses. The possible 
values of the total number of heads observed in the n tosses are 0, . . . , n. 
Drawing from an urn with replacement. An urn contains balls of two colors, 
red and green. The proportion of red balls is 7r. We draw a ball at random from the 
urn, record its color, then return it to the urn, and remix the balls before the next 
random draw. We make a total of n draws, and count the number of times we drew a 
red ball. Since we replace and remix the balls between draws, each draw takes place 
under identical conditions. The outcome of any particular draw is not influenced by 
the previous draw outcomes. The probability of getting a red ball on any particular 
draw remains equal to i~, 
the proportion of red balls in the urn. The possible values 
of the total number of red balls drawn are 0, . . . , n. 
Random sampling from a very large population. Suppose we draw a ran- 
dom sample of size n from a very large population. The proportion of items in the 
population having some attribute is i ~ .  
We count the number of items in the sample 
that have the attribute. Since the population is very large compared to the sample size, 
removing a few items from the population does not perceptibly change the proportion 
of remaining items having the attribute. For all intents and purposes it remains T .  
The random draws are taken under almost identical conditions. The outcome of any 
draw is not influenced by the previous outcomes. The possible values of the number 
of items drawn that have the attribute is 0, . . . , n. 

84 
DISCRETE RANDOM VARIABLES 
Characteristics of the Binomial Distribution 
These three cases all have the following things in common. 
0 There are n independent trials. Each trial can result either in a "success" or a 
"failure." 
0 The probability of "success" is constant over all the trials. Let T be the 
probability of "success." 
0 Y is the number of "successes" that occurred in the n trials. Y can take on 
integer values 0,1, . . . , n. 
These are the characteristics of the binomial (n, T )  distribution. The binomial prob- 
ability function can be found from these characteristics using the laws of probability. 
Any sequence having exactly y successes out of the n independent trials has probabil- 
ity equal to ~ v ( 1  
- T ) ~ - V ,  no matter in which order they occur. The event {Y = y} 
is the union of all sequences such sequences. The sequences are disjoint, so the 
probability function of the binomial random variable Y given the parameter value T 
is written as 
7r)n-y 
for y = 0,1, . . . , n where the binomial coefficient 
y!x(n-y)! represents 
.
,
 
the number of sequences having exactly y successes out of n trials and nY(1- T ) ~ - P  
is the probability of any particular sequence having exactly y successes out of n trials. 
Mean of binomial. The mean of the binomial(n, T )  distribution is the sample 
size times the probability of success since 
n 
E(YlT) = CY x f(YlT) 
y=O 
= ey 
x ( ; 
) T Y ( 1 -  7r)n--y. 
y=O 
We write this as a conditional mean because it is the mean of Y given the value of 
the parameter T .  The first term in the sum is 0, so we can start the sum at y = 1. We 
cancel y in the remaining terms, and factor out nr. This gives 
Factoring nn out of the sum and substituting n' = n - 1 and y' = y - 1, we get 

HYPERGEOMETRIC DISTRIBUTION 
85 
We see the sum is a binomial probability function summed over all possible values. 
Hence it equals one, and the mean of the binomial is 
E(YI7r) = n r .  
(5.7) 
Variance of binomial. The variance is the sample size times the probability of 
success times the probability of failure. We write this as a condtional variance since 
it is the variance of Y given the value of the parameter r. Note that 
n 
W ( Y  - 1)l.) 
= CY(Y - 1) x f(YIT) 
y=o 
The first two terms in the sum equal 0, so we can start summing at y = 2. We cancel 
y(y - 1) out of the remaining terms and factor out n(n - l)r2 to get 
Substituting y’ = y - 2 and n’ = n - 2, we get 
= n(n-1)7r2 
since we are summing a binomial distribution over all possible values. The variance 
can be found by 
Var(Yl7r) = E(Y21T) - [E(YIT)]2 
= E(Y(Y - l)i7r) + E(YI7r) - [E(YIr)]2 
n(n - 1)7r2 + n7r - [..I2. 
= 
Hence the variance of the binomial is the sample size times the probability of success 
times the probability of failure. 
Var(Yl7r) = nr(1 - 7 r ) .  
(5.8) 
5.4 HYPERGEOMETRIC DISTRIBUTION 
The hypergeometric distribution models sampling from an urn without replacement. 
There is an urn containing N balls, R of which are red. A sequence of n balls is 
drawn randomly from the urn without replacement. Drawing a red ball is called a 
“success.” The probability of success T does not stay constant over all the draws. 
At each draw the probability of “success“ is the proportion of red balls remaining in 
the urn, which does depend on the outcomes of previous draws. Y is the number of 
”successes” in the n trials. Y can take on integer values 0,1,. . . , n. 

86 
DISCRETE RANDOM VARIABLES 
Probability Function of Hypergeometric 
The probability function of the hypergeometric random variable Y given the param- 
eters N ,  n, R is written as 
for possible values y = 0,1, . . . , n. 
Mean and variance of hypergeometric. The conditional mean of the hyper- 
geometric distribution is given by 
R 
N 
E(YIN,R,n) = n x -. 
The conditional variance of the hypergeometric distribution is given by 
Vay(YIN,R,n) 
r= V, x N 
We note that $ is the proportion of red balls in the urn. The mean and variance of the 
hypergeometric are similar to that of the binomial, except that the variance is smaller 
due to the finite population correction factor e. 
5.5 POISSON DISTRIBUTION 
The Poisson distribution is another distribution for counts.' Specifically, the Poisson 
is a distribution which counts the number of occurrences of rare events over a period 
of time or space. Unlike the binomial which counts the number of events (successes) 
in a known number of independent trials, the number of trials in the Poisson is so 
large that it isn't known. Nevertheless, looking at the binomial gives us way to start 
our investigation of the Poisson. Let Y be a binomial random variable where n is 
very large, and 7r is very small. The binomial probability function is 
P(Y =yl7r) 
= ( ; ) (7r)"l 
- 7 r y - y  
n! 
(n - y)!y! 
- 
- 
( 7 r ) Y ( l  - 7 r y - y  
for y = 0, . . . , n. Since 7r is small, the only terms that have appreciable probability 
are those where y is much smaller than n. We will look at the probabilities for those 
'First studied by Sirneon Poisson (1781-1840) 

POISSON DISTRIBUTION 
87 
small values of y. Let p = nx. The probability function is 
Rearranging the terms, we get 
x -  ( 1 - -  E)" (1 - y . 
n n - 1  
n - y + l  
py 
P(Y = yip) = - x - 
x ... x 
n
n
 
n 
Y! 
But all the values f , 5, 
. . . , 
are approximately equal to 1 since y is much 
smaller than n. We let n approach infinity, and T approach 0 in such a way that 
p = nx is constant. We know that 
so the Poisson probability function is given by 
for y = 0,1,. . .. Thus the Poisson(p) distribution can be used to approximate a 
binornial(n, T )  when n is large, T is very small, and p = n ~ .  
Characteristics of the Poisson Distribution 
Think of the period of time (or space) divided into n equal parts. The total number 
of occurrences is the sum of the number of occurrences in all n parts. We see from 
the Poisson approximation to the binomial that the Poisson distribution is a limiting 
case of the binomial distribution as n ---$ CQ and T + 0 at such a rate that nT = p is 
constant. 
0 In the binomial, the probability of success remains constant over all the trials. 
It follows that the instantaneous rate of occurrences per unit time (or space) 
for the Poisson is constant. 
In the binomial, the trials are independent. Thus the Poisson occurrences in any 
two non-overlapping intervals will be independent of each other. It follows that 
the Poisson occurrences are randomly occurring through time at the constant 
instantaneous rate. 
In the binomial each trial contributes either one success or one failure. It 
follows that Poisson counts occur one at a time. 
0 The possible values are y = 0,1,. . . . 

88 
DISCRETE RANDOM VARIABLES 
Mean and variance of Poisson. The mean of the Poisson(p) can be found by 
We let y’ = y - 1 and factor out p: 
The sum equals one since it is the the sum is over all possible values of a Poisson 
distribution, so the mean of the Poisson(p) is 
Similarly, we can evaluate 
We let y’ = y - 2, and factor out p2 
The sum equals one since it is the the sum is over all possible values of a Poisson 
distribution, so E ( y  x (y - 1 ) / p )  for a Poisson(p) is given by 
E(Y x (Y - 1)IP) = P 2 .  
The Poisson variance is given by 
V 4 Y l P L )  = E(Y2/P) - [E(YlP)I2 
= E(Y x (Y - 1L)IPL) + W P L )  - [E(YIP)l2 
= 
= p .  
Thus we see the mean and variance of a Poisson(p) are both equal to p. 

JOINT RANDOM VARIABLES 
89 
Table 5.2 Universe of joint experiment 
5.6 JOINT RANDOM VARIABLES 
When two (or more) numbers are determined from the outcome of a random ex- 
periment, we call it a joint experiment. The two numbers are called joint random 
variables and denoted X ,  Y .  If both the random variables are discrete, they each have 
separated possible values xi for i = 1, . . . , I ,  and yj for j = 1, . . , , J .  The universe 
for the experiment is the set of all possible outcomes of the experiment which are 
all possible ordered pairs of possible values. The universe of the joint experiment is 
shown in Table 5.2. 
The joint probability function of two discrete joint random variables is defined at 
each point in the universe: 
f(Zi, yj) = P(X = xi, Y = yj) 
for i = 1,. . . , I ,  and j = 1,. . . , J .  This is the probability that X = 5, and 
Y = yJ simultaneously, in other words, the probability of the intersection of the 
events X = x, and Y = y3. These joint probabilities can be put in a table. 
We might want to consider the probability distribution of just one of the joint 
random variables, for instance, Y .  The event Y = yj for some fixed value y3 is the 
union of all events X = z,, Y = y3, where i = 1,. . . , I ,  and they are all disjoint. 
Thus 
P(Y = y3) = P(U,(X = z,, Y = y3)) = c P(X = z2, 
Y = y3) 
z 
for = 1, . . . , J ,  since probability is additive over a disjoint union. This probability 
distribution of Y by itself is called the marginal distribution of Y .  Putting this 
relationship in terms of the probability function, we get 
(5.10) 

90 
DISCRETE RANDOM VARIABLES 
Table 5.3 Joint and marginal probability distributions 
~ 
f(Yd 
' 
. 
. 
f(Yj) 
' 
. 
. 
f ( Y J )  
I 
for j = 1, . . . J .  So we see that the individual probabilities of Y is found by summing 
the joint probabilities down the columns. Similarly the individual probabilities of X 
can be found by summing the joint probabilities across the rows. We can write them 
on the margins of the table, hence the names marginal probability distribution of Y 
and X respectively. The joint probability distribution and the marginal probability 
distributions are shown in Table 5.3. The joint probabilities are in the main body of 
the table, and the marginal probabilities for X and Y are in the right column and 
bottom row, respectively. 
The expected value of a function of the joint random variables is given by 
a
j
 
Often we wish to find the expected value of a sum of random variables. In that case 
E ( X  + Y) 
i
j
 
i 
j 
We see the mean of the sum of two random variables is the sum of the means. 
E ( X  + Y) 
= E ( X )  + E ( Y )  . 
(5.11) 
This equation always holds. 

JOINT RANDOM VARIABLES 
91 
Independent Random Variables 
Two (discrete) random variables X and Y are independent of each other if and only 
if every element in the joint distribution table equals the product of the corresponding 
marginal distributions. In other words, 
f ( x i , y j )  = f ( x i )  x f ( ~ j )  
for all possible xi and y j .  
The variance of a sum of random variables is given by 
V a r ( X  + Y )  = E ( X  + Y - E ( X  + Y))2 
Multiplying this out and breaking it into three separate sums gives 
~
a
r
(
~
 
+ Y )  = F, 
x ( x i  - ~ ( x ) ) ’  
x f ( x i ,  y j )  
i
j
 
i
j
 
The middle term is 2 x the covariance of the random variables. For independent 
random variables the covariance is given by 
i
j
 
i 
j 
This is clearly equal to 0. Hence for independent random variables we have 
Var(X + Y )  = c ( x i  - E(X)))’ x f(.i) 
+ C(Yj 
- E(Y)I2 x f ( Y j )  ’ 
i 
j 
We see the variance of the sum of two independent random variables is the sum of 
the variances. 
V a r ( X  + Y )  = V a r ( X )  + V a r ( Y )  . 
(5.12) 
This equation only holds for independent* random variables! 
Cov(X, Y )  + Var(Y). 
ZIn general, the variance of a sum of two random variables is given by Var(X + Y )  
= Var(X) + 2 X 

92 
D E C R E E  RANDOM VARlABLES 
Example 7 Let X and Y be jointly distributed discrete random variables. Their 
joint probability distribution is given in the following table: 
.02 
.04 
.06 
.08 
.03 
.01 
.09 .I7 
.05 
$15 .I5 .I5 
f (Y) 
Wejnd the marginal distributions of X and Y by summing across the rows and 
summing down the columns, respectively. That gives the table 
.03 
.01 
.09 .I7 
We see that the joint probability f (xi, 
y3) is not always equal to the product of the 
marginalprobabilities f (xi) x f (yj). Therefore the two random variables X and Y 
are not independent. 
Mean and variance of a difference between two independent random 
variables. When we combine the results of Equations 5.10 and 5.11 with the 
results of Equations 5.4 and 5.5, we find the that mean of a difference between 
random variables is 
E ( X  - Y )  = E ( X )  - E ( Y )  . 
(5.13) 
If the two random variables are independent, we find that the variance of their 
difference is 
Var(X - Y )  = V a r ( X )  + V a r ( Y )  . 
(5.14) 
Variability always adds for independent random variables, regardless of whether we 
are taking the sum or taking the difference. 
5.7 CONDITIONAL PROBABILITY FOR JOINT RANDOM VARIABLES 
If we are given Y = yj, the reduced universe is the set of ordered pairs where the 
second element is yj. This is shown in Table 5.4. It is the only part of the universe that 
remains, given Y = yj. The only part of the event X = xi that remains is the part 
in the reduced universe. This is the intersection of the events X = xi and Y = yj. 

CONDITIONAL PROBABILITY FOR JOINT RANDOM VARIABLES 
93 
Table 5.4 Reduced universe given Y = yj 
Table 5.5 shows the original joint probability function in the reduced universe, along 
with the marginal probability. We see that this is not a probability distribution. The 
sum of the probabilities in the reduced universe sums to the marginal probability, not 
to one! 
The conditional probability that random variable X = xi, given Y = yj is the 
probability of the intersection of the events X = xi and Y = y j  divided by the 
probability that Y = yj from Equation 4.1. Dividing the joint probability by the 
marginal probability scales it up so the probability of the reduced universe equals 1. 
The conditional probability is given by 
(5.15) 
P(X = xi, Y = yj) 
P(Y = Yj) 
f(.iIYj) 
= P ( X  = Xi(Y = yj) = - 
When we put this in terms of the joint and marginal probability functions, we get 
(5.16) 
The conditional probability distribution. Letting x, vary across all possible 
values of X gives us the conditional probability distribution of XIY = yj. 
The 
conditional probability distribution is defined on the reduced universe given Y = yj. 
The conditional probability distribution is shown in Table 5.6. Each entry was found 
by dividing the i,j entry in the joint probability table byfh element in the marginal 
probability. The marginal probability is f(yj) = C, f(x,, yj) and is found by 
summing down the j t h  column of the joint probability table. So the conditional 
probability of x, given y3 is the jth column in the joint probability table, divided by 
the sum of the joint probabilities in the j t h  column. 

94 
DISCRETE RANDOM VARIABLES 
Table 5.5 
probability is found by summing down the column. 
Joint probability function values in the reduced universe Y = yJ . The marginal 
Table 5.6 
The conditional probability function defined on the reduced universe Y = yj 
Example 7 (continued) lfwe want to determine the conditionalprobability P ( X  = 
2 / Y  = 2), we plug in the joint and marginal probabilities into Equation 5.15. This 
gives 
P ( X  = 2 , Y  = 2) 
P(Y = 2 )  
P ( X  = 2 / Y  = 2 )  = 
.01 
.2 
= .05 
- 
- 
- 

MAIN POINTS 
95 
Conditional probability as multiplication rule. Using similar arguments, we 
could find that the conditional probability function of Y given X = xi is given by 
However, we will not use the relationship in this form, since we do not consider 
the random variables interchangeably. In Bayesian statistics, the random variable 
X is the unobservable parameter. The random variable Y is an observable random 
variable that has a probability distribution depending on the parameter. In the next 
chapter we will use the conditional probability relationship as the multiplication rule 
f(Zi, Yj) = f ( 4  x f ( Y j l 4  
(5.17) 
when we develop Bayes’ theorem for discrete random variables. 
Main Points 
0 A random variable Y is a number associated with the outcome of a random 
experiment. 
0 If the only possible values of the random variable are a finite set of separated 
values, y1, . . . , y~ the random variable is said to be discrete. 
0 The probability distribution of the discrete random variable gives the probabil- 
ity associated with each possible value. 
0 The probability of any event associated with the random experiment can be 
calculated from the probability function of the random variable using the laws 
of probability. 
0 The expected value of a discrete random variable is 
where the sum is over all possible values of the random variable. It is the mean 
of the distribution of the random variable. 
0 The variance of a discrete random variable is the expected value of the squared 
deviation of the random variable from its mean. 
Another formula for the variance is 
Var(Y) = E(Y2) - [E(Y)]2. 

96 
DISCRETE RANDOM VARIABLES 
0 The mean and variance of a linear function of a random variable aY + b are 
E(aY + b) = a E ( Y )  + b 
and 
Var(aY + b) = a’ 
x V a r ( Y ) .  
The binomial (n, T )  distribution models the number of successes in n inde- 
The binomial distribution is used for sampling from a finite population with 
pendent trials where each trial has the same success probability, T .  
replacement. 
The hypergeometric distribution is used for sampling from a finite population 
without replacement. 
The Poisson(p) distribution counts the number of occurrences of a rare event. 
Occurrences are occurring randomly through time (or space) at a constant rate 
and occur one at a time. It is also used to approximate the binomiaZ(n, T )  
where n is large and 7r is small and we let p = n ~ .  
0 The joint probability distribution of two discrete random variables X and Y is 
written as joint probability function 
f(Q, yj) = P ( X  = xi, Y = yj) . 
Note: ( X  = x,,Y = yj) is another way of writing the intersection (x = 
5% n Y = yj). This joint probability function can be put in a table. 
0 The marginal probability distribution of one of the random variables can be 
found by summing the joint probability distribution across rows (for X )  or by 
summing down columns (for Y ) .  
0 The mean and variance of a sum of independent random variables are 
E ( X + Y )  = E ( X ) + E ( Y )  
and 
V a r ( X  + Y )  = V a r ( X )  + V a r ( Y )  
The mean and variance of a difference between independent random variables 
are 
E ( X  - Y )  = E ( X )  - E(Y) 
V a r ( X  - Y )  = V a r ( X )  + V a r ( Y )  . 
and 
0 Conditional probability function of X given Y = y j  is found by 

EXERCISES 
97 
This is the joint probability divided by the marginal probability that Y = y3. 
0 The joint probabilities on the reduced universe Y = yj are not a probability 
distribution. They sum to the marginal probability f(yj), not to one. 
abilities, so the sum of probabilities in the reduced universe is one. 
0 Dividing the joint probabilities by the marginal probability scales up the prob- 
Exercises 
5.1 A discrete random variable Y has discrete distribution given in the following 
table: 
Yi 
f(Yi) 
0 
.2 
1 
.3 
2 
.3 
3 
.1 
4 
.1 
(a) Calculate P(l < Y 5 3). 
(b) Calculate E(Y). 
(c) Calculate Var(Y). 
(d) Let W = 2Y + 3. Calculate E(W). 
(e) Calculate Var(W). 
5.2 A discrete random variable Y has discrete distribution given in the following 
table: 
0 
.1 
1 
.2 
2 
.3 
5 
.4 
(a) Calculate P(0 < Y < 2). 
(b) Calculate E(Y). 
(c) Calculate Var(Y). 
(d) Let W = 3Y - 1. Calculate E(W) 
(e) Calculate Var(W). 

98 
DISCRETE RANDOM VARIABLES 
5.3 Let Y be binomial (n = 5, rr = .6). 
(a) Calculate the mean and variance by filling in the following table: 
Sum 
i. E ( Y )  = 
ii. Var(Y) = 
(b) Calculate the mean and variance of Y using Equations 5.7 and 5.8, 
respectively. Do you get the same results as in part (a)? 
5.4 Let Y be binomial (n = 4, T = .3). 
(a) Calculate the mean and variance by filling in the following table: 
i. E ( Y )  = 
ii. Var(Y) = 
(b) Calculate the mean and variance of Y using Equations 5.7 and 5.8, 
respectively. Do you get the same as you got in part (a)? 
5.5 Suppose there is an urn containing 20 green balls and 30 red balls. A single 
trial consists of drawing a ball randomly from the urn, recording its color, and 
then putting it back in the urn. The experiment consists of 4 independent trials. 
List each outcome (sequence of 4 trials) in the sample space together with 
its probability. What do you notice about the probabilities of outcomes 
that have the same number of green balls? 
Let Y be the number of green balls drawn. List the outcomes that make 
up each of the following events: 
Y=O Y = l  Y = 2  Y = 3  Y = 4  

EXERCISES 
99 
X
I
 
Y 
1
2
3
4
5
 
1 
.02 
.04 
.06 
.08 
.05 
2 
.08 
.02 
.10 
.02 
.03 
3 
.05 
.05 
.03 
.02 
.10 
4 
.lo .04 
.05 
.03 
.03 
f (Y) 
(c) What can you say about P ( Y  = y) in terms of "number of outcomes 
where Y = y, and the probability of any particular sequence of outcomes 
where Y = y. 
(d) Explain how this relates to the binomial probability function. 
5.6 Suppose there is an urn containing 20 green balls and 30 red balls. A single 
trial consists of drawing a ball randomly from the urn, recording its color. 
This time the ball is not returned to the urn. The experiment consists of 4 
independent trials. 
(a) List each outcome (sequence of 4 trials) in the sample space together with 
its probability. What do you notice about the probabilities of outcomes 
that have the same number of green balls. 
(b) Let Y be the number of green balls drawn. List the outcomes that make 
up each of the following events: 
Y=O Y = l  Y = 2  Y = 3  Y = 4  
(c) What can you say about P ( Y  = y) in terms of "number of outcomes 
where Y = y, and the probability of any particular sequence of outcomes 
where Y = y. 
(d) Explain what this means in terms of the hypergeometric distribution. 
Hint: write this in terms of factorials, then rearrange the terms. 
5.7 Let Y have the Poisson(p = 2 )  distribution. 
(a) Calculate P ( Y  = 2). 
(b) Calculate P(Y 5 2). 
(c) Calculate P ( l  5 Y < 4). 
5.8 Let Y have the Poisson(p = 3) distribution. 
(a) Calculate P ( Y  = 3). 
(b) Calculate P ( Y  5 3). 
(c) Calculate P(1 5 Y < 5 ) .  
5.9 Let X and Y be jointly distributed discrete random variables. Their joint 
probability distribution is given in the following table: 
f - 
- 

100 
DISCRETE RANDOM VARIABLES 
(a) Calculate the marginal probability distribution of X .  
(b) Calculate the marginal probability distribution of Y. 
(c) Are X and Y independent random variables? Explain why or why not. 
(d) Calculate the conditional probability P(X = 31Y = 1). 
5.10 Let X and Y be jointly distributed discrete random variables. Their joint 
probability distribution is given in the following table: 
1
1
 
2 
3 
4 
5
1
 
1 
1 .015 
.030 
.010 
.020 
.025 I 
.030 
.060 
.020 
.040 
.050 
,045 .090 
.030 
.060 
,075 
.060 
.120 
.040 
,080 
.lo0 
f (Y) 
(a) Calculate the marginal probability distribution of X. 
(b) Calculate the marginal probability distribution of Y. 
(c) Are X and Y independent random variables? Explain why or why not. 
(d) Calculate the conditional probability P(X = 21Y = 3). 

6 
Bayesian Inference 
for Discrete Random 
Variables 
In this chapter we introduce Bayes’ theorem for discrete random variables. Then we 
see how we can use it to revise our beliefs about the parameter, given the sample data 
that depends on the parameter. This is how we will perform statistical inference in a 
Bayesian manner. 
We will consider the parameter to be random variable X ,  which has possible 
values q ,  
. . . ,XI. We never observe the parameter random variable. The random 
variable Y, 
which depends on the parameter, has possible values y1, . . . , y ~ .  
We 
make inferences about the parameter random variable X given the observed value 
Y = y j  using Bayes’ theorem. 
The Bayesian universe consists of the all possible ordered pairs (xi, 
yj) for a = 
1, . . . , I and j = 1, . . . , J .  This is analogous to the universe we used for joint random 
variables in the last chapter. However, we will not consider the random variables X 
and Y the same way. The events (X = q), . . . , (X = x ~ )  
partition the universe, 
but we never observe which one has occurred. The event Y = y j  is observed. 
We know that the Bayesian universe has two dimensions, the horizontal dimension 
which is observable, and the vertical dimension which is unobservable. In the 
horizontal direction it goes across the sample space which is the set of all possible 
values, {yl, . . . , y ~ } ,  
of the observed random variable Y. 
In the vertical direction it 
goes through the parameter space, which is the set of all possible parameter values, 
{ 21, . . . , X I } .  The Bayesian universe for discrete random variables is shown in Table 
6.1. This is analogous the Bayesian universe for events described in Chapter 4. The 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
101 

102 
BAYESIAN lNFERENCE FOR DISCRETE RANDOM VARIABLES 
Table 6.1 The Bayesian universe 
.
.
.
 
.
.
.
 
.
.
.
 
.
.
.
 
.
.
.
 
.
.
.
 
parameter value is unobserved. Probabilities are defined at all points in the Bayesian 
universe. 
We will change our notation slightly. We will use f() to denote a probability dis- 
tribution (conditional or unconditional) that contains the observable random variable 
Y ,  and g() to denote a probability distribution (conditional or unconditional) that 
only contains the (unobserved) parameter random variable X .  This clarifies the dis- 
tinction between Y, 
the random variable that we will observe, and X ,  the unobserved 
parameter random variable that we want to make our inference about. Each of the 
joint probabilities in the Bayesian universe is found using the multiplication rule 
f(%Yj) = &i) x f(Yjl4. 
The marginal distribution of Y is found by summing the columns. We show the joint 
and marginal probability function in Table 6.2. Note that this is similar to how we 
presented the joint and marginal distribution for two discrete random variables in the 
previous chapter (Table 5.3). However, now we have moved the marginal probability 
function of X over to the left-hand side and call it the prior probability function of 
the parameter X to indicate it is known to us at the beginning. We also note the 
changed notation. 
When we observe Y = yj, the reduced Bayesian universe is the set of ordered 
pairs in thejth column. This is shown in Table 6.3. The posterior probability function 
of X given Y = yj is given by 
Let us look at the parts of the formula. 
The prior distribution of the discrete random variable X is given by the prior 
probability function g(zi), for i = 1,. . . , n. This is what we believe the 
probability of each xi to be before we look at the data. It must come from prior 
experience, not from the current data. 

BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
103 
Table 6.2 The joint and marginal distributions of X and Y 
prior 
j]r_l- 
I 
Y1 
Table 6.3 The reduced Bayesian universe given Y = yJ 
0 Since we observed Y = yj, the likelihood of the discrete parameter random 
variable is given by the likelihoodfunction f(yj 1.i) 
for i = 1, . . . , n. This is 
the conditional probability function of Y given X = xi evaluated at yj, the 
value that actually occurred and where X is allowed to vary over its whole 
range for xi, . . . , x,. We must know the form of the conditional observation 
distribution, as it shows how the distribution of the observation Y depends 
on the value of the random variable X ,  but we see that it only needs to be 
evaluated at the value that actually occurred, yj. The likelihood function is the 
conditional observation distribution evaluated on the reduced universe. 
The posterior probability distribution of the discrete random variable is given 
by the posterior probability function g(xi 1 yj) evaluated at xi for i = 1, . . . , n, 
given Y = yj 

104 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
The formula gives us a method for revising our belief probabilities about the possible 
values of X given that we observed Y = y3. 
Example 8 There is an urn containing a total of 5 balls, some of which are red and 
the rest of which are green. We don’t know how many of the balls are red. Let the 
random variable X be the number of red balls in the urn. Possible values of X are 
x, = i for i = 0,. . . , 5 .  Since we don’t have any idea about the number of red balls, 
we will assume all possible values are equally likely. Our prior distribution of X is 
We will draw a ball at random from the urn. The random variable Y=l ifdraw 
is red, 0 otherwise. Conditional observation distribution of Y / X  is P ( Y  = 1/X = 
2,) = i / 5  and P(Y = 0 / X  = xi) = (5 - i ) / 5 .  The jointprobabilities are found by 
multiplying the prior probabilities times the conditional observation probabilities. 
The marginal probabilities of Y are found by summing the joint probabilities down 
the columns. These are shown in Table 6.4. 
Suppose the selected ball is red, so the reduced universe is in the column labelled 
y3 = 1. The conditional observation probabilities in that column are highlighted. 
They form the likelihood function. Table 6.5 shows the steps forjnding the posterior 
distribution of X given Y = 1. 
Notice that the only column that was used to find the posterior probability distri- 
bution was the in the reduced universe, the column Y = 1. The joint probability came 
from multiplying the prior probabilities times the likelihood function. The posterior 
probability equals the prior probability times likelihood divided by the sum of prior 
probabilities times likelihoods: 
d o )  = 9 0 )  = g(2) = d 3 )  = d 4 )  = d 5 )  = 1/6 
Thus a simpler way ofjinding the posterior probability is to use only the column in 
the reduced universe. Its probability is product of the prior times the likelihood. This 
is shown in Table 6.6. 
Steps for Bayes’ Theorem Using Table 
0 Set up a table with columns for parameter value, priol; likelihood, prior x 
0 Put in the parameter values, the prior, and the likelihood in their respective 
0 Multiply each element in the prior column by the corresponding element in 
the likelihood column and put the results in the prior x likelihood column. 
likelihood and posterior. 
columns. 
0 Sum the prior x likelihood column. 
Divide each element of prior x likelihood column by the sum. 
0 Put these posterior probabilities in the posterior column! 

BAYESIAN lNFERENCE FOR DISCRETE RANDOM VARIABLES 
105 
~~ 
Xt 
0 
1 
2 
3 
4 
5 
f ( Y 3 )  
Table 6.4 The joint and marginal probability distributions 
prior 
yj = 0 
probability 
116 
1 x 5 - - 5  
1 x 4 = 4  
1 x 3 - 3  
1 x 2 - - 2  
I x l - l  
1 x o - - o  
6 
5 - 3 0  
1 /6 
6 
5 
30 
116 
6 
5 - 3 0  
6 
5 - 3 0  
116 
6 
5 - 3 0  
6 
5 - 3 0  
116 
1 I6 
15 
R n 
Table 6.5 Finding the posterior probabilities of XIY = 1 
Xi 
prior 
probability 
1 /6 
116 
116 
116 
116 
1 /6 
y j  = 0 
L x 5 - 5  
1 x 4 - 4  
1 x 3 - - 3  
1 x 2 - - 2  
I x I = . L .  
I x O - 0  
6 
5 - 3 0  
6 
5 - 3 0  
6 
5 - 3 0  
6 
5 - 3 0  
6 
5 
30 
6 
5 - 3 0  
yj = 1 
posterior 
probabil it J 
0 
1
1
-
1
 
3Gl5 - i5 
El? - iz 
-1- 
4
1
 
= 4 
3 1 - 3  
30 2 
Table 6.6 Simplified table for finding the posterior probabilities of XIY = 1 
prior 
1 /6 
1 /6 
116 
1 I6 
116 
likelihood 1 
prior x likelihood 
f
i
 
1 
1 x 2 - 2  
1 x 3 - 3  
G X T - 3 0  
6 
5-30 
6 
5-30 
1 
4 - 4  
posterior 1 

106 
I 
1 
- 
Xi 
0 
1 
2 
3 
4 
5 
- 
- 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
Table 6.7 The posterior probability distribution after second observation 
prior 
0 
1/15 
2/15 
3/15 
411 5 
5/15 
1 i kel i hood 
?? 
4 
4 
4 
2 
4 
1 
4 
0 
4 
3 
prior x likelihood 
0 
1 - 
15 
1 - 
10 - 
10 
1 - 
15 
0 
posterior 
6.1 TWO EQUIVALENT WAYS OF USING BAYES' THEOREM 
We may have more than one data set concerning a parameter. They might not even 
become available at the same time. Should we wait for the second data set, combine 
it with the first, and then use Bayes' theorem on the combined data set? This would 
mean that we have to go back to scratch every time more data became available, 
which would result in a lot of work. Another approach requiring less work would 
be to use the posterior probabilities given the first data set, as the prior probabilities 
for analyzing the second data set. We will find that these two approaches lead to the 
same posterior probabilities. This is a significant advantage to Bayesian methods. 
In frequentist statistics, we would have to use the first approach, re-analyzing the 
combined data set when the second one arrives. 
Analyzing the observations sequentially one at a time. Suppose that we 
randomly draw a second ball out of the urn without replacing the first. Suppose 
the second draw resulted in a green ball, so Y = 0. We want to find the posterior 
probabilities of X given the results of the two observations, red first, green second. 
We will analyze the observations in sequence using Bayes' theorem each time. We 
will use the same prior probabilities as before for the first draw. However, we will 
use the posterior probabilities from the first draw as the prior probabilities for the 
second draw. The results are shown in Table 6.7. 
Analyzing the observations all together in a single step. Alternatively, 
we could consider both draws together, then revise the probabilities using Bayes' 
theorem only once. Initially, we are in the same state of knowledge as before. So we 
take the same prior probabilities that we originally used for the first draw when we 
were analyzing the observations in sequence. All possible values of X are equally 
likely. The prior probability function is g(z) = $ for z = 0, . . . , 5 .  
Let YI and Y2 be the outcome of the first and second draw, respectively. The 
probabilities of the second draw depend on the balls left after the first draw. By the 

TWO EQUIVALENT WAYS OF USING BAYES’ THEOREM 
107 
Table 6.8 The joint distribution of X ,  Yl, 
Yz and marginal distribution of Y I ,  
YZ 
prior 
116 
116 
1 16 
116 
1 I6 
116 
f (Yl, Y2) 
Table 6.9 The posterior probability distribution given YI 
= 1 and YZ = 0 
prior 
116 
116 
1 I6 
1 16 
1 I6 
116 
Yjl, Yjz 
090 
20 
120 
12 
120 
6 
120 
2 
120 
- 
- 
- 
- 
0 
0 
0 
4 
120 
6 
120 
6 
120 
4 
120 
- 
- 
- 
- 
0 
0 
4 
120 
6 
120 
6 
120 
4 
120 
- 
- 
- 
0 
Ya 
> Yj2 
1,1 
0 
0 
2 
120 
6 
120 
12 
120 
20 
120 
- 
- 
I 
201120 
1 f(YhY2) I 
posterior 
0 
=O 
120 120 
-5 
4 1 2 0  
- 1  
L
p
L
 =3 
10 
1 2 0 / m  
= 10 
4/20 = -  
1 
120 120 
6 
20 
- 
120 120 
5 
0 
= o  
1 .oo 
multiplication rule, the observation probability conditional on X is 
f(y1,yzlz) = f(Y1lX) x f(YzlY1,X). 
The joint distribution of X and Y1, 
Y2 is given in Table 6.8. The first ball was red, 
second was green, so the reduced universe probabilities are in column yj, , yj2 = 1 , O .  
The likelihood function given by the conditional observation probabilities in that 
column are highlighted. 
The first ball was red, second was green, so the reduced universe probabilities are 
in column yj, , yjz = 1 , O .  The posterior probability of X given Y1 = 1 and YZ = 0 
is found by rescaling the probabilities in the reduced universe so they sum to 1. This 
is shown in Table 6.9. 
We see this is the same as the posterior probabilities we found analyzing the 
observations sequentially, using the posterior after the first as the prior for the second. 

7 08 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
Table 6.70 The posterior probability distribution after both observations 
prior 
1 I6 
116 
116 
116 
1 I6 
116 
la keli hood 
0 
20 
4 
20 
6 
20 
6 
20 
4 
20 
0 
20 
- 
- 
- 
- 
- 
- 
prior x likelihood 
0 
120 
4 
120 
6 
120 
6 
120 
4 
120 
0 
120 
- 
___ 
- 
posterior 
O / L  
120 6 
=O 
Ail; 
= i  
6 1 1  
- 3  
6 1 1  
-
3
 - 
4 / L  
- 1  
O / L  
120 6 
= o  
120 6 
- 10 
120 6 
- 10 
120 6 
- 5  
1 .oo 
1 
6 
This shows that it makes no difference whether you analyze the observations one at 
a time in sequence using the posterior after the previous step as the prior for the next 
step, or whether you analyze all observations together in a single step starting with 
your initial prior! 
Since we only use the column corresponding to the reduced universe, it is simpler 
to find the posterior by multiplying prior times likelihood and rescaling to make it a 
probability distribution. This is shown in Table 6.10. 
6.2 BAYES' THEOREM FOR BINOMIAL WITH DISCRETE PRIOR 
We will look at using Bayes' theorem when the observation comes from the binomial 
distribution, and there are only a few possible values for the parameter. Yl7r has the 
binomial n, 7r distribution. (There are n independent trials, each of which can result 
in "success" or "failure" and the probability of success 7r remains the same for all 
trials. Y is the total number of "successes" over the n trials.) There are I discrete 
possible values of 7r1, . . . , T I .  
Set up a table for the observation distributions. Row i correspond to the binomial 
n, 7ri probability distribution. Column j corresponds to Y = j (There are n + 1 
columns corresponding to 0,. . . , n.) These binomial probabilities can be found in 
Table B.l in Appendix B. The conditional observation probabilities in the reduced 
universe (column that corresponds to the actual observed value) is called the likeli- 
hood. 
0 We decide on our prior probability distribution of the parameter. They give our 
prior belief about each possible value of the parameter 7r. If we have no idea 
beforehand, we can choose the prior distribution that has all values equally 
likely. 
0 The joint probability distribution of the parameter 7r and the observation Y is 
found by multiplying the conditional probability of Y 17r by the prior probability 
of T .  

BAYES' THEOREM FOR BINOMIAL WITH DISCRETE PRIOR 
709 
x 
.4 
.5 
.6 
Table 6.7 7 
The joint probability distribution found by multiplying marginal distribution of 
7r (the prior) by the conditional distribution of Y given T (which is binomial). Y = 3 was 
observed, so the binomial probabilities of Y = 3 (the likelihood) are highlighted. 
prior 
0 
1 
2 
3 
4 
; 
ix.1296 
ix.3456 
ix.3456 
x .I536 ix.0256 
; i x .0625 
$ x .25OO i x.3750 
f x .2500 f x .0625 
3 
1 
ix.0256 
ix.1536 
ix.3456 i x .3456 ix.1296 
x 
prior 
0 
1 
2 
1 
.0432 
.1152 
.1152 
.0208 
.0833 
.1250 
3 
1 
3 
.4 
.5 
3 
4 
.0512 
,0085 
.0833 
.0208 
1 
3 
.6 
marginal 
0 The marginal distribution of Y is found by summing the joint distribution down 
the columns. 
.0085 
.0512 
.1152 
,1152 
.0432 
.0725 
.2491 
.3554 
.2497 
.0725 
Now take the observed value of Y. 
It is the only column that is now relevant. It 
contains the probabilities of the reduced universe. Note that it is the prior times the 
likelihood. The posterior probability of each possible value of x is found by dividing 
that row's element in the relevant column by the marginal probability of Y in that 
column. 
Example 9 Let Y / x  be binomial (n = 4, x). Suppose we consider there are only 
three possible values for x, .4,.5, and .6. We will assume they are equally likely. 
The prior distribution of x and joint distribution of x and Y are given in Table 6.11. 
The joint probability distribution f (xi, 
yj) is found by multiplying the conditional 
observation distribution f (yj ixi) times the prior distribution g(xi). In this case, the 
conditional observation probabilities come from the binomial (n = 4, x) distribution. 
These binomial probabilities come from Table B.l in Appendix B. Suppose Y = 3 
was observed. The reduced universe is the column for Y = 3. The conditional 
observation probabilities in that column is called the likelihood and is highlighted. 
The marginal distribution of Y is found by summing the joint distribution of x and 
Y down the columns. The prior distribution of x , joint probability distribution of 
( R ,  Y )  , and marginal probability distribution of Y are shown in Table 6.12. 
Given that Y = 3 was observed, only the column labelled 3 is relevant. The 
prior distribution of x, joint probability distribution of (x, 
Y ) ,  marginal probability 

110 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
7r 
.4 
.5 
.6 
Table 6.13 The joint, marginal, and posterior probability distribution of n given Y = 3. 
Note the posterior is found by dividing the joint probabilities in the relevant column by their 
sum. 
prior 
0 
1 
2 
3 
4 
posterior 
,0432 
.1152 
.1152 
.0512 
. 0085 - =.205 
i 
,0208 
.0833 
.1250 
.0833 
.0208 - =.334 
i 
,0085 
.0512 
.1152 
.1152 
.0432 # 
=.461 
marginal 
Table 6.14 The simplified table for finding posterior distribution given Y = 3 
.0725 
.2497 
.3554 
.2497 
.0725 
1 .ooo 
7r 
.4 
.5 
.6 
distribution of Y ,  and posterior probability distribution of 7rlY = 3 are shown in 
Table 6.13. 
Note that the posterior is proportional to prior times likelihood. We didn’t have 
to set up the whole joint probability table. It is easier to only look at the reduced 
universe column. The posterior is equal to prior times likelihood divided by the 
marginal probability of the observed value. The results are shown in Table 6.14. 
prior 
likelihood 
prior x likelihood 
posterior 
= .205 
,0512 
,2497 
,2497 
,2497 
.1536 
.0512 
.2500 
.0833 
1 
3 
1 
3 
1 
3 
.0833 
= ,334 
- 
,3456 
.1152 
&!.? 
= .461 
Setting up the Table for Bayes’ Theorem on Binomial with Discrete Prior 
rn Set up a table with columns for parameter value, priol; likelihood, prior x 
likelihood, and posterior. 
rn Put in the parameter values, the prior, and the likelihood in their respective 
columns. The likelihood values are binomial(n,.rri) evaluated at the observed 
value of y. They can be found in Table B. 1, or evaluated from the formula. 
rn Multiply each element in the prior column by the corresponding element in 
the likelihood column and put in the prior x likelihood column. 
rn Sum these prior x likelihood. 
rn Divide each element of prior x likelihood column by the sum of prior x 
0 Put these in the posterior column! 
likelihood column. (This rescales them to sum to 1 .) 
marginal P(Y = 3) 
.2497 
1 .OOo 

IMPORTANT CONSEQUENCES OF BAYES' THEOREM 
11 1 
prior x likelihood 
Table 6.75 
The simplified table for finding posterior distribution given Y = 3. Note 
we are using the proportional likelihood where we have absorbed that part of the binomial 
distribution that does not depend on T into the constant. 
posterior 
prior 
(proportional) 
likelihood 
(proportional) 
.43 x .6l=.0384 
.53 x .5'=.0625 
.63 x .4l=.0864 
marginal sum 
.0384 
.0625 
=.334 
.0864 
6.3 IMPORTANT CONSEQUENCES OF BAYES' THEOREM 
Multiplying all the prior probabilities by a constant does not change 
the result of Bayes’ theorem. Each of the prior x likelihood entries in the 
table would be multiplied by the constant. The marginal entry found by summing 
down the column would also be multiplied by the same constant. Thus the posterior 
probabilities would be the same as before, since the constant would cancel out. The 
relative weights we are giving to each parameter value, not the actual weights, are 
what counts. If there is a formula for the prior, any part of it that does not contain the 
parameter can be absorbed into the constant. This may make calculations simpler for 
us! 
Multiplying the likelihood by a constant does not change the result of 
Bayes’ theorem. The prior x likelihood values would also be multiplied by the 
same constant, which would cancel out in the posterior probabilities. The likelihood 
can be considered the weights given to the possible values by the data. Again, it is 
the relative weights that are important, not the actual weights. If there is a formula 
for the likelihood, any part that does not contain the parameter can be absorbed into 
the constant, simplifying our calculations! 
Example 9 (continued) We used aprior that gave each value equalpriorprobability. 
In this example there are three possible values, so each has a prior probability equal 
to i. Let's multiply each of the 3 prior probabilities by the constant 3 to give prior 
weights equal to 1. This will simplify our calculations. The observations are binomial 
(n = 4, T ) ,  and y = 3 was observed. The formula for the binomial likelihood is 
The binomial coeficient 
does not contain the parametel; so it is a constant 
over the likelihood column. To simplify our calculations, we will absorb it into the 
constant and use only the part of the likelihood that contains the parameter In Table 
6.15 we see that this gives us the same result we obtained before. 

112 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
6.4 BAYES’ THEOREM FOR POISSON WITH DISCRETE PRIOR 
We will see how to apply Bayes’ theorem when the observation comes from a 
Poisson(p) distribution and we have a discrete prior distribution over a few discrete 
possible values for p. Y /mu is the number of counts of a process that is occurring 
randomly through time at a constant rate. The possible values of the parameter are 
p1,. . . , P I .  We decide on the prior probability distribution, g(pi) for i = 1,. . . , I .  
These give our belief weight for each possible value before we have looked at the 
data. In Section 6.2 we learned that we do not have to use the full range of possible 
observations. Instead, we set up a table only using the reduced universe column, ie, 
the value that was observed. 
Setting up the Table for Bayes’ Theorem on Poisson with Discrete Prior 
Set up table with columns for parameter value, priol; likelihood, prior x 
likelihood, and posterior. 
Put the parameter value, priol; and likelihood in their respective columns. The 
likelihood values are Poisson(p) probabilities evaluated at the observed value 
of y. They can be found in Table B.5 in Appendix B, or evaluated from the 
Poisson formula. 
Multiply each element in the prior column by the corresponding element in the 
likelihood column, and enter them in the prior x likelihood column. 
Divide each prior x likelihood by the sum of the prior x likelihood column 
and put them in the posterior column. 
Example 10 Let Y lp be Poisson(p). Suppose that we believe there are only four 
possible values for p, 1,1.5,2, and 2.5. Suppose we consider that the two middle 
values, 1.5 and 2, are twice as likely as the two end values 1 and 2.5. Suppose y = 2 
was observed. Plug the value y = 2 into formula 
to give the likelihood. Alternatively, we couldjnd the values for the likelihood from 
Table B.5 in Appendix B. The results are shown in the Table 6.16. Note: We could 
use the proportional prior and the proportional likelihood and we would get the same 
results for the posteriol: 
Main Points 
The Bayesian universe has two dimensions. The vertical dimension is the 
parameter space and is unobservable. The horizontal dimension is the sample 
space and we observe which value occurs. 

EXERCISES 
113 
P 
1 .o 
1.5 
2.0 
2.5 
Table 6.16 The simplified table for finding posterior distribution given Y = 2 
prior 
likelihood 
1 
l.OZe-’.O =,I839 
1.5 
=.2510 
2.02e-2.0 
2! 
=.2707 
2! 
=.2565 
6 
2! 
1 
3 
1 
3 
1 s 
2 - 1 5  
2.52e-2 
prior x likelihood 
.0307 
.0837 
.0902 
.0428 
.2473 
posterior 
- =.124 
- =.338 
- =.365 
0 The reduced universe is the column for the observed value. 
0 For discrete prior and discrete observation, the posterior probabilities are found 
by multiplying the prior x likelihood and then dividing by their sum. 
0 When our data arrives in batches we can use the posterior from the first batch 
as the prior for the second batch. This is equivalent to combining both batches 
and using Bayes' theorem only once, using our initial prior. 
0 Multiplying the prior by a constant doesn't change the result. Only relative 
weights are important. 
0 Multiplying the likelihood by a constant doesn't change the result. 
0 This means we can absorb any part of formula that doesn't contain the parameter 
into the constant. This greatly simplifies calculations. 
Exercises 
6.1 There is an urn containing 9 balls, which can be either green or red. The 
number of red balls in the urn is not known. One ball is drawn at random from 
the urn, and its color is observed. 
(a) What is the Bayesian universe of the experiment. 
(b) Let X be the number of red balls in the urn. Assume that all possible 
values of X from 0 to 9 are equally likely. Let Yl =I if the first ball 
drawn is red, and Yl=O otherwise. Fill in the joint probability table for 
X and Yl given below: 

X 
prior 
Y1 = 0 
L 
(c) Find the marginal distribution of Yl and put it in the table. 
Y1 = 1 
(d) Suppose a red ball was drawn. What is the reduced Bayesian universe? 
(e) Calculate the posterior probability distribution of X .  
(f) Find the posterior distribution of X by filling in the simplified table: 
prior 
likelihood 
prior x likelihood 
marginal P(Y1 = 1) 
I 
1 
I 
I 
posterior 
6.2 Suppose that a second ball is drawn from the urn, without replacing the first. 
Let Y2 = 1 if the second ball is red, and let it be 0 otherwise. Use the posterior 
distribution of X from the previous question as the prior distribution for X. 
Suppose the second ball is green. Find the posterior distribution of X by filling 
in the simplified table: 

EXERCISES 
likelihood 
115 
prior x likelihood 
prior 
marginal P(Y2 = 0 )  
I 
posterior 
b 
6.3 Suppose we look at the two draws from the urn (without replacement) as a 
single experiment. The results were first draw red, second draw green. Find 
the posterior distribution of X by filling in the simplified table. 
prior & 
prior x likelihood 
posterior 
6.4 Let Yl be the number of successes in n = 10 independent trials where each 
trial results in a success or failure, and T ,  the probability of success, remains 
constant over all trials. Suppose the 4 possible values of 7r are .20, .40, .60, 
and .80. We don’t wish to favor any value over the others so we make them 
equally likely. We observe Yl = 7. Find the posterior distribution by filling in 
the simplified table. 

116 
7r 
BAYESIAN INFERENCE FOR DlSCRETE RANDOM VARIABLES 
prior 
likelihood 
prior x likelihood 
posterior 
prior 
likelihood 
likelihood 
prior x likelihood 
prior x likelihood 
posterior 
marginal P(Y1 = 7 )  
7r 1 
prior 
I-- 
posterior 
marginal P(Y = 9) 
6.7 Let Y be the number of counts of a Poisson random variable with mean b. 
Suppose the 5 possible values of p are 1, 2, 3, 4, and 5. We don’t have any 

COMPUTER EXERCISES 
11 7 
T 
0 
.2 
.4 
.6 
.8 
1 .o 
reason to give any possible value more weight than any other value, so we give 
them equal prior weight. Y = 2 was observed. Find the posterior distribution 
by filling in the simplified table. 
dT) 
.166666 
.166666 
.166666 
,166666 
.166666 
.166666 . 
prior 
1 
margin6 
likelihood 
'(Y = 2 
prior x likelihood 
posterior 
----I- 
Computer Exercises 
6.1 The Minitab macro BinoDPmac or the equivalent R function is used to find 
the posterior distribution of the binomial probability T when the observation 
distribution of Y IT 
is binomial (n, T) and we have a discrete prior for T. 
Details for invoking BinoDPmac are found in Appendix C, and details for the 
equivalent R function are found in Appendix D. 
Suppose we have 8 independent trials and each has one of two possible either 
success or failure. The probability of success remains constant for each trial. 
In that case, Y/T 
is binomial (n = 8, T). Suppose we only allow that there are 
6 possible values of T ,  0, .2, .4, .6, .8, and 1 .O. In that case we say that we have 
a discrete distribution for T. Initially we have no reason to favor one possible 
value over another. In that case our we would give all the possible values of T 
probability equal to 5. 

118 
BAYESIAN INFERENCE FOR DISCRETE RANDOM VARIABLES 
(a) Identify the matrix of conditional probabilities from the output. Relate 
these conditional probabilities to the binomial probabilities in Table B. 1. 
(b) What column in the matrix contains the likelihoods? 
(c) Identify the matrix of joint probabilities from the output. How are these 
(d) Identify the marginal probabilities of Y from the output. How are these 
(e) How are the posterior probabilities found? 
joint probabilities found? 
found? 
6.2 Suppose we take an additional 7 trials, and achieve 2 successes. 
(a) Let the posterior after the 8 trials and 3 successes in the previous problem 
be the prior and use BinoDPmac or the equivalent R function to find the 
new posterior distribution for T .  
(b) In total, we have taken 15 trials and achieved 5 successes. Go back to the 
original prior and use BinoDPmac or the equivalent R function to find 
the posterior after the 15 trials and 5 successes. 
(c) What does this show? 
6.3 The Minitab macro PoisDPmac or the equivalent R function is used to find the 
posterior distribution when the observation distribution of Y i p  is Poisson ( p )  
and we have a discrete prior distribution for p. Details for invoking PoisDRmac 
are in Appendix C. The details for the equivalent R function are in Appendix 
D. 
Suppose there are six possible values p = 1,. . . ,6 and the prior probabilities 
are given by 
.15 
6 
.10 
Suppose the first observation is Yl = 2. Use PoisDPmac or the equivalent R 
function to find the posterior distribution g(p1g). 
(a) Identify the matrix of conditional probabilities from the output. Relate 
(b) What column in the matrix contains the likelihoods? 
these conditional probabilities to the Poisson probabilities in Table B.5. 

COMPUTER EXERCISES 
119 
(c) Identify the matrix of joint probabilities from the output. How are these 
(d) Identify the marginal probabilities of Y from the output. How are these 
(e) How are the posterior probabilities found? 
joint probabilities found? 
found? 
6.4 Suppose we take a second observation. We let the posterior after the first 
observation Yl = 2 which we found in the previous exercise be the prior for 
the second observation. 
(a) The second observation Yz = 1. Use PoisDPmmac or the equivalent R 
(b) Identify the matrix of conditional probabilities from the output. Relate 
(c) What column in the matrix contains the likelihoods? 
(d) Identify the matrix of joint probabilities from the output. How are these 
(e) Identify the marginal probabilities of Y from the output. How are these 
(f) How are the posterior probabilities found? 
function to find the new posterior distribution for p. 
these conditional probabilities to the Poisson probabilities in Table B.5. 
joint probabilities found? 
found? 

This Page Intentionally Left Blank

7 
C'ontinuous 
Random Variables 
When we have a continuous random variable, we believe all values over some 
range are possible if our measurement device is sufficiently accurate. There are 
an uncountably infinite number of real numbers in an interval, so the probability 
of getting any particular value must be zero. This makes it impossible to find the 
probability function of a continuous random variable the same way we did for a 
discrete random variable. We will have to find a different way to determine its 
probability distribution. First we consider a thought experiment similar to those done 
in Chapter 5 for discrete random variables. 
Thought Experiment 4 : Independent trials of a continuous random variable 
We start taking a sequence of independent trials of the random variable. We sketch a 
graph with a spike at each value in the sample equal to the proportion in the sample 
having that value. After each draw we update the proportions in the accumulated 
sample that have each value, and update our graph. The updating of the graph at step 
n is made by scaling all the existing spikes down by the ratio 
and adding A to the 
spike at the value observed at trial n. This keeps the sum of the spike heights equal to 
1. Figure 7.1 shows this after 25 draws. Because there are injinitely many possible 
numbers, it is almost inevitable that we don't draw any of the previous values, so we 
get a new spike at each draw. After n draws we will have n spikes, each having height 
A. Figure 7.2 shows this after 100 draws. As the sample size, n, approaches injiniv, 
the heights of the spikes shrink to zero. This means the probability of getting any 
particular value is zero. The output of this thought experiment is not the probability 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
121 

722 
CONTlNUOUS RANDOM VARIABLES 
Figure 7.7 
Sample probability function after 25 draws. 
Figure 7.2 Sample probability function after 100 draws. 
fimction, which gives theprobability of eachpossible value. This is not like the output 
of the thought experiments in Chapter 5 where the random variable was discrete. 
What we do notice is that there are some places with many spikes close by, and 
there are other places with very few spikes close by. In other words, the density 
of spikes varies. We can think of partitioning the interval into subintervals, and 
recording the number of observations that fall into each subinterval. We can form 

PROBABILITY DENSITY FUNCTION 
123 
Figure 7.3 Density histogram after 100 draws. 
a density histogram by dividing the number in each subinterval by the width of the 
subinterval. This makes the area under the histogram equal to one. Figure 7.3 shows 
the density histogram for thejrst 100 observations. 
Now let n increase, and let the width of the subintervals decrease, but at a slower 
rate than n. Figures 7.4 and 7.5 show the density histogram for thejrst 1000 and 
for the first 10,000 observations, respectively. The proportion of observations in a 
subinterval approaches the probability of being in the subinterval. As n increases, 
we get a larger number of shorter subintervals. The histograms get closer and closer 
to a smooth curve. 
7.1 PROBABILITY DENSITY FUNCTION 
The smooth curve is called the probability density function. It is the limiting shape 
of the histograms as n goes to infinity, and the width of the bars goes to 0. Its height 
at a point is not the probability of that point. The thought experiment showed us that 
probability was equal to zero at every point. Instead, the height of the curve measures 
how dense is the probability at that point. 
Since the areas under the histograms all equaled one, the total area under the 
probability density function must also equal 1: 
The proportion of the observations that lie in an interval (a, b) is given by the area of 
the histogram bars that lie in the interval. In the limit as n increases to infinity, the 
histograms become the smooth curve, the probability density function. The area of 
the bars that lie in the interval becomes the area under the curve over that interval. 
The proportion of observations that lie in the interval becomes the probability that 

124 
CONJlNUOUS RANDOM VARIABLES 
I -  
-1 L# 
I 
Figure 7.4 Density histogram after 1000 draws. 
Figure 7.5 Density histogram after 10,000 draws. 
the random variable lies in the interval. We know the area under a curve is found by 
integration, so we can find the probability that the random variable lies in the interval 
(a, b) by integrating the probability density function over that range: 

PROBABILITY DENSITY FUNCTION 
125 
Mean of a Continuous Random Variable 
In Section 3.3 we defined the mean of the random sample of observations from the 
random variable to be 
Suppose we put the observations in a density histogram where all groups have equal 
width. The grouped mean of the data is 
where mj is the midpoint of the jth bar and % is its relative frequency. Multiplying 
and dividing by the width of the bars, we get 
jj = xrnj x width x 
ni 
n x width ’ 
j 
where the relative frequency density nx2bJidth 
gives the height of bar j .  Multiplying 
it by width gives the area of the bar. Thus the sample mean is the midpoint of each 
bar times the area of that bar summed over all bars. 
Suppose we let n increase without bound, and let the number of bars increase, but 
at a slower rate. For example, as n increases by a factor of 4, we let the number of 
bars increase by a factor of 2 so the width of each bar is divided by 2. As n increases 
without bound, each observation in a group becomes quite close to the midpoint of 
the group, the number of bars increase without bound, and the width of each bar goes 
to zero. In the limit, the midpoint of the bar containing the point y approaches y, 
and the height of the bar containing pointy (which is the relative frequency density) 
approaches f(y). So, in the limit, the relative frequency density approaches the 
probability density and the sample mean reaches its limit 
which is called the expected value of the random variable. The expected value is like 
the mean of all possible values of the random variable. Sometimes it is referred to as 
the mean of the random variable Y and denoted p. 

726 
CONTlNUOUS RANDOM VARlABLES 
Variance of a Continuous Random Variable 
The expected value E(Y - E(Y))' is called the variance of the random variable. 
We can look at the variance of a random sample of numbers and let the sample size 
increase. 
As we let n increase, we decrease the width of the bars. This makes each observation 
become closer to the midpoint of the bar it is in. Now, when we sum over all groups, 
the variance becomes 
Var(y) = c y m j  - $ 2 .  
n 
j 
We multiply and divide by the width of the bar to get 
This is the square of the midpoint minus the mean times the area of the bar summed 
over all bars. As n increases to 00, the relative frequency density approaches the 
probability density, the midpoint of the bar containing the point y approaches y. and 
the sample mean 8 approaches the expected value E(Y), so in the limit the variance 
becomes 
03 
Var(Y) = E[(Y - ~ ( y ) 2 ]  
= / (Y - PI2f(Y) dY. 
V a e )  = S _ _ ( Y Z  - 2PY + P2)f(Y) dY 7 
Y2 f(Y) dY - 2P Sm Y f(Y) dY + P2 1, f(9) dY, 
L 
--oil 
(7.4) 
--oil 
The variance of the random variable is denoted u2. We can square the term in 
brackets, 
03 
break the integral into three terms, 
W 
00 
V a r W  = 
and simplify to get an alternate form for the variance: 
Var(Y) = E(Y2) - [E(Y)]2. 
(7.5) 
7.2 SOME CONTINUOUS DISTRIBUTIONS 
Uniform Distribution 
The random variable has the uniform (0,l) distribution if its probability density 
function is constant over the interval [0,1], and 0 everywhere else. 

SOME CONTINUOUS DISTRIBUTIONS 
127 
3 -  
2 -  
1 -  
0 -  
18 
E l  
_.___. 
__--__ 
*. 
,/- 
-*. 
/ - - - - - - - -  
I 
I 
I 
I 
I 
I 
Figure 7.6 The curve g(z) = kzl( 1 - 5)' for several values of k. 
It is easily shown that the mean and variance of a uniform (0,l) random variable are 
and A, 
respectively. 
Beta Family of Distributions 
The betu(a, b) distribution is another commonly used distribution for a continuous 
random variable that can only take on values 0 5 z 5 1. It has the probability 
density function 
The most important thing is that za-'(l - z)~-' determines the shape of the curve, 
and k is only the constant needed to make this a probability density function. Figure 
7.6 shows the graphs of this for a = 2 and b = 3 for a number of values of k. We 
see that the curves all have the same basic shape but have different areas under the 
curves. The value of k = 12 gives area equal to 1, so that is the one that makes a 
density function. 
The distribution with shape given by za-'(l - z ) ~ - '  is called the betu(a, b) 
distribution. The constant needed to make the curve a density function is given by 
the formula 

128 
CONTINUOUS RANDOM VARIABLES 
where T(c) is the Gamma function, which is ageneralization of the factorial function.' 
The probability density function of the beta(a, b) distribution is given by 
All we need remember is that $$$& is the constant needed to make the curve with 
shape given by xa-l(l - x)~-' a density. a equals one plus the power of x, and b 
equals one plus the power of (1 - x). 
This curve can have different shapes depending on the values a and b, so the 
beta(a, b) is actually a family of distributions. The uniform(0,I) distribution is a 
special case of the beta(a, b) distribution, where a = 1 and b = 1. 
Mean of a beta distribution. The expected value of a continuous random vari- 
able x is found by integrating the variable times the density function over the whole 
range of possible values. (Since the beta(a, b) density equals 0 for 2 outside the 
interval [0,1], the integration only has to go from 0 to 1, not -m to co.) For a 
random variable having the betu(a, b) distribution, 
However, by using our understanding of the beta distribution, we can evaluate this 
integral without having to do the integration. First move the constant out in front of 
the integral, then combine the x terms by adding exponents: 
We recognize the part under the integral sign as a curve that has the betu(a + 1, b) 
shape. So we must multiply inside the integral by the appropriate constant to make 
it integrate to 1, and multiply by the reciprocal of the constant outside of the integral 
to keep the balance: 
The integral equals 1, and when we use the fact that r(c) = (c - 1) x r(c - 1) and 
do some cancellation, we get the simple formula 
U 
E ( X )  = ~ a + b  
(7.7) 
for the mean of a beta(a, b) random variable. 
'When c is an integer, r(c) = (c - l)!. The Gamma function always satisfies the equation r(c) = 
(c - 1) x r ( c  - 1) whether or not cis an integer. 

SOME CONTINUOUS DISTRIBUTIONS 
129 
Variance of a beta distribution. The expected value of afunction of a contin- 
uous random variable is found by integrating the function times the density function 
over the whole range of possible values. For a random variable having the beta(a, b) 
distribution, 
When we evaluate this integral using the properties of the beta(a, b) distribution, we 
get 
a(a + 1) 
E(x2) = (a + b + 1 ) ( ~  
+ b) ‘ 
When we substitute this formula and the formula for the mean of the beta(a, b) into 
Equation 7.5 and simplify, we find the variance of the random variable having the 
beta(a, b) distribution is given by 
ab 
(a + b)2(a + b + I) ’ 
V a r ( X )  = 
(7.8) 
Finding beta probabilities. When X has the beta(a, b) distribution, we often 
want to calculate probabilities such as 
zo 
P(X I 
20) = Jo 
g(z; a, b) dz . 
This can easily be done in Minitab. Pull down the calc menu to probability distribu- 
tions command, over to beta subcommand, and fill out the dialog box. 
Gamma Family of Distributions 
The gamma(r, v) distribution is used for continuous random variables that can take 
on nonnegative values 0 5 z < 03. It the probability density function is given by 
g(z; T ,  v) = k x zr-le--vz for 0 5 z < 03. 
The shape of the curve is determined by zr-le-vz, while k is only the constant 
needed to make this a probability density. Figure 7.7 shows the graphs of this for the 
case where T = 4 and v = 4 for several values of k .  Clearly the curves have the same 
basic shape, but have different areas under the curve. The curve with k = 42.6667 
will have area equal to 1, so it is the exact density. The distribution having shape 
given by zr-le-vx is called the gamma(r, v) distribution. The constant needed to 
make this a probability density function is given by 

130 
CONTINUOUS RANDOM VARIABLES 
2 -- 
1 -  
0 -  
2 1 ,3333 
42.6667 
85.3333 
I 
I 
I 
I 
I 
I 
0 
1 
2 
3 
4 
5 
Figure 7.7 The curve g(z) = kz3e-*” for several values of k. 
where r ( r )  is the Gamma function. The probability density of the gamma(r, w) 
distribution is given by 
(7.9) 
vrxr-l 
-ux 
e 
r ( r )  
d x ;  r, v) = 
for0 5 x < m. 
Mean of Gamma distribution. The expected value of a gamma(?-, w) random 
variable x is found by integrating the variable x times its density function over the 
whole range of possible values. It will be 
E ( X )  = 
xg(x; r, v) dx 
We recognize the part under the integral to be a curve that has the shape of a 
gamma(r + 1, w) distribution. We multiply inside the integral by the appropriate 
constant to make it integrate to 1, and outside the integral we multiply by its reciprocal 
to keep the balance. 
This simplifies to give 
r 
E ( X )  = - 
71 
(7.10) 

Variance of a gamma distribution. First we find 
E ( X 2 )  = bm 
x2g(x; 
r, w) dx 
..T 
r m  
e 
dx 
= Y 
r ( r )  0 
xT+l -vx 
We recognize the shape of a gamma(r + 2, w) under the curve, so this simplifies to 
(r + 1)r 
W 2  
E(X2) = ____ 
When we substitute this, and the formula for the mean of the gamma(r, w) into 
Equation 7.5 and simplify we find the variance of the gamma((r, w) distribution to be 
r 
V a r ( X )  = - 
W 2  
(7.11) 
Finding gamma probabilities. When X has the gamma(r, v) distribution we 
often want to calculate probabilities such as 
r x n  
This can easily be done in Minitab. Pull down the calc menu to probability distribu- 
tions command, over to gamma subcommand, and fill out the dialog box. Note: In 
Minitab the shape parameter is r and the scale parameter is :. 
Normal Distribution 
Very often data appear to have a symmetric bell-shaped distribution. In the early 
years of statistics, this shape seemed to occur so frequently that it was thought 
to be normal. The family of distributions with this shape has become known as 
the normal distribution family. It is also known as the Gaussian distribution after 
the mathematician Gauss, who studied its properties. It is the most widely used 
distribution in statistics. We will see that there is a good reason for its frequent 
occurrence. However, we must remain aware that the term normal distribution is 
only a name, and distributions with other shapes are not abnormal. 
The normal(p, 0’) distribution is the member of the family having mean p and 
variance 0’. The probability density function of a normal(p, 0’) distribution is given 
g(zlpL, a2) = k e - h ( x - p ) 2  
for -00 < x < 00, where k is the constant value needed to make this a probability 
density. The shape of the curve is determined by e-*(z-p)2. 
Figure 7.8 shows the 
curve ke-&(x-p)2 for several values of k. Changing the value of k only changes 
the area under the curve, not its basic shape. To be a probability density function, the 
area under the curve must equal 1. The value of k that makes the curve a probability 
density is k = 2. 
Jz;;c7 
by 

732 
CONTINUOUS RANDOM VARIABLES 
1.0 - 
0.8 - 
0.6 - 
,.*-. 
,' 
'\ 
, 
<\ 
0.50000 
1 .ooooo 
, 
- - , 
’s, 
I
/
 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-4 
-3 
-2 
-1 
0 
1 
2 
3 
4 
Figure 7.8 The curve g(z) = ke-h(5-0)Z for several values of k. 
Central limit theorem. The central limit theorem says that if you take a random 
sample y1 , . . . , yn from any shape distribution having mean p and variance u2, 
then the limiting distribution of 
is normal (0,l). The shape of the limiting 
distribution is normal despite the original distribution not necessarily being normal. 
A linear transformation of a normal distribution is also normal, so the shape of jj 
and C y are also normal. Amazingly, n doesn't have to be particularly large for the 
shape to be approximately normal, n 2 25 is sufficient. 
The key factor of the central limit distribution is that when we are averaging a 
large number of independent effects, each of which is small in relation to the sum, the 
distribution of the sum approaches the normal shape regardless of the shapes of the 
individual distributions. Thus any random variable that arises as the sum of a large 
number of independent effects will be approximately normal! This explains why the 
normal distribution is encountered so frequently. 
Finding probabilities using standard normal table. The standard normal 
density has mean p = 0 and variance u2 = 1. Its probability density function is 
given by 
We note that this curve is symmetric about z = 0. Unfortunately, Equation 7.2, the 
general form for finding the probability P(a 5 z 5 b), isn't of any practical use 
here. There is no closed form for integrating the standard normal probability density 
function. Instead, the area between 0 and z for values of z between 0 and 3.99 has 
been numerically calculated and tabulated in Table B.2 in Appendix B. We use this 
table to calculate the probability we need. 

SOME CONTtNUOUS DlSTRl6UTlON.S 
133 
Figure 7.9 The area between -.62 and 1.37 split into two parts. 
Example 11 Suppose we want tofind P(-.62 5 Z 5 1.37). In Figure 7.9 we see 
that the shaded area between -.62 and 1.37 is the sum of the two areas between 
-.62 and 0 and between 0 and 1.37, respectively. The area between -.62 and 0 is 
the same as the area between 0 and +.62 because the standard normal density is 
symmetric about 0. In Table B.2 we find this area equals .2324. The area between 0 
and 1.37 equals .4147 from the table. So 
P(-.62 5 Z 5 1.37) = .2324 + .4147 
= .6471. 
Any normal distribution can be transformed into a standard normal by subtracting 
the mean and then dividing by the standard deviation. This lets us find any normal 
probability using the areas under the standard normal density found in Table B.2. 
Example 12 Suppose we know Y is normal with mean p = 10.8 and standard 
deviation u = 2.1, and suppose we want tofind the probability P(Y 1 9.9). 
P(Y 2 9.9) = P(Y - 10.8 2 9.9 - 10.8) 
1 %  
Y - 10.8 
9.9 - 10.8 
2.1 
= P(-- 
2.1 
2 
The left side is a standard normal. The right side is a number: Wefind this probability 
from the standard normal: 
P(Y 2 9 . 9 )  = P(Z 2 -.429) 
= .1659 + .5000 
= .6659. 
Finding beta probabilities using normal approximation. We can approx- 
imate a beta(a, b) distribution by the normal distribution having the same mean and 
variance. This approximation is very effective when both a and b are greater than or 
equal to ten. 
Example 13 Suppose Y has the beta(l2,25) distribution and we wish to find 
P(Y > .4). The mean and variance of Y are 
12 
12 x 25 
37 
372 x 38 
E ( Y )  = - = .3243 and V a r ( Y )  = 
~ 
= .005767 , 

134 
CONTINUOUS RANDOM VARIABLES 
respectively. We approximate the beta( 12,25) distribution with anormal(.3243, .005767) 
distribution. The approximate probability is 
Y - .3243 
.4 - .3243 
P(Y > .4) = P 
= P ( Z >  .997) 
= .1594. 
Finding gamma probabilities using normal approximation is not recom- 
mended. As r approaches infinity the gamma(r, v) distribution approaches the 
normal(m, s2) distribution where m = 5 and s2 = 5. However, the approach is 
very slow, and the gamma probabilities calculated using the normal approximation 
will not be very accurate unless r is quite large. (Johnson and Kotz, 1970). They 
recommend that the normal approximation to the gamma not be used for this reason, 
and they give other approximations that are more accurate. 
7.3 JOINT CONTINUOUS RANDOM VARIABLES 
We consider two (or more) random variables distributed together. If both X and Y 
are continuous random variables, they have joint density f(z, 
y), which measures 
the probability density at the point (z, y). This would be found by dividing the plane 
into rectangular regions by partitioning both the z axis and y axis. We look at the 
proportion of the sample that lie in a region. We increase n, the sample size of the 
joint random variables without bound, and at the same time decrease the width of 
the regions (in both dimensions) at a slower rate. In the limit, the proportion of the 
sample lying in the region centered at (z, y) approaches the joint density f(z, 
y). 
Figure 7.10 shows a joint density function. 
We might be interested in determining the density of one of the joint random 
variables by itself, its marginal density. When X and Y are joint random variables 
that are both continuous, the marginal density of Y is found by integrating the joint 
density over the whole range of X: 
f (Y) = sm 
f (zcI Y) dz > 
-m 
and vice versa. (Finding the marginal density by integrating the joint density over 
the whole range of one variable is analogous to finding the marginal probability 
distribution by summing the joint probability distribution over all possible values of 
one variable for jointly distributed discrete random variables.) 
Conditional Probability Density 
The conditional density of X given Y = y is given by 

JOINT CONTINUOUS AND DISCRETE RANDOM VARIABLES 
135 
0.15 
0.10 
0.05 
0.00 
3 
3 
X 
3 
Figure 7.70 A joint density. 
We see that the conditional density of X given Y = y is proportional to the joint 
density where Y = y is held fixed. Dividing by the marginal density f(y) makes the 
integral of the conditional density over the whole range of 5 equal 1. This makes it 
a proper density function. 
7.4 JOINT CONTINUOUS AND DISCRETE RANDOM VARIABLES 
It may be that one of the variables is continuous, and the other is discrete. For 
instance, let X be continuous, and let Y be discrete. In that case, f(z, 
yj) is a joint 
probability-probability density function. In the x direction it is continuous, and in 
the y direction it is discrete. This is shown in Figure 7.1 1. In this case, the marginal 
density of the continuous random variable X is found by 
j 
and the marginal probability function of the discrete random variable Y is found by 
The conditional density of X given Y = yj is given by 

736 
CONTINUOUS RANDOM VARIABLES 
Flgufe 7.7 7 
A joint continuous and discrete distribution. 
We see that this is proportional to the joint probability-probability density function 
f(z, yj) where z is allowed to vary over its whole range. Dividing by the marginal 
probability f(yj) just scales it to be a proper density function (integrates to 1). 
Similarly, the conditional distribution of Y = y j  given x is found by 
This is also proportional to the joint probability-probability density function f (x, yj) 
where x is fixed, and Y is allowed to take on all the possible values y1, . . . , y ~ .  
Main Points 
0 The probability that a continuous random variable equals any particular value 
is zero! 
0 The probability density function of a continuous random variable is a smooth 
curve that measures the density of probability at each value. It is found as the 
limit of density histograms of random samples of the random variable, where 
the sample size increases to infinity and the width of the bars goes to zero. 
0 The probability a continuous random variable lies between two values a and 
b is given by the area under the probability density function between the two 
values. This is found by the integral 
b 
P(a < X < b) = 
f(z) 
d z .  

MAIN POlNTS 
137 
0 The expected value of a continuous random variable X is found by integrating 
z times the density function f(z) 
over the whole range. 
05 
0 A beta(a, b) random variable has probability density 
0 The mean and variance of a beta(a, b) random variable are given by 
a 
a x b  
and 
V a r ( X )  = (a + b)2 x (a + b + 1) 
E ( X )  = ~ a + b  
0 A gamma(r, w) random variable has probability density 
0 The mean and variance of a gamma(r, v) random variable are given by 
r 
r 
E ( X )  = - 
and 
V a r ( X )  = - 
v 
0 2  
0 A normal (b, g 2 )  random variable has probability density 
where p is the mean, and u2 is the variance. 
distribution f(y) having mean ,u and variance cz, 
the distribution of 
0 The central limit theorem says that for a random sample y1,. . . yn from any 
Y-P 
n l f i  
is approximately normal (0,l) for n > 25. This is regardless of the shape of 
the original density f(y). 
0 By reasoning similar to that of the central limit theorem, any random variable 
that is the sum of a large number of independent random variables will be 
approximately normal. This is the reason why the normal distribution occurs 
so frequently. 
f(z, 
y) with respect to 2 over its whole range. 
0 The marginal distribution of y is found by integrating the joint distribution 

138 
CONTlNUOUS RANDOM VARIABLES 
0 The conditional distribution of x given y is proportional to the joint distribution 
f (5, y) where y fixed and x is allowed to vary over its whole range. 
Dividing by the marginal distribution of f(y) scales it properly so that f(yIx) 
integrates to 1 and is a probability density function. 
Exercises 
7.1 Let X have a beta(3,5) distribution. 
(a) Find E ( X ) .  
(b) Find V a r ( X ) .  
7.2 Let X have a beta(l2,4) distribution. 
(a) Find E ( X ) .  
(b) Find V a r ( X ) .  
7.3 Let X have the uniform distribution. 
(a) Find E ( X ) .  
(b) Find V a r ( X ) .  
(c) Find P ( X  5 .25). 
(d) Find P(.33 < X < .75). 
7.4 Let X be a random variable having probability density function 
f(z) = 22 for 0 5 x 5 1. 
(a) Find P(X 2 .75). 
(b) Find P(.25 5 X 5 .6). 
7.5 Let Z have the standard normal distribution. 
(a) Find P(0 5 2 5 .65). 
(b) Find P(Z 2 .54). 
(c) Find P(-.35 5 Z 5 1.34). 
7.6 Let 2 have the standard normal distribution. 
(a) Find P(0 5 2 5 1.52). 
(b) Find P ( Z  2 2.11). 

EXERCISES 
139 
(c) Find P(-1.45 5 2 5 1.74). 
7.7 Let Y be normally distributed with mean p = 120 and variance uz = 64. 
(a) FindP(Y 5 130). 
(b) FindP(Y 1135). 
(c) Find P(114 5 Y 5 127). 
7.8 Let Y be normally distributed with mean p = 860 and variance uz = 576. 
(a) Find P(Y 5 900). 
(b) FindP(Y 2 825). 
(c) Find P(840 5 Y 5 890). 
7.9 Let Y be distributed according to the betu(l0, 12) distribution. 
(a) Find E(Y). 
(b) Find Var(Y). 
(c) Find P ( Y  > .5) using the normal approximation. 
7.10 let Y be distributed according to the betu(l5,lO) distribution. 
(a) Find E(Y). 
(b) Find Var(Y). 
(c) Find P(Y < .5) using the normal approximation. 
7.1 1 Let Y be distributed according to the gummu(l2,4) distribution. 
(a) Find E(Y). 
(b) Find Var(Y). 
(c) FindP(Y 5 4) 
7.12 Let Y be distributed according to the gurnmu(26,5) distribution. 
(a) Find E(Y). 
(b) Find Var(Y). 
(c) Find P(Y > 5 )  

This Page Intentionally Left Blank

Bayesian Inference 
for Binomial Proportion 
Frequently there is a large population where T ,  a proportion of the population, has 
some attribute. For instance, the population could be registered voters living in a 
city, and the attribute is "plans to vote for candidate A for mayor." We take a random 
sample from the population and let Y be the observed number in the sample having 
the attribute, in this case the number who say they plan to vote candidate A for mayor. 
We are counting the total number of "successes" in n independent trials where each 
trial has two possible outcomes, "success" and "failure." Success on trial i means the 
item drawn on trial i has the attribute. The probability of success on any single trial 
is T ,  the proportion in the population having the attribute. This proportion remains 
constant over all trials because the population is large. 
The conditional distribution of the observation Y ,  the total number of successes 
in n trials given the parameter T ,  is binomial(n, T ) .  The conditional probability 
function for y given T is given by 
Here we are holding T fixed and are looking at the probability distribution of y over 
its possible values. 
If we look at this same relationship between T and y, but hold y fixed at the 
number of successes we observed, and let T vary over its possible values, we have 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wdey & Sons, Inc. 
141 

142 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
the likelihood function given by 
We see that we are looking at the same relationship as the distribution of the obser- 
vation y given the parameter T ,  but the subject of the formula has changed to the 
parameter, for the observation held at the value that actually occurred. 
To use Bayes’ theorem, we need a prior distribution g(T) that gives our belief 
about the possible values of the parameter T before taking the data. It is important 
to realize that the prior must not be constructed from the data. Bayes’ theorem 
is summarized by posterior is proportional to the prior times the likelihood. The 
multiplication in Bayes’ theorem can only be justified when the prior is independent 
of the likelihood!’ This means that the observed data must not have any influence 
on the choice of prior! The posterior distribution is proportional to prior distribution 
times likelihood: 
This gives us the shape of the posterior density, but not the exact posterior density 
itself. To get the actual posterior, we need to divide this by some constant k to 
make sure it is a probability distribution, meaning that the area under the posterior 
integrates to 1. We find k by integrating g ( T )  x f ( y l ~ )  over the whole range. So, in 
general, 
which requires an integration. Depending on the prior g ( T )  chosen, there may 
not necessarily be a closed form for the integral, so it may be necessary to do the 
integration numerically. We will look at some possible priors. 
8.1 USING A UNIFORM PRIOR 
If we don’t have any idea beforehand what the proportion T is, we might like to 
choose a prior that does not favor any one value over another. Or, we may want to be 
as objective as possible and not put our personal belief into the inference. In that case 
we should use the uniform prior that gives equal weight to all possible values of the 
success probability T .  Although this does not achieve universal objectivity (which is 
‘We h o w  that for independent events (or random variables) the joint probability (or density) is the 
product of the marginal probabilities (or density functions). If they are not independent, this does not 
hold. Likelihoods come from probability functions or probability density functions, so the same pattern 
holds. They can only be multiplied when they are independent. 

USING A BETA PRIOR 
143 
impossible to achieve), it is objective for this formulation of the problem’: 
g(7r) = 1 for 0 5 7r 5 1. 
Clearly, we see that in this case, the posterior density is proportional to the likelihood: 
We can ignore the part that doesn’t depend on 7r. It is a constant for all values of 
T ,  so it doesn’t affect the shape of the posterior. When we examine that part of the 
formula that shows the shape of the posterior as a function of T ,  we recognize that 
this is a betu(a, b) distribution where a = y + 1 and b = n - y + 1. So in this case, 
the posterior distribution of 7r given y is easily obtained. All that is necessary is to 
look at the exponents of T and (1 - T ) .  We didn’t have to do the integration. 
8.2 USING A BETA PRIOR 
Suppose a betu(a, b) prior density is used for 7r: 
The posterior is proportional to prior times likelihood. We can ignore the constants 
in the prior and likelihood that don’t depend on the parameter, since we know that 
multiplying either the prior or the likelihood by a constant won’t affect the results of 
Bayes’ theorem. This gives 
g(7riy) c( 7 r a + q 1  - 7ry-tn-y-1 
for 
0 5 ~ 5 1 ,  
which is the shape of the posterior as a function of 7r. We recognize that this is the 
beta distribution with parameters a’ = a + y and b’ = b + n - y. That is, we add the 
number of successes to a and add the number of failures to b: 
for 0 5 T 5 1. Again, the posterior density of T has been easily obtained without 
having to go through the integration. 
Figure 8.1 shows the shapes of betu(a, b) densities for values of a = .5,1,2,3 
and b = .5,1,2,3. This shows the variety of shapes that members of the betu(a, b) 
’There are many possible parameterizations of the problem. Any one-to-one function of the parameter 
would also be a suitable parameter. The prior density for the new parameter could be found from the 
prior density of the original parameter using the change of variable formula and would not be flat. In 
other words, it would favor some values of the new parameter over others. You can be objective in a 
given parameterization, but it would not be objective in the new formulation. Universal objectivity is not 
attainable. 

144 
BAYESlAN INFERENCE FOR BINOMIAL PROPORTION 
bela( 5.21 
beta(5 11 
bela(5, 5) 
bM( 5. 31 
betall 31 
befa(1 21 
befall 5) 
befall 1) 
b m ( 2  5) 
befall, 11 
bela(2.2) 
befall 3) 
Figure 8.1 Some beta distributions. 
family can take. When a < b, the density has more weight in the lower half. The 
opposite is true when a > b. When a = b, the betu(a, 6) density is symmetric. When 
a = f much more weight is given to values near 0, and when b = a much more 
weight is given to values near 1. We note that the uniform prior is a special case of 
the betu(a, b) prior where a = 1 and b = 1. 
Conjugate Family of Priors for Binomial Observation is the Beta Family 
When we examine the shape of the binomial likelihood function as a function of T ,  
we see that this is of the same form as the betu(a, b) distribution, a product of 7r to 
a power times (1 - T )  to another power. When we multiply the beta prior times 
the binomial likelihood, we add the exponents of T and (1 - T ) ,  respectively. So 
we start with a beta prior, we get a beta posterior by the simple rule "add successes 

CHOOSING YOUR PRIOR 
145 
to a, add failures to b.“ This makes using beta(a, b) priors when we have binomial 
observations particularly easy. Using Bayes’ theorem moves us to another member 
of the same family. 
We say that the beta distribution is the conjugate3 family for the binomial obser- 
vation distribution. When we use a prior from the conjugate family, we don’t have 
to do any integration to find the posterior. All we have to do is use the observations 
to update the parameters of the conjugate family prior to find the conjugate family 
posterior. This is a big advantage. 
Jeffreys prior for binomial. The beta(;, i) prior is known as the Jeffreys’ 
prior for the binomial. If we think of the parameter as an index of all possible 
densities the observation could come from, then any continuous function of the 
parameter would give an equally valid index.4 Jeffreys’ method gives a pries that 
is invariant under any continuous transformation of the parameter. That means that 
Jeffreys’ prior is objective in the sense that it does not depend on the particular 
parameterization we used.6 However, for most parameterizations, the Jeffreys’ prior 
gives more weight to some values than to others so it is usually informative, not 
noninformative. For further information on Jeffreys’ method for finding invariant 
priors refer to Press (1989), O’Hagan (1994), and Lee (1989). We note that Jeffreys’ 
prior for the binomial is just a particular member of the beta family of priors, so the 
posterior is found using the same updating rules. 
8.3 CHOOSING YOUR PRIOR 
Bayes’ theorem gives you a method to revise your (belief) distribution about the 
parameter, given the data. In order to use it, you must have a distribution that 
represents your belief about the parameter, before we look at the data.’ This is your 
prior distribution. In this section we propose some methods to help you choose your 
prior, as well as things to consider in prior choice. 
3Conjugate priors only exists when the observation distribution comes from the exponential family. In that 
case the observation distribution can be written f(y10) = a(0)b(y)ec(e) x T ( u ) .  The conjugate family of 
priors will then have the same functional form as the likelihood of the observation distribution. 
41f @ = h(0) is a continuous function of the parameter 0, then g+ (@), the prior for @ that corresponds to 
ge(6), the prior for 0 is found by the change of variable formula g+(@) = ge(0(@)) x g. 
5Jeffreys’ invariant prior for parameter 0 is given by g(0) 0: m, 
where I(0iy) is known as Fisher’s 
information and is given by [(sly) = -E w). 
61f we had used another parameterization and found the Jeffreys’ prior for that parameterization, then 
transformed it to our original parameter using the change of variable formula, we would have the Jeffreys’ 
prior for the original parameter. 
’This could be elicited from your coherent betting strategy about the parameter value. Having a coherent 
betting strategy means that if someone started offering you bets about the parameter value, you would not 
take a worse bet than one you already rejected, nor would you refuse to take a better bet than one you 
already accepted. 
( 

146 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
Choosing a Conjugate Prior When You Have Vague Prior Knowledge 
When you have vague prior knowledge, one of the betu(u, b) prior distributions 
shown in Figure 8.1 would be a suitable prior. For example, if your prior knowledge 
about T ,  is that 7r is very small, then betu(.5, l), betu(.5,2), betu(.5,3), betu(l,2), 
or beta( 1,3) would all be satisfactory priors. All of these conjugate priors offer easy 
computation of the posterior, together with putting most of the prior probability at 
small values of T .  It doesn’t matter very much which one you chose; the resulting 
posteriors given the data would be very similar. 
Choosing a Conjugate Prior When You Have Real Prior Knowledge by 
Matching Location and Scale 
The betu(u, b) family of distributions is the conjugate family for binorniul(n, T )  
observations. We saw in the previous section that priors from this family have 
significant advantages computationally. The posterior will be a member of the same 
family, with the parameters updated by simple rules. We can find the posterior without 
integration. The beta distribution can have a number of shapes. The prior chosen 
should correspond to your belief. We suggest choosing a betu(a, b) that matches 
your prior belief about the (location) mean and (scale) standard deviation*. Let TO 
be your prior mean for the proportion, and let 00 be your prior standard deviation for 
the proportion. 
The mean of betu(u, b) distribution is &. Set this equal to what your prior belief 
about the mean of the proportion to give 
U 
“ 0  = - 
a + b ’  
The standard deviation of beta distribution is 4 
(a+b)2$+b+l). 
Set this equal to 
what your prior belief about the standard deviation for the proportion. Noting that 
& = TO and b = 1 - “ 0 ,  we see 
a+b 
“ O ( 1  - “ 0 )  
u + b + l  
0 0  = J 
Solving these two equations for a and b gives your betu(a, b) prior. 
Precautions Before Using Your Conjugate Prior 
1. Graph your betu(a, b) prior. If the shape looks reasonably close to what you 
believe, you will use it. Otherwise, you can adjust TO and 00 until you find a 
8Some people would say that you should not use a conjugate prior just because of these advantages. 
Instead, you should elicit your prior from your coherent betting strategy. I don’t think most people cany 
around a coherent betting strategy in their head. Their prior belief is less structured. They have a belief 
about the location and scale of the parameter distribution. Choosing a prior by finding the conjugate 
family member that matches these beliefs will give a prior on which a coherent betting strategy could be 
based! 

CHOOSING YOUR PRIOR 
147 
prior whose graph approximately corresponds to your belief. As long as the 
prior has reasonable probability over the whole range of values that you think 
the parameter could possibly be in, it will be a satisfactory prior. 
2. Calculate the equivalent sample size of the prior. We note that the sample 
proportion .ir = f from a binomial(n,.rr) distribution has variance equal to 
R(lia). 
We equate this variance (at TO, the prior mean) to the prior variance. 
T O ( 1  - T o )  - 
ab 
- 
neq 
(a + b)2 x (a + b + 1) 
Since TO = & and (1 - TO) = &, the equivalent sample size is neq = 
a + b + 1. It says that the amount of information about the parameter from 
your prior is equivalent to the amount from a random sample of that size. You 
should always check if this is unrealistically high. Ask yourself, "Is my prior 
knowledge about ri really equal to the knowledge about T that I would obtain 
if I checked a random sample of size neq? If it is not, you should increase your 
prior standard deviation and recalculate your prior. Otherwise, you would be 
putting too much prior information about the parameter relative to the amount 
of information that will come from the data. 
Constructing a General Continuous Prior 
Your prior shows the relative weights you give each possible value before you see the 
data. The shape of your prior belief may not match the beta shape. You can construct 
a discrete prior that matches your belief weights at several values over the range you 
believe possible, and then interpolate between them to make the continuous prior. 
You can ignore the constant needed to make this a density, because when you multiply 
the prior by a constant, the constant gets cancelled out by Bayes' theorem. However, 
if you do construct your prior this way, you will have to evaluate the integral of 
the prior times likelihood numerically to get the posterior. This will be shown in 
Example 13. 
Example 14 Three students are constructing their prior belief about ri, the propor- 
tion of Hamilton residents who support building a casino in Hamilton. Anna thinks 
that her prior mean is .2, and her prior standard deviation is .08. The beta(a, b) 
prior that satisfies her prior belief is found by 
.2 x .8 
a + b + l  = .0g2. 
Therefore her equivalent sample size is a + b + 1 = 25. For Anna's priol; a = 4.8 
and b = 19.2. 
Bart is a newcomer to Hamilton, so he is not aware of the local feeling for or 
against the proposed casino. He decides to use a uniform prior: For him, a = b = 1. 
His equivalent sample size is a + b + 1 = 3. 

148 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
.3 
.4 
.5 
Table 8.1 
interpolating between them. 
Chris’s prior weights. The shape of his continuous prior is found by linearly 
Value I 
Weight 
2 
1 
0 
Chris can’tJit a beta(a, b) prior to match his belie$ He believes his prior prob- 
ability has a trapezoidal shape. He gives heights of his prior in Table 8.1, and he 
linearly interpolates between them to get his continuous priol: When we interpolate 
between these points, we see that Chris’s prior is given by 
20n 
for 
0 5 K 5 .lo, 
.2 
for .10 5 T 5 .30, 
5 - 1 0 ~  
for .30 5 T 5 .50. 
d T )  = 
The three priors are shown in the Figure 8.2. Note that Chris’sprior is not actually a 
density since it doesn’t have area equal to one. However; this is not a problem since 
the relative weights given by the shape of the distribution are all that is needed since 
the constant will cancel out. 
Effect of the Prior 
When we have enough data, the effect of the prior we choose will be small compared 
to the data. In that case we will find that we can get very similar posteriors despite 
starting from quite different priors. All that is necessary is that they give reasonable 
weight over the range that is indicated by the likelihood. The exact shape of the prior 
doesn’t matter very much. The data are said to “swamp the prior.” 
Example 14 (continued) The three students take a random sample of n = 100 
Hamilton residents andJind their views on the casino. Out of the random sample, 
y = 26 said they support building a casino in Hamilton. Anna’s posterior is 
beta(4.8+26,19.2+74). Bart’sposterioris beta(l+26, 1 +74). Chris’posterioris 
found using Equation 8.1. We need to evaluate Chris’ prior numerically. To do this, 
we integrate Chris’ prior x likelihood using the Minitab macro tintegral.mac. The 
three posteriors are shown in Figure 8.3. We see that the three students end up with 
very similar posteriors, despite starting with priors having quite di3erent shapes. 

SUMMARIZING THE POSTERIOR DISTRIBUTION 
149 
10 - 
8 -  
6 -  
4 -  
2 -  
0 -  
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Bart‘s posterior 
Chris’ posterior 
Figure 8.2 Anna’s, Bart’s, and Chris’ prior distribution. 
I
I
I
I
I
I
I
I
I
I
I
 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure 8.3 Anna’s, Bart’s, and Chris’ posterior distributions. 
8.4 SUMMARIZING THE POSTERIOR DISTRIBUTION 
The posterior distribution summarizes our belief about the parameter after seeing 
the data. It takes into account our prior belief (the prior distribution) and the data 
(likelihood). A graph of the posterior shows us all we can know about the parameter, 
after the data. A distribution is hard to interpret. Often we want to find a few numbers 

150 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
that characterize it. These include measures of location that determine where most 
of the probability is on the number line, as well as measures of spread that determine 
how widely the probability is spread. They could also include percentiles of the 
distribution. We may want to determine an interval that has a high probability 
of containing the parameter. These are known as Bayesian credible intervals and 
are somewhat analogous to confidence intervals. However, they have the direct 
probability interpretation that confidence intervals lack. 
Measures of Location 
First, we want to know where the posterior distribution is located on the number 
line. There are three possible measures of location we will consider: posterior mode, 
posterior median, and posterior mean. 
Posterior mode. This is the value that maximizes the posterior distribution. If 
the posterior distribution is continuous, it can be found by setting the derivative of 
the posterior density equal to zero. When the posterior g ( r l y )  is betu(a’, b’), its 
derivative is given by 
g’(Tly) = (a’ - 1)7ra’-2 x (1 - r ) b ’ - l  + 7ra’-1 x (-l)(b’ - 1)(1 - r y - 2 .  
(Note: The prime ’ has two meanings in this equation; g ’ ( r / y )  is the derivative of the 
posterior, while a’ and b’ are the constants of the beta posterior found by the updating 
rules.) Setting g’(rly) equal to 0 and solving gives the posterior mode 
a’ - 1 
a’ + b’ - 2 
mode = 
The posterior mode has some potential disadvantages as a measure of location. First, 
it may lie at or near one end of the distribution, and thus not be representative of the 
distribution as a whole. Second, there may be multiple local maximums. When we 
set the derivative function equal to zero and solve, we will find all of them and the 
local minimums as well. 
Posterior median. 
it, 50% above it. If g(r1y) is betu(a’, b’), it is the solution of 
This is the value that has 50% of posterior distribution below 
,-median 
The only disadvantage of the posterior median is that it has to be found numerically. 
It is an excellent measure of location. 
Posterior mean. 
tion. It is the expected value, or mean, of the posterior distribution. 
The posterior mean is a very frequently used measure of loca- 
1 
m’ = 1 r g ( r I y )  d r  . 

SUMMARIZING THE POSTERIOR DISTRIBUTION 
151 
The posterior mean is strongly affected when the distribution has a heavy tail. For a 
skewed distribution with one heavy tail, the posterior mean may be quite a distance 
away from most of the probability. When the posterior g(.rrly) is beta(a’, b’) the 
posterior mean equals 
(8.3) 
a’ 
a’ + b‘ 
‘ 
m‘ = - 
The beta(a, b) distribution is bounded between 0 and 1, so it does not have heavy 
tails. The posterior mean will be a good measure of location for a beta posterior. 
Measures of Spread 
The second thing we want to know about the posterior distribution is how spread 
out it is. If it has large spread, then our knowledge about the parameter, even after 
analyzing the observed data, is still imprecise. 
Posterior variance. This is the variance of posterior distribution. 
(T - m’)2g(rIy) 
d r .  
(8.4) 
When we have a beta(a‘, b‘) posterior the posterior variance is 
a’ x b’ 
(a’ + b’)2 x (a’ + b’ + 1) ’ 
Var(T/y) = 
(8.5) 
The posterior variance is greatly affected for heavy-tailed distributions. For a heavy 
tailed distribution, the variance will be very large, yet most of the probability is 
very concentrated quite close the middle of the distribution. It is also in squared 
units, which makes it hard to interpret its size in relation to the size of the mean. 
We overcome these disadvantages of the posterior variance by using the posterior 
standard deviation. 
Posterior standard deviation. This is the square root of posterior variance. It 
is in terms of units, so its size can be compared to the size of the mean, and it will be 
less affected by heavy tails. 
Percentiles of the posterior distribution. The kth percentile of the posterior 
distribution is the value T k ,  which has Ic% of the area below it. It is found numerically 
by solving 
k = 100 x 1, 
g(.rr)y) dn 
Some percentiles are particularly important. The first (or lower) quartile Q1 is the 
25th percentile. The second quartile , Q2 (or median) is the 50th percentile, and the 
third (or upper) quartile Q3 is the 75th percentile. 
T k  

152 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
Anna 
Bart 
Chris 
Table 8.2 Measures of location and spread of posterior distributions 
beta(30.8,93.2) 
.248 
.247 
.039 
.053 
beta(27,75) 
.265 
.263 
.043 
,059 
numerical 
,261 
.255 
.04 1 
.057 
I Person I 
Posterior 
I 
Mean 
Median 1 
Std. Dev. 
IOR 
The intetquarfile range. The interquartile range 
IQR = Q 3  - &I 
is a useful measure of spread that is not affected by heavy tails. 
Example 14 (continued) Anna, Bart, and Chris computed some measures of loca- 
tion and spread for theirposterior distributions. Anna and Bart used Equations 8.3 
and 8.5 tojnd their posterior mean and variance, respectively, since they had beta 
posteriors. Chris used Equations 8.2 and 8.4 tojnd his posterior mean and variance 
since his posterior did not have the beta distribution. He evaluated the integrals 
numerically using the Minitab macro tintegral.mac. Their posterior means, medians, 
standard deviations, and interquartile ranges are shown in Table 8.2. We see clearly 
that the posterior distributions have similar summary statistics, despite the difSerent 
priors used. 
8.5 ESTIMATING THE PROPORTION 
A point estimate ? is a statistic calculated from the data used as an estimate of the 
parameter T .  Suitable Bayesian point estimates are single values such as measures of 
location calculated from the posterior distribution. The posterior mean and posterior 
median are often used as point estimates. 
The posterior mean square of an estimate. The posterior mean square of 
an estimator 7i of the proportion T is 
P l  
PMS(7i) = lo 
(T - ?)’g(.rrly) d T .  
It measures the average squared distance (with respect to the posterior) that the 
estimate is away from the true value. Adding and subtracting the posterior mean m’, 
we get 
PMS(?) = 1 (T - m’ + m’ - ? ) 2  g(.irly) d~ . 
1 
Multiplying out the square we get 
1 
PMS(?) = 1 [(T - m’)2 + 2 ( ~  
- m’)(m’ - ?) + (m’ - A)’] g(TIy) d T .  

BAYESIAN CREDIBLE INTERVAL 
753 
We split the integral into three integrals. Since both m’ and .ir are constants with 
respect to the posterior distribution when we evaluate the integrals, we get 
PMS(7i) = V a r ( r / y )  + 0 + (m’ - % ) 2 .  
(8.7) 
This is the posterior variance of r plus the square of the distance .ir is away from the 
posterior mean m’. 
The last term is a square and is always greater than or equal to zero. We see 
that on average, the squared distance the true value is away from the posterior mean 
m’ is less than that for any other possible estimate 7i, given our prior belief and the 
observed data. The posterior mean is the optimum estimatorpost-data. That’s a good 
reason to use the posterior mean as the estimate, and it explains why the posterior 
mean is the most widely used Bayesian estimate. We will use the posterior mean as 
our estimate for T .  
8.6 BAYESIAN CREDIBLE INTERVAL 
Often we wish to find a high probability interval for the parameter. A range of values 
that has a known high posterior probability, (1 - a), of containing the parameter is 
known as a Bayesian credible interval. It is sometimes called Bayesian confidence 
interval. In the next chapter we will see that credible intervals answer a more 
relevant question than do ordinary frequentist confidence intervals, because of the 
direct probability interpretation. 
There are many possible intervals with same (posterior) probability. The shortest 
interval with given probability is preferred. It would be found by having the equal 
heights of the posterior density at the lower and upper endpoints, along with a total 
tail area of 1 - a. The upper and lower tails would not necessarily have equal tail 
areas. However, it is often easier to split the total tail area into equal parts and find 
the interval with equal tail areas. 
Bayesian Credible Interval for T 
If we used a beta(a, b) prior, the posterior distribution of ~ l y  
is beta(a’, b’). An equal 
tail area 95% Bayesian credible interval for T can be found by obtaining the difference 
between the 97.5 th and the 2.5 th percentiles. Using Minitab, pull down calc menu to 
probability distributions over to beta and fill in the dialog box. Without Minitab, we 
approximate the beta(a’, b’) posterior distribution by the normal distribution having 
the same mean and variance: 
(.rr(y) is approximately ~ [ r n ’ ;  
(s’)’] 
where the posterior mean is given by 

154 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
Posterior 
I 
Distribution 
j Person 
Anna 
bet~(30.8~93.2) 
Chris 
numerical 
Bart 
beta(27,75) 
Table 8.3 Exact and approximate 95% credible intervals 
Credible Interval 
Credible Interval 
Exact 
Normal Approximation 
Lower 
Upper 
Lower 
Upper 
.177 
,328 
.172 
,324 
.I84 
.354 
.183 
.355 
.181 
.340 
.I81 
.341 
and the posterior variance is expressed as 
a’b’ 
(s’)2 = (a’ + b t ) 2 ( d  + b‘ + 1) 
The (1 - a) x 100% credible region for 7r is approximately 
rn’ f 
2: 
x s’ 
(8.8) 
where zp is the value found from the standard normal table. For a 95% credible 
interval, 2,025 = 1.96. The approximation works very well if we have both a’ 2 10 
and b‘ 2 10. 
Example 14 (continued) Anna, Bart, and Chris calculated 95% credible intervals 
for 7r having equal tail areas two ways: using the exact (beta) densityfinction and 
using the normal approximation. These are shown in Table 8.3. Anna, Bart, and 
Chris have slightly different credible intervals because they started with different 
prior beliefs. But the effect of the data was much greater than the effect of their 
priors and they end up with very similar credible intervals. We see that in each case, 
the 95% credible interval for 7r calculated using the normal approximation is nearly 
identical to the corresponding exact 95% credible interval. 
Main Points 
0 The key relationship is posterior o( prior x likelihood. This gives us the 
shape of the posterior density. We must find the constant to divide this by to 
make it a density, eg., integrate to 1 over its whole range. 
1 
0 The constant we need is k = 1, g(7r) x f(y1n) d7r. In general, this integral 
does not have a closed form, so we have to evaluate it numerically. 
0 If the prior is beta(a, b), then the posterior is beta(a’, b’) where the constants 
are updated by simple rules a’ = a + y (add number of successes to a )  and 
b’ = b + n - y (add number of failures to b). 

EXERCISES 
155 
The beta family of priors is called the conjugate family for binomial observation 
distribution. This means that the posterior is also a member of the same family, 
and it can easily be found without the need for any integration. 
0 It makes sense to choose a prior from the conjugate family, which makes 
finding the posterior easier. Find the beta(a, b) prior that has mean and standard 
deviation that correspond to your prior belief. Then graph it to make sure that 
it looks similar to your belief. If so, use it. If you have no prior knowledge 
about rr at all, you can use the uniform prior which gives equal weight to all 
values. The uniform is actually the beta(1,l) prior. 
0 If you have some prior knowledge, and you can’t find a member of the conjugate 
family that matches it, you can construct a discrete prior at several values over 
the range and interpolate between them to make the prior continuous. Of 
course, you may ignore the constant needed to make this a density, since any 
constant gets cancelled out by when you divide by sprior x likelihood to 
find the exact posterior. 
0 The main thing is that your prior must have reasonable probability over all 
values that realistically are possible. If that is the case, the actual shape doesn’t 
matter very much. If there is a reasonable amount of data, different people will 
get similar posteriors, despite starting from quite different shaped priors. 
0 The posterior mean is the estimate that has the smallest posterior mean square. 
This means that, on average (with respect to posterior), it is closer to the 
parameter than any other estimate. In other words, given our prior belief 
and the observed data, the posterior mean will be, on average, closer to the 
parameter than any other estimate. It is the most widely used Bayesian estimate 
because it is optimal postdata. 
0 A (1 - a) x 100% Bayesian credible interval is an interval that has a posterior 
probability of 1 - cy of containing the parameter. 
0 The shortest (1 - a) x 100% Bayesian credible interval would have equal 
posterior density heights at the lower and upper endpoints; however, the areas 
of the two tails would not necessarily be equal. 
Equal tail area Bayesian credible intervals are often used instead, because they 
are easier to find. 
Exercises 
8.1 In order to determine how effective a magazine is at reaching its target audience, 
a market research company selects a random sample of people from the target 
audience and interviews them. Out of the 150 people in the sample, 29 had 
seen the latest issue. 

156 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
(a) What is the distribution of y, the number who have seen the latest issue? 
(b) Use a uniform prior for T ,  the proportion of the target audience that has 
seen the latest issue. What is the posterior distribution of T? 
8.2 A city is considering building a new museum. The local paper wishes to 
determine the level of support for this project, and is going to conduct a poll 
of city residents. Out of the sample of 120 people, 74 support the city building 
the museum. 
(a) What is the distribution of y, the number who support the building the 
(b) Use a uniform prior for T ,  the proportion of the target audience that 
museum? 
support the museum. What is the posterior distribution of T? 
8.3 Sophie, the editor of the student newspaper, is going to conduct a survey 
of students to determine the level of support for the current president of the 
students association. She needs to determine her prior distribution for T ,  the 
proportion of students who support the president. She decides her prior mean 
is .5, and her prior standard deviation is .15. 
(a) Determine the befu(a, b) prior that matches her prior belief. 
(b) What is the equivalent sample size of her prior? 
(c) Out of the 68 students that she polls, y = 21 support the current president. 
Determine her posterior distribution. 
8.4 You are going to take a random sample of voters in a city in order to estimate 
the proportion T who support stopping the fluoridation of the municipal water 
supply. Before you analyze the data, you need a prior distribution for T .  You 
decide that your prior mean is .4, and your prior standard deviation is .l . 
(a) Determine the betu(a, b) prior that matches your prior belief. 
(b) What is the equivalent sample size of your prior? 
(c) Out of the 100 city voters polled, y = 21 support the removal of flu- 
oridation from the municipal water supply. Determine your posterior 
distribution. 
8.5 In a research program on human health risk from recreational contact with 
water contaminated with pathogenic microbiological material, the National 
Institute of Water and Atmosphere (NIWA) instituted a study to determine the 
quality of New Zealand stream water at a variety of catchment types. This 
study is documented in McBride et al. (2002), where n = 116 one-liter 
water samples from sites identified as having a heavy environmental impact 
from birds (seagulls) and waterfowl. Out of these samples, y = 17 samples 
contained Giurdia cysts. 

EXERCISES 
157 
(a) What is the distribution of y, the number of samples containing Giardia 
(b) Let 7r be the true probability that a one-liter water sample from this type 
of site contains Giardia cysts. Use a beta(l,4) prior for 7r. Find the 
posterior distribution of 7r given y. 
(c) Summarize the posterior distribution by its first two moments. 
(d) Find the normal approximation to the posterior distribution g(rily). 
(e) Compute a 95% credible interval for ri using the normal approximation 
cysts? 
found in part (d). 
8.6 The same study found that y = 12 out of n = 145 samples identified as having 
a heavy environmental impact from dairy farms contained Giardia cysts. 
(a) What is the distribution of y, the number of samples containing Giardia 
(b) Let 7r be the true probability that a one-liter water sample from this type 
of site contains Giardia cysts. Use a beta(l,4) prior for 7r. Find the 
posterior distribution of 7r given y. 
(c) Summarize the posterior distribution by its first two moments. 
(d) Find the normal approximation to the posterior distribution g(7rIy). 
(e) Compute a 95% credible interval for 7r using the normal approximation 
cysts? 
found in part (d). 
8.7 The same study found that y = 10 out of n = 174 samples identified as having 
a heavy environmental impact from pastoral (sheep) farms contained Giardia 
cysts. 
(a) What is the distribution of y, the number of samples containing Giardia 
(b) Let 7r be the true probability that a one-liter water sample from this type 
of site contains Giardia cysts. Use a beta (1,4) prior for 7r. Find the 
posterior distribution of ri given y. 
cysts? 
(c) Summarize the posterior distribution by its first two moments. 
(d) Find the normal approximation to the posterior distribution g(7rly). 
(e) Compute a 95% credible interval for ri using the normal approximation 
found in part (d). 
8.8 The same study found that y = 6 out of n = 87 samples within municipal 
catchments contained Giardia cysts. 
(a) What is the distribution of y, the number of samples containing Giardia 
cysts? 

158 
BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
(b) Let T be the true probability that a one-liter water sample from a site 
within a municipal catchment contains Giardia cysts. Use a beta( 1,4) 
prior for T. Find the posterior distribution of 7r given y. 
(c) Summarize the posterior distribution by its first two moments. 
(d) Find the normal approximation to the posterior distribution g(T1y). 
(e) Calculate a 95% credible interval for 7r using the normal approximation 
found in part (d). 
Computer Exercises 
8.1 We will use the Minitab macro BinoBPmac or the equivalent R function to find 
the posterior distribution of the binomial probability 7r when the observation 
distribution of Y I T  is binomial (n, 
7r) and we have a beta(a, b) prior for T .  The 
beta family of priors is the conjugate family for binomial observations. That 
means that if we start with one member of the family as the prior distribution, 
we will get another member of the family as the posterior distribution. It is 
especially easy, for when we start with a beta(a, b) prior, we get a beta(a', b') 
posterior where a' = a + y and b' = b + n - y. 
Suppose we have 15 independent trials and each trial results in one of two 
possible outcomes, success or failure. The probability of success remains 
constant for each trial. In that case, Y ~ T  
is binomial (n = 15, T ) .  Suppose that 
we observed y = 6 successes. Let us start with a beta(1,l) prior. The details 
for invoking BinoBPmac and the equivalent R function are given in Appendix 
3 and Appendix 4, respectively. Store 7r, the prior g(T), the likelihood f ( y / ~ ) ,  
and the posterior g(T1y) in columns cl-c4 respectively. 
(a) What are the posterior mean and standard deviation? 
(b) Find a 95% credible interval for T .  
8.2 Repeat part (a) with a beta(2,4) prior, storing the likelihood and posterior in 
c5 and c6. 
8.3 Graph both posteriors on the same graph. What do you notice? What do you 
notice about the two posterior means and standard deviations? What do you 
notice about the two credible intervals for 7r? 
8.4 We will use the Minitab macro BinoGCPmac or the equivalent R function to 
find the posterior distribution of the binomial probability 7r when the observa- 
tion distribution of Yl7r is binomial (n, T )  and we have a general continuous 
prior for T .  Suppose the prior has the shape given by 
for 
7r 5 . 2 ,  
I o  
for 
.5 < T .  

COMPUTER EXfRClSfS 
159 
Store the values of IT and prior g(rr) in columns cl and c2, respectively. Suppose 
out of n = 20 independent trials, y = 7 successes were observed. 
(a) Use BinoGCPmac or the equivalent R function to determine the posterior 
distribution g(r1y). Details for invoking BinoGCPmac and the equivalent 
R function are in Appendix 3 and Appendix 4, respectively. 
(b) Use tintegral.mac ind the posterior mean and posterior standard deviation 
of rr. Details for invoking tintegral.mac and the equivalent R function 
are in Appendix 3 and Appendix 4, respectively. 
(c) Find a 95% credible interval for IT by using tintegral.mac or the equivalent 
R function. 
8.5 Repeat the previous question with a unvorm prior for IT. 
8.6 Graph the two posterior distributions on the same graph. What do you notice? 
What do you notice about the two posterior means and standard deviations? 
What do you notice about the two credible intervals for rr? 

This Page Intentionally Left Blank

9 
Comparing 
Bayesian and Frequentist 
Inferences for Proportion 
The posterior distribution of the parameter given the data gives the complete inference 
from the Bayesian point of view. It summarizes our belief about the parameter after we 
have analyzed the data. However, from the frequentist point of view there are several 
different types of inference that can be made about the parameter. These include point 
estimation, interval estimation, and hypothesis testing. These frequentist inferences 
about the parameter require probabilities calculated from the sampling distribution 
of the data, given the fixed but unknown parameter. These probabilities are based on 
all possible random samples that could have occurred. These probabilities are not 
conditional on the actual sample that did occur! 
In this chapter we will see how we can do these types of inferences using the 
Bayesian viewpoint. These Bayesian inferences will use probabilities calculated 
from the posterior distribution. That makes them conditional on the sample that 
actually did occur. 
9.1 FREQUENTIST INTERPRETATION OF PROBABILITY AND 
PARAMETERS 
Most statistical work is done using the frequentist paradigm. A random sample of 
observations is drawn from a distribution with an unknown parameter. The parameter 
is assumed to be a fixed but unknown constant. This doesn’t allow any probability 
distribution to be associated with it. The only probability considered is the probability 
distribution of the random sample of size n, given the parameter. This explains how the 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
161 

162 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
random sample varies over all possible random samples, given the fixed but unknown 
parameter value. The probability is interpreted as long-run relative frequency. 
Sampling Distribution of Statistic 
Let Yl , . . . , Y, be a random sample from a distribution that depends on a parameter 
0. Suppose a statistic S is calculated from the random sample. This statistic can 
be interpreted as a random variable, since the random sample can vary over all 
possible samples. Calculate the statistic for each possible random sample of size n. 
The distribution of these values is called the sampling distribution of the statistic. 
It explains how the statistic varies over all possible random samples of size n. Of 
course, the sampling distribution also depends on the unknown value of the parameter 
6. We will write this sampling distribution as 
However, we must remember that in frequentist statistics, the parameter B is a fixed but 
unknown constant, not a random variable. The sampling distribution measures how 
the statistic varies over all possible samples, given the unknown fixed parameter value. 
This distribution does not have anything to do with the actual data that occurred. It is 
the distribution of values of the statistic that could have occurred, given that specific 
parameter value. Frequentist statistics uses the sampling distribution of the statistic to 
perform inference on the parameter. From a Bayesian perspective, this is a backwards 
form of inference.' 
This contrasts with Bayesian statistics where the complete inference is the posterior 
distribution of the parameter given the actual data that occurred: 
Any subsequent Bayesian inference such as a Bayesian estimate or a Bayesian cred- 
ible interval is calculated from the posterior distribution. Thus the estimate or the 
credible interval depends on the data that actually occurred. Bayesian inference is 
straightforward.2 
'Frequentist statistics performs inferences in the parameter space, which is the unobservable dimension 
of the Bayesian universe, based on a probability distribution in the sample space, which is the observable 
dimension. 
2Bayesian statistics performs inference in the parameter space based on a probability distribution in the 
parameter space. 

POINT ESTIMATION 
163 
9.2 POINT ESTIMATION 
The first type of inference we consider is point estimation, where a single statistic is 
calculated from the sample data and used to estimate the unknown parameter. The 
statistic depends on the random sample, so it is a random variable, and its distribution 
is its sampling distribution. If its sampling distribution is centered close to the true but 
unknown parameter value 8, and the sampling distribution does not have much spread, 
the statistic could be used to estimate the parameter. We would call the statistic an 
estimator of the parameter and the value it takes for the actual sample data an estimate. 
There are several theoretical approaches for finding frequentist estimators, such as 
maximum likelihood estimation (MLE)3 and uniformly minimum variance unbiased 
estimation (UMVUE). We will not go into them here. Instead, we will use the sample 
statistic that corresponds to the population parameter we wish to estimate, such as 
the sample proportion as the frequentist estimator for the population proportion. This 
turns out to be the same estimator that would be found using either of the main 
theoretical approaches (MLE and UMVUE) for estimating the binomial parameter 
From a Bayesian perspective, point estimation means that we would use a sin- 
gle statistic to summarize the posterior distribution. The most important number 
summarizing a distribution would be its location. The posterior mean or the poste- 
rior median would be good candidates here. We will use the posterior mean as the 
Bayesian estimate because it minimizes the posterior mean squared error, as we saw 
in the previous chapter. This means it will be the optimal estimator, given our prior 
belief and this sample data (i.e., post-data). 
IT. 
Frequentist Criteria for Evaluating Estimators 
We don’t know the true value of the parameter, so we can’t judge an estimator from 
the value it gives for the random sample. Instead, we will use a criterion based on 
the sampling distribution of the estimator that is the distribution of the estimator over 
all possible random samples. We compare possible estimators by looking at how 
concentrated their sampling distributions are around the parameter value for a range 
of fixed possible values. When we use the sampling distribution, we are still thinking 
of the estimator as a random variable because we haven’t yet obtained the sample 
data and calculated the estimate. This is a pre-data analysis. 
Although this “what if the parameter has this value” type of analysis comes from 
a frequentist point of view, it can be used to evaluate Bayesian estimators as well. 
It can be done before we obtain the data, and in Bayesian statistics it is called a 
pre-posterior analysis. The procedure is used to evaluate how the estimator performs 
over all possible random samples, given that parameter value. We often find that 
Bayesian estimators perform very well when evaluated this way, sometimes even 
better than frequentist estimators. 
3Maximum likelihood estimation was pioneered by R. A. Fisher. 

164 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
Unbiased Estimators 
The expected value of an estimator is a measure of the center of its distribution. This 
is the average value that the estimator would have averaged over all possible samples. 
An estimator is said to be unbiased if the mean of its sampling distribution is the true 
parameter value. That is, an estimator 6 is unbiased if and only if 
E(6) = 
ef(616)de = 6 ,  
J 
where f(618) is the sampling distribution of the estimator 6 given the parameter 
8. Frequentist statistics emphasizes unbiased estimators because averaged over all 
possible random samples, an unbiased estimator gives the true value. The bias of an 
estimator 8 is the difference between its expected value and the true parameter value. 
(9.1) 
bias(@ = ~ ( 6 )  
- 8 .  
Unbiased estimators have bias equal to zero. 
fact, Bayesian estimators are usually biased. 
In contrast, Bayesian statistics does not place any emphasis on being unbiased. In 
Minimum Variance Unbiased Estimator 
An estimator is said to be a minimum variance unbiased estimator if no other unbiased 
estimator has a smaller variance. Minimum variance unbiased estimators are often 
considered the best estimators in frequentist statistics. The sampling distribution of 
a minimum variance unbiased estimator has the smallest spread (as measured by the 
variance) of all sampling distributions that have mean equal to the parameter value. 
However, it is possible that there may be biased estimators that, on average, are 
closer to the true value than the best unbiased estimator. We need to look at a possible 
trade-off between bias and variance. Figure 9.1 shows the sampling distributions of 
three possible estimators of 8. Estimator 1 and estimator 2 are seen to be unbiased 
estimators. Estimator 1 is the best unbiased estimator, since it has the smallest 
variance among the unbiased estimators. Estimator 3 is seen to be a biased estimator, 
but it has a smaller variance than estimator 1. We need some way of comparison 
that includes biased estimators, to find which one will be closest, on average, to the 
parameter value. 
Mean Squared Error of an Estimator 
The (frequentist) mean squared error of an estimator 6 is the average squared distance 
the estimator is away from the true value: 

COMPARING ESTIMATORS FOR PROPORTION 
765 
e 
Figure 9.7 Sampling distributions of three estimators. 
The frequentist mean squared error is calculated from the sampling distribution of 
the estimator, which means the averaging is over all possible samples given that fixed 
parameter value. It is not the posterior mean square calculated from the posterior 
distribution that we introduced in the previous chapter. It turns out that the mean 
squared error of an estimator is the square of the bias plus the variance of the estimator: 
(9.3) 
MS(8) = bius(8)2 + Vur(8) 
* 
Thus it gives a better frequentist criterion for judging estimators than the bias or the 
variance alone. An estimator that has a smaller mean squared error is closer to the 
true value averaged over all possible samples. 
9.3 COMPARING ESTIMATORS FOR PROPORTION 
Bayesian estimators often have smaller mean squared errors than frequentist estima- 
tors. In other words, on average, they are closer to the true value. Thus Bayesian 
estimators can be better than frequentist estimators, even when judged by the fre- 
quentist criterion of mean squared error. The frequentist estimator for 7r is 
where y, the number of successes in the n trials, has the binomial (n, 7r) distribution. 
.irf is unbiased, and Var(.irf) = w. 
Hence the mean squared error of .irf 
equals 
MS(.irf) = o2 + V U T ( . i r f )  
77 x (1 - 7 r )  
- 
- 
n 

166 
COMPARING BAYESIAN AND FREOUENTIST INFERENCES FOR PROPORTION 
Suppose we use the posterior mean as the Bayesian estimate for n, where we use 
the Beta( 1,l) prior (uniform prior). The estimator is the posterior mean, so 
where a’ = 1 + y and b’ = 1 + n - y. We can rewrite this as a linear function of y, 
the number of successes in the n trials: 
Thus, the mean of its sampling distribution is 
nT 
1 
-+- 
n + 2  
n + 2 ’  
and the variance of its sampling distribution is 
Hence from Equation 9.3, the mean squared error is 
For example, suppose n = .4 and the sample size is n = 10. Then 
.4 x .6 
10 
MS(??f) = - 
= .024 
and 
2 
1 - 2 x . 4  
MS(‘?B) = [ 
12 
12+[&] 
x l O x . 4 x . 6  
= .0169. 
Next, suppose x = .5 and n = 10. Then 
.5 x .5 
10 
= .025 
M S ( ? f )  = 
~ 
and 
= .01736 

INTERVAL ESTIMATION 
167 
u.usu 
0.025 
0.020 
0.01 5 
0.01 0 
0.005 
0.000 
Bayes 
0 \
frequentist 
0 , 
. 
/ 
\ 
/ 
\ 
/ 
\ 
I 
/ 
----- 
/-\ 
I 
\ 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure 9.2 Mean squared error for the two estimators. 
We see that, on average (for these two values of T), the Bayesian posterior estimator 
is closer to the true value than the frequentist estimator. Figure 9.2 shows the mean 
squared error for the Bayesian estimator and the frequentist estimator as a function 
of T .  We see that over most (but not all) of the range, the Bayesian estimator (using 
uniform prior) is better than the frequentist e~timator.~ 
9.4 INTERVAL ESTIMATION 
The second type of inference we consider is interval estimation. We wish to find 
an interval ( 1 ,  u) that has a predetermined probability of containing the parameter. 
In the frequentist interpretation, the parameter is fixed but unknown, and before the 
sample is taken, the interval endpoints are random because they depend on the data. 
After the sample is taken and the endpoints are calculated, there is nothing random, 
so the interval is said to be a confidence interval for the parameter. We know that 
a predetermined proportion of intervals calculated for random samples using this 
method will contain the true parameter. But it doesn't say anything at all about the 
specific interval we calculate from our data. 
4The frequentist estimator, Ff = $, would be Bayesian posterior mean if we used the prior g(r) 0: 
7r-'(l - ..)-I. 
This prior is improper since it does not integrate to 1. An estimator is said to be 
admissible if no other estimator has smaller mean squared error over the whole range of possible values. 
Wald (1950) showed that Bayesian posterior mean estimators that arose from proper priors are always 
admissible. Bayesian posterior mean estimators from improper priors sometimes are admissible, as in this 
case. 

168 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
In Chapter 8, we found a Bayesian credible interval for the parameter n- that has 
the probability that we want. Because it is found from the posterior distribution, it 
has the coverage probability we want for this specific data. 
Confidence Intervals 
Confidence intervals are how frequentist statistics tries to find an interval has a high 
probability of containing the true value of the parameter 8. A (1 - a) x 100% 
confidence interval for a parameter 8 is an interval ( I ,  u) 
such that 
P(1 5 0 5 u) = 1 - a 
This probability is found using the sampling distribution of an estimator for the 
parameter. There are many possible values of 1 and u that satisfy this. The most 
commonly used criteria for choosing them are (1) equal ordinates (heights) on the 
sampling distribution and (2) equal tail area on the sampling distribution. Equal 
ordinates will find the shortest confidence interval. However, the equal tail area 
intervals are often used because they are easier to find. When the sampling distribution 
of the estimator is symmetric, the two criteria will coincide. 
The parameter 8 is regarded as a fixed but unknown constant. The endpoints 1 
and u are random variables since they depend on the random sample. When we 
plug in the actual sample data that occurred for our random sample and calculate the 
values for 1 and u, 
there is nothing left that is random. The interval either contains 
the unknown fixed parameter or it doesn’t, and we don’t know which is true. The 
interval can no longer be regarded as a probability interval. 
Under the frequentist paradigm, the correct interpretation is that (1 - a) x 100% 
of the random intervals calculated this way will contain the true value. Therefore we 
have (1 - a) x 100% conjidence that our interval does. It is a misinterpretation to 
make a probability statement about the parameter 8 from the calculated confidence 
interval. 
Often, the sampling distribution of the estimator used is approximately normal, 
with mean equal to the true value. In this case, the confidence interval has the form 
estimator i 
critical value x standard deviation of the estimator; 
where the critical value comes from the standard normal table. For example, if n is 
large, then the sample proportion 
A
Y
 
7rf = - 
n 
is approximately normal with mean 7r and standard deviation Jv. 
This gives 
an approximate (1 - a) x 100% equal tail area confidence interval for n-: 

HYPOTHESIS TESTING 
169 
Comparing Confidence and Credible Intervals for 7r 
The probability calculations for the confidence interval are based on the sampling 
distribution of the statistic. In other words, how it varies over all possible samples. 
Hence the probabilities are pre-data. They do not depend on the particular sample 
that occurred. This is in contrast to the Bayesian credible interval calculated from 
the posterior distribution that has a direct (degree of belief) probability interpretation 
conditional on the observed sample data. The Bayesian credible interval is more 
useful to the scientist whose data we are analyzing. It summarizes our beliefs about 
the parameter values that could credibly be believed given the observed data that 
occurred. In other words, it is post-data. He/she is not concerned about data that 
could have occurred but did not. 
Example 14 (continued from Chapter 8) Out of a random sample of n = 100 
Hamilton residents, y = 26 said they support building a casino in Hamilton. A 
frequentist 95% conjdence interval for T is 
= (.174, .346). 
Compare this with the 95% credible intervals for T calculated by the three students 
in Chapter 8 and shown in Table 8.3. 
9.5 HYPOTHESIS TESTING 
The third type of inference we consider is hypothesis testing. Scientists do not like 
to claim the existence of an effect where the discrepancy in the data could be due to 
chance alone. If they make their claims too quickly, later studies would show their 
claim was wrong, and their scientific reputation would suffer. 
Hypothesis testing, sometimes called significance testing5, is the frequentist sta- 
tistical method widely used by scientists to guard against making claims unjustified 
by the data. The nonexistence of the treatment effect is set up as the null hypothesis 
that "the shift in the parameter value caused by the treatment is zero." The competing 
hypothesis that there is a nonzero shift in the parameter value caused by the treatment 
is called the alternative hypothesis. Two possible explanations for the discrepancy 
between the observed data and what would be expected under the null hypothesis are 
proposed. 
1. The null hypothesis is true, and the discrepancy is due to random chance alone. 
2. The null hypothesis is false. This causes at least part of the discrepancy. 
'Significance testing was developed by R. A. Fisher as an inferential tool to weigh the evidence against a 
particular hypothesis. Hypothesis testing was developed by Neyman and Pearson as a method to control 
the error rate in deciding between two competing hypotheses. These days, the two terms are used almost 
interchangeably, despite their differing goals and interpretations. This continues to cause confusion. 

170 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
To be consistent with Ockham's razor, we will stick with explanation (l), which 
has the null hypothesis being true and the discrepancy being due to chance alone, 
unless the discrepancy is so large that it is very unlikely to be due to chance alone. 
This means that when we accept the null hypothesis as true, it doesn't mean that we 
believe it is literally true. Rather, it means that chance alone remains a reasonable 
explanation for the observed discrepancy, so we can't discard chance as the sole 
explanation. 
When the discrepancy is too large, we are forced to discard explanation (1) leaving 
us with explanation (21, that the null hypothesis is false. This gives us a backward 
way to establish the existence of an effect. We conclude the effect exists (the 
null hypothesis is false) whenever the probability of the discrepancy between what 
occurred and what would be expected under the null hypothesis is too small to be 
attributed to chance alone. 
Because hypothesis testing is very well established in science, we will show how it 
can be done in a Bayesian manner. There are two situations we will look at. The first 
is testing a one-sided hypothesis where we are only interested in detecting the effect 
in one direction. We will see that in this case, Bayesian hypothesis testing works 
extremely well, without the contradictions required in frequentist tests. The Bayesian 
test of a one-sided null hypothesis is evaluated from the posterior probability of the 
null hypothesis. 
The second situation is where we want to detect a shift in either direction. This is 
a two-sided hypothesis test, where we test a point hypothesis (that the effect is zero) 
against a two-sided alternative. The prior density of a continuous parameter measures 
probability density, not probability. The prior probability of the null hypothesis (shift 
equal to zero) must be equal to 0. So its posterior probability must also be zero,6 
and we cannot test a two-sided hypothesis using the posterior probability of the null 
hypothesis. Rather, we will test the credibility of the null hypothesis by seeing 
if the null value lies in the credible interval. If the null value does lie within the 
credible interval, we cannot reject the null hypothesis, because the null value remains 
a credible value. 
9.6 TESTING A ONE-SIDED HYPOTHESIS 
The effect of the treatment is included as a parameter in the model. The hypothesis 
that the treatment has no effect becomes the null hypothesis the parameter representing 
the treatment effect has the null value that corresponds to no effect of the treatment. 
6We are also wamed that frequentist hypothesis tests of a point null hypothesis never "accept" the null 
hypothesis; rather, they "can't reject the null hypothesis." 

TESTING A ONE-SIDED HYPOTHESIS 
171 
Table 9.1 
Null distribution of Y with a rejection region for a one-sided hypothesis test 
Value 
f (Yl. = .6) 
Region 
0 
.mo1 
accept 
1 
.0016 
accept 
.0106 
.0425 
,1115 
.2007 
.2508 
.2 150 
.1209 
accept 
accept 
accept 
accept 
accept 
accept 
acceDt 
9 
10 
,0403 
.0060 
reject 
reject 
Frequentist Test of One-sided Hypothesis 
The probability of the data (or results even more extreme) given that the null hypothe- 
sis is true is calculated. If this is below a threshold called the level of significance, the 
results are deemed to be incompatible with the null hypothesis, and the null hypothesis 
is rejected at that level of significance. This establishes the existence of the treatment 
effect. This is similar to a "proof by contradiction." However, because of sampling 
variation, complete contradiction is impossible. Even very unlikely data are possible 
when there is no treatment effect. So hypothesis tests are actually more like "proof by 
low probability." The probability is calculated from the sampling distribution, given 
that the null hypothesis is true. This makes it apre-data probability. 
Example 15 Suppose we wish to determine if a new treatment is better than the 
standard treatment. If so, T ,  the proportion of patients who benefit from the new 
treatment, should be better than TO, the proportion who benefitfrom the standard 
treatment. It is known from historical records that rro = .6. A random group of 10 
patients are given the new treatment. Y ,  the number who benefit from the treatment 
will be binomial(n, T ) .  We observe y = 8 patients benefit. This is better than we 
would expect if. 
= .6. But, is it enough better for us to conclude that T > .6 at the 
10% level of significance? 
The steps are: 
1. Set up a null hypothesis about the ($xed but unkn0wn)parametel: For example, 
HO : rr 5 .6. (The proportion who would benefit from the new treatment is 
less than or equal to the proportion who benefitfrom the standard treatment.) 
We include all T values less than the null value .6 in with the null hypothesis 
because we are trying to determine ifthe new treatment is bettel: We have no 

172 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
interest in determining ifthe new treatment is worse. We won’t recommend it 
unless it is demonstrably better than the standard treatment. 
2. The alternative hypothesis is H I  : rr > .6. (The proportion who would benefit 
from the new treatment is greater than the proportion who benefit from the 
standard treatment.) 
3. The null distribution of the test statistic is the sampling distribution of the 
test statistic, given that the null hypothesis is true. In this case, it will be 
binomial(n, .6) where n = 10 is the number ofpatients given the new treatment. 
4. We choose level of SigniJicance for the test to be as close as possible to a = 5%. 
Since y has a discrete distribution, only some values of Q are possible, so we 
will have to choose a value either just above or just below 5%. 
5. The rejection region is chosen so that it has a probability of Q under the null 
distribution.’ I f  we choose the rejection region y 2 9, then a = .0463. The 
null distribution with the rejection region for the one-sided hypothesis test is 
shown in Table 9.1. 
6. If the value of the test statistic for the given sample lies in the rejection region, 
then reject the null hypothesis HO at level a. Otherwise, we can’t reject Ho. 
In this case, y = 8 was observed. This lies in the acceptance region. 
7. The p-value is the probability of getting what we observed, or something even 
more unlikely, given the null hypothesis is true. The p-value is put forward as 
measuring the strength of evidence against the null hypothesis.8 In this case, 
thep-value = ,1672. 
8. I f  the p-value < a, the test statistic lies in the rejection region, and vice versa. 
So an equivalent way of testing the hypothesis is to reject if p-value < a.9 
Looking at it either way, we cannot reject the null hypothesis HO : T 5 .6. 
y = .8 lies in the acceptance region, and the p-value > .05. The evidence is 
not strong enough to conclude that T > .6. 
There is much confusion about the p-value of a test. It is not the posterior 
probability of the null hypothesis being true given the data. Instead, it is the tail 
probability calculated using the null distribution. In the binomial case 
p-value = 
Y o b r  
’This approach is from N e y m  and Pearson. 
*This approach is from R. A. Fisher. 
’Both a andp-value are tail areas calculated from the null distribution. However, a represents the long-run 
rate of rejecting a true null hypothesis, and p-value is looked at as the evidence against this particular null 
hypothesis by this particular data set. Using tail areas as simultaneously representing both the long-run 
and a particular result is inherently contradictory. 

TESTING A TWO-SIDED HYPOTHESIS 
173 
where Yobs is the observed value of y. Frequentist hypothesis tests use a probability 
calculated on all possible data sets that could have occurred (for the fixed parameter 
value), but the hypothesis is about the parameter value being in some range of values. 
Bayesian Tests of a One-sided Hypothesis 
We wish to test 
HO : T 5 TO 
versus HI : T > TO 
at the level of significance (Y using Bayesian methods. We can calculate the posterior 
probability of the null hypothesis being true by integrating the posterior density over 
the correct region: 
We reject the null hypothesis if that posterior probability is less than the level of 
significance (I. Thus a Bayesian one-sided hypothesis test is a "test by low probabil- 
ity" using the probability calculated directly from the posterior distribution of 7r. We 
are testing a hypothesis about the parameter using the posterior distribution of the 
parameter. Bayesian one-sided tests use post-data probability. 
Example 15 (continued) Suppose we use a beta (1,l) prior for T. Then given 
y = 8, the posterior density is beta (9,3). The posterior probability of the null 
hypothesis is 
.6 
P(T 5 .6/y = 8 )  = 1 &7r2(1 
- 7r)*d7r 
= .1189 
when we evaluate it numerically. This is not less than .05, so we cannot reject the 
null hypothesis at the 5% level of sign$cance 5%. Figure 9.3 shows the posterior 
density. The probability of the null hypothesis is the area under the curve to the left 
of T = .6. 
9.7 TESTING A TWO-SIDED HYPOTHESIS 
Sometimes we might want to detect a change in the parameter value in either direction. 
This is known as a two-sided test since we are wanting to detect any changes from 
the value TO. We set this up as testing the point null hypothesis HO : T = TO against 
the alternative hypothesis H I  : 7r # TO. 
Frequentist Test of a Two-sided Hypothesis 
The null distribution is evaluated at TO, and the rejection region is two-sided, as are 
p-values calculated for this test. 

174 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
Figure 9.3 Posterior probability of the null hypothesis, Ho : T 5 .6 is the shaded area. 
Example 16 A coin is tossed 15 times, and we observe 10 heads. Are 10 heads out 
of 15 tosses enough to determine that the coin is not fair? In other words, is T the 
probability of getting a head different than f ? 
The steps are: 
1. Set up the null hypothesis about the $xed but unknown parameter T .  It is 
Ho : T = .5. 
2. The alternative hypothesis is HI : 7r # .5. We are interested in determining a 
difference in either direction, so we will have a two-sided rejection region. 
3. The null distribution is the sampling distribution of Y when the null hypothesis 
is true. It is binomial(n = 15, T = .5). 
4. Since Y has a discrete distribution, we choose the level of significance for the 
test to be as close to 5% as possible. 
5. The rejection region is chosen so that it has a probability of cy under the 
null distribution. If we choose rejection region {Y 5 3) U {Y 2 12}, then 
CY = .0352. The null distribution and rejection region for the two-sided 
hypothesis are shown in Table 9.2. 
6. Ifthe value of the test statistic lies in the rejection region, then we reject the 
null hypothesis HO at level a. Otherwise, we can’t reject Ho. In this case, 
y = 10 was observed. This lies in the region where we can’t reject the null 
hypothesis. We must conclude that chance alone is su.cient to explain the 
discrepancy, so T = .5 remains a reasonable possibility. 

TEST/NG A TWO-SIDED HYPOTHESIS 
775 
Table 9.2 
Null distribution of Y with the rejection region for two-sided hypothesis test 
Value 
f(Yl7r = .5) 
Region 
0 
.oooo 
reject 
1 
.0005 
reject 
2 
.0032 
reject 
3 
.0139 
reject 
4 
5 
6 
7 
8 
9 
10 
11 
.0417 
.0916 
,1527 
.1964 
.1964 
,1527 
.0916 
,0417 
accept 
accept 
accept 
accept 
accept 
accept 
accept 
accept 
12 
13 
14 
15 
.0139 
.0032 
BOO5 
.0000 
reject 
reject 
reject 
reject 
7. The p-value is the probability of getting what we got (10) or something more 
unlikely, given the null hypothesis HO is true. In this case we have a two-sided 
alternative, so the p-value is the P(Y 2 10) + P(Y 5 5 )  = .302. This is 
larger than a, so we can 't reject the null hypothesis. 
Relationship between two-sided hypothesis tests and confidence inter- 
vals. While the null value of the parameter usually comes from the idea of no 
treatment effect, it is possible to test other parameter values. There is a close re- 
lationship between two-sided hypothesis tests and confidence intervals. If you are 
testing a two-sided hypothesis at level a, there is a corresponding (1 - a) x 100% 
confidence interval for the parameter. If the null hypothesis 
is rejected, then the value 7r0 lies outside the confidence interval, and vice versa. If 
the null hypothesis is accepted (can't be rejected), then TO lies inside the confidence 
interval, and vice versa. The confidence interval "summarizes" all possible null 
hypotheses that would be accepted if they were tested. 

176 
COMPARING BAYESlAN AND FREQUENTlST lNFERENCES FOR PROPORTION 
Bayesian Test of a Two-sided Hypothesis 
From the Bayesian perspective, the posterior distribution of the parameter given the 
data sums up our entire belief after the data. However, the idea of hypothesis testing 
as a protector of scientific credibility is well established in science. So we look 
at using the posterior distribution to test a point null hypothesis versus a two-sided 
alternative in a Bayesian way. 
If we use a continuous prior, we will get a continuous posterior. The probability 
of the exact value represented by the point null hypothesis will be zero. We can’t 
use posterior probability to test the hypothesis. Instead, we use a correspondence 
similar to the one between confidence intervals and hypothesis tests, but with credible 
interval instead. 
Compute a (1 - a) x 100% credible interval for 7r. If 7r0 lies inside the credible 
interval, accept (do not reject) the null hypothesis HO : 7r = TO, and if TO lies outside 
the credible interval, then reject the null hypothesis. 
Example 16 (continued) Ifwe use a uniform prior distribution, the posterior is the 
beta(l0 + 1,5 + 1) distribution. A 95% Bayesian credible interval for 7r foundusing 
the normal approximation is 
11 x 6 
((11 + 6)2 x (11 + 6 + 1)) 
11 
- + 1.96 x 
17 
= .647 i 
.221 = (.426, .868). 
The null value 7r = .5 lies within the credible interval, so we cannot reject the null 
hypothesis. It remains a credible value. 
Main Points 
The posterior distribution of the parameter given the data is the entire infer- 
ence from a Bayesian perspective. Probabilities calculated from the posterior 
distribution are post-data because the posterior distribution is found after the 
observed data has been taken into the analysis. 
Under the frequentist perspective there are specific inferences about the pa- 
Frequentist statistics considers the parameter a fixed but unknown constant. 
rameter: point estimation, confidence intervals, and hypothesis tests. 
The only kind of probability allowed is long-run relative frequency. 
0 The sampling distribution of a statistic is its distribution over all possible 
random samples given the fixed parameter value. Frequentist statistics is based 
on the sampling distribution. 
0 Probabilities calculated using the sampling distribution are pre-data because 
they are based on all possible random samples, not the specific random sample 
we obtained. 

MAIN POlNTS 
177 
0 An estimator of a parameter is unbiased if its expected value calculated from 
the sampling distribution is the true value of the parameter. 
0 Frequentist statistics often call the minimum variance unbiased estimator the 
best estimator. 
0 The mean squared error of an estimator measures its average squared distance 
from the true parameter value. It is the square of the bias plus the variance. 
0 Bayesian estimators are often better than frequentist estimators even when 
judged by the frequentist criteria such as mean squared error. 
0 Seeing how a Bayesian estimator performs using frequentist criteria for a range 
of possible parameter values is called a pre-posterior analysis, because it can 
be done before we obtain the data. 
0 A (1 - a )  x 100% confidence interval for a parameter 8 is an interval (1, u) 
such that 
~
(
i
 
5 8 5 u) = 1 - a ,  
where the probability is found using the sampling distribution of an estimator 
for 8. The correct interpretation is that (1 - a) x 100% of the random intervals 
calculated this way do contain the true value. When the actual data are put 
in and the endpoints calculated, there is nothing left to be random. The 
endpoints are numbers; the parameter is fixed but unknown. We say that 
we are (1 - a) x 100% conjdent that the calculated interval covers the true 
parameter. The confidence comes from our belief in the method used to 
calculate the interval. It does not say anything about the actual interval we got 
for that particular data set. 
0 A (1 - a) x 100% Bayesian credible interval for 8 is a range of parameter 
values that has posterior probability (1 - a). 
0 Frequentist hypothesis testing is used to determine whether the actual parameter 
could be a specific value. The sample space is divided into a rejection region 
and an acceptance region such that the probability the test statistic lies in the 
rejection region if the null hypothesis is true is less than the level of significance 
a. If the test statistic falls into the rejection region, we reject the null hypothesis 
at level of significance a. 
0 Or we could calculate the p-value. If the p-value< a, we reject the null 
hypothesis at level a. 
0 The p-value is not the probability the null hypothesis is true. Rather, it is the 
probability of observing what we observed, or even something more extreme, 
given that the null hypothesis is true. 
We can test a one-sided hypothesis in a Bayesian manner by computing the 
posterior probability of the null hypothesis. This probability is found by 

178 
COMPARlNG BAYESlAN AND FREQUENTlST lNfERENCES FOR PROPORTlON 
integrating the posterior density over the null region. If this probability is less 
than the level of significance a, then we reject the null hypothesis. 
We cannot test a two-sided hypothesis by integrating the posterior probability 
over the null region because, with a continuous prior, the prior probability of 
a point null hypothesis is zero, so the posterior probability will also be zero. 
Instead, we test the credibility of the null value by observing whether or not 
it lies within the Bayesian credible interval. If it does, the null value remains 
credible and we can’t reject it. 
Exercises 
9.1 Let x be the proportion of students at a university who approve the governments 
policy on students allowances. The students newspaper is going to take a 
random sample of n = 30 students at a university and ask if they approve of 
the governments policy on student allowances. 
(a) What is the distribution of y, the number who answer “yes“? 
(b) Suppose out of the 30 students, 8 answered yes. What is the frequentist 
(c) Find the posterior distribution g(rly) if we use a uniform prior. 
(d) What would be the Bayesian estimate of x? 
estimate of T .  
9.2 The standard method of screening for a disease fails to detect the presence 
of the disease in 15% of the patients who actually do have the disease. A 
new method of screening for the presence of the disease has been developed. 
A random sample of n = 75 patients who are known to have the disease is 
screened using the new method. Let T be the probability the new screening 
method fails to detect the disease. 
(a) What is the distribution of v, the number of times the new screening 
(b) Of these n = 75 patients, the new method failed to detect the disease in 
(c) Use a beta (1,6) prior for x. Find g(.rrly), the posterior distribution of T .  
(d) Find the posterior mean and variance. 
(e) If x 2 .15, then the new screening method is no better than the standard 
method fails to detect the disease? 
y = 6 cases. What is the frequentist estimator of T? 
method. Test 
Ho : x 2 .15 versus HI : x < .15 
at the 5% level of significance in a Bayesian manner. 

EXERCISES 
179 
9.3 In the study of water quality in New Zealand streams documented in McBride 
et al. (2002) a high level of Campylobacter was defined as a level greater than 
100 per 100 ml of stream water. n = 116 samples were taken from streams 
having a high environmental impact from birds. Out of these, y = 11 had a 
high Campylobacter level. Let T be the true probability that a sample of water 
from this type of stream has a high Campylobacter level. 
(a) Find the frequentist estimator for T .  
(b) Use a beta (1,lO) prior forr. Calculate the posterior distribution g(7riy). 
(c) Find the posterior mean and variance. What is the Bayesian estimator for 
(d) Find a 95% credible interval for T .  
(e) Test the hypothesis 
7r? 
Ho : T = .lo versus H I  : T # .1O 
at the 5% level of significance. 
9.4 In the same study of water quality, n = 145 samples were taken from streams 
having a high environmental impact from dairying. Out of these y = 9 had a 
high Campylobacter level. Let T be the true probability that a sample of water 
from this type of stream has a high Campylobacter level. 
(a) Find the frequentist estimator for T .  
(b) Use a beta (1,lO) prior for 7r. Calculate the posterior distribution g(7rly). 
(c) Find the posterior mean and variance. What is the Bayesian estimator for 
(d) Find a 95% credible interval for 7r. 
(e) Test the hypothesis 
T? 
Ho : T = .10 
versus 
Hi : T # .10 
at the 5% level of significance. 
9.5 In the same study of water quality, n = 176 samples were taken from streams 
having a high environmental impact from sheep farming. Out of these y = 24 
had a high Campylobacter level. Let T be the true probability that a sample of 
water from this type of stream has a high Campylobacter level. 
(a) Find the frequentist estimator for 7r. 
(b) Use a beta (1,lO) prior for T .  Calculate the posterior distribution g ( ~ l y ) .  
(c) Find the posterior mean and variance. What is the Bayesian estimator for 
T? 

180 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR PROPORTION 
(d) Test the hypothesis 
HO : 7r 2 .15 versus H I  : T < .15 
at the 5% level of significance. 
9.6 In the same study of water quality, n = 87 samples were taken from streams 
in municipal catchments. Out of these y = 8 had a high Carnpylobacter level. 
Let 7r be the true probability that a sample of water from this type of stream 
has a high Campylobacter level. 
(a) Find the frequentist estimator for 7r. 
(b) Use a beta ( 1 , l O )  prior for 7r. Calculate the posterior distribution g(T1y). 
(c) Find the posterior mean and variance. What is the Bayesian estimator for 
(d) Test the hypothesis 
7r? 
HO : 7r 2 .10 versus HI : 7r < .10 
at the 5% level of significance. 
Monte Carlo Exercises 
9.1 Comparing Bayesian and frequentist estimators for T .  In Chapter 1 we 
learned that the frequentist procedure for evaluating a statistical procedure, 
namely looking at how it performs in the long-run, for a (range of) fixed but 
unknown parameter values can also be used to evaluate a Bayesian statistical 
procedure. This "what if the parameter has this value " type of analysis would 
be done before we obtained the data and is called a pre-posterior analysis. It 
evaluates the procedure by seeing how it performs over all possible random 
samples, given that parameter value. In Chapter 8 we found that the posterior 
mean used as a Bayesian estimator minimizes the posterior mean squared error. 
Thus it has optimal post-data properties, in other words after making use of 
the actual data. We will see that Bayesian estimators have excellent pre-data 
(frequentist) properties as well, often better than the corresponding frequentist 
estimators. 
We will perfom a Monte Carlo study approximating the sampling distribution 
of two estimators of 7r. The frequentist estimator we will use is +irf = $, the 
sample proportion. The Bayesian estimator we will use is +~TB = s, 
which 
equals the posterior mean when we used a uniform prior for 7r. We will compare 
the sampling distributions (in terms of bias, variance, and mean squared error) 
of the two estimators over a range of 7r values from 0 to 1 .  However, unlike the 
exact analysis we did in Section 9.3, here we will do a Monte Carlo study. For 
each of the parameter values, we will approximate the sampling distribution 

MONTE CARL0 EXERCISES 
181 
of the estimator by an empirical distribution based on 5000 samples drawn 
when that is the parameter value. The true characteristics of the sampling 
distribution (mean, variance, mean squared error) are approximated by the 
sample equivalent from the empirical distribution. You can use either Minitab 
or R for your analysis. 
(a) For T = .1, .2,. . . , .9 
i. Draw 5000 random samples from binomial (n = 10, T ) .  
ii. Calculate the frequentist estimator i?f 
= $ for each of the 5000 
samples. 
iii. Calculate the Bayesian estimator f ? ~  
= 
for each of the 5000 
samples. 
iv. Calculate the means of these estimators over the 5000 samples, and 
subtract T to give the biases of the two estimators. Note that this is 
a function of T .  
v. Calculate the variances of these estimators over the 5000 samples. 
Note that this is also a function of T .  
vi. Calculate the mean squared error of these estimators over the 5000 
samples. The first way is 
MS(f?) = (bius(f?))2 + Vur(;r). 
The second way is to take the sample mean of the squared distance 
the estimator is away from the true value over all 5000 samples. Do 
it both ways, and see that they give the same result. 
(b) Plot the biases of the two estimators versus 7r at those values and connect 
the adjacent points. (Put both estimators on the same graph.) 
i. Does the frequentist estimator appear to be unbiased over the range 
ii. Does the Bayesian estimator appear to be unbiased over the range of 
(c) Plot the mean squared errors of the two estimators versus T over the range 
of T values, connecting adjacent points. (Put both estimators on the same 
of T values? 
the T values? 
graph.) 
i. Does your graph resemble Figure 9.2? 
ii. Over what range of T values does the Bayesian estimator have smaller 
mean squared error than that of the frequentist estimator? 

This Page Intentionally Left Blank

10 
Bayesian Inference for 
Poisson 
The Poisson distribution is used to count the number of occurrences of rare events 
which are occurring randomly through time (or space) at a constant rate. The events 
must occur one at a time. The Poisson distribution could be used to model the number 
of accidents on a highway over a month. However, it could not be used to model the 
number of fatalities occurring on the highway, since some accidents have multiple 
fatalities. 
Bayes' Theorem for Poisson Parameter with a Continuous Prior 
We have a random sample yll . . . yn from a Poisson(p) distribution. The propor- 
tional form of Bayes' theorem is given by posterior c( prior x likelihood 
g(PIYl1.. . >  Yn) c( dPL) x f(Y1, '. ' 1 Y n b )  
* 
The parameter p can have any positive value, so we should use a continuous prior 
defined on all positive values. The proportional form of Bayes' theorem gives the 
shape of the posterior. We need to find the scale factor to make it a density. The 
actual posterior is given by 
(10.1) 
This equation holds for any continuous prior g(p). However, the integration would 
have to be done numerically except for the few special cases which we will investigate. 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
183 

184 
BAYESlAN lNFERENCf FOR POISSON 
Likelihood of Poisson parameter. The likelihood of a single draw from a 
Poisson(p) distribution is given by 
for y = 0,1, . . . and p > 0. The part that determines the shape of the likelihood is 
When y1, . . . , yn is a random sample from a Poisson(p) distribution, the likelihood 
of the random sample is the product of the original likelihoods. This simplifies to 
~ ( Y I , .  
. . >  
YnIp) = 
~ ( Y ~ I P )  
x p , C y v n p ,  
n 
2-1 
We recognize this as the likelihood where C yz. is a single draw from a Poisson(np) 
distribution. It has the shape of a gummu(r', w') density where r' = C yz + 1 and 
u' = n 
10.1 SOME PRIOR DISTRIBUTIONS FOR POISSON 
In order to use Bayes' theorem, we will need the prior distribution of the Poisson 
parameter p. In this section we will look at several possible prior distributions of p 
for which we can work out the posterior density without having to do the numerical 
integration. 
Positive uniform prior density. Suppose we have no idea what the value of p 
is prior to looking at the data. In that case, we would consider that we should give 
all positive values of p equal weight. So we let the positive uniform prior density be 
g(p) = 1 for 
p >  0 
Clearly this prior density is improper since its integral over all possible values is 
infinite. Nevertheless, the posterior will be proper in this case' and we can use 
it for making inference about p. The posterior will be proportional to prior times 
likelihood, so in this case the proportional posterior will be 
g(pIYli...rYn) c( g(pu) f ( ~ ~ ? . . . t ~ n l ~ )  
The posterior is the same shape as the likelihood function so we know that it is a 
gumrnu(r', w') density where r' = ,C y + 1 and w' = n. Clearly the posterior is 
proper despite starting from an improper prior. 
'There are cases where an improper prior will result in an improper posterior, so no inference is possible. 

SOME PRIOR DISTRIBUTIONS FOR POISSON 
185 
Jeffreys prior for Poisson. The parameter indexes all possible observation 
distributions. Any one to one continuous function of the parameter would give an 
equally valid index.2 Jeffreys’ method gives us priors which are objective in the 
sense that they are invariant under any continuous transformation of the parameter. 
The Jeffreys’ prior for the Poisson is 
This also will be an improper prior, since its integral over the whole range of possible 
values is infinite. However, it is not non-informative since it gives more weight to 
small values. The proportional posterior will be the prior times likelihood. Using the 
Jeffreys’ prior the proportional posterior will be 
dPIYl,. . ‘ ,Yn) x d P )  f ( Y l 1 . .  . l  YnlP) 
which we recognize as the shape of a gamma(r‘, v’) density where T’ = 
v’ = n. Again, we have a proper posterior despite starting with an improper prior. 
Conjugate family for Poisson observations is the gamma fami/F The 
conjugate prior for the observations from the Poisson distribution with parameter ( p )  
will have the same form as the likelihood. Hence it has shape given by 
y + $ and 
The distribution having this shape is known as the garnma(r, u )  distribution and has 
density given by 
vrPr- le-up 
r(r) 
d P L ; T , V )  = 
where T - 1 = 1 and v = k and & is the scale factor needed to make this a density. 
When we have a single Poisson(p) observation, and use gamma(r, v) prior for p, the 
shape of the posterior is given by 
*If @ = h(0) is a continuous function of the parameter 0, then g+ (@), the prior for @ that corresponds to 
go (0) is found by the change of variable formula g+ (@) = go (0(@)) x $$. 

186 
BAYESIAN INFERENCE FOR POISSON 
We recognize this to be a gamma(r', w') density where the constants are updated 
by the simple formulas r' = r + y and w' = w + 1. We add the observation y to 
r, and we add 1 to w. Hence when we have a random sample y1,. . . , yn from a 
Poisson(p) distribution, and use a gamma(r, w) prior , we repeat the updating after 
each observation, using the posterior from the ith observation as the prior for the 
i + lSt observation. We end up with a a gamma(r', w') posterior where r' = r + 
y 
and w' = w + n. The simple updating rules are "add the sum of the observations to 
r" , and "add the number of observations to w". Note: these same updating rules 
work for the positive uniform prior, and the Jeffreys' prior for the Poi~son.~ 
We use 
Equation 7.10 and Equation 7.1 1 to find the posterior mean and variance. They are: 
respectively. 
Choosing a conjugate prior. The garnma(r, w) family of distributions is the 
conjugate family for Poisson(p) observations. It is advantageous to use a prior from 
this family, as the posterior will also be from this family and can be found by the 
simple updating rules. This avoids having to do any numerical integration. We want 
to find the garnrna(r, w) that matches our prior belief. 
We suggest that you summarize your prior belief into your prior mean m, and 
your prior standard deviation s. Your prior variance will be the square of your prior 
standard deviation. Then we find the gamma conjugate prior that matches those first 
two prior moments. That means that T and w will be the simultaneous solutions of 
the two equations 
m = -  
r 
and 
s 2 = - .  r 
W 
W 2  
Hence 
Substitute this into the first equation and solve for r. We find 
m2 
s2 . 
r = -  
This gives your gamma(r, w) prior. 
Precautions before using your conjugate prior. 
1 .  Graph your prior. If the shape looks reasonably close to your prior belief 
then use it. Otherwise you can adjust your prior mean m and prior standard 
deviation s until you find a prior with shape matching your prior belief. 
3The positive uniform prior g(p) = 1 has the form of a gamma( 1 , O )  prior, and the Jeffreys' prior for the 
Poisson g(p) = 'u- 4 has the form of a gamma( i, 0 )  prior, They can be considered limiting cases of the 
gamma(r, v) family where v + 0. 

SOME PRIOR DISTRIBUTIONS FOR POISSON 
187 
Table 10.1 
Diana's relative prior weights. The shape of her continuous prior is found 
by linearly interpolating between those values. The constant gets cancelled when finding the 
posterior using Equation 10.1. 
2. Calculate the equivalent sample size of your prior. This is the size of a random 
sample of Poisson(p) variables that matches the amount of prior information 
about p that you are putting in with your prior. We note that if y1, . . . , yn is 
a random sample from Poisson(p), then 
will have mean p and variance X. 
The equivalent sample size will be the solution of 
Setting the mean equal to the prior mean p = 
the equivalent sample size 
of the gamma(r, v) prior for p is neq = v. We check to make sure this isn't 
too large. Ask yourself "Is my prior knowledge about p really equal to the 
knowledge I would get about p if I took a random sample of size neq from the 
Poisson(p) distribution?" If the answer is no, then you should increase your 
prior standard deviation and recalculate your prior. Otherwise you are putting 
in too much prior information relative to the amount you will be getting from 
the data. 
Example 17 The weekly number of trafic accidents on a highway has the Poisson(p) 
distribution. Four students are going to take a observe the number of trafic accidents 
for each of the next eight weeks. They are going to analyze this in a Bayesian mannel; 
so they each need a prior distribution. Aretha says she has no prior informution, 
so will assume all possible values are equally likely. Thus she will use the positive 
uniform prior g(p) = 1 for p > 0, which is improper: Byron also says he has no prior 
information, but she wants his prior to be invariant if the parameter is multiplied by 
a constant. Thus, he uses the JefSreys' prior for the Poisson which is g ( p )  = p-’/2 
which will also be improper: Chase decides that he believes the prior mean should 
be 2.5, and the prior standard deviation is I .  He decides to use the gamma(r, v) that 
matches his prior mean and standard deviation, andflnds that v = 2.5 and r = 6.25. 
His equivalent sample size is neq = 2.5, which he decides is acceptable since he will 
be putting information worth 2.5 observations and there will be 8 observations from 
the data. Diana decides that her prior distribution has a trapezoidal shape found 
by interpolating the prior weights given in Table 10.1. The shapes of the four prior 
distributions are shown in FigurelO.1. The number of accidents on the highway over 

788 
BAYESIAN INFERENCE FOR POISSON 
I 
I 
I 
I 
I 
I 
1
;
 
\
/
 
I 
Aretha’s prior 
I 
Byron’s prior 
I 
Chase‘s prior 
_--__. 
*-. 
r‘ 
ss 
/ 
‘** 
/ 
*$ 
ll 
,! 
Diana’s prior 
I 
0 
I 
5 
I 
10 
figure 70.7 
The shapes of Aretha’s, Byron’s, Chase’s, and Diana’s prior distributions. 
the next 8 weeks are: 
3,2,0,8,2,4,6,1. 
Aretha will have a gamma(27,8)posteriol; Byron will have a gamma(26.5,8)poste- 
riol; and Chase will have a gamma(32.25,10.25)posteriol: 
Dianajnds herposterior 
numerically using Equation 10.1. The four posterior distributions are shown in Fig- 
ure 10.2. We see that the four posterior distributions are similarly shaped, despite 
the very direrent shape priors. 
Summarizing the Posterior Distribution 
The posterior density explains our complete belief about the parameter given the data. 
It shows the relative belief weights we give each possible parameter value, taking 
into account both our prior belief and the data, through the likelihood. However, 
a posterior distribution is hard to interpret, and we like to summarize it with some 
numbers. 
When we are summarizing a distribution, the most important summary number 
would be a measure of location, which characterizes where the distribution is located 
along the number line. Three possible measures of location are the posterior mode, 
the posterior median, and the posterior mean. The posterior mode is the found by 
setting the derivative of the posterior density equal to zero, and solving. When the 
posterior distribution is gamma(r’, d ) ,  its derivative is given by 

SOME PRIOR DISTRIBUTIONS FOR POISSON 
189 
I 
0 
I 
5 
I 
10 
Figure 10.2 Aretha’s, Byron’s, Chase’s, and Diana’s posterior distributions. 
When we set that equal to zero and solve, we find the posterior mode is 
r’ - 1 
mode = - 
W’ 
When the posterior distribution is garnrna(r’, w‘) the posterior median can be found 
using Minitab. The posterior mean will be 
r’ 
m‘ = - 
v’ 
If the posterior distribution has been found numerically, then both the posterior 
median and mean will both have to be found numerically using the Minitab macro 
tintegral. mac. 
The second most important summary number would be a measure of spread, that 
characterizes how spread out the distribution is. Some possible measures of spread 
include the interquartile range IQR = Q 3  - Q1 and the standard deviation s’. When 
the posterior distribution is gamma(r’, v’), the IQR can be found using Minitab. The 
posterior standard deviation will be the square root of the posterior variance. If the 
posterior distribution has been found numerically, then the IQR and the posterior 
variance can be found numerically. 
Example 17 (continued) The four students calculate measures of location and spread 
to summarize their posteriors. Aretha, Byron, and Chase have gamma posteriors, so 
they can calculate them easily using the formula’s, and Diana has a numericalposte- 
rior so she has to calculate them numerically using the Minitab macro tintegral.mac. 
The results are shown in Table 10.2. 

190 
BAYESIAN INFERENCE FOR POISSON 
Table 10.2 Measures of location and spread of posterior distributions 
Mean 
Median 
Mode 
3.375 
3.333 
3.25 
3.313 
3.271 
3.187 
3.071 
3.040 
2.976 
3.353 
3.318 
Aretha 
Byron 
Chase 
Diana 
St.Dev. 
IQR 
.6495 
.8703 
.6435 
3622 
S408 
.7255 
.6266 
3.502 
Posterior 
gamrna(27,g) 
gamma(26;, 8) 
gamrna(32+, 10;) 
Numerical 
10.2 INFERENCE FOR POISSON PARAMETER 
The posterior distribution is the complete inference in the Bayesian approach. It 
explains our complete belief about the parameter given the data. It shows the relative 
belief weights we can give every possible parameter value. However, in the frequentist 
approach there are several types of inference about the parameter we can make. These 
are point estimation, interval estimation, and hypothesis testing. In this section we 
see how we can do these inferences on the parameter p of the Poisson distribution 
using the Bayesian approach, and we compare these to the corresponding frequentist 
inferences. 
Point Estimation 
We want to find the value of the parameter p that best represents the posterior and use 
it as the point estimate. The posterior mean square of fi, an estimator of the Poisson 
mean, measures the average squared distance away from the true value with respect 
to the posterior4 It is given by 
where m’ is the posterior mean. Squaring the term and separating the integral into 
three integrals, we see that 
PMS(fi) = war(p./y) + 0 + (m’ - f i ) 2 .  
We see that the last term is always nonnegative, so that the estimator that has smallest 
posterior mean square is the posterior mean. On the average the squared distance 
the true value is away from the posterior mean is smaller than for any other possible 
4The estimator that minimizes the average absolute distance away from the true value is the posterior 
median. 

INFERENCE FOR POISSON PARAMETER 
191 
e~timator.~ 
That is why we recommend the posterior mean 
A 
r’ 
P B  = - 
W’ 
as the Bayesian point estimate of the Poisson parameter. The frequentist point 
estimate of the would be the f i f  = 0, the sample mean. 
Comparing estimators for the Poisson parameter. Bayesian estimators 
can have superior properties, despite being biased. They often perform better than 
frequentist estimators, even when judged by frequentist criteria. The mean squared 
error of an estimator 
~ ~ ( f i )  
= bias2(p) + war(p) 
(10.2) 
measures the average squared distance the estimator is from the true value. The 
averaging is over all possible values of the sample, so it is a frequentist criterion. 
It combines the bias and the variance of the estimator into a single measure. The 
frequentist estimator of the Poisson parameter is 
c 
Y i  
b f = n .  
This is unbiased, so its mean square equals its variance 
P 
n 
M S ( f i f )  = - . 
When we use a gurnrnu(r, w) prior the posterior will be a gurnrnu(r’, w’). The bias 
will be 
The variance will be 
Often we can find a Bayesian estimator that has smaller mean squared error over the 
range where we believe the parameter lies. 
Suppose we are going to observe the number of chocolate chips in a random 
sample of six chocolate chip cookies. We know that the number of chocolate chips 
SThis is the squared-error loss function approach 

792 
BAYESIAN INFERENCE FOR POlSSON 
Bayes 
Frequentist 
0 
1 
2 
3 
4 
5 
figure 70.3 The mean squared error for the two estimators. 
in a single cookie is a Poisson(p) random variable and we want to estimate p. We 
know that p should be close to 2. The frequentist estimate Pf = will be unbiased 
and its mean squared error will be 
Suppose we decide to use a gamma(2,l) prior, which has prior mean 2 and prior 
variance 2. Using Equation 10.2, we find the mean squared error of the Bayesian 
estimator will be 
The mean squared errors of the two estimators are shown in Figure 10.3. We see 
that on average, the Bayesian estimator is closer to the true value than the frequentist 
estimator in the range from .7 to 5. Since that is the range we believe that p lies, the 
Bayesian estimator would be preferable to the frequentist one. 
Bayesian Credible Interval for p 
An equal tail area 95% Bayesian credible interval for p can be found by obtaining 
the difference between the 97.5th and the 2.5th percentiles of the posterior. When 
we used either the gamma(r, v) prior, the positive uniform prior g(p) = 1 for p > 0, 
or the Jeffreys' prior g ( p )  = pu-i the posterior is gamma(r', w'). Using Minitab, 
pull down the calc menu to probability distributions and over to gamma and fill in 
the dialog box. 

INFERENCE FOR POISSON PARAMETER 
193 
Person 
Aretha 
Byron 
Chase 
Diana 
Posterior 
Credible Interval 
Lower 
Upper 
gamma(27,8) 
2.224 
4.762 
gamma(26;, 8) 
2.174 
4.688 
gamma(32+, 10;) 
2.104 
4.219 
numerical 
2.224 
4.666 
If we had started with a general continuous prior, the posterior would not be a 
gamma. The Bayesian credible interval would still be the difference between the 
97.5th and the 2.5th percentiles of the posterior, but we would find these percentiles 
numerically. 
Example 17 (continued) The Aretha, Byron, Chase, and Diana calculated their 
95% Bayesian credible intervals forp. Aretha, Byron, and Chase all hadgamma(r', v') 
posteriors, with different values of r' and v' because of their diyerent priors. Chase 
has a shorter credible interval because he put in more prior information than the 
others. Diana used a general continuous prior so she had to$nd the credible interval 
numerically. They are shown in Table 10.3. 
Bayesian Test of a One-sided Hypothesis 
Sometimes we have a null value of the Poisson parameter, pa. This is the value that 
the parameter has had before in the past. For instance, the random variable Y may 
be the number of defects occurring in a bolt of cloth, and p is the mean number of 
defects per bolt. The null value po is the mean number of defects when the machine 
manufacturing the cloth is under control. We are interested in determining if the 
Poisson parameter value has got larger than the null value. This means the rate of 
defects has increased. We set this up as a one-sided hypothesis test 
Ho : p 5 po 
versus 
Note: The alternative is in the direction we wish to detect. We test this hypothesis in 
the Bayesian manner by computing the posterior probability of the null hypothesis. 
This is found by integrating the posterior density over the correct region 
H I  : p > pa. 
PO 
+ I 
Po) = JiI 
S(PIY1,. . . > Yn) dP. 
(10.3) 
If the posterior distribution is garnma(r, s )  we can find this probability using Minitab. 
Pull down the calc menu to the probability distributions and over to gamma and fill 
in the dialog box. Otherwise, we can evaluate this probability numerically. We 
compare this probability with the level of significance a. If the posterior probability 

194 
BAYESIAN INFERENCE FOR POISSON 
Person 
Aretha 
Byron 
Chase 
Diana 
Table 10.4 Posterior probability of null hypothesis 
Posterior 
+ I 3.Olyl,. . . , Y ~ )  
= so3 dPLIYlr. . . > Yn) + 
gamma(27,8) 
.2962 
gamma(26:, 8) 
.3312 
gamma(32;, 10;) 
.4704 
numerical 
.3012 
of the null hypothesis is less than a, then we reject the null hypothesis at the a level 
of significance. 
Example 17 (continued) The four students decide to test the null hypothesis 
HO : p 5 3 
versus 
at the 5% level of signzjicance. Aretha, Byron, and Chase all have gamma(r', w') 
posteriors each with their own values of the constants. They each calculate the 
posterior probability of the null hypothesis using Minitab. Diana has a numerical 
prior; so she must evaluate the integral numerically. The results are shown in Table 
10.4. 
H1 : p > 3 
Bayesian Test of a Two-sided Hypothesis 
Sometimes we want to test whether or not the Poisson parameter value has changed 
from its null value in either direction. We would set that up as a two-sided hypothesis 
versus 
HO : p = po 
H1 : p # po 
Since we started with a continuous prior, we will have a continuous posterior. The 
probability that the continuous parameter taking on the null value will be 0, so we 
cannot test the hypothesis by calculating its posterior probability. Instead, we test 
the credibility of the null hypothesis by observing whether or not the null value po 
lies inside the (1 - a) x 100% credible interval for p. If it lies outside, we can reject 
the null hypothesis and conclude p # PO. If it lies inside the credible interval, we 
cannot reject the null hypothesis. We conclude po remains a credible value. 
Main Points 
The Poisson distribution counts the number of occurrence of a rare events 
which occur randomly through time (or space) at a constant rate. The events 
must occur one at a time. 

MAIN POINTS 
195 
The posterior 0; prior x likelihood is the key relationship. We cannot use 
this for inference because it only has the shape of the posterior, and is not an 
exact density. 
The constant k = sprior x likelihood is needed to find the exact posterior 
density 
prior x likelihood 
sprior x likelihood 
posterior = 
so that inference is possible. 
The gamma family of priors is the conjugate family for Poisson observations. 
0 If the prior is gamma(r, w), then the posterior is gamrna(r’, w‘) where the 
constants are updated by the simple rules r’ = r + C  y (add sum of observations 
tor, and w’ = w + n (add number of observations to w. 
It makes sense to use prior from conjugate family if possible. Determine your 
prior mean and prior standard deviation. Choose the gamma(r, v) prior that 
has this prior mean and standard deviation. Graph it to make sure it looks 
similar to your prior belief. 
0 If you have no prior knowledge, you can use a positive uniform prior density 
g(p) = 1 for p > 0, which has the form of a garnma(1,O). Or, you can use the 
Jeffreys’ prior for the Poisson g(p) cx p-3 for p > 0, which has the form of a 
gamma( i, 0). Both of these are improper priors (their integral over the whole 
range is infinite). Nevertheless, the posteriors will work out to be proper, and 
can be found from the same simple rules. 
0 If you can’t find a member of the conjugate family that matches your prior 
belief, construct a discrete prior using your belief weights at several values 
over the range. Interpolate between them to make your general continuous 
prior. You can ignore the constant needed to make this an exact density since 
it will get cancelled out when you divide by lprior x likelihood. 
0 With a good choice of prior the Bayesian posterior mean performs better than 
the frequentist estimator when judged by the frequentist criterion of mean 
squared error. 
0 The (1 - a) x 100% Bayesian credible interval gives a range of values for the 
parameter p that has posterior probability of 1 - a. 
0 We test a one-sided hypothesis in a Bayesian manner by calculating the poste- 
riorprobability of the null hypothesis. If this is less than the level of significance 
alpha, then we reject the null hypothesis. 
0 We can’t test a two-sided hypothesis by calculating the posterior probability 
of the null hypothesis, since it must equal 0 whenever we use a continuous 
prior. Instead, we test the credibility of the null hypothesis value by observing 

196 
BAYESIAN lNFERENCE FOR POISSON 
whether or not the null value lies inside the (1 - a) x 100% credible interval. 
If it lies outside the credible interval, we reject the null hypothesis at the level 
of significance a. Otherwise, we accept that the null value remains credible. 
Exercises 
10.1 The number of particles emitted by a radioactive source during a ten second in- 
terval has the Poisson(p) distribution. The radioactive source is observed over 
five non-overlapping intervals of ten seconds each. The number of particles 
emitted during each interval are: 4, 1, 3, 1, 3. 
(a) Suppose a prior uniform distribution is used for p. 
i. Find the posterior distribution for p. 
ii. What are the posterior mean, median, and variance in this case? 
(b) Suppose Jeffreys’ prior is used for p .  
i. Find the posterior distribution for p. 
ii. What are the posterior mean, median, and variance in this case? 
10.2 The number of claims received by an insurance company during a week follows 
a Poisson(p) distribution. The weekly number of claims observed over a ten 
week period are: 5, 8,4,6, 11,6,6,5,6,4. 
(a) Suppose a prior uniform distribution is used for p. 
i. Find the posterior distribution for p. 
ii. What are the posterior mean, median, and variance in this case? 
(b) Suppose Jeffreys’ prior is used for p .  
i. Find the posterior distribution for p. 
ii. What are the posterior mean, median, and variance in this case? 
10.3 The Russian mathematician Ladislaus Bortkiewicz noted that the Poisson dis- 
tribution would apply to low frequency events in a large population, even when 
the probabilities for individuals in the population varied. In a famous example 
he showed that the number of deaths by horse kick per year in the cavalry corps 
of the Russian army follows the Poisson distribution. The following data is 
reproduced from Hoe1 (1984). 
y 
(deaths) 
1 
0 
1 
2 
3 
4 
n(y) 
(frequency) 1 109 
65 
22 
3 
1 
(a) Suppose a prior uniform distribution is used for p. 
i. Find the posterior distribution for p, 

COMPUTER EXERCISES 
197 
ii. What are the posterior mean, median, and variance in this case? 
(b) Suppose Jeffreys’ prior is used for p. 
i. Find the posterior distribution for p. 
ii. What are the posterior mean, median, and variance in this case? 
10.4 The number of defects per 10 meters of cloth produced by a weaving machine 
has the Poisson distribution with mean p. You examine 100 meters of cloth 
produced by the machine and observe 71 defects. 
Your prior belief about p is that it has mean 6 and standard deviation 2. 
Find a gamma(r, w) prior that matches your prior belief. 
Find the posterior distribution of p given that you observed 7 1 defects in 
100 meters of cloth. 
Calculate a 95% Bayesian credible interval for p. 
Computer Exercises 
10.1 We will use the Minitab macro PoisGamPmac or the equivalent R function 
to find the posterior distribution of the Poisson probability p when we have a 
random sample of observations from a Poisson(p) distribution and we have a 
gamma(r, w) prior for p. The gamma family of priors is the conjugate family 
for Poisson observations. That means that if we start with one member of the 
family as the prior distribution, we will get another member of the family as the 
posterior distribution. The simple updating rules are “add sum of observations 
to r“ and “add sample size to w. When we start with a gamrna(r, w) prior, we 
get a gamma(r‘, w’) posterior where r’ = r + C(y) and w’ = w + n. 
Suppose we have a random sample of five observations from a Poisson(p) 
distribution. They are: 
3 
4 
3 
0 
1 
(a) Suppose we start with a positive uniform prior for p. What gamma(r, v) 
(b) Find the posterior distribution using the Minitab macro PoisGamRmac 
(c) Find the posterior mean and median. 
(d) Find a 95% Bayesian credible interval for p. 
prior will give this form? 
or the equivalent R function. 
10.2 Suppose we start with a Jeffreys’ prior for the Poisson parameter p. 
(a) What gamma(r, w) prior will give this form? 

198 
BAYESIAN lNFERENCE FOR POISSON 
(b) Find the posterior distribution using the Minitab macro PoisGarnPrnac 
(c) Find the posterior mean and median. 
(d) Find a 95% Bayesian credible interval for p. 
or the equivalent R function. 
10.3 Suppose we start with a gamrna(6,2) prior for p. Find the posterior distribution 
using the Minitab macro PoisGarnPrnac or the equivalent R function. 
(a) Find the posterior mean and median. 
(b) Find a 95% Bayesian credible interval for p. 
10.4 Suppose we take an additional five observations from the Poisson(p). They are: 
1 
2 
(a) Use the posterior from Computer Exercise 10.3 as the prior for the new ob- 
servations and find the posterior using the Minitab macro PoisGamPmac 
or the equivalent R function. 
(b) Find the posterior mean and median. 
(c) Find a 95% Bayesian credible interval for p. 
10.5 Suppose we use the entire sample of ten Poisson(p) observations as a single 
(a) Find the posterior given all ten observations using the Minitab macro 
(b) What do you notice from Computer Exercises 10.3-10.57 
(c) Test the null hypothesis Ho : p 5 2 vs H1 : p > 2 at the 5% level of 
10.6 We will use the Minitab macro PoisCCPrnac or the equivalent R function to find 
the posterior when we have a random sample from a Poisson(p) distribution 
and general continuous prior. Suppose we use the data from Computer Exercise 
10.5, and the prior distribution is given by 
sample. We will start with the original prior from Computer Exercise 10.3. 
PoisGamPmac or the equivalent R function. 
significance. 
for 
O < p i 2  
for 
2 < p < 4  
g(pL) = 
6 -  $ 
for 4 < 1 5  8 
Store the values of p and prior g ( p )  in column c l  and c2. 
{ r  
for 
8 < p 
(a) Use PoisCCPrnac or the equivalent R function to determine the posterior 
(b) Use tintegral.mac to find the posterior mean, median, and standard devi- 
(c) Find a 95% Bayesian credible interval for p by using tintegral.mac. 
distribution g(ply1 : . . . , yn). 
ation. 

11 
Bayesian Inference for 
Normal Mean 
Many random variables seem to follow the normal distribution, at least approximately. 
The reasoning behind the central limit theorem suggests why this is so. Any random 
variable that is the sum of a large number of similar-sized random variables from 
independent causes will be approximately normal. The shapes of the individual 
random variables "average out" to the normal shape. Sample data from the sum 
distribution will be well approximated by a normal. The most widely used statistical 
methods are those that have been developed for random samples from a normal 
distribution. In this chapter we show how Bayesian inference on a random sample 
from a normal distribution is done. 
11.1 BAYES' THEOREM FOR NORMAL MEAN WITH A DISCRETE 
PRIOR 
For a Single Normal Observation 
We are going to take a single observation from the conditional density f(ylp) that is 
known to be normal with known variance 0'. The standard deviation, CY, is the square 
root of the variance. There are only m possible values p1, . . . , pm for the mean. We 
choose a discrete prior probability distribution over these values, which summarizes 
our prior belief about the parameter, before we take the observation. If we really 
don't have any prior information, we would give all values equal prior probability. 
Introducfion fo Bayesian Sfufisfics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
199 

200 
BAYESIAN INFERENCE FOR NORMAL MEAN 
We only need to choose the prior probabilities up to a multiplicative constant, since 
it is only the relative weights we give to the possible values that are important. 
The likelihood gives relative weights to all the possible parameter values according 
to how likely the observed value was given each parameter value. It looks like 
the conditional observation distribution given the parameter, p, but instead of the 
parameter being fixed and the observation varying, we fix the observation at the one 
that actually occurred, and vary the parameter over all possible values. We only 
need to know it up to a multiplicative constant since the relative weights are all 
that is needed to apply Bayes’ theorem. The posterior is proportional to prior times 
likelihood, so it equals 
prior x likelihood 
= Cprior x likelihood ‘ 
Any multiplicative constant in either the prior or likelihood would cancel out. 
Likelihood of Single Observation 
The conditional observation distribution of yip is normal with mean p and variance 
u’, which is known. Its density is 
The likelihood of each parameter value is the value of the observation distribution at 
the observed value. The part that doesn’t depend on the parameter p is the same for 
all parameter values, so it can be absorbed into the proportionality constant. The part 
that gives the shape as a function of the parameter p is the important part. Thus the 
likelihood shape is given by 
(11.1) 
where y is held constant at the observed value and p is allowed to vary over all 
possible values. 
Table for Performing Bayes’ Theorem 
We set up a table to help us find the posterior distribution using Bayes’ theorem. 
The first and second columns contain the possible values of the parameter p and 
their prior probabilities. The third column contains the likelihood, which is the 
observation distribution evaluated for each of the possible values pLz where y is held 
at the observed value. This puts a weight on each possible value pi proportional to 
the probability of getting the value actually observed if pi is the parameter value. 
There are two methods we can use to evaluate the likelihood. 

BAYES' THEOREM FOR NORMAL MEAN WITH A DISCRETE PRIOR 
201 
Table 7 7.7 
normal distribution" 
Method 1: Finding posterior using likelihood from Table B.3 "ordinates of 
- 
P 
2.0 
2.5 
3.0 
3.5 
4.0 
~ 
Prior 
.2 
.2 
.2 
.2 
.2 
Z 
-1.2 
-.7 
-.2 
.3 
.8 
-
~
_
_
_
 
I 
I 
Likelihood 
.I942 
.3 123 
.3910 
.3814 
.2897 
I 
I 
Prior x Likelihood 
.03884 
.06246 
.0782 
.07628 
.05794 
Posterior 
.1238 
.1991 
.2493 
,243 1 
.I847 
.3 1372 
I 
1.oooo 
Finding likelihood from the "ordinates of normal distribution " table. 
The first method is to find the likelihood from the "ordinates of the normal distribu- 
tion" table. Let 
Y - P  
Z = -  
0 
for each possible value of p. Z has a standardized normal (0,l) distribution. The 
likelihood can be found by looking up f ( z )  in the "ordinates of the standard normal 
distribution" given in Table B.3 in Appendix B. Note that f (-z) = f ( z )  because of 
standard normal distribution is symmetric about 0 .  
Finding the likelihood from the normal density function. The second 
method is to use the normal density formula given in Equation 1 1.1, holding y fixed 
at the observed value and varying p over all possible values. 
Example 18 Suppose yip is normal with mean p and known variance u2 = 1. We 
know there are onlyfive possible values for p. They are 2.0, 2.5, 3.0, 3.5, and 4. We 
let them be equally likely for our prior We take a single observation of y and obtain 
the value y = 3.2. Let 
Y - P  
z = - .  
0 
The values for the likelihood f(z) are found in Table B.3, "ordinates of normal 
distribution, " in Appendix B. Note that f (-2) = f ( z )  because of standard normal 
density is symmetric about 0. The posterior probability is the prior x likelihood 
divided by sum ofprior x likelihood. The results are shown in Table 11.1. 
If we evaluate the likelihood using the normal density formula, the likelihood is 
proportional to 
, - & ( Y - d 2  
where y is held at 3.2 and p varies over all possible values. Note, we are absorbing 
everything that doesn't depend on p into the proportionality constant. The posterior 
probability is the prior x likelihood divided by sum ofprior x likelihood. The results 
are shown in Table 11.2. We note that the results agree with what we found before 
except for small round-off errors. 

202 
BAYESIAN INFERENCE FOR NORMAL MEAN 
Prior 
.2 
.2 
.2 
,2 
.2 
Table 7 7.2 
Method 2: Finding posterior using likelihood from normal density formula 
Likelihood 
(ignoring constant) 
,-;(3.2-Z.O)' 
=.4868 
,-;(3.2-2.5)' 
=.7827 
,-+(3.2-3.0)' 
=.9802 
,-&(3.2-3.5)' 
=.9560 
,-&(3.2-4.0)' 
=.7261 
P 
__ 
2.0 
2.5 
3.0 
3.5 
4.0 
Prior x Likelihood 1 
Posterior 
.0974 
.1565 
.1960 
,1912 
.1452 
.1239 
.1990 
.2493 
.2432 
.1846 
I
1
 
I 
I 
.7863 
I 
1.0000 
For a Random Sample of Normal Observations 
Usually we have a random sample y1, . . . , yn of observations instead of a single 
observation. The posterior is always proportional to the prior x likelihood. The 
observations in a random sample are all independent of each other, so the joint 
likelihood of the sample is the product of the individual observation likelihoods. 
This gives 
f(Y1,. . . ,YnlP) = f(Y1IP) x f(Y2lPu) x ... x f(YnlP). 
dPIY1,. ' ' 1 Yn) 0: 9(P) x f(Y1IP) x . . . x f(YnlP). 
Thus given a random sample,' Bayes' theorem with a discrete prior is given by 
We are considering the case where the distribution of each observation y j  1~ is normal 
with mean p and variance u2, which is known. 
Finding the posterior probabilities analyzing observations sequentially 
one at a time. We could analyze the observations one at a time, in sequence 
y1, . . . , yn , letting the posterior from the previous observation become the prior for 
the next observation. The likelihood of a single observation y j  is the column of values 
of the observation distribution at each possible parameter value at that observed value. 
The posterior is proportional to prior times likelihood. 
Example 19 Suppose we take a random sample of four observations from a normal 
distribution having mean p and known variance uz = 1. The observations are 3.2, 
2.2, 3.6, and 4.1. 
I De Finetti introduced a condition weaker than independence called exchangeability. Observations are 
exchangeable if the conditional density of the sample f(y1, . . . , yn) is the unchanged for any permutation 
of the subscripts. In other words, the order the observations were taken has no useful information. 
De Finetti (1991) shows that when the observations are exchangeable, f(y1,. . . ,ya) = J v ( 0 )  x 
w(yll0) x Ur(y,lO) 
do, for some parameter 0 where 4 8 )  is some prior distribution and w(y/0) is 
some conditional distribution. The observations are conditionally independent, given 0. The posterior 
g(0) rn v(0) x ~ ( ~ 1 1 0 )  
x w(y,if3). This allows us to treat the exchangeable observations as if they 
come from a random sample. 

BAYES’ THEOREM FOR NORMAL MEAN WITH A DISCRETE PRIOR 
203 
The possible values of p are 2.0, 2.5, 3.0, 3.5, and 4.0. Again, we will use the 
prior that gives them all equal weight. We want to use Bayes’ theorem to find our 
posterior belief about p given the whole random sample. The posterior equals 
prior x likelihood 
g(p‘y)= Cprior x likelihood 
The results of analyzing the observations one at a time are shown in Table 11.3. This 
is clearly a lot of work for a large sample. We will see that it is much easier to use 
the whole sample together 
Finding the posterior probabilities analyzing the sample all together in 
a single step. The posterior is proportional to the prior x likelihood, and the joint 
likelihood of the sample is the product of the individual observation likelihoods. Each 
observation is normal, so it has a normal likelihood. This gives the joint likelihood 
f ( y l , ,  , , , y n P  
1 ) x e-&(~1-p)’ x e-h(Y2-p)’ x . . . x e-&(Yn-p)2 . 
Adding the exponents gives 
- &2 
[(Yl - ~ ) 2 + ( Y 2 - ~ ) 2 + . ~ . + ( Y ~ - ~ ) 2 ]  . 
f(Y1,. . YnlP) 
e 
We look at the term in brackets 
[ ( Y l  - p)2 + . . . + (yn - p)] = y; - 2y1p + p2 + . . . + y; - 2ynp + p 
and combine similar terms to get 
= (y: + . . . + y;) - 2p(y1+ . . . + yn) + np’ . 
When we substitute this back in, factor out n, and complete the square we get 
The likelihood of the normal random sample yl, . . . , yn is proportional 
to the likelihood of the sample mean g. When we absorb the part that doesn’t 
involve p into the proportionality constant we get 
We recognize that this likelihood has the shape of a normal distribution with mean p 
and variance $. We know 9, the sample mean, is normally distributed with mean p 
and variance $. So the joint likelihood of the random sample is proportional to the 
likelihood of the sample mean, which is 

204 
.7863 
BAYESIAN INFERENCE FOR NORMAL MEAN 
1 .moo 
Table 11.3 Analyzing observations one at a time a 
.6336 
1 .oooo 
Prior1 
Prior1 x Likelihood1 1 Posterior1 
Likelihood1 
(ignoring constant) 
,- 
h(3.2-2.5)’ 
=.7827 
,-;(3.2-2.0)’ 
=.4868 
,-$(3.2-3.0)’ 
=.9802 
,-&(3.2-3.5)’ 
=.9560 
,- 
i(3.2-4.0)’ 
=.7261 
2.0 
2.5 
3.0 
3.5 
4.0 
.2 
.2 
.2 
.2 
.2 
.0974 
.1565 
.1960 
.1912 
.1452 
.1239 
.1990 
.2493 
.2432 
.1846 
Likelihood2 
Prior2 x Likelihood2 
Posterior2 
Prior2 
,1239 
.1990 
.2493 
.2432 
.1846 
(ignoring constant) 
,-i(2.2-2.0)’ 
=.9802 
2.0 
2.5 
3.0 
3.5 
4.0 
~ 
.1214 
.1902 
.1810 
.lo45 
.0365 
,1916 
,3002 
.2857 
.1649 
.0576 
,-; 
(2.2-2.5)’ 
=.9560 
,-$(2.2-3.0)’ 
=.7261 
,-%(2.2-3.5)’ 
=.4296 
,-$(2.2-4.0)’ 
=.1979 
Prior3 x Likelihoods 
Posterior3 
Prior3 
P 
Likelihood3 
(ignoring constant) 
,- 
i(3.6-2.5)’ 
=.5461 
e- i(3.6-3.0)’ 
=.8353 
,-$(3.6-3.5)’ 
=.9950 
,-&(3.6-4.0)’ 
=.9231 
,-i(3.6-2.0)’ 
=.2780 
2.0 
2.5 
3.0 
3.5 
4.0 
.1916 
.3002 
.2857 
.1649 
.0576 
.0533 
.0792 
.1639 
I 
.2573 
.2386 
,1641 
.0532 
.3745 
.2576 
.0835 
.673 1 
1 
1.0000 
Prior4 x Likelihood4 
Posterior4 
Prior4 
Likelihood4 
P 
(ignoring constant) 
,- 
&(4.1-2.0)’ 
=.1103 
2.0 
2.5 
3.0 
3.5 
4.0 
,0087 
.07 15 
.2045 
.2152 
.0838 
,0792 
.2573 
.3745 
,2576 
.0835 
.0149 
.1226 
.3508 
.3691 
.1425 
=.2780 
,- 
&(4.1-3.0)’ 
=.5461 
,- 
i (4.1-3.5)’ 
=.8352 
=.9950 
,- 
4(4.1-2.5)’ 
, 
- 4 (4.1 - 4.0) * 
3330 
1 
1.0000 1 
~ 
Vote: 
he prior for observation i is the posterior after previous observation i - 1 

BAYES THEOREM FOR NORMAL MEAN WITH A CONTINUOUS PRIOR 
205 
p 
Prior1 
Likelihoodg 
Prior1 x Likelihoodg 
2.0 
.2 
e 2x:/4(3.275-2.0)2- -.0387 
.0077 
2.5 
.2 
(3,275-2.5)’- -.3008 
.0602 
3.0 
.2 
ZX:I4 
(3.275-3.0)’- -3596 
.1719 
3.5 
.2 
e 
-.9037 
,1807 
4.0 
.2 
e -& 
(3.275-4.0)’- -.3495 
.0699 
.4904 
-- 
-- 
-- 
- 
(3.275-3.5)’- 
Posteriorg 
,0157 
,1228 
.3505 
.3685 
.1425 
1 .OoOo 
We can think of this as drawing a single value, g, the sample mean, from the normal 
distribution with mean p and variance $. This will make analyzing the random 
sample much easier. 
We substitute in the observed value of y, the sample mean, and calculate its 
likelihood. Then we just find the posterior probabilities using Bayes’ theorem in 
only one table. This is much less work! 
Example 19 (continued) In the preceding sample the mean 
= 3.275. We use the 
likelihood of jj which is proportional to the likelihood of the whole sample. The 
results are shown in Table 11.4. We see that they agree with the previous results 
to three jgures. The slight discrepancy in the fourth decimal place is due to the 
accumulation of round off errors when we analyze the observations one at a time. It 
is clearly easier to use jj to summarize the sample, and pelform the calculations for 
Bayes’ theorem only once.2 
11.2 BAYES’ THEOREM FOR NORMAL MEAN WITH A CONTINUOUS 
PRIOR 
We have a random sample yl, . . . , yn from a normal distribution with mean p and 
known variance 02. It is more realistic to believe that all values of p are possible, 
at least all those in an interval. This means we should use a continuous prior. We 
know that Bayes’ theorem can be summarized as posterior proportional to prior 
times likelihood 
g(llIy1,. ’ .  7 Yn) 3: g(p) x f(Y1, ’ . . , YnlP). 
Here we allow g ( p )  to be a continuous prior density. When the prior was discrete, 
we evaluated the posterior by dividing the prior x likelihood by the sum of the prior 
x likelihood over all possible parameter values. Integration for continuous variables 
8 is said to be a sufficient statistic for the parameter p. The likelihood of a random sample yi, . . . , yn 
can be replaced by the likelihood of a single statistic only if the statistic is sufficient for the parameter. 
One-dimensional sufficient statistics only exist for some distributions, notably those that come from the 
one-dimensional exponential family. 

206 
BAYESIAN lNFERENCE FOR NORMAL MEAN 
is analogous to summing for discrete variables. Hence we can evaluate the posterior 
by dividing the prior x likelihood by the integral of the prior x likelihood over the 
whole range of possible parameter values. Thus 
(11.3) 
For a normal distribution, the likelihood of the random sample is proportional to the 
likelihood of the sample mean, g. So 
This works for any continuous prior density g(p). However, it requires an integration, 
which may have to be done numerically. We will look at some special cases where 
we can find the posterior without having to do the integration. For these cases, we 
have to be able to recognize when a density must be normal from the shape given in 
Equation 1 1.1. 
Flat Prior Density for p (Jeffrey's Prior for Normal Mean) 
We know that the actual values the prior gives to each possible value is not important. 
Multiplying all the values of the prior by the same constant would multiply the 
integral of the prior times likelihood by the same constant, so it would cancel out, 
and we would obtain the same posterior. What is important is that the prior gives the 
relative weights to all possible values that we believe before looking at the data. 
The flat prior gives each possible value of p equal weight. It does not favor any 
value over any other value, g(p) = 1. The flat prior is not really a proper prior 
distribution since -m < p < co, so it can't integrate to 1. Nevertheless, this 
improper prior works out all right. Even though the prior is improper, the posterior 
will integrate to 1, so it is proper. The Jeffreys' prior for the mean of a normal 
distribution turns out to be the flat prior. 
A single normal observation y. Let y be a normally distributed observation 
with mean p and known variance 02. 
The likelihood is given by 
if we ignore the constant of proportionality. Since the prior always equals 1, the 
posterior is proportional to this. Rewrite it as 
We recognize from this shape that the posterior is a normal distribution with mean y 
and variance a2. 

BAYES‘ THEOREM FOR NORMAL MEAN WlTHA CONTINUOUS PRIOR 
207 
A normal random sample y1, . . . yn. 
In the previous section we showed that 
the likelihood of a random sample from a normal distribution is proportional to 
likelihood of the sample mean g. We know that is normally distributed with mean 
p and variance $. Hence the likelihood has shape given by 
where we are ignoring the constant of proportionality. Since the prior always equals 
1, the posterior is proportional to this. Rewrite it as 
We recognize from this shape that the posterior distribution is normal with mean g 
and variance < . 
Normal Prior Density for p 
Single observation. The observation y is a random variable taken from a normal 
distribution with mean /.L and variance u2 which is assumed known. We have a prior 
distribution that is normal with mean rn and variance s2. The shape of the prior 
density is given by 
g(p) c( e-&(p-m)’ , 
where we are ignoring the part that doesn’t involve p because multiplying the prior 
by any constant of proportionality will cancel out in the posterior. The shape of the 
likelihood is 
f ( y l p )  c( e-&(v-p)’ 
, 
where we ignore the part that doesn’t depend on p because multiplying the likelihood 
by any constant will cancel out in the posterior. The prior times likelihood is 
- 4 ( P - m )   + ( u - r P  
S(P) x f(YICL) c( e [ 
s2 
O 2  I .  
1 .  
Putting the terms in exponent over the common denominator and expanding them 
out gives 
- 1 [ u 2 ( r  - 2 r m + m  ) + .  ( u 2 -  ~ ~ + r  )  
,2& 
c t e  
We combine the like terms 
and factor out (u2 + s2)/(u2s2). 
Completing the square and absorbing the part that 
doesn’t depend on /.L into the proportionality constant, we have 

208 
BAYESIAN lNFERENCE FOR NORMAL MEAN 
We recognize from this shape that the posterior is a normal distribution having mean 
and variance given by 
(11.4) 
up with a nor- 
0 2 . 2  
( 0 2  + s2) ’ 
and ( s ’ ) ~  
= 
, 
( a 2 m + s 2 y )  
m =  
u2 + s2 
respectively. We started with a normal(m, s2) prior, and ended . 
rnal[m’, ( s ’ ) ~ ]  
posterior. This shows that the nonnal(m, s2) distribution is the con- 
jugate family for the normal observation distribution with known variance. Bayes’ 
theorem moves from one member of the conjugate family to another member. Be- 
cause of this we don’t need to perform the integration in order to evaluate the posterior. 
All that is necessary is to determine the rule for updating the parameters. 
Simple updating rule for normal family. The updating rules given in Equation 
1 1.4 can be simplified. First we introduce the precision of a distribution that is the 
reciprocal of the variance. Recisions are additive. The posterior precision 
1 
0 2 + s 2  
1 
1 
Thus the posterior precision equals prior precision plus the observation precision. 
The posterior mean is given by 
This can be simplified to 
Thus the posterior mean is the weighted average of the prior mean and the observation, 
where the weights are the proportions of the precisions to the posterior precision. 
This updating rule also holds for the flat prior. The flat prior has infinite variance, 
so it has zero precision. The posterior precision will equal the prior precision 
1 / 0 2  = 0 + 1 / 0 2  , 
and the posterior variance equals the observation variance 02. The flat prior doesn’t 
have a well-defined prior mean. It could be anything. We note that 
so the posterior mean using flat prior equals the observation y 

CHOOSING YOUR NORMAL PRIOR 
209 
A random sample y1,. . . , yn. A random sample y1,. . . , yn is taken from a 
normal distribution with mean p and variance u', which is assumed known. We have 
a prior distribution that is normal with mean m and variance s2 given by 
where we are ignoring the part that doesn't involve p because multiplying the prior 
by any constant will cancel out in the posterior. 
We use the likelihood of the sample mean, g which is normally distributed with 
mean p and variance <. The precision of g is (3). 
We see that this is the sum of 
all the observation precisions for the random sample. 
We have reduced the problem to updating given a single normal observation of jj, 
which we have already solved. Posterior precision equals the prior precision plus the 
precision of 0. 
(11.5) 
The posterior variance equals the reciprocal of posterior precision. The posterior 
mean equals the weighted average of the prior mean and g where the weights are the 
proportions of the posterior precision: 
(11.6) 
11.3 CHOOSING YOUR NORMAL PRIOR 
The prior distribution you choose should match your prior belief. When the observa- 
tion is from a normal distribution with known variance, the conjugate family of priors 
for p is the nonnal(m, s2). If you can find a member of this family that matches your 
prior belief, it will make finding the posterior using Bayes' theorem very easy. The 
posterior will also be a member of the same family where the parameters have been 
updated by the simple rules given in Equations 11.5 and 11.6. You won't need to do 
any numerical integration. 
First, decide on your prior mean m. This is the value your prior belief is centered 
on. Then decide on your prior standard deviation s. Think of the points above 
and below that you consider to be the upper and lower bounds of possible values 
of p. Divide the distance between these two points by 6 to get your prior standard 
deviation s. This way you will get reasonable probability over all the region you 
believe possible. 
A useful check on your prior is to consider the "equivalent sample size". Set your 
prior variance s2 = u2/neq 
and solve for neq. This relates your prior precision to the 
precision from a sample. Your belief is of equal importance to a sample of size neq. 
If neq is large, it shows you have very strong prior belief about p. It will take a lot 
of sample data to move your posterior belief far from your prior belief. If it is small, 
your prior belief is not strong, and your posterior belief will be strongly influenced 
by a more modest amount of sample data. 

210 
BAYESIAN INFERENCE FOR NORMAL MEAN 
Arnie's prior 
Barb's prior 
Chucks prior 
I 
I 
I 
I 
I 
10 
20 
30 
40 
50 
Figure 11.1 The shapes of Arnie's, Barb's, and Chuck's priors. 
If you can't find a prior distribution from the conjugate family that corresponds 
to your prior belief, then you should determine your prior belief for a selection of 
points over the range you believe possible, and linearly interpolate between them. 
Then you can determine your posterior distribution by 
Example 20 Arnie, Barb, and Chuck are going to estimate the mean length of one- 
year-old rainbow trout in a stream. Previous studies in other streams have shown 
the length of yearling rainbow trout to be normally distributed with known standard 
deviation of 2 cm. Amie decides his prior mean is 30 cm. He decides that he doesn't 
believe it is possible for a yearling rainbow to be less than I8 cm or greater than 42 
cm. Thus his prior standard deviation is 4 cm. Thus he will use a normal(30, 42) 
priol: Barb doesn't know anything about trout, soshe decides to use the '1Aat"priol: 
Chuck decides his prior belief is not normal. His prior has a trapezoidal shape. His 
prior gives zero weight at 18 cm. It gives weight one at 24 cm, and is level up to 40 
cm, and then goes down to Zero at 46 cm. He linearly interpolates between those 
values. The shapes of the three priors are shown in Figure 11.1. 
They take a random sample of 12 yearling trout from the stream and find the 
sample mean = 32cm. Amie and Barb find their posterior distributions using the 
simple updating rules for the normal conjugate family given by Equations 11.5 and 
11.6. For Arnie 
1 
12 
- 
- 
1 
(s1)2 
Z + F .  

BAYESIAN CREDIBLE INTERVAL FOR NORMAL MEAN 
27 7 
29 
30 
31 
32 
33 
34 
Figure 7 7.2 
posteriors.) 
Amie's, Barb's, and Chuck's posteriors. (Barb and Chuck have nearly identical 
Solving for this gives his posterior variance (s')' = .3265. His posterior standard 
deviation is s1 = 5714. His posterior mean is found by 
1 
12 
- 
m' = 42 
, 
x 30 + 22 x 32 = 31.96. 
363 * 
Barb is using the 'yat"prior; so her posterior variance is 
I 2  
12 
22 
( s )  = - = .3333 
and her posterior standard deviation is s' = 5774. Her posterior mean m' = 32, 
the sample mean. Both Arnie and Barb have normal posterior distributions. 
Chuckjnds his posterior using Equation 11.3 which requires numerical integra- 
tion. The three posteriors are shown in Figure 11.2. Since Chuck used a prior that 
was fiat over the whole region where the likelihood was appreciable, his posterior is 
virtually indistinguishable from Barb's who used the pat improper priol: Arnie who 
used an informative prior has a posterior that is also close to Barb's. This shows that 
given the data, the posteriors are similar despite starting from quite different priors. 
11.4 BAYESIAN CREDIBLE INTERVAL FOR NORMAL MEAN 
The posterior distribution g(ply1, . . . , yn) is the inference we make for p given the 
observations. It summarizes our entire belief about the parameter given the data. 
Sometimes we want to summarize our posterior belief into a range of values that 
we believe cannot be ruled out at some probability level, given the sample data. 

212 
BAYESIAN INFERENCE FOR NORMAL MEAN 
An interval like this is called a Bayesian credible interval. It summarizes the range 
of possible values that are credible at that level. There are many possible credible 
intervals for a given probability level. Generally, the shortest one is preferred. 
However, in some cases it is easier to find the credible interval with equal tail 
probabilities. 
Known Variance 
When y1 , . . . , yn is arandom sample from anormal (pi u2) distribution, the sampling 
distribution of 9, the sample mean, is normal (p, c2/n). Its mean equals that for a 
single observation from the distribution, and its variance equals the variance of single 
observation divided by sample size. Using either a “flat“ prior, or a normal (m, s2) 
prior, the posterior distribution of p given jj is normal [m’, (s’)’], where we update 
according to the rules: 
1. Precision is the reciprocal of the variance. 
2. Posterior precision equals prior precision plus the precision of sample mean. 
3. Posterior mean is weighted sum of prior mean and sample mean, where the 
weights are the proportions of the precisions to the posterior precision. 
Our (1 - a) x 100% Bayesian credible interval for p is 
m’*z: x s’, 
(11.7) 
which is the posterior mean plus or minus the z-value times the posterior standard 
deviation, where the z-value is found in the standard normal table. Our posterior 
probability that the true mean p lies outside the credible interval is a. Since the 
posterior distribution is normal and thus symmetric, the credible interval found using 
Equation 1 1.7 is the shortest, as well as having equal tail probabilities. 
Unknown Variance 
If we don’t know the variance, we don’t know the precision, so we can’t use the 
updating rules directly. The obvious thing to do is to calculate the sample variance 
.
n
 
i=l 
from the data. Then we use Equations 1 1.5 and 1 1.6 to find (5’)’ and m‘ where we 
use the sample variance 82 in place of the unknown variance u. 
There is extra uncertainty here, the uncertainty in estimating c2. 
We should widen 
the credible interval to account for this added uncertainty. We do this by taking the 
values from the table for the Student’s t distribution instead of the standard normal 
table. The correct Bayesian credible interval is 
m ’ f t q  x s. 
(11.8) 

BAYESIAN CREDIBLE INTERVAL FOR NORMAL MEAN 
213 
Table 11.5 95% credible intervals 
Person 
Posterior 
distribution 
Arnie 
Normal(3 I. 96, .3265) 
Normal(32.00, ,3333) 
Chuck 
numerical 
Credible interval 
lower 
upper 
30.84 
33.08 
30.87 
33.13 !
30.82 
33.07 
The t value is taken from the row labelled df = n - 1 (degrees of freedom equals 
number of observations minus l)3. 
Nonnormal Prior 
When we start with a nonnormal prior, we find the posterior distribution for p using 
Bayes' theorem where we have to integrate numerically. The posterior distribution 
will be nonnormal. We can find a (1 - a) x 100% credible interval by finding a 
lower value p1 and an upper value pu such that 
There are many such values. The best choice 111 and pu would give us the shortest 
possible credible interval. These values also satisfy 
Sometimes it is easier to find the credible interval with lower and upper tail areas that 
are equal. 
Example 20 (continued) Amie, and Barb each calculated their 95% credible inter- 
val from their respective posterior distributions using Equation 11.7. Chuck had to 
calculate his numerically from his numerical posterior using the Minitab macro tin- 
tegral.mac. The credible intervals are shown in Table 11.5. Amie, Barb, and Chuck 
end up with slightly different credible intervals because they started with different 
prior beliefs. But the effect of the data was much greater than the effect of their priors 
and their credible intervals are quite similal: 
3The resulting Bayesian credible interval is exactly the same one that we would find if we did the full 
Bayesian analysis with u' 
as a nuisance parameter, using the joint prior distribution for 1 and D' 
made 
up of the same prior for plu’ that we used before ["flat" or normal(m, 8') ]times the prior for D' 
given 
by g(a2) o( (a2)-' . We would find the joint posterior by Bayes' theorem. We would find the marginal 
posterior distribution of 1 by marginalizing out u', 
We would get the same Bayesian credible interval 
using Srudenr's t critical values. 

214 
BAYESIAN INFERENCE FOR NORMAL MEAN 
11.5 PREDICTIVE DENSITY FOR NEXT OBSERVATION 
Bayesian statistics has a general method for developing the conditional distribution 
of the next random observation, given the previous random sample. This is called 
the predictive distribution. This is a clear advantage over frequentist statistics, which 
can only determine the predictive distribution for some situations. The problem is 
how to combine the uncertainty from the previous sample with the uncertainty in 
the observation distribution. The Bayesian approach is called marginalization. It 
entails finding the joint posterior for the next observation and the parameter, given the 
random sample. The parameter is treated as a nuisance parameter, and the marginal 
distribution of the next observation given the random sample is found by integrating 
the parameter out of the joint posterior distribution. 
Let yn+l be the next random variable drawn after the random sample y1, . . . , yn. 
The predictive density of yn+l Iy1, . . . , yn is the conditional density 
f(YntllY1,. . . ,Yn) 
This can be found by Bayes’ theorem. y1, . . . , yn, yn+l is a random sample from 
f(yIp), which is a normal distribution with mean p and known variance C? 
. The 
conditional distribution of the random sample y1, . . . , yn and the next random obser- 
vation yn+l given the parameter p is 
f(Yl,...,YnrYn+l/P) =f(YlIP) x . . ‘ X  f(YnlP) x f(Yn+llP). 
Let the prior distribution be g(p) (either flat prior or nomal(m, s2) prior). The joint 
distribution of the observations and the parameter p is 
The conditional density of yn+l and p given y1, . . . , yn is 
f(Yn+l, PlYl,. . ., Yn) = f(Yn+l/P, Y l ,  . ’ ‘ , Yn) x S(PlY11.. . I Yn) ’ 
We have already found that the posterior g(pL/yl,. . . , yn, ) is normal with posterior 
precision equal to prior precision plus the precision of g and mean equal to the 
weighted average of the prior mean and g where the weights are proportions of the 
precisions to the posterior precision. Say it is normal with mean m, and variance s i .  
The distribution of yn+l given p and y1,. . . , yn only depends on p, because yn+l is 
another random draw from the distribution g(y1p). Thus the joint posterior (to first 
n observations) distribution is 
f(Yn+l,PlYlr.. . ?  Yn) = f(Yn+llP) x 9(4Y1,. . . > Yn) . 
The conditional distribution we want is found by integrating p out of the joint 
posterior distribution. This is the marginal posterior distribution 

MAIN POINTS 
215 
These are both normal under our assumed model, so 
Adding the exponents and combining like terms. 
Factoring out (& + &) of the exponent and completing the square 
The first line is the only part that depends on p, and we recognize that it is 
proportional to a normal density, so integrating it over its whole range gives a 
constant. Reorganizing the second part gives 
(11.9) 
We recognize this as a normal density with mean m’ = m, and variance ( s  ) ~  = 
0’ + s i .  The predictive mean for the observation yn+l is the posterior mean of p 
given the observations y1, . . . , y,. 
The predictive variance is the observation variance 
o2 plus the posterior variance of p given the observations y1, . . . , y,. 
(Part of the 
uncertainty in the prediction is due to the uncertainty in estimating the posterior 
mean.) 
This is one of the advantages of the Bayesian approach. It has a single clear 
approach (marginalization) that is always used to construct the predictive distribution. 
There is no single clear-cut way this can be done in frequentist statistics, although in 
many problems such as the normal case we just did, they can come up with similar 
results. 
Main Points 
Analyzing the observations sequentially one at a time, using the posterior from 
the previous observation as the next prior, gives the same results as analyzing 
all the observations at once using the initial prior. 

216 
BAYESlAN lNFERENCE FOR NORMAL MEAN 
0 The likelihood of a random sample of normal observations is proportional to 
the likelihood of the sample mean. 
0 The conjugate family of priors for normal observations with known variance 
is the normal(m, s2) family. 
If we have a random sample of normal observations and use a normal(m, s2) 
prior the posterior is normal(m’, ( s ’ ) ~ ) ,  where m’ and ( s  ) ~  are found by the 
simple updating rules: 
o The precision is the reciprocal of the variance. 
o Posterior precision is the sum of the prior precision and the precision of 
the sample. 
o The posterior mean is the weighted average of the prior mean and the 
sample mean, where the weights are the proportions of their precisions 
to the posterior precision. 
0 The same updating rules work for the flat prior, remembering the flat prior has 
precision equal to zero. 
0 A Bayesian credible interval for p can be found using the posterior distribution. 
0 If the variance g2 is not known, we use the estimate of the variance calculated 
from the sample, 82, and use the critical values from the Students t table where 
the degrees of freedom is n - 1, the sample size minus 1. Using the Students 
t critical values compensates for the extra uncertainty due to not knowing 02. 
(This actually gives the correct credible interval if we used a prior g ( 0 2 )  0: $ 
and marginalized n2 out of the joint posterior.) 
0 The predictive distribution of the next observation is normal(m’, ( s ’ ) ~ )  
where 
the mean m’ = m,, the posterior mean, and (so2 = o2 + s i ,  the observa- 
tion variance plus the posterior variance. (The posterior variance sz allows 
for the uncertainty in estimating p.) The predictive distribution is found by 
marginalizing p out of the joint distribution f(y,+l, pIy1, . . . , Y,). 
Exercises 
11.1 You are the statistician responsible for quality standards at a cheese factory. 
You want the probability that a randomly chosen block of cheese labelled 
“1 kg” is actually less than 1 kilogram (loo0 grams) to be 1% or less. The 
weight (in grams) of blocks of cheese produced by the machine is normal 

EXERCISES 
217 
(p; 0') where o2 = 3'. The weights (in grams) of 20 blocks of cheese are: 
994 
997 
999 
1003 
994 
998 
1001 
998 
996 
1002 
1004 
995 
994 
995 
998 
1001 
995 
1006 
997 
998 
You decide to use a discrete prior distribution for p with the following proba- 
bilities: 
Value 
Prior Probability 
99 1 
992 
993 
994 
995 
996 
997 
998 
999 
1000 
1001 
1002 
1003 
1004 
100s 
1006 
1007 
1008 
1009 
1010 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
.05 
(a) Calculate your posterior probability distribution. 
(b) Calculate your posterior probability that p < 1000. 
(c) Should you adjust the machine? 
11.2 The city health inspector wishes to determine the mean bacteria count per liter 
of water at a popular city beach. Assume the number of bacteria per liter of 
water is normal with mean p and standard deviation known to be 0 = 15. She 
collects 10 water samples and found the bacteria counts to be: 

218 
BAYESIAN lNFERENCE FOR NORMAL MEAN 
175 
190 
215 
198 
184 
207 
210 
193 
196 
180 
She decides that she will use a discrete prior distribution for p with the follow- 
ing probabilities: 
Value 
Prior Probability 
160 
.125 
170 
.125 
180 
,125 
190 
.125 
200 
.125 
210 
.125 
220 
.125 
230 
,125 
(a) Calculate her posterior distribution. 
11.3 The standard process for making a polymer has mean yield 35%. A chemical 
engineer has developed a modified process. He runs the process on 10 batches 
and measures the yield (in percent) for each batch. They are: 
38.7 
40.4 
37.2 
36.6 
35.9 
34.7 
37.6 
35.1 
37.5 
35.6 
Assume that yield is normal (p, u2) where the standard deviation u = 3 is 
known. 
(a) Use a normal (30, lo2) prior for p. Find the posterior distribution. 
(b) The engineer wants to know if the modified process increases the mean 
yield. Set this up as a hypothesis test stating clearly the null and alternative 
hypotheses. 
(c) Perform the test at the 5% level of significance. 
1 1.4 An engineer takes a sample of 5 steel I beams from a batch, and measures the 
amount they sag under a standard load. The amounts in mm are: 
5.19 
4.72 
4.81 
4.87 
4.88 
It is known that the sag is normal (p, u2) where the standard deviation u = .25 
is known. 

EXERCISES 
219 
Observation 
CRA 
(a) Use a normal (5, .52) prior for p. Find the posterior distribution. 
(b) For a batch of I beams to be acceptable, the mean sag under the standard 
load must be less than 5.20. ( p < 5.20). Set this up as a hypothesis test 
stating clearly the null and alternative hypotheses. 
(c) Perform the test at the 5% level of significance. 
11.5 New Zealand was the last major land mass to be settled by human beings. 
The Shag River Mouth in Otago (lower South Island), New Zealand, is one 
of the sites of early human inhabitation that New Zealand archeologists have 
investigated, in trying to determine when the Polynesian migration to New 
Zealand occurred and documenting local adaptations to New Zealand condi- 
tions. Petchey and Higham (2000) describe the Radiocarbon dating of well- 
preserved barracouta thyrsites atun bones found at the Shag River Mouth site. 
They obtained four acceptable samples, which were analyzed by the Waikato 
University Carbon Dating Unit. Assume that the conventional radiocarbon age 
(CRA) of a sample follows the normal (p, u2) distribution, where the standard 
deviation u = 40 is known. The observations are: 
1 
2 
3 
4 
1010 
lo00 
950 
1050 
Observation 
1 
2 
3 
4 
CRA 
1 
940 
1040 
910 
990 

220 
BAYESIAN INFERENCE FOR NORMAL MEAN 
(a) Use a normal (1000, 2002) prior for p. Find the posterior distribution 
(b) Find a 95% credible interval for p. 
(c) To find the 0, the calibrated date, the Stuiver, Reimer, Braziunas marine 
curve was used. We will approximate this curve with the linear function 
dPIYlr.. . >  
Y4). 
6 = 2203 - ,835 x /.L. 
Find the posterior distribution of 0 given y1, . . . , y4. 
(d) Find a 95% credible interval for 8, the calibrated date. 
Computer Exercises 
1 1.1 Use the Minitab macro NormDPmac to find the posterior distribution of the 
mean p when we have a random sample of observations from a normal (p, 02), 
where u2 is known, and we have a discrete prior for p. 
Suppose we have a random sample of n = 10 observations from a normal 
(p, u2) distribution where it is known o2 = 4. The random sample of obser- 
vations are: 
3.07 7.51 
5.95 
6.83 
8.80 
4.19 
7.44 
7.06 
9.67 
6.89 
We only allow that there are 12 possible values for p, 4.0, 4.5, 5.0, 5.5, 6.0, 
6.5, 7.0, 7.5, 8.0, 8.5, 9.0, and 9.5. If we don’t favor any possible value over 
another, so we give all possible values of p probability equal to A. 
The prior 
distribution is: 
P I  
S b )  
4.0 
4.5 
5.0 
5.5 
6.0 
6.5 
7.0 
7.5 
8.0 
8.5 
9.0 
9.5 
.083333 
.083333 
.083333 
.083333 
.083333 
.083333 
.083333 
.083333 
.083333 
.Of33333 
.083333 
.083333 

COMPUTER EXERCISES 
221 
Use NormDPmac to find the posterior distribution g(pIy1,. . . , ~10). Details 
for invoking NormDRmac are in Appendix 3. 
11.2 Suppose another 6 random observations come later. They are: 
6.22 
3.99 
3.67 
6.35 
7.89 
6.13 
Use NormDPmac to find the posterior distribution, where we will use the 
posterior after the first ten observations y1, . . . , ylo, as the prior for the next 
six observations ~ 1 1 , .  . . , yl6. 
1 1.3 Instead, combine all the observations together to give a random sample of size 
n = 16, and use NormDPmac to find the posterior distribution where we go 
back the original prior that had all the possible values equally likely. What do 
the results of the last two problems show us? 
1 1.4 Instead of thinking of a random sample of size n = 16, let’s think of the sample 
mean as a single observation from its distribution. 
(a) What is the distribution of jj? Calculate the observed value of jj? 
(b) Use NormDPrnac to find the posterior distribution g(p1jj). 
(c) What does this show us? 
1 1.5 We will use the Minitab macro NormNPmac to find the posterior distribution 
of the normal mean p when we have a random sample of size n from a 
normal (p, u’) distribution with known uz, and we use a normal (m, 
s2) 
prior for p. The normal family of priors is the conjugate family for normal 
observations. That means that if we start with one member of the family as the 
prior distribution, we will get another member of the family as the posterior 
distribution. It is especially easy; if we start with a normal (m, s2) prior, we 
get a normal (m’, ( s ’ ) ~ )  
posterior where (s’)’ and rn’ are given by 
and 
respectively. Suppose then = 15 observations from anormal ( p ,  u2 = 42) are: 
26.8 
26.3 
28.03 
28.5 
26.3 
31.9 
28.5 
27.2 
20.9 
27.5 
28.0 
18.6 
22.3 
25.0 
31.5 
Use NormNPmac to find the posterior distribution g(ply1,. . . , yls), where 
we choose a normal (m = 20, s2 = 52) prior for p. The details for invoking 

222 
5AYESlAN lNFERENCE FOR NORMAL MEAN 
NormNPmac are in Appendix 3. Store the likelihood and posterior in c3 and 
c4, respectively. 
(a) What are the posterior mean and standard deviation? 
(b) Find a 95% credible interval for p. 
1 1.6 Repeat part (a) with a normal (30,4’) prior, storing the likelihood and posterior 
in c5 and c6. 
1 1.7 Graph both posteriors on the same graph. What do you notice? What do you 
notice about the two posterior means and standard deviations? What do you 
notice about the two credible intervals for T? 
1 1.8 We will use the Minitab macro NormGCPmac to find the posterior distribution 
of the normal mean p when we have a random samples of size n of normal 
(p, 02) 
observations with known u2 = 22, and we have a general continuous 
prior for p. Suppose the prior has the shape given by 
for O < p < 3 ,  
for 3 < p < 5 ,  
g(p)= 
8 - p  
f o r 5 < p < 8 ,  
{ :  
for 
8 < p .  
Store the values of p and prior g(p) in column c 1 and c2, respectively. Suppose 
the random sample of size n = 16 is: 
4.09 
4.68 
1.87 
2.62 
5.58 
8.68 
4.07 
4.78 
4.79 
4.49 
5.85 
5.90 
2.40 
6.27 
6.30 
4.47 
(a) Use NormGCPmactodeterminetheposteriordistributiong(plIyl, . . . , 11s). 
(b) Use tintegral.mac to find the posterior mean and posterior standard devi- 
(c) Find a 95% credible interval for p by using tintegral.mac. 
Details for invoking NormGCPmac are in Appendix 3. 
ation of p. Details for invoking tintegral.mac are in Appendix 3. 

12 
Comparing 
Bayesian and Frequentist 
Inferences for Mean 
Making inferences about the population mean when we have a random sample from 
a normally distributed population is one of the most widely encountered situations 
in statistics. From the Bayesian point of view, the posterior distribution sums up our 
entire belief about the parameter, given the sample data. It really is the complete 
inference. However, from the frequentist perspective, there are several distinct types 
of inference that can be done: point estimation, interval estimation, and hypothesis 
testing. Each of these types of inference can be performed in a Bayesian manner, 
where they would be considered summaries of the complete inference, the posterior. 
In Chapter 9 we compared the Bayesian and frequentist inferences about the pop- 
ulation proportion T .  In this chapter we look at the frequentist methods for point 
estimation, interval estimation, and hypothesis testing about p, the mean of a normal 
distribution, and compare them with their Bayesian counterparts using frequentist 
criteria. 
12.1 COMPARING FREQUENTIST AND BAYESIAN POINT 
ESTIMATORS 
A frequentist point estimator for a parameter is a statistic that we use to estimate the 
parameter. The simple rule we use to determine a frequentist estimator for p is to use 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
223 

224 
COMPARING BAYESIAN AND FREQUENTIST lNFERENCES FOR MEAN 
the statistic that is the sample analog of the parameter to be estimated. So we use the 
sample mean 
In Chapter 9 we learned that frequentist estimators for unknown parameters are 
evaluated by considering their sampling distribution. In other words, we look at the 
distribution of the estimator over all possible samples. A commonly used criterion 
is that the estimator be unbiased. That is, the mean of its sampling distribution is 
the true unknown parameter value. The second criterion is that the estimator have 
small variance in the class of all possible unbiased estimators. The estimator that 
has the smallest variance in the class of unbiased estimators is called the minimum 
variance unbiased estimator and is generally preferred over other estimators from 
the frequentist point of view. 
When we have a random sample from a normal distribution, we know that the 
sampling distribution of 5 is normal with mean p and variance <. The sample mean, 
$j, turns out to be the minimum variance unbiased estimator of p. 
We take the mean of the posterior distribution to be the Bayesian estimator for p: 
to estimate the population mean p.l 
We know that the posterior mean minimizes the posterior mean square. This means 
that f i ~  
is the optimum estimator in the post-data setting. In other words, it is the 
optimum estimator for p given our sample data and using our prior. 
We will compare its performance to that of fif = 5 under the frequentist assump- 
tion that the true mean p is a fixed but unknown constant. The probabilities will be 
calculated from the sampling distribution of 5. In other words, we are comparing the 
two estimators for p in the pre-data setting. 
The posterior mean is a linear function of the random variable 5, so its expected 
value is 
The bias of the posterior mean is its expected value minus the true parameter value, 
which simplifies to 
U 2  
ns2 + a2 (m - 
The posterior mean is a biased estimator of p. The bias could only be 0 if our prior 
mean coincides with the unknown true value. The probability of that happening is 
0. The bias increases linearly with the distance the prior mean m is from the true 
unknown mean p. The variance of the posterior mean is 
ns2 
‘The maximum likelihood estimator is the value of the parameter that maximizes the likelihood function. 
It turns out that 
is the maximum likelihood estimator of /I for a normal random sample. 

COMPARING FREQUENTISTAND BAYESIAN POINT ESTIMATORS 
225 
990 
1000 
1010 
1020 
1030 
1040 
Figure 72.7 Biases of Arnold's, Beth's, and Carol's estimators. 
and is seen to be clearly smaller than $, which is the variance of the frequentist 
estimator b/ = 8. The mean squared error of an estimator combines both the bias 
and the variance into a single measure: 
hfS(fi,) = bias' + Var(fi). 
The frequentist estimator fi, = 0 is an unbiased estimator of p, so its mean 
squared error equals its variance: 
IS' 
n 
M S ( / l f )  = - . 
#en 
there is prior information, we will see that the Bayesian estimator has smaller 
mean squared error over the range of p values that are realistic. 
Example 21 Arnold, Beth, and Carol want to estimate the mean weight of "I kg" 
packages of milk powder produced at a dairy company. The weight in individual 
packages is subject to random variation. They know that when the machine is adjusted 
properly, the weights are normally distributed with mean 1015 grams, and standard 
deviation 5 g. They are going to base their estimate on a sample of size 10. Arnold 
decides to use a normal prior with mean 1000 g and standard deviation 10 g. Beth 
decides she will use a normal prior with mean I015 g and standard deviation 7.5 
g. Carol decides she will use a 'Iflat"prior. They calculate the bias, variance, and 
mean squared error of their estimators for various values of p to see how well they 
peqorm. 
Figure 12. I shows that only Carol's prior will give an unbiased Bayesian estimator. 
Her posterior Bayesian estimator corresponds exactly to the frequentist estimator 

226 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR MEAN 
4
7
 
2
4
 
I 
I 
I 
I 
I 
990 
1000 
1010 
1020 
1030 
1040 
Figure 12.2 Mean-squared errors of Arnold's, Beth's, and Carol's estimators. 
,hf = fj, since she used the 'j¶at"prior In Figure 12.2 we see the ranges over which 
the Bayesian estimators have smaller MS than thefrequentist estimator In that range 
they will be closer to the true value, on average, than the frequentist estimator The 
realistic range is the target mean (1015) plus or minus 3 standard deviations (5) 
which is from I000 to 1030. 
Although both Arnold and Beth's estimators are biased since they are using the 
Bayesian approach, they have smaller mean squared error over most of the feasible 
range than Carol's estimator (which equals the ordinary frequentist estimator). Since 
they have smaller mean squared errol; on average, they will be closer to the true 
value in most of the feasible range. In particular; Beth's estimator seems to offer 
substantially better performance over most of the feasible range, while Arnold's 
estimator offers somewhat better performance over the entire feasible range. 
12.2 COMPARING CONFIDENCE AND CREDIBLE INTERVALS FOR 
MEAN 
Frequentist statisticians compute confidence intervals for the parameter p to deter- 
mine an interval that "has a high probability of containing the true value." Since they 
are done from the frequentist perspective, the parameter p is considered a fixed but 
unknown constant. The coverage probability is found from the sampling distribution 
of an estimator, in this case jj, the sample mean. The sampling distribution of jj is 
normal with mean p and variance u2. We know before we take the sample that jj is 

COMPARING CONFIDENCE AND CREDIBLE INTERVALS FOR MEAN 
227 
a random variable, so we can make the probability statement about fj: 
where zs is the value from the standard normal table having tail area $. We rearrange 
this probability statement to have p in the middle. The upper inequality in the first 
statement becomes the lower inequality in the second statement, and vice versa: 
The endpoints of the interval are random because they depend on fj, which is the 
random variable in this interpretation. The parameter p is considered a fixed but 
unknown constant. So the correct interpretation is that (1 - a) x 100% of the 
intervals calculated this way will contain the true value. When we take our random 
sample and calculate g, there is nothing random left to attach a probability to. The 
actual interval we calculate either contains the true value or it does not. Only we 
don't know which is true. So we say that we are (1 - a) x 100% conJident that the 
interval we calculated using the observed value of fj, 
U 
f j h z q  x - 
J;;l 
(12.1) 
does contain the true value. Our confidence comes from the sampling distribution of 
the statistic. It does not come from the actual sample values we used to calculate the 
endpoints of the confidence interval. Sometimes we write the confidence interval as 
This contrasts with the Bayesian credible interval for p that we calculated in the 
previous chapter. The probability statement we make is from the posterior distribution 
of the parameter p given the sample data y1 . . . yn. It is conditional on the actual 
sample data we obtained. The probability given in the statement is our probability 
given the actual sample. It is a legitimate probability statement, since p is considered 
random. But it is subjective because we constructed it using our subjective prior. 
Someone else who started with a different prior would end up with a (slightly) 
different credible interval. 
Relationship between Frequentist Confidence Interval and Bayesian 
Credible Interval from "Flat" Prior 
With a flat prior for p, the posterior mean equals m' = fj, and the posterior vari- 
ance equals ( s ’ ) ~  = a2/n. So for this case the Bayesian credible interval and the 
frequentist confidence interval will both have the form 
" >  
( 
J;; 
2 J ; ;  
" 
g - 2 4  x - < p < f j + z z - x -  

228 
COMPARING EAYESlAN AND FREQUENTIST INFERENCES FOR MEAN 
However, they have different interpretations. 
The frequentist interpretation is that p is fixed. The endpoints of the random 
interval are calculated using a probability statement on the sampling distribution of 
the statistic 8. There is no randomness left after the actual sample data have been 
used to calculate the endpoints. No probability statements can be made about the 
actual calculated interval. The confidence level (1 - a) x 100% associated with the 
interval means that (1 - a) x 100% of the random intervals calculated this way will 
contain the true unknown parameter, so we are (1 - a) x 100% conjdent that the 
one we calculate does. 
The Bayesian interpretation lets p be a random variable, so probability statements 
are allowed. The credible interval is calculated from the posterior distribution given 
the actual sample data that occurred. The credible interval has the stated conditional 
probability of containing p, given the data. 
Scientists are not interested in what would happen with hypothetical repetitions 
of the experiment giving all possible data sets. The only data set that matters is 
the one that occurred. They find direct probability statements about the parameter, 
conditional on their actual data set to be the most useful. Scientists often take 
the confidence interval given by the frequentist statistician and misinterpret it as a 
probability interval for the parameter given the data. The statistician knows that this 
interpretation is not the correct one but lets the scientist make the misinterpretation. 
The correct interpretation is scientifically useless. 
Fortunately for frequentist statisticians, when they allow their clients to make 
the probability interpretation from the confidence interval for the mean of a normal 
distribution, p, they can get away with it. Their interval is equivalent to the Bayesian 
credible interval from a "flat" prior, which allows the probability interpretation in this 
case 
Example 20 (continued from Chapter 11) Previous studies have determined that 
the length of yearling trout has a normal (p, o2 = 22) distribution. Arnie, Barb, and 
Chuck obtained a random sample of 12 yearling trout. The sample mean 8 = 32 cm. 
The 95% conjdence interval for p is given by 
Compare this with the 95% credible intervals they found in Table 11.5. We see that 
it is the same as the credible interval Barb found because she used the 'Lflat"prio,: 
12.3 TESTING A ONE-SIDED HYPOTHESIS ABOUT A NORMAL MEAN 
Often we get data from a new population similar to a population we already know 
about. For instance, the new population may be the set of all possible outcomes of an 
experiment, where we have changed one of the experimental factors from its standard 
value to a new value. We know that the mean value of the standard population is 
po. We assume that each observation from the new population is normal (p, 02), 

TESTING A ONE-SIDED HYPOTHESIS ABOUT A NORMAL MEAN 
229 
where u2 is known, and that the observations are independent of each other. The 
question we want to answer is, Is the mean p for the new population greater than the 
mean of the standard population? A one-sided hypothesis test attempts to answer that 
question. We consider that there are two possible explanations to any discrepancy 
between the observed data and PO. 
1 .  The mean of the new population is less than or equal to the mean of the standard 
population, and any discrepancy is due to chance alone. 
2. The mean of the new population is greater than the mean of the standard 
Hypothesis testing is a way to protect our credibility by making sure that we don’t 
reject the first explanation unless it has probability less than our chosen level of 
significance a. Note that we set up the positive answer to the question we are asking 
as the alternative hypothesis. The null hypothesis will be the negative answer to the 
question. We will compare the frequentist and Bayesian approaches. 
population and at least part of the discrepancy is due to this fact. 
Frequentist One-sided Hypothesis Test about p 
As we saw in Chapter 9, frequentist tests are based on the sampling distribution of 
a statistic. This makes the probabilities pre-data in that they arise from all possible 
random samples that could have occurred. The steps are: 
1. 
2. 
3. 
4. 
5 .  
Set up the null and alternative hypothesis 
HO : p 5 PO versus H I  : P > P O .  
Note the alternative hypothesis is the change in the direction we are interested 
in detecting. Any change in the other direction gets lumped into the null 
hypothesis. (We are trying to detect p > po. If p < PO, it is not of any interest 
to us, so those values get included in the null hypothesis.) 
The null distribution of 0 is normal (PO, 
$). This is the sampling distribution 
of 0 when the null hypothesis is true. Hence the null distribution of the 
standardized variable 
- Po 
z = -  
OIJ;; 
will be normal (0,l). 
Choose a level of significance a. Commonly this is .lo, .05, or .01. 
Determine the rejection region. This is a region that has probability a when 
the null hypothesis is true (p = PO). When a = .05, the rejection region is 
z > 1.645. This is shown in Figure 12.3. 
Take the sample data and calculate g. If the value falls in the rejection region, 
we reject the hypothesis at level of significance a = .05; otherwise we can’t 
reject the null hypothesis. 

230 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR MEAN 
-3 
-2 
-1 
0 
1 
2 
3 
figure 12.3 
hypothesis test at 5% level of significance. 
Null distribution of z = p6 
with rejection region for one-sided frequentist 
6. Another way to perform the test is to calculate the p-value which is the proba- 
bility of observing what we observed, or something even more extreme, given 
the null hypothesis HO : p = po is true: 
(1 2.2) 
If p-value( a, then we reject the null hypothesis; otherwise we can’t reject it. 
Bayesian One-sided Hypothesis Test about p 
The posterior distribution g ( p / y I , .  . . , yn) summarizes our entire belief about the 
parameter, after viewing the data. Sometimes we want to answer a specific question 
about the parameter. This could be, Given the data, can we conclude the parameter 
p is greater than po? The value po ordinarily comes from previous experience. If 
the parameter is still equal to that value, then the experiment has not demonstrated 
anything new that requires explaining. We would lose our scientific credibility if we 
go around concocting explanations for effects that may not exist. The answer to the 
question can be resolved by testing 
HO : p 5 po versus H I  : p > po . 
This is an example of a one-sided hypothesis test. We decide on a level of significance 
a that we wish to use. It is the probability below which we will reject the null 

TESTING A ONE-SIDED HYPOTHESIS ABOUTA NORMAL MEAN 
237 
hypothesis. Usually a is small, for instance, .lo, .05, .01, .005, or .001. Testing 
a one-sided hypothesis in Bayesian statistics is done by calculating the posterior 
probability of the null hypothesis: 
PO 
P(H0 : p L POlY1,. . . I Yn) = 
dPlY1,. . . , Yn) +. 
(12.3) 
When the posterior distribution g(ply1,. . . , yn) is normal(m’, ( s ’ ) ~ ) ,  
this can easily 
be found from standard normal tables. 
L 
(12.4) 
where Z is a standard normal random variable. If the probability is less than our 
chosen a, we reject the null hypothesis and can conclude that p > po. Only then can 
we search for an explanation of why p is now larger than po. 
Example 20 (continued from Chapter 11) Ame, Barb, and Chuck read in a jour- 
nal that the mean length of yearling rainbow trout in a typical stream habitat is 31 
cm. The each decide to determine if the mean length of trout in the stream they are 
researching is greater than that by testing 
HO : p 5 31 versus HI : p > 31 
at the a = 5% level. For one-sided Bayesian hypothesis tests, they calculate the 
posterior probability of the null hypothesis. Amie and Barb have normal posteriors, 
so they use Equation 12.4. Chuck has a nonnormal posterior that he calculated 
numerically. He calculates the posterior probability of the null hypothesis using 
Equation 12.3, and he evaluates it numerically using the Minitab macro tintegral.mac. 
The results of the Bayesian hypothesis tests are shown in Table 12.1. 
They also decide that they will perform the corresponding frequentist hypothesis 
test of 
HO 
: p 5 31 versus H I  : p > 31 
and compare the results. The null distribution of z = 
and the correct rejection 
region are given in Figure 12.3. For this data, z = 32-31 = 1.732. This lies in 
the rejection region; hence the null hypothesis is rejected at the 5% level. The other 
way we could perform this frequentist hypothesis test is to calculate the p-value. For 
these data, 
2 l d E  
32 - 31 
2 / 4 5  
p-value = P ( Z  > 
= P ( Z  > 1.732), 

232 
COMPARING BAYESIAN AND FREQUENTIST /NF€RENC€S FOR MEAN 
Person 
Arnie 
Barb 
Chuck 
Table 12.1 Results of Bayesian one-sided hypothesis tests 
Posterior 
P(P I 
311~1,. . . , yn) 
normal(31.96, .57142) 
P(Z 5 31-31.96 
,5714 ) 
=.0465 
reject 
=.0416 
reject 
numerical 
.f-, 
s(ply1,. . . , Y,)@ 
=.0489 
reject 
normal(32.00, .57742) 
P(Z 5 =) 
31-32 
31 
which equals .0416from the standard normal table in Appendix B (Table B.2). This 
is less than the level of significance a, so the null hypothesis is rejected, same as 
before. 
12.4 TESTING A TWO-SIDED HYPOTHESIS ABOUT A NORMAL MEAN 
Sometimes the question we want to have answered is, Is the mean for the new 
population p the same as the mean for the standard population which we know equals 
PO? A two-sided hypothesis test attempts to answer this question. We are interested 
in detecting a change in the mean, in either direction. We set this up as 
Ho : p = po 
versus H1 : p # po . 
(12.5) 
The null hypothesis is known as a point hypothesis. This means that, it is true only 
for the exact value po. This is only a single point along the number line. At all the 
other values in the parameter space the null hypothesis is false. When we think of 
the infinite number of possible parameter values in an interval of the real line, we 
see that the it is impossible for the null hypothesis to be literally true. There are an 
infinite number of values that are extremely close to po but eventually differ from po 
when we look at enough decimal places. So rather than testing whether we believe 
the null hypothesis to actually be true, we are testing whether the null hypothesis is 
in the range that could be true. 
Frequentist Two-sided Hypothesis Test About p 
we are trying to detect a change in either direction. 
1. The null and alternative hypothesis are set up as in Equation 12.5. Note that 
*We note that in this case, the p-value equals Barb's probability of the null hypothesis because she used 
the "flat" prior. For the normal case, thep-value can be interpreted as the posterior probability of the null 
hypothesis when the noninformative "flat" prior was used. However, it is not generally true that p-value 
has any meaning in the Bayesian perspective. 

TESTING A TWO-SIDED HYPOTHESIS ABOUTA NORMAL MEAN 
233 
2. The null distribution of the standardized variable 
Y - PO 
o l f i  
z = -  
will be normal (0,l). 
3. Choose a, the level of significance. This is usually a low value such as .lo, 
.05, .01, or .001. 
4. Determine the rejection region. This is a region that has probability = a when 
the null hypothesis is true. For a two-sided hypothesis test, we have a two-sided 
rejection region. When cy = .05, the rejection region is IzI > 1.96. This is 
shown in Figure 12.4. 
5. Take the sample and calculate z = s, 
If it falls in the rejection region, 
reject the null hypothesis at level of significance a; otherwise we can’t reject 
the null hypothesis. 
6. Another way to do the test is to calculate the p-value which is the probability 
of observing what we observed, or something even more extreme than what 
we observed, given the null hypothesis is true. Note that the p-value includes 
probability of two tails: 
If p-value 5 cy, then we can reject the null hypothesis; otherwise we can’t 
reject it. 
Relationship between two-sided hypothesis test and confidence inter- 
val. We note that the rejection region for the two-sided test at level a is 
and this can be manipulated to give either 
We see that if we reject Ho : p = po at the level a, then po lies outside the 
(1 - a) x 100% confidence interval for p. Similarly, we can show that if we accept 
Ho : p = po at level a, then 10 lies inside (1 - a) x 100% confidence interval for 
p. So the confidence interval contains all those values of po that would be accepted 
if tested for. 

234 
COMPARING BAYESIAN AND FREQUENTIST /NF€R€NC€S FOR MEAN 
-3 
-2 
-1 
0 
1 
2 
3 
Figure 12.4 
hypothesis test at 5% level of significance. 
Null distribution of z = yfi 
with rejection region for two-sided frequentist 
Bayesian Two-sided Hypothesis Test about p 
If we wish to test the two-sided hypothesis 
Ho : p = po 
versus HI : p # po 
in a Bayesian manner, and we have a continuous prior, we can’t calculate the posterior 
probability of the null hypothesis as we did for the one-sided hypothesis. Since we 
have a continuous prior, we have a continuous posterior. We know that the probability 
of any specific value of a continuous random variable always equals 0. The posterior 
probability of the null hypothesis HO 
: p = po will equal zero. This means we can’t 
test this hypothesis by calculating the posterior probability of the null hypothesis and 
comparing it to a. 
Instead, we calculate a (1 - a) x 100% credible interval for p using our posterior 
distribution. If po lies inside the credible interval, we conclude that po still has 
credibility as a possible value. In that case we will not reject the null hypothesis 
HO : p = po, so we consider that it is credible that there is no effect. (However, 
we realize it has zero probability of being exactly true if we look at enough decimal 
places.) There is no need to search for an explanation of a nonexistent effect. 
However, if po lies outside the credible interval, we conclude that po does not 
have credibility as a possible value, and we will reject the null hypothesis. Then 
it is reasonable to attempt to explain why the mean has shifted from po for this 
experiment. 

MAIN POlNTS 
235 
Main Points 
0 When we have prior information on the values of the parameter that are realistic, 
we can find a prior distribution so that the mean of the posterior distribution of 
y (the Bayesian estimator) has a smaller mean squared error than the sample 
mean (the frequentist estimator) over the range of realistic values. This means 
that on the average, it will be closer to the true value of the parameter. 
0 A confidence interval for y is found by inverting a probability statement for jj, 
and then plugging in the sample value to compute the endpoints. It is called a 
confidence interval because there is nothing left to be random, so no probability 
statement can be made after the sample value is plugged in. 
0 The interpretation of a (1 - a) x 100% frequentist confidence interval for y is 
that (1 - a) x 100% of the random intervals calculated this way would cover 
the true parameter, so we are (1 - a) x 100% conjident that the interval we 
calculated does. 
0 A (1 - a) x 100% Bayesian credible interval is an interval such that the 
posterior probability it contains the random parameter is (1 - a) x 100%. 
0 This is more useful to the scientist because hehhe is only interested in hisher 
particular interval. 
0 The (1 - a) x 100% frequentist confidence interval for y corresponds to the 
(1 - a) x 100% Bayesian credible interval for p when we used the "flat prior." 
So, in this case, frequentist statisticians can get away with misinterpreting their 
confidence interval for p as a probability interval. 
0 In the general, misinterpreting a frequentist confidence interval as a probability 
interval for the parameter will be wrong. 
0 Hypothesis testing is how we protect our credibility, by not attributing an effect 
to a cause if that effect could be due to chance alone. 
0 If we are trying to detect an effect in one direction, say p > yo, we set this up 
as the one-sided hypothesis test 
HO : y I 
yo versus H I  : p > P O .  
Note that the alternative hypothesis contains the effect we wish to detect. The 
null hypothesis is that the mean is still at the old value (or is changed in the 
direction we aren't interested in detecting). 
0 If we are trying to detect an effect in either direction, we set this up as the 
two-sided hypothesis test 
HO : p = po 
versus 
HI : y # PO. 

236 
COMPARING BAYESIAN AND FREQUENTIST INFERENCES FOR MEAN 
The null hypothesis contains only a single value po and is called a point 
hypothesis. 
0 Frequentist hypothesis tests are based on the sample space. 
0 The level of significance a is the low probability we allow for rejecting the 
null hypothesis when it is true. We choose a. 
0 A frequentist hypothesis test divides the sample space into a rejection region, 
and an acceptance region such that the probability the test statistic lies in the 
rejection region if the null hypothesis is true is less than the level of significance 
a. If the test statistic falls into the rejection region we reject the null hypothesis 
at level of significance a. 
0 Or we could calculate the p-value. If the p-value< a, we reject the null 
hypothesis at level a. 
0 The p-value is not the probability the null hypothesis is true. Rather, it is the 
probability of observing what we observed, or even something more extreme, 
given the null hypothesis is true. 
0 We can test a one-sided hypothesis in a Bayesian manner by computing the 
posterior probability of the null hypothesis by integrating the posterior density 
over the null region. If this probability is less than the level of significance a, 
then we reject the null hypothesis. 
0 We cannot test a two-sided hypothesis by integrating the posterior probability 
over the null region because with a continuous prior, the prior probability of 
a point null hypothesis is zero, so the posterior probability will also be zero. 
Instead, we test the credibility of the null value by observing whether or not 
it lies within the Bayesian credible interval. If it does, the null value remains 
credible and we can’t reject it. 
Exercises 
12.1 A statistician buys a pack of 10 new golf balls, drops each golf ball from a 
height of one meter, and measures the height in centimeters it returns on the 
first bounce. The ten values are: 
79.9 
80.0 
78.9 
78.5 
75.6 
80.5 
82.5 
80.1 
81.6 
76.7 
Assume that y, the height (in cm) a golf ball bounces when dropped from a 
one-meter height, is normal ( p ,  a’), where the standard deviation a = 2. 
(a) Assume a normal (75, lo2) prior for p. Find the posterior distribution of 
(b) Calculate a 95% Bayesian credible interval for p. 
P. 

EXERCISES 
237 
(c) Perform a Bayesian test of the hypothesis 
HO : p 2 80 versus H I  : p < 80 
at the 5% level of significance. 
12.2 The statistician buys ten used balls that have been recovered from a water 
hazard. He drops each from a height of one meter and measures the height in 
centimeters it returns on the first bounce. The values are: 
73.1 
71.2 
69.8 
76.7 
75.3 
68.0 
69.2 
73.4 
74.0 
78.2 
Assume that y, the height (in cm) a golf ball bounces when dropped from a 
one-meter height, is normal (p, a2), where the standard deviation a = 2. 
(a) Assume a normal (75, lo2) prior for p. Find the posterior distribution of 
(b) Calculate a 95% Bayesian credible interval for p. 
(c) Perform a Bayesian test of the hypothesis 
k. 
HO : p 2 80 versus H I  : p < 80 
at the 5% level of significance. 
12.3 The local consumer watchdog group was concerned about the cost of elec- 
tricity to residential customers over the New Zealand winter months (Southern 
Hemisphere). They took a random sample of 25 residential electricity accounts 
and looked at the total cost of electricity used over the three months of June, 
July, and August. The costs were: 
514 
536 
345 
440 
427 
443 
386 
418 
364 
483 
506 
385 
410 
561 
275 
306 
294 
402 
350 
343 
480 
334 
324 
414 
296 
Assume that the amount of electricity used over the three months by aresidential 
account is normal (p, a2), where the known standard deviation 
= 80. 
(a) Use a normal (325,802) prior for p. Find the posterior distribution for 
(b) Find a 95% Bayesian credible interval for p. 
(c) Perform a Bayesian test of the hypothesis 
P. 
HO : p = 350 versus H I  : p # 350 
at the 5% level. 

238 
COMPARING BAYESIAN AND FR€QUENT/ST lNFERENCES FOR MEAN 
(d) Perform a Bayesian test of the hypothesis 
Ho : p 5 350 versus H I  : p > 350 
at the 5% level. 
12.4 A medical researcher collected the systolic blood pressure reading for a ran- 
dom sample of n = 30 female students under the age of 21 who visited the 
Student’s Health Service. The blood pressures are: 
120 
122 
121 
108 
133 
119 
136 
108 
106 
105 
122 
139 
133 
115 
104 
94 
118 
93 
102 
114 
123 125 
124 
108 
111 
134 
107 
112 
109 
125 
Assume that systolic blood pressure comes from a normal (p, cr2) distribution 
where the standard deviation cr = 12 is known. 
(a) Use a normal (120,15’) prior for p. Calculate the posterior distribution 
(b) Find a 95% Bayesian credible interval for p, 
(c) Suppose we had not actually known the standard deviation cr. Instead, 
the value 5 = 12 was calculated from the sample and used in place of the 
unknown true value. Recalculate the 95% Bayesian credible interval. 
of p. 

13 
Bayesian Inference for 
Difference Between Means 
Comparisons are the main tool of experimental science. When there is uncertainty 
present due to observation errors or experimental unit variation, comparing observed 
values can't establish the existence of a difference because of the uncertainty within 
each of the observations. Instead, we must compare the means of the two distributions 
the observations came from. In many cases the distributions are normal, so we are 
comparing the means of two normal distributions. There are two experimental 
situations that the data could arise from. 
The most common experimental situation is where there are independent random 
samples from each distribution. The treatments have been applied to different random 
samples of experimental units. The second experimental situation is where the 
random samples are paired. It could be that the two treatments have been applied 
to the same set of experimental units (at separate times). The two measurements on 
the same experimental unit cannot be considered independent. Or it could be that 
the experimental units were formed into pairs of similar units, with one of each pair 
randomly assigned to each treatment group. Again, the two measurements in the 
same pair cannot be considered independent. We say the observations are paired. 
The random samples from the two populations are dependent. 
In Section 12.1 we look at how to analyze data from independent random samples. 
If the treatment effect is an additive constant, we get equal variances for the two 
distributions. If the treatment effect is random, not constant, we get unequal variances 
for the two distributions. In Section 12.2 we investigate the case where we have 
independent random samples from two normal distributions with equal variances. In 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
239 

240 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
Section 12.3, we investigate the case where we have independent random samples 
from two normal distributions with unequal variances. In Section 12.4 we investigate 
how to find the difference between proportions using the normal approximation, 
when we have independent random samples. In Section 12.5 we investigate the case 
where we have paired samples. 
13.1 INDEPENDENT RANDOM SAMPLES FROM TWO NORMAL 
DISTRIBUTIONS 
We may want to determine whether or not a treatment is effective in increasing 
growth rate in lambs. We know that lambs vary in their growth rate. Each lamb 
in a flock is randomly assigned to either the treatment group or the control group 
that will not receive the treatment. The assignments are done independently. This is 
called a completely randomized design, and we discussed it in Chapter 2. The reason 
the assignments are done this way is that any differences among lambs enters the 
treatment group and control group randomly. There will be no bias in the experiment. 
On average, both groups have been assigned similar groups of lambs over the whole 
range of the flock. The distribution of underlying growth rates for lambs in each 
group is assumed to be normal with the same means and variances 02. 
The means 
and variances for the two groups are equal because the assignment is done randomly. 
The mean growth rate for a lamb in the treatment group, p1, equals the mean 
underlying growth rate plus the treatment effect for that lamb. The mean growth rate 
for a lamb in the control group, pz, equals the mean underlying growth rate plus 
zero, since the control group doesn’t receive the treatment. Adding a constant to a 
random variable doesn’t change the variance, so if the treatment effect is constant 
for all lambs, the variances of the two groups will be equal. We call that an additive 
model. If the treatment effect is different for different lambs, the variances of the two 
groups will be unequal. This is called a nonadditive model. 
If the treatment is effective, p l  will be greater than p2. In this chapter we will 
develop Bayesian methods for inference about the difference between means p1- pz 
for both additive and nonadditive models. 
13.2 CASE 1 : EQUAL VARIANCES 
We often assume the treatment effect is the same for all units. The observed value for 
a unit given the treatment is the mean for that unit plus the constant treatment effect. 
Adding a constant doesn’t change the variance, so the variance of the treatment group 
is equal to the variance of the control group. That sets up an additive model. 
When the Variance Is Known 
Suppose we know the variance u2. 
Since we know the two samples are independent 
of each other, we will use independent priors for both means. They can either be 

CASE 1: EQUAL VARIANCES 
241 
normal (ml, sf) and normal(m2, s;) 
priors, or we can usejat priors for one or both 
of the means. 
Because the priors are independent, and the samples are independent, the posteriors 
are also independent. The posterior distributions are 
and 
where the mi, (s;)~, mh, and (s;)~ are found using the simple updating formulas 
given by Equations 1 1.5 and 1 1.6. 
Since plIy11, . . . , ynl 1 and ~ 2 1 ~ 1 2 ,  
. . . , yn22 are independent of each other, we 
can use the rules for mean and variance of a difference between independent random 
variables. This gives the posterior distribution of p d  = p1 - p2, It is 
PLdIY11,. . . ,Ynll,Ylzr.~~, 
Yn22 N Normal(m&, (4J2), 
where m& = mi - mi, and (s&)’ = ( s : ) ~  
+ (s;)~). We can use this posterior 
distribution to make further inferences about the difference between means p1 - p2. 
Credible interval for difference between means, known equal variance 
case. The general rule for finding a (1 -a) x 100% Bayesian credible interval when 
the posterior distribution is normal (m‘, ( s ’ ) ~ )  
is to take theposterior mean i 
critical 
value x posterior standard deviation. When the observation variance (or standard 
deviation) is assumed known, the critical value comes from the standard normal table. 
In that case the (1 - a) x 100% Bayesian credible interval for p d  = p1 - p2 is 
m& f 
2% x s&. 
This can be written as 
(13.1) 
(13.2) 
Thus, given the data, the probability that p1 - p2 lies between the endpoints of the 
credible interval equals (1 - a) x 100%. 
Confidence interval for difference between means, known equal vari- 
ance Case. The frequentist confidence interval for pd = p1 - p2 when the two 
distributions have equal known variance is given by 
y1-82 i z q  x u  -+- c 
(13.3) 
This is the same formula as the Bayesian credible interval would be if we had 
used independent “flat“ priors for p1 and p ~ ,  
but the interpretations are different. 
The endpoints of the confidence interval are what is random under the frequentist 
viewpoint. (1 - a) x 100% of the intervals calculated using this formula would 

242 
BAYESIAN lNfERENCE FOR DIFFERENCE BETWEEN MEANS 
contain the fixed, but unknown, value 1-11 - 1-12. We would have that confidence that 
the particular interval we calculated using our data contains the true value. 
Example 22 In Example 3 (Chapter 3), we looked at two series of measurements 
Michelson made on the speed of light, in 1879 and 1882, respectively. The data are 
shown in Table 3.3. (The measurements are$gures given plus 299,000.) Suppose we 
assume each speed of light measurement is normally distributed with known standard 
deviation 100. Let us use independent normal (m, s2) priors for the 1879 and 1882 
measurements, where m = 300,000 and s2 = 5002. 
The posterior distributions of 111879 and PI882 can be found using the updating 
rules. For PI879 they give 
20 
+ - 
= .002004, 
1 
___- 
-- 
1 
(s;879)2 
5002 
loo2 
so (s;879)2 = 499, and 
1 
20 
5002 x 300000+ ___ 
1o02 
x (299000 + 909) = 299909. 
.002004 
mi879 = 
Similarly, for PI882 they give 
1 
23 
+ - 
= .002304, 
-- 
-- 
1 
(S ;~8 2 )~ 
5002 
100’ 
1 
23 
5002 x 300000+ 
~ 1o02 
x (299000 + 756) = 299757. 
.002304 
mi882 = 
The posterior distribution of Pd = 111879 - PI882 will be normal (m&, ( s & ) ~ )  where 
m& = 299909 - 299757 = 152 
and 
( s & ) ~  
= 499 + 434 = 30.52. 
The 95% Bayesian credible interval for P d  = 111879 - p1882 is 
152 i 
1.96 x 30.5 = (92.1,211.9). 
One-sided Bayesian hypothesis test. If we wish to determine whether or not 
the treatment mean 111 is greater than the control mean 1-12, we will use hypothesis 
testing. We test the null hypothesis 
HO : Pd 5 0 versus H1 : p d  > 0 ,  
where Pd = 1-11 - 1-12 is the difference between the two means. To do this test 
in a Bayesian manner, we calculate the posterior probability of the null hypoth- 
esis P(pd 5 Oldata) where data includes the observations from both samples 

CASE 1: EQUAL VARIANCES 
243 
~ 1 1 ,  
. . . , Ynll and ~ 1 2 ,  
. . . , y n 2 2 .  Standardizing by subtracting the mean and dividing 
by the standard deviation gives 
0-m& 
= P ( Z < - ) ,  
0-m& 
4 
(13.4) 
where Z has the standard normal distribution. We find this probability in Table B.2 
in Appendix B. If it is less than a, we can reject the null hypothesis at that level. 
Then we can conclude that p 1  is indeed greater than p 2  at that level of significance. 
Two-sided Bayesian hypothesis test. We can't test the two-sided hypothesis 
HO : ~1 - p2 = 0 versus HI : p1 - p 2  # 0 
in a Bayesian manner by calculating the posterior probability of the null hypothesis. 
It is a point null hypothesis since it is only true for a single value pd = p1 - p 2  = 0. 
When we used the continuous prior, we got a continuous posterior, and the probability 
that any continuous random variable takes on any particular value always equals 0. 
Instead, we use the credible interval for pd. If 0 lies in the interval, we cannot 
reject the null hypothesis and 0 remains a credible value for the difference between 
the means. However, if 0 lies outside the interval, then 0 is no longer a credible value 
at the significance level a. 
Example 22 (continued) The 95% Bayesian credible interval for pd = PI879 - 
p i 8 8 2  is (92.1,211.9). 0 lies outside the interval; hence we reject the null hypothesis 
that the means for the two measurement groups were equal and conclude that they are 
different. This shows that there was a bias in Michelson 'sfirst group of measurements, 
which was very much reduced in the second group of measurements, 
When the Variance Is Unknown and Flat Priors Are Used 
Suppose we use independent "flat" priors for p1 and p2. Then ( s ; ) ~  
= <, (s;)~ = 
0 2  
Credible interval for difference between means, unknown equal variance 
case. If we knew the variance u 2 ,  the credible interval could be written as 
- 
n 2 ,  mi = 
and mk = jj2. 
However, we don't know u 2 .  We will have to estimate it from the data. We can 
get an estimate from each of the samples. The best thing to do is to combine these 
estimates to get the pooled variance estimate 
(13.5) 

244 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
Since we used the estimated 5; instead of the unknown true variance u2, the credible 
interval should be widened to allow for the additional uncertainty. We will get the 
critical value from the Students t table with n1 + n2 - 2 degrees of freedom. The 
approximate (1 - a) x 100% Bayesian credible interval for p1 - p2 is 
81-82? i t q  xi?., 
-+-, 
Kz 
(1 3.6) 
where the critical value comes from the Students t table with n1 + n2 - 2 degrees 
of freedom.’ 
Confidence interval for difference between means, unknown equal vari- 
ance case. The frequentist confidence interval for p d  = p1 - p2 when the two 
distributions have equal unknown variance is 
(13.7) 
where the critical value again comes from the Students t table with n1 + TQ - 2 
degrees of freedom. The confidence interval has exactly the same form as the 
Bayesian credible interval when we use independent “flat“ priors for p1 and p2. Of 
course, the interpretations are different. 
The frequentist has (1 - a) x 100% confidence that the interval contains the true 
value of the difference because (1 - a) x 100% of the random intervals calculated 
this way do contain the true value. The Bayesian interpretation is that given the data 
from the two samples, the posterior probability the random parameter p1 - p2 lies 
in the interval is (1 - a). 
In this case the scientist who misinterprets the confidence interval for a probability 
statement about the parameter gets away with it, because it actually is a probability 
statement using independent j u t  priors. It is fortunate for frequentist statisticians 
that their most commonly used techniques (confidence intervals for means and pro- 
portions) are equivalent to Bayesian credible intervals for some specific prior.’ Thus 
a scientist who misinterpret hisher confidence interval as a probability statement, 
can do so in this case, but he/she is implicitly assuming independent flat priors. The 
’ Actually, we are treating the unknown uz as a nuisance parameter and are using an independent prior 
g(cr2) cx 5 for it. We find the marginal posterior distribution of p1 - p z  from the joint posterior of 
f i ~  - fiz and a by integrating out the nuisance parameter. The marginal posterior will be Student’s t 
with n1 + n 2  - 2 degrees of freedom instead of normal. This gives us the credible interval with the z 
critical value replaced by the t critical value. We see that our approximation gives us the correct credible 
interval for these assumptions. 
21n the case of a single random sample from a normal distribution, frequentist confidence intervals are 
equivalent to Bayesian credible intervals withjut prior for p. In the case of independent random samples 
from normal distributions having equal unknown variance a2, confidence intervals for the difference 
between means are equivalent to Bayesian credible intervals using independent flat priors for fi1 and fiz, 
dong with the improper prior g ( a )  o( 6-l for the nuisance parameter. 

CASE 2: UNEQUAL VARlANCES 
245 
only loss that the scientist will have incurred is helshe didn't get to use any prior 
information helshe may have had.3 
One-sided Bayesian hypothesis test. If we want to test 
HO : /Ld 5 O 
versus H I  : /Ld > O 
when we assume that the two random samples come from normal distributions having 
the same unknown variance uz, and we use the pooled estimate of the variance 6; in 
place of the unknown uz and assume independent "flat" priors for the means p 1  and 
p 2 ,  we calculate the posterior probability of the null hypothesis using Equation 13.4, 
but instead of finding the probability in the standard normal table, we find it from the 
Student's t distribution with n 1  + nz - 2 degrees of freedom. We could calculate it 
using Minitab or R. Alternatively, we could find values that bound this probability in 
the Student's t table. 
Two-sided Bayesian hypothesis test. When we assume that both samples 
come from normal distributions with equal unknown variance u2 and we use the 
pooled estimate of the variance 6; in place of the unknown variance u2 and assume 
independent "flat" priors, we can test the two-sided hypothesis 
HO : p~1 - pz = 0 versus H I  : p1 - 112 # 0 
using the credible interval for p1 - pz given in Equation 13.6. There are n 1 +  722 - 2 
degrees of freedom. If 0 lies in the credible interval, we cannot reject the null 
hypothesis, and 0 remains a credible value for the difference between the means. 
However, if 0 lies outside the interval, then 0 is no longer a credible value at the 
significance level a. 
13.3 CASE 2: UNEQUAL VARIANCES 
When the Variances Are Known 
In this section we will look at a nonadditive model, but with known variances. Let 
~ 1 1 , .  
. . , ynll be a random sample from normal distribution having mean p 1  and 
known variance 01. Let ~ 1 2 , .  . . yn22 be a random sample from normal distribution 
having mean pz and known variance 022. The two random samples are independent 
of each other. 
We use independent priors for p 1  and pz. They can be either normal priors or "flat" 
priors. Since the samples are independent and the priors are independent, we can find 
each posterior independently of the other. We find these using the simple updating 
formulas given in Equations 11.5 and 11.6. The posterior of p1 ly11, . . . , ynl 1 is 
3Frequentist techniques such as the confidence intervals used in many other situations do not have Bayesian 
interpretations. Interpreting the confidence interval as the basis for a probability statement about the 
parameter would be completely wrong in those situations. 

246 
BAYESIAN INFERENCE FOR DIFFfRENCE BETWEEN MEANS 
normal[mi, (s;)~]. 
The posterior of p z I y 1 2 , .  . . , gn22 is normal[m;, (s;)~]. The 
posteriors are independent since the priors are independent and the samples are 
independent. The posterior distribution of p d  = p1 - p2 is normal with mean equal 
to the diyerence of the posterior means, and variance equal to the sum of the posterior 
variances. 
(~dlYll,...rYnilrY12,’..,Yn2z) 
Normal[m&,(s&)2], 
where m& = mi - m; and ( s & ) ~  
= ( s : ) ~  + (~1)' 
Credible interval for difference between means, known unequal variance 
Case. A (1 -a) x 100% Bayesian credible interval for pd = 1.11 -pz, the difference 
between means is 
m & i z F  x (s&), 
(13.8) 
which can be written as 
m i - m i i z p  x d
w
.
 
(13.9) 
Note these are identical to Equations 13.1 and 13.2. 
Confidence interval for difference between means, known unequal vari- 
ance case. The frequentist confidence interval for p d  = p1 - p2 in this case 
would be 
(1 3.10) 
Note that this has the same formula as the Bayesian credible interval we would get if 
we had used flat priors for both p1 and p2. However, the intervals have very different 
interpretations. 
When the Variances Are Unknown 
When the variances are unequal and unknown, each of them will have to be estimated 
from the sample data 
These estimates will be used in place of the unknown true values in the simple 
updating formulas. This adds extra uncertainty. To allow for this, we should use the 
Student's t table to find the critical values. However, it is no longer straightforward 
what degrees of freedom should be used. Satterthwaite suggested that the adjusted 
degrees of freedom be 
rounded down to the nearest integer. 

CASE 2: UNEQUAL VARIANCES 
247 
Credible interval for difference between means, unequal unknown vari- 
ances. When we use the sample estimates of the variances in place of the true 
unknown variances in Equations 11.5 and 11.6, an approximate (1 - a )  x 100% 
credible interval for p d  = p1 - p2 is given by 
ml, - m: i 
t p  x K--- 
s:)~ + (s:)~, 
where we find the degrees of freedom using Satterthwaites adjustment. In the case 
where we use independent “flat“ priors for p1 and p2, this can be written as 
(13.1 1) 
Confidence interval for difference between means, unequal unknown 
variances. An approximate (1 - a) x 100% confidence interval for pd = p 1 -  pz 
is given by 
(13.12) 
We see this is the same form as the (1 - a) x 100% credible interval found when we 
used independent flat  prior^.^ However, the interpretations are different. 
Bayesian hypothesis test of Ho : p1 - p2 5 0 versus HI : p1 - p2 > 0. 
To test 
HO : p1 - p2 5 0 versus H1 : p1 - p2 > 0 
at the level a in a Bayesian manner, we calculate the posterior probability of the 
null hypothesis. We would use Equation 13.4. If the variances 0: and 02” are 
4Findingtheposteriordistributionofp1 - ~ ~ - - @ I - ~ z ) I ~ I I ,  
.. . , ~ ~ ~ l , y l z , .  
..,ynzzintheBayesian 
paradigm, or equivalently finding the sampling distribution of g1 - g2 - ( p ~  
- pz) in the frequentist 
paradigm when the variances are both unknown and not assumed equal has a long and controversial history. 
In the one-sample case, the sampling distribution of @ - p is the same as the posterior distribution of 
p - yiyl, . . . , yn when we use the flat prior for g(p) = 1 and the noninformative prior g($) 
o( 5 and 
marginalize u2 out of the joint posterior. This leads to the equivalence between the confidence interval 
and the credible interval for that case. Similarly, in the two-sample case with equal variances, the sampling 
distribution of g1 - v2 equals the posterior distribution of p1 - 1.12 lyll, . . . , ynl 1, y ~ z ,  
. . . , yn2z where 
we use flat priors for p1 and pz and the noninformative prior g ( u 2 )  o( 5, 
and marginalized u2 out of 
the joint posterior. Again, that led to the equivalence between the confidence interval and the credible 
interval for that case. One might be led to believe this pattern would hold in general. However, it doesn’t 
hold in the two sample case with unknown unequal variances. The Bayesian posterior distribution in this 
case is known as the Behrens-Fisher distribution. The frequentist distribution depends on the ratio of the 
unknown variances. Both of the distributions can be approximated by Srudents t with an adjustment made 
to the degrees of freedom. Satterthwaite suggested that the adjusted degrees of freedom be 
rounded down to the nearest integer. 

248 
BAYESIAN INFERENCE FOR DlFFERENCE BETWEEN MEANS 
known, we get the critical value from the standard normal table. However, when 
we use estimated variances instead of the true unknown variances, we will find the 
probabilities using the Student’s t distribution with degrees of freedom given by 
Satterthwaite’s approximation. If this probability is less than a, then we reject the 
null hypothesis and conclude that p1 > p2. In other words, that the treatment is 
effective. Otherwise, we can’t reject the null hypothesis. 
13.4 BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN TWO 
PROPORTIONS USING NORMAL APPROXIMATION 
Often we want to compare the proportions of a certain attribute in two populations. 
The true proportions in population 1 and population 2 are 
and q, respectively. 
We take a random sample from each of the populations and observe the number of 
each sample having the attribute. The distribution of y117r1 is binomial(n1, T I )  and 
the distribution of y2/7r2 is binomial( n2,7r2), and they are independent of each other 
We know that if we use independent prior distributions for 7r1 and 7r2, we will get 
independent posterior distributions. Let the prior for 7r1 be beta(a1, b l )  and for 7r2 be 
beta(a2, b2). The posteriors are independent beta distributions. The posterior for 7r1 
is beta(a;, b;), where a; = a1 + y1 and b; = bl + n1 - y1. Similarly the posterior 
for 7r2 is beta(ah, bh), where ah = a2 + y2 and bh = b2 + 722 - y2 
Approximate each posterior distribution with the normal distribution having same 
mean and variance as the beta. The posterior distribution of 7rd = T I  - 7r2 is 
approximately normal(m&, ( s & ) ~ )  
where the posterior mean is given by 
and the posterior variance is given by 
a;b; 
sib; 
+ 
1 2 -  
(‘dl - (a; + b;)2(ui + b’, + 1) 
(uh + bh)2(uh + bh + 1 )  ’ 
Credible interval for difference between proportions. We find the (1 - 
a) X 100% Bayesian credible interval for 7rd = 7r1 - 7r2 using the general rule for 
the (approximately) normal posterior distribution. It is 
m& i 
z? x s&. 
(13.13) 
One-sided Bayesian hypothesis test for difference between proportions. 
Suppose we are trying to detect whether T d  = 7r1 - 7r2 > 0. we set this up as a test 
of 
Ho : 7rd 5 0 versus H1 : 7rd > 0 .  
Note, the alternative hypothesis is what we are trying to detect. We calculate the 
approximate posterior probability of the null distribution by 

BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN TWO PROPORTIONS 
249 
(13.14) 
If this probability is less than the level of significance Q that we chose, we would 
reject the null hypothesis at that level and conclude 7r1 > 7r2. Otherwise, we can’t 
reject the null hypothesis. 
Two-sided Bayesian hypothesis test for difference between proportions. 
To test the hypothesis 
Ho : 7r1 - 7rz = 0 versus H I  : 7r1 - T Z  # 0 
in a Bayesian manner, check whether the null hypothesis value (0) lies inside the 
credible interval for r d  given in Equation 13.13. If it lies inside the interval, we 
cannot reject the null hypothesis HO : 7r1 - 7r2 = 0 at the level Q. If it lies outside 
the interval, we can reject the null hypothesis at the level Q and accept the alternative 
H1 : 
- 7r2 # 0. 
Example 23 The student newspaper wanted to write an article on the smoking habits 
of students. A random sample of 200 students (100 males and 100 females) between 
ages of 16 and 21 were asked about whether they smoked cigarettes. Out of the 100 
males, 22 said they were regular smokers, and out of the 100 females, 31 said they 
were regular smokers. The editor of the paper asked Donna, a statistics student, to 
analyze the data. 
Donna considered the male and female samples would be independent. Her prior 
knowledge was that a minority of students smoked cigarettes, so she decided to use 
independent beta(l,2) priors for 7rm and rif, the male and female proportions re- 
spectively. Her posterior distribution of 7rm will be beta(23,80), and her posterior 
distribution of 7rf will be beta(32,71). Hence, her posterior distribution of the differ- 
ence between proportions, 7rd = 7rm - 7rf, will be approximately normal(m&, ( ~ 2 ) ~ )  
where 
23 
32 
23+80 
32+71 
_ _ _ _ _ _ _  
m& = 
and 
23 * 80 
32 * 71 
(s’)2 
= (23 + 80)2 * (23 + 80 + 1) + (32 + 71)2 * (32 + 71 + 1) 
= .0612. 
Her 95% credible interval for 7rd will be (-,207, .032) which contains 0. She can’t 
reject the null hypothesis HO : 7rm - 7rf = 0 at the 5% level, so she tells the editor 
that the data does not conclusively show that there is any difference between the 
proportions of male and female students who smoke. 

250 
BAYESIAN lNFERENCE FOR DIFFERENCE BETWEEN MEANS 
13.5 NORMAL RANDOM SAMPLES FROM PAIRED EXPERIMENTS 
Variation between experimental units often is a major contributor to the variation in the 
data. When the two treatments are administered to two independent random samples 
of the experimental units, this variation makes it harder to detect any difference 
between the treatment effects, if one exists. 
Often designing a paired experiment makes it much easier to detect the difference 
between treatment effects. For a paired experiment, the experimental units are 
matched into pairs of similar units. Then one of the units from each pair is assigned 
to the first treatment, and the other in that pair is assigned the second treatment. This 
is a randomized block experimental design, where the pairs are blocks. We discussed 
this design in Chapter 2. For example, in the dairy industry, identical twin calves 
are often used for experiments. They are exact genetic copies. One of each pair 
is randomly assigned to the first treatment, and the other is assigned to the second 
treatment. 
Paired data can arise other ways. For instance, if the two treatments are applied to 
the same experimental units (at different times) giving the first treatment effect time 
to dissipate before the second treatment is applied. Or, we can be looking at "before 
treatment" and "after treatment" measurements on the same experimental units. 
Because of the variation between experimental units, the two observations from 
units in the same pair will be more similar than two observations from units in 
different pairs. In the same pair, the only difference between the observation given 
treatment A and the observation given treatment B is the treatment effect plus the 
measurement error. In different pairs, the difference between the observation given 
treatment A and the observation given treatment B is the treatment effect plus the 
experimental unit effect plus the measurement error. Because of this we cannot treat 
the paired random samples as independent of each other. The two random samples 
come from normal populations with means 
and p
~
,
 
respectively. The populations 
will have equal variances u2 when we have an additive model. We consider that the 
variance comes from two sources: measurement error plus random variation between 
experimental units. 
Take Differences within Each Pair 
Let yil be the observation from pair i given treatment A, and let yiz be the observation 
from pair i given treatment B. If we take the difference between the observations 
within each pair, di = yil - yiz, then these di will be a random sample from a 
normal population with mean p d  = 
- p
~
,
 
and variance u;. We can treat this 
(differenced) data as a sample from a single normal distribution and do inference 
using techniques found in Chapters 11 and 12. 
Example 24 An experiment was designed to determine whether a mineral supple- 
ment was effective in increasing annual yield in milk. Fifeen pairs of identical twin 
dairy cows were used as the experimental units. One cowfrom each pair was ran- 
domly assigned to the treatment group that received the supplement. The other cow 

NORMAL RANDOM SAMPLES FROM PAIRED EXPERIMENTS 
251 
Twin Set 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
Table 13.1 Milk annual yield 
Milk Yield: Control (liters) 
3525 
4321 
4763 
4899 
3234 
3469 
3439 
3658 
3385 
3226 
367 1 
3501 
3842 
3998 
4004 
Milk Yield: Treatment (liters) 
3340 
4279 
4910 
4866 
3125 
3680 
3965 
3849 
3297 
3124 
3218 
3246 
4245 
4186 
3711 
from the pair was assigned to the control group that did not receive the supplement. 
The annual yields are given in Table 13.1. Assume that the annual yields from cows 
receiving the treatment are normal (pt , u:), and that the annual yields from the cows 
in the control group are normal (pc, u,"). Aleece, Brad, and Curtis decided that since 
the two cows in the same pair share identical genetic background, their responses 
will be more similar than two cows that were from difSerent pairs. There is natural 
pairing. As the samples drawn from the two populations cannot be considered inde- 
pendent of each other; they decided to take differences di = yil - y ~ .  
The differences 
will be normal ( p d ,  u:), where p d  = pt - pc and we will assume that uz = 2702 is 
known. 
Aleece decided she would use a 'Fat" prior for b d .  Brad decided he would use 
a normal (m,s2) prior for p d  where he let m = 0 and s = 200. Curtis decided 
that his prior for p d  matched a triangular shape. He set up a numerical prior that 
interpolated between the heights given in Table 13.2 The shapes of the priors are 
shown in Figure 13.1. 
Aleece used a 'Jat" prior; so her posterior will be normal [m', (s')~] where 
m' = g = 7.07 and ( s ' ) ~  = 2702/15 = 4860. Her posterior standard deviation 
s' = 
= 69.71. Brad used a normal (0, 2002) prior; so his posterior will be 
normal [m', (s')~] where m' and s' are found by using Equations 11.5 and 11.6. 
l5 = 0.000230761, 
1 +- 
( s ' ) ~  
2002 
2702 
- 
- -  
- 
1 

252 
BAYESIAN lNFERENCE FOR DIFFERENCE BETWEEN MEANS 
Table 73.2 
interpolating between them. 
Curtis’ prior weights. The shape of his continuous prior is found by linearly 
300 
0 
I 
I 
I 
I 
I 
-500 
-250 
0 
250 
500 
Figure 13.1 The shapes of Aleece’s, Brad’s, and Curtis’ prior distributions. 
so his s = 65.83, and 
1 
15 
2002 
i702 
x 7.07 = 6.33 
+ .000230761 
m‘ = 
.000230761 
Curtis has tojnd his posterior numerically using Equation 11.3. He uses the Minitab 
macro NormGCPmac to do the numerical integration. The three posteriors are shown 
in Figure 13.2. 
They decided that to determine whether or not the treatment was effective in 
increasing the yield of milk protein, they would peiform the one-sided hypothesis test 
at the 95% level of signacance. Aleece and Brad had normal posteriors, so they used 
Equation 13.4 to calculate the posterior probability of the null hypothesis. Curtis 
had a numerical posterior; so he used Equation 12.3 and peiformed the integration 
using the Minitab macro tintegralmac. The results are shown in Table 13.3. 

MAIN POINTS 
253 
Person 
Posterior 
P(Pd 5 Ojdl,. . ., dn) 
Aleece nomal(7.07, 69.712) P ( Z  5 -) 
=.4596 
Brad 
nomal(6.33, 65.832) P ( Z  5 -) 
=.4619 
Curtis 
numerical 
J: 
g(Pdld1,. . . , d,)dp 
=.4684 
h 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
-500 -400 -300 -200 -1 00 0 100 200 300 400 500 
don’t reject 
don’t reject 
don’t reject 
Figure 73.2 Aleece’s, Brad’s, and Curtis’s posterior distributions. 
Main Points 
0 The difference between normal means are used to make inferences about the 
size of a treatment effect. 
0 Each experimental unit is randomly assigned to the treatment group or control 
group. The unbiased random assignment method ensures that both groups have 
similar experimental units assigned to them. On average, the means are equal. 
0 The treatment group mean is the mean of the experimental units assigned to 
the treatment group, plus the treatment effect. 
0 If the treatment effect is constant, we call it an additive model, and both sets 
of observations have the same underlying variance, assumed to be known. 
0 If the data in the two samples are independent of each other, we use independent 
priors for the two means. The posterior distributions ,u1 lyll, . . . , ynl 1 and 
~ 2 1 ~ 1 2 ,  
. . . , yn22 are also independent of each other and can be found using 
methods from Chapter 11. 

254 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
0 Letpd = p1-p2. Theposteriordistributionofpd/yll, 
. . . , ynl1, Y I Z , .  . . , yn2z 
is normal with mean m& = mi - m; and variance (s&)’ = (~1)’ + (s;)’ 
The (1 - a )  x 100% credible interval for p d  = p1 - p2 is given by 
0 If the variance is unknown, use the pooled estimate from the two samples. The 
credible interval will have to be widened to account for the extra uncertainty. 
This is accomplished by taking the critical values from the Student’s t table 
(with n1 + n2 - 2 degrees of freedom) instead of the standard normal table. 
0 The confidence interval for pdIy11,. . . , ynll, ~ 1 2 , .  
. . , yn22 is the same as the 
Bayesian credible interval where flat priors are used. 
0 If the variances are unknown, and not equal, use the sample estimates as if 
they were the correct values. Use the Student’s t for critical values, with the 
degrees given by Satterthwaite’s approximation. This is true for both credible 
intervals and confidence intervals. 
0 The posterior distribution for a difference between proportions can be found 
using the normal approximation. The posterior variances are known, so the 
critical values for credible interval come from standard normal table. 
0 When the observations are paired, the samples are dependent. Calculate the 
differences di = yzl - yz2 and treat them as a single sample from a normal 
(pd, o,”), 
where /.Ld = p1 - p2. Inferences about p d  are made using the single 
sample methods found in Chapters 11 and 12. 
Exercises 
13.1 The Human Resources Department of a large company wishes to compare 
two methods of training industrial workers to perform a skilled task. Twenty 
workers are selected: 10 of them are randomly assigned to be trained using 
method A, and the other 10 are assigned to be trained using method B. After 
the training is complete, all the workers are tested on the speed of performance 
at the task. The times taken to complete the task are: 

EXERClSES 
255 
Method A 
Method B 
115 
123 
120 
131 
111 
113 
123 
119 
116 
123 
121 
113 
118 
128 
116 
126 
127 
125 
129 
128 
(a) We will assume that the observations come from nOrmal(pA, a’) and 
nOrmd(pB, u’), where u’ = 62. Use independent normal (m, s2) prior 
distributions for p~ and p~g, 
respectively, where m = 100 and s’ = 20’. 
Find the posterior distributions of p~ and pg, respectively. 
(b) Find the posterior distribution of p~ - pug. 
(c) Find a 95% Bayesian credible interval for 
- p
~
.
 
(d) Perform a Bayesian test of the hypothesis 
Ho : P A  - p~ = 0 versus H I  : 
- p~g # 0 
at the 5% level of significance. What conclusion can we draw? 
13.2 A consumer testing organization obtained samples of size 12 from two brands 
of emergency flares and measured the burning times. They are: 

256 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
Brand A 
Brand B 
17.5 
13.4 
21.2 
9.9 
20.3 
14.4 
15.2 
19.3 
21.2 
19.1 
18.1 
14.6 
17.2 
18.8 
13.5 
11.3 
22.5 
14.3 
13.6 
15.2 
13.7 
8.0 
13.6 
11.8 
(a) We will assume that the observations come from n0rmd(pA,O2) and 
nonnal(p5, a2), where c2 = 32. Use independent normal (m, s2) prior 
distributions for p~ and p
~
,
 
respectively, where m = 20 and s2 = 82. 
Find the posterior distributions of p~ and pg, respectively. 
(b) Find the posterior distribution of p~ - p
~
.
 
(c) Find a 95% Bayesian credible interval for p~ - p~g. 
(d) Perform a Bayesian test of the hypothesis 
at the 5% level of significance. What conclusion can we draw? 
13.3 The quality manager of a dairy company is concerned whether the levels of 
butterfat in a product are equal at two dairy factories which produce the product. 
He obtains random samples of size 10 from each of the factories’ output and 
measures the butterfat. The results are: 

EXERCISES 
257 
Factory 1 
Factory 2 
16.2 
12.7 
14.8 
15.6 
14.7 
13.8 
16.7 
13.7 
16.8 
14.7 
16.1 
16.3 
14.0 
16.2 
15.2 
16.5 
14.4 
16.3 
16.9 
13.7 
(a) We will assume that the observations come from normal(p1, a’) and 
normal(p2, a’), wherea2 = 1.2’. Use independentnormal (m, s’) prior 
distributions for p1 and p2, respectively, where m = 15 and s2 = 4’. 
Find the posterior distributions of p1 and p2, respectively. 
(b) Find the posterior distribution of p1 - p2. 
(c) Find a 95% Bayesian credible interval for p1 - p2. 
(d) Perform a Bayesian test of the hypothesis 
Ho : p1 - p~ = 0 versus H1 : p1 - p2 # 0 
at the 5% level of significance. What conclusion can we draw? 
13.4 Independent random samples of ceramic produced by two different processes 
were tested for hardness. The results were: 
Process 1 
Process 2 
8.8 
9.6 
8.9 
9.2 
9.9 
9.4 
9.2 
10.1 
9.2 
9.5 
10.2 
9.5 
9.8 
9.5 
9.3 
9.2 
(a) We will assume that the observations come from normal(p1,a’) and 
normal(p2, 02), where u2 = .42. Use independent normal (m, s’) prior 

258 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
distributions for p1 and p2, respectively, where m = 10 and s2 = 1’. 
Find the posterior distributions of 1-11 and p2, respectively. 
(b) Find the posterior distribution of p1 - 1-12. 
(c) Find a 95% Bayesian credible interval for p1 - p2. 
(d) Perform a Bayesian test of the hypothesis 
Ho : p1 - p2 2 0 versus H I  : p1 - p2 < 0 
at the 5% level of significance. What conclusion can we draw? 
13.5 A thermal power station discharges its cooling water into a river. An environ- 
mental scientist wants to determine if this has adversely affected the dissolved 
oxygen level. She takes samples of water one kilometer upstream from the 
power station, and one kilometer downstream from the power station, and 
measures the dissolved oxygen level. The data are: 
Upstream 
Downstream 
10.1 
9.7 
10.2 
10.3 
13.4 
6.4 
8.2 
7.3 
9.8 
11.7 
8.9 
(a) We will assume that the observations come from normal(p.1, u2) and 
normal(p2, a2), 
where u2 = 22. Use independent normal (m, s2) prior 
distributions for 1-11 and p2, respectively, where m = 10 and s2 = 22. 
Find the posterior distributions of p1 and p2, respectively. 
(b) Find the posterior distribution of p1 - pz. 
(c) Find a 95% Bayesian credible interval for p1 - p2. 
(d) Perform a Bayesian test of the hypothesis 
Ho : p1 - p2 5 0 versus HI : p1 - p2 > 0 
at the 5% level of significance. What conclusion can we draw? 
13.6 Cattle, being ruminants, have multiple chambers in their stomaches. Stimu- 
lating specific receptors causes reflex contraction of the reticular groove and 
swallowed fluid then bypasses the reticulo-rumen and moves directly to the 
abomasum. Scientists wanted to develop a simple nonradioactive, noninva- 
sive test to determine when this occurs. In a study to determine the fate of 
swallowed fluids in cattle, McLeay, Carmthers, and Neil (1997) investigate a 

EXERCISES 
259 
C Administered into 
Reticulum 
carbon-13 (13C) octanoic acid breath test as a means of detecting a reticular 
groove contraction in cattle. Twelve adult cows were randomly assigned to 
two groups of 6 cows. The first group had 200 mg of 13C octanoic acid ad- 
ministered into the reticulum, and the second group had the same dose of 13C 
octanoic acid administered into the reticulo-osmasal orifice. Change in the 
enrichment of 13C in breath was measured for each cow 10 minutes later. The 
results are: 
13C Administered into 
Reticulo-omasal Orifice 
Cow ID 
X 
1 
Cow ID 
U 
8 
9 
10 
11 
12 
13 
1.5 
1.9 
0.4 
-1.2 
1.7 
0.7 
14 
15 
16 
17 
18 
19 
3.5 
4.7 
4.8 
4.1 
4.1 
5.3 
(a) Explain why the observations of variables x and y can be considered 
independent in this experiment. 
(b) Suppose the change in the enrichment of 13C for cows administered in the 
reticulum is normal (1-11, u?), 
where a? = 1.002. Use a normal (2, 22) 
prior for 1-11. Calculate the posterior distribution of ~ 1 1 x 8  . . . , 2 1 3 .  
(c) Suppose the change in the enrichment of 13C for cows administered in 
the reticulo-omasal orijice is normal (pz, ui), where u; = 1.402. Use 
a normal ( 2 ,  22) prior for 1-12. Calculate the posterior distribution of 
PilYi4.. . , Yi9. 
(d) Calculate the posterior distribution of pd = 1-11 - 1-12, the difference 
between the means. 
(e) Calculate a 95% Bayesian credible interval for pd. 
(0 Test the hypothesis 
Ho : 1-11 - p2 = 0 versus HI : 1-11 - 1-12 # 0 
at the 5% level of significance. What conclusion can be drawn. 
13.7 Glass fragments found on a suspect’s shoes or clothes are often used to connect 
the suspect to a crime scene. The index of refraction of the fragments are com- 
pared to the refractive index of the glass from the crime scene. To make this 
comparison rigorous, we need to know the variability the index of refraction 
is over a pane of glass. Bennet et al. (2002) analyzed the refractive index in a 
pane of float glass, searching for any spatial pattern. Here are samples of the 

Edge of Pane 
1.51996 
1.51997 
1.51998 
1.52000 
1.51998 
1.52004 
1.52000 
1.52001 
Middle of Pane 
1.52001 
1.52004 
1.52005 
1.52004 
1.52000 
1.5 1997 
I 
1.52004 
.5 1999 
.5 1997 
,52000 
.52002 
.5 1996 
For these data, 
g1 = 1.51999, 82 = 1.52001, u1 = .00002257, 
and 
(a) Suppose glass at the edge of the pane is normal (PI, a:), where 01 = 
.00003. Calculate the posterior distribution of 1-11 when you use a normal 
(1.52000, .O0Ol2) prior for 1-11. 
(b) Suppose glass in the middle of the pane is normal (1-12, u,"), where u2 = 
.00003. Calculate the posterior distribution of 1-12 when you use a normal 
(1.52000, .O0Ol2) prior for p2. 
(c) Find the posterior distribution of Pd = PI - 1-12. 
(d) Find a 95% credible interval for P d .  
(e) Perform a Bayesian test of the hypothesis 
~2 = .00003075. 
If0 : Pd = 0 versus H1 : Pd # 0 
at the 5% level of significance. 
The last half of the twentieth century saw great change in the role of women in New 
Zealand society. These changes included education, employment, family formation, 
and fertility, where women took control of these aspects of their lives. During those 
years, phrases such as "women's liberation movement" and "the sexual revolution" 
were used to describe the changing role of women in society. In 1995 the Population 
Studies Centre at the University of Waikato sponsored the New Zealand Women 
Family, Employment, and Education Survey (NZFEE) to investigate these changes. 
A random sample of New Zealand women of all ages between 20 and 59 was taken, 
and the women were interviewed about their educational, employment, and personal 
history. The details of this survey are summarized in Marsault et al. (1997). Detailed 
analysis of the data from this survey is in Johnstone et al. (2001). 
13.8 Have the educational qualifications of younger New Zealand women changed 
from those of previous generations of New Zealand women? To shed light on 
this question, we will compare the educational qualifications of two generations 
of New Zealand women 25 years apart. The women in the age group 25-29 at 
the time of the survey were born between 1966 and 1970. The women in the 
age group 50-54 at the time of the survey were born between 1941 and 1945. 

EXERClSES 
267 
(a) Out of 314 women in the age group 25-29, 234 had completed a sec- 
ondary school qualification. Find the posterior distribution of ~
1
,
 
the 
proportion of New Zealand women in that age group who have a com- 
pleted a secondary school qualification. (Use a uniform prior for TI.) 
(b) Out of 219 women in the age group 50-54, 120 had completed a sec- 
ondary school qualification. Find the posterior distribution of 7r2, the 
proportion of New Zealand women in that age group who have a com- 
pleted a secondary school qualification. (Use a uniform prior for nz.) 
(c) Find the approximate posterior distribution of 7r1 - 7r2. 
(d) Find a 99% Bayesian credible interval for 7r1 - 7r2. 
(e) What would be the conclusion if you tested the hypothesis 
HO : 7r1 - 7r2 = 0 versus H I  : 7r1 - 7rz # 0 
at the 1 % level of significance? 
13.9 Are younger New Zealand women more likely to be in paid employment than 
previous generations of New Zealand women? To shed light on this question, 
we will look at the current employment status of two generations of New 
Zealand women 25 years apart. 
(a) Out of 314 women in the age group 25-29, 171 were currently in paid 
employment. Find the posterior distribution of 7r1, the proportion of New 
Zealand women in that age group who are currently in paid employment. 
(Use a uniform prior for TI.) 
(b) Out of 219 women in the age group 50-54, 137 were currently in paid 
employment. Find the posterior distribution of 7 ~ 2 ,  the proportion of New 
Zealand women in that age group who are currently in paid employment. 
(Use a uniform prior for 7r2.) 
(c) Find the approximate posterior distribution of 7r1 - 7rz. 
(d) Find a 99% Bayesian credible interval for 7r1 - 7r2. 
(e) #at 
would be the conclusion if you tested the hypothesis 
Ho : 7r1 - 7r2 = 0 versus H I  : 7r1 - 7r2 # 0 
at the 1 % level of significance? 
13.10 Are younger New Zealand women becoming sexually active at an earlier 
age than previous generations of New Zealand women? To shed light on this 
question, we look at the proportions of New Zealand women who report having 
experienced sexual intercourse before age 18 for the two generations of New 
Zealand women. 
(a) Out of the 298 women in the age group 25-29 who responded to this 
question, 180 report having experienced sexual intercourse before reach- 
ing the age of 18. Find the posterior distribution of T I ,  the proportion 

262 
BAYESIAN INFERENCE FOR DIFFERENCE BETWEEN MEANS 
of New Zealand women in that age group who had experienced sexual 
intercourse before age 18. (Use a uniform prior for T I . )  
(b) Out of the 218 women in the age group 50-54 who responded to this 
question, 52 report having experienced sexual intercourse before reaching 
the age of 18. Find the posterior distribution of 7r2, the proportion of New 
Zealand women in that age group who had experienced sexual intercourse 
before age 18. (Use a uniform prior for ~ 2 . )  
(c) Find the approximate posterior distribution of 7r1 - 7r2. 
(d) Test the hypothesis 
in a Bayesian manner at the 1% level of significance. Can we conclude 
that New Zealand women in the generation aged 25-29 have experienced 
sexual intercourse at an earlier age than New Zealand women in the 
generation aged 50-54? 
13.1 1 Are younger New Zealand women marrying at a later age than previous gener- 
ations of New Zealand women? To shed light on this question, we look at the 
proportions of New Zealand women who report having been married before 
age 22 for the two generations of New Zealand women. 
(a) Out of the 314 women in the age group 25-29, 69 report having been 
married before the age 22. Find the posterior distribution of T I ,  the 
proportion of New Zealand women in that age group who have married 
before age 22. (Use a uniform prior for T I . )  
(b) Out of the 219 women in the age group 50-54, 1 14 report having been 
married before age 22. Find the posterior distribution of 7r2, the proportion 
of New Zealand women in that age group who have been married before 
age 22. (Use a uniform prior for ~ 2 . )  
(c) Find the approximate posterior distribution of 7r1 - 7 r ~ .  
(d) Test the hypothesis 
HO : 7r1 - 7r2 2 0 versus H1 : 7r1 - 7r2 < 0 
in a Bayesian manner at the 1% level of significance. Can we conclude 
that New Zealand women in the generation aged 25-29 have married at 
an earlier age than New Zealand women in the generation aged 50-54? 
13.12 Family formation patterns in New Zealand have changed over the time frame 
covered by the survey. New Zealand society has become more accepting of 
couples co-habiting (living together before or instead of legally marrying). 
When we take this into account, are younger New Zealand women forming 
family-like units at a similar age to previous generations? 

EXERCISES 
263 
(a) Out of the 314 women in the age group 25-29,199 report having formed 
a domestic partnership (either co-habiting or legal marriage) before age 
22. Find the posterior distribution of T I ,  the proportion of New Zealand 
women in that age group who have formed a domestic partnership before 
age 22. (Use a uniform prior for TI.) 
(b) Out of the 219 women in the age group 50-54,116 report having formed 
a domestic partnership before age 22. Find the posterior distribution of 
IQ, the proportion of New Zealand women in that age group who have 
formed a domestic partnership before age 22. (Use a uniform prior for 
T2.1 
(c) Find the approximate posterior distribution of 7r1 - 7r2. 
(d) Find a 99% Bayesian credible interval for TI - 7r2. 
(e) What would be the conclusion if you tested the hypothesis 
Ho : 7r1 - 7r2 = 0 versus H1 : 7r1 - 7r2 # 0 
at the 1 % level of significance. 
13.13 Are young New Zealand women having their children at a later age than 
previous generations? 
(a) Out of the 314 women in the age group 25-29, 136 report having given 
birth to their first child before the age of 25. Find the posterior distribution 
of T I ,  the proportion of New Zealand women in that age group who have 
given birth before age 25. (Use a uniform prior for TI.) 
(b) Out of the 219 women in the age group 50-54, 135 report having given 
birth to their first child before age 25. Find the posterior distribution of 
7r2, the proportion of New Zealand women in that age group who have 
given birth before age 25. (Use a uniform prior for 7r2.) 
(c) Find the approximate posterior distribution of 7r1 - ~
2
.
 
(d) Test the hypothesis 
Ho : 7r1 - 7r2 2 0 versus H I  : 7r1 - 7r2 < 0 
in a Bayesian manner at the 1% level of significance. Can we conclude 
that New Zealand women in the generation aged 25-29 have had their 
first child at a later age than New Zealand women in the generation aged 
50-54? 
13.14 Previous research has suggested that the childhood circumcision of males may 
be a protective factor against the acquisition of sexually transmitted infections 
(STI). Fergusson et. al. (2006) relate the circumcision status and self reported 
STI history using data from 25-year longitudinal study of a cohort of New 
Zealand children, known as the Christchurch Health and Development Study. 

264 
BAYESIAN INFERENCE FOR DlffERENCE BETWEEN MEANS 
Cow ID 
(a) Out of 356 non-circumcised males, 37 reported having had at least one 
STI by age 25. Find the posterior distribution of TI, the probability a non- 
circumcised male reports at least one STI by age 25. (Use a beta(1,lO) 
prior for T I . )  
(b) Out of the 154 circumcised males, 7 reported having at least one STI by 
age 25. Find the posterior distribution of 7r2, the probability a circumcised 
male reports at least one STI by age 25. (Use a beta(1,lO) prior for ~ 2 . )  
(c) Find the approximate posterior distribution of TI - T Z .  
(d) Test the hypothesis 
13C Administered into 
13C Administered into 
Reticulum 
Reticulo-omasal Orijice 
X 
II 
1 
2 
3 
4 
5 
6 
7 
1.1 
3.5 
0.8 
3.6 
1.7 
5.1 
1.1 
5.6 
2.0 
6.2 
1.6 
6.5 
3.1 
8.3 

EXERCISES 
265 
(e) Test the hypothesis 
HO : p d  = 0 versus H I  : F d  # 0 
at the 5% level of significance. What conclusion can be drawn? 
13.16 One of the advantages of Bayesian statistics is that evidence from different 
sources can be combined. In Exercise 6 and Exercise 14, we found posterior 
distributions of p d  using data sets from two different experiments. In the 
first experiment, the two treatments were given to two sets of cows, and the 
measurements were independent. In the second experiment, the two treatments 
were given to a third set of cows at different times and the measurements were 
paired. When we want to find the posterior distribution given data sets from 
two independent experiments, we should use the posterior distribution after the 
first experiment as the prior distribution for the second. 
(a) Explain why the two data sets can be considered independent. 
(b) Find the posterior distribution of PdldUta where the data include all of 
(c) Find a 95% credible interval for pd based on all the data. 
(d) Test the hypothesis 
the measurements 28 . . . 2 1 3 ,  y14 . . . 919, d l ,  . . . , d7. 
HO : /Ad = O 
VWSUS H1 : p d  # O 
at the 5% level of significance. Can we conclude that 13C octanic acid 
breath test is effective in detecting reticular groove contraction in cattle? 

This Page Intentionally Left Blank

I4 
Bayesian Inference for 
Simple Linear Regression 
Sometimes we want to model a relationship between two variables, 
and y. We 
might want to find an equation that describes the relationship. Often we plan to use 
the value of z to help predict y using that relationship. 
The data consist of n ordered pairs of points (xi, 
y,) for i = 1, . . . , n. We think 
of z as the predictor variable (independent variable) and consider that we know it 
without error. We think y is a response variable that depends on x in some unknown 
way, but that each observed y contains an error term as well. We plot the points on a 
two-dimensional scatterplot; the predictor variable is measured along the horizontal 
axis, and the response variable is measured along the vertical axis. 
We examine the scatterplot for clues about the nature of the relationship. To 
construct a regression model, we first decide on the type of equation that appears to 
fit the data. A linear relationship is the simplest equation relating two variables. This 
would give a straight line relationship between the predictor x and the response y. 
We leave the parameters of the line, the slope P, and the y-intercept a0 unknown, so 
all lines are possible. 
Then we determine the best estimates of the unknown parameters by some crite- 
rion. The criterion that is most frequently used is least squares. This is where we 
find the parameter values that minimize the sum of squares of the residuals, which 
are the vertical distances of the observed points to the fitted equation. We do this for 
the simple linear regression in Section 14.1. In Section 14.2 we look at how an ex- 
ponential growth model can be fitted using least squares regression on the logarithm 
of the response variable. 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
267 

268 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
Figure 74.7 
The third line is the least squares line. It minimizes the sum of squares of the residuals. 
Scatterplot with three possible lines, and the residuals from each of the lines. 
At this stage no inferences are possible because there is no probability model for 
the data. In Section 14.3 we construct a regression model that makes assumptions 
on how the response variable depends on the predictor variable and how randomness 
enters the data. Inferences can be done on the parameters of this model. In Section 
14.4 we fit a linear relationship between the two variables using Bayesian methods, 
and perform Bayesian inferences on the parameters of the model. In Section 14.5 
we determine the predictive distribution of yn+l, the next observation, given the data 
and z,+1, 
the value of the predictor variable for the next observation. 
14.1 LEAST SQUARES REGRESSION 
We could draw any number of lines on the scatterplot. Some of them would fit the 
data points fairly well, others would be extremely far from the points. A residual is 
the vertical distance from an observed point on the scatterplot to the line. We can put 
in any line that we like and then calculate the residuals from that line. Least squares 
is a method for finding the line that best fits the points in terms of minimizing sum 
of squares of the residuals. Figure14.1 shows a scatterplot, three possible lines, and 
the residuals from each line. 
The equation of a line is determined by two things: its slope /3 and its y-intercept 
ao. Actually its slope and any other point on the line will do, for instance, a?, the 
intercept of the vertical line at f. Finding the least squares line is equivalent to finding 
its slope and the y-intercept (or another intercept). 
The Normal Equations and the Least Squares Line 
The sum of squares of the residuals from line y = 00 + pz is 
n 
To find values of a0 and /3 that minimize SS,,, using calculus, take derivatives 
with respect to each a0 and /3 and set equal to 0, and solve the resulting set of 
simultaneous equations. First, take the derivative with respect to intercept ao. This 

LEAST SQUARES REGRESSION 
269 
gives the equation, 
which simplifies to 
n XYi - kao - e ~ x i  
= 0 
i= 1 
i= 1 
i= 1 
and further to 
Second, taking the derivative with respect to the slope p gives the equation 
g - a0 - p% = 0 .  
which simplifies to 
n 
n 
n 
2 = 1  
2 = 1  
i=l 
- 
and further to 
- 
xy - aoz - px2 = 0. 
(14.1) 
(14.2) 
Equation 14.1 and Equation 14.2 are known as the normal equations. Here normal 
refers to right angles' and has nothing to do with the normal distribution. Solve 
Equation 14.1 for a0 in terms of p and substitute into Equation 14.2 and solve for 
- 
- 
xy - (g - pqz - px2 = 0 .  
The solution is the least squares slope2 - 
xy - 5.9 
B=- 
x2 - 5.2 
(14.3) 
Note that it is very important that you do not round off when calculating the least 
squares slope using Equation 14.3. Both the numerator and denominator are differ- 
ences, and rounding off will lead to substantial error in the slope estimate! Substitute 
B back into Equation 14.1 and solve for the least squares y-intercept, 
A0 = g - B Z .  
(14.4) 
Again, it is important that you do not round off when calculating the least squares 
intercept using Equation 14.4. The equation of the least squares line is 
y = A0 + Bx. 
(14.5) 
'Least squares finds the projection of the (n-dimensional) observation vector onto the plane containing all 
possible values of (a0 , p) . 
2There are many different formulas for the least squares slope. This can be a source of confusion because 
many books give formulas that look quite dissimilar. However, all can be shown to be equivalent. I use 
this one because it is easy to remember: the average of x x y minus the average of x x the average of y 
all divided by the average of x 2  minus the square of the average of 2. 

270 
BAYESIAN INFERENCE FOR SlMPLE LINE4 R REGRESSlON 
Alternative form for the least squares line. The slope and any other point 
besides y-intercept also determines the line. Say the point is Az, where the least 
squares line intercepts the vertical line at it: 
A, =A,+Bit=g. 
Thus the least squares line goes through the point (it, g), An alternative equation for 
the least squares line is 
y = Az + B ( z  - 5 )  = y +  B(z -it), 
(14.6) 
which is particularly useful 
Estimating the Variance around the Least Squares Line 
The estimate of the variance around the least squares line is 
which is the sum of squares of the residuals divided by n - 2. The reason we use n - 2 
is that we have used two estimates, Az and B in calculating the sum of  square^.^ 
Example 25 A company is manufacturing a food product, and must control the 
moisture level in theJinal product. It is cheaper (and hence preferable) to measure 
the level at an in-process stage rather than in theJinalproduct. Michael, the company 
statistician, recommends to the engineers running the process that a measurement 
of the moisture level at an in-process stage may give a good prediction of what the 
jnal moisture level will be. He organizes the collection of data from 25 batches, 
giving the moisture level at the in-process stage and the final moisture level for each 
batch. These are shown in the first three columns of Table 14.1. Summary statistics 
for these data are: it = 14.3888, g = 14.2208, 2 
= 207.0703, y" = 202.3186, 
and 
= 204.6628. Note that he needs to keep all the signiJicantJigures in the 
squared terms. The formula for B uses subtraction, and if he rounds off too early, 
the difSerences will have too few signijcantfigures and accuracy will be lost. 
He then calculates the least squares line relating thejnal moisture level to the 
in-process moisture level. The slope is given by 
- 
XY - Zjj 
B = -  
- 204.6628 - 14.3888 x 14.2208 
.0425690 - 1.29963 
- 
- 
2 2  - (3)2 
207.0703 - (14.3888)2 
,0327546 
The equation of the least squares line is 
y = 14.2208 + 1.29963 x (X - 14.3888). 
3The general rule for finding an unbiased estimate of the variance is that the sum of squares is divided 
by the degrees of freedom, and we lose a degree of freedom for every estimated parameter in the sum of 
squares formula. 

LEAST SQUARES REGRESSlON 
271 
Table 14.7 In-process and final moisture levels 
Batch 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
Mean 
__ 
In-Process 
Level 
X 
14.36 
14.48 
14.53 
14.52 
14.35 
14.3 1 
14.44 
14.23 
14.32 
14.57 
14.28 
14.36 
14.50 
14.52 
14.28 
14.13 
14.54 
14.60 
14.86 
14.28 
14.09 
14.20 
14.50 
14.02 
14.45 
14.3888 
Final 
LS Fits 
Level 
13.84 
14.41 
14.22 
14.63 
13.95 
14.37 
14.41 
13.99 
13.89 
14.59 
14.32 
14.31 
14.43 
14.44 
14.14 
13.90 
14.37 
14.34 
14.78 
13.76 
13.85 
13.89 
14.22 
13.80 
14.67 & 
14.1833 
14.3392 
14.4042 
14.3912 
14.1703 
14.1183 
14.2872 
14.0143 
14.13 13 
14.4562 
14.0793 
14.1833 
14.3652 
14.3912 
14.0793 
13.8843 
14.4172 
14.4952 
14.8331 
14.0793 
13.8324 
13.9753 
14.3652 
13.74 14 
14.3002 
Residual 
Y - Y  
-0.343256 
0.070792 
-0.1841 88 
0.238808 
-0.220260 
0.25 1724 
0.122776 
-0.024308 
-0.241272 
0.133828 
0.2407 12 
0.126744 
0.064800 
0.048808 
0.0607 1 2 
0.015652 
-0.047184 
-0.155 160 
-0.053056 
-0.319288 
0.017636 
-0.085320 
-0.145200 
0.058608 
0.369780 
Residual’ 
(Y - YI2 
0.117825 
0.0050 1 2 
0.033925 
0.057029 
0.0485 14 
0.063365 
0.015074 
0.000591 
0.058212 
0.017910 
0.057942 
0.0 1 6064 
0.004199 
0.002382 
0.003686 
0.000245 
0.002226 
0.024075 
0.0028 15 
0.101945 
0.0003 1 1 
0.007280 
0.02 1083 
0.003435 
0.136737 
The scatterplot ofjinal moisture level and in-process moisture level together with the 
least squares line is given in Figure 14.2. 
+ B(xi - Z), the residuals, 
and the squared residuals. They are in the last three columns of Table 14.1. The 
He calculates the least squaresjitted values yi = 

272 
BAYESIAN INFEFlENCE FOR SIMPLE LINEAR REGRESSION 
14.0 14.1 14.2 14.3 14.4 14.5 14.6 14.7 14.8 14.9 
Fjgufe 74.2 Scatterplot and least squares line for the moisture data. 
estimated variance about the least squares line is 
To 3rd the estimated standard deviation about the least squares line, he takes the 
square root: 
8 = d
(
m
 
= 0.18672. 
14.2 EXPONENTIAL GROWTH MODEL 
When we look at economic time series, the predictor variable is time t, and we want to 
see how some response variable u depends on t. Often, when we graph the response 
variable versus time on a scatterplot, we notice two things. First, the plotted points 
seem to go up not at a linear rate but at a rate that increases with time. Second, the 
variability of the plotted points seems to be increasing at about the same rate as the 
response variable. This will be shown more clearly if we graph the residuals versus 
time. In this case the exponential growth model will usually give a better fit: 
= p o + P x t  
We note that if we let y = log,(u), then 
Y = Qo + p  x t 

SlMfLE LlNEAR REGRESSlON ASSUMPTIONS 
273 
Year Poultry Production 
Linear 
1987 
44,085 
47,757 
10.7739 
10.7776 
1988 
5 1,646 
48,725 
10.8522 
10.8393 
1989 
57,241 
53,364 
10.9550 
10.9010 
1990 
56,261 
58,004 
10.9378 
10.9628 
1991 
58,257 
62,643 
10.9726 
11.0245 
1992 
60,944 
67,283 
11.0177 
11.0862 
1993 
68,214 
7 1,922 
11.1304 
11.1479 
1994 
74,037 
76,562 
11.2123 
11,2097 
1995 
88,646 
81,201 
11.3924 
11.2714 
1996 
86,869 
85,841 
11.3722 
11.3331 
1997 
86,534 
90,480 
11.3683 
11.3949 
1998 
95,682 
95,120 
11.4688 
11.4566 
1999 
97,400 
99,759 
11.4866 
1 1.5 183 
2000 
10,4927 
104,398 
11.5610 
11.5801 
200 1 
11,4010 
109,038 
11.6440 
11.6418 
t 
U 
Fitted Value 
log,(u) 
Fitted log, u 
Table 74.2 Annual poultry production in New Zealand 
Exponential 
Fitted Value 
47,934 
50,986 
54,232 
57,686 
61,359 
65,266 
69,421 
73,842 
78,543 
83,545 
88,864 
94,522 
100,541 
106,943 
113,752 
is a linear relationship. We can estimate the parameters of the relationship using least 
squares using response variable y. The fitted exponential growth model is 
= e A o + B ~ t  
where B and A0 are the least squares slope and intercept for the logged data. 
Example 26 The annual New Zealand poultry production (in tonnes) for the years 
1987-2001 is given in Table 14.2. 
The scatterplot showing the residuals and least squares line is shown in Figure 
14.3. We see that the residuals are mostly positive at the ends of the data, and mostly 
negative in the center: This indicates that an exponential growth model would give a 
betterfit. The scatterplot, and the exponential growth model found by exponentiating 
the least squares line to the logged data are shown in Figure 14.4. 
14.3 SIMPLE LINEAR REGRESSION ASSUMPTIONS 
The method of least squares is nonparametric or distribution free, since it makes no 
use of the probability distribution of the data. It is really a data analysis tool and can 

274 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
I 
I 
I 
1990 
1995 
2000 
Figure 74.3 Scatterplot and least squares line for the poultry production data. 
be applied to any bivariate data. We can’t make any inferences about the slope and 
intercept nor about any predictions from the least squares model, unless we make 
some assumptions about the probability model underlying the data. The simple linear 
regression assumptions are: 
1. Mean assumption. The conditional mean of y given x is an unknown linear 
function of x. 
PLylz = QO + Pa: 1 
where p is the unknown slope and QO is the unknown y intercept, the intercept 
of the vertical line x = 0. In the alternate parameterization we have 
Pylz = Q P  + P(x - z) , 
where aZ is the unknown intercept of the vertical line z = 2. In this parame- 
terization the least squares estimates AP = jj and B will be independent under 
our assumptions, so the likelihood will factor into a part depending on QZ 
and a part depending on P. This greatly simplifies things, so we will use this 
parameterization. The mean assumption is shown in the first graph of Figure 
14.5. 
2. Error assumption. Observation equals mean plus error, which is normally 
distributed with mean 0 and known variance cr2. All errors have equal variance. 
The equal variance assumption is shown in the second graph of Figure 14.5. 

SIMPLE LlNEAR REGRESSlON ASSUMPTIONS 
275 
I 
I 
1990 
1995 
2000 
Figure 74.4 
data. 
Scatterplot and fitted exponential growth model for the poultry production 
Figure 74.5 Assumptions of linear regression model. The mean of Y given X is a linear 
function. The observation errors are normally distributed with mean 0 and equal variances. 
The observations are independent of each other. 
3. Independence assumption. The errors for all of the observations are indepen- 
dent of each other. The independent draw assumption is shown in the third 
graph of Figure 14.5. 
Using the alternate parameterization we obtain 
where cllz is the mean value for y given z = 3, and /3 is the slope. Each ei is normally 
distributed with mean 0 and known variance uz. The ei are all independent of each 
other. Therefore yilzi is normally distributed with mean aE + p(xi - 3) and variance 
uz and all the yilzi are all independent of each other. 

276 
BAYESlAN lNFERENCE FOR SlMPLE LlNEAR REGRESSlON 
14.4 BAYES' THEOREM FOR THE REGRESSION MODEL 
Bayes' theorem is always summarized by 
posterior c( prior x likelihood, 
so we need to determine the likelihood and decide on our prior for this model. 
The Joint Likelihood for p and ag 
The joint likelihood of the ith observation is its probability density function as a 
function of the two parameters oz and P, where (zz, yz) are fixed at the observed 
values. It gives relative weights to all possible values of both parameters az and P 
from the observation. The likelihood of observation i is 
likelihood2(az, 
p) c( e - & Z  
[ Y * - ( a Z f f l ( x % - E ) ) 1 2  
since we can ignore the part not containing the parameters. The observations are 
all independent, so the likelihood of the whole sample of all the observations is the 
product of the individual likelihoods: 
n 
likelihood,,mpl,(az, P) 0: n e-~[Y~-(a~+P(x~--2))12 
. 
2=1 
The product of exponentials is found by summing the exponents, so 
~ike~ihood,,mpl,(az, 
p) 0: e-& [C~=,[Y~-(am+fl(x~-z))121
, 
The term in brackets in the exponent equals 
Breaking this into three sums and multiplying it out gives us 
n 
n 
i= 1 
i=l 
n 
This simplifies into 
SS, - ZPSS,, + P~SS, + n(az - Y)~, 
where SS, = Cyr1(y2 - j j ) 2 ,  and SS,, = Cy=l(y2 
- Y)(z2 - 5 ) ) ,  and SS, = 
C7="=,z2 
- 5?)2. Thus the joint likelihood can be written as 
likelihood,,,,l,(a~, 
P) c( e - & [ S S , - Z P S S , , + P ~ S S , + ~ ( ~ ~ - ~ ) ~ ]  

BAYES' THEOREM FOR THE REGRESSION MODEL 
277 
Writing this as a product of two exponentials gives 
We factor out SS, in the first exponential, complete the square, and absorb the part 
that doesn't depend on any parameter into the proportionality constant. This gives us 
Note that % = B, the least squares slope, and y = Az, the least squares estimate 
of the intercept of the vertical line z = 3. We have factored the joint likelihood into 
the product of two individual likelihoods 
li keli hoodsample 
(CQ, p) C( li keli hoodsample (a*) x li kel ihoodsample(P) , 
where 
and 
- + 
(,*-Az)' 
likelihoodsamp~,(az) 
c( e 
a / n  
Since the joint likelihood has been factored into the product of the individual like- 
lihoods we know the individual likelihoods are independent. We recognize that the 
likelihood of the slope /? has the normal shape with mean B, the least squares slope, 
and variance &. Similarly the likelihood of az has the normal shape with mean 
Az and variance $, 
The Joint Prior for p and cyB 
If we multiply the joint likelihood by a joint prior, it is proportional to the joint 
posterior. We will use independent priors for each parameter. The joint prior of the 
two parameters is the product of the two individual priors: 
d a z ,  P) = g(az) x d P )  . 
We can either use normal priors, orflat priors. 
Choosing normal priors for /3 and am. Another advantage of using this pa- 
rameterization is that a person has a more intuitive prior knowledge about the cyz, 
the intercept of 2 = 3, than about ao, the intercept of the y axis. Decide on what 
you believe the mean value of the y values to be. That will be ma%, 
your prior mean 
for az. Then think of the points above and below that you consider to be upper and 
lower bounds of the possible values of y. Divide the difference by 6 to get s,, , your 
prior standard deviation of a*. This will give you reasonable probability over the 
whole range you believe possible. 
Usually we are more interested in the slope p. Sometimes we want to determine 
if it could be 0. Therefore we may choose mg = 0 as the prior mean for p. Then we 

278 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
think of the upper and lower bounds of the effect of an increase in x of one unit on 
y. Divide the difference by 6 to get sp, your prior standard deviation of P. In other 
cases, we have prior belief about the slope from previous data. We would use the 
normal(m0, (sp)’) that matches that prior belief. 
The Joint Posterior for p and as 
The joint posterior then is proportional to the joint prior times the joint likelihood. 
Pidata) rn g(a5, 
P) x lilcelihood,,,,l,(cu2,P), 
where the data is the set of ordered pair (XI, 
yl), . . . , (xn, 
yn). The joint prior and 
the joint likelihood both factor into a part depending on cuz and a part depending on 
p. Rearranging them gives the joint posterior factored into the marginal posteriors 
g(cuE,pldata) 0: g(a2ldata) x g(p1data). 
Since the joint posterior is the product of the marginal posteriors, they are independent. 
Each of these marginal posteriors can be found by using the simple updating rules 
for normal distributions, which works for normal andjat priors. For instance, if we 
use a normal(mp, $) prior for P, we get a normal(m&, ( s & ) ~ ) ,  
where 
1 ss, 
- 
1 
( S b ) 2 - $ + 7  
(14.7) 
(14.8) 
The posterior precision equals the prior precision plus the precision of the likelihood. 
The posterior mean equals the weighted average of the prior mean and the likelihood 
mean where the weights are the proportions of the precisions to the posterior precision. 
And the posterior distribution is normal. 
Similarly, ifweuseanormaL(rn,, , s&) priorforaz, wegetanormaL(rn/,3, ( s & , ) ~ )  
where 
1
n
 
-- - - + -  
1 
( S & J 2  
s& 
cl2 
and 
1 
1 
n 
- 
x m , , + - - i - x ~ A z .  
3 
m&* - - 
(sk,)2 
Example 25 (continued) Michael, the company statistician, decides that he will use 
anormal(1, (.3)2)priorforPandanormal (15,12)priorfora~. 
Since hedoesn’t 
know the true variance, he will use the estimated variance about the least squares 
regression line i2 
= .0348644. Note that SS, = ~ ~ = “ = , x z  
- Z)2 = n(x2 - Z2) = 
- 
25 * (207.0703 - 14.3888 * * 2 )  = .81886. 

BAYES’ THEOREM FOR THE REGRESSION MODEL 
279 
I 
I 
I 
I 
0.2 
0.7 
1.2 
1.7 
Figure 14.6 The prior and posterior distribution of the slope. 
The posterior precision of /3 is 
-- _ -  +-= '81886 
34.5981, 
1 
. ( s & ) ~  .32 
.0348644 
so the posterior standard deviation of p is 
~b = 34.5981-4 = .17001. 
The posterior mean of p is 
1 
,81886 
m& = Z x 1 + .o348644 x 1.29963 = 1.2034 
34.5981 
34.5981 
Similarly, the posterior precision of az is 
1 
25 
12 + ___ = 718.064, 
___- 
_ -  
1 
(Sh, )2 
.0348644 
so the posterior standard deviation is 
4% = 718.064-4 = .037318. 
The posterior mean of az is 
1 
25 
mh, = 
x 15 + .o348644 x 14.2208 = 14.2219. 
718.064 
718.064 
The prior andposterior distribution of the slope are shown in Figure 14.6. 

280 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
Bayesian Credible Interval for Slope 
The posterior distribution of ,B summarizes our entire belief about it after examining 
the data. We may want to summarize it by a (1 - a )  x 100% Bayesian credible 
interval for slope 0. 
This will be 
(14.9) 
More realistically, we don’t know c2. A sensible approach in that instance is to use 
the estimate calculated from the residuals 
5 2  = c;=I(Yi 
- (A% + q z i  - 4))2 
n - 2  
We have to widen the confidence interval to account for the increased uncertainty 
due to not knowing g2. We do this by using a Student’s t critical value with n - 2 
degrees of freedom4 instead of standard normal critical value. The credible interval 
becomes 
m i i t s  x m. 
(1 4.10) 
Frequentist Confidence Interval for Slope 
When the variance 0’ is unknown, the (1 - a) x 100% confidence interval for the 
slope 0 is 
where u2 is the estimate of the variance calculated from the residuals from the least 
squares line. The confidence interval is the same form as the Bayesian credible 
interval when we usedjat priors for ,B and az. Of course the interpretation is 
different. Under the frequentist assumptions we are (1 - a) x 100% confident that 
the interval contains the true, unknown parameter value. Once again, the frequentist 
confidence interval is equivalent to a Bayesian credible interval, so if the scientist 
misinterprets it as a probability interval, he/she will get away with it. The only 
loss experienced will be that the scientist did not get to put in any of hisher prior 
knowledge. 
Testing One-sided Hypothesis about Slope 
Often we want to determine whether or not the amount of increase in y associated 
with one unit increase in x is greater than some value, 00. We can do this by testing 
‘Actually we are treating the unknown parameter uz as a nuisance parameter and using the prior g(u2) o( 
(cr2)-. The marginal posterior of p is found by integrating u2 out of the joint posterior. 

PREDICTIVE DISTRIBUTION FOR FUTURE OBSERVATION 
281 
at the a level of significance in a Bayesian manner. To do the test in a Bayesian 
manner, we calculate the posterior probability of the null hypothesis. This is 
P O  
P(P I P0Id.t.) 
= 
(14.11) 
If this probability is less than a, then we reject HO and conclude that indeed the slope 
P is greater than PO. (If we used the estimate of the variance, then we would use a 
Student’s t with n - 2 degrees of freedom instead of the standard normal 2.) 
Testing Two-sided Hypothesis about Slope 
If P = 0, then the mean of y does not depend on z at all. We really would like to test 
HO : /? = 0 versus H1 : P # 0 at the a level of significance in a Bayesian manner, 
before we use the regression model to make predictions. To do the test in a Bayesian 
manner, look where 0 lies in relation to the credible interval. If it lies outside the 
interval, we reject Ho. Otherwise, we can’t reject the null hypothesis, and we should 
not use the regression model to help with predictions. 
Example 25 (continued) Since Michael used the estimated variance in place of the 
unknown true variance, he used Equation 14.10 to find a 95% Bayesian credible 
interval where there are 23 degrees of freedom. The interval is (.852,1.555). This 
credible interval does not contain 0, so clearly he can reject the hypothesis that the 
slope equals 0 and conclude that the final moisture level can be estimated using the 
measured in-process moisture level. 
14.5 PREDICTIVE DISTRIBUTION FOR FUTURE OBSERVATION 
Making predictions of future observations for specified x values is one of the main 
purposes of linear regression modelling. Often, after we have established from the 
data that there is a linear relationship between the explanatory variable z and the 
response variable y, we want to use that relationship to make predictions of the next 
value yn+l, given the next value of the explanatory variable zn+l. We can make 
better predictions using the value of the explanatory variable than without it. The 
best prediction for yn+l given zn+l will be 
Cn+l = 6 E  + P x (%+1 
- 3) , 
where is the slope estimate and BE is the estimate of the intercept of the line z = 3. 
How good is the prediction? There are two sources of uncertainty. First, we are 
using the estimated values of the parameters in the prediction, not the true values, 
which are unknown. We are considering the parameters to be random variables 

282 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
and have found their posterior distribution in the previous section. Second, the new 
observation yn+l contains its own observation error e,+l, which will be independent 
of all previous observation errors. The predictive distribution of the next observation 
yn+l given the value xn+l and the data accounts for both sources of uncertainty. It 
is denoted f(yn+l /xn+l, 
data) and is found by Bayes’ theorem. 
Finding the Predictive Distribution 
The predictive distribution is found by integrating the parameters az and P out of the 
joint posterior distribution of the next observation yn+l and the parameters given the 
next value x,+1 and the previous observations from the model, (21, y l ) ,  . . . , (x,, yn), 
the data. It is 
Integrating out nuisance parameters from the joint posterior like this is known as 
marginalization. This is one of the clear advantages of Bayesian statistics. It has a 
single method of dealing with nuisance parameters that always works. When we find 
the predictive distribution, we consider all the parameters to be nuisance parameters. 
First, we need to determine the joint posterior distribution of the parameters and 
next observation, given the value xn+l and the data: 
f(Yn+l,ag,PIxn+l,data) = f(Yn+llaz,P,xn+l,data) 
x d a z ,  P1xn+l, data) . 
The next observation yn+l, given the parameters az and ,8 and the known value xn+l, 
is just another random observation from the regression model. Given the parameters 
az and /3, the observations are all independent of each other. This means that given 
the parameters, the new observation yn+l does not depend on the data, which are the 
previous observations from the regression. The posterior for az, p, was calculated 
from the data alone and doesn’t depend on the next value of the predictor xn+l. So 
the joint distribution of new observation and parameters simplifies to 
f(Yn+l,a~,PIxn+l,data) = f ( ~ n + l l ~ ~ , P , ~ n + l )  
x g(az,Pldata) 
which is the distribution of the next observation given the parameters, times the 
posterior distribution of the parameters given the previous data. The next observation, 
given the parameters yn+l laz, p, zn+l, is a random observation from the regression 
model given the value xn+l. By our assumptions it is normally distributed with mean 
given by the linear function of the parameters pn+l = az + p(xn+l - 3) and known 
variance a2. 
The posterior distributions of the parameters given the previous data which 
we found using the updating rules in the previous section are independently nor- 
rnal(rn&*, (s&*)~) 
and nonnal(m&, ( s & ) ~ ) ,  respectively. Since the next observation 
only depends on the parameters through the linear function 
pn+1 = 
+ P(xn+1- 2 )  7 

PREDICTIVE DISTRIBUTION FOR FUTURE OBSERVATION 
283 
we will simplify the problem by letting pn+l be the single parameter. The two 
components ae and /? are independent, so the posterior distribution of pn+l will 
be normal with mean m; = mhi + (zn+l - 2 )  x m& and variance ( ~ 1 ) ~  
= 
(s;,)~+(z,+~ 
-Z)2 x ( s & ) ~ )  
given by Equation5.11 andEquation5.12respectively. 
We will find the predictive distribution by marginalizing the pn+l out of the joint 
posterior of yn+l and pn+l. 
The second factor doesn’t depend on pn+l, so it can be brought in front of the 
integral. We recognize that the first term integrates out, so we are left with 
We recognize that this is a normal(rnk,(~&)~), 
where m& = m;, and ( s & ) ~  
= 
( ~ 5 ) ~  
+ c2. Thus the predictive mean of the next observation yn+l taken at zn+l 
is the posterior mean of pn+l = az + P(zn+l - Z), and the predictive variance of 
yn+l is the posterior variance of pn+l = az + P(z,+l - 2 )  plus the observation 
variance cr2. Thus both sources of uncertainty have been allowed for in the predictive 
distribution. 
Credible interval for the prediction. Often we wish to find an interval that has 
posterior probability equal to 1 - a of containing the next value yn+l which will be 
observed at the value 1,+1. 
This will be a (1 - a )  x 100% credible interval for the 
prediction. We know that the mean and the variance of the prediction distribution are 
m; and ( s ; ) ~ ,  
respectively. The credible interval for the prediction is given by 

284 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
when we know the observation variance 02. When we do not know the observation 
variance and instead use the variance estimate calculated from the residuals, the 
credible interval is given by 
m& f t q  x s& 
= m:, +ttq x d(sy + 6 2  
= mk, + m&(Z,+l- 2 )  i 
tq x J(s&.)2 + (s&)2(Zn+l - ~
)
2
 
+ 6 2  
where we get the critical value from the Student’s t distribution with n - 2 degrees of 
freedom. These credible intervals for the prediction are the Bayesian analogs of the 
frequentist prediction intervals, since they allow for both the estimation error and the 
observation error. The Bayesian credible intervals for the prediction generally will 
be shorter than the corresponding frequentist prediction intervals since the Bayesian 
intervals use information from the prior as well as information from the data. They 
give exactly the same results as the frequentist prediction interval when flat priors are 
used for both the slope and intercept. 
Example 25 (continued) Michael calculated the predictive distribution for thejnal 
moisture level (y) as a function of the in-process moisture level (x), and put 95% 
bounds on the prediction. The mean of the predictive distribution is given by 
m& = 14.2219 + 1.2034 x (Z - 14.3888) 
and the variance of the predictive distribution is given by 
(s&) = .0348644 + .0373182 + .170012(2 - 14.3888)2. 
He calculated 95% prediction intervals as 
(m; - t.025 x s&i m& + t.025 x s&) 
A graph of the predictive mean is shown in Figure 14.7, together with the 95% 
prediction bounds. 
Main Points 
0 Our goal is to use one variable Z, called the predictor variable, to help us predict 
another variable y, called the response variable. 
0 We think the two variables are related by a linear relationship, y = a0 + b x Z. 
b is the slope and a0 is the y-intercept (where the line intersects the y-axis.) 
0 The scatterplot of the points (2, 
y) would indicate a perfect linear relationship 
if the points lie along a straight line. 

MAIN POINTS 
285 
14.0 
14.5 
15.0 
Figure 74.7 The predictive mean with 95% prediction bounds. 
0 However, the points usually do not lie perfectly along a line but are scattered 
around, yet still show a linear pattern. 
0 We could draw any line on the scatterplot. The residuals from that line would 
be the vertical distance from the plotted points to the line. 
0 Least squares is a method for finding a line that best fits a plotted points by 
minimizing the sum of squares of residuals from a fitted line. 
0 The slope and intercept of the least squares line are found by solving the normal 
equations. 
0 The linear regression model has three assumptions: 
1. The mean of y is an unknown linear function of x. Each observation yi 
is made at a known value xi. 
2. Each observation yi is subject to a random error that is normally dis- 
tributed with mean 0 and variance u2. We will assume that u2 is known. 
3. The observation errors are independent of each other. 
0 Bayesian regression is much easier if we reparameterize the model to be 
y = a* + p x (x - 3). 
0 The joint likelihood of the sample factors into a part dependent on the slope p 
and a part dependant on CYZ. 
0 We use independent priors for the slope p and intercept aZ. 
They can be either 
normal priors or “flat” priors. The joint prior is the product of the two priors. 
0 The joint posterior is proportional to the joint prior times the joint likelihood. 
Since both the joint prior and joint likelihood factor into a part dependent on 
the slope p and a part dependant on aZ, 
the joint posterior is the product of the 

286 
BAYESlAN lNFERENCE FOR SIMPLE LlNEAR REGRESSlON 
two individual posteriors. Each of them is normal where the constants can be 
found from the simple updating rules. 
0 Ordinarily we are more interested in the posterior distribution of the slope 
@, which is normal (m’, ( s  ) ~ ) .  In particular, we are interested in knowing 
whether the belief ,B = 0 is credible, given the data. If so, we should not be 
using x to help predict y. 
0 The Bayesian credible interval for p is the posterior mean f 
the critical value 
x the posterior standard deviation. 
0 The critical value is taken from the normal table if we assume the variance o2 
is known. If we don’t know it and use the sample estimate calculated from the 
residuals then we take the critical value from the Student’s t table. 
0 The credible interval can be used to test the two-sided hypothesis HO 
: /3 = 0 
versus HI 
: /? # 0. 
0 We can test a one-sided hypothesis HO : p 5 0 versus H1 : /? > 0 by 
calculating the probability of the null hypothesis and comparing it to the level 
of significance. 
0 We can compute the predictive probability distribution for the next observation 
yn+l taken when xn+l. It is the normal distribution with mean equal to the 
mean of the linear function pn+l = aE + (zn+1 - z), and its variance is equal 
to the variance of the linear function plus the observation variance. 
Exercises 
14.1 A researcher measured heart rate (z) and oxygen uptake (y) for one person 
under varying exercise conditions. He wishes to determine if heart rate, which 
is easier to measure, can be used to predict oxygen uptake. If so, then the 
estimated oxygen uptake based on the measured heart rate can be used in place 
of the measured oxygen uptake for later experiments on the individual: 

EXERCISES 
287 
Heart Rate 
Oxygen Uptake 
z 
Y 
94 
.47 
96 
.75 
94 
.83 
95 
.98 
104 
1.18 
106 
1.29 
108 
1.40 
113 
1.60 
115 
1.75 
121 
1.90 
131 
2.23 
(a) Plot a scatterplot of oxygen uptake y versus heart rate 2. 
(b) Calculate the parameters of the least squares line. 
(c) Graph the least squares line on your scatterplot. 
(d) Calculate the estimated variance about the least squares line. 
(e) Suppose that we know that oxygen uptake given the heart rate is normal 
(QO + /3 x 2, 02), where o2 = .132 is known. Use a normal (0,l’) prior 
for p. What is the posterior distribution of /3? 
(f) Find a 95% credible interval for p. 
(8) Perform a Bayesian test of 
Ho : p = 0 versus HI : p # 0 
at the 5% level of significance. 
14.2 A researcher is investigating the relationship between yield of potatoes (y) and 
level of fertilizer (2.) She divides a field into eight plots of equal size and 
applied fertilizer at a different level to each plot. The level of fertilizer and 
yield for each plot is recorded below: 

288 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
Fertilizer Level 
Yield 
X 
Y 
1 
25 
1.5 
31 
2 
27 
2.5 
28 
3 
36 
3.5 
35 
4 
32 
4.5 
34 
(a) Plot a scatterplot of yield versus fertilizer level. 
(b) Calculate the parameters of the least squares line. 
(c) Graph the least squares line on your scatterplot. 
(d) Calculate the estimated variance about the least squares line. 
(e) Suppose that we know that yield given the fertilizer level is normal 
(a0 + p x 2, 02), 
where oz = 3.0' is known. Use a normal (2,2') prior 
for p. What is the posterior distribution of p? 
(f) Find a 95% credible interval for p. 
(8) Perform a Bayesian test of 
Ho : p 5 0 versus H I  : /3 > 0 
at the 5% level of significance. 
14.3 Aresearcher is investigating the relationship between fuel economy and driving 
speed. He makes six runs on a test track, each at a different speed, and measures 
the kilometers travelled on one liter of fuel. The speeds (in kilometers per hour) 
and distances (in kilometers) are recorded below: 
Speed 
Distance 
X 
Y 
80 
55.7 
90 
55.4 
100 
52.5 
110 
52.1 
120 
50.5 
130 
49.2 

EXERClSES 
289 
(a) Plot a scatterplot of distance travelled versus speed. 
(b) Calculate the parameters of the least squares line. 
(c) Graph the least squares line on your scatterplot. 
(d) Calculate the estimated variance about the least squares line. 
(e) Suppose that we know distance travelled, given that the speed is normal 
(a0 + /3 x x, 0’) where u = .57’ is known. Use a normal (0,l’) prior 
for p. What is the posterior distribution of /3? 
(f) Perform a Bayesian test of 
HO : /3 2 0 versus H I  : /3 < 0 
at the 5% level of significance. 
14.4 The Police Department is interested in determining the effect of alcohol con- 
sumption on driving performance. Twelve male drivers of similar weight, age, 
and driving experience were randomly assigned to three groups of four. The 
first group consumed two cans of beer within 30 minutes, the second group 
consumed four cans of beer within 30 minutes, and the third group was the 
control and did not consume any beer. Twenty minutes later, each of the twelve 
took a driving test under the same conditions, and their individual scores were 
recorded. (The higher the score, the better the driving performance.) The 
results were: 
Cans 
X 
Score 
?/ 
0 
0 
0 
0 
2 
2 
2 
2 
4 
4 
4 
4 
78 
82 
75 
58 
75 
42 
50 
55 
27 
48 
49 
39 
(a) Plot a scatterplot of score versus cans. 
(b) Calculate the parameters of the least squares line. 

290 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
(c) Graph the least squares line on your scatterplot. 
(d) Calculate the estimated variance about the least squares line. 
(e) Suppose we know that the driving score given the number of cans of beer 
drunk is normal (a0 + p x 2, a2), where u2 = 122 is known. Use a 
normal (0,102) prior for p. What is the posterior distribution of p? 
(f) Find a 95% credible interval for p. 
(8) Perform a Bayesian test of 
HO : p 2 0 versus H1 : p < 0 
at the 5% level of significance. 
male who will be tested after drinking 213 = 3 cans of beer. 
(h) Find the predictive distribution for the y13 the driving score of the next 
(i) Find a 95% credible interval for the prediction. 
14.5 A textile manufacturer is concerned about the strength of cotton yarn. In 
order to find out whether fiber length is an important factor in determining the 
strength of yarn, the quality control manager checked the fiber length (5) and 
strength (y) for a sample of 10 segments of yarn. The results are: 
Fiber Length 
Strength 
X 
Y 
85 
99 
82 
93 
75 
103 
73 
97 
76 
91 
73 
94 
96 
135 
92 
120 
70 
88 
74 
92 
(a) Plot a scatterplot of strength versus fiber length. 
(b) Calculate the parameters of the least squares line. 
(c) Graph the least squares line on your scatterplot. 
(d) Calculate the estimated variance about the least squares line. 
(e) Suppose we know that the strength given the fiber length is normal 
(a0 + p x z,u2), where o2 = 7.72 is known. Use a normal (0,102) 
prior for p. What is the posterior distribution of /3. 

EXERCISES 
291 
(f) Find a 95% credible interval for p. 
(8) Perfom a Bayesian test of 
NO 
: p 5 0 versus H1 : p > 0 
at the 5% level of significance. 
yarn which has fiber length xll = 90. 
(h) Find the predictive distribution for y11, the strength of the next piece of 
(i) Find a 95% credible interval for the prediction. 
14.6 In Chapter 3, Exercise 7, we were looking at the relationship between log(mass) 
and log(1ength) for a sample of 100 New Zealand slugs of the species Lima 
maximus from a study conducted by Barker and McGhie (1984). These data 
are in the Minitab worksheet slugmtw. We identified observation 90, which 
did not appear to fit the pattern. It is likely that this observation is an outlier 
that was recorded incorrectly, so remove it from the data set. The summary 
statistics for the 99 remaining observations are. Note: x is log(length), and y 
is log(weight) 
z = 352.399 c 
y = -33.6547 c 
x2 = 1292.94 
c z y  = -18.0147 
c y 2  = 289.598 
(a) Calculate the least squares line for the regression of y on z from the 
formulas. 
(b) Using Minitab, calculate the least squares line. Plot a scatterplot of log 
weight on log length. Include the least squares line on your scatterplot. 
(c) Using Minitab, calculate the residuals from the least squares line, and plot 
the residuals versus Z. From this plot, does it appear the linear regression 
assumptions are satisfied? 
(d) Using Minitab, calculate the estimate of the standard deviation of the 
residuals. 
(e) Suppose we use a normal (3, .52) prior for ,B, the regression slope coef- 
ficient. Calculate the posterior distribution of Pidata. (Use the standard 
deviation you calculated from the residuals as if it is the true observation 
standard deviation.) 
(f) Find a 95% credible interval for the true regression slope p. 
(g) If the slug stay the same shape as they grow (allotropic growth) the height 
and width would both be proportional to the length, so the weight would 
be proportional to the cube of the length. In that case the coefficient of 
log(weight) on log(1ength) would equal 3. Test the hypothesis 
Ho : p = 3 versus H1 : # 3 

292 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
at the 5% level of significance. Can you conclude this slug species shows 
allotropic growth? 
14.7 Endophyte is a fungus Neotyphodiurn lolli, which lives inside ryegrass plants. 
It does not spread between plants, but plants grown from endophyte-infected 
seed will be infected. One of its effects is that it produces a range of compounds 
that are toxic to Argentine stem weevil Listronotus bonariensis, which feeds on 
ryegrass. AgResearch New Zealand did a study on the persistence of perennial 
ryegrass at four rates of Argentine stem weevil infestation. For ryegrass that 
was infected with endophyte the following data were observed: 
Infestation Rate 
2 
0 
0 
0 
0 
0 
5 
5 
5 
5 
5 
10 
10 
10 
10 
10 
20 
20 
20 
20 
20 
Number of Ryegrass Plants (n) 
19 
23 
2 
0 
24 
20 
18 
10 
6 
6 
12 
2 
11 
7 
6 
3 
16 
14 
9 
12 
log,(n + 1) 
Y 
2.99573 
3.17805 
1.09861 
o.oo0Oo 
3.21888 
3.04452 
2.94444 
2.39790 
1.94591 
1.94591 
2.5 6495 
1.09861 
2.48491 
2.07944 
1.9459 1 
1.38629 
2.83321 
2.70805 
2.30259 
2.56495 
(a) Plot a scatterplot of number of ryegrass plants versus the infestation rate. 
(b) The relationship between infestation rate and number of ryegrass plants 
is clearly nonlinear. Look at the transformed variable y = log,(n + 1). 
Plot y versus 2 on a scatterplot. Does this appear to be more linear? 
(c) Find the least squares line relating y to 5 .  Include the least squares line 
on your scatterplot. 

EXERCISES 
293 
(d) Find the estimated variance about the least squares line. 
(e) Assume that the observed yi are normally distributed with mean 0% + 
/3 x (xi - 3) and known variance gZ equal to that calculated in part 
(b.) Find the posterior distribution of / ~ I ( x I ,  
y ~ ) ,  
. . . , ( ~ 2 0 ,  
yzo). Use a 
normal (0, l2) prior for p. 
14.8 For ryegrass that was not infected with endophyte the following data were 
observed: 
Infestation Rate 
I
X
 
Number of Ryegrass Plants (n) 
16 
23 
2 
16 
6 
8 
6 
1 
2 
5 
5 
0 
6 
2 
2 
1 
0 
0 
1 
0 
log,(n + 1) 
Y 
2.83321 
3.17805 
1.09861 
2.83321 
1.9459 1 
2.19722 
1.94591 
0.693 15 
1.09861 
1.79176 
1.79176 
0.00000 
1.94591 
1.09861 
1.09861 
0.69315 
0.00000 
0.00000 
0.693 15 
0.00000 
-~ 
(a) Plot a scatterplot of number of ryegrass plants versus the infestation rate. 
(b) The relationship between infestation rate and number of ryegrass plants 
is clearly nonlinear. Look at the transformed variable y = log,(n + 1). 
Plot y versus x on a scatterplot. Does this appear to be more linear? 
(c) Find the least squares line relating y to 2. 
(d) Find the estimated variance about the least squares line. 
(e) Assume that the observed yi are normally distributed with mean 0% + 
(xi - 3) x p and variance equal to that calculated in part (b.) Find the 

294 
BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRESSION 
x 
y 
posterior distribution of pI(x1, yl), . . . , ( 5 2 0 ~  
~ 2 0 ) .  Use a normal (0, 12) 
prior for P. 
14.9 In the previous two problems we found the posterior distribution of the slope 
of y on x, the rate of weevil infestation for endophyte infected and noninfected 
ryegrass. Let 
be the slope for noninfected ryegrass, and let /32 be the slope 
for infected ryegrass 
(a) Find the posterior distribution of P1 - P2. 
(b) Calculate a 95% credible interval for P1 - ,&. 
(c) Test the hypothesis 
HO : - P2 5 0 versus HI : 
- /32 > 0 
at the 10% level of significance. 
1 1  
9 
9 
9 
9 
12 
1 1  
9 
-21.6 
-16.2 
-19.5 
-16.3 
-18.3 
-24.6 
-22.6 
-17.7 
Computer Exercises 

COMPUTER EXERCISES 
295 
x 
y 
(d) Find a 95% credible interval for the prediction. 
14.2 The following 10 observations come from a simple linear regression model 
where the variance u2 = 32 is known. 
x 1 30 
30 
29 
21 
37 
28 
26 
38 
32 
21 
54 
47 
44 
47 
55 
50 
52 
48 
1.7 
4.5 
4.6 
8.9 
0.9 
1.4 
5.2 
6.4 
y 1 22.4 
16.3 
16.2 
30.6 
12.1 
17.9 25.5 
9.8 
20.5 
29.8 
(a) Use BayesLinRegmac or the equivalent R function to find the posterior 
distribution of the slope p when we use a norrnal(0,32) prior for the 
slope. 
(b) Find a 95% Bayesian credible interval for the slope p. 
(c) Test the hypothesis HO : p 2 1 vs. H1 : p < 1 at the 5% level of 
(d) Find the predictive distribution of y11 which will be observed at 211 = 36. 
(e) Find a 95% credible interval for the prediction. 
significance. 
14.3 The following 10 observations come from a simple linear regression model 
where the variance u2 = 32 is known. 
x 1 22 
31 
21 
23 
19 
26 
27 
16 
28 
21 

This Page Intentionally Left Blank

15 
Bayesian Inference for 
Standard Deviation 
When dealing with any distribution, the parameter giving its location is the most 
important, with the parameter giving the spread of secondary importance. For the 
normal distribution, these are the mean and the standard deviation (or its square, the 
variance), respectively. Usually we will be doing inference on the unknown mean, 
with the standard deviation and hence the variance either assumed known, or treated 
as a nuisance parameter. In Chapter 11, we looked at making Bayesian inferences 
on the mean, where the observations came from a normal distribution with known 
variance. We also saw that when the variance was unknown, inferences about the 
mean could be adjusted by using the sample estimate of the variance in its place 
and taking critical values from the Student’s t distribution. The resulting inferences 
would be equivalent to the results we would have obtained if the unknown variance 
was a nuisance parameter and was integrated out of the joint posterior. 
However, sometimes we want to do inferences on the standard deviation of the 
normal distribution. In this case, we reverse the roles of the parameters. We will 
assume that the mean is known, or else we treat it as the nuisance parameter and 
make the necessary adjustments to the inference. We will use Bayes’ theorem on the 
variance. However, the variance is in squared units, and it is hard to visualize our 
belief about it. So for graphical presentation, we will make the transformation to the 
corresponding prior and posterior density for the standard deviation. 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
297 

298 
BAYESIAN lNFERENCE FOR STANDARD DEVIATION 
15.1 BAYES' THEOREM FOR NORMAL VARIANCE WITH A 
CONTINUOUS PRIOR 
We have a random sample yl, . . . , yn from a nomal(p, 02) distribution where the 
mean p is assumed known, but the variance o2 is unknown. Bayes' theorem can be 
summarized by posterior proportional to prior times likelihood 
It is realistic to consider that the variance can have any positive value, so the con- 
tinuous prior we use should be defined on all positive values. Since the prior is 
continuous, the actual posterior is evaluated by 
where the denominator is the integral of theprior x likelihood over its whole range. 
This will hold true for any continuous prior density. However, the integration would 
have to be done numerically, except for a few special prior densities which we will 
investigate later. 
The inverse chi-squared distribution. The distribution with shape given by 
for 0 < x < 03 is called the inverse chi-squared distribution with ti degrees of 
freedom. To make this a probability density function we multiply by the constant 
c = 
The exact density function of the inverse chi-squared distribution 
with IC degrees of freedom is 
2 5 r (n/2) ' 
(15.2) 
for 0 < x < 03. When the shape of the density is given by 
for 0 < x < 03 then we say x has S times an inverse chi-squared distribution with 
K degrees of freedom. The constant c = 
is the scale factor that makes this 
a density. The exact probability density function of S times an inverse chi-squared 
distribution with IC degrees of freedom' is 
'' 
25r(n/2) 
(15.3) 
'This is also known as the inverse Gamma(?-, S) distribution where r = 4. 

BAYES’ THEOREM FOR NORMAL VARIANCE WITH A CONTINUOUS PRIOR 
299 
;\ !: ,! 
I i  
4 df 
_ _ _ _ _  
=------ 
I 
I 
I 
I 
I 
0 
1 
2 
3 
4 
Figure 75.7 
Inverse chi-squared distribution with for 1, . . . , 5  degrees of freedom. As the 
degrees of freedom increase, the probability gets more concentrated at smaller values. Note: 
S = 1 for all these graphs. 
for 0 < x < m. When U has S times an inverse chi-squared distribution with ti 
degrees of freedom, then W = S/U has the chi-squared distribution with r; degrees 
of freedom. This transformation allows us to find probabilities for the inverse chi- 
squared random variables using Table B.6, the upper tail area of the chi-squared 
distribution. 
A random variable X having S times an inverse chi-squared distribution with r; 
degrees of freedom has mean 
S 
6 - 2  
E ( X )  = - 
provided r; > 2 and variance given by 
2 s 2  
(. 
- 2 ) 2  x (. - 4) 
Var(z) = 
provided ti > 4. Some inverse chi-squared distributions with S = 1 are shown in 
Figure 15.1. 
Likelihood of variance for normal random sample The likelihood of the 
variance for a single random draw from a norma@., a2) where p is known is the 
density of the observation at the observed value taken as a function of the variance 
0 2 .  

300 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
We can absorb any part not depending on the parameter u2 into the constant. This 
leaves 
as the part that determines the shape. The likelihood of the variance for the random 
sample y1, . . . , yn from a nonnal(p, a2) where p is known is product of the like- 
lihoods of the variance for each of the observations. The part that gives the shape 
is 
f(yiu2) c( (g2)-+e-&(,-fi) 
n 
f(yl,. . . , ynju2) 
c( 
I - I ( u 2 ) - f e - h ( y s - f i ) ’  
z=1 
c( 
(0 2 -IL z e -+En 
2 0  
c = i ( ~ ~ - ~ ) z  
(15.4) 
where SST = c:=l 
(yi - p)2 is the total sum of squares about the mean. We see that 
the likelihood of the variance has the same shape as SST times an inverse chi-squared 
distribution with n = n - 2 degrees of freedom.2 
15.2 SOME SPECIFIC PRIOR DISTRIBUTIONS AND THE RESULTING 
POSTERIORS 
Since we are using Bayes’ theorem on the normal variance u2, we will need its prior 
distribution. However, the variance is in squared units, not the same units as the 
mean. This means that they are not directly comparable so that it is much harder 
understand a prior density for u2. The standard deviation u is in the same units as the 
mean, so it is much easier to understand. Generally, we will do the calculations to 
find the posterior for the variance u2, but we will graph the corresponding posterior 
for the standard deviation u since it is more easily understood. For the rest of the 
chapter, we will use the subscript on the prior and posterior densities to denote which 
parameter a or u2 we are using. The variance is a function of the standard deviation, 
so we can use the chain rule from Appendix 1 to get the prior density for u2 that 
corresponds to the prior density for u. This gives the change of variable formula3 
which in this case is given by 
Similarly, if we have the prior density for the variance, we can use the change of 
variable formula to find the corresponding prior density for the standard deviation 
gm(u) = g,2(u2) x 2u. 
(15.6) 
*When the mean is not known but considered a nuisance parameter, use the marginal likelihood for uz 
which has same shape as SS, times an inverse chi-squared distribution with n = n - 3 degrees of 
freedom where SS, = c(y - g)2. 
31n general, when ge(8) is the prior density for parameter 0 and if $(8) is a one-to-one function of 8, then 
11, is another possible parameter. The prior density of 1c, is given by g+(+) = go(@($)) x $$. 

SOME SPECIFIC PRIOR DISTRIBUTIONS AND THE RESULTING POSTERIORS 
301 
Positive Uniform Prior Density for Variance 
Suppose we decide that we consider all positive values of the variance o2 to be 
equally likely and don’t wish to favor any particular value over another. We give all 
positive values of a2 equal prior weight. This gives the positive uniform prior density 
for the variance 
guz(02) = 1 for o2 > 0 .  
This is an improper prior since it’s integral over the whole range would be co, 
however that will not cause a problem here. The corresponding prior density for the 
standard deviation would be gu(o) = 2 0  is also clearly improper. (Giving equal 
prior weight to all values of the variance gives more weight to larger values of the 
standard deviation.) The shape of the posterior will be given by 
which we recognize to be SSTX an inverse chi-squared distribution with n - 2 
degrees of freedom. 
Positive Uniform Prior Density for Standard Deviation 
Suppose we decide that we consider all positive values of the standard deviation o 
to be equally likely and don’t wish to favor any particular value over another. We 
give all positive values all equal prior weight. This gives the positive uniform prior 
density for the standard deviation 
g,(a) = 1 for o > 0 .  
This prior is clearly an improper prior since when we integrate it over the whole range 
we get co, however that will not cause trouble in this case. Using Equation 15.5 we 
find the corresponding prior for the variance is 
1 
2a 
gu2(02) = 1 x - 
. 
(We see that giving equal prior weight to all values of the standard deviation gives 
more weight to smaller values of the variance.) The posterior will be proportional to 
the prior times likelihood. We can absorb the part not containing the parameter into 
the constant. The shape of the posterior will be given by 
We recognize this to be SST x an inverse chi-squared distribution with n - 1 degrees 
of freedom. 

302 
BAYESIAN INFERENCE FOR STANDARD DEVIAT/ON 
Jeffreys' Prior Density 
If we think of a parameter as an index of all the possible densities we are considering, 
any continuous function of the parameter will give an equally valid index. Jeffreys' 
wanted to find a prior that would be invariant for a continuous transformation of the 
~arameter.~ 
In the case of the nonnal(p, 0') distribution where p is known, Jeffreys' 
rule gives 
g,2(a2) 0: - for u2 > 0 .  
This prior is also improper, but again in the single sample case this will not cause any 
problem. (Note that the corresponding prior for the standard deviation is g,(o) 0: 
up'.) The shape of the posterior will be given by 
1 
U2 
which we recognize to be SST x an inverse chi-squared with n degrees of freedom. 
Inverse Chi-squared Prior 
Suppose we decide to use S times an inverse chi-squared with rc degrees of freedom 
as the prior for u2. In this case the shape of the prior is given by 
for 0 < u2 < 00. Note the shape of the corresponding prior density for u found 
using the change of variable formula would be 
for 0 < u2 < m. The prior densities for u corresponding to inverse chi-squared 
prior with S = 1 for variance u2 for rc = 1,2,3,4, and 5 degrees of freedom are 
shown in Figure 15.2. We see that as the degrees of freedom increase, the probability 
gets more concentrated at smaller values of u. This suggests that to allow for the 
possibility of a large standard deviation, we should use low degrees of freedom when 
using an inverse chi-squared prior for the variance. The posterior density for u2 will 
have shape given by 
4Jeffreys' invariant prior for parameter 8 is given by g(f3) o( 
information and is given by Z(f3ly) = -E ( '2'o~,&(v1')). 
where I(0ly) is known as Fisher's 

SOME SPECFIC PRIOR DISTRIBUTIONS AND THE RESULTING POSTERIORS 
303 
0 
1 
2 
Figure 15.2 
variance uz where S = 1. 
Prior for standard deviation u corresponding to inverse chi square prior for 
which we recognize as S' times an inverse chi-squared distribution with n' degrees 
of freedom, where S' = S + SST and n' = n + n. So when observations come from 
normal(p, cr2) with known mean p, the conjugate family is the S times an inverse 
chi-squared distribution and the simple updating rule is "add total sum of squares 
about known mean to constant S" and "add sample size to degrees of freedom." 
The corresponding priors for the standard deviation and the variance are shown 
in Table 15.1. All these priors will yield S' times an inverse chi-squared with n’ 
posteriors .5 
Choosing an inverse chi-squared prior. Frequently, our prior belief about cr 
is fairly vague. Before we look at the data, we believe that we decide on a value c 
such that we believe cr < c and cr > c are equally likely. This means that c is our 
prior median. 
We want to choose Sx an inverse chi-squared distribution with IF. degrees of 
freedom that fits our prior median. Since we have only vague prior knowledge about 
cr, we would like the prior to have as much spread as possible, given that it has the 
prior median. W = 3 has a chi-squared distribution with ti degrees of freedom. 
.50 = P(cr > C )  
5The positive uniform prior for st. dev., the positive uniform prior for the variance, and the Jeffreys' prior 
have the form of an inverse chi-squared with S = 0 and n = - 1, -2, and 0, respectively. They can be 
considered limiting cases of the S times an inverse chi-squared family as S -+ 
0. 

304 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
Prior 
POS. unif. for var. 
POS. unif. for st. dev. 
Jeffreys’ 
SX inv. chi-sq n: df 
Table 15.1 
resulting inverse chi-squared posterior 
Corresponding priors for standard deviation and variance, and S’ and IE for the 
d.) 0: 
gu2(c2) 0: 
S’ 
K’ 
SST 
n - 2  
c 
1 
1 
U 
SST 
n - 1  
0 
SST 
( & ) + + I  
1 
e-23 
( u 2 ) $ + 1  e - 3  s + SST 
K + n 
1 
1 
1 
- 
n 
;;z 
- 
1 
Figure 15.3 
1, . . . , 5  degrees of freedom. 
Inverse chi-squared prior densities having same prior medians, for ti = 
= P(W+ 
where W has the chi-squared distribution with 1 degree of freedom. We look in 
Table B.6 to find the 50% point for the chi-squared distribution with K. degree of 
freedom and solve the resulting equation for S. Figure 15.3 shows the prior densities 
having the same median for K = 1, . . . , 5  degrees of freedom. We see that the prior 
with n = 1 degree of freedom has more weight on the lower tail. The upper tail 
is hard to tell as all are squeezed towards 0. We take logarithms of the densities to 
spread out the upper tail. These are shown in Figure 15.4, and clearly the prior with 
n = 1 degrees of freedom shows the most weight in both tails. Thus, the inverse 
chi-squared prior with 1 degree of freedom matching the prior median has maximum 
spread out of all other inverse chi-squared priors that also match the prior median. 

SOME SPECIFIC PRIOR DISTR15UTIONS AND THE RESULTING POSTERIORS 
305 
Figure 15.4 
for n = 1, . . . , 5  degrees of freedom. 
Logarithms of Inverse chi-squared prior densities having same prior medians, 
Example 27 Aroha, Bernardo, and Carlos are three statisticians employed ata dairy 
factory who want to do inference on the standard deviation of the content weights 
of ”I kg “ packages of dried milk powder coming off the production line. The three 
employees consider that the weights of the packages will be normal(b, a’) where p 
is known to be at the target which is 1015 grams. Aroha decides that she will use 
the positive uniform prior for the standard deviation, g(a) = 1 for a > 0. Bernardo 
decides he will use Jeffreys’priorg(a) cc $. Carlosdecides that hisprior belief about 
the standard deviation distribution is that its median equals 5. He looks in Table B.6 
andfinds that the 50%point for the chi-squared distribution with I degree of freedom 
equals .4549, and he calculates S = .4549 x 5 = 11.37. Therefore his prior for 
a’ 
will be 11.37 times an inverse chi-squared distribution with I degree offreedom. 
He converts this to the equivalent prior density for a using the change of variable 
formula. The shapes of the three prior densities for cr are shown in Figure 15.5. We 
see that Aroha’spriordoes not go down as a increases, and that both Bernardo’s and 
Carlos’ prior only goes down very slowly as a increase. This means that all three 
priors will be satisfactory if the data shows much more variation than was expected. 
We also see that Bernardo’s prior increases towards infinity as a goes to zero. This 
means his prior gives a great deal of weight to very small values.6 Carlos’ prior 
does not give much weight to small values, but this doesn’t cause a problem, since 
overestimating the variance is more conservative than underestimating it. They take 
6The prior g(0) rx 0-’ is improper two ways. Its limit of its integral from a to 1 as a approaches 0 
is infinite. This can cause problems in more complicated models where posterior may also be improper 
because the data can’t force the corresponding integral for the posterior to be finite. However, it will not 
cause any problem in this particular case. The limit of the integral from 1 to b of Bemardo’s prior as 
b increases without bounds is also infinite. However, this will not cause any problems, as the data can 
always force the corresponding integral for the posterior to be finite. 

306 
BAYESIAN lNFERENCE FOR STANDARD DEVIATION 
Aroha’s prior 
Bernardo’s prior 
Carlos’ prior 
figure 75.5 
standard deviation (T. 
the shapes of Aroha’s, Bernardo’s, and Carlos’ prior distributions for the 
a random sample of size 10 and measure the content weights in grams. They are: 
The calculations for SST are 
1011 
1009 
1019 
1012 
1011 
1016 
1018 
1021 
1016 
1012 
Value Subtract mean 
Squared 
1011 
-4 
16 
I009 
-6 
36 
1019 
4 
16 
1012 
-3 
9 
1011 
-4 
16 
1016 
I 
I 
1018 
3 
9 
1021 
6 
36 
1016 
1 
I 
1012 
-3 
9 
SST 
149 
Each employee has S’x an inverse chi-squared with R degrees of freedom for 
the posterior distribution for the variance. Aroha’s posterior will be 149 x an 
inverse chi-squared with 9 degrees of freedom, Bernardo’s posterior will be 149 
x an inverse chi-squared with 10 degrees offieedom, and Carlos’ posterior will be 
11.37+149=160.37 x an inverse chi-squaredwith 10+1=11 degreesoffreedom. The 
corresponding posterior densities for the variance u2 and the standard deviation u 

SOME SPECIFIC PRIOR DISTRIBUTIONS AND THE RESULTING POSTERIORS 
307 
Bernardo‘s posterior 
1 
I 
I 
I 
I 
I 
0 
10 
20 
30 
40 
50 
Figure 15.6 
Aroha’s, Bemardo’s, and Carlos’ posterior distributions for variance u2 
Bernardo’s posterior 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0 
1
2
 3 
4 
5 
6 
7 
8 
9 
10 
Figure 15.7 Aroha’s, Bemardo’s, and Carlos’ posterior distributions for standard deviation 
U .  
are shown in Figure 15.6and Figure 15.7, respectively. We see that Aroha ’sposterior 
has a somewhat longer upper tail than the others since her prior gave more weight 
for large values of 0. 

308 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
15.3 BAYESIAN INFERENCE FOR NORMAL STANDARD DEVIATION 
The posterior distribution summarizes our belief about the parameter taking into 
account our prior belief and the observed data. We have seen in the previous section, 
that the posterior distribution of the variance g(a2/yl,. 
. . , yn) is S’X an inverse 
chi-squared with K’ degrees of freedom. 
Bayesian Estimators for u 
Sometimes we want an estimator about the parameter which summarizes the posterior 
distribution into a single number. We will base our Bayesian estimators for a on 
measures of location from the posterior distribution of the variance a2. Calculate the 
measure of location from the posterior distribution of the variance g(02/yl, . . . , Yn) 
and then take the square root for our estimator of the standard deviation a. Three 
possible measures of location are the posterior mean, posterior mode, and posterior 
median. 
Posterior mean of variance u2. The posterior mean is found by taking the 
expectation E(a2g(a2/yl,. 
. . , yn). Lee (1989) shows that when K’ > 2 the posterior 
mean is given by 
The first possible Bayesian estimator for the standard deviation would be its square 
root, 
6 = J”I 
K l - 2   
Posterior mode of variance u2, The posterior distribution of the variance az 
is given by S‘ x an inverse chi-squared distribution with K degrees of freedom. The 
posterior mode is found by setting the derivative of g(a2 ly1,. . . , yn) equal to 0, and 
solving the resulting equation. It is given by 
S’ 
m o d e  = - 
K + 2 
The second possible Bayesian estimator for the standard deviation would be its square 
root, 
POStefiOf median of variance u2. The posterior median is the value that has 
50% of the posterior distribution below it, and 50% above it. It is the solution of 
g(a2/yl,. 
. . , yn)da2 = .5 
I”edian 

BAYESIAN lNFERENCE FOR NORMAL STANDARD DEVIATION 
309 
Person 
Aroha 
Bernard0 
Carlos 
Table 15.2 Posterior estimates of the standard deviation u 
Posterior Parameters 
Estimator Found using Posterior 
S' 
R' 
Mode 
Mean 
Median 
149 
9 
3.680 
4.614 
4.226 
149 
10 
3.524 
4.316 
3.994 
160.37 
11 
3.512 
4.221 
3.938 
which can be found numerically. The third possible Bayesian estimator for the 
standard deviation would be its square root 
6 = drnedian . 
Example 27 (continued) The three employees decide tojind their estimates of the 
standard deviation u. They are shown in Table 15.2. Since the posterior density of 
the standard deviation can seen to be positively skewed with a somewhat heavy right 
tail, the estimates found using the posterior mean would be the best, followed by the 
estimates found using the posterior median. The estimates found using the posterior 
mode would tend to underestimate the standard deviation. 
Bayesian Credible Interval for u 
The posterior distribution of the variance u2 given the sample data is S' x an inverse 
chi-squared with K’ degrees of freedom. Thus W = S'/u2 has the chi-squared 
distribution with R' degrees of freedom. We set up a probability statement about W ,  
and invert it to find the credible interval for u2. Let u be the chi-squared value with 
R' degrees of freedom having upper tail area 1 - 2 and let 1 be the chi-squared value 
having upper tail area 5. These values are found in Table B.6. 
P -,. 
2
2
)
 = 1-a. 
(7 
U 
We take the square roots of the terms inside the brackets to convert this to a credible 
interval for the standard deviation u 
(15.7) 
Example 27 (continued) Each of the three employees has S'X a inverse chi-squared 
distribution with n’ degrees offreedom. They calculate their 95% credible intervals 

310 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
Table 15.3 Credible intervals for the standard deviation u 
Person 
Posterior Parameters 
95% Credible Interval 
Lower Limit 
Upper Limit 
Aroha 
Bernard0 
149 
10 
2.70 
6.77 
Carlos 
160.37 
11 
2.70 
6.48 
for u and put them in Table 15.3. We see that Aroha’s credible interval is shifted 
slightly upwards and has a somewhat larger upper value than the others, which makes 
sense since herposterior distribution has a longer upper tail as seen in Figure 15.7. 
Testing a One-sided Hypothesis about u 
Usually we want to determine whether or not the standard deviation is less than or 
equal to some value. We can set this up as a one-sided hypothesis test about u, 
HO : u 5 uo versus HI : u > uo . 
We will test this by calculating the posterior probability of the null hypothesis and 
comparing this to the level of significance a that we chose. Let W = 5. 
P(HoistruelYl,...,Yn) = P ( u  I a o l Y 1 ,  ..., Yn) 
= P ( U Z  i ailY1,. . . , Yn) 
= P ( W L  WO), 
where WO = z. When the null hypothesis is true, W has the chi-squareddistribution 
with K degrees of freedom. This probability can be bounded by values from Table 
B.6, or alternatively it can be calculated using Minitab or R. 
Example 27 (continued) The three employees want to determine i f  the standard 
deviation is greater than 5.00. They set this up as the one-sided hypothesis test 
No : u 5 5.00 versus H I  : u > 5.00 
and choose a level of signijcance a = .lo. They each calculate their posterior 
probability of the null hypothesis. The results are in the Table 15.4. None of their 
posterior probabilities of the null hypothesis are below a = .lo, so each employee 
accepts the null hypothesis at that level. 
Main Points 
The shape of the S times an inverse chi-squared distribution with K degrees of 
freedom is given by 
1
s
 
g(z) cx r e s .  
zz+l 

MAIN POINTS 
31 1 
Person 
Aroha 
Bernard0 
Carlos 
Table 15.4 Results of Bayesian one-sided hypothesis tests 
Posterior 
P(0 5 51~1,. 
. . , yn) 
149x inv. chi-sq. 9df 
P(W 2 g) =.7439 
Accept 
149x inv. chi-sq. 10df 
P(W 2 9) =.8186 
Accept 
160.37~ inv. chi-sq. 11 df 
P(W 2 w) =.8443 
Accept 
If U has S times an inverse chi-squared distribution with K degrees of freedom, 
then W = 
has the chi-squared distribution with K degrees of freedom. 
Hence inverse chi-squared probabilities can be calculated using the chi-squared 
distribution table. 
When X is random variable having S times an inverse chi-squared distribution 
with K degrees of freedom then then its mean and variance are given by 
S 
E ( X )  = 
~ 
K - 2  
and 
252 
V a r ( X )  = 
( K  - 2)’ X ( K  - 4) ’ 
provided that K > 2 and K > 4, respectively. 
The likelihood of the variance for arandom sample from a normal(p, u2) when 
p is known has the shape of SST times an inverse chi-squared distribution with 
n - 2 degrees of freedom. 
We use Bayes’ theorem on the variance, so we need the prior distribution of 
the variance u2. 
It is much easier to understand and visualize the prior distribution of the 
standard deviation u. 
The prior for the standard deviation can be found from the prior for the variance 
using the change of variable formula, and vice versa. 
0 Possible priors include 
1. Positive uniform prior for variance 
2. Positive uniform prior for standard deviation 
3. Jeffreys’ prior (same for standard deviation and variance) 
4. S times an inverse chi-squared distribution with K degrees of freedom. 
(This is the conjugate family of priors for the variance.) Generally it is 
better to choose a conjugate prior with low degrees of freedom. 
Find Bayesian estimators for standard deviation u by calculating a measure 
of location such as the mean, median, or mode from the posterior distribution 
of the variance u2, and taking the square root. Generally, using the posterior 

312 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
mean as the measure of location works best because the posterior distribution 
has a heavy tail, and it is more conservative to overestimate the variance. 
Bayesian credible intervals for CT can be found by converting the posterior 
distribution of CT’ (which is S’ times an inverse chi-squared with d degrees of 
freedom) to the posterior distribution of W = 5 which is chi-squared with 
r; degrees of freedom. We can find the upper and lower values for W ,  and 
convert them back to find the lower and upper values for the credible interval 
for CT. 
0 One-sided hypothesis tests about he standard deviation CJ can be performed by 
calculating the posterior probability of the null hypothesis and comparing it to 
the chosen level of significance a 
Exercises 
15.1 The strength of an item is known to be normally distributed with mean 200 
and unknown variance CJ’. A random sample of ten items is taken and their 
strength measured. The strengths are: 
215 
186 
216 
203 
221 
188 
202 
192 
208 
195 
(a) What is the equation for the shape of the likelihood function of the 
variance CT’? 
(b) Use a positive uniform prior distribution for the variance CT’. Change 
the variable from the variance to the standard deviation to find the prior 
distribution for the standard deviation CT. 
(c) Find the posterior distribution of the variance CT’. 
(d) Change the variable from the variance to the standard deviation to find 
(e) Find a 95% Bayesian credible interval for the standard deviation CT. 
(f) Test HO : D 5 8 vs. H I  : 0 > 8 at the 5% level of significance. 
the posterior distribution of the standard deviation. 
15.2 The thickness of items produced by a machine is normally distributed with 
mean p = .001 cm and unknown variance 02. 
A random sample of ten items 
are taken and measured. They are: 
.00110 
.00146 
.00102 
.OOO66 
,00139 
.00121 
.00053 
.00144 
.00146 
.OOO75 
(a) What is the equation for the shape of the likelihood function of the 
variance CT’ 
? 

EXERCISES 
313 
Use a positive uniform prior distribution for the variance u2. Change 
the variable from the variance to the standard deviation to find the prior 
distribution for the standard deviation o. 
Find the posterior distribution of the variance 02. 
Change the variable from the variance to the standard deviation to find 
the posterior distribution of the standard deviation. 
Find a 95% Bayesian credible interval for the standard deviation o. 
Test Ho : u 5 .0003 vs. H I  : u > .0003 at the 5% level of significance. 
15.3 The moisture level of a dairy product is normally distributed with mean 15% 
and unknown variance 02. 
A random sample of size 10 is taken and the mois- 
ture level measured. They are: 
15.01 
14.95 
14.99 
14.09 
16.63 
13.98 
15.78 
15.07 
15.64 
16.98 
(a) What is the equation for the shape of the likelihood function of the 
variance u2 ? 
(b) Use Jeffreys’ prior distribution for the variance 0’. Change the variable 
from the variance to the standard deviation to find the prior distribution 
for the standard deviation o. 
(c) Find the posterior distribution of the variance u’. 
(d) Change the variable from the variance to the standard deviation to find 
(e) Find a 95% Bayesian credible interval for the standard deviation o. 
(f) Test Ho : o 2 1.0 vs. H I  : o > 1.0 at the 5% level of significance. 
the posterior distribution of the standard deviation. 
15.4 The level of saturated fats in a brand of cooking oil is normally distributed with 
mean p = 15% and unknown variance u’. The percentages of saturated fat in 
a random sample of ten bottles of the cooking oil are: 
13.65 
14.31 
14.73 
13.88 
14.66 
15.53 
15.36 
15.16 
15.76 
18.55 
(a) What is the equation for the shape of the likelihood function of the 
variance a’? 
(b) Use Jeffreys’ prior distribution for the variance o’. Change the variable 
from the variance to the standard deviation to find the prior distribution 
for the standard deviation u. 
(c) Find the posterior distribution of the variance o’. 
(d) Change the variable from the variance to the standard deviation to find 
the posterior distribution of the standard deviation. 

314 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
(e) Find a 95% Bayesian credible interval for the standard deviation 0. 
(f) Test HO : 0 5 .05 vs. H I  : > .05 at the 5% level of significance. 
15.5 Let a random sample of 5 observations from a nonal(p,u2) distribution 
(where it is known that the mean p = 25) be 
26.05 
29.39 
23.58 
23.95 
23.38 
(a) What is the equation for the shape of the likelihood function of the 
variance u2? 
(b) We believe (before looking at the data) that the standard deviation is as 
likely to be above 4 as it is to be below 4. (Our prior belief is that the 
distribution of the standard deviation has median 4.) Find the inverse 
chi-squared prior with 1 degree of freedom that fits our prior belief about 
the median. 
(c) Change the variable from the variance to the standard deviation to find 
the prior distribution for the standard deviation u. 
(d) Find the posterior distribution of the variance 02. 
(e) Change the variable from the variance to the standard deviation to find 
(0 Find a 95% Bayesian credible interval for the standard deviation 0. 
(8) Test HO : 0 5 5 vs. H I  : u > 5 at the 5% level of significance. 
the posterior distribution of the standard deviation. 
15.6 The weight of milk powder in a "1 kg" package is nomal(p, u2) 
distribution 
(where it is known that the mean p = 1015 g.) Let a random sample of 10 
packages be taken and weighed. The weights are 
1019 
1023 
1014 
1027 
1017 
1031 
1004 
1018 
1004 
1025 
(a) What is the equation for the shape of the likelihood function of the 
variance 02? 
(b) We believe (before looking at the data) that the standard deviation is as 
likely to be above 5 as it is to be below 5. (Our prior belief is that the 
distribution of the standard deviation has median 5.) Find the inverse 
chi-squared prior with 1 degree of freedom that fits our prior belief about 
the median. 
(c) Change the variable from the variance to the standard deviation to find 
the prior distribution for the standard deviation u. 
(d) Find the posterior distribution of the variance c2. 
(e) Change the variable from the variance to the standard deviation to find 
(f) Find a 95% Bayesian credible interval for the standard deviation 0. 
the posterior distribution of the standard deviation. 

COMPUTER EXERCISES 
315 
(8) If there is evidence that the standard deviation is greater than 8, then the 
machine will be stopped and adjusted. Test HO : u 5 8 vs. H1 : u > 8 
at the 5% level of significance. Is there evidence that the packaging 
machine needs to be adjusted? 
Computer Exercises 
15.1 We will use the Minitab macro WarICPrnac or the equivalent R function to 
find the posterior distribution of the standard deviation (T when we have a 
random sample of size n from a norma@, u2) 
distribution and the mean p is 
known. We have Sx an inverse chi-squared(&) prior for the variance u2. This 
is the conjugate family for normal observations with known mean. Starting 
with one member of the family as the prior distribution, we will get another 
member of the family as the posterior distribution. The simple updating rules 
are 
S ' = S + S S T  
and d = & + n ,  
where SST = C ( y i  - p)’. Suppose we have five observations from a nor- 
rnal(p, u2) 
distribution where p = 200 is known. They are: 
206.4 
197.4 
212.7 
208.5 
203.4 
(a) Suppose we start with a positive uniform prior for the standard deviation 
(b) Find the posterior using the Minitab macro NKarlCPrnac or the equivalent 
(c) Find the posterior mean and median. 
(d) Find a 95% Bayesian credible interval for u. 
u. What value of S x an inverse chi-squared( 6) will we use? 
R function. 
15.2 Suppose we start with a Jeffreys' prior for the standard deviation u. What 
value of Sx an inverse chiquared(&) will we use? 
(a) Find the posterior using the Minitab macro WarICPrnac or the equivalent 
(b) Find the posterior mean and median. 
(c) Find a 95% Bayesian credible interval for u. 
R function. 
15.3 Suppose our prior belief is u is just as likely to be below 8 as it is to be above 
8. (Our prior distribution g(o) has median 8.) Determine an Sx an inverse 
chi-squared(&) that matches our prior median where we use R = 1 degree of 
freedom. 
(a) Find the posterior using the Minitab macroNKarlCf!rnac or the equivalent 
R function. 

316 
BAYESIAN INFERENCE FOR STANDARD DEVIATION 
(b) Find the posterior mean and median. 
(c) Find a 95% Bayesian credible interval for o. 
15.4 Suppose we take five additional observations from the nomal(p, 0’) distribu- 
tion where p = 200 is known. They are: 
211.7 
205.4 
206.0 
206.5 
201.7 
(a) Use the posterior from Exercise 15.3 as the prior for the new observa- 
tions and find the posterior using the Minitab macro NVarZCPmac or the 
equivalent R function. 
(b) Find the posterior mean and median. 
(c) Find a 95% Bayesian credible interval for o. 
15.5 Suppose we take the entire sample of ten nomal(p,a2) observations as a 
single sample. We will start with the original prior we found in Exercise 15.3. 
(a) Find the posterior using the Minitab macro WarlCPmac or the equivalent 
(b) What do you notice from the Exercises 15.3-15.5? 
(c) Test the hypothesis Ho : CT 5 5 vs. H I  : o > 5 at the 5% level of 
R function. 
significance. 

16 
Robust Bayesian Methods 
Many statisticians hesitate to use Bayesian methods because they are reluctant to 
let their prior belief into their inferences. In almost all cases they have some prior 
knowledge, but they may not wish to formalize it into a prior distribution. They know 
that some values are more likely than others, and some are not realistically possible. 
Scientists are studying and measuring something they have observed. They know the 
scale of possible measurements. We saw in previous chapters that all priors that have 
reasonable probability over the range of possible values will give similar, although 
not identical, posteriors. And we saw that Bayes’ theorem using the prior information 
will give better inferences than frequentist ones that ignore prior information, even 
when judged by frequentist criteria. The scientist would be better off if he formed a 
prior from his prior knowledge and used Bayesian methods. 
However, it is possible that a scientist could have a strong prior belief, yet that 
belief could be incorrect. When the data are taken, the likelihood is found to be 
very different from that expected from the prior. The posterior would be strongly 
influenced by the prior. Most scientists would be very reluctant to use that posterior. 
If there is a strong disagreement between the prior and the likelihood, the scientist 
would want to go with the likelihood, since it came from the data. 
In this chapter we look at how we can make Bayesian inference more robust 
against a poorly specified prior. We find that using a mixture of conjugate priors 
enables us to do this. We allow a small prior probability that our prior is misspecified. 
If the likelihood is very different from what would be expected under the prior, the 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
317 

318 
ROBUST BAYESIAN METHODS 
posterior probability of misspecification is large, and our posterior distribution will 
depend mostly on the likelihood. 
16.1 EFFECT OF MlSSPEClFlED PRIOR 
One of the main advantages of Bayesian methods is that it uses your prior knowledge, 
along with the information from the sample. Bayes’ theorem combines both prior 
and sample information into the posterior. Frequentist methods only use sample 
information. Thus Bayesian methods usually perform better than frequentist ones 
because they are using more information. The prior should have relatively high 
values over the whole range where the likelihood is substantial. 
However, sometimes this does not happen. A scientist could have a strong prior 
belief, yet it could be wrong. Perhaps he (wrongly) bases his prior on some past data 
that arose from different conditions than the present data set. If a strongly specified 
prior is incorrect, it has a substantial effect on the posterior. This is shown in the 
following two examples. 
Example 28 Archie is going to conduct a survey about how many Hamilton voters 
say they will attend a casino if it is built in town. He decides to base his prior on 
the opinions of his friends. Out of the 25 friends he asks, 15 say they will attend 
the casino. So he decides on a beta(a, b) prior that matches those opinions. The 
prior mean is .6, and the equivalent samples size is 25. Thus a + b + 1 = 25 and 
& = .6. Thus a = 14.4 and b = 9.6. Then he takes a random sample of 100 
Hamilton voters and finds that 25 say they will attend the casino. His posterior 
distribution is beta(39.4,84.60). Archie’s priol; the likelihood, and his posterior are 
shown in Figure 16.1. We see that the prior and the likelihood do not overlap very 
much. The posterior is in between. It gives high posterior probability to values that 
aren’t supported strongly by the data (likelihood) and aren’t strongly supported by 
prior either: This is not satisfactory. 
Example 29 Andrea is going to take a sample of measurements of dissolved oxygen 
level from a lake during the summer: Assume that the dissolved oxygen level is 
approximately normal with mean p and known variance 0’ = 1. She had previously 
done a similar experiment from the river thatjowed into the lake. She considered that 
she had a pretty good idea of what to expect. She decided to use a normal(8.5, .7’) 
prior for p, which was similar to her river survey results. She takes a random 
sample of size 5 and the sample mean is 5.45. The parameters of the posterior 
distribution are found using the simple updating rules for normal. The posterior is 
normal(6.334, .376g2). The priol; likelihood, and posterior are shown in 2. The 
posterior density is between the prior and likelihood, and gives high probability to 
values that aren’t supported strongly either by the data or by the priol; which is a very 
unsatisfactory result. Figure 16.2 shows Andrea’s priol; likelihood, and posterior: 

BAYES’ THEOREM WlTH MIXTURE PRlORS 
319 
I i 
i 
~ 
I
:
 
I 
I 
I 
I 
I 
I 
1 
I 
I 
I 
, 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure 16. I Archie's prior, likelihood, and posterior. 
These two examples show how an incorrect prior can arise. Both Archie and Andrea 
based their priors on past data, each judged to arise from a situation similar the one to 
be analyzed. They were both wrong. In Archie's case he considered his friends to be 
representative of the population. However, they were all similar in age and outlook 
to him. They do not constitute a good data set to base a prior on. Andrea considered 
that her previous data from the river survey would be similar to data from the lake. 
She neglected the effect of water movement on dissolved oxygen. She is basing her 
prior on data obtained from an experiment under different conditions than the one 
she is now undertaking. 
16.2 BAYES' THEOREM WITH MIXTURE PRIORS 
Suppose our prior density is go(6') and it is quite precise, because we have substantial 
prior knowledge. However, we want to protect ourselves from the possibility that we 
misspecified the prior by using prior knowledge that is incorrect. We don't consider 
it likely, but concede that it is possible that we failed to see the reason why our prior 
knowledge will not applicable to the new data. If our prior is misspecified, we don't 
really have much of an idea what values 0 should take. In that case the prior for 6' is 
g1 (Q), which is either a very vague conjugate prior or a flat prior. Let go(Q/yl, . I . , yn) 
be the posterior distribution of 6' given the observations when we start with gO(6') as 
the prior. Similarly, we let g1 (Blyl, . . . , yn) be the posterior distribution of 0, given 

320 
ROBUST BAYESIAN METHODS 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
Figure 76.2 Andrea's prior, likelihood, and posterior. 
the observations when we start with g1(6) as the prior: 
Si(6IYl,..'rYn) 
gi(~)f(Yl,..~,Yni6). 
These are found using the simple updating rules, since we are using priors that are 
either from the conjugate family or are flat. 
The Mixture Prior 
We introduce a new parameter, I ,  that takes two possible values. If i = 0, then 6 
comes from gO(6). However, if i = 1, then 6 comes from gl(6). The conditional 
prior probability of 6 given i is 
We let the prior probability distribution of I be P(I = 0) = pa, where po is some 
high value like .9, .95, or .99, because we think our prior go(@) is correct. The prior 
probability that our prior is misspecified is pl = 1 - pa. The joint prior distribution 
of 6' and I is 
g(6, i) = p i  x gi(6) for 
i = 0 , l .  
We note that this joint distribution is continuous in the parameter 6 and discrete in 
the parameter I .  The marginal prior density of the random variable 0 is found by 

BAYES’ THEOREM WITH MIXTURE PRIORS 
321 
marginalizing (summing I over all possible values) the joint density. It has a mixture 
prior distribution since its density 
(16.1) 
is a mixture of the two prior densities. 
The Joint Posterior 
The joint posterior distribution of 8, I given the observations y1, . . . , yn is propor- 
tional to the joint prior times the joint likelihood. This gives 
g(8,ilyl, ' .  . , yn) = c x g(8, i) x f(yl,. . . , Y n p ,  i) for i = 0, 1 
for some constant c. But the sample only depends on 8, not on i, so the joint posterior 
g(8,ily1,...,yn) = c x p i g i ( 8 ) f ( y l , ~ ~ ~ , y , ~ e )  
for i = O , I  
= cxpihi(8,yl,...,yn) for i = O , 1 ,  
wherehi(8,yl,...,yn) = gi(e)f(yl,. . . ,yn18)isthejointdistributionoftheparam- 
eter and the data, when gi(8) is the correct prior. The marginal posterior probability 
P(I = ijyl, . . . , yn) is found by integrating 8 out of the joint posterior: 
c x pi / h i W ,  
yl,. . . , Y n w  
c x Pzfi(Y1,. . ., Yn) 
- 
- 
= 
for z = 0,1, where fi (y1, . . . , yn) is the marginal probability (or probability density) 
of the data when gi (8) is the correct prior. The posterior probabilities sum to 1, and 
the constant c cancels, so 
Pifi(Yl,...,Yn) 
Ci=oPi.fi(Yl,...lYn) 
P ( I  = 4 Y 1 , .  . .,Yn) = 
1 
These can be easily evaluated. 
The Mixture Posterior 
We find the marginal posterior of 8 by summing all possible values of a out of the 
joint posterior: 
1 

322 
ROBUST BAYESIAN METHODS 
1 
I 
I 
I 
I 
I 
I 
I 
I 
, 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure 76.3 Ben’s mixture prior and components. 
But there is another way the joint posterior can be rearranged from conditional 
probabilities: 
where g(8li, y1,. . . , yn) = gi(8jy1,. . . , yn) is the posterior distribution when we 
started with gi (6) as the prior. Thus the marginal posterior of 8 is 
This is the mixture of the two posteriors, where the weights are the posterior proba- 
bilities of the two values of a given the data. 
Example 28 (continued) One of Archie’s friends, Ben, decided that he would re- 
analyze Archie’s data with a mixture priol: He let go be the same beta(14.4,9.6) 
prior that Archie used. He let g1 be the (uniform) beta(1,l) priol: He let the prior 
probability po = .95. Ben’s mixture prior and its components are shown in Figure 
16.3. His mixture prior is quite similar to Archie’s. Howevel; it has heavier weight 
in the tails. This gives makes his prior robust against prior misspeciJication. In 
this case, hi(7r, y) is a product of a beta times a binomial. Of course, we are only 

BAYES' THEOREM WITH MIXTURE PRIORS 
323 
interested in y = 25, the value that occurred: 
and 
We recognize each of these as a constant times a beta distribution. So integrating 
them with respect to 7r gives 
and 1' hl(7r,y = 25) d n  = (z) 
25!75! 
x 1'7r25(l - 7
~
)
~
~
 
d7r 
r p 6 ) r  (76) 
= (G) 
x 
r(102) 
* 
Remember that r ( a )  = (a - 1) x r ( a  - 1) and $a is an integez r ( a )  = (a - l)! . 
The second integral is easily evaluated and gives 
1 
fl(y = 25) = 
hl(7r,y = 25) d7r = - 
= 9.90099 x 
J,l 
101 
We can evaluate thejrst integral numerically: 
fo(y = 25) = 
ho(n, y = 25) d n  = 2.484 x lop4. 
I' 
So the posteriorprobabilities are P(I = 0125) = 0.323 and P( I  = 1/25) = 0.677. 
Theposterior distribution is the mixture g(nl25) = .323 ~go(n/25)+.677 
x 91(7r/25), 
where go(7rly) and g1 (niy) are the conjugate posterior distributions found using go 
and g1 as the respective priors. Ben's mixture posterior distribution and its two 
components is shown in Figure 16.4. Ben's prior and posteriol; together with the 
likelihood is shown in Figure 16.5. When the prior and likelihooddisagree, we should 
go with the likelihood because it is from the data. SuperJicially, Ben's prior looks 

324 
ROBUST BAYESIAN METHODS 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
1.0 
Figure 16.4 Ben’s mixture posterior and its two components. 
very similar to Archie’s prior However, it has a heavier tail allowed by the mixture, 
and this has allowed his posterior to be very close to the likelihood. We see that this 
is much more satisfactory than ArchieS analysis shown in Figure 16.1. 
Example 29 (continued) Andrea’s friend Caitlin looked at Figure 16.2 and told 
her it was not satisfactory. The values given high posterior probability were not 
supported strongly either by the data or by the prior She considered it likely that the 
prior was misspecified. She said to protect against that, she would do the analysis 
using a mixture of normalpriors. go(@) was the same as Andrea’s, normal(8.5, .72), 
and gl(f?) would be normal (8.5, (4 x .7)2, which has the same mean as Andrea’s 
prior, but with the standard deviation 4 times as large. She allows prior probability 
.05 that Andrea’s prior was misspecijied. Caitlin’s mixture prior and its components 
are shown in Figure 16.6. We see that her mixture prior appears very similar to 
Andrea’s except there is more weight in the tail regions. Caitlin’sposteriorgo(6ijj) is 
normal(6.334, .376g2), the same as forAndrea. Caitlin’s posterior when the original 
prior was misspecijied gl(eig) is normal(5.526, .4416’) where the parameters are 
found by the simple updating rules for the normal. In the normal case 
hz(P1 Y1,. . . > Yn) 
cx 
x f(3IP) 
- & ( P - d  
e-*(g-p)z 
c x e  
1 
> 
where m, and s: are the mean and variance of the prior distribution gz(p). The 
integral 
hz(pl y1 , . . , yn) dp gives the unconditional probability of the sample, 
when g, is the correctprior We multiply out the two terms and then rearrange all the 

BAYES THEOREM WITH MIXTURE PRIORS 
325 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure 76.5 Ben’s mixture prior, likelihood, and mixture posterior. 
terms containing p, which is normal and integrates. The terms that are left simplify 
to 
fa(g) = 1 ht(p, g) + 
Z(e$ez,,) 
( E M % ) 2  
c( Jmxe 
3 
- 
1 
which we recognize as a normal density with mean ma and variance $ + sp. In this 
example, mo = 8.5, .$ = .7’, ml = 8.5, .$ = (4 x .7)2), u2 = 1, and n = 5. The 
data are summarized by the value g = 5.45 that occurred in the sample. Plugging in 
these values we get P(I = O/g = 5.45) = .12 and P ( I  = 115 = 5.45) = .88. Thus 
Caitlin’sposterior is the mixture .12 x go(pIg) + .88 x g l ( & j ) .  Caitlin’s mixture 
posterior and its components are given in Figure 16.7. Caitlin ’sprios likelihood, and 
posterior are shown in Figure 16.8. Comparing this with Andrea’s analysis shown in 
Figure 16.2, we see that using mixtures has given her a posterior that is much closer 
to the likelihood than the one obtained with the original misspecijied priol: This is a 
much more satisfactory result. 
Summary 
Our prior represents our prior belief about the parameter before looking at the data 
from this experiment. We should be getting our prior from past data from similar 
experiments. However, if we think an experiment is similar, but it is not, our prior 

326 
ROBUST BAYESIAN METHODS 
1 
3
4
5
6
7
8
9
1
0
1
1
1
2
 
Figure 16.6 Caitlin’s mixture prior and its components. 
can be quite misspecified. We may think we know a lot about the parameter, but 
what we think is wrong. That makes the prior quite precise, but wrong. It will be 
quite a distance from the likelihood. The posterior will be in between, and will give 
high probability to values neither supported by the data or the prior. That is not 
satisfactory. If there is a conflict between the prior and the data, we should go with 
the data. 
We introduce a indicator random variable that we give a small prior probability 
of indicating our original prior is misspecified. The mixture prior we use is P ( I  = 
0) x go(@) + P(I = 1) x gl(@), where go and g1 are the original prior and a more 
widely spread prior, respectively. We find the joint posterior of distribution of I and 
@ given the data. The marginal posterior distribution of 8, given the data, is found by 
marginalizing the indicator variable out. It will be a the mixture distribution 
This posterior is very robust against a misspecified prior. If the original prior is 
correct, the mixture posterior will be very similar to the original posterior. However, 
if the original prior is very far from the likelihood, the posterior probability p ( i  = 
Olyl, . . .  ,yn) will be very small, and the mixture posterior will be close to the 
likelihood. This has resolved the conflict between the original prior and the likelihood 
by giving much more weight to the likelihood. 

MAIN POINTS 
327 
j 
1 
I 
1 
~ 
I 
3
4
5
6
7
8
9
1
0
1
1
1
2
 
I 
I 
I 
I 
I 
I 
I 
1
7
 
Figure 16.7 Caitlin's mixture posterior and its two components. 
Main Points 
0 If the prior places high probability on values that have low likelihood, and 
low probability on values that have high likelihood, the posterior will place 
high probability on values that are not supported either by the prior or by the 
likelihood. This is not satisfactory. 
0 This could have been caused by a misspecified prior that arose when the scientist 
based hisher prior on past data, which had been generated by a process that 
differs from the process that will generate the new data in some important way 
that the scientist failed to take into consideration. 
0 Using mixture priors protects against this possible misspecification of the prior. 
We use mixtures of conjugate priors. We do this by introducing a mixture index 
random variable that takes on the values 0 or 1. The mixture prior is 
where go(@) is the original prior we believe in, and g1 is another prior that 
has heavier tails and thus allows for our original prior being wrong. The 
respective posteriors that arise using each of the priors are go(Blyl,. . . , yn) 
and 91(Q/Y1,. 
. ' 1 Yn). 
0 We give the original prior go high prior probability by letting the prior probabil- 
itypo = P(I = 0) behighandthepriorprobabilitypl = (1-PO) = P(I = 1) 

328 
ROBUST BAYESIAN METHODS 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
3
4
5
6
7
8
9
1
0
1
1
1
2
 
Figure 76.8 Caitlin’s mixture prior, the likelihood, and her mixture posterior. 
is low. We think the original prior is correct, but have allowed a small proba- 
bility that we have it wrong. 
Bayes’ theorem is used on the mixture prior to determine a mixture posterior. 
The mixture index variable is a nuisance parameter and is marginalized out. 
If the likelihood has most of its value far from the original prior, the mixture 
posterior will be close to the likelihood. This is a much more satisfactory result. 
When the prior and likelihood are conflicting, we should base our posterior 
belief mostly on the likelihood, because it is based on the data. Our prior was 
based on faulty reasoning from past data that failed to note some important 
change in the process we are drawing the data from. 
The mixture posterior is a mixture of the two posteriors, where the mixing 
proportions P(1 = i) for i = 0,1, are proportional to the prior probability 
times the the marginal probability (or probability density) evaluated at the data 
that occurred. 
P(I = i )  
0: pi x fi(y1,. . . yn) 
for 
i = 0,1 
0 They sum to 1, so 

EXERCISES 
329 
Exercises 
16.1 You are going to conduct a survey of the voters in the city you live in. They 
are being asked whether or not the city should build a new convention facility. 
You believe that most of the voters will disapprove the proposal because it 
may lead to increased property taxes for residents. As a resident of the city, 
you have been hearing discussion about this proposal, and most people have 
voiced disapproval. You think that only about 35% of the voters will support 
this proposal, so you decide that a betu(7,13) summarizes your prior belief. 
However, you have a nagging doubt that the group of people you have heard 
voicing their opinions is representative of the city voters. Because of this, you 
decide to use a mixture prior: 
go(.) 
if 
i = 0 ,  
g1(T) if 
i = 1. 
where go(T) is the betu(7,13) density, and g1(T) is the betu(1,l) (uniform) 
density. The prior probability P(I = 0) = .95. You take a random sample of 
n = 200 registered voters who live in the city. Of these, y = 10 support the 
proposal. 
(a) Calculate the posterior distribution of T when go(T) is the prior. 
(b) Calculate the posterior distribution of T when g1(T) is the prior. 
(c) Calculate the posterior probability P(I = OIY). 
(d) Calculate the marginal posterior g(.rrlY). 
16.2 You are going to conduct a survey of the students in your university to find 
out whether they read the student newspaper regularly. Based on your friends 
opinions, you think that a strong majority of the students do read the paper 
regularly. However, you are not sure your friends are representative sample of 
students. Because of this, you decide to use a mixture prior. 
where go(..) 
is the betu(20,5) density, and g1(T) is the beta(1,l) (uniform) 
density. The prior probability P(I = 0) = .95. You take a random sample 
of n = 100 students. Of these, y = 41 say they read the student newspaper 
regularly. 
(a) Calculate the posterior distribution of T when gO(T) is the prior. 
(b) Calculate the posterior distribution of T when g1 (T) 
is the prior. 
(c) Calculate the posterior probability P(I = OIY). 
(d) Calculate the marginal posterior g(TlY). 

330 
ROBUST BAYESIAN METHODS 
16.3 You are going to take a sample of measurements of specific gravity of a chemical 
product being produced. You know the specific gravity measurements are 
approximately normal ( p ,  u2) where u2 = .0052. You have precise normal 
(1.10, .0Ol2) prior for p because the manufacturing process is quite stable. 
However, you have a nagging doubt about whether the process is correctly 
adjusted, so you decide to use a mixture prior. You let go(p) be your precise 
normal (1.10, .0Ol2) prior, you let gl(p) be a normal (1.10, .0l2), and you 
let po = .95. You take a random sample of product and measure the specific 
gravity. The measurements are 
1.10352 1.10247 
1.10305 
1.10415 
1.10382 
1.10187 
(a) Calculate the joint posterior distribution of I and p given the data. 
(b) Calculate the posterior probability P(I = 01~1, . . . ,ye). 
(c) Calculate the marginal posterior g(ply1, . . . , y6). 
16.4 You are going to take a sample of 500g blocks of cheese. You know they 
are approximately normal(p, u2) where u2 = 22. You have a precise nor- 
mal(502, 12) prior for p because this is what the the process is set for. However, 
you have a nagging doubt that maybe the machine needs adjustment, so you 
decide to use a mixture prior. You let go(p) be your precise normal(502, 12) 
prior, you let gl(p) be a normal(502, 22), and you let pa = .95. You take a 
random sample of ten blocks of cheese and weigh them. The measurements 
are 
501.5 
499.1 
498.5 
499.9 
500.4 
498.9 
498.4 
497.9 
498.8 
498.6 
(a) Calculate the joint posterior distribution of I and p given the data. 
(b) Calculate the posterior probability P(I = O(y1, . . . , ylo). 
(c) Calculate the marginal posterior g(pL(y1,. . . , ylo). 
Computer Exercises 
16.1 We will use the Minitab macro BinoMixPmac or the equivalent R function 
to find the posterior distribution of T given an observation y from the bi- 
nomial(n, T )  distribution when we use a mixture prior for T .  Suppose our 
prior experience leads us to believe a beta(7,13) prior would be appropriate. 
However we have a nagging suspicion that our experience was under differ- 
ent circumstances, so our prior belief may be quite incorrect and we need 
a fall-back position. We decide to use a mixture prior where go(T) is the 
beta(7,13) and gI(.rr) is the beta(1,l) distribution, and the prior probability 

COMPUTER EXERCISES 
337 
P(I = 0) = .95. Suppose we take a random sample of n = 100 and observe 
y = 76 successes. 
(a) Use BinoMixp,rnac or the equivalent R function to find the posterior 
(b) Find a 95% Bayesian credible interval for n-. 
(c) Test the hypothesis HO : T 5 .5 vs. H I  : T > .5 at the 5% level of 
distribution g (n- 1 y). 
significance. 
16.2 We are going to observe the number of "successes" in n = 100 independent 
trials. We have prior experience and believe that a beta(6,14) summarizes our 
prior experience. However, we consider that our prior experience may have 
occurred under different conditions, so our prior may be bad. We decide to 
use a mixture prior where go(n-) is the beta(6,14) and gI(T) is the beta(1,l) 
distribution, and the prior probability P(I = 0) = .95. Suppose we take a 
random sample of n = 100 and observe y = 36 successes. 
(a) Use BinoMixpmac or the equivalent R function to find the posterior 
(b) Find a 95% Bayesian credible interval for n-. 
(c) Test the hypothesis HO : n- 5 .5 vs. H I  : n- > .5 at the 5% level of 
distribution g(n-ly). 
significance. 
16.3 We will use the Minitab macro NormMirPrnmac or the equivalent R function 
to find the posterior distribution of p given a random sample y1,. . . , yn from 
the normal(p, u2) distribution where we know the standard deviation u = 5. 
when we use a mixture prior for p. Suppose that our prior experience in 
similar situations leads us to believe that the prior distribution should be nor- 
rnal(1000, 52). However, we consider that the prior experience may have been 
under different circumstances, so we decide to use a mixture prior where go(p) 
is the normal(1000, 52) and gl(p) is the noma1(1000, 152) distribution, and 
the prior probability P(I = 0) = .95. We take a random sample of n = 10 
observations. They are 
1030 
1023 
1027 
1022 
1023 
1023 
1030 
1018 
1015 
101 1 
(a) Use NorrnMixp.rnac or the equivalent R function to find the posterior 
(b) Find a 95% Bayesian credible interval for p. 
(c) Test the hypothesis HO : p 5 1000 vs. HI : p > 1000 at the 5% level of 
16.4 We are taking a random sample from the nonnal(p,u2) distribution where 
we know the standard deviation u = 4. Suppose that our prior experience 
distribution g (ply). 
significance. 

332 
ROBUST BAYESIAN METHODS 
in similar situations leads us to believe that the prior distribution should be 
norm~1(255,4~). 
However, we consider that the prior experience may have 
been under different circumstances, so we decide to use a mixture prior where 
go(p) is the norm~1(255,4~) 
and gI(p) is the n0nna1(255,12~) distribution, 
and the prior probability P(I = 0) = .95. We take a random sample of n = 10 
observations. They are 
249 
258 
255 
26 1 
259 
254 
26 1 
256 
253 
254 
(a) Use NormMixp.mac or the equivalent R function to find the posterior 
(b) Find a 95% Bayesian credible interval for p. 
(c) Test the hypothesis HO : p 5 1000 vs. H I  : p > 1000 at the 5% level of 
distribution g(p1 y). 
significance. 

A 
Introduction to Calculus 
FUNCTIONS 
A function f (x) defined on a set of real numbers, A, is a rule that associates each 
real number x in the set A with one and only one other real number y. The number x 
is associated with the number y by the rule y = f(x). The set A is called the domain 
of the function, and the set of all y that are associated with members of A is called 
the range of the function. 
Often the rule is expressed as an equation. For example, the domain A might be all 
positive real numbers, and the function f(x) = log,(x) associates each element of 
A with its natural logarithm. The range of this function is the set of all real numbers. 
For a second example, the domain A might be the set of real numbers in the 
interval [0,1] and the function f (x) = x4 x (1 - x ) ~ .  
The range of this function is 
the set of real numbers in the interval [0, .44 x .S6]. 
Note that the variable name is merely a cypher, or a place holder. f(x) = x2 and 
f (2) = z2 are the same function, where the rule of the function is associate each 
number with its square. The function is the rule by which the association is made. 
We could refer to the function as f without the variable name, but usually we will 
refer to it as f(x). The notation f (x) is used for two things. First, it represents the 
specific value associated by the function f to the point x. Second, it represents the 
function by giving the rule which it uses. Generally, there is no confusion as it is 
clear from the context which meaning we are using. 
Introduction to Bayesian Stafisrics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
333 

334 
INTRODUCTION TO CALCULUS 
0.0010 - 
0.0005 - 
0.0000 - 
I 
I 
I 
0.0 
0.5 
1 .o 
figureA.7 
Graph of function f(x) = x4 x (1 - z ) ~ .  
Combining Functions 
We can combine two functions algebraically. Let f and g be functions having the 
same domain A, and let kl and k2 be constants. The function h = Icl x f associates 
a number z with y = k l f ( z ) .  Similarly, the function s = lclf f 
k2g associates the 
number z with y = k1 x f(z) f k2 x g(z). The function u = f x g associates 
a number 5 with y = f ( z )  x g(z). Similarly, the function v = $ associates the 
number z with y = fo. 
If function g has domain A and function f has domain that is a subset of the range 
of the function g, then the composite function (function of a function) w = f ( g )  
associates a number 5 with y = f ( g ( z ) ) .  
d") 
Graph of a Function 
The graph of the function f is the graph of the equation y = f(z). 
The graph consists 
of all points (z, f(z)), 
where z E A plotted in the coordinate plane. The graph of 
the function f defined on the closed interval A = [0,1] where f ( z )  = z4 x (1 - 
is shown in Figure A.l. The graph of the function g defined on the open interval 
A = (0, l), where g(z) = 2-4 x (1 - z)-z, is shown in Figure A.2. 

INTRODUCTION TO CALCULUS 
335 
I 
I 
I 
0.0 
0.5 
1 .o 
figure A.2 
Graph of function f(z) 
= z-4 x (1 - z)-h. 
Limit of a Function 
The limit of a function at a point is one of the fundamental tools of calculus. We 
write 
lim f(x) = b 
x-+a 
to indicate that b is the limit of the function f when x approaches a. Intuitively, 
this means that as we take x values closer and closer to (but not equal to) a, their 
corresponding values of f(x) are getting closer and closer to b. We note that the 
function f(x) does not have to be defined at a to have a limit at a. For example, 
0 is not in the domain A of the function f(x) = 9 because division by 0 is not 
allowed. Yet 
sin(x) 
lim 
~ 
= I  
z-+o 
x 
as seen in Figure A.3. We see that if we want to be within a specified closeness to 
y = 1, we can find a degree of closeness to x = 0 such that all points x that are 
within that degree of closeness to x = 0 and are in the domain A will have f(x) 
values within that specified closeness to y = 1. 
We should note that a function may not have a limit at a point a. For example, the 
function f(x) = cos(l/z) does not have a limit at 3c = 0. This is shown in Figure 
A.4, which shows the function at three scales. No matter how close we get to x = 0, 
the possible f(z) values always range from -1 to 1. 

336 
INTRODUCTION TO CALCULUS 
0.5 - 
0.0 - 
I 
1 
1 
Figure A.4 
numbers except for z = 0. 
Graph of f(z) = cos (:) 
at three scales. Note that f is defined at all real 
Theorem 1 Limit Theorems: 
Let f (x) and g(x) befunctions that each have limit at a, and let kl and kz be scalars. 
1. Limit of a sum (difference) offunctions 
lim [kl x f(x) i 
k2 x g(z)] = kl x lim f(z) 
i. k2 x lim g(z). 
x+a 
x+a 
x+a 
2. Limit of a product offunctions 
lim[f(z) x g(z)] = lirn f(x) x lim g(x) . 
x+a 
x+a 
x-a 

lNTRODUCTlON TO CALCULUS 
337 
3. Limit of a quotient offunctions 
4. Limit of a power of afunction 
lim [fn(x)] = [ lim f(x)]" 
x-ia 
x+a 
Let g(x) be afunction that has limit at a equal to b, and let f (x) be afunction that 
has a limit at b. Let ~ ( x )  
= f(g(x)) be a compositefunction. 
5. Limit of a compositefunction 
CONTINUOUS FUNCTIONS 
A function f (x) is continuous at point a if and only if 
lim f(z) = f ( a ) .  
x-a 
This says three things. First, the function has a limit at z = a. Second, a is in the 
domain of the function, so f ( a )  is defined. Third, the limit of the function at x = a 
is equal to the value of the function at x = a. If we want f(x) to be some specified 
closeness to f(a), we can find a degree of closeness so that for all x within that 
degree of closeness to a, f(x) is within the specified closeness to f(a). 
A function that is continuous at all values in an interval is said to be continuous 
over the interval. Sometimes a continuous function is said to be a function that "can 
be graphed over the interval without lifting the pencil." Strictly speaking, this is not 
true for all continuous functions. However, it is true for all functions with formulas 
made from polynomial, exponential, or logarithmic terms. 

338 
INTRODUCTION TO CALCULUS 
Theorem 2 Let f (x) and g(x) be continuousfunctions, and let Icl and k2 be scalars. 
Then: 
I .  A linear function of continuous functions 
2. A product of continuousfunctions 
3. A quotient of continuous finctions 
4. And a composite function of continuous functions 
4 x 1  = f(9(z)) 
are all continuous functions on their range of dejinition. 
Minima and Maxima of Continuous Functions 
One of the main achievements of calculus is that it gives us a method for finding 
where a continuous function will achieve minimum and/or maximum values. 
Suppose f (x) is a continuous function defined on a continuous domain A. The 
function achieves a local maximum at the point x = c if and only if f(x) 5 f ( c )  for 
all points x E A that are sufficiently close to c. Then f (c) is called a local maximum 
of the function. The largest local maximum of a function in the domain A is called 
the global maximum of the function. 
Similarly, the function achieves a local minimum at point z = c if and only if 
f(z) 2 f ( c )  for all points 3: E A that are sufficiently close to c, and f ( c )  is called 
a local minimum of the function. The smallest local minimum of a function in the 
domain A is called the global minimum of the function. 
A continuous function defined on a domain A that is a closed interval [u, b], 
always achieves a global maximum ( and minimum). It can occur at either one of the 
endpoints 3: = u or x = b, or an interior point c E (a, b). For example, the function 
f (x) = x4 x (1 - z ) ~  
defined on A = [0, 11 achieves a global maximum at x = 2 
and a global minimum at z = 0 and z = 1 as can be seen in Figure A. 1. 
A continuous function defined on a domain A that is an open interval (a, b) may 
or may not achieve either a global maximum or minimum. For example, the function 
f(x) = 
(L-l)l,2 
defined on the open interval (0,l) achieves a global minimum 
at x = .5, but it does not achieve a global maximum as can be seen from Figure A.2. 

INTRODUCTION TO CALCULUS 
339 
Figure A.5 
The derivative at a point is the slope of the tangent to the curve at that point. 
DIFFERENTIATION 
The first important use of the concept of a limit is finding the derivative of a continuous 
function. The process of finding the derivative is known as differentiation, and it is 
extremely useful in finding values of x where the function takes a minimum or 
maximum. 
We assume that f(z) is a continuous function whose domain is an interval of the 
real line. The derivative of the function at x = c, a point in the interval is 
f’(c) = lim 
h-+O 
if this limit exists. When the derivative exists at x = c, we say the function f(x) is 
diferentiable at x = c. If this limit does not exist, the function f(x) does not have a 
derivative at z = c. The limit is not easily evaluated, as plugging in h = 0 leaves the 
quotient 
which is undefined. We also use the notation for the derivative at point c 
We note that the derivative at point x = cis the slope of the curve y = f (x) evaluated 
at x = c. It gives the “instantaneous rate of change“ in the curve at x = c. This 
is shown in Figure AS, where f(x), the line joining the point (c, f ( c ) )  and point 
(c + h, f ( c  + h)) for decreasing values of h and its tangent at c are graphed. 
The Derivative Function 
When the function f (x) has a derivative at all points in an interval, the function 
is called the derivative function. In this case we say that f (x) is a differentiable 
function. The derivative function is sometimes denoted $. The derivatives of some 
elementary functions are given in the following table: 

340 
INTRODUCTION TO CALCULUS 
f (x) 
a x x  
X b  
ex 
loge ( X I  
sin(x) 
cos(x) 
tan(x) 
a 
b x xb-’ 
ex 
1 - 
X 
cos(x) 
- sin(x) 
- sec2 (x) 
The derivatives of more complicated functions can be found from these using the 
following theorems: 
Theorem 3 Let f (x) and g be difSerentiablefunctions on an interval, and let kl and 
k2 be constants. 
I .  The derivative of a constant times afunction is the constant times the derivative 
of the function. Let h(x) = kl x f (x). Then h(x) is also a differentiable 
function on the interval, and 
h’(x) = kl x f’(x). 
2. The sum (difference) rule. 
Let s(x) = k1 x f ( x )  f 
k2 x g(x). Then s(x) is also a differentiablefunction 
on the interval, and 
3. The product rule. 
Let u(x) = f ( x )  x g(x). Then u(x) is a differentiablefunction, and 
u’(x) = f ( x )  x d(.) + f’(x) x 9(.) 
’ 
4. The quotient rule. 
Let ~ ( x )  
= #. Then u(x) is also a dgerentiablefunction on the interval, 
and 
Theorem 4 The chain rule. 
Let f ( x )  and g(x) be differentiable functions (dejined over appropriate intervals) 
and let ~ ( x )  
= f(g(x)). Then W(Z) is a differentiablefunction and 
w’(x) = f ’ ( d z ) )  x g’(x). 

INTRODUCTION TO CALCULUS 
341 
Higher Derivatives 
The second derivative of a differentiable function f(x) at a point x = c is the 
derivative of the derivative function f’(x) at the point. The second derivative is given 
if it exists. If the second derivative exists for all points x in an interval, then f”(x) 
is the second derivative function over the interval. Other notation for the second 
derivative at point c and for the second derivative function are 
d2 
dx 
f”(c) = f q c )  = 
and f(’)(x) = T f ( x ) .  
Similarly, the kth derivative is the derivative of the k - lth derivative function 
if it exists. 
Critical Points 
For a function f(x) that is differentiable over an open interval (a, b), the derivative 
function f’(x) is the slope of the curve y = f(x) at each x-value in the interval. This 
gives a method of finding where the minimum and maximum values of the function 
occur. The function will achieve its minimum and maximum at points where the 
derivative equals 0. When x = c is a solution of the equation 
f’b) = 0 1 
c is called a critical point of the function f(z). 
The critical points may lead to local 
maximum or minimum, or to global maximum or minimum, or they may be points 
of inflection. A point of inflection is where the function changes from being concave 
to convex, or vice versa. 
Theorem 5 First derivative test: Iff (x) is a continuous differentiablefunction over 
an interval (a, b) having derivative function f’(x), which is dejned on the same 
interval. Suppose c is a critical point of thefunction. By dejinition, f ‘(c) = 0. 
1. The function achieves a unique local maximum at x = c 8 for all points x that 
are suflciently close to c, 
when x < c 
then 
f’(x) > 0 and 
when x > c 
then 
f’(x) < 0. 
2. Similarly, the function achieves a unique local minimum at x = c $ for all 
points x that are sufficiently close to c, 
when x < c 
then 
f’(x) < 0 and 
when x > c 
then 
f’(x) > 0. 

342 
INTRODUCTION TO CALCULUS 
3. Thefunction has a point of injlection at critical point x = c $for all points x 
that are suficiently close to c, either 
when x < c 
then 
f’(x) < 0 and 
when x > c 
then 
f’(x) < 0 
when x < c 
then 
f’(x) > 0 and 
when z > c 
then 
f’(z) > 0. 
or 
At a point of inflection, thefunction either stops increasing, and then resumes 
increasing, or it stops decreasing, and then resumes decreasing. 
For example, the function f(x) = x3, and its derivative f ’ ( x )  = 3 x x2 are shown 
in Figure A.6. We see that the derivative function f’(z) 
= 32’ is positive for x < 0, 
so the function f (x) = x3 is increasing for x < 0. The derivative function is positive 
for x > 0 so the function is also increasing for x > 0. However at x = 0, the 
derivative function equals 0, so the original function is not increasing at x = 0. Thus 
the function f(x) = x3 has a point of inflection at z = 0. 
Theorem 6 Second derivative test: Iff (x) is a continuous differentiablefunction 
over an interval (a, b) having first derivative function f ‘(x) and second derivative 
function f (’)(x) both defined on the same interval. Suppose c is a critical point of 
thefunction. By definition, f’(c) = 0. 
I .  Thefunction achieves a maximum at x = c i f f  (”(c) < 0. 
2. Thefunction achieves a minimum at x = c i f f  (2)(c) > 0. 
INTEGRATION 
The second main use of calculus is finding the area under a curve using integration. It 
turns out that integration is the inverse of differentiation. Suppose f (x) is a function 
defined on an interval [a, b]. Let the function F ( z )  be an antiderivative of f(z). 
That 
means the derivative function F’(x) = f(z). 
Note that the antiderivative of f(z) 
is not unique. The function F(x) + c will also be an antiderivative of f(x). The 
antiderivative is also called the indefinite integral. 
The Definite Integral: Finding the Area under a Curve 
Suppose we have a nonnegative’ continuous function f (x) 
defined on a closed interval 
[a, b]. f(x) 2 0 for all x E [a, b]. Suppose we partition the the interval [a, b] using 
the partition 50, 
X I ,  . . . , x,, where xo = a and zn = b and x2 < x,+1. Note that 
the partition does not have to have equal length intervals. Let the minimum and 
maximum value of f(x) in each interval be 
I ,  = 
sup 
f(x) and 
m, = 
inf 
f(z), 
zE[s,-l ,+.I 
5 E [ z z  - 1 , z .I 
‘The requirement that f(z) 
be nonnegative is not strictly necessary. However, since we are using the 
definite integral to find the area under probability density functions that are nonnegative, we will impose 
the condition. 

lNTRODUCTlON TO CALCULUS 
343 
I 
I 
I 
-1 
0 
1 
I 
I 
-1 
0 
1 
Figure A.6 
Graph of f(z) = z3 and its derivative. The derivative function is negative 
where the original function is increasing, and it is positive where the original function is 
increasing We see the original function has a point of inflection at z = 0. 
where sup is the least upper bound, and inf is the greatest lower bound. Then the 
area under the curve y = f(x) between x = a and x = b lies between the lower sum 

344 
INTRODUCTION TO CALCULUS 
I 
, 
I 
4 
I 
I 
j 
Figure A.7 
Lower and upper sums over a partition and its refinement. The lower sum has 
increased and the upper sum has decreased in the refinement. The area under the curve is 
always between the lower and upper sums. 
and the upper sum 
n 
xi- 1) 
i=l 
We can refine the partition by adding one more x value to it. Let xi, . . . , 
be a 
refinement of the partition XI,. . . , 2,. Then xb = 50, 
= xn, xi = xi for all 
z < k ,  and 
= xi for all i > k. xk is the new value added to the partition. In the 
lower and upper sum, all the bars except for the kth are unchanged. The kth bar has 
been replaced by two bars in the refinement. Clearly, 
Mxb ,..., x ~ + ,  5 Mxo ...., xn 
and 
Lxb ,.._) 
L LXO? ...) I, 
The lower and upper sums for a partition and its refinement are shown in Figure A.I. 
We see that refining a partition must make tighter bounds on the area under the curve. 
Next we will show that for any continuous function defined on a closed interval 
[a, b] ,we can find a partition 20, . . . , xn for some n that will make the difference 
between the upper sum and the lower sum as close to zero as we wish. Suppose 
E > 0 is the number we want the difference to be less than. We draw lines 6 = 
apart parallel to the horizontal (2) axis. (Since the function is defined on the close 
interval, its maximum and minimum are both finite.) Thus a finite number of the 
horizontal lines will intercept the curve y = f(x) over the interval [a, b]. Where 
one of the lines intercepts the curve, draw a vertical line down to the horizontal axis. 
The 2 values where these vertical lines hit the horizontal axis are the points for our 
partition. For example, the function f(x) = 1 + d
m
 
is defined on the interval 
[0,2]. The difference between the upper sum and the lower sum for the partition for 
that E is given by 
- 
Mxo ,..., 5, - Lo 
,..., xn - 6 x [(Xl - xo) + (z2 - 51) +. . . + (2, - xn-l)] 
= 6 x [ b - a ]  

/NTRODUCTION TO CALCULUS 
345 
0 
1 
2 
........................ 
....... 
0 
1 
2 
FigureA.8 
its refinement where 2 = 1. 
The partition induced for the function f(z) = 1 + v
’
n
 where 1 = 1 and 
We can make this difference as small as we want to by choosing E > 0 small enough. 
for k = 1,. . . , co. This gives us a sequence of partitions such that 
limk,, 
Ek = 0. Hence 
Let Ek = 
lim Mxo ,..., x,k - Lxo ,..., x,k = 0 .  
k-tw 
The partitions for e l  and €2 are shown in Figure A.8. Note that bk = &. 
That means that the area under the curve is the least upper bound for the lower 
sum, and the greatest lower bound for the upper sum. We call it the definite integral 
and denote it 
I” f (x) dx. 
Note the variable z in the formula above is a dummy variable: 
Basic Properties of Definite Integrals 
Theorem 7 Let f(x) and g(z) befunctions defined on the interval [a, b], and let c 
be a constant. Then the following properties hold. 
1. The definite integral of a constant times a function is the constant times the 
dejnite integral of thefunction: 
J,” cf(x) dx = c 
f(x) dx . 
b 
2. The dejnite integral of a sum of twofunctions is a sum of the definite integrals 
of the twofunctions: 

346 
INTRODUCTION TO CALCULUS 
Fundamental Theorem of Calculus 
The methods of finding extreme values by differentiation and finding area under a 
curve by integration were known before the time of Newton and Liebniz. Newton and 
Liebniz independently discovered the fundamental theorem of calculus that connects 
differentiation and integration. Because each was unaware of the others work, they 
are both credited with the discovery of the calculus. 
Theorem 8 Fundamental theorem of calculus. Let f (x) be a continuous function 
dejned on a closed interval. Then: 
1. Thefunction has antiderivative in the interval. 
2. Ifa and b are two numbers in the closed interval such that a < b, and F(x) is 
any antiderivativefinction off (x), then 
Jdb f (x)dx = F(b) - F(a) 
Proof: 
For x E (a, b), dejne thefunction 
I(x) = LX 
f (XI dx . 
Thisfunction shows the area under the curve y = f (x) between a and x. Note that 
the area under the curve is additive over an extended region from a to x + h: 
f (x) dx = Jdx f(x) dx + lx+h 
f (x) dx. 
Jdx+h 
By definition, the derivative of thefunction I(x) is 
Sxx+h f (x) dx 
h 
= lim 
I(. + h) - I(x) 
h 
h-0 
I’(x) = lim 
h-+O 
In the limit as h approaches 0, 
lim f(x’) = f(x) 
h-0 
for all values x’ E [x, 
x + h). Thus 
h x f(x) 
I’(x) = lim 
~ 
= f(x) . 
h-o 
h 
In other words, I(x) is an antiderivative of f(x). 
antiderivative of f(x). Then 
Suppose F(x) is any other 
F(x) = I(x) + c 

INTRODUCTION TO CALCULUS 
347 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Figure A.9 The function f(z) = x - ~ ’ ~ .  
for some constant c. Thus F(b) - F(a) = I(b) - I ( a )  = s,” f(z) 
dz, and the 
theorem is proved. 
For example, suppose f(z) = e-2x for z 2 0. Then F ( z )  = -$ x e-” 
is an 
antiderivative of f(z). The area under the curve between 1 and 4 is given by 
1 
2 
+ - x e-”l. 
l4 
f(z) 
dz = F(4) - F(1) = -- x e-2’4 
1 
2 
Definite Integral of a Function f(z) 
Defined on an Open Interval 
Let f(z) 
be a function defined on the open interval (a, b). In this case, the antideriva- 
tive F ( z )  is not defined at a and b. We define 
F(a) = lim F ( z )  and 
x+a 
F(b) = lim F ( z )  
x-b 
provided those limits exist. Then we define the definite integral with the same formula 
as before: 
I ’ f ( z )  = W )  
- Fta). 
For example, let f(z) = x-’I2. This function is defined over the half-open 
interval (0,1]. It is not defined over the closed interval [0,1] because it is not defined 
at the endpoint x = 0. This curve is shown in Figure A.9. We see the curve has a 

348 
INTRODUCTION TO CALCULUS 
vertical asymptote at x = 0. We will define 
F(0) = lim F ( x )  
= lim 2a1I2 
2-0 
2 -0 
= 0 .  
Then 
Theorem 9 Integration by parts. 
dejined on an interval [a, b]. Then 
Let F ( x )  and G(x) be differentiable functions 
b 
F’(x) x G(x) dx = F ( x )  x G(x)IL - 1 F ( x )  x G’(x) dx 
J,” 
a 
Pro08 Integration by parts is the inverse of jnding the derivative of the product 
F(x) x G(x): 
d 
dx 
-[F(x) x G(x)] = F ( x )  x G’(x) + F ( x )  x G’(x) . 
Integrating both sides, we see that 
F(b) x G(b) - F(a) x G(a) = 
F ( x )  x G’(x) dx + 
F’(z) x G(x) dx. 
Lb 
l 
Theorem 10 Change of variable formula. Let x = g ( y )  be a differentiablefunction 
on the interval [a, b]. Then 
The change of variable formula is the inverse of the chain rule for differentiation. 
The derivative ofthe function of afunction F(g(y)) is 
d 
dx - 
[F(dY)l = F’(9(Y)) x 9’(Y). 
Integrating both sidesfrom y = a to y = b gives 
b 
F(g(b)) - F(g(a)) = s, F’(9(Y)) x S‘(Y)dY. 
The left-hand-side equals sgsa(;) F’(y) dy. Let f ( x )  = F’(x), and the theorem is 
proved. 

INTRODUCTION TO CALCULUS 
349 
MULTIVARIATE CALCULUS 
Partial Derivatives 
In this section we consider the calculus of two or more variables. Suppose we have 
a function of two variables f(z, 
y). The function is continuous at the point (a, b) if 
and only if 
lim 
(z a) - 
(a&) f(z, 
Y) = f(a, 
b) . 
The first partial derivatives at the point (a, b) are defined to be 
and 
provided that these limits exist. In practice, the first partial derivative in the x- 
direction is found by treating y as a constant and differentiating the function with 
respect to x, and vice versa, to find the first partial derivative in the y-direction. 
If the function f(z, 
y) has first partial derivatives for all points (x, y) in a contin- 
uous two-dimensional region, then the first partial derivative function with respect to 
2 is the function that has value at point (2, 
y) equal to the partial derivative of f(z, y) 
with respect to x at that point. It is denoted 
The first partial derivative function with respect to y is defined similarly. The first 
derivative functions fz(z, 
y) and fy(z, y) give the instantaneous rate of change of 
the function in the x-direction and y-direction, respectively. 
The second partial derivatives at the point (a, b) are defined to be 
and 
The second cross partial derivatives at (a, b) are 
and 

350 
INTRODUCTION TO CALCULUS 
For all the functions that we consider, the cross partial derivatives are equal, SO it 
doesn't matter which order we differentiate. 
If the function f (x, y) has second partial derivatives (including cross partial deriva- 
tives) for all points (x, y) in a continuous two-dimensional region, then the second 
partial derivative function with respect to x is the function that has value at point 
(x, y) equal to the second partial derivative of f(x, y) with respect to z at that point. 
It is denoted 
The second partial derivative function with respect to y is defined similarly. The 
second cross partial derivative functions are 
and 
The two cross partial derivative functions are equal. 
manner. 
Partial derivatives of functions having more than 2 variables are defined in a similar 
Finding Minima and Maxima of a Multivariate Function 
A univariate functions with a continuous derivative achieves minimum or maximum 
at an interior point x only at points where the derivative function f '(2) = 0. However, 
not all such points were minimum or maximum. We had to check either the first 
derivative test, or the second derivative test to see whether the critical point was 
minimum, maximum, or point of inflection. 
The situation is more complicated in two dimensions. Suppose a continuous 
differentiable function f (x, y) is defined on a two dimensional rectangle. It is not 
enough that both fz(x, y) = 0 and fy(s, 
y) = 0. 
The directional derivative of the function f(z, y) in direction B at a point measures 
the rate of change of the function in the direction of the line through the point that 
has angle B with the positive x-axis. It is given by 
Def(x, Y) = fz(x, Y) ~ 0 4 4  
+ fdx, Y) ~ 4 6 ' ) .  
Thefunction achievesamaximumor minimumvalueatpoints (5, y),where Def(x, y) = 
0 for all 8. 
Multiple Integrals 
Let f (x, y) > 0 be a nonnegative function defined over a closed a rectangle a1 5 x 5 
bl and a2 5 y 5 bz. Let 20, . . . , x, partition the interval [al, bl], and let y1, . . . , ym 

INTRODUCTION TO CALCULUS 
351 
partition the interval a2, bz. Together these partition the rectangle into j = m x n 
rectangles. The volume under the surface f(z, 
y) over the rectangle A is between 
the upper sum 
and the lower sum 
j=1 
where (tj, u j )  is the point where the function is maximized in the j t h  rectangle, and 
(wj, wj) is the point where the function is minimized in the j t h  rectangle. Refining 
the partition always lowers the upper sum and raises the lower sum. We can always 
find a partition that makes the upper sum arbitrarily close to the lower sum. Hence 
the total volume under the surface denoted 
L; 1; f(s, 
Y) dzdy 
is the least upper bound of the lower sum and the greatest lower bound of the upper 
sum. 

This Page Intentionally Left Blank

Use of Statistical Tables 
BINOMIAL DISTRIBUTION 
Table B. 1 contains values of the binomial (n, rr) probability distribution for n = 
2,3,4,5,6,7,8,9,10,11,12,15, and 20 and for rr = .05, .lo,. . . , .95. Given the 
parameter rr, the binomial probability is obtained by the formula 
P(Y = yl.) 
= ( ; 
) 7r"l - 7ry-y 
(B.1) 
When T 5 .5, use the rr value along the top row to find the correct column of 
probabilities. Go down to the correct n. The probabilities correspond to the y values 
found in the left-hand column. For example, to find P(Y = 6) when Y has the 
binomial (n = 10, T = .3) distribution, go down the table to n = 10 and find the 
row y = 6 on the left side. Look across the top to find the column labelled .30. The 
value in the table at the intersection of that row and column is P(Y = 6 )  = .0368 in 
this example. 
When T > .5 use the rr value along the bottom row to find the correct column of 
probabilities. Go down to the correct n. The probabilities correspond to the y values 
found in the right hand column. For example, to find P(Y = 3) when y has the 
binomial (n = 8, T = .65) distribution, go down the table to n = 8 and find the row 
y = 3 on the right side. Look across the bottom to find the column labelled .65. The 
Introduction to Bayesian Statistics, Second Edition. By William M .  Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
353 

354 
USE OF STATISTICAL TABLES 
I 
I 
0 
Figure 6.1 Standard normal density. 
value in the table at the intersection of that row and column is P(Y = 3) = .0808 in 
this example. 
STANDARD NORMAL DISTRIBUTION 
This section contains two tables. Table B.2 contains the area under the standard 
normal density. Table B.3 contains the ordinates (height) of the standard normal 
density. The standard normal density has mean equal to 0 and variance equal to 1. 
Its density is given by the formula 
We see that the standard normal density is symmetric about 0. The graph of the 
standard normal density is shown in Figure B. 1. 
Area Under Standard Normal Density 
Table B.2 tabulates the area under the standard normal density function between 0 
and z for nonnegative values of t from 0.0 to 3.99 in steps of .01. We read down the 
t column until we come to the value that has the correct units and tenths digits of z. 
This is the correct row. We look across the top row to find the hundredth digit of z. 
This is the correct column. The tabulated value at the intersection of the correct row 
and correct column is P(0 5 2 5 z ) ,  where Z has the normal (0,l) distribution. 
For example, to find P(0 5 2 5 1.23) we go down the z column to 1.2 for the 
correct row and across top to 3 for correct column. We find the tabulated value at the 
intersection of this row and column. For this example, P(0 F. 2 5 1.23) = .3907. 

USE OF STATISTICAL TABLES 
355 
Because the standard normal density is symmetric about 0, 
P(-z 5 z 
5 0 )  = P(0 5 z 
5 z ) .  
Also, since it is a density function, the total area underneath it equals 1 .OOOO, so the 
total area to the right of 0 must equal SOOO. We can proceed to find 
P(Z > Z )  = .5000 - P(Z 5 2 ) .  
Finding Any Normal Probability 
We can standardize any normal random variable to a standard normal random variable 
having mean 0 and variance 1. For instance, if W is a normal random variable having 
mean m and variance s2, we standardize by subtracting the mean and dividing by the 
standard deviation. 
z=----. 
W - m  
S 
This lets us find any normal probability by using the standard normal tables. 
Example 
Suppose W has the normal distribution with mean 120 and variance 
225. (The standard deviation of W is 15.) Suppose we wanted tofind the probability 
P(W 5 129). 
We can subtract the mean from both sides of an inequality without changing the 
inequality: 
We can divide both sides of an inequality by the standard deviation (which is positive) 
without changing the inequality: 
P(W - 120 5 129 - 120). 
P
(
y
5
;
)
.
 
On the left-hand side we have the standard normal Z, and on the right-hand side we 
have the number .60. Therefore 
P(W 5 129) = P ( Z  5 .60) = .5000 + .2258 = .7258. 
Ordinates of the Standard Normal Density 
Figure B.3 shows the ordinate of the standard normal table at z. We see the ordinate 
is the height of the curve at z. Table B.3 contains the ordinates of the standard 
normal density for nonnegative z values from 0.00 to 3.99 in steps of .01. Since the 
standard normal density is symmetric about 0, f ( - z )  = f ( z ) ,  we can find ordinates 
of negative z values. 

356 
USE OF STATISTICAL TABLES 
Pa 
3 
4 
5 
6 
Z 
zi 
Likelihood 
2.60 
.136 
1.6 
.1109 
.6 
.3332 
-.4 
.3683 
Figure 6.2 
B.2. 
Shaded area under standard normal density. These values are shown in Table 
This table is used to find values of the likelihood when we have a discrete prior 
distribution for p. We go down the z column until we find the value that has the units 
and tenths digits. This gives us the correct row. We go across the top until we find 
the hundredth digit. This gives us the correct column. The value at the intersection 
of this row and column is the ordinate of the standard normal density at the value z. 
For instance, if we want to find the height of the standard normal density at z = 1.23 
we go down z column to 1.2 to find the correct row, and across the top to 3 to find the 
correct column. The ordinate of the standard normal at z = 1.23 is equal to .1872. 
(Note: You can verify this is correct by plugging z = 1.23 into Equation B.2.) 
Example Suppose the distribution of Y given p is normal(p, o2 = 1). Also suppose 
there are 4 possible values of p. They are 3,4,5,and 6. We observe y = 5.6. We 
calculate 
5.6 - 

USE OFSTATETlCAL TABLES 
357 
Figure 8.3 
Table B.3. 
Ordinates of standard normal density function. These values are shown in 
STUDENT’S t DISTRIBUTION 
Figure B.4 shows the Students t distribution for several different degrees of freedom, 
along with the standard normal(0,l) distribution. We see the Students t family of 
distributions are similar to the standard normal in that they are symmetric bell shaped 
curves; however, they have more weight in the tails. The heaviness of the tails of the 
Students t decreases as the degrees of freedom increase.’ 
The Students t distribution is used when we use the unbiased estimate of the stan- 
dard deviation 8 instead of the true unknown standard deviation CT in the standardizing 
formula 
Y-P 
g!J 
z=- 
and y is a normally distributed random variable. We know that z will have the 
normal(0,l) distribution. The similar formula 
t = -  Y-P 
8, 
will have the Students t distribution with k degrees of freedom. The degrees of 
freedom k will equal the sample size minus the number of parameters estimated in 
the equation for 8. For instance, if we are using 
the sample mean, its estimated 
standard deviation is given by 8g = $, where 
n 
‘The normnl(0,l) distribution corresponds to the Students t distribution with 00 degrees of freedom 

358 
USE OF STATISTICAL TABLES 
one 
four 
ten 
- 
normal 
-4 
-3 
-2 
-1 
0 
1 
2 
3 
4 
Figure 8.4 
normal (0,l) density which corresponds to Student’s t with co degrees of freedom. 
Student’s t densities for selected degrees of freedom, together with the standard 
and we observe that to use the above formula we have to first estimate 9. Hence, in 
the single sample case we will have k = 
Table B.4 contains the tail areas for the Student’s t distribution family. The degrees 
offreedom are down the left column, and the tabulated tail areas are across the rows 
for the specified tail probabilities. 
- 1 degrees of freedom. 
POISSON DISTRIBUTION 
Table B.5 contains values of the Poisson(p) distribution for some selected values of 
p going from .1 to 4 in increments of .l, from 4.2 to 10 in increments of .2, and 
from 10.5 to 15 in increments of .5. Given the parameter p, the Poisson probability 
is obtained from the formula 
pYe-!-’ 
P(Y = yip) = - 
Y! 
(B.3) 
for y = 0,1,. . .. Theoretically y can take on all non-negative integer values. In 
Table B.5 we include all possible values of y until the probability becomes less than 
,0001. 
Example 30 Suppose the distribution of Y given p is Poisson(p) and there are 3 
possible values of p, .5, .75, and 1.00. We observed y = 2. The likelihood is found 
by looking up values in row y = 2 for the possible values of p. Note, the value for 
p = .75 is not in the table. It is found by linearly interpolating between the values 
for p = .70 and p = .80. 

USE OF STATISTICAL TABLES 
359 
Pi 
.50 
.75 
1.00 
Figure B.5 Uppper tail area of chi-squared distribution. 
(Interpolation if necessary) 
Likelihood 
.0758 
.I327 
.1839 
(.5 x .1217+ .5 x .1438) 
CHI-SQUARED DISTRIBUTION 
Table B.6 contains P(U > a), the upper tail area when U has the chi-squared 
distribution. The values in the table correspond to the shaded area in Figure B.5. The 
posterior distribution of the variance u2 is S’x an inverse chi-squared distribution 
with K‘ degrees of freedom. This means that 5 has the chi-squared distribution with 
6’ degrees of freedom, so we can use the chi-squared table to find credible intervals 
for u and test hypotheses about CT. 
Example 31 Suppose that the posterior distribution of u2 is IlOx an inverse chi- 
squared distribution with 12 degrees of freedom. Then 9 has the chi-squared 
distribution with 12 degrees of freedom. So a 95% Bayesian credible interval is 
found by 
110 
U2 
.95 = P(4.404 < - 
< 23.337) 
23.337 
4.404 
= P(2.17107 < 0 < 4.99773). 
We would test the two-sided hypothesis 
Ho : CT = 2.5 VS. Hi : u # 2.5 

360 
USE OF STATlSTlCAL TABLES 
at the 5% level of sign@cance by observing that 2.5 lies within the credible interval, 
so we must accept the null hypothesis that u = 2.5 is still a credible value. On the 
other hand, ifwe wanted to test the one-sided hypothesis 
Ho : 0 5 2.5 VS. Hi : 0 > 2.5 
at the 5% level, we would calculate the posterior probability of the null hypothesis. 
The value 17.60 lies between 11.340 and 18.549, so the posteriorprobability of the 
null hypothesis is between .SO and .lo. This is larger than the level of sign@cance of 
5%, so we would not reject the null hypothesis. 

Table B. 7 Binomial probability table 
2 0  
1 
2 
3 0  
1 
2 
3 
4 0  
1 
2 
3 
4 
5 0  
1 
2 
3 
4 
5 
6 0  
1 
2 
3 
4 
5 
6 
7 0  
1 
2 
3 
4 
5 
6 
7 
8 0  
1 
2 
3 
4 
5 
6 
7 
8 
.05 
.10 
,9025 
.81 
,0950 
.18 
,0025 
.01 
,8574 
,729 
,1354 
,243 
,0071 
,027 
,0001 
,001 
,8145 
,6561 
,1715 
,2916 
.0135 
,0486 
,0005 
,0036 
.oooo 
.ooo1 
,7738 
S905 
,2036 
,3281 
,0214 
,0729 
,0011 
,0081 
.oooo 
,0005 
.oooo 
.0000 
,7351 
,5314 
,2321 
.3543 
,0305 
,0984 
,0021 
,0146 
.0001 
,0012 
.0000 
.O001 
.oooo 
,0000 
,6983 
,4783 
.2573 
,3720 
,0406 
,1240 
,0036 
,0230 
,0002 
,0026 
.0000 
,0002 
.oooo 
.oooo 
.0000 
.oooo 
,6634 
,4305 
,0515 
,1488 
,0054 
,0331 
,0004 
,0046 
.OOOO 
,0004 
,0000 
.0000 
.oooo 
.oooo 
.0000 
.oooo 
,2793 
,3826 
.95 
.90 
.I5 
,7225 
,2550 
,0225 
,6141 
,3251 
,0574 
,0034 
,5220 
,3685 
,0975 
.0115 
,0005 
,4437 
,3915 
,1382 
,0244 
,0022 
.0001 
,3771 
.3993 
,1762 
,0415 
,0055 
.ooo4 
.0000 
,3206 
,3960 
,2097 
,0617 
,0109 
,0012 
.ooo1 
.mo 
,2725 
,3847 
,2376 
,0839 
,0185 
,0026 
,0002 
.0000 
.oooo 
.85 
- 
- 
.20 
.64 
.32 
.04 
,512 
,384 
,096 
,008 
,4096 
.4096 
,1536 
,0256 
,0016 
,3277 
,4096 
,2048 
,0512 
,0064 
,0003 
,2621 
,3932 
,2458 
,0819 
,0154 
,0015 
,0001 
,2097 
,3670 
,2753 
,1147 
,0287 
,0043 
,0004 
.oooo 
,1678 
,3355 
,2936 
,1468 
,0459 
,0092 
,001 1 
.0001 
.0000 
.80 
x 
.25 
,5625 
,3750 
,0625 
,4219 
,4219 
,1406 
,0156 
,3164 
,4219 
,2109 
,0469 
,0039 
,2373 
,3955 
,2637 
,0879 
,0146 
.0010 
,1780 
,3560 
,2966 
,1318 
,0330 
,0044 
,0002 
,1335 
,3115 
,3115 
,1730 
,0577 
,0115 
,0013 
,0001 
,1001 
,2670 
,3115 
,2076 
,0865 
,0231 
,0038 
,0004 
.oooo 
.75 
- 
- 
T 
.49 
,4225 
.42 
,4550 
.09 
,1225 
,343 
,2746 
,441 
,4436 
.I89 
,2389 
,027 
,0429 
,2401 
,1785 
,4116 
,3845 
,2646 
,3105 
,0756 
,1115 
,0081 
,0150 
,1681 
,1160 
,3601 
,3124 
,3087 
,3364 
,1323 
.I811 
,0284 
,0488 
,0024 
,0053 
,1176 
,0754 
,3025 
,2437 
,3241 
,3280 
,1852 ,2355 
,0595 
,0951 
,0102 
,0205 
,0007 
,0018 
,0824 
,0490 
,2471 
,1848 
,3177 
,2985 
,2269 
,2679 
,0972 
,1442 
,0250 
,0466 
,0036 
,0084 
,0002 
,0006 
,0576 
,0319 
,1977 
,1373 
,2965 
,2587 
,2541 
,2786 
,1361 
,1875 
,0467 
,0808 
,0100 
,0217 
,0012 
,0033 
,0001 
,0002 
.70 
.65 
.40 
.36 
.48 
.16 
,216 
,432 
,288 
,064 
,1296 
,3456 
,3456 
,1536 
,0256 
,0778 
,2592 
,3456 
,2304 
,0768 
,0102 
,0467 
,1866 
,3110 
,2765 
,1382 
,0369 
.0041 
,0280 
,1306 
,2613 
,2903 
,1935 
,0774 
,0172 
,0016 
,0168 
,0896 
,2090 
,2787 
2322 
,1239 
,0413 
.0079 
,0007 
.60 
- 
- 
.45 
,3025 
,4950 
,2025 
,1664 
,4084 
,3341 
,091 1 
,0915 
,2995 
,3675 
,2005 
,0410 
,0503 
,2059 
,3369 
,2757 
,1128 
,0185 
,0277 
,1359 
,2780 
.3032 
,1861 
,0609 
,0083 
,0152 
,0872 
,2140 
,2918 
,2388 
,1172 
,0320 
,0037 
,0084 
,0548 
,1569 
,2568 
,2627 
,1719 
,0703 
,0164 
,0017 
.55 
s o  
.25 
S O  
.25 
,125 
,375 
,375 
,125 
,0625 
,2500 
,3750 
,2500 
.0625 
,0313 
,1563 
,3125 
,3125 
,1563 
,0313 
,0156 
,0937 
,2344 
.3125 
,2344 
,0937 
,0156 
,0078 
,0547 
,1641 
,2734 
,2734 
.1641 
,0547 
,0078 
,0039 
,0313 
,1094 
,2188 
,2734 
,2188 
,1094 
,0313 
,0039 
so 
__ 
- 
___ 
- 
- 
2 
1 
3 
3 
2 
1 
3 
4 
3 
2 
1 
3 
5 
4 
3 
2 
1 
3 
5 
5 
I 
3 
2 
1 
0 
7 
6 
5 
4 
3 
2 
1 
0 
8 
7 
6 
5 
4 
3 
2 
1 
0 
- 
Y 

Table 6.1 (Continued) 
3 
4 
5 
6 
7 
8 
9 
10 0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 0 
1 
2 
3 
4 
5 
6 
7 
1
8
 
9 
10 
11 
- 
- 
? 
3 
7 
5 
5 
1 
3 
2 
1 
0 
0 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
LO 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
12 
11 
1C 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
- 
Y - 
IT 
.I0 
.15 
.20 
.25 
.05 
.35 
.40 
.45 
.SO 
.30 
,0404 
,1556 
,2668 
,2668 
,1715 
,0735 
,0210 
,0039 
.ooo4 
.oooo 
,0282 
,1211 
,2335 
,2668 
,2001 
,1029 
,0368 
,0090 
,0014 
.ooo1 
.oooo 
,0932 
,1998 
,2568 
,2201 
,1321 
,0566 
,0173 
0037 
.ooo5 
.oooo 
.oooo 
,0138 
,0712 
,1678 
,2397 
.23 1 1 
,1585 
,0792 
,0291 
,0078 
,0015 
.ow2 
.0000 
.0000 
.70 
,2316 
,3679 
,2597 
.lo69 
,0283 
,0050 
.oO06 
.0000 
.oooo 
.oooo 
,1342 
,3020 
,3020 
,1762 
,0661 
,0165 
,0028 
.COO3 
.0000 
.oooo 
,075 1 
,2253 
,3003 
,2336 
,1168 
,0389 
,0087 
,0012 
,0001 
.oooo 
,0207 
,1004 
,2162 
,2716 
,2194 
,1181 
,0424 
,0098 
.0013 
,0001 
,0101 
.0605 
,1612 
,2508 
,2508 
,1672 
,0743 
,0212 
,0035 
.0003 
,0046 
,0339 
,1110 
,2119 
,2600 
,2128 
,1160 
,0407 
,0083 
,0008 
.0020 
,0176 
,0703 
,1641 
,2461 
.2461 
,1641 
,0703 
,0176 
,0020 
,3874 
,3874 
,1722 
,0446 
,0074 
.ooO8 
,0001 
.oooo 
.oooo 
.oooo 
,6302 
,2985 
,0629 
,0077 
,0006 
.oOOo 
.oooo 
.0000 
.oooo 
.0000 
,5987 
,3151 
,0746 
,0105 
,0010 
,0001 
.oooo 
.m 
.moo 
.moo 
.oooo 
,3293 
,0867 
,0137 
,0014 
,0001 
.oooo 
.0000 
.oooo 
.m 
.0000 
.0000 
,5404 
,3413 
,0988 
,0173 
,0021 
,0002 
.oooo 
.oooo 
.oooo 
.0000 
.oooo 
.0000 
.oooo 
.95 
,3487 
,3874 
,1937 
.0574 
,0112 
,0015 
.ooo1 
.0000 
.oooo 
.0000 
.oooo 
,1969 
,3474 
,2759 
,1298 
,0401 
,0085 
,0012 
.ooo1 
.oooo 
.moo 
.oooo 
,1074 
,2684 
,3020 
,2013 
,0881 
.0264 
.0055 
,0008 
,0001 
.oooo 
.oooo 
,0563 
,1877 
,2816 
,2503 
,1460 
,0584 
,0162 
,0031 
.ooo4 
.OoOo 
.oOOo 
,0135 
,0725 
,1757 
,2522 
,2377 
,1536 
,0689 
,0212 
,0043 
.ooo5 
.oooo 
,0060 
,0403 
,1209 
,2150 
,2508 
,2007 
,1115 
,0425 
,0106 
,0016 
,0001 
,0025 
,0207 
,0763 
,1665 
,2384 
,2340 
,1596 
.0746 
.0229 
,0042 
,0003 
.0010 
,0098 
,0439 
.1172 
,205 1 
,2461 
,2051 
,1172 
,0439 
,0098 
,0010 
,3835 
,2131 
,0710 
,0158 
,0025 
,0003 
.oooo 
.oooo 
.oooo 
.0000 
.0000 
,3248 
,2866 
,1517 
.0536 
,0132 
,0023 
.ooo3 
.oooo 
.oooo 
.oooo 
.oooo 
.2362 
.2953 
,2215 
,1107 
,0388 
,0097 
,0017 
,0002 
.oooo 
.oooo 
.0000 
,1549 
,2581 
,2581 
,1721 
,0803 
,0268 
,0064 
,001 1 
.ooo1 
.0000 
.oooo 
,0518 
,1395 
,2254 
,2428 
,1830 
,0985 
,0379 
,0102 
,0018 
.Om2 
.0000 
,0266 
.0887 
,1774 
.2365 
,2207 
,1471 
,0701 
,0234 
,0052 
.ooo7 
.0000 
,0125 
,0513 
,1259 
,2060 
,2360 
.1931 
,1128 
,0462 
,0126 
,0021 
.ooo2 
.0054 
,0269 
,0806 
,161 1 
.2256 
,2256 
.1611 
,0806 
,0269 
,0054 
,0005 
,2824 
,3766 
,2301 
,0852 
,0213 
,0038 
,0005 
.0000 
.oooo 
.0000 
.moo 
.oOoO 
.oooo 
,1422 
,3012 
,2924 
,1720 
,0683 
,0193 
,0040 
,0006 
.0001 
.0000 
.0000 
.0000 
.oooo 
,0687 
,2062 
,2835 
,2362 
,1329 
,0532 
.0155 
,0033 
,0005 
,0001 
.0000 
.oooo 
.oooo 
,0317 
,1267 
,2323 
.2581 
,1936 
,1032 
,0401 
,0115 
,0024 
,0004 
.oooo 
.oOm 
.0000 
,0057 
,0368 
,1088 
,1954 
,2367 
,2039 
.1281 
,0591 
,0199 
,0048 
,0008 
.ooo1 
.0000 
,0022 
,0174 
,0639 
,1419 
,2128 
,2270 
,1766 
,1009 
.0420 
,0125 
,0025 
,0003 
.0000 
,0008 
,0075 
,0339 
.0923 
,1700 
,2225 
,2124 
.1489 
,0762 
,0277 
,0068 
,0010 
,0001 
,0002 
,0029 
,0161 
,0537 
,1208 
,1934 
,2256 
,1934 
,1208 
,0537 
,0161 
,0029 
.0002 
.65 
.60 
.55 
S O  
.90 
.85 
.80 
.75 
?T 

Table B. 1 (Continued) 
15 0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
20 0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
7r 
.05 
. 10 
.15 
.20 
.25 
.30 
.35 
.40 
.45 
S O  
4633 
3658 
1348 
0307 
oil49 
0006 
OOOO 
m 
m 
m 
m 
0000 
m 
oooo 
0000 
m 
3585 
3774 
,1887 
,0596 
,0133 
,0022 
.0003 
.oooo 
,0000 
.0000 
.oooo 
.m 
.0000 
.0000 
.0000 
.0000 
.om0 
.0000 
.0000 
.m 
.oooo 
.2059 
,3432 
,2669 
,1285 
,0428 
,0105 
,0019 
.oO03 
.ow0 
.moo 
.m 
.m 
.0000 
.mo 
.OOOO 
.m 
,1216 
,2702 
,2852 
,1901 
.0898 
,0319 
,0089 
,0020 
.0004 
,0001 
.0000 
.m 
,0000 
.m 
.m 
.m 
.m 
.0000 
.oooo 
.0000 
.oooo 
,0874 
,2312 
,2856 
,2184 
,1156 
,0449 
,0132 
,0030 
,0005 
,0001 
.0000 
.0000 
,0000 
.0000 
.0000 
.0000 
,0388 
,1368 
,2293 
.2428 
,1821 
,1028 
,0454 
,0160 
,0046 
,0011 
.0002 
,0000 
.0000 
.m 
.0000 
.m 
.0000 
.0000 
.0000 
.0000 
,0000 
,0352 
,1319 
,2309 
,2501 
,1876 
,1032 
,0430 
,0138 
,0035 
,0007 
,0001 
.m 
.0000 
.m 
.m 
.OoOo 
,0115 
,0576 
,1369 
,2054 
,2182 
,1746 
,1091 
,0545 
,0222 
.0074 
,0020 
,0005 
.0001 
.m 
,0000 
.0000 
.m 
.0000 
,0000 
.0000 
,0000 
.95 
.90 
.85 
.80 
,0134 
,0668 
,1559 
,2252 
,2252 
,1651 
.09 17 
,0393 
,0131 
.0034 
,0007 
,0001 
.m 
.m 
.m 
.m 
,0032 
,0211 
,0669 
.1339 
,1897 
,2023 
,1686 
,1124 
,0609 
,0271 
,0099 
,0030 
,0008 
.ooo2 
.oooo 
.0000 
.m 
.oooo 
.m 
.0000 
.0000 
.75 
7r 
,0047 
,0305 
,0916 
,1700 
.2186 
,2061 
,1472 
.0811 
,0348 
,0116 
.0030 
,0006 
,0001 
.0000 
.mo 
.moo 
,0008 
,0068 
.0278 
,0716 
,1304 
,1789 
,1916 
,1643 
.1144 
,0654 
,0308 
,0120 
,0039 
,0010 
,0002 
.m 
.0000 
.m 
.oO00 
.oooo 
.oooo 
.70 
,0016 
,0126 
,0476 
,1110 
,1792 
,2123 
,1906 
,1319 
,0710 
,0298 
,0096 
,0024 
,0004 
.0001 
.om 
,0000 
,0002 
,0020 
,0100 
,0323 
,0738 
,1272 
,1712 
,1844 
,1614 
,1158 
,0686 
,0336 
,0136 
.0045 
,0012 
,0003 
.m 
.oooo 
.m 
.0000 
.oooo 
.65 
,0005 
,0001 
.OOOO 
,0047 
,0016 
,0005 
,0219 
,0090 
,0032 
,0634 
,0318 
,0139 
,1268 
,0780 
,0417 
,1859 
,1404 
.0916 
2066 
,1914 
,1527 
,1771 
,2013 
,1964 
.1181 
,1647 
,1964 
.0612 
,1048 
.1527 
,0245 
,0515 
,0916 
,0074 
,0191 
,0417 
,0016 
,0052 
,0139 
,0003 
,0010 
,0032 
.0000 
,0001 
,0005 
.m .moo 
.oooo 
,0000 .moo .m 
,0005 
,0001 
.oooO 
,0031 
,0008 
,0002 
,0123 
,0040 
,0011 
,0350 
,0139 
,0046 
,0746 
,0365 
.0148 
,1244 
,0746 
,0370 
,1659 
,1221 
,0739 
,1797 
,1623 
,1201 
,1597 
,1771 
,1602 
,1171 
,1593 
,1762 
,0710 
,1185 
,1602 
,0355 
,0727 
,1201 
,0146 
,0366 
,0739 
,0049 ,0150 
,0370 
,0013 
,0049 
,0148 
,0003 
,0013 
,0046 
.0000 
,0002 
,0011 
.m .oooo 
,0002 
.m .oooo 
,0000 
.0000 
.0000 
.0000 
.60 
.55 
.50 
- 
15 
14 
13 
12 
11 
10 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 
20 
19 
18 
17 
16 
15 
14 
13 
12 
11 
10 
9 
8 
7 
6 
5 
4 
3 
2 
1 
0 - 
Y - 

Table 8.2 Area under standard normal density 
Z 
0.0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1.0 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2.0 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
.OO 
.01 
.02 
.03 
.04 
.05 
.06 
.07 
.08 
.09 
.MOO 
.0040 
,0080 
.0120 
,0160 
,0199 
,0239 
,0279 
.03 19 
,0359 
,0398 
,0438 
,0478 
.05 17 
,0557 
,0596 
,0636 
,0675 
.0714 
,0753 
,0793 
,0832 
,0871 
,0910 
,0948 
.0987 
.lo26 
,1064 
,1103 
.1141 
,1179 
,1217 
.1255 
,1293 
,1331 
,1368 
.1406 
,1443 
,1480 
.1517 
,1554 
,1591 
,1628 
,1664 
,1700 
.1736 
.1772 
.1808 
,1844 
,1879 
.1915 
,1950 
,1985 
.2019 
,2054 
,2088 
.2123 
.2 157 
,2190 
,2224 
,2257 
,2291 
,2324 
,2357 
.2389 
,2422 
.2454 
,2486 
,2517 
,2549 
,2580 
,261 1 
,2642 
,2673 
,2703 
,2734 
,2764 
.2794 
,2823 
.2852 
.2881 
,2910 
.2939 
.2967 
.2995 
,3023 
.305 1 
,3078 
,3106 
,3133 
,3159 
,3186 
,3212 
.3238 
.3264 
.3289 
,3315 
.3340 
.3365 
.3389 
.3413 
.3438 
,3461 
.3485 
,3508 
.3531 
,3554 
.3577 
.3599 
.3621 
.3643 
,3665 
,3686 
,3708 
.3729 
.3749 
.3770 
,3790 
,3810 
.3830 
.3849 
,3869 
.3888 
.3907 
.3925 
,3944 
.3962 
.3980 
.3997 
,4015 
,4032 
.4049 
.4066 
.4082 
.4099 
.4115 
,413 1 
.4147 
,4162 
.4177 
,4192 
,4207 
,4222 
4236 
A251 
,4265 
.4279 
.4292 
,4306 
,4319 
,4332 
,4345 
.4357 
.4370 
.4382 
,4394 
4 0 6  ,4418 
,4429 
.4441 
,4452 
,4463 
.4474 
,4484 
,4495 
,4505 
,4515 
,4525 
,4535 
.4545 
,4554 
4564 
,4573 
,4582 
.4591 
.4599 
.4608 
,4616 
.4625 
,4633 
,4641 
.4649 
,4656 
,4664 
.4671 
.4678 
,4686 
.4693 
,4699 
.4706 
,4713 
,4719 
,4726 
,4732 
,4738 
,4744 
,4750 
,4756 
,4761 
.4767 
4772 
.4778 
,4783 
,4788 
,4793 
.4798 
.4803 
,4808 
,4812 
.4817 
,4821 
.4826 
,4830 
,4834 
.4838 
,4842 
,4846 
,4850 
.4854 
,4857 
.4861 
,4864 
,4868 
,4871 
,4875 
.4878 
.4881 
.4884 
,4887 
,4890 
.4893 
.4896 
.4898 
.4901 
,4904 
,4906 
.4909 
,491 1 
,4913 
.4916 
.4918 
.4920 
,4922 
.4925 
,4927 
,4929 
,493 1 
,4932 
,4934 
,4936 
,4938 
.4940 
,4941 
.4943 
.4945 
,4946 
.4948 
,4949 
,495 1 
,4952 
,4953 
,4955 
.4956 
,4957 
.4959 
.4960 
,4961 
.4962 
.4963 
,4964 
,4965 
,4966 
,4967 
,4968 
.4969 
,4970 
.497 1 
,4972 
.4973 
,4974 
.4974 
,4975 
,4976 
.4977 
,4977 
.4978 
.4979 
,4979 
.4980 
,498 1 
,4981 
,4982 
,4982 
,4983 
,4984 
.4984 
,4985 
,4985 
,4986 
,4986 
3.0 
3.1 
3.2 
3.3 
3.4 
,4987 
.4987 
,4987 
,4988 
.4988 
.4989 
.4989 
,4989 
,4990 
,4990 
.4990 
.4991 
,4991 
,4991 
.4992 
.4992 
,4992 
,4992 
.4993 
,4993 
.4993 
,4993 
,4994 
,4994 
.4994 
.4994 
.4994 
,4995 
,4995 
.4995 
,4995 
.4995 
.4995 
.4996 
.4996 
.4996 
.4996 
,4996 
,4996 
,4997 
.4997 
,4997 
,4997 
.4997 
.4997 
,4997 
,4997 
.4997 
.4997 
,4998 
3.5 
3.6 
3.8 
3.9 
,4998 
.4998 
,4998 
.4998 
,4998 
.4998 
,4998 
,4998 
,4998 
,4998 
.4998 
.4998 
.4999 
,4999 
.4999 
,4999 
,4999 
.4999 
.4999 
,4999 
.4999 
,4999 
.4999 
.4999 
,4999 
.4999 
,4999 
,4999 
.4999 
,4999 
.4999 
,4999 
,4999 
,4999 
.4999 
,4999 
,4999 
,4999 
,4999 
,4999 

Table 8.3 Ordinates of standard normal density 
- 
2 - 
0.0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
0.7 
0.8 
0.9 
1 .o 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2.0 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
3.0 
3.1 
3.2 
3.3 
3.4 
3.5 
3.6 
3.7 
3.8 
3.9 
- 
.OO 
.01 
.02 
.03 
.04 
,3989 .3989 .3989 .3988 ,3986 
,3970 .3965 ,3961 .3956 ,3951 
,3910 ,3902 ,3894 ,3885 ,3876 
,3814 .3802 .3790 ,3778 .3765 
,3683 .3668 ,3653 .3637 .3621 
,3521 ,3503 ,3485 ,3467 ,3448 
,3332 .3312 ,3292 ,3271 ,3251 
,3123 .3101 .3079 ,3056 .3034 
.2897 ,2874 ,2850 ,2827 ,2803 
.2661 ,2637 ,2613 .2589 ,2565 
,2420 .2396 .2371 .2347 ,2323 
,2179 .2155 .2131 .2107 .2083 
,1942 .1919 ,1895 .1872 .1849 
,1714 ,1691 ,1669 ,1647 ,1626 
,1497 .1476 .I456 .1435 ,1415 
,1295 .1276 ,1257 ,1238 ,1219 
,1109 ,1092 .lo74 .lo57 .lo40 
,0940 ,0925 ,0909 .0893 ,0878 
,0790 ,0775 .0761 .0748 .0734 
,0656 ,0644 .0632 .0620 ,0608 
,0540 ,0529 ,0519 ,0508 
.@I98 
,0440 .0431 ,0422 ,0413 ,0404 
,0355 ,0347 .0339 ,0332 ,0325 
,0283 ,0277 ,0270 ,0264 ,0258 
,0224 .0219 ,0213 ,0208 ,0203 
.0175 .0171 ,0167 ,0163 ,0158 
,0136 ,0132 ,0129 ,0126 ,0122 
.0104 .0101 .0099 ,0096 .0093 
,0079 ,0077 .0075 ,0073 .0071 
,0060 .0058 
.0056 ,0055 
,0053 
,0044 
,0043 .0042 .0040 .0039 
,0033 .0032 ,0031 ,0030 ,0029 
.0024 .0023 .0022 .0022 ,0021 
,0017 .0017 .0016 .0016 0015 
,0012 .0012 ,0012 ,0011 ,0011 
,0009 ,0008 ,0008 
,0008 
,0008 
,0006 ,0006 .0006 .0005 
,0005 
,0004 .0004 .OW4 .0004 .OW4 
,0003 .0003 .0003 .0003 
.OW3 
,0002 ,0002 ,0002 ,0002 ,0002 
.05 
.06 
,3984 ,3982 
,3945 ,3939 
,3867 ,3857 
,3752 ,3739 
.3605 .3589 
,3429 .3410 
.3230 .3209 
.3011 .2989 
,2780 ,2756 
.2541 ,2516 
,2299 ,2275 
.2059 .2036 
.1826 .1804 
,1604 ,1582 
,1394 ,1374 
,1200 ,1182 
.lo23 .lo06 
,0863 ,0848 
.0721 .0707 
.0596 ,0584 
.0488 .0478 
.0396 .0387 
.0317 ,0310 
.0252 .0246 
.0198 .0194 
,0154 .0151 
.0119 ,0116 
,0091 .0088 
,0069 ,0067 
,0051 ,0050 
.0038 ,0037 
.0028 ,0027 
,0020 ,0020 
,0015 ,0014 
.0010 
,0010 
.OW7 ,0007 
,0005 
,0005 
.0004 .0003 
.om2 .ow2 
,0002 ,0002 
.07 
.08 
.09 
.3980 .3977 .3973 
,3932 .3925 .3918 
,3847 .3836 ,3825 
.3725 ,3712 .3697 
.3572 .3555 
,3538 
.3391 .3372 .3352 
,3187 .3166 .3144 
.2966 .2943 .2920 
.2732 ,2709 ,2685 
,2492 ,2468 ,2444 
,2251 ,2227 ,2203 
.2012 ,1989 ,1965 
.1781 ,1758 ,1736 
,1561 .1539 ,1518 
.1354 ,1334 ,1315 
,1163 .1145 .1127 
,0989 ,0973 .0957 
,0833 .0818 
,0804 
,0694 .0681 ,0669 
.0573 ,0562 .0551 
,0468 ,0459 
,0449 
,0379 ,0371 .0363 
.0303 ,0297 .0290 
.0241 .0235 ,0229 
.0189 ,0184 .0180 
.0147 .0143 ,0139 
.0113 .0110 
.0107 
,0086 ,0084 .0081 
,0065 ,0063 .0061 
.0048 .0047 ,0046 
,0036 ,0035 .0034 
,0026 ,0025 .0025 
.0019 .0018 ,0018 
.0014 .0013 .0013 
,0010 .0009 .0009 
.0007 .0007 
,0006 
.OOO5 
,0005 
.OOO4 
.OW3 
.0003 .0003 
,0002 ,0002 ,0002 
,0002 .0001 
.0001 

Table 8.4 Critical values of the Student’s t distribution 
Degrees of 
freedom 
(df 1 
1 
Upper Tail Area 
.20 
.10 
.05 
,025 
.01 
.005 
.001 
,0005 
1.376 
3.078 
6.314 
12.71 
31.82 
63.66 
318.3 
636.6 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
40 
60 
80 
100 
co 
1.061 
,979 
.941 
.920 
.906 
,896 
,889 
.883 
,879 
.876 
.873 
,870 
,868 
,866 
,865 
,863 
.862 
,861 
.860 
,859 
,858 
,858 
.857 
356 
,856 
355 
,855 
.854 
,854 
,851 
,848 
,846 
345 
,842 
1.886 
1.638 
1.533 
1.476 
1.440 
1.415 
1.397 
1.383 
1.372 
1.363 
1.3.56 
1.350 
1.345 
1.341 
1.337 
1.333 
1.330 
1.328 
1.325 
1.323 
1.321 
1.319 
1.318 
1.316 
1.315 
1.314 
1.313 
1.311 
1.310 
1.303 
1.296 
1.292 
1.290 
1.282 
2.920 
4.303 
6.965 
2.353 
3.182 
4.541 
2.132 
2.776 
3.747 
2.015 
2.571 
3.365 
1.943 
2.447 
3.143 
1.895 
2.365 
2.998 
1.860 
2.306 
2.896 
1.833 
2.262 
2.821 
1.812 
2.228 
2.764 
1.796 
2.201 
2.718 
1.782 
2.179 
2.681 
1.771 
2.160 
2.650 
1.761 
2.145 
2.624 
1.753 
2.131 
2.602 
1.746 
2.120 
2.583 
1.740 
2.110 
2.567 
1.734 
2.101 
2.552 
1.729 
2.093 
2.539 
1.725 
2.086 
2.528 
1.721 
2.080 
2.518 
1.717 
2.074 
2.508 
1.714 
2.069 
2.500 
1.711 
2.064 
2.492 
1.708 
2.060 
2.485 
1.706 
2.056 
2.479 
1.703 
2.052 
2.473 
1.701 
2.048 
2.467 
1.699 
2.045 
2.462 
1.697 
2.042 
2.457 
1.684 
2.021 
2.423 
1.671 
2.000 
2.390 
1.664 
1.990 
2.374 
1.660 
1.984 
2.364 
1.645 
1.960 
2.326 
9.925 
22.33 
31.60 
5.841 
10.21 
12.92 
4.604 
7.173 
8.610 
4.032 
5.893 
6.868 
3.707 
5.208 
5.959 
3.499 
4.785 
5.408 
3.355 
4.501 
5.041 
3.250 
4.297 
4.781 
3.169 
4.144 
4.587 
3.106 
4.025 
4.437 
3.055 
3.930 
4.318 
3.012 
3.852 
4.221 
2.977 
3.787 
4.140 
2.947 
3.733 
4.073 
2.921 
3.686 
4.015 
2.898 
3.646 
3.965 
2.878 
3.610 
3.922 
2.861 
3.579 
3.883 
2.845 
3.552 
3.850 
2.831 
3.527 
3.819 
2.819 
3.505 
3.792 
2.807 
3.485 
3.768 
2.797 
3.467 
3.745 
2.787 
3.450 
3.725 
2.779 
3.435 
3.707 
2.771 
3.421 
3.690 
2.763 
3.408 
3.674 
2.756 
3.396 
3.659 
2.750 
3.385 
3.646 
2.704 
3.307 
3.551 
2.660 
3.232 
3.460 
2.639 
3.195 
3.416 
2.626 
3.174 
3.390 
2.576 
3.090 
3.291 

Table 6.5 Poisson probability table 
- 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
bJ 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
I 1  
12 
13 
14 
- 
- 
- 
- 
- 
- 
- 
- 
P 
.1 
.2 
.3 
.4 
.5 
.6 
.7 
.8 
.9 
1.0 
,9048 
,8187 
,7408 
,6703 
,6065 
,5488 
,4966 
,4493 
,4066 
,3679 
,0905 
,1637 
,2222 
,2681 
,3033 
,3293 
,3476 
.3595 
,3659 
,3679 
,0045 
,0164 
,0333 
.0536 
,0758 
,0988 
,1217 
,1438 
,1647 
,1839 
,0002 
,001 1 
,0033 
,0072 
,0126 
,0198 
,0284 
,0383 
,0494 
,061 3 
.OOOO 
,0001 
,0003 
,0007 
,0016 
,0030 
,0050 
,0077 
,011 1 
,0153 
,0000 
.0000 
.0000 
,0001 
,0002 
,0004 
,0007 
,0012 
,0020 
,003 1 
.0000 
,0000 
,0000 
.0000 
,0000 
.oooO 
,000 
1 
,0002 
,0003 
,0005 
.m .om 
.oooo 
.oooo .oooo 
,0000 
,0000 .m .0000 
.0001 
P 
1.1 
1.2 
1.3 
1.4 
1.5 
1.6 
1.7 
1.8 
1.9 
2.0 
,3329 
,3012 
,2725 
,2466 
,2231 
,2019 
,1827 
. I653 
. I496 
,1353 
,3662 
,3614 
,3543 
,3452 
,3347 
,3230 
,3106 
,2975 
,2842 
,2707 
,2014 
,2169 
,2303 
,2417 
.25 10 
,2584 
,2640 
,2678 
,2700 
,2707 
,0738 
,0867 
,0998 
,1128 
,1255 
,1378 
,1496 
,1607 
,1710 
,1804 
,0203 
,0260 
,0324 
,0395 
,0471 
,055 1 
,0636 
,0723 
,0812 
,0902 
,0045 
,0062 
,0084 
,011 1 
,0141 
,0176 
,0216 
,0260 
,0309 
,0361 
,0008 
,0012 
,001 8 
,0026 
,0035 
,0047 
,0061 
,0078 
,0098 
,0120 
,0001 
,0002 
,0003 
,0005 
,0008 
.0011 
,0015 
,0020 
,0027 
,0034 
.OOOO 
,0000 
,000 1 
,0001 
,000 1 
,0002 
,0003 
,0005 
.OOO6 
,0009 
.oooo 
.0000 .m .oooo 
.oooo .moo 
,0001 
.0001 
,0001 
,0002 
P 
2.1 
2.2 
2.3 
2.4 
2.5 
2.6 
2.7 
2.8 
2.9 
3.0 
,1225 
,1108 
,1003 
,0907 
,0821 
,0743 
,0672 
,0608 
,0550 
,0498 
,2572 
.2438 
,2306 
,2177 
,2052 
,193 1 
,1815 
,1703 
,1596 
. I494 
,2700 
,2681 
,2652 
,2613 
,2565 
.25 10 
,2450 
,2384 
,2314 
,2240 
,1890 
, I966 
,2033 
,2090 
,2138 
,2176 
,2205 
,2225 
,2237 
,2240 
,0992 
,1082 
,1169 
,1254 
,1336 
,1414 
,1488 
,1557 
,1622 
.1680 
,0417 
,0476 
,0538 
,0602 
,0668 
.0735 
,0804 
,0872 
,0940 
,1008 
.O 146 
,0174 
,0206 
.0241 
,0278 
,0319 
,0362 
,0407 
,0455 
,0504 
,0044 
,0055 
,0068 
,0083 
,0099 
,0118 
,0139 
,0163 
,0188 
,0216 
,001 1 
,0015 
,0019 
,0025 
.0031 
,0038 
,0047 
,0057 
,0068 
,0081 
,0003 
,0004 
,0005 
,0007 
,0009 
,001 1 
,0014 
,001 8 
,0022 
,0027 
.OOOl 
,0001 
.0001 
,0002 
,0002 
,0003 
,0004 
,0005 
,0006 
,0008 
.oooo 
.om0 .m .oooo 
.0000 
.0001 
,000 1 
,000 1 
,0002 
,0002 
.oooo 
,0000 .m .0000 .m ,0000 
.0000 
.0000 
.0000 
,000 
1 
P 
3.1 
3.2 
3.3 
3.4 
3.5 
3.6 
3.7 
3.8 
3.9 
4.0 
,0450 
,0408 
,0369 
,0334 
,0302 
,0273 
.0247 
,0224 
,0202 
,0183 
,1397 
,1304 
. I217 
,1135 
,1057 
,0984 
,0915 
,0850 
,0789 
,0733 
.2 165 
,2087 
,2008 
,1929 
,1850 
,1771 
,1692 
,1615 
,1539 
,1465 
,2237 
,2226 
,2209 
,2186 
,2158 
,2125 
,2087 
,2046 
,2001 
,1954 
,1733 
,1781 
,1823 
,1858 
,1888 
,1912 
,1931 
,1944 
,1951 
,1954 
,1075 
,1140 
,1203 
,1264 
,1322 
,1377 
.1429 
,1477 
,1522 
,1563 
,0555 
,0608 
,0662 
,0716 
,0771 
,0826 
,0881 
,0936 
,0989 
,1042 
,0246 
,0278 
.03 12 
,0348 
,0385 
,0425 
.0466 
,0508 
,055 1 
,0595 
,0095 
.O 
1 1 1 
,0129 
,0148 
.O 169 
,0191 
.02 15 
,024 1 
,0269 
,0298 
,0033 
,0040 
,0047 
,0056 
,0066 
,0076 
,0089 
.O 102 
,0116 
,0132 
.0010 
,0013 
,0016 
,0019 
,0023 
,0028 
,0033 
,0039 
,0045 
,0053 
,0003 
.00W 
,0005 
,0006 
,0007 
,0009 
,001 1 
,0013 
,0016 
,0019 
,0001 
,000 
1 
,000 1 
,0002 
,0002 
.0003 
.0003 
,0004 
,0005 
,0006 
,0000 .m .0000 
.oooo 
,0001 
,0001 
,0001 
.0001 
,0002 
,0002 
.oooo 
.oooo 
.moo 
.oooo 
.0000 
.0000 
,0000 
.0000 
.0000 
,000 1 

Table 8.5 (Continued) 
- 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
Y 
0 
1 
2 
3 
4 
5 
6 
7 
- 
- 
- 
- 
- 
- 
P 
4.2 
4.4 
4.6 
4.8 
5.0 
5.2 
5.4 
5.6 
5.8 
6.0 
.O 150 
,0123 
,0101 
,0082 
.0067 
,0055 
.0045 
,0037 
,0030 
,0025 
,0630 
,1323 
,1852 
,1944 
,1633 
,1143 
,0686 
,0360 
,0168 
,0071 
,0027 
,0009 
,0003 
,0001 
.0000 
,0000 
.0000 
,0540 
,1188 
,1743 
,1917 
,1687 
,1237 
.0778 
,0428 
,0209 
,0092 
,0037 
,0013 
,0005 
,0001 
.0000 
.0000 
.oooo 
,0462 
,0395 
,1063 
,0948 
,1631 
.1517 
,1875 
,1820 
,1725 
,1747 
,1323 
,1398 
,0869 
,0959 
,0500 
,0575 
,0255 
,0307 
,0118 
,0147 
,0049 
,0064 
,0019 
,0026 
,0007 
,0009 
.OOO2 
,0003 
,0001 
.oO01 
.0000 
.Oooo 
.oooo .oooo 
,0337 
,0287 
,0842 
,0746 
,1404 
,1293 
,1755 
,1681 
.1755 
,1748 
,1462 
,1515 
,1044 
,1125 
,0653 
,0731 
,0363 
,0423 
,0181 
,0220 
,0082 
,0104 
,0034 
.0045 
,0013 
,0018 
,0005 
.OW7 
,0002 
,0002 
.0000 
,0001 
.oooo 
.oooo 
P 
.0244 
,0659 
,1185 
,1600 
,1728 
,1555 
,1200 
,0810 
,0486 
,0262 
.O 129 
,0058 
,0024 
,0009 
,0003 
.0001 
.0000 
,0207 
,0580 
,1082 
,1515 
,1697 
,1584 
,1267 
,0887 
,0552 
,0309 
,0157 
,0073 
,0032 
,0013 
,0005 
,0002 
,0001 
,0176 
,0509 
,0985 
,1428 
,1656 
,1601 
,1326 
,0962 
,0620 
,0359 
,0190 
,0092 
,0041 
,0017 
,0007 
,0002 
,0001 
,0149 
,0446 
,0892 
,1339 
,1606 
,1606 
.1377 
.1033 
,0688 
,0413 
,0225 
,0113 
,0052 
,0022 
,0009 
,0003 
,0001 
~ 
6.2 
6.4 
6.6 
6.8 
7.0 
7.2 
7.4 
7.6 
7.8 
8.0 
,0020 
,0017 
,0014 
,001 1 
,0009 
,0007 
,0006 
,0005 
,0004 
,0003 
,0126 
,0106 
.0090 
,0076 
.0064 
,0054 
,0045 
,0038 
,0032 
,0027 
,0390 
.0340 
,0296 
,0258 
,0223 
,0194 
,0167 
,0145 
,0125 
,0107 
,0806 
,0726 
,0652 
,0584 
,0521 
,0464 
,0413 
,0366 
,0324 
,0286 
,1249 
. 1 162 
,1076 
.0992 
.0912 
,0836 
,0764 
,0696 
,0632 
,0573 
,1549 
,1487 
,1420 
,1349 
,1277 
,1204 
. 1 130 
,1057 
,0986 
,0916 
,1601 
,1586 
,1562 
,1529 
,1490 
,1445 
,1394 
,1339 
.1282 
,1221 
,1418 
,1450 
,1472 
,1486 
,1490 
,1486 
,1474 
,1454 
,1428 
,1396 
,1099 
,1160 
,1215 
,1263 
,1304 
,1337 
,1363 
,1381 
,1392 
,1396 
,0757 
,0825 
,0891 
,0954 
,1014 
,1070 
,1121 
,1167 
. I207 
,1241 
,0469 
,0528 
,0588 
,0649 
.07 10 
,0770 
,0829 
,0887 
.0941 
,0993 
,0265 
,0307 
,0353 
,0401 
,0452 
,0504 
,0558 
,0613 
,0667 
,0722 
,0137 
,0164 
,0194 
,0227 
,0263 
,0303 
,0344 
,0388 
,0434 
,0481 
,0065 
,0081 
,0099 
,0119 
,0142 
,0168 
,0196 
,0227 
,0260 
,0296 
,0029 
,0037 
,0046 
,0058 
,007 1 
,0086 
,0104 
.O 123 
,0145 
,0169 
,0012 
,0016 
,0020 
,0026 
,0033 
,0041 
,0051 
,0062 
,0075 
,0090 
,0005 
,0006 
,0008 
,001 1 
,0014 
,0019 
,0024 
,0030 
,0037 
.0045 
,0002 
,0002 
,0003 
,0004 
,0006 
,0008 
,0010 
,001 3 
,0017 
,0021 
,0001 
,0001 
,0001 
,0002 
.0002 
,0003 
,0004 
,0006 
,0007 
,0009 
.0000 
.OOOO 
.OOOO 
,0001 
,0001 
,0001 
,0002 
,0002 
,0003 
,0004 
.oooo 
.0000 
.oooo 
.0000 
.oooo 
.oooo 
,0001 
,0001 
.0001 
,0002 
.0000 
.oooo 
.oooo 
.0000 
.oooo 
.0000 
.oooo 
.0000 
.oooo 
,000 1 
P 
8.2 
8.4 
8.6 
8.8 
9.0 
9.2 
9.4 
9.6 
9.8 
10.0 
,0003 
,0002 
,0002 
,0002 
,000 1 
,000 1 
,000 1 
,000 1 
,000 1 
.OOOO 
,0023 
,0019 
,0016 
,0013 
,001 1 
,0009 
,0008 
,0007 
,0005 
,0005 
,0092 
,0079 
,0068 
,0058 
,0050 
,0043 
,0037 
,0031 
,0027 
,0023 
,0252 
,0222 
,0195 
,017 1 
,0150 
,013 1 
,0115 
,0100 
,0087 
,0076 
.05 17 
,0466 
,0420 
,0377 
,0337 
,0302 
,0269 
.0240 
,0213 
,0189 
,0849 
,0784 
,0722 
,0663 
,0607 
,0555 
,0506 
,0460 
,041 8 
,0378 
,1160 
,1097 
,1034 
,0972 
.09 1 1 
,085 1 
,0793 
,0736 
,0682 
,0631 
,1358 
,13 17 
, I271 
,1222 
,1171 
, 1 1 18 
,1064 
,1010 
,0955 
,0901 

Table 6.5 (Continued) 
- 
Y 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
gr 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
- 
- 
- 
- 
8.2 
,1392 
,1269 
,1040 
,0776 
,0530 
,0334 
,0196 
,0107 
.0055 
,0026 
,0012 
,0005 
,0002 
,0001 
.oooo 
.oooo 
.oooo 
10.5 
.0000 
.0003 
,0015 
,0053 
,0139 
.0293 
,0513 
,0769 
,1009 
,1177 
,1236 
,1180 
,1032 
,0834 
,0625 
,0438 
,0287 
,0177 
.O 104 
,0057 
,0030 
,0015 
,0007 
,0003 
,000 1 
,0001 
.oooo 
.oooo 
.oooo 
.oooo 
.oooo 
.oooo 
.oooo 
8.4 
,1382 
,1290 
,1084 
,0828 
,0579 
,0374 
,0225 
,0126 
,0066 
,0033 
,0015 
,0007 
,0003 
.ooo1 
.moo 
.OOoo 
.oooo 
11.0 
. OOOO 
,0002 
,0010 
,0037 
,0102 
,0224 
,041 1 
,0646 
,0888 
,1085 
,1194 
,1194 
,1094 
,0926 
,0728 
,0534 
,0367 
,0237 
,0145 
,0084 
,0046 
,0024 
,0012 
,0006 
,0003 
,0001 
.oooo 
.oooo 
.oooo 
.0000 
.oooo 
.oooo 
.oooo 
8.6 
,1366 
,1306 
,1123 
,0878 
,0629 
,0416 
,0256 
,0147 
,0079 
.0040 
,0019 
,0009 
,0004 
,0002 
.0001 
.oooo 
.0000 
11.5 
.oooo 
.0001 
,0007 
,0026 
,0074 
,0170 
,0325 
,0535 
,0769 
,0982 
,1129 
,1181 
,1131 
,1001 
.0822 
,0630 
,0453 
,0306 
,0196 
,0119 
.0068 
,0037 
,0020 
,0010 
,0005 
,0002 
,0001 
.oooo 
.0000 
.oooo 
.oooo 
.0000 
.oooo 
8.8 
,1344 
,1315 
,1157 
,0925 
,0679 
,0459 
,0289 
.0169 
,0093 
,0048 
,0024 
,001 1 
,0005 
,0002 
,0001 
.oOOo 
.moo 
12 
.oooo 
,0001 
,0004 
,0018 
,0053 
,0127 
,0255 
,0437 
,0655 
,0874 
,1048 
,1144 
,1144 
.lo56 
,0905 
,0724 
,0543 
,0383 
,0255 
,0161 
,0097 
,0055 
,0030 
,0016 
.0008 
.0004 
,0002 
,000 1 
.0000 
.0000 
.oooo 
.oooo 
.0000 
!J 
9.0 
9.2 
,1318 
,1286 
.1318 
,1315 
,1186 
,1210 
,0970 
,1012 
,0728 
,0776 
,0504 
,0549 
.0324 
,0361 
.0194 
,0221 
,0109 
,0127 
,0058 
,0069 
,0029 
,0035 
,0014 
,0017 
,0006 
.0008 
.OW3 
,0003 
,0001 
,0001 
.m ,0001 
.OOOo 
.oooo 
!J 
12.5 
13.0 
.oOOo 
.0000 
.oooo .0000 
.ooo3 
,0002 
,0012 
.0008 
,0038 
,0027 
,0095 
,0070 
,0197 
,0152 
,0353 
,0281 
,0551 
,0457 
,0765 
,0661 
,0956 
,0859 
,1087 
,1015 
,1132 
,1099 
,1089 
,1099 
,0972 
,1021 
,0810 
,0885 
,0633 
,0719 
,0465 
,0550 
,0323 
,0397 
,0213 
.0272 
,0133 
,0177 
,0079 
,0109 
,0045 
,0065 
,0024 
,0037 
,0013 
,0020 
,0006 
,0010 
.OOO3 
,0005 
,0001 
,0002 
,0001 
,0001 
.oooo 
,0001 
.oooo 
.0000 
.oooo 
.oooo 
.0000 
.oooo 
9.4 
,1251 
,1306 
,1228 
,1049 
,0822 
,0594 
,0399 
,0250 
,0147 
,0081 
,0042 
,0021 
,0010 
,0004 
.ooo2 
,0001 
.00m 
13.5 
.oooo 
.om0 
.0001 
,0006 
,0019 
,0051 
.01 I5 
,0222 
,0375 
,0563 
,0760 
,0932 
,1049 
,1089 
,1050 
,0945 
,0798 
,0633 
,0475 
,0337 
,0228 
,0146 
,0090 
,0053 
.0030 
.0016 
.0008 
,0004 
,0002 
,0001 
.oooo 
.0000 
.oooo 
9.6 
,1212 
,1293 
,1241 
,1083 
,0866 
,0640 
,0439 
,0281 
,0168 
,0095 
,005 1 
,0026 
,0012 
,0006 
,0002 
.OOo1 
.oooo 
14.0 
.0000 
.moo 
,0001 
,0004 
,0013 
,0037 
,0087 
,0174 
,0304 
,0473 
,0663 
,0844 
,0984 
,1060 
,1060 
,0989 
,0866 
,0713 
,0554 
,0409 
,0286 
,0191 
,0121 
,0074 
,0043 
,0024 
,0013 
,0007 
,0003 
,0002 
,0001 
.0000 
.oooo 
9.8 
,1170 
,1274 
,1249 
,1112 
,0908 
,0685 
,0479 
,0313 
,0192 
,0111 
,0060 
,0031 
,0015 
,0007 
,0003 
,0001 
,0001 
14.5 
.oooo 
.0000 
,000 1 
,0003 
,0009 
.0027 
,0065 
,0135 
,0244 
.0394 
,0571 
,0753 
,0910 
,1014 
,1051 
,1016 
,0920 
,0785 
,0632 
,0483 
,0350 
,0242 
,0159 
,0100 
,0061 
,0035 
,0020 
,001 1 
,0005 
,0003 
,0001 
.0001 
.oooo 
10.0 
,1126 
,1251 
,1251 
,1137 
,0948 
,0729 
,0521 
,0347 
,0217 
,0128 
,0071 
,0037 
,0019 
,0009 
,0004 
,0002 
,0001 
15.0 
.oOOO 
.0000 
.moo 
,0002 
,0006 
,0019 
,0048 
,0104 
,0194 
,0324 
,0486 
,0663 
,0829 
,0956 
,1024 
,1024 
,0960 
,0847 
,0706 
,0557 
,0418 
,0299 
,0204 
,0133 
,0083 
,0050 
,0029 
,0016 
,0009 
,0004 
,0002 
.c001 
,0001 
- 
- 
- 

Table B.6 
Chi-squared distribution 
UDDer Tail Area 
- 
df - 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 - 
,995 
.0000 
,0100 
.07 17 
,2070 
,4117 
,6757 
,9893 
1.344 
1.735 
2.156 
2.603 
3.074 
3.565 
4.075 
4.601 
5.142 
5.697 
6.265 
6.844 
7.434 
8.034 
8.643 
9.260 
9.886 
10.520 
11.160 
1 1.808 
12.461 
13.121 
13.787 
14.458 
15.134 
15.815 
16.501 
17.192 
17.887 
18.586 
19.289 
19.996 
20.707 
.99 
,0002 
,0201 
,1148 
,2971 
,5543 
,8721 
1.239 
1.647 
2.088 
2.558 
3.054 
3.571 
4.107 
4.660 
5.229 
5.812 
6.408 
7.015 
7.633 
8.260 
8.897 
9.543 
10.196 
10.856 
11.524 
12.198 
12.879 
13.565 
14.257 
14.954 
15.656 
16.362 
17.074 
17.789 
18.509 
19.233 
19.960 
20.691 
21.426 
22.164 
,975 
,0010 
,0506 
,2158 
,4844 
,8312 
1.237 
1.690 
2.180 
2.700 
3.247 
3.816 
4.404 
5.009 
5.629 
6.262 
6.908 
7.564 
8.231 
8.907 
9.591 
10.283 
10.982 
11.689 
12.401 
13.120 
13.844 
14.573 
15.308 
16.047 
16.791 
17.539 
18.291 
19.047 
19.806 
20.569 
21.336 
22.106 
22.879 
23.654 
24.433 
.95 
,0039 
,1026 
,3518 
,7107 
1.146 
1.635 
2.167 
2.733 
3.325 
3.940 
4.575 
5.226 
5.892 
6.571 
7.261 
7.962 
8.672 
9.391 
10.117 
10.851 
11.591 
12.338 
13.091 
13.848 
14.61 1 
15.379 
16.151 
16.928 
17.708 
18.493 
19.281 
20.072 
20.867 
21.664 
22.465 
23.269 
24.075 
24.884 
25.695 
26.509 
.90 
,0158 
,2107 
,5844 
1.064 
1.610 
2.204 
2.833 
3.490 
4.168 
4.865 
5.578 
6.304 
7.042 
7.790 
8.547 
9.312 
10.085 
10.865 
11.651 
12.443 
13.240 
14.042 
14.848 
15.659 
16.473 
17.292 
18.114 
18.939 
19.768 
20.599 
21.434 
22.271 
23.110 
23.952 
24.797 
25.643 
26.492 
21.343 
28.196 
29.051 
.50 
,4549 
1.386 
2.366 
3.357 
4.352 
5.348 
6.346 
7.344 
8.343 
9.342 
10.341 
11.340 
12.340 
13.339 
14.339 
15.339 
16.338 
17.338 
18.338 
19.337 
20.337 
21.337 
22.337 
23.337 
24.337 
25.337 
26.336 
27.336 
28.336 
29.336 
30.336 
31.336 
32.336 
33.336 
34.336 
35.336 
36.336 
37.336 
38.335 
39.335 
.10 
2.706 
4.605 
6.251 
7.779 
9.236 
10.645 
12.017 
13.362 
14.684 
15.987 
17.275 
18.549 
19.812 
21.064 
22.307 
23.542 
24.769 
25.989 
27.204 
28.412 
29.615 
30.813 
32.007 
33.196 
34.382 
35.563 
36.741 
37.916 
39.088 
40.256 
4 1.422 
42.585 
43.745 
44.903 
46.059 
47.212 
48.363 
49.513 
50.660 
51.805 
.05 
3.842 
5.992 
7.815 
9.488 
11.071 
12.592 
14.067 
15.507 
16.919 
18.307 
19.675 
21.026 
22.362 
23.685 
24.996 
26.296 
27.587 
28.869 
30.144 
31.410 
32.671 
33.924 
35.173 
36.415 
37.652 
38.885 
40.113 
41.337 
42.557 
43.773 
44.985 
46.194 
47.400 
48.602 
49.802 
50.999 
52.192 
53.384 
54.572 
55.759 
,025 
5.024 
7.378 
9.349 
11.143 
12.833 
14.449 
16.013 
17.535 
19.023 
20.483 
21.920 
23.337 
24.736 
26.119 
27.488 
28.845 
30.191 
31.526 
32.852 
34.170 
35.479 
36.781 
38.076 
39.364 
40.647 
41.923 
43.195 
44.461 
45.722 
46.979 
48.232 
49.480 
50.725 
5 1.966 
53.203 
54.437 
55.668 
56.896 
58.120 
59.342 
.01 
6.635 
9.210 
1 1.345 
13.277 
15.086 
16.812 
18.475 
20.090 
21.666 
23.209 
24.725 
26.217 
27.688 
29.141 
30.578 
32.000 
33.409 
34.805 
36.191 
37.566 
38.932 
40.289 
41.638 
42.980 
44.314 
45.642 
46.963 
48.278 
49.588 
50.892 
52.191 
53.486 
54.776 
56.061 
57.342 
58.619 
59.893 
61.162 
62.428 
63.691 
.005 
7.879 
10.597 
12.838 
14.860 
16.750 
18.548 
20.278 
21.955 
23.589 
25.188 
26.757 
28.300 
29.820 
31.319 
32.801 
34.267 
35.719 
37.157 
38.582 
39.997 
41.401 
42.796 
44.181 
45.559 
46.928 
48.290 
49.645 
50.993 
52.336 
53.672 
55.003 
56.328 
57.648 
58.964 
60.275 
61.581 
62.883 
64.181 
65.476 
66.766 

c 
Using the Included 
Minitab Macros 
Minitab macros for performing Bayesian analysis and for doing Monte Carlo sim- 
ulations are included. The address may be downloaded from the Web page for this 
text on the site <www.wiley.com>. The Minitab Macros are zipped up in a package 
called BolstadMinitab. Macros.zip. Some Minitab worksheets are also included at 
that site. 
To use the Minitab macros, define a directory named BAYESMAC on your hard 
disk. The best place is inside the Minitab directory, which is often called MTBWIN 
on PC's running Microsoft Windows. For example, on my PC, BAYESMAC is inside 
MTBWIN, which is within the directory Programjles which is on drive C. The correct 
path I need to invoke to use these macros is C:/progra - l/MTBWIN/BAYESM -11 
(Note that the the filenames are truncated at six characters.) You should also define 
a directory BAYESMTW for the Minitab worksheets containing the data sets. The 
best place is also inside the Minitab directory, so you can find it easily. 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
371 

372 
USING THE INCLUDED MINITAB MACROS 
Table C. 7 Sampling Monte Carlo study 
Minitab Commands 
Meaning 
%<insert path>sscsample.mac c l  100; 
strata c2 3; 
"data are in cl, N = 100" 
"there are 3 strata stored in c2" 
cluster c3 20; 
type 1; 
size 20; 
mcarlo 200; 
"there are 20 clusters stored in c3" 
l=simple, 2=stratified, 3=cluster 
"sample size n = 20" 
"Monte Carlo sample size 200" 
output c6 c7 c8 c9; 
"c6 contains sample means, ~ 7 x 9  
contain numbers in each strata" 
CHAPTER 2: SCIENTIFIC DATA GATHERING 
Sampling Methods 
We use the Minitab macro sscsample.mac to perform a small-scale Monte Carlo study 
on the efficiency of simple, stratified, and cluster random sampling on the population 
data contained in sscsample.mtw. In the "file" menu pull down "open worksheet" 
command. When the dialog box opens, find the directory BAYESMTW and type in 
sscsample.mtw in the filename box and click on "open". In the "edit" menu pull down 
"command line editor" and type in the commands from Table C. 1 into the command 
line editor: 
Experimental Design 
We use the Minitab macro Xdesign.mac to perform a small-scale Monte Carlo study, 
comparing completely randomized design and randomized block design in their ef- 
fectiveness for assigning experimental units into treatment groups. In the "edit" menu 
pull down "command line editor" and type in the commands from Table C.2. 
CHAPTER 6: BAYESIAN INFERENCE FOR DISCRETE RANDOM 
VARIABLES 
Binomial Proportion with Discrete Prior 
BinoDRmac is used to find the posterior when we have binomial (n, T )  observation, 
and we have a discrete prior for T .  For example, suppose T has the discrete distribution 
with three possible values, .3, .4, and .5. Suppose the prior distribution is given in 
Table C.3, and we want to find the posterior distribution after n = 6 trials and observing 
y = 5 successes. In the "edit" menu pull down "command line editor" and type in the 
commands from Table C.4. 

USING THE INCLUDED MINITAB MACROS 
373 
Table C.2 Experimental design Monte Carlo study 
Minitab Commands 
Meaning 
let kl=.8 
random 80 cl c2; 
normal 0 1. 
let c2=sqrt( l-k1**2)*~2+kl*cl 
desc cl c2 
corr cl c2 
plot c2*cl 
%<insertpath>Xdesign.mac c 1 c2; 
size 20; 
treatments 4; 
mcarlo 500; 
output c3 c4 c5. 
code (1:4) 1 (5:8) 2 c5 c6 
desc c4; 
by c6. 
"correlation between other and response 
variables" 
"generate 80 other and response variables 
in cl and c2, respectively" 
"give them correlation kl" 
"summary statistics" 
"shows relationship" 
"other variable in cl, response in c2" 
"treatment groups of 20 units" 
"4 treatment groups" 
"Monte Carlo sample size 500" 
"c3 contains other means, 
c4 contains response means, 
c5 contains treatment groups 
1-4 from completely randomized design 
5-8 from randomized block design" 
"summary statistics " 
Table C.3 Discrete prior distribution for binomial proportion x 
7r 
dr) 
.3 
.2 
.4 
.3 
.5 
.5 
Poisson Parameter with Discrete Prior 
PoisDPmac is used to find the posterior when we have a Poisson(p) observation, and 
a discrete prior for p. For example, suppose p has three possible values p = 1,2, or 3 
where the prior probabilities are given in Table (2.5, and we want to find the posterior 
distribution after observing y = 4. In the "edit" menu go down to "command line 
editor" and type in the commands from Table C.6 . 

374 
USING THE INCLUDED MlNlTAB MACROS 
Table C.4 
for T 
Minitab Commands 
Meaning 
Finding the posterior distribution of binomial proportion with a discrete prior 
set cl 
.3 .4 .5 
end 
set c2 
.2 .3 .5 
end 
%<insert path>BinoDP.mac 6 5 ;  
prior cl c2; 
likelihood c3; 
posterior c4. 
"puts T in cl" 
"puts g(n) in c2" 
"n = 6 trials, y = 5 successes observed" 
"T in cl, prior g(T) in c2" 
"store likelihood in c3" 
"store posterior g(Tly = 5 )  in c4" 
Table C.5 Discrete prior distribution for Poisson parameter p 
2 
.4 
3 
.3 
CHAPTER 8: BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
Beta(a, b) Prior for 7r 
BinoBflmac is used to find the posterior when we have binomial (n, T )  observation, 
and we have a beta(a, b) prior for T .  The beta family of priors is conjugate for 
binomial (n, T )  observations, so the posterior will be another member of the family, 
beta(a', b') where a' = a + y and b' = b + n - y. For example, suppose we have 
n = 12 trials, and observe y = 4 successes, and we use a beta(3,3) prior for T .  In the 
"edit" menu pull down "command line editor" and type in the commands from Table 
C.7. We can find the posterior mean and standard deviation from the output. We can 
determine a Bayesian credible interval for T by looking at the values of T by pulling 
down the "calc" menu to "probability distributions" and over to "beta" and selecting 
"inverse cumulative probability". We can test HO : T 5 TO vs. H I  : T > TO by 
pulling down the "calc" menu to "probability distributions" and over to "beta" and 
selecting cumulative probability" and imputing the value of TO. 

USING THE INCLUDED MINITAB MACROS 
375 
Table C.6 
P 
Finding the posterior distribution of Poisson parameter with a discrete prior for 
Minitab Commands 
Meaning 
set c5 
4 
end 
set cl 
1 2 3  
end 
set c2 
.3 .4 .3 
end 
%<insert path>PoisDP.mac c5; 
prior cl c2; 
likelihood c3; 
posterior c4. 
"puts observation(s) y in c5" 
"puts p in cl " 
"puts g(p) in c2" 
"observations in c5" 
"p in c 1, prior g(p) in c2" 
"store likelihood in c3" 
"store posterior g(nly = 5) in c4" 
Table C.7 
Minitab Commands 
Meaning 
%<insert path>BinoBP.mac 12 4; 
beta 3 3; 
prior cl c2; 
likelihood c3; 
posterior c4. 
Finding the posterior distribution of binomial proportion with a beta prior for K 
"n = 12 trials, y = 4 was observed" 
"the beta prior" 
"stores 7r and the prior g(.rr)" 
"store likelihood in c3" 
"store posterior g(.rrly = 4) in c4" 
General Continuous Prior for ?r 
BinoCCPmac is used to find the posterior when we have binomial (n, 
T )  observation, 
and we have a general continuous prior for 7r. Note, .rr must go from 0 to 1 in equal 
steps, and g(7r) must be defined at each of the 7r values. For example, suppose we 
have n = 12 trials, and observe y = 4 successes, where 7r is stored in c l  and a general 
continuous prior g(r) is stored in c2. In the "edit" menu pull down "command line 
editor" and type in the commands from Table C.8. The output of BinoCCPmac 
does not print out the posterior mean and standard deviation. Neither does it print 
out the values that give the tail areas of the integrated density function that we need 
to determine credible interval for 7r. Instead we use the macro tintegral.mac which 
numerically integrates a function over its range to determine these things. We can 
find the integral of the posterior density g(7rly) using this macro. We can also use 

376 
USING THE INCLUDED MINITAB MACROS 
Table C.8 
for T 
Finding the posterior distribution of binomial proportion with a continuous prior 
Minitab Commands 
Meaning 
%<insert path>BinoGCP.mac 12 4; 
prior cl c2; 
likelihood c3; 
posterior c4. 
" n  = 12 trials, y = 4 successes observed" 
"inputs 7r in cl, prior g(r) in c2" 
"store likelihood in c3" 
"store posterior g(.irly = 4) in c4" 
Table C.9 Bayesian inference using posterior density of binomial proportion T 
Minitab Commands 
Me an i n g 
%<insert path>tintegral.mac c l  c4; 
output kl c6. 
let c7=c 1 *c4 
%<insert path>tintegral.mac c l  c7; 
output kl c8. 
let c9=(cl-k1)**2 * c4 
%<insert path>tintegral.mac c l  c9; 
output k2 c 10. 
let k3=sqrt(k2) 
mint kl -k3 
"integrates posterior density" 
"stores definite integral over range in kl" 
"stores definite integral function in c6" 
"finds posterior mean" 
"7r x d.irlY) 
"finds posterior variance" 
"finds posterior st. deviation" 
tintegral.mac to find the posterior mean and variance by numerically evaluating 
and 
1 
( d ) 2  
= 1 
(7r - rn')2g(7riy) d7r 
In the "edit" menu pull down "command line editor" and type in the commands from 
Table C.9 . A 95% Bayesian credible interval for .ir is found by taking the values in 
cl that correspond to ,025 and ,975 in c6. To test the hypothesis Ho : 7r 5 7r0 vs. 
H I  : 7r > T O ,  we find the value in c6 that corresponds to the value 7r0 in cl. If it is 
less than the desired level of significance a, then we can reject the null hypothesis. 

USING THE INCLUDED MINITAB MACROS 
377 
Table C.10 
for p 
Minitab Commands 
Meaning 
Finding the posterior distribution of a Poisson parameter with a gamma prior 
set c5 
3 4 3 0 1  
end 
let kl=6 
let k2=3 
%<insert path>PoisGamP.mac c5 ; 
gamma kl k2; 
prior cl c2; 
likelihood c3; 
posterior c4. 
"Put observations in c5" 
"r" 
"V" 
"observations in c5" 
"the gamma prior" 
"stores p and the prior g(p)" 
"store likelihood in c3" 
"store posterior g(p1y) in c4" 
CHAPTER 10: BAYESIAN INFERENCE FOR POISSON PARAMETER 
Gamma(r, w) Prior for p 
PoisGamPmac is used to find the posterior when we have a random sample from a 
Poisson(1) distribution, and we have agamma(r, w) prior for p. Thegamma family of 
priors is the conjugate family for Poisson observations, so the posterior will be another 
member of the family, gamma(r', w') where r' = r +  
y and v' = w fn. The simple 
rules are "add sum of observations to r" and "add number of observations to w". For 
example, suppose in column 5 there is a sample five observations from a Poisson(p) 
distribution. Suppose we want to use a gamma(6,3) prior for p. Pull down the "edit" 
menu to the "command line editor" command and type in the commands from Table 
C. 10. We can determine a Bayesian credible interval for p by looking at the values of 
p by pulling down the "calc" menu to "probability distributions" and over to "gamma" 
and selecting "inverse cumulative probability". Note: Minitab uses parameter 1/v 
instead of v. We can test HO : p 5 po vs. H I  : p > po by pulling down the "calc" 
menu to "probability distributions" and over to "gamma" and selecting cumulative 
probability" and imputing the value of po. 
General continuous prior for Poisson parameter p 
PoisGCPmac is used to find the posterior when we have a random sample from a 
Poisson(p) distribution and we have a continuous prior for p. Suppose we have a 
random sample of five observations in column c5. The prior density of p is found 
by linearly interpolating the values in Table C.ll. Pull down the "edit" menu to the 
"command line editor" and type in the commands in Table C.12 . The output of 
PoisGCPmac does not include the posterior mean and standard deviation. Neither 

378 
USING THE INCLUDED MINITAB MACROS 
Table C.71 
interpolating between these values. 
Continuous prior distribution for Poisson parameter p has shape given by 
P 
d P )  
0
0
 
2
2
 
4
2
 
8
0
 
Table C.72 
parameter for p 
Finding the posterior distribution of a Poisson parameter with a continuous 
Minitab Commands 
Meaning 
set c5 
3 4 3 0 1  
"Put observations in c5" 
end 
set cl 
0:8/ .001 
set p 
end 
0:2 / ,001 1999(2) 2:O /-,0005 
end 
%<insert path>PoisGCP.mac c5 ; 
prior cl c2; 
likelihood c3; 
posterior c4. 
set c2 
set d P )  
"observations in c5" 
"p and the prior g(p) in cl and c2" 
"store likelihood in c3" 
"store posterior g(p1y) in c4" 
does it print out the cumulative distribution function that allows us to find credible 
intervals. Instead we use the macro tintegral.mac which numerically integrates the 
posterior to do these things. Pull down the "edit" menu to "command line editor" and 
type in the commands from Table C.13. A 95% Bayesian credible interval for p is 
found by taking the values in cl that correspond to .025 and .975 in c6. To test the 
null hypothesis HO : ,LL 5 ,LLO vs. H I  : ,u > PO, find the value in c6 that corresponds 
to po in c 1. If it is less than the desired level of significance we can reject the null 
hypothesis at that level. 

USING THE INCLUDED MINITAB MACROS 
379 
Table C. 73 Bayesian inference using posterior distribution of Poisson parameter p 
Minitab Commands 
Meaning 
%<insert path>tintegral.mac cl c4; 
output kl c6. 
let c7=cl *c4 
%<insert path>tintegral.mac cl c7; 
output kl c8. 
let c9=(cl-k1)**2 * c4 
%<insert path>tintegral.mac cl c9; 
output k2 c10. 
let k3=sqrt(k2) 
print kl -k3 
"integrates posterior density" 
"stores definite integral over range in kl" 
"stores definite integral function in c6" 
"finds posterior mean" 
"P x S(PIY1,. . . 1  Yn) 
"finds posterior variance" 
"finds posterior st. deviation" 
Table C. 74 Discrete prior distribution for normal mean p 
P 
f(P) 
2 
.1 
2.5 
.2 
3 
.4 
3.5 
.2 
4 
.1 
CHAPTER 11: BAYESIAN INFERENCE FOR NORMAL MEAN 
Discrete Prior for p 
NormDPmac is used to find the posterior when we have a column of nonnal(p, u2) 
observations and u2 is known, and we have a discrete prior for p. (If the standard 
deviation r is not input, the estimate from the observations is used, and the approxi- 
mation to the posterior is found. For example, suppose p has the discrete distribution 
with 5 possible values, 2 2.5, 3, 3.5 and ,4. Suppose the prior distribution is given in 
Table C.14. and we want to find the posterior distribution after a random sample of 
n = 5 observations from a nomal(p, 1') that are 1.52,0.02,3.35,3.49 1.82 . In the 
"edit" menu pull down "command line editor" and type in the commands from Table 
C.15. 

380 
USING THE INCLUDED MINITAB MACROS 
Table C.75 Finding the posterior distribution of a normal mean with discrete prior for p 
Minitab Commands 
Meaning 
set cl 
2141.5 
puts "p in cl " 
end 
set c2 
.I .2.4.2 .I 
end 
set c5 
1.52,0.02,3.35,3.49 1.82 
end 
%<insert path>NormDP.mac c5 ; 
sigma 1; 
prior cl c2; 
likelihood c3; 
"puts g(b) in c2 
"puts data in c5 
"observed data in c5" 
" known u = 1 is used" 
"1 
in cI, prior g ( p )  in c2" 
"store likelihood in c3" 
posterior c4. 
"store posterior g(pjdata) in c4" 
NormaZ(m, s2) Prior for p 
NormNPmac is used when we have a column c5 containing a random sample of 
n observations from a normal(p, u2) distribution (with u2 known) and we use a 
normal(m, s2) prior distribution. If the observation standard deviation u is not input, 
the estimate calculated from the observations is used, and the approximation to the 
posterior is found. If the normal prior is not input, a flat prior is used. The normal 
family of priors is conjugate for normal (p, u2) observations, so the posterior will be 
another member of the family, normal[m', ( s ' ) ~ ]  where the new constants are given 
by 
1 
1
n
 
and 
For example, suppose we have a normal random sample of 4 observations from 
normal(p, 1') which are 2.99,5.56,2.83, and 3.47. Suppose we use anormal(3, 22) 
prior for p. In the "edit" menu pull down "command line editor" and type in the 
commands from Table (2.16 . We can determine a Bayesian credible interval for 
p by looking at the values of p by pulling down the "calc" menu to "probability 
distributions" and over to "normal" and selecting "inverse cumulative probability." 
We can test Ho : p 5 po vs. H I  : p > po by pulling down the "calc" menu to 

USING THE INCLUDED MINITAB MACROS 
381 
Table C. 16 Finding the posterior distribution of a normal mean with a normal prior for p 
Minitab Commands 
Meaning 
set c5 
2.99,5.56, 2.83,3.47 
end 
%<insert path>NormNP,mac c5 ; 
sigma 1 ; 
norm 3 2; 
"puts data in c5 
"observed data in c5" 
"known u = 1 is used" 
"prior mean 3, prior std 2" 
prior cl c2; 
likelihood c3; 
"store p in c 1, prior g(p) in c2" 
"store likelihood in c3" 
posterior c4. 
"store posterior g(p1data) in c4" 
"probability distributions" and over to "normal" and selecting cumulative probability" 
and imputing the value of PO. 
General Continuous Prior for ~1 
NormGCPmac is used when we have a column c5 containing a random sample of n 
observations from a normal (pl u2) distribution (with u2 known) and we have column 
c 1 containing values of p, and a column c2 containing values from a continuous prior 
g(p). If the standard deviation u is not imput, the estimate calculated from the data 
is used, and the approximation to the posterior is found. 
For example, suppose we have a normal random sample of 4 observations from 
normal (pl u2 = 1) which are 2.99, 5.56, 2.83, and 3.47. In the "edit" menu pull 
down "command line editor" and type the following commands from Table C. 17. 
The output of NormGCPmac does not print out the posterior mean and standard 
deviation. Neither does it print out the values that give the tail areas of the integrated 
density function that we need to determine credible interval for p. Instead we use 
the macro tintegralmac which numerically integrates a function over its range to 
determine these things. In the "edit" menu pull down "command line editor" and 
type in the commands from Table C.18. To find a 95% Bayesian credible interval 
we find the values in cl that correspond to .025 and .975 in c6. To test a hypothesis 
HO 
: p 5 po vs. H I  : p > po we find the value in c6 that corresponds to po in cl. 
If this is less than the chosen level of significance we can reject the null hypothesis 
at that level. 
CHAPTER 14: BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRES- 
SION 
BayesLinReg.mac is used to find the posterior distribution of the simple linear re- 
gression slope p when we have a random sample of ordered pairs (xi, 
yi) from the 

382 
USING THE INCLUDfD MINITAB MACROS 
Table C.77 
fi 
Finding the posterior distribution of a normal mean with a continuous prior for 
Minitab Commands 
Meaning 
set c5 
2.99,5.56,2.83,3.47 
"puts data in c5 
end 
%<insert path>NormGCP.mac c5 ; 
"observed data in c5" 
sigma 1; 
prior cl c2; 
likelihood c3; 
"known u = 1 is used" 
"p in cl, prior g(p) in c2" 
"store likelihood in c3" 
posterior c4. 
"store posterior g(p1data) in c4" 
Table C.78 Bayesian inference using posterior distribution of normal mean /I 
Minitab Commands 
Meaning 
%<insert path>tintegral.mac c l  c4; 
output kl c6. 
print c l  c6 
let c7=cl *c4 
%<insert path>tintegral.mac c 1 c7; 
output kl c8. 
let c8=(cl-k1)**2 * c4 
%<insert path>tintegral.mac cl c8; 
output k2 c9. 
let k3=sqrt(k2) 
print kl -k3 
"integrates posterior density 'I 
"stores definite integral over range in kl" 
"stores definite integral function in c6" 
"P x g(4data) 
"finds posterior mean" 
"finds posterior variance" 
"finds posterior st. deviation" 
simple linear regression model 
yi = a0 + P x xi + ei, 
where the observation errors ei are independent nomal(0, u2) with known variance. 
(If the variance is not known the posterior is found using the variance estimate 
calculated from the least squares residuals.) We use independent priors for the slope 
p and the intercept as. These can be either flat priors or normal priors. (The default 
is flat priors for both slope and intercept of z = 2.) This parameterization yields 
independent posterior distribution for slope and intercept with simple updating rules 
"posterior precision equals prior precision plus precision of least squares estimate" 

USING THE lNCLUDED MlNlTAB MACROS 
383 
Table C. 79 Bayesian inference for simple linear regression model 
Minitab Commands 
Meaning 
%<insert path>BayesLinReg.mac c5 c6; 
Sigma 2; 
PriSlope 0 3; 
PriIntcpt 30 10; 
predict c7 c8 c9. 
invcdf .975 k10; 
norm 0 1. 
let clO=c8-k10*c9 
let cl l=c8+klO*c9 
"y (response) in c5,z (predictor) in c6" 
known standard deviation cr = 2 
"normal(mp = 0 ,  sp = 3) prior" 
"normal(m,, = 30, s,, 
= 10) prior" 
"predict for x-values in c7, prediction 
in c8, standard deviations in c9" 
"Find critical value. Use normal when 
variance is known, use student's t 
with n - 2 df when variance not known" 
"Lower credible bound for predictions" 
"Upper credible bound for predictions" 
and "posterior mean is weighted sum of prior mean and the least squares estimate 
where the weights are the proportions of the precisions to the posterior precision. 
Suppose we have y and x in columns c5 and c6 respectively and we know the standard 
deviation u = 2. We wish to use a normal(0,32) prior for /3 and a normal(30, lo2) 
prior for aYz. 
Pull down the "edit" menu to the "command line editor" and type in 
the commands from Table C. 19. If we want to find a credible interval for the slope, 
use Equation 14.9 or Equation 14.10 depending on whether we knew the standard 
deviation or used the value calculated from the residuals. To find the credible interval 
for the predictions, use Equation 14.13 when we know the variance or use Equation 
14.14 when we use the estimate calculated from the residuals. 
CHAPTER 15: BAYESIAN INFERENCE FOR NORMAL STANDARD DEVI- 
ATION 
S x an Inverse Chi-Squared( I.) Prior for u2 
NVarICPmac is used when we have a column c5 containing a random sample of n 
observations from a normal(p, u2) distribution where the mean p is known. The 
Sx an inverse chi-squared(&) family of priors is the conjugate family for normal 
observations with known mean. The posterior will be another member of the family 
where the constants are given by the simple updating rules "add the sum of squares 
around the mean to S" and "add the sample size to the degrees of freedom." For 
example, suppose we have five observations from a normal(p, 02) where p = 200 
which are 206.4, 197.4,212.7,208.5, and 203.4. We want to use a prior that has prior 
median equal to 8. In the "edit" menu pull down to "command line editor" and type 
in the commands from Table C.20. Note: The graphs that are printed out are the prior 

384 
USING THE INCLUDED MINITAB MACROS 
Table C.20 
inverse chi-squared( K )  prior for u2 
Finding posterior distribution of normal standard deviation 0 using Sx an 
Minitab Commands 
Meaning 
set c5 
206.4, 197.4,212.7,208.5,203.4 
end 
%<insert path>NVarICP.mac c5 200; 
IChiSq 29.1 1 1; 
prior c l  c2; 
likelihood c3; 
posterior c4; 
constants kl k2. 
"puts data in c5 
"observed data in c5, known p = 200" 
"29.11 x inverse chi-squared( 1) has 
prior median 8" 
"0 in cl, prior g(o) in c2" 
"store likelihood in c3" 
"store posterior g(oldata) in c4" 
"store S' in kl, K’ in k2" 
Table C.27 
Bayesian inference using posterior distribution of normal standard deviation 0 
Minitab Commands 
Meaning 
let k3=sqrt(kl/(k2-2)) 
Print k3 
InvCDF .5 k4; 
Chisquare k2. 
in k4" 
let kS=sqrt(kl/k4) 
Print kS 
%<insert path>tintegral.mac cl c4; 
output k6 c6. 
"The estimator for o using posterior mean, 
Note: k2 must be greater than 2 " 
"store median of chi-squared (k2) 
"The estimator for u using posterior median" 
"integrates posterior density" 
"stores definite integral over range in k6, 
stores definite integral function in c6" 
distributions of the standard deviation u even though we are doing the calculations 
on the variance. 
If we want to make inferences on the standard deviation u using the posterior 
distribution we found, pull down the "edit" menu to "command line editor" and type 
in the commands given in Table C.21. To find an equal tail area 95% Bayesian 
credible interval for u, we find the values in c l  that correspond to .025 and .97S in 
c6. 

USING THE INCLUDED MINITAB MACROS 
385 
Table C.22 
for ir 
Finding the posterior distribution of binomial proportion with a mixture prior 
Minitab Commands 
Meaning 
%<insertpath>BinoMixP.mac 60 15 ; 
"n = 60 trials, y = 15 successes observed" 
bet0 10 6; 
bet1 1 1; 
"The precise beta prior" 
"The fall-back beta prior" 
prob .95; 
output cl-c4. 
"prior probability of first component" 
"store T ,  prior, likelihood, and posterior 
in cl-c4" 
CHAPTER 16: ROBUST BAYESIAN METHODS 
BinoMixPmac is used to find the posterior when we have a binomial(n, n) observa- 
tions and use a mixture of a beta(a0, bo) and a beta(a1, b l )  for the prior distribution 
for T .  Generally, the first component summarizes our prior belief, so that we give it a 
high prior probability. The second component has more spread to allow for our prior 
belief being mistaken, and we give it a low prior probability. For example, suppose 
our first component is beta(l0,6), and the second component is beta(1,l) and we 
give a prior probability of .95 to the first component. We have taken 60 trials and 
observed y = 15 successes. In the "edit" menu pull down "command line editor" and 
type in the commands from Table C.22. 
NonnMixRrnac is used to find the posterior when we have normal(p,u2) ob- 
servations with known variance u2 and our prior for ,u is a mixture of two normal 
distributions, a noml(rno, sg) and a nonnal(rnl, s:). 
Generally, the first compo- 
nent summarizes our prior belief, so we give it a high prior probability. The second 
component is a fall-back prior that has a much larger standard deviation to allow 
for our prior belief being wrong and has a much smaller prior probability. For ex- 
ample, suppose we have a random sample of observations from a nonnal(p, u2) in 
column c5 where u2 = .22. Suppose we use a mixture of a nomal(l0, .12) and a 
nonnal(l0, .42) prior where the prior probability of the first component is .95. In the 
"edit" menu pull down "command line editor" and type in the commands from Table 
C23. 

386 
USING THE INCLUDED MINITAB MACROS 
Table C.23 Finding the posterior distribution of normal mean with the mixture prior for p 
Minitab Commands 
Meaning 
%<insert pathrNormMixP.mac c5 ; 
sigma .2 ; 
npO 10 .1; 
npl 10.4; 
prob .95; 
output cl-c4. 
"c5 contains observations of normal(p, u2)" 
"known value u = .2 is used" 
"The precise normal(10, .12) prior" 
"The fall-back normal(l0, .4’) prior" 
"prior probability of first component" 
"store p, prior, likelihood, and posterior 
in cl-c4" 

D 
Using the Included 
R Functions 
James Curran 
University of Auckland 
OBTAINING AND USING R AND THE R FUNCTIONS 
R functions for performing Bayesian analysis and for doing Monte Car10 simulations 
are included. The address may be downloaded from the Web page for this text on the 
site www.wiley.com. The R functions are zipped up in a package called Bolstad-0.2- 
1 1. zip. 
The latest version of R (currently 2.41) may always be found at www,r-project.org. 
Compiled versions of R for Linux, Mac 0s (System 8.6 to 9.1 and Mac 0s X), Mac 
0s X (DarwidXl 1) and Windows (95 and later), and the source code (for those who 
wish to compile R themselves) may also be found at this address. 
To install R for Windows, double click on the file nv241.exe and follow the 
installer functions. In the following discussion it is assumed that you have copied the 
file Bolstad-0.2-ll.zip to a location on your computer. You can find it in this way: 
1 .  Start R from the Start menu or by double clicking on the icon on your desktop. 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 02007 John Wiley & Sons, Inc. 
387 

388 
USING THE INCLUDED R FUNCTIONS 
2. Pull down the 'Packages' menu and select the item 'Install package from local 
3. Use the dialog box to locate Bolstad_O.Z-ll.zip, select it and click on 'Open'. 
R will now recognize the package Bol s tad as a package it can load. To use the func- 
tions in the package Bolstad, either type library (Bolstad) at the command 
prompt or select the item 'Load package ...' from the 'Packages' menu. To see the 
list of functions contained within the package, type library (help=Bolstad) . 
This should bring up the list in Table ??: 
zip file ...' 
Function Name 
bayes.lin.reg 
binobp 
binodp 
binogcp 
binomixp 
normdp 
normgcp 
normmixp 
normnp 
nvaricp 
poisdp 
poisgamp 
poisgcp 
sintegral 
sscsample 
xdesign 
Description 
Bayesian inference for simple linear regression 
binomial sampling with a beta prior 
binomial sampling with a discrete prior 
binomial sampling with a general continuous prior 
binomial sampling with a beta mixture prior 
bayesian inference on a normal mean with a discrete prior 
Bayesian inference on a normal mean with a 
general continuous prior 
Bayesian inference on a normal mean with a mixture of 
normal priors 
Bayesian inference on a normal mean with a normal prior 
Bayesian inference for a normal standard deviation with a 
scaled inverse x2 distribution 
Poisson sampling with a discrete prior 
Poisson sampling with a gamma prior 
Poisson sampling with a general continuous prior 
numerical integration using Simpson's Rule 
simple, stratified and cluster sampling 
Monte Car10 study of randomized and blocked designs 
Help on each of the R functions is available once you have loaded the Bolstad 
package. There are a number of ways to access help files under R. The traditional 
way is to use the help or ? function. For example, to see the help file on the 
binodp function, type help (binodp) or ?binodp. HTML-based help is also 
available. To use HTML help, select 'Html help' from the 'Help' menu. Click on 
the 'Packages' link, and then the link for 'Bolstad'. This will bring up an index page 
where you may select the help file for the function you're interested in. 
All of the examples listed in the help file may be executed by using the example 
command. For example, to run the examples listed in the binodp help file type 
example (binodp) . 
Each help file has a standard layout, which is as follows: 
The R language has two special features that may make it confusing to users 
of other programming and statistical languages: default or optional arguments, and 

USING THE INCLUDED R FUNCTIONS 
389 
Title: a brief title that gives some idea of what the function is supposed to do or 
show 
Description: a fuller description of the what the function is supposed to do or show 
Usage: the formal calling syntax of the function 
Arguments: a description of each of the arguments of the function 
Values: a description of the values (if any) returned by the function 
See also: a reference to related functions 
Examples: some examples of how the function may be used. These examples may 
be run either by using the example command (see above) or copied and 
pasted into the R console window 
variable ordering of arguments. An R function may have arguments for which 
the author has specified a default value. Let’s take the function binobp as an 
example. The syntax of binobp is binobp (x, n, a = 1, b = 1, ret 
= FALSE). The function takes five arguments x, n, a, b, and ret. However, 
the author has specified default values for a, b, and ret, namely a = 1, b = 1 
and ret = FALSE. This means that the user only has to supply the arguments x 
and n. Therefore the arguments a, b and ret are said to be optional or default. 
In this example, by default, a betu(a = 1, b = 1) prior is used and the prior, 
likelihood, and posterior distributions (along with some associated information) are 
not returned (ret = FALSE). Hence the simplest example for binobp is given 
as binobp (6, 
8 )  . If the user wanted to change the prior used, say to betu(5,6), 
then they would type binobp ( 6 ,  8 ,  5, 6 ) . There is a slight catch here, which 
leads into the next feature. Assume that the user wanted to use a beta( 1,l) prior, but 
wanted to return the output. One might be tempted to type binobp (6 , 8 , FALSE) . 
This is incorrect. R will think that the value FALSE is the value being assigned 
to the parameter a, and convert it from a logical value, FALSE, to the numerical 
equivalent, 0, which will of course give an error because the parameters of the beta 
distribution must be greater than zero. The correct way to make such a call is to 
use named arguments, such as binobp (6, 
8 ,  ret=FALSE) .This specifically tells 
R which argument is to be assigned the value FALSE. This feature also makes the 
calling syntax more flexible because it means that the order of the arguments does not 
need to be adhered to. For example, binobp (n=8, x=6, ret=FALSE, a=l , 
b=3 would be a perfectly legitimate function call. 
CHAPTER 2: SCIENTIFIC DATA GATHERING 
In this chapter we use the function sscsample to perform a small-scale Monte 
Car10 study on the efficiency of simple, stratified, and cluster random sampling on 

390 
USING THE INCLUDED R FUNCTIONS 
the population data contained in sscsample. data. Make sure the Bolstad 
package is loaded by typing 
1 ibrary (Bol 
s tad) 
first. Type the following commands into the R console: 
sscsample(20,200) 
This calls the sscsample function and asks for 200 samples of size 20 to be 
drawn from the dataset sscsample .data. To return the means and the samples 
themselves, type 
resc-sscsample (20,200, 
ret=T) 
This will store all 200 samples and their means in an R list structure called res. The 
means of the sample may be accessed by typing 
re s $means 
The samples themselves are stored in the columns of a 20 x 200 matrix called 
res$samples. To access the ith sample, where i = 1,. . . ,200, type 
res$samples [ ,  i ]  
For example, to access the 50th sample, type 
res$samples [, 501 
Experimental Design 
We use the function xdes ign to perform a small-scale Monte Carlo study comparing 
completely randomized design and randomized block design in their effectiveness for 
assigning experimental units into treatment groups. Suppose we want to cany out 
our study with four treatment groups, each of size 20, and with a correlation of 0.8 
between the response and the blocking variable. v p e  the following commands into 
the command line editor: 
xdesign ( 1  
Suppose we want to carry out our study with five treatment groups, each of size 
25, and with a correlation of -0.6 between the response and the blocking variable. 
We also want to store the results of the simulation in a variable called res. Type the 
following commands into the command line: 
resc-xdesign(corr=-0.6,size=25,n.treatments=5) 
res is a list containing three member vectors of length 2 x n . treatments x n . rep. 
Each block of n . rep elements contains the simulated means for each Monte Carlo 

USING THE INCLUDED R FUNCTIONS 
391 
replicate with in a specific treatment group. The first n . treatments blocks corre- 
spond to the completely randomizeddesign, and the second n . treatments blocks 
correspond to randomized block design 
0 block. means: a vector of the means of the blocking variable 
0 treat. means: a vector of the means of the response variable 
0 ind: a vector indicating which means belong to which treatment group 
An example of using these results might be 
boxplot(block.means"ind,data=res) 
boxplot(treat.means"ind,data=res) 
CHAPTER 6: BAYESIAN INFERENCE FOR DISCRETE RANDOM 
VARIABLES 
Binomial Proportion with Discrete Prior 
The function binodp is used to find the posterior when we have a binomial (n, T )  
observation, and we have a discrete prior for T .  For example, suppose 7~ has the 
discrete distribution with three possible values, .3, .4, and .5. Suppose the prior 
distribution is as given in Table D. 1 
Table D. 1 An example discrete prior for a binomial proportion x 
.4 
.3 
.5 
.5 
and we want to find the posterior distribution after n = 6 trials and observing y = 5 
successes. Type the following commands into the command line editor: 
pi<-c ( 0 . 3 , O  .4,0.5) 
pi .prior<-c 
( 0 . 2 , O .  3 , O .  5) 
results<-binodp(5,6,uniform=FALSE,pi=pi, 
pi.prior=pi.prior,ret=TRUE) 
Poisson Parameter with Discrete Prior 
poisdp is used to find the posterior when we have a Poisson(p) observation, and a 
discrete prior for p. For example, suppose p has three possible values p = 1,2, or 3 

392 
USING THE INCLUDED R FUNCTIONS 
where the prior probabilities are given in Table D, and we want to find the posterior 
distribution after observing y = 4. 
Table 0.2 Discrete prior distribution for Poisson parameter p 
2 
.4 
3 
.3 
Type the following commands into the command line editor: 
mu<-1:3 
mu.prior<-c(0.3,0.4,0.3) 
poisdp(4, mu, mu.prior) 
CHAPTER 8: BAYESIAN INFERENCE FOR BINOMIAL PROPORTION 
Betu(a, b) Prior for z 
binobp is used to find the posterior when we have a binomial(n, T )  observation, 
and we have a beta(a, b) prior for T .  The beta family of priors is conjugate for 
binornial(n, T )  observations, so the posterior will be another member of the family, 
beta(a’, b’) where a’ = a + y and b‘ = b + n - y. For example, suppose we have 
n = 12 trials, and observe y = 4 successes, and use a beta(3,3) prior for T .  Type 
the following command into the R console: 
binobp (4,12,3,3) 
This should give the following output: 
> binobp (4,12,3,3) 
Posterior Mean 
: 
0.3888889 
Posterior Variance 
: 
0.0125081 
Posterior Std. Deviation : 
0.1118397 
Prob. 
Quantile 
0.005 
0.1370832 
0.01 
0.1552348 
_ _ _ _ _ _  
_ _ _ _ _ _ _ _ _  

USlNG THE lNCLUDED R FUNCTlONS 
393 
0.025 
0.184437 
0.05 
0.2119082 
0.5 
0.3846872 
0.95 
0.5802946 
0.975 
0.6167163 
0.99 
0.6577095 
0.995 
0.6845936 
We can find the posterior mean and standard deviation from the output. We can 
determine an equal tail area credible interval for T by taking the appropriate quantiles 
that correspond to the desired tail area values of the interval. For example, for 95% 
credible interval we take the quantiles with probability 0.025 and 0.975, respectively. 
These are 0.184 and 0.617. 
We can test HO : T 5 T O  vs. H I  : T > TO by using the qbeta function in 
conjuction with the parameters of the posterior beta distribuion. For example, assume 
that TO = 0.1, and that y = 4 successes were observed in n = 12 trials. If we use a 
betu(3,3) prior, then the posterior distribution of T is betu(3 + 4 = 7,3 + 12 - 4 = 11). 
Therefore we can test HO : T 5 TO = 0.1 vs. HI : T > T O  = 0.1 by typing 
qbeta(O.l,7,11). 
General Continuous Prior for 7r 
binogcp is used to find the posterior when we have a binomial (n, T )  observation, 
and we have a general continuous prior for T .  Note that T must go from 0 to 1 
in equal steps of at least 0.01, and g(T) must be defined at each of the T values. 
For example, suppose we have n = 12 trials and observe y = 4 successes. In 
this example our continuous prior for T is a normul(p = 0.5,o = 0.25). Type the 
following commands into the R console: 
bir1ogcp(4,12,density=~~norrnal~~,params=c 
(0.5,O .25) 
) 
This example is perhaps not quite general as it uses some of the built in functionality 
of binogcp. In this second example we use a “user-defined” general continuous 
prior. Let the probability density function be a triangular distribution defined by 
for 0 5 7r 5 0.5, 
= { ?- 4~ 
for 0.5 < 7r 5 1. 
Type the following commands into the R console: 
pi< -seq ( 0,1, 
by=O . 0 01) 
pi.prior<-rep(O,length(pi) 
) 
pi .prior [pi<=O. 
51 <-4*pi [pi<=O. 
51 
pi.prior[pi>0.5]<-4-4*pi 
[pi>0.51 
results<-binogcp 
(4,12, lluserl 
,pi=pi, 
pi.prior=pi.prior,ret=TRUE) 

394 
USlNG THE lNCLUDED R FUNCTlONS 
The output of binogcp does not print out the posterior mean and standard 
deviation. Nor does it print out the values that give the tail areas of the integrated 
density function that we need to determine credible interval for T .  Instead, we use 
the function sintegral, which numerically integrates a function over its range to 
determine these things. We can find the integral of the posterior density g ( r l y )  using 
this macro. Type the following commands into the R console: 
cdfc-sintegral(pi,results$posterior, 
plot (cdf, 
type=llln,xlab=expression(pi 
[ O ]  ) 
n.pts=length(pi) , ret=TRUE) 
,ylab=expression(Pr (pic=pi 
101 ) ) 
These commands created a new variable cdf, which is a list containing vectors x 
and y, where the ith element of cdf $y is equal to Pr (Y 5 x), where x is the ith 
element of cdf $x. To find a 95% credible interval (with equal tail areas) we find 
the values of cdf $x that correspond to .025 and ,975 in cdf $y respectively. 
lbc-cdf$x[with(cdf ,which.max(x[y<=0.0251 
) ) I  
ubc-cdf$x[with(cdf,which.max(x[yc=0.9751)) 1 
cat (paste (llApproximate 
95% credible interval : [I' 
, round ( 1 b ,4 ) , 
IT , round ( ub , 4  ) , 3 \ n , s e p = 
) ) 
To test the hypothesis HO : r I 
TO vs. H I  : T > TO, we find the value in cdf that 
corresponds to the value TO in pi. If the exact value of TO cannot be found in pi 
we can use the function approx to do linear interpolation. If the value is less than 
the desired level of significance a, then we can reject the null hypothesis. E.g. if 
Q = 0.05 in our previous example, and TO = 0.1, then we would type: 
cdfc-sintegral(pi,results$posterior, 
approx (cdf 
$x, cdf $y, 0.1) 
n.pts=length(pi) , ret=TRUE) 
This should give the following output: 
$X 
[l] 0.1 
SY 
[ll 0.001594175 
Given that 0.00159 is substantially less than our significance value of 0.05, then 
We can also find the posterior mean and variance by numerically evaluating 
we would reject Ho. 

USING THE INCLUDED R FUNCTlONS 
395 
1 
and 
(s’)2 
= 1 (7r - rn’)2g(.irly)dr 
using the function sintegral. Type the following commands into the R console: 
dense-pi*results$posterior 
post .mean<-sintegral (pi,dens) 
dens<-(pi-post.mean)^2*results$posterior 
post .var<-sintegral 
(pi,dens) 
post. sdc-sqrt (post .var) 
Of course we can use these values to calculate an approximate 95% credible 
interval using standard theory: 
lbc-post.mean-qnorm(O.975)*post.sd 
ub<-post.mean+qnorm(O.975)*post.sd 
cat (paste (Approximate 95% credible interval : [I1 
, round ( lb , 4  ) , 
, round (ub 
, 4  ) , I \n , sep= II ) ) 
CHAPTER 10: BAYESIAN INFERENCE FOR POISSON PARAMETER 
Garnrna(r, v) Prior for p 
The function poi sgamp is used to find the posterior when we have a random sample 
from a Poisson(p) distribution, and we have a gamma(r, w) prior for p. The gamma 
family of priors is the conjugate family for Poisson observations, so the posterior will 
be another member of the family, gamma(r’, w’) where r’ = r + 
y and w’ = v + n. 
The simple rules are “add sum of observations to r“ and “add number of observations 
to w“. For example, suppose we have a sample five observations from a Poisson(p) 
distribution, 3,4,3,0, 1. Suppose we want to use a gamma(6,3) prior for p. Type 
the following commands into the R console: 
poisgamp ( y ,  6,3 1 
By default poi sgamp returns a 99% Bayesian credible interval for p. If we want 
a credible interval of different width, then we can use the R functions relating to the 
posterior gamma distribution function. For example, if we wanted a 95% credible 
interval using the data above, we would type: 
y<-c (3,4,3,0,1) 

396 
USA" THE lNCLUDED R FUNCT/ONS 
Table 0.3 
interpolating between these values. 
Continuous prior distribution for Poisson parameter p has shape given by 
2
2
 
4
2
 
8
0
 
y<-c (3,4,3,0,1) 
res<-poisgamp(y,6,3,ret=TRUE) 
c .  i. <-qgamma (c (0.025,O. 
975) , res$r, res$v) 
We can test HO : p 5 
vs. H I  : p > po using the pgamma function. For 
example, if in the example above we hypothesize ,LLO = 3 and a = 0.05 then we type: 
pgamma (3, res$r, res$v) 
General continuous prior for Poisson parameter p 
The function poisgcp is used to find the posterior when we have a random sample 
from a Poisson(p) distribution and we have a continuous prior for p. Suppose we 
have a sample five observations from a Poisson(p) distribution, 3, 4, 3, 0, 1. The 
prior density of p is found by linearly interpolating the values in Table D.3. To find 
the posterior density for p with this prior, type the following commands into the R 
console: 
yc-c (3,4,3,0,1) 
mu<-seq(0,8,by=0.001) 
mu.prior<-c (seq(O,2, 
by=O. 0 0 1 )  ,rep (2,1999) 
,seq(2,0,by=-0.0005) 
1/10 
poisgcp (y, lluser’l, 
mu=mu,mu.prior=mu.prior) 
The output of poisgcp does not include the posterior mean and standard devia- 
tion by default. Nor does it print out the cumulative distribution function that allows 
us to find credible intervals. Instead we use the function sintegral which numerically 
integrates the posterior to do these things. Type the following commands into the R 
console to obtain the posterior cummulative distribution function: 
re sul t s < -poi sgcp ( y , user (I , mu=mu , mu. pr ior=mu . prior , re t =T 
) 
cdfc-sintegral(results$mu,results$posterior,ret=T) 
We can use the function approxfun to make the cummulative distribution 
function and inverse cummulative distribution function for the posterior. Type the 
following into the R console: 
F.muc-approxfun(cdf$x,cdf$y) 

USING THE INCLUDED R FUNCTIONS 
397 
Finv.mu<-approxfun(cdf$y,cdf$x) 
We can use the inverse cummulative distribution function to find 95% Bayesian 
credible interval for p. This is done by finding the values of p that correspond to the 
probabilities .025 and .975. Type the following into the R console: 
Finv.mu(c (0.025,O. 
975) ) 
We can use the cummulative distribution function to test the null hypothesis 
HO : p 5 po vs. H I  : p > po. For example if we hypothesis po = 1.8 and our 
significance level is a = 0.05, then 
Fx (1.8) 
returns 0.165 1 134. Given that this is greater than the desired level of significance we 
fail to reject the null hypothesis at that level. 
CHAPTER 11 : BAYESIAN INFERENCE FOR NORMAL MEAN 
Discrete Prior for p 
The function normdp is used to find the posterior when we have a vector of nor- 
mal(p, a’) observations and o2 is known, and we have a discrete prior for p. If 
sigma2 is not known then is it is estimated from the observations. For example, 
suppose p has the discrete distribution with five possible values: 2,2.5,3, 3.5, and 4. 
Suppose the prior distribution is given in Table D.4 and we want to find the posterior 
distribution after we’ve observed a random sample of n = 5 observations from a 
normal (p,o’ = 1) that are 1.52, 0.02, 3.35, 3.49, and 1.82. Type the following 
commands into the R console: 
mu<- seq (2,4, 
by=O .5) 
mu.prior<-c(0.l,0.2,0.4,0.2,0.1) 
normdp ( y  , 1 , mu, mu. prior) 
y<-C (1.52,O. 
02,3.35,3.49,1.82) 
Table 0.4 A discrete prior for the normal mean p 
2.5 
.2 
3 
.4 
3.5 
.2 
4 
.1 

398 
USING 
INCLUDED R FuNcrioNs 
NormaZ(rn, 
s”) Prior for p 
The function normnp is used when we have a vector containing a random sample 
of n observations from a normal(p, u2) distribution (with uz known) and we use 
a normal(m, s2) prior distribution. If the observation standard deviation u is not 
entered, the estimate calculated from the observations is used, and the approximation 
to the posterior is found. If the normal prior is not entered, a flat prior is used. The 
normal family of priors is conjugate for nomal(p, a2) observations, so the posterior 
will be another member of the family, normal[m‘, (s’)~] where the new constants are 
given by 
1 
1
n
 
- +- 
( S ’ y  
52 
u 2  
and 
1 
n 
B 
-2 
02 
02 
m’= 
x m+ +- x g .  
For example, suppose we have a normal random sample of four observations from 
normal(p, u2 = 1) that are 2.99, 5.56, 2.83, and 3.47. Suppose we use a normal 
(3, 22) prior for p. Type the following commands into the R console: 
normnp ( y ,  3 , 2 , 1 )  
This gives the following output: 
y < - C  ( 2 . 9 9 , 5 . 5 6 , 2 . 8 3 , 3 . 4 7 )  
Posterior mean 
: 3 . 6 7 0 5 8 8 2  
Posterior std. deviation : 0.4850713 
Prob. 
0 . 0 0 5  
0 . 0 1  
0 . 0 2 5  
0 . 0 5  
0 . 5  
0 . 9 5  
0 . 9 7 5  
0 . 9 9  
0 . 9 9 5  
_ _ _ _ _ _  
Quant i 1 e 
2 . 4 2 1 1 2 7 5  
2 . 5 4 2 1 4 3 8  
2 . 7 1 9 8 6 6 1  
2 . 8 7 2 7 1 7  
3 . 6 7 0 5 8 8 2  
4 . 4 6 8 4 5 9 4  
4 . 6 2 1 3 1 0 4  
4 . 7 9 9 0 3 2 7  
4 . 9 2 0 0 4 9  
- - - - - - - - - 
We can find the posterior mean and standard deviation from the output. We can 
determine an (equal tail area) credible interval for p by taking the appropriate quantiles 
that correspond to the desired tail area values of the interval. For example, for 99% 
credible interval we take the quantiles with probability 0.005 and 0.995, respectively. 
These are 2.42 and and 4.92. Alternatively we can determine a Bayesian credible 
interval for p by using the posterior mean and standard deviation in the normal inverse 

USING THE INCLUDED R FUNCTIONS 
399 
cumulative distribution function F o r m .  Type the following commands into the R 
console: 
y<-c (2.99,5.56,2.83,3.47) 
res<-normp(y,3,2,1,ret=T) 
c. i. <-qnorm(c (0.005,O .995) , res$mean, res$sd) 
We can test Ho : p 5 po vs. H I  : p > po by using the posterior mean 
and standard deviation in the normal cumulative distribution function pnorm. For 
example, if Ha : po = 2 and our desired level of significance is a = 0.05, then 
pnorm (2, 
res$mean, res$sd) 
returns 0.000287 which would lead us to reject Ha. 
General Continuous Prior for p 
The function normgcp is used when we have a vector containing a random sample 
of n observations from a normal (p, u2) distribution (with u2 known) and we have a 
vector containing values of p, and a vector containing values from a continuous prior 
g(p). If the standard deviation u is not entered, the estimate calculated from the data 
is used, and the approximation to the posterior is found. 
For example, suppose we have a random sample of four observations from a 
normal (p, u2 = 1) distribution. The values are 2.99,5.56,2.83, and 3.47. Suppose 
we have a triangular prior defined over the range -3 to 3 by 
Type the following commands into the R console: 
~<-~(2.99,5.56,2.83,3.47) 
mu<-seq(-3,3,by=0.1) 
mu.prior<-rep(O,length(mu) 
) 
mu.prior   mu<=^] <-1/3+mu[mu<=ol/9 
mu.prior [mu>O] 
<-1/3-mu[mu>O] 
/9 
results< -normgcp (y ,I, 
density= "user 
I), mu=mu, 
mu.prior=mu.prior,ret=T) 
The output of normgcp does not print out the posterior mean and standard 
deviation. Nor does it print out the values that give the tail areas of the integrated 
density function that we need to determine credible interval for p. Instead we use 
the macro sintegral which numerically integrates a function over its range to 
determine these things. We can find the integral of the posterior density g(p1data) 
using this macro. Type the following commands into the R console: 
cdf<-sintegral(mu,results$posterior 

400 
USING THE INCLUDED R FUNCTlONS 
,n.pts=length(mu) 
,ret=TRUE) 
,ylab=expression 
(Pr (mu<=mu[Ol 
) 1 )  
plot (cdf, 
type=T1lll, 
xlab=expression (mu [O] 
) 
These commands created a new variable cdf, which is a list containing vectors x 
and y, where cdf $y is equal to Pr (Y 5 z), i.e. the cumulative density function 
(cdf.) To find a 95% credible interval (with equal tail areas), we find the values of 
cdf $x that correspond to .025 and .975 in cdf $y, 
respectively. 
lb<-cdf$x[with(cdf,which.max(x[y<=O.O25] 
1 )  I 
ub<-cdf$x [with (cdf ,which.max (x [yc=O. 
9751 ) ) 1 
cat (paste 
( IIApproximate 95% credible interval : [I' 
, round ( 1 b , 4  ) , If , round ( ub , 4  ) , 1 \n , sep= I' ) ) 
Alternatively we can use the R function approxf un to create cumulative and 
inverse cumulative distribution functions using the variable cdf . Type the following 
into the R console: 
F.mu<-approxfun(cdf$x, 
cdf$y) 
Finv.mu<-approxfun(cdf$y,cdf$x) 
We can use Finv .mu to find a 95% credible interval for p: 
c . i . < -Finv .mu (c (0.025,O. 
975) ) 
To test a hypothesis HO : p 5 po vs. H I  : p > po we can use our cdf F . mu at po. 
If this is less than the chosen level of significance, we can reject the null hypothesis 
at that level. 
We can also find the posterior mean and variance by numerically evaluating 
= J Pg(Pldata) dP 
and 
(s')2 
= 
J(p - m')2g(pldata) dp 
using the function sintegral. Type the following commands into the R console: 
dens<-mu*results$posterior 
post.mean<-sintegral(mu,dens) 
dens<- (mu-post.mean)A2*results$posterior 
post .var<-sintegral 
(mu, 
dens) 

USING THE INCLUDED 13 FUNCTIONS 
401 
post. sd<-sqrt (post .var) 
Of course, we can use these values to calculate an approximate 95% credible interval 
using standard theory: 
lb<-post.mean-qnorm(O.975)*post.sd 
ub<-post.mean+qnorm(O.975)*post.sd 
cat (paste ("Approximate 9 5 %  credible interval : ['I 
,round (lb, 
4 )  , I( (I, round (ub, 
4 )  , " 1  \rill, sep="") 
) 
CHAPTER 14: BAYESIAN INFERENCE FOR SIMPLE LINEAR REGRES- 
SION 
The function bayes . 1 in. reg is used to find the posterior distribution of the simple 
linear regression slope p when we have a random sample of ordered pairs (xzr 
y,) 
from the simple linear regression model 
Yz = a0 + P x 2% + ez, 
where the observation errors e, are independent normal(0, cr2) with known variance. 
(If the variance is not known the posterior is found using the variance estimate 
calculated from the least squares residuals.) We use independent priors for the slope 
p and the intercept oE. These can be either flat priors, or normal priors. (The default 
is flat priors for both slope and intercept of x = 3.) This parameterization yields 
independent posterior distribution for slope and intercept with simple updating rules 
"posterior precision equals prior precision plus precision of least squares estimate" 
and "posterior mean is weighted sum of prior mean and the least squares estimate 
where the weights are the proportions of the precisions to the posterior precision. 
Suppose we have vectors y and x, respectively, and we know the standard deviation 
o = 2. We wish to use a normal(0,32) prior for P and a normal(30,102) 
prior for 
a*. 
First we create some data for this example. 
x e  -rnorm ( 100) 
ye - 3 *x+2 
2 +rnorm ( 10 0 , 0 , 2  ) 
Now we can use bayes . lin . reg 
bayes.lin.reg(y,x, llnll, "nI1 , 0,3,30,10,2) 
If we want to find a credible interval for the slope use Equation 14.9 or Equation 
14.10 depending on whether we knew the standard deviation or used the value 
calculated from the residuals. In the example above, we know the standard deviation, 
therefore we would type the following into R to find a 95% credible interval for the 
slope: 

402 
USING THE /NCLUD€D R FUNCn0N.S 
resc-bayes.lin.reg(y,x, 
Iln", Itnr1 
,0,3,30,10,2,ret=T) 
c. i. c-qnorm(c (0.025,O. 
975), 
res$post.coef [21, 
res$post . coef . sd [21 ) 
To find the credible interval for the predictions use Equation 14.13 when we 
know the variance or Equation 14.14 when we use the estimate calculated from the 
residuals. In the example above we can ask for predicted values for 2 = 1 , 2 , 3  by 
typing : 
resc-bayes.lin.reg(y,x, 
ltnl1, 
I1nt1 
,0,3,30,10,2, 
pred. x=c ( 1,2,3 
) , ret=T) 
The list res will contain three extra vectors pred. x, pred. y and pred . se. We 
can use these to get a 95% credible interval on each of the predicted values. To do 
this type the following into the R console: 
zc-qnorm(0.975) 
1ower.boundc-res$pred.y-z*res$pred.se 
upper.bound<-res$pred.y+z*res$pred.se 
CHAPTER 15: BAYESIAN INFERENCE FOR NORMAL STANDARD DEVI- 
ATION 
S x an Inverse chi-squared( K )  Prior for u2 
The function nvaricp is used when we have a vector containing a random sample 
of n observations from a normal(p, u 2 )  distribution where the mean p is known. The 
Sx an inverse chi-squared(r;) family of priors is the conjugate family for normal 
observations with known mean. The posterior will be another member of the family 
where the constants are given by the simple updating rules "add the sum of squares 
around the mean to S" and "add the sample size to the degrees of freedom". For 
example, suppose we have five observations from a normal(p, 0 2 )  where p = 200 
which are 206.4, 197.4,212.7,208.5, and 203.4. We want to use a prior that has prior 
median equal to 8. It turns out that 29.11 x inverse chi-squared(r; = 1) distribution 
has prior median equal to 8. Type the following into the R console: 
~<-~(206.4,197.4,212.7,208.5,203.4) 
res<-nvaricp(y,200,29,11,l,ret=T) 
Note: the graphs that are printed out are the prior distributions of the standard 
deviation u even though we are doing the calculations on the variance. 
If we want to make inferences on the standard deviation u using the posterior 
distribution we found, such as finding an equal tail area 95% Bayesian credible 
interval for o type the following commands into the R console: 
cdf<-sintegral(res$sigma,res$posterior,ret=T) 

USlNG THE /NCLUDED R FUNCT/ONS 
403 
Finv.sigmac-approxfun(cdf$y,cdf$x) 
c. i. <-Finv. 
sigma (c (0.025,O. 
975) 
) 
We can also estimate g using the posterior mean of n and S if IE > 2 or posterior 
median. 
post.mean.estc-sqrt(res$Sl/(res$kappal-2)) 
post.median.estc-qchisq(O.5,res$kappal) 
CHAPTER 16: ROBUST BAYESIAN METHODS 
The function binomixp is used to find the posterior when we have a binomial(n, T )  
observations and use a mixture of a beta(ao,bo) and a beta(u1,bl) for the prior 
distribution for T .  Generally, the first component summarizes our prior belief so 
that we give it a high prior probability. The second component has more spread to 
allow for our prior belief being mistaken so we give the the second component a 
low prior probability. For example, suppose our first component is beta(l0,6), and 
the second component is beta(1,l) and we give a prior probability of .95 to the first 
component. We have taken 60 trials and observed y = 15 successes. To find the 
posterior distribution of the binomial proportion with a mixture prior for T type the 
following commands into the R console: 
binomixp ( 15,60, 
c ( 10,6 
) , p= 0.9 5 ) 
The function normmixp is used to find the posterior when we have normal(p, 0 2 )  
observations with known variance oz and our prior for /I is a mixture of two normal 
distributions, a normal(m0, sg) and a normal(ml, s:). 
Generally the first component 
summarizes our prior belief so we give it a high prior probability. The second 
component is a fall-back prior that has a much larger standard deviation to allow 
for our prior belief being wrong and has a much smaller prior probability. For 
example, suppose we have a random sample of observations from a normal(p, 0 2 )  
in a vector x where g 2  = .22. Suppose we use a mixture of a normal(l0, .12) and 
a normal(l0, .42) prior where the prior probability of the first component is .95. To 
find the posterior distribution of normal mean with the mixture prior for p, type the 
following commands into the R console: 
XC-c (9.88,9.78,10.05,10.29,9.77) 
normmixp (x, 
0.2, 
c (10,O. 01) , c (lO,le-4), 
0.95) 

This Page Intentionally Left Blank

E 
0 
0 
1 
1 
2 
2 
3 
4 
Answers to 
leaf unit 
1 
3 3  
5 7 9 9  
1334 
6789 
33 
56789 
3
5
 
4
6
 
34 
Selected Exercises 
405 

406 
ANSWERS TO SELECTED EXERClSES 
(c) Boxplot of SO2 data 
0 
10 
20 
30 
40 
50 
502 
0 
10 
20 
30 
40 
50 
502 
3.3 (a) Stem-and-leaf plot for distance measurements data 
299.4 
299.5 
299.6 
299.7 
299.8 
299.9 
300.0 
300.1 
300.2 
300.3 
300.4 
300.5 
300.6 
300.7 
leaf unit 
.01 
0 
0 
0 
00 
000 
000000 
ooo0Ooo 
ooOooo0O 
0000000 
00 
00000 
OOO 
00 
00 
(b) Median = 300.1 QI = 299.9 
Q3 = 300.35 
(c) Boxplot of distance measurement data 
255 5 
3W 0 
300 5 

ANSWERS TO SELECTED EXfRClSfS 
407 
30 
20 
10 - 
0 -  
(d) Histogram of distance measurement data 
ll:r- 
I
,
!
 
I 
I 
I 
1
,
 
2982 
2996 2888 3 W O  3W2 3 w 4  
3008 
(e) Cumulative frequency polygon of distance measurement data 
50 
30 
20 
10 - 
8 
,
I
,
,
/
 
299 2 
299.B?99.BMO.mw.m 4 
3w 8 
3.5 (a) Histogram of liquid cash reserve 
&- 
~~~ o o w 4  
v) 
5 00003 
0 0002 
0 woi 
0 ww 
oMmw m o o  
40w 
ww 
101 0 
(c) Grouped mean = 1600 

408 
ANSWERS TO SELECTED EXERCISES 
3.7 
(a) Plot of weight versus length (slug data) 
14 
2 
.. . . 
.. '. 
,:* .c .: 
0 
-.-. 
,
,
,
,
I
 
,
,
,
,
,
I
 
0 
10 
20 
30 40 
50 
Bo 70 
80 
90 100 
length 
(b) Plot of log(weight) versus log(1ength) 
I 
1
1
 
. ,.. 
1 0  1 1  1 2  1 3  1 4  1 5  1 6  1 7  1 8  1 9  2 0  
log len 
(c) The point (1.5, -1.5) does not seem to fit the pattern. This corresponds 
to observation 90. Dr. Harold Henderson at AgResearch New Zealand 
has told me that there are two possible explanations for this point. Either 
the digits of length were transposed at recording or the decimal place for 
weight was misplaced. 
Chapter 4: Logic, Probability, and Uncertainty 
4.1 
(a) P(A) = .6 
(b) P ( A  n B )  = .2 
(c) P ( A  U B )  = .7 
(a) P ( A n B )  = .24, P(B) = .4, thereforeP(AnB) = .16. P ( A n B )  = 
4.3 
P(A) x P(B), therefore they are independent. 
(b) P ( A  U B )  = .4 + .4 - .16 = .64 
4.5 (a) R = {1,2,3,4,5,6} 
(b) A = {2,4,6}, P(A) = 
(c) B = {3,6}, P(B) = $ 
(d) A n  B = {6}, P ( A n B )  = 6 
(e) P(A n B )  = P(A) x P(B), therefore they are independent. 

ANSWERS TO SELECTED EXERCISES 
409 
4.7 
(a) 
A =  
P(A) = !$ 
(b) 
(L2) (175) (271) (274) (37 3) (376) 
(472) (475) (5,1) (574) (673) (676) 
B = {  
P(B) = g 
(c) A n B = {(1,5)(274)(3,3)(472)(5,1)(6,6)) 
P ( A n B )  = & 
(d) P(A n B) = P(A) x P(B), yes they are independent, 
4.9 Let D be “the person has the disease” and let T be “The test result was positive.” 
= .0875 
P(D n T )  
P(T) 
P(DIT) = 
4.1 1 Let A be ace drawn, and let F be face card or ten drawn. 
P(”Bluckjuck”) = P(A) x P(FIA) + P ( F )  x P(AIF) 
(they are disjoint ways of getting “Blackjack”) 
16 
64 
64 
208 
207 
208 
P(”Blackj~ck”) 
= - 
x - 
+ - 
x 16207 = 0.047566. 
Chapter 5: Discrete Random Variables 
5.1 (a) P(l < Y 5 3) = .4 
(b) E(Y) = 1.6 
(c) Var(Y) = 1.44 
(d) E(W) = 6.2 
(e) Var(W) = 5.76 

410 
ANSWERS TO SELECTED EXERCISES 
5.3 (a) The filled-in table: 
Yi 
f (Yd 
~i x f (Yi) 
Yt x f(Yi) 
0 
.0102 
.m 
.oooo 
1 
.0768 
.0768 
.0768 
2 
.2304 
,4608 
.9216 
3 
.3456 
1.0368 
3.1104 
4 
.2592 
1.0368 
4.1472 
5 
.0778 
.3890 
1.9450 
Sum 
1 .0000 
3.0000 
10.2000 
5.5 
i. E ( Y )  = 3 
ii. V a r ( Y )  = 10.2 - 3' = 1.2 
(b) Using formulas 
i. E ( Y )  = 5 x .6 = 3 
ii. V a r ( Y )  = 5 x .6 x .4 = 1.2 
(a) 
Outcome 
Probabilitv 
RRRR 
RRGR 
GRRR 
GRGR 
RRGG 
RGRG 
GGRG 
RGGG 
@ x @ x @ x @  
88 
88 
58 
88 
58 
88 
88 
88 
58 
38 
58 
88 
88 
88 
58 
58 
88 
58 
88 
58 
58 
58 
88 
58 
- x - x - x -  
- x - x - x -  
- x - x - x -  
- x - x - x -  
- x - x - x -  
- x - x - x -  
- 
~ ! x s x s x s  
50 
50 
50 
Outcome 
RRRG 
RGRR 
GRRG 
GGRR 
RGGR 
GGGR 
GRGG 
GGGG 
Probabilitv 
~
x
~
x
*
88 
58 
88 
88 
58 
88 
88 
58 
58 
58 
88 
88 
88 
58 
58 
88 
- x - x - x -  
58 
58 
58 
58 
58 
88 
58 
58 
- x - x - x -  
- x - x - x -  
- x - x - x -  
- x - x - x -  
- x - x - x -  
50 
50 
50 
- i! s s 8 
The outcomes having same number of green balls have the same proba- 
bility. 
Y=O 
Y = l  
Y = 2  
Y = 3  
Y = 4  
RRRR 
RRRG 
RRGG 
RGGG 
GGGG 
(b) 
RRGR 
RGRG 
GRGG 
RGRR 
RGGR 
GGRG 
GRRR 
GRRG 
GGGR 
GRGR 
GGRR 
(c) P(Y = y) equals the "number of sequences having Y = y" times "the 
(d) The number of sequences having Y = y is ( ) and the probability 
of any sequence having Y = y successes is xY(1 - x)"-Y where in 
probability of any individual sequence having Y = y." 

ANSWERS TO SELECTED EXERCISES 
41 1 
X
I
 
Y 
this case n = 4 and r = g. This gives the binomial(n, n) probability 
distribution. 
f (x) 
5.7 
(a) P(Y = 2) = 
= .2707 
p
-
2
 
p e - 2  
22e-2 - .1353 + .2707 + .2707 = .6767 
= .2707 + .2707 + .1804 = 
(b) P(Y I 
2) = 
(c) P(1 I y < 4) = 7 
+ 7 
+ 
+ 7 
+ 7 
- 
p e - 2  
2 Z e - 2  
.7218 
2 
3 
4 
f(Y) 
5.9 
The filled-in table: 
.08 
.02 
.10 
.02 
.03 
.25 
.05 
.05 
.03 
.02 
.10 
.25 
.10 
.04 
.05 
.03 
.03 
.25 
2 5  
.15 
.24 
.15 
.2 1 
(a) The marginal distribution of X is found by summing across rows. 
(b) The marginal distribution of Y is found by summing down columns. 
(c) No they are not. The entries in the joint probability table aren’t all equal 
to the products of the marginal probabilities. 
(d) P ( X  = 3/Y = 1) = $ = .2 
(b) The filled-in table: 

412 
ANSWERS TO SELECTED EXERCISES 
X 
prior 
1 
10 - 
0 
1 
- 
10 
1 
2 
- 
10 
1 
3 
- 
10 
1 
4 
- 
10 
1 
5 
- 
10 
1 
6 
- 
10 
1 
7 
- 
10 
1 
8 
- 
10 
1 
9 
- 
10 
1 
Y=O 
A X ;  
& X ;  
+)X; 
&
X
i
 
A X ;  
& X $  
& X $  
&
X
i
 
& X g  
(c) The marginal distribution was found by summing down the columns. 
(d) The reduced Bayesian universe is 

ANSWERS TO SELECTED EXERCISES 
413 
1 
10 - 
(e) The posterior probability distribution is found by dividing the joint prob- 
abilities on the reduced Bayesian universe, by the sum of the joint prob- 
abilities over the reduced Bayesian universe. 
(0 The Simplified table is 
8 
14 
18 
20 
20 
- 
8 
14 
18 
20 
20 
18 
14 
8 
0 
I ZIJ 
- 
; X i  
720 
120 
$ X i  
720 
120 
g X g  
720 
120 
$ X !  
720 
120 
g X ;  
720 
120 
$
X
i
 
720 
120 
8 x 2  
720 
120 
E X &  
720 
120 
; X i  
720 
120 
- 
- 
- 
- 
- 
- 
- 
- 
ia - 
- 
14 - 
- 
a - 
- 
0 - 
- 
- 
- 
x 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
- 
- 
likelihood 
g x 1  
prior 
prior x likelihood 
posterior 
0 
0 
90 
120 
- 
- 
T 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
- 
- 
- 
- 
- 
- 
- 
- 
I? 
T 
prior 
likelihood 
prior x likelihood 
posterior 
.2 
.0017 
.2048 
.oO04 
.0022 
.4 
.0924 
.3456 
.03 19 
.I965 
.6 
.4678 
.2304 
.lo78 
.6633 
.8 
.4381 
.05 12 
.0224 
.1380 
marginal P ( Y 2  = 2 )  
.1625 
1 .Ooo - 
I - 
10 
likelihood 
0 
9 
1 
9 
2 
9 3 
9 4 
9 
5 
9 6 
9 
E 
9 
8 
9 
9 
9 
prior x likelihood - 
90 
1 
90 
2 
90 
3 
90 
4 
90 
5 
% 
6 
90 
7 
90 
8 
90 
9 
sn 
- 
- 
- 
- 
- 
- 
- 
- 
posterior 
u 
45 
1 
45 
2 
45 
3 
45 
4 
45 
5 
45 
6 
45 
7 
45 
45 
9 
45 
- 
- 
- 
- 
- 
- 
- 
a - 
- 
- 
X 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
- 
- 
prior 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
10 
1 
- 
- 
- 
- 
- 
- 
- 
- 
I? 

414 
ANSWERS TO SELECTED EXERCISES 
p 
1 
2 
3 
4 
5 
prior 
likelihood 
.2 
.1839 
.2 
.2707 
.2 
.2240 
.2 
,1465 
.2 
.0842 
prior x likelihood 
.0368 
.054 1 
,2976 
.0448 
.2464 
.0293 
.1611 
.0168 
.0926 
posterior 
.2023 
Chapter 7: Continuous Random Variables 
7.1 
(a) E ( X )  = = ,375 
(b) V a r ( X )  = 
= 0.0260417 
7.3 The uniform distribution is also the beta (1,l) distribution. 
(a) E ( X )  = = .5 
(b) V a r ( X )  = 
= .08333 
(c) P ( X  5 .25) = &25 1 dx = .25 
(d) P(.33 < X < .75) = J:: 
1 dx = .42 
(a) P(0 5 Z < .65) = .2422 
(b) P(Z 2 .54) = .2946 
(c) P(-.35 5 2 5 1.34) = .5467 
(a) P(Y 5 130) = 3944 
7.5 
7.7 
(b) P(Y 2 135) = .0304 
(c) P(114 5 Y 5 127) = .5826 
(a) E ( Y )  = & = .4545 
(b) V a r ( Y )  = 
= .0107797 
(c) P ( Y  > .5) = .3308 
7.9 
Chapter 8: Bayesian Inference for Binomial Proportion 
8.1 
(a) binomial (n = 150, T )  distribution 
(b) beta (30,122) 
(a) a and b are the simultaneous solutions of 
a - 
= .5 
a + b  
8.3 

ANSWERS TO SELECTED EXERCISES 
415 
and 
a x b  
(a + b)2 x (a + b + 1) = .152 
Solution is a = 5.05 and b = 5.05 
(b) The equivalent sample size of her prior is 11.11 
(c) beta (26.05,52.05) 
(a) binomial (n = 116, T )  
(b) beta (18,103) 
8.5 
(c) 
and 
18 x 103 
(121)Z x (122) 
Var(r1y) = 
(d) normal(.l49, .0322’) 
(e) (.086,.212) 
8.7 
(a) binomial(n = 1 7 4 , ~ )  
(b) beta(ll,l68) 
(c) 
11 
E(TlY) = 11+168 = .0614 
and 
168 
= .0003204 
(179)2 x (180) 
Var(Tly) = 
(d) normal(.061, .0179’) 
(e) (.026,.097) 
Chapter 9: Comparing Bayesian and Frequentist Inferences for 
Proportion 
9.1 
(a) binornial(n = 30, T )  
(b) ?irf = $j = .267 
(c) beta(9,23) 
(d) ?B = & = ,281 
9.3 (a) +f = 
= .095 
(b) beta (12,115) 

416 
ANSWERS TO SELECTED EXERCISES 
(c) E(n-1~) 
= ,094 and Var(nly) = .0006684 
(d) (.044,.145) 
(e) The null value 7r = .10 lies in the credible interval, so it remains a credible 
(a) .irf = $ = .136 
(b) beta (25,162) 
(c) E(n-1~) = ,134 and Var(n-ly) = .0006160 
The Bayesian estimator ?jrg = . O W .  
value at the 5% level 
9.5 
The Bayesian estimator .ir~ = .134. 
( 4  
P(7r 2 .15) = .255. 
This is greater than level of significance .05, so we can’t reject the null 
hypothesis HO : 7r 2 .15. 
Chapter 10: Bayesian Inference for Poisson 
10.1 (a) Using positive uniform prior g ( p )  = 1 for p > 0 
i. The posterior is garnrna(l3,5) 
ii. The posterior mean, median , and variance are 
13 
13 
52 
E(ply1,. . . ,y5) = 
(b) Using Jeffreys prior g(p) = p-; 
, median = 2.534, var(p./yl,. . . ,y5) = - . 
i. The posterior is gurnrna(12.5,5) 
ii. The posterior mean, median , and variance are 
12.5 
12.5 
war(p/yl,. . . ,y5) = 52 
E(pIy1,. . . ,y5) = __ , median = 2.434, 
5 
10.3 (a) Using positive uniform prior g(p) = 1 for p > 0 
i. The posterior is gurnma(123,200) 
ii. The posterior mean, median , and variance are 
123 
123 
E(plyi,. . . , YZOO) = 200 , median = .6133 , var(p/y~, 
. . . , YZOO) = 2002 
(b) Using Jeffreys prior g(p) = p-; 
i. The posterior is garnrna(122.5,200) 
ii. The posterior mean, median , and variance are 
122.5 
122 
median = .6108, war(plyl,. . . ,yzoo) = - 
2002 
200 ’ 
E(PIYl,... i YZOO) = - 

ANSWERS TO SELECTED EXERCISES 
417 
Chapter 11 : Bayesian Inference for Normal Mean 
11.1 (a) posterior distribution 
value 
posterior probability 
99 1 
.m 
992 
.m 
993 
.m 
994 
.m 
995 
.m 
996 
.002 1 
997 
.lo48 
998 
,5548 
999 
.3 183 
lo00 
.0198 
1001 
.o001 
1002 
.m 
1003 
.m 
1004 
.m 
1005 
.m 
1006 
.m 
1007 
.m 
1008 
.m 
1009 
.m 
1010 
.m 
(b) P(p < 1000) = .9801. 
1 1.3 (a) The posterior precision equals 
1 
10 
- 
= - 
+ - = 1.1211 
1 
( d ) 2  
102 
32 
The posterior variance equals ( s ’ ) ~  = & = 39197. The posterior 
standard deviation equals s’ = d
m
 
= .9444. The posterior mean 
equals 
1 
10 
m’ = - 
i@ 
x 3 0 + -  
x 36.93 = 36.87. 
1.1211 
1.1211 
The posterior distribution of p is normal(36.87, .9444’). 
(b) Test 
HO : p 5 35 versus H I  : p > 35 
Note that the alternative hypothesis is what we are trying to determine. 
The null hypothesis is that mean yield is unchanged from that of the 
standard process. 
(C) 
p - 36.87 
35 - 36.87) 
( .944 
,944 
P ( p  5 .35) = P 

418 
ANSWERS TO SELECTED EXERClSES 
= P(Z 5 -2.012) = .022. 
This is less than the level of significance cy = .05%, so we reject the null 
hypothesis and conclude the yield of the revised process is greater than 
.35. 
1 1.5 (a) The posterior precision equals 
4 
+ - 
= .002525. 
1 
( s  ) ~  2002 
402 
- 
-~ 
- 
1 
The posterior variance equals ( s ’ ) ~  = & = 396.0 The posterior 
standard deviation equals s’ = 
= 19.9. The posterior mean 
equals 
1 
4 
m ‘ = ~ x 1 O O O + - - - - -  ZP x 970 = 970.3 
.002525 
.002525 
The posterior distribution of p is normal(970.3, .19.g2). 
(b) The 95% credible interval for p is is (931.3, 1009.3). 
(c) The posterior distribution of 0 is nonal(1392.8, 16.62). 
(d) The 95% credible interval for 0 is (1360,1425). 
Chapter 12: Comparing Bayesian and Frequentist Inferences for Mean 
12.1 (a) Posterior precision is given by 
1 
10 
- 
= - 
+ - = 2.51 
1 
( S y  
102 
22 
The posterior variance ( s ’ ) ~  = 
deviation s’ = a 
= .63119. The posterior mean is given by 
= .3984 and the posterior standard 
1 
10 
i P  
2.51 
2.51 
m’ = - 
x 75 + 
x 79.430 = 79.4124. 
The posterior distribution is normal(79.4124, .6311g2). 
(b) The 95% Bayesian credible interval is (78.18,80.65). 
(c) To test 
HO : p  2 80 versus p < 80 
calculate the posterior probability of the null hypothesis. 
p - 79.4124 
80 - 79.4124) 
( .63119 ’ .63119 
P ( p  2 80) = P 
= P ( Z  2 .931) = .176. 

ANSWERS TO SELECTED EXERCISES 
419 
This is greater than the level of significance, so we cannot reject the null 
hypothesis. 
12.3 (a) Posterior precision is expressed as 
1 
1 
25 
(s')' 
802 802 
- 
+ - 
= .0040625. 
The posterior variance (5')' 
= - 
= 246.154 and the posterior 
standard deviation s' = V'= 
= 15.69. The posterior mean 
1 
25 
x 325 + 
~ 
KF 
x 401.96 = 399. 
.0040625 
,0040625 
m ' L  
KF 
The posterior distribution is normal(399,15.69'). 
(b) The 95% Bayesian credible interval is (368,429). 
(c) We observe that the null value (350) lies outside the credible interval, so 
we reject the null hypothesis Ho : p = 350 at the 5% level of significance. 
We can conclude that p # 350. 
(d) We calculate the posterior probability of the null hypothesis. 
P ( p  5 350) = .0009. 
This is less than the level of significance, so we reject the null hypothesis 
and conclude that p > 350. 
Chapter 13: Bayesian Inference for Difference Between Means 
13.1 (a) The posterior distribution of p~ is n0rmal(ll9.4,1.888~) , the posterior 
(b) The posterior distribution of pd = p~ - pg is normal(-3.271, 2.6712). 
(c) The 95% credible interval for p~ - pg is (-8.506,1.965). 
(d) We note that the null value 0 lies inside the credible interval. Hence we 
distribution of pg is norma1(122.7,1.888'), and they are independent. 
cannot reject the null hypothesis. 
13.3 (a) The posterior distribution of p1 is normal(14.96, .3778'), the posterior 
distribution of 1 2  is normal(15.55, .3778'), and they are independent. 
(b) The posterior distribution Of pd = p1 - p1 is normal(-.5847, .5343'). 
(c) The 95% credible interval for 1 1  - p1 is (-1.632, .462). 
(d) We note that the null value 0 lies inside the credible interval. Hence we 
cannot reject the null hypothesis. 
(a) The posterior distribution of p1 is normal(10.283, .816'), the posterior 
distribution of 1-12 is normal(9.186, .756'), and they are independent. 
13.5 

420 
ANSWERS TO SELECTED EXERCISES 
(b) The posterior distribution of pd = p1 - p2 is nonnal(1.097, 1.1132). 
(c) The 95% credible interval for p1 - p2 is (-1.08,3.28). 
(d) We calculate the posterior probability of the null hypothesis 
P(p1 - p2 5 0) = .162. 
This is greater than the level of significance, so we cannot reject the null 
hypothesis. 
13.7 (a) The posterior distribution of p1 is nonnal(1.51999, .0000094442). 
(b) The posterior distribution of p2 is normal( 1.52001, .0000094442). 
(c) The posterior distribution ofpd = p 1 - p ~  isnoma/(- .00002, .0000132). 
(d) A 95% credible interval for pd is (-.000046, .000006). 
(e) We observe that the null value 0 lies inside the credible interval so we 
cannot reject the null hypothesis. 
13.9 (a) The posterior distribution of 7r1 is beta(172,144). 
(b) The posterior distribution of x2 is beta(138,83). 
(c) The approximate posterior distribution of 7r1-7r2 is normal( -.080, .0429’). 
(d) The 99% Bayesian credible interval for 7r1 - 7r2 is (-.190, .031). 
(e) We observe that the null value 0 lies inside the credible interval, so we 
cannot reject the null hypothesis that the proportions of New Zealand 
women who are in paid employment are equal for the two age groups. 
13.1 1 
(a) The posterior distribution of 7r1 is beta(70,246). 
(b) The posterior distribution of 7r2 is beta(ll5,106). 
(c) The approximate posterior distribution of 7r1-7r2 isnonnal( -.299, .04082). 
(d) We calculate the posterior probability of the null hypothesis: 
P(r1 - ~2 2 0) = P(Z 2 7.31) = .OOOO. 
We reject the null hypothesis and conclude that the proportion of New 
Zealand women in the younger group who have been married before age 
22 is less than the proportion of New Zealand women in the older group 
who have been married before age 22. 
13.13 (a) The posterior distribution of 7r1 is beta(137,179). 
(b) The posterior distribution of 7r2 is beta(136,85). 
(c) The approximate posterior distribution of 7r1-7~ isnormal( -.182, .042g2). 
(d) The 99% Bayesian credible interval for 7rl - 7r2 is (-.292, -.071). 
(e) We calculate the posterior probability of the null hypothesis: 
P(T1 - 7T2 2 0) = P(Z 2 4.238) = .0000 . 

ANSWERS TO SELECTED EXERCISES 
421 
We reject the null hypothesis and conclude that the proportion of New 
Zealand women in the younger group who have given birth before age 
25 is less than the proportion of New Zealand women in the older group 
who have given birth before age 25. 
13.15 
(a) The measurements on the same cow form a pair. 
(c) The posterior precision equals 
1
7
 
- + 7 
= .703704. 
32 
1 
The posterior variance equals & 
= .142105 and the posterior mean 
equals 
7 
.703704 
1 
1 
?P 
x 0 + 7.703704 x -3.9143 = -3.89368 
The posterior distribution of p d  is normal(-3.89, .37T2). 
(d) The 95% Bayesian credible interval is (-4.63, -3.15) 
(e) To test the hypothesis 
HO : pd = 0 Versus HI : /-ld # 0, 
we observe that the null value 0 lies outside the credible interval, so we 
reject the null hypothesis. 
Chapter 14: Bayesian Inference for Simple Linear Regression 
14.1 (a) and (c) The scatterplot of oxygen uptake on heart rate with least squares 
line 
0 1  
. 
I 
m 
,m 
110 
1m 
1 9  
Heart rate 
(b) The least squares slope 
145.610 - 107 x 1.30727 
11584.1 - 1072 
B =  
= 0.0426514 
The least squares y-intercept equals 
A0 = 1.30727 - .0426514 x 107 = -3.25643 

422 
ANSWERS TO SELECTED EXERCISES 
(d) The estimated variance about the least squares line is found by taking the 
sum of squares of residuals and dividing by n- 2 and equals 8' = .1303'. 
(e) The likelihood of /3 is proportional to a normal(B, &) 
where B is the 
least squares slope and SS, = n x (2 - iE2) = 1486 and cr2 = .132. 
The prior for p is normaZ(0, 1'). 
The posterior precision will be 
1 
1 ss, 
-- - - + - 
= 87930, 
(s')' 
l2 .132 
the posterior variance will be (s')' = 
posterior mean is 
= .000011373, and the 
1 
m' = iz 
X O f -  %f x .0426514 = .0426509. 
87930 
87930 
The posterior distribution of p is normal( .0426, .00337') 
(f) A 95% Bayesian credible interval for p is ($036, .049). 
(8) We observe that the null value 0 lies outside the credible interval, so we 
(a) and (c)The scatterplot of distance on speed with least squares line 
reject the null hypothesis. 
14.3 
(b) The least squares slope 
5479.83 - 105 x 52.5667 
B =  
= -0.136000 
11316.7 - 1052 
The least squares y-intercept equals 
A0 = 52.5667 - -0.136000 x 105 = 66.8467. 
(d) The estimated variance about the least squares line is found by taking 
the sum of squares of residuals and dividing by n - 2 and equals 6' = 
.571256'. 
(e) The likelihood of p is proportional to a normal(B, &) 
where B is the 
least squares slope and SS, = n x (2 
- Z2) = 1750 and cr' 
= .57'. 
The prior for p is normal(0, 1'). The posterior precision will be 
1 ss, 
(s')2 
1 2  
.57' 
-- - - + - 
= 5387.27, 
1 

ANSWERS TO SELECTED EXERCISES 
423 
the posterior variance ( s ’ ) ~  = & 
= ,000185623, 
and the posterior 
mean is 
% 
1 
,' 
= iz 
x 0 + 
x (-0.136000) = -.135975. 
5387.27 
5387.27 
The posterior distribution of /3 is normal(-.136, .0136’). 
(0 A 95% Bayesian credible interval for /3 is (-.163, -0.109). 
(8) We calculate the posterior probability of the null hypothesis. 
P(p 2 0 )  = P(Z 2 9.98) = .OOOO. 
This is less than the level of significance, so we reject the null hypothesis 
and conclude that ,B < 0. 
14.5 (a) and (c) Scatterplot of strei 
’I 1 
5th on fiber length with least squares line 
fiber iemh 
(b) The least squares slope 
8159.3 - 79.6 x 101.2 
B =  
= 1.47751. 
6406.4 - 79.62 
The least squares y-intercept equals 
A0 = 101.2 - 1.47751 x 79.6 = -16.4095 
(d) The estimated variance about the least squares line is found by taking the 
sum of squares of residuals and dividing by n - 2 and equals 6' = 7.667’. 
(e) The likelihood of /3 is proportional to a nomal(B, A), 
where B is the 
least squares slope and SS, = n x (2 
- 2') = 702.400 and u’ = 7.7’. 
The prior for p is nomaZ(0, lo2). - 9 e  posterior precision will be 
1 
'SS, 
- 
+ - 
= 11.8569, 
102 
7.72 
the posterior variance = & 
= .0843394, 
and the posterior mean is 
1 
x o + -  $?% 
x 1.47751 = 1.47626. 
11.8569 
11.8569 
The posterior distribution of /3 is normal(l.48, 
.2g2). 

424 
ANSWERS TO SELECTED EXERCISES 
(f) A 95% Bayesian credible interval for p is (.91, 2.05). 
(g) To test the hypothesis 
HO : ,B 5 0 versus H I  : ,B > 0 ,  
we calculate the posterior probability of the null hypothesis. 
= P(Z 5 -5.08) = .OOOO. 
This is less than the level of significance, so we reject the null hypothesis 
and conclude p > 0. 
(h) The predictive distribution for the next observation y11 taken for a yarn 
with fiber length 2 1 1  = 90 is noma1(116.553, 8.6222). 
(i) A 95% credible interval for the prediction is 
116.553 & 1.96 x 8.622 = (99.654,133.452). 
14.7 
(a) The scatterplot of number of ryegrass plants on the weevil infestation rate 
where the ryegrass was infected with endophyte. Doesn’t look linear. Has 
dip at infestation rate of 10. 
2cl 
10 71 
0 
10 
20 
(c) The least squares slope is given by 
19.9517 - 8.75 x 2.23694 
131.250 - 8.752 
B =  
= .00691966. 
The least squares y-intercept equals 
Ao = 2.23694 - .00691966 x 8.75 = 2.17640. 

ANSWERS TO SELECTED EXERCISES 
425 
(d) 6’ = .8501112 
(e) The likelihood of P is proportional to a nonnal(B, &), 
where B is 
the least squares slope and SSx = n x ($ - 5?2) = 1093.75 and 
u2 = .850111’. The prior for p is normaZ(0, 12). The posterior precision 
- 
is 
= 1514.45 
- = g + - - -  
1 
1 
SSX 
(s1)2 
.85011l2 
the posterior variance is (so2 = - 
= .000660307, and the posterior 
mean is 
1 
ss 
,’ 
= 2 
x 0 + 
x .00691966 = .00691509. 
1514.45 
1514.45 
The posterior distribution of /3 is normal( .0069, .02572). 
14.9 (a) To find the posterior distribution of P1 - P2, we take the difference 
between the posterior means, and add the posterior variances since 
they are independent. The posterior distribution of P1 - /32 is nor- 
rnaZ(1.012, .032’). 
(b) The 95% credible interval for 
(c) We calculate the posterior probability of the null hypothesis: 
- Pz is (.948,1.075). 
This is less than the level of significance, so we reject the null hypothesis 
and conclude P1 - /?z > 0. This means that infection by endophyte offers 
ryegrass some protection against weevils. 
Chapter 15: Bayesian Inference for Standard Deviation 
15.1 (a) The shape of the likelihood function for the variance 0’ is 
(b) The prior distribution for the variance is positive uniform g(a’) = 1 
for 0’ > 1. (This improper prior can be represented as Sx an inverse 
chi-squared distribution with -2 degrees of freedom where S = 0.) The 
shape of the prior distribution for the standard deviation 0 is found by 
applying the change of variable formula. It is 
gu(0) 0: gu4.2) x 0 
0: u .  

426 
ANSWERS TO SELECTED EXERCISES 
(c) The posterior distribution of the variance is 1428 x an inverse chi-squared 
with 8 degrees of freedom. Its formula is 
(d) The posterior distribution of the standard deviation is found by using the 
change of variable formula. It has shape given by 
(e) A 95% Bayesian credible interval for the standard deviation is 
(/=, 
/=) 
= (9.024,25.596). 
(0 
To test HO : a 2 8 vs. HI 
: 0 > 8 we calculate the posterior probability 
of the null hypothesis. 
1428 
P(u 5 8 )  = P ( W 2  7) 
= P(W 2 22.3125), 
where W has the chi-squared distribution with 8 degrees of freedom. 
From Table B.5 we see that this lies between the upper tail values for 
.005 and .001. (The exact probability of the null hypothesis found us- 
ing Minitab is .0044.) Hence we would reject the null hypothesis and 
conclude u > 8 at the 5% level of significance. 
15.3 (a) The shape of the likelihood function for the variance a’ is 
(b) The prior distribution for the variance is Jeffreys’ prior g(a2) = (a2)- 
for o2 > 1. (This improper prior can be represented as Sx an inverse 
chi-squared distribution with 0 degrees of freedom where S = 0.) The 
shape of the prior distribution for the standard deviation u is found by 
applying the change of variable formula. It is 
gu(0) 
CX 
gu2(a2) x 0 
0: 0 - I .  

ANSWERS TO SELECTED EXERCISES 
427 
(c) The posterior distribution of the variance is 9.4714~ an inverse chi- 
squared with 10 degrees of freedom. Its formula is 
(d) The posterior distribution of the standard deviation is found by using the 
change of variable formula. It has shape given by 
(e) A 95% Bayesian credible interval for the standard deviation is 
(JGEE 
~- 
20.483 ' /-) 
= (.680,1.708). 
(f) To test HO : u 5 1.0 vs. H I  : u > 1.0, we calculate the posterior 
probability of the null hypothesis. 
) 
9.4714 
P(a 5 1.0) = P(W 2 - 
12 
= P(W 2 9.4714) 
where W has the chi-squared distribution with 10 degrees of freedom. 
From Table B.5 we see that this lies between the upper tail values for 
.SO and .lo. (The exact probability of the null hypothesis found using 
Minitab is .4880.) Hence we can not reject the null hypothesis and must 
conclude u 5 1.0 at the 5% level of significance. 
15.5 (a) The shape of the likelihood function for the variance u2 is 
(b) The prior distribution is Sx an inverse chi-squared distribution with 1 
degree of freedom where S = .4549 x 42 = 7.278. Its formula is 
(c) The shape of the prior distribution for the standard deviation is found by 
applying the change of variable formula. It is 
go(g) 
0: g02(a2) x 
1 - 7.278 
K
-
 
( 4 2 e  -zF ' 

428 
ANSWERS TO SELECTED EXERCISES 
(d) The posterior distribution of the variance is 33.40 x an inverse chi-squared 
with 6 degrees of freedom. Its formula is 
(e) The posterior distribution of the standard deviation is found by using the 
change of variable formula. It has shape given by 
gO(a(yl,...,Y5) c( go2(a21Y1,**$Y5) a 
(f) A 95% Bayesian credible interval for the standard deviation is 
(/= 
14.449’ G) 
= (1.520,5.195). 
(8) To test HO : u 5 5 vs. HI : u > 5 we calculate the posterior probability 
of the null hypothesis. 
P ( a < 5 )  = p ( W
>
’
 
- 33j240) 
= P(W 2 1.336), 
where W has the chi-squared distribution with 6 degrees of freedom. 
From Table B.5 we see that this lies between the upper tail values for 
.975 and .95. (The exact probability of the null hypothesis found using 
Minitab is .9696.) Hence we would accept the null hypothesis and 
conclude u 5 5 at the 5% level of significance. 
Chapter 16: Robust Bayesian Methods 
16.1 (a) The posterior go(nly = 10) is beta(7 + 10,13 + 190). 
(b) The posterior g1(nIy = 10) is beta(1 + 10,l + 190). 
(c) The posterior probability P(1 = Oly = 10) = .163. 
(d) The marginal posterior g(nly = 10) = .163 x go(nly = 10) + .837 x 
gl(7rly = 10). This is a mixture of the two beta posteriors where the 
proportions are the posterior probabilities of I .  
16.3 (a) The posterior gO(pIy1,. . . , $3) is norma1(1.10061, .000898’). 
(b) The posterior gl(pIy1,. . . , y6) is normal(1.10302, .002’). 
(c) The posterior probability P(I = Olyl,. . . ,y6) = .972. 

ANSWERS TO SELECTED EXERCISES 
429 
(d) The marginal posterior 
g(pIY1,. . . 1 YS) = .972 x gO(plY1,. . . ,y6) + .028 x gI(pL(Y1,. . . > y6). 
This is a mixture of the two normal posteriors where the proportions are 
the posterior probabilities of I. 

This Page Intentionally Left Blank

References 
1. Barker, G., and McGhie, R. (1984), The Biology of Introduced Slugs (Pulmonata) 
in New Zealand: Introduction and Notes on Limax Maximus, NZ Entomologist 
8, 106-1 11. 
2. Bayes, T. (1 763), An essay towards solving a problem in the doctrine of chances, 
Philosophical Transactions of the Royal. Society 53, 370-41 8. (Reprinted in 
Biometrika 45 (1958), 293-315. 
3. Bennett, R. L., Curran, J. M., Kim, N. D., Coulson, S. A. and Newton, A.W.N. 
(2002), spatial Variation of Refractive Index in a Pane of Float Glass, Science 
andhstice, 43(2) 71-76. 
4. Berry, D. (1996), Statistics: A Bayesian Perspective, Duxbury, Belmont, CA. 
5. Bolstad, W. M., Hunt, L. A,, and McWhirter, J. L. (2001), Sex, Drugs, and 
Rock & Roll Survey in a First-Year Service Course in Statistics, The American 
Statistician Vol. 55, 145-149. 
6. Box, G., and Tiao, G. (1 992), Bayesian Inference in Statistical Analysis, Wiley 
Classics Library, John Wiley & Sons, New York. 
7. De Finetti, B. (1991), Theory of Probability, Volume I and Volume 2, Wiley 
Classics Library, John Wiley & Sons, New York. 
Introduction to Bayesian Statistics, Second Edition. By William M. Bolstad 
Copyright 0 2 0 0 7  John Wiley & Sons, Inc. 
43 1 

432 
REFERENCES 
8. Fergusson, D. M, Boden, J. M., and Horwood, L. J. (2006), Circumcision Status 
and Risk of Sexually Transmitted Infection in Young Adult Males: an Analysis 
of a Longitudinal Birth Cohort, Pediatrics Vol. 118, 1971-1977. 
9. Hoel, P. G. (1 984), Introduction to Mathematical Statistics, 5th Edition, Wiley, 
New York. 
10. Jaynes, E. T. and Bretthorst, G. L. (Editor), (2003), Probability Theory; The 
Logic of Science, Cambridge University Press 
1 1. Johnson, N., Kotz, S. and Balakrishnan, N.( 1970), Continuous Univariate Dis- 
tributions, Volume I ,  Wiley, New York. 
12. Lee, P. (1989), Bayesian Statistics: An Introduction, Edward Arnold, London. 
13. McBride, G., Till, D., Ryan, T., Ball, A,, Lewis, G., Palmer, S., and Weinstein, 
P. (2002)., Freshwater Microbiology Research Programme Pathogen Occurrence 
and Human Risk Assessment Analysis. 
14. McLeay, L. M., Carmthers, V. R., and Neil, P. G. (1997), Use of a breath test to 
determine the fate of swallowed fluids in cattle, American Journal of Veterinary 
Research 58,1314-1319. 
15. O’Hagan, A. (1 994), Kendull’s Advanced Theory of Statistics, Vol. 2B, Bayesian 
Inference, Edward Arnold, London. 
16. Petchey, F. (2000), Radiocarbon dating fish bone from the Houhora archeological 
site, New Zealand, Archeol. Oceania 35, 104-1 15. 
17. Petchey, F. and Higham, T. (2000), Bone diagenesis and radiocarbon dating of 
fish bones at the Shag River mouth site, New Zealand, Journal ofArcheological 
Science 27,135-150. 
18. Press, S .  J. (1989), Bayesian Statistics: Principles, Models, and Applications, 
John Wiley & Sons, New York. 
19. Stigler, S. M. (1977), Do robust estimators work with real data? (With discus- 
sion.), The Annals of Statistics 5, 1055-1098. 
20. Stuiver, M., Reimer, P.J., Braziunas, S. (1998), High precision radiocarbon age 
calibration for terrestial and marine samples, Radiocarbon 40, 1 127-1 15 I .  
2 1. Thorp, E. (1 962), Beat the Dealer, Blaisdell Pub. Co; [ 1st ed.] edition (1962) 
22. Wald, A. (1950), Statistical Decision Functions, Wiley, New York. 

Index 
Bayes' theorem, 66, 73 
Bayes' theorem using table 
binomial observation with discrete prior, 110 
discrete observation with discrete prior, 104 
normal observation with discrete prior, 200 
Poisson observation with discrete prior, 112 
Poisson 
analyzing the observations all together, 106, 203 
analyzing the observations sequentially, 202, 
binomial observation 
beta prior, 143 
continuous prior, 142 
discrete prior, 108 
mixture prior, 322 
uniform prior, 142 
Bayes' theorem 
Jeffreys' prior, 185 
106 
discrete random variables, 101 
events, 63, 65, 68 
linear regression model, 276 
mixture prior, 3 19 
normal observations known mean 
inverse-chi-squared prior for u2, 302 
Jeffreys' prior for u2, 302 
positive uniform prior for uz, 
301 
continuous prior for p, 205 
discrete prior for p, 199 
flat prior for p, 206 
normal observations with known variance 
mixture prior, 324 
normal prior for p, 207 
Poisson observation 
continuous prior, 183 
gamma prior, 185 
positive uniform prior, 184 
Bayes factor, 70 
Bayesian approach to statistics, 6, 1 1  
Bayesian credible interval, 153 
binomial proportion n, 
153 
difference between normal means p1 - p 2  
equal variances, 240 
unequal variances, 246 
difference between proportions "1 - "2.248 
normal mean p, 21 1,226 
normal standard deviation u, 309 
Poisson parameter p, 192 
regression slope p, 280 
used for Bayesian two-sided hypothesis test, 176 
normal mean p, 224 
binomial proportion 7r, 152 
normal u, 308 
one-sided 
Bayesian estimator 
Bayesian hypothesis test 
binomial proportion n, 173, 
difference between normal means fi1 - pz, 
normal mean p, 230 
normal standard deviation u, 
3 10 
242, 
433 

434 
/NOEX 
Poisson parameter p, 193 
regression slope @, 280 
binomial proportion T, 176, 
difference between normal means p1 - pz, 
normal mean p, 234 
Poisson parameter p, 194 
regression slope p, 281 
Bayesian inference for standard deviation, 297 
Bayesian universe, 66, 101, 112 
parameter space dimension, 69,74, 101, 112 
reduced, 67, 102, 113 
sample space dimension, 69, 74, 101, 112 
density, 128 
mean, 128 
normal approximation, 133 
shape, 127 
variance, 129 
response, 16 
sampling, 14 
characteristics of, 84 
mean, 84 
probability function, 84 
table, 361-363 
variance, 85 
blackjack, 71, 76 
boxplot, 30, 48 
stacked, 37 
central limit theorem, 132, 199 
Chi-squared distribution, 359 
conditional probability, 73 
conditional random variable 
two-sided 
243,245 
beta distribution, 127 
bias 
binomial distribution, 83, 96, 141, 353 
continuous 
conditional density, 134 
binomial observation, 144, 155 
Poisson observation, 185-1 86 
continuous random variable, 121 
probability density function, 123, 136 
probability is area under density, 124, 136 
bivariate data set, 46,49 
bivariate data set, 46 
conjugate family of priors 
correlation 
covariance 
cumulative frequency polygon, 35.48 
deductive logic, 56 
degrees of freedom, 43 
unknown variance, 2 13 
simple linear regression, 280 
two samples unknown equal variances, 244 
two samples unknown unequal variances 
Satterthwaite’s adjustment, 246 
derivative, 339 
higher, 341 
partial, 349 
completely randomized design, 18, 22, 2 4 2 5  
randomized block design, 19,22,2425 
designed experiment, 18, 22 
differentiation, 339 
discrete random variable, 77-78, 95 
expected value, 80 
probability distribution, 77, 80,95 
variance, 81 
dotplot, 30 
stacked, 37 
equivalent sample size 
beta prior, 147 
gamma prior, 187 
normal prior, 209 
frequentist, 163, 223 
mean squared error, I64 
minimum variance unbiased, 164, 224 
sampling distribution, I63 
unbiased, 164, 224 
estimator 
Event, 58 
event 
events 
complement, 58, 73 
independent, 6 0 6  I 
intersection, 58, 73 
mutually exclusive (disjoint), 58, 61,73 
partitioning universe, 64 
union, 58, 72 
expected value 
continuous random variable, 125 
discrete random variable, 80, 95 
experimental units, 17-18, 20, 24 
finite population correction factor, 86 
five number summary, 3 1 
frequency table, 33 
frequentist approach to statistics, 5, 11 
frequentist confidence interval, 167 
normal mean p, 226 
regression slope p, 280 
relationship to frequentist hypothesis tests, 175 
p-value, I72 
level of significance, 171 
null distribution, 172 
one-sided 
frequentist confidence intervals 
frequentist hypothesis test 
binomial proportion T ,  171 
normal mean p, 229 
rejection region, 172 
two-sided 
binomial proportion x ,  173 
normal mean p. 232 

/ND€X 
435 
frequentist 
161 
interpretation of probability and parameters, 
function, 333 
antiderivative, 342 
continuous, 337 
differentiable, 339 
graph, 334 
limit at a point, 335 
fundamental theorem of calculus, 346 
gamma distribution, 129 
maximum and minimum, 338 
critical points, 341 
density, 130 
mean, 130 
shape, 129 
variance, 13 I 
histogram, 3 4 3 5 , 4 8  
hypergeometric distribution, 85 
mean, 86 
probability function, 86 
variance, 86 
integration, 342 
definite integral, 342,345,347 
multiple integral, 350 
interquartile range 
data set, 42, 49 
posterior distribution, I52 
inverse chi-squared distribution, 3 10 
density, 298 
Jeffreys’ prior 
binomial, 145 
normal mean, 206 
normal variance, 302 
Poisson, 185 
joint likelihood 
linear regression sample, 276 
joint random variables 
conditional probability, 92 
conditional probability distribution, 93 
continuous, 134 
continuous and discrete, 135 
continuous 
joint density, 134 
marginal density, i 34 
joint probability distribution, 89 
marginal probability distribution, 89 
discrete, 89 
independent, 9 I 
joint probability distribution, 96 
marginal probability distribution, 96 
binomial, 108 
discrete parameter, 103-104 
events partitioning universe, 66 
likelihood 
proportional, 1 11 
mean 
multiplying by constant, 67, I 1  1 
normal mean 
random sample of size n, 203 
using density function, 201 
using ordinates table, 200 
single normal observation, 200 
normal variance, 299 
normal 
Poisson, 184 
regression 
sample mean g, 203 
intercept O L ~ ,  
277 
slope p, 277 
sample mean from normal distribution, 209 
deductive, 72 
inductive, 72 
logic 
lurking variable, 2, 10, 19-20, 25 
marginalization, 2 14, 282 
marginalizing out the mixture parameter, 321 
mean squared error, 225 
mean 
continuous random variable, 125 
data set, 40, 49 
difference between random variables, 92, 96 
discrete random variable, 80 
grouped data, 40 
of a linear function, 82, 96 
sum of random variables, 90, 96 
trimmed, 42,49 
measures of location, 39 
measures of spread, 42 
median 
data set, 41, 47,49 
mixture prior, 3 17 
Monte Carlo study, 7, 11,23-24,71 
nonsampling errors, I6 
normal distribution, 13 1 
area under standard normal density, 354, 364 
density, 131 
mean, 13 1 
ordinates of standard normal density, 355, 365 
shape, 131 
standard normal probabilities, 132 
variance, 13 1 
nuisance parameter, 7,214,282, 297 
nverse chi-squared distribution, 298 
observational study, 17, 22 
Ockham’s razor, 4, 170 
odds ratio, 69 
order statistics, 30, 32, 47 
outcome, 58 
outlier, 40 
parameter, 5-6, 14, 2 I ,  69 
parameter space, 69 

436 
INDEX 
plausible reasoning, 56, 72 
point estimation, 163 
Poisson distribution, 86, 183, 358 
characteristics of, 87 
mean, 88 
probability function, 87 
table, 367-368 
variance, 88 
population, 5, 14, 21 
posterior distribution, 6 
discrete parameter, 103-1 04 
normal with discrete prior, 200 
regression slope 0, 
278 
of an estimator, 152 
as an estimate for x, 152 
beta distribution, 150 
gamma distribution, 189 
as an estimate for x ,  152 
beta distribution, 150 
gamma distribution, 189 
beta distribution, 150 
gamma distribution, I89 
posterior probability distribution 
binomial with discrete prior, 110 
posterior probability 
of an unobservable event, 66 
posterior standard deviation, 15 1 
posterior variance 
beta distribution, 151 
pre-posterior analysis, 8, I 1  
precision 
normal 
posterior mean square 
posterior mean 
posterior median 
posterior mode 
fj, 209 
observation, 208 
posterior, 208 
prior, 208 
likelihood, 278 
posterior, 278 
prior, 278 
normal, 2 14 
regression model, 281 
choosing beta prior for T 
regression 
predictive distribution 
prior distribution, 6 
matching location and scale, 146, 155 
vague prior knowledge, 146 
choosing inverse chi-squared prior for u2, 303 
choosing normal prior for p, 209 
choosing normal priors for regression, 277 
constructing continuous prior for p, 210 
constructing continuous prior for x ,  147, 155 
discrete parameter, 102 
multiplying by constant, 67, I 1 I 
uniform prior for x, 155 
for an unobservable event, 66 
prior probability 
probability, 58 
probability distribution 
conditional, 93 
continuous random variable 
probability density function, I23 
probability 
addition rule, 60 
axioms, 59.72 
conditional, 62 
degree of belief, 69 
joint, 60 
law of total probability, 64, 73 
long run relative frequency, 68 
marginal, 61 
multiplication rule, 63, 73, 94 
data set, 30,48 
from cumulative frequency polygon, 35 
posterior distribution, 15 1 
independent events, 63 
quartiles 
random experiment, 58, 72 
random sampling 
cluster, 16, 22 
simple, 15, 22 
stratified, 15,22 
randomization, 5, 10 
randomized response methods, 16, 22 
range 
regression 
data set, 42,49 
Bayes’ theorem, 276 
least squares, 268 
normal equations, 268 
simple linear regression assumptions, 273 
robust Bayesian methods, 3 17 
sample, 5, 14, 21 
sample space, 69, 72 
sampling distribution, 7, 11, 23-24, 162 
sampling frame, 15 
scatterplot, 44,49,267 
scatterplot matrix, 45, 49 
scientific method, 3, 10 
role of statistics, 4, 10 
standard deviation 
data set, 44.49 
statistic, 14, 21 
statistical inference, 1, 14, 72 
statistics, 5 
stem-and-leaf diagram, 32, 48 
of a random experiment, 58 
back-to-back, 37 

INDEX 
437 
Student’s t distribution, 212, 297, 357 
critical values, 366 
uniform distribution, I26 
universe, 58 
of a joint experiment, 89 
reduced, 62, 65, 92 
binomial proportion T, 145 
normal mean p, 208 
normal variance g2, 303 
updating rule 
Poisson parameter p, 186 
continuous random variable, 126 
data set, 43, 49 
difference between ind. RV’s, 92, 96 
discrete random variable, 81, 95 
grouped data, 43 
linear function, 82, 96 
sum of ind. RV’s, 91, 96 
variance 
Venn diagram, 58, 60 

This Page Intentionally Left Blank

