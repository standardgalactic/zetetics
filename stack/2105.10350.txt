Deﬁnite Non-Ancestral Relations and Structure
Learning
Wenyu Chen1, Mathias Drton2, and Ali Shojaie3
1Department of Statistics, University of Washington, Seattle, WA, USA
2Department of Mathematics, Technical University of Munich, München, Germany
3 Department of Biostatistics, University of Washington, Seattle, WA, USA
May 24, 2021
Abstract
In causal graphical models based on directed acyclic graphs (DAGs), directed paths
represent causal pathways between the corresponding variables. The variable at the
beginning of such a path is referred to as an ancestor of the variable at the end of the
path. Ancestral relations between variables play an important role in causal modeling.
In existing literature on structure learning, these relations are usually deduced from
learned structures and used for orienting edges [4] or formulating constraints of the
space of possible DAGs [12]. However, they are usually not posed as immediate target
of inference. In this work we investigate the graphical characterization of ancestral
relations via CPDAGs and d-separation relations. We propose a framework that can
learn deﬁnite non-ancestral relations without ﬁrst learning the skeleton. This frame-
work yields structural information that can be used in both score- and constraint-based
algorithms to learn causal DAGs more eﬃciently.
1
Introduction
Directed Acyclic Graphs (DAGs) are commonly used as models of causal relations between
random variables in complex systems [15, 21]. In this framework, every random variable is
modeled to be a function of other random variables (its causes) and stochastic noise. A DAG
is used to represent the resulting model for the system of all variables. The vertices of this
DAG represent the random variables and edges represent direct causal eﬀects. The DAG
aids, in particular, in understanding conditional independences that the model imposes on
the joint distribution of the random variables. These conditional independences provide an
alternative characterization of the joint distribution and can be read oﬀthe graph using
the concept of d-separation. If a joint distribution satisﬁes all these imposed conditional
independences, it is said to be Markov with respect to the DAG. Structure learning from
observational data is then the problem of learning a DAG from data sampled independently
from a distribution that is Markov with respect to the DAG.
A crucial aspect of structure learning stems from the fact that the data-generating DAG
may be non-identiﬁable: Many diﬀerent DAGs may yield the same statistical model for
the observational data at hand. These DAGs form a Markov equivalence class, which is
then the object to be learned. The Markov equivalence class can be uniquely recovered
from conditional independences among the corresponding random variables, and it can be
represented via a completed partially directed acyclic graph (CPDAG) [21]. Members of a
Markov equivalence class share the same adjacencies and unshielded colliders [1], which are
represented explicitly in the CPDAG. However, there are also common structures that are
implicit.
In this work, we study deﬁnite ancestral and deﬁnite non-ancestral relations, which are
ancestral and non-ancestral relations shared by all members of a Markov equivalence class.
We provide graphical interpretations of these relations in the CPDAG, and we also provide
a framework for reliably learning deﬁnite non-ancestral (DNA) relations without the need
to recover the skeleton. These relations not only provide causal interpretations (in the form
of “change in X deﬁnitely does not cause change in Y ”), but also facilitate further structure
learning. Indeed, we show that learning DNA relations directly can greatly improve the
statistical and computational eﬃciency of existing structure learning algorithms.
Existing structure learning methods can be broadly categorized into constraint-based,
score-based, and hybrid approaches. In constraint-based approaches, a DAG is learned via
tests of conditional independence. In score-based approaches, a score, for example BIC,
is assigned to each DAG and then an algorithm searches for the DAG that optimizes the
1
arXiv:2105.10350v1  [cs.LG]  20 May 2021

score. Hybrid approaches use schemes in which the two approaches inform each other. In this
paper, we will focus on two prominent examples of learning algorithms: The PC algorithm
[21] and the Sparsest Permutation algorithm [18, 20].
The PC algorithm is the default
constraint-based method and hierarchically performs tests of conditional independence with
conditioning sets of increasing size. Under a faithfulness assumption, the population version
of PC algorithm outputs the correct CPDAG [21]. At the population level (i.e., with “inﬁnite
data”), the faithfulness assumption merely requires that the conditional independences in
the data-generating distributions coincide (to suﬃcient order) with d-separation relations
in the DAG. However, good performance of the sample version is only guaranteed when
the assumption is strengthened to bound signals of conditional dependence away from zero,
which is far more restrictive [10, 23]. On the other hand, the Sparsest Permutation algorithm
[18, 20] is a hybrid learning method that searches among all topological orderings. For each
ordering, a DAG is inferred via conditional independence tests given the ordering, and the
number of edges in the DAG is used as the score. The algorithm looks for the sparsest DAG
under which the data-generating distribution is Markov. The SP algorithm relies on weaker
distributional assumptions than PC, but comes at increased computational cost due to its
score-based searching scheme.
As noted above, we propose here an algorithm to directly learn DNA relations. This
algorithm is constraint-based and derives the relations from conditional independences/de-
pendences. The learning algorithm is based on two well-known rules of conditional inde-
pendence that has been studied extensively in [4, 5, 7, 12]. We will then show that the
learned DNA relations provide order-constraining information. In other words, they deﬁne
a subset of all possible topological orderings (or DAGs) that is guaranteed to contain one
correct ordering (or DAG). Therefore, DNA relations can be used to reduce the number of
conditional independence tests needed for running the PC algorithm. Similarly, the order-
ing information provided by the DNA relations can also help signiﬁcantly reduce the search
space of the SP algorithm, which grows exponentially with graph size. Regarding this last
point, we would like to highlight an independent preprint [22] that uses information from
the moral graph to reduce the search space of SP. Compared with that work, our framework
of DNA relations is more general and can accommodate weaker assumptions.
2
Graphical preliminaries
Our exposition follows standard terminology in graphical modeling; see e.g. Part I of [11].
Here, we brieﬂy summarize the most important concepts.
Let G = (V, E) be a graph with vertex set V and edge set E. Then G is directed (or
undirected) if E contains only directed (or only undirected) edges; these we denote simply
as u →v (or u −v). A mixed graph may contain both directed and undirected edges.
Throughout, and even when considering mixed graphs, we will only consider graphs that
are simple, i.e., there may be at most one edge between any pair of nodes. Two nodes in a
graph are adjacent if they are linked by an edge. The skeleton of a (possibly mixed) graph
G is the undirected graph that has an edge between any pair of nodes that are adjacent in
G. A set of nodes is a clique if any two nodes in the set are adjacent.
A path in a graph is a sequence of distinct and adjacent vertices. A directed path is a
path along directed edges following the arrowheads. Adding an additional directed edge
back to the ﬁrst node gives a directed cycle. A directed acyclic graph (DAG) is a directed
graph without directed cycles. If a graph G contains the edge u →v, then u is a parent
of its child v. We write pa(G, v) for the set of all parents of a node v. Similarly, ch(G, v)
is the set of children of v. If there is a directed path u →· · · →v, then u is an ancestor
of its descendant v. The sets of ancestors and descendants of v are denoted an(G, v) and
de(G, v), respectively. By convention, v ∈an(G, v) and v ∈de(G, v). A set of nodes C is
ancestral if an(G, v) ⊆C for all v ∈C. When clear, we may drop the indication of the
graph G, writing, e.g., an(v) only.
A triple of vertices (u, v, w) is unshielded if v is adjacent to both u and w, but u and
w are not adjacent. A non-endpoint vertex v on a path π is a collider on the path if both
the edges preceding and succeeding it have an arrowhead at v. A non-endpoint vertex v
on a path π which is not a collider is a non-collider on the path. An unshielded triple
(u, v, w) is called a v-structure if v is a collider on the path (u, v, w). The Markov blanket
of a vertex, denoted mb(u), is the union of vertices connecting to it through either an edge,
or a v-structure. For a choice of two vertices u and v, let C ⊂V \ {u, v}. If there is a path
from vertex u to vertex v such that all of its colliders are ancestors of an element of C and
all its non-colliders are outside C, then u and v are d-connected given C. If this is not the
case, then u and v are d-separated given C.
2

u
1
2
v
u
1
2
3
v
4
u
1
2
3
v
Figure 1: In the graph on the left, Auv = {1, 2} is not a clique, so u must have an arrowhead
to one of them, and thus u ⇝v as suggested by Lemma 1. In the middle graph, Auv = {3},
which is trivially a clique, u is not deﬁnitely ancestral to v. In the last graph, Auv = {2, 3},
which forms a clique, and it is possible to orient both 2 and 3 into u and make u /∈an(v),
and therefore u is not deﬁnite ancestral to v according to Lemma 1.
3
Deﬁnite Non-Ancestral Relations
A DAG G is usually not uniquely identiﬁable from the distribution of observational data.
Instead, there may be other graphs G′ that entail the exact same d-separation relations.
Together, these graphs form the Markov equivalence class, [G]. Importantly, the graphs in
[G] share their adjacencies and possibly also some of their edge marks. In other words, the
equivalent graphs G′ ∈[G] may diﬀer only through reversals of directed edges and some of
the directed edges may in fact be oriented the same way in all members of [G]; these latter
set of edges can be unambiguously interpreted as direct causal eﬀects. The characterization
and discovery of such edges is well-studied [1, 14].
Throughout this paper, we will use the adjective ‘deﬁnite’ to highlight the structure
common to all members of a Markov equivalence class, [G]. With this terminology, the
edges that can be oriented the same way in all members of [G] are deﬁnite (as opposed to
incidental) arrows. We similarly deﬁne deﬁnite ancestral and deﬁnite non-ancestral relations
with regard to causal pathways.
Deﬁnition 1 (Deﬁnite Ancestral and Deﬁnite Non-Ancestral Relations). Let u, v be two
nodes in a DAG G. Then u is deﬁnite ancestral to v, denoted u ⇝v, if u ∈an(G′, v) for
all G′ ∈[G]. Similarly, u is deﬁnite non-ancestral to v, or u ̸⇝v, if u ̸∈an(G′, v) for
all G′ ∈[G]. When writing u ⇝W for a set of nodes W, we mean u ∈an(G′, W) for
all G′ ∈[G]; when writing u ̸⇝W, we mean u ̸⇝w for all w ∈W. Finally, we deﬁne
D⇝(G) = {(u, v) : u ⇝v in G} and D̸⇝(G) = {(u, v) : u ̸⇝v in G}.
We emphasize that the two notions are not complementary to each other. While the
acyclicity of the graph entails that u ⇝v implies v ̸⇝u, the converse need not be true.
In the rest of this section, we present two perspectives of deﬁnite ancestral and non-
ancestral relations: the CPDAG perspective and the d-separation perspective. The former
provides a set of rules to read the (non-)ancestral relations oﬀa CPDAG, and the latter
enables eﬃcient learning without knowing the CPDAG. This second perspective provides a
foundation for learning DNA directly from a probability distribution, or data drawn from
it.
3.1
Ancestral and non-ancestral relations from CPDAGs
Let G be any DAG. The Markov equivalence class [G] can be represented using the completed
partially directed acyclic graph (CPDAG) G∗. The CPDAG is a mixed graph containing
directed and undirected edges that has the same skeleton as G and whose edges are directed if
and only if they have the same orientation in all members of the Markov equivalence class.
The CPDAG representation is complete in the sense that all directed edges are deﬁnite
arrows, and for each undirected edge u−v, there exists two DAGs in [G] that contain u →v
and u ←v, respectively.
From the CPDAG perspective, deﬁnite ancestral and non-ancestral relations can be
identiﬁed via possibly directed paths, which are paths between two nodes with no arrow into
the initial node. We say a possibly directed path is unshielded if no three consecutive nodes
on the path form a triangle. Notably, if there exists a possibly directed path from u to v in
the CPDAG, then some subsequence of this path forms an unshielded possibly directed path
[16, 24]. We now give a comprehensive characterization of deﬁnite ancestral and deﬁnite
non-ancestral relations in terms of the CPDAG; all proofs are given in the supplement.
Lemma 1. Let u and v be two nodes in a CPDAG G∗.
• Let Auv be the collection of all nodes that lie immediately after u on some unshielded
possibly directed path to v. Then u ⇝v if and only if either u has a deﬁnite arrow
into Auv or Auv is not a clique.
• u ̸⇝v if and only if there is no possibly directed path from u to v in G∗.
Figure 1 shows examples of deﬁnite ancestral relations.
3

1
2
4
3
5
1
2
4
3
5
deﬁnite
ancestral
2 ⇝4, 3 ⇝4, 4 ⇝5
deﬁnite
non-
ancestral
1 ̸⇝3, 3 ̸⇝1, 2 ̸⇝3, 3 ̸⇝2,
4 ̸⇝{1, 2, 3}, 5 ̸⇝{1, 2, 3}, 5 ̸⇝
4
layering
(3, {1, 2}, {4, 5}) or ({1, 2}, 3, {4, 5})
Figure 2: A Markov equivalence class comprising two DAGs and its deﬁnite ancestral/non-
ancestral relations. All DNA except for 5 ̸⇝4 can be discovered by Lemma 2.
1
v
2
3
u
Figure 3: In this example, u ̸⇝v but cannot be read from the DAG using Lemma 2.
3.2
Ancestral and non-ancestral relations from d-separations
Deﬁnite ancestral relations are easy to interpret, but somewhat delicate to read oﬀthe
CPDAG. In contrast, deﬁnite non-ancestral relations have more subtle interpretations but
are easier to read oﬀthe CPDAG; they are also simpler to learn from d-separation relations.
Next we present well known rules about ancestral eﬀects. (See, e.g., [4, 5, 7, 12].)
Lemma 2 (DA/DNA via d-separation). Let G = (V, E) be a DAG. Let u, v, x be distinct
vertices, and let W ⊆V \ {u, v, x}. Then the following holds:
• If u, v are d-connected given W but d-separated given W ∪x, then x ⇝W ∪{u, v}.
• If u, v are d-separated given W but d-connected given W ∪x, then x ̸⇝u, x ̸⇝v and
x ̸⇝W.
• If u, v are d-separated marginally (i.e., given ∅), then u ̸⇝v and v ̸⇝u.
The claims of Lemma 2 are illustrated in Figure 2. The following result is a special case
of the second statement1:
Corollary 1. If u, v are d-separated given V \{u, v, x} and are d-connected given V \{u, v},
then x is a sink, i.e., x ̸⇝V \ {x}.
Note also that the characterization of deﬁnite (non-)ancestral relations is not complete,
meaning there exist such relations that do not feature the conﬁgurations stated in Lemma 2;
see Figure 3.
In general, Lemma 2 provides a way to identify DNA between two nodes, i.e., in the form
of u ̸⇝v. However, we cannot identify deﬁnite ancestral relations between two nodes (see
Figure 4). For this reason, we only discuss learning and applications of DNA in Section 4
and 5.
4
Learning DNA relations
In this section, we discuss how to learn DNA relations from observational data. Let Ω0(P) =
{(u, v, S) : u ⊥⊥v|S in P} be the collection of all conditional independences, in the form of
tuples, that hold in distribution P. We can express the Markov property as follows.
Deﬁnition 2 (Markov Property). A distribution P is Markov with respect to a DAG G if
u, v are d-separated by S in G =⇒(u, v, S) ∈Ω0(P).
The reverse implication is known as the faithfulness condition.
If P is Markov and
faithful to G, then the Markov equivalence class, [G], can be recovered exactly from P, or
from observations obtained from P.
Lemma 2 states that DNA relations can be identiﬁed from d-separation and a d-connection
relations: DNA relations can be correctly learned from the distribution if d-separation and
d-connection relations correspond exactly to conditional independence and conditional de-
pendence. We formalize this condition as DNA-faithfulness.
1This special case is discussed in Squires et al. [22].
4

v
w
v
v
w
v
v
w
v
Figure 4: Three Markov equivalent DAGs, in all of which nodes u and v are marginally d-
connected and d-separated given w; thus w ⇝{u, v}. However, this does not imply w ⇝u
or w ⇝v.
Algorithm 1: General DNA-learning framework
Input
: An arbitrary constraint based algorithm A
Output: A set of DNA D ⊆D̸⇝(G)
1 D ←∅;
2 Run A, record the conditional independences discovered by A as ΩA;
3 for (x, y, S) ∈ΩA do
4
for z ∈V \ {x, y, S} do
5
if x ̸⊥⊥y|S ∪z then
6
Record z ̸⇝x, z ̸⇝y and z ̸⇝S in D
7 return D.
Deﬁnition 3 (DNA-faithfulness). Let Ωand ¯Ωbe two collection of triples consisting of
two vertices and one set of nodes from a DAG G. We say that a joint distribution P is
DNA-faithful to G with respect to (Ω, ¯Ω) if Ω⊆Ω0(P), ¯Ω∩Ω0(P) = ∅, and it holds for any
three nodes u, v, z and set S ⊆V \ {u, v, z} that
(u, v, S) ∈Ωand (u, v, S ∪z) ∈¯Ω=⇒u, v are d-separated by S in G.
(1)
We can learn DNA relations by a two-step procedure that ﬁrst runs an algorithm to
learn d-separation relations, and then performs additional tests to identify d-connections
and learn DNA by Lemma 2. Let A be an arbitrary constraint-based structure learning
algorithm that tests and collects a set of conditional independence statements ΩA(P) that
hold in a distribution P. Then we perform additional tests to detect conditional dependences
in P, i.e., we form
¯ΩA(P) = {(u, v, S ∪z) : u ̸⊥⊥v|S ∪z, (u, v, S) ∈ΩA(P)}.
We summarize this framework to learn DNA in Algorithm 1.
Theorem 1 (Correctness of DNA Learning). Let P be a distribution Markov to a DAG G,
and let A be a constraint-based learning algorithm such that P is DNA-faithful to G with
respect to (ΩA(P), ¯ΩA(P)). Then, the output of Algorithm 1 is a set of true DNA relations
in G.
4.1
Learning DNA from small d-sep sets
DNA learning begins by looking for d-separation relations.
A classic approach for dis-
covering d-separation relations systematically is to hierarchically test for conditional inde-
pendences given sets of increasing size, 0, 1, 2, . . .. In particular, this is the idea behind
the PC algorithm. Adopting this strategy here, we can use the ﬁrst few rounds of PC to
learn d-separations. In other words, we use the PC algorithm with early stopping as the
constraint-based procedure A in Algorithm 1. The procedure is summarized in Algorithm 2.
Algorithm 2: DNA-learning via small conditioning sets
Input
: A conditional independence test, a level K
Output: A set of DNA relations D ⊆D̸⇝(G)
1 Repeat steps 2-5 of Algorithm 1 using PC with conditioning sets of size 0, . . . , K.
2 return D.
The rationale behind stopping the PC early when learning DNA is that the learning
procedure A in Algorithm 1 only needs to ﬁnd correct d-separations. However, false positive
edges in the output of A are allowed.
In fact, our empirical results in the supplement
(Section 9.2) show that in most cases a large number of DNA relations can be learned from
considering only the ﬁrst two steps of the PC algorithm.
With a view towards practical algorithms, we now focus on data from a multivariate
Gaussian distribution P and adopt a threshold λ to specify signal strengths. Let ρ(u, v|S)
5

be the partial correlation obtained from the conditional distribution for the pair (u, v) given
S. Deﬁne
ΩK
λ (P) := {(u, v, S) : |S| = K and |ρ(u, v|S)| ≤λ},
¯ΩK
λ (P) := {(u, v, S) : |S| = K and |ρ(u, v|S)| > λ}.
Furthermore, let
Ω↑K
λ (P) :=
K
[
k=0
Ωk
λ(P)
be the triples obtained from conditioning sets of size at most K and strength threshold λ.
The next result guarantees the correctness of Algorithm 2.
Theorem 2 (Correctness of DNA learning from small conditioning sets). Let G be a DAG,
and let P be a distribution that is Markov to G and DNA-faithful to G with respect to

Ω↑K
λ (P), ¯ΩK+1
λ
(P)

. Then Algorithm 2 with level K returns a correct set of DNA relations.
For all choices of K, the DNA-faithfulness assumption with respect to (Ω↑K
λ (P), ¯ΩK+1
λ
(P))
is weaker than the λ-strong faithfulness assumption for the PC algorithm; see [23]. However,
it cannot be directly compared with the adjacency and orientation faithfulness, which are
the condition for completeness of PC [17]. Nevertheless, in the special case of K = 0, the
DNA-faithfulness condition is mild: If P is Gaussian, then it is suﬃcient if the correlation
of every pair of marginally d-connected nodes is bounded away from zero. Our numerical
result in Section 6 also conﬁrms that DNA-faithfulness is generally not stronger than other
notions of faithfulness.
Given Theorem 2, we next provide sample guarantees for linear structural equation
models (SEMs) with sub-Gaussian errors. In other words, we assume that the considered
joint distribution P is that of a random vector X that satisﬁes
X = BT X + ε,
where B = (βuv) has entry βuv ̸= 0 only if u →v is an edge in the considered DAG.
The vector ε is comprised of independent random variables. In the following theorem we
assume the distribution of the random vectors to be sub-Gaussian. Given data we may form
sample partial correlation ˆρ(u, v|S) and implement Algorithm 2 by rejecting a hypothesis
of conditional independence when ˆρ(u, v|S) > λ. The following result covers both low- and
high- dimensional problems.
Theorem 3 (Sample guarantee for DNA learning from small conditioning sets). Suppose
data is generated as n independent draws from the joint distribution P of a random vector
X ∈Rp that follows a linear SEM. Let Σ be the covariance matrix and assume that each
Xi/Σ1/2
ii
is sub-Gaussian with parameter σ. Assume P is Markov with respect to a DAG
G, and also DNA-faithful to G with respect to (Ω↑K
λ (P), ¯ΩK+1
λ
(P)). Assume the minimal
eigenvalues of all (K + 3) × (K + 3) submatrices of Σ are bounded below by M > 0. For any
ζ > 0, if the sample size satisﬁes
n ≥

log(p2 + p) −log(ζ/2)
	
128(1 + 4σ2)2 max
i (Σii)2(K + 3)2
M + 1 + 2/λ
M 2
2
,
(2)
then the output of Algorithm 2 with level K and tests based on sample partial correlations
is correct with probability at least 1 −ζ.
4.2
Learning DNA from large d-sep sets
The d-separation relations for DNA can also be learned tractably by testing conditional
independences given large conditioning sets of size p −2, p −3, . . ., where p denotes again
the number of considered variables. In fact, by Corollary 1, if x, y are d-separated given
V \ {x, y, u} and are d-connected given V \ {x, y}, then u ̸⇝V \ u. This means that we can
learn DNA relations from the moral graph, which encodes the d-separation relations that
hold between each pair of nodes when conditioning on all other nodes. A special case of this
approach was implemented as a recursive algorithm in [22]. The theorem below establishes
the correctness of the general strategy for learning DNA relations.
Given a joint distribution P, deﬁne
Ω↓K
λ (P) =
p[
k=p−K
Ωk
λ(P).
6

Algorithm 3: DNA-learning via large conditioning sets
Input
: A conditional independence test, a level K
Output: A set of DNA relations D ⊆D̸⇝(G)
1 Initialize eV = V ;
2 repeat
3
f
M ←moral graph over eV ;
4
for u ∈eV do
5
Mu ←moral graph over eV \ u;
6
f
Mu ←subgraph of f
M over eV \ u;
7
if
f
Mu contains more edges than Mu then
8
Record u ̸⇝eV \ u;
9
Update eV ←eV \ {u}; Break;
10 until |eV | = p −K;
11 return D.
Theorem 4 (Correctness of DNA learning from large conditioning sets). Let G be a DAG,
and let P be a distribution that is Markov to G and DNA-faithful to G with respect to

Ω↓K−1
λ
(P), ¯ΩK
λ (P)

. Then Algorithm 3 with level K returns a correct set of DNA relations.
Next, we establish sample consistency of the algorithm for linear SEMs with sub-Gaussian
errors.
Theorem 5 (Sample Guarantee of DNA Learning from large conditioning sets). Consider
the setup of Theorem 3, but assuming DNA-faithfulness with respect to

Ω↓K−1
λ
(P), ¯ΩK
λ (P)

.
Let
λ∗=
min
A⊆[p],|A|≥p−K,i,j∈A,[(Σ[A])−1]ij̸=0
[(Σ[A])−1]ij
 ,
where Σ[A] denotes the A×A sub-matrix of Σ. Denote the sample covariance matrix as Sn.
(i) Low-dimensional case. Assume the minimal eigenvalue of Σ is bounded below by
M > 0. If the sample size satisﬁes
n ≥{log(p2 + p) −log(ζ/2)}128(1 + 4σ2)2 max
i (Σii)2p2
M + 2/λ∗
M 2
2
,
(3)
then the output of Algorithm 3 at level K and with conditional independence tests that
hard-threshold the inverses of submatrices of Sn with respect to λ∗/2 is correct with high
probability.
(ii) High-dimensional case. Suppose there exists a sequence of λn satisfying λn ≤
λ∗∥Σ−1∥−1
1 /4 such that λn ≥∥Σ−1∥1∥Σ −Sn∥∞with high probability as n, p →∞. Then
the output of Algorithm 3 at level K using CLIME [3] with tuning parameter λn for condi-
tional independence tests is correct with high probability.
5
DNA Applications
5.1
Ordering constraints and Layering
A topological ordering of a DAG G, denoted π, is a total ordering of the vertices such that
π(u) < π(v) for each edge u →v in G. Due to acyclicity, there always exists at least one
such ordering, though the ordering might not be unique.
Without prior knowledge, learning DAGs from orderings requires checking all p! possible
orderings. However, DNA relations constrain the set of possible orderings. More speciﬁcally,
we will show in Lemma 3 that if u ̸⇝v, then there exists a valid topological ordering of G
with π(u) > π(v).
In general, we say an ordering π is compatible with a DNA set D if π(u) > π(v) for
each u ̸⇝v in D.
We say D is an order-constraining DNA set if D contains no DNA
statement cycles, i.e., we cannot follow a sequence of DNA statements in D, such as u ̸⇝
v, v ̸⇝w, . . . and get back to u. Given an arbitrary DNA set D, we can obtain an order-
constraining subset by removing statements until there is no cycle.
Speciﬁcally, if D is
output of Algorithm 1, then we only need to remove one from each pair of (u ̸⇝v, v ̸⇝u).
If D is an order-constraining DNA set, then there must be some topological ordering that
is compatible with both D and some member of the Markov equivalence class.
7

Lemma 3 (Ordering constraints). Let G be a DAG. If u ̸⇝v in G, then there exists a
topological ordering π that is compatible with some G′ ∈[G] and satisﬁes π(u) > π(v). If
D is an order-constraining DNA set for G, then there exists a topological ordering π that is
compatible with some G′ ∈[G] and satisﬁes π(u) > π(v) for all (u, v) ∈D.
Order-constraining DNA sets reduce the set of possible topological orderings to be con-
sidered in structure learning to a subset that is guaranteed to contain at least one correct
ordering. Therefore, this reduced set can be adopted in score-based structure learning meth-
ods such as Greedy Equivalence Search (GES) and Sparsest Permutation (SP) to trim down
their search space.
Moreover, order-constraining DNA sets yield layerings of DAGs. A layering of a DAG
is an ordered partition of the vertex set into layers, which must be such that there is no
arrow pointing from one layer to a preceding layer [13]. The ﬁnest layering we may hope
to infer from conditional independence tests is the one given by the chain components of
the CPDAG [2]. In addition to reducing the number of possible orderings, the layering of a
DAG can also be used to develop eﬃcient algorithms for learning DAGs. We present such
an approach in the next section, but ﬁrst show that layerings can be learned correctly from
DNA.
Algorithm 4: Learning layering from DNA
Input
: DNA set D ⊆D̸⇝(G).
Output: DAG layering L.
1 Sources ←Sinks ←∅; V ′ ←V ;
2 repeat
3
Find the smallest subset S ⊆V ′ such that (u, s) ∈D for all u ∈V ′ \ S, s ∈S, or
(s, v) ∈D for all v ∈V ′ \ S ,s ∈S;
4
In the former case, Sources ←[Sources, S]; in the latter case, Sinks ←[S, Sinks];
5
V ′ ←V ′ \ S;
6 until V ′ = ∅;
7 return L = [Sources, Sinks].
Theorem 6 (DAG Layering via DNA). Let G be a DAG and D ⊆D̸⇝(G). The output of
Algorithm 4 is a valid layering of G.
5.2
Sparsest Permutation with DNA
Let G be a DAG and let π be an arbitrary ordering of its vertices. We deﬁne Gπ = (V, Eπ)
as the DAG deduced via the following rule:
(π(i), π(j)) ∈Eπ iﬀi < j and
π(i), π(j) not d-separated by{π(1), . . . , π(j −1)} \ π(i) in G.
(4)
Clearly, Gπ = G when π is a valid topological ordering of G. We write Gπ(Ω) if we replace
the d-separations with conditional independences in Ω.
The Sparsest Permutation (SP) algorithm is a hybrid structure learning method: it
searches through the space of all orderings to minimize the edge count (which plays the role
of a score) of DAGs induced by the orderings via the constraint-based rule (4); that is, it
ﬁnds
π∗= arg min
π
|Gπ(Ω)|.
Under the Sparsest Markov Representation condition (SMR) — which requires that for any
G′ that is Markov to P, either |G′| > |G| or G′ is Markov equivalent to G — SP recovers
the correct Markov equivalence class of G [18, 20]. Since SMR is a necessary condition for
restricted-faithfulness [18], the correctness of the SP algorithm relies on weaker assumptions
than that of PC.
SP can be implemented as a greedy algorithm because starting from any arbitrary order-
ing, there always exists a non-increasing (in number of edges) sequence of DAGs that ends
up in the correct Markov equivalence class [20]. One particularly eﬃcient greedy approach
is the Triangle SP (TSP), which moves from one ordering to the other by reverting covered
arrows, that is, edges in the form of u →v satisfying pa(v) = pa(v) ∪{u}. By repeatedly
looking for covered arrow reversals that induce sparser DAGs, TSP will recover a DAG in
the target Markov equivalence class [20].
However, greedy search can be computationally burdensome. With an arbitrary initial-
ization of the ordering, it may need to traverse a long non-increasing sequence of DAGs to
reach the target. Moreover, since all members of a Markov equivalence class are connected
8

1
2
3
4
5
u
Figure 5: Algorithm 2 with level K = 0 discovers u being a sink. When accessing conditional
independencies among others, we can exclude u from the search.
via non-increasing sequences, greedy search may spend steps exploring the same equivalent
class before moving on to a sparser one.2
From this perspective, incorporating DNA information can be very useful. On the one
hand, an order-constraining DNA set reduces the search space and provides better initial
orderings; on the other hand, the layering information learned by Algorithm 4 breaks down
the learning problem into smaller sized problems that are easier to tackle.
Given a valid layering, the true DAGs can be learned by applying SP to the ﬁrst layer,
then adjust all the lower layers by the ﬁrst layer and repeat this process until the last layer.
We call this recursive approach the Layered-SP.
Algorithm 5: Layered-SP
Input
: Constraint-based algorithm A
Output: A topological ordering
1 D ←DNA relations learned by Algorithm 1 with A;
2 D′ ←an order-constraining subset of D obtained by dropping cycle-inducing DNA
relations;
3 L = (L1, . . . , Lm) ←layering deduced from D by Algorithm 4;
4 Run SP on L1 with initialization compatible with D′, and record the output
ordering as π1;
5 for l = 2, . . . , m do
6
Run SP on Ll adjusted for ∪l−1
i=1Li with initialization compatible with D′, and
record the output ordering as πl
7 π ←[π1, . . . , πm];
8 return π.
The next shows that our modiﬁed approach is correct.
Theorem 7 (Layered-SP). Let G be a DAG. Suppose observational data are drawn from a
distribution P that is Markov to G, DNA-faithful with respect to (ΩA(P), ¯ΩA(P)), and SMR
to G. Then the output of Algorithm 5, denoted π, satisﬁed Gπ ∈[G].
5.3
PC algorithm with DNA
As noted before, the PC algorithm learns a CPDAG from conditional independencies given
sets of increasing sizes. In the PC algorithm, we may leverage DNA information by ex-
cluding non-ancestral neighbors in conditional independence tests.
When accessing the
independence relation between u and v from the perspective of u in a working skeleton C,
instead of searching for d-separation sets among adj(C, u), we use the following rule:
• If u ̸⇝v, search adj(C, u) \ {w : w ̸⇝u}; otherwise, search adj(C, u) \ {w : w ̸⇝
v and w ̸⇝u}.
In the next lemma we show the correctness of the modiﬁcations, and in Figure 5 we
present a concrete example.
Lemma 4 (DNA and d-sep). The version of PC with neighborhood search replaced by the
above rule is correct.
It is worth noting that if u ̸⇝v and v ̸⇝u, then u and v are non-adjacent in the true
DAG. However, the orientation step of PC requires the d-separator of u, v, and hence we
cannot simply remove the edge u −v without searching and testing.
6
Numerical Results
We examine the performance of structure learning algorithms augmented by DNA. We
generate 200 random Erdős-Renyi DAGs with p = 10 and expected neighborhood size s
2To circumvent the problem of bad initialization and getting stuck in an equivalence class in practice,
Solus et al. [20] suggest implementing Greedy TSP with limited search depth d and restarting with diﬀerent
initialization for r times.
9

0.00
0.25
0.50
0.75
1.00
2
4
6
s
Recovery
6
8
10
2
4
6
s
log2(‘#Tests‘)
DNA
DNA0
DNA1
NoDNA
Algo
PC
SP
0.00
0.25
0.50
0.75
1.00
2
4
6
s
Recovery
7
9
11
2
4
6
s
log2(‘#Tests‘)
DNA
DNA0
DNA1
NoDNA
Algo
PC
SP
Figure 6: Recovery rate (Left) and Number of conditional independence tests (right) of the
population version of SP, PC and their DNA modiﬁcations for random ER graphs with
p = 10 nodes, expected neighborhood size s (x-axis), and λ = 0.01 (top row) and λ = 0.001
(bottom row).
from 2 to 7. For each DAG, we build a linear SEM with coeﬃcients drawn uniformly from
±[0.3, 1] and i.i.d. Gaussian error with variance drawn uniformly from [1, 2].
We ﬁrst compare the population versions of SP and PC with their DNA-modiﬁed ver-
sions. We plug in the conditional independence set Ωλ(P) = {(i, j, S) : |ρ(i, j|S)| ≤λ} with
λ = {10−2, 10−3} where ρ is the partial correlation. We report the number of conditional
independence tests performed by each method, as well as their recovery rate, which is the
proportion of times that the correct Markov equivalence class is recovered. A higher re-
covery rate under ﬁxed λ means the method is less demanding with respect to faithfulness
conditions, and is likely more statistically eﬃcient. The results are shown in Figure 6. The
DNA version of both SP and PC have higher recovery rate, showcasing the improvement on
statistical eﬃciency provided by our proposed algorithms. The number of conditional inde-
pendence tests also highlight the computational gains by augmenting SP and PC with DNA.
Notably, even with low learning levels (K = 0), the DNA modiﬁcations signiﬁcantly reduce
the total number of tests performed, especially when the graph is moderately sparse. Higher
learning level (K = 1) provides more improvement, though it can increase the number of
tests in some settings.
We also compare the sample versions of SP, PC and their DNA-modiﬁed versions. To
prevent false discoveries in the DNA Algorithm 2, we picked a large threshold for the d-
connection step (λ′ = 0.2). All other tests in SP and PC were performed at level λ = 0.02.
We draw 10000 samples from each SEM and use them to infer the CPDAG. We report
the recovery rate as well as the number of tests performed. As expected, DNA provides
improvement over both SP and PC when the underlying truth is moderately sparse, and
the improvement is more signiﬁcant for SP. On the other hand, when the underlying true
DAG is sparse, DNA could not provide much improvement, and false discoveries in DNA
may hinder the performance.
7
Conclusion
We introduced deﬁnite non-ancestral (DNA) relations as intermediate targets of inference
in structure learning. DNA relations can be learned from simple conditional independencies
and lead to computational and statistical gains in DAG structure learning. DNA applica-
tions in graphs with latent variables would be interesting area of future research.
8
Acknowledgements
This work has received funding from the U.S. National Institutes of Science (NSF) and of
Health (NIH) under grants DMS-1161565, DMS-1561814, and R01GM114029, and from the
10

0.00
0.25
0.50
0.75
1
2
3
4
5
s
Recovery
0
500
1000
1
2
3
4
5
s
#Tests
DNA
DNA0
DNA1
NoDNA
Algo
PC
SP
Figure 7: Recovery rate (Left) and Number of conditional independence tests (right) of the
sample version of SP, PC and their DNA modiﬁcations performed on n = 10000 samples
from random ER graphs with p = 10 nodes and expected neighborhood size s on x-axis.
European Research Council (ERC) under the European Union’s Horizon 2020 research and
innovation programme (grant agreement No 883818)
References
[1] Andersson, S. A., Madigan, D., and Perlman, M. D. (1997). A characterization of Markov
equivalence classes for acyclic digraphs. The Annals of Statistics, 25(2):505–541.
[2] Andersson, S. A. and Perlman, M. D. (2006). Characterizing markov equivalence classes
for AMP chain graph models. The Annals of Statistics, 34(2):939–972.
[3] Cai, T., Liu, W., and Luo, X. (2011).
A constrained ℓ-1 minimization approach to
sparse precision matrix estimation.
Journal of the American Statistical Association,
106(494):594–607.
[4] Claassen, T. and Heskes, T. (2011). A logical characterization of constraint-based causal
discovery. In Proceedings of the Twenty-Seventh Conference on Uncertainty in Artiﬁcial
Intelligence, UAI’11, page 135–144, Arlington, Virginia, USA. AUAI Press.
[5] Claassen, T., Mooij, J. M., and Heskes, T. (2013). Learning sparse causal models is
not NP-hard. In Proceedings of the Twenty-Ninth Conference on Uncertainty in Artiﬁcial
Intelligence, UAI’13, pages 172–181, Arlington, Virginia, United States. AUAI Press.
[6] Dor, D. and Tarsi, M. (1992). A simple algorithm to construct a consistent extension
of a partially oriented graph. Technicial Report R-185, Cognitive Systems Laboratory,
UCLA.
[7] Entner, D., Hoyer, P., and Spirtes, P. (2013). Data-driven covariate selection for non-
parametric estimation of causal eﬀects. In Carvalho, C. M. and Ravikumar, P., editors,
Proceedings of the Sixteenth International Conference on Artiﬁcial Intelligence and Statis-
tics, volume 31 of Proceedings of Machine Learning Research, pages 256–264, Scottsdale,
Arizona, USA. PMLR.
[8] Ghoshal, A. and Honorio, J. (2018). Learning linear structural equation models in poly-
nomial time and sample complexity. In International Conference on Artiﬁcial Intelligence
and Statistics, page 1466–1475. PMLR.
[9] Harris, N. and Drton, M. (2013). PC algorithm for nonparanormal graphical models. J.
Mach. Learn. Res., 14(1):3365–3383.
[10] Kalisch, M. and Bühlmann, P. (2007). Estimating high-dimensional directed acyclic
graphs with the PC-algorithm. J. Mach. Learn. Res., 8:613–636.
[11] Maathuis, M., Drton, M., Lauritzen, S., and Wainwright, M. (2019).
Handbook of
Graphical Models. Chapman & Hall/CRC Handbooks of Modern Statistical Methods.
CRC Press, Boca Raton, FL.
[12] Magliacane, S., Claassen, T., and Mooij, J. M. (2017).
Ancestral causal inference.
arXiv:1606.07035 [cs, stat]. arXiv: 1606.07035.
[13] Manzour, H., Küçükyavuz, S., Wu, H.-H., and Shojaie, A. (2021). Integer program-
ming for learning directed acyclic graphs from continuous data. INFORMS Journal on
Optimization, 3(1):46–73.
11

[14] Meek, C. (1995).
Causal inference and causal explanation with background knowl-
edge.
In Proceedings of Eleventh Conference on Uncertainty in Artiﬁcial Intelligence,
page 403–418. Morgan Kaufmann.
[15] Pearl, J. (2009). Causality. Cambridge University Press, Cambridge, second edition.
Models, reasoning, and inference.
[16] Perković, E., Textor, J., Kalisch, M., and Maathuis, M. H. (2018). Complete graphical
characterization and construction of adjustment sets in Markov equivalence classes of
ancestral graphs. Journal of Machine Learning Research, 18(220):1–62.
[17] Ramsey, J., Zhang, J., and Spirtes, P. L. (2006). Adjacency-faithfulness and conserva-
tive causal inference.
[18] Raskutti, G. and Uhler, C. (2013). Learning directed acyclic graphs based on sparsest
permutations.
[19] Ravikumar, P., Wainwright, M. J., Raskutti, G., and Yu, B. (2011). High-dimensional
covariance estimation by minimizing l1 -penalized log-determinant divergence. Electron.
J. Statist., 5:935–980.
[20] Solus, L., Wang, Y., and Uhler, C. (2020).
Consistency guarantees for greedy
permutation-based causal inference algorithms. arXiv preprint. arXiv: 1702.03530.
[21] Spirtes, P., Glymour, C., and Scheines, R. (2000). Causation, prediction, and search.
MIT Press, Cambridge, MA.
[22] Squires, C., Amaniampong, J., and Uhler, C. (2020). Eﬃcient permutation discovery
in causal DAGs. arXiv preprint. arXiv:2011.03610.
[23] Uhler, C., Raskutti, G., Bühlmann, P., and Yu, B. (2013). Geometry of the faithfulness
assumption in causal inference. Annals of Statistics, 41(2):436–463.
[24] Zhang, J. (2008). On the completeness of orientation rules for causal discovery in the
presence of latent confounders and selection bias. Artif. Intell., 172(16-17):1873–1896.
12

9
Appendix
9.1
Proofs
Proof of Lemma 1. We ﬁrst show the ⇒direction of the ﬁrst statement. Suppose u ⇝v and
u does not have a directed arrow into Auv. Suppose Auv is fully connected (or a singleton)
and deﬁne B = {a →u : a ∈Auv}. If the CPDAG has an extension that is consistent
with B, then u /∈an(v) in this extension; for a deﬁnition of extension, see e.g. [6]. We
apply the background knowledge algorithm [16], which is guaranteed to be successful if such
an extension exists. Since Auv is fully connected, at each step, Meek’s rules never orient
edges between nodes in Auv; therefore, also never orient any edge from u into Auv, and the
algorithm can enforce B without causing any conﬂict. But this means there exists a DAG
in the Markov equivalence class (MEC) in which u /∈an(v). Therefore we conclude the ⇒
direction of statement 1. The ⇐direction: since Auv is not a clique, there must be two
nodes a, a′ ∈Au,v that are non-adjacent and (a, u, a′) is not a v-structure. Then every DAG
in the MEC must have either u →a or u →a′, and therefore a directed path to v.
Now we show the second statement. The ⇒direction: if there exists a possibly directed
path from u to v, then there must be an unshielded possibly directed path. However, this
means the ﬁrst edge on this path is oriented out of u in some DAG in the MEC, in which
case u ∈an(v). The ⇐direction follows directly from our deﬁnition.
Proof of Lemma 2. In the ﬁrst statement, x must block some path π = (u, . . . , s, x, t, . . . , v)
that is d-connected given W. Therefore x is a non-collider on π, and the two subpaths
πxu = (x, s . . . , u) and πxv = (x, t, . . . , v) are both d-connected given W. In each DAG of
the MEC, we can pick πxu or πxv, whichever starts with an outgoing arrow (one of them
must do so, since x is not a collider) and follow non-collider arrows until we either reach
{u, v} or encounter a collider which is unblocked by W (meaning it is ancestral to W). In
the ﬁrst case x ∈an(u ∪v) and in the second case x ∈an(W).
In the second statement, x must unblock some path π = (u, . . . , t, . . . , v) that is blocked
by W, where t is a collider on π and ancestor of x (or t = x). Note that t (and also x)
is d-connected to both u and v given W. For this reason x must not be ancestral to W.
On the other hand, if x is ancestral to u, then there exists a directed path π′ = (x, . . . , u).
But then we obtain a d-connecting path between u, v given W by gluing together π′, the
directed path (x, . . . , t) and the subpath of π from t to v, which is a contradiction.
Proof of Theorem 1, 2 and 4. These three results are direct consequences of Lemma 2 and
DNA-faithfulness with respect to the corresponding conditional independence (CI) state-
ments checked by the learning steps.
Proof of Theorem 3. With the stated sample size, we may apply the error propagation com-
putation in Lemma 1 of [19] and in Lemma 6 of [9], which gives the following bound:
P

max
i̸=j,|S|≤K+1
ρ(i, j|S) −bρ(i, j|S)
 ≥λ/2

≤ζ.
(5)
Consequently, with probability at least 1 −ζ, the sets Ω↑K
λ (P) and ¯ΩK+1
λ
(P) are correctly
inferred from the data, and therefore the DNA output is correct.
Proof of Theorem 5. The low-dimensional result can be obtained from an error propagation
computation that is entirely analogous to the one from the proof of Theorem 3 above.
In this case, we have P

∥Σ−1 −S−1
n ∥∞> λ∗/2
	
≤ζ and consequently all moral graphs
are correctly inferred from data. In the high-dimensional case, the theory for the CLIME
estimator bΩguarantees that ∥bΩ−Σ−1∥∞≤4∥Σ−1∥1λn for λn ≥∥Σ−1∥1∥Σ −Sn∥∞[3].
The moral graphs of subgraphs are also correct [see Lemma 5 of 8]. In both cases, the
assumption on non-zero entries in the inverse covariance matrix guarantees that non-sinks
will not be misspeciﬁed as sink. Therefore the algorithm is correct by Theorem 4.
Proof of Lemma 3. To show the ﬁrst statement: Let π0 be an arbitrary ordering of G and
suppose u ̸⇝v is discordant with π0. We claim we can swap the ordering to obtain a new
ordering that is compatible with G and u ̸⇝v. We write π0 = (X, u, Y, v, Z). Denote
A = Y ∩an(v). Since u ̸⇝v, we also have u ̸⇝A. Since π0 is valid for G, there is no edge
between u and A, no edge between Y \ A and v, and all edges between A and Y \ A are in
the form of A →Y \A. So the new ordering π′
0 = (X, A, v, u, Y \A, Z) is valid for G. If D is
order-constraining, then applying the swap operation above does not create new discordant
pairs, and therefore an ordering can be swapped according to D until it agrees with both D
and [G].
13

Proof of Theorem 6. In the output L = (L1, . . . , Lm), for each k = 2, . . . , m, it holds that
v ̸⇝∪k−1
i=1 Li for all v ∈Lk. Consequently, all edges between Lk and layers preceding it must
be directed into Lk in G. Hence, L satisﬁes the requirements to be a valid layering of G.
Proof of Theorem 7. Under the Markov and the DNA-faithfulness assumption, D is a DNA
set of G and, thus, by Theorem 6, L is a layering of G. It is now suﬃcient to show the
DAG can be learned by recursively applying SP. We prove this claim by induction. Under
SMR, no graph on L1 sparser than the subgraph of G over L1 is Markov to the pattern
of conditional independence (CI) relations among L1. Therefore the output of SP on L1
is consistent with the target MEC. Moving on to L1 ∪L2, the sparsest graphs that are
Markov to the CI relations among L1 ∪L2 have L1 ordered exactly as π1.
Hence, the
optimal ordering of L2 can be in an optimization that holds π1 ﬁxed and considers the joint
distribution conditional on XL1. The general induction steps to the layers after L2 proceed
in the same way.
Proof of Lemma 4. We show if u, v are d-separated by S, then they are also d-separated
by S ∩(an(u) ∪an(v)). Let T = S \ (an(u) ∪an(v)). The d-separation relation implies
that every path between u and v either has a non-collider in S or a collider not in S whose
descendants are also not in S. Consider an arbitrary path π between u and v. If π does not
go through T, then π is blocked by S \ T. If π contains some z ∈T and z is a collider on π,
then π is blocked by S \ T for not including z. If z is not a collider on π, then we can follow
the arrows from z on π until we reach a collider, call it x, i.e., (z, . . . , x, . . .) is a subpath of
π. Since x ∈de(z) ⊂T and x is a collider on π that is not included in S \ T, the set S \ T
d-separates u, v.
9.2
DNA Learning with low learning levels
In this section we demonstrate that a large proportion of DNA relations can be learned by
Algorithm 2 with low learning levels. We generate 1000 random Erdős-Renyi or power-law
DAGs, and then run Algorithm 2 at learning level K = 0, 1 with the conditional indepen-
dence oracle. We then deduce a layering of G using Algorithm 4. We report the proportion
of all DNA learned and the proportion of edges in G that lie between the learned layers. A
high proportion of inter-layer edges means L is informative about G.
The results are shown in Figure 8. It is evident that a large proportion of DNA can be
learned by Algorithm 2 with very early stopping, even if the underlying graphs are large
or dense. For this reason we recommend running Algorithm 2 with small K = (0 or 1).
The results also show that the corresponding layering discovered by Algorithm 4 are most
informative when the true DAGs are not too dense nor too sparse.
This is due to the
characterization of Lemma 1 which relies on a conditional independence and a conditional
dependence: if too dense, there are not enough independences; and if too sparse, there are
not enough dependences.
14

0.00
0.25
0.50
0.75
1.00
2
4
6
s
Learnable%
0.00
0.25
0.50
0.75
1.00
2
4
6
s
Inter−layer%
Algo
CPDAG
DNA0
DNA1
0.00
0.25
0.50
0.75
1.00
2
3
4
5
6
s
Learnable%
0.00
0.25
0.50
0.75
1.00
2
3
4
5
6
s
Inter−layer%
Algo
CPDAG
DNA0
DNA1
Figure 8: Proportion of DNA learned (left column), and inter-layer edges (right column)
for Algorithm 2 with levels 0 and 1 in random Erdő-Renyi graphs (top row) and power-law
graphs (bottom row) with p = 10 vertices. The x-axis is average-node degree.
15

