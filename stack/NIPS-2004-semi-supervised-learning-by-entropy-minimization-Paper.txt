Semi-supervised Learning
by Entropy Minimization
Yves Grandvalet ∗
Heudiasyc, CNRS/UTC
60205 Compi`egne cedex, France
grandval@utc.fr
Yoshua Bengio
Dept. IRO, Universit´e de Montr´eal
Montreal, Qc, H3C 3J7, Canada
bengioy@iro.umontreal.ca
Abstract
We consider the semi-supervised learning problem, where a decision rule
is to be learned from labeled and unlabeled data. In this framework, we
motivate minimum entropy regularization, which enables to incorporate
unlabeled data in the standard supervised learning. Our approach in-
cludes other approaches to the semi-supervised problem as particular or
limiting cases. A series of experiments illustrates that the proposed solu-
tion beneﬁts from unlabeled data. The method challenges mixture mod-
els when the data are sampled from the distribution class spanned by the
generative model. The performances are deﬁnitely in favor of minimum
entropy regularization when generative models are misspeciﬁed, and the
weighting of unlabeled data provides robustness to the violation of the
“cluster assumption”. Finally, we also illustrate that the method can also
be far superior to manifold learning in high dimension spaces.
1
Introduction
In the classical supervised learning classiﬁcation framework, a decision rule is to be learned
from a learning set Ln = {xi, yi}n
i=1, where each example is described by a pattern xi ∈X
and by the supervisor’s response yi ∈Ω= {ω1, . . . , ωK}. We consider semi-supervised
learning, where the supervisor’s responses are limited to a subset of Ln.
In the terminology used here, semi-supervised learning refers to learning a decision rule on
X from labeled and unlabeled data. However, the related problem of transductive learning,
i.e. of predicting labels on a set of predeﬁned patterns, is addressed as a side issue. Semi-
supervised problems occur in many applications where labeling is performed by human
experts. They have been receiving much attention during the last few years, but some
important issues are unresolved [10].
In the probabilistic framework, semi-supervised learning can be modeled as a missing data
problem, which can be addressed by generative models such as mixture models thanks
to the EM algorithm and extensions thereof [6].Generative models apply to the joint den-
sity of patterns and class (X, Y ). They have appealing features, but they also have major
drawbacks. Their estimation is much more demanding than discriminative models, since
the model of P(X, Y ) is exhaustive, hence necessarily more complex than the model of
∗This work was supported in part by the IST Programme of the European Community, under the
PASCAL Network of Excellence IST-2002-506778. This publication only reﬂects the authors’ views.

P(Y |X). More parameters are to be estimated, resulting in more uncertainty in the es-
timation process. The generative model being more precise, it is also more likely to be
misspeciﬁed. Finally, the ﬁtness measure is not discriminative, so that better models are
not necessarily better predictors of class labels. These difﬁculties have lead to proposals
aiming at processing unlabeled data in the framework of supervised classiﬁcation [1, 5, 11].
Here, we propose an estimation principle applicable to any probabilistic classiﬁer, aiming
at making the most of unlabeled data when they are beneﬁcial, while providing a control
on their contribution to provide robustness to the learning scheme.
2
Derivation of the Criterion
2.1
Likelihood
We ﬁrst recall how the semi-supervised learning problem ﬁts into standard supervised
learning by using the maximum (conditional) likelihood estimation principle. The learning
set is denoted Ln = {xi, zi}n
i=1, where z ∈{0, 1}K denotes the dummy variable rep-
resenting the actually available labels (while y represents the precise and complete class
information): if xi is labeled ωk, then zik = 1 and ziℓ= 0 for ℓ̸= k; if xi is unlabeled,
then ziℓ= 1 for ℓ= 1, . . . , K.
We assume that labeling is missing at random, that is, for all unlabeled examples,
P(z|x, ωk) = P(z|x, ωℓ), for any (ωk, ωℓ) pair, which implies
P(ωk|x, z) =
zkP(ωk|x)
PK
ℓ=1 zℓP(ωℓ|x)
.
(1)
Assuming independent examples, the conditional log-likelihood of (Z|X) on the observed
sample is then
L(θ; Ln) =
n
X
i=1
log
 K
X
k=1
zikfk(xi; θ)
!
+ h(zi) ,
(2)
where h(z), which does not depend on P(X, Y ), is only affected by the missingness mech-
anism, and fk(x; θ) is the model of P(ωk|x) parameterized by θ.
This criterion is a concave function of fk(xi; θ), and for simple models such as the ones
provided by logistic regression, it is also concave in θ, so that the global solution can
be obtained by numerical optimization. Maximizing (2) corresponds to maximizing the
complete likelihood if no assumption whatsoever is made on P(X) [6].
Provided fk(xi; θ) sum to one, the likelihood is not affected by unlabeled data: unlabeled
data convey no information. In the maximum a posteriori (MAP) framework, Seeger re-
marks that unlabeled data are useless regarding discrimination when the priors on P(X)
and P(Y |X) factorize [10]: observing x does not inform about y, unless the modeler
assumes so. Beneﬁtting from unlabeled data requires assumptions of some sort on the re-
lationship between X and Y . In the Bayesian framework, this will be encoded by a prior
distribution. As there is no such thing like a universally relevant prior, we should look for
an induction bias exploiting unlabeled data when the latter is known to convey information.
2.2
When Are Unlabeled Examples Informative?
Theory provides little support to the numerous experimental evidences [5, 7, 8] showing
that unlabeled examples can help the learning process. Learning theory is mostly developed
at the two extremes of the statistical paradigm: in parametric statistics where examples are
known to be generated from a known class of distribution, and in the distribution-free Struc-
tural Risk Minimization (SRM) or Probably Approximately Correct (PAC) frameworks.
Semi-supervised learning, in the terminology used here, does not ﬁt the distribution-free
frameworks: no positive statement can be made without distributional assumptions, as for

some distributions P(X, Y ) unlabeled data are non-informative while supervised learning
is an easy task. In this regard, generalizing from labeled and unlabeled data may differ
from transductive inference.
In parametric statistics, theory has shown the beneﬁt of unlabeled examples, either for spe-
ciﬁc distributions [9], or for mixtures of the form P(x) = pP(x|ω1) + (1 −p)P(x|ω2)
where the estimation problem is essentially reduced to the one of estimating the mixture
parameter p [4]. These studies conclude that the (asymptotic) information content of un-
labeled examples decreases as classes overlap.1 Thus, the assumption that classes are well
separated is sensible if we expect to take advantage of unlabeled examples.
The conditional entropy H(Y |X) is a measure of class overlap, which is invariant to the
parameterization of the model. This measure is related to the usefulness of unlabeled data
where labeling is indeed ambiguous. Hence, we will measure the conditional entropy of
class labels conditioned on the observed variables
H(Y |X, Z) = −EXY Z[log P(Y |X, Z)] ,
(3)
where EX denotes the expectation with respect to X.
In the Bayesian framework, assumptions are encoded by means of a prior on the model
parameters. Stating that we expect a high conditional entropy does not uniquely deﬁne the
form of the prior distribution, but the latter can be derived by resorting to the maximum
entropy principle.2 Let (θ, ψ) denote the model parameters of P(X, Y, Z); the maximum
entropy prior verifying EΘΨ[H(Y |X, Z)] = c, where the constant c quantiﬁes how small
the entropy should be on average, takes the form
P(θ, ψ) ∝exp (−λH(Y |X, Z))) ,
(4)
where λ is the positive Lagrange multiplier corresponding to the constant c.
Computing H(Y |X, Z) requires a model of P(X, Y, Z) whereas the choice of the diagno-
sis paradigm is motivated by the possibility to limit modeling to conditional probabilities.
We circumvent the need of additional modeling by applying the plug-in principle, which
consists in replacing the expectation with respect to (X, Z) by the sample average. This
substitution, which can be interpreted as “modeling” P(X, Z) by its empirical distribution,
yields
Hemp(Y |X, Z; Ln) = −1
n
n
X
i=1
K
X
k=1
P(ωk|xi, zi) log P(ωk|xi, zi) .
(5)
This empirical functional is plugged in (4) to deﬁne an empirical prior on parameters θ,
that is, a prior whose form is partly deﬁned from data [2].
2.3
Entropy Regularization
Recalling that fk(x; θ) denotes the model of P(ωk|x), the model of P(ωk|x, z) (1) is
deﬁned as follows:
gk(x, z; θ) =
zkfk(x; θ)
PK
ℓ=1 zℓfℓ(x; θ)
.
For labeled data, gk(x, z; θ) = zk, and for unlabeled data, gk(x, z; θ) = fk(x; θ).
From now on, we drop the reference to parameter θ in fk and gk to lighten notation. The
1This statement, given explicitly by [9], is also formalized, though not stressed, by [4], where
the Fisher information for unlabeled examples at the estimate ˆp is clearly a measure of the overlap
between class conditional densities: Iu(ˆp) =
R
(P (x|ω1)−P (x|ω2))2
ˆpP (x|ω1)+(1−ˆp)P (x|ω2) dx.
2Here, maximum entropy refers to the construction principle which enables to derive distributions
from constraints, not to the content of priors regarding entropy.

MAP estimate is the maximizer of the posterior distribution, that is, the maximizer of
C(θ, λ; Ln)
=
L(θ; Ln) −λHemp(Y |X, Z; Ln)
=
n
X
i=1
log
 K
X
k=1
zikfk(xi)
!
+ λ
n
X
i=1
K
X
k=1
gk(xi, zi) log gk(xi, zi) , (6)
where the constant terms in the log-likelihood (2) and log-prior (4) have been dropped.
While L(θ; Ln) is only sensitive to labeled data, Hemp(Y |X, Z; Ln) is only affected by
the value of fk(x) on unlabeled data.
Note that the approximation Hemp (5) of H (3) breaks down for wiggly functions fk(·) with
abrupt changes between data points (where P(X) is bounded from below). As a result, it is
important to constrain fk(·) in order to enforce the closeness of the two functionals. In the
following experimental section, we imposed a smoothness constraint on fk(·) by adding to
the criterion C (6) a penalizer with its corresponding Lagrange multiplier ν.
3
Related Work
Self-Training
Self-training [7] is an iterative process, where a learner imputes the labels
of examples which have been classiﬁed with conﬁdence in the previous step. Amini et al.
[1] analyzed this technique and shown that it is equivalent to a version of the classiﬁcation
EM algorithm, which minimizes the likelihood deprived of the entropy of the partition. In
the context of conditional likelihood with labeled and unlabeled examples, the criterion is
n
X
i=1
log
 K
X
k=1
zikfk(xi)
!
+
K
X
k=1
gk(xi) log gk(xi) ,
which is recognized as an instance of the criterion (6) with λ = 1.
Self-conﬁdent logistic regression [5] is another algorithm optimizing the criterion for λ =
1. Using smaller λ values is expected to have two beneﬁts: ﬁrst, the inﬂuence of unlabeled
examples can be controlled, in the spirit of the EM-λ [8], and second, slowly increasing
λ deﬁnes a scheme similar to deterministic annealing, which should help the optimization
process to avoid poor local minima of the criterion.
Minimum entropy methods
Minimum entropy regularizers have been used in other con-
texts to encode learnability priors (e.g. [3]). In a sense, Hemp can be seen as a poor’s man
way to generalize this approach to continuous input spaces. This empirical functional was
also used by Zhu et al. [13, Section 6] as a criterion to learn weight function parameters in
the context of transduction on manifolds for learning.
Input-Dependent Regularization
Our criterion differs from input-dependent regular-
ization [10, 11] in that it is expressed only in terms of P(Y |X, Z) and does not involve
P(X). However, we stress that for unlabeled data, the regularizer agrees with the complete
likelihood provided P(X) is small near the decision surface. Indeed, whereas a genera-
tive model would maximize log P(X) on the unlabeled data, our criterion minimizes the
conditional entropy on the same points. In addition, when the model is regularized (e.g.
with weight decay), the conditional entropy is prevented from being too small close to the
decision surface. This will favor putting the decision surface in a low density area.
4
Experiments
4.1
Artiﬁcial Data
In this section, we chose a simple experimental setup in order to avoid artifacts stemming
from optimization problems. Our goal is to check to what extent supervised learning can
be improved by unlabeled examples, and if minimum entropy can compete with generative
models which are usually advocated in this framework.

The minimum entropy regularizer is applied to the logistic regression model. It is compared
to logistic regression ﬁtted by maximum likelihood (ignoring unlabeled data) and logistic
regression with all labels known. The former shows what has been gained by handling
unlabeled data, and the latter provides the “crystal ball” performance obtained by guessing
correctly all labels. All hyper-parameters (weight-decay for all logistic regression models
plus the λ parameter (6) for minimum entropy) are tuned by ten-fold cross-validation.
Minimum entropy logistic regression is also compared to the classic EM algorithm for
Gaussian mixture models (two means and one common covariance matrix estimated by
maximum likelihood on labeled and unlabeled examples, see e.g. [6]). Bad local maxima
of the likelihood function are avoided by initializing EM with the parameters of the true
distribution when the latter is a Gaussian mixture, or with maximum likelihood parameters
on the (fully labeled) test sample when the distribution departs from the model. This ini-
tialization advantages EM, since it is guaranteed to pick, among all local maxima of the
likelihood, the one which is in the basin of attraction of the optimal value. Furthermore,
this initialization prevents interferences that may result from the “pseudo-labels” given to
unlabeled examples at the ﬁrst E-step. In particular, “label switching” (i.e. badly labeled
clusters) is avoided at this stage.
Correct joint density model
In the ﬁrst series of experiments, we consider two-class
problems in an 50-dimensional input space. Each class is generated with equal probability
from a normal distribution. Class ω1 is normal with mean (aa . . . a) and unit covariance
matrix. Class ω2 is normal with mean −(aa . . . a) and unit covariance matrix. Parameter
a tunes the Bayes error which varies from 1 % to 20 % (1 %, 2.5 %, 5 %, 10 %, 20 %).
The learning sets comprise nl labeled examples, (nl = 50, 100, 200) and nu unlabeled
examples, (nu = nl × (1, 3, 10, 30, 100)). Overall, 75 different setups are evaluated, and
for each one, 10 different training samples are generated. Generalization performances are
estimated on a test set of size 10 000.
This benchmark provides a comparison for the algorithms in a situation where unlabeled
data are known to convey information. Besides the favorable initialization of the EM al-
gorithm to the optimal parameters, EM beneﬁts from the correctness of the model: data
were generated according to the model, that is, two Gaussian subpopulations with identical
covariances. The logistic regression model is only compatible with the joint distribution,
which is a weaker fulﬁllment than correctness.
As there is no modeling bias, differences in error rates are only due to differences in estima-
tion efﬁciency. The overall error rates (averaged over all settings) are in favor of minimum
entropy logistic regression (14.1 ± 0.3 %). EM (15.6 ± 0.3 %) does worse on average than
logistic regression (14.9 ± 0.3 %). For reference, the average Bayes error rate is 7.7 % and
logistic regression reaches 10.4 ± 0.1 % when all examples are labeled.
Figure 1 provides more informative summaries than these raw numbers. The plots repre-
sent the error rates (averaged over nl) versus Bayes error rate and the nu/nl ratio. The
ﬁrst plot shows that, as asymptotic theory suggests [4, 9], unlabeled examples are mostly
informative when the Bayes error is low. This observation validates the relevance of the
minimum entropy assumption. This graph also illustrates the consequence of the demand-
ing parametrization of generative models. Mixture models are outperformed by the simple
logistic regression model when the sample size is low, since their number of parameters
grows quadratically (vs. linearly) with the number of input features.
The second plot shows that the minimum entropy model takes quickly advantage of un-
labeled data when classes are well separated. With nu = 3nl, the model considerably
improves upon the one discarding unlabeled data. At this stage, the generative models do
not perform well, as the number of available examples is low compared to the number of
parameters in the model. However, for very large sample sizes, with 100 times more unla-

5
10
15
20
10
20
30
40
Bayes Error (%)
Test Error (%)
  1
  3
 10
 30
100
5
10
15
Ratio nu/nl
Test Error (%)
Figure 1: Left: test error vs. Bayes error rate for nu/nl = 10; right: test error vs. nu/nl
ratio for 5 % Bayes error (a = 0.23). Test errors of minimum entropy logistic regression (◦)
and mixture models (+). The errors of logistic regression (dashed), and logistic regression
with all labels known (dash-dotted) are shown for reference.
beled examples than labeled examples, the generative approach eventually becomes more
accurate than the diagnosis approach.
Misspeciﬁed joint density model
In a second series of experiments, the setup is slightly
modiﬁed by letting the class-conditional densities be corrupted by outliers. For each class,
the examples are generated from a mixture of two Gaussians centered on the same mean:
a unit variance component gathers 98 % of examples, while the remaining 2 % are gener-
ated from a large variance component, where each variable has a standard deviation of 10.
The mixture model used by EM is slightly misspeciﬁed since it is a simple Gaussian mix-
ture. The results, displayed in the left-hand-side of Figure 2, should be compared with the
right-hand-side of Figure 1. The generative model dramatically suffers from the misspec-
iﬁcation and behaves worse than logistic regression for all sample sizes. The unlabeled
examples have ﬁrst a beneﬁcial effect on test error, then have a detrimental effect when
they overwhelm the number of labeled examples. On the other hand, the diagnosis models
behave smoothly as in the previous case, and the minimum entropy criterion performance
improves.
  1
  3
 10
 30
100
5
10
15
20
Ratio nu/nl
Test Error (%)
  1
  3
 10
 30
100
0
5
10
15
20
25
30
Ratio nu/nl
Test Error (%)
Figure 2: Test error vs. nu/nl ratio for a = 0.23. Average test errors for minimum entropy
logistic regression (◦) and mixture models (+). The test error rates of logistic regression
(dotted), and logistic regression with all labels known (dash-dotted) are shown for refer-
ence. Left: experiment with outliers; right: experiment with uninformative unlabeled data.
The last series of experiments illustrate the robustness with respect to the cluster assump-
tion, by testing it on distributions where unlabeled examples are not informative, and where
a low density P(X) does not indicate a boundary region. The data is drawn from two Gaus-
sian clusters like in the ﬁrst series of experiment, but the label is now independent of the
clustering: an example x belongs to class ω1 if x2 > x1 and belongs to class ω2 otherwise:

the Bayes decision boundary is now separates each cluster in its middle. The mixture model
is unchanged. It is now far from the model used to generate data. The right-hand-side plot
of Figure 1 shows that the favorable initialization of EM does not prevent the model to be
fooled by unlabeled data: its test error steadily increases with the amount of unlabeled data.
On the other hand, the diagnosis models behave well, and the minimum entropy algorithm
is not distracted by the two clusters; its performance is nearly identical to the one of train-
ing with labeled data only (cross-validation provides λ values close to zero), which can be
regarded as the ultimate performance in this situation.
Comparison with manifold transduction
Although our primary goal is to infer a deci-
sion function, we also provide comparisons with a transduction algorithm of the “manifold
family”. We chose the consistency method of Zhou et al. [12] for its simplicity. As sug-
gested by the authors, we set α = 0.99 and the scale parameter σ2 was optimized on test
results [12]. The results are reported in Table 1. The experiments are limited due to the
memory requirements of the consistency method in our naive MATLAB implementation.
Table 1: Error rates (%) of minimum entropy (ME) vs. consistency method (CM), for
a = 0.23, nl = 50, and a) pure Gaussian clusters b) Gaussian clusters corrupted by outliers
c) class boundary separating one Gaussian cluster
nu
50
150
500
1500
a) ME
10.8 ± 1.5
9.8 ± 1.9
8.8 ± 2.0
8.3 ± 2.6
a) CM
21.4 ± 7.2
25.5 ± 8.1
29.6 ± 9.0
26.8 ± 7.2
b) ME
8.5 ± 0.9
8.3 ± 1.5
7.5 ± 1.5
6.6 ± 1.5
b) CM
22.0 ± 6.7
25.6 ± 7.4
29.8 ± 9.7
27.7 ± 6.8
c) ME
8.7 ± 0.8
8.3 ± 1.1
7.2 ± 1.0
7.2 ± 1.7
c) CM
51.6 ± 7.9
50.5 ± 4.0
49.3 ± 2.6
50.2 ± 2.2
The results are extremely poor for the consistency method, whose error is way above min-
imum entropy, and which does not show any sign of improvement as the sample of unla-
beled data grows. Furthermore, when classes do not correspond to clusters, the consistency
method performs random class assignments. In fact, our setup, which was designed for
the comparison of global classiﬁers, is extremely defavorable to manifold methods, since
the data is truly 50-dimensional. In this situation, local methods suffer from the “curse
of dimensionality”, and many more unlabeled examples would be required to get sensible
results. Hence, these results mainly illustrate that manifold learning is not the best choice
in semi-supervised learning for truly high dimensional data.
4.2
Facial Expression Recognition
We now consider an image recognition problem, consisting in recognizing seven (balanced)
classes corresponding to the universal emotions (anger, fear, disgust, joy, sadness, surprise
and neutral). The patterns are gray level images of frontal faces, with standardized posi-
tions. The data set comprises 375 such pictures made of 140 × 100 pixels.
We tested kernelized logistic regression (Gaussian kernel), its minimum entropy version,
nearest neigbor and the consistency method. We repeatedly (10 times) sampled 1/10 of
the dataset for providing the labeled part, and the remainder for testing. Although (α, σ2)
were chosen to minimize the test error, the consistency method performed poorly with
63.8±1.3 % test error (compared to 86 % error for random assignments). Nearest-neighbor
get similar results with 63.1±1.3 % test error, and Kernelized logistic regression (ignoring
unlabeled examples) improved to reach 53.6±1.3 %. Minimum entropy kernelized logistic
regression regression achieves 52.0 ± 1.9 % error (compared to about 20 % errors for
human on this database). The scale parameter chosen for kernelized logistic regression
(by ten-fold cross-validation) amount to use a global classiﬁer. Again, the local methods

fail. This may be explained by the fact that the database contains several pictures of each
person, with different facial expressions. Hence, local methods are likely to pick the same
identity instead of the same expression, while global methods are able to learn the relevant
directions.
5
Discussion
We propose to tackle the semi-supervised learning problem in the supervised learning
framework by using the minimum entropy regularizer. This regularizer is motivated by the-
ory, which shows that unlabeled examples are mostly beneﬁcial when classes have small
overlap. The MAP framework provides a means to control the weight of unlabeled exam-
ples, and thus to depart from optimism when unlabeled data tend to harm classiﬁcation.
Our proposal encompasses self-learning as a particular case, as minimizing entropy in-
creases the conﬁdence of the classiﬁer output. It also approaches the solution of transduc-
tive large margin classiﬁers in another limiting case, as minimizing entropy is a means to
drive the decision boundary from learning examples.
The minimum entropy regularizer can be applied to both local and global classiﬁers. As a
result, it can improve over manifold learning when the dimensionality of data is effectively
high, that is, when data do not lie on a low-dimensional manifold. Also, our experiments
suggest that the minimum entropy regularization may be a serious contender to generative
models. It compares favorably to these mixture models in three situations: for small sample
sizes, where the generative model cannot completely beneﬁt from the knowledge of the
correct joint model; when the joint distribution is (even slightly) misspeciﬁed; when the
unlabeled examples turn out to be non-informative regarding class probabilities.
References
[1] M. R. Amini and P. Gallinari. Semi-supervised logistic regression. In 15th European Confer-
ence on Artiﬁcial Intelligence, pages 390–394. IOS Press, 2002.
[2] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New York, 2 edition,
1985.
[3] M. Brand. Structure learning in conditional probability models via an entropic prior and param-
eter extinction. Neural Computation, 11(5):1155–1182, 1999.
[4] V. Castelli and T. M. Cover.
The relative value of labeled and unlabeled samples in pat-
tern recognition with an unknown mixing parameter.
IEEE Trans. on Information Theory,
42(6):2102–2117, 1996.
[5] Y. Grandvalet. Logistic regression for partial labels. In 9th Information Processing and Man-
agement of Uncertainty in Knowledge-based Systems – IPMU’02, pages 1935–1941, 2002.
[6] G. J. McLachlan. Discriminant analysis and statistical pattern recognition. Wiley, 1992.
[7] K. Nigam and R. Ghani. Analyzing the effectiveness and applicability of co-training. In Ninth
International Conference on Information and Knowledge Management, pages 86–93, 2000.
[8] K. Nigam, A. K. McCallum, S. Thrun, and T. Mitchell. Text classiﬁcation from labeled and
unlabeled documents using EM. Machine learning, 39(2/3):135–167, 2000.
[9] T. J. O’Neill. Normal discrimination with unclassiﬁed observations. Journal of the American
Statistical Association, 73(364):821–826, 1978.
[10] M. Seeger. Learning with labeled and unlabeled data. Technical report, Institute for Adaptive
and Neural Computation, University of Edinburgh, 2002.
[11] M. Szummer and T. S. Jaakkola. Information regularization with partially labeled data. In
Advances in Neural Information Processing Systems 15. MIT Press, 2003.
[12] D. Zhou, O. Bousquet, T. Navin Lal, J. Weston, and B. Sch¨olkopf. Learning with local and
global consistency. In Advances in Neural Information Processing Systems 16, 2004.
[13] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and
harmonic functions. In 20th Int. Conf. on Machine Learning, pages 912–919, 2003.

