SPARSE CODING AND AN APPLICATION
TO TOPIC MODELS
By Ryan Wang∗
Sparse coding represents data as sparse linear combinations of
basis vectors from a learned dictionary. Algorithms for sparse coding
generally alternate between a coding step, which involves a regular-
ized least squares problem, and a dictionary learning step, which in-
volves a matrix factorization problem. In this paper we review some
recent theoretical work on the performance of procedures used for
solving these problems, such as the LASSO. We then present an appli-
cation to topic modeling, a set of techniques for analyzing collections
of text documents, and discuss some possible extensions that incor-
porate structured sparsity. Finally, we discuss experimental results
from applying a sparse coding topic model to a collection of papers
from the Neural Information Processing Systems (NIPS) conference.
1. Introduction.
Sparse coding provides a set of methods for repre-
senting data in terms of a set of basis vectors called a dictionary. In many
cases it is useful to have sparse representations in terms of the dictionary,
so that only a few elements from the dictionary make up any one represen-
tation. For example, sparse representations have been useful in applications
to high-dimensional data and signal processing. In sparse coding the dictio-
nary is simultaneously learned, in contrast to methods that use predeﬁned
dictionaries such as wavelet bases. In this paper we review some recent work
on aspects of sparse coding and present an application to topic modeling, a
widely used procedure for text analysis.
The general setup is as follows. A response vector yi ∈Rm is represented
as a linear combination of basis vectors b1, ..., bp ∈Rm with code vector
θi = (θ1, ..., θn)T ∈Rp, so that ˆyi = Pp
j=1 θijbj. The problem is then to
minimize reconstruction error with some kind of sparsity inducing penalty.
For observations y1, ..., yn:
(1.1)
min{bj}p
j=1,{θi}n
i=1
1
2
n
X
i=1
||yi −
p
X
j=1
θijbj||2
2 + ω
n
X
i=1
φ(θi)
where the function φ(·) induces sparsity in the coeﬃcient vectors. The ℓ0
pseudo-norm,
||θi||0 ≜#{j s.t. θij ̸= 0}
∗rywang@galton.uchicago.edu
1

2
and its convex relaxation the ℓ1 norm,
||θi||1 ≜
p
X
j=1
|θij|
are most common. We will focus on the ℓ1 norm since it admits widely used
procedures for convex optimization.
The optimization problem in (1.1) also has a matrix representation, which
can be useful for applying matrix factorization methods. Let Y ∈Rm×n be
the matrix of stacked observation vectors, B ∈Rm×p the matrix of stacked
basis vectors, and Θ ∈Rp×n the matrix of stacked codes. Then the problem
becomes:
(1.2)
minB,Θ
1
2||Y −BΘ||2
F + ω
n
X
j=1
φ(Θ·j)
where || · ||F denotes the Frobenius norm for matrices. It is typical to con-
strain the size of the columns of B in order to prevent the codes from be-
coming arbitrarily small. This constraint set often takes the form
C = {B ∈Rm×p : bT
j bj ≤1 for all j = 1, ..., p}.
In general (1.2) does not yield a jointly convex problem, even under an ℓ1
penalty. However, it is convex with respect to B when Θ is ﬁxed, and vice
versa, under an ℓ1 penalty. Thus, the classical approach is to alternately
optimize with respect to each variable holding the other ﬁxed. There has
not been much theoretical study on the properties of this procedure, though
we will brieﬂy discuss some results for each component.
The rest of the paper is organized as follows. Section 2 reviews some rel-
evant theoretical work on sparse estimation procedures. Section 3 describes
an original application of sparse coding to topic modeling. Section 4 presents
numerical experiments. Section 5 discusses some possible extensions.
2. Some Theoretical Results.
Results on the performance of sparse
estimation procedures often take the form of oracle inequalities. An oracle
inequality provides guarantees on the statistical performance of an estimator
in terms of its risk relative to an oracle estimator. The oracle represents
the best model within a given family of models and can be viewed as the
inﬁnite data estimator. Thus if an estimator has risk close to the oracle risk,
then the estimator performs well up to the error incurred from specifying
a particular family of models. We ﬁrst discuss oracle inequalities for an ℓ1-
penalized sparse density estimator. In the case of regression, an equivalent

3
estimator for solving the ℓ1-penalized least squares problem is known as
the least absolute shrinkage and selection operator (LASSO). This work
is relevant because the coding step directly corresponds to solving such a
problem.
2.1. SPADES.
Consider the following density estimation setup from [4].
Let X1, ..., Xn ∈Rm be independent and identically distributed with density
f. Let {f1, ..., fp} be a given ﬁnite set of functions with fj ∈L2(Rm) called
a dictionary. The unknown density f is modeled by linear combinations of
elements of the dictionary
(2.1)
fθ(x) =
p
X
j=1
θjfj(x)
with coeﬃcient vector θ = (θ1, ..., θp) ∈Rp. To induce sparsity in the coeﬃ-
cients, an ℓ1-penalized criterion is minimized.
The SPADES (SPArse Density EStimation) estimator minimizes an em-
pirical counterpart of the squared loss. Noting that the inner product of
f, g ∈L2(Rm) corresponds to an expectation under f, the squared loss be-
comes
(2.2) ||f −fθ||2 = ||f||2 +||fθ||2 −2 < f, fθ >= ||f||2 +||fθ||2 −2E (fθ(X)) .
Minimizing (2.2) with respect to θ corresponds to minimizing
(2.3)
γ(θ) = −2E (fθ(X)) + ||fθ||2.
Replacing the expectation by its empirical counterpart, 1
n
Pn
i=1 fθ(Xi), and
adding an ℓ1-penalty, φ(θ) = ω Pp
j=1 |θj|, the SPADES estimator is then
deﬁned by
ˆfSPADES = fˆθ
where
(2.4)
ˆθ = argmin
θ∈Rp


−2
n
n
X
i=1
fθ(Xi) + ||fθ||2 + ω
p
X
j=1
|θj|


.
2.1.1. General Oracle Inequality.
Deﬁne the noise level by
r(δ) = r(p, n, δ) =
r
log(p/δ)
n

4
where 0 < δ < 1 is speciﬁed. Finally, deﬁne the intra-dictionary correlations
ρ(i, j) = < fi, fj >
||fi||||fj||
for i, j = 1, ..., p. Under a condition on the “maximal local coherence”
ρ(θ) = max
i∈I(θ)max
j̸=i |ρ(i, j)|,
namely that θ ∈Rp satisfy 16Cρ(θ) ≤1, Bunea et al (2010) show that for a
simple choice of regularization parameter
(2.5)
ω ∝r(δ/2)
the following inequality holds with probability at least 1 −δ
(2.6)
||fˆθ −f||2 + C2ω
p
X
j=1
|ˆθj −θj| ≤C3||fθ −f||2 + C4r2(δ/2)g(θ)
where the C’s are constants. This result suggests that when the mixture
representation is a good approximation of the function, that is the approx-
imation error ||fθ −f||2 is small, then simultaneously the SPADES coeﬃ-
cients will be close to the oracle coeﬃcients and the SPADES estimator will
be close to the true density. A similar result holds under a condition on the
“cumulative local coherence” deﬁned by
ρ(θ) =
X
i∈I(θ)
X
j>i
|ρM(i, j)|.
The proofs rely on an application of Bernstein’s inequality, which describes
the concentration of random variables around their mean.
For the purposes of sparse coding, the coherence conditions suggests that
we should consider the correlations between dictionary elements if we are
interested in predictive performance. In particular we do not want the ele-
ments to be highly correlated. This makes intuitive sense as there would be a
problem of identiﬁcation in such a case. The cumulative condition is weaker
than the maximal condition in that it admits cases where the correlations
may be relatively large for only a few pairs of dictionary elements. Note also
that the oracle inequality is non-asymptotic and does not depend on M or
n.

5
2.1.2. Mixture Models.
In the mixture modeling context, we assume that
f = fθ∗has a true mixture representation that is sparse. Denote by I∗⊆
{1, .., p} the set of indices corresponding to the non-zero components of θ∗.
The goal is then to ﬁnd a good estimate ˆθ of θ∗and identify I∗with high
probability, formally p(Iˆθ = I∗) →1 as n →∞. This is often referred to as
model selection consistency, although the result from [4] is non-asymptotic,
as before.
In this setting the coherence condition can be restated as
(2.7)
ρ∗≤
1
16k∗
where ρ∗= ρ(θ∗) and k∗= |I∗|. Intuitively, the condition implies that
stricter conditions on the correlations are required as the number of true
non-zero coeﬃcients, k∗, increases. Put another way, the model selection
problem becomes more diﬃcult when the true coeﬃcient vector is less sparse.
A second condition for correct model selection is
(2.8)
min
j∈I∗|θ∗
j| > 4C
r
log(2p2/δ)
n
where the p2 term replaces the p term from above because the regularization
required for model selection is larger than for good prediction. Intuitively,
this condition implies that mixture weights must be above the noise level
given by the square root term. Under these conditions, the result states
(2.9)
p(Iˆθ = I∗) ≥1 −2δ(1 + 1
p).
Note that the true coeﬃcient vector and its cardinality are unknown so that
these conditions are generally untestable in practice. The proof of this result
also relies on the application of a concentration inequality.
2.2. Generalization Error of Dictionary Learning.
Some theoretical work
has studied the generalization error of dictionary learning. We brieﬂy discuss
some results from [10] who derive generalization bounds for both ℓ1 and ℓ0
penalization for arbitrary loss functions. Denote the approximation error of
y ∈Rm by
hA,B(y) = min
θ∈A ||y −Bθ||.
We focus on the case where A deﬁnes an ℓ1-constraint
A = {θ ∈Rp : ||θ||1 ≤ω}.

6
Note that this representation is equivalent to the one where the ℓ1-penalty
appears directly in the loss function to be minimized. In general the dictio-
nary learning problem can be seen as minimizing
(2.10)
E(B) = E [hA,B(y)]
where the expectation is taken with respect to the unknown density y ∼f.
If a dictionary is learned from n observations y1, ..., yn, then the goal is to
bound the generalization error, ϵ, that arises from learning a dictionary with
a ﬁnite sample
(2.11)
E(B) ≤(1 + η)En(B) + ϵ
where η ≥0. Using a covering number argument for hA,B, a bound for ϵ
on the order of O(
p
mplog(nω)/n) is derived. The result implies that the
generalization error shrinks with the sample size and has only a logarithmic
dependence on the ℓ1-norm of the coeﬃcient vector.
3. Application to Topic Modeling.
Topic modeling is a technique
for text analysis that reduces a large collection of documents into groups of
words with relatively distinct meanings. Such a procedure can be useful in
text analysis for exploring the structure of a corpus or as a preliminary step
to higher-level classiﬁcation procedures. Latent Dirichlet allocation (LDA)
[3] is perhaps the most widely studied model and the basis for many ex-
tensions. The basic LDA model can be described as a mixed-membership
model. In particular the collection of documents is modeled by a shared col-
lection of topics, represented by multinomial distributions over words, and
each document exhibits topics with diﬀerent proportions.
Sparsity is a key feature of topic models. It captures the intuitive notion
that a given document should exhibit only a few of the corpus-wide topics
that emerge. LDA is a Bayesian hierarchical model and achieves sparsity
in topic proportions by introducing a sparse Dirichlet prior. An alterna-
tive way to achieve sparsity might utilize explicit ℓ1-penalization using the
sparse coding framework. As discussed above, this is useful because the the-
oretical literature has produced general results on the performance of sparse
estimation procedures.
3.1. Terminology.
The following terminology is widely used in the topic
modeling literature and will be useful for our discussion. We will try to
remain consistent with the notation used in the previous sections:
• A vocabulary is represented by a one-to-one mapping from the set
of indices {1, ..., m} to a set of words in the colloquial sense. Here

7
m represents the number of words in the vocabulary. Recall that m
previously corresponded to the dimension of the observed data.
• A document is represented by a collection of word counts for each
word, w = (w1, ..., wm), where wi indicates the number of occurrences
of word i. This is referred to as a bag-of-words representation.
• A corpus is represented by a collection of documents, D = (w1, ..., wn).
Here n represents the number of documents in the corpus. Recall that
n previously corresponded to the number of observations.
• A topic is represented by a multinomial distribution βi over the vo-
cabulary, alternatively a vector in the m-dimensional simplex. We will
specify the number of topics, p, to be ﬁt. The topics will be stored as
an m × p matrix, β = [β1, ..., βp].
3.2. Latent Dirichlet Allocation.
The basic LDA model is a Bayesian
hierarchical model. In this discussion let w = (w1, .., wN) be a sequential
representation so that each wi corresponds to a word in the vocabulary,
where N represents the number of words in the document. Consider the
following generative process for a given document:
1. Choose the number of words N ∼Poisson(ξ).
2. Choose topic proportions θ ∼Dirichlet(α).
3. For each word wi,
(a) Choose a topic zi ∼Multinomial(θ).
(b) Choose a word wi ∼p(wi|βzi).
Here θ, z and the β’s are latent variables to be estimated. Since N is in-
dependent of these variables, we ignore it for the purposes of this discus-
sion. The parameter α for the Dirichlet prior will need to be speciﬁed. Note
that there is an implicit assumption of exchangeability for words, called the
“bag-of-words” assumption, since the speciﬁc sequence of words is ignored.
Topics and documents are also assumed to be exchangeable, although these
assumptions can be relaxed.
Fix the topics for now. To conduct inference for θ and z, we require their
posterior distribution
(3.1)
p(θ, z|w, α, β) = p(θ, z, w|α, β)
p(w|α, β)
.
The generative process formulates the joint probability in the numerator as
(3.2)
p(θ, z, w) = p(θ|α)
N
Y
i=1
p(zi|θ)p(wi|zi, β).

8
Marginalizing over the latent variables gives
p(w|α, β)
=
Z
△
p(θ|α)
 N
Y
i=1
m
X
zi=1
p(zi|θ)p(wi|zi, β)
!
dθ
=
Γ(P
j αj)
Q
j Γ(αj)
Z
△


p
Y
j=1
θαj−1
j




N
Y
i=1
p
X
j=1
m
Y
k=1
(θjβjk)1(wi=k)

dθ
where the integral is taken over the (p −1) simplex, △= {θ : Pp
j=1 θj = 1}.
This is computationally intractable since there is a coupling between θ and
β, so approximate methods such as Markov chain Monte Carlo or variational
approximation must be used.
Variational approximation involves an iterative expectation-maximization
(EM) algorithm. In the E-step, a lower bound for the log likelihood of a
document is derived using Jensen’s inequality to deal with the intractability
of the likelihood. In particular, a computationally convenient variational
distribution
(3.3)
q(θ, z|γ, φ) = q(θ|γ)
N
Y
i=1
q(zn|φn)
is used to approximate p(θ, z|w, α, β). Optimizing over the variational pa-
rameters gives
(3.4)
(γ∗, φ∗) = argmin
(γ,φ)
D(q(θ, z|γ, φ) || p(θ, z|w, α, β))
where D(f || g) denotes the Kullback-Leibler divergence. In the M-step, the
resulting lower bound is maximized with respect to the parameters α and
β.
3.3. Sparse Coding Approach.
The sparse coding framework discussed in
Sections 1 and 2.1 seems to provide a natural alternative to the Bayesian
formulation of LDA. In particular the set of topics corresponds to a dic-
tionary and document-level topic proportions correspond to codes. Thus,
we can adapt a sparse coding formulation to specify an alternative sparse
coding topic model (SCTM). Using the standard ℓ1-penalty, we solve the
following sparse coding problem:
(3.5)
min
{θ(i)}n
i=1,{bj}p
j=1
n
X
i=1

1
2||x(i) −
p
X
j=1
θ(i)
j bj||2
2 + ω||θ(i)||1



9
such that
(3.6)
θ(i) ⪰0 for all i and bj ⪰0,
X
k
bjk = 1 for all j
where x(i), i = 1, ..., n represents the observed word frequencies for the ith
document. The θ’s correspond to document-speciﬁc topic proportions and
are constrained to be non-negative for interpretative purposes. We cannot
constrain them to have unit sum due to the ℓ1-penalty, so these topic pro-
portions are not mixture coeﬃcients as they were in LDA. Note that the
theoretical work in [4], as discussed previously, requires no such constraints
for predictive performance or model selection consistency in the coding step.
The b’s represent the dictionary elements to be learned and correspond to
topics. To be interpreted as multinomial distributions over words, each topic
is constrained to be nonnegative and have unit sum. As a result our model
approximates the distribution over words for a document as a mixture. The
probability for the ith word is given by
(3.7)
f(wi) =
p
X
j=1
θjfj(wi)
where fj(wi) ≡bji the ith entry in the vector bj. We use an iterative pro-
cedure to ﬁt the model, solving for document-level coeﬃcients θ(i) while
holding the corpus-level topics [b1, ..., bp] ﬁxed, and vice versa.
3.3.1. Solving for Coeﬃcients.
The objective in (3.5) becomes separable
in i when maximizing with respect to coeﬃcients so we can consider a single
document at a time. For a given document this requires the solution of a
LASSO problem with nonnegative constraints on the coeﬃcients when the
topics are held ﬁxed. The numerical solution of such problems has been well-
studied and algorithms such as coordinate descent and least angle regression
(LARS) work well for the standard LASSO.
We adopt a coordinate descent algorithm with soft thresholding. Denote
the objective by L(θ) = g(θ)+ω||θ||1. Let gk(θ) denote the partial derivative
of g with respect to θk and let θ(0)
−k = θ −θkek denote the vector θ with kth
coordinate set to 0. For a given regularization parameter, θ, the algorithm
takes the form:
1. Choose a direction k ∈{1, .., p}.
2. If |gk(θ(0)
−k)| < ω then set θ = θ(0)
−k. Otherwise obtain θ by a one-
dimensional line search along the kth coordinate.
3. Iterate until convergence.

10
We handle the nonnegativity constraint by limiting the line search to non-
negative coordinates. LARS can also be adapted to handle the nonnegativity
constraint as described in [6].
3.3.2. Solving for Topics.
We now consider maximizing (3.5) with re-
spect to topics. Here it will be helpful to adopt a matrix representation. Let
B = [b1, ..., bp] ∈Rm×p be the dictionary formed by stacking the topics into
a matrix. Form X = [x1, .., xn] ∈Rm×n and θ = [θ1, ..., θn] ∈Rp×n. Holding
θ’s ﬁxed, the problem becomes
(3.8)
ˆB = argmin
B
1
2||X −BΘ||2
F
such that
(3.9)
d
X
i=1
Bij = 1 for all j = 1, ..., p and B ⪰0.
This is a quadratic program with the columns of B constrained to lie in the
d-dimensional simplex and can be solved with a projected gradient descent
algorithm. This approach is easily extendable to an online setting as in [8].
We iterate over columns since the simplex constraint is separable with
respect to the columns. In particular we take a step along the gradient with
respect to one column at a time and project to the simplex, which can be
done using a linear-time algorithm proposed by [5]. This gives an update
equation of the form
(3.10)
bj = Π△

bj −η ∂L
∂bj

where Π△(·) denotes a projection onto the simplex. The gradient is given
by
(3.11)
∂L
∂bj
= Bcj −dj
where C = [c1, ..., cp] = θθT and D = [d1, ..., dp] = XθT . Finally, the step
size can be tuned using a backtracking line search procedure or simply by
taking the inverse of the Hessian given by
(3.12)
∂L2
∂bj∂bj
= CjjI.
Note that taking steps one column at a time, as opposed to to the whole
matrix at once, seems to help with convergence. To summarize, our projected
gradient descent algorithm is as follows:

11
1. Calculate C = θθT and D = XθT .
2. For each topic, perform the following update:
(a) uj ←bj −
1
Cjj (Bcj −dj)
(b) bj ←Π△(uj)
3. Iterate until convergence.
3.3.3. Other Considerations.
A number of other practical considerations
must be addressed in ﬁtting the model. First, the proportions and topics
must be initialized. One diﬃculty is that we may approach a local minimi-
mum when some of the topics become equal or nearly equal to each other.
Indeed, we have found experimentally that a good initialization is important
for learning useful topics. One reason for this may be that the simplex con-
straint on the dictionary can be very unforgiving, since the projection onto
the simplex induces sparsity. Thus early steps can result in words that have
zero probability in many topics. We follow [3] in selecting a small number of
seed documents for each topic and smoothing across the entire vocabulary.
The smoothing is done by drawing from a Dirichlet distribution with the
seeded counts as parameters.
Second, we must choose an appropriate regularization parameter, which
directly aﬀects the sparsity of the topic proportions. This can be done via
cross-validation and adapted to the speciﬁc task for which the topics will
be used. Topic models are often evaluated based on held-out log likelihood
or held-out perplexity, a likelihood-based measure of model ﬁt. Note that
we could choose a separate regularization parameter for each document,
although we do not do so due to speed considerations. A simple heuristic
is to run a coding update for a single document using the initialized topics
and pick a regularization parameter which produces a suitable number of
nonzero coeﬃcients.
Third, we must choose the number of topics to be ﬁt. Again, the most
straightforward procedure is to use some kind of cross-validation procedure.
Bayesian nonparametric extensions of LDA are able to deal with this issue
more naturally using, for example, hierarchical Dirichlet processes. We do
not explore analogous approaches in this paper.
3.4. Comparison to LDA.
Qualitatively, one might view the sparse cod-
ing approach to topic modeling as a frequentist analogue to LDA. As a
result we expect that both methods should output relatively similar top-
ics, depending on the choice of parameters. Connections between LDA and
non-probabilistic approaches to topic modeling involving matrix decompo-

12
sitions were noted in [3]. We also expect that sparse coding should allow for
a potentially large number of topics.
Note that the SCTM does not contain a notion of topic assignment as
in LDA. However, we can see the equivalence by marginalizing over topic
assignments z to get
p(wi|θ, β) =
X
j
θjβij
where βij denotes the probability of the ith word in the jth topic.
4. Numerical Results.
We apply the SCTM described above to to
a pre-processed collection of 1,500 full papers from the Neural Informa-
tion Processing Systems (NIPS) Conference.1 Papers come from ﬁelds such
as computational neuroscience, machine learning and statistics. The pre-
processing consisted of removal of stopwords, tokenization, and truncation
of the vocabulary to words that occurred more than ten times. Nonetheless
the original dataset contains some spurious words such as “a2i” or “aaa”,
so we further truncate the vocabulary to words that ocurred more than
twenty-ﬁve times. The resulting dataset contains 6,727 unique words.
4.1. Sample Output.
The output of the topic model includes topic pro-
portions for each document and a set of topics for the entire corpus. With
these, we can replicate some of the exploratory aspects of topic models illus-
trated for LDA [3, 1]. First, we can visualize topics by examining the most
probable words. See Figure 4.1 for ﬁfteen topics, visualized by their top ﬁve
most probable words, from applying a SCTM with p = 50 topics and regular-
ization parameter ω = 2−12 to the NIPS dataset. This parameter value was
chosen using the simple heuristic previously discussed. Qualitatively, top-
ics identify semantically related sets of words and the most probable words
in a topic seem to correspond to keywords. For example the {node, nodes,
network, tree, graph} topic seems to contain keywords used to describe net-
work analysis. The {bound, number, result, threshold, theorem} topic seems
to contain keywords that more generally capture theoretical concepts. This
topic representation can be useful in itself as a dimensionality reduction tool
for browsing a corpus.
We can use the topic proportions to examine the top documents that ex-
hibit a given topic. Recall that topic proportions from the SCTM are not con-
strained in their sum due to the ℓ1-penalty. To make direct comparisons we
normalize them to have unit sum. For example, the top documents associated
1Newman, D. (2010). UCI Machine Learning Repository [http://archive.ics.uci.
edu/ml/datasets/Bag+of+Words].

13
1
2
3
4
5
neuron
node
map
motion
distribution
synaptic
nodes
space
direction
gaussian
activity
network
feature
velocity
probability
synapses
tree
representation
moving
density
potential
graph
mapping
stage
mean
6
7
8
9
10
algorithm
signal
word
model
bound
gradient
noise
recognition
parameter
number
step
ﬁlter
speech
prediction
result
convergence
processing
character
modeling
threshold
update
component
speaker
structure
theorem
11
12
13
14
15
circuit
spike
training
neural
action
chip
information
error
network
policy
analog
rate
generalization
net
optimal
current
ﬁring
test
recurrent
states
voltage
noise
performance
computation
step
Fig 1. Fifteen topics from a sparse coding model applied to NIPS dataset with p = 50 total
topics and regularization parameter ω = 2−12. Topics are visualized by their top ﬁve most
probable words.
with the {distribution,gaussian,probability} topic are “The Rectiﬁed Gaus-
sian Distribution” and “Approximating Posterior Distributions in Belief
Networks Using Mixtures”. The top documents in the {word,recognition,speech}
topic are “Comparison of Human and Machine Word Recognition” and “Per-
formance Through Consistency: MS-TDNN’s for Large Vocabulary Contin-
uous Speech Recognition”.
We can also look at similar documents based on the distance between
their topic proportion vectors. This can be useful, for example, in document
search or recommendation. Starting with the document “Polynomial Uni-
form Convergence of Relative Frequencies to Probabilities”, the most similar
documents based on Hellinger distance are “On the Distribution of Local
Minima of a Random Function on a Graph” and “Mixture Density Estima-
tion”. All these papers contain a mix of theory and probability as captured
by the {bound, number, result} and {distribution, gaussian, probability} top-
ics.
4.2. Generalization Performance.
We assess generalization performance
relative to LDA by comparing held-out likelihood on a test set. In particular

14
we compute the perplexity score for a document, w ∈Rm given by
perplexity(w) = exp

−log p(w)
N

where N indicates the number of words in the document. Perplexity is com-
monly used in language modeling and can be interpreted as a per-word
measure of likelihood. Lower perplexity indicates better generalization per-
formance. For LDA, we use the topicmodels package in R which provides a
wrapper for the C implementation of variational EM from [3]. We implement
the SCTM in R. For SCTM the cross-validation involves learning topics from
the training set, ﬁtting topic proportions for test documents using LASSO,
and ﬁnally computing the ﬁtted probabilities for each word. The likelihood
is calculated as
log p(w) =
m
X
i=1
niˆp(wi)
where ni indicates the number of times the ith word appeared in the test
document and ˆp(wi) = Pp
j=1 ˆθjbij.
Each model has input parameters other than the number of topics. To
make a fair comparison, we run the models for multiple values of the pa-
rameters and select the best for each model. For LDA, the hyperparameter
α speciﬁes a symmetric Dirichlet prior for the topic proportions. We test for
α ∈{1×10−2, 5×10−2, 1×10−1, 5×10−1}. Note that choosing α < 1 encour-
ages sparsity in the posterior topic proportions. For the SCTM, we choose
the regularization parameter ω ∈{5 × 10−5, 1 × 10−4, 5 × 10−4, 1 × 10−3}.
Strangely, we ﬁnd that for certain documents both models report very
high perplexities (very low likelihoods). While LDA only reports very high
perplexities and never inﬁnite perplexities (zero likelihoods), SCTM occas-
sionally does assign zero probability to words. This happens because we have
induced sparsity in both the topic proportions and the topics themselves.
In particular it is possible that an unlucky linear combination to lead to a
zero ﬁtted probability. In practice it may be necessary to smooth the topics
so that all words in the vocabulary have positive probability. In LDA this
can be implemented naturally by specifying a Dirichlet prior on the top-
ics, introducing a second Dirichlet hyperparameter. In SCTM there is no
such natural implementation, since the simplex constraint on the dictionary
imposes sparsity, so the topics would have to be post-processed in some
manner. For the purposes of this paper, we proceed by plotting the median
and interquartile range.
See Figure 2 for results from a 10-fold cross-validation experiment. In
this experiment we consider a randomly selected 500-document subset of

15
the NIPS corpus for speed purposes. The plot compares median held-out
perplexities over the folds for LDA (α = 0.01) and SCTM (ω = 5 × 10−5).
The results suggest that SCTM can be quite noisy for a small number of
topics but that it performs better than LDA as the number of topics is
increased. However, we should keep in mind that SCTM occassionally assigns
zero probability to a word, leading to inﬁnite perplexity. A simple ﬁx is to
lower the penalization when ﬁtting the topic proportions for test documents
in which words receive zero ﬁtted probability. Future work should resolve this
issue in a more natural way by exploring methods for smoothing the resulting
topics or altering the model speciﬁcation to eliminate this possibility.
It is interesting to note that while the median perplexity of LDA seems to
level oﬀas the number of topics increases, the perplexity of SCTM seems to
continue falling. One possible reason is that sparse coding has been used to
deal with overcomplete dictionaries, where p > n, and thus supports a much
larger number of topics. Indeed the sparsity constraints may lead to very
specialized topics which are more easily recombined for prediction purposes.
Due to slow run times, we have not ﬁt the model with more than 150 topics,
though part of this is due to implementation and can easily be resolved in
a performance-oriented language such as C. It would be interesting to see
at what number of topics the held-out perplexity levels oﬀ. It would also be
interesting to study the tradeoﬀ, if there exists any, between interpretability
of the topics and generalization performance.
Note that the choice of LDA hyperparameter does not aﬀect the gener-
alization performance much, perhaps because the posterior dominates the
prior even with a training set of 450 documents. On the other hand the
choice of SCTM regularization parameter can have a large eﬀect for a small
number of topics. In particular too much regularization can lead to too much
sparseness, aggravating the zero likelihood issue. There is less diﬀerence for
a large number of topics. See Figure 3 for an illustration.
5. Extensions.
Various work has extended the basic LDA model to
incorporate topical structure such as correlated topics [1] and dynamic topics
[2]. Similar extensions can be made to the sparse coding approach by altering
the penalization. Previous literature on structured sparsity has produced
methods for inducing speciﬁc patterns of sparsity. We focus on extending
the model to incorporate correlated topics.
The correlated topic model of [1] replaces the Dirichlet prior on the topic
proportions with a logistic normal prior. In particular a multivariate normal

16
G
G
G
G
G
500
1000
1500
2000
2500
Number of Topics
Held−Out Perplexity
G
G
G
G
G
10
25
50
100
150
Generalization Performance of LDA vs. SCTM
LDA
SCTM
Fig 2. Results from a 10-fold cross-validation experiment comparing held-out perplexities
for LDA with α = 0.01 and SCTM with ω = 5 × 10−5. Note that points represent medians
and error bars represent interquartile ranges.
G
G
G
G
G
500
1000
1500
2000
2500
Number of Topics
Held−Out Perplexity
G
G
G
10
25
50
100
150
Generalization Performance of SCTM with Different Regularization
ω=0.00005
ω=0.0001
Fig 3. A comparison of generalization for SCTM with diﬀerent regularization parameters.
For a small number of topics, too much penalization aggravates the zero likelihood issue.

17
variable η ∼N(µ, Σ) is mapped to the simplex by
(5.1)
θ = f(η) =
exp{η}
P
j exp{ηj}.
Correlation between topics is captured by the covariance matrix Σ. The
graph can be visualized using a variant of the graphical lasso procedure [7].
The graphical lasso produces a sparse estimate of Ω= Σ−1 and positive
entries indicate the existence of an edge.
To implement a similar extension for sparse coding, we would like to anal-
ogously introduce a covariance matrix Σ governing the correlation between
topics θ. To keep this covariance matrix sparse, we could also use graphical
lasso to maximize a term of the form
(5.2)
log det(Ω) −tr(SΩ) −ρ||Ω||1
where S indicates the empircal covariance matrix given by S = 1
n
Pn
i=1 θiθT
i .
Note that this corresponds to a log-likelihood, partially maximized with
respect to the mean, that arises from directly assuming θ is normal.
The objective becomes
(5.3)
min
{θi}n
i=1,B,Ω
n
X
i=1
L(B, θi) + θT Ωθ −log det(Ω) + ρ||Ω||1
where
L(B, θi) = 1
2||yi −Bθi||2
2 + ω||θi||1.
This could be solved via coordinate descent, optimizing with respect to θi’s,
B and Ωseparately. Optimizing with respect to B would remain the same as
before. Optimizing with respect to Ωcan be done using the graphical lasso
procedure from [7]. Finally optimizing with respect to θi’s can be done by
rewriting the term involving Ωto get
(5.4)
L(B, θi) + θT
i Ωθi = || [ yi
0 ] −
 B
LT

θi||2
2 + ω||θi||1
where Ω= LLT is a Cholesky decomposition of the inverse covariance ma-
trix. This now becomes a standard LASSO problem that can be solved as
before. Unfortunately, we have not been able to implement this procedure so
that it converges. In particular there may be a diﬃculty because it implicitly
assigns a normal prior to the θi’s rather than an intermediate variable which
is then normalized to the simplex.

18
6. Conclusion.
In this paper, we have reviewed the technique of sparse
coding and some theoretical issues surrounding both the coding step and
the dictionary learning step. Theoretical work on the joint procedure is
somewhat limited, though some recent results [9] have addressed the noise-
less case. We have also reviewed the technique of topic modeling and pre-
sented an application of sparse coding to topic modeling. The sparse coding
topic model (SCTM) can be seen as a frequentist analogue to the standard
Bayesian model, Latent Dirichlet allocation (LDA). SCTM provides compa-
rable performance to LDA and seems to support a larger number of topics.
The algorithm also can be extended to an online setting as in [8]. Lastly, the
model can be extended using norms for structured sparsity to, for example,
replicate the correlated topic model of [1].
Acknowledgements.
I thank Professor John Laﬀerty for his guidance
throughout the writing of this paper. I also thank Andy Dahl for helpful
discussions.
References.
[1] Blei, D. and Lafferty, J. (2007). A Correlated Topic Model of Science. Annals of
Applied Statistics 1(1): 17 – 35.
[2] Blei, D. and Lafferty, J. (2006). Dynamic Topic Models. In Proceedings of the
International Conference on Machine Learning.
[3] Blei, D., Ng, A., and Jordan, M. (2003). Latent Dirichlet Allocation. Journal of
Machine Learning Research 3: 993–1022.
[4] Bunea, F., Tsybakov, A., Wegkamp, M., and Barbu, A. (2010). SPADES and
Mixture Models. Annals of Statistics 38(5): 2525–2558.
[5] Duchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T. (2008). Eﬃcient
Projections Onto the ℓ1-ball for Learning in High Dimensions. In Proceedings of the
International Conference on Machine Learning.
[6] Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. (2004). Least Angle
Regression. Annals of Statistics 32(2): 407–499.
[7] Friedman, J., Hastie, T., and Tibshirani, R. (2008). Sparse Covariance Estimation
with the Graphical Lasso. Biostatistics 9: 432–441.
[8] Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (2010). Online Learning for
Matrix Factorization and Sparse Coding. Journal of Machine Learning Research 11:
19–60.
[9] Spielman, D., Wang, H., and Wright, J. (2012). Exact Recovery of Sparsely-Used
Dictionaries. In Proceedings of the 25th Annual Conference on Learning Theory.
[10] Vainsencher, D., Mannor, S., and Bruckstein, A. (2011). The Sample Com-
plexity of Dictionary Learning. Journal of Machine Learning Research 12: 3259–3281.

