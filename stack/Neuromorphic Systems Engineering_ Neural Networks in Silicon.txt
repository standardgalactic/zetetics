NEUROMORPHIC SYSTEMS 
ENGINEERING 
Neural Networks in Silicon 

THE KLUWER INTERNATIONAL SERIES 
IN ENGINEERING AND COMPUTER SCIENCE 
ANALOG CIRCUITS AND SIGNAL PROCESSING 
Consulting Editor: Mohammed Ismail. Ohio State University 
Related Titles." 
DESIGN OF MODULATORS FOR OVERSAMPLED CONVERTERS, Feng Wang, Ramesh Harjani, 
ISBN: 0-7923-8063-0 
SYMBOLIC ANALYSIS IN ANALOG INTEGRATED CIRCUIT DESIGN, Henrik Floberg, ISBN: 
0-7923-9969-2 
SWITCHED-CURRENT DESIGN AND IMPLEMENTATION OF OVERSAMPLING A/D 
CONVERTERS, Nianxiong Tan, ISBN: 0-7923-9963-3 
CMOS WIRELESS TRANSCEIVER DESIGN, Jan Crols, Michiel Steyaert, ISBN: 0-7923-9960-9 
DESIGN OF LOW-VOLTAGE, LOW-POWER OPERATIONAL AMPLIFIER CELLS, Ron 
Hogervorst, Johan H. Huijsing, ISBN: 0-7923-9781-9 
VLSI-COMPATIBLE IMPLEMENTATIONS FOR ARTIFICIAL NEURAL NETWORKS, Sied 
Mehdi Fakhraie, Kenneth Carless Smith, ISBN: 0-7923-9825-4 
CHARACTERIZATION METHODS FOR SUBMICRON MOSFETs, edited by Hisham Haddara, 
ISBN: 0-7923-9695-2 
LOW-VOLTAGE LOW-POWER ANALOG INTEGRATED CIRCUITS, edited by Wouter Serdijn, 
ISBN: 0-7923-9608-1 
INTEGRATED VIDEO-FREQUENCY CONTINUOUS-TIME FILTERS: High-Performance 
Realizations in BiCMOS, Scott D. Willingham, Ken Martin, ISBN: 0-7923-9595-6 
FEED-FORWARD NEURAL NETWORKS: Vector Decomposition Analysis, Modelling and Analog 
Implementation, Anne-Johan Annema, ISBN: 0-7923-9567-0 
FREQUENCY COMPENSATION TECHNIQUES LOW-POWER OPERATIONAL 
AMPLIFIERS, Ruud Easchauzier, Johan Hu~jsmg, ISBN: 0-7923-9565-4 
ANALOG SIGNAL GENERATION FOR BIST OF MIXED-SIGNAL INTEGRATED CIRCUITS, 
Gordon W. Roberts, Albert K. Lu, ISBN: 0-7923-9564-6 
INTEGRATED FIBER-OPTIC RECEIVERS, Aaron Buchwald, Kenneth W. Martin, ISBN: 0-7923- 
9549-2 
MODELING WITH AN ANALOG HARDWARE DESCRIPTION LANGUAGE, H. Alan 
Mantooth, Mike Fiegenbaum, ISBN: 0-7923-9516-6 
LOW-VOLTAGE CMOS OPERATIONAL AMPLIFIERS: Theory, Design and Implementation, 
Satoshi Sakurai, Mohammed lsmail, ISBN: 0-7923-9507-7 
ANALYSIS AND SYNTHESIS OF MOS TRANSLINEAR CIRCUITS, Remco d.. Wiegerink, ISBN: 0- 
7923-9390-2 
COMPUTER-AIDED DESIGN OF ANALOG CIRCUITS AND SYSTEMS, L. Richard Carley, Konald 
S. Gyurcsik, ISBN: 0-7923-9351-1 
HIGH-PERFORMANCE CMOS CONTINUOUS-TIME FILTERS, ,]os6 Silva-Martlnez, Michiel 
Steyaert, Willy Sansen, ISBN: 0-7923-9339-2 
SYMBOLIC ANALYSIS OF ANALOG CIRCUITS: Techniques and Applications, Lawrence P. 
Huelsman, Georges G. E. Gielen, ISBN: 0-7923-9324-4 
DESIGN OF LOW-VOLTAGE BIPOLAR OPERATIONAL AMPLIFIERS, M. Jeroen Fonderie, Johan 
H. Huijsing, ISBN: 0-7923-9317-1 
STATISTICAL MODELING FOR COMPUTER-AIDED DESIGN OF MOS VLSI CIRCUITS, 
Christopher Michael, Mohammed lsmail, ISBN: 0-7923-9299-X 
SELECTIVE LINEAR-PHASE SWITCHED-CAPACITOR AND DIGITAL FILTERS, Hussein Baher, 
ISBN: 0-7923-9298-1 
ANALOG CMOS FILTERS FOR VERY HIGH FREQUENCIES, Bram Nauta, ISBN: 0-7923-9272-8 
ANALOG VLSI NEURAL NETWORKS, Yoshiyasu Takefuji, ISBN: 0-7923-9273-6 

NEUROMORPHIC SYSTEMS 
ENGINEERING 
Neural Networks in Silicon 
edited by 
Tor Sverre Lande 
University of Oslo 
Norway 
1~8 
KLUWER ACADEMIC PUBLISHERS 
Boston / Dordrecht / London 

Distributors for North, Central and South America: 
Kluwer Academic Publishers 
101 Philip Drive 
Assinippi Park 
Norwell, Massachusetts 02061 USA 
Distributors for all other countries: 
Kluwer Academic Publishers Group 
Distribution Centre 
Post Office Box 322 
3300 AH Dordrecht, THE NETHERLANDS 
Library of Congress Cataloging-in-Publication Data 
A C.I.P. Catalogue record for this book is available 
from the Library of Congress. 
Copyright Â© 1998 by Kluwer Academic Publishers 
All rights reserved. No part of this publication may be reproduced, stored in a 
retrieval system or transmitted in any form or by any means, mechanical, photo- 
copying, recording, or otherwise, without the prior written permission of the 
publisher, Kluwer Academic Publishers, 101 Philip Drive, Assinippi Park, Norwell, 
Massachusetts 02061 
Printed on acid-free paper. 
Printed in the United States of America 

Contents 
Foreword 
ix 
Preface 
xiii 
Acknowledgements 
Part I 
Cochlear systems 
1 
Filter Cascades as Analogs of the Cochlea 
Richard F. Lyon 
2 
An Analogue VLSI model of Active Cochlea 
Eric Fragni~re, Andrd van Schaik and Eric A. Vittoz 
3 
A kow-Power Wide-Dynamic-Range Analog VkS[ Cochlea 
Rahul Sarpeshkar, Richard F. Lyon, Carver Mead 
4 
Speech Recognition Fxperiments with Silicon Audito~ Models 
John Lazzaro and John Wawrzynek 
xvii 
19 
49 
105 

vi 
NEUROMORPHIC SYSTEMS ENGINEERING 
Part II 
Retinomorphic systems 
5 
The Retinomorphic Approach: PixeI-Parallel 
Adaptive Amplification, Filtering, and Ouantization 
Kwabena A. Boahen 
6 
Analog VLSI Excitatory Feedback Circuits for 
Attentional Shifts and Tracking 
T.G. Morris and S.P. DeWeerth 
7 
Floating-Gate Circuits for Adaptation of 
Saccadic Eye Movement Accuracy 
Timothy K. Horiuchi and Christo] Koch 
129 
151 
175 
Part III 
Neuromorphic Communication 
8 
Introduction to Neuromorphic Communication 
Tot Sverre Lande 
9 
A Pulsed Communication/Computation Framework for 
Analog VLSI Perceptive Systems 
Alessandro Mortara 
10 
Asynchronous Communication of 2D Motion Information 
Using Winner-Takes-All Arbitration 
Zaven Kalayjian and Andreas G. Andreou 
11 
Communicating Neuronal Ensembles between 
Neuromorphic Chips 
Kwabena A. Boahen 
193 
201 
217 
229 

Contents 
vii 
Part IV Neuromorphic Technology 
12 
Introduction: From Neurobiology to Silicon 
Chris Diorio 
13 
A Low-Power Wide-Linear-Range Transconductance Amplifier 
Rahul Sarpeshkar, Richard F. Lyon, Carver Mead 
14 
Floating-Gate MOS Synapse Transistors 
Chris Diorio, Paul Hasler, Bradley A. Minch, And Carver Mead 
15 
Neuromorphic Synapses for Artificial Dendrites 
Wayne C. Westerman, David P. M. Northmore, and John. G. Elias 
16 
Winner-Take-All Networks with Lateral Excitation 
Giecomo Indiveri 
263 
267 
315 
339 
367 
Part V 
Neuromorphic Learning 
17 
Neuromorphic Learning VLSI Systems: A Survey 
Gert Cauwenberghs 
18 
Analog VLSI Stochastic Perturbative Learning Architectures 
Ger~ Cauwenberghs 
19 
Winner-Takes-All Associative Memory 
Philippe O. Pouliquen, Andreas G. Andreou and Kim Strohbehn 
381 
409 
437 
Index 
457 

Foreword 
Today's silicon technology can provide about one billion transistors on a single 
chip, and new process developments will no doubt offer ten times ~s much in 
the early 2000s. This enormous hardware capability offers the potential for 
implementing very complex functions on single chips and for combining these 
chips into highly intelligent systems. 
However, the rate of progression of design techniques -- and of the associated 
design tools -- needed to build such complex systems-on-chips is lagging in- 
creasingly, and threatens to become a real bottleneck in the exploitation of this 
fantastic technology. Power dissipation becomes a limiting problem, not only 
because of the increasing difficulty to evacuate the heat created on-chip, but 
also because of the growing interest for very small, battery-operated, portable 
systems. How can the creativity of designers be dramatically boosted to pul- 
verize these obstacles? 
One possible answer is to look into what life has "invented" along half a bil- 
lion years of evolution. Not in order to copy or directly exploit living organisms 
-- this is the realm of biotechnology -- but to get inspiration from what they 
use as principles, mechanisms or architectures, or even of the very functions 
they carry out. 
Since the enormous potential of microelectronics is essentially for processing 
signals, it is the central nervous system of living creatures which is of most im- 
mediate interest, hence the term "neuromorphic engineering" coined by Carver 
Mead, one of the early promoters of this fascinating approach. Some may prefer 
to speak of "biology-inspired" microelectronics to stress the fact that the struc- 
ture of the electronic solution does not necessarily match that of its biological 
source. 
Numerous principles found in the brain can provide inspiration to circuit 
and system designers. 
Most obvious is the huge amount of repetitive cells 
operating in a massively parallel manner. This may help to solve the problem 
of hierarchically designing very complex systems. Applied to digital systems, it 
also enables the speed of each individual cell to be reduced, in order to obtain 
a proportional reduction of power consumption by reducing the supply voltage. 

x 
NEUROMORPHIC SYSTEMS ENGINEERING 
As a matter of fact, the brain is able to solve, in real time, very complex 
tasks on massively parallel data, by means of neurons operating at a very 
low local speed (of the order of one kilohertz), with the poor local precision of 
analog processing. Power consumption is very low since the whole human brain 
dissipates no more than a few tens of watts. Furthermore, the cells operate in a 
collective manner: they solve a problem by elaborating a consensus on the best 
solution, instead of each separately taking care of a small part of the problem. 
Thus collective computation in massively parallel analog VLSI circuits may 
reasonably be expected to be the best engineering approach to solve perceptive 
tasks such as vision or audition. 
Throughout the evolutionary process, life has opportunistically exploited 
and retained all favorable features of new structures. This opportunism can 
also inspire engineers to exploit all the inherent properties of the available 
technology. This bottom-up approach to optimum solutions is ideally suited 
for analog circuits: all the properties of transistors and of elementary circuits 
are analyzed and characterized for later opportunistic use. Along the same 
idea, a search is made for hidden advantages of apparent defects or parasitic 
effects. In many parts of the brain, the information coming from the various 
sensors is coded and processed by means of maps of activity. The limited range 
of operation of each cell in a map is locally adapted to the level of the signals, 
with the result of a very wide dynamic range of sensitivity. Such schemes are 
very useful to design very low power circuits based on noisy and inaccurate 
analog cells. Learning by examples can be considered an extended form of 
adaptation for solving particular problems. It is a very attractive alternative 
to address the problem of programming complex machines for very complex 
tasks. 
Another salient feature of nervous systems, the ubiquitous use of the fre- 
quency or phase of pulse trains to represent analog signals, turns out to be 
a very efficient way of realizing the dense communication network required 
by collective computation. Furthermore, such a representation facilitates the 
implementation of time-domain processing by means of simple hardware oper- 
ators. 
The state of the art of neuromorphic systems engineering is excellently doc- 
umented by the selection of contributions assembled in this edited book by Tor 
Sverre Lande. Various aspects of the biology-inspired approach are described 
by most of the prominent actors of this fascinating adventure. Although most 
of the work described is still very exploratory and fragmentary, it shows very 
attractive results which definitely support the validity of such an innovative 
engineering approach. Some results, in particular in the domain of early vision, 
are ready for practical applications. 
Others help understand the biological 
processes and are just the very first steps towards building fully operational 
perception machines. 
After the excitation created by the immediate rewards of a new discipline, 
neuromorphic engineering is now in a phase of consolidation, by a rather limited 
number of research groups. A few original products based on it have already 

FOREWORD 
xi 
reached the market, but the functions they implement are very far below what 
can be expected in the future. 
This book is intended to attract a broader 
interest for this promising discipline and to advocate a more massive research 
effort therein. This effort will at least turn out to be a strong action of lateral 
thinking, resulting in more efficient solutions to standard problems. At most, 
it might well bring a revolution in the way very high-density processes will 
be used in the future to build sophisticated systems for advanced perceptive 
processing. 
PROF. ERIC A. VITTOZ 
CSEM, Neuch~tel 

Preface 
The name of this book, Neuromorphic Systems Engineering, emphasizes three 
important aspects of an exciting new research field. The term neuromorphic 
expresses relations to computational models found in biological neural systems, 
which are used as inspiration for building large electronic systems in silicon. 
By adequate engineering, these silicon systems are made useful to mankind. 
While trying to catch the computational paradigms of biology, the strong 
connection between the representation of the neural state and the neural com- 
putation is unveiled. Although there are quantized states in biology, the domi- 
nating state variable is continuous, making our nervous system predominantly 
an analog computational system with computations taking place in real and 
continuous time. In contrast to our completely quantized and error free digital 
systems, biology seems to carry out computational tasks in a fundamentally 
different way. Based on badly conditioned data, complex matching operations, 
like recognizing faces, are carried out in real time, outperforming any known 
digital system. Knowing that these complex operations are carried out with 
extremely limited computational elements with slow and noisy interconnections 
makes the computations of nature even more intriguing. 
Another fascinating and challenging aspect of neuromorphic engineering is 
the interdisciplinary nature. The rapidly growing research field of neuroscience 
is combined with the rapidly growing complexity of microelectronics. 
The 
progress in neuroscience has uncovered a fundamental new understanding of 
the computational aspects of our nervous system. New measurement tech- 
niques have given us detailed knowledge of the basic mechanisms of neural 
computations. An increasing flow of improved biological models based on solid 
physical evidence is a valuable source of inspiration. In particular models of 
our early vision system and the primary auditory system have successfully been 
implemented in silicon, as reported in parts I and II of this book. 
The achievements of microelectronics are evident to everybody, as we are all 
affected in our everyday life. The famous Moore's law of doubled performance 
every second year seems to be unbreakable. The success of digital microelec- 
tronic systems, utilizing millions of transistors, is indisputable, while the old 

xiv 
NEUROMORPHIC SYSTEMS ENGINEERING 
art of analog circuit design is still stuck with less than a hundred transistors. 
Armed with the increased knowledge of our mainly analog, nervous system, we 
may be able to utilize the available silicon in a novel way by building large- 
scale analog systems. In doing so we may even surpass some of the practical 
limitations of digital microelectronics. Several systems in this book use weakly 
inverted (subthreshold) transistors, reducing power consumption to a fraction 
of a similar digital system. In neuromorphic systems, we also try to inherit the 
defect tolerance of biology. 
The intention of this book is to provide a snapshot of neuromorphic engi- 
neering today. Most of the material is taken from a special issue of the interna- 
tional Kluwer journal Analog Integrated Circuits and Signal Processing, Vol. 13, 
May/June, 1997. Significant new material is added, reflecting state-of-the-art 
developments within neuromorphic engineering. 
The book is organized into five parts with a total of nineteen chapters viewing 
the field of neuromorphic engineering from different perspectives. 
Part I, "Cochlear Systems" starts with the chapter "Filter Cascades as 
Analogs of the Cochlea" by Dick Lyon. Lyon together with Carver Mead pio- 
neered cochlea modeling in silicon by implementing a working basilar membrane 
equivalent with a couple of hundred second-order filters cascaded. In spite of 
all predictions, the implemented micropower system worked surprisingly well, 
showing one of the first neuromorphic analog systems in silicon. The power of 
pseudoresonance in cascaded filters is analyzed, showing a system resonance su- 
perseding the resonance of each individual filter stage. The second chapter, "An 
Analogue VLSI model of Active Cochlea" attacks one of the hardest modeling 
issues of the inner ear, namely automatic gain control (AGC). The mechani- 
cal feedback through the outer haircells of the basilar membrane increases the 
tuning sharpness giving a unique system performance with only weakly reso- 
nant filters. The last author, Eric Vittoz, is known for his pioneering work 
on micropower microelectronics, the technological foundation of most neuro- 
morphic silicon. In Chapter three Sarpeshkar et. al. present a state-of-the-art 
implementation of Lyon's basilar membrane model in silicon with high dynamic 
range combined with low power consumption. The feasibility of pseudoreso- 
nance is demonstrated and well documented. The next chapter on "Speech 
Recognition Experiments with Silicon Auditory Models" explores the feasibil- 
ity of speech recognition using three simultaneously running analog cochleae 
with neural spike encoded data processing. An asynchronous communication 
system is developed interfacing the chips to a computer. A classification sys- 
tern for human speech is developed and evaluated. All together this system is 
a strong indicator of the possibilities of neuromorphic engineering. 
Part II is devoted to selected topics on retinomorphic or vision systems in sill- 
con. Several existing implementations of silicon retina are available in the litera- 
ture, but improvements may be done. In the fifth chapter, "The Retinomorphic 
Approach: Pixel-Parallel Adaptive Amplification, Filtering and Quantization" 
by 'Buster' Boahen elegantly shows how simple vital retina functions may be 
implemented using compact, current-mode circuits. The two-dimensional na- 

PREFACE 
xv 
ture of the retina rewards this simplicity by increased resolution. The sixth 
chapter by Morris et. al., "Analog VLSI Excitatory Feedback Circuits for At- 
tentional Shifts and Tracking" approaches another essential property of vision, 
namely tracking of moving objects. By extending the classical winner-take-M1 
circuit, robust tracking systems are presented with experimental results. In 
chapter seven, "Floating-Gate Circuits for Adaptation of Saccadic Eye Move- 
ment Accuracy" by Horiuchi et. al., the floating gate of MOS-transistors is 
used to permanently store the results of the adaptive action taking place in our 
visual system. The feasibility of using analog storage in an adaptive control 
system is discussed and experimentally demonstrated. 
Part III consists of four chapters in the field of "Neuromorphic Commu- 
nication" which is characterized by time-multiplexing of asynchronous neural 
spikes (events) over a digital bus. Chapter eight is a short introduction where 
different approaches are discussed. The concept of weak arbitration is intro- 
duced and measured against other approaches. The next chapter by Mot- 
tara, "A Pulsed Communication/Computation Framework for Analog VLSI 
Perceptive Systems" shows a simple solution allowing collisions of simultane- 
ous events. The very essential inter-spike timing information is maintained, 
but some noise is added to the signal transmitted, due to collisions. Chapter 
ten by Kalayjian et. al. 
"Asynchronous Communication of 2D Motion In- 
formation using winner-take-all Arbitration" proposes an arbitration scheme 
virtually eliminating collisions. In chapter eleven, "Communicating Neuronal 
Ensembles between Neuromorphic Chips," Boahen explains different tradeoffs 
when implementing a full interchip communication system. A full arbitration 
scheme is used and compared to traditional scanning techniques. The errors 
introduced by multiplexing are characterized, documenting both the potential 
and the limitations of a neuromorphic communication channel. 
Part IV is devoted to "Neuromorphic Technology." The chapter called "In- 
troduction: From neurobiology to silicon" by Diorio explains some of the ra- 
tionales for exploring the physics of microelectronics in modeling biology. In 
chapter thirteen "A low-power wide-linear-range transconductance amplifier" 
by Sarpeshkar et. al., the back-gate of the MOS-transistors is used as input to 
a differential pair. Degeneration of both source and gate is applied, extending 
the linear range an order of magnitude. A transconductor with 65dB SNR is 
implemented in weak inversion and demonstrated for use in a cochlea implemen- 
tation. In the next chapter called "Floating-Gate MOS Synapse Transistors" 
by Diorio et. al., the floating-gate of a MOS-transistor is utilized to program 
the strength of a synaptic connection (or weight). The striking simplicity and 
innovative usage of standard CMOS technology favor scalability to large sys- 
tems. Chapter fifteen, "Neuromorphic Synapses for Artificial Dendrites," by 
Westerman et. al. approaches the synaptic connection in a more biological 
manner, mimicking some of the dynamics found in real synapses. In chapter 
sixteen, "Winner-Take-All Networks with Lateral Excitation" by Indiveri, the 
classical winner-take-all circuit is extended to exhibit hysteresis and lateral ex- 

xvi 
NEUROMORPHIC SYSTEMS ENGINEERING 
citation, making the circuit more robust to noise. Its behavior is demonstrated 
on a 1-D silicon retina implementation. 
"Neuromorphic Learning" is the heading of part V and starts with the in- 
troduction, "Neuromorphic Learning VLSI Systems: A Survey," by Cauwen- 
berghs. This is an excellent review of the development of learning systems in 
hardware and the bibliography serves as an excellent reference. Cauwenberghs 
is also the author of chapter eighteen, "Analog VLSI Stochastic Perturbative 
Learning Architectures," presenting three different learning algorithms taking 
the stochastic nature of neural plasticity into account. The techniques apply 
to general reward-based learning and give rise to scalable, robust learning ar- 
chitectures in analog VLSI independent of both the structure of the network 
and the specifics of the learning task. The last chapter of the book, "Winner- 
Take-All Associative Memory: A Hamming Distance Vector Quantizer," by 
Pouliquen et. al., is a remarkably simple, mixed-mode system using a memory- 
based computation to classify input patterns according to a known pattern set. 
The system is shown to work reliably discriminating bitmapped characters, 
even with some error correction. 
Although this book is far from providing complete coverage of this emerging 
field, I sincerely hope that the material presented will spawn some interest and 
make more people join the exciting and highly rewarding research in Neuro- 
morphic Engineering. 
TOR SVERRE LANDE 

Acknowledgements 
Preparing an edited book should be easy, since most of the written material 
is done by somebody else. I have learned that merging material from different 
word-processing systems is far from easy. Thanks to my student, Jan-Tore 
Marienborg, all the individual bibliographies were merged into one conformant 
.BIB-file. He also helped me converting files from different word-processing 
systems. 
The book is completely set and printed using computer facilities at the De- 
partment of Informatics, University of Oslo. As a staff-member of the Mi- 
croelectronic Systems Group my gratitude goes to all of you and especially 
to my college, Yngvar Berg, for valuable corrections and suggestions. Also my 
Ph.D. student, Sigbj~rn Nmss, did an excellent job correcting numerous errors. 
My friend, Mohammed Ismail - the editor the Kluwer journal "Analog Inte- 
grated Circuits and Signal Processing", encouraged me to be the guest-editor 
of a special issue of the journal and gave me valuable advise. 
The central players of this book are all the contributing authors, without 
whom this book never would have been printed. I am proud of being the editor 
of such aa excellent collection of outstanding work and I am grateful for all 
the encouraging feedback from all of you. It is my sincere hope that this book 
is something you will be proud of in the future. I know that several of the 
authors contributing to this book have experienced the great inspiration of 
Carver Mead. I hope this book will carry on his ideas and open up for a wider 
audience the importance of his pioneering work. 
Finally I want to thank my always understanding and patient wife, Elisabeth. 

I Cochlear Systems 

I 
FILTER CASCADES AS ANALOGS 
OF THE COCHLEA 
Richard F. Lyon 
Foveonics Inc., I0131-B Bubb Rd., Cupertino CA 95014 
dicklyon@acm.org 
1.1 
MODELS OF COCHLEAR WAVE PROPAGATION 
Wave propagation in the cochlea can be modeled at various levels and for vari- 
ous purposes. We are interested in making models of cochlear signal processing, 
in analog or digital VLSI or in software, suitable for supporting improved hear- 
ing aids, speech-recognition systems, and other engineered hearing machines. 
We are also interested in developing models that can contribute to a deeper un- 
derstanding of how hearing works. Hence, a neuromorphic approach, in which 
the functionality of the model emerges from a form that is loosely copied from 
the nervous system, is appropriate. 
The filter-cascade approach to modeling the cochlea is based on the obser- 
vation that small segments of the cochlea act as local filters on waves propa- 
gating through them. Thus, a cascade of filters can emulate the whole complex 
distributed hydrodynamic system. This modeling approach can include com- 
pressive and adaptive aspects of the peripheral auditory nervous system as 
well, using analogs of cochlear nonlinear distortion and efferent feedback. We 
summarize the underpinnings, advantages, and limitations of this approach in 
this paper~ so that readers can more readily understand other papers on filter- 
cascade approaches and implementations. 
Figure 1.1 shows the filter-cascade structure that we discuss in this paper. 
1.2 
COCHLEAR HYDRODYNAMICS IN THE LIOUVILLE-GREEN 
APPOXIMATION 
Imagine the cochlea as a three-dimensional (3D) hydrodynamic system with 
a linear or one-dimensional (1D) array of sensors attached to it. In the real 

4 
NEUROMORPHIC SYSTEMS ENGINEERING 
In 
(stapes) 
Filter Cascade Structure 
x (place) dimension 
1
~
 
from Base (high CF) . . . to Apex (low CF) 
Figure 1.1 
A Filter Cascade. This simple structure of cascaded filter stages is a useful 
analog to the hydrodynamic wave propagation system of the cochlea. 
cochlea, the 3D and 1D structures follow a complicated helical path, with the 
sensors being the inner hair cells (IHCs) of the organ of Corti. Abstractly, 
we refer to the one dimension that indexes the sensors as the cochlear place 
dimension. From a functional point of view, we care about the response only 
as a function of the input signal and of the 1D place, so there is only one 
relevant spatial dimension at the model output. 
Wave propagation in the cochlea depends on fluid displacement in three di- 
mensions, on membrane bending and stretching, and on related 2D and 3D and 
micromechanical effects within the organ of Corti, which have mostly unknown 
physical parameters. There have been many arguments in the cochlea-modeling 
business about whether 1D, 2D, or 3D models are good enough to capture the 
essence of the physics. Independent of these arguments, if the results of the 
model are needed at only a sequence of places along one dimension (such as 
at the inner-hair-cell locations), then we can represent the results of the 2D 
or 3D hydrodynamics by a 1D model system, and can do so by using transfer 
functions, more economically than by modeling the fluid motion directly. 
One key property of cochlear physics on which we must rely for this approach 
is the unidirectionality of energy flow. Under normal conditions, sound energy 
enters the ear, propagates, and is absorbed without causing significant energy 
to reflect and propagate back out [2]. This condition is the one that we shall 
model; we discuss exceptions in Section 1.9. 
The method known as Liouville-Green (LG) or Wentzel-Kramers-Brillouin 
(WKB, or WKBJ with Jeffreys) give us easy insight into wave propagation in 
nonuniform media such as the cochlea. This method says that, if a wave is 
propagating from the input along one dimension, then we can get the response 
from the input to any point by composing the relative responses from each 
point to the next along that dimension, using local parameters as though the 
medium were uniform. 
The mathematics is most easily expressed in terms of a wave description 
in which the local (i.e., at any particular place) wave-propagation properties 
are characterized by a complex wavenumber. To make life simple, we consider 

FILTER CASCADES 
5 
only one frequency at a time. To simplify the analysis further, we use complex 
numbers as the values of waves, realizing that we can easily constrain wave 
values to real numbers later by adding pairs of complex waves in a complex- 
conjugate relationship. 
In a uniform medium, a wave propagating toward increasing values of the 
place dimension is given by 
W(x) = A exp(i~t - ikx) 
where A is the amplitude, ~o is the frequency, k is the wavenumber, t is time, 
and x is place. The wavenumber depends on frequency via the physics of the 
medium; we can write it as the function k(~v)--the solution of the physical 
constraints known as the eikonal or the dispersion relation of the system. 
We can think of the wavenumber as the spatial frequency of the wave, in 
radians per meter in MKS units. If k is real, then the wave described simply 
progagates with no change in amplitude, with a wavelength of 2~r/k, at a veloc- 
ity w/k. If k has a nonzero imaginary component, however, then the wave can 
decay in amplitude as it propagates (i.e., in a passive or attenuating medium, for 
a negative imaginary part) or increase as it propagates (i.e., an active amplify- 
ing medium, for a positive imaginary part). Zweig and colleagues [29] presented 
an analysis of the 1D long-wave approximation to the cochlea with resonant 
basilar membrane (BM), using the complex radian wavelength :~ (lambda-bar, 
the reciprocal of the wavenumber). 
By examining the ratio of waves at two places separated by a distance Ax, 
we see that the wave at the farther place is equal to the wave at the nearer place 
multiplied by exp(-ikAx), representing a frequency-dependent filter character- 
izing the stretch of length Ax. 
In a nonuniform medium, there is no single wavenumber for a frequency, and 
possibly certain regions amplify a particular frequency while others attentuate 
it. Under reasonable conditions, however, each point in such a medium (i.e., 
along the place dimension) can be characterized by a local wavenumber, as 
though it were part of a uniform medium. The LG approximation then says 
that a wave propagating an infinitesimal distance dx through that place will be 
multiplied by exp(-ikdx) (and possibly also by a real-valued factor near 1, if a 
constant amplitude does not correspond to a constant power as the parameters 
of the medium change--but let's neglect that factor for now). 
Now consider wavenumber as a function of both frequency and place: k(w, x). 
Within the approximation of the LG method, this function completely char- 
acterizes wave propagation in the nonuniform medium along the x dimension. 
To see what happens between points far apart, we can break the medium into 
infinitesimal segments of length dx, and can multiply together all the factors for 
those segments. The factors are exponentials, and the product of exponentials 
is the exponential of a sum, so the resulting product is the exponential of an 
integral along the x dimension: 

6 
NEUROMORPHIC SYSTEMS ENGINEERING 
~ 
xb 
H = exp(-i 
kdx) 
a 
This complicated-looking frequency-dependent gain and phase factor H is 
the LG method's representation of the transfer function between points xa 
and Xb in a nonuniform medium; it is a generalization of the transfer function 
exp(-ikAx) that characterizes a stretch of a uniform medium. 
The factor H is still just a linear filter in the usual signal-processing sense. 
Furthermore, we can factor this filter into a product, or cascade, of several filters 
by splitting the interval of integration (from x~ to Xb) into N small steps: 
N 
~xxJ 
g -- II exp(-i 
k(w, x)dx) 
j=l 
j-1 
Any number and size of steps leads to a factorization, but, if the steps are 
small enough, then each individual filter will be well approximated from a local 
wavenumber by exp(-ikAx), where Ax = (Xb -- x,~)/N is the step size, making 
it easier to tie the filters directly to a model of the underlying wave mechanics: 
N 
H ~ 1~ exp(-ik(w, xj)Ax) 
j=l 
Therefore, independent of the details and dimensionality of the underlying 
wave mechanics, the responses of the cochlea at a sequence of places are equiv- 
alent to the responses at the outputs of a sequence of cascaded filters. The 
LG method constrains the design of those filters when the underlying physics 
is known. 
How does the filter relate to the wavenumber? For a given value of Ax 
in a uniform medium, the filter and the wavenumber are in 1-to-1 correspon- 
dence via the complex exponential. For a given pair of places xa and Xb in a 
nonuniform medium, the filter is determined uniquely by the function k(w, x), 
although the inverse is not necessarily true (i. e., a different k(w, x) with the 
same integral on that interval, such as a spatial reversal of k(~, x), also would 
be a solution). 
Even for nonlinear and time-varying wave mechanics, we can reasonably as- 
sume that a nonlinear and time-varying filter cascade will be a useful structural 
analog and a fruitful modeling approach: that of modeling local behavior with 
local circuits. The approach is neuromorphic in the sense that it is based on the 
form of wave propagation present in this peripheral part of the sensory nervous 
system. 
If the cochlea's frequency-to-place map is approximately logarithmic, and we 
model equal place increments with filter stages, then the model stages will have 
characteristic frequencies (or time constants) in nearly a geometric sequence. 
We often assume a geometric sequence in model calculations, but the method 
is more flexible and can be used to match realistic cochlear maps in which the 
low-frequency region maps nearly linearly to place. 

FILTER CASCADES 
7 
1.3 
POWER FLOW AND ACTIVE GAIN 
The LG method goes one step further than we just described in providing 
techniques to compute a slowly space-varying amplitude factor to account for 
the varying relationship between wave amplitude and power in a system with 
nonuniform energy-storage mechanisms. For example, if BM stiffness is vary- 
ing, the proportionality between squared volume displacement and potential 
energy is changing; the amplitude of a BM displacement wave needs to be ad- 
justed accordingly. For our purposes, we shall typically jump up one level of 
abstraction by imagining that our wave amplitudes are given in terms of de- 
rived variables, such that constant amplitude corresponds to constant power. 
Therefore, the gain of the filters will be exactly 1 in regions that are passive and 
lossless, as is typical of extremely low frequencies, relative to the characteristic 
frequency (CF) of the cochlear place under consideration. 
For more specific information on hydrodynamic modeling and the LG 
method, relative to analog VLSI implementations and numerical methods, see 
the dissertation of Lloyd Watts [26]. 
We have not yet said how the filters in a cascade should look--we have said 
only that the design can be constrained by models at a lower level. We discuss 
specific filters in Section 1.5. In general, we expect cochlear filters to be passive 
and linear for low frequencies, to provide active gain or power amplification for 
frequencies near CF, and to attentuate high frequencies. Therefore, filters of 
the class of interest have unity gain at DC, followed by a gain somewhat greater 
than unity, and a high-frequency gain less then unity. 
If the model has many such filters in cascade, then the individual filter stages 
do not need to have gains far from unity for the cascade to achieve an aggregate 
pseudoresonant [5] response, with a high peak gain and a large high-frequency 
attenuation. 
The notion of a pseudoresonance differs in a fundamental way from that of 
a resonance, with which engineers generally are familiar. A pseudoresonance is 
a broadly tuned gain bump that results from the collective behavior of a large 
number of broadly tuned (and hence low-precision) stages (or poles), or of a 
distributed system. A resonance, on the other hand, becomes narrowly tuned 
and needs high precision to achieve a high gain at its center. Reliance on such 
a collective computation is another hallmark of the neuromorphic approach. 
1.4 
WIDE-DYNAMIC-RANGE COMPRESSION VIA FILTER 
CASCADES 
One of the most important nonlinear functions of the cochlea is the compres- 
sion of a wide range of sound intensities into a narrower range of cochlear- 
motion intensities at the sensor array, for frequencies near CF. Studies of 
cochlear mechanical response since about 1970 [19] have repeatedly demon- 
strated this frequency-dependent compression in live cochleae, and its absence 
in dead cochleae. 

8 
NEUROMORPHIC SYSTEMS ENGINEERING 
In live cochleae, the overall input output intensity curves for frequencies near 
CF have a slope of typically 0.25 to 0.5 on a log-log plot [20]. This reduced slope 
is known as a compressive nonlinearity. The exact slope depends on the quality 
of the experimental preparation, on the frequency and intensity range, and on 
whether the response is measured at a fixed frequency or at the frequency of 
greatest response, which shifts a little with level. A higher compression (slope 
0.25, or 4-to-1 compression) is more typical at a fixed frequency at or above the 
most sensitive frequency (CF), and a lower compression (slope 0.5, or 2-to-1 
compression) is more typical at the peak response frequency, as the peak moves 
to frequencies below CF at higher sound levels. 
From our filtering point of view, we need level-dependent filters whose gains 
decrease as the signal level increases, to model this mechanical compression. 
Equivalently, we expect that the imaginary part of the wave number will change 
with level, even changing its sign between damping and amplification for some 
combinations of frequency and place. 
Presumably, the dependence of wavenumber on level comes from nonlineari- 
ties in the biomechanics, including the outer hair cells, which are the presumed 
source of the energy needed to provide active gain. These mechanical changes 
modify the way that traveling waves pick up energy, and the resulting cascaded 
filters that model a set of different places are a reflection of the underlying wave 
mechanics. Therefore, a filter-cascade model can, in principle, exhibit a range 
of behaviors similar to those of the mechanical system. 
As a wave picks up energy in traveling across a range of places, each little 
increment of place needs to contribute only a small amount of gain. If the 
filter-cascade model has stages that model small Ax regions, then each filter 
will need to contribute only a small gain; as the overall gain changes, each filter 
will have to change only slightly. 
A power gain, or a filter gain greater than unity, is correlated in this view 
with an active process that we think of as providing an active undamping-- 
effectively a negative viscosity. But even if we do not rely on this notion of a 
literal power gain, the variable-gain filter-cascade structure provides a qualita- 
tive functional model of the variable-gain behavior observed in the cochlea--it 
could be adapted to fit the wave mechanics of a passive model. Indeed, the ba- 
sis for our first use of the filter-cascade technique [11] was a passive long-wave 
analysis [29]. 
In our earlier model [11], we added the gain variation after the filters as 
a functional afterthought, so the model did not have the right frequency- 
dependent properties, such as linearity at low frequencies. Because it incor- 
porates gain variation directly into the cascade as filter-parameter (Q) varia- 
tion, the filter-cascade approach inherently achieves a reasonable constraint on 
how the overall filter gain can vary with frequency and place: The different 
places share most of the same cascade filters. That is, it is not possible to have 
a high peak gain at one place and a low peak gain at a nearby place, even 
if the cascaded filters vary substantially, because the composite responses at 
nearby places share most of the same filters. This property arises because we 

FILTER CASCADES 
9 
are modeling a wave propagation directly, again illustrating the benefit of a 
neuromorphic approach. 
1.5 
FITTING A FILTER CASCADE TO THE COCHLEA 
For most models of the mechanics of cochlear wave propagation, the qualita- 
tive behavior of a stage is just this: the filtering provides a gain bump for 
frequencies "near" CF, and provides attenuation at higher frequencies. What 
are the simplest lumped-parameter filters (i.e., small sets of poles and zeros in 
Laplace or Z-transform spaces) that can model this qualitative behavior? How 
significantly do the details of the stage filter affect the overall pseudoresonant 
response of the cascade? We answer these questions using examples. 
The simplest stage is a two-pole filter, which we refer to as a second-order 
section (SOS). The SOS as commonly used in digital signal processing might 
have either just two poles and no zeros in the simplest case, or might include 1 
or 2 zeros in a higher-complexity alternative. We have focused on the simpler 
all-pole version in recent years [27, 18], whereas our earlier work used both 
poles and zeros [15, 11, 12, 16, 23]. 
An active-undamping approach to a physical basis for a wavenumber solu- 
tion [18] led us to believe that simple two-pole filters may be not sharp enough 
(have narrow enough relative bandwidth) to be realistic, and that a three-pole 
filter would be a closer match. But two poles and two zeros can accomplish 
the same sharpening if that is what we need to fit experimental or theoretical 
data. In spite of this sharpness discrepancy relative to our particular mechan- 
ical model, we see the two-pole filter as a good and useful model of cochlear 
function. We should not rule out this simplest approach without a compelling 
reason. 
Figure 1.2 shows a composite pole-zero diagram representing four alternative 
designs for a single filter stage: two-pole, three-pole, two-pole/two-zero, and a 
sharper two-pole/two-zero designs. The corresponding stage transfer-function 
gains and group delays are shown in Figures 1.3 and 1.4, respectively. 
The 
two-pole/two-zero designs have sharper drops just beyond CF than the all-pole 
designs, but then level out at some gain less than I, rather than continuing to 
drop. 
Figure 1.5 shows the composite gains of long cascades of geometrically spaced 
stages, and Figure 1.6 shows the corresponding total group delays. Note that 
we can sharpen the two-pole response by adding either an extra pole or a pair 
of zeros; the resulting cascade gains can be made similar, but adding poles 
adds to the delay, whereas adding zeros reduces the delay. This dimension of 
flexibility may be useful if we wish to match the model to a delay or phase 
measurement. 
Moving the zeros closer to the poles and closer to the imaginary axis in the s 
plane results in more sharpening, especially of the high side, of the overall filter. 
This configuration fits the notch transfer function of a long-wave mechanical 
approximation, and was the basis of our original cochlea model [ii]. We now 

10 
NEUROMORPHIC SYSTEMS ENGINEERING 
O 
2-pole,2-zero ~ 
o 
x 
2-pote yc'x 
3-pole ~.._~ ~x 
2-pole,2-zero UO 
sharper 
s-plane 
Figure 1.2 
Composite pole-zero diagram. Four different filter designs are specified within 
this composite diagram, so that the pole and zero positions can be compared. Since complex 
poles and zeros are inherently in a complex-conJugate relationship, we label several in the 
top half-plane and several in the bottom half-plane to reduce clutter. 
~0 
-5 
-10 
-15 
-2( 
Stage Gain 
I 
O.1 
 o-pole 
~ 
sharper 
\,~,.x,, 
two-pole, 
.:\,\ 
two-zero 
........ 
".... ~ '\ 
." ." ..'" 
â¢ 
" '~",~ two-pol~ 
~ 
- 
three- '"~2~1'2 ~ 
Freque'ncy (ar~itrawu'nit's) ' ' '1'.0 
Figure 1.3 
Stage transfer-functions gains. For each of the four filter designs of Figure 1.2, 
the magnitude of the stage transfer function is plotted. 
believe that that model was too sharp, due to the unsuitability of the long- 
wave approximation for modeling the real cochlea near CF. At the time, we 
used too-s.harp filter models by trying to match transfer functions to iso-rate 

FILTER CASCADES 
11 
7 
3 
Stage Group Delay 
i ; 
: ! 
' 
" 
i 
.... 
t_hr_ee_-po_l_e _ _ ~ 
/... 
\ 
./" 
..." 
I. 
\\ \,, 
â¢ 
~ 
..... 
â¢ 
snarper. 
~. 
~, 
............. 
~._ - 
two:pole/ 
-x~.,, 
........... 
' .... 
two ~ero ~ 
~ 
.... 
0'.1 
Freqt~ency (~rbit~ary'unitsj " ' i.0 
Figure 1.4 
Stage group delays. For each of the four filter designs, the group delay of the 
filter stage is plotted in arbitrary units. 
tuning curves; that approach is clearly inappropriate, as we have explained 
subsequently [13]. 
1.6 
CASCADE-PARALLEL FILTERBANKS 
Our original filter model [11] was based on the longwave approximation to 
cochlear mechanics, in which a significant membrane mass leads to a true local 
resonance. We used a cascade of notch filters (two-pole/two-zero) to model 
pressure-wave propagation, plus a resonator at each tap of the cascade to con- 
vert pressure to BM displacement or velocity locally. This cascade-parallel 
architecture may still be a useful way to separate the propagated variable from 
the sensed variable, and possibly to simplify the required filters. For the pur- 
poses of that model, however, we noticed that, by constraining the parameters 
and rearranging some poles, we could easily convert the structure to a roughly 
equivalent pure cascade version, saving complexity and computation [12]. 
Figure 1.7 shows the cascade-parallel structure; contrast it with Figure 1.1. 
Notice that the output taps of the cascade-parallel structure are still always 
related by a relative transfer function, such that a pure cascade equivalent 
version exists, although stability of the exactly equivalent pure cascade is not 
ensured unless the parallel filters have stable inverses. 

12 
NEUROMORPHIC SYSTEMS ENGINEERING 
40 
20 
-20 
--40 
-60 
CascadeOain 
twÂ°-pÂ°lÂ° 
~'.~ 
: ~.i\ two-pole, 
_
~
~
-
 
~ 
i!ii\ two-zero 
three-pole 
" ~:i!)i~ ~ 
three-pole 
i(. 
\~! 
shamer 
: ~ ~ 
two-pole'" 
-r -, 
9 i 
~o-zero 
i~,~ 
)~ 
I I 
ti 
(~ 
.
.
.
.
 
~ 
.
.
.
.
.
.
 
I"l 
O. 1 
Frequency (~rbitraw units) 
1.0 
Figure 1.5 
Aggregate transfer-functions gains. For each of the four filter designs, the gain 
of a long geometrically-spaced cascade is plotted. The scale is arbitrary, because it depends 
on the density of stages per octave. 
1.7 
NONLINEAR EFFECTS 
A filter cascade can have an overall strongly compressive nonlinear input- 
output behavior, if the stages are weakly compressive. There are two general 
forms of nonlinearity that are important to consider, and it is likely that both 
operate in the real cochlea: instantaneous nonlinear distortion, and feedback 
control of a peak-gain parameter. For example, the nonlinear model of Kim 
and his colleagues [8] is just a cascade of two-pole filters with a compressive 
nonlinearity in each stage; Kim's later suggestions [7] are of the parameter- 
feedback form, and are motivated as a functional role for the auditory efferent 
system. 
An instantaneous distortion nonlinearity, such as a hyperbolic tangent that 
puts a firm saturation limit on the amplitude out of each stage, leads to the gen- 
eration of intermodulation products, such as the cubic-difference tone 2fl - f2. 
Distortion products are mostly generated where the primary components (fl 
and f2 in this case) are large, and distortion products with frequencies below 
the primaries are then free to propagate farther to their own place. Frequencies 
above and below the primaries can sometimes be detected propagating back out 
of a real cochlea, but unidirectional cascades cannot model that effect. 
As discussed in Section 1.4, we can use feedback of a detected output level 
to affect the filter parameters, implementing a less distorting amplitude com- 

FILTER CASCADES 
13 
140 
120 
100 
~ 80 
,~ 
.~60 
c~ 
" 4o 
m 20 
Cascade Group Delay 
,' 
,, 
/ 
\ 
I I 
~\ 
/ 
~ 
~ 
\ 
/ 
\ 
three-pole 
,, - - 
", 
t
w
o
-
~
"
 
' 
t/ ... 
.: 
\. 
\ 
~" 
f 
'. 
'\. 
~,. 
." 
". 
\ 
â¢ ~ 
.. 
.. 
"\ 
two-pole, tw_orz_ero ..... " 
... 
, 
. . . . . . . . . . . . . .  
. 
'.. 
.~ 
......... ;hamer 
, 
................................... 
~o-pole, ~o-zero '".. 
'.,. 
, 
, 
, 
, 
I 
.
.
.
.
.
.
 
"'~'" 
.......... 
0.1 
Frequency (a'rbitra~ units) 
1.0 
Figure 1.6 
Aggregate group delays. For each of the four filter designs, the group delay of 
the cascade is plotted in arbitrary units. 
Cascade/Parallel Structure 
Figure 1.i' 
The cascade-parallel structure. This cascade-parallel filter configuration can 
model the response of the cochlea with more flexibility than the pure cascade structure has. 
pression known as automatic gain control (AGC). A small reduction in the pole 
Q of each stage in response to output energy can lead to a highly compressive 
overall response. This AGC is one purported function of the auditory efferent 
innervation: to tell the outer hair cells to turn down their level of activity [7]. 
Sound in one ear is even known to drive the efferent neurons to the contralateral 
cochlea, perhaps to keep the gains of the two ears more synchronized than they 
would be otherwise, and thus to aid the brain in binaural level comparisons [1]. 

14 
NEUROMORPHIC SYSTEMS ENGINEERING 
1.8 
SILICON VERSUS COMPUTER MODELS, AND PRACTICAL 
PROBLEMS 
We would need a high-power programmable processor to implement a filter- 
cascade model of the cochlea in real time. Dedicated silicon implementations, 
on the other hand, can be made with less silicon and much lower power con- 
sumption [27, 12, 27]. An important cost factor in analog or digital sampled- 
data implementations is that avoidance of aliasing in the nonlinear opera- 
tions that follow the cochlea model requires a substantial oversampling fac- 
tor. By avoiding clocks, sampling, and high-speed circuits, the continuous- 
time analog approach yields by far the lowest power consumption, but requires 
novel solutions to noise, matching, tuning, stability, and communication prob- 
lems [3, 6, 11, 25, 24, 25, 32, 90]; see also the other papers on neuromorphic 
analog cochleae in this book. 
1.9 
LIMITATIONS 
As we mentioned in Section 1.7, distortion products can propagate backward 
out of a real cochlea, but not out of a unidirectional filter-cascade model. This 
limitation applies to other otoacoustic emissions as well, both stimulated and 
spontaneous. So the filter cascade is not a suitable modeling substrate for such 
effects. 
The filter-cascade model is based on looking at a set of points along only 
one dimension, and as such provides no direct help in our understanding the 
motion of other parts of the cochlea or in the cochlea's fluid. Micromechanical 
models, 2D and 3D models, and other modeling approaches can help to inform 
the design of the filter cascade, but the filter cascade then captures only a slice 
of the more detailed models. 
Any small stretch of cochlear transmission line acts approximately as the 
filter exp(-ikAx); but does this filter, or an approximation to it, have useful 
properties, such as stability or causality? The filter specification derived from 
the wavenumber tells us what happens at all sine frequencies; to address sta- 
bility, however~ we need to consider approximate filter models with poles and 
zeros. We believe, but have not proved, that, if the mechanical wave system is 
stable, then stable rational filters exist that are reasonable approximations to 
the system's frequency response. 
Causality is more complicated, because the response at a point is not phys- 
ically caused by only the action at a different point upstream, even under 
the unidirectional assumption; rather, it is caused by the combined actions of 
nearby points in the whole 2D or 3D motion of the medium. The resulting 
filter, or approximations to it, could conceivably show precursors in response 
to an impulse. So, if we design a filter with the right magnitude frequency 
response, causality may force the filter to have excess delay if the Ax value 
is short compared to the wavelength. Thus, fine division of the place dimen- 
sion may make it increasingly difficult to get the phase right in low-order filter 
appoximations--especially in the case of causal all-pole filters. Adding zeros 

FILTER CASCADES 
15 
helps us to cancel some of the delay of the poles, thus making it easier for us 
to develop a model with reasonable phase. Kates [6] has explored one class of 
filter cascades using zeros to arrive at lower overall delay. 
The LG method breaks down in the cochlea in the cutoff region, where the 
eikonal has multiple complex solutions for k. In this region, the wavenumber 
changes so rapidly that there is effectively a mode coupling phenomenon that 
allows energy to couple into several of these different wave modes, which in- 
terfere with each other in complex ways, and which decay more slowly with x 
than does the original mode [26]. The resulting high-frequency irregularity, or 
plateau, in the response gain, which is found in numerical 2D solutions and is 
sometimes observed in real cochleae, is not easily modeled by cascades of simple 
filters. This discrepancy is an obstacle not to the concept of a filter cascade, 
but rather to the modeling details and to the desire to use simple stages. It 
seems likely, however, that the high-frequency plateau has no functional im- 
portance in normal hearing. Complicated response patterns due to cochlear 
micromechanics may lead to similar considerations, depending on one's goals 
in modeling. 
1.10 
RELATION TO OTHER APPROACHES 
The most common functional approach to computational models of the cochlea 
is the bandpass filterbank. In this approach, every place to be modeled has 
its own filter, which is designed to match experimental data. Because there is 
usually no good basis for constraining a filter design using poles, an all-zero 
(transveral or finite impulse response) filter is often employed. Both of these 
features--independent filters and lack of poles--make the implementation of 
this approach computationally expensive. 
Filterbanks that use poles--such as the gammatone filterbank (GTF) and its 
all-pole variant (APGF) [14, 23J--are becoming more widely used, because of 
their efficiency and simpler parameterization. The GTF is popular, but has an 
inappropriate symmetric passband; the APGF is closely related to a cascade 
of two-pole filters, and is therefore much more realistic in terms of transfer 
function and of the possibility of parametric nonlinearity. 
An analog silicon model of the cochlea that can propagate waves bidirection- 
ally has been reported by Watts [27]. It uses a 2D resistive grid as a substrate 
for directly solving Laplace's equation for wave propagation in a 2D fluid model 
of the cochlea, with second-order filters along one edge modeling the BM-fluid 
interaction. This approach needs to be further developed to see whether it 
leads to an overall advantage in implementing an effective cochlea model. A 
potential problem is that irregularities or the inherent spatial discretization 
may lead to reflections that cause instability, as has sometimes been a problem 
in 2D numerical solutions of active cochlea models. 

16 
NEUROMORPHIC SYSTEMS ENGINEERING 
1.11 
CONCLUSIONS 
The filter-cascade structure for an cochlea model inherits two key advantages 
from its neuromorphic roots: efficiency of implementation, and potential re- 
alism. Both the filter transfer functions, in terms of magnitude and delay 
dispersion, and the nonlinear behaviors of the cochlea, in terms of distortion 
and adaptation, are modeled realistically under the constraints imposed by the 
cascade. Minor problems, such as excess total delay in the finely discretized 
all-pole version, are tolerable in practical applications. 
Analog VLSI implementations of the filter-cascade cochlea model are cur- 
rently being explored at a number of laboratories around the world. The ideal 
of a practical micropower real-time artificial cochlea circuit is rapidly coming 
closer to reality. 
Acknowledgments 
I thank Malcolm Slaney, Lloyd Watts, John Lazzaro, Rahu] Sarpeshkar, and Carver 
Mead 
for their contributions to the work discussed. And I thank Tor Sverre Lande 
for encouraging me to write this paper, and Lyn Dupr~ for skillfully editing several 
drafts of the manuscript. 
References 
[1] J. F. Brugge. An overview of central auditory processing. In A.N. Popper 
and R.R. Fay, editors, The Mammalian Auditory Pathway: Neurophysiol- 
ogy, pages 1 33. Springer-Verlag, 1992. 
[2] E. de Boer and R. MacKay. Reflections on reflections. J. Acoust. Soc. 
Am., 57:882-890, 1980. 
[3] P. l~rth and A. B. Andreou. A design framework for low power analog filter 
banks. IEEE Transactions on Circuits and Systems, Part I: Fundamental 
Theory and Applications, 42:966-971, 1995. 
[4] P. Furth and A. B. Andreou. Linearized differential transconductors in sub- 
threshold CMOS. IEEE Electronics Letters, 31(7):545-547, March 1995. 
[5] M. Holmes and J. D. Cole. Pseudo-resonance in the cochlea. In Boer E. 
de and Viergever M.A., editors, Mechanics of Hearing. Martinus Nijhoff 
Publishers, the Hague, 1983. 
[6] J. M. Kates. A time-domain digital cochlear model. IEEE Trans. Signal 
Processing, 39:2573 2592, December 1991. 
[7] D. O. Kim. Functional roles of the inner- and outer-hair-cell subsystems 
in the cochlea and brainstem. In Berlin C., editor, Hearing Science, pages 
241-261. College-Hill Press, San Diego, 1984. 
[8] D. O. Kim, C. E. Molnar, and R. R. Pfeiffer. A system of non-linear 
diferential equations modeling basilar membrane motion. J. Acoust. Soc. 
Am., 54:1517-1529, 1983. 

FILTER CASCADES 
17 
[9] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie. 
Silicon auditory processors as computer peripherals. 
IEEE Journal of 
Neural Networks, 4(3):523-528, 1993. 
[10] W. Liu, A. Andreou, and M. Goldstein. Voiced-speech representation by 
an analog silicon model of the auditory periphery. IEEE Transactions of 
Neural Networks, 3(3):47?-487, 1992. 
[11] R. F. Lyon. A computational model of filtering, detection and compression 
in the cochlea. In Proc. IEEE Intl. Conf. on Acoust. Speech and Signal 
Proc., pages 1282-1285, Paris, 1982. 
[12] R. F. Lyon. Computational models of neural auditory processing. In Proc. 
IEEE Intl. Conf. on Acoust. Speech and Signal Proc., San Diego, 1984. 
[13] R. F. Lyon. Automatic gain control in cochlear mechanics. In P. Dallos 
et al., editor, The Mechanics and Biophysics of Hearing, pages 395-402. 
Springer-Verlag, 1990. 
[14] R. F. Lyon. All-pole auditory filter models. In E. Lewis, editor, Diversity 
in Auditory Mechanics. World Scientific, In press. 
[15] R. F. Lyon and L. Dyer. Experiments with a computational model of the 
cochlea. In Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Proc., 
pages 1975-1978, Tokyo, 1986. 
[16] R. F. Lyon and N. Lauritzen. Processing speech with the multi-serial signal 
processor. In Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Proc., 
Tampa, 1985. 
[17] R. F. Lyon and C. Mead. An analog electronic cochlea. IEEE Trans. 
Acoust., Speech, Signal Processing, 36:1119-1134, July 1988. 
[18] R. F. Lyon and C. Mead. Cochlear hydrodynamics demystified. Caltech 
Computer Science Technical Report Caltech-CS-TR-88-4, Caltech, 1989. 
[19] W. S. Rhode. Observations of the vibration of the basilar membrane in 
squirrel monkeys using the MSssbauer technique. J. Acoust. Soc. Am., 
49:1218-1231, 1971. 
[20] L. Robles, M. A. Ruggero, and N. C. Rich. Basilar membrane mechanics 
at the base of the chinchilla cochlea, input-output functions, tuning curves 
and response phases. J. Acoust. Soc. Am., 80:1364-1374, 1986. 
[21] R. Sarpeshkar, Lyon R. F., and C. A. Mead. An analog VLSI cochlea with 
new transconductance amplifiers and nonlinear gain control. In Proc. IEEE 
Intl. Conf. on Circuits and Systems, volume 3, pages 292-295, Atlanta, 
May 1996. 
[22] R. Sarpeshkar, Lyon R. F., and C. A. Mead. Nonvolatile correction of 
q-offsets and instabilities in cochlear filters. In Proc. IEEE Intl. Conf. on 
Circuits and Systems, volume 3, pages 329-332, Atlanta, May 1996. 
[23] M. Slaney. Lyon's cochlear model. Apple Technical Report 13, Apple 
Computer, Cupertino, CA, 1991. 

18 
NEUROMORPHIC SYSTEMS ENGINEERING 
[24] C. Summerfield and R. F. Lyon. ASIC implementation of the lyon cochlea 
model. In Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Proc., San 
Francisco, 1990. 
[25] A. van Schaik, E. Fragni~re, and E. A. Vittoz. Improved silicon cochlea 
using compatible lateral bipolar transistors. 
In David S. Touretzky, 
Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neu- 
ral Information Processing Systems, volume 8, pages 671-677. The MIT 
Press, 1996. 
[26] L. Watts. Cochlear Mechanics: Analysis and Analog VLSI. Ph.d. disser- 
tation, California Institute of Technology, 1993. 
[27] L. Watts, Lyon R. F., and Mead C. A bidirectional analog VLSI cochlear 
model. In C. Sequin, editor, Advanced Research in VLSI, Proceedings of 
the 1991 Santa Cruz Conference, pages 153-163, Cambridge, MA, 1991. 
MIT Press. 
[28] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692-700, 
May 1992. 
[29] G. Zweig, R. Lipes, and J. R. Pierce. The cochlear compromise. J. Acoust. 
Soc. Am., 59:975-982, 1976. 

2 
AN ANALOGUE VLSI MODEL OF 
ACTIVE COCHLEA 
Eric Fragni~re I, Andr~ van Schaik I and Eric A. Vittoz 2 
1 MANTRA Centre for Neuromimetic Systems, Department of Computer Science, 
Swiss Federal Institute of Technology (EPFL), CH-I015 Lausanne, Switzerland 
fragniere@di.epfl.ch 
2 CSEM, Centre Suisse d'Electronique et de Microtechnique S.A., 
Jaquet-Droz 1, CH-2007 NeuchStel, Switzerland 
2.1 
INTRODUCTION 
In the last decade, analogue electronics has been almost confined to the con- 
version between data of the physical world and its abstraction by numbers, in 
order to process it with efficient digital computers. Nature, however, could not 
wait for the development of computer science and creatures developed vari- 
ous strategies to interact with their environment. These interactions consist of 
sensing the environment and producing an action on it under the control of a 
decision. To be efficient, this decision is based on a set of representations of the 
environment best suited to the usual action to be taken: the perception of the 
environment. In applications where the perception/decision scheme can be ap- 
plied such as pattern recognition, there is a growing interest to take inspiration 
from strategies developed by nature, especially where computer algorithms still 
fail to be as efficient as their natural counterparts. Analogue VLSI techniques 
seem best suited for an efficient implementation of the perceptive functions as 
an Analogue/Perceptive converter: data of the physical world is converted into 
relevant perceptive information rather than into a sequence of numbers allowing 
its perfect restitution, like in conventional Analogue/Digital converters [20]. 
In automatic speech recognition (ASR), the decision is nowadays usually 
handled by efficient statistical classifiers based on Hidden Markov Models 
(HMMs). However, the efficiency of HMMs strongly relies on restrictive as- 
sumptions on its input [3], which are generally not fulfilled by the preprocessing 
stage of most ASR systems. Attempts to use models of the auditory pathway 

20 
NEUROMORPHIC SYSTEMS ENGINEERING 
as preprocessing stages [14] still do not really improve the speech recognition, 
particularly in noisy environments [4]. The main reasons are that the result- 
ing classifier's input are not in accordance with its working assumptions and 
that the speech model itself is not realistic. An alternative Markov-like recog- 
nition model best adapted to speech signals is currently being developed [29]. 
It models speech as a sequence of relevant spectral and amplitude transitions 
only, instead of the usual sequence of statistical characteristics of stationary 
segments. Promising results are shown in [13], but a preprocessing stage that 
could efficiently emphasise and code these transitions is still to be developed. If 
an Analogue/Perceptive converter dedicated to transform a speech signal into 
relevant transitions can be identified in the auditory pathway, it should be used 
as a guideline for developing such a preprocessing stage. 
In the inner ear's cochlea, the input speech signal induces mechanical vi- 
bration on the basilar membrane (BM). The amplitude of these vibrations are 
frequency dependent, with a most sensitive response at a characteristic fie- 
quency (CF) which depends on the position along the BM. The inner hair 
cells (IHCs) transform the BM movement into a release of neuro-transmitters, 
activating the auditory nerve fibres. The temporal adaptation of this trans- 
duction is said to be responsible for the enhancement of intensity transitions 
measured on the auditory nerve in response to brief tone bursts [11]. Further 
in the auditory pathway, in the cochlear nucleus, the onset cells are neurons 
that spike only at the onset of the tone burst [15]. 
The inner hair cells and the onset cells are often mentioned as processing 
stages where transitions are enhanced. However, the transduction from acous- 
tic pressure to mechanical vibration on the BM already presents the required 
characteristics. In order to get a limited BM movement range for a wide input 
dynamic range, an active process that feeds energy back onto the BM at low in- 
put intensities is hypothesised. This process also explains the sharp frequency 
selectivity around CF measured at a given position of the BM at low input 
intensities [8]. As a consequence of this tuned adaptive gain control, intensity 
transitions near the CF are enhanced. 
Several computer models attempt to capture this active process, with various 
trade-off choices between model accuracy and computational efficiency [3, 5, 18], 
but even the most efficient ones could not run in real time at low cost 
and low power with enough detail. Analogue VLSI models seem to permit 
such performance. 
Lyon's silicon cochlea [27] and its successive improve- 
ments [32, 90] were already widely used as a basic structure for several efficient 
Analogue/Perceptive converters [2, 15, 10]. 
We propose in this paper an analogue VLSI model of an active cochlea based 
on the silicon cochlea [32]. In section 2.2, the model is carefully analysed. After 
a brief description of the functions required to model a single section of the 
cochlea, we outline its analogue VLSI implementation in order to estimate its 
crucial parameters. We show that an individual stage could not support the 
quality factor required to match the physiological data measured on biological 
cochleae. The pseudoresonance, which yields frequency selective high gains 

ACTIVE COCHLEA 
21 
despite limited quality factors in the individual stages is then presented and 
analysed in order to design an appropriate pseudoresonant gain control loop. 
A computer simulation performed on an amplitude gain model of the complete 
cochlea with a single frequency input signal demonstrates the validity of the 
concept. Conditions and behaviour of sustained oscillations that may occur 
in a single stage are determined analytically, while the stability analysis of 
the complete quality factor control loop is outlined. Section 2.3 proposes the 
analogue building blocks required for a VLSI implementation of the model, 
while the effect of the major technological limitations of a standard CMOS 
process are discussed. 
2.2 
THE MODEL 
After several decades of controversies since the first measurements of the BM 
movement, it has become accepted that the sharp frequency tuning at low input 
intensities and the compressive nonlinearities at high intensities are caused by 
an active process which feeds energy back onto the BM. This mechanism is 
attributed to the outer hair cells (OHCs) which present motile properties in 
addition to their sensitive abilities [6]. The effect of the OHCs can be seen as 
a local adaptation of the BM quality factor. 
2.2.1 
Functional model 
Figure 2.1 shows the proposed feedback loop that controls the BM damping 
with the cochlea's appropriate output signal. Similarly to [27, 32, 90], the 
BM is modelled by a cascade of second-order lowpass filter stages. Each stage 
Hk(s) = 1/(s2~-~ + s~-k/Qk + 1) filters the BM displacement db,~ at the out- 
put of the previous stage with a cutoff frequency fck = 1/27rrk and a quality 
factor Qk expressing respectively the CF and the effect of the OHCs at this 
particular stage. The CF of each stage decreases exponentially from the base 
(first stage) to the apex (last stage) of the cascade. Each stage Hk thus mod- 
els a BM section located at a discrete position k along the BM. Because the 
IHC's stereocilia are in a viscous medium, they detect the velocity of the BM 
movement relative to its supporting structure. The IHC model thus includes a 
differentiator Dk(s) = s~-~ which converts the BM displacement dbm into BM 
velocity Vb,~ [32, 90]. In addition the transmitter release in the IHC happens 
only when its stereocilia bend in one direction. The IHC performs therefore 
at first approximation a half-wave rectification of the BM velocity signal Vb,~, 
yielding the IHC output Yihc o~ IVbm]. In this first approach, the temporal adap- 
tation of the transmitter release [11] has been neglected, assuming for simplicity 
the effect of the OHC's active process to be dominant for temporal adaptation 
and transient enhancement. It is reasonable to assume that, in the auditory 
pathway, the mean value of the BM velocity can be estimated at a higher level 
as a mean spiking rate (MSR) on the afferent nerve fibres projected from the 
IHC at the corresponding BM position. As the dynamics of transmitter release 
has been neglected, the instantaneous spiking rate (or spiking probability) is 

22 
NEUROMORPHIC SYSTEMS ENGINEERING 
roughly proportional to the IHC output Yihc, thus a measure of the mean BM 
velocity Y,~s~ ~ IVbm] may be available at the OHC's efferent nerve fibres. The 
adaptive loop is closed by controlling the quality factor of the BM, assumed to 
be the action of the OHC, using an appropriate quality factor modulation by 
this measure Y,~sr of the mean BM velocity. 
MEAN SPIKING RATE (MSR) 
Figure 2.1 
Active cochlear filter cascade, with functional block diagram detailed for one 
stage. 
2.2.2 
Quality factor modulation 
The effect of the OHCs is performed by controlling the quality factor Qk of 
each stage Hk(s). On a single stage H(s)D(s), a sine wave input signal at 
frequency f and with a peak amplitude X produces an output signal having a 
peak amplitude Y = [[H(f)D(f)[]X. At a frequency f close to the CF of the 
stage, the amplitude gain nH(f)D(f)n ~ Q, thus the output signal has a peak 
amplitude Y ~ QX. Modulating the quality factor with a power A of the peak 
output amplitude Y 
Q o~ Y ~ 
(2.1) 
yields 
Y ~ x ~--~, 
(2.2) 
where A < 0 produces the desired compressive input-output function. 

ACTIVE 
COCHLEA 
23 
2.2.3 
AnMogue model 
In the analogue model proposed in [27, 32, 90], the cutoff frequency f~k = 
1/2~r~'k of a second-order stage Hk is determined by ~-k = C,/9,~. 
The capac- 
itances C~ of the BM block are identical for every stage and the transconduc- 
tances 9m~k of its OTAs are controlled by the bias currents I,~ which decrease 
exponentially along the cascade. The quality factor Qk of any stage k is con- 
trolled by the ratio between the transconductances gmQk and g,~,k of its OHC 
OTA and its BM OTAs respectively. As all OTAs operate in weak inversion, 
their transconductance gm-r,Q is proportional to their bias current I,,Q. The 
quality factor is given by 
1 
Q - 
(2.3) 
2(1- a~)' 
where a -- 
g,,~Q/21~ is a constant. 
gm./I. 
A quality factor varying between Qmin and Q~ 
can thus be controlled by 
means of a translinear loop which imposes 
IQ_ Ier~a~ 
(2.4) 
L 
~c +~o 
Using normalised currents i~ = I~/Io and iQmax "= -[Qmax/Io, equation (2.3) 
can be expressed as 
1 [ 
2Q.c~x _~_1_ ] 
(2.5) 
Q = ~ l + 2Q,~axic + l j ' 
where 
1 
Qrnax : 
(2.6) 
2(1 - ~iQmox) 
is obtained for ic = 0 and Q,~n depends on the maximal available value of ic. 
The quality factor control signal ic is made proportional to the peak ampli- 
tude ?.b~ of the BM velocity analogue signal, 
ic = F~vbm, 
by the feedback gain 
F- 
gmaArg,~F 
(2.7) 
galo 
' 
which takes into account the IHC OTA transconductance g,ca, the IHC rectifier 
gain Ar between the DC component of the rectified BM velocity signal and it8 
peak amplitude I?vbm, the DC gain g,~F/ga of the MSR lowpass filter which 
extracts this DC component and the normalising current I0 (figure 2.2). 
The peak BM velocity l)vb,Â¢ is given by the amplitude gain of the BM stage 
[IH(f)II and its differentiator lID(f)l I for an input signal at frequency f having 
a peak amplitude l~din 
9vbm = IlH(f)D(f)II?din" 
(2.8) 

24 
NEUROMORPHIC SYSTEMS ENGINEERING 
The differentiation D(s) = s~- is simply performed by taking the difference 
between the output voltages of both BM OTAs [32], which yields 
8T 
~7- 
â¢ 
H(s)D(s) 
s~'r ~ + -~ + 1 
OHC 
lihc  
Figure 2.2 
Analogue model of one stage of the active cochlear filter cascade. 
2.2.4 
Level compression 
Except for the shift of variable limiting the quality factor between Qm~n and 
Q~ax, with ic proportional to the peak amplitude l~vbm of the output signal 
after stage tt(s)D(s), equation (2.5) respects the form of equation (2.1) with 
A = -1. The required compressive input-output relation expressed by equao 
tion (2.2) is thus obtained and the output peak amplitude ffvbm is proportional 
to the square root of the input peak amplitude ~'~. 

ACTIVE 
COCHLEA 
25 
The limited quality factor range implies that the square root compression is 
performed only on a limited input dynamic range. Under the same assumption 
as in section 2.2.2, equations (2.5), (2.7), and (2.8) yield 
X - 1 + ~/(1 - X) 2 + 8Q~xX 
(2.9) 
Y= 
4 
' 
where X = QmaxF~ai~ and Y = QmaxF~vb,~. This function has an asymp- 
tote limx-~ Y = X/2 and slope Qma~ at the origin X = 0. The square root 
compression is thus active between the corresponding gains Y/X = 1/2 and 
Y/X = Qm~x. According to physiological data [17], an input sound level of 60 
dB SPL corresponds to lnm BM displacement at frequencies below the CF of 
the measured BM position, thus where ]lHkt] ~ 1 according to our model. On 
the other hand, at frequencies close to the CF, the BM displacement equals the 
BM velocity since [ID~[[ ~ 1. A good correspondence between the physiologi- 
cal data [17] and equation (2.9) can be obtained for a peak BM displacement 
lower than 10nm (figure 2.3). At higher intensity values, a saturation that does 
not stem from the quality factor control loop appears. This saturation will 
correspond to the saturation of the physical devices used for the VLSI imple- 
mentation of the model. At lower intensities, the model fits the physiological 
data with a maximal quality factor Qm~x ~ 180, allowing an amplitude gain 
up to 45dB at CF for low input level. 
2.2.5 Pseudoresonance 
The quality factor required to match physiological data cannot be safely im- 
plemented on a single analogue second-order filter, because the mismatch of its 
internal components are likely to drive it into instability. Nevertheless, ampli- 
tude gains corresponding to such a high quality factor can be achieved using 
the pseudoresonance of the cochlear filter cascade. The control is made locally 
at every cascade stage in a narrow range of quality factors (between 1/~/~ and 
2), but the accumulation of these effects on the cascade allows gains up to 45dB 
for low intensity input levels. The compression of an 80dB input dynamic range 
into the 40dB output dynamic range corresponding to the physiological data 
of figure 2.3 is thus possible with reasonable individual quality factors. The 
frequency selectivity, however, cannot be as sharp as measured on biological 
cochleae, since it is determined by the quality factor of a single stage. 
The pseudoresonance results from the accumulation of the individual second- 
order lowpass filter gains Hk (s) all along the cascade [27]. If their CFs are close 
enough to each other to allow their resonant bumps to overlap, their multiplica- 
rive effect can lead to high overall gains at frequencies close to the pseudores- 
onant frequency fPn. Since the cascade consists of filters with decreasing CF, 
this cumulative effect occurs only on a limited number of filters, for which the 
individual gain is substantially greater than unity. Stages with a CF much 
higher than the signal frequency have no effect on this signal (gain close to 

26 
NEUROMORPHIC SYSTEMS ENGINEERING 
50 
40 
] 
20~ 
10 
0 
-10 
-2 0 
./ 
,~ 
. 
,' 
/ 
i 
â¢ 
/" 
.' 
/./," 
X 
" 
I 
'/ 
I 
I 
I 
" 
O 
20 
40 
," }~ 
80 
i00 
120 
~ 
~ / 
, 
/ 
; 
,' / 
IO00x 
[dB SPL] 
~ 
f=-i .5kH z<CF 
[] 
f= 3kH z<CF 
A 
f= 6 kHz<CF 
O 
f=-i 0kHz<CF 
â¢ 
f=-i 8kHz=CF 
Model 
I~kl~l 
Y=X/2 
Y=XQm ax 
Figure 2.3 
Compression curves; the boldface curve plots equation (2.9) for a quality factor 
Q = 180. The factor 1000 on X represents the gain between sound pressure level (SPL) 
and BM displacement at the input of the cochlea. The physiological data are from [17]. 
1), whereas the stages with a CF lower than the signal frequency rapidly at- 
tenuate the signal since their individual gain drops with 12dB/octave. The 
pseudoresonance therefore 
â¢ 
strongly amplifies frequencies f ~ fPR, 
â¢ 
sharply attenuate frequencies f > fPR, and 
â¢ 
has no effect on frequencies f < fPR. 
The decreasing CFs from the cascade's base toward its apex are necessary 
to allow the propagation of a signal modelling the BM travelling wave from the 
input of the first stage to the stage where it accumulates to its peak amplitude 
before rapidly fading away further in the cascade. In this model, the cutoff 
frequency of each stage in the cascade, being defined by its CF, decreases ex- 
ponentially by one octave every b stages from fco at stage 0, which is expressed 

ACTIVE COCHLEA 
27 
by 
fc~Â¢ -- f~o2 -~/~. 
(2.10) 
The transfer function between the input vibration at stage 0 and the BM 
displacement at the output of stage k is given by the product of the transfer 
function of all individual stages between the cascade input to the output of 
stage k. 
These transfer functions are advantageously expressed as functions of fre- 
quency normalised to the CF of stage k. Using equation (2.10), the transfer 
function between the input of the first stage in the cascade and the output of 
stage k is thus expressed by 
k 
k 
1 
(2.11) 
Gk( 
) = 
= I-[ 
t=0 
t=0 1-~t 2~ 
+2~2~- 
where gt = f/fck is the frequency normalised to the CF of stage k. 
The normalised pseudoresonant frequency ~tpn at the output of stage k can 
be estimated by finding the maximum Gpn = I IGk (~gn)ll of the amplitude gain 
function IIGk(~)ll. The pseudoresonance involves a limited number L of basal 
stages, since all stages 1 < k - L have a gain Ht(gt) close to unity. Therefore 
the product in equation (2.11) can start with the index l = k - L if k _> L. 
By a simple change of variables, the transfer function Gk+a(~t) after stage 
k + d can be expressed by Gk (~2 a/~) using equation (2.11) in which the quality 
factors Q~ must be shifted to Qt+a and the product must start with the index 
1 = -d instead of 0. Hence, provided that the stage k (or k+d 
ifd < 0) 
is far enough from the base to allow full pscudoresonance to build up, and 
with the same quality factor for all stages participating to pseudoresonance, 
G(x) = Gk(~ = 2 ~) expresses either the transfer function after stage k at a 
frequency x octaves away from its CF, or the transfer function after stage k + bx 
at the CF of stage k. 
Figure 2.4 shows the accumulation of individual amplitude gains t lH~ (~)II on 
a growing amount of basal stages (plot a) and the accumulated gain I IGa (~)II 
for several cases of resolution b (plot b) and quality factors Q (plot c). An 
important point to notice is that for 4 < b < 10 and for any resonant Q, the 
normalised pseudoresonant frequency ~-~PR is about 0.4 octaves higher than the 
normalised peak frequency ~p of the stage k alone. Since Gk (f~) also measures 
the gain of the stage k + b log2 ~ at the CF of stage k, this implies that the 
stage k + dpn with highest amplitude gain at the CF of stage k is located dpR 
stages apicalward from stage k, where 
dpR = b(0.4 + log~ V/1 - 1/2Q 2) 
is the pseudoresonant distance. For an individual quality factor Q between 1.5 
and 2, the pseudoresonant distance lies between 0.22b and 0.30b, while the pseu- 
doresonant gain GpR easily reaches the 45dB required to match physiological 
data as shown in section 2.2.4. 

28 
NEUROMORPHIC SYSTEMS ENGINEERING 
50 
40 
30 
20 
10 
0 
-10 
-20 
-30 
Q=2, b=6 
-1 
i 
f=t Pk 
i 
0 
1 
f/fck [oct] 
C~ 
-~ 4
0
~
 
~0 
0 -I 
0 
f/fck [oct] 
4
0
f
)
~
,
 
L>> 
0 
0.4 
1 
f/fPk [oct] 
Figure 2.4 
Pseudoresonance; (a) accumulation of amplitude gain of L individual stages 
basalward from stage k ((~k 
k 
= 1-Iz=k-L HI); resulting amplitude gains for several (b) 
resolutions (stages per octave) and (c) quality factors. 
2.2.6 
Quality factor control loop 
In order to locally control the pseudoresonant gain Gk (~) at the output of stage 
k, only the quality factors of the stages participating to the pseudoresonance 
have to be controlled. A natural way of doing this, is to distribute the signal 
controlling the quality factor over this range. 
The spatial distribution of the quality factors Q(d) = Qk+d on the pseu- 
doresonant portion around stage k depends on the distribution of the control 
signals it(d), in response to the distribution ~vb,~(d) of the peak BM velocity 
signals at the output of stages k + d. Similarly to the single stage presented 
in section (~2.2.3), the control signals it(d) are made proportional to the peak 
amplitude Yvb m (d) of the differentiators outputs, but they can interact between 
stages, which is expressed by the spatial convolution product 
it(d) = 
~ 
F(d- d')~Zvb,,,(d'), 
(2.12) 
d , = - c:x~ 
where F(d) represents the spatial distribution of the feedback gains around 
any stages k in the cascade. In the single stage control loop of section 2.2.3, 
F(d) = F for d = 0 and F(d) = 0 otherwise. 
As mentioned previously, the spatial distribution around stage k of the cas- 
cade's output responses at the CF of stage k can be expressed using the gain 

ACTIVE COCHLEA 
29 
function of equation (2.11) in which Ql+d must be used instead of Qt. Since 
the equivalence between stage index shift d and relative frequency ~t applies 
also to the differentiators, the spatial distribution ~vb,~(d) around stage k can 
be expressed by 
~vb,~(d) = IIGD(d, ~)ll~di~O, 
(2.13) 
where IIGD(d, f~)ll = IIGk(~2d/~)]l~2d/~ is the amplitude gain of the cascade 
after stage k + d, including the differentiator Dk+d, at frequency ~fck, while 
~'din0 is the peak amplitude of the cascade input signal. 
Hence, using equations (2.5), (2.12), and (2.13), the distribution of the qual- 
ity factor Q(d) on stages k + d can be calculated by 
l I 
2Q,~ax-i 
] 
(2.14) 
Q(d) = ~ 1+ 2Qmaz~dmOGOL(d) + 1 ' 
where the open loop gain distribution GoL(d) = E~,_-~ F(d-d')IIGD(d', ~t)I] 
is the spatial convolution product of the feedback gain distribution function 
F(d) and the cascade amplitude gain distribution llGD (d, ~t)II. 
Since IIGD(d)ll--and, thus, GoL(d)--depends on the quality factors distri- 
bution Q(d), equation (2.14) has the form of a nonlinear system of equations 
Q(d) = Q(d, ~d~O, Q(-L), ...Q(M)). The size of the system to solve is given 
by the range -L > d > M for which ~d~oGoL(d) substantially modulates the 
quality factors Q(d). 
The amplitude gains IIGD(d)I I with identical quality factors Q(d) = Q Vd 
can be approximated using the normalised gain gD (d) o( Q-pc I I GD (d + dpR)II 
which has its maximum at d = 0 (figure 2.5, plot a). The modulation of 
the quality factors by control signals ic(d) having the same distribution as the 
normalised gain gD(d) and amplitudes varying on the expected 40dB output 
dynamic range (figure 2.5, plot b) yields its maximal effect at a quality factor 
modulation distance dQM close to the pseudoresonance distance dpR. Figure 2.5 
(plot c) shows that on a b = 6 stages per octave cascade, the amplitude gains 
of the stages k + 1 and k + 2 are most attenuated. This quality factor best 
modulation distance dQM between 1 and 2 is similar to the pseudoresonance 
distance dpR lying in this case between 0.22b = 1.32 and 0.30b = 1.8. 
This means that the stage k + dpR has the best response fZvbm(dpR ) to an 
input signal at the CF of stage k, whereas the same stage k + dQM is the most 
sensitive to a control signals i~(d) having the same distribution as the peak BM 
velocity signals ~vb,~(d), but shifted by --dpR in order to have its maximum 
i~(0) at stage k. Since the most efficient quality factor adaptation is performed 
for the highest open loop gain, the feedback gain's spatial distribution F(d) 
should have its maximum at d = --dQM. 
In conclusion, the output of any 
stage in the cascade must control the quality factor of a stage located at a basal 
distance --dQM corresponding to a CF increase between one sixth to one third 
octave (Figure 2.6). 
As mentioned in section 2.2.3, the feedback gain distribution F(d) models the 
IHCs and the MSR blocks. The distribution of F(d) can thus model interactions 

30 
NEUROMORPHIC SYSTEMS ENGINEERING 
1.8 
-2 
1.6 
~-,~ 
~' 
0 1.4 
I~ 
-6 
1.2 
-8 
-2 
-1 
0 
1 
2 
0.~ -2 
d/b 
i b) 
-1 
0 
1 
2 
d/b 
c) 
45 
-
~
 
-~0 
45 
d=l 
~ 1 = 2 ,  
~\ \\\\ 2 
,â¢â¢â¢=1 
d=2 
\/\\\\\\\\ 
~ 
d=2 
~-/J 
- 10 
~ 
- ~  
-,0[\\\\\\\\\\ 
2 
f/fck[oct] 
Figure 2.5 
Most sensitive stage to quality factor modulation; (a) normalised gain function 
for several quality factors; (b) resulting quality factors for several output level; (c) gain curves 
after stages/~ + d. 
between stages that may occur at higher levels of auditory processing, such as 
lateral inhibition, diffusion and nonlinear spatial filtering. As for the basal shift 
-dQM, its biological counterpart is plausible, since afferent fibres of the IHCs 
are paired with the afferent fibres of the OHCs at one seventh to one third 
octave higher CF [6]. The operating point of the OHC is likely to provide the 

ACTIVE COCHLEA 
31 
auditory brainstem with information relative to the signal level, which has been 
lost by the gain adaptation. 
&GDd 
--0 
0 
0 
-d QM i 
sta~e k 
T 
d 
0--0~ 
dpR 
i 
Figure 2.6 
Cascade quality factor control loop; the feedback gain distribution F(d) shifts 
basalward at a distance dQM ~ dpR the output signal with highest peak amplitude occuring 
at stage k + dpR. 
2.2.7 Closed loop amplitude gain 
In order to validate the quality factor control on a pseudoresonant cascade, 
digital simulation cannot be avoided to perform the closed loop analysis of 
such a nonlinear system. The computer model we propose analyses the dynamic 
behaviour of the peak amplitude at the outputs of the cascade when excited 
by an input signal at a given frequency. 
The system to solve is given by equation (2.14). The model uses the simplest 
feedback gain distribution F(d) = F for d = --dF and F(d) = 0 otherwise, 
where dF is an integer such that 1/6 _< dF/b <_ 1/3, according to our conclusion 
about the best quality factor modulation distance dQM. The term GoÂ¢(d) 
in equation (2.14) equals in this case FIIGD(d + d~,f~)t [ and it depends on 

32 
NEUROMORPHIC SYSTEMS ENGINEERING 
the quality factor distribution Q(d), since [IGD(d Ã· dF, ~)[I is the product of 
amplitude gain functions of stages with the different quality factors that follow 
the distribution Q(d). An iterative loop must estimate the distribution Q(d) 
on a range of stages for which Q(d) is substantially modulated by the feedback 
loop. 
This iterative loop can advantageously be driven by the averaging performed 
in the MSR blocks. The MSR is estimated by averaging the IHC output V~u~ of 
every stage in the cascade according to a first order lowpass filtering 1/(s~-~ + 1). 
Using equation (2.12), the quality factor control signal i~(d) can be described 
in the time domain identically for every stage by 
5i~(d,t) _ F~/~b,~(d + dR,E) -- i~(d,t), 
Za 
5t 
where the differentiator's output distribution ~/~b~(d, t) = I[GD(d, ~, t)[ll)din0(t) 
is controlled in time by the input peak amplitude Vdi,~o(t) and in space and time 
by the distribution at time t of the amplitude gain [IGD(d, ~, t)[I of the cascade. 
Figure 2.7 shows the simulation result of such a closed loop analysis, per- 
formed on a b -- 6 stages per octave cascade with a quality factor limited at 
Q,~ =- 1.8. In plot a, the amplitude gain after stage k Ã· d at the CF of stage 
k is represented as function of the stage shift d relative to stage k. The index 
d equivalently represents the frequency in b-th of an octave relative to the CF 
of stage k. The peak amplitude gain adapts from 40dB at -80dB input level 
to nearly -10dB at 0dB input signal, yielding the 50dB dynamic compression 
also measured on biological cochleae. The shift of the peak frequency ~PR 
from one third an octave to minus half an octave relative to the CF as well 
as the frequency selectivity decrease for an increasing input level are also in 
accordance with physiological data. Plot b, in addition to the level adaptation, 
shows the level transition enhancement expected to improve the detection of 
relevant features in the speech signal. 
2.2.8 
Stability 
The closed loop analysis has been performed on the amplitude gains of the 
cascade stages with a single frequency input signal. This analysis, however 
does not take into account the phase of the control loop which may cause the 
system to become unstable. 
Instability can occur locally in a single stage k if in equation (2.6) aiQmax > 1 
leads to a negative value of Qmax. In this case, according to equation (2.5), the 
quality factor Q of this stage becomes negative with ic -- 0, that is, in absence of 
any input signal. As a result, the real part of the poles of the transfer function 
Hk (~t) becomes positive. Any perturbation would cause its output signal Vdbm 
tO oscillate with an amplitude growing exponentially. Thanks to the quality 
factor control loop, as the amplitude of Vdbm grows, the control signal ic grows 
proportionally until it ~yields a quality factor Q -- -~ for which Vdb~ stabilises 
to a peak amplitude Vos~. This amplitude is directly related to the control 

ACTIVE COCHLEA 
33 
40 
30 
20 
~ 10 
L9 
-10 
-20 
a) 
f 
-3b 
b=6,Qmax=l .I 
J 
-2b 
-b 
dF 
b 
-2O 
~ -40 
X 
-60 
-80 
YIdB] 
I 
i 
I 
. . . . .  
I 
~ . . . . .  
d=dF 
rXLd_Bi.,' 
..... 
: 
o 
~ 
~'o 
~'s 
~'o 
~'s 
~'o 
~'~ 
4'o 
~s 
~o 
t' 
Figure 2.7 
Closed loop analysis; (a) amplitude gain for an input signal at the CF of stage 
k with input levels between -80dB and OdB; (b) time response of the output level for a input 
level increasing then decreasing between -80dB and OdB. The time t ~ is normalised to the 
time constant T a of the MSR lowpass filter. 
signal icosc = -1/2Q,~a= yielding Q = -oc. The resulting output oscillation 
at the output of the second-order stage Hk+d~- has therefore a peak amplitude 
~osc -- C~iQrnax 
-- 
i 
F 
This oscillation should however not propagate apically in the cascade. Since 
its frequency equals the CF of the oscillating stage, it is rapidly attenuated by 
the sharp cutoff at the following stages. 

34 
NEUROMORPHIC SYSTEMS ENGINEERING 
This sustained oscillation caused by a misfunction of the OHCs was observed 
in the biological cochlea as an otoacoustic emission, which strongly suggested 
that an active process performs the automatic gain control in the cochlea. 
The stability of the quality factor control loop itself has to be verified. In 
order to estimate the open loop gain, the nonlinear feedback loop can be lin- 
earised around a given operating point. We assume for this analysis that all 
stages k to k + dF participating in the control loop have the same quality factor 
Q(ic). For notation simplification, let us use x for the input of stage k, y for the 
output of the differentiator k + dF, and c for the signal controlling the quality 
factors of the stages k to k+dF. Using the feedback loop gain c/y = F/(s'~a+l), 
the linearisation of y = DdF 1]dd~o Hd(C)X yields (see appendix A) 
dF 
: 
d=0 
where the linearised feedback gain _~(c) is given by 
2- 
DdHd(c) 
= 
(1 + c) 2 
1 + S~'a 
and r(y) expresses a piecewise-linear full-wave rectification r(y) = lYl or half- 
wave rectification r(y) = y if (y > 0) and r(y) = 0 otherwise. 
~-fdF Hd 
As shown in figure 2.8, the open loop gain ~DdF lid=0 
features a nonlin- 
ear element (the rectifier) which generates harmonics. These harmonics, as well 
as the fundamental frequency of the input signal are to be filtered by the MSR 
filter which must therefore have a time constant Ta large enough to remove the 
lowest frequencies in the input signal. The stability of the loop will depend on 
the amplitude and the phase of the oscillation remaining at the output of the 
MSR lowpass filter, which we will address in section 2.3.4. 
2.3 
VLSI IMPLEMENTATION 
In this section we detail the analogue VLSI implementation of the the major 
building blocks of the proposed model. All circuits are designed for a standard 
CMOS technology, including the bipolar transistors implemented as compatible 
lateral bipolar transistors (CLBT) [32, 20]. 
2.3.1 
BM second-order stage with differentiator 
The second-order stage cascade modelling the BM displacement as well as the 
differentiators converting its outputs into the BM velocity all along the cascade 
are the same as described in [32] (figure 2.9). 
The crucial point of this circuit is the control of its quality factor. According 
to equation (2.3), the quality factor of any stage Â£ is driven by the ratio of the 
OHC OTA's bias current IQk and the bias currents I,k of the BM OTAs which 
determine the cutoff frequency of the stage ~. The modulation of IQk/I~ allows 

ACTIVE COCHLEA 
35 
a) 
Hk H 
Hk+l t 
Q(c) I~ 
c 
t_ 
F 
S~a + 
_-, Io. 0 1 
~ 
Hk(c) I 
"IHkÃ· 
 
<c>I 
b) 
I D k+dF 
I 
ZDdHd(c) 
S~a+l 
Figure 2.8 
Linearisation of the quality factor control loop; (a) portion of the cascade 
including; an individual quality factor control loop; (b) linearisation of (a) for small sig;nal 
variation. 
therefore to control the quality factor of any stage in the cascade independently 
of its CF. 
Because of the limited slew rate of the BM OTAs due to their load capaci- 
tance C~, large signal instability may occur when the OTA's outputs saturate 
to their bias current. According to the analysis shown in [9], large signal tran- 
sients always recovers if Iq/I~ < iLSI, where iLS1 lies between 1 and 2 for any 
common mode voltage at the OTA input between the positive voltage supply 
V+ and the ground V_. Using the definition of ~ in equation (2.3), the second- 
order section recovers from large signal instability if gmQ/2gm~- < C~iLSI. On 
the other hand, small signal stability is ensured if g,~Q/2g,~- < 1. Therefore, 
with ~iLSI > 1, large signal transients always recover as long as the small 
signal stability is ensured. If both BM OTAs and OHC OTA have the same 
linear input voltage range I~-,Q/g,~,Q in weak inversion, this condition is never 
respected. The BM OTAs are thus degenerated in order to have a linear input 

aq& 'dIa,t!aaadsaa V/LO DHO aqa pun sV&O INH a~t~ jo O I pun ~I ~uaaana 
sn!q at D Su!:~naaua~ s/LH~ID atla Su!pniau! doo I anau!isuna a ~ Xq PaliOaaUoa aq 
una oSnas Xun aq Â£Duapuadapu! ao~anj X~!innb oqa ~uNnInpotu ~I/bI o!ana aq& 
Ioaauoa aoaav, t Â£a~.[vnO g'Ug 
â¢ P~ sJo~,s!sueJ~, at N ,~q po~e 
-Jaua~ap o JR sv.]_ 0 ~] aLI~ J.O ~!ed iep, uo~ajj!p atp, :~a~,l! J ssedt~o I jap~o-puo~a S 
6"E aJn~!-I 
tIl~ 
mq^ A 
-as!saad o:~ uo!~nliDSo inu~!s 
aSan I a,t~ satoIin aaaau 'doo I ~avqpaaj aoaa~j X~Ivnb aRa Xq anita s~Ra oa pa?~m~[ 
s~ RaN~ pue '(I + u)/~Ig saRavaa ~I uaR~ Suunaao xa~i~q~asu~ ieu~s ii~ms aRa 
a~qa sa!Idm! qaNm 'I ueqa aaa~aa$ sÂ£~l~ s~ u aoaag adoIs oR& "[06 'gg] sV&O 
~a aqa jo a!~d i~auoao~p oql aa~aaua~ap aeql P& saoas~su~aa poaaouuoa-apo~p 
aqa 3o ([Og] aas 'aoas~su~al 80~ jo iapom AHa) aoaag adois aqa s~ u aaaq~ 
, 
g 
~ 
I+u 
Su~Pla~X 'V&O DHO oqa uvqa aoSa~ I sam N (I + u) o~u~a o~VllOa 
DNIN~NIDN~t SIN~KSXS OIHdHOIAIOHfI~N 
9~ 

ACTIVE COCHLEA 
37 
circuit proposed in figure 2.10 generates IQ and Iv from the currents IQmax, 
Ic + I0 and Iv according to equation (2.4). The currents IQ,~ax and Ic + I0 
are imposed at the collectors of the respective CLBTs, whereas Iv is imposed 
by the base voltage V~ picked on the resistive line controlling the exponential 
decrease of the CF along the cascade. The common voltage VQ on the emitters 
of the CLBTs Tq and Tq,~ is controlled by the MOS transistor Te which sinks 
these emitters with a current that allows their respective collectors to sink the 
required currents IQ and IQ~a~. 
An important point to analyse is the effect on the quality factor of the 
dispersion of the translinear loop input currents. According to equation (2.6), 
for a m~imal quality factor Qmax ~ 0 at every stage, the condition aie~ 
< 1 
must be respected for every stage. Allowing variations of a ~ = ~iQmax within 
the range ~ x a  ~ leads to a nominal m~imal quality factor 
1 + ~max 
~ 
- 
, 
(2.~) 
2~amax 
where ~ ~ Â¢~ 
ensures Q~(a' + Â¢~a') > 0. However a negative variation 
within the same range can be shown to reduce Q~ 
by up to a factor two: 
~a~(~'--Z~a~a') = ~ 
;~'~ 
~maxk 
2" 
The variations ~ of ~ depend on the matching of n, IQma~ and Io all along 
the cascade, which may result in important variations of ~'. Equation (2.15) 
shows that ~ 33% variation of ~ allows a nominal Q~ 
= 2, with a worst case 
~ (n + ~)~. 
Q~ 
= 1 while ensuring Q~a~ > 0 in any case, with I0 = ~ 
I~, 
let 
IQmax{, 
I c +10{ , 
V~ 
% 
resistive line 
I 
I 
,Te 
Figure 2.10 
Translinear loop; the emitter current of transistors Tq and Tqm is sinked by 
the transistor T e. 

38 
NEUROMORPHIC SYSTEMS ENGINEERING 
2.3.3 IHC rectifier 
The function of the rectifier is to generate a DC component from the BM 
velocity signal. The BM velocity is available at the level of the BM second-order 
filter as the voltage difference Vvbm. Therefore an OTA suffices to generate a 
current Ivb,~ ---- g,~aVvbm. This current is then half-wave rectified (HWR) by a 
single diode at the output of this IHC OTA (figure 2.2). Full-wave rectification 
(FWR) can be performed by connecting its output to the output of a rectifier 
generating the opposite half wave of the current Iihc. The two HWRs differ 
thus by the inversion of their OTA input (figure 2.11a). 
The delicate points to master in this circuit are related to its need to rectify 
a very small current (few hundreds pA) varying at frequencies up to the highest 
CF of the cascade, i.e,. 5kHz. The current must be as small as possible because 
it will determine the large time constant ~-a (about 10ms) of the MSR lowpass 
filter together with its capacitance Ca, which must in turn be small enough 
(few pF) not to waste chip area. 
The time to switch from a negative current --I~b,~ to a positive one I,b,~ 
depends on the charge accumulated on the parasitic capacitance Cr~ at the 
output of the IHC OTA. This switching time tsw can be estimated roughly by 
C.~ AV.~ 
- 
-
-
,
 
(2.16) 
tsw 
Ivbm 
where AV,~ is the excursion of the IHC OTA output voltage. 
In order to limit AV,~, the conductance at this node must be large enough 
for both positive and negative current Ivb,~. This means that a negative wave 
must be sinked from the node Vn through a transistor Tn whereas the positive 
one is sourced through the transistor Tp on the node VR. The voltages V,~ and 
Vp must be chosen carefully in order to minimise the leakage current flowing 
between their respective nodes when I,b,~ = 0. The transistor Tn and Tp are 
N and P type respectively in order to avoid their gate capacitance at the IHC 
OTA output due to the drain-gate connection of the transistors. The bulk of 
transistors Tp and Tn are connected to the voltage supply V+ and to ground 
V_ = 0 respectively. 
Using the EKV model of the MOS transistor [20], the drain currents Ip and 
In in the respective transistors Tp and Tn as function of V,~ for given Vp and 
Vn are represented in figure 2.11b on a logarithmic scale i -- ln(I/IDon). All 
voltages are normalised to the thermal voltage UT = kT/q (26mV at 300K). 
The reference current IDon ---- Is~ exp(--VTOn/nn) depends on the technologi- 
cal parameters of the transistor Tn, whereas the term A = ln(IDop/IDon) -- 
v+(np -- 1)/np expresses those of transistor Tp relative to transistor Tn. The 
voltage excursion AV,~ at the output of the IHC OTA for a current varying 
from I~bm to --I~b,~ can be estimated by 
Ivbrn 
AVm = 2UTln iv---~, 
(2.17) 
where Ipo is the leakage current. 

ACTIVE 
COCHLEA 
39 
Vvbrr 
a) 
lihc{7 
v' 
i=ln ~1~ 
~lihc 
n ^ 
DOn 
,~ 
nn "~oe. 
AVm 
_~' 
/ 
b) i~o- 
~ 
o 
~ 
A_)H~ i~p i"p iRn li~,~ 
,,p 
V m 
Figure 2.11 
IHC rectifier; (a) a HWR is represented in the grey frame whereas the com- 
plete structure implements a FWR; (b) forward and reverse currents in the transistors T n 
and Tp are represented on a log scale as function of the IHC OTA output voltage normalised 
tO U T. 
Allowing a leakage current less than 1% of Ivb m and a switching time shorter 
than a quarter period at highest CF (5kHz) and assuming a capacitance Cm = 
0.1pF, the IHC OTA output ~vbm current must be larger than 480pA. 
The voltages Vn and Vp determine the value of the IHC OTA output voltage 
V,~0 when Ivb,~ = Ip -- In = 0. The diagram in figure 2.11b shows that this 

40 
NEUROMORPHIC SYSTEMS ENGINEERING 
point is given by 2Vmo = v~/n~ + vp/np - A, or 
v,~0-- ~ ~+--+ 
V+ 
+ 
in 
+ 
-- 
, 
(2.18) 
rtp 
np 
~ 
Isp 
np 
nn J 
where Is~,p, VTOn,p and nn,p are respectively the specific currents, the nor- 
malised threshold voltages VTO,~,p and the slope factors of the transistors T~,p. 
The same diagram shows that a nominal output current Ivb m at least K 
times larger than the leakage current Ipo imposes 
2 
V n 
Vp 
IZvbrn 
VTO p 
VTO n 
np -- 1 
< in -
-
 
+ 
+ 
+ 
v+ - 21nK. 
(2.19) 
rt n 
np 
ISnXSp 
~p 
71 n 
ftp 
The rectified current I~hc can be sourced from node Vp, but the opposite wave 
of l[vbm can also be sinked from the node V~. The rectifier's output voltage 
Vp,~ must respect the condition (2.19) where the voltage V~,p on the opposite 
node is imposed. The IHC OTA output node voltage at operating point V,~0 is 
then determined using equation (2.18). In order to ensure the saturation mode 
of both transistors Tp and T~, their respective drain-source voltage V~ - Vr~0 
and V,~o - Vp must be larger than 3 to 4 UT. 
With nn -= np= 1.5, V+ = 5V, VTO~ = 0.8V, VTO p : 
1V, IsnISp =- (80nA) 2, 
Is,~ ,~ 3Isp (Tp and T~ are minimal size transistor) and a maximal leakage 
current K = 100 times smaller than a nominal current Ivbrn ~- 480pA, the right 
hand side of the condition (2.19) equals 2.36V/UT. Using the node Vp as the 
rectifier output with the node V~ at V+ imposes Vp > 1.46V and Vmo > 3.07V, 
whereas a grounded node Vp requires V~ < 3.54V and V,~0 < 2.09V. 
In both case the node which sinks/sources the half wave which is not further 
used is at a fixed voltage, whereas the node which sources/sinks the rectifier 
output must have a resistance low enough to minimise its voltage variations 
within a few UT. 
2.3.4 
MSR lowpass filter 
In order to estimate the mean value of the BM velocity, the MSR low pass 
filter separates the DC component [ihc from the harmonics generated by the 
rectification of the BM velocity signal Ivbm = g,~aVvb,~. 
With a rectifier's input signal at frequency 1/2~r~-k, the frequency component 
with highest amplitude (the fundament~tl for HWR and the first harmonic for 
FWR) at the output of a MSR first order lowpass filter 1/(s~'a + 1) has an 
attenuation 
AH1 
= 
--~/l+(za/Tk) 2 
HWR 
AF2 
= 
~V/1 + 4(~-a/Tk) 2 
FWR 
with respect to its DC component. This attenuation corresponds to a sig- 
nal/noise ratio (SNR), since the DC component is the information carrying 
signal and the remaining oscillations are to be considered as an additive noise. 

ACTIVE 
COCHLEA 
41 
This SNR is thus larger using a FWR than using a HWR. The SNR improve- 
ment AF2/AH1 lies between 2.35 for ~'a << T~ and 4.71 for 7a >> T~ and it equals 
3.72 for ~-a = ~-~. 
The minimal value of the time constant ~-~ of the MSR is determined by the 
minimal SNR A,~in ensuring the stability of the quality factor control loop as 
mentioned in section 2.2.8: 
/~I "2 ~2 
7kV~-~irnin -- 1 
HWR 
Ta _> 
~ 
/4 A2 
4 V~mi, 
- 1 
FWR 
(2.20) 
If a SNR lower than 2/7~ or 3/2 is allowed, the MSR average is not even 
necessary using a HWR or a FWR respectively. For any SNR larger than 3/2, 
the required MSR time constant is always more than 3~ times smaller using a 
FWR instead of a HWR. At the price of doubling the area of the rectifier, the 
MSR capacitance Ca (thus its area) can be reduced by a factor 10 for a given 
MSR transconductance ga. Hence, the rectifier-MSR block is in any case at 
least 5 time smaller using a FWR rather than a HWR. 
The MSR lowpass filtering is implemented by injecting the rectifier output 
current I~hc into a capacitance Ca in parallel with a conductance ga. 
The 
voltage Va on ga or its current I~ is proportional to the mean BM velocity signal 
(figure 2.2). It will thus be used to generate the current Ic to be injected into 
the translinear loop controlling the quality factor as described in section (2.3.2). 
The conductance ga can be implemented with a single MOS transistor (fig- 
ure 2.12a) or with an OTA (figure 2.12b). 
In both cases the value of the 
linearised conductance ga is determined by the DC current Ia flowing through 
it, thus in weak inversion 
{ 
I~ 
( 
~ 
/ 
~OS 
g~(Ia)= 
~ 
1-~ 
OTA 
' 
2nUT 
I b 
where Ib is the bi~ current of the MSR OTA g~ and the DC current I~ cor- 
responds to the mean rectifier output current [ihc which is proportional to the 
mean BM velocity. Therefore the MSR time constant ~ 
= Ca/ga depends 
on the mean BM velocity. With a single MOS, the time constant grows to 
very large values when I~ becomes small, which means that when the mean 
BM velocity falls close to 0 the MSR memorises its previous value: To avoid 
this problem, the normalising current I0 injected into the translinear loop to 
limit the quality factor to Q~ 
(section 2.3.2) can be injected before the 
MRS with a ratio I~/Io = [ih~/I~. The variation of T~ will be limited be- 
tween 7ama~ = CanUT/I~ and 7,min = Tamaz/(1 + [ihcmaz/I~). 
However, 
using an OTA to implement the conductance g~, the injection of I~ at the in- 
put of the MSR is no longer required and the time constant varies between 
W~mÂ¢, = C~2nUT/I~ and ~m,~ = z~i~/(1 - [yh~/I~), 
which tends to very 
large values for BM velocities yielding mean rectifier output currents [ih~ close 
to the saturation current Ib of the MSR OTA. 

42 
NEUROMORPHIC SYSTEMS ENGINEERING 
We assume that the maximal BM velocity yields a rectifier output current 
having its mean value [ihcmax = 0.531b, which corresponds to a MSR OTA 
output current Ia(Va) 10% lower than its value gaVa with a transconductance 
ga linearised around Ia = 0. In this case, the MSR time constant dynamic 
range Tama=/~-a,~n equals about 1.4. If the single MOS transistor implements 
the conductance ga, the time constant dynamic range ~-a,~a~/~-a,~n is equivalent 
to l+Ic,~a=/Io or, using equation (2.5), (1-1/2Qmaz)/(1-1/2Q,~n). Therefore 
a quality factor varying between 1/x/~ and 2 imposes a time constant dynamic 
range of 2.56. 
The advantage of the single MOS transistor resides in the possibility to 
mirror the current Ia = [~h~ + I~ to the MSR output current I~ + I0 with a 
current gain MR = gmF/ga = Ic/fihc. The area of two OTAs can therefore be 
reduced to the area of two single MOS transistors. 
Nevertheless, at the price of using the area of two OTAs instead of a current 
mirror, the area of the capacitance can be reduced by a factor 2 for the same 
time constant ~-a, since the transconductance of an OTA is twice as small as 
the transconductance of a single MOS transistor when biased with the same 
smallest possible current I~ = Ib. If the capacitance uses much more area than 
two OTAs, the implementation of the MSR with 2 OTAs would even be smaller 
than with a single current mirror. 
The degeneration by a diode-connected transistor at the source of the mir- 
ror's reference transistor or the OTA's differential pair transistors increases 
once more the transconductance by a factor n + 1 at the price of two or four 
additional transistors respectively, since in this case the mirror's output tran- 
sistor or the the feedback OTA's differential pair must also be degenerated to 
keep the transconductance ratio gmF/ga independent of the mean BM velocity. 
 +,01 
0 . Lk 
~ 
ga-II-I-I[-gmF 
~ 
~1 
r. ~ 
OaJ_ 
,oÃ·1 
Figure 2.12 
MSR lowpass filter; implementation using (a) a single MOS transistor and 
(b) an OTA for the conductance 9a. 
According to equation (2.20), the MSR time constant Ta must be large 
enough to sufficiently attenuate the oscillations at the rectifier's output for 
the lowest CF in the cascade. Using degenerated OTAs with a capacitance 
Ca = 10pF, a 200Hz cutoff frequency can be obtained with a bias current 
Ib = 1.76nA. With a maximal mean rectifier output current [~hcma~ = 0.53Ib 
as assumed above, the expected 40dB dynamic range of the BM velocity al- 

ACTIVE COCHLEA 
43 
lows to estimate a minimal mean current [ihc,~ = 9pA at the output of the 
rectifier, which reaches the limits of the leakage current of MOS transistors. 
Since Iih~ is produced by the rectification of the IHC OTA output current 
L,bm, the minimal peak value f,b,~,~n of the IHC OTA output current, assumed 
to be a sine wave, amounts to 30pA for a HWR and 15pA for a FWR. In sec- 
tion 2.3.3 we showed that the current Ivb m must be larger than a few hundreds 
pA (480pA in our example) to ensure a switching time short enough to allow 
frequencies up to 5kHz to be rectified. The leakage current we allowed was 100 
times smaller, thus only 7 and 3.5 times smaller than the respective minimal 
peak IHC OTA output currents estimated above. With f.b,~ = 7Ipo = 30pF, 
equations (2.16) and (2.17) give a switching time allowing a maximal frequency 
of 740Hz to be rectified, instead of the required 5kHz calculated in the same 
conditions with f,b,~ = 480pA. The IHC OTA has its minimal output current 
imposed by the high frequency limit of the rectifier given by the highest CF in 
the cascade, whereas the maximal mean rectifier output current is imposed by 
the large MSR time constant required by the lowest CF in the cascade. There- 
fore, the currents biasing the IHC and MSR OTAs could advantageously be 
graded similarly to the bias current IT defining the CF of the BM second-order 
stage. Another solution could consist of mirroring the rectifier output current 
into the MSR with an attenuation large enough to make possible the large MSR 
time constant together with fast rectifier switching. 
The minimal cutoff frequency of the MSR lowpass filter is anyway limited 
by implementation constraints such as reasonable size and currents larger than 
the leakage currents. In order to sufficiently attenuate the AC component of 
the rectifier output signal to ensure the quality factor control loop stability 
expressed in section 2.2.8, the difficulty to create a large time constant with a 
relatively large current and a reasonable size capacitor can be circumvented in 
our application. The purpose of the MSR lowpass filter is to measure the mean 
value of the BM velocity signal, rectified by the IHC. Since the BM sections are 
modelled by second-order stages, the BM velocities measured at adjacent stages 
have a phase difference close to ~r/2 near their CFs for which they also have 
the largest amplitude. Therefore 4 half-wave-rectified or 2 full-wave-rectified 
sine waves shifted by a quarter period can be used to generate a signal having 
its remaining oscillations attenuated sufficiently to implement the MSR with a 
higher time constant. This "double-wave rectification" can increase the SNR 
of the mean BM velocity signal similarly to the improvement brought by the 
FWR compared to the HWR. 
The design of the MSR lowpass filter well illustrates the difficulties which 
generally occur in implementing VLSI analogue systems processing low fre- 
quency signals typically found in biological models. 
2.3.5 Feedback gain distribution 
According to equation (2.12), the current IcÃ·I0 at the output of the MSR of the 
stage k must control the quality factor of the cascade through the distributed 

44 
NEUROMORPHIC SYSTEMS ENGINEERING 
feedback gain F(d) having its maximal value at d = -dr. The simplest distri- 
bution F(d) = F at d = -dF and F(d) = 0 otherwise is easily implemented 
by connecting the output node sourcing Ic + I0 in the MSR of stage k to the 
collector of the CLBT transistor Tc in the translinear loop of stage k - dR. 
The disadvantage of this fixed connection resides in its limited flexibility, 
since dF has been shown in section 2.2.6 to lie between b/6 and b/3 for the 
most efficient quality factor control loop. The number of stages per octave b 
can be adjusted by the CFs of the first and the last cascade stage through the 
resistive line [32], but it should not extend too far outside the limits 3dF and 
6dR. 
2.4 
CONCLUSIONS 
A model of an active cochlea allowing 40dB dynamic compression has been pro- 
posed. This model was specially developed for an analogue VLSI implementa- 
tion and this constraint highlighted strong functional and structural analogies 
with its biological counterpart. These analogies support the validity of the 
model. 
A careful analysis of the model supported by realistic computer simulations 
has been presented. Together with the available exploratory research already 
done on the topic, we hope this work will facilitate a sound implementation of 
the first analogue VLSI active cochlea known to date. 
The model is based on the original Lyon and Mead's second-order low pass 
filter cascade which still proves to be a very resourceful concept. Hopefully, 
the additional features we added can also be used as a framework general 
enough to progress in further modelling of the auditory pathway. For instance, 
neural processing performed at higher levels of the auditory brainstem could 
be included in the feedback block of the proposed quality factor control loop. 
As preprocessing element for an automatic speech recognition system, the 
frequency selective transient enhancement due to the quality factor control 
is expected to better model the relevant features of the speech signal. 
An 
interface is therefore required on top of the model to provide a classifier with 
the appropriate sequence of acoustic vectors. 
Finally, the possibility of faithfully modelling the cochlea with low power 
analogue VLSI systems also opens the way to a new generation of hearing aids 
and cochlear implants. 
Appendix: A 
The quality factor control loop 
dR 
Y = DdF H Hd(C)X 
d=O 
is linearised by 
_ 
by 
dy 
/x~ 
~x A~ + ~cc A~ 

ACTIVE COCHLEA 
45 
dF 
: 
Dd~ 1--[ Hd(C)Ax 
d=0 
dF 
Ã· 
d=O 
Since 
1 
1 
Hd(c) 
_ 
Pd(C) 
s2~+~ 
+1' 
the partial differentiation of the product becomes 
~ ~o 
1 
dF 
1 
dF 
~ Pd(C) 
= Pd(C)--d=oP~-~d=OII 
E 
Pd(C) 
and, remembering that Dd = 8Td, 
~ 
~Qd(c) 
~cPd(C) = Dd Q~(c) 
Since Qd(C) = Q(c) Vd was ~sumed, and using equation (2.5), 
~Q~(~) 
~ 
~ 
Qma~ 
_ 
~(~) 
(~ + ~)~ 
can be removed from the sum 
dF 
E 
d=O 
Pd(C) 
2 
1 
dF 
Qrnaz 
(C + 1) 2 E DdHd(C). 
d=0 
Fr(y) depends linearly on the rectified 
Finally, since the control signal c - Sra+l 
output signal r(y), Ac 
F 
~r(~)A allows to close the loop 
-- 
s~-~+l 
5y 
Y 
dF 
: 
II 
d=O 
with a linearised feedback gain 
2 
1 
dF 
/~ = C 
-- Qm.-----~- Ed=0 
DdHd(C) 
(c + 1) 2 
8T a ~- 1 
and where r(Ay) = ~yY) Ay for a rectifier r(y) which implements lyl or y if 
(y > 0) and 0 otherwise, since in this case ~yY) is a step or a sign function 
respectively and the operating point y = 0 is imposed by the differentiator 
Dd f â¢ 

46 
NEUROMORPHIC SYSTEMS ENGINEERING 
References 
[1] H. Bourlard and N. Morgan. Connectionist speech recognition: a hybrid 
approach. Kluwer Academic Publishers, Boston, Mass, 1994. 
[2] E. Fragni~re, A. van Schaik, and E. A. Vittoz. Linear predictive cod- 
ing of the speech using an analogue cochlear model. In Proceedings Eu- 
rospeech'95, volume 1, pages 119-123, Madrid, Spain, September 1995. 
[3] C. Gigu~re and P. C. Woodland. Speech analysis using a nonlinear cochlear 
model with feedback regulation. In Martin Cooke, Steve Beet, and Mal~ 
colin Crawford, editors, Visual Representation of Speech Signal, chapter 25, 
pages 257-264. Wiley, 1993. 
[4] T. Hirahara and H. Iwamida. Auditory spectrograms in HMM phoneme 
recognition. In Proceedings ICSLP'90, pages 381-384, 1990. 
[5] T. Hirahara and T. Komakine. A computationnal cochlear nonlinear pre- 
processing model with adaptive Q circuits. In Proceedings ICASSP'89, 
volume 1, pages 496-499, Glasgow, U.K., May 1989. 
[6] D. O. Kim. Active and nonlinear cochlear biomechanics and the role of 
outer-hair-cell subsystem in the mammalian auditory system. Hearing 
Research, 22:105-114, 1986. 
[7] J. Lazzaro, J. Wawrzynek, , and A. Kramer. Systems technologies for 
silicon auditory models. IEEE Micro, 14(3):7-15, June 1994. 
[8] R. F. Lyon and C. Mead. An analog electronic cochlea. IEEE Transo 
Acoust., Speech, Signal Processing, 36:1119-1134, July 1988. 
[9] C. A. Mead. Analog VLSI and Neural Systems, chapter 11, pages 179-192. 
Addison-Wesley, Reading, MA, 1989. 
[10] C. A. Mead, Arreguit X., and J. Lazzaro. Analog VLSI model of binau- 
ral hearing. IEEE transactions on Neural Networks, 2(2):230-236, March 
1991. 
[11] R. Meddis. Simulation of mechanical to neural transduction in the auditory 
receptor. Journal of the Acoustical Society of America, 79(3):702-711, 
March 1986. 
[12] N. Morgan, H. Bourlard, S. Greenberg, and H. Hermansky. Stochastic 
perceptual auditory event-based models for speech recognition. In 1994 
International Conference on Spoken Language Processing, volume 4, pages 
1943-1946, Yokohama, Japan, 1994. 
[13] N. Morgan, S-L Wu, and H. Bourlard. Digit recognition with stochastic 
perceptual speech models. In Proceedings EuroSpeech'95, volume 1, pages 
771-774, Madrid, Spain, September 1995. 
[14] R. D. Patterson, J. Holdsworth, and M. Allerhand. Auditory models as 
preprocessors for speech recognition. In M. E. H Schouten, editor, The 
Auditory Processing of Speech: from Auditory Periphery to Words, pages 
67-89. Mouton de Gruyler, Berlin, 1992. 

ACTIVE 
COCHLEA 
47 
[15] W. S. Rhode and S. Greenberg. Physiology of the cochlear nuclei. In 
Arthur N. Popper and Richard R. Fay, editors, The Mammalian Audi- 
tory Pathway: Neurophysiology, Springer Handbook of Auditory Research, 
chapter 3, pages 94-152. Springer-Verlag, New York, 1992. 
[16] M. A. Ruggero. Response to sound of the basilar membrane of the mam- 
malian cochlea. Current Opinion in Neurobiology, 2:449-456, 1992. 
[17] M. P. Sellick, R. Patuzzi, and B. M. Johnstone. Measurement of basilar 
membrane motion using the Moessbauer technique. Journal of the Acous- 
tical Society of America, 72:131-141, 1982. 
[18] H. W Strube. A computationally efficient basilar-membrane model. Acus- 
tica, 58:207-214, 1985. 
[19] A. van Schaik, E. Fragni~re, and E. A. Vittoz. Improved silicon cochlea 
using compatible lateral bipolar transistors. 
In David S. Touretzky, 
Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neu- 
ral Information Processing Systems, volume 8, pages 671-677. The MIT 
Press, 1996. 
[20] E. A. Vittoz. Analog VLSI signal processing: why, where and how? Ana- 
log Integrated Circuit and Signal Processing and Journal of VLSI Signal 
Processing, 8:27-44, July 1994. Published jointly. 
[21] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692-700, 
May 1992. 

A LOW-POWER 
WIDE-DYNAMIC-RANGE ANALOG VLSI 
COCHLEA 
Rahul Sarpeshkar I, Richard F. Lyon 2, and Carver Mead 3 
I Department of Biological Computation, 
Bell Laboratories, Murray Hill, NJ 07974 
ra h ul~physics, bell-la bs.corn 
2Foveonics Inc., I0131-B Bubb Rd., Cupertino CA 95014 
3physics of Computation Laboratory, California Institute of Technology 
3.1 
INTRODUCTION 
The dynamic range of operation of a system is measured by the ratio of the 
intensities of the largest and smallest inputs to the system. Typically, the 
dynamic range is quoted in the logarithmic units of decibel (dB), with 10dB 
corresponding to 1 order of magnitude. The largest input that a system can 
handle is limited by nonlinearities that cause appreciable distortion or failure 
at the output(s). The smallest input that a system can handle is limited by 
the system's input-referred noise floor. 
At the same given bandwidth of operation, a low-current system typically 
has a higher noise floor than does a high-current system: The low-current 
system averages over fewer electrons per unit time than does the high-current 
system, and, consequently, has higher levels of shot or thermal noise [23]. Thus, 
it is harder to attain a wide dynamic range in low-current systems than in high- 
current systems. A low-voltage system does not have as wide a dynamic range 
as a high-voltage system because of a reduction in the maximum voltage of 
operation. 1 
Low-power systems have low-current or low-voltage levels; consequently, it is 
harder to attain a wide dynamic range in low-power systems than in high-power 
systems. The biological cochlea is impressive in its design because it attains an 
extremely wide dynamic range of 120dB (at 3kHz), although its power dissipa- 

50 
NEUROMORPHIC SYSTEMS ENGINEERING 
tion is only about 14#W. The power dissipation in the biological cochlea has 
been estimated from impedance calculations to be about 0.4#W/mmÃ35mm 
= 14~W [4]. 
The dynamic range of the cochlea at various input frequencies has been 
measured by psychophysical and physiological experiments [7]. The biological 
cochlea has a wide dynamic range because it has an adaptive traveling-wave 
amplifier architecture, and also because it uses a low-noise electromechanical 
technology. 
The electronic cochlea models the traveling-wave amplifier architecture of the 
biological cochlea as a cascade of second-order filters with corner frequencies 
that decrease exponentially from 20kHz to 20Hz (the audio frequency range) [7]. 
The exponential taper is important in creating a cochlea that is roughly scale 
invariant at any time scale; it is easily implemented in subthreshold CMOS, or 
in bipolar technology. 
Prior cochlear designs have paid little or no attention to dynamic range. 
The reports do not give their dynamic ranges [1, 13, 7, 32, 90]. However, we 
know that low-power cochlear designs that pay no attention to noise or gain 
control, like our own initial designs, have a dynamic range of about 30dB to 40 
dB (lmV to 70mV rms) at the small-signal peak frequency (BF) of a typical 
cochlear stage. The lower limit of the dynamic range is determined by the 
input signal level that results in an output signal-to-noise ratio (SNR) of 1. 
The upper limit of the dynamic range is determined by the input- signal level 
that causes a total harmonic distortion (THD) of about 4%. Typically, the 
upper limit is a strong function of the linear range of the transconductance 
amplifiers used in the cochlear filter. 
A single follower-integrator filter in one of our recent designs [24] had a 
dynamic range of 65dB (0.55mV-1000mV rms) because of the use of a wide- 
linear-range transconductance amplifier (WLR) [11]. However, even if the first 
filter in a cochlea has a wide dynamic range, the dynamic range at the output 
of a typical cochlear stage is reduced by the accumulation and amplification 
of noise and distortion from stages preceding it. Nevertheless, the constant 
reduction in the bandwidth of the cochlear stages along the cascade ensures 
that the total noise or distortion eventually becomes invariant with the location 
of the cochlear stage: Noise or distortion accumulates along the cascade, but it 
is also reduced constantly by filtering. However, the asymptotic noise is high 
enough that, in our design [24], the dynamic range for a cochlear stage with a 
BF input was only about 46 dB (5mV to 1000mV rms). In that design, the use 
of nonlinear gain control helped to decrease the small-signal Q with increasing 
input amplitude, and thus mitigated the effects of distortion; however, the 
design's filter topology was not low-noise, and the nature of the nonlinear gain- 
control circuit was such that the circuit increased the noise further. Thus, the 
effects of noise accumulation and amplification limited our ability to attain a 
wide dynamic range. 
In this paper we describe a cochlea that attains a dynamic range of 61dB at 
the BF of a typical cochlear stage by using four techniques: 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
51 
1. The previously described WLR 
2. A low-noise second-order filter topology 
3. Dynamic gain control (AGC) 
4. The architecture of overlapping cochlear cascades 
In addition, we use three techniques that ensure the presence of a robust in- 
frastructure in the cochlea: 
1. Automatic offset-compensation circuitry in each cochlear filter prevents 
offset accumulation along the cochlea. 
2. Cascode circuitry in the WLRs increase the latter's DC gain, and prevent 
low-frequency signal attenuation in the cochlea. 
. Translinear bipolar biasing circuits provide Qs that are approximately in- 
variant with corner frequency, and allow better matching. Bipolar biasing 
circuits were first used in cochlear designs by [32]. 
We shall discuss all of these preceding techniques in this paper. 
The organization of this paper is as follows: In Section 3.2 we discuss the 
architecture and properties of a single cochlear stage. In Section 3.3 we discuss 
the architecture and properties of the cochlea. In Section 3.4 we compare analog 
and digital cochlear implementations with respect to power and area consump- 
tion. In Section 3.5, we discuss the relationship between our electronic cochlea 
and the biological cochlea. In Section 3.6, we discuss possible applications of 
the electronic cochlea for cochlear implants. In Section 3.7, we summarize our 
contributions. 
3.2 
THE SINGLE COCHLEAR STAGE 
Figure 3.1 shows a schematic for a singe cochlear stage. The arrows indicate the 
direction of information flow (input to output). The second-order filter (SOS) 
is composed of two WLR amplifiers, two capacitors, and offset-compensation 
circuitry (LPF and OCR). The corner frequency 1/~- and quality factor Q of 
the filter are proportional to ~ 
and v/~/I2, respectively, where I1 and I2 
are the bias currents of the WLR amplifiers. The tau-and-Q control circuit 
controls the values of the currents I1 and I2 such that the value of 1/~- depends 
on only the bias voltage VT, and the small-signal value of Q depends only on 
the bias voltage VQ. An AGC correction current IA attenuates the small-signal 
value of Q at large-signal levels in a graded fashion. 
The inner-hair-cell circuit (IHC) rectifies, differentiates, and transduces the 
input voltage to a current Ihr. The voltage VA controls the value of an internal 
amplifier bias current in the IHC. The voltage VHR controls the transduction 
gain of the IHC. The peak detector (PD) extracts the peak value of Ihr as a 
DC current Ipk. The current Ipk becomes the AGC correction- current input 

52 
NEUROMORPHIC SYSTEMS ENGINEERING 
Vin 
Â© 
) 
VHR 
VpT ~r 
SOS 
VRF 0 
VOT 
o 
VOF 0 
I 
I~ 
~_ 
.
.
.
.
.
.
.
 
VTO 
I 
TAU & Q 
VqO 
I 
CONTROL 
> 
Vout 
Figure 3.1 
Schematic for a Cochlear Stage. A single cochlear stage is composed of a filter 
(SOS) with offset-adaptation circuitry (LPF and OCR), an inner-hair-cell and peak-detector 
circuit (IHC and PD), and a tau-and-(~ control circuit. 
(IA) to the tau-and-Q control circuit. The bias voltage VpT determines the 
time constant of the peak detector, and thus the response time of the AGC. 
The peak detector is designed such that it can respond to increases in input 
intensity within one cycle of a sinusoidal input at V~ ; its response to decreases 
in input intensity is much slower and is determined by VpT. 
The offset-compensation circuit is composed of a lowpass filter (LPF) whose 
time constant is determined by VOT. The LPF extracts the DC voltage of the 
filter's intermediate node, and compares this voltage with a global reference 
voltage VRF in the offset-correction block (OCR). The OCR applies a correction 
current to the intermediate node to restore that node's voltage to a value near 
VRF. The DC voltage of the output node is then Mso near VRF, because the 
systematic offset voltage of a WLR amplifier is a small negative voltage. The 
maximal correction current of the OCR scales with the bias current I1; the bias 
voltage VoF controls the scaling ratio. Since the restoration is performed at 
every cochlear stage, the output voltage of each stage is near VRF, and offset 
does not accumulate across the cochlea. If there were no offset adaptation, a 
systematic offset voltage in any one stage would accumulate across the whole 
cochlea. 

oq~ u! s~oojja aou~t.o~d~o-oBt.s~a~d pI.OA~ o~l 'ot.~a ~:I ~ q~!Ax '~u!~nuo~ 02~ 
sao~a!m IA!-IAID oq~L "[I I] s~o~s!su~a~ ~] oq~ m.A uot.0,~z!~out.[ dmnq jo onb.tuqoo~ 
aq~ q~nosq~ pu~ 'sao~st.su~a~ ~D oq~ ~!A uot.~aoua~ap a~ 
lo 9nbt.uqoo~ iaAou 
aq~ q~no~q~ pauap!m :totD~n J s! a~u~a a~aut.i aq& "ao~l!ldm~ oq~ jo a~u~s :tvatI.q 
aq~ uop!~ o~ XI~uonbasuo~ pu~ a~um~npuo~su~/t~ :talj.tidllI~ ~omo I o~ 'o~ at D jo 
p~a~su! 'iia~ owl ash am !s:o~s!su~:~ 3A aq~ jo sham oq~ a~ -a pu~ +a s~ndu~ 
aqÂ£ '~agHdm~ a~u~npuo~su~ 
aq~ jo ~n~o oq~ smoqs ~'~ a~n$~ "Xgopq ~ 
aq~sop II~qs om os 'pag~pom XDq~IS uaaq ~q '~aAa~oq '~aiq~o~ ~no m ash a~ 
~q~ H~ 
oq~ jo uots~aA oqÂ£ "[II] Hmop ~o~ u~ poq~sop uoaq s~q H~ 
aq~ 
~/5rAA aqÂ£ 
TZ'~ 
â¢ a~ms Z~olqaoo ozt.~,ua u~ lo sat.~,zadozd IlmOao aq~ ssnaslp a~ 
'9'g'~ uÂ°Daa9 uI "s~aÂ°Iq Gd PU~ DHI aq; ,~ s;lnazp oq; aqlzasap am '~'~'~ uo N 
-aaS ~I 
";lnazia IOZ;Uoa ~-pu~-nm ~OUilSU~Z; oq; ;uasazd am ~'g'9 ~oDaa9 
u I "J~olodo~ ~o~i~ oq~ amm~xo am 'E'~'S uo~oaS uI "~n3~o uoDmd~p~-~as~o 
aq~ aq~aosap am 'g'g'g uoDoo S u I '~n~3 H]~ 
aq] ssnos~p am I'g'g uoDoaS 
uI "I'g aan~d u~ s~noa~o aq~ jo q~a lo suma p aq~ oq~osap mou ii~qs a~ 
â¢ ~oq~o qa~o uo ~uopuodap di~a~ a~ salm~udp a~lioA-~nd~no DV pu~ 
DU oq~ '~ aq~ uo a~uan~ul pIlm ~ oA~q soop a~IOA D~ Oq~ pH~ 'O~IOA D~ 
Oq~ SlOmO[ S~ ~H191~ aO~IS 'IOAO~O H "0 aq~ Ul suoD~IlpSO aO soDHlq~sul ploA~ 
am '~a~qpaaj u~q~ ~oq~ 'p~ojpaoj 
sl d~oiodo~ io~uoa-ul~ aq~ aauls 
"~I!nDJ!D uoNeldepe-l~sjjo ~q~ uJoJj s~ndu] lu~saJd~J .~o~ pue io~ sg~ellOA 
gql "HI S~ ~uaJJn3 se~q ~ql "~Â°I luaJJn~ ~q~ s~ Indlno aql pue '-~ pue +~ ~Je ~a~j~id 
-me aq~ o~ s~ndu[ ~qi 'Ja[i[Id~V a~ue~npuÂ°~sueJl a~ue~-Jeau[q-aP[M aqi 
Z'E a~n~d 
~Â°A 
+AO O -A 
0 
L 
' 
- 
NO o ~A 
~nÂ°l < 1 
~ i~ -Â° ~A 
IÂ° A 
Â£:L 
~ 
T 
f 
? 
~ 
~Ao~ ~ 
i 
~Â°, o~ cI~ 
, 
~ 
VX~IHDOD IÂ£qA DOqVNV XDNVtt-DI~VNACI-~(IIPA 

54 
NEUROMORPHIC SYSTEMS ENGINEERING 
OCR 
VoF 
? 
~ ~I 
0 
~ 
~ 
~ 
~ 
LPF 
~ 
IVRF 
]., 
I 
./~+1 
I 
,~ql 
r 
, 
~ V~ 
; 
v 
/~r /~ 
~1~ 
~ 
~ 
~ 
~ 
~ ~'~'~ ~, ~o ~ 
~ 
~,~ ~o~, 
~; ;~ 
~, 
It 
A 
V 1 
Figure 3.3 
The Offset-Adaptation Circuit. The input to the circuit is the output of the 
1 and 1 
first amplifier of the SOS, V1. The outputs Vo~ 
Vo~ connect to the corresponding inputs 
of the first amplifier of the SOS. 
differential pair. The CP and CN transistors function as cascode transistors; 
they ensure that the DC gain of the amplifier is high such that there is no 
significant low-frequency attenuation in the cochlea (See Section 3.3.1 for fur- 
ther details.). We use CP and CN transistors on both arms of the amplifier to 
avoid any systematic offsets. The CP and CN transistors do not alter the noise 
performance of the amplifier, since their noise currents contribute little to the 
output current of the amplifier. The pFET M transistors implement current 
mirrors. The bias current of the amplifier IB is provided by bipolar transis- 
tors in the tau-and-Q biasing circuit. The extra mirror necessary to convert 
NPN bipolar collector currents to pFET bias currents does not alter the noise 
performance of the amplifier. The offset-correction circuit adds two correction 
currents at the Vot and Vow. nodes. In the filter of Figure 3.1, only the left 
amplifier has correction-current inputs from the OCR. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
55 
3.2.2 The Offset-Adaptation Circuit 
Figure 3.3 shows the offset-adaptation circuit. The LPF is a simple 5-transistor 
nFET OTA-C filter operated in the subthreshold regime. The DC value of the 
V1 input is extracted by the LPF and is compared with VRF in the pFET 
differential pair. The currents in the arms of the differential pair are steered 
via mirrors to the Voz and Vo~ inputs of the left WLR in Figure 3.1, such that 
the sign of the feedback is negative. The cascoding of the offset-correction 
currents prevents the offset-correction circuitry from degrading the DC gain of 
the amplifier. The VB gate voltage of the left WLR, as revealed in Figure 3.2, 
is also the VB gate voltage in the offset-adaptation circuit, such that the offset- 
correction current scales with the bias current of the WLR. The scaling ratio 
is controlled by the source control VOF. 
The value of VOF, which determines the loop gain of the offset-adaptation 
loop, and the value of VOT determine the closed-loop time constant of the offset- 
adaptation loop. A high loop gain speeds up the closed-loop response such that 
the influence of low-frequency inputs is adapted away. Thus, there is a tradeoff 
between strong offset control (high loop gain), which implies some ringing and 
overshoot as well, and coupling of low-frequency inputs into the cochlea. Since 
our lowest-frequency input is 100Hz, we have been able to maintain a good 
control of the DC offset in the cochlea without attenuating any frequencies of 
interest. 
Note that the offset-correction circuitry can correct offsets to only a reso- 
lution that is limited by its own offsets. Since we use subthreshold circuitry 
with small linear ranges in the OCR and LPF, these offsets are on the order 
of 5mV to 15mV; they are small compared with the 1V linear range of the 
WLR. The offset-correction scheme would have been less effective if we had 
used other WLRs to sense and correct the offset of the filter WLRs. In the 
latter case, since the offset of a WLR scales with its linear range, the resolution 
of the offset-correction circuitry would typically have been a significant fraction 
of 1V. 
Note that our offset-compensation circuitry does not require any fioating 
gates and the assosciated need for high voltages, and high-voltage circuits. 
Further, unlike in some floating-gate circuits, the time constant of the offset- 
adaptation circuit may be tuned to be in the 10ms-lsec. range; such values 
ensure that the offset adaptation across an entire cochlea is not excessively 
slow. 
3.2.3 
The Second-Order Filter 
Figure 3.4 shows representations of our second-order filter. The block-diagram 
form of part (b) is convienient for doing noise calculations. For the purposes 
of doing noise calculations we list 12 algebraic relationships: 
I1 
g,~1- 
VL" 
(3.1) 

56 
NEUROMORPHIC SYSTEMS ENGINEERING 
Vin 
Â©> 
(a) 
! 
,~ 
: 
,~ 
", 
2 
(b) 
Vnl 
V, n2 
Figure 3.4 
The Second-Order Filter Circuit. (a) The input is V~n, the output is Y~, and 
the bias currents of the first and second amplifiers are I1 and I2. (b) The block-diagram 
equivalent of the filter is useful for making noise calculations. The voltages Vnl and v~2 
represent the input-referred noise sources of the first and second amplifier respectively. 
I2 
(3.2) 
g~ 
- 
VL" 
C1 
~'1 
-- 
â¢ 
(3.3) 
gml 
C2 
w2 
= 
(3.4) 
gm2 " 
~ 
= 
~ .  
(3.5) 
cI vL 
I~ 
- 
(3.6) 
T1 
C2YL 
Ie 
= 
(3.7) 
~2 
e 
= 
ff~. 
(3.s) 
T 
~ 
~ 
T~ 
Q. 
(3.9) 
~ 
= 
~e. 
(3.~o) 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
57 
cR 
= 
~. 
(3.11) 
C 
= 
~
.
 
(3.12) 
The currents I1 and I2 are the bias currents of the amplifiers, VL is the linear 
range of these amplifiers, and the transconductance gmi of amplifier i is given 
by gmi = Ii/VL. The noise source Vn~, shown in Figure 3.4(b), represents the 
input-referred noise per unit bandwidth of amplifier i. From [11], we know that 
2 = gqY~/Ii, 
Vni 
where N is the effective number of shot-noise sources in the amplifier, and q is 
the charge on the electron. For our amplifiers, N is typically 4.8, whereas the 
amplifiers reported in [11] have N = 5.3. 
From Figure 3.4(b) and the preceding algebraic relationships, it may be 
shown that the output noise per unit bandwidth V,~o is given by 
v,~l + v,~2 ('~s/Q) 
--__ 
VnÂ° 
T282 + ~'s/Q + 1' 
2 
~ 
~,~ + I~-~s:/Q~l 
~ 
vn2 
(3.13) 
V~o 
~ 
]~s ~ + ~s/Q + 1~ ~ " 
In Eq. (3.13) we have used the fact that the noise sources v~ and Vn2 are 
uncorrelated, so there are no cross terms of the form v~v~. ~om the algebraic 
relationships of Eqs. (3.1) through (3.12), by substituting s = ~ 
and ~ = 2r f, 
and by using the normalized frequency x = ~, we can show that 
(NqVL/QC~)(1 +~) 
= 
~ 
f 
~ 
\ NqVL ) 
x: 
V~o(z)dx 
= 
IH(x)l ~ (1 + ~)dx, 
(3.14) 
where H(x) represents the normalized transfer function of a second-order filter, 
1 
g(x) = 1 - x ~ + jx/Q' 
and j is the square root of -1. To get the total noise over all frequencies, we 
integrate the LHS and RHS of Eq. (3.14) from 0 to ~. It can be shown by 
contour integration that 
~ 
IH(x)12dx 
~ ~Q, 
~z~lH(z)l~dz 
= 
~ ~. 

58 
NEUROMORPHIC SYSTEMS ENGINEERING 
2 
The total output noise over all frequencies < v,~ o > is then given by 
~ > 
_ 
1 (NqVL~(Tc 0+_ 
1 
2CLcR -' 
Yno 
= 
(NqV~(I+~), 
~ 4C~ / 
: 
Note that the total output noise is independent of Q for this topology: The 
noise per unit bandwidth scales like l/Q, and the integration of the noise over 
all bandwidths scales like Q, so that the total output noise is independent of Q. 
These relationships are reminiscent of an LCR circuit where the total output 
current noise depends on only the inductance, the total output voltage noise 
depends on only the capacitance, and neither of these noises depends on R; 
only the noise per unit bandwidth and the Q of the LCR circuit are influenced 
by R. In fact, it can be shown that this topology has a transfer function and 
noise properties that are similar to those of an LCR circuit if we make the 
identifications 
~-1 = 
L/R, 
~-2 = 
RC. 
For a given value of C (the geometric mean of C1 and C2), the total output 
noise is minimized if CR = 1--that is, if C1 = C2. 
Figure 3.5 shows the noise spectral density versus frequency and the total 
integrated noise at the output over all frequencies. The data were obtained 
from a test chip that contained a second-order filter with amplifiers identical 
to those in our previous report [11]. The lines are fits to theory. As we expect, 
the total integrated noise is quite constant with Q. The parameters used in the 
fits were N = 5.3, VL = 1V, CR = 1.43, q = 1.6 Ã 10 -19, and C = 697fF. The 
values of N and VL were obtained from measurements on our transconductance 
amplifiers [11]. The value of CR was obtained from least-squares fits to the data. 
We obtained the value of C by having the noise measurements be consistent 
with values of C expected from the layout. 
In filter topologies that have been used in prior cochlear designs e.g. [7] 
or [24], the noise per unit bandwidth increases with Q: The Q is obtained 
through the addition of positive-feedback currents. These currents contribute 
additional shot noise and thus increase the noise per unit bandwidth; in these 
topologies, the integrated noise over all frequencies also increases with Q, so 
both factors contribute to the increase in noise with Q. Although we have 
performed extensive experimental and theoretical analysis of noise in these filter 
topologies as well, we shall present only key findings here: For the topology 
presented in [7], at Qs near 2.5, the rms noise power at the output is 9 times 
higher than it is for our topology. For Qs near 0.707, the rms noise power at 
the output is about 0.8 times lower than it is for our topology. For Qs near 

10 ~ 
"i- 
# 
~: 10 -s 
r~ 
-~ 
i~ 
~.~ 
.~ 
o 
10:0~ 
1 
0.9 
0.8 
0.7 
0.6 
0.5 
0.4 
0.3 
0.2 
0.1 
g 
(a) 
o--q= 2.60 
x -- Q = 0.96 
* -- Q = 0.50 
10 2 
~ 
~ 
~ 
, ~,11 
10 3 
Frequency (Hz) 
10 4 
> 
E 
o 
Z 
0 
WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
59 
o 
o 
(b) 
Â¢, 
o 
o 
~ 
0 
0 
0:s 
i 
1:2 
1.'4 
1:6 
sqrt(Q) 
Figure 3.5 
Noise in the Second-Order Filter Circuit. (a) The noise spectrum changes 
shape as the (~ of the filter is changed. 
(b) The total output noise integrated over all 
frequencies is approximately invariant with Q for this filter. 
1.5, which is where we typically operate, the rms noise power at the output 
is about 2 times higher than it is for our topology. The effects of increased 
noise per unit bandwidth in a single second-order filter are greatly amplified in 
a cochlear cascade. Factors of 2 in noise reduction in a single stage can make a 
significant reduction in the output noise of a cochlear cascade. Thus, using our 
filter topology contributes significantly to reducing noise in a cochlear cascade. 

350 
300 
~" 250 
E 
> 200 
150 
1Â°Â°ols 
2.s 
1 
1.5 
2 
Quality Factor (Q) 
60 
NEUROMORPHIC SYSTEMS ENGINEERING 
Figure 3.6 
Maximum Undistorted Signal in the Filter. The input amplitude at which 
the total harmonic distortion at the output is attenuated by 30dB with respect to the 
fundamental is plotted versus Q. The fundamental frequency is at the BF of the filter. The 
line is an empirical fit. 
Although the noise properties of the filter of Figure 3.4 are superior to those 
of other second-order topologies, this filter's distortion at large amplitudes is 
significantly greater, especially for Qs greater than 1.0: Distortion arises when 
there are large differential voltages across the transconductance-amplifier inputs 
in a filter. The feedback to the first amplifier of Figure 3.4 arises from V2, rather 
than from V1, in contrast to the topology of [7]. Consequently, the accumulation 
of phase shift from two amplifiers, as opposed to that from one amplifier used 
in earlier topologies, causes greater differential voltages and greater distortion 
in the first amplifier. Also, the transfer function of the intermediate node V1 is 
such that the magnitude of voltage at this node is greater than that in other 
topologies for Qs greater than 1.0. Consequently, the differential voltage across 
the second amplifier is larger, and the distortion from the second amplifier is 
also greater. 
It is instructive to find the largest input signal at the BF of a filter for which 
the total harmonic distortion (THD) is about 3%-5%. The amplitude of this 
signal, v,~, is a good measure of the upper limit of dynamic range for a filter, 
in the same way that the input-referred noise is a good measure of the lower 
limit of dynamic range. Figure 3.6 shows the rms amplitude v,~ at a BF of 
140Hz for the filter of Figure 3.4. We observe that, as the Q increases, the 
distortion increases, and the value of v,~ falls. The data were obtained for a 
THD level of 3.3% (30dB attenuation in intensity). The data were empirically 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
61 
12 
I~ 
V~ 
I1 
Fii~ure 3.7 
Translinear tau-and-(~ Biasing Circuit. The voltage VT sets the ~- of the 
filter, and the voltage VQ sets the small-signal Q. The current IA is a placeholder for a 
gain-control-correction current. The currents I1 and I2 are the bias currents of the first 
and second amplifiers of the filter. 
fit by the equation 
vmx(Q) = 128- 1611n 
~ 
. 
The preceding discussion illustrates why an AGC is essential for attaining 
a wide dynamic range with our filter topology: The noise properties of the 
topology are favorable for sensing signals at small amplitudes, and with high 
Qs. However, when the signal levels are large, if the distortion is to be kept 
under control, the Qs must be attenuated. The AGC ensures that the Qs are 
large when the signal is small, and are small when the signal is large. 
3.2.4 
The Tau-and-Q Control Circuit 
In Figure 3.7, we make the following definitions: 
IT = Ise VT/2UT, 
(3.15) 
Qo = e yQ/2Ur, 
(3.16) 
where UT ---- kT/q is the thermal voltage, and Is is the bipolar preexponential 
constant. The current IA is a place holder for an AGC correction current from 
the IHC and peak-detector circuit, and I1 and I2 are output currents that bias 
the first and second amplifiers of Figure 3.4, respectively. A simple translinear 
analysis and the solution of the quadratic equation reveal that, if we define ~ 
to be a normalized AGC correction current, according to 
( 
IA 
) 
(3.17) 
~= 
2I~/Qo' 

62 
NEUROMORPHIC SYSTEMS ENGINEERING 
then 
Q 
= 
Iv~ 2 , 
Independent of the value of IA, the translinear circuit always ensures that 
~ 
= I~. 
(3.18) 
Thus, it is an effective tau-and-Q biasing circuit for the filter in Figure 3.4, 
since it ensures that the AGC affects the Q but not the corner frequency of the 
filter. If we let 
0 = arctan 7, 
(3.19) 
then trigonometric manipulations of Eq. (3.17) reveal that 
If there is no AGC correction current, then 0 = 0 ~nd Q = Q0. In the limit of 
an infinite AGC correction current, 0/2 = ~/4 and Q = 0. 
Figure 3.8(a) shows the corner frequency of the filter in Figure 3.4 ~s ~ 
function of the bias voltage VT. As we expect from Eq. (3.15) and (3.18), and 
from the equations of the filter (Eqs. 3.1 to (3.12)), the corner frequency is 
an exponential function of the bi~ voltage VT. The exponential preconstant 
yields a thermal voltage of 26.7mV, which is fairly close to the expected thermM 
voltage of 26mV at a room temperature of 300K. 
Figure 3.8(b) shows the Q of the filter in the absence of any AGC correction 
current. As we expect from Eq. (3.16) and Eq. (3.20) with ~ = 0 (no AGC 
current), the Q is an exponential function of the bi~ voltage VQ. The exp~ 
nential preconstant yields a thermal voltage of 26.3mV, which is fairly close to 
the expected thermal voltage of 26mV at a room temperature of 300K. 
3.2.5 
The Inner Hair Cell and Peak-Detector Circuits 
Figure 3.9 shows the IHC and PD circuits. The amplifier in the IHC is a 
simple 5-transistor nFET OTA with a fairly high gain (500 to 1000). The bias 
current of the OTA is determined by the voltage VA. The bias current should 
be sufficiently high that the dynamics of the node Vh are much faster than the 
dynamics of the node Vn, for all input frequencies and amplitudes of interest. 
Since the OTA is connected in a follower configuration, the voltage V,~ is very 
nearly a copy of V~n, except for very weak signals, where the bipolar transistor 
BA or the MOS transistor PA are not sufficiently turned on. In practice, the 
signals or noise at the cochlear output taps are sufficiently high that BA or 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
63 
'-I- 
E 
o 
10 s 
10 4 
10 3 
10 2 
0:9 
~ 
1:1 
1:2 
V T (V) 
1.3 
I0 ~ 
v 
_10 Â° 
>., 
..~_ 
o' 
,o:~ 
(b) 
V T = 0.950V 
-4b 
-2~ 
~ 
2b 
Vq (mY) 
40 
Figure 3.8 
Tau-and-Q Control Circuit Characteristics. (a) The corner frequency has an 
exponential dependence on the voltage VT. (b) The quality factor Q has an exponential 
dependence on the voltage VQ. 
PA may be assumed always to be sufficiently turned on. When V~,~ or V,~ are 
rising, the capacitor CHR is discharged primarily by the bipolar transistor BA. 
When Vm or Vn are falling, the capacitor CHR is charged primarily by the MOS 
transistor PA. Thus, during the phases of the signal when the derivative of the 
signal is negative, the current Ihr is an amplified copy of CgRdV~n/dt. The 
amplification factor is given by exp(Vnn/UT). Thus, the IHC differentiates, 

64 
NEUROMORPHIC SYSTEMS ENGINEERING 
Vin 
o 
INNER HAIR CELL 
, 
I 
VHR 
PEAK DETECTOR 
1 
ZT 
J 
Figure 3.9 
The IHC and PD Circuits. The inner hair cell transduces its input Vm to a 
current ]h~ that is then fed to the peak detector. The output of the peak detector ]pk is 
mirrored to the tau-and-Q control circuit as a gain-control-correction current. 
rectifies, amplifies, and transforms the input voltage Y/n into an output current 
Ihr. 
The output current Ihr is fed into the peak detector. The peak detector 
consists of a slow source follower, composed of PF, PT, and CpT, and the 
feedback transistor PI. The transistor PO outputs a copy of the current in 
PI as Ipk. The source follower can follow descending signals in V I rapidly 
because of the exponential dependence of the current of PF on its gate voltage. 
However, the voltage VpT is set near VDD SO that the current source formed by 
the transistor PT is slow in charging the capacitor CpT; consequently, during 
ascending signals in VI, the voltage V8 is slow to respond. Because of the 
fedback nature of the circuit, and the asymmetry in the time constants of the 
source follower, V8 will equilibrate at a value such that the average current 
through PI is slightly below the peak value of Iur. As Ihr alternately reaches 
its peak and moves below that peak, the voltage V~ will undergo large swings 
due to the high gain of the input node of the peak detector. In contrast, the 
voltage V~ will have only small variations from its DC value; they constitute 
the ripple of the peak detector. 
Figure 3.10 shows the waveforms V~n, Vn, Vh, V~, and V~. The labeled 
voltages in the figure indicate the DC voltage v~lues that correspond to the 
horizontal location of the arrow. As we expect, V~n and Vn are very nearly equal 
to each other. The voltage Vh undergoes abrupt transitions during changes in 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
65 
0 
0 
~ 
Vi n 
~ 
3.0~, 
~-b4.6\ 
-~-~f4.6y 
Vs 
~ 
~-~ 4.3\ 
o 
5 
Time (msecs) 
Figure 3.10 The IHC and PD Circuit Waveforms. The waveforms for the voltages Vin-Vs 
illustrate the operation of the circuits of Figure 3.9. 
the sign of the input derivative; these changes correspond to a transition from 
BA being turned off to PA being turned on or vice versa. The voltages V~, and 
Vs in the peak detector undergo rapid downward transitions that are phase 
locked to the downward-going zero crossings of the input waveform where the 
peak value of Ihr occurs. The upward transitions in Vf and Vs are slow because 
of the sluggishness of the current-source transistor PT. The data were taken 
with V~n being a 102mV rms input at lkHz, with VA = 1.0V, with VpT = 
4.039V, VDD = 5.0V, and with VHR ---- 100mV. Typically, we operate VpT near 
4.25V, which results in no discernible ripple in Vs, but these data were taken 
specifically to illustrate better the workings of the peak detector. The transistor 
PT was fabricated as a poly2 transistor. Thus, at the same current level, the 
bias voltages on VpT are higher than those corresponding to bias voltages on 
a polyl transistor. 
From the preceding discussion, we expect that the value of Ipk will be near 
the peak value of CHRdV~/dt amplified by the factor of exp(VHR/UT). Thus, 
if the input amplitude were given by 
Yin = ain sin (2~rf~nt), 
then the value of Ipk would be given by 
Ipk = 2zC f~nCHRame y~R/Ur. 
(3.21) 

66 
NEUROMORPHIC SYSTEMS ENGINEERING 
< 
0 
10 -7 
14 
, 
, 
, 
, 
, 
Input Frequency = 10kHz 
_ J  
12 
VHR = lOOmV ~
-
 
lo 
I 
8 
6 
4 
2 
0 
~ 
0:2 
0:4 
0:6 
0:8 
Input Rms Amplitude (V) 
10 "~ 
14 
.
.
.
.
 
~ 
Input Rms Amplitude = 1V ~ 
12 
. / -  
10 
8 
6 
4 
2 
0 
% 
2obo 
4obo 
6o~o 
8o~o 
lo~oo 12000 
Frequency (Hz) 
Figure 3.11 
IHC and PD Amplitude and Frequency Characteristics. (a) The current I;/~ 
has a linear dependence on the input rms amplitude. (b) The current _f;k has a linear 
dependence on the the input frequency. 
In conformance with Eq. (3.21), Figure 3.11 shows that the response of 
Ipk is linear with the amplitude and with the frequency of the input. The 
data were taken for VA = 1.0V, and VpT = 4.3V. The experimental slopes for 
Figure 3.11(a) and Figure 3.11(b) yielded values for CHR ~ 335fF and CHR = 
313fF, respectively. However, the linear fits to the data reveal that an offset in 
amplitude of about 77.5mV rms in the case of Figure 3.11(a), and an offset in 
frequency of about 276Hz in the case of Figure 3.11(b), needs to be subtracted 
from a~ or f~, respectively. These offsets imply that there is a minimum 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
67 
10 -8 
< 
~- 10-9 
-.~ 
0 
,-~ 
i~ 
I~. 
~0-~o 
0 
Input Rms Amplitude = 1 O0 mV 
/ 
o 
0.~2 
0.~ 
0.56 
0.~8 
0:~ 
0.~2 
VHR(V) 
Figure 3.12 
Dependence of [p/Â¢ on VHR. The current ipk has an exponential dependence 
on the voltage VHR. 
amount of input current Ih~- that is required for the peak detector to output a 
current Ipk. Through experimentation, we have found that this minimum value 
scales approximately linearly with frequency such that the offset for ain always 
lies somewhere in the 50 to 100mV rms region (for a VHR of about 100mV). At 
this time, we do not have a good explanation of what causes these offsets; we 
suspect that they are due to the short-channel length and small Early Voltage 
of transistor PI. 
Figure 3.12 shows that the relationship between Ipk and VHR is described by 
an exponential, as Eq. (3.21) predicts. The thermal voltage UT was determined 
to be around 29.9mV. This voltage is somewhat higher than the 26mV expected 
from theory. The data were taken with VpT â¢ 4.30V, and VA = 1.15V. 
The current Ipk is mirrored by the bipolar transistors BP and BO in Fig- 
ure 3.9 to function as the AGC correction current IA in Figure 3.7. From 
Eqs. 3.1 to (3.12), we know that I~ is given by 2zcfcCVL, where fc -- 1/~- is the 
corner frequency (CF) of the filter. Thus, ~ in Eq. (3.17) is given by 
IA 
2~ / Qo" 
= QÂ°eV"/u~" \ fc ] 2~J 
2VL J " 
(3.22) 

68 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 0 
> 
v 
.~ 10 4 
-~ 
.~_ 
__ 
c~ 
E 
.Â¢~ 10-2 
{/3 
E 
r,,, 
~ 
~0 -3 
10:0, 
100 
> 
~ 
10 4 
.-~ 
.~_ 
_ ~. 
E 
10-2 
< 
(f} 
E 
r~ 
~ 
10 .3 
c~ 
o 
Without AGC 
. . . . . . .  
. 
. . . . . . .  
~ 
(a) 
q = ~.9s 
Q=2.17 
V T = 0.95V 
~ 
VQ = 8mY 
b,b," 
CF = 132Hz 
o Q= 2.22 
102 
103 
Frequency (Hz) 
With AGC 
q=u.5z 
..... 
"__ ~ â¢ .
.
.
.
.
.
.
.
 
Q=1.56 ~ 
- 
~--"~q,,, 
960mV 
~ 
~_-,~,~ ~,,Or~V 
~.~ ~.~.~.*~-- 
"% ~. ~OmV 
,%,,., 160mY 
~ 
~ 
_- _,.,,.=~,-@'~ 
~ 
0 40mY 
Q=2.22 c = o O O ~  
~x~ 
VHR = 65mY 
"~ 
VpT = 4,25V 
O~ 
CF: 147Hz~ 
ll2Hz 
~ 5mV 
lOi'o, 
...... 
i~ 
. . . . . . .  
io 3 
Frequency (Hz) 
Figure 3.13 
Frequency-Response Characteristics of a Stage. (a) Without an AGC, it is 
impossible to obtain smooth and continuous data beyond an input rrns amplitude of 250rnV. 
(b) With an AGC, it is easy to obtain smooth and continuous data up to and beyond a 960mV 
rms input amplitude. 
Thus, the voltage VHR serves to strengthen or weaken the normalized AGC 
correction factor ~. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
69 
3.2.6 
The Properties of an Entire Cochlear Stage 
Figure 3.13 shows the frequency-response characteristics of the filter of Fig- 
ure 3.4 for different input amplitudes. In the absence of an AGC, large input 
amplitudes generate large amounts of distortion in the filter; thus, in Fig- 
ure 3.13(a), it was impossible to obtain smooth frequency responses beyond an 
input rms amplitude of 250mV. In contrast, in Figure 3.13(b), we could easily 
obtain smooth frequency responses up to (and even beyond) input amplitudes 
of 1V rms, because of the presence of an AGC. If the frequency-response curves 
are fitted with the transfer function of a second-order section, 
1 
H(s) = ~.2s 2 + Ts/Q + 1' 
(3.23) 
then we find that the CF (l/T) is reduced with input amplitude, and the Q 
is reduced by the AGC as well. In Figure 3.13(b), the CF is reduced from 
147Hz at small signals to ll2Hz at large signals; the Q is reduced from 2.22 
at small signals to about 0.52 at large signals. These numbers are valid for 
VHR = 65mV, and VpT : 4.25V. Given that we designed Eq. (3.18) in our 
translinear biasing circuit to keep the CF constant, it may seem surprising 
that the CF changed with input amplitude. However, we must realize that we 
are fitting the frequency-response curves of a nonlinear system with a linear 
approximation given by Eq. (3.23) at each rms input amplitude. According to 
Eqs.(3.22) and (3.17), for the same input rms amplitude, the "Q" is lower at 
high frequencies than at low frequencies. The frequency dependence of the Q 
results in disproportionately more attenuation of the input at high frequencies 
than at low frequencies, such that the CF, as measured by the amplitude curves, 
appears to shift. 
If we plot the CF90--that is to say the frequency at which the the second- 
order filter has a phase lag of 90 degrees--versus rms input amplitude, then 
the data of Figure 3.14 reveal that CFgo is approximately constant. At low 
input amplitudes, the AGC has no effect, because Ipk provides no correction 
until the input amplitude is above a certain threshold, as we discussed in Sec- 
tion 3.2.5. Even if there were no offset, the AGC correction in this regime would 
be small. Thus, the system is linear at low amplitudes. Consequently, at these 
amplitudes, the CF9o is identical with 1/~- and with the CF measured by gain 
curves. Since the AGC is designed not to affect the parameter % the CFgo re- 
mains approximately invariant with input amplitude, even at high amplitudes 
where the AGC is active. In fact, Figure 3.14 shows that a strong AGC (higher 
values of VHR) improves the constancy of the CFgo with amplitude, because 
it prevents static nonlinear shifts in CF9o that increase at high Qs. The CF9o 
is the frequency near which center-surround schemes, e.g., those that perform 
spectral extraction on cochlear outputs for use in implants [14], generate their 
maximum output. Thus, the fact that the CFgo is approximately invariant with 
amplitude makes our AGC cochlea attractive as a front end for center-surround 
postprocessing. 

70 
NEUROMORPHIC SYSTEMS ENGINEERING 
g 
N 
O 
z 
1.2 
1 
0.8 
0.6 
0.4 
0.2 
0, 
o* t 
o -- V H~ = 3mV 
x--V H~ =32mV 
* -- V HR = 65rnV 
0:2 
0:4 
0:s 
Input Rms Amplitude (V) 
Figure 3.14 
CF9o Characteristics. The frequency at which the phase lag of the filter is 
90 degrees is relatively invariant with input rrns amplitude. 
Figure 3.15(a) shows data for Q versus a~ measured for three different 
values of VHR for the second-order filter. From Eqs. (3.17), (3.19), and (3.20) 
we would expect the curves to be fit by a fnnction of the form 
Q = Qotan (~ 
arctan(Ga~)) 
2 
" 
(3.24) 
However, from the discussion of Section 3.2.5, we know that a~ in Eq. (3.24) 
should be replaced by 0 below some threshold value a0, and by am - ao above 
this threshold value. The fits in Figure 3.15(a) are functional fits to Eq. (3.24) 
with the free parameter G, and the additional free parameter a0. For V14R = 
3mV, 32mV, and 65mV, we found G -- 1.93, 2.5, 4, and a0 = 0.164, 0.095, 0.06, 
respectively; Q0 was 2.05 for all curves, ain and a0 are in units of V. We took 
data by measuring the gain of the filter at fi,~ = fc = CFgo = 1/% which, for a 
second-order filter, is Q. Figure 3.15(b) plots the output amplitude, Q(a~) Ã 
a~n, rather than Q(a~n), at this frequency. We observe that, before the AGC 
turns on (a~ < a0), the relationship between the input and output amplitudes 
is linear. After the AGC turns on (a~ > ao), the relationship between the 
output and input amplitudes is compressive, although not as compressive as 
theory would predict. Since a0 is large for small values of VHR, the range of 
linearity is large for small values of VHR. 
Figure 3.15 suggests that, at large amplitudes, the static nonlinearities in 
the filter increase the Q slightly. Since the Q of the filter is given by ~//~, we 
deduce that the static nonlinearity is causing ~-2 to increase faster with ain than 
~-1; this deduction is in accordance with the intuition that the second amplifier 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
71 
2.2 
1.8 
CY 1.6 
v 
~, ~.4 
~1.2 
>, 
~ 
.~_ 
-~ 0.8 
& 
0.6 
0.4 
0-2 0 
0:2 
(a) 
o -- V HR = 3mV 
~ ,o 
x -- V HR = 32mV 
0:4 
0:6 
0:8 
Input Rms Amplitude (V) 
0.6 
"-" 0.5 
> 
~.~ 
(D 
~ 
0.4 
.~_ 
~. 
E 0.3 
< 
I/} 
E 
c~ 0.2 
~ 0.~ 
0 
o~ 
' 
' 
' 
' 
0 
o~ 
~
~
0
 
0 u u ~ 
oÂ°~ 
~ 
~ 
(b) o.-v.~ =~mv 
1 
X -- V HR = 32mY 
f 
" 
* -- V HR 
65mY 
0:2 
0:4 
0:6 
0:8 
Input Rms Amplitude (V) 
Fil~ure 3.15 
Q-Adaptation Characteristics. (a) The Q adaptation due to the AGC is well 
fit by theory, except at large input rms amplitudes, and for strong AGC corrections (large 
VHR). (b) The same data as in (a) except that we plot the output rms amplitude, instead 
of the Q). 
in Figure 3.4 is subject to greater differential voltages, and, consequently, to 
more saturation and slowing than the first amplifier. One way to avoid, or even 
to reverse, the nonlinear shift toward higher Qs is to have the linear range of 
the first amplifier be smaller than the linear range of the second amplifier. 
The nature of Eq. (3.24) is such that, independent of the value of G, 
Q(a~n)a~,~ is a monotonically increasing function of a~,~. This property guar- 

72 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 0 
> 
"~10 -~ 
-.~ 
.~ 
~. 
E 10-~ 
< 
$ 
0 
> 
v 
,_ 
E 
< 
0 
10 -3 
10 .4 
10 
10 0 
10 -1 
10 -2 
WZtheutAGC 
.
.
.
.
.
.
.
.
 
(a) 
; :;f 
_ 
ooooÂ°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°Â°o 
x--2f 
o 
* -- 3f 
o o o o 
xxxxxXllx 
X 
â¢ 
~X 
X 
X 
X 
~ 
X 
~ 
~X 
X 
~ 
X 
X 
x X X ~  
â¢ 
X x 
~ 
~ 
X 
~ 
~ 
10 2 
Frequency (Hz) 
~fith ACÂ£ 
0 00V,UU,~ uo (~0 .
.
.
.
.
.
 
OOoooo 
(b) 
0000 
0 0 
xXXXXx x 
o o 
X x 
XX x 
X x 
~ 
X x 
X xx 
~ 
~ 
X x 
â¢ 
X 
~ 
~( 
X 
X 
~4 
~ 
X 
~ 
~ t~ 
t~ 
Inpu! Rms A, mpli,tude, =.25.0,rn.V 
10 8 
o--If 
x -- 2f 
*-- 3f 
. . . . . . . .  
~ Input, Rms .Ampli.tud.e =.96.0.rny 
10-103 
10 2 
10 8 
Frequency (Hz) 
Figure 3.16 
Distortion Characteristics of the Filter. (a) Without an AGC, the distortion 
is already fairly high at a 250mY input rms amplitude. (b) With a strong AGC (VHR = 
65mV), the distortion is comparable to that in (a) at only a 1V rms input amplitude. 
antees that the input-output curve at the BF of a cochlear stage is always 
monotonically increasing, as confirmed by the data of Figure 3.15. 
Figure 3.16 shows that the harmonic-distortion levels at 1V rms with an AGC 
are comparable with the harmonic-distortion levels at 250mV rms without an 
AGC. The AGC data were taken with Vh~- = 65mV. Figure 3.17 shows that 
a strong AGC (large value of VHR) reduces harmonic distortion due to the 
lowering of Q. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
73 
> 
v 
E 
< 
E 
0 
10 0 
101 
10 .2 
i0 -~ 
10 -4 
10 `5 
000000000000000 
0 0 0 0 0 0 0 0 0 0 0 0 0 0 
x 
xxx 
xxx 
~i~ 
~ 
~ 
~ 
~ 
xX:~C f 
O--lf 
x -- 2f 
X 
x 
~ 
* -- 3f 
X 
~ 
x~ 
VHR = 3mY 
~ 
(a) 
o12 
oi~ 
o16 
ols 
Input Rms Amplitude (V) 
10 0 
10 4 
ooOOOOOOOOOOOÂ°O 
0 
> 
.~'Â°~ 
xxxXX~)~<x ~ 
~-<F: 10~ 
xX~ ~ 
~ 
~ 
~ 
ff 
~ 
10-3 
X 
~ F~ 
O 
104 
~0-~ 
0:~ 
0 0 0 0 0 0 0 0 0 0 0 0 0 0  
xxXXXX 
xxXXXxXx 
o--If 
(b) 
x--2, 
* -- 3f 
VHR = 65mV 
o14 
oi~ 
o18 
Input Rms Amplitude (V) 
Figure 3.17 
Distortion Characteristics at BF. (a) With a weak AGC (TV'.~R = 3mY), the 
distortion levels are significant at a 250mV input rms amplitude. (b) With a strong AGC 
(VHR - 65mV), the distortion levels are smaller than are those in (a) even at a IV input 
rms amplitude. 

â¢ oanc~aacgttaav sopvas~a-$u!dd~laaAo ano jo uo!:~anpoa~u! alia 0:~AI.:~OIII 
II!m uo!ssnas~.p aria asn~aoq uo!a~nuaaav Xauanbaaj-~o I jo uo!ssnas!p v qa!~ m~ 
-aq II~tlS aAt '~alqaoa aq:~ jo so!aaadoad aria ssnas!p ii~qs a~ 'uo!aaas s!R a u I 
V3"IHDOD 3H.L =10 S31.1.~3dORd 
Â£'Â£ 
â¢ anita mnvq!I!nba uv 
o~ 'apu:gldmv ~nd~no aql snq~ puv '~ aq~ saJo~saa aoJnos ~uaJJno ~o~a~ap-~ad 
aq~ jo ~u!~a~q~ an!~D~d~ ~OlS aq~ 'Xii~n~ua~ ~ 
'auo~ pno I ~u~pa~azd aq~ o~ 
uoI~md~p~ Lq posn~ ~ ~o I ~ 
o~ onp o~aopom s ! ouo~ ~jos oq~ o~ osnodsoa 
I~!~!u~ aq~ '~jos o~ pno I moJj suo!~!su~ auo~ aq~ uaq~ "anita mn[aqil!nba u~ o~ 
'opna~idm~ ~nd~no oq~ snq~ puv '~ aqa soaoasaa qa~q~ 'aaanos ~uaaana aoaaoaap 
-~od aqa jo ~u~aeqa oa!a!a~d~a ~OlS oqa Xq paaaaaaoa s~ uoDmd~p~aaao oq~ 
â¢ OlaXa ouo u~qa!~ Â£IaU~lsu~ ad~p~aoAO Oa aoaaalap ~ad oqa sasn~a auo[su~aa 
oR& 
'andano aqa ~ uoos s[ asuodsaa aua!suvaa ~ 'ouoa aqa jo loguo aqa aV 
â¢ auoa aaaa~nb ~ oa Ra~suaau~ u ! so~aaaap Â£Iadnaq~ uaqa pu~ 'al~q~ ~ aoj sas~saad 
'aauai~s jo po~aad v aa~jv Rluappns uo suan~ ~vqa aaaI~ aqa jo aa aqa av auoa 
oand ~ s~ snInm~as aqÂ£ :uo~amd~pv 0 ~o sa~mvuÂ£p aq~ saa~a~snlI ~ gI'g oan$~d 
-s~.depe DDV aq~ jo u!e~ aq~ se asuodsaJ ~,nd~no aq:l u! dnpl!n q lenpe~ e 
sasne~ anita ~jos ~ o~ pno I e ~oJj aura a~s aq~ jo &~sua~u~ u~ uo~npaJ aql "~n~Jq~l~nba 
o~ uo~eJo~saJ ~ Xq pa~OllOJ 'apXD ~sJ~j aq~ uo ~ua~sueJ~ jagq e SaShED a~Ual~S jo po~Jad ~ 
~q papa~aJd 38 ~e auo~ pno I e jo ~asuo aql 
"uo~e~depe-~ jo sD~eu~ 
B['Â£ a~n~ 
(spuo~as) auJ!Â± 
s~ 
~ 
s'~ 
~ 
so 
0 
~.nd~.no 
zHOt, I. = ~uanbs, J..-I ),ndul 
AO~'t~ : J~A 
~.ndul 
< 
< 
& 
~NIH~HNIDNH 
SINHJ.SAS OIHdHOINOH~IHN 
WL 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
75 
3.3.1 Low-Frequency Attenuation 
If the open-loop gains of the amplifiers in Figure 3.4 are A1 and A2, then we can 
show that we obtain the low-frequency gain of the filter of Figure 3.4 by simply 
replacing ~ks and ~-2s with 1/AI and 1/A2 in the transfer function. Thus, from 
Eq. (3.23) the low-frequency gain H0 is given by 
1 
H0 
= 
1 
1 + ~ + AIA2 
AI 
A1+1" 
Although H0 is very close to 1, it is not exactly 1. A low-frequency input that 
travels through M stages of a cochlea will suffer a net attenuation, HM, given 
by 
(AI 
 
s-s 
 
= 
' 
~ 
e-M/A1 , 
where the exponential approximation is valid if A1 is a large number. Now, 
~1 = 2VoIVL, where VL is the linear range of the amplifier and 7o is its effective 
Early voltage at the output of the amplifier [11]. In the 1.2/~m ~well Orbit 
~OSI8 process used to fabricate our circuits, 7o is arouud 20V for wide-linear- 
range amplifiers that do not have e~eode transistors at the outputl V~ = 17, 
and A1 is about 40. Thus, we can expect an attenuation of almost one e-fold 
across a 39-stage cochlea built with c~codeless amplifiers. 
Figure 3.19 shows the low-frequency attenuation of a ,10Hz 50mV input to 
a 39-stage cochlea tuned from 900H~ to 100~ for different values of the pa- 
rameter A1. For these experiments, we operated the cochlea with a very low 
~ (Ve = -80mV) so that we could focuss on just the effects of low-frequency 
attenuation. We varied the value of -41 by varying the bias of the casÂ¢ode tran- 
sistors VCN and Vcp , in the amplifier of Figure 3.2. We explored the effects 
of turning off the cascode transistors by biasing them as switches. Thus, to 
turn off the CN cascode transistors, we would set VCN to 5V; to turn off the 
CP cascode transistors we would set VCp to 0V. Figure 3.19 shows the low- 
frequency attenuation for the four cases of both cascodes on, both cascodes off, 
only N cascodes off, or only P cascodes off. We observe from the data that the 
P cascodes are more helpful in reducing low-frequency attenuation than are the 
N cascodes, because the pFETs in our process have a lower Early voltage than 
do the nFETs. With both cascodes on, a 39-stage cochlea has a net attenuation 
that is less than 0.8. We normally operate the cochlea with both cascodes on, 
with VCN = 1.2V, and with VCp = 3.8V. These bias values permit operation 
of our amplifiers over the entire frequency range of the cochlea without any 
saturation effects for input rms amplitudes that exceed 1V rms. 
The attenuation of the gain of signals at other frequencies is by the same 
factor HM. In contrast, the output noise (or distortion) at a cochlear tap is 

76 
NEUROMORPHIC SYSTEMS ENGINEERING 
0.9 
~ 0.8 
~: 0.7 
.9.o 
~ 
0.6 
o ~,. 
._o 0.5 
,g 
Â¢" 0.4 
$ 
4-~ 
'< 0.3 
0.2 0 
, 
, 
, 
~ao oo o 
~ 
OOoo 
Xxx~" 
. o oo Â° Â°oo oo o oÂ° ooo o o o oo Ooo Oo o- 
X 
~ 
ooo 
X x 
~++ 
Xv ~++++++.++++_~ 
^.. 
~ w  
-r 
~T++++ 
~ 
~ 
++__ 
AXxxu 
~ 
t++++ 
~XXXx 
~ 
++ 
XXXXXXXXXxx~ 
~ 
o -~ N ~nd P c~scodes ~ 
X 
+ -~ N cascode o~, P c~scode on 
XXXx 
* -- N c~scode on, P c~scod~ o~ 
X x 
x -~ Both c~scodes off 
X 
~ 
~b 
~b 
~0 
Cochlear Tap Number 
Figure 3.19 
Low-Frequency Attenuation in the Cochlea. The low-frequency attenuation 
for various conditions of open-loop amplifier gain are shown. 
V k 
Vc*Jt 
Figure 3.20 
Noise Accumulation in the Cochlea. The noise at the output tap of a cochlea, 
Your is due to the accumulation, amplification, and filtering of noise from taps preceding 
that tap. 
accumulated through addition over successive stages, as shown in Figure 3.20. 
The noise that is added at the input is attenuated by the same amount as the 
signal, but the amounts of noise that are added at stages successively closer 
to the output tap of interest are attenuated by successively smaller amounts. 
Thus, the output SNR is degraded by low-frequency attenuation. 
To limit the degradation of the SNR of the cochlea through low-frequency 
attenuation, and noise-and-distortion accumulation, we use the architecture 
of overlapping cascades shown in Figure 3.21. Rather than having one large 
cochlea, we use a few small cochleas whose frequency ranges overlap by one 
octave. 
All such cochleas process the input in parallel. 
The filters in the 
overlapping octave serve to mimic the effects of the infinite cascade prior to the 
stages of interest; the outputs of these filters are not used. Since most of the 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
77 
(a) 
REGULAR COCHLEAR CASCADE 
In 
Cochlear Output Taps 
(b) OVERLAPPING COCHLEAR CASCADES 
One Octave 
One Octave 
. 
One Octave 
Input 
Cochlear Output Taps 
Figure 3.21 
Architecture of Overlapping Cascades. (a) In a regular cochlear cascade, the 
input is fed serially to all stages. (b) In an overlapping cochlear cascade, the input is fed in 
parallel to tiny cochlear cascades whose corner frequencies overlap by 1 octave. 
effect of the infinite cascade occurs within an octave of the corner frequency 
of a cochlear stage, we do not sacrifice much in the way of cochlear modeling, 
but we do gain significantly in limiting our SNR degradation. In general, the 
amount of overlap between cochleas, and the number of stages per cochlea can 
be varied to suit the nature of the cochlear application. 
Although the thermal noise in an infinite cascade converges to an equilibrium 
where noise accumulation is matched by noise filtering, the 1/f noise in an 
infinite cascade does not converge and continues to grow in the cascade. The 
1/f noise is significant for only those high-frequency cochlear stages that have 
amplifiers with large bias currents [11]. The overlapp':ng-cascades architecture 
helps to limit the accumulation of 1/f noise. 
A cochlear cascade that is composed of all-pole second-order filters overes- 
timates the group delay of the biological cochlea. The overlapping-cascades 
architecture also helps to reduce the group delay of the silicon cochlea. 

78 
NEUROMORPHIC SYSTEMS ENGINEERING 
The architecture of overlapping cascades may be viewed as a hybrid of an 
architecture that has many parallel filters in a filter bank and of one that has 
one filter cascade with all the filters in serial. 
The cochlea that we discuss in this paper was built out of three 39-stage over- 
lapping cochlear cascades: The low-frequency cochlear cascade was tuned to 
operate in the 100Hz to 900Hz region. The mid-frequency cochlear cascade was 
tuned to operate in the 450Hz to 4050Hz region. The high-frequeny cochlear 
cascade was tuned to operate in the 2000Hz to 18,000Hz region. Thus, each of 
the cochlear cascades had about 11.2 filters per octave, ensuring a fairly sharp 
cochlear rolloff slope. The Qs of the cochleas were tuned to be approximately 
1.5. The voltage gradients in VT corresponding to the three frequency gradients 
of the low-frequency, mid-frequency, and high-frequency cochlear cascades were 
1.040 to 0.9V, 1.130 to 0.990V, and 1.210 to 1.070V respectively. The value 
of VQ that was suitable for operating all three cochleas was -52mV. For the 
remainder of the paper, we shall focuss on the operation of the low-frequency 
cochlear cascade, which we shall call the cochlea. The operation of the other 
cochlear cascades follows by straightforward generalization. The other parame- 
ters that we used for operating our cascades were VOL = 4.93V, VHR = 120mV, 
VpT = 4.25V, VCN "~ 1.2V, Ycp 
: 3.8V, VOT : 0.3V, VRF : 3.0V, and the 
DC value of Vm = 3V. To conserve power, we operated VA at 0.995V in the 
low-frequency cochlea, at 1.05V in the mid-frequency cochlea, and at 1.15V in 
the high-frequency cochlea. It is possible to reduce the power dissipation even 
further by having a tilt in the values of VA in each cochlea. Through exper- 
imentation, we found that VQ = -44mV, -52mV, and -65mV yielded the best 
performance for the low-frequency, mid-frequency, and high-frequency cochleas, 
respectively. We could also speed up the gain adaptation in the mid-frequency 
and high-frequency cochleas by setting VpT in the 4.10V to 4.15V range. We 
used standard shift-register and clocking circuitry to multiplex the outputs 
from the different cochlear taps onto a common output tap. 
3.3.2 
Offset Adaptation 
Figure 3.22 shows the DC output voltage across the cochlea as we scan from 
tap 1 to tap 39. In the absence of any offset adaptation (VoF ---- 4.76V), each 
cochlear stage has a systematic negative offset of about 42mV; by 39 stages the 
DC output voltage has dropped from 3V to 1V. As we strengthen the offset 
adaptation by raising the value of VoF, the offset degradation improves. At 
4.96V, there is little offset accumulation, and there is an almost flat DC response 
across the whole cochlea. Typically, we operate the cochlea at Vo~ = 4.93V 
and tolerate some offset in return for reduced ringing in the offset-adaptation 
loop, and for a lower adaptation corner frequency. 
3.3.3 Frequency Response 
Figure 3.23/a ) shows the frequency response of the cochlea at different input 
amplitudes ranging from 5mV to 1000mV rms at cochlear tap 30. The adapta- 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
79 
> 
v 
0 > 
0 
3 
2.8 
2.6 
2.4 
2.2 
2 
1.8 
1.6 
1.4 
1.2 
1 
Tap I 
Vof = 4.76V 
I 
I 
I 
I 
T~ 39 
Figure 3.22 
Offset Adaptation in the Cochlea. As the loop gain of the offset-adaptation 
loop, controlled by VOF, is increased, the offset accumulation across the taps of the cochlea 
is reduced. 
tion in Q with increasing input amplitude is evident. Figure 3.23(b) plots the 
gain versus frequency such that the curve with the highest gain corresponds 
to the lowest input amplitude of 5mV. The gain adapts from about 30 for the 
5mV rms case to about 0.7 at 1000mV rms. Figure 3.24 shows that the output 
is approximately linear in the input at frequencies before the BF, is compres- 
sive at the BF, and is even more compressive after the BF. These compression 
characteristics are seen in the biological cochlea as well [8]; they arise because 
of the accumulated effects of gain adaptation over several cochlear stages. 
Figure 3.25(a) illustrates that the harmonic distortion is greatest about one 
octave before the BF. This effect occurs because the second-harmonic distortion 
is amplified by the high gain at the BF when the input frequency is 1 octave 
before the BF. When the input frequency is at the BF, the second-harmonic 
distortion drops sharply because 1 octave after the BF there is great attenu- 
ation. These effects imply that nonlinearities in the cochlea cause masking in 
the perception of harmonic frequencies; that is, the threshold for the detection 
of a 2f tone is higher in the presence of a if tone than in the absence of one. 
Psychophysical experiments reveal this effect in humans as well [2]. 
Figure 3.25(b) illustrates the growth and filtering of harmonic distortion 
as the signal travels through the cochlea. The input is a 1V rms signal with 
frequency 162Hz that corresponds to the BF at tap 30. As the signal travels 
from tap 15 to tap 30, the second-harmonic distortion builds until it is at its 
peak value about 1 octave before tap 30 (tap 20). After tap 20, however, it is 

80 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 0 
> 
v 
10 -1 
.-1 
,~ 
~. 10 .2 
E 
< 
~ 
10.3 
n- 
~- 10-4 
o 
10i501 
I dOOmV ' ~  
.
.
.
.
.
.
.
.
.
.
.
.
 
640mV 
320mY ~ 
~
'
~
 
, ~ o m V ~
J
/
/
/
/
~
,
 
T, ~ ~ o 
, 
\ 
.o.v ~.../// 
I 
v.~ = ] ~o~ 
20mV" 
/
/
 
I 
~o~ ~ 
~ 
Stay ~ 
~ 
(a) 
102 
103 
Frequency (Hz) 
102 
100 
(~n 10 -2 
10 .4 
Input Amplitudes 
5mY->1000mV 
(b) 
10-~0 
102 
103 
Frequency (Hz) 
Figure 3.23 
Frequency-Response Curves of the Cochlea. (a) The frequency response for 
various input rms amplitudes is shown. (b) The same data as in (a) except that we plot the 
gain, instead of the ouput rms amplitude. 
gradually filtered away because the second-harmonic frequency begins to fall 
in the cutoff region of the cochlear filters. By the time that the signal is at tap 
30, there is only a small amount of distortion left. Thus, the sharp cochlear 
rolloff ensures that each tap does not suffer much distortion at its BF. 
Figure 3.26 illustrates that, at the BF, the output amplitude and harmonic 
distortion barely change with amplitude for amplitudes beyond about 40mV or 
50mV. The second harmonic is 25dB smaller than the first harmonic for a wide 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
81 
> 
v 
E 
< 
E 
o 
10 0 
i0 -~ 
10 -2 
o -- At BF (162Hz) 
.~1~_ 
~=~ 
x--BeforeBF(4OHz) 
o 
o 
oo 
oo 
o I~ 
* -- After BF (278Hz) 
0 O0 0 
X 
X 
0 
X 
X 
X 
X 
o 
X 
X 
X 
~ ~(~ 
~ 
~( ~(~Y~I 
1 0 3 
-10-4 
10-3 
10-2 
10-1 
10 0 
Input Rms Amplitude (V) 
Figure 3.24 
Compression Characteristics of the Cochlea. The compression at a cochlear 
tap occurs primarily at and beyond the BF, whereas the response at frequencies below the 
BF is linear. 
range of input amplitudes. The reduction in harmonic distortion is due to the 
accumulated effects of the action of the AGC at each cochlear stage, and to 
the sharp cochlear rolloff. Note that, in the corresponding harmonic-distortion 
plots for a single cochlear stage (Figure 3.17(b)), the second harmonic distortion 
at BF is only down a factor of 8 at 1V rms, and there is continued growth of 
all harmonics with input amplitude. 
Although the CF as measured by amplitude curves (Figure 3.13(b)), shifts, 
the CF9o as measured by phase curves (Figure 3.14) does not change apprecia- 
bly. These findings for a single cochlear stage are echoed in the cochlea as well: 
At cochlear tap 30, as Figure 3.27 shows, the phase curves have a relatively 
invariant CF, although the gain curves (shown in Figure 3.23(b)) shift with 
amplitude. The kinks and rising parts of the phase curves of Figure 3.27 are 
due to parasitic capacitances in the cochlear filters. 
3.3.4 Noise, Dynamic Range, and SNR 
Figure 3.20 illustrates that the noise at the output of a cochlear tap has contri- 
butions from the input-referred noise of each cochlear filter preceding that tap. 
To evaluate the total noise at the output we need to evaluate the noise per unit 
bandwidth of each of these sources, to evaluate the transfer function from each 
source to the output, to accumulate the contributions from the various sources, 
and then to integrate over all frequencies. 

82 
NEUROMORPHIC SYSTEMS ENGINEERING 
100 
~-. 10-1 
> 
,~ 
~ 
10 .2 
.i..a 
._ 
_
_
 ~3. 
E 10-3 
< 
~/3 
E 
I~ 10 -4 
~.. 
~ 10 -s 
o 
10i601 
.~% 
I~o Â° 
~,.~ 
~ 
~ o 
,- 
 , xOo 
t 
~X 
o 
â¢ X 
o 
~ 
% 
~ 
o 
(a) 
x 
o 
~ x ~ 
BF 
~ 
~ 
~ 
. 
. . . . . . .  
, 
102 
Frequency (Hz) 
(b) 
> 
0 > 
o--lf 
x -- 2f 
* -- 3f 
Tap 30 
10 filts./oct. 
103 
~ 
Tapl 5 
~ 
Tapl 8 
~ 
Tap 20 
Tap 21 
~ 
Tap 22 
~ 
Tap 23 
Tap 30 
0 
041 
0.~2 
0.~3 
0.~4 
0.05 
Time (Seconds) 
Figure 3.25 
Harmonic Distortion Characteristics of the Cochlea. (a) The harmonic dis- 
tortion is most pronounced 1 octave before the BF, but is sharply attenuated at the BF. 
(b) The dual effect in space reveals that harmonic distortion is most pronounced i octave 
before tap 30 (at tap 20), but is filtered away by tap 30. 
If there are Noct filters per octave, then the frequency ratio r between the ~- 
of any filter and the T of the filter just to the right of that one is given by 
r 
= 
2 -1/yÂ°c~, 
(3.25) 
~_ 
e--1/(Yoct/ln(2)) 
, 
: 
C--1/Nnat. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
83 
> 
v 
E 
< 
E 
0 
10 0 
10 qi 
10 -2 
10 -3 
10 .4 
0 O0 00000~0 
0 
0 
0 0 
0 O0000C(D 
0 00000~ 
x x xx 
x~< 
X xxx 
~ ~ 
X x 
~ 
,~*~ 
.~ 
x xxx)4~< 
x xx)oo~ 
-.:- 
~ ~ 
~ ~ . "  
BF = 162Hz 
o--If 
x-- 2f 
*-- 3f 
10 -5 
10 -3 
10 .2 
10 -1 
100 
Input Rms Amplitude (V) 
Figure 3.26 
Harmonic Distortion at BF in the Cochlea. The total harmonic distortion at 
BF is at least -30dB for all input rms amplitudes. 
-500 
~ -1000 
i~ 
E3 -1500 
~.~ 
t- -2000 
Q- 
-2500 
~ 
Input 
Rms 
Amplitude 
5mV -> 1V 
~ 
~ 
~ 
~ 
~ ~ ~1 
~ 
, 
~ 
i 
~ 
~ 
~1 
"300010 
102 
103 
Frequency (Hz) 
Figure 3.27 
Phase Characteristics of the Cochlea. 
Because the AGC corrects Q, but 
not ]/~-, there is relatively little shift in the phase curves with input rms amplitude. The 
discontinuous parts of the phase curves are due to parasitic effects. 
Thus if x = w~- is the normalized frequency corresponding to the output tap of 
interest, then the filtering effects of the filters preceding the output filter are 
represented by H(x), H(rx), H(r2x),...as shown in Figure 3.20. Similarly, if 

84 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 4 
~_ 10 -2 
o_ 
> 
~ 10 .3 
.~_ 
0 
Z 
~ 
10 -4. 
-
-
 
> 
.~ 10 -s 
Q. 
4,~ 
0 
J 
Tap 31 
10i60~ 
....... 
1'~2 
....... 
1"(~ 3 
....... 
10 4 
Frequency (Hz) 
Figure 3.28 
Typical Noise Spectrum of a Cochlear Tap. The secondary peaks at the high 
frequencies are at multiples of the primary peak frequency and are due to nonlinearities. 
I were the bias current at the output cochlear tap (corresponding to ~ 
at 
each tap), the bias current of the kth preceding filter would be given by I/r k. 
From Eqs. (3.12) and (3.14), the normalized input-referred noise per unit 
bandwidth is given by 
v~(x)dx = \ 2~-~ / 
It is then easy to show that the input-referred noise per unit bandwidth for the 
kth filter preceding the output tap is given by 
4(x) x (Nq.L k  ( 
x2r2   
= \ 27~QC ] 
~ 
+ ~]dx' 
since x --~ rkx and I -~ I/r k. If there are M preceding taps, then the total 
output noise per unit bandwidth V~out(x)dx is given by 
NqVL M 
x2r2 n 
n 
27~QC E r~(~ 
+ ~) 
1-[ IH(rkx)12dx" 
o 
o 
(3.26) 
We obtain the total output noise at the cochlear tap of interest by integrating 
Eq. (3.26) over all x from 0 to ~. Although the expression for the output noise 
at a cochlear tap can be written down, it is hard to solve in closed form. But 
it can be measured easily with a SR780 Spectrum Analyzer. Figure 3.28 shows 
what the noise spectrum of a cochlear tap looks like at tap 31 of our cochlea. 

WIDF~DYNAMIC-RANGE ANALOG VLSI COCHLEA 
85 
> 
E 
O 
> 
E 
< 
O 
104 
10 -2 
10 "a 
10 .4 
10 .5 
Minimum DetectabJeJf~put 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
0.907mV rms input 
SNR = 1.18 
/ 
\ 
/~,~..~-Signal + Noise 
[ a) 
/~ 
(7zmv rm$) 
\ 
~ 
/Noise 
10 -6 
10 
102 
103 
Frequency (Hz) 
100 
[v[axit33um Undistorted Input 
10 "I 
10 -2 
10 .3 
10 .4 
1V rrns input 
THD = 3.87% 
(b) 
315.8mY 
11.85mV 
I.I Im' 
2.82mV 
nV 
10-;0 
102 
103 
Frequency (Hz) 
Figure 3.29 
Dynamic Range of a Cochlear Tap. (a) The spectra of tap 30 when there 
is no input present, and when a BF signal that is just above the threshold of audibility is 
present, are shown. The minimum detectable input at BF was found to be 0.875mV. (b) 
The total harmonic distortion from all harmonics for a IV rms input at BF was less than 
4%. The maximum undistorted input is thus IV. 
It has a form predicted by Eq. (3.26) except for the second and third harmonic 
peaks; these peaks are due to nonlinearities in the filters. 
Figure 3.29 illustrates that the dynamic range at the output of tap 30 of our 
cochlea is greater than 60 dB at the BF of that tap (162Hz): Figure 3.29(a) 
shows the noise spectrum of the background noise at tap 30 which yields a total 
integrated noise of 50mV rms. When a BF sinusoidal signal (162Hz) of 0.907mV 

86 
NEUROMORPHIC SYSTEMS ENGINEERING 
rms magnitude is applied to the input of the cochlea, it is amplified up by a fac- 
tor of 57.1 to 51.SmV. Thus, the rms power of the signal and noise at tap 30 is 
about 72mV rms (V/(51.82 + 502)). Now, at an output SNR ratio of 1, we would 
expect the signal and noise to have an rms power of 50x/~ = 70.7mV rms. The 
fact that the rms power is 72mV means that our minimum detectable signal, 
which corresponds to an output SNR of 1, is actually below 0.907inV. In fact, 
since the system is linear at small input amplitudes, the minimum detectable 
signal is 50mV/57.1 = 0.875mV. Figure 3.29(b) shows that the harmonic dis- 
tortion at a 1V rms input is about v/(11.852 + 2.822 + 1.112 + 0.2452)/315.8 
= 3.87%. This value is less than 4% which is commonly used as a measure of 
the upper limit of dynamic range of measuring-amplifier systems. Thus, at BF, 
we can process input signals over a ratio of 1000/0.875 = 1143 in amplitude, 
or 1.306 x 106 in intensity. This range of intensity corresponds to a dynamic 
range of 101og10(1.306 x 106) = 61.1dB. 
At large signals, the SNR at BF improves for two reasons: The signal ampli- 
tude gets larger--though not in a linear fashion, because of the AGC and the 
noise amplitude drops, because of the lowering of Q. Figure 3.30(a) illustrates 
this effect for a 1V input and for a 0.9mV input. Figure 3.30(b) shows a plot 
of the signal amplitude and the noise amplitude for various input levels. The 
signal amplitude was evaluated as the square root of the power at the BF in 
spectral plots like those in Figure 3.30(a); the power at the harmonic peaks 
was ignored, although the power at these peaks also is due to the signal. We 
evaluated the noise power by integrating the power over all frequencies in the 
noise spectrum. The noise spectrum was obtained by removing all signal and 
harmonic peaks in the spectrum. We interpolated the noise spectrum in the 
regions where we removed the peaks. The noise amplitude was the square root 
of the noise power. 
Figure 3.31 shows a plot of the SNR (signal power/noise power) as a function 
of input amplitude. As the input rms amplitude changes by a factor of about 
61dB in intensity (0.9mV to 1V rms), the SNR changes by a factor of about 
31dB (1 to 1241). 
Figure 3.32 shows how our AGC cochlea extends the dynamic range of a 
hypothetical linear low-Q cochlea. The linear low-Q cochlea can be viewed 
as being representative of just the passive basilar membrane, with no outer 
hair cells [7]. Thus, we call our AGC cochlea with amplification (high-Q) an 
active cochlea, and the linear low-Q cochlea a passive cochlea. Some silicon 
cochleas have been built with a passive cochlea acting as a front end to a bank 
of bandpass filters [13]. 
Suppose that the passive cochlea has the same gain, and the same low Q, 
as the active cochlea at the largest input levels of 1V rms. 
Both cochleas 
will then have the same low-Q noise floor of 8.96mV at 1V. Since the passive 
cochlea maintains the same 0.315 gain at all intensities, its minimum detectable 
signal is given by 8.96mV/0.315 = 28.4mV. The active cochlea has a high Q 
at small input levels such that it amplifies the input signal and the noise. At 
BF, however, it amplifies the signal significantly more than the noise. In fact, 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
87 
100 
10 "1 
10 "2 
10 -3 
10 -4 
10 "5 
10i601 
(a) 
~ 
1V input 
"1- 
> 
v 
o 
= 
~ 
~ 
~ 
= 
= 
~ 
= 
10 2 
Frequency (Hz) 
10 3 
> 
E 
< 
E 
C) 
100 
10 -1 
10 .2 
10 -:0 -4 
::: SNicgi::l (BF at Tap 30) 
(b) 
o 
o 
o 
R 
x 
x 
x 
O 
O 
X 
X 
O 
O 
O 
O 
X 
X 
X 
X 
10 .3 
10 .2 
10 -1 
Input Rms Amplitude (V) 
00 
Figure 3.30 
Signal-and-Noise Amplitude Characteristics. (a) The output spectrum of tap 
30 for a 1V rms and 0.gmV rms input at 13F shows the adaptation in Q and consequent 
reduction in noise. (b) The output rms amplitude of the signal and of the noise at different 
input rms amplitudes are shown. 
its minimum detectable signal occurs when a 0.82mV input at BF has been 
amplified up by a factor of 59 to be just above the noise floor, which has now 
increased to 48.4mV. 2 Thus, the active cochlea extends the dynamic range 
of the passive cochlea by having the minimum detectable signal decrease by 
201og10(28.4/0.82 ) dB = 31dB! It is known that outer hair cells in the biological 
cochlea extend the lower end of our dynamic range of hearing by about 40dB. 

88 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 4 
I:~ 10 3 
g3 
"5 
z, 
E ~0 ~ 
.~ 
f- 
~'~ 
._ 
C~ 
N ~0 ~ 
o 
Tap 30 at BF 
0 
o 
o 
o 
o 
0 
0 
0 
0 
~Â°~Â°o-' 
+o -~ 
+o -+ 
~o-' 
+o Â° 
Input Rms Amplitude (V) 
Figure 3.31 
SNR Amplitude Characteristics. The output SNR improves by about 30dB 
(1 to 1241) as the signal changes intensity by about 60dB (0.gmV to lV) 
Figure 3.33(a) shows the noise spectra of various taps from tap 1 to tap 
37. The corner frequency of the noise spectra successively decrease from tap 
1 to tap 37, while the noise per unit bandwidth successively increases. The 
peak height increases and converges to an asymptotic limit as we traverse the 
cochlea. Note that, because of offsets in the cochlea, there is an abrupt re- 
duction in corner frequency between taps 21 and 25. This abrupt reduction in 
bandwidth lowers the noise below the theoretical value that we would expect 
for taps close to and beyond this region. The total integrated noise over all 
frequencies is shown in Figure 3.33(b). The noise increases due to accumulation 
and amplification as we traverse the filters of the cochlea. However, the suc- 
cessive lowpass filtering limits this growth, until, in the asymptotic limit, there 
is an equilibrium between noise accmlmlation and noise filtering, and the noise 
ceases to grow. The discontinuities in the curve around tap 21 to tap 25 are 
due to the abrupt reductions in bandwidth around this region. The eventual 
convergence of the noise is due to the exponential taper of the cochlea: The 
exponential taper results in an accumulation of noise terms with coei~icients 
that are determined by the terms of a geometric series with geometric ratio r 
(Eq. (3.26)). Since r < 1, the geometric series converges. Note that, as we 
increase the number of filters per octave, by Eq. (3.25), we increase r, and the 
noise increases. There is thus a tradeoff between the sharpness of the rolloff 
slope of the cochlea, which increases with Noct, and noise reduction. The noise 
is also extremely sensitive to the value of Q because of the sensitive dependence 
of the H(rkx) terms of Eq. (3.26) on Q. We used Q = 1.5, and Noct = 11.2 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
89 
> 
E 
< 
E 
O 
100 
10 1 
10 -2 
10;0_ 4 
o -- l~e'ai ~,~ CochIea 
x -- Hypothetical Linear Low-Q (Passive) Cochlea 
0 
0 
O 
~ 
0 
0 
0 
O 
Ã 
O 
High-Q Noise Floor (48.4mV) 
X 
l Low-Q Noise Floor "~.9~'-mV" 
I 
Extension in Dynamic 
Dynamic Range of 
Range 
~ I ~ 
Passive Cochlea 
~ 
v 
~ 
~ 
I 
O.82mV 
[ 28.4mV 
1V 
10 3 
10 "2 
10 "1 
100 
Input Rms Amplitude (V) 
Figure 
3.32 
Extension of Dynamic Range. A hypothetical low-(~ cochlea that is com- 
pletely linear would have a dynamic range of only 30dB due to the uniformly low gain of 
0.315 at all amplitudes; such a cochlea is analogous to the passive biological cochlea with 
no outer hair cells. Our AGC cochlea has a dynamic range of O0dB because faint signals at 
0.82mV are amplified by a factor of 59 to be just above the noise floor of 48.4mV, whereas 
loud signals at IV rms amplitude are attenuated by a factor of 0.315 to prevent distortion; 
such a cochlea is analogous to the active cochlea with outer hair cells. 
as a good compromise between not having too much noise at the output of the 
cochlear taps, and not having broad filters with shallow rolloff slopes. 
Figure 3.34(a) shows the minimum detectable signal and maximum undis- 
torted input at the BF of each tap in the cochlea. The minimum detectable 
signal was measured as described earlier in this section. The maximum undis- 
torted input was measured by finding the input rms value at which the second 
harmonic (by far the dominant harmonic) was attenuated by 25 dB when com- 
pared with the first harmonic. We observe that the maximum undistorted input 
is nearly constant at 1V rms, except for the first few taps, where the action 
of the strong AGCs at each tap have not accumulated sufficiently to reduce 
the distortion. 3 Figure 3.34(b) shows the dynamic range at various taps. The 
dynamic range varies from about 59dB to 64dB. The early taps have little ac- 
cumulation of noise or gain, in contrast with the late taps, which have large 
accumulation of noise and gain. The effect of gain regulation in the AGC causes 
the accumulation of noise and of gain to be approximately in balance, so the 
dynamic range does not suffer a huge variation across taps. However, as we 

10 -1 
(a) 
~ 
10_2 
~1. 
> 
10 "a 
E 
.~ 10-4 
13. 
~/3 
.~ 10 -s 
o 
Z 
10 -6 
10 
37 
90 
NEUROMORPHIC SYSTEMS ENGINEERING 
101 
102 
103 
104 
Frequency (Hz) 
> 
o 
z 
-~ 
> 
$ 
i~_ 
(2) 
N 
~- 
100 
10 -1 
10-~i 
10 -31 
0 
(b) 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
o 
0 0 0 0  
lb 
2b 
3~ 
40 
Tap Number 
Figure 3.33 
Noise Accumulation Across Cochlear Taps. (a) The noise spectra at var- 
ious cochlear taps are shown. (b) The total output noise integrated over all frequencies 
asymptotically converges due to the exponential taper of the cochlea. The discontinuities in 
the curve are due to the discontinuous reduction in bandwidth, in turn due to chip offsets, 
between taps 21 and 25 in (a) 
would expect, Figure 3.35 shows that the maximum output SNR at BF (the 
SNR at 1V rms input) falls as we traverse the cochlea. It is maximum at tap1 
(6.74 x 104 or 48.3dB) where there is the least noise, and minimum at tap 37 
where there is the most noise (649 or 28.1dB). The discontinuities, due to the 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
91 
100 
o 
> 
~ 10 4 
N 
..~ 
â¢ N_ 10 -a 
~_ 
< 
ffl 
~ 
10.3 
$ 
~_ 
c- 
__ 
10 ,4 
0 
(a) 
~
~
,
~
 
- 
~ 
,~ 
~ 
~ 
0 -- Max. Undistorted Input 
x -- Min. Detectable Input 
X 
X 
X x  
X
X
x
x
X
X
 
X 
lb 
2b 
3~ 
4o 
Tap Number 
v 
E 
65 
64 
(b) 
63 
o 
62 
61 
60 
59 
o 
58 
57 
5E o 
0 
0 
0 
O 
O 
1~ 
2b 
3~ 
Tap Number 
O 
0 
0 
O 
40 
Figure 3.34 
Dynamic Range Across Cochlear Taps. (a) The minimum detectable input 
and maximum undistorted input at various cochlear taps are shown. (b) The dynamic range 
at BF at various cochlear taps are shown. 
CF offset of the cochlea around taps 21 to 25, are evident in Figure 3.34 and 
Figure 3.35. 
3.3.5 Spatial Characteristics 
Figure 3.36(a) shows the spatial response of various taps of the cochlea to 
a 162Hz input, for various amplitudes. 
To understand the similarity of 
Figure 3.36(a) to Figure 3.23(a), we can view the cochlea as performing a 
frequency-to-place transformation with -log(f)-~x [7]. Even the harmonic- 
distortion plot of Figure 3.36(b) is quite similar to that of Figure 3.25(a). The 
most severe distortion occurs at a place that corresponds to a corner frequency 

92 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 5 , 
:O 
~ 10 4 
r~ 
z 
$ 
~_ 
g 
O 10 3 
~4 
(~ 
10 
O 
O 
O 
O 
O 
O 
O 
O 
O 
lb 
2b 
3b 
4o 
Tap Number 
Figure 3.35 
SNR Across Cochlear Taps. The maximum output signal-to-noise ratio pro- 
gressively decreases as we travel down the cochlea due to the accumulation of noise. The 
numbers represent the ratio of the signal power to the noise power, that is 105 corresponds 
to 50dB. 
that is 1 octave higher than the corner frequency at the best place (BP). Fig- 
ure 3.37 shows the shift in BP for two different frequency inputs to the cochlea. 
3.3.6 Dynamics of Cain and Offset Adaptation 
Figure 3.38(a) shows the attack response of cochlear tap 30 to the abrupt onset 
of a tone at the tap's BF (162Hz). After a transient at the first cycle, the 
envelope of the response adapts quickly to the new intensity, corresponding to 
the quick onset adaptation of the peak detector. The offset correction has a 
slower rate of adaptation and continues to adapt with some ringing even after 
the envelope adaptation is complete. 
Figure 3.38(b) shows the release response of cochlear tap 30 to an abrupt 
decrease in the intensity of the BF tone. The adaptation of the envelope is 
much slower than that shown in Figure 3.38(a) because of the slow adaptation 
of the peak detector to inputs of decreasing intensity. The DC offset adaptation 
continues to have a rate of adaptation that is slower than the rate of envelope 
adaptation. 
3.3.7 The Mid-Frequency and High-Frequency Cochleas 
So far, we have dwelled almost entirely on the properties of the low-frequency 
cochlea; the properties of the other cochleas are similar. Figure 3.39 shows 
the variation in Q versus corner frequency due to bias-current differences in a 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
93 
10 o 
> v 
"~ 10 4 
._ 
__ O_ 
E 
< 
E 
tw 10-2 
$ 
I~. 
10 < 
~
j
/
 
20mY 
j 
/ 
lOmV 
J 
SmV 
(a) 
~ 
~ 
3~ 
Tap Number 
162Hzinput 
4O 
> v 
E 
.< 
E 
o 
10 o 
~Â°~'"uÂ°Â°Â°Â°Â°~oÂ°ooooooo~ooooooo 
_"L l~2.z~opo~ 
10-11 xxX X:::::::~::::::xXxx 
xxOOÂ°iOo:2rms 
10_2 (X y4y~ ~:~ 
~... X â¢ XXX:~v 
Oo 
!~ 
~ ~Y~ Y~ ~ 
.^ 
0-- If 
~ 
x 
o 
10 .3 
x -- 2f 
I â¢ 
x 
o 
â¢ -- 3f 
~ ~ 
10 .4 
X 
â¢ X x 
â¢ 
i 
~o-Â° 
(b) 
,,', 
8P 
~Â°~ 
1~ 
~ 
3~ 
40 
Tap Number 
Figure 3.36 
Spatial-Response Characteristics. (a) The spatial response at various input 
amplitudes is remarkably similar to the frequency response at various input amplitudes be- 
cause of the cochlear frequency-to-place transformation (log(f) --+ x). (b) The harmonic 
distortion is filtered sharply at the best place (BP); it is at its worst at a place that has a 
corner frequency that is 1 octave above that of the best-place corner frequency. 
cochlear filter. There is a variation in Q as we go from subthreshold behavior 
at low frequencies to above-threshold behavior at high frequencies. However, 
our high-frequency circuits operate in moderate inversion (near the graded 
transition from subthreshold to above threshold), and thus the change in Q 

94 
NEUROMORPHIC SYSTEMS ENGINEERING 
100 
> 
~ 
10 q 
.-~ 
..~ 
~_~ 
â¢ -- 
10-2 
O_ 
E 
< 
~ 
10_3 
~- 10 4 
-~ 
o 
10 -s 
oo-332Hz 
x--162Hz 
ooOOOO 
Â° 
~ 
XÃ 
o Â° 
x ~x 
o o 
X X 
o 
0 O0 
vxXX 
~00 
~vvX x^ 
0 
RR~xxxxx~^^ 
o 
o 
15 
5mVrmsinput 
xXxxXXx x 
X 
X 
X 
X 
X 
X 
o 
: 
O0000 
0 
0 
0 O0 
0 
O' 
3~ 
40 
Tap Number 
Figure 3.37 
The Frequency-to-Place Transformation. The best place for high frequencies 
occurs earlier than that for low frequencies. 
is not significant. 
Figure 3.40 shows that, consequently the "sounds of si- 
lence", that is, the noise spectra at the various taps in the low, mid, and 
high-frequency cochleas are similar in shape across the entire frequency range 
(100Hz to 10kHz). 
3.4 
ANALOG 
VERSUS DIGITAL 
The total resting current consumption of all three of our cochlear cascades 
was measured to be 95#A. Playing microphone speech through our cochleas 
increased the power consumption to about 99~A. Thus, the total power con- 
sumption of our cochlea is about 100#A Ã 5V = 0.StoW. Our area consumption 
was 1.6mmÃ1.6mmÃ3 = 7.7 mme in a 1.2#m process. The pitch of a single 
cochlear stage, including all scanning circuitry and with a conservatively large 
number of power buses (to prevent unwanted coupling through the supplies), 
was 102 #mÃ 444 pm. 
The high-frequency cochlea consumes more than 3/4 of this power. We can 
easily cut our power dissipation to 0.2mW by having a tilt on the VA voltages, 
although we did not implement this tilt on our current design. If only telephone 
bandwidth is required, we can do away with the high-frequency cochlea and cut 
our power dissipation to 0.125mW. If we implement the tilt on the VA voltages 
and do not use the high-frequency cochlea, then our power consumption reduces 
to 50#W. 
We next compare the power and area consumption of our analog cochlea, an 
ASIC digital cochlea, and a noncustom microprocessor (ttP) cochlea. We begin 
by describing the design of the ASIC digital cochlea. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
95 
> 
0 > 
A'Ltack Respc~s~e 
(a) 
Input 
(BF tone) 
Output 
(Tap 30) 
0:5 
; 
Time (Seconds) 
> 
0 
~e~ease__Eespo~e 
~ 
Input 
(BF tone) 
(b) 
Output 
(Tap 30) 
i 
r 
~ 
i 
0 5 
1 Time (Sec165nds) 
2 
2.5 
Figure 3.38 
AGC and OfFset Adaptation. (a) At the onset of a loud input tone after 
a period of silence, there is a brief output transient followed by quick adaptation of the 
envelope. 
The offset adaptation occurs in parallel with the envelope adaptation, which 
happens on a much slower time scale. (b) The reduction in the intensity of a loud input 
tone causes a gradual adaptation in the envelope of the signal. The offset adaptation is still 
slower than the envelope adaptation, but the time scales are more comparable. 
3.4.1 
The ASIC Digital Cochlea 
Figure 3.41 shows a block-level schematic of a digital cochlea, similar to our 
analog cochlea, and described in [12]. Second-order recursive digital filters with 

96 
NEUROMORPHIC SYSTEMS ENGINEERING 
1.5 
v 
~ 
1 
Â¢~0.5 
000 O0 0 O0 O0 0000000000000000~ ....... 
?o 1 
...... 
i'~ 2 
...... 
i~ 3 
...... 
i~' 
...... 
io ' 
Corner Frequency (Hz) 
Figure 3.39 
The (~ across Cochlear Filters. The ~) across various cochlear taps is fairly 
well matched. 
I 0  
-I 
.
.
.
.
.
.
.
.
 
, 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
, 
.
.
.
.
.
.
.
.
 
, 
.
.
.
.
.
.
.
 
~, 
10 -2 
"1- 
~- 
10-3 
> 
~ 
~o -~ 
~1. 
._m 
O 
10-5 
Z 
Frequency (Hz) 
1Â°;oÂ° 
...... 
i~' 
...... 
i~ 2 
...... 
i6 3 
...... 
i6' 
...... 
i0 5 
Figure 3.40 
The Sounds of Silence. The noise spectra at various cochlear taps from the 
low, mid, and high-frequency cochleas are fairly similar in shape. 
tapering filter coefficients model the basilar membrane. Half-wave rectification 
circuits (HWR) perform MSB lookup to model the inner hair cells. Automatic- 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
97 
Vn-1 
CROSS- 
TALK 
r 
Vo 
Figure 3.41 
The ASIC Digital Cochlea 
gain-control circuits (AGC) with cross talk model the olivocochlear efferent 
system. The multiscale AGC is modeled over 4 time scales. 
This is a custom cochlea, designed to be as efficient in power and area con- 
sumption as possible. A digital input, clocked at 50 Khz, forms the input to 
the cochlea; that frequency is slightly over the Nyquist frequency of 36khz for 
the highest-frequency location of the cochlea, and is necessary to obtain robust 
behavior with the filtering and nonlinear operations in the cochlea. It is possi- 
ble to implement a multirate sampling system, but calculations show that the 
bandwidth needed to implement 95 stages of the cochlea from 18Khz to 100Hz 
(as in the analog cochlea) is equivalent to the bandwidth needed to implement 
17 stages at 18kHz. Thus, a multirate system can help only by a factor of 5.6. 
If the overhead in circuitry and complexity needed for a multirate system is 
factored in, there may be no advantage whatsoever. Thus, we shall confine 
ourselves to a system with only one rate of sampling. Note that we need only 
95 stages in the digital cochlea (as opposed to 117 stages), since we do not 
need the redundancy of the overlapping-cascades architecture. To handle the 
input dynamic range of 60dB, (i.e., 10 bits), it is necessary to do fixed-point 
operations at a precision of approximately 24 bits; otherwise, overflow errors 
and round-off-error accumulation can seriously jeopardize the computation. 
The system shown in Figure 3.41 is implemented most efficiently with a 
bit-serial representation, where the bits are processed serially, and each filter, 
HWR, and AGC block is reused 95 times to compute the effect of the entire 
cascade. The reuse of circuitry results in tremendous savings in area and power, 
and makes a digital cochlear implementation feasible on a single chip. There 
is, of course, overhead in the storage that is necessary to implement these 
computations. 
The proposed ASIC digital cochlea was never built. However, we can esti- 
mate what its power dissipation would have been. The Clock Rate is 50 kHz 
Ã 95 stages Ã 24 bits = ll4.0Mhz. The power supply would need to be about 
2.0 V to attain a ll4.0MHz clock rate. Let's assume that the technology is 0.5 

98 
NEUROMORPHIC 
SYSTEMS 
ENGINEERING 
Table 3.1 
Cochleas 
ANALOG 
ASIC DIGITAL 
DEC a 
TECH. 
1.2pm 
0.5 #m 
0.5#m 
VDD 
5V 
2V 
3.3V 
POWER 
0.5row 
150row 
50W 
AREA 
7.Tram 2 
25 mm 2 
299 mm ~ 
#m. The number of gates needed for the computation is roughly 40 (number 
of gates for 1 multiply operation, including storage overhead) Ã 24 (number of 
bits) Ã 7 (3 multiplies in filter and 4 in the AGC) -- 6720 gates + RAM and 
ROM. The 13 add operations comprising 5 adds in the filters and 4 Ã 2 adds 
in the AGC are treated as being essentially free in fixed-point computations. 
The gate.Hz = 6720 Ã x ll4Mhz = 0.77 x 1012 gate Hz. The gate capacitance 
= (0.5 #m x 0.5) #m x 10 (transistors per gate) x 2 fF (cap. per unit area) 
= 50 fF. The switching energy per gate = 50 fF x (2.0) 2 -- 2.0 x 10-13J. The 
power dissipation is therefore 0.77 x 1012 gate.Hz x 2.0 x 10 -13 = 0.154W, 
which we shall round down to 0.15W. The area we would need to build this 
chip is estimated to be 5 mm x 5 mm (in 0.5 #m tech.) -- 25 mm z. 
3.4.2 
#P cochlea 
In FLOPS, we need about 50 Khz (bandwidth) x 95 (number of stages) x 20 
(7 multiplies and 13 adds) = 95 MFLOPs to implement our cochlea. Note that 
adds cannot be treated as free in floating-point operations. On the specfP92 
Ear program, the DEC 21164 running on an Alpha server 8200 5/300 does 
about 1275 times better than a Vax 11/780. The Vax 11/780 is specified at 0.1 
MFLOPS. Thus, the DEC a is capable of 1275 Ã 0.1 = 127.5 MFLOPS which 
is enough for our computation, The DEC a consumes 50 W and has an area 
of 16.5 mm x 18.1 mm = 299 mm 2. 
3.4.3 
Comparison of Analog and Digital Cochleas 
Table 3.1 compares the power and area consumption of the various cochleas. 
Note that our analog cochlea would be more efficient in area by about a factor 
of 2 to 4 if it were also implemented in a 0.5#m technology like the digital 
designs. However, we have not scaled down the analog numbers; we have just 
shown them for our current 1.2#m technology. 
The analog implementations are more efficient in power than are custom 
digital implementations by a factor of 300, and than are noncustom #P imple- 
mentations by a factor of I x 105. The analog cochlea can run on lab batteries 

WIDE-DYNAMIC-RANGE 
ANALOG 
VLSI COCHLEA 
99 
for more than a year (with 100#A current consumption), whereas the best dig- 
ital cochlea would be able to run for only less than 1 day (with 75mA current 
consumption). 
The area comparisons show that, even in an inferior technology (1.2 #m vs. 
0.5 #m), the analog cochlea is about 3 times more efficient than is the custom 
ASIC cochlea, and is about 40 times more efficient than is the microprocessor 
implementation. 
The cochlear comparisons were generous to digital implementations: We 
used a better technology (0.5 #m versus 1.2 #m), operated with a power-saving 
supply voltage (2.0 V versus 5.0 V), used an efficient bit-serial implementation, 
did not include the cost of the 10-bit or 13-bit A/D 
converter, and were more 
conservative in our cost estimates. Nevertheless, the analog implementations 
were two to five orders of magnitude more efficient than the digital implementa- 
tions. To compete with digital systems, the analog systems had to be designed 
with wide-dynamic-range 
circuitry, and had to compensate 
for their offsets. 
In fact, most of the analog cochlea's resources in area were expended in filter 
linearization, low-noise transduction, and offset-compensation 
circuitry. Most 
of the analog cochlea's resources in power were expended in low-noise sensing 
circuitry. 
The number 
of devices needed to do the actual computation 
was 
nevertheless so small that 117 stages could be implemented 
easily on one chip, 
with room to spare. 
By contrast, the digital cochlea's resources in area and power were not pri- 
marily consumed 
in maintaining precision, although extra bits were necessary 
to prevent overflow and roundoff errors. Rather, the actual computation was so 
expensive in digital that only one stage of the cochlear cascade was feasible on 
a single chip. That stage had to be reused 95 times in succession, at a fast rate 
of II4MHz, 
to finish the computation in real time. In other words, the analog 
implementation 
was slow per computational stage, cheap, and completely par- 
allel. The digital implementation 
was fast per computational stage, expensive, 
and fully serial. We might wonder--if the digital implementation 
were slow 
and fully parallel just like the analog one, would the comparisons in efficiency 
seem less drastic? The answer is yes for power consumption because it could be 
reduced by turning down the power-supply voltage and clock frequency. The 
answer is no for area consumption, because it would be 95 times worse. In this 
particular case, however, the size of the chip required for the parallel digital 
implementation 
would be totally unfeasible. In other words, there is no free 
lunch: the inefficiency of using a transistor as a switch will always show up 
somewhere. 
3.5 
THE BIOLOGICAL COCHLEA 
The biological cochlea is far more complex than is our electronic cochlea, and it 
is surprising that we can replicate much of its functionality with just our simple 
circuits. Our aim is not to replicate its functions exactly, as computer modeling 
attempts to do, but rather to exploit its clever computational ideas to build 

100 
NEUROMORPHIC SYSTEMS ENGINEERING 
more efficient electronic architectures for artificial hearing. Such architectures 
may enable the design of superior hearing aids, cochlear implants, or speech- 
recognition front ends. 
In addition, as we shall show in Section 3.5.1, the 
synthesis of an artificial cochlea can help us to improve our understanding of 
how the biological cochlea works. 
The functions of the biological cochlea that we can replicate are: 
1. The frequency-to-place transformation, as implemented by the amplifica- 
tion and propagation of traveling waves 
. A compressive nonlinearity at and beyond the BF of a cochlear tap. Like 
the biological cochlea, our response is linear for frequencies well below 
the BF. Our compression is achieved through an AGC. In the biological 
cochlea, it is still a matter of debate as to how much of the compression 
arises from a dynamic AGC and how much from a static nonlinearity. 
We have reported on cochleas where the compression arises solely from a 
static nonlinearity as well [24]. 
3. An asymmetric attack and release response to transient inputs. 
4. The extension of dynamic range due to active amplification. Our dynamic 
range is extended from 30riB to about 60dB. In the biological cochlea, it is 
believed that amplification by outer hair cells extends the dynamic range 
of the cochlea by about 40dB. 
. The broadening of the pattern of excitation as the input intensity is in- 
creased. The dual effect, which we can also model, is the broadening of 
the frequency-response curves as the input intensity is increased. 
. The shift of the peak frequency towards lower frequencies as the input 
intensity is increased. The dual effect, which we can also model, is the 
shift of the peak place of excitation toward the input of the cochlea as 
the intensity is increased. 
7. A sharp cochlear roll-off slope. 
. Masking of adjacent frequencies and harmonics due to the effects of the 
AGC and nonlinearity, respectively. However, our dominant harmonic is 
the second harmonic. In the biological cochlea, the dominant harmonic 
is the third harmonic. 
3.5.1 3Yaveling-Wave Architectures Versus Bandpass Filters 
Why did nature choose a traveling-wave architecture that is well modeled by 
a filter cascade instead of a bank of bandpass filters? We suggest that nature 
chose wisely, for the following three reasons: 
1. To adapt to input intensities over a 120dB dynamic range, a filter bank 
would require a tremendous change in the Q of each filter. To compress 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
101 
120dB in input intensity to about 40dB in output intensity the filter Qs 
must change by 80dB; a dynamic-range problem in the input is merely 
transformed into a dynamic-range problem in a parameter. In contrast, 
in a filter cascade, due to the exponential nature of gain accumulation, 
enormous changes in the overall gain for an input can be accomplished 
by small distributed changes in the Q of several filters. 
. Large changes in the Q of a filter are accompanied by large changes in 
the filter's window of temporal integration. Thus, in filter banks, faint 
inputs would be sensed with poor temporal resolution, and loud inputs 
would be sensed with good temporal resolution. In contrast, in a filter 
cascade, the shifts in temporal resolution with intensity change only in a 
logarithmic fashion with intensity, as opposed to in a linear fashion as in 
the filter bank. 
. A sharp rolloff slope in a filter is extremely useful in limiting distortion, 
and in enhancing spectral contrasts. A sharp rolloff slope arises naturally 
in the cochlear filter cascade. To accomplish such a rolloff slope in a 
filter bank requires very high-order filters, and consequently an enormous 
amount of circuitry at each tap. In contrast, in the filter cascade, the 
burden of creating a high-order rolloff is shared collectively, so only one 
new filter needs to be added for each new desired corner frequency. 
There are two problems that need to be addressed in a filter cascade: 
1. A filter cascade is prone to noise accumulation and amplification. The 
solution to this problem is either to have an exponential taper in the filter 
time constants such that the output noise converges (the solution found 
at high CFs in the biological cochlea), or to limit the length of the cascade 
(the solution at low CFs in the biological cochlea). The exponential taper 
also results in elegant scale-invariant properties. 
. The overall gain is quite sensitive to the value of each filter's Q. The 
solution to this problem is to have gain control regulate the value of the 
Q's in the cascade. If the gain control is sufficiently strong, then the 
collective adaptation in Q across many filters will compress a wide input 
dynamic range into a narrow output dynamic range. 
3.6 
APPLICATIONS TO COCHLEAR IMPLANTS 
Front-end modules in current cochlear implant devices make use of parallel 
banks of independent bandpass filters. For example, the front-end module of a 
state-of-the-art commercial multichannel cochlear implant devices consists of 20 
fourth-order bandpass filters with center frequencies between 250Hz and 10kHz. 
The filters are implemented using switched-capacitor techniques. The total 
power dissipation of such implementations is on the order of several milliwatts, 
and the dynamic range is only 35 to 40 dB. 

102 
NEUROMORPHIC SYSTEMS ENGINEERING 
Our neuromorphic approach mimics several aspects of the biological cochlea, 
as described in Section 3.5. In addition, our dynamic range exceeds 60dB. Our 
power dissipation for a 117-stage cochlea with a roll-off slope corresponding to 
a high-order filter (10th order to 16th order) is 0.5roW. If we use fewer stages 
and fewer filters per octave to correspond to current values in implant front 
ends, we could, we estimate, cut our power dissipation to 50 #W. This power 
dissipation is about 20-100 times lower than that in current front ends. 
Thus, in terms of biological realism, dynamic range, and power we can do 
much better than current implant front ends. Previously [14], we described 
how a nonlinear center-surround operation on the outputs of the cochlear taps 
can convert cochlear lowpass information into bandpass information without 
degrading the temporal resolution at that tap. A neuromorphic front-end mod- 
ule like ours satisfies the fundamental requirements of future cochlear-implant 
speech processors [3]. 
3.7 
CONCLUSIONS 
We described a liT-stage 100Hz-to-10kHz cochlea that attained a dynamic 
range of 61dB while dissipating 0.5roW of power. The wide dynamic range 
was attained through the use of a wide-linear-range transconductance ampli- 
fier, of a low-noise filter topology, of dynamic gain control (AGC), and of an 
overlapping-cascades architecture. An infrastructure of automatic offset adap- 
tation, small amounts of low-frequency attenuation, and scale-invariant BiC- 
MOS circuit techniques provided robust operation. The low power, wide dy- 
namic range, and biological realism suit our cochlea to be used as a front end 
for cochlear implants. The design of our electronic cochlea suggests why nature 
preferred an active traveling-wave mechanism over a bank of bandpass filters 
as a front end for hearing. 
Notes 
1. We are assuming that the supply voltage limits the range of operation of the system. 
If there is some other voltage that limits the range of operation of the system, then power is 
wasted through an unnecessarily high supply voltage. We choose not to operate the system 
in this nonoptimal situation. 
2. These numbers (gain of 59, noise of 48.4mV, and minimum detectable signal of 0.82mV) 
are slightly different from the numbers that we quoted earlier (gain of 57.1, noise of 50mV, 
and minimum detectable signal of 0.875mV) because of the interpolation procedures used 
in our data processing algorithm, and because of the different times at which the data were 
collected. 
3. We were able even to apply a 1.4V input rms signal and to keep the distortion under 
25 dB (due to the strong AGC), but we refrained from doing so because the input signal then 
would be just at the edge of our DC operating range; operating the cochlea at this extreme 
is possible, but we chose not to so as to leave a safety margin. 
References 
[1] N. Bhadambkar. A variable resolution nonlinear silicon cochlea. Technical 
Report CSL-TR-93-558, Stanford University, 1993. 

WIDE-DYNAMIC-RANGE ANALOG VLSI COCHLEA 
103 
[2] T. D. Clack, J. Erdreich, and R. W. Knighton. Aural harmonics: The 
monoaural phase effects at 1500hz, 2000hz, and 2500hz observed in tone- 
on-tone masking when fl = 1000hz. Journal of the Acoustical Society of 
America, 52(2):536-541, 1972. 
[3] G. Clark. Cochlear implants: Future research directions. Annals of Otology, 
Rhinology, and Laryngology, 104(9):22-27, 1995. 
[4] B. M. Johnstone. enesis of the cochlear endolymphatic potential. Current 
Topics in Bioenergetics, 2:335-352, 1967. 
[5] W. Liu. An Analog Cochlear Model: Signal Representation and VLSI Real- 
ization. PhD thesis, John Hopkins University, Baltimore, Maryland, 1992. 
[6] C. A. Mead. Analog VLSI and Neural Systems, pages 179-192, 279-302. 
Addison-Wesley, Reading, MA, 1989. 
[7] B. C. J. Moore. An Introduction to the Psychology of Hearing, pages 47-83. 
Academic Press Limited, London, 3 edition, 1989. 
[8] M. A. Ruggero. Response to sound of the basilar membrane of the mam- 
malian cochlea. Current Opinion in Neurobiology, 2:449-456, 1992. 
[9] R. Sarpeshkar, T. Delbrfick, and C. Mead. White noise in MOS transistors 
and resistors. IEEE Circuits and Devices, 9(6):23-29, November 1993. 
[10] R. Sarpeshkar, Lyon R. F., and C. A. Mead. An analog VLSI cochlea with 
new transconductance amplifiers and nonlinear gain control. In Proc. IEEE 
Intl. Conf. on Circuits and Systems, volume 3, pages 292-295, Atlanta, 
May 1996. 
[11] R. Sarpeshkar, R. F. Lyon, and C. A. Mead. A low-power wide-linear- 
range transconductance amplifier. Analog Integrated Circuits and Signal 
Processing, 13:123-151, May 1997. Published jointly. 
[12] C. Summerfield and R. F. Lyon. ASIC implementation of the lyon cochlea 
model. In Proc. IEEE Intl. Conf. on Acoust. Speech and Signal Proc., San 
Francisco, 1990. 
[13] A. van Schaik, E. Pragni~re, and E. A. Vittoz. Improved silicon cochlea 
using compatible lateral bipolar transistors. 
In David S. Touretzky, 
Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neu- 
ral Information Processing Systems, volume 8, pages 671-677. The MIT 
Press, 1996. 
[14] R. J. W. Wang, R. Sarpeshkar, M. Jabri, and C. Mead. A low-power 
analog front-end module for cochlear implants. In XVI World Congress 
on Otorhinolaryngology, Sydney, March 1997. 
[15] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692-700, 
May 1992. 

SPEECH RECOGNITION 
EXPERIMENTS WITH SILICON 
AUDITORY MODELS 
John Lazzaro and John Wawrzynek 
CS Division, 
University of California at Berkeley, 
Berkeley, CA 94720-1776. 
l~zzaro~Â¢s.berkeley.edu 
4.1 
INTRODUCTION 
Neurophysiologists and psychoacousticans have made fundamental advances in 
understanding biological audition. Computational models of auditory process- 
ing, which allow the quantitative assessment of proposed theories of auditory 
processing, play an important role in the advancement of auditory science. 
In addition to serving a scientific function, computational models of audition 
may find practical application in engineering systems. Human performance in 
many auditory tasks still exceeds the performance of artificial systems, and the 
specific characteristics of biological auditory processing may play an important 
role in this difference. Current engineering applications of auditory models 
under study include speech recognition [9, 13, 31], sound separation [8], and 
masking models for MPEG-audio encoding [7]. 
Computation time is a major limitation in the engineering application of au- 
ditory models. For example, the complete sound separation system described 
in [4] operates at approximately 4000 times real time, running under UNIX on 
a Sun SPARCstation 1. For most engineering applications, auditory models 
must process input in real time; for many of these applications, an auditory 
model implementation also needs to be low-cost and low-power. Examples of 
these applications include robust pitch-tracking systems for musical instrument 
applications, and robust feature extraction for battery operated speech recog- 
nizers. 

106 
NEUROMORPHIC SYSTEMS ENGINEERING 
One implementation approach for auditory models in these products is to 
design low-power special-purpose digital signal processing systems, as described 
in [5]. However, in many of these potential products, the input takes an ana- 
log form: a voltage signal from a microphone or a guitar pickup. For these 
applications, an alternative architecture is a special-purpose analog to digital 
converter, that computes auditory model representations directly on the analog 
signal before digitization. 
Analog circuits that compute auditory representations have been imple- 
mented and characterized by several research groups - these working research 
prototypes include several generation of cochlear models [25, 27, 32, 90], period- 
icity models [5, 26], spectral-shape models [16, 32], and binaural models [1, 4]. 
A prime benefit of these circuit structures is very low power consumption: 
the circuit techniques used in most of these prototypes were originally developed 
for wristwatch and pacemaker applications. For example, a recent publication 
on cochlear design techniques reports a 51-channel cochlear filterbank that 
consumes only 11 microwatts at 5 volts [90]. Voltage and process scaling, and 
advances in circuit design, could reduce power consumption even further. 
If auditory models offer a performance advantage over standard signal pro- 
cessing techniques in an application, and a compact implementation that only 
consumes a few milliwatts of power is needed, a hybrid system that couples a 
special-purpose analog to digital converter with a low-power digital processor 
may be a competitive alternative to a full-digital implementation. However, 
even if auditory models only offer comparable performance to standard tech- 
niques for an application, an analog auditory model implementation may be 
the best choice for front-end processing, if the system requires microwatt op- 
eration (for example, size limitations dictate a lithium watch battery power 
source). For such micropower systems to become a reality, micropower im- 
plementations of pattern-recognition functions must also be available. Recent 
implementations of micropower pattern classification systems [6] and hidden 
Markov model state decoders [22] are examples of progress in this area. 
Standard analog performance measurements (S/N ratio, dynamic range, 
ect.) aren't sufficient for determining the suitability of analog implementa- 
tions of non-linear, multi-stage auditory models for a particular application. 
This paper documents a more direct approach to evaluating analog auditory 
models: we have integrated a multi-representation analog auditory model with 
a speech recognition system, and measured the performance of the system on 
a speaker-independent, telephone-quality 13-word recognition task. 
The structure of the paper is as follows. We begin with a brief description of 
our multi-representation auditory model hardware implementation. We then 
describe in detail the specific auditory representations we use in our speech 
recognition experiments, and the techniques we use for generating a feature 
vector suitable for speech recognition systems. Next, we assess word recognition 
performance of the system, and compare the results with state-of-the-art feature 
extraction systems. The chapter concludes with discussion and suggestions for 
further research. 

SILICON AUDITORY 
MODELS 
107 
4.2 
SYSTEM DESCRIPTION 
We have designed a special-purpose analog-to-digital converter chip, that per- 
forms several stages of auditory pre-processing in the analog domain before 
digitization [15, 22]. Configurable parameters control the behavior of each 
stage of signal processing. Figure 4.1 shows a block diagram of a system that 
uses three copies of this converter chip: by configuring each chip differently, 
the system produces three different auditory representations in response to an 
analog input. 
Multi-Converter System 
Sound Input 
I 
I Â°Â° 
o~ ~o~o0~o~ 
~ 
Oo 000ooo000o 
........... 
~fl 
~
f
l
~
 
~, ~? 
~ 
Figure 4.1 
Block diagram of the multi-converter system. 
This system acts as a real-time audio input device to a Sun workstation: 
a pre-amplified microphone input can be connected directly to the converters 
for a low-latency, real-time display of spontaneous speech. Alternatively, the 
system can receive analog input from the 8 Khz sampling rate, 8-bit mu-law 
audio output of the workstation, for controlled experiments: all experiments 
reported in this paper were done using this method of sound presentation. The 
dynamic range of the converter chip is 40 to 60 dB, depending on the signal 
processing configuration in use: input sensitivity is 1 mV (peak), and maximum 
recommended signal amplitude is 1 V (peak). 
Figure 4.2 shows the analog signal path of the auditory pre-processor in the 
converter chip. Processing begins with a silicon cochlea circuit [27]. A silicon 
cochlea is an' analog circuit implementation of the differential equations that de- 
scribe the traveling wave motion of physiological cochleas. The cochlea design 
used in this chip maps a linear, one-dimensional partial-differential equation 
into circuits, as a cascade of continuous-time filter sections with exponentially 
decreasing time constants. The second-order filter sections have a low-pass 
response, with a slight resonant peak before cutoff. The cascade acts as a 
discrete-space, continuous-time finite-element approximation of the partial dif- 
ferential equation. 
Like wavelet filterbanks, the silicon cochlea outputs balance temporal and 
spectral acuity. The cochlear output response, a lowpass filter with a sharp 

108 
NEUROMORPHIC SYSTEMS ENGINEERING 
Audio 
Input 
Silicon Cochlea 
I 
I 
1 
I 
I 
l 
1 
1 
1 
Sensory Transduction 
l 
i 
I 
I 
I 
I 
I 
l 
l 
Temporal Autocorrelation 
i 
i 
i 
I 
I 
i 
I 
I 
I 
Temporal Adaptation 
I 
i 
i 
i 
I 
i 
1 
i 
i 
(l l ~ Outpnts~ 
Figure 4.2 
Analog signal path of the silicon auditory model. 
cutoff and a slight resonant peak, derives its spectral selectivity from the col- 
lective interaction of the slightly-resonant circuits in the series cascade, not 
from parallel highly-resonant circuits as in a standard filterbank. By avoiding 
highly-resonant filters, the cochlear processing preserves the temporal details 
in each output channel. 
This cochlear design is the first stage of processing in our chip. The cochlea 
consists of 139 filter stages; we use the outputs of the last 119 stages. The first 
20 outputs are discarded, because their early position in the cascade results in a 
poor approximation to the desired differential equation solution. Four param- 
eters control the tuning of the silicon cochlea, supporting variable frequency 
ranges and resonance behaviors. 
Next in the signal processing chain (Figure 4.2) are circuits that model the 
signal processing that occurs during the sensory transduction of mechanical 
motion in the cochlea. These operations include time differentiation, half-wave 
rectification, amplitude compression, and the conversion of the analog waveform 
representation into probabilistic trains of fixed-width, fixed-height spikes [43]. 
Each of the 119 cascade outputs is coded by 6 probabilistic spiking circuits. 
Note that no time averaging has been done in this signal processing chain; the 
cycle-by-cycle waveform shape is fully coded in each set of 6 spiking outputs. 
Different secondary representations in the brain use the cochlear signal as in- 
put, and produce outputs that represent more specialized aspects of the sound. 
In our chip processing chain, two signal processing blocks follow the sensory 
transduction block, that may be used to model a variety of known and proposed 
secondary representations. 
The first processing block (Figure 4.2) implements temporal autocorrelation, 
in a manner described in detail in [16]. The six spiking outputs associated with 
each cochlear output are sent into a single temporal autocorrelator, which pro- 

SILICON AUDITORY MODELS 
109 
duces a single output. Six parameters fix the autocorrelation time constant and 
autocorrelation window size at both ends of the representation; autocorrelation 
parameters for intermediate taps are exponentially interpolated. 
The temporal autocorrelation block can be configured to generate a repre- 
sentation that codes the spectral shape of a signal. To generate this spectral 
shape representation, the autocorrelator associated with each cochlear channel 
is tuned so that the best frequency of the cochlear channel matches the first 
peak of the autocorrelation function [30]. Figure 4.3 illustrates the algorithm: 
Figure 4.3(a) shows the frequency response of a cochlear output, Figure 4.3(b) 
shows the output of a temporal autocorrelator tuned to the best frequency of 
the cochlear output, and Figure 4.3(c) shows the effect of combining the tem- 
poral autocorrelator and cochlear filter. By cascading the cochlea and temporal 
autocorrelator blocks, a narrow, symmetrical filter is created; this filter is non- 
linear, and achieves a narrow bandwidth without using a highly resonant linear 
filter. 
7OO 5oo ~ 
~ 
r.~ 
3OO 
]00 
0,01 
0.1 
I 
kHz 
(~) 
?oo 
5o0 
300 
Ioo 
10 
0,01 
. 
o.1 
, 
,, 
i 
~ 
0,01 
0.1 
kHz 
kHz 
(b) 
(c) 
I 
10 
250 
~ 15( 
50 
Figure 4.3 
Periodicity-based spectral shape computation: (a) silicon cochlea tuning re- 
sponse (b) temporal autocorrelator tuning response (c) the combination of cochlear and 
autocorrelator processing. 
The final processing block in the signal processing chain (Figure 4.2) imple- 
ments temporal adaptation, which acts to enhance the transient information 
in the signal. Figure 4.4 illustrates temporal adaption: in response to a tone 
burst (top trace), the circuit produces a series of pulses (bottom trace). The 
number of pulses per second is highest at the onset of the sound, and decays 
to a lower rate during the unchanging portion of the tone. Five parameters fix 
the time constant and peak activity rate of temporal adaption at both ends 
of the representation: parameters for intermediate taps are exponentially in- 
terpolated from these fixed values. These parameters support a wide range of 
adaptive responses, including temporal adaptation behaviors typical of audi- 
tory nerve fibers, as well as behaviors typical of on-cell neurons in the cochlear 
nucleus. The circuits used in the temporal adaptation block are described in 
detail in [177]. 

110 
NEUROMORPHIC SYSTEMS ENGINEERING 
~ 
',,-;.=..:~.%, 
, 
, 
:. ;~ ~-:~ 
Figure 4.4 
Temporal adaptation: top trace is audio input (gated tone burst), bottom 
trace shows adaptive response. Bar length is 5ms. 
As shown in Figure 4.4, the final outputs of the auditory model take the 
form of pulse trains. 
These pulses are fixed-width, fixed-height, and occur 
asynchronously; they are not synchronized by a global clock. The information 
sent by a spike is fully encoded by its moment of onset. 
In collaboration 
with other researchers, we have developed efficient methods to transmit the 
information from an array of asynchronous spiking circuits off chip [11], and to 
combine the information from several chips to form a single data stream in an 
efficient way [20]. We use these methods in our multi-representation system. 
Figure 4.5 shows the programmer's model of this data stream. Data from the 
system takes the form of a list of "events": each event corresponds to a single 
spike of an output unit from a chip in the multi-representation system. Each 
event includes information specifying the chip sending the spike, the cochlear 
channel associated with the spike, and the moment of onset of the spike. The 
onset timestamp has a resolution of 20#s; event lists are strictly ordered with 
respect to onset times. 
Data Format for an Event (32 bits) 
XXXXXXXXXXXXXXXX 
XXXXXXXXXXXXXXXX 
16-bit Timestamp 
4 
Output Unit 
(LSB - 20gs) 
Chip Number 
| 
Figure 4.5 
Programmers interface for events. 
We designed a software environment, Aer, to support real-time, low-latency 
visualization of data from the multi-converter system [15]. The environment 
also supports a scripting language for the automatic collection of system re- 
sponse to large sound databases. 

SILICON AUDITORY MODELS 
111 
4.3 
REPRESENTATIONS FOR SPEECH RECOGNITION 
We configured our multi-representation system to generate specialized represen- 
tations for speech analysis: a spectral shape representation for voiced speech, a 
periodicity representation for voice/unvoiced decisions, and an onset represen- 
tation for coding transients. Figure 4.6 shows a screen from Aer, showing these 
three representations as a function of time: the input sound for this screen is 
a short 800 Hz tone burst, followed by a sinusoid sweep from 300 Hz to 3 Khz. 
For each representation, the output channel number is plotted vertically; each 
dot represents a pulse. 
The top representation codes for periodicity-based spectral shape. 
For 
this representation, the temporal autocorrelation block generates responses as 
shown in Figure 4.3, and the temporal adaptation block is inactive. Spectral 
frequency is mapped logarithmically on the vertical dimension, from 300 Hz to 
4 Khz;'the activity in each channel codes the presence of a periodic waveform 
at that frequency. The difference between a periodicity-based spectral method 
and a resonant spectral method can be seen in the response to the 800 Hz sinu- 
soid onset: the periodicity representation shows activity only in a narrow band 
of channels, whereas a spectral representation would show broadband transient 
activity at tone onset. 
The middle representation is a summary autocorrelogram, useful for pitch 
processing and voiced/unvoiced decisions in speech recognition. This represen- 
tation is not raw data from a converter; software post-processing is performed 
on a converter's output to produce the final result. The frequency response 
of the converter is set as in the onset representation; the temporal adaptation 
response, however, is set to a lOOms time constant. The converter output pulse 
rates are set so that the cycle-by-cycle waveform information for each output 
channel is preserved. To complete the representation, a set of running autocor- 
relation functions x(t)x(t 
- 
~-) is computed for T = kl05tts, k = 1... 120, for 
each of the 119 output channels. These autocorrelation functions are summed 
over all output channels to produce the final representation, a summary of 
autocorrelation information across frequency bands. T is plotted as a linear 
function of time on the vertical axis. The correlation multiplication can be 
efficiently implemented by integer subtraction and comparison of event times- 
tamps; the summation over channels is done by merging event lists. Figure 4.6 
shows the qualitative characteristics of the summary autocorrelogram: a repet- 
itive band structure in response to periodic sounds. In contrast, the summary 
autocorrelation function of a noise signal shows no long-term spatial structure. 
The bottom representation codes for temporal onsets. For this representa- 
tion, the temporal adaptation block is active, and the temporal autocorrelation 
block is inactive. The spectral filtering of the representation reflects the silicon 
cochlea tuning: a low-pass response with a sharp cutoff and a small resonant 
peak at the best frequency of the filter. Temporally, the representation pro- 
duces a large number of pulses at the onset of a sound, decaying to a small 
pulse rate with a 10ms time constant. The black, wideband lines at the start of 

112 
NEUROMORPHIC SYSTEMS ENGINEERING 
4 Khz 
(log) 
300 Hz 
0 ms 
(linear) 
12.5 ms 
4 Khz 
(log) 
300 Hz 
~= 
:-----'~W 
~:~.~ 
-.::~,., 
,~=:~. 
.... ~:, ,,~ 
.::~ ~;.~- 
~.~!.$ 
, .,~V~ 
_ 
.:i~:!~ ' 
.. ::~ 
-~ 
~ z .  
. 
. 
' 
~ 
~:.,. 
~ 
i~;~:~22~5~,:~.~.:,i,,(:~,~.,,.:,:,~:~:.,~::.-~ 
.... ::~,.,,,: 
~"-: ~ :  
~ :  
~ "  
~':~-~'~ 
"-~-~,~:~;~.;~:,,:.~A.~:~;'~.:~Z2,~.~:::.': 
,~:~ .... 
:'" ' 
~" A~'. 
~ 
~ Y ~ : : ~ - T ~ ' Â£ - m ~ : ~ : 5 ~ : ~ " ~ ' ~ : ~  
~,~': 
~" ~
:
 
~ 
~ ; ~ 2 ~  
~ 
~ ~ =  
~2:;::~,~.: 
..~. ~: ,~ ~ ~&N ~::~-~::-~4~# :~x:~:::2;~:~:.~ ~ 
~, ~ 
~. 
~ 
~ 
.~# &~-~ 
~: ~ . : ~  
z~-~.,..~ 
~.~ ..,~,~ .:,:~-.~+ 
)~.:~.:, 
~: ~.~.:~ 
e<~':~j.~;s~:~#:~,~4~:~Â¢~:~:~,Â¢.(~,.:..aa.-::.,.:..:,.:~ 
~ ~ ~: 
. ~'~'Â£~u~=~..:.C_~+~:~3.:~W~'~:,~'~,~:~? 
~22.;:::.~,.{ 
t i~A~:,,;Â¢: ~:~~:~ 
â¢ .~,. ..... 
f
~
 
-5;~:~:J~{~:if;~;:,~:~: 
~,.~:.::. 
,~ ~ ~ ~::~':~ i'~%~,~.~:;Â£~,~:~:~:~5~;%-2~ 
:~$~i ~ 
~O;<)~4~ ~ ~;+;~":~ ~;~:~::~::~a~: :~ :5'~: 
. 
~. 
~.~ 
~ 
~.~ ,, ~ 
~ 
~.- ~;~.~,~ 
~..~..~ . ~ . ~ . - ~ . ~  
â¢ f~ 
~,~ 
~ ~ : ~ x ~  
~ ~ 
~ ~ 
~,~ 
~- ,- ,,, ~., ............. 
............. ":, 
"~'":" 
~ ~ ~ ~ !' ~ /$;~ ~ f/~ ~" ~%C~(,~:~;';~:&'~::'~?~2:~:~.::~:~ 
â¢ â¢ â¢ .~ ~g, 
~ ~ ~ .,..,v~, ~ t~ 
~v ,,~Â¢;,c.~- ~:~.- .,a~- ~ 
~Â¢~,~.:~.~,~ .~,,,,~.:~,~.,~Â¢ 
- 
~ :~)~; 
~ $ ~ z : I ~ , ~ , ] ] ~ 1 / , ~ ' : ~ ' ~ . ' ~ : ~ ; ~ : ' ~ 5 " : ~ : ; ~  
~;~::~.5 ~ 
~,:/~::~ 
:~4~~ 
~:.::~,-':~: .'; :' ,:~:,~::~, 
~ ~.:~.;~:~ '~:;~;;.;i~:~::;:~::~: 
. . . .  
...... 
~::.:::;,'.: 
~:~!: ~,;.';:~:,~f~':-~;: 
~.?.~:~..~: :: 
..................... 
~. 
:. 
i~:r:.::~::)-?~;~L:: 
~: ~: .~~ ~.~ 
$ ~: ~)~ :::: :.~ â¢ 
" 
~.?~:'~(~::L 
$~ ~ 
â¢ 
~ 
[~. 
I.~ 
L__I 
200 ms 
Spectral 
Shape 
Summary 
Auto 
Corr. 
Onset 
Figure 4.6 
Data from the multi-converter system, in response to a 800-Hz pure tone, 
followed by a sinusoidal sweep from 300Hz to 3Khz. 
the 800 Hz tone and the sinusoid sweep illustrate the temporal adaptation; the 
tuned response throughout the sinusoid sweep illustrates the low-pass spectral 
tuning. 
Figure 4.7 shows the output response of the multi-converter system in re- 
sponse to telephone-bandwidth-limited speech; the phonetic boundaries of the 

SILICON AUDITORY MODELS 
113 
two words, "five" and "nine", are marked by arrows. The vowel formant in- 
formation is shown most clearly by the strong peaks in the spectral shape 
representation; the wideband information in the "f" of five is easily seen in 
the onset representation. The summary autocorrelation representation shows 
a clear texture break between vowels and the voiced "n" and "v" sounds. 
,~.~ 
~ 
~ 
Ii~t, ~Im ~ 
T~ 
~ 
f~.:::.... ..... 
i 
- ve . 
.~ ~: 
...~,, ~ 
': 
:.-. 
.;:!~ 
z<.~- ~ 
f 
'5~ ' ~'F ~%~: :.. 
r 
â¢ " - 
~:'.~)~.'} 
; 
":"...~ . '.,~i~i~l~.,.~g.>~,F~i' { 
..... 
2~TZiR?.~!Z:~G=.~g:3~:iY,.r.,~ 
. . . . .  
~,~.l..g.~a~z ~ ~:..:.x:~,, 
...... 
~ ~i;;.~t'{~g~ 
~ ~'~:" '," 
............ 
:';~>-.?}'}~O 
,~ 
: 
.g~,~-.{., .
.
.
.
.
.
 
{:':.: ; 
~" : 
{ 
. . . . . . . . . . .  
~.. 
f~ 
i 
, ve 
~ ~i'~,,~?:777;:,~,~:~;",:.~:,: 
~: ; .,:, ..,,:77 ?, " 
~ g~:,.-a~ ": ::.~; ,~ ; ,. i ~ ". ~ ~ A.,:2,..~'~ ~a "~'~ ~ { ~ = ; ~ 4~ ~.;~, ,7 
1~" 
'~.:~ 
~.'.'Â¢~. 
:~'.* 
", .~'. 
:j~,x' 
,- 
, .,.,,, ~,~.~: ,,.g.,~,.<..::y: ~a~ 
~ . ~: 
;..~::. 
~ ~;:~'.Y~: 
e3~.,~;..~r:9,: 
,.~.,.~,~,TÂ¢~,~7~Â¢. 
r g~ ~ , ~  
~.,#.,;Â£.~.~{,: #/:~ ~.',.~ ;'-~{~,(.g~ 
~ ,~ ', ,. 
â¢ , -:'=. ,," ~ .~ ~"'"." 
: ' ".', "'.'~m.'w, :~-'.~. 
~ , ~  
, 
,..~.~.,~,.,~. ~'~:~,,:~,':... ..7:cx~.~: 
"::Â¢~':-"Â¢~:~:',~,~,./~%, 
"'~=~'a'.'~+"v 
- 
g~)a~.,,'~g,.g~9.V.:i4:t~.,~.d~.;::~{.Â¢~.a;~j~ 
, 
.~a~?::,z~N~F,#:~.;:~,:;~{4 
2, :. 
â¢ ,~.~*i;.;~.?~ 
~5"~d~.,~2" ~ .  
%~-iv ~a. ~:~ 
NN~:,~,~f~.~,'~i 
~2..i.~:3~%.~?.Â¢. 
"
~
"
 
'" 
~
3
.
~
-
 
r,-~: ~f, 
~' ~'" 
::.~,~-~..r:.. 
.,.-.-'.-r,. 
. 
~.a-.~.,. ~.... 
~ ~a~*~, 
~Â¢i.:~d:%~i.~ 
,,,, .~, 
~2~.i~#:t~'~.~ 
,,-.~ ...... â¢ .~ ......... 
~ 
,.. 
~.,q .......... 
' ~:~"55~~.~5~35',~'~:g.~>~& 
;~'~d~'~;~:.'::'" 
! ~ ;~ 7~ >D g{4 g;:::~&.2k X:.,. ~;.. ~*e: ;'.'<::~gg... 
{ ~'.'~"2g-~v.~.-,.gk:Â¢~,.,~;,:.,.-: 
;.~,:,,~';Â¢.'~qu~'.<,~ 
~ ~f.~#~,~?Y~,h~9>?<~eee-~a'~'Â¢#~'- 
~ 
~ ~.:,',~,'..Â¢" .,,,~ *,.,, :%,':,q~,,~:~:~',-,a~a = :~4a~'~, ~ 
~ 
?~i~.h,F~:~..~: e~."~ 
: 
~*~!/;iei{~'~*~.~ Z~+~Â£~4'~ Â¢.2: &' gq~:;~.~Z, Le~" i- 
~,,~..~: 
': ~'~,;~,~a,~,,e~Â¢~r ~..:~-~, ,.~.~v,::~. ,.. 
: ~ a ;  
...~ ~?.~.,~ 
,.. ~ ,.., .v ~,.~..JT"~:.~:Z-~ .:.~ 
- 
{~# ,=~',~7~g~A: 
,~,." 
--' ~"" 
f} 
~ 
. ve 
, ~::':~}';,~7,c 
:~:": b'~'.'. ~':<" '."'":' ..'~'.?; <':'.~''-: 
~...2 :.. ..',#: ?2.3..: ~:&:: 2....~ Z ~.:; .~ ?;! .... ', 
.,~,-......~ 
,..,..,:.::.., 
,.~.:.....,.... 
,. 
~ ?~;:...v~.,. ,,..: ~.7;~.:.:~:.:!.i::.:4.:5,;,;;. 7, 
~" ,.; ='~'.,,,..: .','.;,','.~,.'... 
= ,.,...,,,,:: 
.,. 
i~(:- 
:" ."." ;~.<':..:: 
):..'.',:..:', 
=.. 
," 
, ,,':~ ' 
i~?'-_':~?:i:<';? 
.'.".~'~., 
..... 
":"' 
" 
â¢ : ~ .: .. :,;::. ,,.:::,-. ?- ;~ ~.', ':~':: .~ :..., :, :::. ;-:~[ :( ~<. ",., 
, ~'::','.,'.'" 
"}1,;,q;'",": 
,<C,G;;'~.~,~: 
~;J..-:.. 
".:2 
"q. 
:, 
. 
. 
:~,,.:'.. ; :..?~. ,,., ; 
,,=~,..,. 
...... , K 
n 
.
.
.
.
.
.
.
.
 
i 
. 
.. ne 
_ 
'-, -.,," 
.............. 
~ . . . . . . . . . .  
~_;-'..,;Â¢Â¢.;~....~, 
,.,. 
":i'i" ~ ~.i':;i:i;~ ~ F~;-~i 
- :~ ~:2"~ 
~ ~" ":~.~, ~ .? ~" 
,L~fJ~5~. . ~ .~" Y~;~.~f 
t~] ~
K
~
 
, ~,U . t'F" ," 
:~,~ :;;~;:.-" ~ ~: 
.-~ 
~ 
--~ 
.......... 
.~,,.~ 
â¢ ..,,~:~D 
..... 
~. 
~. 
. 
, 
." 
- 
_. 
2"'' 
..... 
: 
~- 
~ 
â¢ 
. ne 
_ 
2 
'. . ,~,:~.~e ~,4:~2Z.~:~:, 
"" ~':~> " 
..~--~" " ~I~&~-?~::g~'.~2~I',~'.~,,:~-:',G:.: 
',:" 
:.:.'.~v~>.~:,,:::,, :, 
~'~.::",~,, 
~'.,~ ~,~'~'.'~,~.~,,~:~ ,, 
~::~Z~.~:-~~:~,'~'~ ~Z~'f~e~,:~:X~,,~2Z~,'5~Z,",, 
+ ~ 
:.#~hL~?~5~@~:;~4~'~,.'~::~t~'~'": 
:.., 
, ~: ~.~:~,,~:~4~.%~.~:~&~,'~; 
:..r~2::' t.2 
;:~,'~',~;~ii~'~;~;~Y~.~:i.~:Z 
~.',~X~,:~,,~C~;~'~'~,:,~~Â£~ 
, 
r~ ~'~Z~.. ;, ~ - ~  
~
~
 
. .,.~.~ ,: 
, .~. 
~..~,~,Â¢~-..~...~.,F~.~-...#~*~v~%..: 
, ~ ~t' ~:' 
~," ~, ,~',.',~,' ~Z ~ v,. ,c..,~ ," ,. ,: :, : ,,~ ',, ,..~ â¢ 
â¢ "~.~,.~.,.,~:~~.~:z.~.,~.~Y~.'~a,:~:~:.~ 
~ 
: 
" 
, 
,~,~.~,:,...:..,.,~ 
~.-~..,..,,,~,~.~,~ 
~,,. 
. 
~,~74~-~:~: ~j~2~ ~:Â£~';~@~ :~7.~.:7~:~:,:..: ).:.2 ~-" , 
%e~_~=~:.r~g,-~:.{}~.~:~.~.~;~a:~aq2. 
â¢ â¢ 
" , ~ 2 : ~  
>~?:.: :5~:E~>V:': 
~ ~:~: 
" ~ 
" ~ 
.. ~a. 
e .~'.,v..'ewav,..a,,vay~ 
~..> 1...:1. 
, :,,~v~:~:~.v<...,.e~+:~,,~.e.-,r..:~.Â¢dg;a;~..~. 
,, 
_C~.,i~':~2;~.~Z:~5~:&kL:~"e:~.<4~2:~:~7 E"?~Fe:. 
. 
â¢ .~Â¢ 
:.,.:ma~., 
,',~<~" .,r<,.:.*~,<,:~.:.,=..:,, 
Z~,..~i~.~7~.~.~,.~,,~..~.;.~...~ 
~ : " 
n 
. 
1 
~ne 
~,+, .,c,.,.,. ,,Â¢,,~.,,: ,.:,~,,., ,~,,.:,,,.~ .... ,,.. 
, 
,, 
, 
"~"= "5- 
. " "'~,';" 'L'.. ~:.'~< "..= ....:, 
.: '~ : " ." 
,=;..~ =,:::q..~.~...:..~<=>.....=,.:. :...~-,,,,: : 
â¢ 
. 
:~-: : 
4 
Khz 
300 Hz 
0ms 
r- 
-Â£ 
.< 
L" 
g 
12.5ms 
4 Khz 
Â© 
300 Hz 
~ 
~'~"~'~'I~'~'2 
I 
~ 
I~S 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
[ _ _ I  
l O0 ms 
Figure 4.7 
Data from the multi-converter system, in response to the word "five" followed 
by the word "nine". 

114 
NEUROMORPHIC SYSTEMS ENGINEERING 
4.4 
FEATURES FOR SPEECH RECOGNITION 
The data shown in Figures 4.6 and 4.7 share many properties with neural re- 
sponses. Each output unit codes information asynchronously, and the effective 
sampling period is adaptive and data dependent. Each representation in the 
system is specialized for a certain property of sound. These representations 
are not uncorrelated: there is considerable redundancy between the represen- 
rations, and among output units of a single representation. These properties 
of neural auditory representations are summarized in Figure 4.8. 
Auditory Models 
Speech Recognition 
Adaptive Sampling 
Specialized Features 
Multiple Representations 
High-Dimensional 
Correlated Features 
Uniform Sampling 
General-Purpose Features 
Single Representation 
Low-Dimensional 
Uncorrelated Features 
Figure 4.8 
Comparison of auditory representations and current speech recognition tech- 
nology. 
Also shown in Figure 4.8 are the contrasting properties of conventional fea- 
ture representations used in speech recognition systems. These representations 
generate features at a single uniform frame rate, typically 10-20ms, unsynchro- 
nized to acoustic features. A single, general-purpose spectral representation 
is typically used: often, both signal energy information and pitch contour in- 
formation is removed from this representation. Finally, low-dimensional repre- 
sentations are used (5 to 15 elements, typically), and the components of the 
representation are often uncorrelated. These front-end properties reflect the 
statistical and architectural properties of recognition systems. 
Figure 4.8 depicts a "representation-recognizer gap" that complicates the 
use of auditory models for speech recognition. We address this issue in two 
ways: by transforming the representations shown in Figures 4.6 and 4.7 to have 
properties closer to conventional front-end representations, and by choosing 
speech recognition technology that is more compatible with auditory models. 
The method we used to extract a feature vector from our multi-representation 
system output is described below. 
The first step in feature extraction is to convert the asynchronous, event- 
list representation into a sequence of uniformly sampled frames. Each frame 
output consists of 3 vectors (one for each representation) with 119 floating 
point elements (one for each output unit), and codes the spike activity that 
occurs during a 25ms interval. Subsequent frames overlap in time by 12.5ms. 
To generate each frame element, the spiking pattern during the 25ms interval 

SILICON AUDITORY MODELS 
115 
is considered as a train of delta functions with unit height: this function is 
multiplied by a Hamming window. After multiplication, the heights of the 
delta functions are summed to yield the final floating-point feature element 
value. These operations are graphically shown in Figure 4.9. 
[ 
(a) 
I 
I 
25ms 
I 
25ms 
Figure 4.9 
Graphic description of algorithm for converting the asynchronous event-list 
representation into uniformly sampled frames. A 25ms series of unit-height events (a) is 
scaled by a Hamming window (b). The heights of the scaled events are added to form the 
frame value. 
To reduce the size of the spectral-shape and onset representations, we sub- 
sample the original ll9-element vectors using symmetrical triangular filters 
with a 50-percent filter response overlap. 
This subsampling produces a 5- 
element vector coding onsets, and an 8-element vector coding spectral shape. 
The subsampling procedure is graphically shown in Figure 4.10. 
[- 
(8 elements) 
-] 
Reduced Vector 
Reduction Filters 
[- 
(119 Elements) 
"~ 
Feature Vector 
Figure 4.10 
Graphic description of algorithm for subsampling full 119-element represen- 
tations into a reduced feature vector. 
To reduce the size of the summary autocorrelogram, we compute the discrete 
cosine transform of the ll9-element vector, and use the first two components 
of the transform as the summary autocorrelogram feature vector. We choose 

116 
NEUROMORPHIC SYSTEMS ENGINEERING 
this reduction technique to enhance the coding of voicing in the representation, 
while de-emphasizing formant information. 
These reduction techniques resolve the feature-size table entries in Figure 4.8; 
they do not, however, address the correlation between feature elements. As 
detailed in the next section, we use recognition technologies that are relatively 
insensitive to correlations between feature elements to address this issue. 
To complete our feature vector computations, we compute temporal differ- 
ence feature vectors ("delta" features) for each primary representations, using 
a 5-frame window to compute differences. The resulting feature vector has 30 
elements: 8 spectral-shape elements, 5 onset elements, 2 summary autocor- 
relogram elements, for a total of 15 primary elements, together with 15 delta 
features. 
4.5 
SPEECH RECOGNITION ARCHITECTURES 
In modern speech recognition architectures, word recognition from a sequence 
of feature vectors is a two-step process. In the first step, a pattern classifier 
maps the sequence of feature vectors into a sequence of predictions of the spo- 
ken phoneme in progress. In the second step, a vocabulary of permitted words 
is introduced (a lexicon), expressed as probabilistic state machines (Hidden 
Markov Models, abbreviated as HMMs). The sequence of phoneme probabili- 
ties is then mapped into a sequence of words from the lexicon, using dynamic 
programming. 
While state models for a lexicons are typically crafted by hand, the features- 
to-phonemes pattern classifier is trained automatically, using a large database 
of example words. One popular classifier for this task uses a linear mixture of 
multivariate Gaussian functions to map the feature vector into the probability a 
particular phoneme is in progress. A complete mixture model has several types 
of parameters: each multi-variate Gaussian function has a mean vector and a 
diagonal covariance matrix, and weighting parameters control the contribution 
of each function in the mixture. 
The choice of a diagonal covariance matrix reduces the number of covariance 
parameters for an N-element feature vector from N 2 to N. This choice enables 
the liberal application of multiple Gaussians to model probability space, using 
an acceptably small number of parameters. Low-parameter models require less 
training data for effective parameter estimation, and often improve generaliza- 
tion properties. 
Choosing a diagonal covariance matrix is warranted if the off-diagonal matrix 
elements are small; this matrix property will be true if the elements of the 
feature vector are uncorrelated. However, as noted in the last section, the 
elements of the feature vector we generate from our multi-representation system 
are indeed correlated. 
An alternative approach for mapping a feature vector into a phoneme proba- 
bility vector is to use a multi-layer perceptron (MLP) architecture, trained with 
the backpropagation algorithm. This approach, as described in [3], is more tol- 

SILICON AUDITORY MODELS 
117 
erant of feature vectors whose elements are correlated. The speech recognition 
results we present in this paper all use this MLP-based recognizer. 
The input to the neural-network classifier is the feature vector of the current 
frame, as well as feature vectors from the 4 previous frames and the 4 upcoming 
frames for context. The net has a single hidden layer; unless otherwise indicated 
we use 200 hidden units in the system. There are 56 network outputs, associated 
with the 56 most-common phonemes; these outputs are the inputs to a dynamic 
programming module that performs word recognition. 
In our speech recognition experiments, we used a database of 200 adults 
speaking 13 English words in isolation (the digits, including both "oh" and 
"zero", plus "yes" and "no"), for a total of 2600 utterances. The database, 
supplied by Bellcore, was recorded over the U.S. public telephone network: the 
recordings typically have good signal-to-noise ratios, but display the limited 
bandwidth typical of the telephone network. Most of our experiments used 
this database directly; in some experiments, we added recorded noise from the 
interior of a running automobile to the speech, at a level resulting in a 10dB 
signal-to-noise ratio. 
The small size of the database results in a significant variance of recognition 
performance, depending on the particular words chosen to be in the training 
set and the test set. To counter this problem, we divide the database into 4 
segments, each consisting of 650 utterances by 50 speakers. We then train up 
4 different recognizers using a different selection of three segments for training 
data, while testing on the fourth segment. Note that a recognizer is never 
tested using utterances used for its training. In this paper, we report the error 
scores for each of these four recognizers, along with the averaged score of the 
four recognizers. 
In our recognition experiments, each word is modeled using a multi-state 
HMM; each state has a self-loop branch and a branch to the next state, with 
fixed transition probabilities of 0.5 for each branch. The model length varies 
with the number of phonemes in the word: "eight" is the shortest model, with 
13 states, while "seven" is the longest model, with 18 states. 
This isolated word database has been used for several previous speech recog- 
nition studies at ICSI, using the MLP-based recognizer in conjunction with 
two popular feature extraction systems, PLP and J-RASTA-PLP. These recog- 
nition studies, summarized in Figure 4.11, serve as a benchmark for com- 
parison with recognition experiments using feature vectors derived from our 
multi-representation auditory model. Perceptual Linear Prediction (PLP) is a 
popular feature extraction system based on human perceptual data from psy- 
chophysics, that works well for speech recorded with high signal-to-noise ratios 
through benign transmission channels [i0]. The J-RASTA-PLP 
system [Ii, 28] 
is an enhanced version of PLP, designed to provide feature vectors relatively 
independent of noise mixed with the speech signal, as well as providing feature 
vectors independent of slowly-varying changes in the spectral properties of the 
speech transmission channel. 

118 
NEUROMORPHIC SYSTEMS ENGINEERING 
Front End 
Conditions 
J-RASTA-PLP 
Clean 
J-RASTA-PLP 
Noisy 
PLP 
Noisy 
1 
2 
3 
4 
Average 
2.3 
1.5 
1.4 
2.0 
1.9 
11.4 
10.2 
10.3 
11.5 
10.9 
42.8 
37.1 
40.8 
49.1 
42.4 
Figure 4.11 
Percent error for PLP-based front-ends (four database partitions). 
The first table entry in Figure 4.11 shows the performance of J-RASTA- 
PLP on the isolated word database, for the 4 data segmentations described 
above: both training and test data are the original "clean" database, without 
added car noise. The 1.8 percent average error score is comparable with current 
commercial systems used in voice-mail applications, using real-world telephony 
data: trade publications for interactive voice response telephony advise users 
to expect 3-5% scores for isolated digit recognizers in the field. Systems with 
error rates under 5% can work well in an application, if good error recovery 
strategies are available for the task. 
The next two table entries in Figure 4.11 describe performance for recogniz- 
ers that were trained "clean" utterances, but whose test utterances were mixed 
with automobile noise (10 dB signal-to-noise ratio), as described earlier. Note 
that in addition to being corrupted with noise, the test utterances were also 
novel: a recognizer is never tested using noisy utterances whose clean versions 
were a part of the training set. These table entries show the benefits of en- 
hancing a feature extraction system to be robust to additive noise: the average 
error for PLP is 4 times greater than for J-RASTA-PLP. However, the absolute 
error of J-RASTA-PLP tested with noisy speech (10.9%) is marginal for use in 
an application. 
4.6 
SPEECH RECOGNITION EXPERIMENTS 
We used the isolated-word database and MLP-based recognition system de- 
scribed in the previous section to evaluate the performance of the 30-element 
feature vector derived from our multi-representation system. Figure 4.12 sum- 
marizes the error performance of the system; all the scores in this table reflect 
"clean" testing and training data. The final line of Figure 4.12 shows recogni- 
tion performance using the full 30-element feature vector. This 4.1% error rate 
is sufficiently low for many applications, although it is significantly larger than 
the J-RASTA-PLP's benchmark error rate (1.8%). 
To illustrate the relative contributions of the three representations in our 
system, we also trained recognizers using subsets of our 30-element feature 
vector, that contained only the elements from one or two of the representations 
in the system: the penultimate lines of Figure 4.12 show these scores. The 
number of hidden units for each recognizer was varied inversely with the number 

SILICON AUDITORY MODELS 
119 
Features 
Parameters 
Hidden Unit,, 
1 
2 
3 
4 
Average 
SS 
65,586 
326 
6.6 
6.9 
5.4 
8.0 
6.7 
SS+Auto 
65,468 
276 
5.7 
5.8 
4.5 
5.5 
5.4 
SS+Onset 
65,531 
225 
4.9 
5.1 
4.3 
4.9 
4.8 
iS+Auto+Onset 
65,456 
200 
4.9 
4.2 
3.2 
4.0 
4.1 
Figure 4.12 
Percent error for feature vectors derived from auditory representations (four 
database partitions). Other fields show number of hidden units and number of parameters 
in the MLP classifier net. Code: SS = spectral shape features, Onset = onset features, 
Auto = autocorrelogram features. 
of elements in the reduced feature vector, to yield an MLP with approximately 
65,000 parameters for each experiment. 
A comparison of the different recognizers in Figure 4.12 shows the effective- 
ness of combining multiple representations of speech. Adding features from two 
additional representations (the onset features and the autocorrelation features) 
to the primary spectral-shape features decreases the average error by 61%. 
Figure 4.13 shows an error analysis of the recognizers of Figure 4.12; for 
each recognizer, the percentage error attributed to the two most likely word 
confusions ("five" and "nine", and "no" and "oh") are shown, along with the 
residual error contributed by all other confusions. The addition of onset features 
and autocorrelogram features improves the recognition performance for all three 
categories of confusions. 
Features 
Total 
9/5 
oh/no 
others 
SS 
SS + Auto 
SS + Onset 
SS + Auto + Onset 
6.7 
5.4 
4.8 
4.1 
1.4 
1.1 
1.0 
0.7 
1.0 
0.8 
0.7 
0.6 
4.4 
3.5 
3.1 
2.8 
Figure 4.13 
Error analysis of the recognition experiments in Figure 4.12 (averaged over 
partitions). Errors due to the two leading word confusions are listed (confusing "five" and 
"nine", and confusing "oh" and "no"), as well as the residual error. 
In Figure 4.13, note that for five/nine confusions, the error improvements 
for adding onset features (1.4% to 1.1%, a 0.3% improvement) and for adding 
autocorrelation features (1.4% to 1.0%, a 0.4% improvement) add to equal the 
error improvement for adding both onset and autocorrelation features to spec- 
tral shape features (1.4% to 0.7%, a 0.7% improvement). This linear addition 
suggests the statistical independence of the information added by the onset and 
autocorrelation features for disambiguating "five" and "nine". Conversely, the 
table shows the statistical dependence of the information added by the onset 

120 
NEUROMORPHIC SYSTEMS ENGINEERING 
and autocorrelation features for disambiguating words in the "others" category. 
If these features were statistically independent, an error rate of 2.2% (not 2.80/0) 
would be expected for the "others" category for the full feature vector. 
Figure 4.14 shows the error performance of the recognizers trained for Fig- 
ure 4.12, when tested on the "noisy" utterances described in the last section. 
This table shows uniformly poor recognition results, comparable with the noisy 
recognition performance of PLP shown in Figure 4.10, and 5.5 times worse than 
the noisy recognition performance of J-RASTA-PLP. Although early studies of 
speech recognition in noisy conditions using auditory models reported encour- 
aging results [9, 31], later studies found no significant noise robustness qualities 
for auditory models [24], and the data in Figure 4.14 confirms this finding. 
Features 
Parameters 
Hidden Units 
SS 
65,586 
326 
SS + Auto 
65,468 
276 
SS + Onset 
65,531 
225 
~S + Auto + Onset 
65,456 
200 
1 
2 
3 
4 
Average 
50 
54 
50 
55 
52 
53 
55 
51 
55 
54 
55 
57 
61 
62 
59 
57 
57 
59 
62 
59 
Figure 4.14 
Recognition results for noisy test data; automobile noise source, mixed with 
speech at signal-to-noise ratio of lOdB (NIST measurement method). 
Figure 4.15 shows the effect of reducing the number of parameters in the 
MLP pattern classifier on error rate, for the full 30-element feature vector. The 
table compares recognizers with approximately 32,000 parameters (100 hidden 
units) and 16,000 parameters (50 hidden units) with the full 65,000 parameter 
recognition system. 
The effect of parameterization on error performance is 
particularly important for low-cost, low-power recognizer implementations. 
Features 
Parameters 
Hidden Units 
~S + Auto + Onset 
65,456 
200 
gS + Auto + Onset 
32,756 
100 
;S + Auto + Onset 
16,406 
50 
1 
2 
3 
4 
Average 
4.9 
4.2 
3.2 
4.0 
4.1 
5.1 
4.6 
3.8 
4.3 
4.5 
5.7 
6.8 
4.6 
6.6 
5.9 
Figure 4.15 
Recognition results showing the effect of the number of parameters in the 
MLP-classifier on recognition results. Results are for clean testing data, using the full feature 
vector (SS+Onset+Auto). 
The 200 speaker, 13-word isolated-word database consists of approximately 
5 hours of speech. The analog processing circuits in the multi-representation 
system are not compensated for temperature drift; we omitted temperature 
compensation circuitry from our prototype system for simplicity. 
Ambient 
temperature variation in our laboratory over five hours results in a significant 
drift in auditory model responses. 

SILICON AUDITORY MODELS 
121 
To counter temperature drift problems during datataking for the experi- 
ments reported above, several steps were taken. Data was presented to the 
chip ordered by word: 200 speakers saying "1," followed by 200 speakers saying 
"2," ect. All chip parameters were recalibrated between each set of 200 ut- 
terances; this recalibration resets parameters with 5% accuracy. The complete 
dataset was taken several times, on different days of the week and different 
times of the day, and pilot recognition experiments guided the choice of the 
final dataset. Within the limits of our present hardware prototype, these er- 
ror scores approximate the performance of a temperature-compensated multi- 
representation system. 
For comparison purposes, Figure 4.16 shows recognition performance for the 
multi-representation system, if less care is taken to reduce temperature effects. 
In these experiments, data was presented to the system ordered by speaker, 
not by word: speaker 1 saying all 13 words, followed by speaker 2 saying all 
13 words, ect. Recalibration of parameters occured every 10 speakers. The 
scores in this table reflect "clean" testing and training data; the final line of 
Figure 4.12 is reproduced in Figure 4.16 to provide a direct comparison between 
the two datasets. 
Order 
Parameters 
Hidden Units 
By Word 
65,456 
200 
By Speaker 
65,456 
200 
1 
2 
3 
4 
Average 
4.9 
4.2 
3.2 
4.0 
4.1 
6.6 
4.9 
6.0 
7.7 
6.3 
Figure 4.16 
Recognition resuits showing the effect of data presentation on recognition 
results. Results are for clean testing data, using the full feature vector. 
4.7 
DISCUSSION 
The speech recognition performance of our multi-representation system, as 
shown in Figure 4.12 and Figure 4.14, is inferior to JoRASTA-PLP, both for 
clean and noisy test data. However, under high signal-to-noise conditions, the 
system provides adequate performance (4.1% error) for many isolated-word ap- 
plications. For specialized applications where a micropower speech feature ex- 
tractor is required, the signal processing technology used in our special-purpose 
analog-to-digital converter chip is a competitive option. For these applications, 
the remaining challenges include the micropower implementation of the rest of 
the recognition system [23], and the identification of end-user applications with 
sufficient market size to support the development effort. 
Apart from the micropower niche, however, analog auditory models are cur- 
rently uncompetitive with conventional front-end approaches for speech recog- 
nition applications. The success of auditory processing in biological systems, 
however, leaves us hopeful that a sustained research effort in using analog au- 
ditory models for speech recognition could result in recognition systems that 

122 
NEUROMORPHIC SYSTEMS ENGINEERING 
perform significantly better than conventional front-end approaches. We see 
the following areas as important elements of such a research effort: 
4.7.1 Improved Circuit Techniques 
The 4.1% error of the multi-representation system, for clean speech, is distinctly 
inferior to the 1.8% error for 3-RASTA-PLP on the same task. In contrast, 
studies of software implementations of similar auditory models [13] typically 
show comparable performance in comparison with conventional front-ends. The 
shortcomings of our analog circuit implementation, including limited signal-to- 
noise ratio, limited dynamic range, and inaccuracy due to parameter variation 
and temperature-related drift, may play a role in this difference. 
The circuit technologies that implement the signal processing datapath 
shown in Figure 4.2 date from the first silicon audition designs [27]. Several 
generations of improved circuits and algorithms for silicon audition have been 
published since these early designs, and research continues in several groups 
worldwide. Many of these improvements focus on signal-to-noise, dynamic 
range, and improving uniformity across cochlea channels. These improvements 
may directly translate to improvements in speech recognition scores, bringing 
silicon auditory models to the performance of their software counterparts. 
Parameter 
drift due to inadequate temperature 
compensation 
is another 
area for improvement, 
the temperature compensation 
approach we use in our 
multi-representation is primitive [15], and parameter drift may be a significant 
source of recognition error, as Figure 4.16 suggests. Improvements 
in this area 
are straightforward, using techniques such as those described in [33]. 
4.7.2 Enhanced Auditory Models 
The cochlear model in our special-purpose analog-to-digital converter chip is 
an extreme simplification of physiological cochlear processing; software-based 
auditory models used in other speech recognition studies share most of these 
simplifications. Key physiological cochlear response characteristics, including 
synchrony suppression, rate suppression, and temporal masking, are absent 
from these models; many auditory theorists believe these characteristics under- 
lie the robust coding of speech in the presence of noise in biological auditory 
systems. Physiological cochleas are deeply non-linear, and exhibit characteris- 
tics consistent with extensive channel-specific automatic gain control: the au- 
ditory models used in speech recognition experiments to date do not correctly 
model these characteristics. We believe that more accurate cochlear models 
are an important part of future research in using auditory representations for 
speech recognition. 
In addition to improving the cochlear models, cleaner implementations of 
the computations underlying the secondary representations in our system (cor- 
relation and temporal adaptation) would add considerable robustness to these 
representations. Also of interest is the addition of other secondary represen- 
tations, in particular models of neural maps that code for temporal offsets, 

SILICON AUDITORY MODELS 
123 
amplitude modulation, frequency modulation, and quick temporal sequences 
typical of the voiced-onset transition in speech. If multi-microphone recordings 
of speech databases are available, binaural representations are another possible 
enhancement to the system. 
4.7.3 Adapting Robust Techniques to Auditory Models 
A variety of techniques for robust feature extraction in noisy environments 
have been developed for use with conventional front-ends for speech recogni- 
tion. Adapting these techniques to function with auditory representations is a 
promising avenue of research. 
One popular method of speech enhancement in noise is spectral subtrac- 
tion [2]. In this approach, a spectral model of the background noise in the 
recent past is generated, and subtracted from the current input. 
Another 
method of speech enhancement in noise, the 3-RASTA-PLP system [11, 28], 
uses information about the temporal properties of speech to filter speech signal 
from background noise. Both approaches could be used in conjunction with 
auditory representations. 
4.7.4 
Closing the Representation-Recognizer Cap 
As Figure 4.8 summarizes, auditory representations are a poor match to cur- 
rent speech recognition systems. This paper makes no significant contribution 
towards closing this "representation-recognizer gap". Our straightforward ap- 
proach of collapsing the spike-based, high-dimensionality auditory representa- 
tions (Hamming windows and gross sub-sampling) destroys most of the unique 
coding aspects of the auditory representation. Apart from choosing an MLP- 
based pattern classifier, no advances in recognition algorithms were made to 
help close the gap from the recognition side. 
We believe that making significant contributions to closing this gap, both 
by modifying core speech recognizer technology, and by developing enhanced 
methods of distilling information from high-dimensional, adaptively-sampled 
representations, is essential to significantly improve speech recognition perfor- 
mance of auditory models. 
Several research groups have done initial work on changing core speech recog- 
nition technology to be more amenable to auditory representations. These 
methods take different approaches to the problem; one recent publication uses 
the visual scene analysis concept of occlusion as a starting point [8], while 
other recent work is motivated by the importance transient information in the 
speech signal [29]. Attacking the problem from the representation side, research 
in mapping in high-dimensional spaces into low-dimensional features has been 
recently applied to cochlear models [12]. 

124 
NEUROMORPHIC SYSTEMS ENGINEERING 
4.8 
SUMMARY 
In this paper, we have evaluated the suitability of analog implementations 
of auditory models, using an empirical approach: 
we integrated a multi- 
representation analog auditory model with a speech recognition system, and 
measured the performance of the system on a speaker-independent, telephone- 
quality 13-word recognition task. The performance of the system is adequate 
for many applications, but inferior to conventional approaches for front-end 
processing. In addition, the auditory models show no advantages for robust 
speech recognition applications. 
Acknowledgments 
We thank Nelson Morgan for providing the speech databases, the speech-recognition 
software, and the computational resources for this study; we also acknowledge his 
extensive advice on MLP design and training. We thank Su-lin Wu for her extensive 
help in the recognition studies; we also thank the entire ICSI speech recognition group 
for suggestions and software maintenance. Thanks to Richard Duda, Steve Greenberg, 
Hynek Hermansky, Richard Lippmann, Richard Lyon, Carver Mead, Malcolm Slaney, 
and the members of the Physics of Computation laboratory at the California Institute 
of Technology for advice and suggestions on auditory modeling. We used the HTK 
Toolkit Version 1.4 for preliminary speech recognition studies preceding this work. 
Research funded by the Office of Naval Research (URI-N00014-92-J-1672). 
References 
[1] N. A. Bhadkamkar. Binaural source localizer chip using subthreshold ana- 
log CMOS. In IEEE International Conference on Neural Networks, vol- 
ume 3, pages 1866-1870, 1994. 
[2] S. Boll. Suppression of acoustic noise in speech using spectral subtraction. 
IEEE Trans. on ASSP, ASSP-27(2):113-120, 1979. 
[3] H. Bourlard and N. Morgan. Connectionist speech recognition: a hybrid 
approach. Kluwer Academic Publishers, Boston, Mass, 1994. 
[4] G. J. Brown and M. P. Cooke. Computational auditory scene analysis. 
Computer Speech and Language, 8(4):297-336, 1994. 
[5] A. P. Chandrakasan and R. W. Brodersen. Low power digital CMOS de- 
sign. Kluwer Academic Publishers, Boston, Mass, 1995. 
[6] R. Coggins, M. Jabri, B. Flower, and S. Pickard. A hybrid analog and dig- 
ital VLSI neural network for intracardiac morphology classification. IEEE 
Journal Solid State Circuits, 30(5):542-550, 1995. 
[7] C. Colomes, M. Lever, J. B. Rault, and Y. F. Dehery. A perceptual model 
applied to audio bit-rate reduction. J. Audio Eng. Soc., 43(4):233-239, 
1995. 

SILICON AUDITORY MODELS 
125 
[8] M. Cooke, P. Green, and M. Crawford. Handling missing data in speech 
recognition. In 1994 International Conference on Spoken Language Pro- 
cessing, volume 3, pages 1555-1558, 1994. 
[9] O. Ghitza. Temporal non-place information in the auditory nerve firing 
patterns as a front-end for speech recognition in a noisy environment. 
Journal of Phonetics, 16(1):109-123, 1988. 
[10] H. Hermansky. Perceptual linear predictive (plp) analysis of speech. Jour- 
nal Acoustical Society of America, 87(4):1738-1752, 1990. 
[11] H. Hermansky and N. Morgan. Rasta processing of speech. IEEE Trans- 
actions of Speech and Audio Processing, 2(4):578-589, 1994. 
[12] N. Intrator. Combining exploratory projection pursuit and projection pur- 
suit regression with application to neural networks. Neural Computation, 
5:443-455, 1993. 
[13] C.R. Jackowoski, H. D. H. Vo, and R. P. Lippmann. A comparison of signal 
processing front ends for automatic word recognition. IEEE Transactions 
of Speech and Audio Processing, 3(4):286-293, 1995. 
[14] J. Lazzaro, J. Wawrzynek, , and A. Kramer. Systems technologies for 
silicon auditory models. IEEE Micro, 14(3):7-15, June 1994. 
[15] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie. 
Silicon auditory processors as computer peripherals. 
IEEE Journal of 
Neural Networks, 4(3):523-528, 1993. 
[16] J. P. Lazzaro. A silicon model of an auditory neural representation of 
spectral shape. IEEE Journal Solid State Circuits, 26:772-777, 1991. 
[17] J. P. Lazzaro and C. Mead. Circuit models of sensory transduction in 
the cochlea. In Mead and Ismail, editors, Analog VLSI Implementation 
of Neural Systems, pages 85-101. Kluwer Academic Publishers, Norwell, 
MA, 1989. 
[18] J. P. Lazzaro and C. Mead. Silicon models of auditory localization. Neural 
Computation, 1:47-57, 1989. 
[19] J. P. Lazzaro and C. Mead. Silicon models of pitch perception. In Proc. 
Natl. Acad. Sci. USA, volume 86, pages 9597-9601, 1989. 
[20] J. P. Lazzaro and J. Wawrzynek. A multi-sender asynchronous extension 
to the address-event protocol. In W. J. Dally, J. W. Poulton, and A. T. 
Ishii, editors, 16th Conference on Advanced Research in VLSI, pages 158- 
169, 1995. 
[21] John Lazzaro. Temporal adaptation in a silicon auditory nerve. In John E. 
Moody, Steve J. Hanson, and Richard P. Lippmann, editors, Advances in 
Neural Information Processing Systems, volume 4, pages 813-820. Morgan 
Kaufmann Publishers, Inc., 1992. 
[22] John Lazzaro and John Wawrzynek. Silicon models for auditory scene anal- 
ysis. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, 

126 
NEUROMORPHIC SYSTEMS ENGINEERING 
editors, Advances in Neural Information Processing Systems, volume 8, 
pages 699 705. The MIT Press, 1996. 
[23] John Lazzaro and John Wawrzynek. Silicon models for auditory scene 
analysis. In M. Jordan, M. Mozer, and T. Petsche, editors, Advances in 
Neural Information Processing Systems, volume 9. The MIT Press, 1997. 
[24] John Lazzaro, John Wawrzynek, and R. Lippmann. A micropower analog 
VLSI HMM state decoder for wordspotting. In M. Jordan, M. Mozer, and 
T. Petsche, editors, Advances in Neural Information Processing Systems, 
volume 9. The MIT Press, 1997. 
[25] W. Liu, A. Andreou, and M. Goldstein. Voiced-speech representation by 
an analog silicon model of the auditory periphery. IEEE Transactions of 
Neural Networks, 3(3):477-487, 1992. 
[26] R. F. Lyon. CCD correlators for auditory models. In IEEE Asilomar 
Conference on Signals, Systems, and Computers, pages 785-789, 1991. 
[27] R. F. Lyon and C. Mead. An analog electronic cochlea. IEEE Trans. 
Acoust., Speech, Signal Processing, 36:1119-1134, July 1988. 
[28] K. W. Ma. Applying large vocabulary hybrid HMM-MLP methods to 
telephone recognition of digits and natural numbers. Technical Report 
TR-95-024, International Computer Science Institute, 1995. 
[29] N. Morgan, H. Bourlard, S. Greenberg, and H. Hermansky. Stochastic 
perceptual auditory event-based models for speech recognition. In 1994 
International Conference on Spoken Language Processing, volume 4, pages 
1943-1946, Yokohama, Japan, 1994. 
[30] M. B. Sachs and E. D. Young. Effects of nonlinearities on speech encoding 
in the auditory nerve. J. Acoust. Soc. Am, 68(3):858-875, 1980. 
[31] S. Seneff. A joint synchrony/mean-rate model of auditory speech process- 
ing. Journal of Phonetics, 16(1):55-76, 1988. 
[32] A. van Schaik, E. Fragni~re, and E. A. Vittoz. Improved silicon cochlea 
using compatible lateral bipolar transistors. 
In David S. Touretzky, 
Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neu- 
ral Information Processing Systems, volume 8, pages 671 677. The MIT 
Press, 1996. 
[33] E. A. Vittoz. The design of high-performance analog circuits on CMOS 
chips. EEE Journal Solid State Circuits, 20(3):657-665, 1985. 
[34] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692-700, 
May 1992. 

I I Retinomorphic Systems 

THE RETINOMORPHIC APPROACH" 
PIXEL-PARALLEL 
ADAPTIVE AMPLIFICATION, FILTERING, 
AND QUANTIZATION 
Kwabena A. Boahen 
Physics of Computation Laboratory, 
MS 136-93, California Institute of Technolos;y, 
Pasadena CA 91125 
buster@pcm p.caltech.edu 
5.1 
SMART-PIXEL ARRAYS 
The migration of sophisticated signal processing down to the pixel level is 
driven by shrinking feature sizes in CMOS technology, allowing higher levels of 
integration to be achieved [24, 18]. New pixel-parallel architectures are required 
to take advantage of the increasing numbers of transistors available [1]. Inspired 
by the pioneering work of Mahowald and Mead [6], I describe in this paper a 
retinomorphic vision system that addresses this need by mimicking biological 
sensory systems. 
In particular, my approach uses the system architecture and neurocircuitry 
of the nervous system as a blueprint for building integrated low-level vision 
systems--systems that are retinomorphic in a literal sense [1, 9]. Morphing of 
biological wetware into silicon-based hardware results in sensory systems that 
maximize information uptake from the environment, while minimizing redun- 
dancy in their output; that achieve high levels of integration, by performing 
several functions within the same structure; and that offer robust system-level 
performance, by distributing computation across several pixels. 
I describe a retinomorphic system that consists of two chips: a focal-plane 
image processor that adaptively amplifies, filters, and quantizes the visual signal 
at the pixel-level, and a postprocessor that has a two-dimensional array of 

130 
NEUROMORPHIC SYSTEMS ENGINEERING 
integrators. The system concept is shown in Figure 5.1. Both chips are fully 
functional; specifications and die micrographs are shown in Table 5.1 and in 
Figure 5.2, respectively. 
I outline the general design principles of the retina, and contrast them with 
standard engineering practice in Section 5.2. Having defined the retinomorphic 
approach to imager design, I describe a design for a retinomorphic pixel in 
Section 5.3, and present test results from the complete two-chip neuromorphic 
system in Section 5.4. The communication channel used to transmit the pulse 
trains from chip to chip is described in detail a companion paper [4]; a brief 
description is already available [186]. My concluding remarks are presented in 
Section 5.5. 
i~ 
"3 .... 
~. 
I 
~ 
~ 
: i~ .~ 
:,~ m 
II1 
~ 
:~,~ 
: ~ I~ 
<[ 
RETINOMORPHIC CHIP 
CORTICAL 
PROCESSING 
INTERCHIP COMMUNICATION NEUROMORPHIC CHIP 
Figure 5.1 
System concept. The retinomorphic chip acquires, amplifies, filters, and quan- 
tizes the image. All these operations are performed at the pixel level. The interchip commu- 
nication channel reads out asynchronous digital pulses from the pixels by transmitting the 
location of pulses as they occur. A second neuromorphic chip decodes these address events, 
and recreates the pulses. 
Table 5.1 
Specifications of the two-chip retinomorphic system. L is the minimum feature 
size, which was 2#m for this process; S/s is samples per second. 
Specification 
Imager 
Postprocessor 
Technology 
Number of pixels 
Pixel size (L 2) 
Transistors/pixel 
Die size (mm 2) 
Supply 
Dissipation (0.2 MS/s) 
Throughput 
2-#m 2-poly 2-metal p-well 
64 x 64 
53 x 49 
31.5 x 23 
32 
8 
8.1 x 7.4 
5.1 x 4.0 
5V 
230 mW (total) 
2 MS/s 

RETINOMORPHIC 
APPROACH 
131 
Table 5.2 
Retinal design principles versus electronic-imager design principles. 
Operation 
Standard 
Retinal 
Detection 
Integrating 
Continuous 
Gain Control 
Global 
Local 
Filtering 
Allpass 
Bandpass 
Quantization 
Fixed 
Adaptive 
Architecture 
Serial 
Parallel 
X ARBffER & ENCODER 
(a) 
(b) 
Figure 5.2 
(a) Retinomorphic focal-plane processor. The core of this chip is a 64 x 64 
array of pixels arranged on a hexagonal grid. Pixels generate pulses and communicate the 
occurence of these pulses by signalling on the column and row request lines. The arbiters 
ensure that pulses are read out of the array one by one, in an orderly manner, by selecting one 
pixel at a time with the column and row select lines. The encoders generate the addresses 
of the selected row and column; this pair of binary words uniquely identifies the location of 
the pulse. (b) Postprocessor. The core of this chip is a 64 Ã 64 array of diode-capacitor 
integrators. We can feed short current pulses to any integrator in the array by supplying its 
row and column addresses to the decoders. We use the scanners (shift registers) to read 
out analog currents from the array for display on a video monitor. 
5.2 
PIXEL-PARALLEL PROCESSING 
The functional and structural organization of the retina is radically different 
from that of standard human-engineered 
imagers. The principles of operation 
of the retina are outlined in Table 5.21 the principles of operation of standard 
imager technology also are listed for comparison. 

132 
NEUROMORPHIC SYSTEMS ENGINEERING 
5.2.1 Sensing: Continuous Versus Integrating 
Integrating detectors (e.g., charge-coupled devices (CCDs) [23] and photo- 
gates [16]) suffer from blooming at high intensity levels and require a destructive 
readout (reset) operation. Continuous-sensing detectors (e.g., photodiodes or 
phototransistors) do not bloom, and can therefore operate over a much larger 
dynamic range [30]. In addition, redundant readout or reset operations can be 
eliminated, with considerable power savings, because charge does not accumu- 
late. 
Continuous-sensing detectors have been shunned, however, because they suf- 
fer from gain and offset mismatches that give rise to salt-and-pepper noise in 
the image. However, Buhman et.al, have shown that the powerful learning 
capabilities of image-recognition systems can easily compensate for this fixed 
pattern noise [ll]. 
The real benefit of using continuous sensors lies in their ability to perform 
analog preprocessing before quantizing the signal. A signal that takes on a dis- 
crete set of values at a discrete set of times (quantized in amplitude and time) 
carries less information than does a signal that takes on the full continuous 
spectrum of amplitudes and times. For instance, graded potentials in the ner- 
vous system can transmit information at the rate of 1650 bits per second--over 
four times the highest rate measured for spike trains [14]. 
The analog operations described in Sections 5.2.2 and 5.2.3 reshape the 
spectral distribution and the amplitude distribution of the analog signal, to 
transmit information efficiently through this bottleneck. 
5.2.2 Amplification: Local Versus Global Control 
Imagers that use global automatic gain control (AGC) can operate under only 
uniform lighting, because the 1000-fold variation of intensity in a scene with 
shadows exceeds their 8-bit dynamic range. 1 A charge-coupled device or pho- 
togate can achieve 12 bits (almost four decades) [16], and a photodiode or 
phototransistor can achieve 20 bits (six decades) [15, 30j--but the phototran- 
sistor's performance in the lowest two decades is plagued by slow temporal 
response. The dynamic range of the system's output, however, is limited by 
the cost of precision analog read-out electronics and A/D converters, and by 
video standards. 
When AGC acts globally, the input dynamic range matches the output dy- 
namic range, and the only way to extend the input range is to extend the 
output dynamic range. In practice, we must reduce the noise floor to improve 
resolution. 
As shown in Figure 5.3, local AGC decouples dynamic range and resolution, 
extending the input dynamic range by mapping different parts of the input 
range to the limited output range, depending on the local intensity level. This 
solution is beneficial if the resolution required to discriminate various shades 
of gray (1 in 100 for the human visual system) is poorer than the resolution 

RETINOMORPHIC APPROACH 
133 
= 3 
m 
.~ 
~ 
,,~ 
-5 
-4 
-3 
-2 
-) 
Intensity (log cd/m^2) 
Â°f 
3= 
.c 
rou= 
. 
log cd/m = 
-5 
-4/ 
-3 / 
-2 ~ 
-1 
0 = 
1100  
/ 
-5 
-4 
-3 
-2 
-1 
0 
Test flash (log cd/m z) 
(a) 
(b) 
Figure 5.3 
Input-output transfer curves for light sensors. (a). As larger and larger input 
ranges are spanned, the slope decreases, and finer resolution is required to detect the same 
percentage change in the input signal. (b). Using transfer curves that can be centered at the 
local intensity level decouples dynamic range and resolution. Each curve spans only a 20-fold 
input range, since local variations in intensity are due primarily to changes in reflectivity: A 
black sheet has a reflectivity of 0.05, and a white sheet has a reflectivity of 0.95. These 
transfer curves were measured for the cat retina, and were reproduced from [35]. 
required to span the range of all possible input levels (at least 1 in 100,000 for 
the photopic range of human vision). 
5.2.3 Preprocessing: Bandpass Versus Allpass 
On average, natural images have a 1/f 2 power spectrum for both spatial and 
temporal frequency [17, 21], whereas noise, due to quantum fluctuations, has a 
flat spectrum. Consequently, imagers that transmit the full range of frequencies 
present pass on useless information at high frequencies, where the signal-to- 
noise is poor, and pass on redundant information at low frequencies, where the 
signM-to-noise is good. Bandpass spatiotemporal filtering rejects the wideband 
noise, and attenuates the redundant low-frequency signals; this strategy is the 
optimal one for removing redundancy in the presence of white noise [2, 18, 39]. 
I illustrate in Figure 5.4b,d the redundancy reduction achieved, for a typical 
outdoor scene, by computing the correlation between pixel values. 2 The corre- 
lation is over 40% for pixels 60 pixels apart in the raw image. In the filtered 
image, pixels more than 10 pixels apart have less than 5% correlation. Com- 
parison of the amplitude histograms before and after filtering (Figure 5.4c,f) 
demonstrates that bandpass filtering has two additional benefits. 
First, bandpass filtering results in a sparse output representation. For our 
sample image, 24.4% of the pixels fall within Â±0.39% of the full-scale range 
(i.e., Â±ILSB at 8-bit resolution); 77.5% of them fall within Â±5% (i.e., Â±13 
at -127 to +127 amplitude range). Hence, if we choose to ignore amplitudes 
smaller than 5%, we need to transmit only 22.5% of the pixels. In practice, 
the degree of sparseness will depend on the cut-off frequency of the bandpass 

134 
NEUROMORPHIC SYSTEMS ENGINEERING 
(a) 
(b) 
(c) 
(d) 
(e) 
I 
(f) 
Figure 5.4 
Bandpass filtering. The top row shows the original 512 Ã 479 Ã 8-bit image 
(a), its autocorrelation (b), and its amplitude histogram (c). The bottom row shows the 
bandpass-filtered image (d), its autocorrelation (e), and its amplitude histogram (f). 
In 
the original image, pixels are highly correlated, and the correlation falls off slowly with 
distance. The distribution of amplitudes in the original image is broad and bimodal, due to 
the relatively bright overcast sky and the dark foreground objects. In contrast, the amplitude 
distribution for the filtered image is clustered around zero (119), and decays rapidly. 
filter. Although rejection of high frequencies introduces some redundancy, this 
rejection is necessary to protect the signal from noise that is introduced by the 
signal source or by the circuit elements. 
Second, bandpass filtering results in a unimodal amplitude distribution that 
falls off exponentially. For our sample image, the distribution is fit by a sum of 
two exponentials that change by a factor of e -- 2.72 whenever the amplitude 
changes by 2.5 and 14.0, on a =E128 scale; the rapidly decaying exponential 
starts out 4.5 times larger. Empirical observations confirm that this simple 
model holds for a wide range of images. 
In contrast, the distribution of raw intensity values is difficult to predict, 
because gross variations occur from scene to scene, due to variations in illumi- 
nation, image-formation geometry (surface and light-source orientation), and 
shadows [34]. These slowly changing components of the image are removed by 
local AGC and bandpass filtering. When the bandpass characteristics are fixed 
and the intensity is normalized, the parameters of the amplitude distribution 

RETINOMORPHIC APPROACH 
135 
vary much less, and the quantizer can exploit this invariance to distribute its 
codes more effectively. 
5.2.4 
Quantization: Adaptive Versus Fixed 
The quantization intervals of traditional A/D converters are set to match the 
maximum rate of change and the smallest amplitude, as shown in Figure 5.5. 
This uniform quantization is optimum only when high frequencies dominate 
and all amplitudes are equally likely. As we have seen, neither case applies 
to natural scenes: the power spectrum decays with frequency, as in 1/f 2, and 
the amplitude probability density decays exponentially. Therefore, uniform 
quantizers produce numerous redundant samples, because changes in the signal 
are relatively rare [17], and underutilize their large amplitude codes, because 
these signal amplitudes occur rarely in natural scenes [34]. 
Assuming that temporal changes are due primarily to motion, we can es- 
timate the amount of redundancy from the spatial-frequency power spectrum 
and from the velocity distribution. The velocity distribution, measured for 
/ 
" 
>: 
AT 
> 
~ 
~ 
t 
~ ~ 
~ ~ ~ 1 
t 
t 
~ 
AV 
AT = ~ 
rnax(dV/dt) 
AV 
AT = ~ 
dV/dt 
, 
/ 
~- 
~v 
~--~ 
v ---~ 
~ ~ "i ............................... I"~ ~ 
-~-~-. 
~,V = 
Range(V) 
2 b 
../ 
v 
i ................................... !l~ii~ ............................................ 
i 
AP 
1 
AV = d Pld~ 
A P = -~ 
2 ~ 
Figure 5.5 
Quantization in time and amplitude. Top row: Time intervals (A~P) are set 
to match the maximum rate of change (left column). The signal is sampled repeatedly, 
even when dV/dt ~ 0--that is, when the chanl~e is insignificant (oversampling). Instead 
of fixing the time step, it is more efficient to fix the voltage step (L~VI, and to adapt the 
time intervals dynamically to achieve this change in voltage, as shown on the right. Bottom 
row: Amplitude intervals (AV) are uniformly distributed. The signal is sampled repeatedly, 
even though dP/dV ~ 0--that is, although the probability that the input amplitude falls 
in this interval is negligible. Instead of fixing the voltage step, it is more efficient to target a 
certain change in the cumulative probability (Ap __-- 2 -b, where b is the number of bits per 
sample), and to choose voltage intervals statistically to achieve this change in probability, 
as shown on the right. 

136 
NEUROMORPHIC SYSTEMS ENGINEERING 
movies and amateur videos, is dominated by low velocities and falls off with 
a power law of 3.7 [17]. High velocities will be even more drastically attenu- 
ated in an active vision system that compensates for global motion, and that 
tracks objects [19]. After bandpass filtering, signals that change gradually over 
space are eliminated and rapid changes occur only rarely and over much more 
restricted areas. 
Due to the absence of high speeds and of nonlocal intensity varaitions, the 
imager's output signals rarely change rapidly. Consequently, adapting the sam- 
pling rate to the rate of change of the signal greatly reduces the number of 
samples produced. Alternatively, this adaptation allows higher temporal band- 
widths to be achieved for a given mean sampling rate. 
Using the amplitude distribution of our bandpass-filter sample image, we can 
calculate the probability of failing to discriminate between a pair of samples 
drawn from that distribution: It is 0.0384 when the 2 s quantization levels are 
uniformly distributed 
an order of magnitude bigger than the the minimum 
confusion rate of 1/256 = 0.0039, which occurs when the quantization levels are 
chosen to make it equally likely that we will draw a sample from each interval. 
In fact, the confusion rate of 0.0384 can be achieved with just log2(1/0.0384 ) = 
4.7bits per sample if the quantization levels are optimally distributed. 
A quantizer that assigns its codes to probable amplitudes, rather than to 
improbable ones, maximizes the probability of discriminating between any two 
amplitude levels drawn from the input distribution; thus, information is maxi- 
mized when all codes are equiprobable [36]. 
5.2.5 
Architecture: Parallel Versus Serial 
In addition to differing in the aforementioned design principles, biological and 
human-made vision systems use radically different architectures. The retina 
performs the four operations listed in Table 5.2 in a pixel-parallel fashion, 
whereas most synthetic imagers perform only detection in the pixel; the few that 
also amplify and quantize the signal perform these operations pixel-serially, and 
set the gain, sampling rate, and step size to be the same for all pixels [16, 22, 25]. 
In sharp contrast to human-engineered imagers, the retina adapts its gain, 
sampling rate, and step size locally, to maximize information uptake; it also 
whitens the signal in space and time, to minimize the redundancy in its output 
samples. I present a pixel design in Section 5.3 that performs continuous sens- 
ing, bandpass spatiotemporal filtering, local AGC, and adaptive quantization. 
5.3 
A RETINOMORPHIC PIXEL 
I designed the pixel circuit shown in Figure 5.6 using the retinomorphic ap- 
proach; it senses, amplifies, filters, and quantizes the visual signal. In general 
terms, this retinomorphic pixel operates as follows. 
The transducer is a vertical bipolar transistor; its emitter current is pro- 
portional to the incident light intensity [30]. Two current-spreading net- 
works [1, 9, 12, 14] diffuse the photocurrent signals over time and space; the 

RETINOMORPHIC APPROACH 
137 
wW::  
OPL 
Irese4 
~'pu 
~
i
~
~
 
Logic 
Pulse Generator 
)Vad Pt r-~~ 
IKtÂ¢ 
(Ay Vw>~l C 
VressectanOu 
t 
) Ry 
% 
;~X x 
A )r"~- J~,. ~,,I-< Sea n I n 
Diode-Capacitor 
Integrator 
Figure 5.6 
Retinomorphic pixel. The outer-plexiform-layer (OPL) circuit performs spa- 
tiotemporal bandpass filtering and local automatic gain control (AGC) using two current- 
spreading networks. Nodes V0 and W0 are connected to their six nearest neighbors on a 
hexagonal grid by the delta-connected transistors. The OPUs output current is converted 
to pulse frequency by the pulse generator. The logic circuit communicates the occurrence 
of a pulse (Vspk) to the chip periphery using the row and column request and select lines 
(Ry/Ay and Rx/Ax), turns on Ireset to terminate the pulse, and takes Vadapt low, to 
feed a current pulse to the integrator; the logic circuit is described elsewhere [4, 186]. The 
integrator's output current (IK) is subtracted from the input to the pulse generator; the 
device in series with the integrator's output, whose gate is tied to a fixed bias Vreset, is 
used to isolate the integrator from the rapid volatges swings that occur at Vmem when 
spikes occur. The two series-connected transistors on the right are used to scan out the 
integrator's output for display on a video monitor. 
first layer (node V0) excites the second layer (node W0), which reciprocates 
by inhibiting the first layer. The result is a spatiotemporally bandpass-filtered 
image [6, 13, 33]. The second layer computes a measure of the local light in- 
tensity, and feeds back this information to the input layer, where the intensity 
information is used to control light sensitivity. The result is local AGC [9]. 
A pulse generator converts current from the excitatory layer into pulse fre- 
quency. The diode-capacitor integrator computes a current that is proportional 
to the short-term average of the pulse frequency; this current is subtracted from 
the pulse generator's input. The difference becomes larger as the input changes 
more rapidly, so pulses are fired more frequently. Hence, the more rapidly the 
input changes, the more rapidly the pulse generator fires. 
Adding a fixed charge quantum to the integrating capacitor produces a mul- 
tiplicative change in current--due to the exponential current-voltage relation 
in subthreshold. Hence, the larger the current level, the larger the step size. 
The result is adaptive quantization. I also use the diode-capacitor integrator in 
the postprocessor to integrate the pulses, and to reconstruct the current level 
that was encoded into pulse frequency. 

138 
NEUROMORPHIC 
SYSTEMS 
ENGINEERING 
I discuss the behavior of these circuits in detail in Sections 5.3.1 
through 5.3.3. The performance of the solutions adopted are analyzed, with 
emphasis on the tradeoffs inherent in the circuit topologies chosen. 
5.3.1 
Preprocessing: Spatiotemporal Bandpass 
Using the small-signal model of the coupled current-spreading networks shown 
in Figure 5.7, I found that 
Io + V~V~/r~c 
= 
g~oV~ + c~o~/~ + g~V~, 
g~Vc + V~V~/r~ 
= 
g~oV~ + C~o~, 
(5.1) 
in the continuum limit. Here, V~ is the voltage in the excitatory network, 
which models retinal cones; Vh is the voltage in the inhibitory network, which 
models retinal horizontal cells (HCs); and Io is the photocurrent [6]. These 
functions are now continuous functions of space, (x, y), and of time, t; ~72f is 
the Laplacian of f (i.e., O~f/cgx ~ + c92f/cgy~), and ] is the temporal derivative 
Io(.r,,_l) .//~o(.~',,} 
~ 
Io(.~',~+1) , 
( 
/ 
IA A 
gco< ~ 
~ 
.
.
.
.
.
.
 
~ 
~ 
~ 
g~,c l "~( :c~,-1 
~ 
~ 
~ 
Figure 5.7 
Linear circuit model of the outer plexiform layer (OPL) circuit. Two resistive 
networks model the intercone and the inter-horizontal-cell electrical synapses (gap junc- 
tions), and transconductances model the reciprocal chemical synapses between cones and 
horizontal cells. ~ is the pixel size; it relates the modeled quantities, which are in current 
per unit area, sheet resitance, conductance per unit area, and capacitance per unit area, to 
quantities in the real circuit. The model is analyzed in the continuum limit, where ~ --~ 0. 

.IOi.A'el.[Oq OIIInS Oq& "[9] so!auanboaj lni,:~nds ti$i, q an ssndmo I somoaoq Stli,lln a inaod 
-mo~ oq:) pun 'sot.auonboaj inaodulo:) q~i,q :~n ssndato I SOUlOaOq ~ui,un ~ In!P, nds oR& 
"aaaIg Faodmaa ssndptlnq n pun aoaitj Fi,:)nds ssndpunq n jo O:)l.sodmoa oq:) .Zld 
-rots aou s~. :)i, 'si. :)nqa !oIqnandos XI:tnoui,I aou si, osuodsoa iin.Ioao oq:) '.IOaOatO H 
â¢ (Xai,ai,ai,suas 
~ui,~n~ ) s~!puonbo N i~odm~ mo I ]~ ss~dpu~q st osuodso~ 2~uonb~j-i~ds 
~q] ~q~ pu~ '(X~DD~suos ~g) 
soDuonbog F~ds tool ~ ss~dpu~q s~ osuods 
-o~ X~u~nbog-F~odmo~ oq] ~q~ ~h~osqo 'Am/VdUO = ~ 
'I'0 = ~ 
'~'0 
= ~ 'sm00g = ~z 'sm0~ = ~z 'oU0 = ~3 'oÂ£0'0 = ~ 
:s~oIIOJ s~ ~ 
posn 
san[~a saaaam~a~d jo aas aq~ -Â£~ o~n~ u~ poalo[d s~ sFÂ£l~uv s~qa moaj potq~a 
-qo ~aomaau auoa XaoamDxa aqa jo asuodsoa Â£auonbaaj lvaodmaao~a~ds aq& 
'(~) ,{3uanbaJj 
leJoduJal pue (d) ,~3uanbaJj lep, eds snsJaA aseqd (q) pue apnl!u:~eua (e) ~oqs s~,old a3ej 
-ms leUO!SUatU!p-aa~q~, asaq_L 'lapoua -1dO ~eaU!l jo ~I!A!I!SUaS lemdma~,op, edcj 8"Â£ am~!_-I 
(q) 
(n) 
(#apIe~eqe 
[(~d/A~)aal#o~ 
"'eSaOA Oai.A pu~ 'auoa oq:) o:) DH Â°tD moaj sut~$ 
O~][OA dooi-uado oqa aa~ ~ pue ~ jo si~aoadpaa aq& "oaumanpuoasu~aa aq~ oa 
oaumanpuoa a~a 
I 1o so~a aqa aa~ ~6/o~6 = ~ pu~ ~6/o~6 = ~ pu~ ~punoa~ 
oa saau~aanpuoa 2q paa~idaa saau~aanpuoasu~aa qa~ 's~ao~aou p~idnoaop aqa jo 
saumsuoa oa~ds aql oar a/t_(~6~a) = ~ pu~ a/~_(~6~) = ~ ~XiaA~aaodsoa 
'$u~Idnoa OHma-auoa aqa qa~m pu~ Su~idnoa auoa-m-OH aqa qa~m poa~pos~ 
sau~asuoa om~a oqa aa~ "~6/o~a = ~ pu~ ~'6/o.a = .~ 'oao H 
â¢[9] (suv:pva 
u~ qaoq) Xauanboaj Faod=aa s~ ~ puv 'Xauanboaj F~a~ds s~ (~d + {d)~ = d 
'(~ '~ 'z), jo ~aojsu~aa aa~ano d aqa soaouop (m' ~d'~O)[ .~i/~d ~ (~'d)~g aaaq~ 
. 
d~faP 
, I + (~ + m~ 
+ z ae~ 
+ m~ 
+ ad~) ~6 
. 
d ~ 
~ + m~, + a ~ 
I 
u~aqo oa suo~avnbo oR10AIOS pu~ 'om~a pu~ aa~ds u~ smaojsuvaa ao[ano d 
a~m I 'suo~l[puoa i~l~u~ snoauo$omoq pu~ ~uolxa F!a~ds oa~ugu~ Su~mnss V 
'[Z~ 'e~ 'g*] ~ 
pazXlvn~ pu~ pasodoad aaa~ auo sNa oa aN~mF sIapo ~ 
'(~0/]0 "a'~) f jo 
6~I 
HDVOHddV DIHdHOINONI&~H 

140 
NEUROMORPHIC SYSTEMS ENGINEERING 
is observed in physiological data measured from cats [20] and psychophysical 
data measured from humans [26]. 
There are tradeoffs among small low-frequency response, large dynamic 
range, and high sensitivity. 
The circuit requires a high-gain cone-to-HC 
synapse (i.e., small eh) to attenuate the cone's response to low spatial and 
temporal frequencies, since /t/c(0,0) = eh/gÂ¢h. However, increasing the gain 
of the cone-to-HC synapse decreases the dynamic range of the cone, (i.e., 
Vc < â¢hVlin, where Vlin is the linear range.) It also makes the circuit ring 
since, Q = (e~x/~ + ehV/~) -1. 
Smith and Sterling realized this constraint on the loop gain, and proposed 
using feedforward inhibition to second-order cells (bipolar cells) to attenuate 
the low frequency response [37]. Alternatively, to maintain temporal stability, 
we can decrease the gain of the HC-to-cone feedback synapse (1/e~), or reduce 
the HC's time constant (~-h). Unfortunately, both changes reduce the peak 
sensitivity of the cone 1)(0, &) = Qv/(~-h/~-~). The circuit implementation shown 
in Figure 5.6 has high gain from the excitatory cone node (V0) to the inhibitory 
HC node (W0), giving it small DC response and high sensitivity, but poor 
temporal stability. 
5.3.2 Amplification: Local Automatic Cain Control 
I achieve local AGC by making the intercone conductance (l/roe) proportional 
to the local average of the photocurrent. This adaptation is realized in the 
circuit simply by the fact that (Vdd-V0) is equal to the sum of the gate-source 
voltages of two devices. The currents passed by these devices represent the 
activity in the inhibitory network, Ih, which is equal to the local average of 
the intensity, and represent the activity of the excitatory network, Ic, which 
is equal to the Laplacian of the smoothed intensity profile (see Equation 5.1). 
Hence, by the extended transfinear principle [1], the current that spreads in 
the excitatory network is proportional to the product, IcIh, of these currents. 
Since Ih scales with the intensity, the internode coupling in the excitatory cone 
network will scale accordingly [9]. 
It remains to show that the response of the excitatory cone network is pro- 
portional to the intercone resistance [5]. Closed-form solutions for the impulse 
response may be obtained in the one-dimensional space: 
Vc(x) = rccIo2~e-lx[/L sin(Ixl/f - ~r/4), 
where L = ~ 
= (rccgchrhhghc) -1/4 is the effective space constant of the 
dual-layer network. These solutions are valid for the case go0 = gho : 0, which 
is a fairly good approximation of the actual circuit. Linear system theory thus 
predicts that the gain of the cone is equal to the product of the space constant 
and the intercone coupling resistance. 
This analysis also reveals that we compromise receptive-field size constancy 
by using the intercone coupling to implement local AGC, because the space 

RETINOMORPHIC APPROACH 
141 
constant depends on the intercone resistance: L = (rccgchrhhghc) -1/4. Thus, 
as we increase rcc to increase the gain, the receptive field contracts. 
The images shown in Figure 5.9 demonstrate the effects of bandpass filter- 
ing and local AGC. These data are from the retinormorphic chip described 
in [9]; images of the same scenes acquired with a CCD camera are included 
for comparison [11]. 3 Bandpass filtering removes gradual changes in intensity 
and enhances edges and curved surfaces. It also reduces the variance of the 
amplitude distribution by setting uniform areas to the mean level (gray). Lo- 
cal AGC extends the dynamic range by boosting the gain in the dark parts of 
the scene. Thus, the retinomorphic chip picks up information in the shadows, 
whereas the output of the CCD camera is zero throughout that region. 
Unfortunately, the retinomorphic chip's output is noisier in the darker parts 
of the image, due to the space constant decreasing with increasing gain. When 
the space constant decreases, wideband salt-and-pepper noise is no longer atten- 
uated, because the cutoff frequency shifts upward. The dominant noise source 
is the poor matching among the small (4L Ã 3.5L; where L, the minimum 
feature size, is 2#m) transistors used--it is not shot noise in the photon flux. 
Nevertheless, when it replaced the CCD as the front-end of a face-recognition 
system, the 90 Ã 90-pixel OPL chip improved the recognition rate from 72.5% 
to 96.3%, with 5% false positives, under variable illumination [11]. 
Figure 5.9 
CCD Camera (top row) versus retinomorphic imager (bottom row) under 
variable illumination. 
The CCD camera has global automatic gain control, whereas the 
retinomorphic imager has local automatic gain control and performs bandpass filtering. 

142 
NEUROMORPHIC SYSTEMS ENGINEERING 
5.3.3 
Quantization: Adaptive Neuron Circuit 
I built an adaptive neuron circuit by taking a pulse generator and placing a 
diode-capacitor integrator around it in a negative-feedback configuration (See 
Figure 5.6). The pulse-generation circuit has a high-gain amplifier (two digital 
inverters) with positive feedback (capacitive divider) [8]. The high-gain ampli- 
fier serves as a thresholding device, and the positive-feedback network provides 
hysteresis. In addition, there is a reset current ([reset) produced by the logic 
circuit that terminates the spike. 
The response of the adaptive neuron circuit to a 14% change in its input 
current is shown in Figure 5.10; these data demonstrate the adaptive sampling 
rate, the adaptive step size, and the integration of pulse trains by the diode- 
K(C|) 
Channel 
Current 
for 
Step 
Input 
.................. 
,~,., 
.,..~..~ 
,.~,~ .......... ?. 
..... ,.",.-~ ..... 
~. 
i 
- ~  
~ 
~- 
~, 
~-.._~J 
~ 
~ -~ - 
(*~ 
! 
"- 
~'\ ~i 
~\'d 
..... 
,~',~r"~ '~'/' "~ 
â¢ ~<,[,.~:~,,,~!4,,~ 
oo~ 
~ ~ 
o.~ 
~ 
~ ~ 
o1~ 
o1~ 
~ ~a 
o1~ 
~ 
T~m~ 
~ e ~  
M 
b 
a 
VoItag 
f 
St 
p 
Input 
em 
r 
ne 
e 
or 
e 
â¢ ! 
~ 
: 
,~,~ 
,~ 
/, 
ill i !!il ~ 
~: 
~:.:r: I il 
i 
/ 
i"!!~!!!i ( ; " / 
~1~ 
c
~
 
ooe 
o.oa 
o.1 
o12 
o14 
o16 
ola 
02 
T~me 
cseos) 
Spike 
Train 
for 
step 
Input 
~ 
~ 
, 
o 
Time 
(S) 
Figure 5.10 
Adaptive neuron's step response. Top: The neuron's input current and the 
integrator's output current. Middle: Input voltage ramping up between the reset (1.5V) and 
threshold levels (2.2V). Bottom: The spike train. The difference between the input current 
and the integrator's output current ramps up the input voltage as the surplus current charges 
the input capacitance. 

RETINOMORPHIC APPROACH 
143 
capacitor integrator. Other designs for adaptive neurons are described in [177, 
52]. 
The diode-capacitor integrator is based on the well-known current-mirror 
circuit. A large capacitor at the input of the mirror integrates charge, and the 
diode-connected transistor leaks charge away. This circuit's temporal behav- 
ior is described by the following nonlinear differential equation in the current 
domain: 
QT 
d/out 
/out(t) 
dt 
- Iin(t) - jflout(t), 
where UT = kT/q is the thermal voltage, A = exp(VA/UT ) is the current gain 
of the mirror, and QT = CUT/n is the charge required to change the current 
by a factor of e = 2.72 [3, 30]. This circuit has a time-constant "r = QT/Iout(t), 
that is proportional to the current level due to exponential current-voltage 
relation in the subthreshold region. 
The output produced by a periodic sequence of current pulses is 
1 
/out(t0 + nT) = 1~iT + (1~lout(to) - l/iT)(1 + a) -n, 
(5.2) 
immediately after the (n + 1)th pulse; it decays as 
/out(t) = 
Iout(t0 + nT) 
Iout(t0+.T) (t - (to + nT)) + 1' 
AQT 
during the interspike interval, to + nT < t < to + (n + 1)T, where a = 
(exp(qa/QT) - 1) is the percentage by which the output current is incremented 
by each spike, and IT = aAQT/T is the steady state [3]. 
We can use the current-mirror gain A to control the decay rate, because the 
time scale is set by ~- = AQT/Iout. The fixed quantity of charge qa supplied 
by each current pulse multiplies the current by exp(qa/QT), since it takes QT 
to change the current by a factor of e = 2.72. Hence, the incremental change 
in the output current caused by a spike is not fixed: It is proportional to the 
output current level at the time that the spike occurs. The peak output current 
levels attained immediately after each spike converge to IT =-- aAQT/T 
when 
(1 + a) -n << 1. Therefore, the equilibrium output current level is proportional 
to the pulse frequency. 
The complete adaptive neuron circuit is described by two coupled differential 
equations: 
dVmem 
Cmem 
dt 
-/in - IK -- Qth~(Vmem -- Vth), 
1 
QT die _ q~5(Vmem - Vth) - XIK, 
IK 
dt 
where/in is the current supplied to node Vmem by the OPL circuit, and Cmem 
is the total capacitance connected to that node; IK is the current subtracted 

144 
NEUROMORPHIC SYSTEMS ENGINEERING 
from node Vmem by the integrator; Cca is the integrator's capacitance; QT = 
CcaUw/~ is the charge required to change the integrator's output current by a 
factor of e = 2.72; and Qth is the repolarization charge (i.e., the charge that we 
must supply to Vmem to bring the latter from the reset level to the threshold 
level (Vth)). 
The parasitic coupling capacitance between Vmem and the integrator's input 
node is not included in these equations. This capacitance can have a large 
influence on the circuit's behavior [3]. In this particular design, however, the 
cascode device between the integrator's output and the pulse generator's input 
(tied to Vreset) eliminates virtually all coupling. 
For a constant input current, these equations may be integrated to obtain 
) 
Qth = Iin/kn 
-
-
 AQT in \k--~T 
+ 1 , 
where An -= tn+l - tn is the interspike interval. 
When adaptation is complete, the interspike intervals become equal, and we 
have IKn = aAQT/An (from Equation 5.2). Hence, 
An = (Qth + 
=  
Qth/Iin 
(remember that q~ -- QT ln(1 + ~)). This result is understood as follows. 
During the interspike interval, An, the input current must supply the charge 
Qth to the capacitors tied to Vmem, and must supply the charge Aq~ removed 
by the integrator, where q~ is the quantity of charge added to the integration 
capacitor by each spike. Notice that firing-rate adaptation reduces the firing 
rate by a factor of y ~ 1 + Aq~/Qth. 
It is preferable to have I~(t) < /in(t) for all t, because V~m stays close to 
the threshold, making the latency shorter, and less variable, and keeping the 
integrator's output device in saturation. The circuit operates in this regime if 
~/ < 2/a [3]. A tradeoff is imposed by my desire to operate in this regime: If 
we want a large adaptation-attenuation factor ~, we must use a small charge 
quantum q~and must turn up A to compensate~making the number of spikes 
required to adapt large. 
5.4 
OVERALL SYSTEM PERFORMANCE 
The output of the postprocessor--after image acquisition, analog preprocessing, 
quantization, interchip communication, and integration of charge packets in the 
receiver's diode-capacitor integrators--is shown in Figure 5.11. The sparseness 
of the output representation is evident. 
When the windmill moves, neurons at locations where the intensity is in- 
creasing (white region invading black) become active; hence, the leading edges 
of the white vanes are more prominent. These neurons fire more rapidly as the 
speed increases because they are driven by the temporal derivative. The time 
constant of the receiver's diode-capacitor integrator is intentionally set shorter 
than that of the sender, so temporal integration occurs at only high spike rates. 

RETINOMORPHIC APPROACH 
145 
Figure 5.11 
Video frames from postprocessor chip showing real-time temporal integration 
of pulses. The stimulus is a windmill pattern (left) that rotates counterclockwise slowly 
(middle) and quickly (right). 
This mismatch attenuates low frequency information, and results in an overall 
highpass frequency response that eliminates the fixed-pattern noise and en- 
hances the imager's response to motion. The mean spike rate was 30Hz per 
pixel, and the two-chip system dissipated 190 mW at this spike rate. 
5.5 
DISCUSSION 
I have described the design and performance tradeoffs of a retinormorphic im- 
ager. This VLSI chip embodies four principles of retinal operation. 
First, the imager adapts its gain locally to extend its input dynamic range 
without decreasing its sensitivity. The gain is set to be inversely proportional to 
the local intensity, discounting gradual changes in intensity and producing an 
output that is proportional to contrast [9]. This adaptation is effective because 
lighting intensity varies by six decades from high noon to twilight, whereas 
contrast varies by at most a factor of 20 [34]. 
Second, the imager bandpass filters the spatiotemporal signal to attenuate 
low-frequency spatial and temporal signals, and to reject wideband noise. The 
increase in gain with frequency, for frequencies below the peak, matches the 
1If 2 decrease in power with frequency for natural image spectra, resulting 
in a flat output power spectrum. This filtering improves information coding 
efficiency by reducing correlations between neighboring samples in space and 
time. It also reduces the variance of the output, and makes the distribution of 
activity sparse. 
Third, the imager adapts its sampling rate locally to minimize redundant 
sampling of low-frequency temporal signals. In the face of limited communi- 
cation resources and energy, this sampling-rate adaptation has the additional 
benefit of freeing up capacity, which is dynamically reallocated to active pixels, 
allowing higher peak sampling rates and shorter latencies to be achieved [4]. 
And fourth, the imager adapts its step size locally to trade resolution at 
high contrast levels, which rarely occur, for resolution at low contrast levels, 
which are much more common. The proportional step size in the adaptive 
neuron, which results in a logarithmic transfer function, matches an exponen- 
tially decaying amplitude probability density, making all quantization intervals 

146 
NEUROMORPHIC SYSTEMS ENGINEERING 
equiprobable. Hence, it maximizes the expected number of signals that can be 
discriminated, given their probability of occurrence. 
For independent samples, information is linearly proportional to bandwidth, 
and is logarithmically proportional to the signal-to-noise ratio (SNR) [36]. We 
increase bandwidth by making the receptors smaller and faster, so that they 
can sample more frequently in space and time. As an unavoidable consequence, 
they integrate over a smaller volume of space time and therefore SNR degrades. 
There is therefore a reciprocal relationship between bandwidth and noise power 
(variance) [38]. Since their goal is to maximize information, biological sensory 
systems aggressively tradeoff SNR for bandwidth, operating at SNRs close to 
unity [14, 38]. 
With this optimization principle in mind, I have proposed compact circuit 
designs that realize local AGC, bandpass filtering, and adaptive quantization 
at the pixel level. The overriding design constraints are to whiten the signal, 
which makes samples independent; to minimize the pixel size and capacitance, 
which makes sampling more dense and more rapid; and to minimize power 
consumption, which makes it possible to acheive very-large scale integration. 
Hence, all circuits use minimal-area devices and operate in subthreshold, where 
the transconductance per unit current is maximum. I realized extremely com- 
pact implementations by modeling these circuits closely after their biological 
counterparts [3, 9]. 
I analyzed three limitations in these simple circuit designs. 
First, attenuating low frequencies by using a high-gain receptor-to-HC 
synapse (ratio of 9hc/gho = 1/eh) results in temporal instability. To break 
this tradeoff, we must regulate the gain dynamically. 
Second, controlling the gain by changing the receptor-to-receptor coupling 
strength compromises the receptive field size. To decouple these parameters, 
we must change one of the synaptic strengths (transconductances, gch or ghc) 
proportionally. 
And third, attenuating the firing rate by using an integrator with a long 
time constant results in extremely slow adaptation, because we must use a 
small charge quantum to avoid sending the integrator's output above the input 
level. The circuit would adapt more rapidly, and fire fewer spikes in the process, 
if it maintains a uniformly high firing rate until the integrator catches up with 
the input, and then switches abruptly to a low firing rate. 
5.6 
CONCLUSIONS 
Taking inspiration from biology, I have described an approach to building ma- 
chine vision systems that perform sophisticated signal processing at the pixel 
level. These retinomorphic systems are adaptive to their inputs, and thereby 
maximize their information-gathering capacity and minimize redundant infor- 
mation in their output data stream. 
These optimization principles are radically different from those that drive the 
design of conventional video cameras. Video cameras are designed to reproduce 

RETINOMORPHIC APPROACH 
147 
any arbitrary image to within a certain worst-case error tolerance, whereas 
biologics exploit the statistical properties of natural spatiotemporal signals, 
giving up worst-case performance to get better average-case performance. 
Optimizing average-case performance maximizes the discrimination ability 
of biologics. Consequently, biomorphic systems promise superior solutions for 
human-made systems that perform perceptive tasks, such as face recognition 
and object tracking, energy efficiently. 
Acknowledgments 
This work was partially supported by the Office of Naval Research; DARPA; the 
Beckman Foundation; the Center for Neuromorphic Systems Engineering, as part 
of the National Sceince Foundation Engineering Research Center Program; and the 
California Trade and Commerce Agency, Office of Strategic Technology. 
I thank my thesis advisor, Carver Mead, for sharing his insights into the operation 
of the nervous system. And I also thank Misha Mahowald, Tobi Delbriick, John 
Lazzaro, and Peter Sterling for numerous helpful discussions. 
Notes 
1. I am assuming a linear encoding--a practice that is the standard. This assumption 
limits the dynamic range to 2 b for a b-bit encoding. 
2. I performed bandpass filtering by convolving the input image with the laplacian of a 
Gaussian with a = 2.5 pixels. I calculated the autocorrelation of the images by subtract- 
ing out the mean, shifting a copy of the image up or right by 1 to 75 pixels, multiplying 
corresponding pixels, and summing; I normalized the results to yield a maximum of unity. 
Rightward shifts are plotted on the positive axis (0 to 75), and upward shifts are plotted on 
the negative axis (0 to -75). 
3. CCD Camera Specifications: COHU Solid State RS170 Camera (142320), auto iris, 
gamma factor enabled, 512 x 480 pixels, 8-bit gray-level outputs. Lens Specifications: COS- 
MICAR TV lens, ES 50inm, 1:1.8. This comparison, and the face recognition studies, were 
done in collaboration with Frank Eeckman, Joachim Buhman, and Martin Lades of the 
Lawrence Livermore National Labs, Livermore CA. 
References 
[1] A. Andreou and K. Boahen. Translinear circuits in subthreshold MOS. J. 
Analog Integrated Circ. Sig. Proc., 9:141-166, 1996. 
[2] J. Atick and N. Redlich. What does the retina know about natural scenes. 
Neural Computation, 4(2):196-210, 1992. 
[3] K. Boahen. The adaptive neuron and the diode-capacitor integrator. In 
preparation. 
[4] K. Boahen. Communication neuronal ensembles between neuromorphic 
chips. In preparation. 
[5] K. Boahen. Toward a second generation silicon retina. Technical Report 
CNS-TR-90-06, California Institute of Technology, Pasadena CA, 1990. 

148 
NEUROMORPHIC SYSTEMS ENGINEERING 
[6] K. Boahen. Spatiotemporal sensitivity of the retina: A physical model. 
Technical Report CNS-TR-91-06, California Institute of Technology, 
Pasadena CA, 1991. 
[7] K. Boahen. Retinomorphic vision systems. In Int. Conf. on Microelec- 
tronics for Neural Networks, volume 16-5, pages 30-39, Los Alamitos, CA, 
1996. EPFL/CSEM/IEEE. 
[8] K. Boahen. 
Retinomorphic vision systems II: Communication channel 
design. In Proceedings of the IEEE International Symposium on Circuits 
and Systems, volume supplement, pages 9-14, Atlanta, GA, May 1996. 
[9] K. Boahen and A. Andreou. A contrast-sensitive retina with reciprocal 
synapses. In J E Moody, editor, Advances in Neural Information Process- 
ing, volume 4, San Mateo CA, 1991. Morgan Kaufman. 
[10] K. A. Boahen. A retinomorphic vision system. IEEE Micro, 16(5):30-39, 
October 1996. 
[11] J. Buhman, M. Lades, and Eeckman F. Illumination-invariant face recog- 
nition with a contrast sensitive silicon retina. In J D Cowan, G Tesauro, 
and J Alspector, editors, Advances in Neural Information Processing, vol- 
ume 6, San Mateo CA, 1994. Morgan Kaufman. 
[12] K. Bult and G. J. Geelen. An inherently linear and compact MOST-only 
current division technique. IEEE J. Solid-State Circ., 27(12):1730-1735, 
1992. 
[13] P. C. Chen and A. W. Freeman. A model for spatiotemporal frequency 
responses in the x cell pathway of cat's retina. Vision Res., 29:271-291, 
1989. 
[14] R. R. de Ruyter van Steveninck and S. B. Laughlin. The rate of information 
transfer at graded-potential synapses. Nature, 379:642-645, February 1996. 
[15] T. Delbriick and C. Mead. Photoreceptor circuit with wide dynamic range. 
In Proceedings of the International Circuits and Systems Meeting, IEEE 
Circuits and Systems Society, London, England, 1994. 
[16] A. Dickinson, B. Ackland, E. El-Sayed, D. Inglis, and E. R. Fossum. Stan- 
dard CMOS active pixel image sensors for multimedia applications. In 
William Dally, editor, Proceedings of the 16th Conference on Advanced Re- 
search in VLSI, pages 214-224, Chapel Hill, North Carolina, 1995. IEEE 
Press, Los Alamitos CA. 
[17] D. Dong and J. Atick. Statistics of natural time-varying scenes. Network: 
Computation in Neural Systems, 6(3):345-358, 1995. 
[18] D. Dong and J. Atick. Temporal decorrelation: A theory of lagged and non- 
lagged responses in the lateral geniculate nucleus. Network: Computation 
in Neural Systems, 6(2):159-178, 1995. 
[19] M. Eckert and G. Buchsbaum. Efficient coding of natural time-varying 
images in the early visual system. Phil. Trans. Royal Soc. Loud. Biol, 
339(1290):385-395, 1993. 

RETINOMORPHIC APPROACH 
149 
[20] C. Enroth-Cugell, J. G. Robson, D. E. Schweitzer-Tong, and A B. Watson. 
Spatiotemporal interactions in cat retinal ganglion cells showing linear 
spatial summation. J. Physiol., 341:279-307, 1983. 
[21] D. J. Field. Relations between statistics of natural images and the response 
properties of cortical cells. J. Opt. Soc. Am., 4:2379-2394, 1987. 
[22] B. Fowler, A. E. Gamal, and D. Yang. A CMOS area image sensor with 
pixel-level A/D conversion. In John H. Wuorinen, editor, Digest of Tech- 
nical Papers, volume 37 of IEEE International Solid-State Circuits Con- 
ference, pages 226-227, San Francisco, California, 1994. 
[23] K. Fujikawa, I. Hirota, H. Mori, T. Matsuda, M. Sato, Y. Takamura, S. Ki- 
tayama, and J. Suzuki. A 1/3 inch 630k-pixel IT-CCD image sensor with 
multi-function capability. In John H. Wuorinen, editor, Digest of Technical 
Papers, volume 38 of IEEE International Solid-State Circuits Conference, 
pages 218-219, San Francisco, CA, 1995. 
[24] B. Hoeneisen and C. Mead. l~ndamental limitations in microelectronics-I: 
MOS technology. IEEE J. Solid-State Circ., 15:819-829, 1972. 
[25] C. Jansson, I. Per, C. Svensson, and R. Forchheimer. An addressable 256 
Ã 256 photodiode image sensor array with 8-bit digital output. Analog 
Integr. Circ. ~ Sig. Proc., 4:37-49, 1993. 
[26] D. H. Kelly. Motion and vision II: Stabilized spatiotemporal threshold 
surface. J. Opt. Soc. Am., 69(10):1340-1349, 1979. 
[27] John Lazzaro. Temporal adaptation in a silicon auditory nerve. In John E. 
Moody, Steve J. Hanson, and Richard P. Lippmann, editors, Advances in 
Neural Information Processing Systems, volume 4, pages 813-820. Morgan 
Kaufmann Publishers, Inc., 1992. 
[28] M. Mahowald and R. Douglas. A silicon neuron. Nature, 354:515 518, 
1991. 
[29] M. Mahowald and C. Mead. The silicon retina. 
Scientific American, 
264(5):76-82, 1991. 
[30] C. Mead. A sensitive electronic photoreceptor. In H. F~chs, editor, 1985 
Chapel Hill Conference on VLSI, pages 463-471, Rockville MD, 1985. 
Computer Science Press, Inc. 
[31] C. Mead. Scaling of MOS technology to submicrometer feature sizes. J. 
of VLSI Signal Processing, 8:9-25, 1994. 
[32] C. A. Mead and M. Ismail, editors. Analog VLSI Implementation of Neural 
Systems. Kluwer, Norwell, MA, 1989. 
[33] S. Ohshima, T. Yagi, and Y. Funashi. Computational studies on the inter- 
action between red cone and H1 horizontal cell. Vision Res., 35(1):149-160, 
1994. 
[34] W. A. Richards. A lightness scale for image intensity. Appl. Opt., 21:2569- 
2582, 1982. 

150 
NEUROMORPHIC SYSTEMS ENGINEERING 
[35] B. Sakman and 0. D. Creutzfeldt. Scotopic and mesopic light adaptation in 
the cat's retina. Pfldgers Archiv f(tr die gesamte physiologie, 313:168-185, 
1969. 
[36] C. E. Shannon and W. Weaver. The Mathematical Theory of Communi- 
cation. Univ. Illinois Press, Urbana IL, 1949. 
[37] R. G. Smith. 
Simulation of an anatomically defined local circuit: The 
cone-horizontal cell network in cat retina. 
Visual Neurosci., 12(3):545- 
561, May-Jun 1995. 
[38] W. R. Softky. Fine analog coding minimizes information transmission. 
Neural Networks, 9(1):15-24, 1996. 
[39] J. H. van Hateren. A theory of maximizing sensory information. Biol. 
Cybern., 68:23-29, 1992. 
[40] E. Vittoz and X. Arreguit. Linear networks based on transistors. Elec- 
tronics Letters, 29(3):297 299, February 1993. 

ANALOG VLSI EXCITATORY 
FEEDBACK CIRCUITS FOR 
ATTENTIONAL SHIFTS AND TRACKING 
T.G. Morris and S.P. DeWeerth 
School of Electrical and Computer Engineering, 
Georgia Institute of Technology Atlanta, GA 30332-0250 U.S.A. 
tgrnorris@sedona.intel.com 
6.1 
INTRODUCTION 
In biological vision systems, the term attention describes the way that infor- 
mation is prioritized and selected [23]. Selective attention is necessary in visual 
processing in order to handle the overwhelming amount of sensory information 
that is available. Visual systems, such as those found in primates, have a hy- 
brid architecture in which low-level processing is performed in parallel across 
the entire visual field, and high-level processing is only performed on a selected 
subregion of the visual field [2]. Low-level tasks that are computed entirely in 
parallel are described as preattentive. Attentive processing uses this preatten- 
tive information to select a smaller region of interest for subsequent high-level 
processing. The duality of parallel computation and serial selections of regions 
of interest exemplifies the trade-off between speed and processing sophistication 
that results from the utilization of a limited amount of processing circuitry. If 
the attentional selection were not performed, an overwhelming amount of neu- 
ral circuitry would be required in order to perform the high-level processing in 
parallel over the entire visual field [1]. 
The transition from preattentive to attentive processing requires the selec- 
tion of a region of interest. In 1985, Koch and Ullman [13] introduced a theory 
of selective shifts of attention based upon a saliency map. The saliency map 
is a measure of levels of interest across the visual field, created through the 
integration of features that are detected preattentively, such as color, orienta- 
tion, spatial derivatives, temporal derivatives, and motion. Specific features 

152 
NEUROMORPHIC 
SYSTEMS 
ENGINEERING 
may be attenuated or amplified, depending upon the visual processing appli- 
cation [4, 23]. The saliency map encodes where interesting features are within 
the visual field, but not what those features are [21]. Measurements of neurM 
activity in the posterior parietal cortex, the pulvinar, and the superior collicu- 
lus demonstrate characteristics that are consistent with the idea that a saliency 
map exists [4]. Koch and Ullman proposed that shifts in selective attention are 
determined by a winner-take-all computation that is performed on the saliency 
map information. Furthermore, the transition of attention to each new location 
exhibits a preference to salient features within close spatial proximity to the 
most recently attended location [13]. 
The attentive search task is defined as a sequential search from one location 
of the visual field to the next. This sequential search method has been ex- 
plained as a "spotlight" of attention that moves across the visual field [2, 27]. 
The selection process, which drives the movement of the spotlight, must deter- 
mine whether to fixate on a presently selected region of interest or move to a 
new region of interest. Two main theories have emerged to describe how the 
spotlight moves from one point to the next [25, 27]. One theory, the analog 
description, states that the spotlight moves in a continuous path, which would 
imply that locations between the two points of interest are not neglected in the 
transition. Another theory, the quantum description, states that there exist 
several spotlights that turn on and off, depending on where the attention is 
directed. The latter theory has been supported by experiments showing that 
the time required to make an attentional shift is independent of the distance 
traversed [24]. The winner-take-all selection mechanism that was described by 
Koch and Ullman [13] is consistent with the quantum theory of attentional 
shifting. 
Attentional processing is important for the localization and discrimination of 
objects within the visual field. The primate visual system is able to accomplish 
these tasks in spite of extremely cluttered environments containing stimuli that 
are constantly moving with respect to the sensing array. The process by which 
regions of interest are selected plays a key role in the success of these visual 
systems. In an attempt to understand how such high performance is attained, 
we have implemented a model of selective attention within the neuromorphic, 
analog very large-scale integrated (VLSI) [18] paradigm. The development of 
these neuromorphic circuits may help us model similar cortical processing ar- 
chitectures found in the visual system of primates. Hardware implementations 
also provide an obvious contribution to machine vision systems by using the 
massively parallel architectures to achieve real-time performance. 
Previous work in the analog VLSI community has focused on visual pro- 
cessing tasks that model the preattentive processing within biological sys- 
tems [1, 182, 10, 51, 18]. These preattentive features, such as spatial deriva- 
tives, temporal derivatives, orientation, color, and motion can be used to create 
a saliency map for the inputs to the selective attention circuitry. A number 
of researchers within the neuroscience community are also working toward an 
understanding of how these features might be combined to create a saliency 

EXCITATORY FEEDBACK CIRCUITS 
153 
map [20, 21, 22]. Our efforts lie in the processing that occurs after the feature 
detection stage, and thus the specific choice of a saliency map is not critical to 
the evaluation of the circuits we present in this paper. 
There are two aspects of selective attention that we have chosen to address 
with our analog VLSI implementation of attentional processing. These aspects 
include the modeling of shifts of selective attention from one object of interest 
to the next and the ability to attend to a single object as it moves across the 
visual field. We use a winner-take-all computation with excitatory feedback to 
model attentive selection [7, 15]. The excitatory feedback enhances a neighbor- 
hood around the location that is presently attended, thus facilitating shifts of 
attention that demonstrate a preference to stimuli that are within close spa- 
tial proximity [8, 19]. The localized excitation also enables tracking of a single 
stimulus within a noisy environment. The shape of the excitatory feedback 
directly influences the system performance, both as a model for shifts in atten- 
tion and as a tracking mechanism. The use of excitation produces a hysteresis 
in the selection process. When the presently winning stimulus is excited, the 
winner-take-all computation resists the selection of a new winner unless that 
stimulus has a value much greater (as set by the excitation level) than the 
present winner [7]. 
In this paper we present hardware implementations of the winner-take-all 
computation with excitatory feedback to perform attentive selection. Specifi- 
cally, we present four different forms of excitation: local excitation, resistive- 
spreading excitation, discrete (nearest-neighbor) excitation, and a combination 
of the resistive-spreading and discrete forms that creates a convolved excita- 
tion. The local excitation only provides excitation to the single pixel that is 
presently winning in the selection process. The resistive-spreading excitation 
uses a current-mode resistive network to create an exponential decay of ex- 
citation with a variable space constant that is controlled by a single voltage 
input. The discrete excitation provides excitation to the winning pixel as well 
as to the nearest neighbors on each side, using a voltage input to determine 
the gain of the communication to the nearest neighbors. The combination of 
resistive-spreading excitation and discrete excitation produces an effective ex- 
citation that is determined by the convolution of the exponentially decaying 
signal of the resistive network and the three-pixel-wide pulse created by the 
discrete excitation. The convolved excitation provides a shape that has similar 
characteristics to that of a gaussian distribution. In Section 6.2 we describe 
the system that was used to test each of these forms of excitation in a selective 
attention computation. We describe each of the circuits and their performance 
in Sections 6.3, 6.4, 6.5 and 6.6. Section 6.7 demonstrates the use of these 
circuits in a tracking application. 
6.2 
SYSTEM ARCHITECTURE 
We have fabricated a one-dimensional, 20-element array of selective atten- 
tion circuits that include (i) phototransistors, (ii) normalization circuits, (iii) 

154 
NEUROMORPHIC SYSTEMS ENGINEERING 
winner-take-all selection circuits with all aforementioned types of excitatory 
feedback, and (iv) position-encoding circuits that indicate the present location 
of attention. 
The chip was fabricated in a 2.0 #m CMOS process through 
the MOSIS fabrication service. Figure 6.1 illustrates the organization of the 
processing layers. 
Photodetectors 
Normalization Circuit 
l 
Selection Circuit with Excitatory Feedback 
] 
Position-encoding Circuit 
Figure 6.1 
System architecture illustrating the arrangement of processing layers in the 
visual attention system. 
We have included the capability of providing explicit input currents at each 
position within the array in order to analyze the performance of the circuits un- 
der well-controlled input conditions. The chip was designed such that we could 
switch between using currents that come from the phototransistors or currents 
that come directly from the input pads. Thus, we are able to achieve two modes 
of testing on a single chip. When evaluating the spreading characteristics of the 
excitation, we use controllable input currents, and when evaluating the tracking 
performance we use intensity levels as the representation of saliency. Vertical 
bipolar phototransistors are used to convert optical inputs to currents that are 
passed into a linear normalization circuit [3]. Normalization is necessary to 
control the range of the inputs to the selective attention processing layers. We 
have access to measurements of the phototransistor currents, the normalization 
currents, the winner-take-all output currents, and the voltage output for the 
position-encoding circuit. 
The selection circuit includes all forms of excitation mentioned previously. 
Each form of excitation can be tested separately by changing the parameters 
associated with the remaining circuitry, such that these influences are turned 

EXCITATORY FEEDBACK CIRCUITS 
155 
"off." 
The separate types of excitation will be further described in subse- 
quent sections. The final stage of processing is a position-encoding circuit that 
receives an array of inputs and computes the centroid to produce a single ana- 
log output [6]. The output of the selection layer serves as the input to the 
position-encoding processing layer. This computation has also been used in 
previous sensorimotor systems, where it is important to convert a large multi- 
dimensional array of inputs into a small number of outputs to control a motor 
response [6, 9]. 
6.3 
LOCAL EXCITATION 
The winner-take-all is a nonlinear computation that receives an array of in- 
puts and produces an equally dimensioned array of outputs such that there is 
a high output in the location of the largest input and low outputs in all other 
locations. When two inputs are very similar in value, the winner-take-all com- 
putation can oscillate from one location to the next due to noise in the system. 
Such oscillations would be counterproductive in an attention system, because 
there would not be enough time to process the information at either location. 
The local excitation circuit provides a hysteretic feedback that eliminates the 
possibility of oscillations in the selection process. 
// 
// 
Ib 
// 
// 
Ih 
J 
'n~ M! ~o~ 
Vn 
I 
Ulll Vc 
~M4 
M3 
 IM6 
Jn 
J~ 
Figure 6.2 
Selection circuit with local excitation. 
One element of the winner-take-all circuit with local excitation is shown in 
Figure 6.2. This circuit is based on a current-mode winner-take-all circuit [15] 
that receives an array of input currents In and produces a single high out- 
put current or voltage that corresponds positionally to the largest input. The 

156 
NEUROMORPHIC SYSTEMS ENGINEERING 
winner-take-all selection is facilitated by a pair of transistors, M1 and M2, at 
each element. The input transistor M1 in the location with the largest input 
current is saturated producing a high output voltage Vn at the input node 
while all other output voltages are near ground, and the corresponding input 
transistors are out of saturation. Thus, the winning output current in the M2 
transistor is equal to the global bias current Ib and all other output currents 
are 0. The bias current is set by the gate voltage Vb on a single transistor 
that sinks the sum of the currents in the M2 transistors via the common node 
Vc. Local hysteresis can be added to the selection computation by feedback 
of the winner-take-all output current to the input node within each element of 
the array [7]. The winner-take-all circuit requires the bias current to be larger 
than the nominal input current levels due to possible ringing behavior other- 
wise [15]; therefore, it is necessary to attenuate the current that is fed back to 
the input, providing the excitation. Previous implementations have mirrored 
the bias current with an adjustable gain that is controlled by changing the 
source voltage on the current mirror [8]. The circuit shown in Figure 6.2 uses 
a different approach, whereby the excitatory (hysteretic) current Ih is gated 
by the output voltage of the winner-take-all circuit. The current is set by the 
gate voltage Vh. This excitatory current is also mirrored as an output current 
Jn, which serves as the input to the position-encoding circuit. For the local 
excitation circuit the output current Jn is equal to Ih at the winning location, 
and is 0 at all other locations. 
The performance of this form of excitation was tested by using current in- 
puts. The chip was optically shielded within a metal test fixture. The input 
current at position 10 within the 20-element array was set to 100 nA. For each 
iteration of testing, we swept a competing input current at a specific location 
within the array. We used a linear current sweep with 1 nA increments. The 
competing current was swept past the value of the "winning" current at po- 
sition 10 until the competing position became the winner. The current level 
at the newly winning position was then lowered until position 10 was once 
again selected. The difference between the two switching currents determines 
the window of hysteresis. The level of excitation that would produce such a 
hysteretic window is approximately one half of the difference between the two 
switching current levels. The measurement of the hysteretic window allows 
us to separate the excitation characteristics from some of the offsets of the 
winner-take-all circuit. The data in Figure 6.3 demonstrates the performance 
of the local excitation circuit when Vh is set to 0.71 V and Vb is set to 0.8 V. 
The hysteretic current, Ih, was measured to be 18 nA for this voltage setting. 
The hysteretic window is large (approximately 30 nA) for every location that 
is compared to the originally winning location at position 10. 
6.4 
RESISTIVE-SPREADING EXCITATION 
Providing excitation at a single winning location is not sufficient for attention if 
the stimulus moves with respect to the imaging array. Once the stimulus moves 

EXCITATORY FEEDBACK CIRCUITS 
157 
40 
Local Excitation 
~" 30 
Â¢- 
-~ 
t- 
.
~
 
N 
20 
.
~
 
m ~,, 
-r 
~0 
O0 
. . . .  
~ 
. . . .  
1'0 . . . .  
1'5 . . . .  
20 
Position 
Figure 6.3 
Experimental data demonstrating local excitation results. 
to the next pixel, it must compete with all other inputs, without the advantage 
of the excitation. This problem has motivated the use of a distribution of the 
excitation to neighboring pixels. The distribution will allow a stimulus to move 
with respect to the imaging array by maintaining some level of enhancement in 
the winner-take-all computation. The theories on shifts of selective attention 
are consistent with this idea of distributed excitation to a surrounding region 
of the presently attended location. With a locally distributed excitation, two 
problems are solved. A single attended stimulus can maintain attention as it 
moves across the visual field, and shifts of attention demonstrate a preference 
to salient stimuli that are in close proximity to presently attended locations. 
One way of distributing a signal to a surrounding group of pixels is through 
the use of a resistive network. 
A current-mode resistive network has been 
developed that uses a small number of transistors to approximate the linear re- 
lationship of a resistive network [1, 14]. The circuit consists of diode-connected 
p-channel MOSFETs that serve as the vertical resistance elements, and lat- 
eral n-channel MOSFETs that serve as the horizontal resistance elements in 
the network. The gates of the n-channel transistors are connected to a global 
voltage, so that the effective resistance of these elements can be changed. The 

158 
NEUROMORPHIC SYSTEMS ENGINEERING 
spreading characteristic of the resistive network follows the shape of a decaying 
exponential, dependent on the distance from the current being sourced. The 
space constant is determined by the relationship between the vertical resistances 
and the horizontal resistances. The use of a resistive network is advantageous 
due to its small size for implementation, and its adaptability to different space 
constants with the use of a single voltage. 
Figure 6.4 
~/ 
J 
// MI' 
II 
II 
M~ 
J~ 
}" 
11 
J~ 
Selection circuit with resistive-spreading excitation. 
The resistive-spreading excitation circuit is shown in Figure 6.4. Lateral 
transistors, MT, connect adjacent elements, creating a current-mode resistive 
network [1, 14] that distributes the excitatory current to the elements in the 
neighborhood of the winner. The gate voltage, Vr, of these transistors sets the 
space constant of the network. As Vr is increased, more current flows from 
the winner into the neighboring elements. The effect of this feedback in the 
winner-take-all competition is that elements near the winner receive preference 
over distant elements, as determined by the decaying exponential. By using 
the exponential relationship between voltage and current for the MOSFET in 
its subthreshold regime, we have derived an equation that relates the current 
for each position of the array to the peak current at the winning location. We 

EXCITATORY FEEDBACK CIRCUITS 
159 
used the following equation to describe the current-voltage relationship for the 
n-channel MOSFET in subthreshold operation [18]. 
~-~b 
~ = ~ 
~ 
The drain current is denoted by Id. The gate and source voltages are denoted 
by Vgb and Vsb, measured with respect to the bulk potential. The thermal 
voltage is denoted by VT, the leakage current is denoted by I0 , and the gate 
efficiency is denoted by ~, which models the backgate effect. The p-channel 
MOSFET is similarly modeled by changing the signs of the voltages. 
The fact that the input current to the resistive network is at a single location 
(determined by the winner-take-all computation) allows us to derive a relation- 
ship that is fairly concise. By approximating ~ for the p-channel transistors as 
1, we were able to derive a closed-form solution that describes the qualitative 
behavior of this circuit. The equation that relates the current output of the 
resistive network to the peak current at the winning position is as follows: 
In = Ino e-~b-nÂ°l 
(6.1) 
where In is the current at location n, and no is the location where the input 
current is being sunk (the winning location). The space constant, a, is defined 
by the following equation which shows the dependence on the voltage V~. 
( v~-~v~ ) 
c~ = nln 
e 
vr 
+1 
As the value of V~ gets very small, the value of a tends toward a large 
number, thus providing virtually no spreading to neighboring elements in the 
array. As the value of V~ increases, the value of c~ decreases and the amount of 
spreading increases. The value of the peak current Ino that occurs at position 
no is also dependent on V~. 
Ioe v~ . Ih 
(6.2) 
Ino : 
~ 
~ v~ 
Ioe VT + 2Ioe-wi-r 
As the spreading increases, with the increase of V~, the peak value decreases, 
as indicated by Equation 6.2. When V~ is very small, the Ioe ydd/yT term in the 
denominator dominates and the value of Ino is very close to Ih. 
Figure 6.5 shows experimental results of the resistive-spreading excitation 
circuit. The same test that was described for the local excitation circuit was 
performed for varying values of the voltage Vr. As Vr was increased, the value 
of the voltage Vh was also increased to achieve a comparable peak current value 
at the winning location. The three curves of Figure 6.5 demonstrate how the 
hysteretic window follows an approximately exponential decay as the competing 
position increases in distance from the originally winning location. The value 

160 
NEUROMORPHIC SYSTEMS ENGINEERING 
40 
Resistive-Spreading Excitation 
~" 30 
t- 
-~ 
~- 
. ~  
N 20 
. ~  
ffl 
~" ~0 
--r 
V r = 6
.
1
~
 
.... 
] .... 
'f' 
O0 
5 
10 
Position 
15 
20 
Figure 6.5 
Experimental data demonstrating resistive-spreading results for varying values 
of V~. 
of Ydd that was used for all the experimental data was 5.00 V. The first curve, 
which demonstrates the least amount of spreading, was generated by setting 
Vr to 6.15 V and Vh to 0.74 V. The peak hysteretic current, Ino, was measured 
to be 18.04 nA. The second curve shows a moderate amount of spreading, and 
was generated by setting Vr to 6.21 V and Vh to 0.77 V. For this parameter 
setting, I~ o was 17.83 nA. The spreading was further increased by setting V~ 
to 6.28 V and Vh to 0.80 V in the third trial, and for this parameter setting, 
Ino was 18.27 nA. 
The current-mode resistive network provides a solution for the distribution 
of excitation that is a compact implementation in an analog VLSI system. The 
transistor count for each pixel only increases by one from the local excitation 
implementation. The use of the voltage V~ provides flexibility in setting the 
spatial extent of the excitation. 
This flexibility is helpful when testing the 
effects of shifts of attention due to the influence of proximal excitation. 

EXCITATORY FEEDBACK CIRCUITS 
161 
6.5 
DISCRETE (NEAREST-NEIGHBOR) EXCITATION 
A problem with the resistive network for the tracking task is that the distri- 
bution follows an exponentially decaying curve, which means that the amount 
of excitation that is fed back to the nearest neighbors is drastically reduced 
from the amount of excitation that the winning element receives. This smaller 
amount of excitation may not be enough to maintain attention on a stimulus 
in a noisy environment when there are other stimuli within the visual field 
that have similar saliency values. When addressing the problem of maintain- 
ing attention on a single stimulus as it moves across the visual field, it makes 
sense to consider distributing the excitation only to the nearest neighbors of 
the winning element. Such an approach would assume continuous movement 
of the attended stimulus from one pixel to the next. Because this approach 
would only require local communication within the processing array, it is well- 
suited for implementation using analog VLSI circuits. We have implemented 
this form of current distribution by mirroring the excitation current from the 
winning element to the nearest neighbors. We refer to this method of distri- 
bution as discrete excitation, because the communication to surrounding areas 
implements spreading to a discrete number of pixels. The communication is 
determined at design time, according to the number of wires that are used to 
connect each pixel to its neighbors. 
Invn 
l U,~ ~ 
M3~44~ 
aI~j n U~ ~- 
// 
// 
[ M2 
Figure 6.6 Selection circuit with discrete (nearest-neighbor) 
excitation. 

162 
NEUROMORPHIC 
SYSTEMS ENGINEERING 
The schematic for the discrete excitation circuit is shown in Figure 6.6. The 
only change from the local excitation circuit is the addition of two transistors, 
Ms and Mg, that are used to mirror the excitation current from each neighbor- 
ing pixel within the array. (A two-dimensional implementation would require 
four or six additional transistors, depending on the sampling grid.) The source 
voltage, Vdisc, for these mirror transistors is used to control the amount of gain 
in the communication. By adjusting this voltage, the distribution can take on 
the shape of a pulse or a windowed triangle function. The relationship between 
the excitatory current at the winning location and the excitatory currents at 
the neighboring locations is described as: 
Ydisa-- Ydd 
Ino+l 
= Ino-1 = Ino e 
VT 
40 
Discrete (Nearest-Neighbor) Excitation 
< 
30 
~- 
-~ 
~- 
o
~
 
N 
20 
.
~
 
i~ 
ffl 
~,, 
-r 
~0 
Vdisc = 5.01 V 
0 0 
.
.
.
.
 
~ 
' 
' 
~ 
f'~-; 
.
.
.
.
.
.
.
 
1 ~5 
20 
Vdisc = 5.02 V ~./t/ 
iI 
I~' 
Vdisc = 5.03 V ~ 
5 
10 
Position 
Figure 6.7 
Experimental data demonstrating discrete excitation ressults for varying values 
of Vdisc. 
The experimental results shown in Figure 6.7 demonstrate the use of this 
form of excitatory distribution for three values of Vdisc. As in previous experi- 
ments, the current at position 10 was set to 100 nA while a competing current 
was swept at each position to determine the hysteretic window. The curve that 
demonstrates a flat signal across positions 9, 10, and 11 was achieved by setting 

EXCITATORY FEEDBACK CIRCUITS 
163 
Vdisc to 5.03 V. The other two curves demonstrate the circuit's operation when 
Vdi~c is set to 5.02 V and 5.01 V. The gain of the mirroring to the neighboring 
pixels increases as Vdi~ increases. 
The discrete excitation circuit is a good solution to the problem of attending 
a single stimulus that moves across the visual field in a noisy environment. 
Further evidence to support this idea is presented in the tracking experiments 
shown in Section 6.7 of this paper. There are disadvantages to using only this 
approach in a selective attention system. The proximal enhancement for shifts 
to new objects of interest is not possible due to the limited communication that 
is feasible with direct wiring. By the same token, the extent of the excitation 
cannot be altered once the hardware has been fabricated. 
6.6 
CONVOLVED EXCITATION 
The primary strength of the discrete excitation circuit is its ability to excite 
neighboring pixels with very little attenuation. The primary strength of the 
resistive-spreading excitation circuit is its ability to provide a distribution of 
excitation that can be changed. The goal in our selective attention system is 
to include the strengths of both forms of excitation. The desired distribution 
would have little attenuation to the nearest neighbors and then decay with a 
space constant that can be controlled during operation. We have implemented 
an excitation circuit that addresses both goals. By combining the resistive- 
spreading circuit with the discrete excitation circuit, we have developed a cir- 
cuit that computes the convolution of the exponentially decaying signal with 
the three-pixel-wide pulse. The result is a distribution that has some similar 
characteristics to that of a gaussian distribution. There are many parameters 
that can be set to adjust the shape of the distribution, such as the shape of the 
pulse (from a square to a triangle), the space constant of the resistive network, 
and the peak current value for the resistive network. 
A schematic for the convolved excitation circuit is shown in Figure 6.8. The 
resistive network spreads the hysteretic current, Ih, in the same way as the 
resistive-spreading excitation circuit. The distributed current is then mirrored 
to the nearest neighbors, where the gain of the mirror can be adjusted by setting 
the source voltage Valise. Setting the gain of these mirrors is the same as setting 
the coefficients for an FIR (finite impulse response) filter. The total current 
that excites each location can be defined as follows: 
I[n] = I~[n] + bl . L.[n + 1] + bl . L.[n - 1] 
where the coefficient bl is the gain of the mirror, e (vd~sc-v~)/vr, and the func- 
tion, 
Zr[n] = 1no e-~ln-nÂ°l 
is the output of the resistive network that was described in (6.1). The result of 
these relationships is the sum of three shifted exponentials, two of which have 
a variable gain in the summation. 

164 
NEUROMORPHIC SYSTEMS ENGINEERING 
/~ 
~l 
/~_ 
" 
Vn 
M3 
IM 
~ 
2 
/~- 
~/ 
I 
~ 
II 
I~ 
II 
Figure 6.8 
Selection circuit with convolved excitation (combination of resistive and dis- 
crete excitation). 
Experimental results demonstrating the operation of the convolved excita- 
tion selection circuit are shown in Figures 6.9 and 6.10. The value of Vr was 
set to 6.21 V and the remaining parameters were changed to produce different 
shapes of the hysteretic window. The first experiment, shown in Figure 6.9, 
demonstrates how the curve changes as the voltage Vdisc is set to 5.03 V, 5.02 V, 
and 5.01 V. This voltage sets the gain of the filter coefficients. As these co- 
efficients increase, the difference between the peak excitation at the winning 
location and the excitation at distant locations becomes much larger. 
This 
trend is evident in the data for each value of Vdisc. The data in Figure 6.10 
demonstrates the trend as the voltage Vh is set to 0.75 V, 0.73 V, and 0.71 V. 
As the input current to the resistive network, Ih, decreases, the excitation curve 
takes on a smaller range of values. 
The convolved excitation addresses the two issues of attentional processing 
that we have stressed in this paper. The high level of excitation that is mir- 
rored to the nearest neighbors of the selected location enables the tracking of a 

EXCITATORY FEEDBACK 
CIRCUITS 
165 
60 
< ~- 
v 
) 
40 
0 
"I~ 
c"- 
,
~
 
.
~
 
~ 
20 
m 
--r 
Convolved Excitation 
0 
5 
10 
15 
20 
Position 
Figure 6.9 
Experimental data demonstrating convolved excitation results for varying val- 
ues of Vaisc, Vr = 6.21 V and Vh = 0.75 V. 
single stimulus within a noisy environment. The ability to change the charac- 
teristics of the spreading (the space constant) during operation is important for 
the modeling of shifts of attention from one stimulus to the next by enhancing 
proximal locations. The convolved excitation circuit does require a few more 
transistors in each pixel, but the approach achieves the excitation shape that 
is necessary for selective attention in regard to the two goals associated with 
shifts of attention and tracking. A previous attempt to model a gaussian shape 
in focal-plane analog circuits used many more transistors [12]. While our im- 
plementation does not precisely model a gaussian function, it is an acceptable 
compromise for the application that we are addressing. 
6.7 
TRACKING PERFORMANCE 
As stated in Section 6.5, the discrete excitation is advantageous for the ap- 
plication of tracking a single stimulus within a noisy environment. We tested 
this hypothesis with our selective attention circuits by using optical inputs in 
a tracking task. The inputs were two LEDs that were attached to a rotat- 
ing disk. The LEDs were imaged such that they moved across the array of 

166 
NEUROMORPHIC SYSTEMS ENGINEERING 
Convolved Excitation 
v 
~o 40 
-0 
.=_ 
â¢ 
20 
~.~ 
- r  
~ 
. 
O0 
. . . .  
~ 
. . . .  
10 . . . .  
1 '5 . . . .  
201 
Position 
Figure 6.10 
Experimental data demonstrating convolved excitation results for varying 
values of Vh, Vr = 6.21 V and Vdisc = 5.01 V. 
phototransistors as the disk rotated. Attempts were made to set the LEDs to 
intensity levels that were very close in value, according to measurements from 
the phototransistors. The LEDs were spaced such that their images were lo- 
cated approximately 12 pixels apart. The goal of the experiment was to track 
the first stimulus once it entered the field of view of the processing array until 
the first stimulus exited the array. Then the processing array could track the 
second stimulus for the remainder of its movement across the array. In order to 
demonstrate the need for an attentional mechanism, it was necessary to have 
some noise in the system that would cause the selection circuit to jump to the 
second stimulus before the first stimulus had left the array. 
The illumination from the LEDs was not entirely consistent as their angle 
changed with respect to the imaging array. The mismatch of the phototransis- 
tots also contributed to inconsistent measurements for a single stimulus as it 
moved across the array. So, when the second LED entered the field of view, its 
measured intensity level was higher than the measured intensity level of the first 
LED, and the use of local excitation did not enable the continuous tracking of 
the first stimulus. This result is shown in Figure 6.11. The voltage levels shown 

EXCITATORY FEEDBACK CIRCUITS 
167 
3.5 
Local Excitation 
o 
> 
v 
o 
o 
3.0 
2.5 
2.0 
1.5 
Time 
10 ms 
Figure 6.11 
Experimental data demonstrating results of tracking with local excitation, 
Vh = 0.77 V. 
on the vertical axis of the tracking plot represent positions that are attended 
at each moment 
in time. These values are the output of the position-encoding 
circuit. The range of voltage values are between 1.85 V and 3.25 V. Each step 
is approximately 
0.07 V and there are 20 steps, denoting each location in the 
array. The plot shown in Figure 6.11 was generated by using only local exci- 
tation. The value of Vh was 0.77 V, thus setting the excitation current, Ih, to 
51 nA. We also measured the output of the normalization circuits (the input 
to the selection circuits) when the LEDs were stationary at the position where 
the second LED 
just entered the field of view. The peak current value for the 
first LED 
was 83 nA and the peak value for the second LED 
was 102 nA. As 
is evident from the plot, the first LED 
is successfully tracked until the second 
LED 
enters the array at the first position. The output of the selection circuit 
shifts briefly to the second LED 
before returning to the first LED, which has 
a measured intensity level that is higher than the second LED 
with the excep- 
tion of the distracting locations at the edge of the array. The local excitation, 
especially the relatively large value of 51 nA, does cause a few of the positions 
to be skipped as the hysteresis holds the winning position even as the stimulus 
moves to the next position. 

168 
NEUROMORPHIC SYSTEMS ENGINEERING 
3.5 
Discrete (Nearest-Neighb0r) Excitation 
O 
> 
v 
O 
O 
3.0 
2.5 
2.0 
1.5 
Time 
10 ms 
Figure 6.12 
Experimental data demonstrating results of tracking with discrete excitation, 
Vdisc = 5.00 V and V h :- 0.77 V. 
In Figure 6.12 a plot demonstrating the output of the attentive processing 
array is shown, where the only difference from the previous plot is that the 
discrete excitation is enabled with Vd~sc set to a value of 5.00 V. The output 
demonstrates successful tracking of the first stimulus even when there is a 
distracting input within the field of view. Also evident in the data shown, is 
the ability of the system to continuously follow the stimulus; the winner-take- 
all output steps sequentially from one position to the next, without skipping 
any of the positions. Thus, all twenty array positions are represented in the 
data. The desired tracking behavior could also be achieved with a smaller level 
of excitation, so long as the distribution of excitation is able to overcome the 
difference between the presently attended stimulus and the distracting stimulus. 
This system has the flexibility of setting numerous parameters to achieve the 
desired relationships between excitation and input values. 
In comparison, we ran the same test using the resistive-spreading excitation 
while the discrete excitation was turned off. The value of Vr was set to 6.21 V. 
The inputs were exactly the same as in the previous experiment. The plot 
shown in Figure 6.13 demonstrates the output of the system when Vh is set to 
0.81 V. The system is not able to overcome the brief high-intensity level of the 

EXCITATORY 
FEEDBACK 
CIRCUITS 
169 
3.5 
Resistive-Spreading Excitation 
O 
> 
v 
c'- 
O 
.
B
 
O 
3.0 
2.5 
2.0 ~ 
1.5 
Time 
10 ms 
Figure 6.13 
Experimental data demonstrating results of tracking with resistive-spreading 
excitation, Vr = 0.21 V and Tl/'h = 0.81 V. 
second LED 
as it enters the array. So, it is necessary to adjust the parameters 
such that the excitation can overcome the deficit at the neighboring locations 
of the winner as the tracked stimulus moves across the array. The adjustment 
of Vr would not enable continuous tracking of the first stimulus, because, as 
the spreading increases, the excitation at the neighboring locations would not 
increase with respect to more distal locations. The distribution would become 
much flatter. The difference between the excitation at the neighboring loca- 
tions and the excitation at all other locations is the key to successful tracking. 
Therefore, we adjusted the value of Vh to increase the overall excitation levels, 
without changing the shape of the distribution. We were able to overcome the 
effect of the distracting input for successful tracking of the first stimulus, as 
shown in Figure 6.14. The lowest value of Vh that produced this result was 
0.92 V, and for this combination of parameter settings, Ino was measured to 
be 114 nA. The problem with this approach is evident in the graph. The in- 
creased overall level of excitation created similar behavior as that of the local 
excitation, where the movement of the selected position did not progress con- 
tinuously across the array. Also, because the excitation currents were so high, 
the second stimulus was not able to overcome the base hysteretic current level. 

170 
NEUROMORPHIC SYSTEMS ENGINEERING 
3.5 
Resistive-Spreading Excitation 
o 
> 
v 
o 
. m  
o 
3.0 
2.5 
2.0 
1.5 
Time 
10 ms 
Figure 6.14 
Experimental data demonstrating results of tracking with resistive-spreading 
excitation, Vr = 6.21 V and Vh = 0.92 V. 
When the first LED exited the field of view, the output of the winner-take-all 
remained at the edge of the array instead of tracking the single stimulus that 
was still within the field of view. While there exist input scenarios for which 
the resistive-spreading excitation would be able to track a moving stimulus 
successfully, we feel that the discrete excitation circuit provides a more robust 
approach to continuous tracking. The data presented here serves as an example 
of the performance of these systems. 
We also took measurements in an attempt to determine the timing limita- 
tions of these circuits for tracking a fast stimulus. It is possible that, if the 
selection circuits could not switch quickly enough to match the stimulus move- 
ment, the discrete excitation would not necessarily perform better than the 
resistive-spreading excitation in the tracking task. The discrete excitation re- 
lies on continuous movement from one pixel to the next. If the input currents, 
or the selection circuit output currents, were not able to keep up with the 
movement of the stimulus, the stimulus might be forced to compete with other 
stimuli without the advantage provided by the excitation. 
We used similar 
inputs as in the previous tracking tests. Only the discrete excitation was en- 
abled. The highest measured speed of tracking was beyond 1500 pixels/second. 

EXCITATORY FEEDBACK CIRCUITS 
171 
Limitations in the testing setup prohibited further increases in speed for the 
stimulus movement. Nevertheless, these measurements indicate that the circuit 
time constants will not be a concern for real-world tracking problems. 
6.8 
CONCLUSION 
We have presented several circuits that constitute essential building blocks for 
an analog VLSI selective attention system. The selection of a region of interest 
must allow for stimulus movement with respect to the imaging array, and the 
shifts of attention from one object to the next should demonstrate a prefer- 
ence to stimuli that are within a proximal location of the presently attended 
stimulus. 
The distributed excitation, centered at the selected location in a 
winner-take-all computation, achieves the desired performance. The shape of 
the excitation determines the efficacy of the attentional processing, for model- 
ing shifts of attention and for tracking a stimulus as it moves across the visual 
field. Four different excitation circuits were presented. The local excitation 
circuit is a building block upon which the remaining three circuits were built. 
The resistive-spreading circuit is advantageous in its compact implementation 
and the adaptability of the distribution of excitation to different spatial ex- 
tents. The discrete excitation circuit demonstrates better performance for the 
tracking task, because of the low-attenuation communication to the nearest 
neighbors. The discrete excitation does not have the capability of adapting 
the spatial extent of the distribution of excitation, however, which nmkes this 
circuit a poor model for shifts of attention to new objects. 
The combination of the resistive-spreading excitation and the discrete excita- 
tion provides a solution that addresses the problems associated with modeling 
shifts of attention and attentive tracking. The convolution of the exponen- 
tially decaying signal from the resistive network and the three-pixel-wide pulse 
from the discrete mirroring provides a shape that mimics the characteristics of a 
gaussian distribution. The convolved excitation provides a feasible analog VLSI 
implementation that performs favorably as a model for selective attention. The 
ability to change several parameters also offers flexibility in defining the shape 
of the excitation. All of the circuits that we have presented can be extended to 
a two-dimensional architecture. We are currently designing a two-dimensional 
selective attention system that utilizes these excitation circuits. 
The larger scope of our work includes the modeling of other aspects of selec- 
tive attention. Inhibition-of-return is a characteristic of visual attention that 
causes a location to be inhibited once it is attended, until a brief amount of time 
has elapsed. We have already built systems that combine excitation circuits 
with inhibition circuits to create complex behavior in analog VLSI visual at- 
tention systems [18]. The use of excitation and inhibition together in a visual 
attention system creates an automatic scanning mechanism as the attention 
moves from one location of the visual field to the next. A variety of behaviors 
can be achieved by adjusting the extent of the excitation and inhibition, or 
by adjusting the timing of the inhibition. The excitation circuits presented in 

1"/2 
NEUROMORPHIC SYSTEMS ENGINEERING 
this paper will enhance the performance of such systems, and the use of dif- 
ferent distributions will offer interesting alternatives in the implementation of 
inhibition. We are also beginning to address the issues surrounding compu- 
tation based on objects within the saliency map, as opposed to a pixel-based 
computation. The goal is to develop a hardware model, operating in real time, 
that we can compare to the behavior of the human visual attention system, as 
observed in psychophysical experiments. 
Acknowledgments 
The authors would like to acknowledge David Klein for his work with the analysis and 
simulation of current-mode resistive networks in support of this project. We would 
also like to thank Tim Horiuchi for countless technical discussions and for his careful 
editing of this document. This research is supported in part by the Georgia Tech 
Research Institute. 
References 
[1] Y. Aloimonos. Active Perception. Lawrence Erlbaum Associates, Hillsdale, 
New Jersey, 1993. 
[2] J. R. Bergen and B. Julesz. 
Parallel versus serial processing in rapid 
pattern discrimination. Nature, 303, June 1983. 
[3] K. A. Boahen and A. G. Andreou. A contrast sensitive silicon retina with 
reciprocal synapses. Advances in Neural Information Processing Systems, 
4:764-772, 1992. 
[4] C. L. Colby. The neuroanatomy and neurophysiology of attention. Journal 
of Child Neurology, 6:90-118, 1991. 
[5] T. Delbriick. Silicon retina with correlation-based velocity-tuned pixels. 
IEEE Transactions on Neural Networks, 4(3):529-541, May 1993. 
[6] S. P. DeWeerth. Converting spatially encoded sensory information to mo- 
tor signals using analog VLSI circuits. Autonomous Robots, 2:93-104, 1995. 
[7] S. P. DeWeerth and T. G. Morris. Analog VLSI circuits for primitive 
sensory attention. In Proceedings of the IEEE International Symposium 
on Circuits and Systems, volume 6, pages 507-510, 1994. 
[8] S. P. DeWeerth and T. G. Morris. CMOS current mode winner-take-all 
circuit with distributed hysteresis. Electronics Letters, 31(13):1051-1053, 
1995. 
[9] B. Gilbert. A monolithic 16-channel analog array normalizer. IEEE Jour- 
nal of Solid State Circuits, SC-19(6):956-963, December 1984. 
[10] J. G. Harris. Analog Models for Early Vision. 
PhD thesis, California 
Institute of Technology, 1991. 
[11] T. K. Horiuchi, B. Bishofberger, and C. Koch. An analog VLSI saccadic eye 
movement system. In Cowan, Tesauro, and Alspector, editors, Advances in 

EXCITATORY FEEDBACK CIRCUITS 
173 
Neural Information Processing Systems 6, pages 582-589, San Francisco, 
1994. Morgan Kaufman. 
[12] H. Kobayashi, J. L. White, and A. A. Abidi. An active resistor network 
for gaussian filtering of images. [EEE Journal of Solid-State Circuits, 
26(5):738-748, May 1991. 
[13] C. Koch and S. Ullman. Shifts in selective visual attention: Towards the 
underlying neural circuitry. Human Neurobiology, 4:219-227, 1985. 
[141 N. Kumar, P. O. Pouliquen, and A. G. Andreou. Device mismatch limita- 
tions on the performance of an associative memory system. In Proceedings 
of the 36 th Midwest Symposium on Circuits and Systems, volume 1, pages 
570-573, 1993. 
[15] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead. Winner- 
take-all networks of o(n) complexity. Technical Report CS-TR-88-21, Com- 
puter Science Department, California Institute of Technology, Pasadena, 
CA, 1989. 
[16] M. Mahowald. VLSI Analogs of Neuronal Visual Processing: a Synthesis of 
Form and Function. Computation and neural systems, California Institute 
of Technology, 1992. 
[17] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
[18] T. G. Morris and S. P. DeWeerth. Analog VLSI circuits for covert attem 
tional shifts. In Proceedings of the Fifth International Conference on Mi- 
croelectronics for Neural Networks and Fuzzy Systems, pages 30-37, Lau- 
sanne, Switzerland, February 1996. IEEE Computer Society Press. 
[19] T. G. Morris, D. M. Wilson, and S. P. DeWeerth. Analog VLSI circuits 
for manufacturing inspection. In Proceedings of the 16th Conference on 
Advanced Research in VLSI, pages 241-255, Los Alamitos, CA, 1995. IEEE 
Computer Society Press. 
[20] E. Niebur and C. Koch. A model for the neuronal implementation of 
selectiave visual attention based on temporal correlation among neurons. 
Journal of Computational Neuroscience, 1:141-158, 1994. 
[21] E. Niebur and C. Koch. Modeling the 'where' visual pathway. In Proceed- 
ings of 2nd Joint Symposium on Neural Computation, volume 5. Caltech- 
UCSD Institute for Neural Computation, 1995. 
[22] B. Olshausen, C. Anderson, and D. van Essen. A neural model of visual 
attention and invariant pattern recognition. 
CNS Memo 18, August 6 
1992. 
[23] M. I. Posner and S. E. Petersen. The attention system of the human brain. 
Annual Review of Neuroscience, 13:25-42, 1990. 
[24] R. W. Remington and L. Pierce. Moving attention: Evidence for time- 
invariant shifts of visual selection attention. Perception ~ Psychophysics, 
35:393-399, 1984. 

174 
NEUROMORPHIC SYSTEMS ENGINEERING 
[25] G. Sperling and E. Weichselgartner. Episodic theory of the dynamics of 
spatial attention. Psychological Review, 3:503 532, 1995. 
[26] E. Vittoz and X. Arreguit. Linear networks based on transistors. Elec- 
tronics Letters, 29(3):297-299, February 1993. 
[27] S. Yantis. Control of visual attention. In H. Pashler, editor, Control of 
Visual Attention. University College Press, London, in press. 

7 
FLOATING-GATE CIRCUITS FOR 
ADAPTATION OF SACCADIC EYE 
MOVEMENT ACCURACY 
Timothy K. Horiuchi and Christof Koch 
Computation and Neural Systems Program, 
California Institute of Technology, 
Pasadena, CA 91125 
timmer@klab.caltech.edu 
7.1 
INTRODUCTION 
The most common eye movements in primates are the quick reorienting move- 
ments known as saccades. Our eyes often reach speeds up to 750 degs/s during 
a saccade which severely impairs our visual acuity. It is therefore important 
to minimize the time during which the eyes are moving. While typical human 
saccades have a duration of 40ms to 150 ms, changes in the optics, the oculo- 
motor plant, or the underlying neural circuitry can cause deficits which delay 
optimal viewing conditions. 
There have been many types of adaptation behavior identified in the primate 
oculomotor system in response to different induced deficits. For example, Op- 
tican and Robinson [11] showed that weakening of the horizontal recti muscles 
in the rhesus monkey initially caused saccades which fell short and exhibited 
post-saccadic drift of the eyeball. Recovery from this type of damage, which 
affects all saccades in a given direction, requires about 3-5 days. In contrast to 
this long adaptation period, which involves hundreds of thousands of saccades, 
experiments where saccadic targets are moved a short distance during the sac- 
cade, require only hundreds of trials for the adaptation to reach steady-state. 
This type of visually-induced modification of saccade amplitude is known as 
short-term adaptation. Experiments by Frens and van Opstal [6] show this 
adaptation to be confined to a limited range of saccade vectors around the 
adaptation target. 

176 
NEUROMORPHIC SYSTEMS ENGINEERING 
Our laboratory is involved in building a hardware model of the primate 
oculomotor system [9] using analog VLSI circuitry. The model oculomotor 
plant simulates linear dynamics and provides a good foundation upon which 
to build biologically-realistic eye movement systems. The current system can 
be triggered using both visual and auditory stimuli [8] and begins to model 
the convergence of multi-modal spatial information at the level of the superior 
colliculus. Three aspects of this modelling system that have become important 
are the compensation for non-linearities, the need for self-calibration, and on- 
chip storage of these parameters. 
The intermingling of memory and computation is an important and pow- 
erful aspect of neural architectures which has not yet been well exploited in 
neuromorphic VLSI designs. Smaller designs have been manageable by the use 
of external sources of parameters or by array structures which share global 
parameters. With the advent of large, multi-chip, neural systems, however, 
the automatic selection, storage, and maintenance of these parameters will 
become an unavoidable issue as it is in biological systems. The majority of 
circuit designs which have attempted to use on-chip storage of parameters have 
used digital RAM or externally-refreshed, capacitive storage, both of which are 
generally bulky and low-precision. Until recently, the use of floating-gates (a 
MOS transistor gate completely isolated from the circuit by silicon dioxide) 
required the use of ultra-violet radiation or bidirectional tunneling processes 
which have also been fraught with difficulties, impeding their widespread use. 
The development of a complementary strategy of tunneling and hot-electron 
injection [28] in a commercially-available BiCMOS process has alleviated some 
of these difficulties. 
In previous work, we demonstrated the use of floating-gate devices in a model 
of the saccadic burst generator to reduce post-saccadic drift using visual motion 
as an error signal [10]. In this paper we present a chip which uses floating-gate 
structures to store a mapping of retinal position to motor command voltage. 
In the beginning of the next section we will discuss the architecture of the chip, 
in section 7.3 we present the circuits and their behavior, and in section 7.4 we 
discuss the chip's performance within the training system. 
7.2 
VECTOR-SPECIFIC ADAPTATION 
In an experiment where human subjects are performing saccades from a fix- 
ation point to a visible target, if the target of a specific retinotopic position 
consistently moves to a new location during the saccade, l~rens and van Opstal 
(1994) have shown results indicating that the adaptation timecourse to learn 
the offset is short (requiring only a few hundred presentations) and that the 
adaptation is confined to a limited range of saccade vectors around the tar- 
get [6]. This type of learning can be explained by a mapping similar to that of 
a look-up table. 
In the previous implementation of our analog VLSI-based saccadic sys- 
tem [9], visual stimuli were mapped linearly from pixel position to motor com- 

SACCADIC 
EYE MOVEMENT 
177 
A 
Centroid 
Output 
Voltage 
External 
Training 
Input 
Figure "/.1 System Block Diagram: This chip consists of an array of 32 pixels which consist 
of an adaptive photoreceptor (P), a temporal derivative circuit (TD), a centroid circuit (C), 
a floating-gate circuit (FG) which provides reference voltages to the centroid circuit, and a 
control circuit (U/D = "up/down") for training the floating-gate. 
mand in a functional model of the deep layers of superior colliculus. Any 
non-linearities in the optics, photoreceptor triggering circuit, burst generator, 
or motor plant would create errors in proper programming of the saccade. We 
have modified the visual-triggering circuit [9] (also in figure 7.2) to use the 
output of a floating-gate circuit to determine the proper motor command for 
each pixel. 
As shown in Figure 7.1, an array of adaptive photoreceptor circuits (P) are 
used to drive a temporal-derivative circuit (TD), activating regions in an image 
where the intensity is changing. These temporal derivative signals trigger three 
circuits: one which activates a slowly decaying memory of which units have been 
active (U/D), another which drives a centroid circuit (C) to map the pixel's 
position to a motor command voltage, and finally a triggering circuit which 
compares the total activity on the chip to a threshold (not shown). The trigger 
circuit provides an output signal from the chip, indicating that something has 
occurred in the image and that the centroid output information is "valid". The 
centroid circuits [4] require reference voltages (motor command voltages) at 

178 
NEUROMORPHIC SYSTEMS ENGINEERING 
photo 
bias 
~-- 
1:6 ~ 
~, 
~ 
I bump I rip 
_ low,ass 
- 
)hoto 
To Up/Down control circuit 
& triggering circuit 
Motor Command 
voltage (from floating-gate 
circuit) 
Centroid Output Voltage 
(common to all pixels) 
Figure 7.2 
Temporal Triggering Circuit (P+TD+C): On the far left, the adaptive pho- 
toreceptor circuit amplifies temporal change in the light intensity while slowly adapting to 
the mean light level. The temporal-derivative (TD) circuit acts as a high-pass filter by mea- 
suring the difference in voltage between the original photoreceptor value and a low-passed 
version of it. The signal is then full-wave rectified and mirrored to the U/D, centroid, and 
thresholding circuits. The centroid circuit (on the right), operates as a follower powered by 
the current from the temporal derivative circuit. The motor command reference voltage is 
received from the floating-gate amplifier circuit (FG). 
each pixel which represent the saccade vector required to center the stimulus 
on the center of the array. 
In previous versions of this visually-based, triggering circuit [9], the motor 
command voltages were provided by a resistive line running across the array. 
Each end of the resistive line was held at a different voltage, providing each pixel 
in the array with a unique voltage reference, which changed linearly across the 
array. In contrast, the pixels in this new system are provided with the output 
voltage of a floating-gate circuit, each of which can, in principle, be set to 
arbitrary values, making it similar to a programmable, look-up table. 
The training input to the system is a global signal indicating whether the 
system's output was too high or too low. Pixel locations which contributed 
to the output remain active for a short amount of time (about 3 sec) via the 
U/D circuit. When the training signal becomes active, after evaluating the 
centroid output voltage, only those units which contributed to the output are 
trained in the appropriate direction. Since the triggering stimuli may activate a 
neighborhood of pixels, the learning is similar to Kohonen's stochastic learning 
algorithm where the topology of the network is preserved by training a node 
and its neighborhood at the same time. This technique has been explored in 
software in the context of saccadic learning by both Ritter et al. [13] and by 
Rao and Ballard [12]. 
The training system consists of a workstation which flashes visual stimuli 
(bars) at different locations on its monitor. The chip, with a lens, is positioned 
to image the stimuli on its photoreceptor array. The centroid output voltage is 
measured after each flash using a GPIB-equipped 
(General Purpose Interface 

SACCADIC EYE MOVEMENT 
179 
Bus) oscilloscope. Each stimulus position on the monitor is assigned a target 
centroid output value. If the measured value is lower than the target value, the 
training input voltage, (driven by a GPIB-equipped voltage source) is lowered 
to a pre-determined training voltage for a fixed amount of time to increase that 
stored value by increasing the tunneling rate. Similarly, if the measured value 
is higher than the target value, the training input is raised. After repeated 
trials, a target function can be learned to a level of accuracy limited primarily 
by the system noise. 
7.3 
CIRCUITS 
The implementation of the architecture described above was fabricated on a 
TinyChip (2.25mm x 2.22mm) using a 2.0 #m, n-well, double-poly, BiCMOS 
process. The chip we discuss in this paper is a one-dimensional array of 32 
pixel elements. 
Figure 7.2 shows the combined circuit schematics for the adaptive photore- 
ceptor (P) (left), the temporal-derivative (TD) (middle), and the centroid cir- 
cuit (C) (right). The adaptive photoreceptor [2] is a high-gain photoreceptor 
circuit which slowly adapts to the average light level to prevent saturation. The 
temporal derivative circuit combines a lowpass filter with a "bump" circuit [1] 
to signal the absolute-value of the temporal-derivative. The centroid circuit [4] 
computes the weighted-average, motor command voltage. Since every cell in 
the array would connect to an n-type mirror, the gray box in the figure denotes 
the use of a single, common mirror on the edge of the array to reduce capaci- 
tance on the output node. An amplifying ratio of 6 to 1 was used on the mirror 
for inverting the bump current to cancel the tail currents of the differential 
pair. Overall, these circuits map the retinotopic location of temporal change 
to a motor command voltage. 
The floating-gate circuit (figure 7.3), is a modification of the circuit used 
by Hasler et al. [28] to train a 2x2 array of floating-gate synaptic elements. 
A tunneling process is used to remove electrons from the floating node and a 
hot-electron injection process is used to put electrons onto the floating node. 
The tunneling current is controlled by manipulating the difference in voltage 
between the floating-node and the high-voltage tunneling line. Larger voltage 
differences produce larger tunneling rates. Injection of electrons is performed in 
an n-type transistor fabricated in the Pbase layer provided for the construction 
of bipolar transistors. The threshold voltage for this type of transistor is near 
6 volts, which allows the gate to capture high-energy electrons flowing through 
the drain while the transistor is still operating in the subthreshold. Since the 
injection current is the product of the injection efficiency (controlled by the 
drain voltage) and the source current, injection current can be adjusted by 
manipulating the source current in the Pbase transistor. 
The floating-gate circuit (Figure 7.3) uses two Pbase transistors, one used 
as an electron injector (PB1) and the other used as the current source for 
the amplifier (PB2). Since PB2 is only setting the amplifier current (and not 

180 
NEUROMORPHIC SYSTEMS ENGINEERING 
Up/Down 
control input 
Vdl __ 
Pbase 
~ 
transistor 
Vsl 
PB1 
cascode 
~.~ 
reference 
N1 
Vd2 
Tunneling 
Voltage (~26 v) 
PB2 
Pbase 
__ 
transistor 
amplifier 
upper limit 
Voutput 
> 
amplifier 
bias 
amplifier 
lower limit 
Figure 
7.3 
Floating-gate amplifier circuit (FG): The floating node defines a subthreshold 
current in transistor PB2 which is mirrored and used in a high-gain amplifier stage which 
has variable output limits. Cascode transistor N1 defines PB2's drain voltage to prevent 
hot-electron injection. 
Nodes Vdl, Vsl, and the high-voltage tunneling node are fixed 
global values which define an equilibrium floating-gate value, and a decay rate towards this 
value. Modification of the floating-gate voltage is performed by capacitively moving the 
floating-gate up or down transiently to either increase injection or increase tunneling. 
injecting), its drain voltage Vd2 can be set to a low voltage allowing the upper 
limit of the amplifier's output range to be fairly large. Modification of the 
floating-gate charge is performed by transiently increasing the rate of either the 
tunneling or injection. This is performed by capacitively raising or lowering the 
floating-gate using the Up/Down control input. Raising the floating-node both 
increases the source current in PB1 and reduces the floating-gate to tunneling 
voltage. Likewise, lowering the floating-node both increases the floating-node 
to tunneling voltage and decreases the source current in PB1. 
As in the system described by Hasler et al. [28], the tunneling and hot- 
electron injection currents are both active, but extremely low and in opposite 
directions. Since both processes operate in a negative-feedback fashion (e.g. 
the tunneling process raises the floating-gate which tends to reduce the rate of 
tunneling), the system reaches an equilibrium value when the tunneling current 
equals the injection current. When the floating-gate voltage is larger than the 
equilibrium voltage, the hot-electron injection current dominates the tunneling 
current and the floating-gate voltage drops. Conversely, when the floating-gate 
voltage is lower than the equilibrium voltage, tunneling dominates and the 
voltage rises. 
While this technique avoids high-voltage switching circuits, it suffers (or 
possibly benefits) from the eventual loss of stored information as the floating- 

SACCADIC EYE MOVEMENT 
181 
Floating-gate 
Center 
Reference 
weakbias 
_ 
External 
----_ 
Training 
Input Voltage 
UpDown 
Control 
"~ 
strong 
bias 
P11  
From 
temporal 
derivative 
mirror 
~ 
~_1-~ Leak 
bias 
_
_
 
_
_
 
Figure 7.4 
Up/Down learning control circuit (U/D): This circuit consists of two competing 
followers, a weak follower carrying the center reference voltage and a stronger follower which 
receives the training voltage from off-chip. When a given pixel in the array generates a pulse 
of current in the TD circuit, this current is mirrored onto transistor PI, charging the capacitor 
node up towards Vdd. A small leak current discharges the capacitor slowly. This node acts 
as a switch to turn on the strong ampifier to drive the floating-gate control node towards 
the globally-received, training voltage. In this fashion, only those circuits which participated 
in generating the output centroid voltage receive the training signal. 
gate decays back to its equilibrium voltage. This decay rate, however, can be set 
to be extremely slow by using a low Vdl (transistor PB1) and a low tunneling 
voltage. Since the tunneling and injection parameters are kept constant, the 
equilibrium voltage should not depend on the stored value and the memory 
should decay towards an equilibrium determined solely by these parameters. 
Memory decay tests of our floating-gates exhibited extremely low, tunneling- 
dominant rates (less than 0.07 mV/hour), while the injection-dominant rates 
showed a decay of about 1.0 mV/hour. For more details of the physics of these 
floating-gate devices, see Hasler et al. [28] and Diorio et al. [5] 
The "learning" can also be turned off by bringing Vdl, Vsl, and the tun- 
neling voltage down to zero. Unfortunately, the absolute voltage level of all 
the floating-gates will be DC-shifted downwards as the tunneling voltage drops 
due to capacitive coupling. This shift can easily be countered by increasing the 
U/D circuit's center reference voltage until the values have returned to their 
trained state. This step, however, may introduce a DC shift error since it is 
done manually. 
To train the chip for a certain mapping, pixels are stimulated and the resul- 
tant centroid output voltage is determined to be either too high, too low, or 
inside a window of tolerance around the target value. Since the pixels which 
contributed to the output value are the ones that need to be modified, some 

182 
NEUROMORPHIC SYSTEMS ENGINEERING 
Photoreceptor 
~ 
Trigger Output Voltage 
(digital) 
-0.01 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
I 
0.00 
0.01 
0.02 
0.03 
0.04 
0.05 
0.06 
0,07 
0.08 
0.09 
0.10 
time (sec) 
Figure 7.5 
Top trace: Photoreceptor voltage, Middle trace: Centroid output voltage 
(analog), Bottom trace: trigger signal (digital). The photoreceptor output voltage jumps 
from 0.96 volts to 1.30 volts during the flash of the stimulus. The oscillation riding on the 
step response of the photoreceptor is due to the flicker induced by the monitor. The centroid 
circuit also shows some O0 Hz noise, resulting from feed-through of noise from the high-gain 
floating-gate circuits. 
mechanism is required to remember those pixels. The Up/Down circuit shown 
in Figure 7.4 performs this function by storing charge at each pixel location that 
contributed to the centroid output. If the pixel has not been active, the circuit 
holds the output to a global reference voltage. If the pixel was just used to 
drive the centroid output, the U/D circuit drives the output to an externally- 
provided voltage level for approximately five seconds (with our current leak 
settings). This external signal is the training voltage which is used to increase 
or decrease the floating-gate voltages at those locations which contributed to 
the previous output. 
Figure 7.5 shows some of the relevant signals during a pulse of light on 
the array. Although not visible, the centroid output rises to a stable value 
approximately 2ms after the beginning of the temporal change. 
The data presented in this paper was taken using a tunneling voltage of 
about 26 volts, Vdl = 3.1 volts~ Vsl = 0.2 volts with the floating-gate values 
centered around 5.5 volts. The Up/Down control line was moved from 4.0 
to 7.0 volts for increased hot-electron injection and from 4.0 to 0.0 volts for 
increased tunneling. 
The coupling coefficient between the U/D control line 

SACCADIC EYE MOVEMENT 
183 
4 
~" 
2 
Z 
0 ~ 
0 
~1 
m 
4 
~ 
2 
o 
< 
.~ 
4 
~ 
~- 
> 
0- 
< 
-2 
3 
0 
0 câ¢â¢Bar 
Width 
1 line-w~dth 
Bar Width 
= 3 line-widths 
o 
o 
o 
~ 
= 5 line-widths 
--e 
4 
5 
6 
7 
8 
9 
l0 
PIXEL LOCATION ON IMAGER 
Figure 7.6 
When a bar of one line-width (defined by the graphics board) is flashed at 
the chip, it stimulates a single photoreceptor as shown in the top plot and the one pixel is 
trained for a mean duration of 2.75 seconds. This timing is primarily determined by the leak 
bias (see Figure 7.4). When the bar is widened to 3 line-widths (middle plot), 2 adjacent 
pixels are stimulated and they are trained together in the same direction. A bar width of 
5 line-widths stimlates 3 pixels as shown in the bottom plot. In the multi-scale training 
regime, all three types of bars were used randomly interleaved in the training set. The bar 
of 5 line-widths was also used to generate Figure 7.10. These plots show the results of 
measuring the mean time each pixel spent training for bars of different widths flashed at a 
position on the monitor near pixel ~7. The mean was computed over 7 trials. 
and the floating-gate was measured to be about 0.6. In order to scan off the 
floating-gate values, we operated the chip using a Vdd of 8 volts. 
7.4 
SYSTEM 
PERFORMANCE 
In training, the chip is aimed at a computer monitor which flashes vertical bars 
at different positions in the field of view. While the current chip has only 32 
pixels, the training system flashes stimuli at the maximum line resolution of the 
screen. Our current optics configuration allows for approximately 75 different 
locations at which we can stimulate the array of 32 pixels. This is done both 
to map the subpixel behavior as the stimulus moves from one pixel location to 
the next and to train the pixels individually rather than as groups of pixels. 
In real-world situations, however, the pixels will be activated in groups and 
the subsequent output will be an appropriate average of the individual pixel 
values. Although training the system with large stimuli does work, the training 

184 
NEUROMORPHIC SYSTEMS ENGINEERING 
0 > 
2.505 
2.504 
2.503 
2.502 
"flat15f.trg" -- 
"flat 15f.dat" ~ 
2.501 -- - 
2.499 -- - 
2.501 
2.5 
2.499 
2.498 
2.497 
2.496 
2.495 
10 
20 
30 
49 
50 
60 
70 
80 
STIMULUS POSITION 
Figure 7.7 
Flat Target Function - In this case, all stimulus positions were trained to lie 
at 2.500 volts. This plot shows the performance of the chip after approximately 20,000 pre- 
sentations spread over 75 positions. The floating-gate outputs were initially spread between 
2.4 and 2.6 volts. After training, the centroid array was "queried" sequentially from left 
to right five times without training. The error bars represent one standard deviation. The 
training procedure continued to modify the floating-gate until the voltage was within i mV 
of the target voltage. 
time dramatically increases since the training must rely on the uniform statistics 
of the training set to sort the proper values out. The training stimulus size also 
sets the minimum size for which the array will report the proper value. For 
this reason it is important to also train at the appropriate resolution. A multi- 
resolution training schedule may be the best strategy since training can occur 
in parallel, yet the smaller stimuli can fill in the details at each position. The 
training positions are chosen by shuffling a list of positions and selecting them 
from the list without replacement. Once the list is exhausted, the whole list is 
reshuffled. This sets an upper bound on the inter-example training time and 
guarantees a uniform distribution. 
After training, the array can be "probed" with either a bar of one line-width 
or a bar of 5 line-widths to stimulate output values. The one line-width bar will 
stimulate individual pixels and the 5 line-width bar will stimulate the average 
of a group of 3 pixels. (See Figure 7.6) The effects of averaging can be seen 
in Figure 7.10 for the case of the sinewave mapping, which is a particularly 
difficult case to learn, since individual pixels cannot satisfy the wide range of 
values occuring on a steep part of the function. 

E 
2.6 
2,55 
2.5 
2.45 
2.4 Â¸ 
SACCADIC EYE MOVEMENT 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
"slope1 e.trg" -- 
"slopele.dat" ~ 
~ 
~* 
~ . ~  
I 
I 
,'o 
20 
; 
4o 
20 
; 
Stimulus Position 
185 
80 
Figure 7.8 
Linear Target Function - This function most closely represents a realistic sen- 
sorimotor mapping function for triggering saccades to a visual target. The training and 
testing procedure is the same as in the previous graph. The error bars represent one stan- 
dard deviation. 
The first test of system level operation we discuss is an experiment in which 
we attempt to load a fiat target function. With this function it is easiest to see 
the accuracy with which the system can learn a specific value. Figure 7.7 shows 
the results after extended training. From initial conditions where the floating: 
gate amplifier outputs were sitting at fairly random voltages, the system was 
presented approximately 20,000 examples at 75 different stimulus locations (ap- 
proximately 625 examples per pixel) and then the system was probed at the 
75 stimulus locations to evaluate the mapping. Noise in the chip and in the 
testing system contribute to the variations seen in repeated trials. It should 
be noted that the floating-gate amplifiers are non-linear and the highest gain 
occurs in the center of the range. Since the target value for the flat function in 
figure 7.7 is in the center of the range, we expect the largest reporting variance 
here due to noise. The error tolerance of the training system for this mapping 
was 1 inV. 
The linear target function (figure 7.8) is the mapping which was previously 
used to map retinal position to motor command, where 2.60 volts represented 
a full-scale saccade to the right and 2.40 volts represented a full-scale saccade 
to the left. In this case and in the following mappings, the error tolerance for 
learning was 2.5 mV. 

186 
NEUROMORPHIC SYSTEMS ENGINEERING 
> 
0 
2.6 
2.55 
2.5 
2.45 
"sine5d.trg" -- 
"sine5d.dat" ~ 
~ 
~ 
~ 
{ 
~ 
Â¢,Â¢~ 
~ 
,~ 
2.4 
0 
10 
20 
30 
40 
70 
80 
Stimulus Position 
Figure 7.9 
Sinewave Target Function - In this case, the target values followed a sinewave. 
Photoreceptor granularity is evident by the "staircasing" seen in the plot. Stimulus locations 
where the flashed bar occurs on the boundary of two pixels exhibit large variations in output 
voltage due to the narrow (one line-width) stimuli being used. Figure 7.10 shows the same 
pattern being probed with a much wider stimulus (three line-widths). The training and 
testing procedure is the same as in the previous graphs. 
The error bars represent one 
standard deviation. 
In order to challenge the system we also tested a sinewave target function 
(figures 7.9 and 7.10) whose spatial derivative was difficult to match with the 
resolution of the current system. The expected final value in this situation 
when training with a uniform distribution of examples and balanced step sizes 
is the average of the different target values associated with the same pixel. 
This behavior is seen most clearly in figure 7.9. Convergence of this mapping 
function takes much longer due to the statistical nature of the equilibrium and 
the final value is not very stable since nearly all the training examples drive 
the pixel away from its current value. 
During the testing process, we determined that modifications should be made 
to reduce the gain of the floating-gate output amplifier. The measured DC gain 
from the floating-gate to the output of the amplifier was found to be approxi- 
mately 60. This created many problems with noise, particularly at 60 Hz due 
to electrical noise in the laboratory and the 60 Hz light flicker coming from 
the monitor. We partially solved this problem by using a considerably smaller 
output voltage range (2.4 volts to 2.6 volts) to push the amplifier's output tran- 
sistors partially out of saturation for the subthreshold current regime. This had 

SACCADIC EYE MOVEMENT 
187 
0 
2.6 
2.55 
2.5 
2.45 
2.4 
i 
i 
"sine5d-b.trg" -- 
"sine5d-b.dat" ~ 
_ 
~ 
2~0 
3~0 
~ 
~ 
~ 
~ 
10 
40 
50 
60 
70 
80 
Stimulus Position 
Figure 7.10 
Sinewave Target Function - In this case, the evaluation of the pattern in 
Figure 7.9 was performed using a bar which spanned 3 pixels. The training and testing 
procedure is the same as in the previous graphs. The error bars represent one standard 
deviation. 
the effect of reducing the gain down to about 2.0, but left a very small signal 
range with which to work. 
7.5 
DISCUSSION 
We have successfully fabricated and tested a trainable array of floating-gate 
memories whose operation and modification is integrally related to a specific 
visual task. By storing information locally about which units contributed to a 
computation, the distribution of the training signal back through the system 
has been made simpler. The hardware approach to this problem of delayed 
assignment-of-error may provide a valuable testbed in which to consider how 
this problem is solved in biological systems. 
The neurobiological substrate for this adaptation is still unknown. Both the 
superior colliculus and the frontal eye fields are attractive areas for investigation 
of this adaptation due to their vector-specific organization for driving saccadic 
eye movements. While both areas are capable of driving of saccadic eye move- 
ments, the frontal eye fields are implicated in the generation of "volitional" 
saccades and the superior colliculus has been implicated in the generation of 
reflexive, visually-guided saccades. Experiments by Deubel [3] indicate that 
there are context-dependent differences in vector-specific, short-term adapta- 
tion. Adaptation performed during reflexive, visually-guided saccades was not 

188 
NEUROMORPHIC SYSTEMS ENGINEERING 
expressed during volitionally-driven saccades. The converse has also been found 
to be true. Prens and van Opstal [6] also demonstrated the transfer of vector- 
specific adaptation to saccades triggered by auditory cues. These experiments 
together point to the interpretation that the adaptation is occurring at a stage 
after integration of these different sensory modalities, but before the parallel 
streams of information from the superior colliculus and frontal eye fields have 
converged. Following these constraints, it is our hope to also demonstrate this 
transfer of adaptation with our VLSI-based auditory localization system. 
The investigation of neural information processing architectures in analog 
VLSI can provide insight into the issues that biological nervous systems face. 
Analog VLSI architectures share many of the advantageous properties with 
neural systems such as speed, space-efficiency, and lower power consumption. 
In addition, analog VLSI must face similar constraints such as real-world noise, 
component variability or failure, and interconnection limitations. 
With the 
development of reliable floating-gate circuits, the powerful ability of neural 
systems to modify and store their parameters locally can finally be realized in 
analog VLSI. 
Beyond our effort to understand neural systems by building large-scale, 
physically-embodied biological models, adaptive analog VLSI sensorimotor sys- 
tems can be applied to many commercial and industrial applications involving 
self-calibrating actuation systems. In particular, we believe that for real-world 
tasks such as mobile robotics or remote sensing, these circuits will be invaluable 
for systems trying to keep up with the ever-changing world. 
Acknowledgments 
The authors would like to thank Reid Harrison for constructing one of the test boards 
and generally motivating the completion of this project, Paul Hasler for valuable 
technical advice on the design and analysis of the floating-gate cell, Tonia Morris 
and Giacomo Indiveri for assistance in the careful editing of this document. T.H. is 
supported by an Office of Naval Research AASERT grant and by the NSF Center for 
Neuromorphic Systems Engineering at Caltech. 
References 
[1] T. Delbrfick. Bump circuits for computing similarity and dissimilarity 
of analog voltages. 
In Proc. of Intl. Joint Conf. on Neural Networks, 
volume 1, pages 475-479, 1991. 
[2] T. Delbrfick. Investigations of Analog VLSI Visual Transduction and Mo- 
tion Processing. PhD thesis, California Institute of Technology, 1993. 
[3] H. Deubel. 
Separate adaptive mechanisms for the control of reactive 
and volitional saccadic eye movements. Vision Res., 35(23/24):3529-3540, 
1995. 
[4] S. P. DeWeerth. Analog VLSI circuits for stimulus localization and centroid 
computation. Intl. J. Comp. Vis., 8(22):191-202, 1992. 

SACCADIC EYE MOVEMENT 
189 
[5] C. Diorio, P. Hasler, B. A. Minch, and C. Mead. A high-resolution non- 
volatile analog memory cell. In Proc. IEEE Intl. Symp. on Circuits and 
Systems, volume 3, pages 2233-2236, 1995. 
[6] M. A. Frens and A. J. van Opstal. Transfer of short-term adaptation in 
human saccadic eye movements. Exp. Brain Res., 100:293-306, 1994. 
[7] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses with long term storage. In IEEE Intl. Syrup. on Circuits and 
Systems, volume 3, pages 1660-1663, 1995. 
[8] T. K. Horiuchi. An auditory localization and coordinate transform chip. 
In Advances in Neural Information Processing Systems 7, pages 787-794. 
MIT Press, 1995. 
[9] T. K. Horiuchi, B. Bishofberger, and C. Koch. An analog VLSI saccadic eye 
movement system. In Cowan, Tesauro, and Alspector, editors, Advances in 
Neural Information Processing Systems 6, pages 582-589, San Francisco, 
1994. Morgan Kaufman. 
[10] T. K. Horiuchi and C. Koch. Analog VLSI circuits for visual motion-based 
adaptation of post-saccadic drift. In 5th Intl. Conf. on Microelectronics 
for Neural Networks and Fuzzy Systems, pages 60-66, Los Alamitos, CA, 
1996. IEEE Computer Society Press. 
[11] L. M. Optican and D. A. Robinson. Cerebellar-dependent adaptive control 
of the primate saccadic system. J. Neurophysiol., 44:1058-1076, 1980. 
[12] R. Rao and D. Ballard. Learning saccadic eye movements using multiscale 
spatial filters. In Advances in Neural Information Processing Systems 7, 
pages 893-900. MIT Press, 1995. 
[13] H. Ritter, T. Martinetz, and K. Schulten. Neural Computation and Self 
Organizing Maps: An Introduction. Addison-Wesley, Reading, MA, 1992. 

III 
Neuromorphic 
Communication 

INTRODUCTION TO 
NEUROMORPHIC COMMUNICATION 
Tot Sverre Lande 
Department of Informatics, 
University of Oslo, 
N-0316 Oslo, Norway 
bassenOifi.uio.no 
8.1 
WHY NEUROMORPHIC COMMUNICATION 
The somewhat artificial term "Neuromorphic Communication" indicates the 
aim of transmitting information similar to our neural system. The "spiky" 
information coding found in our nerve-fibers seems to be quite inadequate for 
microelectronics. With a limited dynamic range of two to three orders of mag- 
nitude and poor noise margins, this kind of information coding may look like 
a bad choice from an engineering perspective. 
There are, however, several interesting properties of the rich neural spiking 
representation. 
â¢ 
One of the essential properties of a sequence of neural spikes is the statis- 
tical information coding. The average number of spikes carries the value 
to be transmitted. The term "mean-rate coding" arises from this inter- 
pretation. Of course, the mean-rate depends heavily on the time-scale or 
integration time, and different time-constants are actively used in neural 
computation. A coding frequently found in biology is the conveyance of 
the rate-of-change or transmitting the derivative of the signal. This kind 
of coding is often called adaptive coding, keeping the circuits within the 
usable range of operation. For mean-rate coded variables some operations 
like multiplication come easy. Both autocorrelation and crosscorrelation 
have been reported using simple AND-gates as multipliers [4, 5]. 
â¢ 
The large number of interconnections in the neural system is overwhelm- 
ing and gives rise to a significant amount of parallelism or redundancy. 

194 
NEUROMORPHIC SYSTEMS ENGINEERING 
Nature seems to remedy the limited dynamic range of a single axon 
with redundant coding of information in several parallel nerves. In [12] 
it is shown that the dynamics of mean-rate coding may be improved 
with redundancy. On the other hand redundant coding introduces fault- 
tolerance, handling defects in a graceful way. In a technological perspec- 
tive the ability to cope with defects may extend the size of implementable 
systems in silicon. 
The coding of states as spikes is still an analog representation. Although 
each spike is "digital", the inter-spike interval is still analog. The ampli- 
tude of an analog state is mapped to the time-domain and encoded with 
the high resolution inherent in an analog state. This analog property is 
most likely the fundamental reason for improvements by redundancy and 
graceful handling of defects. 
This list of interesting properties of neural spike-coding is far from com- 
plete, but exposes some of the more essential properties from an engineering 
perspective. 
The same way as neuromorphic engineering is exploring neural paradigms 
for computational systems, neuromorphic communication is exploring the prop- 
e~'ties of neural information coding to build up modular systems. The digital 
nature of a single spike is inviting to use standard digital communication sys- 
tems like data-buses or techniques known from digital communication such as 
different pulse modulation techniques. The spiking frequency of a neuron is 
less than 1000 spikes/s. With a bus transfer rate of hundreds of Mbyte per sec- 
ond, a digital bus should be able to transfer the spiking activity of a significant 
number of spiking neurons. Each neuron is given a unique number, and this 
number is transmitted on the bus whenever the neuron is spiking. 
8.2 
EARLIER WORK 
One of the early real system implementations using neuromorphic communi- 
cation was done by the late Misha Mahowald [51, 181], implementing a silicon 
model of stereoscopic vision. Three analog chips are interconnected with asyn- 
chronous digital buses and are able to extract depth information in real time 
based on visual stimuli from two silicon retinas. This is truly a remarkable 
achievement and is the first known analog multichip system using neuromor- 
phic communication. 
The term Address-Event-Representation, or AER for short, was proposed 
by Mahowald. 
The spiking event of a neuron is simply coded as a unique 
number on a binary encoded digital bus. This early version was a one-to-one 
communication with one sender and one receiver. The system is working in 
real time and is fully asynchronous. 
Another similar effort was done by Mortara et. al. [63, 14] based on similar 
ideas. In order to understand the different flavors of these approaches, some 
more details have to be explained. 

NEUROMORPHIC COMMUNICATION 
195 
The time-multiplexing of a significant number of unclocked spiking neurons 
or free running oscillators will lead to collisions. Regardless of our bus-speed, 
the possibility of collisions will always be present and concurrent events must be 
accounted for. The strategy for collision handling in the work by Mahowald is a 
full arbitration scheme where spikes are delayed until the bus is free to transmit 
the event. In this way, no collisions occur on the bus, but the events may be 
delayed (or even lost), depending on the bus-load. The approach adopted by 
Mortara is the opposite, where events are transmitted immediately, ensuring 
virtually no delay but increased collision-rate with increased bus-load. 
Both strategies have been demonstrated to work, however the full arbitration 
scheme by Mahowald may tolerate high bus-loads with large volumes of data 
typically generated from vision systems, whereas the simple non-arbitrating 
solution suggested by Mortara is suited for low-volume data typically generated 
by cochlea models. 
8.3 
WEAK ARBITRATION 
One of the essential features of neuromorphic communication is to stay analog 
even on a digital bus. In order to understand how the analog state is maintained 
the following classification may be useful: 
Quantized value, Quantized time is the characteristics of a full blown 
digital system as found in our computers. 
Continuous value, Continuous time is the characteristics of the real world 
and a pure analog system. 
Continuous value, Quantized time is usually called a sampled data sys- 
tem. Switched-capacitor (SC) or switched-current (SI) circuits are typical 
engineering examples of these kind of systems. 
Quantized value, Continuous time is the method of neuromorphic com- 
munication and also found extensively in biology. 
From the classification above we see that our analog state is maintained by 
the continuous time-scale or asynchronous bus strategy. But time-multiplexing 
of events over a digital bus will sometimes delay events or loose events due to 
collisions. Even with a fast bus some noise is introduced to our analog state. 
The question is now what is the best approach: 1) to make sure all events make 
it through the bus using full arbitration (with the extra delay introduced), or 
2) to minimize timing errors by sending everything to the bus immediately? 
The answer to this question is strongly dependent on the expected bus-load 
as indicated above. For low bus-loads, the simplicity and quality of Mortara's 
approach is favorable, but as the accumulated spiking-rate is increasing, errors 
due to collisions will degrade performance. At higher data rates, the full ar- 
bitration scheme will give better performance, but the penalty is a significant 

196 
NEUROMORPHIC SYSTEMS ENGINEERING 
timing-error at low data-rates. In order to reduce the arbitration-time, arbitra- 
tion circuits are usually designed to do local arbitration implying that neigh- 
boring neurons will be preferred in spite of neurons connected further away. 
Although the arbitration-time is reduced, this scheme introduces unfairness. 
The transmission-error of the analog state-variable will depend on where the 
neuron is connected to the arbitration-tree. The errors introduced by collisions 
are, on the other hand, randomly distributed among all the connected neurons 
(provided the spiking outputs from the neurons are stochastically independent). 
There is no doubt that whatever strategy is used, time-multiplexing will 
introduce errors. The question is what is the best way to trade the errors 
due to crowding with the errors due to aging? In recent work Abusland [1] 
has suggested to utilize a protocol known from data communication. CSMA or 
Carrier Sense Multiple Access used for the ethernet available on most computers 
is a weak arbitration scheme where collisions are avoided by listening on the 
bus and delaying the sending of data until the bus is free. But since computers 
have their own independent clock, they may occasionally start transmitting 
exactly at the same time, resulting in collisions. In such a case the data-packet 
is retransmitted after a random delay. The strategy is said to be persistent, 
ensuring that all packets eventually get transmitted. For acceptable loads this 
strategy is doing fine as we all know from our ethernet-connected computers. 
An early version of ethernet used the ALOHA protocol where no sensing is 
done and the packets are transmitted immediately. In our context the Mortara 
approach is a variant of the ALOHA protocol. 
8.3.1 
O-persistent and 1-persisten~ CSMA 
Abusland has modified the CSMA strategy by simply discarding events when 
the bus is busy. This modified strategy is called &persistent CSMA and is a 
significant improvement compared to the ALOHA approach at higher bus-loads. 
Another approach known as 1-persistent CSMA is to latch the event until the 
bus is free and transmit once. If two or more events are latched, a collision will 
occur and the events will be discarded. The way the Carrier-Sense function is 
implemented is by a shared bus-line, which when asserted indicates a busy bus. 
The common busy-line ensures a fair competition for the bus, distributing the 
introduced noise randomly between all transmitted analog states. 
In figure 8.11 the throughput of different arbitration strategies are plotted 
as a function of bus-load. The full arbitration scheme is able to utilize the total 
bus capacity, whereas the ALOHA protocol degrades at low bus-loads. Even 
at 20% bus load the collision-rate is significant and the loss-rate unacceptable. 
Both the 0-persistent and 1-persistent protocols are doing pretty well up to 
50% bus-load from where the collision-rate is degrading the throughput. The 
results presented are consistent with the expected performance of a CSMA 
protocol. The improved performance of the 1-persistent protocol is significant 
for lower bus-loads. The solid lines are estimated performance based on models 
while the circles are measured results from chips. 

NEUROMORPHIC COMMUNICATION 
197 
0.8 
~0.6 
.- 
"~ 
~ o.4 
0.2 
0 ~ 
0 
! 
! 
Ã 
aging chip 
0 
1-persistent ch~p 
o 
O-persistent chip 
0.2 
Ã 
Ã: 
x 
theoritical maximum 
// 
i 
x 
x 
x 
'! 
x 
< : 
Ã 
Ã' 
~ 9i3-queue 
Ã Ã 
Ã Ã: 
Ã 
.
.
.
.
.
.
.
.
.
 
~, . . . . . . . . . . . . . . . . . . . .  
~ 
:x . . . . .  
x 
~ 
Ã 
Ã 1.5:aueue_ 
Ã 
Ã 
Ã 
â¢ 1-queue 
Ã 
:x 
x 
... 
r 
~ 
~ 
. 
0.4 
0.6 
0.8 
1,2 
normalizedofferedloadG 
i 
Aloha 
1.4 
1.6 
1.8 
2 
Figure 8.1 
Bus throughput for different protocols 
spike 
ramp 
arbitration 
Figure 8.2 
Analog arnbitration with aging 
8.3.2 Analog arbitration and aging 
Another strategy proposed by Marienborg [13, 10] is to do an analog arbitration 
between competing events using the well know Winner-Take-All circuit or WTA 
for short [16]. The WTA is a fair arbitration scheme based on a shared wire. 
The original WTA has moderate discrimination properties with the possibility 
of allowing events simultaneous bus access. An improved WTA with feed-back 
[10] improve on the discrimination properties. 
The next improvement is to convert each spike to a ramp were the voltage- 
level of the ramp encodes the time of arrival (figure 8.2). The analog properties 
of the WTA-circuit will grant the oldest event access to the bus. Recognizing 
that older events will contribute with more noise, it may be more efficient to 
discard old events than to steal bus bandwidth with outdated events. This 
may be done by resetting the ramp when a certain voltage-level (indicating 
the age) is reached. With the ramp we have achieved both a FIFO (First- 

198 
NEUROMORPHIC SYSTEMS ENGINEERING 
In-First-Out) ordering, but also implemented aging by discarding old events. 
With these added features, improved throughput is obtained with acceptable 
noise-margins on the transmitted analog states. 
In figure 8.1 the throughput of this analog arbitration scheme is plotted 
against the other strategies with measurements indicated with x in the figure. 
By changing the duration of the ramp different properties appear. 
Setting the ramp very short means discarding events when the bus is busy 
and is identical to the 0-persistent strategy. The Ã-es in figure 8.2 follow 
the 0-persistent model as expected. 
Extending the ramp to latch exactly one event is similar to the 1- 
persistent strategy for low bus-loads. For higher bus-loads, the through- 
put will not degrade due to the discrimination admitting one of the com- 
peting events instead of discarding them both. Again, measurements 
confirm this behavior where the Ã-es follow the the 1-persitent line at 
low bus-loads, but keep increasing instead of decreasing for higher loads. 
By increasing the length of the queue, improved throughput is possible. 
With an estimated queue of 3.3, the collision-rate is acceptable even at 
80% load. Again, these measurements confirm the good discrimination 
properties of the improved WTA-circuit. 
These measurements only indicate the utilization of the bus with no indi- 
cations of the quality of the transmitted analog state variables. A study of 
introduced noise is underway looking for the optimal balance between aging 
and crowding. Unfortunately, these results are not available at the moment of 
writing, but the flexibility of the "analog FIFO" makes these studies feasible. 
8.4 
CONCLUSION 
Weak arbitration combined with aging is a good trade-off between bus utiliza- 
tion and acceptable noise-margins. Both theoretical analysis and early mea- 
surements indicate the feasibility of weak arbitration, which distributes the 
bus-introduced noise randomly between all the transmitted analog states. 
In the following three chapters different aspects of neuromorphic communi- 
cations will be presented. In the first chapter by Mortara, the ALOHA protocol 
is used to convey analog states between a retina-chip and an orientation en- 
hancing circuit. In the next chapter, Kalayjian explores the analog properties of 
the WTA-circuit to implement a communication system from a retina chip. Fi- 
nally, Boahen is using the full arbitration scheme for inter-chip communication 
of 4096 analog states. 
In spite of different solutions, the main objective of transmission of ana- 
log states using asynchronous communication is the same in all the chapters. 
Hopefully the reader will find sufficient material to select a suitable solution. 

NEUROMORPHIC COMMUNICATION 
199 
Notes 
1. This figure was kindly provided by Jan Tore Marienborg based on his own measuements 
and measurements provided by/~nen Abusland 
References 
[1] /~. Abusland, T. Lande, and M. H0vin. A VLSI communication architec- 
ture for stochastically pulse-encoded analog signals. In Proceedings of the 
IEEE International Symposium on Circuits and Systems, volume 3, pages 
401-404, Atlanta, GA, 1996. 
[2] Z. Kalayjian, J. Waskiewicz, D. Yochelson, and A. Andreou. Asynchronous 
sampling of 2d arrays using winner takes all circuits. In Proceedings of the 
IEEE International Symposium on Circuits and Systems, volume 3, pages 
393-396, Atlanta, GA, 1996. 
[3] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead. Winner- 
take-all networks of O(n) complexity. In D.S. Touretzky, editor, Advances 
in neural information processing systems, volume 2, pages 703-711, San 
Mateo- CA, 1989. Morgan Kaufmann. 
[4] J. P. Lazzaro and C. Mead. Silicon models of auditory localization. Neural 
Computation, 1:47-57, 1989. 
[5] J. P. Lazzaro and C. Mead. Silicon models of pitch perception. In Proc. 
Natl. Acad. Sci. USA, volume 86, pages 9597-9601, 1989. 
[6] M. Mahowald. VLSI Analogs of Neuronal Visual Processing: a Synthesis of 
Form and Function. Computation and neural systems, California Institute 
of Technology, 1992. 
[7] M. Mahowald. An Analog VLSI Stereoscopic Vision System. Kluwer Aca- 
demic, Boston, MA, 1994. 
[8] J. Marienborg, T. S. Lande,/~. Absuland, and M. H0vin. An analog ap- 
proach to 'neuromorphic' communication. In Proceedings of the IEEE In- 
ternational Symposium on Circuits and Systems, volume 3, pages 397-400, 
Atlanta, GA, 1996. 
[9] J. Marienborg, T. S. Lande, M. Hovin, and/~. Absuland. Neuromorphic 
analog communication. In Proceedings of the IEEE International Con- 
ference on Neural Networks, volume 2, pages 920-925, Washington DC, 
1996. 
[10] A. Mortara and E. A. Vittoz. A communication architecture tailored for 
analog VLSI neural networks: Intrinsic performance and limitations. IEEE 
Transactions on Neural Networks, TNN-5(3):459-466, May 1994. 
[11] A. Mortara, E. A. Vittoz, and P. Venier. A communication scheme for 
analog VLSI perceptive systems. IEEE Journal of Solid State Circuits, 
SC-30(6):660-669, June 1995. 

200 
NEUROMORPHIC SYSTEMS ENGINEERING 
[12] E. J. Nayly. Spectral analysis of pulse frequency modulation in the nervous 
systems. IEEE Transactions on Bio-Medical Engineering, 15(4):257-265, 
October 1968. 

A PULSED 
COMMUNICATION/COMPUTATION 
FRAMEWORK FOR ANALOG VLSI 
PERCEPTIVE SYSTEMS 
Alessandro Mortara 
Swiss Center for Electronics and Microtechnology, 
Rue Jaquet-Droz 1,CH-2007 Neuch~tel, 
Switzerland 
mortaraC)csemne.ch 
9.1 
INTRODUCTION 
This paper reports on the main properties and some applications of a pulsed 
communication system specifically developed for the service of multichip per- 
ception schemes realized in analog VLSI. The project started with the goal to 
obtain biological-like connectivity among functional subsystems capable of pro- 
cessing sensory data in a collective fashion through several hierarchical layers 
and through convergence, divergence and fusion of data from different origins. 
As a consequence of the thin sheet organization of their biological counter- 
parts, the realized subsystems consist almost invariably of several one- or two- 
dimensional arrays of cells. The output of every cell, its activity, is relevant to 
further processing and should be available for communication to the next layer. 
To relax this requirement, the possibility exists of taking at least some ad- 
vantage of the particular way data are processed by a neural system. It is 
known, for example, that the retina responds faster to the variation of an in- 
tense excitation than to the change of a dark area into an even darker one [2] 
so that an adapted communication system will tolerate a worse performance 
in the communication of weak activities. Along the same line, another inter- 
esting and ubiquitous property of biological perceptive systems is the tendency 
to accentuate and encode variation in the sensorial landscape. The encoding 

202 
NEUROMORPHIC SYSTEMS ENGINEERING 
of variation is only partially related to the usual communication engineering 
data compression methods. The problem in this case is to optimize the bit 
rate (in the sense of information theory) on a bandlimited channel by reducing 
redundancy in the message. This should be achieved while using the whole 
available bandwidth, thereby maximizing energy expense. Conversely, as it has 
been pointed out [11], the long distance biological communication style reduces 
power consumption because no energy is allocated to transmit "useless" infor- 
mation: biological systems do not fully use the available bandwidth to save 
energy. However, if on the one hand a purely sequential conditioning (such as 
A/D conversion or buffering in the case of analog transmission) and scanning 
of the analog activities in a large neural network would mean energy waste, the 
fully parallel communication architecture of some parts of the nervous system 
is not realizable in the VLSI context because of on- and off-chip wiring and 
pin limitations. Probably, the best way out of this apparent dead end is to 
trade the speed of silicon for the connectivity intrinsic to the nervous system. 
This is the way we decided to follow from the outset. It has led at first to a 
communication architecture, described and theoretically evaluated in [63], then 
to a small-scale realization [14] containing all the essential hardware building 
blocks. The scheme is now evolving from a means to set up point-to-point 
connections between single elements located on different chips to a more gem 
eral system where other types of connections can be envisaged (such as, for 
instance diverging connections). This paper focuses on recent developments, 
possible applications and on the experimental verification of the theoretical 
basis of the method. 
9.2 
PRINCIPLE AND THEORETICAL PERFORMANCE OF THE 
COMMUNICATION SYSTEM 
In the proposed system, all cells have access to a common parallel bus. The 
cell's activity is proportional to the frequency of the train of short pulses it 
emits without any deterministic timing relationship to the spikes generated by 
other cells. When a cell emits a pulse, its code appears on the bus for the 
duration of the pulse. Pulses are decoded by the receiver and directed towards 
the proper target cell, which can either simply accumulate the incoming pulses 
or also perform other kinds of operations, such as broadcasting them in a region 
with programmable boundaries as schematically represented by the "diffusion 
network" box in Figure 9.1 [4]. This amounts to a spatial convolution with 
the kernel defined by the diffusion region. It is the designer's decision to keep 
processing at the pulsed representation or to revert to analog voltages and 
currents by demodulating the appropriate spike trains. 
The proposed scheme is similar to the address-event representation of Ma- 
howald [51], but does not include arbitration of bus access when two cells 
simultaneously emit a pulse (this event is called a collision): in this case the 
resulting compound address, obtained by the bitwise OR of the colliding codes, 
is simply ignored by the receiver circuit. A formal discussion of the respec- 

EXCITATORY 
FEEDBACK 
CIRCUITS 
203 
activity-frequency converter 
ii 
~ 
~~-~ 
, 
, 
.:~:]--['-~ 
q 
~ 
II 
I ~ : ~ ' - ~  
~ 
q 
,~:~}---~ 
I 
I 
vÂ°lts 
~ 
.~' I13 
~ I 
code for address 10010 
pulse accumulation 
o 
- 
/ 
~ 
I~ 
i 
l 
~ 
i 
I~. 
i 
J~ 
~ 
l 
~ 
~) 
~ 
B 
~ 
~ 
~ision be~een I and ~/ 
-- 
Figure 9.1 
Communication system block diagram. Pulse transmission is represented on a 
5~wires bus, 
tive merits and disadvantages of the two approaches appears in [14], but the 
NAPFM 
scheme (Non Arbitrated Pulse Frequency Modulation) we propose is 
simpler. 
Address-event 
uses an arbiter to decide which one of a number 
of 
colliding pulses has the right to access the transmission channel and, in some 
implementations, 
to allow a second chance to the losers of this competition. 
This very desirable property is offset by increased circuit and signalling com- 
plexity in the form of arbitration circuits and request and acknowledge signals 
propagating through the body of the circuit. In the alternative we propose, 
collision events are detected through coding and ignored (pulse loss). The 
encoding hardware consists of just connected wires so that speed and design 
simplicity are favored. 
Hardware 
simplicity is essential if arbitrary connections between cells of 
the the same chip must be set up. This necessity has recently appeared in the 
design of a perception-action loop system [6] which uses NAPFM 
to connect in a 
nontopology-preserving 
fashion a sensory and a motor map located on the same 
chip. In this case since communication 
stays internal to the chip, bandwidth is 
maximum 
and collision probability is minimal thus NAPFM 
communication 
is 
in its best performance range for a given technology. In such a configuration, 
every cell must be in a position to decode by itself the bus contents because 
connections are arbitrary and no row-column arrangement holds. To carry out 
decoding, one gate is sufficient (as shown in section 9.3), and to further reduce 
transistor count, one p-type transistor can be used in the pull-up branch of the 
gate. 
The consequence of collisions is uncertainty over the pulse count coming from 
one particular cell. Although in biological perceptive systems like cochleas or 

204 
NEUROMORPHIC SYSTEMS ENGINEERING 
retinas, which respond vigorously and immediately to changes, many events 
(start of an action potential) can be considered simultaneous if they occur in 
a time window of the order of a millisecond; in a VLSI system such as the one 
considered here, simultaneity (in the sense of pulse loss) means events separated 
by a time of the order of ten to a hundred nanoseconds. The latter is a rather 
conservative figure for the minimum event duration needed to elicit a valid 
logic pulse in the receiver. Thus, events that are simultaneous for biological 
structures are sufficiently separated in time in the collision-prone VLSI context 
to be able to notice them and take appropriate action. This is the main reason 
for the viability of the non-arbitrated communication scheme we are presenting. 
To gain a better understanding of system performance, the results of an 
analysis developed in [63] are summarized. The point process "start of a pulse 
emission anywhere in the network" is modeled as a Poisson process whose rate 
is determined by the average activity in the network. 
This seems to be a 
reasonable model for a large population of independently firing cells and, as it 
will be shown in section 9.5, it can be experimentally verified. 
. 
,,. 
. 
granularity 
~
~
 
~2= 10-1 ~
~
 
collisio~ 
10 ~ 
~ 
1 
10 
0.001 
0.01 
0.1 
2 ftot6 
Figure 9.2 
Dynamic range-maximum activity frequency relationship. Each curve shows 
how to choose the best total pulse rate to attain the largest dynamic range (i.e. the smallest 
minimum activity detectable with a given tolerable error) The optimum (minimum of the 
curves) occurs at slightly different values of normalized frequency but satisfactory behavior, 
although suboptimal, is obtained in all cases for 2N~fo6 ~ 0.1 
Consider a network containing N cells. Let 5 be the minimum pulse duration 
necessary to generate a suitable pulse at the receiver and Oifo the cell's pulse 
rate, where 0i is the activity of cell i (0 _< 0i < 1) and f0 is the frequency 
corresponding to maximum activity. Because of the coding, simultaneous access 
of two or more cells to the bus, in the sense specified above, results in the loss 
of all colliding pulses. The probability of emission with no collision can be 
expressed as [63]: 
p = e -2NafÂ°~ 
(9.1) 

= 
:sp~o~ ~po~ jo ~qmn~ oq~, o:~ 
:~Inmaoj s,~u~iaD~ ~u~Â£idd~ pau~mao~op s~ spaomopoa 
jo aaqmnu pu~ saa~ snq jo aoqmnu oqa uoa~aoq d~qsuo~a~ioa a~aoadmL~ aqÂ£ 
â¢ opoa oq~ jo Ll~I~q~I~aS smoIl~ qa~qm p~oqaaao olqvadoaa~ u~ 'Og jom jo soni~a 
oa dn saaI~ g u~ql ssa[ o~ poa~m~i s~ Â£au~punp~a a~qa ~no suana aI 'uo ~ou moaj 
,,I~mDdo. PaIIVa oq IUm apoa s[qÂ£ "g/m jo aaed aa$oau[ oR1 ~ aoj Su~a Xq 
pou~mqo s~ spao~ apoa jo D aaqmnu lsaSa~ I aqa 'snq aqa u~ pasn oa~ soa~ m 
JI "aapoaap aqa Xq poaou$~ Â£[i~aD~moln~ s~ qa~qm ,,sauo,, I + ~ as~aI av qa~ 
opoa ~ u~ salnsaa SlIaa Su~P~IIOa oaa jo sapoa oqa jo ~0 as~a~q aqa ~,,~souo,, 
jo ~ aaqmnu am~s aqa qa~ papoa aa~ sassoapp~ [i v :apoa pa~aojaq$~aas ~ 
saso$$ns Xlaadoad s~qÂ£ "sapoa Su~p~iioa oql uo uoD~aodo HO poa~ os~ma~q oqa 
smaojaod snq oqa 'suoDmuomaldm ~ ano u I 'ssaapp~ as[~j ~ ol puodsoaaoa X~m 
snq oR1 uo ~uasoad apoa oql 'uo~s~iioa ~ Su~anp anq ~Saslnd ~u~P~iioo W jo ssoi 
oqa s~ suo~s~ilOa jo aao~o *IUO aqa j~ pIoq uoDaas ~uspaaaad aqa jo sainsoa oqÂ£ 
DNICIOD SS~I~IO0~ 
Â£'6 
"snq oq~. uo alq~It.~a~ q~,p!~pu~q oq:~ jo %0I :lnoqv aq 
plnoqs oa~a aslnd ImO~ ~q~ :suo~s~IlOa pu~ ~a~inu~s~ jo sms~u~qaam ~uD~m~i 
-oau~mao$aod oma oql uoomaaq oauvl~q [~mDdo oql Sutssoadxo alnsoa oa~a~a~a 
-u~nb aqa s~ (U6) uo~a~IaH 'aa~mpa~q oldm~s Â£q 1no pa~aa~a aq u~a a~ osn~aoq 
o[q~adaaa~ s~ saD~a~la~ jo uo~a~z~I~maou j~ ioaauoa oa Â£1Du~nb I~qoI~ auo~uoA 
-uoo Â£aoa ~ g~ ~ '.~aom~ou oq~ Xq poaa~ma Saslnd jo aaua I~qoi ~ aqa s! ,o,f oaoqm 
V0 ~ 9~o~fg = pOfVNg 
:dt.qsuot.:~eloa oq~ ~ut.oaojuo Â£q pout.mqo oq uuo 
o~uu~ 3!tu~uÂ£p mnmtx'~m ~q:~ osi~ s~oqs U6 oan~H "Â°fg~N~ o~a oslnd i~o~ 
pvziium~ou oq~ pu~ ~ UOAI~ ~ q~im pOAaOSqo (9N~/Â£)(~/~0) = ~ ~[AD3V 
(poz!Iumaou) mnmiuim ~q~ uoo~oq d!qsuo!~ulaa oq~ smoqs ~q~ U6 oan~H ut 
oiqisiA aau suoD~na!s oma aq~ '[OAO[ aoaaa ~OAI~ ~ ~ Olq~AaOSqo ~0 
Xa!ADO~ 
mnm!u!m aqa Â£q aSu~a a!meuKp pu~ aunoa aslnd oqa u~ (o~aua u~om-oa-uoDu!Aa p 
paepu~as oq~ jo aaunbs aq~) :s aoaaa OA[a~[Oa Oqa Xq poq!aaso p aq uva ~s!o N 
â¢ ostou os~oaaii! suo!s!iio~ oaoqm uoD~na!s oawa q$!q ~ oa (Xa!a~Inu~a~) u!uaa asind 
pa~ima oq~ jo po:aad ~q~ o~ poa~dmoa ~aoqs s~ am D uO[~Aa~Sqo Oq~ ~a~qm 
UO~n~S O]~a ~O[ ~ mo~J OAOm Om '0~ Â£~A~O~ mnm~iu oq~ '0f ~uis~oaau: X~ 
â¢ sms!uvqaom auoao~p oma Â£q posn~a ~ lunoa oslnd oqa aoao (os!ou) Â£au!~aoaun 
u~ aoaomoq saInsoa om D uo~l~A~osqo poa~m!i oqÂ£ "auva~!u~s oa~ son[~a oaD~ioa 
Â£[uo o.taqm suoD~a!ldd~ 1sore u~ oauo~uoauoau! ou s~ qa~qm 'd aola~j ~ Â£q poanpoa 
avodd~ oa Â£1~aDa~ Â£aoao osn~a suo!s~IIO D .(ofÂ£)/,~ ql~m Â£a!aDa~ paaaosqo oqa 
soa~m~aso pu~ Â£ om~a uOD~Aaosqo oqa u! ~ IlOa moaj Sutmoa sosind ~ oq1 saunoa 
iiaa aoSa~a oq~ "~aomaou oqa jo Ka!a~aa~ o~aoa~ oqa s~ ~SXN/I = ~ oaoqm 
~0g 
SÂ£I~DI:IID >IDV~t(IX~I AI:IOÂ£Â¥Â±IDX~ 

206 
NEUROMORPHIC SYSTEMS ENGINEERING 
which shows a mildly less-than-exponential growth of number of code words 
with bus width. We now turn to additional practical properties of the code. 
bus 
l 
III 
Figure 9.3 
Logic to decode address 11000, in a 5-wire bus example. The left and right 
gates are equivalent but the right one is simpler and the NOR gate can be implemented 
with just a p-type transistor in the pull-up branch. 
Cells are addressed by monitoring and decoding the configuration of bits on 
the NAPFM bus. An example of the decoding logic is given in Figure 9.3 for 
the code 11000. Note that the AND (or NOR, depending on the chosen config- 
uration) gate needs only 4 inputs; because of the way the code is constructed 
and because no additional zero can be produced by a collision, a codeword is 
completely characterized by the position of its "zeros". A check for a "one" in 
any remaining wire is enough to determine the presence of an event. Thus, for 
this family of codes, the decoder logic needs only one more input as there are 
"zeros" in the code. 
9.4 
A SILICON RETINA EQUIPPED WITH THE NAPFM 
COMMUNICATION SYSTEM 
As an application of the NAPFM communication architecture, this section 
presents a silicon retina that can be used as the first layer of a biomimetic 
vision system. The retina has been successfully interfaced with another analog 
chip, also equipped with NAPFM communication, implementing a cortical layer 
for orientation enhancement. The chip is basically an array of hexagonally 
arranged cells interconnected in such a way that the spatial high-pass filtering 
of the image projected on the chip surface can be carried out and the results 
of the computation passed over to the next layer using the NAPFM technique. 
Every cell contains part of a normalizer and a pulse generator connected to a 
parallel bus in exactly the same way as described in [14] and [1]. This section's 
main interest is to present conclusive measurements concerning the validity of 
the assumptions in [63] and a larger-scale realization than in [14]. 
Edge enhancement can be obtained by subtracting from the image its spa- 
tially low-pass filtered version. This operation results in a center-surround 
receptive field. A low-pass spatial filter can be realized by using a resistance- 
conductance diffusive network implemented with an array of resistors, difficult 
to integrate in CMOS technology if large enough resistances must be used for 

EXCITATORY FEEDBACK CIRCUITS 
207 
low consumption. A solution to this problem is to use a transistor with a par- 
ticular gate bias and to operate it in its conduction mode. A clever gate biasing 
scheme has been proposed [8] to implement with MOSFETs a resistor-like con- 
nection between two nodes. However, the range of linearity of this element's 
conductance is limited and depends on its source/drain potentials in a satu- 
rating fashion (the saturating non linearity, however, turns out to be useful in 
segmentation tasks). 
It has been shown that linear networks can be efficiently implemented using 
CMOS transistors instead of resistors. Reference [14] gives full details about 
the underlying principle which can be so stated: 
The current distribution in the branches of a current-driven resistive net- 
work is the same as that of a current driven MOS network (all transistors 
having the same gate voltage) where resistor terminals are replaced by 
sources and drains and conductance ratios are replaced by W/L ratios. 
Grounded resistor terminals must be replaced by MOS terminals at a 
potential larger than their pinch-off potential VP (pseudo-ground) 
which permits easy extraction of the current entering a pseudo-ground node by 
means of a complementary current mirror. 
In particular, if transistors are restricetd to operate in the weak inversion 
domain, the analog of conductance in the resisitve network, the pseudo conduc- 
tance [6], can be also tuned by adjusting the gate voltage. Pseudo-conductance 
becomes then a linear function of W/L and an exponential function of the gate 
voltage Va. 
~ Inorm 
v-I 
 I,LP 
Figure 9.4 
Schematic diagram of a pixel. 
The schematic diagram of one pixel is shown in Figure 9.4. The photodiode 
is a n-well/substrate junction. The photogenerated current enters the pixel's 
share TN1 -- TN2 -- TN3 of a chip-wide normalizer integrated using MOS transis- 
tors in weak inversion. Although a "real" translinear network should be made 
using bipolar or compatible lateral bipolar transistors [3, 17] the MOS solu- 

208 
NEUROMORPHIC SYSTEMS ENGINEERING 
tion was chosen for its smaller area even with rather large transistors in the 
translinear loop and because exact normalization does not seem critical in this 
application. The normalization current is directly related to the network's total 
activity and can be used to tune bus occupation to the best value according 
to equation (9.2). The normalizer has two outputs TN2 and TN3. The current 
delivered by TN2 is injected in the pseudo-conductance network. The contribu- 
tion to the network of each cell is a conductance Ta to the pseudo-ground node 
PG and three "outgoing" pseudo-resistances to nearest neighbors TR1, TR2 and 
TR3. The three other pseudo-resistive connections are provided by remaining 
neighbors. The current injected in the pseudo-resistive grid is spatially low-pass 
filtered and the result of this operation, ILp, flows through Tc, is collected by 
the current mirror TM1 -- TM2, and subtracted from the current IIM (a nor- 
malized version of the original image) flowing through the second normalizer's 
output TN3. The difference IHp, represents the local value of the spatially 
high-pass filtered version of the image. It drives the pulse generator composed 
by a Schmitt trigger (transistors TT1, TT2, TT3 and TT4) and transistors TD 
and Ts. The input node of the Schmitt trigger (transistors TT1, TT2, TT3 and 
TT4) is alternatively charged by Iup and discharged by Ides (imposed by dis- 
charge transistor TD when switch Ts is ON) between its upper threshold VH + 
and its lower threshold V~. Introducing AV = VH + -- V~ , the pulse frequency 
f depends only on the normalized activity current IHp : f ---- IHp/CAV and its 
duration 5 is controlled by Ides assumed much larger than IHp : ~ ~ C/~Y/Xdis . 
The network's total activity is: 
ftot= I~o,.,~/CAV 
(9.3) 
and therefore, controlling the normalization current is enough to realize con- 
dition (9.2). The cell encodes of course only positive values of the high-pass 
filtered image. Similarly to what has been described in [14], current pulses 
generated by Tc and TL reach a 7-wires column and a 7-wires row internal 
bus. The row and column codes are 7-bits with 3 "ones" (35 possibilities). The 
internal bus current configuration is converted into voltage pulses by the same 
circuitry as in [14]. Figure 9.5 shows a photograph of the chip. 
9.5 
NOISE MEASUREMENTS RESULTS 
The most important theoretical result to test from the communication's point 
of view is the dependence of relative error ~2 as a function of bus occupation 
for a given chip activity. Bus occupation depends only on chip total pulse rate 
and is proportional to the normalization current. 
To perform this measurement, pulses coming from a particular cell have 
been accumulated by the timer of a microcontroller, which also provided ways 
of storing the readings in a file. 
The chip was illuminated with a uniform 
spot and transistor Ta had its gate at V + to exclude filtering action. Mea- 
surements have been done for different values of the normalization current. 
For each current, 35 readings of the pulse counts over different time windows 

EXCITATORY FEEDBACK CIRCUITS 
209 
Figure 9.5 
Die photograph of the silicon retina. 
have been recorded. The average and standard deviation of the counts have 
then been computed with a spreadsheet. Average pulse counts for observation 
times of 10, 20 and 30ms are shown in Figure 9.6 as a function of normal- 
ization current, directly related to the probability of collision, equation (9.1). 
Note the good agreement of the measurements with the expected behaviour 
in I~o~-meXp(-I~or~) (intuitively, the number of received pulses is the product 
of the number of emitted pulses, proportional to Ino~-~, times the probability 
of safe reception, which goes exponentially with -I~o~.,~) This is a first solid 
confirmation of the validity of the Poisson assumption. 
The measured relative error s 2 is shown in Figure 9.7 (only points for T -- 10 
and 30ms are shown for clarity). To obtain an analytical fit, we used equa- 
tion (8), of reference [63]: 
S 2NÂ°zfÂ°6 -- 1 
1/4 
~ 
- 
+ 
-
-
 
(9.4) 
OfoT 
(OfoT) ~ 
in our measurements, T = 10, 20 and 30ms. 
With uniform illumination, 
OfoT = 1/NftotT, with N = 35.35 = 1225 and ftot given by (9.3). To deter- 
mine 6, the system's time resolution in our set-up, we increased the discharge 
current of the pulse generators until a sharp decrease of the pulse count was 
observed for a discharge current between 3 and 4#A. In this case the pulse du- 

210 
NEUROMORPHIC 
SYSTEMS ENGINEERING 
100 t 
~
'
~
 
â¢ 
T=10ms 
80 / 
~ 
"'â¢-â¢ 
~ 
â¢ 
T=20ms 
0 ,o 0t 
~- 4o 
20 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
normalization current (I~A) 
Figure 9.6 
Pulse count coming from one cell as a function of the normalization current. 
Solid line: theoretical fit with an [norm â¢ ez.p(-[~o~.m) -like behaviour. 
0.1 
~2 
0.01 
+ 
T=10ms 
+ 
â¢ T=30~ 
-t- -t- 
++-I- 
nl 
..~...~, 
â¢ â¢ 
-+ 
â¢ 
,~...~" 
â¢ â¢ 
n 
n
=
~
 
~ 
â¢ 
0.001 
. . . . . . .  
, 
...... 
0.1 
1 
10 
normalization current (pA) 
Figure 9.7 
Variation of E 2 as a function of normalization current. Squares: measured 
values, solid line: theoretical prediction based on equation (9.4) 

EXCITATORY FEEDBACK CIRCUITS 
211 
Figure 9.8 
Signal recorded while illuminating the chip with a light bar. Top: average 
of 2 recordings, observation time 20 ms; bottom: difference of the two same recordings 
(visualizes transmission noise). 
ration is too small for the receiver logic to operate properly and a conservative 
estimate for the largest allowable discharge current is 3#A (corresponding to 
~ ~ 300n8, which is not surprisingly large since the measuring set-up included 
a cascade of 2 gate-array-logic chips as decoders, one AND gate and a rather 
adventurous connection to the controller mounted on a printed circuit). Dedi- 
cated receiver hardware can push system bandwidth much higher; an array of 
analog pulse demodulators [12] has demonstrated 40n8 operation and a digital 
receiver chip, soon to be fabricated, has also been designed to that purpose. All 
the parameters have thus been estimated to fit the measurements with (9.4). 
The result of the fitting is shown in Figure 9.7. A good order-of-magnitude esti- 
mate of the relative error is provided by the theory despite the many sources of 
uncertainty (for instance over the determination of the number of active cells, 

212 
NEUROMORPHIC SYSTEMS ENGINEERING 
over the real value of 5, the real value of AV used to determine the chip pulse 
rate ftot and especially real value of 0). 
The same set-up was used to produce images with the chip. In this case 
the cells were sequentially observed and pulses coming from each address ac- 
cumulated over 20ms. Figure 9.8 shows the result obtained by projecting a 
light bar on the chip. Two acquisitions were done to obtain a visual impression 
of the fixed-pattern noise, and of the noise contributed by the communication 
system. The average value of the two acquisitions is displayed on the top of 
the figure (and can be interpreted as a representation of the light signal plus 
fixed-pattern noise) while the absolute value of their difference, originating only 
from transmission noise, is displayed on the bottom of the figure. As can be 
observed, fixed pattern noise definitely dominates over transmission noise and 
contributes high-frequency spatial components difficult to separate from actual 
edges in the signal by a linear spatial filtering operation without other cues. It 
is likely that an adaptation method like the one proposed in [8] is necessary to 
auto-zero the system and reduce mismatch effects. As far as communication is 
concerned, though, the results obtained with this chip are satisfactory. 
9.6 
AN APPLICATION OF NAPFM COMMUNICATION TO 
CORTICAL VISUAL PROCESSING 
The silicon retina described above has also been used as the input stage of a 
multichip system. It feeds a second chip, where the electronic equivalent of an 
axonal arborization is realized: incoming pulses determine the injection of a 
current at the target location which is then diffused by a non-linear network. 
The diffusion network has been originally designed as a building block in the 
analog implementation of a Kohonen map, reference [4] gives all the details on 
how this functionality is achieved. It has been modified to be capable of forming 
an activity bubble with controllable size, aspect ratio and orientation [15]. 
These geometrical parameters are easily set by global biases which, however, 
entails the limitation that all bubbles have the same shape. At the edge of 
the bubble, voltages at the nodes of the non-linear network decrease sharply 
and a threshold can easily set the bubble's exact boundaries. Thus the second 
chip acts as if spanned by axonal arborizations is capable of distributing every 
incoming pulse to several destinations arranged on the bubble. Since cells of the 
second chip receive inputs from several cells with a center-surround receptive 
field, the possibility of implementing Hubel and Wiesel's [5] simple cell receptive 
fields emerges from this architecture. The cortical chip also provides its output 
in the NAPFM format. The operation of the complete chain is demonstrated in 
Figure 9.9 which shows the three steps of processing: the raw image appears on 
the left, the edge-enhanced image in the center and the image with enhanced 
vertical edges on the right. In all images the output has been taken from the 
second chip, using it as a kind of repeater (by setting the bubbles' sizes to one 
pixel) in the first two cases. Note that it would be straightforward to plug on 

EXCITATORY FEEDBACK CIRCUITS 
213 
the retina's output bus to other cortical chips tuned for different orientations 
to implement simultaneous enhancement of edges oriented in several directions. 
~g 
Figure 9.9 
Results of interfacing the silicon retina with the orientation enhancement chip: 
left: photoreceptors response, center: retina ON-center response; right: vertical edge en- 
hancement by cortical chip. All responses recorded from cortical chip. 
9.7 
CONCLUSION 
This paper discussed the principles and presented recent results of the applica- 
tion of a pulsed communication system for analog VLSI perceptive systems.The 
theoretical grounds of the system's design have been experimentally verified. 
As an example of application, a silicon retina interfaced with a cortical orien- 
tation enhancing circuit has been presented. Computation performed by the 
cortical chip relies on the pulsed representation of the information stream it 
receives. It is an example of the opportunistic approach to sensory processing 
possible using analog techniques. Further applications of the communication 
system are foreseen in the design of a sensorimotor loop where it can be used to 
set up non topology-conserving links between sensory and motor maps located 
on the same chip. 
Acknowledgments 
I wish to acknowledge the constant support of Prof. Eric Vittoz under whose guidance 
the project has started and developed and Philippe Venier, who designed the cortical 
chip and kindly provided Figure 9.9. 
Notes 
1. editorial note: this scheme was first proposed by Marienborg in [10] and called Constant 
Weight Coding 

214 
NEUROMORPHIC SYSTEMS ENGINEERING 
References 
[1] X. Arreguit, E. A. Vittoz, F. van Schaik, and A. Mortara. Analog im- 
plementation of low-level vision systems. In ECCTD Proceedings, Davos, 
Switzerland, August 1993. 
[2] J. E. Dowling. The retina: an approachable part of the brain. Technical 
report, Belknap Harvard, 1987. 
[3] B. Gilbert. A monolithic 16-channel analog array normalizer. IEEE Jour- 
nal of Solid State Circuits, SC-19(6):956-963, December 1984. 
[4] P. Heim, B. Hochet, and E. A. Vittoz. Generation of learning neighbour- 
hood on kohonen feature maps by means of a simple nonlinear network. 
Electronics Letters, 27(3), January 1991. 
[5] D. Hubel and T. Wiesel. Brain mechanisms of vision. Scientific American, 
pages 130-144, 1980. 
[6] O. Landolt. Personal communication. 
[7] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie. 
Silicon auditory processors as computer peripherals. 
IEEE Journal of 
Neural Networks, 4(3):523 528, 1993. 
[8] M. Mahowald. Silicon retina with adaptive photoreceptors. In SPIE's In- 
ternational Symposium on Optical Engineering and Photonics in Aerospace 
Sensing, Orlando, FL, 1991. 
[9] M. Mahowald. VLSI Analogs of Neuronal Visual Processing: a Synthesis of 
Form and Function. Computation and neural systems, California Institute 
of Technology, 1992. 
[10] J. Marienborg, T. S. Lande, M. Hovin, and A. Absuland. Neuromorphic 
analog communication. In Proceedings of the IEEE International Con- 
ference on Neural Networks, volume 2, pages 920-925, Washington DC, 
1996. 
[11] A. Mortara and E. A. Vittoz. A communication architecture tailored for 
analog VLSI neural networks: Intrinsic performance and limitations. IEEE 
Transactions on Neural Networks, TNN-5(3):459 466, May 1994. 
[12] A. Mortara and E. A. Vittoz. A 12-transistor pfm demodulator for analog 
neural networks communication. IEEE Transactions on Neural Networks, 
TNN-6(5), September 1995. 
[13] A. Mortara, E. A. Vittoz, and P. Venier. A communication scheme for 
analog VLSI perceptive systems. IEEE Journal of Solid State Circuits, 
SC-30(6):660-669, June 1995. 
[14] L. M. Reyneri. A performance analysis of pulse stream neural and fuzzy 
computing systems. IEEE Transactions on Circuits and Systems II: Ana- 
log and Digital Signal Processing, CAS-42(10), October 1995. 
[15] P. Venier, A. Mortara, X. Arreguit, and E. A. Vittoz. An integrated cortical 
layer for orientation enhancement, submitted to IEEE-JSSC. 

EXCITATORY FEEDBACK CIRCUITS 
215 
[16] E. Vittoz and X. Arreguit. Linear networks based on transistors. Elec- 
tronics Letters, 29(3):297 299, February 1993. 
[17] E. A. Vittoz. MOS transistors operated in the lateral bipolar mode and 
their application in CMOS technology. IEEE Journal of Solid State Cir- 
cuits, SC-18(3), June 1983. 

0 
ASYNCHRONOUS 
COMMUNICATION OF 2D MOTION 
INFORMATION USING 
WINNER-TAKES-ALL ARBITRATION 
Zaven Kalayjian and Andreas G. Andreou 
Department of Electrical and Computer Engineering;, 
Johns Hopkins University, 
Baltimore, MD 21218 
zavenÂ©olym pus.ece.j hu.ed u 
10.1 
THE ASYNCHRONOUS WAY 
Synchronous quantization (data conversion), transmission, and processing of 
information are today common engineering practices. These practices have 
evolved in an era where system components were designed and optimized inde- 
pendantly of each other but with a standard interface. 
Recent trends towards parallel and distributed processing, as well as high 
levels of system integration, have blurred the boundaries between computation 
and communication. Often, computation and communication resources are 
shared by individual processor nodes. In a distributed processing system when 
there is a-priori knowledge that not all nodes are likely to require computa- 
tion/communication resources at the same time, a fixed time-slot (synchronous) 
allocation of resources among all nodes is wasteful. If the demand for resources 
is bursty, computation/communication can be done asynchronously. This is 
also true for the nervous system, where neurons actively generate their own 
output signals when they have salient information to transmit [3]. 
Mahowald [51] and Sivilotti [17], demonstrated this important aspect of 
neural processing for inter-chip communication. They employed [11] binary-tree 
arbitration using asynchronous digital circuits and a full handshaking protocol. 

218 
NEUROMORPHIC SYSTEMS ENGINEERING 
A closed loop system (full handshaking) is one way to perform this task and 
warrants, in theory, a zero probability of information loss. 
If communication bit-rate requirements are low, an open loop system (no 
handshaking) is a possible solution as well. Open loop communication trades 
off the zero probability of loosing relevant information to the simplicity in 
encoder/decoder design. A neuromorphic free-for-all asynchronous communi- 
cation scheme was proposed and experimentally demonstrated for one dimen- 
sional arrays by Andreou and Edwards [14]. Their aim was to communicate 
the position of a bright spot of a slow moving object in a photoreceptor field 
- a task which has very low bit rate requirements. In the centroid computa- 
tion/communication system, the channel encoding was performed by employ- 
ing an array of randomly firing neurons (relaxation oscillators firing in random 
phase and frequency) which were modulated by the stimulus. 
An alternative scheme was proposed and demonstrated experimentally in 
two dimensions by Mortara and Vittoz [14], whose theoretical analysis showed 
that we can trade off channel capacity for a simple encoding scheme that al- 
lows the possibility of collisions and loss of information. Boahen [1, 4] analyzed 
the trade-offs between encoding complexity and bandwidth and concluded that 
arbitered schemes are preferable. More importantly, he achieved reliable com- 
munication with a cycle time of a few tens of nanoseconds in a reasonably 
large-sized retinomorphic-vision architecture [3]. A communication architec- 
ture for stochastically encoded signals was also developed by Abusland and 
colleagues [1]. 
Figure ]0.]. 
Photomicrograph of the AR chip. 

WINNER-TAKES-ALL ARBITRATION 
219 
Our asynchronous retina [10] (AR), based on a 2D array of motion encoding 
phototransduction neurons, qua pixels, employs a full handshaking protocol 
(Figure 10.1). It is different from previous fully handshaked implementations 
in that it uses winner-takes-all (WTA) arbitration, a form of arbitration more 
neuromorphic than binary-trees. A similar scheme was proposed independently 
in Marienborg and associates [13]. Our AR comprises an arbitration circuit and 
an array of pixels, and is connected to a Macintosh computer that acts as an 
off-chip receiving system. 
10.2 
AR SYSTEM ARCHITECTURE 
tâ¢.oâ¢.., 
oo,~o, 
~ 
~ 
.... 
ReÂ¢olvor 
~ ............................ 
~ 
.......... 
Acknowledge 
Figure 10.2 
System architecture. An array of phototransducers transmits motion data 
asynchronously via Winner-Take-All arbiters. 
The system architecture of the AR chip is depicted in Figure 10.2. Data 
transmitted from the AR to the receiver constitutes a temporal stream of pixel 
addresses (bundled data). The receiver reads the address of a communicat- 
ing pixel on the address-output bus, and acknowledges receipt on the receiver 
acknowledge line. 
Information processing in the pixels can be divided into three categories: 
adaptive phototransduction, motion encoding/quantization, and communica- 
tion processing. The adaptive phototransducer [8] senses temporal changes in 
light intensity while maintaining relative immunity to the absolute radiance 
from the imaged scene thus providing an invariant representation of the image. 
The motion encoder circuit produces an output when local temporal intensity 
changes are sensed, thus providing a rudimentary form of data encoding to 
yield what is essentially a motion sensitive system. 
The output of the motion encoding circuitry is a current I, ntn which feeds 
into capacitor C,~m. A threshold is applied to the accumulated charge on 

220 
NEUROMORPHIC 
SYSTEMS ENGINEERING 
C,~e,~ which reduces the signal to a binary value, Qo~t. In this way, its function 
is like the soma of a neuron which receives and integrates dendritic input and 
produces a spike on the axon. The quantizer has memory (C,~e,~), which means 
a spiking pixel will continue to vie for access to the Address Output bus until it 
is finally selected and reset. A more detailed picture of the quantizer is found 
in Figure 10.3. 
Quantizer 
Cmem,F~[~ ~0 
- 
~ Gout 
Qin 
-\ 
I Imtn' 
W 
( Qreset 
v 
Motion Encoder 
Figure 10.3 
Motion encoder and quantizer. The shaded elements (Cfb and inverter) are 
not present in the circuit discussed in this paper. They are part of an improved quantizer 
design. 
The communication processor (Figure 10.4) is responsible for handshaking 
with the WTA arbiters and with the receiver. The communication processor 
also broadcasts the state of a selected pixel - that is, whether the pixel is active 
or has been reset - via the pixel state line. 
WTA arbiters in the periphery regulate pixel communication with the off- 
chip receiver. Arbitration ensures that only one pixel is selected to load its 
address onto the address-output bus at any given time. Whereas other asyn- 
chronous systems have appealed to digital circuits in their design of arbiters 
- for example, using binary trees of cross-coupled NAND gates - WTA ar- 
biters represent a more neuromorphic approach to arbitration. The WTA cells 
are cascaded, voltage-mode, analog circuits (Figure 10.5) based on the single- 
transistor current-conveyor [2]. The second stage in the cascade is designed 
with positive feedback from the output (transistor Mfb), so that, once a pixel 

WINNER-TAKES-ALL ARBITRATION 
221 
A~ 
v~ 
v~ck, 
Hack: iH k, 
~Vrea, iIl_)Hrea, "~.~Pixel 
L 
2~eqi 2~qJ ~ " State 
@ 
= 
........................... 
Figure 10.4 
Circuit schematic of the communication processor for pixeli,j. Each pixel in 
a row (or column) is connected to the VreqÂ¢ (or Hr~qj) line, but the pull-down transistors 
in the shaded box are common to all pixles. 
Vdd~ 
Req~ 
I\~~ 
F~ght~ ~'~]Z 
F 
~--t~ 
Figure 10.5 
FightB~ 
~-~-,~--~0-~I 
Acki 
~l__ 
wT~ 
Reset 
Winner-Take-All Arbiter cell. 
wins, competition is forcibly ended and no subsequently firing pixel can be se- 
lected [99]. Latching the WTA arbiters ensures closed-loop operation; that is, 
until the WTA arbiters are reset, communication is locked. 
Cascading in the WTA cells creates two layers of competition. 
In each 
layer, the WTA units are biased by a common resistive network but compete 
on separate nodes, FightA and FightB. Figure 10.6 shows a simulation of a 
sixteen-unit arbiter. The outputs of the first and second stage of the WTA 

222 
NEUROMORPHIC SYSTEMS ENGINEERING 
~.0_-- . . . . . . . . .  
~ 
A 
- 
i 
2 
1 
~ 
: 
: 
~E~ 
TL 
~ 0 _-- 
3 
~ 
~o ~ 
..... 
.... 
...... 
~ 
N 
~ -  
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
~ 
.
.
.
.
.
.
 
,J 
~.8~5~ 
~ 
INTI~ 
v 
: 
~ 
.
.
.
.
 
~ 
A 
~ ..... 
: 
~ 
~. 
~ 
~ o : 
~ 
=.~:~..._:::~_:~:.~ 
~ 
T 
- 
~ 
~ 
........ 
_ ............ 
NT~ 
~ 
2 o ~ ....... 
: 
~
~
~
:
-
~
-
~
 
......... 
~ ....... 
~ *~4~ 
.... - 
1 
0 : 
.
.
.
.
.
.
.
.
 
: 
~ 
~'~-:~-~ 
........ 
....... 
= 
~INT~- 
- 
- 
- 
~ .:%:: .... ~ ~ ......... 
~ 
...... 
z ~ 
. . . . . .  
o.: 
, 
~ ~ 
~ 
~ ~ 
~c~4 
N 
~CK5 
~ / ~  
= ~- 
~ 
~ ~CKG 
1.0 
...... 
~ 
: ~ 
...... 
~ 
: 
~C~F 
o i 
, 
, 
, 
~ 
,' 
, 
, 
~ ~--, 
~ 
~ 
~ 
;* .......... 
l ou 
1 2ou 
i qou 
~oo 
o~ 
TIH~ 
ZLI~] 
1 ~OU 
Figure I0.6 
Simulation of a sixteen-unit WTA arbiter. Top: The input signal to all sixteen 
units. Middle: Competition in the first layer of the WTA arbiter. Bottom: Output of WTA 
arbiter. 
cells are represented in panel two and panel three, respectively. The same 
input signal, REQ, is given to all sixteen inputs of the WTA arbiter. In the 
first stage, the effect of the resistive-network biasing the WTA units can be 
seen separating the signals. Rather than waiting for equally biased WTA units 
to slowly resolve a winner, this tilt in the biasing quickly sorts out a large 
backlog of pixels requesting arbitration - for example, if the sensor undergoes 
ego-motion causing nearly all the pixels to fire simultaneously. Since the output 
of the first layer of competition becomes the input to the second layer of WTA 
units, the second stage of the cascade chooses a winning pixel based on the 
competition it sees from the first stage. A winner is decided by the second 
stage long before competition in the first stage has ended. 
A typical handshaking cycle with an off-chip receiver is as follows. Pixel 
(2,1) senses a temporal change in light intensity that activates its motion en- 
coder/quantizer. Qo~t of the pixel goes low, and the pixel makes a communi- 
cation request to the vertical WTA arbiter (VWTA) along the V,-eq2 line. The 
VWTA arbitrates, acknowledges the requesting row by lowering line Vack2, and 
simultaneously communicates with the vertical-address encoder, which loads 

WINNER-TAKES-ALL ARBITRATION 
223 
the vertical coordinate (0010) of the firing pixel onto the address-output bus. 
The communication processor of the active pixel acknowledges the VWTA by 
raising HrÂ¢ql, which initiates arbitration by the horizontal WTA (HWTA). The 
HWTA selects a winner, responds along the Ha~kl line, and communicates with 
the horizontal-address encoder, which loads the horizontal coordinate (0001) of 
the firing pixel. At this point, a data-valid signal is generated by the address 
encoders to alert the receiver that the complete address of the communicating 
pixel has been loaded onto the address output bus. 
The receiver reads the address and acknowledges the transmission through 
the receiver-acknowledge port. The receiver-acknowledge is transmitted to all 
pixels, but only the communicating pixel - the one receiving Va~k and Ha~ 
signals from the WTA arbiters - can be reset. Resetting a pixel consists of 
charging up Ct~ak, thus bringing Qr~s~t low. A pixel that has been selected and 
reset will remain dormant for as long as the capacitor CtÂ¢~k is charged - a time 
determined by the leak bias. To ensure a complete loop, each pixel notifies 
the array of its state via the pixel state line, which prevents the arbiters from 
being reset before the pixel has been reset. Once the pixel has been reset (pixel 
state goes low), the pixel state signal ANDs with the receiver acknowledge 
to reset the WTA arbiters. 
When the WTA arbiters are reset, they cease 
communications with the address encoders, which remove the data from the 
address output and reset the data-valid line. 
Resetting the data-valid line 
concludes the communication cycle, and the sender is ready to communicate 
again. 
10.3 
EXPERIMENTAL RESULTS 
We fabricated two chips in 2#m-CMOS processes through MOSIS, and assem- 
bled a receiver system on a Macintosh platform. 
The first chip, a full AR, consists of an array of 108 (9 Ã 12) pixels fabri- 
cated in an n-well process. The AR is capable of generating a video output 
of the phototransducer activity, as well as asynchronously communicating with 
a receiver system. The receiver consists of a Macintosh Quadra 840AV fitted 
with a National Instruments Lab-NB data-acquisition board. We developed 
custom software that allows the Lab-NB board to receive data from the test 
chips, to display graphically the pixel address, and to return an acknowledge 
signal. 
We measured asynchronous-communication speeds of 3kHz between 
AR and computer, which was the maximum rate given the receiver's hardware 
and software limitations. Snapshot images of the output in synchronous-video 
and asynchronous-communication modes are shown in Figure 10.7. In another 
experiment, we removed the Macintosh from the communication loop by feed- 
ing back a slightly delayed version of the data-valid signal into the receiver- 
acknowledge port of the chip using two inverters and a capacitor. With this 
configuration, we recorded communication rates of 400kHz (Figure 10.8). 
Our circuit layout severely affected the functionality of the system. Charge 
pumping rendered control of the pixel's refractory states ineffectual. Charge 

224 
NEUROMORPHIC SYSTEMS ENGINEERING 
| 
I 
Figure 10.7 
Sequence of images from the AR looking at a diagonal edge moving from 
bottom right to top left. Top row: Video output of the adaptive phototransducers. Bottom 
row: Asynchronous output. Black squares indicate communication activity, gray squares 
indicate pixels which have fired previously, white indicates pixels which have not fired. 
15 
10 
Handshaking with Macintosh 
DVal 
Ack 
, . , ~_~ 
I 
5 
1 
115 
Time (ms) 
2.5 
15 
10 
0 ~ 
0 
Handshaking with Inverters 
, 
, 
, 
, 
~ 
~ DVal 
Ack 
i 
i 
i 
~ 
5 
10 
15 
20 
25 
Time (gs) 
Figure 10.8 
Measured handshaking data. Ac/~ is the Receiver Acknowledge signal and 
Dval is Data Valid. 

WINNER-TAKES-ALL ARBITRATION 
225 
stored on Czeak was quickly depleted by transistor switching, so refractory 
times were always negligible. Furthermore, since the pixels were not completely 
shielded by a metal layer, photogenerated leakage currents caused sporadic 
firing in the pixel array. Also, power consumption was unmeasurable due to 
parasitic light-emitting structures on the chip [15]. 
We fabricated, in a p-well process, a second test chip that contained an 
isolated 16-input WTA circuit. We wrote diagnostic software to test 64 possible 
combinations of WTA inputs. The arbitration time of the 16-input WTA was 
approximately 18ns, which compares favorably with our simulations. Of the 
64 input combinations possible for this test setup, three invalid outputs were 
produced at 18ns arbitration rates. 
10.4 
DISCUSSION 
Although our measured results were encouraging, considering the various 
layout-related problems mentioned in Section 10.3, there are two potential im- 
provements to this design. 
First, we can accelerate pixel firings by including feedback in the quantizer. 
It is well known that biological neurons have a positive-feedback mechanism 
(sodium channels) that forces rapid depolarization of the soma once the spiking 
threshold has been crossed. We can implement a similar feedback mechanism 
in the quantizer to speed up spiking of the pixel by including positive feedback 
from Qo~,t to Q,.es~t. The shaded capacitor and inverter in Figure 10.3 shows 
the improved quantizer circuit. 
Second, pipelining of the communication process can greatly enhance perfor- 
mance, as it has in other designs (c.f. Boahen). In brief, we can use pipelining to 
speed up communication in the following way: After arbitration by the VWTA, 
the entire selected row is loaded into a register, from which the HWTA arbi- 
trates. Meanwhile, the VWTA arbiter is available to the pixel array for another 
arbitration cycle. Additional communication circuitry between the pixels and 
the WTA arbiters, as well as between the WTA arbiters and the receiver, will 
be required. 
10.5 
CONCLUSIONS 
We have presented a new design of an asynchronous, fully handshaked, address- 
event-based system. The AR uses adaptive phototransducers, and commu- 
nicates asynchronously through a hierarchical arbitration scheme based on 
voltage-mode WTA arbiters. 
Through the AR, we have shown that WTA 
arbiters - whose analog mode of operation resembles more closely the biologi- 
cal communication system we are trying to emulate - can be used effectively in 
an asynchronous environment. The WTA arbiters also offer processing possi- 
bilities that digital arbitration schemes cannot offer; for example, the AR can 
implement attentional behaviors using distributed hysteretic feedback into the 
WTA inputs [9]. 

226 
NEUROMORPHIC SYSTEMS ENGINEERING 
Acknowledgments 
This work was supported by an ONR Multidisciplinary University Research Initiative 
(MURI) for Automated Vision and Sensing Systems N00014-95-1-0409. Testing of 
the system and preparation of this document were performed while the authors were 
visiting the Physics of Computation Laboratory at Caltech. We thank Professor 
Carver Mead for providing a stimulating laboratory environment for this work. 
References 
[1] A. Abusland, T. Lande, and M. H0vin. A VLSI communication architec- 
ture for stochastically pulse-encoded analog signals. In Proceedings of the 
IEEE International Symposium on Circuits and Systems, volume 3, pages 
401 404, Atlanta, GA, 1996. 
[2] A. G. Andreou and K. A. Boahen. Analog VLSI signal and information 
processing. In M. Ismail and T. Fiez, editors, Neural information process- 
ing II, pages 358-413. McGraw-Hill, New York, 1994. 
[3] H. B. Barlow. Possible principles underlying the transformations of sensory 
messages. In W.A. Rosenblit, editor, Sensory Communications. MIT Press, 
Cambridge, MA, 1961. 
[4] K. Boahen. Retinomorphic vision systems. In Int. Conf. on Microelec- 
tronics for Neural Networks, volume 16-5, pages 30 39, Los Alamitos, CA, 
1996. EPFL/CSEM/IEEE. 
[5] K. Boahen. Retinomorphic vision systems I: Pixel design. In Proceedings 
of the IEEE International Symposium on Circuits and Systems, volu~ne 
supplement, pages 14 19, Atlanta, GA, May 1996. 
[6] K. Boahen. 
Retinomorphic vision systems II: Communication channel 
design. In Proceedings of the IEEE International Symposium on Circuits 
and Systems, volume supplement, pages 9-14, Atlanta, GA, May 1996. 
[7] G. Cauwenberghs and V. Pedroni. A charge-based CMOS parallel analog 
vector quantizer. In Advances in Neural Information Processing Systems, 
volume 7, pages 779-786, Cambridge, MA, 1995. MIT Press. 
[8] T. Delbriick and C. A. Mead. Analog VLSI phototransduction. CNS Memo 
No 30, may 1994. 
[9] S. P. DeWeerth. Analog VLSI circuits for stimulus localization and centroid 
computation. 
International Journal of Computer Vision, 8(3):191 202, 
1992. 
[10] Z. Kalayjian, J. Waskiewicz, D. Yochelson, and A. Andreou. Asynchronous 
sampling of 2d arrays using winner takes all circuits. In Proceedings of the 
IEEE International Symposium on Circuits and Systems, volume 3, pages 
393 396, Atlanta, GA, 1996. 
[11] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie. 
Silicon auditory processors as computer peripherals. 
IEEE Journal of 
Neural Networks, 4(3):523 528, 1993. 

WINNER-TAKES-ALL ARBITRATION 
227 
[12] M. Mahowald. VLSI Analogs of Neuronal Visual Processing: a Synthesis of 
Form and Function. Computation and neural systems, California Institute 
of Technology, 1992. 
[13] J. Marienborg, T. S. Lande, /~. Absuland, and M. Hovin. An analog ap- 
proach to 'neuromorphic' communication. In Proceedings of the IEEE In- 
ternational Symposium on Circuits and Systems, volume 3, pages 397-400, 
Atlanta, GA, 1996. 
[14] A. Mortara, E. A. Vittoz, and P. Venier. A communication scheme for 
analog VLSI perceptive systems. IEEE Journal of Solid State Circuits, 
SC-30(6):660-669, June 1995. 
[15] A. Obeidat, Z. Kalayjian, A. Andreou, and J. Khurgin. A model for visible 
photon emission from reverse-biased silicon p-n juncions. Applied Physics 
Letters, 70(4):470 471, 1997. 
[16] M. Sivilotti. Wiring considerations in analog VLSI systems with applica- 
tions to field programmable networks. PhD thesis, California Institute of 
Technology, Pasadena CA, 1991. 

11 
COMMUNICATING NEURONAL 
ENSEMBLES BETWEEN 
NEUROMORPHIC CHIPS 
Kwabena A. Boahen 
Physics of Computation Laboratory, MS 136-93, 
California Institute of Technology, 
Pasadena CA 91125 
buste@pcmp.caltech.edu 
11.1 
TIME-DIVISION MULTIPLEXING 
The small number of input-output connections available with standard chip- 
packaging technology, and the small number of routing layers available in VLSI 
technology, place severe limitations on the degree of intra- and interchip con- 
nectivity that can be realized in multichip neuromorphic systems. Inspired by 
the success of time-division multiplexing in communications [16] and computer 
networks [19], many researchers have adopted multiplexing to solve the con- 
nectivity problem [12, 67, 17]. Multiplexing is an effective way of leveraging 
the 5 order-of-magnitude difference in bandwidth between a neuron (hundreds 
of Hz) and a digital bus (tens of megahertz), enabling us to replace dedicated 
point-to-point connections among thousands of neurons with a handful of high- 
speed connections and thousands of switches (transistors). This approach pays 
off in VLSI technology because transistors take up a lot less area than wires, 
and are becoming relatively more and more compact as the fabrication process 
scales down to deep submicron feature sizes. 
Four important performance criteria for a communication channel that pro- 
vides virtual point-to-point connections among arrays of cells are: 
Capacity: The maximum rate at which samples can be transmitted. It is 
equal to the reciprocal of the minimum communication cycle period. 

230 
NEUROMORPHIC SYSTEMS ENGINEERING 
Latency: The mean time between sample generation in the sending popula- 
tion and sample reception in the receiving population. 
Temporal Dispersion: The standard deviation of the channel latency. 
Integrity: The fraction of samples that are delivered to the correct destina- 
tion. 
All four criteria together determine the throughput, which is defined as the 
usable fraction of the channel capacity, because the load offered to the channel 
must be reduced to achieve more stringent specifications for latency, temporal 
dispersion, and integrity. 
As far as neuromorphic systems [1] are concerned, a sample is generated 
each time a neuronal spike occurs. These spikes carry information only in their 
time of occurence, since the height and width of the spike is fixed. We must 
make the time it takes to communicate the occurence of each spike as short as 
possible, in order to maximize the throughput. 
The latency of the sending neuron should not be confused with the channel 
latency. Neuronal latency is defined as the time interval between stimulus onset 
and spiking; it is proportional to the strength of the stimulus. Channel latency 
is an undesirable systematic offset. Similarly, neuronal dispersion should not 
be confused with channel dispersion. Neuronal dispersion is due to variabil- 
ity between individual neurons; it is inversely proportional to the strength of 
the stimulus. Channel dispersion is additional variability introduced by uneven 
access to the shared communication channel. Hence, channel latency and chan- 
nel dispersion add systematic and stochastic offsets to spike times and reduce 
timing precision. 
A growing body of evidence suggest that biological neurons have submil- 
lisecond timing precision and synchronize their firing, making it imperative 
to minimize channel latency and dispersion. Although neuronal transmission 
has been shown to be unreliable, with failure occuring at axonal branches and 
synapses, it is most likely that each spike changes the local state of the axon or 
synapse--even if it is rejected--and thereby determines the fate of subsequent 
spikes. So the fact that communication in the nervous system is unreliable 
does not give us license to build an imperfect communication channel, as the 
decision whether or not to transmit a spike is not arbitrary. 
There are several alternatives to using the timing of fixed-width/fixed-height 
pulses to encode information, and several approaches to optimizing channel 
performance ~s shown in Table 11.1; the choices I have made are highlighted. 
I attempt to justify my choices by introducing a simple population activity 
model in Section 11.2. 
I use this model to quantify the tradeoffs faced in 
communication channel design in Section 11.3. The model assumes that the 
activity in the pixel array is whitened (i.e. activity is clustered in space and 
in time). Having motivated my approach to pixel-parallel communication, I 
describe the implementation of a pipelined communication channel, and show 
how a retinomorphic chip is interfaced with another neuromorphic chip in Sec- 
tion 11.4. The paper concludes with a discussion in Section 11.5. Parts of this 

NEUROMORPHIC CHIPS 
231 
Table 11.1 
Time-Multiplexed Communication Channel Design Options. The choices made 
in this work are highlighted. 
Specification 
Approaches 
Remarks 
Activity 
Encoding 
Pulse Amplitude 
Pulse Width 
Pulse Code 
Pulse Timing 
Long settling time and static power 
dissipation 
Channel capacity degrades with 
increasing width 
Inefficient at low precision (< 6 bits) 
Uses minimum-width, rail-to-rail 
pulses 
Latency 
Polling 
Event-driven 
Latency e( Total number of neurons 
Latency e( Number currently active 
Integrity 
Collision Rejection 
Arbitration 
Collisions increase exponentially with 
throughput 
Reorder events to prevent collisions 
Dispersion 
Dumping 
Queueing 
New events are given priority ~ No 
dispersion 
Dispersion e( 1/capacity, at 
constant throughput 
Capacity 
Hard-wired 
Simple ~ Short cycle time 
Pipelined 
Cycle time set by a single stage 
work have been described previously in conference proceedings [1, 4] and in a 
magazine article [186]. 
11.2 
POPULATION ACTIVITY MODEL 
Although a fairly general purpose implementation was sought, my primary 
motivation for developing this communication channel is to read pulse trains off 
a retinomorphic imager chip [1]. Therefore, the channel design was optimized 
for retinal population activity, and an efficient and robust solution that supports 
adaptive pixel-parallel quantization was sought. 
11.2.1 Retinal Processing 
The retina converts spatiotemporal patterns of incident light into spike trains. 
Transmitted over the optic nerve, these discrete spikes are converted back into 
continuous signals by dendritic integration of excitatory postsynaptic potentials 
in the lateral geniculate nucleus of the thalamus. For human vision, contrast 
thresholds of less than 1%, processing speeds of about 20 ms per stage, and 

232 
NEUROMORPHIC SYSTEMS ENGINEERING 
temporal resolution in the submillisecond range are achieved, with spike rates 
as low as a few hundred per second. No more than 10 spikes per input are 
available during this time. The retina achieves such high peprformance by 
minimizing redundancy and maximizing the information carried by each spike. 
The retina must encode stimuli generated by all kinds of events efficiently, 
over a large range of lighting conditions and stimulus velocities. These events 
fall into three broad classes, listed in order of decreasing probability of occur- 
rence: 
1. Static events: Generate stable, long-lived stimuli; examples are buildings 
or trees in the backdrop. 
2. Punctuated events: Generate brief, short-lived stimuli; examples are a 
door opening, a light turning on, or a rapid, short saccade. 
3. Dynamic events: Generate time-varying, ongoing stimuli; examples are 
a spinning wheel, grass vibrating in the wind, or a smooth-pursuit eye 
movement. 
In the absence of any preprocessing, the output activity mirrors the input 
directly. Changes in illumination, which influence large areas, are reflected 
directly in the output of every single pixel in the region affected. Static events, 
such as a stable background, generate persistent activity in a large fraction 
of the output cells, which transmit the same information over and over again. 
Punctuated events generate little activity and are transmitted without any 
urgency. Dynamic events generate activity over areas far out of proportion to 
informative features in the stimulus, when the stimulus rapidly sweeps across 
a large region of the retina. Clearly, these output signals are highly correlated 
over time and over space, resulting in a high degree of redundancy. Hence, 
reporting the raw intensity values makes poor use of the limited throughput of 
the optic nerve. 
The retina has evolved exquisite filtering and adaptation mechanisms to 
improve coding efficiency, six of which are described briefly below[6, 7]: 
1. Local automatic gain control at the receptor level eliminates the depen- 
dence on lighting--the receptors respond to only contrast--extending the 
dynamic range of the retina's input without increasing its output range. 
2. Bandpass spatiotemporal filtering in the first stage of the retina (outer 
plexiform layer or OPL) attenuates signals that do not occur at a fine 
spatial or a fine temporal scale, ameliorating redundant transmission of 
low frequency signals and eliminating noisy high frequency signals. 
3. Highpass temporal and spatial filtering in the second stage of the retina 
(inner plexiform layer or IPL) attenuates signals that do not occur at 
a fine spatial scale and a fine temporal scale, eliminating the redundant 
signals passed by the OPL, which responds strongly to low temporal 
frequencies that occur at high spatial frequencies (sustained response to 

NEUROMORPHIC CHIPS 
233 
static edge) or to low spatial frequencies that occur at high temporal 
frequencies (blurring of rapidly moving edge). 
4. Half-wave rectification, together with dual-channel encoding (ON and 
OFF output cell types), in the relay cells between the OPL and the IPL 
(bipolar cells) and the retina's output cells (ganglion cells) eliminates the 
elevated quiescent neurotransmitter release rates and firing rates required 
to signal both positive and negative excursions using a single channel. 
5. Phasic transient-sustained response in the ganglion cells avoids temporal 
aliasing by transmitting rapid changes in the signal using a brief, high 
frequency burst of spikes, and, at the same time, avoids redundant sam- 
pling, by transmitting slow changes in the signal using modulation of a 
low sustained firing rate. 
6. Foveated architecture, together with actively directing the gaze (saccades), 
eliminates the need to sample all points in the scene at the highest spatial 
and temporal resolution, while providing the illusion of doing so every- 
where. The cell properties are optimized: smaller and more sustained in 
the fovea (parvocellular or X cell type), where the image is stabilized by 
tracking, and larger and more transient in the periphery (magnocellular 
or Y cell type), where motion occurs. 
The resulting activity in the ganglion cells, which convert these preprocessed 
signals to spikes, and transmit the spikes over the optic nerve, is rather different 
from the stimulus pattern. For relatively long periods, the scene captured by 
the retina is stable. These static events produce sparse activity in the OPL's 
output, since the OPL does not respond to low spatial frequencies, and virtually 
no activity in the IPL's output, since the IPL is selective for temporal frequency 
as well as spatial frequency. The OPL's sustained responses drive the 50,000, or 
so, ganglion cells in the fovea, allowing the fine details of the stabilized object 
to be analyzed. While the vast majority of the ganglion cells, about 1 million 
in all, are driven by the IPL, and fire at extremely low quiescent rates of 10S/s 
(spikes per sec), or less, in response to the static event. 
When a punctuated event (e.g. a small light flash) occurs, the IPL responds 
strongly, since both high temporal frequencies and high spatial frequencies are 
present, and a minute subpopulation of the ganglion cells raise their firing rates, 
briefly, to a few hundred spikes per second. A dynamic event, such as a spinning 
windmill, or a flash that lights up an extended region, is effectively equivalent 
to a sequence of punctuated events occurring at different locations in rapid 
succession, and can potentially activate ganglion cells over an extended region 
simultaneously. This is indeed the case for the OPL-driven cells but is not true 
for the IPL-driven cells, which cover most of the retina, because the low spatial 
frequencies produced in the OPL's output by such a stimulus prevent the IPL 
from responding. 
In summary, the activity in the optic nerve is clustered in space and time 
(whitened spectrum), consisting of sporadic short bursts of rapid firing, trig- 

234 
NEUROMORPHIC SYSTEMS ENGINEERING 
gered by punctuated and dynamic events, overlaid on a low, steady background 
firing rate, driven by static events. 
11.2.2 
The Neuronal Ensemble 
We can describe the activity of a neuronal population by an ordered list of 
locations in spacetime 
~ 
= 
{(~:olto),(:~l;tl),...(~:~;td,...}; 
to <tl <'"t~ 
< ..., 
where each coordinate specifies the occurrence of a spike at a particular loca- 
tion, at a particular time. The same location can occur in the list several times 
but a particular time can occur only once--assuming time is measured with 
infinite resolution. 
There is no need to record time explicitly if the system that is logging this 
activity operates on it in real-tim~only the location is recorded and time 
represents itself. In that case, the representation is simply 
~" = {X0, Xl,...Xi,...}; 
to < tl < "''ti < "''. 
This real-time representation is called the address-event representation (AER) 
[12, 17]. I shall present more details about AER later. At present, my goal 
is to develop a simple model of the probability distribution of the neuronal 
population activity described by g. 
g has a great deal of underlying structure that arises from events occurring 
in the real world, to which the neurons are responding. 
The elements of g 
are clustered at temporal locations where these events occur, and are clustered 
at spatial locations determined by the shape of the stimulus. 
Information 
about stimulus timing and shape can therefore be obtained by extracting these 
clusters, g also has an unstructured component that arises from noise in the 
signal, from noise in the system, and from differences in gain and state among 
the neurons. This stochastic component limits the precision with which the 
neurons can encode information about the stimulus. 
We can use a much more compact probabilistic description for g if we know 
the probability distributions of the spatial and temporal components of the 
noise. In that case, each cluster can be described explicitly by its mean spatial 
configuration and its mean temporal location, and the associated standard 
deviations. 
~ 
'~ 
{ (~0, 17x0; ~0, Crt0), (~1, (Tx 1 ; ~i, (Tt 1), 
â¢ .., (~, ~; 
~, ~td, 
â¢ â¢ â¢, }; 
to <tl...t~ <.... 
I shall call these statistically-defined clusters neuronal ensembles. The ith neu- 
ronal ensemble is defined by a probability density function pi(z; t) with param- 
eters â¢ ~, axi, ~, ~ti. The probability that a spike generated at location x at 
time t is a member of the ith ensemble is then obtained by computing pi(x; t). 

NEUROMORPHIC CHIPS 
235 
In what follows, I will assume that the distribution of the temporal compo- 
nent of the noise is Gaussian and the latency (the mean minus the stimulus 
onset time) and the standard deviation are inversely proportional to the stim- 
ulus strength. We can measure the parameters of this distribution by fitting 
a Gaussian to the normalized poststimulus time histogram, as shown in Fig- 
ure ll.la. Here, latency refers to the time it takes to bring a neuron to spiking 
threshold; this neuronal latency is distinct from the channel latency that I 
defined earlier. 
The Gaussian distribution arises from independent random 
variations in the characteristics of these neurons--whether carbon or silicon 
based--as dictated by the Central Limit Theorem. 
.~-~, 
Probability Density 
:~ 
~ 
1 
2 ,/:/ 
Â¢---'~ ~/3a 
O" 
"/~-'2~" 
I "~ "/,j2Y 
~ t 
o 
"/,-'2~" 
t'~ 
"/,j2Y 
(~) 
(b) 
â¢ 
t 
Figure 11.1 
The hypothetical distribution, in time, of samples generated by a subpopula- 
tion of neurons triggered by the same stimulus at t -= 0. (a) Normalized poststimulus time 
histogram with Gaussian fit: 8 is the number of samples per bin, /k is the bin-width, and N 
is the total number of samples; # and (7 are the mean and standard deviation of the fitted 
(~aussian. (b) Rectangular distribution with the same mean and standard deviation as the 
Gaussian. After redistributing samples uniformly, the sampling rate is only 72% of the peak 
rate reached in the original distribution, yielding the minimum capacity required to transmit 
the burst without dispersion. 
The ratio between the standard deviation and the latency, which is called 
the coefficient of variation (cov), is used here as a measure of the variability 
among the neurons. Other researchers have used the cov to measure mismatch 
among transistors and the variability in the steady firing rates of neurons; I use 
it to measure variability in latency across a neuronal ensemble. As the neuronal 
latency decreases, the neuronal temporal dispersion decreases proportionally, 
and hence the cov remains constant. The cov is constant because the charac- 
teristics that vary between individual neurons, such as membrane capacitance, 
channel conductance, and threshold voltage, determine the slope of the latency 
versus input current curves 
rather than the absolute latency or firing rate. 
11.3 
DESIGN TRADEOFFS 
Several options are available to the communication channel designer. Should 
he preallocate the channel capactiy, giving a fixed amount to each node, or 
allocate capacity dynamically, matching each nodes allocation to its current 
needs? Should she allow the users to transmit at will, or implement elaborate 

236 
NEUROMORPHIC SYSTEMS ENGINEERING 
mechanisms to regulate access to the channel? And how does the distribution 
of activity over time and over space impact these choices? Can he assume that 
nodes act randomly, or are there significant correlations between the activities 
of these nodes? I shed some light on these questions in this section, and provide 
some definitive answers. 
Two-Layer Network 
ITT 
INPUTS 
. 
_~II 
1 3 
2 2 2  
" 
â¢ 
~ 
IIII 
_~ 
~o p 
,o 
~ 
_~11 
I1 
k. 
LAYER 1 
LAYER 2/ \/\/ 
OUTPUTS 
CHIP 1 
.~II 
I 
2 2 2 
.~ IIII 
9 ~ .o 
_~II 
I T t LAYER 1 
INPUTS 
Two-Chip Implementation 
CHIP 2 
321322123 1-~ 
l 
, 
time 
II 
I 
xÂ¢ 
N 
~ 
III I 
~Â¢ 
N 
~Â¢ 
II 
I 1 
,~ 
LAYER 2Z 
k 
OUTPUTS 
Figure 11.2 
Two-chip implementation of a two-layer spiking neural network (Adapted 
from [12]). In the two-layer network, the origin of each spike is infered from the line on 
which it arrives. This parallel transmission uses a labeled-line representation. In the two- 
chip implementation, the neurons share a common set of lines (digital bus) and the origin 
of each spike is preserved by broadcasting one of the labels. This serial transmission uses 
an address-event representation (AER). No information about the height or width of 
the spike is transmitted, thus information is carried by spike timing only. A shared address- 
event bus can transparently replace dedicated labeled lines if the encoding, transmission, 
and decoding processes cycle in less than A/n seconds, where/k is the desired spike timing 
precision and n is the maximum number of neurons that are active during this time. 

NEUROMORPHIC CHIPS 
237 
11.3.1 
Allocation: Dynamic or Static? 
We are given a desired sampling rate fNyq, and an array of N signals to be 
quantized. We may use adaptive, 1-bit, quantizers that sample at fNyq when 
the signal is changing, and sample at fNyq/Z when the signal is static. Let 
the probability that a given quantizer samples at fNyq be a. That is, a is the 
fraction of the quantizers whose inputs are changing. Then, each quantizer 
generates bits at the rate 
fbits ~- fNyq(a ~- (1 -- a)/Z)log 2 N, 
because a percent of the time, it samples at fNyq; the remaining (1 -a) percent 
of the time, it samples at fNyq/Z. 
Furthermore, each time that it samples, 
log 2 N bits are sent to encode the location, using the aforemention address- 
event representation (AER) [12, 17]. AER is a fairly general scheme for trans- 
mitting information between arrays of neurons on separate chips, as shown in 
Figure 11.2. 
On the other hand, we may use more conventional qunatizers that sample 
every location at fNyq, and do not locally adapt the sampling rate. In that 
case, there is no need to encode location explicitly; we simply cycle through 
all N locations, according to a fixed squence order, and infer the origin of each 
sample from its position in the sequence. For a constant sampling rate, the bit 
rate per quantizer is simply fNyq" 
Adaptive sampling produces a lower bit rate than fixed sampling if 
a < (Z/(Z - 1))(1/log2 N - l/Z). 
For example, in a 64 Ã 64 array with sampling-rate attenuation, Z, of 40, the 
active fraction, a, must be less than 6.1 percent. In a retinomorphic system, the 
adaptive neuron circuit performs sampling-rate attenuation (Z will be equal to 
the firing rate attenuation factor "y) [6], and the spatiotemporal bandpass filter 
makes the output activity sparse [6], resulting in a low active fraction a. 
It may be more important to minimize the number of samples produced per 
second, instead of minimizing the bit rate, as there are usually sufficient I/O 
pins to transmit all the bits in each sample in parallel. In that case, it is the 
number of samples per second that is fixed by the channel capacity. 
Given a certain fixed channel throughput (Fchan), in samples per second, we 
may compare the effective sampling rates, fNyq, acheived by the two strate- 
gies. For adaptive quantization, channel throughput is allocated dynamically 
in the ratio a : (1 - a)/Z between the active and passive fractions of the node 
population. Hence, 
fNyq = fchan/(a "-b (1 - a)/Z) 
(11.1) 
where fchan = F~han/N is the throughput per node. In contrast, fixed quan- 
tization achieves only f~han- For instance, if fchan ~- 100S/S, adaptive quan- 
tization achieves fNyq = 1.36KS/s, with an active fraction of 5 percent and a 
sampling-rate attenuation factor of 40. Thus, a fourteen-fold increase in tem- 
poral bandwidth is achieved under these conditions; the channel latency also is 
reduced by the same factor. 

238 
NEUROMORPHIC SYSTEMS ENGINEERING 
11.3.2 
Access: Arbitration or ~"ree-for-All? 
If we provide random access to the shared communication channel, in order 
to support adaptive pixel-level quantization [186], we have to deal with con- 
tention for channel access, which occurs when two or more pixels attempt to 
transmit simultaneously. We can introduce an arbitration mechanism to re- 
solve contention and a queueing mechanism to allow nodes to wait for their 
turn. However, arbitration lengthens the communication cycle period, reduc- 
ing the channel capacity, and queuing causes temporal dispersion, corrupting 
timing information. On the other hand, if we simply allow collisions to occur, 
and discard the corrupted samples so generated [14], we may achieve a shorter 
cycle period and reduced dispersion, but sample loss will increase as the load 
increases. 
We may quantify this tradeoff using the following well-known result for the 
collision probability [16]: 
Pcoll = 1 - e -2c, 
(11.2) 
where G is the offered load, expressed as a fraction of the channel capacity. 1 
For U < 1, U is the probability that a sample is generated during the commu- 
nication cycle period. 
If we arbitrate, we will achieve a certain cycle time, and a corresponding 
channel capacity, in a given VLSI technology. An arbitered channel can operate 
at close to 100-percent capacity because the 0.86 collision probability for G = 1 
is not a problem--users just wait their turn. Now, if we do not arbitrate, we 
will achieve a shorter cycle time, with a proportionate increase in capacity. Let 
us assume that the cycle time is reduced by a factor of 10, which is optimistic. 
For the same offered load, we have G = 0.1, and find that pco~l = 18 percent. 
Thus, the simple nonarbitered channel can handle more spikes per second only 
if collision rates higher than 18 percent are acceptable. For lower collision rates, 
the complex, arbitered channel offers more throughput, even though its cycle 
period is 1 order of magnitude longer, because the nonarbitered channel can 
utilize only 10 percent of its capacity. 
Indeed, the arbiterless channel must operate at high error rates to maximize 
utilization of the channel capacity. The throughput is Ge -2c [16], since the 
probability of a successful transmission (i.e., no collision), is e -2a. Throughput 
reaches a maximum when the success rate is 36 percent (e -~) and the collision 
rate is 64 percent. At maximum throughput, the load, U, is 50 percent and, 
hence, the peak channel throughput is only 18 percent. Increasing the load 
beyond the 50% level lowers the channel utilization because the success rate 
falls more rapidly than the load increases. 
In summary, the simple, free-for-all design offers higher throughput if high 
data-loss rates are tolerable, whereas the complex, arbitered design offers higher 
throughput when low-data loss rates are desired. And, due to the fact that the 
maximum channel untilization for the free-for-all channel is only 18 percent, 
the free-for-all channel will not be competitive at all unless it can achieve a 
cycle time that is five times shorter than that of the arbitered channel. 

NEUROMORPHIC CHIPS 
239 
Indeed, the free-for-all protocol was first developed at the University of 
Hawaii in the 1970s to provide multiple access between computer terminals 
and a time-shared mainframe over a wireless link with an extremely short cycle 
time; it is known as the ALOHA protocol [16]. In this application, a vanish- 
ingly small fraction of the wireless link's tens of megahertzs of bandwidth is 
utilized by people typing away at tens of characters per second on a few hun- 
dred computer terminals, and hence the error rates are negligible. However, 
in a neuromorphic system where we wish to service hundreds of thousands of 
neurons, efficient utilization of the channel capacity is of paramount concern. 
The inefficiency of ALOHA h~s been long recognized, and researchers have 
developed more efficient communication protocols. One popular approach is 
CSMA (carrier sense, multiple access), where each user monitors the channel 
and does not transmit when the channel is busy. The Ethernet protocol uses 
this technique, and most local-area-networks work this way. Making available 
information about the state of the channel to all users greatly reduces the num- 
ber of collisions. A collision occurs only when two users attempt to transmit 
within a time interval that is shorter than the time it takes to update the in- 
formation about the channel state. Hence, the collision rate drops if the time 
that users spend transmitting data is longer than the round trip delay; if not, 
CSMA's performance is no better than ALOHA's [16]. Lande's group at the 
CS-dept., Univ. of Oslo, Norway is developing a CSMA-like protocol for neuro- 
morphic communication to avoid the queueing associated with arbitration [1]. 
What about the timing errors introduced by queueing in the arbitered chan- 
nel? It is only fair to ask whether these timing errors are not worse than the 
data-loss errors. The best way to make this comparison is to express the chan- 
nel's latency and temporal dispersion as fractions of the neuronal latency and 
temporal dispersion, respectively. If the neuron fires at a rate fNyq, we may as- 
sume, for simplicity, that its latencies are uniformly distributed between 0 and 
TNyq = 1/fNyq. Hence, the neuronal latency is # -- TNyq/2, and the neuronal 
temporal dispersion a = TNyq/(2v/3). For this fiat distribution, the coefficient 
of variation is c -- 1/v/3, which equals 58 percent. 
To find the latency and temporal dispersion introduced by the queue, we use 
a well-known result from queueing theory which gives the moments of the time 
spent waiting in the queue, ~, 
as a function of the moments of the service 
time, ~ 
[9]: 
Ax 2 
-
-
 
W 
-- 
2(1 - G)' 
-
-
 
Ax 3 
w--~ = 
2~2+ 3(1)G 
~ ; -  
where A is the arrival rate of the samples. 2 An interesting property of the queue, 
which is evident from these results, is that the first moment of the waiting time 
increases linearly with the second moment of the service time. Similarly, the 

240 
NEUROMORPHIC 
SYSTEMS ENGINEERING 
second moment of the waiting time increases linearly with the third moment 
of the service time. 
In our case, we may assume that the service time, A, is fixed; hence x -~ = An. 
In that case, the mean and the variance of the number of cycles spent waiting 
are given by 
~ 
G 
~ 
-- 
-
-
-
 
(11.3) 
zx 
2(1-G)' 
_
_
 
2 
w 2 _ ~2 
_ 
~2 
2 
~m 
-- 
A~ 
+ gin. 
(11.4) 
~or example, at 95-percent capacity, a sample spends 9.5 cycles in the queue, 
on average. This result agrees with intuition: As every twentieth slot is empty~ 
one must wait anywhere from 0 to 19 cycles to be serviced~ which averages out 
to 9.5. Hence the latency is 10.5 cycles, including the mdditional cycle required 
for service~ ~nd the temporal dispersion is 9.8 cycles~virtually equal to the 
latency. In general, the temporal dispersion will be approximately equal to the 
latency whenever the latency is much larger than one cycle. 
If there are a tot~l of ~ neurons~ the cycle time is ~ = O/(N~chan), where 
~ is the normalized load~ and the timing error due to channel l~tency will be 
(m + 1)A 
~Nyq m + I 
% ~ 
- 2G 
~ 
f~h~n 
~ 
Using the expression for the number of cycles spent w~iting (Equation 11.3), 
and the relationship between ~Nyq and f~h~ (Equation 11.1)~ we obtain 
2G 
1 
1 - G/2 
e,= 
N a+(1-a)/Z 
1-G 
For example, at 95 percent load and at 5 percent active fraction, with a sampling 
rate attenuation of 40 and with a population of size 4096 (64 x 64), the latency 
error is 7 percent. 
The error introduced by the temporal dispersion in the 
channel will be similar, as the temporal dispersion is more or less equal to the 
latency. 
Notice that the timing error is inversely proportional to N. This scaling 
occurs because channel capacity must grow with the number of neurons. Hence, 
the cycle time decreases, and there is a proportionate decrease in queueing time, 
even though the number of cycles spent queueing remains the same~for the 
same normalized load. In contrast, the collision rate remains unchanged for 
the same normalized load. Hence, the arbitered channel scales much better 
than the nonarbitered one as technology improves and shorter cycle times are 
achieved. 
11.3.3 
Trat~c: Random or Correlated? 
Correlated spike activity occurs when external stimuli trigger synchronous ac- 
tivity in several neurons. Such structure, which is captured by the neuronal en- 

NEUROMORPHIC 
CHIPS 
241 
semble concept, is much more plausible than totally randomized spike times-- 
especially if neurons are driven by sharply defined object features (high spatial 
and temporal frequencies) and adapt to the background (low spatial and tem- 
poral frequencies). If there are correlations among firing times, the Poisson 
distribution does not apply to the spike times, but, making a few reasonable 
assumptions, it may be used to describe the relative timing of spikes within a 
burst. 
The distribution of sample times within each neuronal ensemble is best de- 
scribed by a Gaussian distribution, centered at the mean time of arrival, as 
shown in Figure ll.la. The mean of the Gaussian, #, is taken to be the de- 
lay between the time the stimulus occurred and the mean sample time, and 
the standard deviation of the Gaussian, ~r, is assumed to scale with the mean, 
i.e. the coefficient of variation (cov) c -- or/#, is constant. 
The minimum capacity, per quantizer, required to transmit a neuronal en- 
semble without temporal dispersion is given by 
Fbur~t 
1 
fburst 
~ 
-
-
 
-- -
-
 
-]Vburs t 
2~/3c#' 
using the equivalent uniform distribution shown in Figure ll.lb. This simple 
model predicts that shorter latencies or less variability can be had only by 
paying for a proportionate increase in channel throughput. For instance, a 
latency of 2ms and a cov of 10% requires 1440 S/s per quantizer. This result 
assumes that the neurons' interspike intervals are large enough that there is no 
overlap in time between successive bursts; this is indeed the case if c < l/x/3 
or 58%. 
There is a strong tendency to conclude that the minimum throughput spec- 
ification is simply equal to the mean sampling rate. This is the case only if 
sampling times are totally random. Random samples are distributed uniformly 
over the period T = l/f, where f is the quantizer's mean sampling rate, as- 
sumed to be the same for all nodes that are part of the ensemble; the latency 
is # = T/2, and the temporal dispersion is a = T/(2v/3 ). Hence, the cov is 
c = l/v/3 =58% and Equation 11.3.3 yields fburst 
= l/T, as expected. For 
a latency of 2ms and a cov of 58%, the required throughput is 250 S/s per 
quantizer compared to 1440 S/s when the cov is 10%. 
The rectangular (uniform) approximation to the Gaussian, shown in Fig- 
ure ll.lb, may be used to calculate the number of collisions that occur during 
a burst. We simply use Equation 11.2, and set the rate of the Poisson process to 
the uniform sampling rate of the rectangular distribution. The result is plotted 
in Figure 11.3. 
11.3.4 Throughput Requirements 
By including the background activity of the neurons that do not participate in 
the neuronal ensemble, we obtain the total throughput requirement 
ftotM = afburst 
-~ (1 -- a)ffire/Z 
, 

242 
NEUROMORPHIC SYSTEMS ENGINEERING 
0.8 
~0.6 
~ 0.4 
0.2 
Collision Probability vs Bandwidth 
1 
, 
0 
i0 
20 
30 
40 
50 
Bandwidth/Minimum 
Figure 11.3 
Theoretical collision probability versus channel capacity (bandwidth) for Gaus- 
sian bursts. Actual capacity is normalized to the minimum capacity requirement. Thus, the 
numbers on the Bandwidth/Minimum axes are the reciprocal of the offered channel load 
~Y. For collision probabilities below 18 percent, capacities ten or more times the minimum 
are required. The dots represent results of a numerical computation based on the Gaussian 
and binomial distributions for sample occurrence within a burst, and the number of sam- 
ples in each time-slot, respectively. The line represents results of a simplified, analytically 
tractable, approximation based on the equivalent rectangular and Poisson distributions, re- 
spectively. The simplified model overestimates the collision probability by no more than 8%; 
the estimation error drops below 4.4% for capacities greater than i0 times the theoretical 
minimum. 
where a is the fraction of the population that participates in the burst; ffire/Z is 
the firing rate of the remaining quantizers, expressed as an attenuation, by the 
factor Z, of the sampling rate of the active quantizers. Assuming f~re = 1/(2#), 
we have ffire/fburst = v/3C, and 
ftotal 
~ 
a + ~/3c(1 - a)/Z 
2~3c# 
' 
per quantizer. For a 2ms latency, a 10 percent coy, a 5 percent active fraction, 
and an attenuation factor of 40, the result is ftotal = 78.1S/s per quantizer. For 
these parameter values, a 15.6 percent temporal dispersion error is incurred, 
assuming a channel loading of 95 percent and a population size of 4096 (64 by 
64 array). 
This result is only valid if samples are not delayed for more than the duration 
of the burst (i.e. errors less than 1/v/3 = 58%). For larger delays, the temporal 
dispersion grows linearly--instead of hyperbolically--because the burst is dis- 
tributed over an interval no greater than (~burst/Fchan)/Tburst, when the sample 

NEUROMORPHIC CHIPS 
243 
rate /~burst exceeds the channel capacity, Fchan, where Tburs t is the duration of 
the burst. 
11.4 
PIPELINED COMMUNICATION CHANNEL 
In this section, I describe an arbitered, random-access communication channel 
design that supports asynchronous pixel-level analog-to-digital conversion. As 
discussed in the previous section, arbitration is the best choice for neuromor- 
phic systems whose activity is sparse in space and in time, because it allows 
us to trade an exponential increase in collisions for a linear increase in tempo- 
ral dispersion. Furthermore, for the same percentage channel utilization, the 
temporal dispersion decreases as the technology improves, and we build larger 
systems with shorter cycle times, whereas the collision probability remains the 
same. 
The downside of arbitration is that this process lengthens the communication 
cycle, reducing channel capacity. I have achieved improvements in throughput 
over previous arbitered designs [12] by adopting three strategies that shorten 
the average cycle time: 
1. Allow several address-events to be in various stages of transmission at the 
same time. This well-known approach to increasing throughput is called 
pipelining; it involves breaking the communication cycle into a series of 
steps and overlapping the execution of these steps as much as possible. 
2. Exploit locality in the arbiter tree. That is, do not arbitrate among all 
the inputs every time; doing so would require spanning all log 2 (N) levels 
of the tree. Instead, find the smallest subtree that has a pair of active 
inputs, and arbitrate between those inputs; this approach minimizes the 
number of levels spanned. 
. Exploit locality in the row-column architecture. That is, do not redo both 
the row arbitration and the column arbitration for each address-event. 
Instead, service all requesting pixels in the selected row, redoing only 
the column arbitration, and redo the row arbitration only when no more 
requests remain in the selected row. 
This work builds on the pioneering contributions of Mahowald [12] and 
Sivilotti [17]. Like their original design, my implementation is completely self- 
timed: Every communication consists of a full four-phase handshaking sequence 
on a pair of wires, as shown in Figure 11.4. Self-timed operation makes queue- 
ing and pipelining straightforward [18]: You stall a stage of the pipeline or make 
a pixel wait simply by refusing to acknowledge it. Lazzaro et.al, have also im- 
proved on the original design [11], and have used their improved interface in a 
silicon auditory model [15]. 

244 
NEUROMORPHIC SYSTEMS ENGINEERING 
j 
~[Req 
22 
Sender 
_ 
Receiver 
I 
El 
Oa,a I 
Da a 
I 
-1 
(a/ 
.... 
R~q 
m \  
(b) 
Figure 11.4 
Self-timed data-transmission protocol using a four-phase handshake. (a) 
Data-bus (data) and data-transfer control signals (Req and Ack). (b) Handshake protocol 
on control lines. The sender initiates the sequence by driving its data onto the bus and 
taking Req high. The receiver reads the data when Req goes high, and drives Ack high 
when it is done. The sender widthdraws its data and takes Req low when Ack goes high. 
The receiver terminates the sequence by taking Ack low after Req goes low, returning the 
bus to its original state. As data is latched on Ack ~', the designer can ensure that the setup 
and hold times of the receiver's input latch are satisfied by delaying; Req ~', relative to the 
data, and by delaying widthdrawing the data after Ack ~. 
11.4.1 Communication Cycle Sequence 
The operations involved in a complete communication cycle are outlined in this 
subsection. This description refers to the block diagram of the channel archi- 
tecture in Figure 11.5; the circuits are described in the next two subsections. At 
the beginning of a communication cycle, the request and acknowledge signals 
are both low. 
On the sender side, spiking neurons first make requests to the Y arbiter, 
which selects only one row at a time. All spiking neurons in the selected row 
then make requests to the X arbiter. At the same time, the Y address encoder 
drives the address of the selected row onto the bus. When the X arbiter selects 
a column, the neuron in that particular column, and in the row selected earier, 
resets itself and withdraws its column and row requests. At the same time, the 
X address encoder drives the addresses of the selected column on to the bus, 
and takes Req high. 
When Ack goes high, the select signals that propagate down the arbiter tree 
are disabled by the AND gates at the top of the X and Y arbiters. As a result, 
the arbiter inactivates the select signals sent to the pixels and to the address- 
encoders. Consequently, the sender withdraws the addresses and the request 
signal Req. 
When it is necessary, the handshake circuit (also known as a C-element [18]) 
between the arbiters and the rows or columns will delay inactivating the select 
signals that drive the pixel, and the encoders, to give the sending pixel enough 
time to reset. The sender's handshake circuit is designed to stall the communi- 
cation cycle by keeping Req high until the pixel withdraws its row and column 

NEUROMORPHIC CHIPS 
245 
~' 
..... ~o 
~" 
~ 
Vreset[ 
i 
" I- x I 
I PixAck 
I 
Vspk 
~ 
~ 
~ 
~,,,,~, 
, ...... 
/ 
.............. 
~ 
..... 
~ 
~ . LReqOrPU 
,~ 
~ 
'
~
~
!
l
.
.
~
'
b
 
~"~'~ 
i~-~.~ 
set 
C-element 
Driver 
C-eh,meut 
Figure 11.5 
Pipelined addess-event channel. The block diagram describes the channel 
architecture; the logic circuits for each block also are shown. Sender Chip: The row and 
column arbiter circuits are identical; the row and column handshaking circuits (C-elements) 
are also identical. The arbiter is built from a tree of two-input arbiter cells that send a 
request signal to, and receive a select signal from, the next level of the tree. The sending 
neuron's logic circuit (upper-left) interfaces between the adaptive neuron circuit, the row 
C-element (lower-left: Ry --~ Rpix, Apix -~ Ay), and the column C-element (lower-left: 
Rx --~ Rpix, Apix --+ Ax). The pull-down chains in the pixel--tied to pull-up elements at 
the right and at the top of the array--and the column and row request lines form wired-OR 
gates. The C-elements talk to the column and row arbiters (detailed circuit not shown) 
and drive the address encoders (detailed circuit not shown). The encoders generate the 
row address (Y), the column address (X), and the chip request (Req). Receiver Chip: 
The receiver's C-element (lower-right) acknowledges the sender, strobes the latches (lower- 
middle), and enables the address decoders (detailed circuit not shown). The receiver's 
C-element also monitors the sender's request (Req) and the receiving neuron's acknowledge 
(Apix). The receiving neuron's logic circuit (upper-right) interfaces between the row and 
column selects (Ry and Rx) and the post-synaptic integrator, and generates the receiving 
neuron's acknowledge (Apix). The pull-down path in the pixel--tied to pull-up elements 
at the left of the array--and the row acknowledge lines form wired-OR gates. An extra 
wired-OR gate, that runs up the left edge of the array, combines the row acknowledges into 
a single acknowledge signal that goes to the C-element. 

246 
NEUROMORPHIC SYSTEMS ENGINEERING 
Sending 
Sender 
Pixel 
C-Element Arbiter 
Reciever 
Recieving 
C-Element 
Pixel 
<t---t---: 
~'~" 
~ 
â¢ "~ l~I~~ 
~ 
Set Phase 
' "~" "~,.. ,..~ ~,,.~ 
, 
~ 
Reset Phase 
Figure 11.6 Pipelined communication cycle sequence for arbitration in one dimension, 
showing four-phase minicycles among five elements. The boxes indicate the duration of the 
current cycle, which may overlap with the reset phase of the preceding cycle and the set 
phase of the succeeding cycle. (Steps associated with the preceding and succeeding cycles 
are shown with dashed-lines.) Thus three address-events may be at different stages in the 
communication channel at the same time. The cycle consists of three smaller interwoven 
minicycles: sending pixel to C-element, C-element to C-element, and C-element to receiving 
pixel. The C-elements--also known as handshake circuits---ensure that the minicycles occur 
in lock-step, synchronizing the activity of the sending pixel, the arbiter, and the receiving 
pixel. 
requests, confirming that the pixel has reset. The exact sequencing of these 
events is shown in Figure 11.6. 
On the receiver side, as soon as Req goes high, the address bits are latched 
and Ack goes high. At the same time, the address decoders are enabled and, 
while the sender chip is deactivating its internal request and select signals, the 
receiver decodes the addresses and selects the corresponding pixel. When the 
sender takes Req low, the receiver responds by taking Ack low, disabling the 
decoders and making the latches transparent again. 
When it is necessary, the receiver's handshake circuit, which monitors the 
sender's request (Req) and the acknowledge from the receiving pixel (Apix), will 
delay disabling the address-decoders to give the receiving pixel enough time to 
read the spike and generate a post-synaptic potential. The reciever's handshake 
circuit is designed to stall the communication cycle by keeping Ack high until 

NEUROMORPHIC CHIPS 
247 
OR Gate 
Flip-Flop 
Steering Circuit 
Figure 11.7 
Arbiter cell circuitry. The arbiter cell consists of an OR gate, a flip-flop, and a 
steering circuit. The OR gate propagates the two incoming active-high requests, F~I and [~2, 
to the next level of the tree by driving F~out. The flip-flop is built from a pair of cross-coupled 
NAND gates. Its active-low set and reset inputs are driven by the incoming requests, F~I 
and R2, and its active-high outputs, Q1 and Q2, control the steering circuit. The steering 
circuit propagates the incoming active-high select signal, Ain, down the appropriate branch 
of the tree by driving either A1 or A2, depending on the state of the flip-flop. This circuitry 
is reproduced from Mahowald 1994 [12]. 
the pixel acknowledges, confirming that the pixel did indeed recieve the spike. 
The exact sequencing of these events also is shown in Figure 11.6. 
11.4.2 Arbiter Operation 
The arbiter works in a hierarchical fashion, using a tree of two-way decision 
cells [11, 12, 17], as shown in Figure 11.5. Thus, arbitration between N inputs 
requires only N - 1 two-input cells. The N-input arbiter is layed out as a 
(N - 1) Ã 1 array of cells, positioned along the edge of the pixel array, with 
inputs from the pixels coming in on one side and wiring between the cells 
running along the other side. 
The core of the two-input arbiter cell is a flip-flop with complementary inputs 
and outputs, as shown in Figure 11.7; these circuits were designed by Sivilotti 
and Mahowald [12, 17]. That is, both the set and reset controls of the flip- 
flop (tied to R1 and R2) are normally active (i.e., low), forcing both of the 
flip-flops outputs (tied to Q1 and Q2) to be active (i.e., high). When one 
of the two incoming requests (R1 or R2) becomes active, the corresponding 
control (either set or reset) is inactivated, and that request is selected when the 
corresponding output (Q1 or Q2) becomes inactive. In case both of the cell's 
incoming requests become active simultaneously, the flip-fiop's set and reset 
controls are both inactivated, and the flip-flop randomly settles into one of its 
stable states, with one output active and the other inactive. Hence, only one 
request is selected. 

248 
NEUROMORPHIC SYSTEMS ENGINEERING 
Before sending a select signal (A1 or A2) to the lower level, however, the cell 
sends a request signal (Rout) up the tree and waits until a select signal (Ain) 
is received from the upper level. At the top of the tree, the request signal is 
simply fed back in, and becomes the select signal that propagates down the 
tree. 
As the arbiter cell continues to select a branch so long as there is an active 
request from that branch, we can keep a row selected, until all the active pixels 
in that row are serviced, simply by ORing together all the requests from that 
row to generate the request to the Y arbiter. Similarly, as the request passed 
to the next level of the tree is simply the OR of the two incoming requests, a 
subtree will remain selected as long as there are active requests in that part 
of the arbiter tree. Thus, each subtree will service all its daughters once it 
is selected. Using the arbiter in this way minimizes the number of levels of 
arbitration performed 
the input that requires the smallest number of levels 
to be crossed is selected. 
To reset the select signals fed into the array--and to the encoder 
previous 
designs removed the in-coming requests at the bottom of the arbiter tree [11, 
12, 17]. Hence, the state of all the flip-flops were erased, and a full log2(N)-level 
row arbitration and a full column arbitration had to be performed for every 
cycle. In my design, I reset the row/column select signals by removing the 
select signal from the top of the arbiter tree; thus, the request signals fed in 
at the bottom are undisturbed, and the state of arbiter is preserved, allowing 
locality in the array and the arbiter tree to be fully exploited. 
I achieve a shorter average cycle time by exploiting locality; this opportuinis- 
tic approach trades fairness for efficiency. Instead of allowing every active pixel 
to bid for the next cycle, or granting service on a strictly first-come first-served 
basis, I take the travelling-salesman approach, and service the customer that 
is closest. Making the average service time as short as possible 
to maximize 
channel capacity--is my paramount concern, because the wait time goes to 
infinity when the channel capacity is exceeded. 
11.4.3 
Logic Circuits and Latches 
In this subsection, I describe the address-bit latches and the four remaining 
asynchronous logic gates in the communication pathway. Namely, the inter- 
faces in the sending and receiving neurons and the C-elements in the sender 
and receiver chips. The interactions between these gates, the neurons, and 
the arbiter--and the sequencing constraints that these gates are designed to 
enforce--are depicted graphically in Figure 11.6. 
The logic circuit in the sending neuron is shown in Figure 11.5; it is similar 
to that described in [11, 12]. The neuron takes Vspk high when it spikes, and 
pulls the row request line Ry low. The column request line Rx is pulled low 
when the row select line Ay goes high, provided Vspk is also high. Finally, Ireset 
is turned on when the column select line Ax goes high, and the neuron is reset. 

NEUROMORPHIC 
CHIPS 
249 
Vadpt is also pulled low to dump charge on the feedback integrator in order to 
adapt the firing rate [3]. 
I added a third transistor, driven by Vspk, to the reset chain to turn off lreset 
as soon as the neuron is reset, i.e. Vspk goes low. Thus, ~reset does not continue 
to discharge the input capacitance while we are waiting for Ax and Ay to go 
low, making the reset pulse width depend on only the delay of elements inside 
the pixel. 
The sender's C-element circuit is shown in the lower-left corner of Fig- 
ure 11.5. It has a flip-flop whose output (Apix) drives the column or row select 
line. This flip-flop is set when Aarb goes high; which happens when the arbiter 
selects that particular row or column. The flip-flop is reset when Rpix goes 
high, which happens when two conditions are satisfied: (i) Aarb is low, and 
(ii) all the pixels tied to the wired-OR line Rpix are reset. Thus the wired-OR 
serves three functions in this circuit: (i) It detects when there is a request in 
that row or column, passing on the request to the arbiter by taking R~rb high; 
(ii) it detects when the receiver acknowledges, by watching for A~rb to go low; 
and (iii) it detects when the pixel(s) in its row or column are reset. 
There are two differences between my handshaking circuit and the hand- 
shaking circuit of Lazzaro et.al. [11]. 
First, Lazzaro et.al, disable all the arbiter's inputs to prevent it from granting 
another request while Ack is high. 
By using the AND gate at the top of 
the arbiter tree to disable the arbiter's outputs (Aarb) when Ack is high, my 
design leaves the arbiter's inputs undisturbed. As I explained in the previous 
subsection, my approach enables us to exploit locality in the arbiter tree and 
in the array. 
Second, Lazzaro et.al, assume that the selected pixel will withdraw its re- 
quest before the receiver acknowledges. This timing assumption may not hold 
if the receiver is pipelined. When the assumption fails, the row or column select 
lines may be cleared before the pixel has been reset. In my circuit, the row 
and column select signals (Apix) are reset only if Aarb is low, indicating that 
the receiver has acknowledged, and no current is being drawn from Rpix by the 
array, indicating that the pixel has been reset. 
Mahowald's original design used a similar trick to ensure that the select 
lines were not cleared prematurely. However, her handshaking circuit used 
dynamic state-holding elements which were susceptible to charge-pumping and 
to leakage currents due to minority carriers injected into the substrate when 
devices are switched off. My design uses fully static stateholding elements. 
The receiver's C-element (it is slightly different from the sender's) is shown in 
the lower-right corner of Figure 11.5. The C-element's output signal drives the 
chip acknowledge (Ack), strobes the latches, and activates the address decoders. 
The flip-flop that determines the state of Ack is set if Req is high, indicating 
that there is a request, and Apix is low, indicating that the decoders' outputs 
and the wired-OR outputs have been cleared. It is reset if Req is low, indicating 
that the sender has read the acknowledge and Apix is high, indicating that the 
receiving neuron got the spike. 

250 
NEUROMORPHIC SYSTEMS ENGINEERING 
The address-bit latch and the logic inside the receiving pixel are also shown 
in the figure (middle of lower row and upper-right corner, respectively). The 
latch is opaque when Ack is high, and is transparent when Ack is low. The pixel 
logic produces an active low spike whose duration depends on the delay of the 
wired-OR and the decoder, and on the duration of the sender's reset phase. 
Circuits for the blocks that are not described here--namely, the address 
encoder and the address decoder--are given in [11, 12]. 
11.4.4 Performance and Improvements 
In this subsection, I characterize the behavior of the channel and present mea- 
surements of cycle times and the results of a timing analysis in this section. For 
unacessible elements, I have calculated estimates for the delays using the device 
and capacitance parameters supplied by MOSIS for the fabrication process. 
Figure 11.8 shows plots of address-event streams that where read out from 
the sender under two vastly different conditions. In one case, the load was 
less than 5% of the channel capacity. In the other case, the load exceeded the 
channel capacity. For small loads, the row arbiter rearranges the address--events 
as it attempts to scan through the rows, going to the nearest row that is active. 
Scanning behavior is not evident in the X addresses because no more than 3 or 
4 neurons are active simultaneously within the same row. For large loads, the 
row arbiter concentrates on one half, or one quarter, of the rows, as it attempts 
to keep up with the data rate. And the column arbiter services all the pixels 
in the selected row, scanning across the row. Sometimes the addresses are 
transposed because each arbiter cell chooses randomly between its left branch 
(lower half of its range) or its right branch (upper half of its range). 
Figure 11.9a shows the relative timing of Req, Ack, the X-address bit Xad0, 
and the acknowledge from the pixel that received the address-event Apix. The 
cycle breaks down as follows. The 18ns delay between Req $ and Ack $ is due 
to the receiver's C-element (8.9ns) and the pad (9.1as); the same holds for the 
delay between Req J" and Ack $. The 57ns delay between Ack $ and Apix $ is due 
to the decoder (13ns), the row wired-OR (41ns), and the second wired-OR that 
runs up the left edge of the array (3.4ns). The 57ns delay between Ack J" and 
Apix ]', breaks down similarly: decoder (38ns), row wired-OR (14ns), left-edge 
wired-OR (3.4ns). The 94ns and 170ns delays between Apix and Req are due 
to the sender. The X-address bits need not be delayed because the C-element's 
8.9ns delay is sufficient set-up time for the latches. 
Figure 11.9b shows the relative timing of Req, the Y-address bits Yad0, and 
the select-enable signals (Xsel, Ysel) fed in at the top of the arbiter trees; Ãsel 
and Ysel are disabled by Ack. The first half of the first cycle breaks down 
as follows. The 48ns delay between Req $ and Ysel ~" is due to the receiver 
(18ns) and the AND circuit (30ns)--it is a chain of two 74HC04 inverters and 
a 74HC02 NOR gate. 3 The 120ns delay between Ysel ? and Yadr0 J" is due to 
the arbiter (propagating a high-going select down the six-level tree takes 19ns 
per stage and 6.Tns for the bottom stage), the row/column C-element (5.2ns), 

NEUROMORPHIC 
CHIPS 
251 
Y ~ , :  Q~ ~, 
â¢ : 
o ! /  
, 
~ 
~ 
; 
; 
.~* 
~ 
â¢ 
. 
~ 
~ 
~ 
~ 
~. 
~ 
~.oO 
~ 
,,. ~.,.~ 
, ~ 
~.~ 
~ 
~ 
~ 
~ 
~ 
~ ~ 
â¢ 
â¢ a, 
.:* ~'~ 
; 
0"" 
,~ 
~ ~ 
: 
*~ 
~ 
. 
~'~ 
, 
â¢ .~ 
.,~.. 
â¢ . 
~. 
~ 
.~ 
~ 
~" 
.. 
â¢ 
. 
~. 
~ ~ . .~ 
~o 
=~. 
~ 
~ ~ 
) 
~. 
.~ 
o ~' 
~ ~ 
, 
." 
.~. 
~ . 
o 
..~ 
." 
", 
01: :f 
~,0 
~ 
~ 
~ 
~ 
~ 
~ 
Y
~
 
IO~a 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
x A ~ =  ~ 
l~p~ 
?0 
":" :':~i'::.:" ".':-: 
" :'. 
":"" 
.* 
. 
â¢ 
Ã·.'* 
... 
â¢ Ã· 
.
.
.
.
 
~. 
â¢ 
~. 
â¢:*. 
..... 
**.** ... 
. 
, 
."~./â¢ 
"-.':: .:.. :. ". :.:'... 
: 
+:. 
+ ~+. 
/.~ 
:-.~..." 
. 
.~ 
: .'- 
./~-.. : 
..': -...'.: 
:~."~. 
. ..... ~'..'-'.: ,'.. 
.~.. ".~ .~... ~ ::.. ".." . :.:..:" 
â¢ 
~ 
. 
+.~ 
~ 
. 
+* 
. 
~ 
~..~ 
~ ~. 
~ 
. 
+ 
... 
*~* 
~ 
/.'+.. 
:..~/ ~ "-..-" ~ â¢ ~ "-~.~ ~.. ~ 
+ â¢ ~ 
, 
~ ~ 
, 
a, 
**+ 
~+, 
. 
., 
1~ 
~ 
~ 
~ 
~ 
x
~
:
~
 
~0 
. 
q 
q 
~ 
~ 
~ 
l 
~ 
/~ 
), 
~ 
, 
s 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
~ '~ 
~ 
4 
~ 
z 
~'~ 
/, 
~ 
,, 
io 
, ~ 
~ 
,~t 
~ 
~ 
,~ 
, 
z, 
~ 
i~ 
ll~ 
ii~ 
i~ 
i~ 
Figure 
11.8 
Recorded address-event streams for small (queue empty) and large (queue 
full) channel loads. X and Y addresses are plotted on the vertical axes, and their position in 
the sequence is plotted on the horizontal axes. Queue Empty (top row): The Y addresses 
tend to increase with sequence number, but the row arbiter sometimes remains at the same 
row, servicing up to 4 neurons, and sometimes picks up a row that is far away from the 
previous row. Whereas the X addresses are distributed randomly. Queue Full (bottom row): 
The Y addresses tend to be concentrated in the top or bottom half of the range, or in the 
third quarter, and so on, and the column arbiter services all 64 neurons in the selected row. 
The X addresses tend to decrease with sequence number, as all the neurons are serviced 
progressively, except for transpositions that occur over regions whose width equals a power 
of 2. Note that the horizontal scale has been magnified by a factor of 30 for the X addresses. 

252 
NEUROMORPHIC SYSTEMS ENGINEERING 
Receiver Timing 
Sender Timing 
_ ~
~
 
5z,. 
J~.. 
. 
s 
15 2 ~5 3 5,L~0,' ..... 
Â°,0, 
2 
, 
0 
Lo~,l ,0 
(a) 
(b) 
~ 
x ~o ~ 
Figure 11.9 
Measured address-event channel timing. All the delays given are measured 
from the preceding transition: (a) Timing of Req and Ack signals relative to X-address bit 
(Xad0) and receiver pixers acknowledge (Apix). Pipelining shaves a total of 113ns off the 
cycle time (twice the duration between Ack and Apix). (b) Timing of Req signal relative 
to the select signals fed into the top of the arbiter trees (Ysel and Xsel, disabled when Ack 
is high), and the Y-address bit (Yad0). Arbitration occurs in both the Y and X dimensions 
during the first cycle, but only in the X dimension during the second cycle; the cycle time is 
730 ns for the first cycle and 420 ns for the second. 
the address encoder (3.2ns), and the pad (Tns). The same applies to the delay 
between Xsel j" and Req â¢. The 190ns delay between Yad0 J" and Xsel ~" is due 
to the column wired-OR (120ns), the arbiter (propagating a high-going request 
up the six-level tree takes 4.Sns per stage and 11ns for the top stage), the pad 
(Tns), and the AND circuit (30ns). The slow events are propagating a high- 
going select down the arbiter (100ns total) and propagating a request from the 
pixel through the column wired-OR (120ns). 
The second half of the first cycle breaks down as follows. The 49ns delay 
between Req ]" and Xsel $, Ysel $ is identical to that for the opposite transitions. 
The 200ns delay between Xsel $, Ysel $ and Req $ is due the arbiter (propagating 
a low-going select signal down the six-level tree takes 6.Sns per stage and 2.5ns 
for the bottom stage), the column and row wired-ORs (120ns), the handshake 
circuit (5.2ns), the encoder (32ns), and the pad (Tns). The slow events are 
propagating a low-going select down the arbiter (36ns total), restoring the 
wired-OR line (120ns), and restoring the address-lines (32ns). 
The second cycle is identical to the first one except that there is no row 
arbitration. Hence, Xsel goes high immediately after Req goes low, eliminating 
the 310ns it takes to propagate a select signal down the Y-arbiter, to select a 
row, to get the column request from the pixel, and to propagate a request up 
the X-arbiter. 
This channel design achieves a peak throughput of 2.5MS/s (million spikes 
per second), for a 64 x 64 array in 2#m CMOS technology. The cycle time is 

NEUROMORPHIC CHIPS 
253 
730ns if arbitration is performed in both dimensions and 420ns when arbitration 
is performed in only the X dimension (i.e., the pixel sent is from the same row 
as was the previous pixel). These cycle times represent a threefold to fivefold 
improvement over the 2#s cycle times reported in the original work [12], and are 
comparable to the shortest cycle time of 500ns reported for a much smaller 10 x 
10 nonarbitered array fabricated in 1.6#m technology [14]. Lazzaro et.al, report 
cycle times in the 100-140ns range for their arbitered design, but the array size 
and the chip size are a lot smaller. 
Pipelining the receiver shaves a total of 113ns off the cycle time--the time 
saved by latching the address-event, instead of waiting for the receiving pixel 
to acknowledge. Pipelining the sending pixel's reset phase did not make much 
difference because most of the time is spent waiting for the row and column 
wired-OR request lines to reset, once the pixel itself is reset. Unfortunately, I 
did not pipeline reseting these request lines: I wait until the receiver's acknowl- 
edge disables the select signals that propagate down the arbiter tree, and Arb 
goes low, before releasing the column and row wired-OR request line. Propa- 
gating these high-going and low-going select signals down the six-level arbiter 
tree (ll0ns+40ns) and resetting the column and row request lines (120as) adds 
a total of 270ns to the cycle time, when arbitration is performed in only the X 
dimension, and adds 400ns when arbitration is performed in both the X and Y 
dimensions. 
The imapct of the critical paths revealed by my timing analysis can be 
reduced significantly by making three architectural modifications to the sender 
chip: 
1. Moving the AND gate that disables the arbiter's select signals to the 
bottom of the tree would shave off a total of 144ns; this change requires 
an AND gate for each row and each column. 4 
2. Removing the input to the column and row wired-OR from the arbiter 
would allow the column and row request lines to be cleared as soon as 
the pixel is reset; this requires adding some logic to reset the flip-flop in 
the sender's C-elemet when Rpix is high and Aarb is low. 
3. Doubling the drive of the two series devices in the pixel that pull down 
the column line would reduce the delay of the wired-OR gate to 60as. 
These modifications, taken together, allow us to hide reseting the column and 
row select lines in the 59ns it takes for Apix? ~ Req~" ~ Ack$ ~ Xsel$, shaving 
off a total of 120ns, when arbitration occurs in only the X dimension, and 
180ns when arbitration occurs in both dimensions. These changes, together, 
will reduce the cycle time to 156ns with arbitration in one dimension, and to 
406ns with arbitration in both dimensions. Purther gains may be made by 
optimizing the sizes of the devices in the arbiter for speed and adding some 
buffers where necessary. 

254 
NEUROMORPHIC SYSTEMS ENGINEERING 
11.4.5 
Bugs and Fixes 
In this section, I describe the bugs I discovered in the channel design and 
propose some ways to fix them. 
During testing, I found that the sender occasionally generates illegitimate 
addresses, i.e. outside the 1 to 64 range of pixel locations. In particular, row 
(Y) addresses higher than 64 were observed. This occurs when row 64 and one 
other row, or more, are selected simultaneously. I traced this problem to the 
sender's C-element (lower-left of Figure 11.5). 
After Ack goes high and Aarb goes low, the pull-up starts charging up RpiÃ. 
When RpiÃ crosses the threshold of the inverter, Rarb goes low. When the 
arbiter sees Rarb go low it selects another row. However, if Rpix has not crossed 
the threshold for resetting the flip-flop, the flip-flop remains set and keeps the 
previous row selected. Hence, two rows will be selected at the same time, and 
the encoder will OR their addresses together. 
This scenario is plausible because the threshold of the inverter that drives 
Rarb is lower than that of the flip-flop's reset input; I calculated 2.83V and 
3.27V, respectively. If any neuron in the previously selected row spikes while 
Rpix is between these two values, Rpix will be pulled back low, and the flip- 
flop will not be reset. 
Rpix spends about 0.44/2.5 Ã 120ns = 21ns in this 
critical window. At a total spike rate of 100KHz, we expect a collision rate of 
0.05Hz, just for row sixty-four alone. I observed a rate of 0.06Hz; the higher 
rate observed may be due to correlations in firing times. To eliminate these 
collisions, we should disable neurons from firing while their row is selected (i.e., 
Ay is high). That way, Rpix will remain low until Apix goes low, ensuring that 
the flip-flop is reset. 5 
Iw ~~~ScanOut 
~. 
~ 
(al 
r _ 
::Â°oL 
NAND Gaw 
Intcg~alor 
(b) 
Figure 11.10 
Comparison between my (a) first and (b) second circuit designs for a receiver 
pixel. The second design eliminated charge-pumping and capacitive turn-on which plagued 
the first design, as explained in the text. 

NEUROMORPHIC CHIPS 
255 
I also had to redesign the receiver pixel to eliminate charge-pumping and 
capacitive turn-on, which plagued the first pixel I designed, and to reduce cross- 
talk between the digital and analog parts by careful layout. Two generations 
of receiver pixel circuit designs are shown in Figure 11.10. 
The pair of transistors controlled by the row and column select lines, R~, and 
RÃ, pump charge to ground when non-overlapping pulses occur on the select 
lines. In my first design, this charge-pump could supply current directly to the 
integrators--irrespective of whether or not that pixel had been selected. The 
pump currents are significant as, on average, a pixel's row or column is selected 
64 times more often than the pixel itself. For a parasitic capacitance of 20fF 
on the node between the transistors, an average spike rate of 100Hz per pixel, 
and a voltage drop of 0.5V, the current is 64pA. This current swamps out the 
subpicoamp current levels we must maintain in the diode-capacitor integrator 
to obtain time constants greater than 10ms using a tiny 300fF capacitor. 
I solved this problem in my second design by adding a pull-up to implement 
an nMOS-style NAND gate. The pull-up can supply a fraction of a milliamp, 
easily overwhelming the pump current. The NAND gate turns on the transis- 
tor that supplies current to the integrator by swinging its source terminal from 
Vdd to GND. As demonstrated by Cauwenberghs [6], this technique can meter 
very minute quantities of charge onto the capacitor. In addition to eliminat- 
ing charge-pumping, this technique circumvents another problem we encounter 
when we attempt to switch a current source on and off: capacitive turn-on. 
Rapid voltage swings on the select line are transmitted to the source termi- 
nal of the current-source transistor by the gate-drain overlap capacitor of the 
switching transistor. In the first design, where this terminal's voltage was close 
to GhlD, these transients could drive the source terminal a few tenths of a volt 
below GSID. As a result, the current source would pass a fraction of a picoamp 
even when Iw was tied to GSID. In the new design, the pull-up holds this node 
up at Vdd and supplies the capacitive current, preventing the node from being 
discharged. 
11.5 
DISCUSSION 
Since technological limitations precluded the use of dedicated lines, I developed 
a time-multiplexed channel that communicates neuronal ensembles between 
chips, taking advantage of the fact that the bandwidth of a metal wire is several 
orders of magnitude greater than that of a nerve axon. Thus, we can reduce the 
number of wires by sharing wires among neurons. We replaced thousands of 
dedicated lines with a handfull of wires and thousands of switches (transistors). 
This approach paid off well because transistors take up much less real estate 
on the chip than wires do. 
I presented three compelling re~sons to provide random access to the shared 
channel, using event-driven communication, and to resolve contention by ar- 
bitration, providing a queue where pixels wait their turn. These choices are 

256 
NEUROMORPHIC SYSTEMS ENGINEERING 
based on the assumption that activity in neuromorphic systems is clustered in 
time and in space. 
First, unlike sequential polling, which rigidly allocates a fixed fraction of the 
channel capacity to each quantizer, an event-driven channel does not service 
inactive quantizers. Instead, it dynamically reallocates the channel capacity 
to active quantizers and allows them to samples more frequently. Despite the 
fact that random access comes at the cost of using log 2 N wires to transmit 
addresses, instead of just one wire to indicate whether a polled node is active 
or not, the event-driven approach results in a lower bit rate and a much higher 
peak sampling rate when activity is sparse. 
Second, an arbiterless channel achieves a maximum throughput of only 18% 
of the channel capacity, with an extremely high collision rate of 64 percent. 
Whereas an arbitered channel can operate at 95% capacity without any losses 
due to collisions--but its latency and temporal dispersion is 10 times the cycle 
period. Thus, unless the cycle-time of the arbiterless channel is 5 times shorter, 
the arbitered channel will offer higher performance in terms of the number of 
spikes that get through per second. Furthermore, the cycle-time of the ar- 
biterless channel must be even shorter if low error rates are desired, as failure 
probabilities of 5 percent require it to operate at only 2.5 percent of its capac- 
ity. A comparable error in timing precision due to temporal dispersion in the 
arbitered channel occurs at 84.8% of the channel capacity, using the numbers 
given in Section 11.3.3. 
And third, the arbitered channel scales much better than the nonarbitered 
one as the technology goes to finer feature sizes, yielding higher levels of in- 
tegration and faster operation. As the number of neurons grows, the cycle 
time must decrease proportionately in order to obtain the desired throughput. 
Hence, there is a proportionate decrease in queueing time and in temporal 
dispersion--even though the number of cycles spent queueing remains the un- 
changed when the same fraction of the channel capacity is in use. Whereas the 
collision probability remains unchanged under the same conditions. 
I described the design and operation of an event-driven, arbitered interchip 
communication channel that reads out pulse trains from a 64 x 64 array of 
neurons on one chip and transmits them to corresponding locations on a 64 x 64 
array of neurons on a second chip. This design acheived a threefold to fivefold 
improvement over the first-generation design [12] by introducing three new 
enhancements. 
First, the channel used a three-stage pipeline, which allowed up to three 
address-events to be processed concurrently. Second, the channel exploited 
locality in the arbiter tree by picking the input that was closest to the previously 
selected input--spanning the smallest number of levels in the tree. And third, 
the channel exploited locality in the row-column organization by sending all 
requests in the selected row without redoing the arbitration between columns. 
I identified three inefficiencies and one bug in my implementation, and I 
suggested circuit modifications to address these issues. First, to reduce the 
propagation delay of the acknowledge signal, we must remove the AND gate 

NEUROMORPHIC CHIPS 
257 
at the top of the arbiter tree, and disable the select signals at the bottom of 
the arbiter tree instead. Second, to reset the column and row wired-OR lines 
while the receiver is latching the address-event and activating the acknowledge 
signal, we must remove the input to the wired-OR from the arbiter and redesign 
the row/column C-element. Third, to decrease the time the selected row takes 
to drive its requests out on the column lines, we must double the size of the 
pull-down devices in the pixel. And fourth, to fix the multiple-row-selection 
bug, we must guarantee that the row request signal is stable by disabling all 
the neurons in a row whenever that row is selected. 
These modifications will provide error-free operation and will push the ca- 
pacity up two and a half times, to 6.4MS/s. According to my calculations, for 
neurons with a mean latency of 2ms, a coefficient of variation of 10%, and an 
firing-rate attenuation factor of 40, this capacity will be enough to service a 
population of up to 82,000 neurons. 
However, as the number of neurons increases, the time it takes for the pixels 
in the selected row to drive the column lines increases proportionately. As this 
interval is a significant fraction of present design's cycle time (38% in the mod- 
ified design), the desired scaling will not be acheived unless the ratio between 
the unit current and the unit capacitance increases linearly with integration 
density. SRAM and DRAM scaling trends indicate that this ratio increases 
sublinearly, and hence the present architecture will not scale well. We need to 
develop new communication channel architectures to address this issue. 
Acknowledgments 
This work was partially supported by the Office of Naval Research; DARPA; the 
Beckman Foundation; the Center for Neuromorphic Systems Engineering, as part 
of the National Sceince Foundation Engineering Research Center Program; and the 
California Trade and Commerce Agency, Office of Strategic Technology. 
I thank my thesis advisor, Carver Mead, for sharing his insights into the operation 
of the nervous system. I also thank Misha Mahowald for making available layouts 
of the arbiter, the address encoders, and the address decoders; John Lazzaro, Alain 
Martin, Jose Tierno, and Tor (Bassen) Lande for helpful discussions on address events 
and asynchronous VLSI; Tobi Delbriick for help with the Macintosh address-event 
interface; and Jeff Dickson for help with PCB design. 
Notes 
1. This result is derived by assuming independent firing probabilities and approximating 
the resulting binomial distribution with the Poisson distribution. 
2. This result is also based on the assumption that samples arrive according to a Poisson 
process. 
3. Xsel does not go high at this point because the X-arbiter has not received any requests, 
as a row has not yet been selected. 
4. This modification was suggested to me by Tobi Delbriick. 
5. This solution was suggested to me by Jose Tierno. 

258 
NEUROMORPHIC SYSTEMS ENGINEERING 
References 
[1] /~. Abusland, T. Lande, and M. Hovin. A VLSI communication architec- 
ture for stochastically pulse-encoded analog signals. In Proceedings of the 
IEEE International Symposium on Circuits and Systems, volume 3, pages 
401-404, Atlanta, GA, 1996. 
[2] K. Boahen. Retinomorphic vision systems. In Int. Conf. on Microelec- 
tronics for Neural Networks, volume 16-5, pages 30-39, Los Alamitos, CA, 
1996. EPFL/CSEM/IEEE. 
[3] K. Boahen. Retinomorphic vision systems I: Pixel design. In Proceedings 
of the IEEE International Symposium on Circuits and Systems, volume 
supplement, pages 14-19, Atlanta, GA, May 1996. 
[4] K. Boahen. Retinomorphic vision systems II: Communication channel 
design. In Proceedings of the IEEE International Symposium on Circuits 
and Systems, volume supplement, pages 9 14, Atlanta, GA, May 1996. 
[5] K. A. Boahen. A retinomorphic vision system. IEEE Micro, 16(5):30-39, 
October 1996. 
[6] K. A. Boahen. The retinomnorphic approach: Pixel-parallel adaptive am- 
plification, filtering, and quantization. Analog Integr. Circ. and Sig. Proc., 
13:53-68, 1997. 
[7] K. A. Boahen. 
Retinomorphic Vision Systems: Reverse Engineering 
the Vertebrate Retina. 
PhD thesis, California Institute of Technology, 
Pasadena CA, 1997. 
[8] G. Cauwenberghs. A micropower CMOS algorithmic A/D/A converter. 
IEEE Transactions on Circuits and Systems I: Fundamental Theory and 
Applications, 42(11):913-919, 1995. 
[9] L. Kleinrock. Queueing Systems. Wiley, New York NY, 1976. 
[10] J. Lazzaro, J. Wawrzynek, , and A. Kramer. Systems technologies for 
silicon auditory models. IEEE Micro, 14(3):7-15, June 1994. 
[11] J. Lazzaro, J. Wawrzynek, M. Mahowald, M. Sivilotti, and D. Gillespie. 
Silicon auditory processors as computer peripherals. 
IEEE Journal of 
Neural Networks, 4(3):523-528, 1993. 
[12] M. Mahowald. An Analog VLSI Stereoscopic Vision System. Kluwer Aca- 
demic Pub., Boston, MA, 1994. 
[13] C. A. Mead. Neuromorphic electronic systems. In Proceedings of the IEEE, 
volume 78-10, pages 1629-1639, 1990. 
[14] A. Mortara, E. A. Vittoz, and P. Venier. A communication scheme for 
analog VLSI perceptive systems. IEEE Journal of Solid State Circuits, 
SC-30(6):660-669, June 1995. 
[15] A. F. Murray and L. Tarassenko. Analogue Neural VLSI: A Pulse Stream 
Approach. Chapman and Hall, London, England, 1994. 

NEUROMORPHIC CHIPS 
259 
[16] M. Schwartz. 
Telecommunication Networks: Protocols, Modeling, and 
Analysis. Addison-Wesley, Reading, MA, 1987. 
[17] M. Sivilotti. Wiring considerations in analog VLSI systems with applica- 
tions to field programmable networks. PhD thesis, California Institute of 
Technology, Pasadena CA, 1991. 
[18] I. E. Sutherland. Micropipelines. Communications of the ACM, 32(6):720- 
738, 1989. 
[19] A. S. Tanenbaum. Computer Networks. 
Prentice-Hall International, 2 
edition, 1989. 

IV Neuromorphic Technology 

snoonb~ u~ u! sau~sqmoua p!d![ ~u!sn dq 'a~a~qa o~uoa~aia s~ u~uoo ol sza[~ 
-a~q X~aua ~[[m~s s~aaaa ma~sds snoAaa~ a~& "uo~aun[ ud ~ u~ aa~az~q X~aaua 
aRa ao 'ap~xo~p uoa~w pu~ uoa~I F uaa~aaq aauaaa~[p uo~aaunj-~ao~ aRa Smsn 
dq 'aSavqa a~uoaaaaI a aqa u~muoa oa saa~aa~q gSaaua aaaaa a~ 'sa~uoaaaala aoaanp 
-uoa~mas u I 'pa~nq~a~s~p uu~mz~[o a oar osiv sa~aaua a!aq~ ~sSu!punoaans a~aqa 
qa~ mn~aq[I[nba l~maaql u~ aa~ sum 'anss~a aAaOU U I "paanq[aas~p uuvmzaioa 
oar sa~$aaua a!oqa ~sSu~punoaans apqa qa~ mn!aq!I!nba Fmaaqa u! aa~ suoaaaoia 
'sa~uoa~aOlO aoaanpuoa~mas u I -uo~aaoguoa oSa~qa 3o s~svq oq~ no dllvdDu:ad 
paa~lnd[u~m s[ uo~avmaoju[ 'onsg!a snoAaaU pu~ sa~naa D paa~aSaau~ qloq uI 
â¢ saoandmoa Im~$~p ano pi~nq oa Xoldmo am a~qa sa[uoaaaOla 
soaanpuoa~mas oq~ sa~Iaapun saFXqd oa~aop a~I~mSs V "pumsaapun pay ~ou~ 
am a~qa saFdqd aa~Aap ~U~dlaapun u~ ~msn os saop a~ 'aAIos oa moq mou~ aou 
op a~ l~qa Smalqoad SaAlOS anss~l snoAaaU q~noq~iV "~u~ssaaoad uo~avmao~u~ 
Fanau ~o saId~au~ad aR~ pumsaapun a~ aauo 'sa~uoaaaala Ra~ a~inma aouuva 
o~ a~qa maasXs snoAaaU aqa u~ ouop s~ ~q~ ~u~q~ou s~ oaaqa a~qa OAO~laq a~ 
â¢ uo!a~andmoa jo maq auop~o oaom 
Â£Dsva - vavp pouo~a~puoa Â£iaood uo - puv ~uaaa~p ~ ~noq~ Â£~OlO~qoanou moaj 
ua~o I u~a am a~qa saldpu[ad I~auam~punj aa~ aaaqÂ£ "pu~asaopun aou op 'saoou 
-tSua pu~ sas~auaDs ~ 'am lvql sÂ£~ ut os op ~oqa ~u~ua~o I puv 'ioaauoa aoaom 
'~u~ssaaoad qaaads pu~ a~m~ us smaIqoad pasod-II ~ 'am~ Faa u~ 'aalOS smaas*s 
snoAaa N 'smstu~ao ~uD~ I jo smaas*s snoaaau oqa u~ posn sald~au[ad Fuo~a~z~u 
-~$ao pu~ i~uo~avandmoa aqa XoIdmo l~qa smaasXs a~uoaaaoio pi[nq oa s~ ivo$ an 0 
NOI.L3nOo~I.LNI 
['~[ 
n pa" uo~u!qsE~'s~)opo!p 
0Â§~-Â§61:86 VAA 'ala.:l-eaS 0S~Â§Â£ xo8 'lleH ~a~S ~T 
uo~u~qse~ jo ~S~aA~U R aq$ 
~u~Jaau~u 3 pue a~uaps Ja~nd~o D jo ~ua~eda o aql 
o!~o!(1 sPHD 
gI 

264 
NEUROMORPHIC SYSTEMS ENGINEERING 
solution. In both systems, when the height of the energy barrier is modulated, 
the resulting current flow is an exponential function of the applied voltage. 
Both systems use this principle to produce devices that exhibit signal gain. 
Transistors use populations of electrons to change their channel conductance, 
in much the same way that neurons use populations of ionic channels to change 
their membrane conductance. 
We believe that the disparity between the computations that can be done by 
a digital computer and those that can be done by the nervous system is a conse- 
quence of the way that the underlying physics is used to effect the computation. 
The state variables in both electronic and nervous systems are analog. They 
are represented in electronic systems by electric charge, and in nervous systems 
by electric charge or by chemical concentrations. The mechanisms by which 
each systems manipulates its state variables to do computation, however, are 
vastly different. In a digital computer, we ignore most of the available states 
in favor of the two binary-valued endpoints: We achieve noise immunity at the 
expense of dynamic range. The nervous system retains the analog dynamic 
range, achieving noise immunity by adjusting the signal-detection threshold 
adaptively. Digital machines quantize their analog inputs, and use restoring 
logic at every computational step. Nervous systems perform primarily analog 
computations, and quantize the computed result. 
Unfortunately, we do not know what computational primitives neural sys- 
tems use, how they represent information, or what their organizing principles 
are. However, because semiconductor electronics allows us to apply, at a high 
level of integration, a device physics similar to that used by neural tissue, 
we conclude that we should be able to build electronic circuits that mimic the 
computational primitives of nervous systems, and that we should be able to use 
these circuits to explore the organizational principles employed by neurobiology. 
We call the approach silicon neuroscience: the development of neurobiologically 
inspired silicon learning systems. 
Our predecessors began these investigations by modeling two of the sensory 
organs available to neural systems: the retina and the cochlea [1, 6, 7, 24]. 
The silicon retina and cochlea are now well developed and mimic some of the 
sensory preprocessing performed by living organisms. The papers in this part 
both extend the existing retina and cochlea models, and chart new pathways 
for investigation. 
In "Winner-take-all networks with lateral excitation," by Giacomo Indiveri, 
the author describes an extension to the class of winner-take-all networks orig- 
inally presented by J. Lazzaro, et. al., in [43]. This improved circuit is certain 
to become a valuable building block in neuromorphic VLSI design, and has 
particular relevance for retinal image-processing systems. In "A low-power 
wide-linear-range transconductance amplifier," by Rahul Sarpeshkar, Richard 
F. Lyon, and Carver Mead, the authors describe and analyze a wide-range 
transconductance amplifier applicable to analog-VLSI silicon cochleae. This 
amplifier permits the construction of artificial, neurally inspired silicon cochleae 
with a much wider dynamic range than was possible previously. 

FROM NEUROBIOLOGY TO SILICON 
265 
Other reseoxchers, myself included, are beginning to model what is perhaps 
the most remarkable aspect of living organisms - their abilities to adapt and to 
learn. In "Floating-gate MOS synapse transistors," by Chris Diorio, Bradley A. 
Minch, Paul Hasler, and Carver Mead, we describe compact, single-transistor 
devices that, like neural synapses [4], implement long-term nonvolatile analog 
memory, allow bidirectional memory updates, and learn from an input signal 
without interrupting the ongoing computation [7, 60]. Although we do not 
believe that a single device can model the complex behavior of a neural synapse 
completely, our synapse transistors do implement a local learning function. 
Finally, still other researchers are exploring neural function in silicon inte- 
grated circuits. Neurons are the nervous system's primary computing elelnents: 
They appear to perform and to learn from spatio-temporal input-signal cor- 
relations. In "Neuromorphic synapses for artificial dendrites," by Wayne C. 
Westerman, David P. M. Northmore, and John G. Elias, the authors describe 
variable-weight neuromorphic synapses on artificial dendrites. The synapses, 
dendrites, and the associated interface circuitry permit the investigation and 
comparison of correlative adaptation mechanisms in a mixed software-hardware 
model of neural function. By fabricating and testing structures such as these, 
the authors may provide valuable insights into adaptation and learning in bio- 
logical and artificial neural systems. 
As a research community, my colleagues and I generally believe that if we can 
understand the principles on which biological information-processing systems 
operate, then we can build circuits and systems that deal naturally with real- 
world data. Our goal, therefore, is to consider the computational principles on 
which neural systems operate, and to model and understand those principles in 
the silicon medium. We consider these papers to represent a small step toward 
achieving our goal. 
References 
[1] K. Boahen. Retinomorphic vision systems. In Int. Conf. on Microelectronics 
for Neural Networks, volume 16-5, pages 30-39, Los Alamitos, CA, 1996. 
EPFL/CSEM/IEEE. 
[2] C. Diorio, P. Hasler, B. A. Minch, and C. Mead. A single-transistor silicon 
synapse. IEEE Trans. Electron Devices, 43(11):1972-1980, 1996. 
[3] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses. In Advances in Neural Information Processing Systems 7, pages 
817-824. MIT Press, Cambridge, MA, 1995. 
[4] C. Koch. Computation and the single neuron. Nature, 385(6613):207-210, 
January 1997. 
[5] J. P. Lazzaro and C. Mead. Circuit models of sensory transduction in 
the cochlea. In Mead and Ismail, editors, Analog VLSI Implementation of 
Neural Systems, pages 85-101. Kluwer Academic Publishers, Norwell, MA, 
1989. 

266 
NEUROMORPHIC SYSTEMS ENGINEERING 
[6] M. Mahowald and C. Mead. 
The silicon retina. 
Scientific American, 
264(5):76-82, 1991. 
[7] C. A. Mead. Analog VLSI and Neural Systems, pages 179-192, 279-302. 
Addison-Wesley, Reading, MA, 1989. 
[8] R. Sarpeshkar, Lyon R. F., and C. A. Mead. An analog VLSI cochlea with 
new transconductance amplifiers and nonlinear gain control. In Proc. IEEE 
Intl. Conf. on Circuits and Systems, volume 3, pages 292-295, Atlanta, May 
1996. 

3 
A LOW-POWER 
WIDE-LINEAR-RANGE 
TRANSCONDUCTANCE AMPLIFIER 
Rahul Sarpeshkar I, Richard F. Lyon 2, and Carver Mead 3 
1Department of Biological Computation, 
Bell Laboratories, Murray Hill, NJ 07974 
ra h ul~physics.bell-labs.com 
:~Foveonics Inc., I0131-B Bubb Rd., Cupertino CA 95014 
3Physics of Computation Laboratory, California Institute of Technology 
13.1 
INTRODUCTION 
In the past few years, engineers have improved the linearity of MOS transcon- 
ductor circuits [2, 5, 10, 11, 19, 20, 26, 28, 29, 32]. These advances have been pri- 
marily in the area of above-threshold, high-power, high-frequency, continuous- 
time filters. Although it is possible to implement auditory filters (20Hz-20khz) 
with these techniques, it is inefficient to do so. The transconductance and cur- 
rent levels in above-threshold operation are so high that large capacitances or 
transistors with very low W/L are required to create low-frequency poles, and 
area and power are wasted. In addition, it is difficult to span 3 orders of mag- 
nitude of transconductance with a square law, unless we use transistors with 
ungainly aspect ratios. However, it is easy to obtain a wide linear range above 
threshold. 
In above-threshold operation, identities such as (x - a) e - (x - b) e = (b - 
a) (2x - a - b) are used to increase the wide linear range even further. In bipo- 
lar devices where the nonlinearity is exponential, rather than second-order, it 
is much more difficult to completely eliminate the nonlinearity. The standard 
solution has been to use the feedback technique of emitter degeneration, which 
achieves wide linear range by reducing transconductance, and is described by 
Gray [7]. A clever scheme for widening the linear range of a bipolar transcon- 
ductor that cancels all nonlinearities up to fifth order, without reducing the 

268 
NEUROMORPHIC SYSTEMS ENGINEERING 
FI 
V. 
W 
GM 
]1 
N 
3g 
~ Vo s 
I 
I <M 
i 
lout 
> ~  
I<M 
Figure 13.1 
The Wide Linear Range Transconductance Amplifier. The figure shows the 
transconductance amplifier with v+ and v_ well-input voltages and the TIIlot~t current output. 
The voltage VB sets the bias current of the amplifier and the voltage Vos allows a fine 
adjustment of its offset if necessary. The S transistors reduce the transconductance of the 
differential-pair W transistors through source degeneration. The GM transistors reduce the 
transconductance of the W transistors through gate degeneration. They also serve to mirror 
the differential-pair currents to the output of the amplifier. The B transistors implement 
bump linearization. The M transistors form parts of the current mirrors in the circuit. 
transconductance, has been proposed by Wilson [31]. A method for getting 
perfect linearity in a bipolar transconductor by using a translinear circuit and 
a resistor has been demonstrated by Chung [3]. Both of the latter methods, 
however, require the use of resistors, and ultimately derive their linearity from 
the presence of a linear element in the circuit. Resistors, however, cannot 
be tuned electronically, and require special process steps. 
Various authors 
have used an MOS device as the resistive element in an emitter-degeneration 
scheme to make a BiCMOS transconductor--for example, the scheme proposed 
by Sanchez [22]. Another BiCMOS technique, reported by Weimin [13], uses 
an above-threshold differential pair to get wide linearity, and scales down the 
output currents via a bipolar Gilbert gain cell, to levels more appropriate for au- 
ditory frequencies. Above-threshold differential pairs, however, result in lower 
open-loop gain and higher voltage offset, and techniques such as cascode mir- 
rors are required to improve these characteristics. Cascode mirrors, however, 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
269 
degrade dc output-voltage operating range and consume area. In addition, 
above-threshold operation results in higher power dissipation. 
Subthreshold MOS technology, like bipolar technology, is based on expo- 
nential nonlinearities. Thus, it is natural to employ source-degeneration tech- 
niques. Methods for getting wider linear range that exploit the Early effect in 
conjunction with a source-degeneration method are described by Arreguit [1]. 
The Early voltage is, however, a parameter with high variance across transis- 
tots; thus, we cannot expect to get good transconductance matching in this 
method. Further, such schemes are highly offset prone, because any current 
mismatch manifests itself as a large voltage mismatch due to the exceptionally 
low transconductance. 
The simple technique of using a diode as a source-degeneration element 
extends the linear range of a differential pair to about +150 mV, as described 
by Watts [90]. However, it is difficult to increase this linear range further 
by using two stacked diodes in series as the degeneration element--the wider 
linear range that is achieved is obtained at the expense of a large loss in dc- 
input operating range. If we operate within a 0 to 5V supply, the signal levels 
remain constrained to take on small values, because of the inadequate dc input 
operating range. 
Three techniques for improving the linear range of subthreshold differen- 
tial pairs have been described in [6]. The authors define the linear range 
to be the point where the transconductance drops by 1%. By that defini- 
tion, the linear range of a conventional transconductance amplifier described 
by Iout = Is tanh (X/VL) is VL/5. Their best technique achieved a value of 
VL = 584 mV, and involved expensive common-mode biasing circuitry. In con- 
trast, our technique yields a VL of 1.7 V, and involves no additional biasing 
circuitry. 
In [21], a 21-transistor subthreshold transconductance amplifier is described. 
From visual inspection of their data, the amplifier has a VL of about 700 mV. 
They estimate from simple theoretical calculations that the effective number 
of shot-noise sources in their circuit is about 20. In contrast, our 13-transistor 
circuit has a VL of 1.7 V, and the effective number of shot-noise sources in our 
circuit is around 5.3 (theory) or 7.5 (experiment). 
We can solve the problem of getting wider linear range by interposing a 
capacitive divider between each input from the outside world and each input 
to the amplifier. Some form of slow adaptation is necessary to ensure that the 
de value of each floating input to the amplifier is constrained. This approach, 
as used in an electronic cochlea, is described in [14]; it did not work well in 
practice because of its sensitivity to circuit parasitics. As we shall see later, 
capacitive-divider schemes bear some similarity to our scheme. We shall discuss 
capacitive-divider techniques in Section 13.5.4. 
To get low transconductance, we begin by picking an input terminal that is 
gifted with low transconductance from birth: the well. We reduce the transcon- 
ductance further by using source degeneration, and a new negative-feedback 
technique, which we call gate degeneration. Finally, we use a novel technique, 

270 
NEUROMORPHIC SYSTEMS ENGINEERING 
which we call bump linearization, to extend the linear range even more; bump 
circuits have been described in [4]. The amplifier circuit that incorporates all 
four techniques is shown in Figure 13.1. 
In Section 13.2, we present all the essential ideas and first-order effects that 
describe the operation of the amplifier. We describe second-order effects, such 
as the common-mode and gain characteristics, in Section 13.3. We discuss the 
operation of this amplifier as a follower-integrator filter in Section 13.4. We 
elaborate on noise and dynamic range in Section 13.5. In Section 13.6, we con- 
clude by summarizing our contributions. Appendix A contains a quantitative 
treatment of common-mode effects on the amplifier's transconductance. Sec- 
tion A.1 describes the effects of changing transconductance; Section A.2 is on 
the effects of parasitic bipolar transistors present in our well-input amplifier. 
Normally, the amplifier operates in the 1V to 5V range, where these bipolar 
transistors are inactive. 
13.2 
FIRST-ORDER EFFECTS 
We begin by expressing basic transistor relationships in a form that will be 
useful in our paper. We use standard IEEE convention for large-signal (iDs), 
dc (IDs), and small-signal (iris) variables. 
13.2.1 
Basic Transistor Relationships 
The current in a subthreshold MOS well transistor in saturation is given by 
(_Kvcs~ 
( 
(1-K)vws) 
(13.1) 
iDS "= Io exp 
UT ] exp 
-- 
~T 
]' 
where vGs and vws are the gate-to-source and well-to-source voltage, respec- 
tively; ~ is the subthreshold exponential coefficient; I0 is the subthreshold 
current-scaling parameter; UT ---- kT/q is the thermal voltage; and VDS >> 5UT. 
Eq. (13.1) illustrates that the gate affects the current through a ~ expo- 
nential term, whereas the well affects the current through a 1 - ~ exponential 
term. Thus, when the gate is effective in modulating the current, the well is 
ineffective, and vice versa. By differentiating Eq. (13.1), we can easily show 
that the gate, well, and source transconductances are 
OiDS 
__ iris 
- IDS 
9gt 
= 
OVG 
-- 
Fg 
= 
--~ 
UT ' 
OiDs _~ 
= 
_(1_~)~ 
(13.2) 
gwl 
-= 
Ovw 
-- 
v~. 
UT ' 
OiDS 
__ iris 
IDS 
BS ~---- Ors 
-- 
V~ 
~-- 
UT ' 
respectively. Thus if and only if ~ > 0.5--which is almost always the case--then 
the well transconductance has a lower magnitude than the gate transconduc- 
tance, and the well is preferable over the gate as a low-transconductance input. 
It is convenient to work with dimensionless, small-signal variables: If id and 
Vd are arbitrary small-signal variables, and we define the dimensionless variables 
i = id/ID, v = Vd/UT, then a relation such as id = gdVd = ~IDVd/UT takes the 

A LOW-POWER 
WIDE-LINEAR-RANGE 
TRANSCONDUCTANCE 
AMPLIFIER 
271 
V s 
( 
v S ?,,o, 
E 
~I E~s (v,-v 
1 - ~)I ~s (v,-v ~ ) 
VD 
V d 
) 
+ 
vg 
(a) 
(b) 
(c) 
Figure 13.2 
Signal-Flow Diagram for a Saturated Well Transistor. (a) A well transistor 
with marked voltages and currents. 
(b) The small-signal equivalent circuit for the well 
transistor. (c) The signal-flow diagram represents the dimensionless relationships between 
the small-signal variables of the transistor. In our paper it shall prove to be the most 
useful and insightful way of analyzing transconductance relationships, rather than the more 
conventional circuit representation shown alongside it. 
In the signal-flow diagram, it is 
understood that all currents are normalized by IDS, all voltages are normalized by UT, and 
all transconductances are normalized by IDs/UT. 
Thus, the small-signal relationship of 
the transistor takes the simple form ids = vs -- EVg -- (1 -- E)v w. 
simple form i = ~v. We notice then that ~ plays the role of a dimensionless 
transconductance; that is, ~ = 9d/(ID/UT) is the dimensionless transconduc- 
tance that we obtain by dividing the real transconductance gd by ID/UT. We 
shall use the dimensionless variable forms to do most of our calculations, and 
then shall convert them back to the real forms. For convenience, we denote the 
dimensionless variable by the same name as that of the variable from which it 
is derived. Thus, Eq. (13.2) when converted to its dimensionless form, simply 
reads ggt = -n, gwl = -(1 - ~), andros = 1. 
Figure 13.2 shows a well transistor, its small-signal-equivalent circuit, and a 
signal-flow diagram that represents its small-signal relations. In this paper, we 
shall ignore the capacitances that would be represented in a complete small- 
signal model of the transistor. 
13.2.2 
Transconductance Reduction Through Degeneration 
The technique of source degeneration is well known, and was first used in 
vacuum-tube design; there it was referred to as cathode degeneration, and was 
described by Landee [12]. Later, it was used in bipolar design, where it is 
referred to as emitter degeneration [7]. The idea behind source degeneration 
is to convert the current flowing through a transistor into a voltage through a 
resistor or diode, and then to feed this voltage back to the emitter or source of 
the transistor to decrease its current. 

272 
NEUROMORPHIC SYSTEMS ENGINEERING 
V C 
~ 
iDS 
1/~p, 
- 
i~ 
~ 
(1-,<)(v~-v 
~,ia~ 
1/~ n 
(a) 
(b) 
v s 
w) 
â¢ 
â¢ 
vg 
(c) 
Figure 13.3 
Transconductance Reduction through Degeneration. (a) A half circuit for 
one differential arm of our amplifier, if we ignore the B transistors of Figure 13.1 for now. 
(b) A small-signal equivalent circuit for this configuration. (c) A signal-flow diagram for 
this configuration. The signal-flow diagram shows that the transconductance reduction is 
achieved through two parallel feedback loops. The top loop is due to source degeneration 
and the bottom loop is due to gate degeneration. A more detailed description of the feedback 
concepts assosciated with transconductance reduction may be found in Section 13.2.2. 
Gate degeneration has never been reported in the literature to our knowl- 
edge. This lacuna probably occurs because most designs use the gate as an 
input and thus never have it free to degenerate. The vacuum-tube literature, 
however, shows familiarity with a similar concept, called screen degeneration, 
as described in Landee [12]. The idea behind gate degeneration is to convert the 
current flowing through a transistor into a voltage through a diode, and then 
to feed this voltage back to the gate of the transistor to decrease its current. 
Figure 13.3a shows a half-circuit for one differential arm of the amplifier of 
Figure 13.1, if we neglect the B transistors for the time being. The source- 
degeneration diode is the pFET connected to the source of the well-input tran- 
sistor; the gate-degeneration diode is the nFET connected to the drain of the 
well-input transistor. 
The gate-degeneration diode is essentially free in our 
circuit, because it is part of the current mirror that feeds the differential-arm 
currents to the output. The voltage Vc represents the common-node voltage 
of the differential arms. In differential-mode ac analysis, the common-node 
voltage is grounded, as explained in the following paragraph. 
In Figure 13.1, if v+ = v_, and the amplifier is perfectly matched, then a 
quiescent current of !~ flows through each branch of the amplifier and iOUT 
will be 0. If we now vary the differential voltage, Vd = v+ -- v_, by ~t small 
amount, the current changes by io~,t = gVd, where g is the transconductance 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
273 
of the amplifier. 
We would like to compute g. 
If we apply Vd, such that 
v+ changes by +~ 
and v_ changes by -~ 
then the common node of the 
2' 
two differential halves (the source of the S transistors) does not change in 
voltage. For the purposes of small-signal analysis, we can treat the common 
node as a virtual ground. Thus, if gh is the transconductance of the half-circuit 
shown in Figure 13.3a, the output current is gh~ - (-gh~ t) = ghVd. Hence, 
the transconductance of the half-circuit, biased to the current level of IB/2, is 
the transconductance of the amplifier. 
The circuit of Figure 13.3a yields the small-signal circuit of Figure 13.3b: The 
source-degeneration diode is represented by a dimensionless resistor of value 
1/np, the gate-degeneration diode is represented by a dimensionless resistor 
of value 1/nn, the gate-controlled current source of Figure 13.2 is represented 
by a dimensionless resistor of value 1/n (the gate is tied to the drain), and 
the well-controlled current source of Figure 13.2 is represented by a dependent 
source, 
as shown. 
The left half of Figure 13.3c represents the signal-flow diagram for the well- 
input transistor, as derived in Figure 13.2. 
The right half of Figure 13.3c 
represents the blocks due to the source or gate degeneration diodes feeding 
back to the source or gate. Thus, we have two negative-feedback loops acting 
in parallel to reduce the transconductance. One loop feeds back the output 
current to the source via a -1/e;p block; the other loop feeds back the output 
current to the gate via a 1/nn block. Since the magnitude of the loop gains of 
~ and A~- 
'~ 
the source-degeneration and gate-degeneration loops are A8 = ~ 
- ~--~ 
respectively, the well transconductance is attenuated by 
1 
â¢ that is to 
l+As+Ag ~ 
say, the transconductance is 
1--~ 
.. 
(13.3) 
g--l+~-~+--~ 
We multiply the dimensionless transconductance thus computed by ~ 
to get 
2UT 
the actual transconductance, since IDS in each differential arm is !~ 
2" 
The W, S, and G transistors of each differential arm may be regarded as a 
single transistor with I-V characteristics given by 
I (x e -(vs-gvW)/vr. 
By following the steps outlined by Mead [17], we can easily derive that 
(g(v+_nv_)~, 
(13.4) 
iour = IB tanh \ 
2UT 
] 
where g is given by Eq. (13.3). 
13.2.3 Bump Linearization 
Bump linearization is a technique for linearizing a tanh and extending the linear 
range of a subthreshold differential pair [15]. We shall first explain how it works 

274 
NEUROMORPHIC SYSTEMS ENGINEERING 
for the simple differential pair; then, we shall extend this explanation to our 
amplifier in a straightforward fashion. 
A bump differential pair has, in addition to its two outer arms, a central 
arm containing two series-connected transistors [4]. The current through the 
transistors in the central arm is a bump-shaped function of the differential 
voltage, so we call these transistors bump transistors. 
Thus, the differential 
output current from the outer two arms, I, is the usual tanh-like function of 
the differential voltage, V, except for a region near the origin, where the bump 
transistors steal current. By ratioing the W/L of the bump transistors to be w 
with respect to the transistors in the outer arms, we can control the properties 
of this I-V curve. A small w will essentially leave it unchanged. A large w will 
cause a fiat zone near the origin, where there is current stealing. The larger 
the w, the larger the width and flatness of the zone. At intermediate values 
of w, the expansive properties of the curve due to the bump compete with the 
compressive properties of the curve due to the tanh, and a curve that is more 
linear than is a tanh is obtained. At w = 2, the curve is maximally linear. 
For a simple subthreshold bump amplifier with bump scaling w, the differ- 
ential output current can be shown to be 
where 
sinh x 
io~t - fl + cos lhx' 
(13.5) 
W 
Z = 
l+g, 
(13.6) 
q~(v+ - v_) 
x 
- 
kT 
(la.7) 
The bias current is assumed to be 1, without loss of generality. Simple calculus 
shows that the first and second derivatives of Eq. (13.5) are 
diout 
1 + ~ cosh x 
-- 
dx 
(/~ + cosh x) 2' 
d2iout 
_ 
sinhx(/~2-Zcoshx-2) 
dx 2 
(~ + coshx) 3 
(13.s) 
(f12 _ flcoshx- 2) ~ 0 
(13.9) 
for all x. Note that in deriving Eq. (13.9), we have used the facts that the 
denominator term of Eq. (13.8) is always positive, and that sinh x changes sign 
at only the origin. The worst possible case for not meeting the constraint given 
If we require that the iout-vs.-x curve have no points of inflection in it except 
at the origin, then it must always be convex or concave in the first or third 
quadrant; this requirement is necessary to ensure that there are no strange 
kinks in the I-V curve. In Eq. (13.8), we must then have 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
275 
by Eq. (13.9) occurs when coshx is at its minimum value of 1 at x = 0. If we 
set cosh x = 1 and solve the resulting quadratic for 3, we obtain 
3_<2, 
=~w 
< 
2. 
-
-
 
Thus, at w = 2, we are just assured of satisfying this constraint. At w = 2, 
the iout-vs.-x curve is maximally linear: If we Taylor expand Eq. (13.5) at 
3 = w = 2, we find that it has no cubic-distortion term. In comparison, the 
function tanhx/2, which is to what Eq. (13.5) reduces when w = 0 or 3 = 1, 
has cubic distortion. 
tanh x 
x 
x 3 
x 5 
17x 7 
-- 
+ 
-
-
 
+ .. 
(13.10) 
2 
2 
24 
240 
40320 
sinh x 
x 
x 5 
x 7 
x 9 
-
-
 
-
-
 
+ 
-
-
 
+ 
. .  
(13.11) 
2 + cosh x 
3 
540 
4536 
77760 
At x = 1, the tanh function has cubic distortion of approximately 8%, as com- 
pared to the linearized tanh function which has no cubic distortion whatsoever, 
but has only fifth-harmonic distortion of less than 1%. Thus, we have linearized 
the tanh. We shall show later, in Section 13.5, that bump linearization is a par- 
ticularly useful technique because it increases the linear range of an amplifier 
without increasing that amplifier's noise. 
Our circuit topology in Figure 13.1 implements a wide-linear-range bump 
differential pair by ratioing the W/L of the the nFET B transistors to be w with 
respect to the W/L of the nFET GM transistors. However, the mathematical 
analysis of the circuit is identical to that of a simple bump differential pair if 
we replace the n of Eq. (13.7) with the g of Eq. (13.3). 
13.2.4 
Experimental Data 
In summary, from Eqs. (13.3) and (13.4), we get the transfer function of the 
amplifier with the B transistors absent: 
Io~t 
= 
IB tanh(Vd/VÂ¢), 
(13.12) 
Vd 
= 
V+-V_, 
vÂ¢ 
: 
(2 
T/q)(1 + 
. 
(13.13) 
Prom Eqs. (13.3), (13.4), (13.5), (13.6), and (13.7), the transfer function of the 
overall amplifier has the form 
sinh(2x) 
(13.14) 
Io~t = IB ~ + w/2 + cosh(2x)' 
where x = Vd/VÂ¢. The W/L of the B transistors is w times the W/L of the 
GM transistors. 

276 
NEUROMORPHIC SYSTEMS ENGINEERING 
We fabricated our transconductance amplifier in a standard 2#m CMOS n- 
well process, and obtained data from it. We have also used the amplifier in 
a working silicon cochlea [24, 25]. Figure 13.4a shows experimental data for 
our amplifier for values of w = 0, 1.86, and 66, fitted to Eq. (13.14). Note 
that, even at rather large w, the nonlinear characteristics are still gentle. This 
property ensures that even fairly large mismatches in w do not degrade the 
operation of the amplifier significantly. For w ~ 2, Figure 13.4b shows that 
the simple tanh fit of Eq. (13.12) is a good approxinmtion to Eq. (13.14) if 
VL ~ (3/2)VL--that is to say, from 1.16 V for w = 0 to 1.7 V for w -- 2. 
Considering the leading-order terms of Eqs. (13.10) and (13.11), the factor of 
3/2 seems natural. However, we shall show, from the follower-integrator data 
of Section 13.4, that the tanh approximation is inadequate for large differential 
inputs, because the curve is more linear than is that of a tanh. 
We verify Eq. (13.13) experimentally in Section A.1. Since ~ varies with 
the well-to-gate voltage, and consequently with the common-mode voltage, the 
verification is involved; we have relegated the details to the appendix. 
13.3 
SECOND-ORDER EFFECTS 
We shall now discuss several second-order effects in our amplifier. We begin by 
describing the common-mode characteristics. 
13.3.1 Common-Mode Characteristics 
Below an input voltage of 1V, the well-to-source junction becomes forward 
biased, and the parasitic bipolar transistors, which are part of every well tran- 
sistor, shunt the current of the amplifer to ground. Thus, as Figure 13.5a shows, 
the output current of the amplifier falls. Figure 13.5b illustrates the same ef- 
fects, but from the perspective of varying the differential-mode voltage, while 
fixing the common-mode voltage. In this figure, we can see that changes in 
~ increase the transconductance of the amplifier as the common-mode voltage 
is lowered. The increase is exhibited as a rise in the slope at the origin. At 
very low common-mode voltages, at a particular differential voltage, the input 
voltage for one arm or the other falls below 1V, and the bipolar transistors in 
that arm begin to shunt the current to ground. The lower the common-mode 
voltage, the smaller the differential voltage at which shunting begins. Thus, 
at low common-mode voltages the current starts to fall at small magnitudes of 
differential voltage. 
We clarify the bipolar effect in Figure 13.6: Figure 13.6a shows a verti- 
cal crosssection of a well transistor in an n-well process; Figure 13.6b shows 
that the equivalent circuit of this well transistor contains two parasitic bipo- 
lar transistors. 
Typically, the well-to-source and well-to-drain junctions are 
always reverse biased, so that these bipolar transistors are turned off. For our 
amplifier, and in most cases, the source-to-drain voltage is sufficiently positive 
that the bipolar transistor at the source is the one that is turned on, if any is, 
whereas the one at the drain is hardly turned on. Thus, in Figure 13.6c, we 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
277 
2.5 x 10 .8 
1.5 
1 
'E 0.5 
~ 
0 
â¢ . -0.5 
N 
-1 
o 
, 
, 
, 
j 
x--- w=66 
-1.5 
-2 
-2.5 
-~ 
~ 
~ 
Differential Voltage (V) 
,3 
o 
x 10 .8 
x --- w = 1.86; V L 
= 1.72 V ~x~ 
1 
0.50 ~ 
-0.5 
-1 
-1.5 
-2 
-2.5 
_~ 
d 
~ 
Differential Voltage (V) 
Figure 13.4 
I-V curves for the Amplifier. (a) Experimental data for our amplifier for 
values of t~ = 0, 1.86, and 66, fitted to Eq. (13.14). (b) For w ~ 9., the simple tanh-fit 
of Eq. (13.12) is a good approximation to Eq. (13.14), if VL --~ (3/2)VL 
have indicated the bipolar transistor at only the source. The bipolar effect is 
described in quantitative detail in Section A.2. 
Typically, we operate the 
amplifier at a common-mode voltage of about 3V, where s~nall dc offsets do not 
significantly affect its transconductance, and where the action of the bipolar 
transistors is negligible. When the bipolar transistors do turn on, there is no 
danger of latchup, because the current that is fed to the substrate is at most 
the tiny subthreshold bias current of the amplifier. Our input operating range 
of 1V to 5V is about the same as that of a simple subthreshold nFET differen- 

278 
NEUROMORPHIC SYSTEMS ENGINEERING 
10-~0 
16 
, 
, 
, 
14 
(a) 
~, 
12 
Differential Voltage = 
~ 10 
~ 
~ 
~ 
Bipolar 
I 
0 
Shunting 
"I 
14 
~ changes 
-o 
~ 
~ 
~ 
~ornmen No6e V~lta~e (V) 
2.5 x 10 8 
2[ 
b 
2.5 v~.~.~. 
~ o
~
 
~'0St 
\ 
~//// 
-'I 
"2"5-2 
-1 
0 
1 
Differential Voltage 
Figure 13.5 
Differential and Common-Mode Curves. (a) The data shows that the current 
rises as the common-mode voltage is decreased from 5V to 1V, because changes in t~ in- 
crease the transconductance of the amplifier. Below 1V, the well-to-source junction becomes 
forward biased, and the parasitic bipolar transistors, which are part of every well transistor, 
shunt the current of the amplifer to ground. (b) The same effects as in (a) but from the 
perspective of varying the dif[erential-mode voltage, while fixing the common-mode voltage. 
Further details may be found in Section 13.3.1. 
tial amplifier. These amplifiers also show transconductance changes, due to n 
changes, that are most abrupt at low common-mode voltages. 
As the common-mode input voltage of our amplifier is decreased, the well- 
to-gate voltage falls, the depletion region beneath the channel of the transistor 
shrinks, n decreases, the transconductance of the amplifier rises from Eq. (13.3), 
and, at a given differential voltage, there is more current. 
Thus, the data 
of Figure 13.5a show that the current rises as the common-mode voltage is 
decreased from 5V to 1V. Note also that the transconductance changes are 
greatest near 1V where the depletion region is thin, and are least near 5V, 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
279 
V w 
V S 
V G 
V D 
(a) 
v w 
v S 
VG 
v D 
n- 
nwell 
p- 
substrate 
(b) 
(c) 
VBEZ>-- 
vsO 
V G 
J 
! 
1 
OVD 
0 Vw 
v+ 
~ Vo s 
Iout 
__>_~ 
Figure 13.6 
The effect of the parasitic bipolar transistor. (a) A vertical crosssection of a 
well transistor in an n-well process. (b) The equivalent circuit of this well transistor contains 
two parasitic bipolar transistors. (c) The bipolar transistor at the source is responsible for 
shunting the amplifier's current to ground at low common-mode voltages, while the bipolar 
transistor at the drain plays a negligible part. 
where the depletion region is thick. The n changes are described in quantitative 
detail in Section A.1. 

280 
NEUROMORPHIC SYSTEMS ENGINEERING 
13.3.2 Bias-Current Characteristics 
Figure 13.7a shows the bias-current characteristics of our amplifier as a func- 
tion of the bias voltage VB. The amplifier is capable of operating with little 
loss in common-mode range from bias currents of pA to #A. Figure 13.7b illus- 
trates that the linear range begins to increase at above-threshold current levels, 
because of an increase in the I/gm ratio of a transistor at these levels. That is 
to say, the characteristic scaling voltage in Eq. (13.13) begins to increase above 
2kT/q. 
Note that the changes in ~ observed with bias current and common-mode 
voltage are not unique to our amplifier: They occur in every subthreshold 
MOS transistor. By measuring the slope of the I-V curve in a simple nFET 
differential pair, we obtained changes in ~ with common-mode voltage and 
with bias current. 
Figure 13.8 shows these data. 
Note that ~ changes are 
most abrupt at low common-mode voltages--that is to say, at low gate-to-bulk 
voltages; ~ also decreases with increasing bias current. 
13.3.3 Gain Characteristics 
The voltage gain of the amplifier is determined by the ratio of its transconduc- 
tance gm to its output conductance go. When w = 0, the transconductance 
is Is/VL. The output conductance is Is/2Vo, where V0 is the effective Early 
voltage of the output M transistors in Figure 13.1. The effective Early volt- 
age of the output M transistors is the parallel combination V~V~/(V~ + V~), 
where V0 ~ and V0 ~ are the Early voltages of the n and p output transistors, 
respectively. When w = 2, g,~ is given by IS/(3/2VL), since the linear range 
increases by a factor of 3/2. The value of go is given by Is/3Vo, as the out- 
put current in each arm falls from Is/2 to Is/3. Thus, effectively, the w = 2 
case corresponds to the w = 0 case, with Is being replaced by 2IB/3. So the 
gain is unchanged, because g,~ and go change proportionately. Independent of 
whether w = 0 or w = 2, the gain is 2Vo/VL. Figure 13.9a shows that the con- 
clusions of the previous paragraph are borne out by experimental data. As the 
common-mode voltage is lowered, the gain increases because of the increasing 
transconductance, or decreasing VL. The w = 0 and w = 2 amplifiers have al- 
most identical gain. Figure 13.9b illustrates that, as a function of bias current, 
the gain initially rises, because the Early voltage increases with current. As the 
bias current starts to go above threshold, however, VL drops faster than V0 in- 
creases, and the gain starts to fall. From Figure 13.7b and Figure 13.9b, we see 
that the location of the gain peak is at a bias current (10 nA) where the linear 
range starts to change significantly, as we would expect. Figure 13.9b shows, 
however, that the w = 0 and w = 2 cases do not have identical gains at all bias 
currents, although the gains are similar. We attribute these small differences 
to differences in g,~ and go at bias-current levels of IB/2 versus Is/3. 
For the amplifier for which data were taken, the output transistors had a 
channel length of 16#m in a 2#m n-well Orbit analog process. If higher gain is 
desired, these channel lengths should be increased, or the positive and negative 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
281 
10 +6 
10 -7 
10 .8 
Â£ 
~ 
10-9 
~9 
.~ 10 1Â° 
r~ 
10 -11 
i0-,~' 
0.6 
(a) 
~o 
o 
~ 
10 = 3,2 
x
~
,
.
 
K = 0"67 1019 A 
0:7 
0:8 
0:9 
~ 
t:l 
Bias Voltage (V) 
(VDD- Vs) 
1.2 
._ 
E 
3 
(b) 
~.5 
2 
X 
1.5 
I 
o 
0.5 
10 -12 
o---w= 0 
x---w= 1.86 
X 
X 
X 
X 
X 
X 
0 
0 
0 
0 
0 
0 
X 
X 
O 
O 
10L10 
' 
10. 8 
Bias Current (A) 
X 
O 
O 
O 
1 -6 
Figure 13.7 Linear Range vs. Bias Current. (a) The bias current IB as a function of the 
bias voltage VB. (b) The linear range of the amplifier VL as a function of the bias current 
IB. 
output currents can be cascoded via pFET and nFET transistors respectively. 
13.3.4 
Offset Characteristics 
A current offset of ik can be compensated for by a voltage offset of vz = ik/g,~, 
where g,~ is the transconductance of the iK -- VL relation (iK and VL are 
arbitrary variables). 
Therefore, if the fractional current offset caused by a 

282 
NEUROMORPHIC SYSTEMS ENGINEERING 
Common-Mode 
VOltage 
(Y) 
0.80 - 
0.78 - 
0.76 -- 
0.74 -- 
0.72 ~ 
0.70 
0.68 
0.66 
2.2 
? 
~/ 
10 7 
,,.0 
0 (9~" 
IO s 
~ 
~ 
Figure 13.8 
The changes in n in a simple nFET differential pair. The changes in n 
are most abrupt at low common-mode voltages, i.e., at low gate-to-bulk voltages; n also 
decreases with increasing bias current. 
threshold or geometry mismatch is ik/IK = ~, then the voltage offset vt is 
ik/g,~ = (IK/g,~)(ik/IK) = (IK/g,~)~. 
The IK/gm ratio is thus the scale 
factor that converts percentage mismatches to voltage offsets. A transconduc- 
tance amplifier has an IB/gm ratio of of 2UT/g. Thus, the same percentage 
mismatch results in a larger voltage offset for an amplifier with a lower g. This 
general and well-known result causes the lowering of transconductance to be 
assosciated with an increase in voltage offset. However, we show next that, 
although this increase in voltage offset is fundamental and unavoidable, certain 
transistor mismatches matter more than do others. By designing the amplifier 
carefully, such that more area is expended where mismatches are more crucial, 
we can minimize the offset. In our amplifier, mismatches may arise in the two 
nFET GM M mirrors, in the pFET M mirror, or between the W, S, and GM 
transistors in the two arms. In a first-order analysis, the B transistors do not 
affect the voltage offset, except that g is replaced by (2/3)g. We shall therefore 
start our analysis with the case of w = 0, and then extend our analysis to the 
w = 2 case. Suppose the net mismatch due to all mirror mismatches is such 
~M ) 
that i+/i_ = ~-~(1 Ã· ~)/L~(1 - ~- 
~ 1 Ã· ~M. The mismatch in currents 
may be referred back to the input as a voltage offset of Ã·VM/2 on one input, 
and a voltage offset of --VM/2 on the other input: 
2 
- 
\ 2U:~ / 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
283 
220 
200 
180 
-~ 160 
E 
< 
~8 140 
.-~ 
~ 
120 
~.) 
100 
80 
140 
120 
$ 
E 
< 100 
._ ~g 
~.~ 
c~ 
80 
60 1040 
o---w= 0 
x---w~2 
IB=35X 10-9 A 
Common Mode Voltage (V) 
(a) 
5 
o---W = 0; 
x---w~ 2; 
x 
(b) 
VcM = 3.0 V 
o 
x 
x 
o 
x 
x 
o 
X o 
X o 
X o 
o 
10 .9 
10-8 
10 .7 
10-S 
Bias Current (A) 
Figure 
13.9 
Gain Characteristics. 
(a) The gain of our amplifier as a function of the 
common-mode voltage. At low common-mode voltages, the gain increases because of the 
rising transconductance of the amplifier. (b) The gain of our amplifier as a function of the 
bias current. Initially, the gain rises because of the increasing Early Voltage of the amplifier, 
and then falls because of the increasing linear range. A more detailed discussion may be 
found in Section 13.3.3. 
U~r 
~VM 
=(~M--. 
g 
Since ~ 
is the IK/gm ratio of the W transistors, a mismatch of 6w between 
those transistors introduces a voltage offset of ~v_-~6w on the well inputs. Be- 
cause UT is the IK/g,~ ratio of the S transistors, a mismatch of 5s between 
t~n 

284 
NEUROMORPHIC SYSTEMS ENGINEERING 
those transistors results in a voltage offset of ~Ss on their gates. This volt- 
age offset is fed back to the sources of the W transistors and is amplified by 
1/(1 - ~) when referred back to the those transistors' well-inputs. Similarly, a 
mismatch of 5G between the G transistors causes a voltage offset of v~ on the 
/~n 
gates of the GM transistors that is amplified by ~/(1 - ~) when referred back 
to the well inputs. The total offset, Vof, is just the sum of the voltage offsets 
introduced by all the mismatches, and is given by 
) 
1 
5w + -- 
5s 
+ 
Vof 
= 
UT 
5M + 1-- ~ 
~p l 
) 
1 --Â£â¢a 
â¢ 
If we use the expression from Eq. (13.3) to substitute for 9, then 
(13.15) 
- 
- -  
-- 
+ 
~ 
+ ~w 
+ 
VÂ°f 
1 - ~ 
~ 
" 
We notice that the greatest contributor to the offset is the mirror mismatch 
5M. Thus, it is important that all mirror transistors in the amplifier be big. 
The matching of the S transistors is more important than is the matching 
of the W transistors since 1/~p > 1. The matching of the GM transistors is 
more important than is that of the W transistors if ~/~n > 1, which is usually 
the case; for ~/~ ratios exceeding 1/~p, it is also more important than the 
matching of the S transistors. The amplifier is thus laid out with big pFET M 
tranistors, moderate GM transistors, moderate nFET M transistors (to match 
the GM transistors), moderate S transistors and small W transistors. 
It is 
interesting that the transistors that matter the least for matching are the input 
transistors. 
If the B transistors are present, a similar analysis shows that the matching 
of the M transistors becomes even more important, because the g in Eq. (13.15) 
is replaced by (2/3)g. In Eq. (13.16), we then have 5M ~ (3/2)5M. 
The sizes of the transistors that we used for the amplifier were 12/12 for the 
W transistors, 12/12 for the bias transistor, 29/12 for the S transistors, 14/16 
for the GM and nFET M transistors, and 26/16 for the pFET M transistors. 
The total dimensions of the amplifier were 85~m x 190~m in a 2~m process. 
Our random offset is about 5 to 10 inV. 
Due to Early voltage effects, the c~caded gains of the GM-M mirror and the 
pFET M mirror in the positive-output-current path, exceed those of the GM- 
M mirror in the negative-output-current path. Hence, with Vos = VDD, the 
amplifier typically has a systematic positive voltage offset that is on the order 
of 10 to 20 inV. The voltage Vos is operated approximately 0.5 mV below VDD, 
to cancel this offset. We have also built automatic offset-adaptation circuits 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
1.5 
285 
..~ 
1 
> 
v 
~ 
0.5 
0 > 
~ 
0 
o 
---- -0.5: 
0 
~. 
VcM = 1.5 V, 3.0 V ,
,
~
,
,
,
~
 
V B = 4.0 V 
. i
~
~
 
~ 
" 
Vc,= 
].s~,,. 
v 
(a) 
~ 
0.05 
Offset Control Voltage (V) 
(VDD - V OS) 
1.5 
..-.. 
1 
> 
v 
--~ 0.5 
0 > 
o 
0 = -0.5 
2 
VDc = 3.0 V 
V B = 4.20 V (top curve) 
~ 
V B = 4.00 V (middle curve) 
J J J  
~ 
, 
(b) 
(~ 
0.05 
Offset Control Voltage (V) 
(VDD -v OS) 
Figure 13.10 
OfFset Characteristics. (a) The offset characteristics of the amplifier at 
two different common-mode voltages. (b) The same characteristics at three different bias 
currents. More details may be found in Section 13.3.4. 
to control the offset of our amplifiers; we shall not elaborate on those schemes 
here. 
The data of Figure 13.10a show the offset voltage of a follower built with this 
amplifier as a function of the Vos voltage. The two current curves correspond 
to two different common-mode voltages. The slope of the input-output relation 

286 
NEUROMORPHIC SYSTEMS ENGINEERING 
is the ratio of the source transconductance at the Vos-input 9s to the transcon- 
ductance of the overall amplifier g. At high common-mode voltages, the offset 
is more sensitive to the Vos voltage, because g decreases with common-mode 
voltage. Figure 13.10b shows that the offset voltage is less sensitive to the Vos 
voltage at high bias-current levels, because gs decreases faster than does g with 
bias current. 
13.4 
FOLLOWER-INTEGRATOR CHARACTERISTICS 
One of the most common uses of transconductance amplifiers is in building fil- 
ters. Thus, it is important to study the properties of a follower-integrator. The 
follower-integrator is the simplest filter that can be built out of a transconduc- 
tance amplifier; it is a first-order, lowpass filter. Figure 13.11a shows the basic 
configuration. The voltage Vr is identical to VB in Figure 13.1, and, with the 
capacitance C determines the corner frequency (CF) of the filter. Figure 13.11b 
shows the de characteristics of such a filter built with our amplifier. Except 
for very low input voltages, where the parasitic bipolar transistors shunt the 
amplifier's current to ground, the output is a faithful replica of the input. Fig- 
ure 13.12 reveals the effects of the parasitic bipolar transistors in detail. At 
input voltages that are within the normal range of operation of the amplifer 
(1V to 5V), such as those shown in the top portion of the figure, the output 
of the follower is a simple lowpass-filtered version of the input. In the bottom 
portion of the figure, we see that, as long as the input voltage is below about 
0.7 V, the amplifier's bias current is shunted to ground, and the output voltage 
barely changes. The constancy of the output voltage occurs because there is no 
current at the output of the amplifier to charge or discharge the capacitor such 
that it barely changes. When the input voltage is outside this range, the output 
voltage tries to follow the input voltage by normal follower action. We show 
these data to illustrate the consistency of our earlier results with the behavior 
of the follower-integrator; we do not recommend operation of the filter in this 
regime. We shall now describe more useful linear and nonlinear characteristics 
of the follower integrator in detail. 
13.4.1 
Linear and Nonlinear Characteristics 
Figure 13.13a and Figure 13.13b show data for the gain and phase character- 
istics of a follower integrator. The phase curve is much more sensitive to the 
presence of parasitics than is the gain curve, which remains ideal for a much 
wider range of frequencies. The higher sensitivity of the phase curve to para- 
sitics is reasonable, because the effect of a parasitic pole or zero on the gain is 
appreciable only at or beyond the pole or zero frequency location. The effect 
of the same pole or zero on the phase is significant a full decade before the 
frequency location of the pole or zero. We have studied the origin of the para- 
sitic capacitances in detail, but we shall not delve into this subject here. The 
parasitics are caused by the large sizes of transistors that we use to reduce our 
1If noise and offset; the dominant parasitics are the well-to-drain capacitance 

A LOW-POWER 
WIDE-LINEAR-RANGE 
TRANSCONDUCTANCE 
AMPLIFIER 
287 
In 
> 
Out 
> [--> 
C 
(a) 
5 
4.5 
Offset = 1 5 mV 
. ,..~~, 
4 
Slope = 0.981 
.~,~x,--. 
~.>2~ a.53 ~ 
~ 2.5 
2 
~ 1.5 
1
:
~
 
(b) 
0.5 
O~ 
I 
2 
3 
4 
DC Input Voltage (V) 
Figure 13.11 Follower DC Characteristics. (a) The basic circuit for a follower-integrator. 
(b) The output is a faithful replica of the input except at very low input voltages. 
of the W transistor, which causes a right-half-plane zero, and the gate-to-bulk 
capacitance of the GM transistors, which causes a left-half-plane pole. Rather 
than make detailed models of the parasitics, we model their effect by simply 
having a different corner frequency (CF) for the gain and phase curves. As 
Figure 13.13a and Figure 13.13b show, the phase CF is slightly lower than the 
gain CF, because of the excess phase due to the parasitics. 
It is possible to reduce the influence of parasitics in our amplifier by having 
the nFET GM-M mirrors be attenuating. Then, the differential-arm para- 
sitics are at higher frequencies, compared with the CF of the filter. Another 
alternative is to use a larger output capacitor. However, for cochlear-filtering 

288 
NEUROMORPHIC SYSTEMS ENGINEERING 
o 
> 
o 
3.5 
2,5 
2 
1.51 ~ 
0.5 
04 
~ 
~ 
~ 
~ 
5 
Time (Secs) 
x 10 .4 
Figure 13.12 
Bipolar Effects in the Follower-Integrator. The top portion of the figure 
shows normal follower-integrator operation, i.e., the filtering of a sinewave input. The output 
waveform is attenuated and phase-shifted with respect to the input waveform. The bottom 
portion of the figure shows operation of the filter at low input voltages. An explanation for 
the strange output curve is given in Section 13.4, and is due to the effects of the parasitic 
bipolar transistors in our amplifier. 
applications, for which this amplifier is mainly intended, there is a steep ampli- 
tude rolloff beyond the CF of the lowpass filter, and fine phase effects are not 
important at these frequencies. In addition, the second-order filter, of which 
this filter is a part, is only an approximation to the more complicated filtering 
that occurs in a real cochlea. Thus, we have not expended energy optimizing 
the parasitics to get an ideal first-order filter. 
Figure 13.13c shows the first, second, and third harmonic rms output am- 
plitudes when the input rms amplitude is 1V (peak-to-peak of 2.8V). The data 
were collected with an EGG3502 lockin amplifier. Because of the wide linear 
range of our amplifiers, there is little distortion even at these large amplitudes. 
At frequencies that are within a decade of the CF, the second harmonic is less 
than 4 x 10 .2 the amplitude of the first harmonic. The third-harmonic dis- 
tortion is negligible at these frequencies. The total harmonic distortion is less 
than 4%. 
Figure 13.14 plots the relative amplitude distortion--that is to say, the ratio 
of the magnitude of the second and third harmonic amplitude to that of the 
first. Note that the X-axis is plotted in multiples of the CF, and that we 
are uninterested in effects more than a decade beyond the 1 point. The chief 
features of interest are that the addition of the bump transistors reduces third- 
harmonic distortion as we expect from Eq. (13.10) and Eq. (13.11). However, 
it also increases the second-harmonic distortion at frequencies just below the 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
289 
CF, for reasons unknown to us. Usually, second-harmonic distortion occurs 
along with dc shifts. Figure 13.15 shows these shifts for an input signal with an 
rms input amplitude of 1V. These shifts exert only a mild influence in cochlear 
designs, because input-signal amplitudes far beyond the CF are small due to 
successive filtering from prior stages. The dc shifts are not significant for small 
inputs. The dc shifts are observed in OTA-C follower-integrators as well; they 
are typically positive for amplifiers with nFET differential pairs and negative 
for amplifiers with pFET differential pairs. 
Figure 13.16a shows the shifts in gain and phase CF for the w = 0 and w = 2 
cases as a function of the input rms amplitude. Note that the gain and phase 
CF shifts are similar. The addition of the B transistors decreases the amount 
of CF shift due to the linearization of the tanh. Figure 13.16b shows the shifts 
in CF as a function of the input dc voltage to the follower-integrator. As we 
expect from prior data on the amplifier (Figure 13.5a), the CF shifts track the 
shifts in transconductance with dc input voltage. The tracking is seen for gain 
and phase CF curves, and for the w = 0 and w = 2 cases. 
13.5 
NOISE AND DYNAMIC RANGE 
The largest signal that a filter can process without excessive distortion is de- 
termined by the linear range of that filter. The smallest signal that a filter 
can process is one whose rms amplitude is just above the filter's input-referred 
noise. The dynamic range of a filter is given by the ratio of the power of the 
largest signal to the power of the smallest signal. We have already discussed 
linear range and distortion in the follower-integrator. Now, we focus on the 
other half of the story: noise in the follower-integrator. After we compute 
and measure the noise, we can tell whether there has been an improvement in 
the dynamic range of the follower-integrator. Since there are no free lunches 
in nature, we also want to discover what price we have paid in our design, in 
terms of the increase in power and area. We shall discuss noise and dynamic 
range in our design in Sections 13.5.1-13.5.3. 
We shall discuss the dynamic 
range of capacitive-divider schemes in Section 13.5.4, since those schemes bear 
similarity to our technique of using the well as an input. 
13.5.1 
Theoretical Computations of Noise in the Amplifier and 
Follower-Integrator 
We now compute the noise in a follower-integrator built with our amplifier. 
We tie the v+ and v_ to a. common constant voltage source, and the output to 
another constant voltage source. Then, we replace each of the 13 transistors in 
Figure 13.1 with its small-signal equivalent circuit. We introduce a noise current 
source between each transistor's drain and source terminals in the small-signal 
equivalent circuit. For each noise source, we compute the ac transfer function 
between its current and the differential output current. 
We sum the noise 
contributions from each of the 13 sources incoherently; that is to say, the square 
of the net output current noise is the sum of the squares of the current noise 

290 
NEUROMORPHIC SYSTEMS ENGINEERING 
~ 
10 .2 
> 
. 
~ 
103 
.- 
~- 
E 
< 
~ 
10.4 
~czczzec:~ InputRms=4.84mV 
Magnitude 
-
~
 
Corner Frequency = 1 1 4 Hz 
~ Oo~-o.o.e~ 
(a) 
~ 10i~0~ 
r~ 
102 
103 
104 
Frequency (Hz) 
O. ~e-~.m~^._. 
â¢ â¢ â¢ â¢, 
........ 
, 
........ 
: 
~,~ -I00 
Â°Ooooo 
~ 
Phase 
Â°OOoo Â° 
~ 
Corner Frequency = 95.3 Hz 
~ -200 
Â°Ooo 
~_ 
Oo 
~ 
(b) 
Oo 
0"300101 
....... 
i~ 2 Fre~que~cy(~zi 'i~ 3 
....... 
iO 4 
I0Â°~.O 
...... 
, 
........ 
, 
........ 
~ ~u~u ~000000000 
0000 
Input Rms Amplitude = 
o o 
IV 
o o 
10-1 
o o 
~ 
O0 0 
XxXXXXXXXx 
00 
~ 
xX 
X x 
O00 
X x 
~.10. 2 X 
000000000000 
X x 
<~ 
â¢ 
â¢ 
~:~ ~ : ~  
Xxx 
~ 
~ 
)~ Xxxx 
~ 
~ 
~ 
N( ~ 
xxXXXXXxxxx 
~)~ 
X X 
~ 
~ 
~ 
0 
~ 
XxX 
Xx 
~ 
Xx 
~10 -3 
~ 
~)~ )~)~ ~)~)~ 
~ 
o -- Fundamental 
~ 
~ 
~ 
x -- Second harmonic 
~ 
â¢ -- Third harmonic 
(c) 
10 
4 
~ 
, 
, 
, 
, 
, ~ 1  
.
.
.
.
.
.
.
.
 
~ 
, 
, 
.
.
.
.
 
,, 
1 0 ~ 
10 ~ Frequency (~z) 10 a 
10 4 
Figure 13.13 
Follower-lntegrator Frequency Response at a dc-input voltage of 3V. (a) 
and (b) show the gain and phase characteristics of the follower-integrator along with fits to 
lowpass filter transfer functions. The phase characteristics are more sensitive to parasitics 
and we model their effect by fitting the phase curves to a different corner frequency than 
the gain curves. (c) The first, second, and third harmonic rms output amplitudes at an 
input rms amplitude of lV (2.8 V peak-to-peak). Because of the wide linear range of the 
amplifier, the total harmonic distortion is low even at this amplitude. 
from each source. The input-referred voltage noise per unit bandwidth is the 
output current noise per unit bandwidth divided by the transconductance of 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
291 
o 
o 
,_ 
E 
< 
o > 
,_ 
10 Â° 
10 -1 
1 0i20_ 1 
Second Harmonic Distortion 
x---w~2 
(a) 
0 --- W = 0 
X~<O xo 
o 
xo 
X 
o 
X o 
xo 
o 
Xo 
X 
X X 
Xo 
xoXoX X X X X 
o 
>$) 
xo 
o 
xo 
o 
o 
o 
o 
x Â° 
o 
X 
10 0 
101 
10 2 
10 3 
0 4 
Multiple of Corner Frequency 
o 
o 
E 
< 
> 
._ 
10 0 
10 -~ 
1 0 -2 
Third Harmonic Distortion 
x---w~2 
o---w 
= o 
(b) 
oOO 
~o 
o 
o 
xX X 
o Ã 
xO Xo 
o 
N 
~ 
Ã 
Ã 
o Ã 
Ã 
xoX~ 
Xo 
X 
10-3 
-110 
. O ........................................ 
10 o 
101 
10 2 
10 3 
10 4 
Multiple of Corner Frequency 
Figure 13.14 
Follower-Integrator Distortion Characteristics. 
(a) The ratio of magnitude 
of the second harmonic to that of the first harmonic vs. normalized frequency, i.e., frequency 
normalized by the CF. (b) The same as (a) except that the third harmonic is plotted instead. 
Note that the addition of the B transistors decreases third harmonic distortion but increases 
second harmonic distortion. 
the amplifier. The total voltage noise is the voltage noise per unit bandwidth 
integrated over the bandwidth of the follower integrator. The bandwidth of the 
follower-integrator is determined by the transconductance of the amplifier and 
the capacitance. If the amplifier is used in a system, where the bandwidth is 

292 
NEUROMORPHIC SYSTEMS ENGINEERING 
-0.5 
(J 
-1.5 
â¢ --;---~,,.58x~ ~ ;~ ............. 
X X Oo Â° 
X 
x 
o 
o 
x 
o---w= 
0 
x--w~2 
X o 
X 
o 
X 
~xxA~o~o~x 
~0" 
' 
..... 
i; Â° 
...... 
i;' 
' ..... 
i; 2 
...... 
4; 3 
...... 
40' 
Multiple of Corner Frequency 
Figure 13.15 
DC Shifts in the Follower-Integrator. The shifts are shown at an input 
rms amplitude of 1V. These shifts exert only a mild influence in cochlear designsâ¢ See 
Section 13.4.1 for details. 
determined by some other part of the system, then this bandwidth determines 
the interval of integration. The parasitic capacitances in the amplifier set an 
upper bound on the maximum possible bandwidth. 
Although there are 13 transistors in Figure 13.1, we do not get 13 transistors' 
worth of current noise at the output. Our calculations will show that we get 
about 5.3 transistors' worth of current noise. This figure is only slightly higher 
than the 4 transistors' worth of noise obtained from a 5-transistor ordinary 
transconductance amplifier (OTA). The reduction in noise occurs for three 
reasons. First, for each noise source, there is a total or partial cancellation of 
its noise current at the output, due to opposing contributions from the two 
circuit arms. 
As an example, the noise current from the bias transistor in 
Figure 13.1 (with gate voltage VB) makes no contribution to the output noise 
current, because it branches into equal portions in the two differential arms, 
which cancel each other at the output. 
Similarly, other noise sources, such 
as those from the B transistors, contribute no noise. The sources from the 
GM transistors have a partial noise cancellation. Second, some of the noise 
current from each source is prevented from contributing to the output by local 
shunting circuit impedances. As an example, the noise currents ti'om the W 
transistors contribute only 0.16 transistors' worth of current noise to the output 
because most of the noise current gets shunted by the W transistors themselves. 
Third, when we compute the incoherent noise average across many sources, a 
given source's contribution to the net noise is proportional to the square of its 
fractional current gain; that is to say, to the square of the output noise current 
divided by source current. Therefore, weak contributions are weakened further. 
If we define C~B, CtS, ~W, C~, and aM to be the current gain between the 
output current of the amplifier and the input drain-to-source noise current of 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
293 
120 
~" 100 
>~ 
8 
=~ 
g 
80 
~- 
~ 
60 
(a) 
Gain 
o 
0 
0 
0 
Phase 
o 
o 
o 
o 
o 
o---w= 0 
x---w~ 2 
Gain 
x 
x 
x 
x 
x 
x 
Phase 
x 
x 
x 
x 
x 
x 
4?0-3 
....... 
. . . . . .  
i'6' 
Input Rms Amplitude (V) 
(b) 
180 I 
160 ~'o 
- 
~,~N 140 
Â° Â° 
~' 120 ~o 
Â°Â°o Â° 
O---w= 0 
~ 
J Â°o 
Â°Â° o 
x---w~2 
o 
~ 100 
XxÂ°o Â° 
o 
~X 
X"Â°Â°Â°~ 
E 
~ 
'~xx x 
o o 
o o o 
~ 
80I/ Xxx 
X X X 
XX 
o X 
6o~ 
ÃÃÃÃÃÃ Ã Ã ÃÃÃÃÃ 
Ã 
40.~ 
1:5 
~ 
2:5 
~ 
/ 
Input DC Voltage (V) 
o 
o 
o 
o 
o 
o 
x 
x 
x 
x 
10 o 
o 
o 
o 
o 
x 
~ 
o 
0 
x 
x 
~ 
X 
X 
X 
X 
3:5 
~ 
4.5 
Figure 13.16 
Corner-Frequency Shifts in the Follower-lntegrator. (a) CF-shifts vs. input 
rms amplitude and (b) CF-shifts vs. input dc voltage. The lower curve is the phase CF for 
the w ---- 0 and the w = 2 cases. 
a B,S,W,G, or M transistor, respectively, we can show that 
O~B 
---- O, 
~;n 
O~ S 
~ 
OZ G 
-~ 
I';n -F t~n t~p -F I':,K,p ' 
I~n l~ p 
t~ n ~- I'~nl'; p -~ t~t~p ' 
I~ n ~ 
I~n l~ p 
t~ n ~- t'~nl';, p ~- N,l'~p ' 

294 
NEUROMORPHIC SYSTEMS ENGINEERING 
Since each transistor has the same dc current flowing through it (i.e. IB/2 in 
the w = 0 case, or IB/3 in the w = 2 case), the magnitude of the noise current 
source across each transistor is the same. Thus, the output-current noise of the 
amplifier is the noise current due to one transistor times an effective number 
of transistorsN, where, N is given by 
.N = 2as 2 4- 2ctw 2 -k 2aa 2 + 4O~M 2. 
For our amplifier with ~ ~ 0.85, ~ ~ 0.7, and ~p ~ 0.75, the numbers work 
out such that N = 5.3. The dominant contribution to N comes from the four 
M transistors which contribute a whole transistor each. The two G transistors 
contribute 0.865 transistors. 
The two S transistors contribute 0.28 transis- 
tors. The two W transistors contribute 0.16 transistors. The B transistors and 
the bias transistor contribute no noise. The most noise-free linearization tech- 
niques, in decreasing order of merit, are bump linearization, the use of the well 
as an input, source degeneration, and gate degeneration. Bump linearization 
is the only technique of the four that adds no noise whatsoever. Note that, 
depending on the circuit configuration, the relative noise efficiencies of the use 
of the well as an input, source degeneration, and gate degeneration mayvary. 
For example, in a well-input amplifier with source degeneration but no gate 
degeneration, aw = as = ~/(~ + ~,). In the latter case, the use of the well 
as an input and gate degeneration each contribute 0.41 transistors' worth of 
noise. 
The magnitudes of the individual noise current sources depend on the dc 
current flowing through the transistor, and are well described by a white-noise 
term for low subthreshold currents [23]. At high subthreshold currents, there 
is also a 1/f-noise germ. Our experimental data in Section 13.5.3 reveal the 
amount of the 1If contribution; we shall model this term empirically, because 
no satisfactory theory for 1If noise currently exists. 
In the first paragraph of this section, we explained the procedure for calcu- 
lating the noise. We shall now perform the calculations. As usual, we begin by 
analyzing the c~e for w = 0 and then extend our analysis to the w = 2 case. 
The output-current noise of the amplifier ~ is given by 
ig = 
x 
} + 
(la. 
z) 
l 
where the first and second terms in the integral correspond to white noise and 
1If noise, respectively; I~ is the bias current, and K is the 1/f noise coe~cient. 
We also assume that there is low-frequency adaptation in the amplifier, so that 
frequencies below f~ are not passed through. This assumption is necessary if we 
are to prevent the 1If noise from growing without bound at low frequencies. 
In our amplifier, we have an offset-adaptation circuit that keeps f~ around 1 
Hz. Also, typically the K for pFETs is smaller than is that for nFETs, and 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
295 
scales inversely with the channel area of the transistor. However, we assume a 
transistor-invariant K, for simplicity. 
The corner frequency of the follower integrator fc is 
IB 
fc- 2~rCVL' 
(13.18) 
-
-
 
2 is 
so from Eq. (13.17), the input-referred voltage noise v~ 
( 
/ 
--v~ = 
~ 
N qI~ + ~ ] 
1 
~ 
~ 
(~B/v~l ~ 
~ + (~/~ 
( NqV~) ~f~ + NKV~ in (1+ 
~ Iz ] ~ 
~ 
(f~/ft)2/~ 
_ 
NqV~ NKVc~ in 
1 + 
4~ + ~ 
2uf~Vi 
(13.19) 
In evaluating Eq. (13.19), we computed two integrals: 
fo ~ 
dx 
_ 
~ 
l+x 
2 
2' 
~ 
dx 
_ 
lln(1 + 1 ). 
, ~(1+ ~) 
~ 
~ 
Note that the 1If noise rises with bias current because of the increasing band- 
width. The magnitude of the 1If noise depends on the bandwidth; that is 
to say, it depends on IB/CVL. The white noise is current invariant, because 
the noise per unit bandwidth and the bandwidth scale as 1lIB and IB, re- 
spectively. The white noise increases linearly with VL, whereas the 1If noise 
increases quadratically with VL. 
13.5.2 Theoretical Computations of Dynamic Range 
The dynamic range DR is defined to be the ratio of rms input power in a 
signal with amplitude VL to the input-referred noise power; this definition is 
implicitly based on our willingness to accept the distortion present at this 
amplitude, which from Figure 13.13 is seen to be reasonable. If we want to be 
more conservative with respect to distortion, then we simple scale VL by the 
necessary fractional amount. So, we have from Eq. (13.19) that 
DR 
= 
V~/2_ 
2 
V i 
v~/2 
NqVL 
~ 
ln (1+ ~ ~ 
~ ~ 
~c + 
~ 
k 
k ~cv~ ] J 

as~ou p~Jzajaz-~ndu~, lUa!~uop! oa~q Â£aII: 1 l~II~ ~OqS U~a ~ 
Uaq~ 'qlpi~puuq 
om~$ aq~ aA~q ao~aN~ul-za~oIIO ~ E = m ~ pu~ 0 = ~ ~ lI "~A(U/~) ~ ~A 
~a~u~ zuouiI puu q~pl~puuq aq~ zo~ "~/~I ~ g/al 'sao~sisu~z~ aq~ q~nozq~ 
s~uo~n3 aq~ ~o d 'di~vilmls spaooo~d ~U puv a~a ~oj slsdi~u~ aq~ '8 = m JI 
_
_
 
â¢ s~aa~ a~a~ I q1~a 
saoasssu~al ash asnm o~ 'aas~o pu~ os~ou//I aanpaa o& :o~u~a a~m~udp ~aaxa 
qa~ pol~Dsoss~ asoa ~aa~ u~ osi~ s~ aaaq& "sopn1~idmv o~av I pu~ aoo~ astou 
aaq~q ~ oa~q soaiosmaqa l~qa sandu~ oa pal~ns oq o~ polvas uaoq oa~q SlOA0 I 
OS~OU pug IVU$~S aqa avql Â£IOaOm s! uo~a~na~s ~ qans u~ Â£a~aeou~i jo aSmu~ape 
oq& 'qap~pu~q avqa Xluo uo spuodop qap~pueq UOAI$ V aoao OS~OU ivaoa oqa 
'o~ "au~aVaU~ auoaana S~ pu~ ~A XIUO uo spuadop q~p~pu~q l~un aod as~ou 
oqa asn~aaq sosu~ aau~avau~ qap~pu~q aqZ "~u~aeau! qap~pu~q s~ aaq~a 
anq 'IU~a~AU~ auaaana aou st as~ou f/I asn~aoq ~A o~I sos~aaaut ostou oqa 'aaao 
-~oq 'anita IVU~$~aO Sa~ Oa paaoasaa uaaq S~q qlp~pu~q aqa ao~V "(gI'gI) "ha 
oa ~u~paoaae 'papuadxo oq oa svq 7 A o1 I~UO~aaodoad aa~od uoqa 'poaaosaad 
oq oa s~ qap~pueq JI "VA/I o~I SOleaS qap~pu~q aqa puv '~A a~I soI~as 
qap~pu~q a~un aod as~ou oR& "a6uw a}mvu~p aq~ laaffv 1ou saop a6uva avau}l 
ap}~ 'sa,vu}~op as}ou f /I ~ 'snqI "~A Â°~I sas~aaam ao~od I~U~S mnm~m 
oq~ "~A Â°~I sos~aaaut aa~od os~ou-//I oqa avqa (6I'~I) 'ba moaj oos a~ 
â¢ aomod u~ os~aaaut 
u~ jo oatad oqa ae poua~o uoaq s~q oSu~a a~mvuJp ~aaxo oqÂ£ Xlaa~aaodsoa '~I 
pu~ aI/I o~I oI~aS qap~mpuvq oql pu~ qap~mpu~q a~un aad os~ou oqa osn~aoq 
sosue oaue~avau~ ~uaaana aqÂ£ 'au~aeau~ auoaana st ostou I~maoqa asn~aoq anita 
om~s oql av summoa os~ou aqÂ£ "7 A oa I~UO!aaodoad anitA ~ av sut~moa liDS 
os~ou oqa 'aoao~oq 'anita [~u~$~ao sa~ oa paaoasaa uooq svq qap!~puvq oqa aoa 
-Jg "(~I'~I) 'ha o~ Su!paoaae 'popuodxo oq oa ~q 7 A oa IVUO!aaodoad aa~od 
uoql 'paAaOsaad oq o1 S ! qlp!~pu~q JI '7A o~!I soIvaS os~ou II~a0AO 0qa a~qa 
qans 7A/I a~i I soi~as qapi~puuq aqa anq '~A o~II soI~aS qapI~puuq aIun aod 
ostou aq~ "~a}paad (ig'gi) "bg ~v 'aSuva ai~vu~p aao~ ~aild~ } aSuva avau}l 
ap}~ 'sa~vui~op asiou a~iy~ /} 'sng~ '~A a~I so~oaam aa~od I~U$~s mnm~x~m 
oq~ 
"7 A a~! I sas~oaam ao~od as!ou-oa~q~ oqa l~qa (6I'~I) "ha moaj oos o~ 
bN 
(IUgI) 
JADa = uO 
â¢ ia~ou s~ 
osaka a~mvudp aqa '(0g'gI) "ba moaj 'snq& "aold avou~i-a~ou!i ~ uo os~ou oa!q~ 
~q paleu!~op s! aoId oqa aopun eaa~ lou aqa '~auonbaaj snsaoa opna~idmv as!ou 
3o ~old $oi-$o I ~ uo aueaaodm~ oq oa savadd~ os!ou//I oqa oaoq~ 'sauoaana s~!q 
q$~q a~ uoaa "SlOAOI auoaana ploqsoaqaqns ~o I oqa a~ q$!q XIaA~a~IOa II~lS s~ os~ou 
aa~q~ asneaaq pu~ 'uo~a~ad~pe aas~o Xq 1no paaaaI~ aae ZHI ~OlO q soDuonboa J 
osnvaoq 'saoas~su~aa &add Ji~som 'oSa~ I ano u~ X jo onlva IlVmS aqa jo osnvaoq 
'maaa//I aqa uvqa aa~a~ I qanm Xii~a~dX 1 s~ mao~ as!ou-oa~q~ oqa '(0g'~I) "ba uI 
(() 
) 
VAOg 
@, 
(ov 
O 
= 
I 
Di'Â¢IHH~INIDN~ ~IAIHK~X$ DIHdHOBIOHflHN 
96g 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
297 
spectra. 
For the bandwidths for the w = 2 and w -- 0 cases to be equal, 
however, the bias current for the w = 2 case has to be increased by a factor 
of (3/2) over that for the w = 0 case. Theoretically, the dynamic range of the 
w = 2 case is (3/2) 2 times the dynamic range of the w = 0 case because of 
the increase in linear range. In practice, however, if the linear range is already 
large in the w = 0 case, the increased linearity in the w = 2 case does not 
reduce distortion further. 
The distortion is typically limited by some other 
mechanism. For example, in our amplifier the dominant distortion is due to ~ 
shifts caused by the nonlinear well-to-channel capacitance. Nevertheless, the 
increased linearity does reduce third-harmonic distortion and CF shifts with 
input amplitude, as shown in Figure 13.14b and Figure 13.16b. 
13.5.3 Experimental Noise Curves 
Figure 13.17a shows noise spectra at low bias-current levels for a follower- 
integrator with a capacitance C -- 1.02 pF. The data were taken with a 
HP3582A spectrum analyzer. 
From Eq. /13.17), we would expect white or 
thermal noise to be dominant at low bias currents. We observe from the figure 
that, even at the lowest frequencies, no 1If noise is visible. We were able to 
fit the data with lowpass-filter transfer functions as shown. Note that, for the 
low bias currents of Figure 13.17a, where 1/f noise is hard to discern, we did 
not use any 1/f terms in our fit. The terms in the integral of Eq. (13.19) 
predict that the noise spectra reveal relatively more 1/f noise at high bias cur- 
rent levels because the white-noise term decreases and the 1If term remains 
roughly constant. The data of Figure 13.17b illustrates that this prediction is 
approximately borne out. However, we were able to empirically fit the voltage 
2 
noise per unit bandwidth vif more accurately by a term of the form 
) 
vii = 
~ 
+A 
. 
(13.22) 
1 + (fife) ~ 
Figure 13.18a shows a typical fit to the data in more detail. From the first line of 
Eq. (13.19), we would expect K8 = V~K/4, n = 1, and A = VL~Nq/IB. Since 
Ks and n are empirical, they do not yield substantial theoretical information, 
although they are useful practically. In the following paragraph, we show how 
to extract the value of N of Eq. (13.21) from the value of A. 
From the value of f~ obtained from the fit to the data, from our knowledge 
of C, from measurements of Im and from Eq. (13.18) we obtain VL. Given 
VL and the fit parameter A, we obtain N, the effective number of transistors 
contributing shot noise. Figure 13.18b shows a plot of N versus the bias current 
I8. We observe that N is roughly 7.5 in subthreshold, and decreases as the bias 
current goes above threshold and space-charge smoothing sets in. The value of 
N in subthreshold is within a factor of 1.4 of our theoreticM prediction of 5.3. 
Above threshold, the space-charge smoothing, that is to say, the modulation of 
the mobile charge concentration by the mobile charges themselves, reduces the 
noise to a value below what we would expect from shot noise. 

298 
NEUROMORPHIC SYSTEMS ENGINEERING 
Figure 13.19 shows a plot of how the Ks and n of Eq. (13.22) vary with bias 
current Is. Since KB = KV~/4, part of the increase in Ks arises from the 
increase in VL and part of it arises from the increase in K. It is also interesting 
that, as the bias current increases, the 1If noise power systematically rises 
from about 0.67 to about 0.95. 
The noise measurements of Figure 13.17 through 13.19 were taken for an 
amplifier with w = 0. We also experimentally confirmed that the noise in 
a w = 2 amplifier was identical to that in a w = 0 amplifier of the same 
bandwidth. 
13.5.4 Capacitive-Divider Techniques 
Our use of the well as an input implicitly involves a capacitive-divider technique: 
The gate, surface potential, and well form three terminals of a capacitive di- 
vider. We chose the well as an input because coupling ratio of the well to the 
surface potential, 1 - ~, is smaller than the coupling ratio of the gate to the 
surface potential, ~. The advantage of this implicit capacitive-divider scheme 
is that the divider is inherently part of the transistor; so, we exploit a parasitic 
capacitance rather than avoiding one. Also, no additional floating-gate adapta- 
tion circuits or control voltages are needed. The disadvantage of the technique 
is that the capacitive-divider ratio is fixed by the physical parameters of the 
process, and is slightly nonlinear. If the divider ratio is not as small as desired, 
we must use other circuit techniques like source degeneration, gate degenera- 
tion or bump linearization to obtain wider linear range. Luckily, in our circuit, 
the additional transistors used to widen the linear range do not increase the 
noise greatly, but they do cost more area. It is logical to ask whether we can do 
better in area consumption with circuits that have explicit capacitive dividers. 
We shall discuss two simple schemes where capacitive dividers are explicitly 
used around OTA's. We assume that any floating-gate inputs of the amplifiers 
are held at a proper dc value by low-frequency adaptation circuits. We further 
assume that the adaptation circuits do not affect noise in the amplifier's tran- 
sistors or in the circuit. In practice, this assumption may not be true of certain 
adaptive schemes. 
Figure 13.20a shows a simple scheme. The voltage VT determines the bias 
current in ~he OTA. In practice, parasitic capacitances between ~he output 
and inpu~ ~erminals of the OTA can hurt ~he design significantly. If V~ is the 
linear range of the OTA, and ~ is the effective number of noise-contributing 
~ransistors in ~he OTA, ~hen it can be shown tha~ the dynamic range Da is 
2 (Co~ + c~+c2 ~ 
DR = 
(13.23) 
Nq 
The analysis leading to the previous equation is similar to that preceding 
Eq. (13.21). We assume that thermal noise dominates. ~om Eq. (13.23), we 
see that Co,t needs to be moderately large. If not, any improvement in dynamic 
range over that of an OTA-C follower-integrator arises only at the expense of 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
299 
10 .3 
,~- 10-4' 
I 1E 
>~ 10 s 
.8 
z 
N 10 -e 
o 
> 
~ 0 -~ 
0 
(a) 
18 = 50pA 
noise is almost exclusively 
~ 
thermal 
-1- 
~_105 
>~ 
.8 
o 
z 
& 
# 
~108 
8 
1Â°1oÂ° 
...... 
i~' 
' ..... 
i~ 2 
...... 
i~ 3 
...... 
i~' 
' 
..... 
io' 
Frequency (Hz) 
10 .4 
............................................ 
(b) 
~ 
18~ 6.5 nA 
"
'
~
~
 
= 24n~ 
~ 
00nA 
At high bias-current levels, 
progressively more 1/f noise 
is revealed 
1Â°~'oÂ° 
...... 
i;' 
' ..... 
i; 2 
...... 
i; 3 
...... 
;;' 
" 
..... 
lo s 
Frequency (Hz) 
Figure 13.17 
Noise Spectra at Various Current Levels. (a) At low bias currents the noise is 
almost solely white or thermal. The bold lines are Iowpass-filter fits to the data. (b) At high 
bias currents, there is relatively more 1/f noise. Nonetheless, the dominant contribution to 
the noise, which is the area under this curve in a linear-linear plot, remains thermal. The 
bold lines are fits to the sum of a 1/f term and a white-noise term. 
an extremely large value of C2. For example, if Co~t were 0, we would need 
C2 to be approximately 15 pF to get a dynamic range improvement of 15 over 
that of an OTA C follower-integrator with lpF. 

300 
NEUROMORPHIC SYSTEMS ENGINEERING 
10 .4 
~10-5 
>~ 
~o 
~ lO-S 
0 
I B = 12nA 
l/f noise 
l/f noise 
e noise 
~Â°(oÂ° 
...... 
iG' 
...... 
;G' 
...... 
i~ ~ ...... 
i~' 
...... 
io Â° 
Frequency(Hz) 
8 
o 
0 
Z 
o 
o 
o 
o 
o 
o 
o 
o 
o 
o 
o 
(b) 
( 
~Oo.,, ..... 
i.~,.,o ..... 
i.~. ~ ..... 
,~,~;., , 
~o ~ 
~o-o 
Bias Current (A) 
Figure 13.18 
Basic Noise Characteristics. (a) A typical noise spectrum. (b) The effective 
number of transistors contributing shot noise in our circuit as a function of the bias current. 
As the bias current goes above threshold, the ef[ective number of transistors contributing 
shot noise decreases because of space-charge smoothing. 
Similarly, for the inverting configuration of Figure 13.20b, we get 
c~(c~.+c~) ~ ( C~+C~n+C~ 
2 (Co'~' + ~ 
] t 
c, 
) VL 
DR = 
Nq 
Once again, we observe that Co~t must be moderately large. If not, any im- 
provement in dynamic range arises only at the cost of an extremely large value 
of (Cin + C1). This configuration also introduces an RHP zero. If the effects of 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
X 10 .9 
4 
........ 
, 
........ 
, 
...... 
~. 
301 
3.5 
'E 
3 
"~ 2.5 
O 
o 
.~ 
~ 
2 
~- 
~. 
t.5 
0"150-9 
O 
O 
O 
O 
10 -8 
10"7 
Bias Current (A) 
(a) 
10 "6 
1 
0.95 
0.9 
O 
0.7 
O 
~ 0.85 
O 
I~. 
m~ 0'8 
Â¢- 
~. 
~ 0.75 
O 
O 
, 
, 
, 
, , ,,,, 
, 
, 
. 
, , ,,,, 
0'6~' 0-9 
10-8 
107 
Frequency(Hz) 
O 
(b) 
10 .6 
Figure 13.19 
Characteristics of 1/f 
noise. (a) The I/f 
noise coefficient -ft'B, used in 
Eq. (13.22), as a function of the bias current _~B. (b) The I/f 
noise power n as a function 
of the bias current _/~. 

302 
NEUROMORPHIC SYSTEMS ENGINEERING 
this zero are to occur at frequencies well past the CF of the follower integrator, 
then 
Cout ( C2 + Cm + Ca ) 
C2 
+ Cin + Ca >> C2. 
Parasitic capacitances can also hurt this design significantly, especially if the 
explicit capacitors in the circuit are small. 
Actually, the circuit of Figure 13.20b does not even need an OTA, as the 
reference input of the OTA is not really used. The OTA can be replaced by a 
two-transistor amplifier, but, in that case, VL also should be replaced by VL/2. 
Thus, from Eq. (13.21), as in a normal OTA, N is still effectively 4. 
Theoretically, by making capacitive-divider ratios appropriately small, and 
by spending power, the dynamic range may be increased to values beyond that 
attained in our design. A floating-gate adaptation scheme combined with a 
two-transistor version of Figure 13.20b is being explored [8, 12]. 
13.5 
CONCLUSIONS 
We conclude by summarizing our key findings: 
. If the amplifier's noise is predominantly thermal, then an increase in 
its linear range increases the follower-integrator's dynamic range. 
If 
the amplifier's noise is predominantly i/f, then an increase in its linear 
range has no effect on the follower integrator's dynamic range. To pre- 
serve follower-integrator bandwidth, power consumption increases pro- 
portionately with an increase in the amplifier's linear range according to 
Eq. (13.18). 
2. In subthreshold, the noise is predominantly due to thermal noise, even at 
high bias currents, where some 1If noise is present. The theory described 
in [23] accurately modeled our thermal noise. Empirical expressions in 
the paper modelled our 1If noise. 
. In subthreshold circuits where thermal noise dominates, a simple formula 
for the dynamic range of a follower-integrator is D/~ = 2CVL/Nq. The 
capacitance of the follower-integrator is C, the linear range of the am- 
plifier is VL, the charge on the electron is q, and the effective number of 
noise-contributing transistors in the amplifier is N. A more complicated 
formula that includes 1If noise is given by Eq. (13.20). 
. Experimentally, we obtained a dynamic range of 65.2 dB in a follower- 
integrator with a capacitance of lpF. A signal with an input rms am- 
plitude of 1V yielded 4% total harmonic distortion. The total measured 
noise of the follower-integrator was 0.55 mV. A simple OTA C follower 
integrator has a theoretical linear range of 75 mV, and a theoretical noise 
floor of 110 #V. Thus, we obtained a dynamic range improvement of at 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
303 
In 
~> 
~ i__ 
~
~
~
 
(a) 
Out 
I 
l 
In 
~> 
C2 
(b) 
I 
k 
I 
OU 
Out 
>[~> 
Figure 13.20 
Capacitive-Divider Schemes for Widening the Linear Range. (a) and (b) 
show two different schemes. Section 13.5.4 contains further details. 
. 
. 
. 
. 
. 
least 8.5 dB over the OTA C follower-integrator. In practice, due to of- 
fchip noise floors on the order of 0.5-1 mV, the improvement can be as 
much as 20 dB. 
Bump linearization is our most efficient linearization technique, because 
it increases the linear range of our amplifier without increasing its noise. 
Gate degeneration is a useful transconductance-reduction technique. It 
can be generalized to the notion of the current increase from one input 
degenerating another input. The technique could be useful in multiple- 
gate-input circuits [18]. 
When the well is used as an input, the gate must be operated at as low a 
voltage as possible in order to obtain maximum dc-input operating range. 
Capacitive-divider techniques that widen the linear range bear similarity 
to our technique of using the well as an input. 
If appropriate atten- 
tion is paid to capacitor sizing, parasitic capacitances, and floating-gate 
adaptation in these techniques, then they may yield dynamic range im- 
provements similar to ours. 
Changes in ~, the subthreshold exponential parameter, are due to changes 
in dc current and to changes in well-to-gate voltage. These two effects 
may be studied separately through the techniques described in the paper. 

304 
NEUROMORPHIC SYSTEMS ENGINEERING 
Acknowledgments 
We thank Lena Peterson for providing the data of Figure 13.8. We thank Paul Hasler 
and Bradley Minch for useful discussions. This work was supported by the Beckman 
Hearing Center, and by the Center for Neuromorphic Systems Engineering as part of 
the National Science Foundation Engineering Research Center Program. 
Appendix: A 
This appendix contains a quantitative discussion of the common-mode effects 
in our amplifier. The data in the appendix were taken for a w = 0 amplifier 
built in a p-well process, as opposed to the data in the rest of the paper, which 
were taken in an n-well process. The ~ for the p-well process is lower, and 
consequently the linear range is near 0.6 V, rather than 1V. We also use the 
grounded-substrate convention [16]. This convention enables us to present the 
data as though they were from an n-well process, as in the rest of the paper. 
The grounded-substrate convention implements the following transformation 
from n-well space to p-well space: V-+ -V, n-channel-~p-channel, and p- 
channel-~n-channel. Note that the transformation is applied to all voltages 
implicitly defined in terms of the gate, source, drain, or well voltages in addition. 
For example, the flatband voltage is defined in terms of vG - vw, and changes 
sign as we move from n-well space to p-well space. Thus, if the flatband voltage 
is quoted as -0.75 V for an n-channel transistor, it's taken as -0.75 V for a native 
transistor in n-well space and as +0.75 V for a well transistor in p-well space. 
A.1 
THE EFFECTS OF CHANGES IN /~ 
In our amplifier, the gates of the W transistors are near ground. As we lower 
the voltages of the well inputs, the well-to-gate voltage decreases; consequently 
~ decreases; the transconductance, which is proportional to 1 - ~, increases. 
We now analyze this effect more quantitatively. 
The parameter ~ is a function of the gate-to-well voltage. We can show that 
g = 1 - 
2 
(13.A.1) 
' 
- 
(va 
- 
vw 
- 
VFB) 
where "~ is the body-effect parameter and VFB is the flatband voltage. 
A well-input amplifier that has no source degeneration or gate degeneration 
has a transconductance of magnitude gw, given by gw = (1 - ~). 
By com- 
puting the transconductance at the origin for various common-mode voltages 
Vc, we measured gw as a function of Vc at a bias current corresponding to 
VDD -- VB : 0.77 V. From Eq. (13.A.1), if we plot 1/(1 - ~)2 versus vw, i.e., 
1/gw 2 versus vc, we get 
- 
vcÃ· 
1Ã· 
g~ 
~ 
, 
4 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
305 
which is a straight lineâ¢ Thus, we can compute 7 and VFB from the slope 
and y-intercept of this line, if we know Va. For our experiment, we grounded 
the gate to allow maximum dc input operating range, so Vc was 0. From the 
â¢ 
1 
data shown in Figure 13.A.la, we computed 7 = 1.06V~ and VFB = 0â¢68 V. In 
comparison, the SPICE parameters from the MOSIS sheets quoted 7 = 1.05VÂ½ 
and VFB = 0.75V. The actual flatband voltages are negative; since the data 
were taken in a p-well process, we use the positive values as explained in the 
first paragraph of this appendix. 
A well-input amplifier with gate degeneration has a transconductance of 
magnitude gg, given by 
1--t~ 
(13.A.2) 
gg -- 1 + (~Â¢/~) 
For such an amplifier, we can determine the functional variation of a with 
Vc from Eq. (13.A.1), using the previously determined values of VFB and 7, 
and with Vc being the amount of diode drop on a G transistor. 
By using 
measured well and native transistor parameters we estimate V6 = 0.69 V given 
that VDD -- VB ---- 0.77 V, and also that ~ = 0â¢714. By using these parametric 
values in Eq. (13.A.2) and Eq. (13.A.1), we predicted the dependence of gg 
with vc. The middle curve of Figure 13.A.lb shows that changes of g~ with Vc 
were in good agreement with the theory of Eq. (13.A.2) and Eq. (13.A.1). The 
uppermost curve of Figure 13.A.lb is that of gw versus Vc and is also plotted 
for reference; it is simply a different way of plotting Figure 13.A.la. 
A well-input amplifier with source and gate degeneration has a transcon- 
ductance g given by Eq. (13.3)â¢ By using the functional variation of a versus 
Vc, the values of ~, the value of V6 estimated in the previous paragraph, and 
ap = 0.753, we were able to predict the variation of g with Vc, as shown by 
the lowest curve of Figure 13.A.1b. The data begin to deviate from theory at 
the lower input voltages, probably because of the change in gp with increasing 
well-to-gate voltage. 
A.2 
THE EFFECTS OF THE PARASITIC BIPOLAR TRANSISTOR 
To understand exactly when the parasitic bipolar transistor present in every 
MOS well transistor becomes significant, we find it instructive to analyze the 
interaction between the bipolar and MOS modes of operation for a well tran- 
sistor: The subthreshold saturation current of an MOS transistor situated in a 
well, which is assumed to be an n-well without loss of generality, is given by 
iM = IMe( uT ), 
(13.A.3) 
where ~b i8 the surface potential, and IM i8 a constant pre-exponential factor. 
The constant IM does have a weak dependence on ~, described in [27], that we 
neglect for simplicity. If ITO is the threshold current of the transistor, and if, 
at this point, the surface potential is below the source potential by an amount 
2Â¢F, then 
IM =- ITO e-2Â¢F/UT â¢ 

306 
NEUROMORPHIC SYSTEMS ENGINEERING 
The constant ITO is typically near #Cox(W/2L)(UT/~) 2. 
If all voltages are referenced to the well (i.e., vw = 0), and we define ~} = -Â¢ 
and gg = -(va - VFB), then we can show that 
We introduce the definitions of ~ and ~ because it is more convenient to work 
with -Â¢ and -(va - V~B) when dealing with transistors in the well. 
The source current of a well transistor is split into an MOS component, 
called iM, which reaches the drain of the transistor and a bipolar component, 
called iu, which is shunted away to ground. The bipolar current is given by 
[
~
 
iu = Iuet 
~ 
~, 
(13.A.4) 
where IB is the saturation current of the bipolar. The MOS current is given 
by Eq. (13.A.3). 
The question that we now ask is this: When does the MOS current exceed 
the bipolar current (iM ~ iB)? The answer to this question provides insight 
into how a well transistor must be operated if it is to have as wide a range of 
MOS operation as possible. We notice that the MOS and bipolar transistors 
have the same dependence on the source voltage, vs. Thus, in subthreshold, the 
answer is independent of the source voltage. The MOS pre-exponential factor, 
IM, is usually 1000 to 10000 times smaller than the bipolar pre-exponential 
factor IB. Thus, if the MOS transistor is to have any hope of competing with 
the bipolar transistor, its surface potential must be below that of the well by 
the amount that compensates for its pre-exponential handicap. 
Hence, the 
gate-to-well voltage must be below the flatband voltage by an amount needed 
to generate an adequate depth of depletion region. We now compute exactly 
how much this amount must be. 
If iM 2 is, then, from Eqs. (13.A.4) and (13.A.3), we must have 
vw-Â¢ 
~ 
UTIn(I~) 
~ 
VBM, 
(13.A.5) 
where V~M, defined as in the previous equation, is a voltage that yields a 
measure of by how much the bipolar's pre-exponential constant exceeds the 
MOS's. Thus, if we reference all voltages to the well, Eq. (13.A.5) yields 
7 2 
7 
> 
VBM. 
+ 
_ 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
30"/ 
20 Â¸ 
18Â¸ 
16 Â¸ 
14 Â¸ 
12 Â¸ 
10 Â¸ 
8 
6 
0.5 
0.40 
Well-to-Gate Voltage (V) 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
0.35 
'6 0.30 
-~ 
~ 0.25 
I,- 
~, 0.20 
(~ 
_ 
,~ 
-~ 
~ o.15 
E 
0.10 
0.05 
0.5 
gw 
gg 
g 
Input Voltage (V) 
1.0 
1.5 
2.0 
2.5 
3.0 
3.5 
4.0 
4.5 
Fil~ure 13.A.1 
The effects of changes in ~. (a) The changes in E with well-to-gate voltage 
may be used to extract the body-efl:ect parameter ~ and the flatband voltage VFB. The 
slope of the graph yields information about ~f, and the intercept then yields information 
about VFB. See Section A.1 for details. (b) Data for the change in transconductance of 
well-input amplifiers with no degeneration (gw), with gate degeneration (gg), and with gate 
and source degeneration (g). The solid lines are fits to theory. 

308 
NEUROMORPHIC SYSTEMS ENGINEERING 
After we perform simple manipulations on the previous equation, we finally 
obtain the criterion for the MOS transistor to dominate over the bipolar tran- 
sistor: 
<_ VCT 
(13.A.7) 
where we define VCT to be the RHS of Eq. (13.A.6). If we require iM ~_ 100iB, 
for robust operation, we must increase VSM by UTlnl00 before using 
Eq. (13.A.7). We now have a recipe for operating well transistors in subthresh- 
old: As long as we ensure that the gate is sufficiently below the well, we are 
in no danger of turning on the bipolar transistor. Thus, if the range of MOS 
operation is to be as wide as possible, the gate voltage must be set to as low a 
value as circuit requirements will allow. The well is then free to operate from 
low values near the gate voltage to VDD. Therefore, when the well is being used 
as an input, the gate should be at ground or near ground. In our amplifier, it 
is one diode drop above ground. 
To understand the effects of the parasitic bipolar transistor in our amplifier, 
we need only to understand the half-circuit shown in Figure 13.A.2a, because 
the bipolar transistor(s) in the arm(s) of our amplifier is (are) activated if the 
voltage of either input gets too low. It is thus simpler to analyze the effects 
of the bipolar transistor in a single arm of the amplifier, and to extend the 
analysis to the case where the bipolar transistors in both arms are activated. 
Hence, we tie one input to VDD to turn Off completely one arm of the amplifier, 
and we concentrate on the turned-on arm. 
The circuit of Figure 13.A.2a is one half of the differential pair of the amplifier 
with the S transistor omitted, and the bipolar transistor drawn explicitly. The 
S transistor is omitted because it affects the drain voltage of the bias transistor 
(the bias-current transistor not shown), but has no effect on the currents is and 
iM, which are our primary concern. Figure 13.A.2b and Figure 13.A.2c show 
the small-signal model and corresponding signal-flow diagram that describe the 
half-circuit of Figure 13.A.2a. When the well-input voltage vxy is in the 1V 
to 5V range, the gate voltage, v~, which is one diode drop above ground, is 
sufficiently below the well voltage that all the bias current is carried by the MOS 
transistor. As the well voltage begins to drop, it starts to get close to the gate 
voltage and the bias current starts to be diverted to the bipolar transistor. The 
data of Figure 13.A.3a show that a differential-pair-like competition between 
the MOS and bipolar transistors yields a sigmoid curve in output current versus 
input voltage. 
The data curves of Figure 13.A.3 were normalized by their 
saturation current values, and were fit by sigmoid equations of the form 
1 
iN = 1 + e -0.326(vI~v-VH)/UT' 
(13.A.8) 
with differing values of VH for the three curves. 
The input voltage at which the bipolar and MOS currents become equal, 
VH, depends on the bias current. Higher bias currents drop larger voltages 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
309 
(a) 
(b) 
~ i~ 
ii I;> 
i. --~i~ _ 
~ 
I~-.IIv~-v,ol 
GM[ 
- 
(c) 
-- 
+ 
-- 
VinO 
~ 
~ 
~ 
~ 
b 
(Vs-V m) 
_ 
Figure 13.A.2 
The parasitic bipolar half circuit. (a) A simplified half circuit for our 
amplifier that is important for understanding the bipolar-mos interaction in it. (b) The small- 
signal equivalent circuit for (a). (c) A signal-flow diagram for the small-signal equivalent 
circuit. 
across the GM transistor, increase the gate voltage of the W transistor, and 
consequently cause the well-input voltage to approach the gate voltage at higher 
values. The data of Figure 13.A.3b show our measurements of this effect, and 
also show that 
VH = 0.81 (VDD -- VB) + 0.019. 
(13.A.9) 
Given the theory that we have developed for the bipolar mos interaction 
(Eq. (13.A.7)), and the effects of changes in n (Eq. (13.A.1)), we can pre- 
dict what Eqs. (13.A.8) and (13.A.9) should be by analyzing the circuit of 
Figure 13.A.2a. Using measured values for the constants VFB, 7, nn, nb (the n 
of the bias-current transistor), and VBM (determined by measurements of the 
bipolar and MOS pre-exponential constants), we were able to obtain reasonable 
agreement between theory and experiment. Prom the signal-flow diagram of 
Figure 13.A.2c, we can show that the constant 0.326 of Eq. (13.A.8) is nearly 
c~ = 2 (~/2)~ 
~/2 + ~n' 

310 
NEUROMORPHIC 
SYSTEMS 
ENGINEERING 
VDD-V B (V) 
0.65 
0.70 
0.75 
0.80 
0.85 
0.90 
1.2 
1.0 
0.8 
0.6 
0.4 
0,2 
0.0 
-0.2 
-0.2 
0.775 
0.750 
0.725 
0.700 
0.675 
0.650 
0.625 
0.600 
0.575 
0.550 
0.525 
0.60 
(a) 
0.800 V o~ 
0.725 V
~
 
Input Voltage (V) 
I 
I 
I 
I 
I 
I 
I 
0.0 
0.2 
0.4 
0.6 
0.8 
1.0 
1.2 
>= 
Figure 13.A.3 
The Bipolar-MOS Characteristics. The sigmoid-like compeitition between 
the MOS and bipolar transistors as the input voltage is varied. The three parametric voltages 
refer to the value of the bias voltage VB of Figure 13.1. (b) The threshold point of this 
sigmoid varies with VB. 
Note that we evaluated ~ with the surface potential at a value that was VBM 
below the well voltage. We found that it was 0.467. Using Eq. (13.A.7) and 
elementary reasoning, we can show that Eq. (13.A.9) is derived from 
v. = ~ (v,~,, _ v.) + v:~ 1. ( *g'~ 
- v~, 
~= 
~ 
t,~g) 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
311 
where I0 ~ and I0 N are the subthreshold scaling parameters for the bias-current 
transistor and for the G transistor respectively. We found that ~b = 0.5615, 
~,~ = 0.714, I0 B = 1.12 Ã 10 -17 A, ION = 1.40 Ã 10 -16 A, IB = 2.12 Ã 10 -16 A, 
I:~0 = 1.8 Ã 10 -7 A, 2Â¢F = 0.749 V, VBM = 0.2165V, ~/= 1.05VÂ½, VFB = 0.75 
V. The most interesting finding is that VBM = 0.26 V, which implies that the 
MOS transistor's pre-exponential constant IM is about 4000 times as weak as 
the bipolar transistor's pre-exponential constant IB. 
References 
[1] X. Arreguit. Compatible Lateral Bipolar Transistors in CMOS Technology: 
Model and Applications. DSc These no. 817, Ecole Polytechnique Federale 
de Lausanne, 1989. 
[2] M. C. H. Cheng and C. Toumazou. Linear composite MOSFETs (COM- 
FETs). Electronics Letters, 27(20):1802-1804, 1991. 
[3] W. Chung, K. Kim, and H. Cha. A linear operational transconductance 
amplifier for instrumentation applications. IEEE Trans. on Instrumenta- 
tion and Measurement, 41(3):441-443, 1992. 
[4] T. Delbriick. Bump circuits for computing similarity and dissimilarity of 
analog voltages. CNS Memo No 26, May 1993. 
[5] S. T. Dupuie and M. Ismail. High frequency CMOS transconductors. In 
C. Toumazou, F.J. Lidgey, and D.G. Haigh, editors, Analogue IC Design: 
the current mode approach. Peter Peregrinus Ltd. on behalf of IEEE, 1990. 
[6] P. Furth and A. B. Andreou. Linearized differential transconductors in sub- 
threshold CMOS. IEEE Electronics Letters, 31(7):545-547, March 1995. 
[7] P. R. Gray and R. G. Meyer. Analysis and Design of Analog Integrated 
Circuits, pages 183 186. John Wiley and Sons, New York, 1987. 
[8] P. Hasler. Personal communication. 
[9] P. Hasler, B. A. Minch, C. Diorio, and C. Mead. An autozeroing amplifier 
using pfet hot-electron injection. In Proc. IEEE Intl. Symp. on Circuits 
and Systems, volume 3, pages 325-328, Atlanta, May 1996. 
[10] H. Khorramabadi and P. R. Gray. High frequency CMOS continuous time 
filters. IEEE J. Solid State Circuits, SC-19(6):939-948, 1984. 
[11] F. Krummenacher and N. Joehl. A 4 MHz CMOS continuous time filter 
with on-chip automatic tuning. IEEE J. Solid-State Circuits, SC-23:750- 
758, 1988. 
[12] R. W. Landee, D. C. Davis, and A. P. Albrecht. Electronic Designers 
Handbook, pages 3-18, 3-34, 3-38. McGraw-Hill Book Co, N.Y., 19957. 
[13] W. Liu. An Analog Cochlear Model: Signal Representation and VLSI Real- 
ization. PhD thesis, John Hopkins University, Baltimore, Maryland, 1992. 
[14] R. F. Lyon. Analog implementations of auditory models. In ARPA Work- 
shop on Speech and Natural Language. Morgan Kaufmann Publishers, San 
Mateo CA, 1991. 

312 
NEUROMORPHIC SYSTEMS ENGINEERING 
[15] R. F. Lyon, T. Delbr/ick, and C. A. Mead. Circuits for wide input range 
analog rectification and correlation. U.S. Patent 5,319,268, June 1994. 
[16] R. F. Lyon and C. A. Mead. The CMOS grounded-substrate convention. 
Caltech Computation and Neural Systems Memo, 27, 1993. 
[17] C. A. Mead. 
Analog VLSI and Neural Systems, pages 33-36, 67-82. 
Addison-Wesley, Reading, MA, 1989. 
[18] B. A. Minch, C. Diorio, P. Hasler, and C. A. Mead. Translinear circuits us- 
ing subthreshold floating-gate MOS transistors. Analog Integrated Circuits 
and Signal Processing, 9(2):1 13, 1996. 
[19] B. Nauta and E. Seevinck. Linear CMOS transconductance element for 
vhf filters. Electronics Letters, 25:448-450, 1989. 
[20] A. Nedungadi and T. R. Viswanathan. Design of linear CMOS transcon- 
ductance elements. IEEE Trans., CAS-31(10):891-894, 1984. 
[21] I. E. Opris and G. T. A. Kovacs. 
Large-signal subthreshold CMOS 
transconductance amplifier. Electronics Letters, 31(9):718-720, April 1995. 
[22] J. Ramirez-Angulo and E. SÂ£nchez-Sinecio. 
Programmable BiCMOS 
transconductor for capacitor-transconductor filters. Electronics Letters, 
28(13):1185-1187, 1992. 
[23] R. Sarpeshkar, T. Delbriick, and C. Mead. White noise in MOS transistors 
and resistors. IEEE Circuits and Devices, 9(6):23-29, November 1993. 
[24] R. Sarpeshkar, Lyon R. F., and C. A. Mead. An analog VLSI cochlea with 
new transconductance amplifiers and nonlinear gain control. In Proc. IEEE 
Intl. Conf. on Circuits and Systems, volume 3, pages 292-295, Atlanta, 
May 1996. 
[25] R. Sarpeshkar, Lyon R. F., and C. A. Mead. Nonvolatile correction of 
q-offsets and instabilities in cochlear filters. In Proc. IEEE Intl. Conf. on 
Circuits and Systems, volume 3, pages 329-332, Atlanta, May 1996. 
[26] S. Szczepanski, J. Jakusz, and A. Czarniak. Differential pair transconduc- 
tot linearisation via electronically controlled current-mode cells. Electron- 
ics Letters, 28(12):1093-1095, 1992. 
[27] Y. Tsividis. Operation and Modeling of the MOS Transistor, pages 136- 
140. McGraw-Hill Book Company, New York, 1987. 
[28] Y. Tsividis, Z. Czarnul, and S. C. Fang. MOS transconductors and inte- 
grators with high linearity. Electronics Letters, 22(5):245-246, 1986. 
[29] Z. Wang and W. Guggenbuhl. A voltage-controllable linear MOS transcon- 
ductor using bias offset technique. 
IEEE J. Solid State Circuits, SC- 
22(3):357-365, 1987. 
[30] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692 700, 
May 1992. 
[31] G. Wilson. Linearised bipolar transconductor, ectronics Letters, 28(4):390 
391, 1992. 

A LOW-POWER WIDE-LINEAR-RANGE TRANSCONDUCTANCE AMPLIFIER 
313 
[32] P. Wu and R. Schaumann. Tunable operational transconductance amplifier 
with extremely high linearity over very large input range. 
Electronics 
Letters, 27(14):1254-1255, 1991. 

14 
FLOATING-GATE MOS SYNAPSE 
TRANSISTORS 
Chris Diorio, Paul Hasler, Bradley A. Minch, And Carver Mead 
Physics of Computation Laboratory, 
California Institute of Technology, 
Pasadena, CA 91125, USA 
ch ris@pcmp.caltech.edu 
14.1 
INTRODUCTION 
Our goal is to develop silicon learning systems. One impediment to achieving 
this goal has been the lack of a simple circuit element combining nonvolatile 
analog memory storage with locally computed memory updates. Existing cir- 
cuits [63, 132] typically are large and complex; the nonvolatile floating-gate de- 
vices, such as EEPROM transistors, typically are optimized for binary-valued 
storage [17], and do not compute their own memory updates. Although floating- 
gate transistors can provide nonvolatile analog storage [1, 15], because writing 
the memory entails the difficult process of moving electrons through SiO2, these 
devices have not seen wide use as memory elements in silicon learning systems. 
We have fabricated synapse transistors that not only possess nonvolatile ana- 
log storage, and compute locally their own memory updates, but also permit 
simultaneous memory reading and writing, and compute locally the product 
of their stored memory value and the applied input. To ensure nonvolatile 
storage, we employ standard floating-gate MOS technology, but we adapt the 
physical processes that write the memory to perform a local learning function. 
Although the SiO2 electron transport still is difficult, and does require high 
voltages, because our devices integrate both memory storage and local compu- 
tation within a single device, we expect them to find wide application in silicon 
learning systems. 
We call our devices synapse transistors because, like neural synapses [11], 
they compute the product of their stored analog memory and the applied in- 
put. Also like neural synapses, they can learn from the input signal, without 

316 
NEUROMORPHIC SYSTEMS ENGINEERING 
interrupting the ongoing computation. Although we do not believe that a sin- 
gle device can model the complex behavior of a neural synapse completely, our 
single-transistor synapses do implement a learning function. With them, we in- 
tend to build autonomous learning systems in which both the system outputs, 
and the memory updates, are computed locally and in parallel. 
We have described previously [6, 60, 28] the four-terminal nFET synapse 
discussed here. We have also described an analog memory cell that employs 
the nFET device [5], and an autozeroing amplifier that employs the pFET 
device [12]. We here present the four-terminal nFET synapse in greater detail 
than we did previously, and for the first time present the four-terminal pFET 
synapse. We have also described previously a three-terminal nFET synapse [7]. 
Although the four-terminal synapses require slightly more layout area than does 
this three-terminal device, the additional terminal gives us greater control over 
the write and erase processes. 
14.2 
THE SYNAPSES 
The nFET and pFET synapses each possess a polyl floating gate, a poly2 
control gate, and an n-well tunneling implant. Both synapses use hot-electron 
injection [23] to add electrons to their floating gates, and Fowler-Nordheim 
(FN) tunneling [16] to remove the electrons. The nFET synapse differs from 
a conventional n-type MOSFET in its use of a moderately doped channel im- 
plant. This implant facilitates hot-electron injection. The pFET synapse, by 
contrast, achieves a sufficient hot-electron gate current using a conventional 
p-type MOSFET; no special channel implant is required. We fabricated both 
synapses in the 2#m n-well Orbit BiCMOS process available from MOSIS. 
In both synapses, the memory is stored as floating-gate charge. Either chan- 
nel current or channel conductance can be selected as the synapse output. 
Inputs typically are applied to the poly2 control gate, which couples capaci- 
tively to the polyl floating gate. From the control gate's perspective, altering 
the floating-gate charge shifts the transistor's threshold voltage Vt, enabling 
the synapse output to vary despite a fixed-amplitude control-gate input. 
We typically operate the synapses in their subthreshold regime [18], and 
select either drain current or source current as the synapse output. We choose 
subthreshold operation for three reasons. 
First, the power consumption of 
a subthreshold MOSFET typically is less than I#W. 
Second, because the 
channel current in a subthreshold MOSFET is an exponential function of the 
gate voltage, only small quantities of oxide charge are required for learning. 
Third, the synapse output is the product of a stored weight and the applied 
input: 
~Vf~ I 
~(Qfl]+Cir~ 
Vi~) 
Q[~ ectVi~ 
~/V~n 
Is = Ioe 
v~ 
= Ioe 
crv~ 
= Ioe Qr e vt 
= WIoe 
vt 
(14.1) 
where Is is the synapse's source current, Io is the pre-exponential current, ~ is 
the coupling coefficient from the floating gate to the channel, Qf~ is the floating- 
gate charge, CT is the total capacitance seen by the floating gate, Ut is the 

FLOATING-GATE MOS SYNAPSE 
317 
thermal voltage kT/q, Cin is the input (polyl to poly2) coupling capacitance, 
V~n is the control-gate input voltage, QT = CTUt/n, ~' =- aCin/CT, W =- 
exp(Qfg/QT), and, for simplicity, the source potential is assumed to be ground 
(vs = 0). 
The synapse weight W is a learned quantity: Its value derives from the 
floating-gate charge, which can change with synapse use. The synapse output 
is the product of W and the source current of an idealized MOSFET that has 
a control-gate input V/n and a coupling coefficient ~ from the control gate to 
the channel. 
Because the tunneling and injection gate currents vary with the synapse 
terminal voltages and channel current, W varies with the terminal voltages, 
which are imposed on the device, and with the channel current, which is the 
synapse output. Consequently, the synapses exhibit a type of learning by which 
their future output depends on both the applied input and the present output. 
14.2.1 The ~FET Synapse 
Top and side views of the nFET synapse are shown in Fig. 14.1. Its principal 
features are the following: 
â¢ 
Electrons tunnel from the floating gate to the tunneling implant through 
the 350_& gate oxide. High voltages applied to the tunneling implant pro- 
vide the oxide electric field required for tunneling. To prevent breakdown 
of the reverse-biased pn junction from the substrate to the tunneling im- 
plant, we surround the n + tunneling implant with a lightly doped n- 
well. Tunneling removes electrons from the floating gate, increasing the 
synapse weight W. 
â¢ 
Electron tunneling is enhanced where the polyl floating gate overlaps 
the heavily doped well contact, for two reasons. First, the gate cannot 
deplete the n + contact, whereas it does deplete the n- well. Thus, the 
oxide electric field is higher over the n +. Second, enhancement at the 
gate edge further augments the oxide field. 
â¢ 
Electrons inject from the drain-to-channel space-charge region to the 
floating gate. To facilitate injection, we apply a p-type bipolar-transistor 
base implant to the MOS transistor channel. This implant serves two 
functions. 
First, it increases the peak drain-to-channel electric field, 
thereby increasing the hot-electron population in the drain-to-channel 
depletion region. Second, it raises the floating-gate voltage, causing the 
drain-to-gate oxide electric field to favor the transport of injected elec- 
trons to the floating gate. Injection adds electrons to the floating gate, 
decreasing the synapse weight W. 
â¢ 
Oxide uniformity and purity determine the initial matching between 
synapses, as well as the learning-rate degradations due to oxide trap- 
ping. We therefore use the thermally grown gate oxide for all SiO2 carrier 
transport. 

318 
NEUROMORPHIC SYSTEMS ENGINEERING 
A. Top View 
source 
contact 
poly2 
m
e
t
a
~
 
~ fi::~ 
!i 
~ 
i 
I 
~ 
n* source 
n + drain 
diffusion 
diffusion 
B. Side View 
interpoly 
polyl 
n Ã· well 
n well 
~ing 
gate 
coitact 
~/ 
i 
, 
\, 
\ 
p substrate 
tunneling junction: 
implant 
thinox over n- 
field-oxide 
interpoly 
capacitor 
channel stop 
oxide 
gate 
electron 
gate 
electron 
oxide 
injection 
oxide tunneling 
p substrate 
C. Electron 
Band 
Diagram 
r source channel 
â¢ 
"~ 
3. 
~ ~drain 
,_o > 
3.2V 
"~= 
.t 
noating gate 
~ .~ 6.0 
~ 
; 
~ 
~ 
SiO~ 
N 
barrier 
33.( 
electron 
f 
~\ injection 
SiO_, 
electron 
-
-
 
~.~ 
tunneling 
2V 
tunneling 
~- "~imp,~lant 
position (lain) v 
Figure 14.1 
The nFET synapse, showing the electron tunneling and injection locations. 
The three diagrams are aligned vertically. Diagrams A and C are drawn to scale; for clarity, 
we have exaggerated the vertical scale in diagram B. In the 2~m Orbit process, the synapse 
length is 48#m, and the width is 17#m. All voltages in the conduction-band diagram 
are referenced to the source potential, and we have assumed subthreshold channel currents 
(Is < 100hA). Although the gate-oxide band diagram actually projects into the plane of 
the page, for clarity we have rotated it by 90 Â° and have drawn it in the channel direction. 
When compared with a conventional nFET, the p-type substrate implant quadruples the 
MOS gate-to-channel capacitance. With a 50fF interpoly capacitor as shown, the coupling 
coefficient between the poly2 control gate and the polyl floating gate is only 0.2. To 
facilitate testing, we enlarged the interpoly capacitor to ipF, thereby increasing the coupling 
to 0.8. 
14.2.2 The pFET Synapse 
Top and side views of the pFET synapse are shown in Fig. 14.2. Its principal 
features are the following: 

FLOATING-GATE MOS SYNAPSE 
319 
A. Top View 
source 
contact 
poly2 
polyl 
n + well 
n well 
metal 
cut 
control gate 
floating gate 
contact 
/ 
~ 
~ 
, 
t 
I 
~- 
I 
\ 
~=/ 
I 
~ 
\ 
-- 
\, 
pÃ· source 
p~ d~ain 
n welI 
tunneling junction: 
diffusion 
diffusion 
thinox over n- 
B. Side View 
interpoly 
field-oxide 
interpoly 
capacitor 
oxide 
~ 
ch .... Istop 
~ 
~
\
 
! 
gate 
electron 
oxide 
injection 
oxide 
tunneling 
p suhstrate 
C. Electron Band Diagram 
Ã·V' 
-- 
~ 0 
e --drain 
electron/~ I ~. 
injection 
~- 
~ 
~ 
~ 
~ 
3.2V 
~ 6.9 Ec~ 
L 
~/ 
-~ ~8-0 E
~
 
~ 
.. 
~
-
~
o
~
.
 
source 
channel 
N 
33.0~ 
~ 
impact ionization 
 .]vl 
floating gate 
~ ~_~/tu 
ling 
t 
tunneling 
I 
i~plant 
1 ?.2v / 
[J~ 
~ 
position (~m) w 
Figure 14.2 
The pFET synapse, showing the electron tunneling and injection locations. 
The well contact is not shown. Like we did in Fig. 14.1, we have aligned the three diagrams 
vertically, drawn diagrams A and C to scale, exaggerated the vertical scale in diagram B, ref- 
erenced the voltages in the band diagram to the source potential, and assumed subthreshold 
(Is < 100nA) operation. Whereas the tunneling process is identical to that in the r~FET 
synapse, the injection process is different. As we describe in the text, we generate the elec- 
trons for oxide injection by means of hole impact ionization at the transistor's drain. In the 
2#rr~ Orbit process, the synapse length is 56#m, and the width is 16#m. With a 5OfF 
interpoly capacitor as shown, the coupling coefficient between the poly2 control gate and 
the polyl floating gate is only 0.25. We enlarged the interpoly capacitor to IpF in the test 
device, thereby increasing the coupling to 0.8. 
Electrons tunnel from the floating gate to the tunneling implant through 
the 350/~ gate oxide. The tunneling implant is identical to that used in 
the nFET synapse. 
As in the nFET synapse, tunneling removes elec- 

320 
NEUROMORPHIC SYSTEMS ENGINEERING 
trons from the floating gate. However, because the pFET and nFET 
synapses are complementary, tunneling has the opposite effect on the 
pFET synapse: It decreases, rather than increases, the synapse weight 
W. 
Electrons inject from the drain-to-channel space-charge region to the 
floating gate. Hole impact ionization generates the electrons for oxide in- 
jection. Channel holes, accelerated in the drain-to-channel electric field, 
collide with the semiconductor lattice to produce additional electron-hole 
pairs. The liberated electrons, promoted to their conduction band by 
the collision, are expelled rapidly from the drain region by this same 
drain-to-channel electric field. Electrons that acquire more than 3.2eV 
of kinetic energy can, if scattered upward into the gate oxide, inject onto 
the floating gate. As in the nFET synapse, injection adds electrons to 
the floating gate; however, because the transistor is a pFET, injection 
increases, rather than decreases, the synapse weight W. 
Like the nFET synapse, the pFET synapse uses gate oxide for all SiO2 
carrier transport. 
14.3 
THE GATE-CURRENT EQUATION 
We intend to build silicon learning systems using subthreshold synapse transis- 
tors. Because the learning behavior of any such system is determined in part 
by the tunneling and injection processes that alter the stored weights, we have 
investigated these processes over the subthreshold operating regime. 
14.3.1 
The Tunneling Process 
The tunneling process, for the nFET and pFET synapses, is shown in the 
energy-band diagrams [9] of Figs. 14.1 and 14.2, respectively. In FN tunnel- 
ing, a potential difference between the tunneling implant and the floating gate 
reduces the effective oxide thickness, facilitating electron tunneling from the 
floating gate, through the SiO2 barrier, into the oxide conduction band. These 
electrons are then swept over to the tunneling implant by the oxide electric 
field. We apply positive high voltages to the tunneling implant to promote 
electron tunneling. 
14.3.2 
The Tunneling Equation 
The data of Fig. 14.3 show tunneling gate current versus the reciprocal of the 
voltage across the tunneling oxide. We fit these data with an FN fit [16, 22]: 
Ig : ~Vo2ze-v~o~ 
(14.2) 

:[OAO adams aa~ uoql StlOaaaaIa osoq& "pu~q uo~aanpuoa ap~xo aqa oau~ 'aa~aa~q 
uo~aunj-~ao~ ~O~S - ~S AUg oq~ aaao 'iouuvqa ao~s~su~a~ oqa moaj aao[u~ suoaa 
-aala "XIaa~ aadsaa 'U~I pu~ I'~I "s$~d jo sm~aSv~p puvq-*$aaua aqa u~ u~oqs s~ 
'sosd~uXs Â£add pu~ &aau aqa qaoq aoj '[0g] ssoaoad uo~aao[u~ uoaaaOlO-ao q oqÂ£ 
ssoaOad uo~aaafu I uoaaao[~-~o H oqÂ£ 
g'g'~ 
â¢ uouaulouaqd aSpo uv .~it.avun.ad st. samst.suva~, osdvuSs oq:~ 
u! ~u!ioaunz 'o~mioa op!xo qa!m Xli~t.auouodxo sos~oaau~ ~u~iouun a Nd osn~aoa 
â¢ +u aqa sdvlsO*o o~v$ ~u~o~ pou$~iv-jios oqa aaoq~ aoq$~q s~ pia~ ap~xo aq~ 
'~a~uoa ilO~ +u oq~ O~oldop Â£iqepaadd~ ~ouu~a o~ 
oq~ osn~ao H "~uaaana 
Su~Iauuna aRl a~ Ra~ pu~ 'a$~aioa ap~xo aa~aaa~a aRa ~upnpaa 'Iia~ _u padop 
XIaR$~I aRa m uo~$aa uo~aalda p v saanpu~ aav~ ~uD~o ~ aRa asn~aaq 'suoaa~m 
I~aU~I u~ 'qaSuo I aSpa +u-oa-oa~$ aqa oa g'~I '~d jo ~a~P aq~ paz~i~maou o~ 
â¢ saumsuoa a~ aa~ Â° A pu~ '~A '} oaoqm 
 (*qA + XÂ°A)  = q 
o A 
:Xiasoia aaouI m~P [e:lUaUlt.,iadxa aqa lgI oa {uo[.~:etlba NÂ¢I aq:l o9~ '~qA 'I~,taua:~ od 
m.-:~it.n q ~ pp~ a~ qat.qa~ u[. 'a~ I~at.a!dma u~ ~oqs osI~ 033 . ",/aaatu~:I~d ~t ~ st 
o~ puv !ap!xo aav$ ~0~ aqa uaat$ '$u!iauuna ~O!S jo [gI] Xaaans luaaaa ~ q:g~ 
:~uaas!suoa s.[ A~86 = ~'A !a~IOA ap.[xo aqa ~t. *o A !auaaana oa~ aqa s! ~I osoqa~ 
â¢ uosgeduJo3 aoj (aU!l 
paqsep) ~!j tu!aqpJoN-~al~ao-I leUO!~UaAUO3 aql Aaoqs OSle a~ :,~laSOl :) aJo~ e~ep le~Uampadxa 
aq~ ~J O~ '~qA 'a~e~loA u[-~l[nq e s~old~a (au H p[los) ~j IeDg#~a Jn 0 '~xa~ aq~ u[ ssn~s~p 
a~ ~Eq~ SUOSEaJ JoJ 'suoJD~ ieaU~l u~ 'q~ua I a~pa +~-o~-a~E~ uo~unf-~u~lauun~ aq~ o~ 
e~ep aq~ pazHe~Jou a~ 'a~e~ ~u~eOlj aq~ pue uo[~Dun[ ~uHauun ~ aq~ uaa~aq aDuaJajj[p 
le~lua~od aql aq o~ ~Â° A au~jap a~ ZOA/I snsJaA 6[ ~uaJJn3 (a~e~) ~uHauun K 
E'~[ aJn~[j 
(A/I) ~Izll oA zp!xo / I- 
I~0"0- 
EE0"0- 
9E0"0- 
I 
I 
I 
I 
lroo- 
l~lmo- 
' 
' 
o~-Â°! 
. . . ~  
~ 
~ 6~- O[ 
i 
~.OI 
91-0I 
IÃleE'I=~I .... 
OI 
~1- 
._o[ 
fl_Ol 
R 
> 
g 
Ig~ 
HSdVhIXS SOBI ~J,VD-DNIJNO'Id 

322 
NEUROMORPHIC SYSTEMS ENGINEERING 
to the floating gate by the oxide electric field. Successful injection, for both 
the nFET and pFET synapses, requires that the following three conditions be 
satisfied: (1) the electrons must possess the 3.2eV required to surmount the 
Si - SiO2 barrier, (2) the electrons must scatter upward into the gate oxide, and 
(3) the oxide electric field must be oriented in the proper direction to transport 
the electrons to the floating gate. 
nFET Injection. 
In a conventional n-type MOSFET, requirements 1 and 
2 are readily satisfied. We merely operate the transistor in its subthreshold 
regime, with a drain-to-source voltage greater than about 3V. Because the 
subthreshold channel-conduction band is flat, the drain-to-channel transition is 
steep, and the electric field is large. Channel electrons are accelerated rapidly 
in this field; a fraction of them acquire the 3.2eV required for hot-electron 
injection. A fraction of these 3.2eV electrons naturally scatter, by means of 
collisions with the semiconductor lattice, upward into the gate oxide. 
It is principally requirement 3 that prevents injection in a conventional 
nFET. Subthreshold operation typically implies gate-to-source voltages less 
than 0.SV. With the drain at 3V, and the gate at 0.SV, the drain-to-gate elec- 
tric field opposes transport of the injected electrons to the floating gate. The 
electrons are instead returned to the drain. 
In the synapse transistor, we promote the transport of injected electrons to 
the floating gate by increasing the bulk channel doping. The additional dopant 
increases the channel surface-acceptor concentration, raising the transistor's 
threshold voltage from 0.SV to 6V. With the drain at 3V, and the gate at 6V, 
the channel current still is subthreshold, but now the oxide electric field sweeps 
injected electrons over to the floating gate, rather than returning them to the 
silicon surface. 
pFET Injection. 
Because the pFET channel current comprises holes, pFET 
hot-electron injection is different from nFET injection. We accelerate channel 
holes in the drain-to-channel depletion region of a subthreshold pFET. A frac- 
tion of these holes collide with the semiconductor lattice at energies sufficient 
to liberate additional electron-hole pairs. The ionized electrons, promoted to 
their conduction band by the collision, are expelled from the drain by the drain- 
to-channel electric field. If these ionized electrons are expelled with more than 
3.2eV of kinetic energy, they can inject onto the floating gate. 
In the pFET synapse, like in the nFET, injection requirements 1 and 2 are 
easily satisfied. We merely operate the transistor in its subthreshold regime, 
with a drain-to-source voltage greater than about 6V. The higher drain-voltage 
requirement, when compared with the nFET synapse, is a consequence of the 
two-step injection process. 
In a subthreshold pFET, the gate-to-source voltage typically is less than 
1V; if the drain-to-source voltage exceeds 6V, the gate voltage must exceed 
the drain voltage by at least 5V. The oxide electric field supports strongly the 
transport of injected electrons to the floating gate, and requirement 3 is always 

FLOATING-GATE MOS SYNAPSE 
323 
satisfied. Unlike conventional nFET transistors, conventional pFET transistors 
naturally inject electrons onto their floating gates (at sufficient drain-to-source 
voltages); we do not need to add a special channel implant to facilitate injection. 
14.3.4 The Injection Equation 
10 -4 
~ 104 
Â¢9 
-6 
v 10 
m 
~ 
10 -7 
~ 
10 .8 
~9 
~) 
N 
~0 10-9 
10 -~o 3 
â¢ 
pFET 
[ 
31.6 \2 
,,Â¢' 
% 
Ig 
-3-t~)- 
~ 
-7-=5.50x10 e 
~ 
L 
\// 
? 
nF~ 
~ 
~ 
6I 
_[_ 8.06 ~ 
r ~9.7~ 
3e-i~] 
~ 
~ I s 
~ 
/ 
I 
I 
I 
4 
5 
6 
7 
8 
~ 
10 
1~1 
12 
&aN-to-chapel voltage (V) 
Figure 14.4 
Injection efficiency versus drain-to-channel voltage, for both the nFET and 
pFET synapses. We held the gate-to-channel voltages fixed during the experiments. For 
the nFET, Vg c = 
5.66V; for the pFET, Vgc = 1.95V. In the nFET synapse, when the 
drain voltage exceeds the floating-gate voltage, the oxide E-field tends to return the injected 
electrons to the silicon surface, rather than transporting them to the floating gate. As a 
result, for drain-to-channel voltages near Vgc = 5.66V, the nFET data deviate from the 
fit. 
The data of Fig. 14.4 show injection efficiency (gate current divided by 
source current) versus drain-to-channel potential, for both the nFET and pFET 
synapses. The data are plotted as efficiency because, for both devices, the gate 
current is linearly proportional to the source current over the entire subthresh- 
old range. Because the hot-electron injection probability varies with the drain- 
to-channel potential, we reference all terminal voltages to the channel. We can 
re-reference our results to the source terminal using the relationship between 
source and channel potential in a subthreshold MOSFET [2, 8]: 
â¢ ~ ~V~ + ~o 
(14.4) 
where q2 is the channel potential, Vyg is the floating-gate voltage, ~ is the 
coupling coefficient from the floating gate to the channel, and ~0 derives from 
the MOS process parameters. 
For both synapses, the injection efficiency is independent, to first-order, of 
the floating-gate-to-channel voltage, as long as V.fg > V d (where V~g and Vd 

324 
NEUROMORPHIC SYSTEMS ENGINEERING 
are the floating gate and drain voltages, respectively). In the pFET synapse, 
this condition is always satisfied. In the nFET synapse, this condition is not 
necessarily satisfied; the data of Fig. 14.4 show what happens when we sweep 
the nFET drain from voltages much less than V/9, to voltages much greater 
than VI~. As Va approaches VI~ , the oxide voltage becomes small, and the gate 
current drops. 
We fit the injection data of Fig. 14.4 empirically; we are currently analyzing 
the relevant electron-transport physics to derive equivalent analytic results. For 
the nFET synapse, we chose not to fit the region where V~ > V~ because, at 
such high drain voltages, the gate currents are too large for use in a practical 
learning system. For both synapses, then, 
~ 
~ 
~2 
I 9 = oIse-' vdc+----~,, 
(14.5) 
where Vdc is the drain-to-channel potential and 7, V~, and Vn are measurable 
device parameters. 
14.3.5 
The Gate-Current Equation 
Because the tunneling and injection gate currents flow in opposite directions, 
we obtain the final gate-current equation, for both synapses, by subtracting 
Eqn. 14.5 from Eqn. 14.2 : 
v~ 
_~ 
v~ 
~ 
~ 
Vox+Vbi 
Ig 
~(Vo~ + Vbi)2e - 
-- ~Ise ' ve~+v,' 
(14.6) 
The principal difference between the nFET and pFET synapses is the sign of 
the learning. In the nFET, tunneling increases the weight, whereas injection 
decreases it; in the pFET, tunneling decreases the weight, whereas injection 
increases it. 
14.3.6 
Impact Ionization 
We choose source current as the synapse output. Because, for both synapses, 
the activation energy for impact ionization is less than the barrier energy for 
injection, a drain-to-channel electric field that generates injection electrons also 
liberates additional electron-hole pairs [21]. For both synapses, the drain cur- 
rent therefore can exceed the source current. If we choose drain current, rather 
than source current, as the synapse output, we can rewrite the gate-current 
equation in terms of drain current using a (modified) lucky-electron [24] for- 
mulation: 
where Id is the drain current and Â¢, V~, and V~ are measurable device 
parameters. In Fig. 14.5, we plot impact-ionization data for both synapses. 

FLOATING-GATE MOS SYNAPSE 
325 
4 
nF'ET 
/~ 481 
' 
i[ 1"015 
ld 
- -  
4 -~-Vdc_~-.87 
] 
~ 
i 
I s 
-- 
~ 
I 
7270 
,~ 
~ 
~-=1+2.37x10 e 
FET 
~ 
~b 
=~ 
\ 
Zd 
~ - x / ~  
~ 
~ 
~ 
~]1.OlO $ 
~ 
~ 
T =1+5"%~10 e 
~ 
~ 
~ 
~ 
~ 
â¢ ~ ~. 
~.oo5 .~ 
~ 
~ 
~ 
~ 
~ 
~ 
1 ~ 
_ 
1.000 
2 
4 
6 
8 
10 
12 
drain-to-channel voltage (V) 
Figure 14.5 
Impact ionization versus drain-to-channel potential, for both the nFET and 
pFET synapses. Impact ionization in the ~FET is markedly more efficient than in the pFET, 
for two reasons. First, as a consequence of its bulk p-type substrate implant, the ~FET 
synapse experiences a higher drain-to-channel electric field than does the pFET, thereby 
increasing the ionization likelihood. Second, the impact-ionization process is naturally more 
efficient for electrons (the nFET charge carriers) than it is for holes (the pFET charge 
carriers). 
14.4 
SYNAPTIC ARRAYS 
A synaptic array, with a synapse transistor at each node, can form the basis of 
a silicon learning system. We fabricated simplified 2 Ã 2 arrays to investigate 
synapse isolation during tunneling and injection, and to measure the synapse 
weight-update rates. Because a 2 Ã 2 array uses the same row-column addressing 
employed by larger arrays, it allows us to characterize the synapse isolation and 
weight-update rules completely. 
14.4.1 
The ~FET Array 
The nFET array is shown in Fig. 14.6. 
We chose, from among the many 
possible ways of using the array, to select source current as the synapse output, 
and to turn off the synapses while tunneling. We applied the voltages shown 
in Table 14.1 to read, tunnel, or inject synapse {1, 1} selectively, while ideally 
leaving the other synapses unchanged. 
The tunneling and drain terminals of the array synapse transistors connect 
within rows, but not within columns. Consequently, the tunneling and injec- 
tion crosstalk between column synapses is negligible. A synapse's tunneling 
gate current increases exponentially with the oxide voltage Vox, (Vo=, in turn, 
decreases linearly with Vyg), and the hot-electron gate current increases lin- 
early with the channel current Is, (Is, in turn, increases exponentially with 

326 
NEUROMORPHIC SYSTEMS ENGINEERING 
column 1 
SotlrÂ£'e 
*x 
row 1 tunneling ~ 
row 1 drain )-- 
i
m
 
row 2 tunneling ~ 
row 2 drain )~ 
gate 
{2,1~ ! 
I 
I 
column 1 column 2 
SOLWCe 
column 2 
gate 
I 
Figure 14.6 
A 2 x 2 array of nFET synapses. Because the row synapses share common 
tunneling and drain wires, tunneling or injection at one row synapse can cause undesired 
tunneling or injection at other row synapses. 
Table 14.1 
The terminal voltages that we applied to the array of Fig. 14.6, to obtain the 
data of Figs. 14.7 and 14.8. 
col 1 
col I 
col 2 
col 2 
row 1 
row 1 
row 2 
row 2 
gate 
source 
gate 
source 
drain 
tun 
drain 
tun 
read 
+5 
0 
0 
0 
+1 
0 
0 
0 
tunnel 
0 
0 
+5 
0 
0 
+31 
0 
0 
inject 
+5 
0 
0 
0 
3.15 
0 
0 
0 
Vfg). 
Consequently, the isolation between row synapses increases exponen- 
tially with the voltage differential between their floating gates. By using 5V 
control-gate inputs, we achieve about a 4V differential between the floating 
gates of selected and deselected synapses; the resulting crosstalk between row 
synapses is < 0.01% for all operations. 
To obtain the data in Fig. 14.7, we initially set all four synapses to Is = 
100pA. We tunneled the {1, 1} synapse up to 100nA, and then injected it back 
down to 100pA, while measuring the source currents of the other three synapses. 
As expected, the row 2 synapses were unaffected by either the tunneling or the 
injection. Coupling to the {1, 2} synapse also was small. 
To obtain the data in Fig. 14.8, we first set all four synapses to Is = 100nA. 
We injected the {1, 1} synapse down to 100pA, and then tunneled it back up 
to 100nA. As in the experiment of Fig. 14.7, crosstalk to the other synapses 
was negligible. Our large (lpF) gate capacitors provide 80% voltage coupling 
between a synapse's control and floating gates, minimizing crosstalk at the 

FLOATING-GATE MOS SYNAPSE 
327 
167 
< 
"~ 10 -8 
g 
~9 ,~ ~= 16 9 
m 
10 -~0 
/ 
/ 
! 
/! 
' 
{ 1,1 } sy~pse 
11,2} synaps~ 
{2,1} and {2,2} synapses 
500 
10~00 
time (s) 
1500 
Figure 14.7 
Isolation in a 2 Ã 2 array of nFET synapses. Source current is the synapse 
output. The {i, i} synapse first is tunneled up to 100hA, then is injected back down 
to 100pA. The tunneling voltage, referenced to the substrate potential, is Vtun -- 31V; 
the injection voltage is Vds : 
3.15V. Crosstalk to the {I, 2} synapse, defined as the 
fractional change in the {i,2} synapse divided by the fractional change in the {i, i} 
synapse, is 0.006% during tunneling, and is 0.002% during injection. 
[i i : 
: 
: iii::2~iy{:,:} e synaps~s~ 
: 
~ 
10.8 
Â¸:Â¸::Â¸:::Â¸:Â¸::::Â¸Â¸:Â¸:Â¸ 
:Â¸ Â¸:Â¸:Â¸ Â¸Â¸Â¸:Â¸:Â¸:Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸Â¸:Â¸Â¸Â¸Â¸Â¸:Â¸ 
g 
~ 10 "9 
10 -~o 
0 
500 
1000 
1500 
time (s) 
Figure 14.8 
Results from the same experiment as in Fig. 14.7, but here the {i, 1} synapse 
first is injected down to 100pA, then is tunneled back up to 100hA. Crosstalk to the {I, 2} 
synapse is 0.001% during injection, and is 0.00:2% during tunneling. 
expense of increased size and decreased weight-update rates. We intend to 
fabricate future synapses with smaller gate capacitors. 

328 
NEUROMORPHIC SYSTEMS ENGINEERING 
14.4.2 
The pFET Array 
column l 
column 1 column 2 
column 2 
source 
gate 
source 
gate 
â¢ 
row 1 tunneling )~ ); 
row t drain }-- 
row 2 tunneling )--- 
~ 
row 2 drain )--' ~ 
I 
I 
I 
I 
.
.
.
.
 
{2,2~ ~ 
.... 
Figure 14.9 
A 2 Ã 2 array of pFET synapses. The well connections are not shown. As 
in the nFET array, because the row synapses share common tunneling and drain wires, 
tunneling or injection at one row synapse can cause undesired tunneling or injection at other 
row synapses. 
The pFET array is shown in Fig. 14.9. We grounded the p-type substrate, 
applied +12V to the n-type well, and referenced all terminal voltages to the 
well potential. 
Table 14.2 
The terminal voltages that we applied to the array of Fig. 14.9, to obtain the 
data of Figs. 14.10 and 14.11. 
col 1 
col 1 
col 2 
col 2 
row 1 
row 1 
row 2 
row 2 
gate 
source 
gate 
source 
drain 
tun 
drain 
tun 
read 
-5 
0 
0 
0 
-5 
0 
0 
0 
tunnel 
-5 
0 
0 
0 
-5 
+28 
0 
0 
inject 
-5 
0 
-4 
0 
-9.3 
0 
0 
0 
We again chose source current as the synapse output, but we left the pFET 
synapses turned on while tunneling, rather than turning them off like we did 
for the nFET array experiment. We applied the voltages shown in Table 14.2 
to read, tunnel, or inject synapse {1, 1} selectively, while ideally leaving the 
other synapses unchanged. 
To obtain the data in Fig. 14.10, we initially set all synapses to Is -- 100pA. 
We injected the {1, 1} synapse up to 100hA, and then tunneled it back down to 
100pA. To obtain the data in Fig. 14.11, we performed the opposite experiment. 

FLOATING-GATE MOS SYNAPSE 
329 
10 -7 
< 
'~o 10-8 
~- 
~ 10 -9 
10 -~o t
J
 
~ 
I 
I 
I 
I 
500 
1000 
1500 
2000 
time (s) 
, 
i 
i 
i 
/ 
y 
{ 1,2} synapse 
12,1} and {2,2} synapses 
Figure 14.10 
Isolation in a 2 Ã 2 array of pFET synapses. Source current is the synapse 
output. The {1, i} synapse first is injected up to 100hA, then is tunneled back down to 
100pA. The injection voltage is Vds ---- -9.3V; the tunneling voltage, referenced to the 
well potential, is Vtun = 28V. Crosstalk to the {1, 2} synapse, defined as the fractional 
change in the {I, 2} synapse divided by the fractional change in the {1, I} synapse, is 
0.016% during injection, and is 0.007% during tunneling. 
< 
~ 10.~ 
~ ~ ~ 10 .9 
10 -~o 
i 
i 
i 
~ 
....... 
{2,1} and {2~} Synapses 
) 
{ i'2} ;ynapse 
I 
500 
1000 
1500 
2000 
time (s) 
Figure 14.11 
Results from the same experiment as in Fig. 14.10, but here the {1, 1} 
synapse first is tunneled down to 100pA, then is injected back up to 100~tA. Crosstalk to 
the {i, 2} synapse is 0.005% during injection, and is 0.004% during tunneling. 
For the pFET array, like for the nFET array, the crosstalk between column 
synapses was negligible, and the crosstalk between row synapses was small. 

330 
NEUROMORPHIC SYSTEMS ENGINEERING 
When we injected the {1, 1} synapse, we applied -4V, rather than 0V, to 
the (1,2} synapse's control gate. 
We did so because hot-electron injection 
can occur in a pFET synapse by a mechanism different from that described 
in Section 14.3: If the floating-gate voltage exceeds the well voltage, and the 
drain-to-channel potential is large, electrons can inject onto the floating gate 
by means of a non-destructive avalanche-breakdown phenomenon [23] at the 
MOS surface. 
14.5 
THE SYNAPSE WEIGHT-UPDATE RULE 
10 -9 
~10 -~ 
~ 
-13 
.~ 
~ 
-15 
e~ 
' 
= 
- 
â¢ 
. 
10 "17 
.
.
.
.
.
.
.
.
 
~ 
.
.
.
.
.
.
.
.
 
~ 
.
.
.
.
.
.
 
-I0 
-9 
-8 
' 
" 
-7 
lO 
lO 
lO 
lO 
source current (A) 
Figure 14.12 
The magnitude of the temporal derivative of the source current versus the 
source current, for an nFET synapse with a continuous tunneling-oxide current. We tunneled 
the {I, I} synapse up as in Fig. 14.7, with the source at ground and the ground-referenced 
tunneling voltage stepped from 29V to 35V in 1V increments. The mean slope is +0.83. 
We repeated the experiments of Figs. 14.7 and 14.10, for several tunneling 
and injection voltages; in Figs. 14.12 through 14.15 we plot, for the nFET and 
pFET synapses, the magnitude of the temporal derivative of the source current 
versus the source current. We held the control-gate input V~n fixed during these 
experiments; consequently, the data show the synapse weight updates 5W/St, 
as can be seen by differentiating Eqn. 14.1. Starting from the gate-current 
equation, Eqn. 14.6, we now derive weight-update rules that fit these data. 
14.5.1 
Tunneling 
We begin by taking the temporal derivative of the synapse weight W, where 
W = exp(Qfg/QT): 
5W_ 
W 5QI 9 _ W i~ 
(14.7) 
5t 
QT 
QT-- 

FLOATING-GATE MOS SYNAPSE 
331 
10_91 
........ 
! 
........ 
: 
. . . .  
C 
t 
~ I0-' 
...... 
< 
~.~ 
.~ 
~= 10_ ~ 
10 17 
.
.
.
.
.
.
.
.
.
 
_ 
1o-" 
.... 
...... ~
'
~
~
 
~ i0 -'3 
...... 
~ 
-10 
-9 
-8 
-7 
10 
10 
10 
10 
source current (A) 
Figure 14.13 
The magnitude of the temporal derivative of the source current versus the 
source current, for an nFET synapse with a continuous hot-electron oxide current. We 
injected the {111} synapse down as in Fig. 14.7, with the source at ground and the ground- 
referenced drain voltage stepped from 2.9V to 3.5V in 0.1V increments. The mean slope 
is -1.76; we have added the minus sign because the synapse weight is injecting down. 
10 .9 
~ 10 -~ 
~= 
~10 ~ 
~ 10-~5 
=~ 
10 "17 
,, ~ 
, ~ 
-I0 
.
.
.
.
.
.
 
-9 
.
.
.
.
.
.
.
 
-8 
.
.
.
.
.
.
.
 
'-7 
I0 
I0 
I0 
lO 
source current (A) 
Figure 14.14 
The magnitude of the temporal derivative of the source current versus 
the source current, for a pFET synapse with a continuous tunneling-oxide current. We 
tunneled the {1, 1} synapse down as in Fig. 14.10, with the source and well at -I-12V and 
the tunneling voltage, referenced to the well potential, stepped from 26V to 32V in IV 
increments. The mean slope is -0.99; we have added the minus sign because the synapse 
weight is tunneling down. 

o 
lO -9 
10 "~ 
: 
....... 
; .... 
â¢ 
....... 
10 "13 
10 -15 
10.17 
' 
10 
332 
NEUROMORPHIC SYSTEMS ENGINEERING 
-10 
-9 
-8 
-7 
l0 
l0 
l0 
source current (A) 
Figure 14.15 
The magnitude of the temporal derivative of the source current versus the 
source current, for a pFET synapse with a continuous hot-electron oxide current. 
We 
injected the {1, 1} synapse up as in Fig. 14.10, with the source and well at +I2V 
and 
the drain voltage, referenced to the source potential, stepped from -8.0V to -11.0V in 
-0.5V increments. The mean slope is +1.89. 
In Appendix A.1, we substitute for the tunneling gate current using 
Eqn. 14.3, and solve for the tunneling weight-update rule: 
5W 
1 
-- 
~ 
W (1-~) 
(14.8) 
5t 
"rtun 
where a and ~-tun are defined in Eqns. 14.A.3 and 14.A.4, respectively. Equa- 
tion 14.8 fits the tunneling weight-update data for both synapses. In the nFET 
synapse, 0.12 < a < 0.22; in the pFET, 0.01 < a < 0.05. 
14.5.2 Injection 
We begin with 5W/fit from Eqn. 14.7. In Appendix A.2, we substitute for the 
injection gate current using Eqn. 14.5, and solve for the injection weight-update 
rule: 
5W _ 
~1 W(2_~) 
(14.9) 
5t 
Ti~y 
where Â¢ and ~-~,~j are defined in Eqns. 14.A.8 and 14.A.9, respectively. Equa- 
tion 14.9 fits the injection weight-update data for both synapses. In the nFET 
synapse, 0.14 < Â¢ < 0.28; in the pFET, 0.08 < ~ < 0.14. 

FLOATING-GATE MOS SYNAPSE 
333 
14.5.3 The Weight-Update Rule 
We obtain the synapse weight-update rule by adding Eqns. 14.8 and 14.9, with 
a leading (Â±) added because the sign of the updates is different in the nFET 
and pFET synapses: 
(~W_~ "~Â± [__1 w(l_a) - 1 W(2-~)] 
(14.10) 
[ Ttun 
Tinj 
For nFET synapses, we use the (+) in Eqn. 14.10; for pFET synapses, we 
use the (-). 
14.5.4 Learning-Rate Degradation 
SiO2 trapping is a well-known issue in floating-gate transistor reliability [3]. In 
digital EEPROMs, it ultimately limits the transistor life. In the synapses, trap- 
ping decreases the weight-update rate. However, because a synapse's weight W 
is exponential in its floating-gate charge Qfg (see Eqn. 14.1), the synapses in a 
subthreshold-MOS learning system will transport only small quantities of total 
oxide charge over the system lifetime. We tunneled and injected lnC of gate 
charge in both synapses, and measured a ~ 20% drop in both the tunneling and 
injection weight-update rates. Because lnC of charge represents an enormous 
change in synapse weight, we believe that oxide trapping can be ignored safely. 
14.6 
CONCLUSION 
We have described complementary single-transistor silicon synapses with non- 
volatile analog memory, simultaneous memory reading and writing, and bidirec- 
tional memory updates that are a function of both the applied terminal voltages 
and the present synapse output. We have fabricated two-dimensional synaptic 
arrays, and have shown that we can address individual array nodes with good 
selectivity. We have derived a synapse weight-update rule, and believe that we 
can build autonomous learning systems, that combine single-transistor analog 
computation with weight updates computed both locally and in parallel, with 
these devices. Finally, we anticipate that our single-transistor synapses will 
allow the development of dense, low-power, silicon learning systems. 
Appendix: A 
A.1 
THE TUNNELING WEIGHT-UPDATE RULE 
We begin with the temporal derivative of the synapse weight W (see Eqn. 14.7): 
5W 
W 
- 
QTIq_ 
(14.A.1) 
6t 
We substitute Eqn. 14.3 for the gate current Ig: 

+ o, + 
:~ pue ~I Jo 
smaaa u! ~ iv!auaaod aa~jans art1 aoj aNos ah~ '~'V'IrI pu~ I~'H 'suba Su!SFl 
(~'V'H) 
~ 
aÂ°I = ~I 
Â£q [~I] so~IoA oaanos pu~ oa~$-$u!a~Olj oqa oa poa~Iaa s~. auaaana 
aaanos aqa 'Â±adgOI~ oav$-$u!a~OlJ ploqsaaqlq ns v u I "~I pu~ ~vA ~o stuaoa m 
'~PA 'IW.~uaa Â°d Iouuvqa-ol-u!eaP s,aoas~.su~aa osdvuKs ~ Su!a!a~aa Â£q utflaq a3A 
31n~1 3.I.VCldn-.I.H913~ NOI.I.:)3rNI 3H.I. 
~:'V 
unaa, 
gp 
-
-
 
~ 
-
-
 
(~-t)M I 
M9 
:olna oa~pdn 
-aq$.to~ Su!ioutln a oqa ao$ oa 'UV'H "ub~t olut. um.~ oanat.lsqns oa,, 'Xli~U~.~ I 
(~v>O 
"+2"" ~: -(~'~A - *~A + ~)7~ 
-- ~'~" 
ougop pu~ 'M 
jo auopuodopu! 'lu~asuoa ~ oq ol ~(etA - ~A + ~*'~91) oa~mt.xoadd~ am 'XlmOIS 
soSu~qa aSValOa oa~$-$ut.lVOlJ at D 'sauaaana oaanos ploqsoaqaqns aoj 'osnvaa~ 
(e'v'>I) 
~(~A + ~)~ 
_= ~ 
~;2OA 
o~oq~ 
(sv>I) 
(~_~)~ ~+~o,~ ~:('~ - *~A + ~)T 
~ 
.x- 
M9 
:olna 
oaupdn-aq$!o~ Su!iouuna oq~ aoj OalOS pu~ '~Ox/etO* O = ~A olna~asqns o~ 
~ 
, ~0 
~9 
C~a+'"a) 
,~a+~,.aa(~fA - ~A + 
n;~ 
~ 
~AÂ°A 
-- 
Â°a- 
M9 
:0AIOS ptI~ 'X + I ~ i-( x -- I) Xq ~uouodxo oq:l 
puvdxo '~c A << ~qA + un~A ~L[~. omnss~ '(.Â£[oA!~odsoa 'so~:~IOA o~-~ti.l:l~Olj pu~ 
opou-~u!IOUUm otI~ oa~ ~'A pu~ ~'~9i oJoqA~) ~gA - ~A 
-- ~Â°A o:m~!~sqns o3A 
.o., ~0 
~ 
,~.,+.o., ~(~A + 
- 
~ 
A)~ 
M9 
DNI~IH~INIDN~I SI~I~&SXS DIHd]:IO!A!O]:IfERN 
17gg 

FLOATING-GATE MOS SYNAPSE 
335 
We now solve for V~: 
Vac = Va - ~ = Vds - ~o - Ut ln ( I~o ) 
(14.A.6) 
The injection gate current Ig is given by Eqn. 14.5. We add a minus sign 
to the gate current, because hot-electron injection decreases the floating-gate 
charge, and substitute for Vac using Eqn. 14.A.6: 
( 
v~ 
T 
-- 
I~ = -~1~ 
~"~Ã·~-~;---~t'n(#Â°) 
V 
2 
~ 
u~ 
-~ 
:__,is~--(Vd~+Vv-'o) 
[1-- vd~+wv-'o ln(~)] 
We expand the exponent by (1 - x) -~ ~ 1 + 2x, substitute for I~ using 
Eqn. 14.1, and solve: 
[ 
( 
(l~e)~t 
V'n 
-~ 
I, ~ -,Ioe[ ~
-
 
' ~+~'-*Â°) ~] W (1-~) 
(14.1.7) 
where 
~ 
(vds + v~ - %)3 
We substitute Eqn. 14.A.7 into 6Wilt, Eqn. 14.A.1, 
( 
5W 
~71o I ~ -  
v~+v,~-~o 
~- 
~ 
w (~-~) 
We define 
(14.A.8) 
[( 
QT e 
,~4-~,-~o 
- 
vt 
(14.A.9) 
_ 
_
_
 
Tiny ~ 7]Io 
Finally, we substitute Tiny into Eqn. 14.A.9 to get the injection weight-update 
rule: 
6W 
1 
_ 
_ _  
W(2-~) 
6t 
Tiny 
Acknowledgments 
Lyn Dupr4 edited the manuscript. This work was supported by the Office of Naval 
Research, by the Advanced Research Projects Agency, by the Beckman Hearing Insti- 
tute, by the Center for Neuromorphic Systems Engineering as a part of the National 
Science Foundation Engineering Research Center Program, and by the California 
Trade and Commerce Agency, Office of Strategic Technology. 

336 
NEUROMORPHIC SYSTEMS ENGINEERING 
References 
[1] T. Allen, A. Greenblatt, C. Mead, and J. Anderson. Writeable analog 
reference voltage storage device. U.S. Patent No. 5,166,562, 1991. 
[2] A. G. Andreou and K. A. Boahen. Analog VLSI signal and information 
processing. In M. Ismail and T. Fiez, editors, Neural information process- 
ing II, pages 358-413. McGraw-Hill, New York, 1994. 
[3] S. Aritome, R. Shirota, G. Hemink, T. Endoh, and F. Masuoka. Reliability 
issues of flash memory cells. In Proc. of the IEEE, volume 82-5, pages 776- 
787, 1993. 
[4] P. Churchland and T. Sejnowski. The Computational Brain. MIT Press, 
1993. 
[5] C. Diorio, P. Hasler, B. A. Minch, and C. Mead. A high-resolution non- 
volatile analog memory cell. In Proc. IEEE Intl. Syrup. on Circuits and 
Systems, volume 3, pages 2233-2236, 1995. 
[6] C. Diorio, P. Hasler, B. A. Minch, and C. Mead. A single transistor sil- 
icon MOS device for long term learning. 
U.S. Patent Office serial no. 
08/399966, Mar. 7 1995. 
[7] C. Diorio, P. Hasler, B. A. Minch, and C. Mead. A single-transistor silicon 
synapse. IEEE Trans. Electron Devices, 43(11):1972 1980, 1996. 
[8] C. C. Enz, F. Krummenacher, and E. A. Vittoz. 
An analytical MOS 
transistor model valid in all regions of operation and dedicated to low- 
voltage and low-current applications. Analog Integrated Circuits and Signal 
Processing, 8:83-114, 1995. 
[9] A. S. Grove. Physics and Technology of Semiconductor Devices. 
John 
Wiley & Sons, New York, 1967. 
[10] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses. In Advances in Neural Information Processing Systems 7, pages 
817-824. MIT Press, Cambridge, MA, 1995. 
[11] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses with long term storage. In IEEE Intl. Syrup. on Circuits and 
Systems, volume 3, pages 1660-1663, 1995. 
[12] P. Hasler, B. A. Minch, C. Diorio, and C. Mead. An autozeroing amplifier 
using pfet hot-electron injection. In Proc. IEEE Intl. Syrup. on Circuits 
and Systems, volume 3, pages 325-328, Atlanta, May 1996. 
[13] B. Hochet, V. Peiris, S. Abdo, and M. J. Declercq. Implementation of 
a learning kohonen neuron based on a new multilevel storage technique. 
IEEE J. Solid-State Circuits, 26(3):262-267, 1991. 
[14] P. Hollis and J. Paulos. A neural network learning algorithm tailored for 
VLSI implementation. IEEE Tran. Neural Networks, 5(5):784-791, 1994. 
[15] J. Lazzaro, J. Wawrzynek, , and A. Kramer. Systems technologies for 
silicon auditory models. IEEE Micro, 14(3):7 15, June 1994. 

FLOATING-GATE MOS SYNAPSE 
337 
[16] M. Lenzlinger and E. H. Snow. Fowler-nordheim tunneling into thermally 
grown sio2. J. of Appl. Phys., 40(6):278-283, 1996. 
[17] F. Masuoka, R. Shirota, and K. Sakui. Reviews and prospects of non- 
volatile semiconductor memories. IEICE Trans., E 74(4):868-874, 1991. 
[18] C. Mead. Scaling of MOS technology to submicrometer feature sizes. J. 
of VLSI Signal Processing, 8:9-25, 1994. 
[19] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
[20] J. J. Sanchez and T. A. DeMassa. 
Review of carrier injection in the 
silicon/silicon-dioxide system. In IEE Proceedings-G, volume 138-3, pages 
377 389, 1991. 
[21] W. Shockley. Problems related to pn junctions in silicon. 
Solid-State 
Electronics, 2(1):35-67, 1961. 
[22] S. M. Sze. Physics of Semiconductor Devices. John Wiley & Sons, New 
York, 1981. 
[23] E. Takeda, C. Yang, and A. Miura-Hamada. Hot-Carrier Effects in MOS 
Devices. Academic Press, San Diego, CA, 1995. 
[24] S. Tam, P. Ko, and C. Hu. Lucky-electron model of channel hot-electron 
injection in MOSFET's. IEEE Trans. Electron Devices, ED-31(9):1116- 
1125, 1984. 

15 
NEUROMORPHIC SYNAPSES 
FOR ARTIFICIAL DENDRITES 
Wayne C. Westerman, David P. M. Northmore, and John. G. Elias 
Departments of Electrical Engineering and Psychology 
University of Delaware Newark, DE 19716 
15.1 
INTRODUCTION 
In the past few years several researchers have developed general-purpose spiking 
silicon neurons, or neuromorphs, which attempt to capture various functional 
behaviors of biological neurons [19, 23, 24]. These CMOS analog VLSI im- 
plementations are intended to be the computational elements linking sensory 
input to motor output in artificial nervous systems. Systems consisting of 
thousands or millions of neuromorphs may be required to realize certain basic 
behaviors. However, until appropriate methods are found to specify and adapt 
network parameters (e.g., connectivity, synapse weights, dynamics, morphol- 
ogy), these systems will remain beyond our grasp. Though various Hebbian 
or correlative adaptation rules have been applied to perceptron based net- 
works (e.g., [77, 11, 141]) and single-compartment model neurons [12, 26], these 
rules are virtually untested with multi-compartment spiking neuromorphs that 
model the spatially extensive dendrites found in biological systems. This paper 
discusses the VLSI design and performance of just one of the adaptive net- 
work parameters: variable synaptic conductances that mimic the fast chemical 
synapses found on dendrites throughout high-order nervous systems. 

340 
NEUROMORPHIC SYSTEMS ENGINEERING 
This paper is organized as follows. First, we present an overview of biological 
synaptic function and discuss those aspects that need to be carefully mimicked 
by artificial synapses. Second, we discuss existing synapse designs for artificial 
VLSI neural network chips and consider implications of the time-multiplexed 
inter-chip communication schemes that have recently been adopted for neuro- 
morphic networks. Third, we evaluate approximations that can simplify neu- 
romorphic, variable weight synapse designs within artificial dendrites. Finally, 
we present results from two complementary implementations, the conductance 
array and the self-timed synapse, and compare the engineering efficiency, mod- 
eling accuracy and on-chip adaptation potential of each. 
We will use the following conventions to distinguish the terms synaptic 
weight and synaptic efficacy. Synaptic weight is the value of the index or con- 
trol signal that modulates artificial synapse conductance or duration. When a 
synapse is activated by a pre-synaptic spike its conductance changes from zero 
to a value set by its weight. The synapse remains activated for a time period 
called the duration. The synaptic efficacy is the total charge transferred by 
an activated synapse, and for conductance synapses is a function of both the 
membrane potential just before activation and the integral of the conductance 
over the duration. 
15.2 
MODELING SYNAPSES AS SWITCHABLE CONDUCTANCES 
Though current sources are easy to build in CMOS technology, they are in- 
appropriate models for the fast chemical synapses found in central nervous 
systems [36]. The following discussion of biological synaptic function will show 
that synaptic activations are more properly modeled as modulations in conduc- 
tance, and thus, charge flow will depend upon the product of the conductance 
and the difference between the channel reversal potential and the intracellular 
potential. When discussing artificial synapses we will use synapse supply po- 
tential as the analog to the biological channel reversal potential. For reviews 
of general synaptic function, see [21, 30, 33, 34]. 
When an axonal spike reaches a pre-synaptic terminal, a complex chemical 
cascade causes packets of neurotransmitter to be released across the correspond- 
ing synaptic cleft, where they briefly bind to receptor molecules embedded in 
the post-synaptic membrane. The transmitter-receptor complex produces a 
channel that permits small ions to enter and exit the post-synaptic cell, thus 
modulating the intracellular potential. In the case of fast synapses the channel 
closes after about a millisecond and the transmitter molecule detaches from the 
receptor. The collective channel openings can be considered a temporary, local 
increase in membrane conductance with driving potential set by the difference 
between the reversal potential of the ion species passed by the channels and the 
intracellular potential. Note that unlike their voltage-sensitive cousins, these 
ligand-sensitive channels closely follow Ohm's law [30]. 
Because the current through a channel is proportional to the difference be- 
tween the local membrane potential and the reversal potential, the change in 

NEUROMORPHIC 
SYNAPSES 
341 
membrane voltage due to synapse activation will be smaller if the membrane 
voltage is already near the reversal potential. For synapses located on dendrites, 
the charge contributed by each activation will depend on the recent activity of 
neighboring synapses, an effect known as sublinear summation [71, 72, 80]. If 
synaptic activations instead caused current injections which were independent 
of membrane voltage, dendrites would still spatio-temporally filter the charge 
injections to form extended post-synaptic potentials (PSPs) at the soma, but 
the filtered PSPs from distinct synaptic events would add together linearly [87]. 
While sublinear summation cannot amplify signals and thus is not as strong 
a nonlinearity as those produced by active channels [60], it still provides a pow- 
erful spatio-temporal computation mechanism for passive dendrites. Koch and 
Poggio [37, 38] have shown sublinear summation can be used for crude multi- 
plication, which is necessary for motion perception. Korn and Mallet [40] have 
suggested it dampens synaptic noise. We have already demonstrated sublin- 
ear summation on artificial dendrites with fixed-weight synaptic conductances, 
where it has proven useful for tasks such as spike train coincidence detection and 
frequency selection [69]. Ensuring that the variable weight synapses presented 
in this paper behave as conductances has thus been a top design priority. 
The most basic function of dendrites is to transfer the charge injected at 
spatially distributed synapses toward the soma, or cell body, where that charge 
can influence the neuron's output firing. In passive dendrites this transfer is 
not a wave-like propagation toward the soma, but rather a slightly asymmet- 
ric spread away from the synapse into all connecting branches. The PSP as 
typically measured by neurophysiologists with electrodes at the soma therefore 
appears severely attenuated and temporally extended compared to the origi- 
nal event at the synapse. Neuroscientists wishing to avoid the computational 
burden of simulating dendrites often use alpha-functions [73] to model the post- 
synaptic current. But in analog VLSI, it is straightforward to build artificial 
dendrites which filter synaptic events using diffusive mechanisms similar to bio- 
logical dendrites. Without artificial dendrites, each synapse circuit would need 
to generate a time-varying current that matched an alpha-function's smooth 
waveform. This would require significantly more complex synapse circuitry 
than that presented in this paper, as well as prevent local interactions between 
synapses [69] and the variety in PSP shape that normally results from differing 
synapse positions [23, 24]. 
15.3 
SYNAPTIC VARIATION 
Understanding stochastic, short-term, and long-term variations in transmission 
at individual synapses is important for designing neuromorphic systems which 
adapt according to biologically plausible rules. The neuromorphic synapses de- 
scribed in this paper do not yet include on-chip adaptation circuitry. Therefore, 
we will restrict the following discussion to the physical consequences of synap- 
tic modification, disregarding the conditions which may lead to potentiation or 
depression. Our current research involves "chip in the loop" experimentation 

342 
NEUROMORPHIC SYSTEMS ENGINEERING 
with adaptation rules (e.g., Hebbian) implemented in software. The results 
of these experiments should provide direction towards designing autonomously 
adapting silicon neurons. 
Long-term potentiation (LTP) in the hippocampus has been studied exten- 
sively (e.g., [7, 16, 22, 57]). It appears to be an example of associative or 
Hebbian learning, in which synaptic weights are increased when synaptic ac- 
tivation coincides with existing depolarization of the post-synaptic membrane. 
Changes in synaptic weight may result from pre-synaptic changes in the amount 
of transmitter released, an increase in the number of post-synaptic channels, 
conformational changes in channel proteins which increase their conductance 
or de-activation times, or a change in transmitter reuptake [3, 35, 50]. At this 
time it is unclear which of these mechanisms is primarily responsible for LTP, 
and some evidence suggests that both pre- and post-synaptic changes occur 
under differing conditions [41, 42]. Most of these mechanisms can be mimiced 
in artificial systems by varying either the synaptic conductance or the duration. 
Most synapses also display at least one of many types of short-term plas- 
ticity which may be associative or may depend only on the recency of past 
activations [21]. Such effects have been shown to mediate habituation and sen- 
sitization of reflexes in simple invertebrates such as Aplysia [34]. They also 
appear in some LTP experiments [9], but their role in the function of higher 
level nervous systems is not clear. Should short-term plasticity prove useful for 
artificial systems, it can be implemented with circuitry that combines a leaky 
integral of synaptic activity with a long-term conductance control signal. 
In addition to deterministic changes in synaptic efficacies, many central ner- 
vous system (CNS) synapses exhibit stochastic, quantized fluctuations in effi- 
cacy from activation to activation. This is because CNS pre-synaptic terminals 
typically have a few or just one active site where a transmitter packet can be 
released, and release from each site is probabilistic [39]. Therefore the collec- 
tive channel conductance and hence the magnitude of successive PSPs varies 
in discrete steps according to the number of packets actually released. Also, 
presynaptic depolarization does not always result in release of any transmit- 
ter [29], and packets can be released spontaneously, causing miniature excita- 
tory synaptic currents [65] which may be as large as the current from a normal 
activation [70]. Many researchers are trying to determine whether the locus 
of LTP is primarily pre- or post-synaptic by examining whether LTP shifts 
the PSP magnitude quantization levels themselves or the distribution of PSPs 
among fixed levels (e.g., [8, 46, 56]). 
Besides what it can reveal about the location of synaptic modifications, the 
apparently unreliable, quantal nature of biological synaptic transmission has 
several implications for robust artificial system design. Though axons transmit 
spikes fairly reliably [49], individual synapses do not generate PSPs with con- 
sistent magnitudes [2]. Therefore, if a neuromorphic network adapts and com- 
putes with mechanisms sufficiently similar to those in the CNS, it should still 
function with unreliable or noisy synapses. Similarly, such a network should 
not fail if the interchip comInunication system randomly loses some of the 

NEUROMORPHIC SYNAPSES 
343 
synaptic activation events corresponding to each source spike. Note, however, 
that the biological analogy does not justify losing spikes at their sources, which 
is equivalent to the much rarer failure of action potential propagation at the 
soma. Whether artificial synapses are reliable or not, any adaptation algorithm 
touted to reflect biology should be able to tolerate synaptic noise. Adaptation 
in the CNS could conceivably depend on this noise [83] yet require precision in 
the mean synaptic weight [66]. If so it may be necessary to purposely generate 
similar unreliability in silicon by superimposing quantal noise on conductance 
or duration control signals. To model changes in synaptic weight which are 
a result of reliability rather than potency modifications, the characteristics of 
the noise rather than the weights themselves would need to be individually 
modifiable. 
Research on LTP mechanisms may also reveal to what degree neighboring 
biological synapses adapt independently. Random deviations in the number of 
post-synaptic channels and pre-synaptic active sites ensures that the efficacies 
of synapses connected to a length of dendrite will be different, but this does 
not imply that they adapt independently. If the second messengers (e.g., intra- 
cellular free calcium) which control changes in efficacy diffuse over fairly large 
areas [54, 55, 65], the weights of all synapses in a dendritic segment with the 
same type of ligand-sensitive channel could be modified in unison. If, on the 
other hand, these changes are governed by reactions within the pre-synaptic 
termini or confined to dendritic spines [91], they could be fully independent. 
Until this issue is resolved we shall take the view that connections to the 
same dendritic compartment from different neuromorphs should have individ- 
ually adjustable weights. Therefore, in our artificial dendrites each compart- 
ment is meant to represent an isopotential length of dendritic branch containing 
many synapses, not just the membrane underlying a single pre-synaptic termi- 
nal. Time multiplexing of synaptic activations allows the synaptic circuitry of 
a single compartment to emulate the tens or hundreds of synapses that might 
be found on an equivalent isopotential segment of biological dendrite. If bi- 
ological synapses do adapt independently but all connections to an artificial 
compartment are forced to use the same locally stored weight, the computa- 
tional and adaptive abilities of the resulting neuromorphic network could be 
relatively limited. 
15.4 
SYNAPSE DESIGNS FOR MULTI-LAYER PERCEPTRON 
NETWORKS 
Many VLSI implementations of multi-layer perceptron networks have been built 
since backpropagation [21] and related supervised learning algorithms appeared 
in the mid 1980's. Synapses in perceptron networks multiply an input signal 
(typically an output of another perceptron unit) by a constant: the synaptic 
weight. Synapse outputs are added together and passed through a non-linear 
transfer function. This high-level model of biological synaptic function is not 
equivalent to dendritic membrane conductances activated by spikes [79]. 

344 
NEUROMORPHIC SYSTEMS ENGINEERING 
Perceptron synapse designs diverge according to whether on-chip computa- 
tion and communication are primarily digital [31] or analog [27]. Among purely 
analog implementations, synapses typically consist of Gilbert or transconduc- 
tance multipliers [173, 2o], with the weights stored locally on floating gates [45] 
or dynamically refreshed capacitors (e.g., [77, 97]). The outputs of each synapse 
are generally currents tied to a common node (e.g. [64]) in order to form a linear 
sum of the synaptic contributions within the constraints of MOSFET technol- 
ogy. In these designs weights must be present continuously at each synapse, 
prompting interest in programmable, non- volatile on-chip storage devices such 
as floating gates (e.g., [13, 28, 55]). 
Pulse stream networks are an interesting alternative in which synap- 
tic multiplication can be implemented with analog, digital, or hybrid tech- 
niques [62, 67, 92]. These include pulse width modulation with digital weight 
shift registers and pulse amplitude scaling with transconductance multipli- 
ers [68]. 
15.5 
SPIKING SILICON NEURONS 
Since the appearance of Mead's seminal book in 1989 [18], several research 
groups have developed silicon neurons and networks more closely based on 
known principles of biological circuitry. Artificial sensory transducers such as 
silicon retinas (e.g., [1, 182, 59]) and cochleas (e.g., [43, 90]) constitute the 
brunt of this work. A few researchers have also built general-purpose neurons 
modeled upon various aspects of biological interneurons. For instance, Dou- 
glas and Mahowald [19, 52] have built single compartment neuromorphs with 
voltage-sensitive channels that produce action potentials, and Linares-Barranco 
et al. [47] have implemented the Fitzhugh-Nagumo soma model in CMOS. Our 
work has focused on constructing neuromorphs having a large number of com- 
partments (Figure 15.1b) that form artificial dendrites (Figure 15.1a) coupled 
to a leaky integrate-and-fire soma model [23]. So far we have concentrated 
our efforts on passive dendrites which lack the strong nonlinearities of voltage- 
sensitive channels [60]. 
The common thread among these efforts is the type of communication be- 
tween sensory input and interneurons. Many of these silicon neuromorphs out- 
put digital spikes or address events which can be passed to other neuromorphs 
over time-multiplexed asynchronous [63] or synchronous communication chan- 
nels. Neuromorph output is coded in the frequency and timing of their spikes 
rather than a multi-valued digital or analog code [18]. While the Address Event 
protocol [51] allows simultaneous activation of synapses, placing source address 
decoders at each synapse consumes lots of space and prevents a synapse from 
receiving events from multiple sources. Virtual Wires (Figure 15.2, [23]) adds a 
connection list to translate from source address space to synapse address space. 
Simple row and column synapse address decoders can then service all synapses 
on a chip. The main advantages of Virtual Wires is that, with the aid of a 
connection list, connections from any source to any destination can be quickly 

NEUROMORPHIC SYNAPSES 
345 
Ca) 
Dendritic Tree 
Threshold Setting Synapses 
,,,,,,,,,,,,,,,,,,, 
1 
~ 
Soma 
IIIIII 
[ 
I 
(b) 
o2 
Gi 
G+ 
> Rrn 
-Cm 
6 
Vresl 
Vexcitatory 
.-~ 
I R m 
Cm 
Vinhibilory 
GND 
Figure 15.1 
(a) Silicon neuron with dendritic tree. Synapses exist on the dendrites at the 
cross points and at the soma. Activating soma synapses sets the spike firing threshold for the 
integrate-and-fire soma. The integration time constant is determined by a programmable 
resistor, 1~, and a fixed capacitor, (7. The integration capacitor is discharged whenever 
a spike is generated by the soma. 
(b) A two-compartment section of dendrite. 
Each 
compartment contains a storage capacitor (Gin) representing the membrane capacitance 
of an isopotential length of dendrite and a set of conductance paths from the capacitor 
to various potentials. The switched capacitor membrane (Rm) and axial (}~a) resistors, 
connect the compartment to a resting potential and adjacent compartments. The excitatory 
(Ge) and inhibitory (Gi) synaptic conductances, which turn on momentarily when a synapse 
is activated, pull the compartment capacitor voltage towards their respective synapse supply 
potentials. 
configured with arbitrary source fanout, arbitrary delays, and supplemental in- 
formation such as weights. Since the diffusive and integrative time constants 
of the underlying analog dendrites and somas are longer than the time to se- 
rially service thousands of connections, analog computations still take place in 
parallel. 
In our system, synapses charge or discharge a compartment capacitor only 
when activated, instead of continuously multiplying an input signal by the 

346 
NEUROMORPHIC SYSTEMS ENGINEERING 
S~keOu~u, 
Compa~ma] 
V~h~g~s 
Figure 15.2 
Virtual wires interchip communication system emulates the fanout and trans- 
mission delays of biological axons. When a neuromorph fires, its address references a block 
of connections in the digital connection list memory. Each connection consists of a synapse 
address, weight, and delay. If the delay is non-zero the state machine will place a connection 
in the delayed connection memory for later activation. Otherwise it will successively activate 
each synapse specified in the connection list memory. The fanout and temporal precision 
are limited only by the bandwidth of the synapse bus and the combined neuromorph firing 
rates. 
synaptic weight. Because the synapse address must appear on the synapse 
bus every activation, a digital synaptic weight can easily be delivered along 
with the synapse address. If the synapse needs to remain on for an extended 
period, the weight can be converted to an analog voltage and deposited on a 
local synapse capacitor at the time of activation. Assuming the synapse always 
turns off before this weight voltage decays significantly, the complexities of 
dynamic refresh circuitry or permanent storage are unnecessary. In addition 
to reducing silicon real estate requirements, storing weights off-chip with other 
connection information allows the weights of connections made from different 
neuromorphs to the same compartment to be independent. 
15.6 
METHODS FOR VARIABLE-WEIGHT SYNAPSE DESIGN IN 
ARTIFICIAL DENDRITES 
Building compact, wide-range variable synaptic conductances in CMOS is dif- 
ficult because MOSFETs behave as voltage-controlled current sources when 
their drain-source voltage exceeds the gate voltage less the threshold voltage. 
The horizontal resistor (HRes) of Mead [18] works well for image segmenta- 
tion in resistive networks of artificial retinas but only has a linear range of 
100inV. Several researchers trying to build integrated active filters have used 
multiple MOSFETs to emulate voltage-controlled resistors (e.g., [1, 5, 61, 89]), 

NEUROMORPHIC SYNAPSES 
347 
but most have a limited operating range of about 2V. Shibata and Ohmi [81] 
report a fairly compact variable resistor with moderate linearity over a 5V 
range and suitability for resistances in the kft range. Switched capacitor tech- 
niques are very effective as globally variable resistances for low frequency sig- 
nals [24, 86, 88], with operating ranges of lVdd -- Vtl ~ 4V. 
A wide range of resistances and durations, and thus a variety of fast synapse 
designs can be used with artificial dendrites when a conductance-duration 
equivalence approximation is invoked. Under certain conditions (i.e. long and 
thin compartments, high axial and membrane resistances, and short durations), 
dendritic filtering validates use of short durations and large conductances. Con- 
sider an isolated compartment capacitor with only a switchable synaptic con- 
ductance attached as shown in Figure 15.3. The compartment voltage V,~(T) 
after the conductance has been active for duration = T seconds is given by 
~Vcx tator~ 
ci 
Ge 
Vm 
Cm 
GND 
Figure 15.3 
Isolated compartment for illustrating the conductance-duration product 
equivalence. The total charge transferred to On when Ge is connected T seconds is 
the same as long as the integral of the conductance over the duration is constant. This is 
only an approximation when axial resistances to neighboring compartments are added. 
= 
(vo 
- 
- e - 
G/cm) + vm(0) 
where V,~(0) is the initial voltage, Ge is the conductance, Ve is the conduc- 
tance supply potential, and C,~ is the compartment capacitance. The argument 
of the exponential in this equation clearly indicates that the final compartment 
voltage and total charge transferred will be the same whenever the product of 
the conductance and duration are the same. In fact, for any two time-varying 
conductances, the final voltage will be the same as long as the integrals of 
the conductance over the duration are equal. This suggests that a very short, 
50as synapse activation pulse can control the duration when used with cor- 
respondingly strong conductances, and that synaptic weight variation can be 
accomplished either by varying the conductance or the duration or both. We 
have fabricated and tested neuromorph chips having both types of synapses: 
one that varies the conductance, and the other that varies the duration. 

348 
NEUROMORPHIC SYSTEMS ENGINEERING 
When leakage through the axial resistances is taken into account, this equiv- 
alence property is only an approximation. The total charge transferred also 
becomes dependent on the potentials of neighboring compartments during ac- 
tivation. If the duration is short enough compared to dendrite time constants, 
interaction with nearby compartments will be negligible until long after the 
synapse has turned off, and the charge transferred by activation of a lmS con- 
ductance for a 50ns duration is a reasonable approximation to that of a weaker 
50nS conductance for lms. 
(a) Favorable Params 
(b) Short Compartments 
(c) Unfavorable Params 
, 
8 
~6 
$ 
,53 
~2 
i 
i 
2 
4 
6 
5 
.
.
.
.
 
4 
3 
2 
1 
o 
~ 
~ 
Initial Neighboring Compartment Voltages (V) 
:: 
40 
35 
30 
25 
20 
15 
10 
5 
0 
i 
Figure 15.4 
Comparison of short duration approximation (dashed line) with long duration 
synapses (solid line) under a) favorable dendritic parameters, b) favorable parameters except 
shorter compartments, c) unfavorable dendritic parameters. For a) and b) the cytoplasmic 
resistivity ri = 270~cm, specific membrane resistance rm = 165k~cm 2, specific mem- 
brane capacitance Cm = .751~F/cm 2, branch diameter D = .75#m, synapse resistances 
1/G~e -- 15k~ for 50ns duration and 300M~ for lms duration. For a) compartment 
length L = 1/5 space constant --- .2A -- .02cm, while for b) L -- .1A = .Olcm. For c) 
r~ = 70ftcm, r,~ = 20kf~cm 2, c,~ = .75#F/cm ~, D = 2.5#m, L = .2A = .02cm, 
1/Ge ---- 3.4kf~ for 50ns duration and 68Mft for lms duration. The synapse was placed 
on the middle compartment of an eleven compartment branch (like Figure 15.1) whose ac- 
tual resistor and capacitor values were calculated from the parameters above with standard 
formulas [77]. Simulations were performed with Spice3f4. See text for further discussion. 
The simulation results shown in Figure 15.4 compare the total charge trans- 
ferred by short (50ns, dashed lines) and long (1ms, solid lines) duration synap- 

NEUROMORPHIC 
SYNAPSES 
349 
tic activations under differing dendrite dynamics and initial conditions. An 
excitatory synaptic conductance was placed on the middle compartment of an 
eleven compartment branch. In all cases this compartment to be activated 
was initialized to the resting potential of 2.5V, and in all but Fig. 15.4b the 
synaptic conductance was normalized to C,~/T. This resulted in a synapse 
time constant, equal to the duration, that produced depolarization of an iso- 
lated compartment to 4V using a 5V excitatory supply potential. The other 
ten compartments (five on each side) were initialized to a single voltage which 
was stepped over simulation runs from the inhibitory supply potential (0V) to 
the excitatory supply potential (5V). The dashed lines (50ns duration) are 
horizontal because the duration is short enough that interactions with neigh- 
boring compartments are negligible, and thus the neighboring compartmental 
voltages are irrelevant. The solid lines (lms duration) each have a negative 
slope because the effective load on the synapse through the axial resistors in- 
creases with hyperpolarization of neighboring compartments. The difference 
between the horizontal and slanted lines at a particular initial neighbor voltage 
represents the error of the short duration approximation. 
The dendrite parameters for Figures 15.4a-b, taken from [53], represent fa- 
vorable conditions for the short duration approximation. Such high cytoplas- 
mic resistivities (ri) and specific membrane resistances (r,~) have been found 
in several recent studies [74, 78, 84, 85]. Results with less favorable but more 
commonly accepted dendrite parameters [6, 14, 77] are shown in Figure 15.4c. 
Compartment lengths for Figures 15.4a and c were normalized to one-fifth of 
the dendrite space constant [77] based on a specific membrane capacitance of 
0.75#F/cm. The compartment length was cut in half for Figure 15.4b, increas- 
ing interactions with neighboring compartments and halving the compartment 
capacitance. However, Figure 15.4b uses the same synaptic conductances as 
Figure 15.4a, so the charging time constants are less than the durations. 
In all cases short-duration activations match long activations best when ad- 
jacent compartments are depolarized. Initializing all adjacent compartments 
to the inhibitory supply potential (0 volts) maximizes the pull on the acti- 
vated compartment through the axial resistors, maximizing interaction under 
long durations. For inhibitory synapses, these relationships are reversed. Note 
that in biological neurons the inhibitory supply potential is much closer to the 
resting potential than the excitatory supply potential, which would lessen the 
error for excitatory synapses with hyperpolarized neighbors. Comparing Fig- 
ure 15.4a to Figure 15.4b indicates that breaking a branch into smaller com- 
partments increases the percent error for the short duration approximation. 
This is counter-intuitive because finer compartmentalization normally makes 
discrete spatial models closer to continuous biological dendrites. The large per- 
cent differences in Figure 15.4c show that the approximation also worsens as 
branch diameter grows and axial and membrane resistances decrease. While 
the charge transfer errors may be significant even under favorable conditions, 
they would be much greater with current-source synapses, even for an isolated 
compartment. 

350 
NEUROMORPHIC SYSTEMS ENGINEERING 
Though the total charge transferred can be about the same with very short 
durations, simulations indicate that the shape of the PSP measured at the orig- 
inating compartment differs significantly for the first couple milliseconds. For 
a 50as duration synapse the local compartment voltage instantaneously jumps 
upon activation and then decays quickly through the axial and membrane resis- 
tances. Upon activation of a 1ms duration synapse, the compartment voltage 
rises gradually to a smoother, smaller peak before decay becomes visible. After 
a few milliseconds artificial dendrites filter out the high frequencies in the PSP 
caused by a short duration, and PSPs from all but the most proximal synapses 
appear smooth at the artificial soma. Nevertheless, because of these differences 
in initial PSP shape, short duration synapses may not be a biologically ac- 
curate model for interactions between adjacent compartments that have been 
stimulated at nearly the same time. Clearly the short duration approximation 
cannot be used for slower neuromodulatory synapses, which require more ex- 
tended conductance control. Nevertheless, we will see that it accommodates 
very simple circuitry and emulation of any number of independently weighted 
connections with one synapse circuit per compartment. 
15.7 
VARIABLE-WEIGHT 
SYNAPSES - TWO DIFFERENT 
APPROACHES 
We have tested two fundamentally different synaptic conductance designs. The 
first design, referred to hereafter as the conductance array (Figure 15.5) em- 
ploys a minimal on-duration equal to the activation pulse width, and achieves 
conductance variation by selecting an appropriate-valued conductance form an 
on-chip array. In the second, or self-timed conductance design (Figure 15.6), 
the controllable decay of a local gate capacitance determines the duration while 
switched capacitors form a constant conductance. Both synapses are meant 
to operate between inhibitory and excitatory synapse supply potentials of zero 
and five volts. Though many hybrids or variations of these designs are certainly 
possible, these two were chosen because the advantages of each are quite com- 
plementary, and they allow us to consider two diverging approaches to on-chip 
adaptation. We will not know which is more suitable for on-chip adaptation 
until we settle on a particular weight and connection modification scheme. 
15.8 
CONDUCTANCE 
ARRAY 
Unfortunately, linear voltage-controlled resistors cannot be made from a sin- 
gle minimum size component in the standard CMOS process, but individual 
synapse circuitry must be small for large numbers of synapses to fit on a chip. 
The most striking aspect of the conductance array design is that all synapses on 
a neuromorph (90 in this case) share two conductance arrays. Since the conduc- 
tance circuitry need not be duplicated at each synapse, its size and complexity 
is much less of a concern. Sharing is only possible when the synapse duration 
is equal to the activation pulse width, so that the conductance array can be 
time-multiplexed along with the synapse address. Most perceptron network 

NEUROMORPHIC SYNAPSES 
351 
~ulter 
Row_n 
itor] 
Figure 15.5 
Sampling and pre-charging circuitry for conductance array synapses. When a 
sample signal is asserted (not shown), the synapse address is decoded which leads to assertion 
of Col_x and Rowl signals thus connecting the n-channel source followers of the appropriate 
compartment to the output buffer. The resulting voltage will be the compartment voltage 
less the n-channel threshold. If pre-charging is desired the precharge signal is asserted, 
charging all pull-up lines to approximately Vm -
-
 Vthr (n) 4- Vthr(p), the threshold voltages 
for n- and p-channel transistors. The pre-charge range is approximately 1 - 4V. 
Figure 15.6 
Vdd 
DAC 
~ 
I 
~o,. 
T 
~ ~ 
p, 
~ _ pclock 
I 
~ pclock2 
Excitatory self-timed synapse circuitry. See text for explanation. 
synapses cannot be shared in this manner because they do multiplications in 
parallel. 
We will only describe the operation of excitatory synapses since inhibitory 
synapses work similarly, except that all p-channel transistors are replaced with 
n-channel and supply voltages are exchanged. 
The excitatory conductance 
array (Figure 15.5) consists of 15 p-channel MOSFETs whose sources are con- 
nected to the excitatory synapse supply potential (Vexcitatory) and whose drains 
are connected to a common pull-up line. Upon presentation of the 4-bit synaptic 
weight and assertion of the synapse activate signal, a weight decoder switches 
the gate voltage of the selected weight transistor from 5V to 0V, biasing the 

~/~ 
~/o~ 
~ 
>/g 
e/~ 
~ 
9/g 
g/~ 
gI 
9If 
g/or 
gI 
Z/~ 
~/9 
II 
o~/g 
>/~ 
o~ 
o~/~ 
~1~ 
6 
6/~ 
9/~ 
~ 
II/~ 
9/~ 
Z 
~I/~ 
6/2 
9 
9~/~ 
z/~ 
~ 
0~/~ 
6/~ 
~ 
~/~ 
~/~ 
~ 
0~/~ 
~/~ 
~ 
9~/~ 
9~/~ 
~ 
7/A4 
7/A4 
~q~aA4 
~o~}q}qu I 
~o~v~ax~ 
a}~dvug~ 
"saz!s JÂ°ls!sueJ.l_ ReJJV a~uel:)npuÂ°D 
I'$I alqe/ 
â¢ S:~IOA!II!m paapunq v anoq~ jo uov~vz!a~iodo p v sasn~a ~soSva~ oq~ aIN~ 
'uo~a~aa~ OlSU~s ~ u~ Ag oa AI moaj ~uom~a~dmoa ~ ~u~$a~qa ~o olq~d~a s~ 
aR~o~ aso~uoals oRZ 'I'~I alq~& u~ u~oRs oa~ aoas~sueaa aq$~o~ Ra~o aq sonata 
qlSuoi/qap~ OR& 'soSuvqa OSmlOa auamaa~dmoa poavds Xluoao XlaaVm~xoaddv 
u~ sainsoa osind uo~a~a~aa~ ~u0g ~ a~qa os paz~s 2wnpD~pu ~ aaa~ saoas~su~aa 
aq$!o~ aqa 'uo~aNnm~s 3 o dlo q oqa qa~ 
'O~I oa O~ moa/oSu~a aDaaD sNa 
u~ pozwoa soau~ls~soa oR1 '.aoas~su~aa aR$~o~ polaOlaS oqa jo aoaa~va~d oau~a 
-anpuoasuvaa pu~ o~l~a qaSuoI/qlp~ oqa oa I~uo~aaodoad ~ oauvaanpuoa I~nla~ 
oR& "iv~uo~od XIddns osd~uXs oqa oa aoas~suval aq$!o~ poaaoios oqa qSnoaqa 
aoaD~d~a auomaa~dmoa aqa moaj qaud oauvaanpuoa-~o I ~ stusq sir & "ouii dn 
-lind oqa oa oaumD~d~a au~aqmam ddi ~ s,luau~aa~d~oa oqa saaouuoa luom 
-aa~dmoa aavvdoadd~ oqa jo (~S) saoas~su~aa aaaios umnioa pu~ ~oa oqa pu~ 
'papoaap si ssaapp~ asd~uXs Xaol~lDxa aRa 'om!a am~s aRa W 
'(AI ~) PI o 
-qsoaqa oAoq~ SI O$~alOa au!I dn-IInd oqa sv ~uo I sv uot$oa a~ou!i sa! u! aoas!su~aa 
DNII:I~NIDN~ SIN~&SAS OIHdltOPIOttI~I~tN 
gSg 

NEUROMORPHIC SYNAPSES 
353 
-2 
weight 0 
i 
weight 15 
-4 
-3 
,.~;:i 
~h~ls .......... 
: 
. 
: 
i 
i 
::ii:i:!:::~:i 
-2 
-1 
Initial Ddving Potential (V) 
weight Oi 
Figure 15.7 
The compartment voltage change upon activation plotted versus the driving 
potential (synapse supply potential - compartment voltage, Vc - Vn(0)) before activation 
for each excitatory and inhibitory weight. The compartment voltages immediately preceding 
and after synapse activation with the selected weight were measured through the internal 
compartment voltage sampling circuitry (cf. Fig. 15.4). Through analog multiplexing, any 
compartmental voltage can be sampled and output on a single pin, with settling times less 
than 100n8 per sample and a linear range of 1 - 5V. The dotted portions of the curves are 
estimates based on our understanding of the circuitry for regions where direct measurement 
was not possible. 
The compartment voltage change upon activation is plotted versus the driv- 
ing potential before activation for each excitatory and inhibitory weight in 
Figure 15.7. If the transistors behaved as ideal conductances the transfer char- 
acteristics would be straight lines with various slopes passing through the origin. 
In regions where the lines become horizontal, the weight MOSFET is operating 
in the saturation rather than the linear region. This occurs mainly for smaller 
weights when the compartment membrane potential is far from the synapse 
supply potential (i.e., IVdsl > IV~s - Vtl). 
The measured transfer characteristics are nearly straight lines, but they do 
not pass through the origin. This is related to the major problem with the con- 
ductance array approach: unavoidable parasitic capacitances along the shared 
pull-up and pull-down lines. The capacitance of each shared line, due mainly 
to the source-bulk and drain- bulk capacitances of the weight and synapse 
transistors connected to it, amounts to approximately lpF, as much as each 

354 
NEUROMORPHIC SYSTEMS ENGINEERING 
compartment's membrane capacitance. If the pull-up line voltage differs from 
the compartment voltage when the two are connected, the pull-up line will 
dump charge into the compartment, causing large voltage offset errors. 
These offset errors are reduced by using the internal sampling circuitry to 
pre-charge the pull-up lines to a compartment's voltage before its synapse is 
activated (Figure 15.5). Because of the limited range and mismatch between 
the n- and p-channel source followers, the pull-up line is not always precharged 
to the correct voltage. The worst errors in the precharging actually occur when 
the membrane potential is less than one volt, where it is out of the range of 
the sampling circuitry and therefore not directly measurable. The remaining 
offset errors could be eliminated by increasing the range of and compensating 
for the offsets in the sampling circuitry with a rail-to-rail op-amp. They could 
also be further reduced by dividing the pull-up line into switchable segments, 
which could break up the parasitic capacitance. Even with good precharging, 
the pull-up line parasitic capacitance ultimately limits the scalability of the 
shared conductance method. Precharging also requires at least one additional 
clock cycle per synapse activation, increasing the minimum time to service a 
connection. 
15.9 
SELF-TIMED SYNAPSES 
The distinguishing feature of a self-timed synapse is that its duration can be 
much longer than the activation pulse width. Capacitor voltage decay in the 
local synapse circuit regulates the duration, leaving the synapse address bus 
free to activate other synapses. In the current design the conductance is held 
constant while the voltage deposited on a small gate capacitance determines the 
synaptic weight by modulating the duration. We were able to use switched ca- 
pacitors to implement the conductances because the durations are long enough 
to smooth out the discrete charge transfers. Alternative self-timed synapse 
designs could have a fixed duration and modulate the conductance. The self- 
timed synapse weights are currently supplied by an external digital-to-analog 
converter, though they could also be stored within each compartment on a 
floating gate or flux capacitor [67]. 
The self-timed synapse circuit is shown in Figure 15.6. If a compartment's 
address is presented to the chip, both row and column select transistors (P3-P6) 
will turn on for about 50ns. This will deposit the synaptic weight manifested 
as a duration control voltage on the parasitic P2 gate capacitance. At the 
same time the P1 gate capacitance is discharged to ground through P3 and 
P4, turning on P1. This forms a path from the compartment capacitor to 
the excitatory synapse supply potential through the switched capacitors (PT- 
PS). Assuming P2 is biased in sub-threshold, it will slowly charge the P1 gate 
back to Ydd after the row and column select transistors turn off. The duration 
will approximately be the length of time until the switched capacitor current 
becomes limited by the P1 drain-source current, at which point the synapse 
will behave like a very weak current source rather than a conductance. Note 

NEUROMORPHIC SYNAPSES 
355 
that P1 does not turn off abruptly, and therefore a clear-cut duration cannot 
be defined. Also note that the P2 gate voltage will have a natural decay of its 
own, increasing or decreasing the charging rate of the P1 gate. The duration 
control voltage need not be refreshed as long as it stays low enough that P1 
turns off before P2. 
The switched capacitor clocks are shared by all synapses on a chip and there- 
fore cannot be used to modulate synaptic conductances individually. However, 
by varying the switched capacitor clock frequency between lOHz and lOMhz, 
the resistance of all synapses can simultaneously be varied between 10M~t and 
10T~. The resistances are typically set so that activation with the maximum 
desired duration, or strongest weight, can fully charge a compartment to the 
synapse supply potential. Weaker weights can then be achieved with shorter 
durations. The maximum desired duration is typically one millisecond to em- 
ulate fast chemical synapses but can be much longer to model slower second- 
messengers synapses. Durations between ten microseconds and one second can 
be achieved with the current chip. 
Voltage transfer characteristics for the self-timed synapse are shown in Fig- 
ure 15.8. The deviations from linearity visible in the transfer characteristic 
are, for the most part, due to temperature drift between successive measure- 
ments. Maintaining duration consistency over moderate temperature changes 
is the most serious problem with self-timed synapses. This first version of the 
self-timed synapse circuitry had several minor layout flaws as well. Because the 
switched capacitors (PT-PS) are always connected to the compartment capac- 
itor, high clock frequencies cause spurious membrane leakage toward ground 
even when P1 is turned off. Switching the placements of P1 and P7-8 would 
eliminate this problem but introduce source-bulk effects to the operation of Pl. 
Also, crosstalk causes the inhibitory synapse to turn on gradually after a few 
excitatory activations, and the Pl gate voltage naturally decays toward ground 
if P2-P4 are all turned off, causing the synapse to turn on spontaneously after 
long periods of inactivation. These latter problems can be eliminated by creat- 
ing a small leakage pathway from the Pl gate to Vdd, and from the corresponding 
inhibitory N1 gate to GND. 
15.10 
ENGINEERING COMPARISONS 
At this point we will compare the two designs directly, first from a purely 
engineering standpoint, second based on their suitability as models of biological 
synapses, and third as paths toward on-chip weight and connection adaptation. 
Both designs were fabricated through MOSIS using a standard CMOS double- 
poly n-well 2.0#m process on 2 Ã 2ram 2 TinyChips. Because of its shared 
conductance circuitry, the conductance array design is more space efficient, 
allowing each compartment to be short (246#m), only 66#m of which is synapse 
activation circuitry. The excitatory and inhibitory conductance arrays servicing 
45 compartments are only 125 Ã 246#m 2 combined, less than ten percent of the 
total compartment area. The local self-timed compartments are much longer 

356 
NEUROMORPHIC SYSTEMS ENGINEERING 
5 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
: ........................ 
: .................. 
,: 
: 
â¢ 
. 
: 
4 
. . . . . . . . .  
........... 
............. 
............ 
i .......... 
............. 
............. 
~edac 
= 
3.95V 
......... 
a- 
) 
...... i ............ 
............. 
............. 
........... 
; ............. 
.......... 
~ ............ 
i 
i 
....... 
....... 
~ 
! 
~
i
 
.... " 
. 
! 
~ 
; 
.... 
: 
'I 
........... 
......... 
. . . .  
: 
" 
~ o~ ..................... 
,,,, 
~,~, ,,,,.i~:: ~: 
. 
_ 
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
 
-4 .................... 
: 
~ 
: 
: 
. . . .  
: 
: 
: 
i 
i 
i 
i 
i 
~ 
i 
i 
-~ 
-4 
-~ 
-2 
-~ 
0 
~ 
2 
~ 
4 
5 
initial 
Driving 
Potential 
(V) 
Figure 15.8 
The peak compartment voltage change upon activation plotted versus the 
driving potential before activation for inhibitory duration control voltages ranging from 
0.95V to 1.10V and excitatory control voltages from 3.83V to 3.95V at 10~tV intervals. 
To facilitate comparison with the conductance array which uses much shorter durations, ax- 
ial resistors were turned off and membrane resistors were set so that the membrane time 
constant was much longer than the maximum duration (2~s). The dotted portions of the 
curves are estimates based on our understanding of the circuitry for regions where direct 
measurement was not possible. 
(360#m), about half or 165#m of which is actually self-timed synapse circuitry. 
Because the self-timed synapse circuitry is two and a half times taller, only three 
instead of four neuromorphs fit on a TinyChip. Both compartment designs are 
24#m wide. 
Though voltage-controlled linear resistors (e.g. 
[81]) could be used for shared 
synaptic conductance circuitry, the conductance array described here is ideal for 
systems with digital weight storage because it performs its own digital-to-analog 
conversion with fully customizable quantization levels. Considering excitatory 
and inhibitory synapses together, the present conductance array decodes thirty 
evenly spaced non-zero weights from five bits in each synapse address. Since 
combining conductances in series or parallel does not result in well- spaced 
weights, each weight requires its own transistor. Thus the conductance array 
width is proportional to the number of weights, not the number of weight 
bits. Because the conductance array is never biased in sub-threshold, weight 

NEUROMORPHIC SYNAPSES 
357 
variations due to changes in temperature are negligible. Weight variations 
between the four chips we tested were also quite low. 
Transistors of the self-timed synapse, on the other hand, must be biased 
in sub-threshold to produce the appropriate durations. Therefore, the dura- 
tions of self-timed synapses are very sensitive to temperature, noise on the 
duration- control voltage, and, presumably, processing variations. The self- 
timed synapses do have an advantage in that they do not share pull-up lines, 
and thus are free from the complications of precharging. 
15.11 
MODELING COMPARISONS 
Though the conductances are not perfectly linear in either design, the currents 
in both exhibit the desired dependence on the voltage of the underlying com- 
partment. With the stronger weights, self-timed synapses stay on long enough 
to exhibit sub-linear dependence on the voltages of neighboring compartments. 
The minimal duration of the conductance array allows the peak of the post- 
synaptic potential to be sampled immediately after activation, which may make 
the off-chip software implementation of Hebbian learning much simpler. 
If a single weight voltage is stored permanently within a compartment's 
synaptic circuitry, all connections made to that compartment must have the 
same weight. This can be a severe limitation on network connectivity if each 
neuromorph has only as a few compartments. One work-around would be to 
associate multiple weight storage devices with each compartment, which could 
still require considerable space even if the synaptic conductance circuitry was 
not duplicated. As long as excitatory and inhibitory synapses have separate 
circuitry, at least the weights of excitatory and inhibitory connections to the 
same compartment will be separate. 
In this regard, associating weights with connections in digital memory is 
preferred at this time. Different connections to the same compartment can use 
separate weights as long as the time between successive activations of the same 
synapse is greater than the duration. 
With our conductance array design, 
any number of connections with separate weights can be made to the same 
compartment. 
Because our self-timed synapses use temporary local weight 
storage and longer durations, they are not quite as flexible. Undesirable weight 
interference will occur if the synapse is activated more than once within one 
duration. For instance, a strong weight could be overwritten by a weak one 
early in its duration and the result would be a short duration. Though less 
likely for variable duration synapses, the result would be more tolerable if a 
weak weight was overwritten by a stronger one. The former type of error can 
be prevented by placing a diode between the incoming duration control voltage 
and the local weight storage capacitance. 
15.12 
ADAPTATION COMPARISONS 
Though off-chip digital weight storage is well-suited for early experiments in 
hybrid software/hardware based adaptation, large systems will eventually need 

358 
NEUROMORPHIC SYSTEMS ENGINEERING 
on-chip analog adaptation circuitry. This usually implies weights should be 
stored on-chip in analog form. Ideally, weight storage and adaptation capabil- 
ity would be combined in a single small device. For example, programmable 
amorphous silicon resistors [32, 75] could serve as synaptic conductances and 
perform simple anti-Hebbian learning by increasing their conductance when 
the compartment voltage before activation is far from the synapse supply po- 
tential. However, we may find that effective adaptation requires more complex 
interactions which cannot be reproduced by a simple device. The next best 
approach might be to use conventional MOSFET design techniques to inte- 
grate capacitive weight storage, local adaptation circuitry and some variation 
of a self-timed synaptic conductance into each compartment. This approach 
could quickly become quite area intensive, especially if it attempted to integrate 
information from neighboring compartments. 
While the inherently digital nature of the conductance array may seem 
unsuitable for on-chip adaptation circuitry, the experience gained with time- 
multiplexed sampling and precharging suggest a space-efficient means for on- 
chip adaptation circuitry to integrate information from arbitrary spatial scales. 
Under direction of the Virtual Wires state machine and connection list, the volt- 
ages of arbitrary compartments can be sampled and deposited on adaptation 
parameter storage capacitors. The resulting spatially averaged dendritic activ- 
ity, representing second messenger concentrations, could complement the local 
information needed by adaptation circuitry. Work on this is now in progress. 
15.13 
CONCLUSIONS 
On-chip analog synaptic weight storage may be unnecessary for neuromorphic 
systems with time-multiplexed synaptic activation busses. Until issues such as 
the degree that adjacent synapses adapt independently and the importance of 
quantal synaptic noise are better understood, off-chip digital weight storage 
provides more flexibility. Digital storage of weights and connections combined 
with sampling of dendrite voltages provides all of the information necessary to 
experiment with correlative adaptation rules in software. Experience gained 
from this hybrid system approach can eventually be used to design appropriate 
on-chip adaptation hardware. 
In order to match the performance of biological neural systems, it is not nec- 
essary to duplicate them in every detail, but only to capture those properties 
underlying their computational and adaptive abilities. Unfortunately, in these 
early days of neuromorphic engineering, we sometimes must guess which mech- 
anisms are critical to neural computation. Therefore, we gradually increase the 
complexity of our artificial dendrite models, noting what useful behaviors can 
be observed with each additional feature. The neuromorphic, variable weight 
synapses presented in this paper are the most recent enhancement to our arti- 
ficial dendrites. 
Both the conductance array and self-timed synapses behave like conduc- 
tances to enable sublinear summation, a basic computational mechanism in 

NEUROMORPHIC SYNAPSES 
359 
passive dendrites which may be essential for realizing biologically plausible 
adaptation. Because the charge transferred into dendrites by fast synapses 
will be approximately the same whenever the product of the conductance and 
duration are equal, synaptic weights can be varied by modulating the conduc- 
tance or duration or both. The longer, more realistic durations produced by 
self-timed synapses can cause more complex sublinear summation and more 
accurately model activation of adjacent compartments at sub- millisecond in- 
tervals. However, these longer durations prevent full weight independence for 
connections to the same compartment which are activated at sub-millisecond 
intervals. Thus, in highly connected networks, the minimal durations used by 
the conductance array synapses may be a more powerful model. If, through 
time-multiplexing, all of the connections to an isopotential dendritic segment 
can be modeled with a single synapse circuit per compartment, it will be possi- 
ble to emulate the thousands of synaptic connections found on corticai neurons 
with the synapse circuits of only tens or hundreds of compartments. 
Acknowledgments 
Support for this work was provided by National Science Foundation Grant (# BCS- 
9315879) and by a NSF Graduate Fellowship for WCW. 
References 
[1] C. Acar and M. S. Ghausi. Fully integrated active-rc filters using MOS 
and non-balanced structure. Int. J. of Circuit Theory and Applications, 
15:105-121, 1987. 
[2] C. Allen and C. F. Stevens. An evaluation of causes for unreliability of 
synapt ic transmission. In Proc. Natl. Acad. Sci. USA, volume 91, pages 
10380-10383, 1994. 
[3] P. O. Anderson. Properties of hippocampal synapses of importance for 
integration and memory. In G. M. Edelman, W. E. Gall, and W. M. 
Cowan, editors, Synaptic Function, pages 403-430. John Wiley & Sons, 
1987. 
[4] Y. Arima, M. Murasaki, T. Yamada, A. Maeda, and H. Shinohara. A 
refreshable analog VLSI neural network chip with 400 neurons and 40k 
synapses. IEEE J. of Solid State Circuits, 27:1854-1861, 1992. 
[5] M. Banu and Y. Tsividis. Floating voltage-controlled resistors in CMOS 
technology. Electron. Lett., 18:678-679, 1982. 
[6] J. N. Barrett and W. E. Crill. Specific membrane properties of cat mo- 
toneurones. J. of Physiol., 239:301-324, 74. 
[7] M. Bawdry and J. L. Davis. Long-Term Potentiation, volume 2. The MIT 
Press, Cambridge, MA, 1994. 
[8] J. M. Bekkers. Quantal analysis of synaptic transmission in the central 
nervous system. Current Opinion in NeurobioIogy, 4:360-365, 1994. 

360 
NEUROMORPHIC SYSTEMS ENGINEERING 
[9] T. V. P. Bliss and G. L. Collingridge. A synaptic model of memory: Long- 
term potentiation in the hippocampus. Nature, 361:31-39, 1993. 
[10] K. A. Boahen and A. G. Andreou. A contrast sensitive silicon retina with 
reciprocal synapses. Advances in Neural Information Processing Systems, 
4:764-772, 1992. 
[11] T. H. Brown, E. W. Kairiss, and C. L. Keenan. Hebbian synapses: Bio- 
physical mechanisms and algorithms. Annu. Rev. of Neurosci., 13:475-511, 
1990. 
[12] D. V. Buonomano and M. M. Merzenich. Temporal information trans- 
formed into a spatial code by a neural network with realistic properties. 
Science, 267:1028-1030, 1995. 
[13] H. A. Castro, S. M. Tam, , and M. A. Holler. Implementation and prefor- 
mance of an analog nonvolatile neural network. Analog Integrated Circuits 
and Signal Processing, 4:97-113, 1993. 
[14] J. D. Clements and S. J. Redman. Cable properties of cat spinal motoneu- 
rons measured by combining voltage clamp, current clamp and intracellular 
staining. J. of Physiol., 409:63-87, 1989. 
[15] M. H. Cohen and A. G. Andreou. Current-node subthreshold MOS imple- 
mentation of the herault-jutten autoadaptive network. IEEE J. of Solid 
State Circuits, 27:714-727, 1992. 
[16] G. L. Collingridge and T.V.P. Bliss. Memories of nmda receptors and ltp. 
Trends in Neurosci., 18:54-56, 1995. 
[17] T. Delbriick. Silicon retina with correlation-based velocity-tuned pixels. 
IEEE Transactions on Neural Networks, 4(3):529-541, May 1993. 
[18] J. Van der Spiegel, P. Mueller, D. Blackman, P. Chance, C. Donham, 
R. Etienne-Cummings, , and P. Kinget. An analog neural computer with 
modular architecture for real-time dynamic computations. 
IEEE J. of 
Solid-State Circuits, 27:82-92, 1992. 
[19] R. Douglas and M. Mahowald. A constructor set for silicon neurons. In 
S. F. Zornetzer, J. Davis, and T. McKenna, editors, An Introduction to 
Neural and Electronic Networks. Academic Press, 1994. 
[20] S. Eberhardt, T. Duong, and A. Thakoor. Design of parallel hardware 
neural network systems from custom analog VLSI 'building blocks' chips. 
In Int. Jr. Conf. Neural Network, volume 2, pages 183-190, 1988. 
[21] G. M. Edelman, W. E. Gall, and W. M. Cowan. Synaptic Function. John 
Wiley & Sons, 1987. 
[22] F. A. Edwards. Ltp- a structural model to explain the inconsistencies. 
Trends in Neurosci., 18:250 255, 1995. 
[23] J. G. Elias. Artificial dendritic trees. Neural Computation, 5:648-663, 
1993. 

NEUROMORPHIC SYNAPSES 
361 
[24] J. G. Elias and D. P. M. Northmore. Switched-capacitor neuromorphs 
with wide-range variable dynamics. IEEE Trans. on Neural Networks, 
6:1542-1548, 1995. 
[25] J. G. Elias, D. P. M. Northmore, and W. Westerman. An analog memory 
device for spiking silicon neurons. Neural Computation, 9:419-440, 1997. 
[26] W. Gerstner, R. Ritz, and J. Leo van Hemmen. Why spikes? hebbian 
learning and retrieval of time-resolved excitation patterns. Biol. @bern., 
69:503-515, 1993. 
[27] H. P. Graf, E. Sackinger, and L. D. Jackel. Recent developments of elec- 
tronic neural nets in north america. J. of VLSI Signal Processing, 5:19-31, 
1993. 
[28] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses with long term storage. In IEEE Intl. Syrup. on Circuits and 
Systems, volume 3, pages 1660-1663, 1995. 
[29] N. A. Hessler, A. M. Shirke, and R. Malinow. The probability of transmit- 
ter release at a mammalian central synapse. Nature, 366:569-573, 1993. 
[30] B. Hille. Ionic Channels of Ezcitable Membranes. Sinauer Associates Inc., 
2 edition, 1992. 
[31] Y. Hirai. Recent VLSI neural networks in japan. J. of VLSI Signal Pro- 
cessing, 6:7 18, 1993. 
[32] A. J. Holmes, R. A. G. Gibson, J. Hajto, A. F. Murray, A. E. Owen, 
M. J. Rose, and A. J. Snell. Use of a-si:h memory devices for non-volatile 
weight storage in artificial neural networks. J. of Non-Crystalline Solids, 
164-166:817-820, 1993. 
[33] T. M. Jessell and E. R. Kandel. Synaptic transmission: A bidirectional and 
self-modifiable form of cell-cell comunication. Cell, 72:1-30, 1993. Neuron 
Vol. 10 (Suppl.) pp. 1-30. 
[34] E. R. Kandel. Part III: Elementary interactions between neurons: Synaptic 
transmission. In J. H. Schwartz and T. M. Jessell, editors, Principles of 
Neural Science, pages 123-260. Appleton and Lange, Norwalk, Connecti- 
cut, 3 edition, 1991. 
[35] E. R. Kandel, M. Klein, B. Hochner, M. Shuster, S. A. Siegelbaum, R. D. 
Hawkins, D. L. Glanzman, V. F. Castellucci, and T. W. Abrams. Synaptic 
modulation and learning: New insights into synaptic transmission from the 
study of behavior. In G. M. Edelman, W. E. Gall, and W. M. Cowan, 
editors, Synaptic Function, pages 471-518. John Wiley & Sons, 1987. 
[36] C. Koch and T. Poggio. The biophysical properties of spines as a basis for 
their electrical function: A comment on kawato and tsakahara (1983). J. 
Theor. Biol., 113:225-230, 1985. 
[37] C. Koch and T. Poggio. Biophysics of computation: Neurons, synapses, 
and membranes. In G. M. Edelman, W. E. Gall, and W. M. Cowan, editors, 
Synaptic Function, pages 637-698. ohn Wiley & Sons, 1987. 

362 
NEUROMORPHIC SYSTEMS ENGINEERING 
[38] C. Koch and T. Poggio. 
Multiplying with synapses and neurons. 
In 
T. McKenna, J. Davis, and S. F. Zornetzer, editors, Single Neuron Com- 
putation, pages 315-346. Academic Press, Inc., 1992. 
[39] H. Korn and D. S. Faber. 
Regulation and significance of probablistic 
release mechanisms at central synapses. In G. M. Edelman, W. E. Gall, 
and W. M. Cowan, editors, Synaptic Function, pages 57-108. John Wiley 
& Sons, 1987. 
[40] H. Korn and A. Mallet. Transformation of binomial input by the postsy- 
naptic membrane at a central synapse. Science, 225:1157-1159, 1984. 
[41] D. M. Kullmann and R. A. Nicoll. Long-term potentiation is associated 
with increases in quantal content and quantal amplitude. Nature, 357:240- 
244, 1992. 
[42] A. Larkman, T. Hannay, K. Stratford, and J. Jack. Presynaptic release 
probability influences the locus of long-term potentiation. Nature, 360:70- 
73, 1992. 
[43] J. P. Lazzaro and C. Mead. Circuit models of sensory transduction in 
the cochlea. In Mead and Ismail, editors, Analo 9 VLSI Implementation 
of Neural Systems, pages 85 101. Kluwer Academic Publishers, Norwell, 
MA, 1989. 
[44] B. W. Lee, B. J. Sheu, , and H. Yang. Analog floating-gate synapses for 
general-purpose VLSI neural computation. IEEE Trans. on Circuits and 
Systems, 38:654-658, 1991. 
[45] B. W. Lee and B. J. Sheu. General-purpose neural chips with electrically 
programmable synapses and gain-adjustable neurons. IEEE J. of Solid- 
State Circuits, 27:1299-1302, 1992. 
[46] D. Liao, N. A. Hessler, and R. Malinow. Activation of post synaptically 
silent synapses during pairing-induced ltp in ca1 region of hippocampal 
slice. Nature, 375:400-404, 1995. 
[47] B. Linares-Barranco, E. SÂ£nchez-Sinencio, A. Rodriguez-Vazquez, and 
J. L. Huertas. A CMOS implementation of fitzhugh-nagumo neuron model. 
IEEE Solid-State Circuits, 26(7):956-965, 1991. 
[48] B. Linares-Barranco, E. SÂ£nchez-Sinencio, A. Rodriguez-Vazquez, and 
J. L. Huertas. A CMOS analog adaptive bam with on-chip learning and 
weight refreshing. IEEE Trans. on Neural Networks, 4:445-457, 1993. 
[49] P. J. Mackenzie, M. Umemiya, , and T. H. Murphy. a2+ imaging of cns 
axons in culture indicates reliable coupling between single action potentials 
and distal functional release sites. Neuron, 16:783-795, 1996. 
[50] K. L. Magleby. Short-term changes in synaptic efficacy. In G. M. Edelman, 
W. E. Gall, and W. M. Cowan, editors, Synaptic Function, pages 21-56. 
John Wiley & Sons, 1987. 
[51] M. Mahowald. VLSI Analogs of Neuronal Visual Processing: a Synthesis of 
Form and Function. Computation and neural systems, California Institute 
of Technology, 1992. 

NEUROMORPHIC SYNAPSES 
363 
[52] M. Mahowald and R. Douglas. A silicon neuron. Nature, 354:515-518, 
1991. 
[53] G. Major, A. U. Larkman, P. Jones, B. Sakmann, and J. J. B. Jack. De- 
tailed passive cable models of whole-cell recorded ca3 pyramidal neurons 
in rat hippocampal slices. J. of Neurosci., 14(8):4613-4638, 1994. 
[54] R. C. Malenka, J. A. Kauer, D. J. Perkel, M. D. Mauk, P: T. Kelly, R. A. 
Nicoll, and M. N. Waxham. An essential role for postsynaptic calmodulin 
and protein kinase activity in long- term potentiation. Nature, 340:554- 
557, 1989. 
[55] R. C. Malenka, J. A. Kauer, R. S. Zucker, and R. A. Nicoll. Postsynaptic 
calcium is sufficient for potentiation of hippocampal synaptic transmission. 
Science, 242:81-84, 1988. 
[56] A. Malgaroli, A. E. Ting, B. Wendland, A. Bergamaschi, A. Villa, R.W. 
Tsien, and R.H. Scheller. Presynaptic component of long-term potentiation 
visualized at individual hippocampal synapses. Science, 268:1624-1628, 
1995. 
[57] J. L. Martinez and B. E. Derrick. Long-term potentiation and learning. 
Annu. Rev. of Psychology, 47:173 203, 1996. 
[58] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
[59] C. A. Mead and M. A. Mahowald. Silicon model of early visual processing. 
Neural Networks, 1:91-97, 1988. 
[60] B. W. Mel. Information processing in dendritic trees. Neural Computation, 
6:1031-1085, 1994. 
[61] G. Moon, M. E. Zaghloul, , and R. W. Newcomb. An enhancement-mode 
MOS voltage-controlled linear resistor with large dynamic range. IEEE 
Trans. on Circuits and Systems, 37:1284-1288, 1990. 
[62] G. Moon, M. E. Zaghloul, , and R. W. Newcomb. VLSI implementation 
of synaptic weighting and summing in pulse coded neural-type cells. EEE 
Trans. Neural Networks, 3:394-403, 1992. 
[63] A. Mortara and E. A. Vittoz. A communication architecture tailored for 
analog VLSI neural networks: Intrinsic performance and limitations. IEEE 
Transactions on Neural Networks, TNN-5(3):459-466, May 1994. 
[64] P. Mueller, J. V. D. Spiegel, D. Blackman, T. Chiu, T. Clare, J. Dao, 
C. Donham, T. Hsieh, and M. Loinaz. A general purpose analog neural 
computer. In Int. Jt. Conf. Neural Network, volume 2, pages 177-182, 
1989. 
[65] T. H. Murphy, J. M. Baraban, W. G. Weir, and L. A. Blatter. Visualization 
of quantal synaptic transmission by dendritic calcium imaging. Science, 
263:529-532, 1994. 
[66] A. F. Murray and P. J. Edwards. Enhanced mlp performance and fault tol- 
erance resulting from synaptic weight noise during training. IEEE Trans. 
on Neural Networks, 5:792-802, 1994. 

364 
NEUROMORPHIC SYSTEMS ENGINEERING 
[67] A. F. Murray and L. Tarassenko. Analogue Neural VLSI: A Pulse Stream 
Approach. Chapman and Hall, London, England, 1994. 
[68] A.F. Murray, D. Del Corso, and L. Tarassenko. Pulse-stream VLSI neural 
networks mixing analog and digital techniques. IEEE Transactions on 
Neural Networks, 2:193-204, 1991. 
[69] D. P. M. Northmore and J. G. Elias. Spike train processing by a sili- 
con neuromorph: The role of sublinear summation in dendrites. Neural 
Computation, 8:1245-1265, 1996. 
[70] N. Otmakhov, A. M. Shrike, and R. Malinow. 
Measuring the impact 
of probabilistic transmission on neuronal output. Neuron, 10:1101-1111, 
1993. 
[71] T. Poggio and V. Torre. A new approach to synaptic interactions. In 
H. Heim and G. Palm, editors, Lecture Notes in Biomathematics. Theo- 
rectical Approaches to Computer Systems, volume 2, pages 89-115. Heidel- 
berg, NewYork, 1977. 
[72] W. Rall. Theoretical significance of dendritic trees for neuronal input- 
output relations. 
In R. F. Reiss, editor, Neural Theory and Modeling. 
Stanford University Press, Palo Alto, 1964. 
[73] W. Rall. Distinguishing theoretical synaptic potentials computed for dif- 
ferent soma-dendritic distributions of synaptic inputs. J. of Neurophys., 
30:1138-1168, 1967. 
[74] M. Rapp, I. Segev, and Y. Yarom. Physiology, morphology and detailed 
passived models of cerebellar purkinje cell. J. of Physiol, 474:101-118, 
1994. 
[75] M. J. Rose, J. Hajto, P. G. Lecomber, S. M. Gage, W. K. Choi, A. J. 
Snell, and A. E. Owen. Amorphous silicon analogue memory devices. J. 
of Non-Crystalline Solids, 115:168-170, 1989. 
[76] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal 
representations by error propagation. In D. E. Rumelhart, J. L.McClelland, 
and the PDP Research Group, editors, Parallel Distributed Processing: 
Explorations in the Microstructures of Cognition, volume I: Foundations. 
MIT Press/Bradford Books, Cambridge, MA, 1986. 
[77] I. Segev, J. W. Fleshman, and R. E. Burke. Compartmental models of 
complex neurons. In C. Koch and I. Segev, editors, Methods in Neuronal 
Modeling: From Synapses to Networks, pages 63 96. The MIT Press, 1989. 
[78] D. P. Shelton. Membrane resistivity estimated for the purkinje neuron by 
means of a passive computer model. Neuroscience, 14:111-131, 1985. 
[79] G. M. Shepherd. Canonical neurons and their computational organization. 
In J. Davis and S. F. Zornetzer, editors, Single Neuron Computation, pages 
27-60. Academic Press, Inc., 1992. 
[80] G. M. Shepherd and C. Koch. Introduction to synaptic circuits. In G.M. 
Shepherd, editor, The Synaptic Organization of the Brain, pages 3-31. 
Oxford University Press, New York, 1990. 

NEUROMORPHIC 
SYNAPSES 
365 
[81] T. Shibata and T. Ohmi. A functional MOS transistor featuring gate-level 
weighted sum and threshold operations. IEEE Trans. on Electron Devices, 
39:1444 1455, 1992. 
[82] T. Shima, T. Kimura, Y. Kamatani, T. Itakura, Y. Fujita, and T. Iida. 
Neuro chips with on-chip back-propagation and/or hebbian learning. IEEE 
J. of Solid-State Circuits, 27(12):1868-1875, 1992. 
[83] C. F. Stevens and Y. Wang. Changes in reliability of synaptic function as 
a mec hanism for plasticity. Nature, 371:704 707, 1994. 
[84] K. J. Stratford, A. J. R. Mason, A. U. Larkman, G. Major, and J. J. B. 
Jack. 
The modelling of pyramidal neurones in the visual cortex. 
In 
R. Durbing, C. Miall, and G. Mitchison, editors, The computing neurone, 
pages 296-321. Addison-Wesley, 1989. 
[85] D. Thurbon, A. Field, and S. J. Redman. Electrotonic profiles of interneu- 
roues in stratum pyramidale of the ca1 region of rat hippocampus. J. of 
Neurophys., 71:1948-1958, 1994. 
[86] J. Tomberg. 
Synchronous pulse density modulation in neural network 
implementation. In M. E. Zaghloul, J. L. Meador, and R. W. Newcomb, 
editors, Silicon Implementation of Pulse Coded Neural Networks, pages 
165-198. Kluwer Academic Publishers, 1994. 
[87] K. Y. Tsai, N. T. Carnevale, and T. H. Brown. Hebbian learning is jointly 
controlled by electronic and input structure. Network, 5:1-19, 1994. 
[88] Y. P. Tsividis and D. Anastassiou. Switched-capacitor neural networks. 
Electron. Lett., 23:958-959, 1988. 
[89] Z. Wang. Novel electronically-controlled floating resistors using MOS tran- 
sistors operating in saturation. Electron. Lett., 27:188-189, 1991. 
[90] L. Watts, D. Kerns, R. F. Lyon, and C. Mead. Improved implementation 
of the silicon cochlea. IEEE Journal Solid-State Circuits, 27(5):692-700, 
May 1992. 
[91] A. Zador, C. Koch, and T. H. Brown. Biophysical model of a hebbian 
synapse. In Proc. Natl. Acad. Sci. U.S.A., pages 6718-6722, 1990. 
[92] M. E. Zaghloul. Silicon Implementation of Pulse Coded Neural Networks. 
Kluwer Academic Publishers, 1994. 

16 
WINNER-TAKE-ALL 
WITH LATERAL 
NETWORKS 
EXCITATION 
Giacomo Indiveri 
Institute of Neuroinformatics, 
Gloriastrasse 32 CH-8006 Zurich, 
Switzerland 
giacomo@ini.phys.et hz.ch 
16.1 
INTRODUCTION 
The analog VLSI current mode winner-take-all (WTA) circuit, originally pre- 
sented in [16] is a good example of a very well designed architecture. It is able to 
process globally all the signals of an input array, it uses a very limited amount 
of transistors per input node and it operates in parallel, with strictly local in- 
terconnections. This architecture has been extensively and successfully used in 
a wide variety of applications [3, 6, 7]. More recently, interesting modifications 
to the original circuit have been proposed in [4] and [12]. In both cases, the 
authors added to each element of the WTA network a local feedback circuit to 
obtain a hysteretic behavior in the selection/de-selection of the winning node: 
every time a new winner is selected the local feedback circuit adds a constant 
bias current to its input. The circuit will then de-select the winner when either 
its input current becomes lower than other inputs by a factor greater than the 
bias current or when the whole network is reset. From a functional point of 
view, this operation enhances the resolution of the network and eliminates in- 
stability problems, providing a robust mechanism that withstands the selection 
of other potential winners unless they are stronger than the selected one by a 
set amount. The authors of [4] proposed a scheme for distributing locally the 
hysteretic component so that the winning input would be able to shift between 
adjacent locations maintaining its winning status, without having to reset the 
network. Following a similar approach, in this paper we present two novel vari- 
ants of the original WTA network that also use a feedback circuit to provide 
hysteresis, but that have a different scheme for implementing lateral excitation. 
367 

368 
NEUROMORPHIC SYSTEMS ENGINEERING 
~lout 
Vr 
M
5
~
 
M, 
Io node 
M2 
to node 
n-I 
n+l 
In M
I
~
M
 
3 
L~ 
' 
.~ 
. . . . . . . . . . . . .  
~ 
to node 
n-I v 
~ Iout 
M4~ Vr 
~
_
 
M2 
to node 
nÃ·l 
~
'
"
 
M3 
M
d
]
~
~
.
 
"1- " -
-
 
2 
t 
~ 
i 
Figure 16.1 
Elements of the modified WTA circuits. (a) Version described in this paper; 
(b) Previously proposed version. The two circuits differ in the way the transistors M2 are 
connected. The power supply voltage Vdd is set to 5V. Input currents typically range from 
picoamperes to microamperes. The laterally connected transistors operate in weak-inversion. 
The two variants differ in the way the output signal is read: the first one has 
a discrete output, with only the winning element active and all others inac- 
tive; the second one represents a generalized version of the WTA 
architecture, 
with all its output elements active simultaneously, which behaves like a non- 
linear filter. These new circuits can be used as handy building blocks for VLSI 
models of attention mechanisms 
[3, Ii] and, more generally, for a larger set of 
neuromorphic 
analog VLSI architectures [5]. 
16.2 
CIRCUIT DESCRIPTIONS 
16.2.1 Discrete output WTA 
Figure 16.1 shows the single elements of the modified winner-take-all circuit 
described in [4] next to the discrete output variant here proposed. As shown, the 
circuits are remarkably similar, yet their operating conditions and functional 
behaviors are quite different. 
In the circuit of Fig. 16.1(b) laterally connected transistors M2 implement 
a "diffusor network" operating in weak-inversion [1, 14], used to distribute 
the hysteretic component of the winner's output (i.e. the feedback current 
Ib flowing through transistor M3) to neighboring units. Lateral excitation is 
independent of the intensity of the winner's input current; on the other hand, in 
the circuit in Fig. 16.1(a), laterally connected transistors are used to implement 
a different type of diffusor network that distributes the sum of both input 
current and hysteretic component to neighboring units. This operation, while 
laterally spreading the hysteretic current, simultaneously performs smoothing 
on the input data. The circuit here proposed will thus tend to favor areas 

WINNER-TAKE-ALL NETWORKS 
369 
that have a higher average input activity rather than selecting the single input 
with maximum intensity. This is instrumental in eliminating errors arising 
from salt-and-pepper noise and is helpful in eliminating errors that arise from 
offsets and device mismatches typical of analog VLSI technology. Moreover, the 
circuit in Fig. 16.1(a) has a discrete output which is convenient for use with 
centroid circuits [2, 13] for encoding the winner's spatial position in the array, 
whereas the WTA network proposed in [4] has multiple outputs that follow the 
current distribution imposed by the diffusor network: one is maximum for the 
winning element and the others decrease exponentially with distance. Another 
significant difference between the two circuits is in the way the bias voltages 
of the laterally connected transistors M2 are set: in order to correctly operate 
the diffusor network of the circuit in Fig. 16.1(b) the gate voltages Vr need to 
be set at values higher than the power supply voltage Vdd; this problem does 
not occur for the circuit in Fig. 16.1(a) for which the voltages V~ are typically 
in the range 0.5V to 1V. 
The circuit of Fig. 16.1(a) works as follows: if n is the winning node, Vn 
is set so that transistor M3 supplies approximately all the bias current Ib and 
V~Â¢n are all set so that the transistors with those gate voltages are out of 
their saturation region and their drain currents are approximately null. The 
bias current is hence copied only in the winning element through the current 
mirror M4-M5 and diffused, along with part of the input current, to neighboring 
elements through the diffusor network. The diffusor network is implemented by 
transistors M1, M2 and all the equivalent ones belonging to the other elements 
of the array. 
Figure 16.2 shows the voltage distribution of the input nodes of a 13 element 
array, for a case in which there is an input current in the center and no input at 
all other nodes. The diffused current flows out of the winning element because 
Vn > Vn + 1 > V~ + 2, and so forth. As shown in the figure, the winner-take-all 
network forces a discontinuity in the voltage distribution at the winning node. 
For all the elements that are more than one node away from the winning one, 
the voltage distribution has a traditional resistive-network form, which can be 
approximated by the equation 
V = V0e -~lxl 
where x represents the distance from the input node and c~ represents the space 
constant of the diffusor network, defined as the rate at which signals die out 
with distance from the source [18]. 
The diffusor network here proposed is a current mode one, hence also the 
current distribution will follow a similar profile. For such a network the space 
constant is defined as: 
---~-~ (v~-vg) 
O~ 
-~- 
e 
2UT 
where ~ is the subthreshold slope coefficient, UT is the thermal voltage and 
V~ and Vg are the gate voltage of the laterally connected transistors and the 
common node voltage (gate voltage of transistors M1 in Fig. 16.1) respectively. 

370 
NEUROMORPHIC SYSTEMS ENGINEERING 
The amount of lateral excitation can thus be controlled by changing the 
bias voltage V~ (see Fig. 16.3(a)) or by having input currents of different in- 
tensities (see Fig. 16.3(b)). Specifically, since the common node voltage Vg 
increases logarithmically with the winning input current (for transistors op- 
erating in weak-inversion), the space constant c~ increases linearly with input 
current intensity. As a consequence, if the winning input is relatively strong 
(high confidence), the lateral excitation area is confined to a small neighbor- 
hood around the winning node. If, on the other hand, the winning input is 
relatively weak (low confidence), the lateral excitation area is wider. 
Having not instrumented transistor M1 of Fig. 16.1(a) (and all other equiv- 
alent transistors in the array), in the hardware implementation, it was not 
possible to verify directly the expected behavior described above. Nonetheless, 
circuit simulations, by means of which it was possible to measure the current 
flowing through the transistors M1 without affecting the behavior of the circuit, 
provide consistent results (see Fig. 16.3). Experimental data was obtained on 
the output nodes of the circuits for a 25 element WTA network (of the type 
shown in Fig. 16.1(a)), implemented on a 2.3mm by 2.3mm chip using ana- 
log CMOS 2#m technology. Fig. 16.4 shows measurement results for a case in 
which 3 input units are active and all others are null. The input current on 
unit 8 is gradually swept from 70 nA to zero and back, while inputs on unit 10 
and on unit 16 are kept constant. Initially the WTA network selects unit 8 as 
the winner. As soon as the current Is decreases to values lower than 61 nA the 
network selects unit 10 as the winner, despite its input current being lower than 
the current on unit 16 (follow dashed line on Fig. 16.4 from left to right). This 
is a consequence of the effect of lateral excitation that, as mentioned previously, 
tends to favor spatial areas with greater average activity. For the particular val- 
ues of bias current Ib and gate voltage V~ used in this experiment, the network 
will switch to selecting unit 16 as the winner, only when Is decreases to a value 
lower than approximately 41 nA. Once the current on node 8 has decreased 
to zero and has started increasing again, the network will switch back to se- 
lecting unit 8, neglecting unit 10, when Is reaches values greater than 61 hA. 
This experiment, while indirectly demonstrating the hysteretic behavior of the 
network, shows how it tends to select elements close to the previously selected 
winner when the average input activity around the winner is sufficiently high, 
and how it tends to function as a traditional WTA network when the winner 
is an isolated input. 
An application of the discrete-output WTA architecture, that exploits this 
property, can be found in [7]: the authors used this architecture as the last 
computational stage of a focus of ezpansion detection chip. Such system was 
designed for selecting heading direction in case of translatory ego-motion and 
tracking it in time. The authors chose to use this variant of the WTA architec- 
ture in order to account for the a-priori assumption that the heading direction 
position (represented by the winning output node of the WTA circuit) shifts 
smoothly in space. 

WINNER-TAKE-ALL NETWORKS 
371 
2000 
1800 
~ 
1400 
~1200 
-~ 
>1000 
Â¢ 
~ ooo 
c~ soo 
j' 
200 
_- 
_- 
-4 
..... 
4 
~--~_.~ 
~ 
_- 
-4 
-2 
2 
~ 
Unit Position 
j' 
Figure 16.2 
Voltage distribution at the input nodes, obtained through circuit simulations 
of an array of 13 WTA elements with -fb = 5hA, V~ = 0.80V, Z0 = 10nA and all other 
input currents null. The inset is a zoom-in of the data for units i to 6 fitted with the 
exponential function f(x) = 
51.39 â¢ e -0"73(x-0'13) -I- 2.53. 
o 
Ã 
Ã 
~ 
~ 
Unit Position 
o ,~=~O~A 
I 
Im=2OnA 
Umt Posit~on 
Figure 16.3 
Sum of input current and hysteretic current flowing through transistors MI of 
the array. (a) Simulation results for different values of Vr with [b = 5hA, -f0 = 10nA and 
all other input currents null; (b) simulation results for different values of ~0 with ]b = 5nA 
and Vr -- 0.80V. Both sets of data are fitted with exponential functions that have realistic 
space constant coeflicients, for positions more than one unit away from the winner. 

372 
NEUROMORPHIC SYSTEMS ENGINEERING 
16.2.2 
Generalized analog output WTA 
The basic element of the generalized WTA architecture with lateral excitation 
and analog output is shown in Fig. 16.5. The circuit is identical to the one 
shown in Fig. 16.1(a) except for the addition of the extra diode-connected 
transistor M6, which is used to read the output current. For a case similar to 
the one of Fig. 16.3, in which there is only one active input in the center of 
the WTA array, the behavior of the network is similar to the one of the circuit 
previously described (see Fig. 16.6). For more realistic cases though, in which 
there is a structured pattern of input values, the architecture behaves like a non- 
linear filter that enhances the input with maximum amplitude and smoothes the 
rest of the data. Specifically the enhancement effect and the smoothing effect 
are superimposed: the diffusor network implemented with transistors M1 and 
M2 performs the smoothing operation on the input data while the winner-take- 
all network adds the hysteretic feedback current to the input with maximum 
intensity. The net result is that of having output currents that correspond to 
the sum of the smoothed input data with the smoothed hysteretic current. 
Figure 16.7 shows measurement results, obtained from a 25 elements gener- 
alized WTA network. In the example shown all nodes of the array have random 
input values set by external potentiometers that control the gates of the input 
transistors. Initially (Fig. 16.7(a)) unit 11 has maximum input. As shown the 
output corresponds to a smoothed version of the input with the winning node 
enhanced. Subsequently (Fig. 16.7(b)) the input on unit 15 is increased to a 
value greater than the one on unit 11. The WTA selects the winning input, 
modifying the rest of the data accordingly. For the data shown, the bias current 
Ib lies within the range of the input currents. By modifying the value of Ib it 
is possible to intensify (or weaken) the strength of the enhancement effect over 
the smoothing effect, thus emphasizing (or de-emphasizing) the winner-take-all 
nature of the circuit. 
16.3 
APPLICATION EXAMPLE 
As an application example, we have designed a system in which the input 
to the WTA network is provided by a 1-D silicon retina as proposed by [9]. 
The circuit diagram of Fig. 16.8 shows one basic element of the overall 1-D 
array. The bottom part of the circuit implements the adaptive photoreceptor 
circuit with spatial coupling between pixels described in [9]. The current mirror 
implemented by the two p-type transistors in the central part of the circuit 
amplifies the photodiode light induced current. The top part of the circuit 
implements both types of WTA architectures proposed: by connecting Vse~ to 
Vdd we will implement the discrete-output variant of the WTA architecture 
described in section 16.2.1 and obtain positive output currents sourced from 
the p-type output transistor, whereas by connecting Vse~ to ground we will 
implement the generalized WTA architecture described in section 16.2.2 and 
obtain negative output currents sunk from the n-type output transistor. 

WINNER-TAKE-ALL NETWORKS 
373 
~,oi 
~ i 
_c 
! 
45! 
35 
2 
4 
6 
10 
12 
~4 
16 
18 
20 
22 
24 
Unit Position 
Figure 16.4 
Chip data measurements for a 25 element WTA network with Vr -- 0.75V 
and V5 = 
0.9V, where V5 is the gate voltage of a 4#m by 4/~m transistor used to generate 
the bias current Ib. The dashed line shows the selection of the winner as the input current 
on unit 8 is swept from 70 nA to zero. 
{o node 
to nodc 
n* / 
~+1 
.. 
I 
~- 
Iou~ ~ 
= 
. . . . . . . . . . . . . . . . . . .  
Figure 16.5 
Circuit diagram of an element of the generalized WTA architecture with 
lateral excitation and analog output. The circuit dif[ers with the one of Fig.l(a) by one 
transistor (M6). 

374 
NEUROMORPHIC SYSTEMS ENGINEERING 
1.75 
17 
1,6~ 
1.Â£ 
1,5~ 
~ t~ 
8 
1.4~ 
1.z 
~ 3~ : 
1.31 
4 
6 
8 
10 
12 
14 
16 
18 
20 
22 
Unit Position 
Figure 16.6 
Chip data measurements for a 25 element generalized WTA network, with 
[5 : 
21hA, V11 = 0.gv and V~#11 = 0.~SV. The output current has been converted into 
voltage using an olin-chip sense amplifier. 
3.4 
2Â¸8 
2Â¸1' 
' 
" 
, 
~ 
/ 
~ 
~'*', ,, 
, , 
, 
, 
3.4 
3. 
28t/ 
', 
i' 
2 
4 
6 
8 
10 
12 
14 
16 
t8 
20 
22 
2 
4 
6 
8 
10 
12 
14 
16 
18 
20 
22 
Unit P~it~on 
Unit Posi~on 
Figure 16.7 
Data measurements of input versus output currents for a 25 element gener- 
alized WTA network with Vr = 2.25V and [5 = 21hA. The solid line represents the output 
of the network, while the dashed line represents its input. In (a) the input on unit 15 is set 
to V15 = 0.89V and the input to unit 11 (set to Vll = 0.91V) is the maximum input 
value; in (b) unit 15 (set to V15 = 0.95V) is the maximum value, with the rest of the input 
values unchanged. 
Note how the output of unit 15 is enhanced (and the rest of the data 
smoothed); a normal smoothing network would have decreased this value, possibly making 
it loose its winning status. 

WINNER-TAKE-ALL NETWORKS 
375 
The spatio-temporal filtering properties of the silicon retina designed allow 
the circuit to extract edges at different spatial frequencies. By changing the 
bias voltages Vgp and V~p it is possible to set the spatial-frequency tuning of 
the filter (see Fig. 16.9). The WTA network will then select the edge with 
spatial-frequency that elicits the strongest response and track it as it moves. 
Depending on the value of Vscl, the output of the chip will correspond either 
to a single active pixel in the output array (representing the winner) or to a 
smoothed contour map of the input image with the strongest edge enhanced. 
The chip, fabricated using a 2#m n-well CMOS process provided by MOSIS, 
contains a 1-D array of 25 pixels. 
16.4 
CONCLUSIONS 
We have presented two circuits that implement current-mode winner-take-all 
networks with lateral excitation. Their behavior was analyzed and compared 
to the one of similar circuits previously proposed. As an application example of 
these WTA networks, we have presented an analog CMOS VLSI system, with 
on-chip adaptive photoreceptors, for selecting the most salient edges present in 
an image. As demonstrated, the circuits are extremely compact and suitable 
for integration with analog VLSI neuromorphic systems. 
Acknowledgments 
I would like to thank Christof Koch for his support, Ernst Niebur and Tonia Morris for 
their constructive comments and Timothy Horiuchi for the insightful discussions on 
winner-take-all circuits. This work was supported in part by the Center for Neuromor- 
phic Systems Engineering as a part of the National Science Foundation Engineering 
Research Center Program; and by the California Trade and Commerce Agency, Of- 
rice of Strategic Technology. Fabrication of the integrated circuits was provided by 
MOSIS. 
References 
[1] K. A. Boahen and A. G. Andreou. A contrast sensitive silicon retina with 
reciprocal synapses. Advances in Neural Information Processing Systems, 
4:764-772, 1992. 
[2] S.P. DeWeerth. Analog VLSI circuits for stimulus localization and centroid 
computation. International Journal of Computer Vision, 8(3):191-202, 
1992. 
[3] S. P. DeWeerth and T. G. Morris. Analog VLSI circuits for primitive 
sensory attention. In Proceedings of the IEEE International Symposium 
on Circuits and Systems, volume 6, pages 507-510, 1994. 
[4] S. P. DeWeerth and T. G. Morris. CMOS current mode winner-take-all 
circuit with distributed hysteresis. Electronics Letters, 31(13):1051-1053, 
1995. 

376 
NEUROMORPHIC SYSTEMS ENGINEERING 
F- . . . . . . . . . . .  
~ 
~ 
-
-
I
 
I "~ 
"~' 1 
I 
I 
I 
~. 
] 
,' 
I 
I 
~ 
.
.
.
.
.
.
.
 
~ 
I 
~ 
r- . . . . . . . . . .  
~ 
~ 
~-'~ ,, 
I 
~ 
~,,.:.~ 
I 
I 
I 
--" I,:,~-~ 
-
-
,
 
, ~; 
~ 
~2 
~ 
.. 
~ . . . . . . . . . . .  
Figure 16.8 
Basic cell for a I-D adaptive retina chip connected to the WTA architecture. 
The bottom part of the figure contains the adaptive photoreceptor circuit. The top part 
of the figure contains a circuit that implements both types of WTA networks described in 
section 16.2 (depending on the value of VseZ). The current mirror in the middle part of the 
figure is used to amplify the output current of the adaptive photoreceptor. 
~ -14(~ 
~14 065 
-5 
4 
-3 
-2 
-I 
0 
1 
2 
3 
4 
5 
Unit Posit~n 
Figure 16.9 
Simulation results of a I-D array of adaptive photoreceptor circuits with 
spatial coupling. The curves plotted represent the spatial impulse response of the system, 
obtained by setting all photodiode currents of the array to 100pA, except for the current 
of unit zero, which was set to 300pA. The photoreceptor bias voltage was set to Vb~as ---- 
0.4V, the p-type coupling transistor gate voltage was set to V~p = 0.gV and the n-type 
coupling transistor gate voltage was varied. As shown, it is possible to obtain center-surround 
convolution kernels with different frequency selectivities by changing the values of these gate 
voltages. 

WINNER-TAKE-ALL NETWORKS 
377 
[5] R. Douglas, M. Mahowald, and C. Mead. Neuromorphic analogue VLSI. 
Annu. Rev. Neurosci., 18:255-281, 1995. 
[6] T. Horiuchi, W. Bair, B. Bishofberger, J. Lazzaro, and C. Koch. Comput- 
ing motion using analog VLSI chips: an experimental comparison among 
different approaches. International Journal of Computer Vision, 8:203- 
216, 1992. 
[7] G. Indiveri, J. Kramer, and C. Koch. System implementations of analog 
VLSI velocity sensors. IEEE Micro, 16(5/:40-49, October 1996. 
[8] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead. Winner- 
take-all networks of O(n) complexity. In D.S. Touretzky, editor, Advances 
in neural information processing systems, volume 2, pages 703-711, San 
Mateo - CA, 1989. Morgan Kaufmann. 
[9] S. Liu and K. Boahen. Adaptive retina with center-surround receptive field. 
In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, editors, Advances in 
neural information processing systems, volume 8. MIT Press, 1996. 
[10] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
[11] E. Niebur and C. Koch. Control of selective visual attention: modeling the 
"where" pathway. In D.S. Touretzky, M.C. Mozer, and M.E. Hasselmo, 
editors, Advances in neural information processing systems, volume 8. MIT 
Press, 1996. 
I12] J. A. Starzyk and X. Fang. CMOS current mode winner-take-all cir- 
cult with both excitatory and inhibitory feedback. Electronic Letters, 
29(10):908-910, May 1993. 
[13] M. Tartagni and P. Perona. Computing centroids in current-mode tech- 
nique. Electronics Letters, 29(21):1811-1813, October 1993. 
[14] E. Vittoz and X. Arreguit. Linear networks based on transistors. Elec- 
tronics Letters, 29(3):297 299, February 1993. 

V Neuromorphic Learning 

17 
NEUROMORPHIC LEARNING 
VLSI SYSTEMS: A SURVEY 
Gert Cauwenberghs 
17.1 INTRODUCTION 
Carver Mead introduced "neuromorphic engineering" [1] as an interdisciplinary 
approach to the design of biologically inspired neural information processing 
systems, whereby neurophysiological models of perception and information pro- 
cessing in biological systems are mapped onto analog VLSI systems that not 
only emulate their functions but also resemble their structure [18]. The mo- 
tivation for emulating neural function and structure in analog VLSI is the 
realization that challenging tasks of perception, classification, association and 
control successfully performed by living organisms can only be accomplished 
in artificial systems by using an implementation medium that matches their 
structure and organization. 
Essential to neuromorphic systems are mechanisms of adaptation and learn- 
ing, modeled after the "plasticity" of synapses and neural structure in biological 
systems [25, 4]. Learning can be broadly defined as a special case of adapta- 
tion whereby past experience is used effectively in readjusting the system re- 
sponse to previously unseen, although similar, stimuli. Based on the nature 
and availability of a training feedback signal, learning algorithms for artificial 
neural networks fall under three broad categories: unsupervised, supervised 
and reward/punishment (reinforcement). Physiological experiments have re- 
vealed plasticity mechanisms in biology that correspond to Hebbian unsuper- 
vised learning [19], and classical (pavlovian) conditioning [17, 22] characteristic 
of reinforcement learning. 

382 
NEUROMORPHIC SYSTEMS ENGINEERING 
Mechanisms of adaptation and learning also provide a means to compen- 
sate for analog imperfections in the physical implementation of neuromorphic 
systems, and fluctuations and uncertainties in the environment in which it op- 
erates. To this end, it is crucial that the learning be continuously performed 
on the system in operation. This enables the system to be functionally self- 
contained, and to adapt continuously to the environment in which they operate. 
For neuromorphic systems which involve a large number of parameters such as 
synapses in a densely connected neural network, it is imperative that the learn- 
ing functions be an integral part of the hardware, implemented locally and 
interfacing directly with the synaptic functions. 
Practical limits of integrated implementation of learning functions are im- 
posed by the degree of locality implied by the learning rule, and the available 
memory bandwidth and fanout provided by the technology. This is an im- 
portant point to consider in the design, and determines whether an electronic, 
optical, or hybrid implementation is most suited for the targeted application. 
A very important consideration as well is the need for locally storing the analog 
or digital parameter values, to retain the information being extracted during 
learning. Not surprisingly, technological issues of adaptation and memory are 
directly related, and both need to be addressed concurrently. 
A vast research literature is dedicated to various styles of neural hardware 
implementations with provisions for learning, some of it with integrated learn- 
ing functions. A selection of the literature (which is bound to be incomplete 
even at the time of printing!) is included in the list of references below. Some 
examples of early implementations of neural systems with integrated adap- 
tation and learning functions can be found in edited volumes such as [8, 9] 
and [10], in conference proceedings such as NIPS, IJCNN (ICNN/WCNN) and 
ISCAS, and in special and regular issues of journals such as IEEE Transactions 
on Neural Networks (May 1992 and 1993 [12, 13]), IEEE Micro (Micro-Neuro 
special issues) and Kluwer's International Journal of Analog Integrated Cir- 
cuits and Signal Processing [14, 15]. The exposition below will serve as a brief 
description of a limited cross-section of research in the field over the last decade 
(mainly focusing on analog VLSI systems), as well as a general coverage of the 
important issues. 
17.2 
ADAPTATION AND LEARNING 
Definitions for the terms adaptation and learning come in several varieties, dif- 
fering with the particular discipline in which it is formulated, such as cognitive 
science, neuroscience, neural computation, artificial intelligence, information 
theory, and control theory. 
From a system level perspective, a general framework for adaptation and 
learning is depicted in Figure 17.1 [18]. A system with adjustable parameters p~ 
(vector p) interacts with the environment through sensory inputs and activation 
outputs. An adaptive element, either internal or external to the system, adjusts 
the parameters of the system to "optimize" some performance index that is 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
383 
"P  LI 
s sTEM 
~(p) 
- 
ELEMENT 
Figure 17.1 
Adaptation and learning in an information processing system by adjusting the 
analog system parameters Pi to optimize a performance index ~'(p). The system interacts 
with the environment through its sensory inputs and activation outputs. 
either defined or implied in relation to the system and its interaction with 
the environment. In most models of learning and adaptation, the measure 
of performance is quantified either as an error index $(p) which needs to be 
minimized: 
p = argmin Â£(p) 
or, equivalently, a quality index which needs to be maximized. The optimization 
is subject to suitable constraints that have to do with physical limits on the 
system as well as other requirements on the system and the way it interacts 
with the environment. 
What distinguishes learning from more general forms of adaptation is the 
way in which the system uses past experience in trying to respond effectively 
to previously unseen, although similar, input stimuli. The distinct objective in 
learning is to generalize beyond the specifics of the presented input samples, 
and minimize the expected value of E(p) from the underlying statistics of the 
training samples: 
p = argmin E(E(p)) . 
Based on the nature and availability of a training feedback signal in the for- 
mulation of E(~(p)), learning algorithms for artificial neural networks (ANNs) 
fall under three broad categories: supervised [30], unsupervised [27], and re- 
ward/punishment (reinforcement)[2]. 
Supervised Learning [19]-[24] assumes that a "teacher" is continuously 
target 
available to produce target values Yk 
(t) for the outputs yk(t), whereby 
the (instantaneous) error index is quantified as the distance between ac- 
tual and target outputs 
$(p;t) = E 
target 
lY~ 
(t) - ye(t)l ~ , 
(17.1) 
k 

s!sariauJs s!q,l. 'smo:~sJs o*!:m!aoss~-oaoaori oA!:ld~p~ Su!z!u~$ao-j[os Â£ii~u 
-aalu! al~aouo$ oa soanaao:~!qaa~ s>iao~aou snoI.IVA ql!m sX~At Â£u~m m 
pau!qmoa oq uva saria~oadd~ Sm.ua~o I pos!Aaadns pu~ pas!AaodnsuFl :sp!aqÂ£ H 
â¢ [aoaderia lxou oq:~] u! UOA!$ OaR aIdmvxo uIOlSÂ£s ~ pu~ IgqA $oI~u~ m 
mo~sÂ£s Su!ua~o I auamoaaoju~oa uo gWla~ "aauoSaoauoa jo paods pas~oaa 
-u~ zoj uoDvmaoju ~ auo~p~a$ I~aoaaoa Su!sn [I~] ,,$u!mm~a$oad a~m~uÂ£p 
aDsvno q poau~ap% pu~ 'uoD~aoa ~ Â£a~iod i~m~ldo aoj oa~ds oams oqa uo 
uo~aaunj anI,* v Sulsn [6g] ~u~uJ~I-b '[g] jo asia i~D~ds ~ ~ [~g] (Y)G& 
ao ,,~u~uava[ aauaaa~p amD,, apnlau~ Â£aga '[0~ 'gg] aaua$~IlaaU~ I~Pg~aav 
u~ ~u!ua~a I jo slopom oa paaNoa Â£IOSOIO 'saa~aodo 1~ RaN~ u~ auomoaDua 
aql pu~ maasXs aq~ jo sdNsuODVlOa i~snva aRl uo suo~admnssv aomd ou 
a~vm qaN~ luamu$~ss~ ~paaa jo sms~u~qaam i~uaalu~ ash odXa ~u~ua~a I 
auamaaaquDa aqa 1o smqlVo$IV 
'pa~oa ao Â£ai~uad aqa Xq paa~a~pu~ 
ssaaans ao aanHg maasds aqa oi ~u~pvaI as~d oR1 u~ suoDa~ aiq~suodsoa 
oa a~paaa aadoad jo auamu$~sv aql s~ Su~uavo I jo adXa sNa u~ Â£alna~p 
aq& 
"sandano maasXs aqa aq pagDads aaSam v lnoqa!~ 'sauamqs~und 
puv spav~aa poXNo p 'omD-oloaas~p oa paa~m~ I s~ oau~maqaad maasXs uo 
~a~qpaoj i~uaoaxa oIqNNA~ oqa samnss~ [6g]@I] ~u!uaeoI ~uomoaamu!o~ 
"[saaadmta ~uDtoIIo j aria jo ouo] u! paq!aas 
-op s! aaz~au~nb aolaoa Â£a~u~q ~u~ua~a[ IS~A V "[0~] Â£aoaqa oau~uosoa 
oa~ad~p~ pu~ '[8g] aaz~au~nb aoaaaa ~ u~ Su~aaasnIa su~am-~ '[98 'Zg] sa~a 
-omam oaDv!aoss~-oanv '~ao~aau I~anau Su~z!u~$aO-jla~ ~ u ! [gg] Su~ua~a I 
u~qqaH apnIau~ sanb~uRaaa Su~ua~aI PaSDaadnsun I~a~dX& "[I~] sass~Ia 
lndano aqa jo qa~uo I uoDd~aaso p OSVaaa~ oq~ oz~m~u~m o1 (Â£DuoI~a~nba) 
ao '[6g] sassvIa andano olaaas~p aN1 puv sandu~ ~oi~u~ aN] uoamaaq uoD~m 
-ao~u~ i~n~nm aqa az~m~x~m 
oa s~ aoqaD 'smaoa aDoaooqa-uo~a~maqu ~ m 
possaadxo aq u~a ~assNa uoa~laq saV~puno q aqa ~uDsn[p~ aq uo~aaa~aa 
oR& "$mssaaoad uoD~maqu ~ auanbasqns aq uoDmuosaadoa mvp olqm~ns 
aaom as~aaRao u~ ao uomsaad~oa m~P jo maq a~os aa~nbaa 2[auaaaRu ~ 
qaN~ s~s'~a aq papua~u~ aa~ adJa sNa jo saag~SSVlO "m~P oqa jo sa~ls~vls 
SU~Xlaopun aqa uo pasvq sandu~ Â£j~ssvia oa sadmaaa~ pue 'aoqa~aa ieuaaa 
-xo u~ moaj ~a~qpaa 1 Xu~ amnss~ aou soop [0g]-[gg] ~u!uaeoI Pos!aaodnsun 
"[ao:~d~ria axou oril] 
m. pa:~uosaad s! sa!tu~udp auoaanaaa ql~ ISqA u! ~u:ua~aI POS!aaodns jo 
ald~xa maas~s V '[>8 '~gJ salq~a~a aa~as aqa u~ sa~m~uJp auaaanaaa Ra~ 
saan~anaas xaldmoa aaom oa puv '[Og] saanaanaas pav~aqpaaj i~aau~ aaom 
oa papuaaxa aq uva RaN~ pu~ 'NNV pa~aqpaaj aoÂ£NDinm v uo (I'ZI) jo 
auaasap aua~pm$ oa po~Idd~ uoD~Duaaa~p 1o alna u~qa aq~ Â£iaaDaa~a s~ 
qaN~ '[Ig] uoD~$~doad~avq s~ smqa~ao$iv Nu~ua~o I ii ~ jo aNndod asom 
oR& "au~i-uo paz~mDdo puv paaenI~aa oq uva 's[~u~s Su~u~aa aa~am aqa 
jo smaaa u~ pagBu~nb Â£[aaaa~p 'xapu~ aau~maqaad oR1 pu~ paugop [ia~ 
s~ Nsm Nu~ua~a I aqa aau~s 'auamaldm ~ oa ~u~ua~a I 1o asia aso~s~a aqa asuas 
~ u~ s~ Su~ua~a I pos~Aaodn S '0 < a ~aou qa~ a~aaam aau~as~p v Sumn 
DNII:I~HNIDN~ SIN~LI,$XS OlHdltOINOHFKtN 
p{~ 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
385 
reaches beyond neural nets in the restricted sense of what is convention- 
ally known as ANNs, and includes fuzzy neural systems [41, 42, 43] as well 
as "hierarchical mixture of experts" models trained with the expectation- 
maximization algorithm [44]. In both cases, internal structure is learned 
using unsupervised clustering techniques based on the input statistics, 
and the output structure is trained through (gradient-based and other) 
supervised learning. 
17.3 
TECHNOLOGY 
Biological neural systems are built out of "wetware" components in an imple- 
mentation medium which is necessarily different from technologies available to 
the implementation of artificial computing systems, such as semiconductors and 
optical propagation media. 
The neuromorphic 
engineering approach extends 
the functionality and structure of biological systems to artificial systems built 
with components and architectures that closely resemble their biological coun- 
terparts at all levels, transparent to differences in technology. Still, the physical 
limits on size, density and connectivity depend strongly on the technology used. 
Most neural hardware implementations use VLSI technology, which is func- 
tionally highly versatile but mostly restricted to two dimensions. 
The planar 
nature of VLSI technology is not necessarily a restriction for neural implemen- 
tations since neural structures such as in the cerebral cortex are mostly two- 
dimensional as well-- after all the brain is itself a folded 2-D structure. Optical 
free-space interconnects, on the other hand, allow synaptic densities presently 
unavailable in state-of-the-art VLSI technology. 
Hybrid opto-electronic sys- 
tems combine the technological advantages of both worlds, with functionally 
rich local VLSI processing and global optical interconnects. 
For learning and adaptation, a central issue in all implementation technolo- 
gies is the local storage of synaptic parameters. 
This issue, together with the 
means of incrementally adapting the stored parameters, is addressed below 
in particular detail. For brevity, the exposition focuses mainly on electronic 
implementations in analog VLSI technology. 
17.3.1 VLSI Subthreshold MOS Technology 
MOS transistors operating in the subthreshold region [25] are attractive for use 
in medium-speed, medium-accuracy analog VLSI processing, because of the 
low current levels and the exponential current-voltage characteristics that span 
a wide dynamic range of currents [48] (roughly from 100 fA to 100 nA for a 
square device in 2 #m CMOS technology at room temperature). Subthreshold 
MOS transistors provide a clear "neuromorph" [1], since their exponential I-V 
characteristics closely resemble the carrier transport though cell membranes in 
biological neural systems, as governed by the same Boltzman statistics [46]. 
The exponential characteristics provide a variety of subthreshold MOS circuit 
topologies that serve as useful computational primitives (such as nonlinear con- 
ductances, sigmoid nonlinearities, etc.) for compact analog VLSI implementa- 

386 
NEUROMORPHIC SYSTEMS ENGINEERING 
/adapt 
AQadapt 
WstÂ°red 
G DII B 
-
-
 
- 
Figure 17.2 
Adaptation and memory in analog VLSI: storage cell with charge buffer. 
tion of neural systems [18]. Of particular interest are translinear subthreshold 
MOS circuits, derived from similar bipolar circuits [48]. They are based on the 
exponential nature of current-voltage relationships, and offer attractive com- 
pact implementations of product and division operations in VLSI. 
17.3.2 Adaptation and Memory 
Learning in analog VLSI systems is inherently coupled with the problem of 
storage of analog information, since after learning it is most often desirable to 
retain the learned weights for an extended period of time. The same is true 
for biological neural systems, and mechanisms of plasticity for short-term and 
long-term synaptic storage are not yet clearly understood. 
In VLSI, analog weights are conveniently stored as charge or voltage on a 
capacitor. A capacitive memory is generically depicted in Figure 17.2. The 
stored weight charge is preserved when brought in contact with the gate of an 
MOS transistor, which serves as a buffer between weight storage and the imple- 
mentation of the synaptic function. An adaptive element in contact with the 
capacitor updates the stored weight in the form of discrete charge increments 
1 
Ystored(t q- /kt) : Vstored(t) if- ~A(~adapt(t) 
(17.2) 
or, equivalently, a continuous current supplying a derivative 
~tVstored(t) = 1 
~Iadapt(t) 
(17.3) 
where AQ~dapt (t) = f~+At/adapt 
(t')dt t. 
On itself, a floating gate capacitor is a near-perfect memory. However, leak- 
age and spontaneous decay of the weights result when the capacitor is in volatile 
contact with the adaptive element, such as through drain or source terminals 
of MOS transistors. This distinguishes volatile from non-volatile storage VLSI 
technology. An excellent review of analog memories for neural computation is 
given in [12]. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
387 
Non-volatile Memories [50]-[61] contain adaptive elements that interface 
with the floating gate capacitor by capacitive coupling across an insu- 
lating oxide. In standard CMOS VLSI technologies, charge transport 
through the oxide is typically controlled by tunneling [85, 51, 50, 52], hot 
electron injection [60] or UV-activated conduction [180, 53, 57, 7]. Flash 
memories offer fast adaptation rates (msecs) and long retention times 
(years) without the need for high programming voltages or UV light, but 
are not standardly available in CMOS processes. 
Volatile Memories [50],[62]-[67] offer fast adaptation rates and instanta- 
neous reprogramming of the parameter values, using a voltage-controlled 
ohmic connection to the capacitor in the form of MOS switches and 
switched current sources. 
A leakage current results from the reverse 
diode formed between source and drain diffusions and bulk connection 
of a switch transistor. The leakage typically resticts the retention time 
of the memory to the msec range, adequate for short-term storage. An 
active refresh mechanism is required for long-term storage [50],[63]-[10]. 
An adaptive element which combines active refresh storage and incremen- 
tal adaptation, and which allows a random-access read and write digital 
interface, is described in [the next chapter]. 
Other implementations frequently use local or external digital storage of the 
parameters, combined with either local or multiplexed D/A conversion. This 
solution is less attractive for large-scale neural processors with local learning 
functions that require incremental adaptation of the parameters, since then 
the increment would need to be performed in digital as well. Both volatile 
and non-volatile analog memories allow incremental updates in direct analog 
format, according to (17.2) or (17.3). 
The non-volatile solution is more attractive than volatile alternatives when 
long-term storage is a more pressing concern than speed of adaptation and 
flexibility of programming. The volatile scheme is particularly useful in multi- 
plexed hardware implementations for multi-purpose applications or to realize 
virtual larger-scale systems, requiring frequent reloading of large blocks of par- 
tial weight matrices. This could be done with an external digital cache memory 
and an array of A/D/A :onverters for bi-directional digital read and write ac- 
cess to the synaptic array [6]. Random-access memory addressing in digital 
format is on itself a valuable feature for system-level interfacing. 
17.3.3 Emerging Technologies 
Innovation and continued progress in information technology benefits the design 
of learning neural systems of larger size and better performance, as it benefits 
other information processing systems. Some relatively new developments in 
VLSI include micro-electromechanical systems (MEMS) [68], wafer-scale in- 
tegration [142, 144], chip-scale packaging [69], and silicon-on-insulator (SOl) 
integrated circuit fabrication [70, 71]. The latter is of special interest to analog 

388 
NEUROMORPHIC SYSTEMS ENGINEERING 
storage, because significant reduction of leakage currents due to bulk reverse 
diodes in MOS switches allows longer retention times of capacitive memories. 
Continued technology developments in optical and optoelectronic informa- 
tion processing in combination with mature VLSI technology hold the potential 
for significant performance improvements in artificial neural information pro- 
cessing systems [151]-[159], promising massive inter-chip connectivity as needed 
for larger size neural networks. High-density optical storage and adaptation 
for integrated learning could be achieved in 3-D optical media such as photo- 
refractive crystals. 
17.4 
ARCHITECTURE 
Learning algorithms that are efficiently implemented on general-purpose digital 
computers do not necessarily map efficiently onto analog VLSI hardware. The 
good news is that the converse is also true, as it is well known that special- 
purpose processors tuned to a given task easily outperform most general- 
purpose computing engines, on that particular task. 
From the perspective 
of computational efficiency, it is therefore important to closely coordinate the 
design of algorithms and corresponding VLSI architecture to ensure an optimal 
match. 
Important guidelines in efficiency of computation dictate the usual principles 
commonly taught in modern VLSI design: locality, scalability, and parallelism. 
The principle of locality confines intensive computations to the cell level, and 
restricts global operations to nearest-neighbor interactions. In addition, certain 
scalar global operations which can be easily performed with a single common 
wire in analog VLSI technology are allowed, such as global summing of currents 
or charges, and global communication of voltage-coded variables. Scalability 
implies that the implemented algorithms cannot scale stronger than second or- 
der in a linear parameter such as the number of neurons, since nothing more 
complex than a 2-D array can be implemented on an extended scale in planar 
VLSI technology. Parallelism in this context implies that the number of opera- 
tions performed concurrently at any given time scales linearly with the number 
of cells. 
Even if the learning algorithm supports a parallel and scalable architecture 
suitable for analog VLSI implementation, inaccuracies in the implementation 
of the learning functions may significantly affect the performance of the trained 
system. Neuromorphic principles call for a distributed architecture not only for 
the network of neurons but also to implement the learning functions, robust to 
localized errors in the implementation. 
17.4.1 
Incremental Outer-Product Learning in Distributed Systems 
For networks with distributed neurons such as linear and multilayer percep- 
trons [21] 
xi = f(~-~ pijxj) 
(17.4) 
J 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
389 
gradient descent of an LMS error functional g defined on the output neurons 
x~ ut gives rise to incremental outer-product learning rules of the form 
Apij = r I xjei 
(17.5) 
where the backpropagation of the error variables ei is derived by application of 
the chain rule for differentiation as [30] 
05 
e~ ut 
=: 
--]~ Ox~Ut 
oj 
-_ j; 
i 
(17.6) 
where f~ denotes the derivative of the function f(.) evaluated at its argument 
in (17.4). Outer-product rules of the form (17.5) are local: synaptic updates 
are constructed from intersecting variables at the location of the synapses. 
The general class of learning algorithms of the incremental outer-product type 
include 
Supervised Learning: 
the delta rule [19] and backpropagation [21] for 
supervised learning in linear or multilayer feedforward perceptrons with a 
functional (17.1). Also included, with stochastic rather than deterministic 
neurons, are Boltzman learning in networks of stochastic neurons [22, 72], 
and pulse firing neural nets [90]. 
Unsupervised Learning: 
hebbian learning [25], where ei = f~x~ corre- 
sponding to a functional g ~ - ~i x~ 2. The k-means clustering algorithm 
for learning vector quantization (LVQ) [28] is a special case of the latter, 
where the nonlinearity in the output layer fk selects a single winner across 
all outputs k. Kohonen topology-preserving maps [27] further include the 
neighbors of the winner k + 1 into the learning updates. Learning in ART 
networks [30] also fits in this category although it is slightly more in- 
volved. Learning in Hopfield networks [26] is hebbian in slightly modified 
form. 
Hybrids and Variants: 
fuzzy maps, hetero-associative neural networks, 
radial basis networks, etc. which conform to the general structure of 
Eqns. (17.4)-(17.6) and their variants and combinations. 
Reinforcement Learning: 
The reinforcement learning updates for both 
the action network and the adaptive critic in [2] are of the general in- 
cremental outer-product form (17.5), although modulated with a global 
(common) reinforcement signal, and low-pass filtered for credit assign- 
ment back in time. See [the next chapter] for more details on the equiva- 
lent gradient-descent outer-product formulation. An outer-product VLSI 
implementation is described in [9]. 

390 
NEUROMORPHIC SYSTEMS ENGINEERING 
.
.
.
.
 
.
.
.
.
 
ei,/ ~ 
X i 
e i 
Figure 17.3 
Incremental outer-product learning. 
model; 
(b) Simplified VLSI architecture. 
(a) Feedforward and backpropagation 
Since all of the above learning algorithm share essentially the same incre- 
mental outer-product learning rule, they can be cast into the same general 
VLSI architecture depicted in Figure 17.3. Clearly, this architecture exhibits 
the desirable properties of locality, parallelism and scalability. Forward and 
backward signal paths xj and e~ traverse in horizontal and vertical directions 
through the array of synapse cells p~j. The neuron nonlinearity f(.) and its 
derivative are implemented at the output periphery the array. Several layers 
of this structure can be cascaded in alternating horizontal and vertical direc- 
tions to form multi-layer perceptrons. The array architecture of Figure 17.3 (b) 
forms the basis for many of the implemented VLSI learning systems [72]-[104]. 
One example, described in [7], arguably contains the densest VLSI array for 
general outer-product learning developed to date, using only two transistors for 
synapse and learning operations per cell. An array of single-transistor learning 
synapses for certain classes of learning is presented in [60]. 
Digital VLSI implementations [140]-[150] differ from the analog architecture 
mainly in that contributions to the summations in (17.4) and (17.6) cannot 
be accumulated onto a single line. Global summations are most commonly 
implemented using a systolic array architecture. 
17.4.2 
Localized Outer-Product Learning in Ceflular Neural Systems 
Notice that the fully interconnected architecture of Figure 17.3 (b) becomes 
inefficient when the network that it implements has sparse connectivity. A lim- 
iting case of sparsely interconnected networks are cellular neural networks [106], 
in which neurons only interact with their immediate neighbors, conveniently ar- 
ranged on a 2-D grid. Since the synaptic connections in networks of this type 
are only peripheral, the implementation architecture is determined directly by 
the topology of the neurons in relation with their neighbors. The synapse and 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
391 
learning functions are integrated at the neuron level, rather than distributed 
over an array as in Figure 17.3 (b). 
Other than that, the same principles 
hold, and rules of the outer-product type as illustrated in Figure 17.3 (a) are 
implemented locally at the neuron inter-cell level [105]-[108]. 
17.4.3 Model-Free Learning Approaches 
Although model-based approaches for learning such as the outerproduct 
learning models described above are fairly robust to mismatches in the im- 
plementation of the learning functions owing to their distributed architec- 
ture [119, 120, 123, 124], the same can not be said a priori of more general 
classes of learning which do not fit the outerproduct type. This is particu- 
larly so for recurrent neural networks with hidden internal dynamics for which 
learning complexity rises sharply with the number of parameters [23, 24], or 
for more complex systems of which a model is difficult to derive or unknown 
to the learning element. Model-free approaches to learning [12] do not assume 
a particular model for the system nor the environment in which it operates, 
and derive parameter updates Ap~ by physically probing the dependency of 
the performance index Â£ on the parameters p~ through perturbations ~ri on the 
parameters. 
The term "model-free" pertains to the learning, and not necessarily to the 
structure of the system itself being adapted, which can be anything and which 
clearly is parametric. The main advantage of model-free learning is that it 
leaves tremendous freedom in configuring the system, which is allowed to change 
structurally on-line as learning progresses, without the need to compile models. 
This is particularly useful for training reconfigurable architectures [136, 112]. 
The insensitivity of learning performance to inaccuracies in the implemented 
system, and the ability to learn systems with intractible models, are direct 
benefits of model-free learning. An additional benefit of stochastic perturba- 
tive learning approaches seems to be that the synaptic noise thus introduced 
improves generalization performance of the learned system [121]. 
Variants on perturbative model-free learning use some limited model infor- 
mation to train feedforward multilayer ANNs more effectively [14, 132, 134]. 
The question of how much model information can be reliably used is impor- 
tant, although truly model-free approaches are most generally applicable and 
expandable, and their performance does not significantly suffer from the lack 
of complete gradient information on the error Â£ as some asymptotic theory 
establishes [4]. 
The model-free nature of learning applies to general learning tasks beyond 
the traditionally supervised and unsupervised, and can be extended to rein- 
forcement learning. 
An extensive study of model-free supervised and rein- 
forcement learning architectures with examples of analog VLSI systems is the 
subject of [the next chapter]. 

392 
NEUROMORPHIC SYSTEMS ENGINEERING 
17.5 
SYSTEMS 
Several examples of adaptive and/or learning VLSI systems with applications 
in vision, speech, signal processing, pattern recognition, communications, con- 
trol and physics are included in the references [171]-[203]. This list is by no 
means complete, and the spectrum of applications will likely expand as the new 
application areas are discovered and research advances create new ways of using 
adaptation and learning in the design of intelligent neuromorphic information 
processing systems. 
Covering such diverse range of disciplines across neurobiology, artificial in- 
telligence, cognitive science, information theory, etc., research on learning sys- 
tems is bound to develop further as different concepts and experimental evi- 
dence combine to bridge the gap between bottom-up and top-down modeling 
approaches, towards the engineering of truly intelligent autonomous learning 
systems, and towards a better understanding of learning mechanisms in biolog- 
ical neural systems at different levels of abstraction. 
References 
[1] C. A. Mead. Neuromorphic electronic systems. In Proceedings of the 
IEEE, volume 78-10, pages 1629-1639, 1990. 
[2] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
NEUROBIOLOGICAL INSPIRATION 
[3] G. M. Shepherd. The Synaptic Organization of the Brain. Oxford Univ. 
Press, New York, 3 edition, 1992. 
[4] P. Churchland and T. Sejnowski. The Computational Brain. MIT Press, 
1990. 
[5] S. R. Kelso and T. H. Brown. Differential conditioning of associative 
synaptic enhancement in hippocampal brain slices. Science, 232:85-87, 
1986. 
[6] R. D. Hawkins, T. W. Abrams, T. J. Carew, and E. R. Kandell. A cel- 
lular mechanism of classical conditioning in aplysia: Activity-dependent 
amplification of presynaptic facilitation. Science, 219:400-405, 1983. 
[7] P. R. Montague, P. Dayan, C. Person, and T. J. Sejnowski. Bee foraging 
in uncertain environments using predictive hebbian learning. 
Nature, 
377(6551):725 728, 1996. 
[81 
EDITED BOOK VOLUMES, JOURNAL ISSUES AND REVIEWS 
C. A. Mead and M. Ismail, editors. Analog VLSI Implementation of 
Neural Systems. Kluwer, Norwell, MA, 1989. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
393 
[9] N. Morgan, editor. Artificial Neural Networks: Electronic Implementa- 
tions. IEEE Computer Society Press, CA, Los Alamitos, 1990. 
[10] E. SÂ£nchez-Sinencio and C. Lau, editors. 
Artificial Neural Networks: 
Electronic Implementations. IEEE Computer Society Press, 1992. 
[11] M.A. Jabri, R.J. Coggins, and B.G. Flower. Adaptive Analog VLSI Neural 
Systems. Chapman Hall, London, UK, 1996. 
[12] E. Sgnchez-Sinencio and R. Newcomb. Special issue on neural network 
hardware. In IEEE Transactions on Neural Networks, volume 3-3. IEEE 
Press, 1992. 
[13] E. S~nchez-Sinencio and R. Newcomb. Special issue on neural network 
hardware. In IEEE Transactions on Neural Networks, volume 4-3. IEEE 
Press, 1993. 
[14] T. S. Lande, editor. Special Issue on Neuromorphic Engineering. Int. J. 
Analog Int. Circ. Signal Proc., March 1997. 
[15] M. Bayoumi G. Cauwenberghs and E. SÂ£nchez-Sinencio, editors. Special 
Issue on Learnin9 in Silicon. Int. J. Analog Int. Circ. Signal Proc., To 
appear. 
[16] G. Cauwenberghs et. al. Learning on silicon. In special session, Proc. Int. 
Symp. Circuits and Systems, Hong Kong, June 1997. 
[17] H. P. Graf and L. D. Jackel. Analog electronic neural network circuits. 
IEEE Circuits and Devices Mag. , 5:44 49, 1989. 
[18] G. Cauwenberghs. Adaptation, learning and storage in analog VLSI. In 
Proceedings of the Ninth Annual IEEE International ASIC Conference, 
Rochester, NY, September 1996. 
LEARNING MODELS 
Supervised Learning 
[19] B. Widrow and M. E. Hoff. Adaptive switching circuits. IRE WESCON 
Convention Record, 4:96-104, 1960. 
[20] P. Werbos. Beyond regression: New tools for prediction and analysis in 
the behavioral sciences. In The Roots of Backpropagation. Wiley, New 
York, 1993. 
[21] D.E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal rep- 
resentations by error propagation. In D. E. Rumelhart, J. L.McClelland, 
and the PDP Research Group, editors, Parallel Distributed Processing: 
Explorations in the Microstructures of Cognition, volume I: Foundations. 
MIT Press/Bradford Books, Cambridge, MA, 1986. 
[22] G. E. Hinton and T. J. Sejnowski. Learning and relearning in boltzman 
machines. In D. E. Rumelhart and J. L. McClelland, editors, Parallel 

394 
NEUROMORPHIC SYSTEMS ENGINEERING 
Distributed Processing, Explorations in the Microstructure of Cognition, 
volume 1. MIT Press, Cambridge, MA, 1986. 
[23] R. J. Williams and D. Zipser. A learning algorithm for continually running 
fully recurrent neural networks. Neural Computation, 1(2):270 280, 1989. 
[24] B. A. Pearlmutter. Learning state space trajectories in recurrent neural 
networks. Neural Computation, 1(2):263-269, 1989. 
Unsupervised Learning 
[25] D. O. Hebb. The Organization of Behavior. Wiley, New York, NY, 1949. 
[26] J. Hopfield. Neural networks and physical systems with emergent collec- 
tive computational abilities. In Proc. Natl. Acad. Sci., volume 97, pages 
2554 2558, 1982. 
[27] T. Kohonen. Self-Organisation and Associative Memory. Springer-Verlag, 
Berlin, 1984. 
[28] A. Gersho and 1~. M. Gray. Vector Quantization and Signal Compression. 
Kluwer, Norwell, MA, 1992. 
[29] R. Linsker. Self-organization in a perceptual network. IEEE Computer, 
21:105-117, 1988. 
[30] G. A. Carpenter. 
Neural network models for pattern-recognition and 
associative memory. Neural Networks, 2(4):243-257, 1989. 
[31] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford Univer- 
sity Press, 1995. 
Reinforcement Learning and Related Models 
[32] K. S. Narendra and M. A. L. Thatachar. Learning automata--a survey. 
In IEEE T. Syst. Man and Cybern., volume SMC-4, pages 323-334, 1974. 
[33] S. Grossberg. A neural model of attention, reinforcement, and discrimi- 
nation learning. International Review of Neurobiology, 18:263 327, 1975. 
[34] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive 
elements that can solve difficult learning control problems. IEEE Trans- 
actions on Systems, Man, and Cybernetics, 13(5):834-846, 1983. 
[35] S. Grossberg and D. S. Levine. Neural dynamics of attentionally modu- 
lated pavlovian conditioning: Blocking, inter-stimulus interval, and sec- 
ondary reinforcement. Applied Optics, 26:5015-5030, 1987. 
[36] R. S. Sutton. Learning to predict by the methods of temporal differences. 
Machine Learning, 3:9 44, 1988. 
[37] P. J. Werbos. A menu of designs for reinforcement learning over time. In 
W. T. Miller, R. S. Sutton, and P. J. Werbos, editors, Neural Networks 
for Control, pages 67 95. MIT Press, Cambridge, MA, 1990. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
395 
[38] W. T. Miller, R. Sutton, and P. Werbos, editors. Neural Networks for 
Control. MIT Press, Cambridge, MA:, 1990. 
[39] C. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 
1992. 
[40] W.-M. Shen. Autonomous Learning from the Environment. Freeman, 
Computer Science Press, New York, NY, 1994. 
Hybrid Learning Approaches 
[41] G. A. Carpenter et al. Fuzzy artmap - a neural network architecture for 
incremental supervised learning of analog multidimentional maps. IEEE 
Transactions on Neural Networks, 3(5):698-713, 1992. 
[42] D. White and D. Sofge, editors. Handbook of Intelligent Control: Neural, 
Adaptive and Fuzzy Approaches. Van Nostrand, New York, 1992. 
[43] P. 3. Werbos. Neurocontrol and elastic fuzzy logic: Capabilities, con- 
cepts, and applications. IEEE Transactions on Industrial Electronics, 
40(2):170-180, 1993. 
[44] M. Jordan and R. Jacobs. Hierarchical mixtures of experts and the em 
algorithm. Neural Computation, 6:181-214, 1994. 
[45] R. M. Sanner and J. J. E. Slotine. Gaussian networks for direct adaptive 
control. IEEE Transactions on Neural Networks, 3(6):837-864, 1992. 
TECHNOLOGY 
Subthreshold MOS Operation 
[46] A. L. Hodgkin and A. F. Huxley. Current carried by sodium and potas- 
sium ions through the membrane of the giant axon of loligo. Journal of 
Physiology, 1952. 
[47] E. Vittoz and J. Fellrath. CMOS analog integrated circuits based on weak 
inversion operation. IEEE Journal on Solid-State Circuits, 12(3):224-231, 
1977. 
[48] A. G. Andreou, K. A. Boahen, P. O. Pouliquen, A. Pavasovid, R. E. 
Jenkins, and K. Strohbehn. Current-mode subthreshold MOS circuits for 
analog VLSI neural systems. IEEE Transactions on Neural Networks, 
2(2):205-213, 1991. 
Analog Storage 
[49] Y. Horio and S. Nakamura. Analog memories for VLSI neurocomputing. 
In E. Sgnchez-Sinencio and C. Lau, editors, Artificial Neural Networks: 
Paradigms, Applications, and Hardware Implementations, pages 344-363. 
IEEE Press, 1992. 

396 
NEUROMORPHIC SYSTEMS ENGINEERING 
[50] E. Vittoz, H. Oguey, M. A. Maher, O. Nys, E. Dijkstra, and 
M. Chevroulet. Analog storage of adjustable synaptic weights. In VLSI 
Design of Neural Networks, pages 47-63. Kluwer Academic, Norwell MA, 
1991. 
[51] M. A. Holler. VLSI implementations of learning and memory systems,. 
In Advances in Neural Information Processing Systems, volume 3, pages 
993-1000. Morgan Kaufman, San Marco, CA, 1991. 
Non-Volatile Analog Storage 
[52] A. Kramer, C. K. Sin, R. Chu, and P. K. Ko. Compact eeprom-based 
weight functions. In Advances in Neural Information Processing Systems, 
volume 3, pages 1001-1007. Morgan Kaufman, San Mateo, CA, 1991. 
[53] D. A. Kerns, J. E. Tanner, M. A. Sivilotti, and J. Luo. CMOS UV- 
writable non-volatile analog storage. In Proc. Advanced Research in VLSI 
Int. Conf., Santa Cruz CA, 1991. 
[54] A. Soennecken, U. Hilleringmann, and K. Goser. Floating gate structures 
as nonvolatile analog memory cells in 1.0#m-LOCOS-CMOS technology 
with PZT dielectrica. Microel Eng, 15:633-636, 1991. 
[55] B. W. Lee, B. J. Sheu, , and H. Yang. Analog floating-gate synapses for 
general-purpose VLSI neural computation. IEEE Trans. on Circuits and 
Systems, 38:654-658, 1991. 
[56] D. A. Durfee and F. S. Shoucair. Low programming voltage floating gate 
analog memory cells in standard VLSI CMOS technology. Electronics 
Letters, 28(10):925 927, May 1992. 
[57] R. G. Benson. Analog VLSI Suprevised Learning System. PhD thesis, 
California Institute of Technology, 1993. 
[58] O. Fujita and Y. Amemiya. A floating-gate analog memory device for 
neural networks. IEEE Device, 40(11):2029 2055, November 1993. 
[59] A. Thomsen and M. A. Brooke. Low control voltage programming of 
floating-gate mosfets and applications. IEEE Circ I, 41(6):443-452, June 
1994. 
[60] P. Hasler, C. Diorio, B. A. Minch, and C. Mead. Single transistor learning 
synapses. In Advances in Neural Information Processing Systems 7, pages 
817-824. MIT Press, Cambridge, MA, 1995. 
[61] H. Won, Y. Hayakawa, K. Nakajima, and Y. Sawada. 'switched diffusion 
analog memory for neural networks with hebbian learning-function and its 
linear-operation. IEICE T. Fund. El. Comm. Comp. Sci.d Elect Commun 
Comp Sci, E79A(6):746-751, June 1996. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
397 
Volatile Analog Storage and Refresh 
[62] D. B. Schwartz, R. E. Howard, and W. E. Hubbard. A programmable 
analog neural network chip. IEEE J. Solid-State Circuits, 24:313-319, 
189. 
[63] B. Hochet, V. Peiris, S. Abdo, and M. 3. Declercq. Implementation of 
a learning kohonen neuron based on a new multilevel storage technique. 
IEEE J. Solid-State Circuits, 26(3):262-267, 1991. 
[64] R. Castello, D. D. Caviglia, M. Franciotta, and F. Montecchi. Selfrefresh- 
ing analog memory cell for variable synaptic weights. Electronics Letters, 
27(20):1871-1873, 1991. 
[65] G. Cauwenberghs and A. Yariv. Fault-tolerant dynamic multi-level stor- 
age in analog VLSI. IEEE Transactions on Circuits and Systems II,, 
41(12):827-829, 1994. 
[66] G. Cauwenberghs. A micropower CMOS algorithmic A/D/A converter. 
IEEE Transactions on Circuits and Systems I: Fundamental Theory and 
Applications, 42(11):913 919, 1995. 
[67] J. G. Elias, D. P. M. Northmore, and W. Westerman. An analog memory 
device for spiking silicon neurons. Neural Computation, 9:419-440, 1997. 
Emerging VLSI Technologies 
[68] B. Gupta, R. Goodman, F. 3iang, Y. C. Tai, S. Tung, and C. M. Ho. Ana- 
log VLSI system for active drag reduction. IEEE Micro Mag., 16(5):53- 
59, October 1996. 
[69] T. Distefano and 3. Fjelstad. Chip-scale packaging meets future design 
needs. Solid State Tech., 39(4):82, April 1996. 
[70] B. Elkareh, B. Chen, and T. Stanley. Silicon-on-insulator - an emerging 
high-leverage technology. IEEE T. Comp. Pack. Man. Techn. Part A, 
18(1):187 194, March 1995. 
[71] C. M. Hu. Soi (silicon-on-insulator) for high-speed ultra large-scale inte- 
gration. Japan JAP 1, 33(1B):365 369, 3anuary 1994. 
ARCHITECTURE 
Outer-Product Supervised Learning 
[72] 3. Alspector, B. Gupta, and R. B. Allen. Performance of a stochastic 
learning microchip. In Advances in Neural Information Processing Sys- 
tems, volume 1, pages 748-760. Morgan Kaufman, San Mateo, CA, 1989. 
[73] F. M. A. Salam and Y. W. Wang. A real-time experiment using a 50- 
neuron CMOS analog silicon chip with on-chip digital learning. IEEE T. 
Neural Networks, 2(4):461-464, 1991. 

398 
[74] 
[75] 
NEUROMORPHIC SYSTEMS ENGINEERING 
C. R. Schneider and H. C. Card. CMOS mean field learning. Electronics 
Letters, 27(19):1702-1704, 1991. 
G. Cauwenberghs, C. F. Neugebauer, and A. Yariv. Analysis and ver- 
ification of an analog VLSI outer-product incremental learning system. 
IEEE Transactions on Neural Networks, 3(3):488-497, 1992. 
[76] S. P. Eberhardt, R. Tawel, T. X. Brown, T. Daud, and A. P. Thakoor. 
Analog VLSI neural networks - implementation issues and examples in 
optimization and supervisvised learning. IEEE T. Ind. El., 39(6):552- 
564, December 1992. 
[77] Y. Arima, M. Murasaki, T. Yamada, A. Maeda, and H. Shinohara. A 
refreshable analog VLSI neural network chip with 400 neurons and 40k 
synapses. IEEE J. of Solid State Circuits, 27:1854-1861, 1992. 
[78] R. G. Benson and D. A. Kerns. UV-activated conductances allow for 
multiple time scale learning. IEEE Transactions on Neural Networks, 
4(3):434-440, 1993. 
[79] K. Soelberg, R. L. Sigvartsen, T. S. Lande, and Y. Berg. An analog 
continuous-time neural-network. Int. J. Analog Integ. Circ. Signal Proc, 
5(3):235-246, May 1994. 
[80] T. Morie and Y. Amemiya. An all-analog expandable neural-network 
lsi with on-chip backpropagation learning. IEEE J. Solid-State Circuits, 
29(9):1086-1093, September 1994. 
[81] F. J. Kub and E. W. Justh. Analog CMOS implementation of high- 
frequency least-mean square error learning circuit. IEEE J. Solid-State 
Circuits, 30(12):1391-1398, December 1995. 
[82] Y. Berg, R. L. Sigvartsen, T. S. Lande, and -&. Abusland. An analog 
feedforward neural-network with on-chip learning. Int. J. Analog Integ. 
Circ. Signal Proc, 9(1):65-75, January 1996. 
[83] J. W. Cho, Y. K. Choi, and S. Y. Lee. Modular neuro-chip with on-chip 
learning and adjustable learning parameters. Neural Proc. Letters, 4(1), 
1996. 
[84] M. Valle, D. D. Caviglia, and G. M. Bisio. An experimental analog VLSI 
neural-network with on-chip backpropagation learning. Int. J. Analog 
Integ. Circ. Signal Proc., 9(3):231-245, April 1996. 
Outer-Product Unsupervised Learning 
[85] 3. P. Sage and R. S. Withers. Analog nonvolatile memory for neural net- 
work implementations. In Artificial Neural Networks: Electronic Imple- 
mentations, pages 22-32. IEEE Computer Society Press, CA, Los Alami- 
tos, 1990. 
[86] K. A. Boahen, P. O. Pouliquen, A. G. Andreou, and R. E. Jenkins. A 
heteroassociative memory using current-mode MOS analog VLSI circuits. 
IEEE T. Circ. Syst, 36(5):747-755, 1989. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
399 
[87] J. R. Mann and S. Gilbert. An analog self-organizing neural network 
chip. In Advances in Neural Information Processing Systems, volume 1, 
pages 739-747. Morgan Kaufman, San Mateo, CA, 1989. 
[88] A. Hartstein and R. H. Koch. A self-learning neural network. In Advances 
in Neural Information Processing Systems, volume 1, pages 769-776. Mor- 
gan Kaufman, San Mateo, CA, 1989. 
[89] M. R. Walker, S. Haghighi, A. Afghan, and L. A. Akers. Training a 
limited-interconnect, synthetic neural ic. In Advances in Neural Infor- 
mation Processing Systems, volume 1, pages 777-784. Morgan Kaufman~ 
San Mateo, CA, 1989. 
[90] A. Murray. Pulse arithmetic in VLSI neural networks. IEEE Micro Mag., 
pages 64-74, December 1989. 
[91] Y. Arima, K. Mashiko, K. Okada, T. Yamada, A. Maeda, and et al. A 
336-neuron, 28k-synapse, self-learning neural network chip with branch- 
neuron-unit architecture. IEEE J. Solid-State Circuits, 26(11):1637-1644, 
1991. 
[92] B. J. Maundy and E. I. Elmasry. A self-organizing switched-capacitor 
neural network. IEEE T. Circ. Syst., 38(12):1556-1563, December 1991. 
[93] D. A. Watola and J. L. Meador. Competitive learning in asynchronous- 
pulse-density integrated-circuits. Int. J. Analog Integ. Circ. Signal Proc., 
2(4):323 344, November 1992. 
[94] J. Donald and L. Akers. An adaptive neural processor node. IEEE 
Transactions on Neural Networks, 4(3):413-426, 1993. 
[95] Y. He and U. Cilingiroglu. A charge-based on-chip adaptation kohonen 
neural network. IEEE Transactions on Neural Networks, 4(3):462-469, 
1993. 
[96] D. Macq, M. Verleysen, P. Jespers, and J. D. Legat. Analog implementa- 
tion of a kohonen map with on-chip learning. IEEE T. Neural Networks, 
4(3):456 461, May 1993. 
[97] B. Linares-Barranco, E. Sgnchez-Sinencio, A. Rodriguez-Vazquez, and 
J. L. Huertas. A CMOS analog adaptive bam with on-chip learning and 
weight refreshing. IEEE Trans. on Neural Networks, 4:445 457, 1993. 
[98] P. Helm and E. A. Vittoz. Precise analog synapse for kohonen feature 
maps. IEEE J. Solid-State Circuits, 29(8):982-985, August 1994. 
[99] G. Cauwenberghs and V. Pedroni. A charge-based CMOS parallel analog 
vector quantizer. In Advances in Neural Information Processing Systems, 
volume 7, pages 779-786, Cambridge, MA, 1995. MIT Press. 
[100] T. Shibata, H. Kosaka, H. Ishii, and T. Ohmi. A neuron-MOS neural- 
network using self-learning-compatible synapse circuits. IEEE J. Solid- 
State Circuits, 30(8):913-922, August 1995. 
[101] R. Y. Liu, C. Y. Wu, and I. C. Jou. A CMOS current-mode design 
of modified learning-vector-quantization neural networks. Int. J. Analog 
Integ. Circ. Signal Proc., 8(2):157-181, September 1995. 

4OO 
[102] 
[103] 
[104] 
NEUROMORPHIC SYSTEMS ENGINEERING 
C. Y. Wu and J. F. Lan. MOS current-mode neural associative memory 
design with on-chip learning. IEEE T. Neural Networks, 7(1):15~181, 
January 1996. 
K. Hosono, K. Tsuji, K. Shibao, E. Io, and H. Yonezu et al. Fundamen- 
tal device and circuits for synaptic connections in self-organizing neural 
networks. IEICE T. Electronics, E79C(4):560-567, April 1996. 
T. Serrano-Gotarredona and B. Linares-Barranco. A real-time clustering 
microchip neural engine. IEEE T. VLSI Systems, 4(2):195-209, June 
1996. 
Adaptive Cellular Neural Networks 
[105] P. Tzionas, P. Tsalides, and A. Thanailakis. Design and VLSI imple- 
mentation of a pattern classifier using pseudo-2d cellular automata. IIEE 
Proc G, 139(6):661-668, December 1992. 
[106] T. Roska and L. O. Chua. The CNN universal machine - an analogic 
array computer. IEEE T. Circ. Syst. II, 40(3):163-173, March 1993. 
[107] Y. Miyanaga and K. Tochinai. Parallel VLSI architecture for multilayer 
self-organizing cellular network. IEICE T. Electronics, E76C(7):1174- 
1181, July 1993. 
[108] S. Espejo, R. Carmona, R. Dominguez-Castro, and A. Rodriguez- 
Vazquez. A CNN universal chip in CMOS technology. Int J. Circuit 
Theory Appl., 24(1):93-109, 1996. 
Adaptive Fuzzy Classifiers 
[109] J. W. Fattaruso, S. S. Mahant-Shetti, and J. B. Barton. A fuzzy logic 
inference processor. IEEE Journal of Solid-State Circuits, 29(4):397-401, 
1994. 
[110] Z. Tang, Y. Kobayashi, O. Ishizuka, and K. Tanno. A learning fuzzy 
network and its applications to inverted pendulum system. IEICE T. 
Fund. El. Comm. Comp. Sci., E78A(6):701-707, June 1995. 
[111] F. Vidal-Verdu and A. Rodriguez-Vazquez. Using building blocks to de- 
sign analog neuro-fuzzy controllers. IEEE Micro, 15(4):49-57, August 
1995. 
[112] W. Pedrycz, C. H. Poskar, and P. J. Czezowski. A reconfigurable fuzzy 
neural-network with in-situ learning. IEEE Micro, 15(4):19-30, August 
1995. 
[113] T. Yamakawa. Silicon implementation of a fuzzy neuron. IEEE Fuz Sy, 
4(4):488-501, November 1996. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
401 
Reinforcement Learning 
[114] C. Schneider and H. Card. 
Analog CMOS synaptic learning circuits 
adapted from invertebrate biology. IEEE T. Circ. Syst., 38(12):1430- 
1438, December 1991. 
[115] T. G. Clarkson, C. K. Ng, and Y. Guan. The pram: An adaptive VLSI 
chip. IEEE Trans. on Neural Networks, 4(3):408-412, 1993. 
[116] A. F. Murray, S. Churcher, A. Hamilton, A. J. Holmes, and G. B. Jackson 
et al. Pulse stream VLSI neural networks. IEEE Micro, 14(3):29 39, June 
1994. 
[117] G. Cauwenberghs. Reinforcement learning in a nonlinear noise shaping 
oversampled A/D converter. In Proc. Int. Symp. Circuits and Systems, 
Hong Kong, June 1997. 
Nonidealities and Error Models 
[118] M. J. S. Smith. n analog integrated neural network capable of learning the 
feigenbaum logistic map. IEEE Transactions on Circuits and Systems, 
37(6):841-844, 1990. 
[119] R. C. Frye, E. A. Rietman, and C.C. Wong. Back-propagation learning 
and nonidealities in analog neural network hardware. IEEE Transactions 
on Neural Networks, 2(1):110-117, 1991. 
[120] L. M. Reyneri and E. Filippi. An analysis on the performance of sili- 
con implementations of backpropagation algorithms for artificial neural 
networks. IEEE Comput, 40(12):1380-1389, 1991. 
[121] A. Murray and P. J. Edwards. Synaptic noise during mlp training en- 
hances fault-tolerance, generalization and learning trajectory. 
In Ad- 
vances in Neural Information Processing Systems, volume 5, pages 491- 
498. Morgan Kaufman, San Mateo, CA, 1993. 
[122] P. Thiran and M. Hasler. Self-organization of a one-dimensional kohonen 
network with quantized weights and inputs. Neural Networks, 7(9):1427- 
1439, 1994. 
[123] G. Cairns and L. Tarassenko. Precision issues for learning with analog 
VLSI multilayer perceptrons. IEEE Micro, 15(3):54-56, June 1995. 
[124] B. K. Dolenko and H. C. Card. Tolerance to analog hardware of on- 
chip learning in backpropagation networks. IEEE T. Neural Networks, 
6(5):1045-1052, September 1995. 
[125] 
Mode1-~ree Learning 
A. Dembo and T. Kailath. Model-free distributed learning. IEEE Trans- 
actions on Neural Networks, 1(1):58-70, 1990. 

402 
[126] 
NEUROMORPHIC SYSTEMS ENGINEERING 
M. Jabri and B. Flower. Weight perturbation: An optimal architecture 
and learning technique for analog VLSI feedforward and recurrent multi- 
layered networks. IEEE Transactions on Neural Networks, 3(1):154-157, 
1992. 
[127] G. Cauwenberghs. A fast stochastic error-descent algorithm for super- 
vised learning and optimization. In Advances in Neural Information Pro- 
cessing Systems, volume 5, pages 244-251, San Mateo, CA, 1993. Morgan 
Kaufman. 
[128] J. Alspector, R. Meir, B. Yuhas, and A. Jayakumar. A parallel gradient 
descent method for learning in analog VLSI neural networks. In Advances 
in Neural Information Processing Systems, volume 5, pages 836-844, San 
Mateo, CA, 1993. Morgan Kaufman. 
[129] B. Flower and M. Jabri. Summed weight neuron perturbation: An ~(n) 
improvement over weight perturbation. In Advances in Neural Informa- 
tion Processing Systems, volume 5, pages 212-219, San Mateo, CA, 1993. 
Morgan Kaufman. 
[130] D. Kirk, D. Kerns, K. Fleischer, and A. Barr. Analog VLSI implementa- 
tion of gradient descent. In Advances in Neural Information Processing 
Systems, volume 5, pages 789-796, San Mateo, CA, 1993. Morgan Kauf- 
man. 
[131] G. Cauwenberghs. 
A learning analog neural network chip with 
continuous-recurrent dynamics. In Advances in Neural Information Pro- 
cessing Systems, volume 6, pages 858-865, San Mateo, CA, 1994. Morgan 
Kaufman. 
[132] P. Hollis and J. Paulos. A neural network learning algorithm tailored for 
VLSI implementation. IEEE Tran. Neural Networks, 5(5):784-791, 1994. 
[133] G. Cauwenberghs. An analog VLSI recurrent neural network learning 
a continuous-time trajectory. IEEE Transactions on Neural Networks, 
7(2), March 1996. 
[134] A. J. Montalvo, R. S. Gyurcsik, and J. J. Paulos. Toward a general- 
purpose analog VLSI neural-network with on-chip learning. IEEE T. 
Neural Networks, 8(2):413-423, March 1997. 
Chip-in-the-Loop Training 
[135] M. Holler, S. Tam, H. Castro, and R. Benson. An electrically trainable 
artificial neural network (etann) with 10240 floating gate synapses. In 
Proc. Int. Joint Conf. Neural Networks, pages 191-196, Washington DC, 
1989. 
[136] S. Satyanarayana, Y. Tsividis, and H. P. Graf. A reconfigurable analog 
VLSI neural network chip. In Advances in Neural Information Processing 
Systems, volume 2, pages 758-768. Morgan Kaufman, San Mateo, CA, 
1990. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
403 
[137] E. Sackinger, B. E. Boser, and L. D. Jackel. A neurocomputer board 
based on the anna neural network chip. In Advances in Neural Infor- 
mation Processing Systems, volume 4, pages 773-780. Morgan Kaufman, 
San Mateo, CA, 1992. 
[138] J. A. Lansner. An experimental hardware neural-network using a cascad- 
able, analog chipset. Int J Elect, 78(4):679 690, April 1995. 
[139] J. O. Klein, H. Pujol, and P. Garda. Chip-in-the-loop learning algorithm 
for boltzmann machine. Electronics Letters, 31(12):986 988, June 1995. 
Digital Implementations 
[140] A. Johannet, L. Personnaz, G. Dreyfus, J. D. Oascuel, and M. Weinfeld. 
Specification and implementation of a digital hopfield-type associative 
memory with on-chip training. IEEE T. Neural Networks, 3(4):529 539, 
July 1992. 
[141] T. Shima, T. Kimura, Y. Kamatani, T. Itakura, Y. Fujita, and T. Iida. 
Neuro chips with on-chip back-propagation and/or hebbian learning. 
IEEE J. of Solid-State Circuits, 27(12):1868-1875, 1992. 
[142] M. Yasunaga, N. Masuda, M. Yagyu, M. Asai, and K. Shibata et al. A 
self-learning digital neural network using wafer-scale lsi. IEEE J. Solid- 
State Circuits, 28(2):106 114, February 1993. 
[143] C. Lehmann, M. Viredaz, and F. Blayo. A generic systolic array building- 
block for neural networks with on-chip learning. IEEE T. Neural Net- 
works, 4(3):400-407, May 1993. 
[144] M. Fujita, Y. Kobayashi, K. Shiozawa, T. Takahashi, and F. Mizuno et al. 
Development and fabrication of digital neural-network wsis. IEICE T. 
Electronics, E76C(7):1182 1190, July 1993. 
[145] P. Murtagh, A. C. Tsoi, and N. Bergmann. Bit-serial systolic array im- 
plementation of a multilayer perceptron. In IEEE Proc E, volume 140-5, 
pages 27~288, September 1993. 
[146] T. Morishita and I. Teramoto. Neural-network multiprocessors applied 
with dynamically reconfigurable pipeline architecture. IEICE T. Elec- 
tronics, E77C(12):1937-1943, December 1994. 
[147] Z. Tang and O. Ishizuka. 
Design and implementations of a learn- 
ing t-model neural-network. IEICE T. Fund. El. Comm. Comp. Sci., 
E78A(2):259-263, February 1995. 
[148] M. P. Perrone and L. N. Cooper. The nil000: High speed parallel VLSI 
for implementing multilayer perceptrons. In Advances in Neural Infor- 
mation Processing Systems, volume 7, pages 747 754. Morgan Kaufman, 
San Mateo, CA, 1995. 
[149] et al. J. Wawrzynek. SPERT-II: A vector microprocessor system and its 
application to large problems in backpropagation training. In Advances in 

404 
NEUROMORPHIC SYSTEMS ENGINEERING 
[150] 
Neural Information Processing Systems, volume 8, pages 619-625. Mor- 
gan Kaufman, San Mateo, CA, 1996. 
S. Rehfuss and D. Hammerstrom. Model matching and sfmd computa- 
tion. In Advances in Neural Information Processing Systems, volume 8, 
pages 713-719. Morgan Kaufman, San Mateo, CA, 1996. 
Optical and Optoelectronic Implementations 
[151] J. Ohta, Y. Nitta, and K. Kyuma. Dynamic optical neurochip using 
variable-sensitivity photodiodes. Optics Lett, 16(10):744-746, 1991. 
[152] D.Z. Anderson, C. Benkert, V. Hebler, J.-S. Jang, D, Montgomery, and 
M. Saffman. Optical implementation of a self-organizing feature extrac- 
tor. In Advances in Neural Information Processing Systems, volume 4, 
pages 821-828. Morgan Kaufman, San Mateo, CA, 1992. 
[153] Y. Nitta, J. Ohta, S. Tai, and K. Kyuma. Optical learning neurochip with 
internal analog memory. Appl Optics, 32(8):1264-1274, March 1993. 
[154] K. Wagner and T. M. Slagle. Optical competitive learning with VLSI 
liquid-crystal winner-take-all modulators. Appl Optics, 32(8):1408 1435, 
March 1993. 
[155] M. Oita, Y. Nitta, S. Tai, and K. Kyuma. Optical associative memory 
using optoelectronic neurochips for image-processing. IEICE T. Electron- 
ics, E77C(1):56-62, January 1994. 
[156] E. Lange, Y. Nitta, and K. Kyuma. Optical neural chips. IEEE Micro, 
14(6):29-41, December 1994. 
[157] A. J. Waddie and J. F. Snowdon. A smart-pixel optical neural-network 
design using customized error propagation. 
Inst. Phys. Conf. Series, 
139:511-514, 1995. 
[158] K. Tsuji, H. Yonezu, K. Hosono, K. Shibao, and N. Ohshima et al. An op- 
tical adaptive device and its application to a competitive learning circuit. 
In Japan JAP 1, volume 34-2B, pages 1056-1060, February 1995. 
[159] W. E. Foor and M. A. Neifeld. Adaptive, optical, radial basis func- 
tion neural-network for handwritten digit recognition. 
Appl Optics, 
34(32):7545-7555, November 1995. 
Architectural Novelties 
[160] J. Alspector, J. W. Gannett, S. Haber, M. B. Parker, and R. Chu. 
A VLSI-efficient technique for generating multiple uncorrelated noise 
sources and its application to stochastic neural networks. IEEE T. Circ. 
Syst., 38(1):109-123, 1991. 
[161] P. A. Shoemaker, M. J. Carlin, and R. L. Shimabukuro. Back propagation 
learning with trinary quantization of weight updates. Neural Networks, 
4(2):231-241, 1991. 

]162] 
[163] 
[164] 
[165] 
[166] 
[167] 
[168] 
[169] 
[170] 
ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
405 
Y. H. Pao and W. Hafez. Analog computational models of concept- 
formation. Int. J. Analog Integ. Circ. Signal Proc., 4(2):265-272, Novem- 
ber 1992. 
T. Morie and Y. Amemiya. Deterministic boltzmann machine learn- 
ing improved for analog lsi implementation. 
IEICE T. Electronics, 
E76C(7):1167-1173, July 1993. 
S. P. Deweerth and D. M. Wilson. Fixed-ratio adaptive thresholding 
using CMOS circuits. Electronics Letters, 31(10):788 789, May 1995. 
M. Vandaalen, J. Zhao, and J. Shawetaylor. 'real-time output derivatives 
for on chip learning using digital stochastic bit stream neurons. Electron- 
ics Letters, 30(21):1775-1777, October 1994. 
V. Petridis and K. Paraschidis. 
On the properties of the feedforward 
method - a simple training law for on-chip learning. IEEE T. Neural 
Networks, 6(6):1536-1541, November 1995. 
H. Singh, H. S. Bawa, and L. Anneberg. Boolean neural-network real- 
ization of an adder subtractor cell. Microel Rel, 36(3):367 369, March 
1996. 
T. Lehmann, E. Bruun, and C. Dietrich. Mixed analog-digital matrix- 
vector multiplier for neural-network synapses. Int. J. Analog Integ. Circ. 
Signal Proc., 9(1):55-63, January 1996. 
T. Serrano-Gotarredona and B. Linares-Barranco. 
A modified art-1 
algorithm more suitable for VLSI implementations. 
Neural Networks, 
9(6):1025-1043, August 1996. 
M. L. Marchesi, F. Piazza, and A. Uncini. Backpropagation without 
multiplier for multilayer neural networks. In IEEE P. Circ., volume 143- 
4, pages 229 232, August 1996. 
[171] 
SYSTEMS APPLICATIONS OF LEARNING 
General Purpose Neural Emulators 
P. Mueller, J. Van der Spiegel, D. Blackman, T. Chiu, T. Clare, C. Don- 
ham, T.P. Hsieh, and M. Lionaz. Design and fabrication of VLSI com- 
ponents for a general purpose analog neural computer. In Analog VLSI 
Implementation of Neural Systems, pages 135-169. Kluwer, Norwell, MA, 
1989. 
Blind Signal Processing 
[172] E. Vittoz and X. Arreguit. CMOS integration of herault-jutten cells for 
separation of sources. In Analog VLSI Implementation of Neural Systems, 
pages 57-83. Kluwer, Norwell, MA, 1989. 

406 
[1731 
[174] 
NEUROMORPHIC SYSTEMS ENGINEERING 
M. H. Cohen and A. G. Andreou. Current-node subthreshold MOS imple- 
tnentation of the herault-jutten autoadaptive network. IEEE J. of Solid 
State Circuits, 27:714-727, 1992. 
R. P. Mackey, J. J. Rodriguez, J. D. Carothers, and S. B. K. Vrudhula. 
Asynchronous VLSI architecture for adaptive echo cancellation. Electron- 
ics Letters, 32(8):710-711, April 1996. 
[175] 
Biomedical Adaptive Signal Processing 
R. Coggins, M. Jabri, M. Flower, and S. Pickard. Iceg morphology clas- 
sification using an analogue VLSI neural network. In Advances in Neural 
Information Processing Systems, volume 7, pages 731-738. Morgan Kauf- 
man, San Marco, CA, 1995. 
Speech Research 
[176] et ah J. Wawrzynek. SPERT-Ih A vector microprocessor system and its 
application to large problems in backpropagation training. In Advances in 
Neural Information Processing Systems, volume 8, pages 619 625. Mor- 
gan Kaufman, San Mateo, CA, 1996. 
[177] John Lazzaro. Temporal adaptation in a silicon auditory nerve. In John E. 
Moody, Steve J. Hanson, and Richard P. Lippmann, editors, Advances in 
Neural Information Processing Systems, volume 4, pages 813-820. Mor- 
gan Kaufmann Publishers, Inc., 1992. 
[1781 
Olfactory Sensory Processing 
P. A. Shoemaker, C. G. Hutchens, and S. B. Patil. 
A hierarchical- 
clustering network based on a model of olfactory processing. 
Int. J. 
Analog Integ. Circ. Signal Proc., 2(4):297-311, November 1992. 
Focal-Plane Sensors and Adaptive Vision Systems 
[179] 3. Tanner and C. A. Mead. An integrated analog optical motion sensor. In 
S. Â¥. Kung, editor, VLSI Signal Processing II, pages 59-76. IEEE Press, 
New York, 1986. 
[180] C. A. Mead. Adaptive retina. In C. Mead and M. Ismail, editors, Analog 
VLSI Implementation of Neural Systems, pages 239-246. Kluwer Aca- 
demic Pub., Norwell, MA, 1989. 
[181] M. Mahowald. An Analog VLSI Stereoscopic Vision System. Kluwer 
Academic, Boston, MA, 1994. 
[182] T. Delbriick. Silicon retina with correlation-based velocity-tuned pixels. 
IEEE Transactions on Neural Networks, 4(3):529-541, May 1993. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
407 
[183] J. C. Lee, B. J. Sheu, and W. C. Fang. VLSI neuroprocessors for video 
motion detection. IEEE Transactions on Neural Networks, 4(2):78-191, 
1993. 
[184] R. Etienne-Cummings, J. Van der Spiegel, and P. Mueller. VI,SI model of 
primate visual smooth pursuit. In Advances in Neural Information Pro- 
cessing Systems, volume 8, pages 707-712. Morgan Kaufman, San Marco, 
CA, 1996. 
[185] R. Sarpeshkar, J. Kramer, G. Indiveri, and C. Koch. Analog VLSI ar- 
chitectures for motion processing - from fundamental limits to system 
applications. P IEEE, 84(7):969-987, July 1996. 
[186] K. A. Boahen. A retinomorphic vision system. IEEE Micro, 16(5):30-39, 
October 1996. 
[187] S. C. Liu and C. Mead. Continuous-time adaptive delay system. IEEE 
T. Circ. Syst. II, 43(11):744-751, November 1996. 
Optical Character Recognition 
[188] B. Y. Chen, M. W. Mao, and J. B. Kuo. Coded block neural network 
VLSI system using an adaptive learning-rate technique to train chinese 
character patterns. Electronics Letters, 28(21):1941 1942, October 1992. 
[189] C. S. Miou, T. M. Shieh, G. H. Chang, B. S. Chien, and M. W. Chang 
et al. Optical chinese character-recognition system using a new pipelined 
matching and sorting VLSI. Opt Eng, 32(7):1623-1632, July 1993. 
[190] S. Maruno, T. Kohda, H. Nakahira, S. Sakiyama, and M. Maruyama. 
Quantizer neuron model and neuroprocessor-named quantizer neuron 
chip. IEEE J. Sel. Areas Comm., 12(9):1503 1509, December 1994. 
[191] 
Image Compression 
W. C. Fang, B. J. Sheu, O. T. C. Chen, and J. Choi. A VLSI neural 
processor for image data-compression using self-organization networks. 
IEEE Transactions on Neural Networks, 3(3):506-518, 1992. 
Communications and Decoding 
[192] J. G. Choi, S. H. Bang, and B. J. Sheu. A programmable analog VLSI 
neural-network processor for communication receivers. IEEE T. Neural 
Networ/~s, 4(3):484-495, May 1993. 
[193] M.I. Chan, W. T. Lee, M. C. Lin, and L. G. Chen. Ic design of an adaptive 
viterbi decoder. IEEE T. Cons. El., 42(1):52-62, February 1996. 
[194] R. Mittal, K. C. Bracken, L. R. Carley, and D. J. Allstot. A low-power 
backward equalizer for dfe read-channel applications. IEEE J. SoSd-State 
Circuits, 32(2):270-273, February 1997. 

408 
[195] 
NEUROMORPHIC SYSTEMS ENGINEERING 
B. C. Rothenberg, J. E. C. Brown, P. J. Hurst, and S. H. Lewis. A 
mixed-signal ram decision-feedback equalizer for disk drives. IEEE J. 
Solid-State Circuits, 32(5):713-721, 1997. 
Clock Skew Timing Control 
[196] W. D. Grover, J. Brown, T. Priesen, and S. Marsh. All-digital multipoint 
adaptive delay compensation circuit for low skew clock distribution. Elec- 
tronics Letters, 31(23):1996-1998, November 1995. 
[197] M. Mizuno, M. Yamashina, K. Furuta, H. Igura, and H. Abiko et al. A 
GHz MOS adaptive pipeline technique using MOS current-mode logic. 
IEEE J. Solid-State Circuits, 31(6):784-791, June 1996. 
[198] E. W. Justh and F. J. Kub. Analog CMOS continuous-time tapped delay- 
line circuit. Electronics Letters, 31(21):1793-1794, October 1995. 
Control and Autonomous Systems 
[199] Y. Harata, N. Ohta, K. Hayakawa, T. Shigematsu, and Y. Kita. 
A 
fuzzy inference lsi for an automotive control. 
IEICE T. Electronics, 
E76C(12):1780-1781, December 1993. 
[200] G. Jackson and A. F. Murray. Competence acquisition is an autonomous 
mobile robot using hardware neural techniques. In Adv. Neural Informa- 
tion Processing Systems, volume 8, pages 1031-1037. MIT Press, Cam- 
bridge, MA, 1996. 
High-Energy Physics 
[201] T. Lindblad, C. S. Lindsey, F. Block, and A. Jayakumar. Using software 
and hardware neural networks in a higgs search. Nucl Inst A, 356(2- 
3):498-506, March 1995. 
[202] C. S. Lindsey, T. Lindblad, G. Sekhniaidze, G. Szkely, and M. Miner- 
skjold. Experience with the ibm zisc036 neural-network chip. Int J. 
Modern Phys. C, 6(4):579-584, August 1995. 
[203] G. Anzellotti, R. Battiti, I. Lazzizzera, G. Soncini, and A. Zorat et al. 
Totem - a highly parallel chip for triggering applications with induc- 
tive learning based on the reactive tabu search. Int J. Modern Phys. C, 
6(4):555-560, August 1995. 

18 
ANALOG VLSI STOCHASTIC 
PERTURBATIVE LEARNING 
ARCHITECTURES 
Gert Cauwenberghs 
18.1 
INTRODUCTION 
Learning and adaptation are central to the design of neuromorphic VLSI sys- 
tems that perform robustly in variable and unpredictable environments. 
Learning algorithms that are efficiently implemented on general-purpose dig- 
ital computers do not necessarily map efficiently onto analog VLSI hardware. 
Even if the learning algorithm supports a parallel and scalable architecture 
suitable for analog VLSI implementation, inaccuracies in the implementation 
of the learning functions may significantly affect the performance of the trained 
system. Learning can only effectively compensate for inaccuracies in the net- 
work implementation when their physical sources are contained directly inside 
the learning feedback loop. Algorithms which assume a particular model for 
the underlying characteristics of the system being trained perform poorer than 
algorithms which directly probe the response of the system to external and 
internal stimuli. 
A second source of concern in the design of neuromorphic VLSI learning 
systems has to do with the assumptions made on the particular form of the 
performance criterion being optimized. In typical physical systems, the learning 
objectives can not be clearly defined in terms of a target response or desired 
state of the system. Learning from external dicrete rewards, in absence of a 
well-defined training signal, requires internal mechanisms of credit assignment 
which make no prior assumptions on the causal relationships of the system and 

410 
NEUROMORPHIC SYSTEMS ENGINEERING 
the enviroment in which it operates. The stereotypical example of a system 
able to learn from a discrete delayed reward or punishment signal is the pole- 
balancer trained with reinforcement learning [2]. 
We use stochastic perturbative algorithms for model-free estimation of gradi- 
ent information [12] in a general framework that includes reinforcement learn- 
ing under delayed and discontinuous rewards [2, 15, 28, 29, 31], suitable for 
learning in physical systems of which the characteristics nor the optimization 
objectives are properly defined. Stochastic error-descent architectures for su- 
pervised learning [4] and computational primitives of reinforcement learning are 
combined into an analog VLSI architecture which offers a modular and cellular 
structure, model-free distributed representation, and robustness to noise and 
mismatches in the implementation. The combined architecture is applicable to 
the most general of learning tasks, where an unknown "black-box" dynamical 
system is adapted using a external "black-box" reinforcement-based delayed 
and possibly discrete reward signal. 
As a proof of principle, we apply the model-free training-free adaptive tech- 
niques to blind optimization of a second-order noise-shaping modulator for 
oversampled data conversion, controlled by a neural classifier. The only eval- 
uative feedback used in training the classifier is a discrete failure signal which 
indicates when some of the integrators in the modulation loop saturate. 
In the following, we review supervised learning and stochastic perturbative 
techniques, and present a corresponding architecture for analog VLSI imple- 
mentation. We then cover a generalized form of reinforcement learning, and 
introduce a stochastic perturbative analog VLSI architecture for reinforcement 
learning. Neuromorphic implementations in analog VLSI and system examples 
are also included. 
18.2 
SUPERVISED LEARNING 
In a metaphorical sense, supervised learning assumes the luxury of a committed 
"teacher", who constantly evaluates and corrects the network by continuously 
feeding it target values for all network outputs. Supervised learning can be 
reformulated as an optimization task, where the network parameters (weights) 
are adjusted to minimize the distance between the targets and actual network 
outputs. Generalization and overtraining are important issues in supervised 
learning, and are beyond the scope of this paper. 
Let y(t) be the vector of network outputs with components y~(t), and cor- 
respondingly ytarget (t) be the supplied target output vector. The network con- 
tains adjustable parameters (or weights) p with components Pk, and state vari- 
ables x(t) with components xi(t) (which may contain external inputs). Then 
the task is to minimize the scalar error index 
g(p;t) = E 
target 
I~ 
(t) - ~(t)l ~ 
i 
in the parameters p~, using a distance metric with norm u > 0. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
411 
18.2.1 
Gradient Descent 
Gradient descent is the most common optimization technique for supervised 
learning in neural networks, which includes the widely used technique of back- 
propagation (or "dynamic feedback") [30] for gradient derivation, applicable to 
general feedforward multilayered networks. 
In general terms, gradient descent minimizes the scalar performance index Â£ 
by specifying incremental updates in the parameter vector p according to the 
error gradient ~'pg: 
p(t + 1) = p(t) - r] Vpg(t) 
(18.1) 
One significant problem with gradient descent and its variants for on-line super- 
vised learning is the complexity of calculating the error gradient components 
Og/Opk from a model of the system. This is especially so for complex systems 
involving internal dynamics in the state variables xj (t): 
Og 
Og(t) 
Oyi(t) 
Ozj(t) 
(18.2) 
Opk - E. . Oy i 
OXj 
Opk 
%? 
where derivation of the dependencies Oxj/Opk over time constitutes a significant 
amount of computation that typically scales super-linearly with the dimension 
of the network [4]. Furthermore, the derivation of the gradient in (18.2) as- 
sumes accurate knowledge of the model of the network (y(t) as a function of 
x(t), and recurrence relations in the state variables x(t)). Accurate model 
knowledge cannot be assumed for analog VLSI neural hardware, due to mis- 
matches in the physical implementation which can not be predicted at the time 
of fabrication. Finally, often a model for the system being optimized may not 
be readily available, or may be too complicated for practica.1 (real-time) evalu- 
ation. In such cases, a black-box approach to optimization is more effective in 
every regard. This motivates the use of the well-known technique of stochastic 
approximation [21] for blind optimization in analog VLSI systems. We ap- 
ply this technique to supervised learning as well as to more advanced models 
of "reinforcement" learning under discrete delayed rewards. The connection 
between stochastic approximation techniques and principles of neuromorphic 
engineering will be illustrated further below, in contrast with gradient descent. 
18.2.2 
Stochastic Approximation Techniques 
Stochastic approximation algorithms [21] have long been known as effective 
tools for constrained and unconstrained optimization under noisy observations 
of system variables [24]. Applied to on-line minimization of an error index 
Â£(p), the algorithms avoid the computational burden of gradient estimation by 
directly observing the dependence of the index g on randomly applied pertur- 
bations in the parameter values. Variants on the Kiefer-Wolfowitz algorithm 
for stochastic approximation [21], essentially similar to random-direction finite- 
difference gradient descent, have been formulated for blind adaptive control [26], 

412 
NEUROMORPHIC SYSTEMS ENGINEERING 
neural networks [12, 27] and the implementation of learning functions in VLSI 
hardware [1, 4, 14, 18]. 
The broader class of neural network learning algorithms under this category 
exhibit the desirable property that the functional form of the parameter up- 
dates is "model-free", i.e., independent of the model specifics of the network 
or system under optimization. The model-free techniques for on-line super- 
vised learning are directly applicable to almost any observable system with 
deterministic, slowly varying, and possibly unknown characteristics. Parallel 
implementation of the stochastic approximation algorithms results into efficient 
and modular learning architectures that map well onto VLSI hardware. Since 
those algorithms use only direct function evaluations and no derivative infor- 
mation, they are functionally simple, and their implementation is independent 
of the structure of the system under optimization. They exhibit robust conver- 
gence properties in the presence of noise in the system and model mismatches 
in the implementation. 
A brief description of the stochastic error-descent algorithm follows below, as 
introduced in [4] for efficient supervised learning in analog VLSI. The integrated 
analog VLSI continous-time learning system used in [5, 8] forms the basis for 
the architectures outlined in the sections that follow. 
18.2.3 
Stochastic Supervised Learning 
Let Â£(p) be the error functional to be minimized, with Â£ a scalar determinis- 
tic function in the parameter (or weight) vector p with components Pi. The 
stochastic algorithm specifies incremental updates in the parameters pi as with 
gradient descent (18.1), although using a stochastic approximation to the true 
gradient 
0Â£(t) est 
= 7ri(t). ~(t) 
(18.3) 
0p~ 
where the differentially perturbed error 
1 
~(t) = ~ 
($(p(t) + ~r(t)) - Â£(p(t) - 7r(t))) 
(18.4) 
is obtained from two direct observations of $ under complementary activation of 
a parallel random perturbation vector ~r(t) with components ~ri(t) onto the pa- 
rameter vector p(t). The perturbation components 7ri(t) are fixed in amplitude 
and random in sign, ~ri(t) = +a with equal probabilities for both polarities. 
The algorithm essentially performs gradient descent in random directions in 
the parameter space, as defined by the position of the perturbation vector. 
As with exact gradient descent, iteration of the updates using (18.3) con- 
verges in the close proximity of a (local) minimum of $, provided the pertur- 
bation amplitude a is sufficiently small. The rate of convergence is necessarily 
slower than gradient descent, since every observation (18.4) only reveals scalar 
information about the gradient vector in one dimension. However, the amount 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
413 
of computation required to compute the gradient at every update may out- 
weigh the higher convergence rate offered by gradient descent, depending on the 
model complexity of the system under optimization. When applied to on-line 
supervised learning in recurrent dynamical systems, the stochastic algorithm 
provides a net computational efficiency rivaling that of exact gradient descent. 
Computational efficiency is defined in terms of the total number of operations 
required to converge, i.e., reach a certain level of $. A formal derivation of the 
convergence properties is presented in [4]. 
18.2.4 Supervised Learning Architecture 
While alternative optimization techniques based on higher-order extensions 
on gradient descent will certainly offer superior convergence rates, the above 
stochastic method achieves its relative efficiency at a much reduced complexity 
of implementation. The only global operations required are the evaluations of 
the error function in (18.4), which are obtained from direct observations on 
the system under complementary activation of the perturbation vector. The 
operations needed to generate and apply the random perturbations, and to 
perform the parameter update increments, are strictly local and identical for 
each of the parameter components. The functional diagram of the local pa- 
rameter processing cell, embedded in the system under optimization, is shown 
in Figure 18.1. The complementary perturbations and the corresponding error 
observations are performed in two separate phases on the same system, rather 
than concurrently on separate replications of the system. The sequential acti- 
vation of the complementary perturbations and corresponding evaluations of Â£ 
are synchronized and coordinated with a three-phase clock: 
Â¢o 
: $(p,t) 
Â¢+ 
: $(p+~r,t) 
(18.5) 
Â¢- 
: $(p- ~r,t) 
This is represented schematically in Figure 18.1 by a modulation signal O(t), 
taking values {-1, 0, 1}. The extra phase Â¢0 (Â¢(t) = 0) is not strictly needed 
to compute (18.4)--it is useful otherwise, e.g. to compute finite difference 
estimates of second order derivatives for dynamic optimization of the learning 
rate ~(t). 
The local operations are further simplified owing to the binary nature of the 
perturbations, reducing the multiplication in (18.3) to an exclusive-or logical 
operation, and the modulation by Â¢(t) to binary multiplexing. Besides effi- 
ciency of implementation, this has a beneficial effect on the overall accuracy of 
the implemented learning system, as will be explained in the context of VLSI 
circuit implementation below. 
18.2.5 Supervised Learning in Dynamical Systems 
In the above, it was assumed that the error functional $(p) is directly observable 
on the system by applying the parameter values Pi. In the context of on-line 

414 
NEUROMORPHIC SYSTEMS ENGINEERING 
t 
-n ~(t) 
~(t) 
_ _  
LOCAL 
i 
NETWORK 
I -n ~(t) ~-~ 
I 
' 
' 
"1 
' 
~i~ 
~ 
I 
~ 
GLOBAL 
I 
~ 
.
.
.
.
.
 
~ 
Figure 18.1 
Architecture implementing stochastic error-descent supervised learning. The 
learning cell is locally embedded in the network. The differential error index is evaluated 
globally, and communicated to all cells. 
supervised learning in dynamical systems, the error functional takes the form 
of the average distance of norm u between the output and target signals over 
a moving time window, 
fl y ~ 
target e.t~ 
g(p;h,tf) 
= 
~ 
/~ ) - ~(t')l~dt ' , 
â¢ 
(18.6) 
which implicitly depends on the training sequence ytarget (t) and on initial con- 
ditions on the internal state variables of the system. An on-line implementation 
prohibits simultaneous observation of the error measure (18.6) under different 
instances of the parameter vector p, as would be required to evaluate (18.4) 
for construction of the parameter updates. However, when the training signals 
are periodic and the interval T = tf - ti spans an integer multiple of periods, 
the measure (18.6) under constant parameter values is approximately invariant 
to time. In that case, the two error observations needed in (18.4) can be per- 
formed in sequence, under complementary piecewise constant activation of the 
perturbation vector. 
In the context of on-line supervised learning in dynamical systems, the re- 
quirement of periodicity on the training signals is a limitation of the stochastic 
error-descent algorithm. 
Next, this requirement will be relaxed, along with 
some more stringent assumptions on the nature of supervised learning. In par- 
ticular, a target training signal will no longer be necessary. Instead, learning 
is based on an external reward signal that conveys only partial and delayed 
information about the performance of the network. 
18.3 
LEARNING 
FROM DELAYED AND DISCRETE 
REWARDS 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
415 
Supervised learning methods rely on a continuous training signal that gives 
constant feedback about the direction in which to steer the response of the 
network to improve its performance. This continuous signal is available in 
the form of target values ytarget(t) for the network outputs y(t). More widely 
applicable but also more challenging are learning tasks where target outputs 
or other continuous teacher feedback are not available, but instead only a non- 
steady, delayed reward (or punishment) signal is available to evaluate the quality 
of the outputs (or actions) produced by the network. The difficulty lies in 
assigning proper credit for the reward (or punishment) to actions that where 
produced by the network in the past, and adapting the network accordingly in 
such a way to reinforce the network actions leading to reward (and conversely, 
avoid those leading to punishment). 
18.3.1 
Reinforcement Learning Algorithms 
Several iterative approaches to dynamic programming have been applied to 
solve the credit assignment problem for training a neural network with delayed 
rewards [2, 28, 29, 31, 15]. They all invoke an "adaptive critic element" which 
is trained along with the network to predict the future reward signal from 
the present state of the network. We define "reinforcement learning" essen- 
tially as given in [2], which includes as special cases "time difference learning" 
or TD(Â£) [28], and, to some extent, Q-learning [29] and "advanced heuristic 
dynamic programming" [31]. The equations are listed below in general form 
to clarify the similarity with the above supervised perturbative learning tech- 
niques. It will then be shown how the above architectures are extended to allow 
learning from delayed and/or impulsive rewards. 
Let r(t) be the discrete delayed reward signal for state vector x(t) of the 
system (components xj(t)), 
r(t) is zero when no signal is available, and is 
negative for a punishment. Let y(t) be the (scalar) output of the network in 
response to an input (or state) x(t), and q(t) the predicted future reward (or 
"value function") associated with state x(t) as produced by the adaptive critic 
element. The action taken by the system is determined by the polarity of the 
network output, sign(y(t)). For example, in the pole balancing experiment 
of [2], y(t) is hard-limited and controls the direction of the fixed-amplitude 
force exterted on the moving cart. Finally, let w and v (components wi and 
vi) be the weights of the network and the adaptive critic element, respectively. 
Then the weight updates are given by 
Awl(t) 
= 
wi(t + 1) -- wi(t) 
(18.7) 
= 
= 
+ 
1) - 
v 
(t) 
= 
/3 Ã·(t). di(t) 
where the "eligibility" functions e~(t) and d~(t) are updated as 
5ei(t) + (1 - 5) sign(y(t)) ~ 
) 
(18.8) 
ei(t + 1) 
~w~ 

416 
NEUROMORPHIC SYSTEMS ENGINEERING 
di(t+l) 
= 
ikdi(t)+(1-A)Oq(t) 
Ovi 
and the reinforcement Ã·(t) is given by 
~(t) = r(t) + 3'q(t) - q(t - 1) 
(18.9) 
The parameters 6 and A define the time span of credit assigned by ei(t) and 
di(t) to actions in the past, weighting recent actions stronger than past actions: 
t--1 
e(t) 
= 
(1-6) E 
6t-t'-i sign(y(t'))OY(t') 
tt =_oc 
~Wi 
t-1 
d(t) 
= 
(1- A) E 
At-t'-zOq(t') 
Ovi 
iff ~ -- oo 
Similarly, the parameter ~ defines the time span for the prediction of future 
reward by q(t), at convergence: 
(3~ 
t 
! 
q(t) 
t~=t+l 
For ~ = 1 and y -= q, the equations reduce to TD(A). Convergence of TD(A) 
with probability one has been proven in the general case of linear networks of 
the form q = ~ vixi [23]. 
Learning algorithms of this type are neuromorphic in the sense that they 
emulate classical (pavlovian) conditioning in pattern association as found in 
biological systems [17] and their mathematical and cognitive models [16, 22]. 
Furthermore, as shown below, the algorithms lend themselves to analog VLSI 
implementation in a parallel distributed architecture which, unlike more compli- 
cated gradient-based schemes, resembles the general structure and connectivity 
of biological neural systems. 
18.3.2 
Reinforcement Learning Architecture 
While reinforcement learning does not perform gradient descent of a (known) 
error functional, the eligibility functions e~(t) and di(t) used in the weight 
updates are constructed from derivatives of output functions to the weights. 
The eligibility functions in equation (18.8) can be explicitly restated as (low- 
pass filtered) gradients of an error function 
with 
S(t) = ly(t)l + q(t) 
ei(t+l) 
= 
5ei(t) + (i-5) c~S(~) 
Ow~ 
di(t + Z) 
= 
Adi(t) + (l-A) OÂ£(t) 
Ovi 
(18.10) 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
417 
Rather than calculating the gradients in (18.10) from the network model, we 
can again apply stochastic pertubative techniques to estimate the gradients 
from direct evaluations on the network. Doing so, all properties of robustness, 
scalability and modularity that apply to stochastic error descent supervised 
learning apply here as well. As in (18.3), stochastic approximation estimates 
of the gradient components in (18.10) are 
0~(t) est 
= 
~(t) g(t) 
Owi 
C95(t) est 
= 
~(t) ~(t) 
Ovi 
where the differential perturbed error 
1 ($(w+w,v+v,t)-g(w-~o,v-v,t)) 
g(t) = ~ 
(18.11) 
Â¢0 
: g(w,v,t) 
Â¢+ 
: g(w+~o,v+v,t) 
Â¢- 
: 
~(w-~,v-v,t) 
(18.12) 
In systems with a continuous-time output response, we assume that the time 
lag between consecutive observations of the three phases of $ is not an issue, 
which amounts to choosing an appropriate sampling rate for t in relation to the 
bandwidth of the system. 
Similarities between the above cellular architectures for supervised learning 
and reinforcement learning are apparent: both correlate local perturbation val- 
ues ~ri, wi or vi with a global scalar index g that encodes the differential effect 
is obtained from two-sided parallel random perturbation w -t- w simultaneous 
with v + v (Iw~l = Ivil = ~). 
A side benefit of the low-pass filtering of the gradient in (18.10) is an im- 
provement of the stochastic gradient estimate (18.11) through averaging. As 
with stochastic error descent supervised learning, averaging reduces the vari- 
ance of the gradient estimate and produces learning increments that are less 
stochastic in nature, although this is not essential for convergence of the learn- 
ing process [4]. 
Figure 18.2 shows the block diagram of a reinforcement learning cell and 
an adaptive critic cell, with stochastic perturbative estimation of the gradient 
according to (18.11). LP~ and LPa denote first-order low-pass filters (18.10) 
with time constants determined by d and A. Other than the low-pass filtering 
and the global multiplicative factor Ã·(t), the architecture is identical to that 
of stochastic error descent learning in Figure 18.1. As before, the estimation 
of ~(t) does not require separate instances of perturbed and non-perturbed 
networks shown in Figure 18.2, and can be computed sequentially by evaluating 
the output of the network and adaptive critic in three phases for every cycle of 

"lla~ ~p.!J~ a^!~.depv (q) 
"lla~ :~u!mea I ~.uama~ao.lu!a 8 (~)) 
'uop.euu!xoadde ~.ua!pe.~:~ 
::)!~,setl::)o~,s ~u~sn ~u!uaea I ~.uauJa3Joju[aJ :~U!~,UaLUaldLU ! aJn],3a~!L~3JE leJaua 9 
E'gI aJn:~!_-I 
(q) 
(Â¢)~a (Â¢)Â¢ + (~)~a 
(lya 
~,~ 
~ 
~ 
.
.
.
.
.
 
A 
(~Â¢ 
(~ u- 
(~,) 
: 
- 
-!~ t 
.i ~!~ 
(~)~o~ (~)Â¢ + (~)~m -~ 
(~)~m 
~,, ...... ,~,;:~~ 
DNIH~[HNIDNH S!NH~LgXg DIHdHO1NOHFIXN 
~II7 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
419 
of the perturbations on the output, and both incrementally update the weights 
pi, w~ or v~ accordingly. The main difference in reinforcement learning is the 
additional gating of the correlate product with a global reinforcement signal Ã· 
after temporal filtering. For many applications, the extra overhead that this 
implies in hardware resources is more than compensated by the utility of the 
reward-based credit assignment mechanism, which does not require an external 
teacher. An example is given below in the case of oversampled A/D conversion. 
18.3.3 System Example: Stable Higher-Order Noise-Shaping Modulation 
We evaluated both exact gradient and stochastive perturbative embodiments of 
the reinforcement learning algorithms on an adaptive neural classifier, control- 
ling a higher-order noise-shaping modulator used for oversampled A/D data 
conversion [3]. The order-n modulator comprises a cascade of n integrators 
xi(t) operating on the difference between the analog input u(t) and the binary 
modulated output y(t): 
xo(t + 1) 
= 
xo(t) +a (u(t) - y(t)) 
(18.13) 
xi(t+l) 
= 
xi(t)+axi_l(t), 
i=l,...n-1 
where a = 0.5. The control objective is to choose the binary sequence y(t) such 
as to keep the excursion of the integration variables within bounds, Ix~(t)t < 
xsat [9]. 
For the adaptive classifier, we specify a one-hidden-layer neural network, 
with inputs x~(t) and output y(t). The network has rn hidden units, a tanh(.) 
sigmoidal nonlinearity in the hidden layer, and a linear output layer. For the 
simulations we set n = 2 and m = 5. The case n = 2 is equivalent to the single 
pole-balancing problem [2]. 
The only evaluative feedback signal used during learning is a failure signal 
which indicates when one or more integration variables saturate, Ix~ (t) l _> Xsat. 
In particular, the signal r(t) counts the number of integrators in saturation: 
r(t) = -b E H(Ix~(t)l - xs~t) 
(18.14) 
i 
where b = 10, and where H(.) denotes a step function (H(x) = 1 if x > 0 and 
0 otherwise). The adaptive critic q(t) is implemented with a neural network 
of identical structure as for y(t). The learning parameters in (18.7), (18.8) 
and (18.9) are 5 = 0.8, ~ = 0.7, 7 = 0.9, c~ = 0.05 and/~ = 0.001. These 
values are consistent with [2], adapted to accommodate for differences in the 
time scale of the dynamics (18.13). The perturbation strength in the stochastic 
version is (r = 0.01. 
Figure 18.3 shows the learning performance for several trials of both ver- 
sions of reinforcement learning, using exact and stochastic gradient estimates. 
During learning, the input sequence u(t) is random, uniform in the range 
-0.5... 0.5. Initially, and every time failure occurs (r(t) < 0), the integration 

420 
NEUROMORPHIC SYSTEMS ENGINEERING 
104~ 
l 
~ 
x x 
['~ 
x 
i 10 
~ 10 ~ 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
~ 
Ã 
Ã 
100 
I 
0 
5 
10 
15 
20 
25 
Trials 
Figure 18.3 
Simulated performance of stochastic perturbative (o) and gradient-based (x) 
reinforcement learning in a second-order noise-shaping modulator. Time between failures 
for consecutive trials from zero initial conditions. 
variables x~(t) and eligibilities ek(t) and dk(t) are reset to zero. Qualitative 
differences observed between the exact and stochastic versions in Figure 18.3 
are minor. Further, in all but one of the 20 cases tried, learning has completed 
(i.e., consequent failure is not observed in finite time) in fewer than 20 consec- 
utive trial-and-error iterations. Notice that a non-zero r(t) is only generated 
at failure, i.e., less than 20 times, and no other external evaluative feedback is 
needed for learning. 
Figure 18.4 quantifies the effect of stochastic perturbative estimation of the 
gradients (18.10) on the quality of reinforcement learning. The correlation in- 
dex c(t) measures the degree of conformity in the eligibilities (both e~(t) and 
d~(t)) between stochastic and exact versions of reinforcement learning. Corre- 
lation is expressed as usual on a scale from -1 to 1, with c(t) = 1 indicating 
perfect coincidence. While c(t) is considerably less than 1 in all cases, c(t) > 0 
about 95 % of the time, meaning that on average the sign of the parameter 
updates (18.7) for exact and stochastic versions are consistent in at least 95 % 
of the cases. The scatterplot c(t) vs. Ã·(t) also illustrates how the adaptive 
critic produces a positive reinforcement Ã·(t) in most of the cases, even though 
the "reward" signal r(t) is never positive by construction. Positive reinforce- 
ment Ã·(t) under idle conditions of r(t) is desirable for stability. Notice that 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
421 
0.6 
0.5 
0.4 
"~ 0â¢3 
~9 ~ 
0.2 
Â© 
~9 0.1 
~>~ 
,6~ 
,~ 
~ 
0 
.~ 
,~ 
.~ -o.1 
-0.2 
..i, :..;.:." ::.":. . 
â¢ â¢ ".' e"=~ 
!.~ ~'~:~ '~...~.. 
. .:"~ 
...~ 
~ 
:1 
â¢ 
â¢ 
",. 
i.. 
! 
â¢ ~ 
~ 
~2~6:. 
~'~".'~ N '" 
"', 
,~..~.. 
,~.~ 
- 
~.: 
~.~ Â£' 
-"~": ,-~ ;i:~. 
...-.~,.. 
â¢ '.;......~ ..:v..:.~.- 
â¢ ::.: ;".?:.!~ " 
â¢ 
.~., 
,.., 
â¢ 
. 
.o ~. 
-0.3 
-0.01 
-0.005 
0 
0.005 
0.01 
0.015 
Reinforcement 
~(t) 
Figure 18.4 
Effect of stochastic perturbative gradient estimation on the reinforcement 
learning, c(t) quantifies the degree of conformity in the eli~;ibilities ei(t) and di(t) between 
exact and stochastic versionsâ¢ 
the failure-driven punishment points (where r(t) < 0) are off-scale of the graph 
and strongly negative. 
We tried reinforcement learning on higher-order modulators, n = 3 and 
higher. Both exact and stochastic versions were successful for n = 3 in the 
majority of cases, but failed to converge for n = 4 with the same parame- 
ter settings. 
On itself, this is not surprising since higher-order delta-sigma 
modulators tend to become increasingly prone to unstabilities and sensitive to 
small changes in parameters with increasing order n, which is why they are 
almost never used in practice [3]. It is possible that more advanced reinforce- 
ment learning techniques such as "Advanced Heuristic Dynamic Programming" 
(AHDP) [31] would succeed to converge for orders n > 3. AHDP offers im- 
proved learning efficiency using a more advanced, gradient-based adaptive critic 
element for prediction of reward, although it is not clear at present how to map 
the algorithm efficiently onto analog VLSI. 
The above stochastic perturbative architectures for both supervised and re- 
inforcement learning support common "neuromorphs" and corresponding ana- 
log VLSI implementations. Neuromorphs of learning in analog VLSI are the 
subject of next section. 

422 
NEUROMORPHIC SYSTEMS ENGINEERING 
18.4 
NEUROMORPHICANALOGVLSI LEARNING 
18.4.1 Adaptive Circuits 
The model-free nature of the stochastic perturbative learning algorithms does 
not impose any particular conditions on the implementation of computational 
functions required for learning. By far the most critical element in limiting 
learning performance is the quality of the parameter update increments and 
decrements, in particular the correctness of their polarity. Relative fluctuations 
in amplitude of the learning updates do not affect the learning process to first 
order, since their effect is equivalent to relative fluctuations in the learning 
rate. On the other hand, errors in the polarity of the learning updates might 
adversely affect learning performance even at small update amplitudes. 
A binary controlled charge-pump adaptive element is described next. Of- 
fering precise control of the update polarity, this circuit element provides the 
primitives for learning as well as memory in the analog VLSI systems covered 
further below. 
Charge-pump adaptive element. 
Figure 18.5 shows the circuit diagram 
of a charge-pump adaptive element implementing a volatile synapse. The cir- 
cuit is a simplified version of the charge pump used in [10] and [8]. When 
enabled by ENn and ENp (at GND and Vdd potentials, respectively), the cir- 
cuit generates an incremental update of which the polarity is determined by 
POL. The amplitude of the current supplying the incremental update is deter- 
mined by gate voltages Vb~ and Vbp, biased deep in subthreshold to allow fine 
(sub-fC) increments if needed. The increment amplitude is also determined by 
the duration of the enabled current, controlled by the timing of ENn and ENp. 
When both EN~ and ENp are set midway between GND and Vdd, the current 
output is disabled. Notice that the switch-off transient is (virtually) free of 
clock feedthrough charge injection, because the current-supplying transistors 
are switched from their source terminals, with the gate terminals being kept at 
constant voltage [10]. 
Measurements on a charge pump with C = 0.5 pF fabricated in a 2 #m 
CMOS process are shown in Figure 18.6. Under pulsed activation of ENn 
and ENp, the resulting voltage increments and decrements are recorded as a 
function of the gate bias voltages Vb~ and Vbp, for both polarities of POL, and 
for three different values of the pulse width At (23 #see, 1 msec and 40 msec). 
In all tests, the pulse period extends 2 msec beyond the pulse width. The 
exponential subthreshold characteristics are evident from Figure 18.6, with 
increments and decrements spanning four orders of magnitude in amplitude. 
The lower limit is mainly determined by junction diode leakage currents, as 
shown in Figure 18.6 (a) for At = 0 (0.01 mV per 2 msec interval at room 
temperature). This is more than adequate to accommodate learning over a 
typical range of learning rates. Also, the binary control POL of the polarity 
of the update is effective for increments and decrements down to 0.05 mV in 
amplitude, corresponding to charge transfers of only a few hundred electrons. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
423 
POL -- 
Figure 18.5 
EN, 
Ybp 
G. 
ENn 
/adapt 
Wstore d 
* 
AQadapt l 
__C 2 
_
_
 
_ 
Charge-pump adaptive element implementing a volatile synapse. 
Analog storage. 
Because of the volatile nature of the adaptive element used, 
a dynamic refresh mechanism is required if long-term local storage of the weight 
values after learning is desired. A robust and efficient self-contained mechanism 
that does not require external storage is "partial incremental refresh" [10] 
+ 1) 
= 
- 
(18.15) 
obtained from binary quantization Q of the parameter value. Stable discrete 
states of the analog dynamic memory under periodic actication of (18.15) are 
located at the positive transitions of Q, illustrated in Figure 18.7. Long-term 
stability and robustness to noise and errors in the quantization requires that 
the separation between neighboring discrete states A be much larger than the 
amplitude of the parameter updates 6, which in turn needs to exceed the spon- 
taneous drift in the parameter value due to leakage between consecutive refresh 
cycles [10]. 
Partial incremental refresh can be directly implemented using the adaptive 
element in Figure 18.8 by driving POL with a binary function of the weight 
value [7]. As in [7], the binary quantization function can be multiplexed over 
an array of storage cells, and can be implemented by retaining the LSB from 
A/D/A conversion [6] of the value to be stored. Experimental observation of 
quantization and refresh in a fabricated 128-element array of memory cells has 
confirmed stable retention of analog storage at 8-bit effective resolution over a 
time interval exceeding 109 refresh cycles (several days) [7]. 
A non-volatile equivalent of the charge-pump adaptive element in Fig- 
ure 18.5, which does not require dynamic refresh, is described in [13]. Corre- 
spondingly, a non-volatile learning cell performing stochastic error descent can 
be obtained by substitution of the core adaptive element in Figure 18.8 below, 
and more intricate volatile and non-volatile circuits implementing stochastic 
reinforcement learning can be derived from extensions on Figure 18.8 and [13]. 

424 
NEUROMORPHIC SYSTEMS ENGINEERING 
~ 10Â° I 
~ 10-1t 
~ 
~ lO-2! 
~ 10_3 
~ 10 -4 
O 
> lo-5 
~Om~ ~ 
/ 
At=O 
. 
. 
â¢ 
. 
. 
â¢ 
...... 
.
.
.
.
 
. 
. 
. 
. 
. 
â¢ 
. 
. 
. 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
Gate Voltage Vbn (V) 
(a) 
E 
o 
> 
lO o 
10-1 
10 -2 
10-3 
10 -4 
10 .5 
0 
0.1 
0.2 
0.3 
0.4 
0.5 
0.6 
Gate Voltage Vbp (V) 
(b) 
Figure 18.6 
Measured characteristics of charge-pump adaptive element in 2 #m CMOS 
with O ---- 0.5 pF. 
(a) n-type decrements (POL -- 0); 
(b) p-type increments 
(POL = 1). 

Figure 18.7 
tal refresh. 
ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
425 
Q (Pi) 
+1 
.... 
I 
l 
..,,----~ 
: 
A 
Example illustrating binary quantization Q and partial incremen- 
The non-volatile solution is especially attractive if long-term storage is a more 
pressing concern than speed of adaptation and flexibility of programming. 
Stochastic perturbative learning cell. 
The circuit schematic of a learning 
cell implementing stochastic error descent is given in Figure 18.8. The incre- 
mental update -~ri~ to be performed in (18.4) is first decomposed in amplitude 
and sign components. This allows for a hybrid digital-analog implementation 
of the learning cell, in which amplitudes of certain opcrands are processed in 
analog format, and their polarities implemented in logic. Since I~ril -- 1, the 
amplitude ~1~1 is conventiently communicated as a global signal to all cells, 
in the form of two gate voltages Vbn and Vbp. The (inverted) polarity POL is 
obtained as the^(inverted) exclusive-or combination of the perturbation 7~i and 
the polarity of $. The decomposition of sign and amplitude ensures proper con- 
vergence of the learning increments in the presence of mismatches and offsets in 
the physical implementation of the learning cell. This is because the polarities 
of the increments are more accurately implemented through logic-controlled 
circuitry, which are independent of analog mismatches in the implementation. 
The perturbation ~ri is applied to pi in three phases (18.4) by capacitive 
coupling onto the storage node C. The binary state of the local perturbation 
~ri selects one of two global perturbation signals to couple onto C. The pertur- 
bation signals (V~ + and its complement Va- ) globally control the three phases 
Â¢o, Â¢+ and Â¢- of (18.4), and set the perturbation amplitude o. The simple 
configuration using a one-bit multiplexer is possible because each perturbation 
component can only take one of two values +0. 
18.4.2 Learning Systems 
Continuous-time trajectory learning in an analog VLSI recurrent 
neural network. 
On-chip learning of continuous-time recurrent dynamics 
has been demonstrated in an analog VLSI neural network, using stochastic 
error descent I5, 8]. We briefly summarize the architecuture, operation and re- 
sults here. The chip contains an integrated network of six fully interconnected 

426 
NEUROMORPHIC SYSTEMS ENGINEERING 
^ 
sign(~g) 
Y~+ 
ENp 
t 
ENn 
Cperturb 
~ 
~ 
pi(t) + ~a(t) r~i(t) 
Cstore 
_
_
 
Figure 18.8 
Circuit schematic of a learning cell implementing stochastic error descent, 
using the charge pump adaptive element. 
continuous-time neurons 
d 
6 
T ~x~ = --xi q-E 
j=l 
W~ a(xj - 0j) + y~ , 
(18.16) 
with xi(t) the neuron states representing the outputs of the network, yi(t) the 
external inputs to the network, and a(.) a sigmoidal activation function. The 
36 connection strengths Wij and 6 thresholds 0j constitute the free parameters 
to be learned, and the time constant r is kept fixed and identical for all neurons. 
The network is trained with target output signals x~(t) and xT~(t) for two 
neuron outputs, i = 1, 2. The other four neurons are hidden to the output, 
and the internal dynamics of these hidden neuron state variables play an im- 
portant part in optimizing the output. Learning consists of minimizing the 
time-averaged error (18.6) with respect to the parameters Wij and 0j, using 
stochastic error descent. For a consistent evaluation of the stochastic gradient, 
the perturbed function measurements $(p Â± ~-) are performed on a time scale 
significantly (60 times) larger than the period of the target signals. 
All local learning functions, including the generation of pseudo-random per- 
turbations and the stochastic learning update, are embedded with the synaptic 
functions (18.16) in a scalable 2-D array of parameter cells Wij and 0j. The 
circuitry implementing the learning functions is essentially that of Figure 18.8. 
The dynamic refresh scheme described above is incorporated locally in the pa- 
rameters cell for long-term storage of the parameters. A micrograph of the chip 
is shown in Figure 18.9. Power dissipation is 1.2 mW from a 5 V supply, for a 
1 kHz signal being trained. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
427 
Figure 18.9 
Micrograph of an analog VLSI recurrent neural network chip that 
learns continuous-time internal dynamics using stochastic error-descent. Cen- 
ter: 6 Ã 7 array of weight and threshold parameter cells with integrated learning 
and storage functions. Bottom: random binary array generator providing the 
parallel parameter perturbations. 

428 
NEUROMORPHIC SYSTEMS ENGINEERING 
Â¢b) 
~:igure 18.10 
Oscillograms of the network outputs and target signals after 
learning, (a) under weak residual teacher forcing, and (b) with teacher forcing 
removed. Top traces: xl(t) and xiT(t). 
Bottom traces: x~(t) and x2T(t). 
The results of training the chip with a periodic analog target signal repre- 
senting a quadrature-phase oscillator are illustrated in Figure 18.10. Learning is 
achieved in roughly 1500 training cycles of 60 msec each, using "teacher forcing" 
during training for synchronization between network and target dynamics, and 
by careful but unbiased choice of initial conditions for the weight parameters 
to avoid local minima. These conditions are less critical in more general appli- 
cations of nonlinear system identification where the network during training is 
presented input signals to be associated with the the target output signals. 
Reinforcement learning in a VLSI neural classifier for nonlinear 
noise-shaping del~a-sigma modulation. 
A VLSI classifier consisting of 
64 locally tuned, hard-thresholding neurons was trained using reinforcement 
learning to produce stable noise-shaping modulation of orders one and two [9]. 
While this system does not implement the stochastic version of reinforcement 
learning studied above, it presents a particularly simple VLSI implementation 
and serves to demonstrate some of the properties also expected of more ad- 
vanced implementations that incorporate stochastic learning with continuous 
neurons. 
Similar to the "boxes-system" used in [2], the classifier implements a look-up 
table from a binary address-encoded representation of the state space spanned 
by u(t) and xi(t). In particular, y(t) = Yx(t) and q(t) = qx(t) where x(t) is 
the index of the address determined by hard-limiting thresholding operations 
on the components u(t) and xi(t). Each neuron cell, identified by address k, 
locally stores the two parameters Yk and qk in analog format, and updates them 
according to the external reinforcement signal defined in (18.14). 
In its simplest form, the implemented reinforcement learning performs up- 
dates in the the eligible y~ parameters opposite to their thresholded output 
values, each time failure occurs (r(t) = -1). Hysteresis is included in the dy- 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
429 
namics of the Yk updates to ensure some degree of stability under persistent 
negative reinforcement during training, even without use of the adaptive critic 
qk. Although this simple form of reinforcement learning with non-adaptive 
hysteretic critic is not meant to be adequate for more complex tasks, it has 
proven sufficient to train the VLSI neural classifier to produce noise-shaping 
modulation of orders 1 and 2. 
The integrated system contains a cascade of 6 integrators, an ll-bit address 
state encoder, and an address-encoded classifier with 64 reinforcement learning 
neurons on a 2.2 mm Ã 2.2 mm chip in 2 #m CMOS technology. A record of a 
learning experiment reinforcing first-order noise-shaping modulation in the first 
integrator, using 2-bit address encoding x(t) of the polarities of u(t) and xl(t), 
is shown in Figure 18.11. As in the simulations above, the input sequence u(t) 
during training is uniformly random with half full-scale maximum amplitude 
(1 V pp), and the integrator variables x~(t) as well as the eligibilities ek(t) are 
reset to zero after every occurrence of failure, r(t) = -1. The dynamics of 
the state variables and parameters recorded in Figure 18.11 shows convergence 
after roughly 150 input presentations. The time step in the experiments was 
T -- 2.5 msec, limited by the bandwidth of the instrumentation equipment in 
the recording. Notice that the learned pattern of ya at convergence conforms 
to that of a standard first-order delta-sigma modulator [3], which it should in 
this rather simple case. Learning succeeded at various values of the learning 
constants 5 and c~, affecting mainly the rate of convergence. 
Tests for higher-order noise-shaping modulation on the same learning system 
only succeeded for order n -- 2, using a total of 8 parameters Yk. For higher 
orders of noise-shaping, a continuous neuron representation and learning are 
required, as the above simulations of the stochastic reinforcement system indi- 
cate. 
18.4.3 Structural Properties 
The general structure of neuromorphic information processing systems has some 
properties differentiating them from some more conventional human-engineered 
computing machinery, which are typically optimized for general-purpose digital 
programming. Some of the desirable properties for neuromorphic architectures 
are: fault-tolerance and robustness to noise through a redundant distributed 
representation, robustness to changes in operating conditions through on-line 
adaptation, real-time bandwidth through massive parallelism, and modularity 
as a result of locality in space and time. 
We illustrate these properties in 
the two architectures for supervised and reinforcement learning in Figures 18.1 
and 18.2. Since both architectures are similar, a distinction between them will 
not explicitly be made. 
Fault-tolerance through statistical averaging in a distributed repre- 
sentation:. 
Direct implementation of gradient descent, based on an explicit 
model of the network, is prone to errors due to unaccounted discrepancies in 

430 
NEUROMORPHIC SYSTEMS ENGINEERING 
". "~''~ 
Â° Â¢~%'" a" ..... ~" ~,~'~'Â°Â¢'." " "~ "'.t,.;'~" . " .. .'t:f~' 
o..'~,.v...,.~ ":".t ""Â°.'" 
-.~.~.~-. "~'"4". "~" ~ ~',.'--~'. , ~ "n" ~ .&'e, ~,_.:~ .~, ~3:.'.~ -" ~ &.. r-;'.'," "~" ,~'i,-,~#2.a,.'+-F' 
U ( t ) 
g.~ ,:.%..'. 
,~.'~,l,:~ ,,,~" ~'eoo,. o,.,,~.e~o-, 
,,'~. ~f,~:..,..,,,,,,~, 
.go ~ Â°,,.:.'g~'~.l 
,.~" ": -.~" 
â¢ :::: ::..: ":<:.,:..:.:.,;: %. 
:.'...". 
." : - â¢ :vo*:..'.;.'.. ,..,":.;.,,.'."if,,,..'.... 
.":'.,':.;,:,.'.'; .,,',.;::~%'~ .:" ,: 
- 
*- - .- - .- ..... 
-- .~:.:~.--'~ ~ 
.:.~, ~...'~-a'_~-~.'.~ ~. a;.'-,:~-,.-_.~.-a:;a.:~., xl (t) 
"~ 
:; 
; ~" "'ie..'" ~" ..~. :~.';;....,.-'.~..~'. 
".: :. "..". 
".~o..Â£" 
,: 
fit) 
....... 
y(t) 
~
~
~
_
 
_
_
j
-
~
 
......................................... 
yll 
~ 
ylO 
j .......................... 
yO1 
yO0 
0 
100 
200 
300 
400 
500 
Time t (units T) 
Figure 18.11 
First-order modulator experiments: recorded dynamics of state variables 
and parameters during on-chip learning. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
431 
the network model and mismatches in the physical implementation of the gra- 
dient. This is due to the localized representation in the computation of the 
gradient as calculated from the model, in which any discrepancy in one part 
may drastically affect the final result. For this and other reasons, it is unlikely 
that biology performs explicit gradient calculation on complex systems such as 
recurrent neural networks with continuous-time dynamics. Stochastic error de- 
scent avoids errors of various kinds by physically probing the gradient onto the 
system rather than deriving it. Using simultaneous and uncorrelated parallel 
perturbations of the weights, the effect of a single error on the outcome is thus 
significantly reduced, by virtue of the statistical nature of the computation. 
However, critical in the accuracy of the implemented learning system is the 
precise derivation and faithful distribution of the global learning signals ~(t) 
and Ã·(t). Stictly speaking, it is essential only to guarantee the correct polarity 
and not the exact amplitude of the global learning signals, as implemented in 
Figure 18.8. 
Robustness to changes in the environment through on-line adapta- 
tion:. 
This property is inherent to the on-line incremental nature of the stud- 
ied supervised and reinforcement learning algorithms, which track structural 
changes in ~(t) or ~(t) on a characteristic time scale determined by learning 
rate constants such as ~ or ~ and/~. Learning rates can be reduced as con- 
vergence is approached, as in the popular notion in cognitive neuroscience that 
neural plasticity decreases with age and experience. 
Real-time bandwidth through parallelism:. 
All learning operations 
are performed in parallel, with exception of the three-phase perturbation 
scheme (18.4) or (18.11) to obtain the differential index ~ under sequential 
activation of complementary perturbations ~r and -~r. We note that the syn- 
chronous three-phase scheme is not essential and could be replaced by an asyn- 
chronous perturbation scheme as in [12] and [20]. While this probably re- 
sembles biology more closely, the synchronous gradient estimate (18.3) using 
complementary perturbations is computationally more efficient as it cancels 
error terms up to second order in the perturbation strength a [21]. In the 
asynchronous scheme, one could envision the role of random noise naturally 
present in biological systems as a source of perturbations, although it is not 
clear how noise sources can be effectively isolated to produce the correlation 
measures necessary for gradient estimation. 
Modular architecture with local connectivity:. 
The learning operations 
are local in the sense that a need for excessive global interconnects between dis- 
tant cells is avoided. The global signals are few in number and common for 
all cells, which implies that no signal interconnects are required between cells 
across the learning architecture, but all global signals are communicated uni- 
formly across cells instead. This allows to embed the learning cells directly 

432 
NEUROMORPHIC SYSTEMS ENGINEERING 
into the network (or adaptive critic) architecture, where they interface phys- 
ically with the synapses they adapt, as in biological systems. The common 
global signals include the differential index ~(t) and reinforcement signal Ã·(t), 
besides common bias levels and timing signals. ~(t) and Ã·(t) are obtained by 
any global mechanism that quantifies the "fitness" of the network response in 
terms of teacher target values or discrete rewards (punishments). Physiologi- 
cal experiments support evidence of local (hebbian [19]) and sparsely globally 
interconnected (reinforcement [17]) mechanisms of learning and adaptation in 
biological neural systems [11, 25]. 
].8.5 
CONCLUSION 
Neuromorphic analog VLSI architectures for a general class of learning tasks im- 
plementation. The architectures make use of distributed stochastic techniques 
for robust estimation of gradient information, accurately probing the effect 
of parameter changes on the performance of the network. 
Two architectures 
have been presented: one implementing stochastic error-descent for supervised 
learning, and the other implementing a stochastic variant on a generalized form 
of reinforcement learning. The two architectures are similar in structure, and 
both are suitable for scalable and robust analog VLSI implementation. 
While both learning architectures can operate on (and be integrated in) 
arbitrary systems of which the characteristics and structure does not need to 
be known, the reinforcement learning architecture additionally supports a more 
general form of learning, using an arbitrary, externally supplied, reward or 
punishment 
signal. This allows the development of more powerful, generally 
applicable devices for "black-box" sensor-motor control which make no prior 
assumptions on the structure of the network and the specifics of the desired 
network response. 
We presented results that demonstrate the effectiveness of perturbative sto- 
chastic gradient estimation for supervised learning and reinforcement learning, 
applied to nonlinear system identification and adaptive oversampled data con- 
version. A recurrent neural network was trained to generate internal dynamics 
producing a target periodic orbit at the outputs. A neural classifier controlling 
the second-order noise-shaping modulator was trained for optimal performance 
with no more evaluative feedback than a discrete failure signal indicating when- 
ever any of the modulation integrators saturate. The critical part in the VLSI 
implementation of adaptive systems of this type is the precision of the polarity, 
rather than the amplitude, of the implemented weight parameter updates. A 
binary controlled charge-pump provides voltage increments and decrements of 
precise polarity spanning four orders of magnitude in amplitude, with charge 
transfers down to a few hundred electrons. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
433 
References 
[1] J. Alspector, R. Melt, B. Yuhas, and A. Jayakumar. 
A parallel gradient 
descent method for learning in analog VLSI neural networks. In Advances 
in Neural Information Processing Systems, volume 5, pages 836-844, San 
Mateo, CA, 1993. Morgan Kaufman. 
[2] A. G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive 
elements that can solve difficult learning control problems. IEEE Trans- 
actions on Systems, Man, and Cybernetics, 13(5):834-846, 1983. 
[3] J. C. Candy and G. C. Temes. Oversampled methods for A/D and D/A 
conversion. In Oversampled Delta-Sigma Data Converters, pages 1-29. 
IEEE Press, 1992. 
[4] G. Cauwenberghs. A fast stochastic error-descent algorithm for supervised 
learning and optimization. In Advances in Neural Information Processing 
Systems, volume 5, pages 244-251, San Mateo, CA, 1993. Morgan Kauf- 
man. 
[5] G. Cauwenberghs. A learning analog neural network chip with continuous- 
recurrent dynamics. In Advances in Neural Information Processing Sys- 
terns, volume 6, pages 858 865, San Mateo, CA, 1994. Morgan Kaufman. 
[6] G. Cauwenberghs. A micropower CMOS algorithmic A/D/A converter. 
IEEE Transactions on Circuits and Systems I: Fundamental Theory and 
Applications, 42(11):913-919, 1995. 
[?] G. Cauwenberghs. Analog VLSI long-term dynamic storage. In Proceedings 
of the IEEE International Symposium on Circuits and Systems, Atlanta, 
GA, 1996. 
[8] G. Cauwenberghs. An analog VLSI recurrent neural network learning a 
continuous-time trajectory. IEEE Transactions on Neural Networks, 7(2), 
March 1996. 
[9] G. Cauwenberghs. Reinforcement learning in a nonlinear noise shaping 
oversampled A/D converter. In Proc. Int. Syrup. Circuits and Systems, 
Hong Kong, June 1997. 
[10] G. Cauwenberghs and A. Yariv. Fault-tolerant dynamic multi-level stor- 
age in analog VLSI. 
IEEE Transactions on Circuits and Systems II,, 
41(12):827-829, 1994. 
[11] P. Churchland and T. Sejnowski. The Computational Brain. MIT Press, 
1993. 
[12] A. Dembo and T. Kailath. Model-free distributed learning. IEEE Trans- 
actions on Neural Networks, 1(1):58-70, 1990. 
[13] C. Diorio, P. Hassler, B. Minch, and C. A. Mead. 'a single-transistor silicon 
synapse. To appear in IEEE Transactions on Electron Devices. 
[14] B. Flower and M. Jabri. Summed weight neuron perturbation: An ~(n) 
improvement over weight perturbation. In Advances in Neural Informa- 

434 
NEUROMORPHIC SYSTEMS ENGINEERING 
tion Processing Systems, volume 5, pages 212-219, San Mateo, CA, 1993. 
Morgan Kaufman. 
[15] S. Grossberg. A neural model of attention, reinforcement, and discrimina- 
tion learning. International Review of Neurobiology, 18:263-327, 1975. 
[16] S. Grossberg and D. S. Levine. Neural dynamics of attentionally modulated 
pavlovian conditioning: Blocking, inter-stimulus interval, and secondary 
reinforcement. Applied Optics, 26:5015-5030, 1987. 
[17] R. D. Hawkins, T. W. Abrams, T. J. Carew, and E. R. Kandell. A cel- 
lular mechanism of classical conditioning in aplysia: Activity-dependent 
amplification of presynaptic facilitation. Science, 219:400-405, 1983. 
[18] M. Jabri and B. Flower. Weight perturbation: An optimal architecture and 
learning technique for analog VLSI feedforward and recurrent multilayered 
networks. IEEE Transactions on Neural Networks, 3(1):154-157, 1992. 
[19] S. R. Kelso and T. H. Brown. Differential conditioning of associative 
synaptic enhancement in hippocampal brain slices. Science, 232:85-87, 
1986. 
[20] D. Kirk, D. Kerns, K. Fleischer, and A. Bart. Analog VLSI implementa- 
tion of gradient descent. In Advances in Neural Information Processing 
Systems, volume 5, pages 789-796, San Mateo, CA, 1993. Morgan Kauf- 
man. 
[21] H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for 
Constrained and Unconstrained Systems. Springer-Verlag, New York, NY, 
1978. 
[22] P. R. Montague, P. Dayan, C. Person, and T. J. Sejnowski. Bee forag- 
ing in uncertain environments using predictive hebbian learning. Nature, 
377(6551) :725-728, 1996. 
[23] F. Pineda. Mean-field theory for batched-td(A). In Neural Computation, 
1996. 
[24] H. Robins and S. Monro. A stochastic approximation method. Annals of 
Mathematical Statistics, 22:400-407, 1951. 
[25] G. M. Shepherd. The Synaptic Organization of the Brain. Oxford Univ. 
Press, New York, 3 edition, 1992. 
[26] J. C. Spall. A stochastic approximation technique for generating maximum 
likelihood parameter estimates. In Proceedings of the 1987 American Con- 
trol Conference, Minneapolis, 1987. 
[27] M. A. Styblinski and T.-S. Tang. Experiments in nonconvex optimization: 
Stochastic approximation with function smoothing and simulated anneal- 
ing. Neural Networks, 3(4):467-483, 1990. 
[28] R. S. Sutton. Learning to predict by the methods of temporal differences. 
Machine Learning, 3:9-44, 1988. 
[29] C. Watkins and P. Dayan. Q-learning. Machine Learning, 8:279-292, 1992. 

ANALOG VLSI STOCHASTIC PERTURBATIVE LEARNING 
435 
[30] P. Werbos. Beyond regression: New tools for prediction and analysis in the 
behavioral sciences. In The Roots of Backpropagation. Wiley, New York, 
1993. 
[31] P. J. Werbos. A menu of designs for reinforcement learning over time. In 
W. T. Miller, R. S. Sutton, and P. J. Werbos, editors, Neural Networks for 
Control, pages 67-95. MIT Press, Cambridge, MA, 1990. 

A 
Philippe O. 
9 
WINNER-TAKES-ALL 
ASSOCIATIVE MEMORY: 
HAMMING DISTANCE VECTOR 
QUANTIZER 
Pouliquen, Andreas G. Andreou and Kim Strohbehn 
19.1 INTRODUCTION 
Associative processing and associative memories [10, 13, 14] are neuromorphic 
computational paradigms, inspired by the high level brain functions of asso- 
ciative memory and recall. Several experimental VLSI systems with digital 
storage but analog processing have been reported in the literature since the 
seminal work by Sivilotti, Emerling and Mead [8]; (see for example [3, 11, 21] 
and Chapters 16 and 18 of [10]). Systems that incorporate analog storage 
capabilities have also been reported [7, 12, 24]. 
In this paper we revisit associative processing in the context of vector quan- 
tization and memory-based-processor architectures designed and optimized for 
low energy dissipation. Energy efficient processing is achieved through a pre- 
cision-on-demand architecture. By exploiting the statistics in the structure of 
the problem we have designed a system that performs the bulk of the compu- 
tations in parallel using low precision, subthreshold analog circuits. However, 
power hungry fast recursive processing can be employed on demand, to perform 
the small volume of high precision computations. 
The analog VLSI system presented in this paper has evolved from our pre- 
vious work, and combines current-mode techniques and ideas employed in the 
implementation of bidirectional associative memory (BAM) models [4, 5] with 

438 
NEUROMORPHIC SYSTEMS ENGINEERING 
B 
~ 
Bll B01 
Boo B~o 
(a) 
(b) 
Figure 19.1 
Memory cells. (a) Four transistor SRAM cell, (b) Six transistor enhanced 
SRAM cell. 
improved circuits (described in section 19.2) to achieve better performance and 
a much higher bit density [21] in an "industrial strength" design. A winner- 
takes-all (WTA) architecture with positive feedback allows the present system 
to select only one of many competing templates, avoiding the undesirable mix- 
ing of output templates common to BAM architectures. 
19.2 
CIRCUITS AND DESIGN METHODOLOGY 
In this section, we present the core circuits that are used in the designs. We 
begin with the memory cells which stores the templates, followed by the an- 
cillary circuits which are needed for biasing and processing the output of the 
memory. 
We operate our circuits in subthreshold region [18, 25] of the MOS transistor. 
This enables the design of low-power, energy efficient current-mode circuits [2] 
which can be operated at low voltages. In the past, we have applied current 
mode 
techniques to the implementation 
of bidirectional associative memories 
(BAM) 
([4, 5]). These small chips proved that current mode techniques such as 
the clamped bit-line architecture now standard in SRAMs 
([22, 23]) and WTA 
circuits are useful in the design of larger size chips. 
19.2.1 Memory cells 
The bistable memory cell that we developed for use in our associative processors 
is based on Lyon and Schediwy's [17] four transistor static RAM (4T-SRAM) 
cell, reproduced in Figure 19.1(a). 
_
_
 
The 4T-SRAM cell is biased by clamping the bit lines B and B to a voltage 
Vbias and holding the word line W sufficiently above Vbias so that a small amount 
of current can flow through the N-type transistors. The cell is then bistable: 
one of the internal nodes is at Vda (the positive supply rail) while the other at 
Vbi~. The state of the cell can be sensed by comparing the currents on the bit 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
439 
lines. For instance, if the stored state C is a 1 (i.e. Vc = Vdd and V~ = Vbias), 
then current flows on line B, otherwise current would flow on line ~. 
The 4T-SRAM cell can be augmented to add a local arithmetic logic unit 
(ALU) by adding two transistors, as shown in Figure 19.1(b). This Enhanced 
static RAM (E-SRAM) cell is capable of performing simple boolean functions 
of the input W and the stored state C. As in the 4T-SRAM cell, the E-SRAM 
cell is biased by clamping the bit lines Boo, B01, B10, and Bll to Vbias. The 
- -  
word lines W and W are normally in complimentary states: one word line is 
held sufficiently above Ybias while the other is held below Vbias- Under these 
conditions, current flows on only one of the four bit lines, so that the cell 
effectively computes the logical AND of the input and the stored state on bit 
line Bll. 
However, if both word lines are held high, current will flow on two bit lines, 
whereas if both word lines are below Vbias, no current will flow out of the cell 
(and the state of the cell will eventually be lost). For instance, if Vdd = 5V 
and Vbi~ = 3V, we can let the word lines swing from 0V to 5V, and we obtain 
the results shown in Table 19.1. For the 2.0#m n well CMOS processes that 
we have used, this results in a bit line current Ib of approximately 10nA. 
Other boolean functions can be computed by combining the currents from 
multiple bit lines. For instance, if bit lines B01 and B~0 are combined into a 
single line Bmismatch, current will flow on bit line Bmisraatch only if the input 
and the stored state are different: the cell therefore effectively computes the 
logical XOR of the input and the stored state. 
Similarly, bit lines Boo and 
Bll can be combined into a single line Bmatc h to compute the logical XNOR. 
Combining three bit lines yields still more boolean functions such as the logical 
NAND and the logical NOR. In fact, it is easy to see that the E-SRAM cell can 
be constructed to perform any boolean function of the input and the stored 
state. 
19.2.2 
Memory cell arrays 
The E-SRAM cell is usually arranged in an array, as shown in Figure 19.2. 
Binary template vectors Co,..., ck-1 are then stored in the array, one vector 
to each row. A binary input vector w is then presented to the array so that 
the array can perform a massively parallel computation. 
Since all the E-SRAM cells on a given row j share the same bit lines, their 
output currents are summed together. Thus the current I11j on bit line B11y 
is: 1 
rt--1 
Illj --IbEW/ACj/ 
=Ib IWACj I 
i=0 
Thus we can compute correlations between the input vector and the template 
vectors. 
Alternatively, by combining bit lines into a Bmatch bit line and Bmismatch 
line (as we proposed in Section 19.2.1), we can compute 

440 
NEUROMORPHIC SYSTEMS ENGINEERING 
W,~_~ W~_~ 
W~ 
Wo 
, 
Sl lk-1 
Blok-i 
,, ,, 
,, ,, 
,, ,, 
,, ,, 
~
_
 
_ ~ 
_ _ 
Boo~ 
Bo~ 
11~--~ 
B~o~ 
Figure 19.2 
Typical E-SRAM cell array. Each memory cell (MC) is an E-SRAM cell. 
n-1 
~ ~ ~c, ~: E (~, ~) 
~ (~ ~) 
- '0~, Ã·'~0 , 
Ib 
i----0 
which is the Hamming distance between the input and template vector cj. 
For certain applications, we may choose to first mirror the current from each 
of the four bit line rather than directly combining them. For instance, we might 
want to compute 
I w~cj I 
Iwl 
This requires computing [ w Â® cj I and 
I wl=lwAcj IÃ·lwA~l = AljÃ·zloj 
Ib 
simultaneously. Therefore bit lines Bo~ and B~o cannot be directly combined. 
19.2.3 
Writing to the memory cells 
The state of either memory cell is changed by altering the balance of currents 
-- 
in the cell. For instance, assume that C = 1 and C = 0. Then the cell is 
flipped to its complementary state by increasing the current drive of a N-type 
transistor connected to node C until it overpowers the P-type pull-up. 
Thus, the state of the of the four transistor SRAM cell is usually altered 
by pulsing one of the bit lines high while the cell is selected (i.e. word line 
high). This action momentarily shorts one of the internal nodes to Ybias (via 
the corresponding N-type transistor) so that the other internal node is pulled 
high. For instance, pulsing bit line B stores a 1 and pulsing ~ stores a 0. 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
441 
Table 19.1 
Example of E-SRAM operating states for Vdd : 
5V and Vbias : 
3V. 
Ib ~ lOnA. 
input 
state 
output 
Vc 
Vw 
V W 
0V 
5V 
0V 
5V 
3V 
5V 
5V 
3V 
/Boo 
IBol 
IBlo 
IBll 
Ib 
0 
0 
0 
o 
I~ 
o 
o 
5V 
0V 
3V 
5V 
0 
0 
~ 
0 
5V 
0V 
5V 
3V 
0 
0 
0 
~ 
5V 
5V 
3V 
5V 
~ 
0 
~ 
0 
5V 
5V 
5V 
3V 
0 
~ 
0 
~ 
0V 
0V 
-- 
-- 
0 
0 
0 
0 
In contrast, the most common method used to alter the state of the E-SRAM 
cell is to pulse bit lines B01 and B10 low simultaneously, so that the state of the 
_
_
 
input W is written to the cell. For instance, if W is low and W is high, pulsing 
bit line B01 low pulls C low also. 
Note that the difference in the approach to writing these memory cells is 
due to the manner in which the cells are arrayed: in a static RAM array, we 
want to write to a single word, that is we want to update all the cells connected 
to a particular word line. In contrast, when we array the E-SRAM cell for 
associative processing, we usually want to write to a single template, that is we 
want to update all the cells that are connected to the same bit lines. 
A restriction of this write method is that pulsing bit lines B0~ and B~0 low 
precludes directly combining bit lines B01 and B~0 with any other bit lines. For 
some boolean functions it will therefore be necessary to first mirror the bit line 
currents and then sum the mirrored currents appropriately. Alternatively, it 
may be possible to update the templates by using multiple steps. For instance, 
pulsing bit lines Boo and B~0 low sets all the E-SRAM cells to state 1. This 
can be followed by pulsing bit line B01 low to clear those cells for which the 
input is a 0. Thus we can still write to the array even in bit lines Boo and B10 
have been combined into a single line. 
Even if the bit lines have not been combined, it may be desirable to pulse 
a single bit line in order to update only some of the cells in a template vector. 
For instance, in Carpenter and Grossberg's ART1 algorithm, which is described 
in [6], a template is updated to the logical AND of the template with the input. 
This is easily achieved by pulsing only bit line B01 low. 
19.2.4 
Current conveyors 
The bit lines in the E-SRAM array need to be clamped to Ybias while the current 
is sensed or mirrored by other circuits. 
This bit line clamping is achieved 
through the use of current conveyors. 

442 
NEUROMORPHIC SYSTEMS ENGINEERING 
__:, 
(a) 
(b) 
Figure 19.3 
Current conveyors. (a) Single transistor voltage controlled. (b) Two transistor 
current controlled. 
Figure 19.4 
Bidirectional communication using a current conveyor. 
A current conveyor is a simple circuit that allows a node to be clamped to a 
particular voltage while the current flowing into (or out of) the node is conveyed 
elsewhere (see also [2]). The simplest instance of such a circuit is a single MOS 
transistor as shown in Figure 19.3(a): the gate node Y clamps the source node 
X to Vx = Vy - Vas, while the current Ix at node X is conveyed to node Z. 
The control Y can also be a current instead of a voltage as shown Fig- 
ure 19.3(b). 
This current controlled current conveyor can also be used for 
bidirectional communication as shown in Figure 19.4: the remote circuit simul- 
taneously mirrors the current Iy as I.~ (by applying the voltage Vx to the gate 
of a transistor), and sends a current Ix back (by directly applying it to node X) 
so that it is conveyed to node Z as current I~. Multiple remote circuits can be 
attached to the same X node such that the conveyed current is the sum of the 
currents sourced by the remote circuits. Thus, the current controlled current 
conveyor was used extensively in earlier BAMs to implement a "neuron" in 
which node X is used as axon and dendritic tree simultaneously (see [3]). 
However, neither of the two current conveyors shown in Figure 19.3 is appro- 
priate for clamping the bit lines, because they source (rather than sink) current 
and because the voltage at node X is process dependent. The first problem can 
be addressed by replacing the N-type transistors with P-type transistors. The 
second problem is solved by embedding a transconductance amplifier (as shown 
in Figure 19.5) so that the node voltage at X will equal the voltage at Y. 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
J 
Vb< 
Y 115}x 
~ 
~ 
443 
Figure 19.5 
Current conveyor as used for bit line clamping. 
t,o  1 
1 
 o1,1 
Pil~i2~tg:n-fl~ ~1! ~ ~lt ___ ~ 
~b~i~ 
= 
# 
l 
. 
~ 
(~) 
(b) 
Figure 19.6 
Winner-takes-all circuits. (a) voltaÂ£e mode. (b) current mode. 
The current conveyors shown in Figures 19.3 and 19.5 can only handle uni- 
directional currents at node X. Although this limitation can be addressed by 
designing bidirectional current conveyors, we have found it useful for imple- 
menting winner-takes-all structures. 
The simplest voltage mode and current mode winner-takes-all circuits (see 
also [16]) are shown in Figure 19.6. In either case, the circuit is composed of 
multiple unidirectional current conveyors attached together at their X nodes, 
and a current sink Ib. Since each current conveyor can only pull the common 
X node up, the current conveyor with the largest input will control the X node 
voltage and its output (or conveyed) current will equal Ib. If multiple current 
conveyors have the same (or very nearly the same) input, then they will share 
the bias current equally (or nearly equally). Therefore, if the output current of 
a particular current conveyor exceeds 1 
Fib, then it clearly must have the largest 
input. 
Current controlled current conveyors are also used to implement CMOS 
translinear circuits. Translinear circuits (see also [9]) are a class of circuits 
which take advantage of the exponential dependence of drain current on gate 
to source voltage (in the MOS transistor in subthreshold operation, see [1, 2]) 
to perform multiplication and division of currents. The archetypical CMOS 
translinear circuit is the normalizer circuit shown in Figure 19.7. 

444 
NEUROMORPHIC SYSTEMS ENGINEERING 
g__ 
+ 
Figure 19.7 
CMOS translinear normalizer. 
By using the approximation that IDS = Ioe~ in subthreshold, we discover 
that the circuit computes 
IaIb 
out 
-
-
 
Ic 
With a few translinear circuits, some current mirrors and current summing 
junctions, we can design circuits which implement almost any polynomial or 
fractional function. 
19.2.5 Mapping Neuromorphic Algorithms to VLSI 
As an example of a simple integrated circuit that can be implemented with 
the circuits described in the previous sections, consider the implementation of 
Carpenter and Grossberg's ART1 algorithm ([6]). 
Given the input pattern w, we need to find the template cj that maximizes 
IWACj } 
o~+ I ~ I 
subject to the vigilance constraint 
IwAc~l 
Iwl 
where a and p are external parameter. 
>p 
With the templates stored in the E-SRAM array, and the bit lines clamped 
by the circuit in Figure 19.5, I w A cj l, I cj t, and I w I can be computed 
trivially by combining the bit line currents as follows: 
11 lj 
I wAcj I= Ib 
I c I;I wAcj I Ã· IwAcj I; Zllj Ã·Iolj 
g 
I w I--I wAcu I + Iwi~ 
I = Illj Ã·I10j 
/u 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
445 
Therefore, using the translinear normalizer (Figure 19.7, we can compute 
IAÂ¢- I wic~ lib_ 
Illj 
O~q- [ Cj [ 
Ia q- Illj q- Iolj Ib 
IBj -- I wAcj lib - 
Illj 
I w I 
Once the template is selected, it is updated to 
Cjnew ~ W A Cjold 
This update is accomplished by pulsing bit line B01j low. 
If no template satisfies the vigilance, a new template must be created. For 
a CMOS implementation, the chip would be initialized with all template bits 
set to one, and all templates disabled. Then, as new templates are needed, 
they would be enabled and written to using the same technique as for updating 
templates. 
19.3 
A WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
The WAM associative memory can be mathematically described as a Hamming 
distance vector quantizer. Given the input pattern w, we need to find the 
template cj that maximizes 
t w*cj I 
We have designed a CMOS chip using the circuits described in the previous 
section (see also [21]) to implement the above function, i.e. to compare an 
input binary vector with the stored templates, and return the id (or template 
number) of the closest matching template. This chip can store up to 116 binary 
vectors (or templates), each of which is up to 124 bits in length. Because it uses 
a winner-takes-all circuit to select the closest matching template, it is called a 
winner-takes-all associative memory, or WAM. 
19.3.1 
Architecture 
The organization of the WAM chip is depicted in Figure 19.8. 
It is composed mainly of an array of memory cells (labeled MC), which 
stores the templates and computes the bitwise exclusive-or of the input with 
each template, and a winner-takes-all circuit (labeled WTA) to select the closest 
matching template. 
In addition, shift registers are used to load the input pattern into the chip 
(shift register SR1), select a row to update (shift register SR2), and selectively 
disable a column (shift register SR3) or a row (shift register SR4). Note that 
the input shift register has an additional latch (which is not shown) which 
allows a new input pattern to be shifted-in while the rest of the chip computes 
the closest match on the previous input pattern. 
Finally, a ROM table (labeled ROM) converts the output of the winner- 
takes-all into a seven bit row id which is the chip's output. 

446 
NEUROMORPHIC SYSTEMS ENGINEERING 
S 
R 
2 
SR1 
SR3 
MC 
TW 
R 
S 
0 
R 
A 
M 
4 
Figure 19.8 
Architecture of the WAry1 chip. 
3V 
Vj 
Bmisma~ch 
Figure 19.9 
Write control circuit. 
19.3.2 Implementation 
The array of E-SRAM cells is like the one shown in Figure 19.2, except that 
the bit lines have been combined into match and mismatch lines as described 
in Section 19.2.1. 
Each mismatch line Bmismatchj is connected to a write control circuit shown 
in Figure 19.9. The write control circuit is controlled by an external write 
signal U, and the select signal Vj from the update shift register SR2. When 
row j is selected for writing, the corresponding signal Vj is set high, and the 
write signal U is pulsed high momentarily. This pulses Bmismatchj low so that 
the data in the input latch is written to row j. 
Each match line Bm~tch is connected to a current conveyor of the type shown 
in Figure 19.5. The conveyed current is fed to node Q of the modified current 
mode winner-takes-all circuit shown in Figure 19.10. 
This winner-takes-all circuit is composed of two transistors, MI and M2 
which form the current controlled current conveyor, a boost circui~ (detailed in 
Figure 19.11), and some supplementary transistors. 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
447 
3V 
Mlo 
disable~ f 
-
-
 
Common 
X line 
_ 
~4 M~I 1~2 
M3~ 
~reset 
Figure 19.10 
Modified WAM winner-takes-all circuit. 
Transistor M3 is part of the current sink Ib required for the winner-takes-all 
circuit (as described in Section 19.2.4. This current sink is distributed among 
each winner-takes-M1 circuit to minimize voltage drops across the common X 
line which would otherwise favor the row closest to the current sink. Transistor 
M4 is connected to shift register SR4 and is used to inhibit the winner-takes-all 
by sinking all the current from the Brnatchj 
line conveyed to node Q. 
The boost circuit is shown in detail in Figure 19.11. Its main purpose is 
to detect when row j is winning. To this end, current Isense is mirrored by 
1 
transistors M5 and M6 and compared to current /threshold ---- ~Ib generated by 
transistor MT. 
3V 
M~ 
~/winner __Z 
I sonso 
~-Vthreshold 
Figure 19.11 
Details of the winner-takes-all boost circuit. 
The resulting signal S and its complement ~ (generated by Ms and Mg) are 
used to drive the ROM cell, an example of which is shown in Figure 19.12. 
Each ROM cell is composed of eight MOS transistors, seven of which encode 
the row number in binary, while the eighth is connected to a "valid" line V (to 
signal the host computer that a valid row number is present on the output). A 
typical ROM cell is shown in Figure 19.12. 
Since the threshold for the boost circuit is 1 
~Ib, only one row can have a high 
S signal. Therefore, only one ROM cell can drive the output lines at any given 

448 
NEUROMORPHIC SYSTEMS ENGINEERING 
D6 
D5 
D4 
D3 
D~ 
i 
D1 
Do 
V 
Figure 19.12 
ROM line for row 37 (binary 0100101). 
time. However, to prevent oscillations between two potential winning rows, 
the ~ signal is also connected to transistor M10 in the winner-takes-all circuit: 
when row j wins, transistor M10 dumps a large amount of current into the 
input (node Q) of the winner-takes-all circuit. This effectively pulls node Q up 
to 3V, raising the common X line sufficiently high (approximately 2.2V) so that 
no other row can win thereafter. Of course, this positive feedback necessitates 
the introduction of a reset signal which drives the common X line to 5V via 
transistor Mll. With the common X line at 5V and node Q at 3V, transistor 
M2 passes no current so that the boost circuit is shut off. 
The final purpose of the boost circuit is to provide an external ADC with a 
copy of the Bmatc h current from the winning row. This is accomplished with 
transistors M12 and M13. Transistor M13's gate is connected to to the gate 
of transistor Mc of the bit line clamp shown in Figure 19.5: when transistor 
M12 is switched on by the boost circuit, M13's source is driven to 3V so that 
it mirrors the current conveyed by the bit line clamp. 
The column driving circuitry, shown in Figure 19.13, takes advantage of 
the special modes of operation of the E-SRAM (shown in the lower portion of 
Table 19.1). A column can be disabled by setting E to 0. Then, the word lines 
-
-
 
W and W are both set to 1 (so that all cells in that column output current on 
both bit lines regardless of the stored state), or to 0 (so that no cell in that 
column outputs a current), depending on the input D. 
Column or row disabling is used mainly to reduce the effective size of the 
memory cell array to the size of the associative processing problem being com- 
puted. However, disabling can also be used in the case of a defective memory 
cell or winner-takes-all circuit to enhanced the "fault tolerance" of the chip. 
Since the outcome of the winner-takes-all depends on the magnitude of the 
currents from the memory cells, it is important that all the match currents 
from the memory cells be the same. 
This means that the voltages on the 
-
-
 
Bmatch, Bmismatch, W, and W lines must be the same. It also means that the 
transistors in the memory cells, the current conveyors, and the winner-takes-all 
circuits must be well matched. 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
449 
:;:i 
[ 
[ 
(a) 
E 
DWW 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
1 
1 
1 
1 
0 
(b) 
Figure 19.13 
Column driver. (a) Circuit. (b) Operation. 
More specifically, since the two N-type transistors attached to the Bmatch 
line control the current to the winner-takes-all circuit, they must be particularly 
well matched. Consequently, the memory cell uses N-type transistors in that 
position rather than P-type transistors because prior research [20] has shown 
N-type transistors to be better matched than their complementary P-types for 
n-well processes. 
19.3.3 Fabrication, Testing, and Operation 
The associative memory chip was implemented in a standard single-poly, 
double-metal 2.0#m n-well CMOS process on a 6.Smm by 6.9mm die. The 
memory array occupies 54% of the chip area, the WTA 17%, the digital cir- 
cuits (including the ROM table) 16%, and the input/output pad circuitry 8%. 
The remaining is occupied by miscellaneous circuitry and routing channels. 
The chip has 124 columns and 116 rows (15Kb) for a total bit density of 534 
bits/mm 2. A photomicrograph of the chip is shown in Figure 19.14. 
We also designed a printed circuit support board to facilitate testing of the 
WAM. The support board allows the host computer to directly control the 
WAM functions. However, the support board also contains FIFO data buffers 
and a finite state machine so that it can be operated in an automatic pipelined 
mode. 
We have tested the WAM in a simulated problem of optical character recog- 
nition. We loaded the WAM with a 8 Ã 15 character font containing the 94 
common keyboard characters (ASCII codes 33 through 126) and 22 additional 
miscellaneous symbols. 
We have found that the WAM always correctly classifies an input pattern 
as long as there are no other templates within a Hamming distance of 10 from 
the correct template. This is consistent with the known transistor mismatching 
limits. Indeed, given the size of the transistors in the memory cell, a variation 
of up to Â±10% in the current is quite normal. Therefore, if two templates 

450 
NEUROMORPHIC SYSTEMS ENGINEERING 
Figure 19.14 
Photomicrograph of the chip. 
are competing, with one template having one hundred matching bits, and the 
other one hundred and ten, it is still possible that the first template will win. 
Therefore, for the proper template to always win under normal circumstances, 
it must have a margin of at least ten percent over the other runner-ups. 
One of the worst cases is shown in Figure 19.15. The characters 'c', 'e', and 
'o' are all quite close to each other. In fact, there are many input patterns that 
equidistant (in Hamming distance) to 'c', 'e', and 'o', one of which is shown 
on the right in Figure 19.15. This particular pattern is at a Hamming distance 
of 5 (i.e 5 bits don't match and 115 bits match) from 'c', 'e', and 'o', and can 
therefore be classified as any of 'c', 'e', or 'o'. However, if we change one, two 
or even three pixels to bring it closer to either 'c', 'e', or 'o', the WAM may 
incorrectly classify it. 
Most character bit patterns are rather far from each other in a Hamming 
distance sense (especially if we limit ourselves to the 94 keyboard characters), 
so that in practice, we don't often encounter mis-classified inputs. Figure 19.16 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
451 
shows a histogram of the Hamming distances between pairs of characters in the 
font. The dark-colored histogram is for the 94 keyboard character subset. 
l: .... :l 
lilUnllU 
Jiillnli 
liliiii! 
liiniiin 
i-'-::::-"i 
Figure 19.15 
WAM processing example. On the left, bit patterns for 'c', 'e', and 'o'. On 
the right, input bit pattern equidistant to 'c', 'e', and 'o'. 
We have compared the chip to state of the art commercially available mi- 
croprocessor based systems. For a given machine, the Hamming distance clas- 
sification algorithm was implemented in the most efficient way (i.e. we do not 
simulate the WAM on the general purpose processor.) In an DEC-Alpha based 
general purpose computer it takes 10000 cycles to do a single pattern matching 
computation and thus it takes a total of 20#s per classification. Power dissi- 
pation is 30W at 500 MHz and therefore the energy per classification is 600#J. 
The Pentium-Pro is worse, because it requires 30W at 150 MHz and more than 
10000 cycles for a single pattern matching. In contrast, the total current in the 
WAM is: (124x116x10) nA continuous bias current for the memory cells at 5V. 
Computation time is approximately 70#s for a total energy per classification of 
approximately 100 nJ. The power dissipation in the buses and memory of the 
general purpose processor systems are not included in this comparison. 
From the above calculations, it can be seen that the WAM chip performs 
virtually as fast as the general purpose microprocessor systems but with three 
orders of magnitude or more of savings in energy per classification. 
19.3.4 Arbritrary Precision Classification 
By modifying slightly the WAM architecture, we have developed algorihtms 
to correct misclassifications due to the low precision analog hardware. When 
comparing patterns with a large proportion of matching bits, the templates 
in the top ten percent category must have an equally large number of bits in 
common. For instance, if as before, one template has one hundred matching 
bits, and the other one hundred and ten, then there must be at least 90 bits 
in the input that match both templates simultaneously, because there are only 
120 bits in each pattern. 
Therefore, by temporarily masking out the columns corresponding to these 
common bits, the effect of device mismatch can be greatly mitigated. In the 
above example, we can ignore at least 90 bits, leaving the first template with 
10 matching bits and the second template with 20 matching bits out of a total 
of 30 bits (or a margin of 33.3% rather than 8.3%). 

452 
NEUROMORPHIC SYSTEMS ENGINEERING 
We can therefore make use of a modified, iterative algorithm with the cur- 
rent WAM: at each iteration, the "winning" template and the "runner-ups" in 
the top ten-percent are selected. The remaining "losing" templates are then 
disabled as well as the common bits, and the process is repeated until a satis- 
factory "winner" is found or further iteration is unproductive. 
Selection of the "runner-ups" is easily achieved by observing the digitized 
current from the boost circuit (which indicates the percentage of matching 
bits). The current "winner" is disabled allowing the next runner-up to win, a 
process that is repeated until the percentage of matching bits drops by more 
than ten percent of the original winner. 
~p of 
pairs 
300.' 
200 
100 
00 
10 
20 
30 
40 
50 
60 
Hamming Distance 
Figure 19.16 
Histogram of the Hamming distance between character pairs from the font 
used in WAM testing. The subset of 94 keyboard characters is in black. 
19.4 
FUTURE WORK 
Limitation of device matching for this WAM chip has already been analyzed 
in [15]. This research has allowed us to focus our efforts on those parts of 
the WAM chip whose increased area would improve the device matching and 
thereby most affect the WAM's performance. 
We have also examined the incorporation of UV adaptation in the winner- 
takes-all circuit to improve the resolution of the WAM (see [19]). 
Finally, we have reorganized the layout of the memory cell thereby reducing 
its total area by 16% while increasing the area of the N-type transistors to 
improve memory cell matching. A long-word version of the WAM could be 
fabricated in the state-of-the-art process using a large die. For a die area (e.g. 
1.4 x 1.2 cm) and a small fabrication feature size (e.g. 0.6 #m process), we 
should be able to achieve a 1.2Mb density with 7.5M transistors. It should be 
noted that the WAM implementation already contains features to deal with 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
453 
fault/defect tolerance which will permit us to achieve high yields despite the 
large die area. 
We also plan to modify the design to allow multichip associative memory 
architectures. The current design limits the WAM to single chip devices. How- 
ever, a different WTA circuits could allow the winner-takes-all process to extend 
beyond the chip boundaries by allowing multiple WAM chips to compete for 
the best match. 
In a future design we will also add a small digital processor to implement 
the arbritrary precision classification algorithm on chip. 
19.5 
CONCLUSIONS 
We have described our methodology for building high-density current-mode 
analog VLSI associative processors. We begin with a very flexible six transistor 
memory cell with which we can perform simple boolean operations of the input 
and the stored bit. We then process the currents from an array of these memory 
cells to perform global computations such as the max and/or normalization 
operations. 
We have also describe the implementation of such an associative processor 
chip. This chip determines the closest match between an input bit pattern 
and multiple stored bit templates based on the Hamming distance. It is pro- 
grammable for template sets of up to 124 bits per template and can store up 
to 116 templates. A fully functional 6.Smm by 6.9mm chip has been fabricated 
in a standard single-poly, double-metal 2.0#m n-well CMOS process. 
The design abstracts on several principles found in biological systems. 
1. Memory and processing are integrated in a single structure; this is anal- 
ogous to the synapse in biology. 
2. The system has an internal model that is related to the problem to be 
solved (prior knowledge). 
This is the template set of patterns to be 
classified. 
3. The system is capable of learning i.e. templates can be changed to adapt 
to a different character set (different problem). This is done at the expense 
of storage capacity -we use a RAM based cell instead of a more compact 
ROM cell-. 
4. The system processes information in a parallel and hierarchical fashion 
in a variable precision architecture. I.e. given the statistics of the prob- 
lem, most of the computation is carried out with low precision (three or 
four bit) analog hardware. Yet arbritrary precision computation is pos- 
sible through recursive processing that exploits a programmable WTA 
(capability to mask specific bits in the winner takes all circuitry). 
5. The system is fault tolerant and gracefully degrades. The same structures 
that is used in the precision-on-demand architecture can also be used 

454 
NEUROMORPHIC SYSTEMS ENGINEERING 
to reconfigure the system for defects in the fabrication process. 
The 
components of the chip that are worse matched can be disabled during 
operation. 
The experimental system presented in this paper suggests that, robust, 
miniature, and energetically efficient hardware VLSI systems can ultimately be 
achieved by following a methodology which optimizes the design at and between 
all levels of system integration; from the device and circuit technique levels all 
the way to algorithmic and architectural level considerations. In the future as 
technologies for memory based systems become available for experimentation, 
we will be able to address the complexity of real world problems. However, 
even with non optimized processor-oriented fabrication processes, useful and 
perhaps even practical memory based neuromorphic systems are still feasible. 
Acknowledgments 
The research was partially supported by NSF grant ECS-9313934; Paul Werbos is the 
program monitor, by the Johns Hopkins Center for Speech and by an ONR Multidis- 
ciplinary University Research Initiative (MURI) for Automated Vision and Sensing 
Systems N00014-95-1-0409. The final version of this document was prepared by one 
of the authors (AGA) during his sabbatical leave at Caltech. We thank Carver Mead 
for his continuing support and encouragement. Chip fabrication was provided by 
MOSIS. 
Notes 
1. A, V, and ~ represent the bit-wise logical AND, logical OR, and logical XOR re- 
spectively. I I is the logical norm, or number of bits set to 1. 
References 
[1] A. Andreou and K. Boahen. Translinear circuits in subthreshold MOS. J. 
Analog Integrated Circ. Sig. Proc., 9:141-166, 1996. 
[2] A. G. Andreou and K. A. Boahen. Analog VLSI signal and information 
processing. In M. Ismail and T. Fiez, editors, Neural information process- 
ing II, pages 358-413. McGraw-Hill, New York, 1994. 
[3] K. A. Boahen and A. G. Andreou. 17. In M. Hassoun, editor, Associative 
Neural Memories: Theory and Implementation. Oxford University Press, 
New York, 1993. 
[4] K. A. Boahen, A. G. Andreou, and P. O. Pouliquen. Architectures for 
associative memories using current-mode analog MOS circuits. In C. L. 
Seitz, editor, Advanced Research in VLSI: Proc. Dec. Caltech Conference 
on VLSI, Cambridge, MA, 1989. MIT. 
[5] K. A. Boahen, P. O. Pouliquen, A. G. Andreou, and R. E. Jenkins. A 
heteroassociative memory using current-mode MOS analog VLSI circuits. 
IEEE T. Circ. Syst, 36(5):747 755, 1989. 

WINNER-TAKES-ALL ASSOCIATIVE MEMORY 
455 
[6] G. A. Carpenter and S. Grossberg. A massively parallel architecture for 
a self-Organizing neural pattern recognition machine. Computer Vision, 
Graphics, and Image Processing, 37:54-115, 1987. 
[7] G. Cauwenberghs, C. F. Neugebauer, and A. Yariv. Analysis and verifica- 
tion of an analog VLSI outer-product incremental learning system. IEEE 
Transactions on Neural Networks, 3(3):488-497, 1992. 
[8] M. R. Emerling, M. A. Sivilotti, and C. A. Mead. VLSI architectures 
for implementation of neural networks. In J. J. Denker, editor, Neural 
Networks for Computing. AIP, Snowbird UT, 1986. 
[9] B. Gilbert. Translinear circuits: A proposed classification. 
Electronics 
Letters, 11(1):14-16, January 1975. 
[10] M. H. Hassoun, editor. Associative Neural Memories: Theory and Imple- 
mentation. Oxford University Press, New York, 1993. 
[11] Y. He, U. Cilingiroglu, and E. SÂ£nchez-Sinencio. 
A high density and 
low-power charge-based hamming network. IEEE Trans. VLSI Systems, 
1(1):55-62, March 1993. 
[12] Y. Horio and S. Nakamura. Analog memories for VLSI neurocomputing. 
In E. SÂ£nchez-Sinencio and C. Lau, editors, Artificial Neural Networks: 
Paradigms, Applications, and Hardware Implementations, pages 344-363. 
IEEE Press, 1992. 
[13] T. Kohonen. Content-Addressable Memories. Springer Verlag, Berlin, 2 
edition, 1987. 
[14] T. Kohonen. Self-Organisation and Associative Memory. Springer-Verlag, 
Berlin, 2 edition, 1988. 
[15] N. Kumar, P. O. Pouliquen, and A. G. Andreou. Device mismatch limita- 
tions on the performance of an associative memory system. In Proceedings 
of the 36 th Midwest Symposium on Circuits and Systems, volume 1, pages 
570-573, 1993. 
[16] J. Lazzaro, S. Ryckebusch, M. A. Mahowald, and C. A. Mead. Winner- 
take-all networks of O(n) complexity. In D.S. Touretzky, editor, Advances 
in neural information processing systems, volume 2, pages 703-711, San 
Mateo - CA, 1989. Morgan Kaufmann. 
[17] R. F. Lyon and R. R. Schcdiwy. CMOS static memory with a new four- 
transistor memory cell. In P. Losleben, editor, Advanced Research in VLSI, 
pages 110-132. MIT Press, Cambridge, MA, 1987. 
[18] C. A. Mead. Analog VLSI and Neural Systems. Addison-Wesley, Reading, 
MA, 1989. 
[19] H. Miwa, N. Kumar, P. O. Pouliquen, and A. G. Andreou. Memory en- 
hancement techniques for mixed digital memory-analog computational en- 
gines. In Proc. IEEE Int. Symp. on Circuits and Systems, volume 5, pages 
45-48, June 1994. 

456 
NEUROMORPHIC SYSTEMS ENGINEERING 
[20] A. Pavasovi~, A. G. Andreou, and C. R. Westgate. Characterization of 
subthreshold MOS mismatch in transistors for VLSI systems. Journal of 
Analog Integrated Circuits and Signal Processing, 6:75-85, July 1994. 
[21] P. O. Pouliquen, A. G. Andreou, K. Strohbehn, and R. E. Jenkins. An 
associative memory integrated system for character recognition. In Proc. 
36th Midwest Syrup. on Circuits and Systems, pages 762-765, Detroit, MI, 
August 1993. 
[22] K. Sasaki, K. Ueda, K. Takasugi, H. Toyoshima, K. Ishibashi, T. Ya- 
manaka, N. Hashimoto, and N. Ohki. A 16-mb CMOS SRAM with a 2.3- 
#m 2 single bit line memory cell. IEEE J. Solid-State Circuits, 28(11):1125 
1130, November 1993. 
[23] K. Seno, K. Knorpp, L.-L. Shu, N. Teshima, H. Kihari, H. Sato, F. Miyaji, 
M. Takeda, M. Sasaki, Y. Tomo, P. T. Chuang, and K. Kobayashi. A 9- 
ns 16-rob CMOS SRAM with offset-Compensated current sense amplifier. 
IEEE J. Solid-State Circuits, 28(11):1119-1124, November 1993. 
[24] H. Yang and B. J. Sheu and J.-C. Lee. A nonvolatile analog neural memory 
using floating-gate MOS transistors. 
In Analog Integrated Circuits and 
Signal Processing, volume 2-1, February 1992. 
[25] E. Vittoz and J. Fellrath. CMOS analog integrated circuits based on weak 
inversion operation. IEEE Journal on Solid-State Circuits, 12(3):224-231, 
1977. 

Index 
Active undamping, 8 
Adaptation algorithm, 343 
Adaptation circuits, 298 
Adaptation rules, 342 
Adaptation, 74, 176, 187, 381 382 
Adapting the sampling rate, 136 
Adaptive critic element, 415 
Adaptive neuron, 142, 237 
Adaptive photoreceptor, 372 
Adaptive phototransduction, 219 
Adaptive quantization, 136, 146 
Adaptive resonance, 384 
Adaptive sampling, 237 
Adaptive traveling-wave, 50 
Adaptive, 3 
Address Event protocol, 344 
Address-event representation, 234 
Address-event streams~ 250 
Address-Event-Representation, 194 
Aer, 110 
Aging, 196 
ALOHA, 196 
Amplitude distribution, 132, 136 
Analog memory storage, 315 
Analog preprocessing, 132 
ANALOG VERSUS DIGITAL, 94 
ANALOG VLSI LEARNING, 422 
Arbitration, 202, 217 
Arbitration, 220 
Arbitration, 238 
Arreguit, 269 
Artificial dendrites, 341 
Artificial nervous systems, 339 
Artificial synapses, 340 
Association, 381 
Associative memory, 437 
Asynchronous retina, 219 
Asynchronous, 217 
Attention system, 171 
Attentional processing, 152 
Attenuating low frequencies, 146 
Attenuating the firing rate, 146 
Auditory models, 106 
Auditory representations, 107, 123 
Auto-associative memories, 384 
Autocorrelation, 109 
AUtomatic Gain Comtrol, 28 
Automatic gain control, 13 
Automatic Gain Control, 31 
Automatic gain control, 34 
Automatic Gain Control, 51, 92 
Automatic gain control, 122, 132, 232 
Automotic Gain Control, 51 
Avalanche-breakdown phenomenon, 330 
Average distance, 414 
Axonal spike, 340 
Backpropagation, 116, 343, 384, 389, 411 
Bandpass filterbank, 15 
Bandpass filtering, 141 
Bandpass filtering, 146 
Bandpass spatial filter, 139 
Bandpass spatiotemporal filtering, 133 
Bandp~ss spatiotemporal filtering, 136 
Bandpass spatiotemporal filtering, 232 
Bandpass temporal filter, 139 
Basilar membrane, 5, 20 
Bidirectional associative memory, 437 
Biological cochlea, 99 
Biological perceptive systems, 203 
Biological synaptic function, 340 
Blooming, 132 
Boltzman learning, 389 
Boltzman statistics, 385 
Boltzmann, 263 
Boolean functions, 439 
Boost circuit, 446 
Bump linearization, 273 
Capacitive memory, 386 
Capacitive-divider, 298 

458 
NEUROMORPHIC SYSTEMS ENGINEERING 
Capacity, 229 
Carrier Sense Multiple Access, 196 
Cascaded filters, 6 
Cascode mirrors, 268 
Cellular neural networks, 390 
Central Limit Theorem, 235 
Chain rule, 384 
Channel capactiy, 235 
Characteristic frequency, 7, 20 
Charge conservation, 263 
Charge, 341 
Charge-pumping, 255 
Classification, 381 
Clustered, 233 
Cochlear implant, 101 
Cochlear models, 122 
Coefficient of variation, 235 
Collision, 202 
Common-mode characteristics, 276 
Communication channel, 130 
Communication processor, 220 
Compact implementations, 146 
Compact probabilistic description, 234 
Comparison, 98 
Compartment capacitor, 352 
Composite gains, 9 
Compression, 7, 24, 44 
Compressive nonlinearity, 8, 100 
Compressive, 3 
Computational primitives, 264 
Connection adaptation, 355 
Contention, 238 
Continuous sensing, 136 
Continuous-time dynamics, 431 
Contour map, 375 
Controlling the gain, 146 
Convolved excitation, 153 
CONVOLVED EXCITATION, 163 
Corner frequency, 51, 62 
Correlate product, 419 
Critical paths, 253 
Crosstalk, 325 
Crowding, 196 
Current conveyor, 442 
Current-mode resistive network, 160 
Current-mode, 375, 438 
Current-source synapses, 349 
Current-spreading networks, 136 
Delta rule, 389 
Dendrite parameters, 349 
Differential pair, 269 
Differential perturbed error, 417 
Differentiation, 384, 389 
Differentiator, 21 
Diffusor network, 368 
Diode-capacitor integrator, 143 
Discrete excitation, 161 
Discrete exicitation, 153 
Discrete-output, 370 
Dispersion relation, 5 
Distortion, 60, 289 
Distributed processing system, 217 
Distributed representation, 429 
Distributed, 388,432 
Distributing locally, 367 
Dynamic programming, 116, 415 
Dynamic range, 85, 100, 107, 132, 289, 295 
Dynamical systems, 413 
Dynamically refreshed capacitors, 344 
Early voltage, 269 
Edge enhancement, 206 
Effective resistance, 157 
Eikonal, 5 
EKV model, 36, 38 
Emitter degeneration, 267, 271 
Encoding of variation, 201 
Enhances edges, 141 
Error performance, 118 
Error-free operation, 257 
Event-driven communication, 255 
Excitatory feedback, 154 
Excitatory synaptic conductance, 349 
Excitatory synaptic currents, 342 
Exploit locality, 243 
Exponential nonlinearities, 269 
Failure-driven, 421 
Fairness, 248 
Fault-tolerance, 429 
Feature extraction, 114 
Feature vector, 116 
Features, 32 
Feedback control, 12 
Filter-cascade, 3, 7-8 
Finite-element, 107 
Floating gate, 386 
Floating gates, 344 
Floating-gate adaptation, 298 
Floating-gate amplifier, 185 
Floating-gate transistors, 315 
Floating-gates, 176 
Follower-integrator, 286 
Four transistor static RAM, 438 
Foveated architecture, 233 
l~requency response, 78 
Frequency-response, 69 
Frequency-to-place, 6, 100 
Full handshaking, 217 
Fuzzy neural systems, 385 
Gain adaptation, 79 
Gain Characteristics, 280 
Gammatone filterbank, 15 
Gate degeneration, 269 
Gate degeneration, 271 
Gaussian distribution, 153 

INDEX 
459 
Gaussian, 235 
Generalization, 410 
Generalized WTA architecture, 372 
Gilbert gain cell, 268 
Gilbert, 344 
Gradient descent, 411 
Gradient estimation, 431 
Group delays, 9 
Half-wave rectification, 233 
Hamming distance, 445 
Hamming window, 115 
Handshake circuit, 244 
Hearing aids, 3 
Hebbian learning, 357, 384 
Hebbian learning, 389 
Hebbian, 339 
Hidden Markov model, 106 
Hidden Markov Models, 19 
Hierarchical arbitration, 225 
Hierarchical, 247 
Highpass temporal and spatial filtering, 232 
Hippocampus, 342 
Hopfield, 389 
Horizontal resistor, 346 
Hot-electron gate current, 316 
Hot-electron injection, 321,330 
Hydrodynamic system, 3 
Hyperpolarization, 349 
Hysteretic behavior, 367, 370 
Hysteretic window, 156 
Image-processing systems, 264 
Impact Ionization, 324 
Information, 263 
Inhibition-of-return, 171 
Inhibitory supply potential, 349 
Inhibitory synapses, 351 
Injection efficiency, 323 
Injection of electrons, 179 
Injection weight-update, 332 
Injection, 317 
Inner-hair-cell, 51 
Input-referred noise, 289 
Instability problems, 367 
Instantaneous distortion nonlinearity, 12 
Instantaneous nonlinear distortion, 12 
Integrity, 230 
Inter-chip communication, 217, 340 
Interchip communication channel, 256 
Interchip communication, 144 
Intracellular potential, 340 
Kohonen map, 212 
Kohonen, 178, 389 
Laplacian, 140 
Latency, 229 
Lateral excitation, 367, 370 
Leakage pathway, 355 
Learning performance, 419 
Learning, 381 382 
Learning-Rate Degradation, 333 
Linear filter, 6 
Linear networks, 207 
Linear range, 289 
Linear target function, 185 
Linearly separable, 139 
LIOUVILLE-GREEN, 3 
Liouville-Green, 4 
Local Automatic Gain Control, 140 
Local excitation, 153, 155 
Local feedback, 367 
Local interconnections, 367 
Local learning functions, 426 
Local membrane potential, 340 
Local perturbation, 417 
Local storage, 385 
Localized excitation, 153 
Logarithmic transfer function, 145 
Logarithmic, 6 
Logarithmically, 370 
Low pass filter, 40 
Low-Frequency Attenuation, 75 
Low-power, 49, 438 
Low-transconductance, 270 
Lumped-parameter, 9 
Magnitude quantization, 342 
Mahowald, 129, 194, 202, 217, 243 
Maximum undistorted, 89 
Mead, 129, 273, 344 
Mean spiking rate, 21 
Micropower, 121 
Minimizing crosstalk, 326 
Minimum detectable, 89 
Model-Free Learning, 391 
Model-free, 412 
Motion encoding, 219 
Multi-layer perceptron, 116, 343 
Multi-representation, 121 
Multichip, 229, 453 
Negative-feedback technique, 269 
Neural synapses, 315 
Neural systems, 264 
Neuromorphic network, 342 
Neuromorphic systems, 229, 243 
Neuromorphic, 3, 6, 9, 102, 114, 152, 176 
Neuromorphic, 193 
Neuromorphic, 218, 230, 264, 340, 368, 375, 
381 
Neuromorphic, 388 
Neuromorphic, 409, 416 
NEUROMORPHIC, 422 
Neuromorphic, 429, 437 
Neuromorphic, 444 
Neuronal dispersion, 230 
Neuronal Ensemble, 234 
Neuronal ensemble, 240 

460 
NEUROMORPHIC SYSTEMS ENGINEERING 
Neuronal ensembles, 234 
Neuronal latency, 230 
Noise accumulation, 101 
Noise amplitude, 86 
Noise spectral density, 58 
Noise spectrum, 84 
Non-arbitrated communication, 204 
Non-linear filter, 372 
Non-linear transfer function, 343 
Non-linear, 122 
Non-volatile Memories, 386 
Nonlinear computation, 155 
Nonlinearities, 70 
Nonlinearity, 267 
Nonvolatile floating-gate, 315 
Normalization, 153 
Normalized, 205, 235 
Normalizer, 207 
Notch, 9 
Offset adaptation, 78 
Offset Characteristics, 281 
Offset-adaptation, 55 
Offset-compensation, 51-52 
Offset-correction, 55 
Ohm's law, 340 
On-chip adaptation, 340, 358 
On-chip analog adaptation, 358 
On-chip computation, 344 
Optimization, 383 
Organizing principles, 264 
Output noise, 57 
Overlapping-cascades architecture, 74 
Overtraining, 410 
Oxide trapping, 333 
Parallelism, 193 
Parasitic capacitance, 255 
Passive dendrites, 341 
Peak detector, 51 
Peak frequency, 100 
Peak throughput, 252 
Perception, 19, 381 
Perceptive systems, 213 
Perceptual Linear Prediction, 117 
Permanent storage, 346 
Phase characteristics, 286 
Phasic transient-sustained response, 233 
Phoneme probability vector, 116 
Phototransistors, 166 
Pipelined communication channel, 230 
Pipelining, 225, 243 
Pixel-parallel communication, 230 
Poisson process, 204 
Polarity, 432 
Position-encoding, 154 
Post-synaptic membrane, 342 
Post-synaptic potentials, 341 
Preattentive, 151 
Precharging, 354 
Presynaptic depolarization, 342 
Precision-on-demand, 437 
Programmable amorphous silicon resistors, 
358 
Pseudoresonance, 20, 25 
Pseudoresonant distance, 27 
Pseudoresonant frequency, 25 
Pseudoresonant, 7 
Punctuated event, 233 
Q-learning, 384, 415 
Quality factor control, 31 
Quality factor modulation distance, 29 
Quality factor, 22, 51 
Quantal noise, 343 
Quantization, 135 
Quantized fluctuations, 342 
Quantizing, 132 
Real time, 105 
Recognition performance, 121 
Recognition, 117 
Rectifier, 38 
Recurrent neural networks, 391 
Redundancy, 129, 135, 193, 202, 232 
Redundant, 429 
Reinforcement learning, 384 
Reinforcement learning, 415-416 
Relative timing, 250 
Resistive-network, 369 
Resistive-spreading excitation, 153 
Resolution, 367 
Resonant, 111 
Retention time, 387 
Retinomorphic pixel, 136 
Retinomorphic system, 237 
Retinomorphic vision system, 129 
Retinomorphic, 129 
Retinormorphic imager, 145 
Reversal potential, 340 
Reward/punishment, 381 
Robust coding, 122 
Robust estimation, 432 
Robust mechanism, 367 
Robustness, 429 
Saccade, 175 
Saccadic eye movements, 187 
Saccadic learning, 178 
Saliency map, 151 
Salt-and-pepper noise, 132, 369 
Sampling circuitry, 354 
Scale invariant, 50 
Schmitt trigger, 208 
Second-order filter, 9, 21, 34, 51, 55, 107 
Second-order filters, 50 
Self-organizing, 384 
Self-timed synapse, 354 
Semiconductor electronics, 264 

INDEX 
461 
Sequential polling, 256 
Short-term adaptation, 175 
Silicon learning systems, 315, 333 
Silicon neurons, 339, 342 
Silicon retina, 206, 375 
Silicon synapses, 333 
Sivilotti, 217, 243 
Slew rate, 35 
Slope coefficient, 369 
Slope factor, 36 
Smoothing, 368 
Soma, 341 
Source degeneration, 271 
Source-degeneration, 269 
Space constant, 158, 369 
Sparse activity, 233 
Spatial derivatives, 152 
Spatial dimension, 4 
Spatial frequency, 5 
Spatial position, 369 
Spatially distributed synapses, 341 
Spatio-temporal computation, 341 
Spatio-temporal filtering, 375 
Spatiotemporal signal, 145 
Spectral distribution, 132 
Spectral subtraction, 123 
SPEECH RECOGNITION, 114 
Speech recognition, 116 
Speech recognizers, 105 
Speech-recognition systems, 3 
Spike train, 341 
Stability, 32 
Statistical information coding, 193 
Stochastic approximation, 411 
Stochastic approximation, 411 
Stochastic component, 234 
Stochastic error-descent, 414 
Stochastic learning, 178 
Stochastic pertubative, 417 
Stochastic perturbative, 410, 421 
Stochastic Supervised Learning, 412 
Stochastic techniques, 432 
Stochastic, 341 
Structural organization, 131 
Structured pattern, 372 
Sublinear summation, 341,358 
Supervised learning, 343 
Supervised Learning, 383 
Supervised learning, 410-411 
Supervised learning, 414 
Supervised, 381 
Switched capacitor, 355 
Synapse transistors, 315 
Synapse weight, 317 
Synapse weight-update rule, 333 
Synapses charge, 345 
Synaptic activations, 340 
Synaptic arrays, 333 
Synaptic efficacies, 342 
Synaptic efficacy, 340 
Synaptic weight, 340 
Temperature, 120 
Temporal adaptation, 109, 111 
Temporal autocorrelation, 111 
Temporal bandwidths, 136 
Temporal derivative, 177, 330 
Temporal derivatives, 152 
Temporal Dispersion, 230 
Temporal dispersion, 241 
Temporal frequency, 133 
Temporary local weight, 357 
Thermal noise, 298 
Three-stage pipeline, 256 
Throughput specification, 241 
Throughput, 230 
Time difference learning, 384, 415 
Time-multiplexed channel, 255 
Timing analysis, 250 
Total harmonic distortion, 50 
Total integrated noise, 88 
Tracking, 164 
Training time, 183 
Transconductance amplifier, 269 
Transconductance, 267 
Translinear loop, 23 
Translinear normalizer, 445 
Translinear, 51 
Translinear, 140, 207, 268, 386, 443 
Transmission noise, 212 
Trapping, 333 
Traveling wave, 107 
Traveling-wave, 100 
Tunneling process, 320 
Tunneling, 179, 317 
Unidirectionality, 4 
Uniform distribution, 184 
Unsupervised clustering, 385 
Unsupervised learning, 384 
Unsupervised, 381 
Variable-gain, 8 
Vector quantization, 437 
Vector quantizer, 384 
Velocity distribution, 135 
Virtual Wires, 344 
Visual processing, 151 
VLSI implementation, 34 
Volatile Memories, 387 
Volatile synapse, 422 
Voltage distribution, 369 
Wave propagation, 3 
Wavelet, 107 
Wavenumber, 4 
WEAK ARBITRATION, 195 
Went zel-Kr amers-Brillouin, 4 

462 
NEUROMORPHIC SYSTEMS ENGINEERING 
Wider linear range, 269 
Winner-take-all, 153 
Winner-Take-All, 197 
Winner-take-all, 264, 367, 375 
Winner-takes-all, 219, 443 
Zweig, 5 
overlapping cochlear cascades, 51 

