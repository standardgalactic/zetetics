Reclaiming AI as a theoretical tool for cognitive science
Iris van Rooij1,2,3, Olivia Guest1,2, Federico Adolfi4,5,
Ronald de Haan6, Antonina Kolokolova7, and Patricia Rich8
1Donders Institute for Brain, Cognition, and Behaviour, Radboud University, The Netherlands
2School of Artificial Intelligence, Radboud University, The Netherlands
3Department of Linguistics, Cognitive Science, and Semiotics & Interacting Minds Centre, Aarhus University, Denmark
4Ernst Strüngmann Institute for Neuroscience in Cooperation with Max-Planck Society, Germany
5School of Psychological Science, University of Bristol, UK
6Institute for Logic, Language and Computation (ILLC), University of Amsterdam, The Netherlands
7Department of Computer Science, Memorial University of Newfoundland, Canada
8Department of Philosophy, University of Bayreuth, Germany
The idea that human cognition is, or can be understood as, a form of computation is a useful
conceptual tool for cognitive science. It was a foundational assumption during the birth of
cognitive science as a multidisciplinary field, with Artificial Intelligence (AI) as one of its
contributing fields. One conception of AI in this context is as a provider of computational
tools (frameworks, concepts, formalisms, models, proofs, simulations, etc.) that support theory
building in cognitive science. The contemporary field of AI, however, has taken the theoretical
possibility of explaining human cognition as a form of computation to imply the practical fea-
sibility of realising human(-like or -level) cognition in factual computational systems; and, the
field frames this realisation as a short-term inevitability. Yet, as we formally prove herein, cre-
ating systems with human(-like or -level) cognition is intrinsically computationally intractable.
This means that any factual AI systems created in the short-run are at best decoys. When we
think these systems capture something deep about ourselves and our thinking, we induce dis-
torted and impoverished images of ourselves and our cognition. In other words, AI in current
practice is deteriorating our theoretical understanding of cognition rather than advancing and
enhancing it. The situation could be remediated by releasing the grip of the currently dominant
view on AI and by returning to the idea of AI as a theoretical tool for cognitive science. In
reclaiming this older idea of AI, however, it is important not to repeat conceptual mistakes of
the past (and present) that brought us to where we are today.
Keywords: artificial intelligence (AI), theory, explanation, engineering, cognitive science,
computational complexity
Introduction
The term ‘Artificial Intelligence’ (AI) means many things to
many people (see Table 1). Sometimes the term ‘AI’ is used
to refer to the idea that intelligence can be recreated in artifi-
cial systems (Russell & Norvig, 2010). Other times it refers
to an artificial system believed to implement some form of in-
telligence (i.e., ‘an AI’). Some claim that such an AI can only
implement domain-specific intelligence, while others believe
that domain-general or human-level AIs—also known as ar-
tificial general intelligence (AGI)—can exist (Bubeck et al.,
2023; cf. Birhane, 2021). The term ‘AI’ is also used to refer
to the research and/or engineering field pursuing the creation
of AI systems based on the idea that doing so is possible
and desirable. Among the more troublesome meanings, per-
haps, is ‘AI’ as the ideology that it is desirable to replace hu-
mans (or, specifically women) by artificial systems (Erscoi,
Kleinherenbrink, & Guest, 2023) and, generally, ‘AI’ as a
way to advance capitalist, kyriarchical1, authoritarian and/or
white supremacist goals (Birhane & Guest, 2021; Crawford,
2021; Erscoi et al., 2023; Kalluri, 2020; McQuillan, 2022;
Spanton & Guest, 2022; Stark & Hutson, 2022). Contem-
porary guises of ‘AI’ as idea, field, system, or ideology are
also sometimes known under the label ‘Machine Learning’
(ML), and a currently dominant view of AI advocates ma-
1Elisabeth Schüssler Fiorenza (1993) coined ‘kyriarchy’ as an
intersectional (Crenshaw, 1989; Osborne, 2015) generalisation of
the notion of ‘patriarchy’, i.e., a complex system of oppressions
that include (intersections of) racism, sexism, homophobia, trans-
phobia, ableism, etc. For an application of the concept in technol-
ogy, see ‘anti-oppressive design’,(Smyth & Dimond, 2014); and for
an application in ‘social justice in climate change adaptation’, see
Osborne (2015).

2
VAN ROOIJ ET AL.
Table 1
A non-comprehensive list of different (not mutually exclusive) meanings of the word AI, including AI as idea, AI as a type of
system, AI as a field of study, and AI as institution(al unit).
Type
Description
Label
Idea
Intelligence can be recreated in artificial systems.
AI-as-engineering
Cognition is, or can be understood as, a form of computation.
AI-as-psychology (a.k.a. computationalism)
Humans can be replaced by artificial systems.
AI-as-ideology
The label ‘AI’ helps to sell technologies and gain funding.
AI-as-marketing
System
A system believed to implement (simulate) a form of cognition.
cognitive system (model)
A system believed to perform (solve) domain-specific cognitive
tasks (problems).
narrow AI
A system believed to perform (solve) domain-general cognitive
tasks (problems; what some may also call AGI).
general AI or AGI
A system believed to realize human-level cognition (what some
may also call AGI).
human-level AI
Field
A (sub)field pursuing the creation of domain-specific AI sys-
tems.
e.g. Bayesian Networks, Decision Support
Systems, Machine Learning, Robotics
A (sub)field pursuing the creation of AGI.
AGI
A (sub)field using AI as an idea to build theories.
e.g., (computational) cognitive science, cog-
nitive simulation, weak AI
A field defined by a collection of fields that each are considered
to be an AI subfield.
the field of AI broadly construed
History
A history of practices reflecting different ideas of AI, resulting
in the pursuit of different kinds of AI systems, and different
kinds of AI-as-field concepts.
named to match practices, e.g., ML-AI, neu-
roAI, etc.
Unit
An organisational or institutional unit going under the label AI.
named to match type of units, e.g. AI re-
search group, AI department, AI centre, AI
network
chine learning methods not just as a practical method for
generating domain-specific artificial systems, but also as a
royal road to AGI (Bubeck et al., 2023; DeepMind, 2023;
OpenAI, 2023).
One meaning of ‘AI’ that seems often forgotten these days
is one that played a crucial role in the birth of cognitive sci-
ence2 as an interdiscipline in the 1970s and ’80s. Back then,
the term ‘AI’ was also used to refer to the aim of using com-
putational tools to develop theories of natural cognition. As
Simon (1983, p. 27) put it 40 years ago,
Artificial Intelligence has two goals. First, AI is
directed toward getting computers to be smart
and do smart things so that human beings don’t
have to do them. And second, AI (sometimes
called cognitive simulation, or information pro-
cessing psychology) is also directed at using
computers to simulate human beings, so that we
can find out how humans work.
This view of ‘AI’ as a research field overlapping with psy-
chology sees computational AI systems as theoretical tools:
“Many early AI researchers were concerned with using com-
puters to model the nature of people’s thinking” (Langley,
2006).3
Accordingly, AI is one of the cognitive sciences (Fig-
ure 1), and for decades there was a close dialogue between
the fields of AI and cognitive psychology (Forbus, 2010;
Gentner, 2010, 2019; Miller, 2003). This is furthermore il-
lustrated by the use of ‘cognitive simulation’ and ‘informa-
tion processing psychology’ as alternative labels for ‘AI,’
2Here, cognitive science is defined as the inter- or multidisci-
plinary study of cognition.
3See also Lighthill’s (1973) report on the state of AI research
and McCorduck’s (2019) history.

RECLAIMING AI
3
favoured by Simon and associates.4 It is also illustrated by
publications of cognitive psychological modelling research
in Artificial Intelligence journals up to the ’90s (Anderson,
1984; R. P. Cooper, Fox, Farringdon, & Shallice, 1996; Tha-
gard, Holyoak, Nelson, & Gochfeld, 1990) and early 2000s
(Thagard, 2007), with still some notable exceptions these
days. At the turn of the millennium, however, these produc-
tive ties between AI and psychology became severed:
the past 20 years have seen an increasing shift
in AI research away from concerns with mod-
elling human cognition and a decreasing famil-
iarity with results from psychology. What began
as a healthy balance [...] has gradually become a
one-sided community that believes AI and psy-
chology have little to offer each other. (Langley,
2006, p. 2)
AI qua information processing psychology was built on
the idea that human cognition is, or can be scientifically un-
derstood as, a form of computation; this view is also known
as (minimal) computationalism5 (cf. Chalmers, 2011; Diet-
rich, 1994; Miłkowski, 2013). Computationalism was seen to
provide useful conceptual tools for cognitive science (Boden,
1988, 2008; Johnson-Laird, 1988) as it affords explicit spec-
ification of hypothesized cognitive processes and reasoning
through the implications of such hypotheses (e.g., with math-
ematical proofs or computer simulations). However, present-
day AI hype and the popularity of AI as technology (Mered-
ith Whittaker, Edward Ongweso Jr., and Sarah Myers West in
Denvir, Yeager, & Johnson, 2023; Timnit Gebru in Marx &
Wickham, 2023; van Rooij, 2023) and AI as a money-maker
(Crawford, 2021) seems to leave little room for AI as a theo-
Historical note: The idea for this paper was conceived in June
2022 at the Lorentz Workshop “What makes a good theory? In-
terdisciplinary perspectives" (20–24 June 2022) when over dinner
OG presented a definition of AI from (Erscoi, Kleinherenbrink,
& Guest, 2023) to IvR (besides FA, also Laura van de Braak and
Marieke Woensdregt were there, and we thank them for contribut-
ing to this initial discussion). While the definition posed by (Er-
scoi, Kleinherenbrink, & Guest, 2023) was perfect for that paper’s
purposes, IvR noted it did not include the meaning of “AI” that is
reclaimed in this paper. Hence, the idea for this paper was born.
Countless discussions have followed between the co-authors to fine
tune the argumentation and this paper was written as a true team
science effort, bringing together all relevant expertise: AI, cognitive
science, philosophy, psychology, history, and computational com-
plexity. We thank the organizers for creating and fostering the cir-
cumstances to make this all possible. This paper was completed
more than a year after it was conceived. In that year a lot happened
on the world stage with respect to AI/ML that in our opinion makes
the need for this paper even greater than we had anticipated when
we started the project.
Philosophy
Psychology
Artificial
Intelligence
Neuroscience
Anthropology
Linguistics
Figure 1. A visual depiction of the connections between the
Cognitive Sciences. Solid lines denote stronger interdisci-
plinary ties; and dashed lines denote weaker ones. This figure
is derived from the original put forth by the Sloan Foundation
in 1978 and reproduced from Figure 4 in Pléh and Gurova
(2013). Different versions of it over time have used ‘Artifi-
cial intelligence’ (as above) instead of ‘Computer Science’
and vice versa (cf. Miller, 2003).
retical tool for cognitive science. Worse even, the products of
present-day AI-as-engineering are sometimes believed to in-
stantiate (parts of) minds. Besides the various psychological,
social, cultural and political problems posed by this confu-
sion (Bender, Gebru, McMillan-Major, & Shmitchell, 2021;
Birhane & van Dijk, 2020; Erscoi et al., 2023; J. Hughes,
2021; Thrall et al., 2018; Vallor, 2015; van der Gun & Guest,
2023; Wood, 1987), here we wish to focus on how this prac-
tice creates distorted and impoverished views of ourselves
and deteriorates our theoretical understanding of cognition,
rather than advancing and enhancing it.
In this paper, we wish to remedy the above situation in
two steps. First, we set out to release the grip of the cur-
rently dominant view on AI (viz., AI-as-engineering aiming
at a human-level AI system, Table 1). This practice has taken
the theoretical possibility of explaining human cognition as
a form of computation to imply the practical feasibility of
realising human(-like or -level) cognition in factual compu-
4See also ‘computational psychology’ as used by Boden (1988)
and ‘theoretical psychology’ as used by Longuet-Higgins (Hüne-
feldt & Brunetti, 2004).
5Importantly, computationalism is to be conceptually distin-
guished from computerism, cognitivism or any commitments to
specific computational architectures (see e.g. Chalmers, 2011; Di-
etrich, 1994).

4
VAN ROOIJ ET AL.
tational systems; and, it is framing this realisation as a short-
term inevitability. In this paper, we undercut these views and
claims by presenting a mathematical proof of inherent in-
tractability (formally, NP-hardness) of the task that these AI
engineers set themselves. This intractability implies that any
factual AI system created in the short-run (say, within the
next few decades or so) is so astronomically unlikely to be
anything like a human mind, or even a coherent capacity
that is part of that mind, that claims of ‘inevitability’ of AGI
within the foreseeable future are revealed to be false and mis-
leading.
Second, we propose a way to return to the idea of AI as
a theoretical tool without falling in the trap of confusing our
maps for the territory. The return must be such that we do
not retrace the trajectory that led us where we are today. To
halt the rerun of history, we think it is vital that the idea
of cognition as computation—and therefore the in-principle
possibility of realising and/or explaining cognition as a form
of computation—is not mistaken for the practical feasibility
of replicating human minds in machines (which we prove
is not feasible). The reader may think that this is a contra-
diction, i.e., that then computationalism is theoretically in-
ert. But that is mistaken. Computationalism can provide both
explanatory challenges for and computational constraints on
cognitive explanations—using formal, conceptual, and math-
ematical analysis—and hence is theoretically informative.
Overview
Flying pigs are also possible in principle; pos-
sible in principle bakes no bread.
— Jerry Fodor (2005, p. 27)
The remainder of this paper will be an argument in “two
acts”. In ACT 1: Releasing the grip we present a formalisa-
tion of the currently dominant approach to AI-as-engineering
that claims that AGI is both inevitable and around the corner.
We do this by introducing a thought experiment in which a
fictive AI engineer, Dr. Ingenia, tries to construct an AGI un-
der ideal conditions.6 For instance, Dr. Ingenia has perfect
data, sampled from the true distribution, and they also have
access to any conceivable ML method—including presently
popular ‘deep learning’ based on artificial neural networks
(ANNs) and any possible future methods—to train an algo-
rithm (“an AI”). We then present a formal proof that the prob-
lem that Dr. Ingenia sets out to solve is intractable (formally,
NP-hard; i.e., possible in principle but provably infeasible;
see the section Ingenia Theorem). We also unpack how and
why our proof shows that the AI-as-engineering approach is
a theoretical dead-end for cognitive science. In ACT 2: Re-
claiming the AI vertex, we explain how the original enthu-
siasm for using computers to understand the mind reflected
many genuine benefits of AI for cognitive science, but also
a fatal mistake. We conclude with ways in which “AI” can
be reclaimed for theory-building in cognitive science without
falling into historical and present-day traps.
ACT 1: Releasing the grip
There is no doubt that [AI] is currently in the
process of rapid hill-climbing. Every year, states
of the art across many [AI] tasks are being im-
proved[, but] the question is whether the hill we
are climbing so rapidly is the right hill.
— Emily M. Bender and Alexander Koller
(2020, p. 5191)
At present, the field of AI is in the grip of a dominant
paradigm that pushes a narrative that AI technology is so
massively successful that, if we keep progressing at our cur-
rent pace, then AGI will inevitably arrive in the near future.
Some multi-million AI companies have even gone so far as
to claim that we are not only climbing a hill towards AGI,
but even on the verge of creating AGI, whether we want to
or not, if we proceed with current ML approaches to AI-as-
engineering. These kinds of claims may be hard to ignore or
counter if anything that is possible in principle also seems
possible in practice. However, not everything that is possible
in principle is possible in practice.
In this first Act, we reveal why claims of the inevitability
of AGI walk on the quicksand of computational intractability.
The main character on the stage is Dr. Ingenia, a fictive AI
engineer, who is pursuing the kind of ML approach claimed
to inevitably lead to AGI. By studying the engineering task
that they have set themselves through a formal, mathematical
lens we are able to construct a proof of intractability. We then
draw out its implications.
Formalising AI-by-Engineering
Give a rigorous, computationally detailed and
plausible account of how learning can be done.
Translation: Rigorous: theorems, please.
— Dana Angluin (1992, p. 351)
Imagine a fictive engineer, called Dr. Ingenia, who is try-
ing to build human-like or -level AI (or AGI)7 using the cur-
rently dominant approach of ‘machine learning’ (in short,
‘ML’) under highly idealised and optimal conditions (Fig-
ure 2). Dr. Ingenia is able to repeatedly sample from a dis-
tribution D. Here, D captures possible behaviours b that hu-
mans may display in different possible situations s. Dr. Inge-
nia can use the information gathered by repeatedly sampling
situation-behaviour pairs from D to try to build (or ‘train’)
6This analytical strategy takes inspiration from Rich et al.
(2021).
7Here and throughout, we use ‘AGI’ in the sense of (domain-
general) human-like or -level AI (cf. Table 1).

RECLAIMING AI
5
Figure 2. A visual illustration of the hypothetical scenario
and its formalisation: Dr. Ingenia has access (magically and
at no cost) to any machine learning method M, present or
future, and by repeatedly sampling data D from the distri-
bution D they can use whatever M they like to create a
program LA that when implemented and run generates be-
haviours b = A(s) when prompted by different situations s.
The goal is to generate with non-negligible probability δ(n)
an algorithm A that behaves (approximately) human-like, in
the sense that A is non-negligibly (ϵ(n)) better than chance
at picking behaviours that are possible for s in D. Here, n
is a measure of the situation complexity, i.e., the maximum
length of strings (|s|) needed to encode the relevant informa-
tion in the situations s.
an algorithm. Their goal is to make an algorithm that behaves
(approximately) like D. Informally, we can characterise the
problem that Dr. Ingenia sets themselves, as follows:
AI-by-Learning (informal)
Given: A way of sampling from a distribution D.
Task: Find an algorithm A (i.e., ‘an AI’) that,
when run for different possible situations as in-
put, outputs behaviours that are human-like (i.e.,
approximately like D for some meaning of ‘ap-
proximate’).
In our hypothetical scenario, we will be granting computa-
tionalism; i.e., we will not be challenging the assumption that
cognition is a form of computation. Hence, Dr. Ingenia can
safely assume that D is generated by a computational pro-
cess, i.e., either by the computational processes that define
a single individual’s cognition, or the cognition of a finite
collection of human beings. This also means that there exists
an algorithm A that can approximate the distribution, namely,
the algorithm that generates D. But there may also be many
more algorithms that deviate in some way from human cog-
nition but whose behaviour is still sufficiently human-like.
Because our aim will be to assess the intrinsic hardness
of Dr. Ingenia’s machine learning problem—independent of
extraneous factors—we will be granting them several ide-
alised conditions. For instance, in our hypothetical scenario,
Dr. Ingenia will have access to perfect data for training their
AI system. The data have no measurement error, nor are
the data contaminated by irrelevant details or sampling bias
or any other distortion on the data that make it indirect or
imperfect. Note that the real-world situation is far removed
from this idealized scenario. In contemporary ML, the pre-
dominant protocol is to train ML algorithms on data which
is typically decontextualized and scraped from the internet
(Birhane, Prabhu, Han, & Boddeti, 2023; Birhane, Prabhu, &
Kahembwe, 2021; Lee, Le, Chen, & Lee, 2023; Liesenfeld,
Lopez, & Dingemanse, 2023). Moreover, real AI engineers
do not have perfect knowledge of which cognizer contributed
which data point, of which exact situation induced which be-
haviour, or of how behaviour is to be parsed (cf. ‘the segmen-
tation problem’; Adolfi, Wareham, & van Rooij, 2022, 2023)
or interpreted (cf. ‘theory-ladeness of data’; Andrews, 2023;
Guest & Martin, 2021; R. I. Hughes, 1997).
By granting Dr. Ingenia these highly idealised conditions
that simplify and abstract away from real-world complica-
tions, we can formally derive a reliable lower-bound on the
real-world complexity of constructing human-like AI from
human data. To do so, we need to make the problem AI-by-
Learning formally precise such that it becomes amenable to
computational complexity analysis (Arora & Barak, 2009;
Garey & Johnson, 1979; van Rooij, Blokpoel, Kwisthout, &
Wareham, 2019). We will do this next.
We assume that Dr. Ingenia expresses candidate algo-
rithms A using a specification language, LA. Any particular
algorithm A ∈A can be described with a program LA ∈LA.
The specification language LA can be thought of as a pro-
gramming language with the constraint that it specifies only
those algorithms in a class A that the engineer assumes is
suitable for designing human(-like or -level) AI. For instance,
A could be the class of artificial neural networks (ANNs) or
any other class of algorithms that Dr. Ingenia deems suffi-
cient to approximate human cognition.
We will add some minimal constraints on LA. We exclude
classes of trivial algorithms that have no chance of captur-
ing human-like or -level cognition. Specifically, we impose
the constraint that LA can minimally express feedforward
neural networks, logical circuits, or finite state machine-
equivalent class of algorithms. Scientifically, we think that
a stronger assumption would be warranted, namely that LA
is in principle expressive enough to be Turing complete; e.g.,
LA can express Turing machines (Turing, 1950) or other-
wise Turing-equivalent algorithms, including certain (highly
idealised) recurrent neural networks (Siegelmann & Sontag,
1992; cf. Pérez, Marinkovi´c, & Barceló, 2019; Weiss, Gold-
berg, & Yahav, 2018). This stronger assumption would en-

6
VAN ROOIJ ET AL.
sure that LA is expressive enough to computationally cap-
ture human cognition. It would be a reasonable assumption
because cognition is generally assumed to have two prop-
erties, known as productivity (i.e., people can in principle
generate infinitely many distinct thoughts, sentences, images,
etc., Fodor, 2005; Fodor & Pylyshyn, 1988) and Turing-
completeness or -equivalence (the ability to compute any
computable function, aided by pen and paper, in principle;
Turing, 1950; Wells, 1998). We nonetheless work with the
more modest assumption.
We will furthermore allow (but not impose) the as-
sumption that all A expressible by LA are computationally
tractable (i.e., can be run on any situation s in polynomial
time, O(nc), where n is some measure of the input size (|s|)
and c is a constant). This constraint ensures that any in-
tractability results we may derive for Dr. Ingenia’s AI-by-
Learning problem are not an artefact of the time-complexity
of running the algorithm A itself. Moreover, it ensures that
even if human cognitive computations are all tractable (cf.
‘the tractable cognition thesis’,van Rooij, 2008; see also
Frixione, 2001), our intractability results for AI-by-Learning
would still hold.
We assume in our formalisation that whenever Dr. Inge-
nia tries to solve an instance of AI-by-Learning and searches
for a program LA that encodes a human (-like or -level) A,
there is a given upper bound K on the size of the program
that they can in principle encode (i.e., |LA| ≤K). One can
think of K as expressing the total amount of space (computer
memory) that Dr. Ingenia has available to store a program.
AI models these days can be very large, and we allow for
their size (e.g., an ANN may have tens of millions of param-
eters; Krizhevsky, Sutskever, & Hinton, 2012). Nonetheless,
they are still bounded by some size K. Dr. Ingenia can buy
more space to work with, in which case they will have a new
K′ > K to work with. AI engineers that claim to be able
to create AGI with ML are (implicitly) assuming that their
approach can work for larger and larger K′.
We formalise the distribution D as follows. A dataset
D drawn from D consists of a list of situation-behaviour
pairs, a.k.a. “samples”: (s1, b1), (s2, b2), (s3, b3), ..., (s|D|, b|D|).
Without loss of generality, we model this structure with
binary strings (e.g., s
=
101010101000001 and b
=
010101111). For any given input distribution Dn, there is
an upper bound n on the description length of situations.
In other words, the set of situations for such an instance
of the problem is defined as S = {0, 1}n. For each situa-
tion s ∈S , there are some appropriate (i.e., human-like) be-
haviours Bs ⊊B = {0, 1}m, for some fixed m.
Lastly, we formalise the notion of “approximate” in Dr.
Ingenia’s AI-by-Learning problem. Recall that we are trying
to estimate a lower-bound on the real-world problem of cre-
ating human-like or -level AI by ML, and therefore give Dr.
Ingenia ‘easier’ conditions than may apply in the real world.
To this end, we will be setting an extremely low bar for what
counts as “approximate”. On the one hand, we do not expect
a guarantee that Dr. Ingenia succeeds, but merely that Dr.
Ingenia succeeds with non-negligible probability (denoted
δ(n), where n is a measure of the size/complexity of situa-
tions; see previous paragraph). Moreover, the performance of
the found A need not have high degrees of human-likeness,
but merely should perform human-like with a probability that
is non-negligibly higher than chance level. Specifically, in
our formalisation of AI-by-Learning, we will make the sim-
plifying assumption that there is a finite set of possible be-
haviours8 and that for each situation s there is a fixed num-
ber of behaviours Bs that humans may display in situation s.
Then |Bs|/|B| expresses chance level, and |Bs|/|B|+ϵ(n) expresses
‘non-negligibly better than chance’.
Given the above considerations, we can now state a for-
malised version of AI-by-Learning:
AI-by-Learning (formal)
Given: An integer K and a way of sampling from
a distribution D.
Task: Find a description LA ∈LA, with length
|LA| ≤K, of an algorithm A ∈A that with prob-
ability ≥δ(n), taken over the randomness in the
sampling, satisfies:
Pr
s∼Dn [A(s) ∈Bs] ≥|Bs|/|B| + ϵ(n).
Here δ(n) and ϵ(n) are arbitrary non-negligible
functions. A function f is non-negligible if there
is some d such that for sufficiently large n, f(n) ≥
1/nd.
Ingenia Theorem
[E]ven with a whole row of the largest imagin-
able computers to help, all the potential distri-
butional potentialities of a whole national lan-
guage cannot possibly be found in any finite
time[.]
— Margaret Masterman (1965, p. iv-19)
In this section we present a formal proof that AI-
by-Learning is intractable. For this we use the following
decision problem called Perfect-vs-Chance.9 While Perfect-
vs-Chance is a somewhat unnatural problem, with no direct
8Given the assumption of productivity of mind (Fodor, 2005;
Fodor & Pylyshyn, 1988), this is a gross underestimation of human
potential.
9This problem was introduced and proven intractable by Shuichi
Hirahara (2022), but they did not give it a name. We have chosen a
name that suffices for our purposes and so as to optimise intuitive-
ness.

RECLAIMING AI
7
real-world analogue, it is useful for our purposes. Bear with
us. It will all make sense in a moment.
Perfect-vs-Chance (decision problem)
Given: A way to sample a given distribution D
over {0, 1}n ×{0, 1}, an integer k, and the promise
that one of the following two cases apply:
1. There is an efficient program M of size at
most k such that Pr(x,y)∼D[M(x) = y] = 1
2. For any program M of size at most k,
Pr(x,y)∼D[M(x) = y] ≤1/2 + 1/2n1−δ
where 0 < δ < 1 is an arbitrary constant.
Question: Is (1) or (2) the case?
In the decision problem Perfect-vs-Chance, the expres-
sion 1/2n1−δ can informally be read as ‘negligible probabil-
ity’, since the denominator grows very fast. Hence, infor-
mally the two cases listed in the Perfect-vs-Chance problem
are (1) there exists a perfect and efficient program or, other-
wise, (2) there exists no program that can work better than
chance, even inefficiently.
Note that in the Perfect-vs-Chance problem it is
promised, before trying to solve the problem, that for the
given distribution either case (1) or (2) holds. One may intuit
that, since (1) and (2) are extreme cases that are very clearly
distinct, telling these two cases apart should be easy to do.
However, it is not easy. In fact, it is provably intractable to
find out in which of the two cases one finds oneself. This
follows from a proof by (Hirahara, 2022).
Theorem 1 (Hirahara, 2022). Perfect-vs-Chance is in-
tractable.10,11
We will use Theorem 1 to prove that AI-by-Learning is
intractable, too. Specifically, we will show that if it were
possible to tractably solve AI-by-Learning, then it would
also be possible to tractably solve Perfect-vs-Chance. Since
Perfect-vs-Chance is known to be intractable (Theorem 1),
by modus tollens,12 it follows that AI-by-Learning must be
intractable as well. In other words, the proof will be by con-
tradiction.
Theorem 2 (Ingenia Theorem). AI-by-Learning is in-
tractable.
Proof (sketch). For full details of the proof we refer the
reader to the Appendix. In this proof sketch we present the
main proof idea. See Figure 3 for an illustration.
We prove by contradiction. Suppose that there exists
a learning mechanism M that solves AI-by-Learning in
polynomial time. We will show that then there exists a
polynomial-time bounded-error probabilistic algorithm that
solves Perfect-vs-Chance.
Figure 3. A visual illustration of the core proof idea for
Theorem 2 (Ingenia Theorem): It is known that Perfect-
vs-Chance is an intractable decision problem. If a tractable
method M for solving AI-by-learning would exist, then we
could use M to solve Perfect-vs-Chance tractably, by plug-
ging it into a sampling-plus-decision procedure that is itself
tractable, easy to construct, and together with M would prov-
ably solve Perfect-vs-Chance. This yields a contradiction.
Therefore we can conclude that AI-by-learning is intractable
as well. See the main text for more information and the Ap-
pendix for full proof details.
The algorithm for Perfect-vs-Chance works as follows.
Take an arbitrary instance for Perfect-vs-Chance, consisting
of integers n and k and a distribution D over {0, 1}n × {0, 1}.
The algorithm will use a subroutine that simulates the learn-
ing mechanism M, where K = k and where the data that the
mechanism M has sampling access to is given by the distribu-
tion D. In this simulation, the set of situations is S = {0, 1}n
and the set of behaviours is B = {0, 1}. The simulation will
yield an algorithm A, that might or might not perform well
on freshly sampled situations.
After running the subroutine that simulates M, the algo-
rithm will evaluate the quality of the resulting learned algo-
10Formally, NP-hard under randomised polynomial-time one-
query reductions. See (Hirahara, 2022) for more details. The re-
duction proves that the problem is not tractable (i.e., computable in
polynomial time) unless NP ⊆BPP. It is widely conjectured that
NP ⊈BPP (see, e.g., Arora & Barak, 2009, Chapter 7). See the
Appendix for more details.
11There are no other constraints on M than specified in the def-
inition of the problem Perfect-vs-Chance. M can be thought of
as a Turing Machine. This is without loss of generality because,
assuming the Invariance Thesis (van Emde Boas, 1990), a Turing
machine can simulate any other computational machine with only
polynomial-time overhead.
12If P then Q. And ¬Q. Therefore, ¬P. Or, more formally,
(P →Q, ¬Q) →¬P.

8
VAN ROOIJ ET AL.
rithm A by using additional samples from D and counting
the number of situations s in which the algorithm A returns
an appropriate behaviour b ∈Bs.
The algorithm runs the simulation subroutine several
times, and for each run of the subroutine, it evaluates the re-
sulting learned algorithm A. From all of these runs, it picks
the algorithm A⋆that performed best in the quality eval-
uation. Based on how well the best learned algorithm A⋆
performs, the algorithm will give an answer for the input
of Perfect-vs-Chance. If A⋆performs non-negligibly better
than chance, then the algorithm will answer Yes, and other-
wise, the algorithm will answer No.
By having the algorithm run the simulation a large enough
(yet polynomial) number of times, and testing the output of
each simulation a large enough (yet polynomial) number of
times with new samples, we can ensure that the algorithm
outputs a correct answer for Perfect-vs-Chance with high
probability.
■
Implications
It is desirable to guard against the possibility
of exaggerated ideas that might arise as to the
powers of [AI]. In considering any new subject,
there is frequently a tendency [...] to overrate
what we find to be already interesting or remark-
able[.]
— Augusta Ada King, Countess of Lovelace
(personal correspondence, July, 1843; Toole et
al., 1998, p. 186)
In the previous section we presented a proof that Dr. In-
genia set themselves a machine learning problem for which
no tractable method exists or can exist. The ‘intractability’
means that, even if the problem may be practically solv-
able for trivially simple situations (small n), any attempts to
scale up to situations of real-world, human-level complexity
(medium to large n) will necessarily consume an astronom-
ical amount of resources (such as time and number of sam-
ples; see Box 1 for an illustration).
The proven intractability holds for the highly simplified
and idealised model of AI-by-Learning that grants Dr. Inge-
nia much better conditions than apply for AI engineers in the
real world. As explained in the section Formalising AI-by-
Engineering, the intractability result holds even if Dr. Ingenia
(a) can sample randomly and unbiasedly, if they so wish; (b)
has data which are noiseless, without error, and uncontami-
nated; (c) is free to use any means or methods for producing
the AI (this can include present and future machine learning
approaches, but is not limited to them); (d) is only required to
produce with a probability slightly higher than chance an AI
that matches human behaviour slightly better than chance;
(e) is guaranteed that there exists an algorithm that meets
those low-bar requirements. Idealizations (a)–(e) make clear
that the computational complexity of Dr. Ingenia’s problem
is a gross underestimation of the true complexity of the much
more messy real-world AI-by-Learning problem.
Given the proof nature of the complexity-theoretic result,
the claim that we are presently on a fast track to inevitably
produce human-like and -level AI poses a logical contradic-
tion. Let us unpack why this is so: While there are many
claims that might reasonably be based on intuitions, any
claim of the inevitability of producing any desired object (or
event, or state of affairs) requires a tractable procedure as a
precondition. To see this, note that, without loss of general-
ity, we can cast the problem of building human-like AI as
that of searching for such an object in some space of pos-
sibilities. To claim that the search for this object is bound
to succeed in practice is to minimally claim that one has a
tractable procedure for conducting the search that provably
finds the object if it exists (i.e., one has a way of performing
the search in a realistic amount of time). That is, to support
the inevitability claim one would have to put forth a set of
arguments, logically and mathematically sound, to prove not
only that such a tractable procedure can exist, but also that
one has it. Theorem 2 (the Ingenia Theorem) shows that this
is impossible.
Given the Ingenia Theorem, how should we interpret what
is happening in practice? In practice, AIs are being continu-
ously produced which are claimed to be either human-like
and human-level AI or inevitably on a path leading there.
Any AIs produced in practice, however, are produced ei-
ther by tractable procedures, or by cutting short a procedure
that would run longer (for an unfeasible amount of time).
Hence, the produced AIs necessarily fail to solve the in-
tractable learning problem. Concretely, this means that they
make lots of errors—deviating substantially from human be-
haviour (e.g., Bowers et al., 2022)—and fail to meet the
low standard set in the Ingenia Theorem (see also the list of
simplifications and idealisations (a)–(e) above). These errors
cannot be contained to be small, and no matter how impres-
sive the produced AIs may appear, they fail to capture the
distribution of human behaviour even approximately.
We realise that the implications that we have drawn out
from our complexity-theoretic results may appear to contra-
dict both intuition and experiences with existing AIs. How-
ever, the pattern of observations is entirely consistent with
and predictable from the Ingenia Theorem. Many AIs do
seem to have truly impressive human likeness and may even
sometimes fool one into thinking that they have agency or
are sentient.13 Moreover, the field of AI-as-engineering has a
13Cf. ‘the Eliza effect’ (Weizenbaum, 1966; but see also Dil-
lon, 2020). For instance, last year, Google fired Blake Lemoine, an
engineer who believed that LaMDa (a large language model that
he worked on) had become sentient (Fluckinger, 2022). It is wry
that Lemoine was fired for expressing this (false) belief, while the

RECLAIMING AI
9
Box 1 — Implications of intractability
Because AI-by-Learning is intractable (formally, NP-
hard under randomized reductions), the sample-and-time
requirements grow non-polynomially (e.g. exponentially
or worse) in n. To illustrate just how quickly this would
exhaust all the resources available in the universe, even
for moderate input size n, let us do a simple thought ex-
periment: Imagine we are looking for an AI that can re-
spond appropriately to different situations corresponding
to conversations of, say, 15 minutes. Since people speak
around 160 words per minute on average (Yuan, Liber-
man, & Cieri, 2006, see also Dingemanse & Liesenfeld,
2022; Liesenfeld & Dingemanse, 2022), let us take 60
words per minute as a generous lower bound. Then a con-
versation would have on average 900 words. For humans,
the appropriate response may depend on the full context
of the conversation, and we have no problem conditioning
our behaviour in this way. To encode such sequences of
spoken words in some binary encoding, we would need
more bits than words; i.e. n > 900. The assumption of us-
ing 1 bit per word is an underestimation, assuming that at
each point, the conversation can continue grammatically
correctly in at least two directions (cf. Parberry, 1997).
Now the AI needs to learn to respond appropriately to
conversations of this size (and not just to short prompts).
Since resource requirements for AI-by-Learning grow ex-
ponentially or worse, let us take a simple exponential
function O(2n) as our proxy of the order of magnitude
of resources needed as a function of n. 2900 ∼10270 is
already unimaginably larger than the number of atoms in
the universe (∼1081). Imagine us sampling this super-
astronomical space of possible situations using so-called
‘Big Data’. Even if we grant that billions of trillions (1021)
of relevant data samples could be generated (or scraped)
and stored, then this is still but a miniscule proportion of
the order of magnitude of samples needed to solve the
learning problem for even moderate size n. It is thus no
surprise that AI companies that are trying to construct
AIs using machine learning are running out of useable
data (Shumailov et al., 2023; P. Villalobos et al., 2022)
and that actual datasets are not being scaled up to more
and more complex and diverse real-world situations and
behaviours, but they are becoming more homogeneous
(with even harmful consequences; Birhane, Prabhu, Han,
& Boddeti, 2023). That nevertheless ‘large data sets’ (in-
correctly) appeared to be sufficient for solving a problem
like AI-by-Learning, can be explained by the fact that
people generally have poor intuitions about large numbers
(Landy, Silbert, & Goldin, 2013) and underestimate how
fast exponential functions grow (van Rooij, 2018; Wage-
naar & Sagaria, 1975; Wagenaar & Timmers, 1978, 1979).
Hence, contrary to intuition, one cannot extrapolate from
the perceived current rate of progress to the conclusion
that AGI is soon to be attained.
habit of interpreting (or selling) “better than the state of the
art” as “good accuracy”, but our results imply that no mat-
ter how much “better” AI gets, it will be off by light-years,
wrong in exponentially many situations. This kind of self-
fooling is possible in part because:
The prioritization of performance values is so
entrenched in the field that generic success
terms, such as ‘success’, ‘progress’, or ‘im-
provement’ are used as synonyms for perfor-
mance and accuracy [...] However, models are
not simply ‘well-performing’ or ‘accurate’ in
the abstract but always in relation to and as
quantified by some metric on some dataset”
(Birhane et al., 2022).
However, the Ingenia Theorem implies that if one were to test
these AIs rigorously and unbiasedly for human-likeness, it
would quickly become evident that they behave qualitatively
differently from humans. That is, if you think your AI is very
human-like, then you are not testing it critically enough (cf.
Bowers et al., 2023).
Unsurprisingly given our theoretical results, we see ex-
actly this play out in practice: AIs appear human-like in non-
rigorous tests, but the likeness is debunked when more rig-
orous tests are made (e.g. Adolfi, Bowers, & Poeppel, 2023;
Dentella, Murphy, Marcus, & Leivada, 2023). For instance,
claims of abilities emerging with the scaling up of models
are often revealed to be trivial products of the researcher’s
choice of metric (Schaeffer, Miranda, & Koyejo, 2023). This
back and forth between claims of human-likeness and de-
bunking (cf. Mitchell, 2021) will keep happening if the field
does not realise that AI-by-learning is intractable, and hence
any model produced in the short run is but a “decoy”.
This is especially troubling since more and more people
are taking AI systems to be candidate models of human cog-
nition (Frank, 2023a, 2023b; Hardy, Sucholutsky, Thomp-
son, & Griffiths, 2023; Mahowald et al., 2023; Tuckute et al.,
2023), or even as replacements for humans. For instance, as
replacement for participants in psychological experiments,
Google CEO himself, Sundar Pichai, expressed no less unrealis-
tic statements in a public interview (Pelley, 2023), suggesting that
these systems have remarkable “emergent” cognitive abilities, e.g.
to know languages not trained on, being creative, able to reason and
to plan (see Bender, 2023, for a critical analysis).

10
VAN ROOIJ ET AL.
(Dillion, Tandon, Gu, & Gray, 2023); but see also: Crock-
ett & Messeri, 2023; Harding, D’Alessandro, Laskowski, &
Long, 2023; or as replacement for workers (Eloundou, Man-
ning, Mishkin, & Rock, 2023; Rose, 2023; Semuels, 2020);
while by now it is clear that this is only possible at the cost
of an exponential increase of hidden, poorly paid and poorly
treated workers; (McCarty Carino & Shin, 2023; Roberts,
Wood, & Eadon, 2023)). Such replacements are a clear case
of “map territory confusion”, and with a poor map at that.
This may seem to make sense if one believes that the AIs ap-
proximate human behaviour (though even then it is not a suf-
ficient condition, Guest & Martin, 2023), but as we explained
above the AIs do not actually approximate human behaviour.
By nevertheless taking the AIs as cognitive models, we—
as a field—distort our view of cognition, and it makes our
cognitive science theoretically weaker.
This argument applies not only to AIs mistaken for mod-
els of (all of human) cognition, but for models of substantive
cognitive capacities, like language, problem solving, reason-
ing, analogizing, or perception (Cummins, 2000; van Rooij
& Baggio, 2021). This can be argued by contradiction. As-
sume it were possible to tractably make approximate mod-
els of such core capacities, or even of restricted capacities,
such that one could make piecemeal models of human cog-
nition. Then one would not be able to put them back together
tractably in order to account for all of human cognition, be-
cause if one were able to, then one would have a tractable
procedure for modelling all of cognition, which is an in-
tractable problem (see also Rich, de Haan, Wareham, & van
Rooij, 2021).
ACT 2: Reclaiming the AI vertex
Computers as such are in principle less crucial
for cognitive science than computational con-
cepts are.
— Margaret A. Boden (2008, p. 14)
Based on our analysis, we reject the view and associated
project that we term ‘makeism’. See Box 2 for a definition; in
other words, all makeists think that building cognition is suf-
ficient for being able to explain it (b), and some think that this
building is also necessary (c). The necessity claim especially
reveals the implicit assumption that it is possible to build cog-
nition (a). While some things can be understood by making
them, it won’t work for human-like or -level cognition, for
one because this cannot plausibly be (re)made through engi-
neering (i.e. (a) is false; see the Ingenia Theorem).
At this point the reader may wonder: if we indeed can-
not (re)make cognition—or coherent parts of cognition—
computationally, then is AI theoretically useless for cogni-
tive science? No: computationalism can be theoretically pro-
ductive even if makeing is futile. In this section we explain
Box 2 — What is makeism?
Makeism: The view that computationalism implies that
(a) it is possible to (re)make cognition computation-
ally; (b) if we (re)make cognition then we can explain
and/or understand it; and possibly (c) explaining and/or
understanding cognition requires (re)making cognition
itself.
The methodology endorsed by makeists has been re-
ferred to as the synthetic methodology or understand-
ing by design and building (Bisig & Pfeifer, 2008;
Pfeifer & Scheier, 2001). A well-known quote from
Feynman (1988), “what I cannot create, I do not un-
derstand”, is often used to support the idea of makeism
in AI (e.g. Karpathy et al., 2016).
Note that it is especially easy for makeists to fall
into map-territory confusion—mistaking their model-
ing artefacts for cognition itself—due to the view that
the made thing could be cognition.
how the notion of computation can help to challenge and con-
strain, and thereby inform, theories of cognition in a way that
steers clear of makeism. This will also allow us to reclaim
‘AI’ as one of the cognitive sciences, i.e., one of the vertices
in the hexagon (Figure 1). In our opinion, this is vital for
retaining (or restoring) cognitive science’s theoretical health
and preventing (further) distortions of our understanding of
human cognition.
What not to reclaim
As we explained in the Introduction, AI was initially con-
ceived as a theoretical tool for cognitive science, and an ac-
tive segment of work in cognitive science was understood as
being part of AI. AI as a field originally included the use of
computational models to study the human mind (variously
referred to as cognitive simulation (Lehnert, 1977), informa-
tion processing psychology (Newell, 1970; Simon, 1983),
computational psychology (Boden, 1988, 2008), or theo-
retical psychology (Hünefeldt & Brunetti, 2004; Longuet-
Higgins, 1981; Newell, 1970).
At present, this perspective on AI is largely forgotten. By
looking at the history of those conceptions of AI and why
they fell out of favour, we can see both the attraction of AI as
cognitive science and the problems with the original vision,
which we do not want to reclaim.
First, let’s consider makeism part (a) (see Box 2). As
noted, the makeist project only makes sense if part (a) is
really true, i.e. if cognition can be programmed into a com-
puter. This seems to have been taken for granted by many,
for example by Simon, who wrote:
It is not my aim to surprise or shock you. [...]

RECLAIMING AI
11
But the simplest way I can summarize is to say
that there are now in the world machines that
think, that learn and create. Moreover, their abil-
ity to do these things is going to increase rapidly
until–in a visible future–the range of problems
they can handle will be coextensive with the
range to which the human mind has been ap-
plied.
Herbert Simon on The General Problem Solver
in 1957, as quoted in Norvig (1992, p. 109)
Here we see the forerunner of the current idea that compu-
tationalism implies the practical realizability of thinking ma-
chines/simulations on par with human-level or -like cogni-
tion. This idea is expressed clearly in Feigenbaum and Feld-
man’s (1963) early characterization of AI (as quoted in Mein-
hart, 1966, emphasis added):
Researchers in the (artificial intelligence) field
hold to the working hypothesis that human
thinking is wholly information-processing activ-
ity within the human nervous system; that ulti-
mately these information processes are perfectly
explicable; that the road to this explication lies
in observation, experimentation, analysis, mod-
elling, model validation, et cetera; and that dig-
ital computers, being general information pro-
cessing devices, can be programmed to carry out
any and all of the information processes thus ex-
plicated.
The last item on the list makes explicit the assumption that
computationalism implies the practical realizability of think-
ing machines/simulations on par with human-level or -like
cognition. And since it suddenly seemed possible to re-create
aspects of cognition using computers, many early cognitive
scientists enthusiastically began trying to do so. For example,
consider representative comments from the late 70s, from a
book by Wendy Grace Lehnert.
If experiments cannot be designed to isolate the
variable factors of a proposed theory, the [exper-
imental] psychologist can go no further. Prob-
lems concerning human cognitive processes are
difficult to study within the paradigm of experi-
mental psychology for precisely this reason. [...]
What experiment can be designed to help us un-
derstand how people are able to answer simple
questions like “What’s your name?” [...] Natural
language processing can be productively stud-
ied within the artificial intelligence paradigm. If
we construct a process model designed to ac-
count for a particular language task [...], then
we can write a computer program to implement
that model. By running that program, we can see
where the model is weak, where it breaks down,
and where it appears competent. [...] The inter-
esting failures are those that occur because the
process model underlying the program failed to
recognize some critical problem or failed to han-
dle some problem adequately (Lehnert, 1977,
pps. 40-41).
If our reading of Lehnert is correct, this expresses, or at least
encourages, makeism. Specifically, it reflects parts (a) and
(b), with the idea that interesting parts of cognition can be
simulated in computers, and that we will gain understanding
in this way. Furthermore, Lehnert leaves the door open to
makeism part (c); it is unclear whether she herself thinks that
simulating natural language is necessary for explaining it, but
one could draw that conclusion.
The problem, of course, is that (a) is false, and without it,
makeism (b) and (c) no longer reflect a promising strategy
for cognitive science, but rather a research program doomed
to fail. Indeed, in light of our demonstration that the task of
creating cognition in computers is unfeasible, it is not sur-
prising that among early researchers pursuing this project,
enthusiasm waned and many people moved on.14
As AI technology has exploded, makeism is enjoying a
renaissance. However, for cognitive science, the engineer-
ing approach worsens theoretical understanding because any
artefacts we could make in the short run would be gross dis-
tortions or “decoys” at best. As Neisser wrote, “[t]he view
that machines will think as man [sic] does reveals misunder-
standing of the nature of human thought” (Neisser, 1963, p.
193). Sixty years later, the risk of being misled by decoys
is even greater, but we can better demonstrate the problem
through the complexity theoretical arguments laid out in the
previous section (ACT 1: Releasing the grip).
To abandon all of the tools and concepts that AI provided,
however, is to throw the baby out with the bath water.15
Hence, we want to reclaim much of the early conception
of AI as a part of cognitive science, but without encourag-
ing makeism. If we reconsider Lehnert’s argument for using
AI, we see many correct, important insights: (1) That exper-
imental psychology is limited in its ability to study cogni-
tion (hence the creation of cognitive science as an interdisci-
plinary field). (2) That cognition can be productively studied
14See for example the beginning of Newell (1970).
15A few people seem to have recognized this, including Newell.
He endorses the view that “the actual theories of cognitive psychol-
ogy are to be expressed as artificial intelligence systems” (p. 368),
and goes on to observe that “[a]fter the fact, one can see that such a
theory might have emerged within psychology (or linguistics) with-
out the advent of the computer. In historical fact, the theory emerged
by trying to program the computer to do non-numerical tasks and
by trying to construct abstract theories of computation and logic”
(Newell, 1970, p. 373).

12
VAN ROOIJ ET AL.
within the AI paradigm. And (3) that models’ problems are
theoretically informative.
These insights are all worth reclaiming; the community
just made one seemingly-small inferential miss-step that has
caused a lot of problems. Makeism is not a forced move.
Hunt pointed this out early on, arguing that
Computer programming seems to be a more ap-
propriate tool for studying the broad implica-
tions of a proposal for how one should think than
for realizing a testable model of how one does
think (Hunt, 1968, p. 160).
Computationalism without makeism is still theoretically
fruitful. We explain how next.
Theory without makeing
[P]rinciples from computer science and engi-
neering can be, if done carefully, imported into
how we carve [...] nature at its joints.
— Olivia Guest and Andrea E. Martin, (2023, p.
221)
How may computationalism help cognitive science ad-
vance if not through makeism? Core to the non-makeist
enterprise is the realisation that computationalism primar-
ily aids cognitive science by providing conceptual and for-
mal tools for theory development and for carefully assess-
ing whether something is computationally possible or not,
in principle and in practice. This paper is itself an example;
nowhere in this paper did we (try to) make a computational
replica of cognitive capacities. Yet, we were able to use a
computationalist framework to make substantial steps in re-
claiming AI for cognitive science. The remainder of this sec-
tion will give further examples of how AI as theoretical psy-
chology or computational cognitive science can be pursued
productively and soundly.16
As a disclaimer, we note that research in cognitive sci-
ence often cannot be cleanly divided into makeist and non-
makeist. We have not seen makeism clearly distinguished
from computationalism in the literature before (see Box 2),
and so cognitive scientists will generally not have thought
about it explicitly, let alone clarified the nature of their work.
Hence, when we cite papers as examples, we wish to high-
light the non-makeist readings of some of the arguments
made, but without implying that there are no problematic
traces of makeism in the original texts.
Levels of explanation.
Formalisms and concepts from
computer science allow us to conceptually distinguish be-
tween cognitive processes (algorithms), the capacities they
realise (the problems that they solve), and their physical im-
plementations (chemical, biological, interactive, etc.). This
conceptual distinction, also often referred to as Marr’s lev-
els (Marr, 1982) is theoretically productive, especially when
pursued in a non-makeist fashion.
The distinction between levels is conceptually useful in
general, but also brings specific benefits when we want to
formalise and reason about our theories, as we explain next.
Capacities as problems.
Within the levels-framework,
the approach known as computational-level modelling has
a strong tradition in cognitive science (with debates on its
proper interpretation continuing to this day; Blokpoel, 2018;
R. P. Cooper & Peebles, 2018; Peebles & Cooper, 2015).
This approach allows us to conceptually engineer cognitive
capacities as ‘computational problems’ and to model them
formally (see e.g. Blokpoel & van Rooij, 2021; van Rooij &
Baggio, 2021; van Rooij & Blokpoel, 2020) without needing
to commit to specific assumptions at the algorithmic or im-
plementation levels (other than computability and tractabil-
ity; more in the next subsection). This is especially useful
since—as argued throughout this paper—we do not know
how to computationally realise substantive cognitive capac-
ities such as human-level perception, reasoning, memory,
categorisation, decision-making, problem-solving, language,
analogising, communication, learning, planning, etc. Yet, as
cognitive scientists, we do want to make progress in devel-
oping a theoretical understanding of these capacities.
Computational modelling of capacities can help us to
make our assumptions precise and explicit, and to draw out
their consequences, without the need to simulate the pos-
tulated computations (though simulations have their uses;
more on that next). For instance, with formal computational-
level models and mathematical proof techniques at hand,
one can critically assess claims of explanatory adequacy
(Blokpoel & van Rooij, 2021; Egan, 2017; van Rooij &
Baggio, 2021), claims of intractability (Adolfi, Wareham, &
van Rooij, 2023), claims of tractability (Kwisthout & van
Rooij, 2020; van Rooij, Evans, Muller, Gedge, & Wareham,
2008), claims of competing theories (Blokpoel & van Rooij,
2021), claims of evolvability (Rich, Blokpoel, de Haan, &
van Rooij, 2020; Woensdregt et al., 2021), and claims of
approximability (Kwisthout & Van Rooij, 2013; Kwisthout,
Wareham, & Van Rooij, 2011).
16We acknowledge that computational modelling can also con-
tribute to productive theory development without committing to
computationalism (Guest & Martin, 2021; Morgan & Morri-
son, 1999). Here we focus specifically on computationalist mod-
elling, because we want to highlight that computationalism with-
out makeism is possible. We also acknowledge existing critiques
of ‘AI as computationalism’. Some of those critiques may target
makeism (and/or dehumanisation; Baria & Cross, 2021; Birhane,
2021; Birhane & van Dijk, 2020; Erscoi et al., 2023; van der Gun &
Guest, 2023) more than computationalism as a theoretical tool per
se (but we will leave that judgement up to the critics). Be that as it
may, we believe it is useful to explain how makeism and computa-
tionalism are dissociable, just as cognitivism and computationalism
are dissociable (M. Villalobos & Dewhurst, 2017, 2018) and rep-
resentationalism and computationalism are dissociable (Miłkowski,
2013, 2018; Piccinini, 2008).

RECLAIMING AI
13
Algorithms and simulations.
Similarly, algorithmic-
and implementation-level models can be postulated and crit-
ically assessed using computational tools. While this can
sometimes be done analytically, more often computer sim-
ulations prove useful for these types of (complex and dy-
namic) models. Using computer simulations, for example,
one can assess claims about possible functioning under net-
work damage (Guest, Caso, & Cooper, 2020), claims of
explanatory scope and adequacy (Adolfi, Bowers, & Poep-
pel, 2023; van de Braak, Dingemanse, Toni, van Rooij, &
Blokpoel, 2021), claims of approximation (Blokpoel & van
Rooij, 2021, Chapter 8), claims of ruling out possible so-
called neural codes (Guest & Love, 2017), and claims of
mechanistic possibilities (Bartlett et al., 2023; ten Oever &
Martin, 2021).
Importantly, this use of simulations is to be distinguished
from makeist uses of simulation that confuse the models (ex-
planans) for the thing modelled (explanandum) and/or take
the simulation results to directly imply something about ‘how
things work’ in the real world (e.g. for real-world brains, cog-
nition, or behaviour). Instead, non-makeist computer simula-
tions are theoretical tools that can demonstrate proof of con-
cept or demonstrate the in-principle (im)possibility of phe-
nomena arising from the theorised constructs and hypoth-
esised mechanisms. Computer simulations support and ex-
tend a scientist’s thinking capacity, and enable computerised
‘thought experiments’ (R. Cooper, 2005) to reason through
‘what ifs’ and answer questions like ‘how possibly’.17 These
simulations—as indeed any models that the cognitive scien-
tist could use—are necessarily abstract and idealised; this is
unproblematic, though, as long as the scientist recognises it
and takes care to draw only those inferences which are really
warranted by the model.
Underdetermination.
A general theoretical property
that follows from computationalism is that cognitive capaci-
ties are multiply realisable, in several ways. Van Rooij and
Baggio (2021) use a sorting problem as a simple illustra-
tion. Sorting can be done by bubble sort, insert sort, or any
of a whole host of distinct sorting algorithms (Knuth, 1968)
which in turn can be physically realised by brains, comput-
ers, water pipes, or even distributed over people (see e.g. Fig-
ure 1 in van Rooij & Blokpoel, 2020; and Box 1 in van Rooij
& Baggio, 2021). This shows how, first, one and the same
problem can be computed by different algorithms, and sec-
ond, one and the same algorithm can be physically realised
in different ways. This implies that we are dealing with mas-
sive underdetermination of theory by data: i.e., if we observe
behaviours consistent with a computational level theory, we
cannot infer which algorithms or neural processes underlie
the behaviour.
The computational lens helps us to appreciate the de-
gree of underdetermination we face. In standard experimen-
tal cognitive psychology, often two or a handful of differ-
ent theories are compared and tested empirically “against
each other”. But the principle of computational multiple re-
alisability shows that for any given behaviour there may be
|A| × |I| many possible algorithmic-implementational the-
ories. This means that some inferential practices in com-
putational cognitive (neuro)science are highly problematic
(Guest & Martin, 2023). Moreover, computational level the-
ories are also themselves underdetermined by data, because
any finite set of observations is also consistent with in-
finitely many functions (capacities).18 This means that all
computational-level theories are—and remain—conjectural.
Nonetheless, we can do some things to evaluate and adjudi-
cate between them.
Computational realisability.
Underdetermined com-
putational level theories can be constrained by the compu-
tationalist requirement that the problems and processes they
postulate must be computationally realisable—by the cogni-
tive system under study, not by scientists—both in princi-
ple and in practice. In-principle realisability is also known as
computability; a problem is computable if there can exist at
least one algorithm for computing it. In-practice realisability
is also known as tractability; a problem is tractable if there
can exist at least one tractable19 algorithm for computing it.
Given that computational-level theories often formalise
capacities as problems (or equivalently functions) while re-
maining agnostic about how these problems are computed,
they can on occasion postulate problems that are uncom-
putable or intractable; in fact, this happens regularly. This
provides the opportunity to critically reflect on the theory,
and if possible, to find a minimal revision that renders the
theory minimally computable and tractable while preserving
the core intuitions and motivations behind the theory. This
process can yield new knowledge, ideas, and research tra-
jectories for cognitive scientists (cf. Adolfi, van de Braak,
& Woensdregt, 2023). For example, it may yield new theo-
retical interpretations or predictions that can be used to fur-
ther assess the explanatory and empirical adequacy of the
(revised) theories. Alternatively, if a theory cannot be suc-
cessfully revised, this can be a sign that it is time to question
its initial motivation and to go back to the drawing board.
The process thereby allows us to sculpt otherwise underde-
17For discussion of how possibly explanation, see e.g. (Bokulich,
2014, 2017; Grüne-Yanoff, 2013; Sullivan, 2022).
18Similarly, no finite set of ‘impressive’ observations about
AIs/LLMs establishes that it exhibits a (human) cognitive capacity.
This link can only be theoretically established; i.e., if we propose
that a machine has a cognitive capacity X, we must also formally
charactise X such that the machine’s capacity Y can be mathemati-
cally proven to be X ≡Y under relevant and naturalistic conditions,
using appropriate proof techniques (see e.g. Blokpoel & van Rooij,
2021, Chapter 5).
19Tractable can be formalised, for instance, as needing a poly-
nomial or fixed-parameter tractable amount of computational re-
sources (Frixione, 2001; van Rooij, 2008; van Rooij et al., 2019)

14
VAN ROOIJ ET AL.
termined theories so as to learn more about how cognition
could or could not work (Blokpoel, 2018). This is no magic
bullet, however; underdetermination of theory by data cannot
be eliminated, and any ways of dealing with it will remain
necessarily incomplete (cf. Adolfi, Wareham, & van Rooij,
2023; Devezer, 2023; Rich et al., 2021).
Slow (computational cognitive) science.
All of this
may seem excruciatingly slow compared to the apparent
speed of progress in today’s machine learning approaches
to AI. But to genuinely make progress we need to go this
slowly, and in fact cannot go any faster (Adolfi & van Rooij,
2023; Rich et al., 2021). There is just no way to proceduralise
or automate either the creation of minds or the explanation
of minds. The way to make progress is through the metic-
ulous development of theoretical ideas, informed by formal
and computational modelling, drawing out limitations, con-
sequences, and building solid knowledge along the way. In
other words, what we advocate is more theoretical thinking
(see also Guest, 2023; Guest & Martin, 2021; van Rooij &
Baggio, 2020), and less (unthinking) machine learning or
less confusion between machine learning and theory (cf. An-
drews, 2023). Then AI can be a useful theoretical tool for
cognitive science and regain its rightful place in the interdis-
ciplinary hexagon.
Conclusion
The thesis of computationalism implies that it is possible
in principle to understand human cognition as a form of com-
putation. However, this does not imply that it is possible in
practice to computationally (re)make cognition. In this paper,
we have shown that (re)making human-like or human-level
minds is computationally intractable (even under highly ide-
alised conditions). Despite the current hype surrounding “im-
pending” AGI, this practical infeasibility actually fits very
well with what we observe (for example, running out of qual-
ity training data and the non-human-like performance of AI
systems when tested rigorously).
Many societal problems surrounding AI have received
thorough treatment elsewhere. Our focus here has been on
a different—but not unrelated—problem, namely that AI-
as-engineering has been trespassing into cognitive science,
with some people drawing overly hasty inferences from en-
gineered AI systems to human cognition. This is a problem
because any such system created now or in the near future is
a mere decoy when our goal is to understand human cogni-
tion, and treating it as a substitute for human cognition for
scientific purposes will only confuse and mislead us.
Early cognitive scientists rightly recognised the tremen-
dous potential of AI as a theoretical tool, but due to
widespread, implicit makeist elements, AI and cognitive
science became increasingly dissociated over time. Now,
interest in AI among cognitive scientists is enjoying a
renaissance—but the interest seems to be in the wrong type
of AI, namely AI-as-engineering, which distorts our under-
standing of cognition and cognitive science. Accordingly,
the time is apt to reclaim AI-as-theoretical-psychology as a
rightful part of cognitive science. As we have argued, this
involves embracing all the valuable tools that computation-
alism provides, but without (explicitly or implicitly) falling
into the trap of thinking that we can or should try to engineer
human(-like or -level) cognition in practice.
Acknowledgements
We thank the Lorentz Center in Leiden, The Netherlands,
and the organizers of the Lorentz Workshop “What makes
a good theory? Interdisciplinary perspectives" (20–24 June
2022), Berna Devezer, Sashank Varma, Joshua Skewes, Todd
Wareham, and Iris van Rooij, for creating and fostering the
circumstances that made this work possible.
IvR acknowledges the support of the Netherlands Institute
for Advanced Studies in the Humanities and Social Sciences
(NIAS-KNAW) and the Lorentz Center for a 2020/21 Dis-
tinguished Lorentz Fellowship, which financed the Lorentz
workshop and a 6-month stay at NIAS (Amsterdam, The
Netherlands). AK acknowledges the support of an NSERC
Discovery grant that funded part of this work.
IvR, OG, and FA thank the Computational Cognitive
Science group at the Donders Centre for Cognition (Ni-
jmegen, The Netherlands) for useful discussions, in particu-
lar Laura van de Braak, Marieke Woensdregt, Nils Donselaar,
Annelies Kleinherenbrink, and Mark Blokpoel. FA thanks
David Poeppel for support and useful discussions. AK thanks
Valentine Kabanets, Russell Impagliazzo and Shuici Hira-
hara for helpful discussions on nuances and consequences
of the Perfect-vs-Chance theorem and related work, and Si-
mons Institute for the Theory of Computing where this theo-
rem was presented and most of these discussions took place.
IvR thanks Marieke Woensdregt and Mark Dingemanse for
useful psycholinguistic advice. Any errors remain our own.
References
Adolfi, F., Bowers, J. S., & Poeppel, D. (2023). Suc-
cesses and critical failures of neural networks in captur-
ing human-like speech recognition. Neural Networks, 199–
211.
Adolfi, F., van de Braak, L., & Woensdregt, M. (2023). From
empirical problem-solving to theoretical problem-finding
perspectives on the cognitive sciences.
Adolfi, F., & van Rooij, I. (2023). Resource demands of an
implementationist approach to cognition. In Proceedings
of the 21st International Conference on Cognitive Model-
ing.
Adolfi, F., Wareham, T., & van Rooij, I. (2022). Com-
putational complexity of segmentation. arXiv preprint
arXiv:2201.13106.

RECLAIMING AI
15
Adolfi, F., Wareham, T., & van Rooij, I. (2023). A compu-
tational complexity perspective on segmentation as a cog-
nitive subcomputation. Topics in Cognitive Science, 15(2),
255–273.
Anderson, J. R. (1984). Cognitive psychology. Artificial In-
telligence, 23(1), 1–11.
Andrews, M. (2023). The immortal science of ML: Machine
learning & the theory-free ideal.
Angluin, D. (1992). Computational learning theory: Survey
and selected bibliography. In Proceedings of the twenty-
fourth annual acm symposium on theory of computing
(pp. 351–369).
Arora, S., & Barak, B. (2009). Computational complexity –
a modern approach. Cambridge University Press.
Baria, A. T., & Cross, K. (2021). The brain is a com-
puter is a brain: Neuroscience’s internal debate and
the social significance of the computational metaphor.
ArXiv, abs/2107.14042. Retrieved from https : / / api .
semanticscholar.org/CorpusID:236493182
Bartlett, M., Simone, K., Dumont, N. D., Furlong, M., Elia-
smith, C., Orchard, J., & Stewart, T. (2023). Improving re-
inforcement learning with biologically motivated continu-
ous state representations. In Proceedings of the 21st Inter-
national Conference on Cognitive Modeling.
Bender, E. M. (2023). Google CEO peddles #AIhype on CBS
60 minutes. Medium. Retrieved from https://medium.com/
@emilymenonbender / google - ceo - peddles - aihype - on -
cbs-60-minutes-4a0e080ef406
Bender,
E.
M.,
Gebru,
T.,
McMillan-Major,
A.,
&
Shmitchell, S. (2021). On the dangers of stochastic par-
rots: Can language models be too big? In Proceedings of
the 2021 acm conference on fairness, accountability, and
transparency (pp. 610–623).
Bender, E. M., & Koller, A. (2020). Climbing towards NLU:
On meaning, form, and understanding in the age of data. In
Proceedings of the 58th annual meeting of the association
for computational linguistics (pp. 5185–5198).
Birhane, A. (2021). The impossibility of automating ambi-
guity. Artificial Life, 27(1), 44–61.
Birhane, A., & Guest, O. (2021). Towards decolonising com-
putational sciences. Kvinder, Køn & Forskning, (1), 60–73.
Birhane, A., Kalluri, P., Card, D., Agnew, W., Dotan, R., &
Bao, M. (2022). The Values Encoded in Machine Learning
Research. arXiv. arXiv: 2106.15590 [cs]
Birhane, A., Prabhu, V., Han, S., & Boddeti, V. N. (2023).
On Hate Scaling Laws For Data-Swamps. arXiv. arXiv:
2306.13141 [cs]
Birhane, A., Prabhu, V. U., & Kahembwe, E. (2021). Mul-
timodal datasets: Misogyny, pornography, and malignant
stereotypes. arXiv preprint arXiv:2110.01963.
Birhane, A., & van Dijk, J. (2020). Robot rights? Let’s
talk about human welfare instead. In Proceedings of
the AAAI/ACM Conference on AI, Ethics, and Society
(pp. 207–213).
Bisig, D., & Pfeifer, R. (2008). Understanding by design. the
synthetic approach to intelligence. Geiser, R., Explorations
in Architecture. Boston: Birkauser.
Blokpoel, M. (2018). Sculpting computational-level models.
Topics in cognitive science, 10(3), 641–648.
Blokpoel, M., & van Rooij, I. (2021). Theoretical modeling
for cognitive science and psychology. Open Textbook. Re-
trieved from https://computationalcognitivescience.github.
io/lovelace/
Boden, M. A. [Margaret A]. (1988). Computer models of
mind: Computational approaches in theoretical psychol-
ogy. Cambridge University Press.
Boden, M. A. [Margaret A.]. (2008). Mind as machine: A
history of cognitive science. Oxford University Press.
Bokulich, A. (2014). How the tiger bush got its stripes:
‘how possibly’ vs. ‘how actually’ model explanations. The
Monist, 97(3), 321–338.
Bokulich, A. (2017). Models and explanation. Springer
handbook of model-based science, 103–118.
Bowers, J. S., Malhotra, G., Adolfi, F. G., Dujmovi´c, M.,
Montero, M. L., Biscione, V., ... Heaton, R. F. (2023). On
the importance of severely testing deep learning models of
cognition. PsyArXiv.
Bowers, J. S., Malhotra, G., Dujmovi´c, M., Montero, M. L.,
Tsvetkov, C., Biscione, V., ... Blything, R. (2022). Deep
Problems with Neural Network Models of Human Vision.
Behavioral and Brain Sciences, 1–74.
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
Horvitz, E., Kamar, E., ... Lundberg, S., et al. (2023).
Sparks of artificial general intelligence: Early experiments
with GPT-4. arXiv preprint arXiv:2303.12712.
Chalmers, D. J. (2011). A computational foundation for the
study of cognition. Journal of Cognitive Science, 12(4),
325–359.
Cooper, R. (2005). Thought experiments. Metaphilosophy,
36(3), 328–347.
Cooper, R. P., Fox, J., Farringdon, J., & Shallice, T. (1996). A
systematic methodology for cognitive modelling. Artificial
Intelligence, 85(1-2), 3–44.
Cooper, R. P., & Peebles, D. (2018). On the relation between
marr’s levels: A response to blokpoel (2017). Topics in
Cognitive Science, 10(3), 649–653.
Crawford, K. (2021). The atlas of AI: Power, politics, and
the planetary costs of artificial intelligence. Yale Univer-
sity Press.
Crenshaw, K. (1989). Demarginalizing the intersection of
race and sex: A black feminist critique of antidiscrimina-
tion doctrine, feminist theory and antiracist politics. u. Chi.
Legal f., 139.

16
VAN ROOIJ ET AL.
Crockett, M., & Messeri, L. (2023). Should large language
models replace human participants? doi:10.31234/osf.io/
4zdx9
Cummins, R. (2000). “how does it work?” versus “what are
the laws?”: Two conceptions of psychological explanation.
In Explanation and cognition (pp. 117–144). MIT Press.
DeepMind.
(2023).
DeepMind
-
About
page.
https://www.deepmind.com/about.
Dentella, V., Murphy, E., Marcus, G., & Leivada, E. (2023).
Testing AI performance on less frequent aspects of lan-
guage reveals insensitivity to underlying meaning. arXiv.
arXiv: 2302.12313 [cs]
Denvir, D., Yeager, L., & Johnson, M. (2023). Interview: AI
hype machine. Podcast. The Dig. Retrieved from https://
thedigradio.com/podcast/ai-hype-machine-w-meredith-
whittaker-ed-ongweso-and-sarah-west/
Devezer, B. (2023). There are no shortcuts to theory. doi:10.
31222/osf.io/umkan
Dietrich, E. (1994). Computationalism. In Thinking comput-
ers and virtual persons (pp. 109–136). Elsevier.
Dillion, D., Tandon, N., Gu, Y., & Gray, K. (2023). Can ai
language models replace human participants? Trends in
Cognitive Sciences.
Dillon, S. (2020). The Eliza effect and its dangers: From de-
mystification to gender critique. Journal for Cultural Re-
search, 24(1), 1–15.
Dingemanse, M., & Liesenfeld, A. (2022). From text to
talk: Harnessing conversational corpora for humane and
diversity-aware language technology. In Proceedings of
the 60th annual meeting of the association for compu-
tational linguistics (volume 1: Long papers) (pp. 5614–
5633). doi:10.18653/v1/2022.acl-long.385
Egan, F. (2017). Function-theoretic explanation and the
search for neural mechanisms. In Explanation and Inte-
gration in Mind and Brain Science (pp. 145–163). Oxford
University Press.
Eloundou, T., Manning, S., Mishkin, P., & Rock, D. (2023).
GPTs are GPTs: An early look at the labor market im-
pact potential of large language models. arXiv preprint
arXiv:2303.10130.
Erscoi, L. A., Kleinherenbrink, A., & Guest, O. (2023). Pyg-
malion Displacement: When Humanising AI Dehumanises
Women. SocArXiv. doi:https://doi.org/10.31235/osf.io/
jqxb6
Feigenbaum, E. A., & Feldman, J. (Eds.). (1963). Computers
and thought. New York McGraw-Hill.
Feynman, R. P. (1988). Caltech Archives. Retrieved from
https : / / digital . archives . caltech . edu / islandora / object /
image:2545
Fiorenza, E. S. (1993). But she said: Feminist practices of
biblical interpretation. Beacon Press.
Fluckinger, D. (2022). Ex-Google engineer Blake Lemoine
discusses sentient AI. Tech Target. Retrieved from https:
/ / www. techtarget . com / searchenterpriseai / feature / Ex -
Google-engineer-Blake-Lemoine-discusses-sentient-AI
Fodor, J. (2005). Reply to Steven Pinker ‘So how does the
mind work?’ Mind & Language, 20(1), 25–32.
Fodor, J., & Pylyshyn, Z. W. (1988). Connectionism and cog-
nitive architecture: A critical analysis. Cognition, 28(1-2),
3–71.
Forbus, K. D. (2010). AI and cognitive science: The past and
next 30 years. Topics in Cognitive Science, 2(3), 345–356.
Frank, M. C. [Michael C]. (2023a). Baby steps in evaluating
the capacities of large language models. Nature Reviews
Psychology, 1–2.
Frank, M. C. [Michael C.]. (2023b). Large language models
as models of human cognition. PsyArXiv.
Frixione, M. (2001). Tractable competence. Minds and Ma-
chines, 11, 379–397.
Garey, M. R., & Johnson, D. S. (1979). Computers and In-
tractability: A Guide to the Theory of NP-Completeness
(1st Edition). New York u.a: W. H. Freeman.
Gentner, D. (2010). Psychology in cognitive science: 1978–
2038. Topics in Cognitive Science, 2(3), 328–344.
Gentner, D. (2019). Cognitive science is and should be plu-
ralistic. Topics in Cognitive Science, 11(4), 884–891.
Grüne-Yanoff, T. (2013). Appraising models nonrepresenta-
tionally. Philosophy of Science, 80(5), 850–861.
Guest, O. (2023). What makes a good theory, and how do we
make a theory good? doi:10.31234/osf.io/8fxds
Guest, O., Caso, A., & Cooper, R. P. (2020). On Simulat-
ing Neural Damage in Connectionist Networks. Computa-
tional Brain & Behavior, 289–321.
Guest, O., & Love, B. C. (2017). What the success of brain
imaging implies about the neural code. eLife, 6, e21397.
doi:10.7554/eLife.21397
Guest, O., & Martin, A. E. [Andrea E]. (2021). How compu-
tational modeling can force theory building in psychologi-
cal science. Perspectives on Psychological Science, 16(4),
789–802.
Guest, O., & Martin, A. E. [Andrea E.]. (2023). On Logi-
cal Inference over Brains, Behaviour, and Artificial Neural
Networks. Computational Brain & Behavior.
Harding, J., D’Alessandro, W., Laskowski, N., & Long, R.
(2023). Ai language models cannot replace human re-
search participants. AI & SOCIETY, 1–3.
Hardy, M., Sucholutsky, I., Thompson, B., & Griffiths, T.
(2023). Large language models meet cognitive science:
Llms as tools, models, and participants. In Proceedings of
the annual meeting of the cognitive science society.
Hirahara, S. (2022). Np-hardness of learning programs and
partial mcsp. In 2022 ieee 63rd annual symposium on foun-
dations of computer science (focs) (pp. 968–979). IEEE.
Hughes, J. (2021). The Deskilling of Teaching and the Case
for Intelligent Tutoring Systems. Journal of Ethics and

RECLAIMING AI
17
Emerging Technologies, 31(2), 1–16. doi:10.55613/jeet.
v31i2.90
Hughes, R. I. (1997). Models and representation. Philosophy
of science, 64(S4), S325–S336.
Hünefeldt, T., & Brunetti, R. (2004). Artificial intelligence as
“theoretical psychology”: Christopher Longuet-Higgins’
contribution to cognitive science. Cognitive Processing, 5,
137–139.
Hunt, E. (1968). Computer simulation: Artificial intelligence
studies and their relevance to psychology. Annual Review
of Psychology, 19(1), 135–168.
Johnson-Laird, P. N. (1988). The computer and the mind:
An introduction to cognitive science. Harvard University
Press.
Kalluri, P. (2020). Don’t ask if artificial intelligence is good
or fair, ask how it shifts power. Nature, 583(7815), 169–
169.
Karpathy, A., Abbeel, P., Brockman, G., Chen, P., Cheung,
V., Duan, R., . . . Zaremba, W. (2016). Generative models.
OpenAI Blog. Retrieved from https://web.archive.org/web/
20180121082551/https://blog.openai.com/generative-
models/
Kearns, M. J., & Vazirani, U. (1994). An introduction to com-
putational learning theory. MIT Press.
Knuth, D. E. (1968). The Art of Computer Programming:
Sorting and Searching. Addison-Wesley Publishing Com-
pany.
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Im-
agenet classification with deep convolutional neural net-
works. Advances in neural information processing sys-
tems, 25.
Kwisthout, J., & Van Rooij, I. (2013). Bridging the gap be-
tween theory and practice of approximate bayesian infer-
ence. Cognitive Systems Research, 24, 2–8.
Kwisthout, J., & van Rooij, I. (2020). Computational re-
source demands of a predictive Bayesian brain. Compu-
tational Brain & Behavior, 3(2), 174–188.
Kwisthout, J., Wareham, T., & Van Rooij, I. (2011). Bayesian
intractability is not an ailment that approximation can cure.
Cogn. Sci., 35(5), 779–784.
Landy, D., Silbert, N., & Goldin, A. (2013). Estimating large
numbers. Cognitive science, 37(5), 775–799.
Langley, P. (2006). Intelligent behavior in humans and ma-
chines. In American association for artificial intelligence.
Lee, J., Le, T., Chen, J., & Lee, D. (2023). Do language mod-
els plagiarize? In Proceedings of the acm web conference
2023 (pp. 3637–3647).
Lehnert, W. G. (1977). The process of question answering.
Yale University.
Liesenfeld, A., & Dingemanse, M. (2022). Building and cu-
rating conversational corpora for diversity-aware language
science and technology. arXiv preprint arXiv:2203.03399.
Liesenfeld, A., Lopez, A., & Dingemanse, M. (2023). Open-
ing up chatgpt: Tracking openness, transparency, and ac-
countability in instruction-tuned text generators. arXiv
preprint arXiv:2307.05532.
Lighthill, J. (1973). Artificial intelligence: A general sur-
vey. In Artificial intelligence: A paper symposium. Lon-
don: Science Research Council.
Longuet-Higgins, H. (1981). Artificial intelligence—a new
theoretical psychology? Cognition.
Mahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N.,
Tenenbaum, J. B., & Fedorenko, E. (2023). Dissociating
language and thought in large language models: A cogni-
tive perspective. arXiv preprint arXiv:2301.06627. arXiv:
2301.06627
Marr, D. (1982). Vision: A computational investigation into
the human representation and processing of visual infor-
mation. San Francisco: W. H. Freeman.
Marx, P., & Wickham, E. (2023). Don’t fall for the AI hype.
Podcast. Tech won’t save us. Retrieved from https : / /
techwontsave.us/ episode/ 151_dont_fall_for_the_ai_
hype_w_timnit_gebru.html
Masterman, M. (1965). Semantic algorithms. In Proceedings
of the conference on computer-related semantics held in
las vegas, nevada, usa (Vol. 4, pp. 1–97).
McCarty Carino, M., & Shin, D. (2023). The human labor
behind ai chatbots and other smart tools. Retrieved from
https://www.marketplace.org/shows/marketplace-tech/
human-labor-behind-ai-chatbots-and-other-smart-tools/
McCorduck, P. (2019). This could be important: My life
and times with the artificial intelligentsia. Carnegie Mel-
lon University: ETC Press.
McQuillan, D. (2022). Resisting AI: An anti-fascist approach
to artificial intelligence. Policy Press.
Meinhart, W. A. (1966). Artificial intelligence, computer
simulation of human cognitive and social processes, and
management thought. Academy of Management Journal,
9(4), 294–307.
Miłkowski, M. (2013). Explaining the computational mind.
MIT Press.
Miłkowski, M. (2018). From computer metaphor to com-
putational modeling: The evolution of computationalism.
Minds and Machines, 28(3), 515–541.
Miller, G. A. (2003). The cognitive revolution: A historical
perspective. Trends in cognitive sciences, 7(3), 141–144.
Mitchell, M. (2021). Why AI is harder than we think. arXiv
preprint arXiv:2104.12871.
Morgan, M. S., & Morrison, M. (1999). Models as medi-
ators: Perspectives on natural and social science. Cam-
bridge University Press.
Neisser, U. (1963). The imitation of man by machine: The
view that machines will think as man does reveals mis-
understanding of the nature of human thought. Science,
139(3551), 193–197.

18
VAN ROOIJ ET AL.
Newell, A. (1970). Remarks on the relationship between arti-
ficial intelligence and cognitive psychology. In Theoretical
Approaches to Non-Numerical Problem Solving: Proceed-
ings of the IV Systems Symposium at Case Western Reserve
University (pp. 363–400). Springer.
Norvig, P. [Peter]. (1992). Paradigms of artificial intelligence
programming: Case studies in common lisp. Morgan Kauf-
mann.
OpenAI.
(2023).
Planning
for
AGI
and
beyond.
https://openai.com/blog/planning-for-agi-and-beyond.
Osborne, N. (2015). Intersectionality and kyriarchy: A
framework for approaching power and social justice in
planning and climate change adaptation. Planning Theory,
14(2), 130–151.
Parberry, I. (1997). Knowledge, Understanding, and Compu-
tational Complexity. In Optimality in biological and arti-
ficial networks? (p. 19). Mahwah, N.J: Lawrence Erlbaum
Associates.
Peebles, D., & Cooper, R. P. (2015). Thirty years after Marr’s
vision: Levels of analysis in cognitive science. Topics in
cognitive science, 7(2), 187–190.
Pelley, S. (2023). Is artificial intelligence advancing too
quickly? what ai leaders at google say. CBS Interactive.
Retrieved from https://www.cbsnews.com/news/google-
artificial-intelligence-future-60-minutes-transcript-2023-
04-16/
Pérez, J., Marinkovi´c, J., & Barceló, P. (2019). On the tur-
ing completeness of modern neural network architectures.
arXiv preprint arXiv:1901.03429.
Pfeifer, R., & Scheier, C. (2001). Understanding intelligence.
MIT press.
Piccinini, G. (2008). Computation without representation.
Philosophical studies, 137, 205–241.
Pléh, C., & Gurova, L. (2013). Existing and would-be ac-
counts of the history of cognitive science: An introduc-
tion. Pléh Csaba–Gurova, Lilia–Ropolyi, László (2013 ed.)
New Perspectives on the History of Cognitive Science.
Akadémiai Kiadó, Budapest.
Rich, P., Blokpoel, M., de Haan, R., & van Rooij, I. (2020).
How intractability spans the cognitive and evolutionary
levels of explanation. Topics in cognitive science, 12(4),
1382–1402.
Rich, P., de Haan, R., Wareham, T., & van Rooij, I. (2021).
How hard is cognitive science? In Proceedings of the an-
nual meeting of the cognitive science society (Vol. 43).
Roberts, S., Wood, S., & Eadon, Y. (2023). "we care about
the internet; we care about everything" understanding so-
cial media content moderators’ mental models and sup-
port needs. In Proceedings of the 56th hawaii international
conference on system sciences.
Rose, I. (2023). The workers already replaced by artificial
intelligence. BBC. Retrieved from https://www.bbc.com/
news/business-65906521
Russell, S. J., & Norvig, P. [P.]. (2010). Artificial intelligence
a modern approach. Pearson Education, Inc.
Schaeffer, R., Miranda, B., & Koyejo, S. (2023). Are Emer-
gent Abilities of Large Language Models a Mirage? arXiv.
arXiv: 2304.15004 [cs]
Semuels, A. (2020). Millions of americans have lost jobs in
the pandemic—and robots and ai are replacing them faster
than ever. Time magazine.
Shumailov, I., Shumaylov, Z., Zhao, Y., Gal, Y., Papernot,
N., & Anderson, R. (2023). The curse of recursion: Train-
ing on generated data makes models forget. arXiv preprint
arxiv:2305.17493.
Siegelmann, H. T., & Sontag, E. D. (1992). On the computa-
tional power of neural nets. In Proceedings of the fifth an-
nual workshop on computational learning theory (pp. 440–
449).
Simon, H. A. (1983). Why should machines learn? In Ma-
chine learning (pp. 25–37). Elsevier.
Smyth, T., & Dimond, J. (2014). Anti-oppressive design. In-
teractions, 21(6), 68–71.
Spanton, R. W., & Guest, O. (2022). Measuring Trustworthi-
ness or Automating Physiognomy? A Comment on Safra,
Chevallier, Gr\ezes, and Baumard (2020). arXiv preprint
arXiv:2202.08674.
Stark, L., & Hutson, J. (2022). Physiognomic artificial intel-
ligence. Fordham Intellectual Property, Media and Enter-
tainment Law Journal, 32(4), 922.
Sullivan, E. (2022). Understanding from machine learning
models. British Journal for the Philosophy of Science,
73(1), 109–133.
ten Oever, S., & Martin, A. E. [Andrea E]. (2021). An os-
cillating computational model can track pseudo-rhythmic
speech by using linguistic predictions. eLife, 10. doi:10.
7554/elife.68066
Thagard, P. (2007). Theory and experiment in cognitive sci-
ence. Artificial Intelligence, 171(18), 1104–1106.
Thagard, P., Holyoak, K. J., Nelson, G., & Gochfeld, D.
(1990). Analog retrieval by constraint satisfaction. Artifi-
cial intelligence, 46(3), 259–310.
Thrall, J. H., Li, X., Li, Q., Cruz, C., Do, S., Dreyer, K.,
& Brink, J. (2018). Deskilling of medical professionals:
an unintended consequence of AI implementation? Jour-
nal of the American College of Radiology, 15(3), 504–508.
doi:10.7413/1827-5834014
Toole, B. A. et al. (1998). Ada, the enchantress of numbers:
Prophet of the computer age, a pathway to the 21st cen-
tury. Critical Connection.
Tuckute, G., Sathe, A., Srikant, S., Taliaferro, M., Wang, M.,
Schrimpf, M., ... Fedorenko, E. (2023). Driving and sup-
pressing the human language network using large language
models. bioRxiv.
Turing, A. (1950). Computing machinery and intelligence-
am turing. Mind, 59(236), 433.

RECLAIMING AI
19
Vallor, S. (2015). Moral Deskilling and Upskilling in a New
Machine Age: Reflections on the Ambiguous Future of
Character. Philosophy and Technology, 28(1), 107–124.
doi:10.1007/s13347-014-0156-9
van de Braak, L. D., Dingemanse, M., Toni, I., van Rooij, I.,
& Blokpoel, M. (2021). Computational challenges in ex-
plaining communication: How deep the rabbit hole goes.
In Proceedings of the annual meeting of the cognitive sci-
ence society (Vol. 43).
van der Gun, L., & Guest, O. (2023). Artificial in-
telligence: Panacea or non-intentional dehumanisation?
doi:10.31235/osf.io/rh4fw
van Emde Boas, P. (1990). Machine models and simulations.
In J. V. Leeuwen (Ed.), Handbook of theoretical computer
science (vol. a) algorithms and complexity (pp. 1–66).
van Rooij, I. (2008). The Tractable Cognition Thesis. Cogni-
tive Science, 32(6), 939–984.
van Rooij, I. (2018). Water lilies. Retrieved from https : / /
irisvanrooijcogsci.com/2018/08/16/water-lilies/
van Rooij, I. (2023). Stop feeding the hype and start resisting.
Retrieved from https://irisvanrooijcogsci.com/2023/01/14/
stop-feeding-the-hype-and-start-resisting/
van Rooij, I., & Baggio, G. (2020). Theory development de-
quires an epistemological sea change. Psychological In-
quiry, 31(4), 321–325.
van Rooij, I., & Baggio, G. (2021). Theory before the test:
How to build high-verisimilitude explanatory theories in
psychological science. Perspectives on Psychological Sci-
ence, 16(4), 682–697.
van Rooij, I., & Blokpoel, M. (2020). Formalizing verbal
theories: A tutorial by dialogue. Social Psychology, 51(5),
285.
van Rooij, I., Blokpoel, M., Kwisthout, J., & Wareham, T.
(2019). Cognition and Intractability: A Guide to Classical
and Parameterized Complexity Analysis. Cambridge Uni-
versity Press.
van Rooij, I., Evans, P., Muller, M., Gedge, J., & Wareham,
T. (2008). Identifying sources of intractability in cognitive
models: An illustration using analogical structure map-
ping. In Proceedings of the annual meeting of the cognitive
science society (Vol. 30).
Villalobos, M., & Dewhurst, J. (2017). Why post-cognitivism
does not (necessarily) entail anti-computationalism. Adap-
tive Behavior, 25(3), 117–128.
Villalobos, M., & Dewhurst, J. (2018). Enactive autonomy in
computational systems. Synthese, 195(5), 1891–1908.
Villalobos, P., Sevilla, J., Heim, L., Besiroglu, T., Hobbhahn,
M., & Ho, A. (2022). Will we run out of data? an analysis
of the limits of scaling datasets in machine learning. arXiv
preprint arXiv:2211.04325.
Wagenaar, W., & Sagaria, S. D. (1975). Misperception of ex-
ponential growth. Perception & Psychophysics, 18, 416–
422.
Wagenaar, W., & Timmers, H. (1978). Extrapolation of ex-
ponential time series is not enhanced by having more data
points. Perception & Psychophysics, 24, 182–184.
Wagenaar, W., & Timmers, H. (1979). The pond-and-
duckweed problem; three experiments on the mispercep-
tion of exponential growth. Acta Psychologica, 43(3), 239–
251.
Weiss, G., Goldberg, Y., & Yahav, E. (2018). On the practical
computational power of finite precision rnns for language
recognition. arXiv preprint arXiv:1805.04908.
Weizenbaum, J. (1966). ELIZA—a computer program for
the study of natural language communication between man
and machine. Communications of the ACM, 9(1), 36–45.
Wells, A. J. (1998). Turing’s analysis of computation and the-
ories of cognitive architecture. Cognitive Science, 22(3),
269–294. doi:10.1016/S0364-0213(99)80041-X
Woensdregt, M. S., Spike, M., de Haan, R., Wareham, T.,
van Rooij, I., & Blokpoel, M. (2021). Why is scaling up
models of language evolution hard? In Proceedings of the
annual meeting of the cognitive science society (Vol. 43).
Wood, S. (1987). The deskilling debate, new technology and
work organization. Acta Sociologica, 30(1), 3–24.
Yuan, J., Liberman, M., & Cieri, C. (2006). Towards an in-
tegrated understanding of speaking rate in conversation. In
Ninth international conference on spoken language pro-
cessing.
APPENDIX
This appendix contains some additional technical material,
leading up to (and including) the detailed proof of Theo-
rem 2. Our aim in this appendix is to make the proof acces-
sible to an audience that has a general (theoretical) computer
science background.
We will begin with some brief reminders of various no-
tions from computational complexity theory needed to fol-
low the proof. We will assume that the reader is familiar
with basic notions such as the complexity classes P and NP,
polynomial-time (many-to-one) reductions, and the notions
of NP-hardness and -completeness. For more details on these
notions, we refer to textbooks on the topic (e.g. Arora &
Barak, 2009).
Probabilistic computation.
The first notion that we will
recap is that of probabilistic algorithms and the complexity
class BPP. Intuitively, a probabilistic algorithm has access
to a random bit generator, and can use these random bits in
its computation. This can be formalized using the notion of
probabilistic Turing machines. The result of this is that the
running time and the output of the algorithm are both random
variables.
The complexity class BPP contains all decision problems
that can be solved by a bounded-error polynomial-time prob-
abilistic algorithm. This means the following. The running
time of the algorithm should be upper bounded by a polyno-

20
VAN ROOIJ ET AL.
mial of the input size (i.e., regardless of the randomness, the
algorithm should halt within polynomial time). Moreover,
the output of the algorithm should be correct with proba-
bility at least 2/3. In other words, the algorithm may make
mistakes (in the sense of outputting the wrong answer), as
long as these occur with low probability. (For more details
on probabilistic algorithms and the class BPP, see e.g. Arora
& Barak, 2009, Chapter 7.)
A (bounded-error) randomized polynomial-time reduction
from one decision problem P1 to another decision problem
P2 is a probabilistic polynomial-time algorithm that takes
an input x for P1 and produces an input y for P2. The al-
gorithm should satisfy the property that (1) if x ∈P1 then the
probability that y ∈P2 is at least 2/3 and (2) if x < P1 then
the probability that y < P2 is at least 2/3. (Remember that y
is a random variable.) In other words, the reduction should
be correct (in the sense that the produced instance is a yes-
instance if and only if the original instance is a yes-instance)
with probability at least 2/3.
Promise problems.
A further notion that plays a role in
the proof of Theorem 2 is that of promise problems. Intu-
itively, a promise problem involves the promise that the input
satisfies a certain property. An algorithm solving a promise
problem is only evaluated on inputs that satisfy this promised
property. This can be formalized as follows. The inputs satis-
fying the promised property are captured by a set P ⊆Σ∗of
strings, over a given alphabet Σ. Then the yes- and no-inputs
of the problem are specified by two sets Pyes, Pno ⊆P such
that Pyes ∩Pno = ∅and Pyes ∪Pno = P. That is, the sets
Pyes, Pno partition P. An algorithm solving the problem then
gets an input x ∈P and has to decide if x ∈Pyes or x ∈Pno.
(When dealing with promise problems, the notions of reduc-
tions have to be modified accordingly, in a straightforward
manner.)
Sampling.
Another notion playing a role in the proof of
Theorem 2 is that of problems where one is given access to a
sampling mechanism from some distribution D. (This notion
originates and features prominently in the theory of proba-
bly approximately correct (PAC) learning.) AI-by-Learning
is a problem where this is the case. For such problems, the
assumption is that there is some fixed probability distribu-
tion D (over data points whose description is of a given
size n), but we do not know what distribution it is. Never-
theless, algorithms are given a way of sampling from this
distribution D, in the form of a (probabilistic) oracle. The
requirement on the algorithm is that it produces an output
that satisfies a given property with high probability, where
the randomness ranges over the randomness in the sampling.
Typically, this property depends on the distribution D. Such
problems thus involve a worst-case interpretation over pos-
sible distributions—that is, the algorithm should satisfy the
requirements regardless of what the actual distribution D is
(for more details on PAC learning, see e.g. Kearns & Vazi-
rani, 1994).
Probability theory.
The proof of Theorem 2 involves
some probability-theoretic analysis. We will assume the
reader to be familiar with basic notions from probability the-
ory. (For a compact overview of basic probability-theoretic
notions, see, e.g., Arora & Barak, 2009, Appendix A.2.) The
following well-known statements will be used in the proof,
which we will briefly overview here.
Proposition (Union bound). Let A1, A2, . . . be a count-
able number of events. Then Pr[S
i Ai] ≤P
i Pr[Ai].
Proposition (Markov’s inequality). Let X be a nonneg-
ative random variable and let a > 0. Then Pr[X ≥a] ≤
E[X]/a, where E[X] is the expected value of X.
Proposition (Hoeffding’s inequality). Let X1, . . . , Xn be
independent random variables such that 0 ≤Xi ≤1 for
all i. Let S n be the sum of these random variables, and
let E[S n] be the expected value of this sum. Then for any
t > 0, Pr[S n −E[S n] ≥t] ≤e−2t2/n.
The proof.
With the above notions in place, we will then
turn to the detailed proof of Theorem 2, which captures the
intractability of AI-by-Learning under the widely-held as-
sumption that NP ⊈BPP (see, e.g. Arora & Barak, 2009,
Chapter 7).
Theorem 2 (Ingenia Theorem). If there is a learning
mechanism that solves AI-by-Learning in polynomial
time, then NP ⊆BPP.
Proof. Suppose that there exists a learning mechanism M
that solves AI-by-Learning in polynomial time. This means
that there exist non-negligible functions δ, ϵ such that when
dealing with situations of description length n, regardless of
the distribution Dn that the mechanism M can sample from,
it learns an algorithm A that with probability at least δ(n)
satisfies that Prs∼Dn[A(s) ∈Bs] ≥|Bs|/|B| + ϵ(n).20 Let d be
such that δ(n) ≥1/nd for sufficiently large n, and let e be
such that ϵ(n) ≥1/ne for sufficiently large n.
We will show that then NP ⊆BPP, by showing that
there exists a polynomial-time probabilistic algorithm that
solves Perfect-vs-Chance with (two-sided) bounded error.
Since Perfect-vs-Chance is NP-hard (under randomized re-
ductions), this suffices to show that NP ⊆BPP, by the
following argument. Since Perfect-vs-Chance is NP-hard,
there is a polynomial-time randomized reduction from any
problem Q in NP to Perfect-vs-Chance. Having a bounded-
error polynomial-time probabilistic algorithm for Perfect-
vs-Chance then allows us to construct a similar such algo-
rithm for Q, by composing the reduction and the algorithm
for Perfect-vs-Chance. Therefore, we can solve any problem
20In the case that there exists no such algorithm A (of description
length |LA| ≤K), the learning mechanism may output anything.
For the sake of presentation, we will assume that in each case, the
mechanism outputs an algorithm A.

RECLAIMING AI
21
in NP in probabilistic polynomial-time with bounded two-
sided error, showing that NP ⊆BPP.
The algorithm for Perfect-vs-Chance works as follows.
Take an arbitrary instance for Perfect-vs-Chance, consisting
of integers n and k and a distribution D over {0, 1}n×{0, 1} (in
the form of a circuit C that takes as input a string specifying
random bits and outputs elements of {0, 1}n × {0, 1}).
The algorithm will use a subroutine that simulates the
learning mechanism M, run on a setting where situations
are (described using strings) of length n, where K = k and
where the data that the mechanism M has sampling access
to is given by the distribution D. In particular, this works as
follows. The set S of situations is the set {0, 1}n, and the set B
of behaviors is {0, 1}. Every time that the mechanism M asks
for a data sample (s, b) consisting of a situation s ∈S with a
corresponding appropriate behavior b ∈Bs, the simulation of
M produces a random string r that is fed as input to the cir-
cuit C, resulting in an element (s, b) ∈{0, 1}n × {0, 1}, which
is given to the mechanism M as data sample. The simulation
of the learning mechanism M will yield a learned algorithm
A, which is returned as output of the subroutine. Moreover,
since the learning mechanism runs in polynomial time, such
a simulation can also be done in polynomial time.
After running the subroutine that simulates M, we will
evaluate the quality of the resulting learned algorithm A by
using additional samples from D and counting on how many
of these situations s, the algorithm A returns an appropriate
behavior b ∈Bs. In particular, we will use L1 additional sam-
ples, where the exact value of L1 is to be specified later.
The algorithm runs the simulation subroutine L2 times,
where the exact value of L2 is to be specified later. For each
run of the subroutine, it evaluates the resulting learned algo-
rithm A as described above. From all of these runs, it picks
the algorithm A⋆that performed best in the quality evalu-
ation. Let ρ be the fraction of data points in the evaluation
phase (i.e., situations s) for which A⋆provided an appropri-
ate behavior b ∈Bs.
Based on the observed correctness rate ρ of the learned
algorithm A⋆, the algorithm will give an answer for the input
of Perfect-vs-Chance. If ρ ≥1/2 + 1/nd, then the algorithm
will answer Yes, and otherwise, the algorithm will answer
No.
We will ensure that L1 and L2 are polynomial in n, so this
algorithm runs in polynomial time. Let us analyze the error
probability of the algorithm for Perfect-vs-Chance. We will
show that it outputs the correct answer with probability at
least 2/3.
Suppose that the instance of Perfect-vs-Chance is a yes-
instance. In this case, because by the promise of Perfect-
vs-Chance there exists an efficient algorithm that makes no
errors at all, we know that there exists an algorithm A with
description length |LA| ≤K such that Pr[A(s) ∈Bs] ≥
|Bs|/|B| + ϵ(n)—call this property (⋆). Therefore, we have the
guarantee that in each single simulation of M, with probabil-
ity at least δ(n), the learned algorithm A satisfies (⋆).
We will consider the probability that the algorithm A
learned in any single simulation achieves a quality evalu-
ation that is < 1/2 + 1/ne. Suppose that A satisfies (⋆).
Then the probability that on L2 independently drawn sam-
ples the algorithm A outputs the incorrect answer at least
(1/2 −1/ne) · L2 + 1 times, by Markov’s inequality, is at most
1 −(L2/2 −L2/ne + 1)−1. This means that the probability
that the observed correctness rate of the learned algorithm A
(based on L2 new samples) is at least 1/2 + 1/ne is at least
(L2/2 −L2/ne + 1)−1. By ensuring that L2 ≥ne (which we
will do), we get that this probability is at least 2/L2. Since
A satisfies (⋆) with probability at least δ(n), we get that the
observed correctness rate of A is at least 1/2+1/ne is at least
2δ(n)/L2 ≥1/(L2nd), for sufficiently large values of n.
Next, we consider the probability that all algorithms A
learned in the L1 simulations of M achieve a quality evalua-
tion that is < 1/2+1/ne. By Hoeffding’s inequality, this prob-
ability is less than e−2L1/(L2nd)2. By ensuring that L1 ≥(L2nd)2
(which we will do), we get that this probability is less than
e−2 ≤1/3. This concludes the argument that if the instance
of Perfect-vs-Chance is a yes-instance, then the algorithm
will output Yes with probability at least 2/3.
Conversely, suppose that the instance of Perfect-vs-
Chance is a no-instance. In this case, we know that regardless
of which learned algorithm A any simulation of M outputs, it
holds that Prs∼Dn[A(s) ∈Bs] ≤1/2 + 2−√n.
We will consider the probability that the algorithm A
learned in any single simulation achieves a quality evalua-
tion that is < 1/2 + 1/ne. Then the probability that on L2 in-
dependently drawn samples the algorithm A outputs the cor-
rect answer at least (1/2 + 1/ne) · L2 times, by Hoeffding’s
inequality, is at most e−2t2/L2 for t = L2/2+ L2/(ne)−L2/2
√n.
Since 2
√n ≥ne for sufficiently large values of n, we get that
this probability is at most e−L2/2.
Next, we consider the probability that all algorithms A
learned in the L1 simulations of M achieve a quality evalua-
tion that is < 1/2+1/ne. By the union bound, this probability
is at most L1e−L2/2. By ensuring that L1 ≤eL2/2/3 for suffi-
ciently large values of n (which we will do), we get that if
the instance of Perfect-vs-Chance is a no-instance, then the
algorithm will output No with probability at least 2/3.
What remains is to fix values of L1 and L2 that are polyno-
mial in n, and that (for sufficiently large values of n) satisfy
the conditions that L2 ≥ne, L1 ≥(L2nd)2, and L1 ≤eL2/2/3.
We let L1 = n2de and L2 = ne, which satisfies all con-
straints.
■

